{"cell_type":{"4e5671cf":"code","7adb3f65":"code","e214842f":"code","07a53cba":"code","a90db9e9":"code","d44d2246":"code","7e4f0c35":"code","b28ce2a0":"code","ec427f80":"code","b9e07eac":"code","faac607e":"code","99c59273":"code","c265eab3":"code","04b1111f":"code","0fce801a":"code","e643abd3":"code","9a6d2a3a":"code","9414577c":"code","b1229867":"code","9b86eb2f":"code","b8f65707":"code","fddc675d":"code","ce37d396":"code","e738e042":"code","7c99eb70":"code","81c418ae":"code","374782c8":"code","5edd0e45":"code","62d90042":"code","6d9d525e":"code","0b820a83":"markdown","04f38071":"markdown","436dde45":"markdown","8e24bd79":"markdown","73a316d7":"markdown","417ee3cc":"markdown","062308b2":"markdown","58d24ea6":"markdown","dd84b88a":"markdown","e6266d08":"markdown","ab50e6e1":"markdown","1ec82c55":"markdown","4e33b5fc":"markdown","23bf6f54":"markdown","bcfd32da":"markdown","762bbef4":"markdown","8b0b383f":"markdown","016a38a3":"markdown","e36525d8":"markdown","28c87400":"markdown","57c29a37":"markdown","c658e7eb":"markdown","72413c5f":"markdown","d8d0ba19":"markdown","cfd329ef":"markdown","2de901ff":"markdown","f867fa28":"markdown","4d49b21b":"markdown","6f89db68":"markdown"},"source":{"4e5671cf":"USE_SUMMARY = True\nFIND_PDFS = True\nSEARCH_MEDRXIV = True\nSEARCH_PUBMED = True","7adb3f65":"import os\n#%%capture\n!curl -O https:\/\/download.java.net\/java\/GA\/jdk11\/9\/GPL\/openjdk-11.0.2_linux-x64_bin.tar.gz\n!mv openjdk-11.0.2_linux-x64_bin.tar.gz \/usr\/lib\/jvm\/; cd \/usr\/lib\/jvm\/; tar -zxvf openjdk-11.0.2_linux-x64_bin.tar.gz\n!update-alternatives --install \/usr\/bin\/java java \/usr\/lib\/jvm\/jdk-11.0.2\/bin\/java 1\n!update-alternatives --set java \/usr\/lib\/jvm\/jdk-11.0.2\/bin\/java\nos.environ[\"JAVA_HOME\"] = \"\/usr\/lib\/jvm\/jdk-11.0.2\"","e214842f":"#%%capture\n!pip install pyserini==0.8.1.0\nfrom pyserini.search import pysearch","07a53cba":"#%%capture\n!wget -O lucene.tar.gz https:\/\/www.dropbox.com\/s\/j55t617yhvmegy8\/lucene-index-covid-2020-04-10.tar.gz\n!tar xvfz lucene.tar.gz\nminDate = '2020\/04\/09'\nluceneDir = 'lucene-index-covid-2020-04-10\/'","a90db9e9":"import tensorflow as tf\nimport tensorflow_hub as hub\n!mkdir \/kaggle\/working\/sentence_wise_email\/\n!mkdir \/kaggle\/working\/sentence_wise_email\/module\/\n!mkdir \/kaggle\/working\/sentence_wise_email\/module\/module_useT\n# Download the module, and uncompress it to the destination folder. \n!curl -L \"https:\/\/tfhub.dev\/google\/universal-sentence-encoder-large\/3?tf-hub-format=compressed\" | tar -zxvC \/kaggle\/working\/\/sentence_wise_email\/module\/module_useT","d44d2246":"import torch\nfrom transformers import BertForQuestionAnswering\nfrom transformers import BertTokenizer\nfrom transformers import BartTokenizer, BartForConditionalGeneration\ntorch_device = 'cuda' if torch.cuda.is_available() else 'cpu'\n\nQA_MODEL = BertForQuestionAnswering.from_pretrained('bert-large-uncased-whole-word-masking-finetuned-squad')\nQA_TOKENIZER = BertTokenizer.from_pretrained('bert-large-uncased-whole-word-masking-finetuned-squad')\nQA_MODEL.to(torch_device)\nQA_MODEL.eval()\n\nif USE_SUMMARY:\n    SUMMARY_TOKENIZER = BartTokenizer.from_pretrained('bart-large-cnn')\n    SUMMARY_MODEL = BartForConditionalGeneration.from_pretrained('bart-large-cnn')\n    SUMMARY_MODEL.to(torch_device)\n    SUMMARY_MODEL.eval()","7e4f0c35":"if FIND_PDFS:\n    !pip install metapub","b28ce2a0":"!pip install biopython\nfrom Bio import Entrez, Medline\n\ntry:\n    from StringIO import StringIO\nexcept ImportError:\n    from io import StringIO\nimport re","ec427f80":"query = 'Which non-pharmaceutical interventions limit tramsission'\nkeywords = '2019-nCoV, SARS-CoV-2, COVID-19, non-pharmaceutical interventions, npi'","b9e07eac":"import json\nsearcher = pysearch.SimpleSearcher(luceneDir)\nhits = searcher.search(query + '. ' + keywords)\nn_hits = len(hits)\n## collect the relevant data in a hit dictionary\nhit_dictionary = {}\nfor i in range(0, n_hits):\n    doc_json = json.loads(hits[i].raw)\n    idx = str(hits[i].docid)\n    hit_dictionary[idx] = doc_json\n    hit_dictionary[idx]['title'] = hits[i].lucene_document.get(\"title\")\n    hit_dictionary[idx]['authors'] = hits[i].lucene_document.get(\"authors\")\n    hit_dictionary[idx]['doi'] = hits[i].lucene_document.get(\"doi\")\n\n## scrub the abstracts in prep for BERT-SQuAD\nfor idx,v in hit_dictionary.items():\n    abs_dirty = v['abstract']\n    # looks like the abstract value can be an empty list\n    v['abstract_paragraphs'] = []\n    v['abstract_full'] = ''\n\n    if abs_dirty:\n        # looks like if it is a list, then the only entry is a dictionary wher text is in 'text' key\n        # looks like it is broken up by paragraph if it is in that form.  lets make lists for every paragraph\n        # and a new entry that is full abstract text as both could be valuable for BERT derrived QA\n\n\n        if isinstance(abs_dirty, list):\n            for p in abs_dirty:\n                v['abstract_paragraphs'].append(p['text'])\n                v['abstract_full'] += p['text'] + ' \\n\\n'\n\n        # looks like in some cases the abstract can be straight up text so we can actually leave that alone\n        if isinstance(abs_dirty, str):\n            v['abstract_paragraphs'].append(abs_dirty)\n            v['abstract_full'] += abs_dirty + ' \\n\\n'\n","faac607e":"def embed_useT(module):\n    with tf.Graph().as_default():\n        sentences = tf.compat.v1.placeholder(tf.string)\n        embed = hub.Module(module)\n        embeddings = embed(sentences)\n        session = tf.compat.v1.train.MonitoredSession()\n    return lambda x: session.run(embeddings, {sentences: x})\nembed_fn = embed_useT('\/kaggle\/working\/sentence_wise_email\/module\/module_useT')","99c59273":"import numpy as np\ndef reconstructText(tokens, start=0, stop=-1):\n    tokens = tokens[start: stop]\n    if '[SEP]' in tokens:\n        sepind = tokens.index('[SEP]')\n        tokens = tokens[sepind+1:]\n    txt = ' '.join(tokens)\n    txt = txt.replace(' ##', '')\n    txt = txt.replace('##', '')\n    txt = txt.strip()\n    txt = \" \".join(txt.split())\n    txt = txt.replace(' .', '.')\n    txt = txt.replace('( ', '(')\n    txt = txt.replace(' )', ')')\n    txt = txt.replace(' - ', '-')\n    txt_list = txt.split(' , ')\n    txt = ''\n    nTxtL = len(txt_list)\n    if nTxtL == 1:\n        return txt_list[0]\n    newList =[]\n    for i,t in enumerate(txt_list):\n        if i < nTxtL -1:\n            if t[-1].isdigit() and txt_list[i+1][0].isdigit():\n                newList += [t,',']\n            else:\n                newList += [t, ', ']\n        else:\n            newList += [t]\n    return ''.join(newList)\n\n\ndef makeBERTSQuADPrediction(document, question):\n    ## we need to rewrite this function so that it chuncks the document into 250-300 word segments with\n    ## 50 word overlaps on either end so that it can understand and check longer abstracts\n    nWords = len(document.split())\n    input_ids_all = QA_TOKENIZER.encode(question, document)\n    tokens_all = QA_TOKENIZER.convert_ids_to_tokens(input_ids_all)\n    overlapFac = 1.1\n    if len(input_ids_all)*overlapFac > 2048:\n        nSearchWords = int(np.ceil(nWords\/5))\n        quarter = int(np.ceil(nWords\/4))\n        docSplit = document.split()\n        docPieces = [' '.join(docSplit[:int(nSearchWords*overlapFac)]), \n                     ' '.join(docSplit[quarter-int(nSearchWords*overlapFac\/2):quarter+int(quarter*overlapFac\/2)]),\n                     ' '.join(docSplit[quarter*2-int(nSearchWords*overlapFac\/2):quarter*2+int(quarter*overlapFac\/2)]),\n                     ' '.join(docSplit[quarter*3-int(nSearchWords*overlapFac\/2):quarter*3+int(quarter*overlapFac\/2)]),\n                     ' '.join(docSplit[-int(nSearchWords*overlapFac):])]\n        input_ids = [QA_TOKENIZER.encode(question, dp) for dp in docPieces]        \n        \n    elif len(input_ids_all)*overlapFac > 1536:\n        nSearchWords = int(np.ceil(nWords\/4))\n        third = int(np.ceil(nWords\/3))\n        docSplit = document.split()\n        docPieces = [' '.join(docSplit[:int(nSearchWords*overlapFac)]), \n                     ' '.join(docSplit[third-int(nSearchWords*overlapFac\/2):third+int(nSearchWords*overlapFac\/2)]),\n                     ' '.join(docSplit[third*2-int(nSearchWords*overlapFac\/2):third*2+int(nSearchWords*overlapFac\/2)]),\n                     ' '.join(docSplit[-int(nSearchWords*overlapFac):])]\n        input_ids = [QA_TOKENIZER.encode(question, dp) for dp in docPieces]        \n        \n    elif len(input_ids_all)*overlapFac > 1024:\n        nSearchWords = int(np.ceil(nWords\/3))\n        middle = int(np.ceil(nWords\/2))\n        docSplit = document.split()\n        docPieces = [' '.join(docSplit[:int(nSearchWords*overlapFac)]), \n                     ' '.join(docSplit[middle-int(nSearchWords*overlapFac\/2):middle+int(nSearchWords*overlapFac\/2)]),\n                     ' '.join(docSplit[-int(nSearchWords*overlapFac):])]\n        input_ids = [QA_TOKENIZER.encode(question, dp) for dp in docPieces]\n    elif len(input_ids_all)*overlapFac > 512:\n        nSearchWords = int(np.ceil(nWords\/2))\n        docSplit = document.split()\n        docPieces = [' '.join(docSplit[:int(nSearchWords*overlapFac)]), ' '.join(docSplit[-int(nSearchWords*overlapFac):])]\n        input_ids = [QA_TOKENIZER.encode(question, dp) for dp in docPieces]\n    else:\n        input_ids = [input_ids_all]\n    absTooLong = False    \n    \n    answers = []\n    cons = []\n    for iptIds in input_ids:\n        tokens = QA_TOKENIZER.convert_ids_to_tokens(iptIds)\n        sep_index = iptIds.index(QA_TOKENIZER.sep_token_id)\n        num_seg_a = sep_index + 1\n        num_seg_b = len(iptIds) - num_seg_a\n        segment_ids = [0]*num_seg_a + [1]*num_seg_b\n        assert len(segment_ids) == len(iptIds)\n        n_ids = len(segment_ids)\n        #print(n_ids)\n\n        if n_ids < 512:\n            start_scores, end_scores = QA_MODEL(torch.tensor([iptIds]).to(torch_device), \n                                     token_type_ids=torch.tensor([segment_ids]).to(torch_device))\n        else:\n            #this cuts off the text if its more than 512 words so it fits in model space\n            #need run multiple inferences for longer text. add to the todo\n            print('****** warning only considering first 512 tokens, document is '+str(nWords)+' words long.  There are '+str(n_ids)+ ' tokens')\n            absTooLong = True\n            start_scores, end_scores = QA_MODEL(torch.tensor([iptIds[:512]]).to(torch_device), \n                                     token_type_ids=torch.tensor([segment_ids[:512]]).to(torch_device))\n        start_scores = start_scores[:,1:-1]\n        end_scores = end_scores[:,1:-1]\n        answer_start = torch.argmax(start_scores)\n        answer_end = torch.argmax(end_scores)\n        #print(answer_start, answer_end)\n        answer = reconstructText(tokens, answer_start, answer_end+2)\n    \n        if answer.startswith('. ') or answer.startswith(', '):\n            answer = answer[2:]\n            \n        c = start_scores[0,answer_start].item()+end_scores[0,answer_end].item()\n        answers.append(answer)\n        cons.append(c)\n    \n    maxC = max(cons)\n    iMaxC = [i for i, j in enumerate(cons) if j == maxC][0]\n    confidence = cons[iMaxC]\n    answer = answers[iMaxC]\n    \n    sep_index = tokens_all.index('[SEP]')\n    full_txt_tokens = tokens_all[sep_index+1:]\n    \n    abs_returned = reconstructText(full_txt_tokens)\n\n    ans={}\n    ans['answer'] = answer\n    #print(answer)\n    if answer.startswith('[CLS]') or answer_end.item() < sep_index or answer.endswith('[SEP]'):\n        ans['confidence'] = -1000000\n    else:\n        #confidence = torch.max(start_scores) + torch.max(end_scores)\n        #confidence = np.log(confidence.item())\n        ans['confidence'] = confidence\n    #ans['start'] = answer_start.item()\n    #ans['end'] = answer_end.item()\n    ans['abstract_bert'] = abs_returned\n    ans['abs_too_long'] = absTooLong\n    return ans","c265eab3":"from tqdm import tqdm\ndef searchAbstracts(hit_dictionary, question):\n    abstractResults = {}\n    for k,v in tqdm(hit_dictionary.items()):\n        abstract = v['abstract_full']\n        if abstract:\n            ans = makeBERTSQuADPrediction(abstract, question)\n            if ans['answer']:\n                confidence = ans['confidence']\n                abstractResults[confidence]={}\n                abstractResults[confidence]['answer'] = ans['answer']\n                #abstractResults[confidence]['start'] = ans['start']\n                #abstractResults[confidence]['end'] = ans['end']\n                abstractResults[confidence]['abstract_bert'] = ans['abstract_bert']\n                abstractResults[confidence]['idx'] = k\n                abstractResults[confidence]['abs_too_long'] = ans['abs_too_long']\n                \n    cList = list(abstractResults.keys())\n\n    if cList:\n        maxScore = max(cList)\n        total = 0.0\n        exp_scores = []\n        for c in cList:\n            s = np.exp(c-maxScore)\n            exp_scores.append(s)\n        total = sum(exp_scores)\n        for i,c in enumerate(cList):\n            abstractResults[exp_scores[i]\/total] = abstractResults.pop(c)\n    return abstractResults","04b1111f":"answers = searchAbstracts(hit_dictionary, query)","0fce801a":"workingPath = '\/kaggle\/working'\nimport pandas as pd\nif FIND_PDFS:\n    from metapub import UrlReverse\n    from metapub import FindIt\nfrom IPython.core.display import display, HTML\n\n#from summarizer import Summarizer\n#summarizerModel = Summarizer()\ndef displayResults(hit_dictionary, answers, question):\n    \n    question_HTML = '<div style=\"font-family: Times New Roman; font-size: 28px; padding-bottom:28px\"><b>Query<\/b>: '+question+'<\/div>'\n    #all_HTML_txt = question_HTML\n    confidence = list(answers.keys())\n    confidence.sort(reverse=True)\n    \n    confidence = list(answers.keys())\n    confidence.sort(reverse=True)\n    \n\n    for c in confidence:\n        if c>0 and c <= 1 and len(answers[c]['answer']) != 0:\n            if 'idx' not in  answers[c]:\n                continue\n            rowData = []\n            idx = answers[c]['idx']\n            title = hit_dictionary[idx]['title']\n            authors = hit_dictionary[idx]['authors'] + ' et al.'\n            doi = '<a href=\"https:\/\/doi.org\/'+hit_dictionary[idx]['doi']+'\" target=\"_blank\">' + title +'<\/a>'\n\n            \n            full_abs = answers[c]['abstract_bert']\n            bert_ans = answers[c]['answer']\n            \n            \n            split_abs = full_abs.split(bert_ans)\n            sentance_beginning = split_abs[0][split_abs[0].rfind('.')+1:]\n            if len(split_abs) == 1:\n                sentance_end_pos = len(full_abs)\n                sentance_end =''\n            else:\n                sentance_end_pos = split_abs[1].find('. ')+1\n                if sentance_end_pos == 0:\n                    sentance_end = split_abs[1]\n                else:\n                    sentance_end = split_abs[1][:sentance_end_pos]\n                \n            #sentance_full = sentance_beginning + bert_ans+ sentance_end\n            answers[c]['full_answer'] = sentance_beginning+bert_ans+sentance_end\n            answers[c]['sentence_beginning'] = sentance_beginning\n            answers[c]['sentence_end'] = sentance_end\n            answers[c]['title'] = title\n            answers[c]['doi'] = doi\n            if 'pdfLink' in hit_dictionary[idx]:\n                answers[c]['pdfLink'] = hit_dictionary[idx]['pdfLink']\n                \n        else:\n            answers.pop(c)\n    \n    \n    ## now rerank based on semantic similarity of the answers to the question\n    cList = list(answers.keys())\n    allAnswers = [answers[c]['full_answer'] for c in cList]\n    \n    messages = [question]+allAnswers\n    \n    encoding_matrix = embed_fn(messages)\n    similarity_matrix = np.inner(encoding_matrix, encoding_matrix)\n    rankings = similarity_matrix[1:,0]\n    \n    for i,c in enumerate(cList):\n        answers[rankings[i]] = answers.pop(c)\n\n    ## now form pandas dv\n    confidence = list(answers.keys())\n    confidence.sort(reverse=True)\n    pandasData = []\n    ranked_aswers = []\n    for c in confidence:\n        rowData=[]\n        title = answers[c]['title']\n        doi = answers[c]['doi']\n        idx = answers[c]['idx']\n        rowData += [idx]            \n        sentance_html = '<div>' +answers[c]['sentence_beginning'] + \" <font color='red'>\"+answers[c]['answer']+\"<\/font> \"+answers[c]['sentence_end']+'<\/div>'\n        \n        rowData += [sentance_html, c, doi]\n        pandasData.append(rowData)\n        ranked_aswers.append(' '.join([answers[c]['full_answer']]))\n    \n    if FIND_PDFS or SEARCH_MEDRXIV:\n        pdata2 = []\n        for rowData in pandasData:\n            rd = rowData\n            idx = rowData[0]\n            if 'pdfLink' in answers[rowData[2]]:\n                rd += ['<a href=\"'+answers[rowData[2]]['pdfLink']+'\" target=\"_blank\">PDF Link<\/a>']\n            elif FIND_PDFS:\n                if str(idx).startswith('pm_'):\n                    pmid = idx[3:]\n                else:\n                    try:\n                        test = UrlReverse('https:\/\/doi.org\/'+hit_dictionary[idx]['doi'])\n                        if test is not None:\n                            pmid = test.pmid\n                        else:\n                            pmid = None\n                    except:\n                        pmid = None\n                pdfLink = None\n                if pmid is not None:\n                    try:\n                        pdfLink = FindIt(str(pmid))\n                    except:\n                        pdfLink = None\n                if pdfLink is not None:\n                    pdfLink = pdfLink.url\n\n                if pdfLink is None:\n\n                    rd += ['Not Available']\n                else:\n                    rd += ['<a href=\"'+pdfLink+'\" target=\"_blank\">PDF Link<\/a>']\n            else:\n                rd += ['Not Available']\n            pdata2.append(rowData)\n    else:\n        pdata2 = pandasData\n        \n    \n    display(HTML(question_HTML))\n    \n    if USE_SUMMARY:\n        ## try generating an exacutive summary with extractive summarizer\n        allAnswersTxt = ' '.join(ranked_aswers[:6]).replace('\\n','')\n    #    exec_sum = summarizerModel(allAnswersTxt, min_length=1, max_length=500)    \n     #   execSum_HTML = '<div style=\"font-family: Times New Roman; font-size: 18px; padding-bottom:18px\"><b>BERT Extractive Summary:<\/b>: '+exec_sum+'<\/div>'\n\n        answers_input_ids = SUMMARY_TOKENIZER.batch_encode_plus([allAnswersTxt], return_tensors='pt', max_length=1024)['input_ids'].to(torch_device)\n        summary_ids = SUMMARY_MODEL.generate(answers_input_ids,\n                                               num_beams=10,\n                                               length_penalty=1.2,\n                                               max_length=1024,\n                                               min_length=64,\n                                               no_repeat_ngram_size=4)\n\n        exec_sum = SUMMARY_TOKENIZER.decode(summary_ids.squeeze(), skip_special_tokens=True)\n        execSum_HTML = '<div style=\"font-family: Times New Roman; font-size: 18px; margin-bottom:1pt\"><b>BART Abstractive Summary:<\/b>: '+exec_sum+'<\/div>'\n        display(HTML(execSum_HTML))\n        warning_HTML = '<div style=\"font-family: Times New Roman; font-size: 12px; padding-bottom:12px; color:#CCCC00; margin-top:1pt\"> Warning this is an autogenerated summary based on semantic search of abstracts, always examine the sources before accepting this conclusion.  If the evidence only mentions topic in passing or the evidence is not clear, the summary will likely not clearly answer the question.<\/div>'\n        display(HTML(warning_HTML))\n\n#    display(HTML('<div style=\"font-family: Times New Roman; font-size: 18px; padding-bottom:18px\"><b>Body of Evidence:<\/b><\/div>'))\n    \n    if FIND_PDFS or SEARCH_MEDRXIV:\n        df = pd.DataFrame(pdata2, columns = ['Lucene ID', 'BERT-SQuAD Answer with Highlights', 'Confidence', 'Title\/Link','PDF Link'])\n    else:\n        df = pd.DataFrame(pdata2, columns = ['Lucene ID', 'BERT-SQuAD Answer with Highlights', 'Confidence', 'Title\/Link'])\n        \n    display(HTML(df.to_html(render_links=True, escape=False)))\n    \n#displayResults(hit_dictionary, answers, query)","e643abd3":"def getrecord(id, db):\n    handle = Entrez.efetch(db=db, id=id, rettype='Medline', retmode='text')\n    rec = handle.read()\n    handle.close()\n    return rec\n\ndef pubMedSearch(terms, db='pubmed', mindate='2019\/12\/01'):\n    handle = Entrez.esearch(db = db, term = terms, retmax=10, mindate=mindate)\n    record = Entrez.read(handle)\n    record_db = {}\n    for id in record['IdList']:\n        try:\n            record = getrecord(id,db)\n            recfile = StringIO(record)\n            rec = Medline.read(recfile)\n            if 'AB' in rec and 'AU' in rec and 'LID' in rec and 'TI' in rec:\n                if '10.' in rec['LID'] and ' [doi]' in rec['LID']:\n                    record_db['pm_'+id] = {}\n                    record_db['pm_'+id]['authors'] = ' '.join(rec['AU'])\n                    record_db['pm_'+id]['doi'] = '10.'+rec['LID'].split('10.')[1].split(' [doi]')[0]\n                    record_db['pm_'+id]['abstract'] = rec['AB']\n                    record_db['pm_'+id]['title'] = rec['TI']\n        except:\n            print(\"Problem trying to retrieve: \" + str(id))\n        \n    return record_db\n\n\nEntrez.email = 'pubmedkaggle@gmail.com'\n","9a6d2a3a":"import requests\nimport json\nfrom bs4 import BeautifulSoup\nimport re\nimport datetime\nimport dateutil.parser as dparser\n\ndef medrxivSearch(query, n_pages=1):\n    results = {}\n    q = query\n    for x in range(n_pages):\n        PARAMS = {\n            'page': x\n        }\n        r = requests.get('https:\/\/www.medrxiv.org\/search\/' + q, params = PARAMS)\n        content = r.text\n        page = BeautifulSoup(content, 'lxml')\n        \n        for entry in page.find_all(\"a\", attrs={\"class\": \"highwire-cite-linked-title\"}):\n            title = \"\"\n            url = \"\"\n            pubDate = \"\"\n            journal = None\n            abstract = \"\"\n            authors = []\n            database = \"medRxiv\"\n            \n            \n            url = \"https:\/\/www.medrxiv.org\" + entry.get('href')\n            \n            request_entry = requests.get(url)\n            content_entry = request_entry.text\n            page_entry = BeautifulSoup(content_entry, 'lxml')\n            doi = page_entry.find(\"span\", attrs={\"class\": \"highwire-cite-metadata-doi\"}).text.split('doi.org\/')[-1]\n            #print(page_entry)\n\n            #getting pubDate\n            pubDate = page_entry.find_all(\"div\", attrs = {\"class\": \"pane-content\"})\n            pubDate = pubDate[10]\n            pubDate = str(dparser.parse(pubDate, fuzzy = True))\n            pubDate = datetime.datetime.strptime(pubDate, '%Y-%m-%d %H:%M:%S')\n            pubDate = pubDate.strftime('%b %d %Y')\n            date = pubDate.split()\n            month = date[0]\n            day = date[1]\n            year = date[2]\n            pubDate = {\n                'year': year,\n                'month': month,\n                'day': day\n            }\n\n            #getting title\n            title = page_entry.find(\"h1\", attrs={\"class\": \"highwire-cite-title\"}).text\n            \n            #getting abstract\n            abstract = page_entry.find(\"p\", attrs = {\"id\": \"p-2\"}).text.replace('\\n', ' ')\n\n            #getting authors \n            givenNames = page_entry.find_all(\"span\", attrs={\"class\": \"nlm-given-names\"})\n            surnames = page_entry.find_all(\"span\",  attrs={\"class\": \"nlm-surname\"})\n            names = list(zip(givenNames,surnames))\n            for author in names:\n                name = author[0].text + ' ' + author[1].text\n                if name not in authors:\n                    authors.append(name)\n            \n            result = {\n                'title': title,\n                'url': url,\n                'pubDate': pubDate,\n                'journal': journal,\n                'abstract': abstract,\n                'authors': authors[0],\n                'database': database,\n                'doi': doi,\n                'pdfLink': url+'.full.pdf'\n            }\n            results['mrx_'+result['doi'].split('\/')[-1]] = result\n            #break\n    return results","9414577c":"def searchDatabase(question, keywords, pysearch, lucene_database, pm_kw = '', minDate='2019\/12\/01', k=20):\n    ## search the lucene database with a combination of the question and the keywords\n    searcher = pysearch.SimpleSearcher(lucene_database)\n    hits = searcher.search(question + '. ' + keywords, k=k)\n    n_hits = len(hits)\n    ## collect the relevant data in a hit dictionary\n    hit_dictionary = {}\n    for i in range(0, n_hits):\n        doc_json = json.loads(hits[i].raw)\n        idx = str(hits[i].docid)\n        hit_dictionary[idx] = doc_json\n        hit_dictionary[idx]['title'] = hits[i].lucene_document.get(\"title\")\n        hit_dictionary[idx]['authors'] = hits[i].lucene_document.get(\"authors\")\n        hit_dictionary[idx]['doi'] = hits[i].lucene_document.get(\"doi\")\n        \n    \n    titleList = [h['title'] for h in hit_dictionary.values()]\n    \n    if pm_kw:\n        if SEARCH_PUBMED:\n            new_hits = pubMedSearch(pm_kw, db='pubmed', mindate=minDate)\n            for id,h in new_hits.items():\n                if h['title'] not in titleList:\n                    titleList.append(h['title'])\n                hit_dictionary[id] = h\n        if SEARCH_MEDRXIV:\n            new_hits = medrxivSearch(pm_kw)\n            for id,h in new_hits.items():\n                if h['title'] not in titleList:\n                    titleList.append(h['title'])\n                hit_dictionary[id] = h\n    \n    ## scrub the abstracts in prep for BERT-SQuAD\n    for idx,v in hit_dictionary.items():\n        abs_dirty = v['abstract']\n        # looks like the abstract value can be an empty list\n        v['abstract_paragraphs'] = []\n        v['abstract_full'] = ''\n\n        if abs_dirty:\n            # looks like if it is a list, then the only entry is a dictionary wher text is in 'text' key\n            # looks like it is broken up by paragraph if it is in that form.  lets make lists for every paragraph\n            # and a new entry that is full abstract text as both could be valuable for BERT derrived QA\n\n\n            if isinstance(abs_dirty, list):\n                for p in abs_dirty:\n                    v['abstract_paragraphs'].append(p['text'])\n                    v['abstract_full'] += p['text'] + ' \\n\\n'\n\n            # looks like in some cases the abstract can be straight up text so we can actually leave that alone\n            if isinstance(abs_dirty, str):\n                v['abstract_paragraphs'].append(abs_dirty)\n                v['abstract_full'] += abs_dirty + ' \\n\\n'\n    ## Search collected abstracts with BERT-SQuAD\n    answers = searchAbstracts(hit_dictionary, question)\n    \n    ## display results in a nice format\n    displayResults(hit_dictionary, answers, question)\n","b1229867":"#searchDatabase(query, keywords, pysearch, luceneDir, minDate=minDate)","9b86eb2f":"all_topics=[\n    'What is known about transmission, incubation, and environmental stability?',\n    'What do we know about COVID-19 risk factors?',\n    'What do we know about virus genetics, origin, and evolution?',\n    'What do we know about vaccines and therapeutics?',\n    'What do we know about non-pharmaceutical interventions?',\n    'What has been published about medical care?',\n    'What do we know about diagnostics and surveillance?'\n    'What has been published about information sharing and inter-sectoral collaboration?',\n    'What has been published about ethical and social science considerations?'\n]\ntopic_area = {}\n\n#0\n#What is known about transmission, incubation, and environmental stability?\nquestion_list = []\nkw_list = []\npm_kw_list = []\nquestion_list.append(\"Is the virus transmitted by aerisol, droplets, food, close contact, fecal matter, or water\")\nkw_list.append(\"2019-nCoV,SARS-CoV-2, COVID-19, coronavirus, novel coronavirus, person to person, human to human, interpersonal contact, air, water,fecal, surfaces, aerisol, transmission, shedding\")\npm_kw_list.append(\"2019-nCoV, transmission, shedding\")\n\nquestion_list.append( \"How long is the incubation period for the virus\")\nkw_list.append(\"2019-nCoV, SARS-CoV-2, COVID-19, coronavirus, novel coronavirus, hours, days, period\")\npm_kw_list.append(\"2019-nCoV, incubation period\")\n\nquestion_list.append(\"Can the virus be transmitted asymptomatically or during the incubation period\")\nkw_list.append(\"2019-nCoV, COVID-19, coronavirus, novel coronavirus, asymptomatic, person to person, human to human, transmission\")\npm_kw_list.append(\"2019-nCoV, asymptomatic, transmission\")\n\nquestion_list.append(\"What is the quantity of asymptomatic shedding\")\nkw_list.append(\"2019-nCoV, SARS-CoV-2, COVID-19, asymptomatic, shedding, percentage, rate, quantity, pediatric\")\npm_kw_list.append(\"2019-nCoV, asymptomatic, shedding\")\n\nquestion_list.append(\"How does temperature and humidity affect the tramsmission of 2019-nCoV\")\nkw_list.append(\"2019-nCoV, COVID-19, coronavirus, novel coronavirus, temperature, humidity\")\npm_kw_list.append(\"2019-nCoV, temperature, humidity\")\n\nquestion_list.append(\"How long can 2019-nCoV remain viable on inanimate, environmental, or common surfaces\")\nkw_list.append(\"2019-nCoV, SARS-CoV-2, COVID-19, inanimate, environmental, touch, copper, plastic, steel, wood, fabric, glass, porous, nonporous\")\npm_kw_list.append(\"2019-nCoV, surface\")\n\nquestion_list.append(\"What types of inanimate or environmental surfaces affect transmission, survival, or  inactivation of 2019-nCov\")\nkw_list.append(\"2019-nCoV, SARS-CoV-2, COVID-19, inanimate, environmental, touch, copper, plastic, steel, wood, fabric, glass, porous, nonporous\")\npm_kw_list.append(\"2019-nCoV, surface\")\n\nquestion_list.append(\"Can the virus be found in nasal discharge, sputum, urine, fecal matter, or blood\")\nkw_list.append(\"2019-nCoV, SARS-CoV-2, COVID-19, shedding, body fluid\")\npm_kw_list.append(\"2019-nCoV, body fluids\")\n\ntopic_area['What is known about transmission, incubation, and environmental stability?'] = list(zip(question_list,kw_list, pm_kw_list))\n\n\n\n#1\n#What do we know about COVID-19 risk factors?\nquestion_list = []\nkw_list = []\npm_kw_list = []\nquestion_list.append(\"What risk factors contribute to the severity of 2019-nCoV\")\nkw_list.append(\"2019-nCoV, SARS-CoV-2, COVID-19, coronavirus, novel coronavirus, susceptible, smoking, smoker, neonates, pregnant, socio-economic, behavioral, age, elderly, young, old, children\")\npm_kw_list.append(\"2019-nCoV, risk factors\")\n\nquestion_list.append(\"How does hypertension affect patients\")\nkw_list.append(\"2019-nCoV, SARS-CoV-2, COVID-19, hypertension, blood pressure, comorbidity\")\npm_kw_list.append(\"2019-nCoV, hypertension, comorbidity\")\n\nquestion_list.append(\"How does heart disease affect patients\")\nkw_list.append(\"2019-nCoV, SARS-CoV-2, COVID-19, heart disease, comorbidity\")\npm_kw_list.append(\"2019-nCoV, heart disease, comorbidity\")\n\nquestion_list.append(\"How does copd affect patients\")\nkw_list.append(\"2019-nCoV, SARS-CoV-2, COVID-19, copd, chronic obstructive pulmonary disease\")\npm_kw_list.append(\"2019-nCoV, copd, chronic obstructive pulmonary disease\")\n\nquestion_list.append(\"How does smoking affect 2019-nCoV patients\")\nkw_list.append(\"2019-nCoV, SARS-CoV-2, COVID-19, smoking, smoker\")\npm_kw_list.append(\"2019-nCoV, smoking, smoker\")\n\nquestion_list.append(\"How does pregnancy affect patients\")\nkw_list.append(\"2019-nCoV, SARS-CoV-2, COVID-19, pregnant, pregnancy\")\npm_kw_list.append(\"2019-nCoV, pregnant, pregnancy\")\n\nquestion_list.append(\"What are the case fatality rates for 2019-nCoV patients\")\nkw_list.append(\"2019-nCoV, SARS-CoV-2, COVID-19, coronavirus, novel coronavirus, fatality rate\")\npm_kw_list.append(\"2019-nCoV, fatality, statistics, death\")\n\nquestion_list.append(\"What is the case fatality rate in Italy\")\nkw_list.append(\"2019-nCoV, SARS-CoV-2, COVID-19, coronavirus, novel coronavirus, fatality rate, Italy\")\npm_kw_list.append(\"2019-nCoV, fatality, statistics, death\")\n\nquestion_list.append(\"What public health policies prevent or control the spread of 2019-nCoV\")\nkw_list.append(\"2019-nCoV, SARS-CoV-2, COVID-19, coronavirus, novel coronavirus, guidance, prevention measures, public health, community, prevention, administration, government, health department, policy, control measures, travel\")\npm_kw_list.append(\"2019-nCoV, guidance, public health,  policy, control measures\")\n\ntopic_area['What do we know about COVID-19 risk factors?'] = list(zip(question_list,kw_list, pm_kw_list))\n\n\n#2\n#What do we know about virus genetics, origin, and evolution?\nquestion_list = []\nkw_list = []\npm_kw_list = []\nquestion_list.append(\"Can animals transmit 2019-nCoV\")\nkw_list.append(\"2019-nCoV, SARS-CoV-2, COVID-19, coronavirus, novel coronavirus, animals, zoonotic, farm, spillover, animal to human, human to animal, reservoir\")\npm_kw_list.append(\"2019-nCoV, spillover, reservoir\")\n\nquestion_list.append(\"What animal did 2019-nCoV come from\")\nkw_list.append(\"2019-nCoV, SARS-CoV-2, COVID-19, coronavirus, novel coronavirus, animals, zoonotic, farm, spillover, animal to human, bats, snakes, exotic animals\")\npm_kw_list.append(\"2019-nCoV, zoonotic\")\n\nquestion_list.append(\"What real-time genomic tracking tools exist\")\nkw_list.append(\"2019-nCoV, SARS-CoV-2, COVID-19, coronavirus, novel coronavirus, real-time, gene, typing, tracking, software, reporting\")\npm_kw_list.append('\"2019-nCoV, real-time, genomic, tracking')\n\nquestion_list.append(\"What regional genetic variations (mutations) exist\")\nkw_list.append(\"2019-nCoV, SARS-CoV-2, COVID-19, coronavirus, novel coronavirus, geography, region, genome, mutations\")\npm_kw_list.append(\"2019-nCoV, geneome, region\")\n\nquestion_list.append(\"What effors are being done in asia to prevent further outbreaks\")\nkw_list.append(\"2019-nCoV, SARS-CoV-2, COVID-19, surveylance, wildlife, livestock, monitoring, asia, prevent, prevention, outbreaks\")\npm_kw_list.append(\"2019-nCoV, surveylance\")\n\nquestion_list.append(\"What is the weighted prevalence of sars-cov-2 or covid-19 in general population\")\nkw_list.append(\"2019-nCoV, sars-cov-2, covid-19\")\npm_kw_list.append(\"antibody, prevalence, COVID-19\")\n\ntopic_area['What do we know about virus genetics, origin, and evolution?'] = list(zip(question_list,kw_list, pm_kw_list))\n\n#3\n#What do we know about vaccines and therapeutics?\nquestion_list = []\nkw_list = []\npm_kw_list = []\n\nquestion_list.append(\"Does the presense neutralizing antibodies correlate to disease protection\")\nkw_list.append(\"2019-nCoV, sars-cov-2, covid-19\")\npm_kw_list.append(\"sars-cov-2, neutralizing antibody\")\n\nquestion_list.append(\"What drugs or therapies are being investigated\")\nkw_list.append(\"2019-nCoV,  COVID-19, coronavirus, novel coronavirus, drug, antiviral, testing, clinical trial, study\")\npm_kw_list.append(\"2019-nCoV,  drug, therapy\")\n\nquestion_list.append(\"What clinical trials for hydroxychloroquine have been completed\")\nkw_list.append(\"2019-nCoV, SARS-CoV-2, COVID-19, hydroxychloroquine, clinical trial\")\npm_kw_list.append(\"2019-nCoV, hydroxychloroquine\")\n\nquestion_list.append(\"What antiviral drug clinical trials have been completed\")\nkw_list.append(\"2019-nCoV, SARS-CoV-2, COVID-19, clinical trial\")\npm_kw_list.append(\"2019-nCoV, antiviral\")\n\nquestion_list.append(\"Are anti-inflammatory drugs recommended\")\nkw_list.append(\"2019-nCoV, SARS-CoV-2, COVID-19, coronavirus, novel coronavirus, ibuprofen, advil, NSAID, anti-inflamatory, treatment\")\npm_kw_list.append('2019-nCoV, ibuprofen, NSAID')\n\ntopic_area['What do we know about vaccines and therapeutics?'] = list(zip(question_list,kw_list, pm_kw_list))\n\n\n#4\n#What do we know about non-pharmaceutical interventions?\nquestion_list = []\nkw_list = []\npm_kw_list = []\nquestion_list.append(\"Which non-pharmaceutical interventions limit tramsission\")\nkw_list.append(\"2019-nCoV, SARS-CoV-2, COVID-19, non-pharmaceutical interventions, npi\")\npm_kw_list.append(\"2019-nCoV, npi\")\n\nquestion_list.append(\"What are most important barriers to compliance\")\nkw_list.append(\"2019-nCoV, SARS-CoV-2, COVID-19, non-pharmaceutical interventions, npi\")\npm_kw_list.append('2019-nCoV, npi, barrier to compliance')\n\ntopic_area['What do we know about non-pharmaceutical interventions?'] = list(zip(question_list,kw_list, pm_kw_list))\n\n#5\n#What has been published about medical care?\nquestion_list = []\nkw_list = []\npm_kw_list = []\nquestion_list.append(\"How does extracorporeal membrane oxygenation affect 2019-nCoV patients\")\nkw_list.append(\"2019-nCoV, SARS-CoV-2, COVID-19, extracorporeal membrane oxygenation, ecmo\")\npm_kw_list.append('2019-nCoV, ecmo')\n\nquestion_list.append(\"What telemedicine and cybercare methods are most effective\")\nkw_list.append(\"2019-nCoV, SARS-CoV-2, COVID-19, coronavirus, novel coronavirus, telemedicine, 5G, cell phone, cyber, cybercare, information technolog, remote, over the phone, internet, web\")\npm_kw_list.append('2019-nCoV, telemedicine, cybercare')\n\nquestion_list.append(\"How is artificial intelligence being used in real time health delivery\")\nkw_list.append(\"2019-nCoV, ai, real-time\")\npm_kw_list.append('')\n\nquestion_list.append(\"What adjunctive or supportive methods can help patients\")\nkw_list.append(\"2019-nCoV, SARS-CoV-2, COVID-19, adjunctive, supportive\")\npm_kw_list.append('')\n\ntopic_area['What has been published about medical care?'] = list(zip(question_list,kw_list, pm_kw_list))\n\n#6\n#What do we know about diagnostics and surveillance?\nquestion_list = []\nkw_list = []\npm_kw_list = []\nquestion_list.append(\"What diagnostic tests (tools) exist or are being developed to detect 2019-nCoV\")\nkw_list.append(\"2019-nCoV, SARS-CoV-2, COVID-19, coronavirus, novel coronavirus, diagnosis, tools, detetion\")\npm_kw_list.append('2019-nCoV, diagnostic, tools, detetion')\n\nquestion_list.append(\"What is being done to increase testing capacity or throughput\")\nkw_list.append(\"2019-nCoV, sars-cov-2, covid-19, diagnostic, testing, throughput\")\npm_kw_list.append(\"2019-nCoV, testing capacity OR throughput\")\n\nquestion_list.append(\"What point of care tests are exist or are being developed\")\nkw_list.append(\"2019-nCoV, sars-cov-2, covid-19\")\npm_kw_list.append(\"2019-nCoV, point-of-care\")\n\nquestion_list.append(\"What is the minimum viral load for detection\")\nkw_list.append(\"2019-nCoV, sars-cov-2, covid-19\")\npm_kw_list.append(\"2019-nCoV, viral load\")\n\nquestion_list.append(\"What markers are used to detect or track COVID-19\")\nkw_list.append(\"2019-nCoV, sars-cov-2, covid-19\")\npm_kw_list.append(\"2019-nCoV, markers\")\n\ntopic_area['What do we know about diagnostics and surveillance?'] = list(zip(question_list,kw_list, pm_kw_list))\n\n\n\n#7\n#What has been published about information sharing and inter-sectoral collaboration?\nquestion_list = []\nkw_list = []\npm_kw_list = []\nquestion_list.append('What collaborations are happening within the research community')\nkw_list.append('inter-sectorial, international, collaboration, global, coronavirus, novel coronavirus, sharing')\npm_kw_list.append('2019-nCoV, collaboration, sharing')\n\nquestion_list.append(\"What communication strategies are effective for high risk populations\")\nkw_list.append(\"2019-nCoV, sars-cov-2, covid-19\")\npm_kw_list.append(\"2019-nCoV, communication, at risk\")\n\ntopic_area['What has been published about information sharing and inter-sectoral collaboration?'] = list(zip(question_list,kw_list, pm_kw_list))\n\n\n#8\n#What has been published about ethical and social science considerations?\nquestion_list = []\nkw_list = []\npm_kw_list = []\n\nquestion_list.append(\"What are the major ethical issues related pandemic outbreaks\")\nkw_list.append(\"ehtics, pandemic\")\npm_kw_list.append(\"ethics, pandemic\")\n\nquestion_list.append(\"How do pandemics affect the physical and\/or psychological health of doctors and nurses\")\nkw_list.append(\"2019-nCoV, sars-cov-2, covid-19, caregivers, health care workers\")\npm_kw_list.append(\"2019-nCoV, physical OR psychological\")\n\nquestion_list.append(\"What strategies can help doctors and nurses cope with stress in a pandemic\")\nkw_list.append(\"2019-nCoV, sars-cov-2, covid-19, caregivers, health care workers\")\npm_kw_list.append(\"2019-nCoV, physical OR psychological\")\n\nquestion_list.append(\"What factors contribute to rumors and misinformation\")\nkw_list.append(\"2019-nCoV, sars-cov-2, covid-19, social media\")\npm_kw_list.append(\"2019-nCoV, misinformation OR social media\")\n\ntopic_area['What has been published about ethical and social science considerations?'] = list(zip(question_list,kw_list, pm_kw_list))\n\n\n\n#-1\n# Other interesting Questions\nquestion_list = []\nkw_list = []\npm_kw_list = []\nquestion_list.append(\"What is the immune system response to 2019-nCoV\")\nkw_list.append(\"2019-nCoV, SARS-CoV-2, COVID-19, coronavirus, novel coronavirus, immune, immune system, response, immunity, antibodies\")\npm_kw_list.append('2019-nCoV, immune system, immunity, antibodie')\n\nquestion_list.append(\"Can personal protective equipment prevent the transmission of 2019-nCoV\")\nkw_list.append(\"2019-nCoV, SARS-CoV-2, COVID-19, coronavirus, novel coronavirus, ppe, masks, gloves, face shields, gown, eye protection\")\npm_kw_list.append('2019-nCoV, ppe')\n\nquestion_list.append(\"Can 2019-nCoV infect patients a second time\")\nkw_list.append(\"2019-nCoV, SARS-CoV-2, COVID-19, coronavirus, novel coronavirus, reinfected, multiple infections, second time, permenant immunity\")\npm_kw_list.append('2019-nCoV, reinfected')\n\ntopic_area['Other interesting Questions'] = list(zip(question_list,kw_list, pm_kw_list))\n\n\ndef runAllQuestionsByTopic(topic_dict, topic_name):\n    for q,kw, pm_kw in topic_dict[topic_name]:\n        if q:\n            searchDatabase(q, kw, pysearch, luceneDir, pm_kw=pm_kw, minDate=minDate)","b8f65707":"question_list = []\nkw_list = []\npm_kw_list=[]\n\nquestion_list.append(\"What is the weighted prevalence of sars-cov-2 or covid-19 in general population\")\nkw_list.append(\"2019-nCoV, sars-cov-2, covid-19\")\npm_kw_list.append(\"antibody, prevalence, COVID-19\")\n\n\nsearchDatabase(question_list[0], kw_list[0], pysearch, luceneDir, pm_kw=pm_kw_list[0], minDate=minDate, k=20)","fddc675d":"runAllQuestionsByTopic(topic_area, 'What is known about transmission, incubation, and environmental stability?')","ce37d396":"runAllQuestionsByTopic(topic_area, 'What do we know about COVID-19 risk factors?')","e738e042":"runAllQuestionsByTopic(topic_area, 'What do we know about virus genetics, origin, and evolution?')","7c99eb70":"runAllQuestionsByTopic(topic_area, 'What do we know about vaccines and therapeutics?')","81c418ae":"runAllQuestionsByTopic(topic_area, 'What do we know about non-pharmaceutical interventions?')","374782c8":"runAllQuestionsByTopic(topic_area, 'What has been published about medical care?')","5edd0e45":"runAllQuestionsByTopic(topic_area, 'What do we know about diagnostics and surveillance?')","62d90042":"runAllQuestionsByTopic(topic_area, 'What has been published about information sharing and inter-sectoral collaboration?')","6d9d525e":"runAllQuestionsByTopic(topic_area, 'What has been published about ethical and social science considerations?')","0b820a83":"Now we can write a function to do an Open Domain QA on all the abstracts","04f38071":"now lets get the transformer models","436dde45":"# What do we know about COVID-19 risk factors?","8e24bd79":"# Now we should be set to make our first query of the CORD-19 database.  Lets look at the top 10 results based on our query.\n\nlest break our query up into two parts:\n\n1) a natural language question\n\n2) a set of keywords that can help drive the answerini search enginge towards the most interesting results for the question we want to ask\n\nThis is beneficial becuase the answerini portion of the search is not really contextual and cant discipher meaning so the keywords will help drive the search.  This could be refined eventually by using a BERT model to create an embedding from the question being asked.  For right now, this is good enough.","73a316d7":"Now lets get Pyserini (python wrapped Anserini) setup","417ee3cc":"# What has been published about information sharing and inter-sectoral collaboration?","062308b2":"# What do we know about vaccines and therapeutics?","58d24ea6":"# What is known about transmission, incubation, and environmental stability?","dd84b88a":"Lets search pubmed too to fill in the gaps and get the latest papers that may not be in the lucene database","e6266d08":"now lets get metapub to be able to find pdfs if available","ab50e6e1":"# Use this block to refine specific questions before adding them to the list of all questions","1ec82c55":"# What do we know about diagnostics and surveillance?","4e33b5fc":"See this [notebook](https:\/\/www.kaggle.com\/dirktheeng\/universal-sentence-encoder-for-nlp-matching) for a stripped own example.","23bf6f54":"Lets try this with the same question and kw to see if it produces the same results we just got","bcfd32da":"Now lets get biopython set up so we can go search pubmed if we want to","762bbef4":"Great that worked as expected.  Now lets try some new questions","8b0b383f":"Now we need the lucene searchable CORD-19 database","016a38a3":"# What has been published about medical care?","e36525d8":"# What do we know about virus genetics, origin, and evolution?","28c87400":"\n![200309-D-HN545-003.JPG](attachment:200309-D-HN545-003.JPG)\n\n# Project Description\nThe objective of this project is to integrate the state of the art Natural Language Processing (NLP) techniques to create a method to ask a database a question and have it return sensible results that answer that question.  In addition, we can apply modern document analysis methods to exctract tabular, figure, and numeric data when possible.  Eventually, this could be built into a scientific search system that can reduce a body of knowledge to a small number of records and then extract data into machine readable formats to support further modeling and data analysis.\n\n# Background\n\nI am interested in semantic NLP for mining the CORD-19 data set.  My initial literature review lead me to Open Domain Question and Answering and semantic search engines as a means to quickly reduce scientific information to just a few records that would be of most interest.  I found a fairly simple [mehtod](https:\/\/arxiv.org\/abs\/1902.01718) that had very good results.  Full credit to Yang and colleagues at University of Waterloo.\n\n## Semantic search with BERT-SQuAD\n![image.png](attachment:image.png)\nFigure 1: BERT-Serini Achitecture proposed by [Yang et al](https:\/\/arxiv.org\/abs\/1902.01718)\n\nIn this work, Yang et al combined Anserini and Bert-SQuAD to produce a single \"best\" answer.  [Anserini](https:\/\/github.com\/castorini\/anserini) is a search engine which is built on top of Apache Lucene which can go and search literature and return ranked records.  In their work, they used an NL question to drive Anserini and return a ranked number of segments from all of wikipidia.  Following this, they ran the same question over all the segments using BERT-SQuAD to produce set of spans.  The spans were then ranked by a linear combination of the span and segment score and the top was then taken as the answer.  I believe this is a great way to go because everything is analyzed by semantics of the question being posed and the number of records that neat to be searched by BERT-SQuAD is minimal which is good becuase that is quite intense.\n\nThis intrigued me so I went to go see what I could do with this and when I went searching for Anserini, I found that they had already made a [CORD-19 Lucene data base](https:\/\/github.com\/castorini\/anserini\/blob\/master\/docs\/experiments-covid.md).  This was convenient and openly available.  I also found a starter example using BERT on [colab](https:\/\/colab.research.google.com\/drive\/1L_yWXM4tOhZsHpMDNIIux-hfp1-pW3RL#forceEdit=true&sandboxMode=true&scrollTo=KVXWA6WS0aqJ).  Essentially, I thought it would be valuable to impliment the Yang et al methodology but use the CORD-19 data base instead of wikipedia.  I had enough to get going using Anserini and figured out how to switch out BERT for a BERT-SQaAD method like they did in Yang et al.\n\n## Semantic Ranking with Universal Sentence Encocer\n\nI found a good method to rank retrieved results that is probably better for this application than Yang's linear combination method.  I am applying Google's Universal Sentence Encoder to produce a similarity matrix that mathmatically represents the semantic similarity of all the answers to that of the question.  I use that information to rank the order of the returned results.  It is not too slow to be useful and does not take up much more space.  I believe this is useful because the best answers are usually semantically simillar to the question being asked.\n\n![semanticText%20%282%29.jpg](attachment:semanticText%20%282%29.jpg)\nFigure 2: Example Semantic Ranking\n  * M0 = \"How does weather, heat, and humidity affect the tramsmission of 2019-nCoV\"\n  * M1 = \"conclusions : a wide range of continuous warm and dry weather is conducive to the survival of 2019-ncov .\"\n  * M2 = \"formalin fixation and heating samples to 56oc, as used in routine tissue processing, were found to inactivate several coronaviruses and it is believed that 2019-ncov would be similarly affected .\"\n  * M3 = \"objective to investigate the impact of temperature and absolute humidity on the coronavirus disease 2019 (covid-19) outbreak .\"\n  * M4 = \"taken chinese cities as a discovery dataset, it was suggested that temperature, wind speed, and relative humidity combined together could best predict the epidemic situation .\"\n  * M5 = \"in the next years the engagement of the health sector would be working to develop prevention and adaptation programs in order to reduce the costs and burden of climate change.\"\n\n## Auto Summarization with BART\nI thought it would be good to generate a summarization based on the top results.  I found that hugging face transformers just released a [pretrained \"BART\" model](https:\/\/sshleifer.github.io\/blog_v2\/jupyter\/2020\/03\/12\/bart.html) for \"Abstractive Summarization\". I found this works fairly well when the evidence deals directly with the question.  If the generated summary does not appear to answer the question, it is an indication that the body of evidence either does not directly deal with the question (ie the evidence is there but mentioned in passing) or that the evidence is not clear.\n\n## Full PDF Links\nIn order to do further data analysis like table\/figure extraction, we need full text pdfs. I found a [tool](https:\/\/pypi.org\/project\/metapub\/) that can follow a pubmed link and find a direct link to a publically available PDF.  I integrated this tool into the search.  With the ability to pull full PDF's, we can apply common table and figure extraction methods.\n\n## Understanding How to Get Best Results\nIt is important to remember the sequence of events to generate the most relevant results.  First, answerini is fed the question and keywords.  BERT-SQuAD is then used to find passages that are relevant.  The Universal Sentance Encoder similarity search is then used to rank the passeges found by BERT.  The BART model is then used to abstractively summarize the top results.  Because so much is done by symantic comparison, small changes in the question can rerank results and affect what BART sees for the sumary.\n\nIn general, I find that this proceedure is best:\n1) start with a general question like \"what is the effect of smoking\" and put synonyms for the disease and other cofactors in the keywords to drive answerini to relevant results\n2) review the evidence and see if there are more keywords to add to get the search results better\n3) start working on the question a little bit by trying to make it more or less specific, for instance: change the question to \"what is the effect of smoking on patients\" or \"what is the effect of smoking on 2019-nCov patients\".  This will help drive the ranking and thus the summary that is generated.\n\nI find that 2-3 searches is all it takes to generate highly specific results.  \n\n## Future Plans Past Kaggle Competition\nI think this is a tool that would be generally useful for all of scientific R&D and would like to build this into a publically available web based tool that would be free to use.\n\n## Acknowledgements\nI think [Jimmy Lin](https:\/\/cs.uwaterloo.ca\/~jimmylin\/), Edwin Zhang, and Nikhil Gupta at the University of Waterloo deserve major credit here.  Jimmy is on the arXiv paper and also on the anserini project.  Props to him for making this possible.\n\n\n## Notes About any Prises\nI am not interested in any prises and ask that anything gained be directly donated to further research and support.  If anything, Jimmy probably deserves more credit for this than me.  I just got his method set up and working here and started askign questions.  I hope to someday meet Jimmy and thank him\n\n# Updates and Notes\nversion 59 - prevalence of virus in general population\n\nversion 58 - cancelled\n\nversion 57 - medrxiv interface, neutralizing antibodies and protection\n\nversion 56 - fixed bug in pubmed search, handle id error\n\nversion 55 - update with latest lucene db\n\nverstion 54 - more questions.\n\nversion 53 - filling out questions and making features turn on or off for quicker run time.\n\nversion 52 - Built interface that allows for analysis of abstracts that result in token lengths of up to 1024 tokens.  Can easily be expanded to longer results, but for abstract analysis this is all that is needed (I think).  It detects if the tokenizer gives more than 512 tokens and then splits the document into two pieces.  In most cases, the total token length was under 800.  This gives plenty of overlap to pick out the best segment.  Also corrected a couple bugs that should make it possible to run on GPU.\n\nversion 51 - refining questions to get more relevant results.\n\nversion 50 - upped the num_beams to 10 for the BART model and allowed the summary to be longer.  It tends to produce better results given these settings.  However, increasing the num_beams parameter makes it run slower.  Updated to latest database released on 4\/3\/2020\n\nversion 49 - code refactoring to make it easier to manage.  Starting to tackle the BERT 512 token limit.  Tagged results that are too long.  Need to rewrite the function so that long passages are split up with some overlap and then rejoined in a sensible manner.  The \"Too Long\" heading in the tables indicates if the passage was too long for a single run through BERT.\n\nversion 48 - cancelled\n\nversion 47 - fixed bug around find it\n\nversion 46 - found some tools in [metapub](https:\/\/pypi.org\/project\/metapub\/) that can go get a direct link to a publically available pdf.  This is slow but it allows us to do further document analysis if we can get a full pdf.\n\nVersion 45 - canceled\n\nVersion 44 - Switched to \"Abstractive Summarizer\" based on BART (yes thats spelled correctly) in transformers.  Does a better job of generating meaningful summaries though it is slower.  Redid heat map so that it is easier to see and read the text.\n\nVersion 43 - Implemented an \"Extractive Summarizer\" based on BERT to automatically summarize the body of evicence.  It does well when there is a decent body of evidence to pull from.  If there isn't... well not so much.\n\nVersion 42 - cancelled\n\nVersion 41 - Implemented a new ranking methodology based on Google's Universal Sentance Encoder and a similarity matrix.  See this [notebook](https:\/\/www.kaggle.com\/dirktheeng\/universal-sentence-encoder-for-nlp-matching) for a foundational example.  Answers are now ranked based on semantic similarity to the question asked.\n\nVersion 40 - Switched terms from context to semantic.  I'm not an expert in this field, but I beleive this is a better way to talk about the work.\n\nVersion 39 - Got segment highlighting turned on in the reported table so we can see exactly what bert-squad focused on in the text and possibly make better decisions about refinement.\n\nVersion 38 - updated intro to provide more context to users about the project, how it works, and why it may be a good thing to use.\n\nVersion 37 - Fixed an error in the confidence calculation that was causing a NAN and thus the abstracts to be excluded from analysis.  Changed the calculation method for ranking to switch to probability within the body of retrieved results.  I am not sure that this is very meaningful yet as the highest score results also tend to contain the fewest tokens coming out of BERT.  That is not always meaningful and I am not sure that this means that the answer is more relevant than any other.  I would like to build a second model on top of all the search results to rerank the returned results somehow.  Perhaps a tf-idf method would work?  I also rewrote the sentance recontstruction method from tokens.\n\nVersion 35-36 - Funny story.  This morning before work I went to start on completing a couple tasks.  I got a phone call and left my computer within reach of my 2 year old son.  He managed to insert a bunch of text in the code and check it in as well as change my screen settings in a way that I had never seen before (Took me like 20 minutes to fiture out how to fix it).  He also made a bunch of appointments on my calendar and was somehow browsing wikipedia for pictures of the earth from space.  I'm not making this up.  I didn't even see that the code had issues until i got off work.  I couldn't help but laugh.  Anyhow, hopefully this works now.\n\nVersion 33 - reached a decent level of stability.  Had an email discussion with Jimmy Lin.  Found out that AI2 is updatding the COVID-19 database weekly.  So far, UW has been updating Anserini with the [new data](https:\/\/github.com\/castorini\/anserini\/blob\/master\/docs\/experiments-covid.md) as it comes out.  In order to make this more responsive to the needs of researchers, I integrated a pubmed search engine that goes and grabbs the newest data from the last lucine update and merges it into the search automatically.  This capability is built on top of [Biopython](https:\/\/biopython.org\/).  In order to use it, you need to define a set of pubmed keywords.  Pubmed searches don't seem to like a large number of keywords.  It seems to narrow the reults rather than widen them (seems the opposite of Anserini).  See the competition question setup for examples.\n\n# Tasks to be completed\n\n~~1) build a better sentance reconstruction function to put text back together from BERT-SQuAD~~\n\n2) build an interfact to Google Scholar using [scholarly](https:\/\/pypi.org\/project\/scholarly\/)\n\n~~3) build a better answer ranking system than relying on confidence from answers from individual text~~\n\n~~4) build a method better handle cases that exceed 512 tokens (right now they are cut off at 512)~~\n\n~~5) build a summarization system based on distilBert see this [notebook](https:\/\/www.kaggle.com\/latong\/extractive-text-summarization-ner-exploration)~~\n\n~~6) make table link text be the title of the paper~~\n\n7) use sciBert\/biobert for NER to extract keywords to drive supmemental searches","57c29a37":"First we need to go get openJDK 11 set up","c658e7eb":"Lets put this together in a more eye pleasing way\n\nI noticed that the more confident the BERT-SQuAD is, the less text it seems to highlight.  To make sure that we get the full human understandable concept highlighted, I will set it to highlight the sentance that BERT-SQuAD identified.","72413c5f":"# What has been published about ethical and social science considerations?","d8d0ba19":"# Define All The Questions for the Competition","cfd329ef":"Lets try doing a simple BERT-SQuAD QA model first and see how it does\n\nOriginally, found a good example of runnign a BertSQuAD model by Raj Kamil at:\nhttps:\/\/github.com\/kamalkraj\/BERT-SQuAD\n\nHowever, the link to the completly pretrained model broke on 3\/28.  Unfortunatly, I did not think to download it and save it myself.  Thus I rebuilt this book based on the completely pretrained model in transformers.  I don't have as good of a text formatting as before, but I think this model works and has the ability (i think) to return an \"i don't know, or this isn't relevant\" which the other model didn't.  Also, I seem to get an invalid number crash with this new model.  Thats not good as the abstract is not searched.  I will have to look into training a model myself at some point.\n\nHere is a great tutorial and a helpful resource to me as I got the transformers code up and running:\nhttps:\/\/mccormickml.com\/2020\/03\/10\/question-answering-with-a-fine-tuned-BERT\/\n\nYou will find many similarities between his example code and the function I built to extract abstract information.","2de901ff":"# What do we know about non-pharmaceutical interventions?","f867fa28":"Now Lets get the Universal Sentence Encoder","4d49b21b":"# Build a semantic similarity search capability to rank answers in terms of how closely they line up to the meaning of the NL question","6f89db68":"Set up some parameters in the kernel"}}