{"cell_type":{"a73063dd":"code","de6007ba":"code","f14d679c":"code","21d2d2e4":"code","120f699d":"code","cb7b8d06":"code","37e7e178":"code","227e3427":"code","700123f7":"code","3336153c":"code","5988930e":"code","ee7fcc9c":"code","45b2b358":"code","d2b2bacd":"code","6a65d83d":"code","22e37c42":"code","85d2e854":"code","29c61054":"code","9294bdf3":"code","6430880c":"code","a17326cf":"code","9ffd10ed":"code","aea49ee6":"code","a9a398ea":"code","4cf43379":"code","01d99f1f":"code","dd7bc253":"code","e9d68edb":"code","35e171fa":"code","014b8723":"code","fb21131b":"code","1e7c3fdf":"code","08d3fbc3":"code","85e2d131":"code","8f7829b8":"code","47d33479":"code","d0c4ca9b":"code","36d52aa1":"markdown","741f70f4":"markdown","777458ac":"markdown","a2f99f22":"markdown","5a2eb468":"markdown","0cfc311b":"markdown","128281e9":"markdown","72f966dd":"markdown","0f3810a7":"markdown","e9704609":"markdown","fde6b5b9":"markdown","76ea91b1":"markdown","46f02f2c":"markdown","96cdb8fa":"markdown","e6d305b8":"markdown","16976270":"markdown","7e3298a4":"markdown","66152e39":"markdown","1f0beece":"markdown","ec51ffe0":"markdown","af692718":"markdown"},"source":{"a73063dd":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","de6007ba":"# Importing what we need\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport time\nimport warnings\nfrom termcolor import cprint\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score","f14d679c":"#Read the data\nX = pd.read_csv(\"..\/input\/titanic\/train.csv\")\nX_test_data = pd.read_csv(\"..\/input\/titanic\/test.csv\")\n\n#Setting indexes\nX.set_index('PassengerId', inplace=True)  #, \nX_test_data.set_index('PassengerId', inplace=True)  #, \n\n# Read the extented data\nX_extented = pd.read_csv(\"..\/input\/titanic-extented-data\/train_extented.csv\")\nX_test_data_extented = pd.read_csv(\"..\/input\/titanic-extented-data\/test _extented.csv\")\n\n# Setting indexes in the extented data\nX_extented.set_index('PassengerId', inplace=True)  #, \nX_test_data_extented.set_index('PassengerId', inplace=True)\n\n# Here i add the Lifeboat column from X_extented to X\nX = pd.concat([X, X_extented[\"Lifeboat\"].rename(\"Lifeboat\")],  axis=1)    \n\n# Here i add the Lifeboat column from X_test_data_extented to X_test_data\nX_test_data = pd.concat([X_test_data, X_test_data_extented[\"Lifeboat\"].rename(\"Lifeboat\")], axis=1)\n\n#Inspect the train data\ncprint(\"First 5 rows of X:\",\"green\")\nprint(X.head())\ncprint(\"Basic information about X:\", \"green\")\nprint(X.info())\ncprint(\"Shape of the X is: \", \"green\")\nprint(X.shape)\n\n#Inspect the test data\ncprint(\"First 5 rows of X_test_data :\",\"green\")\nprint(X_test_data.head())\ncprint(\"Basic information about X_test_data :\", \"green\")\nprint(X_test_data.info())\ncprint(\"Shape of the X_test_data is: \", \"green\")\nprint(X_test_data.shape)\n\n# Remove rows with missing target, separate target from predictors\nX.dropna(axis=0, subset=['Survived'], inplace=True)\ny = X.Survived  ","21d2d2e4":"# get the number of missing data points per column in train data\ncprint(\"Missing data in train data: \", \"green\")\nX_mis_val_count = X.isnull().sum()\nprint(X_mis_val_count)\n\n# total missing values of train data (X)\nX_total_cells = np.product(X.shape)\nX_total_missing = X_mis_val_count.sum()\n\n# percent of data that is missing in train data\nX_percent_missing = (X_total_missing\/X_total_cells) * 100\nprint(\"Percent of data that is missing in train data \" + str(X_percent_missing))\n\n\n# get the number of missing data points per column in test data\ncprint(\"Missing data in test data: \", \"green\")\nX_test_data_mis_val_count = X_test_data.isnull().sum()\nprint(X_test_data_mis_val_count)\n\n# total missing values of test data (X_test_data)\nX_test_data_total_cells = np.product(X_test_data.shape)\nX_test_data_total_missing = X_test_data_mis_val_count.sum()\n\n# percent of data that is missing in test data\nX_test_data_percent_missing = (X_test_data_total_missing\/X_test_data_total_cells) * 100\nprint(\"Percent of data that is missing in test data: \" + str(X_test_data_percent_missing))","120f699d":"# Here, i take a look in every categorical column's cardinality (i.e. the amount of unique value of feature)\ncprint(\"Cardinality of categorical columns:\",\"green\")\nfor c in X.columns:\n    if X[c].dtype == \"object\":\n        print(c, X[c].nunique())\n\n# It appears that some have very high cardinality, so i will make a list of them and examine them later\nhigh_cardinality_cols = [cname for cname in X.columns if X[cname].nunique() > 10 and \n                        X[cname].dtype == \"object\"]\n\ncprint(\" Columns with high cardinality are:\", \"green\")\nprint(high_cardinality_cols)","cb7b8d06":"# The Cabin feature has huge cardinality, and the vast majority of its data points is missing. There are 2 scenarios. Either the data is lost so i better drop this column, or missing data points belong to passengers that did not had a cabin at all\n# In case the latter is True, then Cabin is an important feature and i will use it\n\n#Because of its high cardinality, i will construct an new feature \"HasCabin\". If the passenger had one then its value is 1, else it is 0\nfor i in X[\"Cabin\"]:\n    X['HasCabin'] = (X['Cabin'].notnull().astype('int'))\n\nfor i in X_test_data[\"Cabin\"]:\n    X_test_data['HasCabin'] = (X_test_data['Cabin'].notnull().astype('int'))    \n\n# Now i can drop the original \"Cabin\" column as it is useless\nX.drop([\"Cabin\"], axis=1, inplace=True)\nX_test_data.drop([\"Cabin\"], axis=1, inplace=True)","37e7e178":"# Handling missing data of the \"Age\" feature in both X and X_test_data\n# Here, i replace NaN values in both X and X_test_data with the Column's median\nX[\"Age\"].fillna(X.groupby([\"Pclass\", \"Sex\"])[\"Age\"].transform(\"mean\"), inplace=True)\nX_test_data[\"Age\"].fillna(X_test_data.groupby([\"Pclass\", \"Sex\"])[\"Age\"].transform(\"mean\"), inplace=True)","227e3427":"# Handling of missing data of the \"Embarked\" feature in both X and X_test_data (the latter does not have any missing data)\n\n# These observations contain null values in the Embarked feature, so i will Label Encode them manually with the map() function\nemb_map = {'S':0,'C':1,'Q':2}\nX.Embarked = X.Embarked.map(emb_map)\nX_test_data.Embarked = X_test_data.Embarked.map(emb_map).astype(int)\n\n# I will fill the two NaN's with the column's median\nprint(\"mean of embarked column:\",X.Embarked.median())\nX.Embarked.fillna(X.Embarked.median(), inplace = True)\nX.Embarked = X.Embarked.astype(int)","700123f7":"# Handling of missing data of the \"Fare\" feature in both X and X_test_data (the former does not have any missing data)\nX_test_data[\"Fare\"].fillna(X_test_data.groupby(['Pclass', 'Sex'])['Fare'].transform(\"median\"), inplace=True)","3336153c":"# Now i check \nprint(X.info())\nprint(X_test_data.info())","5988930e":"# Some basic Exploratory data analysis\ncorr = X.corr()\nsns.heatmap(corr, annot = True).set_title(\"Correlation Map\")","ee7fcc9c":"#Age analysis\n\nfig = plt.figure(figsize =(8, 5))\nplt.boxplot(X[\"Age\"])\nplt.title('Age')\nplt.ylabel('Years')\nplt.show()","45b2b358":"# These are the bins\n# I may toggle them later to see how my model accuracy will react\nbins = [0, 10, 20, 30, 40, 50, 60, 70, 80, np.inf]\n\n#Binning the X\nX[\"AgeBins\"] = pd.cut(X[\"Age\"], bins)\n\n#Binning the X_test_data\nX_test_data[\"AgeBins\"] = pd.cut(X_test_data[\"Age\"], bins)","d2b2bacd":"label_encoder = LabelEncoder()\n\n# Encode labels in column AgeBins in X \nX['AgeBins'] = label_encoder.fit_transform(X[\"AgeBins\"])\n\n# Encode labels in column AgeBins in X_test_data\nX_test_data[\"AgeBins\"] = label_encoder.fit_transform(X_test_data[\"AgeBins\"])","6a65d83d":"fig, axs = plt.subplots(figsize=(12, 5))\nsns.countplot(x=X[\"AgeBins\"], hue='Survived', data=X).set_title(\"Age Bins\",fontdict= { 'fontsize': 20, 'fontweight':'bold'});","22e37c42":"#Fare analysis\n\nfig = plt.figure(figsize =(4, 6))\nplt.boxplot(X[\"Fare\"])\nplt.title('Fare')\nplt.ylabel('Currency')\nplt.show()","85d2e854":"# It is visible that there is a considerable number of outliers in the \"Fare\" feature","29c61054":"# We will bin this feature, but i will toggle the bin's boundaries later to see how our models will react\nbins = [0,  100,  200,  300,  400,  500, np.inf]\n\n#Binning the X\nX[\"FareBins\"] = pd.cut(X[\"Fare\"], bins)\n#Binning the X_test_data\nX_test_data[\"FareBins\"] = pd.cut(X_test_data[\"Fare\"], bins)","9294bdf3":"fig, axs = plt.subplots(figsize=(12, 5))\nsns.countplot(x=X[\"FareBins\"], hue='Survived', data=X).set_title(\"Fare Bins\",fontdict= { 'fontsize': 20, 'fontweight':'bold'});","6430880c":"# Encode labels in column FareBins in X \nX['FareBins'] = label_encoder.fit_transform(X[\"FareBins\"])\n\n# Encode labels in column AgeBins in X_test_data\nX_test_data[\"FareBins\"] = label_encoder.fit_transform(X_test_data[\"FareBins\"])","a17326cf":"# Sex analysis\n\ngender_mapping = {'female':0,'male':1}\n# In X \nX.Sex = X.Sex.map(gender_mapping)\n# In X_test_data\nX_test_data.Sex = X_test_data.Sex.map(gender_mapping)","9ffd10ed":"fig, axs = plt.subplots(figsize=(12, 5))\nsns.countplot(x=X[\"Sex\"], hue='Survived', data=X).set_title(\"Sex\",fontdict= { 'fontsize': 20, 'fontweight':'bold'});","aea49ee6":"fig, axs = plt.subplots(figsize=(12, 5))\nsns.countplot(x=X[\"Embarked\"], hue='Survived', data=X).set_title(\"Embarkation port\",fontdict= { 'fontsize': 20, 'fontweight':'bold'});","a9a398ea":"# Pclass analysis\nfig, axs = plt.subplots(figsize=(12, 5))\nsns.countplot(x=X[\"Pclass\"], hue='Survived', data=X).set_title(\"Pclass (ticket class)\",fontdict= { 'fontsize': 20, 'fontweight':'bold'});","4cf43379":"# Feature extraction\n\n# Name column\n# Many people had titles in their names, which might be interesting indicators about their social and\/or economical status, thus this could influence their survival chances\n\n# I extract alphanumericals that are exactly before the fullstop for every name, and a make a new column with them for every passenger for both X and X_test_data\nX['Title'] = X.Name.str.extract('([A-Za-z]+)\\.', expand = False)\nX_test_data['Title'] = X_test_data.Name.str.extract('([A-Za-z]+)\\.', expand = False)\n\nX.Title.value_counts()\n# As we can see, Mr, Miss, Mrs, Master\n\n# All rare titles will be aliased as \"Rare\" in our new column to prevent overfitting\nrare_titles = ['Rev','Dr','Major','Col','Capt','Jonkheer','Countess']\nX.Title = X.Title.replace(rare_titles,'Rare')\nX_test_data.Title = X_test_data.Title.replace(rare_titles,'Rare')\n\n# I will label encode the new feature now in both X and X_test_data\nX['Title'] = LabelEncoder().fit_transform(X['Title']) \nX_test_data['Title'] = LabelEncoder().fit_transform(X_test_data['Title']) ","01d99f1f":"# FamilyMembers (SibSp and Parch)\n\n# I get the total number of each family adding SibSp and Parch. (+1) is the same passenger for both X and X_test_data.\n\n# For X\nX['FamilyMembers'] = X['SibSp'] + X['Parch']+1\nX.drop(['SibSp','Parch'], axis = 1, inplace = True)\n\n# For X_test_data\nX_test_data['FamilyMembers'] = X_test_data['SibSp'] + X_test_data['Parch']+1\nX_test_data.drop(['SibSp','Parch'], axis = 1, inplace = True)","dd7bc253":"fig, axs = plt.subplots(figsize=(15, 5))\nsns.countplot(x='FamilyMembers', hue='Survived', data=X).set_title(\"No. of family members\",fontdict= { 'fontsize': 20, 'fontweight':'bold'});\nsns.despine()","e9d68edb":"# I will bin this feature to prevent overfitting by having way too many catgories with a pretty small number each\n# Binning the column in X\nX.loc[ X['FamilyMembers'] == 1, 'FamilyMembers'] = 0                            \nX.loc[(X['FamilyMembers'] > 1) & (X['FamilyMembers'] <= 4), 'FamilyMembers'] = 1  \nX.loc[(X['FamilyMembers'] > 4) & (X['FamilyMembers'] <= 6), 'FamilyMembers'] = 2  \nX.loc[X['FamilyMembers']  > 6, 'FamilyMembers'] = 3 \n\n# Binning the column in X_test_data\nX_test_data.loc[ X_test_data['FamilyMembers'] == 1, 'FamilyMembers'] = 0                            \nX_test_data.loc[(X_test_data['FamilyMembers'] > 1) & (X_test_data['FamilyMembers'] <= 4), 'FamilyMembers'] = 1  \nX_test_data.loc[(X_test_data['FamilyMembers'] > 4) & (X_test_data['FamilyMembers'] <= 6), 'FamilyMembers'] = 2  \nX_test_data.loc[X_test_data['FamilyMembers']  > 6, 'FamilyMembers'] = 3 ","35e171fa":"# I plot the feature again\nfig, axs = plt.subplots(figsize=(15, 5))\nsns.countplot(x='FamilyMembers', hue='Survived', data=X).set_title(\"No. of family members\",fontdict= { 'fontsize': 20, 'fontweight':'bold'});\nsns.despine()","014b8723":"# As we saw in the plot with the AgeBins, the age group 0-10 years old passengers (0) is the only groups that survivors were more than non-survivors. Also, in the Sex plot we saw that most women survived and that most men died, so we conclude that being female and a child means a better chance of survival than being male and not a child. Thus, i will make a new feature \"FemAndChild\"\n\n# In X\nX[\"FemAndChild\"] = ((X.AgeBins == 0) | (X.Sex == 1))\n# In X_test_data\nX_test_data[\"FemAndChild\"] = ((X_test_data.AgeBins == 0) | (X_test_data.Sex == 1))\n\n# If the passenger is female and child, the i assign 1, and if not, i assign 0\n# In X\nX.loc[ X['FemAndChild'] == True, 'FemAndChild'] = 1  \nX.loc[ X['FemAndChild'] == False, 'FemAndChild'] = 0  \n\n# In X_test_data\nX_test_data.loc[ X_test_data['FemAndChild'] == True, 'FemAndChild'] = 1  \nX_test_data.loc[ X_test_data['FemAndChild'] == False, 'FemAndChild'] = 0 ","fb21131b":"# Lifeboat analysis\nfig, axs = plt.subplots(figsize=(15, 5))\nsns.countplot(x='Lifeboat', hue='Survived', data=X).set_title(\"Passengers that entered a lifeboat\",fontdict= { 'fontsize': 20, 'fontweight':'bold'});\nsns.despine()","1e7c3fdf":"# Here i make a new feature \"AccesToLifeboat\" which is equal to 0 if the person did not had access to a lifeboat, and equal to 1 if the person had access\nfor i in X[\"Lifeboat\"]:\n    X['AccessToLifeboat'] = (X['Lifeboat'].notnull().astype('int'))\n\nfor i in X_test_data[\"Lifeboat\"]:\n    X_test_data['AccessToLifeboat'] = (X_test_data['Lifeboat'].notnull().astype('int'))   \n\n# Now i can safely drop th original \"Lifeboat\" column\nX.drop([\"Lifeboat\"], axis=1, inplace=True)\nX_test_data.drop([\"Lifeboat\"], axis=1, inplace=True)","08d3fbc3":"# Checking that data types are same in X and X_test_data\nprint(X.info())\nprint(X_test_data.info())\nprint(X.head())","85e2d131":"# Finalizing data\n\nX.drop(['Name', 'Ticket'], axis = 1, inplace = True)\nX_test_data.drop(['Name', 'Ticket'], axis = 1, inplace = True)\n\nprint(X.info())\nprint(X_test_data.info())","8f7829b8":"# Modelling\n\nwarnings.filterwarnings(action='ignore')\n\nX['FemAndChild'] = pd.to_numeric(X['FemAndChild'])\nX_test_data['FemAndChild'] = pd.to_numeric(X_test_data['FemAndChild'])\n\n\nX.drop(['Survived'], axis=1, inplace=True)\n\nX_train, X_valid, y_train, y_valid = train_test_split(X, y, train_size=0.8, test_size=0.2, random_state=0)","47d33479":"# I use Logistic Regression\nlogreg = LogisticRegression()\nlogreg.fit(X_train, y_train)\n\nY_pred = logreg.predict(X_valid)\ncprint(\"Accuracy of the model with train and validation data is : \", \"green\")\naccuracy_score(y_valid, Y_pred)","d0c4ca9b":"#Submission is ready\npreds_test = logreg.predict(X_test_data) \nfinal_data_1 = {'PassengerId': X_test_data.index, 'Survived': preds_test}\nsubmission_1 = pd.DataFrame(data=final_data_1)\nsubmission_1.to_csv('Submission_Log_Regr.csv',index =False)","36d52aa1":"* Read, inspect & index data","741f70f4":"* Embarked analysis","777458ac":"As it can be seen, there are 22 unique values in the lifeboat column, while the RMS Titanic had (according to Wikipedia) 20 lifeboats (specificaly 16 Big Lifeboats: 1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16 & 4 Smaller wooden lifeboats: A,B,C,D). Also according to Wikipedia again, not every passenger who entered a lifeboat survived (for instance some died out of cold, others fell in the water etc.). The 2 extra unique values are 14? and ?. As long as people survived in these 2 values, we can conclude that the entered a lifeboat indeed as it was the only ay to survive, but we just do not know which boat was exactly ","a2f99f22":"* Fare analysis","5a2eb468":"* Sex analysis","0cfc311b":"* Feature extraction","128281e9":"* Exploratory data analysis","72f966dd":"* Dealing with missing data","0f3810a7":"As it can be seen, the only age group where the survivors were more than non-survivors, is the 0-10 years old group (0)\nAlso, the 20-30 years old group (2), had disproportionally more deaths than any other age group, making our data distribution pretty skewed","e9704609":"The lifeboat column has a lot of null values. There 2 scenarios in that case: either the data was lost, or passengers who don't have a lifeboat value did not had indeed lifeboat access. In both X_extented and X_test_data_extented, the sum of missing values in Lifeboat columns is approximately 62% and according to Wikipedia the RMS Titanic had lifeboats enough for the 1\/3 of passengers, so i speculate that the later scenario is True","fde6b5b9":"* Pclass analysis","76ea91b1":"* Modelling","46f02f2c":"Now the distribution is not that skewed as it was in the previous plot, so the chances of overfitting are lower now\nAlso, we can see that that people with no other family members alone (0) constituted the biggest part of passengers aboard (maybe because there was a lot of staff also aboard).","96cdb8fa":"1) There is a decent positive corelation between SibSp (Siblings and\/or Spouses aboard) and Parch (Parents and\/or Children aboard) which totally makes sense. I could maybe concatenate them and make a new feature later\n\n2) There is a slightly positive correlation between the Fare and whether the person Survived or not, so Fare appears to be a pretty valuable feature\n\n3) There is also a moderately negative correlation between Pclass and Fare which obviously makes sense too\n\n4) Another pretty negative correlation is the one between HasCabin and Pclass. That probably indicates that passengers with cabins had a specific kind of pclass only","e6d305b8":"The vast majority of Fare holders belong in the first bin as expected, making our data distribution extremely skewed\nThe first group is also the only one in which survivors are outnumbered by non-survivors","16976270":"* Age analysis","7e3298a4":"As it can be seen, the majority of male passengers did not survive, whereas the majority of females survived\nIt appears that \"Sex\" feature is pretty valuable as expected","66152e39":"There are no missing data anymore. I can procceed to analyze them now","1f0beece":"It appears that there is a considerable number of outliers (black dots in the boxplot)\nI will use Binning, to make the model more robust and prevent overfitting. However, this method has a cost on the performance","ec51ffe0":"It seems that the survivors to no-survivors ratio is different for every embarkation port.\nFor Southampton(0), the ratio is approximately equal to 2, while for Cherbourg(1) and Queenstown(2) is closer to 1, so we can cocnlude that port of embarkation has to do with survival possibility","af692718":"As expected, the Pclass (ticket class) is decently related with the chance of survival\nClass 1 ticket holders have a ratio of survivors to no-survivors of around 1,8 , while Class 2 ticket holders had a ratio of approximately 1 and class 3 ticket holders had the worst ratio of around 3\nIt appears that ticket class was an important factor in the survival chances, and also shows that there was some element of luck too"}}