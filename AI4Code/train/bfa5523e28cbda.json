{"cell_type":{"e742530b":"code","6227f9e7":"code","92816b53":"code","77d861f6":"code","8fd91ff5":"code","72d99c12":"code","4a072396":"code","bae8044d":"code","b91cd99b":"code","966a0a87":"code","4f96a974":"code","b46406de":"code","76df895d":"code","9a795029":"code","e609d68d":"code","be3a639b":"code","a7fe0778":"code","5f963c2b":"code","f280e520":"code","f2f9dc9b":"code","639210ad":"code","4a1a20b9":"code","51caa1fb":"code","2c05b9c3":"code","af908ae5":"code","095fdb76":"code","b608bda8":"code","6fe2f1ab":"code","98758667":"code","f400cd7a":"code","4fbafa51":"code","6ae5ef7b":"code","cc566e9c":"code","4ffe7c34":"code","10db3032":"code","8eb5c318":"code","e7362d87":"code","49104996":"code","6d968c94":"markdown","ef496834":"markdown","d04bb633":"markdown","bf200ac0":"markdown","a0a7883e":"markdown","6e0af7a5":"markdown","3f4acae1":"markdown","c8636b65":"markdown","927795c4":"markdown","ccac1360":"markdown","c3cb680d":"markdown","3e9946a5":"markdown","6ead586e":"markdown","ef48586e":"markdown"},"source":{"e742530b":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","6227f9e7":"#import required modules\nfrom pandas_profiling import ProfileReport\nimport missingno as msno\n\nimport pandas as pd\nimport numpy as np\n\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\n\n#Pre-Processing libraries\nfrom sklearn.base import BaseEstimator, TransformerMixin\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.pipeline import FeatureUnion, Pipeline\n\n#Sk-Learn Models\nfrom sklearn.linear_model import LogisticRegression, RidgeClassifier, SGDClassifier\nfrom sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor\nfrom sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, BaggingClassifier\nfrom sklearn.naive_bayes import CategoricalNB\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import SVC\n\n#Sk-Learn Model Selection\nfrom sklearn.model_selection import cross_val_score, KFold, GridSearchCV, RandomizedSearchCV, train_test_split, StratifiedKFold\nfrom sklearn.feature_selection import RFECV, RFE\nfrom sklearn.metrics import confusion_matrix, accuracy_score, ConfusionMatrixDisplay\n\nfrom imblearn.over_sampling import SMOTE\n\nfrom scipy import stats","92816b53":"#Load train and test data\ntrain = pd.read_csv(\"\/kaggle\/input\/titanic\/train.csv\")\ntest = pd.read_csv(\"\/kaggle\/input\/titanic\/test.csv\")","77d861f6":"#Visualize the dataset\ntrain.head()","8fd91ff5":"#Display all rows\n# pd.set_option('display.max_rows', None)","72d99c12":"#View the data info - Check number of Null fields in each column\ntrain.info()\n\n\n#Key Observations\n#Cabin column has a lot of Null values. Hence, we will delete the column\n#We need to impute Age column and Embarked columns as they have null values","4a072396":"##Print percentage of missing values for each column\nprint('Missing values Percentage: \\n\\n',round (train.isnull().sum().sort_values(ascending=False)\/len(train)*100,1))","bae8044d":"# profile = ProfileReport(train, title=\"Pandas Profiling Report\", explorative=True)\n# profile.to_notebook_iframe()","b91cd99b":"# list of columns to be dropped\ndropCols = ['Name', 'FirstName', 'Ticket', 'Cabin', 'LastName']\nnumCols = ['SibSp', 'Parch','TicketLen', \"Pclass\", \"Age\", \"Fare\"]\ncatCols = ['Title', 'Embarked', 'Sex', 'TicketFirst', 'TicketLast']","966a0a87":"#Transformer to add new columns - FirstName, LastName and Title\nclass AddNewCols(BaseEstimator, TransformerMixin):\n    \n    #Class Constructor\n    def __init__(self):\n        pass\n    \n    #Return self nothing else to do here\n    def fit( self, X, y = None  ):\n        return self\n    \n    #Transformer method to drop the columns\n    def transform(self, X , y = None ):\n        \n        #Create New columns for FirstName, LastName and Title\n        X[\"FirstName\"] = X[\"Name\"].apply(lambda x: x.split(',')[1:][0])\n        X[\"LastName\"] = X[\"Name\"].apply(lambda x: x.split(',')[0])\n        X[\"Title\"] = X[\"FirstName\"].apply(lambda x: x.split('.')[0])\n        \n        X[\"TicketLen\"] = X[\"Ticket\"].apply(lambda x: len(x))\n        X[\"TicketFirst\"] = X[\"Ticket\"].apply(lambda x: x[0:3])\n        X[\"TicketLast\"] = X[\"Ticket\"].apply(lambda x: x[-2])\n        return X","4f96a974":"#Transformer to impute Missing Values\nclass MissingValue(BaseEstimator, TransformerMixin):\n    \n    #Class Constructor\n    def __init__(self):\n        pass\n    \n    \n    def getAgeTitleVals(X):\n        \n        #create an empty dict to store title and median age value for the given title\n        ageVals = {}\n        \n        #get the list of age titles\n        ageTitles = X.loc[X[\"Age\"].isnull()][\"Title\"].value_counts()\n\n        #Loop over ageTitles\n        #Get the median age value for each Title and store in ageVals dictionary\n        for eachTitle in ageTitles.index:\n            ageVals[eachTitle] = X.loc[X[\"Title\"] == eachTitle][\"Age\"].median()\n                \n        return ageVals\n        \n    \n    #Return self nothing else to do here\n    def fit( self, X, y = None  ):\n        self.ageVals = MissingValue.getAgeTitleVals(X)\n        self.ageMode = X['Age'].mode()[0]\n        self.embarkedMode = X['Embarked'].mode()[0]\n        self.fare = X['Fare'].mean()\n        return self\n    \n    def imputeAge(ageVals, X, y = None):\n        for eachTitle, medianAgeVal in ageVals.items():\n            X.loc[(X[\"Title\"] == eachTitle) & (X[\"Age\"].isnull()),[\"Age\"]] = medianAgeVal\n        return X\n        \n    #Transformer method to drop the columns\n    def transform(self, X , y = None ):\n        #Fill Missing values\n        X = MissingValue.imputeAge(self.ageVals, X, y = None)\n        X['Age'].fillna(self.ageMode, inplace=True)\n        X['Embarked'].fillna(self.embarkedMode, inplace=True)\n        X['Fare'].fillna(self.fare, inplace=True)\n        return X","b46406de":"#Custom Transformer that transforms Categorical columns\nclass DummyEncoding( BaseEstimator, TransformerMixin):\n    \n    #Class Constructor\n    def __init__(self, dummyCols):\n        self.catCols = catCols\n        \n    #Return self nothing else to do here\n    def fit( self, X, y = None  ):\n        return self\n    \n    #Transformer method we wrote for this transformer \n    def transform(self, X , y = None ):\n        \n        #One-Hot Encoding of categorical columns\n        #Get dummy variables\n        for each_col in self.catCols:\n            X[each_col] = pd.factorize(X[each_col])[0]\n                    \n        return X","76df895d":"#Custom Transformer that scales Numerical columns\nclass CustomScaler( BaseEstimator, TransformerMixin):\n    \n    #Class Constructor\n    def __init__(self, numCols):\n        self.numCols = numCols\n        self.scaler = MinMaxScaler()\n        \n    #Return self nothing else to do here\n    def fit( self, X, y = None  ):\n        self.scaler.fit(X[numCols])\n        return self\n    \n    #Transformer method we wrote for this transformer \n    def transform(self, X , y = None ):\n        \n        X[numCols] = self.scaler.transform(X[numCols])\n        \n        return X","9a795029":"#Transformer to drop selected columns\nclass DelColumns(BaseEstimator, TransformerMixin):\n    \n    #Class Constructor\n    def __init__(self, dropCols):\n        self.dropCols = dropCols\n        \n    #Return self nothing else to do here\n    def fit( self, X, y = None  ):\n        return self\n    \n    #Transformer method to drop the columns\n    def transform(self, X , y = None ):\n        #Drop selected columns\n        X = X.drop(self.dropCols, axis=1)\n        return X","e609d68d":"#Transformer to convert selected columns to categorical\nclass Binning(BaseEstimator, TransformerMixin):\n    \n    #Class Constructor\n    def __init__(self):\n        pass\n    \n    #Return self nothing else to do here\n    def fit( self, X, y = None  ):\n        return self\n    \n    #Transformer method to drop the columns\n    def transform(self, X , y = None ):\n        #Binning of Age column\n#         X['Age'] = pd.qcut(X['Age'], q=10)\n#         X['Fare'] = pd.qcut(X['Fare'], q=10)\n        X['Age'] = pd.cut(X['Age'], list(range(0,110,5)))\n        X['Fare'] = pd.cut(X['Fare'], [0, 10, 20, 30, 40, 50, 60,70, 700])\n\n        return X","be3a639b":"# Pipeline to add new cols\naddNewColsPipeline = Pipeline( steps = [('addNewColsPipeline', AddNewCols())])\n\n#Defining the steps in the numerical pipeline     \nmissingValuePipeline = Pipeline( steps = [ ('missingValueImputer', MissingValue())])\n\n#Binning Age and Fare\nbinning_pipeline = Pipeline( steps = [ ('binning', Binning())] )\n\n#Dummy Encoding for categorical pipeline\ncategoricalPipeline = Pipeline( steps = [('dummyEncoding', DummyEncoding(catCols))])\n\n#Defining the steps in the numerical pipeline     \nnumericalPipeline = Pipeline( steps = [( 'customScaler', CustomScaler(numCols) )])\n\n#Pipeline for dropping selected cols\ndropPipeline = Pipeline( steps = [('dropCols', DelColumns(dropCols))])","a7fe0778":"#Combining numerical and categorical piepline into one full big pipeline horizontally \npreProcessingPipeline = Pipeline( steps = [ ( 'addNewColsPipeline', addNewColsPipeline),\n                                            ( 'missingValuePipeline', missingValuePipeline),\n                                            ( 'categoricalPipeline', categoricalPipeline),\n                                            ( 'numericalPipeline', numericalPipeline),\n                                            ( 'dropPipeline', dropPipeline)\n                                            ] )","5f963c2b":"#Fit_transform the pipeline on training data\ntrain_transform = preProcessingPipeline.fit_transform(train)","f280e520":"#Transform the pipeline on test data\ntest_transform = preProcessingPipeline.transform(test)","f2f9dc9b":"test_transform.info()","639210ad":"# Putting response variable to y\ny_train = train_transform.pop('Survived')\nX_train = train_transform\n\n# Putting response variable to y\nX_test = test_transform","4a1a20b9":"X_train.set_index('PassengerId', inplace=True)\nX_test.set_index('PassengerId', inplace=True)","51caa1fb":"sm = SMOTE(random_state=2)\nX_train, y_train = sm.fit_resample(X_train, y_train.ravel())","2c05b9c3":"#Visualize X_train\nX_train.head()","af908ae5":"#Initialize the mode\n# model = RandomForestClassifier(n_estimators=150, max_depth=8, random_state = 23)\nmodel = RandomForestClassifier()","095fdb76":"#Fit the model\nmodel.fit(X_train, y_train)","b608bda8":"#Scoring parameter\nscoring = \"roc_auc\"\n\n#Number of splits in K-Fold Cross Validation\nn_splits = 5\n\n#Random state\nrandom_state = 23\n\n#Shuffle in K-Fold cross validation\nshuffle = True","6fe2f1ab":"kfold = KFold(n_splits=n_splits, random_state=random_state, shuffle=shuffle)\n\nresults = cross_val_score(model, X_train, y_train, cv=kfold, scoring=scoring)\n\nprint(\"Results of Cross-Validation: \", results)\n\nprint(\"Average of Cross-Validation results: \", results.mean())","98758667":"# Box Plot of Model Results\nfig = plt.figure()\nfig.suptitle('Algorithm Comparison')\nax = fig.add_subplot(111)\nplt.boxplot(results)\nax.set_xticklabels([\"RandomForest\"])\nplt.xticks(rotation=45)\nplt.show()","f400cd7a":"paramsRF = {'n_estimators': [25, 50, 100, 150, 200], \n            'max_depth': [5,7,9,11,13], \n            'max_features': [\"auto\", \"sqrt\", \"log2\"], \n            'random_state': [42]\n           }","4fbafa51":"#Initialize HyperParameters\ngrid_search = RandomizedSearchCV(estimator=model, \n                           param_distributions=paramsRF, \n                           cv=n_splits, n_jobs=-1, verbose=1, scoring = scoring)\n        \n#Fit the optimized model\ngrid_search.fit(X_train, y_train)\n        \n#Optimized Model\nmodel = grid_search.best_estimator_","6ae5ef7b":"#Visualize the best model\nmodel","cc566e9c":"#Cross Validation on the selected Model\n\nkfold = KFold(n_splits=n_splits, random_state=random_state, shuffle=shuffle)\n\nresults = cross_val_score(model, X_train, y_train, cv=kfold, scoring=scoring)\n\nprint(\"Results of Cross-Validation: \", results)\n\nprint(\"Average of CV Results of best model: \", results.mean())","4ffe7c34":"#Predict Y\ny_train_pred = model.predict(X_train)","10db3032":"confusion_matrix(y_train, y_train_pred)","8eb5c318":"predictions = model.predict(X_test)\nprint(\"This is predictions: \", predictions)","e7362d87":"output = pd.DataFrame({'PassengerId': X_test.index, 'Survived': predictions})\noutput.to_csv('my_submission.csv', index=False)","49104996":"X_train.head()","6d968c94":"# 1. Import Libaries","ef496834":"# Initialize the model","d04bb633":"### Setup Pipelines","bf200ac0":"# Submit the Predictions","a0a7883e":"# HyperParameter Optimization","6e0af7a5":"**Methodology of Age Imputation**\n- Instead of simply imputing missing values in Age column by overall Mean\/Median, we can take a more analytical approach. \n- We should ideally use different Age values to impute age of a child, bachelor, married man, married woman etc.\n- But, how do we know which passenger is a child or an adult?\n- The clue lies in the `Name` column. Each name has a `Title` in it. Using string manipulation, we can extract the `Title`\n- We can group `Age` by `Title` and calculate the median Age value for each Title and impute accordingly","3f4acae1":"# 3. Overview of the dataset ","c8636b65":"### Parameters for K-Fold Cross Validation","927795c4":"### Data Pre-Processing Steps\n- Add new columns (FirstName, LastName and Title)\n- Impute missing values (Age, Embarked and Fare)\n- Delete following columns (FirstName, LastName, Name, Ticket, Cabin)\n- Dummy Encoding (Title, Embarked and Sex)\n- Scaling (Age, Fare, SibSp and Parch)","ccac1360":"# 3. Data Pre-Processing","c3cb680d":"# Model Predictions","3e9946a5":"# 2. Load the datasets","6ead586e":"### Extract X and y","ef48586e":"# Cross Validation"}}