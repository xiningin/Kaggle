{"cell_type":{"32f4e48b":"code","7838f61b":"code","9c5eda40":"code","b53984ba":"code","31274a6e":"code","a9471de8":"code","00f926f3":"code","82e76eb5":"code","dfe412ba":"code","817a0540":"code","e7ae1b67":"code","2f9f6c69":"code","47eb52e1":"code","2d336224":"code","d36608dd":"code","cc9d39bf":"code","e2f5097f":"code","5c867ea9":"code","d552b174":"code","44fa6aad":"code","ce1e1f25":"code","dfadbab8":"code","8dde75f2":"code","7982145e":"code","dc0bffea":"code","fcafe50f":"code","de0796eb":"code","1cd6616e":"code","cae6b1ae":"code","ed95e913":"code","36a0b827":"code","c3f24058":"code","4ba12fc9":"code","7dee6a56":"code","eeb62ba1":"code","e9f09653":"code","362d9055":"code","56483a17":"code","e18a25c9":"code","884af337":"code","d7545383":"code","fe04c1ad":"code","c66ff31d":"code","b96da906":"code","84ea49c5":"code","8e6e0e59":"code","ae960a99":"code","b33929bd":"code","f71e5b49":"code","d13603da":"code","4ed4925c":"code","a3c4792b":"code","ea51affe":"code","6d706dac":"code","2adaf370":"code","cd64c4c1":"code","e81cc67e":"code","60928a38":"code","a92bc9a2":"code","457d471b":"code","acc744dc":"code","1c4be5b5":"code","ebc4f3a9":"code","e3a543a6":"code","5bf45d88":"code","fbfae216":"code","8498e1fe":"code","ae8d38db":"code","0223f39a":"code","270c5112":"code","ccbc40ce":"code","df0cdd21":"code","4b62b910":"code","e85ec269":"code","2193a2c7":"code","b651dba3":"code","c937481c":"code","3510a345":"code","9916680c":"markdown","a1cf78b4":"markdown","df6f12fb":"markdown","b12d2dfc":"markdown","bfc2e006":"markdown","8d234bc6":"markdown","2cb82e42":"markdown","c89e7105":"markdown","c4bc0fb1":"markdown","c0e279fd":"markdown","9d3f99ff":"markdown","4253d8ec":"markdown","a22df73a":"markdown","e2c8037d":"markdown","6398678d":"markdown","58939c65":"markdown","9a75baaa":"markdown","d709ad32":"markdown","1dc3e43c":"markdown","5fb72b4a":"markdown","cd2dec10":"markdown","df024ad4":"markdown","8c984843":"markdown","e673c077":"markdown","9027c1a8":"markdown","dbeb70b1":"markdown","f5c9221a":"markdown","eb3b369c":"markdown","d70f6f84":"markdown","64e71d48":"markdown","a2cd11a4":"markdown"},"source":{"32f4e48b":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","7838f61b":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom datetime import datetime\n\n#\n\nfrom scipy import stats\nfrom scipy.stats import skew, boxcox_normmax, norm\nfrom scipy.special import boxcox1p\n\n#\n\nimport matplotlib.gridspec as gridspec\nfrom matplotlib.ticker import MaxNLocator\n\n#\n\nimport warnings\npd.options.display.max_columns = 250\npd.options.display.max_rows = 250\nwarnings.filterwarnings('ignore')\nplt.style.use('fivethirtyeight')","9c5eda40":"# Loading datasets.\n\npower_df = pd.read_csv('..\/input\/powerforecast\/power_actual.csv')\nweather_df = pd.read_csv('..\/input\/powerforecast\/weather_actuals.csv')","b53984ba":"print(power_df.shape, weather_df.shape)","31274a6e":"power_df.head()","a9471de8":"power_df.describe()","00f926f3":"weather_df.head()","82e76eb5":"weather_df.describe()","dfe412ba":"'''\nDropping unnecessary Unnamed: 0 column from both frames .\nplant_id, datetime_utc from  weather_df and ghi, gti from power_df\n'''\npower_df.drop(columns = ['Unnamed: 0', 'ghi', 'gti'], axis=1, inplace=True)\nweather_df.drop(columns = ['Unnamed: 0', 'plant_id','datetime_utc'], axis=1, inplace=True)","817a0540":"power_df.head()","e7ae1b67":"weather_df.head()","2f9f6c69":"# Display numerical correlations (pearson) between features on heatmap.\n\nsns.set(font_scale=1.1)\ncorrelation_train = weather_df.corr()\nmask = np.triu(correlation_train.corr())\nplt.figure(figsize=(20, 20))\nsns.heatmap(correlation_train,\n            annot=True,\n            fmt='.1f',\n            cmap='coolwarm',\n            square=True,\n            mask=mask,\n            linewidths=1,\n            cbar=False)\n\nplt.show()","47eb52e1":"power_df.head()","2d336224":"# dropping columns with zero standard deviation from weather_df\ncolumn_to_drop = ['wind_chill', 'heat_index', 'qpf', 'snow', 'pop', 'fctcode' ,'precip_accumulation','sunrise', 'sunset','updated_at']\nweather_df.drop(columns = column_to_drop, axis=1, inplace=True)","d36608dd":"#replace -9999.000000 with nan (invalid entries)\nweather_df = weather_df.replace(-9999.000000, np.nan)","cc9d39bf":"def missing_percentage(df):\n    \n    \"\"\"A function for returning missing ratios.\"\"\"\n    \n    total = df.isnull().sum().sort_values(\n        ascending=False)[df.isnull().sum().sort_values(ascending=False) != 0]\n    percent = (df.isnull().sum().sort_values(ascending=False) \/ len(df) *\n               100)[(df.isnull().sum().sort_values(ascending=False) \/ len(df) *\n                     100) != 0]\n    return pd.concat([total, percent], axis=1, keys=['Total', 'Percent'])","e2f5097f":"# Checking 'NaN' values.\n\nmissing = missing_percentage(weather_df)\n\nfig, ax = plt.subplots(figsize=(20, 5))\nsns.barplot(x=missing.index, y='Percent', data=missing, palette='Reds_r')\nplt.xticks(rotation=90)\n\ndisplay(missing.T.style.background_gradient(cmap='Reds', axis=1))","5c867ea9":"#List of columns to drop\ncolumn_to_drop = ['precip_type']\nweather_df.drop(columns = column_to_drop, axis=1, inplace=True)\n\n# List of 'NaN' including columns where NaN's actually missing gonna replaced with mode.\n\nfreq_cols = ['uv_index', 'precip_probability', 'precip_intensity', 'ozone', 'wind_gust', 'pressure', 'wind_bearing', 'wind_speed', 'cloud_cover', 'visibility']\n\nfor col in freq_cols:\n    weather_df[col].replace(np.nan, weather_df[col].mode()[0], inplace=True)","d552b174":"sns.set(font_scale=1.1)\ncorrelation_train = weather_df.corr()\nmask = np.triu(correlation_train.corr())\nplt.figure(figsize=(20, 20))\nsns.heatmap(correlation_train,\n            annot=True,\n            fmt='.1f',\n            cmap='coolwarm',\n            square=True,\n            mask=mask,\n            linewidths=1,\n            cbar=False)\n\nplt.show()","44fa6aad":"# Dropping apparent_temperature\ncolumn_to_drop = ['apparent_temperature']\nweather_df.drop(columns = column_to_drop, axis=1, inplace=True)","ce1e1f25":"power_df.describe()","dfadbab8":"# converting datetime column to proper datetime format\npower_df['datetime'] = pd.to_datetime(power_df['datetime'])\n# Create temp power_df dataframe to find outliers in the data\npower_df.set_index('datetime', inplace=True)","8dde75f2":"power_df['power'].plot(marker='.', alpha=0.5, linestyle='None', figsize=(15, 15), subplots=True, ylabel='Power')","7982145e":"power_df['2018-02-25':'2018-03-20':].resample('W').mean()","dc0bffea":"to_replace = power_df.loc['2019-02-25':'2019-03-18', 'power'].tolist()\npower_df.loc['2018-02-25':'2018-03-18','power'] = to_replace","fcafe50f":"power_df['2018-02-25':'2018-03-20':].resample('W').mean()","de0796eb":"power_df['2018-07-25':'2018-08-20':].resample('W').mean()","1cd6616e":"to_replace = power_df.loc['2019-01-01':'2019-01-22', 'power'].tolist()\npower_df.loc['2018-07-29':'2018-08-19','power'] = to_replace","cae6b1ae":"power_df['2018-07-25':'2018-08-20':].resample('W').mean()","ed95e913":"# monthwise mean power generation\npower_df['2019'].resample('M').mean()","36a0b827":"power_df.loc['2019-07-05':'2019-08-15'].resample('4D').mean()","c3f24058":"to_replace = power_df.loc['2018-07-13':'2018-08-06', 'power'].tolist()\npower_df.loc['2019-07-13':'2019-08-06','power'] = to_replace","4ba12fc9":"power_df.loc['2019-07-05':'2019-08-15'].resample('4D').mean()","7dee6a56":"power_df['power'].plot(marker='.', alpha=0.5, linestyle='None', figsize=(15, 15), subplots=True, ylabel='Power')","eeb62ba1":"weather_df.head()","e9f09653":"## setting datetime_local as index for upsampling\nweather_df['datetime_local'] = pd.to_datetime(weather_df['datetime_local'])\nweather_df['datetime'] = weather_df['datetime_local'].copy()\nweather_df.set_index('datetime', inplace=True)\nweather_df.head()","362d9055":"# Backing up","56483a17":"weather_df = weather_df.resample('15Min').mean()\nweather_df_columns = weather_df.columns\nfor column in weather_df_columns:\n    weather_df[column] = weather_df[column].interpolate()\n    \nweather_df.head()","e18a25c9":"nRow, nCol = weather_df.shape\nprint(f'There are {nRow} rows and {nCol} columns after interpolation')","884af337":"nRow, nCol = power_df.shape\nprint(f'There are {nRow} rows and {nCol} columns for power_df')","d7545383":"weather_df.head()","fe04c1ad":"df = pd.merge(power_df,weather_df, how='inner', left_index=True, right_index=True)\ndf.sample(4)","c66ff31d":"# Plotting numerical features with polynomial order to detect outliers by eye.\n\ndef srt_reg(y, df , subplot_y, subplot_x):\n    fig, axes = plt.subplots(subplot_y, subplot_x, figsize=(25, 80))\n    axes = axes.flatten()\n\n    for i, j in zip(df.select_dtypes(include=['number']).columns, axes):\n\n        sns.regplot(x=i,\n                    y=y,\n                    data=df,\n                    ax=j,\n                    order=3,\n                    ci=None,\n                    color='#e74c3c',\n                    line_kws={'color': 'black'},\n                    scatter_kws={'alpha':0.4})\n        j.tick_params(labelrotation=45)\n        j.yaxis.set_major_locator(MaxNLocator(nbins=10))\n\n        plt.tight_layout()\n","b96da906":"srt_reg('power', df, 5,  3)","84ea49c5":"df['datetime_hour'] = pd.DatetimeIndex(df.index).hour\ndf.sample(5)","8e6e0e59":"df['hour_cos'] = np.cos(2 * np.pi * df['datetime_hour']\/max(df['datetime_hour']))","ae960a99":"df.head()","b33929bd":"# daypart function \ndef daypart(hour):\n    if 0<= hour <= 5:\n        return \"Night\"\n    elif 5 < hour < 19:\n        return \"Day\"\n    elif 19 <= hour <=23:\n        return \"Night\"\n    else: return \"Night\"\n#day and night onehot encoding\ndf['day_part'] = df['datetime_hour'].apply(daypart)\n\n# mapping day as 1 and night as 0\nday_part_map = {\"Day\" : 1, \"Night\": 0}\ndf['day_part'] = df['day_part'].map(day_part_map).astype('int')","f71e5b49":"skewed = ['cloud_cover', 'temperature', 'humidity', 'dew_point',\n       'wind_bearing', 'wind_speed', 'wind_gust', 'pressure', 'uv_index',\n       'ozone', 'precip_intensity', 'precip_probability', 'visibility', 'power']\n\n# Finding skewness of the numerical features.\n\nskew_features = np.abs(df[skewed].apply(lambda x: skew(x)).sort_values(\n    ascending=False))\n\n# Filtering skewed features.\n\nhigh_skew = skew_features[skew_features > 0.3]\n\n# Taking indexes of high skew.\n\nskew_index = high_skew.index\n\n# Applying boxcox transformation to fix skewness.\n\nfor i in skew_index:\n    df[i] = boxcox1p(df[i], boxcox_normmax(df[i] + 1))","d13603da":"'''\nshould be greater than zero as there are lagged versions of power column\n'''\nprint(f'Number of missing values: {df.isna().sum().sum()}') \n","4ed4925c":"srt_reg('power', df, 6,3 )","a3c4792b":"# Display numerical correlations (pearson) between features on heatmap.\n\nsns.set(font_scale=1.1)\ncorrelation_train = df.corr()\nmask = np.triu(correlation_train.corr())\nplt.figure(figsize=(20, 20))\nsns.heatmap(correlation_train,\n            annot=True,\n            fmt='.1f',\n            cmap='coolwarm',\n            square=True,\n            mask=mask,\n            linewidths=1,\n            cbar=False)\n\nplt.show()","ea51affe":"# # Features to drop:\n\nto_drop = ['datetime_hour', 'visibility','precip_intensity', 'precip_probability',\n           'wind_gust', 'pressure', 'humidity', 'dew_point', \n           'ozone', 'cloud_cover']\n\n# Dropping features.\n\ndf.drop(columns=to_drop, inplace=True)","6d706dac":"# Display numerical correlations (pearson) between features on heatmap.\n# After dropping correlation\nsns.set(font_scale=1.1)\ncorrelation_train = df.corr()\nmask = np.triu(correlation_train.corr())\nplt.figure(figsize=(20, 20))\nsns.heatmap(correlation_train,\n            annot=True,\n            fmt='.1f',\n            cmap='coolwarm',\n            square=True,\n            mask=mask,\n            linewidths=1,\n            cbar=False)\n\nplt.show()","2adaf370":"def plot_dist3(df, feature, title):\n    \n    # Creating a customized chart. and giving in figsize and everything.\n    \n    fig = plt.figure(constrained_layout=True, figsize=(12, 8))\n    \n    # creating a grid of 3 cols and 3 rows.\n    \n    grid = gridspec.GridSpec(ncols=3, nrows=3, figure=fig)\n\n    # Customizing the histogram grid.\n    \n    ax1 = fig.add_subplot(grid[0, :2])\n    \n    # Set the title.\n    \n    ax1.set_title('Histogram')\n    \n    # plot the histogram.\n    \n    sns.distplot(df.loc[:, feature],\n                 hist=True,\n                 kde=True,\n                 fit=norm,\n                 ax=ax1,\n                 color='#e74c3c')\n    ax1.legend(labels=['Normal', 'Actual'])\n\n    # customizing the QQ_plot.\n    \n    ax2 = fig.add_subplot(grid[1, :2])\n    \n    # Set the title.\n    \n    ax2.set_title('Probability Plot')\n    \n    # Plotting the QQ_Plot.\n    stats.probplot(df.loc[:, feature].fillna(np.mean(df.loc[:, feature])),\n                   plot=ax2)\n    ax2.get_lines()[0].set_markerfacecolor('#e74c3c')\n    ax2.get_lines()[0].set_markersize(12.0)\n\n    # Customizing the Box Plot:\n    \n    ax3 = fig.add_subplot(grid[:, 2])\n    # Set title.\n    \n    ax3.set_title('Box Plot')\n    \n    # Plotting the box plot.\n    \n    sns.boxplot(df.loc[:, feature], orient='v', ax=ax3, color='#e74c3c')\n    ax3.yaxis.set_major_locator(MaxNLocator(nbins=24))\n\n    plt.suptitle(f'{title}', fontsize=24)","cd64c4c1":"plot_dist3(df, 'power', 'Power Before Log Transformation')","e81cc67e":"df['power'] = np.log10(df['power']+1)\nplot_dist3(df, 'power', 'Power After log10(x+1) Transformation')","60928a38":"# Backing up target variables and dropping them from train data.\n\ny = df['power'].reset_index(drop=True)\ndf = df.drop(['power'], axis=1)","a92bc9a2":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(df, y, test_size=0.5, random_state=42)","457d471b":"# Loading neccesary packages for modelling.\nfrom sklearn.model_selection import cross_val_score, KFold, cross_validate\nfrom sklearn.preprocessing import RobustScaler\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.metrics import mean_squared_error\nfrom xgboost import XGBRegressor","acc744dc":"# Setting kfold for future use.\n\nkf = KFold(4, random_state=42)\n# xgboost:\n\nxgboost = XGBRegressor(\n    learning_rate=0.0139,\n    n_estimators=4500,\n    max_depth=4,\n    min_child_weight=0,\n    subsample=0.7968,\n    colsample_bytree=0.4064,\n    nthread=-1,\n    scale_pos_weight=2,\n    seed=42,\n)\n\n","1c4be5b5":"def model_check(X, y, estimators, cv):\n    \n    ''' A function for testing multiple estimators.'''\n    \n    model_table = pd.DataFrame()\n\n    row_index = 0\n    for est, label in zip(estimators, labels):\n\n        MLA_name = label\n        model_table.loc[row_index, 'Model Name'] = MLA_name\n\n        cv_results = cross_validate(est,\n                                    X,\n                                    y,\n                                    cv=cv,\n                                    scoring='neg_root_mean_squared_error',\n                                    return_train_score=True,\n                                    n_jobs=-1)\n\n        model_table.loc[row_index, 'Train RMSE'] = -cv_results[\n            'train_score'].mean()\n        model_table.loc[row_index, 'Test RMSE'] = -cv_results[\n            'test_score'].mean()\n        model_table.loc[row_index, 'Test Std'] = cv_results['test_score'].std()\n        model_table.loc[row_index, 'Time'] = cv_results['fit_time'].mean()\n\n        row_index += 1\n\n    model_table.sort_values(by=['Test RMSE'],\n                            ascending=True,\n                            inplace=True)\n\n    return model_table","ebc4f3a9":"\nestimators = [xgboost]\nlabels = ['XGBRegressor']","e3a543a6":"# # # # Executing cross validation.\nraw_models = model_check(X_train, y_train, estimators, kf)","5bf45d88":"display(raw_models.style.background_gradient(cmap='summer_r'))","fbfae216":"estimators = [xgboost]\nfor est in estimators:\n    est.fit(X_train, y_train)\n\n    y_pred = est.predict(X_test)\n\n    mse=mean_squared_error(y_test, y_pred)\n\n    print(np.sqrt(mse), str(est))","8498e1fe":"df.head()","ae8d38db":"# Fitting the models on full data.\n\nprint('=' * 20, 'START Fitting', '=' * 20)\nprint('=' * 55)\nprint(datetime.now(), 'XGboost')\nxgb_model_full_data = xgboost.fit(df.values, y.values)\nprint('=' * 20, 'FINISHED Fitting', '=' * 20)\nprint('=' * 58)","0223f39a":"weather_forecast_df = pd.read_csv('..\/input\/powerforecast\/weather_forecast.csv')\nweather_forecast_df.head()","270c5112":"def preprocess (df_preprocess):\n    '''\n    preprocess forecast df for power prediction\n    '''\n    #List of columns to drop\n    column_to_drop = ['Unnamed: 0', 'plant_id','datetime_utc','wind_chill', 'heat_index', 'qpf', 'snow', 'pop',\n                      'fctcode' ,'precip_accumulation', 'sunrise', 'sunset','updated_at',\n                      'precip_probability', 'precip_intensity', 'ozone', 'wind_gust',\n                      'pressure', 'cloud_cover', 'visibility','precip_type','apparent_temperature', 'humidity', 'dew_point']\n    df_preprocess.drop(columns = column_to_drop, axis=1, inplace=True) # zero std\n    #replace -9999.000000 with nan (invalid entries)\n    df_preprocess = df_preprocess.replace(-9999.000000, np.nan)\n    # List of 'NaN' including columns where NaN's actually missing gonna replaced with mode.\n\n    freq_cols = ['wind_bearing', 'wind_speed','uv_index']\n\n    for col in freq_cols:\n        df_preprocess[col].replace(np.nan, df_preprocess[col].mode()[0], inplace=True)\n        \n    ## setting datetime_local as index for upsampling\n    df_preprocess['datetime_local'] = pd.to_datetime(df_preprocess['datetime_local'])\n    df_preprocess['datetime'] = df_preprocess['datetime_local'].copy()\n    df_preprocess.set_index('datetime', inplace=True)\n    # upsampling\n    df_preprocess = df_preprocess.resample('15Min').mean()\n    weather_df_columns = df_preprocess.columns\n    for column in weather_df_columns:\n        df_preprocess[column] = df_preprocess[column].interpolate()\n    \n    # hour from datetime    \n    df_preprocess['datetime_hour'] = pd.DatetimeIndex(df_preprocess.index).hour\n\n    # cyclic features of hour\n    df_preprocess['hour_cos'] = np.cos(2 * np.pi * df_preprocess['datetime_hour']\/max(df_preprocess['datetime_hour']))\n\n    #day and night extraction\n    df_preprocess['day_part'] = df_preprocess['datetime_hour'].apply(daypart)\n\n    # mapping day as 1 and night as 0\n    day_part_map = {\"Day\" : 1, \"Night\": 0}\n    df_preprocess['day_part'] = df_preprocess['day_part'].map(day_part_map).astype('int')\n\n\n    # skewed fixes\n    skewed = ['temperature','wind_bearing', 'wind_speed', 'uv_index']\n\n    # Finding skewness of the numerical features.\n\n    skew_features = np.abs(df_preprocess[skewed].apply(lambda x: skew(x)).sort_values(\n        ascending=False))\n\n    # Filtering skewed features.\n\n    high_skew = skew_features[skew_features > 0.3]\n\n    # Taking indexes of high skew.\n\n    skew_index = high_skew.index\n\n    # Applying boxcox transformation to fix skewness.\n\n    for i in skew_index:\n        df_preprocess[i] = boxcox1p(df_preprocess[i], boxcox_normmax(df_preprocess[i] + 1))\n\n    # # Features to drop:\n    to_drop = ['datetime_hour']\n\n    # Dropping features.\n    df_preprocess.drop(columns=to_drop, inplace=True)\n    \n\n    \n    return df_preprocess","ccbc40ce":"df_forecast = preprocess(weather_forecast_df) # preprocess\ndf_forecast.head()","df0cdd21":"df_forecast = df_forecast.reset_index()","4b62b910":"df_forecast.head()","e85ec269":"df_forecast_clean = df_forecast[['temperature','wind_bearing','wind_speed','uv_index','hour_cos','day_part']]","2193a2c7":"df_forecast_clean.head()","b651dba3":"def xgboost_predict(X):\n    return xgb_model_full_data.predict(X)","c937481c":"# Inversing log10(x+1) scaled sale price predictions and additional feature day_part is multiplied\ndf_forecast['power'] = 10**(xgboost_predict(df_forecast_clean.values))*(df_forecast_clean['day_part'])\n# Defining outlier quartile ranges\nq1 = df_forecast['power'].quantile(0.0050)\nq2 = df_forecast['power'].quantile(0.99)\n\n# Applying weights to outlier ranges to smooth them\ndf_forecast['power'] = df_forecast['power'].apply(lambda x: x if x > q1 else x * 0.77)\ndf_forecast['power'] = df_forecast['power'].apply(lambda x: x if x < q2 else x * 1.1)","3510a345":"submission = df_forecast[['datetime','power']]\nsubmission.to_csv('power_prediction.csv', index=False)\nprint('Save submission', datetime.now(),)\nsubmission.head()","9916680c":"Repalce values for '2018-07-29' to '2018-08-19' with power data from '2019-01-1' to '2019-01-22'","a1cf78b4":"## saving predictions to submission.csv","df6f12fb":"## After replacing outliers and zero values we will now plot time series for power once again","b12d2dfc":"## Columns icon and summary are categorical columns and contain summary of the cloud cover and precip_probability and visibility hence can be dropped","bfc2e006":"## Let us now analyze and clean the power_df","8d234bc6":"## Plotting time series scatter plot for power","2cb82e42":"## Here we are getting root mean sqaured error of 0.10 for never seen samples using xgboost.","c89e7105":"Now combine power_df and weather_df on datetime column (innerjoin)","c4bc0fb1":"## Weekwise resampling shows that outliers lie between the dates '2019-07-13' to '2019-08-06'\n\n## Replace values for power between 2019-07-13 to 2019-08-06 with power values for dates 2018-07-13 to 2018-08-06\n","c0e279fd":"## Transforming the Data\nSome of the continuous values are not distributed evenly and not fitting on normal distribution, we can fix them by using couple transformation approaches. We're going to use boxcox here","9d3f99ff":"## We can clearly see that in July 2019 we have mean power generation of 602.85 which represents most of the outliers in our data.","4253d8ec":"Repalce values for '2018-02-25' to '2018-03-18' with power data from '2019-02-25' to '2019-03-18'","a22df73a":"## Numeric Data analysis","e2c8037d":"### Fixing missing values\n\n* First we will drop the columns where there are a lot of NaN values and the column does not provide additional information\n* There are some actual missing data, by checking general trends of these features we can fill them with most frequent value(with mode).\n* Upon mannual inspection it can be observed that precip_type has a lot of missing values beacause the precip_type is rain if precip_probability > 0 so we can safely drop precip_type without any worries as it does not contain any useful and unique information\n\n","6398678d":"## After replacing almost all missing data we can plot correlation of the clean weather_df","58939c65":"### In **weather_df** each observation is 1 hr apart from the previous observation but in **power_df** each observation is 15 minute after previous observation. So we need to increase the number of observations in the weather_df using a technique called interpolation   \n\n","9a75baaa":"### Observations\n\n* almost 50% of the data from power column has value zero.\n* standard devaition for \"power\" column is almost 500 while averge is 28 which indicates severe outliers. This can happen due to various reasons (sensor failure, wrong unit conversions). We will replace these outliers with more suitable numbers so that there is minumum standard deviation in the data. Even the average is biased due to some of these high values","d709ad32":"## Analysis Time!\n\n* There is strong relationship between time of day and power generation\n* There are some outliers in power column which made standard deviation for that column too large compared to the average value\n* apparent_temperature and temperature in weather_df are strongly correlated\n* Cloud cover is inversely proportional to the power production given day time\n* There are a lot of outliers in the weather_df which can be recognised by high standard deviation values\n* Some of the columns of weather_df do not provide any information since they have standard deviation of zero","1dc3e43c":"## Here we are using 4 fold valdation technique with train data and we are getting  root mean square train_error of about 0.09 and root mean square test_error of about 0.1. ","5fb72b4a":"## Data cleaning and feature engineering","cd2dec10":"## Now lets train our model on entire data set and predict power generation for given weather forecast","df024ad4":"### Checking New Features","8c984843":"## Top 5 features with missing values are precip_type, uv_index, precip_probability, precip_intensity and ozone ","e673c077":"## Encoding Cyclical Features\nA common method for encoding cyclical data is to transform the data into two dimensions using a sine and consine transformation.\n\nWe can do that using the following transformations:\n* $x_{\\cos} = \\cos \\Big(\\frac{2\\pi x}{\\max(x)}\\Big)$\n\nLet's do this for our hourly data:","9027c1a8":"After log transformation\n","dbeb70b1":"### From the graphs we can see some interesting relationships between power generation and various features\n\n1. Cloud cover is inversely proportional to power (duh!)\n1. Power is directly proportional to temperature. (makes sense as day temperatures are higher than night tempratures)\n1. Humidity is inversely correlated to temperature implies power inversely proportional to humidity ( Negative slope in regression)\n1. Wind speed directly proportional to power (interesting)\n1. uv index proportional to power (kind of makes sense most of uv rays reaching upto solar panels)\n1. precipitation probability and precipitation intensity inversely proportional to power (cloud cover -> precipitation -> less power)\n","f5c9221a":"### Droping feautures\nHere we dropping some unnecessary features had their use in feature engineering or not needed at all","eb3b369c":"## Creating New Features","d70f6f84":"## These spikes are due to some very large outliers in the month of august 2019 after replacing them we can observe more accurate time series for power ","64e71d48":"### There are some zero values in month of august and march 2018 we can replace these zero values with non zero values from 2019\n","a2cd11a4":"## As we have previously deduced we can see that temperature and apparent_temperature are perfectly correlated hence we can drop one of these values and still maintain all the relevant information\n"}}