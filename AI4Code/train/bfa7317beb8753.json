{"cell_type":{"a09e5a13":"code","9582db89":"code","3acadd4e":"code","fc3b3c18":"code","b72aa69b":"code","b92afb88":"code","7d046c79":"code","32d3a835":"code","f84a5cf2":"code","b423ca08":"code","72d5121e":"code","4ec6ec8f":"code","5c433c3d":"code","d6ea2ef1":"code","a3bea739":"code","b4b66dc3":"code","e1a5b526":"code","ac1012a2":"code","6f54b4b4":"code","c07cdc3e":"code","ee4f5356":"code","4f3cdfc6":"code","f5d733d3":"code","6c599529":"code","ade96795":"code","4e8e2003":"code","a75c16b1":"code","08f3b230":"code","e8c85088":"code","d12da277":"code","6e31b8c1":"code","b6011d23":"code","bdbfe1d6":"code","871fec4d":"code","39b0a027":"code","ecc8cb2a":"code","a9c2a74d":"code","f50de88b":"markdown","7f90b43f":"markdown","2697c0d6":"markdown","ab424896":"markdown","77c95245":"markdown","96e29a63":"markdown","4a385875":"markdown","e3af8cfb":"markdown","ea4033f9":"markdown","56892727":"markdown","b9a04377":"markdown","3ed96877":"markdown","25b5925f":"markdown"},"source":{"a09e5a13":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.\n\\\\","9582db89":"dt=pd.read_csv('..\/input\/creditcard.csv')","3acadd4e":"dt[dt['Class']==1].head()","fc3b3c18":"import matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline","b72aa69b":"plt.figure(figsize=(12,8))\ng=plt.hist(dt['Amount'],bins=1000)\nplt.xlabel('Amount')\nplt.xlim(-100,1000)","b92afb88":"fig,axes=plt.subplots(1,2,figsize=(12,6))\n#plt.figure(figsize=(12,8))\n# g=sns.distplot(dt['Amount'],bins=100)\nplt.xlim(0,1000)\nax1=sns.distplot(dt[dt['Class']==1]['Amount'],bins=100,ax=axes[1],label='1')\nax1.set_title('1')\n#ax1.xlim(0,1000)\nax2=sns.distplot(dt[dt['Class']==0]['Amount'],bins=100,ax=axes[0],label='0')\nax2.set_title('0')\n#ax2.xlim(0,1000)\nax2.set(xlim=(0,10000))","7d046c79":"dt.info()","32d3a835":"dt.describe()","f84a5cf2":"#let us look at some scatter plots\nsns.scatterplot('V1','V5',data=dt,hue='Class')","b423ca08":"sns.scatterplot('Time','Amount',data=dt,hue='Class')","72d5121e":"sns.scatterplot('Time','V1',data=dt,hue='Class')","4ec6ec8f":"sns.scatterplot('V1','V2',data=dt,hue='Class')\n#The failure points are following some path in the high dimensional space","5c433c3d":"#We have over 2 lakh observations of which 492 are fraudulent cases.Hence this data set may be of a problem.\ndt['Class'].value_counts()\nlen(dt.columns)","d6ea2ef1":"#let us fit a simple model say logistic regression and check the accuracy\n\nfrom sklearn.model_selection import train_test_split\nx_train,x_test,y_train,y_test=train_test_split(dt.iloc[:,0:30],dt.iloc[:,30],test_size=0.3,random_state=42)\n\n\nfrom sklearn.linear_model import LogisticRegression\nmodel=LogisticRegression(class_weight='balanced')\nmodel_fit=model.fit(x_train,y_train)\nmodel_predict=model.predict(x_test)","a3bea739":"from sklearn.metrics import classification_report,confusion_matrix,accuracy_score\nprint('accuracy',accuracy_score(y_test,model_predict))\nprint('\\n classification report')\nprint(classification_report(y_test,model_predict))\nprint('\\n confusion matrix')\nprint(confusion_matrix(y_test,model_predict))\n\n#When the data set is unbalanced we should use precision recall curve and not roc curve.We are more interested in \n#in knowing if our model has predicted the positives,which are a minority, correctly.PR does not \n# account for true negatives (as TN is not a component of either Precision or Recall), and doesn't highlight how many\n# negitives did we predict correctly.PR lays empahsis on how many true positives did we predict correctly.Therefore when we \n#have class imbalance we look at PR curve.\n\nfrom sklearn.metrics import precision_recall_curve\nprecision,recall,_=precision_recall_curve(y_test,model_predict)\n\nplt.figure(figsize=(12,8))\nplt.plot(precision,recall)\nplt.xlabel('recall')\nplt.ylabel('precision')\n\nfrom sklearn.metrics import average_precision_score\naverage_precision = average_precision_score(y_test, model_predict)\nprint('average precision score',average_precision)\n","b4b66dc3":"#Now let us start with the data based approach\nprint('len(dt)',len(dt))\nprint('len(dt) class =1:',len(dt[dt['Class']==1]))\n","e1a5b526":"from sklearn.model_selection import train_test_split\nx_train,x_test,y_train,y_test=train_test_split(dt.iloc[:,0:30],dt.iloc[:,30],test_size=0.3,random_state=42)\ntrain=pd.concat([x_train,y_train],axis=1)\n\n\nno_rare=len(train[train['Class']==1])\nindices_common=train[train['Class']==0].index\n#Randomly select no_rare number of common events\nrandom_selection_of_common_events=np.random.choice(indices_common,no_rare,replace=False,)\n\n#concat the randomly selected common events with rare events\nunder_sampled_indices=np.concatenate([train[train['Class']==1].index,random_selection_of_common_events])\n\ndf1=train.loc[under_sampled_indices]\ndf1.head()","ac1012a2":"from sklearn.linear_model import LogisticRegression\nmodel=LogisticRegression()\nmodel_fit=model.fit(df1.iloc[:,:30],df1.iloc[:,30])\nmodel_predict=model.predict(x_test)\n\nfrom sklearn.metrics import classification_report,confusion_matrix,accuracy_score\nprint('accuracy',accuracy_score(y_test,model_predict))\nprint('\\n classification report')\nprint(classification_report(y_test,model_predict))\nprint('\\n confusion matrix')\n\nprint(confusion_matrix(y_test,model_predict))\nfrom sklearn.metrics import roc_curve,roc_auc_score\nfpr,tpr,thresholds=roc_curve(y_test,model_predict)\nscore_auc=roc_auc_score(y_test,model_predict)\nprint('score_auc',score_auc)\n\nplt.figure(figsize=(12,8))\nplt.plot(fpr,tpr)\nplt.xlabel('fpr')\nplt.ylabel('Tpr')","6f54b4b4":"acc=[]\ncm=np.zeros((2,2))\npre=[]\nauc_sc=[]\nav_pre=[]\ndef S(train,x_test,y_test,n):\n    cm=np.zeros((2,2))\n    \n    no_rare=len(train[train['Class']==1])\n    indices_common=train[train['Class']==0].index\n    #Randomly select no_rare number of common events\n    for i in range(n):\n        random_selection_of_common_events=np.random.choice(indices_common,no_rare,replace=False,)\n\n        #concat the randomly selected common events with rare events\n        under_sampled_indices=np.concatenate([train[train['Class']==1].index,random_selection_of_common_events])\n\n        df1=train.loc[under_sampled_indices]\n\n        model=LogisticRegression()\n        model_fit=model.fit(df1.iloc[:,:30],df1.iloc[:,30])\n        model_predict=model.predict(x_test)\n        pre.append(model_predict)\n        acc.append(accuracy_score(y_test,model_predict))\n        cm=cm+np.array(confusion_matrix(y_test,model_predict))\n        score_auc=roc_auc_score(y_test,model_predict)\n        auc_sc.append(score_auc)\n        average_precision = average_precision_score(y_test, model_predict)\n        av_pre.append(average_precision)\n        \n        #cm=confusion matrix\n    return(np.mean(acc),np.mean(auc_sc),np.mean(av_pre),cm)\na,b,c,d=S(train,x_test,y_test,100)\n    \n   \n    \n    \n\n\n","c07cdc3e":"cm=d\/100\ncm=cm.astype(int)\nprint('confusion matrix\\n',cm)","ee4f5356":"rare_events_indices=list(train[train['Class']==1].index)\n#rare_events=train[train['Class']==1]\n#Repeatedly sample rare events\n#print(rare_events_indices)\nrare=[]\nfor i in range(300):\n    rare.append(rare_events_indices)\nrare=np.asarray(rare)\nrare=list(rare.reshape(-1,1))\nrare=np.squeeze(rare)\nprint(len(rare))\n\nrare_events_over_sampled=train.loc[rare]\nrare_events_over_sampled.head()\n\n#concat the randomly selected common events with rare events\nover_sampled_data=np.concatenate([train[train['Class']==0],rare_events_over_sampled])\nover_sampled_data=pd.DataFrame(over_sampled_data,columns=rare_events_over_sampled.columns)\ndf1=over_sampled_data\n","4f3cdfc6":"from sklearn.linear_model import LogisticRegression\nmodel=LogisticRegression()\nmodel_fit=model.fit(df1.iloc[:,:30],df1.iloc[:,30])\nmodel_predict=model.predict(x_test)\n\nfrom sklearn.metrics import classification_report,confusion_matrix,accuracy_score\nprint('accuracy',accuracy_score(y_test,model_predict))\nprint('\\n classification report')\nprint(classification_report(y_test,model_predict))\nprint('\\n confusion matrix')\n\nprint(confusion_matrix(y_test,model_predict))","f5d733d3":"from imblearn.over_sampling import SMOTE\nsm=SMOTE()\nrare_events_indices=list(train[train['Class']==1].index)\nrare_events=train[train['Class']==1]\n#Repeatedly sample rare events\n#print(rare_events_indices)\nx_train,y_train=sm.fit_resample(train.iloc[:,:30],train.iloc[:,30])\nprint(len(y_train[y_train==0]))\nfrom sklearn.linear_model import LogisticRegression\nmodel=LogisticRegression()\nmodel_fit=model.fit(df1.iloc[:,:30],df1.iloc[:,30])\nmodel_predict=model.predict(x_test)\n\nfrom sklearn.metrics import classification_report,confusion_matrix,accuracy_score\nprint('accuracy',accuracy_score(y_test,model_predict))\nprint('\\n classification report')\nprint(classification_report(y_test,model_predict))\nprint('\\n confusion matrix')\n\nprint(confusion_matrix(y_test,model_predict))","6c599529":"from sklearn.metrics import roc_curve,roc_auc_score\nfpr,tpr,thresholds=roc_curve(y_test,model_predict)\nscore_auc=roc_auc_score(y_test,model_predict)\nprint('score_auc',score_auc)","ade96795":"plt.figure(figsize=(12,8))\nplt.plot(fpr,tpr)\nplt.xlabel('fpr')\nplt.ylabel('Tpr')","4e8e2003":"from sklearn.metrics import precision_recall_curve\nprecision,recall,_=precision_recall_curve(y_test,model_predict)\n# score_auc=roc_auc_score(y_test,model_predict)\n# print('score_auc',score_auc)\n\nplt.figure(figsize=(12,8))\nplt.plot(precision,recall)\nplt.xlabel('recall')\nplt.ylabel('precision')\n\nfrom sklearn.metrics import average_precision_score\naverage_precision = average_precision_score(y_test, model_predict)\nprint('average precision score',average_precision)\n\n","a75c16b1":"#When to use precision recall curve.Why did undersampling perform better?","08f3b230":"#Okay now let us look at how different models perform in the 3 cases.\n#Then use cross validation\n#then use neural nets","e8c85088":"#Bagging\nfrom sklearn.ensemble import BaggingClassifier\nclf=BaggingClassifier()\n\nfrom sklearn.model_selection import ParameterGrid\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import SVC\n#grid = ParameterGrid({\"max_samples\": [0.5, 1.0],\n                      #\"max_features\": [1,2, 10]})\n# base_estimator =[LogisticRegression(),\n#                        DecisionTreeClassifier(),\n#                        KNeighborsClassifier(),\n#                        SVC()]\n\n\nclf=BaggingClassifier(base_estimator=LogisticRegression(), random_state=0,bootstrap=True,bootstrap_features=False)\nclf.fit(x_train, y_train)\npred=clf.predict(x_test) \nprint('accuracy',accuracy_score(y_test,pred))\nprint(confusion_matrix(y_test,model_predict))","d12da277":"fpr,tpr,thresholds=roc_curve(y_test,pred)\nscore_auc=roc_auc_score(y_test,pred)\nprint('score_auc',score_auc)","6e31b8c1":"plt.figure(figsize=(12,8))\nplt.plot(fpr,tpr)\nplt.xlabel('fpr')\nplt.ylabel('Tpr')\n","b6011d23":"#Adaboost\nfrom sklearn.ensemble import AdaBoostClassifier\nclf=AdaBoostClassifier(LogisticRegression(),random_state=0,n_estimators=100)\nfit_=clf.fit(x_train,y_train)\npred=clf.predict(x_test)\nprint('accuracy',accuracy_score(y_test,pred))\nprint(confusion_matrix(y_test,model_predict))\n","bdbfe1d6":"fpr,tpr,thresholds=roc_curve(y_test,pred)\nscore_auc=roc_auc_score(y_test,pred)\nprint('score_auc',score_auc)","871fec4d":"plt.figure(figsize=(12,8))\nplt.plot(fpr,tpr)\nplt.xlabel('fpr')\nplt.ylabel('Tpr')","39b0a027":"#XG Boost\nfrom xgboost import XGBClassifier\nclf=XGBClassifier(reg_alpha=0.8)\nfit_=clf.fit(x_train,y_train)\npred=clf.predict(x_test)\nprint('accuracy',accuracy_score(y_test,pred))\nprint(confusion_matrix(y_test,model_predict))\n\n","ecc8cb2a":"fpr,tpr,thresholds=roc_curve(y_test,pred)\nscore_auc=roc_auc_score(y_test,pred)\nprint('score_auc',score_auc)","a9c2a74d":"plt.figure(figsize=(12,8))\nplt.plot(fpr,tpr)\nplt.xlabel('fpr')\nplt.ylabel('Tpr')\n#XG Boost was much faster than all other ML algorithms, and we can see that it gives best performance on test data.","f50de88b":"# fitting a simple model","7f90b43f":"Even though the accuracy was pretty high precision is too bad.\n\nAccuracy is high because the instances which were common were classified correctly.\n\naverage precision score is pathetic.\n","2697c0d6":"## 3)SMOTE:Synthetic minority over sampling technique\nAdvantages\n\n\nMitigates the problem of overfitting caused by random oversampling as synthetic examples are generated rather than replication of instances\nNo loss of useful information\n\nDisadvantages\n\nWhile generating synthetic examples SMOTE does not take into consideration neighboring examples from other classes. This can result in increase in overlapping of classes and can introduce additional noise\nSMOTE is not very effective for high dimensional data","ab424896":"## 2)Random over sampling\nIncrease the instances of rare events by repeatedly sampling them.","77c95245":"What all does an roc curve tell us?\n\nThe area under the ROC curve ( AUC ) is a measure of how well a parameter can distinguish between two diagnostic groups (diseased\/normal).\n\nBeyond AUC, the ROC curve can also help debug a model. By looking at the shape of the ROC curve, we can evaluate what the model is misclassifying. For example, if the bottom left corner of the curve is closer to the random line, it implies that the model is misclassifying at X=0. Whereas, if it is random on the top right, it implies the errors are occurring at X=1. Also, if there are spikes on the curve (as opposed to being smooth), it implies the model is not stable.\n\nRandom line is the 45 degree line\n\n\nTherefore from the graph we can see that the model is not doing a good job in classifying the rare events","96e29a63":"### boosting\nAda boost\n\nXG Boost","4a385875":"### Woah!\nIn our case false positive is a much costly mistake than fasle negitive.Our aim should be to bring down the number 2066.\n\nWhen I first read about undersampling it sounded insane.I did not understand why anyone would try to make predictions using only a part of the information they have, until I came across this post on quora.\nlink:https:\/\/qr.ae\/TW1Kip\n","e3af8cfb":"We can see that the data set we have is highly unbalanced\n\n## What is the problem with unbalanced data sets?   \n\nAns:The problem with imbalanced classes is there may not be sufficient patterns belonging to the minority classes to represent\ntheir distribution.Hence unbalanced data set can be a problem for any classification algorithm.The problem is not with the rarity of the events but with the small number of samples of the rare events.If you have a sample size of 1000 but only 20 events, you have a problem. If you have a sample size of 10,000 with 200 events, you may be OK. If your sample has 100,000 cases with 2000 events, you\u2019re golden.\n\n\n## How do we handle an unbalanced data set?\n\nAns:1)Data based approach:Generate more data representing the minority case to make the data set more balanced.\n\nCan use-Random under sampling, Random over sampling, SMOTE, MSMOTE, Cluster based over sampling etc.\n\nNOTE:The important thing to remember while using a more balanced data set is the estimates are obtained using the posterior probability of the balanced data set and may end up biasing the model.\n\n2)Modifying the classification algorithms to fit the unbalanced data","ea4033f9":"-Smote, bagging,Ada boost, over sampling gave us similar results. \n\n-XGBoost and under sampling gave us similar results.","56892727":"Under sampling performed better than over sampling in terms of number of false negitives.But under sampling performed a lot worse in terms of false positives.","b9a04377":"We need high TP and low FP.\nHence let us take a look at ROC curve","3ed96877":"## Let us now try out algorithmic ensemblem techniques.\n\n1)Bagging based method:to decrease the model\u2019s variance\n\n2)Boosting based method:to decrease the model\u2019s bias\n\n3)Stacking: to increase the predictive force of the classifier\n\n\n\n","25b5925f":"## 1)Random under sampling:\nDecrease the size of the common class.\n\nBut this will result in a loss of a lot of information.The sample chosen by random under sampling may be a biased sample and  will not be an accurate representation of the population. Thereby, resulting in inaccurate results with the actual test data set.\nBut we can try it and see what the results will be "}}