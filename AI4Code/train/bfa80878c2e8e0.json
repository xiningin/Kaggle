{"cell_type":{"26c55c46":"code","dc3da1d6":"code","52cc0bf1":"code","cf40440b":"code","86b1abea":"code","37140ee2":"code","a0ecc054":"code","292a4765":"code","eb613c21":"code","ecd4031c":"code","5003eb98":"code","e3c6745d":"code","51696416":"code","4051820d":"code","b5571238":"code","2e5ff131":"code","d3190a42":"code","5d2e84b4":"code","692603f1":"code","a9a8f0b8":"code","6c5bf328":"code","8695d20d":"markdown","1a05d30a":"markdown","74cb44c1":"markdown","d2b128be":"markdown","c4d5a48a":"markdown","f4ebb91c":"markdown","3eb70764":"markdown","f8599f29":"markdown","cc2b9e60":"markdown","cb5a7c9f":"markdown","94db10c2":"markdown","2fbd092d":"markdown","7807069d":"markdown","d237e7c0":"markdown","57451064":"markdown","bf1f8707":"markdown"},"source":{"26c55c46":"import numpy as np\nimport pandas as pd\n\nrng = np.random.RandomState(123)\ntrain = pd.read_csv(\"..\/input\/hr-analytics-job-change-of-data-scientists\/aug_train.csv\")\ntest = pd.read_csv(\"..\/input\/hr-analytics-job-change-of-data-scientists\/aug_test.csv\")\n\ntrain.head()","dc3da1d6":"test.head()","52cc0bf1":"train.loc[:,train.dtypes == \"object\"] = train.loc[:,train.dtypes == \"object\"].astype(\"category\")\ntrain['target'] = train['target'].astype(\"category\")\n\ntrain.select_dtypes(\"category\").describe()","cf40440b":"train.select_dtypes({\"int64\",\"float64\"}).drop(columns = \"enrollee_id\").describe()","86b1abea":"target_counts = pd.DataFrame({'target': train['target']})\nprint(\"Percent of training set with target = 0: %.3f\" % (len(target_counts[target_counts['target'] == 0]) \/ len(target_counts)) )\nprint(\"Percent of training set with target = 1: %.3f\" % (len(target_counts[target_counts['target'] == 1]) \/ len(target_counts)) )","37140ee2":"pd.DataFrame({'dtype': train.dtypes,\n              'null_count': train.isna().sum()})","a0ecc054":"import matplotlib.pyplot as plt\n\ncat_list = train.select_dtypes(\"category\").columns.drop('target').tolist()\nfor i in cat_list:\n    val_counts = pd.DataFrame(train[i].value_counts())\n    plt.figure(figsize = (12, 5))\n    plt.title(i, fontsize = 20)\n    if i == 'city':\n        plt.bar(range(len(val_counts)), val_counts[i], color=\"r\", align=\"center\")\n        plt.xticks(range(len(val_counts)), fontsize = 8)\n    else:\n        plt.bar(val_counts.index, val_counts[i], color=\"r\", align=\"center\")\n        plt.xticks(val_counts.index, fontsize = 12)\n    plt.show()","292a4765":"for i in cat_list:\n    unique_vals = pd.Series(train[i].unique())\n    plot_df = pd.DataFrame()\n    plt.figure(figsize = (12, 5))\n    plt.title(i, fontsize = 20)\n    for j in unique_vals:\n        width = .35\n        unique_df = pd.DataFrame(train[train[i] == j])\n        t0 = len(unique_df[unique_df['target'] == 0])\n        t1 = len(unique_df[unique_df['target'] == 1])\n        new = pd.DataFrame({'j' : j, 't0' : t0, 't1' : t1}, index = [0])\n        plot_df = plot_df.append(new)\n    plot_df = plot_df.reset_index(drop = True)\n    plot_df = plot_df.dropna(axis = 0)\n    if i == 'city':\n        plt.bar(range(len(unique_vals)), plot_df['t0'], color=\"b\", align=\"center\")\n        plt.bar(range(len(unique_vals)), plot_df['t1'], color=\"r\", align=\"center\")\n    else:\n        plt.bar(plot_df['j'], plot_df['t0'], color=\"b\", align=\"center\")\n        plt.bar(plot_df['j'], plot_df['t1'], color=\"r\", align=\"center\")\n    plt.show()","eb613c21":"from scipy.stats import chi2_contingency\n\nfor i in cat_list:\n    table = pd.crosstab(train[i], train['target'])\n    chi2_stat, p, dof, expected = chi2_contingency(table)\n    print(\"Variable [\",i,\"] has a chi2 statistic of [%.2f] with a p-value of [%.3f]\" % (chi2_stat, p))","ecd4031c":"from numpy import mean\n\nplt.figure(figsize = (12,5))\nplt.title(\"City Development Index\")\nx1 = train[train['target'] == 0].loc[:,'city_development_index']\nx2 = train[train['target'] == 1].loc[:,'city_development_index']\ny1 = train[train['target'] == 0].loc[:,'target']\ny2 = train[train['target'] == 1].loc[:,'target']\nplt.scatter(x1, y1, color = 'b')\nplt.scatter(x2, y2, color = 'r')\nplt.show()\n\nplt.figure(figsize = (12,5))\nplt.title(\"Mean CDI by Target\")\ncdi_df = train.loc[:,['target','city_development_index']].groupby('target').agg({'city_development_index': 'mean'})\nplt.bar(cdi_df.index, cdi_df['city_development_index'], color = ['b','r'])\nplt.ylim(.7,.9)\nplt.show()","5003eb98":"plt.figure(figsize = (12,5))\nplt.title(\"Training Hours\")\nx1 = train[train['target'] == 0].loc[:,'training_hours']\nx2 = train[train['target'] == 1].loc[:,'training_hours']\ny1 = train[train['target'] == 0].loc[:,'target']\ny2 = train[train['target'] == 1].loc[:,'target']\nplt.scatter(x1, y1, color = 'b')\nplt.scatter(x2, y2, color = 'r')\nplt.show()\n\nplt.figure(figsize = (12,5))\nplt.title(\"Mean Training Hours by Target\")\nth_df = train.loc[:,['target','training_hours']].groupby('target').agg({'training_hours': 'mean'})\nplt.bar(th_df.index, th_df['training_hours'], color = ['b','r'])\nplt.ylim(60,68)\nplt.show()","e3c6745d":"from scipy.stats import ttest_ind\n\ncdi0 = train[train['target'] == 0].loc[:,'city_development_index']\ncdi1 = train[train['target'] == 1].loc[:,'city_development_index']\ncdi_ttest = ttest_ind(cdi0, cdi1)\n\n\nth0 = train[train['target'] == 0].loc[:,'training_hours']\nth1 = train[train['target'] == 1].loc[:,'training_hours']\nth_ttest = ttest_ind(th0, th1)\n\n\nprint(\"t-test of CDI between target categories = %.3f with p-value = %.3f\" % (cdi_ttest.statistic, cdi_ttest.pvalue))\nprint(\"t-test of training hours between target categories = %.3f with p-value = %.3f\" % (th_ttest.statistic, th_ttest.pvalue))","51696416":"from sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import minmax_scale\n\nX_train = train.drop(['enrollee_id', 'target', 'city'], axis = 1)\ny_train = train['target']\n\nimputer = SimpleImputer(strategy = 'most_frequent')\n\nX_train_cols = X_train.columns\nX_train = imputer.fit_transform(X_train)\nX_train = pd.DataFrame(X_train, columns = X_train_cols)\nX_train.training_hours = minmax_scale(X_train.training_hours)\nX_train.city_development_index = X_train.city_development_index.astype('float64')\nX_train = pd.get_dummies(X_train)\nX_train_cols = X_train.columns\nX_train","4051820d":"from sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import cross_val_score\n\nrf = RandomForestClassifier(random_state = rng, class_weight = \"balanced_subsample\").fit(X_train, y_train)\n\ncrossval_acc = cross_val_score(rf, X_train, y_train, scoring = 'accuracy')\ncrossval_prec = cross_val_score(rf, X_train, y_train, scoring = 'precision')\ncrossval_rec = cross_val_score(rf, X_train, y_train, scoring = 'recall')\ncrossval_f1 = cross_val_score(rf, X_train, y_train, scoring = 'f1_macro')\ncrossval_auroc = cross_val_score(rf, X_train, y_train, scoring = 'roc_auc')\n\nprint(\"%.3f cross validated mean accuracy, with a std of %0.3f\" % (crossval_acc.mean(), crossval_acc.std()))\nprint(\"%.3f cross validated mean precision, with a std of %0.3f\" % (crossval_prec.mean(), crossval_prec.std()))\nprint(\"%.3f cross validated mean recall, with a std of %0.3f\" % (crossval_rec.mean(), crossval_rec.std()))\nprint(\"%.3f cross validated mean F1 score, with a std of %0.3f\" % (crossval_f1.mean(), crossval_f1.std()))\nprint(\"%.3f cross validated mean AUROC, with a std of %0.3f\" % (crossval_auroc.mean(), crossval_auroc.std()))","b5571238":"# from sklearn.model_selection import GridSearchCV\n\n# search = {'max_features': ['auto', 'sqrt'],\n#           'n_estimators': [100, 200],\n#           'oob_score': [True, False]}\n\n# rf_ht = GridSearchCV(rf, search, scoring = 'roc_auc').fit(X_train, y_train)\n\n# print(rf_ht.best_estimator_)\n# print(rf_ht.best_params_)","2e5ff131":"# RandomForestClassifier(class_weight='balanced_subsample', n_estimators=200,\n#                        oob_score=True,\n#                        random_state=RandomState(MT19937) at 0x7F0606FD3160)\n# {'max_features': 'auto', 'n_estimators': 200, 'oob_score': True}","d3190a42":"rf_final = RandomForestClassifier(max_features = 'auto', n_estimators = 200, oob_score = True,\n                                  random_state = rng, class_weight = \"balanced_subsample\").fit(X_train, y_train)\n\ncrossval_acc = cross_val_score(rf_final, X_train, y_train, scoring = 'accuracy')\ncrossval_prec = cross_val_score(rf_final, X_train, y_train, scoring = 'precision')\ncrossval_rec = cross_val_score(rf_final, X_train, y_train, scoring = 'recall')\ncrossval_f1 = cross_val_score(rf_final, X_train, y_train, scoring = 'f1_macro')\ncrossval_auroc = cross_val_score(rf_final, X_train, y_train, scoring = 'roc_auc')\n\nprint(\"%.3f cross validated mean accuracy, with a std of %0.3f\" % (crossval_acc.mean(), crossval_acc.std()))\nprint(\"%.3f cross validated mean precision, with a std of %0.3f\" % (crossval_prec.mean(), crossval_prec.std()))\nprint(\"%.3f cross validated mean recall, with a std of %0.3f\" % (crossval_rec.mean(), crossval_rec.std()))\nprint(\"%.3f cross validated mean F1 score, with a std of %0.3f\" % (crossval_f1.mean(), crossval_f1.std()))\nprint(\"%.3f cross validated mean AUROC, with a std of %0.3f\" % (crossval_auroc.mean(), crossval_auroc.std()))","5d2e84b4":"feat_imp = rf_final.feature_importances_\nindices = np.argsort(feat_imp)[::-1]\nfeat_df = pd.DataFrame({'importances': feat_imp[indices],\n                        'labels' : X_train_cols})\nplt.figure(figsize = (20,10))\nplt.title(\"Feature Importances\")\nplt.bar(feat_df.loc[0:5,'labels'], feat_df.loc[0:5,'importances'], color = 'r')\nplt.show()","692603f1":"feat_df[0:25]","a9a8f0b8":"X_test = test.drop(['enrollee_id', 'city'], axis = 1)\nX_test_cols = X_test.columns\nX_test = imputer.fit_transform(X_test)\nX_test = pd.DataFrame(X_test, columns = X_test_cols)\nX_test.training_hours = minmax_scale(X_test.training_hours)\nX_test.city_development_index = X_test.city_development_index.astype('float64')\nX_test = pd.get_dummies(X_test)\nX_test_cols = X_test.columns\nX_test","6c5bf328":"en_id = test['enrollee_id']\nprobs = rf.predict_proba(X_test)\nprobs = probs[:,1]\nsubmission = pd.DataFrame({'enrollee_id' : en_id,\n                         'target' : probs})\nsubmission.to_csv(\"submission.csv\")\nsubmission","8695d20d":"It seems that within each categorical variable, the levels of the variable differ in terms of the proportion of new job seekers versus those who want to remain at their current job. Let's use a chi-squared test to further examine the relationship between each feature and the target variable. The test examines how different the observed counts than what is expected, under the assumption that they are independent.","1a05d30a":"# **Predicting Job Change using Python**\n\nThis notebook covers how to use Python machine learning libraries to construct a predictive model from HR data. The dataset is from the Kaggle task \"HR Analytics: Job Change of Data Scientists\" provided by user M\u00f6bius (https:\/\/www.kaggle.com\/arashnic\/hr-analytics-job-change-of-data-scientists)\n\nFirst, we'll take a look at the two datasets. We'll load up some useful libraries, and create dataframes from the .csv files provided. We'll also set a RandomState for reproducibility.","74cb44c1":"# Closing Remarks\n\nTo finish, we'll apply the sample preprocessing pipeline to the test data and generate our predicted classes, thus completing the assigned task. As with any machine learning model, this could be improved upon further by exploring other techniques not covered here, such as:\n\n* Under\/oversampling (e.g., SMOTE)\n* Exploring other models (logistic regression, kNN, multilayer perceptron)\n* More advanced missing data imputation (e.g., multivariate techniques, model-based techniques as with kNNImputer)\n* Permutation-based feature importances \n\n...and much more!","d2b128be":"This is a good start. However, our AUROC metric (which will be the final performance metric for the task) is a little on the low side at .75. Furthermore, looking at the precision and recall scores tells us the model tends to miss people who are actually seeking new jobs. In practice, it's likely that we are mostly interested in correctly identifying new job seekers, so the low mean recall is especially troubling.\n\nWe can try to improve the performance of the model by performing hyperparameter tuning. We'll try just a short list of values to see if the model improves. Since this can take a long time, the output from a previous run is shown with code commented out.","c4d5a48a":"It looks like CDI may be more related to the target variable than training hours is. Now that we've gotten to know our data a little bit, let's move on to preprocessing and model fitting.","f4ebb91c":"It appears as though the city development index, followed by the number of training hours, are the variables the model heavily relied on to distinguish between non-job seekers from job seekers. However, the default feature importance metrics may be skewed toward numerical variables and\/or high cardinality categorical features.\n","3eb70764":"# Exploratory Data Analysis\n\nLet's begin by looking at the distribution of our categorical variables.","f8599f29":"Let's also take a look at the relationship between each categorical variable and the outcome. The y-axis indicates a count. In the plots below, <span style=\"color:blue\"> **blue** <\/span> indicates a person is not seeking a new job, whereas <span style=\"color:red\">**red**<\/span> indicates that they are.","cc2b9e60":"We see a slight increase to performance metrics by changing some of the parameters. In practice, we could search across many different values for each parameter and look at new parameters to adjust, but let's stick with these values for now.","cb5a7c9f":"We'll fit a final model using these parameters and examine model performance.","94db10c2":"# Feature Importance\n\nTo finish, we'll get a simple feature importance measure from the model:","2fbd092d":"# Contact Information\n\nMy name is Adrian Abellanoza, M.S. I am a Ph.D candidate in experimental psychology at the University of Texas at Arlington. In addition to being an academic researcher, I am an aspiring data scientist\/analyst with previous internship experience in eCommerce, UX, and business intelligence. Feel free to contact me with questions, comments, or advice! I'd love to hear your input.\n\n* LinkedIn: https:\/\/www.linkedin.com\/in\/aabellanoza\/\n* Email: a.abellanoz93@gmail.com\n* GitHub: https:\/\/github.com\/AAbellanoza","7807069d":"Many of the p-values are small (< .001), which is expected from such a large dataset. Looking at the magnitude of the chi-squared values, we can see that city and experience are strongly related to whether or not a data scientist wants to change jobs. A person's relevent experience and enrollment status also seem to be associated with seeking a new job. However, city is a high cardinality feature. We may do better by dropping this feature altogether and relying on the city_development_index to give us information about someone's city.  \n\nSpeaking of which, let's also examine the continuous variables. We'll plot all values, colored the same way (<span style=\"color:blue\"> **blue** <\/span> = not seeking a new job, <span style=\"color:red\">**red**<\/span> = seeking a new job. We'll also compare the mean values per target class, both visually and with a t-test.","d237e7c0":"# Model Fitting and Hyperparameter Tuning\n\nNow we're ready to fit our model! We'll use a random forest model, as it is a great classifier for mixed data types. We can try to adjust the model for imbalanced classes using weighting.","57451064":"# Data Preprocessing\n\nWe'll begin by preprocessing the data. We have a lot of missing values, which we'll deal with using a simple imputation procedure. It will also be helpful to convert categorical variables to sets of binary variables. Since there are so many cities, and we have a relevant continuous variable measuring a relevant aspect of one's city (city_development_index), we'll drop the feature for now. Training hours and city development index are also both on different scales. To rectify this, we'll rescale training hours to be between 0 and 1.","bf1f8707":"As the documentation mentioned, the attributes of the data may pose some issues while modeling, such as:\n\n1. Imbalanced data set\n2. High cardinality (uniqueness) in some features\n3. Missing data\n\nLet's explore our training set further and do some basic summary\/visualization to explore the data."}}