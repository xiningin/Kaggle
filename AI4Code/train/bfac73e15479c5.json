{"cell_type":{"66d0582d":"code","66e891cf":"code","bd8a31d6":"code","e4ebf49c":"code","aaea36d6":"code","d61135d3":"code","ea64c40c":"code","cf6c7f41":"code","f7b6a8d2":"code","19ca9cbf":"code","39ecb004":"code","b1fc57f8":"code","39492dcd":"code","1c173528":"code","e5056428":"code","638426d4":"code","8cbd8235":"code","8df3362a":"code","84f5bdaf":"code","0906a4bf":"code","09a22dc3":"code","74efd576":"code","d0299dc2":"code","388396ea":"code","76df820c":"code","b8906eb6":"code","d26f08f5":"code","c675f748":"code","d17dbad8":"code","f843f082":"code","3f59ffce":"code","fe6b5dc2":"code","b8f1a5f2":"code","2376b961":"code","7ee347d0":"code","abde2051":"code","64051bac":"code","e0f38965":"code","8b09378f":"code","1dc746a9":"code","c2c4dfea":"code","bb4142f2":"code","206a4ae5":"code","5ea19c3d":"code","b9266a80":"code","a60b398d":"code","1aba1877":"code","c50afd8e":"code","555be4ab":"code","6496eaac":"code","00ac0217":"code","7e47d6f6":"code","ef17dc5a":"code","dfc9e57b":"code","4b5d3539":"code","8f452225":"code","6d17dbcb":"code","1a1646ec":"code","91a23ac7":"code","97338cae":"markdown","ff3f645b":"markdown","072c7389":"markdown","ae1eb309":"markdown","79801565":"markdown","7380aaf2":"markdown","38216bdb":"markdown","ada211ba":"markdown","290bb09b":"markdown","72338895":"markdown","9265fae2":"markdown","fb4ad810":"markdown","7df47f2f":"markdown","6f688149":"markdown","39918797":"markdown","8bb0495a":"markdown","bcd6673b":"markdown","f98297bf":"markdown"},"source":{"66d0582d":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nfrom fbprophet import Prophet\nimport matplotlib.pyplot as plt\nimport math as math\n\n%matplotlib inline","66e891cf":"# Load the data\ntrain = pd.read_csv(\"..\/input\/web-traffic-time-series-forecasting\/train_1.csv.zip\", compression=\"zip\")\nkeys = pd.read_csv(\"..\/input\/web-traffic-time-series-forecasting\/key_1.csv.zip\", compression=\"zip\")\nss = pd.read_csv(\"..\/input\/web-traffic-time-series-forecasting\/sample_submission_1.csv.zip\", compression=\"zip\")","bd8a31d6":"train.head()","e4ebf49c":"# Check the data\nprint(\"Check the number of records\")\nprint(\"Number of records: \", train.shape[0], \"\\n\")\n\nprint(\"Null analysis\")\nempty_sample = train[train.isnull().any(axis=1)]\nprint(\"Number of records contain 1+ null: \", empty_sample.shape[0], \"\\n\")","aaea36d6":"empty_sample.iloc[np.r_[0:10, len(empty_sample)-10:len(empty_sample)]]","d61135d3":"# plot 3 the time series\ndef plot_time_series(df, row_num, start_col =1, ax=None):\n    if ax is None:\n            fig = plt.figure(facecolor='w', figsize=(10, 6))\n            ax = fig.add_subplot(111)\n    else:\n        fig = ax.get_figure()\n        \n    series_title = df.iloc[row_num, 0]\n    sample_series = df.iloc[row_num, start_col:]\n    sample_series.plot(style=\".\", ax=ax)\n    ax.set_title(\"Series: %s\" % series_title)\n\nfig, axs  = plt.subplots(4,1,figsize=(12,12))\nplot_time_series(empty_sample, 1, ax=axs[0])\nplot_time_series(empty_sample, 10, ax=axs[1])\nplot_time_series(empty_sample, 100, ax=axs[2])\nplot_time_series(empty_sample, 1005, ax=axs[3])\n\nplt.tight_layout()","ea64c40c":"# series with all NaN\nempty_sample.iloc[1000:1010]","cf6c7f41":"import re\n\ndef breakdown_topic(str):\n    m = re.search('(.*)\\_(.*).wikipedia.org\\_(.*)\\_(.*)', str)\n    if m is not None:\n        return m.group(1), m.group(2), m.group(3), m.group(4)\n    else:\n        return \"\", \"\", \"\", \"\"\n\nprint(breakdown_topic(\"\u0420\u0443\u0434\u043e\u0432\u0430,_\u041d\u0430\u0442\u0430\u043b\u044c\u044f_\u0410\u043b\u0435\u043a\u0441\u0430\u043d\u0434\u0440\u043e\u0432\u043d\u0430_ru.wikipedia.org_all-access_spider\"))\nprint(breakdown_topic(\"\u53f0\u7063\u707d\u96e3\u5217\u8868_zh.wikipedia.org_all-access_spider\"))\nprint(breakdown_topic(\"File:Memphis_Blues_Tour_2010.jpg_commons.wikimedia.org_mobile-web_all-agents\"))","f7b6a8d2":"page_details = train.Page.str.extract(r'(?P<topic>.*)\\_(?P<lang>.*).wikipedia.org\\_(?P<access>.*)\\_(?P<type>.*)')\n\npage_details[0:10]","19ca9cbf":"unique_topic = page_details[\"topic\"].unique()\nprint(unique_topic)\nprint(\"Number of distinct topics: \", unique_topic.shape[0])","39ecb004":"fig, axs  = plt.subplots(3,1,figsize=(12,12))\n\npage_details[\"lang\"].value_counts().sort_index().plot.bar(ax=axs[0])\naxs[0].set_title('Language - distribution')\n\npage_details[\"access\"].value_counts().sort_index().plot.bar(ax=axs[1])\naxs[1].set_title('Access - distribution')\n\npage_details[\"type\"].value_counts().sort_index().plot.bar(ax=axs[2])\naxs[2].set_title('Type - distribution')\n\nplt.tight_layout()","b1fc57f8":"# Generate train and validate dataset\ntrain_df = pd.concat([page_details, train], axis=1)\n\ndef get_train_validate_set(train_df, test_percent):\n    train_end = math.floor((train_df.shape[1]-5) * (1-test_percent))\n    train_ds = train_df.iloc[:, np.r_[0,1,2,3,4,5:train_end]]\n    test_ds = train_df.iloc[:, np.r_[0,1,2,3,4,train_end:train_df.shape[1]]]\n    \n    return train_ds, test_ds\n\nX_train, y_train = get_train_validate_set(train_df, 0.1)\n\nprint(\"The training set sample:\")\nprint(X_train[0:10])\nprint(\"The validation set sample:\")\nprint(y_train[0:10])","39492dcd":"def extract_series(df, row_num, start_idx):\n    y = df.iloc[row_num, start_idx:]\n    df = pd.DataFrame({ 'ds': y.index, 'y': y.values})\n    return df","1c173528":"def smape(predict, actual, debug=False):\n    '''\n    predict and actual is a panda series.\n    In this implementation I will skip all the datapoint with actual is null\n    '''\n    actual = actual.fillna(0)\n    data = pd.concat([predict, actual], axis=1, keys=['predict', 'actual'])\n    data = data[data.actual.notnull()]\n    if debug:\n        print('debug', data)\n    \n    evals = abs(data.predict - data.actual) * 1.0 \/ (abs(data.predict) + abs(data.actual)) * 2\n    evals[evals.isnull()] = 0\n    #print(np.sum(evals), len(data), np.sum(evals) * 1.0 \/ len(data))\n    \n    result = np.sum(evals) \/ len(data) * 100.0\n    \n    return result\n\n# create testing series\ntesting_series_1 = X_train.iloc[0, 5:494]\ntesting_series_2 = X_train.iloc[0, 5:494].shift(-1)\ntesting_series_3 = X_train.iloc[1, 5:494]\ntesting_series_4 = pd.Series([0,0,0,0])","e5056428":"testing_series_1","638426d4":"np.repeat(3, 500)","8cbd8235":"random_series_1 = pd.Series(np.repeat(3, 500))\nrandom_series_2 = pd.Series(np.random.normal(3, 1, 500))\nrandom_series_3 = pd.Series(np.random.normal(500, 20, 500))\nrandom_series_4 = pd.Series(np.repeat(500, 500))\n\n# testing 1 same series\nprint(\"\\nSMAPE score to predict a constant array of 3\")\nprint(\"Score (same series): %.3f\" % smape(random_series_1, random_series_1))\nprint(\"Score (same series - 1) %.3f\" % smape(random_series_1, random_series_1-1))\nprint(\"Score (same series + 1) %.3f\" % smape(random_series_1, random_series_1+1))\n\n# testing 2 same series shift by one\nprint(\"\\nSMAPE score to predict a array of normal distribution around 3\")\nprint(\"Score (random vs mean) %.3f\" % smape(random_series_2, random_series_1))\nprint(\"Score (random vs mean-1) %.3f\" % smape(random_series_2, random_series_2-1))\nprint(\"Score (random vs mean+1) %.3f\" % smape(random_series_2, random_series_2+1))\nprint(\"Score (random vs mean*0.9) %.3f\" % smape(random_series_2, random_series_2*0.9))\nprint(\"Score (random vs mean*1.1) %.3f\" % smape(random_series_2, random_series_2*1.1))\n\n# testing 3 totally different series\nprint(\"\\nSMAPE score to predict a array of normal distribution around 500\")\nprint(\"Score (random vs mean) %.3f\" % smape(random_series_3, random_series_4))\nprint(\"Score (random vs mean-20) %.3f\" % smape(random_series_3, random_series_3-20))\nprint(\"Score (random vs mean+20) %.3f\" % smape(random_series_3, random_series_3+20))\nprint(\"Score (random vs mean*0.9) %.3f\" % smape(random_series_3, random_series_3*0.9))\nprint(\"Score (random vs mean*1.1) %.3f\" % smape(random_series_3, random_series_3*1.1))","8df3362a":"y_true_1 = pd.Series(np.random.normal(1, 1, 500))\ny_true_2 = pd.Series(np.random.normal(2, 1, 500))\ny_true_3 = pd.Series(np.random.normal(3, 1, 500))\ny_pred = pd.Series(np.ones(500))\nx = np.linspace(0,10,1000)\nres_1 = list([smape(y_true_1, i * y_pred) for i in x])\nres_2 = list([smape(y_true_2, i * y_pred) for i in x])\nres_3 = list([smape(y_true_3, i * y_pred) for i in x])\nplt.plot(x, res_1, color='b')\nplt.plot(x, res_2, color='r')\nplt.plot(x, res_3, color='g')\nplt.axvline(x=1, color='k')\nplt.axvline(x=2, color='k')\nplt.axvline(x=3, color='k')","84f5bdaf":"def plot_prediction_and_actual_2(train, forecast, actual, xlim=None, ylim=None, figSize=None, title=None):\n    fig, ax  = plt.subplots(1,1,figsize=figSize)\n    ax.plot(pd.to_datetime(train.index), train.values, 'k.')\n    ax.plot(pd.to_datetime(actual.index), actual.values, 'r.')\n    ax.plot(pd.to_datetime(forecast.index), forecast.values, 'b-')\n    ax.set_title(title)\n    plt.show()","0906a4bf":"def median_model(df_train, df_actual, p, review=False, figSize=(12, 4)):\n    \n    def nanmedian_zero(a):\n        return np.nan_to_num(np.nanmedian(a))\n    \n    df_train['y'] = df_train['y'].astype('float')\n    df_actual['y'] = df_actual['y'].astype('float')\n    visits = nanmedian_zero(df_train['y'].values[-p:])\n    train_series = df_train['y']\n    train_series.index = df_train.ds\n\n    idx = np.arange(p) + np.arange(len(df_train)- p+1)[:,None]\n    b = [row[row>=0] for row in df_train.y.values[idx]]\n    pre_forecast = pd.Series(np.append(([float('nan')] * (p-1)), list(map(nanmedian_zero,b))))\n    pre_forecast.index = df_train.ds\n    \n    forecast_series = pd.Series(np.repeat(visits, len(df_actual)))\n    forecast_series.index = df_actual.ds\n    \n    forecast_series = pre_forecast.append(forecast_series)\n    \n    actual_series = df_actual.y\n    actual_series.index = df_actual.ds\n    \n    if(review):\n        plot_prediction_and_actual_2(train_series, forecast_series, actual_series, figSize=figSize, title='Median model')\n    \n    return smape(forecast_series, actual_series)","09a22dc3":"df_train = extract_series(X_train, 2, 5)\ndf_actual = extract_series(y_train, 2, 5)","74efd576":"df_train","d0299dc2":"def nanmedian_zero(a):\n    return np.nan_to_num(np.nanmedian(a))","388396ea":"p = 15","76df820c":"df_train['y'] = df_train['y'].astype('int')\ndf_actual['y'] = df_actual['y'].astype('int')\nvisits = nanmedian_zero(df_train['y'].values[-p:])\ntrain_series = df_train['y']\ntrain_series.index = df_train.ds","b8906eb6":"idx = np.arange(p) + np.arange(len(df_train)- p+1).reshape(-1, 1)","d26f08f5":"np.arange(p) + np.arange(len(df_train)- p+1).reshape(-1, 1)","c675f748":"b = [row[row>=0] for row in df_train.y.values[idx]]","d17dbad8":"df_train.y.values[idx]","f843f082":"pd.Series(np.append(([float('nan')] * (p-1)), list(map(nanmedian_zero,b))))","3f59ffce":"pre_forecast = pd.Series(np.append(([float('nan')] * (p-1)), list(map(nanmedian_zero,b))))\npre_forecast.index = df_train.ds","fe6b5dc2":"pre_forecast","b8f1a5f2":"forecast_series = pd.Series(np.repeat(visits, len(df_actual)))\nforecast_series.index = df_actual.ds\n\nforecast_series = pre_forecast.append(forecast_series)","2376b961":"forecast_series","7ee347d0":"actual_series = df_actual.y\nactual_series.index = df_actual.ds","abde2051":"actual_series","64051bac":"# This is to demo the median model\nprint(train.iloc[[2]])\n\ndf_train = extract_series(X_train, 2, 5)\ndf_actual = extract_series(y_train, 2, 5)\nlang = X_train.iloc[2, 1]\nscore = median_model(df_train.copy(), df_actual.copy(), 15, review=True)\nprint(\"The SMAPE score is : %.5f\" % score)","e0f38965":"# holiday variable\n#holiday_en = ['2015-01-01', '2015-01-19', '2015-04-03', '2015-05-04', '2015-05-25', '2015-07-01', '2015-07-03', '2015-09-07', '2015-11-26', '2015-11-27', '2015-12-25', '2015-12-26', '2015-12-28', '2016-01-01', '2016-01-18', '2016-03-25', '2016-05-02', '2016-05-30', '2016-07-01', '2016-07-04', '2016-09-05', '2016-11-11', '2016-11-24', '2016-12-25', '2016-12-26', '2016-12-27', '2017-01-01', '2017-01-02', '2017-01-16', '2017-04-14', '2017-05-01', '2017-05-29', '2017-07-01', '2017-07-03', '2017-07-04', '2017-09-04', '2017-11-10', '2017-11-23', '2017-12-25', '2017-12-26']\n\nholiday_en_us = ['2015-01-01', '2015-01-19', '2015-05-25', '2015-07-03', '2015-09-07', '2015-11-26', '2015-11-27', '2015-12-25', '2016-01-01', '2016-01-18', '2016-05-30', '2016-07-04', '2016-09-05', '2016-11-11', '2016-11-24', '2016-12-26', '2017-01-01', '2017-01-02', '2017-01-16', '2017-05-29', '2017-07-04', '2017-09-04', '2017-11-10', '2017-11-23', '2017-12-25']\nholiday_en_uk = ['2015-01-01', '2015-04-03', '2015-05-04', '2015-05-25', '2015-12-25', '2015-12-26', '2015-12-28', '2016-01-01', '2016-03-25', '2016-05-02', '2016-05-30', '2016-12-26', '2016-12-27', '2017-01-01', '2017-04-14', '2017-05-01', '2017-05-29', '2017-12-25', '2017-12-26']\nholiday_en_canada = ['2015-01-01', '2015-07-01', '2015-09-07', '2015-12-25', '2016-01-01', '2016-07-01', '2016-09-05', '2016-12-25', '2017-01-01', '2017-07-01', '2017-07-03', '2017-09-04', '2017-12-25']\n\nholiday_ru_russia = ['2015-01-01', '2015-01-02', '2015-01-05', '2015-01-06', '2015-01-07', '2015-01-08', '2015-01-09', '2015-02-23', '2015-03-09', '2015-05-01', '2015-05-04', '2015-05-09', '2015-05-11', '2015-06-12', '2015-11-04', '2016-01-01', '2016-01-04', '2016-01-05', '2016-01-06', '2016-01-07', '2016-02-22', '2016-02-23', '2016-03-08', '2016-05-01', '2016-05-09', '2016-06-12', '2016-06-13', '2016-11-04', '2017-01-01', '2017-01-02', '2017-01-03', '2017-01-04', '2017-01-05', '2017-01-06', '2017-01-07', '2017-02-23', '2017-02-24', '2017-03-08', '2017-05-01', '2017-05-08', '2017-05-09', '2017-06-12', '2017-11-04', '2017-11-06']\n#holiday_es = ['2015-01-01', '2015-01-06', '2015-01-12', '2015-02-02', '2015-03-16', '2015-03-23', '2015-04-02', '2015-04-03', '2015-05-01', '2015-05-18', '2015-06-08', '2015-06-15', '2015-06-29', '2015-07-20', '2015-08-07', '2015-08-17', '2015-09-16', '2015-10-12', '2015-11-01', '2015-11-02', '2015-11-16', '2015-12-06', '2015-12-08', '2015-12-12', '2015-12-25', '2016-01-01', '2016-01-06', '2016-01-11', '2016-02-01', '2016-03-21', '2016-03-24', '2016-03-25', '2016-05-01', '2016-05-09', '2016-05-30', '2016-06-06', '2016-07-04', '2016-07-20', '2016-08-07', '2016-08-15', '2016-09-16', '2016-10-12', '2016-10-17', '2016-11-01', '2016-11-02', '2016-11-07', '2016-11-14', '2016-11-21', '2016-12-06', '2016-12-08', '2016-12-12', '2016-12-25', '2016-12-26', '2017-01-01', '2017-01-02', '2017-01-06', '2017-01-09', '2017-02-06', '2017-03-20', '2017-04-13', '2017-04-14', '2017-05-01', '2017-05-29', '2017-06-19', '2017-06-26', '2017-07-03', '2017-07-20', '2017-08-07', '2017-08-15', '2017-09-16', '2017-10-12', '2017-10-16', '2017-11-01', '2017-11-02', '2017-11-06', '2017-11-13', '2017-11-20', '2017-12-06', '2017-12-08', '2017-12-12', '2017-12-25']\n\nholiday_es_mexico = ['2015-01-01', '2015-02-02', '2015-03-16', '2015-04-02', '2015-04-03', '2015-05-01', '2015-09-16', '2015-10-12', '2015-11-02', '2015-11-16', '2015-12-12', '2015-12-25', '2016-01-01', '2016-02-01', '2016-03-21', '2016-03-24', '2016-03-25', '2016-05-01', '2016-09-16', '2016-10-12', '2016-11-02', '2016-11-21', '2016-12-12', '2016-12-25', '2016-12-26', '2017-01-01', '2017-01-02', '2017-02-06', '2017-03-20', '2017-04-13', '2017-04-14', '2017-05-01', '2017-09-16', '2017-10-12', '2017-11-02', '2017-11-20', '2017-12-12', '2017-12-25']\nholiday_es_spain = ['2017-01-01', '2017-01-06', '2017-04-14', '2017-05-01', '2017-08-15', '2017-10-12', '2017-11-01', '2017-12-06', '2017-12-08', '2017-12-25', '2016-01-01', '2016-01-06', '2016-03-25', '2016-05-01', '2016-08-15', '2016-10-12', '2016-11-01', '2016-12-06', '2016-12-08', '2016-12-25', '2015-01-01', '2015-01-06', '2015-04-03', '2015-05-01', '2015-10-12', '2015-11-01', '2015-12-06', '2015-12-08', '2015-12-25']\nholiday_es_colombia = ['2015-01-01', '2015-01-12', '2015-03-23', '2015-04-02', '2015-04-03', '2015-05-01', '2015-05-18', '2015-06-08', '2015-06-15', '2015-06-29', '2015-07-20', '2015-08-07', '2015-08-17', '2015-10-12', '2015-11-02', '2015-11-16', '2015-12-08', '2015-12-25', '2016-01-01', '2016-01-11', '2016-03-21', '2016-03-24', '2016-03-25', '2016-05-01', '2016-05-09', '2016-05-30', '2016-06-06', '2016-07-04', '2016-07-20', '2016-08-07', '2016-08-15', '2016-10-17', '2016-11-07', '2016-11-14', '2016-12-08', '2016-12-25', '2017-01-01', '2017-01-09', '2017-03-20', '2017-04-13', '2017-04-14', '2017-05-01', '2017-05-29', '2017-06-19', '2017-06-26', '2017-07-03', '2017-07-20', '2017-08-07', '2017-08-15', '2017-10-16', '2017-11-06', '2017-11-13', '2017-12-08', '2017-12-25']\n\nholiday_fr_france = ['2015-01-01', '2015-04-06', '2015-05-01', '2015-05-08', '2015-05-14', '2015-05-25', '2015-07-14', '2015-08-15', '2015-11-01', '2015-11-11', '2015-12-25', '2016-01-01', '2016-03-28', '2016-05-01', '2016-05-05', '2016-05-08', '2016-05-16', '2016-07-14', '2016-08-15', '2016-11-01', '2016-11-11', '2016-12-25', '2017-01-01', '2017-04-17', '2017-05-01', '2017-05-08', '2017-05-25', '2017-06-05', '2017-07-14', '2017-08-15', '2017-11-01', '2017-11-11', '2017-12-25']\nholiday_jp_japan = ['2015-01-01', '2015-01-12', '2015-02-11', '2015-03-21', '2015-04-29', '2015-05-03', '2015-05-04', '2015-05-05', '2015-05-06', '2015-07-20', '2015-09-21', '2015-09-22', '2015-09-23', '2015-10-12', '2015-11-03', '2015-11-23', '2015-12-23', '2016-01-01', '2016-01-11', '2016-02-11', '2016-03-21', '2016-04-29', '2016-05-03', '2016-05-04', '2016-05-05', '2016-07-18', '2016-08-11', '2016-09-19', '2016-09-22', '2016-10-10', '2016-11-03', '2016-11-23', '2016-12-23', '2017-01-01', '2017-01-09', '2017-02-11', '2017-03-20', '2017-04-29', '2017-05-03', '2017-05-04', '2017-05-05', '2017-07-17', '2017-08-11', '2017-09-18', '2017-09-22', '2017-10-09', '2017-11-03', '2017-11-23', '2017-12-23']\n\n#holiday_de = ['2015-01-01', '2015-01-06', '2015-04-03', '2015-04-06', '2015-05-01', '2015-05-14', '2015-05-25', '2015-06-04', '2015-08-01', '2015-08-15', '2015-10-03', '2015-10-26', '2015-11-01', '2015-12-08', '2015-12-25', '2015-12-26', '2016-01-01', '2016-01-06', '2016-03-25', '2016-03-28', '2016-05-01', '2016-05-05', '2016-05-16', '2016-05-26', '2016-08-01', '2016-08-15', '2016-10-03', '2016-10-26', '2016-11-01', '2016-12-08', '2016-12-25', '2016-12-26', '2017-01-01', '2017-01-06', '2017-04-14', '2017-04-17', '2017-05-01', '2017-05-25', '2017-06-05', '2017-06-15', '2017-08-01', '2017-08-15', '2017-10-03', '2017-10-26', '2017-10-31', '2017-11-01', '2017-12-08', '2017-12-25', '2017-12-26']\n\nholiday_de_germany = ['2015-01-01', '2015-04-03', '2015-04-06', '2015-05-01', '2015-05-14', '2015-05-14', '2015-05-25', '2015-10-03', '2015-12-25', '2015-12-26', '2016-01-01', '2016-03-25', '2016-03-28', '2016-05-01', '2016-05-05', '2016-05-16', '2016-10-03', '2016-12-25', '2016-12-26', '2017-01-01', '2017-04-14', '2017-04-17', '2017-05-01', '2017-05-25', '2017-06-05', '2017-10-03', '2017-10-31', '2017-12-25', '2017-12-26']\nholiday_de_austria = ['2015-01-01', '2015-01-06', '2015-04-06', '2015-05-01', '2015-05-14', '2015-05-25', '2015-06-04', '2015-08-15', '2015-10-26', '2015-11-01', '2015-12-08', '2015-12-25', '2015-12-26', '2016-01-01', '2016-01-06', '2016-03-28', '2016-05-01', '2016-05-05', '2016-05-16', '2016-05-26', '2016-08-15', '2016-10-26', '2016-11-01', '2016-12-08', '2016-12-25', '2016-12-26', '2017-01-01', '2017-01-06', '2017-04-17', '2017-05-01', '2017-05-25', '2017-06-05', '2017-06-15', '2017-08-15', '2017-10-26', '2017-11-01', '2017-12-08', '2017-12-25', '2017-12-26']\nholiday_de_switzerland = ['2015-01-01', '2015-04-03', '2015-05-14', '2015-08-01', '2015-12-25', '2016-01-01', '2016-03-25', '2016-05-05', '2016-08-01', '2016-12-25', '2017-01-01', '2017-04-14', '2017-05-25', '2017-08-01', '2017-12-25']\n\n#holiday_zh = ['2015-01-01', '2015-02-18', '2015-02-19', '2015-02-20', '2015-02-21', '2015-02-22', '2015-02-23', '2015-02-27', '2015-04-03', '2015-04-04', '2015-04-05', '2015-04-06', '2015-04-07', '2015-05-01', '2015-05-25', '2015-06-19', '2015-06-20', '2015-07-01', '2015-09-03', '2015-09-28', '2015-10-01', '2015-10-09', '2015-10-10', '2015-10-21', '2015-12-25', '2015-12-26', '2016-01-01', '2016-02-07', '2016-02-08', '2016-02-09', '2016-02-10', '2016-02-11', '2016-02-12', '2016-02-29', '2016-03-25', '2016-03-26', '2016-03-28', '2016-04-04', '2016-04-05', '2016-05-01', '2016-05-02', '2016-05-14', '2016-06-09', '2016-06-10', '2016-07-01', '2016-09-15', '2016-09-16', '2016-09-28', '2016-10-01', '2016-10-10', '2016-12-25', '2016-12-26', '2016-12-27', '2017-01-01', '2017-01-02', '2017-01-27', '2017-01-28', '2017-01-29', '2017-01-30', '2017-01-31', '2017-02-01', '2017-02-27', '2017-02-28', '2017-04-03', '2017-04-04', '2017-04-14', '2017-04-15', '2017-04-17', '2017-05-01', '2017-05-03', '2017-05-29', '2017-05-30', '2017-07-01', '2017-10-01', '2017-10-02', '2017-10-04', '2017-10-05', '2017-10-09', '2017-10-10', '2017-10-28', '2017-12-25', '2017-12-26']\n\nholiday_zh_hongkong = ['2015-01-01', '2015-02-19', '2015-02-20', '2015-04-03', '2015-04-04', '2015-04-05', '2015-04-06', '2015-04-07', '2015-05-01', '2015-05-25', '2015-06-20', '2015-07-01', '2015-09-03', '2015-09-28', '2015-10-01', '2015-10-21', '2015-12-25', '2015-12-26', '2016-01-01', '2016-02-08', '2016-02-09', '2016-02-10', '2016-03-25', '2016-03-26', '2016-03-28', '2016-04-04', '2016-05-01', '2016-05-02', '2016-05-14', '2016-06-09', '2016-07-01', '2016-09-16', '2016-10-01', '2016-10-10', '2016-12-25', '2016-12-26', '2016-12-27', '2017-01-01', '2017-01-02', '2017-01-28', '2017-01-30', '2017-01-31', '2017-04-04', '2017-04-14', '2017-04-15', '2017-04-17', '2017-05-01', '2017-05-03', '2017-05-30', '2017-07-01', '2017-10-01', '2017-10-02', '2017-10-05', '2017-10-28', '2017-12-25', '2017-12-26']\nholiday_zh_taiwan = ['2015-01-01', '2015-02-18', '2015-02-19', '2015-02-20', '2015-02-21', '2015-02-22', '2015-02-23', '2015-02-23', '2015-02-27', '2015-04-03', '2015-04-05', '2015-04-06', '2015-06-19', '2015-06-20', '2015-09-28', '2015-10-09', '2015-10-10', '2016-01-01', '2016-02-07', '2016-02-08', '2016-02-09', '2016-02-10', '2016-02-11', '2016-02-12', '2016-02-29', '2016-04-04', '2016-04-05', '2016-06-09', '2016-06-10', '2016-09-15', '2016-09-16', '2016-09-28', '2016-10-10', '2017-01-01', '2017-01-02', '2017-01-27', '2017-01-28', '2017-01-29', '2017-01-30', '2017-01-31', '2017-02-01', '2017-02-27', '2017-02-28', '2017-04-03', '2017-04-04', '2017-05-01', '2017-05-29', '2017-05-30', '2017-10-04', '2017-10-09', '2017-10-10']\n\nholidays_en_us = pd.DataFrame({\n  'holiday': 'US public holiday',\n  'ds': pd.to_datetime(holiday_en_us),\n  'lower_window': 0,\n  'upper_window': 0,\n})\n\nholidays_en_uk = pd.DataFrame({\n  'holiday': 'UK public holiday',\n  'ds': pd.to_datetime(holiday_en_uk),\n  'lower_window': 0,\n  'upper_window': 0,\n})\n\nholidays_en_canada = pd.DataFrame({\n  'holiday': 'Canada public holiday',\n  'ds': pd.to_datetime(holiday_en_canada),\n  'lower_window': 0,\n  'upper_window': 0,\n})\n\nholidays_en = pd.concat((holidays_en_us, holidays_en_uk, holidays_en_canada))\n\nholidays_ru_russia = pd.DataFrame({\n  'holiday': 'Russia public holiday',\n  'ds': pd.to_datetime(holiday_ru_russia),\n  'lower_window': 0,\n  'upper_window': 0,\n})\n\nholidays_ru = holidays_ru_russia\n\nholidays_es_mexico = pd.DataFrame({\n  'holiday': 'Mexico public holiday',\n  'ds': pd.to_datetime(holiday_es_mexico),\n  'lower_window': 0,\n  'upper_window': 0,\n})\n\nholidays_es_spain = pd.DataFrame({\n  'holiday': 'Spain public holiday',\n  'ds': pd.to_datetime(holiday_es_spain),\n  'lower_window': 0,\n  'upper_window': 0,\n})\n\nholidays_es_colombia = pd.DataFrame({\n  'holiday': 'Colombia public holiday',\n  'ds': pd.to_datetime(holiday_es_colombia),\n  'lower_window': 0,\n  'upper_window': 0,\n})\n\nholidays_es = pd.concat((holidays_es_mexico, holidays_es_spain, holidays_es_colombia))\n\nholidays_fr_france = pd.DataFrame({\n  'holiday': 'France public holiday',\n  'ds': pd.to_datetime(holiday_fr_france),\n  'lower_window': 0,\n  'upper_window': 0,\n})\n\nholidays_fr = holidays_fr_france\n\nholidays_jp_japan = pd.DataFrame({\n  'holiday': 'Japan public holiday',\n  'ds': pd.to_datetime(holiday_jp_japan),\n  'lower_window': 0,\n  'upper_window': 0,\n})\n\nholidays_jp = holidays_jp_japan\n\nholidays_de_germany = pd.DataFrame({\n  'holiday': 'Germany public holiday',\n  'ds': pd.to_datetime(holiday_de_germany),\n  'lower_window': 0,\n  'upper_window': 0,\n})\n\nholidays_de_austria = pd.DataFrame({\n  'holiday': 'Austria public holiday',\n  'ds': pd.to_datetime(holiday_de_austria),\n  'lower_window': 0,\n  'upper_window': 0,\n})\n\nholidays_de_switzerland = pd.DataFrame({\n  'holiday': 'Switzerland public holiday',\n  'ds': pd.to_datetime(holiday_de_switzerland),\n  'lower_window': 0,\n  'upper_window': 0,\n})\n\nholidays_de = pd.concat((holidays_de_germany, holidays_de_austria, holidays_de_switzerland))\n\nholidays_zh_hongkong = pd.DataFrame({\n  'holiday': 'HK public holiday',\n  'ds': pd.to_datetime(holiday_zh_hongkong),\n  'lower_window': 0,\n  'upper_window': 0,\n})\n\nholidays_zh_taiwan = pd.DataFrame({\n  'holiday': 'Taiwan public holiday',\n  'ds': pd.to_datetime(holiday_zh_taiwan),\n  'lower_window': 0,\n  'upper_window': 0,\n})\n\nholidays_zh = pd.concat((holidays_zh_hongkong, holidays_zh_taiwan))\n\nholidays_dict = {\"en\": holidays_en, \n                 \"ru\": holidays_ru, \n                 \"es\": holidays_es, \n                 \"fr\": holidays_fr, \n                 \"ja\": holidays_jp,\n                 \"de\": holidays_de,\n                 \"zh\": holidays_zh}","8b09378f":"df_train['ds'] = pd.to_datetime(df_train['ds'])\ndf_train.ds.dt.dayofweek","1dc746a9":"df_train = extract_series(X_train, 2, 5)\ndf_actual = extract_series(y_train, 2, 5)\nlang = X_train.iloc[2, 1]","c2c4dfea":"df_train['ds'] = pd.to_datetime(df_train['ds'])\ndf_actual['ds'] = pd.to_datetime(df_actual['ds'])\ntrain_series = df_train['y']\ntrain_series.index = df_train.ds","bb4142f2":"df_train.ds.isin(holidays_dict[lang].ds)","206a4ae5":"if(isinstance(lang, float) and math.isnan(lang)):\n    df_train['holiday'] = df_train.ds.dt.dayofweek >=5\n    df_actual['holiday'] = df_actual.ds.dt.dayofweek >=5\nelse:\n    df_train['holiday'] = (df_train.ds.dt.dayofweek >=5) | df_train.ds.isin(holidays_dict[lang].ds)\n    df_actual['holiday'] = (df_actual.ds.dt.dayofweek >=5) | df_actual.ds.isin(holidays_dict[lang].ds)","5ea19c3d":"df_train['y'] = df_train.y.astype(int).values\ndf_actual['y'] = df_actual.y.astype(int).values","b9266a80":"def nanmedian_zero(a):\n    return np.nan_to_num(np.nanmedian(a))","a60b398d":"holiday = True\np = 15\nsample = df_train[-p:]\nif(holiday):\n    sample = sample[sample['holiday']]\nelse:\n    sample = sample[~sample['holiday']]","1aba1877":"visits = nanmedian_zero(sample['y'])","c50afd8e":"def median_holiday_model(df_train, df_actual, p, lang, review=False, figSize=(12, 4)):\n    # Split the train and actual set\n    df_train['ds'] = pd.to_datetime(df_train['ds'])\n    df_actual['ds'] = pd.to_datetime(df_actual['ds'])\n    train_series = df_train['y']\n    train_series.index = df_train.ds\n    \n    if(isinstance(lang, float) and math.isnan(lang)):\n        df_train['holiday'] = df_train.ds.dt.dayofweek >=5\n        df_actual['holiday'] = df_actual.ds.dt.dayofweek >=5\n    else:\n        df_train['holiday'] = (df_train.ds.dt.dayofweek >=5) | df_train.ds.isin(holidays_dict[lang].ds)\n        df_actual['holiday'] = (df_actual.ds.dt.dayofweek >=5) | df_actual.ds.isin(holidays_dict[lang].ds)\n     \n    # Combine the train and actual set\n    predict_holiday = median_holiday_helper(df_train, df_actual[df_actual.holiday], p, True)\n    predict_non_holiday = median_holiday_helper(df_train, df_actual[~df_actual.holiday], p, False)\n\n    forecast_series = predict_non_holiday.combine_first(predict_holiday)\n    \n    actual_series = df_actual.y\n    actual_series.index = df_actual.ds\n    \n    if(review):\n        plot_prediction_and_actual_2(train_series, forecast_series, actual_series, figSize=figSize, title='Median model with holiday')\n    \n    return smape(forecast_series, actual_series)\n\n\ndef median_holiday_helper(df_train, df_actual, p, holiday):\n    def nanmedian_zero(a):\n        return np.nan_to_num(np.nanmedian(a))\n    \n    df_train['y'] = df_train['y'].astype('float').values\n    df_actual['y'] = df_actual['y'].astype('float').values\n    \n    sample = df_train[-p:]\n    if(holiday):\n        sample = sample[sample['holiday']]\n    else:\n        sample = sample[~sample['holiday']]\n\n    visits = nanmedian_zero(sample['y'])\n    \n    idx = np.arange( p) + np.arange(len(df_train)- p+1)[:,None]\n    b = [row[row>=0] for row in df_train.y.values[idx]]\n    pre_forecast = pd.Series(np.append(([float('nan')] * (p-1)), list(map(nanmedian_zero,b))))\n    pre_forecast.index = df_train.ds\n    \n    forecast_series = pd.Series(np.repeat(visits, len(df_actual)))\n    forecast_series.index = df_actual.ds\n    \n    forecast_series = pre_forecast.append(forecast_series)\n    \n    return forecast_series","555be4ab":"predict_holiday = median_holiday_helper(df_train, df_actual[df_actual.holiday], p, True)\npredict_non_holiday = median_holiday_helper(df_train, df_actual[~df_actual.holiday], p, False)","6496eaac":"predict_holiday","00ac0217":"predict_non_holiday.combine_first(predict_holiday)","7e47d6f6":"# This is to demo the median model - weekday, weekend and \nprint(train.iloc[[2]])\n\ndf_train = extract_series(X_train, 2, 5)\ndf_actual = extract_series(y_train, 2, 5)\nlang = X_train.iloc[2, 1]\nscore = median_holiday_model(df_train.copy(), df_actual.copy(), 15, lang, review=True)\nprint(\"The SMAPE score is : %.5f\" % score)","ef17dc5a":"from statsmodels.tsa.arima_model import ARIMA   \nimport warnings\n\ndef arima_model(df_train, df_actual, p, d, q, figSize=(12, 4), review=False):\n    df_train = df_train.fillna(0)\n    train_series = df_train.y\n    train_series.index = df_train.ds\n\n    result = None\n    with warnings.catch_warnings():\n        warnings.filterwarnings('ignore')\n        try:\n            arima = ARIMA(train_series ,[p, d, q])\n            result = arima.fit(disp=False)\n        except Exception as e:\n            print('\\tARIMA failed', e)\n                \n    #print(result.params)\n    start_idx = df_train.ds[d]\n    end_idx = df_actual.ds.max()\n    forecast_series = result.predict(start_idx, end_idx, typ='levels')\n    \n    actual_series = df_actual.y\n    actual_series.index = pd.to_datetime(df_actual.ds)\n\n    if(review):\n        plot_prediction_and_actual_2(train_series, forecast_series, actual_series, figSize=figSize, title='ARIMA model')\n    \n    return smape(forecast_series, actual_series)","dfc9e57b":"df_train = df_train.fillna(0)\ntrain_series = df_train.y\ntrain_series.index = df_train.ds\n\nresult = None\nwith warnings.catch_warnings():\n    warnings.filterwarnings('ignore')\n    try:\n        arima = ARIMA(train_series ,[4, 1, 4])\n        result = arima.fit(disp=False)\n    except Exception as e:\n        print('\\tARIMA failed', e)","4b5d3539":"print(\"AR params:\", result.arparams, \"MA params:\", result.maparams)","8f452225":"start_idx = df_train.ds[1]\nend_idx = df_actual.ds.max()\nforecast_series = result.predict(start_idx, end_idx, typ='levels')","6d17dbcb":"plt.plot(forecast_series.index, forecast_series.values)\nplt.show()","1a1646ec":"forecast_series","91a23ac7":"# This is to demo the ARIMA model\nprint(train.iloc[[2]])\n\ndf_train = extract_series(X_train, 2, 5)\ndf_actual = extract_series(y_train, 2, 5)\nlang = X_train.iloc[2, 1]\nscore = arima_model(df_train.copy(), df_actual.copy(), 2, 1, 2, review=True)\nprint(\"The SMAPE score is : %.5f\" % score)","97338cae":"## E. Extreme data","ff3f645b":"## B. Load libraries and data files, file structure and content","072c7389":"## C. Median model - weekday, weekend and holiday","ae1eb309":"#### \u043a\u043e\u043d\u0441\u0442\u0440\u0443\u0438\u0440\u0443\u0435\u043c \u043d\u0435\u0441\u043a\u043e\u043b\u044c\u043a\u043e \u0442\u0438\u043f\u043e\u0432 baseline","79801565":"## D. Data visualization","7380aaf2":"# 2. Data transformation and helper functions\n## A. Article names and metadata","38216bdb":"### First and last 10 lines with nulls","ada211ba":"## B. Simple median model","290bb09b":"# Web Traffic Time Series Forecasting (Experimenting with different method)\n\n\n1. Introduction\n    1. Competition details\n    2. Load libraries and data files, file structure and content\n    3. Missing values\n    4. Data visualization\n    5. Extreme data\n2. Data transformation and helper functions\n    1. Article names and metadata\n    2. Split into train and validation dataset\n3. Forecast methods\n    1. SMAPE, the measurement\n    2. Simple median model\n    3. Median model - weekday, weekend and holiday\n    4. ARIMA model\n    5. Facebook prophet model\n    6. Sample series analysis (For script reconciliation)\n4. Selected model performance (validation score) over train dataset\n    1. Simple median model\n    2. Median model - weekday, weekend, holiday\n    3. ARIMA model\n    4. Facebook model\n    5. mixed model","72338895":"# 1. Introduction \n\n## A. Competition details\n\n### First stage\n* Training data from 2015-07-01 to 2016-12-29\n* Testing data from 2017-01-01 to 2017-03-01\n* Length of training vs length of testing = 547 days vs 59 days\n* Predict interval is ~10.7% of the training interval\n\n### Second stage\n* Training data from 2015-07-01 to 2017-09-01\n* Testing data from 2017-09-10 to 2017-11-10\n* Length of training vs length of testing = 793 days vs 61 days\n* Predict interval is ~7.7% of the training interval","9265fae2":"## D. ARIMA model\n\nThe below use the ARIMA from a Python library statsmodels. Please refer to http:\/\/www.statsmodels.org\/dev\/generated\/statsmodels.tsa.arima_model.ARIMA.html for details.\n\nThe model is slow and may throw exception if the model cannot find a solution. (Make the model difficult to build for all series).\n\nWe will further investigate it performance in the later section.","fb4ad810":"# 3 Forecast methods\n\n\u0412 \u044d\u0442\u043e\u0439 \u0441\u0435\u043a\u0446\u0438\u0438 \u0440\u0430\u0441\u0441\u043c\u043e\u0442\u0440\u0438\u043c \u043d\u0430\u0438\u0431\u043e\u043b\u0435\u0435 \u043f\u043e\u043f\u0443\u043b\u044f\u0440\u043d\u044b\u0435 \u043c\u0435\u0442\u043e\u0434\u044b \u0440\u0430\u0431\u043e\u0442\u044b \u0441 \u0432\u0440\u0435\u043c\u0435\u043d\u043d\u044b\u043c\u0438 \u0440\u044f\u0434\u0430\u043c\u0438","7df47f2f":"#### \u0420\u0430\u0441\u043f\u0440\u0435\u0434\u0435\u043b\u0435\u043d\u0438\u044f \u043f\u043e \u0442\u0438\u043f\u0430\u043c \u043f\u0430\u0440\u0430\u043c\u0435\u0442\u0440\u043e\u0432","6f688149":"#### \u0414\u043e\u0441\u0442\u0430\u0435\u043c \u043f\u0430\u0440\u0430\u043c\u0435\u0442\u0440\u044b \u0432\u0441\u0435\u0445 wiki-\u0441\u0442\u0440\u0430\u043d\u0438\u0446","39918797":"#### \u041a\u043e\u043b-\u0432\u043e \u0443\u043d\u0438\u043a\u0430\u043b\u044c\u043d\u044b\u0445 \u0441\u0442\u0430\u0442\u0435\u0439","8bb0495a":"## A. SMAPE, the measurement\n\nSMAPE is harsh when the series is near zero.\nA notebook https:\/\/www.kaggle.com\/cpmpml\/smape-weirdness give a very good visualization of the SMAPE function.\n\nAfter you find that there is no way to further improve quality of the result, you may consider doing a little bit hacking on SMAPE to give you better score.","bcd6673b":"## B. Split into train and validation dataset","f98297bf":"## C. Missing values"}}