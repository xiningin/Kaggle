{"cell_type":{"926b0971":"code","3f5fdb96":"code","4ccd2535":"code","cb19594c":"code","c6e707ba":"code","1423ba0c":"code","8840380f":"code","d4abd5ba":"code","0aff1024":"code","cf263a71":"code","a29500f2":"code","27eee454":"code","2adc1048":"code","3f5beec2":"code","f9a5e261":"code","01b77590":"code","0b740efa":"code","4040255e":"code","844b6f4a":"code","14e26991":"code","7d2ab208":"code","9ef827c1":"code","eb39a2c7":"code","52bb3973":"code","298ca37a":"code","11580ebd":"code","9c4c2c4f":"code","31cef954":"code","0fff9892":"code","638996b4":"code","e09bb071":"code","7e7dddef":"code","4815d7e9":"code","fbc4f5dd":"code","3611a3ef":"code","def34e72":"code","5e6fc605":"code","665cd660":"code","5e6b5879":"code","901a9bc5":"code","774ddc83":"code","a5691469":"code","f60cdea9":"code","ff137476":"markdown","b4d60a41":"markdown","444664b7":"markdown","99706231":"markdown","b6c34334":"markdown","b8ecc577":"markdown","d5f46738":"markdown","5a1497eb":"markdown","81ca5b52":"markdown","cb115f5f":"markdown","defa3546":"markdown","4a050095":"markdown","1c017943":"markdown"},"source":{"926b0971":"import numpy as np \nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing import sequence\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.losses import SparseCategoricalCrossentropy\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.layers import Dense, Dropout, Embedding, LSTM, Conv1D, GlobalMaxPooling1D, Bidirectional, SpatialDropout1D, MaxPooling1D, GRU\nfrom tensorflow.keras.models import load_model\nfrom tensorflow.keras.utils import to_categorical\nfrom tensorflow.keras.metrics import MeanAbsoluteError, MeanSquaredError\n\nfrom collections import Counter\nfrom imblearn.over_sampling import SMOTE \n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import classification_report\nfrom sklearn.linear_model import LinearRegression, Lasso, Ridge\nfrom sklearn.svm import SVR\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom xgboost import XGBRegressor\nfrom sklearn.metrics import confusion_matrix, accuracy_score, recall_score, f1_score, classification_report, mean_squared_error, mean_absolute_error\n\nimport warnings\n\nplt.style.use('ggplot')\nwarnings.filterwarnings('ignore')","3f5fdb96":"df = pd.read_csv('\/kaggle\/input\/apple-iphone-se-reviews-ratings\/APPLE_iPhone_SE.csv')\ndf.head()","4ccd2535":"df.info()","cb19594c":"df.describe(include='all')","c6e707ba":"df['Ratings'].value_counts()","1423ba0c":"fig, ax = plt.subplots(figsize=(20, 10), subplot_kw=dict(aspect=\"equal\"))\n\ndata = df['Ratings'].value_counts().values\nlabel = df['Ratings'].value_counts().index\n\n\ndef func_label(pct, allvals):\n    absolute = int(round(pct\/100.*np.sum(allvals)))\n    return \"{:.1f}% ({:d})\".format(pct, absolute)\n#     return \"{:.1f}%\\n({:d})\".format(pct, absolute)\"\"\n\n\nwedges, texts, autotexts = ax.pie(data, autopct=lambda pct: func_label(pct, data),\n                                  textprops=dict(color=\"w\"))\n\nax.legend(wedges, label,\n          title=\"Rating\",\n          loc=\"center left\",\n          bbox_to_anchor=(1, 0, 0.5, 1))\n\nplt.setp(autotexts, size=15, weight=\"bold\")\n\nax.set_title(\"Pie Chart Rating Dist for Iphone SE\")\nplt.show()","8840380f":"df['Comment'].value_counts()","d4abd5ba":"fig, ax = plt.subplots(2,1,figsize=(28,15))\n\nax[0].bar(df['Comment'].value_counts().index[:5], df['Comment'].value_counts().values[:5])\nax[0].set_title('Barplot Top 5 Comments')\n\nax[1].bar(df['Comment'].value_counts().index[-5:], df['Comment'].value_counts().values[-5:])\nax[1].set_title('Barplot Least 5 Comments')\nplt.show()","0aff1024":"print('Null values in Ratings : {} \\nNaN values in Ratings : {}'.format(df['Ratings'].isnull().sum(), df['Ratings'].isna().sum()))\nprint('Null values in Comments : {} \\nNaN values in Comments : {}'.format(df['Comment'].isnull().sum(), df['Comment'].isna().sum()))\nprint('Null values in Reviews : {} \\nNaN values in Reviews : {}'.format(df['Reviews'].isnull().sum(), df['Reviews'].isna().sum()))","cf263a71":"print('Total Reviews Data : {} Data'.format(df['Reviews'].shape[0]))","a29500f2":"plt.figure(figsize=(10, 8))\nsplot=sns.barplot(df['Ratings'].value_counts().index, df['Ratings'].value_counts().values)\nfor p in splot.patches:\n    splot.annotate(format(p.get_height(), '.1f'), \n                   (p.get_x() + p.get_width() \/ 2., p.get_height()), \n                   ha = 'center', va = 'center', \n                   xytext = (0, 9), \n                   textcoords = 'offset points')\nplt.xlabel(\"Ratings\", size=14)\nplt.ylabel(\"Count\", size=14)\nplt.title('Rating Distribution', fontsize=14)\nplt.show()","27eee454":"from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\n\nabbreviations = {\n    \"$\" : \" dollar \",\n    \"\u20ac\" : \" euro \",\n    \"4ao\" : \"for adults only\",\n    \"a.m\" : \"before midday\",\n    \"a3\" : \"anytime anywhere anyplace\",\n    \"aamof\" : \"as a matter of fact\",\n    \"acct\" : \"account\",\n    \"adih\" : \"another day in hell\",\n    \"afaic\" : \"as far as i am concerned\",\n    \"afaict\" : \"as far as i can tell\",\n    \"afaik\" : \"as far as i know\",\n    \"afair\" : \"as far as i remember\",\n    \"afk\" : \"away from keyboard\",\n    \"app\" : \"application\",\n    \"approx\" : \"approximately\",\n    \"apps\" : \"applications\",\n    \"asap\" : \"as soon as possible\",\n    \"asl\" : \"age, sex, location\",\n    \"atk\" : \"at the keyboard\",\n    \"ave.\" : \"avenue\",\n    \"aymm\" : \"are you my mother\",\n    \"ayor\" : \"at your own risk\", \n    \"b&b\" : \"bed and breakfast\",\n    \"b+b\" : \"bed and breakfast\",\n    \"b.c\" : \"before christ\",\n    \"b2b\" : \"business to business\",\n    \"b2c\" : \"business to customer\",\n    \"b4\" : \"before\",\n    \"b4n\" : \"bye for now\",\n    \"b@u\" : \"back at you\",\n    \"bae\" : \"before anyone else\",\n    \"bak\" : \"back at keyboard\",\n    \"bbbg\" : \"bye bye be good\",\n    \"bbc\" : \"british broadcasting corporation\",\n    \"bbias\" : \"be back in a second\",\n    \"bbl\" : \"be back later\",\n    \"bbs\" : \"be back soon\",\n    \"be4\" : \"before\",\n    \"bfn\" : \"bye for now\",\n    \"blvd\" : \"boulevard\",\n    \"bout\" : \"about\",\n    \"brb\" : \"be right back\",\n    \"bros\" : \"brothers\",\n    \"brt\" : \"be right there\",\n    \"bsaaw\" : \"big smile and a wink\",\n    \"btw\" : \"by the way\",\n    \"bwl\" : \"bursting with laughter\",\n    \"c\/o\" : \"care of\",\n    \"cet\" : \"central european time\",\n    \"cf\" : \"compare\",\n    \"cia\" : \"central intelligence agency\",\n    \"csl\" : \"can not stop laughing\",\n    \"cu\" : \"see you\",\n    \"cul8r\" : \"see you later\",\n    \"cv\" : \"curriculum vitae\",\n    \"cwot\" : \"complete waste of time\",\n    \"cya\" : \"see you\",\n    \"cyt\" : \"see you tomorrow\",\n    \"dae\" : \"does anyone else\",\n    \"dbmib\" : \"do not bother me i am busy\",\n    \"diy\" : \"do it yourself\",\n    \"dm\" : \"direct message\",\n    \"dwh\" : \"during work hours\",\n    \"e123\" : \"easy as one two three\",\n    \"eet\" : \"eastern european time\",\n    \"eg\" : \"example\",\n    \"embm\" : \"early morning business meeting\",\n    \"encl\" : \"enclosed\",\n    \"encl.\" : \"enclosed\",\n    \"etc\" : \"and so on\",\n    \"faq\" : \"frequently asked questions\",\n    \"fawc\" : \"for anyone who cares\",\n    \"fb\" : \"facebook\",\n    \"fc\" : \"fingers crossed\",\n    \"fig\" : \"figure\",\n    \"fimh\" : \"forever in my heart\", \n    \"ft.\" : \"feet\",\n    \"ft\" : \"featuring\",\n    \"ftl\" : \"for the loss\",\n    \"ftw\" : \"for the win\",\n    \"fwiw\" : \"for what it is worth\",\n    \"fyi\" : \"for your information\",\n    \"g9\" : \"genius\",\n    \"gahoy\" : \"get a hold of yourself\",\n    \"gal\" : \"get a life\",\n    \"gcse\" : \"general certificate of secondary education\",\n    \"gfn\" : \"gone for now\",\n    \"gg\" : \"good game\",\n    \"gl\" : \"good luck\",\n    \"glhf\" : \"good luck have fun\",\n    \"gmt\" : \"greenwich mean time\",\n    \"gmta\" : \"great minds think alike\",\n    \"gn\" : \"good night\",\n    \"g.o.a.t\" : \"greatest of all time\",\n    \"goat\" : \"greatest of all time\",\n    \"goi\" : \"get over it\",\n    \"gps\" : \"global positioning system\",\n    \"gr8\" : \"great\",\n    \"gratz\" : \"congratulations\",\n    \"gyal\" : \"girl\",\n    \"h&c\" : \"hot and cold\",\n    \"hp\" : \"horsepower\",\n    \"hr\" : \"hour\",\n    \"hrh\" : \"his royal highness\",\n    \"ht\" : \"height\",\n    \"ibrb\" : \"i will be right back\",\n    \"ic\" : \"i see\",\n    \"icq\" : \"i seek you\",\n    \"icymi\" : \"in case you missed it\",\n    \"idc\" : \"i do not care\",\n    \"idgadf\" : \"i do not give a damn fuck\",\n    \"idgaf\" : \"i do not give a fuck\",\n    \"idk\" : \"i do not know\",\n    \"ie\" : \"that is\",\n    \"i.e\" : \"that is\",\n    \"ifyp\" : \"i feel your pain\",\n    \"IG\" : \"instagram\",\n    \"iirc\" : \"if i remember correctly\",\n    \"ilu\" : \"i love you\",\n    \"ily\" : \"i love you\",\n    \"imho\" : \"in my humble opinion\",\n    \"imo\" : \"in my opinion\",\n    \"imu\" : \"i miss you\",\n    \"iow\" : \"in other words\",\n    \"irl\" : \"in real life\",\n    \"j4f\" : \"just for fun\",\n    \"jic\" : \"just in case\",\n    \"jk\" : \"just kidding\",\n    \"jsyk\" : \"just so you know\",\n    \"l8r\" : \"later\",\n    \"lb\" : \"pound\",\n    \"lbs\" : \"pounds\",\n    \"ldr\" : \"long distance relationship\",\n    \"lmao\" : \"laugh my ass off\",\n    \"lmfao\" : \"laugh my fucking ass off\",\n    \"lol\" : \"laughing out loud\",\n    \"ltd\" : \"limited\",\n    \"ltns\" : \"long time no see\",\n    \"m8\" : \"mate\",\n    \"mf\" : \"motherfucker\",\n    \"mfs\" : \"motherfuckers\",\n    \"mfw\" : \"my face when\",\n    \"mofo\" : \"motherfucker\",\n    \"mph\" : \"miles per hour\",\n    \"mr\" : \"mister\",\n    \"mrw\" : \"my reaction when\",\n    \"ms\" : \"miss\",\n    \"mte\" : \"my thoughts exactly\",\n    \"nagi\" : \"not a good idea\",\n    \"nbc\" : \"national broadcasting company\",\n    \"nbd\" : \"not big deal\",\n    \"nfs\" : \"not for sale\",\n    \"ngl\" : \"not going to lie\",\n    \"nhs\" : \"national health service\",\n    \"nrn\" : \"no reply necessary\",\n    \"nsfl\" : \"not safe for life\",\n    \"nsfw\" : \"not safe for work\",\n    \"nth\" : \"nice to have\",\n    \"nvr\" : \"never\",\n    \"nyc\" : \"new york city\",\n    \"oc\" : \"original content\",\n    \"og\" : \"original\",\n    \"ohp\" : \"overhead projector\",\n    \"oic\" : \"oh i see\",\n    \"omdb\" : \"over my dead body\",\n    \"omg\" : \"oh my god\",\n    \"omw\" : \"on my way\",\n    \"p.a\" : \"per annum\",\n    \"p.m\" : \"after midday\",\n    \"pm\" : \"prime minister\",\n    \"poc\" : \"people of color\",\n    \"pov\" : \"point of view\",\n    \"pp\" : \"pages\",\n    \"ppl\" : \"people\",\n    \"prw\" : \"parents are watching\",\n    \"ps\" : \"postscript\",\n    \"pt\" : \"point\",\n    \"ptb\" : \"please text back\",\n    \"pto\" : \"please turn over\",\n    \"qpsa\" : \"what happens\", #\"que pasa\",\n    \"ratchet\" : \"rude\",\n    \"rbtl\" : \"read between the lines\",\n    \"rlrt\" : \"real life retweet\", \n    \"rofl\" : \"rolling on the floor laughing\",\n    \"roflol\" : \"rolling on the floor laughing out loud\",\n    \"rotflmao\" : \"rolling on the floor laughing my ass off\",\n    \"rt\" : \"retweet\",\n    \"ruok\" : \"are you ok\",\n    \"sfw\" : \"safe for work\",\n    \"sk8\" : \"skate\",\n    \"smh\" : \"shake my head\",\n    \"sq\" : \"square\",\n    \"srsly\" : \"seriously\", \n    \"ssdd\" : \"same stuff different day\",\n    \"tbh\" : \"to be honest\",\n    \"tbs\" : \"tablespooful\",\n    \"tbsp\" : \"tablespooful\",\n    \"tfw\" : \"that feeling when\",\n    \"thks\" : \"thank you\",\n    \"tho\" : \"though\",\n    \"thx\" : \"thank you\",\n    \"tia\" : \"thanks in advance\",\n    \"til\" : \"today i learned\",\n    \"tl;dr\" : \"too long i did not read\",\n    \"tldr\" : \"too long i did not read\",\n    \"tmb\" : \"tweet me back\",\n    \"tntl\" : \"trying not to laugh\",\n    \"ttyl\" : \"talk to you later\",\n    \"u\" : \"you\",\n    \"u2\" : \"you too\",\n    \"u4e\" : \"yours for ever\",\n    \"utc\" : \"coordinated universal time\",\n    \"w\/\" : \"with\",\n    \"w\/o\" : \"without\",\n    \"w8\" : \"wait\",\n    \"wassup\" : \"what is up\",\n    \"wb\" : \"welcome back\",\n    \"wtf\" : \"what the fuck\",\n    \"wtg\" : \"way to go\",\n    \"wtpa\" : \"where the party at\",\n    \"wuf\" : \"where are you from\",\n    \"wuzup\" : \"what is up\",\n    \"wywh\" : \"wish you were here\",\n    \"yd\" : \"yard\",\n    \"ygtr\" : \"you got that right\",\n    \"ynk\" : \"you never know\",\n    \"zzz\" : \"sleeping bored and tired\"\n}","2adc1048":"import re\nimport nltk\nfrom nltk.stem.wordnet import WordNetLemmatizer\nfrom nltk.tokenize import TweetTokenizer\nfrom nltk.stem.wordnet import WordNetLemmatizer\n\nnltk.download('wordnet')\nnltk.download('stopwords')\nnltk.download('punkt')\nstop_words = nltk.corpus.stopwords.words(['english'])\nlem = WordNetLemmatizer()\n\nprint(stop_words)","3f5beec2":"def word_abbrev(word):\n    return abbreviations[word.lower()] if word.lower() in abbreviations.keys() else word\n\n# Replace all abbreviations\ndef replace_abbrev(text):\n    string = \"\"\n    for word in text.split():\n        string += word_abbrev(word) + \" \"        \n    return string\n\ndef cleaning(data):\n    \n    #remove urls\n    tweet_without_url = re.sub(r'http\\S+',' ', data)\n\n    #remove hashtags\n    tweet_without_hashtag = re.sub(r'#\\w+', ' ', tweet_without_url)\n\n    #3. Remove mentions and characters that not in the English alphabets\n    tweet_without_mentions = re.sub(r'@\\w+',' ', tweet_without_hashtag)\n    precleaned_tweet = re.sub('[^A-Za-z]+', ' ', tweet_without_mentions)\n\n    #2. Tokenize\n    tweet_tokens = TweetTokenizer().tokenize(precleaned_tweet)\n    \n    #3. Remove Puncs\n    tokens_without_punc = [w for w in tweet_tokens if w.isalpha()]\n    \n    #4. Removing Stopwords\n    tokens_without_sw = [t for t in tokens_without_punc if t not in stop_words]\n    \n    #5. lemma\n    text_cleaned = [lem.lemmatize(t) for t in tokens_without_sw]\n    \n    #6. Joining\n    return \" \".join(text_cleaned)","f9a5e261":"df['Clean_Reviews'] = df['Reviews'].apply(replace_abbrev)\ndf['Clean_Reviews'] = df['Clean_Reviews'].apply(cleaning)\ndf['Clean_Reviews'] = df['Clean_Reviews'].apply(lambda x: x.lower())","01b77590":"print('Before Cleaning : ', df['Reviews'][0], '\\n')\nprint('After Cleaning : ', df['Clean_Reviews'][0])","0b740efa":"tokenizer = Tokenizer()\ntokenizer.fit_on_texts(df['Clean_Reviews'])\n\nX = tokenizer.texts_to_sequences(df['Clean_Reviews'])\ny = df[\"Ratings\"]\n\nvocab_size = len(tokenizer.word_index)+1","4040255e":"vocab_size","844b6f4a":"max_len = 50\n\nprint(\"Vocabulary size: {}\".format(vocab_size))\nprint(\"\\n----------Example----------\\n\")\nprint(\"Sentence:\\n{}\".format(df[\"Clean_Reviews\"][6]))\nprint(\"\\nAfter tokenizing :\\n{}\".format(X[6]))\n\nX = pad_sequences(X, padding='post', maxlen = max_len)  # adding padding of zeros to obtain uniform length for all sequences\nprint(\"\\nAfter padding :\\n{}\".format(X[6]))","14e26991":"sm = SMOTE(random_state=42)\nX_res, y_res = sm.fit_resample(X, y)","7d2ab208":"y_res.value_counts()","9ef827c1":"(X_train, X_test, y_train, y_test) = train_test_split(X_res, y_res, test_size = .25, random_state=42)","eb39a2c7":"print(X_train.shape, y_train.shape)\nprint(X_test.shape, y_test.shape)","52bb3973":"# y_train = to_categorical(y_train)\n# y_test = to_categorical(y_test)","298ca37a":"def fit_pred(model, X_train,X_test,y_train,y_test):\n    clf = model\n    clf.fit(X_train, y_train)\n    y_pred = clf.predict(X_test)\n#     f,ax = plt.subplots(figsize=(3,3))\n#     sns.heatmap(cmatx,annot=True,linewidths=0.5,cbar=False,linecolor=\"red\",fmt='.0f',ax=ax)\n#     plt.xlabel(\"y_predict\")\n#     plt.ylabel(\"y_true\")\n#     ax.set(title=str(clf))\n#     plt.show()\n    mse = mean_squared_error(y_test, y_pred)\n    mae = mean_absolute_error(y_test, y_pred)\n    rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n    print('-------------{}-------------'.format(type(model).__name__))\n    print('MSE Score : {}'.format(mse))\n    print('MAE score : {}'.format(mae))\n    print('RMSE Score : {}'.format(rmse))\n    return type(model).__name__, mse, mae, rmse\n    ","11580ebd":"models=[\n        LinearRegression(),\n        Lasso(),\n        Ridge(),\n        SVR(),\n        DecisionTreeRegressor(),\n        KNeighborsRegressor(),\n        XGBRegressor()\n       ]","9c4c2c4f":"model = []\nmse = []\nmae = []\nrmse = []\nfor m in models:\n    name, mse_score, mae_score,rmse_score = fit_pred(m, X_train, X_test, y_train, y_test)\n    model.append(name)\n    mse.append(mse_score)\n    mae.append(mae_score)\n    rmse.append(rmse_score)","31cef954":"EPOCHS = 3\nBATCH_SIZE = 32 \nembedding_dim = 16\nunits = 256\n\nmodel_gru = Sequential()\n\nmodel_gru.add(Embedding(vocab_size, embedding_dim, input_length = max_len))\nmodel_gru.add(Bidirectional(GRU(128, return_sequences=True)))\nmodel_gru.add(Dropout(0.2))\n\nmodel_gru.add(GlobalMaxPooling1D())\nmodel_gru.add(Dropout(0.2))\n\nmodel_gru.add(Dense(256,activation = 'linear'))\nmodel_gru.add(Dropout(0.2))\n\nmodel_gru.add(Dense(1,activation = 'linear'))  # we have numb_class categories so we have to use softmax \n\nmodel_gru.compile(loss = 'mean_squared_error', optimizer = 'adam', metrics=[MeanAbsoluteError(), MeanSquaredError()])\nmodel_gru.summary()","0fff9892":"model_cnn = Sequential()\n\nmodel_cnn.add(Embedding(vocab_size, embedding_dim, input_length=max_len))\nmodel_cnn.add(Conv1D(filters=32, kernel_size=3, padding='same', activation='relu'))\nmodel_cnn.add(MaxPooling1D(pool_size=2))\n\nmodel_cnn.add(Conv1D(filters=64, kernel_size=3, padding='same', activation='relu'))\nmodel_cnn.add(MaxPooling1D(pool_size=2))\n\nmodel_cnn.add(LSTM(128, dropout=0.2, recurrent_dropout=0.2))\nmodel_cnn.add(Dense(1,activation='linear'))\n\nmodel_cnn.compile(loss = 'mean_squared_error', optimizer = 'adam', metrics=[MeanAbsoluteError(), MeanSquaredError()])\nmodel_cnn.summary()","638996b4":"from tensorflow.keras.callbacks import ModelCheckpoint, ReduceLROnPlateau, CSVLogger, EarlyStopping\n\ndef callbacks():\n  cb =[]\n  reduceLROnPlat = ReduceLROnPlateau(monitor='val_mean_squared_error',  \n                                       factor=0.5, patience=1, \n                                       verbose=1, mode='min', \n                                       min_delta=0.0001, min_lr=0,\n                                       restore_best_weights=True)\n  cb.append(reduceLROnPlat)\n  log = CSVLogger('log.csv')\n  cb.append(log)\n\n  es = EarlyStopping(monitor='val_loss', patience=30, verbose=0,\n                       mode='min', restore_best_weights=True)\n  cb.append(es)\n\n  return cb","e09bb071":"history = model_gru.fit(X_train, y_train, batch_size=BATCH_SIZE, validation_split=0.2, epochs = 20, callbacks = callbacks())","7e7dddef":"model_evaluate_gru = model_gru.evaluate(X_test, y_test)","4815d7e9":"history_gru = history","fbc4f5dd":"history_gru.history['mean_squared_error']","3611a3ef":"root_mean_squared_error = [np.sqrt(i) for i in history_gru.history['mean_squared_error']]","def34e72":"val_root_mean_squared_error = [np.sqrt(i) for i in history_gru.history['val_mean_squared_error']]","5e6fc605":"fig, ax = plt.subplots(3,1,figsize=(15,14))\nax[0].plot(range(len(history.history['mean_absolute_error'])) ,history.history['mean_absolute_error'], label = 'mean_absolute_error')\nax[0].plot(range(len(history.history['val_mean_absolute_error'])) ,history.history['val_mean_absolute_error'], label='val_mean_absolute_error')\nax[0].legend()\nax[0].set_title('Mean Absolute Error', fontsize=15)\n\nax[1].plot(range(len(history.history['mean_squared_error'])) ,history.history['mean_squared_error'], label = 'mean_squared_error')\nax[1].plot(range(len(history.history['val_mean_squared_error'])) ,history.history['val_mean_squared_error'], label='val_mean_squared_error')\nax[1].legend()\nax[1].set_title('Mean Squared Error', fontsize=15)\n\nax[2].plot(range(len(root_mean_squared_error)) ,root_mean_squared_error, label = 'root_mean_squared_error')\nax[2].plot(range(len(val_root_mean_squared_error)) ,val_root_mean_squared_error, label='val_root_mean_squared_error')\nax[2].legend()\nax[2].set_title('Root Mean Squared Error', fontsize=15)\nfig.show()","665cd660":"model.append('Deep Learning (GRU)')\nmae.append(model_evaluate_gru[1])\nmse.append(model_evaluate_gru[2])\nrmse.append(np.sqrt(model_evaluate_gru[2]))","5e6b5879":"history_cnn = model_cnn.fit(X_train, y_train, batch_size=BATCH_SIZE, validation_split=0.2, epochs = 20, callbacks = callbacks())","901a9bc5":"model_evaluate_cnn = model_cnn.evaluate(X_test, y_test)","774ddc83":"model.append('Deep Learning (CNN)')\nmae.append(model_evaluate_cnn[1])\nmse.append(model_evaluate_cnn[2])\nrmse.append(np.sqrt(model_evaluate_cnn[2]))","a5691469":"results = pd.DataFrame({'Model':model, 'MAE':mae, 'MSE':mse, 'RMSE':rmse})\nresults","f60cdea9":"fig, ax = plt.subplots(figsize=(12,10))\nsns.barplot(x='Model', y='RMSE', data=results, ax=ax)\nax.set_xticklabels(rotation=45, labels=results['Model'])\nax.set_title('Results', fontsize=18)\nfig.show()","ff137476":"## Missing values in every columns","b4d60a41":"# Splitting Data","444664b7":"## Handling Imbalance Data","99706231":"# Read Data","b6c34334":"## Build Model","b8ecc577":"# Column Values ","d5f46738":"## Cleaning Text","5a1497eb":"# Text Preprocessing","81ca5b52":"# Train Data using ML Method","cb115f5f":"## Comment Values","defa3546":"## How many Reviews Data","4a050095":"## Tokenizing","1c017943":"## Ratings Values"}}