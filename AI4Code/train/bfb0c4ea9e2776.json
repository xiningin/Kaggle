{"cell_type":{"f02f602a":"code","940377cf":"code","ceead193":"code","9b0205c1":"code","768bd0e1":"code","753ee2db":"code","b3dc6089":"code","8ce9c9b7":"code","0d2657bf":"code","95941f38":"code","d12ff008":"code","63693f28":"code","7ba7c709":"code","1e3b8da6":"code","238360dc":"code","773708ad":"code","13024713":"code","438c8b5e":"code","b07611f4":"code","8fb98dc8":"code","261ed549":"code","00abe612":"code","2e1a77d0":"code","654f2884":"code","beb70cd5":"markdown","ca2c5279":"markdown","50088496":"markdown","fcfa30f5":"markdown","75babbcd":"markdown","b9b5a70e":"markdown","6cc39a29":"markdown","d2afac06":"markdown","c954fa4f":"markdown","88223089":"markdown","3b62f4cc":"markdown","2f34e8b4":"markdown","6b061e03":"markdown","4a46cd3d":"markdown","2d7ec576":"markdown","0892a8a0":"markdown","55dff242":"markdown","18595dbb":"markdown"},"source":{"f02f602a":"import pandas as pd\nimport numpy as np","940377cf":"data = pd.read_csv('..\/input\/credit_card_clients_split.csv')\nprint(data.shape)\ndata.head()","ceead193":"data['target'] = np.log(1 + data.avg_monthly_turnover)\ndata['log_income']= np.log(1 + data.income)\ndata['positive_target'] = (data.avg_monthly_turnover > 0).astype(int)","9b0205c1":"train_data = data[data.avg_monthly_turnover.notnull()].copy()","768bd0e1":"from sklearn.model_selection import train_test_split","753ee2db":"train, test = train_test_split(train_data, test_size=0.5, random_state=1, shuffle=True)","b3dc6089":"train_pos = train[train.avg_monthly_turnover > 0]","8ce9c9b7":"from sklearn.linear_model import LinearRegression\nlinr = LinearRegression().fit(train_pos[['log_income']], train_pos['target'])","0d2657bf":"from sklearn.pipeline import make_pipeline\nfrom sklearn.linear_model import LogisticRegression\nchannel_dummy = pd.get_dummies(train.sales_channel_id)\nlogr = LogisticRegression().fit(channel_dummy, train.positive_target)","95941f38":"def predict(dataset):\n    p_positive = logr.predict_proba(pd.get_dummies(dataset.sales_channel_id))[:, 1]\n    log_positive = linr.predict(dataset[['log_income']])\n    prediction = np.exp(p_positive * log_positive)\n    return prediction","d12ff008":"from sklearn.metrics import mean_squared_log_error","63693f28":"print(mean_squared_log_error(train.avg_monthly_turnover, predict(train)) ** 0.5)\nprint(mean_squared_log_error(test.avg_monthly_turnover, predict(test)) ** 0.5)","7ba7c709":"numeric_predictors = ['log_income', 'age']\ncategorical_predictors = ['education', 'sales_channel_id', 'wrk_rgn_code']\nx_train = pd.get_dummies(train[categorical_predictors + numeric_predictors], columns=categorical_predictors)\nx_train_columns = x_train.columns\nprint(x_train_columns)","1e3b8da6":"def preprocess(dataset):\n    x = pd.get_dummies(dataset[categorical_predictors + numeric_predictors], columns=categorical_predictors)\n    x = x.reindex(x_train_columns, axis=1).fillna(0)\n    return x\nx_train = preprocess(train)\nprint(x_train.shape)","238360dc":"from sklearn.linear_model import Ridge","773708ad":"level1 = Ridge(alpha=1)\nlevel1.fit(x_train[train.positive_target == 1], train.target[train.positive_target==1])","13024713":"level0 = LogisticRegression(C=1, solver='liblinear')\nlevel0.fit(x_train, train.positive_target)","438c8b5e":"def predict(dataset):\n    x = preprocess(dataset)\n    p_positive = level0.predict_proba(x)[:, 1]\n    log_positive = level1.predict(x)\n    prediction = np.exp(p_positive * log_positive)\n    return prediction","b07611f4":"print(mean_squared_log_error(train.avg_monthly_turnover, predict(train)) ** 0.5)\nprint(mean_squared_log_error(test.avg_monthly_turnover, predict(test)) ** 0.5)","8fb98dc8":"def preprocess(dataset):\n    x = pd.get_dummies(dataset[categorical_predictors + numeric_predictors], columns=categorical_predictors)\n    x = x.reindex(x_train_columns, axis=1).fillna(0)\n    # adding cross_products of features\n    for i, c1 in enumerate(x_train_columns):\n        for j, c2 in enumerate(x_train_columns):\n            if j > i and c1[:10] != c2[:10]:\n                x[c1 + '_' + c2] = x[c1] * x[c2]\n    return x\n\nx_train = preprocess(train)\nprint(x_train.shape)","261ed549":"level1 = Ridge(alpha=1)\nlevel1.fit(x_train[train.positive_target == 1], train.target[train.positive_target==1])","00abe612":"level0 = LogisticRegression(C=1, solver='liblinear')\nlevel0.fit(x_train, train.positive_target)","2e1a77d0":"def predict(dataset):\n    x = preprocess(dataset)\n    p_positive = level0.predict_proba(x)[:, 1]\n    log_positive = level1.predict(x)\n    prediction = np.exp(p_positive * log_positive)\n    return prediction","654f2884":"print(mean_squared_log_error(train.avg_monthly_turnover, predict(train)) ** 0.5)\nprint(mean_squared_log_error(test.avg_monthly_turnover, predict(test)) ** 0.5)","beb70cd5":"Here is the logistic regression for the zero-or-positive part. It is based on the binary (\"dummy\") variables, which are a just a way to put a categorical variable (`sales_channel_id`) into the formula\n$$P(turnover>0)\\approx \\frac{1}{1+\\exp\\left(-(\\alpha+\\sum_j \\beta_j [channelid = c_j]\\right)}$$","ca2c5279":"Select only the data with present target variable","50088496":"Here is the linear regression from the baseline notebook - for the positive part.\n\n$$\\mathbb{E}(\\log(turnover)|turnover>0) \\approx \\alpha + \\beta \\log(income)$$","fcfa30f5":"Create a table for fitting the model of positive turnover","75babbcd":"Now we try to evaulate prediction quality for both models, using the same metric (root means squaret logarithmic error) as in Kaggle. ","b9b5a70e":"The quality has improved even more, but not by much - most of the work has been already done by the simpler regerssions. ","6cc39a29":"Because now there are a lot of variables, we will make models more stable by applying regularization to estimated coefficients - that is, assuming their prior distribution $\\mathcal{N}(0,1)$.  In `scikit-learn`, the class `Ridge` does this for linear regression. ","d2afac06":"This is the function for transforming the dataset and applying both models to it. ","c954fa4f":"In this notebook, we will explore some more things we can do about predicting credit card turnover.","88223089":"Split it into train and test sets for better evaluation of prediction quality","3b62f4cc":"# Estimating the quality of the baseline","2f34e8b4":"For logistic regression, amount of regularization is controlled with coefficient `C`","6b061e03":"Wow! the error has decreased a lot!","4a46cd3d":"Let's look at the data once more:","2d7ec576":"# Adding interactions between the features\nHere we are going to improve the model even more, by including pairwise products of all features - for the case if their combinations affect the target in an interesting way. \n\nNumber of features grows a lot: from 77 to 1235 variables. Without regularization, our models would probably break now. ","0892a8a0":"We write a function for transforming both train and test datasets into the same expanded form. ","55dff242":"We start by creating some variables we are going to use in two models","18595dbb":"# Multivariate models\n\nHere we try to increase prediction quality by using all the available variables for both models. \n\nAll the categorical variables will be encoded in binary format. The output of the cell below lists the resulting set of numerical variables, usable for both linear and logistic regression"}}