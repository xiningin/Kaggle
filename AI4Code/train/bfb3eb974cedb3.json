{"cell_type":{"ad6f5003":"code","353e183e":"code","9b86be8b":"code","4df55099":"code","a390e99c":"code","a5bfb35d":"code","c9e512e9":"code","efe815cb":"code","2844a8d3":"code","cd6fb89c":"code","286135c6":"code","f26113b2":"code","1f3a7c60":"code","ced1ad13":"code","10431ede":"markdown","6bcabe05":"markdown","fd4fa4e4":"markdown","fd5fda0d":"markdown","aface9fd":"markdown","db21356b":"markdown","ef4b09e1":"markdown","01d19d8e":"markdown","84b11d93":"markdown","59bc378f":"markdown","06c59604":"markdown","2ae2319f":"markdown","d4e00942":"markdown","dd855162":"markdown","e993d5e4":"markdown","47bb4014":"markdown","b7244ce8":"markdown","a44f22f7":"markdown"},"source":{"ad6f5003":"import numpy as np\nimport pandas as pd\nimport os\nfrom sklearn.ensemble import RandomForestClassifier\nimport xgboost\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score\nfrom xgboost import XGBClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.linear_model import LogisticRegression","353e183e":"train = pd.read_csv('..\/input\/titanic\/train.csv')\ntest = pd.read_csv(\"..\/input\/titanic\/test.csv\")\n\ntrain_features = pd.read_csv('..\/input\/data-w-features\/train (1).csv')\ntest_features = pd.read_csv('..\/input\/data-w-features\/test (1).csv')","9b86be8b":"train.head()","4df55099":"train_features.head()","a390e99c":"train['Sex'] = train['Sex'].astype('category').cat.codes\ntest['Sex'] = test['Sex'].astype('category').cat.codes\ntrain['Embarked']  = train['Embarked'].astype('category').cat.codes\ntest['Embarked']  = test['Embarked'].astype('category').cat.codes","a5bfb35d":"x = train.drop(['Survived','Name','Ticket','Cabin'],axis = 1)\ny = train.Survived\ntrain_x,val_x,train_y,val_y = train_test_split(x,y,random_state = 242)\nmodel = XGBClassifier()\nmodel.fit(train_x,train_y)\npreds = model.predict(val_x)\nprint(accuracy_score(val_y,preds))","c9e512e9":"train['Age'] = train['Age'].fillna(train['Age'].mean())\ntrain['Embarked'] = train['Embarked'].fillna(train['Embarked'].mode())\nx = train.drop(['Survived','Name','Ticket','Cabin'],axis = 1)\ny = train.Survived\ntrain_x,val_x,train_y,val_y = train_test_split(x,y,random_state = 242)\nforest = RandomForestClassifier(random_state=2)\nforest.fit(train_x,train_y)\nforest_preds=forest.predict(val_x)\nforest_score = accuracy_score(forest_preds,val_y)\nprint(forest_score)","efe815cb":"neigh = KNeighborsClassifier (n_neighbors=5)\nneigh.fit(train_x,train_y)\nneigh_preds = neigh.predict(val_x)\n#neigh_preds = np.round(neigh_preds,0,out=None)\n#neigh_preds.astype(int)\nneigh_score = accuracy_score(neigh_preds,val_y)\nprint(neigh_score)","2844a8d3":"logs = LogisticRegression()\nlogs.fit(train_x,train_y)\nlogs_preds = logs.predict(val_x)\nlogs_preds = np.round(logs_preds,0,out=None)\nlogs_preds.astype(int)\nlogs_score = accuracy_score(logs_preds,val_y)\nprint (logs_score)","cd6fb89c":"x = train_features.drop(['Survived'],axis = 1)\ny = train_features.Survived\ntrain_x,val_x,train_y,val_y = train_test_split(x,y,random_state = 242)","286135c6":"model = XGBClassifier()\nmodel.fit(train_x,train_y)\npreds = model.predict(val_x)\nprint(accuracy_score(val_y,preds))","f26113b2":"forest = RandomForestClassifier(random_state=2)\nforest.fit(train_x,train_y)\nforest_preds=forest.predict(val_x)\nforest_score = accuracy_score(forest_preds,val_y)\nprint(forest_score)","1f3a7c60":"neigh = KNeighborsClassifier (n_neighbors=5)\nneigh.fit(train_x,train_y)\nneigh_preds = neigh.predict(val_x)\n#neigh_preds = np.round(neigh_preds,0,out=None)\n#neigh_preds.astype(int)\nneigh_score = accuracy_score(neigh_preds,val_y)\nprint(neigh_score)","ced1ad13":"logs = LogisticRegression()\nlogs.fit(train_x,train_y)\nlogs_preds = logs.predict(val_x)\nlogs_preds = np.round(logs_preds,0,out=None)\nlogs_preds.astype(int)\nlogs_score = accuracy_score(logs_preds,val_y)\nprint (logs_score)","10431ede":"**XGBClassifier**","6bcabe05":"**LogisticRegression**","fd4fa4e4":"Let's load in the data and take a look at the features we are working with.","fd5fda0d":"# **The Data**","aface9fd":"**RandomForestClassifier**","db21356b":"**LogisticRegression**","ef4b09e1":"All of the models except the LogisticRegression increased. Suprisingly, the LogisticRegression actually dropped drastically which is indicative that LogisticRegression may not do well with the null values and extra features. We also are able to hit our first >0.8 score with XGBoost by just using the base model.","01d19d8e":"# **Testing Models With Normal Data**","84b11d93":" In order for the data to be trainable, I will have to assign categorical coding for the \"Sex\" and \"Embarked\"features. ","59bc378f":"**RandomForestClassifier**","06c59604":"Feature engineering is a very important concept in machine learning. The amount of features and data that you have are essential to how well your model does. I will be testing how well the features created from [this notebook](https:\/\/www.kaggle.com\/nzhongahtan\/feature-engineering-from-the-titanic-dataset). Please check out this notebook if you want more details on how the features were created. I will be testing an XGBoostClassifier, RandomForestClassifier, KNeighborsClassifier, and LogisticRegression.","2ae2319f":"**KNeighborsClassifier**","d4e00942":"Right off the bat, we can see how the original dataset has only 11 features, 3 of which are string features which have to be removed because all of the models we are training cannot handle the string data without more data extraction. So we have 8 workable features, versus almost 29 features in the new train features. On the other hand, the data on the extra features data has been one-hot encoded and there are some very interesting features created such as \"Nanny\" and \"TicketNum\". ","dd855162":"**XGBoostClassifier**","e993d5e4":"As we can see 3\/4 of the models score around 0.78 with the KNeighborsClassifier scoring even lower. Let's see if we can improve these scores with more features.","47bb4014":"# **Comparing The Effect of Extra Features on Predictions**","b7244ce8":"# **Testing Models With Extra Features**","a44f22f7":"**KNeighborsClassifier**"}}