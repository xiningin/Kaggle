{"cell_type":{"29eb159c":"code","9b11b29d":"code","c203ebce":"code","c441ca6c":"code","8b38b291":"code","685d3fe8":"code","3d8668df":"code","c77d0975":"code","55a32932":"code","edd05bc7":"code","c1adbfd1":"code","9c8f0f8d":"code","fec07b9a":"code","8d77f274":"code","c8fe6511":"markdown","6027f9b1":"markdown","ca2337cb":"markdown","59e4d8de":"markdown","11b68e98":"markdown","6d436098":"markdown","07295875":"markdown","5c52a460":"markdown","2f21110a":"markdown"},"source":{"29eb159c":"\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport seaborn as sns #easy plotting\nimport matplotlib.pyplot as plt #extra features for plotting\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import train_test_split\nimport scipy.stats as st\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","9b11b29d":"df = pd.read_csv('\/kaggle\/input\/30-days-of-ml\/train.csv')\ndf.head()","c203ebce":"print('There are ', df.shape[0], 'instances')\nprint('Missing values per column:')\ndf.isna().sum()","c441ca6c":"sns.set(rc={'figure.figsize':(11.7,8.27)})\nsns.heatmap(df.corr(), annot = True)\nplt.title('Correlations between Numerical Features')","8b38b291":"fig, axes = plt.subplots(2, 5, figsize=(18, 10))\n\nfig.suptitle('Cathegorical Features vs Target')\n\nsns.scatterplot(ax=axes[0, 0], data=df, x='cat0', y='target', alpha = 0.3)\nsns.scatterplot(ax=axes[0, 1], data=df, x='cat1', y='target', alpha = 0.3)\nsns.scatterplot(ax=axes[0, 2], data=df, x='cat2', y='target', alpha = 0.3)\nsns.scatterplot(ax=axes[0, 3], data=df, x='cat3', y='target', alpha = 0.3)\nsns.scatterplot(ax=axes[0, 4], data=df, x='cat4', y='target', alpha = 0.3)\nsns.scatterplot(ax=axes[1, 0], data=df, x='cat5', y='target', alpha = 0.3)\nsns.scatterplot(ax=axes[1, 1], data=df, x='cat6', y='target', alpha = 0.3)\nsns.scatterplot(ax=axes[1, 2], data=df, x='cat7', y='target', alpha = 0.3)\nsns.scatterplot(ax=axes[1, 3], data=df, x='cat8', y='target', alpha = 0.3)\nsns.scatterplot(ax=axes[1, 4], data=df, x='cat9', y='target', alpha = 0.3)","685d3fe8":"fig, axes = plt.subplots(2, 3, figsize=(18, 10))\n\nfig.suptitle('Cathegorical Features vs Target')\n\nsns.boxplot(ax=axes[0, 0], data=df, x='cat3', y='target', hue = df.cat3)\nsns.boxplot(ax=axes[0, 1], data=df, x='cat4', y='target', hue = df.cat4)\nsns.boxplot(ax=axes[0, 2], data=df, x='cat5', y='target', hue = df.cat5)\nsns.boxplot(ax=axes[1, 0], data=df, x='cat6', y='target', hue = df.cat6)\nsns.boxplot(ax=axes[1, 1], data=df, x='cat7', y='target', hue = df.cat7)\nsns.boxplot(ax=axes[1, 2], data=df, x='cat8', y='target', hue = df.cat8)","3d8668df":"print('Categories present before encoding: ')\ndf.cat0.unique(), df.cat1.unique(), df.cat2.unique()","c77d0975":"df.cat0 = df.cat0.map(dict(B=1, A=0))\ndf.cat1 = df.cat1.map(dict(B=1, A=0))\ndf.cat2 = df.cat2.map(dict(B=1, A=0))\n\nprint('Categories present after encoding: ')\ndf.cat0.unique(), df.cat1.unique(), df.cat2.unique()","55a32932":"counts = df.cat4.value_counts()\ncounts","edd05bc7":"df['cat4'] = df['cat4'].map(counts)\ndf.cat4","c1adbfd1":"X = df.drop(columns = 'target')\ny = df.target\nX_train, X_test, y_train, y_test = train_test_split(X,y, test_size = 0.2, random_state = 42)","9c8f0f8d":"cats = df.cat8.unique()","fec07b9a":"interval_mean = []\nfor i in cats:\n    y = y_train[X_train['cat8'] == i]\n    X_temp = X_train[X_train['cat8'] == i]\n    inter = st.t.interval(0.99, len(y)-1, loc=np.mean(y), \n                          scale=st.sem(y))\n    inter_min = inter[0]\n    inter_max = inter[1]\n    mean_int = (inter_min + inter_max)\/2\n    interval_mean = np.append(interval_mean, mean_int)\n\nprint(cats,interval_mean)","8d77f274":"df.cat8 = df.cat8.replace(cats, interval_mean)\ndf.cat8.unique()","c8fe6511":"<a id=\"enc\"><\/a>\n### Encoding\n\nBased on the information adquired before, let's tackle the Encoding problem. I'll start with the Binary categories.\n\n<a id=\"onehot\"><\/a>\n#### One-Hot Encoding\n\nThis name comes from assigning a True(1) value for each category at a time. If we had 5 categories, for example, a One-Hot Encoding over that feature would create 4 new features: Each with a 1 if True and 0 if False. You would define the last category implicitly: if the other 4 features are 0, means a new category.\n\nThe problem applying One-Hot Encoding to each and every categorical feature is that your dimensionality(number of features) will increase very quickly. Using this dataset as an example, imagine using One-Hot Encoding for all the categorical columns, we would end up having 70 or so features: this is known as **Curse of Dimensionality**.\n\nBut for Binary features(cat0, 1 and 2), it's a good starting point: you will replace one of the categories for 0 and the other for 1. As 0 means False and 1 means True, there is no scaled relation between those values: in theory, \u00a8\u00eft doesn't matter which is which**.\n\nThere are many python libraries that help with this task: LabelBinarizer, OrdinalEncoder, etc.\n\nI'm going to use pd.DataFrame.map.","6027f9b1":"<a id=\"ana\"><\/a>\n### Analyzing Distributions\n\nFrom the above plot, you can get **A LOT** of information.\nSome of the insights you might have:\n\n* There are categories(0, 1 and 2) that are **binary**: This means there are **two categories** in the feature: A and B, True and False, etc. are examples of binary categorization.\n* Others(like cat5) seem to have a relation between the values **target** can adopt and the category: depending of the category(A, B, C or D), target adopts a different **range** of values. \n* Features like cat3 have **many** instances on one cathegory, and looks like this **frequency** pattern reflects on the target, too.\n\n**Note**: This is not an exhaustive analysis. It's a way to introduce the different types of Encoding. Each person is different and might look at the data in different ways. Feel free to do your own, and let me know in the comments if it worked for you =).","ca2337cb":"<a id=\"rank\" ><\/a>\n### Rank-Hot Encoding\n\nOften, the category of a feature is related **directly** to it's output. It would be convenient to rank the categories, making the highest rank category match the one with the highest output value. \n\n**It's crucial not to \"look\" into your test or dev set** to seelct this rank, you would be **overfitting** your model.\n\n#### Confidence Intervals\n\nA confidence interval tells us in which range of target values it's certain to find our category. For example, if i have a 95% confidence interval = (-1,1) for category A, 95% perfecnt of the times, Category A will have an output between (-1,1).\n\nTo decide the rank positions, I'm going to calculate a 99% confidence interval for each category within the feature. Then, I'll use the mid point of it, and that will give me the measure for the rank position.","59e4d8de":"### Correlations\n\nNow, I want to know if there are any correlations between features. This can be very useful for taking decisions for **Feature Engineering**.\nI'll use Seaborn Heatmap for this. It's visual, easy to read, and in one line of code.\n\nIt won't display cathegorical data. It must be Encoded for this. We have many continuous features, so I'll do it now to see their correlations.\n<a id = \"corr\"><\/a>","11b68e98":"<a id=\"cat\"><\/a>\n### Categorical Data\n\nCategorical Data is, very often, in the form of **text**. Computers are not good at analyzing text directly, we need to transform it into numerical data. This is what **Encoding** is for.\n\nBefore attempting the Encoding phase, let's visualize the categorical features.\nSome of the answers I'm looking for are:\n* How many categories are per feature?\n* Are there any pattern present?","6d436098":"<a id=\"final\"><\/a>\n### Final Thoughts\n\nI've showed some of the Encoding techniques I came across so far. After all, finding patterns and developing useful features it's a huge part of making a good predictive model.\n\nIf you have any comments, things to add, thing to review, etc. just let me know.\nHope you enjoyed it.\n\nDrK~","07295875":"<a id='freq'><\/a>\n### Frequency Encoding\n\nSometimes, there is a relation between the target variable and the times a category is present in a certain feature. In that case, i would say **The frequency of a given category of a feature, it's related with the output value**. \nIf this is the case, a good starting point would be replacing this categories with its frequency. Depending on the model you're building, **Scaling may be needed**.\n\n#### Note\n\nThis may or may not lead to better performance while modeling. With this dataset, where classes are unbalanced and with similar distributions, this approach might not be the best one. But I haven't seen much around of it, so I'll do it anyways.","5c52a460":"# 30 Days of ML - Different types of Encoding\n\nHello Kagglers! In this notebook I'll be analyzing the **30 Days of ML** dataset. It has many categorical and numerical data, so it's perfect for the task.\n **Encoding** a feature means transforming it's **cathegorical** data to **numerical**. This is important since the ML models love numbers.\n \n Let's get started!\n \n* [Exploratory Data Analysis](#eda)\n    * [Correlations](#corr)\n* [Categorical Data: Distributions](#cat)\n    * [Analyzing Distributions](#ana)\n* [Encoding](#enc)\n    * [One-Hot Encoding](#onehot)\n    * [Frequency Encoding](#freq)\n    * [Rank-Hot Encoding](#rank)\n        * Confidence Intervals\n        \n        \n* [Final Thoughts](#final)","2f21110a":"<a id = \"eda\"><\/a>\n## Exploratory Data Analysis\n\nI'm going to import the data, and do some basic analysis. I want to know:\n* How many instances do I have?\n* How many features? Cathegorical or Numerical?\n* Are there any missing values?\n* Are there any Outliers? For a good Kaggle Kernel about this topic, visit  [Akash Dey's notebook][1].\n[1]: https:\/\/www.kaggle.com\/aimack\/how-to-handle-outliers\/comments#1465354"}}