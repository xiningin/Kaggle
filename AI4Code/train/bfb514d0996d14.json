{"cell_type":{"0e6579e0":"code","0139158b":"code","46f52d4d":"code","83207802":"code","6aabed6c":"code","c53fc223":"code","47164321":"code","60b2515f":"code","ab3816e5":"code","bf3601ea":"code","f02c0620":"code","c6234a34":"code","bcf1f8d6":"code","bd683027":"code","c5906bb0":"markdown"},"source":{"0e6579e0":"import numpy as np\nimport pandas as pd\nimport math \nimport random as rn\nimport matplotlib.pyplot as plt","0139158b":"data=pd.read_csv('..\/input\/iris-flower-dataset\/IRIS.csv')","46f52d4d":"X=data.iloc[:,0]\nY=data.iloc[:,1]\nplt.scatter(X,Y)\nplt.show()","83207802":"m=0\nc=0\nL=0.0001\nepochs=1000\nn=float(len(X))\nfor i in range(epochs): \n    Y_pred = m*X + c  # The current predicted value of Y\n    D_m = (-2\/n) * sum(X * (Y - Y_pred))  # Derivative wrt m\n    D_c = (-2\/n) * sum(Y - Y_pred)  # Derivative wrt c\n    m = m - L * D_m  # Update m\n    c = c - L * D_c  # Update c\n    \nprint (m, c)","6aabed6c":"Y_pred = m*X + c\n\nplt.scatter(X, Y) \nplt.plot([min(X), max(X)], [min(Y_pred), max(Y_pred)], color='red')  # regression line\nplt.show()","c53fc223":"df = pd.read_csv('..\/input\/iris-flower-dataset\/IRIS.csv')\ndf.head()","47164321":"X = df.iloc[0:100, [1, 2]].values\n\ny = df.iloc[0:100,4 ].values\n\ny = np.where(y == 'Iris-setosa', 1, 0)","60b2515f":"X_std = np.copy(X)\n\nX_std[:,0] = (X_std[:,0] - X_std[:,0].mean()) \/ X_std[:,0].std()\nX_std[:,1] = (X_std[:,1] - X_std[:,1].mean()) \/ X_std[:,1].std()","ab3816e5":"def sigmoid(X, theta):\n    \n    z = np.dot(X, theta[1:]) + theta[0]\n    \n    return 1.0 \/ ( 1.0 + np.exp(-z))","bf3601ea":"def lrCostFunction(y, hx):\n  \n    # compute cost for given theta parameters\n    j = -y.dot(np.log(hx)) - ((1 - y).dot(np.log(1-hx)))\n    \n    return j","f02c0620":"def lrGradient(X, y, theta, alpha, num_iter):\n    # empty list to store the value of the cost function over number of iterations\n    cost = []\n    \n    for i in range(num_iter):\n        # call sigmoid function \n        hx = sigmoid(X, theta)\n        # calculate error\n        error = hx - y\n        # calculate gradient\n        grad = X.T.dot(error)\n        # update values in theta\n        theta[0] = theta[0] - alpha * error.sum()\n        theta[1:] = theta[1:] - alpha * grad\n        \n        cost.append(lrCostFunction(y, hx))\n        \n    return cost","c6234a34":"m, n = X.shape\n\ntheta = np.zeros(1+n)\n\nalpha = 0.01\nnum_iter = 500\n\ncost = lrGradient(X_std, y, theta, alpha, num_iter)","bcf1f8d6":"plt.plot(range(1, len(cost) + 1), cost)\nplt.xlabel('Iterations')\nplt.ylabel('Cost')\nplt.title('Logistic Regression')","bd683027":"print ('\\n Logisitc Regression bias(intercept) term :', theta[0])\nprint ('\\n Logisitc Regression estimated coefficients :', theta[1:])","c5906bb0":"<h1 align=\"center\">Assignment<\/h1>\n<h2 align=\"center\">Faisal Akhtar<\/h2>\n<h2 align=\"center\">Roll No.: 17\/1409<\/h2>\n<p>Machine Learning - B.Sc. Hons Computer Science - Vth Semester<\/p>\n<p>Write a python program to implement linear regression and logistic regression using gradient descent algorithm. Remember the different cost functions.<\/p>"}}