{"cell_type":{"2af934fa":"code","4d0018dd":"code","8633a7cc":"code","4b30e71c":"code","762e6068":"code","00741d47":"code","6aff7cd6":"code","edc78769":"code","7ccfe079":"code","5737d4ac":"code","682df103":"code","0ee3a8fb":"code","cd9f5193":"code","ba21de5c":"code","f307fb2c":"code","fa0fb108":"code","688d4818":"code","761e7c87":"code","c4421704":"code","4a96439f":"code","bab51769":"code","ab948113":"code","c8ce41ab":"code","aa46a16d":"code","389ce796":"code","9e180167":"code","f96569c7":"code","86e28e6e":"markdown","8d243f3d":"markdown","1358434d":"markdown","7913d36c":"markdown","966e366f":"markdown","8326c751":"markdown","850ff072":"markdown","8c44508c":"markdown","f8eb76bf":"markdown","5c47f737":"markdown","6e95964b":"markdown","9b307cbc":"markdown","5cc0f8c8":"markdown","06f34fd9":"markdown","fdecc054":"markdown","13f2f09e":"markdown","44da0618":"markdown","267c428f":"markdown","d4a06777":"markdown","5e431305":"markdown","93a97594":"markdown","f97fb891":"markdown","4ef7b8c4":"markdown","c8b0dc23":"markdown","2cf7d104":"markdown","5317175f":"markdown","e17dfd93":"markdown"},"source":{"2af934fa":"import numpy as np \nimport cv2\nimport glob\nimport os\nimport matplotlib.pyplot as plt\nimport string\nfrom mlxtend.plotting import plot_decision_regions\nfrom mpl_toolkits.mplot3d import Axes3D\nfrom sklearn.decomposition import PCA\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeClassifier \nfrom sklearn.model_selection import train_test_split, cross_val_score\nfrom sklearn.utils.multiclass import unique_labels\nfrom sklearn import metrics\nfrom sklearn.svm import SVC\n\nprint(os.listdir(\"..\/input\"))\ndim = 100","4d0018dd":"def getYourFruits(fruits, data_type, print_n=False, k_fold=False):\n    images = []\n    labels = []\n    val = ['Training', 'Test']\n    if not k_fold:\n        path = \"..\/input\/*\/fruits-360\/\" + data_type + \"\/\"\n        for i,f in enumerate(fruits):\n            p = path + f\n            j=0\n            for image_path in glob.glob(os.path.join(p, \"*.jpg\")):\n                image = cv2.imread(image_path, cv2.IMREAD_COLOR)\n                image = cv2.resize(image, (dim, dim))\n                image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n                images.append(image)\n                labels.append(i)\n                j+=1\n            if(print_n):\n                print(\"There are \" , j , \" \" , data_type.upper(), \" images of \" , fruits[i].upper())\n        images = np.array(images)\n        labels = np.array(labels)\n        return images, labels\n    else:\n        for v in val:\n            path = \"..\/input\/*\/fruits-360\/\" + v + \"\/\"\n            for i,f in enumerate(fruits):\n                p = path + f\n                j=0\n                for image_path in glob.glob(os.path.join(p, \"*.jpg\")):\n                    image = cv2.imread(image_path, cv2.IMREAD_COLOR)\n                    image = cv2.resize(image, (dim, dim))\n                    image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n                    images.append(image)\n                    labels.append(i)\n                    j+=1\n        images = np.array(images)\n        labels = np.array(labels)\n        return images, labels\n    \ndef getAllFruits():\n    fruits = []\n    for fruit_path in glob.glob(\"..\/input\/*\/fruits-360\/Training\/*\"):\n        fruit = fruit_path.split(\"\/\")[-1]\n        fruits.append(fruit)\n    return fruits\n    ","8633a7cc":"#Choose your Fruits\nfruits = ['Pineapple' , 'Cocos'] #Binary classification\n\n#Get Images and Labels \nX_t, y_train =  getYourFruits(fruits, 'Training', print_n=True, k_fold=False)\nX_test, y_test = getYourFruits(fruits, 'Test', print_n=True, k_fold=False)\n\n#Get data for k-fold\nX,y = getYourFruits(fruits, '', print_n=True, k_fold=True)\n\n#Scale Data Images\nscaler = StandardScaler()\nX_train = scaler.fit_transform([i.flatten() for i in X_t])\nX_test = scaler.fit_transform([i.flatten() for i in X_test])\nX = scaler.fit_transform([i.flatten() for i in X])","4b30e71c":"def plot_image_grid(images, nb_rows, nb_cols, figsize=(15, 15)):\n    assert len(images) == nb_rows*nb_cols, \"Number of images should be the same as (nb_rows*nb_cols)\"\n    fig, axs = plt.subplots(nb_rows, nb_cols, figsize=figsize)\n    \n    n = 0\n    for i in range(0, nb_rows):\n        for j in range(0, nb_cols):\n            axs[i, j].axis('off')\n            axs[i, j].imshow(images[n])\n            n += 1        ","762e6068":"print(fruits[y_train[0]])\nplot_image_grid(X_t[0:100], 10, 10)","00741d47":"print(fruits[y_train[490]])\nplot_image_grid(X_t[490:590], 10, 10)","6aff7cd6":"def getClassNumber(y):\n    v =[]\n    i=0\n    count = 0\n    for index in y:\n        if(index == i):\n            count +=1\n        else:\n            v.append(count)\n            count = 1\n            i +=1\n    v.append(count)        \n    return v\n\ndef plotPrincipalComponents(X, dim):\n    v = getClassNumber(y_train)\n    colors = 'b', 'g', 'r', 'c', 'm', 'y', 'k', 'grey', 'orange', 'purple'\n    markers = ['o', 'x' , 'v', 'd']\n    tot = len(X)\n    start = 0 \n    if(dim == 2):\n        for i,index in enumerate(v):\n            end = start + index\n            plt.scatter(X[start:end,0],X[start:end,1] , color=colors[i%len(colors)], marker=markers[i%len(markers)], label = fruits[i])\n            start = end\n        plt.xlabel('PC1')\n        plt.ylabel('PC2')\n    \n    if(dim == 3):\n        fig = plt.figure()\n        ax = fig.add_subplot(111, projection='3d')\n        for i,index in enumerate(v):\n            end = start + index\n            ax.scatter(X[start:end,0], X[start:end,1], X[start:end,2], color=colors[i%len(colors)], marker=markers[i%len(markers)], label = fruits[i])\n            start = end\n        ax.set_xlabel('PC1')\n        ax.set_ylabel('PC2')\n        ax.set_zlabel('PC3')\n\n\n    plt.legend(loc='lower left')\n    plt.xticks()\n    plt.yticks()\n    plt.show()\n\ndef plot_confusion_matrix(y_true, y_pred, classes,\n                          normalize=False,\n                          title=None,\n                          cmap=plt.cm.Blues):\n    \"\"\"\n    This function prints and plots the confusion matrix.\n    Normalization can be applied by setting `normalize=True`.\n    \"\"\"\n    if not title:\n        if normalize:\n            title = 'Normalized confusion matrix'\n        else:\n            title = 'Confusion matrix, without normalization'\n\n    # Compute confusion matrix\n    cm = metrics.confusion_matrix(y_true, y_pred)\n    # Only use the labels that appear in the data\n    classes = unique_labels(y_true, y_pred)\n    if normalize:\n        cm = cm.astype('float') \/ cm.sum(axis=1)[:, np.newaxis]\n\n    fig, ax = plt.subplots()\n    im = ax.imshow(cm, interpolation='nearest', cmap=cmap)\n    ax.figure.colorbar(im, ax=ax)\n    # We want to show all ticks...\n    ax.set(xticks=np.arange(cm.shape[1]),\n           yticks=np.arange(cm.shape[0]),\n           # ... and label them with the respective list entries\n           xticklabels=fruits, yticklabels=fruits,\n           title=title,\n           ylabel='True label',\n           xlabel='Predicted label')\n\n    # Rotate the tick labels and set their alignment.\n    plt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\",\n             rotation_mode=\"anchor\")\n\n    # Loop over data dimensions and create text annotations.\n    fmt = '.2f' if normalize else 'd'\n    thresh = cm.max() \/ 2.\n    for i in range(cm.shape[0]):\n        for j in range(cm.shape[1]):\n            ax.text(j, i, format(cm[i, j], fmt),\n                    ha=\"center\", va=\"center\",\n                    color=\"white\" if cm[i, j] > thresh else \"black\")\n    fig.tight_layout()\n    return cm,ax","edc78769":"pca = PCA(n_components=2)\ndataIn2D = pca.fit_transform(X_train)\nplotPrincipalComponents(dataIn2D, 2)","7ccfe079":"pca = PCA(n_components=3)\ndataIn3D = pca.fit_transform(X_train)\nplotPrincipalComponents(dataIn3D, 3)","5737d4ac":"def showPCA(image,X2, X10, X50):\n    fig = plt.figure(figsize=(15,15))\n    ax1 = fig.add_subplot(1,4,1)\n    ax1.axis('off')\n    ax1.set_title('Original image')\n    plt.imshow(image)\n    ax1 = fig.add_subplot(1,4,2)\n    ax1.axis('off') \n    ax1.set_title('50 PC')\n    plt.imshow(X50)\n    ax1 = fig.add_subplot(1,4,3)\n    ax1.axis('off') \n    ax1.set_title('10 PC')\n    plt.imshow(X10)\n    ax2 = fig.add_subplot(1,4,4)\n    ax2.axis('off') \n    ax2.set_title('2 PC')\n    plt.imshow(X2)\n    plt.show()\n\ndef computePCA(n, im_scaled, image_id):\n    pca = PCA(n)\n    principalComponents = pca.fit_transform(im_scaled)\n    im_reduced = pca.inverse_transform(principalComponents)\n    newImage = scaler.inverse_transform(im_reduced[image_id])\n    return newImage\n\ndef showVariance(X_train):\n    #Compute manually the principal components\n    cov_matr=np.dot(X_train, X_train.T)\n    eigval,eigvect=np.linalg.eig(cov_matr)\n\n    index=np.argsort(eigval)[::-1] #take in order the index of ordered vector (ascending order)\n\n    #eigvect[:,i] is associated to eigval[i] so \n    eigvect=eigvect[:,index]\n    eigval=eigval[index]\n\n    n_PC=[]\n    var_explained=[]\n    var_temp=[]\n    var_tmp=0\n    for i in range(10):\n        var_tmp=var_tmp+eigval[i]\n        n_PC.append(i)\n        var_temp.append(eigval[i]\/(eigval.sum())*100)\n        var_explained.append(var_tmp\/(eigval.sum())*100)\n\n    fig, ax = plt.subplots(figsize=(8,8))\n\n    ind = np.arange(10)    \n    width = 0.35         # the width of the bars\n    p1 = ax.bar(ind, var_temp, width, color='b')\n    p2 = ax.bar(ind + width, var_explained, width, color='r')\n\n    ax.legend((p1[0], p2[0]), ('Individual explained variance', 'Cumulative explained variance'))\n\n    ax.set_title('Variance explained using PCs')\n    ax.set_xticks(ind + width \/ 2)\n    ax.set_xticklabels(('1', '2', '3', '4', '5', '6', '7', '8', '9', '10'))\n\n    plt.xlabel('Number of PC')\n    plt.ylabel('Variance exaplained in %')\n\n    ax.autoscale_view()\n\n    plt.show()","682df103":"image_id = 2\nimage = X_t[image_id]\n\n#Compute PCA\nX_2 = computePCA(2, X_train,image_id)\nX_10 = computePCA(10, X_train,image_id)\nX_50 = computePCA(50, X_train,image_id)\n\n#Reshape in order to plot images\nX2 = np.reshape(X_2, (dim,dim,3)).astype(int)\nX10 = np.reshape(X_10, (dim,dim,3)).astype(int)\nX50 = np.reshape(X_50, (dim,dim,3)).astype(int)\n\n#Plot\nshowPCA(image, X2, X10, X50)","0ee3a8fb":"showVariance(X_train)","cd9f5193":"svm = SVC(gamma='auto', kernel='linear', probability=True)\nsvm.fit(X_train, y_train) \ny_pred = svm.predict(X_test)\n\n#Evaluation\nprecision = metrics.accuracy_score(y_pred, y_test) * 100\nprint(\"Accuracy with SVM: {0:.2f}%\".format(precision))\ncm , _ = plot_confusion_matrix(y_test, y_pred,classes=y_train, normalize=True, title='Normalized confusion matrix')\nplt.show()\n\n# calculate the FPR and TPR for all thresholds of the classification\nprobs = svm.predict_proba(X_test)\nprobs = probs[:, 1]\nsvm_fpr, svm_tpr, thresholds = metrics.roc_curve(y_test, probs)\nsvm_auc = metrics.roc_auc_score(y_test, probs)\n","ba21de5c":"pred_kfold = cross_val_score(svm, X, y, cv=5) \nprint(\"Accuracy with SVM and K-FOLD CROSS VALIDATION: %0.2f (+\/- %0.2f)\" % (pred_kfold.mean(), pred_kfold.std() * 2))","f307fb2c":"pca = PCA(n_components=2)\nX_train2D = pca.fit_transform(X_train)\nX_test2D = pca.fit_transform(X_test)\n\nsvm.fit(X_train2D, y_train) \ntest_predictions = svm.predict(X_test2D)\nprecision = metrics.accuracy_score(test_predictions, y_test) * 100\nprint(\"Accuracy with SVM considering only first 2PC: {0:.2f}%\".format(precision))\n\n#Plotting decision boundaries\nplot_decision_regions(X_train2D, y_train, clf=svm, legend=1)\nplt.xlabel('PC1')\nplt.ylabel('PC2')\nplt.title('Linear SVM Decision Boundaries')\nplt.show()","fa0fb108":"svm_with_kernel = SVC(gamma=0.01, kernel='rbf', probability=True)\nsvm_with_kernel.fit(X_train2D, y_train) \ny_pred = svm_with_kernel.predict(X_test2D)\nprecision = metrics.accuracy_score(y_pred, y_test) * 100\nprint(\"Accuracy with Not-Linear SVM considering only first 2PC: {0:.2f}%\".format(precision))\n\n#Plotting decision boundaries\nplot_decision_regions(X_train2D, y_train, clf=svm_with_kernel, legend=1)\nplt.xlabel('PC1')\nplt.ylabel('PC2')\nplt.title('Kernel SVM Decision Boundaries')\nplt.show()","688d4818":"knn = KNeighborsClassifier(n_neighbors=2)\nknn.fit(X_train, y_train)\ny_pred = knn.predict(X_test)\n#Evaluation\nprecision = metrics.accuracy_score(y_pred, y_test) * 100\nprint(\"Accuracy with K-NN: {0:.2f}%\".format(precision))\ncm , _ = plot_confusion_matrix(y_test, y_pred, classes=y_train, normalize=True, title='Normalized confusion matrix')\nplt.show()\n\n# calculate the FPR and TPR for all thresholds of the classification\nprobs = knn.predict_proba(X_test)\nprobs = probs[:, 1]\nknn_fpr, knn_tpr, thresholds = metrics.roc_curve(y_test, probs)\nknn_auc = metrics.roc_auc_score(y_test, probs)\n","761e7c87":"#KNN + K-FOLD\npred_kfold = cross_val_score(knn, X, y, cv=5) \nprint(\"Accuracy with K-NN and K-FOLD CROSS VALIDATION: %0.2f (+\/- %0.2f)\" % (pred_kfold.mean(), pred_kfold.std() * 2))","c4421704":"#CHANGING VALUES OF N\naccuracy_train = []\naccuracy_test = []\n\nfor i in range(1,15):   #check all possible values for 1 to 15\n    k_nn = KNeighborsClassifier(n_neighbors=i)\n    k_nn.fit(X_train,y_train)\n    pred_i = k_nn.predict(X_test)\n    accuracy_train.append(k_nn.score(X_train,y_train)*100)\n    accuracy_test.append(k_nn.score(X_test,y_test)*100)\n    \naccuracy_train_array=np.asarray(accuracy_train)\naccuracy_test_array=np.asarray(accuracy_test)\n    \nplt.figure(figsize=(10,6))\nplt.plot(range(1,15),accuracy_train_array, label='Training_Accuracy', color='blue')\nplt.plot(range(1,15),accuracy_test_array, label='Testing_Accuracy', color='red')\nplt.legend()\nplt.title('Accuracy vs K value')\nplt.xlabel('K')\nplt.ylabel('Accuracy%')\n\nplt.show()","4a96439f":"#K-NN + PCA\nknn.fit(X_train2D, y_train)\ny_pred = knn.predict(X_test2D)\nprecision = metrics.accuracy_score(y_pred, y_test) * 100\nprint(\"Accuracy with K-NN considering only first 2PC: {0:.2f}%\".format(precision))\n\n#Plotting decision boundaries\nplot_decision_regions(X_train2D, y_train, clf=knn, legend=1)\nplt.xlabel('PC1')\nplt.ylabel('PC2')\nplt.title('K-NN Decision Boundaries')\nplt.show()","bab51769":"tree = DecisionTreeClassifier()\ntree = tree.fit(X_train,y_train)\ny_pred = tree.predict(X_test)\n\n#Evaluation\nprecision = metrics.accuracy_score(y_pred, y_test) * 100\nprint(\"Accuracy with Decision Tree: {0:.2f}%\".format(precision))\ncm , _ = plot_confusion_matrix(y_test, y_pred, classes=y_train, normalize=True, title='Normalized confusion matrix')\nplt.show()\n\n# calculate the FPR and TPR for all thresholds of the classification\nprobs = tree.predict_proba(X_test)\nprobs = probs[:, 1]\ntree_fpr, tree_tpr, thresholds = metrics.roc_curve(y_test, probs)\ntree_auc = metrics.roc_auc_score(y_test, probs)","ab948113":"#DECISION TREE + K-FOLD\npred_kfold = cross_val_score(tree, X, y, cv=5) \nprint(\"Accuracy with DECISION TREE and K-FOLD CROSS VALIDATION: %0.2f (+\/- %0.2f)\" % (pred_kfold.mean(), pred_kfold.std() * 2))","c8ce41ab":"# CHANGING MAX_DEPTH\nscore_train=[]\nscore_test=[]\n\nfor i in range(1,10):\n    dtree_md = DecisionTreeClassifier(max_depth=i)\n    dtree_md.fit(X_train,y_train)\n    \n    score_train.append(dtree_md.score(X_train,y_train)*100)\n    score_test.append(dtree_md.score(X_test,y_test)*100)\n    \nscore_train_array=np.asarray(score_train)\nscore_test_array=np.asarray(score_test)\nplt.figure(figsize=(10,6))\nplt.plot(range(1,10),score_train_array,color='blue', label=\"Training_accuracy\")\nplt.plot(range(1,10),score_test_array,color='red',label=\"Testing_accuracy\")\n\nplt.legend()\nplt.xlabel('max_depth')\nplt.ylabel('Accuracy%')\nplt.show()","aa46a16d":"#DECISION TREE + PCA\ntree = tree.fit(X_train2D,y_train)\ny_pred = tree.predict(X_test2D)\nprecision = metrics.accuracy_score(y_pred, y_test) * 100\nprint(\"Accuracy with Decision Tree considering only first 2PC: {0:.2f}%\".format(precision))\n\n#Plotting decision boundaries\nplot_decision_regions(X_train2D, y_train, clf=tree, legend=1)\nplt.xlabel('PC1')\nplt.ylabel('PC2')\nplt.title('Decision Tree Decision Boundaries')\nplt.show()","389ce796":"#ROC CURVE\nplt.title('ROC Curve')\nplt.plot([0, 1], [0, 1], linestyle='--')\nplt.plot(svm_fpr, svm_tpr, 'b', marker='.', label = 'SVM = %0.3f' % svm_auc )\nplt.plot(knn_fpr, knn_tpr, 'g', marker='.', label = 'K-NN = %0.3f' % knn_auc)\nplt.plot(tree_fpr, tree_tpr, 'r', marker='.',label = 'DECISION TREE = %.3f' % tree_auc)\nplt.legend(loc = 'lower right')\nplt.ylabel('True Positive Rate')\nplt.xlabel('False Positive Rate')\nplt.show()","9e180167":"fruits = ['Orange', 'Banana' , 'Strawberry', 'Apple Golden 1', 'Kiwi' , 'Lemon', 'Cocos' , 'Pineapple' , 'Peach', 'Cherry 1', 'Cherry 2', 'Mandarine']\n#fruits = getAllFruits() #Be sure to have enough free memory\n\n#Get Images and Labels\nX, y =  getYourFruits(fruits, 'Training')\nX_test, y_test = getYourFruits(fruits, 'Test')\n\n#Scale Data Images\nscaler = StandardScaler()\nX_train = scaler.fit_transform([i.flatten() for i in X])\nX_test = scaler.fit_transform([i.flatten() for i in X_test])","f96569c7":"#SVM\nmodel = SVC(gamma='auto', kernel='linear')\nmodel.fit(X_train, y) \ny_pred = model.predict(X_test)\nprecision = metrics.accuracy_score(y_pred, y_test) * 100\nprint(\"Accuracy with SVM: {0:.2f}%\".format(precision))\n\n#K-NN\nmodel = KNeighborsClassifier(n_neighbors=5)\nmodel.fit(X_train, y)\ny_pred = model.predict(X_test)\nprecision = metrics.accuracy_score(y_pred, y_test) * 100\nprint(\"Accuracy with K-NN: {0:.2f}%\".format(precision))\n\n#DECISION TREE\nmodel = DecisionTreeClassifier()\nmodel.fit(X_train,y)\ny_pred = model.predict(X_test)\nprecision = metrics.accuracy_score(y_pred, y_test) * 100\nprint(\"Accuracy with Decision Tree: {0:.2f}%\".format(precision))","86e28e6e":"## VARIANCE EXPLAINED USING PC","8d243f3d":"Accuracy on Training phase increase while the Accuracy on Test phase is decreasing, this means that the model **overfit** increasing the max depth of the tree.","1358434d":"## DATA IN LOWER DIMENSIONS\nIn order to discover how our data appears in lower dimension we need to reduce dimensionality of the dataset in 2 or 3 dimension so that we can plot and visualize them. To do this I've decided to use Principal Component Analysis, explained in the next chapter, but a better solution could be use t-SNE (T-distributed Stochastic Neighbor Embedding) or MDS (Multi Dimensional Scaling), nonlinear dimensionality reduction techniques well-suited for embedding high-dimensional data for visualization in a low-dimensional space of two or three dimensions.<br> \nNo assumption about linearity of the dataset could be make considering those graphs.","7913d36c":"#### COMMENT\nFrom those images is possible to understand how considering only a sample and obtaining its principal components from the whole dataset, it's possible for **us** to classify easly the fruit just considering a low number dimension instead of all. This means a lot of data less.<br>\nObviously for a classification algorithm accuracy of the classification will be lower but instead training time will be faster, if the classes were linearly separable the accuracy could be satisfactory.","966e366f":"<br> \n# K-NEAREST NEIGHBOR\n\nK-NN is a supervised learning method that considers the K closest training examples to the point of interest for predicting its class. The point is assigned to the class that is closest. <br>\nCould be applied different distance metrics such as: Euclidian, Weighted, Gaussian, etc.\nSteps are pretty easy:<br>\n\n* \u200aReceive an unclassified data\n\n*  Measure the distance with choosen metrics from the new data to all others data that are already classified.\n\n* \u200aGets the K smaller distances\n\n* \u200aCheck the list of classes that had the shortest distance and count the amount of each class that appears\n\n*  Takes as correct class the class that appeared the most times\n\n*  Classifies the new data with the class that you took in previous step \n<br>","8326c751":"# VISUALIZATION OF DATA\nLet's see now how one of our samples appears","850ff072":"#### COMMENT\n\nFrom this graph is possible to understand how the best value of K is equal to **2**, because the Test Accuracy reaches the best accuracy score and then start decreasing. \n\nTraining accuracy still maintain 100% accuracy starting decreasing for last numbers of K ","8c44508c":"### DATA IN 3D","f8eb76bf":"#### COMMENT\n\nAs we can see from this graph the first PC captures only 16-17% of variance, using first two the percentage of information captured is less than **30%**. \n\nIn the next section I'll perform also classification using only first two dimension, this is not to get a good accuracy, because with so low information classifiers will not be able to learn so much, but instead is made in order to show **decision boundaries** of classifiers when using two dimensions. \n\n\nUsing first 10PC, variance captured is **40%**, not a bad results considering that the dataset has hundreds of dimension.\n\n\n","5c47f737":"# BINARY CLASSIFICATION\nLet's start now to classify our dataset, firstly we take only two class in order to perform a classic binary classification, at the end the entire dataset (or a sub-part of it) will be classified. \nI'll use 3 different technique: **SVM**, **K-NN**, **Decision Tree**.<br>\nAt the end of the Binary Classification there will be a comparison between all the methods.","6e95964b":"#### SVM + K-FOLD","9b307cbc":"### COMMENT \nSVM is the classification algorithm that performs better in the multi-class classification task.","5cc0f8c8":"<br>\n# SUPPORT VECTOR MACHINES\nA Support Vector Machine (SVM) is a supervised classification method, that after a training\nphase can identify if a new point belongs to a class or another with the highest\nmathematically accuracy.\nIt's a binary classification method, but using an approach called One vs All is possible to use SVM for multi-class classification.\n\nIf a dataset is linearly separable it means that we could use a Hard margin approach, or rather find the two parallel hyperplanes that separate the two classes of data, so that the distance between them is as large as possible and so we are being able to identify to which class belongs each point of the dataset. But mostly of the time datasets are not linearly\nseparable and so we can take two ways, one is to continue using a Linear approach using Soft Margin, or simplifying, admitting some misclassification. While the second way is to use a Non-Linear Kernel (that must satisfy Mercer Condition) or rather a mapping of the data on a higher dimensional space, where the data is linearly separable and the classification task can be solved easly,without even need to calculate the points projections.\n\n\nGeneral solution | Optimal solution\n- | - \n![](https:\/\/cdn-images-1.medium.com\/max\/720\/0*9jEWNXTAao7phK-5.png) | ![](https:\/\/cdn-images-1.medium.com\/max\/720\/0*0o8xIA4k3gXUDCFU.png)\n\n<br><br>\n\nIf we want to use a Linear Soft-Margin approach the optimization task is to find a  margin that should be as big as possible and we need then to add a penalty if a point is misclassified.\nThis is made by adding to the optimization problem another term of Loss, regularizated by a slack variable C. This term will say how well we want to fit our data and how many mistakes we grant to do.\n\nThe Loss function used  for this problem is the Hinge Loss. <br>\n\n$$ L(y, f(x)) = \\max ( 0 , 1 - y \u00b7 f(x) ) $$\n\n![Hinge Loss Function](https:\/\/i.stack.imgur.com\/Ifeze.png)\n\n<br>\nThe optimization problem to solve will be: \n\n$$ minimize \\frac{1}{2} ||{w}||^2 + C \\sum_{i=1}^n \\xi_i  \n   \\;\\;\\;,\\;subject\\;to\\;\\;\\;y_i[x_i\u00b7w + b] \\geq 1 - \\xi_i $$\n\n<br> Where $\\frac{1}{||w||}$ is margin size, C our hyperparameter and $\\xi_i$ the distance from the point to the boundarie line.<br>\n\nAt the end we need to find the right trade-off between margin and penalty.\nSo, the only parameter that we can choose is C because the optimization problem and the calculus are committed to the calculators.<br> Kernel type could be set to linear if the problem is linearly separable, otherwise a Gaussian kernel (RBF) fit well most of the time. Gamma is a kernel hyperparameter that tries to exactly fit the training data.","06f34fd9":"# FRUIT CLASSIFICATION\n## DEVELOPED BY WALTER MAFFIONE <br>\nAt the moment of this work (05-2019) this dataset contains 103 class of different fruits and 53177 total images.\nMy idea is to perform different classification algorithms, in particular SVM, K-NN, Decision Tree, firstly for a binary classification task, then for a multi-class one. \nI'll also apply Principal Component Analysis in order to reduce the dimensionality of the dataset, see the variance of each class and then I'll try to apply classification algorithm having only two dimension. \nAt the end I'll make a comparison between all methods in order to find which of them perform better on this dataset.\n\nIn the future I'd like to implement a CNN, out of the purpose of this work, that almost surely will be a solution better than others in terms of accuracy, but for the moment I limit myself to the mentioned algorithms.","fdecc054":"### COMMENT:\nSVM and K-NN performs better with this classification with an AUC = 0.99x, instead Decision Tree is worst with an AUC of 0.65 ","13f2f09e":"#### LINEAR SVM","44da0618":"# MODEL EVALUATION\n\nIn order to find the most suitable algorithm to this dataset, different evaluation methods will be presented: **Accuracy, Confusion Matrix, ROC Curve.**\n<br>\nGiven: \n* TP = #samples for which the prediction is Fruit1 and the true label is Fruit1\n* FP = #samples for which the prediction is Fruit2 but the true label is Fruit1\n* TN = #samples for which the prediction is Fruit2 and the true label is Fruit2\n* FN = #samples for which the prediction is Fruit1 but the true label is Fruit2\n\nWe can define: \n\n* **ACCURACY: $\\frac{TP+TN}{TP+FP+TN+FN}$ that is the percentage of samples classified correctly. **\n\n* **CONFUSION MATRIX:  A simple table with previous values used to show performance of a classifier**\n\n* **ROC CURVE: Area Under the Receiver Operating Characteristic curve (AUC)**<br><br>\nTo introduce this concept, we define the following two metrics:<br>\n\n    * **True positive rate (TPR):**   TPR = recall = $\\frac{TP}{FN+TP}$<br><br>\n    \n    * **False positive rate (FPR):**   FPR = $\\frac{FP}{TN+FP}$<br>\n    \nIn order to plot the Receiver Operating Characteristic (ROC) curve we need to compute TPR and FPR and choose a number of thresholds for the classification (AUG). Area under the ROC curve, performed plotting TPR and FPR is used as evaluation matrics for the different classifiers.","267c428f":"## PCA EXAMPLE","d4a06777":"### DATA IN 2D","5e431305":"# DECISION TREE\nIn a decision tree each intermediate node of the tree contains splitting attributes used to build different paths, while leaves contains class labels.\n\nThere are differt algorithms to build a decision tree, all are made with a greedy approach, optimal locally.<br>The most famous is **Hunt's algoritm**. <br>\n\n![](https:\/\/cdn-images-1.medium.com\/max\/880\/0*QctkHiOX2G2pvfD_.jpg)\n\nStrarting from an empty tree, we need to find iteratively best attribute on which split the data locally at each step. If a subset contains records that belongs to the same class then the leaf containing such class label is created, otherwise if a subset is empty is assigned to default to mayor class.\n\nCritical points of decision trees are test condition, the selection of the best attribute and the splitting condition. \nFor the selection of the best attribute is generally choosen the attribute that generate homogeneus nodes. \nThere are different metrics in order to find the best splitting homogenity, the most common are:\n* **GINI IMPURITY INDEX**: Given **$n$** classes and $p_i$ the fraction of items of class $i$ in a subset p, for $i$\u2208{1,2,...,n}. Then the GINI index is defined as: $$ GINI = 1 \u2212 \\sum_{i=1}^n p_i^2 $$\n\n* **INFORMATION GAIN RATIO**: The information gain is based on the decrease of entropy after a data-set is split on an attribute. Constructing a decision tree is all about finding attribute that returns the highest information gain (i.e., the most homogeneous branches).<br>Entropy is defined as $H(i) = -\\sum_{i=1}^n p_i\\log_2 p_i $.<br> So then Information gain is defined as: \n\n$$ IG = H(p) - H(p,i)  = H(p) - \\sum_{i=1}^n \\frac{n_i}{n} H(i) $$\n\nwhere p is the parent node.\nAdvantages of Decision Trees are velocity, easy to interpretate and good accuracy, but they could be affected by missing values.","93a97594":"#### KERNEL SVM + PCA","f97fb891":"## K-FOLD CROSS VALIDATION\n\nDataset is divided yet in Training and Test set by the authors of the dataset it self.<br>\nIn proportion approximately **75% Training images, 25% Test images**.<br>\nModels will be trained considering only Training set and then Test set will be used in order to evaluate their performance in terms of accuracy.<br>\nThis approach not always the best choice, because due to sample variability between training and test set, our model could gives a better prediction on training data but fail to generalize on test data; and the subset choosen could have bias and not be representative of the entire dataset.\n\nFrom this problem comes the technique of **Cross-validation**.<br>Cross-validation is a statistical technique which involves **partitioning the data into subsets**, **training the data on a subset and use the other subset to evaluate the model\u2019s performance.**\nTo reduce variability we perform multiple rounds of cross-validation with different subsets from the same data.\nWe combine the results from these multiple rounds to come up with an estimate of the model\u2019s predictive performance.<BR>\nCross-validation will give us a more accurate estimate of a model\u2019s performance.\n    \n**K-FOLD Cross Validation** in particular involves randomly dividing the dataset into k groups or folds of approximately equal size.<br> The first fold is kept for testing and the model is trained on k-1 folds.<br>\nThe process is repeated K times and each time different fold or a different group of data points are used for validation.<BR>\n![](https:\/\/i.imgur.com\/WPHsss9.png)","4ef7b8c4":"#### LINEAR SVM + PCA","c8b0dc23":"# PRINCIPAL COMPONENT ANALYSIS\nPrincipal Component Analysis is a technique used in order to reduce the dimensionality of a dataset while preserving as mush information as possible. Data is reprojected in a lower dimensional space, in particular we need to find a projection that minimizes the squared error in reconstructing the original data. <br>\n\n![Principal Component Analysis](https:\/\/sebastianraschka.com\/images\/faq\/lda-vs-pca\/pca.png)\n\n<br>\nThere are 3 different technique in order to apply PCA:\n1. **Sequential**  \n2. **Sample Covariance Matrix**\n3. **Singular Value Decomposition (SVD)** <br>\n\nI'll explain the Sample Covariance Matrix technique:\n* The first thing to do is to standardize the data, so for each sample we need to substract the mean of the full dataset and then divide it by the variance, so as having an unitary variance for each istance. This last process is not completly necessary but it is usefull to let the CPU work less.\n$$\n    Z = \\frac{X-\\mu}{\\sigma^2} \n$$\n<br>\n* Then we need to compute Covariance Matrix, given data { $x_1 ,x_2, ..., x_n$ } with $n$ number of samples, covariance matrix is obtained by:<br><br>\n$$\n\\Sigma = \\frac {1}{n}\\sum_{i=1}^n (x_i - \\bar{x})(x - \\bar{x})^T $$    $\\;\\;$  where  $$\\bar{x} = \\frac {1}{n}\\sum_{i=i}^n x_i $$ <br>\nOr simply by multiplying the standardized matrix Z by it self transposed<br>\n$$ COV(X) = Z Z^T $$<br>\n\n* Principal Components will be the eigenvectors of the Covariance Matrix sorted in order of importance by the respective eigenvalues.<br>**Larger eigenvalues $\\Rightarrow$ more important eigenvectors.**<br> They represent the most of the useful information on the entire dataset in a single vector","2cf7d104":"# MULTI-CLASS CLASSIFICATION ","5317175f":"## CHOOSE YOUR CLASS\nI've implemented two function in order to choose the fruit you want, for binary classification I've decided to take **Cocos** and **Pineapple** cause they look similar and so the classification task will be not too easy.","e17dfd93":"Each image is converted in a 100x100 numpy array for each RGB dimension (x3). Then has been flatted in one single vector (Image Features Vector) and then scaled subtracting the mean of the dataset in order to perform classification algorithms.\n\nIn the image below the last matrix is our **X_train**, with a shape of 30000x980\n\n![](https:\/\/i.imgur.com\/lC1pcrV.jpg)"}}