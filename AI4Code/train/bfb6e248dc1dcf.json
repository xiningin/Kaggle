{"cell_type":{"41cfcdfe":"code","f8d445df":"code","cdb96d22":"code","40278127":"code","58c8a894":"code","53df3375":"code","412e0b25":"code","a9f817d1":"code","0e7ec5d2":"code","32e3b7f4":"code","dd4beef7":"code","668a4277":"code","e5f0407d":"code","8c45642d":"code","ddc25692":"code","72a9f432":"code","caaa0450":"code","d3d15f19":"code","cd53e2a9":"code","1251629b":"code","132259a7":"code","5afb0e6f":"code","c9093bb6":"code","f4a2e8d5":"code","c11ac969":"code","9c0993da":"code","b05ae2f1":"code","03d1ec06":"code","6239fa6c":"code","7ebfd704":"code","3cadd029":"code","f562bb9b":"code","aaddcd59":"code","16b7f7ca":"code","c3cc803e":"code","8f4a2efb":"code","87233788":"code","633d805c":"code","ae191652":"code","39289565":"code","5ca5b960":"markdown","6864ca33":"markdown","820f1745":"markdown","0e89dc2d":"markdown","fedfeb88":"markdown","4e7a764f":"markdown","12b41883":"markdown","5542ba47":"markdown","2396c087":"markdown","bc085c99":"markdown","4097aff1":"markdown","0db3627d":"markdown","0376f90d":"markdown","acd3ba77":"markdown","3fcda8ad":"markdown"},"source":{"41cfcdfe":"# Importing Libraries\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport matplotlib\n%matplotlib inline\n\nSEED=42\n\n# Importing Preprocessing Library\nfrom sklearn.preprocessing import OrdinalEncoder\n\n\n# Importing Model Selection Library\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import KFold\n\n\n#Model\nfrom sklearn.neighbors import KNeighborsRegressor\n\n# Accuracy Metrics\nfrom sklearn.metrics import r2_score\n\nplt.rcParams['xtick.labelsize']=11\nplt.rcParams['ytick.labelsize']=11\n\n\nimport missingno as miss\n\nimport warnings\nwarnings.simplefilter(action='ignore', category=FutureWarning)\n\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.preprocessing import LabelEncoder\n\n\nfrom sklearn.metrics import accuracy_score","f8d445df":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","cdb96d22":"df = pd.read_csv('\/kaggle\/input\/breast-cancer-wisconsin-data\/data.csv')\ndf.head()","40278127":"print('The shape of the Wisconsin Breast Cancer Dataset is {}'.format(df.shape))","58c8a894":"# Visualizing the Matrix Plot\n\nfig = plt.figure(figsize=(20,8))\nax1 = fig.add_subplot(1,1,1)\nmiss.matrix(df, labels=True, fontsize=12, ax=ax1, sparkline=False)\nax1.set_title('Matrix plot for missing values in the Cancer dataset', size=20, color='red')","53df3375":"y = df['diagnosis']\nX = df.drop(['diagnosis', 'id','Unnamed: 32'], axis=1)","412e0b25":"# count plot \nsns.countplot(x='diagnosis',data=df, palette='Set2')\nbenign, malignant = y.value_counts()\nprint('Total Benign counts in the Cancer datatset {}'.format(benign))\nprint('Total Malignant counts in Cancer datatset {}'.format(malignant))","a9f817d1":"num_var = [var for var in X.columns if X[var].dtype != 'O']\nprint(num_var)","0e7ec5d2":"import matplotlib.pyplot as plt\nplt.figure(figsize=(20,20))\nplt.tight_layout()\nimport matplotlib.pyplot as plt\nfor key, value in enumerate(num_var):\n  plt.subplot(6,5,key+1)\n  g = sns.histplot(X[value], color='m', label= 'Skewness: {:.2f}'.format(X[value].skew()), kde=True)\n  plt.legend(loc='best')\n  plt.suptitle('Distribution plot of Numerical Features', size=14).set_position([.5, 1.02])\n  plt.tight_layout()","32e3b7f4":"plt.figure(figsize=(30,20))\nfor key, value in enumerate(num_var):\n  plt.subplot(5,6,key+1)\n  g = sns.kdeplot(data=df, x=value, hue='diagnosis', shade=True)\n  plt.suptitle('KDE (Kernel Density Estimation) Plot of Numerical features', size=15).set_position([.5, 1.02])\n  plt.tight_layout()","dd4beef7":"X.head()","668a4277":"from sklearn.preprocessing import StandardScaler\nstandard_scaler = StandardScaler()\nstandard_scaler.fit(X)\n\nscaled_X = standard_scaler.transform(X)","e5f0407d":"from sklearn.preprocessing import LabelEncoder\n\nlabel_encoder = LabelEncoder()\ny = label_encoder.fit_transform(y)\n\nlist(label_encoder.classes_)","8c45642d":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, random_state=SEED, test_size=0.2)\n\nprint('Size of train dataset after the split in subsets {}'.format(X_train.shape))\nprint('Size of test\/validation dataset after the split in subsets {}'.format(X_test.shape))\n","ddc25692":"from sklearn.naive_bayes import GaussianNB\nnb_model = GaussianNB()\nnb_model.fit(X_train, y_train)\nnb_predict = nb_model.predict(X_test)\nnb_accuracy = accuracy_score(y_test, nb_predict)","72a9f432":"print('Accuracy of the Naive Bayes classifier is {:.3f}'.format(nb_accuracy))","caaa0450":"from sklearn.svm import SVC\nSVM_model = SVC(kernel='linear', C=10)\nSVM_model.fit(X_train, y_train)\nSVM_predict=SVM_model.predict(X_test)\nSVM_accuracy_C10 = accuracy_score(y_test, SVM_predict)","d3d15f19":"print('Accuracy of the SVM classifier with  Linear Kernel is {:.3f}'.format(SVM_accuracy_C10))","cd53e2a9":"from sklearn.metrics import plot_confusion_matrix\n\nclass_names = label_encoder.classes_\n\ntitle_options = [('Confusion Matrix, without Normalize', None), ('Normalized confusion Matrix', 'true')]\n\nfor title, normalize in title_options:\n    disp = plot_confusion_matrix(SVM_model, X_test, y_test,\n                                display_labels = class_names,\n                                cmap= plt.cm.Blues,\n                                normalize = normalize)\n    disp.ax_.set_title(title)","1251629b":"SVM_model = SVC(kernel='linear', C=0.1)\nSVM_model.fit(X_train, y_train)\nSVM_predict = SVM_model.predict(X_test)\nSVM_accuracy_C01 = accuracy_score(y_test, SVM_predict)","132259a7":"print('Accuracy of the SVM classifier with  Linear Kernel is {:.3f}'.format(SVM_accuracy_C01))","5afb0e6f":"\ntitle_options = [('Confusion Matrix, without Normalize', None), ('Normalized confusion Matrix', 'true')]\n\nfor title, normalize in title_options:\n    disp = plot_confusion_matrix(SVM_model, X_test, y_test,\n                                display_labels = class_names,\n                                cmap= plt.cm.Blues,\n                                normalize = normalize)\n    disp.ax_.set_title(title)","c9093bb6":"from sklearn.tree import DecisionTreeClassifier\ntree_model = DecisionTreeClassifier()\ntree_model.fit(X_train, y_train)\ntree_predict = tree_model.predict(X_test)\ntree_accuracy = accuracy_score(y_test,tree_predict)","f4a2e8d5":"print('Accuracy of the Decision Tree classifier is {:.3f}'.format(tree_accuracy))","c11ac969":"accuracy_dataframe = {'Estimator':['Naive Bayes', 'SVM_C=10', 'SVM_C=0.1', 'Decision Tree'], \n                      'Accuracy': [nb_accuracy, SVM_accuracy_C10, SVM_accuracy_C01, tree_accuracy ]}\naccuracy_df = pd.DataFrame(accuracy_dataframe)\n\nprint(accuracy_df)","9c0993da":"df = df.drop(['id', 'Unnamed: 32'], axis=1)\ndf_features = df.drop(['diagnosis'], axis=1)\n\ndf_features.head()","b05ae2f1":"standard_scaler = StandardScaler()\nstandard_scaler.fit(df_features)\n\nscaled_features = standard_scaler.transform(df_features)\n\nscaled_features","03d1ec06":"from sklearn.decomposition import PCA\npca_model = PCA(n_components=3)\npca_model.fit(scaled_features)\n\nX_pca = pca_model.transform(scaled_features)\n\nprint('Shape of the dataset after PCA transformation is {}'.format(X_pca.shape))","6239fa6c":"def encoder(data):\n    if data=='M':\n        return 1\n    else:\n        return 0\n    \ndf_target  = df['diagnosis'].apply(encoder)","7ebfd704":"from mpl_toolkits.mplot3d import Axes3D\nfig = plt.figure(figsize=(15,8))\nax = fig.add_subplot(111, projection='3d')\nax.scatter(X_pca[:, 0],X_pca[:,1],X_pca[:,2], c=df_target, s=100)\nax.set_xlabel('First Principal Component', color='black', size=12)\nax.set_ylabel('Second Principal Component',color='black', size=12)\nax.set_zlabel('Third Principal Component', color='black', size=12)\n#ax.legend()\n#ax.set_label(loc='best')","3cadd029":"pca_df = pd.DataFrame(X_pca, columns=['pca0', 'pca1', 'pca2'])          \npca_df['diagnosis'] = df['diagnosis']\nprint('Shape of PCA dataset is {}'.format(pca_df.shape))\npca_df.head()","f562bb9b":"sns.pairplot(pca_df, hue='diagnosis', markers=[\"o\", \"s\"], corner=False)","aaddcd59":"X = pca_df.drop(['diagnosis'], axis=1)\ny = df_target\n\nX_train, X_test, y_train, y_test = train_test_split(X,y,random_state=SEED,test_size=0.2)","16b7f7ca":"nb_model_pca = GaussianNB()\nnb_model_pca.fit(X_train, y_train)\nnb_predict_pca = nb_model_pca.predict(X_test)\nnb_accuracy_pca = accuracy_score(y_test, nb_predict_pca)\n\nprint('The accuracy score after PCA using Naive bayes : {:.3f}'.format(nb_accuracy_pca))","c3cc803e":"SVM_model_pca = SVC(kernel='linear', C=0.1)\nSVM_model_pca.fit(X_train, y_train)\nSVM_predict_pca = SVM_model_pca.predict(X_test)\nSVM_accuracy_pca_C01 = accuracy_score(y_test,SVM_predict_pca)\n\nprint('SVM accuracy after PCA and with C=0.1 is  {:.3f}'.format(SVM_accuracy_pca_C01))","8f4a2efb":"SVM_model_pca = SVC(kernel='linear', C=10)\nSVM_model_pca.fit(X_train, y_train)\nSVM_predict_pca = SVM_model_pca.predict(X_test)\nSVM_accuracy_pca_C10 = accuracy_score(y_test,SVM_predict_pca)","87233788":"print('SVM accuracy after PCA and with C=10 is {:.3f}'.format(SVM_accuracy_pca_C10))","633d805c":"tree_model_pca = DecisionTreeClassifier()\ntree_model_pca.fit(X_train, y_train)\ntree_predict_pca = tree_model_pca.predict(X_test)\ntree_accuracy_pca = accuracy_score(y_test,tree_predict_pca)\n\nprint('Accuracy of SVM classifier with Linear Kernel is: {:.3f}'.format(tree_accuracy_pca))","ae191652":"accuracy_dataframe = {'Estimator':['Naive Bayes_PCA', 'SVM_PCA_C=10', 'SVM_PCA_C=0.1', 'Decision Tree_PCA'], \n                      'Accuracy': [nb_accuracy_pca, SVM_accuracy_pca_C10, SVM_accuracy_pca_C01, tree_accuracy_pca ]}\npca_accuracy_df = pd.DataFrame(accuracy_dataframe)\n\nprint(pca_accuracy_df)","39289565":"final_accuracy_table = {'Classifiers': ['Naive Bayes Classifier', 'Support Vector Machine_C=10', 'Support Vector Machine_C=0.1', 'Decision Tree'],\n                'Accuracy_without_PCA': [nb_accuracy, SVM_accuracy_C10, SVM_accuracy_C01, tree_accuracy],\n                'Accuracy_with_PCA': [nb_accuracy_pca, SVM_accuracy_pca_C10, SVM_accuracy_pca_C01, tree_accuracy_pca]}\nfinal_accuracy_dataframe = pd.DataFrame(final_accuracy_table)\n\n\nfinal_accuracy_dataframe.head()","5ca5b960":"1. Naive Bayes  ","6864ca33":"# Conclusion ","820f1745":"1. Standardization of the Data","0e89dc2d":"# **PCA: Principal Component Analysis**\n\n\nPCA is fundametally unsupervised learning method used for dimensionality reduction but it can be very useful as a tool of visualization, noise filtering, for feature extraction and engineering.\n\nThe unsupervised learning method rather then predicting the y value from x, attempts to learn about the relationship between the x and y values. In case of PCA one quantifies the relationship by finding a list of principal axes in the data, and using those axes to describe the dataset.","fedfeb88":"2. Support Vector Machine","4e7a764f":"# 2. Analysis of Numerical Features","12b41883":"2. Support Vector Machines\n\nIt is a type of discriminative classifier in which we do not model each class rather find a line, curve or a hyperplane that devides the classes from each other.\n\n\nThe basic intuition behind the SVMs is that rather simply drawing a zero width line between two classes, we can draw around each line a margin of some width, upto the nearest point. The line that maximises the margin is the one we will choose as the optimal model, SVMs are the examples of such maximim margin estimators","5542ba47":"1. Naive Bayes Classifier\n\nNaive Bayes models area group of extremely simple classification algorithms that are often suitable for high dimensional datasets because they are ver fast and have few tunable parameters so they end up being very useful as a baseline algorithm for a classification problems","2396c087":"**Tuning the SVM: Softening the Margin**\n\nSVM softness index alows some points to creep into tge margin if that allows a better fit for the model. The hardness of the margin is controlled by the tuning parameter most often known as C","bc085c99":"# 1. Importing Libraries","4097aff1":"As last column Unnamed 32 is completely empty, better to drop this from the dataset","0db3627d":"# 3. Data Preprocessing ","0376f90d":"# 4. Estimators","acd3ba77":"3. Decision Tree","3fcda8ad":"3. Decision Tree\n\nRandom forest a non parametric algorithm is an example of ensemble method, a method that relies on aggregating the result of an ensemble of simple estimators. The surprising result from the ensemble methods are a majority vote among the number of estimator can end up being better than any of individual estimator doing the voting."}}