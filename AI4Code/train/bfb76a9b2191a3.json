{"cell_type":{"beb5f6b3":"code","c13a747a":"code","b4a5555b":"code","543d4fa1":"code","739ba58b":"code","a6f67b83":"code","ae26450d":"code","c3f30b54":"code","e18af641":"code","0b823496":"code","aaded67a":"code","9d1545d2":"code","e835f9b5":"code","4bc4587f":"code","ea38da35":"code","ac372824":"code","6f492c60":"code","30903652":"code","b060b46d":"code","acb1e812":"code","b7646861":"code","2a592166":"code","59afc636":"code","50b60ec7":"code","6e049ebe":"code","2978ea10":"code","a0d8bc8a":"code","cd567ade":"code","0aeb5290":"code","a00c4aa4":"code","dab5750a":"code","5ca340c6":"code","5d241967":"code","84fc1c4b":"code","3fde56ca":"code","47ed892c":"code","49636c39":"code","ddde0dda":"code","61e8f8e1":"code","b60c41b7":"code","041b757c":"code","92c8c1ed":"markdown","85cca702":"markdown","dbf8cc5b":"markdown","8420da85":"markdown","0193cbc6":"markdown","f608c1f3":"markdown","5ad07315":"markdown","f92e327a":"markdown","358e0e51":"markdown","b1236105":"markdown","317d1d44":"markdown","0c1fe03b":"markdown","fae18439":"markdown","4aaf622e":"markdown","91797381":"markdown","bfd70641":"markdown","022a7a61":"markdown"},"source":{"beb5f6b3":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","c13a747a":"import seaborn as sns\nimport matplotlib.pyplot as plt\nfrom pandas.plotting import scatter_matrix\nimport scipy.stats as ss","b4a5555b":"df=pd.read_csv(\"\/kaggle\/input\/travel-insurance-prediction-data\/TravelInsurancePrediction.csv\")","543d4fa1":"df.head()","739ba58b":"df.drop(df.columns[0],axis=1,inplace=True)","a6f67b83":"for feature in ['Age','AnnualIncome']:\n    sns.histplot(data=df,x=df[feature],hue=df['TravelInsurance'])\n    plt.show()","ae26450d":"df_cat=df.drop(['Age','AnnualIncome','TravelInsurance'],axis=1).columns\ndf_cat","c3f30b54":"df.info()","e18af641":"for feature in df_cat:\n    sns.countplot(data=df,x=feature,hue='TravelInsurance')\n    plt.show()","0b823496":"def cramers_v(x, y):\n    confusion_matrix = pd.crosstab(x,y)\n    chi2 = ss.chi2_contingency(confusion_matrix)[0]\n    n = confusion_matrix.sum().sum()\n    phi2 = chi2\/n\n    r,k = confusion_matrix.shape\n    phi2corr = max(0, phi2-((k-1)*(r-1))\/(n-1))\n    rcorr = r-((r-1)**2)\/(n-1)\n    kcorr = k-((k-1)**2)\/(n-1)\n    return np.sqrt(phi2corr\/min((kcorr-1),(rcorr-1)))","aaded67a":"for feature in df_cat:\n    print(feature,cramers_v(df[feature],df.iloc[:,-1]).round(3))","9d1545d2":"names=[]\nscores=[]\nfor feature in df_cat:\n    names.append(feature)\n    scores.append(cramers_v(df[feature],df.iloc[:,-1]))\npd.DataFrame(scores,index=names).sort_values(by=0).plot(kind='barh')","e835f9b5":"df.drop([df.columns[2],df.columns[5]],axis=1,inplace=True)","4bc4587f":"df.TravelInsurance.value_counts()","ea38da35":"df.TravelInsurance.value_counts()\/len(df)","ac372824":"from sklearn.model_selection import train_test_split, KFold, cross_val_score, GridSearchCV\n\ntrain,validation=train_test_split(df)\nX,y=train.drop(train.columns[-1],axis=1),train.iloc[:,-1]\nX_val,y_val=validation.drop(validation.columns[-1],axis=1),validation.iloc[:,-1]","6f492c60":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.svm import SVC","30903652":"models = []\nmodels.append(('LR', LogisticRegression(solver='liblinear')))\nmodels.append(('LDA', LinearDiscriminantAnalysis()))\nmodels.append(('KNN', KNeighborsClassifier()))\nmodels.append(('CART', DecisionTreeClassifier()))\nmodels.append(('NB', GaussianNB()))\nmodels.append(('SVM', SVC(gamma='auto')))","b060b46d":"from sklearn.preprocessing import MinMaxScaler, StandardScaler, LabelEncoder, OneHotEncoder, Normalizer, QuantileTransformer, RobustScaler,\\\nPowerTransformer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.compose import ColumnTransformer","acb1e812":"X_cat=X.select_dtypes(include='object').columns\nX_num=X.select_dtypes(include='int64').columns\ncat_transformer=OneHotEncoder(handle_unknown='ignore',sparse=False)\nct=ColumnTransformer([('cat',cat_transformer,X_cat)],remainder='passthrough')\n\nresults = []\nnames = []\nfor name, model in models:\n    kfold = KFold(n_splits=10, random_state=0, shuffle=True)\n    pipe=Pipeline([('ct',ct),('model',model)])\n    cv_results = cross_val_score(pipe, X, y, cv=kfold, scoring='accuracy')\n    results.append(cv_results.mean().round(3))\n    names.append(name)\nr1=pd.DataFrame(data=results,index=names,columns=['Baseline'])\nr1","b7646861":"from sklearn.ensemble import AdaBoostClassifier, GradientBoostingClassifier, RandomForestClassifier, ExtraTreesClassifier","2a592166":"X_cat=X.select_dtypes(include='object').columns\nX_num=X.select_dtypes(include='int64').columns\ncat_transformer=OneHotEncoder(handle_unknown='ignore',sparse=False)\nct=ColumnTransformer([('cat',cat_transformer,X_cat)],remainder='passthrough')\nmodel=RandomForestClassifier()\npipe=Pipeline([('ct',ct),('model',model)])\npipe.fit(X,y)","59afc636":"importances=pipe.named_steps.model.feature_importances_\nimportances","50b60ec7":"ohe_cat=pipe.named_steps.ct.named_transformers_.cat.get_feature_names(X_cat)\nohe_cat","6e049ebe":"ct_cols=list(ohe_cat)+list(X_num)\nct_cols","2978ea10":"pd.DataFrame(data=importances,index=ct_cols,columns=['importance']).sort_values(by='importance').plot(kind='barh')","a0d8bc8a":"from sklearn.feature_selection import SelectKBest, f_classif, SelectFromModel","cd567ade":"X_cat=X.select_dtypes(include='object').columns\nX_num=X.select_dtypes(include='int64').columns\ncat_transformer=OneHotEncoder(handle_unknown='ignore',sparse=False)\nct=ColumnTransformer([('cat',cat_transformer,X_cat)],remainder='passthrough')\nsel = SelectFromModel(RandomForestClassifier())\npipe=Pipeline([('ct',ct),('sel',sel)])\npipe.fit(X,y)","0aeb5290":"support=pipe.named_steps.sel.get_support()\nsupport","a00c4aa4":"ohe_cat=pipe.named_steps.ct.named_transformers_.cat.get_feature_names(X_cat)\nohe_cat","dab5750a":"ct_cols=list(ohe_cat)+list(X_num)\nct_cols","5ca340c6":"from itertools import compress\nlist(compress(ct_cols,support))","5d241967":"X_cat=X.select_dtypes(include='object').columns\nX_num=X.select_dtypes(include='int64').columns\n\nspace={'ct__scaler':[MinMaxScaler(),StandardScaler(),Normalizer(),RobustScaler(),PowerTransformer(),QuantileTransformer()]}\n\n\nnames=[]\nparams=[]\nscores=[]\nfor name, model in models:\n    kfold = KFold(n_splits=10, random_state=0, shuffle=True)\n    cat_transformer=OneHotEncoder(handle_unknown='ignore',sparse=False)\n    ct=ColumnTransformer([('ohe',cat_transformer,X_cat),('scaler',MinMaxScaler(),X_num)])\n    pipe=Pipeline([('ct',ct),('model',model)])\n    grid = GridSearchCV(estimator=pipe,param_grid=space, cv=10, scoring='accuracy')\n    grid.fit(X,y)\n    print(grid.best_params_,grid.best_score_.round(3),name)\n    names.append(name)\n    params.append(grid.best_params_)\n    scores.append(grid.best_score_.round(3))\nr2=pd.DataFrame(data=scores,index=names,columns=['Scaled'])\nr2","84fc1c4b":"X_cat=X.select_dtypes(include='object').columns\nX_num=X.select_dtypes(include='int64').columns\n\nspace={'ct__scaler':[MinMaxScaler(),StandardScaler(),Normalizer(),RobustScaler(),PowerTransformer(),QuantileTransformer()]}\ncat_transformer=OneHotEncoder(handle_unknown='ignore',sparse=False)\nct=ColumnTransformer([('ohe',cat_transformer,X_cat),('scaler',MinMaxScaler(),X_num)])\nsel = SelectFromModel(RandomForestClassifier())\n\nnames=[]\nparams=[]\nscores=[]\nfor name, model in models:\n    pipe=Pipeline([('ct',ct),('sel',sel),('model',model)])\n    grid = GridSearchCV(estimator=pipe,param_grid=space, cv=10, scoring='accuracy')\n    grid.fit(X,y)\n    print(grid.best_params_,grid.best_score_.round(3),name)\n    names.append(name)\n    params.append(grid.best_params_)\n    scores.append(grid.best_score_.round(3))\nr3=pd.DataFrame(data=scores,index=names,columns=['Scaled+Feature Selection'])\nr3","3fde56ca":"pd.concat([r1,r2,r3],axis=1)","47ed892c":"from sklearn.ensemble import AdaBoostClassifier, GradientBoostingClassifier, RandomForestClassifier, ExtraTreesClassifier\nensembles = []\nensembles.append(('AB', AdaBoostClassifier()))\nensembles.append(('GBM', GradientBoostingClassifier()))\nensembles.append(('RF', RandomForestClassifier(n_estimators=10)))\nensembles.append(('ET', ExtraTreesClassifier(n_estimators=10)))","49636c39":"X_cat=X.select_dtypes(include='object').columns\nX_num=X.select_dtypes(include='int64').columns\ncat_transformer=OneHotEncoder(handle_unknown='ignore',sparse=False)\nct=ColumnTransformer([('cat',cat_transformer,X_cat)],remainder='passthrough')\n\nresults = []\nnames = []\nfor name, model in ensembles:\n    kfold = KFold(n_splits=10, random_state=0, shuffle=True)\n    pipe=Pipeline([('ct',ct),('model',model)])\n    cv_results = cross_val_score(pipe, X, y, cv=kfold, scoring='accuracy')\n    results.append(cv_results.mean().round(3))\n    names.append(name)\nr4=pd.DataFrame(data=results,index=names,columns=['Ensemble Baseline'])\nr4","ddde0dda":"X_cat=X.select_dtypes(include='object').columns\nX_num=X.select_dtypes(include='int64').columns\n\nspace={'ct__scaler':[MinMaxScaler(),StandardScaler(),Normalizer(),RobustScaler(),PowerTransformer(),QuantileTransformer()]}\n\n\nnames=[]\nparams=[]\nscores=[]\nfor name, model in ensembles:\n    kfold = KFold(n_splits=10, random_state=0, shuffle=True)\n    cat_transformer=OneHotEncoder(handle_unknown='ignore',sparse=False)\n    ct=ColumnTransformer([('ohe',cat_transformer,X_cat),('scaler',MinMaxScaler(),X_num)])\n    pipe=Pipeline([('ct',ct),('model',model)])\n    grid = GridSearchCV(estimator=pipe,param_grid=space, cv=10, scoring='accuracy')\n    grid.fit(X,y)\n    print(grid.best_params_,grid.best_score_.round(3),name)\n    names.append(name)\n    params.append(grid.best_params_)\n    scores.append(grid.best_score_.round(3))\nr5=pd.DataFrame(data=scores,index=names,columns=['Scaled'])\nr5","61e8f8e1":"X_cat=X.select_dtypes(include='object').columns\nX_num=X.select_dtypes(include='int64').columns\n\nspace={'ct__scaler':[MinMaxScaler(),StandardScaler(),Normalizer(),RobustScaler(),PowerTransformer(),QuantileTransformer()]}\ncat_transformer=OneHotEncoder(handle_unknown='ignore',sparse=False)\nct=ColumnTransformer([('ohe',cat_transformer,X_cat),('scaler',MinMaxScaler(),X_num)])\nsel = SelectFromModel(RandomForestClassifier())\n\nnames=[]\nparams=[]\nscores=[]\nfor name, model in ensembles:\n    pipe=Pipeline([('ct',ct),('sel',sel),('model',model)])\n    grid = GridSearchCV(estimator=pipe,param_grid=space, cv=10, scoring='accuracy')\n    grid.fit(X,y)\n    print(grid.best_params_,grid.best_score_.round(3),name)\n    names.append(name)\n    params.append(grid.best_params_)\n    scores.append(grid.best_score_.round(3))\nr6=pd.DataFrame(data=scores,index=names,columns=['Scaled+Feature Selection'])\nr6","b60c41b7":"pd.concat([r4,r5,r6],axis=1)","041b757c":"from sklearn.metrics import confusion_matrix\n\nX_cat=X.select_dtypes(include='object').columns\nX_num=X.select_dtypes(include='int64').columns\n\ncat_transformer=OneHotEncoder(handle_unknown='ignore',sparse=False)\nct=ColumnTransformer([('ohe',cat_transformer,X_cat),('scaler',MinMaxScaler(),X_num)])\nsel = SelectFromModel(RandomForestClassifier())\nmodel=GradientBoostingClassifier()\npipe=Pipeline([('ct',ct),('sel',sel),('model',model)])\npipe.fit(X,y)\npred=pipe.predict(X_val)\nprint(confusion_matrix(y_val,pred))","92c8c1ed":"# Baseline Model Comparison","85cca702":"# Feature Importance","dbf8cc5b":"# Ensembles","8420da85":"# Modeling","0193cbc6":"We observe lower rates of Travel Insurance in the middle of the age range and bottom two-thirds of the income distribution. ","f608c1f3":"# Create training and validation sets","5ad07315":"- Even though the target is imbalanced, we can still use classification accuracy since the majority class is less than 80%.\n- For more details see: [Step-By-Step Framework for Imbalanced Classification Projects](https:\/\/machinelearningmastery.com\/framework-for-imbalanced-classification-projects\/)","f92e327a":"# Compare scalers with GridSearch","358e0e51":"# Compare scalers with GridSearch + SelectFromModel","b1236105":"- Associations between categorical features can be found using Cramer's V. \n- The linked article explains how to measure correlation when your data contains categorical features. \n\n[The Search for Categorical Correlation](https:\/\/towardsdatascience.com\/the-search-for-categorical-correlation-a1cf7f1888c9)","317d1d44":"# Visualization","0c1fe03b":"# Baseline","fae18439":"# Feature selection","4aaf622e":"# Make predictions on the validation set","91797381":"# Scaling","bfd70641":"# Correlation","022a7a61":"# Check for class imbalance"}}