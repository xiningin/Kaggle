{"cell_type":{"e88ea825":"code","65f3dfc2":"code","f98900da":"code","4e5a8311":"code","91eab5ba":"code","97d4f277":"code","4be890cf":"code","0aebdc35":"code","0fd7bbbc":"code","915882ed":"code","3df7a893":"code","9732181b":"code","0082b8b1":"code","ff8a21bc":"code","ab233b6a":"code","be16f554":"markdown","a2727a7b":"markdown","79e751c6":"markdown","759ae2e2":"markdown","efdf3536":"markdown","345014ef":"markdown","61d45b70":"markdown"},"source":{"e88ea825":"# Set-up libraries\nimport os\nimport pandas as pd\nimport seaborn as sns\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split\nfrom sklearn import svm\nfrom mpl_toolkits import mplot3d\nfrom sklearn.metrics import classification_report","65f3dfc2":"# Check input data source\nfor dirname, _, filenames in os.walk('..\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","f98900da":"# Read-in data\ndf = pd.read_csv('..\/input\/iris\/Iris.csv')","4e5a8311":"# Check some details\ndf.info()","91eab5ba":"# Check some records\ndf.head()","97d4f277":"# Check for missing values\ndf.isna().sum()","4be890cf":"# Check for duplicate values\ndf.duplicated().sum()","0aebdc35":"# Check breakdown of label\nsns.countplot(df.Species)\ndf.Species.value_counts()","0fd7bbbc":"# Summarise\ndf.describe()","915882ed":"# Grab subset of data\ndf = df[['SepalLengthCm', 'SepalWidthCm', 'Species']]","3df7a893":"# Split dataset into 80% train and 20% validation\nX = df.drop('Species', axis=1)\ny = df['Species']\n\nX_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=0)","9732181b":"# Build and train the model(s)\nC = 1.0\nmodel_svc = svm.SVC(kernel='linear', C=C).fit(X_train, y_train)\nmodel_svc_rbf = svm.SVC(kernel='rbf', gamma=0.7, C=C).fit(X_train, y_train)\nmodel_svc_pol = svm.SVC(kernel='poly', degree=3, C=C).fit(X_train, y_train)\nmodel_svc_lin = svm.LinearSVC(C=C).fit(X_train, y_train)","0082b8b1":"# Apply model(s) to validation data\ny_predict_svc = model_svc.predict(X_val)\ny_predict_svc_rbf = model_svc_rbf.predict(X_val)\ny_predict_svc_pol = model_svc_pol.predict(X_val)\ny_predict_svc_lin = model_svc_lin.predict(X_val)","ff8a21bc":"# Compare actual and predicted values\ndef get_df_name(df):\n    name =[x for x in globals() if globals()[x] is df][0]\n    return name\n\npredictions = [y_predict_svc,\n               y_predict_svc_rbf,\n               y_predict_svc_pol,\n               y_predict_svc_lin]\n\nfor pred in predictions:\n    actual_vs_predict = pd.DataFrame({'Model': get_df_name(pred),\n                                      'Actual': y_val,\n                                      'Predict': pred\n                                    })\n    print(actual_vs_predict.sample(5))","ab233b6a":"# Evalute models\nprint('SVC with linear kernel: \\n', classification_report(y_val, y_predict_svc))\nprint('Linear SVC with linear kernel: \\n', classification_report(y_val, y_predict_svc_lin))\nprint('SVC rbf kernel: \\n', classification_report(y_val, y_predict_svc_rbf))\nprint('SVC with polynomial kernel: \\n', classification_report(y_val, y_predict_svc_pol))","be16f554":"Let's work on the first two features. Keeping things simple for now helps avoid ugly slicing with a two dimensional dataset.","a2727a7b":"## Step 3: Model and evaluate\nThis last step is three-fold.\n\nWe create the model(s) and fit the model to the data we prepared for training.\n\nWe then proceed to classifying with the data we prepared for validation.\n\nLastly, we evaluate the model's performance with mainstream classification metrics.","79e751c6":"## Step 0: Understand the problem\nWhat we're trying to do here is to classify species of Iris flowers.","759ae2e2":"## About\nThis notebook contains a very fast fundamental Support Vector Machine (SVM) example in Python.\n\nThis work is part of a series called [Machine learning in minutes - very fast fundamental examples in Python](https:\/\/www.kaggle.com\/jamiemorales\/machine-learning-in-minutes-very-fast-examples).\n\nThe approach is designed to help grasp the applied machine learning lifecycle in minutes. It is not an alternative to actually taking the time to learn. What it aims to do is help someone get started fast and gain intuitive understanding of the typical steps early on","efdf3536":"## Step 1: Set-up and understand data\nThis step helps uncover issues that we will want to address in the next step and take into account when building and evaluating our model. We also want to find interesting relationships or patterns that we can possibly leverage in solving the problem we specified.","345014ef":"## Learn more\nIf you found this example interesting, you may also want to check out:\n\n* [Machine learning in minutes - very fast fundamental examples in Python](https:\/\/www.kaggle.com\/jamiemorales\/machine-learning-in-minutes-very-fast-examples)\n* [List of machine learning methods & datasets](https:\/\/www.kaggle.com\/jamiemorales\/list-of-machine-learning-methods-datasets)\n\nThanks for reading. Don't forget to upvote.","61d45b70":"## Step 2: Preprocess data and understand some more\nThis step typically takes the most time in the cycle but for our purposes, most of the datasets chosen in this series are clean.\n\nReal-world datasets are noisy and incomplete. The choices we make in this step to address data issues can impact downstream steps and the result itself. For example, it can be tricky to address missing data when we don't know why it's missing. Is it missing completely at random or not? It can also be tricky to address outliers if we do not understand the domain and problem context enough."}}