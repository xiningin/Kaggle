{"cell_type":{"e3f85b70":"code","fce84af1":"code","e9d32712":"code","d3d8d011":"code","6a4ccd27":"code","3dc9d21d":"code","78518ff4":"code","52cb234a":"code","f6a7cbff":"code","dad7fce9":"code","f3f15df8":"code","5da76096":"code","b00de802":"code","b3165498":"code","c50d607a":"code","75647786":"code","14ba685f":"markdown","9319047d":"markdown","19dfbcff":"markdown","4ffda9fd":"markdown","f6da373f":"markdown","4666db47":"markdown","a4aad423":"markdown","3522412b":"markdown","d6cd02f2":"markdown","0e3f2e07":"markdown","a60b3971":"markdown","defb281e":"markdown","0f5d6ca3":"markdown","3c7b1b6a":"markdown","90992c63":"markdown","6fd11e2f":"markdown","9d50e709":"markdown","7c986f93":"markdown","af19addd":"markdown","dea5e604":"markdown","1d147424":"markdown","4d6a55ae":"markdown","40be97c0":"markdown","ef0dc723":"markdown","75a28317":"markdown","c5412692":"markdown","a287558c":"markdown","c7abba1c":"markdown","39d99b55":"markdown","8cb8d121":"markdown"},"source":{"e3f85b70":"\nimport numpy as np \nimport pandas as pd \nimport os\nfrom PIL import Image\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.cluster import KMeans\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom torch.utils.data import Dataset, DataLoader\nfrom torchvision import transforms\n\nfrom tqdm.notebook import tqdm\n\n\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n","fce84af1":"torch.cuda.empty_cache()","e9d32712":"if torch.cuda.is_available():\n    print(\"GPU available\")\n    device = \"cuda:0\"\nelse:\n    print(\"CPU available\")\n    device = \"cpu\"\n    \ndevice = torch.device(device)\nprint(device)\n\n\n","d3d8d011":"data_dir = os.path.join(\"\/kaggle\", \"input\", \"cityscapes-image-pairs\", \"cityscapes_data\")\ntrain_dir = os.path.join(data_dir, \"train\") \nval_dir = os.path.join(data_dir, \"val\")\ntrain_fns = os.listdir(train_dir)\nval_fns = os.listdir(val_dir)\n\nprint(\"train patch size:\", len(train_fns))\nprint(\"validation patch size:\", len(val_fns))","6a4ccd27":"sample_image_fp = os.path.join(train_dir, train_fns[0])\nsample_image = Image.open(sample_image_fp).convert(\"RGB\")\nplt.imshow(sample_image)\n\ndef split_image(image):\n    image = np.array(image)\n    cityscape = image[:, :256, :] \n    label = image[:, 256:, :]\n    return cityscape, label\n\nsample_image = np.array(sample_image)\nprint(\"sample image shape: \",sample_image.shape)\n\ncityscape, label = split_image(sample_image)\nprint(\"image shape:\" , cityscape.shape)\nprint(\"label shape:\" , label.shape)\n\ncityscape = Image.fromarray(cityscape) \nlabel = Image.fromarray(label)\n\nfig, axes = plt.subplots(1, 2, figsize=(10, 5))\naxes[1].imshow(cityscape)\naxes[0].imshow(label)","3dc9d21d":"num_items = 1000\ncolor_array = np.random.choice(range(256), 3*num_items).reshape(-1, 3)\nprint(color_array.shape)\nprint(color_array[:5, :])\n\nnum_classes = 10\nlabel_model = KMeans(n_clusters=num_classes)\nlabel_model.fit(color_array)\n\nlabel_model.predict(color_array[:5, :])\n\ncityscape, label = split_image(sample_image)\nlabel_class = label_model.predict(label.reshape(-1, 3)).reshape(256, 256)\nfig, axes = plt.subplots(1, 3, figsize=(15, 5))\naxes[0].imshow(cityscape)\naxes[1].imshow(label)\naxes[2].imshow(label_class)\nprint(label_class)","78518ff4":"class CityscapeDataset(Dataset):\n    \n    def __init__(self, image_dir, label_model):\n        self.image_dir = image_dir\n        self.image_fns = os.listdir(image_dir)\n        self.label_model = label_model\n        \n    def __len__(self):\n        return len(self.image_fns)\n    \n    def __getitem__(self, index):\n        image_fn = self.image_fns[index]\n        image_fp = os.path.join(self.image_dir, image_fn)\n        image = Image.open(image_fp).convert('RGB')\n        image = np.array(image)\n        cityscape, label = self.split_image(image)\n        label_class = self.label_model.predict(label.reshape(-1, 3)).reshape(256, 256)\n        cityscape = self.transform(cityscape)\n        label_class = torch.Tensor(label_class).long()\n        return cityscape, label_class\n    \n    def split_image(self, image):\n        image = np.array(image)\n        cityscape, label = image[:, :256, :], image[:, 256:, :]\n        return cityscape, label\n    \n    def transform(self, image):\n        transform_ops = transforms.Compose([\n            transforms.ToTensor(),\n            transforms.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n        ])\n        return transform_ops(image)","52cb234a":"class UNet(nn.Module):\n    \n    def __init__(self, num_classes):\n        super(UNet, self).__init__()\n        self.num_classes = num_classes\n        self.contracting_11 = self.conv_block(in_channels=3, out_channels=64)\n        self.contracting_12 = nn.MaxPool2d(kernel_size=2, stride=2)\n        self.contracting_21 = self.conv_block(in_channels=64, out_channels=128)\n        self.contracting_22 = nn.MaxPool2d(kernel_size=2, stride=2)\n        self.contracting_31 = self.conv_block(in_channels=128, out_channels=256)\n        self.contracting_32 = nn.MaxPool2d(kernel_size=2, stride=2)\n        self.contracting_41 = self.conv_block(in_channels=256, out_channels=512)\n        self.contracting_42 = nn.MaxPool2d(kernel_size=2, stride=2)\n        self.middle = self.conv_block(in_channels=512, out_channels=1024)\n        self.expansive_11 = nn.ConvTranspose2d(in_channels=1024, out_channels=512, kernel_size=3, stride=2, padding=1, output_padding=1)\n        self.expansive_12 = self.conv_block(in_channels=1024, out_channels=512)\n        self.expansive_21 = nn.ConvTranspose2d(in_channels=512, out_channels=256, kernel_size=3, stride=2, padding=1, output_padding=1)\n        self.expansive_22 = self.conv_block(in_channels=512, out_channels=256)\n        self.expansive_31 = nn.ConvTranspose2d(in_channels=256, out_channels=128, kernel_size=3, stride=2, padding=1, output_padding=1)\n        self.expansive_32 = self.conv_block(in_channels=256, out_channels=128)\n        self.expansive_41 = nn.ConvTranspose2d(in_channels=128, out_channels=64, kernel_size=3, stride=2, padding=1, output_padding=1)\n        self.expansive_42 = self.conv_block(in_channels=128, out_channels=64)\n        self.output = nn.Conv2d(in_channels=64, out_channels=num_classes, kernel_size=3, stride=1, padding=1)\n        \n    def conv_block(self, in_channels, out_channels):\n        block = nn.Sequential(nn.Conv2d(in_channels=in_channels, out_channels=out_channels, kernel_size=3, stride=1, padding=1),\n                                    nn.ReLU(),\n                                    nn.BatchNorm2d(num_features=out_channels),\n                                    nn.Conv2d(in_channels=out_channels, out_channels=out_channels, kernel_size=3, stride=1, padding=1),\n                                    nn.ReLU(),\n                                    nn.BatchNorm2d(num_features=out_channels))\n        return block\n    \n    def forward(self, X):\n        contracting_11_out = self.contracting_11(X) \n        contracting_12_out = self.contracting_12(contracting_11_out) \n        contracting_21_out = self.contracting_21(contracting_12_out) \n        contracting_22_out = self.contracting_22(contracting_21_out) \n        contracting_31_out = self.contracting_31(contracting_22_out) \n        contracting_32_out = self.contracting_32(contracting_31_out) \n        contracting_41_out = self.contracting_41(contracting_32_out) \n        contracting_42_out = self.contracting_42(contracting_41_out) \n        middle_out = self.middle(contracting_42_out) \n        expansive_11_out = self.expansive_11(middle_out) \n        expansive_12_out = self.expansive_12(torch.cat((expansive_11_out, contracting_41_out), dim=1))\n        expansive_21_out = self.expansive_21(expansive_12_out) \n        expansive_22_out = self.expansive_22(torch.cat((expansive_21_out, contracting_31_out), dim=1))\n        expansive_31_out = self.expansive_31(expansive_22_out) \n        expansive_32_out = self.expansive_32(torch.cat((expansive_31_out, contracting_21_out), dim=1))\n        expansive_41_out = self.expansive_41(expansive_32_out) \n        expansive_42_out = self.expansive_42(torch.cat((expansive_41_out, contracting_11_out), dim=1)) \n        output_out = self.output(expansive_42_out) \n        return output_out","f6a7cbff":"batch_size = 16\nepochs = 30\nlr = 0.001","dad7fce9":"dataset = CityscapeDataset(train_dir, label_model)\ndata_loader = DataLoader(dataset, batch_size=batch_size)\n\nmodel = UNet(num_classes=num_classes).to(device)\nprint(model)\n\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.Adam(model.parameters(), lr=lr)","f3f15df8":"step_losses = []\nepoch_losses = []\nfor epoch in tqdm(range(epochs)):\n    epoch_loss = 0\n    for X, Y in tqdm(data_loader, total=len(data_loader), leave=False):\n        X, Y = X.to(device), Y.to(device)\n        optimizer.zero_grad()\n        Y_pred = model(X)\n        loss = criterion(Y_pred, Y)\n        loss.backward()\n        optimizer.step()\n        epoch_loss += loss.item()\n        step_losses.append(loss.item())\n    epoch_losses.append(epoch_loss\/len(data_loader))","5da76096":"fig, axes = plt.subplots(1, 2, figsize=(10, 5))\naxes[0].plot(step_losses)\naxes[1].plot(epoch_losses)","b00de802":"model_name = \"U-Net.pth\"\ntorch.save(model.state_dict(), model_name)\nmodel_path = \"\/kaggle\/working\/U-Net.pth\"\nmodel_ = UNet(num_classes=num_classes).to(device)\nmodel_.load_state_dict(torch.load(model_path))","b3165498":"test_batch_size = 8\ndataset = CityscapeDataset(val_dir, label_model)\ndata_loader = DataLoader(dataset, batch_size=test_batch_size)\n\nX, Y = next(iter(data_loader))\nX, Y = X.to(device), Y.to(device)\nY_pred = model_(X)\nprint(Y_pred.shape)\nY_pred = torch.argmax(Y_pred, dim=1)\nprint(Y_pred.shape)\n\ninverse_transform = transforms.Compose([\n    transforms.Normalize((-0.485\/0.229, -0.456\/0.224, -0.406\/0.225), (1\/0.229, 1\/0.224, 1\/0.225))\n])","c50d607a":"fig, axes = plt.subplots(test_batch_size, 3, figsize=(3*5, test_batch_size*5))fig, axes = plt.subplots(test_batch_size, 3, figsize=(3*5, test_batch_size*5))\n\niou_scores = []\n\nfor i in range(test_batch_size):\n    \n    landscape = inverse_transform(X[i]).permute(1, 2, 0).cpu().detach().numpy()\n    label_class = Y[i].cpu().detach().numpy()\n    label_class_predicted = Y_pred[i].cpu().detach().numpy()\n    \n    # IOU score\n    intersection = np.logical_and(label_class, label_class_predicted)\n    union = np.logical_or(label_class, label_class_predicted)\n    iou_score = np.sum(intersection) \/ np.sum(union)\n    iou_scores.append(iou_score)\n\n    axes[i, 0].imshow(landscape)\n    axes[i, 0].set_title(\"Landscape\")\n    axes[i, 1].imshow(label_class)\n    axes[i, 1].set_title(\"Label Class\")\n    axes[i, 2].imshow(label_class_predicted)\n    axes[i, 2].set_title(\"Label Class - Predicted\")\n\nprint(sum(iou_scores) \/ len(iou_scores))\n","75647786":"import cv2 \nfrom PIL import Image\nfrom torchvision import transforms\n\n\nvideo_name = input(\"Enter video name and format\\n\")\n\ncap = cv2.VideoCapture(\"{}\".format(video_name))\n\nwhile(True):\n    ret, frame = cap.read()\n    im_pil = Image.fromarray(img)\n    frame = transforms.ToTensor()(im_pil)\n    \n    Y_pred = model_(frame)\n    Y_pred = torch.argmax(Y_pred, dim=1)\n    landscape = inverse_transform(frame[i]).permute(1, 2, 0).cpu().detach().numpy()\n    label_class_predicted = Y_pred[i].cpu().detach().numpy()\n\n    font = cv2.FONT_HERSHEY_SIMPLEX \n    org = (10, 10) \n    fontScale = 1\n    color = (255, 0, 0) \n    thickness = 2\n    image = cv2.putText(Y_pred, 'OpenCV', org, font,  \n                   fontScale, color, thickness, cv2.LINE_AA) \n    \n    cv2.imshow(\"SemanticSegmentation\", image)\n    if cv2.waitKey(1) & 0xFF == ord('q'):\n        break\ncap.release()\ncv2.destroyAllWindows()\n    ","14ba685f":"in initialize function, we indicated variables for using other functions. These variables are image direcory, images list and label model.","9319047d":"<a id = \"6\"><\/a><br>\n\n# Initializing Hyperparameters and Functions","19dfbcff":"<a id = \"7\"><\/a><br>\n\n# Training and Visualizing Results","4ffda9fd":"<a id = \"1\"><\/a><br>\n\n# Importing Libraries","f6da373f":"in len function, we indicate list funtion's length.","4666db47":"In t\u0131hs section, Dataset are initialized and loaded. After that, we initialize deep learning model and send GPU device. Final step, we indicate Loss and optimizer function.","a4aad423":"<a id = \"10\"><\/a><br>\n\n# Testing on Video with OpenCV","3522412b":"<a id = \"8\"><\/a><br>\n\n# Saving and Loading Model","d6cd02f2":"![](https:\/\/www.researchgate.net\/profile\/Alan_Jackson9\/publication\/323597886\/figure\/fig2\/AS:601386504957959@1520393124691\/Convolutional-neural-network-CNN-architecture-based-on-UNET-Ronneberger-et-al.png)","0e3f2e07":"in this section, our training mission is finished. After training process, we will save our model. If we test our model, we load pth file and initilize model. After that we  load model parameters from model. ","a60b3971":"In training section, we train our model. During training, we will to store step losses and epoch losses. ","defb281e":"we will clear GPU memory","0f5d6ca3":"In this section, we create deep learning class. We will use UNet model. ","3c7b1b6a":"<a id = \"9\"><\/a><br>\n\n# Testing and Visualizing Results","90992c63":"<font color = 'red'>\nContent: \n\n1. [Importing Libraries](#1)\n1. [Clearing GPU Memory](#2)   \n1. [Selecting Device](#3)\n1. [Importing Dataset Directory and Show Examples](#4)\n1. [Creating Deep Learning Model](#5)\n1. [Initializing Hyperparameters and Functions](#6)\n1. [Training and Visualizing Results](#7)\n1. [Saving and Loading Model](#8)    \n1. [Testing and Visualizing Results](#9)\n1. [Testing on Video with OpenCV](#10)","6fd11e2f":"selected sample image from train data and Open image with Image class. each image has 256 rows and 512 columns. first 256 columns is image. Second 256 columns is label. we split image to image and label. Finally Variable was printed and was showed with imshow class. ","9d50e709":"in transform functions, there are two process. These process are converted to tensor and normalizing.","7c986f93":"<a id = \"2\"><\/a><br>\n\n# Clearing GPU Memory","af19addd":"REFERENCES\n* GokulKarthik","dea5e604":"<a id = \"3\"><\/a><br>\n\n# Selecting Device","1d147424":"we visualize losses lists.","4d6a55ae":"Library are imported to semantic segmantation. we will use torch library and tools, numpy for matrix operations, pandas for data processing, matplotlib and PIL for visualizing.","40be97c0":"in this section, we will create Dataset class and we will use to initialize and data loader parts. CityscapeDataset class include five functions. Initialize, len, getitem, split image and transform. \n","ef0dc723":"<a id = \"4\"><\/a><br>\n\n# Importing Dataset Directory and Show Examples","75a28317":"we indicate data directory. We will use Train and validation. ","c5412692":"We select device. If GPU is available, we select GPU otherwise we select CPU. After that initialize device. ","a287558c":"Hyperparameter selection is very important issue. There are three hyperparameters: batch size, epochs, learning rate. we indicated commonly used values. You can select d\u0131fferent values as your system power.","c7abba1c":"in getitem function, we take image from image direcitory. Then image are opened and converted to RGB color format by Image class. After that image converted to array format. we will use split funtion, this function was explained. we will predict label model to indicate label class. Then we will use transform functions. Finally label class are converted to tensor. this function returns two variables: cityspace( original image) and label class.  ","39d99b55":"<a id = \"5\"><\/a><br>\n\n# Creating Deep Learning Model","8cb8d121":"we will show original image, label and label class in this section. Fistly we indicate color array as randomly. we will use Kmeans algorithm to perict label model. classes number is 10, iteration numbers is 1000. This is hyperparameter. you can change these two variables. "}}