{"cell_type":{"7818af8d":"code","8390d3c1":"code","146d9b6b":"code","7d9bb380":"code","e8edd9cb":"code","87c3deae":"code","45fbc860":"code","7a3bf947":"code","5906849f":"code","aabaf7c4":"code","8996aec4":"code","09545fbb":"code","e5630c5b":"code","79eb52bc":"code","40e29fba":"code","f49f0178":"code","a73d18e2":"code","e0d4fc75":"code","8f389a21":"code","142a2f60":"code","47fbc759":"code","324845f6":"code","2365887e":"code","1f7bccc5":"markdown","a94f2ad7":"markdown","ae181efc":"markdown","83867af8":"markdown","964b7a74":"markdown","6888b35d":"markdown","04dffecb":"markdown","c517f26d":"markdown","2450ec0d":"markdown","ac4e74a3":"markdown","ed8b6de7":"markdown","6f1d9300":"markdown","cc02ef36":"markdown","3d5fd232":"markdown","3b74cdc4":"markdown","51ab196c":"markdown","533f833a":"markdown","4040f07f":"markdown","e19d6c66":"markdown"},"source":{"7818af8d":"import os\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nimport numpy as np \nimport pandas as pd \n\nimport matplotlib.pyplot as plt\n%matplotlib inline","8390d3c1":"TRAINPATH = '..\/input\/ny-taxis\/train_w_zones2.csv'\nTESTPATH = '..\/input\/ny-taxis\/test_w_zones2.csv'\ndf_train = pd.read_csv(TRAINPATH)\ndf_test = pd.read_csv(TESTPATH)\nprint('df_train:', df_train.shape, '\\ndf_test:', df_test.shape)\nl=df_train.shape[0]\ndf_train.head()","146d9b6b":"# remove trip of less than 10m (#8665)\n#print('There is ', df_train[df_train['distances']<=0.01].shape[0], 'travels of less than 100m before filtering')\n#df_train = df_train[df_train['distances']>0.01]\n#print('There is ', df_train[df_train['distances']<=0.01].shape[0], 'travels of less than 100m after filtering')","7d9bb380":"#remove trips that lasted less than 1 min (#4933 left after previous filtering)\n#print('There is', df_train[df_train['trip_duration']<=1*60].shape[0], 'travels of less than 1min before filtering')\n#df_train = df_train[df_train['trip_duration']>1*60]\n#print('There is', df_train[df_train['trip_duration']<=1*60].shape[0], 'travels of less than 1min after filtering')","e8edd9cb":"#remove trip with an average speed greater than 200 km\/h (distances are in straigth lines, I could probably choose a smaller number) (#22 after the two filters)\n#print('There is', df_train[df_train['distances']\/(df_train['trip_duration'])>=200\/3600].shape[0], 'travels with an average speed faster than 200km\/h before filtering')\n#df_train = df_train[df_train['distances']\/(df_train['trip_duration'])<200\/3600]\n#print('There is', df_train[df_train['distances']\/(df_train['trip_duration'])>=200\/3600].shape[0], 'travels with an average speed faster than 200km\/h after filtering')","87c3deae":"# remove trips that took longer that 3 hours (Who does that ??) (#2101 after filtering)\n#print('There is', df_train[df_train['trip_duration']>=3*3600].shape[0], 'travels that took longer than 3 hours before filtering')\n#df_train = df_train[df_train['trip_duration']<3*3600]\n#print('There is', df_train[df_train['trip_duration']>=3*3600].shape[0], 'travels that took longer than 3 hours after filtering')","45fbc860":"#remove trip with an average speed less than 1 km\/h (I could probably choose a bigger number) (#3233 after the filters)\n#print('There is', df_train[df_train['distances']\/(df_train['trip_duration'])<=1\/3600].shape[0], 'travels with an average speed slower than 1km\/h before filtering (you walk at ~5km\/h)')\n#df_train = df_train[df_train['distances']\/(df_train['trip_duration'])>1\/3600]\n#print('There is', df_train[df_train['distances']\/(df_train['trip_duration'])<=1\/3600].shape[0], 'travels with an average speed slower than 1km\/h after filtering')","7a3bf947":"#print('We filtered','{:.3}'.format((l-df_train.shape[0])\/l*100), '% of the dataset' )","5906849f":"def featuresSelection(df_in):\n    VARS_CAT = [ 'store_and_fwd_flag' ]\n    VARS_NUM = ['pickup_latitude', 'pickup_longitude', 'dropoff_latitude', 'dropoff_longitude', 'zone', 'distances', 'pickup_Month', 'pickup_Hour', 'pickup_Weekend', 'passenger_count', 'vendor_id' ]\n    vars_cat = VARS_CAT\n    vars_num = VARS_NUM\n\n    X=df_in.loc[:, vars_cat + vars_num]\n\n    for cat in vars_cat:\n        X[cat] = X[cat].astype('category').cat.codes\n\n    return X","aabaf7c4":"X_train = featuresSelection(df_train)\ntarget = 'trip_duration'\ny_train = df_train.loc[:, target]\nprint(X_train.shape, y_train.shape)\ny_train = np.log1p( y_train )\nX_train.head()","8996aec4":"from sklearn.model_selection import cross_val_score, train_test_split\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.linear_model import SGDRegressor\nfrom sklearn.metrics import mean_squared_log_error as MSLE\nimport xgboost","09545fbb":"X_train_sample, X_validation, y_train_sample, y_validation = train_test_split(X_train, y_train, test_size=.2, random_state=42 )\nprint(X_train_sample.shape, y_train_sample.shape , X_validation.shape, y_validation.shape)\nX_train_sample.head(5)","e5630c5b":"#min_samples_leaf = {  1: 0.14335025261894946,  2: 0.13981831370645642,   3: 0.13852060557356807,  4: 0.1374604137021863, 5: 0.13701190316428685, 6: 0.13719592541154788,  7: 0.13647552678899308,   8: 0.13668619429239404,  9: 0.13678934918189598, 10: 0.13720206662667936, 15: 0.1378838545097919,   20: 0.13858468007164235,  25: 0.1397767624826059, 30: 0.14040835836429333, 35: 0.14162848146663448,  40: 0.14219905657487034,  45: 0.14265841548835242, 50: 0.14374664124817566, 100: 0.14924626267746,   150: 0.15302159678464494, 200: 0.15600849362124466, 250: 0.1578977545855252,  300: 0.16053779676581148, }\n#plt.plot(min_samples_leaf.keys(), min_samples_leaf.values());\n#plt.title('min_samples_leaf optimization');\n#plt.legend(\" with hyperparameters: RandomForestRegressor(bootstrap=True, criterion='mse', max_depth=None, max_features=0.4, max_leaf_nodes=None, min_impurity_decrease=0.0, min_impurity_split=None, min_samples_leaf=9, min_samples_split=2, min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=-1, oob_score=False, random_state=50, verbose=0, warm_start=False)\") #plt.legend(\" with features: pickup_latitude\tpickup_longitude\tdropoff_latitude\tdropoff_longitude\tzone\tdistances\tpickup_Month\tpickup_Hour\tpickup_Weekend\tpassenger_count \")\n#plt.xlabel('min_samples_leaf');\n#plt.ylabel('MSLE score');\n#min(min_samples_leaf, key=min_samples_leaf.get)","79eb52bc":"#%%time\n#modelparams = { 'booster':'gbtree', 'verbosity':1, 'max_depth':15, 'subsample': 1, 'lamda':0, 'max_delta_step':3, 'objective':'reg:linear', 'learning_rate':0.08, 'colsample_bytree':0.9, 'colsample_bylevel':0.9}\n#data_train = xgboost.DMatrix(X_train_sample,y_train_sample)\n#model = xgboost.train(modelparams, data_train, num_boost_round=200)","40e29fba":"#real = list(np.expm1(y_validation))\n#predicted = list(np.expm1(model.predict(xgboost.DMatrix(X_validation))))\n#print('\\nMean Square Log Error score:', MSLE(real, predicted))","f49f0178":"%%time\n#rf = RandomForestRegressor( n_estimators=100, min_samples_leaf=1, max_depth=None, max_features=.4, oob_score=False, bootstrap=True, n_jobs=-1 )\n\nmodelparams = { 'booster':'gbtree', 'verbosity':1, 'max_depth':15, 'subsample': 1, 'lamda':0, 'max_delta_step':3, 'objective':'reg:linear', 'learning_rate':0.08, 'colsample_bytree':0.9, 'colsample_bylevel':0.9}\ndata_train = xgboost.DMatrix(X_train,y_train)\nxg = xgboost.train(modelparams, data_train, num_boost_round=200)","a73d18e2":"#%%time\n#rf.fit( X_train, y_train );\n#rf.feature_importances_","e0d4fc75":"#rf1_scores=-cross_val_score( rf, X_train, y_train, cv=5, scoring='neg_mean_squared_log_error' )\n#rf1_scores, np.mean(rf1_scores)","8f389a21":"X_test = featuresSelection(df_test)\nX_test.head()","142a2f60":"#y_test_predict = model_final.predict(X_test)\n#y_test_predict = np.expm1(y_test_predict)\n#y_test_predict[:5]","47fbc759":"y_test_predict = np.expm1(xg.predict(xgboost.DMatrix(X_test)))\ny_test_predict[:5]","324845f6":"submission = pd.DataFrame(df_test.loc[:, 'id'])\nsubmission['trip_duration']=y_test_predict\nprint(submission.shape)\nsubmission.head()","2365887e":"submission.to_csv(\"submit_file.csv\", index=False)","1f7bccc5":" ## **<a id=\"one-b\">I.b Features Selection & Extraction<\/a>**","a94f2ad7":"# Machine Learning: RF and XGBoost","ae181efc":"|parameters | value||parameters | value||parameters | value||parameters | value||parameters | value|\n|--------------|----------||--------------|----------||--------------|----------||--------------|----------||--------------|----------|\n|'booster'|'gbtree'| |'verbosity'|1| |'max_depth'|15| |'subsample'|1| |'objective'|'reg:linear'| \n | 'lamda'|0| | 'max_delta_step'|3| |'colsample_bytree'|0.9| |'colsample_bylevel'|0.9||'learning_rate'|0.08|\n","83867af8":"Without filters on the training data (as it is better) and after optimizings the hyperparameters of the random forest, my score is: 0.407 on the scoreboard. Not bad, but can I do better with XGBoost ?","964b7a74":"## **<a id=\"two-a\"> II.a Model Selection<\/a>**","6888b35d":"# Table Of Contents\n\n----------\n\n**[I. Features engineering](#one)**\n- [a. Filters](#one-a)\n- [b. Features Selection & Extraction](#one-b)\n\n**[II. Machine Learning](#two)**\n- [a. Model Selection](#two-a)\n    - [1.  Random Forest](#two-a-1)\n    - [2.  SGBoost](#two-a-2)\n\n- [b. Model Training](#two-b)\n- [c. Predictions](#two-c)\n\n**[III. Submission](#three)**\n\n--------------------","04dffecb":"There is a few hyper-parameters to optimize for the RandomForestRegressor:\n1. `n_estimators` or the number of trees in the forest. The bigger, the better, but the longer it takes. 10 is 30 sec, 100 is <10 min. \n2. `min_samples_leaf` or the number of samples in the final leaf. Best at 1, to get all the small variations.\n3. `max_features` or the number of features used for each trees. Best at 0.4 (40% of total, or 4 features used)\n\nThis model work best without any filters on the training data.","c517f26d":"I did a lot of hyperparameters optimizations for this model. Here is the parameters that worked best for me:","2450ec0d":"  # **<a id=\"two\"> II. Machine Learning<\/a>**","ac4e74a3":"With XGBoost, I managed to get 0.3994 on the public score with the filtering. Pretty much the same, too bad. Let's use the XGBoost model since it's technically under 0.40.","ed8b6de7":"### **<a id=\"two-a-2\"> II.a.2 XGBoost<\/a>**","6f1d9300":"## **<a id=\"two-c\"> II.c Predictions<\/a>**","cc02ef36":" ## **<a id=\"one-a\">I.a Filters<\/a>**","3d5fd232":"### **<a id=\"two-a-1\"> II.a.1 Random Forest<\/a>**","3b74cdc4":"I thougth of a few filters we can apply to the training data. Like delete the small distances, the fast trips... These filters didn't seemed to be helping.\nIt turns out that the models works better without it.","51ab196c":" # **<a id=\"one\"> I. Features engineering<\/a>**","533f833a":"Version 18 is another look at this competition. I am using data engineered on the fork *Maps and EDA*.\n\nThis is only the Machine Learning part. I already studied and engineered the data on an other Kernel (forked).\n\nBasically, I created a column with the distances in a straight line between the pickup and the dropoff (in km). The other feature, called `zones` is an indication of where the pickups are: inside Manhattan, near John F. Kennedy International Airport, near LaGuardia Airport, around Newark, on the rest of the city area, or outside.\n\nWithout that enginneering, I was at 0.43. Here, I will try to reach 0.40 or under","4040f07f":"## **<a id=\"two-b\"> II.b Model training<\/a>**","e19d6c66":"# **<a id=\"three\">Submission<\/a>**"}}