{"cell_type":{"2175b067":"code","16d01046":"code","7dc5024a":"code","9969e0eb":"code","04b81ce0":"code","91e015d3":"code","a45d07f0":"code","a7967afb":"code","2fe3de80":"code","7cb10afb":"code","c4248c8d":"code","fc7182d0":"code","36f5844a":"code","38f28622":"code","d4051b1d":"code","fae8c785":"code","565c94f2":"code","e638d682":"code","51526fd6":"code","9fdb536b":"code","2cbabac8":"code","a6ed5c22":"code","9e67543a":"code","5e4c334e":"code","902c8178":"code","4b8ee680":"code","27255568":"code","fbda6938":"code","aa1b0e3d":"code","6b10f7f9":"code","d433ce71":"code","582dfddc":"code","67173606":"code","4dfd498b":"code","d832e36d":"code","1e42f086":"code","ebc5d458":"code","648c951e":"code","6e507411":"markdown","5f2d18b8":"markdown","d3f88e68":"markdown","826e6f4e":"markdown","0fc9beb5":"markdown","f547fce2":"markdown","d72af3b0":"markdown","ab7d2cce":"markdown"},"source":{"2175b067":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","16d01046":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn \nimport tensorflow as tf\nimport sklearn\nimport math","7dc5024a":"Data=pd.read_csv('..\/input\/marketing-data\/marketing_data.csv')\nData.head()","9969e0eb":"Data.info()","04b81ce0":"list_medium_income=[]\nlist0=[]\na=pd.DataFrame(Data[' Income '].isna().values,columns=['Incomenan'])\nfor i in Data[' Income '].copy().dropna():\n    foo=int(i.split('$')[1].split(' ')[0].split('.')[0].replace(\",\", \"\"))\n    list_medium_income.append(foo)\nData[' Income ']=Data[' Income '].fillna(int(np.mean(list_medium_income)))\nfor i in Data[' Income ']:\n    if i ==int(np.mean(list_medium_income)):\n        list0.append(i*1000)\n        pass\n    else:\n        foo=int(i.split('$')[1].split(' ')[0].split('.')[0].replace(\",\", \"\"))\n        list0.append(foo*1000)\nData=pd.concat([Data.drop(columns=[' Income ','ID']),pd.DataFrame(list0,columns=['Income']),a],axis=1)","91e015d3":"day=[]\nmonth=[]\nyear=[]\nfor _ in Data['Dt_Customer']:\n    _=_.split('\/')\n    day.append(int(_[1]))\n    month.append(int(_[0]))\n    year.append(int(_[2]))\nData=pd.concat([Data.drop(columns=['Dt_Customer']),pd.DataFrame(day,columns=['day']),pd.DataFrame(month,columns=['month']),pd.DataFrame(year,columns=['year'])],axis=1)","a45d07f0":"# How many unique values are in Columns of Data ?\nfor i in Data.columns:\n    print(F'{i}:',len(Data[i].unique()))","a7967afb":"Data.head(5)","2fe3de80":"Data.info()","7cb10afb":"plt.figure(figsize=(20,10))\nseaborn.heatmap(Data.corr(),annot=True)\nplt.show()","c4248c8d":"label = 'Accept','Not Aceept'\nplt.pie([len(Data['Response'][Data['Response']==1]),len(Data['Response'][Data['Response']==0])],labels=label,autopct='%1.1f%%',shadow=True, startangle=90)\nplt.title('Last Campagin');","fc7182d0":"Data.describe().T","36f5844a":"for i in Data.columns:\n    if Data[i].dtype=='<i8': \n        print('type int64:',i,'Mean:',int(Data[i].mean()))\n    else:\n        print('type object:',i,'Mode:',Data[i].mode()[0])","38f28622":"for _ in Data.columns:\n    if _=='Response'or _=='Income':\n        pass\n    else:\n        fig = plt.figure(figsize = (10, 5))\n        uniqe_list=[i for i in Data[Data['Response']==1][f'{_}'].unique()]\n        uniqe_values=[len(Data[Data['Response']==1][f'{_}'][Data[Data['Response']==1][f'{_}']==i]) for i in Data[Data['Response']==1][f'{_}'].unique()]\n\n        plt.bar(uniqe_list, uniqe_values, color ='pink',width = 0.4)\n        plt.xlabel(f\"{_} values of  who accepted last campaign  features\",fontsize=10)\n        plt.ylabel(f\"{_} values of  who accepted last campaign  Values\",fontsize=10)\n        plt.title(f\"{_} values of  who accepted last campaign \",fontsize=15);","d4051b1d":"# mean,mode of accept response \nfor i in Data.columns:\n    if Data[i].dtype=='<i8': \n        print('type int64:',i,'Mean:',int(Data[i][Data['Response']==1].mean()))\n    else:\n        print('type object:',i,'Mode:',Data[i][Data['Response']==1].mode()[0])","fae8c785":"# mean,mode of don't accept response \nfor i in Data.columns:\n    if Data[i].dtype=='<i8': \n        print('type int64:',i,'Mean:',int(Data[i][Data['Response']==0].mean()))\n    else:\n        print('type object:',i,'Mode:',Data[i][Data['Response']==0].mode()[0])","565c94f2":"Data.head()","e638d682":"education=pd.get_dummies(Data['Education'],drop_first=True)\nmarital=pd.get_dummies(Data['Marital_Status'],drop_first=True)\ncountry=pd.get_dummies(Data['Country'],drop_first=True)\n\u0131ncomenan=pd.get_dummies(Data['Incomenan'],drop_first=True)","51526fd6":"new_data=pd.concat([Data.drop(columns=['Education','Marital_Status','Incomenan','Country']),education,marital,country,\u0131ncomenan],axis=1)","9fdb536b":"new_data.head()","2cbabac8":"x=new_data.copy().drop(columns=['Response']).values\ny=new_data['Response'].values","a6ed5c22":"from sklearn.model_selection import train_test_split\nx_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.1, random_state=42)","9e67543a":"print('Train_x:',x_train.shape)\nprint('Train_y:',y_train.shape)\nprint('Test_x:',x_test.shape)\nprint('Test_y:',y_test.shape)","5e4c334e":"from sklearn.preprocessing import StandardScaler\nobje_ss=StandardScaler()\n\nx_train_ss=obje_ss.fit_transform(x_train)\nx_test_ss=obje_ss.fit_transform(x_test)","902c8178":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import GridSearchCV\n\nmodel_le=LogisticRegression(random_state=42,max_iter=10000)\nmodel_le.fit(x_train,y_train)\nparameters = {'C':[0.8,0.9,1,1.1,1.2]}\nmodel_le_grid = GridSearchCV(model_le, parameters,cv=10,verbose=1,n_jobs=-1).fit(x_train,y_train)\nprint(model_le_grid.best_params_)\n\nprint('Logistic Regression Train score:',model_le.score(x_train,y_train)*100)\nprint('Logistic Regression Cros validation score:',model_le_grid.best_score_*100)","4b8ee680":"model_le=LogisticRegression(random_state=42,C=0.8)\nmodel_le.fit(x_train,y_train)\nprint('Logistic Regression Train score:',model_le.score(x_train,y_train)*100)","27255568":"from sklearn import svm\n\nmodel_svc=svm.SVC(random_state=42)\nparameters = {'kernel':('linear', 'rbf','poly'), 'C':[0.8,0.9,1,1.1,1.2],'degree':[3,4,5,6]}\nmodel_svc_grid = GridSearchCV(model_svc, parameters,cv=10,verbose=1,n_jobs=-1).fit(x_train_ss,y_train)\nprint(model_svc_grid.best_params_)\n\n\nprint('Support Vecktor Classification Cros validation score:',model_svc_grid.best_score_*100)","fbda6938":"model_svc=svm.SVC(random_state=42,C=0.8,kernel='linear')\nmodel_svc.fit(x_train_ss,y_train)\nprint('Support Vecktor Classification Train score:',model_svc.score(x_train_ss,y_train)*100)","aa1b0e3d":"from sklearn.ensemble import RandomForestClassifier\n\nmodel_rfc=RandomForestClassifier(n_jobs=-1)\nparameters = {'n_estimators':[50,100,200,300,400],'max_depth':[3,4,5,6]}\nmodel_rfc_grid = GridSearchCV(model_rfc, parameters,cv=10,verbose=1,n_jobs=-1).fit(x_train,y_train)\nprint(model_rfc_grid.best_params_)\n\nprint('Random Forest Classifier Cros validation score:',model_rfc_grid.best_score_*100)","6b10f7f9":"model_rfc=RandomForestClassifier(n_jobs=-1,n_estimators=100,max_depth=6)\nmodel_rfc.fit(x_train,y_train)\nprint('Random Forest Classifier Train score:',model_rfc.score(x_train,y_train)*100)","d433ce71":"from xgboost import XGBClassifier\n\nmodel_xgb=XGBClassifier(n_jobs=-1,random_state=42,eval_metric='logloss')\nparameters = {'n_estimators':[50,100,200],'max_depth':[3,4,5,6],'learning_rate':[0.1,0.01]}\nmodel_xgb_grid = GridSearchCV(model_xgb, parameters,cv=10,verbose=1).fit(x_train,y_train)\nprint(model_xgb_grid.best_params_)\n\nprint('XGB Classifier Cros validation score:',model_xgb_grid.best_score_*100)","582dfddc":"model_xgb=XGBClassifier(n_jobs=-1,random_state=42,n_estimators=50,max_depth=5,learning_rate=0.1,eval_metric='logloss')\nmodel_xgb.fit(x_train,y_train)\nprint('XGB Classifier Classifier Train score:',model_xgb.score(x_train,y_train)*100)","67173606":"model_dnn=tf.keras.Sequential()\n\nmodel_dnn.add(tf.keras.layers.Dense(25,activation='relu',input_dim=44))\nmodel_dnn.add(tf.keras.layers.Dropout(0.5))\nmodel_dnn.add(tf.keras.layers.Dense(10,activation='relu'))\nmodel_dnn.add(tf.keras.layers.Dropout(0.5))\nmodel_dnn.add(tf.keras.layers.Dense(1,activation='sigmoid'))\nmodel_dnn.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.01),loss=\"binary_crossentropy\",metrics=['accuracy'])","4dfd498b":"model_dnn.summary()","d832e36d":"epoch=300\nhistory=model_dnn.fit(x_train_ss,y_train,epochs=epoch,verbose=0)   ","1e42f086":"print('Loss:',history.history['loss'][-1])\nprint('Accuracy: %',history.history['accuracy'][-1]*100)","ebc5d458":"plt.plot(history.history['loss'], label='loss')\nplt.xlim([0,epoch])\nplt.xlabel('Epoch')\nplt.ylabel('Error')\nplt.legend()\nplt.grid(True)","648c951e":"from sklearn.metrics import classification_report\n\ndef test_score(model_name):\n  for i in model_name: \n    print(f'{i.__class__} \\n{classification_report(y_test,i.predict(x_test))}')\n\ndef test_score_ss(model_name):\n  for i in model_name:\n    print(f'{i.__class__} \\n{classification_report(y_test,i.predict(x_test_ss))}') \n\nliste_test=[model_le,model_rfc,model_xgb]\nliste_test_ss=[model_svc]\n\ntest_score(liste_test)\ntest_score_ss(liste_test_ss)\n\nliste_dnn=[]\nfor i in model_dnn.predict(x_test_ss):\n  if i<0.5:\n    liste_dnn.append(0)\n  else:\n    liste_dnn.append(1)\nprint(f'{model_dnn.__class__} \\n{classification_report(y_test,liste_dnn)}')","6e507411":"## Support Vector Classication ","5f2d18b8":"## DNN","d3f88e68":"## Tree","826e6f4e":"# Model","0fc9beb5":"## Logistic Regression","f547fce2":"1. **ID**=Customer's unique identifier\n1. **Year_Birth**=Customer's birth year\n1. **Education**=Customer's education level\n1. **Marital_Status**=Customer's marital status\n1. **Income**=Customer's yearly household income\n1. **Kidhome**=Number of children in customer's household\n1. **Teenhome**=Number of teenagers in customer's household\n1. **Dt_Customer**=Date of customer's enrollment with the company\n1. **Recency**=Number of days since customer's last purchase\n1. **MntWines**=Amount spent on wine in the last 2 years\n1. **MntFruits**=Amount spent on fruits in the last 2 years\n1. **MntMeatProducts**=Amount spent on meat in the last 2 years\n1. **MntFishProducts**=Amount spent on fish in the last 2 years\n1. **MntSweetProducts**=Amount spent on sweets in the last 2 years\n1. **MntGoldProds**=Amount spent on gold in the last 2 years\n1. **NumDealsPurchases**=Number of purchases made with a discount\n1. **NumWebPurchases**=Number of purchases made through the company's web site\n1. **NumCatalogPurchases**=Number of purchases made using a catalogue\n1. **NumStorePurchases**=Number of purchases made directly in stores\n1. **NumWebVisitsMonth**=Number of visits to company's web site in the last month\n1. **AcceptedCmp3**=1 if customer accepted the offer in the 3rd campaign, 0 otherwise\n1. **AcceptedCmp4**=1 if customer accepted the offer in the 4th campaign, 0 otherwise\n1. **AcceptedCmp5**=1 if customer accepted the offer in the 5th campaign, 0 otherwise\n1. **AcceptedCmp1**=1 if customer accepted the offer in the 1st campaign, 0 otherwise\n1. **AcceptedCmp2**=1 if customer accepted the offer in the 2nd campaign, 0 otherwise\n1. **Response**=1 if customer accepted the offer in the last campaign, 0 otherwise\n1. **Complain**=1 if customer complained in the last 2 years, 0 otherwise\n1. **Country**=Customer's location","d72af3b0":"Task Details\nYou're a marketing analyst and you've been told by the Chief Marketing Officer that recent marketing campaigns have not been as effective as they were expected to be. You need to analyze the data set to understand this problem and propose data-driven solutions.\n\nExpected Submission\nSubmit a well documented notebook with these four sections:\n\n* Section 01: Exploratory Data Analysis\n1. Are there any null values or outliers? How will you wrangle\/handle them?\n1. Are there any variables that warrant transformations?\n1. Are there any useful variables that you can engineer with the given data?\n1. Do you notice any patterns or anomalies in the data? Can you plot them?\n* Section 02: Statistical Analysis\nPlease run statistical tests in the form of regressions to answer these questions & propose data-driven action recommendations to your CMO. Make sure to interpret your results with non-statistical jargon so your CMO can understand your findings.\n\n1. What factors are significantly related to the number of store purchases?\n1. Does US fare significantly better than the Rest of the World in terms of total purchases?\n1. Your supervisor insists that people who buy gold are more conservative. Therefore, people who spent an above average amount on gold in the last 2 years would have more in store purchases. Justify or refute this statement using an appropriate statistical test\n1. Fish has Omega 3 fatty acids which are good for the brain. Accordingly, do \"Married PhD candidates\" have a significant relation with amount spent on fish? What other factors are significantly related to amount spent on fish? (Hint: use your knowledge of interaction variables\/effects)\n1. Is there a significant relationship between geographical regional and success of a campaign?\n* Section 03: Data Visualization\nPlease plot and visualize the answers to the below questions.\n\n1. Which marketing campaign is most successful?\n1. What does the average customer look like for this company?\n1. Which products are performing best?\n1. Which channels are underperforming?\n* Section 04: CMO Recommendations\nBring together everything from Sections 01 to 03 and provide data-driven recommendations\/suggestions to your CMO.\n\n**Evaluation**\nThis is not a formal competition, so results won't be measured using a strict metric. Rather, what one would like to see is a well-defined process of exploratory and statistical analysis with insightful conclusions.\n\n1. Data Exploration - Was the data wrangled properly? How well was the data analyzed? Are there any useful visualizations? Does the reader learn any new techniques through this submission? A great entry will be informative and thought provoking.\n1. Statistical Analysis - Were the right statistical tests used? How well was the statistical output interpreted? A great entry will interpret results without the use of any statistical jargon.\n1. Business Recommendation - Were the recommendations tied to your analysis in Sections 1-3? Are they data-driven and focused on marketing concepts such as targets, channels, or products?\n1. Documentation - Are your code, and notebook well documented so a reader can understand what you did? Are your sources clearly cited? A high quality analysis should be concise and clear at each step so the rationale is easy to follow and the process is reproducible.","ab7d2cce":"![market_analysis-1.jpg](attachment:0fa06826-cec0-41c7-adbc-118dd56012ef.jpg)"}}