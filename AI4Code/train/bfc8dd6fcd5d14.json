{"cell_type":{"5a79abfd":"code","b70a3d1d":"code","76647504":"code","015aaacb":"code","a1a5cb21":"code","fb70d8e3":"code","21b167ce":"code","fc2cf0e8":"code","5b18b10b":"code","d73f93aa":"code","c1cbce62":"code","226735d4":"code","8893a929":"code","4384f8aa":"code","69a22349":"code","0a33fdc8":"code","92ca8a21":"code","b8919fd7":"code","daec10a2":"code","ebcbdb6c":"code","9e9f8670":"code","694aeb6e":"markdown","cf8b2225":"markdown","7f2d7db3":"markdown","b31b49d8":"markdown"},"source":{"5a79abfd":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport os\nprint(os.listdir('..\/input'))\n#Read the data in dataframe\ndf = pd.read_json('..\/input\/news-category-dataset\/News_Category_Dataset.json', lines=True)\nprint(df.shape)\ndf.head()","b70a3d1d":"df.groupby(by='category')['category'].count().sort_values(ascending=False)","76647504":"#Merge `THE WORLDPOST` and `WORLDPOST` into single category\ndf.category = df.category.map(lambda x: \"WORLDPOST\" if x == \"THE WORLDPOST\" else x)\ndf.groupby(by='category')['category'].count().sort_values(ascending=False)","015aaacb":"#Number of articles published by month\ndf['date'] = pd.to_datetime(df['date'], errors='coerce')\ndf.groupby(pd.Grouper(key='date', freq='M'))['date'].count().sort_values(ascending=False)","a1a5cb21":"#Average articles per month\ndf.groupby(pd.Grouper(key='date', freq='M'))['date'].count().mean()","fb70d8e3":"#Popular category per month\ndf.groupby(pd.Grouper(key='date', freq='M'))['category'].agg(lambda x:x.value_counts().index[0])","21b167ce":"from keras.preprocessing.text import Tokenizer, text_to_word_sequence\n\n#Tokenize headline\ntokenizer = Tokenizer()\ntokenizer.fit_on_texts(df.headline)\nX = tokenizer.texts_to_sequences(df.headline)\ndf['words'] = X\ndf.head()","fc2cf0e8":"#Use GLOVE pretrained word-embeddings\nEMBEDDING_DIMENSION=100\nembeddings_index = {}\nf = open('..\/input\/glove-global-vectors-for-word-representation\/glove.6B.100d.txt')\nfor line in f:\n    values = line.split()\n    word = values[0]\n    coefs = np.asarray(values[1:], dtype='float32')\n    embeddings_index[word] = coefs\nf.close()","5b18b10b":"#Create a weight matrix for words in training docs\nword_index = tokenizer.word_index\nembedding_matrix = np.zeros((len(word_index) + 1, 100))\n\nfor word, index in word_index.items():\n    embedding_vector = embeddings_index.get(word)\n    if embedding_vector is not None:\n        embedding_matrix[index] = embedding_vector","d73f93aa":"from keras.layers.embeddings import Embedding\nfrom keras.initializers import Constant\n\n#Create embedding layer from embedding matrix\nembedding_layer = Embedding(len(word_index)+1, EMBEDDING_DIMENSION,\n                            embeddings_initializer=Constant(embedding_matrix),\n                            input_length=50, trainable=False)","c1cbce62":"from keras.utils import np_utils\nfrom keras.preprocessing import sequence\nfrom sklearn.model_selection import train_test_split\n\n#Prepare training and test data\nX = np.array(list(sequence.pad_sequences(df.words, maxlen=50)))\n\ncategory_dict = dict((i,k) for k,i in enumerate(list(df.groupby('category').groups.keys())))\ndf['labels'] = df['category'].apply(lambda x: category_dict[x])\nY = np_utils.to_categorical(list(df.labels))","226735d4":"from keras.models import Sequential\nfrom keras.layers import Dense, LSTM, Dropout, BatchNormalization, Flatten\n\n#RNN with LSTM\nmodel = Sequential()\n\nmodel.add(embedding_layer)\nmodel.add(LSTM(300, dropout=0.25, recurrent_dropout=0.25))\nmodel.add(Dropout(0.25))\nmodel.add(Dense(256, activation='sigmoid'))\nmodel.add(Dropout(0.25))\nmodel.add(BatchNormalization())\nmodel.add(Dense(len(category_dict), activation='softmax'))\nmodel.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n\nmodel.summary()","8893a929":"model_history = model.fit(X, Y, batch_size=128, validation_split=0.4, epochs=15)","4384f8aa":"import matplotlib.pyplot as plt\n\nacc = model_history.history['acc']\nval_acc = model_history.history['val_acc']\nloss = model_history.history['loss']\nval_loss = model_history.history['val_loss']\nepochs = range(1, len(acc) + 1)\n\nplt.title('Training and validation accuracy')\nplt.plot(epochs, acc, 'red', label='Training acc')\nplt.plot(epochs, val_acc, 'blue', label='Validation acc')\nplt.legend()\n\nplt.figure()\nplt.title('Training and validation loss')\nplt.plot(epochs, loss, 'red', label='Training loss')\nplt.plot(epochs, val_loss, 'blue', label='Validation loss')\nplt.legend()\n\nplt.show()","69a22349":"tokenizer = Tokenizer()\ntokenizer.fit_on_texts(df.short_description)\nX = tokenizer.texts_to_sequences(df.short_description)\ndf['short_description_tokenize'] = X\ndf.head()","0a33fdc8":"word_index = tokenizer.word_index\nembedding_matrix = np.zeros((len(word_index) + 1, 100))\n\nfor word, index in word_index.items():\n    embedding_vector = embeddings_index.get(word)\n    if embedding_vector is not None:\n        embedding_matrix[index] = embedding_vector","92ca8a21":"embedding_layer = Embedding(len(word_index)+1, EMBEDDING_DIMENSION,\n                            embeddings_initializer=Constant(embedding_matrix),\n                            input_length=50, trainable=False)","b8919fd7":"X = np.array(list(sequence.pad_sequences(df.short_description_tokenize, maxlen=50)))","daec10a2":"model = Sequential()\n\nmodel.add(embedding_layer)\nmodel.add(LSTM(300, dropout=0.25, recurrent_dropout=0.25))\nmodel.add(Dropout(0.25))\nmodel.add(Dense(256, activation='sigmoid'))\nmodel.add(Dropout(0.25))\nmodel.add(BatchNormalization())\nmodel.add(Dense(len(category_dict), activation='softmax'))\nmodel.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n\nmodel.summary()","ebcbdb6c":"model_history = model.fit(X, Y, batch_size=128, validation_split=0.3, epochs=15)","9e9f8670":"import matplotlib.pyplot as plt\n\nacc = model_history.history['acc']\nval_acc = model_history.history['val_acc']\nloss = model_history.history['loss']\nval_loss = model_history.history['val_loss']\nepochs = range(1, len(acc) + 1)\n\nplt.title('Training and validation accuracy')\nplt.plot(epochs, acc, 'red', label='Training acc')\nplt.plot(epochs, val_acc, 'blue', label='Validation acc')\nplt.legend()\n\nplt.figure()\nplt.title('Training and validation loss')\nplt.plot(epochs, loss, 'red', label='Training loss')\nplt.plot(epochs, val_loss, 'blue', label='Validation loss')\nplt.legend()\n\nplt.show()","694aeb6e":"## Model","cf8b2225":"### Using Short Description to train model","7f2d7db3":"## Preprocessing","b31b49d8":"## Analysis"}}