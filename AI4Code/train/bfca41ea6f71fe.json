{"cell_type":{"38f50bd5":"code","633f128c":"code","27ed7f9f":"code","dd688635":"code","63eb8b98":"code","9d6dc95a":"code","c881939f":"code","d117fef4":"code","1a2ed39b":"code","1939bc0e":"code","5ffac844":"code","f535c0cd":"code","ab07d3d3":"code","c323c1b0":"code","1ce8fa10":"code","bc70209d":"code","27775a8a":"code","4331d944":"code","4a2a3a0c":"code","540e3cff":"code","97a62c86":"code","0c2094d9":"code","9fd80db3":"code","d1a69cb6":"code","313d52ff":"code","ecd4fcb0":"code","592a1a05":"code","91033a7f":"code","5d7faa19":"code","efd39608":"code","e1c72392":"code","995e89ab":"code","8bb7866b":"code","bd9b94dc":"code","d24a2111":"code","53a1aecd":"code","e0b0dc03":"code","22d6e3a5":"code","fcea4f9f":"code","61fc1159":"code","40d74660":"code","079dad1d":"code","58ec94e4":"code","7647236d":"code","3745d2f4":"code","f57689a5":"code","aa0f92fd":"code","9e6da388":"code","a920c1f9":"code","9bd808ae":"code","a348fa5d":"code","bd0b0316":"code","66a53cd1":"code","3fd5a2f0":"markdown"},"source":{"38f50bd5":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","633f128c":"import json\nimport re\nimport nltk\nfrom bs4 import BeautifulSoup\nfrom tensorflow.python.keras.preprocessing.sequence import pad_sequences\nfrom tensorflow.python.keras.preprocessing.text import Tokenizer\nimport zipfile","27ed7f9f":"import matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline","dd688635":"nltk.download('stopwords')\nfrom nltk.corpus import stopwords","63eb8b98":"print(os.listdir(\"..\"))\nprint(os.listdir(\"..\/input\"))\nprint(os.listdir(\"..\/input\/word2vec-nlp-tutorial\"))","9d6dc95a":"raw_train_data = pd.read_csv(\"..\/input\/word2vec-nlp-tutorial\/labeledTrainData.tsv.zip\", delimiter='\\t')\nraw_train_data.head()","c881939f":"raw_train_len = raw_train_data[\"review\"].apply(len)\nraw_train_len","d117fef4":"plt.hist(raw_train_len, bins=300, color='g')\nplt.yscale('log')\nplt.title('Log-Scale Number of Reviews vs. Length of Reviews')\nplt.xlabel('length of review')\nplt.ylabel('number of review')","1a2ed39b":"raw_train_len.describe()","1939bc0e":"fig, axe = plt.subplots(ncols=1)\nfig.set_size_inches(6,3)\nsns.countplot(raw_train_data['sentiment'])","5ffac844":"print('number of + : {}'.format(raw_train_data['sentiment'].value_counts()[1]))\nprint('number of - : {}'.format(raw_train_data['sentiment'].value_counts()[0]))","f535c0cd":"train_word_cnt = raw_train_data['review'].apply(lambda x:len(x.split(' '))) # number of words in each review \ntrain_word_cnt","ab07d3d3":"plt.figure(figsize=(8,5))\nplt.hist(train_word_cnt, bins=50, color='g')\nplt.yscale('log')\nplt.title('Log-Scale Number of Reviews vs. Number of Reviews')\nplt.xlabel('number of words')\nplt.ylabel('number of reviews')","c323c1b0":"train_word_cnt.describe()","1ce8fa10":"r_qmarks = np.mean(raw_train_data['review'].apply(lambda x: '?' in x))\nr_fullstops = np.mean(raw_train_data['review'].apply(lambda x: '.' in x))\nr_capitals = np.mean(raw_train_data['review'].apply(lambda x: max(y.isupper() for y in x)))\nr_numbers = np.mean(raw_train_data['review'].apply(lambda x: max(y.isdigit() for y in x)))\n\nprint('\ubb3c\uc74c\ud45c\uac00 \uc788\ub294 \ub9ac\ubdf0 : {:.2f}%'.format(r_qmarks * 100))\nprint('\ub9c8\uce68\ud45c\uac00 \uc788\ub294 \ub9ac\ubdf0 : {:.2f}%'.format(r_fullstops * 100))\nprint('\ub300\ubb38\uc790\uac00 \uc788\ub294 \ub9ac\ubdf0 : {:.2f}%'.format(r_capitals * 100))\nprint('\uc22b\uc790\uac00 \uc788\ub294 \ub9ac\ubdf0 : {:.2f}%'.format(r_numbers * 100))","bc70209d":"review = raw_train_data['review'][0]\nreview_text = BeautifulSoup(review,\"html5lib\").get_text() # html \ud0dc\uadf8\ub97c \uc81c\uac70\ud55c\ub2e4.\nreview_text = re.sub(\"[^a-zA-Z]\",\" \",review_text) # \uc54c\ud30c\ubcb3\uc744 \uc81c\uc678\ud558\uace0 \ubaa8\ub450 \uacf5\ubc31\uc73c\ub85c \ubc14\uafbc\ub2e4.","27775a8a":"print(review)","4331d944":"print(review_text)","4a2a3a0c":"stop_words = set(stopwords.words('english'))\n\nreview_text = review_text.lower()\nwords = review_text.split()\nwords = [w for w in words if not w in stop_words]","540e3cff":"words[:10]","97a62c86":"clean_review = ' '.join(words)\nprint(clean_review)","0c2094d9":"def preprocess(review,remove_stopwords = False):\n    # html \uc81c\uac70\n    review_text = BeautifulSoup(review,\"html5lib\").get_text()\n    \n    # \ud2b9\uc218\ubb38\uc790 \uc81c\uac70\n    review_text = re.sub(\"[^a-zA-Z]\",\" \",review_text)\n    \n    # \uc18c\ubb38\uc790\ub85c \ud1b5\uc77c \ud6c4 \ub9ac\uc2a4\ud2b8\ud654\n    words = review_text.lower().split()\n    \n    if remove_stopwords:\n        # \ubd88\uc6a9\uc5b4 \uc81c\uac70\n        stop_words = set(stopwords.words('english'))\n        words = [w for w in words if not w in stop_words]\n \n    clean_review = ' '.join(words)\n    \n    return clean_review","9fd80db3":"clean_train_reviews = []\nfor review in raw_train_data['review']:\n    clean_train_reviews.append(preprocess(review,remove_stopwords = True))","d1a69cb6":"clean_train_reviews[0]","313d52ff":"clean_train_df = pd.DataFrame({'id':raw_train_data['id'], 'review':clean_train_reviews, 'sentiment':raw_train_data['sentiment']})","ecd4fcb0":"clean_train_df","592a1a05":"tokenizer = Tokenizer(oov_token='<UNK>')\n#oov_token(out of vocab token)\uc740 fitting\ub41c tokenzier\uac00 \ucc98\uc74c\ubcf4\ub294 \ub2e8\uc5b4\ub97c \uc5b4\ub5bb\uac8c \ub2e4\ub8f0\uc9c0\n#\uc989 \uc0ac\uc804\uc5d0 \uc5c6\ub294 \ub2e8\uc5b4\uc5d0 \uc5b4\ub5a4 \uac12\uc744 \ucde8\ud560\uac74\uc9c0 \uacb0\uc815\ud55c\ub2e4.\n#\ubcf8\uc778\uc740 <UNK>\uc73c\ub85c \uc124\uc815\ud558\uc600\uc73c\ub098 \ubb50\ub85c \ud558\ub358 \ud06c\uac8c \uc0c1\uad00\uc5c6\ub2e4.\ntokenizer.fit_on_texts(clean_train_reviews)\ntext_sequences = tokenizer.texts_to_sequences(clean_train_reviews)","91033a7f":"print(clean_train_reviews[0])","5d7faa19":"print(text_sequences[0])","efd39608":"word_vocab = tokenizer.word_index\nprint(word_vocab)\n#<UNK>\uc758 \uc778\ub371\uc2a4\uac00 1\uc778\uac83\uc744 \ud655\uc778\ud560 \uc218 \uc788\ub2e4.","e1c72392":"print(word_vocab[\"stuff\"])","995e89ab":"print(\"\uc804\uccb4 \ub2e8\uc5b4 \uc218: \",len(word_vocab))\n#\ub9cc\uc57d oov_token\uc744 \ucd94\uac00\ud558\uc9c0 \uc54a\uc744 \uacbd\uc6b0 \uc0ac\uc804\uc758 \ud06c\uae30\ub294 74066-1=74065\uac00 \ub41c\ub2e4.","8bb7866b":"data_configs = {}\n\ndata_configs['vocab'] = word_vocab\ndata_configs['vocab_size'] = len(word_vocab) + 1","bd9b94dc":"MAX_SEQUENCE_LENGTH = 174\ntrain_inputs = pad_sequences(text_sequences,maxlen=MAX_SEQUENCE_LENGTH, padding='post')\n\nprint('shape of train data: ', train_inputs.shape)","d24a2111":"train_labels = np.array(raw_train_data['sentiment'])\nprint('shape of train labels: ',train_labels.shape)","53a1aecd":"DATA_IN_PATH = '.\/data_in\/'\nTRAIN_INPUT_DATA = 'train_input.npy'\nTRAIN_LABEL_DATA = 'train_label.npy'\nTRAIN_CLEAN_DATA = 'train_clean.csv'\nDATA_CONFIGS = 'data_configs.json'\n\nif not os.path.exists(DATA_IN_PATH):\n    os.makedirs(DATA_IN_PATH)","e0b0dc03":"np.save(open(DATA_IN_PATH + TRAIN_INPUT_DATA, 'wb'), train_inputs)\nnp.save(open(DATA_IN_PATH + TRAIN_LABEL_DATA, 'wb'), train_labels)\n\nclean_train_df.to_csv(DATA_IN_PATH + TRAIN_CLEAN_DATA, index = False)\n\njson.dump(data_configs, open(DATA_IN_PATH + DATA_CONFIGS, 'w'), ensure_ascii=False)","22d6e3a5":"test_data = pd.read_csv(\"..\/input\/word2vec-nlp-tutorial\/testData.tsv.zip\", delimiter='\\t')\ntest_data.head()","fcea4f9f":"test_data","61fc1159":"clean_test_reviews = []\n\nfor review in test_data['review']:\n    clean_test_reviews.append(preprocess(review, remove_stopwords = True))\nclean_test_df = pd.DataFrame({'review': clean_test_reviews, 'id': test_data['id']})\ntest_id = np.array(test_data['id'])\n\n#\uc5ec\uae30\uc11c \ud14c\uc2a4\ud2b8\uc14b\uc5d0 \ub300\ud574 tokenizer\ub97c fitting \ud558\uc9c0 \uc54a\ub294\ub2e4\ub294 \uac83\uc744 \uc720\uc758\ud558\uc790.\ntext_sequences = tokenizer.texts_to_sequences(clean_test_reviews)\ntest_inputs = pad_sequences(text_sequences, maxlen=MAX_SEQUENCE_LENGTH, padding='post')","40d74660":"TEST_INPUT_DATA = 'test_input.npy'\nTEST_CLEAN_DATA = 'test_clean.csv'\nTEST_ID_DATA = 'test_id.npy'\n\nnp.save(open(DATA_IN_PATH + TEST_INPUT_DATA, 'wb'), test_inputs)\nnp.save(open(DATA_IN_PATH + TEST_ID_DATA, 'wb'), test_id)\nclean_test_df.to_csv(DATA_IN_PATH + TEST_CLEAN_DATA, index = False)","079dad1d":"import tensorflow as tf\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Embedding, GlobalAveragePooling1D, Dense, LSTM, Dropout, Bidirectional","58ec94e4":"vocab_size = len(word_vocab)+1\nvocab_size","7647236d":"def small_model():\n    model = Sequential()\n    model.add(Embedding(vocab_size, 16))\n    model.add(GlobalAveragePooling1D())\n    model.add(Dense(16, activation = 'relu'))\n    model.add(Dropout(0.5))\n    model.add(Dense(1, activation = 'sigmoid'))\n    model.summary()\n    \n    model.compile(optimizer='adam',\n              loss='binary_crossentropy',\n              metrics=['accuracy'])\n    return model","3745d2f4":"model = small_model()","f57689a5":"history = model.fit(train_inputs,\n                    train_labels,\n                    epochs=10,\n                    batch_size=256,\n                    validation_split = 0.3)","aa0f92fd":"del model\ndel history","9e6da388":"def big_model():\n    model = Sequential()\n    model.add(Embedding(vocab_size, 16))\n    model.add(Bidirectional(LSTM(64, recurrent_dropout=0.1)))\n    model.add(Dropout(0.5))\n    model.add(Dense(32, activation = 'relu'))\n    model.add(Dropout(0.4))\n    model.add(Dense(1, activation = 'sigmoid'))\n    model.summary()\n    \n    model.compile(optimizer='adam',\n              loss='binary_crossentropy',\n              metrics=['accuracy'])\n    return model","a920c1f9":"model = big_model()","9bd808ae":"history = model.fit(train_inputs,\n                    train_labels,\n                    epochs=10,\n                    batch_size=256,\n                    validation_split = 0.3)","a348fa5d":"model.save('my_model.h5')","bd0b0316":"%%time\ntest_label = model.predict_classes(test_inputs)","66a53cd1":"def submit(predictions):\n    test_data['sentiment'] = predictions\n    test_data.to_csv('submission.csv', index=False, columns=['id','sentiment'])\n\nsubmit(test_label)","3fd5a2f0":"# Build the model"}}