{"cell_type":{"ca65e8da":"code","71b581ea":"code","7e46043e":"code","2dd476d6":"code","85b32e28":"code","d1828844":"code","fc66375d":"code","2adbd6e1":"code","4d39ca6f":"code","d04c8fe9":"code","4a36d3fb":"code","4e940803":"code","ba14bd13":"code","31490fe6":"code","87564246":"code","74d6662a":"code","a0bc9b63":"code","afde0a30":"code","2f1ab195":"code","bd14fcb4":"code","8c747fb5":"code","2c715f52":"code","e911e598":"code","1ce9dca0":"markdown","d1258d83":"markdown","7ba07cd2":"markdown","74c9fc41":"markdown","48f5e309":"markdown","d16d28e3":"markdown","68519584":"markdown","67c62da4":"markdown"},"source":{"ca65e8da":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","71b581ea":"heart = pd.read_csv('\/kaggle\/input\/heart-disease-uci\/heart.csv')\nheart.head()","7e46043e":"heart.info()","2dd476d6":"import matplotlib.pyplot as plt\nimport seaborn as sns","85b32e28":"# Histogram\n\nsns.distplot(heart['age'], bins = 15)\nplt.title('Distribution of Age')\nplt.show()","d1828844":"# Count how many people have the disease and how many people don't.\n\nax = plt.subplot()\nsns.countplot(heart['target'])\nax.set_xticks([0,1])\nax.set_xticklabels(['No Disease', 'Disease'])\nplt.show()","fc66375d":"# How many men and women have heart disease?\n\nax = plt.subplot()\nsns.countplot(x = heart['target'], hue = 'sex', data = heart)\nax.set_xticklabels(['No Disease', 'Disease'])\nplt.legend(['Female', 'Male'])\nplt.title('Heart Disease Based on Sex')\nplt.show()","2adbd6e1":"# Correlation Matrix\n\ncorr_matrix = heart.corr()\nplt.figure(figsize = (16,8))\nsns.heatmap(corr_matrix, annot = True)\nplt.show()","4d39ca6f":"# Let's investigate the distibution of age based on heart disease.\n\nplt.figure(figsize = (12,8))\nsns.distplot(heart.age[heart['target'] == 0], label = 'No Disease', bins = 20)\nsns.distplot(heart.age[heart['target'] == 1], label = 'Disease', bins = 20)\nplt.legend()\nplt.title('Age Distribution Based on Disease')\nplt.show()","d04c8fe9":"plt.figure(figsize = (10,6))\nplt.scatter(x = heart.age[heart['target'] == 0], y = heart.thalach[heart['target'] == 0], c = 'red')\nplt.scatter(x = heart.age[heart['target'] == 1], y = heart.thalach[heart['target'] == 1], c = 'blue')\nplt.legend(['No Disease', 'Disease'])\nplt.xlabel('Age')\nplt.ylabel('Maximum Heart Rate')\nplt.show()","4a36d3fb":"# Import necessary libraries\n\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.metrics import accuracy_score, confusion_matrix, classification_report\nfrom sklearn.metrics import roc_curve, auc","4e940803":"# Create dummy variables.\n\na = pd.get_dummies(heart['sex'], prefix = 'sex')\nb = pd.get_dummies(heart['cp'], prefix = 'cp')\nc = pd.get_dummies(heart['restecg'], prefix = 'restecg')\nd = pd.get_dummies(heart['fbs'], prefix = 'fbs')\ne = pd.get_dummies(heart['exang'], prefix = 'exang')\nf = pd.get_dummies(heart['slope'], prefix = 'slope')\ng = pd.get_dummies(heart['ca'], prefix = 'ca')\nh = pd.get_dummies(heart['thal'], prefix = 'thal')\n\ndummies = [heart, a, b, c, d, e, f, g, h]\nheart = pd.concat(dummies, axis = 1)\nheart = heart.drop(columns = ['sex', 'cp', 'fbs', 'restecg', 'exang', 'slope', 'ca', 'thal'])\nheart.head()","ba14bd13":"# Split up data by features and target variables\n\nX = heart.drop(['target'], axis = 1).values\ny = heart['target']\nprint(X.shape)\nprint(y.shape)","31490fe6":"# Scale feature columns\n\nscaler = StandardScaler()\nX_scaled = scaler.fit_transform(X)","87564246":"# Split data into train and test sets.\n\nX_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size = 0.2, random_state = 42)\nprint(X_train.shape)\nprint(X_test.shape)\nprint(y_train.shape)\nprint(y_test.shape)","74d6662a":"from sklearn.linear_model import LogisticRegression\n\nlogreg = LogisticRegression()\nlogreg.fit(X_train, y_train)\ny_pred = logreg.predict(X_test)\nprint(logreg.score(X_test, y_test))\nprint(logreg.score(X_train, y_train))","a0bc9b63":"# Pick hyperparameters that result in the best accuracy rate.\n\nfrom sklearn.svm import SVC\n\nlargest = {'value':0, 'gamma':1, 'C':1}\nfor gamma in range(1,7):\n    for C in range(1,7):\n        classifier = SVC(kernel = 'linear', C = C, gamma = gamma)\n        classifier.fit(X_train, y_train)\n        score = classifier.score(X_test, y_test)\n        if (score > largest['value']):\n            largest['value'] = score\n            largest['gamma'] = gamma\n            largest['C'] = C\n\nprint(largest)\nprint(classifier.score(X_train, y_train))","afde0a30":"from sklearn.ensemble import RandomForestClassifier\n\nrf = RandomForestClassifier(n_estimators = 25, bootstrap = True, max_features = 'sqrt')\nrf.fit(X_train, y_train)\ny_pred_rf = rf.predict(X_test)\nprint(rf.score(X_train, y_train))\nprint(rf.score(X_test, y_test))","2f1ab195":"from sklearn.neighbors import KNeighborsClassifier\n\ntrain_accuracies = []\ntest_accuracies = []\nfor k in range(1,11):\n    knn = KNeighborsClassifier(n_neighbors = k)\n    knn.fit(X_train, y_train)\n    train_accuracies.append(knn.score(X_train, y_train))\n    test_accuracies.append(knn.score(X_test, y_test))\n    \n# Plotting the results.\n\nk_list = range(1,11)\nplt.plot(k_list, test_accuracies)\nplt.xlabel('k')\nplt.ylabel('Validation Accuracy')\nplt.title('Accuracy Scores')\nplt.show()\n\nprint('Accuracy score for KNN: ', round(max(train_accuracies) * 100), '%.')\nprint('Accuracy score for KNN: ', round(max(test_accuracies) * 100), '%.')","bd14fcb4":"from sklearn.naive_bayes import GaussianNB\n\ngnb = GaussianNB()\ngnb.fit(X_train, y_train)\nprint(gnb.score(X_train, y_train))\nprint(gnb.score(X_test, y_test))","8c747fb5":"accuracy_scores = {'Model':['Logistic Regression', 'SVM', 'Random Forest', 'KNN', 'Naive Bayes'], \n                   'Training Score':[87.6,87.19,99.58,100,46.69],\n                   'Test Score':[86.88,83.60,83.6,87,47.54]}\naccuracy_scores_df = pd.DataFrame(accuracy_scores)\naccuracy_scores_df","2c715f52":"# Let's compare train set accuracy score.\n\nsns.barplot(x = 'Model', y = 'Training Score', data = accuracy_scores_df)\nplt.title('Training Accuracy Rate per Model')\nplt.show()","e911e598":"# Let's compare test set accuracy score.\n\nsns.barplot(x = 'Model', y = 'Test Score', data = accuracy_scores_df)\nplt.title('Test Accuracy Rate per Model')\nplt.show()","1ce9dca0":"### Logistic Regression","d1258d83":"### KNN ","7ba07cd2":"### Support Vector Machine","74c9fc41":"### Naive Bayes Classifier","48f5e309":"### Random Forest ","d16d28e3":"# Visualizing the Data","68519584":"# Machine Learning","67c62da4":"### KNN algorithm has 100% accuracy rate from its training set and an 87% accuracy rate from its test set, making it the best choice for classifying heart disease patients. Naive Bayes was not a good classifier, it got less than half of the target values correct."}}