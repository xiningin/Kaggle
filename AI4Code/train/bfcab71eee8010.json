{"cell_type":{"0bcd86d0":"code","e11157de":"code","3053c061":"code","c6d94134":"code","a907dae4":"code","da4a0321":"code","c8d55868":"code","20243167":"code","412bcd3c":"code","199a5f5d":"code","b2c31384":"code","5a34ba30":"code","9a4d1055":"code","56da6fb5":"code","7765ab56":"code","d318a5cf":"markdown","3950e90b":"markdown","650f157e":"markdown","5467a290":"markdown"},"source":{"0bcd86d0":"# Functions to read and show images.\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\n\n   \nd0 = pd.read_csv('..\/input\/train.csv')\n\nprint(d0.head(5)) # print first five rows of d0.\n\n# save the labels into a variable l.\nl = d0['label']\n\n# Drop the label feature and store the pixel data in d.\nd = d0.drop(\"label\",axis=1)\n\n    \n        \n","e11157de":"print(d.shape)\nprint(l.shape)","3053c061":"# display or plot a number.\nplt.figure(figsize=(7,7))\nidx = 9\n\ngrid_data = d.iloc[idx].values.reshape(28,28)  # reshape from 1d to 2d pixel array\nplt.imshow(grid_data, interpolation = \"none\", cmap = \"gray\")\nplt.show()\n\nprint(l[idx])","c6d94134":"labels = l\ndata = d\n\nprint(\"the shape of sample data = \", data.shape)\n","a907dae4":"# Data-preprocessing: Standardizing the data\n\nfrom sklearn.preprocessing import StandardScaler\nstandardized_data = StandardScaler().fit_transform(data)\nprint(standardized_data.shape)\n","da4a0321":"#find the co-variance matrix which is : A^T * A\/n\nsample_data = standardized_data\nn = sample_data.shape[1]\n# matrix multiplication using numpy\ncovar_matrix = np.matmul(sample_data.T , sample_data)\ncovar_matrix = covar_matrix\/n\nprint ( \"The shape of variance matrix = \", covar_matrix.shape)","c8d55868":"# finding the top two eigen-values and corresponding eigen-vectors \n# for projecting onto a 2-Dim space.\n\nfrom scipy.linalg import eigh \n\n# the parameter 'eigvals' is defined (low value to heigh value) \n# eigh function will return the eigen values in asending order\n# this code generates only the top 2 (782 and 783) eigenvalues.\nvalues, vectors = eigh(covar_matrix, eigvals=(782,783))\n\nprint(\"Shape of eigen vectors = \",vectors.shape)\n# converting the eigen vectors into (2,d) shape for easyness of further computations\nvectors = vectors.T\n\nprint(\"Updated shape of eigen vectors = \",vectors.shape)\n# here the vectors[1] represent the eigen vector corresponding 1st principal eigen vector\n# here the vectors[0] represent the eigen vector corresponding 2nd principal eigen vector","20243167":"# projecting the original data sample on the plane \n#formed by two principal eigen vectors by vector-vector multiplication.\n\nimport matplotlib.pyplot as plt\nnew_coordinates = np.matmul(vectors, sample_data.T)\n\nprint (\" resultanat new data points' shape \", vectors.shape, \"X\", sample_data.T.shape,\" = \", new_coordinates.shape)","412bcd3c":"import pandas as pd\n\n# appending label to the 2d projected data\nnew_coordinates = np.vstack((new_coordinates, labels)).T\n\n# creating a new data frame for ploting the labeled points.\ndataframe = pd.DataFrame(data=new_coordinates, columns=(\"1st_principal\", \"2nd_principal\", \"label\"))\nprint(dataframe.head())","199a5f5d":"print(new_coordinates.shape)","b2c31384":"# ploting the 2d data points with seaborn\nimport seaborn as sn\nsn.FacetGrid(dataframe, hue=\"label\", size=6).map(plt.scatter, '1st_principal', '2nd_principal').add_legend()\nplt.show()","5a34ba30":"# initializing the pca\nfrom sklearn import decomposition\npca = decomposition.PCA()\n","9a4d1055":"# configuring the parameteres\n# the number of components = 2\npca.n_components = 2\npca_data = pca.fit_transform(sample_data)\n\n# pca_reduced will contain the 2-d projects of simple data\nprint(\"shape of pca_reduced.shape = \", pca_data.shape)\n\n","56da6fb5":"# attaching the label for each 2-d data point \npca_data = np.vstack((pca_data.T, labels)).T\n\n# creating a new data fram which help us in ploting the result data\npca_df = pd.DataFrame(data=pca_data, columns=(\"1st_principal\", \"2nd_principal\", \"label\"))\nsn.FacetGrid(pca_df, hue=\"label\", size=6).map(plt.scatter, '1st_principal', '2nd_principal').add_legend()\nplt.show()","7765ab56":"# PCA for dimensionality redcution (non-visualization)\n\npca.n_components = 784\npca_data = pca.fit_transform(sample_data)\n\npercentage_var_explained = pca.explained_variance_ \/ np.sum(pca.explained_variance_);\n\ncum_var_explained = np.cumsum(percentage_var_explained)\n\n# Plot the PCA spectrum\nplt.figure(1, figsize=(6, 4))\n\nplt.clf()\nplt.plot(cum_var_explained, linewidth=2)\nplt.axis('tight')\nplt.grid()\nplt.xlabel('n_components')\nplt.ylabel('Cumulative_explained_variance')\nplt.show()\n\n\n# If we take 200-dimensions, approx. 90% of variance is expalined.","d318a5cf":"# PCA using Scikit-Learn","3950e90b":"# PCA for dimensionality redcution (not for visualization)","650f157e":"#  2D Visualization using PCA ","5467a290":"# Load MNIST Data "}}