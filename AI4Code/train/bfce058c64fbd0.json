{"cell_type":{"e3d662fd":"code","fae7f6b9":"code","e7a1c26c":"code","b87ce387":"code","de78be14":"code","f1bd7209":"code","561e56fd":"code","7b5084d6":"code","18f4fff8":"code","7078b921":"code","8fe18325":"code","8a647c41":"code","5488d7a8":"code","f3d51bca":"code","1ba1918e":"code","7e79c848":"code","6ceaa629":"markdown","6faa569e":"markdown","80ce9575":"markdown","919c5f8f":"markdown","b3753c5f":"markdown","9e0614e1":"markdown","9e5f8dbe":"markdown","dda01454":"markdown","d3cf2f53":"markdown","0d5a9d2c":"markdown","2d906352":"markdown","20a7ffd9":"markdown","7a6a305d":"markdown"},"source":{"e3d662fd":"import os\nimport pandas as pd \nimport numpy as np\nimport tensorflow.compat.v1 as tf\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport re\n\nfrom nltk.stem import WordNetLemmatizer\nfrom nltk.tokenize import sent_tokenize, word_tokenize\nfrom nltk import pos_tag\nfrom nltk.corpus import wordnet\n\nimport sys\nimport warnings\nimport random\n\nif not sys.warnoptions:\n    warnings.simplefilter(\"ignore\")\ntf.disable_v2_behavior()","fae7f6b9":"df_train_csv=pd.read_csv('..\/input\/predict-product-category-from-given-invoice\/Dataset\/Train.csv')\ndf_test_csv=pd.read_csv('..\/input\/predict-product-category-from-given-invoice\/Dataset\/Test.csv')","e7a1c26c":"print(df_train_csv.shape)\nprint(df_test_csv.shape)\ndf_train_csv.head(1)","b87ce387":"df_test_csv.head(1)","de78be14":"def category_statistic(dataset):\n    \"\"\"\n    Check category distribution .\n    \"\"\"\n    category_count = dataset['Product_Category'].value_counts()\n    sns.set(style=\"darkgrid\")\n    sns.set(rc={'figure.figsize':(10,5)})\n    sns.barplot( category_count.index, category_count.values, alpha=0.9)\n    plt.title('Frequency Distribution of category (train set)',fontsize=14)\n    plt.ylabel('Number of Occurrences', fontsize=14)\n    plt.xticks(size='small',rotation=90,fontsize=6)\n    plt.xlabel('Category', fontsize=14)\n    plt.show()\n\ndf_csv = df_train_csv\ncategory_statistic(df_csv)","f1bd7209":"cat_sample_max = max(df_csv['Product_Category'].value_counts())\ndelete_cate = ['CLASS-1248','CLASS-1688','CLASS-2015','CLASS-2146','CLASS-1957','CLASS-1838','CLASS-1567','CLASS-1919','CLASS-1850',\n               'CLASS-2112','CLASS-1477','CLASS-2241','CLASS-1870','CLASS-1429','CLASS-2003','CLASS-1309','CLASS-1964','CLASS-1322',\n               'CLASS-1294','CLASS-1770','CLASS-1983','CLASS-1652','CLASS-1867','CLASS-2038','CLASS-1805','CLASS-2152']\n\nfor l in  delete_cate:\n    df_csv.drop(df_csv.loc[df_csv['Product_Category']==l].index, inplace=True)\n\nall_text = list(df_csv['Item_Description'][...])\nall_cate = np.array(df_csv['Product_Category'].tolist())","561e56fd":"def clean_str(string):\n    \"\"\"\n    String cleaning .\n    \"\"\"\n    string = re.sub(r\"[^A-Za-z0-9]\", \" \", string) # remove unused charactor other than english letter and number, use space to replace\n    return string.strip()                         # delete the first and last space\n","7b5084d6":"\ndef get_wordnet_pos(treebank_tag):\n    \"\"\"\n    Return the POS of each word for later usage .\n    \"\"\"\n    if treebank_tag.startswith('J'):\n        return wordnet.ADJ\n    elif treebank_tag.startswith('V'):\n        return wordnet.VERB\n    elif treebank_tag.startswith('N'):\n        return wordnet.NOUN\n    elif treebank_tag.startswith('R'):\n        return wordnet.ADV\n    else:\n        return wordnet.NOUN   # no need to change \n","18f4fff8":"def text_lemmatization(l,lemmatizer,t_text, word_count):\n    \"\"\"\n    Tokenization: Split the text into words. \n    Lemmatize the text .\n    \"\"\"\n    for i in range(len(t_text)):\n        text_clean = clean_str(t_text[i])     # clean texts, remove useless symbols\n        text_word = word_tokenize(text_clean) # set each individual token\n        text_pos = pos_tag(text_word)         # pos tagging each token [word,POS]\n        text_lemma = \"\"\n        for item in text_pos:                 # lemmatizing each token\n            # Put each word after lemmatization into the list\n            text_lemma = text_lemma + \" \" + (lemmatizer.lemmatize(item[0],get_wordnet_pos(item[1])))     \n        l.append(text_lemma.strip())          # append the preprocessed sample to x_train list, remove the space\n        word_count.append(len(text_pos))\n    return l, word_count   ","7078b921":"# preprocessing\ntext = list()\nword_count = list()                   #statistics how many words in each sample\nlemmatizer = WordNetLemmatizer()      # model used to lemmatize word (defined by package nltk)\ntext_lemmatized, word_count = text_lemmatization(text,lemmatizer,all_text, word_count)","8fe18325":"class Indexer:\n    # Tokenizer\n    def __init__(self):\n        self.counter = 1\n        self.d = {\"<unk>\": 0}\n        self.rev = {}\n        self._lock = False\n        self.word_count = {}\n        self.rev_d = {}\n        self.rev[0] = \"unk\"\n    def convert(self, w):\n        if w not in self.d:\n            if self._lock:\n                return self.d[\"<unk>\"]\n            self.d[w] = self.counter\n            self.rev[self.counter] = w\n            self.counter += 1\n            self.word_count[self.d[w]] = 0\n        self.word_count[self.d[w]] = self.word_count[self.d[w]] + 1\n        return self.d[w]\n    def convertback(self, w):\n        return self.rev[w]\n    def lock(self):\n        self._lock = True\n","8a647c41":"all_data = []\nsplit_data = []\ntokenizer = Indexer()\nmax_len_sent = 0\nnum_of_padding = 0\nvocabulary = 0\nmax_index = 0\nfor i, t in enumerate(text_lemmatized):\n    current_convert = [tokenizer.convert(w) for w in t.split()]\n    m = max(current_convert)\n    max_index = max(max_index,max(current_convert))\n    max_len_sent = max(max_len_sent, len(current_convert))\n    split_data.append(current_convert)\nvocabulary_size = max_index + 1    \n    \n    \nfor i, t in enumerate(split_data):  \n    if (len(t) < max_len_sent):\n        num_of_padding = max_len_sent - len(t)\n        for n in range(0,num_of_padding):\n            t.append(0)        \n    all_data.append(t)\n\n\ntext_df = pd.DataFrame(all_data)\ncate_df = pd.DataFrame(df_csv['Product_Category'])\ndf_text_cate =  text_df \ndf_text_cate['category'] = 'cat'\nfor i in range(0,len(cate_df)):\n    df_text_cate.at[i,'category'] = cate_df.iloc[i]['Product_Category']\n\nlabels =  list(df_text_cate.category.unique())\nfor l in labels:\n    df_text_cate[l] = 0\nfor i in range(0,len(df_text_cate)):\n    cat = df_text_cate.iloc[i]['category']\n    df_text_cate.at[i,cat] = 1  ","5488d7a8":"\ndef oversampling_and_split_dataset(df,trainingratio,l):\n    \n    trainingratio = 0.7\n    index_list = {}\n    number_of_samples_cat = {}\n    training_samples = {}\n    train_random_indices = {}\n    duplicated_train_indices = {}\n    for l in labels:\n        index_list[l] = np.array(df[df.category == l].index)\n        number_of_samples_cat[l] = len(index_list[l]) \n        training_samples[l] = round(number_of_samples_cat[l]*trainingratio)  \n        train_random_indices[l] = np.random.choice(index_list[l], training_samples[l], replace = False)\n        if (l == 'CLASS-1758'): \n            duplicated_train_indices[l] = train_random_indices[l]\n        else:\n            baseline = int(round(cat_sample_max*trainingratio))\n            duplicated_train_indices[l] = np.random.choice(train_random_indices[l], baseline , replace = True)\n            duplicated_train_indices[l] = np.array(duplicated_train_indices[l])  \n    \n    train_indices = duplicated_train_indices['CLASS-1249']\n    for l in labels:\n        if (l != 'CLASS-1249'): \n            train_indices  = np.concatenate([train_indices, duplicated_train_indices[l]])\n   \n    training_data = df.iloc[train_indices,:]      \n    testing_data = df.drop(train_indices,axis=0)  \n\n    #shuffle the data\n    training_data=training_data.sample(frac=1).reset_index(drop=True)\n\n    split = int(len(testing_data)\/2)\n    train_y = training_data.iloc[:,27:57]\n    train_x = training_data.iloc[:,0:26] \n    \n    valid_y = testing_data.iloc[0:split,27:57]\n    valid_x = testing_data.iloc[0:split,0:26]  \n    \n    test_y = testing_data.iloc[split:,27:57]\n    test_x = testing_data.iloc[split:,0:26] \n    \n    return train_x, train_y, valid_x, valid_y, test_x, test_y\n    \n\ntrain_ratio = 0.7\ntraining_x, training_y, validation_x, validation_y, testing_x, testing_y = oversampling_and_split_dataset(df_text_cate,train_ratio,labels)\n","f3d51bca":"# Model Hyperparameters\nembedding_dim = 50\nfilter_sizes = \"3,4,5\"\nnum_filters = 100\ndropout_keep_prob = 0.5\nl2_reg_lambda = 0.0\n\n# Training parameters\nbatch_size = 64\nnum_epochs = 80\nnum_batches =len(training_x)\/64 ","1ba1918e":"\nclass TextCNN(object):\n    \"\"\"\n    A CNN for text classification.\n    Uses an embedding layer, followed by a convolutional, max-pooling and softmax layer.\n    \"\"\"\n    def __init__(\n      self, sequence_length, num_classes, vocab_size,\n      embedding_size, filter_sizes, num_filters, l2_reg_lambda=0.0):\n\n        # Placeholders for input, output and dropout\n        self.input_x = tf.placeholder(tf.int32, [None, sequence_length], name=\"input_x\")\n        self.input_y = tf.placeholder(tf.float32, [None, num_classes], name=\"input_y\")\n        self.dropout_keep_prob = tf.placeholder(tf.float32, name=\"dropout_keep_prob\")\n\n        # Keeping track of l2 regularization loss (optional)\n        l2_loss = tf.constant(0.0)\n\n        # Embedding layer\n        with tf.device('\/cpu:0'), tf.name_scope(\"embedding\"):\n            self.W = tf.Variable(tf.random_uniform([vocab_size, embedding_size], -1.0, 1.0),name=\"W\")  # We initialize embedding matrix using a random uniform distribution.\n            self.embedded_chars = tf.nn.embedding_lookup(self.W, self.input_x)                        \n            self.embedded_chars_expanded = tf.expand_dims(self.embedded_chars, -1) \n\n        # Create a convolution + maxpool layer for each filter size\n        pooled_outputs = []\n        for i, filter_size in enumerate(filter_sizes): \n            with tf.name_scope(\"conv-maxpool-%s\" % filter_size):\n                # Convolution Layer\n                filter_shape = [filter_size, embedding_size, 1, num_filters] \n                W = tf.Variable(tf.truncated_normal(filter_shape, stddev=0.1), name=\"W\")  \n                b = tf.Variable(tf.constant(0.1, shape=[num_filters]), name=\"b\")  \n                # Each filter slides over the whole embedding, but varies in how many words it covers\n                conv = tf.nn.conv2d(\n                    self.embedded_chars_expanded,  \n                    W,                                  \n                    strides=[1, 1, 1, 1],           \n                    padding=\"VALID\",\n                    name=\"conv\")\n                # h is the result of applying the nonlinearity to the convolution output. \n                h = tf.nn.relu(tf.nn.bias_add(conv, b), name=\"relu\") \n                # Maxpooling over the outputs\n                pooled = tf.nn.max_pool(\n                    h,\n                    ksize=[1, sequence_length - filter_size + 1, 1, 1],  \n                    strides=[1, 1, 1, 1],\n                    padding='VALID',\n                    name=\"pool\")\n                pooled_outputs.append(pooled)\n\n        # Combine all the pooled features\n        num_filters_total = num_filters * len(filter_sizes)\n        #  Once we have all the pooled output tensors from each filter size we combine them into one long feature vector of shape [batch_size, num_filters_total]. \n        self.h_pool = tf.concat(pooled_outputs, 3)\n        self.h_pool_flat = tf.reshape(self.h_pool, [-1, num_filters_total])\n\n        # Add dropout\n        with tf.name_scope(\"dropout\"):\n            self.h_drop = tf.nn.dropout(self.h_pool_flat, self.dropout_keep_prob)\n\n        # Final (unnormalized) scores and predictions\n        with tf.name_scope(\"output\"):\n            W = tf.get_variable(\n                \"W\",\n                shape=[num_filters_total, num_classes])\n            b = tf.Variable(tf.constant(0.1, shape=[num_classes]), name=\"b\")\n            l2_loss += tf.nn.l2_loss(W)\n            l2_loss += tf.nn.l2_loss(b)\n            self.scores = tf.nn.xw_plus_b(self.h_drop, W, b, name=\"scores\")\n            self.predictions = tf.argmax(self.scores, 1, name=\"predictions\")\n\n        # Calculate mean cross-entropy loss\n        with tf.name_scope(\"loss\"):\n            losses = tf.nn.softmax_cross_entropy_with_logits(logits=self.scores, labels=self.input_y)\n            self.loss = tf.reduce_mean(losses) + l2_reg_lambda * l2_loss\n\n        # Accuracy\n        with tf.name_scope(\"accuracy\"):\n            correct_predictions = tf.equal(self.predictions, tf.argmax(self.input_y, 1))\n            self.accuracy = tf.reduce_mean(tf.cast(correct_predictions, \"float\"), name=\"accuracy\")\n","7e79c848":"\nsess = tf.Session()\nwith sess.as_default():\n    cnn = TextCNN(\n        sequence_length=training_x.shape[1],\n        num_classes=training_y.shape[1],\n        vocab_size=vocabulary_size,\n        embedding_size=embedding_dim,\n        filter_sizes=list(map(int, filter_sizes.split(\",\"))),\n        num_filters=num_filters,\n        l2_reg_lambda=l2_reg_lambda)\n    global_step = tf.Variable(0, name=\"global_step\", trainable=False)\n    optimizer = tf.train.AdamOptimizer(1e-3)\n    grads_and_vars = optimizer.compute_gradients(cnn.loss)\n    train_op = optimizer.apply_gradients(grads_and_vars, global_step=global_step)\n\n    # Summaries for loss and accuracy\n    loss_summary = tf.summary.scalar(\"loss\", cnn.loss)\n    acc_summary = tf.summary.scalar(\"accuracy\", cnn.accuracy)\n\n    # Initialize all variables\n    sess.run(tf.global_variables_initializer())\n\n    for epoch in range(int(num_epochs)) :     \n        for i in range(int(num_batches)) :\n            off_1 = i * batch_size\n            off_2 = i * batch_size + batch_size\n            batch_x = training_x[off_1:off_2]\n            batch_y = training_y[off_1:off_2]\n            batch_x = np.asarray(batch_x)\n            batch_y = np.asarray(batch_y)\n            \n            batch_val_x = np.asarray(validation_x)\n            batch_val_y = np.asarray(validation_y)\n            \n        feed_dict_train = {\n                  cnn.input_x: batch_x,\n                  cnn.input_y: batch_y,\n                  cnn.dropout_keep_prob: dropout_keep_prob\n                }\n        \n        feed_dict_valid = {\n                  cnn.input_x: batch_val_x,\n                  cnn.input_y: batch_val_y,\n                  cnn.dropout_keep_prob: 1.0\n                }        \n        _, step, train_loss, train_accuracy = sess.run([train_op, global_step, cnn.loss, cnn.accuracy],feed_dict_train)\n        valid_loss, valid_accuracy= sess.run([cnn.loss, cnn.accuracy], feed_dict_valid)  \n        if epoch % 5 == 0:\n            print(\"Epoch{}: train_loss {:g}, train_acc {:g}, valid_loss {:g}, valid_acc {:g},\".format(epoch, train_loss, train_accuracy, valid_loss, valid_accuracy))\n\n    feed_dict_test = {\n                  cnn.input_x: testing_x,\n                  cnn.input_y: testing_y,\n                  cnn.dropout_keep_prob: 1.0\n                }\n    accuracy = sess.run([cnn.accuracy],feed_dict_test)\n    print(\"The final testing accuracy is:\")\n    print(accuracy)\n","6ceaa629":"## Text preprocessing\nIn this part, we preprocess the Item_Description column of the dataset. Firt, remove unused charactor other than english letter and number, use space to replace. Then delete the first and last space.","6faa569e":"Define a class Indexer. It can tokenize the words.","80ce9575":"From the plot above, we can see the dataset is extremely unbalanced. For the convenience of experiments, and to ensure that sufficient data in each category can be divided into training set, validation set, and testing set, we remove all categories with less than 100 samples.","919c5f8f":"In this project, we use convolutional neural network (CNN) to classify the category of invoice. In these samples of invoices, the message inside gives much information about the invoice category. CNN is commonly used in image classification. But here, instead of image pixels, the input are message sentences as a matrix. ","b3753c5f":"Here we found that the Product_Category column in testing dataset is missing! So what we can do is using training dataset as the whole dataset, and divide training dataset into training dataset, validation dataset and testing dataset.","9e0614e1":"# Invoice category classification using CNN (Tensorflow)","9e5f8dbe":"## Data insight\n### Load data and make some statistics on the dataset","dda01454":"### Input Placeholders\ntf.placeholder creates a placeholder variable that we feed to the network when we execute it at train or test time. The second argument is the shape of the input tensor. None means that the length of that dimension could be anything. In our case, the first dimension is the batch size, and using None allows the network to handle arbitrarily sized batches.The probability of keeping a neuron in the dropout layer is also an input to the network because we enable dropout only during training. We disable it when evaluating the model.\n\n### Embedding Layer\nThe first layer we define is the embedding layer, which maps vocabulary word indices into low-dimensional vector representations. It\u2019s essentially a lookup table that we learn from data. W is our embedding matrix that we learn during training\n\n### Convolution and Max-Pooling Layers\nWhen we build our convolutional layers followed by max-pooling, remember that we use filters of different sizes. Because each convolution produces tensors of different shapes we need to iterate through them, create a layer for each of them, and then merge the results into one big feature vector.\n\n### Dropout Layer\nDropout is the perhaps most popular method to regularize convolutional neural networks. The idea behind dropout is simple. A dropout layer stochastically \u201cdisables\u201d a fraction of its neurons. This prevent neurons from co-adapting and forces them to learn individually useful features. The fraction of neurons we keep enabled is defined by the dropout_keep_prob input to our network. We set this to something like 0.5 during training, and to 1 (disable dropout) during evaluation.\n\n### Loss and Accuracy\nUsing our scores we can define the loss function. The loss is a measurement of the error our network makes, and our goal is to minimize it. The standard loss function for categorization problems it the cross-entropy loss. tf.nn.softmax_cross_entropy_with_logits is a convenience function that calculates the cross-entropy loss for each class, given our scores and the correct input labels. We then take the mean of the losses. ","d3cf2f53":"Function get_wordnet_pos() returns the POS of each word.","0d5a9d2c":"Tokenizing the data. Convert each token into integer and calculate vocabulary_size for future model usage. Since the word amounts of each sample maybe not same, we add padding at the end of the short-length samples to reach the same length as the longest sample.","2d906352":"Function text_lemmatization() conducts 2 tasks: (1)Split the text into words (2) Lemmatize the text","20a7ffd9":"Split dataset into training data, validation data and testing data. Then we conduct oversampling on the smaller sampling sets.","7a6a305d":"## CNN model (Tensorflow)\nAfter preprocessing the data, we need to create the CNN model using Tensorflow. Before creating the model, we should define the model Hyperparameters and training parameters. I plan to use Tensorflow flag to define the parameters (for example tf.flags.DEFINE_float(\"dropout_keep_prob\", 0.7, \"Dropout keep probability (default: 0.7)\")), but this Kaggle kernel does not support it. "}}