{"cell_type":{"95cf39e1":"code","2ef2b9ef":"code","c0bad5ff":"code","b4f4f7cd":"code","008b7ef3":"markdown","3f336402":"markdown","f54c3152":"markdown"},"source":{"95cf39e1":"%%bash\nwget https:\/\/repo.anaconda.com\/miniconda\/Miniconda3-latest-Linux-x86_64.sh\nchmod 777 Miniconda3-latest-Linux-x86_64.sh\n.\/Miniconda3-latest-Linux-x86_64.sh -b -p \/usr\/local\/miniconda\nexport PATH=$PATH\":\/usr\/local\/miniconda\/bin\"\nconda update -y -n base -c defaults conda\nconda init bash\nsource ~\/.bashrc\n__conda_setup=\"$('\/usr\/local\/miniconda\/bin\/conda' 'shell.bash' 'hook' 2> \/dev\/null)\"\nif [ $? -eq 0 ]; then\n    eval \"$__conda_setup\"\nelse\n    if [ -f \"\/usr\/local\/miniconda\/etc\/profile.d\/conda.sh\" ]; then\n        . \"\/usr\/local\/miniconda\/etc\/profile.d\/conda.sh\"\n    else\n        export PATH=\"\/usr\/local\/miniconda\/bin:$PATH\"\n    fi\nfi\nunset __conda_setup\n# <<< conda initialize <<<\nconda activate base","2ef2b9ef":"!__conda_setup=\"$('\/usr\/local\/miniconda\/bin\/conda' 'shell.bash' 'hook' 2> \/dev\/null)\" && \\\neval \"$__conda_setup\" && \\\nunset __conda_setup && \\\nconda activate base && \\\nconda install -y pip && \\\npip3 install jupyterlab mlflow kaggle slackweb optuna numpy pandas scipy scikit-learn matplotlib seaborn psutil logzero japanize_matplotlib pylint yapf pyarrow pytorch-tabnet google-cloud-bigquery cython ipywidgets numba transformers datasets accelerate pytorch_lightning &&\\\npip3 install -U scikit-learn sentencepiece","c0bad5ff":"script=\"\"\"def run():\n    import os\n    import math\n    import random\n    import time\n\n    import numpy as np\n    import pandas as pd\n\n    import torch\n    import torch.nn as nn\n    from torch.utils.data import Dataset\n    from torch.utils.data import DataLoader\n\n    from transformers import AdamW\n    from transformers import AutoTokenizer\n    from transformers import AutoModel\n    from transformers import AutoConfig\n    from transformers import get_cosine_schedule_with_warmup\n    #from adabelief_pytorch import AdaBelief\n\n    from sklearn.model_selection import KFold\n\n    import gc\n    gc.enable()\n\n    SEED = 9000#int(time.time()) #random\n    #SEED = 11111 #manual\n    print(\"SEED=\",SEED) \n    NUM_GPUS=1\n    NUM_FOLDS = 15\n    NUM_EPOCHS = 4\n    BATCH_SIZE = 8\n    MAX_LEN = 250\n    LR_MIDDLE = 5e-5\n    LR_INPUT = 1e-4\n    EVAL_SCHEDULE = [(0.50, 16), (0.49, 8), (0.48, 4), (0.47, 2), (-1., 1)]\n    WEIGHT_DECAY=0.01#0.00075\n    LR_RATE=250\n    LR_HEAD = 1e-5\n    LR_MIDDLE = 5e-5\n    LR_INPUT = 1e-4\n\n    # ROBERTA_PATH = \"input\/deberta-xlarge\"\n    # TOKENIZER_PATH = \"input\/deberta-xlarge\"\n    # ROBERTA_PATH = \"..\/input\/debertalarge\"\n    # TOKENIZER_PATH = \"..\/input\/debertalarge\"\n    INPUT=\"..\/input\/\"\n    OUTPUT=\".\/\"\n    ROBERTA_PATH = f\"{INPUT}\/robertalarge\"\n    TOKENIZER_PATH = f\"{INPUT}\/robertalarge\"\n    DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n\n    train_df = pd.read_csv(f\"{INPUT}\/commonlitreadabilityprize\/train.csv\")\n\n    # Remove incomplete entries if any.\n    train_df.drop(train_df[(train_df.target == 0) & (train_df.standard_error == 0)].index,\n                  inplace=True)\n    train_df.reset_index(drop=True, inplace=True)\n\n    test_df = pd.read_csv(f\"{INPUT}\/commonlitreadabilityprize\/test.csv\")\n    submission_df = pd.read_csv(f\"{INPUT}\/commonlitreadabilityprize\/sample_submission.csv\")\n    tokenizer = AutoTokenizer.from_pretrained(TOKENIZER_PATH)\n\n\n    class LitDataset(Dataset):\n        def __init__(self, df, inference_only=False):\n            super().__init__()\n\n            self.df = df        \n            self.inference_only = inference_only\n            self.text = df.excerpt.tolist()\n            \n\n            if not self.inference_only:\n                self.target = torch.tensor(df.target.values, dtype=torch.float32)        \n\n            self.encoded = tokenizer.batch_encode_plus(\n                self.text,\n                padding = 'max_length',            \n                max_length = MAX_LEN,\n                truncation = True,\n                return_attention_mask=True\n            )        \n\n\n        def __len__(self):\n            return len(self.df)\n\n\n        def __getitem__(self, index):        \n            input_ids = torch.tensor(self.encoded['input_ids'][index])\n            attention_mask = torch.tensor(self.encoded['attention_mask'][index])\n\n            if self.inference_only:\n                return (input_ids, attention_mask)            \n            else:\n                target = self.target[index]\n                return (input_ids, attention_mask, target)\n\n    import pytorch_lightning as pl\n\n    TARGET_CENTER=train_df[\"target\"].median()\n    print(TARGET_CENTER)\n\n    class LitModel(pl.LightningModule):\n        def __init__(self):\n            super().__init__()\n\n            config = AutoConfig.from_pretrained(ROBERTA_PATH)\n            config.update({\"output_hidden_states\":True, \n                           \"hidden_dropout_prob\": 0.0,\n                           \"layer_norm_eps\": 1e-7})                       \n\n            self.roberta = AutoModel.from_pretrained(ROBERTA_PATH, config=config)  \n\n            self.attention = nn.Sequential(            \n                nn.Linear(1024, 512),    \n                nn.Tanh(),                       \n                #nn.Mish(),\n                nn.Linear(512, 1),\n                nn.Softmax(dim=1)\n            )\n\n            self.regressor = nn.Sequential(                        \n                nn.Linear(1024, 1)                        \n            )\n        def forward(self, input_ids, attention_mask):\n            roberta_output = self.roberta(input_ids=input_ids,\n                                          attention_mask=attention_mask)        \n\n            # There are a total of 13 layers of hidden states.\n            # 1 for the embedding layer, and 12 for the 12 Roberta layers.\n            # We take the hidden states from the last Roberta layer.\n            hidden_states = roberta_output.hidden_states\n\n            # The number of cells is MAX_LEN.\n            # The size of the hidden state of each cell is 768 (for roberta-base).\n            # In order to condense hidden states of all cells to a context vector,\n            # we compute a weighted average of the hidden states of all cells.\n            # We compute the weight of each cell, using the attention neural network.\n            hid=hidden_states[-1]\n            weights = self.attention(hid)\n            # weights.shape is BATCH_SIZE x MAX_LEN x 1\n            # last_layer_hidden_states.shape is BATCH_SIZE x MAX_LEN x 768        \n            # Now we compute context_vector as the weighted average.\n            # context_vector.shape is BATCH_SIZE x 768\n            context_vector=torch.sum(weights * hid, dim=1)\n\n            # Now we reduce the context vector to the prediction score.\n            return self.regressor(context_vector)\n\n        def configure_optimizers(self):\n            named_parameters = list(self.named_parameters())  \n            roberta_parameters = named_parameters[:len(named_parameters)-8]    \n            attention_parameters = named_parameters[len(named_parameters)-6:len(named_parameters)-2]\n            regressor_parameters = named_parameters[len(named_parameters)-2:]\n\n\n            attention_group = [params for (name, params) in attention_parameters]\n            regressor_group = [params for (name, params) in regressor_parameters]\n\n            parameters = []\n            parameters.append({\"params\": attention_group})\n            parameters.append({\"params\": regressor_group})\n\n            for layer_num, (name, params) in enumerate(roberta_parameters):\n                weight_decay = 0.0 if \"bias\" in name else WEIGHT_DECAY\n\n                lr = LR_HEAD*LR_RATE\n\n                if layer_num >= len(named_parameters)\/3:        \n                    lr = LR_MIDDLE*LR_RATE\n\n                if layer_num >= len(named_parameters)*2\/3:\n                    lr = LR_INPUT*LR_RATE\n\n\n                parameters.append({\"params\": params,\n                                   \"weight_decay\": weight_decay,\n                                   \"lr\": lr})\n            optimizer=AdamW(parameters)\n            #optimizer=AdaBelief(parameters)\n            scheduler = get_cosine_schedule_with_warmup(\n                optimizer,\n                num_training_steps=NUM_EPOCHS * len(train_loader),\n                num_warmup_steps=50)   \n            return {\n                \"optimizer\":optimizer,\n                \"lr_scheduler\":{\n                    'scheduler': scheduler,\n                    'interval': 'step', # or 'epoch'\n                    'frequency': 1\n                }}\n\n        def training_step(self,train_batch,batch_idx):\n            input_ids,attention_mask,target=train_batch\n            pred = self.forward(input_ids, attention_mask)  \n            mse = nn.BCEWithLogitsLoss(reduction=\"mean\")(pred.flatten(), torch.nn.Sigmoid()(target-TARGET_CENTER))\n            return {\"loss\":mse}\n\n        def validation_step(self,val_batch,batch_idx):\n            input_ids,attention_mask,target=val_batch\n            preds = self.forward(input_ids, attention_mask).flatten()\n            targets=target\n            return preds.to(\"cpu\").detach(),targets.to(\"cpu\").detach()\n\n        def validation_epoch_end(self,outputs):\n            preds=[]\n            targets=[]\n            for pred,target in outputs:\n                preds.append(pred)\n                targets.append(target)\n            preds=torch.cat(preds)\n            targets=torch.cat(targets)\n            mse = nn.MSELoss(reduction=\"mean\")(preds+TARGET_CENTER, targets)\n            rmse=torch.sqrt(mse)\n            self.log(\"rmse\",rmse,prog_bar=True)\n            return {\"rmse\":rmse}\n\n        def predict_step(self,val_batch,batch_idx,dataloader_idx=None):\n            input_ids,attention_mask,target=val_batch\n            pred = self.forward(input_ids, attention_mask)                           \n            return pred.flatten()+TARGET_CENTER\n\n    from pytorch_lightning.callbacks import ModelCheckpoint\n    from pytorch_lightning.callbacks import Callback\n    class ChangeValPeriodCallback(Callback):\n        def on_validation_end(self,trainer,mod):\n            rmse=trainer.logged_metrics[\"rmse\"]\n            for val,steps in EVAL_SCHEDULE:\n                if rmse>val:\n                    break\n            trainer.val_check_batch=steps\n\n    # Remove outliners\n    drop_list=['590467bd6','c2013b87b','cd19e2350','0684bb254','d2556a097']\n    is_ok=((train_df.target != 0) | (train_df.standard_error != 0)) & (~train_df[\"id\"].isin(drop_list))\n\n    kf=KFold(NUM_FOLDS,random_state=SEED,shuffle=True)\n    val_scores=[]\n    #is_ok.iloc[100:]=False #for debug\n    for fold,(train_indices,valid_indices) in reversed(list(enumerate(kf.split(train_df)))):\n        if os.path.exists(f\"{OUTPUT}\/model{fold}.ckpt\"):\n            continue\n        print(fold)\n        train_flags=train_df.index.isin(train_indices)\n        valid_flags=train_df.index.isin(valid_indices)\n        train_dataset = LitDataset(train_df[train_flags&is_ok], inference_only=False)\n        valid_dataset=LitDataset(train_df[valid_flags&is_ok],inference_only=False)\n        train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE,\n                                 drop_last=True, shuffle=True, num_workers=0)\n        val_loader = DataLoader(valid_dataset, batch_size=BATCH_SIZE,\n                                 drop_last=False, shuffle=False, num_workers=0)\n        pl.seed_everything(SEED)\n        checkpoint_callback = ModelCheckpoint(monitor='rmse',save_top_k=1,save_weights_only=True,dirpath=f\"{OUTPUT}\",filename=f\"model{fold}\")\n        vpcallback=ChangeValPeriodCallback()\n        trainer=pl.Trainer(\n            precision=16,\n            gpus=NUM_GPUS,\n            deterministic=True,\n            val_check_interval=16,\n            callbacks=[checkpoint_callback,vpcallback],\n            max_epochs=NUM_EPOCHS,\n            min_epochs=NUM_EPOCHS,\n            gradient_clip_val=0.1\n        )\n        print(\"model creation\")\n        model=LitModel()\n        print(\"fit\")\n        trainer.fit(model,train_loader,val_loader)\n        del model\n        vals=trainer.validate(ckpt_path=\"best\",val_dataloaders=val_loader)\n        val_scores.append(vals)\n        pd.DataFrame({\"val_score\":[vals]}).to_csv(f\"{OUTPUT}\/fold_{fold}.csv\",index=False)\n        del trainer,train_loader,val_loader,train_dataset,valid_dataset\n        torch.cuda.empty_cache()\n        # this is also stuck\n        pl.utilities.memory.garbage_collection_cuda()\n        gc.collect()\nrun()\"\"\"\nwith open(\"run.py\",\"w\") as f:\n    f.write(script)","b4f4f7cd":"!__conda_setup=\"$('\/usr\/local\/miniconda\/bin\/conda' 'shell.bash' 'hook' 2> \/dev\/null)\" && eval \"$__conda_setup\" && unset __conda_setup && conda activate base && python run.py","008b7ef3":"## Construct training environment\nWe used google colab pro and used miniconda to train the model. To reproduce our experiments on kaggle kernel, you need to reproduce the environment too.","3f336402":"There are several packages that aren't necessary for training. Please remove them if you want.","f54c3152":"# Training RoBERTa-Large with cross entropy"}}