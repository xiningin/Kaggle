{"cell_type":{"f5130aff":"code","2810d826":"code","8f76dfda":"code","e85dae69":"code","e19b4058":"code","31da1913":"code","7886ebe9":"code","d892a0ae":"code","7f3d6927":"code","74ad8402":"code","1eec382f":"code","3ca26e4c":"code","fd7c97b7":"code","30bdf755":"code","c10dd1aa":"code","b4299771":"code","5118e953":"code","f4876b17":"code","9e326346":"code","06abfe02":"code","1427b992":"code","08275b63":"code","19691259":"code","7bcd9b58":"code","90830c1d":"code","e922d8df":"code","58d90a58":"code","f39e32fe":"code","c2ae94ad":"code","0ea202ef":"code","186edb25":"code","f2df1813":"code","c7738791":"code","3abfd4e1":"code","4160a004":"code","2515164d":"code","cac76094":"code","e0c596c9":"code","66367505":"code","7d242779":"code","ecda5c1f":"code","e8a644ec":"code","fa3902b5":"code","80620cf1":"code","7a836cc6":"code","3a7fffb3":"code","28e634a8":"code","d163c0e4":"code","b7a0bee8":"code","eec8d9d8":"code","e005f015":"code","e50249ab":"code","a36f0c77":"code","728f5816":"code","35e69735":"code","d52d65a1":"code","d4568aa3":"code","12b0cddc":"code","940aab30":"code","ababa75f":"code","2bc8b8f8":"code","c5ae5b92":"code","b7ec5da7":"code","1962f559":"markdown","858bb15f":"markdown","80da36ef":"markdown"},"source":{"f5130aff":"import os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","2810d826":"import pandas as pd\nimport pandas as pd\nimport numpy as np\nfrom sklearn.preprocessing import LabelEncoder\nfrom scipy.stats import chi2_contingency\nimport scipy.stats as ss","8f76dfda":"data = pd.read_csv(\"\/kaggle\/input\/dataset-responses-ai\/Dataset_of_AI_Responses.csv\", encoding='latin1')\ndata = data.iloc[:,1:]\ndata['Google maps  GM'].replace({\"Netural\": \"Neutral\"}, inplace=True)\ndata_cat = data\ndata = data.astype(str)\ndata.head()\n","e85dae69":"print(data.dtypes)","e19b4058":"import category_encoders as ce\n\n# Get a new clean dataframe\nobj_df = data.select_dtypes(include=['object']).copy()\n\n# Specify the columns to encode then fit and transform\nencoder = ce.backward_difference.BackwardDifferenceEncoder()\nencoder.fit(obj_df, verbose=1)\n\n# Only display the first 8 columns for brevity\ndata = encoder.transform(obj_df)\ndata = data.iloc[:, 1:]","31da1913":"# # instantiate labelencoder object\n# le = LabelEncoder()\n# # Categorical boolean mask\n# categorical_feature_mask = data.dtypes==object# filter categorical columns using mask and turn it into a list\n# categorical_cols = data.columns[categorical_feature_mask].tolist()","7886ebe9":"# # apply le on categorical feature columns\n# data[categorical_cols] = data[categorical_cols].apply(lambda col: le.fit_transform(col))\n# data.head(10)","d892a0ae":"# #Correlation plot of the google application variables \n# corr = data.iloc[:,5:10].corr()\n# corr.style.background_gradient(cmap='coolwarm')\n","7f3d6927":"# #Correlation plot of the AI application variables \n# corr = data.iloc[:,10:].corr()\n# corr.style.background_gradient(cmap='coolwarm')","74ad8402":"data.columns","1eec382f":"data_cat.columns","3ca26e4c":"#Frequency table\npd.crosstab(data_cat['A1'],data_cat['Google Smart replies  GSR'])","fd7c97b7":"print(chi2_contingency(pd.crosstab(data_cat['A1'],data_cat['Google Smart replies  GSR'])))","30bdf755":"#Chisuare results sig\/non sig for the nominal variable gender  G1\np_value = []\nx = []\ny = []\nsignificant = []\nfor i in range(0,1,1):\n    for j in range(6,len(data_cat.columns),1):\n        p_value.append(chi2_contingency(pd.crosstab(data_cat.iloc[:,i],data_cat.iloc[:,j]))[1])\n        if chi2_contingency(pd.crosstab(data_cat.iloc[:,i],data_cat.iloc[:,j]))[1]<=0.005:\n            significant.append(\"Significant\")\n        else:\n            significant.append(\"Not Significant\")\n        x.append(data_cat.columns[i])\n        y.append(data_cat.columns[j])\n        \nAI = {'Independent variable': x, 'Response variable': y, 'P_value' : p_value, 'Relationship': significant}\nAI = pd.DataFrame.from_dict(AI)\nAI","c10dd1aa":"#Chisuare results sig\/non sig for the nominal variable age A1\np_value = []\nx = []\ny = []\nsignificant = []\nfor i in range(1,2,1):\n    for j in range(6,len(data_cat.columns),1):\n        p_value.append(chi2_contingency(pd.crosstab(data_cat.iloc[:,i],data_cat.iloc[:,j]))[1])\n        if chi2_contingency(pd.crosstab(data_cat.iloc[:,i],data_cat.iloc[:,j]))[1]<=0.005:\n            significant.append(\"Significant\")\n        else:\n            significant.append(\"Not Significant\")\n        x.append(data_cat.columns[i])\n        y.append(data_cat.columns[j])\n        \nAI = {'Independent variable': x, 'Response variable': y, 'P_value' : p_value, 'Relationship': significant}\nAI = pd.DataFrame.from_dict(AI)\nAI","b4299771":"#Chisuare results sig\/non sig for the nominal variable married  M1\np_value = []\nx = []\ny = []\nsignificant = []\nfor i in range(2,3,1):\n    for j in range(6,len(data_cat.columns),1):\n        p_value.append(chi2_contingency(pd.crosstab(data_cat.iloc[:,i],data_cat.iloc[:,j]))[1])\n        if chi2_contingency(pd.crosstab(data_cat.iloc[:,i],data_cat.iloc[:,j]))[1]<=0.005:\n            significant.append(\"Significant\")\n        else:\n            significant.append(\"Not Significant\")\n        x.append(data_cat.columns[i])\n        y.append(data_cat.columns[j])\n        \nAI = {'Independent variable': x, 'Response variable': y, 'P_value' : p_value, 'Relationship': significant}\nAI = pd.DataFrame.from_dict(AI)\nAI","5118e953":"# Chisuare results sig\/non sig for the nominal variable employment  E1\np_value = []\nx = []\ny = []\nsignificant = []\nfor i in range(3,4,1):\n    for j in range(6,len(data_cat.columns),1):\n        p_value.append(chi2_contingency(pd.crosstab(data_cat.iloc[:,i],data_cat.iloc[:,j]))[1])\n        if chi2_contingency(pd.crosstab(data_cat.iloc[:,i],data_cat.iloc[:,j]))[1]<=0.005:\n            significant.append(\"Significant\")\n        else:\n            significant.append(\"Not Significant\")\n        x.append(data_cat.columns[i])\n        y.append(data_cat.columns[j])\n        \nAI = {'Independent variable': x, 'Response variable': y, 'P_value' : p_value, 'Relationship': significant}\nAI = pd.DataFrame.from_dict(AI)\nAI","f4876b17":"#Chisuare results sig\/non sig for the nominal variable degree  Q1\np_value = []\nx = []\ny = []\nsignificant = []\nfor i in range(4,5,1):\n    for j in range(6,len(data_cat.columns),1):\n        p_value.append(chi2_contingency(pd.crosstab(data_cat.iloc[:,i],data_cat.iloc[:,j]))[1])\n        if chi2_contingency(pd.crosstab(data_cat.iloc[:,i],data_cat.iloc[:,j]))[1]<=0.005:\n            significant.append(\"Significant\")\n        else:\n            significant.append(\"Not Significant\")\n        x.append(data_cat.columns[i])\n        y.append(data_cat.columns[j])\n        \nAI = {'Independent variable': x, 'Response variable': y, 'P_value' : p_value, 'Relationship': significant}\nAI = pd.DataFrame.from_dict(AI)\nAI","9e326346":"#Relationship among the google application variables \np_value = []\nx = []\ny = []\nsignificant = []\nfor i in range(10,len(data_cat.columns)-1,1):\n    for j in range(i+1,len(data_cat.columns),1):\n        p_value.append(chi2_contingency(pd.crosstab(data_cat.iloc[:,i],data_cat.iloc[:,j]))[1])\n        if chi2_contingency(pd.crosstab(data_cat.iloc[:,i],data_cat.iloc[:,j]))[1]<=0.005:\n            significant.append(\"significant\")\n        else:\n            significant.append(\"not significant\")\n        x.append(data_cat.columns[i])\n        y.append(data_cat.columns[j])\n        \nAI = {'attr1': x, 'attr2': y, 'P_value' : p_value, 'Relationship': significant}\nAI = pd.DataFrame.from_dict(AI)\nAI","06abfe02":"p_value = []\nx = []\ny = []\nsignificant = []\nfor i in range(0,len(data_cat.columns)-1,1):\n    for j in range(i+1,len(data_cat.columns),1):\n        p_value.append(chi2_contingency(pd.crosstab(data_cat.iloc[:,i],data_cat.iloc[:,j]))[1])\n        if chi2_contingency(pd.crosstab(data_cat.iloc[:,i],data_cat.iloc[:,j]))[1]<=0.005:\n            significant.append(\"significant\")\n        else:\n            significant.append(\"not significant\")\n        x.append(data_cat.columns[i])\n        y.append(data_cat.columns[j])\n        \nAI = {'attr1': x, 'attr2': y, 'P_value' : p_value, 'Relationship': significant}\nAI = pd.DataFrame.from_dict(AI)\nAI[AI['Relationship']==\"significant\"][:30]","1427b992":"#Relationship among AI application variables  \np_value = []\nx = []\ny = []\nsignificant = []\nfor i in range(5,9,1):\n    for j in range(i+1,10,1):\n        p_value.append(chi2_contingency(pd.crosstab(data_cat.iloc[:,i],data_cat.iloc[:,j]))[1])\n        if chi2_contingency(pd.crosstab(data_cat.iloc[:,i],data_cat.iloc[:,j]))[1]<=0.005:\n            significant.append(\"significant\")\n        else:\n            significant.append(\"not significant\")\n        x.append(data_cat.columns[i])\n        y.append(data_cat.columns[j])\n        \ngoogle = {'attr1': x, 'attr2': y, 'P_value' : p_value, 'Relationship': significant}\ngoogle = pd.DataFrame.from_dict(google)\ngoogle","08275b63":"p_value = []\nx = []\ny = []\nsignificant = []\nfor i in range(0,9,1):\n    for j in range(i+1,10,1):\n        p_value.append(chi2_contingency(pd.crosstab(data_cat.iloc[:,i],data_cat.iloc[:,j]))[1])\n        if chi2_contingency(pd.crosstab(data_cat.iloc[:,i],data_cat.iloc[:,j]))[1]<=0.005:\n            significant.append(\"significant\")\n        else:\n            significant.append(\"not significant\")\n        x.append(data_cat.columns[i])\n        y.append(data_cat.columns[j])\n        \ngoogle = {'attr1': x, 'attr2': y, 'P_value' : p_value, 'Relationship': significant}\ngoogle = pd.DataFrame.from_dict(google)\ngoogle[google['Relationship']==\"significant\"]","19691259":"import matplotlib.pyplot as plt\n\nfrom sklearn.cluster import AgglomerativeClustering \nfrom sklearn.preprocessing import StandardScaler, normalize\nfrom sklearn.decomposition import PCA\nfrom sklearn.metrics import silhouette_score\nimport scipy.cluster.hierarchy as shc","7bcd9b58":"data.columns","90830c1d":"normalized_df = data.iloc[:,21:44]\nnormalized_df = pd.DataFrame(normalized_df) ","e922d8df":"# Standardize data\nscaler = StandardScaler() \nscaled_df = scaler.fit_transform(data.iloc[:,20:44]) \n  \n# Normalizing the Data \nnormalized_df = normalize(scaled_df) \n  \n# Converting the numpy array into a pandas DataFrame \nnormalized_df = pd.DataFrame(normalized_df) ","58d90a58":"pca = PCA(n_components = 2) \nX_principal = pca.fit_transform(normalized_df) \nX_principal = pd.DataFrame(X_principal) \nX_principal.columns = ['P1', 'P2'] \n  \nX_principal.head(2)","f39e32fe":"plt.figure(figsize =(6, 6)) \nplt.title('Visualising the data') \nDendrogram = shc.dendrogram((shc.linkage(X_principal, method ='ward'))) \n\n","c2ae94ad":"silhouette_scores = [] \n\nfor n_cluster in range(2, 8):\n    silhouette_scores.append( \n        silhouette_score(X_principal, AgglomerativeClustering(n_clusters = n_cluster).fit_predict(X_principal))) \n    \n# Plotting a bar graph to compare the results \nk = [2, 3, 4, 5, 6,7] \nplt.bar(k, silhouette_scores) \nplt.xlabel('Number of clusters', fontsize = 10) \nplt.ylabel('Silhouette Score', fontsize = 10) \nplt.show() \n\n","0ea202ef":"hc = AgglomerativeClustering(n_clusters=3,affinity = 'euclidean',linkage = 'ward')\ny_hc = hc.fit_predict(X_principal)","186edb25":"y_hc","f2df1813":"# Visualizing the clustering \nplt.figure(figsize = (10,10))\nplt.scatter(X_principal['P1'], X_principal['P2'],  \n           c = AgglomerativeClustering(n_clusters = 3).fit_predict(X_principal), cmap =plt.cm.winter) \nplt.show() ","c7738791":"new_data = data_cat.iloc[:,:10]\nnew_data['cluster'] = y_hc","3abfd4e1":"new_data.columns","4160a004":"print(new_data[new_data['cluster']==2]['Google maps  GM'].value_counts())\nprint(new_data[new_data['cluster']==2]['Google Smart replies  GSR'].value_counts())\nprint(new_data[new_data['cluster']==2]['Google search query  GSQ '].value_counts())\nprint(new_data[new_data['cluster']==2]['Google page  rank   GPR           '].value_counts())\nprint(new_data[new_data['cluster']==2]['Google Email filter   GEF  '].value_counts())","2515164d":"print(new_data[new_data['cluster']==2]['G1'].value_counts())\nprint(new_data[new_data['cluster']==2]['A1'].value_counts())\nprint(new_data[new_data['cluster']==2]['M1'].value_counts())\nprint(new_data[new_data['cluster']==2]['E1'].value_counts())\nprint(new_data[new_data['cluster']==2]['Q1'].value_counts())","cac76094":"print(new_data[new_data['cluster']==1]['Google maps  GM'].value_counts())\nprint(new_data[new_data['cluster']==1]['Google Smart replies  GSR'].value_counts())\nprint(new_data[new_data['cluster']==1]['Google search query  GSQ '].value_counts())\nprint(new_data[new_data['cluster']==1]['Google page  rank   GPR           '].value_counts())\nprint(new_data[new_data['cluster']==1]['Google Email filter   GEF  '].value_counts())","e0c596c9":"print(new_data[new_data['cluster']==1]['G1'].value_counts())\nprint(new_data[new_data['cluster']==1]['A1'].value_counts())\nprint(new_data[new_data['cluster']==1]['M1'].value_counts())\nprint(new_data[new_data['cluster']==1]['E1'].value_counts())\nprint(new_data[new_data['cluster']==1]['Q1'].value_counts())","66367505":"print(new_data[new_data['cluster']==0]['Google maps  GM'].value_counts())\nprint(new_data[new_data['cluster']==0]['Google Smart replies  GSR'].value_counts())\nprint(new_data[new_data['cluster']==0]['Google search query  GSQ '].value_counts())\nprint(new_data[new_data['cluster']==0]['Google page  rank   GPR           '].value_counts())\nprint(new_data[new_data['cluster']==0]['Google Email filter   GEF  '].value_counts())","7d242779":"print(new_data[new_data['cluster']==0]['G1'].value_counts())\nprint(new_data[new_data['cluster']==0]['A1'].value_counts())\nprint(new_data[new_data['cluster']==0]['M1'].value_counts())\nprint(new_data[new_data['cluster']==0]['E1'].value_counts())\nprint(new_data[new_data['cluster']==0]['Q1'].value_counts())","ecda5c1f":"# Standardize data\nscaler = StandardScaler() \nscaled_df = scaler.fit_transform(data.iloc[:,45:]) \n  \n# Normalizing the Data \nnormalized_df = normalize(scaled_df) \n  \n# Converting the numpy array into a pandas DataFrame \nnormalized_df = pd.DataFrame(normalized_df) ","e8a644ec":"pca = PCA(n_components = 2) \nX_principal = pca.fit_transform(normalized_df) \nX_principal = pd.DataFrame(X_principal) \nX_principal.columns = ['P1', 'P2'] \n  \nX_principal.head(2)","fa3902b5":"plt.figure(figsize =(6, 6)) \nplt.title('Visualising the data') \nDendrogram = shc.dendrogram((shc.linkage(X_principal, method ='ward'))) ","80620cf1":"silhouette_scores = [] \n\nfor n_cluster in range(2, 8):\n    silhouette_scores.append( \n        silhouette_score(X_principal, AgglomerativeClustering(n_clusters = n_cluster).fit_predict(X_principal))) \n    \n# Plotting a bar graph to compare the results \nk = [2, 3, 4, 5, 6,7] \nplt.bar(k, silhouette_scores) \nplt.xlabel('Number of clusters', fontsize = 10) \nplt.ylabel('Silhouette Score', fontsize = 10) \nplt.show() \n\n\n","7a836cc6":"hc = AgglomerativeClustering(n_clusters=2,affinity = 'euclidean',linkage = 'ward')\ny_hc = hc.fit_predict(X_principal)","3a7fffb3":"# Visualizing the clustering \nplt.figure(figsize = (10,10))\nplt.scatter(X_principal['P1'], X_principal['P2'],  \n           c = AgglomerativeClustering(n_clusters = 2).fit_predict(X_principal), cmap =plt.cm.winter) \nplt.show() ","28e634a8":"new_data = data_cat\nnew_data['cluster'] = y_hc","d163c0e4":"new_data.columns","b7a0bee8":"print(new_data[new_data['cluster']==0]['G1'].value_counts())\nprint(new_data[new_data['cluster']==0]['A1'].value_counts())\nprint(new_data[new_data['cluster']==0]['M1'].value_counts())\nprint(new_data[new_data['cluster']==0]['E1'].value_counts())\nprint(new_data[new_data['cluster']==0]['Q1'].value_counts())","eec8d9d8":"print(new_data[new_data['cluster']==0].iloc[:,10].value_counts())\nprint(new_data[new_data['cluster']==0].iloc[:,11].value_counts())\nprint(new_data[new_data['cluster']==0].iloc[:,12].value_counts())\nprint(new_data[new_data['cluster']==0].iloc[:,13].value_counts())\nprint(new_data[new_data['cluster']==0].iloc[:,14].value_counts())\nprint(new_data[new_data['cluster']==0].iloc[:,15].value_counts())","e005f015":"print(new_data[new_data['cluster']==1]['G1'].value_counts())\nprint(new_data[new_data['cluster']==1]['A1'].value_counts())\nprint(new_data[new_data['cluster']==1]['M1'].value_counts())\nprint(new_data[new_data['cluster']==1]['E1'].value_counts())\nprint(new_data[new_data['cluster']==1]['Q1'].value_counts())","e50249ab":"print(new_data[new_data['cluster']==1].iloc[:,10].value_counts())\nprint(new_data[new_data['cluster']==1].iloc[:,11].value_counts())\nprint(new_data[new_data['cluster']==1].iloc[:,12].value_counts())\nprint(new_data[new_data['cluster']==1].iloc[:,13].value_counts())\nprint(new_data[new_data['cluster']==1].iloc[:,14].value_counts())\nprint(new_data[new_data['cluster']==1].iloc[:,15].value_counts())","a36f0c77":"# Standardize data\nscaler = StandardScaler() \nscaled_df = scaler.fit_transform(data.iloc[:,:20]) \n  \n# Normalizing the Data \nnormalized_df = normalize(scaled_df) \n  \n# Converting the numpy array into a pandas DataFrame \nnormalized_df = pd.DataFrame(normalized_df) ","728f5816":"pca = PCA(n_components = 2) \nX_principal = pca.fit_transform(normalized_df) \nX_principal = pd.DataFrame(X_principal) \nX_principal.columns = ['P1', 'P2'] \n  \nX_principal.head(2)","35e69735":"plt.figure(figsize =(6, 6)) \nplt.title('Visualising the data') \nDendrogram = shc.dendrogram((shc.linkage(X_principal, method ='ward'))) ","d52d65a1":"silhouette_scores = [] \n\nfor n_cluster in range(2, 8):\n    silhouette_scores.append( \n        silhouette_score(X_principal, AgglomerativeClustering(n_clusters = n_cluster).fit_predict(X_principal))) \n    \n# Plotting a bar graph to compare the results \nk = [2, 3, 4, 5, 6,7] \nplt.bar(k, silhouette_scores) \nplt.xlabel('Number of clusters', fontsize = 10) \nplt.ylabel('Silhouette Score', fontsize = 10) \nplt.show() \n","d4568aa3":"hc = AgglomerativeClustering(n_clusters=3,affinity = 'euclidean',linkage = 'ward')\ny_hc = hc.fit_predict(X_principal)","12b0cddc":"# Visualizing the clustering \nplt.figure(figsize = (10,10))\nplt.scatter(X_principal['P1'], X_principal['P2'],  \n           c = AgglomerativeClustering(n_clusters = 3).fit_predict(X_principal), cmap =plt.cm.winter) \nplt.show() ","940aab30":"new_data = data_cat.iloc[:,:5]\nnew_data['cluster'] = y_hc","ababa75f":"new_data","2bc8b8f8":"print(new_data[new_data['cluster']==0].iloc[:,0].value_counts())\nprint(new_data[new_data['cluster']==0].iloc[:,1].value_counts())\nprint(new_data[new_data['cluster']==0].iloc[:,2].value_counts())\nprint(new_data[new_data['cluster']==0].iloc[:,3].value_counts())\nprint(new_data[new_data['cluster']==0].iloc[:,4].value_counts())\n","c5ae5b92":"print(new_data[new_data['cluster']==1].iloc[:,0].value_counts())\nprint(new_data[new_data['cluster']==1].iloc[:,1].value_counts())\nprint(new_data[new_data['cluster']==1].iloc[:,2].value_counts())\nprint(new_data[new_data['cluster']==1].iloc[:,3].value_counts())\nprint(new_data[new_data['cluster']==1].iloc[:,4].value_counts())\n","b7ec5da7":"print(new_data[new_data['cluster']==2].iloc[:,0].value_counts())\nprint(new_data[new_data['cluster']==2].iloc[:,1].value_counts())\nprint(new_data[new_data['cluster']==2].iloc[:,2].value_counts())\nprint(new_data[new_data['cluster']==2].iloc[:,3].value_counts())\nprint(new_data[new_data['cluster']==2].iloc[:,4].value_counts())","1962f559":"# Cluster customers on the basis of demographics ","858bb15f":"# Cluster customers on the basis of attitudes towards AI ","80da36ef":"# Cluster customers on the basis of attitudes towards Google products"}}