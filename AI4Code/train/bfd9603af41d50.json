{"cell_type":{"13f397e7":"code","fd4cf323":"code","3c73e99d":"code","1b815c51":"code","894f87c3":"code","5b254256":"code","e521b157":"code","c47b2148":"code","ad7159a1":"code","a81f242f":"code","83d40848":"code","56f3c844":"code","f2f85ea5":"code","4d6d9964":"code","268295a3":"code","a1cad76e":"code","c8a0d599":"code","89dc7720":"code","4ef93561":"code","f545f448":"code","1266b863":"code","feecdc9a":"code","243e7ef1":"code","ef8862d0":"code","a6e9d541":"code","aa572fdb":"code","5731d3d9":"code","326f668e":"code","1675bd35":"code","efad02e4":"code","7455761c":"code","5d0ad582":"code","b81e0430":"code","40e003c0":"code","0b569d90":"code","b2cdcda2":"code","1ae4dbeb":"code","3388aeff":"code","06a68781":"code","58c63627":"markdown","94f140db":"markdown","cdc3bbcc":"markdown","6cc05232":"markdown","5d76215b":"markdown","e07f220c":"markdown","f9cd909f":"markdown"},"source":{"13f397e7":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","fd4cf323":"import matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline","3c73e99d":"df_train = pd.read_csv('\/kaggle\/input\/hdfc-2019\/DataSet\/Train.csv')\ndf_test = pd.read_csv('\/kaggle\/input\/hdfc-2019\/DataSet\/Test.csv')","1b815c51":"df_train.shape, df_test.shape","894f87c3":"df_train.head()","5b254256":"df_test.head()","e521b157":"X = df_train.drop(['Col1','Col2'], axis=1)\ny = df_train.Col2\n\nXTest = df_test.drop(['Col1'], axis=1)","c47b2148":"y.value_counts(normalize=True) * 100","ad7159a1":"X.info()","a81f242f":"XTest.info()","83d40848":"for col in X.select_dtypes('object').columns:\n    print(col)\n    print(X[col].unique())\n    print(\"--------------------------------------------------------------------\\n\")","56f3c844":"for col in XTest.select_dtypes('object').columns:\n    print(col)\n    print(XTest[col].unique())\n    print(\"--------------------------------------------------------------------\\n\")","f2f85ea5":"def convert_to_float(row):\n    if row == '-':\n        return np.nan\n    else:\n        return float(row)","4d6d9964":"columns_need_treatment = list(X.select_dtypes('object').columns) + list(XTest.select_dtypes('object').columns)\nprint(len(columns_need_treatment))\nprint(columns_need_treatment)","268295a3":"for col in columns_need_treatment:\n    X[col] = X[col].apply(convert_to_float)\n    XTest[col] = XTest[col].apply(convert_to_float)","a1cad76e":"duplicate_rows_in_train = X.duplicated()\nduplicate_rows_in_test = XTest.duplicated()\n\nprint(\"Train data contains %d duplicate rows and Test data contains %d duplicate rows.\"%(sum(duplicate_rows_in_train), \n                                                                                             sum(duplicate_rows_in_test)))","c8a0d599":"y[duplicate_rows_in_train].value_counts(normalize=True)","89dc7720":"y[duplicate_rows_in_train].head(30)","4ef93561":"X['duplicate_row'] = False\nXTest['duplicate_row'] = False\n\nX.loc[duplicate_rows_in_train, 'duplicate_row'] = True\nXTest.loc[duplicate_rows_in_test, 'duplicate_row'] = True","f545f448":"features = X.columns\nduplicate_columns = set()\nfor i in range(len(features)):\n    for j in range(i+1, len(features)):\n        if np.all(X[features[i]] == X[features[j]]):\n            print(features[i], features[j])\n            duplicate_columns.add(features[j])","1266b863":"print(\"Number of duplicate columns:\",len(duplicate_columns))","feecdc9a":"selected_features = [_ for _ in X.columns if _ not in duplicate_columns]\nprint(\"Selected_Features :\",len(selected_features))","243e7ef1":"RANDOM_SEED = 1","ef8862d0":"from sklearn.model_selection import GridSearchCV, train_test_split\nfrom sklearn.metrics import f1_score\nimport xgboost as xgb\nimport lightgbm as lgb\nfrom hyperopt import STATUS_OK, Trials, fmin, hp, tpe\nfrom sklearn.model_selection import StratifiedKFold","a6e9d541":"X_train, X_valid, y_train, y_valid = train_test_split(X[selected_features], y, \n                                                      random_state=RANDOM_SEED, \n                                                      test_size=0.2)","aa572fdb":"def score(params):\n    try:\n\n        print(\"Training with params: \",params)\n        num_round = int(params['n_estimators'])\n        del params['n_estimators']\n        dtrain = xgb.DMatrix(X_train, label=y_train)\n        dvalid = xgb.DMatrix(X_valid, label=y_valid)\n        watchlist = [(dtrain, 'train'),(dvalid, 'eval')]\n        gbm_model = xgb.train(params, dtrain, num_round,\n                              evals=watchlist,\n                              verbose_eval=False)\n        predictions = gbm_model.predict(dvalid,\n                                        ntree_limit=gbm_model.best_iteration + 1)\n        predictions = (predictions >= 0.5).astype('int')\n        score = f1_score(y_valid, predictions, average='weighted')\n        print(\"\\tScore {0}\\n\\n\".format(score))\n        \n        # The score function should return the loss (1-score)\n        # since the optimize function looks for the minimum\n        loss = 1 - score\n        return {'loss': loss, 'status': STATUS_OK}\n   \n    # In case of any exception or assertionerror making score 0, so that It can return maximum loss (ie 1)\n    except AssertionError as obj:\n        #print(\"AssertionError: \",obj)\n        loss = 1 - 0\n        return {'loss': loss, 'status': STATUS_OK}\n\n    except Exception as obj:\n        #print(\"Exception: \",obj)\n        loss = 1 - 0\n        return {'loss': loss, 'status': STATUS_OK}\n","5731d3d9":"def optimize(\n             trials, \n             max_evals, \n             random_state=RANDOM_SEED):\n\n\n    \"\"\"\n    This is the optimization function that given a space (space here) of \n    hyperparameters and a scoring function (score here), finds the best hyperparameters.\n    \"\"\"\n    # To learn more about XGBoost parameters, head to this page: \n    # https:\/\/github.com\/dmlc\/xgboost\/blob\/master\/doc\/parameter.md\n    space = {\n        'n_estimators': hp.quniform('n_estimators', 100, 300, 1),\n        'eta': hp.quniform('eta', 0.025, 0.5, 0.025),\n        # A problem with max_depth casted to float instead of int with\n        # the hp.quniform method.\n        'max_depth':  hp.choice('max_depth', np.arange(1, 7, dtype=int)),\n        'min_child_weight': hp.quniform('min_child_weight', 1, 6, 1),\n        'subsample': hp.quniform('subsample', 0.5, 1, 0.05),\n        'gamma': hp.quniform('gamma', 0, 1, 0.05),\n        'colsample_bytree': hp.quniform('colsample_bytree', 0.5, 1, 0.05),\n        'scale_pos_weight': hp.quniform('scale_pos_weight', 1,4, 0.05),\n        \"reg_alpha\": hp.quniform('reg_alpha', 0, 1, 0.05),\n        \"reg_lambda\": hp.quniform('reg_lambda', 1, 5, 0.05),\n        'eval_metric': 'logloss',\n        'objective': 'binary:logistic',\n        # Increase this number if you have more cores. Otherwise, remove it and it will default \n        # to the maxium number. \n        'nthread': 4,\n        'booster': 'gbtree',\n        'tree_method': 'exact',\n        'silent': 1,\n        'seed': random_state\n    }\n    # Use the fmin function from Hyperopt to find the best hyperparameters\n    best = fmin(score, \n                space, \n                algo=tpe.suggest, \n                trials=trials, \n                max_evals=max_evals)\n    return best\n","326f668e":"trials = Trials()\nMAX_EVALS = 25\n\nbest_hyperparams = optimize(trials, MAX_EVALS)\nprint(\"The best hyperparameters are: \", \"\\n\")\nprint(best_hyperparams)","1675bd35":"best_hyperparams","efad02e4":"param = best_hyperparams\nnum_round = int(param['n_estimators'])\ndel param['n_estimators']","7455761c":"num_splits = 5\nskf = StratifiedKFold(n_splits= num_splits, random_state= RANDOM_SEED, shuffle=True)","5d0ad582":"dxtest = xgb.DMatrix(XTest[selected_features])","b81e0430":"y_test_pred = np.zeros((XTest[selected_features].shape[0], 1))\nprint(y_test_pred.shape)\ny_valid_scores = []\n\nX_TRAIN = X[selected_features].copy()\nY_TRAIN = y.copy()\nX_TRAIN = X_TRAIN.reindex()\nY_TRAIN = Y_TRAIN.reindex()\n\nfold_cnt = 1\nfor train_index, test_index in skf.split(X_TRAIN,Y_TRAIN):\n    print(\"FOLD .... \",fold_cnt)\n    fold_cnt += 1\n    \n    print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n    X_train, X_valid = X_TRAIN.iloc[train_index], X_TRAIN.iloc[test_index]\n    y_train, y_valid = Y_TRAIN.iloc[train_index], Y_TRAIN.iloc[test_index]\n    \n    dtrain = xgb.DMatrix(X_train, label=y_train)\n    dvalid = xgb.DMatrix(X_valid, label=y_valid)\n    \n    evallist = [(dtrain, 'train'), (dvalid, 'eval')]\n\n    # Training xgb model\n    bst = xgb.train(param, dtrain, num_round, evallist, verbose_eval=50)\n    \n    # Predict Validation\n    y_pred_valid = bst.predict(dvalid, ntree_limit=bst.best_iteration + 1)\n    y_valid_scores.append(f1_score(y_valid, (y_pred_valid >= 0.5).astype(int), average='weighted'))\n   \n    # Predict Test \n    y_pred = bst.predict(dxtest, ntree_limit=bst.best_iteration+1)\n    \n    y_test_pred += y_pred.reshape(-1,1)\n\n#Normalize test predicted probability\ny_test_pred \/= num_splits","40e003c0":"y_valid_scores","0b569d90":"print(\"Average validation_score: \",np.mean(y_valid_scores))","b2cdcda2":"output = df_test[['Col1']].copy()\noutput['Col2'] = (y_test_pred >= 0.5).astype(int)","1ae4dbeb":"output.head()","3388aeff":"output['Col2'].value_counts()\/output.shape[0] * 100","06a68781":"output.to_csv(\".\/predict_hdfc_xgb_oof.csv\", index=False)","58c63627":"# OOF (Out of Fold Prediciton): \n","94f140db":"# Col1 has an unique data & Col2 is Target variable","cdc3bbcc":"# More information about train and test dataframe","6cc05232":"# Something is fishy:\n> **X** has 2 object type features but **XTest** has 11 object type features\n\n> Ideally X and XTest dataframe should contain features with same datatype.","5d76215b":"# Clean the data:\n\n> * We will convert string numerical data to float.\n\n> * We will convert '-' sign to nan\n\n","e07f220c":"# Does data contains duplicate rows or features ?\n","f9cd909f":"# Best Hyper-parameters"}}