{"cell_type":{"f2c427eb":"code","1db1a2b0":"code","95fe2016":"code","c6d43429":"code","dd974096":"code","2ebc8e2b":"code","f23ab12f":"code","515e2fb1":"code","5a19e6a3":"code","bc30b9fe":"code","5a072407":"code","d30cc4bb":"code","703835ea":"code","9d0270da":"code","75e48077":"markdown","1d1cc413":"markdown","a4afa05b":"markdown","2ce2427c":"markdown","a27f4cc1":"markdown","5468ef33":"markdown","b6926d10":"markdown","a9500682":"markdown","116079df":"markdown","026b8a17":"markdown","3a0aff6a":"markdown","35734a57":"markdown","f7065ef4":"markdown","ca57e164":"markdown","b29cb121":"markdown","b76f61b9":"markdown","012ec7c4":"markdown","5ab70ed7":"markdown","77a16740":"markdown","4df667b8":"markdown","8e3a1b7b":"markdown","cd3ff8f2":"markdown","8c741a02":"markdown","fe37f680":"markdown","30cb19c2":"markdown"},"source":{"f2c427eb":"!wget https:\/\/s3-api.us-geo.objectstorage.softlayer.net\/cf-courses-data\/CognitiveClass\/DL0321EN\/data\/concrete_data_week4.zip","1db1a2b0":"!unzip concrete_data_week4.zip","95fe2016":"from tensorflow.keras.applications import VGG16, ResNet50\nfrom tensorflow.keras.applications.vgg16 import preprocess_input\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom tensorflow.keras import Sequential\nfrom tensorflow.keras.layers import Dense, Flatten\nfrom tensorflow.keras import Input\nimport numpy as np\nfrom kaggle_secrets import UserSecretsClient\nfrom kaggle_datasets import KaggleDatasets\nimport matplotlib.pyplot as plt\nimport os\nimport tensorflow as tf\nfrom tensorflow.keras import mixed_precision","c6d43429":"try:\n    tpu_resolver = tf.distribute.cluster_resolver.TPUClusterResolver() # TPU detection\n    mixed_precision.set_global_policy('mixed_bfloat16')\n    os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n    os.environ['TF_XLA_FLAGS'] = '--tf_xla_enable_xla_devices'\n    os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\nexcept ValueError:\n    tpu_resolver = None\n    gpus = tf.config.experimental.list_logical_devices(\"GPU\") # GPU detection\n    mixed_precision.set_global_policy('mixed_float16')\n    os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n    os.environ['TF_XLA_FLAGS'] = '--tf_xla_enable_xla_devices'\n\n# Select appropriate distribution strategy\nif tpu_resolver:\n    tf.config.experimental_connect_to_cluster(tpu_resolver)\n    tf.tpu.experimental.initialize_tpu_system(tpu_resolver)\n    strategy = tf.distribute.TPUStrategy(tpu_resolver)\n    print('Running on TPU ', tpu_resolver.cluster_spec().as_dict()['worker'])\nelif len(gpus) > 1:\n    strategy = tf.distribute.MirroredStrategy([gpu.name for gpu in gpus])\n    print('Running on multiple GPUs ', [gpu.name for gpu in gpus])\nelif len(gpus) == 1:\n    strategy = tf.distribute.get_strategy() # default strategy that works on CPU and single GPU\n    print('Running on single GPU ', gpus[0].name)\nelse:\n    strategy = tf.distribute.get_strategy() # default strategy that works on CPU and single GPU\n    print('Running on CPU')\n    os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n    os.environ['TF_XLA_FLAGS'] = '--tf_xla_enable_xla_devices'\n    print(\"Number of accelerators: \", strategy.num_replicas_in_sync)","dd974096":"# # Step 1: Get the credential from the Cloud SDK\n# user_secrets = UserSecretsClient()\n# user_credential = user_secrets.get_gcloud_credential()\n\n# # Step 2: Set the credentials\n# user_secrets.set_tensorflow_credential(user_credential)\n\n# # Step 3: Use a familiar call to get the GCS path of the dataset\n# GCS_DS_PATH = KaggleDatasets().get_gcs_path('jigsaw-multilingual-toxic-comment-classification')\ntrain_dataset_dir = '\/kaggle\/working\/concrete_data_week4\/train'\nvalid_dataset_dir = '\/kaggle\/working\/concrete_data_week4\/valid'\ntest_dataset_dir = '\/kaggle\/working\/concrete_data_week4\/test'\n","2ebc8e2b":"batch_size = 100\nwidth, length, channels = 224, 224, 3\nnum_classes = 2","f23ab12f":"data_generator = ImageDataGenerator(preprocessing_function=preprocess_input)\n\ntrain_generator = tf.keras.preprocessing.image_dataset_from_directory(\n    train_dataset_dir,\n    batch_size=batch_size,\n    validation_split = 0,\n    label_mode='categorical',\n    image_size=(width,length)\n    )\nvalid_generator = tf.keras.preprocessing.image_dataset_from_directory(\n    valid_dataset_dir,\n    batch_size=batch_size,\n    validation_split = 0,\n    label_mode='categorical',\n    image_size=(width,length)\n    )\ntest_generator = tf.keras.preprocessing.image_dataset_from_directory(\n    test_dataset_dir,\n    batch_size=batch_size,\n    validation_split = 0,\n    label_mode='categorical',\n    image_size=(width,length),\n    shuffle=False)","515e2fb1":"def create_model1():\n    with strategy.scope():\n        model = Sequential()\n        for layer in VGG16(input_shape=(224,224,3)).layers[:-1]:\n            model.add(layer)\n        for layer in model.layers:\n            layer.trainable = False\n        model.add(Dense(num_classes,activation='softmax'))\n        model.compile(loss='categorical_crossentropy', optimizer='adam')\n        return model\n\n\nvggmodel = create_model1()\n\nif tpu_resolver:\n    checkpoint = tf.train.Checkpoint(model=vggmodel)\n    local_device_option = tf.train.CheckpointOptions(experimental_io_device=\"\/job:localhost\")\n    checkpoint.write(\"\/tmp\/ckpt\", options=local_device_option)\n\nvggmodel.summary()","5a19e6a3":"vgg_history = vggmodel.fit(train_generator,epochs=50,validation_data=valid_generator,verbose=1)","bc30b9fe":"vggscores = vggmodel.evaluate(test_generator)\nvggscores","5a072407":"def create_model2():\n    with strategy.scope(): \n        model = Sequential()\n        model.add(ResNet50(weights='imagenet',include_top=False,input_shape=(224,224,3),classes=num_classes))\n        model.add(Flatten())\n        model.add(Dense(num_classes,activation='softmax'))\n        model.layers[0].trainable = False\n        model.compile(loss='categorical_crossentropy', optimizer='adam')\n        return model\n\n\nresmodel = create_model2()\n\nif tpu_resolver:\n    checkpoint = tf.train.Checkpoint(model=resmodel)\n    local_device_option = tf.train.CheckpointOptions(experimental_io_device=\"\/job:localhost\")\n    checkpoint.write(\"\/tmp\/ckpt\", options=local_device_option)\n\nresmodel.summary()","d30cc4bb":"res_history = resmodel.fit(train_generator,epochs=50,validation_data=valid_generator)","703835ea":"resscores = resmodel.evaluate(test_generator,verbose=0)\nresscores","9d0270da":"test_generator = data_generator.flow_from_directory(\n    test_dataset_dir,\n    batch_size=1,\n    class_mode='categorical',\n    target_size=(width,length),\n    shuffle=False);\nindex = -1\nfor x, y in test_generator:\n    index +=1\n    if index >= 10:\n        break\n    pred = vggmodel.predict_on_batch(x)\n    class_idx = np.argmax(y, axis=-1)\n    prediction_idx = np.argmax(pred, axis=-1)\n    if class_idx == prediction_idx:\n        print('Positive')\n    else:\n        print('Negative')\n        plt.imshow(x.astype(\"uint8\").reshape(width, length, channels))\n        plt.show();","75e48077":"## Data Acquisition and Preparation","1d1cc413":"**Reporting the class predictions of the first ten images in the test set.**","a4afa05b":"initializing the accelerator","2ce2427c":"Setting variables","a27f4cc1":"Building a classifier using the VGG16 pre-trained model.","5468ef33":"<a id=\"item42\"><\/a>","b6926d10":"Use the <code>wget<\/code> command to download the data","a9500682":"Creating data generator for the models","116079df":"## Table of Contents\n\n<div class=\"alert alert-block alert-info\" style=\"margin-top: 20px\">\n\n<font size = 3>    \n\n1. <a href=\"#item41\">Data Acquisition and Preparation<\/a>\n2. <a href=\"#item42\">VGG16<\/a>\n3. <a href=\"#item43\">ResNet50<\/a>  \n4. <a href=\"#item44\">Prediction Results<\/a>  \n\n<\/font>\n    \n<\/div>","026b8a17":"<a id=\"item41\"><\/a>","3a0aff6a":"## Introduction\n","35734a57":"Fitting ResNet50 model","f7065ef4":"Fitting VGG16 model","ca57e164":"<a id=\"item44\"><\/a>","b29cb121":"<a id=\"item43\"><\/a>","b76f61b9":"Building a classifier using the ResNet50 pre-trained model.","012ec7c4":"Printing the performance of the classifier using the VGG16 pre-trained model.","5ab70ed7":"Declaring dataset folders","77a16740":"Printing the performance of the classifier using the ResNet50 pre-trained model.","4df667b8":"In this lab, you will build an image classifier using the VGG16 pre-trained model, and you will evaluate it and compare its performance to the model we built in the last module using the ResNet50 pre-trained model. Good luck!","8e3a1b7b":"Importing libraries","cd3ff8f2":"## VGG16","8c741a02":"## Prediction Results","fe37f680":"## ResNet50","30cb19c2":"After you unzip the data, you fill find the data has already been divided into a train, validation, and test sets."}}