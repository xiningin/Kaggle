{"cell_type":{"2b39fd49":"code","23bdcb12":"code","b7e3c681":"code","a80c6853":"code","f549315d":"code","47e9d6da":"code","b7a85d4e":"code","abc69ba3":"code","ea2cab28":"code","cc5ae28d":"code","6cd6f5ff":"code","cf01ca06":"code","ebcacddf":"code","952eafea":"code","47ac86e9":"code","8f3dabac":"code","94fd5d41":"code","f33a9282":"code","fef63e0d":"code","52c2ee3e":"code","3b2ff4ef":"code","afcbc9be":"code","d6b2917c":"code","ebeacc32":"code","95623979":"code","ccace517":"code","f2e4f898":"code","21cd41aa":"markdown","7e389fa0":"markdown","a5331349":"markdown","30cc67a9":"markdown","68359568":"markdown","91bba326":"markdown","2631b8b7":"markdown","f5b04487":"markdown","9370c0ac":"markdown","4ca4ba1c":"markdown"},"source":{"2b39fd49":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\n# from sklearn.preprocessing import RobustScaler\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","23bdcb12":"train_csv = pd.read_csv('..\/input\/older-dataset-for-dont-overfit-ii-challenge\/train.csv')\n\ny = train_csv['target'] \nx = train_csv.drop([\"id\" ,'target'],axis=1)\n","b7e3c681":"x.head(2)\n","a80c6853":"x.info()","f549315d":"y = pd.DataFrame(y)\ny.info()","47e9d6da":"x.var()","b7a85d4e":"test_csv  = pd.read_csv('..\/input\/older-dataset-for-dont-overfit-ii-challenge\/test.csv')\ntest = test_csv.drop([\"id\"],axis=1)\ntest.info()","abc69ba3":"test.info()","ea2cab28":"print(x.isnull().values.any() ,'\\n',test.isnull().values.any())","cc5ae28d":"test.info()","6cd6f5ff":"x.shape\n","cf01ca06":"y.shape\n","ebcacddf":"test.shape","952eafea":"import matplotlib.pyplot as plt\ny.hist()\n","47ac86e9":"plt.bar(range(2), (x.shape[0], test.shape[0]))\nplt.xticks(range(2), ('train','test'))\nplt.ylabel('Number of data') \nplt.title('Can we avoid overfitting')\nplt.show()","8f3dabac":"print('Distributions of the first 28 columns')\nplt.figure(figsize=(12,12))\nfor i, col in enumerate(list(x.columns)[2:30]):\n    plt.subplot(7,4,i+1)\n    plt.hist(x[col])\n    plt.title(col)","94fd5d41":"# from sklearn.preprocessing import StandardScaler\n# x_final = x\n# ss = StandardScaler().fit(x_final)\n# ss.transform(x_final)\n# x_final = pd.DataFrame(x_final)\n#x_final","f33a9282":"from sklearn.preprocessing import RobustScaler\nrbst = RobustScaler()\nx_final = rbst.fit_transform(x)\ntest_final = rbst.fit_transform(test)\n\n\nx_final = pd.DataFrame(x_final)\ntest_final = pd.DataFrame(test_final)","fef63e0d":"#Split data\nfrom sklearn.model_selection import train_test_split\nx_train, x_val, y_train, y_val = train_test_split(x_final , y , train_size=0.85, test_size=0.15, random_state = 42 ,stratify = y)","52c2ee3e":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import GridSearchCV\n\nrandom_state = 0\nlogreg_clf = LogisticRegression(solver='liblinear',random_state = random_state)\nparam_grid = {'class_weight' : ['balanced', None], \n                'penalty' : ['l2','l1'],  \n                'C' : [0.001, 0.01, 0.1, 1, 10]}\n\ngrid = GridSearchCV(estimator = logreg_clf, param_grid = param_grid , scoring = 'roc_auc', verbose = 1, n_jobs = -1, cv = 35 )\n\ngrid.fit(x_train,y_train)\n\nprint(\"Best Score:\" + str(grid.best_score_))\nprint(\"Best Parameters: \" + str(grid.best_params_))\n\nbest_parameters = grid.best_params_","3b2ff4ef":"#Based on the grid search i came out with best hyperparameters\n\nfrom sklearn.linear_model import LogisticRegression\n\nlogreg_clf = LogisticRegression(solver='liblinear' , C =0.2 , class_weight='balanced' , \n                                penalty='l1' , random_state = 45)\n\nlogreg_clf.fit(x_train, y_train) \ny_pred_lr = logreg_clf.predict(x_val)\n\nfrom sklearn.metrics import accuracy_score\n\naccuracy_lg = accuracy_score(y_val,  y_pred_lr)\n\nprint('prediction accuracyy',accuracy_lg*100,'%')\n\nprint('model score' , logreg_clf.score(x_val, y_val)*100 ,'%')","afcbc9be":"from sklearn.ensemble import RandomForestClassifier \nfrom sklearn.model_selection import GridSearchCV\n\nrandom_state = 42\nRFC_model = RandomForestClassifier(random_state = random_state,n_estimators = 10 ,criterion = 'gini')\n\nparam_grid_RFC = {'max_depth' : [5,7,9,11], \n                'min_samples_leaf' : [1, 2,5,7,9] ,\n                'class_weight' : ['balanced', 'balanced_subsample' ]}\n\ngrid_rf = GridSearchCV(estimator = RFC_model, param_grid = param_grid_RFC , scoring = 'roc_auc', verbose = 1, n_jobs = -1, cv = 35 )\n\ngrid_rf.fit(x_train,y_train)\n\nprint(\"Best Score:\" + str(grid_rf.best_score_))\nprint(\"Best Parameters: \" + str(grid_rf.best_params_))","d6b2917c":"from sklearn.ensemble import RandomForestClassifier \nRFC_model = RandomForestClassifier(random_state = 42 ,n_estimators = 100 ,criterion = 'gini' , \n                                   class_weight='balanced', max_depth = 9, min_samples_leaf= 7)\nRFC_model.fit(x_train, y_train)\n\nfrom sklearn.metrics import accuracy_score\ny_pred_rf = RFC_model.predict(x_val)\naccuracy_rf = accuracy_score(y_val,  y_pred_rf)\nprint('Validation accuracyy',accuracy_rf*100,'%')\nprint('model score' , RFC_model.score(x_train, y_train))\n","ebeacc32":"from sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.model_selection import GridSearchCV\n\nrandom_state = 42\nknn_model = KNeighborsClassifier(metric ='minkowski' ,metric_params = None ,  leaf_size=30)\n\nparam_grid_knn = {'n_neighbors' : [3,4,5,7],\n                  'weights':['uniform', 'distance'] , \n                  'p' :[2,3,4,5] }\n\ngrid_knn = GridSearchCV(estimator = knn_model, param_grid = param_grid_knn , verbose = 1, n_jobs = -1, cv = 20 )\n\ngrid_knn.fit(x_train,y_train)\n\nprint(\"Best Score:\" + str(grid_knn.best_score_))\nprint(\"Best Parameters: \" + str(grid_knn.best_params_))","95623979":"from sklearn.neighbors import KNeighborsClassifier\n\nkn_model = KNeighborsClassifier(metric ='minkowski' ,metric_params = None ,  leaf_size=2 , \n                                n_neighbors= 3, p=2,weights='uniform')\nkn_model.fit(x_train, y_train)\n\nfrom sklearn.metrics import accuracy_score\n\ny_pred_kn = kn_model.predict(x_val)\n\naccuracy_kn = accuracy_score(y_val,  y_pred_kn)\nprint('Prediction accuracyy',accuracy_kn*100,'%')\nprint('Train score' , kn_model.score(x_train, y_train))","ccace517":"\ny_pred_lr_test = logreg_clf.predict(test_final) #to be saved\ny_pred_lr_test = pd.DataFrame({\"ID\": test_csv[\"id\"],\"Target\": y_pred_lr_test})\n\n\ny_pred_lr_test.to_csv('submission.csv', index=False)  \ny_pred_lr_test","f2e4f898":"y_pred_lr_test['Target'].hist()","21cd41aa":"I will use first use grid search with models to choose best hyper parameters \n","7e389fa0":"**Model 2 : RandomForestClassifi**","a5331349":"**Grid search fot his model to get best hyperparameters**","30cc67a9":"**Model :1 - logistic regression**","68359568":"**First use Grissearch to get best parameters for Logistic regression**","91bba326":"# **Submission**","2631b8b7":"**Model 3 : KNeighborsClassifier**","f5b04487":"**Try grid search first**","9370c0ac":"# Read CSVs","4ca4ba1c":"# **Scaling data**"}}