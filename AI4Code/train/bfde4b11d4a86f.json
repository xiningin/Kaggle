{"cell_type":{"cb17f17f":"code","cacf6361":"code","b0ec586d":"code","5c1e96cd":"code","7264096c":"code","ebd48d94":"code","719abce6":"code","a7fb0bd2":"code","a9e292a2":"markdown","bc0e807e":"markdown","846c2bc6":"markdown","3f56223a":"markdown","91480878":"markdown","8544ec4e":"markdown","595f2266":"markdown","014d915d":"markdown","674f860a":"markdown","3ae0ffb5":"markdown","018c572f":"markdown","224b0612":"markdown","ae52850e":"markdown","e6e9704c":"markdown","bacc024e":"markdown","7574ac52":"markdown"},"source":{"cb17f17f":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","cacf6361":"from sklearn import datasets #import datasets to use in this notebook\nfrom sklearn import model_selection #model_selection provides the methods for hyperparameter tuning\nfrom skopt import BayesSearchCV #importing BayesSearchCV for Bayesian Optimization","b0ec586d":"import sklearn\nsklearn.metrics.SCORERS.keys()","5c1e96cd":"#loading bostonhousing dataset for regression problems\nboston_data = datasets.load_boston()\nr_X = pd.DataFrame(boston_data.data,columns=boston_data.feature_names)\nr_y = boston_data.target\nprint(f\"Shape of the dataset : {r_X.shape}\")\nprint(r_X.head())\n\n#applying standard scaling on the dataset\nfrom sklearn.preprocessing import StandardScaler\nsc_r = StandardScaler()\nsc_r_X = sc_r.fit_transform(r_X)\n\n#split the dataset into test and train \nfrom sklearn.model_selection import train_test_split\n#taking 20% data as test data and 80% data as train data\nr_X_train,r_X_test,r_Y_train,r_Y_test = train_test_split(sc_r_X,r_y, test_size=0.20, random_state=42) ","7264096c":"#loading Iris dataset for regression problems\niris_data = datasets.load_iris()\nc_X = pd.DataFrame(iris_data.data,columns=iris_data.feature_names)\nc_y = iris_data.target\nprint(f\"Shape of the dataset : {c_X.shape}\")\nprint(c_X.head())\n\n#applying standard scaling on the dataset\nfrom sklearn.preprocessing import StandardScaler\nsc_c = StandardScaler()\nsc_c_X = sc_c.fit_transform(c_X)\n\n#split the dataset into test and train \nfrom sklearn.model_selection import train_test_split\n#taking 20% data as test data and 80% data as train data\nc_X_train,c_X_test,c_Y_train,c_Y_test = train_test_split(sc_c_X,c_y, test_size=0.20, random_state=42) ","ebd48d94":"from sklearn.linear_model import Ridge\nR_reg = Ridge()\n#parameter grid dictionary\nparams = {\n    'alpha' : [0.01, 0.1, 1.0, 10, 100],\n    'fit_intercept' : [True,False],\n    'normalize' : [True,False]\n}\n#r2_score is choosen as evaluation matrics\ngs = model_selection.GridSearchCV(estimator=R_reg,param_grid=params,scoring='r2',cv=3,n_jobs = -1)\nprint(gs)\n#fit our traing data to GridSearchCV\ngs.fit(r_X_train,r_Y_train)\nbest_est = gs.best_estimator_\nprint(f\"Best Estimator is : {best_est}\")","719abce6":"from sklearn import svm\nsvm_clf = svm.SVC(probability=True)\n#parameter grid dictionary\nparams = {\n    'C': [0.1, 1, 10, 100, 1000],  \n    'gamma': [1, 0.1, 0.01, 0.001, 0.0001], \n    'kernel': ['linear','rbf'] #other kernels are \u2018sigmoid\u2019, \u2018precomputed\u2019, 'ploy'\n    \n}\n#neg_log_loss is choosen as evaluation matrics\n#here I am using n_iter = 20 only you can choose more\ngs = model_selection.RandomizedSearchCV(n_iter=20,estimator=svm_clf,param_distributions=params,scoring='neg_log_loss',cv=3,n_jobs = -1)\nprint(gs)\n#fit our traing data to RandomizedSearchCV\ngs.fit(c_X_train,c_Y_train)\nbest_est = gs.best_estimator_\nprint(f\"Best Estimator is : {best_est}\")","a7fb0bd2":"#importing skopt library for optimization\nimport skopt\nfrom sklearn import ensemble\n\nrf_clf = ensemble.RandomForestClassifier()\n#parameter grid dictionary\nparams = {\n   'n_estimators': [int(x) for x in np.linspace(50,1000, num=20)], #this return a list with 20 equally spaced numbers between 50 and 1000\n   'max_features': [len(c_X.columns), \"sqrt\", \"log2\"],\n   'max_depth': [int(x) for x in np.linspace(5,50,num=10)],\n   'min_samples_split': [3,4,6,7,8,9,10],\n   'min_samples_leaf': [1,3,5,7,9,10],\n   'bootstrap': [True,False]\n    \n}\n\n#here I am using n_iter = 50 only you can choose more\ngs = skopt.BayesSearchCV(n_iter=50,estimator=rf_clf,search_spaces=params,optimizer_kwargs={'base_estimator': 'RF'},cv=5,n_jobs = -1)\nprint(gs)\n#fit our traing data to BayesSearchCV\ngs.fit(c_X_train,c_Y_train)\nbest_est = gs.best_estimator_\nbest_score = gs.best_score_\nprint(f\"Best Estimator is : {best_est}\")\nprint(f\"Best Score is : {best_score}\")","a9e292a2":"    Following evaluation metrics are available in sklearn library.","bc0e807e":"I am using boston house price data for in this notebook for Regression algorithms.","846c2bc6":"### RandomizedSearchCV :\nRandomizedSearchCV generates all possible combination of given hyperparameters and then randomly choose fix numbers of combinations. How many combinations of hyperparameters to be used is given by us. It trains the model for those hyperparameter combinations and use cross-validation technique to measure the accuracy and return the best estimator. It takes less time then GridSearchCV.<br>\n\n\nSome important parameters and attributes which are important for our model :<br>\n\n ***Parameters*** :\n - n_iter : It is the parameter that decides for how much combinations model is going to be trained. The default value is 10. If the value fo n_iter is high there may be a high chance we get the best estimator.\n - estimatior : In this parameter, we pass the function of our model for which we want to tune hyperparameters. The information is only for sklearn library.\n - param_distributions : This parameter takes a dictionary which consist of hyperparameter name and their values.\n - scoring : This parameter take a string which define for what evaluation matrics we want to optimize our model.\n - cv : Number of cross validations we want split our training dataset. Default value is 5. <br>\n \n ***Attributes***\n - best_estimator_ : It provide which estimatior gives best result for letf-out data.\n - best_params_ : Parameter setting that gave the best results on the hold out data.\n - best_score_ : Mean cross-validated score of the best_estimator.","3f56223a":"*So in this notebook, I discussed some common approaches for hyperparameter tuning which you can be applied in your machine learning algorithm optimization. If you find this notebook useful upvote the note or some mistake in the notebook please mention it in the comment section.*","91480878":"### Hyperparameter Tuning of Random Forest Classifier","8544ec4e":"Here we discuss how we can tune **Ridge_regression hyperparameters** uning GridSearchCV.","595f2266":"https:\/\/scikit-optimize.github.io\/stable\/modules\/generated\/skopt.BayesSearchCV.html#skopt.BayesSearchCV\n\nhttps:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.model_selection.RandomizedSearchCV.html\n\nhttps:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.model_selection.GridSearchCV.html\n\nhttps:\/\/towardsdatascience.com\/a-conceptual-explanation-of-bayesian-model-based-hyperparameter-optimization-for-machine-learning-b8172278050f\n\n","014d915d":"Here I am going to discuss we can tune hyperparameters for **support vector machine** classifier.","674f860a":"### GridSearchCV :\nGridSearchCV uses every possible combination of given entries for our hyperparameters and gives the output for which set of hyperparameter our model performs best. This method takes very much time because it tries out each and every possible combination. Here we discuss how we can use GridSearchCV using sklearn library.<br>\n\n\nSome important parameters and attributes which are important for our model :<br>\n\n ***Parameters*** :\n - estimatior : In this parameter, we pass the function of our model for which we want to tune hyperparameters. The information is only for sklearn library.\n - param_grid : This parameter takes a dictionary which consist of hyperparameter name and their values.\n - scoring : This parameter take a string which define for what evaluation matrics we want to optimize our model.\n - cv : Number of cross validations we want split our training dataset. Default value is 5. <br>\n \n ***Attributes***\n - best_estimator_ : It provide which estimatior gives best result for letf-out data.\n - best_params_ : Parameter setting that gave the best results on the hold out data.\n - best_score_ : Mean cross-validated score of the best_estimator.","3ae0ffb5":"**What is hyperparameter ?**\n\nA hyperparameter is a variable in our machine learning algorithm which we can not learn by training the model and its value can not be estimated based on data. We have to simply put the values according to over model performance and see for which value of hyperparameters the model is giving the best result.\n\nHyperparameter tuning is a very time-consuming process in the machine learning pipeline if we do it manually and try out every possible combination of hyperparameters for the machine learning algorithm. There are various methods and libraries are available for this task such as :\n\n- GridSearchCV \n\n- RandomizedSearchCV\n\n- Bayesian Optimization\n\n- Sequential Model Based Optimization(Tuning a scikit-learn estimator with skopt)\n\n- Optuna\n\n- Genetic Algorithms \n\n- Hyperopt\n","018c572f":"Here we can see for alpha = 0.01 and normalise = True our model gives best r2_score.","224b0612":"Using Iris flower dataset for classification algorithms","ae52850e":"Hyperparameter tuning is a very essential part of our machine learning project, with good hyperparameters we can achieve a very good result for our machine learning model. To optimize our machine leanring model performance, we must tune the hyperparameters.","e6e9704c":"In this notebook, I am going to discuss how we can tune hyperparameters using GridSearchCV, RandomizedSearchCV, and Bayesian Optimization for various machine learning algorithms and discuss how these methods work.","bacc024e":"**References** :","7574ac52":"### Bayesian Optimization :\nBayesian optimization works fast and provides more efficient results compare to GridSearchCV and RandomizedSearchCV. This technique is based on Bayes theorem and provides a probabilistically principled method for global optimization. Bayesian Optimization keeps tracks of past combinations that we previously used and according to that, it provides hyperparameter to the model in such a manner that results in improves in each iteration.<br>\n\n***Baysian optimzation is not present in sklearn library for this we use skopt library.***\n\nSome important parameters and attributes which are important for our model :<br>\n\n - n_iter : It is the parameter that decides for how much combinations model is going to be trained. The default value is 50. If the value fo n_iter is high there may be a high chance we get the best estimator.\n - estimatior : In this parameter, we pass the function of our model for which we want to tune hyperparameters. The information is only for sklearn library.\n - search_spaces : This parameter takes a dictionary which consist of hyperparameter name and their values.\n - optimizer_kwargs : Dictionary of arguments passed to Optimizer. For example, {'base_estimator': 'RF'} would use a Random Forest surrogate instead of the default Gaussian Process.\n - cv : Number of cross validations we want to split our training dataset. Default value is 3. <br>\n \n ***Attributes***\n - best_estimator_ : It provide which estimatior gives best result for letf-out data.\n - best_params_ : Parameter setting that gave the best results on the hold out data.\n - best_score_ : Mean cross-validated score of the best_estimator."}}