{"cell_type":{"a4b3ebf3":"code","be7b8b82":"code","02a761e6":"code","e04b1c66":"code","a86c0b16":"code","9ed63df7":"code","1b7524b1":"code","b8c1620d":"code","a31ebdc5":"code","823cb27c":"code","344103ca":"code","b60128dd":"code","252df8af":"code","52511788":"code","b4305a6f":"code","8e3a5606":"code","6e1a5297":"code","31579f62":"code","cdd277a2":"code","2e302033":"code","eed195b2":"code","b4842774":"code","b1be495d":"code","fd4b77de":"code","ac703aba":"code","c110ff87":"code","609ecdaf":"code","0dbb2037":"code","bff1a993":"code","07999a0b":"code","69c3157a":"code","3f630e03":"code","cce7a129":"code","e5598a3a":"code","6dd2d75f":"code","1a231b8b":"code","3cbc8eaf":"markdown","164e15e6":"markdown","36fafb16":"markdown","1f9660b2":"markdown","d27d4792":"markdown","a9475bf4":"markdown","9ed01b3d":"markdown","f823c733":"markdown","f96802d6":"markdown","025acf56":"markdown","13a9fca5":"markdown","1ad5919a":"markdown","14fff246":"markdown"},"source":{"a4b3ebf3":"import numpy as np\nimport matplotlib.pylab as plt\nimport pandas as pd\n\n#from PIL import ImageDraw, Image\nimport cv2\n\nfrom tqdm import tqdm","be7b8b82":"train_df = pd.read_csv(\"..\/input\/pku-autonomous-driving\/train.csv\")\nprint(train_df.shape)\n\ntrain_df.head()","02a761e6":"test_df = pd.read_csv(\"..\/input\/pku-autonomous-driving\/sample_submission.csv\")\nprint(test_df.shape)\n\ntest_df.head()","e04b1c66":"train_average_img = np.zeros([2710, 3384, 3])\ntrain_dir = \"..\/input\/pku-autonomous-driving\/train_images\/\"\n\nfor imgid in tqdm(train_df[\"ImageId\"]):\n    color_img = cv2.imread(train_dir + imgid + \".jpg\")\n    train_average_img += color_img","a86c0b16":"train_average_img_ = (train_average_img \/ train_df.shape[0]).astype(np.uint8)\ntrain_average_img_ = cv2.cvtColor(train_average_img_, cv2.COLOR_BGR2RGB)","9ed63df7":"plt.figure(figsize=(13, 13))\nplt.imshow(train_average_img_)","1b7524b1":"plt.imsave(\"train_all_average.png\",train_average_img_)","b8c1620d":"train_template = train_average_img_[2600:,2200:2800,:]\nplt.imshow(train_template)","a31ebdc5":"imgid = train_df.iloc[0][\"ImageId\"]\nimgid","823cb27c":"tmp_img = cv2.imread(\"..\/input\/pku-autonomous-driving\/train_images\/\" + imgid + \".jpg\")\ntmp_img = cv2.cvtColor(tmp_img, cv2.COLOR_BGR2RGB)\n\n# for \ntmp_img = tmp_img[1500:,:,:]\n\nplt.figure(figsize=(13, 13))\nplt.imshow(tmp_img)","344103ca":"_, w,h = train_template.shape[::-1]\n\nresult = cv2.matchTemplate(tmp_img, train_template, cv2.TM_CCOEFF_NORMED)\nmin_val, max_val, min_loc, max_loc = cv2.minMaxLoc(result)\nprint(f\"max value: {max_val}, position: {max_loc}\")\n\ntop_left = max_loc\nbottom_right = (top_left[0] + w, top_left[1] + h)\n\ntmp_img_rect = cv2.rectangle(tmp_img,top_left, bottom_right, [255,0,0], 3)\n\nplt.figure(figsize=(13, 13))\nplt.imshow(tmp_img_rect)","b60128dd":"imgid_list = []\nx_list = []\ny_list = []\n\ntest_dir = \"..\/input\/pku-autonomous-driving\/test_images\/\"\n\nfor imgid in tqdm(test_df[\"ImageId\"]):\n    filename = test_dir+imgid+\".jpg\"\n    \n    color_img = cv2.imread(filename)\n    \n    color_img = color_img[2000:,1500:,:] # for matching speedup\n    \n    color_img = cv2.cvtColor(color_img, cv2.COLOR_BGR2RGB)\n    color_img = cv2.copyMakeBorder(color_img,0,55,0,0,cv2.BORDER_REPLICATE) # for robust matching\n    color_img = color_img.astype(np.uint8)\n    \n    result = cv2.matchTemplate(color_img, train_template, cv2.TM_CCOEFF_NORMED)\n    min_val, max_val, min_loc, max_loc = cv2.minMaxLoc(result)\n\n    imgid_list.append(imgid)\n    x_list.append(max_loc[0])\n    y_list.append(max_loc[1])","252df8af":"matched_df = pd.DataFrame({\n    \"ImageId\":imgid_list,\n    \"x\":x_list,\n    \"y\":y_list,\n})","52511788":"print(matched_df.shape)\nmatched_df.head()","b4305a6f":"plt.scatter(matched_df[\"x\"],matched_df[\"y\"])","8e3a5606":"plt.scatter(matched_df[\"x\"],matched_df[\"y\"])\nplt.xlim(645,760)\nplt.ylim(580,620)\nplt.grid()","6e1a5297":"matched_df[\"no_crop_no_flip\"] = (\n    (matched_df[\"x\"] >= 680) &\n    (matched_df[\"x\"] <= 720) & \n    (matched_df[\"y\"] >= 592) & \n    (matched_df[\"y\"] <= 608)).astype(\"int\")","31579f62":"matched_df.head()","cdd277a2":"x_list = []\ny_list = []\n\ntest_dir = \"..\/input\/pku-autonomous-driving\/test_images\/\"\n\nfor imgid in tqdm(test_df[\"ImageId\"]):\n    \n    if matched_df[matched_df[\"ImageId\"] == imgid].iloc[0][\"no_crop_no_flip\"] ==1: # no need to match again\n        x_list.append(-1)\n        y_list.append(-1)\n        continue\n    \n    filename = test_dir+imgid+\".jpg\"\n    \n    color_img = cv2.imread(filename)\n    color_img = np.fliplr(color_img).copy()\n    \n    color_img = color_img[2000:,1500:,:] # for matching speedup\n    \n    color_img = cv2.cvtColor(color_img, cv2.COLOR_BGR2RGB)\n    color_img = cv2.copyMakeBorder(color_img,0,55,0,0,cv2.BORDER_REPLICATE) # for robust matching\n    color_img = color_img.astype(np.uint8)\n    \n    result = cv2.matchTemplate(color_img, train_template, cv2.TM_CCOEFF_NORMED)\n    min_val, max_val, min_loc, max_loc = cv2.minMaxLoc(result)\n\n    x_list.append(max_loc[0])\n    y_list.append(max_loc[1])","2e302033":"matched_df[\"flip_x\"] = x_list\nmatched_df[\"flip_y\"] = y_list","eed195b2":"plt.scatter(matched_df[\"flip_x\"],matched_df[\"flip_y\"])\nplt.xlim(645,760)\nplt.ylim(580,620)\nplt.grid()","b4842774":"matched_df[\"no_crop_flip\"] = (\n    (matched_df[\"flip_x\"] >= 680) &\n    (matched_df[\"flip_x\"] <= 720) & \n    (matched_df[\"flip_y\"] >= 592) & \n    (matched_df[\"flip_y\"] <= 608)).astype(\"int\")","b1be495d":"matched_df.head()","fd4b77de":"print((matched_df[\"no_crop_no_flip\"] == 1).sum())\nprint((matched_df[\"no_crop_flip\"] == 1).sum())\nprint(((matched_df[\"no_crop_no_flip\"] != 1) & (matched_df[\"no_crop_flip\"] != 1)).sum())","ac703aba":"fig = plt.figure(figsize=(17, 17))\nnum=1\n\nfor idx,row in matched_df[matched_df[\"no_crop_no_flip\"] == 1].sample(16).iterrows():\n    filename = test_dir+row[\"ImageId\"] +\".jpg\"\n    img = cv2.imread(filename)\n    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n    ax = fig.add_subplot(4, 4, num)\n    ax.set_aspect('equal')\n    \n    ax.imshow(img)\n    num+=1","c110ff87":"fig = plt.figure(figsize=(17, 17))\nnum=1\n\nfor idx,row in matched_df[matched_df[\"no_crop_flip\"] == 1].sample(16).iterrows():\n    filename = test_dir+row[\"ImageId\"] +\".jpg\"\n    img = cv2.imread(filename)\n    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n    ax = fig.add_subplot(4, 4, num)\n    ax.set_aspect('equal')\n    \n    ax.imshow(img)\n    num+=1","609ecdaf":"fig = plt.figure(figsize=(17, 17))\nnum=1\n\nfor idx,row in matched_df[(matched_df[\"no_crop_no_flip\"] != 1) & (matched_df[\"no_crop_flip\"] != 1)].sample(16).iterrows():\n    filename = test_dir+row[\"ImageId\"] +\".jpg\"\n    img = cv2.imread(filename)\n    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n    ax = fig.add_subplot(4, 4, num)\n    ax.set_aspect('equal')\n    \n    ax.imshow(img)\n    num+=1","0dbb2037":"matched_df.to_csv(\"matched_df.csv\",index=None)","bff1a993":"orig_sub_df = pd.read_csv(\"..\/input\/sample-submission\/predictions.csv\")\nprint(orig_sub_df.shape)","07999a0b":"orig_sub_df[\"no_crop_flip\"] = matched_df[\"no_crop_flip\"]\norig_sub_df.head()","69c3157a":"PredictionString_list=[]\nfor idx,row in orig_sub_df.iterrows():\n    if row[\"no_crop_flip\"] == 0:\n        PredictionString_list.append(row[\"PredictionString\"])\n    else:\n        PredictionString_list.append(\"\") # blank prediction","3f630e03":"test_df = pd.read_csv(\"..\/input\/pku-autonomous-driving\/sample_submission.csv\")\nprint(test_df.shape)","cce7a129":"test_df[\"PredictionString\"] = PredictionString_list","e5598a3a":"(test_df[\"PredictionString\"] == \"\").sum()","6dd2d75f":"test_df[18:25]","1a231b8b":"test_df.to_csv('submission.csv', index=False)","3cbc8eaf":"246\/2021 images are blank predictions,  \nthen Public or Private score must be worse.\n\nBut Public and Private LB score remains same.","164e15e6":"You can try with your submission file.  \nNote: This template matching method is not perfect, so you may encount small score change.","36fafb16":"### visualize","1f9660b2":"no crop, but flipped images","d27d4792":"In 2021 test images, there are 1230 no-crop and no-flip images, and 246 no-crop and h-flip images.  \nRest(545) images are cropped.","a9475bf4":"No crop, no flip images","9ed01b3d":"cropped images","f823c733":"If a image is not cropped and\/or flipped, this \"EV bonnet mark\" should be found here.","f96802d6":"flip and do same template matching","025acf56":"### template matching\nEV bonnet mark as a template","13a9fca5":"This kernel shows:\n* How to check image flip\/crop with simple way\n* Flip\/crop images are not inclueded leaderboard score calculation","1ad5919a":"### check submission score\nhttps:\/\/www.kaggle.com\/hocop1\/centernet-baseline?scriptVersionId=23634825  \nOriginal Private Score:0.029  \nOriginal Public Score:0.027","14fff246":"### average train images"}}