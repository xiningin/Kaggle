{"cell_type":{"3d73bb23":"code","8cc02b5b":"code","55ac4039":"code","8a2ea44b":"code","3c3b041c":"code","c0302792":"code","cc394708":"code","3f3bc6cf":"code","0cefaf9f":"code","2390def6":"code","b14d9369":"code","227c8cec":"code","a04c366a":"code","62e1953f":"code","630a7659":"code","89a96677":"code","0b0825ae":"code","4cf44d8b":"code","e575f7c7":"code","9d792be0":"code","a1856247":"code","0434c1f1":"code","2759b60e":"code","17c47ccd":"code","03dc1b3d":"code","7727046a":"code","2ce20afb":"code","714727e0":"code","900c8e29":"code","959ed417":"code","aebd99bc":"code","aafd81d2":"code","d3e88c4e":"code","642269f9":"code","47b824e9":"code","fa57471f":"code","fce6f2c9":"code","1fc5f362":"markdown","7b88f7d8":"markdown","429cd3bb":"markdown","4fd99055":"markdown","83932122":"markdown","ec960ade":"markdown","cd373af0":"markdown","d31af2fd":"markdown","1b9c5cef":"markdown"},"source":{"3d73bb23":"# Importar os principais pacotes\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nsns.set()\n\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nfrom tqdm import tqdm_notebook as tqdm\nimport re\nimport codecs\nimport time\nimport datetime\nimport gc\nfrom numba import jit\nfrom collections import Counter\nimport copy\nfrom typing import Any\n\n# Evitar que aparece os warnings\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\n# Seta algumas op\u00e7\u00f5es no Jupyter para exibi\u00e7\u00e3o dos datasets\npd.set_option('display.max_columns', 200)\npd.set_option('display.max_rows', 200)\n\n# Variavel para controlar o treinamento no Kaggle\nTRAIN_OFFLINE = False","8cc02b5b":"# Importa os pacotes de algoritmos\nfrom sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\nimport lightgbm as lgb\n\n# Importa os pacotes de algoritmos de redes neurais (Keras)\nimport keras\nfrom keras.losses import binary_crossentropy, categorical_crossentropy\nfrom keras.utils import to_categorical\nfrom keras.layers import Dense,Input,Flatten,concatenate,Dropout,Lambda,BatchNormalization\nfrom keras.layers import Activation\nfrom keras.models import Sequential, Model\nfrom keras.callbacks import Callback,EarlyStopping,ModelCheckpoint\nimport keras.backend as K\nfrom keras.optimizers import Adam\n#from keras_radam import RAdam\nfrom keras import optimizers\nfrom keras.utils import np_utils\n\n# Importa pacotes do sklearn\nfrom sklearn import preprocessing\nimport sklearn.metrics as mtr\nfrom sklearn.model_selection import KFold, train_test_split, GridSearchCV\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.preprocessing import OneHotEncoder, LabelEncoder\nfrom sklearn.preprocessing import scale, MinMaxScaler, StandardScaler\nfrom sklearn import model_selection\nfrom sklearn.utils import class_weight","55ac4039":"def reduce_mem_usage(df, use_float16=False):\n    \"\"\" iterate through all the columns of a dataframe and modify the data type\n        to reduce memory usage.        \n    \"\"\"\n    start_mem = df.memory_usage().sum() \/ 1024**2\n    print('Memory usage of dataframe is {:.2f} MB'.format(start_mem))\n    \n    for col in df.columns:\n        if is_datetime(df[col]) or is_categorical_dtype(df[col]):\n            # skip datetime type or categorical type\n            continue\n        col_type = df[col].dtype\n        \n        if col_type != object:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)  \n            else:\n                if use_float16 and c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                    df[col] = df[col].astype(np.float16)\n                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)\n    \n\n    end_mem = df.memory_usage().sum() \/ 1024**2\n    print('Memory usage after optimization is: {:.2f} MB'.format(end_mem))\n    print('Decreased by {:.1f}%'.format(100 * (start_mem - end_mem) \/ start_mem))\n    \n    return df","8a2ea44b":"def add_datepart(df: pd.DataFrame, field_name: str,\n                 prefix: str = None, drop: bool = True, time: bool = True, date: bool = True):\n    \"\"\"\n    Helper function that adds columns relevant to a date in the column `field_name` of `df`.\n    from fastai: https:\/\/github.com\/fastai\/fastai\/blob\/master\/fastai\/tabular\/transform.py#L55\n    \"\"\"\n    field = df[field_name]\n    prefix = ifnone(prefix, re.sub('[Dd]ate$', '', field_name))\n    attr = ['Year', 'Month', 'Week', 'Day', 'Dayofweek', 'Is_month_end', 'Is_month_start']\n    if date:\n        attr.append('Date')\n    if time:\n        attr = attr + ['Hour', 'Minute']\n    for n in attr:\n        df[prefix + n] = getattr(field.dt, n.lower())\n    if drop:\n        df.drop(field_name, axis=1, inplace=True)\n    return df\n\n\ndef ifnone(a: Any, b: Any) -> Any:\n    \"\"\"`a` if `a` is not None, otherwise `b`.\n    from fastai: https:\/\/github.com\/fastai\/fastai\/blob\/master\/fastai\/core.py#L92\"\"\"\n    return b if a is None else a","3c3b041c":"def quadratic_kappa(actuals, preds, N=4):\n    \"\"\"This function calculates the Quadratic Kappa Metric used for Evaluation in the PetFinder competition\n    at Kaggle. It returns the Quadratic Weighted Kappa metric score between the actual and the predicted values \n    of adoption rating.\"\"\"\n    w = np.zeros((N,N))\n    O = confusion_matrix(actuals, preds)\n    for i in range(len(w)): \n        for j in range(len(w)):\n            w[i][j] = float(((i-j)**2)\/(N-1)**2)\n    \n    act_hist=np.zeros([N])\n    for item in actuals: \n        act_hist[item]+=1\n    \n    pred_hist=np.zeros([N])\n    for item in preds: \n        pred_hist[item]+=1\n                         \n    E = np.outer(act_hist, pred_hist);\n    E = E\/E.sum();\n    O = O\/O.sum();\n    \n    num=0\n    den=0\n    for i in range(len(w)):\n        for j in range(len(w)):\n            num+=w[i][j]*O[i][j]\n            den+=w[i][j]*E[i][j]\n    return (1 - (num\/den))","c0302792":"def read_data():\n    \n    if TRAIN_OFFLINE:\n        train = pd.read_csv('..\/data\/train.csv')\n        test = pd.read_csv('..\/data\/test.csv')\n        train_labels = pd.read_csv('..\/data\/train_labels.csv')\n        specs = pd.read_csv('..\/data\/specs.csv')\n        sample_submission = pd.read_csv('..\/data\/sample_submission.csv')\n    else:\n        train = pd.read_csv('\/kaggle\/input\/data-science-bowl-2019\/train.csv')\n        test = pd.read_csv('\/kaggle\/input\/data-science-bowl-2019\/test.csv')\n        train_labels = pd.read_csv('\/kaggle\/input\/data-science-bowl-2019\/train_labels.csv')\n        specs = pd.read_csv('\/kaggle\/input\/data-science-bowl-2019\/specs.csv')\n        sample_submission = pd.read_csv('\/kaggle\/input\/data-science-bowl-2019\/sample_submission.csv')\n    \n    return train, test, train_labels, specs, sample_submission","cc394708":"# read data\ntrain, test, train_labels, specs, sample_submission = read_data()","3f3bc6cf":"# Memory reduce\nfrom pandas.api.types import is_datetime64_any_dtype as is_datetime\nfrom pandas.api.types import is_categorical_dtype\n\ntrain=reduce_mem_usage(train)\ntest=reduce_mem_usage(test)\ntrain_labels=reduce_mem_usage(train_labels)\nspecs=reduce_mem_usage(specs)\nsample_submission=reduce_mem_usage(sample_submission)","0cefaf9f":"def encode_title(train, test, train_labels):\n    # encode title\n    train['title_event_code'] = list(map(lambda x, y: str(x) + '_' + str(y), train['title'], train['event_code']))\n    test['title_event_code'] = list(map(lambda x, y: str(x) + '_' + str(y), test['title'], test['event_code']))\n    all_title_event_code = list(set(train[\"title_event_code\"].unique()).union(test[\"title_event_code\"].unique()))\n    \n    # make a list with all the unique 'titles' from the train and test set\n    list_of_user_activities = list(set(train['title'].unique()).union(set(test['title'].unique())))\n    \n    # make a list with all the unique 'event_code' from the train and test set\n    list_of_event_code = list(set(train['event_code'].unique()).union(set(test['event_code'].unique())))\n    list_of_event_id = list(set(train['event_id'].unique()).union(set(test['event_id'].unique())))\n    \n    # make a list with all the unique worlds from the train and test set\n    list_of_worlds = list(set(train['world'].unique()).union(set(test['world'].unique())))\n    \n    # create a dictionary numerating the titles\n    activities_map = dict(zip(list_of_user_activities, np.arange(len(list_of_user_activities))))\n    activities_labels = dict(zip(np.arange(len(list_of_user_activities)), list_of_user_activities))\n    activities_world = dict(zip(list_of_worlds, np.arange(len(list_of_worlds))))\n    assess_titles = list(set(train[train['type'] == 'Assessment']['title'].value_counts().index).union(set(test[test['type'] == 'Assessment']['title'].value_counts().index)))\n    \n    # replace the text titles with the number titles from the dict\n    train['title'] = train['title'].map(activities_map)\n    test['title'] = test['title'].map(activities_map)\n    train['world'] = train['world'].map(activities_world)\n    test['world'] = test['world'].map(activities_world)\n    train_labels['title'] = train_labels['title'].map(activities_map)\n    win_code = dict(zip(activities_map.values(), (4100*np.ones(len(activities_map))).astype('int')))\n    \n    # then, it set one element, the 'Bird Measurer (Assessment)' as 4110, 10 more than the rest\n    win_code[activities_map['Bird Measurer (Assessment)']] = 4110\n    \n    # convert text into datetime\n    train['timestamp'] = pd.to_datetime(train['timestamp'])\n    test['timestamp'] = pd.to_datetime(test['timestamp'])\n    \n    return train, test, train_labels, win_code, list_of_user_activities, list_of_event_code, activities_labels, assess_titles, list_of_event_id, all_title_event_code","2390def6":"# get usefull dict with maping encode\ntrain, test, train_labels, win_code, list_of_user_activities, list_of_event_code, activities_labels, assess_titles, list_of_event_id, all_title_event_code = encode_title(train, test, train_labels)","b14d9369":"def get_data(user_sample, test_set=False):\n\n    # Constants and parameters declaration\n    last_activity = 0\n    \n    user_activities_count = {'Clip':0, 'Activity': 0, 'Assessment': 0, 'Game':0}\n    \n    # new features: time spent in each activity\n    last_session_time_sec = 0\n    accuracy_groups = {0:0, 1:0, 2:0, 3:0}\n    all_assessments = []\n    accumulated_accuracy_group = 0\n    accumulated_accuracy = 0\n    accumulated_correct_attempts = 0 \n    accumulated_uncorrect_attempts = 0\n    accumulated_actions = 0\n    counter = 0\n    time_first_activity = float(user_sample['timestamp'].values[0])\n    durations = []\n    last_accuracy_title = {'acc_' + title: -1 for title in assess_titles}\n    event_code_count: Dict[str, int] = {ev: 0 for ev in list_of_event_code}\n    event_id_count: Dict[str, int] = {eve: 0 for eve in list_of_event_id}\n    title_count: Dict[str, int] = {eve: 0 for eve in activities_labels.values()} \n    title_event_code_count: Dict[str, int] = {t_eve: 0 for t_eve in all_title_event_code}\n    \n    # itarates through each session of one instalation_id\n    for i, session in user_sample.groupby('game_session', sort=False):\n        # i = game_session_id\n        # session is a DataFrame that contain only one game_session\n        \n        # get some sessions information\n        session_type = session['type'].iloc[0]\n        session_title = session['title'].iloc[0]\n        session_title_text = activities_labels[session_title]        \n            \n        # for each assessment, and only this kind off session, the features below are processed\n        # and a register are generated\n        if (session_type == 'Assessment') & (test_set or len(session)>1):\n            # search for event_code 4100, that represents the assessments trial\n            all_attempts = session.query(f'event_code == {win_code[session_title]}')\n            # then, check the numbers of wins and the number of losses\n            true_attempts = all_attempts['event_data'].str.contains('true').sum()\n            false_attempts = all_attempts['event_data'].str.contains('false').sum()\n            # copy a dict to use as feature template, it's initialized with some itens: \n            # {'Clip':0, 'Activity': 0, 'Assessment': 0, 'Game':0}\n            features = user_activities_count.copy()\n            features.update(last_accuracy_title.copy())\n            features.update(event_code_count.copy())\n            features.update(event_id_count.copy())\n            features.update(title_count.copy())\n            features.update(title_event_code_count.copy())\n            features.update(last_accuracy_title.copy())\n            \n            # get installation_id for aggregated features\n            features['installation_id'] = session['installation_id'].iloc[-1]\n            # add title as feature, remembering that title represents the name of the game\n            features['session_title'] = session['title'].iloc[0]\n            \n            features['timestamp'] = session['timestamp'].iloc[0]\n            \n            # the 4 lines below add the feature of the history of the trials of this player\n            # this is based on the all time attempts so far, at the moment of this assessment\n            features['accumulated_correct_attempts'] = accumulated_correct_attempts\n            features['accumulated_uncorrect_attempts'] = accumulated_uncorrect_attempts\n            accumulated_correct_attempts += true_attempts \n            accumulated_uncorrect_attempts += false_attempts\n            # the time spent in the app so far\n            if durations == []:\n                features['duration_mean'] = 0\n            else:\n                features['duration_mean'] = np.mean(durations)\n            durations.append((session.iloc[-1, 2] - session.iloc[0, 2] ).seconds)\n            # the accurace is the all time wins divided by the all time attempts\n            features['accumulated_accuracy'] = accumulated_accuracy\/counter if counter > 0 else 0\n            accuracy = true_attempts\/(true_attempts+false_attempts) if (true_attempts+false_attempts) != 0 else 0\n            accumulated_accuracy += accuracy\n            last_accuracy_title['acc_' + session_title_text] = accuracy\n            # a feature of the current accuracy categorized\n            # it is a counter of how many times this player was in each accuracy group\n            if accuracy == 0:\n                features['accuracy_group'] = 0\n            elif accuracy == 1:\n                features['accuracy_group'] = 3\n            elif accuracy == 0.5:\n                features['accuracy_group'] = 2\n            else:\n                features['accuracy_group'] = 1\n            features.update(accuracy_groups)\n            accuracy_groups[features['accuracy_group']] += 1\n            # mean of the all accuracy groups of this player\n            features['accumulated_accuracy_group'] = accumulated_accuracy_group\/counter if counter > 0 else 0\n            accumulated_accuracy_group += features['accuracy_group']\n            # how many actions the player has done so far, it is initialized as 0 and updated some lines below\n            features['accumulated_actions'] = accumulated_actions\n            \n            # there are some conditions to allow this features to be inserted in the datasets\n            # if it's a test set, all sessions belong to the final dataset\n            # it it's a train, needs to be passed throught this clausule: session.query(f'event_code == {win_code[session_title]}')\n            # that means, must exist an event_code 4100 or 4110\n            if test_set:\n                all_assessments.append(features)\n            elif true_attempts+false_attempts > 0:\n                all_assessments.append(features)\n                \n            counter += 1\n        \n        # this piece counts how many actions was made in each event_code so far\n        def update_counters(counter: dict, col: str):\n                num_of_session_count = Counter(session[col])\n                for k in num_of_session_count.keys():\n                    x = k\n                    if col == 'title':\n                        x = activities_labels[k]\n                    counter[x] += num_of_session_count[k]\n                return counter\n            \n        event_code_count = update_counters(event_code_count, \"event_code\")\n        event_id_count = update_counters(event_id_count, \"event_id\")\n        title_count = update_counters(title_count, 'title')\n        title_event_code_count = update_counters(title_event_code_count, 'title_event_code')\n\n        # counts how many actions the player has done so far, used in the feature of the same name\n        accumulated_actions += len(session)\n        if last_activity != session_type:\n            user_activities_count[session_type] += 1\n            last_activitiy = session_type \n                        \n    # if it't the test_set, only the last assessment must be predicted, the previous are scraped\n    if test_set:\n        return all_assessments[-1]\n    # in the train_set, all assessments goes to the dataset\n    return all_assessments","227c8cec":"def get_train_and_test(train, test):\n    compiled_train = []\n    compiled_test = []\n    \n    for i, (ins_id, user_sample) in tqdm(enumerate(train.groupby('installation_id', sort = False)), total = 17000):\n        compiled_train += get_data(user_sample)\n        \n    for ins_id, user_sample in tqdm(test.groupby('installation_id', sort = False), total = 1000):\n        test_data = get_data(user_sample, test_set = True)\n        compiled_test.append(test_data)\n        \n    reduce_train = pd.DataFrame(compiled_train)\n    reduce_test = pd.DataFrame(compiled_test)\n    categoricals = ['session_title']\n    \n    return reduce_train, reduce_test, categoricals","a04c366a":"# tranform function to get the train and test set\nreduce_train, reduce_test, categoricals = get_train_and_test(train, test)","62e1953f":"add_datepart(reduce_train, 'timestamp')\nadd_datepart(reduce_test, 'timestamp')","630a7659":"def preprocess(reduce_train, reduce_test):\n    for df in [reduce_train, reduce_test]:\n        df['installation_session_count'] = df.groupby(['installation_id'])['Clip'].transform('count')\n        df['installation_duration_mean'] = df.groupby(['installation_id'])['duration_mean'].transform('mean')\n        df['installation_title_nunique'] = df.groupby(['installation_id'])['session_title'].transform('nunique')\n        \n        df['sum_event_code_count'] = df[[2050, 4100, 4230, 5000, 4235, 2060, 4110, 5010, 2070, 2075, 2080, 2081, 2083, 3110, 4010, 3120, 3121, 4020, 4021, \n                                        4022, 4025, 4030, 4031, 3010, 4035, 4040, 3020, 3021, 4045, 2000, 4050, 2010, 2020, 4070, 2025, 2030, 4080, 2035, \n                                        2040, 4090, 4220, 4095]].sum(axis = 1)\n        \n        df['installation_event_code_count_mean'] = df.groupby(['installation_id'])['sum_event_code_count'].transform('mean')\n        \n    features = reduce_train.loc[:, reduce_train.notnull().any(axis = 0)]\n    features = [x for x in reduce_train if x not in ['accuracy_group', 'installation_id']] + ['acc_' + title for title in assess_titles]\n  \n    return reduce_train, reduce_test, features","89a96677":"# call feature engineering function\nreduce_train, reduce_test, features = preprocess(reduce_train, reduce_test)","0b0825ae":"reduce_train.shape, reduce_test.shape","4cf44d8b":"reduce_train.head()","e575f7c7":"reduce_train['accuracy_group'].value_counts(normalize=True)","9d792be0":"# Import\u00e2ncia do Atributo com o Random Forest Regressor\nreduce_train.drop(['installation_id','timestampDate'], axis=1, inplace=True)\n\nX_ = reduce_train.drop('accuracy_group', axis=1)\ny_ = reduce_train['accuracy_group']\n\n# Padronizando os dados (0 para a m\u00e9dia, 1 para o desvio padr\u00e3o)\nscaler = StandardScaler()\nX_ = scaler.fit_transform(X_)\n\n# Cria\u00e7\u00e3o do Modelo - Feature Selection\nmodeloRF = RandomForestRegressor(bootstrap=False, \n                                 max_features=0.3, \n                                 min_samples_leaf=15, \n                                 min_samples_split=8, \n                                 n_estimators=50, \n                                 n_jobs=-1, \n                                 random_state=42)\nmodeloRF.fit(X_, y_)\n\n# Convertendo o resultado em um dataframe\nfeature_importance_df = pd.DataFrame(reduce_train.drop('accuracy_group', axis=1).columns,columns=['Feature'])\nfeature_importance_df['importance'] = pd.DataFrame(modeloRF.feature_importances_.astype(float))\n\n# Realizando a ordenacao por Importancia (Maior para Menor)\nresult = feature_importance_df.sort_values('importance',ascending=False)\nprint(result)","a1856247":"cols = (feature_importance_df[[\"Feature\", \"importance\"]]\n        .groupby(\"Feature\")\n        .mean()\n        .sort_values(by=\"importance\", ascending=False)[:25].index)\n\nbest_features = feature_importance_df.loc[feature_importance_df.Feature.isin(cols)]\n\nplt.figure(figsize=(18,16))\nsns.barplot(x=\"importance\",\n           y=\"Feature\",\n           data=best_features.sort_values(by=\"importance\",\n                                          ascending=False))\nplt.title('Importance Features')\nplt.tight_layout()","0434c1f1":"# Criar um dataset somente com as colunas mais importantes conforme Feature Selection\nnew_X = reduce_train.loc[:,best_features['Feature']]\n\ntrain_x = scaler.fit_transform(new_X)\ntrain_y = np_utils.to_categorical(reduce_train['accuracy_group'])","2759b60e":"def get_nn(x_tr,y_tr,x_val,y_val,shape):\n    K.clear_session()\n    \n    inp = Input(shape = (x_tr.shape[1],))\n\n    x = Dense(1024, input_dim=x_tr.shape[1], activation='relu')(inp)\n    x = Dropout(0.5)(x)    \n    x = BatchNormalization()(x)\n    \n    x = Dense(512, activation='relu')(x)\n    x = Dropout(0.5)(x)    \n    x = BatchNormalization()(x)\n    \n    x = Dense(256, activation='relu')(x)\n    x = Dropout(0.5)(x)\n    x = BatchNormalization()(x)\n    \n    out = Dense(4, activation='softmax')(x)\n    model = Model(inp,out)\n    \n    model.compile(optimizer = 'Adam',\n                  loss='categorical_crossentropy', \n                  metrics=['categorical_accuracy'])\n     \n    es = EarlyStopping(monitor='val_loss', \n                       mode='min',\n                       restore_best_weights=True, \n                       verbose=1, \n                       patience=20)\n\n    mc = ModelCheckpoint('best_model.h5',\n                         monitor='val_loss',\n                         mode='min',\n                         save_best_only=True, \n                         verbose=1, \n                         save_weights_only=True)\n    \n    model.fit(x_tr, y_tr,\n              validation_data=[x_val, y_val],\n              callbacks=[es,mc],\n              epochs=100, \n              batch_size=128,\n              verbose=1,\n              class_weight=class_weight_y,\n              shuffle=True)\n    \n    model.load_weights(\"best_model.h5\")\n    \n    y_pred = model.predict(x_val)\n    y_valid = y_val\n    \n    kappa = quadratic_kappa(y_valid.argmax(axis=1), y_pred.argmax(axis=1))\n\n    return model, kappa","17c47ccd":"gc.collect()","03dc1b3d":"%%time\n\nloop = 2\nfold = 5\n\noof_nn = np.zeros([loop, train_y.shape[0], train_y.shape[1]])\nmodels_nn = []\nkappa_csv_nn = []\n\nclass_weight_y = class_weight.compute_class_weight('balanced',np.unique(y_), y_)\n\nfor k in range(loop):\n    kfold = KFold(fold, random_state = 42 + k, shuffle = True)\n    for k_fold, (tr_inds, val_inds) in enumerate(kfold.split(train_y)):\n        print(\"-----------\")\n        print(f'Loop {k+1}\/{loop}' + f' Fold {k_fold+1}\/{fold}')\n        print(\"-----------\")\n        \n        tr_x, tr_y = train_x[tr_inds], train_y[tr_inds]\n        val_x, val_y = train_x[val_inds], train_y[val_inds]\n        \n        # Train NN\n        nn, kappa_nn = get_nn(tr_x, tr_y, val_x, val_y, shape=val_x.shape[0])\n        models_nn.append(nn)\n        print(\"the %d fold kappa (NN) is %f\"%((k_fold+1), kappa_nn))\n        kappa_csv_nn.append(kappa_nn)\n        \n        #Predict OOF\n        oof_nn[k, val_inds, :] = nn.predict(val_x)\n        \n    print(\"PARTIAL: mean kappa (NN) is %f\"%np.mean(kappa_csv_nn))        ","7727046a":"kappa_oof_nn = []\n\nfor k in range(loop):\n    kappa_oof_nn.append(quadratic_kappa(oof_nn[k,...].argmax(axis=1), train_y.argmax(axis=1)))","2ce20afb":"print(\"mean kappa (NN) is %f\"%np.mean(kappa_csv_nn))\nprint(\"mean OOF kappa (NN) is %f\"%np.mean(kappa_oof_nn))","714727e0":"plt.figure(figsize=(40, 20))\nplt.subplot(2, 1, 1)\nplt.plot(models_nn[0].history.history[\"loss\"], \"o-\", alpha=.4, label=\"loss\")\nplt.plot(models_nn[0].history.history[\"val_loss\"], \"o-\", alpha=.4, label=\"val_loss\")\nplt.axhline(1, linestyle=\"--\", c=\"C2\")\nplt.legend()\nplt.subplot(2, 1, 2)\nplt.plot(models_nn[0].history.history[\"categorical_accuracy\"], \"o-\", alpha=.4, label=\"categorical_accuracy\")\nplt.plot(models_nn[0].history.history[\"val_categorical_accuracy\"], \"o-\", alpha=.4, label=\"val_categorical_accuracy\")\nplt.axhline(.7, linestyle=\"--\", c=\"C2\")\nplt.legend()\nplt.show()","900c8e29":"def predict(x_te, models_nn):\n    \n    model_num_nn = len(models_nn)\n\n    for k,m in enumerate(models_nn):\n        if k==0:\n            y_pred_nn = m.predict(x_te)\n        else:\n            y_pred_nn += m.predict(x_te)\n            \n    y_pred_nn = y_pred_nn \/ model_num_nn\n    \n    return y_pred_nn","959ed417":"reduce_train.accuracy_group = reduce_train.accuracy_group.astype(\"int\")\n\nresult = predict(train_x, models_nn)\n\nquadratic_kappa(reduce_train.accuracy_group, result.argmax(axis=1))","aebd99bc":"@jit\ndef qwk(a1, a2):\n    \"\"\"\n    Source: https:\/\/www.kaggle.com\/c\/data-science-bowl-2019\/discussion\/114133#latest-660168\n\n    :param a1:\n    :param a2:\n    :param max_rat:\n    :return:\n    \"\"\"\n    max_rat = 3\n    a1 = np.asarray(a1, dtype=int)\n    a2 = np.asarray(a2, dtype=int)\n\n    hist1 = np.zeros((max_rat + 1, ))\n    hist2 = np.zeros((max_rat + 1, ))\n\n    o = 0\n    for k in range(a1.shape[0]):\n        i, j = a1[k], a2[k]\n        hist1[i] += 1\n        hist2[j] += 1\n        o +=  (i - j) * (i - j)\n\n    e = 0\n    for i in range(max_rat + 1):\n        for j in range(max_rat + 1):\n            e += hist1[i] * hist2[j] * (i - j) * (i - j)\n\n    e = e \/ a1.shape[0]\n\n    return 1 - o \/ e","aafd81d2":"from functools import partial\nimport scipy as sp\nclass OptimizedRounder(object):\n    \"\"\"\n    An optimizer for rounding thresholds\n    to maximize Quadratic Weighted Kappa (QWK) score\n    # https:\/\/www.kaggle.com\/naveenasaithambi\/optimizedrounder-improved\n    \"\"\"\n    def __init__(self):\n        self.coef_ = 0\n\n    def _kappa_loss(self, coef, X, y):\n        \"\"\"\n        Get loss according to\n        using current coefficients\n        \n        :param coef: A list of coefficients that will be used for rounding\n        :param X: The raw predictions\n        :param y: The ground truth labels\n        \"\"\"\n        X_p = pd.cut(X, [-np.inf] + list(np.sort(coef)) + [np.inf], labels = [0, 1, 2, 3])\n\n        return -qwk(y, X_p)\n\n    def fit(self, X, y):\n        \"\"\"\n        Optimize rounding thresholds\n        \n        :param X: The raw predictions\n        :param y: The ground truth labels\n        \"\"\"\n        loss_partial = partial(self._kappa_loss, X=X, y=y)\n        initial_coef = [0.5, 1.5, 2.5]\n        self.coef_ = sp.optimize.minimize(loss_partial, initial_coef, method='nelder-mead')\n\n    def predict(self, X, coef):\n        \"\"\"\n        Make predictions with specified thresholds\n        \n        :param X: The raw predictions\n        :param coef: A list of coefficients that will be used for rounding\n        \"\"\"\n        return pd.cut(X, [-np.inf] + list(np.sort(coef)) + [np.inf], labels = [0, 1, 2, 3])\n\n\n    def coefficients(self):\n        \"\"\"\n        Return the optimized coefficients\n        \"\"\"\n        return self.coef_['x']","d3e88c4e":"optR = OptimizedRounder()\nresult = predict(train_x, models_nn)\n\noptR.fit(result.reshape(-1,), y_)\ncoefficients = optR.coefficients()\ncoefficients","642269f9":"test_x = scaler.transform(reduce_test.loc[:, best_features['Feature']])\npreds = predict(test_x, models_nn)\npreds","47b824e9":"sample_submission['accuracy_group'] = preds.argmax(axis=1)\nsample_submission.to_csv('submission.csv', index=False)\nsample_submission['accuracy_group'].value_counts(normalize=True)","fa57471f":"sample_submission['accuracy_group'].value_counts(normalize=True)","fce6f2c9":"plt.hist(sample_submission.accuracy_group)\nplt.show()","1fc5f362":"Fork: https:\/\/www.kaggle.com\/hengzheng\/bayesian-optimization-optimizedrounder\n\nInsights: https:\/\/www.kaggle.com\/damienpark\/baseline-dnn-first\n\nPlease, up-vote both.\n","7b88f7d8":"## Feature Selection","429cd3bb":"## Criar e avaliar alguns algoritmos de Machine Learning","4fd99055":"## Predict","83932122":"### One-Hot encoding \/ Scaling \/ Feature Selection","ec960ade":"# CHALLENGE 2019 Data Science Bowl\n### Uncover the factors to help measure how young children learn","cd373af0":"### Modelo Rede Neural (MLP)","d31af2fd":"## Importing libraries","1b9c5cef":"## Submission"}}