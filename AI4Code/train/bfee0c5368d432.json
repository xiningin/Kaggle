{"cell_type":{"87601234":"code","5a707aaa":"code","0a6de312":"code","6df7f1be":"code","07873cb7":"code","3226e92f":"code","6ddc6f40":"code","66d87708":"code","fb37df5f":"code","035f449d":"code","f1f893c8":"code","8ee68c6a":"code","77adb1d1":"code","b6469751":"code","d61d9d11":"code","aa9c3051":"code","1c047c08":"code","83f97cd2":"code","06029fda":"code","85e7f68d":"code","631be81e":"code","03a0bfe5":"markdown","fdbacebf":"markdown","cdfa55fc":"markdown","98a7930b":"markdown","2579e425":"markdown","cef72366":"markdown","f2565270":"markdown","4e1f2219":"markdown","a8157270":"markdown","479dd6e2":"markdown"},"source":{"87601234":"import pandas\nimport os\nimport numpy\nfrom tqdm import tqdm_notebook\nimport gc\nfrom PIL import Image","5a707aaa":"!ln -s \/kaggle\/input\/3d-object-detection-for-autonomous-vehicles\/train_images images\n!ln -s \/kaggle\/input\/3d-object-detection-for-autonomous-vehicles\/train_maps maps\n!ln -s \/kaggle\/input\/3d-object-detection-for-autonomous-vehicles\/train_lidar lidar","0a6de312":"shape = (100, 100, 3)\nMAX_VALUE = 140","6df7f1be":"# Directories :-\nPATH = '\/kaggle\/input\/3d-object-detection-for-autonomous-vehicles\/'\nTRAIN_PATHS = [i for i in os.listdir(PATH) if 'train' in i]\nTEST_PATHS = [i for i in os.listdir(PATH) if 'test' in i]\n\nTRAIN_PATHS, TEST_PATHS","07873cb7":"!pip install lyft-dataset-sdk\nfrom pyquaternion import Quaternion\nfrom lyft_dataset_sdk.lyftdataset import LyftDataset\nlyft_data = LyftDataset(data_path = '.', json_path = PATH + 'train_data', verbose = True)","3226e92f":"train_data = pandas.read_csv(PATH + 'train.csv')","6ddc6f40":"categories = [i['name'] for i in lyft_data.category]\ncategories","66d87708":"columns = ['confidence' ,'center_x', 'center_y', \"center_z\", 'width', 'length', 'height', 'rotate_w', 'rotate_x', 'rotate_y', 'rotate_z', 'class']\nsensors = lyft_data.sensor\nsensors = [i['channel'] for i in sensors]\nsensors = [i for i in sensors if 'LIDAR' not in i]","fb37df5f":"def getImageFileNames(token : str):\n    \n    list_of_filenames = []\n    \n    for sensor in sensors:\n        filename = lyft_data.get('sample_data', lyft_data.get('sample', token)['data'][sensor])['filename']\n        list_of_filenames.append(filename)\n        \n    return list_of_filenames        ","035f449d":"def one_hot_encoding(value):\n    global categories\n    \n    x = categories.index(value)\n    \n    return [0] * (x) + [1] + [0] * (len(categories) - x)","f1f893c8":"def getData(token):\n    \n    list_of_values = []\n    list_of_anns = lyft_data.get('sample', token)['anns']\n    \n    for annotation_token in list_of_anns:\n        values = [1.0]\n        sample_data = lyft_data.get('sample_annotation', annotation_token)\n        values = values + sample_data['translation'] + sample_data['size'] + sample_data['rotation'] + one_hot_encoding(sample_data['category_name'])\n        list_of_values.append(values)\n        \n    for _ in range(MAX_VALUE - len(list_of_anns)):\n        list_of_values.append([0]*11 + one_hot_encoding('other_vehicle'))\n    \n    return numpy.array(list_of_values)","8ee68c6a":"def convertToLD(source, isPrediction : bool):\n    dest = []\n    for record in tqdm_notebook(source):\n        temp = {}\n        if isPrediction:\n            temp['score'] = record[0]\n        temp['translate'] = record[1:4].tolist()\n        temp['size'] = record[4:7].tolist()\n        temp['rotation'] = record[7:11].tolist()\n        temp['class'] = categories[numpy.argmax(record[11:])]\n        dest.append(temp.copy())\n        del temp\n        gc.collect()\n        \n    return dest","77adb1d1":"from tensorflow.keras.layers import Dense, Conv2D, MaxPool2D, Dropout, concatenate, GlobalAveragePooling2D, add\nfrom tensorflow.keras import Input\nfrom tensorflow.keras.models import Model","b6469751":"from keras import backend as K\nimport tensorflow as tf\n\ndef mse(y_true, y_pred):\n     return K.mean(K.square(y_pred - y_true), axis=-1)\n\ndef LossFunction(y_true, y_pred):\n     return mse(y_true[:,0], y_pred[:,0]) + mse(y_true[:,1:4], y_pred[:,1:4]) + mse(y_true[:,4:7], y_pred[:,4:7]) + mse(y_true[:,7:11], y_pred[:,7:11]) + K.categorical_crossentropy(y_true[:,11:], y_pred[:,11:])\n\ndef IOU_Metric(true, pred): #any shape can go - can't be a loss function\n    \n    def iou_loss_core(true,pred):  #this can be used as a loss if you make it negative\n        intersection = true * pred\n        notTrue = 1 - true\n        union = true + (notTrue * pred)\n\n        return (K.sum(intersection, axis=-1) + K.epsilon()) \/ (K.sum(union, axis=-1) + K.epsilon())\n    \n    def castF(x):\n        return K.cast(x, K.floatx())\n\n    def castB(x):\n        return K.cast(x, bool)\n    \n    tresholds = [0.5 + (i*.05)  for i in range(10)]\n\n    #flattened images (batch, pixels)\n    true = K.batch_flatten(true)\n    pred = K.batch_flatten(pred)\n    pred = castF(K.greater(pred, 0.5))\n\n    #total white pixels - (batch,)\n    trueSum = K.sum(true, axis=-1)\n    predSum = K.sum(pred, axis=-1)\n\n    #has mask or not per image - (batch,)\n    true1 = castF(K.greater(trueSum, 1))    \n    pred1 = castF(K.greater(predSum, 1))\n\n    #to get images that have mask in both true and pred\n    truePositiveMask = castB(true1 * pred1)\n\n    #separating only the possible true positives to check iou\n    testTrue = tf.boolean_mask(true, truePositiveMask)\n    testPred = tf.boolean_mask(pred, truePositiveMask)\n\n    #getting iou and threshold comparisons\n    iou = iou_loss_core(testTrue,testPred) \n    truePositives = [castF(K.greater(iou, tres)) for tres in tresholds]\n\n    #mean of thressholds for true positives and total sum\n    truePositives = K.mean(K.stack(truePositives, axis=-1), axis=-1)\n    truePositives = K.sum(truePositives)\n\n    #to get images that don't have mask in both true and pred\n    trueNegatives = (1-true1) * (1 - pred1) # = 1 -true1 - pred1 + true1*pred1\n    trueNegatives = K.sum(trueNegatives) \n\n    return (truePositives + trueNegatives) \/ castF(K.shape(true)[0])","d61d9d11":"# Create your own Model\n# This is a test model\ndef custom_model():\n    \n    inputs = []\n    x = []\n    for i in range(len(sensors)):\n        inputs.append(Input(shape, batch_size = 1, name = 'CAM_INPUT_' + str(i)))\n\n    for i in range(len(sensors)):\n        y = Conv2D(64, (3, 3))(inputs[i])\n        y = Conv2D(64, (3, 3))(y)\n        y = Dropout(0.05)(y)\n\n        y = Conv2D(128, (3, 3))(y)\n        y = MaxPool2D()(y)\n\n        x.append(y)\n        \n    x = add(x)\n    x = GlobalAveragePooling2D()(x)\n\n    outputs = []\n    for i in range(MAX_VALUE):\n\n        confidence = Dense(1, activation = 'sigmoid', name = 'CONFIDENCE_' + str(i+1))(x)\n        center = Dense(3, activation = 'linear', name = 'CENTER_' + str(i+1))(x)\n        size = Dense(3, activation = 'linear', name = 'SIZE_' + str(i+1))(x)\n        rotation = Dense(4, activation = 'linear', name = 'ROTATION_' + str(i+1))(x)\n        category = Dense(10, activation = 'softmax', name = 'CATEGORY_' + str(i+1))(x)\n\n        output_layer = concatenate([confidence, center, size, rotation, category], name = 'OUTPUT_LAYER_' + str(i))\n        \n        outputs.append(output_layer)\n        \n    model = Model(inputs, outputs)\n    model.compile(loss = LossFunction, optimizer = 'adam', metrics = ['accuracy', IOU_Metric])\n    \n    return model","aa9c3051":"def model_fit(model):\n    \n    for token in train_data['Id']:\n        values = getData(token)\n        values = values.reshape((values.shape[0],) + (1, ) + (values.shape[1], ))\n        filenames = getImageFileNames(token)\n        \n        images = [numpy.asarray(Image.open(i).resize(shape[:-1])).reshape((1, ) + shape) for i in filenames]\n        \n        print(images[0].shape, values.shape)\n        print(\"Training the model . . . . . \")\n        model.fit(images, values.tolist())\n        print(\"Model Trained for token : {}\".format(token))\n        \n        del images, values\n        gc.collect()","1c047c08":"%%time\nmodel = custom_model()\n# print(model.summary())","83f97cd2":"model_fit(model)","06029fda":"# !rm images\n# !rm maps\n# !rm lidar","85e7f68d":"# !ln -s \/kaggle\/input\/3d-object-detection-for-autonomous-vehicles\/test_images images\n# !ln -s \/kaggle\/input\/3d-object-detection-for-autonomous-vehicles\/test_maps maps\n# !ln -s \/kaggle\/input\/3d-object-detection-for-autonomous-vehicles\/test_lidar lidar","631be81e":"# test_lyft_data = LyftDataset(data_path = '.', json_path = PATH + 'test_data', verbose = True)\n# test_data = pandas.read_csv(PATH + 'sample_submission.csv')\n\n# x = test_lyft_data.get('sample_data', test_lyft_data.get('sample', test_data['Id'][0])['data']['CAM_BACK'])\n# y = test_lyft_data.get('calibrated_sensor', x['calibrated_sensor_token'])\n# z = test_lyft_data.get('ego_pose', x['ego_pose_token'])\n\n# y, z, test_lyft_data.get_sample_data(test_lyft_data.get('sample_data', test_lyft_data.get('sample', test_data['Id'][0])['data']['CAM_BACK'])['token'])","03a0bfe5":"# Utility Functions","fdbacebf":"# TODO : Prediction and Testing","cdfa55fc":"# Creating a Model with Custom Metrics and Loss Function","98a7930b":"# Understanding Directories","2579e425":"# Getting the training data and necessary features","cef72366":"# Hyperparameters","f2565270":"# Importing Required Libraries","4e1f2219":"* > **WARNING**\n* > ****Please Change the Loss Function and Metrics and Use your own Custom Model****\n* > ****This is just a Blueprint and will not work if you commit or submit****\n* > ****The time took for the current test model to be created is around 60s and fitting the model I genuinely have no idea, I tried training it for 10 records it took around an hour on GPU. I've changed the code and have no idea if it works.****","a8157270":"# Making Symbolic Links","479dd6e2":"# Installing and Making the LyftDataset"}}