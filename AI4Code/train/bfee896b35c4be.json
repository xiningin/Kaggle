{"cell_type":{"b151ee71":"code","258c447d":"code","d923a8f1":"code","7eb9b19c":"code","36d88139":"code","98e09ec4":"code","2612e7de":"code","38009aeb":"code","8e6836d4":"code","7b06ee71":"code","f22ca3d3":"code","2d2e2fef":"code","d24e15e8":"code","1d5173f0":"code","a0289b9c":"code","e7763511":"code","94fdb710":"code","84af42eb":"code","6d28cc97":"code","8c1d2008":"code","ff96b293":"code","6b0fa53d":"code","9009d461":"code","c6163bb3":"code","eb1aff1d":"code","35495a71":"markdown","49ec108f":"markdown","6dc52b56":"markdown","33d9ae0d":"markdown","21d555fb":"markdown","1081a503":"markdown","64b71ea1":"markdown","1676fcb1":"markdown","80c7281e":"markdown","4439bd3c":"markdown","84881222":"markdown"},"source":{"b151ee71":"pip install xlrd","258c447d":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\nimport nltk\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nfrom tpot import TPOTClassifier\nfrom sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\nfrom sklearn.datasets import load_digits\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.pipeline import Pipeline\nimport cudf\nfrom nltk.stem.snowball import SnowballStemmer\n\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","d923a8f1":"# Load data from excel into a pandas df\ndata_path = '..\/input\/aeronautics-astronautics-journal-abstracts\/aiaa_dataset.xls'\n\ndf = pd.read_excel(data_path) #import data\ncdf = cudf.from_pandas(df)","7eb9b19c":"# Check for objects that are non-null\ncdf.info()","36d88139":"cdf.head()","98e09ec4":"# Preprocess text for NLP\n\nSTOPWORDS = nltk.corpus.stopwords.words('english')\n\nfilters = [ '!', '\"', '#', '$', '%', '&', '(', ')', '*', '+', '-', '.', '\/',  '\\\\', ':', ';', '<', '=', '>',\n           '?', '@', '[', ']', '^', '_', '`', '{', '|', '}', '\\t','\\n',\"'\",\",\",'~' , '\u2014']\n\ndef preprocess_text(input_strs , filters=None , stopwords=STOPWORDS):\n    \"\"\"\n        * filter punctuation\n        * to_lower\n        * remove stop words (from nltk corpus)\n        * remove multiple spaces with one\n        * remove leading spaces    \n    \"\"\"\n    \n    # filter punctuation and case conversion\n    translation_table = {ord(char): ord(' ') for char in filters}\n    input_strs = input_strs.str.translate(translation_table)\n    input_strs = input_strs.str.lower()\n        \n    # remove stopwords\n    stopwords_gpu = cudf.Series(stopwords)\n    input_strs =  input_strs.str.replace_tokens(STOPWORDS, ' ')\n        \n    # replace multiple spaces with single one and strip leading\/trailing spaces\n    input_strs = input_strs.str.normalize_spaces( )\n    input_strs = input_strs.str.strip(' ')\n    \n    return input_strs\n\ndef preprocess_text_df(df, text_cols=['text'], **kwargs):\n    for col in text_cols:\n        df[col] = preprocess_text(df[col], **kwargs)\n    return  df\n\ndf = preprocess_text_df(cdf, text_cols=['abstract','title'],filters=filters).to_pandas()\n\ndf.head()","2612e7de":"title_feature = df['title'].tolist()\n\nabstract_feature = df['abstract'].tolist()\n\ntitle_and_abstract = []\nfor index in range(len(title_feature)):\n    title_and_abstract.append(title_feature[index] + abstract_feature[index])","38009aeb":"# One more sanity check to make sure the size of the lists match\nprint(len(title_feature))\nprint(len(abstract_feature))\nprint(len(title_and_abstract))","8e6836d4":"# hot encode target to binary \ntarget = df['journal']\ntarget = target.map({'JPP':0, 'JTHT':1})","7b06ee71":"target.value_counts()","f22ca3d3":"stemmer = SnowballStemmer(\"english\", ignore_stopwords=True)\n\nclass StemmedCountVectorizer(CountVectorizer):\n    def build_analyzer(self):\n        analyzer = super(StemmedCountVectorizer, self).build_analyzer()\n        return lambda doc: ([stemmer.stem(w) for w in analyzer(doc)])","2d2e2fef":"# Manual clean for TPOT since it doesn't work w\/ pipelines for some reason\n\n#count_vect = CountVectorizer()\ncount_vect = StemmedCountVectorizer(stop_words='english')\ntfidf_transformer = TfidfTransformer()\nclf = MultinomialNB()\nprint(len(abstract_feature))","d24e15e8":"abstract_tpot_counts = count_vect.fit_transform(abstract_feature)\nabstract_tpot_tfidf = tfidf_transformer.fit_transform(abstract_tpot_counts)\nabstract_tpot_tfidf.shape","1d5173f0":"title_tpot_counts = count_vect.fit_transform(title_feature)\ntitle_tpot_tfidf = tfidf_transformer.fit_transform(title_tpot_counts)\ntitle_tpot_tfidf.shape","a0289b9c":"taa_tpot_counts = count_vect.fit_transform(title_and_abstract)\ntaa_tpot_tfidf = tfidf_transformer.fit_transform(taa_tpot_counts)\ntaa_tpot_tfidf.shape","e7763511":"# test\/train split abstract data\nX_abstract_train, X_abstract_test, y_abstract_train, y_abstract_test = train_test_split(abstract_tpot_tfidf,target,test_size=0.1)","94fdb710":"# test\/train split title data\nX_title_train, X_title_test, y_title_train, y_title_test = train_test_split(title_tpot_tfidf,target,train_size=0.7, test_size=0.1)","84af42eb":"# test\/train split title + abstract data\nX_taa_train, X_taa_test, y_taa_train, y_taa_test = train_test_split(taa_tpot_tfidf,target,train_size=0.7, test_size=0.1)","6d28cc97":"# Title: NB Classifier \ntitle_clf = clf.fit(X_title_train, y_title_train)\npredicted= title_clf.predict(X_title_test)\nnb_title_score=np.mean(predicted == y_title_test)\nprint(f\"Accuracy using title only: {round(nb_title_score*100,5)}%\")","8c1d2008":"# Abstract: NB Classifier \nabstract_clf = clf.fit(X_abstract_train, y_abstract_train)\npredicted= abstract_clf.predict(X_abstract_test)\nnb_abstract_score=np.mean(predicted == y_abstract_test)\nprint(f\"Accuracy using abstract only: {round(nb_abstract_score*100,5)}%\")","ff96b293":"# Title + Abstract: NB Classifier \ntaa_clf = clf.fit(X_taa_train, y_taa_train)\npredicted= abstract_clf.predict(X_taa_test)\nscore=np.mean(predicted == y_taa_test)\nprint(f\"Accuracy using title and abstact: {round(score*100,5)}%\")","6b0fa53d":"cuML_sparse = {\n    \n    # cuML + DMLC\/XGBoost Classifiers\n    'cuml.neighbors.KNeighborsClassifier': {\n        'n_neighbors': range(1, 101),\n        'weights': [\"uniform\",],\n    },\n    \n    'cuml.linear_model.LogisticRegression': {\n        'penalty': [\"l1\", \"l2\",\"elasticnet\"],\n        'C': [1e-4, 1e-3, 1e-2, 1e-1, 0.5, 1., 5., 10., 15., 20., 25.],\n    },\n\n    'xgboost.XGBClassifier': {\n        'n_estimators': [100],\n        'max_depth': range(3, 10),\n        'learning_rate': [1e-2, 1e-1, 0.5, 1.],\n        'subsample': np.arange(0.05, 1.01, 0.05),\n        'min_child_weight': range(1, 21),\n        \"alpha\": [1, 10],\n        \"tree_method\": [\"gpu_hist\"],\n        'n_jobs': [1],\n        'verbosity': [0]\n    },\n\n    'cuml.svm.LinearSVC': {\n        'penalty': [\"l1\", \"l2\"],\n        'loss': [\"hinge\", \"squared_hinge\"],\n        'tol': [1e-5, 1e-4, 1e-3, 1e-2, 1e-1],\n        'C': [1e-4, 1e-3, 1e-2, 1e-1, 0.5, 1., 5., 10., 15., 20., 25.]\n    },\n    \n    #sklearn Preprocessors\n\n    \"sklearn.preprocessing.Binarizer\": {\n        \"threshold\": np.arange(0.0, 1.01, 0.05)\n    },\n\n    \"sklearn.decomposition.FastICA\": {\n        \"tol\": np.arange(0.0, 1.01, 0.05)\n    },\n\n    \"sklearn.cluster.FeatureAgglomeration\": {\n        \"linkage\": [\"ward\", \"complete\", \"average\"],\n        \"affinity\": [\"euclidean\", \"l1\", \"l2\", \"manhattan\", \"cosine\"]\n    },\n\n    \"sklearn.preprocessing.MaxAbsScaler\": {\n    },\n\n    \"sklearn.preprocessing.MinMaxScaler\": {\n    },\n\n    \"sklearn.preprocessing.Normalizer\": {\n        \"norm\": [\"l1\", \"l2\", \"max\"]\n    },\n\n    \"sklearn.kernel_approximation.Nystroem\": {\n        \"kernel\": [\"rbf\", \"cosine\", \"chi2\", \"laplacian\", \"polynomial\", \"poly\", \"linear\", \"additive_chi2\", \"sigmoid\"],\n        \"gamma\": np.arange(0.0, 1.01, 0.05),\n        \"n_components\": range(1, 11)\n    },\n\n    \"sklearn.decomposition.PCA\": {\n        \"svd_solver\": [\"randomized\"],\n        \"iterated_power\": range(1, 11)\n    },\n\n    \"sklearn.kernel_approximation.RBFSampler\": {\n        \"gamma\": np.arange(0.0, 1.01, 0.05)\n    },\n\n    \"sklearn.preprocessing.RobustScaler\": {\n    },\n\n    \"sklearn.preprocessing.StandardScaler\": {\n    },\n\n    \"tpot.builtins.ZeroCount\": {\n    },\n        \n    'tpot.builtins.OneHotEncoder': {\n        \"minimum_fraction\": [0.05, 0.1, 0.15, 0.2, 0.25],\n        \"sparse\": [True],\n        \"threshold\": [10]\n    },\n\n    # Selectors\n\n    'sklearn.feature_selection.SelectFwe': {\n        'alpha': np.arange(0, 0.05, 0.001),\n        'score_func': {\n            'sklearn.feature_selection.f_classif': None\n        }\n    },\n\n    'sklearn.feature_selection.SelectPercentile': {\n        'percentile': range(1, 100),\n        'score_func': {\n            'sklearn.feature_selection.f_classif': None\n        }\n    },\n\n    'sklearn.feature_selection.VarianceThreshold': {\n        'threshold': np.arange(0.05, 1.01, 0.05)\n    },\n\n    'sklearn.feature_selection.RFE': {\n        'step': np.arange(0.05, 1.01, 0.05),\n        'estimator': {\n            'sklearn.ensemble.ExtraTreesClassifier': {\n                'n_estimators': [100],\n                'criterion': ['gini', 'entropy'],\n                'max_features': np.arange(0.05, 1.01, 0.05)\n            }\n        }\n    },\n\n    'sklearn.feature_selection.SelectFromModel': {\n        'threshold': np.arange(0, 1.01, 0.05),\n        'estimator': {\n            'sklearn.ensemble.ExtraTreesClassifier': {\n                'n_estimators': [100],\n                'criterion': ['gini', 'entropy'],\n                'max_features': np.arange(0.05, 1.01, 0.05)\n            }\n        }\n    },\n\n}","9009d461":"# Title: TPOT\ntitle_optimizer= TPOTClassifier(generations=20, population_size=100,cv=5, early_stop=20,\n                                   random_state=42, verbosity=2, config_dict='TPOT sparse')\n#title_optimizer= TPOTClassifier(generations=50, population_size=50,cv=5, early_stop=20,\n#                                    random_state=42, verbosity=2, config_dict='TPOT cuML')\ntitle_optimizer.fit(X_title_train, y_title_train)\nprint(title_optimizer.score(X_title_test, y_title_test))\ntitle_optimizer.export('title_exported_pipeline.py')","c6163bb3":"# Abstract: TPOT\nabstract_optimizer= TPOTClassifier(generations=20, population_size=100, cv=5,early_stop=20,\n                                    random_state=42, verbosity=2, config_dict='TPOT sparse')\nabstract_optimizer.fit(X_abstract_train, y_abstract_train)\nprint(abstract_optimizer.score(X_abstract_test, y_abstract_test))\nabstract_optimizer.export('abstract_exported_pipeline.py')","eb1aff1d":"# Title + Abstract: TPOT\ntaa_optimizer= TPOTClassifier(generations=20, population_size=100, cv=5, early_stop=20,\n                                    random_state=42, verbosity=2, config_dict='TPOT sparse')\ntaa_optimizer.fit(X_taa_train, y_taa_train)\nprint(taa_optimizer.score(X_abstract_test, y_abstract_test))\ntaa_optimizer.export('taa_exported_pipeline.py')","35495a71":"# 5. Scoring Model","49ec108f":"# 3. Data Wrangling & Prep","6dc52b56":"## Table of Contents\n\n1. Goal\n2. Load & Inspect Data\n3. Data Wrangling & Prep\n4. Modeling\n5. Scoring Model\n6. Conclusion","33d9ae0d":"We have an almost equal amount of both journals.","21d555fb":"Kaggle doesn't have xlrd nor nltk in it's pre-installed libraries, so we must be install them","1081a503":"# 4. Modeling","64b71ea1":"We can note from the above snapshot of the dataframe that the text hasn't been prepared for NLP, so lets fix that.","1676fcb1":"# 6. Conclusion","80c7281e":"# 1. Goal\n\nThe questions of this study are as follows:\n\n1. With what level of accuracy can we determinine whether a scholarly article belongs to the research field of propulsion or heat transfer? (the two fields share a gread deal of jargon)\n\n    A. *What accuracy results when analyzing only article abstracts?*\n    \n    B. *What accuracy results when analyzyin only article title*\n    \n    C. *What accuracy results when analyzing article title and article abstract, and abstract & title?*\n\n\nAbout the data:\n- title (str): title of research paper\n- abstract (str) : abstract of research paper\n- journal (str) : journal of publication (JPP = Propulsion; JTHT = Thermophysics & Heat Transfer)\n- volume (int):  volume of journal... this wasn't clear, so I assume it to mean the quantity of publications in the journal. Given that I don't understand the implications of this column, it will be ignored.","4439bd3c":"NB Classifier used on title + abstract seems to be *less* accurate than simply using the title. I would infer from this that some of the words in the word frequency matrix are extraneous and are actually detracting from the accuracy. But this is only one method, and one test-- let's see how TPOT performs.","84881222":"# 2. Load & Inspect Data\n\n- Load the data from excel into a pandas dataframe\n- Check data for nulls"}}