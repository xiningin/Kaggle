{"cell_type":{"9068f6f5":"code","5ca3b382":"code","a507cbb9":"code","8f1b75ae":"code","3860edb2":"code","74076714":"code","d035656c":"code","fcd9292d":"code","f75f295f":"code","40033d10":"code","e8326270":"code","fb65ecac":"code","9f87979e":"code","62a92efc":"code","0690d2fe":"code","e98d95f3":"code","e795baae":"code","15cdf3c6":"code","6298d468":"code","5c9179d3":"code","457b1c06":"code","de77d889":"code","e8bbb307":"code","a35a6d51":"code","930171c5":"markdown","d14cc6bc":"markdown","f981a6b1":"markdown","951546d9":"markdown","a67ae947":"markdown","d60db52a":"markdown","4f554037":"markdown","62e466a3":"markdown","6133051e":"markdown","04ee3fd4":"markdown","edd9f85c":"markdown","d3334ebd":"markdown","3cafc551":"markdown","4bd97a26":"markdown","62778e9c":"markdown","6f661474":"markdown","a7b40f5b":"markdown","833dd3cc":"markdown","7d37f264":"markdown","4f62367d":"markdown","db56d2e9":"markdown","41f24706":"markdown","cc69ba04":"markdown","6f13ad6f":"markdown","0037c937":"markdown","4f0c5310":"markdown","5bd6b50f":"markdown","ad94840d":"markdown","c4f07a19":"markdown","62da033d":"markdown","e13ad280":"markdown"},"source":{"9068f6f5":"#importing libraries that are used in this notebook\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom scipy.stats import pearsonr\nfrom sklearn.model_selection import train_test_split as tts\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier as KNN\nfrom sklearn.svm import SVC\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.ensemble import RandomForestClassifier as RFC\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.metrics import confusion_matrix as CM","5ca3b382":"#importing dataset\ndataset = pd.read_csv('\/kaggle\/input\/red-wine-quality-cortez-et-al-2009\/winequality-red.csv')\ndataset.head()","a507cbb9":"dataset.shape","8f1b75ae":"dataset.describe()","3860edb2":"dataset.drop(labels='density', axis=1, inplace=True)","74076714":"dataset.isnull().sum()","d035656c":"#Plotting boxplots to see if there are any outliers in our data (considering data betwen 25th and 75th percentile as non outlier)\nfig, ax = plt.subplots(ncols=5, nrows=2, figsize=(15, 5))\nax = ax.flatten()\nindex = 0\nfor i in dataset.columns:\n  if i != 'quality':\n    sns.boxplot(y=i, data=dataset, ax=ax[index])\n    index +=1\nplt.tight_layout(pad=0.4)\nplt.show()","fcd9292d":"fig, ax = plt.subplots(ncols=5, nrows=2, figsize=(15, 5))\nax = ax.flatten()\nindex=0\nfor i in dataset.columns:\n  if i != 'quality':\n    sns.barplot(x='quality', y=i, data=dataset, ax=ax[index])\n    index+=1\nplt.tight_layout(pad=0.4)\nplt.show","f75f295f":"plt.figure(figsize=(10, 10))\nsns.heatmap(dataset.corr(method='pearson'), annot=True, square=True)\nplt.show()\n\nprint('Correlation of different features of our dataset with quality:')\nfor i in dataset.columns:\n  corr, _ = pearsonr(dataset[i], dataset['quality'])\n  print('%s : %.4f' %(i,corr))","40033d10":"#for a better view this way can be used\nprint('Another (more clear) view of correlations among features:\\n')\ndataset.corr().style.background_gradient(cmap=\"coolwarm\")","e8326270":"#our dataset\ndataset.head()","fb65ecac":"bins = (2, 6.5, 8)\ngroup_names = ['bad', 'good']\ndataset['quality'] = pd.cut(dataset['quality'], bins = bins, labels = group_names)\ndataset.head()","9f87979e":"dataset['quality'] = dataset['quality'].map({'bad' : 0, 'good' : 1})\ndataset.head(10)","62a92efc":"print(dataset['quality'].value_counts())\nfig, ax = plt.subplots(ncols=2, nrows=1, figsize=(15, 5))\nax = ax.flatten()\nprint('\\nVisualisation of accuracies of differnt classification models')\ndataset['quality'].value_counts().plot(x=0, y=1, kind='pie', figsize=(15,5), ax=ax[0])\nsns.countplot(dataset['quality'], ax=ax[1])\nplt.show()","0690d2fe":"X = dataset.iloc[:, :-1]\nY = (dataset.iloc[:, 10])","e98d95f3":"X_train, X_test, Y_train, Y_test = tts(X, Y, test_size=0.20, random_state=0)","e795baae":"from sklearn.preprocessing import StandardScaler as ss\nSS = ss()\nX_train = SS.fit_transform(X_train)\nX_test = SS.transform(X_test)","15cdf3c6":"logisticRegression = LogisticRegression(solver='lbfgs', random_state=0)\nlogisticRegression.fit(X_train, Y_train)\nY_pred_logisticRegression = logisticRegression.predict(X_test)\nY_compare_logisticRegression = pd.DataFrame({'Actual' : Y_test, 'Predicted' : Y_pred_logisticRegression})\nprint(Y_compare_logisticRegression.head())\nprint('\\nConfussion matrix:')\nprint(CM(Y_test, Y_pred_logisticRegression))","6298d468":"knn = KNN(n_neighbors=2, metric='minkowski', p=2,)\nknn.fit(X_train, Y_train)\nY_pred_knn = knn.predict(X_test)\nY_compare_knn = pd.DataFrame({'Actual' : Y_test, 'Predicted' : Y_pred_knn})\nprint(Y_compare_knn.head())\nprint('\\nConfussion matrix:')\nprint(CM(Y_test, Y_pred_knn))","5c9179d3":"svc = SVC(kernel='rbf', gamma='scale', random_state=0)\nsvc.fit(X_train, Y_train)\nY_pred_svc = svc.predict(X_test)\nY_compare_svc = pd.DataFrame({'Actual' : Y_test, 'Predicted' : Y_pred_svc})\nprint(Y_compare_svc.head())\nprint('\\nConfussion matrix:')\nprint(CM(Y_test, Y_pred_svc))","457b1c06":"nb = GaussianNB()\nnb.fit(X_train, Y_train)\nY_pred_nb = nb.predict(X_test)\nY_compare_nb = pd.DataFrame({'Actual' : Y_test, 'Predicted' : Y_pred_nb})\nprint(Y_compare_nb.head())\nprint('\\nConfussion matrix:')\nprint(CM(Y_test, Y_pred_nb))","de77d889":"rfc = RFC(n_estimators=25, criterion='gini', random_state=0,)\nrfc.fit(X_train, Y_train)\nY_pred_rfc = rfc.predict(X_test)\nY_compare_rfc = pd.DataFrame({'Actual' : Y_test, 'Predicted' : Y_pred_rfc})\nprint(Y_compare_rfc.head())\nprint('\\nConfussion matrix:')\nprint(CM(Y_test, Y_pred_rfc))","e8bbb307":"#K-fold cross validation\nmodelNames = ['Logistic Regression', 'K-Nearest Neighbour', 'Support Vector', 'Naive Bayes', 'Random Forrest']\nmodelClassifiers = [logisticRegression, knn, svc, nb, rfc]\nmodels = pd.DataFrame({'modelNames' : modelNames, 'modelClassifiers' : modelClassifiers})\ncounter=0\nscore=[]\nfor i in models['modelClassifiers']:\n  accuracy = cross_val_score(i, X_train, Y_train, scoring='accuracy', cv=10)\n  print('Accuracy of %s Classification model is %.2f' %(models.iloc[counter,0],accuracy.mean()))\n  score.append(accuracy.mean())\n  counter+=1","a35a6d51":"pd.DataFrame({'Model Name' : modelNames,'Score' : score}).sort_values(by='Score', ascending=True).plot(x=0, y=1, kind='bar', figsize=(15,5), title='Comparison of accuracies of differnt classification models')\nplt.show()","930171c5":"# **Viewing and understanding the basic details of our dataset**","d14cc6bc":"From the above scores and visualiations we can conclude that Random Forrest Classification model gives the best score and we can use it to predict the quality of wine for this particular problem.\n\nHowever other models like Logisgic Regression, KNN and SVC also have comparable score to Random Forrest and may also be used to predict quality of wine.","f981a6b1":"Dividing quality of wine in two buckets, ie. Good wine and Bad wine, and on the basis of this we will give our final result.","951546d9":"Support Vector Classification","a67ae947":"This dataset is related to red variants of the Portuguese \"Vinho Verde\" wine. Due to privacy and logistic issues, only physicochemical (inputs) and sensory (the output) variables are available (e.g. there is no data about grape types, wine brand, wine selling price, etc.).\n\nThese datasets can be viewed as classification tasks. The classes are ordered and not balanced (e.g. there are many more normal wines than excellent or poor ones). Outlier detection algorithms could be used to detect the few excellent or poor wines. Also, we are not sure if all input variables are relevant. So it could be interesting to test feature selection methods.\n\nInput variables (based on physicochemical tests):\n1. fixed acidity\n2. volatile acidity\n3. citric acid\n4. residual sugar\n5. chlorides\n6. free sulfur dioxide\n7. total sulfur dioxide\n8. density\n9. pH\n10. sulphates\n11. alcohol\nOutput variable (based on sensory data):\n12. quality (score between 0 and 10)\n\nOur goal is to make a **machine learning** model that can predict the quality of wine based on the input variable (features) given above.","d60db52a":"# **About dataset**","4f554037":"**Please comment your suggestions**\n\n**Please upvote if this notebook is helpful**","62e466a3":"K-Nearest Neighbour Classification","6133051e":"Random Forrest Classification","04ee3fd4":"From above data engineering and machine learning (classification) techniques we can conclude that:\n\n1. We have chosen not to remove outliers and extract the more relavant features form out dataset - as we were looking for accuracy to minute levels, not just some approximation (high quality wine may have very rare composition from other average quality wines)\n2. Random Forrest Classification model gave the best accuracy and can be considered as a good model for predictiong the quality of wine for this problem.\n3. However other models like Logisgic Regression, KNN and SVC also have comparable score to Random Forrest and may also be used to predict quality of wine.\n4. Naive Bayes model gave the least accuracy, which can be considered bad model to predict the quality of fine.\n5. Performance tuning using methods like Grid Search, etc. can be done to improve the accuracy of these models. So, accuracy of these models will improve and we might get another best model for our problem.\n6. We might(will) get different results if we remove outliers and consider feature extraction.\n\nFinally I would like to end this notebook the fact, that no Data Science technique is perfect, there are many other ways\/ models to get better results and there is always scope for imporvements.","edd9f85c":"**From the above visualisation we derieve that:**\n1. Features fixed acidity and residual sugar might not give any specification to classify\/predict the quality.\n2. Quality increases with\n    * decrease in volatile acidity.\n    * increase in citric acid.\n    * decrease in chlorides.\n    * decrease in pH.\n    * increase in sulphates.\n    * increase in alcohol.\n3. Free sulfur dioxide alone will not be able to predict the quality.\n4. Total sulfur dioxide alone will not be able to predict the quality.\n\n**Plotting correlation heatmap to verify the above statements**\n\n\n","d3334ebd":"**First let's prepare our dataset for Machine Learning**","3cafc551":"Plotting bar plots to see relation between each independent feature with dependent feature 'Quality'","4bd97a26":"Let's count and visualise the total number of different wine samples","62778e9c":"Naive Bayes Calssification","6f661474":"# **Checking for missing values**","a7b40f5b":"From above data description we can conclude that value of density is almost similar throughout the dataset, because mean, min, 25%, 50%, 75% are all ~0.99.\n\nSo I am choosing to drop density feature as it will not have much significance in predicting quality of wine.","833dd3cc":"Creating set of independent and dependent features","7d37f264":"# **Checking for outliers in our dataset**","4f62367d":"From the above box plots we can clearly see that there are outliers in all features.\n\n**BUT**\n\nHere I am choosing not remove\/modify outliers as we are looking for accuracy to minute levels, not just some approximation \u2014 high quality wine may have very rare composition (hence outlier) from other average quality wines, so we can not remove or modify outlier values in out dataset.","db56d2e9":"**Checking accuracy of different classification models**","41f24706":"# **Machine learning**\n\nNow implementing classification algorithms based machine learning models and selecting the best out of them based on some score.","cc69ba04":"From the above code have divided the quality of wine in two buckets:\n* Bad wine : range 2 - 6.5\n* Good wine : range 6.5 - 8\n\nThis can be changed as per the requiremnt of our client.\n\nNow we will map the values of bad and good to 0 and 1 respectively, as machine learning models can perform calculation only on numerical data.","6f13ad6f":"Feature scaling, but not scaling dependent variable as it has catagorical data","0037c937":"Logistic Regression Classification","4f0c5310":"# **Final Summary**","5bd6b50f":"Creating traing and test set","ad94840d":"**Now implementing machine learning models**","c4f07a19":"# **Feature Extraction**","62da033d":"There are no missing values in our dataset","e13ad280":"**From the above plots and values we can conclude:**\n1. volatile acidity, chlorides and ph are negatively correlated to quality -- hence our statement was right that quality increases with decrease in value of these features; and vice versa for other features.\n2. free sulfur dioxide and total sulfur dioxide are highly correlated to each other with correlation of 0.67.\n3. There are many features with correlation < 0.5 to quality, and may be removed from the dataset.\n\nBUT for the same reason as mentioned above in outlier section, that -- we are looking for accuracy to minute levels, not just some approximation \u2014 high quality wine may have very rare composition from other average quality wines, hence we need to take every feature in account while predicting quality of wine, so we can not remove or modify outlier values in out dataset."}}