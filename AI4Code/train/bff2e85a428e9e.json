{"cell_type":{"0b8b4bb8":"code","a3869143":"code","4edc7f64":"code","caf4c447":"code","5bbe2a81":"code","0bbdb477":"code","9c377d19":"code","26763b7f":"code","ac966230":"code","a6bbcec5":"code","5fee5d23":"code","2275faed":"code","d1fc24c1":"code","2711b386":"code","21ca72c1":"code","33222fa3":"code","3f211312":"code","9937bc6c":"code","5878661d":"code","a4406959":"code","11974117":"code","b53e5970":"code","7ff878b0":"code","23fb060e":"code","94f26421":"code","dafaac33":"code","2dcd9fb4":"code","2ff25c10":"code","33822837":"code","ac79c8c6":"code","98b85103":"code","59da8275":"code","a19719c3":"code","79d08305":"markdown","be11cac9":"markdown","99837faa":"markdown","eedc665a":"markdown","573eab1c":"markdown","4807e2b6":"markdown","da574ad8":"markdown"},"source":{"0b8b4bb8":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport seaborn as sns\nimport matplotlib\nfrom matplotlib import pyplot as plt\n%matplotlib inline\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.datasets import make_classification\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","a3869143":"train_data = pd.read_csv(\"..\/input\/train.csv\")\ntest_data = pd.read_csv(\"..\/input\/test.csv\")\n","4edc7f64":"train_data.head()","caf4c447":"test_data.head()","5bbe2a81":"#plt.figure(figsize=(100, 100))\ntrain_crr=train_data.copy()\ntrain_crr.drop(['ID_code', 'target'],axis=1, inplace=True)\ncorr = train_crr.apply(lambda x: pd.factorize(x)[0]).corr()\n#ax = sns.heatmap(corr, xticklabels=corr.columns, yticklabels=corr.columns, linewidths=.2, cmap=\"YlGnBu\")\nprint(corr)\ncorr.to_csv(\"corr.csv\")","0bbdb477":"#print(\"Correlation Matrix\")\n#print(train_crr.corr())\n#print()\n\ndef get_redundant_pairs(df):\n    '''Get diagonal and lower triangular pairs of correlation matrix'''\n    pairs_to_drop = set()\n    cols = df.columns\n    for i in range(0, df.shape[1]):\n        for j in range(0, i+1):\n            pairs_to_drop.add((cols[i], cols[j]))\n    return pairs_to_drop\n\ndef get_top_abs_correlations(df, n=5):\n    au_corr = df.corr().abs().unstack()\n    labels_to_drop = get_redundant_pairs(df)\n    au_corr = au_corr.drop(labels=labels_to_drop).sort_values(ascending=False)\n    return au_corr[0:n]\n\nprint(\"Top Absolute Correlations\")\nprint(get_top_abs_correlations(train_crr, 20))\n#all_crr=get_top_abs_correlations(train_crr, 200)\n#all_crr.to_csv(\"all_crr.csv\")","9c377d19":"train_data.shape\n#(200000, 202)\n\n\nsample_data=train_data.sample(5000)\nsample_data=sample_data.drop([\"ID_code\"], axis=1)\n\nsample_x = sample_data.loc[:, ~sample_data.columns.isin(['target'])]\nsample_y = sample_data.loc[:,'target']","26763b7f":"sample_x.head()","ac966230":"sample_y.head()","a6bbcec5":"# Import train_test_split function\nfrom sklearn.model_selection import train_test_split\n\n# Split dataset into training set and test set\nX_train, X_test, y_train, y_test = train_test_split(sample_x, sample_y, test_size=0.3) # 70% training and 30% test\n\n#Import Random Forest Model\nfrom sklearn.ensemble import RandomForestClassifier\n\n#Create a Gaussian Classifier\nclf=RandomForestClassifier(n_estimators=100)\n\n#Train the model using the training sets y_pred=clf.predict(X_test)\nclf.fit(X_train,y_train)\n\ny_pred=clf.predict(X_test)\n\n#Import scikit-learn metrics module for accuracy calculation\nfrom sklearn import metrics\n\n# Model Accuracy, how often is the classifier correct?\nprint(\"Accuracy:\",metrics.accuracy_score(y_test, y_pred))\n\n# Model F1 Score\nprint(\"F Score:\" , metrics.fbeta_score(y_test, y_pred, beta=0.5))","5fee5d23":"feature_imp = pd.Series(clf.feature_importances_,index=sample_x.columns).sort_values(ascending=False)\nfeature_imp","2275faed":"df=train_data.copy()","d1fc24c1":"def kdeplot(feature):\n    plt.figure(figsize=(9, 4))\n    plt.title(\"KDE for {}\".format(feature))\n    ax0 = sns.kdeplot(df[df['target'] == 0][feature].dropna(), color= 'navy', label= 'target: 0')\n    ax1 = sns.kdeplot(df[df['target'] == 1][feature].dropna(), color= 'orange', label= 'target: 1')","2711b386":"kdeplot('var_147')","21ca72c1":"def categorical_summarized(dataframe, x=None, y=None, hue=None, palette='Set1', verbose=True):\n    '''\n    Helper function that gives a quick summary of a given column of categorical data\n\n    Arguments\n    =========\n    dataframe: pandas dataframe\n    x: str. horizontal axis to plot the labels of categorical data, y would be the count\n    y: str. vertical axis to plot the labels of categorical data, x would be the count\n    hue: str. if you want to compare it another variable (usually the target variable)\n    palette: array-like. Colour of the plot\n\n    Returns\n    =======\n    Quick Stats of the data and also the count plot\n    '''\n    if x == None:\n        column_interested = y\n    else:\n        column_interested = x\n    series = dataframe[column_interested]\n    print(series.describe())\n    print('mode: ', series.mode())\n    if verbose:\n        print('='*80)\n        print(series.value_counts())\n\n    #sns.countplot(x=x, y=y, hue=hue, data=dataframe, palette=palette)\n    #plt.show()","33222fa3":"# Target Variable: Survival\nc_palette = ['tab:blue', 'tab:orange']\ncategorical_summarized(df, y = 'var_147', palette=c_palette)","3f211312":"def quantitative_summarized(dataframe, x=None, y=None, hue=None, palette='Set1', ax=None, verbose=True, swarm=False):\n    '''\n    Helper function that gives a quick summary of quantattive data\n\n    Arguments\n    =========\n    dataframe: pandas dataframe\n    x: str. horizontal axis to plot the labels of categorical data (usually the target variable)\n    y: str. vertical axis to plot the quantitative data\n    hue: str. if you want to compare it another categorical variable (usually the target variable if x is another variable)\n    palette: array-like. Colour of the plot\n    swarm: if swarm is set to True, a swarm plot would be overlayed\n\n    Returns\n    =======\n    Quick Stats of the data and also the box plot of the distribution\n    '''\n    series = dataframe[y]\n    print(series.describe())\n    print('mode: ', series.mode())\n    if verbose:\n        print('='*80)\n        print(series.value_counts())\n\n    sns.boxplot(x=x, y=y, hue=hue, data=dataframe, palette=palette, ax=ax)\n\n    if swarm:\n        sns.swarmplot(x=x, y=y, hue=hue, data=dataframe,\n                      palette=palette, ax=ax)\n\n    plt.show()","9937bc6c":"# univariate analysis\nc_palette = ['tab:blue', 'tab:orange']\nquantitative_summarized(dataframe= df, y = 'var_147', palette=c_palette, verbose=False, swarm=False)\n","5878661d":"df = train_data.copy()","a4406959":"df = df.drop([\"ID_code\"],axis=1)","11974117":"df.head()","b53e5970":"df.iloc[:,0:10].hist(figsize=(15, 15), bins=40, xlabelsize=8, ylabelsize=8); # ; avoid having the matplotlib verbose informations","7ff878b0":"df_new_1 = df[df.target==1]\n#Multiply by some constant except the target variable\ndf_new_1.iloc[:,0:10].hist(figsize=(15, 15), bins=40, xlabelsize=8, ylabelsize=8);","23fb060e":"df_new_0 = df[df.target==0]\n#Multiply by some constant except the target variable\ndf_new_0.iloc[:,0:10].hist(figsize=(15, 15), bins=40, xlabelsize=8, ylabelsize=8);","94f26421":"\ndf_corr = df.corr()['target'][:-1] # -1 because the latest row is target\ngolden_features_list = df_corr[abs(df_corr) > 0.1].sort_values(ascending=False)\nprint(\"There is {} strongly correlated values with target:\\n{}\".format(len(golden_features_list), golden_features_list))","dafaac33":"for i in range(0, 5 , 5):\n    sns.pairplot(data=df,\n                x_vars=df.columns[i:i+5],\n                y_vars=['target'])","2dcd9fb4":"for i in range(5, 14 , 5):\n    sns.pairplot(data=df,\n                x_vars=df.columns[i:i+5],\n                y_vars=['target'])","2ff25c10":"plt.figure(figsize = (5, 5))\nax = sns.boxplot(y='var_0', x='target', data=df)\nplt.setp(ax.artists, alpha=.5, linewidth=2, edgecolor=\"k\")\nplt.xticks(rotation=45)","33822837":"plt.close()\nplt.figure(figsize = (5, 5))\nax = sns.boxplot(y='var_10', x='target', data=df)\nplt.setp(ax.artists, alpha=.5, linewidth=2, edgecolor=\"k\")","ac79c8c6":"# Detect outliers from IQR\nQ1 = df.quantile(0.25)\nQ3 = df.quantile(0.75)\nIQR = Q3 - Q1\nprint(IQR)","98b85103":"print((df < (Q1 - 1.5 * IQR)) |(df > (Q3 + 1.5 * IQR)))","59da8275":"print(\"df.shape:\",df.shape)\ndf_out = df[~((df < (Q1 - 1.5 * IQR)) |(df > (Q3 + 1.5 * IQR))).any(axis=1)]\nprint(\"df_out.shape:\",df_out.shape)","a19719c3":"tdata = train_data.copy()\n\ntdata = tdata.loc[tdata.index & df_out.index]\ntdata = tdata.loc[np.intersect1d(tdata.index, df_out.index)]\ntdata = tdata.loc[tdata.index.intersection(df_out.index)]\n\nprint(\"tdata.shape:\",tdata.shape)\n\ntdata.head()\n\ntdata.to_csv(\"train_data_new.csv\")","79d08305":"Lets take the raw data and run a random forest model to check feature importance. This will give us an order in which we can explore the variables. We will take a sample of 5000 rows and remove ID column","be11cac9":"As per above results, we will explore variable 147 first","99837faa":"************************ NEW CODE ****************************************","eedc665a":"Taking helper function from https:\/\/towardsdatascience.com\/a-starter-pack-to-exploratory-data-analysis-with-python-pandas-seaborn-and-scikit-learn-a77889485baf","573eab1c":"There is not a significant differnece between target values for var_147","4807e2b6":"* Huge difference between minimum and maximum\n* large proportion of values are negative","da574ad8":"1. ID_code is unique Identifier\n2. Target is binary\n3. Variables from 0 to 199"}}