{"cell_type":{"b47ced2e":"code","3163cba0":"code","79219860":"code","b3ac9170":"code","9f71411a":"code","8836118d":"code","1f011e36":"code","18f3a72e":"code","46029330":"code","eb893a72":"code","f25fd0df":"code","1d896e7e":"code","5a88155c":"code","0d8ad524":"code","bc29875a":"code","11a008a3":"code","216881bb":"code","adba1274":"code","38a47149":"code","20841498":"code","4cfffea3":"code","14da0b58":"code","69c1910c":"code","d8a1f022":"code","7ca19e63":"code","781af8b5":"code","70f5a5eb":"code","94949751":"code","5ccfa44e":"code","5f5fe331":"code","40a84041":"code","e9b37513":"code","ded54d07":"code","6b9c86b8":"markdown","b7d91369":"markdown","2883dc2c":"markdown","321c37a5":"markdown","93ccd400":"markdown","5c2acd7e":"markdown","14d216d1":"markdown","95553560":"markdown","1f4a0fad":"markdown","1b61611b":"markdown","230b57b5":"markdown"},"source":{"b47ced2e":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport scipy\nfrom sklearn import ensemble\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import classification_report\nfrom sklearn import preprocessing\nfrom sklearn.model_selection import cross_val_score\nfrom imblearn.over_sampling import SMOTE\n%matplotlib inline","3163cba0":"fraud = pd.read_csv('..\/input\/creditcard.csv')","79219860":"fraud.shape","b3ac9170":"fraud.columns","9f71411a":"fraud.describe()","8836118d":"#Determine missing values across dataframe\nfraud.info()","1f011e36":"#First look at Time\nsns.distplot(fraud.Time)\nplt.title('Distribution of Time')\nplt.show()","18f3a72e":"#Now look at Amount\nsns.boxplot(x=fraud['Amount'])\nplt.title('Distribution of Amount')\nplt.show()","46029330":"fraud_total = fraud['Class'].sum()\nprint('Percent Fraud: ' + str(round((fraud_total\/fraud.shape[0])*100, 2)) + '%')","eb893a72":"plt.figure(figsize=(6,3))\n\nplt.subplot(1,2,1)\nplt.title(\"Time < 100,000\")\nfraud[fraud['Time']<100000]['Time'].hist()\n\n\nplt.subplot(1,2,2)\nplt.title(\"Time >= 100,000\")\nfraud[fraud['Time']>=100000]['Time'].hist()\n\nplt.tight_layout()\nplt.show()","f25fd0df":"fraud['10k_time'] = np.where(fraud.Time<100000, 1,0)","1d896e7e":"features = fraud.drop(['Time'], 1)","5a88155c":"# look at distribution below $5.00\nnp.log10(features[features.Amount < 5]['Amount']+1).hist()","0d8ad524":"# how many frauds are actually 0 dollars?\nprint(\"Non-Fraud Zero dollar Transactions:\")\ndisplay(features[(features.Amount == 0) & (features.Class == 0)]['Class'].count())\nprint(\"Fraudulent Zero dollar Transactions:\")\ndisplay(features[(features.Amount == 0) & (features.Class == 1)]['Class'].count())","bc29875a":"features = fraud[fraud.Amount > 0]","11a008a3":"features.head()","216881bb":"features.Amount.quantile(.99)","adba1274":"display(features[(features.Amount < 1000) & (features.Class == 1)]['Class'].count()\/features[features.Class==1].shape[0])","38a47149":"# set features equal to purchases less than $1000\n\n# features = features[features.Amount<1000]","20841498":"sns.distplot(np.log10(features['Amount'][features.Amount>=2]))","4cfffea3":"# create feature for > $2.00 transaction\nfeatures['dollar2'] = np.where(features.Amount > 2, 1, 0)\n# features['dollar_1000'] = np.where(features.Amount > 1000, 1, 0)","14da0b58":"features = features.drop(['Amount'], 1)","69c1910c":"features.head()","d8a1f022":"# Set X and y for model\nX = features.drop(['Class'], 1)\ny = features['Class']\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=0)","7ca19e63":"# try logistic regression model\n\nlr = LogisticRegression(penalty='l2', solver='liblinear')\n\n# Fit the model.\nlr.fit(X_train, y_train)","781af8b5":"print(lr.score(X_test, y_test))\n# display(cross_val_score(lr, X, y, cv=5))","70f5a5eb":"y_pred = lr.predict(X)\nprint(classification_report(y, y_pred))\ndisplay(pd.crosstab(y_pred, y))","94949751":"# Create smote object\nsm = SMOTE(random_state=0)\n\n# Create resampled data\nX_res, y_res = sm.fit_resample(X, y)","5ccfa44e":"# create new training and test set\nX_train, X_test, y_train, y_test = train_test_split(X_res, y_res, test_size=0.2, random_state=0)","5f5fe331":"# fit model\nlr.fit(X_train, y_train)","40a84041":"print(lr.score(X_test, y_test))\n# display(cross_val_score(lr, X_res, y_res, cv=5))\n\ny_pred = lr.predict(X)\nprint(pd.crosstab(y_pred, y))\n\nprint(classification_report(y, y_pred))","e9b37513":"#Set up function to run our model with different trees, criterion, max features and max depth\nrfc = ensemble.RandomForestClassifier(random_state=0, n_estimators=100, n_jobs=-1)\n%time rfc.fit(X_train, y_train)\nprint('\\n Percentage accuracy for Random Forest Classifier')\nprint(rfc.score(X_test, y_test)*100, '%')\n# print(cross_val_score(rfc, X, Y, cv=5))\n\n# display(cross_val_score(rfc, X_res, y_res, cv=5))","ded54d07":"y_pred = rfc.predict(X)\nprint(pd.crosstab(y_pred, y))\n\nprint(classification_report(y, y_pred))","6b9c86b8":">## What percentage of fraud is less than the 99th percentile for purchase amount?","b7d91369":"## Set baseline accuracy with Logistic Regression","2883dc2c":"### Dataset Overview\nThe dataset contains transactions made by credit cards in September of 2013 by European cardholders. This dataset presents transactions that occurred within a two day period. A link to the dataset can be found below:\n\nhttps:\/\/www.kaggle.com\/mlg-ulb\/creditcardfraud\n\nThe dataset contains **284807 observations across 31 columns**. All columns, with the exception of 'Time', 'Amount', and 'Class' have been censored due to confidentiality issues.","321c37a5":">## Is it safe to drop from the data?\n     -maybe?","93ccd400":"### Read the data in and get a feel for its statistical properties","5c2acd7e":">## Approximately 98% of all fraudulent transactions are less than $1000","14d216d1":"## Look at distribution of time greater than 100,000 seconds","95553560":"## We can see a great improvement in recall after oversampling, which may be more important to this model because false positives aren't as important as true negatives.\nbut can we do better?\n- yes","1f4a0fad":"# Here we have 97% precision and 100% recall on the original data, using a model trained on the oversampled data.\n\n    Final overall accuracy is 99.99%","1b61611b":"### Exploratory Analysis","230b57b5":">## Try using SMOTE to improve precision and recall"}}