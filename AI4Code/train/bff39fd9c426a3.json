{"cell_type":{"062e654e":"code","fe183380":"code","29d1958f":"code","0a9ab5db":"code","aa7d0dd8":"code","ee89f32f":"code","4d01006e":"code","2d68a2a1":"code","e0958e34":"code","cf1821d0":"code","622b5c32":"code","cc428c2c":"code","e226843b":"code","d00c15b5":"markdown","6828fffe":"markdown","2a235fa8":"markdown","77cca424":"markdown","b479f26d":"markdown","a27f10d2":"markdown","f0e05559":"markdown","d51ddd55":"markdown","cfe9fc83":"markdown","dcfd60ca":"markdown","4f880122":"markdown","1c81c400":"markdown","e0c0f7d4":"markdown","35abc778":"markdown","8e1d1cd1":"markdown"},"source":{"062e654e":"# Seed value\n# Apparently you may use different seed values at each stage\nseed_value= 32\n\n# 1. Set the `PYTHONHASHSEED` environment variable at a fixed value\nimport os\nos.environ['PYTHONHASHSEED']=str(seed_value)\n\n# 2. Set the `python` built-in pseudo-random generator at a fixed value\nimport random\nrandom.seed(seed_value)\n\n# 3. Set the `numpy` pseudo-random generator at a fixed value\nimport numpy as np\nnp.random.seed(seed_value)\n\n# 4. Set the `tensorflow` pseudo-random generator at a fixed value\nimport tensorflow as tf\ntf.set_random_seed(seed_value)\n\n# 5. Configure a new global `tensorflow` session\nfrom keras import backend as K\nsession_conf = tf.ConfigProto(intra_op_parallelism_threads=1, inter_op_parallelism_threads=1)\nsess = tf.Session(graph=tf.get_default_graph(), config=session_conf)\nK.set_session(sess)","fe183380":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport tensorflow as tf\n\nimport os\nprint(os.listdir(\"..\/input\"))\n%matplotlib inline\nplt.rcParams['figure.figsize'] = (10.0, 8.0) # set default size of plots\nplt.rcParams['image.interpolation'] = 'nearest'\nplt.rcParams['image.cmap'] = 'gray'\n","29d1958f":"from tensorflow.python.keras.utils.np_utils import to_categorical\nfrom sklearn.model_selection import train_test_split\n\n# Import Data\ntrain = pd.read_csv(\"..\/input\/train.csv\")\ntest= pd.read_csv(\"..\/input\/test.csv\")\nprint(\"Train size:{}\\nTest size:{}\".format(train.shape, test.shape))\n\n# Transform Train and Test into images\\labels.\nx_train = train.drop(['label'], axis=1).values.astype('float32') # all pixel values\ny_train = train['label'].values.astype('int32') # only labels i.e targets digits\nx_test = test.values.astype('float32')\nx_train = x_train.reshape(x_train.shape[0], 28, 28) \/ 255.0\nx_test = x_test.reshape(x_test.shape[0], 28, 28) \/ 255.0\n\nx_train, x_val, y_train, y_val = train_test_split(x_train, y_train, test_size = 0.02, random_state=32)\n\nprint(x_train.shape)\nprint(x_val.shape)\nprint(y_train.shape)\nprint(x_test.shape)","0a9ab5db":"# classes for title\n# num classes for amount of examples\nclasses = ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9']\nprint(x_train.shape)\nnum_classes = len(classes)\nsamples_per_class = 7\nplt.figure(0)\nfor y, cls in enumerate(classes):\n    idxs = np.flatnonzero(y_train == y)\n    idxs = np.random.choice(idxs, samples_per_class, replace=False)\n    for i, idx in enumerate(idxs):\n        plt_idx = i * num_classes + y + 1\n        plt.subplot(samples_per_class, num_classes, plt_idx)\n        # plt.imshow(x_train[idx].astype('uint8'))\n        plt.imshow(x_train[idx])\n        plt.axis('off')\n        if i == 0:\n            plt.title(cls)\nplt.show()","aa7d0dd8":"x_train = x_train.reshape(x_train.shape[0], 28, 28,1)  \nx_val = x_val.reshape(x_val.shape[0], 28, 28,1)  \nx_test = x_test.reshape(x_test.shape[0], 28, 28,1) \nprint(\"Train size:{}\\nvalidation size:{}\\nTest size:{}\".format(x_train.shape,x_val.shape, x_test.shape))\n\nmean_px = x_train.mean().astype(np.float32)\nstd_px = x_train.std().astype(np.float32)\n","ee89f32f":"from tensorflow.python.keras.layers import Input , Dense , Conv2D , Activation , Add,ReLU,MaxPool2D,Flatten,Dropout,BatchNormalization\nfrom tensorflow.python.keras.models import Model\n\n\n\ninput = Input(shape=[28, 28, 1])\nx = Conv2D(64, (5, 5), strides=1, padding='same')(input)\nx = Activation('relu')(x)\nx = Conv2D(64, (5, 5), strides=1, padding='same')(x)\nx = Activation('relu')(x)\nx = MaxPool2D(pool_size=2, strides=2, padding='same')(x)\nx = Dropout (0.35)(x)\n\nx = Conv2D(128, (3, 3), strides=1, padding='same')(x)\nx = Activation('relu')(x)\nx = Conv2D(128, (3, 3), strides=1, padding='same')(x)\nx = Activation('relu')(x)\nx = Conv2D(64, (3, 3), strides=1, padding='same')(x)\nx = Activation('relu')(x)\n\nx = MaxPool2D(pool_size=2, strides=2, padding='same')(x)\nx = Dropout (0.35)(x)\nx = Flatten()(x)\nx = Dense(256)(x)\nx = Activation('relu')(x)\nx = Dense(128)(x)\nx = Activation('relu')(x)\nx = BatchNormalization()(x)\nx = Dense(10)(x)\nx = Activation('softmax')(x)\n\nmodel = Model(inputs = input, outputs =x)\nprint(model.summary())","4d01006e":"from tensorflow.python.keras.preprocessing.image import ImageDataGenerator\nfrom tensorflow.python.keras.callbacks import ReduceLROnPlateau, ModelCheckpoint\nfrom tensorflow.python.keras.optimizers import Adam ,RMSprop\n\n\n# optimizer = RMSprop(lr=0.001, rho=0.95, epsilon=1e-08, decay=0.0)\nepochs = 70\nLearning_rate = 0.001\ndecay= 5 * Learning_rate \/ epochs\n# optimizer = Adam(lr=Learning_rate, decay= 3 * Learning_rate \/ epochs)\noptimizer = RMSprop(lr=Learning_rate, rho=0.9, epsilon=1e-08, decay= decay)\nmodel.compile(optimizer=optimizer,\n              loss='sparse_categorical_crossentropy',\n              metrics=['accuracy'])\n\n#               loss='sparse_categorical_crossentropy',\n\n# Set a learning rate annealer\nlearning_rate_reduction = ReduceLROnPlateau(monitor='val_acc', \n                                            patience=4, \n                                            verbose=0, \n                                            factor=0.5, \n                                            min_lr=0.00001)\n\n# Data augmentation\naug_num = 16\ndatagen = ImageDataGenerator(\n        featurewise_center=False,  # set input mean to 0 over the dataset\n        samplewise_center=False,  # set each sample mean to 0\n        featurewise_std_normalization=False,  # divide inputs by std of the dataset\n        samplewise_std_normalization=False,  # divide each input by its std\n        zca_whitening=False,  # apply ZCA whitening\n        rotation_range= aug_num,  # randomly rotate images in the range (degrees, 0 to 180)\n        zoom_range = aug_num \/ 100, # Randomly zoom image \n        width_shift_range= aug_num \/ 100,  # randomly shift images horizontally (fraction of total width)\n        height_shift_range= aug_num \/ 100,  # randomly shift images vertically (fraction of total height)\n        horizontal_flip=False,  # randomly flip images horizontally\n        vertical_flip=False)  # randomly flip images vertically\n\n\ndatagen.fit(x_train)\nbatch_size = 64\n# batch_size = 256\n# Max value lr_min = 0.000125\ncheckpoint = ModelCheckpoint(\"best_weights.hdf5\", monitor='val_acc', verbose=1, save_best_only=True, mode='max')\nhistory = model.fit_generator(datagen.flow(x_train,y_train, batch_size=batch_size),\n                              epochs = epochs, validation_data = (x_val,y_val),\n#                               verbose = 0, steps_per_epoch=x_train.shape[0] \/\/ batch_size,callbacks=[checkpoint,learning_rate_reduction])\n                                verbose = 2, steps_per_epoch=x_train.shape[0] \/\/ batch_size,callbacks=[learning_rate_reduction])\n\n# model.load_weights(\"best_weights.hdf5\") \n","2d68a2a1":"plt.figure(1)\nplt.plot(history.history['loss'])\nplt.plot(history.history['val_loss'])\nplt.title('Model Complexity Graph:  Training vs. Validation Loss')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.legend(['train', 'validate'], loc='upper right')\n\nplt.figure(2)\nplt.plot(history.history['acc'])\nplt.plot(history.history['val_acc'])\nplt.title('Model Accuracy Graph:  Training vs. Validation accuracy')\nplt.ylabel('accuracy')\nplt.xlabel('epoch')\nplt.legend(['train', 'validate'], loc='upper right')\nplt.show()\n\n","e0958e34":"# predicted class\nnum_rows = 5\nnum_cols = 14\nsample_size = num_rows * num_cols\nindices = np.arange(sample_size)\nx_pred = x_test[indices,:,:]\npredictions = model.predict(x_pred)\nx_pred = np.squeeze(x_test[indices,:,:])\ny_pred = np.argmax(predictions,axis=1)\n\nnum_images = num_rows*num_cols\nplt.figure(figsize=(num_cols*2, num_rows*2))\nplt.subplots_adjust(left=None, bottom=None, right=None, top=None, wspace=None, hspace=0.6)\nfor i in range(num_images):\n  plt.subplot(num_rows, num_cols, i+1)\n  plt.imshow(x_pred[i])\n  plt.title(classes[y_pred[i]])\n\n\n\nplt.show()","cf1821d0":"from sklearn.metrics import confusion_matrix\n\ny_vecs = model.predict(x_val)\ny_pred = np.argmax(y_vecs, axis=1)\ny_true = y_val\ncm = confusion_matrix(y_true, y_pred)\n# print(cm)\n\n# plt.imshow(cm, cmap = 'ocean')\n# plt.colorbar\n\nmin_val, max_val = 0, 15\nplt.figure(11)\nfig, ax = plt.subplots()\nax.matshow(cm, cmap=plt.cm.Blues)\n\nfor i in range(10):\n    for j in range(10):\n        c = cm[j,i]\n        ax.text(i, j, str(c), va='center', ha='center')\n\n\nplt.xticks(range(10))\nplt.yticks(range(10))\nplt.title('Confusion matrix',size = 28)\nplt.xlabel('True labeling',size = 20)\nplt.ylabel('Predicted labeling',size = 20)\nplt.rcParams.update({'font.size': 22})\n\n","622b5c32":"# Display some error results \n# y_vecs = model.predict(x_test)\n# y_pred = np.argmax(y_vecs, axis=1)\nY_true = y_val\nY_pred_classes =  y_pred\nY_pred = y_vecs\nX_val = x_val\n# Errors are difference between predicted labels and true labels\nerrors = (Y_pred_classes - Y_true != 0)\n\nY_pred_classes_errors = Y_pred_classes[errors]\nY_pred_errors = Y_pred[errors]\nY_true_errors = Y_true[errors]\nX_val_errors = X_val[errors]\n\ndef display_errors(errors_index,img_errors,pred_errors, obs_errors):\n    \"\"\" This function shows 6 images with their predicted and real labels\"\"\"\n    n = 0\n    nrows = 2\n    ncols = 2\n    fig, ax = plt.subplots(nrows,ncols,sharex=True,sharey=True)\n    plt.subplots_adjust(left=None, bottom=None, right=None, top=None, wspace=None, hspace=1)\n    plt.figure(figsize=(num_cols, num_rows))\n\n    for row in range(nrows):\n        for col in range(ncols):\n            error = errors_index[n]\n            ax[row,col].imshow((img_errors[error]).reshape((28,28)))\n            ax[row,col].set_title(\"Predicted  :{}\\nTrue  :{}\".format(pred_errors[error],obs_errors[error]), fontsize=14)\n            n += 1\n\n# Probabilities of the wrong predicted numbers\nY_pred_errors_prob = np.max(Y_pred_errors,axis = 1)\n\n# Predicted probabilities of the true values in the error set\ntrue_prob_errors = np.diagonal(np.take(Y_pred_errors, Y_true_errors, axis=1))\n\n# Difference between the probability of the predicted label and the true label\ndelta_pred_true_errors = Y_pred_errors_prob - true_prob_errors\n\n# Sorted list of the delta prob errors\nsorted_dela_errors = np.argsort(delta_pred_true_errors)\n\n# Top 6 errors \nmost_important_errors = sorted_dela_errors[-25:]\n\n# Show the top 6 errors\n# display_errors(most_important_errors, X_val_errors, Y_pred_classes_errors, Y_true_errors)","cc428c2c":"# predict results\nresults = model.predict(x_test)\n# select the indix with the maximum probability\nresults = np.argmax(results,axis = 1)\nresults = pd.Series(results,name=\"Label\")\nsubmission = pd.concat([pd.Series(range(1,28001),name = \"ImageId\"),results],axis = 1)\nsubmission.to_csv(\"MNIST-CNN-ENSEMBLE.csv\",index=False)","e226843b":"# model.save('gdrive\/My Drive\/DL-ML\/mnist\/2203 995 percent.h5')","d00c15b5":"## Save model","6828fffe":"## Option 1:\nA good architecture (~99.96) network \n<brr>Dropout for avoiding overfitting\n","2a235fa8":"## Plotting prediction\nStright forward taking some images and plotting predictions","77cca424":"## Starting with Kaggle - imporing data \n\nImportant: Switch settings to GPU","b479f26d":"## Miss-labeled data","a27f10d2":"## Train the model using data augmentation","f0e05559":"## Importing the data the data","d51ddd55":"## Defining the architecture","cfe9fc83":"## Submission","dcfd60ca":"# Introduction to Computer Vision: MNIST Challenge\nHi!\n<brr>\n0.99742 on version 84\n<brr>\nThis is a following kernel  my first kernel: https:\/\/www.kaggle.com\/shaygu\/fast-cnn-for-beginners-0-9923\nI tried to push a little bit to get better results. \n\n<brr>\nTo run the script you can choose between running single neural net or ensamble of them. Currently the ensamble doesnt give better results, I suspect becaues there isnt enough variance between models. \n<brr>\nAnd architecture was inspired by: https:\/\/www.kaggle.com\/josh24990\/recognising-digits-with-keras-top-8-score\n\nFew conclusions from this attempt:\n\nFrom observing the predicted data and reading other discussions\\kernels i've learned that some of the data isnt labeled currectly (confusion between labels 1-7, 9-4 or 3-8) so a result of 99.97-99.98 is probably the limit accuracy. The limit can be pushed using the complete data set which is available.\n\nIn my solution I use a lot of parameters (~1 million) and heavy regularization (Dropout,augmantation) one can probably come up with a better solution using less parameters and less regularization. I think its reasonable that a function that takes 28^2 positive values (pixels) and classifies it to 10 classes needs less parameters than that. \nUsing so many parameters I did have to fight overfitting. Alongside regularization I choose to predict on the network that did the best on the evaluation, usually taken from epoch num ~40 from 100 epochs, before the network begin overfitting. Thus, the evaluation size has to be large enough. \n\nOther hyper-parameters that were significant are the learning rate decay (or learning rate function) and the batch size. Inreasing the batch size and controlling the learning rate decay changed the graph of loss as a function of epochs. For relative small learning rate decay and batch size of 64 the loss variance was big even for high epoch number. Using large batch size improved the variance a lot and the schedualed learning rate improved the performance. \n\n\n#### Author: Shay Guterman, Hebrew University of Jerusalem: shaygu62@gmail.com\n","4f880122":"## Visualize some examples from the dataset.\nShowing some example per class","1c81c400":"## To get repreducable results, generate random number using seed\n\nNetowrk performance may vary for different initializations","e0c0f7d4":"## Confusion matrix","35abc778":"## Adding dimensions for keras","8e1d1cd1":"## Visualizations"}}