{"cell_type":{"5eadb203":"code","fd10d138":"code","31528b6e":"code","827bc3e1":"code","8480aa2d":"code","2aadbb42":"code","8e8dc8d9":"code","9f3e584a":"code","d9754f2d":"code","7bae510c":"code","7f9233ff":"code","b2d42f38":"code","05e6fa36":"code","a8e28f9e":"code","6723897c":"code","775b7a7e":"code","a0153df4":"code","de8c047a":"code","2bc58a15":"markdown","b14b30d2":"markdown","27aaa740":"markdown","eb61f894":"markdown","608e9c9a":"markdown","f46f38bb":"markdown"},"source":{"5eadb203":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","fd10d138":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom sklearn.preprocessing import MinMaxScaler,StandardScaler\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.model_selection import cross_val_score,RepeatedStratifiedKFold,train_test_split\nfrom sklearn.impute import KNNImputer\nfrom sklearn.ensemble import GradientBoostingClassifier\n\nsns.set_style('darkgrid')\n\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis, QuadraticDiscriminantAnalysis\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.feature_selection import SelectKBest,chi2,mutual_info_classif\nfrom sklearn.metrics import roc_auc_score\n\nfrom sklearn.model_selection import RandomizedSearchCV\nfrom collections import Counter","31528b6e":"train=pd.read_csv('\/kaggle\/input\/titanic\/train.csv',index_col='PassengerId')\ntest=pd.read_csv('\/kaggle\/input\/titanic\/test.csv',index_col='PassengerId')\nsubmit=pd.DataFrame(test.index)","827bc3e1":"#------------    Survived    -----------------\nfig,ax=plt.subplots(2,2,figsize=(15,13))\nsns.countplot(train['Survived'],hue=train['Sex'],ax=ax[0,0])\nprint('!--In fig 1 we can see that female has more chance to survive in comparison to men')\n\nsns.countplot(train['Pclass'],hue=train['Survived'],ax=ax[0,1])\n\nprint('!---In fig 2 we can see that people from 3 rd class has less chance to survive')\n\nsns.ecdfplot(x='Fare',ax=ax[1,0],hue='Survived',data=train)\n\nprint('!--About 80 % of people whose fare was less than 100 died where person who has fare about 200-300 has more chance to survive')\n\nsns.swarmplot(x='Survived',y='Age',data=train,ax=ax[1,1])\n\nprint('--!In fig 4 below age 10 has a greater chance to survive(children)')","8480aa2d":"sns.catplot(x='Pclass',y='Age',data=train,kind='bar',col='Survived',row='Embarked',hue='Sex')\n\nprint('If people is from first class nd from C and Q has more chance to survive')\nprint('1st class female from Southampton ')","2aadbb42":"fig,ax=plt.subplots(1,2,figsize=(15,6))\nsns.distplot(train['Age'],bins=10,ax=ax[0])# We can see a little bump in age below 10  has chance to survive\nsns.distplot(train['Fare'],ax=ax[1])       #Fare looks more like a gamma distribution so we need to change","8e8dc8d9":"train['Sex']=train['Sex'].replace({'male':0,'female':1})\n#---------------------------------------------------------------\n\ndef Age_group(age):\n    a=''\n    if(age<3):\n        a='infant'\n    elif(age<9):\n        a='child'\n    elif(age<17):\n        a='young'\n    elif(age<37):\n        a='blood'\n    elif(age<60):\n        a='middle'\n    else:\n        a='old'\n    return a\ntrain['age_group']=train['Age'].map(Age_group)\n#-------------------------------------------------------------------\n\na=[]\nfor i in range(len(train)):\n    a.append(train['Name'].iloc[i].split(',')[1].split('.')[0][1:])\ntrain['title']=a\ntrain['title']=train['title'].replace(['Don', 'Rev', 'Dr', 'Mme', 'Ms','Major', 'Lady', 'Sir', 'Mlle', 'Col', 'Capt', 'the Countess','Jonkheer'],'Rare')\n#----------------------------------------------------------------------------\n\nfor i in range(len(train)):\n    if not(pd.isnull(train['Cabin'].iloc[i])):\n        train['Cabin'].iloc[i]=train['Cabin'].iloc[i][0] \n        \n#---------------------------------------------------------------------------\n\ntrain['age_group']=train['age_group'].replace({'blood':1,'old':2,'middle':3,'young':4,'child':5,'infant':6})\ntrain['title']=train['title'].replace({'Mr':1,'Miss':2,'Mrs':3,'Master':4,'Rare':5})\ntrain['Cabin']=train['Cabin'].replace({'C':1,'B':2,'D':3,'E':4,'A':5,'F':6,'G':7,'T':8})\n\n#-----------------------------------------------------------------------------\n\ntrain=pd.get_dummies(train,columns=['Embarked','title'],drop_first=True)\n\n","9f3e584a":"x=train.drop(['Name','Ticket','Survived','SibSp'],axis=1)\ny=train['Survived']","d9754f2d":"\npipeline=Pipeline(steps=[('impute',KNNImputer(n_neighbors=6)),('d',StandardScaler()),])\npipeline.fit(x,y)\nx=pipeline.fit_transform(x)","7bae510c":"np.random.seed(42)\n# Split into train & test set\nX_train, X_test, y_train, y_test = train_test_split(x,y, test_size = 0.25) \n\n\nmodels = {\"KNN\": KNeighborsClassifier(),\n          \"Logistic Regression\": LogisticRegression(max_iter=10000), \n          \"Random Forest\": RandomForestClassifier(),\n          \"SVC\" : SVC(probability=True),\n          \"DecisionTreeClassifier\" : DecisionTreeClassifier(),\n          \"AdaBoostClassifier\" : AdaBoostClassifier(),\n          \"GradientBoostingClassifier\" : GradientBoostingClassifier(),\n          \"GaussianNB\" : GaussianNB(),\n          \"LinearDiscriminantAnalysis\" : LinearDiscriminantAnalysis(),\n          \"QuadraticDiscriminantAnalysis\" : QuadraticDiscriminantAnalysis()}\ndef fit_and_score(models, X_train, X_test, y_train, y_test):\n\n    # Random seed for reproducible results\n    np.random.seed(42)\n    # Make a list to keep model scores\n    model_scores = {}\n    # Loop through models\n    for name, model in models.items():\n        # Fit the model to the data\n        model.fit(X_train, y_train)\n        # Predicting target values\n        y_pred = model.predict(X_test)\n        # Evaluate the model and append its score to model_scores\n        #model_scores[name] = model.score(X_test, y_test)\n        model_scores[name] = roc_auc_score(y_pred, y_test)\n    return model_scores\nmodel_scores = fit_and_score(models=models,\n                             X_train=X_train,\n                             X_test=X_test,\n                             y_train=y_train,\n                             y_test=y_test)\nmodel_scores","7f9233ff":"model=SVC()\ncv=RepeatedStratifiedKFold(n_splits=10,n_repeats=3)\nscore=cross_val_score(model,x,y,scoring='accuracy',cv=cv)\nnp.mean(score)","b2d42f38":"test['Sex']=test['Sex'].replace({'male':0,'female':1})\n#------------------------------------------------------------\n#---------------------------------------------------------------\ndef Age_group(age):\n    a=''\n    if(age<3):\n        a='infant'\n    elif(age<9):\n        a='child'\n    elif(age<17):\n        a='young'\n    elif(age<37):\n        a='blood'\n    elif(age<60):\n        a='middle'\n    else:\n        a='old'\n    return a\ntest['age_group']=test['Age'].map(Age_group)\n#-------------------------------------------------------------------\n\na=[]\nfor i in range(len(test)):\n    a.append(test['Name'].iloc[i].split(',')[1].split('.')[0][1:])\ntest['title']=a\ntest['title']=test['title'].replace(['Don', 'Rev', 'Dr', 'Mme', 'Ms','Major', 'Lady', 'Sir', 'Mlle', 'Col',\"Dona\",'Capt', 'the Countess','Jonkheer'],'Rare')\n#----------------------------------------------------------------------------\n\nfor i in range(len(test)):\n    if not(pd.isnull(test['Cabin'].iloc[i])):\n           test['Cabin'].iloc[i]=test['Cabin'].iloc[i][0] \n        \n#---------------------------------------------------------------------------\n\ntest['age_group']=test['age_group'].replace({'blood':1,'old':2,'middle':3,'young':4,'child':5,'infant':6})\ntest['title']=test['title'].replace({'Mr':1,'Miss':2,'Mrs':3,'Master':4,'Rare':5})\ntest['Cabin']=test['Cabin'].replace({'C':1,'B':2,'D':3,'E':4,'A':5,'F':6,'G':7,'T':8})\n\n#-----------------------------------------------------------------------------\n\ntest=pd.get_dummies(test,columns=['Embarked','title'],drop_first=True)\n\ntest['Fare']=np.sqrt(test['Fare'])\n\n#----------------------------------\n\ntest=test.drop(['Name','Ticket','SibSp'],axis=1)","05e6fa36":"test=pipeline.fit_transform(test)","a8e28f9e":"model=SVC()\nmodel.fit(x,y)\nfrom sklearn.model_selection import GridSearchCV\n\n# defining parameter range\nparam_grid = {'C': [0.1, 1, 10, 100, 1000],\n'gamma': [1, 0.1, 0.01, 0.001, 0.0001],\n'kernel': ['rbf']}\n\ngrid = GridSearchCV(model, param_grid, refit= True,verbose = 3)\n\n# fitting the model for grid search\ngrid.fit(X_train, y_train)\n","6723897c":"\nsubmit['Survived']=grid.predict(test)\nsubmit.to_csv('ver.csv',index=False)","775b7a7e":"x.shape","a0153df4":"test.shape","de8c047a":"submit","2bc58a15":"# Data Visualization","b14b30d2":"# Modelling","27aaa740":"# Data Manupulation and Feature Extraction","eb61f894":"**1.Male from 1st class and 2 nd class from embarked=Q has the least chance to survive----------------**\n**2.Male from 2nd class from Embarked-C has least chance to survive**","608e9c9a":"# Importing modules","f46f38bb":"# Test workdown"}}