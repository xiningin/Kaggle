{"cell_type":{"ba18bebf":"code","a9928c48":"code","0ec16531":"code","78ba3094":"code","04ec65cc":"code","1d893fe5":"code","1267a46c":"code","35dad771":"code","d1b614b7":"code","e2854a99":"code","2be82ee1":"code","5c075ffe":"code","abff8065":"code","a21f6211":"code","b47f902e":"code","126d9e80":"code","48dca3b4":"code","6f7f943a":"code","0e66ed74":"code","28a87aee":"code","8401acab":"code","f7dade52":"code","7441561c":"code","021dc02e":"code","3f3671e6":"code","2561c4eb":"code","ae0d8497":"code","da33825d":"code","b455bc32":"markdown","4402e5b3":"markdown","8d5401fc":"markdown","3c521ca0":"markdown","419b0355":"markdown","adc983b4":"markdown","20e00d8a":"markdown","fe0bc255":"markdown","09381b28":"markdown","a99c771a":"markdown"},"source":{"ba18bebf":"#Preliminaries\nimport os\nimport numpy as np \nimport pandas as pd \nimport re\nimport random\nimport string\nfrom tqdm import tqdm\nfrom tqdm.autonotebook import tqdm\nfrom sklearn import model_selection\nfrom sklearn import metrics\n\n#Tokenizers from hugging face\nimport tokenizers\n\n#Pytorch\nimport torch\nimport torch.nn as nn\nfrom torch.nn import functional as F\nfrom torch.optim import lr_scheduler\n\n#Transformers\nimport transformers\nfrom transformers import AdamW\nfrom transformers import get_linear_schedule_with_warmup\n\n#Ignore Warnings\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\n# Printing Paths\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","a9928c48":"def seed_everything(seed):\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = True","0ec16531":"seed=2020\nseed_everything(seed)","78ba3094":"MAX_LEN = 124\nTRAIN_BATCH_SIZE = 50\nVALID_BATCH_SIZE = 32\nEPOCHS = 5\nROBERTA_PATH = \"..\/input\/roberta-base\"\nTOKENIZER = tokenizers.ByteLevelBPETokenizer(\n    vocab_file=f\"{ROBERTA_PATH}\/vocab.json\", \n    merges_file=f\"{ROBERTA_PATH}\/merges.txt\", \n    lowercase=True,\n    add_prefix_space=True\n)","04ec65cc":"class AverageMeter(object):\n    def __init__(self):\n        self.reset()\n\n    def reset(self):\n        self.val = 0\n        self.avg = 0\n        self.sum = 0\n        self.count = 0\n\n    def update(self, val, n=1):\n        self.val = val\n        self.sum += val * n\n        self.count += n\n        self.avg = self.sum \/ self.count","1d893fe5":"class EarlyStopping:\n    def __init__(self, patience=7, mode=\"max\", delta=0.001):\n        self.patience = patience\n        self.counter = 0\n        self.mode = mode\n        self.best_score = None\n        self.early_stop = False\n        self.delta = delta\n        if self.mode == \"min\":\n            self.val_score = np.Inf\n        else:\n            self.val_score = -np.Inf\n\n    def __call__(self, epoch_score, model, model_path):\n\n        if self.mode == \"min\":\n            score = -1.0 * epoch_score\n        else:\n            score = np.copy(epoch_score)\n\n        if self.best_score is None:\n            self.best_score = score\n            self.save_checkpoint(epoch_score, model, model_path)\n        elif score < self.best_score + self.delta:\n            self.counter += 1\n            print('EarlyStopping counter: {} out of {}'.format(self.counter, self.patience))\n            if self.counter >= self.patience:\n                self.early_stop = True\n        else:\n            self.best_score = score\n            self.save_checkpoint(epoch_score, model, model_path)\n            self.counter = 0\n\n    def save_checkpoint(self, epoch_score, model, model_path):\n        if epoch_score not in [-np.inf, np.inf, -np.nan, np.nan]:\n            print('Validation score improved ({} --> {}). Saving model!'.format(self.val_score, epoch_score))\n            torch.save(model.state_dict(), model_path)\n        self.val_score = epoch_score","1267a46c":"def onehot(size, target):\n    vec = torch.zeros(size, dtype=torch.long)\n    vec[target] = 1.\n    return vec","35dad771":"def jaccard(str1, str2): \n    a = set(str1.lower().split()) \n    b = set(str2.lower().split())\n    c = a.intersection(b)\n    return float(len(c)) \/ (len(a) + len(b) - len(c))","d1b614b7":"class TweetModel(transformers.BertPreTrainedModel):\n    def __init__(self, conf):\n        super(TweetModel, self).__init__(conf)\n        self.roberta = transformers.RobertaModel.from_pretrained(ROBERTA_PATH, config=conf)\n        \n        self.d0 = nn.Dropout(0.1)\n        self.d1 = nn.Dropout(0.1)\n        \n        self.l0 = nn.Linear(768 * 2, 2)\n        self.l1 = nn.Linear(768 * 2, 3)\n        \n        torch.nn.init.normal_(self.l0.weight, std=0.02)\n        torch.nn.init.normal_(self.l1.weight, std=0.02)\n    \n    def forward(self, ids, mask, token_type_ids):\n        _, _, out = self.roberta(\n            ids,\n            attention_mask=mask,\n            token_type_ids=token_type_ids\n        )\n\n        out = torch.cat((out[-1], out[-2]), dim=-1)\n        \n        # Head 1\n        x = self.d0(out)\n        logits = self.l0(x)\n        start_logits, end_logits = logits.split(1, dim=-1)\n        start_logits = start_logits.squeeze(-1)\n        end_logits = end_logits.squeeze(-1)\n        \n        # Head 2\n        y = self.d1(out)\n        y = self.l1(y)\n        y = y[:,0,:]\n        sentiment = y.squeeze(-2)\n\n        return start_logits, end_logits, sentiment","e2854a99":"def loss_fn(start_logits, end_logits, sent_logits, start_positions, end_positions, sentiments):\n    loss_fct = nn.CrossEntropyLoss()\n    \n    start_loss = loss_fct(start_logits, start_positions)\n    end_loss = loss_fct(end_logits, end_positions)\n    total_loss = (start_loss + end_loss)\n    \n    sent_loss = loss_fct(sent_logits, sentiments)\n    \n    return torch.mean(torch.stack([total_loss, sent_loss]))","2be82ee1":"def process_data(tweet, selected_text, sentiment, tokenizer, max_len):\n    tweet = \" \" + \" \".join(str(tweet).split())\n    selected_text = \" \" + \" \".join(str(selected_text).split())\n\n    len_st = len(selected_text) - 1\n    idx0 = None\n    idx1 = None\n\n    for ind in (i for i, e in enumerate(tweet) if e == selected_text[1]):\n        if \" \" + tweet[ind: ind+len_st] == selected_text:\n            idx0 = ind\n            idx1 = ind + len_st - 1\n            break\n\n    char_targets = [0] * len(tweet)\n    if idx0 != None and idx1 != None:\n        for ct in range(idx0, idx1 + 1):\n            char_targets[ct] = 1\n    \n    tok_tweet = tokenizer.encode(tweet)\n    input_ids_orig = tok_tweet.ids\n    tweet_offsets = tok_tweet.offsets\n    \n    target_idx = []\n    for j, (offset1, offset2) in enumerate(tweet_offsets):\n        if sum(char_targets[offset1: offset2]) > 0:\n            target_idx.append(j)\n    \n    targets_start = target_idx[0]\n    targets_end = target_idx[-1]\n    \n    sentiment = str(sentiment)\n\n    sentiment_id = {\n        '2': 1313,\n        '0': 2430,\n        '1': 7974\n    }\n    \n    input_ids = [0] + [sentiment_id[sentiment]] + [2] + [2] + input_ids_orig + [2]\n    token_type_ids = [0, 0, 0, 0] + [0] * (len(input_ids_orig) + 1)\n    mask = [1] * len(token_type_ids)\n    tweet_offsets = [(0, 0)] * 4 + tweet_offsets + [(0, 0)]\n    targets_start += 4\n    targets_end += 4\n\n    padding_length = max_len - len(input_ids)\n    if padding_length > 0:\n        input_ids = input_ids + ([1] * padding_length)\n        mask = mask + ([0] * padding_length)\n        token_type_ids = token_type_ids + ([0] * padding_length)\n        tweet_offsets = tweet_offsets + ([(0, 0)] * padding_length)\n            \n    return {\n        'ids': input_ids,\n        'mask': mask,\n        'token_type_ids': token_type_ids,\n        'targets_start': targets_start,\n        'targets_end': targets_end,\n        'orig_tweet': tweet,\n        'orig_selected': selected_text,\n        'sentiment': int(sentiment),\n        'offsets': tweet_offsets\n    }","5c075ffe":"class TweetDataset:\n    def __init__(self, tweet, sentiment, selected_text):\n        self.tweet = tweet\n        self.sentiment = sentiment\n        self.selected_text = selected_text\n        self.tokenizer = TOKENIZER\n        self.max_len = MAX_LEN\n    \n    def __len__(self):\n        return len(self.tweet)\n\n    def __getitem__(self, item):\n        data = process_data(\n            self.tweet[item], \n            self.selected_text[item],\n            self.sentiment[item],\n            self.tokenizer,\n            self.max_len\n        )\n        \n        #sentiment = onehot(3, self.sentiment[item])\n\n        return {\n            'ids': torch.tensor(data[\"ids\"], dtype=torch.long),\n            'mask': torch.tensor(data[\"mask\"], dtype=torch.long),\n            'token_type_ids': torch.tensor(data[\"token_type_ids\"], dtype=torch.long),\n            'targets_start': torch.tensor(data[\"targets_start\"], dtype=torch.long),\n            'targets_end': torch.tensor(data[\"targets_end\"], dtype=torch.long),\n            'orig_tweet': data[\"orig_tweet\"],\n            'orig_selected': data[\"orig_selected\"],\n            'sentiment': torch.tensor(data['sentiment'], dtype=torch.long),\n            'offsets': torch.tensor(data[\"offsets\"], dtype=torch.long)\n        }","abff8065":"def calculate_jaccard_score(\n    original_tweet, \n    target_string, \n    sentiment_val, \n    idx_start, \n    idx_end, \n    offsets,\n    verbose=False):\n    \n    if idx_end < idx_start:\n        idx_end = idx_start\n    \n    filtered_output  = \"\"\n    for ix in range(idx_start, idx_end + 1):\n        filtered_output += original_tweet[offsets[ix][0]: offsets[ix][1]]\n        if (ix+1) < len(offsets) and offsets[ix][1] < offsets[ix+1][0]:\n            filtered_output += \" \"\n\n    if sentiment_val == \"neutral\" or len(original_tweet.split()) < 2:\n        filtered_output = original_tweet\n\n    #jac = 0\n    jac = jaccard(target_string.strip(), filtered_output.strip())\n    return jac, filtered_output","a21f6211":"def eval_fn(data_loader, model, device):\n    \n    model.eval()\n    losses = AverageMeter()\n    jaccards = AverageMeter()\n    accuracy = AverageMeter()\n    \n    tk0 = tqdm(data_loader, total=len(data_loader), desc=\"Validating\")\n    \n    with torch.no_grad():\n        for bi, d in enumerate(tk0):\n            ids = d[\"ids\"]\n            token_type_ids = d[\"token_type_ids\"]\n            mask = d[\"mask\"]\n            sentiment = d[\"sentiment\"]\n            orig_selected = d[\"orig_selected\"]\n            orig_tweet = d[\"orig_tweet\"]\n            targets_start = d[\"targets_start\"]\n            targets_end = d[\"targets_end\"]\n            offsets = d[\"offsets\"].cpu().numpy()\n\n            ids = ids.to(device, dtype=torch.long)\n            token_type_ids = token_type_ids.to(device, dtype=torch.long)\n            mask = mask.to(device, dtype=torch.long)\n            \n            targets_start = targets_start.to(device, dtype=torch.long)\n            targets_end = targets_end.to(device, dtype=torch.long)\n            sentiment = sentiment.to(device, dtype=torch.long)\n\n            outputs_start, outputs_end, outputs_sent = model(\n                ids=ids,\n                mask=mask,\n                token_type_ids=token_type_ids\n            )\n            \n            loss = loss_fn(outputs_start, outputs_end, outputs_sent, targets_start, targets_end, sentiment)\n            outputs_start = torch.softmax(outputs_start, dim=1).cpu().detach().numpy()\n            outputs_end = torch.softmax(outputs_end, dim=1).cpu().detach().numpy()\n            outputs_sent = torch.softmax(outputs_sent, dim=1).cpu().detach().numpy().argmax(axis=1)\n            \n            jaccard_scores = []\n            \n            for px, tweet in enumerate(orig_tweet):\n                selected_tweet = orig_selected[px]\n                tweet_sentiment = sentiment[px]\n                jaccard_score, _ = calculate_jaccard_score(\n                    original_tweet=tweet,\n                    target_string=selected_tweet,\n                    sentiment_val=tweet_sentiment,\n                    idx_start=np.argmax(outputs_start[px, :]),\n                    idx_end=np.argmax(outputs_end[px, :]),\n                    offsets=offsets[px]\n                )\n                jaccard_scores.append(jaccard_score)\n            \n            acc = metrics.accuracy_score(sentiment.cpu().detach().numpy(), outputs_sent)\n\n            jaccards.update(np.mean(jaccard_scores), ids.size(0))\n            losses.update(loss.item(), ids.size(0))\n            accuracy.update(acc, ids.size(0))\n            tk0.set_postfix(loss=losses.avg, jaccard=jaccards.avg,accuracy=accuracy.avg)\n\n            \n    return losses.avg, jaccards.avg, accuracy.avg","b47f902e":"def train_fn(data_loader, model, optimizer, device, scheduler=None):\n    model.train()\n    \n    losses = AverageMeter()\n    jaccards = AverageMeter()\n    accuracy = AverageMeter()\n    \n    tk0 = tqdm(data_loader, total=len(data_loader), desc=\"Training\")\n    \n    for bi, d in enumerate(tk0):\n        ids = d[\"ids\"]\n        token_type_ids = d[\"token_type_ids\"]\n        mask = d[\"mask\"]\n        targets_start = d[\"targets_start\"]\n        targets_end = d[\"targets_end\"]\n        sentiment = d[\"sentiment\"]\n        orig_selected = d[\"orig_selected\"]\n        orig_tweet = d[\"orig_tweet\"]\n        targets_start = d[\"targets_start\"]\n        targets_end = d[\"targets_end\"]\n        offsets = d[\"offsets\"]\n\n        ids = ids.to(device, dtype=torch.long)\n        token_type_ids = token_type_ids.to(device, dtype=torch.long)\n        mask = mask.to(device, dtype=torch.long)\n        \n        targets_start = targets_start.to(device, dtype=torch.long)\n        targets_end = targets_end.to(device, dtype=torch.long)\n        sentiment = sentiment.to(device, dtype=torch.long)\n\n        model.zero_grad()\n        \n        outputs_start, outputs_end, outputs_sent = model(\n            ids=ids,\n            mask=mask,\n            token_type_ids=token_type_ids,\n        )\n        \n        loss = loss_fn(outputs_start, outputs_end, outputs_sent, \\\n                       targets_start, targets_end, sentiment)\n        loss.backward()\n        optimizer.step()\n        scheduler.step()\n        \n        outputs_start = torch.softmax(outputs_start, dim=1).cpu().detach().numpy()\n        outputs_end = torch.softmax(outputs_end, dim=1).cpu().detach().numpy()\n        outputs_sent = torch.softmax(outputs_sent, dim=1).cpu().detach().numpy().argmax(axis=1)\n        \n        jaccard_scores = []\n        for px, tweet in enumerate(orig_tweet):\n            selected_tweet = orig_selected[px]\n            tweet_sentiment = sentiment[px]\n            jaccard_score, _ = calculate_jaccard_score(\n                original_tweet=tweet,\n                target_string=selected_tweet,\n                sentiment_val=tweet_sentiment,\n                idx_start=np.argmax(outputs_start[px, :]),\n                idx_end=np.argmax(outputs_end[px, :]),\n                offsets=offsets[px]\n            )\n            jaccard_scores.append(jaccard_score)\n        \n            \n        acc = metrics.accuracy_score(sentiment.cpu().detach().numpy(), outputs_sent)\n\n        jaccards.update(np.mean(jaccard_scores), ids.size(0))\n        losses.update(loss.item(), ids.size(0))\n        accuracy.update(acc, ids.size(0))\n        tk0.set_postfix(loss=losses.avg, jaccard=jaccards.avg,accuracy = accuracy.avg)","126d9e80":"from sklearn import model_selection\n\ndfx = pd.read_csv(\"\/kaggle\/input\/tweet-sentiment-extraction\/train.csv\")\ndfx = dfx.dropna().reset_index(drop=True)\ndfx[\"kfold\"] = -1\n\ndfx = dfx.sample(frac=1,random_state=seed).reset_index(drop=True)\n\nkf = model_selection.StratifiedKFold(n_splits=10,random_state=seed)\n\nfor fold, (trn_, val_) in enumerate(kf.split(X=dfx, y=dfx.sentiment.values)):\n    print(len(trn_), len(val_))\n    dfx.loc[val_, 'kfold'] = fold","48dca3b4":"sent = {'negative': 0, 'neutral': 1, 'positive': 2}\ndfx['sentiment'] = dfx['sentiment'].map(sent)","6f7f943a":"def run(fold):\n    df_train = dfx[dfx.kfold != fold].reset_index(drop=True)#.sample(frac= 1, random_state=2020)\n    df_valid = dfx[dfx.kfold == fold].reset_index(drop=True)#.sample(frac= 1, random_state=2020)\n    \n    train_dataset = TweetDataset(\n        tweet=df_train.text.values,\n        sentiment=df_train.sentiment.values,\n        selected_text=df_train.selected_text.values\n    )\n\n    train_data_loader = torch.utils.data.DataLoader(\n        train_dataset,\n        batch_size=TRAIN_BATCH_SIZE,\n        num_workers=4\n    )\n\n    valid_dataset = TweetDataset(\n        tweet=df_valid.text.values,\n        sentiment=df_valid.sentiment.values,\n        selected_text=df_valid.selected_text.values\n    )\n\n    valid_data_loader = torch.utils.data.DataLoader(\n        valid_dataset,\n        batch_size=VALID_BATCH_SIZE,\n        num_workers=2\n    )\n\n    device = torch.device(\"cuda\")\n    model_config = transformers.RobertaConfig.from_pretrained(ROBERTA_PATH)\n    model_config.output_hidden_states = True\n    model = TweetModel(conf=model_config)\n    model.to(device)\n\n    num_train_steps = int(len(df_train) \/ TRAIN_BATCH_SIZE * EPOCHS)\n    param_optimizer = list(model.named_parameters())\n    no_decay = [\"bias\", \"LayerNorm.bias\", \"LayerNorm.weight\"]\n    optimizer_parameters = [\n        {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)], 'weight_decay': 0.001},\n        {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)], 'weight_decay': 0.0},\n    ]\n    optimizer = AdamW(optimizer_parameters, lr=3e-5)\n    scheduler = get_linear_schedule_with_warmup(\n        optimizer, \n        num_warmup_steps=0, \n        num_training_steps=num_train_steps\n    )\n\n    es = EarlyStopping(patience=4, mode=\"max\")\n    print(f\"Training is Starting for fold={fold}\")\n    \n    # I'm training only for 3 epochs even though I specified 5!!!\n    for epoch in range(5):\n        train_fn(train_data_loader, model, optimizer, device, scheduler=scheduler)\n        loss,jaccard,acc = eval_fn(valid_data_loader, model, device)\n        print(f\"Jaccard Score = {jaccard}\")\n        es(jaccard, model, model_path=f\"model_{fold}.bin\")\n        if es.early_stop:\n            print(\"Early stopping\")\n            break","0e66ed74":"run(fold=0)","28a87aee":"run(fold=1)","8401acab":"run(fold=2)","f7dade52":"run(fold=3)","7441561c":"run(fold=4)","021dc02e":"run(fold=5)","3f3671e6":"run(fold=6)","2561c4eb":"run(fold=7)","ae0d8497":"run(fold=8)","da33825d":"run(fold=9)","b455bc32":"# Cooking Up Data","4402e5b3":"# Config","8d5401fc":"# Training function","3c521ca0":"# Mr.Loss","419b0355":"# Utils","adc983b4":"# Seed Up","20e00d8a":"# Decoder","fe0bc255":"# Model","09381b28":"# Evaluation","a99c771a":"# Training"}}