{"cell_type":{"7d6088bf":"code","d8da481b":"code","12ed5d81":"code","cf052981":"code","fe1fa685":"code","5bb71727":"code","0071ad44":"code","665893b1":"code","1b13056d":"code","8820b6d1":"code","f5ae286e":"code","109f0a40":"code","c4657af6":"code","7c7e7a9c":"code","34997042":"code","2dcdf73a":"markdown","bb50b0a5":"markdown","d5a34d61":"markdown","08d449cc":"markdown","1c8f1f50":"markdown","bae15e64":"markdown","a93bd25a":"markdown","d562e91e":"markdown","d7fcaaa9":"markdown","513ef7d5":"markdown","037bbe7f":"markdown","c0488c18":"markdown"},"source":{"7d6088bf":"#Import necessary libraries and packages\n\nimport pandas as pd\nimport os\nimport numpy as np\n\nimport tensorflow as tf\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom tensorflow.keras.layers import Embedding, LSTM, Dense, Bidirectional\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.optimizers import Adam","d8da481b":"medium_data = pd.read_csv(r\"C:\\Users\\parit\\OneDrive\\Desktop\\AI for ALL\\medium_data.csv\\medium_data.csv\")\nmedium_data.head()","12ed5d81":"#Here, we have a 10 different fields and 6508 records but we will only use title field for predicting next word.\nprint(\"Number of records: \", medium_data.shape[0])\nprint(\"Number of fields: \", medium_data.shape[1])","cf052981":"#Display titles of various articles and preprocess them\nmedium_data['title']","fe1fa685":"medium_data['title'] = medium_data['title'].apply(lambda x: x.replace(u'\\xa0',u' '))\nmedium_data['title'] = medium_data['title'].apply(lambda x: x.replace('\\u200a',' '))","5bb71727":"tokenizer = Tokenizer(oov_token='<oov>') # For those words which are not found in word_index\ntokenizer.fit_on_texts(medium_data['title'])\ntotal_words = len(tokenizer.word_index) + 1\n\nprint(\"Total number of words: \", total_words)\nprint(\"Word: ID\")\nprint(\"------------\")\nprint(\"<oov>: \", tokenizer.word_index['<oov>'])\nprint(\"Strong: \", tokenizer.word_index['strong'])\nprint(\"And: \", tokenizer.word_index['and'])\nprint(\"Consumption: \", tokenizer.word_index['consumption'])","0071ad44":"input_sequences = []\nfor line in medium_data['title']:\n    token_list = tokenizer.texts_to_sequences([line])[0]\n    #print(token_list)\n    \n    for i in range(1, len(token_list)):\n        n_gram_sequence = token_list[:i+1]\n        input_sequences.append(n_gram_sequence)\n\n# print(input_sequences)\nprint(\"Total input sequences: \", len(input_sequences))","665893b1":"# pad sequences \nmax_sequence_len = max([len(x) for x in input_sequences])\ninput_sequences = np.array(pad_sequences(input_sequences, maxlen=max_sequence_len, padding='pre'))\ninput_sequences[1]","1b13056d":"# create features and label\nxs, labels = input_sequences[:,:-1],input_sequences[:,-1]\nys = tf.keras.utils.to_categorical(labels, num_classes=total_words)","8820b6d1":"print(xs[5])\nprint(labels[5])\nprint(ys[5][14])","f5ae286e":"model = Sequential()\nmodel.add(Embedding(total_words, 100, input_length=max_sequence_len-1))\nmodel.add(Bidirectional(LSTM(150)))\nmodel.add(Dense(total_words, activation='softmax'))\nadam = Adam(lr=0.01)\nmodel.compile(loss='categorical_crossentropy', optimizer=adam, metrics=['accuracy'])\nhistory = model.fit(xs, ys, epochs=50, verbose=1)\n#print model.summary()\nprint(model)","109f0a40":"import matplotlib.pyplot as plt\n\n\ndef plot_graphs(history, string):\n    plt.plot(history.history[string])\n    plt.xlabel(\"Epochs\")\n    plt.ylabel(string)\n    plt.show()","c4657af6":"plot_graphs(history, 'accuracy')","7c7e7a9c":"plot_graphs(history, 'loss')","34997042":"seed_text = \"implementation of\"\nnext_words = 2\n  \nfor _ in range(next_words):\n    token_list = tokenizer.texts_to_sequences([seed_text])[0]\n    token_list = pad_sequences([token_list], maxlen=max_sequence_len-1, padding='pre')\n    predicted = model.predict_classes(token_list, verbose=0)\n    output_word = \"\"\n    for word, index in tokenizer.word_index.items():\n        if index == predicted:\n            output_word = word\n            break\n    seed_text += \" \" + output_word\nprint(seed_text)","2dcdf73a":"Make all titles with same length by using padding\nThe length of every title has to be the same. To make it, we need to find a title that has a maximum length, and based on that length, we have to pad rest of titles.","bb50b0a5":"Long Short-Term Memory (LSTM) networks is an advance recurrent neural network which is apable to store order states by using its cell state feature.","d5a34d61":"# Bi- LSTM Neural Network Model training","08d449cc":"# Plotting model accuracy and loss","1c8f1f50":"Tokenzaion is the process in which we provide an unique id to all the words and make a word index or we can say vocabulary.","bae15e64":"# Tokenzation\n","a93bd25a":"Looking at titles, we can see there are some of unwanted characters and words in it which can not be useful for us to predict infact it might decrease our model accuracy so we have to remove it.","d562e91e":"# Prepare features and labels","d7fcaaa9":"# Predicting next word of title","513ef7d5":"Titles text into sequences and make n_gram model\nsuppose, we have sentence like \"I am Paritosh\" and this will convert into a sequence with their respective tokens {'I': 1,'am': 2,'Paritosh': 3}. Thus, output will be [ '1' ,'2' ,'3' ]\n\nLikewise, our all titles will be converted into sequences.\n\nThen, we will make a n_gram model for good prediction.","037bbe7f":"# Removing unwanted characters and words in titles","c0488c18":"Here, we consider last element of all sequences as a label.Then, We need to perform onehot encoding on labels corresponding to total_words."}}