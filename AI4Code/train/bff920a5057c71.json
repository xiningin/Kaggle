{"cell_type":{"bb11fe18":"code","d3f07a64":"code","4ee8bef8":"code","c1edffdf":"code","d2e87b3c":"code","4f7d9827":"code","dc165567":"code","ebc5468d":"code","a59cf7d3":"code","e4d872e4":"code","f90fa875":"code","139f4c72":"code","ac93efc0":"code","cb3150a4":"code","a52cdc30":"code","4891acf2":"code","b3257221":"code","a1b515c8":"code","2d70d18c":"code","e5bc1f7b":"code","988d10c7":"code","71553e10":"code","308906ba":"code","8e1d8fea":"code","829afc12":"code","3765c319":"code","b6fb55e4":"code","80e7d7e6":"code","6e73bb49":"code","0f2750b4":"code","289e7ef6":"code","1c30e230":"markdown","eb49e94a":"markdown","aa742060":"markdown","e6fd67ec":"markdown","fb807b1c":"markdown","154488ca":"markdown","5177fe46":"markdown","b9c43fa2":"markdown","1323e22b":"markdown","cabfa900":"markdown","46fdef4a":"markdown","30f04fd0":"markdown","48ebc43b":"markdown"},"source":{"bb11fe18":"from pathlib import Path\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport keras \nfrom keras.layers import Dense, Conv2D, BatchNormalization, Activation \nfrom keras.layers import AveragePooling2D, Input, Flatten \nfrom keras.optimizers import Adam \nfrom keras.callbacks import ModelCheckpoint, LearningRateScheduler \nfrom keras.callbacks import ReduceLROnPlateau \nfrom keras.preprocessing.image import ImageDataGenerator \nfrom keras.regularizers import l2 \nfrom keras import backend as K \nfrom keras.models import Model \n\n%matplotlib inline","d3f07a64":"DATA_PATH = Path(\"..\/input\/magnet-nasa\")\n\ndst = pd.read_csv(DATA_PATH \/ \"dst_labels.csv\")\ndst.timedelta = pd.to_timedelta(dst.timedelta)\ndst.set_index([\"period\", \"timedelta\"], inplace=True)\n\nsunspots = pd.read_csv(DATA_PATH \/ \"sunspots.csv\")\nsunspots.timedelta = pd.to_timedelta(sunspots.timedelta)\nsunspots.set_index([\"period\", \"timedelta\"], inplace=True)\n\nsolar_wind = pd.read_csv(DATA_PATH \/ \"solar_wind.csv\")\nsolar_wind.timedelta = pd.to_timedelta(solar_wind.timedelta)\nsolar_wind.set_index([\"period\", \"timedelta\"], inplace=True)","4ee8bef8":"print(\"Dst shape: \", dst.shape)\ndst.head()","c1edffdf":"dst.groupby(\"period\").describe()","d2e87b3c":"print(\"Solar wind shape: \", solar_wind.shape)\nsolar_wind.head()","4f7d9827":"print(\"Sunspot shape: \", sunspots.shape)\nsunspots.head()","dc165567":"solar_wind.groupby(\"period\").describe().T","ebc5468d":"sunspots.groupby(\"period\").describe().T","a59cf7d3":"def show_raw_visualization(data):\n    fig, axes = plt.subplots(nrows=3, ncols=2, figsize=(15, 15), dpi=80)\n    for i, key in enumerate(data.columns):\n        t_data = data[key]\n        ax = t_data.plot(\n            ax=axes[i \/\/ 2, i % 2],\n            title=f\"{key.capitalize()}\",\n            rot=25,\n        )\n\n    fig.subplots_adjust(hspace=0.8)\n    plt.tight_layout()\n\n\ncols_to_plot = [\"bx_gse\", \"bx_gsm\", \"bt\", \"density\", \"speed\", \"temperature\"]\nshow_raw_visualization(solar_wind[cols_to_plot].iloc[:1000])","e4d872e4":"solar_wind.isna().sum()","f90fa875":"corr = solar_wind.join(sunspots).join(dst).fillna(method=\"ffill\").corr()\n\nplt.figure(figsize=(10, 5))\nplt.matshow(corr, fignum=1)\nplt.xticks(range(corr.shape[1]), corr.columns, fontsize=14, rotation=90)\nplt.gca().xaxis.tick_bottom()\nplt.yticks(range(corr.shape[1]), corr.columns, fontsize=14)\n\n\ncb = plt.colorbar()\ncb.ax.tick_params(labelsize=14)\nplt.title(\"Feature Correlation Heatmap\", fontsize=16)\nplt.show()","139f4c72":"from numpy.random import seed\nfrom tensorflow.random import set_seed\n\nseed(2020)\nset_seed(2021)","ac93efc0":"from sklearn.preprocessing import StandardScaler\n\n# subset of solar wind features to use for modeling\nSOLAR_WIND_FEATURES = [\n    \"bt\",\n    \"temperature\",\n    \"bx_gse\",\n    \"by_gse\",\n    \"bz_gse\",\n    \"speed\",\n    \"density\",\n]\n\n# all of the features we'll use, including sunspot numbers\nXCOLS = (\n    [col + \"_mean\" for col in SOLAR_WIND_FEATURES]\n    + [col + \"_std\" for col in SOLAR_WIND_FEATURES]\n    + [\"smoothed_ssn\"]\n)\n\n\ndef impute_features(feature_df):\n    \"\"\"Imputes data using the following methods:\n    - `smoothed_ssn`: forward fill\n    - `solar_wind`: interpolation\n    \"\"\"\n    # forward fill sunspot data for the rest of the month\n    feature_df.smoothed_ssn = feature_df.smoothed_ssn.fillna(method=\"ffill\")\n    # interpolate between missing solar wind values\n    feature_df = feature_df.interpolate()\n    return feature_df\n\n\ndef aggregate_hourly(feature_df, aggs=[\"mean\", \"std\"]):\n    \"\"\"Aggregates features to the floor of each hour using mean and standard deviation.\n    e.g. All values from \"11:00:00\" to \"11:59:00\" will be aggregated to \"11:00:00\".\n    \"\"\"\n    # group by the floor of each hour use timedelta index\n    agged = feature_df.groupby(\n        [\"period\", feature_df.index.get_level_values(1).floor(\"H\")]\n    ).agg(aggs)\n    # flatten hierachical column index\n    agged.columns = [\"_\".join(x) for x in agged.columns]\n    return agged\n\n\ndef preprocess_features(solar_wind, sunspots, scaler=None, subset=None):\n    \"\"\"\n    Preprocessing steps:\n        - Subset the data\n        - Aggregate hourly\n        - Join solar wind and sunspot data\n        - Scale using standard scaler\n        - Impute missing values\n    \"\"\"\n    # select features we want to use\n    if subset:\n        solar_wind = solar_wind[subset]\n\n    # aggregate solar wind data and join with sunspots\n    hourly_features = aggregate_hourly(solar_wind).join(sunspots)\n\n    # subtract mean and divide by standard deviation\n    if scaler is None:\n        scaler = StandardScaler()\n        scaler.fit(hourly_features)\n\n    normalized = pd.DataFrame(\n        scaler.transform(hourly_features),\n        index=hourly_features.index,\n        columns=hourly_features.columns,\n    )\n\n    # impute missing values\n    imputed = impute_features(normalized)\n\n    # we want to return the scaler object as well to use later during prediction\n    return imputed, scaler","cb3150a4":"features, scaler = preprocess_features(solar_wind, sunspots, subset=SOLAR_WIND_FEATURES)\nprint(features.shape)\nfeatures.head()","a52cdc30":"YCOLS = [\"t0\", \"t1\"]\n\n\ndef process_labels(dst):\n    y = dst.copy()\n    y[\"t1\"] = y.groupby(\"period\").dst.shift(-1)\n    y.columns = YCOLS\n    return y\n\n\nlabels = process_labels(dst)\nlabels.head()","4891acf2":"data = labels.join(features)\ndata.head()","b3257221":"def get_train_test_val(data, test_per_period, val_per_period):\n    \"\"\"Splits data across periods into train, test, and validation\"\"\"\n    # assign the last `test_per_period` rows from each period to test\n    test = data.groupby(\"period\").tail(test_per_period)\n    interim = data[~data.index.isin(test.index)]\n    # assign the last `val_per_period` from the remaining rows to validation\n    val = interim.groupby(\"period\").tail(val_per_period)\n    # the remaining rows are assigned to train\n    train = interim[~interim.index.isin(val.index)]\n    return train, test, val\n\n\n","a1b515c8":"train, test, val = get_train_test_val(data, test_per_period=6_000, val_per_period=3_000)","2d70d18c":"ind = [0, 1, 2]\nnames = [\"train_a\", \"train_b\", \"train_c\"]\nwidth = 0.75\ntrain_cnts = [len(df) for _, df in train.groupby(\"period\")]\nval_cnts = [len(df) for _, df in val.groupby(\"period\")]\ntest_cnts = [len(df) for _, df in test.groupby(\"period\")]\n\np1 = plt.barh(ind, train_cnts, width)\np2 = plt.barh(ind, val_cnts, width, left=train_cnts)\np3 = plt.barh(ind, test_cnts, width, left=np.add(val_cnts, train_cnts).tolist())\n\nplt.yticks(ind, names)\nplt.ylabel(\"Period\")\nplt.xlabel(\"Hourly Timesteps\")\nplt.title(\"Train\/Validation\/Test Splits over Periods\", fontsize=16)\nplt.legend([\"Train\", \"Validation\", \"Test\"])","e5bc1f7b":"print(train.shape)\ntrain.head()","988d10c7":"print(test.shape)\ntest.head()","71553e10":"print(val.shape)\nval.head()","308906ba":"from keras import preprocessing\n\n\ndata_config = {\n    \"timesteps\": 34,\n    \"batch_size\": 34,\n}\n\n\ndef timeseries_dataset_from_df(df, batch_size):\n    dataset = None\n    timesteps = data_config[\"timesteps\"]\n\n    # iterate through periods\n    for _, period_df in df.groupby(\"period\"):\n        # realign features and labels so that first sequence of 32 is aligned with the 33rd target\n        inputs = period_df[XCOLS][:-timesteps]\n        outputs = period_df[YCOLS][timesteps:]\n\n        period_ds = preprocessing.timeseries_dataset_from_array(\n            inputs,\n            outputs,\n            timesteps,\n            batch_size=batch_size,\n        )\n\n        if dataset is None:\n            dataset = period_ds\n        else:\n            dataset = dataset.concatenate(period_ds)\n\n    return dataset\n\n\ntrain_ds = timeseries_dataset_from_df(train, data_config[\"batch_size\"])\nval_ds = timeseries_dataset_from_df(val, data_config[\"batch_size\"])\n\nprint(f\"Number of train batches: {len(train_ds)}\")\nprint(f\"Number of val batches: {len(val_ds)}\")","8e1d8fea":"import tensorflow as tf\nfrom tensorflow.keras.layers import Dense, LSTM,GRU, Bidirectional, Dropout, GlobalAveragePooling1D,Input,Concatenate,Flatten,Embedding,Reshape,Conv1D,TimeDistributed,BatchNormalization,GaussianNoise\nfrom tensorflow.keras.models import Sequential\n\n# define our model\nmodel_config = {\"n_epochs\": 30, \"n_neurons\": 192*2 , \"dropout\": 0.0, \"stateful\": False}\n\ninput1 = Input(shape=( data_config[\"timesteps\"], len(XCOLS)), name='x1')\nlstm1= Bidirectional(LSTM(\n        model_config[\"n_neurons\"],\n        stateful=model_config[\"stateful\"],\n        dropout=model_config[\"dropout\"],return_sequences=True\n    ))(input1)\ngru1= Bidirectional(GRU(\n         model_config[\"n_neurons\"] *3,   \n        stateful=model_config[\"stateful\"],\n        dropout=0.0,return_sequences=True\n    ))(lstm1)\n\ngaverage = Flatten() (gru1)\ndense1 = Dense(96)(gaverage)\ndense1 = Dense(128)(dense1)\ndense1 = Dense(64)(dense1)\ndense = Dense(2)(dense1)\n\n\nmodel = keras.models.Model(inputs=input1, outputs=dense)\nmodel.compile(\n    loss='mean_squared_error',\n    optimizer=tf.keras.optimizers.Adam(0.0001),\n)\n\nmodel.summary()","829afc12":"history = model.fit(\n    train_ds,\n    batch_size=data_config[\"batch_size\"],\n    epochs=model_config[\"n_epochs\"],\n    verbose=-1,\n    shuffle=False,\n    validation_data=val_ds,\n)","3765c319":"for name, values in history.history.items():\n    plt.plot(values)","b6fb55e4":"test_ds = timeseries_dataset_from_df(test, data_config[\"batch_size\"])","80e7d7e6":"mse = model.evaluate(test_ds)\nprint(f\"Test RMSE: {mse**.5:.2f}\")","6e73bb49":"import json\nimport pickle\n\nmodel.save(\"model\")\n\nwith open(\"scaler.pck\", \"wb\") as f:\n    pickle.dump(scaler, f)\n\ndata_config[\"solar_wind_subset\"] = SOLAR_WIND_FEATURES\nprint(data_config)\nwith open(\"config.json\", \"w\") as f:\n    json.dump(data_config, f)","0f2750b4":"model = keras.models.load_model(\"model\")\n","289e7ef6":"%%writefile predict.py\n\nimport json\nimport pickle\nfrom typing import Tuple\n\nimport keras\nimport numpy as np\nimport pandas as pd\n\n# Load in serialized model, config, and scaler\nmodel = keras.models.load_model(\"model\")\n\nwith open(\"config.json\", \"r\") as f:\n    CONFIG = json.load(f)\n\nwith open(\"scaler.pck\", \"rb\") as f:\n    scaler = pickle.load(f)\n\n# Set global variables    \nTIMESTEPS = CONFIG[\"timesteps\"]\nSOLAR_WIND_FEATURES = [\n    \"bt\",\n    \"temperature\",\n    \"bx_gse\",\n    \"by_gse\",\n    \"bz_gse\",\n    \"speed\",\n    \"density\",\n]\nXCOLS = (\n    [col + \"_mean\" for col in SOLAR_WIND_FEATURES]\n    + [col + \"_std\" for col in SOLAR_WIND_FEATURES]\n    + [\"smoothed_ssn\"]\n)\n\n\n# Define functions for preprocessing\ndef impute_features(feature_df):\n    \"\"\"Imputes data using the following methods:\n    - `smoothed_ssn`: forward fill\n    - `solar_wind`: interpolation\n    \"\"\"\n    # forward fill sunspot data for the rest of the month\n    feature_df.smoothed_ssn = feature_df.smoothed_ssn.fillna(method=\"ffill\")\n    # interpolate between missing solar wind values\n    feature_df = feature_df.interpolate()\n    return feature_df\n\n\ndef aggregate_hourly(feature_df, aggs=[\"mean\", \"std\"]):\n    \"\"\"Aggregates features to the floor of each hour using mean and standard deviation.\n    e.g. All values from \"11:00:00\" to \"11:59:00\" will be aggregated to \"11:00:00\".\n    \"\"\"\n    # group by the floor of each hour use timedelta index\n    agged = feature_df.groupby([feature_df.index.floor(\"H\")]).agg(aggs)\n\n    # flatten hierachical column index\n    agged.columns = [\"_\".join(x) for x in agged.columns]\n    return agged\n\n\ndef preprocess_features(solar_wind, sunspots, scaler=None, subset=None):\n    \"\"\"\n    Preprocessing steps:\n        - Subset the data\n        - Aggregate hourly\n        - Join solar wind and sunspot data\n        - Scale using standard scaler\n        - Impute missing values\n    \"\"\"\n    # select features we want to use\n    if subset:\n        solar_wind = solar_wind[subset]\n\n    # aggregate solar wind data and join with sunspots\n    hourly_features = aggregate_hourly(solar_wind).join(sunspots)\n\n    # subtract mean and divide by standard deviation\n    if scaler is None:\n        scaler = StandardScaler()\n        scaler.fit(hourly_features)\n\n    normalized = pd.DataFrame(\n        scaler.transform(hourly_features),\n        index=hourly_features.index,\n        columns=hourly_features.columns,\n    )\n\n    # impute missing values\n    imputed = impute_features(normalized)\n\n    # we want to return the scaler object as well to use later during prediction\n    return imputed, scaler\n\n\n# THIS MUST BE DEFINED FOR YOUR SUBMISSION TO RUN\ndef predict_dst(\n    solar_wind_7d: pd.DataFrame,\n    satellite_positions_7d: pd.DataFrame,\n    latest_sunspot_number: float,\n) -> Tuple[float, float]:\n    \"\"\"\n    Take all of the data up until time t-1, and then make predictions for\n    times t and t+1.\n    Parameters\n    ----------\n    solar_wind_7d: pd.DataFrame\n        The last 7 days of satellite data up until (t - 1) minutes [exclusive of t]\n    satellite_positions_7d: pd.DataFrame\n        The last 7 days of satellite position data up until the present time [inclusive of t]\n    latest_sunspot_number: float\n        The latest monthly sunspot number (SSN) to be available\n    Returns\n    -------\n    predictions : Tuple[float, float]\n        A tuple of two predictions, for (t and t + 1 hour) respectively; these should\n        be between -2,000 and 500.\n    \"\"\"\n    # Re-format data to fit into our pipeline\n    sunspots = pd.DataFrame(index=solar_wind_7d.index, columns=[\"smoothed_ssn\"])\n    sunspots[\"smoothed_ssn\"] = latest_sunspot_number\n    \n    # Process our features and grab last 32 (timesteps) hours\n    features, s = preprocess_features(\n        solar_wind_7d, sunspots, scaler=scaler, subset=SOLAR_WIND_FEATURES\n    )\n    model_input = features[-TIMESTEPS:][XCOLS].values.reshape(\n        (1, TIMESTEPS, features.shape[1])\n    )\n    \n    # Make a prediction\n    prediction_at_t0, prediction_at_t1 = model.predict(model_input)[0]\n\n    # Optional check for unexpected values\n    if not np.isfinite(prediction_at_t0):\n        prediction_at_t0 = -12\n    if not np.isfinite(prediction_at_t1):\n        prediction_at_t1 = -12\n\n    return prediction_at_t0, prediction_at_t1","1c30e230":"![](https:\/\/drivendata-public-assets.s3.amazonaws.com\/noaa-cover-img.png)","eb49e94a":"# SPLITTING THE DATA","aa742060":"# EVALUATION","e6fd67ec":"# FEATURE GENERATION","fb807b1c":"# DESIGNING AN LSTM","154488ca":"**The goal of this challenge was to develop models for forecasting Dst that**\n\n1) push the boundary of predictive performance.\n\n2) under operationally viable constraints.\n\n3) using specified real-time solar-wind data feeds.\n\nThis is a hard problem where the best approaches are not evident at the outset. Competitors were tasked with improving forecasts both for the current Dst value (t0) and Dst one hour in the future (t1). Participants needed to submit code that could execute in a simulated real-time environment with operations constraints on runtime and programming inputs.","5177fe46":"# PREPARING FOR SUBMISSION","b9c43fa2":"# BUILDING OUR MODEL","1323e22b":"# DATA EXPLORATION","cabfa900":"Model Can be found at : [BiGRU](https:\/\/github.com\/drivendataorg\/magnet-geomagnetic-field\/tree\/main\/1st_Place) by : [Ammarali32](https:\/\/www.drivendata.org\/users\/Ammarali32\/)","46fdef4a":"# TRAIN MODEL","30f04fd0":"# MagNet: Model the Geomagnetic Field\n\n","48ebc43b":"# GETTING SET UP"}}