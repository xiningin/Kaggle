{"cell_type":{"87abe2bc":"code","99e9a097":"code","555c328b":"code","c2a1729a":"code","f226226f":"code","765448c1":"code","76199f25":"code","c2ab6f4c":"code","d74c7693":"code","d6288bc7":"code","89a52000":"code","8642057e":"code","a03b2a54":"code","2089d5a2":"code","b92e33ed":"code","f41e32b6":"markdown","16edd54a":"markdown","62ee0b6b":"markdown","0e4a509d":"markdown","b5502dc2":"markdown","fde0f230":"markdown","70a2e475":"markdown","8ac449d0":"markdown","9eea7d98":"markdown","f8e854a5":"markdown","3884ea76":"markdown","13d471ce":"markdown","3ee6aa70":"markdown"},"source":{"87abe2bc":"import numpy as np\nimport pandas as pd\nimport keras as K\nimport random\nimport sqlite3\n\nfrom keras.layers import Input, Dropout, Dense, concatenate, Embedding\nfrom keras.layers import Flatten, Activation\nfrom keras.optimizers import Adam\nfrom keras.models import Model\nfrom keras.utils import np_utils\n\nfrom keras.preprocessing import sequence\nfrom keras.models import Sequential\nfrom keras.models import load_model\nfrom keras.layers import LSTM, CuDNNGRU, CuDNNLSTM\nfrom keras.layers import MaxPooling1D\nfrom keras.callbacks import EarlyStopping, ModelCheckpoint, Callback\n\nimport warnings\nwarnings.filterwarnings('ignore')\nimport os\nprint(os.listdir(\"..\/input\"))","99e9a097":"df = pd.read_csv('..\/input\/scripts.csv')\n\ndf.Character = df.Character.astype(str)\ndf.Dialogue = df.Dialogue.astype(str)\ndf = df[[\"Character\",\"Dialogue\"]]\n\ndf[:10]","555c328b":"%%time\nAll_Seinfeld_Scripts = ''\nlast_seg = ''\n\nfor line in df.values:\n#     print(line[0])\n#     print(line[2])\n    character = line[0].lower()\n    dialogue = line[1].lower()\n    script = character+\": \"+dialogue+\" \\n\\n\"\n    All_Seinfeld_Scripts += script\n\nprint(All_Seinfeld_Scripts[:2000])","c2a1729a":"text_file = open(\"All_Seinfeld_Scripts.txt\", \"w\")\ntext_file.write(All_Seinfeld_Scripts)\ntext_file.close()","f226226f":"Text_Data = All_Seinfeld_Scripts.lower()\n\nif len(Text_Data) > 500000:\n    Text_Data = Text_Data[:500000]\n\ncharindex = list(set(Text_Data))\ncharindex.sort() \nprint(charindex)\n\nnp.save(\"charindex.npy\", charindex)","765448c1":"%%time\nCHARS_SIZE = len(charindex)\nSEQUENCE_LENGTH = 75\nX_train = []\nY_train = []\nfor i in range(0, len(Text_Data)-SEQUENCE_LENGTH, 1 ): \n    X = Text_Data[i:i + SEQUENCE_LENGTH]\n    Y = Text_Data[i + SEQUENCE_LENGTH]\n    X_train.append([charindex.index(x) for x in X])\n    Y_train.append(charindex.index(Y))\n\nX_train = np.reshape(X_train, (len(X_train), SEQUENCE_LENGTH))\n\nY_train = np_utils.to_categorical(Y_train)","76199f25":"def get_model():\n    model = Sequential()\n    inp = Input(shape=(SEQUENCE_LENGTH, ))\n    x = Embedding(CHARS_SIZE, 75, trainable=False)(inp)\n    x = CuDNNLSTM(512, return_sequences=True,)(x)\n    x = CuDNNLSTM(512, return_sequences=True,)(x)\n    x = CuDNNLSTM(512,)(x)\n    x = Dense(256, activation=\"elu\")(x)\n    x = Dense(128, activation=\"elu\")(x)\n    outp = Dense(CHARS_SIZE, activation='softmax')(x)\n    \n    model = Model(inputs=inp, outputs=outp)\n    model.compile(loss='categorical_crossentropy',\n                  optimizer=Adam(lr=0.001),\n                  metrics=['accuracy'],\n                 )\n\n    return model\n\nmodel = get_model()\n\nmodel.summary()","c2ab6f4c":"filepath=\"model_checkpoint.hdf5\"\n\ncheckpoint = ModelCheckpoint(filepath,\n                             monitor='loss',\n                             verbose=1,\n                             save_best_only=True,\n                             mode='min')\n\nearly = EarlyStopping(monitor=\"loss\",\n                      mode=\"min\",\n                      patience=1)","d74c7693":"class TextSample(Callback):\n\n    def __init__(self):\n       super(Callback, self).__init__() \n\n    def on_epoch_end(self, epoch, logs={}):\n        pattern = X_train[700]\n        outp = []\n        seed = [charindex[x] for x in pattern]\n        sample = 'TextSample:' +''.join(seed)+'|'\n        for t in range(100):\n          x = np.reshape(pattern, (1, len(pattern)))\n          pred = self.model.predict(x)\n          result = np.argmax(pred)\n          outp.append(result)\n          pattern = np.append(pattern,result)\n          pattern = pattern[1:len(pattern)]\n        outp = [charindex[x] for x in outp]\n        outp = ''.join(outp)\n        sample += outp\n        print(sample)\n\ntextsample = TextSample()","d6288bc7":"# model = load_model(filepath)","89a52000":"model_callbacks = [checkpoint, early, textsample]\nmodel.fit(X_train, Y_train,\n          batch_size=256,\n          epochs=26,\n          verbose=2,\n          callbacks = model_callbacks)","8642057e":"# model = load_model(filepath)\nmodel.save_weights(\"full_train_weights.hdf5\")\nmodel.save(\"full_train_model.hdf5\")","a03b2a54":"\n%%time\nTEXT_LENGTH  = 5000\nLOOPBREAKER = 3\n\n\nx = np.random.randint(0, len(X_train)-1)\npattern = X_train[x]\noutp = []\nfor t in range(TEXT_LENGTH):\n  if t % 500 == 0:\n    print(\"%\"+str((t\/TEXT_LENGTH)*100)+\" done\")\n  \n  x = np.reshape(pattern, (1, len(pattern)))\n  pred = model.predict(x, verbose=0)\n  result = np.argmax(pred)\n  outp.append(result)\n  pattern = np.append(pattern,result)\n  pattern = pattern[1:len(pattern)]\n  ####loopbreaker####\n  if t % LOOPBREAKER == 0:\n    pattern[np.random.randint(0, len(pattern)-10)] = np.random.randint(0, len(charindex)-1)","2089d5a2":"outp = [charindex[x] for x in outp]\noutp = ''.join(outp)\n\nprint(outp)","b92e33ed":"f =  open(\"output_text_sample.txt\",\"w\")\nf.write(outp)\nf.close()","f41e32b6":"# Pythonic Python Script for Making *Seinfeld* Scripts\n(This notebook is a fork of my much better named \"Pythonic Python Script for Making Monty Python Scripts\" kernal. This is to show that the text generator will work just as well by simply changing the text input. The after the Changing Database to Text section, this notebook is the same. Check it out here: https:\/\/www.kaggle.com\/valkling\/pythonicpythonscript4makingmontypythonscripts) \n\nNow we can also generate an unlimited supply of new Seinfeld scripts! This notebook is a compressed version of my text generating AI. Text generator like this one require a lot of computational power so it only became really feasible to do them on Kaggle Kernels when they upgraded to have a GPU and a 6 hour computational limit. Even so, 6 hours is still kind of lean for an LSTM text generator but we can make it work quite well anyways. \n\nThe goal of this notebook is to serve as a introduction to text generation NLPs. These LSTM text generator are actually not that difficult to make. However, most tutorials on the topic are incomplete and\/or generate poor results. I'll try to talk about every step of the process thoroughly and clearly. Other than that, this notebook is pretty easy to adapt to any text generation you might want to do. Just pop in any sizeable txt file and the model will learn to make more text in that style. Things like Shakespeare are common and work well for this type of text generation. Make sure that GPU is enabled in settings. Now lets make an AI generate something completely different.\n\n## Imports\nAs always, a block of imports.\n","16edd54a":"Now that we have our scripts, let's save it and move on to real work.","62ee0b6b":"# Generating New Seinfeld Scripts\nThis block generates new text in the style of the input text of TEXT_LENGTH size in characters. It takes a random seed pattern from the training set, predicts the next character, adds it to the end of the pattern, then drops the first character of the pattern and predicts on the new pattern and so forth.\n\nPretty much this text generator *tries* to accurately duplicate the Seinfeld script but inevitably makes errors ,and those errors compound, but is still trained well enough that it ends up making Seinfeld *like* scripts \n\n## The Loopbreaker\nThis is simple bit of I came up with while putting this together. Every so many character predictions, the program just changes one of the characters in the pattern to predict on (except the last few, to prevent spelling errors). This causes our model to perceive a slightly different text which causes it to change it's overall predictions slightly too. Without this, even a well trained model might start to repeat itself at some point and get caught in a loop. The loopbreaker can even prevent overfitting or allow under trained models to perform much better. Without a loopbreaker like this, models will need to be trained for many more hours before they can function without looping in on themselves.\n\nChanging this value up and down an interesting way to significantly change the output. Setting it high will have more repeated speech, slightly lower might get many line starting the same then vering off into different directions, really low will get lots of varied text but line structures and format might become unstable. Probably keep it somewhere between 1 and 10.\n","0e4a509d":"# Create Sequences\nIn a nutshell, this model will look at the last 75 characters in the script and attempt to predict the 76th. Our X variable will be a 75 character sequence and our Y variable will be the 76th character. This block chops the text data into such sequences of characters. \n\nNote that this part also tokenizes the characters, which is to say it replaces each character with a number that corresponds to it's index in charindex. This is why it is important to save a copy of the charindex with your model just in case. We will need it to decode our predictions later.\n","b5502dc2":"# Checkpoints and Custom Callback\nWe will use 3 callbacks. Checkpoint, EarlyStopping, and a custom TextSample callback. Text sample prints a sample line at the end of every epoch to see how the model is progressing each epoch. For Kaggle, this is less important as you have to commit your code to run this long enough to output results.","fde0f230":"\n# Train Model\nEven with a GPU, this can take a while. As is, I'm setting this notebook to take almost the full 6 hour limit. I have played around with training these types of models for 12 or even 24 hours wit more layers.  However, usually if gotten to roughly around 1.0 loss the generator is good enough to go. Can train almost indefinitely on most models. We are not *really* worried about overfitting. Hypothetically, if the loss gets too low the text might become overfit, which in this case means just copying the text in the most inefficient way. However, it should take an unrealistically long time to get to that point (or maybe just impossible).\n","70a2e475":"# Changing Database into Text\nThe first thing that needs to be done is change the Monty Python database into one nice long string of all the scripts. Originally, I tried putting both the directions and dialogue into the text. However, since there are so many directions this ends up making a script that is mostly stuff like \"cut to a picture of a man in the street.\" or \"cut to stock video of a train\" ect. We will just focus on the dialogue to make something more interesting to read. ","8ac449d0":"# Prep the Text for the RNN\nNext we will prepare an index of every unique character in our text. We are only getting rid of capitalization for simplicity, but still keeping all special characters. This will give us an output that retains the punctuation and format of the original. \n\nNote that if you want to replace the Seinfeld scripts with some other text to duplicate, here would be the place to do it. Just replace the All_Seinfeld_Scripts with any other text file and the rest of the notebook will run the same. (the bigger the better, anything ~1MB+ is great) \n\nWhile we could work with the every Seinfeld script, it ends up being a lot of data to go through within the time limit. As such, I added an if block to limit the text data to just the first half million characters. One of the goals of this notebook is to have an almost general AI that can learn to generate any text, with as little input from humans as possible, in a 6 hour time limit. Using more text and training longer is a valid option for improving the output with more training time.","9eea7d98":"# Let's See the Results\nAs you can see, the output is not bad. Text generators like this are pretty good on a line by line basis. Some of the lines seem really plausible as Seinfeld dialogue. Plot and scene structure is off. Different characters show up talking about irrelevant things. In some ways that works comedy. Still, more AI structures are needed to keep track of the plot and such. Anyways, this is the extent of most AI text generation these days without more structured custom code.\n\nFinal thought: Seinfeld is kind of a tough text to generate from. As a general rule, text generators work better the more abstract the language.  Well, not neccarily *better* just that abstract language makes the imperfect text generation less detectable. This is why stuff like Shakespeare and poetry are popular for AI generation (Also why jazz is popular for AI music generators). People have trouble detecting flaws in less everyday language.\n","f8e854a5":"\n# Load Model\nLoad models or weights here. For following up on partially trained models saved by checkpoint. If you want more training time than the 6 hour limit while still using Kaggle, upload the full_train_model.hdf5 output to a private dataset and load it here. Now I *could* do that myself but we are seeing how good we can get it in 6 hours.","3884ea76":"# Create the Model\nThe model uses 3 LSTMs stacked on top of each. Adding another LSTM layer and\/or running it a lot longer or in multiple session will give better results. However, the 3 LSTM should do fine in 6 hour and adding the loopbreaker to our code later will make even under trained models give good results. Also note that we are using CuDNNLSTMs. If you don't know what that is, it is a special LSTM layer specially made for NIVDA GPUs. These function the same as regular LSTM layers but are automatically optimised for the GPU. You lose some customization with these layers but they work roughly twice as fast as regular LSTMs layers if conditions are right.\n","13d471ce":"# Save Text Output","3ee6aa70":"# Save the Model\nTraining is done, save it. This is also a great place to load any pretrained models before generating new text."}}