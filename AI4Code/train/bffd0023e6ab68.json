{"cell_type":{"83032669":"code","b97c05ad":"code","51848a09":"code","a953afec":"code","ec363f6a":"code","43431988":"code","b9efe0c1":"code","24e7a2a2":"code","605dd286":"code","480b27e2":"code","07d5e0e8":"code","2d684646":"code","66583d24":"code","c56e86e6":"code","3842f1e6":"code","6de93c64":"code","0b43d047":"code","a4599d0d":"code","24b20ea5":"code","afa475aa":"code","e559b868":"markdown","7a88a9fb":"markdown","4138e4d4":"markdown","1b59db31":"markdown","917b76a5":"markdown","cbdcff02":"markdown","0534d914":"markdown","ba4e660d":"markdown","c7078578":"markdown","0ed9c9e0":"markdown"},"source":{"83032669":"import numpy as np\nimport pandas as pd\n\nfrom sklearn.metrics.pairwise import cosine_similarity\nfrom sklearn.feature_extraction.text import CountVectorizer\n\ndata = pd.read_csv('..\/input\/goodbooks-10k\/books.csv')\ndata.head()","b97c05ad":"data.columns","51848a09":"data.isnull().sum()","a953afec":"#Extract relevant columns that would influence a book's rating based on book title. \nbooks_title = data[['book_id', 'title']]\nbooks_title.head()","ec363f6a":"#Lets vectorize all these titles\nfrom sklearn.feature_extraction.text import CountVectorizer\n\n#initialize vectorizer\nvect = CountVectorizer(analyzer = 'word', ngram_range = (1,2), stop_words = 'english', min_df = 0.002) #min_df = rare words, max_df = most used words\n#ngram_range = (1,2) - if used more than  1(value), lots of features or noise\n\n#Fit into the title\nvect.fit(books_title['title'])\ntitle_matrix = vect.transform(books_title['title'])\ntitle_matrix.shape","43431988":"#Lets find vocabulary\/features\nfeatures = vect.get_feature_names()\nfeatures","b9efe0c1":"from sklearn.metrics.pairwise import cosine_similarity\ncosine_sim_titles = cosine_similarity(title_matrix, title_matrix)\ncosine_sim_titles.shape","24e7a2a2":"#Get books which are similar to a given title\ntitle_id = 100\nbooks_title['title'].iloc[title_id]","605dd286":"#Find out what features have been considered  by the vectorizer for a given title ?\nfeature_array = np.squeeze(title_matrix[title_id].toarray()) #squeeze activity matrix into array\nidx = np.where(feature_array > 0)\nidx[0]\n[features[x] for x in idx[0]]","480b27e2":"# Find index of feature\nidx[0]","07d5e0e8":"#Cosine similarity with other similar titles\nn = 15 #how many books to be recommended\ntop_n_idx = np.flip(np.argsort(cosine_sim_titles[title_id,]), axis = 0)[0:n]\ntop_n_sim_values = cosine_sim_titles[title_id, top_n_idx]\ntop_n_sim_values","2d684646":"#find top n with values > 0\ntop_n_idx = top_n_idx[top_n_sim_values > 0]\n#Matching books\nbooks_title['title'].iloc[top_n_idx]","66583d24":"# lets wrap the above code in a function\ndef return_sim_books(title_id, title_matrix, vectorizer, top_n = 10):\n    \n    # generate sim matrix\n    sim_matrix = cosine_similarity(title_matrix, title_matrix)\n    features = vectorizer.get_feature_names()\n\n    top_n_idx = np.flip(np.argsort(sim_matrix[title_id,]),axis=0)[0:top_n]\n    top_n_sim_values = sim_matrix[title_id, top_n_idx]\n    \n    # find top n with values > 0\n    top_n_idx = top_n_idx[top_n_sim_values > 0]\n    scores = top_n_sim_values[top_n_sim_values > 0]\n    \n    \n    # find features from the vectorized matrix\n    sim_books_idx = books_title['title'].iloc[top_n_idx].index\n    words = []\n    for book_idx in sim_books_idx:\n        try:\n            feature_array = np.squeeze(title_matrix[book_idx,].toarray())\n        except:\n            feature_array = np.squeeze(title_matrix[book_idx,])\n        idx = np.where(feature_array > 0)\n        words.append([\" , \".join([features[i] for i in idx[0]])])\n        \n    # collate results\n    res = pd.DataFrame({\"book_title\" : books_title['title'].iloc[title_id],\n           \"sim_books\": books_title['title'].iloc[top_n_idx].values,\"words\":words,\n           \"scores\":scores}, columns = [\"book_title\",\"sim_books\",\"scores\",\"words\"])\n    \n    \n    return res","c56e86e6":"vect = CountVectorizer(analyzer='word',ngram_range=(1,2),stop_words='english', min_df = 0.001)\nvect.fit(books_title['title'])\ntitle_matrix = vect.transform(books_title['title'])\nprint(books_title['title'][10])\nreturn_sim_books(10,title_matrix,vect,top_n=10)","3842f1e6":"from sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.metrics.pairwise import linear_kernel\n\ntf = TfidfVectorizer(analyzer = 'word', ngram_range = (1,2), min_df = 0, stop_words = 'english')\ntfidf_matrix = tf.fit_transform(books_title['title'])\ncosine_sim = cosine_similarity(tfidf_matrix, tfidf_matrix)\ncosine_sim","6de93c64":"titles = books_title['title']\nindices = pd.Series(books_title.index, index = books_title['title']) #converting all titles into a Series\n\n#Function that gets book recommendations based on the cosine similarity score of book titles\ndef book_recommendations(title, n):\n    idx = indices[title]\n    sim_scores = list(enumerate(cosine_sim[idx]))\n    sim_scores = sorted(sim_scores, key = lambda x:x[1], reverse = True)\n    sim_scores = sim_scores[1:n+1]\n    book_indices = [i[0] for i in sim_scores]\n    return titles.iloc[book_indices]","0b43d047":"#Recommend n books for a book having index 1\nbook_index = 10\nn = 20\n\nprint(books_title['title'][book_index])\nbook_recommendations(books_title.title[book_index],n)","a4599d0d":"book_recommendations('A Tale of Two Cities',3)","24b20ea5":"my_list = pd.read_csv('..\/input\/books-read\/books_read.csv')\nmy_list","afa475aa":"result = pd.DataFrame()\nn = 3 #Recommend top n similar books\nprint('Recommended books: ')\nfor i in my_list['Books read']:\n    output = pd.DataFrame(book_recommendations(i,n))\n    result = result.append(output)\nresult = result.drop_duplicates()\nresult","e559b868":"# Count Vectorizer\nConvert a collection of text documents to a matrix of token counts. It's a data table that is obtained after normalization of next-generation sequencing data.\n\n![image.png](attachment:image.png)\n\n*For more info -> [Count Vectorizer](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.feature_extraction.text.CountVectorizer.html)*\n\n**Things to do:**\n\n* Initialize & Fit CountVectorizer into 'title' -> to create count_matrix this is useful for cosine similarity\n* Check all words\/ features in the vocabulary\n* Generate cosine similarity","7a88a9fb":"*Special credits to -> Bhushan's [kernel](https:\/\/www.kaggle.com\/bshirude2\/goodreads-content-based-book-recommendation) it uses Tfidf vectorizer for book recommendation* \n\n# Summary & Model deployment\n\nAs we infer, Tfidf vectorizer gives better results, I tried extracting keywords using Rake but it did not give right outputs. I'll add in Rake later. \n\nThings to keep in mind while pushing this code for production,\n\n* How would you provide the training inputs *(read book index number\/ title)* as the dataset needs to be regularly updated with latest books\n* How would you input a user's list of books he read\n* User's list of books may not be in the same format\/ title as your input data. You will have to look for ways like converting everything to .lower() or convert the books_read csv into a format that matches input data.\n\n![image.png](attachment:image.png)","4138e4d4":"# Cosine Similarity between Titles\n\n**Things to do:**\n* Initialize Cosine similarity into title matrix\n* Extract features from book title\n* Using Cosine similarity between this title and all other titles to be recommended the top 10 books ","1b59db31":"# Data Preprocessing\n\n**Things to do:**\n\n* Extract only relevant columns (the ones having text we need)\n* Check if there are null values in extracted columns","917b76a5":"# Import packages and dataset","cbdcff02":"**Using CountVectorizer we could extract text from book titles and recommend similar titles. A limitation I came across was for titles like \"The Kite Runner\" & \"The Dinner\". It was unable to recommend similar books because of the rare words like kite and dinner in the dataset so we have to look at other better ways for recommendations.**\n\n*Further steps, I will use Tf-Idf and Rake to see which of these give us better results.*\n\n# Using Tf-Idf Vectorizer\n\nHere we consider overall document weightage of a word, useful while dealing with most frequent words\n\n![image.png](attachment:image.png)\n\n**Things to do:**\n* Initialize Tfidf vectorizer & fit into title column\n* Calculate cosine similarity \n* Convert all titles into a Series associated with book index numbers\n* Function that gets book recommendations based on the cosine similarity score of book titles\n","0534d914":"The total no. of features we could extract are 261 due to min_df threshold we applied.\n\n\n**What happens when we change the value of min_df value? What is this argument controlling?**\n\nWhen building the vocabulary ignore terms that have a document frequency strictly lower than the given threshold. This value is also called cut-off in the literature. If float, the parameter represents a proportion of documents, integer absolute counts. This parameter is ignored if vocabulary is not None.\n\nIf we repeat the above steps from while changing the min_df to 0.001, the threshold level is reduced and further more words will be added to features i.e 704 words instead of 261 earlier\n","ba4e660d":"# Recommend similar books based on a list of books read\n\nInput list of books","c7078578":"## For content based book recommendation we have to use NLP techniques like\n\n* Keyword extraction -> Extract keywords from title\n* Cosine Similarity -> Find cosine similarity between all movie titles\n\n# Keyword Extraction\nKeyword extraction is automatic detection of terms that best describe the subject of a document. \n\n*For more info -> [Wiki](https:\/\/en.wikipedia.org\/wiki\/Keyword_extraction) & [Keyword Extraction](https:\/\/monkeylearn.com\/keyword-extraction\/)*\n\n**For keyword extraction we use one of the below based on our need,**\n\n* **CountVectorizer** - Provides a simple way to both tokenize a collection of text documents and build a vocabulary of known words, but also to encode new documents using that vocabulary.\n\n\n* **Tf-Idf Vectorizer** - Tf-Idf finds the no. of times a word occurs in a document and then compares this count to the no. of times the word shows up in a bunch of other documents in a collection. It then generates a rank for each word where it is important to a document if it shows up a lot in that particular document but doesn\u2019t show up a lot in all the other documents.\n\n\n* **Rake** - Rake is used when you want to extract keywords without any specific context *(though it does use a generalized set of stopwords)*\n\n*Wonderful articles -> [Comparison of Rake & Tf-Idf algorithms](https:\/\/nzmattgrant.wordpress.com\/2018\/01\/31\/a-comparison-of-rake-and-tf-idf-algorithms-for-finding-keywords-in-text\/) & [CountVectorizer vs Tf-Idf Vectorizer](https:\/\/machinelearningmastery.com\/prepare-text-data-machine-learning-scikit-learn\/)* \n\nWe will use CountVectorizer as we want to extract keywords just from book titles.","0ed9c9e0":"# What is Recommendation System ?\nRecommender\/recommendation system is a subclass of information filtering system that seeks to predict the rating\/ preference a user would give to an item.\n\nThey are primarily used in applications where a person\/ entity is involved with a product\/ service. To further improve their experience with this product, we try to personalize it to their needs. For this we have to look up at their past interactions with this product.\n\nIn one line -> **Specialized content for everyone.**\n\nFor further info, [Wiki](https:\/\/en.wikipedia.org\/wiki\/Recommender_system#:~:text=A%20recommender%20system%2C%20or%20a,would%20give%20to%20an%20item.)\n\n**Types of Recommender System**\n* 1). [Popularity Based](https:\/\/www.kaggle.com\/sasha18\/popularity-based-movie-recommendation)\n* 2). Classification Based\n* 3). [Content Based](https:\/\/www.kaggle.com\/sasha18\/recommend-books-using-count-tfidf-on-titles)\n* 4). Collaborative Based\n* 5). [Hybrid Based (Content + Collaborative)](https:\/\/www.kaggle.com\/sasha18\/recommend-top-restaurants-based-on-preference)\n* 6). [Association Based Rule Mining](https:\/\/www.kaggle.com\/sasha18\/perform-market-basket-analysis-with-e-comm-data)\n\n# Content based recommender system\n\nRecommends content based on product description. Here we would recommend books based on book titles only. Similar books would have similar names thus having a high cosine similarity. \n\nThis code is a short version of [Content Based Movie Recommendation using NLP](https:\/\/www.kaggle.com\/sasha18\/content-based-movie-recommendation-using-nlp) as the Movie dataset had many features like movie description, director, actor that would help to better recommend a movie. \n\n![image.png](attachment:image.png)\n\n**Here is to recommend a good book**"}}