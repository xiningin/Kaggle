{"cell_type":{"d16f7368":"code","96b8219a":"code","376bd632":"code","770c9c6d":"code","5e523a17":"code","c86f8b7c":"code","b93ef6db":"code","738ce968":"code","3e724e79":"code","0441529f":"code","fd7aff2d":"code","62601667":"code","d161cc42":"code","b489a116":"code","f66aee1c":"code","1341a290":"code","829fe389":"code","de99a23f":"code","d0ade8b1":"code","23ce2521":"code","ab779f0a":"code","878c7bc3":"code","bb024a24":"code","fbe4bf97":"code","431fe1f7":"code","ba56e3a1":"code","8b82fce7":"code","84683c64":"code","00d9a345":"code","50223a61":"code","854c0779":"code","65d17067":"code","fe48ae46":"code","2cb23d55":"code","2bf3c027":"code","6294edea":"code","064759a7":"code","bb430085":"code","793dfd7a":"code","ebbed2be":"code","8bfca51f":"markdown","a8199042":"markdown","89df6919":"markdown","d4ded43a":"markdown","a197520d":"markdown","9f20835a":"markdown","9718ba57":"markdown","d0c28212":"markdown","9bd449d4":"markdown","6e150cb1":"markdown","b02eb6e4":"markdown","0a55da21":"markdown","0bfe3ca8":"markdown","5117f560":"markdown","0d613235":"markdown","8a95f291":"markdown","86ae8e0e":"markdown","78dde365":"markdown","b77934ad":"markdown","8754b729":"markdown","60b59044":"markdown","61b84152":"markdown","0e543c1f":"markdown","c399d42a":"markdown","a5d88b70":"markdown","b331f630":"markdown","20cbafac":"markdown","8d4a5573":"markdown","aee0930b":"markdown","f079e146":"markdown","c22df854":"markdown","91783a95":"markdown","bb449999":"markdown","b00bc744":"markdown","d3598d75":"markdown","1a9aba15":"markdown","6edb1a73":"markdown","1eb4bee7":"markdown"},"source":{"d16f7368":"import os\nimport joblib\nimport numpy as np\nimport pandas as pd\n\nimport matplotlib\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\npd.set_option('display.max_rows', None)\npd.set_option('display.max_columns', None)","96b8219a":"train_df = pd.read_csv('\/kaggle\/input\/tabular-playground-series-mar-2021\/train.csv')\ntest_df = pd.read_csv('\/kaggle\/input\/tabular-playground-series-mar-2021\/test.csv')","376bd632":"cat_features = [feature for feature in train_df.columns if 'cat' in feature]\ncont_features = [feature for feature in train_df.columns if 'cont' in feature]","770c9c6d":"print('Rows and Columns in train dataset:', train_df.shape)\nprint('Rows and Columns in test dataset:', test_df.shape)","5e523a17":"print('Missing values in train dataset:', sum(train_df.isnull().sum()))\nprint('Missing values in test dataset:', sum(test_df.isnull().sum()))","c86f8b7c":"train_df.head()","b93ef6db":"test_df.head()","738ce968":"fig = plt.figure(figsize=(15, 10), facecolor='#f6f5f5')\ngs = fig.add_gridspec(4, 4)\ngs.update(wspace=0.2, hspace=0.05)\n\nbackground_color = \"#f6f5f5\"\n\nrun_no = 0\nfor col in range(0, 4):\n    for row in range(0, 3):\n        locals()[\"ax\"+str(run_no)] = fig.add_subplot(gs[row, col])\n        locals()[\"ax\"+str(run_no)].set_facecolor(background_color)\n        locals()[\"ax\"+str(run_no)].set_yticklabels([])\n        locals()[\"ax\"+str(run_no)].tick_params(axis='y', which=u'both',length=0)\n        for s in [\"top\",\"right\", 'left']:\n            locals()[\"ax\"+str(run_no)].spines[s].set_visible(False)\n        run_no += 1\n\nax0.text(-0.15, 4.5, 'Continuous Features Distribution on Train Dataset', fontsize=20, fontweight='bold', fontfamily='serif')\nax0.text(-0.15, 4, 'Continuous features have multimodal', fontsize=13, fontweight='light', fontfamily='serif')        \n\nrun_no = 0\nfor col in cont_features:\n    sns.kdeplot(train_df[col], ax=locals()[\"ax\"+str(run_no)], shade=True, color='#f088b7', alpha=0.9, zorder=2)\n    locals()[\"ax\"+str(run_no)].grid(which='major', axis='x', zorder=0, color='gray', linestyle=':', dashes=(1,5))\n    locals()[\"ax\"+str(run_no)].set_ylabel(col, fontsize=10, fontweight='bold').set_rotation(0)\n    locals()[\"ax\"+str(run_no)].yaxis.set_label_coords(1, 0)\n    locals()[\"ax\"+str(run_no)].set_xlim(-0.2, 1.2)\n    locals()[\"ax\"+str(run_no)].set_xlabel('')\n    run_no += 1\n    \nax11.remove()","3e724e79":"train_df[cont_features].describe()","0441529f":"fig = plt.figure(figsize=(15, 10), facecolor='#f6f5f5')\ngs = fig.add_gridspec(4, 4)\ngs.update(wspace=0.2, hspace=0.05)\n\nbackground_color = \"#f6f5f5\"\n\nrun_no = 0\nfor col in range(0, 4):\n    for row in range(0, 3):\n        locals()[\"ax\"+str(run_no)] = fig.add_subplot(gs[row, col])\n        locals()[\"ax\"+str(run_no)].set_facecolor(background_color)\n        locals()[\"ax\"+str(run_no)].set_yticklabels([])\n        locals()[\"ax\"+str(run_no)].tick_params(axis='y', which=u'both',length=0)\n        for s in [\"top\",\"right\", 'left']:\n            locals()[\"ax\"+str(run_no)].spines[s].set_visible(False)\n        run_no += 1\n\nax0.text(-0.15, 4.5, 'Continuous Features Distribution on Test Dataset', fontsize=20, fontweight='bold', fontfamily='serif')\nax0.text(-0.15, 4, 'Continuous features on test dataset is similar to train dataset', fontsize=13, fontweight='light', fontfamily='serif')        \n\nrun_no = 0\nfor col in cont_features:\n    sns.kdeplot(test_df[col], ax=locals()[\"ax\"+str(run_no)], shade=True, color='#f088b7', alpha=0.9, zorder=2)\n    locals()[\"ax\"+str(run_no)].grid(which='major', axis='x', zorder=0, color='gray', linestyle=':', dashes=(1,5))\n    locals()[\"ax\"+str(run_no)].set_ylabel(col, fontsize=10, fontweight='bold').set_rotation(0)\n    locals()[\"ax\"+str(run_no)].yaxis.set_label_coords(1, 0)\n    locals()[\"ax\"+str(run_no)].set_xlim(-0.2, 1.2)\n    locals()[\"ax\"+str(run_no)].set_xlabel('')\n    run_no += 1\n    \nax11.remove()","fd7aff2d":"test_df[cont_features].describe()","62601667":"background_color = \"#f6f5f5\"\n\nfig = plt.figure(figsize=(18, 8), facecolor=background_color)\ngs = fig.add_gridspec(1, 2)\nax0 = fig.add_subplot(gs[0, 0])\nax1 = fig.add_subplot(gs[0, 1])\ncolors = [\"#f088b7\", \"#f6f5f5\",\"#f088b7\"]\ncolormap = matplotlib.colors.LinearSegmentedColormap.from_list(\"\", colors)\n\nax0.set_facecolor(background_color)\nax0.text(0, -1, 'Features Correlation on Train Dataset', fontsize=20, fontweight='bold', fontfamily='serif')\nax0.text(0, -0.4, 'Highest correlation in the dataset is 0.9', fontsize=13, fontweight='light', fontfamily='serif')\n\nax1.set_facecolor(background_color)\nax1.text(-0.1, -1, 'Features Correlation on Test Dataset', fontsize=20, fontweight='bold', fontfamily='serif')\nax1.text(-0.1, -0.4, 'Features in test dataset are similar with features in train dataset ', \n         fontsize=13, fontweight='light', fontfamily='serif')\n\nsns.heatmap(train_df[cont_features].corr(), ax=ax0, vmin=-1, vmax=1, annot=True, square=True, \n            cbar_kws={\"orientation\": \"horizontal\"}, cbar=False, cmap=colormap, fmt='.1g')\n\nsns.heatmap(test_df[cont_features].corr(), ax=ax1, vmin=-1, vmax=1, annot=True, square=True, \n            cbar_kws={\"orientation\": \"horizontal\"}, cbar=False, cmap=colormap, fmt='.1g')\n\nplt.show()","d161cc42":"cat5_category = list(pd.DataFrame(train_df['cat5'].value_counts()\/len(train_df['cat5'])*100)[:2].index)\ncat7_category = list(pd.DataFrame(train_df['cat7'].value_counts()\/len(train_df['cat7'])*100)[:13].index)\ncat8_category = list(pd.DataFrame(train_df['cat8'].value_counts()\/len(train_df['cat8'])*100)[:13].index)\ncat10_category = list(pd.DataFrame(train_df['cat10'].value_counts()\/len(train_df['cat10'])*100)[:13].index)\ntrain_df['cat5'] = np.where(~train_df['cat5'].isin(cat5_category), 'Others', train_df['cat5'])\ntrain_df['cat7'] = np.where(~train_df['cat7'].isin(cat7_category), 'Others', train_df['cat7'])\ntrain_df['cat8'] = np.where(~train_df['cat8'].isin(cat8_category), 'Others', train_df['cat8'])\ntrain_df['cat10'] = np.where(~train_df['cat10'].isin(cat10_category), 'Others', train_df['cat10'])","b489a116":"background_color = \"#f6f5f5\"\n\nfig = plt.figure(figsize=(20, 15), facecolor=background_color)\ngs = fig.add_gridspec(7, 3)\ngs.update(wspace=0.2, hspace=0.2)\n\nrun_no = 0\nfor row in range(0, 7):\n    for col in range(0, 3):\n        locals()[\"ax\"+str(run_no)] = fig.add_subplot(gs[row, col])\n        locals()[\"ax\"+str(run_no)].set_facecolor(background_color)\n        for s in [\"top\",\"right\", 'left']:\n            locals()[\"ax\"+str(run_no)].spines[s].set_visible(False)\n        run_no += 1\n\nax0.text(-0.8, 115, 'Categorical Features Proportion on Train Dataset (%)', fontsize=20, fontweight='bold', fontfamily='serif')\nax0.text(-0.8, 100, 'Some features are dominated by one category', fontsize=13, fontweight='light', fontfamily='serif')        \n\nrun_no = 0\nfor col in cat_features:\n    chart_df = pd.DataFrame(train_df[col].value_counts() \/ len(train_df) * 100)\n    sns.barplot(x=chart_df.index, y=chart_df[col], ax=locals()[\"ax\"+str(run_no)], color='#f088b7', zorder=2)\n    locals()[\"ax\"+str(run_no)].grid(which='major', axis='y', zorder=0, color='gray', linestyle=':', dashes=(1,5))\n    run_no += 1\n    \nax19.remove()\nax20.remove()","f66aee1c":"cat5_category = list(pd.DataFrame(test_df['cat5'].value_counts()\/len(test_df['cat5'])*100)[:2].index)\ncat7_category = list(pd.DataFrame(test_df['cat7'].value_counts()\/len(test_df['cat7'])*100)[:13].index)\ncat8_category = list(pd.DataFrame(test_df['cat8'].value_counts()\/len(test_df['cat8'])*100)[:13].index)\ncat10_category = list(pd.DataFrame(test_df['cat10'].value_counts()\/len(test_df['cat10'])*100)[:13].index)\ntest_df['cat5'] = np.where(~test_df['cat5'].isin(cat5_category), 'Others', test_df['cat5'])\ntest_df['cat7'] = np.where(~test_df['cat7'].isin(cat7_category), 'Others', test_df['cat7'])\ntest_df['cat8'] = np.where(~test_df['cat8'].isin(cat8_category), 'Others', test_df['cat8'])\ntest_df['cat10'] = np.where(~test_df['cat10'].isin(cat10_category), 'Others', test_df['cat10'])","1341a290":"background_color = \"#f6f5f5\"\n\nfig = plt.figure(figsize=(20, 15), facecolor=background_color)\ngs = fig.add_gridspec(7, 3)\ngs.update(wspace=0.2, hspace=0.2)\n\nrun_no = 0\nfor row in range(0, 7):\n    for col in range(0, 3):\n        locals()[\"ax\"+str(run_no)] = fig.add_subplot(gs[row, col])\n        locals()[\"ax\"+str(run_no)].set_facecolor(background_color)\n        for s in [\"top\",\"right\", 'left']:\n            locals()[\"ax\"+str(run_no)].spines[s].set_visible(False)\n        run_no += 1\n\nax0.text(-0.8, 115, 'Categorical Features Proportion on Test Dataset (%)', fontsize=20, fontweight='bold', fontfamily='serif')\nax0.text(-0.8, 100, 'Categorical features on test dataset similar to train dataset', \n         fontsize=13, fontweight='light', fontfamily='serif')        \n\nrun_no = 0\nfor col in cat_features:\n    chart_df = pd.DataFrame(test_df[col].value_counts() \/ len(test_df) * 100)\n    sns.barplot(x=chart_df.index, y=chart_df[col], ax=locals()[\"ax\"+str(run_no)], color='#f088b7', zorder=2)\n    locals()[\"ax\"+str(run_no)].grid(which='major', axis='y', zorder=0, color='gray', linestyle=':', dashes=(1,5))\n    run_no += 1\n    \nax19.remove()\nax20.remove()","829fe389":"# reset train and test dataset\ntrain_df = pd.read_csv('\/kaggle\/input\/tabular-playground-series-mar-2021\/train.csv')\ntest_df = pd.read_csv('\/kaggle\/input\/tabular-playground-series-mar-2021\/test.csv')\n\nprint('Propotion of target variable: 0 is', len(train_df[train_df['target']==0])\/len(train_df))\nprint('Propotion of target variable: 1 is', len(train_df[train_df['target']==1])\/len(train_df))","de99a23f":"fig = plt.figure(figsize=(15, 10), facecolor='#f6f5f5')\ngs = fig.add_gridspec(4, 4)\ngs.update(wspace=0.2, hspace=0.05)\n\nbackground_color = \"#f6f5f5\"\n\nrun_no = 0\nfor col in range(0, 4):\n    for row in range(0, 3):\n        locals()[\"ax\"+str(run_no)] = fig.add_subplot(gs[row, col])\n        locals()[\"ax\"+str(run_no)].set_facecolor(background_color)\n        locals()[\"ax\"+str(run_no)].set_yticklabels([])\n        locals()[\"ax\"+str(run_no)].tick_params(axis='y', which=u'both',length=0)\n        for s in [\"top\",\"right\", 'left']:\n            locals()[\"ax\"+str(run_no)].spines[s].set_visible(False)\n        run_no += 1\n\nax0.text(-0.15, 4.5, 'Continuous Features Distribution & Target Variables', fontsize=20, fontweight='bold', fontfamily='serif')\nax0.text(-0.15, 4, 'Continuous features distribution of target 0 and 1 are similar', fontsize=13, fontweight='light', fontfamily='serif')        \n\nrun_no = 0\nfor col in cont_features:\n    sns.kdeplot(train_df.loc[train_df['target'] == 0, col], ax=locals()[\"ax\"+str(run_no)], \n                shade=True, color='#ffd514', alpha=0.9, zorder=2)\n    sns.kdeplot(train_df.loc[train_df['target'] == 1, col], ax=locals()[\"ax\"+str(run_no)], \n                shade=True, color='#f088b7', alpha=0.9, zorder=2)\n    locals()[\"ax\"+str(run_no)].grid(which='major', axis='x', zorder=0, color='gray', linestyle=':', dashes=(1,5))\n    locals()[\"ax\"+str(run_no)].set_ylabel(col, fontsize=10, fontweight='bold').set_rotation(0)\n    locals()[\"ax\"+str(run_no)].yaxis.set_label_coords(1, 0)\n    locals()[\"ax\"+str(run_no)].set_xlim(-0.2, 1.2)\n    locals()[\"ax\"+str(run_no)].set_xlabel('')\n    run_no += 1\n    \nax11.remove()","d0ade8b1":"# reset train and test dataset\ntrain_df = pd.read_csv('\/kaggle\/input\/tabular-playground-series-mar-2021\/train.csv')\ntest_df = pd.read_csv('\/kaggle\/input\/tabular-playground-series-mar-2021\/test.csv')","23ce2521":"cat5_category = list(pd.DataFrame(train_df.loc[train_df['target']==0, 'cat5'].value_counts()\/len(train_df['cat5'])*100)[:2].index)\ncat7_category = list(pd.DataFrame(train_df.loc[train_df['target']==0, 'cat7'].value_counts()\/len(train_df['cat7'])*100)[:13].index)\ncat8_category = list(pd.DataFrame(train_df.loc[train_df['target']==0, 'cat8'].value_counts()\/len(train_df['cat8'])*100)[:13].index)\ncat10_category = list(pd.DataFrame(train_df.loc[train_df['target']==0, 'cat10'].value_counts()\/len(train_df['cat10'])*100)[:13].index)\ntrain_df['cat5'] = np.where(~train_df['cat5'].isin(cat5_category), 'Others', train_df['cat5'])\ntrain_df['cat7'] = np.where(~train_df['cat7'].isin(cat7_category), 'Others', train_df['cat7'])\ntrain_df['cat8'] = np.where(~train_df['cat8'].isin(cat8_category), 'Others', train_df['cat8'])\ntrain_df['cat10'] = np.where(~train_df['cat10'].isin(cat10_category), 'Others', train_df['cat10'])","ab779f0a":"background_color = \"#f6f5f5\"\n\nfig = plt.figure(figsize=(20, 15), facecolor=background_color)\ngs = fig.add_gridspec(7, 3)\ngs.update(wspace=0.2, hspace=0.2)\n\nrun_no = 0\nfor row in range(0, 7):\n    for col in range(0, 3):\n        locals()[\"ax\"+str(run_no)] = fig.add_subplot(gs[row, col])\n        locals()[\"ax\"+str(run_no)].set_facecolor(background_color)\n        for s in [\"top\",\"right\", 'left']:\n            locals()[\"ax\"+str(run_no)].spines[s].set_visible(False)\n        run_no += 1\n\nax0.text(-0.8, 70, 'Categorical Features & Target Variables: 0', fontsize=20, fontweight='bold', fontfamily='serif')\nax0.text(-0.8, 60, 'Categorical features compared with target: 0 as a percentage of total train dataset', \n         fontsize=13, fontweight='light', fontfamily='serif')        \n\nrun_no = 0\nfor col in cat_features:\n    chart_df = pd.DataFrame(train_df.loc[train_df['target']==0, col].value_counts() \/ len(train_df) * 100)\n    sns.barplot(x=chart_df.index, y=chart_df[col], ax=locals()[\"ax\"+str(run_no)], color='#f088b7', zorder=2)\n    locals()[\"ax\"+str(run_no)].grid(which='major', axis='y', zorder=0, color='gray', linestyle=':', dashes=(1,5))\n    run_no += 1\n    \nax19.remove()\nax20.remove()","878c7bc3":"cat5_category = list(pd.DataFrame(train_df.loc[train_df['target']==1, 'cat5'].value_counts()\/len(train_df['cat5'])*100)[:2].index)\ncat7_category = list(pd.DataFrame(train_df.loc[train_df['target']==1, 'cat7'].value_counts()\/len(train_df['cat7'])*100)[:13].index)\ncat8_category = list(pd.DataFrame(train_df.loc[train_df['target']==1, 'cat8'].value_counts()\/len(train_df['cat8'])*100)[:13].index)\ncat10_category = list(pd.DataFrame(train_df.loc[train_df['target']==1, 'cat10'].value_counts()\/len(train_df['cat10'])*100)[:13].index)\ntrain_df['cat5'] = np.where(~train_df['cat5'].isin(cat5_category), 'Others', train_df['cat5'])\ntrain_df['cat7'] = np.where(~train_df['cat7'].isin(cat7_category), 'Others', train_df['cat7'])\ntrain_df['cat8'] = np.where(~train_df['cat8'].isin(cat8_category), 'Others', train_df['cat8'])\ntrain_df['cat10'] = np.where(~train_df['cat10'].isin(cat10_category), 'Others', train_df['cat10'])","bb024a24":"background_color = \"#f6f5f5\"\n\nfig = plt.figure(figsize=(20, 15), facecolor=background_color)\ngs = fig.add_gridspec(7, 3)\ngs.update(wspace=0.2, hspace=0.2)\n\nrun_no = 0\nfor row in range(0, 7):\n    for col in range(0, 3):\n        locals()[\"ax\"+str(run_no)] = fig.add_subplot(gs[row, col])\n        locals()[\"ax\"+str(run_no)].set_facecolor(background_color)\n        for s in [\"top\",\"right\", 'left']:\n            locals()[\"ax\"+str(run_no)].spines[s].set_visible(False)\n        run_no += 1\n\nax0.text(-0.8, 34, 'Categorical Features & Target Variables: 1', fontsize=20, fontweight='bold', fontfamily='serif')\nax0.text(-0.8, 29\n         , 'Categorical features compared with target: 1 as a percentage of total train dataset', \n         fontsize=13, fontweight='light', fontfamily='serif')        \n\nrun_no = 0\nfor col in cat_features:\n    chart_df = pd.DataFrame(train_df.loc[train_df['target']==1, col].value_counts() \/ len(train_df) * 100)\n    sns.barplot(x=chart_df.index, y=chart_df[col], ax=locals()[\"ax\"+str(run_no)], color='#f088b7', zorder=2)\n    locals()[\"ax\"+str(run_no)].grid(which='major', axis='y', zorder=0, color='gray', linestyle=':', dashes=(1,5))\n    run_no += 1\n    \nax19.remove()\nax20.remove()","fbe4bf97":"from sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import KFold\nfrom catboost import CatBoostClassifier\nfrom xgboost import XGBClassifier\nfrom lightgbm import LGBMClassifier\nfrom sklearn.ensemble import VotingClassifier\nfrom sklearn.metrics import roc_auc_score","431fe1f7":"train_df = pd.read_csv('..\/input\/tabular-playground-series-mar-2021\/train.csv')\ntest_df = pd.read_csv('..\/input\/tabular-playground-series-mar-2021\/test.csv')\ncombine_df = pd.concat([train_df, test_df], axis=0)\naverage_all_df = pd.DataFrame()\naverage_cat_df = pd.DataFrame()\naverage_cont_df = pd.DataFrame()\n\nle = LabelEncoder()\nfor col in cat_features:\n    combine_df[col] = le.fit_transform(combine_df[col])\ntrain_df = combine_df.iloc[:len(train_df), :]\ntest_df = combine_df.iloc[len(train_df):, :]\ntest_df = test_df.drop('target', axis=1)\n\nfolds = 10\nkf = KFold(n_splits=folds, shuffle=True, random_state=42)\n\nfeatures = [feature for feature in train_df.columns if feature not in ['id', 'target']]","ba56e3a1":"train_oof = np.zeros((300000,))\n\nfor fold, (train_idx, valid_idx) in enumerate(kf.split(train_df[features], train_df['target'])):\n    train, valid = train_df.iloc[train_idx], train_df.iloc[valid_idx]\n    \n    X_train = train[features]\n    X_valid = valid[features]\n    y_train = train['target']\n    y_valid = valid['target']\n\n    model = CatBoostClassifier(verbose=0,\n                                eval_metric=\"AUC\",\n                                random_state=42,\n                                cat_features=[x for x in range(len(cat_features))],\n                                task_type=\"GPU\",\n                                devices=\"0\")\n\n    model =  model.fit(X_train, y_train, eval_set=[(X_valid, y_valid)])\n    temp_oof = model.predict_proba(X_valid)[:,1]\n    train_oof[valid_idx] = temp_oof\n    print(f'Fold {fold} AUC: ', roc_auc_score(y_valid, temp_oof))\n    \nprint(f'OOF AUC - All features: ', roc_auc_score(train_df['target'], train_oof))\naverage_all_df['catboost'] = train_oof ","8b82fce7":"train_oof = np.zeros((300000,))\n\nfor fold, (train_idx, valid_idx) in enumerate(kf.split(train_df[features], train_df['target'])):\n    train, valid = train_df.iloc[train_idx], train_df.iloc[valid_idx]\n    \n    X_train = train[cat_features]\n    X_valid = valid[cat_features]\n    y_train = train['target']\n    y_valid = valid['target']\n\n    model = CatBoostClassifier(verbose=0,\n                                eval_metric=\"AUC\",\n                                random_state=42,\n                                cat_features=[x for x in range(len(cat_features))],\n                                task_type=\"GPU\",\n                                devices=\"0\")\n\n    model =  model.fit(X_train, y_train, eval_set=[(X_valid, y_valid)])\n    temp_oof = model.predict_proba(X_valid)[:,1]\n    train_oof[valid_idx] = temp_oof\n    print(f'Fold {fold} AUC: ', roc_auc_score(y_valid, temp_oof))\n    \nprint(f'OOF AUC - Categorical features only: ', roc_auc_score(train_df['target'], train_oof))\naverage_cat_df['catboost'] = train_oof ","84683c64":"train_oof = np.zeros((300000,))\n\nfor fold, (train_idx, valid_idx) in enumerate(kf.split(train_df[features], train_df['target'])):\n    train, valid = train_df.iloc[train_idx], train_df.iloc[valid_idx]\n    \n    X_train = train[cont_features]\n    X_valid = valid[cont_features]\n    y_train = train['target']\n    y_valid = valid['target']\n\n    model = CatBoostClassifier(verbose=0,\n                                eval_metric=\"AUC\",\n                                random_state=42,\n                                task_type=\"GPU\",\n                                devices=\"0\")\n\n    model =  model.fit(X_train, y_train, eval_set=[(X_valid, y_valid)])\n    temp_oof = model.predict_proba(X_valid)[:,1]\n    train_oof[valid_idx] = temp_oof\n    print(f'Fold {fold} AUC: ', roc_auc_score(y_valid, temp_oof))\n    \nprint(f'OOF AUC - Continuous features only: ', roc_auc_score(train_df['target'], train_oof))\naverage_cont_df['catboost'] = train_oof ","00d9a345":"train_oof = np.zeros((300000,))\n\nfor fold, (train_idx, valid_idx) in enumerate(kf.split(train_df[features], train_df['target'])):\n    train, valid = train_df.iloc[train_idx], train_df.iloc[valid_idx]\n    \n    X_train = train[features]\n    X_valid = valid[features]\n    y_train = train['target']\n    y_valid = valid['target']\n\n    model = XGBClassifier(eval_metric=\"auc\",\n                          random_state=42,\n                          tree_method=\"gpu_hist\",\n                          gpu_id=\"0\",\n                          use_label_encoder=False,)\n\n    model =  model.fit(X_train, y_train, eval_set=[(X_valid, y_valid)], verbose=0)\n    temp_oof = model.predict_proba(X_valid)[:,1]\n    train_oof[valid_idx] = temp_oof\n    print(f'Fold {fold} AUC: ', roc_auc_score(y_valid, temp_oof))\n    \nprint(f'OOF AUC - All features: ', roc_auc_score(train_df['target'], train_oof))\naverage_all_df['xgboost'] = train_oof ","50223a61":"train_oof = np.zeros((300000,))\n\nfor fold, (train_idx, valid_idx) in enumerate(kf.split(train_df[features], train_df['target'])):\n    train, valid = train_df.iloc[train_idx], train_df.iloc[valid_idx]\n    \n    X_train = train[cat_features]\n    X_valid = valid[cat_features]\n    y_train = train['target']\n    y_valid = valid['target']\n\n    model = XGBClassifier(eval_metric=\"auc\",\n                          random_state=42,\n                          tree_method=\"gpu_hist\",\n                          gpu_id=\"0\",\n                          use_label_encoder=False,)\n\n    model =  model.fit(X_train, y_train, eval_set=[(X_valid, y_valid)], verbose=0)\n    temp_oof = model.predict_proba(X_valid)[:,1]\n    train_oof[valid_idx] = temp_oof\n    print(f'Fold {fold} AUC: ', roc_auc_score(y_valid, temp_oof))\n    \nprint(f'OOF AUC - Categorical features only: ', roc_auc_score(train_df['target'], train_oof))\naverage_cat_df['xgboost'] = train_oof ","854c0779":"train_oof = np.zeros((300000,))\n\nfor fold, (train_idx, valid_idx) in enumerate(kf.split(train_df[features], train_df['target'])):\n    train, valid = train_df.iloc[train_idx], train_df.iloc[valid_idx]\n    \n    X_train = train[cont_features]\n    X_valid = valid[cont_features]\n    y_train = train['target']\n    y_valid = valid['target']\n\n    model = XGBClassifier(eval_metric=\"auc\",\n                          random_state=42,\n                          tree_method=\"gpu_hist\",\n                          gpu_id=\"0\",\n                          use_label_encoder=False,)\n\n    model =  model.fit(X_train, y_train, eval_set=[(X_valid, y_valid)], verbose=0)\n    temp_oof = model.predict_proba(X_valid)[:,1]\n    train_oof[valid_idx] = temp_oof\n    print(f'Fold {fold} AUC: ', roc_auc_score(y_valid, temp_oof))\n    \nprint(f'OOF AUC - Continuous features only: ', roc_auc_score(train_df['target'], train_oof))\naverage_cont_df['xgboost'] = train_oof ","65d17067":"train_oof = np.zeros((300000,))\n\nfor fold, (train_idx, valid_idx) in enumerate(kf.split(train_df[features], train_df['target'])):\n    train, valid = train_df.iloc[train_idx], train_df.iloc[valid_idx]\n    \n    X_train = train[features]\n    X_valid = valid[features]\n    y_train = train['target']\n    y_valid = valid['target']\n\n    model = LGBMClassifier(metric=\"auc\",\n                          random_state=42,\n                          cat_feature=[x for x in range(len(cat_features))],\n                          device='gpu')\n\n    model =  model.fit(X_train, y_train, eval_set=[(X_valid, y_valid)], verbose=0)\n    temp_oof = model.predict_proba(X_valid)[:,1]\n    train_oof[valid_idx] = temp_oof\n    print(f'Fold {fold} AUC: ', roc_auc_score(y_valid, temp_oof))\n    \nprint(f'OOF AUC - All features: ', roc_auc_score(train_df['target'], train_oof))\naverage_all_df['lgbm'] = train_oof ","fe48ae46":"train_oof = np.zeros((300000,))\n\nfor fold, (train_idx, valid_idx) in enumerate(kf.split(train_df[features], train_df['target'])):\n    train, valid = train_df.iloc[train_idx], train_df.iloc[valid_idx]\n    \n    X_train = train[cat_features]\n    X_valid = valid[cat_features]\n    y_train = train['target']\n    y_valid = valid['target']\n\n    model = LGBMClassifier(metric=\"auc\",\n                          random_state=42,\n                          cat_feature=[x for x in range(len(cat_features))],\n                          device='gpu')\n\n    model =  model.fit(X_train, y_train, eval_set=[(X_valid, y_valid)], verbose=0)\n    temp_oof = model.predict_proba(X_valid)[:,1]\n    train_oof[valid_idx] = temp_oof\n    print(f'Fold {fold} AUC: ', roc_auc_score(y_valid, temp_oof))\n    \nprint(f'OOF AUC - Categorical features only: ', roc_auc_score(train_df['target'], train_oof))\naverage_cat_df['lgbm'] = train_oof ","2cb23d55":"train_oof = np.zeros((300000,))\n\nfor fold, (train_idx, valid_idx) in enumerate(kf.split(train_df[features], train_df['target'])):\n    train, valid = train_df.iloc[train_idx], train_df.iloc[valid_idx]\n    \n    X_train = train[cont_features]\n    X_valid = valid[cont_features]\n    y_train = train['target']\n    y_valid = valid['target']\n\n    model = LGBMClassifier(metric=\"auc\",\n                          random_state=42,\n                          device='gpu')\n\n    model =  model.fit(X_train, y_train, eval_set=[(X_valid, y_valid)], verbose=0)\n    temp_oof = model.predict_proba(X_valid)[:,1]\n    train_oof[valid_idx] = temp_oof\n    print(f'Fold {fold} AUC: ', roc_auc_score(y_valid, temp_oof))\n    \nprint(f'OOF AUC - Continuous features only: ', roc_auc_score(train_df['target'], train_oof))\naverage_cont_df['lgbm'] = train_oof ","2bf3c027":"average_all_df['average'] = average_all_df.mean(axis=1)\naverage_cat_df['average'] = average_cat_df.mean(axis=1)\naverage_cont_df['average'] = average_cont_df.mean(axis=1)\n\nprint(f'OOF AUC - All features: ', roc_auc_score(train_df['target'],average_all_df['average']))\nprint(f'OOF AUC - Cat features: ', roc_auc_score(train_df['target'],average_cat_df['average']))\nprint(f'OOF AUC - Cont features: ', roc_auc_score(train_df['target'],average_cont_df['average']))","6294edea":"average_tuned_df = pd.DataFrame()","064759a7":"train_oof = np.zeros((300000,))\n\nfor fold, (train_idx, valid_idx) in enumerate(kf.split(train_df[features], train_df['target'])):\n    train, valid = train_df.iloc[train_idx], train_df.iloc[valid_idx]\n    \n    X_train = train[features]\n    X_valid = valid[features]\n    y_train = train['target']\n    y_valid = valid['target']\n\n    model = CatBoostClassifier( \n                                verbose=0,\n                                eval_metric=\"AUC\",\n                                loss_function=\"Logloss\",\n                                random_state=2021,\n                                num_boost_round=20000,\n                                od_type=\"Iter\",\n                                od_wait=200,\n                                task_type=\"GPU\",\n                                devices=\"0\",\n                                cat_features=[x for x in range(len(cat_features))],\n                                bagging_temperature=1.288692494969795,\n                                grow_policy=\"Depthwise\",\n                                l2_leaf_reg=9.847870133539244,\n                                learning_rate=0.01877982653902465,\n                                max_depth=8,\n                                min_data_in_leaf=1,\n                                penalties_coefficient=2.1176668909602734)\n\n    model =  model.fit(X_train, y_train, eval_set=[(X_valid, y_valid)])\n    temp_oof = model.predict_proba(X_valid)[:,1]\n    train_oof[valid_idx] = temp_oof\n    print(f'Fold {fold} AUC: ', roc_auc_score(y_valid, temp_oof))\n    \nprint(f'OOF AUC - All features: ', roc_auc_score(train_df['target'], train_oof))\naverage_tuned_df['catboost'] = train_oof","bb430085":"train_oof = np.zeros((300000,))\n\nfor fold, (train_idx, valid_idx) in enumerate(kf.split(train_df[features], train_df['target'])):\n    train, valid = train_df.iloc[train_idx], train_df.iloc[valid_idx]\n    \n    X_train = train[features]\n    X_valid = valid[features]\n    y_train = train['target']\n    y_valid = valid['target']\n\n    model = XGBClassifier(        \n                        seed=2021,\n                        n_estimators=10000,\n                        verbosity=1,\n                        eval_metric=\"auc\",\n                        tree_method=\"gpu_hist\",\n                        gpu_id=0,\n                        alpha=7.105038963844129,\n                        colsample_bytree=0.25505629740052566,\n                        gamma=0.4999381950212869,\n                        reg_lambda=1.7256912198205319,\n                        learning_rate=0.011823142071967673,\n                        max_bin=338,\n                        max_depth=8,\n                        min_child_weight=2.286836198630466,\n                        subsample=0.618417952155855,\n                        use_label_encoder=False)\n\n    model =  model.fit(X_train, y_train, eval_set=[(X_valid, y_valid)], verbose=0)\n    temp_oof = model.predict_proba(X_valid)[:,1]\n    train_oof[valid_idx] = temp_oof\n    print(f'Fold {fold} AUC: ', roc_auc_score(y_valid, temp_oof))\n    \nprint(f'OOF AUC - All features: ', roc_auc_score(train_df['target'], train_oof))\naverage_tuned_df['xgboost'] = train_oof","793dfd7a":"train_oof = np.zeros((300000,))\n\nfor fold, (train_idx, valid_idx) in enumerate(kf.split(train_df[features], train_df['target'])):\n    train, valid = train_df.iloc[train_idx], train_df.iloc[valid_idx]\n    \n    X_train = train[features]\n    X_valid = valid[features]\n    y_train = train['target']\n    y_valid = valid['target']\n\n    model = LGBMClassifier(\n                cat_feature=[x for x in range(len(cat_features))],\n                random_state=2021,\n                cat_l2=25.999876242730252,\n                cat_smooth=89.2699690675538,\n                colsample_bytree=0.2557260109926193,\n                early_stopping_round=200,\n                learning_rate=0.00918685483594994,\n                max_bin=788,\n                max_depth=81,\n                metric=\"auc\",\n                min_child_samples=292,\n                min_data_per_group=177,\n                n_estimators=1600000,\n                n_jobs=-1,\n                num_leaves=171,\n                reg_alpha=0.7115353581785044,\n                reg_lambda=5.658115293998945,\n                subsample=0.9262904583735796,\n                subsample_freq=1,\n                verbose=-1)\n    model =  model.fit(X_train, y_train, eval_set=[(X_valid, y_valid)], verbose=0)\n    temp_oof = model.predict_proba(X_valid)[:,1]\n    train_oof[valid_idx] = temp_oof\n    print(f'Fold {fold} AUC: ', roc_auc_score(y_valid, temp_oof))\n    \nprint(f'OOF AUC - All features: ', roc_auc_score(train_df['target'], train_oof))\naverage_tuned_df['lgbm'] = train_oof","ebbed2be":"average_tuned_df['average'] = average_tuned_df.mean(axis=1)\nprint(f'OOF AUC - All features: ', roc_auc_score(train_df['target'], average_tuned_df['average']))","8bfca51f":"**First 5 rows in the test dataset**","a8199042":"**Using categorical features only**","89df6919":"[back to top](#table-of-contents)\n<a id=\"5.4\"><\/a>\n## 5.4 Tuned voting classifier","d4ded43a":"[back to top](#table-of-contents)\n<a id=\"5.3\"><\/a>\n## 5.3 LGBM tuned model","a197520d":"[back to top](#table-of-contents)\n<a id=\"2.3\"><\/a>\n## 2.3. First 5 rows\n\n**First 5 rows in the train dataset**","9f20835a":"[back to top](#table-of-contents)\n<a id=\"5.1\"><\/a>\n## 5.1 Catboost tuned model","9718ba57":"[back to top](#table-of-contents)\n<a id=\"2.4\"><\/a>\n## 2.4. Basic statistics on continuous features\n**Train dataset**","d0c28212":"[back to top](#table-of-contents)\n<a id=\"4.2\"><\/a>\n## 4.3 XGBoost baseline model\n\n`XGBClassifier` is used for the baseline model in this notebooks without hyperparameters tuning using 10 fold cross validation. \n\n**Observations**:\n* Using `all features` still resulting the best AUC compared to use `categorical` or `continuous` features only.\n* The AUC gap between `continuous features only` and `categorical features only` is big.\n* Below are the OOF AUC results:\n    * `all features` resulting a `0.88986` OOF AUC.\n    * `categorical features only` resulting a `0.88412` OOF AUC.\n    * `continuous features only` resulting a `0.81767` OOF AUC.\n\n\n**Note:** Remove `tree_method=\"gpu_hist\"` and `gpu_id=\"0\"` to use CPU only.","9bd449d4":"[back to top](#table-of-contents)\n<a id=\"4.2\"><\/a>\n## 4.2 Catboost baseline model\n\n`CatBosstClassifier` is used for the baseline model in this notebooks without hyperparameters tuning using 10 fold cross validation. \n\n**Observations**:\n* Using `all features` still resulting the best AUC compared to use `categorical` or `continuous` features only.\n* The AUC gap between `continuous features only` and `categorical features only` is big.\n* Below are the OOF AUC results:\n    * `all features` resulting a `0.88977` OOF AUC.\n    * `categorical features only` resulting a `0.88512` OOF AUC.\n    * `continuous features only` resulting a `0.81459` OOF AUC.\n\n\n**Note:** Remove `task_type=\"GPU\"` and `devices=\"0\"` to use CPU only.","6e150cb1":"[back to top](#table-of-contents)\n<a id=\"6\"><\/a>\n## 6. Winners Solutions\n\nCongratulations for all the winners and thank you for sharing your solution. Below are the winners and their solutions:\n* 1st place position: [Dave E](https:\/\/www.kaggle.com\/davidedwards1) - [#1 LB Ideas](https:\/\/www.kaggle.com\/c\/tabular-playground-series-mar-2021\/discussion\/229833)\n* 2nd place position: [danzel](https:\/\/www.kaggle.com\/springmanndaniel) - [2nd place solution - dae + embeddings](https:\/\/www.kaggle.com\/c\/tabular-playground-series-mar-2021\/discussion\/229868)\n* 3rd place position: [BIZEN](https:\/\/www.kaggle.com\/hiro5299834) - [3rd place solution - stacking](https:\/\/www.kaggle.com\/c\/tabular-playground-series-mar-2021\/discussion\/230101)","b02eb6e4":"**Correlation between continuous variables**","0a55da21":"[back to top](#table-of-contents)\n<a id=\"5\"><\/a>\n# 5. Tuned Model\n\nThis section will explore model had has been tuned using `all features`. Hyperparmaters are taken from [TPS Mar 2021 - Stacked Starter](https:\/\/www.kaggle.com\/craigmthomas\/tps-mar-2021-stacked-starter) by [Craig Thomas](https:\/\/www.kaggle.com\/craigmthomas)\n\n**Observations:**\n* With a proper hyperparameters tuning, tuned model produce higher OOF AUC compared to baseline model. Tuned `Catboost`, `XGBoost` and `LGBM` AUC are consistently increased by around `0.005` which is quite a significant increased in a competitive competition so it's very important to tune the model properly. Below are the comparison between tuned and baseline models:\n    - `Catboost` AUC performance increased by `0.00511` from `0.88977` to `0.89488`.\n    - `XGBoost` AUC performance also increased from `0.88986` to `0.89467` which is an increased by `0.00481`.\n    - `LGBM` AUC performance of `0.89689.` increased by `0.00533` from `0.89156`.\n* Ensembling the 3 models by averaging them beats all the individual tuned models. It generates an OOF AUC of `0.89699` which is slightly above the LGBM tuned model of `0.89689`.","0bfe3ca8":"[back to top](#table-of-contents)\n<a id=\"4\"><\/a>\n# 4. Baseline Model\n\nThis section will evaluate the performance of `Catboost`, `XGBoost` and `LGBM` using 3 dataset `all features`, `categorical features only` and `continuous features only`.. At the end, `Voting Classifiers` will be used to ensemble all of the model.\n\n**Observations:**\n* `LGBM` has the highest AUC of `0.89156` for using `all features` and `categorical features only` dataset but the results is close with `XGBoost` and `Catboost`.\n* `XGBoost` performs the best on `continuous features only` dataset with AUC of `0.88412` and again the result is close with other model.\n* Ensembling the 3 models by averaging them beats the individual baseline model with OOF AUC of `0.89360` for `all features`, `0.89017` for `categorical features only` and `0.86194` for `continuous features only`.","5117f560":"[back to top](#table-of-contents)\n<a id=\"3.2\"><\/a>\n## 3.2 Categorical features\n**Target: 0**","0d613235":"**Test dataset**","8a95f291":"[back to top](#table-of-contents)\n<a id=\"4.4\"><\/a>\n## 4.4 LGBM baseline model\n\n`XGBClassifier` is used for the baseline model in this notebooks without hyperparameters tuning using 10 fold cross validation. \n\n**Observations**:\n* Using `all features` still resulting the best AUC compared to use `categorical` or `continuous` features only.\n* The AUC gap between `continuous features only` and `categorical features only` is big.\n* Below are the OOF AUC results:\n    * `all features` resulting a `0.89156` OOF AUC.\n    * `categorical features only` resulting a `0.88602` OOF AUC.\n    * `continuous features only` resulting a `0.81028` OOF AUC.\n\n\n**Note:** Remove `device='gpu'` to use CPU only.","86ae8e0e":"[back to top](#table-of-contents)\n<a id=\"2\"><\/a>\n# 2. General\n\n**Observations:**\n\n* `Train` set has 300,000 rows while `test` set has 200,000 rows.\n* There are 19 categorical features from `cat0` - `cat18` and 11 continuous features from `cont0` - `cont10` with total of 30 features.\n* There is no missing values in the train and test dataset.\n* `Continuous` features on train anda test dataset ranging from -0.03 to 1 which are a multimodal distribution and similar between train and test dataset.\n* Correlation between `continuous` features:\n    * `cont1` has the highest correlation with `cont2` with a correlation of 0.9.\n    * `cont10` has a high correlation of 0.8 with `cont0` and `cont7`.\n    * `Continuous` features that have a correlation of 0.7 are:\n        * between `cont0` and `cont7`\n        * between `cont8` with `cont1`\n        * between `cont8` with `cont2`\n* `Category` features in `train` and `test` are similar at each others. Below are category features that have more than 50 categories:  \n    * `cat5` has 84 categories mostly from `BI` category which has propotion around 79% followed by `AB` that has propotion around 14%.\n    * `cat7` has 51 categories with `AH`, `E` and `AS` are the top 3 categories which have propotion around 15%, 13% and 8% respectively. Total propotion on top 20 categories is around 87%.\n    * `cat8` has 61 categories with `BM`, `AE` and `AX` are the top 3 categories which have propotion of 14.1%, 8.1% and 7.4% respectively. Total propotion on top 20 categories is also around 87% same as `cat7`.\n    * `cat10` has 299 categories with `DJ`, `HK` and `DP` are the top 3 categories which have propotion of 10.5%, 10.3% and 7.9% respectively. Total propotion on top 20 categories is also around 73%.\n* Categories on `cat10` are different between `train` and `test` where `train` has 299 categories and `test` has 295 categories, this can be found on category `BS`, `JF`, `CH`, `MW`, `AW`, `FW`, `MO`, `MK`, `IL`, `GH`, `CX`, `LK` which are not found in `test` and there are `KM`, `BW`, `EJ`, `BU`, `CA`, `JM`, `DG`, `KE` which can not be found in `train`.\n* There is an imbalance data on `target` variable where target variable: `0` is 73.5% while target variable: `1` is 26.5%. This should be treated carefully especially when creating cross validation.","78dde365":"[back to top](#table-of-contents)\n<a id=\"3.1\"><\/a>\n## 3.1 Continuous features","b77934ad":"[back to top](#table-of-contents)\n<a id=\"2.5\"><\/a>\n## 2.5. Categorical features proportion\n**Train dataset**","8754b729":"[back to top](#table-of-contents)\n<a id=\"4.1\"><\/a>\n## 4.1 Preparation\n\n**Steps:**\n1. Load `packages` for performing label encoding, cross validation, modeling and AUC measurement.\n2. Combine `train` and `test` dataset, the purpose is to tackle missing categories on `train` and `test` when performing label encoding.\n3. Label encode all the `categorical` features.\n4. Split back `combine` dataset that has been label encoded into `train` and `test` dataset.","60b59044":"**Test dataset**","61b84152":"[back to top](#table-of-contents)\n<a id=\"2.2\"><\/a>\n## 2.2. Numbers of missing values","0e543c1f":"[back to top](#table-of-contents)\n<a id=\"5.2\"><\/a>\n## 5.2 XGBoost tuned model","c399d42a":"**Target: 1**","a5d88b70":"**Using categorical features only**","b331f630":"**Using continuous features only**","20cbafac":"**Using continuous features only**","8d4a5573":"[back to top](#table-of-contents)\n<a id=\"4.5\"><\/a>\n## 4.5 Average baseline model\n\n`Votingclassifier` is used to ensemble `Catboost`, `XGBoost` and `LGBM`. \n\n**Observations**:\n* As expected, using `all features` still resulting the best AUC compared to use `categorical` or `continuous` features only.\n* The AUC gap between `continuous features only` and `categorical features only` is big.\n* Below are the OOF AUC results:\n    * `all features` resulting a `0.89330` OOF AUC.\n    * `categorical features only` resulting a `0.88726` OOF AUC.\n    * `continuous features only` resulting a `0.81651` OOF AUC.","aee0930b":"[back to top](#table-of-contents)\n<a id=\"2.1\"><\/a>\n## 2.1. Numbers of rows and columns","f079e146":"**Using continuous features only**","c22df854":"[back to top](#table-of-contents)\n<a id=\"3\"><\/a>\n# 3. Features & Target Relations\n\n**Observations:**\n* `Continuous` features:\n    * Target variable: `0` is marked by color <span style='color:#facd00' > Yellow <\/span> while continuous features with target `1` marked by color <span style='color:#f088b7' > Pink <\/span>.\n    * In general, there is no distinct distribution on `continuous` features between target `0` and target `1`.\n* `Categorical` features:\n    * `cat0`: `A`, `cat5`: `BI`, `cat6`: `A`, `cat9`: `A`, `cat11`: `A`, `cat12`: `A`, `cat13`: `A`, `cat14`: `A`, `cat15`: `B`, `cat16`: `D`, `cat17`: `D` and `cat18`: `B` are categories that have more than 40% target variable: `0` of total train dataset.\n    * There is no category in `categorical` features that has target variable: `1` above 25% of total train dataset.","91783a95":"[back to top](#table-of-contents)\n<a id=\"1\"><\/a>\n# 1. Introduction\n\nKaggle competitions are incredibly fun and rewarding, but they can also be intimidating for people who are relatively new in their data science journey. In the past, Kaggle have launched many Playground competitions that are more approachable than Featured competition, and thus more beginner-friendly.\n\nThe goal of these competitions is to provide a fun, but less challenging, tabular dataset. These competitions will be great for people looking for something in between the Titanic Getting Started competition and a Featured competition.\n\nThe dataset is used for this competition is synthetic but based on a real dataset and generated using a CTGAN. The original dataset deals with predicting the amount of an insurance claim. Although the features are anonymized, they have properties relating to real-world features.","bb449999":"**Using all features**","b00bc744":"# Table of Contents\n<a id=\"table-of-contents\"><\/a>\n* [1. Introduction](#1)\n* [2. General](#2)\n    * [2.1. Numbers of rows and columns](#2.1)\n    * [2.2. Numbers of missing values](#2.2)\n    * [2.3. First 5 rows](#2.3)\n    * [2.4. Basic statistics on continuous features](#2.4)\n    * [2.5. Count on categorical features](#2.5)\n    * [2.6. Target variables](#2.6)\n* [3. Features & Target Relation](#3)\n    * [3.1. Continuous features](#3.1)\n    * [3.2. Categorical features](#3.2)\n* [4. Baseline Model](#4)\n    * [4.1 Preparation](#4.1)\n    * [4.2 Catboost baseline model](#4.2)\n    * [4.3 XGBoost baseline model](#4.3)\n    * [4.4 LGBM baseline model](#4.4)\n    * [4.5 Average baseline model](#4.5)\n* [5. Tuned Model](#5)\n    * [5.1 Catboost tuned model](#5.1)\n    * [5.2 XGBoost tuned model](#5.2)\n    * [5.3 LGBM tuned model](#5.3)\n    * [5.4 Average tuned model](#5.4)\n* [6. Winners Solutions](#6)","d3598d75":"**Using all features**","1a9aba15":"**Using all features**","6edb1a73":"[back to top](#table-of-contents)\n<a id=\"2.5\"><\/a>\n## 2.6. Target","1eb4bee7":"**Using categorical features only**"}}