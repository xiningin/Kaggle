{"cell_type":{"badab99e":"code","e6f6bd2a":"code","f627f8ef":"code","82768a30":"code","62a7a7af":"code","7dbfd31b":"code","c1bdd041":"code","3dc9cd48":"code","98f5d0eb":"code","11501300":"code","bdefb6e6":"code","f69135b3":"code","7b16f4dd":"code","8e8162d1":"code","776d6281":"code","36ce61be":"code","72e259e0":"code","b8861921":"code","7b77e183":"code","3d12cac9":"code","00377856":"code","ff1c751b":"code","ed78bf1e":"code","a0e95b80":"code","f5484ac7":"code","a7ad4b6f":"code","675d3e10":"code","c5d08837":"code","441ff49f":"code","62facc84":"code","4efd1baf":"markdown","322c6bca":"markdown","fdd980fd":"markdown","88f7790e":"markdown","17f6ffab":"markdown","47f785c6":"markdown","af578d9d":"markdown","02aa4420":"markdown","9467deb0":"markdown","1ec1b922":"markdown","35cbbb3d":"markdown","30c6750e":"markdown","78b3b005":"markdown","ef088e33":"markdown","263aaab5":"markdown","1a88b2a4":"markdown","896f94ab":"markdown","c6953fcd":"markdown","17be3725":"markdown","59270c7a":"markdown","286354d0":"markdown","22fa046d":"markdown","c20e2d65":"markdown","545c1e43":"markdown","64a8801f":"markdown","b8bb6927":"markdown","3795ba00":"markdown","4a8dc741":"markdown","c9c4cf1b":"markdown","ed12bb14":"markdown","da1b81ae":"markdown","315b41ce":"markdown","0693348a":"markdown","2825b396":"markdown","494a7c83":"markdown","56a6ac28":"markdown","928c7743":"markdown","f3cb5057":"markdown","23ebc5e2":"markdown","2d2f527c":"markdown","6520d2dc":"markdown","1d76e89f":"markdown","4befcde9":"markdown","429ab59e":"markdown","5fb99a57":"markdown","3af13715":"markdown"},"source":{"badab99e":"%reload_ext autoreload \n%autoreload 2 \n%matplotlib inline","e6f6bd2a":"from fastai import *\nfrom fastai.vision import *","f627f8ef":"bs = 64","82768a30":"path = untar_data(URLs.PETS)\npath_anno = path\/'annotations'\npath_img = path\/'images'\nfnames = get_image_files(path_img)\nnp.random.seed(2)\npat = re.compile(r'\/([^\/]+)_\\d+.jpg$')","62a7a7af":"bs=4","7dbfd31b":"# tfms=ds_tfms=get_transforms(do_flip=False, max_rotate=0, max_zoom=1, max_lighting=0, max_warp=0\n#                               )\ntfms = None","c1bdd041":"np.random.seed(42)\ndata = ImageDataBunch.from_name_re(path_img, fnames, pat, ds_tfms=tfms, size=224,  bs=bs, num_workers=0\n                                  ).normalize(imagenet_stats)\nlearn = create_cnn(data, models.resnet18, metrics=error_rate)\nlearn.fit_one_cycle(15)","3dc9cd48":"np.random.seed(42)\ndata = ImageDataBunch.from_name_re(path_img, fnames, pat, ds_tfms=get_transforms(), size=224,  bs=bs, num_workers=0\n                                  ).normalize(imagenet_stats)\nlearn = create_cnn(data, models.resnet18, metrics=error_rate)\nlearn.fit_one_cycle(15)","98f5d0eb":"tfms = get_transforms()\ntype(tfms)","11501300":"tfms = get_transforms()\nlen(tfms)","bdefb6e6":"tfms","f69135b3":"#Helper functions from fastai docs\ndef get_ex(): return open_image(path\/'images\/beagle_192.jpg')\n\ndef plots_f(rows, cols, width, height, **kwargs):\n    [get_ex().apply_tfms(tfms[0], **kwargs).show(ax=ax) for i,ax in enumerate(plt.subplots(\n        rows,cols,figsize=(width,height))[1].flatten())]","7b16f4dd":"plots_f(2, 4, 12, 6, size=224)","8e8162d1":"tfms = get_transforms(max_rotate=180)\nplots_f(2, 4, 12, 6, size=224)","776d6281":"def get_ex(): return open_image(\"..\/input\/satt.jpg\")\n\ndef plots_f_sate(rows, cols, width, height, **kwargs):\n    [get_ex.apply_tfms(tfms[0], **kwargs).show(ax=ax) for i,ax in enumerate(plt.subplots(\n        rows,cols,figsize=(width,height))[1].flatten())]\n    \ntfms = get_transforms(max_rotate=180)\nplots_f(2, 4, 12, 6, size=224)","36ce61be":"def get_ex(): return open_image(path\/'images\/beagle_192.jpg')\n\ndef plots_f(rows, cols, width, height, **kwargs):\n    [get_ex().apply_tfms(tfms[0], **kwargs).show(ax=ax) for i,ax in enumerate(plt.subplots(\n        rows,cols,figsize=(width,height))[1].flatten())]","72e259e0":"fig, axs = plt.subplots(1,5,figsize=(12,4))\nfor change, ax in zip(np.linspace(0.1,0.9,5), axs):\n    brightness(get_ex(), change).show(ax=ax, title=f'change={change:.1f}')","b8861921":"def get_ex(): return open_image(\"..\/input\/contrast_example.jpg\")\n\ndef plots_f(rows, cols, width, height, **kwargs):\n    [get_ex().apply_tfms(tfms[0], **kwargs).show(ax=ax) for i,ax in enumerate(plt.subplots(\n        rows,cols,figsize=(width,height))[1].flatten())]\n    \nfig, axs = plt.subplots(1,5,figsize=(48,24))\nfor change, ax in zip(np.linspace(0.1,0.9,5), axs):\n    brightness(get_ex(), change).show(ax=ax, title=f'change={change:.1f}')\n","7b77e183":"def get_ex(): return open_image(path\/'images\/beagle_192.jpg')\n\ndef plots_f(rows, cols, width, height, **kwargs):\n    [get_ex().apply_tfms(tfms[0], **kwargs).show(ax=ax) for i,ax in enumerate(plt.subplots(\n        rows,cols,figsize=(width,height))[1].flatten())]\n\nfig, axs = plt.subplots(1,5,figsize=(12,4))\nfor scale, ax in zip(np.exp(np.linspace(log(0.5),log(2),5)), axs):\n    contrast(get_ex(), scale).show(ax=ax, title=f'scale={scale:.2f}')\n","3d12cac9":"def get_ex(): return open_image(\"..\/input\/contrast_example.jpg\")\n\ndef plots_f(rows, cols, width, height, **kwargs):\n    [get_ex().apply_tfms(tfms[0], **kwargs).show(ax=ax) for i,ax in enumerate(plt.subplots(\n        rows,cols,figsize=(width,height))[1].flatten())]\n    \nfig, axs = plt.subplots(1,5,figsize=(48,4))\nfor scale, ax in zip(np.exp(np.linspace(log(0.5),log(2),5)), axs):\n    contrast(get_ex(), scale).show(ax=ax, title=f'scale={scale:.2f}')","00377856":"fig, axs = plt.subplots(1,5,figsize=(12,4))\nfor center, ax in zip([[0.,0.], [0.,1.],[0.5,0.5],[1.,0.], [1.,1.]], axs):\n    crop(get_ex(), 300, *center).show(ax=ax, title=f'center=({center[0]}, {center[1]})')","ff1c751b":"fig, axs = plt.subplots(1,5,figsize=(12,4))\nfor size, ax in zip(np.linspace(200,600,5), axs):\n    crop_pad(get_ex(), int(size), 'zeros', 0.,0.).show(ax=ax, title=f'size = {int(size)}')","ed78bf1e":"fig, axs = plt.subplots(1,5,figsize=(12,4))\nfor size, ax in zip(np.linspace(200,600,5), axs):\n    crop_pad(get_ex(), int(size), 'reflection', 0.,0.).show(ax=ax, title=f'size = {int(size)}')","a0e95b80":"def get_ex(): return open_image(path\/'images\/beagle_192.jpg')\n\ndef plots_f(rows, cols, width, height, **kwargs):\n    [get_ex().apply_tfms(tfms[0], **kwargs).show(ax=ax) for i,ax in enumerate(plt.subplots(\n        rows,cols,figsize=(width,height))[1].flatten())]\n\n    \nfig, axs = plt.subplots(2,4,figsize=(12,8))\nfor k, ax in enumerate(axs.flatten()):\n    dihedral(get_ex(), k).show(ax=ax, title=f'k={k}')\nplt.tight_layout()","f5484ac7":"def get_ex(): return open_image(\"..\/input\/satt.jpg\")\n\ndef plots_f_sate(rows, cols, width, height, **kwargs):\n    [get_ex.apply_tfms(tfms[0], **kwargs).show(ax=ax) for i,ax in enumerate(plt.subplots(\n        rows,cols,figsize=(width,height))[1].flatten())]\n    \ntfms = get_transforms(max_rotate=180)\nplots_f(2, 4, 12, 6, size=224)","a7ad4b6f":"def get_ex(): return open_image(path\/'images\/beagle_192.jpg')\n\ndef plots_f(rows, cols, width, height, **kwargs):\n    [get_ex().apply_tfms(tfms[0], **kwargs).show(ax=ax) for i,ax in enumerate(plt.subplots(\n        rows,cols,figsize=(width,height))[1].flatten())]\n\nfig, axs = plt.subplots(1,5,figsize=(12,4))\nfor magnitude, ax in zip(np.linspace(-0.05,0.05,5), axs):\n    tfm = jitter(magnitude=magnitude)\n    get_ex().jitter(magnitude).show(ax=ax, title=f'magnitude={magnitude:.2f}')","675d3e10":"def get_ex(): return open_image(path\/'images\/beagle_192.jpg')\n\ndef plots_f(rows, cols, width, height, **kwargs):\n    [get_ex().apply_tfms(tfms[0], **kwargs).show(ax=ax) for i,ax in enumerate(plt.subplots(\n        rows,cols,figsize=(width,height))[1].flatten())]\n\nfig, axs = plt.subplots(2,4,figsize=(12,8))\nfor i, ax in enumerate(axs.flatten()):\n    magnitudes = torch.tensor(np.zeros(8))\n    magnitudes[i] = 0.5\n    perspective_warp(get_ex(), magnitudes).show(ax=ax, title=f'coord {i}')","c5d08837":"def get_ex(): return open_image(\"..\/input\/contrast_example.jpg\")\n\ndef plots_f(rows, cols, width, height, **kwargs):\n    [get_ex().apply_tfms(tfms[0], **kwargs).show(ax=ax) for i,ax in enumerate(plt.subplots(\n        rows,cols,figsize=(width,height))[1].flatten())]\n\n    \nfig, axs = plt.subplots(2,4,figsize=(12,8))\nfor i, ax in enumerate(axs.flatten()):\n    magnitudes = torch.tensor(np.zeros(8))\n    magnitudes[i] = 0.5\n    perspective_warp(get_ex(), magnitudes).show(ax=ax, title=f'coord {i}')","441ff49f":"def get_ex(): return open_image(\"..\/input\/cereal_ex.jpg\")\n\ndef plots_f(rows, cols, width, height, **kwargs):\n    [get_ex().apply_tfms(tfms[0], **kwargs).show(ax=ax) for i,ax in enumerate(plt.subplots(\n        rows,cols,figsize=(width,height))[1].flatten())]\n    \ntfm = symmetric_warp(magnitude=(-0.2,0.2))\n_, axs = plt.subplots(2,4,figsize=(12,6))\nfor ax in axs.flatten():\n    img = get_ex().apply_tfms(tfm, padding_mode='zeros')\n    img.show(ax=ax)","62facc84":"def get_ex(): return open_image(\"..\/input\/cereal_ex.jpg\")\n\ndef plots_f(rows, cols, width, height, **kwargs):\n    [get_ex().apply_tfms(tfms[0], **kwargs).show(ax=ax) for i,ax in enumerate(plt.subplots(\n        rows,cols,figsize=(width,height))[1].flatten())]\n    \nfig, axs = plt.subplots(2,4,figsize=(12,8))\nfor i in range(4):\n    get_ex().tilt(i, 0.4).show(ax=axs[0,i], title=f'direction={i}, fwd')\n    get_ex().tilt(i, -0.4).show(ax=axs[1,i], title=f'direction={i}, bwd')","4efd1baf":"Following is the list of transforms supported by the library.\n\nWe'll take a look at where these might be useful.\n\n## List of transforms:\n\n[Source: fastai docs](https:\/\/docs.fast.ai\/vision.transform.html)\n\n- brightness\n- contrast\n- crop\n- crop_pad\n- dihedral\n- dihedral_affine\n- flip_lr\n- flip_affine\n- jitter\n- pad\n- perspective_warp\n- resize\n- rotate\n- skew\n- squish\n- symmetric_warp\n- tilt\n- zoom\n- cutout\n\n### Convenience functions\n- rand_crop\n- rand_pad\n- rand_zoom","322c6bca":"Dihedral transforms rotates the images in the 8 possible directions\/angles of a dihedron.","fdd980fd":"## Cutout","88f7790e":"## Brightness","17f6ffab":"## perspective_warp","47f785c6":"Disabling everything","af578d9d":"## jitter","02aa4420":"Consider the case where your user for the OCR app takes the image at an angle and this transform might come in handy for that scenario","9467deb0":"## Tilt","1ec1b922":"Crop_pad, crops and pads based on the set mode. The one recommended by fastai dev(s) is \"reflection padding\". See the examples below for zero and reflection padding.","35cbbb3d":"## Dihedral","30c6750e":"We can vary the brightness from 0 to 1, 0.5 being the default.","78b3b005":"## Does Data Augmentation really help?\n\nBelow is a simple comparision of 2 training loops, one where we run with data augmentations off and on respectively. \nThis is to compare our resulting accuracy\/loss values.\nLet's train Resnet 18 architecture for 15 epochs, with data augmentation off and on.","ef088e33":"## Data Augmentation ON","263aaab5":"Tilt can be another transform handy for the same scenario.","1a88b2a4":"```\ndef get_ex(): return open_image(path\/'images\/beagle_192.jpg')\n\ndef plots_f(rows, cols, width, height, **kwargs):\n    [get_ex().apply_tfms(tfms[0], **kwargs).show(ax=ax) for i,ax in enumerate(plt.subplots(\n        rows,cols,figsize=(width,height))[1].flatten())]\n\ntfms = [cutout(n_holes=(1,4), length=(10, 160), p=.5)]\nfig, axs = plt.subplots(1,5,figsize=(12,4))\nfor ax in axs:\n    get_ex().apply_tfms(tfms).show(ax=ax)\n\n```","896f94ab":"For this loop, we enable the defaults and create a separate learner.","c6953fcd":"# fin","17be3725":"The tuple contating transforms has 2 lists nested. \n\nOne is for the training dataset.\n\nSecond one is for the validation dataset that involves minimal transforms\/just resizing.","59270c7a":"By default, fastai already does a few transforms as named above, these are defined by calling get_transforms()\nLet's try re-sizing the image to 224 pixels","286354d0":"## An Introduction to Image Augmentation using fastai Library","22fa046d":"This transform changes the perspective of the image as if our object was moved around.","c20e2d65":"Consider the example below of a sattelite image where this might be useful.","545c1e43":"[](http:\/\/)[Code from fastai Lesson 1](course.fast.ai)\n\nLet's first download the PETS Dataset to do a proof by example. \n\nAfter downloading the dataset, we've set the reg_ex","64a8801f":"By example, symmetric warp might be handy when you're building a product detector and you need to add more examples\/different angles to your dataset.","b8bb6927":"Setting the magic functions.","3795ba00":"## symmetric_warp","4a8dc741":"## Resizing","c9c4cf1b":"Contrast can be varied from a scale of 0 to 2. \n1 being the default","ed12bb14":"I'll be using the fastai vision module to showcase the transforms featured inside of the library.","da1b81ae":"Lets again look at the example from the previous scenario. \n\nFor performing text extraction, we'd like to have the text \"stand-out\" as much from the background as possible. \nFor our example, 2 seems to be the best value.","315b41ce":"Crop helps crop in and out of our images.","0693348a":"```\nget_transforms(do_flip:bool=True, flip_vert:bool=False, max_rotate:float=10.0, max_zoom:float=1.1, max_lighting:float=0.2, max_warp:float=0.2, p_affine:float=0.75, p_lighting:float=0.75, xtra_tfms:Optional[Collection[Transform]]=None) \u2192 Collection[Transform]\n```","2825b396":"Jitter is used to add \"jitter\/noise\" to the image","494a7c83":"## Data Aug OFF","56a6ac28":"## Transformations in fastai\n\n- How do we enable transforms?\n- Knowing the defaults.\n- List of Transforms \n- Examples and potential use-cases\n- Under the hood: Where and how are these implemented?","928c7743":"Source: [fastai docs](docs.fast.ai)","f3cb5057":"## Contrast","23ebc5e2":"## Crop","2d2f527c":"## Rotation\n(max_rotate=angle) toggles random rotations between -angle to + angle specified here.","6520d2dc":"### Why do we have 2 tfms?","1d76e89f":"Set batch_size to lower values if this gives errors.","4befcde9":"Let's look at an example, where our target is to perform text extraction on this image. \nBased on the real world scenario, we might have to play around with the brightness for such cases. In the example below, 0.3 seems to be the best result","429ab59e":"## Crop_pad","5fb99a57":"### Enabling Transforms\n\nTransforms are passed on when creating the \"ImageDataBunch\" objects.\n\nGenearlly, you may enable the \"Default\" transforms by calling ```tfms = get_transforms()```. Let's check the default transforms:","3af13715":"This might make more sense in a sattelite image. Let's look at an example"}}