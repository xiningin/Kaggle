{"cell_type":{"b05ff360":"code","ab505439":"code","7bf578a9":"code","41b596fb":"code","f9f445f7":"code","d98767c7":"code","8e5d316f":"code","75b36351":"code","f0b96ea6":"code","bd83de24":"code","640d9e5e":"code","44c4fdb4":"code","d44a37ff":"code","c7e6dd7d":"code","71ba18cc":"code","73803ac9":"code","532387e1":"code","be815884":"code","bf12ef19":"code","834e64fd":"code","a837a91a":"code","45ed99a3":"code","60eb7854":"code","79735c9a":"code","60b127e5":"code","d10562ec":"code","4f6c71f2":"code","84a7ef40":"code","548ce719":"code","59e4301e":"markdown","3ef5dd7d":"markdown","cce42606":"markdown","27140778":"markdown","234825ae":"markdown","2e367ecc":"markdown","e366b4b9":"markdown","1636a493":"markdown","6d78e661":"markdown","605037b5":"markdown","8b385972":"markdown","c8befa6f":"markdown","9befaacb":"markdown","db0c6fb4":"markdown"},"source":{"b05ff360":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","ab505439":"# Import relevant libraries\nimport pandas as pd\nimport numpy as np\nimport os\nimport glob\nfrom functools import reduce\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport xgboost as xgb\nfrom sklearn.datasets import load_boston\nfrom sklearn.model_selection import train_test_split\nimport matplotlib.pyplot as plt\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import GridSearchCV\nimport keras\nfrom keras.callbacks import ModelCheckpoint\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Activation, Flatten\nfrom keras import backend as K\nimport tensorflow as tf\n\n# Import training data\ntrain = pd.read_csv('..\/input\/optiver-realized-volatility-prediction\/train.csv')\n\n# Grab Unique Array IDs\nstock_id_array = train.stock_id.unique()\n\n","7bf578a9":"train.head()","41b596fb":"train.shape","f9f445f7":"# Take a sample from the book data\n\nsample = pd.read_parquet(f'..\/input\/optiver-realized-volatility-prediction\/book_train.parquet\/stock_id=1')","d98767c7":"sample.head()","8e5d316f":"sample.shape","75b36351":"# Calculate Bid Ask Spread\n\n# BAS calculation for a stock\ndef bas_calculation_per_id(stock_id, data_type):\n    df_book_data = pd.read_parquet(f'..\/input\/optiver-realized-volatility-prediction\/book_{data_type}.parquet\/stock_id={stock_id}')\n    df_book_data['bas'] = df_book_data[['ask_price1', 'ask_price2']].min(axis=1)\/df_book_data[['bid_price1', 'bid_price2']].max(axis=1) - 1\n    df_book_data['stock_id'] = stock_id\n    return df_book_data\n\n# Loop through each stock\ndef bas_calculation(stock_id_array, data_type):\n    df_bas = pd.DataFrame()\n    for stock_id in stock_id_array:\n        df_bas_id = bas_calculation_per_id(stock_id, data_type).groupby(by = ['stock_id', 'time_id'], as_index = False)['bas'].mean()\n        df_bas = pd.concat([df_bas,df_bas_id])\n    return df_bas\n\ndf_bas = bas_calculation(stock_id_array, 'train')\nprint(df_bas)","f0b96ea6":"# Calculate Weighted Average Price\n\n# WAP calculation for a stock\ndef wap_calculation_per_id(stock_id, data_type):\n    df_book_data = pd.read_parquet(f'..\/input\/optiver-realized-volatility-prediction\/book_{data_type}.parquet\/stock_id={stock_id}')\n    df_book_data['stock_id'] = stock_id\n    df_book_data['wap'] = (df_book_data['bid_price1'] * df_book_data['ask_size1'] + df_book_data['ask_price1'] \n                           * df_book_data['bid_size1']) \/ (df_book_data['bid_size1'] + df_book_data['ask_size1'])           \n    return df_book_data[['stock_id', 'time_id', 'seconds_in_bucket', 'wap']]\n\n# Loop through each stock\ndef wap_calculation(stock_id_array, data_type):\n    df_wap = pd.DataFrame()\n    for stock_id in stock_id_array:\n        df_wap_id = wap_calculation_per_id(stock_id, data_type).groupby(by = ['stock_id', 'time_id'], as_index = False)['wap'].mean()\n        df_wap = pd.concat([df_wap,df_wap_id])\n    return df_wap\n\ndf_wap = wap_calculation(stock_id_array, 'train')\nprint(df_wap)","bd83de24":"# Calculate Log Returns (LR)\n\n# LR calculation for a list of stock prices\ndef lr(list_stock_prices):\n    list_stock_prices['lr'] = np.log(list_stock_prices['wap']).diff()\n    return list_stock_prices\n\n# LR calculation for a stock\ndef lr_calculation_per_id(stock_id, data_type):\n    df_book_data = wap_calculation_per_id(stock_id, data_type)\n    df_lr_per_id = lr(df_book_data)\n    return df_lr_per_id[['stock_id', 'time_id', 'seconds_in_bucket', 'lr']]\n\n# Loop through each stock    \ndef lr_calculation(stock_id_array, data_type):\n    df_lr = pd.DataFrame()\n    for stock_id in stock_id_array:\n        df_lr_id = lr_calculation_per_id(stock_id, data_type).groupby(by = ['stock_id', 'time_id'], as_index = False)['lr'].mean()\n        df_lr = pd.concat([df_lr,df_lr_id])\n    return df_lr\n\ndf_lr = lr_calculation(stock_id_array, 'train')\nprint(df_lr)","640d9e5e":"# Calculate Past Realized Volatility (RV)\n\n# RV calculation for a series of LRs\ndef realized_volatility(series_log_return):\n    return np.sqrt(np.sum(series_log_return['lr']**2))\n    \n# Loop through each stock\ndef rv_calculation(stock_id_array, data_type):\n    df_rv = pd.DataFrame()\n    for stock_id in stock_id_array:\n        df_book_data = lr_calculation_per_id(stock_id, data_type)\n        df_rv_id = df_book_data.groupby(by = ['stock_id', 'time_id'], as_index = False).apply(realized_volatility)\n        df_rv_id.columns = ['stock_id', 'time_id', 'rv']\n        df_rv = pd.concat([df_rv,df_rv_id])\n    return df_rv\n\ndf_rv = rv_calculation(stock_id_array, 'train')\nprint(df_rv)","44c4fdb4":"# Grab and Aggregate other given Relevant Features\n\n# WAP calculation for a stock\ndef wap_calculation_per_id(stock_id, data_type):\n    df_book_data = pd.read_parquet(f'..\/input\/optiver-realized-volatility-prediction\/book_{data_type}.parquet\/stock_id={stock_id}')\n    df_book_data['stock_id'] = stock_id\n    df_book_data['wap'] = (df_book_data['bid_price1'] * df_book_data['ask_size1'] + df_book_data['ask_price1'] \n                           * df_book_data['bid_size1']) \/ (df_book_data['bid_size1'] + df_book_data['ask_size1'])           \n    return df_book_data[['stock_id', 'time_id', 'seconds_in_bucket', 'wap']]\n\n# Loop through each stock\ndef wap_calculation(stock_id_array, data_type):\n    df_wap = pd.DataFrame()\n    for stock_id in stock_id_array:\n        df_wap_id = wap_calculation_per_id(stock_id, data_type).groupby(by = ['stock_id', 'time_id'], as_index = False)['wap'].mean()\n        df_wap = pd.concat([df_wap,df_wap_id])\n    return df_wap\n\ndf_wap = wap_calculation(stock_id_array, 'train')\nprint(df_wap)","d44a37ff":"# Gather and Aggregate other given features\n\n# Grab features for each stock\ndef other_calculation_per_id(stock_id, data_type):\n    df_book_data = pd.read_parquet(f'..\/input\/optiver-realized-volatility-prediction\/book_{data_type}.parquet\/stock_id={stock_id}')\n    df_book_data['stock_id'] = stock_id\n    df = df_book_data.groupby(by = ['stock_id', 'time_id'], \n                      as_index = False)[['bid_price1', 'ask_price1', 'bid_price2', \n                      'ask_price2', 'bid_size1', 'ask_size1', 'bid_size2', 'ask_size2']].mean()\n    df_two =  df_book_data.groupby(by = ['stock_id', 'time_id'],as_index = False)[['seconds_in_bucket']].max()\n    return pd.merge(df, df_two,  how='left', left_on=['stock_id','time_id'], right_on = ['stock_id','time_id'])\n\n# Loop through each stock\ndef other_calculation(stock_id_array, data_type):\n    df_other = pd.DataFrame()\n    for stock_id in stock_id_array:\n        df_other_id = other_calculation_per_id(stock_id, data_type)\n        df_other = pd.concat([df_other,df_other_id])\n    return df_other\n\ndf_other = other_calculation(stock_id_array, 'train')\nprint(df_other)","c7e6dd7d":"# Concatonate each statistic into one dataframe\n\ndf_features = df_bas.merge(df_wap,on=['stock_id', 'time_id'])\ndf_features = df_features.merge(df_lr,on=['stock_id', 'time_id'])\ndf_features = df_features.merge(df_rv,on=['stock_id', 'time_id'])\ndf_features = df_features.merge(df_other,on=['stock_id', 'time_id'])\ndf_features = df_features.merge(train,on=['stock_id', 'time_id'])\n\n# Delete rows with NaN or missing values\ndf_features = df_features.dropna()\n\n# Make feature dataframe copy\ndf_features_copy = df_features\nprint(df_features)","71ba18cc":"df_example = df_features.loc[(df_features['stock_id'] == 0)]\n\ndf_example.plot.scatter(x = \"time_id\", y = 'target')","73803ac9":"feature_array = ['bas', 'wap', 'lr', 'rv', 'seconds_in_bucket', \n                 'bid_price1', 'ask_price1', 'bid_price2', 'ask_price2', \n                 'bid_size1', 'ask_size1', 'bid_size2', 'ask_size2']\n\nfor feature in feature_array:\n    df_example.plot.scatter(x = feature, y = 'target')\n\ncorr = df_example[feature_array].corr()\nfig, ax = plt.subplots(figsize=(15, 15))\nax.matshow(corr)\nplt.xticks(range(len(corr.columns)), corr.columns)\nplt.yticks(range(len(corr.columns)), corr.columns)","532387e1":"scalers = []\nfor i in range(len(feature_array)):\n    scaler = StandardScaler()\n    feature = feature_array[i]\n    df_features[feature] = scaler.fit_transform(np.asarray(df_features[feature]).reshape(-1,1))\n    scalers.append(scaler)","be815884":"# Examine Data spread for stock ID 0 features again\ndf_example = df_features.loc[(df_features['stock_id'] == 0)]\n\nfor feature in feature_array:\n    df_example.plot.scatter(x = feature, y = 'target')\n\ncorr = df_example[feature_array].corr()\nfig, ax = plt.subplots(figsize=(15, 15))\nax.matshow(corr)\nplt.xticks(range(len(corr.columns)), corr.columns)\nplt.yticks(range(len(corr.columns)), corr.columns)","bf12ef19":"# Train Test Splitting\n\nX = df_features[feature_array]\nY = df_features['target']\n\nX_train, X_test, Y_train, Y_test = train_test_split(X, Y, train_size = .9)\n\n# Train Test Splitting for pre-standardized data\n\nX_copy = df_features_copy[feature_array]\nY_copy = df_features_copy['target']\n\nX_train_copy, X_test_copy, Y_train_copy, Y_test_copy = train_test_split(X_copy, Y_copy, train_size = .9)","834e64fd":"# Linear Regression Model Fitting \n\nlinreg_copy = LinearRegression().fit(X_train_copy, Y_train_copy)\nprint(\"Pre Standardization\")\nprint(\"R2 Score: \")\nprint(linreg_copy.score(X_train_copy, Y_train_copy))\nprint(\"Coefficients: \")\nprint(linreg_copy.coef_)\nprint(\"Intercepts: \")\nprint(linreg_copy.intercept_)\n\nlinreg = LinearRegression().fit(X_train, Y_train)\nprint(\"Post Standardization\")\nprint(\"R2 Score: \")\nprint(linreg.score(X_train, Y_train))\nprint(\"Coefficients: \")\nprint(linreg.coef_)\nprint(\"Intercepts: \")\nprint(linreg.intercept_)","a837a91a":"# XGBoost Model Fitting \n\nregressor_copy = xgb.XGBRegressor(\n    n_estimators=50,\n    reg_lambda=1,\n    gamma=0,\n    max_depth=20\n)\n\nregressor_copy.fit(X_train_copy, Y_train_copy)\n\n\nregressor = xgb.XGBRegressor(\n    n_estimators=50,\n    reg_lambda=1,\n    gamma=0,\n    max_depth=20\n)\n\nregressor.fit(X_train, Y_train)","45ed99a3":"# Examine Feature Importance\nprint(\"Pre Standardization Feature Importance\")\nprint(pd.DataFrame(regressor_copy.feature_importances_.reshape(1, -1), columns=feature_array))\n\nprint(\"Post Standardization Feature Importance\")\nprint(pd.DataFrame(regressor.feature_importances_.reshape(1, -1), columns=feature_array))","60eb7854":"# Calculate Root Mean Squared Prediction Error (RMSPE)\n\ndef RMSPE(actual, predict):\n    return (np.sqrt(np.mean(np.square((actual - predict) \/ actual))))\n\nY_predict_copy = linreg_copy.predict(X_test_copy)\nerror = RMSPE(Y_test_copy, Y_predict_copy)\nprint(\"Pre Standardization\")\nprint(\"Linear Regression Error:\")\nprint(error)\nY_predict_copy = regressor_copy.predict(X_test_copy)\nerror = RMSPE(Y_test_copy, Y_predict_copy)\nprint(\"XGBoost Error:\")\nprint(error)\n\n\nY_predict = linreg.predict(X_test)\nerror = RMSPE(Y_test, Y_predict)\nprint(\"Post Standardization\")\nprint(\"Linear Regression Error:\")\nprint(error)\nY_predict = regressor.predict(X_test)\nerror = RMSPE(Y_test, Y_predict)\nprint(\"XGBoost Error:\")\nprint(error)","79735c9a":"# XGBRegressor Hyperparameter Tuning\n\n\"\"\"params = {\n    'reg_lambda': [1],\n    'gamma': [0],\n    'learning_rate': [0.01, 0.1],\n    'min_child_weight': [1, 3, 5],\n    'n_estimators' : [50 ,100, 200],\n    'max_depth': [20, 40, 60],\n    'objective': ['reg:squarederror']\n}\n\nxgb_regressor = xgb.XGBRegressor()\n\ngsearch = GridSearchCV(estimator = xgb_regressor,\n                       param_grid = params,\n                       # 2 Stratified Kfold\n                       cv = 2,\n                       # Use All Processors\n                       n_jobs = -1,\n                       # Display time taken per fold and paramater candidate is displayed\n                       # Display score\n                       verbose = 4)\n\ngsearch.fit(X_train,Y_train)\n\nprint(gsearch.best_params_)\"\"\"\n\n# May take 5+ hours to run. Run this only once if needed.","60b127e5":"# Test Best Hyperparameter Configuration on Validation Data\n\nregressor = xgb.XGBRegressor(\n    reg_lambda=1,\n    gamma=0,\n    max_depth=20,\n    learning_rate=0.1,  \n    min_child_weight=3, \n    n_estimators=200,\n    objective='reg:squarederror'\n)\n\nregressor.fit(X_train, Y_train)\n\nY_predict = regressor.predict(X_test)\nerror = RMSPE(Y_test, Y_predict)\nprint(\"Hyperparameter Tuned XGBoost Error:\")\nprint(error)","d10562ec":"# Create RMSPE Function for the keras model\ndef rmspe(y_true, y_pred):\n    return K.sqrt(K.mean(K.square((y_true - y_pred) \/ y_true)))\n\n# Load Sequential to start forming our DNN\nDNN = Sequential()\n\n# The 128 Unit Input Layer:\nDNN.add(Dense(128, kernel_initializer='normal',input_dim = X_train.shape[1], activation='relu'))\n\n# The 256 Unit Hidden Layer:\nDNN.add(Dense(256, kernel_initializer='normal',activation='relu'))\n\n# The 128 Unit Hidden Layer:\nDNN.add(Dense(128, kernel_initializer='normal',activation='relu'))\n\n# The 64 Unit Hidden Layer:\nDNN.add(Dense(64, kernel_initializer='normal',activation='relu'))\n\n# The 32 Unit Hidden Layer\nDNN.add(Dense(32, kernel_initializer='normal',activation='relu'))\n\n# The 16 Unit Hidden Layer\nDNN.add(Dense(16, kernel_initializer='normal',activation='relu'))\n\n# The 1 Unit Outer Layer:\nDNN.add(Dense(1, kernel_initializer='normal',activation='linear'))\n\n# Compile the network:\nDNN.compile(loss=rmspe, optimizer='adam')\nDNN.summary()","4f6c71f2":"# Initialize Callbacks\n\"\"\"my_callbacks = [\n    tf.keras.callbacks.EarlyStopping(patience=2),\n    tf.keras.callbacks.ModelCheckpoint(filepath='model.{epoch:02d}-{val_loss:.2f}.h5'),\n    tf.keras.callbacks.TensorBoard(log_dir='.\/logs'),\n]\"\"\"\n\n# Fit the DNN to our training data\nhistory = DNN.fit(X, Y, epochs=50, batch_size=128, validation_split = 0.2, verbose=1)","84a7ef40":"# Plot loss and validation loss per epoch\npd.DataFrame(history.history).plot(figsize=(8,5))\nplt.show()","548ce719":"# Import test Data\n\ntest = pd.read_csv('..\/input\/optiver-realized-volatility-prediction\/test.csv')\n\n# Grab Unique Array IDs\nstock_id_array = test.stock_id.unique()\n\ndf_bas = bas_calculation(stock_id_array, 'test')\ndf_wap = wap_calculation(stock_id_array, 'test')\ndf_lr = lr_calculation(stock_id_array, 'test')\ndf_rv = rv_calculation(stock_id_array, 'test')\ndf_other = other_calculation(stock_id_array, 'test')\n\n# Concatonate each statistic into one dataframe\n\ndf_features = df_bas.merge(df_wap,on=['stock_id', 'time_id'])\ndf_features = df_features.merge(df_lr,on=['stock_id', 'time_id'])\ndf_features = df_features.merge(df_rv,on=['stock_id', 'time_id'])\ndf_features = df_features.merge(df_other,on=['stock_id', 'time_id'])\ndf_features = df_features.merge(test,on=['stock_id', 'time_id'])\n\n# Standardize the feature data\n\nfor i in range(len(feature_array)):\n    scaler = scalers[i]\n    feature = feature_array[i]\n    df_features[feature] = scaler.transform(np.asarray(df_features[feature]).reshape(-1,1))\n\n# Make Predictions\nX_predict = df_features[feature_array]\nY_predict = DNN.predict(X_predict)\n\ndata = []\nfor i in range(len(Y_predict)):\n    data.append([str(df_features['stock_id'].iloc[i]) + \"-\" + str(df_features['time_id'].iloc[i]), Y_predict[i][0]])\ndf = pd.DataFrame(data, columns = ['row_id', 'target'])\ndf.to_csv('submission.csv', index=False)\npd.read_csv('submission.csv')","59e4301e":"Even with standardization, our features appear to follow the same pattern in relation to the target as they did pre-standardization. We will try out both standardized and pre-standardized training data in our following models.","3ef5dd7d":"Our DNN seems to have beaten our previous regression models in RMSPE by quite a bit, so we will be using it in our final predictions.","cce42606":"Appears as though there is no correlation between time ID and realized volatility, so we will go on looking at the more relevant features we have gathered.","27140778":"As expected, past bid ask spread and realized volatility had the most importance in determining future, followed by Log Return, and our other features with much less importance.","234825ae":"We will now attempt to create a regression model from a Deep Neural Network (DNN) that aims to lower the best root mean squared error from our previous models. Due to computing constraints, we will only use standardized data on a single DNN.\n\nThe model will be as follows:\n\n1 Input Layer with Relu Activation, 128 Units, and a input dimension that matches our training data shape\n \n1 Hidden Layer with Relu Activation and 256 Units\n\n1 Hidden Layer with Relu Activation and 128 Units\n\n1 Hidden Layer with Relu Activation and 64 Units\n\n1 Hidden Layer with Relu Activation and 32 Units\n\n1 Hidden Layer with Relu Activation and 16 Units\n\n1 Output Layer with Linear Activation and 1 Unit to match our output data shape\n\nAll layers will be kernel initalized to normal to match our standardized data and we will use the Adam optimizer as it appears to have give us the smoothest downward trend in our loss and validation loss history plot.","2e367ecc":"**Linear Regression and XGBoost Regression**","e366b4b9":" **Predictions for the true test set**","1636a493":"**Apply Standard Scaling**","6d78e661":"Since our XGBRegressor model with vanilla parameters did fairly well (below .4 RMSPE for both standardized and non-standardized data), we will be using the same model with tuned hyperparameters to make our final test predictions.","605037b5":"We will now examine the structure of the data that we were provided.","8b385972":"Higher than the error from the other naive approach given to us in Optiver's tutorial, but not terrible for a first approach. There are still quite a few things we could obviously optimize in terms of our XGBoost Regression model, which has almost default settings right now. Additionally, standardization appeared to help in our models (the results above may not show it, but the majority of times that this has run, standardization has helped), so we will use that for our predictions.","c8befa6f":"There appears to be a relationship that we can capture with regression models between realized volatility, bid ask spread (and associated features), and past realized volatility. We will add the other features, which appear to have little correlation with realized volatility, to see if our later models can catch a pattern that we cannot see. The data also appears to need standardization before we train our models on them because of how small and spread out the feature data is. We will now apply standardization to all relevant features.","9befaacb":"Let us now explore stock ID 0 at all time IDs and try to spot the patterns within our features, if there exists any in the first place.","db0c6fb4":"**Feature Gathering**\nAside from the other features already given to us, we want to gather the 4 statistics that we can develop with our given data and that are most likely related to future realized volatility: Bid\/Ask Spread (BAS), Weighted Average Price (WAP), Log Return, and Past Realized Volatility. Since BAS, WAP, and Log Returns are calculated upon each second in bucket, we will have to aggregate these so that there is only one BAS and WAP per time ID and stock ID pair. We will also need to aggregate the other features we are already given from the competition repository."}}