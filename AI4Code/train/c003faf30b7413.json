{"cell_type":{"6f9a7f00":"code","c12653b3":"code","38a9113d":"code","c0fcc9a5":"code","f9e70454":"code","2d25f54b":"code","b8d37b73":"code","7c0ec492":"code","23c72807":"code","7a5ec7da":"code","c7c5d9f9":"code","5b16025a":"code","dea8d4b3":"code","bf73875a":"code","a5a7a888":"code","aceb0707":"code","59639054":"markdown","14bbd2c0":"markdown","73903194":"markdown","4db5c1a4":"markdown","1d6500f4":"markdown","d4cdd579":"markdown","650083f8":"markdown","626c99c4":"markdown"},"source":{"6f9a7f00":"import sys\nsys.path.append('..\/input\/timm-pytorch-image-models\/pytorch-image-models-master')","c12653b3":"import os\nimport cv2\nimport math\nimport random\nimport numpy as np\nimport pandas as pd\nfrom tqdm import tqdm\n\nimport albumentations\nfrom albumentations.pytorch.transforms import ToTensorV2\n\nimport torch\nimport timm\nimport torch\nimport torch.nn as nn\nfrom torch.nn import functional as F\nfrom torch.utils.data import Dataset,DataLoader\n\nimport gc\nimport matplotlib.pyplot as plt\nimport cudf\nimport cuml\nimport cupy\nfrom cuml.feature_extraction.text import TfidfVectorizer\nfrom cuml import PCA\nfrom cuml.neighbors import NearestNeighbors","38a9113d":"class CFG:\n    seed = 54\n    classes = 11014 \n    scale = 30 \n    margin = 0.5\n    model_name =  'tf_efficientnet_b4'\n    fc_dim = 512\n    img_size = 512\n    batch_size = 20\n    num_workers = 4\n    device = device = 'cuda' if torch.cuda.is_available() else 'cpu'\n    model_path = '..\/input\/utils-shopee\/arcface_512x512_tf_efficientnet_b4_LR.pt'","c0fcc9a5":"def read_dataset():\n\n    df = pd.read_csv('..\/input\/shopee-product-matching\/test.csv')\n    df_cu = cudf.DataFrame(df)\n    image_paths = '..\/input\/shopee-product-matching\/test_images\/' + df['image']\n\n    return df, df_cu, image_paths","f9e70454":"def seed_torch(seed=42):\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True\n    \nseed_torch(CFG.seed)","2d25f54b":"def f1_score(y_true, y_pred):\n    y_true = y_true.apply(lambda x: set(x.split()))\n    y_pred = y_pred.apply(lambda x: set(x.split()))\n    intersection = np.array([len(x[0] & x[1]) for x in zip(y_true, y_pred)])\n    len_y_pred = y_pred.apply(lambda x: len(x)).values\n    len_y_true = y_true.apply(lambda x: len(x)).values\n    f1 = 2 * intersection \/ (len_y_pred + len_y_true)\n    return f1","b8d37b73":"def combine_predictions(row):\n    x = np.concatenate([row['image_predictions'], row['text_predictions']])\n    return ' '.join( np.unique(x) )","7c0ec492":"# Create Model\n\n\nclass ArcMarginProduct(nn.Module):\n    def __init__(self, in_features, out_features, scale=30.0, margin=0.50, easy_margin=False, ls_eps=0.0):\n        super(ArcMarginProduct, self).__init__()\n        self.in_features = in_features\n        self.out_features = out_features\n        self.scale = scale\n        self.margin = margin\n        self.ls_eps = ls_eps\n        self.weight = nn.Parameter(torch.FloatTensor(out_features, in_features))\n        nn.init.xavier_uniform_(self.weight)\n\n        self.easy_margin = easy_margin\n        self.cos_m = math.cos(margin)\n        self.sin_m = math.sin(margin)\n        self.th = math.cos(math.pi - margin)\n        self.mm = math.sin(math.pi - margin) * margin\n\n    def forward(self, input, label):\n        cosine = F.linear(F.normalize(input), F.normalize(self.weight))\n        sine = torch.sqrt(1.0 - torch.pow(cosine, 2))\n        phi = cosine * self.cos_m - sine * self.sin_m\n        if self.easy_margin:\n            phi = torch.where(cosine > 0, phi, cosine)\n        else:\n            phi = torch.where(cosine > self.th, phi, cosine - self.mm)\n    \n        one_hot = torch.zeros(cosine.size(), device='cuda')\n        one_hot.scatter_(1, label.view(-1, 1).long(), 1)\n        if self.ls_eps > 0:\n            one_hot = (1 - self.ls_eps) * one_hot + self.ls_eps \/ self.out_features\n\n        output = (one_hot * phi) + ((1.0 - one_hot) * cosine)\n        output *= self.scale\n\n        return output, nn.CrossEntropyLoss()(output,label)\n\n\nclass ShopeeModel(nn.Module):\n\n    def __init__(\n        self,\n        n_classes = CFG.classes,\n        model_name = CFG.model_name,\n        fc_dim = CFG.fc_dim,\n        margin = CFG.margin,\n        scale = CFG.scale,\n        use_fc = True,\n        pretrained = True):\n\n        super(ShopeeModel,self).__init__()\n        print('Building Model Backbone for {} model'.format(model_name))\n\n        self.backbone = timm.create_model(model_name, pretrained=pretrained)\n        in_features = self.backbone.classifier.in_features\n        self.backbone.classifier = nn.Identity()\n        self.backbone.global_pool = nn.Identity()\n        self.pooling =  nn.AdaptiveAvgPool2d(1)\n        self.use_fc = use_fc\n\n        if use_fc:\n            self.dropout = nn.Dropout(p=0.1)\n            self.classifier = nn.Linear(in_features, fc_dim)\n            self.bn = nn.BatchNorm1d(fc_dim)\n            self._init_params()\n            in_features = fc_dim\n\n        self.final = ArcMarginProduct(\n            in_features,\n            n_classes,\n            scale = scale,\n            margin = margin,\n            easy_margin = False,\n            ls_eps = 0.0\n        )\n\n    def _init_params(self):\n        nn.init.xavier_normal_(self.classifier.weight)\n        nn.init.constant_(self.classifier.bias, 0)\n        nn.init.constant_(self.bn.weight, 1)\n        nn.init.constant_(self.bn.bias, 0)\n\n    def forward(self, image, label):\n        features = self.extract_features(image)\n        if self.training:\n            logits = self.final(features, label)\n            return logits\n        else:\n            return features\n\n    def extract_features(self, x):\n        batch_size = x.shape[0]\n        x = self.backbone(x)\n        x = self.pooling(x).view(batch_size, -1)\n\n        if self.use_fc and self.training:\n            x = self.dropout(x)\n            x = self.classifier(x)\n            x = self.bn(x)\n        return x\n","23c72807":"def get_image_neighbors(df, embeddings, KNN=50):\n\n    model = NearestNeighbors(n_neighbors = KNN)\n    model.fit(embeddings)\n    distances, indices = model.kneighbors(embeddings)\n    \n    threshold = 4.5\n    predictions = []\n    for k in tqdm(range(embeddings.shape[0])):\n        idx = np.where(distances[k,] < threshold)[0]\n        ids = indices[k,idx]\n        posting_ids = df['posting_id'].iloc[ids].values\n        predictions.append(posting_ids)\n        \n    del model, distances, indices\n    gc.collect()\n    return df, predictions","7a5ec7da":"def get_test_transforms():\n    return albumentations.Compose([\n        albumentations.Resize(CFG.img_size, CFG.img_size, always_apply=True),\n        albumentations.Normalize(),\n        ToTensorV2(p=1.0)\n    ])","c7c5d9f9":"class ShopeeDataset(Dataset):\n\n    def __init__(self, image_paths, transforms=None):\n        self.image_paths = image_paths\n        self.augmentations = transforms\n\n    def __len__(self):\n        return self.image_paths.shape[0]\n\n    def __getitem__(self, index):\n        image_path = self.image_paths[index]\n        image = cv2.imread(image_path)\n        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n        if self.augmentations:\n            augmented = self.augmentations(image=image)\n            image = augmented['image']\n        \n        return image, torch.tensor(1)","5b16025a":"def get_image_embeddings(image_paths):\n\n    model = ShopeeModel(pretrained=False).to(CFG.device)\n    model.load_state_dict(torch.load(CFG.model_path))\n    model.eval()\n\n    image_dataset = ShopeeDataset(image_paths=image_paths, transforms=get_test_transforms())\n    image_loader = torch.utils.data.DataLoader(\n        image_dataset,\n        batch_size=CFG.batch_size,\n        num_workers=CFG.num_workers\n    )\n\n    embeds = []\n    with torch.no_grad():\n        for img,label in tqdm(image_loader): \n            img = img.cuda()\n            label = label.cuda()\n            features = model(img,label)\n            image_embeddings = features.detach().cpu().numpy()\n            embeds.append(image_embeddings)\n\n    del model\n    image_embeddings = np.concatenate(embeds)\n    print(f'Our image embeddings shape is {image_embeddings.shape}')\n    del embeds\n    gc.collect()\n    return image_embeddings","dea8d4b3":"def get_text_predictions(df, max_features=25_000):\n    \n    model = TfidfVectorizer(stop_words='english',\n                            binary=True,\n                            max_features=max_features)\n    text_embeddings = model.fit_transform(df_cu['title']).toarray()\n\n    print('Finding similar titles...')\n    CHUNK = 1024 * 4\n    CTS = len(df) \/\/ CHUNK\n    if (len(df)%CHUNK) != 0:\n        CTS += 1\n\n    preds = []\n    for j in range( CTS ):\n        a = j * CHUNK\n        b = (j+1) * CHUNK\n        b = min(b, len(df))\n        print('chunk', a, 'to', b)\n\n        # COSINE SIMILARITY DISTANCE\n        cts = cupy.matmul(text_embeddings, text_embeddings[a:b].T).T\n        for k in range(b-a):\n            IDX = cupy.where(cts[k,]>0.75)[0]\n            o = df.iloc[cupy.asnumpy(IDX)].posting_id.values\n            preds.append(o)\n\n    del model,text_embeddings\n    gc.collect()\n    return preds","bf73875a":"df,df_cu,image_paths = read_dataset()\ndf.head()","a5a7a888":"# Get neighbors for image_embeddings\n\nimage_embeddings = get_image_embeddings(image_paths.values)\ntext_predictions = get_text_predictions(df, max_features=25_000)\ndf, image_predictions = get_image_neighbors(df, image_embeddings, KNN=50 if len(df)>3 else 3)\ndf.head()","aceb0707":"df['image_predictions'] = image_predictions\ndf['text_predictions'] = text_predictions\ndf['matches'] = df.apply(combine_predictions, axis=1)\ndf[['posting_id', 'matches']].to_csv('submission.csv', index=False)","59639054":"# Config","14bbd2c0":"# Import Packages","73903194":"# Text Predictions","4db5c1a4":"# Calculating Predictions","1d6500f4":"# Image Predictions","d4cdd579":"# Utils","650083f8":"# About Notebook\n\n* This notebook works with any EfficientNet(B0 - B7) Model.\n* Training Notebook for EfficientNet-B4 can be found [here](https:\/\/www.kaggle.com\/vatsalmavani\/shopee-training-eff-b4)\n\n#### **NOTE:**\n* By changing `threshold` value in `get_image_neighbors` function, one may get higher F1 score.\n* In new version, `text_threshold = 0.75` which increase lb from 0.727 --> 0.728\n","626c99c4":"# Preparing Submission"}}