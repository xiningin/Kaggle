{"cell_type":{"0c488258":"code","2b74acfc":"code","23a39ead":"code","9c4aafbf":"code","80f0538a":"code","9b99cae3":"markdown","a6ee1e10":"markdown","307c3b3f":"markdown","f7db8e07":"markdown","27653d11":"markdown","42a754e1":"markdown","7ef20be5":"markdown","086a4053":"markdown","afc900c3":"markdown","6e8e691c":"markdown"},"source":{"0c488258":"import numpy as np\nimport cv2\n\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Dropout, Flatten\nfrom keras.layers import Conv2D\nfrom keras.optimizers import Adam\nfrom keras.layers import MaxPooling2D\nfrom keras.preprocessing.image import ImageDataGenerator","2b74acfc":"# Path to the images\ntrain_dir = '..\/input\/fer2013\/train'\nval_dir = '..\/input\/fer2013\/test'\n\n# Generates new images from existing ones \ntrain_datagen = ImageDataGenerator(rescale=1.\/255)\nval_datagen = ImageDataGenerator(rescale=1.\/255)\n\n# New train images\ntrain_generator = train_datagen.flow_from_directory(\n                                train_dir,\n                                target_size=(48,48),\n                                batch_size=64,\n                                color_mode=\"grayscale\",\n                                class_mode='categorical')\n# New validation images\nvalidation_generator = val_datagen.flow_from_directory(\n                                val_dir,\n                                target_size=(48,48),\n                                batch_size=64,\n                                color_mode=\"grayscale\",\n                                class_mode='categorical')","23a39ead":"emotion_model = Sequential()\n\nemotion_model.add(Conv2D(32, kernel_size=(3, 3), activation='relu', input_shape = (48, 48, 1)))\nemotion_model.add(Conv2D(64, kernel_size=(3, 3), activation='relu'))\nemotion_model.add(MaxPooling2D(2, 2))\nemotion_model.add(Dropout(0.25))\n\nemotion_model.add(Conv2D(128, kernel_size=(3, 3), activation='relu'))\nemotion_model.add(MaxPooling2D(pool_size=(2, 2)))\nemotion_model.add(Conv2D(128, kernel_size=(3, 3), activation='relu'))\nemotion_model.add(MaxPooling2D(pool_size=(2, 2)))\nemotion_model.add(Dropout(0.25))\n\nemotion_model.add(Flatten())\nemotion_model.add(Dense(1024, activation='relu'))\nemotion_model.add(Dropout(0.5))\nemotion_model.add(Dense(7, activation='softmax'))","9c4aafbf":"# Compile model\nemotion_model.compile(optimizer=Adam(lr=0.0001, decay=1e-6), loss='categorical_crossentropy', metrics=['accuracy'])\n\n# Train the model with the generated images\nemotion_model_info = emotion_model.fit_generator(\n                                    train_generator,\n                                    steps_per_epoch=28709 \/\/ 64,\n                                    epochs=50,\n                                    validation_data=validation_generator,\n                                    validation_steps=7178 \/\/ 64\n                                    )","80f0538a":"emotion_model.save_weights('emotion_model.h5')","9b99cae3":"### The dataset","a6ee1e10":"# Save model","307c3b3f":"# Modeling","f7db8e07":"# Libraries","27653d11":"# About the project","42a754e1":"# Train model","7ef20be5":"In this project, we will classify human facial expressions to filter and map corresponding emojis.","086a4053":"# Load data","afc900c3":"The FER2013 dataset (Facial Expression Recognition) consist of 48x48 pixel grayscale face images. These are the categories:\n- **0**: angry\n- **1**: disgust\n- **2**: feat\n- **3**: happy\n- **4**: sad\n- **5**: surprise\n- **6**: natural","6e8e691c":"<p style=\"color:red\">ATTENTION: <\/p> This project is still in progress..."}}