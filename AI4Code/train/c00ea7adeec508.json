{"cell_type":{"70f1972e":"code","4592a51f":"code","52a8da49":"code","a1d488c8":"code","e03830ba":"code","31456aaa":"code","73977c49":"code","ceef54b4":"code","adcfc188":"code","579a4511":"code","256116c7":"code","f3aece36":"code","05e88eba":"code","cdb4b759":"code","73c32bbb":"code","eec3c6c7":"code","9f55bd75":"code","410b7e35":"code","f93df33f":"code","88a0cf5e":"code","51356a68":"code","cf1faa51":"code","3b47cce3":"code","1313f70a":"code","f664b3e5":"code","66df0f45":"code","1372e91d":"code","55a1cb0e":"code","fa591587":"code","e436761e":"code","4fa882b7":"code","5834f677":"markdown","d3e65148":"markdown","d59af549":"markdown","c70f8fcb":"markdown","d378df1c":"markdown","2d2f1a2c":"markdown","d8033288":"markdown","fd38f984":"markdown","44e33770":"markdown","dd14bec9":"markdown","0d1e86a3":"markdown","ed51abe0":"markdown","6f7344ee":"markdown","c4c61c34":"markdown","2653faa2":"markdown","c90f48e2":"markdown","c62e001b":"markdown","30822cd3":"markdown","61edbeb0":"markdown","f3a4c032":"markdown"},"source":{"70f1972e":"import spacy\nimport random\nfrom collections import Counter #for counting\nimport seaborn as sns #for visualization\nimport matplotlib.pyplot as plt\nimport pandas as pd\nplt.style.use('seaborn')\nsns.set(font_scale=2)\nimport json\ndef pretty_print(pp_object):\n    print(json.dumps(pp_object, indent=2))\n    \nfrom IPython.display import Markdown, display\ndef printmd(string, color=None):\n    colorstr = \"<span style='color:{}'>{}<\/span>\".format(color, string)\n    display(Markdown(colorstr))","4592a51f":"!pip install --upgrade pip\n!pip install textacy","52a8da49":"!python -m spacy download en_core_web_lg\nnlp = spacy.load('en_core_web_lg')\n# python -m spacy download en_vectors_web_lg","a1d488c8":"tweets = pd.read_csv(\"..\/input\/all-djtrum-tweets\/all_djt_tweets.csv\")","e03830ba":"def explain_text_entities(text):\n    doc = nlp(text)\n    for ent in doc.ents:\n        print(f'Entity: {ent}, Label: {ent.label_}, {spacy.explain(ent.label_)}')","31456aaa":"explain_text_entities(tweets['text'][9])","73977c49":"one_sentence = tweets['text'][0]\ndoc = nlp(one_sentence)\nspacy.displacy.render(doc, style='ent',jupyter=True)","ceef54b4":"one_sentence = tweets['text'][240]\ndoc = nlp(one_sentence)\nspacy.displacy.render(doc, style='ent',jupyter=True)","adcfc188":"one_sentence = tweets['text'][300]\ndoc = nlp(one_sentence)\nspacy.displacy.render(doc, style='ent',jupyter=True)","579a4511":"one_sentence = tweets['text'][450]\ndoc = nlp(one_sentence)\nspacy.displacy.render(doc, style='ent',jupyter=True)","256116c7":"def redact_names(text):\n    doc = nlp(text)\n    redacted_sentence = []\n    for ent in doc.ents:\n        ent.merge()\n    for token in doc:\n        if token.ent_type_ == \"PERSON\":\n            redacted_sentence.append(\"[REDACTED]\")\n        else:\n            redacted_sentence.append(token.string)\n    return \"\".join(redacted_sentence)","f3aece36":"printmd(\"**Before**\", color=\"blue\")\none_sentence = tweets['text'][450]\ndoc = nlp(one_sentence)\nspacy.displacy.render(doc, style='ent',jupyter=True)\nprintmd(\"**After**\", color=\"blue\")\none_sentence = redact_names(tweets['text'][450])\ndoc = nlp(one_sentence)\nspacy.displacy.render(doc, style='ent',jupyter=True)\n\nprintmd(\"Notice that `Obama W.H.` was removed\", color=\"#6290c8\")","05e88eba":"example_text = tweets['text'][9]\ndoc = nlp(example_text)\nspacy.displacy.render(doc, style='ent', jupyter=True)\n\nfor idx, sentence in enumerate(doc.sents):\n    for noun in sentence.noun_chunks:\n        print(f\"sentence {idx+1} has noun chunk '{noun}'\")","cdb4b759":"one_sentence = tweets['text'][300]\ndoc = nlp(one_sentence)\nspacy.displacy.render(doc, style='ent', jupyter=True)\n\nfor token in doc:\n    print(token, token.pos_)","73c32bbb":"text = tweets['text'].str.cat(sep=' ')\n# spaCy enforces a max limit of 1000000 characters for NER and similar use cases.\n# Since `text` might be longer than that, we will slice it off here\nmax_length = 1000000-1\ntext = text[:max_length]\n\n# removing URLs and '&amp' substrings using regex\nimport re\nurl_reg  = r'[a-z]*[:.]+\\S+'\ntext   = re.sub(url_reg, '', text)\nnoise_reg = r'\\&amp'\ntext   = re.sub(noise_reg, '', text)","eec3c6c7":"doc = nlp(text)","9f55bd75":"items_of_interest = list(doc.noun_chunks)\n# each element in this list is spaCy's inbuilt `Span`, which is not useful for us\nitems_of_interest = [str(x) for x in items_of_interest]\n# so we've converted it to string","410b7e35":"df_nouns = pd.DataFrame(items_of_interest, columns=[\"TrumpSays\"])\nplt.figure(figsize=(5,4))\nsns.countplot(y=\"TrumpSays\",\n             data=df_nouns,\n             order=df_nouns[\"TrumpSays\"].value_counts().iloc[:10].index)\nplt.show()","f93df33f":"trump_topics = []\nfor token in doc:\n    if (not token.is_stop) and (token.pos_ == \"NOUN\") and (len(str(token))>2):\n        trump_topics.append(token)\n        \ntrump_topics = [str(x) for x in trump_topics]","88a0cf5e":"df_nouns = pd.DataFrame(trump_topics, columns=[\"Trump Topics\"])\ndf_nouns\nplt.figure(figsize=(5,4))\nsns.countplot(y=\"Trump Topics\",\n             data=df_nouns,\n             order=df_nouns[\"Trump Topics\"].value_counts().iloc[:10].index)\nplt.show()","51356a68":"trump_topics = []\nfor ent in doc.ents:\n    if ent.label_ not in [\"PERCENT\", \"CARDINAL\", \"DATE\"]:\n#         print(ent.text,ent.label_)\n        trump_topics.append(ent.text.strip())","cf1faa51":"df_ttopics = pd.DataFrame(trump_topics, columns=[\"Trump Nouns\"])\nplt.figure(figsize=(5,4))\nsns.countplot(y=\"Trump Nouns\",\n             data=df_ttopics,\n             order=df_ttopics[\"Trump Nouns\"].value_counts().iloc[1:11].index)\nplt.show()\n# from collections import Counter\n# item_counter = Counter(items_of_interest)\n# item_counter.most_common()","3b47cce3":"from spacy.lang.en.stop_words import STOP_WORDS\nfrom wordcloud import WordCloud\nplt.figure(figsize=(10,5))\nwordcloud = WordCloud(background_color=\"white\",\n                      stopwords = STOP_WORDS,\n                      max_words=45,\n                      max_font_size=30,\n                      random_state=42\n                     ).generate(str(trump_topics))\nplt.imshow(wordcloud)\nplt.axis(\"off\")\nplt.show()","1313f70a":"from spacy.matcher import Matcher\n# doc = nlp(text)\nmatcher = Matcher(nlp.vocab)\nmatched_sents = [] # collect data of matched sentences to be visualized\n\ndef collect_sents(matcher, doc, i, matches, label='MATCH'):\n    \"\"\"\n    Function to help reformat data for displacy visualization\n    \"\"\"\n    match_id, start, end = matches[i]\n    span = doc[start : end]  # matched span\n    sent = span.sent  # sentence containing matched span\n    \n    # append mock entity for match in displaCy style to matched_sents\n    \n    if doc.vocab.strings[match_id] == 'DEMOCRATS':  # don't forget to get string!\n        match_ents = [{'start': span.start_char - sent.start_char,\n                   'end': span.end_char - sent.start_char,\n                   'label': 'DEMOCRATS'}]\n        matched_sents.append({'text': sent.text, 'ents': match_ents })\n    elif doc.vocab.strings[match_id] == 'RUSSIA':  # don't forget to get string!\n        match_ents = [{'start': span.start_char - sent.start_char,\n               'end': span.end_char - sent.start_char,\n               'label': 'RUSSIA'}]\n        matched_sents.append({'text': sent.text, 'ents': match_ents })\n    elif doc.vocab.strings[match_id] == 'I':  # don't forget to get string!\n        match_ents = [{'start': span.start_char - sent.start_char,\n               'end': span.end_char - sent.start_char,\n               'label': 'NARC'}]\n        matched_sents.append({'text': sent.text, 'ents': match_ents })\n    \n# declare different patterns\nrussia_pattern = [{'LOWER': 'russia'}, {'LEMMA': 'be'}, {'POS': 'ADV', 'OP': '*'},\n           {'POS': 'ADJ'}]\ndemocrats_pattern = [{'LOWER': 'democrats'}, {'LEMMA': 'be'}, {'POS': 'ADV', 'OP': '*'},\n           {'POS': 'ADJ'}]\ni_pattern = [{'LOWER': 'i'}, {'LEMMA': 'be'}, {'POS': 'ADV', 'OP': '*'},\n           {'POS': 'ADJ'}]\n\nmatcher.add('DEMOCRATS', collect_sents, democrats_pattern)  # add pattern\nmatcher.add('RUSSIA', collect_sents, russia_pattern)  # add pattern\nmatcher.add('I', collect_sents, i_pattern)  # add pattern\nmatches = matcher(doc)\n\nspacy.displacy.render(matched_sents, style='ent', manual=True, jupyter=True,  options = {'colors': {'NARC': '#6290c8', 'RUSSIA': '#cc2936', 'DEMOCRATS':'#f2cd5d'}})","f664b3e5":"pretty_print(matched_sents[:3])","66df0f45":"example_text = tweets['text'][180]\ndoc = nlp(example_text)","1372e91d":"options = {'compact': True, 'bg': '#09a3d5',\n           'color': 'white', 'font': 'Trebuchet MS'}\nspacy.displacy.render(doc, jupyter=True, style='dep', options=options)","55a1cb0e":"for token in doc:\n    print(token.text, token.dep_, token.head.text, token.head.pos_,\n          [child for child in token.children])","fa591587":"for token in doc:\n    print(f\"token: {token.text},\\t dep: {token.dep_},\\t head: {token.head.text},\\t pos: {token.head.pos_},\\\n    ,\\t children: {[child for child in token.children]}\")","e436761e":"from textacy.spacier import utils as spacy_utils\n\ndef para_to_ques(eg_text):\n    \"\"\"\n    Generates a few simple questions by slot filling pieces from sentences\n    \"\"\"\n    doc = nlp(eg_text)\n    results = []\n    for sentence in doc.sents:\n        root = sentence.root\n        ask_about = spacy_utils.get_subjects_of_verb(root)\n        answers = spacy_utils.get_objects_of_verb(root)\n        if len(ask_about) > 0 and len(answers) > 0:\n            if root.lemma_ == \"be\":\n                question = f'What {root} {ask_about[0]}?'\n            else:\n                question = f'What does {ask_about[0]} {root.lemma_}?'\n            results.append({'question':question, 'answers':answers})\n    return results","4fa882b7":"example_text = tweets['text'][180]\ndoc = nlp(example_text)\nspacy.displacy.render(doc, style='ent', jupyter=True)\nprint(para_to_ques(example_text))","5834f677":"To generate our questions, let's actually use these two ideas:\n- Subject of Verb\n- Object of Verb\n","d3e65148":"# What's did we really see here?\n\n## Don't Panic! Nothing is really beyond spaCy\n\n![](https:\/\/i.kym-cdn.com\/photos\/images\/newsfeed\/001\/022\/354\/081.jpeg)\n\n1. [Named Entity Recognition](#Named-Entity-Recognition-aka-NER), the different entities and it's visualization with `displacy`\n2. [Part of Speech Tagging](#Part-of-Speech-Tagging), and exploring what Trump says with *word clouds*!\n3. [Using Linguistic annotations with spaCy Match](#Using-Linguistic-annotations-with-spaCy-Match)\n4. Dependency Parsing, for [**Automatic Question and Answer Generation**](#Automatic-Question-and-Answer-Generation)\n\n# Bookmark this with [http:\/\/bit.ly\/spacykernel](http:\/\/bit.ly\/spacykernel)","d59af549":"Let's continue exploring NER for some more examples, with different entities: ","c70f8fcb":"Natural Language Processing is the use of machines to manipulate natural language. Here, we focus on written language, or in simpler words: text.\n\n# Don't Panic! Hitchhiker's Guide to NLP with spaCy\n\n## _Any semblance of order or coherence is purely accidental_\n\nHumans are the only known species to have developed written languages. Yet, children don't learn to read and write on their own. This is to highlight the complexity of text processing and NLP.\n\nThe study of natural language processing has been around for more than 50 years. The famous Turing test for General Artificial Intelligence is based on machine comprehension. The field has grown with linguistics and the computational techniques both.\n\nSome Applications of NLP\n\n*  Sentiment Analysis on Social Media\n*  Automated Customer Service\n*  Chatbots, such as that of Uber, Intercom\n","d378df1c":"Adding some explainer text in the output itself:","2d2f1a2c":"#### Let's preview 2-3 elements used in the displaCy visualization above. This is what the list of dictionaries looks like: ","d8033288":"## Using Linguistic annotations with spaCy Match\n\n> Based on the [Rule Matching docs at spaCy](https:\/\/spacy.io\/usage\/linguistic-features#section-rule-based-matching)\n\nWe want to find out what Trump is saying about \n\n1. Himself e.g. \"I am rich\". \n2. Russia\n3. Democrats\n\nWe want to start off by finding _adjectives following_ \"Democrats are\" or \"Democrats were\". \n\nThis is obviously a very rudimentary solution, but it'll be fast, and a great way get an idea for what's in your data. Our pattern looks like this:\n\n```bash\n[{'LOWER': 'Russia'}, {'LEMMA': 'be'}, {'POS': 'ADV', 'OP': '*'}, {'POS': 'ADJ'}]\n```\n\nThis translates to a token whose lowercase form matches \"democrats\" (like Democrats, democrats or DEMoCrats), followed by a token with the lemma \"be\" (for example, is, was, or 's), followed by an optional adverb, followed by an adjective. \n\nThe optional adverb makes sure you won't miss adjectives with intensifiers, like \"pretty awful\" or \"very nice\".\n\nThis kind of adjective mining can then be used as features to do _aspect-based sentiment analysis_, which is finding sentiment with respect to specific entities or words. ","fd38f984":"# Automatic Question and Answer Generation\n\n> ### When asked to produce The Ultimate Question, Deep Thought says that it cannot...\n>\n> -- Douglas Adams\n\n### The Challenge\n\nCan you automatically convert a sentence to a question?\n\nFor instance, \n```bash\nMartin Luther King Jr. was a civil rights activist and skilled orator\n``` \n\nto \n\n```js\nWho was Martin Luther King Jr.?\n```\n\nNotice that when we convert a sentence to a question, the answer might not be in the original sentence anymore. To me, the answer to that question might be something different and that's fine. We are not aiming for _correct_ answers here.\n\n## Question Generation using Dependency Parsing\n\n\nDependency parsing analyzes the grammatical structure of a sentence. It establishes a \"tree\" like structure between a \"root\" word and those that are related to it by branches of some manner. ","44e33770":"You might notice that Part-of-Speech tagging is different from our NER results. In this particular example, `Stock Market` is not an entity, but definitely a noun. \n\nWhat are the \"Parts of Speech that we can pull out of such sentences? ","dd14bec9":"This is still not very insightul! Let's investigate the entities from Trump tweets instead?\n\n## Exploring Entities","0d1e86a3":"## Redacting Names\n\nOne simple use case for NER is to automatically redact names. This is important and quite useful. \n\nFor example, \n\n- to ensure that your company data actually complies with GDPR \n- when journalists wants to publish a large set of documents while still hiding the identity of their sources\n\nWe do this redaction by following broad steps:\n\n```markdown\n1. find all PERSON names\n2. replace these by a filler like [\"REDACTED\"]\n```","ed51abe0":"#### Wow! Trump is really obsessed with Democrats, himself and Hillary. ","6f7344ee":"## Part-of-Speech Tagging\n\nSometimes, we want to quickly pull out keywords, or keyphrases from a larger body of text. This helps us mentally paint a picture of what this text is about. This is particularly helpful in analysis of texts like long emails or essays.\n\nAs a quick hack, we can pull out all relevant \"nouns\". This is because most keywords are in fact nouns of some form.\n\n### Noun Chunks\nWe need noun chunks. Noun chunks are noun phrases - not a single word, but a short phrase which describes the noun. For example, \"the blue skies\" or \"the world\u2019s largest conglomerate\".\n\nTo get the noun chunks in a document, simply iterate over doc.noun_chunks:\n","c4c61c34":"Hmm, this is interesting in stating he uses \"I\" a lot more than \"we\" and \"We\", put together, but not much beyond that. \nWhat topics does he talk about these filler words? \n\n**Let's remove these filler words and try again!**","2653faa2":"\n## About spaCy\n\nspaCy is a free open-source library for Natural Language Processing in Python. \n\nIt features NER, POS tagging, dependency parsing, word vectors and more. The name spaCy comes from spaces + Cython. This is because spaCy started off as an industrial grade solution for tokenization - and eventually expanding to other challenges. Cython allows spaCy to be incredibly fast as compared to other solutions like NLTK. \n\n![](http:\/\/)It has trainable, or in other words customizable and extendable models for most of these tasks - while providing some really good models out of the box. ","c90f48e2":"# What does Trump talk about? \n\nIt might be interesting to explore what does Trump even talk about? Is it always them 'Angry Dems'? Or is he a narcissist with too many mentions of The President and the USA? \n\nOne way to explore this would be to mine out all the entities and noun chunks from all his tweets! Let's go ahead and do that with amazing ease using spaCy","c62e001b":"## Data\nWe explore some tweets from President Donald Trump\n\n![](https:\/\/screenshotscdn.firefoxusercontent.com\/images\/69402d7a-bfec-4cd4-9c57-cca42b6e7b86.png)\n\n## What's In Here?\nIn this kernel, we will learn how to use spaCy in Python to generate questions and answers from *any free text*. We will learn about named entitiy recognition, dependency parsing, part of speech tagging, and more!\n\n1. [Named Entity Recognition](#Named-Entity-Recognition-aka-NER),  visualization with `displacy` and **redacting names automatically without a dictionary**!\n2. [Part of Speech Tagging](#Part-of-Speech-Tagging), and exploring what Trump says with *word clouds*!\n3. [Using Linguistic annotations with spaCy Match](#Using-Linguistic-annotations-with-spaCy-Match)\n4. Dependency Parsing, for [**Automatic Question and Answer Generation**](#Automatic-Question-and-Answer-Generation)","30822cd3":"We can understand these relationship as a parent-child format as well, looking at one word at a time","61edbeb0":"For simply hinting at the power of `textacy` and `spaCy`, I have written only two simple rules to create questions - by adding more, with more nuanced examples, we can generate a large number of specific questions _and_ answers. \n\nYou can find a throrough and **Complete guide to question formation in English on [StackExchange here](https:\/\/ell.stackexchange.com\/a\/1198)**","f3a4c032":"# Named Entity Recognition aka NER\n\n> spaCy can recognise various types of named entities in a document, by asking the model for a prediction. Because models are statistical and strongly depend on the examples they were trained on, this doesn't always work perfectly and might need some tuning later, depending on your use case.\n> \n>  -- from the amazing [spaCy docs](https:\/\/spacy.io\/usage\/linguistic-features#section-named-entities)\n\n## Entities Explained\n\n| Type | \tDescription|\n|:---|:---\n| PERSON |\tPeople, including fictional. |\n| NORP | Nationalities or religious or political groups.| \n| FAC|  \tBuildings, airports, highways, bridges, etc.| \n| ORG|  \tCompanies, agencies, institutions, etc.| \n| GPE|  \tCountries, cities, states.| \n| LOC|  \tNon-GPE locations, mountain ranges, bodies of water.| \n| PRODUCT|  \tObjects, vehicles, foods, etc. (Not services.)| \n| EVENT|  \tNamed hurricanes, battles, wars, sports events, etc.| \n| WORK_OF_ART|  \tTitles of books, songs, etc.| \n| LAW|  \tNamed documents made into laws.| |\n| LANGUAGE|  \tAny named language.| \n| DATE|  \tAbsolute or relative dates or periods.| \n| TIME|  \tTimes smaller than a day.| \n| PERCENT|  \tPercentage, including \"%\".| \n| MONEY|  \tMonetary values, including unit.| \n| QUANTITY|  \tMeasurements, as of weight or distance.| \n| ORDINAL|  \t\"first\", \"second\", etc.| \n| CARDINAL|  \tNumerals that do not fall under another type.| \n\nLet's look at some examples of above in real world sentences. We will also use the `spacy.explain()` on all entities for one example - to build a quick mental model of how these things work."}}