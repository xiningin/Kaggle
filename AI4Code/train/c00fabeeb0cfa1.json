{"cell_type":{"f9b7184e":"code","3f6fcacf":"code","115764ba":"code","5e438e83":"code","bd260e22":"code","c48ef2e9":"code","c90d0ca7":"code","bb5966d7":"code","481e47a1":"code","96547d38":"code","d6924918":"code","e0a639da":"code","32921cf2":"code","bf4393a1":"code","85e6bb02":"code","ddd648d5":"code","82fb7b28":"code","878a831c":"code","202719e9":"code","817e93e5":"code","946e76a8":"code","3c234a9a":"code","d312412a":"code","7e8c2678":"code","69b7a64d":"code","5a98d924":"code","56453608":"code","e3305941":"code","b10f0ef5":"code","ccfcbd82":"code","550fc9dc":"code","f8f6052f":"code","e3c5ac03":"code","1dc279dc":"code","332c2b68":"code","e2f6feca":"code","3ee357dc":"code","dc148f58":"code","4706013a":"code","9856754b":"code","47299876":"code","f551c7f4":"code","3e6b0000":"code","341147cf":"code","6de1f9bf":"code","7888f03c":"code","efe0a67b":"code","fe4bf0df":"code","be728bc9":"code","c400b15b":"code","7e51c2ab":"code","3a966445":"code","c9afd587":"code","d61745cb":"code","dd771a58":"code","50d57888":"code","25482615":"code","7815778b":"code","2068364c":"code","cf713aa4":"code","a39d74d4":"code","f4efabda":"code","e0d2ecb4":"code","ccfc5809":"code","68b35f60":"code","23c7c9d4":"code","4c15c4f3":"code","3c40bd1b":"code","8319189e":"code","265e7c34":"code","e3b805d8":"code","dfad9d5c":"code","9576edc0":"code","34a663e6":"code","bdc152c9":"code","a7bb818c":"code","3daba3cf":"code","3118e345":"code","5e24d200":"code","58e3ac8a":"code","d0eb1388":"code","8832648a":"code","0d0f234b":"code","6e79f146":"code","cf0e1c25":"code","94da8f68":"code","ee193ec3":"code","a2c57a0c":"code","b0d6caaf":"code","aca779d0":"code","0b1dc8ab":"code","57c88954":"code","e0229a32":"code","10b10af3":"code","fd848702":"code","97c3d247":"markdown","acc3d5ed":"markdown","6ac96d3d":"markdown","998d6e8b":"markdown","87a80e43":"markdown","4d5d1967":"markdown","b42cc3af":"markdown","7c94d319":"markdown","5c82b77e":"markdown","e59a1487":"markdown","1dbcacf6":"markdown","afe66760":"markdown","802dbf61":"markdown","b8019f9c":"markdown","a83743ae":"markdown","137dbc3f":"markdown","0a6e34bf":"markdown","d4eebec5":"markdown","922e96d9":"markdown","35702187":"markdown","f2b861a9":"markdown","ad446831":"markdown","e2bd3432":"markdown","80f51d57":"markdown","de76cc62":"markdown","2e9bfb19":"markdown"},"source":{"f9b7184e":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nplt.style.use('seaborn')\nsns.set(font_scale=2.5)\n\nimport missingno as msno\n\n#ignore warnings\nimport warnings\nwarnings.filterwarnings('ignore') \n\n%matplotlib inline\n\nfrom collections import Counter\n\nfrom sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier, ExtraTreesClassifier, VotingClassifier\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.model_selection import GridSearchCV, cross_val_score, StratifiedKFold, learning_curve","3f6fcacf":"df_train = pd.read_csv('..\/input\/train.csv')\ndf_test = pd.read_csv('..\/input\/test.csv')\n\n# separate dataset","115764ba":"# Outlier detection by IQR\ndef detect_outliers(df, n, features):\n    outlier_indices = []\n    for col in features:\n        Q1 = np.percentile(df[col], 25)\n        Q3 = np.percentile(df[col], 75)\n        IQR = Q3 - Q1\n        \n        outlier_step = 1.5 * IQR\n        \n        outlier_list_col = df[(df[col] < Q1 - outlier_step) | (df[col] > Q3 + outlier_step)].index\n        outlier_indices.extend(outlier_list_col)\n    outlier_indices = Counter(outlier_indices)\n    multiple_outliers = list(k for k, v in outlier_indices.items() if v > n)\n        \n    return multiple_outliers\n        \nOutliers_to_drop = detect_outliers(df_train, 2, [\"Age\", \"SibSp\", \"Parch\", \"Fare\"])","5e438e83":"# check the row where the outlier was found\ndf_train.loc[Outliers_to_drop]","bd260e22":"# remove outlier\ndf_train = df_train.drop(Outliers_to_drop, axis = 0).reset_index(drop=True)","c48ef2e9":"df_train.head(10)","c90d0ca7":"df_train.describe()\n# statistical figures representation of data of train data","bb5966d7":"df_test.describe()\n# statistical figures representation of data of test data","481e47a1":"df_train.columns\n# attribute of each column","96547d38":"for col in df_train.columns:\n    msg = 'column: {:>10}\\t Percent of NaN value: {:.2f}%'.format(col, 100 * (df_train[col].isnull().sum() \/ df_train[col].shape[0]))\n    print(msg)\n    \n    # The process of identifying the missing value of each column","d6924918":"for col in df_test.columns:\n    msg = 'column: {:>10}\\t Percent of NaN value: {:.2f}%'.format(col, 100 * (df_test[col].isnull().sum() \/ df_test[col].shape[0]))\n    print(msg)","e0a639da":"msno.matrix(df=df_train.iloc[:, :], figsize=(8,8), color=(0.1, 0.6, 0.8))\n\n# msno.matrix creates the same matrix as shown below \n# empty space NULL data ","32921cf2":"msno.bar(df=df_train.iloc[:, :], figsize=(8,8), color=(0.1, 0.6, 0.8))\n\n# It makes with the graph of the bar type.","bf4393a1":"f, ax = plt.subplots(1,2, figsize = (18,8)) # \ub3c4\ud654\uc9c0\ub97c \uc900\ube44(\ud589,\uc5f4,\uc0ac\uc774\uc988)\n\ndf_train['Survived'].value_counts().plot.pie(explode = [0, 0.1], autopct = '%1.1f%%', ax=ax[0], shadow = True)\n# It draws a series-type pieplot. \nax[0].set_title('Pie plot - Survived')\n# Set the title for the first plot\nax[0].set_ylabel('')\n# Set the ylabel for the first plot\nsns.countplot('Survived', data = df_train, ax=ax[1])\n#  Draw a count plot.\nax[1].set_title('Count plot - Survived')\n# Set the title for the count plot\nplt.show()\n\n# The ratio of survival 0 to 1 of the train set is shown graphically.","85e6bb02":"df_train[['Pclass', 'Survived']].groupby(['Pclass'], as_index = True).count()\n\n# Tie with groupby and count how many counts.","ddd648d5":"pd.crosstab(df_train['Pclass'], df_train['Survived'], margins = True).style.background_gradient(cmap='Pastel1')\n# margin show total","82fb7b28":"df_train[['Pclass', 'Survived']].groupby(['Pclass'], as_index = True).mean().sort_values(by='Survived', ascending = False).plot.bar()\n\n# It means Survival rate","878a831c":"y_position = 1.02\nf, ax = plt.subplots(1, 2, figsize= (18,8))\ndf_train[\"Pclass\"].value_counts().plot.bar(color = [\"#CD7F32\", \"#FFDF00\", \"#D3D3D3\"], ax = ax[0])\nax[0].set_title(\"Number of passengers By Pclass\")\nax[0].set_ylabel(\"Count\")\nsns.countplot(\"Pclass\", hue = \"Survived\", data = df_train, ax = ax[1])\nax[1].set_title(\"Pclass: Survived vs Dead\", y = y_position)\nplt.show()\n     \n# The number of passengers and the survival rate according to the Passenger Class can be known.3 class (I think it is economy) was the most on board, and FirstClass passengers had the highest survival rate.","202719e9":"print(\"the oldest passenger : {:.1f} years\".format(df_train[\"Age\"].max()))\nprint(\"the youngest passenger : {:.1f} years\".format(df_train[\"Age\"].min()))\nprint(\"average of passengers age : {:.1f} years\".format(df_train[\"Age\"].mean()))\n      ","817e93e5":"fix, ax = plt.subplots(1, 1, figsize = (9, 5))\nsns.kdeplot(df_train[df_train[\"Survived\"] == 1][\"Age\"], ax=ax)\nsns.kdeplot(df_train[df_train[\"Survived\"] == 0][\"Age\"], ax=ax)    \nplt.legend([\"Survived == 1\", \"Survived == 0\"])\nplt.show()\n\n# kdeplot is used to estimate the distribution of data.","946e76a8":"fix, ax = plt.subplots(1, 1, figsize = (9, 7))\nsns.kdeplot(df_train[df_train[\"Pclass\"] == 1][\"Age\"], ax=ax)\nsns.kdeplot(df_train[df_train[\"Pclass\"] == 2][\"Age\"], ax=ax)\nsns.kdeplot(df_train[df_train[\"Pclass\"] == 3][\"Age\"], ax=ax)\nplt.xlabel(\"Age\")\nplt.title(\"Age Distribution within classes\")\nplt.legend([\"1st Class\", \"2nd Class\", \"3rd Class\"])\nplt.show()                       \n\n# In this situation, if you use histogram, you can use kde because you can not overlap\n# The Age distribution according to the Class can be known.","3c234a9a":"fig, ax  = plt.subplots(1, 1, figsize = (9, 5))\nsns.kdeplot(df_train[(df_train[\"Survived\"] == 0) & (df_train[\"Pclass\"] == 1)][\"Age\"], ax=ax)\nsns.kdeplot(df_train[(df_train[\"Survived\"] == 1) & (df_train[\"Pclass\"] == 1)][\"Age\"], ax=ax)\nplt.legend([\"Survived == 0\", \"Survived == 1\"])\nplt.title(\"1st Class\")\nplt.show()\n\n# Age distribution of non-survival people with first class\n# Age distribution of survival people with first class","d312412a":"fig, ax  = plt.subplots(1, 1, figsize = (9, 5))\nsns.kdeplot(df_train[(df_train[\"Survived\"] == 0) & (df_train[\"Pclass\"] == 2)][\"Age\"], ax=ax)\nsns.kdeplot(df_train[(df_train[\"Survived\"] == 1) & (df_train[\"Pclass\"] == 2)][\"Age\"], ax=ax)\nplt.legend([\"Survived == 0\", \"Survived == 1\"])\nplt.title(\"2nd Class\")\nplt.show()\n\n# Age distribution of non-survival people with second class\n# Age distribution of survival people with second class","7e8c2678":"fig, ax  = plt.subplots(1, 1, figsize = (9, 5))\nsns.kdeplot(df_train[(df_train[\"Survived\"] == 0) & (df_train[\"Pclass\"] == 3)][\"Age\"], ax=ax)\nsns.kdeplot(df_train[(df_train[\"Survived\"] == 1) & (df_train[\"Pclass\"] == 3)][\"Age\"], ax=ax)\nplt.legend([\"Survived == 0\", \"Survived == 1\"])\nplt.title(\"3rd Class\")\nplt.show()\n\n# Age distribution of non-survival people with third class\n# Age distribution of survival people with third class","69b7a64d":"chage_age_range_survival_ratio = []\ni = 80\nfor i in range(1,81):\n    chage_age_range_survival_ratio.append(df_train[df_train[\"Age\"] < i][\"Survived\"].sum()\/len(df_train[df_train[\"Age\"] < i][\"Survived\"])) # i\ubcf4\ub2e4 \uc791\uc740 \ub098\uc774\uc758 \uc0ac\ub78c\ub4e4\uc774 \uc0dd\uc874\ub960\n\nplt.figure(figsize = (7, 7))\nplt.plot(chage_age_range_survival_ratio)\nplt.title(\"Survival rate change depending on range of Age\", y = 1.02)\nplt.ylabel(\"Survival rate\")\nplt.xlabel(\"Range of Age(0-x)\")\nplt.show()\n    \n# The younger the age, the higher the probability of survival, The older the age, the less the probability of survival.","5a98d924":"f, ax = plt.subplots(1, 2, figsize=(18, 8))\nsns.violinplot(\"Pclass\",\"Age\", hue = \"Survived\", data = df_train, scale = \"count\", split = True, ax=ax[0])\nax[0].set_title(\"Pclass and Age vs Survived\")\nax[0].set_yticks(range(0, 110, 10))\n\nsns.violinplot(\"Sex\", \"Age\", hue = \"Survived\", data = df_train, scale = \"count\", split = True, ax=ax[1])\nax[1].set_title(\"Sex and Age vs Survived\")\nax[1].set_yticks(range(0, 110, 10))\n\nplt.show()\n\n# Based on age, the survival rate according to Pclass and the survival rate according to gender can be seen at a glance.\n# As a result, the better the Pclass, the higher the survival rate and the higher the survival rate of women than men.","56453608":"f, ax = plt.subplots(1, 1, figsize=(7,7))\ndf_train[[\"Embarked\",\"Survived\"]].groupby([\"Embarked\"], as_index=True).mean().sort_values(by=\"Survived\",\n                                                                                         ascending = False).plot.bar(ax=ax)","e3305941":"f, ax = plt.subplots(2, 2, figsize=(20,15))\nsns.countplot(\"Embarked\", data = df_train, ax=ax[0,0])\nax[0,0].set_title(\"(1) No. of Passengers Boared\")\n\nsns.countplot(\"Embarked\", hue = \"Sex\", data = df_train, ax=ax[0,1])\nax[0,1].set_title(\"(2) Male-Female split for Embarked\")\n\nsns.countplot(\"Embarked\", hue = \"Survived\", data = df_train, ax=ax[1,0])\nax[1,0].set_title(\"(3) Embarked vs Survived\")\n\nsns.countplot(\"Embarked\", hue = \"Pclass\", data = df_train, ax=ax[1,1])\nax[1,1].set_title(\"(4) Embarked vs Pclass\")\n\nplt.subplots_adjust(wspace = 0.4, hspace = 0.5) \nplt.show()\n\n# As a result, the survival rate is high because the people on board C have a lot of first class and many women.","b10f0ef5":"df_train[\"FamilySize\"] = df_train[\"SibSp\"] + df_train[\"Parch\"]+1\ndf_test[\"FamilySize\"] = df_test[\"SibSp\"] + df_test[\"Parch\"]+1\n\n# Create a new feature, \"FamilySize\".","ccfcbd82":"df_train[\"FamilySize\"].head(5)","550fc9dc":"df_test[\"FamilySize\"].head()","f8f6052f":"print(\"Maximum size of Family: \", df_train[\"FamilySize\"].max())\nprint(\"Minimum size of Family: \", df_train[\"FamilySize\"].min())","e3c5ac03":"f, ax = plt.subplots(1, 3, figsize = (40, 10))\nsns.countplot(\"FamilySize\", data = df_train, ax = ax[0])\nax[0].set_title(\"(1) No. of Passenger Boarded\", y = 1.02)\n\nsns.countplot(\"FamilySize\", hue = \"Survived\", data = df_train, ax = ax[1])\nax[1].set_title(\"(2) Survived countplot depending of FamilySize\")\n\ndf_train[[\"FamilySize\", \"Survived\"]].groupby([\"FamilySize\"], as_index = True).mean().sort_values(by = \"Survived\",\n                                                                                                      ascending = False).plot.bar(ax = ax[2])\nax[2].set_title(\"(3) Survived rate depending on FamilySize\", y = 1.02)\n\nplt.subplots_adjust(wspace = 0.2, hspace = 0.5)\nplt.show()\n\n# The first plot is the number of passengers according to the number of family members (1 to 11), the second plot is the number of survivors according to the number of family members, and the third plot is the survival rate according to the number of family members.\n# The family with four families has the highest survival rate.","1dc279dc":"f, ax = plt.subplots(1, 1, figsize = (8,8))\ng = sns.distplot(df_train[\"Fare\"], color = \"b\", label=\"Skewness: {:2f}\".format(df_train[\"Fare\"].skew()), ax=ax)\ng = g.legend(loc = \"best\")\n\n# The skewness tells us how asymmetric the distribution is.","332c2b68":"# Log to get rid of the skewness.\n\ndf_train[\"Fare\"] = df_train[\"Fare\"].map(lambda i:np.log(i) if i>0 else 0)","e2f6feca":"df_train[\"Fare\"].head(5)","3ee357dc":"f, ax = plt.subplots(1, 1, figsize = (8,8))\ng = sns.distplot(df_train[\"Fare\"], color = \"b\", label=\"Skewness: {:2f}\".format(df_train[\"Fare\"].skew()), ax=ax)\ng = g.legend(loc = \"best\")\n\n# normal approximation","dc148f58":"# First, fill NULL data.\n\ndf_train[\"Age\"].isnull().sum()","4706013a":"# It groups by using the title (mr, ms, mrs) etc. entering into name.\n# It extracts by using the normal expression.\n\ndf_train[\"Initial\"] = df_train[\"Name\"].str.extract(\"([A-Za-z]+)\\.\") # Initial\ub85c \ud638\uce6d\uc744 \uc800\uc7a5\ud574\uc900\ub2e4.\ndf_test[\"Initial\"] = df_test[\"Name\"].str.extract(\"([A-Za-z]+)\\.\")","9856754b":"df_train.head()","47299876":"df_test.head()","f551c7f4":"pd.crosstab(df_train[\"Initial\"], df_train[\"Sex\"]).T.style.background_gradient(cmap = \"Pastel2\")\n\n# Identify the title by gender and use crossstab to create a frequency table.","3e6b0000":"# The several titles is substituted simply. \n\ndf_train[\"Initial\"].replace([\"Mlle\",\"Mme\", \"Ms\", \"Dr\",\"Major\",\"Lady\",\"Countess\", \"Jonkheer\", \"Col\", \"Rev\", \"Capt\", \"Sir\", \"Don\", \"Dona\"],\n                           [\"Miss\", \"Miss\",\"Miss\", \"Mr\", \"Mr\", \"Mrs\", \"Mrs\", \"Other\", \"Other\", \"Other\", \"Mr\", \"Mr\", \"Mr\", \"Mr\"], inplace = True)\n\ndf_test[\"Initial\"].replace([\"Mlle\",\"Mme\", \"Ms\", \"Dr\",\"Major\",\"Lady\",\"Countess\", \"Jonkheer\", \"Col\", \"Rev\", \"Capt\", \"Sir\", \"Don\", \"Dona\"],\n                           [\"Miss\", \"Miss\",\"Miss\", \"Mr\", \"Mr\", \"Mrs\", \"Mrs\", \"Other\", \"Other\", \"Other\", \"Mr\", \"Mr\", \"Mr\", \"Mr\"], inplace = True)","341147cf":"df_train.groupby(\"Initial\").mean()","6de1f9bf":"df_train.groupby(\"Initial\")[\"Survived\"].mean().plot.bar()\n\n# Women such as Miss and Mrs have a high survival rate, and Master has a low average age, but a high survival rate.\n# In addition, Mr has a low survival rate","7888f03c":"# train, test Two data sets are combined and statistics is confirmed.\n# (Use concat: A function that builds a dataset on a dataset)\ndf_all = pd.concat([df_train,df_test])\ndf_all.shape","efe0a67b":"df_all.groupby(\"Initial\").mean()\n\n# NULL values are filled by using the average of Age.","fe4bf0df":"df_train.loc[(df_train[\"Age\"].isnull()) & (df_train[\"Initial\"] == \"Mr\"), \"Age\"] = 33\ndf_train.loc[(df_train[\"Age\"].isnull()) & (df_train[\"Initial\"] == \"Master\"),\"Age\"] = 5\ndf_train.loc[(df_train[\"Age\"].isnull()) & (df_train[\"Initial\"] == \"Miss\"), \"Age\"] = 22\ndf_train.loc[(df_train[\"Age\"].isnull()) & (df_train[\"Initial\"] == \"Mrs\"), \"Age\"] = 37\ndf_train.loc[(df_train[\"Age\"].isnull()) & (df_train[\"Initial\"] == \"Other\"), \"Age\"] = 45\n\ndf_test.loc[(df_test[\"Age\"].isnull()) & (df_test[\"Initial\"] == \"Mr\"), \"Age\"] = 33\ndf_test.loc[(df_test[\"Age\"].isnull()) & (df_test[\"Initial\"] == \"Master\"),\"Age\"] = 5\ndf_test.loc[(df_test[\"Age\"].isnull()) & (df_test[\"Initial\"] == \"Miss\"), \"Age\"] = 22\ndf_test.loc[(df_test[\"Age\"].isnull()) & (df_test[\"Initial\"] == \"Mrs\"), \"Age\"] = 37\ndf_test.loc[(df_test[\"Age\"].isnull()) & (df_test[\"Initial\"] == \"Other\"), \"Age\"] = 45\n\n# The Age returns NULL, Initial is Mr, draws only the Age column and fills it all with 33 (average of the Age seen above).\n# Fill all the NULL data in the same way.","be728bc9":"df_train[\"Age\"].isnull().sum()","c400b15b":"df_test[\"Age\"].isnull().sum()","7e51c2ab":"df_train[\"Embarked\"].isnull().sum()","3a966445":"df_train.shape\n\n# Of the 891 rows, only two have no missing value, so replace it with the most frequent value.","c9afd587":"df_train[\"Embarked\"].fillna(\"S\", inplace = True)\n\n# fillna fills the missing value value with the designated value.\n# In the EDA process, S is the most common, so it replaces.","d61745cb":"df_test[\"Embarked\"].isnull().sum()","dd771a58":"df_train[\"Age_Categ\"] = 0\ndf_test[\"Age_Categ\"] = 0\n\n# create a new feature","50d57888":"def category_age(x):\n    if x < 10:\n        return 0\n    elif x < 20:\n        return 1\n    elif x < 30:\n        return 2\n    elif x < 40:\n        return 3 \n    elif x < 50:\n        return 4\n    elif x < 60: \n        return 5\n    elif x < 70: \n        return 6\n    else:\n        return 7\n    \n# Make a function for apply use.  ","25482615":"df_train[\"Age_Categ\"] = df_train[\"Age\"].apply(category_age)\ndf_test[\"Age_Categ\"] = df_test[\"Age\"].apply(category_age)\n\n# By using the apply function, the information of the age column categorized is added to train and test set.","7815778b":"df_train[\"Age_Categ\"].head(10)","2068364c":"df_test[\"Age_Categ\"].head(10)","cf713aa4":"df_train.head()","a39d74d4":"df_test.head()","f4efabda":"# Since categorizing Age, the unnecessary Age column is deleted.\n\ndf_train.drop([\"Age\"], axis = 1 ,inplace = True)\ndf_test.drop([\"Age\"], axis = 1, inplace = True)","e0d2ecb4":"df_train.head()","ccfc5809":"df_train[\"Initial\"].unique()","68b35f60":"df_train[\"Initial\"] = df_train[\"Initial\"].map({\"Master\" : 0, \"Miss\" : 1, \"Mr\" : 2, \"Mrs\" : 3, \"Other\" : 4})\ndf_test[\"Initial\"] = df_test[\"Initial\"].map({\"Master\" : 0, \"Miss\" : 1, \"Mr\" : 2, \"Mrs\" : 3, \"Other\" : 4})","23c7c9d4":"df_train[\"Embarked\"].value_counts()","4c15c4f3":"df_train[\"Embarked\"] = df_train[\"Embarked\"].map({\"C\" : 0, \"Q\" : 1, \"S\" : 2})\ndf_test[\"Embarked\"] = df_test[\"Embarked\"].map({\"C\" : 0, \"Q\" : 1, \"S\" : 2})","3c40bd1b":"df_train.head()","8319189e":"df_test.head()","265e7c34":"df_train[\"Sex\"].unique()","e3b805d8":"df_train[\"Sex\"] = df_train[\"Sex\"].map({\"female\" : 0, \"male\" : 1})\ndf_test[\"Sex\"] = df_test[\"Sex\"].map({\"female\" : 0, \"male\" : 1})","dfad9d5c":"heatmap_data = df_train[[\"Survived\", \"Pclass\", \"Sex\", \"Fare\", \"Embarked\", \"FamilySize\", \"Initial\", \"Age_Categ\"]]","9576edc0":"colormap = plt.cm.PuBu\nplt.figure(figsize=(10, 8))\nplt.title(\"Person Correlation of Features\", y = 1.05, size = 15)\nsns.heatmap(heatmap_data.astype(float).corr(), linewidths = 0.1, vmax = 1.0,\n           square = True, cmap = colormap, linecolor = \"white\", annot = True, annot_kws = {\"size\" : 16})\n\n\n# Correlation coefficient analysis shows whether there are overlapping features and which features show correlation.","34a663e6":"# o improve the performance of the model, it is a task to change the form to use the information of the data that was categorized.\n# One Hot Encoding is to make these vector (Dummy)\n\ndf_train = pd.get_dummies(df_train, columns = [\"Initial\"], prefix = \"Initial\")\ndf_test = pd.get_dummies(df_test, columns = [\"Initial\"], prefix = \"Initial\")","bdc152c9":"df_train = pd.get_dummies(df_train, columns = [\"Embarked\"], prefix = \"Embarked\")\ndf_test = pd.get_dummies(df_test, columns = [\"Embarked\"], prefix = \"Embarked\")","a7bb818c":"# Features which are not used are deleted.\ndf_train.head(1)","3daba3cf":"df_train.drop([\"PassengerId\", \"Name\", \"SibSp\", \"Parch\", \"Ticket\", \"Cabin\"], axis = 1, inplace = True)\ndf_test.drop([\"PassengerId\", \"Name\", \"SibSp\", \"Parch\", \"Ticket\", \"Cabin\"], axis = 1, inplace = True)","3118e345":"df_train.head() ","5e24d200":"df_test.head()","58e3ac8a":"kfold = StratifiedKFold(n_splits=10)","d0eb1388":"df_train[\"Survived\"] = df_train[\"Survived\"].astype(int)\n\nY_train = df_train[\"Survived\"]\n\nX_train = df_train.drop(labels = [\"Survived\"],axis = 1)","8832648a":"# modeling test with various algorithms\nrandom_state = 2\nclassifiers = []\nclassifiers.append(SVC(random_state = random_state))\nclassifiers.append(DecisionTreeClassifier(random_state = random_state))\nclassifiers.append(AdaBoostClassifier(DecisionTreeClassifier(random_state = random_state), random_state = random_state, learning_rate = 0.1))\nclassifiers.append(RandomForestClassifier(random_state = random_state))\nclassifiers.append(ExtraTreesClassifier(random_state = random_state))\nclassifiers.append(GradientBoostingClassifier(random_state = random_state))\nclassifiers.append(MLPClassifier(random_state = random_state))\nclassifiers.append(KNeighborsClassifier())\nclassifiers.append(LogisticRegression(random_state = random_state))\nclassifiers.append(LinearDiscriminantAnalysis())\n\ncv_results = []\nfor classifier in classifiers:\n    cv_results.append(cross_val_score(classifier, X_train, y = Y_train, scoring = \"accuracy\", cv = kfold, n_jobs = 4))\n    \ncv_means = []\ncv_std = []\nfor cv_result in cv_results:\n    cv_means.append(cv_result.mean())\n    cv_std.append(cv_result.std())\n    \ncv_res = pd.DataFrame({\"CrossValMeans\": cv_means, \"CrossValerrors\": cv_std,\n                       \"Algorithm\": [\"SVC\", \"DecisionTree\", \"AdaBoost\", \"RandomForest\",\n                                     \"ExtraTrees\", \"GradientBoosting\", \"MultipleLayerPerceptron\", \"KNeighboors\",\n                                    \"LogisticRegression\", \"LinearDiscriminantAnalysis\"]})\n\ng = sns.barplot(\"CrossValMeans\", \"Algorithm\", data = cv_res, palette = \"Set3\",\n               orient = \"h\", **{'xerr': cv_std})\ng.set_xlabel(\"Mean Accuracy\")\ng = g.set_title(\"Cross validation scores\")","0d0f234b":"# Grid Search Optimization for Five Models\n    \n# Adaboost\nDTC = DecisionTreeClassifier()\nadaDTC = AdaBoostClassifier(DTC, random_state = 7)\n\nada_param_grid = {\"base_estimator__criterion\" : [\"gini\", \"entropy\"],\n                 \"base_estimator__splitter\": [\"best\", \"random\"],\n                 \"algorithm\": [\"SAMME\", \"SAMME.R\"],\n                 \"n_estimators\": [1,2],\n                 \"learning_rate\": [0.0001,0.001, 0.01, 0.1, 0.2, 0.3, 1.5]}\n\ngsadaDTC = GridSearchCV(adaDTC,param_grid = ada_param_grid, cv = kfold, scoring = \"accuracy\",\n                       n_jobs = 4, verbose = 1)\ngsadaDTC.fit(X_train, Y_train)\nada_best = gsadaDTC.best_estimator_\n\ngsadaDTC.best_score_","6e79f146":"# ExtraTrees\n\nExtC = ExtraTreesClassifier()\n\n# Search Grid for Optimal Parameters\n\nex_param_grid = {\"max_depth\": [None],\n                \"max_features\": [1,2,10],\n                \"min_samples_split\": [2, 3, 10],\n                \"min_samples_leaf\": [1,3,10],\n                \"bootstrap\": [False],\n                \"n_estimators\": [100, 300],\n                \"criterion\": [\"gini\"]}\n\ngsExtC = GridSearchCV(ExtC, param_grid = ex_param_grid, cv = kfold, scoring = \"accuracy\",\n                     n_jobs = 4, verbose = 1)\n\ngsExtC.fit(X_train, Y_train)\nExtC_best = gsExtC.best_estimator_\n\ngsExtC.best_score_","cf0e1c25":"# RandomForestClassifier\nRFC = RandomForestClassifier()\n\n# Search Grid for Optimal Parameters\nrf_param_grid = {\"max_depth\": [None],\n                \"max_features\": [1,3,10],\n                \"min_samples_split\": [2,3,10],\n                \"min_samples_leaf\": [1,2,10],\n                \"bootstrap\": [False],\n                \"n_estimators\": [100,300],\n                \"criterion\": [\"gini\"]}\n\ngsRFC = GridSearchCV(RFC, param_grid = rf_param_grid, cv=kfold, scoring = \"accuracy\", n_jobs = 4,\n                    verbose = 1)\n\ngsRFC.fit(X_train,Y_train)\nRFC_best = gsRFC.best_estimator_\n\ngsRFC.best_score_\n","94da8f68":"# Gradient boosting tunning\n\nGBC = GradientBoostingClassifier()\ngb_param_grid = {\"loss\": [\"deviance\"],\n                \"n_estimators\": [100,200,300],\n                \"learning_rate\": [0.1, 0.05, 0.01],\n                \"max_depth\": [4, 8],\n                \"min_samples_leaf\": [100,150],\n                \"max_features\": [0.3, 0.1]}\n\ngsGBC = GridSearchCV(GBC,param_grid = gb_param_grid, cv = kfold, scoring = \"accuracy\",\n                    n_jobs = 4, verbose = 1)\n\ngsGBC.fit(X_train, Y_train)\nGBC_best = gsGBC.best_estimator_\n\ngsGBC.best_score_","ee193ec3":"# SVC classifier\nSVMC = SVC(probability=True)\nsvc_param_grid = {'kernel': ['rbf'], \n                  'gamma': [ 0.001, 0.01, 0.1, 1],\n                  'C': [1, 10, 50, 100,200,300, 1000]}\n\ngsSVMC = GridSearchCV(SVMC,param_grid = svc_param_grid, cv=kfold, scoring=\"accuracy\", n_jobs= 4, verbose = 1)\n\ngsSVMC.fit(X_train,Y_train)\n\nSVMC_best = gsSVMC.best_estimator_\n\n# Best score\ngsSVMC.best_score_","a2c57a0c":"def plot_learning_curve(estimator, title, X, y, ylim=None, cv=None,\n                        n_jobs=-1, train_sizes=np.linspace(.1, 1.0, 5)):\n    \"\"\"Generate a simple plot of the test and training learning curve\"\"\"\n    plt.figure()\n    plt.title(title)\n    if ylim is not None:\n        plt.ylim(*ylim)\n    plt.xlabel(\"Training examples\")\n    plt.ylabel(\"Score\")\n    train_sizes, train_scores, test_scores = learning_curve(\n        estimator, X, y, cv=cv, n_jobs=n_jobs, train_sizes=train_sizes)\n    train_scores_mean = np.mean(train_scores, axis=1)\n    train_scores_std = np.std(train_scores, axis=1)\n    test_scores_mean = np.mean(test_scores, axis=1)\n    test_scores_std = np.std(test_scores, axis=1)\n    plt.grid()\n\n    plt.fill_between(train_sizes, train_scores_mean - train_scores_std,\n                     train_scores_mean + train_scores_std, alpha=0.1,\n                     color=\"r\")\n    plt.fill_between(train_sizes, test_scores_mean - test_scores_std,\n                     test_scores_mean + test_scores_std, alpha=0.1, color=\"g\")\n    plt.plot(train_sizes, train_scores_mean, 'o-', color=\"r\",\n             label=\"Training score\")\n    plt.plot(train_sizes, test_scores_mean, 'o-', color=\"g\",\n             label=\"Cross-validation score\")\n\n    plt.legend(loc=\"best\")\n    return plt\n\ng = plot_learning_curve(gsRFC.best_estimator_,\"RF mearning curves\",X_train,Y_train,cv=kfold)\ng = plot_learning_curve(gsExtC.best_estimator_,\"ExtraTrees learning curves\",X_train,Y_train,cv=kfold)\ng = plot_learning_curve(gsSVMC.best_estimator_,\"SVC learning curves\",X_train,Y_train,cv=kfold)\ng = plot_learning_curve(gsadaDTC.best_estimator_,\"AdaBoost learning curves\",X_train,Y_train,cv=kfold)\ng = plot_learning_curve(gsGBC.best_estimator_,\"GradientBoosting learning curves\",X_train,Y_train,cv=kfold)","b0d6caaf":"votingC = VotingClassifier(estimators = [(\"rfc\", RFC_best), (\"extc\", ExtC_best),\n                                        (\"svc\", SVMC_best), (\"adac\", ada_best),\n                                        (\"gbc\", GBC_best)], voting = \"soft\", n_jobs = 4)\n\nvotingC = votingC.fit(X_train, Y_train)","aca779d0":"submission = pd.read_csv(\"..\/input\/gender_submission.csv\")","0b1dc8ab":"submission.head()","57c88954":"df_test[\"Fare\"].fillna(\"35.6271\", inplace = True)\nX_test = df_test.values","e0229a32":"prediction = votingC.predict(X_test)","10b10af3":"submission[\"Survived\"] = prediction","fd848702":"submission.to_csv(\".\/The_first_submission.csv\", index = False)","97c3d247":"### Age Feature Engineering","acc3d5ed":"### combining models","6ac96d3d":"****Machine Leanrning Model Development****","998d6e8b":"#### The survival rate confirmation according to Cabin is excluded because most of the missing values are.","87a80e43":"### The Survival Rate of the Related to the Fare","4d5d1967":"Check the accuracy of the bar graph and determine which models to ensemble.","b42cc3af":"## EDA","7c94d319":"### Embarked Feature Engineering","5c82b77e":"## Feature Engineering","e59a1487":"### The Survival Rate of the Related to the Pclass","1dbcacf6":"## Submission","afe66760":"### Hyperparameter tunning for best models","802dbf61":"### Family: SibSp + Parch","b8019f9c":"### One Hot Encoding","a83743ae":"![](http:\/\/imgbntnews.hankyung.com\/bntdata\/images\/photo\/201802\/46e751857d79621b9b2c0422b13c57d1.jpg)\n\n***It's a kernel for beginners who are first introduced.***\n\n***It's based on the existing kernels and tried to explain them as easily as possible.***\n\n***This kernel can achieve the top 28 percent and I hope it will help beginners a lot.***","137dbc3f":"#### Pearson Coefficient Hitmap","0a6e34bf":"### Replace string data with numerical data","d4eebec5":"### The Survival Rate of the Related to the Age","922e96d9":"## Ensemble Practice","35702187":"### plot learning curves","f2b861a9":"### Age, Sex, Pclass Violin plot","ad446831":"#### Clean up the last data before creating the Machine Learning Model","e2bd3432":"## Data Check","80f51d57":"### The Survival Rate of the Related to the Fare\n****It deals with in the feature engineering and the new feature is created and it confirms.\nFirst of all, it excludes.****","de76cc62":"### The Survival Rate of the Related to the Embarked","2e9bfb19":"### Use Age to transform continuous variables into categorical variables"}}