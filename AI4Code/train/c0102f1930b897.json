{"cell_type":{"ed11aa8d":"code","cd058ed7":"code","270437b0":"code","92806527":"code","7fac61df":"code","5dc5b302":"code","9acd766f":"code","ca380b0f":"code","cc2ab11d":"code","69461a39":"code","175577d4":"code","896f41ef":"code","2f5d0410":"code","d837104d":"code","6e0223d3":"code","c43a6956":"code","51587766":"code","c8b70008":"code","8dda17aa":"code","60605e37":"code","8f0c25c8":"code","cba1d0f2":"code","f5d46368":"code","25518d15":"code","790e99de":"code","e58fcaea":"code","9f5a92c6":"code","55e1608b":"code","d6e7e95c":"code","67e1abe9":"code","95ba1b65":"code","c9693f42":"code","0c1058f8":"code","591d29da":"code","fc1c7ad7":"code","84e923b4":"code","40d87db5":"markdown","aab559dd":"markdown","fe6bf345":"markdown","3f2800d7":"markdown","c6d93079":"markdown","eacc7776":"markdown","a9a6cd02":"markdown","5221e5ee":"markdown","81f275b0":"markdown","756b85ad":"markdown","f7307087":"markdown","9412b703":"markdown","56b85953":"markdown","0724d5f8":"markdown","e40b8d15":"markdown","6c234399":"markdown","ca3c2eb4":"markdown"},"source":{"ed11aa8d":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","cd058ed7":"# Import Dependencies\n%matplotlib inline\n\n# Start Python Imports\nimport math, time, random, datetime\n\n# Data Manipulation\nimport numpy as np\nimport pandas as pd\n\n# Visualization \nimport matplotlib.pyplot as plt\nimport missingno\nimport seaborn as sns\nplt.style.use('seaborn-whitegrid')\n\n# Preprocessing\nfrom sklearn.preprocessing import OneHotEncoder, LabelEncoder, label_binarize\n# Machine learning\nimport catboost\nfrom sklearn.model_selection import train_test_split\nfrom sklearn import model_selection, tree, preprocessing, metrics, linear_model\nfrom sklearn.svm import LinearSVC\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.linear_model import LinearRegression, LogisticRegression, SGDClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom catboost import CatBoostClassifier, Pool, cv\nfrom sklearn.preprocessing import StandardScaler\nfrom keras.layers.advanced_activations import ReLU\nfrom keras.models import Sequential, Model\nfrom keras.layers import Activation, Convolution2D, MaxPooling2D, BatchNormalization, Flatten, Dense, Dropout, Conv2D,MaxPool2D, ZeroPadding2D\n\n\n\n\n# Let's be rebels and ignore warnings for now\nimport warnings\nwarnings.filterwarnings('ignore')\n","270437b0":"train_data = pd.read_csv('..\/input\/facial-keypoints-detection\/training.zip', compression='zip', header=0, sep=',', quotechar='\"')\ntest_data = pd.read_csv('..\/input\/facial-keypoints-detection\/test.zip', compression='zip', header=0, sep=',', quotechar='\"')\nIdLookupTable = pd.read_csv('..\/input\/facial-keypoints-detection\/IdLookupTable.csv',header=0, sep=',', quotechar='\"')\nSampleSubmission = pd.read_csv('..\/input\/facial-keypoints-detection\/SampleSubmission.csv',header=0, sep=',', quotechar='\"')\n\n\n\n","92806527":"train_data.head()","7fac61df":"train_data.head().T.tail()\n","5dc5b302":"len(train_data['Image'][1])\n","9acd766f":"train_data.info()","ca380b0f":"test_data.head()","cc2ab11d":"len(test_data['Image'][1])\n","69461a39":"test_data.info()","175577d4":"IdLookupTable.head()","896f41ef":"IdLookupTable.info()","2f5d0410":"SampleSubmission.head()","d837104d":"\ndisplay(train_data.isnull().sum())\n","6e0223d3":"train_data.isnull().sum().plot(kind='bar',color = 'pink')\nplt.title(\"Missing Data \")\nplt.show()","c43a6956":"train_data.fillna(method = 'ffill',inplace = True)\n","51587766":"train_data.isnull().sum().plot(kind='bar',color = 'pink')\nplt.title(\"Missing Data \")\nplt.show()","c8b70008":"display(test_data.isnull().sum())\n","8dda17aa":"test_data.isnull().sum().plot(kind='bar',color = 'pink')\nplt.title(\"Missing Data \")\nplt.show()","60605e37":"display(IdLookupTable.isnull().sum())\n","8f0c25c8":"Vis = []\n\nfor i in range(len(train_data)):\n  Vis.append(train_data['Image'][i].split(' '))\n","cba1d0f2":"array_float = np.array(Vis, dtype='float')\n\nX_train = array_float.reshape(-1,96,96,1)","f5d46368":"photo_visualize = array_float[1].reshape(96,96)\n\nplt.imshow(photo_visualize, cmap='gray')\nplt.title(\"Visualize Image\")\nplt.show()","25518d15":"\nFacial_Keypoints_Data = train_data.drop(['Image'], axis=1)\nfacial_pnts = []\n\nfor i in range(len(Facial_Keypoints_Data)):\n  facial_pnts.append(Facial_Keypoints_Data.iloc[i])\n  ","790e99de":"training_data = train_data.drop('Image',axis = 1)\n\ny_train = []\nfor i in range(len(train_data)):\n    y = training_data.iloc[i,:]\n    y_train.append(y)\n    \n    \ny_train = np.array(y_train,dtype = 'float')","e58fcaea":"\nfacial_pnts_float = np.array(facial_pnts, dtype='float')\n","9f5a92c6":"photo_visualize_pnts = Facial_Keypoints_Data.iloc[1].values\n\nplt.imshow(photo_visualize, cmap='gray')\nplt.scatter(photo_visualize_pnts[0::2], photo_visualize_pnts[1::2], c='Pink', marker='*')\nplt.title(\"Image with Facial Keypoints\")\nplt.show()","55e1608b":"train_data.shape","d6e7e95c":"model = Sequential()\n\n\n# layer 1\nmodel.add(Convolution2D(32, (3,3), activation = 'relu', padding='same', use_bias=False, input_shape=(96,96,1)))\nmodel.add(BatchNormalization())\nmodel.add(MaxPool2D(pool_size=(2, 2)))\n\n# layer 2\n\nmodel.add(Convolution2D(32, (3,3), activation = 'relu', padding='same', use_bias=False))\nmodel.add(MaxPool2D(pool_size=(2, 2)))\n\n# layer 3\n\nmodel.add(Convolution2D(64, (3,3), activation = 'relu', padding='same', use_bias=False))\nmodel.add(BatchNormalization())\nmodel.add(MaxPool2D(pool_size=(2, 2)))\n\n# layer 4\n\nmodel.add(Convolution2D(128, (3,3), activation = 'relu', padding='same', use_bias=False))\nmodel.add(BatchNormalization())\nmodel.add(MaxPool2D(pool_size=(2, 2)))\n\n\nmodel.add(Flatten())\nmodel.add(Dense(256,activation='relu'))\nmodel.add(Dropout(0.1))\nmodel.add(Dense(30))\n","67e1abe9":"model.summary()","95ba1b65":"model.compile(optimizer='adam', \n              loss='mean_squared_error',\n              metrics=['acc'])","c9693f42":"model.fit(X_train,y_train,epochs = 5,batch_size = 32,validation_split = 0.2)","0c1058f8":"test_images = []\nfor i in range(len(test_data)):\n    item = test_data['Image'][i].split(' ')\n    test_images.append(item)","591d29da":"array_float_test = np.array(test_images,dtype = 'float')\nX_test = array_float_test.reshape(-1,96,96,1) ","fc1c7ad7":"predict = model.predict(X_test)\n","84e923b4":"# from IdLookupTable table \nFeatureName = list(IdLookupTable['FeatureName'])\nImageId = list(IdLookupTable['ImageId']-1)\nRowId = list(IdLookupTable['RowId'])\n\n\n# predict results\npredict = list(predict)\n\n\nData = []\nfor i in list(FeatureName):\n    Data.append(FeatureName.index(i))\n    \n    \nData_Pre = []\nfor x,y in zip(ImageId,Data):\n    Data_Pre.append(predict[x][y])\n    \n    \n\n\n\n\n\nRowId = pd.Series(RowId,name = 'RowId')\nLocation = pd.Series(Data_Pre,name = 'Location')\n\nsubmission = pd.concat([RowId,Location],axis = 1)\nsubmission.to_csv('.\/Submit.csv',index=False)\n\n","40d87db5":"## Keras CNN ","aab559dd":"## prepare data test","fe6bf345":"### test","3f2800d7":"#### prepare data x train","c6d93079":"# Visualize Data","eacc7776":"# predict","a9a6cd02":"# Prepare and Split Data","5221e5ee":"# Build Model","81f275b0":"# train Data","756b85ad":"# submission","f7307087":"### IdLookupTable","9412b703":"#### show Photo","56b85953":"#### Show Photo image with facial points","0724d5f8":"### Train","e40b8d15":"##### prepare data y train","6c234399":"# Check Missing Data\n","ca3c2eb4":"#### Facial Keypoints"}}