{"cell_type":{"752b9e9c":"code","fa67f359":"code","66a53f7d":"code","96837288":"code","e9cef9ce":"code","31541226":"code","674b6f43":"code","e7b421fd":"code","68919ba8":"code","d4d49d3e":"code","6391a34e":"code","8048f6c1":"code","3502e92c":"code","ee8cd6d9":"code","086151fd":"code","51a75bbe":"code","f146fb98":"code","d9899f77":"code","aebd942e":"code","82b8efd4":"code","1d2351ad":"code","127303c2":"code","00257f04":"markdown","97c073fc":"markdown","5a2faaa9":"markdown","401167e7":"markdown","e157940e":"markdown","b5ecf073":"markdown","2d9df687":"markdown","11527cd7":"markdown"},"source":{"752b9e9c":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","fa67f359":"import pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt","66a53f7d":"df_train = pd.read_csv(\"\/kaggle\/input\/digit-recognizer\/train.csv\")\ndf_test = pd.read_csv(\"\/kaggle\/input\/digit-recognizer\/test.csv\")","96837288":"df_train.head()","e9cef9ce":"df_train.info()","31541226":"df_train.describe()","674b6f43":"sns.countplot(x= df_train[\"label\"]) # Almost balanced dataset\nplt.show()","e7b421fd":"y_train = df_train.pop('label')\nX_train = df_train","68919ba8":"X_train = X_train.values.reshape(-1, 28, 28, 1)\nX_test = df_test.values.reshape(-1, 28, 28, 1)","d4d49d3e":"X_train = X_train\/255\nX_test = X_test\/255","6391a34e":"plt.imshow(X_train[5], cmap='gray')\nplt.show()\nplt.imshow(X_train[8], cmap='gray')\nplt.show()","8048f6c1":"X_train.shape, y_train.shape","3502e92c":"from tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Conv2D,MaxPooling2D,Flatten,Dense,Dropout\nfrom tensorflow.keras import callbacks","ee8cd6d9":"model=Sequential([\n    Conv2D(32, (5,5) , activation='relu' , input_shape=(28,28,1)),\n    MaxPooling2D(pool_size=(2,2)),\n    \n    Conv2D(64,(5,5), activation ='relu'),\n    MaxPooling2D(pool_size=(2,2)),\n    Dropout(0.25),\n    \n    Conv2D(64,(3,3), activation ='relu'),\n    MaxPooling2D(pool_size=(2,2)),\n    Dropout(0.25),\n    \n    Flatten(),\n    Dense(64, activation='relu'),\n    Dense(10, activation='softmax')\n])","086151fd":"callback = callbacks.EarlyStopping(monitor='loss', patience=5)","51a75bbe":"model.compile(optimizer='adam',loss='sparse_categorical_crossentropy',metrics=['accuracy'])","f146fb98":"model.fit(X_train, y_train, epochs=50, batch_size=64, callbacks=[callback])","d9899f77":"y_test = model.predict(X_test)","aebd942e":"y_test = np.argmax(y_test, axis = 1)","82b8efd4":"index_list = []\nfor i in list(df_test.index):\n    index_list.append(i+1)","1d2351ad":"submission_df = pd.DataFrame({\n    \"ImageId\": index_list,\n    \"Label\": y_test\n})","127303c2":"submission_df.to_csv(\"submission_cnn.csv\", index = False)","00257f04":"# Reshaping and scaling the predictors","97c073fc":"# Model Building","5a2faaa9":"# MNIST Digits - Classification Using SVM","401167e7":"# Load Dataset","e157940e":"# Loading Libraries","b5ecf073":"# Model Predictions","2d9df687":"In this notebook, we'll explore the popular MNIST dataset and build an SVM model to classify handwritten digits. <a href='http:\/\/yann.lecun.com\/exdb\/mnist\/'>Here is a detailed description of the dataset.<\/a>\n\nWe'll divide the analysis into the following parts:\n- Data understanding and cleaning\n- Data preparation for model building\n- Building an SVM model - hyperparameter tuning, model evaluation etc.","11527cd7":"# Spliting into predictor and target"}}