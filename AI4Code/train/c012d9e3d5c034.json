{"cell_type":{"0a04d96f":"code","dbf6e64e":"code","37cfe3ca":"code","bd926ddb":"code","d92b0062":"code","5cea2d09":"code","d4f04dcb":"code","63e8bf99":"code","6773fff5":"code","88d9de1a":"code","d9426e6f":"code","549dd218":"code","dac4f670":"code","7c6276bb":"code","30a552d9":"code","a99d5c12":"code","e8d1b676":"code","d90a5809":"code","e9231daf":"code","cffe5bb2":"code","ff5f9fcc":"code","0f18dd03":"code","28579afc":"code","59eee7dc":"code","1d72d583":"code","6771440f":"code","5643af43":"code","ad90918e":"markdown","8a6f73e2":"markdown","eea9eb65":"markdown","566f4f10":"markdown","89a5cd18":"markdown","22785016":"markdown","73f52513":"markdown","76a8607c":"markdown","04e00883":"markdown","67543434":"markdown","44ad2a39":"markdown","a062737a":"markdown","9843b934":"markdown"},"source":{"0a04d96f":"#!pip install category_encoders\n#!pip install LightGBM\n\nimport os\nimport gc\nimport math\nimport numpy as np\nimport pandas as pd\nimport category_encoders as ce\nimport lightgbm as lgbm\nimport matplotlib.pyplot as plt\n\nfrom copy import copy\nfrom sklearn.model_selection import KFold\nfrom sklearn.metrics import mean_absolute_error as mae","dbf6e64e":"# Setting to display more columns than default\n\npd.set_option(\"display.max_rows\", 1000)\ngc.collect()","37cfe3ca":"# Data Loading\ntrain = pd.read_csv(\"..\/input\/train.csv\")\ntest = pd.read_csv(\"..\/input\/test.csv\")\nsample_sub = pd.read_csv(\"..\/input\/sample_submission.csv\")\nstructures = pd.read_csv(\"..\/input\/structures.csv\")\n\n# Original size\nprint(f\"train.shape: {train.shape}\")\nprint(f\"test.shape: {test.shape}\")","bd926ddb":"X_train = train.drop(columns=['scalar_coupling_constant']).copy()\ny_train = train['scalar_coupling_constant'].copy()\nX_test = test.copy()\n\nX_train = X_train.drop(columns = ['id'])\nX_test = X_test.drop(columns = ['id'])\n\n# This is done because some steps in data pre-processing can chnage the order of rows in X_train\n# and order of rows in target may not be chnaged becuase there is not processing done on it. So to maintain which row in X_train corresponds to \n# target the index is reset.\nX_train = X_train.reset_index()\nX_test = X_test.reset_index()","d92b0062":"# function to convert object columns to category\n\ndef convert_object_to_categories(X_train, X_test):\n    for col in X_train.columns:\n        if X_train[col].dtype == 'O':\n            X_train[col] = X_train[col].astype('category')\n            X_test[col] = X_test[col].astype('category')\n    return X_train, X_test\n\nX_train, X_test = convert_object_to_categories(X_train, X_test)","5cea2d09":"# The score function\n\ndef calc_score(X_train, y_train, y_val):\n    X_train_new = X_train.copy()\n    y_train_new = y_train.copy()\n    y_val_new = y_val.copy()\n    y_val_new = pd.Series(y_val_new)\n    X_train_new = X_train_new.reset_index(drop=True)\n    y_train_new = y_train_new.reset_index(drop=True)\n    X_train_new = X_train_new.merge(pd.DataFrame(y_train_new, columns=['scalar_coupling_constant']), left_index=True, right_index=True)\n    X_train_new = X_train_new.merge(pd.DataFrame(y_val_new, columns=['y_val']), left_index=True, right_index=True)\n    X_train_new['error'] = (X_train_new['scalar_coupling_constant'] - X_train_new['y_val']).abs()\n    X_train_new['count'] = 1\n    score_df = X_train_new.groupby(by = ['type']).agg({'count': 'count', 'error': 'sum'})\n    score_df['error'] = (score_df['error']\/score_df['count']).apply(np.log, dtype=float)\n    score = (1\/score_df.shape[0])*(score_df['error'].sum())\n    return score","d4f04dcb":"# Cross Validation\n\ndef cross_val(X, y):\n    kf = KFold(n_splits=5, shuffle=True, random_state=10)\n    fold = 0\n    for train_index, val_index in kf.split(X):\n        fold +=1\n        lgbm_model = lgbm.LGBMRegressor(random_state=10, n_estimators=1000)\n        lgbm_model.fit(X.loc[train_index,:], y[train_index])\n        y_val = lgbm_model.predict(X.loc[val_index,:])\n        print(f\"fold{fold} score: {calc_score(X.loc[val_index,:],y[val_index],y_val)}\")","63e8bf99":"# Baseline cross validation score\n# cross_val(X_train, y_train)\n\n# score: 1.13 approx","6773fff5":"# Baseline model\n# lgbm_model = lgbm.LGBMRegressor()\n# lgbm_model.fit(X_train, y_train)\n# y_predict = lgbm_model.predict(X_test)","88d9de1a":"# Joining training\/test and structures\n\nX_train = X_train.merge(structures, left_on = ['molecule_name','atom_index_0'], right_on = ['molecule_name', 'atom_index'], sort = True)\nX_train = X_train.rename(columns={'atom_index': 'atom_index_0_0', 'x':'atom_index_0_x', 'y':'atom_index_0_y', 'z':'atom_index_0_z', 'atom': 'atom_0'})\nX_test = X_test.merge(structures, left_on = ['molecule_name','atom_index_0'], right_on = ['molecule_name', 'atom_index'], sort = True)\nX_test = X_test.rename(columns={'atom_index': 'atom_index_0_0', 'x':'atom_index_0_x', 'y':'atom_index_0_y', 'z':'atom_index_0_z', 'atom': 'atom_0'})\n\n\nX_train = X_train.merge(structures, left_on = ['molecule_name','atom_index_1'], right_on = ['molecule_name', 'atom_index'], sort = True)\nX_train = X_train.rename(columns={'atom_index': 'atom_index_1_1', 'x':'atom_index_1_x', 'y':'atom_index_1_y', 'z':'atom_index_1_z', 'atom': 'atom_1'})\nX_test = X_test.merge(structures, left_on = ['molecule_name','atom_index_1'], right_on = ['molecule_name', 'atom_index'], sort = True)\nX_test = X_test.rename(columns={'atom_index': 'atom_index_1_1', 'x':'atom_index_1_x', 'y':'atom_index_1_y', 'z':'atom_index_1_z', 'atom': 'atom_1'})","d9426e6f":"# Dropping redundant columns\n\nX_train = X_train.drop(columns=['atom_index_0_0','atom_index_1_1'])\nX_test = X_test.drop(columns=['atom_index_0_0','atom_index_1_1'])","549dd218":"# Distance between atoms\n\nX_train['distance'] = ((X_train['atom_index_0_x'] - X_train['atom_index_1_x'])**2 + (X_train['atom_index_0_y'] - X_train['atom_index_1_y'])**2 + (X_train['atom_index_0_z'] - X_train['atom_index_1_z'])**2)**0.5\nX_test['distance'] = ((X_test['atom_index_0_x'] - X_test['atom_index_1_x'])**2 + (X_test['atom_index_0_y'] - X_test['atom_index_1_y'])**2 + (X_test['atom_index_0_z'] - X_test['atom_index_1_z'])**2)**0.5  ","dac4f670":"# Making column based in join type (eg. 2JHC is 2J type of join)\n\nX_train['join_type'] = X_train['type'].str.slice(0,2)\nX_test['join_type'] = X_test['type'].str.slice(0,2)","7c6276bb":"print(X_train['atom_0'].unique())\nprint(X_test['atom_0'].unique())\n\n# So basically atom 0 in all molecules is always hydrogen. Hence will not hep in predicting target so we can drop it\n\nX_train = X_train.drop(columns=['atom_0'])\nX_test = X_test.drop(columns=['atom_0'])","30a552d9":"# Number of atoms in molecule\n\nX_train['num_atoms']=X_train.groupby(['molecule_name'])['atom_index_0'].transform('max') + 1\nX_test['num_atoms']=X_test.groupby(['molecule_name'])['atom_index_0'].transform('max') + 1","a99d5c12":"# Lets check correlation between all the variables\n\ndf = X_train.set_index(keys='index', drop=False).merge(pd.DataFrame(y_train, columns=['scalar_coupling_constant']), left_index=True, right_index=True)\ndf.corr()","e8d1b676":"X_train, X_test = convert_object_to_categories(X_train, X_test)","d90a5809":"# Setting index column as index so that cross validation function can pick matching rows from X_train_new and y_train. This is the reason \n# index was reset earlier. Reseting and setting index for test is not important would have made no chnage but I did because code looks more \n# consistent and also doing it is no harm\n\nX_train_new = X_train.set_index(keys='index')\nX_test_new = X_test.set_index(keys='index')","e9231daf":"# cross_val(X_train_new, y_train)\n# score is 0.7 approx","cffe5bb2":"# So that dataframe looks fine.\n\nX_train_new = X_train_new.sort_index(axis=0)\nX_test_new = X_test_new.sort_index(axis=0)","ff5f9fcc":"# Making num_of_bonds in between atoms\n# This is just an intuition 1J is H to direct carbon(ie one bond), 2J- 'HH' hydrogens attached to a common carbon atom so (2 bonds in between) or HC means hydrogen and carbvon but different carbon which is two bonds away   \n# 3J 'HH' mean hydrgen attached to diferent carbons (which are 3 bonds away)\n\nX_train_new['num_bonds'] = X_train_new['join_type'].str.slice(0,1)\nX_test_new['num_bonds'] = X_test_new['join_type'].str.slice(0,1)\n\n# 'num_bonds' should be integer\nX_train_new['num_bonds'] = X_train_new['num_bonds'].astype('int')\nX_test_new['num_bonds'] = X_test_new['num_bonds'].astype('int')","0f18dd03":"# Making angle between atoms, may be an important feature engineering. Angle will be in radians\n\ndef angle_between_vectors(df):\n    dot_products = (df['atom_index_0_x']*df['atom_index_1_x'] + df['atom_index_0_y']*df['atom_index_1_y'] + df['atom_index_0_z']*df['atom_index_1_z'])\n    magnitudes_product = (df['atom_index_0_x']**2+df['atom_index_0_y']**2+df['atom_index_0_z']**2)**0.5*(df['atom_index_1_x']**2+df['atom_index_1_y']**2+df['atom_index_1_z']**2)**0.5\n    df['angle'] = np.arccos(dot_products\/magnitudes_product)\n    return df\n\nX_train_new = angle_between_vectors(X_train_new)\nX_test_new = angle_between_vectors(X_test_new)","28579afc":"# cross_val(X_train_new, y_train)\n\n# score: 0.66 approx","59eee7dc":"# Training error\n\n# lgbm_model = lgbm.LGBMRegressor(random_state=10, n_estimators=1000)\n# lgbm_model.fit(X_train_new, y_train)\n# y_predict = lgbm_model.predict(X_train_new)\n# print(f\"training score: {calc_score(X_train_new, y_train, y_predict)}\")\n\n# training score: 0.6372931683989951","1d72d583":"# Modelling. I tried to see if increasing n_estomators improves score. It does so I am taking n_estomator=1000 \n# More estimators also takes a lot of time to compute.\n\nlgbm_model = lgbm.LGBMRegressor(random_state=10, n_estimators=1000)\nlgbm_model.fit(X_train_new, y_train)\ny_predict = lgbm_model.predict(X_test_new)","6771440f":"fig, ax = plt.subplots(figsize=(12,9))\nlgbm.plot_importance(lgbm_model, ax)","5643af43":"sample_sub['scalar_coupling_constant'] = list(y_predict)\nsample_sub.to_csv(\"submission.csv\", index=False)","ad90918e":"First defining some functions","8a6f73e2":"Making traning, target and testing dataframes","eea9eb65":"As we can see num_bonds is not very important feature. There can be two reasons either the intuition of making num_bonds is wrong or simply num_bonds is not enough information i.e. for for two rows with num_bonds=2, scalar_coupling constant can be very different since it can also depend upon whether it is between two hydrogen atoms or one hydrogen and one carbon. Also worth noting that the feature 'type' and 'join_type' both are more important than num_bonds but 'type' is much more important than both of them.","566f4f10":"Note: There is a high negative correlation between distance and scalar_coupling_constant","89a5cd18":"I watched this video to get an intuition for making more features\n1. https:\/\/www.youtube.com\/watch?v=CUI9bWH1i1Y ","22785016":" So new variables especially distance is very important they are decreasing error","73f52513":"## Preprocessing","76a8607c":"## Data Loading","04e00883":"### Baseline Model","67543434":"Making Submission file.","44ad2a39":"### References","a062737a":"## Import statements","9843b934":"Plot importance"}}