{"cell_type":{"b0746c15":"code","a52efe45":"code","d57958cc":"code","00dd626b":"code","1814e4a5":"code","ae15514e":"code","f9a0fb7f":"code","4d00f46d":"code","7145499e":"code","f35181cb":"code","20585639":"code","9739044f":"code","837fb716":"code","2b94901d":"code","1a43d26c":"code","d107ba64":"code","a6ee6f2e":"code","b9b56063":"code","f1dccb9d":"code","3944150b":"code","87473c2f":"code","5f608b48":"code","c80554c7":"code","dcb7c603":"code","5c29f5d7":"code","75239bd8":"code","3ccaea79":"code","6dbfd16c":"code","1cd315ae":"code","4cc085d3":"code","af0d638c":"code","d2bf31ca":"code","40e4a6df":"code","c767ecc4":"code","4cb23cde":"markdown","f070954a":"markdown","221fdcff":"markdown","5e4cbc48":"markdown","d07ecfbd":"markdown","5999d560":"markdown","df6c5460":"markdown","b983c385":"markdown","aa470ffa":"markdown","8aa7f98e":"markdown","dbc8491b":"markdown","70ede69e":"markdown"},"source":{"b0746c15":"import pandas as pd \nimport tensorflow as tf\nfrom tensorflow import keras\nimport numpy as np \nimport matplotlib.pyplot as plt\nimport os\n\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import classification_report, confusion_matrix\nfrom tensorflow.keras.layers import Embedding\nimport seaborn as sns\npd.set_option('display.max_colwidth', -1)","a52efe45":"dataset = pd.read_json('..\/input\/news-category-dataset\/News_Category_Dataset_v2.json', lines=True)\ndataset.drop(['authors', 'link', 'date'], axis = 1, inplace = True) \ndataset.head()","d57958cc":"dataset.info()","00dd626b":"dataset.describe()","1814e4a5":"print(\"We have a total of {} categories\".format(dataset['category'].nunique()))\ndataset['category'].value_counts()","ae15514e":"fig = plt.figure(figsize=(20,20))\nplt.pie(dataset['category'].value_counts().values, \n        labels=dataset['category'].value_counts().index,\n        autopct='%1.1f%%');","f9a0fb7f":"categories = dataset['category'].value_counts().index\n\ndef groupper(grouplist,name):\n    for ele in categories:\n        if ele in grouplist:\n            dataset.loc[dataset['category'] == ele, 'category'] = name","4d00f46d":"groupper( grouplist= ['WELLNESS', 'HEALTHY LIVING','HOME & LIVING','STYLE & BEAUTY' ,'STYLE'] , name =  'LIFESTYLE AND WELLNESS')\n\ngroupper( grouplist= [ 'PARENTING', 'PARENTS' ,'EDUCATION' ,'COLLEGE'] , name =  'PARENTING AND EDUCATION')\n\ngroupper( grouplist= ['SPORTS','ENTERTAINMENT' , 'COMEDY','WEIRD NEWS','ARTS'] , name =  'SPORTS AND ENTERTAINMENT')\n\ngroupper( grouplist= ['TRAVEL', 'ARTS & CULTURE','CULTURE & ARTS','FOOD & DRINK', 'TASTE'] , name =  'TRAVEL-TOURISM & ART-CULTURE')\n\ngroupper( grouplist= ['WOMEN','QUEER VOICES', 'LATINO VOICES', 'BLACK VOICES'] , name =  'EMPOWERED VOICES')\n\ngroupper( grouplist= ['BUSINESS' ,  'MONEY'] , name =  'BUSINESS-MONEY')\n\ngroupper( grouplist= ['THE WORLDPOST' , 'WORLDPOST' , 'WORLD NEWS'] , name =  'WORLDNEWS')\n\ngroupper( grouplist= ['ENVIRONMENT' ,'GREEN'] , name =  'ENVIRONMENT')\n\ngroupper( grouplist= ['TECH', 'SCIENCE'] , name =  'SCIENCE AND TECH')\n\ngroupper( grouplist= ['FIFTY' , 'IMPACT' ,'GOOD NEWS','CRIME'] , name =  'GENERAL')\n\ngroupper( grouplist= ['WEDDINGS', 'DIVORCE',  'RELIGION','MEDIA'] , name =  'MISC')","7145499e":"print(\"We have a total of {} categories now\".format(dataset['category'].nunique()))\ndataset['category'].value_counts()","f35181cb":"fig = plt.figure(figsize=(10,20))\nplt.pie(dataset['category'].value_counts().values, \n        labels=dataset['category'].value_counts().index,\n        autopct='%1.1f%%');","20585639":"df = dataset.copy() #Let's create a copy of the data frame\n","9739044f":"df.duplicated().sum()  #total duplicates","837fb716":"df.drop_duplicates(keep='last', inplace=True) ","2b94901d":"df.duplicated(subset=['short_description','headline']).sum() #duplicates under 'short_description' and 'headline'","1a43d26c":"df.drop_duplicates(subset=['short_description','headline'],keep='last',inplace=True)","d107ba64":"print(len(df[df['headline'] == \"\"]))","a6ee6f2e":"df.loc[df['headline'] == \"\", 'headline'] = np.nan\ndf.dropna(subset=['headline'], inplace=True)\nprint(len(df[df['headline'] == \"\"]))","b9b56063":"print(len(df[df['short_description'] == \"\"]))\n","f1dccb9d":"df.loc[df['short_description'] == \"\", 'short_description'] = np.nan\ndf.dropna(subset=['short_description'], inplace=True)\nprint(len(df[df['short_description'] == \"\"]))","3944150b":"from sklearn.utils import shuffle\ndf = shuffle(df)\ndf.reset_index(inplace=True, drop=True) ","87473c2f":"df.head()","5f608b48":"df['desc'] = df['headline'].astype(str)+\"-\"+df['short_description']\ndf.drop(columns =['headline','short_description'],axis = 1, inplace=True)\ndf.astype(str)\ndf.head()","c80554c7":"X,Y = df['desc'],df['category']\n\n#80% to train , 10% for validation , 10% for testing\nX_train, X_val, y_train, y_val = train_test_split(X,Y, test_size=0.2, random_state=42)\nX_val, X_test , y_val, y_test= train_test_split(X_val,y_val, test_size=0.5, random_state=42)","dcb7c603":"vocab_size =20000\nmax_length = 150\ntrunc_type='post'\npadding_type='post'\noov_tok = \"<OOV>\"","5c29f5d7":"tokenizer = tf.keras.preprocessing.text.Tokenizer(num_words = vocab_size, oov_token=oov_tok)\ntokenizer.fit_on_texts(X_train)\n\nword_index = tokenizer.word_index\n\nX_train = tokenizer.texts_to_sequences(X_train)\nX_train = pad_sequences(X_train,maxlen= max_length,padding=padding_type, truncating=trunc_type)\ny_train = np.asarray(y_train)\ny_train = pd.get_dummies(y_train)\n\nX_val = tokenizer.texts_to_sequences(X_val)\nX_val = pad_sequences(X_val,maxlen= max_length,padding=padding_type, truncating=trunc_type)\ny_val = np.asarray(y_val)\ny_val = pd.get_dummies(y_val)\n\ntrain_set = np.array(X_train)\nval_set = np.array(X_val)\n\ntrain_label = np.array(y_train)\nval_label = np.array(y_val)\n\n\ny_test = pd.get_dummies(y_test)\ny_test = np.asarray(y_test)\ny_test = np.argmax(y_test,axis=1)   #this would be our ground truth label while testing\n\nprint(train_set.shape)\nprint(train_label.shape)\n\n\nprint(val_set.shape)\nprint(val_label.shape)\n\n\n","75239bd8":"!wget http:\/\/nlp.stanford.edu\/data\/glove.6B.zip #downloadingu glove vec word embeddings\n","3ccaea79":"!unzip -q glove.6B.zip #unzipping ","6dbfd16c":"path_to_glove_file =  '.\/glove.6B.100d.txt'","1cd315ae":"#Initialising the embedding matrix with glove vec embeddings\n\nnum_tokens = len(tokenizer.word_index.items()) + 2\nembedding_dim = 100\nhits = 0\nmisses = 0\n\n\nembeddings_index = {}\nwith open(path_to_glove_file) as f:\n    for line in f:\n        word, coefs = line.split(maxsplit=1)\n        coefs = np.fromstring(coefs, \"f\", sep=\" \")\n        embeddings_index[word] = coefs\n\nprint(\"Found %s word vectors.\" % len(embeddings_index))\n\n\n# Prepare embedding matrix\nembedding_matrix = np.zeros((num_tokens, embedding_dim))\nfor word, i in tokenizer.word_index.items():\n    embedding_vector = embeddings_index.get(word)\n    if embedding_vector is not None:\n        # Words not found in embedding index will be all-zeros.\n        # This includes the representation for \"padding\" and \"OOV\"\n        embedding_matrix[i] = embedding_vector\n        hits += 1\n    else:\n        misses += 1\nprint(\"Converted %d words (%d misses)\" % (hits, misses))","4cc085d3":"early_stop=tf.keras.callbacks.EarlyStopping(monitor='val_loss', \n                                            patience=3, min_delta=0.0001)\n\n\ntf.keras.backend.clear_session()\nembed_size = 100\nmodel = keras.models.Sequential([\n                                 \n        Embedding(num_tokens,\n        embedding_dim,\n        embeddings_initializer=keras.initializers.Constant(embedding_matrix),\n        mask_zero=True,input_shape=[None],trainable=False),\n        keras.layers.Bidirectional(keras.layers.LSTM(256, dropout = 0.4)),\n        keras.layers.Dense(12, activation=\"softmax\")\n            \n        ])\n\n\nmodel.summary()\n","af0d638c":"opt = keras.optimizers.Adam(learning_rate=0.001)\nmodel.compile(loss=\"categorical_crossentropy\", optimizer=opt, metrics=[\"accuracy\"])\nhistory = model.fit( train_set,train_label,\n                     batch_size = 32,\n                     steps_per_epoch=len(X_train) \/\/ 32, \n                     validation_data = (val_set , val_label),\n                     validation_steps = len(val_set)\/\/32, epochs=20,\n                     callbacks=  early_stop )","d2bf31ca":" fig = plt.figure(figsize=(10,10))\n\n# Plot accuracy\nplt.subplot(221)\nplt.plot(history.history['accuracy'],'bo-', label = \"acc\")\nplt.plot(history.history['val_accuracy'], 'ro-', label = \"val_acc\")\nplt.title(\"train_accuracy vs val_accuracy\")\nplt.ylabel(\"accuracy\")\nplt.xlabel(\"epochs\")\nplt.grid(True)\nplt.legend()\n\n# Plot loss function\nplt.subplot(222)\nplt.plot(history.history['loss'],'bo-', label = \"loss\")\nplt.plot(history.history['val_loss'], 'ro-', label = \"val_loss\")\nplt.title(\"train_loss vs val_loss\")\nplt.ylabel(\"loss\")\nplt.xlabel(\"epochs\")\nplt.grid(True)\nplt.legend()\n","40e4a6df":"classes = dataset['category'].value_counts().index\n\ndef prediction(inference_data):\n    X = tokenizer.texts_to_sequences(inference_data)\n    X = pad_sequences(X,maxlen= max_length,padding=padding_type, truncating=trunc_type)\n    pred = model.predict(X)\n    pred_value = tf.argmax(pred,axis =1).numpy()                \n    return pred_value","c767ecc4":"y_pred = prediction(X_test)\nprint(classification_report(np.asarray(y_test),np.asarray( y_pred)))\ncf_matrix = confusion_matrix(y_test, y_pred)\n\n\nplt.figure(figsize=(10,10))\nheatmap = sns.heatmap(cf_matrix, xticklabels=classes,\n                      yticklabels=classes,\n                      annot=True, fmt='d', color='blue')\nplt.xlabel('Predicted class')\nplt.ylabel('True class')\nplt.title('Confusion matrix of model')","4cb23cde":"## Data Tidying","f070954a":"This is definitely not the best model as it performs well on certain classes only but adding more data to the classes with less data and groupping the data efficiently will improve the accuracy and specificity of the model significantly.\n\nLet me know if I can help you understand from the notebook! \nHave a great day!","221fdcff":"# Tokenizing and Padding","5e4cbc48":"# Model Training","d07ecfbd":"Let's try to group these categories and try to balance the dataset at the same time","5999d560":"# Evaluating and Making Predictions","df6c5460":"# Data Quality Assesment","b983c385":"## Group the Categories","aa470ffa":"### Duplicate Values","8aa7f98e":"# Embedding Matrix for our model","dbc8491b":"## Removing empty values and duplicates","70ede69e":"### Empty Values"}}