{"cell_type":{"ded7d617":"code","94d4331e":"code","1be8b98f":"code","2dcf51a4":"code","f3f49183":"code","a396d6f5":"code","c36db127":"code","c6b1b419":"code","3bb7f126":"markdown","6c386b50":"markdown","58e9f636":"markdown","8fbbc2e9":"markdown","48cf0c6a":"markdown","00ccafa5":"markdown","6144c285":"markdown","2109e567":"markdown","c81e58d3":"markdown","2a2bb3c8":"markdown","a781462b":"markdown"},"source":{"ded7d617":"import numpy as np \nimport pandas as pd \nfrom tensorflow.keras import Sequential\nfrom tensorflow.keras.layers import Dense, LSTM, CuDNNLSTM, Dropout, BatchNormalization\nfrom tensorflow.keras.optimizers import Adam, SGD, RMSprop\nfrom tensorflow.keras.callbacks import Callback, EarlyStopping\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.utils import shuffle\nfrom sklearn.model_selection import train_test_split\nfrom imblearn.over_sampling import BorderlineSMOTE\n\nimport datetime as dt","94d4331e":"def Load_LSTM_Data(data_dir, stock_file_list, resampling='BorderlineSMOTE2'):    \n    print('Start loading LSTM data')\n    t1 = dt.datetime.now()\n    LSTM_X_data = np.zeros((1, time_steps, features), dtype=np.float32)\n    LSTM_Y_data = np.zeros((1), dtype=np.int16)\n    fail_list = []\n    for stock in stock_file_list:\n        try:\n            #print('Processing symbol ', stock)\n            numpy_data_X_file = stock + '-LSTM_X-timestep40-forward_day5-StandardScaler.npy'\n            numpy_data_Y_file = stock + '-LSTM_Y-timestep40-forward_day5-StandardScaler.npy'\n            data_X = np.load(data_dir + numpy_data_X_file)\n            data_Y = np.load(data_dir + numpy_data_Y_file)        \n            # Resample data to avoid imbalance classes\n            data_X = data_X.reshape(data_X.shape[0], time_steps*features)\n            if resampling=='BorderlineSMOTE2':\n                data_X, data_Y = BorderlineSMOTE(kind='borderline-2').fit_resample(data_X, data_Y)\n            data_X = data_X.reshape(data_X.shape[0], time_steps, features)            \n            LSTM_X_data = np.append(LSTM_X_data, data_X, axis=0)\n            LSTM_Y_data = np.append(LSTM_Y_data, data_Y, axis=0)\n        except:\n            print('*** Can not processing stock ', stock)\n            fail_list.append(stock)       \n    LSTM_X_data, LSTM_Y_data = shuffle(LSTM_X_data[1:], LSTM_Y_data[1:], random_state=42)\n    t2 = dt.datetime.now()\n    print('Failed list: ', fail_list)\n    print('Loaded LSTM data. Finished in ', t2-t1)    \n    return LSTM_X_data, LSTM_Y_data\n\n\nfeature_columns = ['Month', 'Day', 'DayofWeek', 'DayofYear',\n           'Open', 'High', 'Low', 'Close', 'Volume',\n           'MA10', 'MA20', 'MA50', 'MA100', 'MA200', 'MAV50', 'RSI14', 'MFI14', 'BB_top', 'BB_bot',\n           'MACD_ml', 'MACD_sl', 'MACD_histogram', 'ADX', 'PDI', 'MDI', 'ATR', 'CCI',\n           'Chaikin', 'OBV', 'ROC', 'StochasticMomentum', 'RS_Index', 'Accumulation_Distribution',\n           'VNINDEX_O', 'VNINDEX_H', 'VNINDEX_L', 'VNINDEX_C', 'VNINDEX_V', 'VNINDEX_MA10', 'VNINDEX_MA20',\n           'VNINDEX_MA50', 'VNINDEX_MA100', 'VNINDEX_MA200', 'VNINDEX_MAV50', 'VNINDEX_RSI14', 'VNINDEX_MFI14', 'VNINDEX_BB_top', 'VNINDEX_BB_bot',\n           'DJI_O', 'DJI_H', 'DJI_L', 'DJI_C', 'DJI_V', 'DJI_MA50', 'DJI_MA100', 'DJI_MA200', 'DJI_MAV50',\n           'SP500_O', 'SP500_H', 'SP500_L', 'SP500_C', 'SP500_V', 'SP500_MA50', 'SP500_MA100', 'SP500_MA200', 'SP500_MAV50']\ntime_steps = 40\nfeatures = len(feature_columns)\nforward_day = 5\ndata_dir = '..\/input\/vn30-vnindex-dji-sp500-5classes-standardscaler\/'\n# Stock symbol list for VN30\nstock_file_list = ['CII', 'CTD', 'CTG', 'DHG', 'DPM', 'EIB', 'FPT', 'GAS', 'GMD', 'HDB', 'HPG', 'MBB', 'MSN', \n                   'MWG', 'NVL', 'PNJ', 'REE', 'ROS', 'SAB', 'SBT', 'SSI', 'STB', 'TCB', 'VCB', 'VHM', 'VIC', \n                   'VJC', 'VNM', 'VPB', 'VRE']\n\n# Load shuffled LSTM data, without resampling\nLSTM_X_data, LSTM_Y_data = Load_LSTM_Data(data_dir, stock_file_list, resampling='None')\n#Check the shape of data\nprint('LSTM_X_data.shape: ', LSTM_X_data.shape)\nprint('LSTM_Y_data.shape: ', LSTM_Y_data.shape)\n\nunique_origin, counts_origin = np.unique(LSTM_Y_data, return_counts=True)\ndata_dict = dict(zip(unique_origin, counts_origin))\nprint('Origin labels: ', data_dict)\n\n# Plot \n#sns.countplot(LSTM_Y_data, palette=sns.color_palette(\"RdYlGn_r\", 5))","1be8b98f":"def Create_LSTM_Model(unit_per_layer=1000, drop_out=0.5, optimizer='Adam', lr=1e-3):\n    # Create model with LSTM & Dense layers\n    model = Sequential()\n    model.add(CuDNNLSTM(units=unit_per_layer, input_shape=(time_steps, features), return_sequences=True))\n    model.add(Dropout(drop_out))\n    model.add(BatchNormalization())\n    model.add(CuDNNLSTM(units=unit_per_layer))     \n    model.add(Dropout(drop_out))\n    model.add(BatchNormalization())\n    model.add(Dense(units=unit_per_layer, activation='tanh'))\n    model.add(Dropout(drop_out))\n    model.add(BatchNormalization())\n    model.add(Dense(units=5, activation='softmax'))\n\n    if optimizer.upper()=='ADAM':\n        opti_func = Adam(lr=lr, amsgrad=True)\n    elif optimizer.upper()=='SGD':\n        opti_func = SGD(lr=lr)\n    elif optimizer.upper()=='RMSPROP':\n        opti_func = RMSprop(lr=lr)\n              \n    model.compile(optimizer=opti_func, loss='sparse_categorical_crossentropy', metrics=['accuracy']) \n   \n    return model","2dcf51a4":"model = Create_LSTM_Model(unit_per_layer=1000, drop_out=0.5, optimizer = 'Adam', lr=5e-4)\nt1 = dt.datetime.now()\nX_train, X_test, y_train, y_test = train_test_split(LSTM_X_data, LSTM_Y_data, test_size=0.1, random_state=42)\nhistory = model.fit(X_train, y_train, epochs=30, batch_size=512, validation_data=(X_test, y_test))\nt2 = dt.datetime.now()\nprint('Training time: ', t2-t1)  \n\nmodel.evaluate(LSTM_X_data, LSTM_Y_data)","f3f49183":"print('Origin labels: ', dict(zip(unique_origin, counts_origin)))\nLSTM_y_hat = model.predict(LSTM_X_data)\nLSTM_y_hat_pos = np.argmax(LSTM_y_hat, axis=1)\nunique_predict, counts_predict = np.unique(LSTM_y_hat_pos, return_counts=True)\nprint('Predicted labels: ', dict(zip(unique_predict, counts_predict)))\nconfusion_m = confusion_matrix(LSTM_Y_data, LSTM_y_hat_pos)\nprint('Confusion matrix of predicted data:')\nprint(confusion_m)\nfor i in range(len(unique_predict)):\n    print('Label {0} accuracy: {1:0.1f}%'.format(i, 100*confusion_m[i,i]\/counts_origin[i]))","a396d6f5":"# Load shuffled LSTM data, and oversample when load each stock\nLSTM_X_data_resample, LSTM_Y_data_resample = Load_LSTM_Data(data_dir, stock_file_list, resampling='BorderlineSMOTE2')\n#Check the shape of data\nprint('LSTM_X_data_resample.shape: ', LSTM_X_data_resample.shape)\nprint('LSTM_Y_data_resample.shape: ', LSTM_Y_data_resample.shape)\n\nunique_origin_resample, counts_origin_resample = np.unique(LSTM_Y_data_resample, return_counts=True)\ndata_dict_resample = dict(zip(unique_origin_resample, counts_origin_resample))\nprint('Origin labels: ', data_dict_resample)\n\n# Plot \n#sns.countplot(LSTM_Y_data_resample, palette=sns.color_palette(\"RdYlGn_r\", 5))","c36db127":"model_resample = Create_LSTM_Model(unit_per_layer=1000, drop_out=0.5, optimizer = 'Adam', lr=5e-4)\nt1 = dt.datetime.now()\nX_train_resample, X_test_resample, y_train_resample, y_test_resample = train_test_split(LSTM_X_data_resample, LSTM_Y_data_resample, \n                                                                                        test_size=0.1, random_state=42)\nhistory = model_resample.fit(X_train_resample, y_train_resample, epochs=30, batch_size=512, \n                             validation_data=(X_test_resample, y_test_resample))\n\nt2 = dt.datetime.now()\nprint('Training time: ', t2-t1)  \n\nmodel_resample.evaluate(LSTM_X_data_resample, LSTM_Y_data_resample)\n\nmodel_resample.evaluate(LSTM_X_data, LSTM_Y_data)","c6b1b419":"print('Origin labels: ', dict(zip(unique_origin, counts_origin)))\nLSTM_y_hat2 = model_resample.predict(LSTM_X_data)\nLSTM_y_hat_pos2 = np.argmax(LSTM_y_hat2, axis=1)\nunique_predict2, counts_predict2 = np.unique(LSTM_y_hat_pos2, return_counts=True)\nprint('Predicted labels: ', dict(zip(unique_predict2, counts_predict2)))\nconfusion_m2 = confusion_matrix(LSTM_Y_data, LSTM_y_hat_pos2)\nprint('Confusion matrix of predicted data:')\nprint(confusion_m2)\nfor i in range(len(unique_predict2)):\n    print('Label {0} accuracy: {1:0.1f}%'.format(i, 100*confusion_m2[i,i]\/counts_origin[i]))","3bb7f126":"**Explore result**\n\nThe model get quite good result after 30 epochs: : Accuracy is 80% for training dataset and just 67% for validation dataset. It means the model is overfit for training set.\n> Epoch 30\/30\n> 50580\/50580 [==============================] - 25s 489us\/sample - loss: 0.3693 - acc: 0.8378 - val_loss: 0.8591 - val_acc: 0.6867\n","6c386b50":"**Dataset**\n\nVN30 can be consider as the collection of 30 best stocks in Vietnam stock market. In this kernel, I use VN30 stocks' historical data and indexes' historical data as input data (X) and the price actions in next 5 days (Y) to avoid short term volatility, fraud actions... as target classes. \n\nThe data is pre-interpreted into numpy 3D data in format (samples, time_steps, features) where time_steps and features are chosen 40 and 66 respectly. Each sample has 40 time steps and in each time step, there are 66 features. The data is normalize using sklearn StandardScaler.\n\nThe origin data of VN30 to 2019-07-01 consist of 1193 samples Wide Rally, 8522 samples Rally, 27558 samples Fluctuating, 16078 samples Correction and 2850 samples Widely Correction. \n![image.png](attachment:image.png)\n\nThe chart shows that in most of time, stocks fluctuate sideway in a certain range in short-term. The cases of \"Wide Rally\" and \"Wide Correction\" are rare. It also shows that the target data is very imbalance and suggest it is very hard to train the model.\n\nFollowing code will load data to LSTM_X_data, LSTM_Y_data variable to feed into neural network.","58e9f636":"**Training the model**\n\nThe model chosen has 2 LSTM layers and 2 Dense layers and 1000 nodes per layer. \n\nDropout rate of 0.5 and BatchNormalization are used to avoid overfit and better convergence.","8fbbc2e9":"**Oversample to get better result**\n\nFor such imbalance problem, the common solution is oversampling to make almost equal dataset.\nIn this kernel, I use borderline-2 SMOTE technique to create equal dataset. \n![image.png](attachment:image.png)\n\n*Note: While oversampling using SMOTE, some stock with not enough data such as HDB, VJC, VRE will generate error. For such little data, we can just ignore this symbol or add origin data to oversampled data.*","48cf0c6a":"**Stock market prediction method**\n\nFinancial market is very volatility. The price fluctuate every day, maybe in a random rhythm and can not predictable (as point out in  Burton Malkiel's \"A Random  Walk Down Wall Street\"). The price depends on almost all the things happened in the world: news, oil price, FED's interest rate... even accidents \n\nThrough history, people developed many methods to predict market trends. It can be categorised by:\n\n1) Fundamental analysis (FA): identify \"real\" value of stock by macroeconomic factors such as the state of the economy, the industry conditions to microeconomic factors like the effectiveness of the company's management.\n\n2) Technical analysis (TA): identify stock trends by analyzing patterns of price movements, trading signals and various other analytical charting tools to evaluate a security's strength or weakness.\n\n3) Combining both of them to find best time to enter and exit\n\nAs mentioned, stock market is very volatility. So accuracy prediction for next day close price is impossible. However stocks move by trend (short-term trend and long-term trend). And most of stock move along with indexes (VN-Index in Vietnam, DJI and S&P500 in USA). \n\nMy method is to predict short-term trend to see how the stock move based on histrical technical data of it own and of indexes. The movement will be categorised in to 5 classes (0: Wide Rally, 1: Rally, 2: Fluctuating, 3: Correction, 4: Widely Correction)\n\n","00ccafa5":"The model works well with oversampled data. The accuracy is almost 94% for training set and is 87% for testing set. The model is also less overfit. That is impressive!\n\n> Epoch 30\/30\n> 122296\/122296 [==============================] - 60s 487us\/sample - loss: 0.1528 - acc: 0.9381 - val_loss: 0.4423 - val_acc: 0.8665\n\n","6144c285":"To explore the result further, I create confusion matrix to calculate each target label accuracy of whole dataset. The result is:\n\n> Label 0 accuracy: 86.9%\n\n> Label 1 accuracy: 83.9%\n\n> Label 2 accuracy: 82.7%\n\n> Label 3 accuracy: 82.5%\n\n> Label 4 accuracy: 89.2%","2109e567":"Let's check if the accuracy for origin data is improved:\n> Label 0 accuracy: 96.1%\n\n> Label 1 accuracy: 91.6%\n\n> Label 2 accuracy: 88.3%\n\n> Label 3 accuracy: 89.1%\n\n> Label 4 accuracy: 94.1%\n\nThe result is really impressive with almost 90% accuracy for all labels.","c81e58d3":"**Conclusions:**\n\nAs the result, I can have some quick conclusions:\n\n1) LSTM neural network is good method to predict next stock market trend\n\n2) As the result, accuracy of almost 90% is very impressive. It can be use in combine with portfolio management to win in stock market.\n\n3) Even with impressive accuracy, stock market is always hard to predict. And the model has not been using in real-time to see if it is workable.","2a2bb3c8":"**Stock market prediction**\n\n\nWhen we mention to stock market, we first one to remember Warrant Buffet, the third-wealthiest person in the world, and the most successfull investor in the world. Buffet has created his own investment philosophy around the concept of value investing, a systemetically method to choose good company and invest into them. However, there are many other investors with different good methods getting rich everyday with fundamental analysis (FA) or technical analysis (TA) or combining both of them.\n\nIn AI era nowaday, using AI to predict stock market is a new trend. In this kernel, I try to make a prediction model for VN30 stock (top 30 stock in HOSE, Vietname). Data feeding to the model is basic data as OHLC and its derivation data such as indicators (RSI, MFI...). Let's explore to see if it is possible to predict stock market. \n\nFirst things first. We need to import neccessary libraries to get data, make model and make predictions: numpy, pandas, tensorflow, sklearn...","a781462b":"**Neural Network**\n\nA biological neural network is composed of a group or groups of chemically connected or functionally associated neurons. An Artificial Neural Network (ANN) is a network of neurons or nodes, try to mimic human brain's operation. ANN can be used to create software to recognize simple things to human such as digits or even more complex things that even human can not recognize.\n\nIn this kernel, I use LSTM neural network to recognize and predict price trend.\n\n**Why LSTM?**\nLong short-term memory (LSTM) is a special type of RNN which has \"memories\" to preceed sequence data such as speech or video... As mentioned, this kernel is trying to predict stock trend by using historical data, so LSTM is a good architect to use.\n![image.png](attachment:image.png)\n\n"}}