{"cell_type":{"f56bffde":"code","4e396ddb":"code","d6cdfbd5":"code","6d337c2c":"code","ea5e3c0b":"code","d049e969":"code","d6a92080":"code","28426d6d":"code","413cb0d6":"code","249efc94":"code","d4515227":"code","d4a26486":"code","8b9d73f7":"code","41fe832e":"code","fcbcc72e":"code","21dcdbf7":"code","0c80f84b":"code","033abad4":"code","3c09f60f":"code","b6c6803d":"code","72116c72":"code","a2ee1bbe":"code","b23a6481":"code","c0ea94db":"code","07748b9c":"code","aadc6247":"code","3633fccb":"code","92a71e8d":"code","ac2f11c5":"code","b50dfab7":"code","4d239164":"code","ee1a7275":"code","f20370c0":"markdown","8cefea31":"markdown","bb5b081c":"markdown","6ee5c47d":"markdown","716b4920":"markdown","4c9a5fbd":"markdown","f02129d8":"markdown","a2e6396e":"markdown","4bf94de7":"markdown","31d1efbd":"markdown","96f49356":"markdown","3b001ee4":"markdown","1d14b4f8":"markdown"},"source":{"f56bffde":"from IPython.display import HTML\n\nHTML('<iframe width=\"100%\" height=\"500\" src=\"https:\/\/www.youtube.com\/embed\/yb-3zYncYvY?start=2&autoplay=1&rel=0&amp;controls=0&amp;showinfo=0\" frameborder=\"0\" allowfullscreen><\/iframe>')","4e396ddb":"# We start with some easy steps. You don't need to authorize GCP at this point.  \nfrom google.cloud import bigquery","d6cdfbd5":"# Client is needed for configuring API requests. Leaving it empty will initiate Kaggle's public dataset BigQuery integration.\nclient = bigquery.Client()","6d337c2c":"# Construct a reference to the \"hacker_news\" dataset\ndataset_ref = client.dataset(\"hacker_news\", project=\"bigquery-public-data\")\n\n# API request - fetch the dataset\ndataset = client.get_dataset(dataset_ref)","ea5e3c0b":"# List all the tables in the \"hacker_news\" dataset\ntables = list(client.list_tables(dataset))\n\n# Print names of all tables in the dataset (there are four!)\nfor table in tables:  \n    print(table.table_id)","d049e969":"# Construct a reference to the \"full\" table\ntable_ref = dataset_ref.table(\"full\")\n\n# API request - fetch the table\ntable = client.get_table(table_ref)","d6a92080":"# Print information on all the columns in the \"full\" table in the \"hacker_news\" dataset\ntable.schema","28426d6d":"# Preview the first five lines of the \"full\" table: Pandas style\nclient.list_rows(table, max_results=5).to_dataframe()","413cb0d6":"# Let's create our first SQL query on HN database. \n# We would like to return all stories and comments published after '2018-01-01' containing word \"privacy\" or \"Privacy\" in their titles or texts.\n# We select all available columns, hence we use \"*\"\nquery = \"\"\"\n        SELECT *\n        FROM `bigquery-public-data.hacker_news.full`\n        WHERE (REGEXP_CONTAINS(text, r\"(p|P)rivacy\") OR REGEXP_CONTAINS(title, r\"(P|p)rivacy\")) AND timestamp > '2018-01-01'\n        \"\"\"","249efc94":"# Set up the query\nquery_job = client.query(query)","d4515227":"# API request - run the query, and return a pandas DataFrame\ndf = query_job.to_dataframe()","d4a26486":"df.head()","8b9d73f7":"df[df['type']=='story'][:3]","41fe832e":"from google.cloud import bigquery","fcbcc72e":"# BigQuery authentication\n# Please add your project_id\nPROJECT_ID = 'hackernews-301014'\n\n# We create a new client object, this time we make a reference to a specific project created in GCP.\nclient = bigquery.Client(project=PROJECT_ID)","21dcdbf7":"# Thic cell shows how you can create an empty dataset within your project. It will serve us a space for appending sql tables.\nDATASET_ID='priv'\ndataset_ref=client.dataset(DATASET_ID)\n\ndataset=bigquery.Dataset(dataset_ref)\n\ndataset=client.create_dataset(dataset)","0c80f84b":"# Construct a reference to the \"hacker_news\" dataset\ndataset_ref = client.dataset(\"priv\", project=PROJECT_ID)\n\n# API request - fetch the dataset\ndataset = client.get_dataset(dataset_ref)","033abad4":"# Right now within the dataset object you can see the name of your project and a new dataset called \"priv\"\ndataset","3c09f60f":"# We start with creating a new table within the dataset 'priv' called 'priv_stories'. It will contain a subset of HN dataset consisting of stories related to privacy issues.\n\n# TODO(developer): Set table_id to the ID of the destination table.\n# table_id = \"your-project.your_dataset.your_table_name\"\ntable_id = \"hackernews-301014.priv.priv_stories\"\n\njob_config = bigquery.QueryJobConfig(destination=table_id)\n\nquery = \"\"\"\n    SELECT title, timestamp, text, url, score, descendants, type, id AS id_0\n    FROM `bigquery-public-data.hacker_news.full`\n    WHERE (REGEXP_CONTAINS(text, r\"(p|P)rivacy\") OR REGEXP_CONTAINS(title, r\"(P|p)rivacy\")) AND timestamp > '2018-01-01' AND type='story'\n    \"\"\"\n\n# Start the query, passing in the extra configuration.\nquery_job = client.query(query, job_config=job_config)  # Make an API request.\nquery_job.result()  # Wait for the job to complete.\n\nprint(\"Query results loaded to the table {}\".format(table_id))","b6c6803d":"# List all the tables\ntables = list(client.list_tables(dataset))\n\n# Print names of all tables in the dataset\nfor table in tables:  \n    print(table.table_id)","72116c72":"# Our first join operation. Let's join our new table priv_stories with the full HN dataset using id->parent relation\n# After this operation we should reveive all stories about privacy and their immediate (first level) comments\n# We save the results to a new table 'priv_c0'\n# In order to prevent creating duplicated column names (BQ would throw an error) we add column aliases \"_0\" (see: AS clause)\n# id will refer to story id, id_0 to first level comment's id. Same logic applies to text column. \n\n# TODO(developer): Set table_id to the ID of the destination table.\n# table_id = \"your-project.your_dataset.your_table_name\"\ntable_id = \"hackernews-301014.priv.priv_c1\"\n\njob_config = bigquery.QueryJobConfig(destination=table_id)\n\nquery= '''\n    SELECT p.title, p.timestamp, p.text AS text_0, p.url, p.score, p.descendants, p.type, id_0, o.parent, o.text AS text_1, o.by, o.id AS id_1\n    FROM `hackernews-301014.priv.priv_stories` p\n    LEFT JOIN `bigquery-public-data.hacker_news.full` o ON p.id_0 = o.parent\n    '''\n\n# Start the query, passing in the extra configuration.\nquery_job = client.query(query, job_config=job_config)  # Make an API request.\nquery_job.result()  # Wait for the job to complete.\n\nprint(\"Query results loaded to the table {}\".format(table_id))","a2ee1bbe":"main_table_id = 'hackernews-301014.priv.priv_c'\nfor lvl in range(2,7):\n    table_id = \"{}{}\".format(main_table_id, lvl)\n    print(table_id)\n    job_config = bigquery.QueryJobConfig(destination=table_id)\n    p_ids = ', '.join(['p.id_{}'.format(i) for i in range(lvl)])\n    p_texts = ', '.join(['p.text_{}'.format(i) for i in range(lvl)])\n    query= 'SELECT p.title, p.timestamp, p.url, p.score, p.descendants, p.type, {}, {}, o.parent, o.text AS text_{}, o.by, o.id AS id_{}'.format(p_ids, p_texts, lvl, lvl) +\\\n        ' FROM `{}{}` p'.format(main_table_id, lvl-1) +\\\n        ' LEFT JOIN `bigquery-public-data.hacker_news.full` o ON p.id_{} = o.parent'.format(lvl-1)\n    print(query)\n    # Start the query, passing in the extra configuration.\n    query_job = client.query(query, job_config=job_config)  # Make an API request.\n    query_job.result()  # Wait for the job to complete.\n    print(\"Query results loaded to the table {}\".format(table_id))","b23a6481":"# Construct a reference to the \"priv_c6\" table\ntable_ref = dataset_ref.table(\"priv_c6\")\n\n# API request - fetch the table\ntable = client.get_table(table_ref)","c0ea94db":"df=client.list_rows(table).to_dataframe()","07748b9c":"len(df)","aadc6247":"df.head()","3633fccb":"df.to_csv('\/kaggle\/working\/hn.csv')","92a71e8d":"df_stories=df.drop_duplicates('id_0')[['title','url','text_0','score','descendants','type','id_0', 'timestamp']]\ndf_stories.reset_index(inplace=True, drop=True)","ac2f11c5":"def comments(row, df):\n    temp=df[df['id_0']==row['id_0']]\n    comments_list=[]\n    for i, rowtemp in temp.iterrows():\n        for i in range(0,7):\n            col='text_'+str(i)\n            if pd.isnull(rowtemp[col]) is False:\n                comments_list.append(rowtemp[col])\n    return list(set(comments_list))","b50dfab7":"import pandas as pd\ndf_stories['comments']=df_stories.apply(lambda row: comments(row, df), axis=1)","4d239164":"df_stories.head()","ee1a7275":"len(df_stories['comments'][3])","f20370c0":"# HackerNews analysis with BigQuery\nauthors: DELab UW NGI Forward Team\n\n<div style=\"text-align: justify\">\nIn this tutorial we will work with a dataset of stories and comments on <a href=\"https:\/\/news.ycombinator.com\">Hacker News<\/a> - an aggregator website focusing on:\n<\/div>\n\n> _Anything that good hackers would find interesting._    ","8cefea31":"<div style=\"text-align: justify\">\nAnalysis will be carried out using BigQuery (BQ), a Google's web service that lets you apply SQL to huge datasets.\nWe will use BQ integration with Kaggle Kernels (thanks Kaggle!). It is very convenient for a number of reasons. First and foremost it will save you the hassle of dealing with Google Cloud Platform (GCP) \"exquisite\" UI (sorry Google!). Nevertheless, you will still need to create a GCP account <a href='https:\/\/console.cloud.google.com\/'>here<\/a> for more advanced steps. In this tutorial it refers to creating tables and saving them in your project's dataset. Registration is free (you even get 300$ for start \ud83e\udd11) but your credit card number is required for authentication. We will mark the point from which you need to be registered with the \"\u00ae\ufe0f\" sign.\n<\/div>","bb5b081c":"# Run basic SQL queries against chosen dataset\n","6ee5c47d":"# View basic information about the chosen dataset\n","716b4920":"# Connect to BQ from the Kaggle kernel","4c9a5fbd":"### In the next step we create tables in a for loop\n- We follow the same procedure as the one described above\n- But this time we join comments to their comments\n- You can define the range of the loop in order to get fewer or more levels of comments\n- We print consecutive SQL queries","f02129d8":"# Data cleaning\nIn the following part we create a condensed df.\nOne row represents a single story and all its comments are located in the list in comments column.","a2e6396e":"### As you have seen above you can find two types of elements in the Hacker News dataset: stories and comments (see: column type).\n\n### Each story and comment have their unique `id`. Comments also have `parent_id`, which indicate a higher level entity given comment is referring to. It may be either a story or another comment as HN comments have a multilevel structure (see e.g.: https:\/\/news.ycombinator.com\/item?id=26364285)\n\n\n### Suppose you want to collect all comments related to their stories. First step is straightforward. You simply join stories with comments using relation `id` -> `parent_id`. But what about comments of comments, then their comments and so on? This time you do not have information about the story id anymore. \n\n### You can approach this issue in a few ways. Felipe Hoffa proposed an elegant [solution](https:\/\/news.ycombinator.com\/item?id=10441435) to this problem, but it may seem a bit intimidating for a beginner SQL user. Moreover, it is written in Legacy SQL a non-standard variant of SQL.\n\n### In the following part of this tutorial we will join HN comments and stories using incremental steps and basic queries.","4bf94de7":"# Sources and further materials:\n- https:\/\/www.kaggle.com\/learn\/intro-to-sql\n- TBD: Towards Data Science blog post","31d1efbd":"\u00ae\ufe0f\n\nFrom this moment on you need to be registered GCP user and add authorization to your GCP account as it is showed in the opening video to this tutorial.","96f49356":"# Run SQL joins on the HN dataset","3b001ee4":"# Create datasets and tables using BQ API\n","1d14b4f8":"<div style=\"text-align: justify\">\nFor those of you who experimented with <a href=\"https:\/\/googleapis.dev\/python\/bigquery\/latest\/index.html\">Python's client for BQ<\/a> in your local jupyter notebooks, you know that authentication process involved can be troublesome. Easy integration is an advantage of Kaggle over local solutions. I show you how to authorize your Google account in the short video below. Please note that I have already signed in to my G-account before in Kaggle so I only need to attach my account to the current notebook. If you have never done this, then you need to do one more step, namely you need to click \"add account\" first.\n<\/div>\n\n\nAlso please note that:\n\n> _Each Kaggle user can scan 5TB every 30 days for free. Once you hit that limit, you'll have to wait for it to reset._\n\nBut do not worry as queries presented in this tutorial are far lighter.\n\n### First part presents how you can:\n- connect to BQ from the Kaggle kernel\n- view basic information about the chosen dataset\n- run basic SQL queries against it\n\nThis part uses code samples from [Getting Started With SQL and BigQuery]('https:\/\/www.kaggle.com\/dansbecker\/getting-started-with-sql-and-bigquery') kernel.\n\n### Second part focuses on:\n- creating datasets and tables using BQ API\n- running SQL joins on the HN dataset\n- finding a workaround for the issues related to the multilevel structure of HN dataset"}}