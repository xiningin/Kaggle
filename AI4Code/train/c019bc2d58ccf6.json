{"cell_type":{"c821c25d":"code","68152db2":"code","1789738f":"code","99c3434b":"code","aaf38a4e":"code","afb50c2b":"code","9b60791d":"code","2a9aee13":"code","ff9e9e32":"code","f4f423fb":"code","d6671b72":"code","b1b0da1f":"code","204d5d27":"code","63c73abc":"code","0876f2cb":"code","364297fb":"code","424a1772":"code","f6663fb3":"code","839d75be":"code","864edf91":"code","6b310a42":"code","84596092":"code","3cf1c955":"code","2234743c":"code","2cedf347":"code","7ab4a5eb":"code","07096efc":"code","fa05780f":"code","241d3cc9":"code","5e3e1335":"code","cfe14ba5":"code","cfb52762":"code","85fd4b71":"code","1b26f1a0":"code","bc492b02":"code","035571c6":"code","c43578f5":"code","d5185e55":"code","d0d0c1d8":"code","02ae5816":"code","1298fcf7":"code","e9f02359":"code","a760beda":"code","c9c69699":"code","be9fafa1":"code","b57ef25d":"code","ede524cf":"code","d4301142":"code","ad6dc131":"code","c14acfbf":"code","f27d7d9c":"code","d3a262be":"code","35c5fed6":"code","121891bd":"code","7ec6a271":"code","68047314":"code","7e502400":"code","ad5ff1ed":"code","f420bc5d":"code","e5a66ba5":"code","44da6a0c":"code","ae9e7f1a":"code","66a471ce":"code","04cbf877":"code","91418683":"code","d0c197c9":"code","7f3e7ba2":"code","9a3704e1":"code","677c7161":"code","59926444":"code","1b7f3e5a":"code","8d3c0cbb":"code","bd15fa94":"code","21e329fb":"code","b948f9bc":"code","fadbedb9":"code","e4f5f92a":"code","d7e1d271":"code","08f713e9":"code","98d2c14a":"code","5ae17aca":"code","432d927e":"code","7624231d":"code","64f5a4d5":"code","abdbc28c":"code","e0175472":"code","b3a00803":"code","d0ca03a0":"code","aca535c7":"code","b54e5fa5":"code","98c15903":"code","3ae79081":"code","2e980e20":"code","cbd57231":"code","ccba573f":"code","025b19ba":"code","0b9d9322":"code","bbbcf3bf":"code","6c0f0424":"code","fcb2efbb":"code","0fb26afe":"code","5ff23bdb":"code","bff72987":"code","031e5dac":"code","1ac6c83f":"code","b835bc24":"code","cb944294":"code","78112cab":"code","a66c9d16":"code","af2de2ec":"code","c9f4228a":"code","e6198d64":"code","6760d21a":"code","fa395a7f":"code","f56e496b":"code","874b76c1":"code","b9a9aa7f":"code","f1b6d67f":"code","b2389213":"code","bf4431e5":"code","7ab4e94b":"code","10c8482e":"code","7329ed10":"code","2a710e78":"code","3302d001":"markdown","9a980214":"markdown","0a07c33b":"markdown","ac5e9c22":"markdown","2f645dd0":"markdown","b7706d36":"markdown","80d5e5ae":"markdown","2c9aa93a":"markdown","2785b214":"markdown","4980ba7e":"markdown","3149f130":"markdown","fbe72c61":"markdown","8ef52e21":"markdown","96cadd85":"markdown","697906e1":"markdown","9398923c":"markdown","4d42cc09":"markdown","ac998147":"markdown","21af180c":"markdown","59185060":"markdown","b36a6bd0":"markdown","c9bf8dc8":"markdown","fff5da28":"markdown","092a80de":"markdown","489465be":"markdown","7eb615c6":"markdown","e5dada65":"markdown","f0949508":"markdown","53f14637":"markdown","47f6688f":"markdown","af788bd5":"markdown","05326a0a":"markdown","a3ca0247":"markdown","013ebcf4":"markdown","09d54f7c":"markdown","b3a88296":"markdown","cb0faef8":"markdown","440bfb40":"markdown","21de2512":"markdown","dc75e23d":"markdown","05942fcd":"markdown","c3f3fd4e":"markdown","93ab16ec":"markdown","dc7d2d73":"markdown","0f95cd3e":"markdown","4466948c":"markdown","3dd25a64":"markdown","98985155":"markdown","bc86dbda":"markdown","a6611bcc":"markdown","7ab96022":"markdown"},"source":{"c821c25d":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","68152db2":"df = pd.read_csv('..\/input\/a-fine-windy-day-hackerearth-ml-challenge\/train_data.csv')\ndf_test = pd.read_csv('..\/input\/a-fine-windy-day-hackerearth-ml-challenge\/test_data.csv')","1789738f":"%matplotlib inline\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set_style('whitegrid')\nplt.style.use('seaborn-deep')\nplt.style.use('fivethirtyeight')\nplt.rcParams['font.family'] = 'sans-serif'\nplt.rcParams['font.monospace'] = 'Ubunto Mono'\nplt.rcParams['font.size'] = 10\nplt.rcParams['axes.labelsize'] = 12\nplt.rcParams['axes.titlesize'] = 12\nplt.rcParams['xtick.labelsize'] = 12\nplt.rcParams['ytick.labelsize'] = 12\nplt.rcParams['legend.fontsize'] = 12\nplt.rcParams['figure.titlesize'] = 14\nplt.rcParams['figure.figsize'] = (16,10)\n\nimport warnings\nwarnings.filterwarnings('ignore')\npd.options.mode.chained_assignment = None\npd.options.display.float_format = '{:.2f}'.format\npd.set_option('display.max_columns', 200)\npd.set_option('display.width', 400)","99c3434b":"df","aaf38a4e":"df_test","afb50c2b":"df.info()","9b60791d":"## Showing the number of unique values of every feature \ndf.nunique()","2a9aee13":"df_test.nunique()","ff9e9e32":"## Missing values in train dataset\nsns.heatmap(df.isnull(),cbar=False,yticklabels=False,cmap = 'viridis')","f4f423fb":"df.isna().sum()","d6671b72":"## Missing values in test dataset\nsns.heatmap(df_test.isnull(),cbar=False,yticklabels=False,cmap = 'viridis')","b1b0da1f":"df_test.isna().sum()","204d5d27":"corr = df.corr()\nplt.figure(figsize=(20,10))\nmask = np.zeros_like(corr,dtype=np.bool)\nmask[np.triu_indices_from(mask)] = True\nsns.heatmap(corr,mask=mask,annot=True)\nplt.show()","63c73abc":"## Spliting the train dataset into categorical and numerical features\ndef getFeatures(df):\n    num_features = df.select_dtypes(include=[np.number])\n    cat_features = df.select_dtypes(include=[np.object])\n    return num_features,cat_features","0876f2cb":"num_features,cat_features = getFeatures(df)","364297fb":"num_features","424a1772":"cat_features","f6663fb3":"## Box plot of numerical features\nfig = plt.figure(figsize=(30,20))\nfor i in range(len(num_features.columns)):\n    fig.add_subplot(4,5,i+1)\n    sns.boxplot(y = num_features.iloc[:,i])\nplt.tight_layout()\nplt.show()","839d75be":"## Hist plot for categorical features\n'''fig = plt.figure(figsize=(20,10))\nfor i in range(len(cat_features.columns)):\n    fig.add_subplot(4,1,i+1)\n    cat_features.iloc[:,i].hist()\n    plt.xlabel([cat_features.columns[i]])\nplt.tight_layout()\nplt.show()'''","864edf91":"sns.pairplot(df)\nplt.show()","6b310a42":"df.columns","84596092":"skew_features = num_features.apply(lambda x :x.skew()).sort_values(ascending=True)\nskew_features","3cf1c955":"## Copying the train dataframe into new dataframe and we will be performing changes on the new dataframe\ndf_cpy = df.copy()","2234743c":"def comparing_train_and_test_feature(df,df_test,col):\n    fig = plt.figure(figsize=(16,10))\n    ax0 = fig.add_subplot(1,2,1)\n    ax1 = fig.add_subplot(1,2,2)\n    df[col].plot(kind='kde',ax=ax0)\n    df_test[col].plot(kind='kde',ax=ax1)\n    ax0.set_xlabel(col)\n    ax1.set_xlabel(col)\n    ax0.set_title(\"Density plot of \" + str(col) + \" of training set\")\n    ax1.set_title(\"Density plot of \" + str(col) + \" of testing set\")\n    plt.show()","2cedf347":"## Comparing the density plot of features in training and testing set\ncomparing_train_and_test_feature(df,df_test,'wind_speed(m\/s)')\n## Distribution of Feature wind_speed(m\/s) of training and testing dataset are very similar","7ab4a5eb":"sns.boxplot(y='wind_speed(m\/s)',data=df)","07096efc":"sns.scatterplot(x='wind_speed(m\/s)',y='windmill_generated_power(kW\/h)',hue='cloud_level',data=df)","fa05780f":"df['wind_speed(m\/s)'].value_counts()","241d3cc9":"## Comparing the density plot of features in training and testing set\ncomparing_train_and_test_feature(df,df_test,'atmospheric_temperature(\u00b0C)')\n## Distribution of Feature \"atmospheric_temperature(\u00b0C)\" of training and testing dataset is very similar","5e3e1335":"sns.boxplot(y='atmospheric_temperature(\u00b0C)',data=df)","cfe14ba5":"sns.scatterplot(x='atmospheric_temperature(\u00b0C)',y='windmill_generated_power(kW\/h)',hue='cloud_level',data=df)","cfb52762":"df[df['atmospheric_temperature(\u00b0C)'] < -50]","85fd4b71":"## Comparing the density plot of features in training and testing set\ncomparing_train_and_test_feature(df,df_test,'shaft_temperature(\u00b0C)')\n## Distribution of Feature \"shaft_temperature(\u00b0C)\" of training and testing dataset are almost same","1b26f1a0":"sns.boxplot(y='shaft_temperature(\u00b0C)',data=df)","bc492b02":"sns.scatterplot(x='shaft_temperature(\u00b0C)',y='windmill_generated_power(kW\/h)',hue='cloud_level',data=df)","035571c6":"plt.figure(figsize=(20,10))\nsns.scatterplot(x='shaft_temperature(\u00b0C)',y='wind_speed(m\/s)',hue='cloud_level',data=df)","c43578f5":"## Comparing the density plot of features in training and testing set\ncomparing_train_and_test_feature(df,df_test,'blades_angle(\u00b0)')\n## Distribution of Feature \"blades_angle(\u00b0)\" of training and testing dataset are almost same","d5185e55":"sns.boxplot(y='blades_angle(\u00b0)',data=df)","d0d0c1d8":"sns.scatterplot(x='blades_angle(\u00b0)',y='windmill_generated_power(kW\/h)',hue='cloud_level',data=df)","02ae5816":"## Comparing the density plot of features in training and testing set\ncomparing_train_and_test_feature(df,df_test,'gearbox_temperature(\u00b0C)')\n## Distribution of Feature \"gearbox_temperature(\u00b0C)\" of training and testing dataset are almost similar","1298fcf7":"sns.boxplot(y='gearbox_temperature(\u00b0C)',data=df)","e9f02359":"sns.scatterplot(x='gearbox_temperature(\u00b0C)',y='windmill_generated_power(kW\/h)',hue='cloud_level',data=df)","a760beda":"## Here we are removing extreme outliers which are present in a very less number\nlow = df_cpy['gearbox_temperature(\u00b0C)'] < -200\nhigh = df_cpy['gearbox_temperature(\u00b0C)'] > 300\nlow = np.where(low)\nhigh = np.where(high)\ndf_cpy.drop(low[0],inplace=True)\ndf_cpy.drop(high[0],inplace=True)\ndf_cpy.index = range(df_cpy.shape[0])","c9c69699":"plt.figure(figsize=(20,10))\nsns.scatterplot(x='gearbox_temperature(\u00b0C)',y='windmill_generated_power(kW\/h)',hue='cloud_level',data=df_cpy)","be9fafa1":"## Comparing the density plot of features in training and testing set\ncomparing_train_and_test_feature(df,df_test,'engine_temperature(\u00b0C)')\n## Distribution of Feature \"engine_temperature(\u00b0C)\" of training and testing dataset are almost similar","b57ef25d":"sns.boxplot(y='engine_temperature(\u00b0C)',data=df)","ede524cf":"sns.scatterplot(x='engine_temperature(\u00b0C)',y='windmill_generated_power(kW\/h)',hue='cloud_level',data=df)","d4301142":"## Dropping extreme outliers\nlow = df_cpy['engine_temperature(\u00b0C)'] < 38\nlow = np.where(low)\ndf_cpy.drop(low[0],inplace=True)\ndf_cpy.index = range(df_cpy.shape[0])","ad6dc131":"plt.figure(figsize=(20,10))\nsns.scatterplot(x='engine_temperature(\u00b0C)',y='windmill_generated_power(kW\/h)',hue='cloud_level',data=df_cpy)","c14acfbf":"## Comparing the density plot of features in training and testing set\ncomparing_train_and_test_feature(df,df_test,'motor_torque(N-m)')\n## Distribution of Feature \"motor_torque(N-m)\" of training and testing dataset are almost similar","f27d7d9c":"sns.boxplot(y='motor_torque(N-m)',data=df)","d3a262be":"sns.scatterplot(x='motor_torque(N-m)',y='windmill_generated_power(kW\/h)',hue='cloud_level',data=df)","35c5fed6":"## Comparing the density plot of features in training and testing set\ncomparing_train_and_test_feature(df,df_test,'generator_temperature(\u00b0C)')\n## Distribution of Feature \"generator_temperature(\u00b0C)\" of training and testing dataset are almost same","121891bd":"sns.boxplot(y='generator_temperature(\u00b0C)',data=df)","7ec6a271":"sns.scatterplot(x='generator_temperature(\u00b0C)',y='windmill_generated_power(kW\/h)',hue='cloud_level',data=df)","68047314":"## Comparing the density plot of features in training and testing set\ncomparing_train_and_test_feature(df,df_test,'atmospheric_pressure(Pascal)')\n## Distribution of Feature \"atmospheric_pressure(Pascal)\" of training and testing dataset are almost same","7e502400":"sns.boxplot(y='atmospheric_pressure(Pascal)',data=df)","ad5ff1ed":"sns.scatterplot(x='atmospheric_pressure(Pascal)',y='windmill_generated_power(kW\/h)',hue='cloud_level',data=df)","f420bc5d":"## Comparing the density plot of features in training and testing set\ncomparing_train_and_test_feature(df,df_test,'area_temperature(\u00b0C)')\n## Distribution of Feature \"area_temperature(\u00b0C)\" of training and testing dataset are almost same","e5a66ba5":"sns.boxplot(y='area_temperature(\u00b0C)',data=df)","44da6a0c":"sns.scatterplot(x='area_temperature(\u00b0C)',y='windmill_generated_power(kW\/h)',hue='cloud_level',data=df)","ae9e7f1a":"## Droping extreme outliers\nlow = df_cpy['area_temperature(\u00b0C)'] < 10\nlow = np.where(low)\ndf_cpy.drop(low[0],inplace=True)\ndf_cpy.index = range(df_cpy.shape[0])","66a471ce":"sns.scatterplot(x='area_temperature(\u00b0C)',y='windmill_generated_power(kW\/h)',hue='cloud_level',data=df_cpy)","04cbf877":"## Comparing the density plot of features in training and testing set\ncomparing_train_and_test_feature(df,df_test,'windmill_body_temperature(\u00b0C)')\n## Distribution of Feature \"windmill_body_temperature(\u00b0C)\" of training and testing dataset is little bit different\n## as density plot of windmill_body_temperature(\u00b0C) in testing dataset is broader than in training set","91418683":"sns.boxplot(y='windmill_body_temperature(\u00b0C)',data=df)","d0c197c9":"sns.scatterplot(x='windmill_body_temperature(\u00b0C)',y='windmill_generated_power(kW\/h)',hue='cloud_level',data=df)","7f3e7ba2":"df[df['windmill_body_temperature(\u00b0C)']< -90]","9a3704e1":"## Comparing the density plot of features in training and testing set\ncomparing_train_and_test_feature(df,df_test,'wind_direction(\u00b0)')\n## Distribution of Feature \"wind_direction(\u00b0)\" of training and testing dataset is almost same\n","677c7161":"sns.boxplot(y='wind_direction(\u00b0)',data=df)","59926444":"sns.scatterplot(x='wind_direction(\u00b0)',y='windmill_generated_power(kW\/h)',hue='cloud_level',data=df)","1b7f3e5a":"## Comparing the density plot of features in training and testing set\ncomparing_train_and_test_feature(df,df_test,'resistance(ohm)')\n## Distribution of Feature \"resistance(ohm)\" of training and testing dataset is almost same\n","8d3c0cbb":"sns.boxplot(y='resistance(ohm)',data=df)","bd15fa94":"sns.scatterplot(x='resistance(ohm)',y='windmill_generated_power(kW\/h)',hue='cloud_level',data=df)","21e329fb":"## Comparing the density plot of features in training and testing set\ncomparing_train_and_test_feature(df,df_test,'rotor_torque(N-m)')\n## Distribution of Feature \"rotor_torque(N-m)\" of training and testing dataset is almost same\n","b948f9bc":"sns.boxplot(y='rotor_torque(N-m)',data=df)","fadbedb9":"sns.scatterplot(x='rotor_torque(N-m)',y='windmill_generated_power(kW\/h)',hue='cloud_level',data=df)","e4f5f92a":"## Comparing the density plot of features in training and testing set\ncomparing_train_and_test_feature(df,df_test,'blade_length(m)')\n## Distribution of Feature \"blade_length(m)\" of training and testing dataset is almost same\n","d7e1d271":"sns.boxplot(y='blade_length(m)',data=df)","08f713e9":"sns.scatterplot(x='blade_length(m)',y='windmill_generated_power(kW\/h)',hue='cloud_level',data=df)","98d2c14a":"## Removing extreme outliers\nlow = df_cpy['blade_length(m)'] < -20\nlow = np.where(low)\ndf_cpy.drop(low[0],inplace=True)\ndf_cpy.index = range(df_cpy.shape[0])","5ae17aca":"plt.figure(figsize=(20,10))\nsns.scatterplot(x='blade_length(m)',y='windmill_generated_power(kW\/h)',hue='cloud_level',data=df_cpy)","432d927e":"## Comparing the density plot of features in training and testing set\ncomparing_train_and_test_feature(df,df_test,'blade_breadth(m)')\n## Distribution of Feature \"blade_breadth(m)\" of training and testing dataset is almost same\n","7624231d":"sns.boxplot(y='blade_breadth(m)',data=df)","64f5a4d5":"sns.scatterplot(x='blade_breadth(m)',y='windmill_generated_power(kW\/h)',hue='cloud_level',data=df)","abdbc28c":"## Comparing the density plot of features in training and testing set\ncomparing_train_and_test_feature(df,df_test,'windmill_height(m)')\n## Distribution of Feature \"windmill_height(m)\" of training and testing dataset is almost same\n","e0175472":"sns.boxplot(y='windmill_height(m)',data=df)","b3a00803":"sns.scatterplot(x='windmill_height(m)',y='windmill_generated_power(kW\/h)',hue='cloud_level',data=df)","d0ca03a0":"df_cpy.drop(['generator_temperature(\u00b0C)','windmill_body_temperature(\u00b0C)'],inplace=True,axis=1)\ndf_test.drop(['generator_temperature(\u00b0C)','windmill_body_temperature(\u00b0C)'],inplace=True,axis=1)","aca535c7":"df_cpy.info()","b54e5fa5":"df_cpy.describe()","98c15903":"df_cpy['gearbox_temperature(\u00b0C)'].fillna(df_cpy['gearbox_temperature(\u00b0C)'].mean(),inplace=True)\ndf_cpy['area_temperature(\u00b0C)'].fillna(df_cpy['area_temperature(\u00b0C)'].mean(),inplace=True)\ndf_cpy['rotor_torque(N-m)'].fillna(df_cpy['rotor_torque(N-m)'].mean(),inplace=True)\ndf_cpy['blade_length(m)'].fillna(df_cpy['blade_length(m)'].mean(),inplace=True)\ndf_cpy['blade_breadth(m)'].fillna(df_cpy['blade_breadth(m)'].mean(),inplace=True)\ndf_cpy['windmill_height(m)'].fillna(df_cpy['windmill_height(m)'].mean(),inplace=True)\ndf_cpy['cloud_level'].fillna(df_cpy['cloud_level'].mode()[0],inplace=True)\ndf_cpy['atmospheric_temperature(\u00b0C)'].fillna(df_cpy['atmospheric_temperature(\u00b0C)'].mean(),inplace=True)\ndf_cpy['atmospheric_pressure(Pascal)'].fillna(df_cpy['atmospheric_pressure(Pascal)'].mean(),inplace=True)\ndf_cpy['wind_speed(m\/s)'].fillna(df_cpy['wind_speed(m\/s)'].mean(),inplace=True)\ndf_cpy['shaft_temperature(\u00b0C)'].fillna(df_cpy['shaft_temperature(\u00b0C)'].mean(),inplace=True)\ndf_cpy['blades_angle(\u00b0)'].fillna(df_cpy['blades_angle(\u00b0)'].mean(),inplace=True)\ndf_cpy['engine_temperature(\u00b0C)'].fillna(df_cpy['engine_temperature(\u00b0C)'].mean(),inplace=True)\ndf_cpy['motor_torque(N-m)'].fillna(df_cpy['motor_torque(N-m)'].mean(),inplace=True)\ndf_cpy['wind_direction(\u00b0)'].fillna(df_cpy['wind_direction(\u00b0)'].mean(),inplace=True)","3ae79081":"df_test['gearbox_temperature(\u00b0C)'].fillna(df_test['gearbox_temperature(\u00b0C)'].mean(),inplace=True)\ndf_test['area_temperature(\u00b0C)'].fillna(df_test['area_temperature(\u00b0C)'].mean(),inplace=True)\ndf_test['rotor_torque(N-m)'].fillna(df_test['rotor_torque(N-m)'].mean(),inplace=True)\ndf_test['blade_length(m)'].fillna(df_test['blade_length(m)'].mean(),inplace=True)\ndf_test['blade_breadth(m)'].fillna(df_test['blade_breadth(m)'].mean(),inplace=True)\ndf_test['windmill_height(m)'].fillna(df_test['windmill_height(m)'].mean(),inplace=True)\ndf_test['cloud_level'].fillna(df_test['cloud_level'].mode()[0],inplace=True)\ndf_test['atmospheric_temperature(\u00b0C)'].fillna(df_test['atmospheric_temperature(\u00b0C)'].mean(),inplace=True)\ndf_test['atmospheric_pressure(Pascal)'].fillna(df_test['atmospheric_pressure(Pascal)'].mean(),inplace=True)\ndf_test['wind_speed(m\/s)'].fillna(df_test['wind_speed(m\/s)'].mean(),inplace=True)\ndf_test['shaft_temperature(\u00b0C)'].fillna(df_test['shaft_temperature(\u00b0C)'].mean(),inplace=True)\ndf_test['blades_angle(\u00b0)'].fillna(df_test['blades_angle(\u00b0)'].mean(),inplace=True)\ndf_test['engine_temperature(\u00b0C)'].fillna(df_test['engine_temperature(\u00b0C)'].mean(),inplace=True)\ndf_test['motor_torque(N-m)'].fillna(df_test['motor_torque(N-m)'].mean(),inplace=True)\ndf_test['wind_direction(\u00b0)'].fillna(df_test['wind_direction(\u00b0)'].mean(),inplace=True)","2e980e20":"df_cpy.info()","cbd57231":"df_cpy.dropna(how='any',axis=0,inplace=True)","ccba573f":"df_cpy.info()","025b19ba":"df_test.info()","0b9d9322":"## Feature \"cloud_level\" is categorical with 3 unique values\ndf_cpy['cloud_level'].replace(['Medium', 'Low', 'Extremely Low'],[2,1,0],inplace=True)\ndf_test['cloud_level'].replace(['Medium', 'Low', 'Extremely Low'],[2,1,0],inplace=True)","bbbcf3bf":"df_cpy['turbine_status'].value_counts()","6c0f0424":"## Using dummy variables for feature \"turbine_status\"\ndum = ['turbine_status']\ndf_dum = pd.get_dummies(df_cpy[dum])\ndf_test_dum = pd.get_dummies(df_test[dum])\ndf_dum","fcb2efbb":"df_cpy = pd.concat([df_cpy,df_dum],axis=1)\ndf_test = pd.concat([df_test,df_test_dum],axis=1)","0fb26afe":"## Converting the feature \"datetime\" into pandas datetime format\ndf_cpy['datetime'] = pd.to_datetime(df_cpy['datetime'])\ndf_test['datetime'] = pd.to_datetime(df_test['datetime'])","5ff23bdb":"## Extracting some informations for the feature \"datetime\" and making new features from it\ndf_cpy['dmonth'] = df_cpy['datetime'].dt.month\ndf_cpy['dday'] = df_cpy['datetime'].dt.day\ndf_cpy['ddayofweek'] = df_cpy['datetime'].dt.dayofweek\n\ndf_test['dmonth'] = df_test['datetime'].dt.month\ndf_test['dday'] = df_test['datetime'].dt.day\ndf_test['ddayofweek'] = df_test['datetime'].dt.dayofweek","bff72987":"corr = df_cpy.corr()\nplt.figure(figsize=(20,10))\nmask = np.zeros_like(corr,dtype=np.bool)\nmask[np.triu_indices_from(mask)] = True\nsns.heatmap(corr,mask=mask,annot=True)\nplt.show()","031e5dac":"corr = df_test.corr()\nplt.figure(figsize=(20,10))\nmask = np.zeros_like(corr,dtype=np.bool)\nmask[np.triu_indices_from(mask)] = True\nsns.heatmap(corr,mask=mask,annot=True)\nplt.show()","1ac6c83f":"df_cpy.info()","b835bc24":"X = df_cpy.drop(['tracking_id','datetime','windmill_generated_power(kW\/h)','turbine_status'],axis=1)\nY = df_cpy['windmill_generated_power(kW\/h)']\nX_test = df_test.drop(['tracking_id','datetime','turbine_status'],axis=1)\nprint(X.shape,Y.shape)\nprint(X_test.shape)","cb944294":"from sklearn.preprocessing import RobustScaler\nscaler = RobustScaler()\nX = scaler.fit_transform(X)\nX_test = scaler.transform(X_test)","78112cab":"from sklearn.model_selection import train_test_split\nx_train,x_test,y_train,y_test = train_test_split(X,Y,train_size=0.8,random_state=42)\nprint(x_train.shape,y_train.shape)\nprint(x_test.shape,y_test.shape)","a66c9d16":"from sklearn.linear_model import LinearRegression, Lasso\nfrom sklearn.metrics import r2_score\nlr = LinearRegression()\nlr.fit(x_train,y_train)\ny_train_pred = lr.predict(x_train)\ny_test_pred = lr.predict(x_test)\nprint(r2_score(y_true=y_train,y_pred=y_train_pred))\nprint(r2_score(y_true=y_test,y_pred=y_test_pred))","af2de2ec":"from sklearn.linear_model import RidgeCV\nridge_model = RidgeCV(scoring=\"r2\",\n                          alphas=[0.0001,0.0005,0.001,0.005,0.01,0.1,1.0,10],cv=5)\nridge_model.fit(x_train,y_train)\ny_train_pred = ridge_model.predict(x_train)\ny_test_pred = ridge_model.predict(x_test)\nprint(r2_score(y_true=y_train,y_pred=y_train_pred))\nprint(r2_score(y_true=y_test,y_pred=y_test_pred))","c9f4228a":"from sklearn.linear_model import LassoCV\nlasso_model = LassoCV(alphas=[0.0001,0.0005,0.001,0.005,0.01,0.1,1.0,10],cv=5)\nlasso_model.fit(x_train,y_train)\ny_train_pred = lasso_model.predict(x_train)\ny_test_pred = lasso_model.predict(x_test)\nprint(r2_score(y_true=y_train,y_pred=y_train_pred))\nprint(r2_score(y_true=y_test,y_pred=y_test_pred))","e6198d64":"from sklearn.linear_model import ElasticNetCV\nenet_model = ElasticNetCV(l1_ratio = [0.1, 0.5, 0.7, 0.9, 0.95, 0.99, 1],\n                    alphas = [1, 0.1, 0.01, 0.001, 0.0005], cv=5)\nenet_model.fit(x_train, y_train)\n\n# predict\ny_train_pred = enet_model.predict(x_train)\ny_test_pred = enet_model.predict(x_test)\nprint(r2_score(y_train,y_train_pred))\nprint(r2_score(y_test,y_test_pred))","6760d21a":"from sklearn.ensemble import ExtraTreesRegressor\nextra_model = ExtraTreesRegressor(criterion='mse', random_state=0, n_jobs=-1, \n                                min_samples_leaf=1, max_depth=20, \n                                min_samples_split=3, n_estimators=1000\n                               )\n\nextra_model.fit(x_train, y_train)\n\n# predict\ny_train_pred = extra_model.predict(x_train)\ny_test_pred = extra_model.predict(x_test)\nprint(r2_score(y_train,y_train_pred))\nprint(r2_score(y_test,y_test_pred))","fa395a7f":"from sklearn.ensemble import RandomForestRegressor\nrf = RandomForestRegressor()\nrf.fit(x_train,y_train)\ny_train_pred = rf.predict(x_train)\ny_test_pred = rf.predict(x_test)\nprint(r2_score(y_train,y_train_pred))\nprint(r2_score(y_test,y_test_pred))","f56e496b":"from sklearn.ensemble import GradientBoostingRegressor\ngb_model = GradientBoostingRegressor(criterion='mse',random_state=0,max_depth=5,\n                                     n_estimators=500,min_samples_split=2,min_samples_leaf=2)\ngb_model.fit(x_train,y_train)\ny_train_pred = gb_model.predict(x_train)\ny_test_pred = gb_model.predict(x_test)\nprint(r2_score(y_train,y_train_pred))\nprint(r2_score(y_test,y_test_pred))","874b76c1":"from xgboost import XGBRegressor\nxgb = XGBRegressor(n_estimators=500,max_depth=5,booster='gbtree',n_jobs=-1,learning_rate=0.1,reg_lambda=0.01,reg_alpha=0.3)\nxgb.fit(x_train,y_train)\ny_train_pred = xgb.predict(x_train)\ny_test_pred = xgb.predict(x_test)\nprint(r2_score(y_train,y_train_pred))\nprint(r2_score(y_test,y_test_pred))","b9a9aa7f":"import sklearn.base as skb\nclass MixModel(skb.BaseEstimator,skb.RegressorMixin,skb.TransformerMixin):\n    def __init__(self,algs):\n        self.algs = algs\n    def fit(self,X,y):\n        self.algs_ = [skb.clone(x) for x in self.algs]\n        for alg in self.algs_:\n            alg.fit(X,y)\n        return self\n    def predict(self,X):\n        predictions = np.column_stack([\n            stacked_model.predict(X) for stacked_model in self.algs_\n        ])\n        return np.mean(predictions,axis=1)","f1b6d67f":"## Using mixed model of random forest, gradient boosting and XGB Regressor\nmixed_model = MixModel(algs = [xgb,rf,gb_model])\nmixed_model.fit(x_train, y_train)\ny_train_pred = mixed_model.predict(x_train)\ny_test_pred = mixed_model.predict(x_test)\nprint(r2_score(y_train,y_train_pred))\nprint(r2_score(y_test,y_test_pred))","b2389213":"df_test = pd.read_csv('..\/input\/a-fine-windy-day-hackerearth-ml-challenge\/test_data.csv')","bf4431e5":"df_sub = df_test[['tracking_id','datetime']]","7ab4e94b":"results = mixed_model.predict(X_test)","10c8482e":"results","7329ed10":"df_sub['windmill_generated_power(kW\/h)'] = results","2a710e78":"df_sub.to_csv('.\/sub.csv',header=True,index=False)\ndf_sub","3302d001":"From the above density plot of feature \"rotor_torque(N-m)\", we found that the distribution of \"rotor_torque(N-m)\" is almost same in training and testing dataset, so we are not changing anything in it.\n","9a980214":"From the above density plot of feature \"blades_angle(\u00b0)\", we found that the distribution of \"blades_angle(\u00b0)\" is almost same in training and testing dataset, so we are not changing anything in it.\n","0a07c33b":"### Ridge Regression","ac5e9c22":"After removing extreme outliers of feature \"gearbox_temperature(\u00b0C)\", it's scatterplot is shown below.","2f645dd0":"#### atmospheric_temperature(\u00b0C)","b7706d36":"# Task 3 - Data preparation","80d5e5ae":"#### blades_angle(\u00b0)","2c9aa93a":"#### wind_speed(m\/s)","2785b214":"#### windmill_height(m)","4980ba7e":"After removing extreme outliers, the scatterplot of feature \"blade_length(m)\" is shown","3149f130":"# Task 2 - Exploratory Data Analysis (EDA)","fbe72c61":"#### area_temperature(\u00b0C)","8ef52e21":"#### rotor_torque(N-m)","96cadd85":"#### blade_breadth(m)","697906e1":"## Now let us begin with analysing each and every feature","9398923c":"From the above plot, features \"motor_torque(N-m)\" and \"generator_temperature(\u00b0C)\" are highly correlated and therefore we will be dropping one of them in the end.","4d42cc09":"### GradientBoostingRegressor","ac998147":"From the above density plot of feature \"shaft_temperature(\u00b0C)\", we found that the distribution of \"shaft_temperature(\u00b0C)\" is almost same in training and testing dataset, so we are not changing anything in it.","21af180c":"### Linear Regression","59185060":"## Mixed model of three different models","b36a6bd0":"### ExtraTreesRegressor","c9bf8dc8":"From the above density plot of feature \"atmospheric_pressure(Pascal)\", we found that the distribution of \"atmospheric_pressure(Pascal)\" is almost same in training and testing dataset, so we are not changing anything in it.\n","fff5da28":"#### engine_temperature(\u00b0C)","092a80de":"From the above density plot of feature \"blade_breadth(m)\", we found that the distribution of \"blade_breadth(m)\" is almost same in training and testing dataset, so we are not changing anything in it.\n","489465be":"#### shaft_temperature(\u00b0C)","7eb615c6":"### Lasso Regression","e5dada65":"#### gearbox_temperature(\u00b0C)","f0949508":"After dropping the extreme outliers, scatterplot of feature \"area_temperature(\u00b0C)\" is shown below","53f14637":"# Task 5 - Test Evaluation and Submission","47f6688f":"# Task 1 - Importing libraries and dataset","af788bd5":"Since, there are missing values in testing data as well so we have to make arrangments for them and therefore\nreplacing the missing values using statistical tools like mean, median and mode","05326a0a":"#### generator_temperature(\u00b0C)","a3ca0247":"#### blade_length(m)","013ebcf4":"### ElasticNet","09d54f7c":"This feature \"windmill_body_temperature(\u00b0C)\" doesn't have same distribution in training and testing set and it is not much correlated with targetFeature as well. So, we concluded to drop it in the end.","b3a88296":"From the above density plot of feature \"resistance(ohm)\", we found that the distribution of \"resistance(ohm)\" is almost same in training and testing dataset, so we are not changing anything in it.\n","cb0faef8":"### RandomForestRegressor ","440bfb40":"From the above density plot of feature \"motor_torque(N-m)\", we found that the distribution of \"motor_torque(N-m)\" is almost same in training and testing dataset, so we are not changing anything in it.\n","21de2512":"From the above density plot of feature \"generator_temperature(\u00b0C)\", we found that the distribution of \"generator_temperature(\u00b0C)\" is almost same in training and testing dataset, so we are not changing anything in it.\n","dc75e23d":"From the above density plot of feature \"windmill_height(m)\", we found that the distribution of \"windmill_height(m)\" is almost same in training and testing dataset, so we are not changing anything in it.\n","05942fcd":"### XGBRegressor","c3f3fd4e":"#### windmill_body_temperature(\u00b0C)","93ab16ec":"#### motor_torque(N-m)","dc7d2d73":"#### resistance(ohm)","0f95cd3e":"# Last Notes\nThis dataset is not based on the real world scenario and is synthetically generated as some of the features like blade_width, wind_speed and so on, have negative values which are meaningless.\nThe testing set is also the subset of training set as both are having almost same distributions among almost every feature. By removing all outliers from the training set, its distribution will differ highly from the testing set which results in less score on testing set and therefore only extreme outliers are removed by us, so that the distribution remains almost same.\n\nHere, we used models like Linear Regression ,Lasso Regression, Ridge Regression, Random Forest Regressor, ExtraTress Regressor, Gradient Boosting Regressor and XGB Regressor with manual hyper parameter tunning. For me, Random Forest , Gradient Boosting and XGB Regressor was performing best and hence, used a mixed model of these three models with brings me the best results.\nI would suggest to do more feature engineering and hyper parameter tunning with different models may bring excellent results.\n\nIf you like my work, show your appreciation with an upvote and share this notebook.\n\nThank You............!!!","4466948c":"From the above density plot of feature \"wind_direction(\u00b0)\", we found that the distribution of \"wind_direction(\u00b0)\" is almost same in training and testing dataset, so we are not changing anything in it.\n","3dd25a64":"#### wind_direction(\u00b0)","98985155":"From the above density plot of feature \"atmospheric_temperature(\u00b0C)\", we found that the distribution of \"atmospheric_temperature(\u00b0C)\" is almost same in training and testing dataset, so we are not changing anything in it.","bc86dbda":"After dropping extreme outliers, the scatterplot of \"engine_temperature(\u00b0C)\" is shown.","a6611bcc":"#### atmospheric_pressure(Pascal)","7ab96022":"# Task 4 - Data modelling"}}