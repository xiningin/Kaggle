{"cell_type":{"131700d5":"code","0c2745a5":"code","c8f36c51":"code","eff61dc9":"code","17e94a22":"code","aef2e705":"code","00668663":"code","7bea69a8":"code","535e4ad2":"code","fafbb3c2":"code","50b66b23":"code","eca6dcfa":"code","e83d18f1":"code","dc1c64ef":"code","d8030fc1":"code","a7d21001":"code","f5d47f76":"code","d5d41248":"code","3064cd20":"code","1d95f223":"code","63a05fe6":"code","d32ebea2":"code","248816ba":"code","d5adf5e0":"code","56a9fb17":"code","9a73079e":"code","00219b64":"code","4c36d466":"code","d3705792":"code","18ec855b":"code","fd705da2":"code","64eb3c3d":"code","df6afe60":"code","e3dd4ceb":"code","9e646cb6":"markdown"},"source":{"131700d5":"import os\nimport numpy as np\nimport pandas as pd\nimport cv2\nfrom IPython.display import Image, display \nfrom PIL import Image","0c2745a5":"print('the dictionaries present are', os.listdir('\/kaggle\/input\/asl-rgb-depth-fingerspelling-spelling-it-out\/dataset5\/'))\ndicts = os.listdir('\/kaggle\/input\/asl-rgb-depth-fingerspelling-spelling-it-out\/dataset5\/')","c8f36c51":"y_train = []\nX_train = []\ny_val = []\nX_val = []\ny_test = []\nX_test = []\npath = '\/kaggle\/input\/asl-rgb-depth-fingerspelling-spelling-it-out\/dataset5\/'+ dicts[0]\nlabels = os.listdir(path)\nfor label in labels:\n    i = 0\n    img_names = os.listdir(path+'\/'+label)\n    for img_name in img_names:\n        if img_name.startswith('color'):\n            img = cv2.imread(path+'\/'+label+ '\/'+img_name)\n            img_gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n            img_resized = cv2.resize(img_gray, (32,32))\n            X_train.append(np.array(img_resized))\n            y_train.append(label)\n            i = i+1\n            if i > 750:\n                break\npath = '\/kaggle\/input\/asl-rgb-depth-fingerspelling-spelling-it-out\/dataset5\/'+ dicts[1]\nlabels = os.listdir(path)\nfor label in labels:\n    i = 0\n    img_names = os.listdir(path+'\/'+label)\n    for img_name in img_names:\n        if img_name.startswith('color'):\n            img = cv2.imread(path+'\/'+label+ '\/'+img_name)\n            img_gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n            img_resized = cv2.resize(img_gray, (32,32))\n            X_val.append(np.array(img_resized))\n            y_val.append(label)\n            i = i+1\n            if i > 250:\n                break\npath = '\/kaggle\/input\/asl-rgb-depth-fingerspelling-spelling-it-out\/dataset5\/'+ dicts[2]\nlabels = os.listdir(path)\nfor label in labels:\n    i = 0\n    img_names = os.listdir(path+'\/'+label)\n    for img_name in img_names:\n        if img_name.startswith('color'):\n            img = cv2.imread(path+'\/'+label+ '\/'+img_name)\n            img_gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n            img_resized = cv2.resize(img_gray, (32,32))\n            X_test.append(np.array(img_resized))\n            y_test.append(label)\n            i = i+1\n            if i >250:\n                break","eff61dc9":"X_train = np.array(X_train)\ny_train = np.array(y_train)\nX_val = np.array(X_val)\ny_val = np.array(y_val)\nX_test = np.array(X_test)\ny_test = np.array(y_test)\nprint('the sizes of train data are:', X_train.shape, 'and', y_train.shape, '\\n', 'the sizes of test data are',\n      X_test.shape, 'and', y_test.shape, '\\n', 'the sizes of validation data are:', X_val.shape, 'and', y_val.shape)","17e94a22":"print('the number of labels in y_train', len(np.unique(y_train)), '\\n',\n     'the number of labels in y_test', len(np.unique(y_test)), '\\n',\n     'the number of labels in y_val', len(np.unique(y_val)))","aef2e705":"X_train_std = X_train.astype('float32')\/255\nX_test_std =X_test.astype('float32')\/255\nX_val_std = X_val.astype('float32')\/255","00668663":"X_train_std.shape[1:]","7bea69a8":"from sklearn.preprocessing import LabelEncoder\nfrom keras.utils import to_categorical\ny_train_coded = LabelEncoder().fit_transform(y_train)\ny_train_coded = to_categorical(y_train_coded)\ny_test_coded = LabelEncoder().fit_transform(y_test)\ny_test_coded = to_categorical(y_test_coded)\ny_val_coded = LabelEncoder().fit_transform(y_val)\ny_val_coded = to_categorical(y_val_coded)","535e4ad2":"from keras import models, layers","fafbb3c2":"#using max pooling(size = 2,2) subsampling layers:\nmodel_maxpool_2 = models.Sequential()\nmodel_maxpool_2.add(layers.Conv2D(filters = 32, kernel_size = (5,5), activation = 'relu', input_shape = (32,32,1)))\nmodel_maxpool_2.add(layers.MaxPool2D(pool_size = (2,2)))\nmodel_maxpool_2.add(layers.Conv2D(filters = 64, kernel_size = (3,3), activation = 'relu'))\nmodel_maxpool_2.add(layers.MaxPool2D(pool_size = (2,2)))\nmodel_maxpool_2.add(layers.Dropout(rate = 0.25))\nmodel_maxpool_2.add(layers.Flatten())\nmodel_maxpool_2.add(layers.Dense(256, activation = 'relu'))\nmodel_maxpool_2.add(layers.Dropout(rate = 0.5))\nmodel_maxpool_2.add(layers.Dense(24, activation = 'softmax'))","50b66b23":"# compile the model\nmodel_maxpool_2.compile(loss = 'categorical_crossentropy',\n                        optimizer = 'adam',\n                        metrics = ['accuracy'])","eca6dcfa":"#let's take only 2000 samples of train data, 500 samples of val data and 200 samples of test data\nr = np.arange(X_train_std.shape[0])\nnp.random.seed(42)\nnp.random.shuffle(r)\nX_train = X_train_std[r]\nX_train_data = X_train[:2000,:,:]\nX_train_data = X_train_data.reshape((2000, 32, 32, 1))\ny_train = y_train_coded[r]\ny_train_data = y_train[:2000]\nX_test_data = X_train[2001:2501,:,:]\nX_test_data = X_test_data.reshape((500, 32, 32, 1))\ny_test_data = y_train[2001:2501]\nX_val_data = X_train[2501:3001,:,:]\nX_val_data = X_val_data.reshape((500, 32, 32, 1))\ny_val_data = y_train[3001:3501]","e83d18f1":"#fitting the model\nhistory = model_maxpool_2.fit(X_train_data, y_train_data, epochs=25, validation_data=(X_val_data, y_val_data))","dc1c64ef":"#Display of the accuracy and the loss values\nimport matplotlib.pyplot as plt\n\nplt.figure(figsize = (8,8))\nplt.plot(history.history['accuracy'], label='training accuracy')\nplt.plot(history.history['val_accuracy'], label='val accuracy')\nplt.plot(history.history['loss'], label='training loss')\nplt.plot(history.history['val_loss'], label='val loss')\nplt.title('Loss\/accuracy')\nplt.xlabel('epochs')\nplt.ylabel('loss')\nplt.legend()\nplt.show()","d8030fc1":"y_pred_maxpool_2 = model_maxpool_2.predict_classes(X_test_data)\ny_test_maxpool_2 = np.argmax(y_test_data, axis = 1)\nfrom sklearn.metrics import accuracy_score\naccuracy_maxpool_2 = accuracy_score(y_pred_maxpool_2,y_test_maxpool_2)\nprint('the accuracy obtained on the test set is:', accuracy_maxpool_2)","a7d21001":"from sklearn.metrics import classification_report\nprint(classification_report(y_test_maxpool_2, y_pred_maxpool_2))","f5d47f76":"#Let's see how average pooling works on the data instead of maxpooling\nmodel_avgpool_2 = models.Sequential()\nmodel_avgpool_2.add(layers.Conv2D(filters = 32, kernel_size = (5,5), activation = 'relu', input_shape = (32,32,1)))\nmodel_avgpool_2.add(layers.AveragePooling2D(pool_size = (2,2)))\nmodel_avgpool_2.add(layers.Conv2D(filters = 64, kernel_size = (3,3), activation = 'relu'))\nmodel_avgpool_2.add(layers.AveragePooling2D(pool_size = (2,2)))\nmodel_avgpool_2.add(layers.Dropout(rate = 0.25))\nmodel_avgpool_2.add(layers.Flatten())\nmodel_avgpool_2.add(layers.Dense(256, activation = 'relu'))\nmodel_avgpool_2.add(layers.Dropout(rate = 0.5))\nmodel_avgpool_2.add(layers.Dense(24, activation = 'softmax'))","d5d41248":"# compile the model\nmodel_avgpool_2.compile(loss = 'categorical_crossentropy',\n                        optimizer = 'adam',\n                        metrics = ['accuracy'])","3064cd20":"#fitting the model\nhistory_avgpool_2 = model_avgpool_2.fit(X_train_data, y_train_data, epochs=25, validation_data=(X_val_data, y_val_data))","1d95f223":"#Display of the accuracy and the loss values\nplt.figure(figsize = (8,8))\nplt.plot(history_avgpool_2.history['accuracy'], label='training accuracy')\nplt.plot(history_avgpool_2.history['val_accuracy'], label='val accuracy')\nplt.plot(history_avgpool_2.history['loss'], label='training loss')\nplt.plot(history_avgpool_2.history['val_loss'], label='val loss')\nplt.title('Loss\/accuracy')\nplt.xlabel('epochs')\nplt.ylabel('loss')\nplt.legend()\nplt.show()","63a05fe6":"y_pred_avgpool_2 = model_avgpool_2.predict_classes(X_test_data)\ny_test_avgpool_2 = np.argmax(y_test_data, axis = 1)\nfrom sklearn.metrics import accuracy_score\naccuracy_avgpool_2 = accuracy_score(y_pred_avgpool_2,y_test_avgpool_2)\nprint('the accuracy obtained on the test set is:', accuracy_avgpool_2)","d32ebea2":"from sklearn.metrics import classification_report\nprint(classification_report(y_test_avgpool_2, y_pred_avgpool_2))","248816ba":"#What if the kernel size in the convolution layers is increased?\nmodel_kernel_up = models.Sequential()\nmodel_kernel_up.add(layers.Conv2D(filters = 32, kernel_size = (8,8), activation = 'relu', input_shape = (32,32,1)))\nmodel_kernel_up.add(layers.AveragePooling2D(pool_size = (2,2)))\nmodel_kernel_up.add(layers.Conv2D(filters = 64, kernel_size = (5,5), activation = 'relu'))\nmodel_kernel_up.add(layers.AveragePooling2D(pool_size = (2,2)))\nmodel_kernel_up.add(layers.Dropout(rate = 0.25))\nmodel_kernel_up.add(layers.Flatten())\nmodel_kernel_up.add(layers.Dense(256, activation = 'relu'))\nmodel_kernel_up.add(layers.Dropout(rate = 0.5))\nmodel_kernel_up.add(layers.Dense(24, activation = 'softmax'))","d5adf5e0":"# compile the model\nmodel_kernel_up.compile(loss = 'categorical_crossentropy',\n                        optimizer = 'adam',\n                        metrics = ['accuracy'])","56a9fb17":"#fitting the model\nhistory_kernel_up = model_kernel_up.fit(X_train_data, y_train_data, epochs=25, validation_data=(X_val_data, y_val_data))","9a73079e":"#Display of the accuracy and the loss values\nplt.figure(figsize = (8,8))\nplt.plot(history_kernel_up.history['accuracy'], label='training accuracy')\nplt.plot(history_kernel_up.history['val_accuracy'], label='val accuracy')\nplt.plot(history_kernel_up.history['loss'], label='training loss')\nplt.plot(history_kernel_up.history['val_loss'], label='val loss')\nplt.title('Loss\/accuracy')\nplt.xlabel('epochs')\nplt.ylabel('loss')\nplt.legend()\nplt.show()","00219b64":"y_pred_kernel_up = model_kernel_up.predict_classes(X_test_data)\ny_test_kernel_up = np.argmax(y_test_data, axis = 1)\nfrom sklearn.metrics import accuracy_score\naccuracy_kernel_up = accuracy_score(y_pred_kernel_up,y_test_kernel_up)\nprint('the accuracy obtained on the test set is:', accuracy_kernel_up)","4c36d466":"from sklearn.metrics import classification_report\nprint(classification_report(y_test_kernel_up, y_pred_kernel_up))","d3705792":"#what the effect if the pooling size is increased?\n\nmodel_avgpool_5 = models.Sequential()\nmodel_avgpool_5.add(layers.Conv2D(filters = 32, kernel_size = (5,5), activation = 'relu', input_shape = (32,32,1)))\nmodel_avgpool_5.add(layers.AveragePooling2D(pool_size = (3,3)))\nmodel_avgpool_5.add(layers.Conv2D(filters = 64, kernel_size = (3,3), activation = 'relu'))\nmodel_avgpool_5.add(layers.AveragePooling2D(pool_size = (3,3)))\nmodel_avgpool_5.add(layers.Dropout(rate = 0.25))\nmodel_avgpool_5.add(layers.Flatten())\nmodel_avgpool_5.add(layers.Dense(256, activation = 'relu'))\nmodel_avgpool_5.add(layers.Dropout(rate = 0.5))\nmodel_avgpool_5.add(layers.Dense(24, activation = 'softmax'))","18ec855b":"# compile the model\nmodel_avgpool_5.compile(loss = 'categorical_crossentropy',\n                        optimizer = 'adam',\n                        metrics = ['accuracy'])","fd705da2":"#fitting the model\nhistory_avgpool_5 = model_avgpool_5.fit(X_train_data, y_train_data, epochs=25, validation_data=(X_val_data, y_val_data))","64eb3c3d":"#Display of the accuracy and the loss values\nplt.figure(figsize = (8,8))\nplt.plot(history_avgpool_5.history['accuracy'], label='training accuracy')\nplt.plot(history_avgpool_5.history['val_accuracy'], label='val accuracy')\nplt.plot(history_avgpool_5.history['loss'], label='training loss')\nplt.plot(history_avgpool_5.history['val_loss'], label='val loss')\nplt.title('Loss\/accuracy')\nplt.xlabel('epochs')\nplt.ylabel('loss')\nplt.legend()\nplt.show()","df6afe60":"y_pred_avgpool_5 = model_avgpool_5.predict_classes(X_test_data)\ny_test_avgpool_5 = np.argmax(y_test_data, axis = 1)\nfrom sklearn.metrics import accuracy_score\naccuracy_avgpool_5 = accuracy_score(y_pred_avgpool_5,y_test_avgpool_5)\nprint('the accuracy obtained on the test set is:', accuracy_avgpool_5)","e3dd4ceb":"from sklearn.metrics import classification_report\nprint(classification_report(y_test_avgpool_5, y_pred_avgpool_5))","9e646cb6":"1) From the above results we can confirm that Average pooling of layers can make signicant change in the output accuracy, but the time for training is increased.\n2) The kernel size increase doesnt mean their will be a increase in efficieny in output, but it makes computation fast, at the cost of low accuracy.\n3) Increasing the subsampling size(or pooling size) is always a bad idea. Highly lowers the efficiency of the model"}}