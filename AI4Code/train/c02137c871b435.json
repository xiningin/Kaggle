{"cell_type":{"af17582d":"code","8a452f8a":"code","1b420164":"code","3379eb35":"code","18781dc9":"code","e04b0061":"code","7e6ecf31":"code","30d4bb74":"code","422bec67":"code","8b7acad1":"code","1e5de5cd":"code","dbaf7016":"code","aceffcdd":"code","c2291c58":"code","631591a7":"code","0f4ae1cc":"code","a13abc53":"code","cdc4620a":"code","64ef6be1":"code","a5496dda":"code","b73e452b":"code","48b0a3dc":"code","6f3988d0":"code","bb497b20":"code","4aae33d8":"code","ef319019":"code","56d06afd":"code","5a2d13e1":"code","5d1e0917":"code","ff049ebe":"code","e0c45ad0":"code","c909e5e0":"code","d5bedcc8":"code","e6c18559":"code","541cbb63":"code","acae711c":"code","0c13b627":"code","36023217":"code","df6cb364":"code","80534c21":"code","5a7d5fcc":"code","a1f46f6a":"code","84729794":"code","0d79e0f9":"code","e50358f2":"code","5dba744d":"code","b6d4cf23":"markdown","2429ba60":"markdown","988cf5e6":"markdown","186e8469":"markdown","677fbed9":"markdown","7ee2791a":"markdown","bc400aa6":"markdown","6fad3da4":"markdown","96781680":"markdown","1dc756e9":"markdown","2ef92321":"markdown"},"source":{"af17582d":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","8a452f8a":"import base64\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport re\nimport string\n\n# Plotly imports\nimport plotly.offline as py\npy.init_notebook_mode(connected=True)\nimport plotly.graph_objs as go\nimport plotly.tools as tls\n\n# Other imports\nfrom collections import Counter\n# from scipy.misc import imread\nfrom sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\nfrom sklearn.decomposition import NMF, LatentDirichletAllocation\nfrom matplotlib import pyplot as plt\n%matplotlib inline\n\nfrom sklearn.ensemble import RandomForestClassifier\nfrom lightgbm.sklearn import LGBMClassifier\nfrom xgboost.sklearn import XGBClassifier\n\nfrom wordcloud import WordCloud\n\n#To ignore warnings\nimport warnings\nwarnings.filterwarnings('ignore')","1b420164":"file = r'\/kaggle\/input\/janatahack-independence-day-2020-ml-hackathon\/'\ntrain_df = pd.read_csv(file+'train.csv')\ntest_df = pd.read_csv(file+'test.csv')\nsub_df = pd.read_csv(file+'sample_submission_UVKGLZE.csv')","3379eb35":"train_df.head()","18781dc9":"test_df.head()","e04b0061":"sub_df.head()","7e6ecf31":"print('size of train data',train_df.shape)\nprint('size of test data',test_df.shape)\nprint('size of sub data',sub_df.shape)","30d4bb74":"train_df.columns","422bec67":"train_df.sample(5)","8b7acad1":"cols_target = ['Computer Science', 'Physics', 'Mathematics',\n       'Statistics', 'Quantitative Biology', 'Quantitative Finance']","1e5de5cd":"# check missing values in numeric columns\ntrain_df.describe()","dbaf7016":"unlabelled_in_all = train_df[(train_df['Computer Science']!=1) & (train_df['Physics']!=1) & (train_df['Mathematics']!=1) & \n                            (train_df['Statistics']!=1) & (train_df['Quantitative Biology']!=1) & (train_df['Quantitative Finance']!=1)]\nprint('Percentage of unlabelled comments is ', len(unlabelled_in_all)\/len(train_df)*100)","aceffcdd":"# check for any 'null' comment\nno_comment = train_df[train_df['TITLE'].isnull()]\nlen(no_comment)","c2291c58":"# check for any 'null' comment\nno_comment = train_df[train_df['ABSTRACT'].isnull()]\nlen(no_comment)","631591a7":"no_comment = test_df[test_df['ABSTRACT'].isnull()]\nno_comment","0f4ae1cc":"no_comment = test_df[test_df['TITLE'].isnull()]\nno_comment","a13abc53":"# let's see the total rows in train, test data and the numbers for the various categories\nprint('Total rows in test is {}'.format(len(test_df)))\nprint('Total rows in train is {}'.format(len(train_df)))\nprint(train_df[cols_target].sum())","cdc4620a":"# Here is the total number of samples belongs to each class\nx = train_df.iloc[:,3:].sum()\nprint('total number of comment:',len(train_df),'\\n','samples belongs to each class','\\n',x)\n\nplt.figure(figsize=(15,5))\nsns.barplot(x.index,x.values)\nplt.xticks(rotation=90)\nplt.title('class distribution')\nplt.show()","64ef6be1":"y = train_df.corr()\nplt.figure(figsize=(8,8))\nsns.heatmap(y,annot=True,center=True,square=True)\nplt.title('heatmap showing correlation between classes')\nplt.show()\n#Here i intentionally included seventh class which we created","a5496dda":"# Let's look at the character length for the rows in the training data and record these\ntrain_df['TITLE_char_length'] = train_df['TITLE'].apply(lambda x: len(str(x)))\ntrain_df['ABSTRACT_char_length'] = train_df['ABSTRACT'].apply(lambda x: len(str(x)))","b73e452b":"# look at the histogram plot for text length\nsns.set()\ntrain_df['TITLE_char_length'].hist()\nplt.show()","48b0a3dc":"# look at the histogram plot for text length\nsns.set()\ntrain_df['ABSTRACT_char_length'].hist()\nplt.show()","6f3988d0":"# Let's look at the character length for the rows in the training data and record these\ntest_df['TITLE_char_length'] = test_df['TITLE'].apply(lambda x: len(str(x)))\ntest_df['ABSTRACT_char_length'] = test_df['ABSTRACT'].apply(lambda x: len(str(x)))","bb497b20":"train_df['comment_text'] = train_df['TITLE'] + train_df['ABSTRACT']\ntest_df['comment_text'] = test_df['TITLE'] + test_df['ABSTRACT']","4aae33d8":"from nltk.corpus import stopwords\nstop = stopwords.words('english')","ef319019":"word_counter = {}\n\n\ndef clean_text(text):\n    text = re.sub('[{}]'.format(string.punctuation), ' ', text.lower())\n    return ' '.join([word for word in text.split() if word not in (stop)])\n\nfor categ in cols_target:\n    d = Counter()\n    train_df[train_df[categ] == 1]['comment_text'].apply(lambda t: d.update(clean_text(t).split()))\n    word_counter[categ] = pd.DataFrame.from_dict(d, orient='index')\\\n                                        .rename(columns={0: 'count'})\\\n                                        .sort_values('count', ascending=False)\n    \nfor w in word_counter:\n    wc = word_counter[w]\n\n    wordcloud = WordCloud(\n          background_color='black',\n          max_words=200,\n          max_font_size=100, \n          random_state=4561\n         ).generate_from_frequencies(wc.to_dict()['count'])\n\n    fig = plt.figure(figsize=(12, 8))\n    plt.title(w)\n    plt.imshow(wordcloud)\n    plt.axis('off')\n\n    plt.show()","56d06afd":"train_df['comment_text'] = train_df['comment_text'].map(lambda com : clean_text(com))\n\ntest_df['comment_text'] = test_df['comment_text'].map(lambda com : clean_text(com))","5a2d13e1":"# train_df = train_df.drop('char_length',axis=1)\nX = train_df.comment_text\ntest_X = test_df.comment_text\nprint(X.shape, test_X.shape)","5d1e0917":"# import and instantiate TfidfVectorizer\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvect = TfidfVectorizer(max_features=5000,stop_words='english')\nvect","ff049ebe":"# learn the vocabulary in the training data, then use it to create a document-term matrix\nX_dtm = vect.fit_transform(X)\n# examine the document-term matrix created from X_train\nX_dtm","e0c45ad0":"# transform the test data using the earlier fitted vocabulary, into a document-term matrix\ntest_X_dtm = vect.transform(test_X)\n# examine the document-term matrix from X_test\ntest_X_dtm","c909e5e0":"# import and instantiate the Logistic Regression model\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score\nlogreg = LogisticRegression(C=12.0)\n\n# create submission file\nsubmission_binary = sub_df.copy()\n\nfor label in cols_target:\n    print('... Processing {}'.format(label))\n    y = train_df[label]\n    # train the model using X_dtm & y\n    logreg.fit(X_dtm, y)\n    # compute the training accuracy\n    y_pred_X = logreg.predict(X_dtm)\n    print('Training accuracy is {}'.format(accuracy_score(y, y_pred_X)))\n    # compute the predicted probabilities for X_test_dtm\n    test_y_prob = logreg.predict_proba(test_X_dtm)[:,1]\n    submission_binary[label] = test_y_prob","d5bedcc8":"submission_binary.head()","e6c18559":"for col in cols_target:\n    submission_binary[col] = submission_binary[col].apply(lambda x: 1 if x >= 0.4 else 0)","541cbb63":"submission_binary.head()","acae711c":"submission_binary.to_csv('log_reg_baseline.csv', index=False)","0c13b627":"# import and instantiate the Logistic Regression model\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score\nlogreg = LogisticRegression(C=12.0)\nxgb=XGBClassifier(max_depth=4,base_score=0.5,learning_rate=0.1,n_estimators=350)\n# create submission file\nsubmission_binary = sub_df.copy()\n\nfor label in cols_target:\n    print('... Processing {}'.format(label))\n    y = train_df[label]\n    # train the model using X_dtm & y\n    xgb.fit(X_dtm, y)\n    # compute the training accuracy\n    y_pred_X = xgb.predict(X_dtm)\n    print('Training accuracy is {}'.format(accuracy_score(y, y_pred_X)))\n    # compute the predicted probabilities for X_test_dtm\n    test_y_prob = xgb.predict_proba(test_X_dtm)[:,1]\n    submission_binary[label] = test_y_prob","36023217":"submission_binary.head()","df6cb364":"for col in cols_target:\n    submission_binary[col] = submission_binary[col].apply(lambda x: 1 if x >= 0.5 else 0)","80534c21":"submission_binary.to_csv('lgbm_baseline.csv', index=False)","5a7d5fcc":"#  create submission file\nsubmission_chains = sub_df.copy()\n\n# create a function to add features\ndef add_feature(X, feature_to_add):\n    '''\n    Returns sparse feature matrix with added feature.\n    feature_to_add can also be a list of features.\n    '''\n    from scipy.sparse import csr_matrix, hstack\n    return hstack([X, csr_matrix(feature_to_add).T], 'csr')","a1f46f6a":"for label in cols_target:\n    print('... Processing {}'.format(label))\n    y = train_df[label]\n    # train the model using X_dtm & y\n    logreg.fit(X_dtm,y)\n    # compute the training accuracy\n    y_pred_X = logreg.predict(X_dtm)\n    print('Training Accuracy is {}'.format(accuracy_score(y,y_pred_X)))\n    # make predictions from test_X\n    test_y = logreg.predict(test_X_dtm)\n    test_y_prob = logreg.predict_proba(test_X_dtm)[:,1]\n    submission_chains[label] = test_y_prob\n    # chain current label to X_dtm\n    X_dtm = add_feature(X_dtm, y)\n    print('Shape of X_dtm is now {}'.format(X_dtm.shape))\n    # chain current label predictions to test_X_dtm\n    test_X_dtm = add_feature(test_X_dtm, test_y)\n    print('Shape of test_X_dtm is now {}'.format(test_X_dtm.shape))\n","84729794":"submission_chains.head()","0d79e0f9":"for col in cols_target:\n    submission_chains[col] = submission_chains[col].apply(lambda x: 1 if x >= 0.5 else 0)","e50358f2":"submission_chains.head()","5dba744d":"submission_binary.to_csv('log_reg_baseline_chains.csv', index=False)","b6d4cf23":"One way to approach a multi-label classification problem is to transform the problem into separate single-class classifier problems. This is known as 'problem transformation'. There are three methods:\n\nBinary Relevance. This is probably the simplest which treats each label as a separate single classification problems. The key assumption here though, is that there are no correlation among the various labels.\n\nClassifier Chains. In this method, the first classifier is trained on the input X. Then the subsequent classifiers are trained on the input X and all previous classifiers' predictions in the chain. This method attempts to draw the signals from the correlation among preceding target variables.\n\n\nLabel Powerset. This method transforms the problem into a multi-class problem where the multi-class labels are essentially all the unique label combinations. In our case here, where there are six labels, Label Powerset would in effect turn this into a 2^6 or 64-class problem. {Thanks Joshua for pointing out.}","2429ba60":"### Vectorize the data\u00b6","988cf5e6":"### Refernces\/Credits:\n\nAs this hackathon is similar to the contest conducted in Kaggle - I referred to these kernels.\n\nhttps:\/\/www.kaggle.com\/clinma\/eda-toxic-comment-classification-challenge\n\nhttps:\/\/www.kaggle.com\/c\/jigsaw-unintended-bias-in-toxicity-classification\/discussion\/100661\n\nhttps:\/\/www.kaggle.com\/rhodiumbeng\/classifying-multi-label-comments-0-9741-lb","186e8469":"### Solving a multi-label classification problem","677fbed9":"### Classifier Chains - build a multi-label classifier using Logistic Regression","7ee2791a":"### Define X from entire train & test data for use in tokenization by Vectorizer","bc400aa6":"There are some messages which belongs to multiple classes and as you can see in the above image classes are also not evenlt spread,that means class imbalance also there, let us check how one class is correlated with other class with the help of heapmaps","6fad3da4":"As mentioned earlier, majority of the comments in the training data are not labelled in one or more of these categories.","96781680":"### Clean up the comment text","1dc756e9":"### Binary Relevance - build a multi-label classifier using Logistic Regression","2ef92321":"### Load packages\n"}}