{"cell_type":{"dfc2fb53":"code","5a015e70":"code","d3c3be89":"code","485c4634":"code","ee623fce":"code","88dcd22a":"code","23934edb":"code","f591fd0d":"code","2230a33e":"code","45bb766a":"code","e002546b":"code","9636cb63":"code","bfb8b5d0":"code","3472b4c3":"code","94a2acd0":"code","d53b3ff2":"code","b0f8b1b9":"code","3e9347f2":"code","8ca8bcaf":"code","bade73db":"code","2132f5c5":"code","30e71d09":"code","cc74d8f0":"code","d634fcba":"code","2acf848e":"code","ece9eb4b":"code","6e997bd7":"code","4371ea71":"code","5f09af6f":"code","8d3532c0":"code","d6346c9b":"code","bedb5fc5":"code","b453fc40":"code","43fb2a9c":"code","53c29bd0":"code","ae328327":"code","13a3733f":"code","233b09b4":"code","69c7d52c":"code","3379686a":"code","a0ee11f0":"code","718bd1ee":"code","208ef28f":"code","a79c7732":"code","c0a45beb":"code","297671b7":"code","7ad7d2d5":"code","7cbc7bd6":"code","a0155b4e":"code","16641229":"code","5b2cc6ec":"code","6e1f4666":"code","6ceb6b64":"code","1d14b3d1":"code","7255c263":"code","4598fcaa":"code","f3a7c0c6":"code","9b4c1b5f":"code","3e7a6a07":"markdown","7bd9a348":"markdown","510f20a2":"markdown","692f5a5f":"markdown","a3885936":"markdown","f8dd93d3":"markdown","75bea2a8":"markdown","c58cd864":"markdown","1f90b51b":"markdown","b9ce3fe3":"markdown","3f84a615":"markdown","1255154a":"markdown","47837478":"markdown","6666627b":"markdown","6d1b82d9":"markdown","5d9db2f5":"markdown","c7acdd0d":"markdown","ca65c960":"markdown","99ca9218":"markdown","cc02a91a":"markdown","50df1eba":"markdown","4fcf2a05":"markdown","bdadea40":"markdown","024ce831":"markdown","681b22b1":"markdown","5679e340":"markdown","efe73087":"markdown","f7a54b83":"markdown","40651a0a":"markdown","d94874cb":"markdown","a1a94520":"markdown","31a66101":"markdown","52760485":"markdown","9aafa886":"markdown","8edb0cf0":"markdown","b2090242":"markdown"},"source":{"dfc2fb53":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt \nimport seaborn as sns\nfrom scipy.stats import uniform, randint\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import cross_val_score, GridSearchCV\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.ensemble import ExtraTreesClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.model_selection import KFold\nfrom sklearn.svm import SVC, LinearSVC\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.linear_model import Perceptron\nfrom sklearn.linear_model import SGDClassifier\nfrom sklearn.metrics import f1_score\nfrom sklearn.metrics import recall_score\nimport xgboost as xgb\nfrom sklearn.impute import KNNImputer\nfrom sklearn.metrics import roc_curve, roc_auc_score\nfrom xgboost import plot_tree\n\ndef one_hot(X):\n    return pd.get_dummies(X)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","5a015e70":"#1. Data collection\n# Read CSV train data file into DataFrame\ntrain_df = pd.read_csv(\"..\/input\/nida-competition1\/train.csv\")\n\n# Read CSV test data file into DataFrame\ntest_df = pd.read_csv(\"..\/input\/nida-competition1\/test.csv\")\n\n# preview train data\ntrain_df.head()","d3c3be89":"train_df.describe()","485c4634":"train_df.describe(include=['O'])","ee623fce":"train_df = train_df.replace('unknown',np.nan)\ntest_df = test_df.replace('unknown',np.nan)\n\ntrain_df[\"y\"] = train_df[\"y\"].replace(\"no\",0)\ntrain_df[\"y\"] = train_df[\"y\"].replace(\"yes\",1)\n\ntrain_df.isnull().sum()","88dcd22a":"train_df['marital'] = train_df['marital'].replace(np.nan,'unknown')\ntest_df['marital'] = test_df['marital'].replace(np.nan,'unknown')\n\ntrain_df[['marital', 'y']].groupby(['marital'], as_index=False).mean().sort_values(by='y', ascending=False)","23934edb":"freq = train_df.marital.dropna().mode()[0]\nfreq","f591fd0d":"train_df['marital'] = train_df['marital'].replace('unknown','married')\ntest_df['marital'] = test_df['marital'].replace('unknown','married')\n\ntrain_df[['marital', 'y']].groupby(['marital'], as_index=False).mean().sort_values(by='y', ascending=False)","2230a33e":"train_df['job'] = train_df['job'].replace(np.nan,'unknown')\ntest_df['job'] = test_df['job'].replace(np.nan,'unknown')\n\ntrain_df[['job', 'y']].groupby(['job'], as_index=False).mean().sort_values(by='y', ascending=False)","45bb766a":"freq = train_df.job.dropna().mode()[0]\nfreq","e002546b":"train_df['job'] = train_df['job'].replace('unknown','admin.')\ntest_df['job'] = test_df['job'].replace('unknown','admin.')\n\ntrain_df[['job', 'y']].groupby(['job'], as_index=False).mean().sort_values(by='y', ascending=False)","9636cb63":"train_df[['job', 'y']].groupby(['job'], as_index=False).mean().sort_values(by='y', ascending=False)","bfb8b5d0":"combine = [train_df, test_df]","3472b4c3":"train_df['education'] = train_df['education'].replace(np.nan,'unknown')\ntest_df['education'] = test_df['education'].replace(np.nan,'unknown')\n\ntrain_df[['education', 'y']].groupby(['education'], as_index=False).mean().sort_values(by='y', ascending=False)","94a2acd0":"train_df['education'] = train_df['education'].replace('unknown',np.nan)\ntest_df['education'] = test_df['education'].replace('unknown',np.nan)\n\nfreq = train_df.education.dropna().mode()[0]\nfreq","d53b3ff2":"for dataset in combine:\n    dataset['education'] = dataset['education'].fillna(freq)\n    \ntrain_df[['education', 'y']].groupby(['education'], as_index=False).mean().sort_values(by='y', ascending=False)","b0f8b1b9":"combine = [train_df, test_df]","3e9347f2":"train_df['housing'] = train_df['housing'].replace(np.nan,'unknown')\ntest_df['housing'] = test_df['housing'].replace(np.nan,'unknown')\n\ntrain_df[['housing', 'y']].groupby(['housing'], as_index=False).mean().sort_values(by='y', ascending=False)","8ca8bcaf":"train_df['housing'] = train_df['housing'].replace('unknown',np.nan)\ntest_df['housing'] = test_df['housing'].replace('unknown',np.nan)\n\nfreq = train_df.housing.dropna().mode()[0]\nfreq","bade73db":"for dataset in combine:\n    dataset['housing'] = dataset['housing'].fillna(freq)\n    \ntrain_df.head()","2132f5c5":"train_df[['housing', 'y']].groupby(['housing'], as_index=False).mean().sort_values(by='y', ascending=False)","30e71d09":"train_df['loan'] = train_df['loan'].replace(np.nan,'unknown')\ntest_df['loan'] = test_df['loan'].replace(np.nan,'unknown')\n\n\ntrain_df[['loan', 'y']].groupby(['loan'], as_index=False).mean().sort_values(by='y', ascending=False)","cc74d8f0":"train_df['loan'] = train_df['loan'].replace('unknown',np.nan)\ntest_df['loan'] = test_df['loan'].replace('unknown',np.nan)\n\nfreq = train_df.loan.dropna().mode()[0]\nfreq","d634fcba":"for dataset in combine:\n    dataset['loan'] = dataset['loan'].fillna(freq)\n    \ntrain_df[['loan', 'y']].groupby(['loan'], as_index=False).mean().sort_values(by='y', ascending=False)","2acf848e":"train_df = train_df.drop(columns='default')\ntest_df = test_df.drop(columns='default')","ece9eb4b":"train_df.isnull().sum()","6e997bd7":"combine = [train_df, test_df]\n\ntrain_df['AgeBand'] = pd.cut(train_df['age'], 6)\ntrain_df[['AgeBand', 'y']].groupby(['AgeBand'], as_index=False).mean().sort_values(by='AgeBand', ascending=True)","4371ea71":"for dataset in combine:    \n    dataset.loc[ dataset['age'] <= 30.5, 'age'] = 0\n    dataset.loc[(dataset['age'] > 30.5) & (dataset['age'] <= 44), 'age'] = 1\n    dataset.loc[(dataset['age'] > 44) & (dataset['age'] <= 57.5), 'age'] = 2\n    dataset.loc[(dataset['age'] > 57.5) & (dataset['age'] <= 71), 'age'] = 3\n    dataset.loc[(dataset['age'] > 71) & (dataset['age'] <= 84.5), 'age'] = 4\n    dataset.loc[ dataset['age'] > 84.5, 'age'] = 5\n\ntrain_df = train_df.drop(['AgeBand'], axis=1)\ncombine = [train_df, test_df]\ntrain_df.head()","5f09af6f":"train_df[['job', 'y']].groupby(['job'], as_index=False).mean().sort_values(by='y', ascending=False)","8d3532c0":"for dataset in combine:\n    dataset['job'] = dataset['job'].map( {'student': 0, 'retired': 1, 'unemployed': 2,'admin.': 3,'self-employed' :4,'technician':5,'management':6,'housemaid':7,'entrepreneur':8,'services':9,'blue-collar':10} )\n\ntrain_df.head()","d6346c9b":"train_df[['contact', 'y']].groupby(['contact'], as_index=False).mean().sort_values(by='y', ascending=False)","bedb5fc5":"for dataset in combine:\n    dataset['contact'] = dataset['contact'].map( {'cellular': 0, 'telephone': 1} )\n\ntrain_df.head()","b453fc40":"train_df[['month', 'y']].groupby(['month'], as_index=False).mean().sort_values(by='y', ascending=False)","43fb2a9c":"for dataset in combine:\n    dataset['month'] = dataset['month'].map( {'dec': 4, 'nov':4,'oct':4,'mar': 1,'jan':1,'feb':1,'aug':5,'sep':3,'jun':2,'jul':3,'apr':3,'may':2} )\n\ntrain_df.head()","53c29bd0":"train_df[['day_of_week', 'y']].groupby(['day_of_week'], as_index=False).mean().sort_values(by='y', ascending=False)","ae328327":"for dataset in combine:\n    dataset['day_of_week'] = dataset['day_of_week'].map( {'thu': 0, 'wed': 1,'tue':2,'fri':3,'mon':4} )\n\ntrain_df.head()","13a3733f":"train_df[['poutcome', 'y']].groupby(['poutcome'], as_index=False).mean().sort_values(by='y', ascending=False)","233b09b4":"for dataset in combine:\n    dataset['poutcome'] = dataset['poutcome'].map( {'success': 2, 'failure': 1, 'nonexistent': 0} )\n\ntrain_df.head()","69c7d52c":"train_df[['marital', 'y']].groupby(['marital'], as_index=False).mean().sort_values(by='y', ascending=False)","3379686a":"for dataset in combine:\n    dataset['marital'] = dataset['marital'].map( {'single': 1, 'divorced': 0, 'married':0 } )\n\ntrain_df.head()","a0ee11f0":"train_df[['education', 'y']].groupby(['education'], as_index=False).mean().sort_values(by='y', ascending=False)","718bd1ee":"for dataset in combine:\n    dataset['education'] = dataset['education'].map( {'university.degree': 6, 'illiterate': 5, 'professional.course': 4,'high.school': 3,'basic.4y' :2,'basic.6y':1,'basic.9y':0} )\n\ntrain_df.head()","208ef28f":"train_df[['previous', 'y']].groupby(['previous'], as_index=False).mean().sort_values(by='y', ascending=False)","a79c7732":"for dataset in combine:\n    dataset['previous'] = dataset['previous'].replace(7, -1)\n\ntrain_df.head()","c0a45beb":"for dataset in combine:\n    dataset['loan'] = dataset['loan'].map( {'yes': 1, 'no': 0} )\n\ntrain_df.head()","297671b7":"for dataset in combine:\n    dataset['housing'] = dataset['housing'].map( {'yes': 0, 'no': 1} )\n\ntrain_df.head()","7ad7d2d5":"X_train = train_df.drop(\"y\", axis=1)\nY_train = train_df[\"y\"]\nX_test  = test_df","7cbc7bd6":"from sklearn import preprocessing\n\ntemp = X_train.copy()\ntemp2 = X_test.copy()\n\nduration_train = preprocessing.scale(X_train['duration']) \nduration_test = preprocessing.scale(X_test['duration']) \nX_train['duration'] = duration_train\nX_test['duration'] = duration_test\n\nconspriceidx_train = preprocessing.scale(X_train['cons.price.idx']) \nconspriceidx_test = preprocessing.scale(X_test['cons.price.idx']) \nX_train['cons.price.idx'] = conspriceidx_train\nX_test['cons.price.idx'] = conspriceidx_test\n\nemp_train= preprocessing.scale(X_train['emp.var.rate']) \nemp_test = preprocessing.scale(X_test['emp.var.rate']) \nX_train['emp.var.rate'] = emp_train\nX_test['emp.var.rate'] = emp_test\n\nconf_train= preprocessing.scale(X_train['cons.conf.idx']) \nconf_test = preprocessing.scale(X_test['cons.conf.idx']) \nX_train['cons.conf.idx'] = conf_train\nX_test['cons.conf.idx'] = conf_test\n\nedu_train = preprocessing.scale(X_train['education']) \nedu_test = preprocessing.scale(X_test['education']) \nX_train['education'] = edu_train\nX_test['education'] = edu_test\n\ncampaign_train = preprocessing.scale(X_train['campaign']) \ncampaign_test = preprocessing.scale(X_test['campaign']) \nX_train['campaign'] = campaign_train\nX_test['campaign'] = campaign_test\n\nprevious_train = preprocessing.scale(X_train['previous']) \nprevious_test = preprocessing.scale(X_test['previous']) \nX_train['previous'] = previous_train\nX_test['previous'] = previous_test\n\nnr_train = preprocessing.scale(X_train['nr.employed']) \nnr_test = preprocessing.scale(X_test['nr.employed']) \nX_train['nr.employed'] = nr_train\nX_test['nr.employed'] = nr_test\n\nage_train = preprocessing.scale(X_train['age']) \nage_test = preprocessing.scale(X_test['age']) \nX_train['age'] = nr_train\nX_test['age'] = nr_test\n\njob_train = preprocessing.scale(X_train['job']) \njob_test = preprocessing.scale(X_test['job']) \nX_train['job'] = nr_train\nX_test['job'] = nr_test","a0155b4e":"#Imbalance Data\nfrom sklearn.utils import resample\n\n# setting up testing and training sets\nX_1, X_2, y_1, y_2 = train_test_split(X_train, Y_train, test_size=0.25, random_state=27)\n\n# concatenate our training data back together\nX = pd.concat([X_1, y_1], axis=1)\n\n# separate minority and majority classes\nnot_fraud = X[X.y==0]\nfraud = X[X.y==1]\n\n# upsample minority\nfraud_upsampled = resample(fraud,\n                          replace=True, # sample with replacement\n                          n_samples=len(not_fraud), # match number in majority class\n                          random_state=27) # reproducible results\n\n# combine majority and upsampled minority\nupsampled = pd.concat([not_fraud, fraud_upsampled])\n\n# check new class counts\nupsampled.y.value_counts()\n\n# trying logistic regression again with the balanced dataset\ny_3 = upsampled.y\nX_3 = upsampled.drop('y', axis=1)\n\ny_4 = y_2\nX_4 = X_2","16641229":"from IPython.display import Image\nimport os\n\nImage(\"..\/input\/picture\/L2Jaqm8.png\")","5b2cc6ec":"from sklearn.ensemble import RandomForestClassifier\n#170,5 ,190\n\nrandom_forest = RandomForestClassifier(n_estimators=190,\n                                       min_samples_split=2,\n                                       min_samples_leaf=5,\n                                       criterion ='entropy',\n                                       max_features=True,\n\n                                       random_state=42)\nrandom_forest.fit(X_3, y_3,)\n\noutput = pd.DataFrame()\noutput['y_forest'] = random_forest.predict_proba(X_test)[:,1]\n\nprint('roc_auc_score:',roc_auc_score(y_4, random_forest.predict_proba(X_4)[:,1]))","6e1f4666":"import lightgbm as lgb\n\nmdl = lgb.LGBMClassifier(boosting_type= 'gbdt',\n                         objective = 'binary',\n                         num_leaves = 31,\n                         max_depth=19,\n                         silent = True,\n#                          random_state=42,\n                         learning_rate=0.06,\n                         n_estimators=90)\nlgbm = mdl.fit(X_3,y_3, eval_set=[(X_2, y_2)]\n                 ,early_stopping_rounds= 40\n                 )\n\noutput['y_lgb'] = lgbm.predict_proba(X_test)[:,1]\nprint('roc_auc_score:',roc_auc_score(y_4, lgbm.predict_proba(X_4)[:,1]))","6ceb6b64":"from catboost import CatBoostClassifier\ncat = CatBoostClassifier(\n    iterations=150, \n    learning_rate=0.20,\n    random_state=42,\n    verbose=5\n)\ncat.fit(\n    X_3, y_3\n)\noutput['y_cat'] = cat.predict_proba(X_test)[:,1]\nprint('roc_auc_score:',roc_auc_score(y_4, cat.predict_proba(X_4)[:,1]))","1d14b3d1":"from sklearn.ensemble import GradientBoostingClassifier\n#200\ngdbt = GradientBoostingClassifier(n_estimators=200,\n                                  learning_rate=0.1,\n                                  random_state =42)\ngdbt.fit(X_3, y_3)\noutput['y_gdbt']= gdbt.predict_proba(X_test)[:,1]\n\nprint('roc_auc_score:',roc_auc_score(y_4, gdbt.predict_proba(X_4)[:,1]))","7255c263":"xgb_model = xgb.XGBClassifier(objective=\"binary:logistic\", \n                              random_state=42, \n                              eval_metric=\"auc\",\n                              eta=0.20)\n\nxgb_model.fit(X_3,y_3,early_stopping_rounds=20, eval_set=[(X_2, y_2)])\n\noutput['y_xgb']  = xgb_model.predict_proba(X_test)[:,1]\n\nprint('roc_auc_score:',roc_auc_score(y_4, xgb_model.predict_proba(X_4)[:,1]))","4598fcaa":"output","f3a7c0c6":"output['y'] = ( 1.5*output['y_forest'] +0.8*output['y_xgb'] + 4.0*output['y_lgb']+ 4.0*output['y_cat']+1.7*output['y_gdbt']) \/12\noutput['Id'] = test_df.index + 1","9b4c1b5f":"submission = output[['Id','y']]\n \nsubmission.to_csv(\"submission.csv\", index=False)\n \nsubmission.tail()","3e7a6a07":"Model \u0e17\u0e35\u0e48\u0e43\u0e0a\u0e49\n- Random Forest\n- LightGBM\n- CatBoost\n- Gradient Boosting Classifer\n- XGBClassifier","7bd9a348":"2.1 marital","510f20a2":"2.4 housing","692f5a5f":"3.4 month","a3885936":"3.12 housing","f8dd93d3":"2.5 default   \nDrop Default \u0e40\u0e19\u0e37\u0e48\u0e2d\u0e07\u0e08\u0e32\u0e01 \u0e21\u0e35 Missing Data \u0e40\u0e22\u0e2d\u0e30","75bea2a8":"3.2 Job","c58cd864":"4.3 Catboost","1f90b51b":"3.1 Age","b9ce3fe3":"4.5 XGBClassifier    \n* XGBoost is a workhorse gradient boosted decision tree algorithm.     \n* weak classifier\n\nparameter   \n* objective : Specify the learning task and the corresponding learning objective   \n   * \"binary:logistic\" : logistic regression for binary classification, output probability\n* eta : learning rate   \n* eval_metric : Evaluation metrics for validation data\n","3f84a615":"Output File","1255154a":"3.3 contact","47837478":"3.13 scale","6666627b":"4.6 Ensemble - weighted average","6d1b82d9":"2.4 loan","5d9db2f5":" Private Score = 0.95075","c7acdd0d":"3.16 Feature Selection  \n\u0e40\u0e19\u0e37\u0e48\u0e2d\u0e07\u0e08\u0e32\u0e01 Model \u0e17\u0e35\u0e48\u0e40\u0e25\u0e37\u0e2d\u0e01\u0e43\u0e0a\u0e49\u0e40\u0e1b\u0e47\u0e19 tree based model \u0e17\u0e31\u0e49\u0e07\u0e2b\u0e21\u0e14 \u0e0b\u0e36\u0e48\u0e07\u0e01\u0e32\u0e23\u0e40\u0e25\u0e37\u0e2d\u0e01 Variable \u0e43\u0e19\u0e01\u0e32\u0e23\u0e41\u0e15\u0e01\u0e01\u0e34\u0e48\u0e07\u0e02\u0e2d\u0e07 tree based model \u0e08\u0e30\u0e40\u0e1b\u0e47\u0e19\u0e01\u0e32\u0e23 Feature Selection \u0e2d\u0e22\u0e39\u0e48\u0e41\u0e25\u0e49\u0e27","ca65c960":"\u0e15\u0e23\u0e27\u0e08\u0e2a\u0e2d\u0e1a Missing data","99ca9218":"Types of ensembling :\n\nBasic Ensemble Techniques\n\n* Max Voting   \n* Averaging   \n* Weighted Average   ","cc02a91a":"# 4. Model","50df1eba":"# 2. Cleaning Data","4fcf2a05":"3.10 previous","bdadea40":"\u0e2a\u0e21\u0e32\u0e0a\u0e34\u0e01   \n6220422022\t\u0e19\u0e20\u0e31\u0e2a\u0e2a\u0e23\t\u0e15\u0e31\u0e19\u0e15\u0e32\u0e28\u0e19\u0e35   \n6220422053\t\u0e01\u0e0a\u0e01\u0e23\t\u0e2a\u0e23\u0e32\u0e27\u0e38\u0e12\u0e34\u0e19\u0e31\u0e19\u0e17\u0e4c\t   \n6220422054\t\u0e20\u0e32\u0e04\u0e20\u0e39\u0e21\u0e34\t\u0e40\u0e25\u0e34\u0e28\u0e2a\u0e27\u0e31\u0e2a\u0e14\u0e34\u0e4c\u0e27\u0e34\u0e0a\u0e32\t \n6310422077\t\u0e42\u0e2a\u0e27\u0e23\u0e23\u0e13\t\u0e22\u0e38\u0e17\u0e18\u0e22\u0e37\u0e19\u0e22\u0e07\t ","024ce831":"3.9 education","681b22b1":"3.6 poutcome","5679e340":"# 1. Collect Date","efe73087":"3.15 Imbalance Data","f7a54b83":"4.1 Random Forest   \nBagging (random with replacement) \n\nParameter   \n* n_estimators : The number of trees in the forest.   \n* min_samples_split : \nThe minimum number of samples required to split an internal node\n* criterion : The function to measure the quality of a split.   \n* min_samples_leaf : \nThe minimum number of samples required to be at a leaf node.\n* max_features : The number of features to consider when looking for the best split.","40651a0a":"4.2 LightGBM   \nweak classifier","d94874cb":"4.4 Gradient Boosting Classifer","a1a94520":"3.5 day_of_week","31a66101":"3.11 loan","52760485":"2.2 job","9aafa886":"3.8 marital","8edb0cf0":"# 3. Feature Engineering","b2090242":"2.3 education"}}