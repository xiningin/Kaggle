{"cell_type":{"a5d797ad":"code","13e6f0d2":"code","c7425cbe":"code","6f73e3bd":"code","30b7a073":"code","30dbfad7":"code","1f30f5c7":"code","936bfa18":"code","02b9a3af":"code","76702cac":"code","e429d319":"code","614ac88a":"code","ee6ff4e3":"code","058af084":"code","3a857922":"code","d98766b4":"code","66892967":"code","3386b000":"markdown","594cf383":"markdown","5da2d5bd":"markdown"},"source":{"a5d797ad":"import warnings, re\nfrom tqdm import tqdm\nimport pandas as pd\nimport numpy as np","13e6f0d2":"train_data=pd.read_csv('\/kaggle\/input\/fake-news\/train.csv',index_col='id')\ntest_data=pd.read_csv('\/kaggle\/input\/fake-news\/test.csv',index_col='id')","c7425cbe":"train_data.fillna('Not Mentioned',inplace=True)\ntest_data.fillna('Not Mentioned',inplace=True)","6f73e3bd":"len(train_data.loc[train_data['label']==1]), len(train_data.loc[train_data['label']==0])","30b7a073":"def text_cleaning(text):\n    \"\"\"\n    Removing all characters except alphabets\n    \"\"\"\n    text = re.sub(r'[^a-zA-Z]', ' ', text)\n    return text\n\ntrain_data['text']=train_data['text'].apply(text_cleaning)\ntrain_data['title']=train_data['title'].apply(text_cleaning)\ntrain_data['author']=train_data['author'].apply(text_cleaning)\n\n#applying the same preprocessing for test data\ntest_data['text']=test_data['text'].apply(text_cleaning)\ntest_data['title']=test_data['title'].apply(text_cleaning)\ntest_data['author']=test_data['author'].apply(text_cleaning)","30dbfad7":"train_data.head()","1f30f5c7":"from sklearn.model_selection import train_test_split\n\ntrain_data, val_data=train_test_split(train_data, test_size=0.1, shuffle=True)\nlen(train_data), len(val_data)","936bfa18":"import transformers\n\n#configuration\nMAX_LEN=512\nTrain_batch_size=8\nValid_batch_size=4\nEpochs=10\nAccumulation=2\nBert_path='..\/input\/bert-base-uncased'\nmodel_path='model.pt'\nTrain_file='..\/input\/fake-news\/train.csv'\nTokenizer=transformers.BertTokenizer.from_pretrained(Bert_path, do_lower_case=True)","02b9a3af":"import torch\n\nclass FakeNewsDataset:\n    def __init__(self, text, label):\n        self.text=text\n        self.label=label\n        self.tokenizer=Tokenizer\n        self.max_len=MAX_LEN\n    \n    def __len__(self):\n        return len(self.text)\n    \n    def __getitem__(self, idx):\n        text=str(self.text[idx])\n        \n        inputs=self.tokenizer.encode_plus(text, None, add_special_tokens=True, \n                                         max_length=self.max_len, truncation=True)\n        \n        ids=inputs['input_ids']\n        mask=inputs['attention_mask']\n        token_type_ids=inputs['token_type_ids']\n        \n        padding_length=self.max_len-len(ids)\n        ids=ids+([0]*padding_length)\n        mask=mask+([0]*padding_length)\n        token_type_ids=token_type_ids+([0]*padding_length)\n        \n        \n        return {\n            'ids':torch.LongTensor(ids),\n            'mask':torch.LongTensor(mask),\n            'token_type_ids':torch.LongTensor(token_type_ids),\n            'targets':torch.tensor(self.label[idx], dtype=torch.float)\n        }\n    \n    \ntrainLoader=torch.utils.data.DataLoader(\n    FakeNewsDataset(train_data['text'].values, train_data['label'].values),\n    batch_size=Train_batch_size,\n    num_workers=4)\n\nvalLoader=torch.utils.data.DataLoader(\n    FakeNewsDataset(val_data['text'].values, val_data['label'].values),\n    batch_size=Valid_batch_size,\n    num_workers=1)","76702cac":"import torch.nn as nn\n\nclass FakeBERTuncased(nn.Module):\n    def __init__(self):\n        super(FakeBERTuncased, self).__init__()\n        self.bert = transformers.BertModel.from_pretrained(Bert_path)\n        self.bert_drop = nn.Dropout(0.3)\n        self.fc = nn.Linear(768, 1)\n\n    def forward(self, ids, mask, token_type_ids):\n        _, pool = self.bert(ids, attention_mask=mask, token_type_ids=token_type_ids)\n        pool = self.bert_drop(pool)\n        output = self.fc(pool)\n        return output\n    \ndevice=torch.device('cuda' if torch.cuda.is_available() else 'cpu')    \n\nmodel=FakeBERTuncased()\nmodel.to(device)\n\nprint(\"Training on \",device)\nprint(sum(p.numel() for p in model.parameters() if p.requires_grad),' trainable prams')","e429d319":"criterion=nn.BCEWithLogitsLoss()\nparam_optimizer=list(model.named_parameters())\nno_decay=['bias', 'LayerNorm.bias', 'LayerNorm.weight']\n\noptimizer_params=[\n    {'params':[p for n,p in param_optimizer if not any(nd in n for nd in no_decay)], \n     'weight_decay':0.001},\n    {'params':[p for n,p in param_optimizer if any(nd in n for nd in no_decay)], \n     'weight_decay': 0}]\n\nnum_train_steps=int(len(train_data)\/ Train_batch_size*Epochs)\noptimizer=transformers.AdamW(optimizer_params, lr=2e-5)\nscheduler=transformers.get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0,\n                                                        num_training_steps=num_train_steps)\n\ndef train(model, iterator, optimizer, scheduler, device, accumulation_steps=None):\n    model.train()\n    \n    for i, batch in tqdm(enumerate(iterator), total=len(iterator)):\n        ids, token_type_ids, mask, targets= batch['ids'].to(device, dtype=torch.long),batch['token_type_ids'].to(device, dtype=torch.long), batch['mask'].to(device, dtype=torch.long), batch['targets'].to(device, dtype=torch.float)\n        \n        optimizer.zero_grad()\n        preds=model(ids, mask, token_type_ids).squeeze(1)\n\n        loss=criterion(preds, targets)\n        loss.backward()\n        \n        #if (i+1)% accumulation_steps==0:\n        optimizer.step()\n        scheduler.step()\n        \ndef evaluation(model, iterator, device):\n    model.eval()\n    \n    fin_targets, fin_outputs=[], []\n    with torch.no_grad():\n        for i, batch in tqdm(enumerate(iterator), total=len(iterator)):\n            ids, token_type_ids, mask, targets= batch['ids'].to(device, dtype=torch.long),batch['token_type_ids'].to(device, dtype=torch.long), batch['mask'].to(device, dtype=torch.long), batch['targets'].to(device, dtype=torch.float)\n            preds=model(ids, mask, token_type_ids)\n\n            #loss=criterion(preds, targets)\n            \n            fin_targets.extend(targets.cpu().detach().numpy().tolist())\n            fin_outputs.extend(torch.round(torch.sigmoid(preds)).cpu().detach().numpy().tolist())\n            \n        return fin_outputs, fin_targets","614ac88a":"#Done with training\n# best_accuracy=0\n\n# for epoch in range(Epochs):\n#     train(model, trainLoader, optimizer, scheduler, device)\n    \n#     preds, targets=evaluation(model, valLoader, device)\n    \n#     accuracy=accuracy_score(targets, preds)\n#     print(epoch+1,\" Accuracy: \",accuracy)\n    \n#     if accuracy>best_accuracy:\n#         torch.save(model.state_dict(),model_path)\n#         best_accuracy=accuracy","ee6ff4e3":"# torch.cuda.empty_cache()","058af084":"from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n\ndef infer(text):\n    with torch.no_grad():\n        inputs=Tokenizer.encode_plus(text, None, max_length=512, truncation=True)\n        \n        ids, mask, token_type_ids=inputs['input_ids'], inputs['attention_mask'], inputs[\"token_type_ids\"]\n\n        padding_length = 512 - len(ids)\n        ids = ids + ([0] * padding_length)\n        mask = mask + ([0] * padding_length)\n        token_type_ids = token_type_ids + ([0] * padding_length)\n\n        ids = torch.LongTensor(ids).unsqueeze(0).to(device)\n        mask = torch.LongTensor(mask).unsqueeze(0).to(device)\n        token_type_ids = torch.LongTensor(token_type_ids).unsqueeze(0).to(device)\n\n        preds = model(ids=ids, mask=mask, token_type_ids=token_type_ids)\n        preds = torch.round(torch.sigmoid(preds)).cpu().detach().numpy()\n    return preds[0][0]","3a857922":"model.load_state_dict(torch.load('..\/input\/modelchkpt\/model(1).pt'))#,map_location='cpu'))\nmodel.eval()\n\ntrue=[label for label in val_data.label.values]\npredicted=[infer(text) for text in tqdm(val_data.text.values)]\n\nprint(confusion_matrix(true , predicted),'\\n\\n\\n')\nprint(classification_report(true,predicted,target_names=['real','fake']))","d98766b4":"predicted=[infer(text) for text in tqdm(test_data.text.values)]","66892967":"my_submissions=pd.DataFrame({'id':test_data.index.values,'label':predicted})\nmy_submissions.to_csv('submission.csv', index=False)","3386b000":"## Inference\n\nUsing validation set to create classification report","594cf383":"## Data Cleaning","5da2d5bd":"Test Results and Submission"}}