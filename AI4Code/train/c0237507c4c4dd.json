{"cell_type":{"589e91e6":"code","bbcfbeb8":"code","33c79ac0":"code","26762dd2":"code","873e4122":"code","c250a4c4":"code","9ab77b80":"code","ba1e1ff3":"code","8365cb58":"code","fc2857c2":"code","be17ff3e":"code","9d70fcc9":"code","d876c622":"code","531a71f0":"code","eead706a":"code","839a6379":"markdown","78682475":"markdown","362c7f8f":"markdown","41fdb77d":"markdown","e75622b6":"markdown","45379400":"markdown","77027e45":"markdown","ab475305":"markdown","6afb9864":"markdown","94dddb71":"markdown","8ccf70d1":"markdown"},"source":{"589e91e6":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","bbcfbeb8":"train = pd.read_csv(\"..\/input\/house-prices-advanced-regression-techniques\/train.csv\")\ntest = pd.read_csv(\"..\/input\/house-prices-advanced-regression-techniques\/test.csv\")\nsubmission_df = pd.read_csv('\/kaggle\/input\/house-prices-advanced-regression-techniques\/sample_submission.csv')","33c79ac0":"# show correlation of numerical features with SalePrice:\ndef calculate_correlation_on_target(df, target='SalePrice'):\n    nums = df.select_dtypes('number')\n    corr_series = nums.corr()[target].to_frame(target)\n    corr_series = corr_series.sort_values(target, ascending=False)\n    print(corr_series)\n    return\ncalculate_correlation_on_target(train)\n\n","26762dd2":"keep_num_cols = ['OverallQual', 'GarageCars', 'TotRmsAbvGrd', 'YearBuilt', 'FullBath', 'Fireplaces']","873e4122":"def conditional_entropy(x,y):\n    from collections import Counter\n    import math\n    # entropy of x given y\n    y_counter = Counter(y)\n    xy_counter = Counter(list(zip(x,y)))\n    total_occurrences = sum(y_counter.values())\n    entropy = 0\n    for xy in xy_counter.keys():\n        p_xy = xy_counter[xy] \/ total_occurrences\n        p_y = y_counter[xy[1]] \/ total_occurrences\n        entropy += p_xy * math.log(p_y\/p_xy)\n    return entropy\n\n\ndef theil_u(x,y):\n    from collections import Counter\n    import scipy.stats as ss\n    s_xy = conditional_entropy(x,y)\n    x_counter = Counter(x)\n    total_occurrences = sum(x_counter.values())\n    p_x = list(map(lambda n: n\/total_occurrences, x_counter.values()))\n    s_x = ss.entropy(p_x)\n    if s_x == 0:\n        return 1\n    else:\n        return (s_x - s_xy) \/ s_x\n\n\ndef calculate_theil_u_on_target(df, target='SalePrice'):\n    import pandas as pd\n    cats = df.select_dtypes('object')\n    thes = []\n    columns = cats.columns\n    for j in range(0, len(columns)):\n        u = theil_u(df[target].tolist(),cats[columns[j]].tolist())\n        thes.append(u)\n    ser = pd.Series(thes, index=columns).to_frame(target)\n    ser = ser.sort_values(target, ascending=False)\n    print(ser)\n    return\n\ncalculate_theil_u_on_target(train)","c250a4c4":"keep_cat_cols = ['Neighborhood']","9ab77b80":"train = train[[x for x in keep_cat_cols+keep_num_cols]]\ntrain.info()","ba1e1ff3":"def encode_cols(df, cols=keep_cat_cols):\n    import pandas as pd\n    for col in cols:\n        dumm = pd.get_dummies(data=df[col],prefix=col)\n        df = pd.concat([df, dumm], axis=1)\n        df = df.drop(col, axis=1)\n    return df\ntrain = encode_cols(train)","8365cb58":"def preprocess_train_test_data(df, target='SalePrice'):\n    import numpy as np\n    # keep only cols:\n    X = df[[x for x in keep_cat_cols+keep_num_cols]]\n    X = encode_cols(X)\n    # extract target from train and apply log:\n    if target in df.columns:\n        y = df['SalePrice'].apply(np.log1p)\n        return X, y\n    else:\n        return X","fc2857c2":"train = pd.read_csv(\"..\/input\/house-prices-advanced-regression-techniques\/train.csv\")\nX, y = preprocess_train_test_data(train)","be17ff3e":"def feature_importances(model, X, y):\n    import pandas as pd\n    model.fit(X, y)\n    fi = pd.DataFrame(model.feature_importances_)\n    fi.index = X.columns\n    fi.columns = ['FI']\n    fi = fi.sort_values('FI', ascending=False)\n    print(fi)\n    return\n\nfrom sklearn.ensemble import RandomForestRegressor\n\nrf = RandomForestRegressor(max_depth=5, n_estimators=250)\nfeature_importances(rf, X, y)\n","9d70fcc9":"def make_rmse():\n    from sklearn.metrics import mean_squared_error, make_scorer\n    rmse = make_scorer(mean_squared_error,\n                       greater_is_better=False, squared=False)\n    return rmse\n\ndef cross_validate(X, y, model, cv=10):\n    import numpy as np\n    from sklearn.model_selection import cross_val_score\n    print('CV of {} KFolds.'.format(cv))\n    cv = cross_val_score(model, X, y, cv=cv, scoring=make_rmse())\n    print('Mean: {}'.format(np.mean(cv)))\n    print('Std: {}'.format(np.std(cv)))\n    return\n\ncross_validate(X, y, rf)\n","d876c622":"X_test = preprocess_train_test_data(test)\nX_test.info()\n","531a71f0":"X_test['GarageCars'] = X_test['GarageCars'].fillna(0)","eead706a":"def predict_on_test(X_test, model, submission_df, target='SalePrice'):\n    import numpy as np\n    preds_test = model.predict(X_test)\n    submission_df.loc[:, target] = np.expm1(preds_test)\n    submission_df.to_csv('submission.csv', index=False)\n    return\n\npredict_on_test(X_test,rf,submission_df)","839a6379":"So, interestly, Neighborhood is not that important...\nOk, let's do some cross validation with RMSE metric:","78682475":"So Neighborhood is the most associated feature with SalePrice, as expected (location, location, location).","362c7f8f":"It looks as GarageCars has a NaN, we impute it with 0:","41fdb77d":"Now, split train into X, y using the latter function in order to do some modelling and validating:","e75622b6":"Now, we initialize a RandomForestRegressor and fit it on the training data and gain the feature importances:","45379400":"Now, we check the association of the categorical features with SalePrice using Thiel's U, taken from e.g., https:\/\/www.kaggle.com\/akshay22071995\/alone-in-the-woods-using-theil-s-u-for-survival?kernelSessionId=14747826.","77027e45":"Actually, we can wrap all the steps we did in a nice preproccesing function:","ab475305":"Good, no NaNs and the only category feature is Neightborhood, so we One Hot Encode it:","6afb9864":"Now we submit the solution:","94dddb71":"OK, so the model is robust enough, let's preproccess the test set:","8ccf70d1":"so, negative correlations features are too weak so we ignore them. Some positive correlated features such as GarageCars and GarageArea are probably highly correlated, thus we keep only highly correlated features that are discrete (since maybe people that buy houses think of number of rooms instead of total area). More specifically, the Quality, number of cars in a garage, total rooms above ground, year built, number of full bathrooms and number of fireplaces:"}}