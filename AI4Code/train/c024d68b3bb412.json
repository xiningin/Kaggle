{"cell_type":{"5add5656":"code","8d183414":"code","6fc37259":"code","72785fb5":"code","bb679db9":"code","1c7334a8":"code","7b4b8aa0":"code","239946d1":"code","2d4304ed":"code","851c06a6":"code","266f4c84":"code","d64b29f9":"markdown","8384015f":"markdown","4ae29627":"markdown","7fd6a4f4":"markdown","450c0fab":"markdown","13053dbe":"markdown","9f956439":"markdown","1bbe3f79":"markdown","381b7bda":"markdown","0c142530":"markdown","0b5c9814":"markdown"},"source":{"5add5656":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn import tree\nimport graphviz\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split\n\nnp.random.seed(0)","8d183414":"# lets generate a dataset\ndataset=pd.DataFrame(np.zeros((5000,3)), columns=['var_0', 'var_1', 'target'])\n\ndataset.loc[:2499, 'var_0'] = np.random.randn(2500) * 3 + 1\ndataset.loc[:2499, 'var_1'] = np.random.randn(2500) * 3 + 0\ndataset.loc[:2499, 'target'] = 1\n\ndataset.loc[2500:, 'var_0'] = np.random.randn(2500) * 3 + 0\ndataset.loc[2500:, 'var_1'] = np.random.randn(2500) * 3 + 1\ndataset.loc[2500:, 'target'] = 0\n\ndataset = dataset.sample(frac=1) # shuffle\n\nplt.figure(figsize=(6, 6))\nplt.scatter(dataset.var_0, dataset.var_1, c=dataset.target, alpha=0.6)\nplt.xlabel('var_0')\nplt.ylabel('var_1')\nplt.show()","6fc37259":"train, target = dataset.iloc[:, :2], dataset.target\ncols = train.columns\nX_train, X_valid, y_train, y_valid = train_test_split(train, target, test_size=0.1, random_state=2019)\n\n\n#model = tree.DecisionTreeClassifier(max_leaf_nodes=8)\nmodel = tree.DecisionTreeRegressor(max_leaf_nodes=4)\nmodel.fit(X_train, y_train)\n#pred_val = model.predict_proba(valid[cols])[:,1]\npred_val = model.predict(X_valid)\n\n# print('AUC = ',round( roc_auc_score(y_valid,pred_val),4 ) )\n\n\ntree_graph = tree.export_graphviz(model, out_file=None, max_depth = 10,\n    impurity = False, feature_names = cols, class_names = ['0', '1'],\n    rounded = True, filled= True )\ngraphviz.Source(tree_graph)  ","72785fb5":"f,axs=plt.subplots(1,2,figsize=(20,4))\naxs[0].hist(X_train.loc[target==1, 'var_0'], label='target=1', bins=100, color='red', alpha=0.4)\naxs[0].hist(X_train.loc[target==0, 'var_0'], label='target=0',bins=100, color='blue' , alpha=0.4)\naxs[0].set_title('var_0')\naxs[0].legend()\naxs[1].hist(X_train.loc[target==1, 'var_1'], label='target=1', bins=100, color='red', alpha=0.4)\naxs[1].hist(X_train.loc[target==0, 'var_1'], label='target=0',bins=100, color='blue' , alpha=0.4)\naxs[1].set_title('var_1')\naxs[1].legend()\nplt.show()","bb679db9":"dataset['var_0_plus_var_1'] = dataset['var_0'] + dataset['var_1']\ndataset['var_0_minus_var_1'] = dataset['var_0'] - dataset['var_1']","1c7334a8":"plt.scatter(dataset.var_0_plus_var_1, dataset.var_0_minus_var_1, c=dataset.target, alpha=0.9)\nplt.xlabel('var_0_plus_var_1')\nplt.ylabel('var_0_minus_var_1')\nplt.show()","7b4b8aa0":"train, target = dataset.loc[:, ['var_0_minus_var_1', 'var_0_plus_var_1']], dataset.target\ncols = train.columns\nX_train, X_valid, y_train, y_valid = train_test_split(train, target, test_size=0.1, random_state=2019)\n\n\n#model = tree.DecisionTreeClassifier(max_leaf_nodes=8)\nmodel = tree.DecisionTreeRegressor(max_leaf_nodes=4)\nmodel.fit(X_train, y_train)\n#pred_val = model.predict_proba(valid[cols])[:,1]\npred_val = model.predict(X_valid)\n\ntree_graph = tree.export_graphviz(model, out_file=None, max_depth = 10,\n    impurity = False, feature_names = cols, class_names = ['0', '1'],\n    rounded = True, filled= True )\ngraphviz.Source(tree_graph)  ","239946d1":"f,axs=plt.subplots(1,2,figsize=(20,5))\naxs[0].hist(X_train.loc[target==1, 'var_0_plus_var_1'], label='target=1', bins=40, color='red', alpha=0.4)\naxs[0].hist(X_train.loc[target==0, 'var_0_plus_var_1'], label='target=0',bins=40, color='blue' , alpha=0.4)\naxs[0].set_title('var_0_plus_var_1')\naxs[0].legend()\naxs[1].hist(X_train.loc[target==1, 'var_0_minus_var_1'], label='target=1', bins=40, color='red', alpha=0.4)\naxs[1].hist(X_train.loc[target==0, 'var_0_minus_var_1'], label='target=0',bins=40, color='blue' , alpha=0.4)\naxs[1].set_title('var_0_minus_var_1')\naxs[1].legend()\nplt.show()","2d4304ed":"from sklearn.preprocessing import StandardScaler\nreverse_list = [0,1,2,3,4,5,6,7,8,11,15,16,18,19,\n                22,24,25,26,27,41,29,\n                32,35,37,40,48,49,47,\n                55,51,52,53,60,61,62,103,65,66,67,69,\n                70,71,74,78,79,\n                82,84,89,90,91,94,95,96,97,99,\n                105,106,110,111,112,118,119,125,128,\n                130,133,134,135,137,\n                140,144,145,147,151,155,157,159,\n                161,162,163,164,167,168,\n                170,171,173,175,176,179,\n                180,181,184,185,187,189,\n                190,191,195,196,199]\ndef plot_means(X_r,X_o):\n    # Plotting min and max of each class\n    d_mean_o=np.abs(X_o[ind0].mean(axis=0)-X_o[~ind0].mean(axis=0))\n    d_mean_r=np.abs(X_r[ind0].mean(axis=0)-X_r[~ind0].mean(axis=0))\n    plt.figure(figsize=(15,8))\n    fn=np.arange(200)\n    plt.plot(fn,d_mean_o,'k',label='Original Data')\n    plt.plot(fn,d_mean_r,'b',label='Rotated Data')\n    plt.xlabel('Feature Number');plt.ylabel('Distance of class means along the axis')\n    plt.legend()\n\ntrain = pd.read_csv('..\/input\/train.csv')\ntrain, target = train.iloc[:, 2:], train['target']\n\nss=StandardScaler()\nind0=target==0\ntrain_scaled=ss.fit_transform(train)\n\nx0m=train_scaled[ind0].mean(axis=0,keepdims=True)\nx1m=train_scaled[~ind0].mean(axis=0,keepdims=True)\nxm=np.r_[x0m,x1m]\nL,s,R=np.linalg.svd(xm)\ntrain_rotated=train_scaled @ R\n\nplot_means(train_rotated,train_scaled)","851c06a6":"f,axs=plt.subplots(2,2,figsize=(18,8))\naxs[0,0].hist(train_scaled[ind0,0], label='class 0', bins=100, color='r', alpha=0.4,density=True)\naxs[0,0].hist(train_scaled[~ind0,0], label='class 1', bins=100, color='b', alpha=0.4,density=True)\naxs[0,0].set_title('rotated var_0')\naxs[0,1].hist(train_scaled[ind0,4], label='class 0', bins=100, color='r', alpha=0.4,density=True)\naxs[0,1].hist(train_scaled[~ind0,4], label='class 1', bins=100, color='b', alpha=0.4,density=True)\naxs[0,1].set_title('rotated var_4')\naxs[1,0].hist(train_rotated[ind0,0], label='class 0', bins=100, color='r', alpha=0.4,density=True)\naxs[1,0].hist(train_rotated[~ind0,0], label='class 1', bins=100, color='b', alpha=0.4,density=True)\naxs[1,0].set_title('rotated var_0')\naxs[1,1].hist(train_rotated[ind0,4], label='class 0', bins=100, color='r', alpha=0.4,density=True)\naxs[1,1].hist(train_rotated[~ind0,4], label='class 1', bins=100, color='b', alpha=0.4,density=True)\naxs[1,1].set_title('rotated var_4')\naxs[1,1].legend()\nplt.tight_layout()\nplt.show()","266f4c84":"train.iloc[:,reverse_list]*=-1\ntrain_scaled=ss.fit_transform(train)\n\nx0m=train_scaled[ind0].mean(axis=0,keepdims=True)\nx1m=train_scaled[~ind0].mean(axis=0,keepdims=True)\nxm=np.r_[x0m,x1m]\nL,s,R=np.linalg.svd(xm)\ntrain_rotated=train_scaled @ R\n\nplot_means(train_rotated,train_scaled)","d64b29f9":"Let's take another look at the scatter plot.","8384015f":"Now, let's take a look at variables distribution. They are quite similar to Santander's data, aren't they? ","4ae29627":"It seems quite similar to the original dataset. However, a decision tree should be able to split it using vertical and horizontal lines easier this time. Training the same model on the rotated data gets AUC of 0.6337! **0.0216** improvement from 0.6121.\nSounds familiar?","7fd6a4f4":"As known, decision trees like the data when the boundary is inline with axes which is not the case here. So naturally a tree based model (DecisionTreeRegressor) so the performance is expected to be poor (AUC of 0.6121). Here's the visualisation of the tree and histogram distributions in each axes.","450c0fab":"Now let's try to rotate the competition dataset systematically.","13053dbe":"# What a tree cannot see?\nThat is the hint Chris pointed out several times in his comments as the magic. We are exploring one aspect in this kernel. We first explore the idea on synthetic data, then apply it on the competition data. The improvement on synthetic data is drastic, but did not improve our LB. If your engineered features got you beyond 0.901 and this helps, leave a comment so we know it wasn't in vain. \n\n\nFirst, let's generate a dataset which consists of only two variables with close class distribution similar to the this competition.","9f956439":"This is interesting, now SVD can rotate the axes so all the features (except the infamous `var_68`) to be perfectly aligned with the axes. But it doesn't matter as we mentioned in another [kernel](https:\/\/www.kaggle.com\/mhviraf\/there-are-no-categorical-features), that variable is the residual of the PCA done on data in the processing. The distributions don't change much compared to the histograms above. Also the rotation matrix could be inspected to see the change of feature orders in the rotated data.\n\nAlthough this rotation was not helpful here, we hope it'd be useful to someone later.","1bbe3f79":"But SVD does not mirror the axes. I remember somewhere in the discussions pointed that some features could be flipped (credit goes to the guy I don't remember). So let's try that and rotate again.","381b7bda":"Ok, let's go back to the hint! what a decision tree **CANNOT** see? remember trees split using boundaries inline with axes. The workaround is to rotate the data. But we don't know much about the boundaries (the problem was solved if we knew), so instead we use the location of in-class means. Here, the data is rotated 45 degrees (using prior knowledge of how the data was generated! but the degree of rotation is not hard to find even without that knowledge).","0c142530":"You can see that all the distances has been accumulated in the first feature except for var_68. Now let's plot the histogram of the first feature and a random one.","0b5c9814":"This rotation separates one of the histograms but collapse all the others:"}}