{"cell_type":{"fd8d01c8":"code","feb0596d":"code","68d01726":"code","7c8699ca":"code","1d777fbe":"code","15541b34":"code","d22f71b3":"code","624111d9":"code","88f07400":"markdown","e94cc976":"markdown","33197931":"markdown","56ace3bd":"markdown"},"source":{"fd8d01c8":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\n#for dirname, _, filenames in os.walk('\/kaggle\/input'):\n#    for filename in filenames:        print(os.path.join(dirname, filename))\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","feb0596d":"from matplotlib import pyplot as pl\nfrom sklearn import cluster,metrics\nfrom scipy.cluster import hierarchy as hc\nfrom scipy import signal\ndef flattenMatrix(m):\n    # helper to get the upper-triangle values \n    if np.shape(m)[0] != np.shape(m)[1]:\n        print('Input is not a square %s' % m.shape)\n        return\n    n = int(m.shape[0])\n    flatv = np.zeros(int((n*(n-1))\/2))\n    count = 0 \n    for i in range(n):\n        for j in range(1+i,n):\n            flatv[count] = m[i,j]\n            count += 1\n    return flatv\n# Confirmed cases in the US\ndf_us_conf = pd.read_csv('\/kaggle\/input\/covid19-data-from-john-hopkins-university\/CONVENIENT_us_confirmed_cases.csv',header=[0,1],index_col=0)\n#print(df_us_conf.shape)\nstates = np.unique(df_us_conf.columns.get_level_values(0))#print(len(states),states)\nn_states = len(states)\ndates = df_us_conf.axes[0]\ndates = dates.map(lambda x: '-'.join(x.split('\/')[:-1]))\nn_days = len(dates)\n# Sum up numbers for each state\nds = np.zeros((n_states, n_days))\nfor ist, st in enumerate(states):\n    '''If to exclude cases Out of state and Unassigned:\n    counties = df_us_conf[st].axes[1]\n    ctr = (counties.map(lambda x: x.startswith('Out of')))\n    ctr = np.asarray((ctr.values | (counties=='Unassigned')),dtype=bool)\n    counties = counties[~ctr]'''\n    print('%d %s totalN=%d' % (ist,st,df_us_conf[st].sum().sum()))\n    # sum all counties\n    ds[ist] = df_us_conf[st].sum(1).values","68d01726":"pl.figure(figsize=(18,5))\npl.plot(ds.T)\nfor ist, st in enumerate(states):\n    pl.text(ds[ist].argmax(), ds[ist].max(),st)\npl.xticks(np.arange(0,n_days,30),dates[np.arange(0,n_days,30)])\npl.show()","7c8699ca":"# May do some simple smoothing to mitigate day to day variation due to uninteresting reason (e.g., weekends)\nlmbd = 1\nds_sm = np.empty_like(ds)\nfor ist, st in enumerate(states):\n    ds_sm[ist] = signal.cspline1d(ds[ist],lmbd)\npl.figure(figsize=(18,5))\npl.plot(ds_sm.T)\nfor ist, st in enumerate(states):\n    pl.text(int(ds_sm[ist].argmax()), int(ds_sm[ist].max()), st)\npl.xticks(np.arange(0,n_days,15),dates[np.arange(0,n_days,15)])\npl.show()","1d777fbe":"# start and end date ('month-day')\ndate_begin = '2-15'\ndate_end = '11-18'\ntime_period = np.nonzero((dates==date_begin) | (dates==date_end))[0]\n# regions\/states to exclude (Territories)\nstates_exclude = ['American Samoa','Diamond Princess', 'Grand Princess','Northern Mariana Islands','Guam','Virgin Islands','Puerto Rico']\nlbl_exclude_states = np.asarray([(st not in states_exclude) for st in states])\nstates_sub = states[lbl_exclude_states]\nds_tmp = ds_sm[lbl_exclude_states, time_period[0]:time_period[1]]\nprint(ds_tmp.shape)\ncm = np.corrcoef(ds_tmp) \npl.figure(figsize=(5,5))\npl.imshow(cm,cmap='jet')\npl.xticks(range(n_states-len(states_exclude)),states_sub,rotation=90)\npl.yticks(range(n_states-len(states_exclude)),states_sub)\npl.colorbar()\npl.show()","15541b34":"# Perform simple Hierarchical clustering\ndm = 1-cm \ndm[np.isnan(dm)] = 1\nZ = hc.linkage(flattenMatrix(dm), method='ward')\npl.figure(figsize=(10,4))\ndn = hc.dendrogram(Z , labels=states_sub)\npl.show()","d22f71b3":"# Flatten the dendrogram into given n of clusters\nn_clusters = 6\nclustering = hc.fcluster(Z,n_clusters,criterion='maxclust')#clustering\nfor i in range(1,1+n_clusters):\n    print('#%d n=%d' % (i,sum(clustering==i)))\n    print(states_sub[clustering==i])","624111d9":"# May assign a label arbitrarily for each cluster for vis purpose, otherwise just use the first state's name plus clustersize:\nclusterNames =  []#\nfor i in range(1,1+n_clusters):\n    clusterNames.append(states_sub[clustering==i][0]+str(sum(clustering==i)))\npl.figure(figsize=(18,5))\nfor targCluster in range(1,1+n_clusters):\n    #pl.subplot(n_clusters,1,targCluster)\n    subds = ds_sm[lbl_exclude_states][clustering==targCluster]\n    subds = (subds - np.nanmean(subds))\/np.nanstd(subds) #scaling for visualization purpose\n    pl.errorbar(range(n_days),np.mean(subds,0),yerr=np.std(subds,axis=0)\/np.sqrt(subds.shape[0]),\\\n                linewidth=2,label = clusterNames[targCluster-1])\npl.xticks(np.arange(0,n_days,10),dates[np.arange(0,n_days,10)])\npl.axvspan(xmin=time_period[0], xmax=time_period[1],color='gray',alpha=.16)#marked the time period used for clustering\npl.legend()\npl.xlabel('Date')\npl.ylabel('Normalized confirmed case N')\npl.show()","88f07400":"# 1. Preparation and having a glance of the overall data","e94cc976":"# 3. Visualize the mean time-course of each cluster of states\n### Note that the absolute numbers (i.e., y-axis) are not of interest here (thus the values are scaled).\n### The focus is the trend along the time of each cluster.","33197931":"## Aim: Exploring the temporal and topographic pattern of Covid19 confirmed cases in the US\n### 1. How the case numbers fluctuate in the US across the states?\n### 2. How are the states related to each other in terms of their case fluctuation?\n\n## Methods:\n1. Calculate covariation of the fluctuation since Mid Feb to present among all the states (Pairwise Pearson correlation matrix)\n2. Perform clustering on the covariation pattern to group the states into clusters\n\n## Results\n### There are several temporal patterns across the Covid19 case fluctuation in the US (50 states + D.C.), e.g.,\n    * NY and neighbour stats saw early high rise and has been okay since (e.g., NY, CT)\n    * The South saw igh rise during the summer (e.g., AZ, FL)\n    * Mid-atlantic has been stably high to moderate except a short period in June (e.g., MD)\n    * Of couser it has been high everywhere recently...\n    * Hawaii is by its own, with high rise around August and hasn't been affected too much by the recent surge.\n    \n## Conclusion\n* States may be grouped by geography and\/or similar socioeconomic status.\n* It would be intersting to see how things are going to unfold in the future.","56ace3bd":"# 2. Clustering the states based on their pairwise correlation \n** Use Pearson correlation to make it scale invariant, since the numbers are not normalized by population.\n* Selecting a time-period by specifying start and end day\n* Excluding some states\/regions: here I chose to exclude the territories due to potential unreliability"}}