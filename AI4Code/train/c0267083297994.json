{"cell_type":{"cb540e3d":"code","18d6683b":"markdown","5b6068f4":"markdown"},"source":{"cb540e3d":"import cv2 \nfrom skimage.color import rgb2gray\nfrom skimage import filters\nimport numpy as np\nfrom urllib.request import urlopen\n\nvideourl = urlopen(\"https:\/\/github.com\/code2k13\/motiondetection\/raw\/master\/processed\/ciliate_processed.mp4\")\nwith open('ciliate.mp4','wb') as output:\n  output.write(videourl.read())\n\n\nq = 12  # use higher q for detecting larger objects.\nframe_ctr,fps,width,height = 0,0,0,0\ninput_file = \"ciliate.mp4\"\noutput_file = \"ciliate_processed.mp4\"\ncap = cv2.VideoCapture(input_file) \nif cap.isOpened(): \n    width  = cap.get(3) \n    height = cap.get(4) \n    fps = cap.get(cv2.CAP_PROP_FPS)\n    \n\nfourcc = cv2.VideoWriter_fourcc('m','p','4','v') #change this for other video formats\nout = cv2.VideoWriter(output_file,fourcc , int(fps), (int(width),int(height)))\n\ncolor = (255, 0, 0) \nsuccess = True\nim_old = None\nwhile success:\n    success,im = cap.read() \n\n    if not success:\n        break\n\n    if im_old is not None :         \n        image = np.copy(im)   \n        im = rgb2gray(im)       \n        im_old = rgb2gray(im_old)\n        frame_diff = cv2.absdiff(im,im_old)        \n        frame_diff[frame_diff < frame_diff[np.nonzero(frame_diff)].mean()] = 0\n        frame_diff[frame_diff > frame_diff[np.nonzero(frame_diff)].mean()] = 1\n        frame_diff = filters.sobel(frame_diff)\n        frame_diff = np.stack((frame_diff,)*3, axis=-1)*255      \n         \n        diff_image = cv2.cvtColor(frame_diff.astype('uint8'), cv2.COLOR_BGR2GRAY)\n        ret, thresh = cv2.threshold(diff_image, 0, 255, cv2.THRESH_BINARY)\n        contours, hierarchy = cv2.findContours(thresh,cv2.RETR_TREE,cv2.CHAIN_APPROX_SIMPLE)\n \n        for idx,c in enumerate(contours):\n            if hierarchy[0][idx][3] == -1 :\n                x,y,w,h = cv2.boundingRect(c)\n                if w*h <= q**2:\n                    continue\n                image = cv2.rectangle(image, (x,y),(x+w,y+h), color, 1)\n        out.write(image)\n    im_old = im\n    frame_ctr = frame_ctr + 1\n    \n    if frame_ctr%30 == 0:\n        print(\"Frames processed so far :\",frame_ctr)\n\ncap.release()\nout.release()\ncv2.destroyAllWindows()\nprint(\"Motion detection complete !\")","18d6683b":"Last year, I was introduced to a wonderful scientific instrument called the [\u2018Foldscope\u2019](https:\/\/www.foldscope.com\/). I have spent hours observing things with it. One of my favorite past times is to observe ciliates using the Foldscope. Ciliates are very simple single cell organisms which are easy to find and come in numerous shapes and sizes. Most ciliates move very fast, and you need some skill with a microscope to follow them on the slide. This inspired me to write some code that could detect moving objects in a video and draw rectangles around them. Amazingly, I believe I was able to do a decent job with under 60 lines of python code.\n\nIn this post I will discuss concepts which I used for detecting moving objects and how the work together to come up with the end results.\n\n### Reading videos with Python and OpenCV\nThe first thing we need to be able to do is load frames one by one from a video. OpenCV makes this task very easy. OpenCV has a very convenient function called \u2018cv2.VideoCapture\u2019 which returns an object that can be used to find out information about the video (like width, height, frame rate). The same object allows us to read a single frame from the video by calling \u2018read()\u2019 method on it. The \u2018read()\u2019 method returns two values, a boolean indicating success of the operation and the frame as an image.\n\n```python\ncap = cv2.VideoCapture(\"video_input.mp4\") \nif cap.isOpened(): \n    width  = cap.get(3) # float\n    height = cap.get(4) # float\n    fps = cap.get(cv2.CAP_PROP_FPS)\n```\nThe full video can be read frame by frame using following code:\n\n```python\nsuccess = True\nwhile success:\n    success,frame = cap.read() \n    if not success:\n        break\n    #do something with the frame\n```\n\n### Writing videos using OpenCV\nWriting videos with OpenCV is also very easy. Similar to \u2018VideoCapture\u2019 function, the  \u2018VideoWriter\u2019 function can be used to write video , frame by frame. This function expects path of output video, codec information, frames per second, width and height of output video as parameters.\n\n```python\nfourcc = cv2.VideoWriter_fourcc('m','p','4','v')\nout = cv2.VideoWriter('video_output.mp4',fourcc , int(fps), (int(width),int(height)))\n```\nWriting a frame to the video is as easy as calling:\n\n```python\nout.write(image_obj)\n```\n### Finding frame difference\n\n[![IMAGE ALT TEXT HERE](https:\/\/img.youtube.com\/vi\/PdOXJt3mMD0\/0.jpg)](https:\/\/www.youtube.com\/watch?v=PdOXJt3mMD0)\n*The above video was generated out of frame diffs from the original video*\n\nImages are represented as matrices in the memory. OpenCV has a function called \u2018cv2.absdiff()\u2019 which can be used to calculated absolute difference of two images. This is the basis of our motion detection. We are relying on the fact that when something in the video moves \u2018absdiff \u2018 will be non zero for those pixels. However if something is stationary and has not moved in two consequent frames, the absdiff will be zero. So, as we read the video frame by frame, we compare current frame with older frame and calculate absdiff matrix.\n\nSounds easy , right ? But there are some problems with this approach. Firstly, cameras and software produce artifacts when they capture and encode videos. Such artifacts give us non-zero diff even when the object is stationary .  Uneven lighting and focusing can also cause non-zero diffs for stationary portions of videos.\n\nAfter experimenting with some approaches, I found out that thresholding the diff image using mean values works very well to eliminate such \u2018noise\u2019\n```python\nframe_diff[frame_diff < frame_diff[np.nonzero(frame_diff)].mean()] = 0\nframe_diff[frame_diff > frame_diff[np.nonzero(frame_diff)].mean()]\n```\n### Using edge detection to improve accuracy\n[![IMAGE ALT TEXT HERE](https:\/\/img.youtube.com\/vi\/gkm9Ch0u-XU\/0.jpg)](https:\/\/www.youtube.com\/watch?v=gkm9Ch0u-XU)\n*Edge detection using \u2018Sobel\u2019 filter performed on the frame diff video*\n\nAs microorganisms move, they push matter around them, which gives positive pixels after diff-ing. But we want to differentiate micro-organisms from other things. Also focusing plays an important part here. Generally a lot of  out of focus moving objects will also give positive frame differences. Mostly these are blurred objects which we simply want to ignore. This is where edge detection comes into play, to find edges and borders in the image. This can be easily achieved by using \u2018Sobel\u2018 filter from scikitlearn package.\n```python\nfrom skimage import filters\noutput = filters.sobel(frame_diff)\n```\n### Using contour detection to detect objects\n\n[![IMAGE ALT TEXT HERE](https:\/\/img.youtube.com\/vi\/WvaO9ieQTyM\/0.jpg)](https:\/\/www.youtube.com\/watch?v=WvaO9ieQTyM)\n*Video generated after performing contour detection on the Sobel filter output*\n\nMost protozoans like ciliates will not always show a clear border (because they are mostly transparent). So when we use edge detection to detect shapes\/outlines of moving objects, we get broken edges. In my experience contour detection works very well to group such broken edges and generate a more continuous border.\n\n```python\ncontours, hierarchy = cv2.findContours(thresh,cv2.RETR_TREE,cv2.CHAIN_APPROX_SIMPLE)\n```\n\nOpenCV has built-in functions to find contours. The best part is, the function is able to find nested contour structures and return a hierarchy. The \u2018cv2.findCountours\u2018 function returns a hierarchy of contours. We only consider top level contours (who don\u2019t have a parent). If, for  a contour \u2018idx\u2018 , \u2018hierarchy[0][idx][3]\u2018 returns -1, it means that it is a top level contour and it does not have any parent. Everything else we ignore.\n\n### Drawing bounding boxes around objects\n\n[![IMAGE ALT TEXT HERE](https:\/\/img.youtube.com\/vi\/Vcz49aZtVZQ\/0.jpg)](https:\/\/www.youtube.com\/watch?v=Vcz49aZtVZQ)\n*Video generated after performing contour detection on the Sobel filter output*\n\nCreating boxes around contours can require a bit of math. Luckily OpenCV has a convenient function \u2018cv2.boundingRect\u2018 which returns center coordinates, width and height of bounding rectangle around a given contour. Once we have that, a rectangle on our frame can simply be drawn using cv2.rectangle function. We can pass the color and border-width when drawing the rectangle to this function.\n\n```python\nif hierarchy[0][idx][3]== -1 :\n    x,y,w,h = cv2.boundingRect(contour)\n    if (w*h <= (q)**2):\n        continue\n    image = cv2.rectangle(image, (x,y),(x+w,y+h), color, 1)\n```\n\n### The concept of \u2018q\u2019\n\nLike I explained earlier, videos taken using a microscope can be messy. There can be a lot going on. We may be only interested in detecting objects of a certain size. This is where I have introduced a parameter called \u2018q\u2019. This parameter was used for altering settings for various filters I experimented with. Currently this is only used to filter out bounding rects which are smaller than q^2 in area. You should experiment with different values of \u2018q\u2019 , depending on the resolution of your video and size of objects you are interested in.\n\n### Things to improve\nI want to make this approach fast enough so that it can run in real time. Also it would be nice if I can get this ported to a mobile phone. I also plan to experiment with ML based segmentation techniques for better detection.\n\n### Full code\nThe full code is given below it is also availbale on GitHub Full code\nhttps:\/\/github.com\/code2k13\/motiondetection","5b6068f4":"> Note: I as not able to find way to embed videos which I had generated painstakingly to explain some concepts. So I have added images of the videos instead. Clicking on the images will open youtube video on new tab.\n\n[![IMAGE ALT TEXT HERE](https:\/\/img.youtube.com\/vi\/SDsHBp8nBEQ\/0.jpg)](https:\/\/www.youtube.com\/watch?v=SDsHBp8nBEQ)\n\n"}}