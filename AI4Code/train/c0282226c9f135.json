{"cell_type":{"73cbc772":"code","d24f014b":"code","e7b81daa":"code","f28312d2":"code","8e8f0a30":"code","8b819781":"code","346d0d00":"code","2dca36f8":"code","4ed282d6":"code","5e44c361":"code","ed741f16":"code","c31dcbcd":"code","21377bf8":"code","510a62d6":"code","7489983d":"code","f3d11fd9":"code","cc18dd0c":"code","cf1a088b":"code","a057bb9b":"code","9bcd7b2c":"code","4bc05dbc":"code","f4703751":"code","d556460b":"code","27dc2010":"code","ec5cf89c":"code","13198734":"code","96b3543b":"code","7c73e03f":"code","aecde78c":"code","95d14c85":"code","a53a95f8":"code","5eeb9673":"code","a5e9cce4":"code","739e4059":"code","8bcf98a6":"code","67daf7b5":"code","6ba382f7":"code","3f955e03":"code","a04e0238":"code","10f54194":"markdown","1591c54c":"markdown","71cc74a3":"markdown","4fac2a5d":"markdown","bf6b72c1":"markdown","1569472d":"markdown","f2e0e3f1":"markdown","b489305f":"markdown","1846abb5":"markdown","b99d54f3":"markdown","d97af6ab":"markdown","033efec4":"markdown","7f37f8ba":"markdown","bd729603":"markdown","300cb615":"markdown","f81cf304":"markdown","784defe3":"markdown","b1335c25":"markdown","158316cf":"markdown"},"source":{"73cbc772":"import nltk\nnltk.download('popular') #Downloading popular packages","d24f014b":"dir(nltk)","e7b81daa":"from nltk.corpus import stopwords\n\nstopwords.words('english')[0:500:25]","f28312d2":"# Read in the raw text\nrawData = open(\"..\/input\/SMSSpamCollectionDataset\/SMSSpamCollection.tsv\").read()\n\n# Print the raw data\nrawData[0:500]","8e8f0a30":"parsedData = rawData.replace('\\t', '\\n').split('\\n')","8b819781":"parsedData[0:5]","346d0d00":"labelList = parsedData[0::2]\ntextList = parsedData[1::2]","2dca36f8":"print(labelList[0:5])\nprint(textList[0:5])","4ed282d6":"import pandas as pd\n\nfullCorpus = pd.DataFrame({\n    'label': labelList,\n    'body_list': textList\n})\n\nfullCorpus.head()","5e44c361":"print(len(labelList))\nprint(len(textList))","ed741f16":"print(labelList[-5:])","c31dcbcd":"fullCorpus = pd.DataFrame({\n    'label': labelList[:-1],\n    'body_list': textList\n})\n\nfullCorpus.head()","21377bf8":"dataset = pd.read_csv(\"..\/input\/SMSSpamCollectionDataset\/SMSSpamCollection.tsv, sep=\"\\t\", header=None)\ndataset.head()","510a62d6":"import pandas as pd\n\nfullCorpus = pd.read_csv(\"..\/input\/SMSSpamCollectionDataset\/SMSSpamCollection.tsv\", sep='\\t', header=None)\nfullCorpus.columns = ['label', 'body_text']\n\nfullCorpus.head()","7489983d":"# What is the shape of the dataset?\n\nprint(\"Input data has {} rows and {} columns\".format(len(fullCorpus), len(fullCorpus.columns)))","f3d11fd9":"# How many spam\/ham are there?\n\nprint(\"Out of {} rows, {} are spam, {} are ham\".format(len(fullCorpus),\n                                                       len(fullCorpus[fullCorpus['label']=='spam']),\n                                                       len(fullCorpus[fullCorpus['label']=='ham'])))","cc18dd0c":"# How much missing data is there?\n\nprint(\"Number of null in label: {}\".format(fullCorpus['label'].isnull().sum()))\nprint(\"Number of null in text: {}\".format(fullCorpus['body_text'].isnull().sum()))","cf1a088b":"import re\n\nre_test = 'This is a made up string to test 2 different regex methods'\nre_test_messy = 'This      is a made up     string to test 2    different regex methods'\nre_test_messy1 = 'This-is-a-made\/up.string*to>>>>test----2\"\"\"\"\"\"different~regex-methods'","a057bb9b":"re.split('\\s', re_test)","9bcd7b2c":"re.split('\\s', re_test_messy)","4bc05dbc":"re.split('\\s+', re_test_messy)","f4703751":"re.split('\\s+', re_test_messy1)","d556460b":"re.findall('\\S+', re_test)","27dc2010":"re.findall('\\S+', re_test_messy)","ec5cf89c":"re.findall('\\S+', re_test_messy1)","13198734":"re.findall('\\w+', re_test_messy1)","96b3543b":"pep8_test = 'I try to follow PEP8 guidelines'\npep7_test = 'I try to follow PEP7 guidelines'\npeep8_test = 'I try to follow PEEP8 guidelines'\n","7c73e03f":"import re\n\nre.findall('[a-z]+', pep8_test)","aecde78c":"re.findall('[A-Z]+', pep8_test)","95d14c85":"re.sub('[A-Z]+[0-9]+', 'PEP8 Python Styleguide', peep8_test)","a53a95f8":"import pandas as pd\npd.set_option('display.max_colwidth', 100)\n\ndata = pd.read_csv(\"..\/input\/SMSSpamCollectionDataset\/SMSSpamCollection.tsv\", sep='\\t', header=None)\ndata.columns = ['label', 'body_text']\n\ndata.head()","5eeb9673":"# What does the cleaned version look like?\ndata_cleaned = pd.read_csv(\"..\/input\/SMSSpamCollectionDataset\/SMSSpamCollection_cleaned.tsv\", sep='\\t')\ndata_cleaned.head()","a5e9cce4":"import string\nstring.punctuation","739e4059":"\"I like NLP.\" == \"I like NLP\"","8bcf98a6":"def remove_punct(text):\n    text_nopunct = \"\".join([char for char in text if char not in string.punctuation])\n    return text_nopunct\n\ndata['body_text_clean'] = data['body_text'].apply(lambda x: remove_punct(x))\n\ndata.head()","67daf7b5":"import re\n\ndef tokenize(text):\n    tokens = re.split('\\W+', text)\n    return tokens\n\ndata['body_text_tokenized'] = data['body_text_clean'].apply(lambda x: tokenize(x.lower()))\n\ndata.head()","6ba382f7":"'NLP' == 'nlp'","3f955e03":"import nltk\n\nstopword = nltk.corpus.stopwords.words('english')\nprint(stopword)","a04e0238":"def remove_stopwords(tokenized_list):\n    text = [word for word in tokenized_list if word not in stopword]\n    return text\n\ndata['body_text_nostop'] = data['body_text_tokenized'].apply(lambda x: remove_stopwords(x))\n\ndata.head()","10f54194":"##Exploring the dataset","1591c54c":"### Read in semi-structured text data","71cc74a3":"## Reading the data","4fac2a5d":"### Tokenization","bf6b72c1":"### Download NLTK data","1569472d":"### Remove punctuation","f2e0e3f1":"### What can you do with NLTK?","b489305f":"### Splitting a sentence into a list of words","1846abb5":"### Using regular expressions in Python\n\nPython's `re` package is the most commonly used regex resource. More details can be found [here](https:\/\/docs.python.org\/3\/library\/re.html).","b99d54f3":"### How to install NLTK on your local machine\n\nBoth sets of instructions below assume you already have Python installed. These instructions are taken directly from [http:\/\/www.nltk.org\/install.html](http:\/\/www.nltk.org\/install.html).\n\n**Mac\/Unix**\n\nFrom the terminal:\n1. Install NLTK: run `pip install -U nltk`\n2. Test installation: run `python` then type `import nltk`\n\n**Windows**\n\n1. Install NLTK: [http:\/\/pypi.python.org\/pypi\/nltk](http:\/\/pypi.python.org\/pypi\/nltk)\n2. Test installation: `Start>Python35`, then type `import nltk`","d97af6ab":"### Remove stopwords","033efec4":" ## Learning how to use regular expressions","7f37f8ba":"## Implementing a pipeline to clean text","bd729603":"### Read in text data","300cb615":"### Explore the dataset","f81cf304":"### Other examples of regex methods\n\n- re.search()\n- re.match()\n- re.fullmatch()\n- re.finditer()\n- re.escape()","784defe3":"### Pre-processing text data\n\nCleaning up the text data is necessary to highlight attributes that you're going to want your machine learning system to pick up on. Cleaning (or pre-processing) the data typically consists of a number of steps:\n1. **Remove punctuation**\n2. **Tokenization**\n3. **Remove stopwords**\n4. Lemmatize\/Stem\n\nThe first three steps are covered in this chapter as they're implemented in pretty much any text cleaning pipeline. Lemmatizing and stemming are covered in the next chapter as they're helpful but not critical.","b1335c25":"### Replacing a specific string","158316cf":"# Spam Ham Classifier using Natural Language Processing\n### This code snippets will contain the basics understanding of the NLP to design a Ham Spam Classifier."}}