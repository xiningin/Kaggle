{"cell_type":{"efe5471f":"code","e56a809e":"code","613d65f8":"code","9398feca":"code","86baba20":"code","3b3614ed":"code","98265be7":"code","fc65b9e2":"code","78412742":"code","94699a04":"code","00b4ab71":"code","ca67341c":"code","b6b7b268":"code","f7cc0d35":"code","df8c0fe8":"code","14fa0e6a":"code","a73a7994":"code","9fa9f88e":"code","d51987d1":"code","b4a24557":"code","510ffaf3":"code","457f5357":"code","e3232c6a":"code","fc54d0df":"code","c8d752ad":"code","788b9d53":"code","342d5161":"code","838e23ac":"code","b385ba7e":"code","1388adfb":"code","7843730c":"code","12ded54f":"code","6a679338":"code","b4b485a1":"code","4218a84f":"code","c7172087":"code","2dadb398":"code","8b550bde":"code","043ab15b":"code","2aebfe85":"code","7ec4f35c":"code","272470e3":"code","d8f67c08":"code","da6bcc49":"code","af6da7c2":"code","9e1c9e3b":"code","03c5140e":"code","9228b327":"code","3bd7a900":"code","6f3c48d3":"code","1e063c44":"code","36c4a998":"code","593a8de2":"code","497fd752":"code","0f670414":"code","3a243246":"code","d781d626":"code","39ffe492":"code","fc9d401d":"code","625cba8c":"code","8c2aea43":"code","17ea103b":"code","6093b909":"code","10262d88":"code","908e2188":"code","eb81501d":"code","6675f59c":"code","6a630e14":"code","779bb8cc":"code","57fa43bb":"code","8289ee6e":"code","53b18f9c":"code","abfccbd6":"code","fdb85757":"code","26f2c4ca":"code","8985c70c":"code","a0e77cf9":"code","6f99b142":"code","9f131fef":"code","feb40daa":"code","2d2e4060":"code","8c82dc36":"code","4046fd59":"code","0e197cb5":"code","fc39e03b":"markdown","58222e85":"markdown","1ef6d511":"markdown","498081a9":"markdown","e224d3ff":"markdown","71ac8cd0":"markdown","20a2adb0":"markdown","2cc73eeb":"markdown","41e59ee6":"markdown","57273ef9":"markdown","50a17142":"markdown","36f11b02":"markdown","b9737285":"markdown","eca81242":"markdown","0b251ee4":"markdown","02ae5526":"markdown","b71c5805":"markdown","e10c7341":"markdown","27d2af2e":"markdown","72831030":"markdown","03fb4baf":"markdown","fb89378b":"markdown","287f18ff":"markdown","c51f5844":"markdown","4cff1622":"markdown","7bd977fd":"markdown","43dc0754":"markdown","4d68454e":"markdown"},"source":{"efe5471f":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","e56a809e":"# \ud328\ud0a4\uc9c0 \ubd88\ub7ec\uc624\uae30\nimport pandas as pd # \ubd84\uc11d\nimport matplotlib.pyplot as plt # \uc2dc\uac01\ud654\nimport seaborn as sns # \uc2dc\uac01\ud654\nimport numpy as np # \ubd84\uc11d\nfrom scipy.stats import norm # \ubd84\uc11d\nfrom sklearn.preprocessing import StandardScaler # \ubd84\uc11d\nfrom scipy import stats # \ubd84\uc11d\nimport warnings\nwarnings.filterwarnings('ignore')\n%matplotlib inline\n\nimport gc","613d65f8":"train = pd.read_csv('..\/input\/2019-2nd-ml-month-with-kakr\/train.csv')\ntest = pd.read_csv('..\/input\/2019-2nd-ml-month-with-kakr\/test.csv')\nprint(\"train.csv Shape : \", train.shape)\nprint(\"test.csv Shape : \", test.shape)","9398feca":"train.head(2)","86baba20":"train['price'].describe()","3b3614ed":"# histogram\nf, ax = plt.subplots(figsize = (8,6))\nsns.distplot(train['price'])","98265be7":"#skewness and kurtosis (\uc65c\ub3c4\uc640 \ucca8\ub3c4)\nprint(\"Skewness : %f \" % train['price'].skew())\nprint(\"Kurtosis : %f \" % train['price'].kurt())\n","fc65b9e2":"fig = plt.figure(figsize = (15,10))\n\nfig.add_subplot(1,2,1)\nres = stats.probplot(train['price'], plot = plt)\n\nfig.add_subplot(1,2,2)\nres = stats.probplot(np.log1p(train['price']), plot = plt)","78412742":"train['price'] = np.log1p(train['price'])\n#histogram\nf, ax = plt.subplots(figsize = (8, 6))\nsns.distplot(train['price'])","94699a04":"#saleprice correlation matrix\nk = 10 # \ud788\ud2b8\ub9f5 \ubcc0\uc218 \uc218\ncorrmat = abs(train.corr(method = \"spearman\"))\ncols = corrmat.nlargest(k, 'price').index # nlargest : Return this many descending sorted values\ncm = np.corrcoef(train[cols].values.T) # correlation \ud2b9\uc815 \uceec\ub7fc\uc5d0 \ub300\ud574\uc11c\nsns.set(font_scale = 1.25)\nf, ax = plt.subplots(figsize = (18,8))\nhm = sns.heatmap(cm, cbar = True, annot = True, square = True,\n                fmt = '.2f',annot_kws = {'size' : 8}, yticklabels = cols.values,\n                xticklabels = cols.values)\nplt.show()","00b4ab71":"# \uc0c1\uad00\uacc4\uc218\uac00 \uac00\uc7a5 \ub0ae\uc74c 10\uac1c \ubc18\uc751\ubcc0\uc218 \ncols = corrmat.nsmallest(k, 'price').index # nsmallest : Return this many descending sorted values\ncm = np.corrcoef(train[cols].values.T) # correlation \ud2b9\uc815 \uceec\ub7fc\uc5d0 \ub300\ud574\uc11c\nsns.set(font_scale = 1.25)\nf, ax = plt.subplots(figsize = (18,8))\nhm = sns.heatmap(cm, cbar = True, annot = True, square = True,\n                fmt = '.2f',annot_kws = {'size' : 8}, yticklabels = cols.values,\n                xticklabels = cols.values)\nplt.show()","ca67341c":"data = pd.concat([train['price'], train['grade']], axis = 1)\nf, ax = plt.subplots(figsize = (8,6))\nfig = sns.boxplot(x = 'grade', y = 'price', data = data)","b6b7b268":"data = pd.concat([train['price'], train['sqft_living']], axis = 1)\nf, ax = plt.subplots(figsize = (8,6))\nfig = sns.regplot(x = 'sqft_living', y = 'price', data = data)","f7cc0d35":"data = pd.concat([train['price'], train['sqft_living15']], axis = 1)\nf, ax = plt.subplots(figsize = (8,6))\nfig = sns.regplot(x = 'sqft_living15', y = 'price', data = data)","df8c0fe8":"data = pd.concat([train['price'], train['sqft_above']], axis = 1)\nf, ax = plt.subplots(figsize = (8,6))\nfig = sns.regplot(x = 'sqft_above', y = 'price', data = data)","14fa0e6a":"data = pd.concat([train['price'], train['lat']], axis = 1)\nf, ax = plt.subplots(figsize = (8,6))\nfig = sns.regplot(x = 'lat', y = 'price', data = data)","a73a7994":"train_latlong = pd.concat([train['price'], train['long'], train['lat']], axis = 1)\ntrain_latlong['price'] = np.log1p(train_latlong['price'])\ntrain_latlong['long'] = np.log1p(abs(train_latlong['long']))\ntrain_latlong['lat'] = np.log1p(train_latlong['lat'])\n","9fa9f88e":"f, ax = plt.subplots(figsize = (8,6))\nfig = sns.regplot(x = 'lat', y = 'price', data = train_latlong)","d51987d1":"f, ax = plt.subplots(figsize = (8,6))\nfig = sns.regplot(x = 'long', y = 'price', data = train_latlong)","b4a24557":"data = pd.concat([train['price'], train['long']], axis = 1)\nf, ax = plt.subplots(figsize = (8,6))\nfig = sns.regplot(x = 'long', y = 'price', data = data)","510ffaf3":"test.head()","457f5357":"data = pd.concat([train['price'], train['bathrooms']], axis = 1)\nf, ax = plt.subplots(figsize = (18,6))\nfig = sns.boxplot(x = 'bathrooms', y = 'price', data = data)","e3232c6a":"data = pd.concat([train['price'], train['bedrooms']], axis = 1)\nf, ax = plt.subplots(figsize = (18,6))\nfig = sns.boxplot(x = \"bedrooms\", y= \"price\", data =data)","fc54d0df":"train.isnull().sum()","c8d752ad":"test.isnull().sum()","788b9d53":"def generate_color():\n    color = '#{:02x}{:02x}{:02x}'.format(*map(lambda x: random.randint(0, 255), range(3)))\n    return color\n","342d5161":"import plotly.graph_objs as go\nimport random\nimport plotly.offline as py\nfrom plotly import tools\npy.init_notebook_mode(connected=True)\nfrom sklearn import model_selection, preprocessing, metrics, ensemble, naive_bayes, linear_model\nfrom sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\nfrom sklearn.decomposition import TruncatedSVD\nimport time\n\ntrain_unique = []\ncolumns = ['bedrooms', 'bathrooms','floors','waterfront','view',\n          'condition','grade']\n\nfor i in columns :\n    train_unique.append(len(train[i].unique()))\n\nunique_train = pd.DataFrame()\nunique_train['Columns'] = columns\nunique_train['Unique_value'] = train_unique\n\ndata = [\n    go.Bar(\n    x = unique_train['Columns'],\n    y = unique_train['Unique_value'],\n    name = 'Unique value in features',\n    textfont = dict(size = 20),\n    marker = dict(\n    line = dict(\n    color = generate_color(),\n    #width = 2,\n    ), opacity = 0.45\n    )\n    ),\n]\n\nlayout = go.Layout(title = \" Unique Value By Column\",\n                  xaxis = dict(title = 'Columns', ticklen = 5,\n                              zeroline = False, gridwidth = 2),\n                  yaxis = dict(title = 'Value Count',\n                              ticklen = 5, gridwidth = 2),\n                  showlegend = True)\nfig = go.Figure(data = data, layout = layout)\npy.iplot(fig, filename = 'skin')","838e23ac":"data = pd.concat([train['price'], train['sqft_living']], axis=1)\nf, ax = plt.subplots(figsize=(8, 6))\nfig = sns.regplot(x='sqft_living', y=\"price\", data=data)","b385ba7e":"train.loc[train['sqft_living'] > 13000]","1388adfb":"test.loc[test['sqft_living'] > 13000]","7843730c":"train = train.loc[train['id'] != 8912]","12ded54f":"data = pd.concat([train['price'], train['grade']], axis=1)\nf, ax = plt.subplots(figsize=(8, 6))\nfig = sns.boxplot(x='grade', y=\"price\", data=data)","6a679338":"train.loc[(train['price'] > 14.7) & (train['grade'] == 8)]","b4b485a1":"train.loc[(train['price'] > 15.5) & (train['grade'] == 11)]","4218a84f":"train = train.loc[train['id'] != 7173]\ntrain = train.loc[train['id'] != 2775]","c7172087":"data = pd.concat([train['price'], train['bedrooms']], axis=1)\nf, ax = plt.subplots(figsize=(8, 6))\nfig = sns.boxplot(x='bedrooms', y=\"price\", data=data)","2dadb398":"train.loc[train['bedrooms'] >= 10]","8b550bde":"## \ud14c\uc2a4\ud2b8\ub3c4 \ud655\uc778\ntest.loc[test['bedrooms'] >= 10]","043ab15b":"train_latlong = pd.concat([train['price'], train['long'], train['lat']], axis = 1)\ntrain_latlong['price'] = np.log1p(train_latlong['price'])\ntrain_latlong['long'] = np.log1p(abs(train_latlong['long']))\n","2aebfe85":"f, ax = plt.subplots(figsize = (8,6))\nfig = sns.regplot(x = 'long', y = 'price', data = train_latlong)","7ec4f35c":"train.loc[np.log1p(abs(train['long'])) < 4.807]","272470e3":"test.loc[np.log1p(abs(test['long'])) < 4.807]","d8f67c08":"train = train.loc[train['long'] != -121.359]\ntrain = train.loc[train['long'] != -121.315]\ntrain = train.loc[train['long'] != -121.352]\ntrain = train.loc[train['long'] != -121.319]\ntrain = train.loc[train['long'] != -121.316]\ntrain = train.loc[train['long'] != -121.321]\ntrain = train.loc[train['long'] != -121.325]","da6bcc49":"for df in [train, test] :\n    df['date'] = df['date'].apply(lambda x : x[0:8])\n    df['yr_renovated'] = df['yr_renovated'].apply(lambda x : np.nan if x == 0 else x)\n    df['yr_renovated'] = df['yr_renovated'].fillna(df['yr_built'])\n    df['month'] = df['date'].apply(lambda x : x[4:6])\n    df['year'] = df['date'].apply(lambda x : x[0:4])\n    df['day'] = df['date'].apply(lambda x : x[6:8])\n","af6da7c2":"del train['date']\ndel test['date']","9e1c9e3b":"train.info()","03c5140e":"for df in [train, test]:\n    df['total_rooms'] = df['bedrooms'] + df['bathrooms']\n    df['grade_condition'] = df['grade'] * df['condition']\n    df['sqft_total'] = df['sqft_living'] + df['sqft_lot']\n    df['sqft_total_size'] = df['sqft_living'] + df['sqft_lot'] + df['sqft_above'] + df['sqft_basement']\n    df['sqft_total15'] = df['sqft_living15'] + df['sqft_lot15'] \n    df['is_renovated'] = df['yr_renovated'] - df['yr_built']\n    df['is_renovated'] = df['is_renovated'].apply(lambda x: 0 if x == 0 else 1)\n    df['abslong'] = abs(df['long'])\n    df['month'] = df['month'].astype('int')\n    df['floor_grade'] = df['floors'] * df['grade']\n    df['floor_view'] = df['floors'] + df['view']\n    df['grade_bed'] = df['grade'] * df['bathrooms']\n    df['long_lat'] = df['long'] + df['lat']\n    df['longlat'] = df['long'] * df['lat']\n    df['view_water'] = df['view'] + df['waterfront']\n    df['long_bathroom'] = df['long'] * df['bathrooms']\n    df['year'] = df['year'].astype('int')\n    df['day'] = df['day'].astype('int')","9228b327":"from sklearn.linear_model import LinearRegression\nfrom sklearn.model_selection import KFold, cross_val_score\nfrom sklearn.model_selection import cross_val_predict\nfrom sklearn.preprocessing import RobustScaler\nfrom sklearn.pipeline import make_pipeline\n\nfrom sklearn.linear_model import RidgeCV\n\ny_reg = train['price']\ndel train['price']\ndel train['id']\ntest_id = test['id']\ndel test['id']\n\nkfolds = KFold(n_splits=5, shuffle=True, random_state=42)\n\ndef cv_rmse(model):\n    rmse = np.sqrt(-cross_val_score(model, train, y_reg, \n                                   scoring=\"neg_mean_squared_error\", \n                                   cv = kfolds))\n    return(rmse)\n\ndef ridge_selector(k):\n    ridge_model = make_pipeline(RobustScaler(),\n                                RidgeCV(alphas = [k],\n                                        cv=kfolds)).fit(train, y_reg)\n    \n    ridge_rmse = cv_rmse(ridge_model).mean()\n    return(ridge_rmse)\n\nr_alphas = [.0001, .0003, .0005, .0007, .0009, \n          .01, 0.05, 0.1, 0.3, 1, 3, 5,6,7,8,9,10]\n\nridge_scores = []\nfor alpha in r_alphas:\n    score = ridge_selector(alpha)\n    ridge_scores.append(score)\n    \nplt.plot(r_alphas, ridge_scores, label='Ridge')\nplt.legend('center')\nplt.xlabel('alpha')\nplt.ylabel('score')\n","3bd7a900":"alphas_alt = [5.8,5.9,6,6.1,6.2,6.3,6.4,6.5,6.6,6.7,6.8,6.9,7]\n\nridge_model2 = make_pipeline(RobustScaler(),\n                            RidgeCV(alphas = alphas_alt,\n                                    cv=kfolds)).fit(train, y_reg)\n\nprint(\"Ridge rmse : \",cv_rmse(ridge_model2).mean())","6f3c48d3":"print(\"Best of alpha in ridge model :\" ,ridge_model2.steps[1][1].alpha_)","1e063c44":"\nridge_coef = pd.DataFrame(np.round_(ridge_model2.steps[1][1].coef_, decimals=3), \ntest.columns, columns = [\"penalized_regression_coefficients\"])\n# remove the non-zero coefficients\nridge_coef = ridge_coef[ridge_coef['penalized_regression_coefficients'] != 0]\n# sort the values from high to low\nridge_coef = ridge_coef.sort_values(by = 'penalized_regression_coefficients', \nascending = False)\n\n# plot the sorted dataframe\nfig = plt.figure(figsize = (25,25))\nax = sns.barplot(x = 'penalized_regression_coefficients', y= ridge_coef.index , \ndata=ridge_coef)\nax.set(xlabel='Penalized Regression Coefficients')","36c4a998":"## lightgbm\ntrain_columns = [c for c in train.columns if c not in ['id']]","593a8de2":"import lightgbm as lgb\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.model_selection import KFold, cross_val_score\nfrom sklearn.model_selection import cross_val_predict\nfrom sklearn.preprocessing import RobustScaler\nfrom sklearn.pipeline import make_pipeline\n\nfrom sklearn.linear_model import RidgeCV\n\n\n\nparam = {'num_leaves': 31,\n         'min_data_in_leaf': 30, \n         'objective':'regression',\n         'max_depth': -1,\n         'learning_rate': 0.01,\n         \"min_child_samples\": 20,\n         \"boosting\": \"gbdt\",\n         \"feature_fraction\": 0.9,\n         \"bagging_freq\": 1,\n         \"bagging_fraction\": 0.9 ,\n         \"bagging_seed\": 11,\n         \"metric\": 'rmse',\n         \"lambda_l1\": 0.1,\n         \"verbosity\": -1,\n         \"nthread\": 4,\n         \"random_state\": 4950}\n\n#prepare fit model with cross-validation\nfolds = KFold(n_splits=5, shuffle=True, random_state=42)\noof = np.zeros(len(train))\npredictions = np.zeros(len(test))\nfeature_importance_df = pd.DataFrame()\n\n#run model\nfor fold_, (trn_idx, val_idx) in enumerate(folds.split(train)):\n    trn_data = lgb.Dataset(train.iloc[trn_idx][train_columns], label=y_reg.iloc[trn_idx])#, categorical_feature=categorical_feats)\n    val_data = lgb.Dataset(train.iloc[val_idx][train_columns],\n                           label=y_reg.iloc[val_idx])#, categorical_feature=categorical_feats)\n    num_round = 10000\n    clf = lgb.train(param, trn_data, num_round, valid_sets = [trn_data, val_data], \n                    verbose_eval=500, early_stopping_rounds = 100)\n    oof[val_idx] = clf.predict(train.iloc[val_idx][train_columns], \n                               num_iteration=clf.best_iteration)\n    #feature importance\n    fold_importance_df = pd.DataFrame()\n    fold_importance_df[\"Feature\"] = train_columns\n    fold_importance_df[\"importance\"] = clf.feature_importance()\n    fold_importance_df[\"fold\"] = fold_ + 1\n    feature_importance_df = pd.concat([feature_importance_df, \n                                       fold_importance_df], axis=0)\n    #predictions\n    predictions += clf.predict(test[train_columns], num_iteration=clf.best_iteration) \/ folds.n_splits\n    \ncv = np.sqrt(mean_squared_error(oof, y_reg))\nprint(cv)\n\n","497fd752":"##plot the feature importance\ncols = (feature_importance_df[[\"Feature\", \"importance\"]]\n        .groupby(\"Feature\")\n        .mean()\n        .sort_values(by=\"importance\", ascending=False)[:1000].index)\nbest_features = feature_importance_df.loc[feature_importance_df.Feature.isin(cols)]\n\nplt.figure(figsize=(14,26))\nsns.barplot(x=\"importance\", y=\"Feature\", data=best_features.sort_values(by=\"importance\",ascending=False))\nplt.title('LightGBM Features (averaged over folds)')\nplt.tight_layout()\nplt.savefig('lgbm_importances.png')","0f670414":"## xgboost","3a243246":"import pandas as pd\nimport numpy as np\nimport xgboost as xgb\nfrom sklearn import model_selection\nimport warnings \nwarnings.filterwarnings('ignore')\nimport matplotlib.pyplot as plt\nimport seaborn as sns","d781d626":"train = pd.read_csv('..\/input\/2019-2nd-ml-month-with-kakr\/train.csv')\ntest = pd.read_csv('..\/input\/2019-2nd-ml-month-with-kakr\/test.csv')\n","39ffe492":"y_train = train.price\nx_train = train.drop(['id', 'price'], axis=1)\nx_test = test.drop(['id'], axis=1)\nfor df in [x_train,x_test] :\n    df['year'] = df.date.apply(lambda x: x[0:4]).astype(int)\n    df['month'] = df.date.apply(lambda x: x[4:6]).astype(int)\n    df['day'] = df.date.apply(lambda x: x[6:8]).astype(int)\n","fc9d401d":"for df in [x_train, x_test] :\n    df['year_month'] = df['year']*100 + df['month']\n    df['month_day'] = df['month']*100 + df['day']\n    df['ym_freq'] = df.groupby('year_month')['year_month'].transform('count')\n    df['md_freq'] = df.groupby('month_day')['month_day'].transform('count')\n","625cba8c":"def preprocessing(df):\n    # log\n    df.sqft_living2 = np.log(df.sqft_living)\n    df.sqft_lot2 = np.log(df.sqft_lot)\n    df.sqft_above2 = np.log(df.sqft_above)\n    df.sqft_basement2 = np.log(df.sqft_basement)\n    df.sqft_lot152 = np.log(df.sqft_lot15)\n    \n    df['roomsum'] = np.log(df.bedrooms + df.bathrooms)\n    df['roomsize'] = df.sqft_living \/ df.roomsum\n    \n    df['pos'] = df.long.astype(str) + ', ' + df.lat.astype(str)\n    df['density'] = df.groupby('pos')['pos'].transform('count')\n    \n    df = df.drop(['pos'], axis=1)\n    \n    return df\n\nx_train = preprocessing(x_train)\nx_test = preprocessing(x_test)","8c2aea43":"for df in [x_train, x_test]:\n    df['total_rooms'] = df['bedrooms'] + df['bathrooms']\n    df['grade_condition'] = df['grade'] * df['condition']\n    df['sqft_total'] = df['sqft_living'] + df['sqft_lot']\n    df['sqft_total_size'] = df['sqft_living'] + df['sqft_lot'] + df['sqft_above'] + df['sqft_basement']\n    df['sqft_total15'] = df['sqft_living15'] + df['sqft_lot15'] \n    df['is_renovated'] = df['yr_renovated'] - df['yr_built']\n    df['is_renovated'] = df['is_renovated'].apply(lambda x: 0 if x == 0 else 1)\n    df['abslong'] = abs(df['long'])\n    df['month'] = df['month'].astype('int')\n    df['floor_grade'] = df['floors'] * df['grade']\n    df['floor_view'] = df['floors'] + df['view']\n    df['grade_bed'] = df['grade'] * df['bathrooms']\n    df['long_lat'] = df['long'] + df['lat']\n    df['longlat'] = df['long'] * df['lat']\n    df['view_water'] = df['view'] + df['waterfront']\n    df['long_bathroom'] = df['long'] * df['bathrooms']\n    df['year'] = df['year'].astype('int')\n    df['day'] = df['day'].astype('int')\n    \n\ndel x_train['date']\ndel x_test['date']\n","17ea103b":"xgb_params = {\n    'eta': 0.01,\n    'max_depth': 6,\n    'subsample': 0.8,\n    'colsample_bytree': 0.8,\n    'objective': 'reg:linear',\n    'eval_metric': 'rmse',\n    'silent': 1\n}\n\nprint('Transform DMatrix...')\ndtrain = xgb.DMatrix(x_train, y_train)\ndtest = xgb.DMatrix(x_test)\n\nprint('Start Cross Validation...')\n\ncv_output = xgb.cv(xgb_params, dtrain, num_boost_round=5000, early_stopping_rounds=20,verbose_eval=50, show_stdv=False)\ncv_output[['train-rmse-mean', 'test-rmse-mean']].plot()\nprint('best num_boost_rounds = ', len(cv_output))\nrounds = len(cv_output)","6093b909":"model = xgb.train(xgb_params, dtrain, num_boost_round = rounds)\npreds = model.predict(dtest)\n","10262d88":"\nsub = test[['id']]\nsub['price'] = preds\nsub.to_csv('goodnight.csv', index=False)","908e2188":"test_ridge_preds = np.expm1(ridge_model2.predict(test))\n","eb81501d":"test_ensemble_preds = 0.7*test_lgb_preds + 0.3*test_ridge_preds","6675f59c":"\ntest_lgb_preds = np.expm1(predictions)\n","6a630e14":"submission0 = pd.DataFrame({'id': test_id, 'price': test_ridge_preds})\nsubmission0.to_csv('ridge4_.csv', index=False)","779bb8cc":"submission = pd.DataFrame({'id': test_id, 'price': test_lgb_preds})\nsubmission.to_csv('lightgbm12_rate001.csv', index=False)","57fa43bb":"test_xgb_preds = np.expm1(y_pred)\nsubmission2 = pd.DataFrame({'id' : test_id, 'price' : y_pred})\nsubmission2.to_csv('xgboost.csv', index = False)","8289ee6e":"submission1 = pd.DataFrame({'id': test_id, 'price': test_ensemble_preds})\nsubmission1.to_csv('ensemble.csv', index=False)","53b18f9c":"gg = pd.read_csv('gg.csv')\nxgb = pd.read_csv('sub_xgb3.csv')","abfccbd6":"library(xgboost)\nlibrary(ranger)\nlibrary(tictoc)\nlibrary(corrplot)\nlibrary(tidyverse)\nlibrary(gridExtra)\nlibrary(data.table)\nlibrary(caret)","fdb85757":"train <- fread(\"..\/input\/2019-2nd-ml-month-with-kakr\/train.csv\")\ntest <- fread(\"..\/input\/2019-2nd-ml-month-with-kakr\/test.csv\")","26f2c4ca":"train[ , filter:=\"train\"] # train\/test\ub97c \uad6c\ubd84\ud560 key\ubcc0\uc218 \"filter\"\ub97c \ub9cc\ub4ed\ub2c8\ub2e4.\ntest[ , \":=\"(price=-999, filter=\"test\")] # train\/test\ub97c \uad6c\ubd84\ud560 key\ubcc0\uc218 \"filter\"\ub97c \ub9cc\ub4e4\uace0, row bind\ub97c \uc704\ud574 test set\uc5d0 price \ubcc0\uc218\ub97c \ub9cc\ub4e4\uace0 \ubaa8\ub4e0 \uac12\uc744 -999\ub85c \ucc44\uc6b0\uaca0\uc2b5\ub2c8\ub2e4.\nfull <- rbind(train, test)\nfull[,filter:=factor(filter, levels=c(\"train\", \"test\"))]","8985c70c":"## transform to factor type for categorical variables\nfull[, yyyymm:=factor(yyyymm)]\nfull[, zipcode:=factor(zipcode)]\n\ncat_vars <- c(\"waterfront\", \"yyyymm\", \"zipcode\")\ndel_vars <- c(\"id\", \"filter\", \"price\", \"mm\", \"dd\", \"yyyy\", \"yr_built\", \"date\", \"sqft_basement\")\nnum_vars <- setdiff(colnames(full), c(cat_vars, del_vars))\n\n## \uc218\uce58\ud615 \ubcc0\uc218 \ud45c\uc900\ud654\nX_train_num <- full[filter==\"train\",num_vars, with=F]\nX_test_num <- full[filter==\"test\",num_vars, with=F]\n\nmean.tr <- apply(X_train_num, 2, mean)\nsd.tr <- apply(X_train_num, 2, sd)\n\nX_train_num <- scale(X_train_num, center=mean.tr, scale=sd.tr)\nX_test_num <- scale(X_test_num, center=mean.tr, scale=sd.tr)\n\nX_train <- model.matrix(~.-1, data=cbind(X_train_num, full[filter==\"train\", cat_vars, with=F])) \nX_test <- model.matrix(~.-1, data=cbind(X_test_num, full[filter==\"test\", cat_vars, with=F]))\nY_train <- log(full[filter==\"train\", price])","a0e77cf9":"tuneGrid <- expand.grid(\n  max_depth             = c(6, 60),        # default : 6\n  subsample             = c(0.8, 1),       # default : 1\n  colsample_bytree      = c(0.9, 1)        # default : 1\n)\n\nRMSE_exp <- function(preds, dtrain) {\n  labels <- xgboost::getinfo(dtrain, \"label\")\n  err <- sqrt(mean((exp(labels)-exp(preds))^2))\n  return(list(metric = \"RMSE_exp\", value = err))\n}\n\n# put our testing & training data into two seperates Dmatrixs objects\ndtrain <- xgb.DMatrix(data = as.matrix(X_train), label= Y_train)\ndtest <- xgb.DMatrix(data = as.matrix(X_test))\nresults <- list(val_rmse = rep(0, nrow(tuneGrid)),\n                nrounds = rep(0, nrow(tuneGrid)))\nfor (i in 1:nrow(tuneGrid)){\n    params <- list(\n      objective         = \"reg:linear\",\n      metric            = \"rmse\",\n      booster           = \"gbtree\",\n      eta               = 0.01,                         # default : 0.3\n      gamma             = 0,                            # default : 0\n      min_child_weight  = 1,                            # default : 1\n      max_depth         = tuneGrid[i, \"max_depth\"],\n      subsample         = tuneGrid[i, \"subsample\"],\n      colsample_bytree  = tuneGrid[i, \"colsample_bytree\"]\n    )\n   # tic(\"xgbcv\")\n    xgbcv <- xgb.cv(params = params, \n                    data = dtrain, \n                    nfold = 5, \n                    nrounds = 10000,\n                    early_stopping_rounds = 100,\n                    feval = RMSE_exp,\n                    print_every_n = 100,\n                    maximize = F, \n                    seed=42)\n   # toc()\n    results[[\"val_rmse\"]][i] <- unlist(xgbcv$evaluation_log[xgbcv$best_iteration, \"test_RMSE_exp_mean\"])\n    results[[\"nrounds\"]][i] <- xgbcv$best_iteration\n}","6f99b142":"min.index <- which.min(results[[\"val_rmse\"]])\ntuneGrid[min.index, ]","9f131fef":"cbind(tuneGrid, RMSE=unlist(results[[\"val_rmse\"]]))\n","feb40daa":"default_param <- list(\n      objective         = \"reg:linear\",\n      booster           = \"gbtree\",\n      eta               = 0.01,\n      gamma             = 0,\n      min_child_weight  = 1,\n      max_depth         = tuneGrid[min.index, \"max_depth\"],\n      subsample         = tuneGrid[min.index, \"subsample\"],\n      colsample_bytree  = tuneGrid[min.index, \"colsample_bytree\"]\n    )\n\n# train the model using the best iteration found by cross validation\nfit.xgb <- xgb.train(data = dtrain, \n                     params = default_param, \n                     nrounds = results[[\"nrounds\"]][min.index], \n                     seed=42)","2d2e4060":"predictions_xgb <- exp(predict(fit.xgb, dtest)) # need to reverse the log to the real values\nhead(predictions_xgb)","8c82dc36":"submission_xgb <- read.csv('..\/input\/sample_submission.csv')\nsubmission_xgb$price <- predictions_xgb\nwrite.csv(submission_xgb, file='submission_xgb.csv', row.names = F)","4046fd59":"submi = pd.read_csv('..\/input\/submission3\/submission123.csv')\ngoodnight = pd.read_csv('..\/input\/submission3\/goodnight.csv')","0e197cb5":"## \uc559\uc0c1\ube14\nk = (submi + goodnight)\/2\nk.to_csv('dataal.csv', index = False)","fc39e03b":"\uc0c1\uad00\ub3c4\uac00 \ub192\uc740 \ubcc0\uc218\uc5d0 lat\ub77c\ub294 \uc704\ub3c4 \uac12\uc774 \ub4e4\uc5b4\uac10\n\n\ucd94\ud6c4\uc5d0 \ud65c\uc6a9 \uac00\ub2a5\uc131\uc744 \ub0a8\uaca8\ub460","58222e85":"* \ubaa9\uc801\ubcc0\uc218\uc640 \uc0c1\uad00\ub3c4\uac00 \ub192\uc740 \ubcc0\uc218 - \uc9d1\uc758 \ub4f1\uae09, \uc8fc\uac70\uacf5\uac04\uc758 \ud06c\uae30, \uc804\uccb4 \ud06c\uae30 \uc21c.\n* \ubaa9\uc801\ubcc0\uc218\uc640 \uc0c1\uad00\ub3c4\uac00 \ub0ae\uc740 \ubcc0\uc218 - \uc9d1\uc758 \uc804\ubc18\uc801\uc778 \uc0c1\ud0dc, id, \uc804\ubc29\uc5d0 \uac15\uc774 \ud750\ub974\ub294\uc9c0 \uc720\ubb34","1ef6d511":"\ub370\uc774\ud130 \ud0d0\uc0c9\n- \ubb38\uc81c \uc815\uc758\n- \ubcc0\uc218\uc124\uba85\n- \uc2dc\uac01\ud654\n- \uc774\uc0c1\uce58 \ubc0f \uacb0\uce21\uce58 \ud655\uc778\n- \uc720\ub2c8\ud06c\uac1c\uc218\n\n\ub370\uc774\ud130 \uc804\ucc98\ub9ac\n- \uc774\uc0c1\uce58 \ubc0f \uacb0\uce21\uce58 \ucc98\ub9ac\n- \uc815\uaddc\ud654\n\n\ubcc0\uc218\uc0dd\uc131\n\ubaa8\ub378\ub9c1\n- \ud68c\uadc0\n- GBM ( LGB \/ XGB )\n\uc794\ucc28\ubd84\uc11d\n\n## \uc720\uc758\uc0ac\ud56d\n- \uc81c\uac00 R \uacfc python\uc744 \uc11e\uc5b4\uc11c \uc9dc\ub290\ub77c \ucf54\ub4dc\uac00 \uc911\uac04\uc5d0 \uaf2c\uc774\uac70\ub098 \uc5c6\ub294 \ubd80\ubd84\uc774 \uc788\uc744 \uc218 \uc788\uc2b5\ub2c8\ub2e4.\n- \ub2e4\uc74c\ubd80\ud130\ub294 \uc5b8\uc5b4 \ud558\ub098\ub9cc \uac00\uc9c0\uace0 \uc9dc\uc57c\uaca0\uc2b5\ub2c8\ub2e4. (\uc8c4\uc1a1\ud569\ub2c8\ub2e4....)\n- \ucf54\ub4dc \uc77d\ub294\ub370 \uc5b4\ub824\uc6c0\uc774 \uc788\uc744\ud14c\ub2c8 \ubbf8\ub9ac \uc0ac\uacfc\ub4dc\ub9bd\ub2c8\ub2e4.\n- \uc0dd\uc560 \uccab \ucee4\ub110\uc785\ub2c8\ub2e4 \u3160\u315c\u3160\n- \uc81c\uac00 \ucc38\uace0\ud55c \ucee4\ub110\uc774 \uc5ec\ub7ec\uac1c\uc785\ub2c8\ub2e4.\n- \ub9c1\ud06c\ub294 \ucd94\uac00\ub85c \uc801\uaca0\uc2b5\ub2c8\ub2e4.\n- \ucc38\uace0\ud55c \ubd84\ub4e4 \ucee4\ub110 \ub9c1\ud06c\uac00 \ub204\ub77d\ub420 \uc218\ub3c4 \uc788\ub294\ub370, \uc54c\ub824\uc8fc\uc2dc\uba74 \ubc14\ub85c \uc218\uc815\ud574\uc11c \ucd94\uac00\ub85c \uc62c\ub9ac\uaca0\uc2b5\ub2c8\ub2e4.\n\n##### \ucee4\ub110 \ub9c1\ud06c\n- [EDA and LASSO, RF, SVM, XGB grid search](https:\/\/www.kaggle.com\/psystat\/eda-and-lasso-rf-svm-xgb-grid-search)\n- [House Price Prediction EDA (updated 2019.03.12)](https:\/\/www.kaggle.com\/chocozzz\/house-price-prediction-eda-updated-2019-03-12)\n","498081a9":"\uc9d1\uc758 \ub4f1\uae09\uc774 \ub192\uc73c\uba74 \uac00\uaca9\uc774 \uc88b\uc544\uc9c0\ub294 \uac83\uc740 \ub2f9\uc5f0\ud558\ub2e4. \uc0c1\uc2b9\uace1\uc120\uc758 \ud615\ud0dc\ub3c4 \ubcf4\uc774\uace0 \uc788\ub2e4. \uadf8\ub7ec\ub098\n\n* \ub4f1\uae09 6,7,8,9\uc5d0\uc11c \uc774\uc0c1\uce58\uac00 \ub9ce\uc74c\n* \ub4f1\uae09 8\uacfc 11\uc5d0\uc11c \ucc28\uc774\uac00 \ud070 \uc774\uc0c1\uce58\uac00 \ub098\ud0c0\ub0a8\n\n\uc704\uc758 \ub450 \uac00\uc9c0\ub97c \ud655\uc778","e224d3ff":"### 2. \ub370\uc774\ud130\uc804\ucc98\ub9ac\n\n#### 2.1 \uc774\uc0c1\uce58\uc81c\uac70\n\uc704\uc5d0\uc11c \uc911\uc694\ud55c \ubcc0\uc218\ub4e4\uc5d0 \ub300\ud574 \uc2dc\uac01\ud654\ub97c \ud588\uc744\ub54c, \ub2e4\uc74c\uc758 \ubcc0\uc218\ub4e4\uc5d0 \ub300\ud574 \uc774\uc0c1\uce58\uac00 \uc788\ub294\uac83\uc744 \ud655\uc778\ud560 \uc218 \uc788\uc5c8\ub2e4.\n\n* sqrt_living, grade, bedrooms","71ac8cd0":"bathrooms\uac19\uc740 \uacbd\uc6b0 \uc18c\uc218\uc810\uc774 \uc788\uc5b4\uc11c \ub9ce\uc774 \ud5f7\uac08\ub9b4 \uc218 \uc788\uc2b5\ub2c8\ub2e4. \uac01\uac01\uc758 \uac12\uc774 \uc758\ubbf8\ud558\ub294\uac83\uc740 \uc544\ub798\uc640 \uac19\uace0 \uc704\uc758 \uac12\ub9cc \uac00\uc9c0\uace0 \uac01\uac01\uc758 \uac12\uc774 \uba87\uac1c\uc788\ub294\uc9c0\ub294 \uad6c\ubd84\ud558\uae30 \ud798\ub4e4\uc5b4 \ubcf4\uc778\ub2e4.\n* 0.5 : \uc138\uba74\ub300, \ud654\uc7a5\uc2e4\n* 0.75 : \uc138\uba74\ub300, \ud654\uc7a5\uc2e4, \uc0e4\uc6cc\uc2e4\n* 1 : \uc138\uba74\ub300, \ud654\uc7a5\uc2e4, \uc0e4\uc6cc\uc2e4, \uc695\uc870","20a2adb0":"## xgboost \ubaa8\ub378\ub9c1\n\n- \ub370\uc774\ud130\ubd88\ub7ec\uc624\uae30\n- \ub370\uc774\ud130\uc804\ucc98\ub9ac\n- \ubaa8\ub378\ub9c1\n- \uc559\uc0c1\ube14","2cc73eeb":"1. \ub370\uc774\ud130 \ud0d0\uc0c9\n\n\n1.1 \ubb38\uc81c\uc815\uc758\n\n\uc774\ubc88 \ub370\uc774\ud130\uc758 \ubaa9\uc801\uc740 \uc9d1\uc758 \uc815\ubcf4\ub97c \uac00\uc9c0\uace0 \uac00\uaca9\uc744 \uc608\uce21\ud558\ub294 \ud68c\uadc0\ubb38\uc81c\n\n\ud3c9\uac00 : RMSE\n\n> \ube44\uc2b7\ud55c \ub300\ud68c\ub294 \ub370\uc774\ucf58\uc5d0\uc11c \uc5f4\ub9b0 \uc9c1\ubc29 \ub300\ud68c\uc640 \uce90\uae00\uc5d0\uc11c \uc5f4\ub9b0 House Prices : Advanced Regression Techniques\uac00 \uc788\ub2e4.","41e59ee6":"data\ub294 \uc55e\uc758 8\uc790\ub9ac\ub9cc \ud544\uc694\ud574 \ubcf4\uc778\ub2e4\n\n\ubcc0\uc218\ub97c \uc5f0\ub3c4, \uc6d4, \ub0a0\uc9dc\ub85c \ub9cc\ub4e4\uac83\n\nyr_renovated\uc758 \uacbd\uc6b0 0\uc774 \uc874\uc7ac\ud568. 0\ub144\ub3c4\uc5d0 \uc7ac\uac74\ucd95\uc774 \ub418\uc5c8\ub2e4\ub294 \uc758\ubbf8\uac00 \uc544\ub2c8\ub77c \uc7ac\uac74\ucd95\uc744 \ud558\uc9c0 \uc54a\uc558\ub2e4\ub294 \uc758\ubbf8\uc774\ub2e4.","57273ef9":"\uc704\uc758 3\uac12 \ubaa8\ub450 \ud2b9\ubcc4\ud55c \uc774\uc720\uac00 \uc5c6\uc774 \uac00\uaca9\uc774 \ub192\uc544\ubcf4\uc774\ubbc0\ub85c \uc774\uc0c1\uce58\ub85c \uaddc\uc815\ud558\uace0 \uc81c\uac70\ud558\ub3c4\ub85d \ud558\uaca0\uc2b5\ub2c8\ub2e4.","50a17142":"### 1.5 \uc720\ub2c8\ud06c\ud55c \uac2f\uc218","36f11b02":"# \uce90\uae00 2019 2nd ML month with KaKR","b9737285":"\uce68\uc2e4\uc758 \uc218\ub97c \ubcf4\uba74 9\uae4c\uc9c0\ub294 price\uac00 \uc99d\uac00\ud558\ub294 \ubaa8\uc2b5\uc744 \ubcf4\uc774\uc9c0\ub9cc 10\uc774\uc0c1\ubd80\ud130\ub294 \uadf8\ub807\uc9c0 \uc54a\uc2b5\ub2c8\ub2e4.","eca81242":"train['price'].describe()","0b251ee4":"#### 2.1.2 grade","02ae5526":"### 1.3 \ubcc0\uc218\uc2dc\uac01\ud654(\ubc18\uc751\ubcc0\uc218)\n\n* \ud53c\uc5b4\uc2a8 \uc0c1\uad00\uad00\uacc4 : \uc5f0\uc18d\ud615 \ubcc0\uc218\uc5d0 \uc0ac\uc6a9\n* \uc2a4\ud53c\uc5b4\ub9cc \uc21c\uc704 \uc0c1\uad00\uad00\uacc4 : \ubc94\uc8fc\ud615 \ubcc0\uc218\ub3c4 \ud3ec\ud568\ub418\uc5c8\uc744 \uacbd\uc6b0\uc5d0 \uc0ac\uc6a9\n","b71c5805":"\uc544\ub798\ub294 \ubaa9\uc801\ubcc0\uc218\uc778 \uac00\uaca9\uacfc \uac00\uc7a5 \uc0c1\uad00\uad00\uacc4\uac00 \ub192\uc740 \uc21c\uc11c\ub300\ub85c \uc2a4\ud53c\uc5b4\ub9cc \uc21c\uc704 \uc0c1\uad00\uad00\uacc4\ub97c \ubcf8 \ud788\ud2b8\ub9f5\uc785\ub2c8\ub2e4. \uc804\ubd80 \ubcf4\ub294 \uac83\uc774 \uc544\ub2c8\ub77c \ubd84\uc11d\uc758 \ud3b8\uc758\ub97c \uc704\ud574 \uc0c1\uad00\ub3c4\uac00 \ub192\uc740 \uc0c1\uc704 10\uac1c\uc758 \uac12\ub9cc \ubd04","e10c7341":"#### 2.1.3 bedrooms","27d2af2e":"### 3. \ubcc0\uc218 \uc0dd\uc131","72831030":"sqft_living\uc758 \uacbd\uc6b0 15\ub144\ub3c4\ub97c \uae30\uc900\uc73c\ub85c \ud588\uae30\uc5d0 sqft_living\uc5d0 \ube44\ud574 \uac00\uaca9 \ubd84\uc0b0\uc774 \ud06c\ub2e4. \n\n\uc544\ubb34\ub798\ub3c4 \uc7ac\uac74\ucd95\uc73c\ub85c \uc778\ud55c \uc601\ud5a5\uc77c \uac83\uc73c\ub85c \uc608\uc0c1.\n\nsqft_living15 \uadf8 \uc790\uccb4\ub97c \uc0ac\uc6a9\ud558\uae30 \ubcf4\ub2e4\ub294 \uc801\uc808\ud55c \ubcc0\ud658\uc774 \ud544\uc694\ud574\ubcf4\uc784","03fb4baf":"1.2 \ubcc0\uc218\uc124\uba85\n- id : \uc9d1\uc744 \uad6c\ubd84\ud558\ub294 \ubc88\ud638\n- data : \uc9d1\uc744 \uad6c\ub9e4\ud55c \ub0a0\uc9dc\n- price : \uc9d1\uc758 \uac00\uaca9(\ud0c0\ucf13 \ubcc0\uc218)\n- bedrooms : \uce68\uc2e4\uc758 \uc218\n- bathrooms : \ud654\uc7a5\uc2e4\uc758 \uc218\n- sqft_living : \uc8fc\uac70 \uacf5\uac04\uc758 \ud3c9\ubc29 \ud53c\ud2b8(\uba74\uc801)\n- sqft_lot : \ubd80\uc9c0\uc758 \ud3c9\ubc29 \ud53c\ud2b8 (\uba74\uc801)\n- floors : \uc9d1\uc758 \uce35 \uc218\n- waterfront : \uc9d1\uc758 \uc804\ubc29\uc5d0 \uac15\uc774 \ud750\ub974\ub294\uc9c0 \uc720\ubb34\n- view : \uc9d1\uc774 \uc5bc\ub9c8\ub098 \uc88b\uc544 \ubcf4\uc774\ub294\uc9c0\uc758 \uc815\ub3c4\n- condition : \uc9d1\uc758 \uc804\ubc18\uc801\uc778 \uc0c1\ud0dc\n- grade : King Country grading \uc2dc\uc2a4\ud15c \uae30\uc900\uc73c\ub85c \ub9e4\uae34 \uc9d1\uc758 \ub4f1\uae09\n- sqft_above : \uc9c0\ud558\uc2e4\uc744 \uc81c\uc678\ud55c \ud3c9\ubc29 \ud53c\ud2b8 (\uba74\uc801)\n- sqft_basement : \uc9c0\ud558\uc2e4\uc758 \ud3c9\ubc29 \ud53c\ud2b8 (\uba74\uc801)\n- yr_built : \uc9c0\uc5b4\uc9c4 \ub144\ub3c4\n- yr_renovated : \uc9d1\uc744 \uc7ac\uac74\ucd95\ud55c \ub144\ub3c4\n- zipcode : \uc6b0\ud3b8\ubc88\ud638\n- lat : \uc704\ub3c4\n- long : \uacbd\ub3c4\n- sqft_living15 : 2015\ub144 \uae30\uc900 \uc8fc\uac70 \uacf5\uac04\uc758 \ud3c9\ubc29\ud53c\ud2b8(\uba74\uc801, \uc9d1\uc744 \uc7ac\uac74\ucd95\ud588\ub2e4\uba74, \ubcc0\ud654\uac00 \uc788\uc744 \uc218 \uc788\uc74c)\n- sqft_lot15 : 2015\ub144 \uae30\uc900 \ubd80\uc9c0\uc758 \ud3c9\ubc29 \ud53c\ud2b8 ( \uba74\uc801, \uc9d1\uc744 \uc7ac\uac74\ucd95\ud588\ub2e4\uba74, \ubcc0\ud654\uac00 \uc788\uc744 \uc218 \uc788\uc74c)","fb89378b":"### 2.1.4 long","287f18ff":"### 2.3 \ubcc0\uc218 \uc218\uc815","c51f5844":"\ud2b9\uc774\ud558\uac8c \ubc29 \uac2f\uc218\uac00 30\uc774 \ub118\ub294 \uac12\ub4e4\uc774 \uc788\uace0 price\ub294 \ube44\uad50\uc801 \ub0ae\uc74c.\n\uc219\ubc15\uc6a9 \uc2dc\uc124\uc774\ub77c \uc608\uc0c1","4cff1622":"\ubaa9\ucc28","7bd977fd":"\uc804\ubc18\uc801\uc73c\ub85c \uc120\ud615\uc131\uc744 \ubcf4\uc774\uc9c0\ub9cc \uac00\uaca9\uc774 14~15 \uc0ac\uc774\uc5d0 sqft_living\uc774 14000\uc5d0 \uac00\uae4c\uc6b4 \uac12\uc740 \uaf64\ub098 \uc774\uc0c1\ud55c \uac12(\uc774\uc0c1\uce58)\ub85c \uc0dd\uac01\ub418\uc5b4\uc9c4\ub2e4.\n\n\uc81c\uac70\ud560\uc9c0 \ub9d0\uc9c0\ub294 \uc800 \uac12\uc744 \ud655\uc778\ud574\ubcf4\uace0 \uacb0\uc815","43dc0754":"## \ucd94\uac00","4d68454e":"### 1.4 \uacb0\uce21\uce58"}}