{"cell_type":{"02c0f304":"code","beefc41f":"code","ad385874":"code","c93f46f4":"code","b30f3cc7":"code","cbf994e8":"code","8f94eccf":"code","2bfb2794":"code","0130cba1":"code","c6c2be3d":"code","a0352ff1":"code","e846d068":"code","a2bff624":"code","709481d9":"code","6e4037d6":"code","3be8a7a5":"code","7956c1db":"code","5fcbbc9b":"code","879a9df3":"code","778e5921":"code","cdf68a67":"code","fe42fc4a":"code","37840de4":"code","07d0a07c":"code","4ef4e760":"code","3e47e97d":"code","d4c09f16":"code","d4c91082":"code","ee1e67a1":"code","30c35557":"code","fe29225b":"code","b4965b5f":"code","704e4207":"code","2cd529f7":"code","bb1caf58":"markdown","8ec1a258":"markdown","f48da971":"markdown","a9655834":"markdown","ac32b70a":"markdown","401dc691":"markdown","72eb396f":"markdown","796166da":"markdown","791a7320":"markdown","ea361314":"markdown","cbcfba07":"markdown","d65b3fc1":"markdown","f96ac8bf":"markdown","ba4accb5":"markdown","855a1c54":"markdown","42966033":"markdown","02adf242":"markdown","ba4679af":"markdown","83824013":"markdown","7d52f2f7":"markdown","7c25aeb0":"markdown","0de9c7cc":"markdown","b9d5e437":"markdown","3a95f597":"markdown","9e4d14b1":"markdown","9048a453":"markdown","9e902761":"markdown","16e3e2e3":"markdown","02de4513":"markdown","c2e91b95":"markdown","03e61929":"markdown","0d68a60c":"markdown","3ad27176":"markdown","40d2de08":"markdown","b43f57fa":"markdown","9bd94515":"markdown","cb41c18a":"markdown","9215fd02":"markdown","7a932df8":"markdown","afe7a634":"markdown","81698d06":"markdown","943a6551":"markdown","b89e29fd":"markdown","6c4da679":"markdown","d41efce8":"markdown","e28087ec":"markdown","8a597a7d":"markdown","0193aa40":"markdown","f7d3cdfe":"markdown","04b8abd5":"markdown","81c344ac":"markdown"},"source":{"02c0f304":"import numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\npd.set_option(\"display.max_columns\", None)\n\nimport seaborn as sns\nimport cufflinks as cf\nimport plotly.offline\n\ncf.go_offline()\ncf.set_config_file(offline=False, world_readable=True)\n\nfrom sklearn.preprocessing import StandardScaler\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import GridSearchCV\n\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.linear_model import Lasso\nfrom sklearn.linear_model import ElasticNet\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom sklearn.svm import SVR\n\nfrom sklearn.pipeline import Pipeline\n\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.ensemble import GradientBoostingRegressor\nfrom sklearn.ensemble import ExtraTreesRegressor\nfrom sklearn.ensemble import AdaBoostRegressor\nfrom sklearn.metrics import mean_squared_error, r2_score","beefc41f":"names = ['CRIM', 'ZN', 'INDUS', 'CHAS', 'NOX', 'RM', 'AGE', 'DIS', 'RAD', 'TAX', 'PTRATIO',\n'B', 'LSTAT', 'MEDV']\ndf = pd.read_csv(\"..\/input\/boston-house-prices\/housing.csv\", delim_whitespace=True, names=names)","ad385874":"print(f\"Data has {df.shape[0]} instances and {df.shape[1] - 1} attributes.\")","c93f46f4":"df.dtypes","b30f3cc7":"df.head()","cbf994e8":"df.describe().T","8f94eccf":"df.corr()","2bfb2794":"def color_red(val):\n    \n    if val > 0.70 and val < 0.99:\n        color = 'red'\n    else:\n        color = 'black'\n    return f'color: {color}'","0130cba1":"pd.DataFrame(df).corr().style.applymap(color_red)","c6c2be3d":"for i in df.columns:\n    df[i].iplot(kind=\"hist\", title=i, bins=15)","a0352ff1":"for i in df.columns:\n    df[i].iplot(kind=\"box\", title=i, boxpoints=\"all\")","e846d068":"sns.pairplot(df);","a2bff624":"plt.figure(figsize=(12,8), dpi=200)\nsns.heatmap(df.corr(), cmap='coolwarm');","709481d9":"X = df.drop(columns=\"MEDV\")\ny = df.MEDV","6e4037d6":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1845)","3be8a7a5":"models = []\n\nmodels.append((\"LR\", LinearRegression()))\nmodels.append((\"LASSO\", Lasso()))\nmodels.append((\"EN\", ElasticNet()))\nmodels.append((\"KNN\", KNeighborsRegressor()))\nmodels.append((\"CART\", DecisionTreeRegressor()))\nmodels.append((\"SVR\", SVR()))","7956c1db":"results = []\nnames = []\n\nfor name, model in models:\n    kfold = KFold(n_splits=10, shuffle=True, random_state=1845)\n    \n    cv_results = cross_val_score(model, X_train, y_train, cv=kfold, scoring=\"neg_root_mean_squared_error\")\n    \n    results.append(cv_results)\n    names.append(name)\n    \n    print(f\"{name}: {round(cv_results.mean(), 4)} ({round(cv_results.std(), 4)})\")","5fcbbc9b":"results_df = pd.DataFrame(results, index=names).T\nresults_df","879a9df3":"results_df.iplot(kind=\"box\", boxpoints=\"all\", title=\"Algorithm Comparison\")","778e5921":"pipelines = []\n\npipelines.append((\"ScaledLR\", Pipeline([(\"Scaler\", StandardScaler()), (\"LR\", LinearRegression())])))\npipelines.append((\"ScaledLASSO\", Pipeline([(\"Scaler\", StandardScaler()), (\"LASSO\", Lasso())])))\npipelines.append((\"ScaledEN\", Pipeline([(\"Scaler\", StandardScaler()), (\"EN\", ElasticNet())])))\npipelines.append((\"ScaledKNN\", Pipeline([(\"Scaler\", StandardScaler()), (\"KNN\", KNeighborsRegressor())])))\npipelines.append((\"ScaledCART\", Pipeline([(\"Scaler\", StandardScaler()), (\"CART\", DecisionTreeRegressor())])))\npipelines.append((\"ScaledSVR\", Pipeline([(\"Scaler\", StandardScaler()), (\"SVR\", SVR())])))","cdf68a67":"results = []\nnames = []\n\nfor name, model in pipelines:\n    kfold = KFold(n_splits=10, shuffle=True, random_state=1845)\n    \n    cv_results = cross_val_score(model, X_train, y_train, cv=kfold, scoring=\"neg_root_mean_squared_error\")\n    \n    results.append(cv_results)\n    names.append(name)\n    \n    print(f\"{name}: {round(cv_results.mean(), 4)} ({round(cv_results.std(), 4)})\")","fe42fc4a":"results_df = pd.DataFrame(results, index=names).T\nresults_df","37840de4":"results_df.iplot(kind=\"box\", boxpoints=\"all\", title=\"Algorithm Comparison\")","07d0a07c":"scaler = StandardScaler().fit(X_train)\nscaled_X_train = scaler.transform(X_train)\n\nk_values = [i for i in range(1, 22, 2)]\n\nparam_grid = dict(n_neighbors=k_values)\n\nmodel = KNeighborsRegressor()\n\nkfold = KFold(n_splits=10, random_state=1845, shuffle=True)\ngrid = GridSearchCV(model,  param_grid=param_grid, scoring=\"neg_root_mean_squared_error\", cv=kfold)\ngrid_result = grid.fit(scaled_X_train, y_train)","4ef4e760":"print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_)) \n\nmeans = grid_result.cv_results_['mean_test_score']\nstds = grid_result.cv_results_['std_test_score']\nparams = grid_result.cv_results_['params']\n\nfor mean, stdev, param in zip(means, stds, params):\n    print(\"%f (%f) with: %r\" % (mean, stdev, param))","3e47e97d":"ensembles = []\n\nensembles.append((\"ScaledAB\", Pipeline([(\"Scaler\", StandardScaler()), (\"AB\", AdaBoostRegressor())])))\nensembles.append((\"ScaledGBM\", Pipeline([(\"Scaler\", StandardScaler()), (\"GBM\", GradientBoostingRegressor())])))\nensembles.append((\"ScaledRF\", Pipeline([(\"Scaler\", StandardScaler()), (\"RF\", RandomForestRegressor())])))\nensembles.append((\"ScaledET\", Pipeline([(\"Scaler\", StandardScaler()), (\"ET\", ExtraTreesRegressor())])))","d4c09f16":"results = []\nnames = []\n\nfor name, model in ensembles:\n    kfold = KFold(n_splits=10, shuffle=True, random_state=1845)\n    \n    cv_results = cross_val_score(model, X_train, y_train, cv=kfold, scoring=\"neg_root_mean_squared_error\")\n    \n    results.append(cv_results)\n    names.append(name)\n    \n    print(f\"{name}: {round(cv_results.mean(), 4)} ({round(cv_results.std(), 4)})\")","d4c91082":"results_df = pd.DataFrame(results, index=names).T\nresults_df","ee1e67a1":"results_df.iplot(kind=\"box\", boxpoints=\"all\", title=\"Algorithm Comparison\")","30c35557":"scaler = StandardScaler().fit(X_train)\nrescaledX = scaler.transform(X_train)\nparam_grid = dict(n_estimators=[i for i in range(50, 401, 50)])\nmodel = GradientBoostingRegressor(random_state=1845)\nkfold = KFold(n_splits=10, random_state=1845, shuffle=True)\ngrid = GridSearchCV(estimator=model, param_grid=param_grid, scoring=\"neg_root_mean_squared_error\", cv=kfold)\ngrid_result = grid.fit(rescaledX, y_train)","fe29225b":"print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_)) \n\nmeans = grid_result.cv_results_['mean_test_score']\nstds = grid_result.cv_results_['std_test_score']\nparams = grid_result.cv_results_['params']\n\nfor mean, stdev, param in zip(means, stds, params):\n    print(\"%f (%f) with: %r\" % (mean, stdev, param))","b4965b5f":"pipe = Pipeline([(\"Scaler\", StandardScaler()), (\"GBM\", GradientBoostingRegressor(n_estimators=150))])\n\npipe.fit(X_train, y_train)","704e4207":"y_pred = pipe.predict(X_test)\n\nprint(f\"RMSE of our final model: {round(np.sqrt(mean_squared_error(y_test, y_pred)), 4)}\")","2cd529f7":"print(f\"R2 Score of our final model: {round(r2_score(y_test, y_pred), 4)}\")","bb1caf58":"# Improve Results With Tuning Ensemble Methods","8ec1a258":"- It looks like LR has the lowest RMSE, followed closely by CART.","f48da971":"- We know from the results in the previous section that KNN achieves good results on a scaled version of the dataset. But can it do better. The default value for the number of neighbors in KNN is 5. We can use a grid search to try a set of different numbers of neighbors and see if we can improve the score. The below example tries odd k values from 1 to 21. Each k value (n neighbors) is evaluated using 10-fold cross validation on a standardized copy of the training dataset.","a9655834":"- We can see that some of the higher correlated attributes do show good structure in their relationship. Not linear, but nice predictable curved relationships.","ac32b70a":"# Load the Dataset","401dc691":"- We can confirm that the scales for the attributes are all over the place because of the differing units. We may benefit from some transforms later on.","72eb396f":"## Unimodal Data Visualizations","796166da":"There is a lot of structure in this dataset. We need to think about transforms that we could use later to better expose the structure which in turn may improve modeling accuracy. So far it would be worth trying:\n- Feature selection and removing the most correlated attributes.\n- Normalizing the dataset to reduce the effect of differing scales.\n- Standardizing the dataset to reduce the effects of differing distributions.","791a7320":"https:\/\/machinelearningmastery.com","ea361314":"Another way that we can improve the performance of algorithms on this problem is by using ensemble methods. In this section we will evaluate four different ensemble machine learning algorithms, two boosting and two bagging methods:\n\n- Boosting Methods: AdaBoost (AB) and Gradient Boosting (GBM).\n- Bagging Methods: Random Forests (RF) and Extra Trees (ET).\n\nWe will use the same test harness as before, 10-fold cross validation and pipelines that standardize the training data for each fold.","cbcfba07":"This is the end of the my second machine learning notebook. Huge credit to Jason Brownlee since the techniques I used was from his great book, Machine Learning Mastery with Python. I highly encourage people who are in the beginning of the Data Science journey to take a look at his great books. I will be leaving a link to his all books.\n\nStay well. ","d65b3fc1":"- We now have a better feeling for how different the attributes are. The min and max values as well are the means vary a lot. We are likely going to get better results by rescaling the data in some way.","f96ac8bf":"- Now, let\u2019s now take a look at the correlation between all of the numeric attributes.","ba4accb5":"- The dark red color shows positive correlation whereas the dark blue color shows negative correlation. We can also see some dark red and dark blue that suggest candidates for removal to better improve accuracy of models later on.","855a1c54":"- \nWe can now take a closer look at our loaded data.","42966033":"- I did not use XGBoostRegressor in this notebook which is pretty strong algorithm in machine learning. We might get a better result by using XGB and tuning the models more in depth. Since this notebook is for beginners, I tried to make it as clear as possible.","02adf242":"- The differing scales of the data is probably hurting the skill of all of the algorithms and perhaps more so for SVR and KNN. In the next section we will look at running the same algorithms using a standardized copy of the data.\n","ba4679af":"# Algorithms After Standardization","83824013":"# Validation Dataset","7d52f2f7":"* We can now load the dataset.","7c25aeb0":"# Improve Results With Tuning","0de9c7cc":"Let\u2019s create a baseline of performance on this problem and spot-check a number of different algorithms. We will select a suite of different algorithms capable of working on this regression problem. The six algorithms selected include:\n- Linear Algorithms: Linear Regression (LR), Lasso Regression (LASSO) and ElasticNet (EN).\n- Nonlinear Algorithms: Classification and Regression Trees (CART), Support Vector Regression (SVR) and k-Nearest Neighbors (KNN).","b9d5e437":"- Running the example provides a list of root mean squared errors. We can see that scaling did have an effect on KNN, driving the error lower than the other models.\n- Also SVR benefitted greatly by scaling the data.","3a95f597":"- We can see that all of the attributes are numeric, mostly real values (float) and some have been interpreted as integers (int).","9e4d14b1":"- We can see similar distributions for the regression algorithms and perhaps a tighter distribution of scores for CART.","9048a453":"- It is a good idea to use a validation hold-out set. This is a sample of the data that we hold back from our analysis and modeling. We use it right at the end of our project to confirm the accuracy of our final model. It is a smoke test that we can use to see if we messed up and to give us confidence on our estimates of accuracy on unseen data. We will use 80% of the dataset for modeling and hold back 20% for validation.","9e902761":"- That's better.","16e3e2e3":"# Analyze Data","02de4513":"- We will finalize the gradient boosting model and evaluate it on our hold out validation dataset. First we need to prepare the model and train it on the entire training dataset. This includes standardizing the training dataset before training.","c2e91b95":"- This is interesting. We can see that many of the attributes have a strong correlation (e.g.> 0.70 or < \u22120.70). For example: \n    - **NOX** and **INDUS** with 0.76.\n    - **DIS** and **INDUS** with -0.71. \n    - **TAX** and **INDUS** with 0.72. \n    - **AGE** and **NOX** with 0.73.\n    - **DIS** and **NOX** with -0.77.\n- It also looks like **LSTAT** has a good negative correlation with the output variable **MEDV** with a value of -0.74.","03e61929":"# Finalize Model","0d68a60c":"- Let\u2019s take a look at the distribution of scores across all cross validation folds by algorithm.","3ad27176":"- It looks like Gradient Boosting has a better mean score with Extra Trees close second.","40d2de08":"- You can see that the best for k (n neighbors) is 1 providing a root mean squared error of -4.440091, the best so far.","b43f57fa":"- Let\u2019s look at visualizations of individual attributes. It is often useful to look at your data using multiple different visualizations in order to spark ideas. Let\u2019s look at histograms of each attribute to get a sense of the data distributions.","9bd94515":"- Let\u2019s also visualize the correlations between the attributes.","cb41c18a":"Each record in the database describes a Boston suburb or town. The data was drawn from the Boston Standard Metropolitan Statistical Area (SMSA) in 1970. The attributes are de\ufb01ned as follows (taken from the UCI Machine Learning Repository1)\n\n- **1. CRIM**: per capita crime rate by town\n- **2. ZN**: proportion of residential land zoned for lots over 25,000 sq.ft.\n- **3. INDUS**: proportion of non-retail business acres per town\n- **4. CHAS**: Charles River dummy variable (= 1 if tract bounds river; 0 otherwise)\n- **5. NOX**: nitric oxides concentration (parts per 10 million)\n- **6. RM**: average number of rooms per dwelling\n- **7. AGE**: proportion of owner-occupied units built prior to 1940 \n- **8. DIS**: weighted distances to five Boston employment centers \n- **9. RAD**: index of accessibility to radial highways\n- **10. TAX**: full-value property-tax rate per 10,000 dollars\n- **11. PTRATIO**: pupil-teacher ratio by town\n- **12. B**: 1000(Bk \u2212 0.63)2 where Bk is the proportion of blacks by town \n- **13. LSTAT**: % lower status of the population\n- **14. MEDV**: Median value of owner-occupied homes in 1000s dollars\nWe can see that the input attributes have a mixture of units.\n\n#### Acknowledgements\n\nThanks to Dr.Jason","9215fd02":"- We suspect that the differing scales of the raw data may be negatively impacting the skill of some of the algorithms. Let\u2019s evaluate the same algorithms with a standardized copy of the dataset. This is where the data is transformed such that each attribute has a mean value of zero and a standard deviation of 1. We also need to avoid data leakage when we transform the data. A good way to avoid leakage is to use pipelines that standardize the data and build the model for each fold in the cross validation test harness. That way we can get a fair estimation of how each model with standardized data might perform on unseen data.","7a932df8":"- Let\u2019s start off by loading the libraries required for this project.","afe7a634":"## Multimodal Data Visualizations","81698d06":"- The algorithms all use default tuning parameters. Let\u2019s compare the algorithms. We will display the mean and standard deviation of RMSE for each algorithm as we calculate it and collect the results for use later.","943a6551":"- Let\u2019s look at some visualizations of the interactions between variables. The best place to start is a scatter plot matrix.","b89e29fd":"- Let\u2019s start off by confirming the dimensions of the dataset, e.g. the number of rows and columns.","6c4da679":"# Baseline Algorithms","d41efce8":"# Data Visualizations","e28087ec":"- We can see that KNN has both a tight distribution of error and has the lowest score.","8a597a7d":"# Ensemble Methods","0193aa40":"## Summary of Ideas","f7d3cdfe":"- We can see that some attributes may have an exponential distribution, such as **CRIM**, **ZN**, **AGE** and **B**. We can see that others may have a bimodal distribution such as **RAD** and **TAX**.","04b8abd5":"- Running the example calculates the root mean squared error for each method using the default parameters. We can see that we\u2019re generally getting better scores than our linear and nonlinear algorithms in previous sections.","81c344ac":"## Descriptive Statistics"}}