{"cell_type":{"a39f9d9d":"code","4f6726ea":"code","1227210d":"code","5980f186":"code","4973cf61":"code","d57ae29c":"code","ef406bba":"code","5c13e73f":"code","a43f126b":"code","628b26a4":"code","ce930177":"code","1bc18f69":"code","1737327f":"code","ca456ad3":"code","81c42b10":"code","b2dc801f":"code","a1c13144":"code","3451322f":"code","f158c56b":"code","bf3d0cdf":"code","3dbf6784":"code","e8c0efb9":"code","49e544e9":"code","f9948441":"code","d1c4769c":"code","1690f502":"code","3ca0c511":"code","83d6f219":"code","ebcf8bf0":"code","0f8402b0":"code","51e27d1f":"code","15f70801":"code","26aa6dce":"code","1e7f9f09":"code","cf0a1743":"code","62147219":"code","41520938":"code","965b89c4":"code","9738a7ac":"code","453f4d20":"code","b8f863dd":"code","d9c8adc2":"code","43ffaac8":"code","a3954c77":"code","edf25bbf":"code","cab65bcd":"code","dcea7238":"code","e173141f":"code","2c694ec0":"code","dca71186":"markdown","d7b84584":"markdown","efe8e5aa":"markdown","2a95deaf":"markdown","e5077571":"markdown","d3a30f32":"markdown","87fdb8b7":"markdown","8343b335":"markdown","a720cc2b":"markdown","fa867bb8":"markdown","bdda1d97":"markdown","1955681c":"markdown","cf645362":"markdown","a47c3125":"markdown","b40660d5":"markdown","059c476c":"markdown","622bc317":"markdown","b24bc1ac":"markdown","01331178":"markdown","92e9a931":"markdown","36646ec3":"markdown","ac16bcda":"markdown","3e6aff6a":"markdown","45d5da10":"markdown","976d77a7":"markdown","27720421":"markdown","d26c1651":"markdown","ebad581e":"markdown","fb4a3fab":"markdown","92580d95":"markdown","4e9e8efe":"markdown"},"source":{"a39f9d9d":"# Import libraries and set desired options\nimport os\nimport pickle\nimport numpy as np\nimport pandas as pd\nfrom scipy.sparse import hstack\n# !pip install eli5\nimport eli5\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import TimeSeriesSplit, cross_val_score, GridSearchCV\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.linear_model import LogisticRegression\nfrom matplotlib import pyplot as plt\nimport seaborn as sns\nfrom IPython.display import display_html","4f6726ea":"PATH_TO_DATA = '..\/input\/'\nSEED = 17","1227210d":"def prepare_sparse_features(path_to_train, path_to_test, path_to_site_dict,\n                           vectorizer_params):\n    times = ['time%s' % i for i in range(1, 11)]\n    train_df = pd.read_csv(path_to_train,\n                       index_col='session_id', parse_dates=times)\n    test_df = pd.read_csv(path_to_test,\n                      index_col='session_id', parse_dates=times)\n\n    # Sort the data by time\n    train_df = train_df.sort_values(by='time1')\n    \n    # read site -> id mapping provided by competition organizers \n    with open(path_to_site_dict, 'rb') as f:\n        site2id = pickle.load(f)\n    # create an inverse id _> site mapping\n    id2site = {v:k for (k, v) in site2id.items()}\n    # we treat site with id 0 as \"unknown\"\n    id2site[0] = 'unknown'\n    \n    # Transform data into format which can be fed into TfidfVectorizer\n    # This time we prefer to represent sessions with site names, not site ids. \n    # It's less efficient but thus it'll be more convenient to interpret model weights.\n    sites = ['site%s' % i for i in range(1, 11)]\n    train_sessions = train_df[sites].fillna(0).astype('int').apply(lambda row: \n                                                     ' '.join([id2site[i] for i in row]), axis=1).tolist()\n    test_sessions = test_df[sites].fillna(0).astype('int').apply(lambda row: \n                                                     ' '.join([id2site[i] for i in row]), axis=1).tolist()\n    # we'll tell TfidfVectorizer that we'd like to split data by whitespaces only \n    # so that it doesn't split by dots (we wouldn't like to have 'mail.google.com' \n    # to be split into 'mail', 'google' and 'com')\n    vectorizer = TfidfVectorizer(**vectorizer_params)\n    X_train = vectorizer.fit_transform(train_sessions)\n    X_test = vectorizer.transform(test_sessions)\n    y_train = train_df['target'].astype('int').values\n    \n    # we'll need site visit times for further feature engineering\n    train_times, test_times = train_df[times], test_df[times]\n    \n    return X_train, X_test, y_train, vectorizer, train_times, test_times","5980f186":"%%time\nX_train_sites, X_test_sites, y_train, vectorizer, train_times, test_times = prepare_sparse_features(\n    path_to_train=os.path.join(PATH_TO_DATA, 'train_sessions.csv'),\n    path_to_test=os.path.join(PATH_TO_DATA, 'test_sessions.csv'),\n    path_to_site_dict=os.path.join(PATH_TO_DATA, 'site_dic.pkl'),\n    vectorizer_params={'ngram_range': (1, 5), \n                       'max_features': 50000,\n                       'tokenizer': lambda s: s.split()}\n)","4973cf61":"print(X_train_sites.shape, X_test_sites.shape)","d57ae29c":"vectorizer.get_feature_names()[:10]","ef406bba":"vectorizer.get_feature_names()[10000:10010]","5c13e73f":"time_split = TimeSeriesSplit(n_splits=10)","a43f126b":"logit = LogisticRegression(C=1, random_state=SEED, solver='liblinear')","628b26a4":"%%time\n\ncv_scores1 = cross_val_score(logit, X_train_sites, y_train, cv=time_split, \n                            scoring='roc_auc', n_jobs=4) # hangs with n_jobs > 1, and locally this runs much faster","ce930177":"cv_scores1, cv_scores1.mean()","1bc18f69":"logit.fit(X_train_sites, y_train)","1737327f":"eli5.show_weights(estimator=logit, \n                  feature_names=vectorizer.get_feature_names(), top=30)","ca456ad3":"# A helper function for writing predictions to a file\ndef write_to_submission_file(predicted_labels, out_file,\n                             target='target', index_label=\"session_id\"):\n    predicted_df = pd.DataFrame(predicted_labels,\n                                index = np.arange(1, predicted_labels.shape[0] + 1),\n                                columns=[target])\n    predicted_df.to_csv(out_file, index_label=index_label)","81c42b10":"logit_test_pred = logit.predict_proba(X_test_sites)[:, 1]\nwrite_to_submission_file(logit_test_pred, 'subm1.csv') # 0.91807","b2dc801f":"def train_and_predict(model, X_train, y_train, X_test, site_feature_names=vectorizer.get_feature_names(), \n                      new_feature_names=None, cv=time_split, scoring='roc_auc',\n                      top_n_features_to_show=30, submission_file_name='submission.csv'):\n    \n    \n    cv_scores = cross_val_score(model, X_train, y_train, cv=cv, \n                            scoring=scoring, n_jobs=4)\n    print('CV scores', cv_scores)\n    print('CV mean: {}, CV std: {}'.format(cv_scores.mean(), cv_scores.std()))\n    model.fit(X_train, y_train)\n    \n    if new_feature_names:\n        all_feature_names = site_feature_names + new_feature_names \n    else: \n        all_feature_names = site_feature_names\n    \n    display_html(eli5.show_weights(estimator=model, \n                  feature_names=all_feature_names, top=top_n_features_to_show))\n    \n    if new_feature_names:\n        print('New feature weights:')\n    \n        print(pd.DataFrame({'feature': new_feature_names, \n                        'coef': model.coef_.flatten()[-len(new_feature_names):]}))\n    \n    test_pred = model.predict_proba(X_test)[:, 1]\n    write_to_submission_file(test_pred, submission_file_name) \n    \n    return cv_scores","a1c13144":"cv_scores1 = train_and_predict(model=logit, X_train=X_train_sites, y_train=y_train, \n                  X_test=X_test_sites, site_feature_names=vectorizer.get_feature_names(),              \n                  cv=time_split, submission_file_name='subm1.csv')","3451322f":"session_start_hour = train_times['time1'].apply(lambda ts: ts.hour).values","f158c56b":"sns.countplot(session_start_hour);","bf3d0cdf":"plt.subplots(1, 2, figsize = (12, 6)) \n\nplt.subplot(1, 2, 1)\nsns.countplot(session_start_hour[y_train == 1])\nplt.title(\"Alice\")\nplt.xlabel('Session start hour')\n          \nplt.subplot(1, 2, 2)\nsns.countplot(session_start_hour[y_train == 0])\nplt.title('Others')\nplt.xlabel('Session start hour');","3dbf6784":"morning = ((session_start_hour >= 7) & (session_start_hour <= 11)).astype('int')\nday = ((session_start_hour >= 12) & (session_start_hour <= 18)).astype('int')\nevening = ((session_start_hour >= 19) & (session_start_hour <= 23)).astype('int')\nnight = ((session_start_hour >= 0) & (session_start_hour <= 6)).astype('int')","e8c0efb9":"pd.crosstab([morning, day, evening, night], y_train, rownames=['morning', 'day', 'evening', 'night'])","49e544e9":"def add_time_features(times, X_sparse, add_hour=True):\n    hour = times['time1'].apply(lambda ts: ts.hour)\n    morning = ((hour >= 7) & (hour <= 11)).astype('int').values.reshape(-1, 1)\n    day = ((hour >= 12) & (hour <= 18)).astype('int').values.reshape(-1, 1)\n    evening = ((hour >= 19) & (hour <= 23)).astype('int').values.reshape(-1, 1)\n    night = ((hour >= 0) & (hour <=6)).astype('int').values.reshape(-1, 1)\n    \n    objects_to_hstack = [X_sparse, morning, day, evening, night]\n    feature_names = ['morning', 'day', 'evening', 'night']\n    \n    if add_hour:\n        # we'll do it right and scale hour dividing by 24\n        objects_to_hstack.append(hour.values.reshape(-1, 1) \/ 24)\n        feature_names.append('hour')\n        \n    X = hstack(objects_to_hstack)\n    return X, feature_names","f9948441":"%%time\nX_train_with_times1, new_feat_names = add_time_features(train_times, X_train_sites)\nX_test_with_times1, _ = add_time_features(test_times, X_test_sites)","d1c4769c":"X_train_with_times1.shape, X_test_with_times1.shape","1690f502":"cv_scores2 = train_and_predict(model=logit, X_train=X_train_with_times1, y_train=y_train, \n                               X_test=X_test_with_times1, \n                               site_feature_names=vectorizer.get_feature_names(),\n                               new_feature_names=new_feat_names,\n                               cv=time_split, submission_file_name='subm2.csv')","3ca0c511":"cv_scores2 > cv_scores1","83d6f219":"%%time\nX_train_with_times2, new_feat_names = add_time_features(train_times, X_train_sites, add_hour=False)\nX_test_with_times2, _ = add_time_features(test_times, X_test_sites, add_hour=False)\n\n\ncv_scores3 = train_and_predict(model=logit, X_train=X_train_with_times2, y_train=y_train, \n                               X_test=X_test_with_times2, \n                               site_feature_names=vectorizer.get_feature_names(),\n                               new_feature_names=new_feat_names,\n                               cv=time_split, submission_file_name='subm3.csv')","ebcf8bf0":"cv_scores3 > cv_scores1","0f8402b0":"cv_scores3 > cv_scores2","51e27d1f":"def add_session_duration_incorrect(times, X_sparse):\n    new_feat = (times.max(axis=1) - times.min(axis=1)).astype('timedelta64[ms]').astype(int)\n    return hstack([X_sparse, new_feat.values.reshape(-1, 1)])","15f70801":"X_train_with_time_incorrect = add_session_duration_incorrect(train_times, X_train_with_times2)\nX_test_with_time_incorrect = add_session_duration_incorrect(test_times, X_test_with_times2)","26aa6dce":"cv_scores4 = train_and_predict(model=logit, X_train=X_train_with_time_incorrect, y_train=y_train, \n                               X_test=X_test_with_time_incorrect, \n                               site_feature_names=vectorizer.get_feature_names(),\n                               new_feature_names=new_feat_names + ['sess_duration'],\n                               cv=time_split, submission_file_name='subm4.csv')","1e7f9f09":"train_durations = (train_times.max(axis=1) - train_times.min(axis=1)).astype('timedelta64[ms]').astype(int)\ntest_durations = (test_times.max(axis=1) - test_times.min(axis=1)).astype('timedelta64[ms]').astype(int)\n\nscaler = StandardScaler()\ntrain_dur_scaled = scaler.fit_transform(train_durations.values.reshape(-1, 1))\ntest_dur_scaled = scaler.transform(test_durations.values.reshape(-1, 1))","cf0a1743":"X_train_with_time_correct = hstack([X_train_with_times2, train_dur_scaled])\nX_test_with_time_correct = hstack([X_test_with_times2, test_dur_scaled])","62147219":"cv_scores5 = train_and_predict(model=logit, X_train=X_train_with_time_correct, y_train=y_train, \n                               X_test=X_test_with_time_correct, \n                               site_feature_names=vectorizer.get_feature_names(),\n                               new_feature_names=new_feat_names + ['sess_duration'],\n                               cv=time_split, submission_file_name='subm5.csv')","41520938":"cv_scores5 > cv_scores3","965b89c4":"def add_day_month(times, X_sparse):\n    day_of_week = times['time1'].apply(lambda t: t.weekday()).values.reshape(-1, 1)\n    month = times['time1'].apply(lambda t: t.month).values.reshape(-1, 1) \n    # linear trend: time in a form YYYYMM, we'll divide by 1e5 to scale this feature \n    year_month = times['time1'].apply(lambda t: 100 * t.year + t.month).values.reshape(-1, 1) \/ 1e5\n    \n    objects_to_hstack = [X_sparse, day_of_week, month, year_month]\n    feature_names = ['day_of_week', 'month', 'year_month']\n        \n    X = hstack(objects_to_hstack)\n    return X, feature_names","9738a7ac":"X_train_final, more_feat_names = add_day_month(train_times, X_train_with_time_correct)\nX_test_final, _ = add_day_month(test_times, X_test_with_time_correct)","453f4d20":"cv_scores6 = train_and_predict(model=logit, X_train=X_train_final, y_train=y_train, \n                               X_test=X_test_final, \n                               site_feature_names=vectorizer.get_feature_names(),\n                               new_feature_names=new_feat_names + ['sess_duration'] + more_feat_names,\n                               cv=time_split, submission_file_name='subm6.csv')","b8f863dd":"# here we've already narrowed down c_values to such a range.\n# typically, you would start with a wider range of values to check\nc_values = np.logspace(-2, 2, 20)\n\nlogit_grid_searcher = GridSearchCV(estimator=logit, param_grid={'C': c_values},\n                                  scoring='roc_auc', n_jobs=4, cv=time_split, verbose=1)","d9c8adc2":"%%time\nlogit_grid_searcher.fit(X_train_final, y_train); ","43ffaac8":"logit_grid_searcher.best_score_, logit_grid_searcher.best_params_","a3954c77":"final_model = logit_grid_searcher.best_estimator_","edf25bbf":"cv_scores7 = train_and_predict(model=final_model, X_train=X_train_final, y_train=y_train, \n                               X_test=X_test_final, \n                               site_feature_names=vectorizer.get_feature_names(),\n                               new_feature_names=new_feat_names + ['sess_duration'] + more_feat_names,\n                               cv=time_split, submission_file_name='subm7.csv')","cab65bcd":"cv_scores7 > cv_scores6","dcea7238":"cv_means = [np.round(cv_scores.mean(), 5) for cv_scores in [cv_scores1, cv_scores2, cv_scores3,\n                                                                 cv_scores4, cv_scores5, cv_scores6, cv_scores7]]\ncv_stds = [np.round(cv_scores.std(), 5) for cv_scores in [cv_scores1, cv_scores2, cv_scores3,\n                                                                 cv_scores4, cv_scores5, cv_scores6, cv_scores7]]\npublic_lb_scores = [0.91807, 0.93135, 0.94526, 0.67016, 0.94620, 0.95061, 0.95055]\n\nsubm_df = pd.DataFrame({'CV_mean': cv_means, 'CV_std': cv_stds, 'LB': public_lb_scores},\n                      index=range(1, len(cv_means) + 1))\nsubm_df","e173141f":"subm_df['cv_lb_weighted'] =  0.6 * subm_df['LB'] + (1 - 0.6) * subm_df['CV_mean']\nsubm_df","2c694ec0":"# so we'll treat the last submission as the best one\n!cp subm7.csv submission.csv","dca71186":"Now the same separately for Alice and everybody else.","d7b84584":"## Submission 7: Tuning params\nWhen you're done with feature engineering (no more ideas) you can tune hyperparameters in your model. Typically, at this point you create nice code for your pipeline, and then tune various params for a long time. Here we used several params - `ngram_range`, `max_features`. Choosing between `CountVectorizer` and `TfIdfVectorizer` might also be treated as a hyperparameter. But now we'll tune only regularization strength `C`.","efe8e5aa":"## Submission 1: \"Bag of sites\" baseline\n\n**We'll start with basic site features, applying the \"bag of words\" approach. Here we read training and test sets, sort train set by session start time.**","2a95deaf":"**Display model weights with eli5**","e5077571":"**Now we see which sites are descriptive of Alice. At the same time we notice that Alice doesn't use Gmail and Google Plus. Let's make predictions for test set and form a submission file.**","d3a30f32":"**By running the above function, we get sparse train and test matrices (`X_train_sites`, `X_test_sites`), vector of train targets `y_train` (0's and 1's - whether a session belongs to Alice or not), an instance of `TfidfVectorizer` (we'll need site name from it) and site visit times both for training and test sets, for further feature engineering. Go back to the function defined above, read comments, and experiment yourself to understand what we've done here.**","87fdb8b7":"## Submissions 4 and 5: The importance of feature scaling","8343b335":"## Analyzing submission history\nLet's summarize CV results and LB scores for all 7 submissions:","a720cc2b":"**We see an improvement almost for all folds as compared to the results of previous cross-validation.**","fa867bb8":"**Now cross-validation is much more stable. 3rd CV results are better for each and every fold as compared to the first ones. And actually the situation is better than with the `hour` feature for 5 folds out of 10, but in such case we'll prefer a model with less variation of CV results, i.e. the last one.**","bdda1d97":"## Submission 3: Example of overfitting","1955681c":"**These are basic features - sequences of visited web-sites.**","cf645362":"The correlation between CV mean and LB exists, but it's not perfect. So which submission to choose in the end? A popular method is to treat mean CV and LB results with weights, proportional to train and test sizes. However, considering time, we'll trust public LB score more and set it's weight to 0.6 (no theory here, only practical experience). Also, as we've seen, standard deviation of CV results is also important, so you can experiment here. ","a47c3125":"**New model predicts better on 9 folds out of 10 as compared to the model with sites and time features. Submitting yields 0.94630 Public LB ROC AUC.**","b40660d5":"**Performing time series cross-validation, we see an improvement in ROC AUC.**","059c476c":"**Making a new submission, we notice a leaderboard score improvement as well (0.91807 ->  0.93089). Looks good as compared to sites only. But we might be a bit wary that feature `hour` gets such a big weight. Let's try the same features but without `hour`.**","622bc317":"## Submission 6: Adding more time features\n\nIn a real competition it's very important to keep track of Public Kernels and borrow some ideas form them.  For instance, we can explore [this Kernel](https:\/\/www.kaggle.com\/hakeydotcom\/additional-time-features-and-logit) and decide to add to features: day of week and month. In this case it's easy to come up to such features on your own, but still typically there's a lot to find out in Kernels.  ","b24bc1ac":"**We'll be performing time series cross-validation, see [the previous kernel](https:\/\/www.kaggle.com\/kashnitsky\/correct-time-aware-cross-validation-scheme) for an explanation.**","01331178":"**Train logistic regression with all training data**","92e9a931":"Making a new submission we get... Wow! **0.94535** Public LB score instead of **0.93089** when adding the `hour` feature. We were right, the `hour` feature leads to overfitting, and it's better not to add it. ","36646ec3":"## Submission 2: Coming up with time features via EDA\nHere we'll build just a few plots to motivate feature engineering. While competing, you'll need to create much more plots and build more features based on your observations. Take a look at [Kernels](https:\/\/www.kaggle.com\/c\/catch-me-if-you-can-intruder-detection-through-webpage-session-tracking2\/kernels?sortBy=voteCount&group=everyone&pageSize=20&competitionId=7173) for this competition, eg. at [this \"Initial EDA\"](https:\/\/www.kaggle.com\/adityaecdrid\/initial-eda). You can do much more with sites, but let's switch to site visit times.","ac16bcda":"**We are going to repeat these steps several more times. It's a nice practice to substitute repeated chunks of code with function calls. So let's write a function which performs cross-validation, model training, displaying feature importance, making predictions for a test set and forming a submission file.**","3e6aff6a":"**What's the reason of such a deterioration?**\nObvious! The new feature is session duration in milliseconds, it's maximal value is very high (check it). We need to either scale a new feature or, alternatively, measure it in some different units. You can check that actually, measuring it in seconds (rather than milliseconds will do). But instead we'll perform feature scaling, it's a more universal technique to apply for numeric features which can take high values.","45d5da10":"**Perform time series cross-validation with logistic regression.**","976d77a7":"We've got an intuition: different people might prefer to visit (even the same) sites at *different* times of the day. Let's first draw the distribution of all session start hours. ","27720421":"## Conclusions\n - keep track of cross-validation improvements for *each* fold (or at least as many folds as possible)\n - take a look at cross-validation std, not only mean\n - try to build a CV scheme so that CV improvements correlate with LB improvements (it's very important)\n - exploring feature importance might help, sometimes even in detecting overfitting\n - spend most of the competition time exploring data and building features\n \nGood luck!\n\n<img src=\"https:\/\/habrastorage.org\/webt\/ai\/xu\/sy\/aixusyqysuiou9rsll7x909jyw0.jpeg\" width=50% \/> \n*<div style=\"text-align: center\"> image credit <a href='http:\/\/www.alicekristiansen.com\/'>www.alicekristiansen.com<\/a> <\/div>*\n","d26c1651":"**Now we'll add a new feature: session duration. But beware: first we'll do it in an incorrect way, then we'll correct ourselves.**","ebad581e":"Here tuning params helps only for 6 folds out of 10. Typically in such situation you'll make one more submission to compare LB scores as well. 0.94954 - it's less than without hyperparameter tuning. **Bad news in the end:** our CV scheme is not perfect. Try  to improve it! (*hint:* is all training set needed for a good prediction?). ","fb4a3fab":"**Let's create a separate function to add new features (this will keep the code cleaner). To demonstrate possible overfitting, we'll keep a flag - whether to add an `hour` feature or not.**","92580d95":"<center>\n<img src=\"https:\/\/habrastorage.org\/files\/fd4\/502\/43d\/fd450243dd604b81b9713213a247aa20.jpg\" \/>\n<\/center> \n     \n## <center>  [mlcourse.ai](https:\/\/mlcourse.ai) \u2013 Open Machine Learning Course \n\n#### <center> Author: [Yury Kashnitskiy](https:\/\/yorko.github.io) (@yorko) \n\n## <center> A disciplined approach to cross-validation in a Kaggle (Inclass) competition \n\n\nHere we'll show how to progress with feature engineering in the [Alice](https:\/\/www.kaggle.com\/c\/catch-me-if-you-can-intruder-detection-through-webpage-session-tracking2) competition, we'll simulate some mistakes that can be done, show how to interpret model weights with [eli5](https:\/\/github.com\/TeamHG-Memex\/eli5), and discuss our validation scheme even further. Prerequisites: [Alice - logistic regression baseline](https:\/\/www.kaggle.com\/kashnitsky\/alice-logistic-regression-baseline) and [Correct time-aware cross-validation scheme](https:\/\/www.kaggle.com\/kashnitsky\/correct-time-aware-cross-validation-scheme).\n \n \nPlan:\n - [Submission 1: \"Bag of sites\" baseline](#Submission-1:-\"Bag-of-sites\"-baseline)\n - [Submission 2: Coming up with time features via EDA](#Submission-2:-Coming-up-with-time-features-via-EDA)\n - [Submission 3: Example of overfitting](#Submission-3:-Example-of-overfitting)\n - [Submissions 4 and 5: The importance of feature scaling](#Submissions-4-and-5:-The-importance-of-feature-scaling)\n - [Submission 6: Adding more time features](#Submission-6:-Adding-more-time-features)\n - [Submission-7: Tuning-params](#Submission-7:-Tuning-params)\n - [Analyzing submission history](#Analyzing-submission-history)\n - [Conclusions](#Conclusions)","4e9e8efe":"Now we definitely see that Alice mostly prefers 4-5 pm for browsing. So let's create features 'morning', 'day' and 'evening' and 'night'. Separators between these times of the day will be almost arbitrary: 0 am, 7 am, 12 am, and 7 pm. However, you can tune this."}}