{"cell_type":{"633a4035":"code","b57bdb13":"code","e2bb99c6":"code","0c528c5d":"code","b63f8be2":"code","020abd88":"code","8c627447":"code","bbc70e74":"code","8685c2ce":"code","f8f6dce4":"code","9c027aef":"code","81953019":"code","3cec145d":"code","5073d441":"code","7ba609da":"markdown","797afbf6":"markdown","f9068eb3":"markdown","f936b721":"markdown","bd414186":"markdown","3ef15402":"markdown","c90697ef":"markdown","a2c90a4a":"markdown"},"source":{"633a4035":"from __future__ import division\nimport random,pickle,csv,cv2,os,scipy,pickle,warnings,matplotlib\nimport pandas as pd\nimport numpy as np\nimport tensorflow as tf\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport gc\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.utils import shuffle\nfrom scipy.stats import norm,skew\nfrom itertools import islice\n\nimport keras.backend as K\nfrom keras.callbacks import History\nfrom keras.layers import Dense,Flatten,Dropout,Concatenate,GlobalAveragePooling2D,Lambda,ZeroPadding2D,GlobalMaxPooling2D\nfrom keras.layers import SeparableConv2D,BatchNormalization,MaxPooling2D,Conv2D\nfrom keras.applications.mobilenet_v2 import MobileNetV2\nfrom keras.utils import print_summary\nfrom keras.models import Sequential\nfrom keras.callbacks import ModelCheckpoint,EarlyStopping,TensorBoard,CSVLogger,ReduceLROnPlateau,LearningRateScheduler\nfrom keras.optimizers import Adam,SGD\nfrom keras import applications\nfrom keras.utils.vis_utils import plot_model\n\nprint(os.listdir('..\/input\/self driving car training data\/data'))\nwarnings.filterwarnings('ignore')","b57bdb13":"def show_final_history(history):\n    fig, ax = plt.subplots(1, 2, figsize=(15,5))\n    ax[0].set_title('loss')\n    ax[0].plot(history.epoch, history.history[\"loss\"], label=\"Train loss\")\n    ax[0].plot(history.epoch, history.history[\"val_loss\"], label=\"Validation loss\")\n    ax[1].set_title('acc')\n    ax[1].plot(history.epoch, history.history[\"acc\"], label=\"Train acc\")\n    ax[1].plot(history.epoch, history.history[\"val_acc\"], label=\"Validation acc\")\n    ax[0].legend()\n    ax[1].legend()","e2bb99c6":"def image_preprocessing(img):\n    resized_image = cv2.resize((cv2.cvtColor(img,cv2.COLOR_RGB2HSV))[:,:,1],(40,40))\n    return resized_image","0c528c5d":"def load_training(delta):\n    logs = []\n    features = []\n    labels = []\n    with open(labels_file,'rt') as f:\n        reader = csv.reader(f)\n        for line in reader:\n            logs.append(line)\n        log_labels = logs.pop(0)\n        \n    for i in range(len(logs)):\n        for j in range(3):\n            img_path = logs[i][j]\n            img_path = features_directory + 'IMG' + (img_path.split('IMG')[1]).strip()\n            img = plt.imread(img_path)\n            features.append(image_preprocessing(img))\n            \n            if j == 0:\n                labels.append(float(logs[i][3]))\n            elif j == 1:\n                labels.append(float(logs[i][3]) + delta)\n            else:\n                labels.append(float(logs[i][3]) - delta)\n    return features,labels","b63f8be2":"def loadFromPickle():\n    with open('features','rb') as f:\n        features = np.array(pickle.load(f))\n    with open('labels','rb') as f:\n        labels = np.array(pickle.load(f))\n    return features,labels\n\ndef augmentData(features,labels):\n    features = np.append(features,features[:,:,::-1],axis=0)\n    labels = np.append(labels,-labels,axis=0)\n    return features,labels","020abd88":"features_directory = '..\/input\/self driving car training data\/data\/'\nlabels_file = '..\/input\/self driving car training data\/data\/driving_log.csv'","8c627447":"delta = 0.2\nfeatures,labels = load_training(delta)\n\nfeatures = np.array(features).astype('float32')\nlabels = np.array(labels).astype('float32')\n\nwith open('features','wb') as f:\n    pickle.dump(features,f,protocol=4)\nwith open('labels','wb') as f:\n    pickle.dump(labels,f,protocol=4)","bbc70e74":"pan = pd.Panel(features)\ndf = pan.swapaxes(1,2).to_frame()\ndf.index = df.index.droplevel('major')\ndf.index = df.index+1","8685c2ce":"features,labels = loadFromPickle()\nfeatures,labels = shuffle(features,labels)\n\nx_train,x_val,y_train,y_val = train_test_split(features,labels,random_state=42,test_size=0.2)\n\nx_train = x_train.reshape(x_train.shape[0],40,40,1)\nx_val = x_val.reshape(x_val.shape[0],40,40,1)","f8f6dce4":"base_model = MobileNetV2(include_top=False,weights=None,input_shape=(40,40,1))\n\nfor layer in base_model.layers:\n    layer.trainable = False\n    \nmodel = Sequential()\nmodel.add(base_model)\nmodel.add(GlobalAveragePooling2D())\nmodel.add(Dropout(0.5))\nmodel.add(Dense(1,activation='sigmoid'))\nmodel.summary()\nplot_model(model, to_file='model_plot.png', show_shapes=True, show_layer_names=True)","9c027aef":"#-------Callbacks-------------#\nbest_model_weights = '.\/base.model'\ncheckpoint = ModelCheckpoint(\n    best_model_weights,\n    monitor='val_loss',\n    verbose=1,\n    save_best_only=True,\n    mode='min',\n    save_weights_only=False,\n    period=1\n)\nearlystop = EarlyStopping(\n    monitor='val_loss',\n    min_delta=0.001,\n    patience=10,\n    verbose=1,\n    mode='auto'\n)\ntensorboard = TensorBoard(\n    log_dir = '.\/logs',\n    histogram_freq=0,\n    batch_size=16,\n    write_graph=True,\n    write_grads=True,\n    write_images=False,\n)\n\ncsvlogger = CSVLogger(\n    filename= \"training_csv.log\",\n    separator = \",\",\n    append = False\n)\n\n#lrsched = LearningRateScheduler(step_decay,verbose=1)\n\nreduce = ReduceLROnPlateau(\n    monitor='val_loss',\n    factor=0.5,\n    patience=10,\n    verbose=1, \n    mode='auto',\n    cooldown=1 \n)\n\ncallbacks = [checkpoint,tensorboard,csvlogger,reduce]","81953019":"opt = SGD(lr=1e-4,momentum=0.99)\nopt1 = Adam(lr=1e-3)\n\nmodel.compile(\n    loss='binary_crossentropy',\n    optimizer='rmsprop',\n    metrics=['accuracy']\n)\n    \nhistory = model.fit(\n    x_train,\n    y_train,\n    validation_data=(x_val,y_val),\n    epochs = 50, \n    verbose = 1,\n    callbacks=callbacks,\n    batch_size = 256\n)","3cec145d":"show_final_history(history)\nmodel.load_weights(best_model_weights)\n\nmodel_json = model.to_json()\nwith open(\"model.json\",\"w\") as json_file:\n    json_file.write(model_json)\n    \nmodel.save(\"model.h5\")\nprint(\"Weights Saved\")\nprint(\"JSON Saved\")","5073d441":"!wget https:\/\/bin.equinox.io\/c\/4VmDzA7iaHb\/ngrok-stable-linux-amd64.zip\n!unzip ngrok-stable-linux-amd64.zip\nLOG_DIR = '.\/logs' # Here you have to put your log directory\nget_ipython().system_raw(\n    'tensorboard --logdir {} --host 0.0.0.0 --port 8080 &'\n    .format(LOG_DIR)\n)\nget_ipython().system_raw('.\/ngrok http 8080 &')\n! curl -s http:\/\/localhost:4040\/api\/tunnels | python3 -c \\\n    \"import sys, json; print(json.load(sys.stdin)['tunnels'][0]['public_url'])\"","7ba609da":"# Import Dependencies","797afbf6":"# Visualize the training and save the weights and json file","f9068eb3":"# Now we train the model","f936b721":"# Helper Functions","bd414186":"# TensorBoard","3ef15402":"# Load in the data","c90697ef":"# Callbacks","a2c90a4a":"# Create The Model"}}