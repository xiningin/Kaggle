{"cell_type":{"eb753b9d":"code","2922a93e":"code","b844bf7b":"code","5218c6f8":"code","c80d6520":"code","54ac5930":"code","100b03b8":"code","03f4b253":"code","ee600051":"code","52e2ab8d":"code","d40566ac":"code","f6e2885a":"code","4b0e6958":"code","be18c4e5":"code","70e19eec":"code","851fef54":"code","ba914901":"code","aac21da0":"code","e7f8ba33":"code","10f4305d":"code","41d82c4e":"code","4f901741":"code","d454d9ec":"code","f957633b":"code","248d0f2f":"code","26ac68f7":"code","45a453e6":"code","01c1f371":"code","79bfb78f":"code","8616a803":"code","883737ee":"code","2e7294db":"code","5f73a4df":"code","eb93b445":"code","bd978b2a":"code","5966de91":"code","137bce58":"code","ce71c68d":"code","fd7a46be":"code","8eab1ee5":"code","fe9beb51":"code","bf74d753":"markdown"},"source":{"eb753b9d":"import pandas as pd\nimport numpy as np\nimport csv\nimport os\nimport random\nimport torch\nimport torch\ntorch.backends.cudnn.benchmark = True\ntorch.autograd.set_detect_anomaly(False)\ntorch.autograd.profiler.profile(False)\ntorch.autograd.profiler.emit_nvtx(False)\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import Dataset, DataLoader\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import classification_report\nimport transformers\nimport sklearn.model_selection as model_selection\nfrom transformers import XLNetTokenizer, XLNetForSequenceClassification\nimport time\nimport copy\nfrom tqdm.notebook import tqdm","2922a93e":"def set_seed(seed=42):\n    os.environ['PYTHONHASHSEED']=str(seed)\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)","b844bf7b":"df=pd.read_csv(\"..\/input\/amazon-ml-challenge-2021-hackerearth\/train.csv\", escapechar = \"\\\\\", quoting = csv.QUOTE_NONE)","5218c6f8":"num_labels=df[\"BROWSE_NODE_ID\"].nunique()","c80d6520":"id2lbl={lbl: idx for idx,lbl in enumerate(list(df[\"BROWSE_NODE_ID\"].unique()))}\nlbl2id={lbl:idx for idx,lbl in id2lbl.items()}","54ac5930":"set_seed()","100b03b8":"df[\"BROWSE_NODE_ID\"]=df[\"BROWSE_NODE_ID\"].map(id2lbl)","03f4b253":"def create_folds(data, num_splits):\n    data[\"kfold\"] = -1\n    data = data.sample(frac=1).reset_index(drop=True)\n    y=df[\"BROWSE_NODE_ID\"]\n    kf = model_selection.StratifiedKFold(n_splits=num_splits)\n    for f, (t_, v_) in enumerate(kf.split(X=data, y=y)):\n        data.loc[v_, 'kfold'] = f\n    return data","ee600051":"df=create_folds(df, 5)","52e2ab8d":"df=df.loc[df.kfold.isin([0,1])]\ndf=df.reset_index(drop=True)\ndf.head()","d40566ac":"df.loc[df['BROWSE_NODE_ID']==1045].head()","f6e2885a":"df.info()","4b0e6958":"df.isnull().sum()","be18c4e5":"temp=df.dropna(subset=['TITLE'])\ntemp=temp.reset_index(drop=True)","70e19eec":"temp.isnull().sum()","851fef54":"total_words=0\nfor title in temp['TITLE']:\n    total_words+=len(title.split())","ba914901":"total_words\/len(temp)","aac21da0":"temp=df.dropna(subset=['DESCRIPTION'])\ntotal_words=0\nfor title in temp['DESCRIPTION']:\n    total_words+=len(title.split())\ntotal_words\/len(temp)","e7f8ba33":"temp=df.dropna(subset=['TITLE'])\ntemp=temp.fillna(\" \")","10f4305d":"temp.head()","41d82c4e":"\" \".join(temp[\"BULLET_POINTS\"][0].split(\",\"))[:-1][1:]","4f901741":"wholeSentence=[]\nfor idx,row in temp.iterrows(): \n    if(idx%100000==0):\n        print(f\"{idx} Done\")\n    wholeSentence.append(row[0]+row[1]+\" \".join(row[2].split(\",\"))[:-1][1:])","d454d9ec":"temp[\"WHOLE SENTENCE\"]=wholeSentence","f957633b":"temp=temp.reset_index(drop=True)","248d0f2f":"temp.head()","26ac68f7":"dev=torch.device('cuda')","45a453e6":"temp[\"BROWSE_NODE_ID\"].value_counts()","01c1f371":"train_text, val_text, train_labels, val_labels = train_test_split(temp['WHOLE SENTENCE'], temp['BROWSE_NODE_ID'],\n                                                                    test_size=0.05)","79bfb78f":"train_text=train_text.reset_index(drop=True)\ntrain_labels=train_labels.reset_index(drop=True)\nval_text=val_text.reset_index(drop=True)\nval_labels=val_labels.reset_index(drop=True)","8616a803":"tokenizer = XLNetTokenizer.from_pretrained('xlnet-base-cased')","883737ee":"seq_len = [len(i.split()) for i in train_text]\n\npd.Series(seq_len).hist(bins = 30)","2e7294db":"max_seq_len = 64","5f73a4df":"class amazonDataset(Dataset):\n  def __init__(self,text,label,tokenizer):\n    self.sentence=text\n    self.label=label\n    self.tokenizer=tokenizer\n\n  def __len__(self):\n    return len(self.sentence)\n  \n  def __getitem__(self,idx):\n    inp_tokens=self.tokenizer.encode_plus(self.sentence[idx], \n                                          padding=\"max_length\", \n                                          add_special_tokens=True,\n                                          max_length=max_seq_len, \n                                          truncation=True)\n    inp_id=inp_tokens.input_ids\n    inp_mask=inp_tokens.attention_mask\n    inp_type_ids=inp_tokens.token_type_ids\n    labels=self.label[idx]\n\n    return {\n#         \"text\":self.sentence,\n        \"input_ids\":torch.tensor(inp_id, dtype=torch.long),\n        \"input_attention_mask\":torch.tensor(inp_mask, dtype=torch.long),\n        \"input_type_ids\":torch.tensor(inp_type_ids, dtype=torch.long),\n        \"labels\":torch.tensor(labels, dtype=torch.float)\n    }","eb93b445":"train_dataset = amazonDataset(train_text, train_labels, tokenizer)\nval_dataset = amazonDataset(val_text, val_labels, tokenizer)","bd978b2a":"train_dataloader=DataLoader(train_dataset,\n                            batch_size=164,\n                            shuffle=True,\n                            num_workers=2,\n                           pin_memory=True)\nval_dataloader=DataLoader(val_dataset,\n                            batch_size=164,\n                            shuffle=False,\n                            num_workers=2,\n                           pin_memory=True)","5966de91":"dataloaders={'train':train_dataloader, 'eval':val_dataloader }\ndataset_sizes={'train':len(train_dataset), 'eval':len(val_dataset)}","137bce58":"# class BERTBaseUncased(nn.Module):\n#     def __init__(self):\n#         super(BERTBaseUncased, self).__init__()\n#         self.bert=AutoModel.from_pretrained('bert-base-uncased')\n#         self.dropout = nn.Dropout(0.1)\n#         self.relu =  nn.ReLU()\n#         self.fc1 = nn.Linear(768,9919)\n        \n#     def forward(self,ids,mask,token_type_ids):\n#         a, o2 = self.bert(\n#             ids,\n#             attention_mask=mask,\n#             token_type_ids=token_type_ids)\n#         bo=self.dropout(o2)\n#         output=self.fc1(bo)\n#         return output","ce71c68d":"# model=XLNetForSequenceClassification.from_pretrained('xlnet-base-cased',\n#                                                     num_labels=9919)\n# print(model)\nmodel=torch.load(\"..\/input\/amazon-ml-models\/XLNetNoset.pth\")\nprint(model)\nmodel.to(dev)\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.SGD(model.parameters(), lr=1e-5, momentum=0.9)\nexp_lr_scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.1)","fd7a46be":"def train_fn(model,loss_fn,optimizer,scheduler,num_epochs=1):\n    since=time.time()\n    best_wts=copy.deepcopy(model.state_dict())\n    best_loss=float('inf')\n    for epoch in range(num_epochs):\n        print(f'Epoch:{epoch}\/{num_epochs}')\n        print('-'*10)\n        \n        for mode in ['train','eval']:\n            if mode=='train':\n                model.train()\n            elif mode=='eval':\n                model.eval()\n            \n            running_loss=0.0\n            running_corrects=0.0\n            \n            for data in tqdm(dataloaders[mode]):\n                input_ids = data[\"input_ids\"].to(dev, dtype=torch.long)\n                labels = data['labels'].to(dev, dtype=torch.long)\n                mask = data[\"input_attention_mask\"].to(dev, dtype=torch.long)\n                token_type_ids = data['input_type_ids'].to(dev, dtype=torch.long)\n            \n                optimizer.zero_grad()\n                with torch.set_grad_enabled(mode=='train'):\n                    outputs=model(\n                                input_ids =input_ids,\n                                attention_mask=mask,\n                                token_type_ids=token_type_ids,\n                                labels=labels\n                            )\n                    loss, logits=outputs.loss, outputs.logits\n                    _,preds=torch.max(logits,1)\n                    \n                    if mode=='train':\n                        loss.backward()\n                        optimizer.step()                    \n                    running_loss += loss.item()                    \n                    running_corrects += torch.sum(preds == labels.data)\n\n            if mode == 'train':\n                scheduler.step()\n                \n            epoch_loss=running_loss\/dataset_sizes[mode]\n            epoch_accuracy=running_corrects.double()\/dataset_sizes[mode]\n            \n            print('{} Loss: {:.4f} Acc: {:.4f}'.format(\n                mode, epoch_loss, epoch_accuracy))\n            \n            if mode=='eval' and epoch_loss<best_loss:\n                best_wts=copy.deepcopy(model.state_dict())\n                best_acc=epoch_accuracy\n                best_loss=epoch_loss\n            \n            print()\n\n        time_elapsed = time.time() - since\n        print('Training complete in {:.0f}m {:.0f}s'.format(\n            time_elapsed \/\/ 60, time_elapsed % 60))\n        print('Best val loss: {:4f}'.format(best_loss))\n        print('Best val Acc: {:4f}'.format(best_acc))\n    \n        model.load_state_dict(best_wts)\n    return model","8eab1ee5":"model = train_fn(model, \n               criterion, \n               optimizer, \n               exp_lr_scheduler,\n               num_epochs=3)","fe9beb51":"torch.save(model,\"XLNetNoset.pth\")","bf74d753":"# XLNET"}}