{"cell_type":{"8c1227a2":"code","c8580c15":"code","37236bb6":"code","af0c684e":"code","01af3483":"code","37e1d55e":"code","b5f85696":"code","1f8a582c":"code","4b50c091":"code","3820b188":"code","fc278f30":"code","c0810450":"code","e5ad71cb":"code","d9677b01":"code","3f4272ec":"code","784b274e":"code","87fed2c4":"code","0a392908":"markdown","9efda68d":"markdown","afdba8c1":"markdown","c65bc79b":"markdown","5da1091e":"markdown","596df19f":"markdown","2b66b06a":"markdown"},"source":{"8c1227a2":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n#import matplotlib.image as mpimg\n%matplotlib inline\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom torch.autograd import Variable\n#from torch.utils.data import Dataset\nfrom torchvision.models import vgg19\nimport torchvision.transforms as transforms\n#from collections import OrderedDict\n%pylab inline\nprint('Pytorch version: {}'.format(torch.__version__))\n\n\nfrom PIL import Image\nimport time\nimport os\nprint(os.listdir(\"..\/input\"))\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint('Computational device: {}'.format(device))","c8580c15":"StylePath = '\/kaggle\/input\/styles\/'\nContentPath = '\/kaggle\/input\/content\/'\n\nname_imgC = 'WechatIMG1.jpeg'\nname_imgS = 'picasso1.jpg'\n\ncont_image_path = ContentPath + name_imgC\nstyle_image_path = StylePath + name_imgS\n\nimageC = Image.open(cont_image_path)\nimageS = Image.open(style_image_path)","37236bb6":"fig, ax = plt.subplots(1,2, figsize=(18, 12))\nax[0].set_title('Content image', fontsize=\"20\")\nax[0].imshow(imageC.resize((512,512)))  \nax[1].set_title('Style image', fontsize=\"20\")\nax[1].imshow(imageS.resize((512,512)))","af0c684e":"imsize = 512\n\nprep = transforms.Compose([transforms.Resize((imsize,imsize)),\n                           transforms.ToTensor(),\n                           transforms.Lambda(lambda x: x[torch.LongTensor([2,1,0])]), \n                           transforms.Normalize(mean=[0.40760392, 0.45795686, 0.48501961], #subtract imagenet mean\n                                                std=[1,1,1]),\n                           transforms.Lambda(lambda x: x.mul_(255)),\n                          ])\n\n\npostpa = transforms.Compose([transforms.Lambda(lambda x: x.mul_(1.\/255)),\n                           transforms.Normalize(mean=[-0.40760392, -0.45795686, -0.48501961], #add imagenet mean\n                                                std=[1,1,1]),\n                           transforms.Lambda(lambda x: x[torch.LongTensor([2,1,0])]), #turn to RGB\n                           ])\n\n\npostpb = transforms.Compose([transforms.ToPILImage()])\n\n# Preprocessing: convert image to array\ndef prep_img(img_path):\n    image = Image.open(img_path)\n    image = Variable(prep(image))\n    # network's input need at least a batch size 1\n    image = image.unsqueeze(0)\n    return image.to(device,torch.float)\n\n\n\n\n# Postprocessing: convert array to image\ndef postp(tensor): \n    t = postpa(tensor)\n    # to clip results in the range [0,1]\n    t[t>1] = 1    \n    t[t<0] = 0\n    img = postpb(t)\n    return img","01af3483":"# input style image and content inmage\nstyle_img = prep_img(style_image_path)\ncontent_img = prep_img(cont_image_path)\n#assert style_img.size() == content_img.size(),\"import style and content images are not in the same size\"\n\n# load model in eval mode (model uses BN or Dropout)\nvgg = vgg19(pretrained=True).features.to(device).eval()\n\n# set requires_grad as false, as a result no backprop of the gradients\nfor param in vgg.parameters():\n    param.requires_grad = False\n#    print(param.requires_grad)\n\n\n# initialize the output image as same as the content image or a random noise. The image need to be modified. \nopt_img = Variable(content_img.data.clone(),requires_grad=True)\n\n#input_img = torch.randn(content_img.data.size(), device=device)\n#opt_img = Variable(input_img,requires_grad=True)","37e1d55e":"#style_img.shape\n#content_img.shape\n#opt_img\nvgg","b5f85696":"# choose layers for style\nstyle_layers = [1,6,11,20,26,35]\n\n# one layer for content\ncontent_layers = [29]","1f8a582c":"# use hook to extract activations during forward prop\nclass LayerActivations():\n    features=[]\n    \n    def __init__(self,model,layer_nums):\n        \n        self.hooks = []\n# register activation after forword at eatch layer \n        for layer_num in layer_nums:\n            self.hooks.append(model[layer_num].register_forward_hook(self.hook_fn))\n#     \n    def hook_fn(self,module,input,output):\n        self.features.append(output)\n\n    \n    def remove(self):\n        for hook in self.hooks:\n            hook.remove()","4b50c091":"def extract_layers(layers,img,model=None):\n    la = LayerActivations(model,layers)\n    #Clearing the cache \n    la.features = []\n    # forward prop img and hook registes automatically activations\n    out = model(img)\n    # remove hook but features are already extracted.\n    la.remove()\n    return la.features","3820b188":"class ContentLoss(nn.Module):    \n    \n    def forward(self,inputs,targets):\n        assert inputs.size() == targets.size(),\"need the same size\"\n        b,c,h,w = inputs.size()\n        loss = nn.MSELoss()(inputs, targets)\n        loss.div_(4*c*h*w)\n        return (loss)","fc278f30":"class GramMatrix(nn.Module):\n    \n    def forward(self,input):\n# batch, channel, height, width        \n        b,c,h,w = input.size()\n        features = input.view(b,c,h*w)\n# batch matrix product (b*n*m)        \n        gram_matrix =  torch.bmm(features,features.transpose(1,2))\n        return gram_matrix","c0810450":"class StyleLoss(nn.Module):\n    def forward(self,inputs,targets):\n        assert inputs.size() == targets.size(),\"need the same size\"\n        b,c,h,w = inputs.size()\n        loss = F.mse_loss(GramMatrix()(inputs), GramMatrix()(targets))\n        loss.div_(4*(c*h*w)**2)\n        return (loss)","e5ad71cb":"a_c = extract_layers(content_layers,content_img,model=vgg)\na_c = [t.detach() for t in a_c]\n\na_s = extract_layers(style_layers,style_img,model=vgg)\na_s = [t.detach() for t in a_s]\n\nactivations = a_s + a_c \n#activations","d9677b01":"loss_fns = [StyleLoss()] * len(style_layers) + [nn.MSELoss()] * len(content_layers)","3f4272ec":"# weight of layers (alpha and beta)\nstyle_weights = [100000 for n in range(len(style_layers))]\ncontent_weights = [1]\nweights = style_weights + content_weights","784b274e":"max_iter = 500\nshow_iter = 100\n\n# parameters to optimize (tensors or dicts)\noptimizer = optim.LBFGS([opt_img]);\nn_iter=[0]\n\nwhile n_iter[0] <= max_iter:\n    \n    # evaluate the model and return the loss for optimizer\n    def closure():\n        \n        # clean cach\n        optimizer.zero_grad()\n        \n        # extract acivations of the output image\n        out_sty = extract_layers(style_layers,opt_img,model=vgg)\n        out_cnt = extract_layers(content_layers,opt_img,model=vgg)\n        out =  out_sty + out_cnt\n        \n        # compute losses\n        layer_losses = [weights[a] * loss_fns[a](A, activations[a]) for a,A in enumerate(out)]\n        #print(layer_losses[0])\n        \n        # .backward apply to a scaler\n        loss = sum(layer_losses)\n        \n        # compute gradients\n        loss.backward()\n        n_iter[0]+=1\n        \n        if n_iter[0]%show_iter == (show_iter-1):\n            print('Iteration: %d, loss: %f'%(n_iter[0]+1, loss.item()))\n\n        return loss\n    # parameters update\n    optimizer.step(closure)\n    ","87fed2c4":"#display result\nout_img_hr = postp(opt_img.data[0].cpu().squeeze())\n\nimshow(out_img_hr)\ngcf().set_size_inches(10,10)","0a392908":"# Precompute activations of the content and style images","9efda68d":"# Main loop","afdba8c1":"#### Im newbie in this new field and I practice pytorch here.\n\n- *The main objective of this notebook is to understand and hands-on coding in Pytorch.*\n- *This notebook is inspired from serveral works and I listed them in the Reference at the end.*\n- *Key words:*\n> - pretrained VGG19\n> - hook function\n> - GPU\n> - content loss, style loss, gram matrix\n- *Feel free to discuss :)*","c65bc79b":"# Loss functions\n- forward propagate image S and obtain the activation  $a^{(S)}$  at some layers (style).\n- forward propagate image C and obtain the activation  $a^{(C)}$  at one layer (content).\n- forward propagate image G and obtain the activation  $a^{(G)}$  at some layer (style + content).\n\n- content loss function  $J_{content}(C,G)$ :\n\n$J_{content}(C,G) =  \\frac{1}{4 \\times n_H \\times n_W \\times n_C}\\sum _{ \\text{all entries}} (a^{(C)} - a^{(G)})^2\\tag{1} $\n\n- style loss function $J_{style}(S,G)$ \n> - Gram matix: measures how similar the activations of filter i are to the activations of filter j, $G_{ij}=A_iA^T_j$.\n> - $J_{style}^{[l]}(S,G) = \\frac{1}{4 \\times {n_C}^2 \\times (n_H \\times n_W)^2} \\sum _{i=1}^{n_C}\\sum_{j=1}^{n_C}\n(G^{(S)}_{(gram)i,j} - G^{(G)}_{(gram)i,j})^2\\tag{2} $\n> - $J_{style}(S,G) = \\sum_{l} \\lambda^{[l]} J^{[l]}_{style}(S,G)\\tag{3}  $ \n\n- total loss:  $J(G) = \\alpha J_{content}(C,G) + \\beta J_{style}(S,Gv) \\tag{4} $","5da1091e":"# Load the pretrained model (VGG19)","596df19f":"# Preprocessing and postprocessing utility functions\n### transforms:\n> - Resize\n> - convert PIL Image to np array ( [0,255] -> [0,1] )\n> - RGB < - > BGR\n> - Normalize (use imagenet mean and std)\n> - Varaible, Lambda, Normalize\n> - unsqueeze(0): add one dimension because the input needs one sample in the batch ( ex. (4,)->(1,4) )\n","2b66b06a":"# Reference\n- [PyTorch Manual](https:\/\/pytorch.org\/)\n- Sylvin, Chateau, [Style transfer](https:\/\/www.kaggle.com\/schateau\/style-transfer)\n- [Deep Learning with PyTorch](https:\/\/www.packtpub.com\/big-data-and-business-intelligence\/deep-learning-pytorch?utm_source=github&utm_medium=repository&utm_campaign=9781788624336)\n- [NEURAL TRANSFER USING PYTORCH](https:\/\/pytorch.org\/tutorials\/advanced\/neural_style_tutorial.html#style-loss)\n- Neural Networks and Deep Learning by deeplearning.ai"}}