{"cell_type":{"d2896643":"code","3f8352b2":"code","c4161934":"code","ffbe18db":"code","6af6306d":"code","8cd081c4":"code","7c576352":"code","cad2f88e":"code","bf5c2bde":"code","4b09ae5d":"code","3c628e4a":"code","b6fa7e49":"code","a8c94581":"code","d983a7f2":"code","30096b58":"code","24614820":"code","9317b882":"code","d208210e":"code","f6af6a98":"code","a11e5fc0":"code","90b5bcae":"code","5fee8da7":"code","3416a860":"code","7bb8fc9c":"code","8280811a":"code","db0f92ae":"code","64d7f0d5":"code","767c5cfa":"code","0465cee0":"code","78ddf308":"code","f0628c3c":"code","e19dbd58":"code","3aaa2f0a":"code","0b60ab74":"code","49df1352":"code","8e4586d5":"code","27ab55ea":"code","eac0ae02":"code","4f198b59":"code","128f67b6":"code","881214ad":"code","2143ba70":"code","6212ff51":"code","b1f07095":"code","94c7d28a":"code","81291b9e":"code","0ddf30ca":"code","e92260f7":"code","3cdf882f":"code","903df8a2":"code","8d82bf3d":"code","83c3e5b7":"code","18e5af6c":"code","d60a711d":"code","cdbb6ce9":"code","5791bb08":"code","459ab56b":"code","31064889":"code","9bbbb8a2":"code","f0cf607c":"code","2b222b3c":"code","82125f74":"code","7f743fcd":"markdown","912b3866":"markdown","1070a2cf":"markdown","8a452387":"markdown","dc0619b0":"markdown","b4f2015f":"markdown","d582bbc2":"markdown"},"source":{"d2896643":"# \u67e5\u770b\u6570\u636e\u96c6\u6587\u4ef6\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","3f8352b2":"import sys\nimport tensorflow as tf\nimport tensorflow_datasets as tfds     # \u9700\u8981pip install tensorflow-datasets\nfrom tensorflow.keras import Model, Input\nfrom tensorflow.keras.layers import Dense, Lambda, Dropout, add, LayerNormalization, \\\n     Embedding\nfrom tensorflow.keras.utils import plot_model\nfrom tensorflow.keras.losses import SparseCategoricalCrossentropy\nfrom tensorflow.keras.metrics import sparse_categorical_accuracy\nfrom tensorflow.keras.optimizers.schedules import LearningRateSchedule\nfrom tensorflow.math import multiply, rsqrt, minimum\nfrom tensorflow.keras.optimizers import Adam\nimport os\nimport re\nimport numpy as np\nfrom time import time\nimport matplotlib.pyplot as plt\nimport json\nimport codecs\n\ntf.random.set_seed(2021)","c4161934":"# \u63a2\u6d4bTPU\ntry:\n    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n    print('Running on TPU ', tpu.master())\nexcept ValueError:\n    tpu = None\n\nif tpu:\n    tf.config.experimental_connect_to_cluster(tpu)\n    tf.tpu.experimental.initialize_tpu_system(tpu)\n    tpu_strategy = tf.distribute.experimental.TPUStrategy(tpu)\nelse:\n    # Default distribution strategy in Tensorflow. Works on CPU and single GPU.\n    strategy = tf.distribute.get_strategy()\nprint(\"REPLICAS: {}\".format(tpu_strategy.num_replicas_in_sync))","ffbe18db":"dialog_list = json.loads(codecs.open(\"..\/input\/natural-conversation\/dialog_release.json\", \"r\", \"utf-8\").read())\ndialog_list[0]","6af6306d":"document_list = json.loads(codecs.open(\"..\/input\/natural-conversation\/document_url_release.json\", \"r\", \"utf-8\").read())\ndocument_list[0]","8cd081c4":"datafile = 'test.txt'\nwith open(f'..\/input\/natural-conversation\/{datafile}','r') as f:\n    data_list = f.readlines()\n    for i in range(len(data_list)):\n        data_list[i] = re.sub(r'\\n', '', data_list[i])\ndata_list[:10]","7c576352":"# \u4ee5\u4e0b\u53c2\u6570\u53ef\u6839\u636e\u9700\u8981\u8c03\u6574\uff0c\u4e3a\u4e86\u6f14\u793a\u4e4b\u76ee\u7684\uff0c\u53ef\u4ee5\u5c06\u76f8\u5173\u53c2\u6570\u8c03\u4f4e\u4e00\u4e9b\u3002\n# \u6700\u5927\u53e5\u5b50\u957f\u5ea6\nMAX_LENGTH = 40\n\n# \u6700\u5927\u6837\u672c\u6570\u91cf\n# MAX_SAMPLES = 120000  # \u53ef\u4ee5\u63a7\u5236\u6837\u672c\u6570\n\n# For tf.data.Dataset\nBATCH_SIZE = 64 * tpu_strategy.num_replicas_in_sync  #\u6279\u5904\u7406\u5927\u5c0f\n# BUFFER_SIZE = 30000  # \u6570\u636e\u96c6\u7f13\u51b2\u533a\u5927\u5c0f\n\n# Transformer\u53c2\u6570\u5b9a\u4e49\nNUM_LAYERS = 4   # \u7f16\u7801\u5668\u89e3\u7801\u5668block\u91cd\u590d\u6570\uff0c\u8bba\u6587\u4e2d\u662f6\nD_MODEL = 128   # \u7f16\u7801\u5668\u89e3\u7801\u5668\u5bbd\u5ea6\uff0c\u8bba\u6587\u4e2d\u662f512\nNUM_HEADS = 4   # \u6ce8\u610f\u529b\u5934\u6570\uff0c\u4e0e\u8bba\u6587\u4e00\u81f4\nUNITS = 512    # \u5168\u8fde\u63a5\u7f51\u7edc\u5bbd\u5ea6\uff0c\u8bba\u6587\u4e2d\u8f93\u5165\u8f93\u51fa\u4e3a512\nDROPOUT = 0.1 # \u4e0e\u8bba\u6587\u4e00\u81f4\nVOCAB_SIZE = 21128  # \u8bcd\u5178\u957f\u5ea6\n\nSTART_TOKEN = [VOCAB_SIZE]  # \u5e8f\u5217\u8d77\u59cb\u6807\u5fd7\nEND_TOKEN = [VOCAB_SIZE+1] # \u5e8f\u5217\u7ed3\u675f\u6807\u5fd7\nVOCAB_SIZE = VOCAB_SIZE + 2  # \u52a0\u4e0a\u5f00\u59cb\u4e0e\u7ed3\u675f\u6807\u5fd7\u540e\u7684\u8bcd\u5178\u957f\u5ea6\nEPOCHS = 50  # \u8bad\u7ec3\u4ee3\u6570","cad2f88e":"# \u6790\u53d6\u6570\u636e\u96c6\uff08\u8bad\u7ec3\u96c6\u3001\u9a8c\u8bc1\u96c6\u3001\u6d4b\u8bd5\u96c6\uff09\uff0c\u5c06\u6240\u6709\u7684\u201c\u95ee\u201d\u4e0e\u201c\u7b54\u201d\u5206\u5f00\ndef extract_conversations(datafile):\n    with open(f'..\/input\/natural-conversation\/{datafile}','r') as f:\n        data_list = f.readlines()\n    for i in range(len(data_list)):\n        data_list[i] = re.sub(r'\\n', '', data_list[i])\n    \n    inputs, outputs = [], []  # \u95ee\u7b54\u5217\u8868\n    for dialog in dialog_list:\n        if dialog['dialog_id'] in data_list:\n            if len(dialog['content']) % 2 == 0:\n                i = 0\n                for line in dialog['content']:\n                    if (i % 2 == 0):\n                        inputs.append(line) # \u201c\u95ee\u201d\u5217\u8868\n                    else:        \n                        outputs.append(line)  # \u201c\u7b54\u201d\u5217\u8868\n                    i += 1\n#                     if len(inputs) >= MAX_SAMPLES:\n#                         return inputs, outputs\n    return inputs, outputs           \n\n# \u6790\u53d6\u8bad\u7ec3\u96c6\ntrain_questions, train_answers = extract_conversations('train.txt')\n# \u6790\u53d6\u9a8c\u8bc1\u96c6\nvalid_questions, valid_answers = extract_conversations('dev.txt')\n# \u6790\u53d6\u6d4b\u8bd5\u96c6\ntest_questions, test_answers = extract_conversations('test.txt')","bf5c2bde":"print(f'\u8bad\u7ec3\u96c6\u63d0\u95ee\u6837\u672c\u89c2\u5bdf: {train_questions[10]}')\nprint(f'\u8bad\u7ec3\u96c6\u5e94\u7b54\u6837\u672c\u89c2\u5bdf: {train_answers[10]}')","4b09ae5d":"print(f'\u9a8c\u8bc1\u96c6\u63d0\u95ee\u6837\u672c\u89c2\u5bdf: {valid_questions[10]}')\nprint(f'\u9a8c\u8bc1\u96c6\u5e94\u7b54\u6837\u672c\u89c2\u5bdf: {valid_answers[10]}')","3c628e4a":"print(f'\u6d4b\u8bd5\u96c6\u63d0\u95ee\u6837\u672c\u89c2\u5bdf: {test_questions[10]}')\nprint(f'\u6d4b\u8bd5\u96c6\u5e94\u7b54\u6837\u672c\u89c2\u5bdf: {test_answers[10]}')","b6fa7e49":"print(f'\u8bad\u7ec3\u96c6\u6837\u672c\u603b\u6570: {len(train_questions)}')\nprint(f'\u9a8c\u8bc1\u96c6\u6837\u672c\u603b\u6570: {len(valid_questions)}')\nprint(f'\u6d4b\u8bd5\u96c6\u6837\u672c\u603b\u6570: {len(test_questions)}')","a8c94581":"len(train_answers)","d983a7f2":"train_questions[:10]","30096b58":"train_answers[:10]","24614820":"!pip install bert-for-tf2","9317b882":"from bert.tokenization.bert_tokenization import FullTokenizer\nbert_vocab_file = '..\/input\/natural-conversation\/vocab.txt'\ntokenizer = FullTokenizer(bert_vocab_file)","d208210e":"tokens = tokenizer.tokenize(train_questions[10])\ntokens","f6af6a98":"tokenizer.convert_tokens_to_ids(tokens)","a11e5fc0":"tokenizer.convert_ids_to_tokens([1511,\n 1435,\n 8024,\n 2769,\n 6206,\n 1343,\n 1391,\n 7649,\n 749,\n 8024,\n 1086,\n 6224,\n 1506,\n 8013])","90b5bcae":"%%time\n# \u5206\u8bcd\uff0c\u8fc7\u6ee4\u6389\u8d85\u8fc7\u957f\u5ea6\u7684\u53e5\u5b50\uff0c\u77ed\u53e5\u8865\u9f50\ndef tokenize_and_filter(inputs, outputs):\n    tokenized_inputs, tokenized_outputs = [], []\n\n    for (sentence1, sentence2) in zip(inputs, outputs):\n        sentence1 = tokenizer.tokenize(sentence1) # \u5206\u8bcd\n        sentence1 = tokenizer.convert_tokens_to_ids(sentence1) # ids\n        sentence2 = tokenizer.tokenize(sentence2)\n        sentence2 = tokenizer.convert_tokens_to_ids(sentence2)\n        sentence1 = START_TOKEN + sentence1 + END_TOKEN\n        sentence2 = START_TOKEN + sentence2 + END_TOKEN       \n        \n        if len(sentence1) <= MAX_LENGTH and len(sentence2) <= MAX_LENGTH:\n            tokenized_inputs.append(sentence1)\n            tokenized_outputs.append(sentence2)\n\n    # \u8865\u9f50\n    tokenized_inputs = tf.keras.preprocessing.sequence.pad_sequences( \\\n        tokenized_inputs, maxlen=MAX_LENGTH, padding='post')\n    tokenized_outputs = tf.keras.preprocessing.sequence.pad_sequences( \\\n        tokenized_outputs, maxlen=MAX_LENGTH, padding='post')\n\n    return tokenized_inputs, tokenized_outputs\n\n\ntrain_questions, train_answers = tokenize_and_filter( \\\n                 list(train_questions), list(train_answers))","5fee8da7":"train_questions[10]","3416a860":"print(f'\u8bad\u7ec3\u96c6\u6700\u65b0\u6837\u672c\u6570: {len(train_questions)}')","7bb8fc9c":"# \u6784\u5efa\u8bad\u7ec3\u96c6\ntrain_dataset = tf.data.Dataset.from_tensor_slices((\n    {\n        'inputs': train_questions, \n        # \u89e3\u7801\u5668\u4f7f\u7528\u6b63\u786e\u7684\u6807\u7b7e\u505a\u4e3a\u8f93\u5165\n        'dec_inputs': train_answers[:, :-1] # \u53bb\u6389\u6700\u540e\u4e00\u4e2a\u5143\u7d20\u6216 END_TOKEN\n    },\n    {\n        'outputs': train_answers[:, 1:]  # \u53bb\u6389 START_TOKEN\n    },\n))\n\ntrain_dataset = train_dataset.cache()\ntrain_dataset = train_dataset.shuffle(len(train_questions))\ntrain_dataset = train_dataset.batch(BATCH_SIZE)\ntrain_dataset = train_dataset.prefetch(tf.data.experimental.AUTOTUNE)","8280811a":"# \u6784\u5efa\u9a8c\u8bc1\u96c6\nvalid_questions, valid_answers = tokenize_and_filter( \\\n                 list(valid_questions), list(valid_answers))\nvalid_dataset = tf.data.Dataset.from_tensor_slices((\n    {\n        'inputs': valid_questions, \n        # \u89e3\u7801\u5668\u4f7f\u7528\u6b63\u786e\u7684\u6807\u7b7e\u505a\u4e3a\u8f93\u5165\n        'dec_inputs': valid_answers[:, :-1] # \u53bb\u6389\u6700\u540e\u4e00\u4e2a\u5143\u7d20\u6216 END_TOKEN\n    },\n    {\n        'outputs': valid_answers[:, 1:]  # \u53bb\u6389START_TOKEN\n    },\n))\n\nvalid_dataset = valid_dataset.cache()\nvalid_dataset = valid_dataset.shuffle(len(valid_questions))\nvalid_dataset = valid_dataset.batch(BATCH_SIZE)\nvalid_dataset = valid_dataset.prefetch(tf.data.experimental.AUTOTUNE)","db0f92ae":"print(train_dataset)","64d7f0d5":"# \u8ba1\u7b97\u6ce8\u610f\u529b\ndef scaled_dot_product_attention(query, key, value, mask):\n    matmul_qk = tf.matmul(query, key, transpose_b=True)\n\n    # \u8ba1\u7b97qk\n    depth = tf.cast(tf.shape(key)[-1], tf.float32)\n    logits = matmul_qk \/ tf.math.sqrt(depth)\n\n    # \u6dfb\u52a0\u63a9\u7801\u4ee5\u5c06\u586b\u5145\u6807\u8bb0\u5f52\u96f6\n    if mask is not None:\n        logits += (mask * -1e9)\n\n    # \u5728\u6700\u540e\u4e00\u4e2a\u8f74\u4e0a\u5b9e\u65bdsoftmax\n    attention_weights = tf.nn.softmax(logits, axis=-1)\n\n    output = tf.matmul(attention_weights, value)\n\n    return output","767c5cfa":"# \u5b9a\u4e49\u591a\u5934\u6ce8\u610f\u529b\u7c7b\uff0c\u7ee7\u627f\u4e86Layer\u7c7b\nclass MultiHeadAttention(tf.keras.layers.Layer):\n\n    def __init__(self, d_model, num_heads, name=\"multi_head_attention\"):\n        super(MultiHeadAttention, self).__init__(name=name)\n        self.num_heads = num_heads\n        self.d_model = d_model\n\n        assert d_model % self.num_heads == 0\n\n        self.depth = d_model \/\/ self.num_heads\n\n        self.query_dense = Dense(units=d_model)\n        self.key_dense = Dense(units=d_model)\n        self.value_dense = Dense(units=d_model)\n\n        self.dense = Dense(units=d_model)\n\n    def get_config(self):\n        config = super(MultiHeadAttention, self).get_config()\n        config.update({\n            'num_heads':self.num_heads,\n            'd_model':self.d_model,\n        })\n        return config\n\n    def split_heads(self, inputs, batch_size):\n        inputs = Lambda(lambda inputs:tf.reshape(inputs, \\\n                shape=(batch_size, -1, self.num_heads, self.depth)))(inputs)\n        return Lambda(lambda inputs: tf.transpose(inputs, perm=[0, 2, 1, 3]))(inputs)\n\n    def call(self, inputs):\n        query, key, value, mask = inputs['query'], inputs['key'], inputs[\n            'value'], inputs['mask']\n        batch_size = tf.shape(query)[0]\n\n        # \u7ebf\u6027\u5c42\u53d8\u6362\n        query = self.query_dense(query)\n        key = self.key_dense(key)\n        value = self.value_dense(value)\n\n        # \u5206\u5934\n        query = self.split_heads(query, batch_size)\n        key = self.split_heads(key, batch_size)\n        value = self.split_heads(value, batch_size)\n\n        # \u5b9a\u4e49\u7f29\u653e\u7684\u70b9\u79ef\u6ce8\u610f\u529b\n        scaled_attention = scaled_dot_product_attention(query, key, value, mask)\n        scaled_attention = Lambda(lambda scaled_attention: tf.transpose(\n            scaled_attention, perm=[0, 2, 1, 3]))(scaled_attention)\n\n        # \u5806\u53e0\u6ce8\u610f\u529b\u5934\n        concat_attention = Lambda(lambda scaled_attention: tf.reshape(scaled_attention,\n                                      (batch_size, -1, self.d_model)))(scaled_attention)\n\n        # \u591a\u5934\u6ce8\u610f\u529b\u6700\u540e\u4e00\u5c42\n        outputs = self.dense(concat_attention)\n\n        return outputs  ","0465cee0":"def create_padding_mask(x):\n    mask = tf.cast(tf.math.equal(x, 0), tf.float32)\n    # (batch_size, 1, 1, sequence length)\n    return mask[:, tf.newaxis, tf.newaxis, :]","78ddf308":"print(create_padding_mask(tf.constant([[1, 2, 0, 3, 0], [0, 0, 0, 4, 5]])))","f0628c3c":"def create_look_ahead_mask(x):\n    seq_len = tf.shape(x)[1]\n    look_ahead_mask = 1 - tf.linalg.band_part(tf.ones((seq_len, seq_len)), -1, 0)\n    padding_mask = create_padding_mask(x)\n    return tf.maximum(look_ahead_mask, padding_mask)","e19dbd58":"print(create_look_ahead_mask(tf.constant([[1, 2, 0, 4, 5]])))","3aaa2f0a":"# \u4f4d\u7f6e\u7f16\u7801\u7c7b\nclass PositionalEncoding(tf.keras.layers.Layer):\n\n    def __init__(self, position, d_model):\n        super(PositionalEncoding, self).__init__()\n        self.pos_encoding = self.positional_encoding(position, d_model)\n  \n    def get_config(self):\n        config = super(PositionalEncoding, self).get_config()\n        config.update({\n            'position': self.position,\n            'd_model': self.d_model,\n\n        })\n        return config\n\n    def get_angles(self, position, i, d_model):\n        angles = 1 \/ tf.pow(10000, (2 * (i \/\/ 2)) \/ tf.cast(d_model, tf.float32))\n        return position * angles\n\n    def positional_encoding(self, position, d_model):\n        angle_rads = self.get_angles( \\\n            position=tf.range(position, dtype=tf.float32)[:, tf.newaxis], \\\n            i=tf.range(d_model, dtype=tf.float32)[tf.newaxis, :], \\\n            d_model=d_model)\n        # \u5947\u6570\u4f4d\u7f6e\u7528sin\u51fd\u6570\n        sines = tf.math.sin(angle_rads[:, 0::2])\n        # \u5076\u6570\u4f4d\u7f6e\u7528cos\u51fd\u6570\n        cosines = tf.math.cos(angle_rads[:, 1::2])\n\n        pos_encoding = tf.concat([sines, cosines], axis=-1)\n        pos_encoding = pos_encoding[tf.newaxis, ...]\n        return tf.cast(pos_encoding, tf.float32)\n\n    def call(self, inputs):\n        return inputs + self.pos_encoding[:, :tf.shape(inputs)[1], :]","0b60ab74":"sample_pos_encoding = PositionalEncoding(50, 512)\n\nplt.pcolormesh(sample_pos_encoding.pos_encoding.numpy()[0], cmap='RdBu')\nplt.xlabel('Depth')\nplt.xlim((0, 512))\nplt.ylabel('Position')\nplt.colorbar()\nplt.show()","49df1352":"# \u7f16\u7801\u5668\u4e2d\u7684\u4e00\u5c42\ndef encoder_layer(units, d_model, num_heads, dropout, name=\"encoder_layer\"):\n    inputs = tf.keras.Input(shape=(None, d_model), name=\"inputs\")\n    padding_mask = tf.keras.Input(shape=(1, 1, None), name=\"padding_mask\")\n\n    attention = MultiHeadAttention( \\\n      d_model, num_heads, name=\"attention\")({ \\\n          'query': inputs,\n          'key': inputs,\n          'value': inputs,\n          'mask': padding_mask\n      })\n    attention = Dropout(rate=dropout)(attention)\n    add_attention = add([inputs,attention])\n    attention = LayerNormalization(epsilon=1e-6)(add_attention)\n\n    outputs = Dense(units=units, activation='relu')(attention)\n    outputs = Dense(units=d_model)(outputs)\n    outputs = Dropout(rate=dropout)(outputs)\n    add_attention = add([attention,outputs])\n    outputs = LayerNormalization(epsilon=1e-6)(add_attention)\n\n    return Model(inputs=[inputs, padding_mask], outputs=outputs, name=name)","8e4586d5":"sample_encoder_layer = encoder_layer(\n    units=512,\n    d_model=128,\n    num_heads=4,\n    dropout=0.3,\n    name=\"sample_encoder_layer\")\n\nplot_model(sample_encoder_layer, to_file='encoder_layer.png', show_shapes=True)","27ab55ea":"# \u5b9a\u4e49\u7f16\u7801\u5668\ndef encoder(vocab_size,\n            num_layers,\n            units,\n            d_model,\n            num_heads,\n            dropout,\n            name=\"encoder\"):\n    inputs = Input(shape=(None,), name=\"inputs\")\n    padding_mask = Input(shape=(1, 1, None), name=\"padding_mask\")\n\n    embeddings = Embedding(vocab_size, d_model)(inputs)\n    embeddings *= Lambda(lambda d_model: tf.math.sqrt(tf.cast(d_model, tf.float32)))(d_model)\n    embeddings = PositionalEncoding(vocab_size,d_model)(embeddings)\n\n    outputs = Dropout(rate=dropout)(embeddings)\n\n    for i in range(num_layers):\n        outputs = encoder_layer(\n            units=units,\n            d_model=d_model,\n            num_heads=num_heads,\n            dropout=dropout,\n            name=\"encoder_layer_{}\".format(i),\n        )([outputs, padding_mask])\n\n    return Model(inputs=[inputs, padding_mask], outputs=outputs, name=name)","eac0ae02":"sample_encoder = encoder(\n    vocab_size=21128,\n    num_layers=2,\n    units=512,\n    d_model=128,\n    num_heads=4,\n    dropout=0.3,\n    name=\"sample_encoder\")\n\nplot_model(sample_encoder, to_file='encoder.png', show_shapes=True)","4f198b59":"# \u5b9a\u4e49\u89e3\u7801\u5668\u4e2d\u7684\u4e00\u5c42\ndef decoder_layer(units, d_model, num_heads, dropout, name=\"decoder_layer\"):\n    inputs = Input(shape=(None, d_model), name=\"inputs\")\n    enc_outputs = Input(shape=(None, d_model), name=\"encoder_outputs\")\n    look_ahead_mask = Input(shape=(1, None, None), name=\"look_ahead_mask\")\n    padding_mask = Input(shape=(1, 1, None), name='padding_mask')\n\n    attention1 = MultiHeadAttention(\n      d_model, num_heads, name=\"attention_1\")(inputs={\n          'query': inputs,\n          'key': inputs,\n          'value': inputs,\n          'mask': look_ahead_mask\n      })\n    add_attention = tf.keras.layers.add([attention1,inputs])    \n    attention1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)(add_attention)\n\n    attention2 = MultiHeadAttention(\n      d_model, num_heads, name=\"attention_2\")(inputs={\n          'query': attention1,\n          'key': enc_outputs,\n          'value': enc_outputs,\n          'mask': padding_mask\n      })\n    attention2 = Dropout(rate=dropout)(attention2)\n    add_attention = add([attention2,attention1])\n    attention2 = LayerNormalization(epsilon=1e-6)(add_attention)\n\n    outputs = Dense(units=units, activation='relu')(attention2)\n    outputs = Dense(units=d_model)(outputs)\n    outputs = Dropout(rate=dropout)(outputs)\n    add_attention = add([outputs,attention2])\n    outputs = LayerNormalization(epsilon=1e-6)(add_attention)\n\n    return Model(\n          inputs=[inputs, enc_outputs, look_ahead_mask, padding_mask],\n          outputs=outputs,\n          name=name)","128f67b6":"sample_decoder_layer = decoder_layer(\n    units=512,\n    d_model=128,\n    num_heads=4,\n    dropout=0.3,\n    name=\"sample_decoder_layer\")\n\nplot_model(sample_decoder_layer, to_file='decoder_layer.png', show_shapes=True)","881214ad":"# \u5b9a\u4e49\u89e3\u7801\u5668\ndef decoder(vocab_size,\n            num_layers,\n            units,\n            d_model,\n            num_heads,\n            dropout,\n            name='decoder'):\n    inputs = Input(shape=(None,), name='inputs')\n    enc_outputs = Input(shape=(None, d_model), name='encoder_outputs')\n    look_ahead_mask = Input(shape=(1, None, None), name='look_ahead_mask')\n    padding_mask = Input(shape=(1, 1, None), name='padding_mask')\n\n    embeddings = Embedding(vocab_size, d_model)(inputs)\n    embeddings *= Lambda(lambda d_model: tf.math.sqrt( \\\n                        tf.cast(d_model, tf.float32)))(d_model)\n    embeddings = PositionalEncoding(vocab_size, d_model)(embeddings)\n\n    outputs = Dropout(rate=dropout)(embeddings)\n\n    for i in range(num_layers):\n        outputs = decoder_layer(\n            units=units,\n            d_model=d_model,\n            num_heads=num_heads,\n            dropout=dropout,\n            name='decoder_layer_{}'.format(i),\n        )(inputs=[outputs, enc_outputs, look_ahead_mask, padding_mask])\n\n    return Model(\n          inputs=[inputs, enc_outputs, look_ahead_mask, padding_mask],\n          outputs=outputs,\n          name=name)","2143ba70":"sample_decoder = decoder(\n    vocab_size=21128,\n    num_layers=2,\n    units=512,\n    d_model=128,\n    num_heads=4,\n    dropout=0.3,\n    name=\"sample_decoder\")\n\nplot_model(sample_decoder, to_file='decoder.png', show_shapes=True)","6212ff51":"# \u5b9a\u4e49Transformer\u6a21\u578b\ndef transformer(vocab_size,\n                num_layers,\n                units,\n                d_model,\n                num_heads,\n                dropout,\n                name=\"transformer\"):\n    inputs = Input(shape=(None,), name=\"inputs\")\n    dec_inputs = Input(shape=(None,), name=\"dec_inputs\")\n\n    enc_padding_mask = Lambda(\n      create_padding_mask, output_shape=(1, 1, None),\n      name='enc_padding_mask')(inputs)\n    # \u89e3\u7801\u5668\u7b2c\u4e00\u4e2a\u6ce8\u610f\u529b\u5757\u7684\u524d\u5411\u63a9\u7801\n    look_ahead_mask = Lambda(\n      create_look_ahead_mask,\n      output_shape=(1, None, None),\n      name='look_ahead_mask')(dec_inputs)\n    # \u5bf9\u7f16\u7801\u5668\u8f93\u51fa\u5230\u89e3\u7801\u5668\u7b2c2\u4e2a\u6ce8\u610f\u529b\u5757\u7684\u5185\u5bb9\u63a9\u7801\n    dec_padding_mask = Lambda(\n      create_padding_mask, output_shape=(1, 1, None),\n      name='dec_padding_mask')(inputs)\n\n    enc_outputs = encoder(\n          vocab_size=vocab_size,\n          num_layers=num_layers,\n          units=units,\n          d_model=d_model,\n          num_heads=num_heads,\n          dropout=dropout,\n        )(inputs=[inputs, enc_padding_mask])\n\n    dec_outputs = decoder(\n          vocab_size=vocab_size,\n          num_layers=num_layers,\n          units=units,\n          d_model=d_model,\n          num_heads=num_heads,\n          dropout=dropout,\n        )(inputs=[dec_inputs, enc_outputs, look_ahead_mask, dec_padding_mask])\n\n    outputs = Dense(units=vocab_size, name=\"outputs\")(dec_outputs)\n\n    return Model(inputs=[inputs, dec_inputs], outputs=outputs, name=name)","b1f07095":"sample_transformer = transformer(\n    vocab_size=21128,\n    num_layers=4,\n    units=512,\n    d_model=128,\n    num_heads=4,\n    dropout=0.3,\n    name=\"sample_transformer\")\n\nplot_model(sample_transformer, to_file='transformer.png', show_shapes=True)","94c7d28a":"# \u5b9a\u4e49\u635f\u5931\u51fd\u6570\ndef loss_function(y_true, y_pred):\n    y_true = tf.reshape(y_true, shape=(-1, MAX_LENGTH - 1))\n\n    loss = SparseCategoricalCrossentropy(\n         from_logits=True, reduction='none')(y_true, y_pred)\n\n    mask = tf.cast(tf.not_equal(y_true, 0), tf.float32)\n    loss = tf.multiply(loss, mask)\n\n    return tf.reduce_mean(loss)","81291b9e":"# \u5b66\u4e60\u7387\u52a8\u6001\u8c03\u5ea6\nclass CustomSchedule(LearningRateSchedule):\n\n    def __init__(self, d_model, warmup_steps=4000):\n        super(CustomSchedule, self).__init__()\n\n        self.d_model = tf.constant(d_model,dtype=tf.float32)\n        self.warmup_steps = warmup_steps\n\n    def get_config(self):\n        return {\"d_model\": self.d_model,\"warmup_steps\":self.warmup_steps}\n\n    def __call__(self, step):\n        arg1 = rsqrt(step)\n        arg2 = step * (self.warmup_steps**-1.5)\n\n        return multiply(rsqrt(self.d_model), minimum(arg1, arg2))","0ddf30ca":"sample_learning_rate = CustomSchedule(d_model=256)\n\nplt.plot(sample_learning_rate(tf.range(200000, dtype=tf.float32)))\nplt.ylabel(\"Learning Rate\")\nplt.xlabel(\"Train Step\")","e92260f7":"learning_rate = CustomSchedule(D_MODEL) # \u5b66\u4e60\u7387\n# \u4f18\u5316\u7b97\u6cd5\noptimizer = Adam(learning_rate, beta_1=0.9, beta_2=0.98, epsilon=1e-9)\n# \u81ea\u5b9a\u4e49\u51c6\u53bb\u7387\u51fd\u6570\ndef accuracy(y_true, y_pred):\n    # \u8c03\u6574\u6807\u7b7e\u7684\u7ef4\u5ea6\u4e3a\uff1a(batch_size, MAX_LENGTH - 1)\n    y_true = tf.reshape(y_true, shape=(-1, MAX_LENGTH - 1))\n    return sparse_categorical_accuracy(y_true, y_pred)\n\n# TPU\u6a21\u5f0f\u5b9a\u4e49\u6a21\u578b\nwith tpu_strategy.scope():    \n    model = transformer(\n          vocab_size=VOCAB_SIZE,\n          num_layers=NUM_LAYERS,\n          units=UNITS,\n          d_model=D_MODEL,\n          num_heads=NUM_HEADS,\n          dropout=DROPOUT)\n\n    model.compile(optimizer=optimizer, loss=loss_function, metrics=[accuracy])\n\n\nmodel.summary()","3cdf882f":"#\u56de\u8c03\u51fd\u6570\nfrom tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping","903df8a2":"# \u5b9a\u4e49\u56de\u8c03\u51fd\u6570\uff1a\u4fdd\u5b58\u6700\u4f18\u6a21\u578b\u3002                   \ncheckpoint = ModelCheckpoint(\"robot_weights.h5\",\n                             monitor=\"val_loss\",\n                             mode=\"min\",\n                             save_best_only = True,\n                             save_weights_only=True,\n                             verbose=1)\n# \u5b9a\u4e49\u56de\u8c03\u51fd\u6570\uff1a\u63d0\u524d\u7ec8\u6b62\u8bad\u7ec3\nearlystop = EarlyStopping(monitor = 'val_loss', \n                          min_delta = 0, \n                          patience = 10,\n                          verbose = 1,\n                          restore_best_weights = True)\n\n# \u5c06\u56de\u8c03\u51fd\u6570\u7ec4\u7ec7\u4e3a\u56de\u8c03\u5217\u8868\ncallbacks = [checkpoint, earlystop]","8d82bf3d":"# \u6a21\u578b\u8bad\u7ec3\nhistory = model.fit(train_dataset, epochs=EPOCHS, \\\n                    validation_data=valid_dataset, \\\n                    callbacks = callbacks)","83c3e5b7":"# \u635f\u5931\u51fd\u6570\u66f2\u7ebf\nplt.figure(figsize=(12,6))\nx = range(1,len(history.history['loss'])+1)\nplt.plot(x, history.history['loss'])\nplt.plot(x, history.history['val_loss'])\nplt.xticks(x)\nplt.ylabel('Loss')\nplt.xlabel('Epoch')\nplt.legend(['train', 'test'])\nplt.title('Loss over training epochs')\nplt.show();","18e5af6c":"# \u51c6\u786e\u7387\u66f2\u7ebf\nplt.figure(figsize=(12,6))\nplt.plot(x, history.history['accuracy'])\nplt.plot(x, history.history['val_accuracy'])\nplt.ylabel('Accuracy')\nplt.xlabel('Epoch')\nplt.xticks(x)\nplt.legend(['train', 'test'])\nplt.title('Accuracy over training epochs')\nplt.show();","d60a711d":"# \u9884\u6d4b\u6d4b\u8bd5\ndef evaluate(sentence):\n    sentence = tokenizer.tokenize(sentence)\n    sentence = START_TOKEN + tokenizer.convert_tokens_to_ids(sentence) + END_TOKEN\n    sentence = tf.expand_dims(sentence, axis=0)\n\n    output = tf.expand_dims(START_TOKEN, 0)\n\n    for i in range(MAX_LENGTH):\n        predictions = model(inputs=[sentence, output], training=False)\n\n        # \u9009\u62e9\u6700\u540e\u4e00\u4e2a\u8f93\u51fa\n        predictions = predictions[:, -1:, :]\n        predicted_id = tf.cast(tf.argmax(predictions, axis=-1), tf.int32)\n\n        # \u5982\u679c\u662fEND_TOKEN\u5219\u7ed3\u675f\u9884\u6d4b\n        if tf.equal(predicted_id, END_TOKEN[0]):\n            break\n\n        # \u628a\u5df2\u7ecf\u5f97\u5230\u7684\u9884\u6d4b\u503c\u4e32\u8054\u8d77\u6765\uff0c\u505a\u4e3a\u89e3\u7801\u5668\u7684\u65b0\u8f93\u5165\n        output = tf.concat([output, predicted_id], axis=-1)\n\n    return tf.squeeze(output, axis=0)\n\n\ndef predict(question):\n    prediction = evaluate(question)\n    predicted_answer = tokenizer.convert_ids_to_tokens(\n      np.array([i for i in prediction if i < VOCAB_SIZE-2]))\n\n    print(f'\u95ee\u8bdd\u8005: {question}')\n    print(f'\u7b54\u8bdd\u8005: {\"\".join(predicted_answer)}')\n\n    return predicted_answer","cdbb6ce9":"output1 = predict('\u55e8\uff0c\u4f60\u597d\u5440\u3002')  # \u8bad\u7ec3\u96c6\u4e2d\u7684\u6837\u672c","5791bb08":"output2 = predict('\u4eca\u5929\u5fc3\u60c5\u5982\u4f55\uff1f')  # \u968f\u673a\u95ee\u8bdd","459ab56b":"output3 = predict('\u6628\u5929\u7684\u6bd4\u8d5b\u600e\u4e48\u6837\uff1f')  # \u968f\u673a\u95ee\u8bdd","31064889":"output4 = predict('\u4eca\u5929\u7684\u5de5\u4f5c\u5b8c\u6210\u4e86\u5417\uff1f')  # \u968f\u673a\u95ee\u8bdd","9bbbb8a2":"# \u591a\u8f6e\u5bf9\u8bdd\u6d4b\u8bd5\uff0c\u81ea\u95ee\u81ea\u7b54\nsentence = '\u4f60\u6700\u8fd1\u6709\u542c\u8bf4\u8fc7\u300a\u4e2d\u56fd\u5973\u6392\u300b\u8fd9\u90e8\u7535\u5f71\u561b\uff1f'\nfor _ in range(5):\n    sentence = \"\".join(predict(sentence))\n    print(\"\")","f0cf607c":"#\u7528BLEU\u65b9\u6cd5\u8bc4\u4f30\u6a21\u578b\nfrom nltk.translate.bleu_score import sentence_bleu","2b222b3c":"reference = '\u662f\u5440\uff0c\u6211\u89c9\u5f97\u8fd9\u90e8\u300a\u4e2d\u56fd\u5973\u6392\u300b\u5e94\u8be5\u80fd\u62ff\u4e0b\u5f88\u9ad8\u7684\u6536\u89c6\u7387\u3002'\npred_sentence = predict(reference)\nreference = tokenizer.tokenize(reference)\n","82125f74":"# 1-gram BLEU\u8ba1\u7b97\nBLEU_1 = sentence_bleu([reference], pred_sentence, weights=(1, 0, 0, 0))\nprint(f\"\\n BLEU-1 \u8bc4\u5206: {BLEU_1}\")\n\n# 2-gram BLEU\u8ba1\u7b97\nBLEU_2 = sentence_bleu([reference], pred_sentence, weights=(0.5, 0.5, 0, 0))\nprint(f\"\\n BLEU-2 \u8bc4\u5206: {BLEU_2}\")\n\n# 3-gram BLEU\u8ba1\u7b97\nBLEU_3 = sentence_bleu([reference], pred_sentence, weights=(0.33, 0.33, 0.33, 0))\nprint(f\"\\n BLEU-3 \u8bc4\u5206:: {BLEU_3}\")\n\n# 4-gram BLEU\u8ba1\u7b97\nBLEU_4 = sentence_bleu([reference], pred_sentence, weights=(0.25, 0.25, 0.25, 0.25))\nprint(f\"\\n BLEU-4 \u8bc4\u5206:: {BLEU_4}\")\n\n# 5-gram BLEU\u8ba1\u7b97\nBLEU_5 = sentence_bleu([reference], pred_sentence, weights=(0.2, 0.2, 0.2, 0.2, 0.2))\nprint(f\"\\n BLEU-5 \u8bc4\u5206:: {BLEU_5}\")","7f743fcd":"#### \u5982\u679c\u672c\u6587\u6863\u5bf9\u60a8\u6709\u6240\u542f\u53d1\u5e2e\u52a9\uff0c\u8bf7\u70b9\u8d5e\u652f\u6301\uff0c\u8c22\u8c22\uff01","912b3866":"### <b>\u5982\u679c\u672c\u6587\u6863\u5bf9\u60a8\u6709\u6240\u542f\u53d1\u5e2e\u52a9\uff0c\u8bf7\u70b9\u8d5e\u652f\u6301\uff0c\u8c22\u8c22\uff01<\/b>","1070a2cf":"# \u673a\u5668\u4eba\u804a\u5929","8a452387":"#### \u6570\u636e\u96c6\u91c7\u7528\u4e86\u817e\u8baf\u4eba\u5de5\u667a\u80fd\u5b9e\u9a8c\u5ba42021\u5e74\u53d1\u5e03\u7684\u4e3b\u9898\u9a71\u52a8\u7684\u591a\u8f6e\u4e2d\u6587\u804a\u5929\u6570\u636e\u96c6NaturalConv\u3002\n\u30101\u3011\u6570\u636e\u96c6\u4e0b\u8f7d\u5730\u5740\uff1ahttps:\/\/ai.tencent.com\/ailab\/nlp\/dialogue\/#datasets <br\/>\n\u30102\u3011\u8bba\u6587\u5730\u5740\uff1ahttps:\/\/arxiv.org\/pdf\/2103.02548.pdf","dc0619b0":"\u5b66\u4e60\u672c\u6848\u4f8b\u9700\u8981\u5bf9<b>\u8bcd\u5d4c\u5165\u5411\u91cf\u8868\u793a\u6cd5\u3001\u6ce8\u610f\u529b\u673a\u5236\u3001Transformer\u6a21\u578b<\/b>\u6709\u8f83\u597d\u7684\u7406\u89e3\u3002<br\/>\nTensorFlow\u5b98\u7f51\u6709\u4e00\u4e2a\u7528Transformer\u5c06\u8461\u8404\u7259\u8bed\u7ffb\u8bd1\u6210\u82f1\u8bed\u7684\u6559\u5b66\u6848\u4f8b\u3002\u53ef\u4ee5\u9605\u8bfb\u53c2\u8003\u3002\u672c\u6848\u4f8b\u4e2d\u7684\u6a21\u578b\u5b9a\u4e49\u90e8\u5206\u53c2\u7167\u4e86\u5176\u4e2d\u7684\u7f16\u7801<br\/>\n\u6848\u4f8b\u94fe\u63a5\uff1ahttps:\/\/tensorflow.google.cn\/tutorials\/text\/transformer?hl=zh_cn  <b\/>","b4f2015f":"#### \u53c2\u8003\u4e86\u4f5c\u8005bryanlimy\u5728Github\u4e0a\u5206\u4eab\u7684\u9879\u76eetf2-transformer-chatbot\nhttps:\/\/github.com\/bryanlimy\/tf2-transformer-chatbot\/blob\/master\/tf2_tpu_transformer_chatbot.ipynb","d582bbc2":"\u673a\u5668\u5728\u81ea\u7136\u8bed\u8a00\u7406\u89e3\u65b9\u9762\uff0c\u62e5\u6709\u65e0\u9650\u4e0a\u5347\u7a7a\u95f4\uff01"}}