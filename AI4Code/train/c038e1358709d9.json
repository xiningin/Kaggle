{"cell_type":{"d45bc378":"code","7c2f046b":"code","714dc6f7":"code","83c7e884":"code","00cd3e59":"code","e39597f1":"code","d1277e8a":"code","089f8dc0":"code","7a883044":"code","9637760f":"code","9681a098":"code","c3df1878":"code","f9518cbb":"code","d0e59d28":"code","f0fd124b":"code","b2bcdb95":"code","e667ab4d":"code","7d723b7c":"code","7129bdd6":"code","c7f10ce9":"code","983b4149":"code","67791e68":"markdown","7605731b":"markdown","09bcdf67":"markdown","8a849950":"markdown","b556c575":"markdown","b6ee8963":"markdown","816b8865":"markdown","b9b4b72f":"markdown","aec64445":"markdown","ce0c786c":"markdown","0a83720b":"markdown","02775f32":"markdown"},"source":{"d45bc378":"import numpy as np\nimport pandas as pd\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nimport sklearn\nfrom sklearn import datasets\n\nfrom sklearn.model_selection import train_test_split\n\n# from sklearn.ensemble import RandomForestRegressor\nfrom sklearn.ensemble import RandomForestClassifier\n\nfrom sklearn.metrics import accuracy_score, confusion_matrix, classification_report","7c2f046b":"# Load dataset\niris = datasets.load_iris()","714dc6f7":"type(iris)","83c7e884":"iris.target_names","00cd3e59":"iris.feature_names","e39597f1":"# iris.data","d1277e8a":"# iris.data[:, 0]","089f8dc0":"iris.target","7a883044":"# Creating a DataFrame of given iris dataset\n\niris_data = pd.DataFrame({\n    'sepal length': iris.data[:, 0],\n    'sepal width' : iris.data[:, 1],\n    'petal length': iris.data[:, 2],\n    'petal width': iris.data[:, 3],\n    'species': iris.target\n})","9637760f":"type(iris_data)","9681a098":"iris_data.head()","c3df1878":"X = iris_data.iloc[:, :-1].values  # features \ny = iris_data.iloc[:, -1].values   # target ","f9518cbb":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=0)","d0e59d28":"model = RandomForestClassifier()                  # Model loading  \nmodel.fit(X_train, y_train)                       # Model training or development ","f0fd124b":"y_pred = model.predict(X_test)                    # Model prediction ","b2bcdb95":"accuracy_score(y_test, y_pred)","e667ab4d":"model1 = RandomForestClassifier(n_estimators=100)\nmodel1.fit(X_train, y_train)","7d723b7c":"# model1.feature_importances_?","7129bdd6":"feature_imp = pd.Series(data=model1.feature_importances_, index=iris.feature_names)\nfeature_imp = feature_imp.sort_values(ascending=False)","c7f10ce9":"feature_imp","983b4149":"# Creating a bar plot\nsns.barplot(x=feature_imp, y=feature_imp.index)\n# Add labels to your graph\nplt.xlabel('Feature Importance Score')\nplt.ylabel('Features')\nplt.title(\"Visualizing Important Features\")\nplt.show()","67791e68":"Let\u2019s understand the algorithm in layman\u2019s terms. Suppose you want to go on a trip and you would like to travel to a place which you will enjoy.\n\nSo what do you do to find a place that you will like? You can search online, read reviews on travel blogs and portals, or you can also ask your friends.\n\nLet\u2019s suppose you have decided to ask your friends, and talked with them about their past travel experience to various places. You will get some recommendations from every friend. Now you have to make a list of those recommended places. Then, you ask them to vote (or select one best place for the trip) from the list of recommended places you made. The place with the highest number of votes will be your final choice for the trip.\n\nIn the above decision process, there are two parts. First, asking your friends about their individual travel experience and getting one recommendation out of multiple places they have visited. This part is like using the decision tree algorithm. Here, each friend makes a selection of the places he or she has visited so far.\n\nThe second part, after collecting all the recommendations, is the voting procedure for selecting the best place in the list of recommendations. This whole process of getting recommendations from friends and voting on them to find the best place is known as the random forests algorithm.","7605731b":"It technically is an ensemble method (based on the divide-and-conquer approach) of decision trees generated on a randomly split dataset. This collection of decision tree classifiers is also known as the forest. The individual decision trees are generated using an attribute selection indicator such as information gain, gain ratio, and Gini index for each attribute. Each tree depends on an independent random sample. In a classification problem, each tree votes and the most popular class is chosen as the final result. In the case of regression, the average of all the tree outputs is considered as the final result. It is simpler and more powerful compared to the other non-linear classification algorithms.","09bcdf67":"**Understanding Random Forests Classifiers in Python**","8a849950":"**Building a Classifier using Scikit-learn**","b556c575":"**Happy Learning :)**","b6ee8963":"**Random Forests vs Decision Trees**","816b8865":"**How does the algorithm work?**","b9b4b72f":"* Random forests is a set of multiple decision trees.\n* Deep decision trees may suffer from overfitting, but random forests prevents overfitting by creating trees on random subsets.\n* Decision trees are computationally faster.\n* Random forests is difficult to interpret, while a decision tree is easily interpretable and can be converted to rules.","aec64445":"Random forests is a supervised learning algorithm. It can be used both for classification and regression. It is also the most flexible and easy to use algorithm. A forest is comprised of trees. It is said that the more trees it has, the more robust a forest is. Random forests creates decision trees on randomly selected data samples, gets prediction from each tree and selects the best solution by means of voting. It also provides a pretty good indicator of the feature importance.","ce0c786c":"It works in four steps:\n\n1. Select random samples from a given dataset.\n1. Construct a decision tree for each sample and get a prediction result from each decision tree.\n1. Perform a vote for each predicted result.\n1. Select the prediction result with the most votes as the final prediction.","0a83720b":"Random forests has a variety of applications, such as recommendation engines, image classification and feature selection. It can be used to classify loyal loan applicants, identify fraudulent activity and predict diseases. It lies at the base of the Boruta algorithm, which selects important features in a dataset.","02775f32":"**The Random Forests Algorithm**"}}