{"cell_type":{"8fc5953a":"code","b871eec7":"code","78523780":"code","f05c4279":"code","3ccf49bf":"code","8499e5f2":"code","6beaf03e":"code","4877d59a":"code","ce96e670":"code","c8465212":"code","7f42d7a8":"code","83df5deb":"code","fe301006":"code","040e67b1":"code","b3d2f316":"code","e9b62dc1":"code","4ff35b81":"code","7985f593":"code","f12d5540":"code","75dfc7a5":"code","c9c51b47":"code","2b8fee0d":"code","0eddee3c":"code","aab71841":"code","97d08779":"code","8842edc3":"code","f5d7e2b9":"code","40fec7bc":"code","8f8f43b3":"code","b18c6530":"code","a753eb63":"code","fe39759f":"code","2e92e1dd":"code","99339e44":"code","070983d3":"code","23bfdc71":"markdown"},"source":{"8fc5953a":"# C\u00e0i \u0111\u1eb7t Apache Spark\n!curl -O https:\/\/archive.apache.org\/dist\/spark\/spark-3.1.2\/spark-3.1.2-bin-hadoop3.2.tgz    \n!tar xf spark-3.1.2-bin-hadoop3.2.tgz\n!ls -l \/kaggle\/working\/spark-3.1.2-bin-hadoop3.2\n!mv spark-3.1.2-bin-hadoop3.2 \/opt\/spark\n\n%env SPARK_HOME = \/opt\/spark\n\n!pip install findspark\n\nimport findspark\nfindspark.init()","b871eec7":"# N\u1ea1p th\u01b0 vi\u1ec7n\nfrom pyspark.sql.session import SparkSession\nfrom pyspark.sql.functions import udf, col, when\nfrom pyspark.sql import functions as f\nfrom pyspark.sql.types import FloatType, StringType\n\nfrom pyspark.ml import Pipeline\n\nfrom pyspark.ml.feature import StringIndexer, OneHotEncoder, VectorAssembler\nfrom pyspark.ml.feature import StopWordsRemover, RegexTokenizer\nfrom pyspark.ml.feature import HashingTF, IDF, Tokenizer\n\nfrom pyspark.ml.classification import DecisionTreeClassifier, RandomForestClassifier, NaiveBayes, LogisticRegression\nfrom pyspark.ml.classification import MultilayerPerceptronClassifier\nfrom pyspark.ml.evaluation import MulticlassClassificationEvaluator\nfrom pyspark.ml.tuning import ParamGridBuilder, CrossValidator\nfrom pyspark.ml.functions import vector_to_array\n\nimport seaborn as sns\nimport html","78523780":"spark = (\n    SparkSession\n        .builder\n        .appName(\"NLP\")\n        .config(\"spark.executor.memory\",\"8g\")\n        .config(\"spark.driver.memory\", \"8g\")\n        .getOrCreate()\n)","f05c4279":"# D\u1eef li\u1ec7u hu\u1ea5n luy\u1ec7n\ntrain = (\n    spark.read.csv(\n        path = \"\/kaggle\/input\/it2034ch1502-nlp\/train.csv\",\n        header = True,\n        inferSchema = True\n    )\n    .select(\"id\",\"text\",\"polarity\")\n)\n\ntrain.printSchema()\ntrain.show(3, False)","3ccf49bf":"train.groupBy(\"polarity\").agg(f.count(\"polarity\").alias(\"total\")).show()\nsns.countplot(x = \"polarity\", data = train.toPandas())","8499e5f2":"# D\u1eef li\u1ec7u ki\u1ec3m tra\ntest = (\n    spark.read.csv(\n        path = \"\/kaggle\/input\/it2034ch1502-nlp\/test.csv\",\n        header = True,\n        inferSchema = True\n    )\n    .select(\"id\",\"text\")\n)\n\ntest.printSchema()\ntest.show(3, False)","6beaf03e":"def preprocessing(data):\n    \n    htmlUnescape = udf(lambda text: html.unescape(text), StringType()) \n\n    data = data.withColumn(\"text\", htmlUnescape(\"text\"))\n    \n    regexList = {\n        \"userRegex\" : \n            [r\"(@\\w{1,15})\",\"\"],\n\n        \"emailRegex\" : \n            [r\"[\\w.-]+@[\\w.-]+\\.[a-zA-Z]{1,}\",\"\"],\n        \n        \"urlRegex\" : \n            [r\"((https?|ftp|file):\\\/{2,3})+([-\\w+&@#\/%=~|$?!:,.]*)|(www.)+([-\\w+&@#\/%=~|$?!:,.]*)\",\"\"],\n        \n        \"hashtagRegex\" : \n            [r\"#(\\w{1,})\",\"\"],\n        \n        \"numRegex\" : \n            [\"[^a-zA-Z]\",\" \"],\n        \n        \"repeatCharacterRegex\" : \n            [r\"(\\w)\\1{2,}\",\"$1\"],\n        \n        \"singleCharacterRegex\" :\n            [r\"\\b(\\w)\\b\",\"\"],\n                 \n        \"spaceRegex\" : \n            [\" +\",\" \"]\n    }\n\n    for item in regexList:\n        data = data.withColumn(\"text\", f.regexp_replace(\"text\", regexList[item][0], regexList[item][1]))\n\n    data = data.withColumn(\"text\", f.lower(\"text\")).withColumn(\"text\", f.trim(\"text\"))\n           \n    return data","4877d59a":"preTrain, preTest = preprocessing(train), preprocessing(test)","ce96e670":"# X\u1eed l\u00fd d\u1eef li\u1ec7u v\u00e0 tr\u00edch xu\u1ea5t \u0111\u1eb7c tr\u01b0ng\nregexTokenizer = (\n    RegexTokenizer(\n        inputCol = \"text\",\n        outputCol = \"words\",\n        pattern = \"\\\\W\"\n    )\n)\n\nstopWordsRemover = (\n    StopWordsRemover(\n        inputCol = \"words\", \n        outputCol = \"selectedWords\", \n        stopWords = StopWordsRemover.loadDefaultStopWords(\"english\")\n    )\n)\n\nhashingTF = (\n    HashingTF(\n        numFeatures = 262144,\n        inputCol = \"selectedWords\",\n        outputCol = \"tf\"\n    )\n)\n\nidf = (\n    IDF(\n        inputCol = \"tf\",\n        outputCol = \"features\",\n        minDocFreq = 5\n    )\n)","c8465212":"pipeline = Pipeline(stages = [regexTokenizer, stopWordsRemover, hashingTF, idf])\n\nproTrain = pipeline.fit(preTrain).transform(preTrain)\nproTest = pipeline.fit(preTest).transform(preTest)\n\ndataList = [proTrain, proTest]\n\nfor data in dataList:\n    data.select(\n        col(\"id\"),\n        col(\"words\"),\n        col(\"selectedWords\").alias(\"filtered words\"),\n        col(\"tf\").alias(\"term frequency\"),\n        col(\"features\")\n    ).show(3)\n    \n    data.select(col(\"words\")).show(3, False)\n    data.select(col(\"selectedWords\").alias(\"filtered words\")).show(3, False)\n    data.select(col(\"tf\").alias(\"term frequency\")).show(3, False)","7f42d7a8":"training, validate = preTrain.randomSplit([0.9999, 0.0001], seed = 1234)\ntraining.count(), validate.count()","83df5deb":"def getCrossValidator(estimator, code):\n   \n    # Decision Tree\n    if code == 0:\n        paramGrid = ParamGridBuilder() \\\n            .addGrid(estimator.maxDepth, [10, 30]) \\\n            .addGrid(estimator.impurity, [\"entropy\", \"gini\"]) \\\n            .build()      \n\n    # Random Forest\n    if code == 1:\n        paramGrid = ParamGridBuilder() \\\n            .addGrid(estimator.maxDepth, [10, 30]) \\\n            .addGrid(estimator.numTrees, [100]) \\\n            .addGrid(estimator.impurity, [\"entropy\", \"gini\"]) \\\n            .build()        \n        \n    # Naive Bayes\n    if code == 2:\n        paramGrid = ParamGridBuilder() \\\n            .addGrid(estimator.smoothing, [1]) \\\n            .addGrid(estimator.modelType, [\"multinomial\"]) \\\n            .build() \n    \n    # Logistic Regression\n    if code == 3:\n        paramGrid = ParamGridBuilder() \\\n            .addGrid(estimator.maxIter, [30, 100]) \\\n            .addGrid(estimator.regParam, [0.05, 0.1]) \\\n            .build()\n\n    evaluator = MulticlassClassificationEvaluator(\n        metricName = \"accuracy\",\n        labelCol = \"polarity\",\n        predictionCol = \"prediction\"\n    )\n    \n    pipeline = Pipeline(stages = [regexTokenizer, stopWordsRemover, hashingTF, idf, estimator])\n        \n    crossValidator = CrossValidator(\n        estimator = pipeline,\n        estimatorParamMaps = paramGrid,\n        evaluator = evaluator\n    )\n\n    return crossValidator","fe301006":"def evaluateModel(estimator, code):\n    cv = getCrossValidator(estimator, code)\n    model = cv.fit(training)\n\n    bestModel = model.bestModel\n    predictions = bestModel.transform(validate)\n    \n    evaluator = MulticlassClassificationEvaluator(\n        metricName = \"accuracy\", \n        labelCol = \"polarity\", \n        predictionCol = \"prediction\"\n    )\n    \n    print(f\"Accuracy: {evaluator.evaluate(predictions) * 100:.5f}%\")\n    \n    return bestModel","040e67b1":"# Decision Tree\ndt = DecisionTreeClassifier(featuresCol = \"features\", labelCol = \"polarity\")\nmodel = evaluateModel(dt, 0)","b3d2f316":"# Random Forest\nrf = RandomForestClassifier(featuresCol = \"features\", labelCol = \"polarity\")\nmodel = evaluateModel(rf, 1)","e9b62dc1":"# Naive Bayes\nnb = NaiveBayes(featuresCol = \"features\", labelCol = \"polarity\")\nmodel = evaluateModel(nb, 2)","4ff35b81":"# Logistic Regression\nlr = LogisticRegression(featuresCol = \"features\", labelCol = \"polarity\")\nmodel = evaluateModel(lr, 3)","7985f593":"predictions = model.transform(test).select(\"id\", \"text\", \"probability\", \"prediction\")\npredictions.groupBy(\"prediction\").agg(f.round((f.count(\"prediction\") \/ test.count()), 2).alias(\"pecentage\")).show()\nsns.countplot(x = \"prediction\", data = predictions.toPandas())","f12d5540":"from textblob import TextBlob\n\ntextBlob = udf(lambda text: TextBlob(text).sentiment.polarity, FloatType())\n\nsentiment1 = predictions.withColumn(\"sentiment\", textBlob(\"text\"))\nsentiment1 = sentiment1 \\\n            .withColumn(\"sentiment\", when(col(\"sentiment\") < 0, 0) \\\n            .when(col(\"sentiment\") == 0, 2) \\\n            .otherwise(4)).select(\"id\", \"sentiment\")","75dfc7a5":"# Th\u1eed nghi\u1ec7m 1\ntestSolutions = predictions \\\n    .join(sentiment1, [\"Id\"], \"left\") \\\n    .withColumn(\"probability\", vector_to_array(col(\"probability\"))[0].cast(\"float\"))\n\ntestSolutions = testSolutions \\\n    .withColumn(\"polarity\", when(col(\"probability\") > 0.60, col(\"prediction\")) \\\n    .when(col(\"probability\") < 0.10, col(\"prediction\")) \\\n    .otherwise(2))\n\nsolutions = testSolutions \\\n    .withColumn(\"polarity\", when((col(\"polarity\") == 2) & (col(\"sentiment\") == 0), 0) \\\n    .when((col(\"polarity\") == 2) & (col(\"prediction\") == col(\"sentiment\")) & (col(\"sentiment\") == 4), 4) \\\n    .otherwise(col(\"polarity\")))","c9c51b47":"# Th\u1eed nghi\u1ec7m 2\ntestSolutions = predictions \\\n    .join(sentiment1, [\"Id\"], \"left\") \\\n    .withColumn(\"probability\", vector_to_array(col(\"probability\"))[0].cast(\"float\"))\n\ntestSolutions = testSolutions \\\n    .withColumn(\"polarity\", when(col(\"probability\") > 0.60, col(\"prediction\")) \\\n    .otherwise(2))\n\nsolutions = testSolutions \\\n    .withColumn(\"polarity\", when((col(\"polarity\") == 2) & (col(\"sentiment\") == 0), 0) \\\n    .when((col(\"polarity\") == 2) & (col(\"probability\") < 0.53) & (col(\"sentiment\") == 4), 4) \\\n    .otherwise(col(\"polarity\")))","2b8fee0d":"!pip install vaderSentiment","0eddee3c":"from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n\nsentimentAnalyzer = SentimentIntensityAnalyzer()\n\nvader = udf(lambda text: sentimentAnalyzer.polarity_scores(text)[\"compound\"], FloatType())\n\nsentiment2 = predictions.withColumn(\"sentiment\", vader(\"text\"))\nsentiment2 = sentiment2 \\\n            .withColumn(\"sentiment\", when(col(\"sentiment\") < 0, 0) \\\n            .when(col(\"sentiment\") == 0, 2) \\\n            .otherwise(4)).select(\"id\", \"sentiment\")","aab71841":"# Th\u1eed nghi\u1ec7m 3\ntestSolutions = predictions \\\n    .join(sentiment2, [\"Id\"], \"left\") \\\n    .withColumn(\"probability\", vector_to_array(col(\"probability\"))[0].cast(\"float\"))\n\ntestSolutions = testSolutions \\\n    .withColumn(\"polarity\", when(col(\"probability\") > 0.64, col(\"prediction\")) \\\n    .when(col(\"probability\") < 0.14, col(\"prediction\")) \\\n    .otherwise(2))\n\nsolutions = testSolutions \\\n    .withColumn(\"polarity\", when((col(\"polarity\") == 2) & (col(\"sentiment\") == 0), 0) \\\n    .when((col(\"polarity\") == 2) & (col(\"prediction\") == col(\"sentiment\")) & (col(\"sentiment\") == 4), 4) \\\n    .otherwise(col(\"polarity\")))","97d08779":"# Th\u1eed nghi\u1ec7m 4\ntestSolutions = predictions \\\n    .join(sentiment2, [\"Id\"], \"left\") \\\n    .withColumn(\"probability\", vector_to_array(col(\"probability\"))[0].cast(\"float\"))\n\ntestSolutions = testSolutions \\\n    .withColumn(\"polarity\", when(col(\"probability\") > 0.66, col(\"prediction\")) \\\n    .otherwise(2))\n\nsolutions = testSolutions \\\n    .withColumn(\"polarity\", when((col(\"polarity\") == 2) & (col(\"sentiment\") == 0), 0) \\\n    .when((col(\"polarity\") == 2) & (col(\"probability\") < 0.60) & (col(\"sentiment\") == 4), 4) \\\n    .otherwise(col(\"polarity\")))","8842edc3":"avgP0 = solutions.filter(col(\"polarity\") == 0).select(\"probability\").groupBy().avg().first()[0]\nprint (\"Average Polarity 0: %s\" % str(avgP0))","f5d7e2b9":"avgP2 = 1 - solutions.filter(col(\"polarity\") == 2).select(\"probability\").groupBy().avg().first()[0]\nprint (\"Average Polarity 2: %s\" % str(avgP2))","40fec7bc":"avgP4 = 1 - solutions.filter(col(\"polarity\") == 4).select(\"probability\").groupBy().avg().first()[0]\nprint (\"Average Polarity 4: %s\" % str(avgP4))","8f8f43b3":"solutions.groupBy(col(\"polarity\")).agg(f.count(col(\"polarity\")).alias(\"Total\")).show()","b18c6530":"solutions.groupBy(\"polarity\").agg(f.round((f.count(\"polarity\") \/ test.count()), 2).alias(\"pecentage\")).show()","a753eb63":"sns.countplot(x = \"polarity\", data = solutions.toPandas())","fe39759f":"sns.distplot(solutions.select(\"polarity\").toPandas())","2e92e1dd":"solutions = solutions \\\n    .withColumn(\"polarity\", col(\"polarity\").cast(\"int\")) \\\n    .select(\"id\", \"polarity\")","99339e44":"solutions.show(10)","070983d3":"solutions.toPandas().to_csv(\"testSolutions.csv\", header = True, index = False)","23bfdc71":"# M\u00f4n h\u1ecdc: X\u1eed l\u00fd d\u1eef li\u1ec7u l\u1edbn\n\n**B\u00e0i t\u1eadp 3:** Sentiment Analysis of Tweets (NLP)\n\n**Gi\u1ea3ng vi\u00ean:** TS. \u0110\u1ed7 Tr\u1ecdng H\u1ee3p\n\n**Nh\u00f3m:** Smiley Team\n\n**Th\u00e0nh vi\u00ean:**\n1. L\u00ea Anh Tu\u1ea5n - CH1902037\n2. Nguy\u1ec5n C\u00f4ng Danh - CH1902029\n3. V\u00f5 Ho\u00e0ng V\u0169 - CH1902039"}}