{"cell_type":{"a8798e05":"code","1c86ef11":"code","866b746d":"code","24857a91":"code","a33baf4f":"code","896e3d2b":"code","4cf2b569":"code","b6ed6e34":"code","0365e845":"code","9fe11e03":"code","953fd690":"code","473a9e8e":"code","affdecde":"code","933d7870":"code","7b634909":"code","3c52f793":"code","50366687":"code","66e1a237":"code","32c8e4c6":"code","bfd4e5ce":"code","3f578c57":"code","176383c0":"code","af42144b":"code","a5784e1b":"code","91889aff":"code","6bffe32e":"code","3f218b94":"code","dfdc7228":"code","79482d8b":"code","f814cadd":"code","d16c59d0":"code","617f24f2":"code","9f28348f":"code","b3429fed":"code","406fce35":"code","ef85a65b":"code","14096f1f":"code","c630e17d":"code","bb48d58b":"code","55172700":"code","a93449cb":"code","24e03a15":"code","52962cba":"code","4ab75ae0":"code","347fe688":"code","439f8ab2":"code","6eea91ca":"code","7dbd61a9":"code","ad21f7d3":"code","d53c5bb1":"code","7399368b":"code","b64f7a9c":"code","773223f6":"code","53997072":"code","f1b3ee25":"code","68f69a01":"code","0ca4f9b9":"code","af4c45e6":"code","6245139b":"code","93df3ae9":"code","63c7becd":"markdown"},"source":{"a8798e05":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","1c86ef11":"# C\u00f3digo para mostrar sempre todas as colunas\npd.set_option('max.column', None)","866b746d":"# Carregando os dados\ndf = pd.read_csv('\/kaggle\/input\/melbourne-housing-snapshot\/melb_data.csv')\n\ndf.shape","24857a91":"# Dando uma olhada nos dados\ndf.head()","a33baf4f":"# Final do dataframe\ndf.tail()","896e3d2b":"# Analisando tamanhos e colunas\ndf.info()","4cf2b569":"# Uma forma f\u00e1cil de separar as colunas n\u00famericas das colunas categ\u00f3ricas\nnum_feats = df.select_dtypes(['int', 'float']).columns\n\ncat_feats = df.select_dtypes('object').columns\n\n# Visualizando\nprint(num_feats , len(num_feats))\nprint(cat_feats, len(cat_feats))","b6ed6e34":"# Verificando os valores nulos\ndf.isna().sum().sort_values(ascending=False)","0365e845":"# S\u00e3o 4 colunas com valores faltantes. Vamos verificar os dados\ndf[['BuildingArea', 'YearBuilt', 'CouncilArea', 'Car']].describe(include='all')","9fe11e03":"# Verificando os registros onde 'Car' \u00e9 nulo\ndf[df['Car'].isna()]","953fd690":"# Tratamento de valores faltantes\n\n# Como a coluna 'Car' s\u00f3 tem 62 valores faltantes, \u00e9 l\u00f3gico imaginar\n# que os valores faltantes correspondem a casas sem garagem.\n# Ent\u00e3o podemos preencher com 0\ndf['Car'].fillna(0, inplace=True)","473a9e8e":"# A coluna 'CouncilArea' \u00e9 categ\u00f3rica, ent\u00e3o vamos preencher com os valor mais comum\n\n# Qual a \u00e1rea mais comum\ndf['CouncilArea'].mode()[0]","affdecde":"# Mas esse preenchimento n\u00e3o tem muita l\u00f3gica, j\u00e1 que a \u00e1rea \u00e9 dependente do bairro\n\n# Verificando os 'Suburb' que possuem \u00e1reas como NA\ndf[df['CouncilArea'].isna()]['Suburb']","933d7870":"# Qual a quantidade?\ndf[df['CouncilArea'].isna()]['Suburb'].nunique()","7b634909":"# Exemplo -> Wheelers Hill\ndf[df['Suburb'] == 'Wheelers Hill']['CouncilArea'].nunique()","3c52f793":"df[df['Suburb'] == 'Wheelers Hill']['CouncilArea'].mode()","50366687":"df[df['Suburb'] == 'Wheelers Hill']['CouncilArea']","66e1a237":"df[df['Suburb'] == 'Wheelers Hill']","32c8e4c6":"# Exemplo -> Williamstown\ndf[df['Suburb'] == 'Williamstown']['CouncilArea'].nunique()","bfd4e5ce":"df[df['Suburb'] == 'Williamstown']['CouncilArea']","3f578c57":"df[df['Suburb'] == 'Williamstown']['CouncilArea'].mode()","176383c0":"# Montar c\u00f3digo para preencher os valores de 'CouncilArea' dentro de cada 'Suburb' de acordo com o valor mais frequente\n# dentro daquele mesmo 'Suburb'","af42144b":"# N\u00famero de 'Suburb' com 'CouncilArea' nulo\ndf[df['CouncilArea'].isna()]['Suburb'].nunique()","a5784e1b":"# Lista dos 'Suburb' com 'CouncilArea' nulo\nsuburb_list = df[df['CouncilArea'].isna()]['Suburb'].unique()\n\nsuburb_list","91889aff":"# Precorrer a lista\nfor sub in suburb_list:\n    # Verificando se existe algum 'CouncilArea' nesse \"Suburb\"\n    if (df.loc[df['Suburb'] == sub]['CouncilArea'].nunique() == 0):\n        # Se n\u00e3o existir preenche com o padr\u00e3o (maior ocorr\u00eancia)\n        df.loc[df['Suburb'] == sub, 'CouncilArea'] = 'Moreland'\n    else:\n        # Se existir preenche com o valor mais comum dentro do pr\u00f3prio 'Suburb'\n        df.loc[df['Suburb'] == sub, 'CouncilArea'] = df.loc[df['Suburb'] == sub]['CouncilArea'].mode()[0]","6bffe32e":"# Vamos verificar os valores nulos\ndf.isna().sum().sort_values(ascending=False)","3f218b94":"# O ano de constru\u00e7\u00e3o ser\u00e1 preenchido como o ano mais frequente\ndf['YearBuilt'] = df['YearBuilt'].fillna(df['YearBuilt'].mode()[0])\n\n# E a \u00e1rea constru\u00eddo com a m\u00e9dia\ndf['BuildingArea'] = df['BuildingArea'].fillna(df['BuildingArea'].mean())","dfdc7228":"# Ainda temos valores nulos?\ndf.isna().any()","79482d8b":"# Vamos analisar as coluna categ\u00f3ricas\n\n# Qual quantidade de categorias?\nfor col in cat_feats:\n    print(col + \" =\",  len(df[col].unique()))","f814cadd":"# Vamos retirar da lista a coluna 'Address'\ncat_feats = cat_feats.drop('Address')","d16c59d0":"# Transformando as colunas categ\u00f3ricas em num\u00e9ricas\nfrom sklearn.preprocessing import LabelEncoder\n\nfor col in cat_feats:\n    # Instancia o LabelEncoder\n    l_encoder = LabelEncoder()\n    # Fit e Transform ao mesmo tempo\n    df[col] = l_encoder.fit_transform(df[col])","617f24f2":"# Preparando a lista de colunas\nfeats = list(num_feats) + list(cat_feats)\n\nfeats.remove('Price')\n\nfeats","9f28348f":"# Separando o dataframe\nfrom sklearn.model_selection import train_test_split\n\ntrain, test = train_test_split(df, random_state = 42)\n\ntrain.shape, test.shape","b3429fed":"# Antes de rodar diversos modelos, vamos criar um dataframe para armazenar\n# os dados de execu\u00e7\u00e3o de cada modelo e poder comparar os melhores\nmodels = pd.DataFrame([],\n                      columns=['model_name', \n                               'prediction_score',\n                               'mean_absolute_error'])\nmodels","406fce35":"# Importa\u00e7\u00e3o dos modelos\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.ensemble import  RandomForestRegressor\nfrom sklearn.ensemble import  BaggingRegressor \nfrom sklearn.ensemble import  AdaBoostRegressor\nfrom sklearn.ensemble import  GradientBoostingRegressor\nfrom xgboost import XGBRegressor\n\n# e da m\u00e9trica\nfrom sklearn.metrics import mean_absolute_error","ef85a65b":"# Regress\u00e3o Linear\nlr_model = LinearRegression()\n\nlr_model.fit(train[feats], train['Price'])\n\nlr_model_predicted = lr_model.predict(test[feats])\n\nlr_model_score = lr_model.score(test[feats], test['Price'])\n\nmae = mean_absolute_error(test['Price'], lr_model_predicted)\n\nlr_model_score, mae","14096f1f":"models = models.append({\n    'model_name': lr_model.__class__.__name__,\n    'prediction_score': lr_model_score,\n    'mean_absolute_error' : mae\n}, ignore_index=True)\n\nmodels","c630e17d":"# Decision Tree\nDtree_model = DecisionTreeRegressor(random_state=42)\n\nDtree_model.fit(train[feats], train['Price'])\n\nDtree_model_predicted = Dtree_model.predict(test[feats])\n\nDtree_model_score = Dtree_model.score(test[feats], test['Price'])\n\nmae = mean_absolute_error(test['Price'], Dtree_model_predicted)\n\nDtree_model_score, mae","bb48d58b":"models = models.append({\n    'model_name': Dtree_model.__class__.__name__,\n    'prediction_score': Dtree_model_score,\n    'mean_absolute_error' : mae\n}, ignore_index=True)\n\nmodels.sort_values('prediction_score', ascending=False)","55172700":"# Random Forest\nRFRModel = RandomForestRegressor(n_estimators=200, random_state=42)\n\nRFRModel.fit(train[feats], train['Price'])\n\nRFRModel_predicted = RFRModel.predict(test[feats])\n\nRFRModel_score = RFRModel.score(test[feats], test['Price'])\n\nmae = mean_absolute_error(test['Price'], RFRModel_predicted)\n\nRFRModel_score, mae","a93449cb":"models = models.append({\n    'model_name': RFRModel.__class__.__name__,\n    'prediction_score': RFRModel_score,\n    'mean_absolute_error' : mae\n}, ignore_index=True)\n\nmodels.sort_values('prediction_score', ascending=False)","24e03a15":"# Bagging Regressor\nBGR_model = BaggingRegressor()\n\nBGR_model.fit(train[feats], train['Price'])\n\nBGR_model_predicted = BGR_model.predict(test[feats])\n\nBGR_model_score = BGR_model.score(test[feats], test['Price'])\n\nmae = mean_absolute_error(test['Price'], BGR_model_predicted)\n\nBGR_model_score, mae","52962cba":"models = models.append({\n    'model_name': BGR_model.__class__.__name__,\n    'prediction_score': BGR_model_score,\n    'mean_absolute_error' : mae\n}, ignore_index=True)\n\nmodels.sort_values('prediction_score', ascending=False)","4ab75ae0":"# ADABoost\nADB_model = AdaBoostRegressor()\n\nADB_model.fit(train[feats], train['Price'])\n\nADB_model_predicted = ADB_model.predict(test[feats])\n\nADB_model_score = ADB_model.score(test[feats], test['Price'])\n\nmae = mean_absolute_error(test['Price'], ADB_model_predicted)\n\nADB_model_score, mae","347fe688":"models = models.append({\n    'model_name': ADB_model.__class__.__name__,\n    'prediction_score': ADB_model_score,\n    'mean_absolute_error' : mae\n}, ignore_index=True)\n\nmodels.sort_values('prediction_score', ascending=False)","439f8ab2":"# Gradiente Boosting\nGBR_model = GradientBoostingRegressor(n_estimators=200, random_state=42)\n\nGBR_model.fit(train[feats], train['Price'])\n\nGBR_model_predicted = GBR_model.predict(test[feats])\n\nGBR_model_score = GBR_model.score(test[feats], test['Price'])\n\nmae = mean_absolute_error(test['Price'], GBR_model_predicted)\n\nGBR_model_score, mae","6eea91ca":"models = models.append({\n    'model_name': GBR_model.__class__.__name__,\n    'prediction_score': GBR_model_score,\n    'mean_absolute_error' : mae\n}, ignore_index=True)\n\nmodels.sort_values('prediction_score', ascending=False)","7dbd61a9":"# XGBoost\nXGBR_model = XGBRegressor()\n\nXGBR_model.fit(train[feats], train['Price'])\n\nXGBR_model_predicted = XGBR_model.predict(test[feats])\n\nXGBR_model_score = XGBR_model.score(test[feats], test['Price'])\n\nmae = mean_absolute_error(test['Price'], XGBR_model_predicted)\n\nXGBR_model_score, mae","ad21f7d3":"models = models.append({\n    'model_name': XGBR_model.__class__.__name__,\n    'prediction_score': XGBR_model_score,\n    'mean_absolute_error' : mae\n}, ignore_index=True)\n\nmodels.sort_values('prediction_score', ascending=False)","d53c5bb1":"# Melhorarando alguns par\u00e2metros do XGBoost\nXGBR_model_500 = XGBRegressor(n_estimators=500, max_depth=10, learning_rate=0.05)\n\nXGBR_model_500.fit(train[feats], train['Price'])\n\nXGBR_model_500_predicted = XGBR_model_500.predict(test[feats])\n\nXGBR_model_500_score = XGBR_model_500.score(test[feats], test['Price'])\n\nmae = mean_absolute_error(test['Price'], XGBR_model_500_predicted)\n\nXGBR_model_500_score, mae","7399368b":"models = models.append({\n    'model_name': 'XGBRegressor_500',\n    'prediction_score': XGBR_model_500_score,\n    'mean_absolute_error' : mae\n}, ignore_index=True)\n\nmodels.sort_values('prediction_score', ascending=False)","b64f7a9c":"# Tunning de Hiperpar\u00e2metros\n# Vamos usar os melhores modelos e tentar melhorar ainda mais o resultado\n# usando GridSearchCV e RandomizedSearchCV\nfrom sklearn.model_selection import GridSearchCV, RandomizedSearchCV","773223f6":"# Random Forest - Usando GridSearchCV\nrfr_model = RandomForestRegressor(random_state=42)\n\nrfr_params_grid = {\n    'n_estimators' : [100,150,200,300],\n    'max_depth' : [7],\n    'max_features': [5],\n    'min_samples_leaf' : [3],\n    'min_samples_split' : [4, 6 ,9]\n}\n\ngscv_rfr_cv = GridSearchCV(estimator = rfr_model, \n                                       param_grid=rfr_params_grid,\n                                       cv = 5 ,\n                                       n_jobs = -1,\n                                       verbose = 5)\n\ngscv_rfr_cv.fit(train[feats], train['Price'])\n\ngscv_rfr_cv_score = gscv_rfr_cv.best_score_\n\ngscv_rfr_cv_pred = gscv_rfr_cv.predict(test[feats])\n\nmae = mean_absolute_error(test['Price'], gscv_rfr_cv_pred)\n\ngscv_rfr_cv_score, mae","53997072":"models = models.append({\n    'model_name': 'RF Regressor GridSearchCV',\n    'prediction_score': gscv_rfr_cv_score,\n    'mean_absolute_error' : mae\n}, ignore_index=True)\n\nmodels.sort_values('prediction_score', ascending=False)","f1b3ee25":"# Random Forest - Usando RandomizedSearchCV\nrfr_model2 = RandomForestRegressor(random_state=42)\n\nrfr_params_grid = {\n    'n_estimators' : [100,150,200,300],\n    'max_depth' : [7],\n    'max_features': [5],\n    'min_samples_leaf' : [3],\n    'min_samples_split' : [4, 6 ,9]\n}\n\nrdcv_rfr_rv = RandomizedSearchCV(estimator = rfr_model2, \n                                       param_distributions=rfr_params_grid,\n                                       cv = 5 ,\n                                       n_jobs = -1,\n                                       verbose = 5)\n\nrdcv_rfr_rv.fit(train[feats], train['Price'])\n\nrdcv_rfr_rv_score = rdcv_rfr_rv.best_score_\n\nrdcv_rfr_rv_pred = rdcv_rfr_rv.predict(test[feats])\n\nmae = mean_absolute_error(test['Price'], rdcv_rfr_rv_pred)\n\nrdcv_rfr_rv_score, mae","68f69a01":"models = models.append({\n    'model_name': 'RF Regressor RandomizedSearchCV',\n    'prediction_score': rdcv_rfr_rv_score,\n    'mean_absolute_error' : mae\n}, ignore_index=True)\n\nmodels.sort_values('prediction_score', ascending=False)","0ca4f9b9":"# XGBoost usando GridSearchCV\n#xgbr_model = XGBRegressor()\n\n#params = {\n#    'n_estimators': [110, 120, 130, 140], \n#    'learning_rate': [ 0.05, 0.075, 0.1],\n#    'max_depth': [ 7, 9],\n#    'reg_lambda': [0.3, 0.5]\n#}\n\n#xgb_reg = GridSearchCV(estimator=xgbr_model, param_grid=params, cv=5, n_jobs=-1, verbose = 5)\n\n#xgb_reg.fit(train[feats], train['Price'])\n\n#xgbr_model_score = xgb_reg.best_score_\n\n#xgbr_model_pred = xgb_reg.predict(test[feats])\n\n#mae = mean_absolute_error(test['Price'], xgbr_model_pred)\n\n#xgbr_model_score, mae","af4c45e6":"#models = models.append({\n#    'model_name': 'XGBRegressor GridSearchCV',\n#    'prediction_score': xgbr_model_score,\n#    'mean_absolute_error' : mae\n#}, ignore_index=True)\n\n#models.sort_values('prediction_score', ascending=False)","6245139b":"# XGBoost usando RandomizedSearchCV\n#xgbr_model2 = XGBRegressor()\n\n#params = {\n #   'n_estimators': [110, 120, 130, 140], \n  #  'learning_rate': [ 0.05, 0.075, 0.1],\n  #  'max_depth': [ 7, 9],\n  #  'reg_lambda': [0.3, 0.5]\n#}\n\n#xgb_reg = RandomizedSearchCV(estimator=xgbr_model2, param_distributions=params, cv=5, n_jobs=-1, verbose = 5)\n\n#xgb_reg.fit(train[feats], train['Price'])\n\n#xgbr_model_score = xgb_reg.best_score_\n\n#xgbr_model_pred = xgb_reg.predict(test[feats])\n\n#mae = mean_absolute_error(test['Price'], xgbr_model_pred)\n\n#xgbr_model_score, mae","93df3ae9":"#models = models.append({\n#    'model_name': 'XGBRegressor RandomizedSearchCV',\n#    'prediction_score': xgbr_model_score,\n#    'mean_absolute_error' : mae\n#}, ignore_index=True)\n\n#models.sort_values('prediction_score', ascending=False)","63c7becd":"# IESB - CIA035 - Aula 08 - Exemplo Regress\u00e3o"}}