{"cell_type":{"034e8d36":"code","1750af11":"code","dedf8bb7":"code","abf5cf63":"code","21754807":"code","50124227":"code","3cda52a0":"code","2ee6a678":"code","2d3b130c":"code","48ab4307":"code","af707708":"code","05ed5a89":"code","0f01c022":"code","0dfe2b48":"code","c4e29b60":"code","92fff343":"code","18eef1fe":"code","4c547d2b":"code","2bfce60a":"code","53aa0a78":"code","89a9b943":"code","1021afad":"code","97158ef3":"code","ffcf95cc":"code","7ae8d885":"code","eded9fbd":"code","dfe8a276":"code","22ab1833":"code","78948be1":"code","4861e698":"code","89c6c38a":"code","a5a9ad15":"code","1473b9eb":"code","ef28e62b":"code","449645da":"code","5d0159e3":"code","660ed7cb":"code","a1b88bdd":"code","ad581c2e":"markdown","7bca788f":"markdown","226ecd12":"markdown","1d92d007":"markdown","2c3f661c":"markdown","c30717a9":"markdown","2cafd892":"markdown","fd0c65bd":"markdown","19951c2c":"markdown","ebf6a4b6":"markdown","a4254044":"markdown","4c9441a3":"markdown","d83d5bb1":"markdown","4bffda50":"markdown","04dc1432":"markdown","8b8de031":"markdown","66c4a502":"markdown","3645047b":"markdown","ea640598":"markdown","2da796ac":"markdown","e11a9310":"markdown"},"source":{"034e8d36":"import os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","1750af11":"# Ignore the warnings\nimport warnings\nwarnings.filterwarnings('always')\nwarnings.filterwarnings('ignore')\n\n# System related and data input controls\nimport os\n\n# Data manipulation, visualization and useful functions\nimport pandas as pd\npd.options.display.float_format = '{:,.2f}'.format\npd.options.display.max_rows = 50\npd.options.display.max_columns = 40\nimport numpy as np\nfrom tqdm import tqdm\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Modeling algorithms\n# General(Statistics\/Econometrics)\nfrom sklearn import preprocessing\nimport statsmodels.api as sm\nimport statsmodels.tsa.api as smt\nimport statsmodels.formula.api as smf\nfrom statsmodels.stats.outliers_influence import variance_inflation_factor\nfrom scipy import stats\n\n# Regression\nfrom sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet\nfrom sklearn.kernel_ridge import KernelRidge\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom sklearn.svm import SVR\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.ensemble import RandomForestRegressor, BaggingRegressor, GradientBoostingRegressor, AdaBoostRegressor\nfrom xgboost import XGBRegressor\nfrom lightgbm import LGBMRegressor\n\n# Model selection\nfrom sklearn.model_selection import train_test_split,cross_validate\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import GridSearchCV\n\n# Evaluation metrics\n# for regression\nfrom sklearn.metrics import mean_squared_error","dedf8bb7":"def feature_engineering_scaling(scaler, X_train, X_test):\n    # preprocessing.MinMaxScaler()\n    # preprocessing.StandardScaler()\n    # preprocessing.RobustScaler()\n    # preprocessing.Normalizer()\n    scaler = scaler\n    scaler_fit = scaler.fit(X_train)\n    X_train_scaling = pd.DataFrame(scaler_fit.transform(X_train), \n                               index=X_train.index, columns=X_train.columns)\n    X_test_scaling = pd.DataFrame(scaler_fit.transform(X_test), \n                               index=X_test.index, columns=X_test.columns)\n    return X_train_scaling, X_test_scaling\n\ndef compare_hist(train_num_scale, test_num_scale):\n    for i in range(len(train_num_scale.columns)):\n        f, ax = plt.subplots(1, 2, figsize = (10, 5))\n        sns.distplot(train_num_scale.iloc[:,i], norm_hist='True', fit=stats.norm, ax=ax[0])\n        sns.distplot(test_num_scale.iloc[:,i], norm_hist='True', fit=stats.norm, ax=ax[1])\n        \n### Functionalize\n### extract non-multicollinearity variables by VIF \ndef feature_engineering_XbyVIF(X_train, num_variables):\n    vif = pd.DataFrame()\n    vif['VIF_Factor'] = [variance_inflation_factor(X_train.values, i) \n                         for i in range(X_train.shape[1])]\n    vif['Feature'] = X_train.columns\n    X_colname_vif = vif.sort_values(by='VIF_Factor', ascending=True)['Feature'][:num_variables].values\n    return X_colname_vif\n# X_colname_vif = feature_engineering_XbyVIF(X_train_femm, 10)\n# X_colname_vif\n\n### Feature engineering of default\ndef non_feature_engineering(raw):\n    if 'datetime' in raw.columns:\n        raw['datetime'] = pd.to_datetime(raw['datetime'])\n        raw['DateTime'] = pd.to_datetime(raw['datetime'])\n    if raw.index.dtype == 'int64':\n        raw.set_index('DateTime', inplace=True)\n    # bring back\n    # if raw.index.dtype != 'int64':\n    #     raw.reset_index(drop=False, inplace=True)\n    raw = raw.asfreq('H', method='ffill')\n    raw_nfe = raw.copy()\n    return raw_nfe\n# raw_rd = non_feature_engineering(raw_all)\n\n\n### Data split of cross sectional\ndef datasplit_cs(raw, Y_colname, X_colname, test_size, random_seed=123):\n    X_train, X_test, Y_train, Y_test = train_test_split(raw[X_colname], raw[Y_colname], test_size=test_size, random_state=random_seed)\n    print('X_train:', X_train.shape, 'Y_train:', Y_train.shape)\n    print('X_test:', X_test.shape, 'Y_test:', Y_test.shape)\n    return X_train, X_test, Y_train, Y_test\n# X_train, X_test, Y_train, Y_test = datasplit_cs(raw_fe, Y_colname, X_colname, 0.2)\n\n\n### Data split of time series\ndef datasplit_ts(raw, Y_colname, X_colname, criteria):\n    raw_train = raw.loc[raw.index < criteria,:]\n    raw_test = raw.loc[raw.index >= criteria,:]\n    Y_train = raw_train[Y_colname]\n    X_train = raw_train[X_colname]\n    Y_test = raw_test[Y_colname]\n    X_test = raw_test[X_colname]\n    print('Train_size:', raw_train.shape, 'Test_size:', raw_test.shape)\n    print('X_train:', X_train.shape, 'Y_train:', Y_train.shape)\n    print('X_test:', X_test.shape, 'Y_test:', Y_test.shape)\n    return X_train, X_test, Y_train, Y_test\n# X_train, X_test, Y_train, Y_test = datasplit_ts(raw_fe, Y_colname, X_colname, '2012-07-01')\n\n\n### Evaluation of 1 pair of set\ndef evaluation(Y_real, Y_pred, graph_on=False):\n    loss_length = len(Y_real.values.flatten()) - len(Y_pred)\n    if loss_length != 0:\n        Y_real = Y_real[loss_length:]\n    if graph_on == True:\n        pd.concat([Y_real, pd.DataFrame(Y_pred, index=Y_real.index, columns=['prediction'])], axis=1).plot(kind='line', figsize=(20,6),\n                                                                                                           xlim=(Y_real.index.min(),Y_real.index.max()),\n                                                                                                           linewidth=3, fontsize=20)\n        plt.title('Time Series of Target', fontsize=20)\n        plt.xlabel('Index', fontsize=15)\n        plt.ylabel('Target Value', fontsize=15)\n    MAE = abs(Y_real.values.flatten() - Y_pred).mean()\n    RMSE = np.sqrt(((Y_real.values.flatten() - Y_pred)**2).mean())\n    MAPE = (abs(Y_real.values.flatten() - Y_pred)\/Y_real.values.flatten()*100).mean()\n    Score = pd.DataFrame([MAE, RMSE, MAPE], index=['MAE', 'RMSE', 'MAPE'], columns=['Score']).T\n    Residual = pd.DataFrame(Y_real.values.flatten() - Y_pred, index=Y_real.index, columns=['Error'])\n    return Score, Residual\n# Score_tr, Residual_tr = evaluation(Y_train, pred_tr_reg1, graph_on=True)\n\n\n### Evaluation of train\/test pairs\ndef evaluation_trte(Y_real_tr, Y_pred_tr, Y_real_te, Y_pred_te, graph_on=False):\n    Score_tr, Residual_tr = evaluation(Y_real_tr, Y_pred_tr, graph_on=graph_on)\n    Score_te, Residual_te = evaluation(Y_real_te, Y_pred_te, graph_on=graph_on)\n    Score_trte = pd.concat([Score_tr, Score_te], axis=0)\n    Score_trte.index = ['Train', 'Test']\n    return Score_trte, Residual_tr, Residual_te\n# Score_reg1, Resid_tr_reg1, Resid_te_reg1 = evaluation_trte(Y_train, pred_tr_reg1, Y_test, pred_te_reg1, graph_on=True)\n\n","abf5cf63":"train = pd.read_csv(\"\/kaggle\/input\/30-days-of-ml\/train.csv\", low_memory=False)\ntest = pd.read_csv(\"\/kaggle\/input\/30-days-of-ml\/test.csv\", low_memory=False)\ntrain.info(memory_usage=\"deep\")","21754807":"test.info(memory_usage=\"deep\")","50124227":"train","3cda52a0":"# Get number of unique entries in each column with categorical data\nobject_cols = [col for col in train.columns if train[col].dtype == \"object\"]\n\nobject_nunique = list(map(lambda col: train[col].nunique(), object_cols))\nd = dict(zip(object_cols, object_nunique))\n\n# Print number of unique entries by column, in ascending order\nsorted(d.items(), key=lambda x: x[1])","2ee6a678":"# Get number of unique entries in each column with categorical data\nobject_cols = [col for col in test.columns if test[col].dtype == \"object\"]\n\nobject_nunique = list(map(lambda col: test[col].nunique(), object_cols))\nd = dict(zip(object_cols, object_nunique))\n\n# Print number of unique entries by column, in ascending order\nsorted(d.items(), key=lambda x: x[1])","2d3b130c":"train_obj = train[object_cols]\ntrain_num = train.iloc[:,1:-1].drop(object_cols, axis=1)\ntest_obj = test[object_cols]\ntest_num = test.iloc[:,1:].drop(object_cols, axis=1)","48ab4307":"from sklearn.preprocessing import LabelEncoder\n\nencoder = LabelEncoder()\ntrain_obj_label = train_obj.copy()\ntest_obj_label = test_obj.copy()\nfor i in range(10):\n    train_obj_label.iloc[:,i] = encoder.fit_transform(train_obj.iloc[:,i])\n    test_obj_label.iloc[:,i] = encoder.fit_transform(test_obj.iloc[:,i])","af707708":"train_obj = pd.get_dummies(train_obj)\ntest_obj = pd.get_dummies(test_obj)","05ed5a89":"list = [1,3,5,9,13,17,24,32,40,55]\n\ntrain_obj=train_obj.drop(train_obj.columns[list],axis=1)\ntest_obj=test_obj.drop(test_obj.columns[list],axis=1)","0f01c022":"compare_hist(train_num,test_num)","0dfe2b48":"train_num_scale, test_num_scale = feature_engineering_scaling(preprocessing.MinMaxScaler(), train_num, test_num)\ncompare_hist(train_num_scale,test_num_scale)","c4e29b60":"train_num_scale, test_num_scale = feature_engineering_scaling(preprocessing.StandardScaler(), train_num, test_num)\ncompare_hist(train_num_scale,test_num_scale)","92fff343":"train_num_scale, test_num_scale = feature_engineering_scaling(preprocessing.RobustScaler(), train_num, test_num)\ncompare_hist(train_num_scale,test_num_scale)","18eef1fe":"train_num_scale, test_num_scale = feature_engineering_scaling(preprocessing.Normalizer(), train_num, test_num)\ncompare_hist(train_num_scale,test_num_scale)","4c547d2b":"X = pd.concat([train_obj_label, train_num_scale], axis=1)\nX_test = pd.concat([test_obj_label, test_num_scale], axis=1)\ny = train[\"target\"]\n\nX_train, X_val, Y_train, Y_val = train_test_split(X, y, test_size=0.8, random_state=0)\n\n# Applying Base Model\nfit_reg2_rd = sm.OLS(Y_train, X_train).fit()\ndisplay(fit_reg2_rd.summary())\npred_tr_reg2_rd = fit_reg2_rd.predict(X_train).values\npred_te_reg2_rd = fit_reg2_rd.predict(X_val).values\n\n# Evaluation\nScore_reg2_rd, Resid_tr_reg2_rd, Resid_te_reg2_rd = evaluation_trte(Y_train, pred_tr_reg2_rd, \n                                                                Y_val, pred_te_reg2_rd, graph_on=False)\ndisplay(Score_reg2_rd)\nprint(Score_reg2_rd.iloc[1,1])","2bfce60a":"X = pd.concat([train_obj, train_num_scale], axis=1)\nX_test = pd.concat([test_obj, test_num_scale], axis=1)\ny = train[\"target\"]\n\nX_train, X_val, Y_train, Y_val = train_test_split(X, y, test_size=0.8, random_state=0)\n\n# Applying Base Model\nfit_reg2_rd = sm.OLS(Y_train, X_train).fit()\ndisplay(fit_reg2_rd.summary())\npred_tr_reg2_rd = fit_reg2_rd.predict(X_train).values\npred_te_reg2_rd = fit_reg2_rd.predict(X_val).values\n\n# Evaluation\nScore_reg2_rd, Resid_tr_reg2_rd, Resid_te_reg2_rd = evaluation_trte(Y_train, pred_tr_reg2_rd, \n                                                                Y_val, pred_te_reg2_rd, graph_on=False)\ndisplay(Score_reg2_rd)\nprint(Score_reg2_rd.iloc[1,1])","53aa0a78":"# extract effective features using variance inflation factor\nX_temp=X.drop(['cat6_A','cat4_B','cat3_C','cat7_E'],axis=1)\n\nvif = pd.DataFrame()\nvif['VIF_Factor'] = [variance_inflation_factor(X_temp.values, i) \n                     for i in range(X_temp.shape[1])]\nvif['Feature'] = X_temp.columns\nvif.sort_values(by='VIF_Factor', ascending=True)","89a9b943":"X = pd.concat([train_obj, train_num_scale], axis=1)\nX_test = pd.concat([test_obj, test_num_scale], axis=1)\ny = train[\"target\"]\n\nX_train, X_val, Y_train, Y_val = train_test_split(X.drop(['cat6_A','cat4_B','cat3_C','cat7_E'],\n                                                         axis=1), y, test_size=0.8, random_state=0)\n\n# Applying Base Model\nfit_reg2_rd = sm.OLS(Y_train, X_train).fit()\ndisplay(fit_reg2_rd.summary())\npred_tr_reg2_rd = fit_reg2_rd.predict(X_train).values\npred_te_reg2_rd = fit_reg2_rd.predict(X_val).values\n\n# Evaluation\nScore_reg2_rd, Resid_tr_reg2_rd, Resid_te_reg2_rd = evaluation_trte(Y_train, pred_tr_reg2_rd, \n                                                                Y_val, pred_te_reg2_rd, graph_on=False)\ndisplay(Score_reg2_rd)\nprint(Score_reg2_rd.iloc[1,1])","1021afad":"# reduce model\n\nX = pd.concat([train_obj, train_num_scale], axis=1)\nX_test = pd.concat([test_obj, test_num_scale], axis=1)\ny = train[\"target\"]\n\nX_train, X_val, Y_train, Y_val = train_test_split(X.drop(['cat6_A','cat4_B','cat3_C','cat7_E',\n                            'cat6_E','cat7_B','cat6_I','cat8_B','cat4_C','cat3_B','cat6_B','cat7_A','cat7_I','cat4_A','cat6_G','cat6_D','cat7_F','cat9_D','cat7_C','cat5_A'],\n                                                         axis=1), y, test_size=0.8, random_state=0)\n\n# Applying Base Model\nfit_reg2_rd = sm.OLS(Y_train, X_train).fit()\ndisplay(fit_reg2_rd.summary())\npred_tr_reg2_rd = fit_reg2_rd.predict(X_train).values\npred_te_reg2_rd = fit_reg2_rd.predict(X_val).values\n\n# Evaluation\nScore_reg2_rd, Resid_tr_reg2_rd, Resid_te_reg2_rd = evaluation_trte(Y_train, pred_tr_reg2_rd, \n                                                                Y_val, pred_te_reg2_rd, graph_on=False)\ndisplay(Score_reg2_rd)\nprint(Score_reg2_rd.iloc[1,1])","97158ef3":"# extract effective features using variance inflation factor\nX_temp=X.drop(['cat6_A','cat4_B','cat3_C','cat7_E',\n               'cont7','cont5','cont0','cont9','cont12','cont8','cat2_A','cont2','cont3','cont11',\n              'cont6','cont13','cont4','cont1'],axis=1)\n\nvif = pd.DataFrame()\nvif['VIF_Factor'] = [variance_inflation_factor(X_temp.values, i) \n                     for i in range(X_temp.shape[1])]\nvif['Feature'] = X_temp.columns\nvif.sort_values(by='VIF_Factor', ascending=True)","ffcf95cc":"X = pd.concat([train_obj, train_num_scale], axis=1)\nX_test = pd.concat([test_obj, test_num_scale], axis=1)\ny = train[\"target\"]\n\nX_train, X_val, Y_train, Y_val = train_test_split(X.drop(['cat6_A','cat4_B','cat3_C','cat7_E',\n               'cont7','cont5','cont0','cont9','cont12','cont8','cat2_A','cont2','cont3','cont11','cont6','cont13','cont4','cont1'],axis=1), y, test_size=0.8, random_state=0)\n\n# Applying Base Model\nfit_reg2_rd = sm.OLS(Y_train, X_train).fit()\ndisplay(fit_reg2_rd.summary())\npred_tr_reg2_rd = fit_reg2_rd.predict(X_train).values\npred_te_reg2_rd = fit_reg2_rd.predict(X_val).values\n\n# Evaluation\nScore_reg2_rd, Resid_tr_reg2_rd, Resid_te_reg2_rd = evaluation_trte(Y_train, pred_tr_reg2_rd, \n                                                                Y_val, pred_te_reg2_rd, graph_on=False)\ndisplay(Score_reg2_rd)\nprint(Score_reg2_rd.iloc[1,1])","7ae8d885":"# reduce model\n\nX = pd.concat([train_obj, train_num_scale], axis=1)\nX_test = pd.concat([test_obj, test_num_scale], axis=1)\ny = train[\"target\"]\n\nX_train, X_val, Y_train, Y_val = train_test_split(X.drop(['cat6_A','cat4_B','cat3_C','cat7_E',\n               'cont7','cont5','cont0','cont9','cont12','cont8','cat2_A','cont2','cont3','cont11','cont6','cont13','cont4','cont1',\n                'cat6_G','cat7_C','cat7_A','cat4_C','cat6_E','cat7_F'],axis=1), y, test_size=0.8, random_state=0)\n\n# Applying Base Model\nfit_reg2_rd = sm.OLS(Y_train, X_train).fit()\ndisplay(fit_reg2_rd.summary())\npred_tr_reg2_rd = fit_reg2_rd.predict(X_train).values\npred_te_reg2_rd = fit_reg2_rd.predict(X_val).values\n\n# Evaluation\nScore_reg2_rd, Resid_tr_reg2_rd, Resid_te_reg2_rd = evaluation_trte(Y_train, pred_tr_reg2_rd, \n                                                                Y_val, pred_te_reg2_rd, graph_on=False)\ndisplay(Score_reg2_rd)\nprint(Score_reg2_rd.iloc[1,1])","eded9fbd":"from mlxtend.feature_selection import SequentialFeatureSelector as SFS \nfrom sklearn.linear_model import LinearRegression\n\nlr=LinearRegression() \nsfs1 = SFS(lr, \n           k_features=10, \n           forward=False, \n           floating=True, \n           verbose=2, \n           n_jobs=1,\n           cv=5,\n           scoring='neg_root_mean_squared_error') \n\nsfs1 = sfs1.fit(X, y) ","dfe8a276":"feature = ['cat0_A', 'cat1_A', 'cat2_A', 'cat3_B', 'cat3_C', 'cat4_A', 'cat4_C', 'cat5_B', 'cat5_C', 'cat6_B',\n 'cat6_C', 'cat6_D', 'cat7_C', 'cat7_F', 'cat7_I', 'cat8_A', 'cat8_B', 'cat8_C', 'cat8_E', 'cat9_A', 'cat9_E',\n 'cat9_F', 'cat9_I', 'cat9_J', 'cat9_K', 'cat9_M', 'cont0', 'cont1', 'cont2', 'cont3', 'cont5', 'cont6', 'cont7',\n 'cont8', 'cont9', 'cont10', 'cont11', 'cont12', 'cont13']","22ab1833":"X = pd.concat([train_obj, train_num_scale], axis=1)\nX_test = pd.concat([test_obj, test_num_scale], axis=1)\ny = train[\"target\"]\n\nX_train, X_val, Y_train, Y_val = train_test_split(X[feature], y, test_size=0.8, random_state=0)\n\n# Applying Base Model\nfit_reg2_rd = sm.OLS(Y_train, X_train).fit()\ndisplay(fit_reg2_rd.summary())\npred_tr_reg2_rd = fit_reg2_rd.predict(X_train).values\npred_te_reg2_rd = fit_reg2_rd.predict(X_val).values\n\n# Evaluation\nScore_reg2_rd, Resid_tr_reg2_rd, Resid_te_reg2_rd = evaluation_trte(Y_train, pred_tr_reg2_rd, \n                                                                Y_val, pred_te_reg2_rd, graph_on=False)\ndisplay(Score_reg2_rd)\nprint(Score_reg2_rd.iloc[1,1])","78948be1":"# reduce model\nX = pd.concat([train_obj, train_num_scale], axis=1)\nX_test = pd.concat([test_obj, test_num_scale], axis=1)\ny = train[\"target\"]\n\nX_train, X_val, Y_train, Y_val = train_test_split(X[feature].drop(['cat4_C','cat6_B','cat7_F','cat9_J','cat7_I','cat8_B','cat9_M','cat7_C','cat9_I','cat4_A','cat9_E','cat9_A','cat6_D','cat6_C'],axis=1), y, test_size=0.8, random_state=0)\n\n# Applying Base Model\nfit_reg2_rd = sm.OLS(Y_train, X_train).fit()\ndisplay(fit_reg2_rd.summary())\npred_tr_reg2_rd = fit_reg2_rd.predict(X_train).values\npred_te_reg2_rd = fit_reg2_rd.predict(X_val).values\n\n# Evaluation\nScore_reg2_rd, Resid_tr_reg2_rd, Resid_te_reg2_rd = evaluation_trte(Y_train, pred_tr_reg2_rd, \n                                                                Y_val, pred_te_reg2_rd, graph_on=False)\ndisplay(Score_reg2_rd)\nprint(Score_reg2_rd.iloc[1,1])","4861e698":"!pip install prince","89c6c38a":"train_obj_ = train[object_cols]\ntest_obj_ = test[object_cols]","a5a9ad15":"import prince\nmca = prince.MCA(n_components=-1)\nmca = mca.fit(train_obj_)\ntrain_obj_mca = mca.transform(train_obj_)\ntest_obj_mca = mca.transform(test_obj_)","1473b9eb":"from sklearn.decomposition import PCA\npca = PCA(svd_solver='full')\ntrain_reduction=pd.concat([train_obj_mca,train_num_scale],axis=1)\ntest_reduction=pd.concat([test_obj_mca,test_num_scale],axis=1)\n\npca.fit(train_reduction)\ntrain_reduction = pd.DataFrame((pca.transform(train_reduction)))\ntest_reduction = pd.DataFrame((pca.transform(test_reduction)))","ef28e62b":"X_train, X_val, Y_train, Y_val = train_test_split(train_reduction.drop([10,11,13,14,15,16,17,20],axis=1), y, test_size=0.8, random_state=0)\nX_train, X_val = feature_engineering_scaling(preprocessing.Normalizer(), X_train, X_val)\n\n# Applying Base Model\nfit_reg2_rd = sm.OLS(Y_train, X_train).fit()\ndisplay(fit_reg2_rd.summary())\npred_tr_reg2_rd = fit_reg2_rd.predict(X_train).values\npred_te_reg2_rd = fit_reg2_rd.predict(X_val).values\n\n# Evaluation\nScore_reg2_rd, Resid_tr_reg2_rd, Resid_te_reg2_rd = evaluation_trte(Y_train, pred_tr_reg2_rd, \n                                                                Y_val, pred_te_reg2_rd, graph_on=False)\ndisplay(Score_reg2_rd)\nprint(Score_reg2_rd.iloc[1,1])","449645da":"X = pd.concat([train_obj, train_num_scale], axis=1)\nX_test = pd.concat([test_obj, test_num_scale], axis=1)\ny = train[\"target\"]\n\n# Applying Base Model\nfit_reg2_rd = sm.OLS(y, X).fit()\ndisplay(fit_reg2_rd.summary())\npred_tr_reg2_rd = fit_reg2_rd.predict(X_test).values","5d0159e3":"submission = pd.read_csv(\"\/kaggle\/input\/30-days-of-ml\/sample_submission.csv\", low_memory=False)","660ed7cb":"submission.iloc[:,1]=pred_tr_reg2_rd","a1b88bdd":"submission.to_csv('submission_linear_regression.csv',index=False)","ad581c2e":"We can use only numeric variables in linear regression.","7bca788f":"0.74278","226ecd12":"The value of Cond. No. is smaller than the previous model.\n\nNext, P>|t| eliminated variables larger than 0.05, making them simpler models.","1d92d007":"A VIF is used to verify the following assumptions:\n\n- \"10\" as the maximum level of VIF (Hair et al., 1995)\n\n\nVariables with VIFs of 10 or more sequentially were removed.","2c3f661c":"**[2021-08-27 00:59:17] Features: 39\/10 -- score: -0.7380876945372996[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.**\n\n**[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    1.9s remaining:    0.0s**\n\n**[Parallel(n_jobs=1)]: Done  39 out of  39 | elapsed:  1.3min finished**\n\n**[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.**\n\n**[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    2.0s remaining:    0.0s**\n\n**[Parallel(n_jobs=1)]: Done  21 out of  21 | elapsed:   41.9s finished**","c30717a9":"# Feature engineering and Linear Regression model\n\nI just use only linear regression. So, data should be made available for regression analysis.\n\nLinear regression need only numerical variables. \n\nAlso,there are four assumptions associated with a linear regression model:\n\n1. Linearity: The relationship between X and the mean of Y is linear.\n2. Homoscedasticity: The variance of residual is the same for any value of X.\n3. Independence: Observations are independent of each other.\n4. Normality: For any fixed value of X, Y is normally distributed.\n\nI separated object and number features.\n\nThen, I just use label and one-hot encoding in object features.\nAlso, standardization and normalization are used in number features.","2cafd892":"There are still some assumptions that are violated, but there are some things that we can see as a result of:\n- Variables that affect the target variable a lot : cat0_A, cat1_A, cat5_B, cat5_C, cat8_A, cat8_C, cat8_E, cat9_F, cat9_K, cont10\n- Variables that do not affect the target variable much : cat6_A, cat4_B, cat7_E, cat6_E, cat7_A, cat6_G, ","fd0c65bd":"We don't have any missing values in the all variable and different values in the objective variable. Nice!!","19951c2c":"## Normality: For any fixed value of X, Y is normally distributed.","ebf6a4b6":"Although the performance is slightly lower, the problem of multicollinearity has been greatly solved.","a4254044":"# 30 Days - Only Linear Regression\nSimple linear regression is the simplest and most descriptive model of regression problems. \n\nModels such as ensembles and deep learning do not know the effects of variables, so they do not know the relationship between how much they affect the target variable. \n\nYour goal is not to increase the performance of the model, but rather a good model that you can use to identify and explain the effects and influence of variables. \n\nSince the model is intuitive, a well-made regression model can sometimes expect good performance.","4c9441a3":"## Load Data & Function & Libarary","d83d5bb1":"# Dimensionality reduction\nDimension reduction, is the transformation of data from a high-dimensional space into a low-dimensional space so that the low-dimensional representation retains some meaningful properties of the original data, ideally close to its intrinsic dimension. Working in high-dimensional spaces can be undesirable for many reasons; raw data are often sparse as a consequence of the curse of dimensionality, and analyzing the data is usually computationally intractable. Dimensionality reduction is common in fields that deal with large numbers of observations and\/or large numbers of variables.\n\nWe intend to use the following methods to solve the multicollinearity problem. Continuous variables are converted through PCA and categorical variables are converted using MCA. The PCA is a distance-based dimension reduction that squares the data on the nearest axis. At this time, standardization and normalization processes are needed to prevent the effects of certain variables from increasing. MCA uses one-hot encoded variables as a measure of distance using a chi-square distribution to reduce dimensions of categorical variables.","4bffda50":"Normalizer looks the best graph.","04dc1432":"## Multicollinearity\nIn statistics, multicollinearity (also collinearity) is a phenomenon in which one predictor variable in a multiple regression model can be linearly predicted from the others with a substantial degree of accuracy. In this situation, the coefficient estimates of the multiple regression may change erratically in response to small changes in the model or the data. Multicollinearity does not reduce the predictive power or reliability of the model as a whole, at least within the sample data set; it only affects calculations regarding individual predictors. That is, a multivariate regression model with collinear predictors can indicate how well the entire bundle of predictors predicts the outcome variable, but it may not give valid results about any individual predictor, or about which predictors are redundant with respect to others.","8b8de031":"The assumptions of regression analysis were well satisfied, but the explanatory power of the regression model decreased significantly, resulting in poor results.\n\nIn conclusion, the performance of the model with one-hot encoding and normalization and without variable selection was the best. But it's too early to be disappointed that we know which variables are important and not important to the target. I also learned about the relationship between them. While improving scores is also important, it is important to keep in mind that explaining the relationships and influences between variables in the policy and decision-making process is also an important role as a data scientist.","66c4a502":"One-hot encoding has better grades than label encoding!!!!\n\nBut, detailed interpretation and assumption verification in model are required.\n\nBelow is an interpretation of the following regression results:","3645047b":"- \"5\" as the maximum level of VIF (Ringle et al., 2015)","ea640598":"## First table (regression model result)\n- Dep. Variable\n\n    - target variable\n\n- Model\n\n    - Regression Model (OLS here)\n\n- Method\n\n    - To optimize parameters (where LS)\n\n- No. Observations\n\n    - Number of pairs of data used for analysis, number of observations. The above results indicate that regression was performed with 60000 pairs of data.\n\n- DF Residuals\n\n    - The total number of observations is subtracted from the total number of parameters in the regression model. \"No.observations - (Df Model + 1).\n\n- DF Model\n\n    - In the example above, there are 60 predictors, so the Df Model is 60. The Df Model is the number of predictors. Usually, the whole parameter contains a dependent variable in regression, so the Df Model subtracts one from the entire variable.\n\n\n- R-squared (coefficient of determination)\n\n    - It is one of the indicators of how well the model fits the data. We can see that the above data is represented by 0.992 which means R-squared. Simply, it is a figure that expresses how linear (y = x0 * weight + x1 * weight + ...). The range is between 0 and 1 and the model has no explanatory power at all, and the closer it is to 1, the better the model can state the data. Usually, 0.4 or higher is a good model.\n\n- Adj. R-squared\n\n    - Number of parameters, the coefficient of determination adjusted according to the data that helps the model\n\n- F-statistic\n\n    - Is the derived regression expression appropriate? (the closer to 0 the more appropriate)\n\n- Prob(F-statistic)\n\n    - How significant is the regression expression (below 0.05 determines that variables are very relevant)\n\n- AIC (Akaike Information Criterion)\n\n    - Score to evaluate the model based on the number of observations and complexity of the model itself (lower the better)\n\n- BIC (Bayesian Information Criterion)\n\n    - Similar to AIC, but better performance than AIC because there are more penalty terms (lower is better)\n\n## Second table (variable's effect in the model)\n\n- coef(Coefficient)\n\n    - This is an estimate of the coefficients obtained from the data. If you put this in a linear regression model expression, you can assume that the expression 'y = x0 * coef0 + x1 * coef1 + ...' was created. Thise represents the relationship between independent and dependent variables.\n\n- std err\n\n    - Standard error of coefficient estimates; smaller values mean more accurate coefficients\n\n- t\n\n    - Relationship between independent variables and dependent variables (greater values, greater effects)\n\n- P > |t|\n\n    - The significance probability of the independent variable is usually determined to be significant when the independent variable has a 95% confidence, and the significance probability is less than 0.05. In other words, if less than 0.05, it is significant that the independent variable affects the dependent variable.\n\n    - The p-value(significance probability) is defined as the probability of having more extreme results(observation) than the results obtained when assuming 'Null hypothesis' are correct. Typically based on p-value < 0.05 or 0.01. If the calculated p-value is less than the reference value, you reject the null hypothesis, which means that it is extremely unlikely that the null hypothesis will occur.\n \n\n ## Third table (assumptions in the model)\n\n- Omnibus(D'Angostino's Test)\n    - D'Angostino's Test, a normality test that combines asymmetry and kurtosis (a larger value means a normal distribution)\n\n- Skewness\n    - Whether the residuals around the mean are symmetrical. The closer to zero, the more symmetrical it is.\n\n- Kurtosis\n    - Distribution shape of residuals, bell shape (normal shape), closer to zero (negative is flat, positive is pointy)\n\n- Durbin - Watson\n    - Above, it is shown as 2.011. The figure shows the independence of the residuals.\n\n    - If zero, the residuals have positive autocorrelation.\n    - Two has self-regardless independence.\n    - Four means the residuals have negative autocorrelation.\n    - Usually between 1.5 and 2.5 means independent and a regression model is appropriate. If it is close to 0 or 4, it means that the residuals have autocorrelation, so it increases the t, F, and R squares from the real world, distorting meaningless results into meaningful results.\n \n- Jarque-Bera(JB)\n    - Jacques Vera normality test, the larger the value, the more normal the data was used.\n\n- Cond. No\n    - Multicollinearity test, whether there is a correlation between independent variables (considering that there is a multicollinearity problem if there is 10 or more, meaningless if there is 1 variable)","2da796ac":"When only variables with a VIF of 5 or less were selected, their performance was significantly lower than before. In regression analysis, it seems important for the analyst to set a good threshold for each assumption.","e11a9310":"## Backward FeatureSelection\nIf you put performance first, try this variable selection method."}}