{"cell_type":{"98376d96":"code","ef680b4e":"code","bc72cdaf":"code","0589f1a6":"code","a8ecfd56":"code","0f020969":"code","0acbf80a":"code","3d88de45":"code","1b2d3275":"code","15c50644":"code","5e7e16a7":"code","1fec7de0":"code","eb41199d":"code","ded1d150":"code","a70bf230":"code","4abe0fc3":"code","bc64b95e":"code","27c7570d":"code","be236637":"code","05daaeca":"code","a0d8cffd":"code","8be453e7":"code","8ce023ce":"code","acb38e8d":"code","0b8ba106":"code","87e5bbca":"code","e40e5a8a":"code","fa3a0261":"code","be2eb9ee":"code","acfdac78":"code","208c28d9":"code","48e888d9":"code","5a80d69a":"code","90180bba":"code","2d6438b7":"code","d333b84e":"code","84642b18":"code","82ea3ad0":"code","ca77ba23":"code","a01d1185":"code","cf895359":"markdown","c2ab3b5b":"markdown","736d21c2":"markdown","6f7316e2":"markdown","4650102b":"markdown","d1d40e5b":"markdown","e791edd2":"markdown","81cb145b":"markdown","e5d698cd":"markdown","66a14fbf":"markdown","db08cdaa":"markdown","4ace827d":"markdown","90071159":"markdown","6a7e297d":"markdown","99b30de6":"markdown","dc199cde":"markdown","e5cd73f3":"markdown","012a26fc":"markdown","4114f8ee":"markdown","94ef2083":"markdown","5c860640":"markdown","6075ed1b":"markdown","9127a2fd":"markdown","4a89edaa":"markdown","29eeed91":"markdown","30bcb540":"markdown","e84c7063":"markdown"},"source":{"98376d96":"import pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport warnings\nfrom fbprophet import Prophet\nfrom statsmodels.tsa.statespace.varmax import VARMAX\nimport warnings\nwarnings.filterwarnings('ignore')","ef680b4e":"train_orgnl = pd.read_csv('..\/input\/into-the-future\/train.csv')\ntest = pd.read_csv('..\/input\/into-the-future\/test.csv')","bc72cdaf":"train_orgnl","0589f1a6":"train_orgnl.info()","a8ecfd56":"train_orgnl['time'] = pd.to_datetime(train_orgnl.time)\ntest['time'] = pd.to_datetime(test.time)","0f020969":"test.info()","0acbf80a":"print(train_orgnl['time'].min())\nprint(train_orgnl['time'].max())","3d88de45":"train = train_orgnl.copy()","1b2d3275":"train_orgnl.set_index('time', inplace=True)\ntrain_orgnl.drop('id', axis=1, inplace=True)","15c50644":"train_orgnl['feature_1'].plot(kind='line',color='red', figsize=(20,10))","5e7e16a7":"train_orgnl['feature_2'].plot(kind='line',color='red', figsize=(20,10))","1fec7de0":"print('Number of missing values in feature_1 of Training Data: ' ,train['feature_1'].isnull().sum())\nprint('Number of missing values in feature_2 of Training Data: ' ,train['feature_2'].isnull().sum())\nprint('Number of missing values in feature_1 of Test Data: ' ,test['feature_1'].isnull().sum())","eb41199d":"train1 = train[:int(0.9*len(train))]\nvalid = train[int(0.9*len(train)):]","ded1d150":"print(train1.shape)\nprint(valid.shape)","a70bf230":"mdl = VARMAX(train_orgnl)\nmdl_fit = mdl.fit()","4abe0fc3":"prdn = mdl_fit.forecast(steps=len(valid))\nprdn.head(10)","bc64b95e":"valid.shape","27c7570d":"from sklearn.metrics import mean_squared_error as ms\nfrom sklearn.metrics import mean_absolute_error as ma","be236637":"valid.set_index('time', inplace=True)\nvalid.drop('id',axis=1, inplace=True)","05daaeca":"prdn.head()","a0d8cffd":"import math\nrmse=math.sqrt(ms(prdn,valid))\nprint('Mean absolute error is: '+ str(ma(prdn,valid)))\nprint('Root Mean Squared error is: ' + str(rmse))","8be453e7":"fbp_data = pd.DataFrame(columns=['ds', 'y', 'add1'])\nfbp_data['ds'] = train['time']\nfbp_data['y'] = train['feature_2']\nfbp_data['add1'] = train['feature_1']","8ce023ce":"size = int(fbp_data.shape[0]*0.9)\nx_train = fbp_data[:size]\nx_valid = fbp_data[size:]","acb38e8d":"model = Prophet()\nmodel.fit(x_train)","0b8ba106":"print(x_train.shape)\nprint(x_valid.shape)","87e5bbca":"pred = model.predict(x_valid.drop('y', axis=1))","e40e5a8a":"pred","fa3a0261":"model.plot_components(pred)","be2eb9ee":"f_prediction = pred['yhat']","acfdac78":"f_prediction.head()","208c28d9":"f2_valid = x_valid['y']\nplt.plot(f_prediction, 'r')\nplt.plot(f2_valid.reset_index(drop=True), 'b')","48e888d9":"rmse_fb=np.sqrt(ms(f2_valid, f_prediction))\nprint('Mean absolute error is: '+ str(ma(f_prediction,f2_valid)))\nprint('Root Mean Squared error is: ' + str(rmse_fb))","5a80d69a":"model_full = Prophet()\nmodel_full.fit(fbp_data)","90180bba":"id_test = test['id'].values\ndate_test = test['time'].values","2d6438b7":"x_test = pd.DataFrame(data=date_test, columns=['ds'])\nx_test['add1'] = test['feature_1'].values","d333b84e":"prediction_test = model.predict(x_test)","84642b18":"model_full.plot_components(prediction_test)","82ea3ad0":"prediction = prediction_test['yhat']","ca77ba23":"prediction_test.head()","a01d1185":"prediction_test.to_csv('results.csv')","cf895359":"Now lets discuss the parameters involved in this model,\nWhen I plotted my data, I saw a trend that keeps on growing with no real saturation insight, so my best bet would be to go with ***linear*** instead of ***logistic***. \nAnd for ***n_changepoints***, I started out with 10 and gradually took it to 100. Now while this reduced the ***RMSE***, this model works best when you let it choose it's own parameters.\n\nSo I'm going to leave it alone to let it decide on it's own.\n\nYou can take a look at [this](https:\/\/towardsdatascience.com\/implementing-facebook-prophet-efficiently-c241305405a3), to get sense out of it.","c2ab3b5b":"Even though fbprophet seems like the best model in our case, I thought of checking with **VARMAX (Vector Autoregressive Moving Average model with eXogenous variables)** first.","736d21c2":"At about 15 minutes, we can see that there is a sudden dip","6f7316e2":"This actually performed better than **VARMAX**","4650102b":"For time series analysis, we need the time feature to be in datetime format\n\nLets do that","d1d40e5b":"Lets check","e791edd2":"Lets now look at predictions","81cb145b":"Loading the necessary libraries","e5d698cd":"Now we split the data into train and test set\nSince the data is less, we use 9:1 ratio for train and test set","66a14fbf":"Looking at **feature_2**, we can see that there is a bit of seasonality, but it's not much.\n\nAlso we can see that **feature_1** and **feature_2** are negatively correlated.\n\nNoticeable trends and changing levels can be also be seen in **feature_1** and **feature_2**\n","db08cdaa":"Importing evaluation metrics like **mean_squared_error** and **mean_absolute error**","4ace827d":"# Fb Prophet","90071159":"Everything checksout.\n\nLets take a look the start and end time","6a7e297d":"Splitting the data into train and validation data (90:10)","99b30de6":"Loading the data sets","dc199cde":"One final check at the shape of the data.","e5cd73f3":"Now we could maybe apply **fbprobhet** model","012a26fc":"Looking at **RMSE** and **MAE**","4114f8ee":"Lets check for the missing values in our data","94ef2083":"Seperating **id** and **time** features for now.","5c860640":"Lets set **time** feature as index which helps us in analyzing trend of the data","6075ed1b":"Now this might be an outlier\/random event, but since we cannot quite put our finger on the reason of this random event, we cannot decide whether or not we should remove it.\n\nI'll keep it just in case, because even though we might be able to achieve a better score\/ lower RMSE, it's not the right way.\n\nAnyways let's continue.","9127a2fd":"Checking the shape of train and test set after the split","4a89edaa":"Making a copy of the original dataframe, just in case things get messed up ","29eeed91":"Now lets fit it on our full data","30bcb540":"Checking for the type of features in the data","e84c7063":"You can also see that there's a sudden dip from about 54479 to 47888. This is also an outlier\/random event.\nI'm going to keep it as well."}}