{"cell_type":{"f9fc1e69":"code","e9d33254":"code","1a8a7356":"code","99c24138":"code","931658dd":"code","04ce64e9":"code","c668ddcd":"code","2a3aeb1f":"code","0c1e8402":"code","04e7ab50":"code","26c58075":"code","b8c87fe4":"code","0d854b59":"code","816e9347":"code","5b9447c2":"code","fd155b43":"code","6119e0a2":"code","76103f52":"code","de4760b9":"code","2fbb1427":"code","b9f189da":"code","c13f5163":"code","b4ebf268":"code","794f4533":"markdown","a064f16a":"markdown","5ee86a92":"markdown","771edf9b":"markdown","2de38339":"markdown","75dd4a69":"markdown","7e1fda53":"markdown","def7c5f6":"markdown"},"source":{"f9fc1e69":"import numpy as np\nimport pandas as pd\nfrom fastai.text.all import *\nimport re","e9d33254":"dir_path = \"\/kaggle\/input\/nlp-getting-started\/\"\ntrain_df = pd.read_csv(dir_path + \"train.csv\")\ntest_df = pd.read_csv(dir_path + \"test.csv\")","1a8a7356":"train_df","99c24138":"train_df = train_df.drop(columns=[\"id\", \"keyword\", \"location\"])","931658dd":"train_df[\"target\"].value_counts()","04ce64e9":"def remove_URL(text):\n    url = re.compile(r'https?:\/\/\\S+|www\\.\\S+')\n    return url.sub(r'',text)\n\ntrain_df[\"text\"] = train_df[\"text\"].apply(remove_URL)\ntest_df[\"text\"] = test_df[\"text\"].apply(remove_URL)","c668ddcd":"def remove_html(text):\n    html=re.compile(r'<.*?>')\n    return html.sub(r'',text)\n\ntrain_df[\"text\"] = train_df[\"text\"].apply(remove_html)\ntest_df[\"text\"] = test_df[\"text\"].apply(remove_html)","2a3aeb1f":"def remove_emoji(text):\n    emoji_pattern = re.compile(\"[\"\n                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n                           u\"\\U00002702-\\U000027B0\"\n                           u\"\\U000024C2-\\U0001F251\"\n                           \"]+\", flags=re.UNICODE)\n    return emoji_pattern.sub(r'', text)\n\ntrain_df[\"text\"] = train_df[\"text\"].apply(remove_emoji)\ntest_df[\"text\"] = test_df[\"text\"].apply(remove_emoji)","0c1e8402":"train_df","04e7ab50":"train_df[\"text\"].apply(lambda x:len(x.split())).plot(kind=\"hist\");","26c58075":"from transformers import AutoTokenizer, AutoModelForSequenceClassification","b8c87fe4":"tokenizer = AutoTokenizer.from_pretrained(\"roberta-large\")","0d854b59":"train_tensor = tokenizer(list(train_df[\"text\"]), padding=\"max_length\",\n                        truncation=True, max_length=30,\n                        return_tensors=\"pt\")[\"input_ids\"]","816e9347":"class TweetDataset:\n    def __init__(self, tensors, targ, ids):\n        self.text = tensors[ids, :]\n        self.targ = targ[ids].reset_index(drop=True)\n    \n    def __len__(self):\n        return len(self.text)\n    \n    def __getitem__(self, idx):\n        \n        t = self.text[idx]\n        y = self.targ[idx]\n        \n        return t, tensor(y)","5b9447c2":"train_ids, valid_ids = RandomSplitter()(train_df)\n\n\ntarget = train_df[\"target\"]\n\ntrain_ds = TweetDataset(train_tensor, target, train_ids)\nvalid_ds = TweetDataset(train_tensor, target, valid_ids)\n\ntrain_dl = DataLoader(train_ds, bs=64)\nvalid_dl = DataLoader(valid_ds, bs=512)\ndls = DataLoaders(train_dl, valid_dl).to(\"cuda\")","fd155b43":"bert = AutoModelForSequenceClassification.from_pretrained(\"roberta-large\", num_labels=2).train().to(\"cuda\")\n\nclass BertClassifier(Module):\n    def __init__(self, bert):\n        self.bert = bert\n    def forward(self, x):\n        return self.bert(x).logits\n\nmodel = BertClassifier(bert)","6119e0a2":"learn = Learner(dls, model, metrics=[accuracy, F1Score()]).to_fp16()\nlearn.lr_find()","76103f52":"learn.fit_one_cycle(3, lr_max=1e-5)","de4760b9":"from sklearn.metrics import f1_score\n\npreds, targs = learn.get_preds()\n\nmin_threshold = None\nmax_f1 = -float(\"inf\")\nthresholds = np.linspace(0.3, 0.7, 50)\nfor threshold in thresholds:\n    f1 = f1_score(targs, F.softmax(preds, dim=1)[:, 1]>threshold)\n    if f1 > max_f1:\n        min_threshold = threshold\n        min_f1 = f1\n    print(f\"threshold:{threshold:.4f} - f1:{f1:.4f}\")","2fbb1427":"test_tensor = tokenizer(list(test_df[\"text\"]),\n                        padding=\"max_length\",\n                        truncation=True,\n                        max_length=30,\n                        return_tensors=\"pt\")[\"input_ids\"]","b9f189da":"class TestDS:\n    def __init__(self, tensors):\n        self.tensors = tensors\n    \n    def __len__(self):\n        return len(self.tensors)\n    \n    def __getitem__(self, idx):\n        t = self.tensors[idx]\n        return t, tensor(0)\n\ntest_dl = DataLoader(TestDS(test_tensor), bs=128)","c13f5163":"test_preds = learn.get_preds(dl=test_dl)","b4ebf268":"sub = pd.read_csv(dir_path + \"sample_submission.csv\")\nprediction = (F.softmax(test_preds[0], dim=1)[:, 1]>min_threshold).int()\nsub = pd.read_csv(dir_path + \"sample_submission.csv\")\nsub[\"target\"] = prediction\nsub.to_csv(\"submission.csv\", index=False)","794f4533":"# Get the model","a064f16a":"# Import the data and clean it","5ee86a92":"From the graph above, we can know that the longest tweet has 30 words, so I set the `max_length` to 30.","771edf9b":"# Start training","2de38339":"# Get tokens for the transformer","75dd4a69":"# Make prediction on the test set and submit the prediction","7e1fda53":"# Preparing datasets and dataloaders","def7c5f6":"# Find the best threshold for f1 score"}}