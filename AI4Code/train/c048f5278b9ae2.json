{"cell_type":{"b30c058a":"code","0e66e697":"code","7698adbc":"code","e2ac4347":"code","b5366a60":"code","00808abe":"code","48d99d4e":"code","eabcd53c":"code","ca1ac531":"code","c15c4d5e":"code","2b7fe956":"code","c4af0744":"code","f29538aa":"code","bdff8af2":"code","47ee800d":"code","4393d232":"code","319a55f7":"code","4c907aac":"code","34358604":"code","f72d7674":"code","7384a971":"code","dd7c39b6":"code","32041251":"code","ff099fa4":"code","a7b7c074":"code","df7e15b9":"code","120d62de":"markdown","f7641dcf":"markdown","5139d9b3":"markdown","7e28a121":"markdown","12a20a22":"markdown","1db909b1":"markdown","522abb09":"markdown","e27d3c20":"markdown"},"source":{"b30c058a":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","0e66e697":"df = pd.read_csv('\/kaggle\/input\/real-time-advertisers-auction\/Dataset.csv')","7698adbc":"df.head()","e2ac4347":"def weird_division(n, d):\n    return n \/ d if d else 0\n\ndf['CPM'] = df.apply(lambda x: weird_division((x['total_revenue'] * 100), x['measurable_impressions']) * 1000 , axis=1)","b5366a60":"df['CPM'].mean()","00808abe":"import matplotlib.pyplot as plt\n\ndf.plot(x='date', y='CPM')","48d99d4e":"columns_to_remove = ['date', 'total_revenue']\n\nX_train = df.loc[df['date'] <= '2019-06-21'].drop(columns=columns_to_remove)\nX_train = X_train[(X_train['CPM'] >= 0) & (X_train['CPM'] <= X_train['CPM'].quantile(.95))]\ny_train = X_train['CPM']\nX_train.drop(columns=['CPM'], inplace=True)\n\nX_test = df.loc[df['date'] > '2019-06-21'].drop(columns=columns_to_remove)\nX_test = X_test[(X_test['CPM'] >= 0) & (X_test['CPM'] <= X_test['CPM'].quantile(.95))]\ny_test = X_test['CPM']\nX_test.drop(columns=['CPM'], inplace=True)","eabcd53c":"from xgboost import XGBRegressor\nfrom sklearn.metrics import mean_squared_error\n\nxgb_start = XGBRegressor(random_state=0)\nxgb_start.fit(X_train, y_train, eval_metric='rmse')\npred = xgb_start.predict(X_test)\nmean_squared_error(y_test, pred)","ca1ac531":"def get_feature_importance(clsf, ftrs):\n    imp = clsf.feature_importances_.tolist()\n    feat = ftrs\n    result = pd.DataFrame({'feat':feat,'score':imp})\n    result = result.sort_values(by=['score'],ascending=False)\n    return result\n\nget_feature_importance(xgb_start, X_train.columns)","c15c4d5e":"columns_to_remove = ['date', 'total_revenue', 'measurable_impressions', 'order_id' , 'line_item_type_id', 'integration_type_id' , 'revenue_share_percent']\n\nX_train = df.loc[df['date'] <= '2019-06-21'].drop(columns=columns_to_remove)\nX_train = X_train[(X_train['CPM'] >= 0) & (X_train['CPM'] <= X_train['CPM'].quantile(.95))]\ny_train = X_train['CPM']\nX_train.drop(columns=['CPM'], inplace=True)\n\nX_test = df.loc[df['date'] > '2019-06-21'].drop(columns=columns_to_remove)\nX_test = X_test[(X_test['CPM'] >= 0) & (X_test['CPM'] <= X_test['CPM'].quantile(.95))]\ny_test = X_test['CPM']\nX_test.drop(columns=['CPM'], inplace=True)","2b7fe956":"xgb = XGBRegressor(random_state=0)\nxgb.fit(X_train, y_train, eval_metric='rmse')\npred = xgb.predict(X_test)\nmean_squared_error(y_test, pred)","c4af0744":"get_feature_importance(xgb, X_train.columns)","f29538aa":"from sklearn.linear_model import LinearRegression, LogisticRegression, Ridge, Lasso, ElasticNet \nfrom sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor \nfrom lightgbm import LGBMRegressor","bdff8af2":"models = [\n          GradientBoostingRegressor(random_state=0), \n          XGBRegressor(random_state=0), \n          RandomForestRegressor(random_state=0), \n          LGBMRegressor(random_state=0), \n          Ridge(random_state=0)\n          ]","47ee800d":"for model in models:\n    try:\n        model.fit(X_train, y_train, eval_metric='rmse')\n    except:\n        model.fit(X_train, y_train)\n    pred = model.predict(X_test)\n    model_name = type(model).__name__\n    print(f'{model_name} - MAE {mean_squared_error(y_test, pred)}')","4393d232":"pred = None\nmodels = [\n          XGBRegressor(random_state=0), \n          RandomForestRegressor(random_state=0), \n          LGBMRegressor(random_state=0), \n          ]\nfor model in models:\n    try:\n        model.fit(X_train, y_train, eval_metric='rmse')\n    except:\n        model.fit(X_train, y_train)\n\nfor i in range(len(models)):\n    if pred is None: \n        pred = models[i].predict(X_test)\n    else:\n        pred += models[i].predict(X_test)\n\nprint(mean_squared_error(y_test, pred \/ 3))","319a55f7":"from sklearn.model_selection import GridSearchCV","4c907aac":"xg_reg = XGBRegressor(random_state=0)\nparams = {\n        'max_depth': [5, 10, 15],\n        'n_estimators': [50, 75, 100]\n        }\n\ngrid_search = GridSearchCV(xg_reg, params, cv=5, n_jobs=-1, verbose=2, scoring='neg_mean_squared_error')\n\ngrid_search.fit(X_train, y_train)","34358604":"grid_search.best_params_","f72d7674":"xgb = XGBRegressor(random_state=0, max_depth=10, n_estimators=75)\nxgb.fit(X_train, y_train, eval_metric='rmse')\npred = xgb.predict(X_test)\nmean_squared_error(y_test, pred)","7384a971":"rf_reg = RandomForestRegressor(random_state=0)\nparams = {\n        'max_depth': [15, 30, 25],\n        'n_estimators': [150, 200]\n        }\n\ngrid_search_rf = GridSearchCV(rf_reg, params, cv=5, n_jobs=-1, verbose=2, scoring='neg_mean_squared_error')\n\ngrid_search_rf.fit(X_train, y_train)","dd7c39b6":"rf = RandomForestRegressor(random_state=0, max_depth = 25, n_estimators = 200)\nrf.fit(X_train, y_train)\npred = rf.predict(X_test)\nmean_squared_error(y_test, pred)","32041251":"lgbm_reg = LGBMRegressor(random_state=0)\nparams = {\n        'max_depth': [15, 50, 100],\n        'n_estimators': [200, 500, 1000]\n        }\n\ngrid_search_lgbm = GridSearchCV(lgbm_reg, params, cv=5, n_jobs=-1, verbose=2, scoring='neg_mean_squared_error')\n\ngrid_search_lgbm.fit(X_train, y_train)","ff099fa4":"lgbm = LGBMRegressor(random_state=0, max_depth = 50, n_estimators = 1000)\nlgbm.fit(X_train, y_train, eval_metric='rmse')\npred = lgbm.predict(X_test)\nmean_squared_error(y_test, pred)","a7b7c074":"from sklearn.ensemble import VotingRegressor\nVotReg = VotingRegressor(estimators=[('rf', rf), ('xgb', xgb), ('lgbm', lgbm),])\nVotReg.fit(X_train, y_train)","df7e15b9":"pred = VotReg.predict(X_test)\nmean_squared_error(y_test, pred)","120d62de":"# Voiting","f7641dcf":"# Search best params for models","5139d9b3":"# Calculating and analysis of CPM","7e28a121":"will remove the column 'total_revenue' as it directly affects the results. also we don't need a column 'date'","12a20a22":"# Prepare Ensemble of best models","1db909b1":"# Find best models","522abb09":"Apparently, the columns 'measurable_impressions', 'order_id' , 'line_item_type_id' are some leaks of the data, so the author of the initial solution removed them. Will remove it too. \n\nthe columns 'integration_type_id' , 'revenue_share_percent have no effect but the author of the initial solution removed them. ","e27d3c20":"of course, more detailed investigation will improve the result"}}