{"cell_type":{"19098752":"code","4f4537f2":"code","10844c33":"code","87331679":"code","ba6877cc":"code","f30b7307":"code","e82aee96":"code","d3e0b771":"code","7d665190":"code","601271ff":"code","ab26e782":"code","a6f2d999":"code","4e2d0cfd":"code","3f1dce8d":"code","9e9942e3":"code","8786ca5b":"code","5523ed74":"code","2126ac13":"code","f9e1c8ba":"code","8f4c8120":"code","473211f6":"code","8a8b16ff":"code","65804a2b":"markdown","13bbacd4":"markdown","b234abdd":"markdown","13574630":"markdown","a9bb7137":"markdown","50dacb97":"markdown","4e59abbf":"markdown"},"source":{"19098752":"import numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport pandas as pd\nimport tensorflow as tf\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import train_test_split","4f4537f2":"# Load data\nIris = pd.read_csv(\"..\/input\/iris\/Iris.csv\")","10844c33":"Iris.head()","87331679":"# split data and target\ndata = Iris[['SepalLengthCm', 'SepalWidthCm', 'PetalLengthCm', 'PetalWidthCm']]\ntarget = Iris['Species']","ba6877cc":"print('Number of records: ', len(data))","f30b7307":"# check null value: there is no missing value\ndata.isnull().sum()","e82aee96":"target.isnull().sum()","d3e0b771":"# The 4 features are likely to be at the same scale\ndata.describe()","7d665190":"# distribution of each feature\nsns.boxplot(data=data)","601271ff":"# distribution of each feature within each class of target\nplt.figure(figsize=(20,10))\nplt.subplot(2,2,1)\nsns.violinplot(x=data['SepalLengthCm'], y=target)\nplt.subplot(2,2,2)\nsns.violinplot(x=data['SepalWidthCm'], y=target)\nplt.subplot(2,2,3)\nsns.violinplot(x=data['PetalLengthCm'], y=target)\nplt.subplot(2,2,4)\nsns.violinplot(x=data['PetalWidthCm'], y=target)","ab26e782":"# label encode target to numerical\nle = LabelEncoder()\ntarget_label_encoded = le.fit_transform(target)\n# create a temporary variable to concat data and encoded target\ntemp = data.copy()\ntemp['Species'] = target_label_encoded\ntemp.corr()['Species'].sort_values(ascending=False)","a6f2d999":"sns.heatmap(temp.corr())","4e2d0cfd":"# scatter plot on 2 features\nsns.scatterplot(x=data['SepalLengthCm'], y=data['PetalLengthCm'], hue=target)","3f1dce8d":"sns.scatterplot(x=data['SepalWidthCm'], y=data['PetalWidthCm'], hue=target)","9e9942e3":"# split data set into train and test set\nX_train, X_test, y_train, y_test = train_test_split(data, target_label_encoded, test_size=0.2)","8786ca5b":"# create tf dataset for training and validation\n# because tf dataset needs input as type numpy ndarray so we need convert pandas df into numpy array by using .value\ntrain_ds = tf.data.Dataset.from_tensor_slices((X_train.values, y_train)).shuffle(10000).batch(32)\ntest_ds = tf.data.Dataset.from_tensor_slices((X_test.values, y_test)).batch(32)","5523ed74":"# define model architecture\ndef build_model():\n    # Our model will have 1 input layer of 4 nodes, 1 Dense layer with 32 nodes \n    # and 1 output layer 3 nodes corresponding to 3 species classes\n    x = tf.keras.layers.Input(shape=(4,))\n    dense1 = tf.keras.layers.Dense(32, activation='relu')(x)\n    y = tf.keras.layers.Dense(3, activation='softmax')(dense1)\n    model = tf.keras.models.Model(inputs=x, outputs=y)\n\n    return model\n\nmodel = build_model()\nmodel.summary()","2126ac13":"# define loss functions\nloss_object = tf.keras.losses.SparseCategoricalCrossentropy()\n\n# optimizer adam\noptimizer = tf.keras.optimizers.Adam(learning_rate=0.01)\n\n# tensor that holds metric values such as loss value and accuracy value\ntrain_loss = tf.keras.metrics.Mean(name='train_loss')\ntrain_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name='train_accuracy')\n\ntest_loss = tf.keras.metrics.Mean(name='test_loss')\ntest_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name='test_accuracy')","f9e1c8ba":"@tf.function\ndef train_step(features, targets):\n    with tf.GradientTape() as tape:\n        # compute the predictions\n        predictions = model(features)\n        # compute loss between predictions and targets\n        loss = loss_object(targets, predictions)\n    # use gradient tape to track the gradients\n    gradients = tape.gradient(loss, model.trainable_variables)\n    # use gradients computed above to update trainable weights\n    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n    \n    train_loss(loss)\n    train_accuracy(targets, predictions)\n    \n@tf.function\ndef test_step(images, labels):\n    predictions = model(images)\n    t_loss = loss_object(labels, predictions)\n\n    test_loss(t_loss)\n    test_accuracy(labels, predictions)","8f4c8120":"EPOCHS = 15\nlist_train_losses = []\nlist_train_accs = []\nlist_test_losses = []\nlist_test_accs = []\n\nfor epoch in range(EPOCHS):\n    # Reset the metrics at the start of the next epoch\n    train_loss.reset_states()\n    train_accuracy.reset_states()\n    test_loss.reset_states()\n    test_accuracy.reset_states()\n\n    for features, targets in train_ds:\n        train_step(features, targets)\n\n    for features, targets in test_ds:\n        test_step(features, targets)\n    \n    list_train_losses.append(train_loss.result())\n    list_test_losses.append(test_loss.result())\n    list_train_accs.append(train_accuracy.result())\n    list_test_accs.append(test_accuracy.result())\n    \n    template = 'Epoch {}, Loss: {}, Accuracy: {}, Test Loss: {}, Test Accuracy: {}'\n    print(template.format(epoch+1,\n                        train_loss.result(),\n                        train_accuracy.result()*100,\n                        test_loss.result(),\n                        test_accuracy.result()*100))","473211f6":"# plot training, testing loss curves\nplt.plot(np.arange(EPOCHS), list_train_losses)\nplt.plot(np.arange(EPOCHS), list_test_losses)\nplt.xlabel('Epoch')\nplt.ylabel('Loss')","8a8b16ff":"# plot training, testing accuracy curves\nplt.plot(np.arange(EPOCHS), list_train_accs, 'r--')\nplt.plot(np.arange(EPOCHS), list_test_accs, 'b--')\nplt.xlabel('Epoch')\nplt.ylabel('Accuracy')","65804a2b":"# 1. Quick introduction:\n- Tensorflow 2.0 with the use of Keras high level API is a good approach of on many machine learning and deep learning problem\n- This notebook shows how to use tf.GradientTape() to optimize model explicitly","13bbacd4":"These code is referenced to https:\/\/www.tensorflow.org\/tutorials\/quickstart\/advanced","b234abdd":"- Our assumption when looking at the plot seems right","13574630":"# 2. Basic EDA","a9bb7137":"- It looks like PetalLengthCm and PetalWidthCm are the most useful features because their distributions divide into 3 seperated parts corresponding to 3 classes of target. Let's use correlation heatmap to check if our assumption is right","50dacb97":"# 3. Training and Evaluating","4e59abbf":"# 2. Load dataset:\n- Iris dataset is to be used\n- We need to use 4 features (SepalLengthCm\tSepalWidthCm\tPetalLengthCm\tPetalWidthCm) to predict the target variable Species "}}