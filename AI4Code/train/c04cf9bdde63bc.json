{"cell_type":{"0a2a22c8":"code","d098565a":"code","1d30a055":"code","04d3ce5b":"code","c6c9dc36":"code","fbec198e":"code","f0217829":"code","f1f2947f":"code","9c508bd9":"code","0024e9fc":"code","12e63dbe":"code","f5439f7d":"code","7e1686b7":"code","b2273ebd":"code","81f4ae04":"code","b6c2fb0c":"code","fa0535d1":"code","188e907a":"code","779a6b88":"code","b67c88ab":"code","7892d77f":"code","8a32628c":"code","1759d5a9":"code","87b210e6":"code","9fa4810d":"code","0bb2b238":"code","1a4f7f3a":"code","e51a10d9":"code","ccab4218":"code","a0b321e3":"code","7c8dccf6":"code","49f35b82":"code","be5dbfb4":"code","d44da37b":"code","3e043d8a":"code","d5e94123":"code","98a7b2c7":"code","176ea882":"code","fff998a0":"code","a36ae506":"code","61afef92":"code","713c0b1b":"code","bed3497d":"code","e61d275b":"code","aafeaa87":"code","9b7c8eef":"code","485fbad8":"code","84158edc":"code","f1931ad2":"code","ed3bc56a":"code","ac0eff8b":"code","e5176ec7":"code","b1d6a563":"code","48296ecd":"code","77d9c6fd":"code","23609151":"code","082ad952":"code","ac26e9b4":"code","cfbb20bd":"code","7126c06b":"code","ea91d4d5":"code","d2a68dc2":"code","cddb3e6e":"code","cf757e30":"code","e8fcf048":"markdown","3023e7a7":"markdown","d6785f59":"markdown","8dbbfa20":"markdown","235c2062":"markdown","acd1b8f5":"markdown","1de82dad":"markdown","23ca86d8":"markdown","bbd9bbae":"markdown","dde050dc":"markdown","bb590422":"markdown","13070c22":"markdown","23c21ba0":"markdown"},"source":{"0a2a22c8":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","d098565a":"#import some necessary librairies\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n%matplotlib inline\nimport matplotlib.pyplot as plt  # Matlab-style plotting\nimport seaborn as sns\ncolor = sns.color_palette()\nsns.set_style('darkgrid')\nimport warnings\ndef ignore_warn(*args, **kwargs):\n    pass\nwarnings.warn = ignore_warn #ignore annoying warning (from sklearn and seaborn)\n\n\nfrom scipy import stats\nfrom scipy.stats import norm, skew \nfrom scipy.special import boxcox1p\nfrom scipy.stats import boxcox_normmax #for some statistics\n\n\npd.set_option('display.float_format', lambda x: '{:.3f}'.format(x)) #Limiting floats output to 3 decimal points\n\n\nfrom subprocess import check_output\nprint(check_output([\"ls\", \"..\/input\"]).decode(\"utf8\")) #check the files available in the directory\n\nfrom datetime import datetime\nfrom sklearn.linear_model import ElasticNetCV, LassoCV, RidgeCV\nfrom sklearn.ensemble import GradientBoostingRegressor\nfrom sklearn.svm import SVR\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import RobustScaler\nfrom sklearn.model_selection import KFold, cross_val_score\nfrom sklearn.metrics import mean_squared_error\nfrom mlxtend.regressor import StackingCVRegressor\nfrom xgboost import XGBRegressor\nfrom lightgbm import LGBMRegressor\nimport sklearn.linear_model as linear_model\nfrom sklearn.manifold import TSNE\nfrom sklearn.cluster import KMeans\nfrom sklearn.decomposition import PCA\nfrom sklearn.preprocessing import StandardScaler\n\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# Any results you write to the current directory are saved as output.","1d30a055":"from numpy import sqrt","04d3ce5b":"train_data = pd.read_csv(\"\/kaggle\/input\/house-prices-advanced-regression-techniques\/train.csv\")\ntest_data = pd.read_csv(\"\/kaggle\/input\/house-prices-advanced-regression-techniques\/test.csv\")","c6c9dc36":"# Check for duplicates\nidsUnique = len(set(train_data.Id))\nidsTotal = train_data.shape[0]\nidsDupli = idsTotal - idsUnique\nprint(\"Train data: there are \" + str(idsDupli) + \" duplicate IDs for \" + str(idsTotal) + \" total entries\")\n\n# Drop Id column\ntrain_data.drop(\"Id\", axis = 1, inplace = True)","fbec198e":"# Check for duplicates\nidsUnique = len(set(test_data.Id))\nidsTotal = test_data.shape[0]\nidsDupli = idsTotal - idsUnique\nprint(\"Test data: there are \" + str(idsDupli) + \" duplicate IDs for \" + str(idsTotal) + \" total entries\")\n\n# Drop Id column\ntest_data.drop(\"Id\", axis = 1, inplace = True)","f0217829":"train_data.head(5)","f1f2947f":"test_data.head(5)","9c508bd9":"print(train_data.shape)\nprint('-'* 20)\nprint(test_data.shape)","0024e9fc":"train_data.describe()","12e63dbe":"train_data['SalePrice'].describe()","f5439f7d":"y = train_data['SalePrice']\nplt.figure(1); plt.title('Johnson SU') \nsns.distplot(y, kde=False, fit=stats.johnsonsu)\nplt.figure(2); plt.title('Normal')\nsns.distplot(y, kde=False, fit=stats.norm)\nplt.figure(3); plt.title('Log Normal')\nsns.distplot(y, kde=False, fit=stats.lognorm)\nfig = plt.figure()\nres = stats.probplot(train_data['SalePrice'], plot=plt)\nplt.show()","7e1686b7":"#We use the numpy fuction log1p which  applies log(1+x) to all elements of the column\ntrain_data[\"SalePrice\"] = np.log1p(train_data[\"SalePrice\"])\n\n#Check the new distribution \nsns.distplot(train_data['SalePrice'] , fit=norm);\n\n# Get the fitted parameters used by the function\n(mu, sigma) = norm.fit(train_data['SalePrice'])\nprint( '\\n mu = {:.2f} and sigma = {:.2f}\\n'.format(mu, sigma))\n\n#Now plot the distribution\nplt.legend(['Normal dist. ($\\mu=$ {:.2f} and $\\sigma=$ {:.2f} )'.format(mu, sigma)],\n            loc='best')\nplt.ylabel('Frequency')\nplt.title('SalePrice distribution')\n\n#Get also the QQ-plot\nfig = plt.figure()\nres = stats.probplot(train_data['SalePrice'], plot=plt)\nplt.show()","b2273ebd":"train_data[\"SalePrice\"].describe","81f4ae04":"quantitative = list(train_data.dtypes[train_data.dtypes != \"object\"].index)\nlen(quantitative)","b6c2fb0c":"qualitative = list(train_data.dtypes[train_data.dtypes == \"object\"].index)\nlen(qualitative)","fa0535d1":"#Correlation map to see how features are correlated with SalePrice (only numerial features will be considered)\ncorrmat = train_data.corr()\nplt.subplots(figsize=(15,12))\nsns.heatmap(corrmat, vmax=0.9, square=True)","188e907a":"def encode(frame_train, frame_test,feature):\n    ordering = pd.DataFrame()\n    ordering['val'] = frame_train[feature].unique()\n    ordering.index = ordering.val\n    ordering['spmean'] = frame_train[[feature, 'SalePrice']].groupby(feature).mean()['SalePrice']\n    ordering = ordering.sort_values('spmean')\n    ordering['ordering'] = range(1, ordering.shape[0]+1)\n    ordering = ordering['ordering'].to_dict()\n    \n    for cat, o in ordering.items():\n        frame_train.loc[frame_train[feature] == cat, feature+'_E'] = o\n        frame_test.loc[frame_test[feature] == cat, feature+'_E'] = o\n    \nqual_encoded = []\nfor q in qualitative:  \n    encode(train_data, test_data,q)\n    qual_encoded.append(q+'_E')\nprint(qual_encoded)","779a6b88":"# to make what encode function does more clear:\nordering = pd.DataFrame()\nordering['val'] = train_data['MSZoning'].unique()\nordering.index = ordering.val\nordering['spmean'] = train_data[['MSZoning', 'SalePrice']].groupby('MSZoning').mean()['SalePrice']\nordering = ordering.sort_values('spmean')\nordering['ordering'] = range(1, ordering.shape[0]+1)\nordering = ordering['ordering'].to_dict()","b67c88ab":"ordering","7892d77f":"q = list(qualitative)","8a32628c":"def spearman(frame, features):\n    spr = pd.DataFrame()\n    spr['feature'] = features\n    spr['spearman'] = [frame[f].corr(frame['SalePrice'], 'spearman') for f in features]\n    spr = spr.sort_values('spearman')\n    plt.figure(figsize=(6, 0.25*len(features)))\n    sns.barplot(data=spr, y='feature', x='spearman', orient='h')\n    \nfeatures = quantitative + qual_encoded\nspearman(train_data, features)","1759d5a9":"fig, ax = plt.subplots()\nax.scatter(x = train_data['GrLivArea'], y = train_data['SalePrice'])\nplt.ylabel('SalePrice', fontsize=13)\nplt.xlabel('GrLivArea', fontsize=13)\nplt.show()","87b210e6":"fig, ax = plt.subplots()\nax.scatter(x = train_data['OverallQual'], y = train_data['SalePrice'])\nplt.ylabel('SalePrice', fontsize=13)\nplt.xlabel('OverallQual', fontsize=13)\nplt.show()","9fa4810d":"fig, ax = plt.subplots()\nax.scatter(x = train_data['Neighborhood_E'], y = train_data['SalePrice'])\nplt.ylabel('SalePrice', fontsize=13)\nplt.xlabel('Neighborhood_E', fontsize=13)\nplt.show()","0bb2b238":"plt.figure(num=1,figsize=(0.25*len(quantitative),0.25*len(quantitative)))\ncorr = train_data[quantitative].corr()\nsns.heatmap(corr)\n\nplt.figure(2,figsize=(0.25*len(qual_encoded+['SalePrice']),0.25*len(qual_encoded+['SalePrice'])))\ncorr = train_data[qual_encoded+['SalePrice']].corr()\nsns.heatmap(corr)\n\nplt.figure(3,figsize=(0.25*len(qual_encoded+['SalePrice']),0.25*len(quantitative)))\ncorr = pd.DataFrame(np.zeros([len(quantitative)+1, len(qual_encoded)+1]), index=quantitative+['SalePrice'], columns=qual_encoded+['SalePrice'])\nfor q1 in quantitative:\n    for q2 in qual_encoded+['SalePrice']:\n        corr.loc[q1, q2] = train_data[q1].corr(train_data[q2])\nsns.heatmap(corr)","1a4f7f3a":"# Deleting outliers\ntrain_data = train_data[train_data.GrLivArea < 4500]","e51a10d9":"train_data.shape","ccab4218":"ntrain = train_data.shape[0]\nntest = test_data.shape[0]\ny_train = train_data.SalePrice.values\nall_data = pd.concat((train_data, test_data),ignore_index = True) #\u62fc\u63a5\u7684\u65f6\u5019\uff0c\u5c31\u7b97\u6709\u4e0d\u5171\u6709\u7684\u5217\u4e5f\u53ef\u4ee5\u4e00\u8d77\u62fc\u63a5\nall_data.drop(['SalePrice'], axis=1, inplace=True)\nprint(\"all_data size is : {}\".format(all_data.shape))","a0b321e3":"y_train.max()","7c8dccf6":"print(all_data.isnull().sum()) # too many which cannot be put in the command","49f35b82":"#delete the categorial feature\nqx = list(set(qualitative).difference(['Neighborhood']))\nall_data.drop(qx,axis = 1,inplace=True)","be5dbfb4":"all_data_na = pd.DataFrame((all_data.isnull().sum()\/ len(all_data)) * 100)\nall_data_na = all_data_na[all_data_na[0] > 0]\nall_data_na.sort_values(ascending=False,by = [0],inplace = True)\nall_data_na.head(20)","d44da37b":"print(all_data_na.shape)\nplt.figure(figsize=(15, 12))\nplt.xticks(rotation='90')\nsns.barplot(x=all_data_na.index, y=all_data_na[0])\nplt.xlabel('Features', fontsize=15)\nplt.ylabel('Percent of missing values', fontsize=15)\nplt.title('Percent missing data by feature', fontsize=15)","3e043d8a":"#delete those two missing ratio > 90 \nall_data.drop(['PoolQC_E','MiscFeature_E','Alley_E'],axis = 1,inplace=True)","d5e94123":"all_data.shape","98a7b2c7":"# for those missing rate < 10, we use mode() to replace the missing values\nfea = list(all_data_na[all_data_na[0]<20].index)\nfor f in fea:\n    all_data[f] = all_data.groupby('Neighborhood')[f].apply(lambda x: x.fillna(x.median()))","176ea882":"all_data.drop('Neighborhood',axis = 1,inplace=True)","fff998a0":"all_data_na1 = pd.DataFrame((all_data.isnull().sum()\/ len(all_data)) * 100)\nall_data_na1 = all_data_na1[all_data_na1[0] > 0]","a36ae506":"all_data_na1","61afef92":"def spearman_specific(frame, features,target):\n    spr = pd.DataFrame()\n    spr['feature'] = features\n    spr['spearman'] = [frame[f].corr(frame[target], 'spearman') for f in features]\n    spr = spr.sort_values('spearman')\n    plt.figure(figsize=(6, 0.25*len(features)))\n    sns.barplot(data=spr, y='feature', x='spearman', orient='h')","713c0b1b":"i = 0\nfor item in all_data_na1.index:\n    plt.figure(i)\n    ++i\n    spearman_specific(train_data,all_data.columns,item)","bed3497d":"all_data['FireplaceQu_E'] = all_data.groupby('KitchenQual_E')['FireplaceQu_E'].apply(lambda x: x.fillna(x.median()))\nall_data['Fence_E'] = all_data.groupby('KitchenQual_E')['Fence_E'].apply(lambda x: x.fillna(x.median()))","e61d275b":"numeric_dtypes = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\nnumerics2 = []\nfor i in all_data.columns:\n    if all_data[i].dtype in numeric_dtypes:\n        numerics2.append(i)","aafeaa87":"len(quantitative)","9b7c8eef":"num = list(set(quantitative).difference(['YearBuilt','YearRemodAdd', 'MoSold', 'YrSold']))","485fbad8":"len(num)","84158edc":"skew_features = all_data.apply(lambda x: skew(x)).sort_values(ascending=False)\n\nhigh_skew = skew_features[abs(skew_features) > 0.75]\nprint(\"There are {} skewed numerical features to Box Cox transform\".format(high_skew.shape[0]))\n\nskew_index = high_skew.index\n\nfor i in skew_index:\n    all_data[i] = boxcox1p(all_data[i], boxcox_normmax(all_data[i] + 1))","f1931ad2":"garage = ['GarageType_E', 'GarageFinish_E', 'GarageQual_E', 'GarageCond_E','GarageYrBlt','GarageCars','GarageArea']\nplt.figure(2,figsize=(0.25*len(qual_encoded+['SalePrice']),0.25*len(qual_encoded+['SalePrice'])))\ncorr = train_data[garage].corr()\nsns.heatmap(corr)","ed3bc56a":"# y_train has already defined earlier\ntrain = all_data[:ntrain]\ntest = all_data[ntrain:]","ac0eff8b":"train.shape","e5176ec7":"#cross-valiation\nkfolds = KFold(n_splits=10, shuffle=True, random_state=42)\n\ndef rmsle(y, y_pred):\n    return np.sqrt(mean_squared_error(y, y_pred))\n\ndef cv_rmse(model, X=train):\n    rmse = np.sqrt(-cross_val_score(model, X, y_train, scoring=\"neg_mean_squared_error\", cv=kfolds))\n    return (rmse)","b1d6a563":"#define parameters\nalphas_alt = [14.5, 14.6, 14.7, 14.8, 14.9, 15, 15.1, 15.2, 15.3, 15.4, 15.5]\nalphas2 = [5e-05, 0.0001, 0.0002, 0.0003, 0.0004, 0.0005, 0.0006, 0.0007, 0.0008]\ne_alphas = [0.0001, 0.0002, 0.0003, 0.0004, 0.0005, 0.0006, 0.0007]\ne_l1ratio = [0.8, 0.85, 0.9, 0.95, 0.99, 1]","48296ecd":"# normalize + regression\nridge = make_pipeline(RobustScaler(), RidgeCV(alphas=alphas_alt, cv=kfolds))\nlasso = make_pipeline(RobustScaler(), LassoCV(max_iter=1e7, alphas=alphas2, random_state=42, cv=kfolds))\nelasticnet = make_pipeline(RobustScaler(), ElasticNetCV(max_iter=1e7, alphas=e_alphas, cv=kfolds, l1_ratio=e_l1ratio))                                \nsvr = make_pipeline(RobustScaler(), SVR(C= 20, epsilon= 0.008, gamma=0.0003,))","77d9c6fd":"lightgbm = LGBMRegressor(objective='regression', \n                                       num_leaves=4,\n                                       learning_rate=0.01, \n                                       n_estimators=5000,\n                                       max_bin=200, \n                                       bagging_fraction=0.75,\n                                       bagging_freq=5, \n                                       bagging_seed=7,\n                                       feature_fraction=0.2,\n                                       feature_fraction_seed=7,\n                                       verbose=-1,\n                                       )","23609151":"gbr = GradientBoostingRegressor(n_estimators=3000, learning_rate=0.05, max_depth=4, max_features='sqrt', min_samples_leaf=15, min_samples_split=10, loss='huber', random_state =42)  ","082ad952":"xgboost = XGBRegressor(learning_rate=0.01,n_estimators=3460,\n                                     max_depth=3, min_child_weight=0,\n                                     gamma=0, subsample=0.7,\n                                     colsample_bytree=0.7,\n                                     objective='reg:linear', nthread=-1,\n                                     scale_pos_weight=1, seed=27,\n                                     reg_alpha=0.00006)","ac26e9b4":"stack_gen = StackingCVRegressor(regressors=(ridge, lasso, elasticnet, gbr, xgboost, lightgbm),\n                                meta_regressor=xgboost,\n                                use_features_in_secondary=True)","cfbb20bd":"score = cv_rmse(ridge)\nprint(\"RIDGE: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()), datetime.now(), )\n\nscore = cv_rmse(lasso)\nprint(\"LASSO: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()), datetime.now(), )\n\nscore = cv_rmse(elasticnet)\nprint(\"elastic net: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()), datetime.now(), )\n\nscore = cv_rmse(svr)\nprint(\"SVR: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()), datetime.now(), )\n\nscore = cv_rmse(lightgbm)\nprint(\"lightgbm: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()), datetime.now(), )\n\nscore = cv_rmse(gbr)\nprint(\"gbr: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()), datetime.now(), )\n\nscore = cv_rmse(xgboost)\nprint(\"xgboost: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()), datetime.now(), )","7126c06b":"print('START Fit')\n\nprint('stack_gen')\nstack_gen_model = stack_gen.fit(np.array(train), np.array(y_train))\n\nprint('elasticnet')\nelastic_model_full_data = elasticnet.fit(train, y_train)\n\nprint('Lasso')\nlasso_model_full_data = lasso.fit(train, y_train)\n\nprint('Ridge')\nridge_model_full_data = ridge.fit(train, y_train)\n\nprint('Svr')\nsvr_model_full_data = svr.fit(train, y_train)\n\nprint('GradientBoosting')\ngbr_model_full_data = gbr.fit(train, y_train)\n\nprint('xgboost')\nxgb_model_full_data = xgboost.fit(train, y_train)\n\nprint('lightgbm')\nlgb_model_full_data = lightgbm.fit(train, y_train)","ea91d4d5":"def blend_models_predict(X):\n    return ((0.1 * elastic_model_full_data.predict(X)) + \\\n            (0.1 * lasso_model_full_data.predict(X)) + \\\n            (0.1 * ridge_model_full_data.predict(X)) + \\\n            (0.1 * svr_model_full_data.predict(X)) + \\\n            (0.1 * gbr_model_full_data.predict(X)) + \\\n            (0.1 * xgb_model_full_data.predict(X)) + \\\n            (0.1 * lgb_model_full_data.predict(X)) + \\\n            (0.3 * stack_gen_model.predict(np.array(X))))","d2a68dc2":"print('RMSLE score on train data:')\nprint(rmsle(y_train, np.expm1(blend_models_predict(train)))","cddb3e6e":"y_train.mean()","cf757e30":"test_data = pd.read_csv(\"\/kaggle\/input\/house-prices-advanced-regression-techniques\/test.csv\")\noutput = pd.DataFrame({'Id': test_data.Id, 'SalePrice': np.floor(np.expm1(blend_models_predict(test))) })\noutput.to_csv('my_submission.csv', index=False)\nprint(\"Your submission was successfully saved!\")","e8fcf048":"The target variable is right skewed. As (linear) models love normally distributed data , we need to transform this variable and make it more normally distributed. So we apply log(1+x) to all elements of the column. Also, we could see that while log transformation does pretty good job, best fit is unbounded Johnson distribution. We don't use the convertion for Johnson distribution to normal distribution, becuase 4 parameters need to be calculated in the convertion.","3023e7a7":"Find the numeriable and categorial variables.","d6785f59":"3 features waiting for to be filled:\n* LotFrontage\uff1aLotArea\uff0c1stFlrSF\n* FireplaceQu_E\uff1aKitchenQual_E,TotalBsmtSF\n* Fence_E\uff1aOverallQual,KitchenQual_E","8dbbfa20":"**Spearman correlation** is better to work with in this case because it picks up relationships between variables even when they are nonlinear. \n* OverallQual is main criterion in establishing house price. \n* Neighborhood has big influence, partially it has some intrisinc value in itself, but also houses in certain regions tend to share same characteristics (confunding) what causes similar valuations.","235c2062":"# 2. Model","acd1b8f5":"Next, we want to find if there are varibales that can be conbined.","1de82dad":"Next, we do data processing.\n\nInvestigate the missing values.\n\nps: this is different from the case we meet earlier, because the number of variables is too large, just a print function couldn't show all varible containing missing values. Therefore, we put the result into a DataFrame.\n\nconcat: https:\/\/pandas.pydata.org\/pandas-docs\/stable\/reference\/api\/pandas.concat.html\n","23ca86d8":"Before diving into the problem, we notice that there are 79 variables in the dataset, from which we may have a initial idea that we may focus on feature engineering in this dataset. So let's begin!","bbd9bbae":"Next step, for 3 notebooks I learned from, all of them use normal distribution to understand the data. Although I don't really know why (notions'), I still choose to follow them.\n\nwhy\uff1fhttps:\/\/blog.csdn.net\/lamusique\/article\/details\/96495077](https:\/\/blog.csdn.net\/lamusique\/article\/details\/96495077)","dde050dc":"More feature engineering!","bb590422":"# 1. EDA","13070c22":"This showed that there are 37 numerial variables in this dataset. Because there are too many varibales, we put our energy on the target variable first.","23c21ba0":"explore relationships between variables\n\nwe take research the relationship between variables and the target predicting values ('SalePrice) as the next step. Here are three questions that we should consider:\n* Do we need to take this variable into consideration?\n* How important is this variable to our prediction?\n* Is the information contained in this variable could be found out through investigating other variables?\n\ncorrelation: https:\/\/guangchuangyu.github.io\/statistics_notes\/correlation.html\n\nSpearman's Rank-Order Correlation: https:\/\/statistics.laerd.com\/statistical-guides\/spearmans-rank-order-correlation-statistical-guide.php"}}