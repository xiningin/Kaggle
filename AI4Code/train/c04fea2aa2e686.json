{"cell_type":{"dd22d932":"code","1dcca62d":"code","547788b5":"code","3cea4afa":"code","b3b24949":"code","90da6f7f":"code","a3d7a064":"code","322d335d":"code","b3d3a208":"code","5421bef0":"code","656c2808":"code","092bbc6b":"code","d1e22784":"code","d7630b59":"code","ea41172f":"code","b2b88be1":"markdown","74af80bb":"markdown","83054c1c":"markdown","a4a58725":"markdown","3911fc49":"markdown","dc3763ed":"markdown","285e853d":"markdown","e0bd1118":"markdown","53dbaf81":"markdown"},"source":{"dd22d932":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","1dcca62d":"%%time\n\ntrain_df = pd.read_csv('\/kaggle\/input\/m5-forecasting-accuracy\/sales_train_validation.csv')\ncalendar = pd.read_csv('\/kaggle\/input\/m5-forecasting-accuracy\/calendar.csv')\nprices = pd.read_csv('\/kaggle\/input\/m5-forecasting-accuracy\/sell_prices.csv')\n\ntrain_fold_df = train_df.iloc[:, :-28]\nvalid_fold_df = train_df.iloc[:, -28:]","547788b5":"from typing import Union\n\nimport numpy as np\nimport pandas as pd\nfrom tqdm.auto import tqdm as tqdm\n\nclass WRMSSEEvaluator(object):\n    \n    group_ids = ( 'all_id', 'state_id', 'store_id', 'cat_id', 'dept_id', 'item_id',\n        ['state_id', 'cat_id'],  ['state_id', 'dept_id'], ['store_id', 'cat_id'],\n        ['store_id', 'dept_id'], ['item_id', 'state_id'], ['item_id', 'store_id'])\n\n    def __init__(self, \n                 train_df: pd.DataFrame, \n                 valid_df: pd.DataFrame, \n                 calendar: pd.DataFrame, \n                 prices: pd.DataFrame):\n        '''\n        intialize and calculate weights\n        '''\n        self.calendar = calendar\n        self.prices = prices\n        self.train_df = train_df\n        self.valid_df = valid_df\n        self.train_target_columns = [i for i in self.train_df.columns if i.startswith('d_')]\n        self.weight_columns = self.train_df.iloc[:, -28:].columns.tolist()\n\n        self.train_df['all_id'] = \"all\"\n\n        self.id_columns = [i for i in self.train_df.columns if not i.startswith('d_')]\n        self.valid_target_columns = [i for i in self.valid_df.columns if i.startswith('d_')]\n\n        if not all([c in self.valid_df.columns for c in self.id_columns]):\n            self.valid_df = pd.concat([self.train_df[self.id_columns], self.valid_df],\n                                      axis=1, \n                                      sort=False)\n        self.train_series = self.trans_30490_to_42840(self.train_df, \n                                                      self.train_target_columns, \n                                                      self.group_ids)\n        self.valid_series = self.trans_30490_to_42840(self.valid_df, \n                                                      self.valid_target_columns, \n                                                      self.group_ids)\n        self.weights = self.get_weight_df()\n        self.scale = self.get_scale()\n        self.train_series = None\n        self.train_df = None\n        self.prices = None\n        self.calendar = None\n\n    def get_scale(self):\n        '''\n        scaling factor for each series ignoring starting zeros\n        '''\n        scales = []\n        for i in tqdm(range(len(self.train_series))):\n            series = self.train_series.iloc[i].values\n            series = series[np.argmax(series!=0):]\n            scale = ((series[1:] - series[:-1]) ** 2).mean()\n            scales.append(scale)\n        return np.array(scales)\n    \n    def get_name(self, i):\n        '''\n        convert a str or list of strings to unique string \n        used for naming each of 42840 series\n        '''\n        if type(i) == str or type(i) == int:\n            return str(i)\n        else:\n            return \"--\".join(i)\n    \n    def get_weight_df(self) -> pd.DataFrame:\n        \"\"\"\n        returns weights for each of 42840 series in a dataFrame\n        \"\"\"\n        day_to_week = self.calendar.set_index(\"d\")[\"wm_yr_wk\"].to_dict()\n        weight_df = self.train_df[[\"item_id\", \"store_id\"] + self.weight_columns].set_index(\n            [\"item_id\", \"store_id\"]\n        )\n        weight_df = (\n            weight_df.stack().reset_index().rename(columns={\"level_2\": \"d\", 0: \"value\"})\n        )\n        weight_df[\"wm_yr_wk\"] = weight_df[\"d\"].map(day_to_week)\n        weight_df = weight_df.merge(\n            self.prices, how=\"left\", on=[\"item_id\", \"store_id\", \"wm_yr_wk\"]\n        )\n        weight_df[\"value\"] = weight_df[\"value\"] * weight_df[\"sell_price\"]\n        weight_df = weight_df.set_index([\"item_id\", \"store_id\", \"d\"]).unstack(level=2)[\n            \"value\"\n        ]\n        weight_df = weight_df.loc[\n            zip(self.train_df.item_id, self.train_df.store_id), :\n        ].reset_index(drop=True)\n        weight_df = pd.concat(\n            [self.train_df[self.id_columns], weight_df], axis=1, sort=False\n        )\n        weights_map = {}\n        for i, group_id in enumerate(tqdm(self.group_ids, leave=False)):\n            lv_weight = weight_df.groupby(group_id)[self.weight_columns].sum().sum(axis=1)\n            lv_weight = lv_weight \/ lv_weight.sum()\n            for i in range(len(lv_weight)):\n                weights_map[self.get_name(lv_weight.index[i])] = np.array(\n                    [lv_weight.iloc[i]]\n                )\n        weights = pd.DataFrame(weights_map).T \/ len(self.group_ids)\n\n        return weights\n\n    def trans_30490_to_42840(self, df, cols, group_ids, dis=False):\n        '''\n        transform 30490 sries to all 42840 series\n        '''\n        series_map = {}\n        for i, group_id in enumerate(tqdm(self.group_ids, leave=False, disable=dis)):\n            tr = df.groupby(group_id)[cols].sum()\n            for i in range(len(tr)):\n                series_map[self.get_name(tr.index[i])] = tr.iloc[i].values\n        return pd.DataFrame(series_map).T\n    \n    def get_rmsse(self, valid_preds) -> pd.Series:\n        '''\n        returns rmsse scores for all 42840 series\n        '''\n        score = ((self.valid_series - valid_preds) ** 2).mean(axis=1)\n        rmsse = (score \/ self.scale).map(np.sqrt)\n        return rmsse\n\n    def score(self, valid_preds: Union[pd.DataFrame, np.ndarray]) -> float:\n        assert self.valid_df[self.valid_target_columns].shape == valid_preds.shape\n\n        if isinstance(valid_preds, np.ndarray):\n            valid_preds = pd.DataFrame(valid_preds, columns=self.valid_target_columns)\n\n        valid_preds = pd.concat([self.valid_df[self.id_columns], valid_preds],\n                                axis=1, \n                                sort=False)\n        valid_preds = self.trans_30490_to_42840(valid_preds, \n                                                self.valid_target_columns, \n                                                self.group_ids, \n                                                True)\n        self.rmsse = self.get_rmsse(valid_preds)\n        self.contributors = pd.concat([self.weights, self.rmsse], \n                                      axis=1, \n                                      sort=False).prod(axis=1)\n        return np.sum(self.contributors)\n","3cea4afa":"from typing import Union\n\nimport numpy as np\nimport pandas as pd\nfrom scipy.sparse import csr_matrix\nfrom tqdm.auto import tqdm as tqdm\n\n\nclass FastWRMSSEEvaluator(object):\n\n    group_ids = ('all_id', 'state_id', 'store_id', 'cat_id', 'dept_id', 'item_id',\n                 ['state_id', 'cat_id'], ['state_id', 'dept_id'], ['store_id', 'cat_id'],\n                 ['store_id', 'dept_id'], ['item_id', 'state_id'], ['item_id', 'store_id'])\n\n    def __init__(self,\n                 train_df: pd.DataFrame,\n                 valid_df: pd.DataFrame,\n                 calendar: pd.DataFrame,\n                 prices: pd.DataFrame):\n        '''\n        intialize and calculate weights\n        '''\n        self.calendar = calendar\n        self.prices = prices\n        self.train_df = train_df\n        self.valid_df = valid_df\n        self.train_target_columns = [i for i in self.train_df.columns if i.startswith('d_')]\n        self.weight_columns = self.train_df.iloc[:, -28:].columns.tolist()\n\n        self.train_df['all_id'] = \"all\"\n\n        self.id_columns = [i for i in self.train_df.columns if not i.startswith('d_')]\n        self.valid_target_columns = [i for i in self.valid_df.columns if i.startswith('d_')]\n\n        val_cols = set(self.valid_df.columns)\n        val_col_diff = set(self.id_columns) - val_cols\n        if val_col_diff != set():\n            self.valid_df = pd.concat([self.train_df[val_col_diff], self.valid_df],\n                                      axis=1,\n                                      sort=False)\n\n        self.valid_df = self.valid_df.sort_values(by='id').reset_index()\n        self.train_df = self.train_df.sort_values(by='id').reset_index()\n        summing_matrix = self.create_summing_matrix(self.valid_df.sort_values(by='id').reset_index(), self.group_ids)\n        summing_matrix = summing_matrix.reindex(sorted(summing_matrix.columns), axis=1)\n        self.summing_index = summing_matrix.index\n        self.summing_matrix = csr_matrix(summing_matrix)\n\n        self.train_series = self.trans_30490_to_42840(self.train_df,\n                                                      self.train_target_columns,\n                                                      self.group_ids)\n        self.valid_series = self.trans_30490_to_42840(self.valid_df,\n                                                      self.valid_target_columns,\n                                                      self.group_ids)\n        self.weights = self.get_weight_df()\n        self.scale = self.get_scale()\n        self.train_series = None\n        self.train_df = None\n        self.prices = None\n        self.calendar = None\n\n    def get_scale(self):\n        '''\n        scaling factor for each series ignoring starting zeros\n        '''\n        scales = []\n        for i in tqdm(range(len(self.train_series))):\n            series = self.train_series.iloc[i].values\n            series = series[np.argmax(series != 0):]\n            scale = ((series[1:] - series[:-1]) ** 2).mean()\n            scales.append(scale)\n        return np.array(scales)\n\n    def get_name(self, i):\n        '''\n        convert a str or list of strings to unique string\n        used for naming each of 42840 series\n        '''\n        if type(i) == str or type(i) == int:\n            return str(i)\n        else:\n            return \"--\".join(i)\n\n    def get_weight_df(self) -> pd.DataFrame:\n        \"\"\"\n        returns weights for each of 42840 series in a dataFrame\n        \"\"\"\n        day_to_week = self.calendar.set_index(\"d\")[\"wm_yr_wk\"].to_dict()\n        weight_df = self.train_df[[\"item_id\", \"store_id\"] + self.weight_columns].set_index(\n            [\"item_id\", \"store_id\"]\n        )\n        weight_df = (\n            weight_df.stack().reset_index().rename(columns={\"level_2\": \"d\", 0: \"value\"})\n        )\n        weight_df[\"wm_yr_wk\"] = weight_df[\"d\"].map(day_to_week)\n        weight_df = weight_df.merge(\n            self.prices, how=\"left\", on=[\"item_id\", \"store_id\", \"wm_yr_wk\"]\n        )\n        weight_df[\"value\"] = weight_df[\"value\"] * weight_df[\"sell_price\"]\n        weight_df = weight_df.set_index([\"item_id\", \"store_id\", \"d\"]).unstack(level=2)[\n            \"value\"\n        ]\n        weight_df = weight_df.loc[\n            zip(self.train_df.item_id, self.train_df.store_id), :\n        ].reset_index(drop=True)\n        weight_df = pd.concat(\n            [self.train_df[self.id_columns], weight_df], axis=1, sort=False\n        )\n        weights_map = {}\n        for i, group_id in enumerate(tqdm(self.group_ids, leave=False)):\n            lv_weight = weight_df.groupby(group_id)[self.weight_columns].sum().sum(axis=1)\n            lv_weight = lv_weight \/ lv_weight.sum()\n            for i in range(len(lv_weight)):\n                weights_map[self.get_name(lv_weight.index[i])] = np.array(\n                    [lv_weight.iloc[i]]\n                )\n        weights = pd.DataFrame(weights_map).T \/ len(self.group_ids)\n\n        return weights\n\n    def create_summing_matrix(self, df, aggregation_levels):\n        # we are going to alter this, so copy it\n        aggregation_levels = [x for x in aggregation_levels]\n\n        for i, agg_cols in enumerate(aggregation_levels):\n            if type(agg_cols) is list:\n                new_col_name = \"_\".join(agg_cols)\n\n                df[new_col_name] = df[agg_cols[0]]\n                for c in agg_cols[1:]:\n                    df[new_col_name] += \"--\" + df[c]\n                aggregation_levels[i] = new_col_name\n\n        dummies = [pd.get_dummies(df[agg_cols], drop_first=False,\n                                  dtype=np.int8).T for agg_cols in aggregation_levels]\n        matrix = pd.concat(dummies)\n\n        return matrix\n\n    def trans_30490_to_42840(self, df, cols, group_ids, dis=False):\n        '''\n        transform 30490 sries to all 42840 series\n        '''\n        summed_values = self.summing_matrix@df.sort_index()[cols].values.astype(np.float32)\n        return pd.DataFrame(summed_values, columns=cols, index=self.summing_index)\n\n    def get_rmsse(self, valid_preds) -> pd.Series:\n        '''\n        returns rmsse scores for all 42840 series\n        '''\n        score = ((self.valid_series - valid_preds) ** 2).mean(axis=1)\n        rmsse = (score \/ self.scale).map(np.sqrt)\n        return rmsse\n\n    def validate_valid_preds(self, valid_preds):\n        assert self.valid_df[self.valid_target_columns].shape == valid_preds.shape\n        assert set(self.valid_df['id'].values) == set(valid_preds.index.values)\n\n    def score(self, valid_preds: Union[pd.DataFrame, np.ndarray]) -> float:\n        if isinstance(valid_preds, np.ndarray):\n            valid_preds = pd.DataFrame(valid_preds, columns=self.valid_target_columns, index=self.valid_df.set_index('id').index)\n        # tweak: use join instead of concat\n        # to be safe..\n        valid_preds = self.valid_df\\\n            .set_index('id')[set(self.id_columns) - set(['id'])]\\\n            .join(valid_preds)\\\n            .sort_index()\\\n            .reset_index()\n        #         valid_preds = pd.concat([self.valid_df[self.id_columns], valid_preds],\n        #                                 axis=1,\n        #                                 sort=False)\n        valid_preds = self.trans_30490_to_42840(valid_preds,\n                                                self.valid_target_columns,\n                                                self.group_ids,\n                                                True)\n        self.rmsse = self.get_rmsse(valid_preds)\n        self.contributors = pd.concat([self.weights, self.rmsse],\n                                      axis=1,\n                                      sort=False).prod(axis=1)\n        return np.sum(self.contributors)\n","b3b24949":"%%time\ne = WRMSSEEvaluator(train_fold_df.copy(), valid_fold_df.copy(), calendar, prices)","90da6f7f":"%%time\nfast_e = FastWRMSSEEvaluator(train_fold_df.copy(), valid_fold_df.copy(), calendar, prices)","a3d7a064":"valid_preds = np.random.randint(4, size=valid_fold_df.shape)","322d335d":"%%timeit -n 100 -r 5\n# n - execute the statement n times \n# r - repeat each loop r times and return the best\n\nfast_score = fast_e.score(valid_preds)","b3d3a208":"%%timeit -n 3 -r 3\n# n - execute the statement n times \n# r - repeat each loop r times and return the best\n\nscore = e.score(valid_preds)","5421bef0":"valid_preds = np.random.randint(4, size=valid_fold_df.shape)\ntest_sub = pd.DataFrame(valid_preds, columns=e.valid_target_columns, index=e.valid_df['id'].values)\nscore, fast_score = e.score(test_sub.values), fast_e.score(test_sub)\nprint(score, fast_score)\nassert np.isclose(score, fast_score)","656c2808":"valid_preds = np.random.randint(3, size=valid_fold_df.shape)\ntest_sub = pd.DataFrame(valid_preds, columns=e.valid_target_columns, index=e.valid_df['id'].values)\nscore, fast_score = e.score(test_sub.values), fast_e.score(test_sub)\nprint(score, fast_score)\nassert np.isclose(score, fast_score)","092bbc6b":"valid_preds = np.random.randint(2, size=valid_fold_df.shape)\ntest_sub = pd.DataFrame(valid_preds, columns=e.valid_target_columns, index=e.valid_df['id'].values)\nscore, fast_score = e.score(test_sub.values), fast_e.score(test_sub)\nprint(score, fast_score)\nassert np.isclose(score, fast_score)","d1e22784":"np.allclose(fast_e.contributors, e.contributors)","d7630b59":"np.allclose(fast_e.rmsse, e.rmsse)","ea41172f":"very_bad_preds = np.random.randint(30, size=valid_fold_df.shape)\n\ntest_sub = pd.DataFrame(very_bad_preds, columns=e.valid_target_columns, index=e.valid_df['id'].values)\nscore, fast_score = e.score(test_sub.values), fast_e.score(test_sub)\nprint(score, fast_score)\nassert np.isclose(score, fast_score)","b2b88be1":"# Verify implementation correctness","74af80bb":"## Compare with original evaluator","83054c1c":"# Summing matrix overview","a4a58725":"# Fast WRMSSE Evaluator with extra features\n\nHere I try to make implementation of `WRMSSEEvaluator` faster from amazing notebook [WRMSSE Evaluator with extra features](https:\/\/www.kaggle.com\/dhananjay3\/wrmsse-evaluator-with-extra-features) by applying matrix multiplication for hierarchical aggregation from this wonderful notebook: [Fast & Clear WRMSSE 18ms](https:\/\/www.kaggle.com\/sibmike\/fast-clear-wrmsse-18ms)\n\n\n|         |            |  |\n| ------------- |:-------------:| -----:|\n| WRMSSEEvaluator | ~15.1 s | ~15100 ms |\n| This notebook, FastWRMSSEEvaluator | ~0.21 s      |   ~214 ms |\n\n\nIt could be improved further to get to 18 ms, but if you don't use evaluator as a loss function, there is no a lot of difference between 18 ms and 166 ms.\n\n**Version 5**: Fix bug with indexing of validation predictions. Need to clean up and double check\n\n_Note: Below I use some test to verify that this implementation is correct, but encourage you to double check. Apppreciate your feedback!_\n","3911fc49":"# Speed comparison","dc3763ed":"Quote from [Forecasting: Principles and Practice](https:\/\/otexts.com\/fpp2\/hts.html)\n\nThese equations can be thought of as aggregation constraints or summing equalities, and can be more efficiently represented using matrix notation. We construct an  `n\u00d7m` matrix **S** (referred to as the \u201csumming matrix\u201d) which dictates the way in which the bottom-level series are aggregated.\n\nSample hierarchy:\n\n![image.png](attachment:image.png)\n\n\nAggregation matrix for this hierarchy:\n\n\n$$\n\\begin{bmatrix}\n    y_{t} \\\\\n    y{A}{t} \\\\\n    y{B}{t} \\\\\n    y{AA}{t} \\\\\n    y{AB}{t} \\\\\n    y{AC}{t} \\\\\n    y{BA}{t} \\\\\n    y{BB}{t}\n  \\end{bmatrix}\n  =\n  \\begin{bmatrix}\n    1 & 1 & 1 & 1 & 1 \\\\\n    1 & 1 & 1 & 0 & 0 \\\\\n    0 & 0 & 0 & 1 & 1 \\\\\n    1  & 0  & 0  & 0  & 0  \\\\\n    0  & 1  & 0  & 0  & 0  \\\\\n    0  & 0  & 1  & 0  & 0  \\\\\n    0  & 0  & 0  & 1  & 0  \\\\\n    0  & 0  & 0  & 0  & 1\n  \\end{bmatrix}\n  \\begin{bmatrix}\n    y{AA}{t} \\\\\n    y{AB}{t} \\\\\n    y{AC}{t} \\\\\n    y{BA}{t} \\\\\n    y{BB}{t}\n  \\end{bmatrix}\n$$\n\nor in more compact notation\n$$\n\\begin{equation}\n  \\textbf{y}_t=\\textbf{S}\\textbf{b}_{t}\n\\end{equation}\n$$\n\n","285e853d":"Here we verify that scores are the same on different random predictions","e0bd1118":"# Difference from original version","53dbaf81":"Method `trans_30490_to_42840` for aggregating top levels of time-series hierarchy used python loop + `groupby`, that is quite slow:\n\n\n```python\ndef trans_30490_to_42840(self, df, cols, group_ids, dis=False):\n    '''\n    transform 30490 sries to all 42840 series\n    '''\n    series_map = {}\n    for i, group_id in enumerate(tqdm(self.group_ids, leave=False, disable=dis)):\n        tr = df.groupby(group_id)[cols].sum()\n        for i in range(len(tr)):\n            series_map[self.get_name(tr.index[i])] = tr.iloc[i].values\n    return pd.DataFrame(series_map).T\n```\n\nNow it uses matrix multiplication:\n```python\ndef trans_30490_to_42840(self, df, cols, group_ids, dis=False):\n    '''\n    transform 30490 sries to all 42840 series\n    '''\n    summed_values = self.summing_matrix@df.sort_index()[cols].values.astype(np.int32)\n    return pd.DataFrame(summed_values, columns=cols, index=self.summing_index)\n```\n\nSumming matrix is initalized with this method:\n\n```python\ndef create_summing_matrix(self, df, aggregation_levels):\n    # we are going to alter this, so copy it\n    aggregation_levels = [x for x in aggregation_levels]\n\n    for i, agg_cols in enumerate(aggregation_levels):\n        if type(agg_cols) is list:\n            new_col_name = \"_\".join(agg_cols)\n\n            df[new_col_name] = df[agg_cols[0]]\n            for c in agg_cols[1:]:\n                df[new_col_name] += \"--\" + df[c]\n            aggregation_levels[i] = new_col_name\n\n    dummies = [pd.get_dummies(df[agg_cols], drop_first=False,\n                              dtype=np.int8).T for agg_cols in aggregation_levels]\n    matrix = pd.concat(dummies)\n\n    return matrix\n```\n\nAnd then transformed to sparse matrix for reducing memory usage and speed:\n\n```python\nsumming_matrix = self.create_summing_matrix(self.valid_df, self.group_ids)\nsumming_matrix = summing_matrix.reindex(sorted(summing_matrix.columns), axis=1)\nself.summing_index = summing_matrix.index\nself.summing_matrix = csr_matrix(summing_matrix)\n```"}}