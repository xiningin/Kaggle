{"cell_type":{"cdc1526c":"code","caefecfe":"code","658a18ff":"code","ca54422b":"code","2f0adcc9":"code","f400047a":"code","6556a892":"code","0905a5e2":"code","fe6926c9":"code","79c62163":"code","14ef71b1":"code","e2640428":"code","a7255876":"code","1f7508d0":"code","6b504afc":"code","54dfa332":"code","0e115ad0":"code","2856c86e":"code","7fb56c95":"code","35a82f22":"code","7f035c40":"code","4bd39de1":"code","7e7c2e68":"code","b89448b9":"code","a636258b":"code","eecac4fd":"code","2fd1e028":"code","5004f9ea":"code","42b791f2":"code","8a1ad28a":"code","ea5bfd58":"code","85300033":"code","8725cd37":"code","e388ed9a":"code","a33e6652":"code","a4284d5a":"code","f218ddcb":"code","95c7a353":"code","7bc9b6e3":"code","44c9aef3":"code","7b60af08":"code","b345d242":"code","75b2b6ba":"code","56cccfd9":"code","7c676996":"code","0c3e7af7":"code","9be20776":"code","a5030999":"code","97dec03c":"code","52c5fcee":"code","33625b0f":"code","47b5df5b":"code","ece6eb6f":"code","af98a82c":"code","a08863f6":"code","958810d3":"code","f4573ed5":"code","69c5f38b":"code","661061a9":"code","a379e83e":"code","d0464634":"code","e01b0e82":"code","95aa5e6b":"code","7a044423":"code","c9d7384e":"code","f92f6f8a":"code","4df4876c":"code","98e197c1":"code","83e9bb8e":"code","b0e67109":"code","ab86240d":"code","afbdac5c":"code","3bbfe89e":"code","bb1fe9b2":"code","0f7265bd":"code","8873990c":"code","16e3799e":"code","ad18822b":"code","cbfc7726":"code","f8b24e7b":"code","1b45aafc":"markdown","96094b06":"markdown","a4caeaca":"markdown"},"source":{"cdc1526c":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","caefecfe":"df=pd.read_csv(\"..\/input\/nlp-getting-started\/train.csv\")","658a18ff":"df.head()","ca54422b":"import string\nfrom nltk.corpus import stopwords\nfrom nltk.stem.porter import PorterStemmer","2f0adcc9":"def clean_text(text):\n    s=text.split(' ')\n    s=[w.lower() for w in s]\n    table = str.maketrans('', '', string.punctuation)\n    s = [w.translate(table) for w in s]\n    s = [word for word in s if word.isalpha()]\n#     from nltk.corpus import stopwords\n    stop_words = set(stopwords.words('english'))\n    s = [w for w in s if not w in stop_words]\n    \n    porter = PorterStemmer()\n    s = [porter.stem(word) for word in s]\n    final_text=' '.join(s)\n    return final_text","f400047a":"df=df.loc[:,[\"id\",\"text\",\"target\"]]","6556a892":"df.head()","0905a5e2":"df.loc[:,\"Cleanedtext\"]=df[\"text\"].map(clean_text)","fe6926c9":"df.head()","79c62163":"df.to_csv(\"PreprocessedTweets.csv\",index=False)","14ef71b1":"df=pd.read_csv(\".\/PreprocessedTweets.csv\")","e2640428":"df.head()","a7255876":"df.isnull().sum()","1f7508d0":"print(df.shape)","6b504afc":"df=df.dropna()","54dfa332":"df.isnull().sum()","0e115ad0":"from sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.model_selection import train_test_split\nimport sklearn\nfrom sklearn.linear_model import LogisticRegression\nfrom  sklearn.naive_bayes import MultinomialNB\n","2856c86e":"tfidf=TfidfVectorizer(max_features=4000)","7fb56c95":"tfidf.fit(df[\"Cleanedtext\"])","35a82f22":"x=tfidf.transform(df[\"Cleanedtext\"])","7f035c40":"type(x),x.shape","4bd39de1":"y=df['target'].values","7e7c2e68":"X_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.10, random_state=42)","b89448b9":"import xgboost as xg\nxgc=xg.XGBClassifier()\nxgc.fit(X_train,y_train)\ny_pred=xgc.predict(X_test)\nprint(sklearn.metrics.f1_score(y_test,y_pred))","a636258b":"from sklearn.linear_model import LogisticRegression\nlr=LogisticRegression()\nlr.fit(X_train,y_train)\ny_pred=lr.predict(X_test)\nprint(sklearn.metrics.f1_score(y_test,y_pred))","eecac4fd":"from  sklearn.naive_bayes import MultinomialNB\nmb=MultinomialNB()\nmb.fit(X_train,y_train)\ny_pred=mb.predict(X_test)\nprint(sklearn.metrics.f1_score(y_test,y_pred))","2fd1e028":"# sklearn.naive_bayes.BernoulliNB\nfrom  sklearn.naive_bayes import BernoulliNB\nbb=BernoulliNB()\nbb.fit(X_train,y_train)\ny_pred=bb.predict(X_test)\nprint(sklearn.metrics.f1_score(y_test,y_pred))","5004f9ea":"from sklearn.svm import SVC\nsvc=SVC()\nsvc.fit(X_train,y_train)\ny_pred=svc.predict(X_test)\nprint(sklearn.metrics.f1_score(y_test,y_pred))","42b791f2":"from sklearn.tree import DecisionTreeClassifier\ndt=DecisionTreeClassifier()\ndt.fit(X_train,y_train)\ny_pred=dt.predict(X_test)\nprint(sklearn.metrics.f1_score(y_test,y_pred))","8a1ad28a":"final_model=MultinomialNB()\nfinal_model.fit(x,y)","ea5bfd58":"df_test=pd.read_csv(\"..\/input\/nlp-getting-started\/test.csv\")","85300033":"df_test.loc[:,\"CleanedText\"]=df_test[\"text\"].map(clean_text)","8725cd37":"df_test.head()","e388ed9a":"x_test=tfidf.transform(df_test.loc[:,\"CleanedText\"])","a33e6652":"y_pred=final_model.predict(x_test)","a4284d5a":"sub=pd.read_csv(\"..\/input\/nlp-getting-started\/sample_submission.csv\")","f218ddcb":"sub.head()","95c7a353":"sub[\"target\"]=y_pred\nsub.to_csv(\"Submmissiontfidf.csv\",index=False)","7bc9b6e3":"import string\nfrom nltk.corpus import stopwords\nfrom nltk.stem.porter import PorterStemmer","44c9aef3":"def clean_text(text):\n    s=text.split(' ')\n    s=[w.lower() for w in s]\n    table = str.maketrans('', '', string.punctuation)\n    s = [w.translate(table) for w in s]\n    s = [word for word in s if word.isalpha()]\n    final_text=' '.join(s)\n    return final_text","7b60af08":"df=pd.read_csv(\"..\/input\/nlp-getting-started\/train.csv\")","b345d242":"df.head()","75b2b6ba":"df.loc[:,\"CleanedText\"]=df[\"text\"].map(clean_text)","56cccfd9":"df.head()","7c676996":"df=df.loc[:,[\"id\",\"target\",\"CleanedText\"]]","0c3e7af7":"df.dropna()","9be20776":"df[\"target\"].nunique()","a5030999":"import tensorflow as tf\nimport matplotlib.pyplot as plt\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom sklearn.model_selection import train_test_split","97dec03c":"tokenizer=Tokenizer(oov_token='<UNk>')","52c5fcee":"tokenizer.fit_on_texts(df[\"CleanedText\"])","33625b0f":"len(tokenizer.word_index)","47b5df5b":"def get_maxlen(s):\n    m=0\n    for i in s:\n        m=max(m,len(i.split(' ')))\n    return m\nprint(get_maxlen(df[\"CleanedText\"]))","ece6eb6f":"vocab_size=len(tokenizer.word_index)+1\nmaxlen=35","af98a82c":"def padding_seq(tokenizer,seq):\n    seq=tokenizer.texts_to_sequences(seq)\n    padded_seq=pad_sequences(seq,truncating='post',padding='post',maxlen=maxlen)\n    return padded_seq","a08863f6":"X=padding_seq(tokenizer,df[\"CleanedText\"])","958810d3":"y=df[\"target\"].values","f4573ed5":"X.shape,y.shape","69c5f38b":"model=tf.keras.models.Sequential([\n                                tf.keras.layers.Embedding(vocab_size,400,input_length=maxlen),\n                                tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(100,return_sequences=True)),\n                                tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(50)),\n                                tf.keras.layers.Dropout(0.1),\n                                tf.keras.layers.Dense(2,activation='softmax')\n])\nmodel.compile(\n    loss='sparse_categorical_crossentropy',\n    optimizer='adam',\n    metrics=['accuracy']\n)","661061a9":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)","a379e83e":"h=model.fit(\n    X_train,y_train,\n    validation_data=(X_test,y_test),\n    epochs=100,\n    callbacks=[\n               tf.keras.callbacks.EarlyStopping(monitor='val_accuracy',patience=3)\n    ]\n)","d0464634":"y_pred=model.predict(X_test)\ny_pred.shape","e01b0e82":"y_pred=y_pred.argmax(axis=1)","95aa5e6b":"import sklearn","7a044423":"print(sklearn.metrics.f1_score(y_test,y_pred))","c9d7384e":"df_test=pd.read_csv(\"..\/input\/nlp-getting-started\/test.csv\")\ndf_test.loc[:,\"CleanedText\"]=df_test[\"text\"].map(clean_text)\nx_test=padding_seq(tokenizer,df_test[\"CleanedText\"])\ny_pred=model.predict(x_test)\ny_pred=y_pred.argmax(axis=1)\nsub=pd.read_csv(\"..\/input\/nlp-getting-started\/sample_submission.csv\")\nsub[\"target\"]=y_pred\nsub.to_csv(\"SubmissionLSTM.csv\",index=False)","f92f6f8a":"# !pip install tensorflow-text","4df4876c":"# import tensorflow as tf\n# import tensorflow_hub as hub\n# import tensorflow_text as text","98e197c1":"# bert_preprocess = hub.KerasLayer(\"https:\/\/tfhub.dev\/tensorflow\/bert_en_uncased_preprocess\/3\")\n# bert_encoder = hub.KerasLayer(\"https:\/\/tfhub.dev\/tensorflow\/bert_en_uncased_L-12_H-768_A-12\/4\")","83e9bb8e":"# import string\n# from nltk.corpus import stopwords\n# from nltk.stem.porter import PorterStemmer\n# def clean_text(text):\n#     s=text.split(' ')\n#     s=[w.lower() for w in s]\n#     table = str.maketrans('', '', string.punctuation)\n#     s = [w.translate(table) for w in s]\n#     s = [word for word in s if word.isalpha()]\n#     final_text=' '.join(s)\n#     return final_text","b0e67109":"# df=pd.read_csv(\"..\/input\/nlp-getting-started\/train.csv\")","ab86240d":"# df.loc[:,\"CleanedText\"]=df[\"text\"].map(clean_text)","afbdac5c":"# df.head()","3bbfe89e":"# X=df[\"CleanedText\"]\n# y=df[\"target\"]","bb1fe9b2":"# import sklearn","0f7265bd":"# X_train, X_test, y_train, y_test = sklearn.model_selection.train_test_split(X, y, test_size=0.3, random_state=42)","8873990c":"# text_input = tf.keras.layers.Input(shape=(), dtype=tf.string, name='text')\n# preprocessed_text = bert_preprocess(text_input)\n# outputs = bert_encoder(preprocessed_text)\n\n# # Neural network layers\n# l = tf.keras.layers.Dropout(0.1, name=\"dropout\")(outputs['pooled_output'])\n# l = tf.keras.layers.Dense(2, activation='softmax', name=\"output\")(l)\n\n# # Use inputs and outputs to construct a final model\n# model = tf.keras.Model(inputs=[text_input], outputs = [l])","16e3799e":"# METRICS = [\n#       tf.keras.metrics.BinaryAccuracy(name='accuracy'),\n#       tf.keras.metrics.Precision(name='precision'),\n#       tf.keras.metrics.Recall(name='recall')\n# ]\n\n# model.compile(optimizer='adam',\n#               loss='sparse_categorical_crossentropy',\n#               metrics=METRICS)","ad18822b":"# model.fit(X_train, y_train,validation_data=(X_test,y_test),epochs=10)","cbfc7726":"# y_pred=model.predict(X_test)\n# # y_pred.shape\n# y_pred=y_pred.argmax(axis=1)\n# print(sklearn.metrics.f1_score(y_test,y_pred))","f8b24e7b":"# df_test=pd.read_csv(\"..\/input\/nlp-getting-started\/test.csv\")\n# df_test.loc[:,\"CleanedText\"]=df_test[\"text\"].map(clean_text)\n# x_test=df_test[\"CleanedText\"]\n# y_pred=model.predict(x_test)\n# y_pred=y_pred.argmax(axis=1)\n# sub=pd.read_csv(\"..\/input\/nlp-getting-started\/sample_submission.csv\")\n# sub[\"target\"]=y_pred\n# sub.to_csv(\"Sub.csv\",index=False)","1b45aafc":"# Using BERT from tensorflow\n\n### it takes a lot of time so i did not runned it ,the code is correct just it takes more time\n### watch [this](https:\/\/www.youtube.com\/watch?v=hOCDJyZ6quA&t=957s) in case of any problem","96094b06":"# Using Tfidf","a4caeaca":"# using tensorflow lstms"}}