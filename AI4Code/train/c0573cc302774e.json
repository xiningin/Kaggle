{"cell_type":{"b48e03fa":"code","ad235ca6":"code","e75e3f27":"code","372b6fb6":"code","7c90324e":"code","ac900ca4":"code","a0da784c":"code","dbab2d18":"code","518390e3":"code","78b2640d":"code","a1eccd6f":"code","9ed1db76":"code","09ed74bf":"code","2fbbaea0":"code","461e2d42":"code","3ffa2212":"code","2174b647":"code","31c8cf04":"code","ebfcd952":"code","6ee6f369":"code","1fb1281d":"code","2607b381":"code","702483c6":"code","434821e8":"code","ed97f23f":"code","f0af7c5a":"code","adecb27d":"code","46451e34":"code","c5c121ca":"code","ab4a063d":"code","957ef679":"code","34c3ed8d":"code","68a539f7":"code","2b57d39e":"code","b911dd26":"code","555a2176":"code","980dcee8":"code","c8d21d5b":"code","72fcf07d":"code","e28ebcdf":"code","f6f0c489":"code","69b6bba1":"code","23986cc5":"code","5ac5f313":"code","d6ef723c":"code","b601e76b":"code","b66b8108":"code","92589ef9":"markdown","fe2c1a55":"markdown","8feb5b36":"markdown","45c8edc5":"markdown","0bf5ff92":"markdown","ce05c47f":"markdown","ea7d0795":"markdown","71ef3ad1":"markdown","3533bc1d":"markdown","5b0d6603":"markdown","352babf9":"markdown","26f0414b":"markdown","a262c882":"markdown","bda02e52":"markdown","96ee7d8c":"markdown","a322f12d":"markdown","ebc43422":"markdown","d324df01":"markdown"},"source":{"b48e03fa":"# Imports\nimport pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import cross_val_score, train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.linear_model import LinearRegression, RidgeCV, LassoCV, ElasticNetCV\nfrom sklearn.metrics import mean_squared_error, make_scorer\nfrom scipy.stats import skew\nfrom IPython.display import display\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Definitions\npd.set_option('display.float_format', lambda x: '%.3f' % x)\n%matplotlib inline\n#njobs = 4","ad235ca6":"# Get data\ntrain = pd.read_csv(\"..\/input\/train.csv\")\ntest = pd.read_csv(\"..\/input\/test.csv\")\nprint(\"train : \" + str(train.shape))","e75e3f27":"train.head()","372b6fb6":"test.head()","7c90324e":"# Check for duplicates\nidsUnique = len(set(train.Id))\nidsTotal = train.shape[0]\nidsDupli = idsTotal - idsUnique\nprint(\"There are \" + str(idsDupli) + \" duplicate IDs for \" + str(idsTotal) + \" total entries\")\n\n# Drop Id column\ntrain.drop(\"Id\", axis = 1, inplace = True)\ntest.drop(\"Id\", axis=1, inplace=True)","ac900ca4":"import seaborn as sns\n\nsns.set(rc={'figure.figsize':(11.7,8.27)})","a0da784c":"# Looking for outliers, as indicated in https:\/\/ww2.amstat.org\/publications\/jse\/v19n3\/decock.pdf\nplt.scatter(train.GrLivArea, train.SalePrice, c = \"blue\", marker = \"s\")\nplt.title(\"Looking for outliers\")\nplt.xlabel(\"GrLivArea\")\nplt.ylabel(\"SalePrice\")\nplt.show()\n\ntrain = train[train.GrLivArea < 4000]","dbab2d18":"# Log transform the target for official scoring\ntrain.SalePrice = np.log1p(train.SalePrice)\ny = train.SalePrice\ntrain = train.drop(\"SalePrice\", axis=1)","518390e3":"def prefill(train):    \n    # Handle missing values for features where median\/mean or most common value doesn't make sense\n\n    # Alley : data description says NA means \"no alley access\"\n    train.loc[:, \"Alley\"] = train.loc[:, \"Alley\"].fillna(\"None\")\n\n    # BedroomAbvGr : NA most likely means 0\n    train.loc[:, \"BedroomAbvGr\"] = train.loc[:, \"BedroomAbvGr\"].fillna(0)\n\n    # BsmtQual etc : data description says NA for basement features is \"no basement\"\n    train.loc[:, \"BsmtQual\"] = train.loc[:, \"BsmtQual\"].fillna(\"No\")\n    train.loc[:, \"BsmtCond\"] = train.loc[:, \"BsmtCond\"].fillna(\"No\")\n    train.loc[:, \"BsmtExposure\"] = train.loc[:, \"BsmtExposure\"].fillna(\"No\")\n    train.loc[:, \"BsmtFinType1\"] = train.loc[:, \"BsmtFinType1\"].fillna(\"No\")\n    train.loc[:, \"BsmtFinType2\"] = train.loc[:, \"BsmtFinType2\"].fillna(\"No\")\n    train.loc[:, \"BsmtFullBath\"] = train.loc[:, \"BsmtFullBath\"].fillna(0)\n    train.loc[:, \"BsmtHalfBath\"] = train.loc[:, \"BsmtHalfBath\"].fillna(0)\n    train.loc[:, \"BsmtUnfSF\"] = train.loc[:, \"BsmtUnfSF\"].fillna(0)\n\n    # CentralAir : NA most likely means No\n    train.loc[:, \"CentralAir\"] = train.loc[:, \"CentralAir\"].fillna(\"N\")\n    # Condition : NA most likely means Normal\n    train.loc[:, \"Condition1\"] = train.loc[:, \"Condition1\"].fillna(\"Norm\")\n    train.loc[:, \"Condition2\"] = train.loc[:, \"Condition2\"].fillna(\"Norm\")\n\n\n\n    # EnclosedPorch : NA most likely means no enclosed porch\n    train.loc[:, \"EnclosedPorch\"] = train.loc[:, \"EnclosedPorch\"].fillna(0)\n\n\n    # External stuff : NA most likely means average\n    train.loc[:, \"ExterCond\"] = train.loc[:, \"ExterCond\"].fillna(\"TA\")\n    train.loc[:, \"ExterQual\"] = train.loc[:, \"ExterQual\"].fillna(\"TA\")\n\n    # Fence : data description says NA means \"no fence\"\n    train.loc[:, \"Fence\"] = train.loc[:, \"Fence\"].fillna(\"No\")\n    # FireplaceQu : data description says NA means \"no fireplace\"\n    train.loc[:, \"FireplaceQu\"] = train.loc[:, \"FireplaceQu\"].fillna(\"No\")\n    train.loc[:, \"Fireplaces\"] = train.loc[:, \"Fireplaces\"].fillna(0)\n    # Functional : data description says NA means typical\n    train.loc[:, \"Functional\"] = train.loc[:, \"Functional\"].fillna(\"Typ\")\n    # GarageType etc : data description says NA for garage features is \"no garage\"\n    train.loc[:, \"GarageType\"] = train.loc[:, \"GarageType\"].fillna(\"No\")\n    train.loc[:, \"GarageFinish\"] = train.loc[:, \"GarageFinish\"].fillna(\"No\")\n    train.loc[:, \"GarageQual\"] = train.loc[:, \"GarageQual\"].fillna(\"No\")\n    train.loc[:, \"GarageCond\"] = train.loc[:, \"GarageCond\"].fillna(\"No\")\n    train.loc[:, \"GarageArea\"] = train.loc[:, \"GarageArea\"].fillna(0)\n    train.loc[:, \"GarageCars\"] = train.loc[:, \"GarageCars\"].fillna(0)\n    # HalfBath : NA most likely means no half baths above grade\n    train.loc[:, \"HalfBath\"] = train.loc[:, \"HalfBath\"].fillna(0)\n    # HeatingQC : NA most likely means typical\n    train.loc[:, \"HeatingQC\"] = train.loc[:, \"HeatingQC\"].fillna(\"TA\")\n    # KitchenAbvGr : NA most likely means 0\n    train.loc[:, \"KitchenAbvGr\"] = train.loc[:, \"KitchenAbvGr\"].fillna(0)\n    # KitchenQual : NA most likely means typical\n    train.loc[:, \"KitchenQual\"] = train.loc[:, \"KitchenQual\"].fillna(\"TA\")\n    # LotFrontage : NA most likely means no lot frontage\n    train.loc[:, \"LotFrontage\"] = train.loc[:, \"LotFrontage\"].fillna(0)\n    # LotShape : NA most likely means regular\n    train.loc[:, \"LotShape\"] = train.loc[:, \"LotShape\"].fillna(\"Reg\")\n    # MasVnrType : NA most likely means no veneer\n    train.loc[:, \"MasVnrType\"] = train.loc[:, \"MasVnrType\"].fillna(\"None\")\n    train.loc[:, \"MasVnrArea\"] = train.loc[:, \"MasVnrArea\"].fillna(0)\n    # MiscFeature : data description says NA means \"no misc feature\"\n    train.loc[:, \"MiscFeature\"] = train.loc[:, \"MiscFeature\"].fillna(\"No\")\n    train.loc[:, \"MiscVal\"] = train.loc[:, \"MiscVal\"].fillna(0)\n    # OpenPorchSF : NA most likely means no open porch\n    train.loc[:, \"OpenPorchSF\"] = train.loc[:, \"OpenPorchSF\"].fillna(0)\n    # PavedDrive : NA most likely means not paved\n    train.loc[:, \"PavedDrive\"] = train.loc[:, \"PavedDrive\"].fillna(\"N\")\n    # PoolQC : data description says NA means \"no pool\"\n    train.loc[:, \"PoolQC\"] = train.loc[:, \"PoolQC\"].fillna(\"No\")\n    train.loc[:, \"PoolArea\"] = train.loc[:, \"PoolArea\"].fillna(0)\n    # SaleCondition : NA most likely means normal sale\n    train.loc[:, \"SaleCondition\"] = train.loc[:, \"SaleCondition\"].fillna(\"Normal\")\n    # ScreenPorch : NA most likely means no screen porch\n    train.loc[:, \"ScreenPorch\"] = train.loc[:, \"ScreenPorch\"].fillna(0)\n    # TotRmsAbvGrd : NA most likely means 0\n    train.loc[:, \"TotRmsAbvGrd\"] = train.loc[:, \"TotRmsAbvGrd\"].fillna(0)\n    # Utilities : NA most likely means all public utilities\n    train.loc[:, \"Utilities\"] = train.loc[:, \"Utilities\"].fillna(\"AllPub\")\n    # WoodDeckSF : NA most likely means no wood deck\n    train.loc[:, \"WoodDeckSF\"] = train.loc[:, \"WoodDeckSF\"].fillna(0)\n    \n    return train","78b2640d":"train = prefill(train)\ntrain.shape","a1eccd6f":"test = prefill(test)\ntest.shape","9ed1db76":"def num_to_cats(train):\n    # Some numerical features are actually really categories\n    train = train.replace({\"MSSubClass\" : {20 : \"SC20\", 30 : \"SC30\", 40 : \"SC40\", 45 : \"SC45\", \n                                           50 : \"SC50\", 60 : \"SC60\", 70 : \"SC70\", 75 : \"SC75\", \n                                           80 : \"SC80\", 85 : \"SC85\", 90 : \"SC90\", 120 : \"SC120\", \n                                           150 : \"SC150\", 160 : \"SC160\", 180 : \"SC180\", 190 : \"SC190\"},\n                           \"MoSold\" : {1 : \"Jan\", 2 : \"Feb\", 3 : \"Mar\", 4 : \"Apr\", 5 : \"May\", 6 : \"Jun\",\n                                       7 : \"Jul\", 8 : \"Aug\", 9 : \"Sep\", 10 : \"Oct\", 11 : \"Nov\", 12 : \"Dec\"}\n                          })\n    return train","09ed74bf":"train = num_to_cats(train)\ntest = num_to_cats(test)\ntrain.shape","2fbbaea0":"test.shape","461e2d42":"def cats_to_num(train):\n    # Encode some categorical features as ordered numbers when there is information in the order\n    train = train.replace({\"Alley\" : {\"Grvl\" : 1, \"Pave\" : 2},\n                           \"BsmtCond\" : {\"No\" : 0, \"Po\" : 1, \"Fa\" : 2, \"TA\" : 3, \"Gd\" : 4, \"Ex\" : 5},\n                           \"BsmtExposure\" : {\"No\" : 0, \"Mn\" : 1, \"Av\": 2, \"Gd\" : 3},\n                           \"BsmtFinType1\" : {\"No\" : 0, \"Unf\" : 1, \"LwQ\": 2, \"Rec\" : 3, \"BLQ\" : 4, \n                                             \"ALQ\" : 5, \"GLQ\" : 6},\n                           \"BsmtFinType2\" : {\"No\" : 0, \"Unf\" : 1, \"LwQ\": 2, \"Rec\" : 3, \"BLQ\" : 4, \n                                             \"ALQ\" : 5, \"GLQ\" : 6},\n                           \"BsmtQual\" : {\"No\" : 0, \"Po\" : 1, \"Fa\" : 2, \"TA\": 3, \"Gd\" : 4, \"Ex\" : 5},\n                           \"ExterCond\" : {\"Po\" : 1, \"Fa\" : 2, \"TA\": 3, \"Gd\": 4, \"Ex\" : 5},\n                           \"ExterQual\" : {\"Po\" : 1, \"Fa\" : 2, \"TA\": 3, \"Gd\": 4, \"Ex\" : 5},\n                           \"FireplaceQu\" : {\"No\" : 0, \"Po\" : 1, \"Fa\" : 2, \"TA\" : 3, \"Gd\" : 4, \"Ex\" : 5},\n                           \"Functional\" : {\"Sal\" : 1, \"Sev\" : 2, \"Maj2\" : 3, \"Maj1\" : 4, \"Mod\": 5, \n                                           \"Min2\" : 6, \"Min1\" : 7, \"Typ\" : 8},\n                           \"GarageCond\" : {\"No\" : 0, \"Po\" : 1, \"Fa\" : 2, \"TA\" : 3, \"Gd\" : 4, \"Ex\" : 5},\n                           \"GarageQual\" : {\"No\" : 0, \"Po\" : 1, \"Fa\" : 2, \"TA\" : 3, \"Gd\" : 4, \"Ex\" : 5},\n                           \"HeatingQC\" : {\"Po\" : 1, \"Fa\" : 2, \"TA\" : 3, \"Gd\" : 4, \"Ex\" : 5},\n                           \"KitchenQual\" : {\"Po\" : 1, \"Fa\" : 2, \"TA\" : 3, \"Gd\" : 4, \"Ex\" : 5},\n                           \"LandSlope\" : {\"Sev\" : 1, \"Mod\" : 2, \"Gtl\" : 3},\n                           \"LotShape\" : {\"IR3\" : 1, \"IR2\" : 2, \"IR1\" : 3, \"Reg\" : 4},\n                           \"PavedDrive\" : {\"N\" : 0, \"P\" : 1, \"Y\" : 2},\n                           \"PoolQC\" : {\"No\" : 0, \"Fa\" : 1, \"TA\" : 2, \"Gd\" : 3, \"Ex\" : 4},\n                           \"Street\" : {\"Grvl\" : 1, \"Pave\" : 2},\n                           \"Utilities\" : {\"ELO\" : 1, \"NoSeWa\" : 2, \"NoSewr\" : 3, \"AllPub\" : 4}}\n                         )\n    return train","3ffa2212":"train = cats_to_num(train)\ntest = cats_to_num(test)\ntrain.shape[1] == test.shape[1]","2174b647":"def create_new_feat(train):\n    # Create new features\n    # 1* Simplifications of existing features\n    train[\"SimplOverallQual\"] = train.OverallQual.replace({1 : 1, 2 : 1, 3 : 1, # bad\n                                                           4 : 2, 5 : 2, 6 : 2, # average\n                                                           7 : 3, 8 : 3, 9 : 3, 10 : 3 # good\n                                                          })\n    train[\"SimplOverallCond\"] = train.OverallCond.replace({1 : 1, 2 : 1, 3 : 1, # bad\n                                                           4 : 2, 5 : 2, 6 : 2, # average\n                                                           7 : 3, 8 : 3, 9 : 3, 10 : 3 # good\n                                                          })\n    train[\"SimplPoolQC\"] = train.PoolQC.replace({1 : 1, 2 : 1, # average\n                                                 3 : 2, 4 : 2 # good\n                                                })\n    train[\"SimplGarageCond\"] = train.GarageCond.replace({1 : 1, # bad\n                                                         2 : 1, 3 : 1, # average\n                                                         4 : 2, 5 : 2 # good\n                                                        })\n    train[\"SimplGarageQual\"] = train.GarageQual.replace({1 : 1, # bad\n                                                         2 : 1, 3 : 1, # average\n                                                         4 : 2, 5 : 2 # good\n                                                        })\n    train[\"SimplFireplaceQu\"] = train.FireplaceQu.replace({1 : 1, # bad\n                                                           2 : 1, 3 : 1, # average\n                                                           4 : 2, 5 : 2 # good\n                                                          })\n    train[\"SimplFireplaceQu\"] = train.FireplaceQu.replace({1 : 1, # bad\n                                                           2 : 1, 3 : 1, # average\n                                                           4 : 2, 5 : 2 # good\n                                                          })\n    train[\"SimplFunctional\"] = train.Functional.replace({1 : 1, 2 : 1, # bad\n                                                         3 : 2, 4 : 2, # major\n                                                         5 : 3, 6 : 3, 7 : 3, # minor\n                                                         8 : 4 # typical\n                                                        })\n    train[\"SimplKitchenQual\"] = train.KitchenQual.replace({1 : 1, # bad\n                                                           2 : 1, 3 : 1, # average\n                                                           4 : 2, 5 : 2 # good\n                                                          })\n    train[\"SimplHeatingQC\"] = train.HeatingQC.replace({1 : 1, # bad\n                                                       2 : 1, 3 : 1, # average\n                                                       4 : 2, 5 : 2 # good\n                                                      })\n    train[\"SimplBsmtFinType1\"] = train.BsmtFinType1.replace({1 : 1, # unfinished\n                                                             2 : 1, 3 : 1, # rec room\n                                                             4 : 2, 5 : 2, 6 : 2 # living quarters\n                                                            })\n    train[\"SimplBsmtFinType2\"] = train.BsmtFinType2.replace({1 : 1, # unfinished\n                                                             2 : 1, 3 : 1, # rec room\n                                                             4 : 2, 5 : 2, 6 : 2 # living quarters\n                                                            })\n    train[\"SimplBsmtCond\"] = train.BsmtCond.replace({1 : 1, # bad\n                                                     2 : 1, 3 : 1, # average\n                                                     4 : 2, 5 : 2 # good\n                                                    })\n    train[\"SimplBsmtQual\"] = train.BsmtQual.replace({1 : 1, # bad\n                                                     2 : 1, 3 : 1, # average\n                                                     4 : 2, 5 : 2 # good\n                                                    })\n    train[\"SimplExterCond\"] = train.ExterCond.replace({1 : 1, # bad\n                                                       2 : 1, 3 : 1, # average\n                                                       4 : 2, 5 : 2 # good\n                                                      })\n    train[\"SimplExterQual\"] = train.ExterQual.replace({1 : 1, # bad\n                                                       2 : 1, 3 : 1, # average\n                                                       4 : 2, 5 : 2 # good\n                                                      })\n\n    # 2* Combinations of existing features\n    # Overall quality of the house\n    train[\"OverallGrade\"] = train[\"OverallQual\"] * train[\"OverallCond\"]\n    # Overall quality of the garage\n    train[\"GarageGrade\"] = train[\"GarageQual\"] * train[\"GarageCond\"]\n    # Overall quality of the exterior\n    train[\"ExterGrade\"] = train[\"ExterQual\"] * train[\"ExterCond\"]\n    # Overall kitchen score\n    train[\"KitchenScore\"] = train[\"KitchenAbvGr\"] * train[\"KitchenQual\"]\n    # Overall fireplace score\n    train[\"FireplaceScore\"] = train[\"Fireplaces\"] * train[\"FireplaceQu\"]\n    # Overall garage score\n    train[\"GarageScore\"] = train[\"GarageArea\"] * train[\"GarageQual\"]\n    # Overall pool score\n    train[\"PoolScore\"] = train[\"PoolArea\"] * train[\"PoolQC\"]\n    # Simplified overall quality of the house\n    train[\"SimplOverallGrade\"] = train[\"SimplOverallQual\"] * train[\"SimplOverallCond\"]\n    # Simplified overall quality of the exterior\n    train[\"SimplExterGrade\"] = train[\"SimplExterQual\"] * train[\"SimplExterCond\"]\n    # Simplified overall pool score\n    train[\"SimplPoolScore\"] = train[\"PoolArea\"] * train[\"SimplPoolQC\"]\n    # Simplified overall garage score\n    train[\"SimplGarageScore\"] = train[\"GarageArea\"] * train[\"SimplGarageQual\"]\n    # Simplified overall fireplace score\n    train[\"SimplFireplaceScore\"] = train[\"Fireplaces\"] * train[\"SimplFireplaceQu\"]\n    # Simplified overall kitchen score\n    train[\"SimplKitchenScore\"] = train[\"KitchenAbvGr\"] * train[\"SimplKitchenQual\"]\n    # Total number of bathrooms\n    train[\"TotalBath\"] = train[\"BsmtFullBath\"] + (0.5 * train[\"BsmtHalfBath\"]) + \\\n    train[\"FullBath\"] + (0.5 * train[\"HalfBath\"])\n    # Total SF for house (incl. basement)\n    train[\"AllSF\"] = train[\"GrLivArea\"] + train[\"TotalBsmtSF\"]\n    # Total SF for 1st + 2nd floors\n    train[\"AllFlrsSF\"] = train[\"1stFlrSF\"] + train[\"2ndFlrSF\"]\n    # Total SF for porch\n    train[\"AllPorchSF\"] = train[\"OpenPorchSF\"] + train[\"EnclosedPorch\"] + \\\n    train[\"3SsnPorch\"] + train[\"ScreenPorch\"]\n    # Has masonry veneer or not\n    train[\"HasMasVnr\"] = train.MasVnrType.replace({\"BrkCmn\" : 1, \"BrkFace\" : 1, \"CBlock\" : 1, \n                                                   \"Stone\" : 1, \"None\" : 0})\n    # House completed before sale or not\n    train[\"BoughtOffPlan\"] = train.SaleCondition.replace({\"Abnorml\" : 0, \"Alloca\" : 0, \"AdjLand\" : 0, \n                                                          \"Family\" : 0, \"Normal\" : 0, \"Partial\" : 1})\n    return train","31c8cf04":"train = create_new_feat(train)\ntest = create_new_feat(test)\ntrain.shape[1] == test.shape[1]","ebfcd952":"def create_poly(train):\n\n    # Create new features\n    # 3* Polynomials on the top 10 existing features\n    train[\"OverallQual-s2\"] = train[\"OverallQual\"] ** 2\n    train[\"OverallQual-s3\"] = train[\"OverallQual\"] ** 3\n    train[\"OverallQual-Sq\"] = np.sqrt(train[\"OverallQual\"])\n    train[\"AllSF-2\"] = train[\"AllSF\"] ** 2\n    train[\"AllSF-3\"] = train[\"AllSF\"] ** 3\n    train[\"AllSF-Sq\"] = np.sqrt(train[\"AllSF\"])\n    train[\"AllFlrsSF-2\"] = train[\"AllFlrsSF\"] ** 2\n    train[\"AllFlrsSF-3\"] = train[\"AllFlrsSF\"] ** 3\n    train[\"AllFlrsSF-Sq\"] = np.sqrt(train[\"AllFlrsSF\"])\n    train[\"GrLivArea-2\"] = train[\"GrLivArea\"] ** 2\n    train[\"GrLivArea-3\"] = train[\"GrLivArea\"] ** 3\n    train[\"GrLivArea-Sq\"] = np.sqrt(train[\"GrLivArea\"])\n    train[\"SimplOverallQual-s2\"] = train[\"SimplOverallQual\"] ** 2\n    train[\"SimplOverallQual-s3\"] = train[\"SimplOverallQual\"] ** 3\n    train[\"SimplOverallQual-Sq\"] = np.sqrt(train[\"SimplOverallQual\"])\n    train[\"ExterQual-2\"] = train[\"ExterQual\"] ** 2\n    train[\"ExterQual-3\"] = train[\"ExterQual\"] ** 3\n    train[\"ExterQual-Sq\"] = np.sqrt(train[\"ExterQual\"])\n    train[\"GarageCars-2\"] = train[\"GarageCars\"] ** 2\n    train[\"GarageCars-3\"] = train[\"GarageCars\"] ** 3\n    train[\"GarageCars-Sq\"] = np.sqrt(train[\"GarageCars\"])\n    train[\"TotalBath-2\"] = train[\"TotalBath\"] ** 2\n    train[\"TotalBath-3\"] = train[\"TotalBath\"] ** 3\n    train[\"TotalBath-Sq\"] = np.sqrt(train[\"TotalBath\"])\n    train[\"KitchenQual-2\"] = train[\"KitchenQual\"] ** 2\n    train[\"KitchenQual-3\"] = train[\"KitchenQual\"] ** 3\n    train[\"KitchenQual-Sq\"] = np.sqrt(train[\"KitchenQual\"])\n    train[\"GarageScore-2\"] = train[\"GarageScore\"] ** 2\n    train[\"GarageScore-3\"] = train[\"GarageScore\"] ** 3\n    train[\"GarageScore-Sq\"] = np.sqrt(train[\"GarageScore\"])\n    \n    return train","6ee6f369":"train = create_poly(train)\ntest = create_poly(test)\ntrain.shape[1] == test.shape[1]","1fb1281d":"# Differentiate numerical features (minus the target) and categorical features\n'''categorical_features = train.select_dtypes(include = [\"object\"]).columns\nnumerical_features = train.select_dtypes(exclude = [\"object\"]).columns\nnumerical_features = numerical_features.drop(\"SalePrice\")\nprint(\"Numerical features : \" + str(len(numerical_features)))\nprint(\"Categorical features : \" + str(len(categorical_features)))\ntrain_num = train[numerical_features]\ntrain_cat = train[categorical_features]\n'''","2607b381":"test.columns == train.columns","702483c6":"categorical_features = train.select_dtypes(include = [\"object\"]).columns\nnumerical_features = train.select_dtypes(exclude = [\"object\"]).columns\ndummies = pd.get_dummies(train[categorical_features]).columns.to_list()\ndummies2 = pd.get_dummies(test[categorical_features]).columns.to_list()\n\ncolumns = dummies\nfor col in dummies2:\n    if col not in columns:\n        columns.append(col)","434821e8":"def pre_numerical_and_dummies(df):\n    #categorical_features = df.select_dtypes(include = [\"object\"]).columns\n    #numerical_features = df.select_dtypes(exclude = [\"object\"]).columns\n    train_num = df[numerical_features]\n    print(len(numerical_features))\n    print(len(categorical_features))\n    train_cat = df[categorical_features]\n    train_num = train_num.fillna(train_num.median())\n    skewness = train_num.apply(lambda x: skew(x))\n    skewness = skewness[abs(skewness) > 0.5]\n    skewed_features = skewness.index\n    train_num[skewed_features] = np.log1p(train_num[skewed_features])\n    train_cat = pd.get_dummies(train_cat)\n    for col in columns:\n        if col not in train_cat.columns:\n            train_cat[col] = 0\n    train_cat.columns= dummies\n    print(train_cat.shape)\n    df = pd.concat([train_num, train_cat], axis = 1)\n    return df\n        ","ed97f23f":"train_copy = train.copy()\ntest_copy = test.copy()","f0af7c5a":"train_cat = train[categorical_features]","adecb27d":"test_cat = test[categorical_features]","46451e34":"pd.get_dummies(train_cat).columns","c5c121ca":"train = pre_numerical_and_dummies(train)\n","ab4a063d":"test = pre_numerical_and_dummies(test)","957ef679":"train.shape[1] == test.shape[1]","34c3ed8d":"X_train, X_test, y_train, y_test = train_test_split(train, y, test_size = 0.3, random_state = 0)\nprint(\"X_train : \" + str(X_train.shape))\nprint(\"X_test : \" + str(X_test.shape))\nprint(\"y_train : \" + str(y_train.shape))\nprint(\"y_test : \" + str(y_test.shape))","68a539f7":"# Standardize numerical features\nstdSc = StandardScaler()\nX_train.loc[:, numerical_features] = stdSc.fit_transform(X_train.loc[:, numerical_features])\nX_test.loc[:, numerical_features] = stdSc.transform(X_test.loc[:, numerical_features])\n\ntest.loc[:, numerical_features] = stdSc.transform(test.loc[:, numerical_features])","2b57d39e":"# Define error measure for official scoring : RMSE\nscorer = make_scorer(mean_squared_error, greater_is_better = False)\n\ndef rmse_cv_train(model):\n    rmse= np.sqrt(-cross_val_score(model, X_train, y_train, scoring = scorer, cv = 10))\n    return(rmse)\n\ndef rmse_cv_test(model):\n    rmse= np.sqrt(-cross_val_score(model, X_test, y_test, scoring = scorer, cv = 10))\n    return(rmse)","b911dd26":"train[train.isnull().values]","555a2176":"# Linear Regression\nlr = LinearRegression()\nlr.fit(X_train, y_train)\n\n# Look at predictions on training and validation set\nprint(\"RMSE on Training set :\", rmse_cv_train(lr).mean())\nprint(\"RMSE on Test set :\", rmse_cv_test(lr).mean())\ny_train_pred = lr.predict(X_train)\ny_test_pred = lr.predict(X_test)\n\n# Plot residuals\nplt.scatter(y_train_pred, y_train_pred - y_train, c = \"blue\", marker = \"s\", label = \"Training data\")\nplt.scatter(y_test_pred, y_test_pred - y_test, c = \"lightgreen\", marker = \"s\", label = \"Validation data\")\nplt.title(\"Linear regression\")\nplt.xlabel(\"Predicted values\")\nplt.ylabel(\"Residuals\")\nplt.legend(loc = \"upper left\")\nplt.hlines(y = 0, xmin = 10.5, xmax = 13.5, color = \"red\")\nplt.show()\n\n# Plot predictions\nplt.scatter(y_train_pred, y_train, c = \"blue\", marker = \"s\", label = \"Training data\")\nplt.scatter(y_test_pred, y_test, c = \"lightgreen\", marker = \"s\", label = \"Validation data\")\nplt.title(\"Linear regression\")\nplt.xlabel(\"Predicted values\")\nplt.ylabel(\"Real values\")\nplt.legend(loc = \"upper left\")\nplt.plot([10.5, 13.5], [10.5, 13.5], c = \"red\")\nplt.show()","980dcee8":"test.shape","c8d21d5b":"train.shape","72fcf07d":"samplesub = pd.read_csv(\"..\/input\/sample_submission.csv\")","e28ebcdf":"samplesub","f6f0c489":"lrf = LinearRegression()\nlrf.fit(train, y)\npredictions = pd.DataFrame(pd.read_csv(\"..\/input\/test.csv\")[\"Id\"])\npredictions[\"SalePrice\"] = lrf.predict(test)\npredictions.to_csv(\"linear.csv\", index=False)","69b6bba1":"# 2* Ridge\nridge = RidgeCV(alphas = [0.01, 0.03, 0.06, 0.1, 0.3, 0.6, 1, 3, 6, 10, 30, 60])\nridge.fit(X_train, y_train)\nalpha = ridge.alpha_\nprint(\"Best alpha :\", alpha)\n\nprint(\"Try again for more precision with alphas centered around \" + str(alpha))\nridge = RidgeCV(alphas = [alpha * .6, alpha * .65, alpha * .7, alpha * .75, alpha * .8, alpha * .85, \n                          alpha * .9, alpha * .95, alpha, alpha * 1.05, alpha * 1.1, alpha * 1.15,\n                          alpha * 1.25, alpha * 1.3, alpha * 1.35, alpha * 1.4], \n                cv = 10)\nridge.fit(X_train, y_train)\nbest_alpha = ridge.alpha_\nprint(\"Best alpha :\", best_alpha)\n\nprint(\"Ridge RMSE on Training set :\", rmse_cv_train(ridge).mean())\nprint(\"Ridge RMSE on Test set :\", rmse_cv_test(ridge).mean())\ny_train_rdg = ridge.predict(X_train)\ny_test_rdg = ridge.predict(X_test)\n\n# Plot residuals\nplt.scatter(y_train_rdg, y_train_rdg - y_train, c = \"blue\", marker = \"s\", label = \"Training data\")\nplt.scatter(y_test_rdg, y_test_rdg - y_test, c = \"lightgreen\", marker = \"s\", label = \"Validation data\")\nplt.title(\"Linear regression with Ridge regularization\")\nplt.xlabel(\"Predicted values\")\nplt.ylabel(\"Residuals\")\nplt.legend(loc = \"upper left\")\nplt.hlines(y = 0, xmin = 10.5, xmax = 13.5, color = \"red\")\nplt.show()\n\n# Plot predictions\nplt.scatter(y_train_rdg, y_train, c = \"blue\", marker = \"s\", label = \"Training data\")\nplt.scatter(y_test_rdg, y_test, c = \"lightgreen\", marker = \"s\", label = \"Validation data\")\nplt.title(\"Linear regression with Ridge regularization\")\nplt.xlabel(\"Predicted values\")\nplt.ylabel(\"Real values\")\nplt.legend(loc = \"upper left\")\nplt.plot([10.5, 13.5], [10.5, 13.5], c = \"red\")\nplt.show()\n\n# Plot important coefficients\ncoefs = pd.Series(ridge.coef_, index = X_train.columns)\nprint(\"Ridge picked \" + str(sum(coefs != 0)) + \" features and eliminated the other \" +  \\\n      str(sum(coefs == 0)) + \" features\")\nimp_coefs = pd.concat([coefs.sort_values().head(10),\n                     coefs.sort_values().tail(10)])\nimp_coefs.plot(kind = \"barh\")\nplt.title(\"Coefficients in the Ridge Model\")\nplt.show()","23986cc5":"from sklearn.linear_model import Ridge\n\nridgef = Ridge(alpha=best_alpha)\nridgef.fit(train, y)\npredictions = pd.DataFrame(pd.read_csv(\"..\/input\/test.csv\")[\"Id\"])\npredictions[\"SalePrice\"] = ridgef.predict(test)\npredictions.to_csv(\"ridge.csv\", index=False)","5ac5f313":"# 3* Lasso\nlasso = LassoCV(alphas = [0.0001, 0.0003, 0.0006, 0.001, 0.003, 0.006, 0.01, 0.03, 0.06, 0.1, \n                          0.3, 0.6, 1], \n                max_iter = 50000, cv = 10)\nlasso.fit(X_train, y_train)\nbest_alpha = lasso.alpha_\nalpha = best_alpha\nprint(\"Best alpha :\", best_alpha)\n\nprint(\"Try again for more precision with alphas centered around \" + str(alpha))\nlasso = LassoCV(alphas = [alpha * .6, alpha * .65, alpha * .7, alpha * .75, alpha * .8, \n                          alpha * .85, alpha * .9, alpha * .95, alpha, alpha * 1.05, \n                          alpha * 1.1, alpha * 1.15, alpha * 1.25, alpha * 1.3, alpha * 1.35, \n                          alpha * 1.4], \n                max_iter = 50000, cv = 10)\nlasso.fit(X_train, y_train)\nbest_alpha = lasso.alpha_\nprint(\"Best alpha :\", best_alpha)\n\nprint(\"Lasso RMSE on Training set :\", rmse_cv_train(lasso).mean())\nprint(\"Lasso RMSE on Test set :\", rmse_cv_test(lasso).mean())\ny_train_las = lasso.predict(X_train)\ny_test_las = lasso.predict(X_test)\n\n# Plot residuals\nplt.scatter(y_train_las, y_train_las - y_train, c = \"blue\", marker = \"s\", label = \"Training data\")\nplt.scatter(y_test_las, y_test_las - y_test, c = \"lightgreen\", marker = \"s\", label = \"Validation data\")\nplt.title(\"Linear regression with Lasso regularization\")\nplt.xlabel(\"Predicted values\")\nplt.ylabel(\"Residuals\")\nplt.legend(loc = \"upper left\")\nplt.hlines(y = 0, xmin = 10.5, xmax = 13.5, color = \"red\")\nplt.show()\n\n# Plot predictions\nplt.scatter(y_train_las, y_train, c = \"blue\", marker = \"s\", label = \"Training data\")\nplt.scatter(y_test_las, y_test, c = \"lightgreen\", marker = \"s\", label = \"Validation data\")\nplt.title(\"Linear regression with Lasso regularization\")\nplt.xlabel(\"Predicted values\")\nplt.ylabel(\"Real values\")\nplt.legend(loc = \"upper left\")\nplt.plot([10.5, 13.5], [10.5, 13.5], c = \"red\")\nplt.show()\n\n# Plot important coefficients\ncoefs = pd.Series(lasso.coef_, index = X_train.columns)\nprint(\"Lasso picked \" + str(sum(coefs != 0)) + \" features and eliminated the other \" +  \\\n      str(sum(coefs == 0)) + \" features\")\nimp_coefs = pd.concat([coefs.sort_values().head(10),\n                     coefs.sort_values().tail(10)])\nimp_coefs.plot(kind = \"barh\")\nplt.title(\"Coefficients in the Lasso Model\")\nplt.show()","d6ef723c":"from sklearn.linear_model import Lasso\n\nlassof = Lasso(alpha=best_alpha)\nlassof.fit(train, y)\npredictions = pd.DataFrame(pd.read_csv(\"..\/input\/test.csv\")[\"Id\"])\npredictions[\"SalePrice\"] = lassof.predict(test)\npredictions.to_csv(\"lasso.csv\", index=False)","b601e76b":"# 4* ElasticNet\nelasticNet = ElasticNetCV(l1_ratio = [0.1, 0.3, 0.5, 0.6, 0.7, 0.8, 0.85, 0.9, 0.95, 1],\n                          alphas = [0.0001, 0.0003, 0.0006, 0.001, 0.003, 0.006, \n                                    0.01, 0.03, 0.06, 0.1, 0.3, 0.6, 1, 3, 6], \n                          max_iter = 50000, cv = 10)\nelasticNet.fit(X_train, y_train)\nalpha = elasticNet.alpha_\nratio = elasticNet.l1_ratio_\nprint(\"Best l1_ratio :\", ratio)\nprint(\"Best alpha :\", alpha )\n\nprint(\"Try again for more precision with l1_ratio centered around \" + str(ratio))\nelasticNet = ElasticNetCV(l1_ratio = [ratio * .85, ratio * .9, ratio * .95, ratio, ratio * 1.05, ratio * 1.1, ratio * 1.15],\n                          alphas = [0.0001, 0.0003, 0.0006, 0.001, 0.003, 0.006, 0.01, 0.03, 0.06, 0.1, 0.3, 0.6, 1, 3, 6], \n                          max_iter = 50000, cv = 10)\nelasticNet.fit(X_train, y_train)\nif (elasticNet.l1_ratio_ > 1):\n    elasticNet.l1_ratio_ = 1    \nalpha = elasticNet.alpha_\nratio = elasticNet.l1_ratio_\nprint(\"Best l1_ratio :\", ratio)\nprint(\"Best alpha :\", alpha )\n\nprint(\"Now try again for more precision on alpha, with l1_ratio fixed at \" + str(ratio) + \n      \" and alpha centered around \" + str(alpha))\nelasticNet = ElasticNetCV(l1_ratio = ratio,\n                          alphas = [alpha * .6, alpha * .65, alpha * .7, alpha * .75, alpha * .8, alpha * .85, alpha * .9, \n                                    alpha * .95, alpha, alpha * 1.05, alpha * 1.1, alpha * 1.15, alpha * 1.25, alpha * 1.3, \n                                    alpha * 1.35, alpha * 1.4], \n                          max_iter = 50000, cv = 10)\nelasticNet.fit(X_train, y_train)\nif (elasticNet.l1_ratio_ > 1):\n    elasticNet.l1_ratio_ = 1    \nbest_alpha = elasticNet.alpha_\nbest_ratio = elasticNet.l1_ratio_\nprint(\"Best l1_ratio :\", best_ratio)\nprint(\"Best alpha :\", best_alpha )\n\nprint(\"ElasticNet RMSE on Training set :\", rmse_cv_train(elasticNet).mean())\nprint(\"ElasticNet RMSE on Test set :\", rmse_cv_test(elasticNet).mean())\ny_train_ela = elasticNet.predict(X_train)\ny_test_ela = elasticNet.predict(X_test)\n\n# Plot residuals\nplt.scatter(y_train_ela, y_train_ela - y_train, c = \"blue\", marker = \"s\", label = \"Training data\")\nplt.scatter(y_test_ela, y_test_ela - y_test, c = \"lightgreen\", marker = \"s\", label = \"Validation data\")\nplt.title(\"Linear regression with ElasticNet regularization\")\nplt.xlabel(\"Predicted values\")\nplt.ylabel(\"Residuals\")\nplt.legend(loc = \"upper left\")\nplt.hlines(y = 0, xmin = 10.5, xmax = 13.5, color = \"red\")\nplt.show()\n\n# Plot predictions\nplt.scatter(y_train, y_train_ela, c = \"blue\", marker = \"s\", label = \"Training data\")\nplt.scatter(y_test, y_test_ela, c = \"lightgreen\", marker = \"s\", label = \"Validation data\")\nplt.title(\"Linear regression with ElasticNet regularization\")\nplt.xlabel(\"Predicted values\")\nplt.ylabel(\"Real values\")\nplt.legend(loc = \"upper left\")\nplt.plot([10.5, 13.5], [10.5, 13.5], c = \"red\")\nplt.show()\n\n# Plot important coefficients\ncoefs = pd.Series(elasticNet.coef_, index = X_train.columns)\nprint(\"ElasticNet picked \" + str(sum(coefs != 0)) + \" features and eliminated the other \" +  str(sum(coefs == 0)) + \" features\")\nimp_coefs = pd.concat([coefs.sort_values().head(10),\n                     coefs.sort_values().tail(10)])\nimp_coefs.plot(kind = \"barh\")\nplt.title(\"Coefficients in the ElasticNet Model\")\nplt.show()","b66b8108":"from sklearn.linear_model import ElasticNet\n\nen = ElasticNet(alpha=best_alpha, l1_ratio = best_ratio)\nen.fit(train, y)\npredictions = pd.DataFrame(pd.read_csv(\"..\/input\/test.csv\")[\"Id\"])\npredictions[\"SalePrice\"] =en.predict(test) \npredictions.to_csv(\"en.csv\", index=False)","92589ef9":"The optimal L1 ratio used by ElasticNet here is equal to 1, which means it is exactly equal to the Lasso regressor we used earlier (and had it been equal to 0, it would have been exactly equal to our Ridge regressor). The model didn't need any L2 regularization to overcome any potential L1 shortcoming.","fe2c1a55":"We're getting a much better RMSE result now that we've added regularization. The very small difference between training and test results indicate that we eliminated most of the overfitting. Visually, the graphs seem to confirm that idea.  \n\nRidge used almost all of the existing features.","8feb5b36":"RMSE results are better both on training and test sets. The most interesting thing is that Lasso used only one third of the available features. Another interesting tidbit : it seems to give big weights to Neighborhood categories, both in positive and negative ways. Intuitively it makes sense, house prices change a whole lot from one neighborhood to another in the same city.  \n\nThe \"MSZoning_C (all)\" feature seems to have a disproportionate impact compared to the others. It is defined as *general zoning classification : commercial*. It seems a bit weird to me that having your house in a mostly commercial zone would be such a terrible thing.","45c8edc5":"**3* Linear Regression with Lasso regularization (L1 penalty)**\n\nLASSO stands for *Least Absolute Shrinkage and Selection Operator*. It is an alternative regularization method, where we simply replace the square of the weights by the sum of the absolute value of the weights. In contrast to L2 regularization, L1 regularization yields sparse feature vectors : most feature weights will be zero. Sparsity can be useful in practice if we have a high dimensional dataset with many features that are irrelevant.  \n\nWe can suspect that it should be more efficient than Ridge here.","0bf5ff92":"Standardization cannot be done before  the partitioning, as we don't want to fit the StandardScaler on some observations that will later be used in the test set.","ce05c47f":"There seems to be 2 extreme outliers on the bottom right, really large houses that sold for really cheap. More generally, the author of the dataset recommends removing 'any houses with more than 4000 square feet' from the dataset.  \nReference : https:\/\/ww2.amstat.org\/publications\/jse\/v19n3\/decock.pdf","ea7d0795":"RMSE on Training set shows up weird here (not when I run it on my computer) for some reason.  \nErrors seem randomly distributed and randomly scattered around the centerline, so there is that at least. It means our model was able to capture most of the explanatory information.","71ef3ad1":"**4* Linear Regression with ElasticNet regularization (L1 and L2 penalty)**\n\nElasticNet is a compromise between Ridge and Lasso regression. It has a L1 penalty to generate sparsity and a L2 penalty to overcome some of the limitations of Lasso, such as the number of variables (Lasso can't select more features than it has observations, but it's not the case here anyway).","3533bc1d":"**Conclusion**  \n\nPutting time and effort into preparing the dataset and optimizing the regularization resulted in a decent score, better than some public scripts which use algorithms that historically perform better in Kaggle contests, like Random Forests. Being fairly new to the world of machine learning contests, I will appreciate any constructive pointer to improve, and I thank you for your time.","5b0d6603":"**Introduction**\n\nThis kernel is an attempt to use every trick in the books to unleash the full power of Linear Regression, including a lot of preprocessing and a look at several Regularization algorithms.\n\nAt the time of writing, it achieves a score of about 0.121 on the public LB, just using regression, no RF, no xgboost, no ensembling etc. All comments\/corrections are more than welcome.","352babf9":"Note : I'm not getting nearly the same numbers in local CV compared to public LB, so I'm a tad worried that my CV process may have an issue somewhere. If you spot something, please let me know.","26f0414b":"**Preprocessing**","a262c882":"**1* Linear Regression without regularization**","bda02e52":"Note : I tried to remove the \"MSZoning_C (all)\" feature, it resulted in a slightly worse CV score, but slightly better public LB score.","96ee7d8c":"**Modeling**","a322f12d":"Then we will create new features, in 3 ways : \n\n 1. Simplifications of existing features\n 2. Combinations of existing features\n 3. Polynomials on the top 10 existing features","ebc43422":"**2* Linear Regression with Ridge regularization (L2 penalty)**\n\nFrom the *Python Machine Learning* book by Sebastian Raschka :  Regularization is a very useful method to handle collinearity, filter out noise from data, and eventually prevent overfitting. The concept behind regularization is to introduce additional information (bias) to penalize extreme parameter weights.  \n\nRidge regression is an L2 penalized model where we simply add the squared sum of the weights to our cost function.","d324df01":"Taking logs means that errors in predicting expensive houses and cheap houses will affect the result equally."}}