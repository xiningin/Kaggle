{"cell_type":{"0bc86c3a":"code","a4f5fd06":"code","780d4189":"code","b247ba5f":"code","c9a6b442":"code","31e32ee0":"code","26a45eeb":"code","592f3a86":"code","5fddf7b8":"code","dddfccf2":"code","a2dbe58c":"code","3604f64c":"code","4039be2f":"code","42f2dc1e":"code","33eef0d2":"code","22b31e3b":"code","d946815a":"code","245c4289":"code","b7754308":"code","911c118a":"code","49987215":"code","79dceb55":"code","a5be4c6b":"code","92656103":"code","17975c5b":"code","a5c56bdd":"code","34b45d34":"code","a8a95537":"markdown","9a219804":"markdown","c10c27b4":"markdown"},"source":{"0bc86c3a":"import numpy as np \nimport pandas as pd \nimport os \nimport random \nimport json \nfrom tqdm import tqdm \nimport re \nfrom functools import partial \nimport string","a4f5fd06":"RANDOM_SEED = 42 \ndef seed_everything(seed=RANDOM_SEED):\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    random.seed(seed)\nseed_everything()","780d4189":"submission = pd.read_csv('..\/input\/coleridgeinitiative-show-us-the-data\/sample_submission.csv')","b247ba5f":"train_files_path = '..\/input\/coleridgeinitiative-show-us-the-data\/train'\ntest_files_path = '..\/input\/coleridgeinitiative-show-us-the-data\/test'","c9a6b442":"train_df = pd.read_csv('..\/input\/coleridgeinitiative-show-us-the-data\/train.csv')\n\ntrain_df.head()","31e32ee0":"train_df.info()","26a45eeb":"def read_append_return(filename, train_files_path=train_files_path, output='text'):\n    ''' Read json file and then reutrn the text data from them and append to the dataframe'''\n    json_path = os.path.join(train_files_path, (filename+'.json'))\n    headings = []\n    contents = []\n    combined = []\n\n    with open(json_path, 'r') as f:\n        json_decode = json.load(f)\n        for data in json_decode:\n            headings.append(data.get('section_title'))\n            contents.append(data.get('text'))\n            combined.append(data.get('section_title'))\n            combined.append(data.get('text'))\n\n    all_headings = ' '.join(headings)\n    all_contents = ' '.join(contents)\n    all_data = '. '.join(combined)\n\n    if output == 'text':\n        return all_contents\n    elif output == 'head':\n        return all_headings \n    else:\n        return all_data\n","592f3a86":"%%time \ntqdm.pandas()\ntrain_df['text'] = train_df['Id'].progress_apply(read_append_return)","5fddf7b8":"train_df.head()","dddfccf2":"%%time\ntqdm.pandas()\nsubmission['text'] = submission['Id'].progress_apply(partial(read_append_return, train_files_path=test_files_path))\n\n","a2dbe58c":"submission.head()","3604f64c":"def text_cleaning(text):\n    text = ''.join([k for k in text if k not in string.punctuation])\n    text = re.sub('[^A-Za-z0-9]+', ' ', str(text).lower()).strip()\n\n    return text ","4039be2f":"%%time\ntqdm.pandas()\ntrain_df['text'] = train_df['text'].progress_apply(text_cleaning)","42f2dc1e":"%%time\ntqdm.pandas()\nsubmission['text'] = submission['text'].progress_apply(text_cleaning)","33eef0d2":"ds_label = [text_cleaning(s) for s in train_df[\"dataset_label\"].unique()]","22b31e3b":"cleaned_label = [text_cleaning(s) for s in train_df[\"cleaned_label\"].unique()]","d946815a":"ds_title = [text_cleaning(s) for s in train_df[\"dataset_title\"].unique()]","245c4289":"label_references = set(ds_label + cleaned_label + ds_title)","b7754308":"len(label_references)","911c118a":"prediction_labels = []","49987215":"for item in tqdm(submission[\"text\"]):\n    labels = []\n    for label in label_references:\n        if label in item:\n            labels.append(text_cleaning(label))\n            \n    prediction_labels.append(\"|\".join(labels))","79dceb55":"submission[\"PredictionString\"] = prediction_labels","a5be4c6b":"submission","92656103":"submission = submission[[\"Id\", \"PredictionString\"]]","17975c5b":"submission","a5c56bdd":"submission[\"PredictionString\"].iloc[0]","34b45d34":"submission.to_csv(\"submission.csv\", index=False)","a8a95537":"Clean the data","9a219804":"String matching","c10c27b4":"Prepare data"}}