{"cell_type":{"983ac4bf":"code","e8a2aebf":"code","e34cf09b":"code","25527f00":"code","8e0832d2":"code","57fcd1e3":"code","f963964b":"code","76b0a980":"code","2ba17814":"code","81cc0d21":"code","6151e06a":"code","5d7a7a82":"code","452801bb":"code","a85613d1":"code","569c8ca6":"code","57c3179a":"code","44cc64c2":"code","1a69fb66":"code","2edaad59":"code","d2993914":"code","a51272f5":"code","e5304f04":"code","6de4d4db":"code","b717ea83":"code","e58cddcb":"code","a9b14287":"code","e2d11e8c":"code","bfc7f06e":"code","90e76a22":"code","f5cfa54a":"code","a24302f9":"code","6753144b":"code","3d2cbdbc":"code","07d18a44":"code","cfcaf1ac":"code","2bbfad69":"code","2e2c3ab8":"code","b62253ae":"code","17b6c8d3":"markdown","a4dacd2a":"markdown","7c819670":"markdown","ded01c0c":"markdown","44ee4d79":"markdown","f36c2d6c":"markdown","f989a1ae":"markdown","20b53896":"markdown","baea308b":"markdown","0ff22dae":"markdown","5ca3a29f":"markdown","faaaeacd":"markdown","7f42d01b":"markdown","11589e1a":"markdown","7dcde5c2":"markdown","803bdcf5":"markdown","42e7a540":"markdown","add57c4e":"markdown","16c5f983":"markdown","f3ccf73e":"markdown","fe04e7d8":"markdown","fafe7993":"markdown","f64d5902":"markdown","eaa5dc06":"markdown","97d92d75":"markdown"},"source":{"983ac4bf":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","e8a2aebf":"import re\nimport json\nimport itertools\nimport collections\nfrom nltk.corpus import stopwords \nfrom nltk.tokenize import word_tokenize \nfrom wordcloud import WordCloud, ImageColorGenerator\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom PIL import Image","e34cf09b":"stop_words = set(stopwords.words('english')) \ncustom_stopwords = [\"https\", \"co\", \"he\", \"i\", \"you\", \"we\", \"l\", \"u\"]\n\nwith open(\"..\/input\/wordcloud-hindi-font\/Hindi_StopWords.txt\",encoding='utf-8') as f:\n    hindi_stopword= f.read().strip('\\ufeff')\nhindi_stopword = hindi_stopword.split(\", \")\nhindi_stopword = [i.strip(\"'\") for i in hindi_stopword]\n\nfor sw in (hindi_stopword + custom_stopwords):\n    stop_words.add(sw)","25527f00":"data = pd.read_csv(\"\/kaggle\/input\/national-unemployment-day\/NUD_tweets.csv\")","8e0832d2":"data.info()","57fcd1e3":"data.isna().sum()","f963964b":"data = data.fillna(\"NONE\")","76b0a980":"data.head()","2ba17814":"with open(\"\/kaggle\/input\/wordcloud-hindi-font\/city_states_data.json\") as f:\n    CITY_STATE = json.load(f)","81cc0d21":"HINDI_ENG_LOC = {\n    \"\u0917\u094b\u0930\u0916\u092a\u0941\u0930\": \"uttar pradesh\",\n    \"\u0917\u094b\u0935\u093e\": \"goa\",\n    \"\u0926\u0930\u092d\u0902\u0917\u093e\": \"bihar\",\n    \"\u0920\u093e\u0923\u0947\": \"maharashtra\",\n    \"\u0936\u094b\u0939\u0930\u0924\u0917\u0922\": \"uttar pradesh\",\n    \"\u0930\u0940\u0935\u093e\": \"madhya pradesh\",\n    \"\u0939\u0938\u0928\u092a\u0941\u0930\": \"uttar pradesh\",\n    \"\u091b\u0924\u094d\u0924\u0940\u0938\u0917\u095d\": \"chhattisgarh\",\n    \"\u092a\u091f\u0928\u093e\": \"bihar\",\n    \"\u0939\u092e\u0940\u0930\u092a\u0941\u0930\": \"uttar pradesh\",\n    \"\u0939\u0938\u094d\u0924\u093f\u0928\u093e\u092a\u0941\u0930\": \"uttar pradesh\",\n    \"\u092d\u094b\u092a\u093e\u0932\": \"madhya pradesh\",\n    \"\u0907\u091f\u093e\u0935\u093e\": \"uttar pradesh\",\n    \"\u092a\u094b\u0932\u0938\u0930\u093e\": \"odisha\",\n    \"\u092d\u091f\u093f\u0902\u0921\u093e\": \"punjab\",\n    \"\u092c\u093f\u0939\u093e\u0930\": \"bihar\",\n    \"\u091d\u093e\u0930\u0916\u0902\u0921\": \"jharkhand\",\n    \"\u092c\u0932\u0930\u093e\u092e\u092a\u0941\u0930\": \"uttar pradesh\",\n    \"\u092d\u0941\u0935\u0928\u0947\u0936\u094d\u0935\u0930\": \"odisha\",\n    \"\u0906\u091c\u093c\u092e\u0917\u0922\u093c\": \"uttar pradesh\",\n    \"\u092e\u0941\u0902\u092c\u0908\": \"maharashtra\",\n    \"\u0906\u092e\u091a\u0940 \u092e\u0941\u0902\u092c\u0908 \": \"maharashtra\",\n    \"\u0930\u0947\u0935\u093e\u0921\u093c\u0940\": \"haryana\",\n    \"\u092a\u094d\u0930\u092f\u093e\u0917\u0930\u093e\u091c\": \"uttar pradesh\",\n    \"\u0932\u0916\u0928\u090a\": \"uttar pradesh\",\n    \"\u0909\u0924\u094d\u0924\u0930\u092a\u094d\u0930\u0926\u0947\u0936\": \"uttar pradesh\",\n    \"\u0909\u0924\u094d\u0924\u0930 \u092a\u094d\u0930\u0926\u0947\u0936\": \"uttar pradesh\",\n    \"\u092c\u094d\u0930\u0939\u092e\u092a\u0941\u0930\": \"odisha\",\n    \"\u0915\u094b\u0932\u0915\u093e\u0924\u093e\": \"west bengal\",\n    \"\u0930\u093e\u091c\u0938\u094d\u0925\u093e\u0928\": \"rajasthan\",\n    \"\u0905\u0932\u0935\u0930\": \"rajasthan\",\n    \"\u0930\u093e\u091c\u0938\u094d\u0925\u093e\u0928\": \"rajasthan\",\n    \"\u0906\u0938\u093e\u092e\": \"assam\",\n    \"\u092e\u0925\u0941\u0930\u093e\": \"uttar pradesh\",\n    \"\u0927\u0928\u092c\u093e\u0926\": \"jharkhand\",\n    \"\u092e\u0927\u094d\u092f\u092a\u094d\u0930\u0926\u0947\u0936\": \"madhya pradesh\",\n    \"\u0909\u0924\u094d\u0924\u0930 \u092a\u094d\u0930\u0926\u0947\u0936\": \"uttar pradesh\",\n    \"\u0909\u0924\u094d\u0924\u0930\u092a\u094d\u0930\u0926\u0947\u0936\": \"uttar pradesh\",\n    \"\u0926\u093f\u0932\u094d\u0932\u0940\": \"delhi\",\n    \"\u0939\u0930\u093f\u092f\u093e\u0923\u093e\": \"haryana\",\n    \"\u0915\u094b\u091c\u093c\u093f\u0915\u094b\u0921\": \"kerala\",\n    \"\u091c\u092f\u092a\u0941\u0930\": \"rajasthan\",\n    \"\u0917\u0941\u091c\u0930\u093e\u0924\": \"gujarat\",\n    \"\u0906\u0938\u0928\u0938\u094b\u0932\": \"west bengal\",\n    \"\u092b\u093c\u0930\u0940\u0926\u093e\u092c\u093e\u0926\": \"haryana\",\n    \"\u0928\u0948\u0928\u0940\u0924\u093e\u0932\": \"uttarakhand\",\n    \"\u092e\u0948\u0902\u0917\u0932\u094b\u0930\": \"karnataka\",\n    \"\u091b\u093f\u0902\u0926\u0935\u093e\u0921\u093c\u093e\": \"madhya pradesh\",\n    \"\u0917\u093e\u091c\u093c\u093f\u092f\u093e\u092c\u093e\u0926\": \"uttar pradesh\",\n    \"\u0928\u094b\u090f\u0921\u093e\": \"uttar pradesh\",\n    \"\u092e\u0939\u093e\u0930\u093e\u0937\u094d\u091f\u094d\u0930\": \"maharashtra\",\n    \"jambughoda\": \"gujarat\",\n    \"canada\": \"canada\",\n    \"nelamanagala\": \"karnataka\",\n    \"u.p\": \"uttar pradesh\",\n    \"ghaziyabad\": \"uttar pradesh\",\n    \"\ud835\udc11\ud835\udc00\ud835\udc09\ud835\udc00\ud835\udc12\ud835\udc13\ud835\udc07\ud835\udc00\ud835\udc0d\": \"rajasthan\"\n}\n\nCUSTOM_INDIA_LOC = {\n    \"\u0939\u093f\u0902\u0926\u0941\u0938\u094d\u0924\u093e\u0928\": \"india\",\n    \"\u0939\u093f\u0928\u094d\u0926\u0941\u0938\u094d\u0924\u093e\u0928\u0940\": \"india\",\n    \"\u0907\u0902\u0921\u093f\u092f\u093e\": \"india\",\n    \"\u092d\u093e\u0930\u0924\": \"india\",\n    \"bharat\": \"india\",\n    \"hindu-stan\": \"india\",\n    \"hindustan\": \"india\",\n    \"india\": \"india\"\n}","6151e06a":"ALL_STATES = [\"Andhra Pradesh\",\"Arunachal Pradesh \",\"Assam\",\"Bihar\",\"Chhattisgarh\",\"Goa\",\"Gujarat\",\"Haryana\",\"Himachal Pradesh\",\"Jammu and Kashmir\",\"Jharkhand\",\"Karnataka\",\"Kerala\",\"Madhya Pradesh\",\"Maharashtra\",\"Manipur\",\"Meghalaya\",\"Mizoram\",\"Nagaland\",\"Odisha\",\"Punjab\",\"Rajasthan\",\"Sikkim\",\"Tamil Nadu\",\"Telangana\",\"Tripura\",\"Uttar Pradesh\",\"Uttarakhand\",\"West Bengal\",\"Andaman and Nicobar Islands\",\"Chandigarh\",\"Dadra and Nagar Haveli\",\"Daman and Diu\",\"Lakshadweep\",\"Delhi\",\"Puducherry\"]\nALL_STATES = [each_string.lower() for each_string in ALL_STATES]\nALL_STATES.sort()","5d7a7a82":"import requests\nresponse = requests.get(\"https:\/\/upload.wikimedia.org\/wikipedia\/en\/thumb\/1\/1e\/Bharatiya_Janata_Party_logo.svg\/1200px-Bharatiya_Janata_Party_logo.svg.png\")\nfile = open(\"bjp.png\", \"wb\")\nfile.write(response.content)\nfile.close()\n\nMAP = np.array(Image.open('.\/bjp.png'))\nImage.open('.\/bjp.png')","452801bb":"def only_str(line):\n    return line.apply(lambda x : re.findall(r'([0-9a-zA-Z]+)',x))\n\ndef is_eng(line):\n    return line == line.encode(\"unicode-escape\").decode()\n\ndef simple_text(line):\n    line = line.lower()\n    line = line.replace('\u201c', \"\").replace('\u201d', \"\").replace('\u2026', \"\").replace('_', \"\").replace(\"co\", \"\")\n    return line","a85613d1":"def create_list(df, column):\n    temp = df.copy()\n    temp[column] = temp[column].apply(lambda x: word_tokenize(x))\n    temp[column] = temp[column].apply(lambda x: [w for w in x if w not in stop_words])\n    temp[column] = temp[column].apply(lambda x: ' '.join(x))\n    t = temp[column].apply(lambda x: word_tokenize(x)).apply(lambda x: ' '.join(x)).tolist()\n    return t\n\n\ndef create_word_cloud(df, column): \n    t = \" \".join(create_list(df, column))\n    wordcloud = WordCloud(font_path=\"..\/input\/wordcloud-hindi-font\/Nirmala.ttf\", \n                          background_color='white', max_words=800, width=800, height=400).generate(t)\n    plt.figure( figsize=(20,10) )\n    plt.imshow(wordcloud)\n    plt.axis('off')\n    plt.show()\n\n\ndef bjp_word_cloud(df, column):\n    long_text = ' '.join(eng_data[column].tolist())\n    # Generate a word cloud image\n    mask = np.array(Image.open(\".\/bjp.png\"))\n    wordcloud_usa = WordCloud(font_path=\"..\/input\/wordcloud-hindi-font\/Nirmala.ttf\",\n                              stopwords=stop_words, background_color=\"white\", mode=\"RGBA\", \n                              max_words=800, mask=mask, width=1000, height=1000).generate(long_text)\n\n    # create coloring from image\n    image_colors = ImageColorGenerator(mask)\n    plt.figure(figsize=[10,10])\n    plt.imshow(wordcloud_usa.recolor(color_func=image_colors), interpolation=\"bilinear\")\n    plt.axis(\"off\")\n    plt.savefig('bjp_word_cloud.png')\n    plt.show()\n\n\ndef word_count_graph(df, column, num):\n    temp = pd.DataFrame()\n    temp[column] = only_str(df[column]).apply(lambda x: ' '.join(x))\n    all_words = word_tokenize(\" \".join(create_list(temp, column)))\n    word_counts = collections.Counter(all_words)\n    word_counts_data = pd.DataFrame(word_counts.most_common(num),columns=['words', 'count'])\n    \n    fig, ax = plt.subplots(figsize=(10, 10))\n    # Plot horizontal bar graph\n    word_counts_data.sort_values(by='count').plot.barh(x='words', y='count', ax=ax)\n\n    ax.set_title(\"Most Common Words\")\n    plt.show()\n\n\ndef word_count_to_df(df, column):\n    filtered = pd.DataFrame()\n    filtered[column] = only_str(df[column]).apply(lambda x: ' '.join(x))\n    filtered_list = create_list(filtered,column)\n    return pd.DataFrame({column:word_tokenize(\" \".join(filtered_list))})\n\ndef create_sns_graph(df, column, num):\n    count  = df[column].value_counts()\n    count = count[:num,]\n    plt.figure(figsize=(16,9))\n    sns.barplot(count.index, count.values, alpha=1)\n    # plt.title('Tweets vs User Location')\n    plt.ylabel('Number of Occurrences', fontsize=12)\n    # plt.xlabel('State', fontsize=12)\n    plt.xticks(rotation=90)\n    plt.show()","569c8ca6":"def replace_text1(x):\n    if \"NONE\" in x:\n        return \"NONE\"\n    for state in ALL_STATES:\n        if state in x.lower():\n            return state\n    return x.lower()\n\ndef replace_text(x, func_type):\n    if \"NONE\" in x:\n        return \"NONE\"\n    x = x.lower()\n    if func_type == \"city_state\":\n        for cs in CITY_STATE.keys():\n            if cs.lower() in x:\n                return CITY_STATE[cs].lower()\n    \n    if func_type == \"all_state\":\n        for state in ALL_STATES:\n            if state in x.lower():\n                return state\n\n    if func_type == \"hindi_eng\":\n        for he in HINDI_ENG_LOC.keys():\n            if he.lower() in x:\n                return HINDI_ENG_LOC[he]\n    \n    if func_type == \"india\":\n        for ci in CUSTOM_INDIA_LOC.keys():\n            if ci.lower() in x:\n                return CUSTOM_INDIA_LOC[ci]\n    \n    return x\n\ndef replace_spec(x, spec, change):\n    if \"NONE\" in x:\n        return \"NONE\"\n    if spec.lower() in x.lower():\n        return change.lower()\n    else:\n        return x.lower()","57c3179a":"unique_loc = data.user_location\nunique_loc.nunique()","44cc64c2":"unique_loc = unique_loc.apply(replace_text, args=(\"hindi_eng\",))\nunique_loc.nunique()","1a69fb66":"unique_loc = unique_loc.apply(replace_text, args=(\"city_state\",))\nunique_loc.nunique()","2edaad59":"unique_loc = unique_loc.apply(replace_text, args=(\"all_state\",))\nunique_loc.nunique()","d2993914":"unique_loc = unique_loc.apply(replace_text, args=(\"india\",))\nunique_loc.nunique()","a51272f5":"data.user_location = unique_loc\n# data.user_location.nunique()  # 488","e5304f04":"data.user_location.value_counts()","6de4d4db":"# create a dictionary of classes and their totals\nd = data.user_location.value_counts().loc[lambda x : x>50] .to_dict()  # counts greater than 5\n\nfig = plt.figure(figsize = (18, 6))\nax = fig.add_subplot()\n\n# plot the data using matplotlib\nax.pie(d.values(), # pass the values from our dictionary\n       labels = d.keys(), # pass the labels from our dictonary\n       autopct = '%1.1f%%', # specify the format to be plotted\n       textprops = {'fontsize': 10, 'color' : \"white\"} # change the font size and the color of the numbers inside the pie\n      )\n\n# set the title\nax.set_title(\"Twitter Users\")\n\n# set the legend and add a title to the legend\nax.legend(loc = \"upper left\", bbox_to_anchor = (1, 0, 0.5, 1), fontsize = 10, title = \"User's Location\");","b717ea83":"word_count_graph(data, \"user_location\", 15)","e58cddcb":"data.user_verified.value_counts()","a9b14287":"# create a dictionary of classes and their totals\nd = data.user_verified.value_counts().to_dict()\n\nfig = plt.figure(figsize = (18, 6))\nax = fig.add_subplot()\n\n# plot the data using matplotlib\nax.pie(d.values(), # pass the values from our dictionary\n       labels = d.keys(), # pass the labels from our dictonary\n       autopct = '%1.1f%%', # specify the format to be plotted\n       textprops = {'fontsize': 10, 'color' : \"white\"} # change the font size and the color of the numbers inside the pie\n      )\n\n# set the title\nax.set_title(\"Twitter Users\")\n\n# set the legend and add a title to the legend\nax.legend(loc = \"upper left\", bbox_to_anchor = (1, 0, 0.5, 1), fontsize = 10, title = \"Verified Twitter User\");","e2d11e8c":"data[\"text\"] = data[\"text\"].apply(simple_text)\neng_data = data[data[\"hashtags\"].apply(is_eng) == True]\neng_data = eng_data[eng_data[\"hashtags\"] != \"NONE\"]\nnon_eng_data = data[data[\"hashtags\"].apply(is_eng) == False]\nnon_eng_data = non_eng_data[non_eng_data[\"hashtags\"] != \"NONE\"]\nno_hashtag = data[data[\"hashtags\"]==\"NONE\"]","bfc7f06e":"print(eng_data.shape)\nprint(no_hashtag.shape)\nprint(non_eng_data.shape)","90e76a22":"eng_data = eng_data.reset_index(drop=True)\nno_hashtag = no_hashtag.reset_index(drop=True)\nnon_eng_data = non_eng_data.reset_index(drop=True)","f5cfa54a":"word_df = word_count_to_df(data, \"text\")\ncreate_sns_graph(word_df, \"text\", 30)","a24302f9":"word_df = word_count_to_df(eng_data, \"text\")\ncreate_sns_graph(word_df, \"text\", 30)","6753144b":"word_df = word_count_to_df(non_eng_data, \"text\")\ncreate_sns_graph(word_df, \"text\", 30)","3d2cbdbc":"word_df = word_count_to_df(no_hashtag, \"text\")\ncreate_sns_graph(word_df, \"text\", 30)","07d18a44":"bjp_word_cloud(data, \"text\")","cfcaf1ac":"create_word_cloud(data, \"text\")","2bbfad69":"create_word_cloud(eng_data, \"text\")","2e2c3ab8":"create_word_cloud(non_eng_data, \"text\")","b62253ae":"create_word_cloud(no_hashtag, \"text\")","17b6c8d3":"## 5.5 Create WordCloud for most frequent words in \"text\" column which have no hashtags.","a4dacd2a":"## 4.2 Create bar graph for most frequent words in \"text\" column in \"english\" hashtags.","7c819670":"## 2.1 See it on PIE Chart","ded01c0c":"## 1.3 Let's check total number of tweets from different states","44ee4d79":"# Defining global variables","f36c2d6c":"Fill NaN cells with NONE.\nBecause in the columns \"user_location\" and \"hashtags\", we can not fill any other value.","f989a1ae":"## 5.3 Create WordCloud for most frequent words in \"text\" column for \"english\" hashtags.","20b53896":"### Download BJP Image for masking WorldCloud","baea308b":"## 4.4 Create bar graph for most frequent words in \"text\" column in \"NONE\" hashtags.","0ff22dae":"## 4.3 Create bar graph for most frequent words in \"text\" column in \"non_english\" hashtags.","5ca3a29f":"# 5. WordCloud visualizations\n## 5.1. Create WordCloud for most frequent words in \"text\" column of whole dataset. #BJP","faaaeacd":"### From ***1830*** to ***488***, user_location column have 488 unique values now.","7f42d01b":"# 3. Let's check \"hashtags\" column","11589e1a":"# 4. Bar-Graph Visualizations\n## 4.1 Create bar graph for most frequent words in \"text\" column of whole dataset.","7dcde5c2":"# 1. Get number of tweets from different states\n\n## 1.1 Cleaning and try to get all the location to states. \n### But remember, it also contains non-english words and text which doesn't belongs to any city or state.","803bdcf5":"# 2. Verified <-> Non-Verified user","42e7a540":"## 1.4 See it on PIE Chart","add57c4e":"## 5.4 Create WordCloud for most frequent words in \"text\" column for \"non_english\" hashtags.","16c5f983":"### 3.1.2 Check number of records in each dataframes created and it should be equal to total records in original dataset.","f3ccf73e":"## 5.2 Create WordCloud for most frequent words in \"text\" column of whole dataset.","fe04e7d8":"## Have a look on data","fafe7993":"## 3.1 Sperate hashtags column according to the data it contains\n### 3.1.1 Creating different Dataframes on the basis of \"hashtags\" column.","f64d5902":"## 1.2 Normalize user_location column\nInitially, the unique values in \"user_location\" column are 1830.","eaa5dc06":"### Create \"stop_words\" list so that we can ignore it when creating WordCloud.","97d92d75":"# National Unemployment Day : Visualizations\n\n## Let's have a look on data and try to create visualizations from it."}}