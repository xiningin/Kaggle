{"cell_type":{"78ec850b":"code","8c1ccd2f":"code","998ffa99":"code","210cc806":"code","d2816ed3":"code","45bec8aa":"code","00406a73":"code","86cbf282":"code","c107e815":"code","c9d03a23":"code","f93acfe7":"code","cecda3fa":"code","28ca7ec4":"code","7e1d791a":"code","768e50d0":"code","8646eea6":"code","184c6929":"code","ab9540f0":"code","46831770":"code","e7162888":"code","6836ccf5":"code","80ed4b51":"code","7fbb98ad":"code","1236cc5b":"code","865af7d9":"code","85cd01f7":"code","433a459f":"code","e1643d95":"code","64d1f444":"code","e51b428e":"code","8de05549":"code","0403268b":"code","b6fb83f2":"code","eb5f9db0":"code","19920da2":"code","a5644291":"code","c7921479":"code","b34ce08d":"code","da8aa60a":"code","66583d21":"code","1bc1b39c":"code","344fdb64":"code","77a5fd82":"code","b272b11a":"code","fe51f42b":"code","e66ed6c4":"code","aec183f8":"code","df1d89a5":"code","7fa4c81d":"code","069a8869":"code","054c43d1":"code","54036420":"code","9eb02cb1":"code","809d4325":"code","26683cae":"code","bb787fbd":"code","4874ddad":"code","0ea382d2":"code","b7de6500":"code","ec886ef1":"code","868399b3":"code","ca6361f7":"code","c3bdf122":"code","00c99b31":"code","a9399abb":"code","a9a94682":"code","4c2099db":"code","25afd69a":"code","2fca483a":"code","ae797691":"code","1e28bdb4":"code","348b7724":"code","f92cbafc":"markdown","1ff68995":"markdown","c1c1a018":"markdown","b4c5d7b7":"markdown","1c5a5606":"markdown"},"source":{"78ec850b":"# 0909d -1-1-1\n# epoch \u957f\u4e00\u70b970->100\n# thrd 0.01 -> 0.005\n# \u6539\u4e3a pytorch lightning\n# pyg 172->201\n# 0917a 512 256 128\n# 0918a \u6539\u4e3a128 128 128 +mlp\n# \u53d1\u6563","8c1ccd2f":"nnn = ['time_id',\n     'log_return1_realized_volatility_0c1',\n     'log_return1_realized_volatility_1c1',     \n     'log_return1_realized_volatility_3c1',\n     'log_return1_realized_volatility_4c1',     \n     'log_return1_realized_volatility_6c1',\n     'total_volume_mean_0c1',\n     'total_volume_mean_1c1', \n     'total_volume_mean_3c1',\n     'total_volume_mean_4c1', \n     'total_volume_mean_6c1',\n     'trade_size_mean_0c1',\n     'trade_size_mean_1c1', \n     'trade_size_mean_3c1',\n     'trade_size_mean_4c1', \n     'trade_size_mean_6c1',\n     'trade_order_count_mean_0c1',\n     'trade_order_count_mean_1c1',\n     'trade_order_count_mean_3c1',\n     'trade_order_count_mean_4c1',\n     'trade_order_count_mean_6c1',      \n     'price_spread_mean_0c1',\n     'price_spread_mean_1c1',\n     'price_spread_mean_3c1',\n     'price_spread_mean_4c1',\n     'price_spread_mean_6c1',   \n     'bid_spread_mean_0c1',\n     'bid_spread_mean_1c1',\n     'bid_spread_mean_3c1',\n     'bid_spread_mean_4c1',\n     'bid_spread_mean_6c1',       \n     'ask_spread_mean_0c1',\n     'ask_spread_mean_1c1',\n     'ask_spread_mean_3c1',\n     'ask_spread_mean_4c1',\n     'ask_spread_mean_6c1',   \n     'volume_imbalance_mean_0c1',\n     'volume_imbalance_mean_1c1',\n     'volume_imbalance_mean_3c1',\n     'volume_imbalance_mean_4c1',\n     'volume_imbalance_mean_6c1',       \n     'bid_ask_spread_mean_0c1',\n     'bid_ask_spread_mean_1c1',\n     'bid_ask_spread_mean_3c1',\n     'bid_ask_spread_mean_4c1',\n     'bid_ask_spread_mean_6c1',\n     'size_tau2_0c1',\n     'size_tau2_1c1',\n     'size_tau2_3c1',\n     'size_tau2_4c1',\n     'size_tau2_6c1'] ","998ffa99":"!pip install ..\/input\/pyg201whl\/*.whl","210cc806":"'''\u673a\u5668\u5b66\u4e60\u533a'''\nimport numpy as np\nimport numpy.matlib\nimport pandas as pd\nfrom sklearn.model_selection import GroupKFold,KFold\nfrom sklearn.preprocessing import MinMaxScaler, QuantileTransformer\nfrom sklearn.cluster import KMeans\n\n'''\u7ed8\u56fe\u533a'''\nimport matplotlib.pyplot as plt\nimport matplotlib.style as style\nimport seaborn as sns\nfrom matplotlib import pyplot\nfrom matplotlib.ticker import ScalarFormatter\nsns.set_context(\"talk\")\n\n'''pytroch'''\nimport torch\nimport torch.nn.functional as F\nimport torch.nn as nn\nfrom torch.nn import ModuleList, BatchNorm1d\nimport torch.optim as optim\nfrom torch.utils.data import Dataset,DataLoader\nfrom torch import Tensor\nimport pytorch_lightning as pl\nfrom pytorch_lightning import Trainer, seed_everything\n\nfrom pytorch_lightning.callbacks import EarlyStopping\n'''pyg'''\nimport torch_geometric\nfrom torch_geometric.datasets import Reddit\nfrom torch_geometric.data import Data,NeighborSampler\nfrom torch_geometric.data import NeighborSampler as RawNeighborSampler\nif torch_geometric.__version__ != '1.7.2':\n    from torch_geometric.loader.neighbor_sampler import EdgeIndex\nelse:\n    from torch_geometric.data.sampler import EdgeIndex\nfrom torch_geometric.nn import GCNConv,GATConv, ChebConv,SAGEConv\n\n'''\u7cfb\u7edf\u8d44\u6e90\u533a'''\nfrom tqdm.notebook import tqdm\nfrom typing import Optional, List, NamedTuple\nimport multiprocessing\nfrom joblib import Parallel, delayed\nfrom glob import glob\nimport warnings\nwarnings.filterwarnings('ignore')\nimport pickle\nimport gc\nfrom torch_geometric.nn import TransformerConv\nheads = 4","d2816ed3":"DEBUG = 0\nTHRD = 0.02","45bec8aa":"BASE = '..\/input\/optiver-realized-volatility-prediction\/'\nEBASE = '..\/input\/optiver-clf\/'\nEBASE1 = '..\/input\/opt-0924\/'\nBOOK_TST = BASE+'book_test.parquet\/'\nBOOK_TRN = BASE+'book_train.parquet\/'\nTRADE_TST = BASE+'trade_test.parquet\/'\nTRADE_TRN = BASE+'trade_train.parquet\/'\nTRN = BASE+'train.csv'\nTST = BASE+'test.csv'\n\nif DEBUG == 1:\n    TST = TRN\n    BOOK_TST = BOOK_TRN\n    \n    trn = pd.read_csv(TRN)\n    trn['row_id'] = trn['stock_id'].astype(str) + '-' + trn['time_id'].astype(str)\n    tst = trn.copy()\n    y_true = tst['target']\n    tst.drop('target',axis=1,inplace =True)\nelse:\n    trn = pd.read_csv(TRN)\n    trn['row_id'] = trn['stock_id'].astype(str) + '-' + trn['time_id'].astype(str)\n    tst = pd.read_csv(TST)\nprint('trn',trn.shape,'tst',tst.shape)","00406a73":"trn_time_id = trn.time_id.unique()\nall_stock_id = trn.stock_id.unique()","86cbf282":"def load_book_data_by_stock_id(stock_id,is_train=True):\n    if is_train == True:\n        df = pd.read_parquet(f'{BOOK_TRN}stock_id={stock_id}')\n    else:\n        df = pd.read_parquet(f'{BOOK_TST}stock_id={stock_id}')\n    return df\n\ndef book_feature_by_stock_id_for_trn(stock_id):\n    return book_feature_by_stock_id(stock_id)\n\ndef book_feature_by_stock_id_for_tst(stock_id):\n    return book_feature_by_stock_id(stock_id,is_train=False)\n\ndef calc_wap1(df):\n    return (df['bid_price1'] * df['ask_size1'] + df['ask_price1'] * df['bid_size1']) \/ (df['bid_size1'] + df['ask_size1'])\n\ndef calc_price(df):\n    diff = abs(df[['bid_price1','ask_price1','bid_price2','ask_price2']].diff())\n    min_diff = np.nanmin(diff.where(lambda x: x > 0))\n    n_ticks = (diff \/ min_diff).round()\n    scale = 0.01 \/ np.nanmean(diff \/ n_ticks)\n    return scale\n\ndef calc_prices(stock_id,is_train):\n    try:\n        book = load_book_data_by_stock_id(stock_id,is_train)\n    except:\n        return pd.DataFrame()\n    book['wap1'] = calc_wap1(book)\n    df = book.groupby('time_id').apply(calc_price).to_frame('price').reset_index()\n    df['stock_id'] = stock_id\n    df['first_wap'] = df['price'] * book.groupby('time_id')['wap1'].first().values\n    df['last_wap'] = df['price'] * book.groupby('time_id')['wap1'].last().values\n    df['mean_wap'] = df['price'] * book.groupby('time_id')['wap1'].mean().values\n    return df","c107e815":"stock_ids = trn.stock_id.unique()","c9d03a23":"SEED=42\n\ndef set_all_seed(SEED=42):\n    np.random.seed(SEED)\n    torch.manual_seed(SEED)\n    torch.cuda.manual_seed(SEED)\n    torch.cuda.manual_seed_all(SEED)\n\n    torch.backends.cudnn.deterministic=True\n    torch.backends.cudnn.benchmark = False\n    seed_everything(SEED)\nset_all_seed(SEED)","f93acfe7":"def get_cluster_knn_values(out_train):\n    np.random.seed(SEED)\n    out_train = out_train.pivot(index='time_id', columns='stock_id', values='target')\n\n    # out_train[out_train.isna().any(axis=1)]\n    out_train = out_train.fillna(out_train.mean())\n    out_train.head()\n\n    # Code to add the just the read data after first execution\n\n    # Data separation based on knn ++\n    nfolds = 5 # number of folds\n    index = []\n    totDist = []\n    values = []\n\n    # Generates a matriz with the values of \n    mat = out_train.values\n    scaler = MinMaxScaler(feature_range=(-1, 1))\n    mat = scaler.fit_transform(mat)\n    nind = int(mat.shape[0]\/nfolds) # number of individuals\n\n    # Adds index in the last column\n    mat = np.c_[mat,np.arange(mat.shape[0])]\n    lineNumber = np.random.choice(np.array(mat.shape[0]), size=nfolds, replace=False)\n    lineNumber = np.sort(lineNumber)[::-1]\n    for n in range(nfolds):\n        totDist.append(np.zeros(mat.shape[0]-nfolds))\n\n    # Saves index\n    for n in range(nfolds):    \n        values.append([lineNumber[n]])\n\n    s=[]\n    for n in range(nfolds):\n        s.append(mat[lineNumber[n],:])\n        mat = np.delete(mat, obj=lineNumber[n], axis=0)\n\n    for n in range(nind-1):    \n        luck = np.random.uniform(0,1,nfolds)\n\n        for cycle in range(nfolds):\n            # Saves the values of index           \n            s[cycle] = np.matlib.repmat(s[cycle], mat.shape[0], 1)\n            sumDist = np.sum( (mat[:,:-1] - s[cycle][:,:-1])**2 , axis=1)   \n            totDist[cycle] += sumDist        \n\n            # Probabilities\n            f = totDist[cycle]\/np.sum(totDist[cycle]) # normalizing the totDist\n            j = 0\n            kn = 0\n            for val in f:\n                j += val        \n                if (j > luck[cycle]): # the column was selected\n                    break\n                kn +=1\n            lineNumber[cycle] = kn\n\n            # Delete line of the value added    \n            for n_iter in range(nfolds):\n                totDist[n_iter] = np.delete(totDist[n_iter],obj=lineNumber[cycle], axis=0)\n                j= 0\n\n            s[cycle] = mat[lineNumber[cycle],:]\n            values[cycle].append(int(mat[lineNumber[cycle],-1]))\n            mat = np.delete(mat, obj=lineNumber[cycle], axis=0)\n\n    for n_mod in range(nfolds):\n        values[n_mod] = out_train.index[values[n_mod]]    \n    return values \n    \nknn_values = get_cluster_knn_values(trn)","cecda3fa":"def get_cluster_labels(train_p):\n    train_p = train_p.pivot(index='time_id', columns='stock_id', values='target')\n    corr = train_p.corr()\n    ids = corr.index\n    kmeans = KMeans(n_clusters=7, random_state=0).fit(corr.values)\n    print(kmeans.labels_)\n    l = []\n    for n in range(7):\n        l.append ( [ (x-1) for x in ( (ids+1)*(kmeans.labels_ == n)) if x > 0] )\n    return l\n\ncluster_labels = get_cluster_labels(trn)","28ca7ec4":"from IPython.core.display import display, HTML\n\nimport glob\nimport os\nimport gc\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt \nimport seaborn as sns\nimport numpy.matlib\n\nimport plotly.express as px\nimport plotly.graph_objects as go\n\nfrom joblib import Parallel, delayed\n\nfrom sklearn import preprocessing, model_selection\nfrom sklearn.preprocessing import MinMaxScaler, QuantileTransformer\nfrom sklearn.metrics import r2_score\nfrom sklearn.cluster import KMeans\n\nfrom numpy.random import seed\nseed(42)\n\nimport tensorflow as tf\ntf.random.set_seed(42)\nfrom tensorflow import keras\nfrom keras import backend as K\nfrom keras.backend import sigmoid\nfrom keras.utils.generic_utils import get_custom_objects\nfrom keras.layers import Activation\n\nimport warnings\nwarnings.filterwarnings('ignore')","7e1d791a":"def read_train_test():\n    # Function to read our base train and test set\n    \n    train = pd.read_csv('..\/input\/optiver-realized-volatility-prediction\/train.csv')\n    test = pd.read_csv('..\/input\/optiver-realized-volatility-prediction\/test.csv')\n\n    # Create a key to merge with book and trade data\n    train['row_id'] = train['stock_id'].astype(str) + '-' + train['time_id'].astype(str)\n    test['row_id'] = test['stock_id'].astype(str) + '-' + test['time_id'].astype(str)\n    print(f'Our training set has {train.shape[0]} rows')\n    \n    return train, test\n\n# Read train and test\ntrain, test = read_train_test()","768e50d0":"# data directory\ndata_dir = '..\/input\/optiver-realized-volatility-prediction\/'\n\ndef calc_wap1(df):\n    # Function to calculate first WAP\n    wap = (df['bid_price1'] * df['ask_size1'] + df['ask_price1'] * df['bid_size1']) \/ (df['bid_size1'] + df['ask_size1'])\n    return wap\n\ndef calc_wap2(df):\n    # Function to calculate second WAP\n    wap = (df['bid_price2'] * df['ask_size2'] + df['ask_price2'] * df['bid_size2']) \/ (df['bid_size2'] + df['ask_size2'])\n    return wap\n\ndef log_return(series):\n    # Function to calculate the log of the return\n    return np.log(series).diff()\n\ndef realized_volatility(series):\n    # Calculate the realized volatility\n    return np.sqrt(np.sum(series**2))\n\ndef count_unique(series):\n    # Function to count unique elements of a series\n    return len(np.unique(series))\n\ndef book_preprocessor(file_path):\n    # Function to preprocess book data (for each stock id)\n    \n    df = pd.read_parquet(file_path)\n    \n    # Calculate Wap\n    df['wap1'] = calc_wap1(df)\n    df['wap2'] = calc_wap2(df)\n    \n    # Calculate log returns\n    df['log_return1'] = df.groupby(['time_id'])['wap1'].apply(log_return)\n    df['log_return2'] = df.groupby(['time_id'])['wap2'].apply(log_return)\n    \n    # Calculate wap balance\n    df['wap_balance'] = abs(df['wap1'] - df['wap2'])\n    \n    # Calculate spread\n    df['price_spread'] = (df['ask_price1'] - df['bid_price1']) \/ ((df['ask_price1'] + df['bid_price1']) \/ 2)\n    df['price_spread2'] = (df['ask_price2'] - df['bid_price2']) \/ ((df['ask_price2'] + df['bid_price2']) \/ 2)\n    df['bid_spread'] = df['bid_price1'] - df['bid_price2']\n    df['ask_spread'] = df['ask_price1'] - df['ask_price2']\n    df[\"bid_ask_spread\"] = abs(df['bid_spread'] - df['ask_spread'])\n    df['total_volume'] = (df['ask_size1'] + df['ask_size2']) + (df['bid_size1'] + df['bid_size2'])\n    df['volume_imbalance'] = abs((df['ask_size1'] + df['ask_size2']) - (df['bid_size1'] + df['bid_size2']))\n    \n    # Dict for aggregations\n    create_feature_dict = {\n        'wap1': [np.sum, np.mean, np.std],\n        'wap2': [np.sum, np.mean, np.std],\n        'log_return1': [np.sum, realized_volatility, np.mean, np.std],\n        'log_return2': [np.sum, realized_volatility, np.mean, np.std],\n        'wap_balance': [np.sum, np.mean, np.std],\n        'price_spread':[np.sum, np.mean, np.std],\n        'price_spread2':[np.sum, np.mean, np.std],\n        'bid_spread':[np.sum, np.mean, np.std],\n        'ask_spread':[np.sum, np.mean, np.std],\n        'total_volume':[np.sum, np.mean, np.std],\n        'volume_imbalance':[np.sum, np.mean, np.std],\n        \"bid_ask_spread\":[np.sum, np.mean, np.std],\n    }\n    \n    def get_stats_window(seconds_in_bucket, add_suffix = False):\n        # Function to get group stats for different windows (seconds in bucket)\n        \n        # Group by the window\n        df_feature = df[df['seconds_in_bucket'] >= seconds_in_bucket].groupby(['time_id']).agg(create_feature_dict).reset_index()\n        \n        # Rename columns joining suffix\n        df_feature.columns = ['_'.join(col) for col in df_feature.columns]\n        \n        # Add a suffix to differentiate windows\n        if add_suffix:\n            df_feature = df_feature.add_suffix('_' + str(seconds_in_bucket))\n        return df_feature\n    \n    # Get the stats for different windows\n    df_feature = get_stats_window(seconds_in_bucket = 0, add_suffix = False)\n    df_feature_400 = get_stats_window(seconds_in_bucket = 400, add_suffix = True)\n    df_feature_300 = get_stats_window(seconds_in_bucket = 300, add_suffix = True)\n    df_feature_200 = get_stats_window(seconds_in_bucket = 200, add_suffix = True)\n    \n    # Merge all\n    df_feature = df_feature.merge(df_feature_400, how = 'left', left_on = 'time_id_', right_on = 'time_id__400')\n    df_feature = df_feature.merge(df_feature_300, how = 'left', left_on = 'time_id_', right_on = 'time_id__300')\n    df_feature = df_feature.merge(df_feature_200, how = 'left', left_on = 'time_id_', right_on = 'time_id__200')\n\n    # Drop unnecesary time_ids\n    df_feature.drop(['time_id__400', 'time_id__300', 'time_id__200'], axis = 1, inplace = True)\n    \n    \n    # Create row_id so we can merge\n    stock_id = file_path.split('=')[1]\n    df_feature['row_id'] = df_feature['time_id_'].apply(lambda x: f'{stock_id}-{x}')\n    df_feature.drop(['time_id_'], axis = 1, inplace = True)\n    \n    return df_feature\n\n\ndef trade_preprocessor(file_path):\n    # Function to preprocess trade data (for each stock id)\n    \n    df = pd.read_parquet(file_path)\n    df['log_return'] = df.groupby('time_id')['price'].apply(log_return)\n    \n    # Dict for aggregations\n    create_feature_dict = {\n        'log_return':[realized_volatility],\n        'seconds_in_bucket':[count_unique],\n        'size':[np.sum, realized_volatility, np.mean, np.std, np.max, np.min],\n        'order_count':[np.mean,np.sum,np.max],\n    }\n    \n    def get_stats_window(seconds_in_bucket, add_suffix = False):\n        # Function to get group stats for different windows (seconds in bucket)\n        \n        # Group by the window\n        df_feature = df[df['seconds_in_bucket'] >= seconds_in_bucket].groupby(['time_id']).agg(create_feature_dict).reset_index()\n        \n        # Rename columns joining suffix\n        df_feature.columns = ['_'.join(col) for col in df_feature.columns]\n        \n        # Add a suffix to differentiate windows\n        if add_suffix:\n            df_feature = df_feature.add_suffix('_' + str(seconds_in_bucket))\n        return df_feature\n    \n    # Get the stats for different windows\n    df_feature = get_stats_window(seconds_in_bucket = 0, add_suffix = False)\n    df_feature_400 = get_stats_window(seconds_in_bucket = 400, add_suffix = True)\n    df_feature_300 = get_stats_window(seconds_in_bucket = 300, add_suffix = True)\n    df_feature_200 = get_stats_window(seconds_in_bucket = 200, add_suffix = True)\n    \n    def tendency(price, vol):    \n        df_diff = np.diff(price)\n        val = (df_diff\/price[1:])*100\n        power = np.sum(val*vol[1:])\n        return(power)\n    \n    lis = []\n    for n_time_id in df['time_id'].unique():\n        df_id = df[df['time_id'] == n_time_id]        \n        tendencyV = tendency(df_id['price'].values, df_id['size'].values)      \n        f_max = np.sum(df_id['price'].values > np.mean(df_id['price'].values))\n        f_min = np.sum(df_id['price'].values < np.mean(df_id['price'].values))\n        df_max =  np.sum(np.diff(df_id['price'].values) > 0)\n        df_min =  np.sum(np.diff(df_id['price'].values) < 0)\n        abs_diff = np.median(np.abs( df_id['price'].values - np.mean(df_id['price'].values)))        \n        energy = np.mean(df_id['price'].values**2)\n        iqr_p = np.percentile(df_id['price'].values,75) - np.percentile(df_id['price'].values,25)\n        abs_diff_v = np.median(np.abs( df_id['size'].values - np.mean(df_id['size'].values)))        \n        energy_v = np.sum(df_id['size'].values**2)\n        iqr_p_v = np.percentile(df_id['size'].values,75) - np.percentile(df_id['size'].values,25)\n        \n        lis.append({'time_id':n_time_id,'tendency':tendencyV,'f_max':f_max,'f_min':f_min,'df_max':df_max,'df_min':df_min,\n                   'abs_diff':abs_diff,'energy':energy,'iqr_p':iqr_p,'abs_diff_v':abs_diff_v,'energy_v':energy_v,'iqr_p_v':iqr_p_v})\n    \n    df_lr = pd.DataFrame(lis)\n        \n   \n    df_feature = df_feature.merge(df_lr, how = 'left', left_on = 'time_id_', right_on = 'time_id')\n    \n    # Merge all\n    df_feature = df_feature.merge(df_feature_400, how = 'left', left_on = 'time_id_', right_on = 'time_id__400')\n    df_feature = df_feature.merge(df_feature_300, how = 'left', left_on = 'time_id_', right_on = 'time_id__300')\n    df_feature = df_feature.merge(df_feature_200, how = 'left', left_on = 'time_id_', right_on = 'time_id__200')\n\n    # Drop unnecesary time_ids\n    df_feature.drop(['time_id__400', 'time_id__300', 'time_id__200','time_id'], axis = 1, inplace = True)\n    df_feature = df_feature.add_prefix('trade_')\n    stock_id = file_path.split('=')[1]\n    df_feature['row_id'] = df_feature['trade_time_id_'].apply(lambda x:f'{stock_id}-{x}')\n    df_feature.drop(['trade_time_id_'], axis = 1, inplace = True)\n    \n    return df_feature\n\n\ndef get_time_stock(df):\n    # Function to get group stats for the stock_id and time_id\n    \n    # Get realized volatility columns\n    vol_cols = ['log_return1_realized_volatility', 'log_return2_realized_volatility', 'log_return1_realized_volatility_400', 'log_return2_realized_volatility_400', \n                'log_return1_realized_volatility_300', 'log_return2_realized_volatility_300', 'log_return1_realized_volatility_200', 'log_return2_realized_volatility_200', \n                'trade_log_return_realized_volatility', 'trade_log_return_realized_volatility_400', 'trade_log_return_realized_volatility_300', 'trade_log_return_realized_volatility_200']\n\n    # Group by the stock id\n    df_stock_id = df.groupby(['stock_id'])[vol_cols].agg(['mean', 'std', 'max', 'min', ]).reset_index()\n    \n    # Rename columns joining suffix\n    df_stock_id.columns = ['_'.join(col) for col in df_stock_id.columns]\n    df_stock_id = df_stock_id.add_suffix('_' + 'stock')\n\n    # Group by the stock id\n    df_time_id = df.groupby(['time_id'])[vol_cols].agg(['mean', 'std', 'max', 'min', ]).reset_index()\n    \n    # Rename columns joining suffix\n    df_time_id.columns = ['_'.join(col) for col in df_time_id.columns]\n    df_time_id = df_time_id.add_suffix('_' + 'time')\n    \n    # Merge with original dataframe\n    df = df.merge(df_stock_id, how = 'left', left_on = ['stock_id'], right_on = ['stock_id__stock'])\n    df = df.merge(df_time_id, how = 'left', left_on = ['time_id'], right_on = ['time_id__time'])\n    df.drop(['stock_id__stock', 'time_id__time'], axis = 1, inplace = True)\n    \n    return df\n    \n    \ndef preprocessor(list_stock_ids, is_train = True):\n    # Funtion to make preprocessing function in parallel (for each stock id)\n    \n    # Parrallel for loop\n    def for_joblib(stock_id):\n        # Train\n        if is_train:\n            file_path_book = data_dir + \"book_train.parquet\/stock_id=\" + str(stock_id)\n            file_path_trade = data_dir + \"trade_train.parquet\/stock_id=\" + str(stock_id)\n        # Test\n        else:\n            file_path_book = data_dir + \"book_test.parquet\/stock_id=\" + str(stock_id)\n            file_path_trade = data_dir + \"trade_test.parquet\/stock_id=\" + str(stock_id)\n    \n        # Preprocess book and trade data and merge them\n        df_tmp = pd.merge(book_preprocessor(file_path_book), trade_preprocessor(file_path_trade), on = 'row_id', how = 'left')\n        \n        # Return the merge dataframe\n        return df_tmp\n    \n    # Use parallel api to call paralle for loop\n    df = Parallel(n_jobs = -1, verbose = 1)(delayed(for_joblib)(stock_id) for stock_id in list_stock_ids)\n    \n    # Concatenate all the dataframes that return from Parallel\n    df = pd.concat(df, ignore_index = True)\n    \n    return df\n\n\ndef rmspe(y_true, y_pred):\n    # Function to calculate the root mean squared percentage error\n    return np.sqrt(np.mean(np.square((y_true - y_pred) \/ y_true)))\n\ndef feval_rmspe(y_pred, lgb_train):\n    # Function to early stop with root mean squared percentage error\n    y_true = lgb_train.get_label()\n    return 'RMSPE', rmspe(y_true, y_pred), False","8646eea6":"# Get unique stock ids \ntrain_stock_ids = train['stock_id'].unique()\n\n# Preprocess them using Parallel and our single stock id functions\n#train_ = preprocessor(train_stock_ids, is_train = True)\n#train = train.merge(train_, on = ['row_id'], how = 'left')\n\n# Get unique stock ids \ntest_stock_ids = test['stock_id'].unique()\n\n# Preprocess them using Parallel and our single stock id functions\ntest_ = preprocessor(test_stock_ids, is_train = False)\ntest = test.merge(test_, on = ['row_id'], how = 'left')\n\n# Get group stats of time_id and stock_id\n#train = get_time_stock(train)\ntest = get_time_stock(test)","184c6929":"# replace by order sum (tau)\n#train['size_tau'] = np.sqrt(1\/train['trade_seconds_in_bucket_count_unique'])\ntest['size_tau'] = np.sqrt(1\/test['trade_seconds_in_bucket_count_unique'])\n#train['size_tau_400'] = np.sqrt(1\/train['trade_seconds_in_bucket_count_unique_400'])\ntest['size_tau_400'] = np.sqrt(1\/test['trade_seconds_in_bucket_count_unique_400'])\n#train['size_tau_300'] = np.sqrt(1\/train['trade_seconds_in_bucket_count_unique_300'])\ntest['size_tau_300'] = np.sqrt(1\/test['trade_seconds_in_bucket_count_unique_300'])\n#train['size_tau_200'] = np.sqrt(1\/train['trade_seconds_in_bucket_count_unique_200'])\ntest['size_tau_200'] = np.sqrt(1\/test['trade_seconds_in_bucket_count_unique_200'])","ab9540f0":"# tau2 \n#train['size_tau2'] = np.sqrt(1\/train['trade_order_count_sum'])\ntest['size_tau2'] = np.sqrt(1\/test['trade_order_count_sum'])\n#train['size_tau2_400'] = np.sqrt(0.25\/train['trade_order_count_sum'])\ntest['size_tau2_400'] = np.sqrt(0.25\/test['trade_order_count_sum'])\n#train['size_tau2_300'] = np.sqrt(0.5\/train['trade_order_count_sum'])\ntest['size_tau2_300'] = np.sqrt(0.5\/test['trade_order_count_sum'])\n#train['size_tau2_200'] = np.sqrt(0.75\/train['trade_order_count_sum'])\ntest['size_tau2_200'] = np.sqrt(0.75\/test['trade_order_count_sum'])\n\n# delta tau\n#train['size_tau2_d'] = train['size_tau2_400'] - train['size_tau2']\ntest['size_tau2_d'] = test['size_tau2_400'] - test['size_tau2']","46831770":"trn_df = pd.read_parquet('..\/input\/opt-0924\/train0923.parq')\ntst_df = test.copy()","e7162888":"num_col = trn_df.columns.difference(['time_id','target','row_id','stock_id']).tolist()\nnum_col = [col for col in num_col if '_stock' not in col ]\ncat_col = 'stock_id'\nfea_col = [cat_col]+num_col\nprint(len(fea_col),len(num_col),cat_col)\nprint(np.nanmean(trn_df[num_col],axis=0).round(5).tolist())","6836ccf5":"'''\u5148qt'''\ntrn_df.replace([np.inf, -np.inf], np.nan,inplace=True)\ntst_df.replace([np.inf, -np.inf], np.nan,inplace=True)\nqt_train = []\n\nfor col in num_col:\n    qt = QuantileTransformer(random_state=21,n_quantiles=2000, output_distribution='normal')\n    trn_df[col] = qt.fit_transform(trn_df[[col]])\n    tst_df[col] = qt.transform(tst_df[[col]])    \n    qt_train.append(qt)\n","80ed4b51":"print(np.nanmean(trn_df[num_col],axis=0).round(5).tolist())","7fbb98ad":"def get_agg_fea(trn_df,tst_df,cluster_labels):\n    mat = []\n    matTest = []\n    n = 0\n    for ind in cluster_labels:\n        print(ind)\n        newDf = trn_df.loc[trn_df['stock_id'].isin(ind) ]\n        newDf = newDf.groupby(['time_id']).agg(np.nanmean)\n        newDf.loc[:,'stock_id'] = str(n)+'c1'\n        mat.append ( newDf )\n        newDf = tst_df.loc[tst_df['stock_id'].isin(ind) ]    \n        newDf = newDf.groupby(['time_id']).agg(np.nanmean)\n        newDf.loc[:,'stock_id'] = str(n)+'c1'\n        matTest.append ( newDf )\n        n+=1\n\n    mat1 = pd.concat(mat).reset_index()\n    mat1.drop(columns=['target'],inplace=True)\n    mat2 = pd.concat(matTest).reset_index()\n\n    '''\u53d8\u6362\u5f62\u72b6\uff0c\u6539\u540d'''\n    mat2 = pd.concat([mat2,mat1.loc[mat1.time_id==5]])#\u4e0b\u9762\u662fleft join\n    mat1 = mat1.pivot(index='time_id', columns='stock_id')\n    mat1.columns = [\"_\".join(x) for x in mat1.columns.ravel()]\n    mat1.reset_index(inplace=True)\n\n    mat2 = mat2.pivot(index='time_id', columns='stock_id')\n    mat2.columns = [\"_\".join(x) for x in mat2.columns.ravel()]\n    mat2.reset_index(inplace=True)    \n    \n    '''\u62fc\u63a5'''\n    trn_df = pd.merge(trn_df,mat1[nnn].set_index('time_id'),how='left',left_on='time_id' ,right_index=True)\n    tst_df = pd.merge(tst_df,mat2[nnn].set_index('time_id'),how='left',left_on='time_id' ,right_index=True)\n    return trn_df,tst_df","1236cc5b":"trn_df,tst_df = get_agg_fea(trn_df,tst_df,cluster_labels)#\u4e3b\u8981\u662fagg\u548cscaler\u90e8\u5206","865af7d9":"prices_df1 = pd.read_parquet(EBASE+'\/df_prices1.pq') #\u660e\u793a\u6570\u636e\u7684\u4ef7\u683c\u77e9\u9635\nprices_df2 = pd.concat(Parallel(n_jobs=4, verbose=51)(delayed(calc_prices)(r,False) for r in all_stock_id))\nprices_df = pd.concat([prices_df1,prices_df2]).reset_index(drop=True)\nprint(f\"\u51fa\u73b0time_id:{prices_df.time_id.nunique()}\")\ntrn_df = trn_df.merge(prices_df[['time_id','stock_id','price']],how='left')\ntst_df = tst_df.merge(prices_df[['time_id','stock_id','price']],how='left')","85cd01f7":"num_col = trn_df.columns.difference(['time_id','target','row_id','stock_id','price']).tolist()\nnum_col = [col for col in num_col if '_stock' not in col ]\ncat_col = 'stock_id'\nfea_col = [cat_col]+num_col\nprint(len(fea_col),len(num_col),cat_col)\nprint(np.nanmean(trn_df[num_col],axis=0).round(5).tolist())","433a459f":"trn_df[num_col] = trn_df[num_col].fillna(trn_df[num_col].mean())\ntst_df[num_col] = tst_df[num_col].fillna(trn_df[num_col].mean())\nscaler = MinMaxScaler(feature_range=(-1, 1))\ngg = 50\nfor i in range(len(num_col)\/\/gg+1):\n    col = num_col[i*gg:i*gg+gg]\n    trn_df[col] = scaler.fit_transform(trn_df[col])\n    tst_df[col] = scaler.transform(tst_df[col])\nnode_df = pd.concat([trn_df,tst_df]).drop_duplicates().reset_index(drop=True)\nprint(np.nanmean(trn_df[num_col],axis=0).round(5).tolist())","e1643d95":"# prices_df1 = pd.read_parquet(EBASE+'\/df_prices1.pq') #\u660e\u793a\u6570\u636e\u7684\u4ef7\u683c\u77e9\u9635\n# prices_df2 = pd.concat(Parallel(n_jobs=4, verbose=51)(delayed(calc_prices)(r,False) for r in stock_ids)) #\u5728\u7ebf\u4e0a\u662ftest\u90e8\u5206\u7684\u4ef7\u683c\u77e9\u9635\n# prices_df = pd.concat([prices_df1,prices_df2]).reset_index(drop=True)\n# print(f\"\u51fa\u73b0time_id:{prices_df.time_id.nunique()}\")","64d1f444":"'''\u6bcf\u4e2atime_id\u7684\u8d77\u59cb\u4ef7\u683c\u548c\u7ed3\u675f\u4ef7\u683c\u7684query\u77e9\u9635'''\nfirst_df = prices_df[['time_id', 'stock_id', 'first_wap']].pivot('time_id', 'stock_id', 'first_wap')\nlast_df = prices_df[['time_id', 'stock_id', 'last_wap']].pivot('time_id', 'stock_id', 'last_wap')\nfirst_df = first_df.fillna(first_df.mean())\nlast_df = last_df.fillna(first_df.mean())","e51b428e":"def get_nearby_time_id(query_df,template_df):\n    def get_nearby_time_id_(time_id):\n        template_df_=template_df.drop(time_id,axis=1)\n        query = query_df[time_id].values.repeat(len(template_df_.columns)).reshape(-1,len(template_df_.columns))\n        diffs = np.square((template_df_- query)\/query).sum(axis=0)\n        diffs = diffs.sort_values()[:2].reset_index().rename(columns = {'time_id':'tid',0:'loss'})\n        diffs['time_id'] = time_id\n        return diffs    \n    edge_df = pd.concat(Parallel(n_jobs=4, verbose=1)(delayed(get_nearby_time_id_)(time_id) for time_id in tqdm(query_df.columns)))\n    edge_df = edge_df[['time_id','tid','loss']]\n    edge_df .columns = ['a','b','w']\n    return edge_df","8de05549":"'''\u6bcf\u4e2atime_id\u5206\u522b\u6309\u7167\u6536\u5c3e\u4e24\u5934\u5404\u627e5\u4e2a\uff0c\u7136\u540e\u505a\u65e0\u5411\u56fe\u4e14\u53bb\u91cd\uff0c\u8fb9\u7684\u6743\u91cd\u53d6\u51fa\u73b0\u8fc7\u7684\u6700\u5c0f\u503c'''\nlast_edge_df = get_nearby_time_id(last_df.T,first_df.T)\nfirst_edge_df = get_nearby_time_id(first_df.T,last_df.T)\n# last_edge_df = pd.read_parquet(EBASE1+'last_edge_df.pq')\n# first_edge_df = pd.read_parquet(EBASE1+'first_edge_df.pq')\nedge_df = pd.concat([first_edge_df,last_edge_df])\n#edge_df = pd.concat([edge_df,edge_df.rename(columns={'a':'b','b':'a'})])\nedge_df = edge_df.groupby(['a','b'])['w'].min().to_frame('w').reset_index().rename(columns={'a':'b','b':'a'})","0403268b":"edge_df.shape","b6fb83f2":"'''\u9608\u503c\u622a\u65ad'''\nedge_df = edge_df.query('w < @THRD').reset_index(drop = True)\nedge_df.shape","eb5f9db0":"edge_df.shape","19920da2":"'''\u62d3\u5c55\u5230\u6240\u6709stock'''\nedge_dfs = []\nfor stock_id in all_stock_id:\n    edge_df_ = edge_df.copy()\n    edge_df_['a'] = str(stock_id)+'-' +edge_df.a.astype(str)\n    edge_df_['b'] = str(stock_id)+'-' +edge_df.b.astype(str)\n    edge_dfs.append(edge_df_)\nedge_df = pd.concat(edge_dfs).reset_index(drop=True)\nedge_df","a5644291":"'''\u5b64\u7acb\u70b9'''\nisolate_node = list(set(node_df['row_id']) - set(edge_df['a'].unique()))\nisolate_edge_df = pd.DataFrame({'a':isolate_node,'b':isolate_node,'w':0})\nedge_df = pd.concat([edge_df,isolate_edge_df]).reset_index(drop=True)\n\n'''\u91cd\u65b0\u7f16\u7801'''\nrow_id_index_dict = dict(zip(node_df.row_id,node_df.index))\n\nedge_df['a'] = edge_df['a'].map(row_id_index_dict)\nedge_df['b'] = edge_df['b'].map(row_id_index_dict)\n\n'''\u6784\u9020\u8fb9\u4f7f\u7528\u4e86\u7b1b\u5361\u5c14\u79ef\uff0c\u4f46\u662f\u6709\u4e9b\u70b9\u4e0d\u5b58\u5728'''\nedge_df = edge_df[~edge_df.a.isna() & ~edge_df.b.isna()]\nedge_df.shape","c7921479":"class MyFLod:\n    def __init__(self,time_id_list):\n        self.time_id_list=time_id_list\n        \n    def __iter__(self):\n        self.flod = 0\n        self.n_folds = len(self.time_id_list)\n        return self\n    \n    def __next__(self):\n        if self.flod >= self.n_folds:\n            raise StopIteration\n        time_ids = np.arange(self.n_folds).astype(int)    \n        time_ids = np.delete(time_ids,obj=self.flod, axis=0) \n        val_time_id = list(self.time_id_list[self.flod])\n        trn_time_id = \\\n        list(self.time_id_list[time_ids[0]])+\\\n        list(self.time_id_list[time_ids[1]])+\\\n        list(self.time_id_list[time_ids[2]])+\\\n        list(self.time_id_list[time_ids[3]])\n        self.flod += 1\n        return trn_time_id,val_time_id\n            \n\nknn_flod = MyFLod(knn_values)","b34ce08d":"class GS_CFG():\n    save_path = 'gs.pth' #\u6a21\u578b\u8def\u5f84\n    lr = 0.0005 #\u5b66\u4e60\u7387\n    label_col = 'fraud'\n    batch_size = 512  # \u6bcf\u6279\u7684\u6837\u672c\u6570\u91cf\n    epoch = 80  # \u904d\u5386\u591a\u5c11\u904d\u6837\u672c\u96c6\n    verbose = 1\n    output_num = 1\n    fea_num = len(num_col)\n    stock_embedding_size = 24","da8aa60a":"if len(tst) == 3:\n    GS_CFG.epoch=2","66583d21":"def RMSPELoss(y_pred, y_true):\n    return torch.sqrt(torch.mean(((y_true - y_pred) \/ y_true) ** 2 ))\ndef rmspe(y_true, y_pred):\n    # Function to calculate the root mean squared percentage error\n    return np.sqrt(np.mean(np.square((y_true - y_pred) \/ y_true)))","1bc1b39c":"class Batch(NamedTuple):\n    x: Tensor\n    y: Tensor\n    adjs: List[EdgeIndex]\n\nclass OptGNNDM(pl.LightningDataModule):\n    def __init__(self, train_mask,val_mask,test_mask):\n        super().__init__()\n        self.x = torch.FloatTensor(node_df[['stock_id']+num_col].values)\n        self.y = torch.tensor(node_df['target']).reshape(-1,1)\n        self.edge_index = torch.LongTensor(edge_df.drop_duplicates(['a','b'])[['a','b']].values).t()\n        self.train_mask = train_mask\n        self.val_mask = val_mask\n        self.test_mask = test_mask\n        \n    def train_dataloader(self):\n        return NeighborSampler(self.edge_index, node_idx=self.train_mask,\n                               sizes=[-1,-1,-1], return_e_id=True,\n                               transform=self.convert_batch, batch_size=GS_CFG.batch_size,\n                               num_workers=8, shuffle=True)\n\n    def val_dataloader(self):\n        return NeighborSampler(self.edge_index, node_idx=self.val_mask,\n                               sizes=[-1,-1,-1], return_e_id=True,\n                               transform=self.convert_batch, batch_size=GS_CFG.batch_size,\n                               num_workers=4, shuffle=False)\n\n    def test_dataloader(self):\n        return NeighborSampler(self.edge_index, node_idx=self.test_mask,\n                               sizes=[-1,-1,-1], return_e_id=True,\n                               transform=self.convert_batch, batch_size=GS_CFG.batch_size,\n                               num_workers=4, shuffle=False)\n\n    def convert_batch(self, batch_size, n_id, adjs):\n        return Batch(\n            x=self.x[n_id],\n            y=self.y[n_id[:batch_size]],\n            adjs=adjs,\n        )","344fdb64":"class GraphSAGE(pl.LightningModule):\n    def __init__(self, in_channels: int, out_channels: int, hidden_channels: int = 128,num_layers=3):\n        super().__init__()\n        #self.save_hyperparameters()\n        self.num_layers = num_layers\n        self.hidden_channels = hidden_channels\n        \n        hidden_channels = self.hidden_channels\n        self.convs = ModuleList()\n        self.convs.append(TransformerConv(in_channels, hidden_channels\/\/heads,heads=heads))\n        self.convs.append(TransformerConv(hidden_channels, hidden_channels\/\/heads,heads=heads))\n        self.convs.append(TransformerConv(hidden_channels, hidden_channels\/\/heads,heads=heads))\n        self.embedding = nn.Embedding(127, GS_CFG.stock_embedding_size)\n        self.Lin1 = nn.Linear(hidden_channels,out_channels)\n                 \n    def forward(self, x: Tensor, adjs: List[EdgeIndex]) -> Tensor:\n        cat_data = x[:,0].long()\n        num_data = x[:,1:]\n        x = torch.cat([self.embedding(cat_data),num_data],axis=1)\n        for i, (edge_index, _, size) in enumerate(adjs):\n            x_target = x[:size[1]]  # Target nodes are always placed first.\n            x = self.convs[i]((x, x_target), edge_index)\n            x = F.relu(x)\n        x = self.Lin1(x)\n        return x\n\n    def training_step(self, batch: Batch, batch_idx: int):\n        x, y, adjs = batch\n        out = model(x, adjs)\n        loss = RMSPELoss(out, y)\n        self.log(\"train_loss\", loss, prog_bar=True, on_step=False, on_epoch=True, logger=True)\n        return loss\n    \n    def validation_step(self, batch: Batch, batch_idx: int):\n        x, y, adjs = batch\n        y_pred = model(x, adjs)\n        \n        return {'y_pred':y_pred,'y':y}\n        #loss = RMSPELoss(out, y)\n        #self.log(\"valid_loss\", loss, prog_bar=True, on_step=False, on_epoch=True, logger=True)\n\n    def validation_epoch_end(self, outputs):\n        y_pred = torch.cat([out['y_pred'] for out in outputs], dim=0)\n        y = torch.cat([out['y'] for out in outputs], dim=0)\n        loss = RMSPELoss(y_pred,y)\n        self.log(\"valid_loss\", loss, prog_bar=True, on_step=False, on_epoch=True, logger=True)\n        return {'dummy_item': 0} \n    \n    def test_step(self, batch: Batch, batch_idx: int):\n        x, y, adjs = batch\n        y_pred = model(x, adjs)\n        return {'y_pred': y_pred}\n    \n    def test_epoch_end(self, outputs):\n        y_pred = torch.cat([out['y_pred'] for out in outputs], dim=0)\n        y_pred = y_pred.detach().cpu().numpy().reshape(-1)\n        self.test_y_pred = y_pred  # Save prediction internally for easy access\n        # We need to return something \n        return {'dummy_item': 0} \n    \n    def configure_optimizers(self):\n        optimizer = torch.optim.Adam(self.parameters(), lr=GS_CFG.lr)\n        scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience = 3,threshold=0.001,  factor = 0.5, verbose =True)\n        return {\n           'optimizer': optimizer,\n           'lr_scheduler': scheduler, # Changed scheduler to lr_scheduler\n           'monitor': 'valid_loss'\n       }","77a5fd82":"import gc\ndel trn_df\ngc.collect()","b272b11a":"edge_index = torch.LongTensor(edge_df.drop_duplicates(['a','b'])[['a','b']].values).t()\nx = torch.FloatTensor(node_df[['stock_id']+num_col].values)","fe51f42b":"model_list = pickle.load(open(EBASE1+\"0928b_model_list25\", \"rb\"))","e66ed6c4":"tst_preds = 0","aec183f8":"model_list_ = model_list[0:5]\nn_folds = 5\nscores = []\ntst_pred = 0\n\nprint(edge_index.shape)\nfor n_count,(trn_time_id,val_time_id) in enumerate(iter(knn_flod)):\n    flod = n_count+1\n    print('CV {}\/{}'.format(flod, n_folds))\n    \n    train_mask = torch.tensor(node_df.time_id.isin(trn_time_id))\n    val_mask = torch.tensor(node_df.time_id.isin(val_time_id))\n    test_mask = ~(train_mask | val_mask)\n    opt_gnn_dm = OptGNNDM(train_mask,val_mask,test_mask)\n\n    model = GraphSAGE(GS_CFG.fea_num+GS_CFG.stock_embedding_size, \n                      GS_CFG.output_num)\n    model.load_state_dict(model_list_[n_count])\n    trainer = Trainer(gpus=1)\n    trainer.test(model, opt_gnn_dm.test_dataloader())\n    tst_pred += model.test_y_pred.reshape(1,-1)[0].clip(0,0.09)\n    scores.append(trainer.validate(model, opt_gnn_dm.val_dataloader())[0]['valid_loss'])\n    del trainer,model,opt_gnn_dm\n    gc.collect()\nprint(np.array(scores).round(5),'cv',np.mean(scores).round(5))\nprint(tst_pred\/flod)\ntst_preds += tst_pred\/flod","df1d89a5":"model_list_ = model_list[5:10]\nn_folds = 5\nscores = []\ntst_pred = 0\n\nprint(edge_index.shape)\nfor n_count,(trn_time_id,val_time_id) in enumerate(iter(knn_flod)):\n    flod = n_count+1\n    print('CV {}\/{}'.format(flod, n_folds))\n    \n    train_mask = torch.tensor(node_df.time_id.isin(trn_time_id))\n    val_mask = torch.tensor(node_df.time_id.isin(val_time_id))\n    test_mask = ~(train_mask | val_mask)\n    opt_gnn_dm = OptGNNDM(train_mask,val_mask,test_mask)\n\n    model = GraphSAGE(GS_CFG.fea_num+GS_CFG.stock_embedding_size, \n                      GS_CFG.output_num)\n    model.load_state_dict(model_list_[n_count])\n    trainer = Trainer(gpus=1)\n    trainer.test(model, opt_gnn_dm.test_dataloader())\n    tst_pred += model.test_y_pred.reshape(1,-1)[0].clip(0,0.09)\n    scores.append(trainer.validate(model, opt_gnn_dm.val_dataloader())[0]['valid_loss'])\n    del trainer,model,opt_gnn_dm\n    gc.collect()\nprint(np.array(scores).round(5),'cv',np.mean(scores).round(5))\nprint(tst_pred\/flod)\ntst_preds += tst_pred\/flod","7fa4c81d":"model_list_ = model_list[10:15]\nn_folds = 5\nscores = []\ntst_pred = 0\n\nprint(edge_index.shape)\nfor n_count,(trn_time_id,val_time_id) in enumerate(iter(knn_flod)):\n    flod = n_count+1\n    print('CV {}\/{}'.format(flod, n_folds))\n    \n    train_mask = torch.tensor(node_df.time_id.isin(trn_time_id))\n    val_mask = torch.tensor(node_df.time_id.isin(val_time_id))\n    test_mask = ~(train_mask | val_mask)\n    opt_gnn_dm = OptGNNDM(train_mask,val_mask,test_mask)\n\n    model = GraphSAGE(GS_CFG.fea_num+GS_CFG.stock_embedding_size, \n                      GS_CFG.output_num)\n    model.load_state_dict(model_list_[n_count])\n    trainer = Trainer(gpus=1)\n    trainer.test(model, opt_gnn_dm.test_dataloader())\n    tst_pred += model.test_y_pred.reshape(1,-1)[0].clip(0,0.09)\n    scores.append(trainer.validate(model, opt_gnn_dm.val_dataloader())[0]['valid_loss'])\n    del trainer,model,opt_gnn_dm\n    gc.collect()\nprint(np.array(scores).round(5),'cv',np.mean(scores).round(5))\nprint(tst_pred\/flod)\ntst_preds += tst_pred\/flod","069a8869":"model_list_ = model_list[15:20]\nn_folds = 5\nscores = []\ntst_pred = 0\n\nprint(edge_index.shape)\nfor n_count,(trn_time_id,val_time_id) in enumerate(iter(knn_flod)):\n    flod = n_count+1\n    print('CV {}\/{}'.format(flod, n_folds))\n    \n    train_mask = torch.tensor(node_df.time_id.isin(trn_time_id))\n    val_mask = torch.tensor(node_df.time_id.isin(val_time_id))\n    test_mask = ~(train_mask | val_mask)\n    opt_gnn_dm = OptGNNDM(train_mask,val_mask,test_mask)\n\n    model = GraphSAGE(GS_CFG.fea_num+GS_CFG.stock_embedding_size, \n                      GS_CFG.output_num)\n    model.load_state_dict(model_list_[n_count])\n    trainer = Trainer(gpus=1)\n    trainer.test(model, opt_gnn_dm.test_dataloader())\n    tst_pred += model.test_y_pred.reshape(1,-1)[0].clip(0,0.09)\n    scores.append(trainer.validate(model, opt_gnn_dm.val_dataloader())[0]['valid_loss'])\n    del trainer,model,opt_gnn_dm\n    gc.collect()\nprint(np.array(scores).round(5),'cv',np.mean(scores).round(5))\nprint(tst_pred\/flod)\ntst_preds += tst_pred\/flod","054c43d1":"model_list_ = model_list[20:25]\nn_folds = 5\nscores = []\ntst_pred = 0\n\nprint(edge_index.shape)\nfor n_count,(trn_time_id,val_time_id) in enumerate(iter(knn_flod)):\n    flod = n_count+1\n    print('CV {}\/{}'.format(flod, n_folds))\n    \n    train_mask = torch.tensor(node_df.time_id.isin(trn_time_id))\n    val_mask = torch.tensor(node_df.time_id.isin(val_time_id))\n    test_mask = ~(train_mask | val_mask)\n    opt_gnn_dm = OptGNNDM(train_mask,val_mask,test_mask)\n\n    model = GraphSAGE(GS_CFG.fea_num+GS_CFG.stock_embedding_size, \n                      GS_CFG.output_num)\n    model.load_state_dict(model_list_[n_count])\n    trainer = Trainer(gpus=1)\n    trainer.test(model, opt_gnn_dm.test_dataloader())\n    tst_pred += model.test_y_pred.reshape(1,-1)[0].clip(0,0.09)\n    scores.append(trainer.validate(model, opt_gnn_dm.val_dataloader())[0]['valid_loss'])\n    del trainer,model,opt_gnn_dm\n    gc.collect()\nprint(np.array(scores).round(5),'cv',np.mean(scores).round(5))\nprint(tst_pred\/flod)\ntst_preds += tst_pred\/flod","54036420":"tst_preds = tst_preds\/(len(model_list)\/\/5)\ntst_preds","9eb02cb1":"tst_preds1 = tst_preds","809d4325":"'''\u6bcf\u4e2atime_id\u7684\u8d77\u59cb\u4ef7\u683c\u548c\u7ed3\u675f\u4ef7\u683c\u7684query\u77e9\u9635'''\nfirst_df = prices_df[['time_id', 'stock_id', 'first_wap']].pivot('time_id', 'stock_id', 'first_wap')\nlast_df = prices_df[['time_id', 'stock_id', 'last_wap']].pivot('time_id', 'stock_id', 'last_wap')\nfirst_df = first_df.fillna(first_df.mean())\nlast_df = last_df.fillna(first_df.mean())","26683cae":"def get_nearby_time_id(query_df,template_df):\n    def get_nearby_time_id_(time_id):\n        template_df_=template_df.drop(time_id,axis=1)\n        query = query_df[time_id].values.repeat(len(template_df_.columns)).reshape(-1,len(template_df_.columns))\n        diffs = np.square((template_df_- query)\/query).sum(axis=0)\n        diffs = diffs.sort_values()[:3].reset_index().rename(columns = {'time_id':'tid',0:'loss'})\n        diffs['time_id'] = time_id\n        return diffs    \n    edge_df = pd.concat(Parallel(n_jobs=4, verbose=1)(delayed(get_nearby_time_id_)(time_id) for time_id in tqdm(query_df.columns)))\n    edge_df = edge_df[['time_id','tid','loss']]\n    edge_df .columns = ['a','b','w']\n    return edge_df","bb787fbd":"'''\u6bcf\u4e2atime_id\u5206\u522b\u6309\u7167\u6536\u5c3e\u4e24\u5934\u5404\u627e5\u4e2a\uff0c\u7136\u540e\u505a\u65e0\u5411\u56fe\u4e14\u53bb\u91cd\uff0c\u8fb9\u7684\u6743\u91cd\u53d6\u51fa\u73b0\u8fc7\u7684\u6700\u5c0f\u503c'''\nlast_edge_df = get_nearby_time_id(last_df.T,first_df.T)\nfirst_edge_df = get_nearby_time_id(first_df.T,last_df.T)\n# last_edge_df = pd.read_parquet(EBASE1+'last_edge_df.pq')\n# first_edge_df = pd.read_parquet(EBASE1+'first_edge_df.pq')\nedge_df = pd.concat([first_edge_df,last_edge_df])\n#edge_df = pd.concat([edge_df,edge_df.rename(columns={'a':'b','b':'a'})])\nedge_df = edge_df.groupby(['a','b'])['w'].min().to_frame('w').reset_index().rename(columns={'a':'b','b':'a'})","4874ddad":"edge_df.shape","0ea382d2":"'''\u9608\u503c\u622a\u65ad'''\nedge_df = edge_df.query('w < @THRD').reset_index(drop = True)\nedge_df.shape","b7de6500":"'''\u62d3\u5c55\u5230\u6240\u6709stock'''\nedge_dfs = []\nfor stock_id in all_stock_id:\n    edge_df_ = edge_df.copy()\n    edge_df_['a'] = str(stock_id)+'-' +edge_df.a.astype(str)\n    edge_df_['b'] = str(stock_id)+'-' +edge_df.b.astype(str)\n    edge_dfs.append(edge_df_)\nedge_df = pd.concat(edge_dfs).reset_index(drop=True)\nedge_df","ec886ef1":"'''\u5b64\u7acb\u70b9'''\nisolate_node = list(set(node_df['row_id']) - set(edge_df['a'].unique()))\nisolate_edge_df = pd.DataFrame({'a':isolate_node,'b':isolate_node,'w':0})\nedge_df = pd.concat([edge_df,isolate_edge_df]).reset_index(drop=True)\n\n'''\u91cd\u65b0\u7f16\u7801'''\nrow_id_index_dict = dict(zip(node_df.row_id,node_df.index))\n\nedge_df['a'] = edge_df['a'].map(row_id_index_dict)\nedge_df['b'] = edge_df['b'].map(row_id_index_dict)\n\n'''\u6784\u9020\u8fb9\u4f7f\u7528\u4e86\u7b1b\u5361\u5c14\u79ef\uff0c\u4f46\u662f\u6709\u4e9b\u70b9\u4e0d\u5b58\u5728'''\nedge_df = edge_df[~edge_df.a.isna() & ~edge_df.b.isna()]\nedge_df.shape","868399b3":"class MyFLod:\n    def __init__(self,time_id_list):\n        self.time_id_list=time_id_list\n        \n    def __iter__(self):\n        self.flod = 0\n        self.n_folds = len(self.time_id_list)\n        return self\n    \n    def __next__(self):\n        if self.flod >= self.n_folds:\n            raise StopIteration\n        time_ids = np.arange(self.n_folds).astype(int)    \n        time_ids = np.delete(time_ids,obj=self.flod, axis=0) \n        val_time_id = list(self.time_id_list[self.flod])\n        trn_time_id = \\\n        list(self.time_id_list[time_ids[0]])+\\\n        list(self.time_id_list[time_ids[1]])+\\\n        list(self.time_id_list[time_ids[2]])+\\\n        list(self.time_id_list[time_ids[3]])\n        self.flod += 1\n        return trn_time_id,val_time_id\n            \n\nknn_flod = MyFLod(knn_values)","ca6361f7":"class GS_CFG():\n    save_path = 'gs.pth' #\u6a21\u578b\u8def\u5f84\n    lr = 0.0005 #\u5b66\u4e60\u7387\n    label_col = 'fraud'\n    batch_size = 512  # \u6bcf\u6279\u7684\u6837\u672c\u6570\u91cf\n    epoch = 80  # \u904d\u5386\u591a\u5c11\u904d\u6837\u672c\u96c6\n    verbose = 1\n    output_num = 1\n    fea_num = len(num_col)\n    stock_embedding_size = 24","c3bdf122":"model_list = pickle.load(open(EBASE1+\"0928_model_list25\", \"rb\"))","00c99b31":"tst_preds = 0","a9399abb":"model_list_ = model_list[0:5]\nn_folds = 5\nscores = []\ntst_pred = 0\n\nprint(edge_index.shape)\nfor n_count,(trn_time_id,val_time_id) in enumerate(iter(knn_flod)):\n    flod = n_count+1\n    print('CV {}\/{}'.format(flod, n_folds))\n    \n    train_mask = torch.tensor(node_df.time_id.isin(trn_time_id))\n    val_mask = torch.tensor(node_df.time_id.isin(val_time_id))\n    test_mask = ~(train_mask | val_mask)\n    opt_gnn_dm = OptGNNDM(train_mask,val_mask,test_mask)\n\n    model = GraphSAGE(GS_CFG.fea_num+GS_CFG.stock_embedding_size, \n                      GS_CFG.output_num)\n    model.load_state_dict(model_list_[n_count])\n    trainer = Trainer(gpus=1)\n    trainer.test(model, opt_gnn_dm.test_dataloader())\n    tst_pred += model.test_y_pred.reshape(1,-1)[0].clip(0,0.09)\n    scores.append(trainer.validate(model, opt_gnn_dm.val_dataloader())[0]['valid_loss'])\n    del trainer,model,opt_gnn_dm\n    gc.collect()\nprint(np.array(scores).round(5),'cv',np.mean(scores).round(5))\nprint(tst_pred\/flod)\ntst_preds += tst_pred\/flod","a9a94682":"model_list_ = model_list[5:10]\nn_folds = 5\nscores = []\ntst_pred = 0\n\nprint(edge_index.shape)\nfor n_count,(trn_time_id,val_time_id) in enumerate(iter(knn_flod)):\n    flod = n_count+1\n    print('CV {}\/{}'.format(flod, n_folds))\n    \n    train_mask = torch.tensor(node_df.time_id.isin(trn_time_id))\n    val_mask = torch.tensor(node_df.time_id.isin(val_time_id))\n    test_mask = ~(train_mask | val_mask)\n    opt_gnn_dm = OptGNNDM(train_mask,val_mask,test_mask)\n\n    model = GraphSAGE(GS_CFG.fea_num+GS_CFG.stock_embedding_size, \n                      GS_CFG.output_num)\n    model.load_state_dict(model_list_[n_count])\n    trainer = Trainer(gpus=1)\n    trainer.test(model, opt_gnn_dm.test_dataloader())\n    tst_pred += model.test_y_pred.reshape(1,-1)[0].clip(0,0.09)\n    scores.append(trainer.validate(model, opt_gnn_dm.val_dataloader())[0]['valid_loss'])\n    del trainer,model,opt_gnn_dm\n    gc.collect()\nprint(np.array(scores).round(5),'cv',np.mean(scores).round(5))\nprint(tst_pred\/flod)\ntst_preds += tst_pred\/flod","4c2099db":"model_list_ = model_list[10:15]\nn_folds = 5\nscores = []\ntst_pred = 0\n\nprint(edge_index.shape)\nfor n_count,(trn_time_id,val_time_id) in enumerate(iter(knn_flod)):\n    flod = n_count+1\n    print('CV {}\/{}'.format(flod, n_folds))\n    \n    train_mask = torch.tensor(node_df.time_id.isin(trn_time_id))\n    val_mask = torch.tensor(node_df.time_id.isin(val_time_id))\n    test_mask = ~(train_mask | val_mask)\n    opt_gnn_dm = OptGNNDM(train_mask,val_mask,test_mask)\n\n    model = GraphSAGE(GS_CFG.fea_num+GS_CFG.stock_embedding_size, \n                      GS_CFG.output_num)\n    model.load_state_dict(model_list_[n_count])\n    trainer = Trainer(gpus=1)\n    trainer.test(model, opt_gnn_dm.test_dataloader())\n    tst_pred += model.test_y_pred.reshape(1,-1)[0].clip(0,0.09)\n    scores.append(trainer.validate(model, opt_gnn_dm.val_dataloader())[0]['valid_loss'])\n    del trainer,model,opt_gnn_dm\n    gc.collect()\nprint(np.array(scores).round(5),'cv',np.mean(scores).round(5))\nprint(tst_pred\/flod)\ntst_preds += tst_pred\/flod","25afd69a":"model_list_ = model_list[15:20]\nn_folds = 5\nscores = []\ntst_pred = 0\n\nprint(edge_index.shape)\nfor n_count,(trn_time_id,val_time_id) in enumerate(iter(knn_flod)):\n    flod = n_count+1\n    print('CV {}\/{}'.format(flod, n_folds))\n    \n    train_mask = torch.tensor(node_df.time_id.isin(trn_time_id))\n    val_mask = torch.tensor(node_df.time_id.isin(val_time_id))\n    test_mask = ~(train_mask | val_mask)\n    opt_gnn_dm = OptGNNDM(train_mask,val_mask,test_mask)\n\n    model = GraphSAGE(GS_CFG.fea_num+GS_CFG.stock_embedding_size, \n                      GS_CFG.output_num)\n    model.load_state_dict(model_list_[n_count])\n    trainer = Trainer(gpus=1)\n    trainer.test(model, opt_gnn_dm.test_dataloader())\n    tst_pred += model.test_y_pred.reshape(1,-1)[0].clip(0,0.09)\n    scores.append(trainer.validate(model, opt_gnn_dm.val_dataloader())[0]['valid_loss'])\n    del trainer,model,opt_gnn_dm\n    gc.collect()\nprint(np.array(scores).round(5),'cv',np.mean(scores).round(5))\nprint(tst_pred\/flod)\ntst_preds += tst_pred\/flod","2fca483a":"model_list_ = model_list[20:25]\nn_folds = 5\nscores = []\ntst_pred = 0\n\nprint(edge_index.shape)\nfor n_count,(trn_time_id,val_time_id) in enumerate(iter(knn_flod)):\n    flod = n_count+1\n    print('CV {}\/{}'.format(flod, n_folds))\n    \n    train_mask = torch.tensor(node_df.time_id.isin(trn_time_id))\n    val_mask = torch.tensor(node_df.time_id.isin(val_time_id))\n    test_mask = ~(train_mask | val_mask)\n    opt_gnn_dm = OptGNNDM(train_mask,val_mask,test_mask)\n\n    model = GraphSAGE(GS_CFG.fea_num+GS_CFG.stock_embedding_size, \n                      GS_CFG.output_num)\n    model.load_state_dict(model_list_[n_count])\n    trainer = Trainer(gpus=1)\n    trainer.test(model, opt_gnn_dm.test_dataloader())\n    tst_pred += model.test_y_pred.reshape(1,-1)[0].clip(0,0.09)\n    scores.append(trainer.validate(model, opt_gnn_dm.val_dataloader())[0]['valid_loss'])\n    del trainer,model,opt_gnn_dm\n    gc.collect()\nprint(np.array(scores).round(5),'cv',np.mean(scores).round(5))\nprint(tst_pred\/flod)\ntst_preds += tst_pred\/flod","ae797691":"tst_preds = tst_preds\/(len(model_list)\/\/5)\ntst_preds","1e28bdb4":"tst['target'] = tst_preds1*0.4+tst_preds*0.6\ntst[['row_id','target']].to_csv('submission.csv', index=False)\ntst.head()","348b7724":"import sys as sys\nobject_v_li = []\nobject_size_li = []\nfor object_v in dir():\n    \n    getsizeof_str = 'sys.getsizeof('+ object_v +')'\n    try:\n        size = eval(getsizeof_str)\n    except NameError as e:\n        print('except:', e)\n        continue\n        \n    #print(object_v)\n    object_v_li.append(object_v)\n    object_size_li.append(size)\n    \ndddd = pd.DataFrame({'object_name':object_v_li,'size':pd.Series(object_size_li)\/1024\/1024})\ndddd.sort_values('size',ascending = False).head(10)","f92cbafc":"# GNN2","1ff68995":"# edge","c1c1a018":"# node","b4c5d7b7":"# GNN","1c5a5606":"# edge2"}}