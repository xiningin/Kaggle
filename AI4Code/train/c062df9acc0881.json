{"cell_type":{"6acfc10a":"code","be417aa6":"code","555e12f9":"code","e557a5d1":"code","1ce93db3":"code","ce6e4130":"code","ddb04633":"code","7668ba38":"code","c36e9b88":"code","e049717a":"code","449e928e":"code","4aa6f79c":"code","a333a85a":"code","d5553d75":"code","3ef104ae":"code","2a43c7b4":"code","7ea98d87":"code","022ab3c6":"code","2c2f05bd":"code","371074e3":"markdown","7011614b":"markdown","1e7a924d":"markdown","28e1bfa8":"markdown","86036daf":"markdown","d3c10972":"markdown","0f5b8e02":"markdown","f158e89f":"markdown","2aed1a04":"markdown","07199b33":"markdown"},"source":{"6acfc10a":"# update torch and torch vision\n!pip install -q torch==1.6.0+cu101 torchvision==0.7.0+cu101 -f https:\/\/download.pytorch.org\/whl\/torch_stable.html","be417aa6":"# install kornia, we will give it a try to accelarate our preprocessing\n!pip install -q --upgrade kornia\n!pip install -q allennlp==1.1.0.rc4","555e12f9":"# and install fastai2\n!pip install -q --upgrade fastai","e557a5d1":"# Now, let's check the installed libraries\n\nimport torch\nprint(torch.__version__)\nprint(torch.cuda.is_available())\n\nimport fastai\nprint(fastai.__version__)\n\nfrom fastai.vision.all import *","1ce93db3":"# other imports\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom pathlib import Path\nfrom PIL import Image","ce6e4130":"def open_tif(fn, chnls=None, cls=torch.Tensor):\n    im = (np.array(Image.open(fn))).astype('float32')\n    return cls(im)","ddb04633":"# Just checking our function\nbase_path = Path('..\/input\/95cloud-cloud-segmentation-on-satellite-images\/95-cloud_training_only_additional_to38-cloud\/train_red_additional_to38cloud')\n\nopen_tif(base_path\/'red_patch_100_5_by_16_LC08_L1TP_035031_20160120_20170224_01_T1.TIF').shape","7668ba38":"# Note that we can pass the desired output class and it casts automatically. Here we receive a TensorImage\nopen_tif(base_path\/'red_patch_100_5_by_16_LC08_L1TP_035031_20160120_20170224_01_T1.TIF', cls=TensorImage)","c36e9b88":"# Note that we can cast to the desired class implicitly or explicitly\nmask = TensorMask(open_tif('..\/input\/95cloud-cloud-segmentation-on-satellite-images\/95-cloud_training_only_additional_to38-cloud\/train_gt_additional_to38cloud\/gt_patch_100_5_by_16_LC08_L1TP_030034_20170815_20170825_01_T1.TIF'))\nmask.show()","e049717a":"# get items from both datasets\nitems_95 = get_files(base_path, extensions='.TIF')\nitems_38 = get_files('\/kaggle\/input\/38cloud-cloud-segmentation-in-satellite-images\/38-Cloud_training\/train_red\/', extensions='.TIF')\nall_items = items_95 + items_38","449e928e":"# now select just the non empty ones\nn_empty = pd.read_csv('\/kaggle\/input\/95cloud-cloud-segmentation-on-satellite-images\/95-cloud_training_only_additional_to38-cloud\/training_patches_95-cloud_nonempty.csv')\n\ndef non_empty(item):\n    \n    if n_empty.name.isin([item.stem[4:]]).any():\n        return True\n    else:\n        return False\n    \nitems_mask = all_items.map(non_empty)\nitems = all_items[items_mask]\nitems","4aa6f79c":"# The map_filename function makes it easier to map from one folder to another by replacing strings\ndef map_filename(base_fn, str1, str2):\n    return Path(str(base_fn).replace(str1, str2))\n\ndef get_filenames(red_filename):\n    return [red_filename,\n            map_filename(red_filename, str1='red', str2='green'),\n            map_filename(red_filename, str1='red', str2='blue'),\n            map_filename(red_filename, str1='red', str2='nir'),\n           ]\n\n\n# the open multi-spectral tif function will be in charge of opening the separate tifs and collate them\ndef open_ms_tif(files):\n    ms_img = None\n    \n    for path in files:\n        img = open_tif(path)\n        \n        if ms_img is None:\n            ms_img = img[None]\n        else:\n            ms_img = np.concatenate([ms_img, img[None]], axis=0)\n            \n    return TensorImage(ms_img)\n    \n\n# Check if it is mapping correcly\nfor path in get_filenames(items[0]):\n    assert path.exists() == True\n\n# check if we can open an image\nopen_ms_tif(get_filenames(items[0])).shape","a333a85a":"ImageBlock = TransformBlock(type_tfms=[get_filenames,\n                                       open_ms_tif, \n                                       lambda x: x\/10000\n                                      ])","d5553d75":"MaskBlock = TransformBlock(type_tfms=[partial(open_tif, cls=TensorMask),\n                                      AddMaskCodes(codes=['clear', 'cloudy'])\n                                     ])","3ef104ae":"# in this example, we don't need to specify the get_items function \n# because we will pass the list of files as the source\ndb = DataBlock(blocks=(ImageBlock, MaskBlock),\n               get_y = partial(map_filename, str1='red', str2='gt'),\n               splitter=RandomSplitter(valid_pct=0.5)\n              )\n\n# We will call db.summary to see if everything goes well\n# Note that our final sample is composed by a tuple (X, Y) as we wanted\n%time db.summary(source=items)","2a43c7b4":"dl = db.dataloaders(source=items, bs=12) #, num_workers=0)\n\n# now, let's check if the batch is being created correctly\nbatch = dl.one_batch()\nbatch[0].shape, batch[1].shape\n\n# Remember that we cannot use visual funcs like show_batch as the TensorImage does not support multi channel images\n# for that, we have to subclass the TensorImage as I explained in Medium:\n# How to create a DataBlock for Multispectral Satellite Image Segmentation with the Fastai-v2\n# https:\/\/towardsdatascience.com\/how-to-create-a-datablock-for-multispectral-satellite-image-segmentation-with-the-fastai-v2-bc5e82f4eb5","7ea98d87":"def acc_metric(input, target):\n    target = target.squeeze(1)\n    return (input.argmax(dim=1)==target).float().mean()\n\ndef loss_fn(pred, targ):\n    targ[targ==255] = 1\n    return torch.nn.functional.cross_entropy(pred, targ.squeeze(1).type(torch.long))\n\nlearn = unet_learner(dl, resnet18, n_in=4, n_out=2, pretrained=False, loss_func=loss_fn, metrics=acc_metric)","022ab3c6":"# At this notebook version we are starting from a previously saved checkpoint\n# We will then, load the previous weights and train for 5 more epochs\n\n# The final objective is to compare the final accuracy, with the accuracy introducing augmentations\ntry:\n    learn.load('\/kaggle\/input\/remotesensing-fastai2-simpletraining\/models\/95_cloud-resnet18-50-10epochs.learner')\n    print('Loaded sucessfully')\n    learn.fit_one_cycle(5, lr_max=1e-4, wd=1e-2)\n    learn.save('.\/95_cloud-resnet18-50-10epochs.learner')\nexcept:\n    \n    print('failed loading checkpoint')\n    \n\n# learn.lr_find()","2c2f05bd":"# And we save our first model to test it later\n","371074e3":"# Cloud detection using Fastai2 \n### *** This version is trained with 50% version of the two datasets and 10 epochs\nThe objective of this notebook is to show how to use the recently released Fastai2 library for the cloud detection task.<br>\nThe Fastai2 library has been launched 21\/08\/2020, as well as a book and a brand new 2020 course. More information can be found in their webpage: https:\/\/www.fast.ai\/2020\/08\/21\/fastai2-launch\/.\n\nFor a brief explanation on how to use the Fastai2 data blocks to create processing pipelines, please refer to my story on TowardsDataScience: https:\/\/towardsdatascience.com\/how-to-create-a-datablock-for-multispectral-satellite-image-segmentation-with-the-fastai-v2-bc5e82f4eb5\n\nOne difference to this dataset is that it keeps the bands in separate folders.\n\nIn this notebook we will see how to:\n- Install fastai 2 into kaggle and use the GPU\n- Create a customized datablock in Fastai2 to collate files from different folders into a single output\n- Join files from two different Kaggle datasets (38 cloud and 98 cloud)\n- Find the best Learning Rate using the LR-Finder\n- Train a U-Net architecture to detect clouds\n- Evaluate the model\n\nWe will not cover the data itself, because there is already a good EDA for this dataset:\nhttps:\/\/www.kaggle.com\/polavr\/cloud95-exploracion-de-datos\n\nSo, Let's go:\n\nAs fastai2 is not yet default in Kaggle, our first task is to install the library and its dependencies.\n","7011614b":"## Combining the blocks into a DataBlock","1e7a924d":"### MaskBlock (Y or Target)\nFor the mask block, we can use the get_y function to map the filename (red) to the gt filename.","28e1bfa8":"### Image Block\nAs the files are in separate folders and we want to create a single 4 channels tensor to train with, our first challenge is to collate the images. We will need 2 separate functions:\n- One function to get the base filename and return all 4 filenames\n- Another function to receive this list with 4 filenames and open it into a single tensor\n\nLet's try it:","86036daf":"## Train a model","d3c10972":"## Create a dataloader","0f5b8e02":"## Opening the images","f158e89f":"## Creating the datablock\nNow that we already have a way to open both images (and cast them to the desired class) we will create the pipeline for Xs and Ys. This picture summarizes how the DataBlock works.\n![image.png](attachment:image.png)\nAs I did in the Cloud-38 notebook, I will point the items to the red folder, and then derive the other bands from there.","2aed1a04":"In the medium story I created a new class MSTensorImage to keep basic functionalities of Fastai2. However, to keep things really simple here, I will just create a function tha opens the .TIF file (TensorImages doesn't support it) and another function to collate the 4 bands.","07199b33":"## Opening the masks\nFor the masks, we will use the TensorMask class already defined, like so:"}}