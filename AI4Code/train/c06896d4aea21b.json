{"cell_type":{"41939a60":"code","9090a5bb":"code","0dde054a":"code","c6a9dfab":"code","69263011":"code","c45a0056":"code","91390807":"code","6c787b4a":"code","74939c69":"code","0027a7dc":"code","c1cebcec":"code","52ab1c2c":"code","d0c91bea":"code","5e3114ed":"code","026597c9":"code","7c017913":"code","a034940d":"code","1f967021":"code","0f3f4ace":"code","9b1fe881":"code","44b9ed16":"code","1e8abbaa":"code","8d55e162":"code","a5ddd462":"code","92aaee90":"code","955ea670":"code","ee5c2c89":"code","85b1f559":"code","7a2ec911":"code","dfb363db":"code","245cf00d":"code","e5302d03":"code","a7af2e38":"code","283bbb1c":"code","9b494f7e":"code","7c3816a6":"code","66b729bf":"code","7f4f4b52":"code","63b91b86":"code","3dfeabd9":"code","5f9d2ec6":"code","a467aa4d":"code","30b4f7ae":"code","65856f7a":"code","a7ca9720":"code","7adec30f":"code","28cbcced":"code","8228b378":"code","ef07d75e":"code","e8f3ab4f":"code","c66cf20e":"code","fbde4c2b":"code","4a22963c":"code","ad84b2ab":"code","28a325bb":"code","ce3c1e0a":"code","60d4019e":"code","5f3d919f":"code","ae41f2f0":"code","ca185883":"code","3caf9b7f":"code","afa28224":"code","eaf3e183":"code","1d1b08e5":"code","d5acc385":"code","deb7ee32":"code","a9e529e2":"code","7854e74f":"code","5b92d0eb":"code","05a37d81":"code","80d46846":"code","d5aed9a1":"code","60fa7030":"code","5284b002":"code","1a52e1f4":"code","0df41595":"code","c884c3c9":"code","6b054511":"code","6ea153ec":"code","5293ddc3":"code","36cc8945":"code","8835707d":"code","3a7589fc":"code","793f773b":"code","a07964b5":"code","cd0a6f44":"code","091bc36b":"code","c6db9da1":"code","e2f83dd8":"markdown","a9718f49":"markdown","6b14476c":"markdown","dd93e4d2":"markdown","bbc2156d":"markdown","3b355fda":"markdown","0a502069":"markdown","e90ca92d":"markdown","6b06d501":"markdown","1cb4617a":"markdown","3d06291e":"markdown","43f6a2f9":"markdown","fac4844c":"markdown","3914391d":"markdown","d5f26393":"markdown","c0c84ddf":"markdown","07fbc023":"markdown","491145ac":"markdown","025ae153":"markdown","5e33fdf8":"markdown","592c4fb6":"markdown","a4a639f3":"markdown","17d60427":"markdown","5fe94aae":"markdown","b58a94e8":"markdown","d9bd3ec9":"markdown","086404d8":"markdown","1b70a456":"markdown","43103949":"markdown","816b157e":"markdown","da76af19":"markdown","1e3681af":"markdown","e3e567cb":"markdown","dd06d259":"markdown","09f3a634":"markdown","77c774e9":"markdown"},"source":{"41939a60":"import re\nimport pandas as pd\nimport numpy as np\nimport os\nimport random\nimport seaborn as sns \nimport matplotlib.pyplot as plt\nfrom collections import defaultdict, Counter\nfrom sklearn import feature_extraction, linear_model, model_selection, preprocessing, metrics,manifold\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.ensemble import RandomForestClassifier, VotingClassifier\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.experimental import enable_halving_search_cv\nfrom sklearn.model_selection import HalvingGridSearchCV\nimport nltk\nfrom nltk.corpus import stopwords\nfrom nltk.tokenize import word_tokenize \nfrom nltk.stem import PorterStemmer \nimport shelve\nfrom wordcloud import WordCloud \nimport spacy \nimport xgboost as xgb\nfrom xgboost import XGBClassifier","9090a5bb":"train_df=pd.read_csv(\"\/kaggle\/input\/nlp-getting-started\/train.csv\")\ntest_df=pd.read_csv(\"\/kaggle\/input\/nlp-getting-started\/test.csv\")","0dde054a":"train_df.head()","c6a9dfab":"train_df.axes","69263011":"test_df.head()","c45a0056":"test_df.axes","91390807":"nulls = pd.DataFrame({\"Num_Null\": train_df.isnull().sum()})\nnulls[\"Pct_Null\"] = nulls[\"Num_Null\"] \/ train_df.count() * 100\nnulls","6c787b4a":"nulls = pd.DataFrame({\"Num_Null\": test_df.isnull().sum()})\nnulls[\"Pct_Null\"] = nulls[\"Num_Null\"] \/ test_df.count() * 100\nnulls","74939c69":"target_vc = train_df[\"target\"].value_counts(normalize=True)\nprint(\"Non Disaster: {:.2%}, Disaster: {:.2%}\".format(target_vc[0], target_vc[1]))\nsns.barplot(x=target_vc.index, y=target_vc)\nplt.title(\"Histogramme Disaster vs Non-Disaster\")\nplt.xlabel(\"0 = Non-Disaster, 1 = Disaster\")\nplt.show()","0027a7dc":"train_df[\"tweet_length\"] = train_df[\"text\"].apply(len)\ng = sns.FacetGrid(train_df, col=\"target\", height=5)\ng = g.map(sns.histplot, \"tweet_length\")\nplt.suptitle(\"Distribution Tweet Length\")\nplt.show()","c1cebcec":"keywords_vc = pd.DataFrame({\"Count\": (train_df[\"keyword\"].append(test_df[\"keyword\"])).value_counts()})\nsns.set()\nplt.figure(figsize = (10,40))\nsns.barplot(y=keywords_vc[0:300].index, x=keywords_vc[0:300][\"Count\"])\nplt.title(\"Keywords les plus utilis\u00e9s\", fontsize = 20)\nplt.xlabel(\"nb occurences\", fontsize = 15)\nplt.ylabel(\"Keywords\",fontsize = 15)\nplt.show()","52ab1c2c":"disaster_keywords = train_df.loc[train_df[\"target\"] == 1][\"keyword\"].value_counts()\nnondisaster_keywords = train_df.loc[train_df[\"target\"] == 0][\"keyword\"].value_counts()\n\nfig, ax = plt.subplots(1,2, figsize=(20,8))\nsns.barplot(y=disaster_keywords[0:30].index, x=disaster_keywords[0:30], orient='h', ax=ax[0], palette=\"Reds_d\")\nsns.barplot(y=nondisaster_keywords[0:30].index, x=nondisaster_keywords[0:30], orient='h', ax=ax[1], palette=\"Blues_d\")\nax[0].set_title(\"Keywords les plus utilis\u00e9s dans les Disaster Tweets\",fontsize = 25)\nax[0].set_xlabel(\"nb occurences\",fontsize = 20)\nax[1].set_title(\"Keywords les plus utilis\u00e9s dans les Non-Disaster Tweets\",fontsize = 25)\nax[1].set_xlabel(\"nb occurences\",fontsize = 20)\nplt.tight_layout()\nplt.show()","d0c91bea":"#Dictionnaire avec les keywords de train_df et le nombre de fois qu'ils apparaissent dans train_df\nkeywords_proba_df = pd.DataFrame({\"keyword\":train_df[\"keyword\"].value_counts().index,\"Count\": (train_df[\"keyword\"].value_counts().values)})","5e3114ed":"def proba_keywords(n, df=keywords_proba_df[\"keyword\"]):    \n    \"\"\"\n    Arguments :\n    :n: integer\n    :df: liste de keyword\n    :return: [int,float]\n    \n    Cette fonction renvoie un couple d'int \n    dont le premier indique si le keyword df[n] a \u00e9t\u00e9 d\u00e9termin\u00e9 comme tweet disaster(1) ou non (0), \n    et le second indique la probabilit\u00e9 qu'il soit un tweet disaster ou non.\n    \"\"\"\n    tweets_keywords=train_df.loc[train_df[\"keyword\"]==df[n]]\n    tot=0\n    for i in tweets_keywords.index:\n        if tweets_keywords['target'][i]==1:\n            tot+=1\n    if len(tweets_keywords) >0:proba=tot\/len(tweets_keywords)\n    else: proba=-1\n    return([0, proba] if proba<0.4297 else [-1,proba] if proba==-1 else [1, proba])","026597c9":"for i in range(len(keywords_proba_df)):\n    keywords_proba_df.loc[i, \"proba_target\"],keywords_proba_df.loc[i, \"proba\"] =proba_keywords(i)\n    \nkeywords_proba_df.head()","7c017913":"disaster_keywords = train_df.loc[train_df[\"target\"] == 1][\"keyword\"].value_counts()\n\ndk_proba = pd.DataFrame({\"keyword\":disaster_keywords.keys(), \"Count\" :disaster_keywords.values})\n\nfor i in range(len(dk_proba)):\n    dk_proba.loc[i, \"proba_target\"],dk_proba.loc[i, \"proba\"]=proba_keywords(i,disaster_keywords.keys() )\n    \ndk_proba","a034940d":"x = dk_proba[\"Count\"]\ny = dk_proba[\"proba\"]\nplt.plot(x, y)\nplt.xlabel(\"Nombre d'apparitions du keyword dans train\", fontsize=15)\nplt.ylabel(\"Probabilit\u00e9 de disaster\", fontsize=15)\nplt.show()\nplt.close()","1f967021":"non_disaster_keywords = train_df.loc[train_df[\"target\"] == 0][\"keyword\"].value_counts()\n\nndk_proba = pd.DataFrame({\"keyword\":non_disaster_keywords.keys(), \"Count\" :non_disaster_keywords.values})\n\nfor i in range(len(ndk_proba)):\n    ndk_proba.loc[i, \"proba_target\"],ndk_proba.loc[i, \"proba\"]=proba_keywords(i,non_disaster_keywords.keys() )\n    \nndk_proba","0f3f4ace":"x = ndk_proba[\"Count\"]\ny = ndk_proba[\"proba\"]\nfig, ax = plt.subplots()\n\nplt.plot(x, y)\nplt.xlabel(\"Nombre d'apparitions du keyword dans train\", fontsize=15)\nplt.ylabel(\"Probabilit\u00e9 de non disaster\", fontsize=15)\nax.invert_yaxis()\nplt.show()\nplt.close()","9b1fe881":"def cleaning(sentence):\n    \"\"\"\n    Arguments :\n    :sentence: str\n    :return: str \n    \n    Cette fonction renvoie la phrase pass\u00e9e en param\u00e8tre \"n\u00e9toy\u00e9e\".\n    Elle est pass\u00e9e en minuscule, les abr\u00e9viations sont remplac\u00e9es par les mots complets\n    et la ponctuation est enlev\u00e9e.\n    \"\"\"\n    sentence = sentence.lower()\n    sentence = re.sub(r\"won't\", \"will not\", sentence)\n    sentence = re.sub(r\"gon\", \"going\", sentence)\n    sentence = re.sub(r\"gonna\", \"going\", sentence)\n    sentence = re.sub(r\"whats\", \"what is\", sentence)\n    sentence = re.sub(r\"im\", \"i am\", sentence)\n    sentence = re.sub(r\"ill\", \"i will\", sentence)\n    sentence = re.sub(r\"na\", \"\", sentence)\n    sentence = re.sub(r\"can\\'t\", \"can not\", sentence)\n    sentence = re.sub(r\"cant\", \"can not\", sentence)\n    sentence = re.sub(r\"cannot\", \"can not\", sentence)\n    sentence = re.sub(r\"didnt\", \"did not\", sentence)\n    sentence = re.sub(r\"n\\'t\", \" not\", sentence)\n    sentence = re.sub(r\"\\'re\", \" are\", sentence)\n    sentence = re.sub(r\"\\'s\", \" is\", sentence)\n    sentence = re.sub(r\"\\'d\", \" would\", sentence)\n    sentence = re.sub(r\"\\'ll\", \" will\", sentence)\n    sentence = re.sub(r\"\\'t\", \" not\", sentence)\n    sentence = re.sub(r\"\\'ve\", \" have\", sentence)\n    sentence = re.sub(r\"\\'m\", \" am\", sentence)\n    sentence = re.sub(\"\\S*\\d\\S*\", \"\", sentence).strip()\n    sentence = re.sub('[^A-Za-z]+', ' ', sentence)\n    sentence = re.sub(r'[?|!|\\'|\"|#]',r'',sentence)\n    sentence = re.sub(r'[.|,|)|(|\\|\/]',r' ',sentence) \n    return sentence\n\nstop_words = set(stopwords.words('english'))  ","44b9ed16":"stop_words = set(stopwords.words('english')+[\"co\",\"http\"])\nsnow = nltk.stem.SnowballStemmer('english')\n\ndef mots_cles(df):\n    \"\"\"\n    Arguments :\n    :df: dataframe\n    \n    Cette fonction ajoute deux colonnes au dataframe pass\u00e9 en param\u00e8tre.\n    Une colonne \"mots_cl\u00e9s\" qui correspond au tweet de base d\u00e9pourvus des mots inutiles (stowords)\n    et une colonne \"mots_cl\u00e9s_snowball\" qui correspond \u00e0 ces m\u00eames mots appliqu\u00e9s \u00e0 la fonction snowballStemmer.\n    \"\"\"\n    temp = []\n    temp_s=[]\n\n    for each_sentence in df.text:\n        each_sentence = cleaning(each_sentence)\n        each_word=[]\n        each_word_s=[]\n        for word in each_sentence.split():\n            if word not in stop_words:\n                each_word.append(word)\n                each_word_s.append(snow.stem(word))\n        \n        temp.append(each_word)\n        temp_s.append(each_word_s) \n        \n        final_word = []\n        final_word_s=[]\n\n    for row in temp:\n        seq = ''\n        for word in row:\n            seq = seq + word + ' '\n        final_word.append(seq)\n        \n    for row in temp_s:\n        seq = ''\n        for word in row:\n            seq = seq + word + ' '\n        final_word_s.append(seq)\n\n    for i in range(len(df)):\n        df.loc[i, \"mots_cles\"]=final_word[i]\n        df.loc[i, \"mots_cles_snowball\"]=final_word_s[i]\n    \n    return df","1e8abbaa":"train_df=mots_cles(train_df)\ntest_df=mots_cles(test_df)\n\ntrain_df","8d55e162":"def to_corpus(target):\n    corpus = []\n\n    for w in train_df.loc[train_df[\"target\"] == target][\"mots_cles\"].str.split():\n        for i in w:\n            corpus.append(i)\n            \n    return corpus\n\ndef corpus_to_dict(target):\n    corpus = to_corpus(target)\n            \n    dict = defaultdict(int)\n    for w in corpus:\n        dict[w] += 1\n    return sorted(dict.items(), key=lambda x:x[1], reverse=True)","a5ddd462":"disaster_dict = corpus_to_dict(0)\nnon_disaster_dict = corpus_to_dict(1)\n\ndisaster_x, disaster_y = zip(*disaster_dict)\nnon_disaster_x, non_disaster_y = zip(*non_disaster_dict)\n\nfig, ax = plt.subplots(1,2, figsize=(20,8))\nsns.barplot(y=list(disaster_x)[0:30], x=list(disaster_y)[0:30], orient='h', palette=\"Reds_d\", ax=ax[0])\nsns.barplot(y=list(non_disaster_x)[0:30], x=list(non_disaster_y)[0:30], orient='h', palette=\"Blues_d\", ax=ax[1]) \nax[0].set_title(\"mots non stop words les plus utilis\u00e9s Disaster Tweets\",fontsize = 25)\nax[0].set_xlabel(\"nb occurences\",fontsize = 20)\nax[1].set_title(\"mots non stop words les plus utilis\u00e9s Non-Disaster Tweets\",fontsize = 25)\nax[1].set_xlabel(\"nb occurences\",fontsize = 20)\nplt.tight_layout()\nplt.show()","92aaee90":"def bigrams(target):\n    corpus = train_df[train_df[\"target\"] == target][\"mots_cles\"]\n    count_vec = CountVectorizer(ngram_range=(2, 2)).fit(corpus)\n    mots = count_vec.transform(corpus)\n    somme_mots = mots.sum(axis=0) \n    freq_mots = [(w, somme_mots[0, idx]) for w, idx in count_vec.vocabulary_.items()]\n    freq_mots =sorted(freq_mots, key = lambda x: x[1], reverse=True)\n    return freq_mots","955ea670":"bigrams_disaster = bigrams(1)[:15]\nbigrams_non_disaster = bigrams(0)[:15]\n\nx_disaster, y_disaster = map(list, zip(*bigrams_disaster))\nx_non_disaster, y_non_disaster = map(list, zip(*bigrams_non_disaster))\n\nfig, ax = plt.subplots(1,2, figsize=(20,8))\nsns.barplot(x=y_disaster, y=x_disaster, orient='h', palette=\"Reds_d\", ax=ax[0])\nsns.barplot(x=y_non_disaster, y=x_non_disaster, orient='h', palette=\"Blues_d\", ax=ax[1])\n\nax[0].set_title(\"Top 15 Bigrams - Disaster Tweets\", fontsize=20)\nax[0].set_xlabel(\"Frequence\", fontsize=20)\nax[1].set_title(\"Top 15 Bigrams - Non-Disaster Tweets\", fontsize=20)\nax[1].set_xlabel(\"Frequence\", fontsize=20)\nplt.tight_layout()\nplt.show()","ee5c2c89":"def wordcloud(mots, titre):\n    wordcloud = WordCloud(width=800, height=500, random_state=21, max_font_size=130).generate(mots) \n    plt.figure(figsize=(10, 7)) \n    plt.imshow(wordcloud, interpolation=\"bilinear\")\n    plt.axis('off') \n    print(titre)\n    plt.show()","85b1f559":"wordcloud(' '.join([text for text in (train_df[\"mots_cles\"].append(test_df[\"mots_cles\"]))]), \"Mots principaux tweets totaux\")","7a2ec911":"wordcloud(' '.join([text for text in train_df['mots_cles'][train_df['target'] == 1]]) , \"Mots principaux train tweets disaster\")","dfb363db":"wordcloud(' '.join([text for text in train_df['mots_cles'][train_df['target'] == 0]]) , \"Mots principaux train tweets non disaster\")","245cf00d":"def conf_matrix(pred, titre):\n    \"\"\"\n    Arguments :\n    :pred: int[]\n    :titre: str\n    \n    Cette fonction cr\u00e9e une matrice de confusion entre des pr\u00e9dictions pass\u00e9es en parametre et les pr\u00e9dictions officielles de train_df\"\"\"\n    cmatrix = confusion_matrix(train_df[\"target\"],pred)\n    f,ax = plt.subplots(figsize=(3,3))\n    sns.heatmap(cmatrix,annot=True,linewidths=0.5,cbar=False,fmt='.0f',ax=ax)\n    plt.xlabel(\"y_predict\")\n    plt.ylabel(\"y_true\")\n    ax.set(title=titre)\n    plt.show()","e5302d03":"#Exemple de matrice de confusion avec les valeurs r\u00e9elles des tweets\nconf_matrix(train_df[\"target\"],\"Exemple\")","a7af2e38":"compare_model=[]\n#tableau de dictionnaires qui va nous permettre de comparer les diff\u00e9rente fa\u00e7ons de calculer les tweet disaster ","283bbb1c":"def score(pred):\n    \"\"\"\n    Arguments :\n    :pred: int[]\n    \n    Calcule le taux de r\u00e9ussite des pr\u00e9dictions\"\"\"\n    tot=0\n    for i in range(len(train_df)):\n        if pred[i]==train_df[\"target\"][i]:\n            tot+=1\n    s=tot\/len(train_df)\n    print (\"Score Train -->\", round(s *100,2), \" %\")\n    return s","9b494f7e":"keywords_proba_df.head()","7c3816a6":"def prediction_keywords(i,df=test_df, affichage=False):\n    \"\"\"\n    Arguments :\n    :i: int\n    :df: dataframe\n    :affichage: booleen\n    \n    Determine si le tweet est disaster ou non gr\u00e2ce au tableau keywords_proba_df\"\"\"\n    if affichage :\n        print (\"tweet test\u00e9 :\",df.text[i])\n        print(\"keyword :\",df.keyword[i])\n        if df.text[i]==train_df.text[i]:\n            print(\"target :\",df.target[i],\"\\n\")\n            \n    if df.keyword[i] in list(keywords_proba_df[\"keyword\"]):\n        pred=round(keywords_proba_df.proba_target[keywords_proba_df.keyword==df.keyword[i]].values[0])\n#   else :pred=(random.randint(0, 1)) #score : 73.74\n#   else :pred=9 #score : 73.74\n    else :pred=1 #score : 74.04\n    if affichage : print (\"test :\",pred,\"--> tweet non disaster\" if pred==0 else \" --> tweet disaster\")\n    else: return pred\n        ","66b729bf":"prediction_keywords(508,train_df, True)","7f4f4b52":"prediction_keywords(509,train_df, True)","63b91b86":"prediction_keywords(66,affichage =True)","3dfeabd9":"predictions_train_keywords=[]\nfor i in range(len(train_df)):\n    predictions_train_keywords.append(prediction_keywords(i,train_df))","5f9d2ec6":"conf_matrix(predictions_train_keywords, \"pred keywords\")","a467aa4d":"score_keywords=score(predictions_train_keywords)","30b4f7ae":"predictions_keywords=[]\nfor i in range(len(test_df)):\n    predictions_keywords.append(prediction_keywords(i))","65856f7a":"pred={\"model\":\"keywords()\", \"predictions\":np.array(predictions_keywords), \"train_predictions\":np.array(predictions_train_keywords), \"score\":score_keywords, \"tfidf\":False}\ncompare_model.append(pred)","a7ca9720":"nlp = spacy.load(\"en_core_web_lg\")\n\ndef valeurTweet(dct):\n    \"\"\"\n    Arguments :\n    :dct: dictionnary\n    :return:liste de taille 300 de float\n    \n    Cr\u00e9e les vecteurs de chaque mots du dictionnaire avec Spacy (word embedding)\"\"\"\n    vec = []\n    for i in range(len(dct)):\n        valTweet={}\n        for token in dct[i][\"Mots\"]:\n            valTweet[str(token)] = nlp(token).vector\n        vec.append(valTweet)\n    return vec","7adec30f":"#Calcul des poids tfidf de chaque mots de la data\nmodel_tfidf=TfidfVectorizer()\ntfidf_general = model_tfidf.fit_transform(list(train_df[\"mots_cles\"])+list(test_df[\"mots_cles\"]))","28cbcced":"def dictionnaire(df, tfidf, decalage=0):\n    \"\"\"\n    Arguments :\n    :df: dataframe\n    :tfidf: matrice tfidf\n    :decalage: int\n    :return: dictionnary\n    \n    Cr\u00e9e le dictionnaire des donn\u00e9es n\u00e9c\u00e9ssaires par la suite \"\"\"\n    dct=[]\n    for i in df.index:\n        dct.append(\n            {\"id\": df.id[i]}) \n    \n    for i in df.index:\n        dct[i][\"Mots\"]=[df[\"mots_cles\"][i].split()[j] for j in range(len(df[\"mots_cles\"][i].split())) if df[\"mots_cles\"][i].split()[j] in model_tfidf.vocabulary_]\n        \n    for i in range(len(dct)):\n        dct[i][\"index\"]=[model_tfidf.vocabulary_[str(k)] for k in dct[i][\"Mots\"] if str(k) in model_tfidf.vocabulary_ ]\n        \n    for i in range(len(dct)):\n        dct[i][\"poids_tfidf\"]=[(tfidf[i+decalage, k]) for k in dct[i][\"index\"]]\n        \n    vec=valeurTweet(dct)\n    for i in range(len(dct)):\n        dct[i][\"vectors\"]=[]\n        dct[i][\"vectors_tfidf\"]=[]\n        for k in range(len(dct[i][\"Mots\"])):\n            dct[i][\"vectors\"].append([0 for l in range(300)])\n            dct[i][\"vectors_tfidf\"].append([0 for l in range(300)])\n        for j in range(len(dct[i][\"Mots\"])):\n            #print(dct[i][\"Mots\"][j], i)\n            dct[i][\"vectors\"][j]=vec[i][dct[i][\"Mots\"][j]]\n            dct[i][\"vectors_tfidf\"][j]=dct[i][\"vectors\"][j]*dct[i][\"poids_tfidf\"][j]\n            \n    for i in range(len(dct)):\n        if dct[i][\"Mots\"]==[]:\n            dct[i][\"moy_vector\"]=[0 for l in range(300)]\n            dct[i][\"moy_vector_tfidf\"]=[0 for l in range(300)]\n        else : \n            dct[i][\"moy_vector\"]=np.mean([(dct[i][\"vectors\"][j]) for j in range(len(dct[i][\"vectors\"]))], axis=0)\n            dct[i][\"moy_vector_tfidf\"]=np.mean([(dct[i][\"vectors_tfidf\"][j]) for j in range(len(dct[i][\"vectors_tfidf\"]))], axis=0)\n            \n    return dct","8228b378":"# import json\n\n# dct_train=dictionnaire(train_df, tfidf_general)\n# dct_test=dictionnaire(test_df, tfidf_general, len(train_df))\n\n# def list_to_file(list_dct, nomFichier):\n#     shelf = shelve.open(nomFichier)\n#     shelf[\"my_dict\"] = list_dct\n#     shelf.close()\n#     return shelf \n\n# shelf_train=list_to_file(dct_train, \"list_dct_train.shlf\")\n# shelf_test=list_to_file(dct_test, \"list_dct_test.shlf\")","ef07d75e":"!cp -r ..\/input\/dictionnaries .\/\n\nshelf_train = shelve.open(\".\/dictionnaries\/list_dct_train.shlf\")\ndct_train = shelf_train[\"my_dict\"]\nshelf_train.close()\nshelf_test = shelve.open(\".\/dictionnaries\/list_dct_test.shlf\")\ndct_test = shelf_test[\"my_dict\"]\nshelf_test.close()","e8f3ab4f":"print(\"id :\",dct_train[0]['id'])\nprint(\"Mots :\",dct_train[0]['Mots'])\nprint(\"index :\",dct_train[0]['index'])\nprint(\"poids_tfidf :\",dct_train[0]['poids_tfidf'])\nprint(\"vectors :\",dct_train[0]['vectors'][0][:5], \"etc...\")\nprint(\"vectors tfidf:\",dct_train[0]['vectors_tfidf'][0][:5], \"etc...\")\nprint(\"moy vector:\",dct_train[0]['moy_vector'][:5], \"etc...\")\nprint(\"moy vector tfidf:\",dct_train[0]['moy_vector_tfidf'][:5], \"etc...\")","c66cf20e":"X=np.array([dct_train[i][\"moy_vector\"] for i in range(len(dct_train))]) #Tableau des vecteurs par tweets train\nX_test=np.array([dct_test[i][\"moy_vector\"] for i in range(len(dct_test))]) #Tableau des vecteurs par tweets test\n\nX_tfidf=np.array([dct_train[i][\"moy_vector_tfidf\"] for i in range(len(dct_train))]) #Tableau des vecteurs tfidf par tweets train\nX_test_tfidf=np.array([dct_test[i][\"moy_vector_tfidf\"] for i in range(len(dct_test))]) #Tableau des vecteurs tfidf par tweets test\n\ny = train_df[\"target\"] #Tableau des target de train","fbde4c2b":"modele_tsne = manifold.TSNE(2)\ndef tsne (x=X, y_tsne=y):\n    \"\"\"\n    Arguments :\n    :x:int[]\n    :y_tsne:int[]\n    :return: figure tsne\n    \n    Cr\u00e9e et affiche le mod\u00e8le tsne de x y_tsne\"\"\"\n    modele_tsne_fit=modele_tsne.fit_transform(x)\n\n    df_plot=pd.DataFrame(modele_tsne_fit)\n    df_plot['target']=y_tsne\n    df_plot.columns=[\"x\",\"y\",'target']\n    return sns.scatterplot(data=df_plot, x='x', y='y', hue='target')","4a22963c":"tsne()","ad84b2ab":"tsne(x=X_tfidf)","28a325bb":"distance_vec=metrics.pairwise_distances(X_test,X) #distance entre les vecteurs de test_df et train_df\ndistance_vec_train=metrics.pairwise_distances(X,X)#distance entre les vecteurs de train_df\ndistance_vec_tfidf=metrics.pairwise_distances(X_test_tfidf,X_tfidf)#distance entre les vecteurs tfidf de test_df et train_df\ndistance_vec_train_tfidf=metrics.pairwise_distances(X_tfidf,X_tfidf)#distance entre les vecteurs tfidf de train_df","ce3c1e0a":"def vecteurs_proches(id_tweet_test\u00e9, nb_tweets=100, dist=distance_vec, df=test_df, affichage=False):\n    \"\"\"\n    Arguments :\n    :id_tweet_test\u00e9:int\n    :nb_tweets:int\n    :dist: float[]\n    :df: dataframe\n    :affichage: booleen\n    :return: int\n    \n    Calcule si le tweet df[id_tweet_test\u00e9] est un disaster(1) ou non(0) \n    grace \u00e0 la moyenne des nb_tweets target des vecteurs train les plus proches de lui\"\"\"\n    tab=dist[id_tweet_test\u00e9]\n    if affichage:\n        print (\"Tweet test\u00e9 :\",df.text[id_tweet_test\u00e9])\n        print (\"Tweet plus ressemblant :\",train_df.text[tab.argsort()[0]])\n        print(\"Target :\",train_df.target[tab.argsort()[0]])\n    val=[]\n    for i in tab.argsort()[0:nb_tweets]:\n        val.append(train_df.target[i])\n    if affichage: print (\"Test :\",\"0 --> tweet non disaster\" if np.mean(val)<0.4297else \"1 --> tweet disaster\")\n    return(0 if np.mean(val)<0.4297 else 1)","60d4019e":"r=vecteurs_proches(0, affichage=True)","5f3d919f":"r=vecteurs_proches(0, dist=distance_vec_tfidf, affichage=True)","ae41f2f0":"r=vecteurs_proches(1,affichage=True)","ca185883":"r=vecteurs_proches(1, dist=distance_vec_tfidf,affichage=True)","3caf9b7f":"r=vecteurs_proches(0,dist=distance_vec_train, df=train_df,affichage=True)","afa28224":"predictions_train_vectors=[]\nfor i in range(len(train_df)):\n    predictions_train_vectors.append(vecteurs_proches(i,dist=distance_vec_train, df=train_df))","eaf3e183":"conf_matrix(predictions_train_vectors, \"100 tweets proches\")","1d1b08e5":"score_vectors=score(predictions_train_vectors)","d5acc385":"predictions_vectors=[]\nfor i in range(len(test_df)):\n    predictions_vectors.append(vecteurs_proches(i))","deb7ee32":"pred={\"model\":\"vecteurs_proches()\", \"predictions\":np.array(predictions_vectors), \"train_predictions\":np.array(predictions_train_vectors), \"score\":score_vectors, \"tfidf\":False}\ncompare_model.append(pred)","a9e529e2":"predictions_train_vectors_tfidf=[]\nfor i in range(len(train_df)):\n    predictions_train_vectors_tfidf.append(vecteurs_proches(i,dist=distance_vec_train_tfidf, df=train_df))","7854e74f":"conf_matrix(predictions_train_vectors_tfidf, \"100 tweets proches tfidf\")","5b92d0eb":"score_vectors_tfidf=score(predictions_train_vectors_tfidf)","05a37d81":"predictions_vectors_tfidf=[]\nfor i in range(len(test_df)):\n    predictions_vectors_tfidf.append(vecteurs_proches(i,dist=distance_vec_tfidf))","80d46846":"pred_tfidf={\"model\":\"vecteurs_proches()\", \"predictions\":np.array(predictions_vectors_tfidf), \"train_predictions\":np.array(predictions_train_vectors_tfidf), \"score\":score_vectors_tfidf, \"tfidf\":True}\ncompare_model.append(pred_tfidf)","d5aed9a1":"def predire(model, titre='', tfidf=False):\n    \"\"\"\n    Arguments :\n    :model:classifier\n    :titree:str\n    :tfidf:booleen\n    :return: dictionnary\n    \n    Cr\u00e9e un dictionnaire avec les pr\u00e9dictions sur test_df, les pr\u00e9dictions sur train_df et le score fait avec le mod\u00e8le model\"\"\"\n    if tfidf:\n        train_X=X_tfidf \n        test_X=X_test_tfidf\n    else :\n        train_X=X\n        test_X=X_test\n    predictions = model.predict(test_X)\n    train_predictions=model.predict(train_X)\n    conf_matrix(train_predictions , titre)\n    score=model.score(train_X, y)\n    print (\"Score Train -->\", round(score *100,2), \" %\")\n    pred={\"model\":str(model), \"predictions\":predictions, \"train_predictions\":train_predictions, \"score\":score, \"tfidf\":tfidf}\n    return pred","60fa7030":"def grid_search(param, base,tfidf=False):\n    \"\"\"\n    Arguments :\n    :param:parametre du classificateur\n    :base:model classifier\n    :tfidf:booleen\n    :return: model classifier\n    \n     Recherche du meilleur classificateur grace \u00e0 une grid search\"\"\"\n    \n    if tfidf:\n        train_X=X_tfidf \n        test_X=X_test_tfidf\n    else :\n        train_X=X\n        test_X=X_test\n    sh=HalvingGridSearchCV(base, param, cv=5,factor=2, max_resources=50).fit(train_X, y)\n    model=sh.best_estimator_\n    print(str(model)[:200])\n    return model","5284b002":"pred_lc=predire(LogisticRegression().fit(X,y), \"logistic regression\")\npred_lc_tfidf=predire(LogisticRegression().fit(X_tfidf,y),\"logistic regression tfidf\", tfidf=True)\n\ncompare_model.append(pred_lc)\ncompare_model.append(pred_lc_tfidf)","1a52e1f4":"param_lc = { 'max_iter':[50,100,150,200], 'random_state':[None, 0,1]}\nbase_lc = LogisticRegression()\n\npred_lc_gs=predire(grid_search(param_lc, base_lc), \"logistic regression grid search\")\npred_lc_tfidf_gs=predire(grid_search(param_lc, base_lc, tfidf=True), \"logistic regression grid search tfidf\",tfidf=True)\n\ncompare_model.append(pred_lc_gs)\ncompare_model.append(pred_lc_tfidf_gs)","0df41595":"pred_rf=predire(RandomForestClassifier().fit(X,y), \"random forest\")\npred_rf_tfidf=predire(RandomForestClassifier().fit(X_tfidf,y),\"random forest tfidf\", tfidf=True)\n\ncompare_model.append(pred_rf)\ncompare_model.append(pred_rf_tfidf)","c884c3c9":"param_rf = {'n_estimators':[1,10, 100, 150], 'max_depth':[None, 3,10,50,100], 'random_state':[None, 0,1]}\nbase_rf = RandomForestClassifier()\n\npred_rf_gs=predire(grid_search(param_rf, base_rf), \"random forest grid search\")\npred_rf_tfidf_gs=predire(grid_search(param_rf, base_rf, tfidf=True), \"random forest grid search tfidf\", tfidf=True)\n\ncompare_model.append(pred_rf_gs)\ncompare_model.append(pred_rf_tfidf_gs)","6b054511":"pred_gnb=predire(GaussianNB().fit(X,y), \"Gaussian NB\")\npred_gnb_tfidf=predire(GaussianNB().fit(X_tfidf,y),\"Gaussian NB tfidf\", tfidf=True)\n\ncompare_model.append(pred_gnb)\ncompare_model.append(pred_gnb_tfidf)","6ea153ec":"pred_xgb=predire(XGBClassifier(use_label_encoder=False, eval_metric=[\"auc\"], random_state=0).fit(X,y), \"xgboost\")\npred_xgb_tfidf=predire(XGBClassifier(use_label_encoder=False, eval_metric=[\"auc\"], random_state=0).fit(X_tfidf,y),\"xgboost tfidf\", tfidf=True)\n\ncompare_model.append(pred_xgb)\ncompare_model.append(pred_xgb_tfidf)","5293ddc3":"param_xgb = {'n_estimators':[1,10, 100, 150], 'random_state':[None, 0,1], 'max_depth':[None, 3,10,50,100]}\nbase_xgb = XGBClassifier(use_label_encoder=False, eval_metric=[\"auc\"])\n\npred_xgb_gs=predire(grid_search(param_xgb, base_xgb), \"xgboost grid search\")\npred_xgb_tfidf_gs=predire(grid_search(param_xgb, base_xgb, tfidf=True), \"xgboost grid search tfidf\", tfidf=True)\n\ncompare_model.append(pred_xgb_gs)\ncompare_model.append(pred_xgb_tfidf_gs)","36cc8945":"clf1 = LogisticRegression()\nclf2 = RandomForestClassifier()\nclf3 = GaussianNB()\nclf4=XGBClassifier(use_label_encoder=False, eval_metric=[\"auc\"])\n\ndef voting_classifier(param='hard', lr=1, rf=1, gnb=1, xgb=1, titre='', tfidf=False):\n    \"\"\"\n    Arguments :\n    :param:str, parametre du voting classifier (\"hard\" ou \"soft\")\n    :lr,rf,xgb: int\n    :titre: str\n    :tfidf:booleen\n    :return: fct predire()\n    \n     Cr\u00e9ation du mod\u00e8le voting classifier avec ses poids lr, rf, gnb, xgb et param\"\"\"\n    if tfidf:\n        train_X=X_tfidf \n        test_X=X_test_tfidf\n    else :\n        train_X=X\n        test_X=X_test\n        \n    eclf1 = VotingClassifier(estimators=[('lr', clf1), ('rf', clf2),('gnb', clf3), ('xgb', clf4)], voting=param, weights=[lr,rf,gnb, xgb]) \n    eclf1 = eclf1.fit(train_X, y)\n    return predire(eclf1, titre, tfidf)","8835707d":"pred_voting_hard=voting_classifier(\"hard\",titre=\"Voting hard\")\npred_voting_hard_tfidf=voting_classifier(\"hard\",titre=\"voting hard tfidf\",tfidf=True)\n\ncompare_model.append(pred_voting_hard)\ncompare_model.append(pred_voting_hard_tfidf)","3a7589fc":"pred_voting_soft=voting_classifier(\"soft\",titre=\"voting soft\")\npred_voting_soft_tfidf=voting_classifier(\"soft\", titre=\"voting soft tfidf\",tfidf=True)\n\ncompare_model.append(pred_voting_soft)\ncompare_model.append(pred_voting_soft_tfidf)","793f773b":"pred_soft_1_5rfxgb=voting_classifier(\"soft\",titre=\"voting soft, poids random forest=1.5 xgboost=1.5\",rf=1.5, xgb=1.5)\npred_soft_1_5rfxgb_tfidf=voting_classifier(\"soft\", titre=\"voting soft, poids random forest=1.5 xgboost=1.5 tfidf\",rf=1.5, xgb=1.5, tfidf=True)\n\ncompare_model.append(pred_soft_1_5rfxgb)\ncompare_model.append(pred_soft_1_5rfxgb_tfidf)","a07964b5":"pred_hard_1_5rfxgb=voting_classifier(\"hard\",titre=\"voting hard, poids random forest=1.5 xgboost=1.5\",rf=1.5, xgb=1.5)\npred_hard_1_5rfxgb_tfidf=voting_classifier(\"hard\", titre=\"voting hard, poids random forest=1.5 xgboost=1.5 tfidf\",rf=1.5, xgb=1.5, tfidf=True)\n\ncompare_model.append(pred_hard_1_5rfxgb)\ncompare_model.append(pred_hard_1_5rfxgb_tfidf)","cd0a6f44":"best_score=np.max(np.array([compare_model[i][\"score\"] for i in range(len(compare_model))]))\nmeilleurs_models=[]\nfor i in range(len(compare_model)):\n    if compare_model[i][\"score\"]==best_score :\n        meilleurs_models.append(compare_model[i])\nfor i in range(len(meilleurs_models)):\n    print(meilleurs_models[i][\"model\"][:200],\"\\n tfidf:\",meilleurs_models[i][\"tfidf\"],\"\\n\")","091bc36b":"print(meilleurs_models[0])","c6db9da1":"output = pd.DataFrame({'id': test_df.id, 'target': meilleurs_models[0][\"predictions\"]})\noutput.to_csv('my_submission.csv', index=False)\nsubmition_df=pd.read_csv('my_submission.csv')\nsubmition_df.head()","e2f83dd8":"#### On utilise les r\u00e9sultats du premier mod\u00e8le ayant le meilleur score.\n","a9718f49":"#### On obtient maintenant les mots utiles de nos tweets dans \"mots_cles\". On s'est d\u00e9barrass\u00e9 des stopwords.\n\n#### La fonction snowball consiste \u00e0 retirer les pr\u00e9-fixes et post-fixes des mots pour n'obtenir que les corps de ces derniers. Par exemple \"evacuation\" et \"evacuate\" deviennent tous deux \"evacuat\". Dans le code qui suit, je n'ai pas utilis\u00e9 ces mots cl\u00e9s car ils donnaient de bien moins bons r\u00e9sultats.","6b14476c":"### LAFOURCADE Emie p1802635\n## **Projet RC3 - Kaggle Challenge**\n### Lifprojet Printemps 2021\n### Encadrant : CAZABET Remy","dd93e4d2":"## **II.4-Longueurs des tweets**","bbc2156d":"#### La longueur des tweet n'est pas d\u00e9terminante dans notre recherche de \"disaster tweets\".","3b355fda":"# **III-Traitement des donn\u00e9es**","0a502069":"## IV.1-Fonctions utiles ","e90ca92d":"## **IV.2 Solutions** \n## **Solution 1 : Keywords**\n\n#### On r\u00e9utilise la fonction proba_keywords d\u00e9finie au dessus pour d\u00e9terminer si les tweets \u00e0 tester sont des disaster ou non selon la moyenne des r\u00e9sultats des train tweets partageant le m\u00eame keyword.","6b06d501":"## **II.5-Keywords les plus utilis\u00e9s**","1cb4617a":"#### Valeurs nulles dans test_df","3d06291e":"#### Plusieurs tweets ne poss\u00e8dent pas de keywords. Dans ce cas on met ces tweets \u00e0 1 (disaster) car cela donne un meilleur score quand on teste cette technique sur les train tweets.","43f6a2f9":"#### M\u00eame chose pour les vecteurs pond\u00e9r\u00e9s avec tfidf.","fac4844c":"## **II.2-El\u00e9ments nuls**","3914391d":"### Calculs des distances et vecteurs proches.","d5f26393":"#### Pour chaque tweet, on a son id dans son dataframe, les mots importants qui le composent, son numero d'index dans le dictionnaire tfidf, son poids tfidf, un vecteur de taille 300 pour chaque mot du tweet, ce m\u00eame vecteur multipli\u00e9 par le poids tfidf du mot, un vecteur de taille 300 faisant la moyenne des vecteurs de chaque mots et un vecteur de taille 300 faisant la moyenne pond\u00e9r\u00e9e avec tfidf des vecteurs de chaque mot.","c0c84ddf":"# **I-Importation des librairies et donn\u00e9es**","07fbc023":"## Wordclouds","491145ac":"# **V-Resultats**\n#### On r\u00e9ccup\u00e8re les mod\u00e8les de calcul ayant le meilleur score.","025ae153":"## **II.1-Forme des donn\u00e9es**","5e33fdf8":"# **II-Data Visualization**","592c4fb6":"#### test_df est compos\u00e9 de 4 colonnes : id, keyword, location et text et 3263 lignes","a4a639f3":"### **Description :**\nLe projet consiste \u00e0 d\u00e9couvrir Kaggle et participer \u00e0 une \"comp\u00e9tition\" sur cette plateforme.\n\nSur ce notebook se trouve le code que j'ai r\u00e9alis\u00e9 pour la comp\u00e9tition [\"Natural Language Processing with Disaster Tweets\"](http:\/\/www.kaggle.com\/c\/nlp-getting-started). Il s'agit de d\u00e9terminer si les tweets \u00e0 tester sont des tweets parlant d'une catastrophe (disaster) ou non \u00e0 partir d'une base de donn\u00e9es de tweets dont nous connaissons l'issue (disaster ou non disaster).\n\n### **Ex\u00e9cution :**\nPour \u00e9x\u00e9cuter ce code il faut se mettre en \"edit mode\" et cliquer sur la double fl\u00e8che \"Run all\". Cependant, il met une 15\u00e8ne de minutes \u00e0 s'\u00e9x\u00e9cuter. Toutes les sorties que vous obtiendrez sont affich\u00e9es en viewer mode alors il n'est pas n\u00e9c\u00e9ssaire de le lancer.\nCe code n'est \u00e9x\u00e9cutable que sur Kaggle (emplacement des donn\u00e9es et librairies propre).\n\nD'autres noteboooks sont consultables sur mon profil.\n### **R\u00e9sultats :**\nJ'ai test\u00e9 plusieurs solutions et la meilleure reste celle avec le random forest classificateur qui donne un r\u00e9sultat d'environ 79% de r\u00e9ussite.\n\n### **Sommaire :**\n*   I. Importation des librairies et donn\u00e9es\n*   II. Data Visualization\n*   III. Traitement des donn\u00e9es\n*   IV. Exploitations des donn\u00e9es\n*   V. Resultats","17d60427":"## **II.3-Taux des tweets disaster - non disaster**","5fe94aae":"#### Environ la moitier des tweets totaux n'indique pas de localisation. La colonne \"location\" ne sera donc pas utilis\u00e9e. \n\n#### On pourra cependant utiliser la colonne des mots cl\u00e9s \"keywords\".","b58a94e8":"#### train_df est compos\u00e9 de 5 colonnes : id, keyword, location, text et target et 7613 lignes","d9bd3ec9":"## **Solution 2 : Vecteurs, Word Embedding**\n#### On cr\u00e9e les vecteurs des tweets puis on regarde quels sont les vecteurs des train tweets les plus proches pour chaque tweet \u00e0 tester. Selon la moyenne des \"target\" de ces train tweets on d\u00e9termine si le tweet est un disaster ou non.\n### Cr\u00e9ation des vecteurs ","086404d8":"# **IV-Exploitation des donn\u00e9es** ","1b70a456":"#### Les vecteurs sans tfidf forment un nuage de points mais on observe une l\u00e9g\u00e8re s\u00e9paration des tweets disaster et non disaster. ","43103949":"#### Les keywords les plus utilis\u00e9s dans les disaster tweets sont un bon indicateur pour classer les tweets. Un tweet ayant comme keyword un mot tr\u00e8s utilis\u00e9 dans les disaster tweets a de grandes chances d'en \u00eatre un aussi.","816b157e":"#### Le mod\u00e8le tsne permet de rammener les vecteurs de taille 300 \u00e0 une taille 2 ce qui permet de visualiser si les vecteurs des tweets disaster et non disaster sont distinguables dans un environnement 2D.","da76af19":"## **Solution 3 : Classificateurs**\n#### On passe  les vecteurs cr\u00e9\u00e9s au dessus \u00e0 diff\u00e9rents mod\u00e8les qui classifient.\n#### On teste le Random forest, logistic regression, Gaussian NB, XGboost avec les param\u00e8tres minimum puis ces m\u00eames mod\u00e8les avec des param\u00e8tres d\u00e9termin\u00e9s avec grid search et enfin un mod\u00e8le \"voting classifier\" avec ces 4 classificateurs.","1e3681af":"#### Valeurs nulles dans train_df","e3e567cb":"#### **Lire une matrice de confusion :**\n#### La case en haut \u00e0 gauche indique le nombre de tweets que le mod\u00e8le a estim\u00e9 \u00e0 0 et qui valent r\u00e9ellement 0\n#### La case en haut \u00e0 droite indique le nombre de tweets que le mod\u00e8le a estim\u00e9 \u00e0 1 et qui valent r\u00e9ellement 0\n#### La case en bas \u00e0 gauche indique le nombre de tweets que le mod\u00e8le a estim\u00e9 \u00e0 0 et qui valent r\u00e9ellement 1\n#### La case en bas \u00e0 droite indique le nombre de tweets que le mod\u00e8le a estim\u00e9 \u00e0 1 et qui valent r\u00e9ellement 1","dd06d259":"#### On ne lance pas cette cellule car elle met beaucoup de temps \u00e0 s'\u00e9x\u00e9cuter. Elle permet la cr\u00e9ation de nos dictionnaires et l'\u00e9criture de ceux-ci dans des fichiers shelves. On r\u00e9cup\u00e8re les donn\u00e9es de ces fichiers dans la cellule suivante.","09f3a634":"## Visualisation des donn\u00e9es trait\u00e9es","77c774e9":"#### M\u00eame chose pour les non-disaster tweets."}}