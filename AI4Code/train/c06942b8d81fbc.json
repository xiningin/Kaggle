{"cell_type":{"febc0c38":"code","2ebd6734":"code","7b142b80":"code","c619d404":"code","e7dc1c00":"code","a35356ac":"code","ce81c548":"code","04528fb1":"code","02cdfdf5":"code","8d3973fc":"code","a6783642":"code","e1f634fc":"code","90af17f1":"code","04f55a61":"markdown","c73a75f4":"markdown","27c33d87":"markdown","9c932729":"markdown","6b1ce54e":"markdown","3a5e8b55":"markdown","bbeb7c84":"markdown","df240755":"markdown","733aa6e9":"markdown"},"source":{"febc0c38":"# Download the logging script\n!wget https:\/\/raw.githubusercontent.com\/scottclowe\/cpu-gpu-utilisation-logging-python\/master\/log_gpu_cpu_stats.py","2ebd6734":"# Start the logger running in a background process. It will keep running until you tell it to stop.\n# We will save the CPU and GPU utilisation stats to a CSV file every 0.2 seconds.\nimport subprocess\n!rm -f log_compute.csv\nlogger_fname = 'log_compute.csv'\nlogger_pid = subprocess.Popen(\n    ['python', 'log_gpu_cpu_stats.py',\n     logger_fname,\n     '--loop',  '0.2',  # Interval between measurements, in seconds (optional, default=1)\n    ])\nprint('Started logging compute utilisation')","7b142b80":"import os\nimport time\n\nimport numpy as np\nimport torch","c619d404":"t_per_exp = 2\nt_sleep = 2\n\ntgen_list = [\n    ('ones_float', lambda s, d: torch.ones(s, dtype=torch.float, device=d)),\n    #('rand_float', lambda s, d: torch.rand(s, dtype=torch.float, device=d)),\n]\nop_list = [\n    ('ADD', lambda x, y: x + y),\n    ('MUL', lambda x, y: x * y),\n    ('MATMUL', lambda x, y: torch.matmul(x, y)),\n]\n\n# Do some compute on CPU and GPU\nfor regen_tensors in [False, True]:\n    for tgen_name, tgen_fn in tgen_list:\n        print(\"\\n{} tensors ({})...\".format(tgen_name, 'regenerate inputs' if regen_tensors else 'static inputs'))\n        for op_name, op_fun in op_list:\n            print(\"\\n  {} operations...\".format(op_name))\n            time.sleep(5)\n            for device in ['cpu', 'cuda']:\n                for shp in [(8, 8), (64, 64), (512, 512), (4096, 4096)]: #[(10, 10), (100, 100), (1000, 1000), (10000, 10000)]:\n                    print(\n                        '    Beginning {:<12} {} {:<6} operations on {:<4} for {}s ({})'\n                        ''.format(str(shp), tgen_name, op_name, device.upper(), t_per_exp,\n                                 'regenerate inputs' if regen_tensors else 'static inputs')\n                    )\n                    i = 0\n                    t_start = time.time()\n                    t_gen = 0\n                    t_op = 0\n                    if not regen_tensors:\n                        x = tgen_fn(shp, device)\n                        y = tgen_fn(shp, device)\n                    while time.time() - t_start < t_per_exp:\n                        t0 = time.time()\n                        if regen_tensors:\n                            x = tgen_fn(shp, device)\n                            y = tgen_fn(shp, device)\n                        t1 = time.time()\n                        t_gen += t1 - t0\n                        z = op_fun(x, y)\n                        t_op += time.time() - t1\n                        i += 1\n                    dur = time.time() - t_start\n                    print(\n                        '      Completed {:>7} iterations in {:.1f}s ({:10.3f}it\/s);'\n                        ' {:.1f}% was generating tensors'\n                        ''.format(i, dur, i \/ dur, 100 * t_gen \/ (t_gen + t_op + 0.001))\n                    )\n                    time.sleep(t_sleep)","e7dc1c00":"!head log_compute.csv","a35356ac":"!tail log_compute.csv","ce81c548":"time.sleep(60)\n!tail log_compute.csv","04528fb1":"import pandas as pd\nfrom matplotlib import pyplot as plt","02cdfdf5":"logger_df = pd.read_csv(logger_fname)","8d3973fc":"logger_df","a6783642":"t = pd.to_datetime(logger_df['Timestamp (s)'], unit='s')\ncols = [col for col in logger_df.columns\n        if 'time' not in col.lower() and 'temp' not in col.lower()]\nplt.figure(figsize=(15, 9))\nplt.plot(t, logger_df[cols])\nplt.legend(cols)\nplt.xlabel('Time')\nplt.ylabel('Utilisation (%)')\nplt.show()","e1f634fc":"for col in logger_df.columns:\n    if 'time' in col.lower(): continue\n    plt.figure(figsize=(15, 9))\n    plt.plot(t, logger_df[col])\n    plt.xlabel('Time')\n    plt.ylabel(col)\n    plt.show()","90af17f1":"# End the background process logging the CPU and GPU utilisation.\nlogger_pid.terminate()\nprint('Terminated the compute utilisation logger background process')","04f55a61":"## Running simulated computation on CPU and GPU\nWe're going to do adds, multiplies, and matrix multiplies with matrices of different sizes. As the size increases, the number of operations to be simultaneously performed increases. We can see that the CPU is faster with small matrices, but the GPU has so many threads available to it that there is virtually no penalty for increasing the size of the matrix up to a certain point. If you change the option over to random floats instead of ones, you'll see a big slow down due to the increase in time generating the matrices. I haven't investigated half\/double precision here because Pytorch doesn't support it on CPU.\n\nBecause there is a large disparity in the rate at which iterations can be performed, we fix the duration of compute and see how many iterations could be achieved in that timespan.","c73a75f4":"## Downloading and running the logging script\nWe start a background process running which, at regular intervals, records the CPU and GPU compute and memory utilisation to a file. We do this as a background process because that way it can record everything that happens while the notebook is running, and it doesn't block the notebook from moving on from the cell to the next cell.\n\nStarting a subprocess with Popen gives us a process ID, which we need to close when we've finished with the logging (see the final cell in this notebook).\n\nAs a cautionary note, I have obsereved that the background process performing the logging is sometimes killed prematurely. If your code is running something which prints output, you will see an extra blank line in the output whenever this occurs (both things occur because of something going on periodically behind the scenes on the server, I'm not sure what).","27c33d87":"We can plot all the utilisation stats at once on a single plot.","9c932729":"We're done with the simulated work, now lets see what was recorded in the log file.","6b1ce54e":"# CPU and GPU logging demo\nThis kernel uses my script from https:\/\/github.com\/scottclowe\/cpu-gpu-utilisation-logging-python to track the CPU and GPU utilisation while the kernel is running.\n\nIn this demo kernel, we have the network do matrix multiplication repeatedly in a simulation of useful computation.","3a5e8b55":"## Plotting utilisation over time\nWe load up the logged data from the CSV file and plot the utilisation over time.","bbeb7c84":"## Finally, close the logging process","df240755":"Note that the CPU is working hard even when we are running the computation on the GPU!\n\nWe can also plot the graphs individually. Here, we also include the temperature of the GPU (which is in degrees Celcius, not %).","733aa6e9":"The GPU is still registered as running at 100%. This happens consistently for me after running at 100% for a while, though I am not sure why the GPU does this. If you wait long enough, it will return to 0%, or if you do a small amount of compute it appears become accurate again.\n\nLet's wait and see if the GPU level falls back down again."}}