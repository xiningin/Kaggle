{"cell_type":{"688dd299":"code","ef2b4eed":"code","391c335c":"code","48cce261":"code","f1799604":"code","905b5e5e":"code","d5dcb2bc":"code","1df483a3":"code","7a1734f2":"code","993f7777":"code","b35f1fe6":"code","f355b6af":"code","e4556ad9":"code","47baa430":"code","cc894dd1":"code","c27b6afb":"code","695a1f4c":"code","d5abf9b9":"markdown","7398daf3":"markdown","cd2e6622":"markdown","425b1ec3":"markdown","065794d5":"markdown","7b1d1d42":"markdown","427074ae":"markdown","293ef167":"markdown","3a80e5e7":"markdown","2b509aa6":"markdown","d1e61321":"markdown","107b377e":"markdown","1cd44d3c":"markdown","bc781265":"markdown"},"source":{"688dd299":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport nltk\nimport os\nfrom nltk.corpus import stopwords\nimport string\nimport re\nfrom sklearn import svm\nfrom nltk.sentiment.vader import SentimentIntensityAnalyzer\nfrom nltk import word_tokenize, pos_tag, ne_chunk, tree2conlltags","ef2b4eed":"# Load the data\ndf_train = pd.read_csv(\"..\/input\/train.csv\")\ndf_test = pd.read_csv(\"..\/input\/test.csv\")\ndf_sample = pd.read_csv(\"..\/input\/sample_submission.csv\")","391c335c":"sia = SentimentIntensityAnalyzer()\ndef sentiment_nltk(text):\n    res = sia.polarity_scores(text)\n    return res['compound']","48cce261":"def chars_between_commas(text):\n    return np.mean([len(chunk) for chunk in text.split(\",\")])","f1799604":"def count_unknown_symbols(text):\n    symbols_known = string.ascii_letters + string.digits + string.punctuation\n    return sum([not x in symbols_known for x in text])","905b5e5e":"def get_persons(text):\n    # Some names have family and given names, but both belong to the same person\n    # Bind them!\n    def bind_names(tagged_words):\n        names = list()\n        name = list()\n        # Bind several consequtive words with 'PERSON' tag\n        for i, w in enumerate(tagged_words):\n            if i == 0:\n                continue\n            if \"PERSON\" in w[2]:\n                name.append(w[0])\n            else:\n                if len(name) != 0:\n                    names.append(\" \".join(name))\n                name = list()\n        return names\n        \n    res_ne_tree = ne_chunk(pos_tag(word_tokenize(text)))\n    res_ne = tree2conlltags(res_ne_tree)\n    res_ne_list = [list(x) for x in res_ne]\n    return bind_names(res_ne_list)","d5dcb2bc":"class WordCloudIntersection():\n    \n    def __init__(self, stopwords=list(), punctuation=list(), stemmer=None, ngram=1):\n        self.stopwords = stopwords\n        self.punctuation = punctuation\n        self.remove = self.stopwords + self.punctuation\n        self.clouds = dict()\n        self.texts = dict()\n        self.stemmer = stemmer\n        self.ngram = ngram\n    \n    def find_ngrams(self, input_list, n):\n        return [\" \".join(list(i)) for i in zip(*[input_list[i:] for i in range(n)])]\n    \n    # It would be much  more correct to call this function 'get_tokens'\n    # it extracts not only words, but n-grams as well\n    def get_words(self, text):\n        words = nltk.tokenize.word_tokenize(text)\n        words = [w for w in words if not w in self.remove]\n        if not self.stemmer is None:\n            words = [self.stemmer.stem(w) for w in words]\n        \n        if self.ngram > 1:\n            words = self.find_ngrams(words, self.ngram)\n        return words\n    \n    # Jaccard distance again\n    def relative_intersection(self, x, y):\n        try:\n            return len(x & y)\/len(x | y)\n        except:\n            return 0.0\n    \n    def fit(self, x, categories, data_train, data_test=None):\n        cat_names = np.unique(data_train[categories])\n        \n        text_train = \" \".join(list(data_train[x]))\n        text_test = \"\"\n        if not data_test is None:\n            text_test = \" \".join(list(data_test[x]))\n        \n        # Tokens presenting in both train and test data\n        words_unique = self.get_words((text_train + text_test).lower())\n        \n        for cat in cat_names:\n            self.texts[cat] = (\" \".join(list(data_train[x][data_train[categories] == cat]))).lower()\n            words = self.get_words(self.texts[cat])\n            self.clouds[cat] = pd.value_counts(words)\n        \n        # use only tokens presented in both train and test data, \n        # feature will force your model to overfit to the train data otherwise    \n        for cat in cat_names:\n            self.clouds[cat] = self.clouds[cat][list(set(self.clouds[cat].index) & set(words_unique))]\n        \n        # Keep only author-specific tokens\n        for cat in cat_names:\n            key_leftover = list(set(cat_names) - set([cat]))\n            bigrams_other = set(self.clouds[key_leftover[0]].index) | set(self.clouds[key_leftover[1]].index)\n            self.clouds[cat] = self.clouds[cat][list(set(self.clouds[cat].index) - bigrams_other)]\n        \n    def transform(self, x, data):\n        intersection = dict()\n        prefix = '_intersect_'\n        if self.ngram > 1:\n            prefix = '%s-gram%s' % (self.ngram, prefix)\n        else:\n            prefix = 'word' + prefix\n        for key in self.clouds.keys():\n            category_words_set = set(self.clouds[key].index)\n            intersection[prefix+key] = list()\n            for text in data[x]:\n                unique_words = set(self.get_words(text.lower()))\n                fraction = self.relative_intersection(unique_words, category_words_set)\n                intersection[prefix+key].append(fraction)\n        return pd.DataFrame(intersection)","1df483a3":"\n# Split text into words\ndef get_words(text):\n    words = nltk.tokenize.word_tokenize(text)\n    return [word for word in words if not word in string.punctuation]\n\n# string.punctuation = '!\"#$%&\\'()*+,-.\/:;<=>?@[\\\\]^_`{|}~'\ndef count_punctuation(text):\n    return sum([x in string.punctuation for x in text])\n\ndef count_capitalized_words(text):\n    return sum([word.istitle() for word in get_words(text)])\n\ndef count_uppercase_words(text):\n    return sum([word.isupper() for word in get_words(text)])\n    \ndef count_tokens(text, tokens):\n    return sum([w in tokens for w in get_words(text)])\n\ndef first_word_len(text):\n    return len(get_words(text)[0])\n\ndef last_word_len(text):\n    return len(get_words(text)[-1])\n\ndef symbol_id(x):\n    symbols = [x for x in string.ascii_letters + string.digits + string.punctuation]\n    return np.where(np.array(symbols) == x)[0][0]\n\n# It is not a feature! It is just Jaccard distance\ndef relative_len(set_x, set_y):\n    return len(set_x & set_y)\/len(set_x | set_y)","7a1734f2":"stopwords = nltk.corpus.stopwords.words('english')\nbigci = WordCloudIntersection(stopwords=stopwords, \n                            punctuation=list(string.punctuation),\n                            stemmer=nltk.stem.SnowballStemmer('english'), ngram=2)\nbigci.fit(x='text', categories='author', data_train=df_train, data_test=df_test)","993f7777":"df_train_intersections = bigci.transform(x='text', data=df_train)\ndf_test_intersections = bigci.transform(x='text', data=df_test)","b35f1fe6":"df_train = pd.concat([df_train, df_train_intersections], axis=1)\ndf_test = pd.concat([df_test, df_test_intersections], axis=1)","f355b6af":"_, axes = plt.subplots(1, 3, figsize=(16,6))\nsns.violinplot(x='author', y='2-gram_intersect_EAP', data=df_train, ax=axes[0])\nsns.violinplot(x='author', y='2-gram_intersect_HPL', data=df_train, ax=axes[1])\nsns.violinplot(x='author', y='2-gram_intersect_MWS', data=df_train, ax=axes[2])\nplt.show()","e4556ad9":"text_EAP = \" \".join(list(df_train['text'][df_train['author'] == \"EAP\"]))\ntext_HPL = \" \".join(list(df_train['text'][df_train['author'] == \"HPL\"]))\ntext_MWS = \" \".join(list(df_train['text'][df_train['author'] == \"MWS\"]))\npersons_EAP = set(get_persons(text_EAP))\npersons_HPL = set(get_persons(text_HPL))\npersons_MWS = set(get_persons(text_MWS))\n# Keep only names related to the authors without any intersections with others\npersons_EAP = persons_EAP - persons_HPL - persons_MWS\npersons_HPL = persons_HPL - persons_EAP - persons_MWS\npersons_MWS = persons_MWS - persons_EAP - persons_HPL","47baa430":"for df, name in zip([df_train, df_test], [\"train\", \"test\"]):\n    df['persons_EAP_frac'] = df['text'].apply(lambda x: relative_len(persons_EAP, set(get_persons(x))))\n    df['persons_HPL_frac'] = df['text'].apply(lambda x: relative_len(persons_HPL, set(get_persons(x))))\n    df['persons_MWS_frac'] = df['text'].apply(lambda x: relative_len(persons_MWS, set(get_persons(x))))","cc894dd1":"_, axes = plt.subplots(1, 3, figsize=(16,6))\nsns.violinplot(x='author', y='persons_EAP_frac', data=df_train, ax=axes[0])\nsns.violinplot(x='author', y='persons_HPL_frac', data=df_train, ax=axes[1])\nsns.violinplot(x='author', y='persons_MWS_frac', data=df_train, ax=axes[2])\nplt.show()","c27b6afb":"for df, name in zip([df_train, df_test], [\"train\", \"test\"]):\n    print(\"Generating features for %s...\" % name)\n    words_count = df['text'].apply(lambda x: len(get_words(x)))\n    chars_count = df['text'].apply(lambda x: len(x))\n    \n    print(\"\\tFeatures related to words\")\n    df['capitalized_words_frac'] = df['text'].apply(lambda x: count_capitalized_words(x))\/words_count\n    df['uppercase_words_frac'] = df['text'].apply(lambda x: count_uppercase_words(x))\/words_count\n    df['single_frac'] = df['text'].apply(lambda x: count_tokens(x, ['is', 'was', 'has', 'he', 'she', 'it', 'her', 'his']))\/words_count\n    df['plural_frac'] = df['text'].apply(lambda x: count_tokens(x, ['are', 'were', 'have', 'we', 'they']))\/words_count\n    \n    print(\"\\tFeatures related to chars\")\n    df['unknown_symb_frac'] = df['text'].apply(lambda x: count_unknown_symbols(x))\/chars_count\n    df['chars_between_commas_relative'] = df['text'].apply(chars_between_commas)\/chars_count   \n    \n    df['first_word_len_relative'] = df['text'].apply(lambda x: first_word_len(x))\/chars_count\n    df['last_word_len_relative'] = df['text'].apply(lambda x: last_word_len(x))\/chars_count\n        \n    df['sentiment'] = df['text'].apply(sentiment_nltk)\n    df['first_symbol_id'] = df['text'].apply(lambda x: symbol_id(x[0]))\n    df['last_symbol_id'] = df['text'].apply(lambda x: symbol_id(x[-1]))","695a1f4c":"features = ['capitalized_words_frac', 'uppercase_words_frac', \n            'single_frac', 'plural_frac', 'unknown_symb_frac', \n            'chars_between_commas_relative', 'first_word_len_relative', \n            'last_word_len_relative', 'sentiment', 'first_symbol_id', 'last_symbol_id']\n_, axes = plt.subplots(4, 3, figsize=(16,16))\nfor i, feature in enumerate(features):\n    sns.violinplot(x='author', y=feature, data=df_train, ax=axes[int(i\/3),i%3])\nplt.show()","d5abf9b9":"# Introduction\n","7398daf3":"**Named Entity Recognition** task is quite non-trivial, and there are a lot of dedicated studies re;ated to the field, but NLTK offers the tool to deal with with problem. I would not say that it is state of the art, but it enought for fast prototyping and educational purposes.","cd2e6622":"Theoretically this feature should work very well for EAP","425b1ec3":"### Remaining features","065794d5":"Intersection of tokens clouds (words or n-grams) related to the authors","7b1d1d42":"## Generate features","427074ae":"NLTK has very basic sentiment analyzer","293ef167":"## Features which have been engineered","3a80e5e7":"Looks like that some texts have non ASCII chars, well, those chars not unknown but yet not recognized","2b509aa6":"Some authors really loves commas, moreover commas are very common to appear in poems!","d1e61321":"### Bigram clouds","107b377e":"### Prepare data for names of persons intersection","1cd44d3c":"Feel free to use these features if you like","bc781265":"This notebook simply gives the set of features you can use to get a good score on the Leaderbord."}}