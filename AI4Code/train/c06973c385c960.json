{"cell_type":{"88718c70":"code","db282883":"code","7fc66ecf":"code","f508a057":"code","40d912ac":"code","40e5bcbb":"code","ef02efee":"code","442c4633":"code","07b4856b":"code","2441b77a":"code","71bdae51":"code","312c766c":"code","9484a7c7":"code","3090323b":"code","a1028853":"code","f4e17c6b":"code","910c24e0":"code","a6e0fb92":"code","c2592809":"code","b850ae7d":"code","79b555a1":"code","d2d4aaad":"code","decfc271":"code","9152788f":"code","7bf57690":"code","2b67d358":"code","62ea214c":"code","049af5a6":"code","45c7da2b":"code","ff624f11":"code","d7507ba7":"code","bbd6db01":"code","00ee1706":"code","f55dfb2e":"code","436e2f3d":"code","6a24a4b8":"code","616428ec":"code","8c24714f":"code","adb2b1b5":"code","295eda37":"markdown","f6360c25":"markdown","d14408ef":"markdown","33eccdfa":"markdown","998b9a29":"markdown","ac616283":"markdown","d1dc65c3":"markdown","9ad3e338":"markdown","041279ff":"markdown","42d80780":"markdown","85f220c7":"markdown","29a95ba1":"markdown","8c7ec4aa":"markdown","c9aed27f":"markdown","5969eb35":"markdown","e9d78c61":"markdown","0008361d":"markdown","16096a6f":"markdown","705f477f":"markdown"},"source":{"88718c70":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","db282883":"import matplotlib.pyplot as plt\nimport seaborn as sns\n\nimport tensorflow as tf\nfrom tensorflow import keras\n\nfrom sklearn.model_selection import train_test_split\n\nimport re\nfrom nltk.corpus import stopwords\n\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences","7fc66ecf":"df_test = pd.read_csv(\"..\/input\/nlp-getting-started\/test.csv\")\n\ndf_train = pd.read_csv(\"..\/input\/nlp-getting-started\/train.csv\")\ndf_train.head()","f508a057":"df_test.head()","40d912ac":"df_train.drop(['keyword','location'],axis=1,inplace=True)\ndf_test.drop(['keyword','location'],axis=1,inplace=True)\n\nprint(f\"train shape >> {df_train.shape}\")\nprint(f\"test shape >> {df_test.shape}\")\n\n\ndf_train.head()","40e5bcbb":"sns.set_style('darkgrid')\nsns.countplot(x=df_train.target)\nplt.show()","ef02efee":"df_train.text.head()","442c4633":"at_chunk = re.compile(r\"@[a-zA-Z0-9]*\")\nurl = re.compile(r\"https?:\/+[a-zA-Z0-9.\/]*\")\nshortword = re.compile(r\"\\b\\w{1,2}\\b\")\n\nstop_words = set(stopwords.words('english'))\n\ndef clean(text):\n    text = re.sub('#','',text)\n    text = re.sub(at_chunk,'',text)\n    text = re.sub(url,'',text)\n    text = re.sub(shortword,'',text)\n    \n    text = text.split()\n    text = [w for w in text if w not in stop_words]\n    \n    text = \" \".join(text)\n    text = text.strip()\n    \n    return text\n\ndf_train.text = df_train.text.apply(clean)\ndf_test.text = df_test.text.apply(clean)","07b4856b":"df_train.text.head()","2441b77a":"df_train.text.isnull().any()","71bdae51":"tok = Tokenizer()\n\ntrain_text = df_train.text\ntrain_label = df_train.target\ntest_text = df_test.text\n\ntok.fit_on_texts(train_text)\n\nword_size = len(tok.index_word)\nvocab_size = word_size+1\n\nprint(f\"{word_size} words are used!\")\n\nprint(\"Tokenizing train texts\\n\")\ntrain_text = tok.texts_to_sequences(train_text)\nprint(\"Tokenizing train texts finished!\\n\")\n\nprint(\"Tokenizng test texts with the same tokenizer\\n\")\ntest_text = tok.texts_to_sequences(test_text)\nprint(\"Tokenizing test texts finished!\\n\")","312c766c":"lengths = [len(s) for s in train_text]\nprint(f\"Max of sequence size >> {np.max(lengths)}\")\nprint(f\"Average of sequence size >> {int(np.round(np.mean(lengths)))}\")\n\nplt.hist(lengths,bins=100)\nplt.show()\n\nsequence_size=21","9484a7c7":"train_text = pad_sequences(train_text,maxlen=sequence_size,padding='post',truncating='post')\ntest_text = pad_sequences(test_text,maxlen=sequence_size,padding='post',truncating='post')\n\nprint(f\"train text shape >> {train_text.shape}\")\nprint(f\"test text shape >> {test_text.shape}\")","3090323b":"train_data = train_text\ntest_data = test_text","a1028853":"from keras.layers import Input,Embedding,Bidirectional,LSTM,TimeDistributed,Dense,Dropout,BatchNormalization,GlobalMaxPool1D,GlobalAveragePooling1D\nfrom keras.utils import plot_model\nfrom keras.models import Model\n\n#vocab_size\n#sequence_size\nword_vec_size = 128\nhidden_size = 128\n\ndef create_lstm1():\n    X = Input(shape=[sequence_size])\n    \n    H = Embedding(vocab_size,word_vec_size,input_length=sequence_size,mask_zero=True)(X)\n    H = Dropout(0.1)(H)\n    H = Bidirectional(LSTM(hidden_size,return_sequences=True))(H)\n    H = Dropout(0.1)(H)\n    H = Bidirectional(LSTM(hidden_size,return_sequences=False))(H)\n    H = BatchNormalization()(H)\n    H = Dense(32,activation='relu')(H)\n    H = BatchNormalization()(H)\n    \n    Y = Dense(1,activation='sigmoid')(H)\n    \n    model = Model(X,Y)\n    model.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])\n    \n    return model","f4e17c6b":"lstm1 = create_lstm1()\nhist = lstm1.fit(train_data,train_label,validation_split=0.1,epochs=10,batch_size=32)","910c24e0":"plot_model(lstm1)","a6e0fb92":"def create_lstm2():\n    X = Input(shape=[sequence_size])\n    \n    H = Embedding(vocab_size,word_vec_size,input_length=sequence_size,mask_zero=True)(X)\n    H = Dropout(0.1)(H)\n    H = Bidirectional(LSTM(hidden_size,return_sequences=True))(H)\n    H = Dropout(0.1)(H)\n    H = Bidirectional(LSTM(hidden_size,return_sequences=True))(H)\n    H = GlobalAveragePooling1D()(H)\n    H = BatchNormalization()(H)\n    H = Dense(32,activation='relu')(H)\n    H = BatchNormalization()(H)\n    \n    Y = Dense(1,activation='sigmoid')(H)\n    \n    model = Model(X,Y)\n    model.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])\n    \n    return model","c2592809":"lstm2 = create_lstm2()\nhist = lstm2.fit(train_data,train_label,validation_split=0.1,epochs=10,batch_size=32)","b850ae7d":"plot_model(lstm2)","79b555a1":"def create_lstm3():\n    X = Input(shape=[sequence_size])\n    \n    H = Embedding(vocab_size,word_vec_size,input_length=sequence_size,mask_zero=True)(X)\n    H = Dropout(0.1)(H)\n    H = Bidirectional(LSTM(hidden_size,return_sequences=True))(H)\n    H = Dropout(0.1)(H)\n    H = Bidirectional(LSTM(hidden_size,return_sequences=True))(H)\n    H = GlobalMaxPool1D()(H)\n    H = BatchNormalization()(H)\n    H = Dense(32,activation='relu')(H)\n    H = BatchNormalization()(H)\n    \n    Y = Dense(1,activation='sigmoid')(H)\n    \n    model = Model(X,Y)\n    model.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])\n    \n    return model","d2d4aaad":"lstm3 = create_lstm3()\nhist = lstm3.fit(train_data,train_label,validation_split=0.1,epochs=7,batch_size=32)","decfc271":"plot_model(lstm3)","9152788f":"import os\n\n!wget http:\/\/nlp.stanford.edu\/data\/glove.6B.zip\n!unzip glove*.zip","7bf57690":"embedding_dict = dict()\n\nf = open(os.path.join('glove.6B.200d.txt'),encoding='utf-8')\n\nfor line in f:\n    tokens = line.split()\n    word = tokens[0]\n    vector = tokens[1:]\n    vector =  np.asarray(vector,dtype='float32')\n    embedding_dict[word] = vector\n    \nf.close()\n\nembedding_size = len(embedding_dict['world'])\nprint(f\"There are {len(embedding_dict)} embedding vectors in total\")\nprint(f\"The size of embedding vector here >> {embedding_size}\")\n\nembedding_matrix =  np.zeros((vocab_size,embedding_size))\nfor word,idx in tok.word_index.items():\n    vector = embedding_dict.get(word)\n    if vector is not None:\n        embedding_matrix[idx] = np.asarray(vector,dtype='float32')","2b67d358":"#embedding_size\ndef create_lstm_glove1():\n    X = Input(shape=[sequence_size])\n    \n    H = Embedding(vocab_size,embedding_size,input_length=sequence_size,weights=[embedding_matrix],trainable=False,mask_zero=True)(X)\n    H = Dropout(0.2)(H)\n    H = Bidirectional(LSTM(hidden_size,return_sequences=True))(H)\n    H = Dropout(0.1)(H)\n    H = Bidirectional(LSTM(hidden_size,return_sequences=True))(H)\n    H = Dropout(0.1)(H)\n    H = Bidirectional(LSTM(hidden_size,return_sequences=True))(H)\n    H = GlobalMaxPool1D()(H)\n    H = BatchNormalization()(H)\n    \n    H = Dense(32,activation='relu')(H)\n    H = BatchNormalization()(H)\n    Y = Dense(1,activation='sigmoid')(H)\n    \n    model = Model(X,Y)\n    model.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])\n    \n    return model","62ea214c":"lstm_glove1 = create_lstm_glove1()\nhist = lstm_glove1.fit(train_data,train_label,validation_split=0.1,epochs=6,batch_size=32)","049af5a6":"plot_model(lstm_glove1)","45c7da2b":"#embedding_size\ndef create_lstm_glove2():\n    X = Input(shape=[sequence_size])\n    \n    H = Embedding(vocab_size,embedding_size,input_length=sequence_size,weights=[embedding_matrix],trainable=False,mask_zero=True)(X)\n    H = Dropout(0.2)(H)\n    H = Bidirectional(LSTM(hidden_size,return_sequences=True))(H)\n    H = Dropout(0.1)(H)\n    H = Bidirectional(LSTM(hidden_size,return_sequences=True))(H)\n    H = Dropout(0.1)(H)\n    H = Bidirectional(LSTM(hidden_size,return_sequences=True))(H)\n    H = GlobalAveragePooling1D()(H)\n    H = BatchNormalization()(H)\n    \n    H = Dense(32,activation='relu')(H)\n    H = BatchNormalization()(H)\n    Y = Dense(1,activation='sigmoid')(H)\n    \n    model = Model(X,Y)\n    model.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])\n    \n    return model","ff624f11":"lstm_glove2 = create_lstm_glove2()\nhist = lstm_glove2.fit(train_data,train_label,validation_split=0.1,epochs=7,batch_size=32)","d7507ba7":"plot_model(lstm_glove2)","bbd6db01":"from keras.layers import Conv1D,Concatenate,LeakyReLU,Flatten\n\ndef create_conv1():\n    X = Input(shape=[sequence_size])\n    \n    H = Embedding(vocab_size,embedding_size,input_length=sequence_size,mask_zero=True,weights=[embedding_matrix],trainable=True)(X)\n    H = Dropout(0.1)(H)\n    \n    conv_blocks=[]\n    kernel_filters=[256,256,128,128]\n    kernel_sizes=[3,4,5,6]\n    \n    for i in range(len(kernel_sizes)):\n        conv = Conv1D(filters=kernel_filters[i],kernel_size=kernel_sizes[i])(H)\n        conv = GlobalMaxPool1D()(conv)\n        conv = Flatten()(conv)\n        conv_blocks.append(conv)\n    \n    H = Concatenate()(conv_blocks)\n    H = Dropout(0.1)(H)\n    \n    H = Dense(256)(H)\n    H = BatchNormalization()(H)\n    H = LeakyReLU()(H)\n    \n    H = Dense(32)(H)\n    H = BatchNormalization()(H)\n    H = LeakyReLU()(H)\n    \n    Y = Dense(1,activation='sigmoid')(H)\n        \n    model = Model(X,Y)\n    model.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])\n    \n    return model","00ee1706":"conv1 = create_conv1()\nhist = conv1.fit(train_data,train_label,validation_split=0.1,epochs=7,batch_size=32)","f55dfb2e":"plot_model(conv1)","436e2f3d":"def create_conv2():\n    X = Input(shape=[sequence_size])\n    \n    H = Embedding(vocab_size,embedding_size,input_length=sequence_size,mask_zero=True,weights=[embedding_matrix],trainable=True)(X)\n    H = Dropout(0.1)(H)\n    \n    conv_blocks=[]\n    kernel_filters=[256,256,128,128]\n    kernel_sizes=[3,4,5,6]\n    \n    for i in range(len(kernel_sizes)):\n        conv = Conv1D(filters=kernel_filters[i],kernel_size=kernel_sizes[i])(H)\n        conv = Conv1D(filters=kernel_filters[i],kernel_size=kernel_sizes[i])(conv)\n        conv = GlobalMaxPool1D()(conv)\n        conv = Flatten()(conv)\n        conv_blocks.append(conv)\n    \n    H = Concatenate()(conv_blocks)\n    H = Dropout(0.1)(H)\n    \n    H = Dense(256)(H)\n    H = BatchNormalization()(H)\n    H = LeakyReLU()(H)\n    \n    H = Dense(32)(H)\n    H = BatchNormalization()(H)\n    H = LeakyReLU()(H)\n    \n    Y = Dense(1,activation='sigmoid')(H)\n        \n    model = Model(X,Y)\n    model.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])\n    \n    return model","6a24a4b8":"conv2 = create_conv2()\nhist = conv2.fit(train_data,train_label,validation_split=0.1,epochs=7,batch_size=32)","616428ec":"plot_model(conv2)","8c24714f":"#lstm2, lstm_glove1, conv1\n#test_data\n\ntest_id = df_test.id\n\ndef get_submission(model,filename):\n    pred = model.predict(test_data)\n    pred = pred.reshape(-1)\n    submission = pd.DataFrame({\n        'id':test_id,\n        'target':pred\n    })\n    submission.target = submission.target.apply(lambda x:1 if x>0.5 else 0)\n    print(\"Making submission DataFrame Finished!\")\n    submission.to_csv(filename+\".csv\",index=False)\n    print(\"Making CSV file Finished!\\n\\n\")\n    return submission","adb2b1b5":"get_submission(lstm2,\"lstm2\")\nget_submission(lstm_glove1,\"lstm_glove1\")\nget_submission(conv1,\"conv1\")\n\nprint(0)","295eda37":"(1-2) Bidirectional Stacked LSTM model without using pre-trained Embedding Vectors : return all the hidden cells and use global average pooling","f6360c25":"(5) Submission","d14408ef":"[2] Preprocessing","33eccdfa":"-> I think the balance is ok","998b9a29":"(4) Padding","ac616283":"(1-3) Bidirectional Stacked LSTM model without using pre-trained Embedding Vectors : return all the hidden cells and use global max pooling","d1dc65c3":"(1-1) Bidirectional stacked LSTM without pre-trained Embedding Vectors from GLOVE : return only last hidden cell on the last lstm layer","9ad3e338":"(1) Remove #, @chunk, urls, shortwords(length is either 1 or 2), stopwords","041279ff":"(2-1) Bidirectional Stacked LSTM model with pre-trained Embedding vectors","42d80780":"[3] Modelling","85f220c7":"(3-1) Multi-kernel Conv1D model using pre-trained Embedding vectors","29a95ba1":"[2-3] Text Preprocessing","8c7ec4aa":"[2-2] Check if imbalanced","c9aed27f":"(4) BERT","5969eb35":"[1] Load dataset, both train & test","e9d78c61":"[2-1] Drop unnecessary columns","0008361d":"(3-2) Multi-kernel Stacked Conv1D model using pre-trained Embedding vectors","16096a6f":"(2) Check NULL","705f477f":"(3) Tokenizer"}}