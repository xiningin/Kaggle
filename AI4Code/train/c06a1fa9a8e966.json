{"cell_type":{"82d41b31":"code","569a3e9e":"code","ebe02611":"code","673ff17f":"code","370bba43":"code","50554083":"code","905b20d3":"code","7cb5cbd0":"code","42a3b89d":"code","39e39fc8":"code","f6cb49f0":"code","4e7faa95":"code","6dc0f403":"code","9ce7e9ca":"code","f6a7c2e6":"code","08cc3a5f":"code","2201936e":"code","854e6ad6":"markdown","60586246":"markdown","56fa0d25":"markdown","cdf50e0c":"markdown","5473e66a":"markdown","5fe7ac37":"markdown","8c562d4f":"markdown","be50f65b":"markdown","3ada8828":"markdown"},"source":{"82d41b31":"# Imports required packages\n\nimport numpy as np\nimport pandas as pd\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.metrics import mean_squared_error\n\nimport tensorflow as tf\nimport tensorflow.keras as keras\nfrom tensorflow.keras import layers\nfrom tensorflow.keras.layers.experimental.preprocessing import TextVectorization\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.metrics import RootMeanSquaredError\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns","569a3e9e":"# Some initialization\n\nsns.set_theme()","ebe02611":"# Loads data\n\ntrain = pd.read_csv(\"..\/input\/commonlitreadabilityprize\/train.csv\")\ntest = pd.read_csv(\"..\/input\/commonlitreadabilityprize\/test.csv\")\nsubmission = pd.read_csv(\"..\/input\/commonlitreadabilityprize\/sample_submission.csv\")","673ff17f":"# Checks how the data looks from top few samples\n\ndisplay(train.head())","370bba43":"# Making all excerpts in lowercase\n\ntrain.excerpt = train.excerpt.str.lower()","50554083":"# Sets configurations\n\nn_bins = 20\nn_splits = 5","905b20d3":"# Segments discrete interval of label by marking each sample with a bin number\n\ntrain[\"bin\"] = pd.cut(x=train.target, bins=n_bins, labels=[i for i in range(n_bins)])","7cb5cbd0":"# Ensurs interval of each bin is near to equal\n\ndisplay(train.groupby([\"bin\"]).target.apply(lambda x: x.max()-x.min()))","42a3b89d":"# Creates K-Fold cross validator\n\nk_fold_cv= StratifiedKFold(n_splits=n_splits, shuffle=True)\n\nvalidation_scores = []    # Stores validation scores across folds\ntest_predictions = np.zeros(shape=(len(submission), n_splits))","39e39fc8":"# Create K-Fold cross validation splitter\n\nk_fold_cv_generator = k_fold_cv.split(train, y=train.bin)\n\n# Plots to verify if every split gets equal distribution of samples from every bin\n\nplt.figure(figsize=(12, 6))\n\nfor fold, (id_x_train, id_x_val) in enumerate(k_fold_cv_generator):\n    sns.histplot(data=train.iloc[id_x_train], x=\"target\", bins=n_bins, fill=False, label=f\"Fold {fold}\")\n\nplt.title('Label Distribution for Each Fold')\nplt.legend()\nplt.show()","f6cb49f0":"# Removes columns considered not to be required\n\ntrain.drop([\"url_legal\", \"license\"], axis=1, inplace=True)\ntest.drop([\"url_legal\", \"license\"], axis=1, inplace=True)","4e7faa95":"max_features = 30000    # Vocabulary size was little more than 30000 found during analysis\nmax_length = 250        # Considering a little more than 205 found during analysis\nembedding_dim = 300     # 300 dimensional GloVe word embedding is being considered\nbatch_size = 32         # Size of the batch of samples to take as input at a time during model training\nepochs = 20             # Number of epochs to train model","6dc0f403":"# Creates vectorizer\nvectorizer = TextVectorization(max_tokens=max_features, output_sequence_length=max_length)","9ce7e9ca":"# Extracts and loads vectors from GloVe word embeddings\n\nembeddings_index = {}\n\nwith open(\"..\/input\/glove6b\/glove.6B.300d.txt\") as file:\n    for i, line in enumerate(file):\n        word, coefs = line.split(maxsplit=1)\n        coefs = np.fromstring(coefs, dtype=float, sep=\" \")\n        embeddings_index[word] = coefs","f6a7c2e6":"fig, axes = plt.subplots(1, n_splits, sharex=True, sharey=True, figsize=(20, 5))\nfig.suptitle(\"K-Fold Cross Validation Performance\")\n\nk_fold_cv_generator = k_fold_cv.split(train, y=train.bin)\n\n# Iterates over each fold to train the linear regression model, \n# predict on validation data and stores into a list to get its mean at the end\n\nfor fold, (idx_train, idx_val) in enumerate(k_fold_cv_generator):\n    print(f\"Fold {fold}...\")\n\n    text_ds = tf.data.Dataset.from_tensor_slices(train.excerpt.iloc[idx_train]).batch(32)\n    vectorizer.adapt(text_ds)\n    \n    voc = vectorizer.get_vocabulary()\n    word_index = dict(zip(voc, range(len(voc))))\n    \n    missing_words_in_pretrained_embeddings = 0\n    embeddings_matrix = np.zeros((max_features, embedding_dim))\n    for word, i in word_index.items():\n        embedding_vector = embeddings_index.get(word)\n        if embedding_vector is not None:\n            embeddings_matrix[i] = embedding_vector\n        else:\n            missing_words_in_pretrained_embeddings += 1\n    print(f\"\\tNumber of missing words in Glove: {missing_words_in_pretrained_embeddings}\")\n    \n    embedding_layer = layers.Embedding(max_features, \n                                   embedding_dim, \n                                   embeddings_initializer=keras.initializers.Constant(embeddings_matrix),\n                                   input_length=max_length, \n                                   trainable=False)\n    \n    sequences_input = layers.Input(shape=(None,), dtype=\"int64\")\n    x = embedding_layer(sequences_input)\n    x = layers.Conv1D(128, 5, activation=\"relu\")(x)\n    x = layers.MaxPooling1D(5)(x)\n    x = layers.Conv1D(128, 5, activation=\"relu\")(x)\n    x = layers.MaxPooling1D(5)(x)\n    x = layers.Conv1D(128, 5, activation=\"relu\")(x)\n    x = layers.GlobalMaxPooling1D()(x)\n    x = layers.Dense(128, activation=\"relu\")(x)\n    #x = layers.Dropout(0.1)(x)\n    predictions_output = layers.Dense(1)(x)\n\n    model = keras.Model(sequences_input, predictions_output)\n    \n    x_train = vectorizer(np.array([[s] for s in train.excerpt.iloc[idx_train]])).numpy()\n    x_val = vectorizer(np.array([[s] for s in train.excerpt.iloc[idx_val]])).numpy()\n\n    y_train = np.array(train.target.iloc[idx_train])\n    y_val = np.array(train.target.iloc[idx_val])\n    \n    print(\"\\tTraining model...\", end=\" \")\n    model.compile(optimizer=\"rmsprop\", loss= \"mse\", metrics=[RootMeanSquaredError()])\n    history = model.fit(x_train, y_train, batch_size=batch_size, epochs=epochs, \n                        verbose=False, shuffle=False, validation_data=(x_val, y_val))\n    print(\"completed.\")\n    \n    history = history.history\n    \n    axes[fold].set_title(f\"Fold {fold}\")\n    axes[fold].plot(range(1, len(history[\"root_mean_squared_error\"]) + 1), history[\"root_mean_squared_error\"], \"bo\", label=\"Loss\")\n    axes[fold].plot(range(1, len(history[\"val_root_mean_squared_error\"]) + 1), history[\"val_root_mean_squared_error\"], \"b\", label=\"Validation Loss\")\n    axes[fold].set_xlabel(\"Epochs\")\n    if fold == 0:\n        axes[fold].set_ylabel(\"Loss (RMSE)\")\n    axes[fold].legend()\n    \n    rmse = np.mean(history[\"val_root_mean_squared_error\"])\n    validation_scores.append(rmse)\n    print(f\"\\tRMSE over validation data: {rmse}\")\n    # Performs predictions on test data\n    print(f\"\\tPerforming predictions on fold {fold}...\", end=\" \")\n    x_test = vectorizer(np.array([[s] for s in test.excerpt])).numpy()\n    predictions = model.predict(x_test)\n    predictions.resize((len(test)))\n    test_predictions[:,fold] = predictions.copy()\n    print(\"done.\")\n    print()\n    \nprint(f\"K-Fold cross validation RMSE performance: {np.mean(validation_scores)}\")","08cc3a5f":"# Averaging predictions across folds\n\nsubmission.target = test_predictions.mean(axis=1)","2201936e":"# Submitting by saving predictions into submission file\n\nsubmission.to_csv(\"submission.csv\", index=False)","854e6ad6":"## Submission","60586246":"## Configurations","56fa0d25":"As the distribution of labels are not equal, the labels should be splitted with stratification so that each fold get nearly equal distribution from data segments. ","cdf50e0c":"## Modeling","5473e66a":"## Determinining Baseline Performance with 1D ConvNet with Pre-trained Word Embeddings","5fe7ac37":"## Data Preparation","8c562d4f":"### Segmenting (Discrete Interval) Label","be50f65b":"# **CommonLit Readability Assessment**","3ada8828":"### Loading Data & Counting Samples"}}