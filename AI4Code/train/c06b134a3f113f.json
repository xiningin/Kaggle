{"cell_type":{"bf1caa62":"code","ea2e2138":"code","6e6f64e8":"code","3d1991c6":"code","566183c4":"code","63227a2f":"code","83848059":"code","89d3f2a7":"code","8a79425c":"code","bb6f342a":"code","fbe89bf3":"code","a961852e":"code","d4cb59bc":"code","0cbd47a7":"code","d054941f":"code","04a63f49":"markdown","49f0508c":"markdown","0fdc326c":"markdown","5ace106c":"markdown","6070ace9":"markdown","aff3990d":"markdown","910ebba8":"markdown","4382a4b5":"markdown","870be068":"markdown","d68ca6dd":"markdown","c21785c3":"markdown","0911e183":"markdown"},"source":{"bf1caa62":"#Importing packages\nimport numpy as np \nimport pandas as pd\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")","ea2e2138":"#Load dataset\ntrain = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/train.csv')\ntest  = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/test.csv')","6e6f64e8":"#Feature to predict\ntarget = list(set(train.columns) - set(test.columns))\ntarget = target[0] ","3d1991c6":"X_train = train.loc[:, train.columns != target]\nY_train = train[target]\n\nX_test = test\n\n#Drop id\nX_test_id = X_test[\"Id\"]\n\nX_train.drop(columns='Id',inplace=True)\nX_test.drop(columns='Id',inplace=True)\n\nX_train.reset_index(drop=True, inplace=True)\nX_test.reset_index(drop=True, inplace=True)","566183c4":"def fillNa_df(df):\n    \n    #select object columns\n    obj_col = df.columns[df.dtypes == 'object'].values\n\n    #select non object columns\n    num_col = df.columns[df.dtypes != 'object'].values\n\n    #replace null value in obj columns with None\n    df[obj_col] = df[obj_col].fillna('None')\n\n    #replace null value in numeric columns with 0\n    df[num_col] = df[num_col].fillna(0)\n    \n    return df\n\nX_train_001 = fillNa_df(X_train)\nX_test_001 = fillNa_df(X_test)","63227a2f":"from sklearn.preprocessing import OneHotEncoder\n\ndef oneHotEncoding(df_train, df_test):\n    \n    #select object columns\n    obj_col = df_train.columns[df_train.dtypes == 'object'].values\n\n    # creating instance of one-hot-encoder\n    enc = OneHotEncoder(handle_unknown='ignore')\n\n    # Ordinal features\n    ordinal_features = [x for x in obj_col]\n\n    # passing cat column (label encoded values)\n    df_train_encoded = pd.DataFrame(enc.fit_transform(df_train[ordinal_features]).toarray())\n    df_test_encoded  = pd.DataFrame(enc.transform(df_test[ordinal_features]).toarray())\n    \n    df_train_encoded.reset_index(drop=True, inplace=True)\n    df_test_encoded.reset_index(drop=True, inplace=True)\n\n    # merge with main df\n    df_train_encoded = pd.concat([df_train, df_train_encoded], axis=1)\n    df_test_encoded  = pd.concat([df_test,  df_test_encoded], axis=1)\n\n    # drop ordinal features\n    df_train_encoded.drop(columns=ordinal_features, inplace=True)\n    df_test_encoded.drop(columns=ordinal_features, inplace=True)\n    \n    return df_train_encoded, df_test_encoded\n\nX_train_002, X_test_002 = oneHotEncoding(X_train_001, X_test_001)","83848059":"def featureEng(df):\n\n    #TotalBath\n    df['TotalBath'] = (df['FullBath'] + df['HalfBath'] + df['BsmtFullBath'] + df['BsmtHalfBath'])\n\n    #TotalPorch\n    df['TotalPorch'] = (df['OpenPorchSF'] + df['3SsnPorch'] + df['EnclosedPorch'] + df['ScreenPorch'] + df['WoodDeckSF'])\n\n    #Modeling happen during the sale year\n    df[\"RecentRemodel\"] = (df[\"YearRemodAdd\"] == df[\"YrSold\"]) * 1\n\n    #House sold in the year it was built\n    df[\"NewHouse\"] = (df[\"YearBuilt\"] == df[\"YrSold\"]) * 1\n\n    #YrBltAndRemod\n    df[\"YrBltAndRemod\"] = df[\"YearBuilt\"] + df[\"YearRemodAdd\"]\n\n    #Total_sqr_footage\n    df[\"Total_sqr_footage\"] = df[\"BsmtFinSF1\"] + df[\"BsmtFinSF2\"] + df[\"1stFlrSF\"] + df[\"2ndFlrSF\"]\n\n    df['TotalSF'] = df['TotalBsmtSF'] + df['1stFlrSF'] + df['2ndFlrSF']\n\n    df['Area_Qual'] = df['TotalSF'] * df['OverallQual']\n\n    #HasPool\n    df['HasPool'] = df['PoolArea'].apply(lambda x: 1 if x > 0 else 0)\n\n    #HasFireplaces\n    df['HasFirePlace'] = df['Fireplaces'].apply(lambda x: 1 if x > 0 else 0)\n\n    #Has2ndFloor\n    df['Has2ndFloor'] = df['2ndFlrSF'].apply(lambda x: 1 if x > 0 else 0)\n\n    #HasGarage\n    df['HasGarage'] = df['GarageArea'].apply(lambda x: 1 if x > 0 else 0)\n\n    #HasBsmnt\n    df['HasBsmnt'] = df['TotalBsmtSF'].apply(lambda x: 1 if x > 0 else 0)\n    \n    return df\n\n#Feature Engineering\nX_train_003 = featureEng(X_train_002)\nX_test_003 = featureEng(X_test_002)","89d3f2a7":"from sklearn.ensemble import IsolationForest\nfrom sklearn.covariance import EllipticEnvelope\nfrom sklearn.neighbors import LocalOutlierFactor\nfrom sklearn.svm import OneClassSVM\n\n\ndef dropoutlier(df_X, df_Y, method='IsolationForest'):\n\n    if(method=='IsolationForest'):\n        #Isolation Forest\n\n        # identify outliers in the training dataset\n        iso = IsolationForest(contamination=0.01)\n        yhat = iso.fit_predict(df_X)\n\n    if(method=='MinimumCovarianceDeterminant'):\n        #Minimum Covariance Determinant\n\n        # identify outliers in the training dataset\n        ee = EllipticEnvelope(contamination=0.01)\n        yhat = ee.fit_predict(df_X)\n\n    if(method=='LocalOutlierFactor'):\n        #Local Outlier Factor\n\n        # identify outliers in the training dataset\n        lof = LocalOutlierFactor()\n        yhat = lof.fit_predict(df_X)\n\n    if(method=='OneClassSVM'):\n        #One-Class SVM\n\n        # identify outliers in the training dataset\n        ee = OneClassSVM(nu=0.001)\n        yhat = ee.fit_predict(df_X)\n\n    # select all rows that are not outliers\n    mask = yhat != -1\n    df_X_drop, df_Y_drop = df_X[mask], df_Y[mask]\n\n    # select all rows that are outliers\n    masko = yhat == -1\n    df_X_o, df_Y_o = df_X[masko], df_Y[masko]\n    \n    return df_X, df_Y, [df_X_o, df_Y_o]\n\n#Drop outliers\nX_train_004, Y_train_004, df_o = dropoutlier(X_train_003, Y_train)\n\n# summarize the shape of the updated training dataset\nprint('Total: ', X_train_004.shape)\nprint('Not Outliers: ', X_train_003.shape)\nprint('Outliers: ', df_o[0].shape)","8a79425c":"#Plot GrLivArea vs SalePrice\nplt.scatter(X_train_004['GrLivArea'], Y_train_004, color='blue', alpha=0.5)\nplt.scatter(df_o[0]['GrLivArea'],   df_o[1],   color='red',  alpha=0.5, label='Outlier')\nplt.legend(loc=\"upper left\")\nplt.title(\"GrLivArea vs SalePrice\")\nplt.xlabel(\"GrLivArea\")\nplt.ylabel(\"SalePrice\")\nplt.show()","bb6f342a":"#Importing packages\nfrom sklearn.model_selection import train_test_split\n\n#Particiona o data set originalmente Train em Train(Treino) e Val(valida\u00e7\u00e3o)\nX_train_005, X_val_005, Y_train_005, Y_val_005 = train_test_split(X_train_004, \n                                                                  Y_train_004, \n                                                                  test_size=0.2, \n                                                                  random_state=42)\n\nX_train_005.shape, X_val_005.shape","fbe89bf3":"import tensorflow as tf\n\nfrom keras.models import Sequential\nfrom keras.layers import Dense, DenseFeatures, Activation, Flatten, Dropout, BatchNormalization\nfrom keras.optimizers import RMSprop,Adam\nfrom keras.callbacks import ReduceLROnPlateau","a961852e":"NN_model = Sequential()\n\n# The Input Layer :\nNN_model.add(Dense(324, kernel_initializer='normal',input_dim = X_train_005.shape[1], activation='relu'))\n\n# The Hidden Layers :\nNN_model.add(Dense(512, activation='relu'))\nNN_model.add(Dense(1024, activation='relu'))\nNN_model.add(Dense(1024, activation='relu'))\nNN_model.add(Dense(512, activation='relu'))\nNN_model.add(Dense(256, activation='relu'))\n\n\nNN_model.add(Flatten())\n\n\nNN_model.add(Dense(4096, activation='relu'))\nNN_model.add(Dropout(0.5))\n\nNN_model.add(Dense(4096, activation='relu'))\nNN_model.add(Dropout(0.5))\n\n# The Output Layer :\nNN_model.add(Dense(1, kernel_initializer='normal',activation='linear', kernel_regularizer = tf.keras.regularizers.l1(l=0.01) ))\n\noptimizer=Adam(learning_rate=0.01, beta_1=0.9, beta_2=0.999)\n\n# Compile the network :\nNN_model.compile(optimizer=optimizer, loss='mean_absolute_error', metrics=['mean_absolute_error'])\nNN_model.summary()","d4cb59bc":"NN_model.fit(X_train_005, Y_train_005, epochs=100, batch_size=128, verbose=0)","0cbd47a7":"results = NN_model.evaluate(X_val_005, Y_val_005)","d054941f":"def make_submission(prediction, sub_name):\n  my_submission = pd.DataFrame({'Id':X_test_id,'SalePrice':prediction})\n  my_submission.to_csv('{}.csv'.format(sub_name),index=False)\n  print('A submission file has been made')\n\nY_test_pred = NN_model.predict(X_test_003)\n\nmake_submission(Y_test_pred[:,0],'submission')","04a63f49":"## Encoding ordinal\/categorical features","49f0508c":"# Test the model","0fdc326c":"## Fill NaN values","5ace106c":"# Split dataframe - Train Validation","6070ace9":"## X\/Y datasets","aff3990d":"# Feature Engineering","910ebba8":"## Model evaluate","4382a4b5":"## Train the model","870be068":"# Imports","d68ca6dd":"# Outlier Detection\n\nPerhaps the most important hyperparameter in the model is the \u201ccontamination\u201d argument, which is used to help estimate the number of outliers in the dataset. This is a value between 0.0 and 0.5 and by default is set to 0.1.","c21785c3":"# Processing the dataset","0911e183":"# Make the Deep Neural Network"}}