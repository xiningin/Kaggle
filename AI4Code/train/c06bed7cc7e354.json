{"cell_type":{"8c78431c":"code","adf07ec5":"code","06f2d817":"code","7b5a1dd6":"code","2d94a962":"code","77e646a2":"code","89fdc362":"code","3b5ad8d4":"code","e03e3359":"code","51e314d9":"code","6b942fbc":"code","9f693b00":"code","d282d49e":"code","79a3bd4a":"code","c1976383":"code","9096655f":"code","cc584313":"code","c0fd066c":"code","0b8a74c6":"code","f874887a":"code","4703d853":"code","6b9380d7":"code","98bbeab2":"code","336f31d9":"code","41108a13":"code","cc4fdba5":"code","3817de9c":"code","d4a3c37d":"code","f1aadb15":"code","deee69c6":"code","9756c70b":"code","5297153b":"code","6630e9b3":"code","2ceda30e":"code","f1bdface":"markdown","7522b112":"markdown","6b59debf":"markdown","a75fa85a":"markdown","51201fd6":"markdown","b9eccf5e":"markdown","9378cfad":"markdown","5b46be3e":"markdown","5c19754f":"markdown","79725e9e":"markdown","2f2c7cc3":"markdown","8765ed9e":"markdown","e7b98fbd":"markdown"},"source":{"8c78431c":"import pandas as pd\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport seaborn as sns\nimport pandas_profiling\nimport xgboost as xgb\nimport time\nfrom sklearn.model_selection import train_test_split, GridSearchCV\nfrom sklearn.metrics import mean_squared_log_error\nfrom sklearn.metrics import mean_squared_error\nimport optuna\nfrom sklearn.model_selection import KFold\nfrom xgboost import plot_importance\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import cross_val_score\n\nfrom sklearn.preprocessing import StandardScaler\nfrom statistics import mean\nimport warnings\nwarnings.filterwarnings(\"ignore\")\nimport lightgbm as lgbm","adf07ec5":"df_train = pd.read_csv('..\/input\/envision-racing\/train.csv',skipinitialspace=True)\ndf_train_weather = pd.read_csv('..\/input\/envision-racing\/train_weather.csv',skipinitialspace=True)\ndf_test = pd.read_csv('..\/input\/envision-racing\/test.csv',skipinitialspace=True)\ndf_test_weather = pd.read_csv('..\/input\/envision-racing\/test_weather.csv',skipinitialspace=True)\ndf_submission = pd.read_csv('..\/input\/envision-racing\/submission.csv',skipinitialspace=True)","06f2d817":"pd.set_option('display.max_columns', None)\ndf_train.head(5)","7b5a1dd6":"df_train_weather.head()","2d94a962":"df_test.head(5)","77e646a2":"df_test_weather.head()","89fdc362":"df_test_weather.rename({'EVENTS': 'EVENT'}, axis=1, inplace=True)","3b5ad8d4":"print(df_train.shape)\ndf_train = df_train.drop_duplicates()\nprint(df_train.shape)","e03e3359":"df_train.nunique()","51e314d9":"df_train.info()","6b942fbc":"df_test.nunique()","9f693b00":"df_train.isnull().sum()","d282d49e":"df_train['MSEC'] = df_train['KPH'] * 0.277778\ndf_test['MSEC'] = df_test['KPH'] * 0.277778\n\ndf_train = df_train.drop([\"KPH\"],axis=1)\ndf_train = df_train.reset_index(drop=True)\n\ndf_test = df_test.drop([\"KPH\"],axis=1)\ndf_test = df_test.reset_index(drop=True)","79a3bd4a":"df_train = df_train.drop(['CROSSING_FINISH_LINE_IN_PIT','DRIVER_NUMBER'],axis=1)\ndf_train = df_train.reset_index(drop=True)\n\ndf_test = df_test.drop(['CROSSING_FINISH_LINE_IN_PIT','DRIVER_NUMBER'],axis=1)\ndf_test = df_test.reset_index(drop=True)","c1976383":"def time2seconds(time):\n  if type(time) != str:\n    return time\n\n  parts = [float(p) for p in time.split(':')]\n  parts = [p * (60 ** i) for i, p in enumerate(reversed(parts))]\n  return sum(parts)","9096655f":"TIME_COLUMNS = ['ELAPSED', 'S1', 'S2', 'S3', 'S1_LARGE', 'S2_LARGE', 'S3_LARGE', 'PIT_TIME', 'HOUR']\n\ndf_all = pd.concat([df_train, df_test], axis=0)\n\nfor c in TIME_COLUMNS:\n  df_all[c] = df_all[c].apply(time2seconds)\n#df_all = pd.get_dummies(df_all)  # xgb, in contrast to catboost and others doesn't handle non-numeric types automatically\n\ndf_train = df_all.iloc[:len(df_train)]\ndf_test = df_all.iloc[len(df_train):]\n\ndf_test.rename({'HOUR': 'SECOND'}, axis=1, inplace=True)\ndf_train.rename({'HOUR': 'SECOND'}, axis=1, inplace=True)","cc584313":"rs = np.random.RandomState(0)\ndf = pd.DataFrame(rs.rand(10, 10))\ncorr = df_train.corr()\ncorr.style.background_gradient(cmap='coolwarm')","c0fd066c":"df_train = df_train.drop(['S1_LARGE','S2_LARGE','S3_LARGE','GROUP','S1_IMPROVEMENT','S2_IMPROVEMENT','S3_IMPROVEMENT'],axis=1)\ndf_train = df_train.reset_index(drop=True)\n\ndf_test = df_test.drop(['S1_LARGE','S2_LARGE','S3_LARGE','GROUP','S1_IMPROVEMENT','S2_IMPROVEMENT','S3_IMPROVEMENT'],axis=1)\ndf_test = df_test.reset_index(drop=True)","0b8a74c6":"df_weather = pd.concat([df_train_weather, df_test_weather])\n\ndf_weather[['DATE', 'TIME_UTC']] = df_weather['TIME_UTC_STR'].str.split(' ', expand=True)\n\ndf_weather = df_weather.drop([\"TIME_UTC_STR\"],axis=1)\n\n","f874887a":"df_weather.tail()","4703d853":"df_weather['DATE']= pd.to_datetime(df_weather['DATE'])\n\ndf_weather['WEEKDAY'] = df_weather['DATE'].dt.day_name()\n","6b9380d7":"pd.set_option('display.max_rows', None)\nprint(df_weather.groupby(['LOCATION','EVENT','DATE','WEEKDAY']).size())","98bbeab2":"filters = [\n   (df_train.LOCATION == 'Location 1') & (df_train.EVENT == 'Free Practice 1'),\n   (df_train.LOCATION == 'Location 1') & (df_train.EVENT == 'Free Practice 1'),\n   (df_train.LOCATION == 'Location 1') & (df_train.EVENT == 'Free Practice 1'),\n]\nvalues = [\"2021-02-25\", \"2021-02-26\", \"2021-02-27\"]","336f31d9":"df_train[\"DATE\"] = np.select(filters, values)\ndf_train.head()","41108a13":"df_train.groupby(['DRIVER_NAME','TEAM']).size()","cc4fdba5":"pandas_profiling.ProfileReport(df_train)","3817de9c":"df_train.head()","d4a3c37d":"df_all = pd.concat([df_train, df_test], axis=0)\ndf_all = pd.get_dummies(df_all,drop_first=True)  # xgb, in contrast to catboost and others doesn't handle non-numeric types automatically\n\ndf_train = df_all.iloc[:len(df_train)]\ndf_test = df_all.iloc[len(df_train):]\n","f1aadb15":"#Drop target variable from X and copy to y\ncolumns = [col for col in df_train.columns.to_list()if col not in ['LAP_TIME']]\nData=df_train[columns]\ntarget=df_train['LAP_TIME']","deee69c6":"#XGBRegressor\ndef objective(trial,data=Data,target=target):\n    \n    train_x, test_x, train_y, test_y = train_test_split(Data, target, test_size=0.2, random_state=42)\n    param = {\n        'tree_method':'gpu_hist',  # this parameter means using the GPU when training our model to speedup the training process\n        'lambda': trial.suggest_loguniform('lambda', 1e-3, 10.0),\n        'alpha': trial.suggest_loguniform('alpha', 1e-3, 10.0),\n        'colsample_bytree': trial.suggest_categorical('colsample_bytree', [0.3,0.4,0.5,0.6,0.7,0.8,0.9, 1.0]),\n        'subsample': trial.suggest_categorical('subsample', [0.4,0.5,0.6,0.7,0.8,1.0]),\n        'learning_rate': trial.suggest_categorical('learning_rate', [0.008,0.009,0.01,0.012,0.014,0.016,0.018, 0.02]),\n        'n_estimators': 4000,\n        'max_depth': trial.suggest_categorical('max_depth', [5,7,9,11,13,15,17,20]),\n        'random_state': trial.suggest_categorical('random_state', [24, 48,2020]),\n        'min_child_weight': trial.suggest_int('min_child_weight', 1, 300),\n    }\n    model = xgb.XGBRegressor(**param)  \n    \n    model.fit(train_x,train_y,eval_set=[(test_x,test_y)],early_stopping_rounds=100,verbose=False)\n    \n    preds = model.predict(test_x)\n    \n    rmse = np.sqrt(mean_squared_log_error(test_y, preds)) \n    \n    return rmse\n\nstudy = optuna.create_study(direction='minimize')\nstudy.optimize(objective, n_trials=50)\nprint('Number of finished trials:', len(study.trials))\nprint('Best trial:', study.best_trial.params)\n\n","9756c70b":"\nBest_trial= {'lambda': 2.8459912496382613, 'alpha': 0.37711072310872307, 'colsample_bytree': 0.7, 'subsample': 0.5, 'learning_rate': 0.01, 'max_depth': 5, 'random_state': 2020, 'min_child_weight': 282}\n\n\npreds = np.zeros(df_test.shape[0])\nkf = KFold(n_splits=5,random_state=48,shuffle=True)\nrmse=[]  # list contains rmse for each fold\nn=0\nfor trn_idx, test_idx in kf.split(df_train[columns],df_train['LAP_TIME']):\n    X_tr,X_val=df_train[columns].iloc[trn_idx],df_train[columns].iloc[test_idx]\n    y_tr,y_val=df_train['LAP_TIME'].iloc[trn_idx],df_train['LAP_TIME'].iloc[test_idx]\n    model = xgb.XGBRegressor(**Best_trial)\n    model.fit(X_tr,y_tr,eval_set=[(X_val,y_val)],early_stopping_rounds=100,verbose=False)\n    preds+=model.predict(df_test[columns])\/kf.n_splits\n    rmse.append(np.sqrt(mean_squared_log_error(y_val, model.predict(X_val))))\n    print(n+1,rmse[n])\n    n+=1\nscore = np.sqrt(mean_squared_log_error(y_val, model.predict(X_val)))\nprint(score)","5297153b":"#LGBMRegressor\ndef objective(trial,data=Data,target=target):\n    \n    train_x, test_x, train_y, test_y = train_test_split(Data, target, test_size=0.2, random_state=42)\n    param = {\n         \"n_estimators\": trial.suggest_categorical(\"n_estimators\", [10000]),\n        \"learning_rate\": trial.suggest_float(\"learning_rate\", 0.01, 0.3),\n        \"num_leaves\": trial.suggest_int(\"num_leaves\", 20, 3000, step=20),\n        \"max_depth\": trial.suggest_int(\"max_depth\", 3, 12),\n        \"min_data_in_leaf\": trial.suggest_int(\"min_data_in_leaf\", 200, 10000, step=100),\n        \"lambda_l1\": trial.suggest_int(\"lambda_l1\", 0, 100, step=5),\n        \"lambda_l2\": trial.suggest_int(\"lambda_l2\", 0, 100, step=5),\n        \"min_gain_to_split\": trial.suggest_float(\"min_gain_to_split\", 0, 15),\n        \"bagging_fraction\": trial.suggest_float(\n            \"bagging_fraction\", 0.2, 0.95, step=0.1\n        ),\n        \"bagging_freq\": trial.suggest_categorical(\"bagging_freq\", [1]),\n        \"feature_fraction\": trial.suggest_float(\n            \"feature_fraction\", 0.2, 0.95, step=0.1\n        ),\n        }\n    model = lgbm.LGBMRegressor(**param)  \n    \n    model.fit(train_x,train_y,eval_set=[(test_x,test_y)],early_stopping_rounds=100,verbose=False)\n    \n    preds = model.predict(test_x)\n    \n    rmse = np.sqrt(mean_squared_log_error(test_y, preds)) \n    \n    return rmse\n\n\nstudy = optuna.create_study(direction='minimize')\nstudy.optimize(objective, n_trials=50)\nprint('Number of finished trials:', len(study.trials))\nprint('Best trial:', study.best_trial.params)\n\n\n\n","6630e9b3":"Best_trial= {'n_estimators': 10000, 'learning_rate': 0.2384551197192349, 'num_leaves': 1440, 'max_depth': 7, 'min_data_in_leaf': 900, 'lambda_l1': 20, 'lambda_l2': 40, 'min_gain_to_split': 5.380627283264955, 'bagging_fraction': 0.9, 'bagging_freq': 1, 'feature_fraction': 0.9}\npreds = np.zeros(df_test.shape[0])\nkf = KFold(n_splits=5,random_state=48,shuffle=True)\nrmse=[]  # list contains rmse for each fold\nn=0\nfor trn_idx, test_idx in kf.split(df_train[columns],df_train['LAP_TIME']):\n    X_tr,X_val=df_train[columns].iloc[trn_idx],df_train[columns].iloc[test_idx]\n    y_tr,y_val=df_train['LAP_TIME'].iloc[trn_idx],df_train['LAP_TIME'].iloc[test_idx]\n    model = lgbm.LGBMRegressor(**Best_trial)\n    model.fit(X_tr,y_tr,eval_set=[(X_val,y_val)],early_stopping_rounds=100,verbose=False)\n    preds+=model.predict(df_test[columns])\/kf.n_splits\n    rmse.append(np.sqrt(mean_squared_log_error(y_val, model.predict(X_val))))\n    print(n+1,rmse[n])\n    n+=1\nscore = np.sqrt(mean_squared_log_error(y_val, model.predict(X_val)))\nprint(score)\n","2ceda30e":"df_test = df_test[columns]\nfinal = model.predict(df_test)\nnew = pd.DataFrame({'LAP_TIME': final})\nnew.to_csv('output.csv',index=False)","f1bdface":"As we will change everything into sec , We will change the KPH to MSEC(Meter per second)","7522b112":"We can see that the name of the column is EVENTS rather than EVENT.We will have to rename it","6b59debf":"It looks like some of the events have different dates.If they had one date we could have used it as a feature for df_train","a75fa85a":"## DATA EXPLORATION","51201fd6":"So, It looks like driver J has crossed over two teams","b9eccf5e":"We can see that S1 with S1_LARGE,S3 with S3_LARGE and S2_LARGE and S2 are corelated  nearly 100 percent and if we look into them it looks like *LARGE columns are round of versions of S1,S2 and S3 columns. We will drop *LARGE columns.","9378cfad":"Removing dulicate columns","5b46be3e":"## INFORMARTION\n\n## Attributes\n\nNUMBER: Number in sequence\n\nDRIVER_NUMBER: Driver number\n\n\nLAP_NUMBER: lap number\n\nLAP_TIME: Lap time in seconds\n\nLAP_IMPROVEMENT: Number of Lap Improvement\n\nCROSSING_FINISH_LINE_IN_PIT\n\nS1: Sector 1 in [min:sec.microseconds]\n\nS1_IMPROVEMENT: Improvement in sector 1\n\nS2: Sector 2 in [min:sec.microseconds]\n\nS2_IMPROVEMENT: Improvement in sector 2\n\nS3: Sector 3 in [min:sec.microseconds]\n\nS3_IMPROVEMENT: Improvement in sector 3\n\nKPH: speed in kilometer\/hour\n\nELAPSED: Time elapsed in [min:sec.microseconds]\n\nHOUR: in [min:sec.microseconds]\n\nS1_LARGE: in [min:sec.microseconds]\n\nS2_LARGE: in [min:sec.microseconds]\n\nS3_LARGE: in [min:sec.microseconds]\n\nDRIVER_NAME: Name of the driver\n\nPIT_TIME: time taken to car stops in the pits for fuel and other consumables to be renewed or replenished\n\nGROUP: Group of driver\n\nTEAM: Team name\n\nPOWER: Brake Horsepower(bhp)\n\nLOCATION: Location of the event\n\nEVENT: Free practice or qualifying\n\ntest.csv - 420 rows x 25 columns(Includes target column as LAP_TIME)\n\nsubmission.csv -Please check the Evaluation section for more details on how to generate a valid submission.\n\nThe challenge is to predict the LAP_TIME for the qualifying groups of location 6, 7 and 8.","5c19754f":"We can see that DRIVER_NUMBER and CROSSING_FINISH_LINE_IN_PIT has single values.So, we will be dropping them.","79725e9e":"df_train[\"MSECxELAPSED\"] = df_train[\"MSEC\"] * df_train[\"ELAPSED\"]\ndf_test[\"MSECxELAPSED\"] = df_test[\"MSEC\"] * df_test[\"ELAPSED\"]\n\ndf_train[\"MSECxSECOND\"] = df_train[\"MSEC\"] * df_train[\"SECOND\"]\ndf_test[\"MSECxSECOND\"] = df_test[\"MSEC\"] * df_test[\"SECOND\"]\n\ndf_train = df_train.drop(['SECOND','MSEC','ELAPSED'],axis=1)\ndf_train = df_train.reset_index(drop=True)\n\ndf_test = df_test.drop(['SECOND','MSEC','ELAPSED'],axis=1)\ndf_test = df_test.reset_index(drop=True)\n","2f2c7cc3":"Number is highly corealted with Team and Driver name \nand Team is highly corelated  with Driver name.I will try to figure out if Driver name with Team are distinct or there are some cross over.","8765ed9e":"df_all['DRIVER_NAME'] = df_all['DRIVER_NAME'].astype('category')                                                              \ndf_all['TEAM'] = df_all['TEAM'].astype('category')                                                              \ndf_all['LOCATION'] = df_all['LOCATION'].astype('category')                                                              \ndf_all['EVENT'] = df_all['EVENT'].astype('category')                                                              \n\n\ndf_all['DRIVER_NAME'] = df_all['DRIVER_NAME'].cat.codes\ndf_all['TEAM'] = df_all['TEAM'].cat.codes\ndf_all['LOCATION'] = df_all['LOCATION'].cat.codes\ndf_all['EVENT'] = df_all['EVENT'].cat.codes\n\ndf_train = df_all.iloc[:len(df_train)]\ndf_test = df_all.iloc[len(df_train):]","e7b98fbd":"A F1 race happens in weekend where:\n\n1. Thrusday - Media day\n\n2. Friday -   For testing car and knowing the tracks\n    Free Practice 1                                    \n    Free Practice 2\n\n3. Saturday - It determines the starting order for the final race day\n    Free Practice 3\n\n4. Sunday\n    Race day\n\n\nSo, we will try to find the week day for the dates in the df_train_weather and use that to find the dates for different round in df_train."}}