{"cell_type":{"f9d7cb52":"code","dcaf6de0":"code","e1b779ab":"code","86c0736f":"code","decae0f0":"code","ef92ee36":"code","6b30f32e":"code","fbd2cd52":"code","199cd2a9":"code","46af82c0":"code","8a135abe":"code","f56caf85":"code","35dc8d08":"code","e5207c42":"code","5f16532f":"code","04771c59":"code","6e678ffc":"code","226c1ab5":"code","24938c63":"code","7223fc12":"code","3d8e9f89":"code","88a7bdee":"markdown","972f0ef1":"markdown","7605ff2a":"markdown","491e0b8a":"markdown","c91fe253":"markdown","7e105fdc":"markdown","e8676eaa":"markdown","7db49618":"markdown","331f3926":"markdown","b77c6d25":"markdown","3749bc54":"markdown","466a0d22":"markdown","e993f540":"markdown","231526fc":"markdown","aa0a8dd6":"markdown","bb878498":"markdown","45470370":"markdown","21a47e50":"markdown","d81a065e":"markdown","e61b5208":"markdown","3731724a":"markdown","4e268731":"markdown","b5cfb1c4":"markdown","2c935ff6":"markdown","a66dc43e":"markdown","db2ed4d2":"markdown","4881331e":"markdown"},"source":{"f9d7cb52":"import numpy as np\nimport pandas as pd\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","dcaf6de0":"data = pd.read_csv('\/kaggle\/input\/housesalesprediction\/kc_house_data.csv')","e1b779ab":"data.head()","86c0736f":"data.drop('id',axis=1,inplace=True)","decae0f0":"def get_year(date):\n    return int(str(date)[:4])\ndata['year'] = data['date'].apply(get_year)\ndata.drop('date',axis=1,inplace=True)","ef92ee36":"data.head()","6b30f32e":"from sklearn.model_selection import train_test_split\nX = data.drop('price',axis=1)\ny = data['price']\nX_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.3)","fbd2cd52":"from sklearn.linear_model import LinearRegression\nfrom sklearn.linear_model import Ridge\nfrom sklearn.linear_model import Lasso\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.neural_network import MLPRegressor\n\nlr = LinearRegression()\nlr.fit(X_train, y_train)\n\nrr = Ridge(alpha=0.01)\nrr.fit(X_train, y_train)\n\nrr100 = Ridge(alpha=100) #  comparison with alpha value\nrr100.fit(X_train, y_train)\n\nlasso = Lasso()\nlasso.fit(X_train,y_train)\n\ndtr = DecisionTreeRegressor()\ndtr.fit(X_train,y_train)\n\nmlpr = MLPRegressor()\nmlpr.fit(X_train,y_train)","199cd2a9":"from sklearn.metrics import mean_absolute_error as mae\n#lr, rr, rr100, lasso\nprint(\"LR\")\nprint(mae(y_test,lr.predict(X_test)))\nprint(\"RR\")\nprint(mae(y_test,rr.predict(X_test)))\nprint(\"RR100\")\nprint(mae(y_test,rr100.predict(X_test)))\nprint(\"LASSO\")\nprint(mae(y_test,lasso.predict(X_test)))\nprint(\"Decision Tree Regression\")\nprint(mae(y_test,dtr.predict(X_test)))\nprint(\"Neural Network Regression\")\nprint(mae(y_test,mlpr.predict(X_test)))","46af82c0":"import shap\n\nexplainer = shap.TreeExplainer(dtr)\nshap_values = explainer.shap_values(X_test)","8a135abe":"shap.summary_plot(shap_values, X_test, plot_type=\"bar\")","f56caf85":"import eli5\nfrom eli5.sklearn import PermutationImportance\nperm = PermutationImportance(dtr, random_state=1).fit(X_test, y_test)\neli5.show_weights(perm, feature_names = X_test.columns.tolist())","35dc8d08":"from pdpbox import pdp, info_plots\nimport matplotlib.pyplot as plt\n\nbase_features = data.columns.values.tolist()\nbase_features.remove('price')\n\nfeat_name = 'sqft_living'\npdp_dist = pdp.pdp_isolate(model=dtr, dataset=X_test, model_features=base_features, feature=feat_name)\n\npdp.pdp_plot(pdp_dist, feat_name)\nplt.show()\n\nfeat_name = 'grade'\npdp_dist = pdp.pdp_isolate(model=dtr, dataset=X_test, model_features=base_features, feature=feat_name)\n\npdp.pdp_plot(pdp_dist, feat_name)\nplt.show()","e5207c42":"inter1  =  pdp.pdp_interact(model=dtr, dataset=X_test, model_features=base_features, \n                            features=['sqft_living', 'grade'])\n\npdp.pdp_interact_plot(pdp_interact_out=inter1, feature_names=['sqft_living','grade'], plot_type='contour')\nplt.show()","5f16532f":"inter1  =  pdp.pdp_interact(model=dtr, dataset=X_test, model_features=base_features, \n                            features=['lat', 'long'])\n\npdp.pdp_interact_plot(pdp_interact_out=inter1, feature_names=['lat','long'], plot_type='contour')\nplt.show()","04771c59":"import folium\nfrom folium.plugins import HeatMap\n\n# Create basic Folium crime map\ncrime_map = folium.Map(location=[47.5112,-122.257], \n                       tiles = \"Stamen Terrain\",\n                      zoom_start = 9)\n\n# Add data for heatmp \ndata_heatmap = data[['lat','long','price']]\ndata_heatmap = data.dropna(axis=0, subset=['lat','long','price'])\ndata_heatmap = [[row['lat'],row['long']] for index, row in data_heatmap.iterrows()]\nHeatMap(data_heatmap, radius=10, \n        gradient = {.35: 'blue',.55: 'purple',.68:'lime',.78:'red'}).add_to(crime_map)\n# Plot!\ncrime_map","6e678ffc":"for column in data.columns.drop('price'):\n    feat_name = column\n    pdp_dist = pdp.pdp_isolate(model=dtr, dataset=X_test, model_features=base_features, \n                               feature=feat_name)\n\n    pdp.pdp_plot(pdp_dist, feat_name)\n    plt.show()","226c1ab5":"data.head()","24938c63":"inter1  =  pdp.pdp_interact(model=dtr, dataset=X_test, model_features=base_features, \n                            features=['sqft_living', 'sqft_lot'])\n\npdp.pdp_interact_plot(pdp_interact_out=inter1, feature_names=['sqft_living','sqft_lot'], \n                      plot_type='contour')\nplt.show()","7223fc12":"def heart_disease_risk_factors(model, patient):\n\n    explainer = shap.TreeExplainer(model)\n    shap_values = explainer.shap_values(patient)\n    shap.initjs()\n    return shap.force_plot(explainer.expected_value, shap_values, patient)\n\ndata_for_prediction = X_test.iloc[8,:].astype(float)\nheart_disease_risk_factors(dtr, data_for_prediction)","3d8e9f89":"shap_values = explainer.shap_values(X_train.iloc[:50])\nshap.force_plot(explainer.expected_value, shap_values, X_test.iloc[:50])","88a7bdee":"### PDPs for All Factors","972f0ef1":"### Loading Data","7605ff2a":"### Creating a Model","491e0b8a":"Wow, there seems to be a pretty clean-cut difference in price value by location! Let's graph it out on a map. Where are the most expensive or cheap houses?","c91fe253":"**Bedrooms:** The amount of bedrooms doesn't matter very much, although variability is very high.\n\n**Bathrooms:** The amount of bathrooms increases a little bit per additional bathroom, but only after 3 bathrooms.\n\n**Square Feet of Living Space:** The amount of square feet of living space increases a little bit for the first 2100 square feet, then rises rather significantly from 2100 square feet to 3000 square feet, then rises only a little bit per addition square feet of living.\n\n**Square Feet Lot:** The price increases drastically for the first 20,000 square feet, and then doesn't increase very much after this.\n\n**Floors:** The amount of floors doesn't matter very much (although variability is high).\n\n**Waterfront:** Having a waterfront drastically increases the house price by 500,000.\n\n**View:** As the view score increases, the house price increases, in a linear fashion.\n\n**Condition:** The condition only starts to increase the house price semi-significantly after the condition reaches 3.0.\n\n**Grade:** The grade only starts to increase the house price significantly after a grade of 7.\n\n**Square Feet Above:** The square feet is pretty linear, not increasing the house price by very much.\n\n**Square Feet Basement:** The square feet of the basement does not impact the house price at all, all though variability is relatively high.\n\n**Year Built:** It seems that houses built before 1930 have about the same house price as ones built after (about) 2015, and that there is a dip in between that lowers the house price.\n\n**Zip Code:** Although Zip Code cannot be treated as a linear variable and the number itself as a cause of price difference, in general the higher the zip code number, the less the price is.\n","7e105fdc":"#### Training Models","e8676eaa":"We need to remove the ID:","7db49618":"Final newly formatted head:","331f3926":"### SHAP","b77c6d25":"### PDPs for grade, sqft_living, lat & long","3749bc54":"The price goes up in this order: blue, purple, lime, red.\nWe've got some interesting insights into which places have the most expensive houses. The Seattle area has the strongest red (and therefore the highest prices). This makes sense; houses in Seattle are more expensive than the surrounding area.","466a0d22":"## Which features contribute the most to the final price?","e993f540":"Now we can formulate some business questions!","231526fc":"### Permutation Importance","aa0a8dd6":"Analysis:\n\nsqft_living - the largest jump is somewhere between 1200 and 1500 square feet. However, this isn't very evident though - it's pretty linear.\n\ngrade - the largest jump is from grade 8 to grade 9. This can increase the house price by almost 175,000 dollars.","bb878498":"### Viewing First Version of Data","45470370":"From these two tests, we can confidently say that the most important features are:\n- lat & long (location)\n- grade\n- sqft_living\n\n\nLet's graph these out using a PDP (Partial Dependence Plot) to see how their values affect house price.","21a47e50":"Now that we have a good model, let's use two techniques to find the importance of features - Permutation Importance and SHAP.","d81a065e":"## Loading, Viewing, & Cleaning Data","e61b5208":"Check out my website here: https:\/\/expail.weebly.com","3731724a":"The goal of this house sales analysis is to gain some business insights from the house sales in King County.\nFirst, let's clean the data so we can frame appropiate questions.","4e268731":"And make sure the date is properly formatted into year:","b5cfb1c4":"Decision Tree Regression is the best regression model.","2c935ff6":"# King County House Sales Analysis","a66dc43e":"Although some factors are not very important, in terms of return over investment it performs higher. Let's do a PDP on all the columns.","db2ed4d2":"#### Train Test Split","4881331e":"#### Get Accuracies"}}