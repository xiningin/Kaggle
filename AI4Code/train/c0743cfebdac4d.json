{"cell_type":{"dc92d503":"code","7f8bd584":"code","2a181e8c":"code","739db18a":"code","3350427d":"code","bcf8a3f3":"code","60d57bd3":"code","62b811da":"code","03fba4c4":"code","e6719cb4":"code","ef4c6c55":"code","807945a7":"code","35a393b2":"code","b5f61564":"code","fdf2d2a8":"code","2b0247ff":"code","418328f5":"code","bc8a19e5":"code","8dfb5558":"markdown","1b22765a":"markdown"},"source":{"dc92d503":"%matplotlib inline\nimport os\nos.environ[\"KERAS_BACKEND\"] = \"tensorflow\"\n\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport sys\nfrom sklearn import preprocessing\nfrom sklearn.preprocessing import MinMaxScaler, StandardScaler, RobustScaler\nfrom sklearn.metrics import confusion_matrix,accuracy_score, classification_report\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics.pairwise import euclidean_distances, cosine_similarity\nfrom sklearn.utils import shuffle\nfrom sklearn.utils import class_weight\nfrom sklearn.metrics import r2_score\nfrom sklearn.metrics import mean_absolute_error\nimport tensorflow as tf\nimport tensorflow.keras\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, Dropout, Flatten, Activation, BatchNormalization,SpatialDropout1D,Bidirectional, Embedding, LSTM\nfrom tensorflow.keras import backend as K\nfrom tensorflow.keras.utils import to_categorical\nfrom tensorflow.keras.callbacks import EarlyStopping\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom tensorflow.keras.preprocessing.text import Tokenizer, text_to_word_sequence\nimport re\n\nnp.set_printoptions(threshold=sys.maxsize)\n\npd.set_option('display.max_rows',1000)\npd.set_option('display.max_columns',1000)\n\nsns.set(rc={'figure.figsize':(11.7,8.27)})\n\n# Voor GPU support\nphysical_devices = tf.config.experimental.list_physical_devices('GPU')\nif len(physical_devices) > 0:\n    tf.config.experimental.set_memory_growth(physical_devices[0], True)","7f8bd584":"# Inlezen van de dataset\n\ndataset = pd.read_csv('\/kaggle\/input\/text-embeddings\/Airline_sentiment.csv',encoding = 'ISO-8859-1')\ndataset.tail()","2a181e8c":"sns.countplot(data=dataset, x='airline_sentiment')","739db18a":"sns.countplot(data=dataset, x='airline_sentiment', hue='airline')","3350427d":"print('Max word lenght for tweet:', max(dataset['text'].apply(lambda value: len(value.split(' ')))))\nprint('Min word lenght for tweet:', min(dataset['text'].apply(lambda value: len(value.split(' ')))))","bcf8a3f3":"dataset.notnull().sum()","60d57bd3":"# Split features and targets\nX = dataset['text']\ny = dataset['airline_sentiment']\n\ny = y.replace({'neutral': 0, 'positive': 1, 'negative': 2})\n\ny = to_categorical(y)","62b811da":"# Generate sequences with tokenizer\ntokenizer = Tokenizer(num_words=None)\ntokenizer.fit_on_texts(X)\nsequences = tokenizer.texts_to_sequences(X)","03fba4c4":"# Sequence padding\nX_padded = pad_sequences(sequences, maxlen=36, padding='post')","e6719cb4":"# Train\/test split\nX_train, X_test, y_train, y_test = train_test_split(X_padded, y, test_size=3000, random_state=42)","ef4c6c55":"#### GloVe 300d word embeddings\nembeddings_index = dict()\nwith open('\/kaggle\/input\/text-embeddings\/glove.6B.300d.txt') as f:\n    for line in f:\n        values = line.split()\n        word = values[0]\n        coefs = np.asarray(values[1:], dtype='float32')\n        embeddings_index[word] = coefs\n\nprint('Number of word vector loaded: %s' % len(embeddings_index))","807945a7":"vocabulary_size = X_train.max()\n\nembedding_matrix = np.zeros((vocabulary_size+1, 300))\nfor word, i in tokenizer.word_index.items():\n    embedding_vector = embeddings_index.get(word)\n    if embedding_vector is not None:\n         embedding_matrix[i] = embedding_vector","35a393b2":"# LSTM\n\nmodel = Sequential()\nmodel.add(Embedding(vocabulary_size+1, 300, weights=[embedding_matrix], input_length=40, trainable=False))\nmodel.add(SpatialDropout1D(0.3))\nmodel.add(Bidirectional(LSTM(300, dropout=0.3,return_sequences = True)))\nmodel.add(Bidirectional(LSTM(300, dropout=0.3)))\nmodel.add(Dense(512, activation='relu'))\nmodel.add(Dropout(0.4))\nmodel.add(Dense(512, activation='relu'))\nmodel.add(Dropout(0.3))\nmodel.add(Dense(3, activation='softmax'))\nmodel.compile(loss='categorical_crossentropy', optimizer=tensorflow.keras.optimizers.Adam(lr=0.0004),  metrics=['accuracy'])\n","b5f61564":"from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n\nstopper = EarlyStopping(patience=7)\ncheckpointer = ModelCheckpoint('lstm-sentiment.h5', save_best_only=True, verbose=1)\n\nhist = model.fit(X_train, y_train, epochs=25, verbose=1,batch_size=32, callbacks=[stopper, checkpointer], validation_split=0.2)","fdf2d2a8":"from tensorflow.keras.models import load_model\n\nlstm = load_model('\/kaggle\/working\/lstm-sentiment.h5')\n\ny_pred = lstm.predict_classes(X_test)\ny_true = np.argmax(y_test, axis=1)\n\nprint('Results')\nprint(classification_report(y_true, y_pred))\nprint('\\n')\ncf = confusion_matrix(y_true, y_pred)\nprint(cf)","2b0247ff":"# Voordeel geven aan 'negative' classificaties\n# 1. class_weight instellen zodat de 'negative' klasse meer gaat doorwegen\n# 2. custom loss functie gebruike, die een kleinere loss geeft voor de 'negative' klasse","418328f5":"stopper = EarlyStopping(patience=7)\ncheckpointer = ModelCheckpoint('lstm-sentiment-class-weight.h5', save_best_only=True, verbose=1)\n\nhist = model.fit(X_train, y_train, epochs=25, verbose=1,batch_size=32, class_weight={0: 1, 1: 1, 2: 3}, callbacks=[stopper, checkpointer], validation_split=0.2)","bc8a19e5":"lstm = load_model('\/kaggle\/working\/lstm-sentiment-class-weight.h5')\n\ny_pred = lstm.predict_classes(X_test)\ny_true = np.argmax(y_test, axis=1)\n\nprint('Results')\nprint(classification_report(y_true, y_pred))\nprint('\\n')\ncf = confusion_matrix(y_true, y_pred)\nprint(cf)","8dfb5558":"- Onderzoek de gebalanceerdheid van de dataset. Stel grafisch de verdeling voor van het sentiment.\n- Visualiseer de verdeling van het sentiment per airline. Maak daarvoor per airline een Seaborn countplot van het sentiment.\n- Welke airline lijkt op basis van deze tweet het best te scoren en welke het slechtst?\n- Uit hoeveel woorden bestaat het langste twitter bericht en uit hoeveel woorden het kortste?\n- Onderzoek of er mogelijks foutieve of ontbrekende data aanwezig is. \n- Splits op in een training set en test set. Zorg ervoor dat er 3000 tweets in de test set steken.\n- Train een LSTM\/GRU classifier die uit de tweet het sentiment zo nauwkeurig mogelijk kan voorspellen. Test op de test set.\n- Stel dat de airlines vooral ge\u00efnteresseerd zijn in het correct opsporen van negatieve tweets. Welke aanpassingen zou je kunnen doen om ervoor te zorgen dat het model minder negatieve tweets verkeerd classificeert? Test deze aanpassingen.","1b22765a":"### Airline Twitter sentiment\n\nDe dataset 'Airlines_sentiment.csv' bevat tweets over verschillende airlines. Jouw taak bestaat erin om een classifier te trainen die zo goed mogelijk het sentiment van de tweets kan classificeren als positive, neutral en negative.\nHet sentiment bevindt zich in de kolom 'airline_sentiment' en de tweet zelf in de kolom 'text'."}}