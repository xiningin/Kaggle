{"cell_type":{"48dec1c0":"code","9b5254a7":"code","8269c013":"code","c9117e25":"code","5f8ef135":"code","d08c5af5":"code","c0cc27cc":"code","1f1ee543":"code","39a036e6":"code","65243ce5":"code","4f2f93eb":"code","0d8307ec":"code","379b16b0":"code","77150638":"code","6864fb08":"code","fe9316d0":"code","788ba18d":"code","6dedcabe":"code","a69cdf27":"code","0221f71d":"code","6872fde6":"code","3603c56e":"code","f7a3a8c9":"code","c2d6ad8a":"code","c2bb23b8":"code","8c670a23":"code","ec69488f":"code","8840e1ef":"code","8be367b4":"code","22ed06a3":"code","720eefc4":"code","7ce6906c":"code","dc0bc36a":"code","16a0348a":"code","029d52e5":"markdown","a8035b7c":"markdown","c268d311":"markdown","47c2c437":"markdown","d0c73498":"markdown","293f1e81":"markdown","ec652aea":"markdown","817efefc":"markdown","17eacabf":"markdown","617a0c6d":"markdown","44838d2a":"markdown"},"source":{"48dec1c0":"# Base Modules\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport matplotlib.gridspec as gridspec\nimport seaborn as sns\nfrom bokeh.plotting import output_notebook, figure, show\noutput_notebook()\n\n# Import models\nfrom keras import models, layers, optimizers\nfrom keras.utils import to_categorical\nfrom keras.preprocessing.image import ImageDataGenerator\nfrom keras.callbacks import EarlyStopping\n\n# Other items\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, classification_report","9b5254a7":"train = pd.read_csv('..\/input\/fashionmnist\/fashion-mnist_train.csv')\ntrain.head(3)","8269c013":"validation = pd.read_csv('..\/input\/fashionmnist\/fashion-mnist_test.csv')\nvalidation.head(3)","c9117e25":"# Plot a few items\nclass_names = ['T-shirt\/top', 'Trouser', 'Pullover', 'Dress', 'Coat', 'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']\nplt.figure(figsize=(20,10))\ni=1\n\nfor c in range(10):\n    subset = train[train['label']==c]\n    pictures = np.random.randint(low=0,high=subset.shape[0],size=20)\n    for p in pictures:\n        plt.subplot(10,20,i)\n        picture_array = np.array(subset.iloc[p,1:]).reshape(28,28)\n        plt.imshow(picture_array,cmap=plt.cm.binary)\n        plt.title(class_names[c])\n        plt.axis('off')\n        i+=1\n        \nplt.tight_layout()","5f8ef135":"# Create variables for model\n\n# Prepare training data\nX = np.array(train.drop(columns=['label']))\ny = np.array(train['label'])\n\n# Prepare validation data\nvalidation_x = validation.drop(columns=['label'])\nvalidation_y = validation['label']\n\n# Split training data into train and test\ntrain_x, test_x, train_y, test_y = train_test_split(X, y, test_size=0.2, random_state=0)\n\n# Convert labels to categories\ntrain_y_encoded = to_categorical(train_y)\ntest_y_encoded = to_categorical(test_y)\nval_y_encoded = to_categorical(validation_y)\n\n# Change to numpy arrays\ntrain_x = np.array(train_x)\/255\ntest_x = np.array(test_x)\/255\n\ntrain_y_encoded = np.array(train_y_encoded)\ntest_y_encoded = np.array(test_y_encoded)\n\nvalidation_x = np.array(validation_x)\nval_y_encoded = np.array(val_y_encoded)","d08c5af5":"# Model Definition\ninput_size = 784\noutput_size=10\nm1 = models.Sequential()\nm1.add(layers.Dense(input_size, activation='relu'))\nm1.add(layers.Dense(output_size, activation='softmax'))\n\nm1.compile(optimizer='rmsprop',loss='categorical_crossentropy',metrics=['accuracy'])","c0cc27cc":"m1history = m1.fit(train_x, train_y_encoded, epochs=20, batch_size=128, validation_data=(test_x,test_y_encoded), verbose=0)\nm1.save('m1.h5')","1f1ee543":"p = figure(plot_width=800, plot_height=400, title='Train & Test Accuracy', x_axis_label='Epoch', y_axis_label='Accuracy')\nx = np.arange(1,21,1)\np.line(x,m1history.history['accuracy'], legend_label='Train Accuracy', line_width=3)\np.line(x,m1history.history['val_accuracy'], legend_label='Validation Accuracy', color='green', line_width=3)\np.legend.location = \"top_left\"\nshow(p)","39a036e6":"# Predict results\ny_pred = m1.predict_classes(validation_x)\n\n# Confusion matrix\ncm = pd.DataFrame(confusion_matrix(y_pred, validation_y), index=class_names, columns=class_names)\ncm","65243ce5":"# Classification report\ncr = classification_report(y_pred, validation_y,target_names=class_names, output_dict=True)\ncr_df = pd.DataFrame(cr).transpose()\ncr_df = np.round(cr_df,2)\ncr_df","4f2f93eb":"plt.figure(figsize=(25,5))\nsns.heatmap(cm, center=250, cmap='Blues')","0d8307ec":"print('Test Accuracy:', np.round(cr['accuracy']*100,2),'%')","379b16b0":"# Model Definition\nm2 = models.Sequential()\nm2.add(layers.Dense(input_size, activation='relu'))\nm2.add(layers.Dropout(0.5))\nm2.add(layers.Dense(392, activation='relu'))\nm2.add(layers.Dropout(0.5))\nm2.add(layers.Dense(196, activation='relu'))\nm2.add(layers.Dense(output_size, activation='softmax'))\n\nm2.compile(optimizer='rmsprop',loss='categorical_crossentropy',metrics=['accuracy'])\nm2history = m2.fit(train_x, train_y_encoded, epochs=60, batch_size=128, validation_data=(test_x,test_y_encoded), verbose=0)\nm2.save('m2.h5')","77150638":"p = figure(plot_width=800, plot_height=400, title='Train & Test Accuracy', x_axis_label='Epoch', y_axis_label='Accuracy')\nx = np.arange(1,61,1)\np.line(x,m2history.history['accuracy'], legend_label='Train Accuracy', line_width=3)\np.line(x,m2history.history['val_accuracy'], legend_label='Validation Accuracy', color='green', line_width=3)\np.legend.location = \"bottom_right\"\nshow(p)","6864fb08":"# Predict results\ny_pred = m2.predict_classes(validation_x)\n\n# Confusion matrix\ncm = pd.DataFrame(confusion_matrix(y_pred, validation_y), index=class_names, columns=class_names)\n\n# Classification report\ncr = classification_report(y_pred, validation_y,target_names=class_names, output_dict=True)\ncr_df = pd.DataFrame(cr).transpose()\ncr_df = np.round(cr_df,2)\n\nplt.figure(figsize=(25,5))\nsns.heatmap(cm, center=250, cmap='Blues')","fe9316d0":"print('Test Accuracy:', np.round(cr['accuracy']*100,2),'%')","788ba18d":"# Create model inputs\nimage_shape=(28,28,1)\n\ntr_x = np.array(train_x).reshape(train_x.shape[0],*image_shape)\nte_x = np.array(test_x).reshape(test_x.shape[0],*image_shape)\nva_x = np.array(validation_x).reshape(validation_x.shape[0],*image_shape)","6dedcabe":"m3 = models.Sequential()\n\nm3.add(layers.Conv2D(28, kernel_size=(3,3),activation='linear',padding='same',input_shape=(28,28,1)))\nm3.add(layers.LeakyReLU(alpha=0.1))\nm3.add(layers.MaxPooling2D((2, 2),padding='same'))\nm3.add(layers.Dropout(0.2))\n\nm3.add(layers.Conv2D(56, (3,3), activation='linear',padding='same'))\nm3.add(layers.LeakyReLU(alpha=0.1))\nm3.add(layers.MaxPooling2D(pool_size=(2, 2),padding='same'))\nm3.add(layers.Dropout(0.2))\n\nm3.add(layers.Conv2D(56, (3,3), activation='linear',padding='same'))\nm3.add(layers.LeakyReLU(alpha=0.1))                  \nm3.add(layers.MaxPooling2D(pool_size=(2, 2),padding='same'))\nm3.add(layers.Dropout(0.2))\n\nm3.add(layers.Flatten())\nm3.add(layers.Dense(112, activation='linear'))\nm3.add(layers.LeakyReLU(alpha=0.1))           \nm3.add(layers.Dropout(0.2))\nm3.add(layers.Dense(10, activation='softmax'))","a69cdf27":"m3.compile(optimizer='rmsprop',loss='categorical_crossentropy',metrics=['accuracy'])","0221f71d":"m3history = m3.fit(tr_x, train_y_encoded, epochs=55, batch_size=128, validation_data=(te_x,test_y_encoded), verbose=0)\nm3.save('m3.h5')","6872fde6":"p = figure(plot_width=800, plot_height=400, title='Train & Test Accuracy', x_axis_label='Epoch', y_axis_label='Accuracy')\nx = np.arange(1,56,1)\np.line(x,m3history.history['accuracy'], legend_label='Train Accuracy', line_width=3)\np.line(x,m3history.history['val_accuracy'], legend_label='Validation Accuracy', color='green', line_width=3)\np.legend.location = \"bottom_right\"\nshow(p)","3603c56e":"# Predict results\ny_pred = m3.predict_classes(va_x)\n\n# Confusion matrix\ncm = pd.DataFrame(confusion_matrix(y_pred, validation_y), index=class_names, columns=class_names)\n\n# Classification report\ncr = classification_report(y_pred, validation_y,target_names=class_names, output_dict=True)\ncr_df = pd.DataFrame(cr).transpose()\ncr_df = np.round(cr_df,2)\n\nplt.figure(figsize=(25,5))\nsns.heatmap(cm, center=250, cmap='Blues')","f7a3a8c9":"print('Test Accuracy:', np.round(cr['accuracy']*100,2),'%')","c2d6ad8a":"m4 = models.Sequential()\n\nm4.add(layers.Conv2D(28, kernel_size=(3,3),activation='linear',padding='same',input_shape=(28,28,1)))\nm4.add(layers.LeakyReLU(alpha=0.1))\nm4.add(layers.MaxPooling2D((2, 2),padding='same'))\nm4.add(layers.Dropout(0.3))\n\nm4.add(layers.Conv2D(56, (3,3), activation='linear',padding='same'))\nm4.add(layers.LeakyReLU(alpha=0.1))\nm4.add(layers.MaxPooling2D(pool_size=(2, 2),padding='same'))\nm4.add(layers.Dropout(0.3))\n\nm4.add(layers.Conv2D(112, (3,3), activation='linear',padding='same'))\nm4.add(layers.LeakyReLU(alpha=0.1))                  \nm4.add(layers.MaxPooling2D(pool_size=(2, 2),padding='same'))\nm4.add(layers.Dropout(0.3))\n\nm4.add(layers.Flatten())\nm4.add(layers.Dense(256, activation='linear'))\nm4.add(layers.LeakyReLU(alpha=0.1))           \nm4.add(layers.Dropout(0.3))\nm4.add(layers.Dense(10, activation='softmax'))","c2bb23b8":"datagen = ImageDataGenerator(rotation_range=5, \n                             width_shift_range=0.1,\n                             height_shift_range=0.1, \n                             shear_range=0.1, \n                             zoom_range=0.1, \n                             horizontal_flip=False, \n                             fill_mode='nearest')","8c670a23":"# Visualize augmented data\nclass_names = ['T-shirt\/top', 'Trouser', 'Pullover', 'Dress', 'Coat', 'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']\nplt.figure(figsize=(20,10))\n\ni=1\nfor c in range(10):\n    subset = train[train['label']==c]\n    pic = np.random.randint(low=0,high=subset.shape[0],size=1)\n    for p in range(20):\n        plt.subplot(10,20,i)\n        picture_x = np.array(subset.iloc[pic,1:]).reshape(1,28,28,1)\n        picture_y = np.array(subset.iloc[pic,1])\n        picture_y = to_categorical(picture_y, num_classes=10).reshape(1,10)\n        pic_x, pic_y = datagen.flow(picture_x, picture_y).next()\n        plt.imshow(pic_x.reshape((28,28)),cmap=plt.cm.binary)\n        plt.title(class_names[c])\n        plt.axis('off')\n        i+=1\nplt.tight_layout()","ec69488f":"m4.compile(optimizer='rmsprop',loss='categorical_crossentropy',metrics=['accuracy'])\ncallbacks = EarlyStopping(monitor='val_sparse_categorical_accuracy', patience=150)\nm4history = m4.fit_generator(datagen.flow(tr_x, train_y_encoded), \n                             steps_per_epoch=100, \n                             epochs=100, \n                             validation_data=(te_x,test_y_encoded),\n                             validation_steps=50,\n                             callbacks=callbacks,\n                             verbose=2)\nm4.save('m4.h5')","8840e1ef":"p = figure(plot_width=800, plot_height=400, title='Train & Test Accuracy', x_axis_label='Epoch', y_axis_label='Accuracy')\nx = np.arange(1,101,1)\np.line(x,m4history.history['accuracy'], legend_label='Train Accuracy', line_width=3)\np.line(x,m4history.history['val_accuracy'], legend_label='Validation Accuracy', color='green', line_width=3)\np.legend.location = \"bottom_right\"\nshow(p)","8be367b4":"# Predict results\ny_pred = m4.predict_classes(va_x)\n\n# Confusion matrix\ncm = pd.DataFrame(confusion_matrix(y_pred, validation_y), index=class_names, columns=class_names)\n\n# Classification report\ncr = classification_report(y_pred, validation_y,target_names=class_names, output_dict=True)\ncr_df = pd.DataFrame(cr).transpose()\ncr_df = np.round(cr_df,2)\n\nplt.figure(figsize=(25,5))\nsns.heatmap(cm, center=250, cmap='Blues')","22ed06a3":"print('Test Accuracy:', np.round(cr['accuracy']*100,2),'%')","720eefc4":"# Look through mis-classified items\nvalidation['predicted']=y_pred\nmisclassified_shirts = validation[(validation['label']==6) & (validation['predicted']!=6)]\nmisclassified_shirts_pics = misclassified_shirts.drop(columns=['label', 'predicted']).reset_index(drop=True)","7ce6906c":"# Plot a few items\nplt.figure(figsize=(20,5))\nfor i in range(30):\n    pic_ind = np.random.randint(low=0,high=misclassified_shirts_pics.shape[0],size=1)\n    pic = np.array(misclassified_shirts_pics.iloc[pic_ind]).reshape(28,28)\n    plt.subplot(3,10,i+1)\n    plt.imshow(pic,cmap=plt.cm.binary)\n    plt.axis('off')\n        \nplt.tight_layout()","dc0bc36a":"# Look through correctly classified items\nvalidation['predicted']=y_pred\nclassified_shirts = validation[(validation['label']==6) & (validation['predicted']==6)]\nclassified_shirts_pics = classified_shirts.drop(columns=['label', 'predicted']).reset_index(drop=True)","16a0348a":"# Plot a few items\nplt.figure(figsize=(20,5))\nfor i in range(30):\n    correct_ind = np.random.randint(low=0,high=classified_shirts_pics.shape[0],size=1)\n    correct = np.array(classified_shirts_pics.iloc[correct_ind]).reshape(28,28)\n    plt.subplot(3,10,i+1)\n    plt.imshow(correct,cmap=plt.cm.binary)\n    plt.axis('off')\n        \nplt.tight_layout()","029d52e5":"# Import and prepare data; perform EDA","a8035b7c":"### Model 2: Sequential Model (additional hidden and drop-out layers)","c268d311":"## Model 4: Sequential Model (Conv2d,MaxPool & DropOut layers) with Data Augmentation","47c2c437":"### Model 1: Sequential Model (1 input layer, 1 output layer)","d0c73498":"## Model 3: Sequential Model (Conv2d,MaxPool & DropOut layers)","293f1e81":"# Fashion MNIST Dataset\nThis code will load fashion MNIST dataset, and try a few CNN models to understand how accuracy changes as model parameters are enhanced","ec652aea":"It seems like shirts are being mistaken for coats, pullovers and t-shirts. Same thing for pull-overs and coats; they are being mistaken for each other as well. Most of the other categories are much more accurate.","817efefc":"# Import Modules","17eacabf":"**MODEL RESULTS**  \nModel 1 - Input\/Output Layer - Accuracy:  89.15%  \nModel 2 - More hidden layers - Accuracy:  88.06%  \nModel 3 - CNN with dropout - Accuracy:   82.54%  \nModel 4 - CNN with dropout & augmentation - Accuracy:  82.09%\n  \n**ISSUES**  \nShirts, coats, pull-overs and t-shirts appear to be very similar and result in the highest numbers of mis-categorized items. ","617a0c6d":"# Modeling","44838d2a":"# Conclusion"}}