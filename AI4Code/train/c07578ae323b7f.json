{"cell_type":{"a889cd1d":"code","6bdea537":"code","d9c18527":"code","0324120e":"code","db4f9c06":"code","3b5e4313":"code","4e91f9aa":"code","6dd62ce9":"code","6df7f4ce":"code","225f073c":"code","36b52e28":"code","aaf9e078":"code","7ddf8608":"code","f9a6bdd7":"code","610053b0":"code","9de87400":"code","f874e061":"code","770d6645":"code","1ca89003":"code","9be76641":"code","0f81ae4b":"code","88bd878a":"code","f3d381d7":"code","59c39b2b":"code","dce43c07":"code","5c07451c":"code","74079eba":"code","1169831f":"code","f9e7a0c9":"code","a23e07a2":"code","903f6262":"code","5600a94e":"code","cb5d7ecb":"code","38e504b7":"code","e341b15c":"code","2dfacae1":"code","858c68ff":"code","e38dfd2d":"code","f274b6c4":"code","d608105f":"markdown","ce87e434":"markdown","31de8a12":"markdown","31ef4a11":"markdown"},"source":{"a889cd1d":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd\nimport tensorflow as tf\nimport cv2\nfrom sklearn.metrics import confusion_matrix\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom tensorflow.keras.optimizers import RMSprop\nfrom sklearn.metrics import classification_report\nfrom sklearn.model_selection import train_test_split\n\n\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimg_dir=[]\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n        img_dir.append(os.path.join(dirname, filename))\n        \n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","6bdea537":"len(img_dir)","d9c18527":"labels=[]\n\nfor i in img_dir:\n    if 'tulip' in i:\n        labels.append(\"tulip\")\n    elif 'daisy' in i :\n        labels.append(\"daisy\")\n    elif 'rose' in i:\n        labels.append(\"rose\")\n    elif 'dandelion' in i:\n        labels.append(\"dandelion\")\n    elif 'sunflower' in i:\n        labels.append(\"sunflower\")\n        \n        ","0324120e":"df=pd.DataFrame({\"Labels\":labels})","db4f9c06":"df.head()","3b5e4313":"plt.figure(figsize=(10,5))\nsns.countplot(\"Labels\",data=df) # Tulip and dandelion are more than other categories","4e91f9aa":"from tensorflow.keras.applications.inception_v3 import InceptionV3\n\npre_trained_model = InceptionV3(input_shape = (128, 128, 3), \n                                include_top = False, \n                                weights = 'imagenet')\n\nfor layer in pre_trained_model.layers:\n    layer.trainable = False\n    \n    \npre_trained_model.summary()\n","6dd62ce9":"last_layer = pre_trained_model.get_layer('mixed7')\n\nprint('last layer output shape: ', last_layer.output_shape)\n\nlast_output = last_layer.output","6df7f4ce":"from tensorflow.keras.layers import Dense, BatchNormalization\nfrom tensorflow.keras import layers\nfrom tensorflow.keras import Model\nfrom keras.optimizers import Adam\n\nx = layers.Flatten()(last_output)\nx = layers.Dense(256, activation='relu')(x)\nx=layers.BatchNormalization()(x)\nx = layers.Dropout(0.5)(x)\nx = layers.Dense(512, activation='relu')(x)\nx=layers.BatchNormalization()(x)\nx = layers.Dropout(0.5)(x)\nx = layers.Dense  (5, activation='softmax')(x)\n\nmodel = Model(pre_trained_model.input, x) \n\nmodel.compile(optimizer = RMSprop(lr=0.001),\n              loss = 'categorical_crossentropy', \n              metrics = ['accuracy'])","225f073c":"plt.figure(figsize=(16,16))\n\nfor i in range(25):\n    img = cv2.imread(img_dir[i])\n    plt.subplot(5,5,(i%25)+1)\n    plt.grid(False)\n    plt.xticks([])\n    plt.yticks([])\n    plt.imshow(img)\n    plt.xlabel(\n        \"Class:\"+str(df['Labels'].iloc[i])\n    )\nplt.show()","36b52e28":"plt.figure(figsize=(16,16))\n\nfor i in range(1000,1025):\n    img = cv2.imread(img_dir[i])\n    plt.subplot(5,5,(i%25)+1)\n    plt.grid(False)\n    plt.xticks([])\n    plt.yticks([])\n    plt.imshow(img)\n    plt.xlabel(\n        \"Class:\"+str(df['Labels'].iloc[i])\n    )\nplt.show()","aaf9e078":"plt.figure(figsize=(16,16))\n\nfor i in range(2100,2125):\n    img = cv2.imread(img_dir[i])\n    plt.subplot(5,5,(i%25)+1)\n    plt.grid(False)\n    plt.xticks([])\n    plt.yticks([])\n    plt.imshow(img)\n    plt.xlabel(\n        \"Class:\"+str(df['Labels'].iloc[i])\n    )\nplt.show()","7ddf8608":"plt.figure(figsize=(16,16))\n\nfor i in range(3000,3025):\n    img = cv2.imread(img_dir[i])\n    plt.subplot(5,5,(i%25)+1)\n    plt.grid(False)\n    plt.xticks([])\n    plt.yticks([])\n    plt.imshow(img)\n    plt.xlabel(\n        \"Class:\"+str(df['Labels'].iloc[i])\n    )\nplt.show()","f9a6bdd7":"plt.figure(figsize=(16,16))\n\nfor i in range(4000,4025):\n    img = cv2.imread(img_dir[i])\n    plt.subplot(5,5,(i%25)+1)\n    plt.grid(False)\n    plt.xticks([])\n    plt.yticks([])\n    plt.imshow(img)\n    plt.xlabel(\n        \"Class:\"+str(df['Labels'].iloc[i])\n    )\nplt.show()","610053b0":"img_list=[]\nlabel_list=[]\n\n\n\n\nfor i in os.listdir(\"\/kaggle\/input\/flowers-recognition\/flowers\/flowers\/dandelion\"):\n    \n    if i!=\"flickr.py\" and i!=\"run_me.py\" and i!=\"flickr.pyc\": # those cause a problem so I need to filter them \n    \n        path=\"\/kaggle\/input\/flowers-recognition\/flowers\/flowers\/dandelion\/\"+i\n    \n        img=cv2.imread(path)\n        img = cv2.resize(img,(128,128))\n        img_list.append(img)\n    \n        label=0 # 0 for dandelion\n        label_list.append(label)\n        \n        \nfor i in os.listdir(\"\/kaggle\/input\/flowers-recognition\/flowers\/flowers\/tulip\"):\n    \n    path=\"\/kaggle\/input\/flowers-recognition\/flowers\/flowers\/tulip\/\"+i\n    \n    img=cv2.imread(path)\n    img = cv2.resize(img,(128,128))\n    img_list.append(img)\n        \n    label=1 # 1 for tulip\n    label_list.append(label)\n    \n    \nfor i in os.listdir(\"\/kaggle\/input\/flowers-recognition\/flowers\/flowers\/sunflower\"):\n    \n    path=\"\/kaggle\/input\/flowers-recognition\/flowers\/flowers\/sunflower\/\"+i\n    \n    img=cv2.imread(path)\n    img = cv2.resize(img,(128,128))\n    img_list.append(img)\n        \n    label=2 # 2 for sunflower\n    label_list.append(label)\n        \nfor i in os.listdir(\"\/kaggle\/input\/flowers-recognition\/flowers\/flowers\/rose\"):\n    \n    path=\"\/kaggle\/input\/flowers-recognition\/flowers\/flowers\/rose\/\"+i\n    \n    img=cv2.imread(path)\n    img = cv2.resize(img,(128,128))\n    img_list.append(img)\n        \n    label=3 # 3 for rose \n    label_list.append(label)\n        \nfor i in os.listdir(\"\/kaggle\/input\/flowers-recognition\/flowers\/flowers\/daisy\"):\n    \n    path=\"\/kaggle\/input\/flowers-recognition\/flowers\/flowers\/daisy\/\"+i\n    \n    img=cv2.imread(path)\n    img = cv2.resize(img,(128,128))\n    img_list.append(img)   \n        \n    label=4 #4 for daisy\n    label_list.append(label)\n        \n    \n    \n        \n\n","9de87400":"len(img_list) # good ","f874e061":"len(label_list) # good ","770d6645":"img_list= np.array(img_list) # turn img_list into numpy array","1ca89003":"from keras.utils.np_utils import to_categorical\nlabels = to_categorical(label_list,num_classes = 5)\n","9be76641":"labels[0] # Labels","0f81ae4b":"img_list.shape # my image data is ready","88bd878a":"X_train, X_test, y_train, y_test = train_test_split(img_list,labels, test_size=0.1, random_state=42)","f3d381d7":"print('X_train shape:', X_train.shape)\nprint('X_test shape: ',X_test.shape)\nprint('y_train shape: ',y_train.shape)\nprint('y_test shape: ',y_test.shape)","59c39b2b":"datagen = ImageDataGenerator(\n        rotation_range=20,\n        shear_range=0.2,\n        zoom_range=0.2,\n        width_shift_range=0.2, \n        height_shift_range=0.2,  \n        horizontal_flip=True,\n        vertical_flip=False)\n\ndatagen.fit(X_train)","dce43c07":"batch_size = 64\nepochs = 10","5c07451c":"history = model.fit_generator(datagen.flow(X_train,y_train,batch_size=batch_size),epochs=epochs,validation_data=(X_test,y_test))","74079eba":"%matplotlib inline\nacc = history.history['accuracy']\nval_acc = history.history['val_accuracy']\nloss = history.history['loss']\nval_loss = history.history['val_loss']\n\nepochs = range(len(acc))\n\nplt.plot(epochs, acc, 'r', label='Training accuracy')\nplt.plot(epochs, val_acc, 'b', label='Validation accuracy')\nplt.title('Training and validation accuracy')\nplt.legend()\nplt.figure()\n\nplt.plot(epochs, loss, 'r', label='Training Loss')\nplt.plot(epochs, val_loss, 'b', label='Validation Loss')\nplt.title('Training and validation loss')\nplt.legend()\n\nplt.show()","1169831f":"print(\"Accuracy of the model on Training Data is - \" , model.evaluate(X_train,y_train)[1]*100 , \"%\")\nprint(\"Accuracy of the model on Testing Data is - \" , model.evaluate(X_test,y_test)[1]*100 , \"%\")","f9e7a0c9":"y_pred=model.predict(X_test)","a23e07a2":"y_pred[:5]\n# \u0130t is going to give array that has 5 list in itself.\n#Each list has 5 numbers,This numbers are probabilty of classes.\n#Example, if first number bigger than others in the list,This image could be class 0 so dandelion.","903f6262":"import seaborn as sns\n\n# Convert predictions classes to one hot vectors \nY_pred_classes = np.argmax(y_pred,axis = 1) \n\n# Convert validation observations to one hot vectors\nY_true = np.argmax(y_test,axis = 1) \n\n# compute the confusion matrix\nconfusion_mtx = confusion_matrix(Y_true, Y_pred_classes) \n\n# plot the confusion matrix\nf,ax = plt.subplots(figsize=(8, 8))\nsns.heatmap(confusion_mtx, annot=True, linewidths=0.01,cmap=\"Greens\",linecolor=\"gray\", fmt= '.1f',ax=ax)\nplt.xlabel(\"Predicted Label\")\nplt.ylabel(\"True Label\")\nplt.title(\"Confusion Matrix\")\nplt.show()","5600a94e":"from sklearn.metrics import classification_report\n\nprint(classification_report(Y_true, Y_pred_classes)) \n\n#0: dandelion \n#1: Tulip\n#2: Sunflower\n#3: Rose\n#4: Daisy","cb5d7ecb":"import tensorflow as tf\nfrom tensorflow.keras.layers import Dense, BatchNormalization\n\nmodel=tf.keras.models.Sequential([\n    \ntf.keras.layers.Conv2D(16, (3,3), activation='relu', input_shape=(128,128,3),padding='same'),\ntf.keras.layers.MaxPooling2D(2,2),\nBatchNormalization(),\ntf.keras.layers.Dropout(0.25),\n    \ntf.keras.layers.Conv2D(32, (3,3), activation='relu',padding='same'),\ntf.keras.layers.MaxPooling2D(2,2),\nBatchNormalization(),\ntf.keras.layers.Dropout(0.25),\n    \ntf.keras.layers.Conv2D(64, (3,3), activation='relu',padding=\"same\"),\ntf.keras.layers.MaxPooling2D(2,2),\nBatchNormalization(),\ntf.keras.layers.Dropout(0.25),\n    \ntf.keras.layers.Conv2D(128, (3,3), activation='relu',padding=\"same\"),\ntf.keras.layers.MaxPooling2D(2,2),\nBatchNormalization(),\ntf.keras.layers.Dropout(0.25),\n\n    \ntf.keras.layers.Flatten(),\ntf.keras.layers.Dense(128, activation='relu'),\nBatchNormalization(),\ntf.keras.layers.Dropout(0.5),\n    \ntf.keras.layers.Dense(512, activation='relu'),\nBatchNormalization(),\ntf.keras.layers.Dropout(0.5),\ntf.keras.layers.Dense(5, activation='softmax')])\n\nmodel.summary()\n\n\nfrom tensorflow.keras.optimizers import RMSprop\n\nmodel.compile(optimizer = RMSprop(lr=0.001),\n              loss = 'categorical_crossentropy',\n              metrics=['accuracy'])\n\n\n","38e504b7":"batch_size = 64\nepochs = 30","e341b15c":"history = model.fit_generator(datagen.flow(X_train,y_train,batch_size=batch_size),epochs = epochs, validation_data = (X_test,y_test))","2dfacae1":"%matplotlib inline\nacc = history.history['accuracy']\nval_acc = history.history['val_accuracy']\nloss = history.history['loss']\nval_loss = history.history['val_loss']\n\nepochs = range(len(acc))\n\nplt.plot(epochs, acc, 'r', label='Training accuracy')\nplt.plot(epochs, val_acc, 'b', label='Validation accuracy')\nplt.title('Training and validation accuracy')\nplt.legend()\nplt.figure()\n\nplt.plot(epochs, loss, 'r', label='Training Loss')\nplt.plot(epochs, val_loss, 'b', label='Validation Loss')\nplt.title('Training and validation loss')\nplt.legend()\n\nplt.show()","858c68ff":"print(\"Accuracy of the model on Training Data is - \" , model.evaluate(X_train,y_train)[1]*100 , \"%\")\nprint(\"Accuracy of the model on Testing Data is - \" , model.evaluate(X_test,y_test)[1]*100 , \"%\")","e38dfd2d":"y_pred=model.predict(X_test)","f274b6c4":"import seaborn as sns\n\n# Convert predictions classes to one hot vectors \nY_pred_classes = np.argmax(y_pred,axis = 1) \n\n# Convert validation observations to one hot vectors\nY_true = np.argmax(y_test,axis = 1) \n\n# compute the confusion matrix\nconfusion_mtx = confusion_matrix(Y_true, Y_pred_classes) \n\n# plot the confusion matrix\nf,ax = plt.subplots(figsize=(8, 8))\nsns.heatmap(confusion_mtx, annot=True, linewidths=0.01,cmap=\"Greens\",linecolor=\"gray\", fmt= '.1f',ax=ax)\nplt.xlabel(\"Predicted Label\")\nplt.ylabel(\"True Label\")\nplt.title(\"Confusion Matrix\")\nplt.show()","d608105f":"Please upvote :)","ce87e434":"Now image augmentation","31de8a12":"## Transfer Learning (InceptionV3)","31ef4a11":"Model was created now I am going to preprocessing images"}}