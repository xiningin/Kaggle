{"cell_type":{"ef479ec2":"code","ee256664":"code","4b4c1328":"code","3cc9f820":"code","38325ee6":"code","82698ebe":"code","4760210f":"code","3f72dee3":"code","423ecf1e":"code","cbcf289c":"code","f0d4f8b6":"code","51133e43":"code","e8dd1f0f":"code","115aed05":"code","6ecaf1e3":"code","607cbc21":"code","3712219e":"code","867c0415":"code","c25ccef2":"markdown","a1cb8045":"markdown","2fe9e2a3":"markdown","b26422ed":"markdown","cda52588":"markdown","68c4e14c":"markdown","1dcec7b9":"markdown"},"source":{"ef479ec2":"%config Completer.use_jedi = False","ee256664":"import numpy as np\nimport pandas as pd\nimport seaborn as sns\n\nimport matplotlib.pyplot as plt\nfrom matplotlib.pyplot import imshow, imread\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import KFold\nfrom sklearn.decomposition import PCA\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.cluster import KMeans\nfrom sklearn.mixture import GaussianMixture\n\nimport scipy.stats as stats\n\nimport lightgbm as lgb\nimport warnings","4b4c1328":"R_SEED = 2017","3cc9f820":"submit = False # for some testing","38325ee6":"submission_ex = pd.read_csv('\/kaggle\/input\/tabular-playground-series-sep-2021\/sample_solution.csv')\ntrain_df = pd.read_csv('\/kaggle\/input\/tabular-playground-series-sep-2021\/train.csv')\ntest_df = pd.read_csv('\/kaggle\/input\/tabular-playground-series-sep-2021\/test.csv')","82698ebe":"targets_df = train_df[['claim']].copy()\ntrain_df.drop(['id', 'claim'], axis=1, inplace=True) \ntest_df.drop(['id'], axis=1, inplace=True) ","4760210f":"train_df = train_df.fillna(train_df.median())\ntest_df = test_df.fillna(test_df.median())","3f72dee3":"all_df = pd.concat([train_df, test_df])\n# 1-------------------vvv","423ecf1e":"warnings.filterwarnings(\"ignore\")\n\nfig = plt.figure(figsize = (30,60))\nax = fig.gca()\nhist = all_df.hist(bins = 50, layout = (24,5), color='k', alpha=0.5,  ax = ax)","cbcf289c":"# warnings.filterwarnings(\"ignore\")\ndef plot_fea_hist(fea_name):\n    fig = plt.figure(figsize = (10, 10))\n    ax = fig.gca()\n    hist = all_df[fea_name].hist(bins=150, ax = ax)","f0d4f8b6":"def plot_kmeans(data, labels, no_of_cl, fea_name, ax):\n    ax.hist(data, 100, density = True)\n    for cl in range(no_of_cl):\n        ax.hist(data[labels == cl], 1, density = True, alpha = 0.5)\n    ax.set_title(fea_name)\n#     plt.show()","51133e43":"# guesswork\nfor_kmeans = [('f6', 2), ('f11', 2), ('f13', 2), ('f20', 3), ('f24', 2), ('f27', 3), ('f28', 3), ('f29', 3), ('f31', 3), \n              ('f38', 3), ('f40', 4), ('f42', 5), ('f47', 2), ('f49', 3), ('f54', 2), ('f56', 5), ('f61', 5), ('f65', 2), \n              ('f67', 5), ('f70', 2), ('f75', 4), ('f81', 8), ('f85', 4), ('f88', 4), ('f91', 3),  \n              ('f99', 2), ('f100', 5), ('f109', 4)]\n\nfig, axes = plt.subplots(nrows = 7, ncols = 4, figsize=(30, 60))\ni = 1\nfor f, n_clusters in for_kmeans:\n#     print(str(i) + ' of ' + str(len(for_kmeans)))\n    \n    # KMeans\n    data = all_df[[f]].values\n    km = KMeans(n_clusters = n_clusters, n_init = 50)\n    km.fit(data)\n    k_clus = km.labels_\n    \n    # print(km.cluster_centers_)\n    # print(pd.value_counts(km.labels_))\n\n    ax = axes[(i-1) \/\/ 4, (i-1) % 4]\n    plot_kmeans(data, k_clus, n_clusters, f, ax)\n\n    i += 1\n    \n    # # one_h_clus = np.zeros((k_clus.size, k_clus.max()+1))\n    # # one_h_clus[np.arange(k_clus.size), k_clus] = 1\n    # # for i in range(n_clusters):\n    # #     all_df['clus_' + str(i)] = one_h_clus[:,i]\n\n#     all_df[f + '_clus'] = k_clus\n    _dist = km.transform(data)\n    _dict = {f + '_dist_from_' + str(i): _dist[:,i] for i in range(n_clusters)}\n    for k, v in _dict.items():\n        all_df[k] = v\n#     del all_df[f]\nplt.show()","e8dd1f0f":"def plot_gmm(model, data, fea_name, ax):\n    weights = model.weights_\n    means = model.means_\n    covars = model.covariances_\n\n    n, bins, patches = ax.hist(data, 100, density = True, alpha = 0.2, color = 'k')\n    x = np.arange(np.min(data), np.max(data), (np.max(data) - np.min(data)) \/ 100)\n    for i in range(len(weights)):\n        ax.plot(x, weights[i] * stats.norm.pdf(x,means[i],np.sqrt(covars[i])[0]), alpha = 0.7, linewidth = 3)\n    ax.set_title(fea_name)\n#     plt.show()","115aed05":"# guesswork\nfor_gmm = [('f11', 2), ('f13', 2), ('f23', 2), ('f24', 2), ('f38', 3), ('f47', 2), ('f49', 3),\n           ('f54', 2), ('f55', 2), ('f58', 3), ('f59', 4), ('f60', 3), ('f65', 3), ('f66', 2), ('f94', 3), ('f99', 3), \n           ('f105', 2), ('f106', 5), ('f107', 4)]\n\nfig, axes = plt.subplots(nrows = 5, ncols = 4, figsize=(30, 60))\n\ni = 1\nfor f, n_clusters in for_gmm:\n#     print(str(i) + ' of ' + str(len(for_gmm)))\n    \n    # GMM\n    data = all_df[[f]].values\n    \n    gm = GaussianMixture(n_components = n_clusters, n_init = 50)\n    gm.fit(data)\n    k_clus_1 = gm.predict(data)\n    k_clus_2 = gm.predict_proba(data)\n\n    ax = axes[(i-1) \/\/ 4, (i-1) % 4]\n    plot_gmm(gm, data, f + '_clus_gmm', ax)\n    i += 1\n    \n    all_df[f + '_clus_gmm'] = k_clus_1\n    for j in range(len(k_clus_2[0])):\n        all_df[f + '_clus_gmm_' + str(j)] = k_clus_2[:, j]\n\nplt.show()","6ecaf1e3":"all_df.head()","607cbc21":"all_df.columns.values","3712219e":"# 1-------------------^^^\ntrain_df, test_df = all_df.iloc[:train_df.shape[0],:].copy(), all_df.iloc[-test_df.shape[0]:,:].copy()","867c0415":"train_df.to_csv(\"trainfeat.csv\", index=False)\ntest_df.to_csv(\"testfeat.csv\", index=False)","c25ccef2":"#### step by step\nI know that kmeans is not very good for this, but, its fast enough for just a try.","a1cb8045":"I created some new features as distance from cluster centroids for specified original features.","2fe9e2a3":"#### KMeans","b26422ed":"#### GaussianMixture","cda52588":"Mimicking the [work](https:\/\/www.kaggle.com\/ivankontic\/003-2-kmeans-and-gmm-feature-extraction-tps-aug) of last month winner @ivankontic for Sepetember dataset.","68c4e14c":"### Main idea\nIf we merge train data with test data and perform series of transformation on them, maybe we create additional bond between them. Just an idea, let's see what will happen.","1dcec7b9":"gmm features represent probability that value of original feature belongs to certain distribution"}}