{"cell_type":{"e8afec33":"code","bcecc9a4":"code","2116334b":"code","cfe9f7a1":"code","9a9a95c0":"code","2c9ee84d":"code","cdef5729":"code","f5740e25":"code","1d4a1b7a":"code","7e188ccd":"code","8e3e0e93":"code","5d2b3419":"code","62fef100":"code","a77a865e":"code","3fade4a4":"code","8040450a":"code","b2f0fde4":"code","43786f8f":"code","ce713869":"code","7281e6e1":"code","1d543f1d":"code","1992e685":"code","4d036081":"code","a4e91d10":"code","45ac23a0":"code","cb2f305b":"code","a4eb416a":"code","d53ff47b":"code","0c5bdc37":"code","0314dd15":"code","90fa5141":"code","4c577a8c":"code","07ae4fa3":"code","d3cf9ed9":"code","5b2063b0":"code","b649cf02":"code","70cb0625":"code","677803a9":"code","afcd4460":"code","2ff5fe72":"code","d1fa3142":"code","40c72f7c":"code","d69ee4d8":"code","067188b9":"code","35eea0d1":"code","ba34bc08":"code","b618e19d":"code","b6aa1fe4":"code","9c095a71":"code","898a7b0a":"code","5860c31d":"code","3c0d5485":"code","f4908aef":"code","9d3b96e6":"code","1c608aba":"code","8ffcfca3":"code","b26abdea":"code","7015ee17":"code","1a6c936b":"code","00fff48e":"code","d0b13504":"code","ef40aa67":"code","10bf1622":"code","383f3527":"code","59b5b627":"code","02835d9d":"code","747ad7e6":"code","451d2a2a":"code","985d1eb9":"code","3a19b8e1":"code","7bb921bb":"code","ac204bcb":"code","1f7e5842":"code","f065fe6a":"code","3ef12423":"code","bdd59b26":"code","796c0d9d":"code","5f58f545":"code","88b16445":"code","f7ff55ba":"code","4ac689e8":"code","d8789739":"code","f3ae59b0":"code","e85919db":"code","5d87841e":"code","c0b9183c":"code","b0666da8":"code","c121564b":"code","657b839d":"code","2c682e17":"code","8cede0dc":"code","b764b8e5":"code","d4b8fa51":"code","9ddb9ace":"code","3c14dff0":"code","0180e8a2":"code","a5c43dd1":"code","4e38b7de":"code","efdda377":"code","1db8963d":"code","3e58410f":"code","52cf186d":"code","e43cd643":"code","22bf4837":"code","0e6467ee":"code","eea693f3":"code","0797a131":"code","7c8a4d93":"code","3cd680d2":"code","2ad5ec77":"code","fe72873e":"code","8e40370b":"code","dbdc5f88":"code","829c53b6":"code","eabf7b08":"code","2d2900ae":"code","1a35cc46":"code","1c0c9a71":"code","0a2adfd4":"code","7bbde6d4":"code","35c80985":"code","65ba40af":"code","fedba097":"code","ae31509a":"code","a341ae1b":"code","f9a5f09f":"code","fe934043":"code","f69d4bd6":"code","22b1e2c8":"code","7390864d":"code","7e6bd549":"code","0dfdc7df":"code","49f2a5d8":"code","26c8b377":"code","ca92380d":"code","865d180c":"code","d606c011":"code","9ae84c95":"code","7bc63650":"code","bcda24b1":"code","f6229f84":"code","8d485588":"code","74acf7ec":"code","f1fbc6ac":"code","47222464":"code","3b865e78":"code","21d3d777":"code","1ffc89ea":"code","ac8cef84":"code","97a2a7dd":"code","ed7acfba":"code","46eeb8b3":"code","b87f201e":"code","edb1c5a9":"code","dba9938a":"code","ffe17ea1":"code","9a801e10":"code","046c39a4":"code","b0458f8b":"markdown","a37f5bb9":"markdown","3202c5a8":"markdown","525b6576":"markdown","ed18a473":"markdown","f190516b":"markdown","d216fc16":"markdown","06455d0a":"markdown","5df8529d":"markdown","45968038":"markdown","da9fc9c7":"markdown","2f1279ff":"markdown","a60167eb":"markdown","788c1055":"markdown","8d1974cd":"markdown","60b2effc":"markdown","46c10fb9":"markdown","15a06a01":"markdown","45bbbf9c":"markdown","8745afe0":"markdown","be906aac":"markdown","7810355e":"markdown","4023fab6":"markdown","df5e3d83":"markdown","eb6aba48":"markdown","f538d784":"markdown","e2b2fcf5":"markdown","719c40a4":"markdown","9322ae4a":"markdown","2eabeb92":"markdown","e95be2dc":"markdown","b0929b8b":"markdown","c45d5646":"markdown","a31af428":"markdown","50d94b4e":"markdown","a534d378":"markdown","15feab45":"markdown","41bfd13d":"markdown","774fd373":"markdown","1347f1d8":"markdown","c8aad53d":"markdown","16c5f050":"markdown","ea756d1b":"markdown","852e92be":"markdown","4d813e97":"markdown","c4165895":"markdown","c6c6d210":"markdown","676078dc":"markdown","9abe0f29":"markdown","95ad97a8":"markdown","63fa6aed":"markdown","05130d52":"markdown","a2dac44b":"markdown","faeb4883":"markdown","e49ff309":"markdown","92f93fe2":"markdown","8b7ef23b":"markdown","cf3361e1":"markdown","e7ea2875":"markdown","bef30591":"markdown","5ad5eb4c":"markdown","9ba22100":"markdown","6dcbcfc6":"markdown","9258fcc9":"markdown","58482ba0":"markdown"},"source":{"e8afec33":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.\n\n\nimport matplotlib.pyplot as plt\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# Any results you write to the current directory are saved as output.","bcecc9a4":"admission = pd.read_excel('..\/input\/admission_details.xlsx')","2116334b":"admission.head(2)","cfe9f7a1":"admission.info()","9a9a95c0":"admission.isnull().values.any()","2c9ee84d":"admission.payer_code.value_counts()","cdef5729":"admission.medical_specialty.value_counts()","f5740e25":"admission = admission.drop(['medical_specialty','payer_code'],axis = 1)","1d4a1b7a":"admission.time_in_hospital.value_counts()","7e188ccd":"admission.columns","8e3e0e93":"admission.head(2)","5d2b3419":"admission.shape","62fef100":"diabetic = pd.read_csv('..\/input\/diabetic_data.csv')","a77a865e":"diabetic.columns","3fade4a4":"diabetic.info()","8040450a":"diabetic.readmitted.value_counts()","b2f0fde4":"diabetic.diabetesMed.value_counts()","43786f8f":"diabetic.change.value_counts()","ce713869":"diabetic.isnull().values.any()","7281e6e1":"Diabetic_transform=diabetic.replace(['No','Steady','Up','Down'],[0,1,1,1])\nDiabetic_transform.set_index('encounter_id',inplace=True)\n","1d543f1d":"Diabetic_transform.head()","1992e685":"Diabetic_transform.sum(axis=1).value_counts()","4d036081":"#diabetic.set_index('encounter_id',inplace=True)","a4e91d10":"i1 = Diabetic_transform[Diabetic_transform['insulin']==1].sum(axis = 1).replace([1,2,3,4,5,6],['insulin','io','io','io','io','io'])","45ac23a0":"i1.value_counts()","cb2f305b":"i0=Diabetic_transform[Diabetic_transform['insulin']==0].sum(axis=1).replace([0,1,2,3,4,5,6],['no med','other','other','other','other','other','other'])","a4eb416a":"i0.value_counts()","d53ff47b":"treatments=pd.concat([i1,i0])\ntreatments = pd.DataFrame({'treatments':treatments})","0c5bdc37":"treatments.head()","0314dd15":"diabetic=diabetic.join(treatments,on='encounter_id') #setting index as encounter_id","90fa5141":"diabetic.head(2)","4c577a8c":"diabetic.shape","07ae4fa3":"diabetic.columns","d3cf9ed9":"lab_sessions = pd.read_excel('..\/input\/Lab-session.xlsx')","5b2063b0":"lab_sessions.columns","b649cf02":"lab_sessions.info()","70cb0625":"lab_sessions.columns","677803a9":"lab_sessions.isnull().values.any()","afcd4460":"lab_sessions.shape","2ff5fe72":"patient_details = pd.read_excel('..\/input\/Paitent_details.xlsx')","d1fa3142":"patient_details.columns","40c72f7c":"patient_details.isnull().values.any()","d69ee4d8":"patient_details = patient_details.drop(['weight'],axis = 1)","067188b9":"patient_details.race.value_counts()","35eea0d1":"patient_details['race']=patient_details.race.replace('?',np.nan)\n","ba34bc08":"patient_details['race'].fillna(patient_details['race'].mode()[0], inplace=True)\n","b618e19d":"patient_details.race.isnull().sum()","b6aa1fe4":"patient_details.shape","9c095a71":"print(\"Admission\" , admission.shape)\nprint(\"Diabetic\" ,diabetic.shape)\nprint(\"Lab Sessions\",lab_sessions.shape)\nprint(\"Patient_details\",patient_details.shape)","898a7b0a":"data = pd.concat([patient_details,admission,lab_sessions,diabetic],axis=1)","5860c31d":"data.shape","3c0d5485":"#data = pd.read_csv('Final_Diabetes_withallrows.csv')","f4908aef":"#data[['admission_source_id','time_in_hospital']] = admission[['admission_source_id','time_in_hospital']]","9d3b96e6":"#data[['num_lab_procedures','num_medication','number_outpatient','number_emergency','number_inpatient','num_procedures']] = lab_sessions[['num_lab_procedures','num_medications',\n                                                                                                                        #'number_outpatient','number_emergency','number_inpatient','num_procedures']]","1c608aba":"data.columns","8ffcfca3":"df1=data.iloc[:,:2]\n\ndf1.head()","b26abdea":"df2=data.drop(['encounter_id','patient_nbr'],axis=1)\n\ndf2.head()","7015ee17":"data_final=pd.concat([df1,df2],axis=1)\n\ndata_final.shape\n\n","1a6c936b":"data_final.head()","00fff48e":"\ndata_diamed_yes=data_final[data_final.diabetesMed=='Yes']\ndata_diamed_yes.shape","d0b13504":"data_readmit_no=data_diamed_yes[data_diamed_yes.readmitted=='NO']\ndata_readmit_no.shape","ef40aa67":"data_new=data_readmit_no[~data_readmit_no.discharge_disposition_id.isin([11,13,14,19,20])]\ndata_new.shape","10bf1622":"data_model=data_new[data_new.treatments!='other']","383f3527":"data_model.info()","59b5b627":"data_model.shape","02835d9d":"data_model.head().T","747ad7e6":"#data_cat = data_model.select_dtypes(include=['object']).copy()\ndata_model.treatments.value_counts()","451d2a2a":"data_model = data_model.drop(['metformin',\n       'repaglinide', 'nateglinide', 'chlorpropamide', 'glimepiride',\n       'acetohexamide', 'glipizide', 'glyburide', 'tolbutamide',\n       'pioglitazone', 'rosiglitazone', 'acarbose', 'miglitol', 'troglitazone',\n       'tolazamide', 'examide', 'citoglipton', 'insulin',\n       'glyburide-metformin', 'glipizide-metformin',\n       'glimepiride-pioglitazone', 'metformin-rosiglitazone',\n       'metformin-pioglitazone'],axis = 1)\ndata_model.shape","985d1eb9":"data_model.columns","3a19b8e1":"data_model.num_procedures.plot(kind='hist')\nplt.xlabel(\"No.of Lab Procedures\")","7bb921bb":"import seaborn as sns\nsns.barplot(data_model.discharge_disposition_id)","ac204bcb":"data_model.num_medications.plot(kind='hist')\nplt.xlabel(\"No.of Medications\")","1f7e5842":"data_onehot = pd.get_dummies(data_model, columns=['race', 'gender','max_glu_serum', 'A1Cresult', 'change',\n       'diabetesMed', 'readmitted'])","f065fe6a":"from sklearn.preprocessing import LabelEncoder\nle = LabelEncoder()","3ef12423":"data_onehot['age']=data_onehot.apply(le.fit_transform)","bdd59b26":"data_onehot[['discharge_disposition_id','admission_type_id', 'admission_source_id','num_lab_procedures',\n       'num_procedures','time_in_hospital',\n       'num_medications', 'number_outpatient', 'number_emergency',\n       'number_inpatient']]=  data_model[['discharge_disposition_id','admission_type_id','admission_source_id','num_lab_procedures',\n       'num_procedures','time_in_hospital',\n       'num_medications', 'number_outpatient', 'number_emergency',\n       'number_inpatient']]","796c0d9d":"data_onehot.shape","5f58f545":"data_onehot.info()","88b16445":"data_onehot.head()","f7ff55ba":"#data_onehot = data_onehot.drop(['Unnamed: 0'],axis = 1)","4ac689e8":"data_onehot.shape","d8789739":"data_onehot.columns","f3ae59b0":"import pandas as pd\nimport numpy as np\nimport scipy.stats as stats\nfrom scipy.stats import chi2_contingency\n\nclass ChiSquare:\n    def __init__(self, dataframe):\n        self.df = dataframe\n        self.p = None #P-Value\n        self.chi2 = None #Chi Test Statistic\n        self.dof = None\n        \n        self.dfObserved = None\n        self.dfExpected = None\n        \n    def _print_chisquare_result(self, colX, alpha):\n        result = \"\"\n        if self.p<alpha:\n            result=\"{0} is IMPORTANT for Prediction\".format(colX)\n        else:\n            result=\"{0} is NOT an important predictor. (Discard {0} from model)\".format(colX)\n\n        print(result)\n        \n    def TestIndependence(self,colX,colY, alpha=0.05):\n        X = self.df[colX].astype(str)\n        Y = self.df[colY].astype(str)\n        \n        self.dfObserved = pd.crosstab(Y,X) \n        chi2, p, dof, expected = stats.chi2_contingency(self.dfObserved.values)\n        self.p = p\n        self.chi2 = chi2\n        self.dof = dof \n        \n        self.dfExpected = pd.DataFrame(expected, columns=self.dfObserved.columns, index = self.dfObserved.index)\n        \n        self._print_chisquare_result(colX,alpha)","e85919db":"data_onehot['dummyCat'] = np.random.choice([0, 1], size=(len(data_onehot),), p=[0.5, 0.5])\n\ndata_onehot.dummyCat.value_counts()","5d87841e":"#Initialize ChiSquare Class\ncT = ChiSquare(data_onehot)\n\n#Feature Selection\ntestColumns = ['encounter_id', 'patient_nbr', 'age', 'admission_type_id',\n       'discharge_disposition_id', 'admission_source_id', 'time_in_hospital',\n       'num_lab_procedures', 'num_procedures', 'num_medications',\n       'number_outpatient', 'number_emergency', 'number_inpatient',\n        'race_AfricanAmerican', 'race_Asian', 'race_Caucasian',\n       'race_Hispanic', 'race_Other', 'gender_Female', 'gender_Male',\n       'max_glu_serum_>200', 'max_glu_serum_>300', 'max_glu_serum_None',\n       'max_glu_serum_Norm', 'A1Cresult_>7', 'A1Cresult_>8', 'A1Cresult_None',\n       'A1Cresult_Norm', 'change_Ch', 'change_No', 'diabetesMed_Yes',\n       'readmitted_NO']\nfor var in testColumns:\n    cT.TestIndependence(colX=var,colY=\"treatments\" ) ","c0b9183c":"from scipy.stats import chisquare,chi2_contingency\n\ncat_col = []\nchi_pvalue = []\nchi_name = []\n\ndef chi_sq(i):\n    ct = pd.crosstab(data_onehot['treatments'],data_onehot[i])\n    chi_pvalue.append(chi2_contingency(ct)[1])\n    chi_name.append(i)\n\nfor i in testColumns:\n    chi_sq(i)\n\nchi_data = pd.DataFrame()\nchi_data['Pvalue'] = chi_pvalue\nchi_data.index = chi_name\n\nplt.figure(figsize=(11,8))\nplt.title('P-Values of Chisquare with ''Treatments'' as Target Categorical Attribute',fontsize=16)\nx = chi_data.Pvalue.sort_values().plot(kind='barh')\nx.set_xlabel('P-Values',fontsize=15)\nx.set_ylabel('Independent Categorical Attributes',fontsize=15)\nplt.show()","b0666da8":"# Import `RandomForestClassifier`\nfrom sklearn.ensemble import RandomForestClassifier","c121564b":"# Isolate Data, class labels and column values\nX = data_onehot.drop(['treatments'],axis=1)\nY = data_onehot['treatments']\nY=Y.replace(['insulin','io'],[0,1])\nnames = data_onehot.columns.values","657b839d":"X.head()","2c682e17":"Y.head()","8cede0dc":"Y.shape","b764b8e5":"# Build the model\nrfc = RandomForestClassifier()\n\n# Fit the model\nrfc.fit(X, Y)","d4b8fa51":"#Finding the feature importance using Random Forest\nfeature_imp=pd.DataFrame({'Features':X.columns,'Importance':rfc.feature_importances_})\nfeature_imp.sort_values(by = 'Importance',ascending=True)","9ddb9ace":"data_onehot.columns","3c14dff0":"X = data_onehot.drop(['encounter_id','patient_nbr','age','num_lab_procedures','number_outpatient','number_emergency',\n                      'race_Asian','race_Other','diabetesMed_Yes','max_glu_serum_>200','A1Cresult_>8','A1Cresult_Norm',\n                      'readmitted_NO','dummyCat','treatments'],axis=1)\nX.info()","0180e8a2":"from sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.naive_bayes import MultinomialNB,GaussianNB,BernoulliNB\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import accuracy_score,confusion_matrix,classification_report","a5c43dd1":"X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.3, random_state=2)\nX_train.shape,X_test.shape,y_train.shape,y_test.shape","4e38b7de":"y_p=[]\nfor i in range(y_test.shape[0]):\n    y_p.append(y_test.mode()[0])#Highest class is assigned to a list which is compared with ytest\nlen(y_p) ","efdda377":"y_pred=pd.Series(y_p)","1db8963d":"accuracy_score(y_test,y_pred)","3e58410f":"#Logistic Regression\nm1=LogisticRegression()\nm1.fit(X_train,y_train)\ny_pred_lr=m1.predict(X_test)\nTrain_Score_lr = m1.score(X_train,y_train)\nTest_Score_lr = accuracy_score(y_test,y_pred_lr)","52cf186d":"print('Training Accuracy is:',Train_Score_lr)\nprint('Testing Accuracy is:',Test_Score_lr)\nprint(classification_report(y_test,y_pred_lr))","e43cd643":"from sklearn.metrics import roc_curve, auc\nfpr,tpr, _ = roc_curve(y_test,y_pred_lr)\nroc_auc_lr = auc(fpr, tpr)\n\nprint('Auc for Logistic Regression is:',roc_auc_lr)\nplt.figure()\nplt.plot(fpr, tpr)\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.0])\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('Receiver operating characteristic')\nplt.show()","22bf4837":"m2 = KNeighborsClassifier()\nm2.fit(X_train,y_train)\ny_pred_knn = m2.predict(X_test)\nTrain_Score_knn = m2.score(X_train,y_train)\nTest_Score_knn = accuracy_score(y_test,y_pred_knn)","0e6467ee":"print('Training Accuracy is :',Train_Score_knn)\nprint('Testing Accuracy is:',Test_Score_knn)\nprint(classification_report(y_test,y_pred_knn))","eea693f3":"fpr,tpr, _ = roc_curve(y_test,y_pred_knn)\nroc_auc_knn = auc(fpr, tpr)\n\nprint('Auc for KNN is:',roc_auc_knn)\nplt.figure()\nplt.plot(fpr, tpr)\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.0])\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('Receiver operating characteristic')\nplt.show()","0797a131":"m3=BernoulliNB()\nm3.fit(X_train,y_train)\ny_pred_bnb=m3.predict(X_test)\nTrain_Score_bnb = m3.score(X_train,y_train)\nTest_Score_bnb = accuracy_score(y_test,y_pred_bnb)","7c8a4d93":"print('Training Accuracy :',Train_Score_bnb)\nprint('Testing Accuracy  :',Test_Score_bnb)\nprint(classification_report(y_test,y_pred_bnb))","3cd680d2":"fpr,tpr, _ = roc_curve(y_test,y_pred_bnb)\nroc_auc_bnb = auc(fpr, tpr)\n\nprint('Auc for Bernoulli Naive Bayes is:',roc_auc_bnb)\nplt.figure()\nplt.plot(fpr, tpr)\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.0])\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('Receiver operating characteristic')\nplt.show()","2ad5ec77":"m4 = DecisionTreeClassifier()\nm4.fit(X_train,y_train)\ny_pred_dt=m4.predict(X_test)\nTrain_Score_dt = m4.score(X_train,y_train)\nTest_Score_dt = accuracy_score(y_test,y_pred_dt)","fe72873e":"print('Training Accuracy :',Train_Score_dt)\nprint('Testing Accuracy :',Test_Score_dt)\nprint(classification_report(y_test,y_pred_dt))","8e40370b":"fpr,tpr, _ = roc_curve(y_test,y_pred_dt)\nroc_auc_dt = auc(fpr, tpr)\n\nprint('Auc for Decision Tree is:',roc_auc_dt)\nplt.figure()\nplt.plot(fpr, tpr)\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.0])\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('Receiver operating characteristic')\nplt.show()","dbdc5f88":"m5 = RandomForestClassifier()\nm5.fit(X_train,y_train)\ny_pred_rf=m5.predict(X_test)\nTrain_Score_rf = m5.score(X_train,y_train)\nTest_Score_rf = accuracy_score(y_test,y_pred_rf)","829c53b6":"print('Training Accuracy :',Train_Score_rf)\nprint('Testing Accuracy :',Test_Score_rf)\nprint(classification_report(y_test,y_pred_rf))","eabf7b08":"fpr,tpr, _ = roc_curve(y_test,y_pred_rf)\nroc_auc_rf = auc(fpr, tpr)\n\nprint('Auc for Random Forest is:',roc_auc_rf)\nplt.figure()\nplt.plot(fpr, tpr)\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.0])\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('Receiver operating characteristic')\nplt.show()","2d2900ae":"# GridSearchCV to find optimal max_depth\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import GridSearchCV\n\n\n# specify number of folds for k-fold CV\nn_folds = 3\n\n# parameters to build the model on\nparameters = {'max_depth': range(5, 15, 5),\n    'min_samples_leaf': range(50, 150, 50),\n    'min_samples_split': range(50, 150, 50),\n    'criterion': [\"entropy\", \"gini\"]}\n\n# instantiate the model\ndtree = DecisionTreeClassifier(random_state = 100)\n\n# fit tree on training data\ntree = GridSearchCV(dtree, parameters, \n                    cv=n_folds, \n                   scoring=\"accuracy\")\ntree.fit(X_train, y_train)","1a35cc46":"tree.best_params_","1c0c9a71":"m6 = DecisionTreeClassifier(criterion='entropy',max_depth=5,min_samples_leaf=100,min_samples_split=50)\nm6.fit(X_train,y_train)\ny_pred_tdt=m6.predict(X_test)\nTrain_Score_tdt = m6.score(X_train,y_train)\nTest_Score_tdt = accuracy_score(y_test,y_pred_tdt)","0a2adfd4":"print('Training Accuracy :',Train_Score_tdt)\nprint('Testing Accuracy  :',Test_Score_tdt)\nprint(classification_report(y_test,y_pred_tdt))\n","7bbde6d4":"fpr,tpr, _ = roc_curve(y_test,y_pred_tdt)\nroc_auc_tdt = auc(fpr, tpr)\n\nprint('Auc for Tuned Decision Tree is:',roc_auc_tdt)\nplt.figure()\nplt.plot(fpr, tpr)\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.0])\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('Receiver operating characteristic')\nplt.show()","35c80985":"#Gridsearch CV to find Optimal K value for KNN model\ngrid = {'n_neighbors':np.arange(1,50)}\nknn=KNeighborsClassifier()\nknn_cv=GridSearchCV(knn,grid,cv=3)\nknn_cv.fit(X_train,y_train)\n\n\nprint(\"Tuned Hyperparameter k: {}\".format(knn_cv.best_params_))","65ba40af":"m7 = KNeighborsClassifier(n_neighbors=45)\nm7.fit(X_train,y_train)\ny_pred_tknn=m7.predict(X_test)\nTrain_Score_tknn = m7.score(X_train,y_train)\nTest_Score_tknn = accuracy_score(y_test,y_pred_tknn)","fedba097":"print('Training Accuracy :',Train_Score_tknn)\nprint('Testing Accuracy  :',Test_Score_tknn)\nprint(classification_report(y_test,y_pred_tknn))","ae31509a":"fpr,tpr, _ = roc_curve(y_test,y_pred_tknn)\nroc_auc_tknn = auc(fpr, tpr)\n\nprint('Auc for Tuned KNN is:',roc_auc_tknn)\nplt.figure()\nplt.plot(fpr, tpr)\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.0])\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('Receiver operating characteristic')\nplt.show()","a341ae1b":"parameter={'n_estimators':np.arange(1,101)}\ngs = GridSearchCV(m5,parameter,cv=3)\ngs.fit(X_train,y_train)\ngs.best_params_","f9a5f09f":"m8 = RandomForestClassifier(n_estimators=71)\nm8.fit(X_train,y_train) \ny_pred_trf=m8.predict(X_test)\nTrain_Score_trf = m8.score(X_train,y_train)\nTest_Score_trf = accuracy_score(y_test,y_pred_trf)","fe934043":"print('Training Accuracy :',Train_Score_trf)\nprint('Testing Accuracy  :',Test_Score_trf)\nprint(classification_report(y_test,y_pred_trf))","f69d4bd6":"fpr,tpr, _ = roc_curve(y_test,y_pred_trf)\nroc_auc_trf = auc(fpr, tpr)\n\nprint('Auc for Tuned Random Forest is:',roc_auc_trf)\nplt.figure()\nplt.plot(fpr, tpr)\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.0])\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('Receiver operating characteristic')\nplt.show()","22b1e2c8":"data_model.treatments.replace(['insulin','io'],[0,1],inplace = True)","7390864d":"data_model.head().T","7e6bd549":"a = data_model.drop(['age','diabetesMed','readmitted','treatments'],axis=1)\nb = data_model.treatments","0dfdc7df":"a.dtypes","49f2a5d8":"cate_features_index = np.where(a.dtypes != int)[0]\n","26c8b377":"xtrain,xtest,ytrain,ytest = train_test_split(a,b,train_size=.70,random_state=2)\n","ca92380d":"from catboost import CatBoostClassifier, Pool,cv\n#let us make the catboost model, use_best_model params will make the model prevent overfitting\nmodel = CatBoostClassifier(eval_metric='Accuracy',use_best_model=True,random_seed=42)","865d180c":"model.fit(xtrain,ytrain,cat_features=cate_features_index,eval_set=(xtest,ytest))","d606c011":"#show the model test acc, but you have to note that the acc is not the cv acc,\n#so recommend to use the cv acc to evaluate your model!\nprint('the test accuracy is :{:.6f}'.format(accuracy_score(ytest,model.predict(xtest))))\ntest_score_catboost = accuracy_score(ytest,model.predict(xtest))\nprint(\"the train accuracy is :\",model.score(xtrain,ytrain))\ntrain_score_catboost = model.score(xtrain,ytrain)","9ae84c95":"fpr,tpr, _ = roc_curve(ytest,model.predict(xtest))\nroc_auc_cb = auc(fpr, tpr)\n\nprint('Auc for Cat Boost is:',roc_auc_cb)\nplt.figure()\nplt.plot(fpr, tpr)\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.0])\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('Receiver operating characteristic')\nplt.show()","7bc63650":"model.predict(xtest)","bcda24b1":"Model_Scores=pd.DataFrame({'Models':['Logistic Regression','KNN','Bernauli Naives Bayes','Decision Tree','Random Forest','Tuned Decison Tree','Tuned KNN','Tuned Random Forest','Cat Boost'],\n             'Training Accuracy':[Train_Score_lr,Train_Score_knn,Train_Score_bnb,Train_Score_dt,Train_Score_rf,Train_Score_tdt,Train_Score_tknn,Train_Score_trf,train_score_catboost],\n             'Testing Accuracy':[Test_Score_lr,Test_Score_knn,Test_Score_bnb,Test_Score_dt,Test_Score_rf,Test_Score_tdt,Test_Score_tknn,Test_Score_trf,test_score_catboost],\n                'AUC':[roc_auc_lr,roc_auc_knn,roc_auc_bnb,roc_auc_dt,roc_auc_rf,roc_auc_tdt,roc_auc_tknn,roc_auc_trf,roc_auc_cb]})\n\nModel_Scores.sort_values(by=('Testing Accuracy'),ascending=False)","f6229f84":"from sklearn.ensemble import AdaBoostClassifier,BaggingClassifier\nbslr=AdaBoostClassifier(base_estimator=LogisticRegression())\nbslr.fit(X_train,y_train)\n","8d485588":"y_pred_blr=bslr.predict(X_test)\nTrain_Score_bslr = bslr.score(X_train,y_train)\nTest_Score_bslr = accuracy_score(y_test,y_pred_blr)","74acf7ec":"print('Training Accuracy :',Train_Score_bslr)\nprint('Testing Accuracy  :',Test_Score_bslr)\nprint(classification_report(y_test,y_pred_blr))","f1fbc6ac":"fpr,tpr, _ = roc_curve(y_test,y_pred_blr)\nroc_auc_bslr = auc(fpr, tpr)\n\nprint('Auc for Boosted Logistic Regression is:',roc_auc_bslr)\nplt.figure()\nplt.plot(fpr, tpr)\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.0])\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('Receiver operating characteristic')\nplt.show()","47222464":"bglr=BaggingClassifier(base_estimator=LogisticRegression())\nbglr.fit(X_train,y_train)","3b865e78":"y_pred_bglr=bglr.predict(X_test)\nTrain_Score_bglr = bglr.score(X_train,y_train)\nTest_Score_bglr = accuracy_score(y_test,y_pred_blr)","21d3d777":"print('Training Accuracy :',Train_Score_bglr)\nprint('Testing Accuracy  :',Test_Score_bglr)\nprint(classification_report(y_test,y_pred_bglr))","1ffc89ea":"fpr,tpr, _ = roc_curve(y_test,y_pred_bglr)\nroc_auc_bglr = auc(fpr, tpr)\n\nprint('Auc for Bagged Logistic Regression is:',roc_auc_bglr)\nplt.figure()\nplt.plot(fpr, tpr)\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.0])\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('Receiver operating characteristic')\nplt.show()","ac8cef84":"Model_Scores=pd.DataFrame({'Models':['Logistic Regression','Boosted Logistic Regression','Cat Boost'],\n             'Training Accuracy':[Train_Score_lr,Train_Score_bslr,train_score_catboost],\n             'Testing Accuracy':[Test_Score_lr,Test_Score_bslr,test_score_catboost],\n                    'AUC':[roc_auc_lr,roc_auc_bslr,roc_auc_cb]})\n\n","97a2a7dd":"Model_Scores.sort_values(by='Testing Accuracy',ascending=False)","ed7acfba":"from sklearn.ensemble import VotingClassifier\nfrom sklearn.model_selection import cross_val_score\nstacked = VotingClassifier(estimators=[('Logistic Regression',m1),('KNN',m2),('Naive Bayes',m3),('Decision Tree',m4),\n                                      ('RandomForest',m5),('Tuned Decision Tree',m6),('Tuned KNN',m7),\n                                       ('Tuned Random Forest',m8),('Boosted Logistic Regression',bslr)],voting='hard')\n\n","46eeb8b3":"for model, name in zip([m1,m2,m3,m4,m5,m6,m7,m8,bslr,stacked],['Logistic Regression','KNN','Naive Bayes','Decision Tree','RandomForest',\n                                                               'Tuned Decision Tree','Tuned KNN','Tuned Random Forest',\n                                                               'Boosted Logistic Regression','stacked']):\n    scores=cross_val_score(model,X,Y,cv=5,scoring='accuracy')\n    print('Accuarcy: %0.02f (+\/- %0.4f)(%s)'%(scores.mean(),scores.var(),name))","b87f201e":"from sklearn.ensemble import GradientBoostingClassifier\ngbdt=GradientBoostingClassifier(n_estimators=150,random_state=2)\ngbdt.fit(X_train,y_train)","edb1c5a9":"y_pred_gbdt=gbdt.predict(X_test)\nTrain_Score_gbdt = gbdt.score(X_train,y_train)\nTest_Score_gbdt = accuracy_score(y_test,y_pred_gbdt)","dba9938a":"print('Training Accuracy :',Train_Score_gbdt)\nprint('Testing Accuracy  :',Test_Score_gbdt)\nprint(classification_report(y_test,y_pred_gbdt))","ffe17ea1":"models=[]\nmodels.append(('Logistic_Regression',m1))\nmodels.append(('KNN',m2))\nmodels.append(('Bernoulli_NB',m3))\nmodels.append(('Decison Tree',m4))\nmodels.append(('Random Forest',m5))\nmodels.append(('Tuned Decision Tree',m6))\nmodels.append(('Tuned KNN',m7))\nmodels.append(('Tuned Random Forest',m8))\nmodels.append(('Bagged Logistic Regression',bglr))\nmodels.append(('Boosted Logistic regression',bslr))","9a801e10":"from sklearn.model_selection import KFold\nfrom sklearn.model_selection import cross_val_score\nresults=[]\nnames=[]\nscoring='accuracy'\nfor name,model in models:\n    kfold=KFold(n_splits=5,random_state=2)\n    cv_results=cross_val_score(model,X,Y,cv=kfold,scoring=scoring)\n    results.append(cv_results)\n    names.append(name)\n    msg=\"%s: %f (%f)\"%(name,np.mean(cv_results),cv_results.var())\n    print(msg)\n#boxplot alogorithm comparision\nfig=plt.figure(figsize=(16,8))\nfig.suptitle('Algorithm Comparision')\nax=fig.add_subplot(111)\nplt.boxplot(results)\nax.set_xticklabels(names)\nplt.show()","046c39a4":"from sklearn.metrics import log_loss\n\nlog_loss(y_test, y_pred_lr, eps=1e-15)","b0458f8b":"### Hyper Tuning the Random Forest Model","a37f5bb9":"#### The training accuracy and testing accuracy of model is almost similar which helps us to understand that model is neither overfitting nor underfitting","3202c5a8":"**1. When the value of Insuin is '1' , creating the classes \"insulin\" & \"io\" (insulin + others )********","525b6576":"# Gradient Boosting Algorithm to check the accuracy","ed18a473":"**Filtering the records of patients having Diabetes**","f190516b":"## Given were the 5 datasets,we would like to work on the EDA and Feature Engineering of data separately and then merge them together for bringing out the insights from the data and for predictive analysis","d216fc16":"# CV method to check the bias and variance error","06455d0a":"**One hot encoding the nominal categorical values**","5df8529d":"# Let us tune the hyper parameters of Non-parametric models ( Decision Tree and KNN ) using GridSearch","45968038":"# After applying boosted technique on Logistic Regression still the best accuracy is given by the Benchmark Model","da9fc9c7":"### Feature Engineering - Creating a new feature \"Treatments\"","2f1279ff":"**2. When the value of Insuin is '0' , creating the classes \"others\" & \"no med\"**","a60167eb":"# Applying Boosting Technique on Benchmark Model (Logistic Regression)","788c1055":"#### The training accuracy and testing accuracy of model is almost similar which helps us to understand that model is neither overfitting nor underfitting","8d1974cd":"#### The training accuracy when compared to testing accuracy of model is more which helps us to understand that model is overfitting","60b2effc":"# Dropping all Duplicate columns from DataFrame","46c10fb9":"#### The training accuracy and testing accuracy of model is almost similar which helps us to understand that model is neither overfitting nor underfitting","15a06a01":"## KNN Classifier\n","45bbbf9c":"# Random Forest","8745afe0":"##### 2. Filling the NaN's with the mode","be906aac":"# Concatenation of df1 and df2 Dataframes","7810355e":"### Importing the Patient Details Dataset","4023fab6":"## Baseline model","df5e3d83":"### Checking for null values","eb6aba48":"# Logistic Regression","f538d784":" #### Custom encoding for the 23 Drug Features\n","e2b2fcf5":"### Loading the Diabetes data of the patients","719c40a4":"**We can observe that the \"Race\" Feature has some missing values**","9322ae4a":"**Considering records of Diabetic Patients who didn't Readmit**","2eabeb92":"**Excluding the patients who are Dead and are in Hospice**","e95be2dc":"### We can observe that, Payer code and medical speciality have more than 50% of the missing data, and prefer to drop those features.","b0929b8b":"##### 1. Replacing the ? with NaN's","c45d5646":"# Building a decision tree model with tuned parameters","a31af428":"#### Checking for any NULL values","50d94b4e":"# Comparing Benchmark model with Boosted Logistic Regression","a534d378":"#### The training accuracy when compared to testing accuracy of model is more which helps us to understand that model is overfitting","15feab45":"# CatBoost","41bfd13d":"#### The training accuracy when compared to testing accuracy of model is more which helps us to understand that model is overfitting","774fd373":"**Importing the dataset Admission Details** ","1347f1d8":"****Choosing the records with treatments Insulin and Insulin + other ( w.r.t Problem Statement)**","c8aad53d":"### Importing Lab Sessions Dataset","16c5f050":"**Since Treatments column is the combination of the 23 drug features,we will be dropping them**","ea756d1b":"**As per occum's razor rule and also by considering bias-variance trade off, base logistic regression stands out to be the best model with 76 percent accuracy**\n","852e92be":"# Concatinatin the Dataframes \"Admission\" , \"Diabetic\" ,\"Patient_details\" & \"Lab_sessions\"","4d813e97":"# Performing Stacking technique on the models","c4165895":"#### Baseline Accuracy is around 54.4%","c6c6d210":"**Label Encoding the AGE(ordinal) categorical column**","676078dc":"**Adding the new feature to the Diabetic Dataframe**","9abe0f29":"#### Checking for NULL values","95ad97a8":"# Here the features which contains numeric values are of type Discrete Quantitative and has a finite set of values. Discrete data can be both Quantitative and Qualitative. So treating outliers in this dataset is not possible","63fa6aed":"**Feature selection**","05130d52":"## Bernoulli Naives Bayes","a2dac44b":"#### The training accuracy when compared to testing accuracy of model is more which helps us to understand that model is overfitting","faeb4883":"# Chi-Square Test of Independence","e49ff309":"# Patients are Given at max a combination of 6 drugs for treating diabetes","92f93fe2":"# Storing Encounter_id and Patient_nbr columns in a seperate DataFrame","8b7ef23b":"**Missing value Imputation using MODE for Race Feature as most of the people in the Dataset are Caucasian**","cf3361e1":"**Since weight has more than 50% missing values, we tend to drop that feature**","e7ea2875":"# Logistic Regression gives us the Best Test Accuracy so we choose Logistic Regression as our BenchMark model","bef30591":"#### The training accuracy when compared to testing accuracy of model is more which helps us to understand that model is overfitting","5ad5eb4c":"# Conclusion","9ba22100":"## Out of Two techniques for Feature Selection, Chi-Square Test of Independence seems to be more efficient when compared to Random Forest. So dropping the Features which are labelled as not important predictor by Chi-Square test\n","6dcbcfc6":"### Below Built every model is a base model so there is furthur possibilty that a model can perform better by tuning it.","9258fcc9":"# Decision Tree","58482ba0":"**Feature Selection using Random Forest Algorithm**"}}