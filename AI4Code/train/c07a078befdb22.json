{"cell_type":{"be1826d2":"code","dc518ef4":"code","d8a1f1f0":"code","d08a20a5":"code","a743a8c5":"code","9aa4aebe":"code","67ca7346":"code","942ab008":"code","0dc05d0a":"code","675c4ae7":"code","abb70b36":"code","097c8ccd":"code","bfb9f5e0":"code","11226008":"code","6882252f":"code","dfb6d816":"code","f999655b":"code","7a8fd8c3":"code","4b6ab322":"code","00922a09":"code","dea45d74":"code","830d2154":"markdown","90daabc9":"markdown","db672352":"markdown","7bbc9e80":"markdown","b4b26ae9":"markdown","3fed4960":"markdown"},"source":{"be1826d2":"from sklearn.preprocessing import LabelEncoder\nfrom category_encoders import HashingEncoder, PolynomialEncoder\nfrom category_encoders import TargetEncoder, LeaveOneOutEncoder, MEstimateEncoder, WOEEncoder, JamesSteinEncoder, CatBoostEncoder, GLMMEncoder\nfrom keras import utils\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport warnings, gc, time\nwarnings.simplefilter('ignore') # once | error | always | default | module\n\nfrom tqdm import tqdm_notebook\n\n# We shall be compiling a summary table as we go along.\nsummary = pd.DataFrame({'inp2out_map': pd.Series(dtype=object),   # input-to-output map\n                        'nunique'    : pd.Series(dtype=int),      # number of unique (or distinct) values in output\n                        'unique'     : pd.Series(dtype='object'), # unique values in output\n                        'shape'      : pd.Series(dtype=int),      # rows-by-columns of output array\n                        'tictoc'     : pd.Series(dtype=int)})     # computation time i seconds\nsummary.index.name = 'encoder'\n# The grand summary is printed at the end of this notebook.","dc518ef4":"train = pd.read_csv('\/kaggle\/input\/tabular-playground-series-mar-2021\/train.csv', index_col='id')\ntrain.sample(5)","d8a1f1f0":"# Let's zoom into a single column.\ntrain['cat10'].nunique(), train['cat10'].unique()\n# cat10 alone has 299 unique values altogether. This value in termed 'cardinality'.\n# This is an extreme case. Cardinalities are usually lower e.g. exam grades = A, B, C, D, E would have cardinality=5.","d08a20a5":"# Now pick another column; just to have a look.\ntrain['cat5'].nunique(), train['cat5'].unique()\n# Lower cardinality in this column; 84 only.","a743a8c5":"%%time\nfor n_components in [8, 16, 32]:\n    inp = train['cat10']\n    tic = time.time()\n    out = HashingEncoder(n_components=n_components).fit_transform(inp)\n    tictoc = time.time() - tic\n\n    inp2out_map = pd.concat([pd.DataFrame({'inp': train['cat10']}, columns=['inp']),\n                             pd.DataFrame(out, index=train.index)], axis=1).drop_duplicates()\n    inp2out_map.set_index('inp', inplace=True, drop=True)\n    unik = np.unique(inp2out_map.values)\n    summary.loc[f'HashingEncoder, {n_components}'] = inp2out_map, len(unik), unik, inp2out_map.shape, tictoc\ncolumns_show = ['nunique', 'unique', 'shape', 'tictoc']\nsummary[columns_show]\n# We find that no matter what n_components we asked for, the mapped values always consist of 0 and 1, and nothing else.\n# When we ask for n_components=8, we get 8 columns in the output. When we ask for n_components=16, we get 16 columns. When we ask for n_components=32, we get 32 columns.","9aa4aebe":"summary.loc['HashingEncoder, 8', 'inp2out_map']\n# Some rows in inp2out_map contain null values. \n# This shows that some categories in the original input (train['cat10']) are not mapped to anything. \n# HashingEncoder therefore doesn't map one-to-one i.e. some of the original info is lost in the encoding process.","67ca7346":"# Now we filter out the null rows and show only non-null rows.\nnon_null_idx = ~summary.loc['HashingEncoder, 8', 'inp2out_map'].isnull().any(axis=1)\nnon_null_rows = summary.loc['HashingEncoder, 8', 'inp2out_map'].loc[non_null_idx]\nnon_null_rows","942ab008":"# Next, we see if there are any duplicate rows that can be removed.\nnon_null_rows.drop_duplicates()\n# We are left with just 8 rows! That means many input categories got mapped to the same output value. This loss of info is called *collision*.","0dc05d0a":"# Let's repeat what we did in the previous two cells for ```n_components``` = 8, 16, 32:\nprint('{:15s}{}'.format('n_components', 'unique values of output'))\nfor n_components in [8, 16, 32]:\n    non_null_idx = ~summary.loc[f'HashingEncoder, {n_components}', 'inp2out_map'].isnull().any(axis=1)\n    non_null_rows = summary.loc[f'HashingEncoder, {n_components}', 'inp2out_map'].loc[non_null_idx]\n    print('{:<15d}{}'.format(n_components, len(non_null_rows.drop_duplicates())))\n# The lower number of unique values of output, the higher the collisions i.e. we suffer a greater info loss.","675c4ae7":"inp = train['cat5']\ntic = time.time()\nout = PolynomialEncoder().fit_transform(inp)\ntictoc = time.time() - tic\n\ninp2out_map = pd.concat([pd.DataFrame({'inp': train['cat5']}, columns=['inp']),\n                         pd.DataFrame(out, index=train.index)], axis=1).drop_duplicates()\ninp2out_map.set_index('inp', inplace=True, drop=True)\nunik = np.unique(inp2out_map.values)\nsummary.loc['PolynomialEncoder'] = inp2out_map, len(unik), unik, inp2out_map.shape, tictoc\nsummary.loc['PolynomialEncoder', 'inp2out_map']\n# As shown in [Part 1](https:\/\/www.kaggle.com\/marychin\/category-encoders-catalog-experiments-part-1) of this notebook series, contrast encoders output an ```intercept``` column.","abb70b36":"# Now we filter out the null rows and show only non-null rows.\nnon_null_idx = ~summary.loc['PolynomialEncoder', 'inp2out_map'].isnull().any(axis=1)\nnon_null_rows = summary.loc['PolynomialEncoder', 'inp2out_map'].loc[non_null_idx]\nnon_null_rows","097c8ccd":"# Next, we see if there are any duplicate rows that can be removed.\nnon_null_rows.drop_duplicates()\n# We get 84 rows still. No collision in this case (unlike HashingEncoder).","bfb9f5e0":"feature = 'cat5'\nfor which in [TargetEncoder, LeaveOneOutEncoder, MEstimateEncoder, WOEEncoder, JamesSteinEncoder, GLMMEncoder, CatBoostEncoder]:\n#   Grab the label, apply some minor hiding cosmetics:\n    label = str(which).split('.')[-1].split(\"'\")[0]\n\n    tic = time.time()\n    out = which().fit_transform(train[feature], train['target'])\n    tictoc = time.time() - tic\n    inp2out_map = pd.concat([pd.DataFrame({'inp': train[feature]}, columns=['inp']),\n                             pd.DataFrame(out, index=train.index)], axis=1).drop_duplicates()\n    inp2out_map.set_index('inp', inplace=True, drop=True)\n    unik = np.unique(inp2out_map.values)\n    summary.loc[label] = inp2out_map, len(unik), unik, inp2out_map.shape, tictoc\n\n#   Test if encoding depends on the order of rows.\n    shuffled = train[[feature, 'target']].copy()\n    shuffled = shuffled.sample(frac=1)\n    out_shuffled = which().fit_transform(shuffled[feature], shuffled['target'])\n    out.rename(columns={feature: 'tis'}, inplace=True)\n    out_shuffled.rename(columns={feature: 'tat'}, inplace=True)\n    tistat = pd.concat([out, out_shuffled], names=['tis', 'tat'], axis=1)\n    if not np.allclose(tistat['tis'], tistat['tat']):\n        print(label, 'is order-dependent.')\ncolumns_show = ['nunique', 'unique', 'shape', 'tictoc']\nsummary[columns_show]\n# Output reports that GLMMEncoder and CatBoostEncoder depend on the order of rows.","11226008":"ncoda = LabelEncoder()\nx = ncoda.fit_transform(train['cat5'])\ny = train['target']\nz = TargetEncoder().fit_transform(train['cat5'], train['target'])\n\nfig = plt.figure(figsize=(15, 15))\nax = fig.add_subplot(projection='3d')\nax.scatter3D(x, y, z, c=z, cmap='hot')\nax.set_xlabel('cat 5'); ax.set_ylabel('target'); ax.set_zlabel(label)\n_ = ax.set_xticks(np.arange(0, len(ncoda.classes_), 5))\n_ = ax.set_xticklabels(ncoda.classes_[::5])\n# 3D plot shows how encoders output depends on both cat5 and target.","6882252f":"# How does TargetEncoder encode? It takes the mean of the target of the given category.\n# Let's have a goal doing this manually, then compare with TargetEncoder's output.\nmanual_auto = pd.DataFrame( {'manual': train.groupby('cat5')['target'].mean()} )\nmanual_auto = pd.concat([manual_auto, summary.loc['TargetEncoder', 'inp2out_map']], axis=1)\nnp.allclose(manual_auto['manual'], manual_auto['cat5'], atol=1e-7)\n# So it is confirmed that our manual back-of-envelop calculation agrees with the output by TargetEncoder.","dfb6d816":"# train contains too many rows. To avoid prohibitive runtimes let's reduce train to a manageable subset.\nreduced = train.sample(10000, random_state=77).reset_index(drop=True)\n\n# Now let us try encoding leave-one-out manually.\nmanual = pd.Series()\nfor grp_idx, grp_data in tqdm_notebook(reduced.groupby('cat5'), total=reduced['cat5'].nunique()):\n    for row_idx, row_data in grp_data.iterrows():\n        manual.loc[row_idx] = grp_data.drop(row_idx)['target'].mean()\nmanual.name = 'loo_manual'\nreduced = pd.concat([reduced, manual], axis=1)\nreduced['loo_auto'] = LeaveOneOutEncoder().fit_transform(reduced['cat5'], reduced['target'])\nplt.plot(reduced['loo_auto'], reduced['loo_manual'], '.'); plt.axis('square'); plt.grid(True)\n# Looks good. Eyeballing the plot suggests good agreement between our manual encoding and LeaveOneOutEncoder's output.","f999655b":"# Next, put the comparison through a quantitative litmus test.\nnp.allclose(reduced['loo_auto'].values, reduced['loo_manual'].values, atol=1)\n# It fails the litmus test. There is disagreement that escaped eyeballing of the plot in the previous cell.","7a8fd8c3":"# First suspect: null values?\nreduced.loc[reduced['loo_manual'].isnull(), 'cat5']\n# Indeed, that's the culprit.","4b6ab322":"# Next line of investigation: where do those null values originate from?\npblem_grp = reduced.loc[reduced['loo_manual'].isnull(), 'cat5'].values\nreduced['cat5'].value_counts().loc[pblem_grp]\n# They are from category groups which exist only on a single row. \n# Our manual calculation was correct, because by definition it is not possible to encode Leave-One-Out for category groups with count=1. \n# That's because by definition in Leave-One-Out encoding a given row leaves itself out, in this case it is left with no row.","00922a09":"# So how did LeaveOneOutEncoder get a non-null value?\nreduced.loc[reduced['cat5'].isin(pblem_grp)][['target', 'loo_manual', 'loo_auto']]\n# So LeaveOneOutEncoder plugs in as surrogate a constant value it found somewhere, 0.2626.","dea45d74":"# Let us make a wild guess where LeaveOneOutEncoder found the value 0.2626.\nreduced['target'].mean()\n# Voila.","830d2154":"# 2. Polynomial encoder","90daabc9":"# 3.2 Leave-One-Out Encoder\nLeaveOneOutEncoder is the conservative step up from the vanilla TargetEncoder. It reduces data leakage and overfitting by taking the target mean from rows other than a given row. A worked back-of-envelop example would explain best:","db672352":"We get 24 encoders from 4 libraries:\n\n| library | one-hot encoders | other simple encoders | contrast encoders | target\/Bayesian encoders |\n| --- | --- | --- | --- | --- |\n| [sklearn.preprocessing](https:\/\/scikit-learn.org\/stable\/modules\/classes.html#module-sklearn.preprocessing) | OneHotEncoder | LabelEncoder <br> OrdinalEncoder <br> LabelBinarizer | |\n| [category_encoders](https:\/\/contrib.scikit-learn.org\/category_encoders) | OneHotEncoder | OrdinalEncoder <br> BinaryEncoder <br> BaseNEncoder <br> CountEncoder <br> **HashingEncoder** | HelmertEncoder <br> SumEncoder <br> BackwardDifferenceEncoder <br> **PolynomialEncoder** | **TargetEncoder <br> LeaveOneOutEncoder <br> CatBoostEncoder** <br> MEstimateEncoder <br> WOEEncoder <br> JamesSteinEncoder <br> GLMMEncoder |\n| [pandas](https:\/\/pandas.pydata.org) | get_dummies | factorize | | |\n| [keras.utils](https:\/\/keras.io\/api\/utils) | to_categorical | | | |\n\n\n<br>This notebook explores step-by-step the Hashing Encoder, the Polynomial Encoder and some flavours\/variations of the target encoder. All flavours of target encoders peeks into the target; we therefore need to be mindful of data leakage. Options are available to regulate\/control data leakage and overfitting. *TargetEncoder* is the vanilla flavour. *LeaveOneOutEncoder* is the conservative option, where a given sample sees other samples' target but blindfolded from it's own. *CatBoostEncoder* is sensitive to row ordering; a given sample sees the target of preceding samples only. *JamesSteinEncoder* is for normal distributions.\n\nIn particular, we have in this notebook\n* ```TargetEncoder``` (the vanilla form) demonstrated in detail to tell the principle behind target encoding, which underlies all flavours of target encoding. Manual back-of-envelop derivation is compared with automated output from ```TargetEncoder```.\n* ```LeaveOneOutEncoder``` demonstrated in detail as a conservative step up to reduce data leakage and overfitting. Manual back-of-envelop derivation is compared with automated output from ```LeaveOneOutEncoder```.\n\n<br> This notebook continues from an earlier notebook, [Category Encoders: Catalog & Experiments (Part 1)](https:\/\/www.kaggle.com\/marychin\/category-encoders-catalog-experiments-part-1).\n\n<br>\nWhen to use which encoder to solve what problems? There is a good guide here: [Encode Smarter: How to Easily Integrate Categorical Encoding into Your Machine Learning Pipeline](https:\/\/innovation.alteryx.com\/encode-smarter).","7bbc9e80":"# 3.1 Target Encoder (vanilla)","b4b26ae9":"# 1. Hashing Encoder\nWhat is a Hashing Encoder? The question becomes immediately self-explanatory the moment we read the word ```hashing``` in the light of [MD5, SHA, ...](https:\/\/en.wikipedia.org\/wiki\/Secure_Hash_Algorithms). Yes, it's that same ```hash``` that the hashing encoder is about. \n\n```HashingEncoder``` takes ```n_components``` as an argument. Let us do a test with ```n_components```= 8, 16, 32:","3fed4960":"# 3. Target Encoders"}}