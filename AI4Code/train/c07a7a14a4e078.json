{"cell_type":{"625241ed":"code","624d415b":"code","c48fea7c":"code","d2918971":"code","cd9ddac8":"code","190e50c2":"code","168e1111":"code","db3f393b":"code","64f0de28":"code","435c8745":"code","ebcb51ec":"code","4c4d9609":"code","20469d2b":"code","ea516637":"code","fe637ecf":"code","7ce5ce8c":"code","44f822fa":"code","873fc968":"code","7d4692b3":"code","1e8e26a8":"code","2363aecd":"code","484c145a":"code","6b96b8c6":"code","74cbba14":"code","30d47904":"code","273ef4db":"code","4c270fc2":"code","50aeaebe":"code","9d94dfb8":"code","745ab49c":"code","52685b1b":"code","cd4cbb0e":"code","e30e2bd9":"code","9357e5b8":"code","150c2383":"code","fa0b21c1":"code","88ead7c2":"code","a142601e":"code","8c4e2414":"code","259c6823":"code","b4bb090c":"code","30011c6f":"code","4c7f8cd1":"code","ae091a31":"code","5cb32541":"code","0cd223ee":"code","a11d080f":"code","8e5b53e7":"code","caa6f299":"code","37a2ea45":"code","33abb022":"code","ec8788fc":"code","108bbc2a":"code","99fbf8f8":"code","3cfd4b7b":"code","86cb7bcc":"code","5d077db3":"code","9d12f406":"code","0f6bb029":"code","30a84bdc":"code","e36e7c48":"code","4109b2b1":"code","fa46a7e4":"code","c64c70c1":"code","1266c59f":"code","5c577c63":"code","a1eae4fb":"code","cf71171f":"code","1120be73":"code","55157587":"code","86873211":"code","2ce124ef":"code","da97d153":"code","d46a14a7":"code","47c4fbe1":"code","74f43868":"code","b10eb994":"code","521756f0":"code","f8a128e1":"code","58007bad":"code","1ae8ebf8":"code","c5ccac7f":"code","3b9ba8cd":"code","830648a4":"code","7ff8877c":"code","188bdb23":"code","a9be301d":"code","95d049ce":"code","d0b32e9b":"code","eb5899fa":"code","4ce58553":"code","ba40494f":"code","fb0b0c01":"code","d7cee6d7":"code","159399ed":"code","e86ec1fe":"code","35eaef00":"code","753a518f":"code","e1c19ec1":"code","defbbdac":"code","e53d6321":"code","a7092efd":"code","5341714b":"code","3ef01eac":"code","efc5d02c":"code","8f1248d5":"code","ed649714":"code","7598b511":"code","f0c9f3ee":"code","80aa752d":"markdown","6cf3c368":"markdown","556751d7":"markdown","cd0cfee2":"markdown","6f622556":"markdown","9f857cc6":"markdown","dca48ef0":"markdown","a2ba97b1":"markdown","d7dd962d":"markdown","f6599b12":"markdown","0c7e0afd":"markdown","4c856488":"markdown","a7e84890":"markdown","0f88ae1f":"markdown","449ddd47":"markdown","0a10e798":"markdown","5b04bb99":"markdown","d8bf4efa":"markdown","908e6d96":"markdown","720d2984":"markdown","f4f02fac":"markdown","56abc1c3":"markdown","6dd572a5":"markdown","3283f12b":"markdown","66420ffe":"markdown","78a99835":"markdown","5c374f0f":"markdown","1944b717":"markdown","627574c1":"markdown","7b41d86f":"markdown","057b5fcc":"markdown","a6940324":"markdown","679a9a0d":"markdown","5a10da3c":"markdown","22a9de50":"markdown","05a9efb5":"markdown","4275f25f":"markdown","fe1f8b1f":"markdown","2a51fc0e":"markdown","6abb1347":"markdown","7c8db3c3":"markdown","78b1e801":"markdown","e7d081eb":"markdown","fda7ebe7":"markdown","a2bb53eb":"markdown","3d0d2453":"markdown","79037390":"markdown","17fcceec":"markdown","9a6fab47":"markdown","73a9e7cd":"markdown","6e811644":"markdown","5acc1321":"markdown","ee4113ba":"markdown"},"source":{"625241ed":"!pip install nltk\n!pip install spacy\n!python -m spacy download en\n!pip install scipy\n!pip install -U scikit-learn\n!pip install seaborn\n!pip install tensorflow\n!pip install tf-nightly","624d415b":"import pandas as pd\nimport numpy as np\nfrom numpy import array,asarray,zeros\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\nfrom bs4 import BeautifulSoup\nimport re\nimport nltk\nnltk.download('stopwords')\nnltk.download('wordnet')\nfrom nltk.corpus import stopwords\nfrom nltk.stem.porter import PorterStemmer\nfrom nltk.stem import WordNetLemmatizer\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.svm import LinearSVC\nfrom sklearn.linear_model import LogisticRegression, SGDClassifier\nfrom sklearn.metrics import classification_report,confusion_matrix, accuracy_score\nfrom sklearn import metrics\nfrom sklearn.feature_selection import SelectKBest, chi2\nfrom sklearn.naive_bayes import MultinomialNB\n\nfrom keras.utils.np_utils import to_categorical \nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras import Sequential\nfrom keras.layers import Embedding,LSTM\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Activation, Flatten\nfrom keras.layers.embeddings import Embedding\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\nimport tensorflow as tf\ntf.random.set_seed(4)","c48fea7c":"#df = pd.read_csv(\"..\/input\/imdb-dataset\/IMDB Dataset.csv\",encoding=\"ISO-8859-1\")\ndf = pd.read_csv(\"..\/input\/imdb-dataset\/IMDB Dataset.csv\")\ndf.head()","d2918971":"df.sentiment.value_counts()\n#df.describe","cd9ddac8":"example_sentences = df['review'].loc[2]\nexample_sentences","190e50c2":"# 1. X\u00f3a content HTML\nsoup = BeautifulSoup(example_sentences, \"html.parser\")\nexample_sentences = soup.get_text()\nexample_sentences","168e1111":"# 2. S\u1eed d\u1ee5ng Bi\u1ec3u th\u1ee9c ch\u00ednh quy \u0111\u1ec3 x\u00f3a m\u1ecdi th\u1ee9\nexample_sentences = re.sub('\\[[^]]*\\]', ' ',example_sentences)\nexample_sentences = re.sub('[^a-zA-Z]', ' ',example_sentences)\nexample_sentences","db3f393b":"# 3. Chuy\u1ec3n qua t\u1eeb th\u01b0\u1eddng(LowerCase)\nexample_sentences = example_sentences.lower()\nexample_sentences","64f0de28":"# 4. T\u00e1ch t\u1eeb (split)\nexample_sentences = example_sentences.split()\nexample_sentences","435c8745":"# 5. X\u1eed l\u00ed t\u1eeb d\u1eebng(stopword)\nexample_sentences = [word for word in example_sentences if not word in set(stopwords.words('english'))]\nexample_sentences","ebcb51ec":"# 6. Stemming\/Lemmatization\nps = PorterStemmer()\nps_example_sentences = [ps.stem(word) for word in example_sentences]\nps_example_sentences","4c4d9609":"# 7. Lemmatization\/WordNetLemmatizer\nlem = WordNetLemmatizer()\nexample_sentences = [lem.lemmatize(word) for word in example_sentences]\nexample_sentences","20469d2b":"# 8. N\u1ed1i l\u1ea1i c\u00e1c t\u1eeb(join)\nexample_sentences = ' '.join(example_sentences)\nexample_sentences","ea516637":"# Kh\u1edfi t\u1ea1o m\u1ed9t kho(corpus) \u0111\u1ec3 ch\u1ee9a d\u1eef li\u1ec7u c\u00e2u\ncorpus = []\ncorpus.append(example_sentences)","fe637ecf":"# Vector h\u00f3a c\u00e2u b\u1eb1ng m\u00f4 h\u00ecnh t\u00fai t\u1eeb (Bag of Words)--> ch\u00fang ta s\u1ebd th\u1ea5y c\u00f3 s\u1ed1 1,2 & 3...\ndem_vecto = CountVectorizer()\nsentences_dem_vecto = dem_vecto.fit_transform(corpus)\nsentences_dem_vecto.toarray()","7ce5ce8c":"# bi\u1ebfn \u0111\u1ed5i t\u00fai t\u1eeb th\u00e0nh 1- Vectorizer v\u1edbi Binary = True s\u1ebd l\u00e0m \u0111\u01b0\u1ee3c vi\u1ec7c \u0111\u00f3\ndem_vecto_bin = CountVectorizer(binary=True)\nsentences_dem_vecto_bin = dem_vecto_bin.fit_transform(corpus)\nsentences_dem_vecto_bin.toarray()","44f822fa":"tfidf_vecto = TfidfVectorizer()\nsentences_tfidf_vecto = tfidf_vecto.fit_transform(corpus)\nsentences_tfidf_vecto.toarray()","873fc968":"x_train, x_test, y_train, y_test= train_test_split(df['review'], df['sentiment'], test_size=0.25, random_state=42)\ny_train = (y_train.replace({'positive': 1, 'negative': 0})).values\ny_test  = (y_test.replace({'positive': 1, 'negative': 0})).values\nprint(y_train)\nprint(y_test)","7d4692b3":"corpus_train = []\nfor i in range(x_train.shape[0]):\n    soup = BeautifulSoup(x_train.iloc[i], \"html.parser\")\n    review = soup.get_text()\n    review = re.sub('\\[[^]]*\\]', ' ', review)\n    review = re.sub('[^a-zA-Z]', ' ', review)\n    review = review.lower()\n    review = review.split()\n    review = [word for word in review if not word in set(stopwords.words('english'))]\n    lem = WordNetLemmatizer()\n    review = [lem.lemmatize(word) for word in review]\n    review = ' '.join(review)\n    corpus_train.append(review)","1e8e26a8":"corpus_test  = []\nfor j in range(x_test.shape[0]):\n    soup = BeautifulSoup(x_test.iloc[j], \"html.parser\")\n    review = soup.get_text()\n    review = re.sub('\\[[^]]*\\]', ' ', review)\n    review = re.sub('[^a-zA-Z]', ' ', review)\n    review = review.lower()\n    review = review.split()\n    review = [word for word in review if not word in set(stopwords.words('english'))]\n    lem = WordNetLemmatizer()\n    review = [lem.lemmatize(word) for word in review]\n    review = ' '.join(review)\n    corpus_test.append(review)","2363aecd":"corpus_train[-1]","484c145a":"corpus_test[-1]","6b96b8c6":"# Vect\u01a1 h\u00f3a b\u1eb1ng k\u1ef9 thu\u1eadt TF-IDF - Ph\u1ea7n n\u00e0y s\u1eed d\u1ee5ng chung cho m\u00f4 h\u00ecnh\ntfidf_vecto = TfidfVectorizer(ngram_range=(1, 3))\ntfidf_vecto_train = tfidf_vecto.fit_transform(corpus_train)\ntfidf_vecto_test = tfidf_vecto.transform(corpus_test)","74cbba14":"LSVC = LinearSVC(C=0.5, random_state=42)\nLSVC.fit(tfidf_vecto_train, y_train)\ndu_doan = LSVC.predict(tfidf_vecto_test)","30d47904":"from sklearn.metrics import classification_report,confusion_matrix, accuracy_score\nfrom sklearn import metrics\nprint(\"Classification report of Linear SVC: \\n\",classification_report(y_test,du_doan, target_names = ['Positive','Negative']))\nprint(\"Confusion Matrix of Linear SVC: \\n\",confusion_matrix(y_test,du_doan))\ndochinhxac = metrics.accuracy_score(y_test,du_doan)\nprint(\"\u0110\u1ed9 ch\u00ednh x\u00e1c c\u1ee7a vi\u1ec7c s\u1eed d\u1ee5ng TF-IDF fit v\u1edbi m\u00f4 h\u00ecnh Linear SVC: \\n\" +str('{:04.2f}'.format(dochinhxac *100)) + \"%\")","273ef4db":"dem_vecto = CountVectorizer(ngram_range=(1,3), binary=False)\nfalse_dem_vecto_train = dem_vecto.fit_transform(corpus_train)\nfalse_dem_vecto_test = dem_vecto.transform(corpus_test)","4c270fc2":"false_dem_LSVC = LinearSVC(C=0.5, random_state=42, max_iter=5000)\nfalse_dem_LSVC.fit(false_dem_vecto_train,y_train)\nfalse_dem_du_doan_LSVC = false_dem_LSVC.predict(false_dem_vecto_test)","50aeaebe":"print(\"K\u1ebft qu\u1ea3 c\u1ee7a vi\u1ec7c s\u1eed d\u1ee5ng CountVectorizer (binary = False) v\u00e0 fit v\u1edbi m\u00f4 h\u00ecnh LinearSVC\\n\")\nprint(\"Classification report of Linear SVC (binary = False): \\n\",classification_report(y_test,false_dem_du_doan_LSVC, target_names = ['Positive','Negative']))\nprint(\"Confusion Matrix of Linear SVC (binary = False): \\n\",confusion_matrix(y_test,false_dem_du_doan_LSVC))\naccuracy_score_false_lsvc = metrics.accuracy_score(y_test,false_dem_du_doan_LSVC)\nprint(\"\u0110\u1ed9 ch\u00ednh x\u00e1c c\u1ee7a vi\u1ec7c s\u1eed d\u1ee5ng CountVectorizer (binary = False) fit v\u1edbi m\u00f4 h\u00ecnh Linear SVC: \\n\" +str('{:04.2f}'.format(accuracy_score_false_lsvc *100)) + \"%\")","9d94dfb8":"true_dem_vecto = CountVectorizer(ngram_range=(1,3), binary=True)\ntrue_dem_vecto_train = true_dem_vecto.fit_transform(corpus_train)\ntrue_dem_vecto_test = true_dem_vecto.transform(corpus_test)","745ab49c":"true_dem_LSVC = LinearSVC(C=0.5, random_state=42,max_iter=5000)\ntrue_dem_LSVC.fit(true_dem_vecto_train,y_train)\ntrue_dem_du_doan_LSVC = true_dem_LSVC.predict(true_dem_vecto_test)","52685b1b":"print(\"K\u1ebft qu\u1ea3 c\u1ee7a vi\u1ec7c s\u1eed d\u1ee5ng CountVectorizer (binary = True) v\u00e0 fit v\u1edbi m\u00f4 h\u00ecnh LinearSVC\\n\")\nprint(\"Classification report of Linear SVC binary = True: \\n\",classification_report(y_test,true_dem_du_doan_LSVC, target_names = ['Positive','Negative']))\nprint(\"Confusion Matrix of Linear SVC binary = True: \\n\",confusion_matrix(y_test,true_dem_du_doan_LSVC))\naccuracy_score_true_LSVC = metrics.accuracy_score(y_test,true_dem_du_doan_LSVC)\nprint(\"\u0110\u1ed9 ch\u00ednh x\u00e1c c\u1ee7a vi\u1ec7c s\u1eed d\u1ee5ng CountVectorizer (binary = True) fit v\u1edbi m\u00f4 h\u00ecnh Linear SVC: \\n\" +str('{:04.2f}'.format(accuracy_score_true_LSVC *100)) + \"%\")","cd4cbb0e":"tfidf_vecto_bayes = TfidfVectorizer(ngram_range=(1, 1))\ntfidf_vecto_bayes_train = tfidf_vecto_bayes.fit_transform(corpus_train)\ntfidf_vecto_bayes_test = tfidf_vecto_bayes.transform(corpus_test)\nprint(tfidf_vecto_bayes_train.toarray().shape,tfidf_vecto_bayes_test.toarray().shape)","e30e2bd9":"from sklearn.feature_selection import SelectKBest, chi2\nchichi = SelectKBest(chi2, k=50000)\ntfidf_vecto_bayes_train = chichi.fit_transform(tfidf_vecto_bayes_train,y_train)\ntfidf_vecto_bayes_test = chichi.transform(tfidf_vecto_bayes_test)","9357e5b8":"feat_names = tfidf_vecto_bayes.get_feature_names()\nfeat_names = [feat_names[i] for i in chichi.get_support(indices=True)]\nfeat_names = np.asarray(feat_names)\nfeat_names[32245]","150c2383":"clf_MNB = MultinomialNB()\nclf_MNB.fit(tfidf_vecto_bayes_train,y_train)\ndu_doan_MNB = clf_MNB.predict(tfidf_vecto_bayes_test)","fa0b21c1":"print(\"K\u1ebft qu\u1ea3 c\u1ee7a vi\u1ec7c s\u1eed d\u1ee5ng TF-IDF fit v\u1edbi m\u00f4 h\u00ecnh Multinomial Naive Bayes\\n\")\nprint(\"Classification report of Multinomial Naive Bayes: \\n\",classification_report(y_test,du_doan_MNB, target_names = ['Positive','Negative']))\nprint(\"Confusion Matrix of Multinomial Naive Bayes: \\n\",confusion_matrix(y_test,du_doan_MNB))\naccuracy_score_MNB = metrics.accuracy_score(y_test,du_doan_MNB)\nprint(\"\u0110\u1ed9 ch\u00ednh x\u00e1c c\u1ee7a vi\u1ec7c s\u1eed d\u1ee5ng TF-IDF fit v\u1edbi m\u00f4 h\u00ecnh Multinomial Naive Bayes: \\n\" +str('{:04.2f}'.format(accuracy_score_MNB *100)) + \"%\")","88ead7c2":"vecto_MNB = CountVectorizer(ngram_range=(1, 3), binary=False)\ndem_vecto_train_MNB = vecto_MNB.fit_transform(corpus_train)\ndem_vecto_test_MNB = vecto_MNB.transform(corpus_test)","a142601e":"dem_MNB = MultinomialNB()\ndem_MNB.fit(dem_vecto_train_MNB,y_train)\ndem_du_doan_MNB = dem_MNB.predict(dem_vecto_test_MNB)","8c4e2414":"print(\"K\u1ebft qu\u1ea3 c\u1ee7a vi\u1ec7c s\u1eed d\u1ee5ng CountVectorizer (binary = False) v\u00e0 fit v\u1edbi m\u00f4 h\u00ecnh Multinomial Naive Bayes\\n\")\nprint(\"Classification report of Multinomial Naive Bayes binary = False: \\n\",classification_report(y_test,dem_du_doan_MNB, target_names = ['Positive','Negative']))\nprint(\"Confusion Matrix of Multinomial Naive Bayes binary = False: \\n\",confusion_matrix(y_test,dem_du_doan_MNB))\naccuracy_score_dem_MNB = metrics.accuracy_score(y_test,dem_du_doan_MNB)\nprint(\"\u0110\u1ed9 ch\u00ednh x\u00e1c c\u1ee7a vi\u1ec7c s\u1eed d\u1ee5ng CountVectorizer (binary = False) fit Multinomial Naive Bayes: \\n\" +str('{:04.2f}'.format(accuracy_score_dem_MNB *100)) + \"%\")","259c6823":"true_dem_vecto_MNB = CountVectorizer(ngram_range=(1,3), binary=True)\ntrue_dem_vecto_train_MNB = true_dem_vecto_MNB.fit_transform(corpus_train)\ntrue_dem_vecto_test_MNB = true_dem_vecto_MNB.transform(corpus_test)","b4bb090c":"true_dem_MNB = MultinomialNB()\ntrue_dem_MNB.fit(true_dem_vecto_train_MNB,y_train)\ntrue_dem_du_doan_MNB = true_dem_MNB.predict(true_dem_vecto_test_MNB)","30011c6f":"print(\"K\u1ebft qu\u1ea3 c\u1ee7a vi\u1ec7c s\u1eed d\u1ee5ng CountVectorizer (binary = True) v\u00e0 fit v\u1edbi m\u00f4 h\u00ecnh Multinomial Naive Bayes\\n\")\nprint(\"Classification report of Multinomial Naive Bayes binary = True: \\n\",classification_report(y_test,true_dem_du_doan_MNB, target_names = ['Positive','Negative']))\nprint(\"Confusion Matrix of Multinomial Naive Bayes binary = True: \\n\",confusion_matrix(y_test,true_dem_du_doan_MNB))\naccuracy_score_true_dem_MNB = metrics.accuracy_score(y_test,true_dem_du_doan_MNB)\nprint(\"\u0110\u1ed9 ch\u00ednh x\u00e1c c\u1ee7a vi\u1ec7c s\u1eed d\u1ee5ng CountVectorizer (binary = True) fit Multinomial Naive Bayes: \\n\" +str('{:04.2f}'.format(accuracy_score_true_dem_MNB *100)) + \"%\")","4c7f8cd1":"#Count vectorizer for bag of words\ndem_vecto_SGD = CountVectorizer(binary=False,ngram_range=(1,3))\ndem_vecto_train_SGD=dem_vecto_SGD.fit_transform(corpus_train)\ndem_vecto_test_SGD=dem_vecto_SGD.transform(corpus_test)\nprint('Count vectorizer for train:',dem_vecto_train_SGD.shape)\nprint('Count vectorizer for test:',dem_vecto_test_SGD.shape)\n\nSGDC = SGDClassifier(loss='hinge',random_state=42)\nfalse_dem_SGDC = SGDC.fit(dem_vecto_train_SGD,y_train)\n\ntfidf_SGDC= SGDC.fit(tfidf_vecto_train,y_train)\n#Predicting the model for bag of words\nfalse_dem_du_doan_SGDC=false_dem_SGDC.predict(dem_vecto_test_SGD)\nprint(false_dem_du_doan_SGDC)\n#Predicting the model for tfidf features\ntfidf_du_doan_SGDC=tfidf_SGDC.predict(tfidf_vecto_test)\nprint(tfidf_du_doan_SGDC)","ae091a31":"print(\"K\u1ebft qu\u1ea3 c\u1ee7a vi\u1ec7c s\u1eed d\u1ee5ng TF_IDF fit v\u1edbi m\u00f4 h\u00ecnh SGD Classifier\\n\")\nprint(\"Classification report of SGD Classifier: \\n\",classification_report(y_test,tfidf_du_doan_SGDC, target_names = ['Positive','Negative']))\nprint(\"Confusion Matrix of SGD Classifier: \\n\",confusion_matrix(y_test,tfidf_du_doan_SGDC))\naccuracy_score_tfidf_SGDC = metrics.accuracy_score(y_test,tfidf_du_doan_SGDC)\nprint(\"\u0110\u1ed9 ch\u00ednh x\u00e1c c\u1ee7a vi\u1ec7c s\u1eed d\u1ee5ng TF_IDF fit v\u1edbi m\u00f4 h\u00ecnh SGD Classifier: \\n\" +str('{:04.2f}'.format(accuracy_score_tfidf_SGDC *100)) + \"%\")","5cb32541":"print(\"K\u1ebft qu\u1ea3 c\u1ee7a vi\u1ec7c s\u1eed d\u1ee5ng CountVectorizer (binary = False) fit v\u1edbi m\u00f4 h\u00ecnh SGD Classifier\\n\")\nprint(\"Classification report of SGD Classifier: \\n\",classification_report(y_test,false_dem_du_doan_SGDC, target_names = ['Positive','Negative']))\nprint(\"Confusion Matrix of SGD Classifier: \\n\",confusion_matrix(y_test,false_dem_du_doan_SGDC))\naccuracy_score_false_count_SGDC = metrics.accuracy_score(y_test,false_dem_du_doan_SGDC)\nprint(\"\u0110\u1ed9 ch\u00ednh x\u00e1c c\u1ee7a vi\u1ec7c s\u1eed d\u1ee5ng CountVectorizer (binary = False) fit v\u1edbi m\u00f4 h\u00ecnh SGD Classifier: \\n\" +str('{:04.2f}'.format(accuracy_score_false_count_SGDC *100)) + \"%\")","0cd223ee":"true_dem_vecto=CountVectorizer(binary=True,ngram_range=(1,3))\ntrue_dem_vecto_train=true_dem_vecto.fit_transform(corpus_train)\ntrue_dem_vecto_test=true_dem_vecto.transform(corpus_test)\n\ntrue_SGDC = SGDClassifier(loss='hinge',random_state=42)\ntrue_dem_SGDC=true_SGDC.fit(true_dem_vecto_train,y_train)\n\ntrue_dem_du_doan_SGDC=true_dem_SGDC.predict(true_dem_vecto_test)\nprint(\"K\u1ebft qu\u1ea3 c\u1ee7a vi\u1ec7c s\u1eed d\u1ee5ng CountVectorizer (binary = True) fit v\u1edbi m\u00f4 h\u00ecnh SGD Classifier\\n\")\nprint(\"Classification report of SGD Classifier: \\n\",classification_report(y_test,true_dem_du_doan_SGDC, target_names = ['Positive','Negative']))\nprint(\"Confusion Matrix of SGD Classifier: \\n\",confusion_matrix(y_test,true_dem_du_doan_SGDC))\naccuracy_score_true_count_SGDC = metrics.accuracy_score(y_test,true_dem_du_doan_SGDC)\nprint(\"\u0110\u1ed9 ch\u00ednh x\u00e1c c\u1ee7a vi\u1ec7c s\u1eed d\u1ee5ng CountVectorizer (binary = True) fit SGD Classifier: \\n\" +str('{:04.2f}'.format(accuracy_score_true_count_SGDC *100)) + \"%\")","a11d080f":"LR_dem_vecto=CountVectorizer(binary=False,ngram_range=(1,3))\nLR_false_dem_vecto_train=LR_dem_vecto.fit_transform(corpus_train)\nLR_false_dem_vecto_test=LR_dem_vecto.transform(corpus_test)\n\nLR=LogisticRegression(penalty='l2',max_iter=500,C=1,random_state=42)\ndem_LR=LR.fit(LR_false_dem_vecto_train,y_train)\ntfidf_LR=LR.fit(tfidf_vecto_train,y_train)\n\nfalse_du_doan_LR=dem_LR.predict(LR_false_dem_vecto_test)\ndu_doan_tfidf_LR=tfidf_LR.predict(tfidf_vecto_test)\n","8e5b53e7":"print(\"K\u1ebft qu\u1ea3 c\u1ee7a vi\u1ec7c s\u1eed d\u1ee5ng TF-IDF fit v\u1edbi m\u00f4 h\u00ecnh h\u1ed3i quy logistic\\n\")\nprint(\"Classification report of Logistic Regression: \\n\",classification_report(y_test,du_doan_tfidf_LR, target_names = ['Positive','Negative']))\nprint(\"Confusion Matrix of Logistic Regression: \\n\",confusion_matrix(y_test,du_doan_tfidf_LR))\naccuracy_score_tfidf_LR = metrics.accuracy_score(y_test,du_doan_tfidf_LR)\nprint(\"\u0110\u1ed9 ch\u00ednh x\u00e1c c\u1ee7a vi\u1ec7c s\u1eed d\u1ee5ng TF-IDF fit v\u1edbi m\u00f4 h\u00ecnh h\u1ed3i quy logistic: \\n\" +str('{:04.2f}'.format(accuracy_score_tfidf_LR *100)) + \"%\")","caa6f299":"print(\"K\u1ebft qu\u1ea3 c\u1ee7a vi\u1ec7c s\u1eed d\u1ee5ng CountVectorizer (binary=False) fit v\u1edbi m\u00f4 h\u00ecnh h\u1ed3i quy logistic\\n\")\nprint(\"Classification report of Logistic Regression: \\n\",classification_report(y_test,false_du_doan_LR, target_names = ['Positive','Negative']))\nprint(\"Confusion Matrix of Logistic Regression: \\n\",confusion_matrix(y_test,false_du_doan_LR))\naccuracy_score_false_count_LR = metrics.accuracy_score(y_test,false_du_doan_LR)\nprint(\"\u0110\u1ed9 ch\u00ednh x\u00e1c c\u1ee7a vi\u1ec7c s\u1eed d\u1ee5ng CountVectorizer (binary=False) fit v\u1edbi m\u00f4 h\u00ecnh h\u1ed3i quy logistic: \\n\" +str('{:04.2f}'.format(accuracy_score_false_count_LR *100)) + \"%\")","37a2ea45":"true_dem_LR_vecto=CountVectorizer(binary=True,ngram_range=(1,3))\ntrue_dem_LR_vecto_train=true_dem_LR_vecto.fit_transform(corpus_train)\ntrue_dem_LR_vecto_test=true_dem_LR_vecto.transform(corpus_test)\n\ntrue_LR=LogisticRegression(penalty='l2',max_iter=500,C=1,random_state=42)\ntrue_dem_LR=true_LR.fit(true_dem_LR_vecto_train,y_train)\n\ntrue_du_doan_LR=true_dem_LR.predict(true_dem_LR_vecto_test)\n\nprint(\"K\u1ebft qu\u1ea3 c\u1ee7a vi\u1ec7c s\u1eed d\u1ee5ng CountVectorizer (binary = True) fit v\u1edbi m\u00f4 h\u00ecnh h\u1ed3i quy logistic\\n\")\nprint(\"Classification report of Logistic Regression: \\n\",classification_report(y_test,true_du_doan_LR, target_names = ['Positive','Negative']))\nprint(\"Confusion Matrix of Logistic Regression: \\n\",confusion_matrix(y_test,true_du_doan_LR))\naccuracy_score_true_count_LR = metrics.accuracy_score(y_test,true_du_doan_LR)\nprint(\"\u0110\u1ed9 ch\u00ednh x\u00e1c c\u1ee7a vi\u1ec7c s\u1eed d\u1ee5ng CountVectorizer (binary = True) fit v\u1edbi m\u00f4 h\u00ecnh h\u1ed3i quy logistic: \\n\" +str('{:04.2f}'.format(accuracy_score_true_count_LR *100)) + \"%\")","33abb022":"accuracy = {'LSVC': ['90.29','89.77','89.46'],\n            'MNB': ['86.63','88.75','89.10'],\n            'SGDC':['88.67','85.84','89.46'],\n            'LR' :['88.50','85.62', '89.65']}\naccuracy = pd.DataFrame(accuracy,index=['TF-IDF','Binary=False','Binary=True'])\naccuracy","ec8788fc":"df_predicted = x_test.copy()\ndf_predicted = pd.DataFrame(df_predicted)\ndf_predicted.columns = ['review']\ndf_predicted = df_predicted.reset_index()\ndf_predicted = df_predicted.drop(['index'], axis=1)\ndf_predicted.head()","108bbc2a":"y_test_hientai = y_test.copy()\ny_test_hientai = pd.DataFrame(y_test_hientai)\ny_test_hientai.columns = ['sentiment']\ny_test_hientai['sentiment'] = y_test_hientai['sentiment'].replace({1: 'positive', 0: 'negative'})","99fbf8f8":"y_test_predicted = du_doan.copy()\ny_test_predicted = pd.DataFrame(y_test_predicted)\ny_test_predicted.columns = ['predicted_sentiment']\ny_test_predicted['predicted_sentiment'] = y_test_predicted['predicted_sentiment'].replace({1: 'positive', 0: 'negative'})","3cfd4b7b":"ketqua_test = pd.concat([df_predicted, y_test_hientai, y_test_predicted], axis=1)\nketqua_test.head()","86cb7bcc":"# T\u1ea1o m\u1ed9t t\u1eeb \u0111\u1ec3 l\u1eadp ch\u1ec9 m\u1ee5c t\u1eeb \u0111i\u1ec3n t\u1eeb m\u00f4 \u0111un tokenizer \n# M\u1ed7i t\u1eeb \u0111\u01b0\u1ee3c s\u1eed d\u1ee5ng l\u00e0m kh\u00f3a trong khi ch\u1ec9 m\u1ee5c duy nh\u1ea5t t\u01b0\u01a1ng \u1ee9ng v\u00e0 \u0111\u01b0\u1ee3c s\u1eed d\u1ee5ng l\u00e0m gi\u00e1 tr\u1ecb cho kh\u00f3a.\ntokenizer = Tokenizer(num_words=5000)\ntokenizer.fit_on_texts(x_train)\n \nx_train = tokenizer.texts_to_sequences(x_train)\nx_test = tokenizer.texts_to_sequences(x_test)","5d077db3":"#x_train\n#print(y_train)\n#print(y_test)\n#x_test","9d12f406":"do_dai_danhsach_x = [len(i) for i in x_train + x_test]\nprint(f'\u0110\u1ed9 d\u00e0i t\u1ed1i \u0111a c\u1ee7a c\u00e2u:{max(do_dai_danhsach_x)}')\nprint(f'\u0110\u1ed9 d\u00e0i trung b\u00ecnh c\u1ee7a c\u00e2u:{np.mean(do_dai_danhsach_x)}')","0f6bb029":"# S\u1eed d\u1ee5ng seaborn \u0111\u1ec3 v\u1ebd \u0111\u1ed9 d\u00e0i c\u00e2u\nplt.figure(figsize=(8,5), dpi= 80)\nsns.distplot(do_dai_danhsach_x, color='blue')\nplt.title('Bi\u1ec3u \u0111\u1ed3 th\u1ec3 hi\u1ec7n s\u1ed1 l\u01b0\u1ee3ng t\u1eeb',fontsize=20,color='black')\nplt.xlabel('S\u1ed1 t\u1eeb')\nplt.ylabel('S\u1ed1 l\u01b0\u1ee3t review')\nplt.show()","30a84bdc":"vocabulary_size = len(tokenizer.word_index) + 1\nmaxlength = 100\n\nx_train_padding = pad_sequences(x_train, padding=\"post\",maxlen=maxlength)\nx_test_padding = pad_sequences(x_test, padding=\"post\",maxlen=maxlength)\n# Ki\u1ec3m tra danh s\u00e1ch ng\u1eabu nhi\u00ean trong X_train (c\u00e1c c\u00e2u tr\u01b0\u1edbc \u0111\u00f3)\n# c\u00f3 c\u00f9ng \u0111\u1ed9 d\u00e0i l\u00e0 100\nlen(x_train[3])","e36e7c48":"# Ki\u1ec3m tra x\u00edu\nx_train_padding","4109b2b1":"vocabulary_size","fa46a7e4":"tudien_duoc_nhung = dict()\nglove_file = open('..\/input\/glove6b100d\/glove.6B.100d.txt',encoding='utf-8')\nfor line in glove_file:\n    record = line.split()\n    word = record[0]\n    kich_thuoc = asarray(record[1:], dtype = \"float32\")\n    tudien_duoc_nhung[word] = kich_thuoc\nglove_file.close()","c64c70c1":"ma_tran_nhung = zeros((vocabulary_size,100))\nfor word, index in tokenizer.word_index.items():\n    vecto_nhung = tudien_duoc_nhung.get(word)\n    if vecto_nhung is not None:\n        ma_tran_nhung[index] = vecto_nhung","1266c59f":"vecto_nhung","5c577c63":"ma_tran_nhung","a1eae4fb":"# B\u1ea1n c\u00f3 th\u1ec3 x\u00f3a d\u1ea5u # \u0111\u1ec3 in ra 'tudien_duoc_nhung'\n#tudien_duoc_nhung","cf71171f":"mo_hinh = Sequential()\nlop_nhung = Embedding(vocabulary_size, 100, weights = [ma_tran_nhung],input_length=maxlength,trainable=False)\nmo_hinh.add(lop_nhung)\nmo_hinh.add(Flatten())\nmo_hinh.add(Dense(1, activation = 'sigmoid'))","1120be73":"# Bi\u00ean d\u1ecbch\nmo_hinh.compile(optimizer = 'adam', loss = 'binary_crossentropy',metrics = ['acc'])\nprint(mo_hinh.summary())","55157587":"history = mo_hinh.fit(x_train_padding, y_train, batch_size=128,epochs = 20, verbose=1, validation_split=0.2)","86873211":"diem_danh_gia = mo_hinh.evaluate(x_test_padding, y_test, verbose = 1)","2ce124ef":"#Ki\u1ec3m tra \u0111\u1ed9 ch\u00ednh x\u00e1c\nprint(\"\u0110\u1ed9 ch\u00ednh x\u00e1c: {:.2%}\".format(diem_danh_gia[1]))","da97d153":"du_doan_y = mo_hinh.predict(x_test_padding)\ndu_doan_y = np.round(du_doan_y).astype(int)\nprint(classification_report(y_test,du_doan_y))","d46a14a7":"plt.plot(history.history['acc'])\nplt.plot(history.history['val_acc'])\nplt.title('Bi\u1ec3u \u0111\u1ed3 hi\u1ec7u su\u1ea5t ch\u00ednh x\u00e1c c\u1ee7a m\u00f4 h\u00ecnh',fontsize=15)\nplt.xlabel('Epochs',fontsize=15)\nplt.ylabel('Accuracy',fontsize=15)\nplt.legend(['Train data','Test data'],loc='upper left')\nplt.show()\n\nplt.plot(history.history['loss'])\nplt.plot(history.history['val_loss'])\nplt.title('Bi\u1ec3u \u0111\u1ed3 hi\u1ec7u su\u1ea5t th\u1ea5t b\u1ea1i c\u1ee7a m\u00f4 h\u00ecnh',fontsize=15)\nplt.xlabel('Epochs',fontsize=10)\nplt.ylabel('Loss',fontsize=10)\nplt.legend(['train','test'],loc='upper left')\nplt.show()","47c4fbe1":"mo_hinh = tf.keras.Sequential([tf.keras.layers.Embedding(vocabulary_size, 100,weights=[ma_tran_nhung],input_length=maxlength, trainable=False),\n                                tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(128)),tf.keras.layers.Dense(128, activation='relu'),\n                                tf.keras.layers.Dense(1, activation='sigmoid')])","74f43868":"mo_hinh.compile(loss='binary_crossentropy',optimizer=tf.keras.optimizers.Adam(1e-4),metrics=['accuracy'])","b10eb994":"mo_hinh.summary()","521756f0":"history = mo_hinh.fit(x_train_padding, y_train, epochs=10,batch_size = 128, verbose = 1,validation_split = 0.2)","f8a128e1":"acc_score = mo_hinh.evaluate(x_test_padding, y_test, verbose = 1)\nacc_score","58007bad":"print(\"Accuracy Test: {:.2%}\".format(acc_score[1]))\nprint(\"Accuracy Score: {:.2%}\".format(acc_score[0]))","1ae8ebf8":"# h\u00e0m d\u1ef1 \u0111\u00f3\u00e1n\ndu_doan_y = mo_hinh.predict(x_test_padding)\ndu_doan_y = np.round(du_doan_y).astype(int)\nprint(classification_report(y_test,du_doan_y))","c5ccac7f":"plt.plot(history.history['accuracy'],color=\"red\")\nplt.plot(history.history['val_accuracy'])\nplt.title(\"Hi\u1ec7u Su\u1ea5t Ch\u00ednh X\u00e1c M\u00f4 H\u00ecnh S\u1eed D\u1ee5ng RNN\",fontsize=15)\nplt.xlabel(\"Epochs\",fontsize=10)\nplt.ylabel(\"Accuracy\",fontsize=10)\nplt.legend([\"Train Data\",\"Text data\"],loc='upper left')\nplt.show()","3b9ba8cd":"plt.plot(history.history['loss'],color=\"red\")\nplt.plot(history.history['val_loss'])\nplt.title(\"Hi\u1ec7u Su\u1ea5t Th\u1ea5t B\u1ea1i M\u00f4 H\u00ecnh S\u1eed D\u1ee5ng RNN\",fontsize=15)\nplt.xlabel(\"Epochs\",fontsize=10)\nplt.ylabel(\"loss\",fontsize=10)\nplt.legend([\"train Data\",\"text data\"],loc='upper left')\nplt.show()","830648a4":"mo_hinh = tf.keras.Sequential([tf.keras.layers.Embedding(vocabulary_size, 100,weights=[ma_tran_nhung],input_length=maxlength, trainable=False),\n            tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(50, return_sequences = True)),\n            tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(25)),\n            tf.keras.layers.Dense(50, activation='relu'),\n            tf.keras.layers.Dropout(0.5),\n            tf.keras.layers.Dense(1, activation='sigmoid')])","7ff8877c":"mo_hinh.summary()","188bdb23":"mo_hinh.compile(loss='binary_crossentropy',optimizer=tf.keras.optimizers.Adam(1e-4),metrics=['accuracy'])","a9be301d":"history = mo_hinh.fit(x_train_padding, y_train, epochs= 10,batch_size = 128, verbose = 1,validation_split = 0.2)","95d049ce":"history = mo_hinh.fit(x_train_padding, y_train, epochs= 5,batch_size = 128, verbose = 1,validation_split = 0.2)","d0b32e9b":"history = mo_hinh.fit(x_train_padding, y_train, epochs= 5,batch_size = 128, verbose = 1,validation_split = 0.2)","eb5899fa":"history = mo_hinh.fit(x_train_padding, y_train, epochs= 2,batch_size = 128, verbose = 1,validation_split = 0.2)","4ce58553":"history = mo_hinh.fit(x_train_padding, y_train, epochs= 2,batch_size = 128, verbose = 1,validation_split = 0.2)","ba40494f":"history = mo_hinh.fit(x_train_padding, y_train, epochs= 1,batch_size = 128, verbose = 1,validation_split = 0.2)","fb0b0c01":"history = mo_hinh.fit(x_train_padding, y_train, epochs= 1,batch_size = 128, verbose = 1,validation_split = 0.2)","d7cee6d7":"acc_score = mo_hinh.evaluate(x_test_padding, y_test, verbose = 1)\nacc_score","159399ed":"print(\"Accuracy Score: {:.2%}\".format(acc_score[0]))\nprint(\"Accuracy Train:{:.2%}\".format(acc_score[1]))","e86ec1fe":"du_doan_y = mo_hinh.predict(x_test_padding)\ndu_doan_y = np.round(du_doan_y).astype(int)\nprint(classification_report(y_test,du_doan_y))","35eaef00":"from keras.layers.core import Activation, Dropout, Dense\nfrom keras.layers import Flatten\nfrom keras.layers import GlobalMaxPooling1D,Conv1D,LSTM\n\nmo_hinh = Sequential()\n\nlop_nhung = Embedding(vocabulary_size, 100, weights=[ma_tran_nhung], input_length=maxlength , trainable=False)\nmo_hinh.add(lop_nhung)\n\nmo_hinh.add(Conv1D(128, 5, activation='relu'))\nmo_hinh.add(GlobalMaxPooling1D())\nmo_hinh.add(Dropout(0.2)),\nmo_hinh.add(Dense(1, activation='sigmoid'))","753a518f":"mo_hinh.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])","e1c19ec1":"print(mo_hinh.summary())","defbbdac":"history = mo_hinh.fit(x_train_padding, y_train, batch_size=128, epochs=6, verbose=1, validation_split=0.2)\naccuracy_score = mo_hinh.evaluate(x_test_padding, y_test, verbose=1)","e53d6321":"history = mo_hinh.fit(x_train_padding, y_train, batch_size=128, epochs=4, verbose=1, validation_split=0.2)\n\naccuracy_score = mo_hinh.evaluate(x_test_padding, y_test, verbose=1)","a7092efd":"history = mo_hinh.fit(x_train_padding, y_train, batch_size=128, epochs=10, verbose=1, validation_split=0.2)\n\naccuracy_score = mo_hinh.evaluate(x_test_padding, y_test, verbose=1)","5341714b":"du_doany = mo_hinh.predict(x_test_padding)\ndu_doany = np.round(du_doany).astype(int)\nprint(classification_report(y_test,du_doany))","3ef01eac":"accuracy_score","efc5d02c":"print(\"Test Score: {:.2%}\".format(accuracy_score[0]))\nprint(\"Test Accuracy: {:.2%}\".format(accuracy_score[1]))","8f1248d5":"from wordcloud import WordCloud \nplt.figure(figsize=(15,5))\npositive_text = corpus_train[2]\nWC=WordCloud(width=1500,height=1000,max_words=500,min_font_size=5)\npositive_words=WC.generate(positive_text)\nplt.imshow(positive_words,interpolation='bilinear')\nplt.show","ed649714":"from wordcloud import WordCloud \nplt.figure(figsize=(15,5))\npositive_text = corpus_train[20]\nWC=WordCloud(width=1500,height=1000,max_words=500,min_font_size=5)\npositive_words=WC.generate(positive_text)\nplt.imshow(positive_words,interpolation='bilinear')\nplt.show","7598b511":"index = np.random.randint(x_train_padding.shape[0])\nex = corpus_train[index]\nprint(ex)","f0c9f3ee":"ex = tokenizer.texts_to_sequences(ex)\nds = []\nfor e in ex:\n    for i in e:\n        ds.append(i)\nds = [ds]\nex = pad_sequences(ds, padding = 'post', maxlen = maxlength)\nmo_hinh.predict(ex)","80aa752d":"**\u0110\u1ed9 ch\u00ednh x\u00e1c khi s\u1eed d\u1ee5ng CNN tr\u00ean t\u1eadp d\u1eef li\u1ec7u l\u00e0 84.71%**","6cf3c368":"**\u0110\u1ed9 ch\u00ednh x\u00e1c c\u1ee7a vi\u1ec7c s\u1eed d\u1ee5ng CountVectorizer (binary = True) fit v\u1edbi m\u00f4 h\u00ecnh Linear SVC: 89.46%**\n\n***TUY\u1ec6T V\u1edcI! ch\u00fang ta \u0111\u1ea1t \u0111\u01b0\u1ee3c \u0111\u1ed9 ch\u00ednh x\u00e1c g\u1ea7n nh\u01b0 t\u1ed1i \u0111a b\u1eb1ng c\u00e1ch s\u1eed d\u1ee5ng c\u00f4ng c\u1ee5 vect\u01a1 TF-IDF. V\u00e0 c\u0169ng b\u1eb1ng c\u00f4ng c\u1ee5 n\u00e0y t\u00f4i s\u1ebd s\u1eed d\u1ee5ng th\u00eam m\u00f4 h\u00ecnh th\u1ee9 2-Naive Bayes. M\u00f4 h\u00ecnh Bayes s\u1eed d\u1ee5ng c\u00e1c x\u00e1c su\u1ea5t tr\u01b0\u1edbc \u0111\u1ec3 d\u1ef1 \u0111o\u00e1n x\u00e1c su\u1ea5t sau r\u1ea5t h\u1eefu \u00edch cho vi\u1ec7c ph\u00e2n lo\u1ea1i v\u1edbi c\u00e1c t\u00ednh n\u0103ng r\u1eddi r\u1ea1c nh\u01b0 ph\u00e2n lo\u1ea1i v\u0103n b\u1ea3n.***\n___\n    \n   # M\u00f4 h\u00ecnh th\u1ee9 hai: Bayes Th\u01a1 Ng\u00e2y(Multinomial Naive Bayes-MNB)","556751d7":"# GOOD BYE (^_^) Thanks you very much, Thanks come from BAO BI!\n\n**Class A1, Computer Science, Can Tho University, Vietnam**\n\n**FB:** https:\/\/www.facebook.com\/baobi1998\n\n**Kaggle:** https:\/\/www.kaggle.com\/lienchibaob1812254\n\n**github:** https:\/\/github.com\/lienchibao1998","cd0cfee2":"# **TH\u01af VI\u1ec6N C\u1ea6N THI\u1ebeT**","6f622556":"# **GI\u1edaI THI\u1ec6U**\n\n* Xin ch\u00e0o! H\u00f4m nay t\u00f4i s\u1ebd ph\u1ea3i ho\u00e0n th\u00e0nh m\u1ed9t nhi\u1ec7m v\u1ee5 kh\u00e1 th\u00fa v\u1ecb \u0111\u00f3 l\u00e0 **\"Ph\u00e2n t\u00edch \u0111\u00e1nh gi\u00e1 c\u1ea3m x\u00fac ng\u01b0\u1eddi xem qua b\u1ed9 d\u1eef li\u1ec7u 50K IMDB** b\u1eb1ng t\u1ea5t c\u1ea3 c\u00e1c **ph\u01b0\u01a1ng ph\u00e1p h\u1ecdc m\u00e1y(g\u1ed3m c\u00f3: LSVC,Bayes Th\u01a1 Ng\u00e2y,SGDC,LR,LSTM) v\u00e0 h\u1ecdc s\u00e2u(CNN)** m\u00e0 t\u00f4i bi\u1ebft. Vui l\u00f2ng cho \u00fd ki\u1ebfn kh\u00e1ch quan \u0111\u1ec3 t\u1ea5t c\u1ea3 ch\u00fang ta c\u00f3 th\u1ec3 trao \u0111\u1ed5i v\u1edbi nhau t\u00edch c\u1ef1c h\u01a1n v\u00e0 hi\u1ec7u qu\u1ea3 h\u01a1n. C\u1ea3m \u01a1n c\u00e1c b\u1ea1n \u0111\u00e3 xem!\n\n* Tr\u01b0\u1edbc khi b\u1eaft \u0111\u1ea7u th\u00ec t\u00f4i s\u1ebd cho b\u1ea1n **bi\u1ebft tr\u01b0\u1edbc k\u1ebft qu\u1ea3 thu \u0111\u01b0\u1ee3c** s\u1ebd ra sao \u0111\u1ec3 ch\u00fang ta c\u00f3 th\u1ec3 d\u1ec5 d\u00e0ng r\u00e0 so\u00e1t ch\u00fang m\u1ed9t c\u00e1ch d\u1ec5 d\u00e0ng v\u00e0 hi\u1ec7u qu\u1ea3.**(\u1edf \u0111\u00e2y t\u00f4i ch\u1ec9 tr\u00ecnh b\u00e0y k\u1ebft qu\u1ea3 \u0111\u1ea1t \u0111\u01b0\u1ee3c v\u1ec1 \u0111\u1ed9 ch\u00ednh x\u00e1c c\u1ee7a t\u1eebng m\u00f4 h\u00ecnh, c\u00e1c b\u1ea1n c\u1ea3m th\u1ea5y hay v\u00e0 h\u1ee9ng th\u00fa c\u00f3 th\u1ec3 xem t\u1eebng chi ti\u1ebft code & ch\u00fa th\u00edch m\u00e0 t\u00f4i \u0111\u1ec3 b\u00ean d\u01b0\u1edbi).**\n\n* B\u1ea1n c\u00f3 th\u1ec3 xem s\u1ed1 li\u1ec7u **\"so s\u00e1nh b\u1ea3ng m\u00f4 h\u00ecnh\" t\u1ea1i d\u00f2ng \"Nh\u1eadn x\u00e9t v\u1ec1 k\u1ebft qu\u1ea3 thu \u0111\u01b0\u1ee3c t\u1eeb 4 m\u00f4 h\u00ecnh\" b\u00ean d\u01b0\u1edbi**, t\u00f4i \u0111\u00e3 n\u00e9n l\u1ea1i d\u01b0\u1edbi d\u1ea1ng dataFrame cho b\u1ea1n d\u1ec5 nh\u00ecn & so s\u00e1nh kh\u00e1ch quan.\n\n* ***\u0110\u01b0\u1ee3c r\u1ed3i! ch\u00fang ta kh\u00f4ng v\u00f2ng vo n\u1eefa, v\u00e0o vi\u1ec7c n\u00e0o!Ch\u00fac b\u1ea1n 1 ng\u00e0y t\u1ed1t l\u00e0nh...LOVE***","9f857cc6":"**C\u00f3 th\u1ec3 th\u1ea5y r\u1eb1ng trong kho ch\u00fang ta c\u00f3 81301 d\u1eef li\u1ec7u... V\u00ec v\u1eady, t\u00f4i \u0111\u1ec1 xu\u1ea5t d\u00f9ng Chi-Square(Chi Ph\u00e2n ph\u1ed1i b\u00ecnh ph\u01b0\u01a1ng) khi d\u1eef li\u1ec7u c\u1ee7a ch\u00fang ta \u1edf d\u1ea1ng \u0111\u1ebfm t\u1ea7n su\u1ea5t v\u00e0 \u0111i\u1ec1u ch\u00fang ta quan t\u00e2m l\u00e0 s\u1ed1 l\u01b0\u1ee3ng \u0111\u1ed1i t\u01b0\u1ee3ng thu\u1ed9c c\u00e1c lo\u1ea1i kh\u00e1c nhau, Chi-Square s\u1ebd gi\u00fap ch\u00fang ta th\u1ef1c hi\u1ec7n vi\u1ec7c ki\u1ec3m tra g\u1ea7n \u0111\u00fang v\u1ec1 m\u1eb7t \u00fd ngh\u0129a \u0111\u1ed1i v\u1edbi s\u1ef1 li\u00ean k\u1ebft(association) gi\u1eefa hai bi\u1ebfn l\u1eadp nh\u00f3m (categorical variables). B\u1ea1n c\u00f3 th\u1ec3 l\u1ea5y bao nhi\u00eau features c\u0169ng \u0111\u01b0\u1ee3c(\u1edf \u0111\u00e2y t\u00f4i s\u1ebd l\u1ea5y ng\u1eabu nhi\u00ean 50000 features)**\n\nC\u00e1c b\u1ea1n c\u00f3 th\u1ec3 tham kh\u1ea3o Chi-Square t\u1ea1i: https:\/\/www.statisticssolutions.com\/free-resources\/directory-of-statistical-analyses\/using-chi-square-statistic-in-research\/","dca48ef0":"**\u0110\u1ed9 ch\u00ednh x\u00e1c khi s\u1eed d\u1ee5ng l\u1edbp RNN b\u1ed5 sung th\u00eam LSTM(2 l\u1edbp RNN + LSTM) + propout  l\u00e0 83%.**","a2ba97b1":"# **M\u00f4 h\u00ecnh h\u1ecdc S\u00e2u (Deep Learning Models)**\n\n***\u1ede m\u00f4 h\u00ecnh h\u1ecdc s\u00e2u th\u00ec n\u00f3 h\u01a1i kh\u00e1c so v\u1edbi c\u00e1c m\u00f4 h\u00ecnh m\u00e1y h\u1ecdc... Nh\u01b0ng khi t\u00ecm hi\u1ec3u th\u00ec m\u00ecnh th\u1ea5y c\u00f3 r\u1ea5t nhi\u1ec1u th\u1ee9 hay m\u00e0 ch\u00fang ta c\u00f3 th\u1ec3 kh\u00e1m ph\u00e1 khi d\u1ef1 \u0111o\u00e1n cho t\u1eadp d\u1eef li\u1ec7u ch\u00fang ta \u0111ang x\u00e9t...Come on! c\u00f9ng \u0111i v\u1edbi t\u00f4i n\u00e0o! GO!***","d7dd962d":"**\u0110\u1ed9 ch\u00ednh x\u00e1c c\u1ee7a vi\u1ec7c s\u1eed d\u1ee5ng CountVectorizer (binary = False) fit v\u1edbi m\u00f4 h\u00ecnh Linear SVC: 89.77%**\n\n***B\u00e2y gi\u1edd t\u00f4i s\u1ebd vect\u01a1 h\u00f3a b\u1eb1ng c\u00e1ch s\u1eed d\u1ee5ng CountVectorizer (binary = True) v\u00e0 fit v\u1edbi m\u00f4 h\u00ecnh LinearSVC \u0111\u1ec3 xem k\u1ebft qu\u1ea3 so v\u1edbi (binary = false) s\u1ebd th\u1ebf n\u00e0o?***","f6599b12":"# **\u0110\u1eccC T\u1eacP D\u1eee LI\u1ec6U V\u00c0 HI\u1ec2N TH\u1eca T\u1eacP D\u1eee LI\u1ec6U**","0c7e0afd":"**\u0110\u1ed9 ch\u00ednh x\u00e1c c\u1ee7a vi\u1ec7c s\u1eed d\u1ee5ng CountVectorizer (binary=False) fit v\u1edbi m\u00f4 h\u00ecnh h\u1ed3i quy logistic: 85.62%**","4c856488":"**L\u1eafp m\u00f4 h\u00ecnh v\u1edbi b\u1ed9 train \u0111\u1ec3 training v\u00e0 t\u00f4i s\u1eed d\u1ee5ng 30 Epochs \u0111\u1ec3 t\u00ecm \u0111\u1ed9 ch\u00ednh x\u00e1c t\u1ed1i \u01b0u nh\u1ea5t c\u00f3 th\u1ec3.Ok! \u0110\u1ee3i t\u00ed xem k\u1ebft qu\u1ea3 n\u00e0o!**","a7e84890":"   # T\u1ea1o tr\u00ecnh t\u1ea1o \u0111\u00e1nh gi\u00e1 ng\u1eabu nhi\u00ean\n\n* **T\u1ea1o m\u1ed9t bi\u1ebfn cho b\u00e0i \u0111\u00e1nh gi\u00e1 phim \u0111\u01b0\u1ee3c t\u1ea1o ng\u1eabu nhi\u00ean**\n* **in b\u00e0i \u0111\u00e1nh gi\u00e1 ng\u1eabu nhi\u00ean**","0f88ae1f":"   # Convolutional Neural Network with dropout","449ddd47":"**\u0110\u1ed9 ch\u00ednh x\u00e1c c\u1ee7a vi\u1ec7c s\u1eed d\u1ee5ng CountVectorizer (binary = True) fit Multinomial Naive Bayes: 89.10%**\n___","0a10e798":"   # M\u00f4 h\u00ecnh th\u1ee9 t\u01b0: H\u1ed3i Quy Logistic (Logistic regression)\n\n   ***C\u0169ng nh\u01b0 m\u00f4 h\u00ecnh th\u1ee9 ba, t\u00f4i s\u1ebd code nhanh ph\u1ea7n n\u00e0y***","5b04bb99":"# **Ti\u1ec1n x\u1eed l\u00ed d\u1eef li\u1ec7u cho t\u1eadp d\u1eef li\u1ec7u 50K**\n**Ok! M\u1ecdi th\u1ee9 g\u1ea7n \u1ed5n! Gi\u1edd \u0111\u1ebfn l\u00fac t\u00f4i s\u1ebd cho l\u00e0m s\u1ea1ch to\u00e0n t\u1eadp d\u1eef li\u1ec7u... T\u00f4i s\u1ebd chia l\u00e0m 2 ph\u1ea7n.. Ph\u1ea7n train v\u00e0 ph\u1ea7n test \u0111\u1ec3 l\u00e0m s\u1ea1ch... (^_^)vui l\u00f2ng ch\u1edd v\u00ec d\u1eef li\u1ec7u l\u1edbn s\u1ebd m\u1ea5t nhi\u1ec1u th\u1eddi gian c\u1ee7a b\u1ea1n**","d8bf4efa":"   # M\u00f4 h\u00ecnh h\u1ecdc s\u00e2u \u0111\u01a1n gi\u1ea3n\n* ***\u0110\u1ec3 kh\u00f4ng b\u1ecb r\u1ed1i th\u00ec t\u00f4i s\u1ebd l\u00e0m ph\u1ea7n \u0111\u01a1n gi\u1ea3n cho b\u1ea1n d\u1ec5 h\u00ecnh dung m\u00f4 h\u00ecnh h\u1ecdc s\u00e2u \u0111\u01b0\u1ee3c thi\u1ebft k\u1ebf th\u1ebf n\u00e0o? ok!***\n* ***T\u1ea1o m\u1ed9t m\u1ea1ng n\u01a1-ron h\u1ecdc s\u00e2u \u0111\u01a1n gi\u1ea3n. Sau \u0111\u00f3 t\u1ea1o m\u1ed9t l\u1edbp nh\u00fang(class embed) b\u1eb1ng c\u00e1ch ch\u1ec9 \u0111\u1ecbnh c\u00e1c tham s\u1ed1 m\u00e0 ch\u00fang t\u00f4i \u0111\u00e3 t\u1ea1o tr\u01b0\u1edbc \u0111\u00f3 v\u00e0 th\u00eam(add) n\u00f3 v\u00e0o m\u00f4 h\u00ecnh(model)***\n* ***Flatten l\u1edbp nh\u00fang l\u1ea1i v\u00ec t\u00f4i \u0111ang k\u1ebft n\u1ed1i th\u1eb3ng l\u1edbp nh\u00fang v\u1edbi l\u1edbp Dense(Dense layer)***\n* ***Cu\u1ed1i c\u00f9ng, t\u00f4i th\u00eam m\u1ed9t l\u1edbp dense v\u1edbi ch\u1ee9c n\u0103ng k\u00edch ho\u1ea1t sigmoid.***\n* ***T\u00f4i bi\u1ebft b\u1ea1n \u0111ang t\u1ef1 h\u1ecfi Sigmoid v\u00e0 Dense l\u00e0 g\u00ec? Ch\u00fang c\u00f3 quan h\u1ec7 g\u00ec?***\n\n*     B\u1ea1n c\u00f3 th\u1ec3 tham kh\u1ea3o t\u1ea1i \u0111\u00e2y: https:\/\/vn.got-it.ai\/blog\/tim-hieu-sigmoid-function-va-lich-su-hinh-thanh-cua-no\n*     B\u1ea1n c\u00f3 th\u1ec3 tham kh\u1ea3o t\u1ea1i \u0111\u00e2y: https:\/\/trituenhantao.io\/tu-dien-thuat-ngu\/dense-layer\/","908e6d96":"**Ok! Gi\u1edd t\u00f4i s\u1ebd fit n\u00f3 v\u1edbi m\u00f4 h\u00ecnh Multinomial Naive Bayes.**","720d2984":"**\u0110\u1ed9 ch\u00ednh x\u00e1c c\u1ee7a vi\u1ec7c s\u1eed d\u1ee5ng TF-IDF fit v\u1edbi m\u00f4 h\u00ecnh Multinomial Naive Bayes: 86.63%**\n\n**Ph\u00e2n ph\u1ed1i \u0111a th\u1ee9c th\u01b0\u1eddng y\u00eau c\u1ea7u s\u1ed1 feature s\u1ed1 nguy\u00ean. Tuy nhi\u00ean, trong th\u1ef1c t\u1ebf, c\u00e1c ph\u00e9p \u0111\u1ebfm ph\u00e2n s\u1ed1 nh\u01b0 tf-idf c\u0169ng c\u00f3 th\u1ec3 ho\u1ea1t \u0111\u1ed9ng. B\u00e2y gi\u1edd t\u00f4i s\u1ebd s\u1eed d\u1ee5ng CountVectorizer l\u00e0m m\u00f4 h\u00ecnh Bag-of-Words tr\u01b0\u1edbc khi \u00e1p d\u1ee5ng m\u00f4 h\u00ecnh MultinomialNB \u0111\u1ec3 xem th\u1ebf n\u00e0o nha.**\n\n***T\u00f4i s\u1ebd vect\u01a1 h\u00f3a b\u1eb1ng c\u00e1ch s\u1eed d\u1ee5ng CountVectorizer (binary = False) v\u00e0 fit v\u1edbi m\u00f4 h\u00ecnh LinearSVC \u0111\u1ec3 ki\u1ec3m tra xem th\u1ebf n\u00e0o?***","f4f02fac":"***Ho\u00e0n h\u1ea3o! B\u1ea1n th\u1ea5y g\u00ec kh\u00f4ng? d\u00f2ng th\u1ee9 3 tr\u00f4ng Dataset \u0111\u00e3 \u0111\u01b0\u1ee3c x\u1eed l\u00ed s\u1ea1ch s\u1ebd...Ok! M\u1ecdi th\u1ee9 c\u00f3 v\u1ebb \u1ed5n! Ch\u00fang ta s\u1ebd \u0111\u1ebfn ph\u1ea7n Vecto h\u00f3a v\u0103n b\u1ea3n***\n___","56abc1c3":"***\u0110\u1ed9 ch\u00ednh x\u00e1c c\u1ee7a t\u1eadp d\u1eef li\u1ec7u hu\u1ea5n luy\u1ec7n v\u00e0 ki\u1ec3m tra g\u1ea7n nhau, v\u00ec v\u1eady m\u00f4 h\u00ecnh RNN (LSTM) ho\u1ea1t \u0111\u1ed9ng t\u1ed1t tr\u00ean d\u1eef li\u1ec7u m\u1edbi.**","6dd572a5":"# **TRI\u1ec2N KHAI M\u00d4 H\u00ccNH**\n**Xong! M\u1ecdi th\u1ee9 \u0111\u00e3 \u1ed5n...Gi\u1edd l\u00e0 l\u00fac t\u00f4i x\u00e2y d\u1ef1ng m\u00f4 h\u00ecnh t\u1eebng m\u00f4 h\u00ecnh cho t\u1eadp d\u1eef li\u1ec7u v\u00e0 \u0111\u01b0a ra m\u00f4 h\u00ecnh c\u00f3 \u0111\u1ed9 ch\u00ednh x\u00e1c cao nh\u1ea5t.**\n___\n\n   # T\u00f4i s\u1ebd b\u1eaft \u0111\u1ea7u v\u1edbi m\u00f4 h\u00ecnh Linear Support Vector Classification(LSVC)","3283f12b":"**K\u1ebf ti\u1ebfp, t\u00f4i s\u1ebd t\u1ea1o ra m\u1ed9t ma tr\u1eadn nh\u00fang(matrix embedding). \u1ede \u0111\u00e2y, m\u1ed7i s\u1ed1 h\u00e0ng s\u1ebd t\u01b0\u01a1ng \u1ee9ng v\u1edbi ch\u1ec9 m\u1ee5c c\u1ee7a t\u1eeb trong kho ng\u1eef li\u1ec7u. Ma tr\u1eadn s\u1ebd ch\u1ee9a c\u00e1c embedding t\u1eeb GloVe cho c\u00e1c t\u1eeb(word) trong kho d\u1eef li\u1ec7u c\u1ee7a t\u00f4i c\u00f3 \u0111\u01b0\u1ee3c.Ok!**","66420ffe":"***\u0110\u1ed9 ch\u00ednh x\u00e1c tr\u00ean t\u1eadp d\u1eef li\u1ec7u hu\u1ea5n luy\u1ec7n cao h\u01a1n so v\u1edbi t\u1eadp d\u1eef li\u1ec7u th\u1eed nghi\u1ec7m n\u00ean m\u1ea1ng n\u01a1-ron s\u00e2u kh\u00f4ng t\u1ed1t cho c\u00e1c d\u1ef1 \u0111o\u00e1n.**","78a99835":"**Ok! \u0110\u01b0\u1ee3c r\u1ed3i! Ti\u1ebfp \u0111\u1ebfn ch\u00fang ta s\u1ebd ti\u1ebfp c\u1eadn s\u1eed d\u1ee5ng TfidfVectorizer (TF-IDF) & T\u00ednh (TF * IDF) c\u1ee7a m\u00f4 h\u00ecnh t\u00fai t\u1eeb**","5c374f0f":"***Ok! Tr\u01b0\u1edbc khi d\u1ef1 \u0111o\u00e1n m\u00f4 h\u00ecnh ch\u00fang ta ph\u1ea3i l\u00e0m s\u1ea1ch d\u1eef li\u1ec7u (ti\u1ec1n x\u1eed l\u00ed d\u1eef li\u1ec7u). \u0110\u1ec3 qu\u00e1 tr\u00ecnh l\u00e0m s\u1ea1ch d\u1eef li\u1ec7u di\u1ec5n ra hi\u1ec7u qu\u1ea3 v\u00e0 quy m\u00f4 v\u1edbi kh\u1ed1i l\u01b0\u1ee3ng d\u1eef li\u1ec7u l\u1edbn, ch\u00fang ta c\u1ea7n x\u00e2y d\u1ef1ng h\u00e0m l\u00e0m s\u1ea1ch d\u1eef li\u1ec7u ng\u1eabu nhi\u00ean 1 c\u00e2u trong t\u1eadp d\u1eef li\u1ec7u. \u0110\u01b0\u1ee3c r\u1ed3i! t\u1edbi \u0111\u00e2y ch\u1eafc b\u1ea1n \u0111\u00e3 hi\u1ec3u r\u1ed3i. T\u00f4i s\u1ebd d\u00f9ng d\u00f2ng th\u1ee9 3 trong t\u1eadp d\u1eef li\u1ec7u...loc[2]***","1944b717":"***Ok! Nh\u01b0 c\u00e1c b\u1ea1n th\u1ea5y sau khi \u0111\u1ecdc d\u1eef li\u1ec7u ch\u00fang ta c\u00f3 50k c\u1ea3m x\u00fac, trong \u0111\u00f3 c\u00f3 25k c\u1ea3m x\u00fac t\u00edch c\u1ef1c (Positive) v\u00e0 25k c\u1ea3m x\u00fac ti\u00eau c\u1ef1c(Negative). Nh\u01b0ng \u0111\u00e2y ch\u1ec9 l\u00e0 d\u1eef li\u1ec7u th\u00f4 ch\u00fang ta c\u00f3, vi\u1ec7c ch\u00fang ta c\u1ea7n l\u00e0m s\u1ebd d\u1ef1 \u0111o\u00e1n l\u1ea1i c\u00e1c c\u1ea3m x\u00fac (sentiment) \u0111\u1ec3 c\u00f3 c\u00e1i nh\u00ecn kh\u00e1ch quan h\u01a1n v\u00e0 xem m\u00f4 h\u00ecnh n\u00e0o s\u1ebd l\u00e0m \u0111\u1ed9 ch\u00ednh x\u00e1c c\u1ee7a d\u1eef li\u1ec7u \u0111\u01b0\u1ee3c d\u1ef1 \u0111o\u00e1n l\u00e0 cao nh\u1ea5t.***","627574c1":"# Word cloud for positive review words","7b41d86f":"# T\u1ea1o l\u1edbp nh\u00fang (Creation the embedding layer)\n***Nh\u01b0 \u0111\u00e3 tr\u00ecnh b\u00e0y, t\u00f4i c\u1ea7n m\u1ed9p l\u1edbp nh\u00fang chuy\u1ec3n \u0111\u1ed5i d\u1eef li\u1ec7u v\u0103n b\u1ea3n c\u1ee7a t\u00f4i th\u00e0nh d\u1eef li\u1ec7u s\u1ed1. N\u00f3 \u0111\u01b0\u1ee3c s\u1eed d\u1ee5ng l\u00e0m l\u1edbp \u0111\u1ea7u ti\u00ean cho c\u00e1c m\u00f4 h\u00ecnh h\u1ecdc s\u00e2u trong keras. \u0110\u00e2y l\u00e0 b\u01b0\u1edbc \u0111\u1ea7u ti\u00ean \u0111\u1ec3 t\u00f4i c\u00f3 th\u1ec3 ti\u1ebfn t\u1edbi m\u00f4 h\u00ecnh h\u1ecdc s\u00e2u... N\u00f3 c\u0169ng kh\u00e1 nhanh v\u00e0 kh\u00f4ng m\u1ea5t nhi\u1ec1u th\u1eddi gian c\u1ee7a b\u1ea1n \u0111\u00e2u...c\u00f9ng \u0111i nh\u00e9!ok!Go!!!***","057b5fcc":"**T\u00f4i s\u1ebd \u0111\u1eb7t k\u00edch th\u01b0\u1edbc t\u1ed1i \u0111a c\u1ee7a m\u1ed7i danh s\u00e1ch l\u00e0 100. N\u1ebfu danh s\u00e1ch n\u00e0o c\u00f3 maxlength > 100 s\u1ebd b\u1ecb c\u1eaft b\u1edbt v\u00e0 danh s\u00e1ch n\u00e0o c\u00f3 maxlength < 100 s\u1ebd \u0111\u01b0\u1ee3c th\u00eam 0 cho \u0111\u1ebfn khi c\u00f3 \u0111\u1ed9 d\u00e0i t\u1ed1i \u0111a = maxlength.D\u1ec5 hi\u1ec3u \u0111\u00fang kh\u00f4ng?**\n**Qu\u00e1 tr\u00ecnh n\u00e0y \u0111\u01b0\u1ee3c g\u1ecdi l\u00e0 padding. \u0110o\u1ea1n code d\u01b0\u1edbi \u0111\u00e2y s\u1ebd t\u00ecm k\u00edch th\u01b0\u1edbc vocabulary v\u00e0 sau \u0111\u00f3 l\u00e0 perfom padding tr\u00ean c\u1ea3 X_train v\u00e0 X_test , tokenizer.word_index**","a6940324":"   # Bi\u1ec3u di\u1ec5n k\u1ebft qu\u1ea3 d\u1ef1 \u0111o\u00e1n t\u1eadp d\u1eef li\u1ec7u IMDB 50K","679a9a0d":"**\u0110\u1ed9 ch\u00ednh x\u00e1c c\u1ee7a vi\u1ec7c s\u1eed d\u1ee5ng CountVectorizer (binary = False) fit Multinomial Naive Bayes: 88.75%**","5a10da3c":"**\u0110\u1ed9 ch\u00ednh x\u00e1c c\u1ee7a vi\u1ec7c s\u1eed d\u1ee5ng CountVectorizer (binary = True) fit SGD Classifier: 89.46%**\n___","22a9de50":"# Chia t\u1eadp d\u1eef li\u1ec7u\n**T\u00f4i s\u1ebd chia t\u1eadp d\u1eef li\u1ec7u \u0111\u1ec3 hu\u1ea5n lu\u1eadn m\u00f4 h\u00ecnh v\u1edbi 75% d\u1eef li\u1ec7u l\u00e0 traning, 25% l\u00e0 test, random_state = 42 (ch\u00fang ta c\u00f3 th\u1ec3 l\u1ea5y random t\u00f9y \u00fd) & sau \u0111\u00f3, t\u00f4i c\u1ea7n thay th\u1ebf \"review\" v\u00e0 \"sentiment\" v\u1ec1 d\u1ea1ng 2 ki\u1ec3u s\u1ed1 l\u00e0 \"Positive: 1\" v\u00e0 \"Nagative: 0 \u0111\u1ec3 fit v\u1edbi m\u00f4 h\u00ecnh\"**","05a9efb5":"   # M\u00f4 h\u00ecnh th\u1ee9 ba: Stochastic Gradient Descent Classifier(SGDC)\n**Ok, C\u00d3 th\u1ec3 b\u1ea1n \u0111ang b\u1ecb r\u1ed1i x\u00edu... \u1ede m\u00f4 h\u00ecnh th\u1ee9 3 n\u00e0y,\u0111\u1ec3 ti\u1ebft ki\u1ec7m th\u1eddi gian cho b\u1ea1n th\u00ec t\u00f4i s\u1ebd g\u01a1m g\u1ecdn code l\u1ea1i v\u00e0 \u0111i nhanh ph\u1ea7n TfidfVectorizerv\u00e0 CountVectorizer b\u1eb1ng c\u00e1ch s\u1eed d\u1ee5ng \"binary = True\" v\u00e0 \"binary = False\" lu\u00f4n... Ok ch\u1ee9! B\u1eaft \u0111\u1ea7u n\u00e0o!**\n\n**CH\u00da \u00dd: T\u1eeb m\u00f4 h\u00ecnh s\u1ed1 3 n\u00e0y tr\u1edf v\u1ec1 sau, TF-IDF t\u00f4i s\u1ebd s\u1eed d\u1ee5ng m\u1eb7c \u0111\u1ecbnh c\u00e0i s\u1eb5n \u1edf tr\u00ean v\u00e0 code nhanh, t\u00f4i ch\u1ec9 l\u1eb7p l\u1ea1i ph\u1ea7n countvectorier cho moi ng\u01b0\u1eddi d\u1ec5 ph\u00e2n bi\u1ec7t...Ok!**\n\nB\u1ea1n c\u00f3 th\u1ec3 \u0111\u1ecdc th\u00ean t\u1ea1i \u0111\u00e2y:https:\/\/scikit-learn.org\/stable\/modules\/sgd.html","4275f25f":"* **K\u1ebft qu\u1ea3 \u0111\u00e3 ch\u1ec9 ra r\u1eb1ng t\u00f4i c\u00f3 109137 t\u1eeb duy nh\u1ea5t trong kho v\u00e0 t\u00f4i s\u1ebd t\u1ea1o m\u1ea1ng noron \u0111\u01b0\u1ee3c thi\u1ebft k\u1ebf \u0111\u1ec3 t\u1eeb d\u1eef li\u1ec7u s\u1ed1.**\n\n* **Nh\u00fang t\u1eeb(Word Embedding) s\u1ebd \u0111\u1ea3m nhi\u1ec7m vi\u1ec7c c\u1ea3i thi\u1ec7n kh\u1ea3 n\u0103ng m\u1ea1ng noron h\u1ecdc t\u1eeb d\u1eef li\u1ec7u v\u0103n b\u1ea3n v\u00e0 c\u00e1c vect\u01a1 \u0111\u01b0\u1ee3c g\u1ecdi chung l\u00e0 Embeddings.**\n\n* **Ti\u1ebfp theo t\u00f4i s\u1ebd t\u1ea1o ra ma tr\u1eadn feature m\u00e0 c\u1ee5 th\u1ec3 c\u00f4ng c\u1ee5 t\u00f4i s\u1ebd s\u1eed d\u1ee5ng GloVe Embeddings. Theo \u0111\u1ecbnh ngh\u0129a, GloVe l\u00e0 vi\u1ebft t\u1eaft c\u1ee7a Global Vectors d\u00f9ng \u0111\u1ec3 bi\u1ec3u di\u1ec5n t\u1eeb. \u0110\u00e2y l\u00e0 m\u1ed9t thu\u1eadt to\u00e1n h\u1ecdc kh\u00f4ng gi\u00e1m s\u00e1t \u0111\u01b0\u1ee3c ph\u00e1t tri\u1ec3n b\u1edfi Stanford cho vi\u1ec7c t\u1ea1o nh\u00fang t\u1eeb b\u1eb1ng c\u00e1ch t\u1ed5ng h\u1ee3p ma tr\u1eadn \u0111\u1ed3ng xu\u1ea5t hi\u1ec7n c\u00e1c word-word to\u00e0n c\u1ee5c t\u1eeb m\u1ed9t kho ng\u1eef li\u1ec7u m\u00e0 ch\u00fang ta thu \u0111\u01b0\u1ee3c.**\n\n* **K\u1ebft qu\u1ea3 nh\u00fang \u0111\u01b0\u1ee3c hi\u1ec3n th\u1ecb trong c\u00e1c c\u1ea5u tr\u00fac tuy\u1ebfn t\u00ednh th\u00fa v\u1ecb c\u1ee7a t\u1eeb trong kh\u00f4ng gian vect\u01a1. N\u00f3i c\u00e1ch kh\u00e1c,n\u1ebfu hai t\u1eeb c\u00f9ng xu\u1ea5t hi\u1ec7n nhi\u1ec1u l\u1ea7n, \u0111i\u1ec1u \u0111\u00f3 c\u00f3 ngh\u0129a l\u00e0 ch\u00fang c\u00f3 m\u1ed9t s\u1ed1 \u0111i\u1ec3m t\u01b0\u01a1ng \u0111\u1ed3ng v\u1ec1 ng\u00f4n ng\u1eef ho\u1eb7c ng\u1eef ngh\u0129a. Ok ch\u1ee9!**\n* **GloVe word Embedding s\u1ebd t\u1ea1o m\u1ed9t t\u1eeb \u0111i\u1ec3n v\u00e0 s\u1ebd ch\u1ee9a c\u00e1c t\u1eeb l\u00e0m kh\u00f3a(keys) v\u00e0 danh s\u00e1ch nh\u00fang t\u01b0\u01a1ng \u1ee9ng c\u1ee7a ch\u00fang d\u01b0\u1edbi d\u1ea1ng gi\u00e1 tr\u1ecb(values)**\n\n    * B\u1ea1n c\u00f3 th\u1ec3 tham kh\u1ea3o th\u00eam t\u1ea1i \u0111\u00e2y: https:\/\/trituenhantao.io\/kien-thuc\/huong-dan-su-dung-glove\/\n    * C\u00e1c b\u1ea1n c\u00f3 th\u1ec3 tham kh\u1ea3o t\u1ea1i \u0111\u00e2y: https:\/\/ichi.pro\/vi\/nhung-tu-la-gi-71370589666818","fe1f8b1f":"# **C\u00c0I \u0110\u1eb6T**","2a51fc0e":"**\u0110\u1ed9 ch\u00ednh x\u00e1c c\u1ee7a vi\u1ec7c s\u1eed d\u1ee5ng TF-IDF fit v\u1edbi m\u00f4 h\u00ecnh Linear SVC: \n90.29%**\n\n**Khoan! T\u00f4i s\u1ebd vect\u01a1 h\u00f3a b\u1eb1ng c\u00e1ch s\u1eed d\u1ee5ng CountVectorizer (binary = False) v\u00e0 fit v\u1edbi m\u00f4 h\u00ecnh LinearSVC \u0111\u1ec3 ki\u1ec3m tra xem th\u1ebf n\u00e0o?**","6abb1347":"**B\u00e2y gi\u1edd t\u00f4i s\u1ebd vect\u01a1 h\u00f3a b\u1eb1ng c\u00e1ch s\u1eed d\u1ee5ng CountVectorizer (binary = True) v\u00e0 fit v\u1edbi m\u00f4 h\u00ecnh SGDC \u0111\u1ec3 xem k\u1ebft qu\u1ea3 so v\u1edbi (binary = True) s\u1ebd th\u1ebf n\u00e0o?**","7c8db3c3":"# Vector h\u00f3a v\u0103n b\u1ea3n\n   T\u00f4i s\u1ebd \u00e1p d\u1ee5ng 3 ph\u01b0\u01a1ng ph\u00e1p cho c\u00f4ng vi\u1ec7c n\u00e0y nh\u01b0 sau:\n\n*   **CountVectorizer (Bag of Words Model-M\u00f4 H\u00ecnh T\u00fai T\u1eeb)**\n*   **TfidfVectorizer (Bag of Words Model)**\n*   **Keras Tokenizer (Embedding)--> Ph\u1ea7n n\u00e0y t\u00f4i s\u1ebd n\u00f3i th\u00eam \u1ede M\u1ee4C NH\u00daNG T\u1eea**\n\n**CountVector & TF_IDF s\u1ebd l\u00e0 2 k\u0129 thu\u1eadt m\u00e0 t\u00f4i s\u1ebd c\u00e0i \u0111\u1eb7t,x\u1eed l\u00ed v\u00e0 ki\u1ec3m tra cho c\u00e1c m\u00f4 h\u00ecnh...**","78b1e801":"# **Nh\u1eadn x\u00e9t v\u1ec1 k\u1ebft qu\u1ea3 thu \u0111\u01b0\u1ee3c t\u1eeb 4 m\u00f4 h\u00ecnh:**\n\n**Ch\u00fang ta c\u00f3 th\u1ec3 th\u1ea5y r\u1eb1ng m\u00f4 h\u00ecnh LinearSVC(LSVC) c\u00f3 TF-IDF \u0111\u1ea1t \u0111\u1ed9 ch\u00ednh x\u00e1c cao nh\u1ea5t:90.29%.** V\u00ec v\u1eady, t\u00f4i s\u1ebd bi\u1ec3u di\u1ec5n cho b\u1ea1n th\u1ea5y v\u1ec1 k\u1ebft qu\u1ea3 d\u1ef1 \u0111o\u00e1n d\u1ef1 tr\u00ean t\u1eadp d\u1eef li\u1ec7u ch\u00fang ta c\u00f3 \u0111\u01b0\u1ee3c. Ok! \u0111i n\u00e0o","e7d081eb":"   # M\u1ea1ng N\u01a1-ron H\u1ed3i Quy V\u1edbi (2 RNN layer or 2 LSTM layer) + dropout\n   **T\u01af\u01a0NG T\u1ef0 nh\u01b0 tr\u00ean: Nh\u01b0ng \u1edf \u0111\u00e2y t\u00f4i th\u00eam v\u00e0o DROPOUT, ta \u0111\u01b0\u1ee3c:**\n* **Th\u1ee9 nh\u1ea5t, t\u00f4i x\u00e2y d\u1ef1ng c\u1ea5u tr\u00fac m\u00f4 h\u00ecnh v\u00e0 t\u1ea1o m\u1ed9t l\u1edbp nh\u00fang b\u1eb1ng c\u00e1ch ch\u1ec9 \u0111\u1ecbnh c\u00e1c tham s\u1ed1 m\u00e0 ch\u00fang ta \u0111\u00e3 t\u1ea1o tr\u01b0\u1edbc \u0111\u00f3.**\n* **Ti\u1ebfp theo,t\u1ea1o m\u1ea1ng n\u01a1-ron l\u1eb7p l\u1ea1i. \u1ede \u0111\u00e2y, t\u00f4i s\u1ebd s\u1eed d\u1ee5ng LTSM-Long Short Term Memory networks(M\u1ea1ng b\u1ed9 nh\u1edb d\u00e0i-ng\u1eafn).**\n* **Hai chi\u1ec1u c\u00f3 ngh\u0129a l\u00e0 tr\u00ecnh t\u1ef1 x\u1eed l\u00fd RNN t\u1eeb \u0111\u1ea7u \u0111\u1ebfn cu\u1ed1i v\u00e0 ng\u01b0\u1ee3c l\u1ea1i \u0110i\u1ec1u n\u00e0y s\u1ebd l\u00e0m cho m\u00f4 h\u00ecnh ho\u1ea1t \u0111\u1ed9ng t\u1ed1t h\u01a1n.\u00f4i \u0111\u00e3 th\u00eam m\u1ed9t l\u1edbp \u1ea9n kh\u00e1c v\u00e0 bao g\u1ed3m m\u1ed9t ch\u1ee9c n\u0103ng k\u00edch ho\u1ea1t l\u00e0 relu**\n* **th\u00eam Dropout \u0111\u1ec3 ng\u0103n m\u00f4 h\u00ecnh kh\u00f4ng v\u1eeba v\u1eb7n**\n* **lo\u1ea1i b\u1ecf ng\u1eabu nhi\u00ean m\u1ed9t s\u1ed1 t\u1ebf b\u00e0o th\u1ea7n kinh trong c\u00e1c l\u1edbp \u1ea9n**\n* **Cu\u1ed1i c\u00f9ng, t\u00f4i th\u00eam m\u1ed9t l\u1edbp dense v\u1edbi ch\u1ee9c n\u0103ng k\u00edch ho\u1ea1t sigmoid.**","fda7ebe7":"**\u0110\u1ed9 ch\u00ednh x\u00e1c c\u1ee7a vi\u1ec7c s\u1eed d\u1ee5ng CountVectorizer (binary = False) fit v\u1edbi m\u00f4 h\u00ecnh SGD Classifier: 85.84%**","a2bb53eb":"   # M\u1ea1ng n\u01a1-ron h\u1ed3i quy (RNN - Recurrent Neural Network) + LSTM\n   \n   **M\u1ea1ng n\u01a1-ron h\u1ed3i quy ho\u1ea1t \u0111\u1ed9ng t\u1ed1t tr\u00ean d\u1eef li\u1ec7u sequence. D\u1eef li\u1ec7u v\u0103n b\u1ea3n l\u00e0 m\u1ed9t chu\u1ed7i sequence data n\u00ean RNN l\u00e0 l\u1ef1a ch\u1ecdn t\u1ed1t \u0111\u1ec3 gi\u1ea3i quy\u1ebft c\u00e1c v\u1ea5n \u0111\u1ec1 li\u00ean quan \u0111\u1ebfn v\u0103n b\u1ea3n.**\n* **Th\u1ee9 nh\u1ea5t, t\u00f4i x\u00e2y d\u1ef1ng c\u1ea5u tr\u00fac m\u00f4 h\u00ecnh v\u00e0 t\u1ea1o m\u1ed9t l\u1edbp nh\u00fang b\u1eb1ng c\u00e1ch ch\u1ec9 \u0111\u1ecbnh c\u00e1c tham s\u1ed1 m\u00e0 ch\u00fang ta \u0111\u00e3 t\u1ea1o tr\u01b0\u1edbc \u0111\u00f3.**\n* **Ti\u1ebfp theo,t\u1ea1o m\u1ea1ng n\u01a1-ron l\u1eb7p l\u1ea1i. \u1ede \u0111\u00e2y, t\u00f4i s\u1ebd s\u1eed d\u1ee5ng LTSM-Long Short Term Memory networks(M\u1ea1ng b\u1ed9 nh\u1edb d\u00e0i-ng\u1eafn).**\n* **Hai chi\u1ec1u c\u00f3 ngh\u0129a l\u00e0 tr\u00ecnh t\u1ef1 x\u1eed l\u00fd RNN t\u1eeb \u0111\u1ea7u \u0111\u1ebfn cu\u1ed1i v\u00e0 ng\u01b0\u1ee3c l\u1ea1i \u0110i\u1ec1u n\u00e0y s\u1ebd l\u00e0m cho m\u00f4 h\u00ecnh ho\u1ea1t \u0111\u1ed9ng t\u1ed1t h\u01a1n.\u00f4i \u0111\u00e3 th\u00eam m\u1ed9t l\u1edbp \u1ea9n kh\u00e1c v\u00e0 bao g\u1ed3m m\u1ed9t ch\u1ee9c n\u0103ng k\u00edch ho\u1ea1t l\u00e0 relu.s**\n* **Cu\u1ed1i c\u00f9ng, t\u00f4i th\u00eam m\u1ed9t l\u1edbp dense v\u1edbi ch\u1ee9c n\u0103ng k\u00edch ho\u1ea1t sigmoid.**\n\n* B\u1ea1n c\u00f3 th\u1ec3 tham kh\u1ea3o t\u1ea1i \u0111\u00e2y:https:\/\/nttuan8.com\/bai-14-long-short-term-memory-lstm\/\n* B\u1ea1n c\u00f3 th\u1ec3 tham kh\u1ea3o t\u1ea1i \u0111\u00e2y: https:\/\/dominhhai.github.io\/vi\/2017\/10\/what-is-rnn\/","3d0d2453":"# K\u1ebft Lu\u1eadn\n* **C\u1ea3 5 gi\u1ea3i thu\u1eadt th\u00ec gi\u1ea3i thu\u1eadt hi\u1ec3u qu\u1ea3 nh\u1ea5t l\u00e0 Linear SVC v\u1edbi \u0111\u1ed9 ch\u00ednh x\u00e1c 90.29 %**\n* **T\u1ea5t c\u1ea3 c\u00e1c gi\u1ea3i thu\u1eadt c\u00f2n l\u1ea1i \u0111i\u1ec1u ho\u1ea1t \u0111\u1ed9ng t\u1ed1t v\u00e0 hi\u1ec7u qu\u1ea3 c\u0169ng kh\u00e1 cao**\n* **Vi\u1ec7c t\u1ed1i \u01b0u h\u00f3a m\u00f4 h\u00ecnh c\u00f3 th\u1ec3 mang l\u1ea1i k\u1ebft qu\u1ea3 t\u1ed1t h\u01a1n khi s\u1eed d\u1ee5ng nhi\u1ec1u d\u1eef li\u1ec7u h\u01a1n.**\n* **\u0110\u1ed9 ch\u00ednh x\u00e1c t\u1eebng m\u00f4 h\u00ecnh**\n    * **1.  LSVC:**\n        * **\u0110\u1ed9 ch\u00ednh x\u00e1c c\u1ee7a vi\u1ec7c s\u1eed d\u1ee5ng TF-IDF fit v\u1edbi m\u00f4 h\u00ecnh Linear SVC: 90.29%**\n        * \u0110\u1ed9 ch\u00ednh x\u00e1c c\u1ee7a vi\u1ec7c s\u1eed d\u1ee5ng CountVectorizer (binary = False) fit Linear SVC: 89.77%\n        * \u0110\u1ed9 ch\u00ednh x\u00e1c c\u1ee7a vi\u1ec7c s\u1eed d\u1ee5ng CountVectorizer (binary = False) fit Linear SVC: 89.46%\n\n    * **2.  MNB:**\n        * \u0110\u1ed9 ch\u00ednh x\u00e1c c\u1ee7a vi\u1ec7c s\u1eed d\u1ee5ng TF-IDF fit v\u1edbi m\u00f4 h\u00ecnh Multinomial Naive Bayes: 86.63%\n        * \u0110\u1ed9 ch\u00ednh x\u00e1c c\u1ee7a vi\u1ec7c s\u1eed d\u1ee5ng CountVectorizer (binary = False) fit Multinomial Naive Bayes: 88.75%\n        * \u0110\u1ed9 ch\u00ednh x\u00e1c c\u1ee7a vi\u1ec7c s\u1eed d\u1ee5ng CountVectorizer (binary = True) fit Multinomial Naive Bayes: 89.10%\n\n    * **3.  SGDC:**\n        * TF-IDF: \u0110\u1ed9 ch\u00ednh x\u00e1c c\u1ee7a SGD Classifier: 88.67%\n        * \u0110\u1ed9 ch\u00ednh x\u00e1c c\u1ee7a SGD Classifier binary = False: 85.84%\n        * \u0110\u1ed9 ch\u00ednh x\u00e1c c\u1ee7a vi\u1ec7c s\u1eed d\u1ee5ng CountVectorizer (binary = True) fit SGD Classifier: 85.84%\n\n    * **4.  LR:**\n        * \u0110\u1ed9 ch\u00ednh x\u00e1c c\u1ee7a vi\u1ec7c s\u1eed d\u1ee5ng TF-IDF fit v\u1edbi m\u00f4 h\u00ecnh h\u1ed3i quy logistic: 88.50%\n        * \u0110\u1ed9 ch\u00ednh x\u00e1c c\u1ee7a vi\u1ec7c s\u1eed d\u1ee5ng CountVectorizer fit v\u1edbi m\u00f4 h\u00ecnh h\u1ed3i quy logistic: 85.62%\n        * \u0110\u1ed9 ch\u00ednh x\u00e1c c\u1ee7a vi\u1ec7c s\u1eed d\u1ee5ng CountVectorizer (binary = True) fit v\u1edbi m\u00f4 h\u00ecnh h\u1ed3i quy logistic: 89.65%\n        \n* **Ngo\u00e0i ra,vi\u1ec7c s\u1eed d\u1ee5ng ph\u01b0\u01a1ng ph\u00e1p h\u1ecdc s\u00e2u th\u00ec:**\n        * \u0110\u1ed9 ch\u00ednh x\u00e1c khi s\u1eed d\u1ee5ng CNN tr\u00ean t\u1eadp d\u1eef li\u1ec7u l\u00e0 84.71%\n        * \u0110i\u1ec3m ch\u00ednh x\u00e1c khi s\u1eed d\u1ee5ng m\u1ea1ng n\u01a1-ron h\u1ecdc s\u00e2u \u0111\u01a1n gi\u1ea3n l\u00e0 71.80%%\n        * \u0110\u1ed9 ch\u00ednh x\u00e1c khi s\u1eed d\u1ee5ng l\u1edbp RNN b\u1ed5 sung th\u00eam LSTM(2 l\u1edbp RNN + LSTM) l\u00e0 83%.\n        * \u0110\u1ed9 ch\u00ednh x\u00e1c khi s\u1eed d\u1ee5ng 2 l\u1edbp RNN v\u1edbi LSTM l\u00e0 80.89%\n        * M\u00f4 h\u00ecnh CNN cho \u0111\u1ed9 ch\u00ednh x\u00e1c t\u1ed1t nh\u1ea5t trong s\u1ed1 t\u1ea5t c\u1ea3 v\u1edbi 84.71% tr\u00ean t\u1eadp d\u1eef li\u1ec7u train v\u00e0 test.\n        * S\u1eed d\u1ee5ng RNN (LSTM) v\u1edbi hai l\u1edbp \u1ea9n c\u0169ng l\u00e0 c\u00e1ch t\u1ed1t nh\u1ea5t mang l\u1ea1i \u0111\u1ed9 ch\u00ednh x\u00e1c 80.89% tr\u00ean c\u1ea3 t\u1eadp d\u1eef li\u1ec7u \u0111\u00e0o t\u1ea1o v\u00e0 ki\u1ec3m tra.\n     * **M\u00f4 h\u00ecnh M\u1ea1ng N\u01a1-ron hi\u1ec7u qu\u1ea3 \u0111\u1ec3 ph\u00e2n t\u00edch c\u1ea3m x\u00fac tr\u00ean c\u00e1c \u0111\u00e1nh gi\u00e1 IMDB**","79037390":"**=> \u0110\u00e1nh gi\u00e1 ng\u1eabu nhi\u00ean t\u00f4i t\u1ea1o ra cho k\u1ebft qu\u1ea3 0,99 g\u1ea7n b\u1eb1ng 1, c\u00f3 ngh\u0129a l\u00e0 \u0111\u00f3 l\u00e0 \u0111\u00e1nh gi\u00e1 T\u00cdCH C\u1ef0C(POSITIVE).**","17fcceec":"**\u0110\u1ed9 ch\u00ednh x\u00e1c c\u1ee7a vi\u1ec7c s\u1eed d\u1ee5ng TF-IDF fit v\u1edbi m\u00f4 h\u00ecnh h\u1ed3i quy logistic: 88.50%**","9a6fab47":"**K\u1ebeT QU\u1ea2 B\u00c1O TR\u01af\u1edaC(K\u1ebeT QU\u1ea2 IN \u0110\u1eacM L\u00c0 K\u1ebeT QU\u1ea2 KI\u1ec2M TRA C\u00d3 \u0110\u1ed8 CH\u00cdNH X\u00c1C CAO NH\u1ea4T)**\n1.  **LSVC:**\n    * **\u0110\u1ed9 ch\u00ednh x\u00e1c c\u1ee7a vi\u1ec7c s\u1eed d\u1ee5ng TF-IDF fit v\u1edbi m\u00f4 h\u00ecnh Linear SVC: 90.29%**\n    * \u0110\u1ed9 ch\u00ednh x\u00e1c c\u1ee7a vi\u1ec7c s\u1eed d\u1ee5ng CountVectorizer (binary = False) fit Linear SVC: 89.77%\n    * \u0110\u1ed9 ch\u00ednh x\u00e1c c\u1ee7a vi\u1ec7c s\u1eed d\u1ee5ng CountVectorizer (binary = False) fit Linear SVC: 89.46%\n\n2.  **MNB:**\n    * \u0110\u1ed9 ch\u00ednh x\u00e1c c\u1ee7a vi\u1ec7c s\u1eed d\u1ee5ng TF-IDF fit v\u1edbi m\u00f4 h\u00ecnh Multinomial Naive Bayes: 86.63%\n    * \u0110\u1ed9 ch\u00ednh x\u00e1c c\u1ee7a vi\u1ec7c s\u1eed d\u1ee5ng CountVectorizer (binary = False) fit Multinomial Naive Bayes: 88.75%\n    * \u0110\u1ed9 ch\u00ednh x\u00e1c c\u1ee7a vi\u1ec7c s\u1eed d\u1ee5ng CountVectorizer (binary = True) fit Multinomial Naive Bayes: 89.10%\n\n3.  **SGDC:** \n    * TF-IDF: \u0110\u1ed9 ch\u00ednh x\u00e1c c\u1ee7a SGD Classifier: 88.67%\n    * \u0110\u1ed9 ch\u00ednh x\u00e1c c\u1ee7a SGD Classifier binary = False: 85.84%\n    * \u0110\u1ed9 ch\u00ednh x\u00e1c c\u1ee7a vi\u1ec7c s\u1eed d\u1ee5ng CountVectorizer (binary = True) fit SGD Classifier: 85.84%\n\n4.  **LR:**\n    * \u0110\u1ed9 ch\u00ednh x\u00e1c c\u1ee7a vi\u1ec7c s\u1eed d\u1ee5ng TF-IDF fit v\u1edbi m\u00f4 h\u00ecnh h\u1ed3i quy logistic: 88.50%\n    * \u0110\u1ed9 ch\u00ednh x\u00e1c c\u1ee7a vi\u1ec7c s\u1eed d\u1ee5ng CountVectorizer fit v\u1edbi m\u00f4 h\u00ecnh h\u1ed3i quy logistic: 85.62%\n    * \u0110\u1ed9 ch\u00ednh x\u00e1c c\u1ee7a vi\u1ec7c s\u1eed d\u1ee5ng CountVectorizer (binary = True) fit v\u1edbi m\u00f4 h\u00ecnh h\u1ed3i quy logistic: 89.65%\n5.  **DEEP LEARNING**\n    * **M\u00f4 h\u00ecnh CNN cho \u0111\u1ed9 ch\u00ednh x\u00e1c t\u1ed1t nh\u1ea5t trong s\u1ed1 t\u1ea5t c\u1ea3 v\u1edbi 84.71% tr\u00ean t\u1eadp d\u1eef li\u1ec7u train v\u00e0 test**\n___","73a9e7cd":"***\u1ede \u0111\u00e2y,T\u00f4i mu\u1ed1n d\u1ef1 \u0111o\u00e1n \u0111\u00e1nh gi\u00e1 ng\u1eabu nhi\u00ean l\u00e0 poitive or negative(t\u00edch c\u1ef1c ho\u1eb7c ti\u00eau c\u1ef1c) b\u1eb1ng c\u00e1ch s\u1eed d\u1ee5ng m\u00f4 h\u00ecnh \u0111\u00e3 t\u1ea1o. V\u00ec v\u1eady, b\u00e0i \u0111\u00e1nh gi\u00e1 s\u1ebd \u0111\u01b0\u1ee3c x\u1eed l\u00fd tr\u01b0\u1edbc t\u01b0\u01a1ng t\u1ef1 nh\u01b0 nh\u1eefng g\u00ec m\u00e0 t\u00f4i \u0111\u00e3 l\u00e0m trong t\u1eadp d\u1eef li\u1ec7u tr\u01b0\u1edbc khi hu\u1ea5n luy\u1ec7n, t\u1ee9c l\u00e0 c\u00e1c t\u1eeb s\u1ebd \u0111\u01b0\u1ee3c chuy\u1ec3n \u0111\u1ed5i th\u00e0nh s\u1ed1 nguy\u00ean v\u00e0 \u0111\u1ed9 d\u00e0i t\u1ed1i \u0111a l\u00e0 100. Danh s\u00e1ch tr\u1ed1ng \u0111\u01b0\u1ee3c t\u1ea1o ra v\u00e0 s\u1ebd l\u01b0u tr\u1eef c\u00e1c gi\u00e1 tr\u1ecb. Cu\u1ed1i c\u00f9ng, t\u00f4i t\u1ea1o m\u1ed9t v\u00f2ng l\u1eb7p for \u0111\u1ec3 l\u1eb7p qua danh s\u00e1ch. Ch\u00fang ta s\u1ebd d\u1ef1 \u0111o\u00e1n \u0111\u01b0\u1ee3c \u0111\u00e1nh gi\u00e1 ng\u01b0\u1eddi xem th\u1ebf n\u00e0o?***\n\n**QUY \u01af\u1edaC:**\n\n* **D\u1ef1 \u0111o\u00e1n ng\u1eabu nhi\u00ean g\u1ea7n b\u1eb1ng 1 => \u0110\u00e1nh gi\u00e1 t\u00edch c\u1ef1c(Positive)**\n* **D\u1ef1 \u0111o\u00e1n ng\u1eabu nhi\u00ean = 0 => \u0110\u00e1nh gi\u00e1 ti\u00eau c\u1ef1c(Negative)**","6e811644":"**\u0110\u1ed9 ch\u00ednh x\u00e1c c\u1ee7a vi\u1ec7c s\u1eed d\u1ee5ng CountVectorizer (binary = True) fit v\u1edbi m\u00f4 h\u00ecnh h\u1ed3i quy logistic: 89.65%**\n___","5acc1321":"**\u0110\u1ed9 ch\u00ednh x\u00e1c c\u1ee7a vi\u1ec7c s\u1eed d\u1ee5ng TF_IDF fit v\u1edbi m\u00f4 h\u00ecnh SGD Classifier: 88.67%**","ee4113ba":"**TI\u1ec0N X\u1eec L\u00cd D\u1eee LI\u1ec6U CHO 1 C\u00c2U**"}}