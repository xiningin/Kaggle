{"cell_type":{"4bcfb214":"code","56051c49":"code","c964ad87":"code","5011938b":"code","5148d2c7":"code","9805b59a":"code","d1140130":"code","42f84c62":"code","461feead":"code","5fdd798d":"code","94515d36":"code","4824ea32":"code","8dd0b9cb":"code","3aa6b2c8":"code","2b9585d5":"code","88c0274a":"code","a11d7039":"code","a25f928f":"code","6bf04ff0":"code","f052ac96":"code","2e13cc77":"code","da915450":"code","77b7d8d1":"code","b3f3c495":"code","3434809c":"code","ff9d922b":"code","4909520e":"code","e77d3ae1":"markdown"},"source":{"4bcfb214":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport warnings\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nfrom collections import Counter\n\nfrom sklearn.preprocessing import StandardScaler\n\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.model_selection import learning_curve\n\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.ensemble import AdaBoostClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.ensemble import VotingClassifier\n\nfrom sklearn.svm import SVC\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.linear_model import LogisticRegression\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session\n\n\nwarnings.filterwarnings('ignore')","56051c49":"# Import Files\ntrain_data = pd.read_csv('\/kaggle\/input\/titanic\/train.csv')\ntest_data = pd.read_csv('\/kaggle\/input\/titanic\/test.csv')\nid_person = pd.read_csv('\/kaggle\/input\/titanic\/gender_submission.csv')\nid_person.drop('Survived',inplace=True, axis=1)\n\n# Dataframe statistics\ntrain_data.describe()","c964ad87":"# Dataframe statistics\ntest_data.describe()","5011938b":"print(train_data['Cabin'].dropna().to_numpy)","5148d2c7":"test_data.head()","9805b59a":"id_person.head()","d1140130":"# Outliers Treatment\n\ndef detect_outliers(df, n, columns):\n    outlier_indices = []\n    \n    for col in columns:\n        # 1st Quartile (25%) and 3rd Quartil (50%)\n        Q1 = np.percentile(df[col], 25)\n        Q3 = np.percentile(df[col], 75)\n        \n        # Interquartile range (IQR)\n        IQR = Q3 - Q1\n        \n        # Outlier step\n        outlier_step = IQR * 1.5\n        \n        # Create a list of indices\n        outlier_list_col = df[(df[col] < Q1 - outlier_step) | (df[col] > Q3 + outlier_step)].index\n        \n        # Append value to the list\n        outlier_indices.extend(outlier_list_col)\n        \n    # Filter containing more then 2 outliers\n    outlier_indices = Counter(outlier_indices)\n    multiple_outliers = list(k for k,v in outlier_indices.items() if v > n)\n    return multiple_outliers\n\nprint('Checking and removing Outliers')\n\n# Looking for outliers\noutliers_to_drop = detect_outliers(train_data, 2, ['Age', 'SibSp', 'Parch', 'Fare'])\n\n# Show Outliers\ntrain_data.loc[outliers_to_drop]\n\n# Drop outliers\ntrain_data = train_data.drop(outliers_to_drop, axis=0).reset_index(drop=True)\nprint(train_data)","42f84c62":"# Joing Train and Test sets\n# Concatenate train and test so all the features will be taken in consideration for the categorical conversion\ntest_len = len(test_data)\ndataset = pd.concat(objs=[train_data, test_data], axis = 0).reset_index(drop=True)\nprint(dataset)","461feead":"# Missing and NULL\ndataset = dataset.fillna(np.nan)\ndataset.isnull().sum()","5fdd798d":"# Fill up Age with median of similar rows according to Pclass, Parc, SibSp\n\nindex_Nan_age = list(dataset['Age'][dataset['Age'].isnull()].index)\n\nfor i in index_Nan_age:\n    # make median of the complete set of Age                               \n    age_median = dataset['Age'].median()\n    \n    # make median with subset of Age (same SibSp, Parch and Pclass and make median)\n    age_predicted = dataset['Age'][((dataset['SibSp'] == dataset.iloc[i]['SibSp']) &\n                                    (dataset['Parch'] == dataset.iloc[i]['Parch']) &\n                                    (dataset['Pclass'] == dataset.iloc[i]['Pclass']))].median()\n    if not np.isnan(age_predicted):\n        dataset['Age'].iloc[i] = age_predicted\n    else:\n        dataset['Age'].iloc[i] = age_median\n        \n# Fill up Fare with average of price\nindex_Nan_Fare = list(dataset['Fare'][dataset['Fare'].isnull()].index)\nfor i in index_Nan_Fare:\n    dataset['Fare'].iloc[i] = dataset['Fare'].median()\n    \n\n# Fill up Embarked\nindex_NaN_Embarked = list(dataset['Embarked'][dataset['Embarked'].isnull()].index)\nfor i in index_NaN_Embarked:\n    dataset['Embarked'].iloc[i] = dataset['Embarked'].mode(dropna=True)[0]\n\ndataset = dataset.fillna(np.nan)\ndataset.isnull().sum()","94515d36":"# Feature Analysis\n","4824ea32":"# Categorical Conversion\ndataset['Sex'] = dataset['Sex'].map({'male':0, 'female':1})\ndataset['Embarked'] = dataset['Embarked'].map({'S':0, 'C':1, 'Q':2})\ndataset.head()","8dd0b9cb":"# Define the TARGET and FEATURES\ntarget = 'Survived'\nfeatures = ['Pclass', 'Sex', 'Age', 'SibSp', 'Parch', 'Fare', 'Embarked']\n\ny_dataset = dataset[target]\nX_dataset = dataset[features]\n\n#Scaling the Features\nscaler = StandardScaler()\nX_dataset_scaled = scaler.fit_transform(X_dataset)\n\n# Split Train and Val\nX = X_dataset_scaled[:-test_len]\nX_val = X_dataset_scaled[-test_len:]\n\ny = y_dataset[:-test_len]\ny_val = y_dataset[-test_len:] # NaN (to be predicted)\nid_val = dataset['PassengerId']\nid_val = id_val[-test_len:]\n\n#print('X sets')\n#print(X_train)\n#print(X_val)\n#print(X_test)\n\n#print('Y sets')\n#print(y_train)\n#print(y_val)\n#print(y_test)","3aa6b2c8":"# K Fold\nkf = StratifiedKFold(n_splits = 10)","2b9585d5":"random_state = 86\nclf_list = []\nclf_list.append(SVC(random_state=random_state))\nclf_list.append(DecisionTreeClassifier(random_state=random_state))\nclf_list.append(AdaBoostClassifier(DecisionTreeClassifier(random_state=random_state), random_state=random_state))\nclf_list.append(RandomForestClassifier(random_state=random_state))\nclf_list.append(KNeighborsClassifier())\nclf_list.append(LogisticRegression())\nclf_list.append(GradientBoostingClassifier(random_state=random_state))\n\nclf_name_list = ['SVC',\n                'Decision Tree',\n                'Ada Boost',\n                'Random Forest',\n                'KNeighbors',\n                'Logistic Regression',\n                'Gradient Boosting']\nprint(clf_name_list)","88c0274a":"cv_result_list = []\nfor clf in clf_list:\n    cv_result_list.append(cross_val_score(clf,\n                         X,\n                         y = y,\n                         scoring='accuracy',\n                         cv=kf,\n                         n_jobs=5))\ncv_means = []\ncv_stds = []\n\nfor cv_r in cv_result_list:\n    cv_means.append(cv_r.mean())\n    cv_stds.append(cv_r.std())\n\ncv_df = pd.DataFrame({'Means: ' : cv_means,\n                      'Standard Dev: ' : cv_stds,\n                      'Classifier: ' : clf_name_list})\n\ncv_df.head(7)","a11d7039":"### Hyperparameter Tunning\n\n# Logistic Regression (lowest std)\n# Gradient Boosting (highest mean)\n# Random Forest (another ensemble)\n# Ada Boost (another ensemble)","a25f928f":"# Logistic Regression (lowest std)\n\nclf_lr = LogisticRegression()\nhp_lr = {'penalty':['l1','l2'],\n         'C':np.logspace(-4, 4, 20),\n         'solver':['liblinear', 'lbfgs'],\n        'multi_class':['ovr', 'multinomial']}\n\ngs_lr = GridSearchCV(clf_lr,\n                       param_grid=hp_lr,\n                       cv=kf,\n                       scoring='accuracy',\n                       n_jobs=4,\n                       verbose=1)\n\ngs_lr.fit(X, y)\n\n\nbestModel_lr = gs_lr.best_estimator_\nbestScore_lr = gs_lr.best_score_\n\nprint('Logistic Regression:')\nprint('Best Score', bestScore_lr)\nprint('Best Model: ', bestModel_lr)\n","6bf04ff0":"# Gradient Boosting (highest mean)\nclf_gb = GradientBoostingClassifier()\nhp_gb = {'loss':['deviance', 'exponential'],\n         'n_estimators':[100, 250, 500],\n         'learning_rate':[0.1, 0.05, 0.01, 0.005],\n         'max_depth': [3, 5, 10],\n         'max_features':['sqrt', 'log2']}\n\ngs_gb = GridSearchCV(clf_gb,\n                       param_grid=hp_gb,\n                       cv=kf,\n                       scoring='accuracy',\n                       n_jobs=4,\n                       verbose=1)\n\ngs_gb.fit(X, y)\n\n\nbestModel_gb = gs_gb.best_estimator_\nbestScore_gb = gs_gb.best_score_\n\nprint('Gradient Boost:')\nprint('Best Score', bestScore_gb)\nprint('Best Model: ', bestModel_gb)","f052ac96":"# Random Forest (another ensemble)\n\nclf_rf = RandomForestClassifier()\nhp_rf = {'max_depth':[None],\n         'max_features':[1, 3, 6],\n         'min_samples_split':[2, 5, 10, 15],\n         'min_samples_leaf':[1, 5, 10],\n         'bootstrap':[False],\n         'n_estimators':[100, 250, 500],\n         'criterion':['gini', 'entropy']}\n\ngs_rf = GridSearchCV(clf_rf,\n                    param_grid=hp_rf,\n                    cv=kf,\n                    scoring='accuracy',\n                    n_jobs=4,\n                    verbose=1)\n\ngs_rf.fit(X, y)\n\nbestModel_rf = gs_rf.best_estimator_\nbestScore_rf = gs_rf.best_score_\n\nprint('Random Forest:')\nprint('Best Score', bestScore_rf)\nprint('Best Model: ', bestModel_rf)","2e13cc77":"# Ada Boost (another ensemble)\n\nclf_ab = AdaBoostClassifier(DecisionTreeClassifier(), random_state=random_state)\nhp_ab = {'base_estimator__splitter':['best', 'random'],\n         'algorithm':['SAMME', 'SAMME.R'],\n         'learning_rate':[0.0001, 0.0010, 0.0100, 0.1000, 1.0],\n         'n_estimators':[100, 250, 500],\n         'base_estimator__criterion':['gini', 'entropy']}\n\ngs_ab = GridSearchCV(clf_ab,\n                    param_grid=hp_ab,\n                    cv=kf,\n                    scoring='accuracy',\n                    n_jobs=4,\n                    verbose=1)\n\ngs_ab.fit(X, y)\n\n\nbestModel_ab = gs_ab.best_estimator_\nbestScore_ab = gs_ab.best_score_\n\nprint('Ada Boosting:')\nprint('Best Score', bestScore_ab)\nprint('Best Model: ', bestModel_ab)\n","da915450":"# KNeighbors\nclf_kn = KNeighborsClassifier()\nhp_kn = {'n_neighbors':[5, 7, 9, 11, 13, 15],\n         'algorithm':['ball_tree', 'kd_tree', 'brute'],\n         'weights':['uniform', 'distance']}\n\ngs_kn = GridSearchCV(clf_kn,\n                    param_grid=hp_kn,\n                    cv=kf,\n                    scoring='accuracy',\n                    n_jobs=4,\n                    verbose=1)\n\ngs_kn.fit(X, y)\n\n\nbestModel_kn = gs_kn.best_estimator_\nbestScore_kn = gs_kn.best_score_\n\nprint('KNN:')\nprint('Best Score', bestScore_kn)\nprint('Best Model: ', bestModel_kn)","77b7d8d1":"# SVC\nclf_svc = SVC()\nhp_svc = {'kernel':['linear', 'poly', 'sigmoid'],\n         'degree':[3, 4, 5],\n         'gamma':['auto','scale']}\n\ngs_svc = GridSearchCV(clf_svc,\n                    param_grid=hp_svc,\n                    cv=kf,\n                    scoring='accuracy',\n                    n_jobs=4,\n                    verbose=1)\n\ngs_svc.fit(X, y)\n\n\nbestModel_svc = gs_svc.best_estimator_\nbestScore_svc = gs_svc.best_score_\n\nprint('SVM:')\nprint('Best Score', bestScore_svc)\nprint('Best Model: ', bestModel_svc)","b3f3c495":"#\nclf_voted = VotingClassifier(estimators=[('Logistic Regression',bestModel_lr),\n                                        ('Gradient Boost',bestModel_gb),\n                                        ('Random Forest',bestModel_rf),\n                                        ('Ada Boost',bestModel_ab),\n                                        ('SVC',bestModel_svc),\n                                        ('KNeighbors',bestModel_kn)],\n                                            n_jobs=4)\n\n\nclf_voted = clf_voted.fit(X, y)\n\nprint('Clf Voted: ', clf_voted)\n#score_array = np.array([bestScore_lr, bestScore_gb, bestScore_rf, bestScore_ab])\n#model_array = np.array([bestModel_lr, bestModel_gb, bestModel_rf, bestModel_ab])\n\n#print('Scores: ', score_array)\n","3434809c":"# https:\/\/scikit-learn.org\/stable\/modules\/model_evaluation.html#the-scoring-parameter-defining-model-evaluation-rules\nscoring = 'f1'\nf1_score = cross_val_score(clf_voted, X, y, cv=kf,scoring=scoring)\nprint(f1_score)","ff9d922b":"# Prediction\nprediction = clf_voted.predict(X_val)\nprint(prediction)","4909520e":"prediction_df = pd.concat([test_data['PassengerId'], \n                   pd.Series(prediction, name ='Survived')], \n                  axis=1)\n\nprediction_df = prediction_df.astype(int)\n\nprint(prediction_df)\nprediction_df.to_csv('my_submission.csv', index=False)","e77d3ae1":"References:\n\nhttps:\/\/www.kaggle.com\/sh0wmaker\/guide-to-feature-analysis-ensemble-modeling\n\nhttps:\/\/scikit-learn.org\/stable\/modules\/model_evaluation.html#the-scoring-parameter-defining-model-evaluation-rules\n\n"}}