{"cell_type":{"345b1456":"code","df115bc0":"code","47db45fe":"code","ea750b6b":"code","ebdc7e0e":"code","14d2d499":"code","8e966b27":"code","9f10e080":"markdown","e10a35a0":"markdown","5dd819b3":"markdown","0bc5b67f":"markdown","7b47138b":"markdown"},"source":{"345b1456":"import numpy as np, pandas as pd, tensorflow as tf, os, cv2\nfrom tensorflow.keras import Model\nfrom tensorflow.keras.layers import Conv2D, LeakyReLU, BatchNormalization, Input, UpSampling2D, ZeroPadding2D, Lambda\nfrom tensorflow.keras.callbacks import ReduceLROnPlateau, TensorBoard, ModelCheckpoint, EarlyStopping\nfrom tensorflow.keras.optimizers import Adam\n\nimport urllib.request as urlr\nfrom PIL import Image, ImageDraw, ImageFont\nfrom IPython.display import display\nfrom seaborn import color_palette, palplot\n\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        #print(os.path.join(dirname, filename))\n        pass","df115bc0":"def parse_config_file(configfile):\n    \n    with open(configfile,'r') as fullfile:\n        lines=[line.rstrip('\\n') for line in fullfile if line[0]!='#' and line!='\\n']\n    \n    #'lines' now holds the entire content of configfile with unnecessary '#' and only-newline occurences skipped\n    \n    \n    holder={}    #will be used as a dictionary to hold all <layer_info:value> pairs in a particular block\n    blocks=[]\n    \n    for line in lines:\n        if line[0]=='[':\n            #if a new block of layers is starting\n            line= 'type=' + line[1:-1].rstrip()\n            if len(holder)!=0:\n                blocks.append(holder)\n                holder={}\n        key,value=line.split('=')\n        holder[key.rstrip()]=value.lstrip()\n    \n    blocks.append(holder)\n    \n    return blocks\n\n\n\n\ndef YoloV3Net(configfile, model_size, n_classes):\n    \n    blocks=parse_config_file(configfile)\n    \n    outputs={}\n    output_filters=[]\n    filters=[]\n    out_pred=[]\n    scale=0\n    \n    inputs = input_image = Input(model_size)\n    inputs\/=255.\n    \n    for i,block in enumerate(blocks[1:]):\n        \n        #MAIN CONVOLUTIONAL LAYER LOGIC WITH AND WITHOUT BATCH-NORMALIZATION\n        if block['type'] == 'convolutional':\n            activation = block['activation']\n            filters = int(block['filters'])\n            kernel_size = int(block['size'])\n            strides = int(block['stride'])\n            padding='same'\n            \n            if strides>1:\n                inputs=ZeroPadding2D(padding=((1,0),(1,0))) (inputs)  #to avoid downsampling\n                padding='valid'\n            \n            inputs=Conv2D(\n                kernel_size=kernel_size, filters=filters, strides=strides, padding=padding, \n                use_bias=False if 'batch_normalize' in block else True, name='conv_'+str(i)\n            )(inputs)\n            \n            if 'batch_normalize' in block:\n                inputs=BatchNormalization(name='batchNorm_'+str(i))(inputs)\n                inputs=LeakyReLU(alpha=0.1, name ='leaky_'+str(i))(inputs)\n                \n                \n        #UPSAMPLE LAYER LOGIC TO AVOID DOWNSAMPLING AND LOSING INFORMATION FROM PREVIOUS FEATURE-MAP\n        elif block['type']=='upsample':\n            stride = int(block['stride'])\n            inputs=UpSampling2D(size=stride)(inputs)\n            \n            \n        \n        elif block['type']=='route':\n            block['layers']=block['layers'].split(',')\n            start=int(block['layers'][0])\n            \n            if len(block['layers'])>1:\n                end=int(block['layers'][1])-i\n                filters=output_filters[i+start] + output_filters[end]\n                inputs=tf.concat([outputs[i+start],outputs[i+end]],axis=-1)\n            else:\n                filters=output_filters[i+start]\n                inputs=outputs[i+start]\n                \n                \n        #SKIP CONNECTION LOGIC\n        elif block['type']=='shortcut':\n            from_=int(block['from'])\n            inputs=outputs[i-1] + outputs[i+from_]\n            \n            \n            \n        #PREDICTION LAYER LOGIC\n        elif block['type']=='yolo':\n            mask = block['mask'].split(',')\n            mask=[int(i) for i in mask]\n            anchors = block['anchors'].split(',')\n            anchors=[(int(anchors[i]),int(anchors[i+1])) for i in range(0,len(anchors),2)]\n            anchors=[anchors[i] for i in mask]\n            n_anchors=len(anchors)\n            \n            out_shape=inputs.get_shape().as_list()\n            \n            inputs=tf.reshape(tensor=inputs,shape=(-1, n_anchors*out_shape[1]*out_shape[2], 5+n_classes))\n            \n            box_centers=inputs[:,:,:2]\n            box_shapes=inputs[:,:,2:4]\n            confidence=inputs[:,:,4:5]\n            classes=inputs[:,:,5:n_classes+5]\n            \n            \n            box_centers=tf.nn.sigmoid(box_centers)\n            confidence=tf.nn.sigmoid(confidence)\n            classes=tf.nn.sigmoid(classes)\n            \n            anchors=tf.tile(anchors, [out_shape[1]*out_shape[2], 1])\n            box_shapes=tf.exp(box_shapes) * tf.cast(anchors,dtype=tf.float32)\n            \n            x=tf.range(out_shape[1],dtype=tf.float32)\n            y=tf.range(out_shape[2],dtype=tf.float32)\n            \n            cx,cy=tf.meshgrid(x,y)\n            cx=tf.reshape(cx,(-1,1))\n            cy=tf.reshape(cy,(-1,1))\n            cxy=tf.concat([cx,cy],axis=-1)\n            cxy=tf.tile(cxy,[1,n_anchors])\n            cxy=tf.reshape(cxy,[1,-1,2])\n            \n            strides=(input_image.shape[1]\/\/out_shape[1] , input_image.shape[2]\/\/out_shape[2])\n            \n            box_centers = (box_centers+cxy)*strides\n            #print(\"i=\",i)\n            #print(box_centers.shape,box_shapes.shape,confidence.shape,classes.shape,\"\\n\\n\")\n            prediction=tf.concat([box_centers,box_shapes,confidence,classes],axis=-1)\n            \n            \n            if scale:\n                #for 2nd and 3rd scale prediction\n                out_pred=tf.concat([out_pred,prediction],axis=1)\n            else:\n                #for 1st scale prediction\n                out_pred=prediction\n                scale=1\n        \n        \n        #TO KEEP TRACK OF PREVIOUS LAYERS' INPUTS FOR PURPOSES OF 'shortcut' AND 'route' LAYERS\n        outputs[i]=inputs\n        output_filters.append(filters)\n    \n    model=Model(input_image,out_pred)\n    model.summary()\n    return model","47db45fe":"def load_weights(model, config_file, weights_file):\n    fp=open(weights_file,'rb')\n    \n    #skip the first 5 values from the weights file\n    np.fromfile(fp, dtype=np.int32, count=5)\n    \n    blocks=parse_config_file(config_file)\n    \n    for i, block in enumerate(blocks[1:]):\n        if block['type']=='convolutional':\n            conv_layer=model.get_layer('conv_'+str(i))\n            print('layer: ',i+1,conv_layer)\n            \n            filters=conv_layer.filters\n            kernel_size=conv_layer.kernel_size[0]\n            in_dim=conv_layer.input_shape[-1]\n            \n            if 'batch_normalize' in block:\n                norm_layer=model.get_layer('batchNorm_'+str(i))\n                print('layer: ',i+1,norm_layer)\n                bn_weights=np.fromfile(fp, dtype=np.float32, count=4*filters)\n                size=np.prod(norm_layer.get_weights()[0].shape)\n                \n                #in the weights file BatchNorm weights for 1 filter are stored as [gamma,beta,mean,variance]\n                #but we want [beta,gamma,mean,variance], hence the reshape below\n                bn_weights=bn_weights.reshape(4,filters)[[1,0,2,3]]  \n                \n            else:\n                conv_bias=np.fromfile(fp, dtype=np.float32, count=filters)\n        \n            #darknet shape [out_dim, in_dim, height, width]\n            conv_shape=(filters, in_dim, kernel_size, kernel_size)\n            conv_weights=np.fromfile(fp, dtype=np.float32, count=np.product(conv_shape))\n            \n            #tensorflow shape [height, width, in_dim, out_dim]\n            conv_weights=conv_weights.reshape(conv_shape).transpose([2,3,1,0])\n            \n            if 'batch_normalize' in block:\n                norm_layer.set_weights(bn_weights)\n                conv_layer.set_weights([conv_weights])\n            else:\n                conv_layer.set_weights([conv_weights,conv_bias])\n            \n    assert len(fp.read())==0, 'Failed to read all data'\n    fp.close()\n            ","ea750b6b":"weights_file='..\/input\/yolov3weightsconfigdataset\/YOLOv3_Weights_Config_Dataset\/yolov3.weights'\nconfig_file='..\/input\/yolov3weightsconfigdataset\/YOLOv3_Weights_Config_Dataset\/yolov3.cfg'\nclass_names_file='..\/input\/yolov3weightsconfigdataset\/YOLOv3_Weights_Config_Dataset\/coco.names'\nfont_path='..\/input\/yolov3weightsconfigdataset\/futur.ttf'\nimages=['..\/input\/yolov3weightsconfigdataset\/YOLOv3_Weights_Config_Dataset\/Flowers.jpg','..\/input\/yolov3weightsconfigdataset\/YOLOv3_Weights_Config_Dataset\/Manly_beach.jpg']\n\nmodel_size=(416,416,3)\nnum_classes=80\nmax_output_size=40\nmax_output_size_per_class=20\niou_threshold=.6\nconfidence_threshold=.6\n\nyolomodel=YoloV3Net(configfile=config_file, model_size=model_size, n_classes=num_classes)\nload_weights(model=yolomodel, config_file=config_file, weights_file=weights_file)\n\ntry:\n    wfile='yolov3_weights.tf'\n    yolomodel.save_weights(wfile)\n    print('The model\\'s weights have been saved successfully at {}'.format(wfile))\nexcept IOError:\n    print('Couldn\\'t write the file \\'yolov3_weights.tf\\'')","ebdc7e0e":"def non_max_suppression(inputs, model_size, max_output_size, max_output_size_per_class, confidence_threshold, iou_threshold):\n    '''\n        'max_output_size' ie max_total_size => total # of bounding boxes predictions for all classes combined\n        'max_output_size_per_class'         => total # of bounding boxes predictions for each class\n    '''\n    bbox, confidence, classes_probs=tf.split(inputs,[4,1,-1],axis=-1)\n    bbox=bbox\/model_size[0]\n    \n    scores=confidence*classes_probs\n    boxes, scores, classes, valid_detections = tf.image.combined_non_max_suppression(\n        boxes=tf.reshape(bbox, (tf.shape(bbox)[0],-1,1,4)),\n        scores=tf.reshape(scores, (tf.shape(scores)[0],-1,tf.shape(scores)[-1])),\n        max_total_size=max_output_size, max_output_size_per_class=max_output_size_per_class,\n        score_threshold=confidence_threshold, iou_threshold=iou_threshold\n    )\n    return boxes,scores,classes,valid_detections\n\n\n\ndef resize_image(inputs,model_size):\n    inputs=image.resize(size=(model_size[0],model_size[1]))\n    return inputs\n\n\n\ndef load_class_names(file_name):\n    with open(file_name,'r') as fp:\n        classnames=fp.read().splitlines()\n    return classnames\n\n\ndef output_boxes(inputs, model_size, max_output_size, max_output_size_per_class, score_threshold, iou_threshold):\n    \n    x, y, w, h, confidence, classes_probs= tf.split(inputs,[1,1,1,1,1,-1],axis=-1)\n    \n    topleft_x     = x - w\/2.\n    topleft_y     = y - h\/2.\n    bottomright_x = x + w\/2.\n    bottomright_y = y + h\/2.\n    \n    inputs=tf.concat([topleft_x,topleft_y,bottomright_x,bottomright_y,confidence, classes_probs],axis=-1)\n    \n    boxes_dicts = non_max_suppression(inputs, model_size, max_output_size, max_output_size_per_class, score_threshold, iou_threshold)\n    #(boxes,scores,classes,valid_detections)\n    \n    return boxes_dicts\n\n\n\ndef draw_boxes_in_PIL(image_path, class_names, font_path, model_size, boxes, confidence, classes, nums):\n    '''\n        nums     => Number of boxes to draw\n        boxes    => Actual box 4 coordinates topleft (x,y) and bottomright (x,y) with values between 0-1\n    '''\n    print('Welcome to the draw_boxes_in_PIL() function.\\n\\nIMPORTANT: We\\'ll be resizing the predictions of box coordinates by size of image itself!')\n    boxes, confidence, classes, nums = boxes[0], confidence[0], classes[0], nums[0]\n    print('nums=',nums,'\\n\\nboxes.shape=',boxes.shape)           \n                                                           #   =====> nums=3   and boxes=<40,4> with data\n    colors=((255 * np.array(color_palette(palette='hls', n_colors=len(class_names)) ) )).astype(int)\n    \n    \n    if 'http' in image_path:\n        image=Image.open(urlr.urlopen(image_path))\n    else:\n        image=Image.open(image_path)\n    drawable=ImageDraw.Draw(image)\n    font = ImageFont.truetype(font=font_path, size=(image.size[0] + image.size[1]) \/\/ 100)\n    print('\\nOriginal image.size=',image.size)             #   =====> image=<3024,4032>\n    \n    resize_factor= (image.size[0], image.size[1])          # Don't divide by the model_size, since this implementation predicts values betn 0-1\n    print('resize_factor=',resize_factor)                  #   =====> resize_factor=(7.3 , 9.7)\n    \n    for i in range(nums):\n        color=colors[i]\n        xy=boxes[i,0:4]\n        xy = [xy[j] * resize_factor[j % 2] for j in range(4)]\n        x0,y0 = xy[0], xy[1]\n        \n        thickness = (image.size[0] + image.size[1]) \/\/ 200\n        for t in np.linspace(0, 1, thickness):\n            xy[0], xy[1] = (xy[0] + t) , (xy[1] + t)          #adjusting top left x and y\n            xy[2], xy[3] = (xy[2] - t) , (xy[3] - t)          #adjusting bottom right x and y\n            drawable.rectangle(xy, outline=tuple(color))\n        \n        text = '{} {:.1f}%'.format(class_names[int(classes[i])], confidence[i] * 100)\n                    \n        text_size = drawable.textsize(text, font=font)\n                    \n        drawable.rectangle( [x0, y0 - text_size[1], x0 + text_size[0], y0],   fill=tuple(color))\n                    \n        drawable.text((x0, y0 - text_size[1]), text, fill='black', font=font)\n    \n    display(image)","14d2d499":"weights_file='..\/input\/yolov3weightsconfigdataset\/YOLOv3_Weights_Config_Dataset\/yolov3.weights'\nconfig_file='..\/input\/yolov3weightsconfigdataset\/YOLOv3_Weights_Config_Dataset\/yolov3.cfg'\nclass_names_file='..\/input\/yolov3weightsconfigdataset\/YOLOv3_Weights_Config_Dataset\/coco.names'\nfont_path='..\/input\/yolov3weightsconfigdataset\/futur.ttf'\nimages=['..\/input\/yolov3weightsconfigdataset\/YOLOv3_Weights_Config_Dataset\/Madame Toussades-III.jpg',\n        '..\/input\/yolov3weightsconfigdataset\/YOLOv3_Weights_Config_Dataset\/Manly_beach.jpg',\n        'https:\/\/images-na.ssl-images-amazon.com\/images\/I\/81nwy-sys5L._SL1500_.jpg',\n        'https:\/\/encrypted-tbn0.gstatic.com\/images?q=tbn%3AANd9GcSoFbdE6jbpy3U1qbytmNvZW7o_mqJAPcLjtIg2NYZkL8srLbM9&usqp=CAU',\n        'https:\/\/upload.wikimedia.org\/wikipedia\/commons\/2\/2f\/Culinary_fruits_front_view.jpg'\n       ]\n\nclass_names=load_class_names(class_names_file)\nmodel_size=(416,416,3)\nnum_classes=80\nmax_output_size=40\nmax_output_size_per_class=20\niou_threshold=.6\nconfidence_threshold=.4\n\n\nfor i in range(len(images)):\n    predictions, image=[], []\n    boxes, confidence, classes, nums=[],[],[],[]\n    \n    print('Image being preprocessed for prediction= ',images[i])\n    \n    if 'http' in images[i]:\n        image=Image.open(urlr.urlopen(images[i]))\n    else:\n        image=Image.open(images[i])\n    print('\\timage.size for prediction=(',image.size[0],',',image.size[1],')')\n    \n    \n    resized_image=resize_image(image,model_size)\n    print('\\tresized_image.size for prediction=',resized_image.size)\n\n    \n    resized_image=np.array(resized_image,dtype=np.float32)\n    resized_image=np.expand_dims(resized_image,axis=0)\n    print('\\tresized_image.shape for prediction=',resized_image.shape)\n    \n    predictions = yolomodel.predict(resized_image)\n    print('\\tPredictions.shape=',predictions.shape)\n    \n    boxes, confidence, classes, nums = output_boxes(\n        predictions, model_size, max_output_size, max_output_size_per_class, \n        confidence_threshold, iou_threshold)\n\n    print('\\tPredicted boxes.shape=',boxes.shape)\n    print('\\n\\n################ PREDICTION PHASE COMPLETE - NOW DRAWING BOXES ################\\n\\n')\n\n    draw_boxes_in_PIL(images[i], class_names, font_path, model_size, boxes, confidence, classes, nums)","8e966b27":"'''\n#COMMENTED OUT THIS CELL COZ I'M TRYING TO ADDITIONALLY TRAIN THE MODEL ON A CUSTOM DATASET\n\nfiles=[file for _,_,file in os.walk('\/kaggle\/input\/')]\ntrain_files=files[4]       # 204\nvalidation_files=files[5]  # 60\ntest_files=files[6]        # 31\nprint(len(train_files),len(validation_files),len(test_files))\n\nyolomodel2=YoloV3Net(configfile=config_file, model_size=model_size, n_classes=num_classes)\nload_weights(model=yolomodel2, config_file=config_file, weights_file=weights_file)\n\ndataset_dirname='\/kaggle\/input\/roboflow-yolov3-chess-pieces-dataset\/train\/'\n\n\n\n\n_FREEZE_BODY = 2\n\ndef YoloV3Net_2(configfile, model_size, n_classes, num=2):\n'''\n        #Parse the config file, retrieve the blocks of layers as per above format, and then start stacking up layers.\n'''\n    blocks=parse_config_file(configfile)\n    \n    outputs={}\n    output_filters=[]\n    filters=[]\n    out_pred=[]\n    scale=0\n    \n    inputs = input_image = Input(model_size)\n    inputs\/=255.\n    \n    for i,block in enumerate(blocks[1:]):\n        if block['type'] == 'convolutional':\n            activation = block['activation']\n            filters = int(block['filters'])\n            kernel_size = int(block['size'])\n            strides = int(block['stride'])\n            padding='same'\n            \n            if strides>1:\n                inputs=ZeroPadding2D(padding=((1,0),(1,0))) (inputs)  #to avoid downsampling\n                padding='valid'\n            \n            inputs=Conv2D(\n                kernel_size=kernel_size, filters=filters, strides=strides, padding=padding, \n                use_bias=False if 'batch_normalize' in block else True, name='conv_'+str(i)\n            )(inputs)\n            \n            if 'batch_normalize' in block:\n                inputs=BatchNormalization(name='batchNorm_'+str(i))(inputs)\n                inputs=LeakyReLU(alpha=0.1, name ='leaky_'+str(i))(inputs)\n                \n                \n        \n        elif block['type']=='upsample':\n            stride = int(block['stride'])\n            inputs=UpSampling2D(size=stride)(inputs)\n            \n            \n        \n        elif block['type']=='route':\n            block['layers']=block['layers'].split(',')\n            start=int(block['layers'][0])\n            \n            if len(block['layers'])>1:\n                end=int(block['layers'][1])-i\n                filters=output_filters[i+start] + output_filters[end]\n                inputs=tf.concat([outputs[i+start],outputs[i+end]],axis=-1)\n            else:\n                filters=output_filters[i+start]\n                inputs=outputs[i+start]\n                \n                \n        #SKIP CONNECTION LOGIC\n        elif block['type']=='shortcut':\n            from_=int(block['from'])\n            inputs=outputs[i-1] + outputs[i+from_]\n            \n            \n            \n        \n        elif block['type']=='yolo':\n            mask = block['mask'].split(',')\n            mask=[int(i) for i in mask]\n            anchors = block['anchors'].split(',')\n            anchors=[(int(anchors[i]),int(anchors[i+1])) for i in range(0,len(anchors),2)]\n            anchors=[anchors[i] for i in mask]\n            n_anchors=len(anchors)\n            \n            out_shape=inputs.get_shape().as_list()\n            \n            inputs=tf.reshape(tensor=inputs,shape=(-1, n_anchors*out_shape[1]*out_shape[2], 5+n_classes))\n            \n            box_centers=inputs[:,:,:2]\n            box_shapes=inputs[:,:,2:4]\n            confidence=inputs[:,:,4:5]\n            classes=inputs[:,:,5:n_classes+5]\n            \n            \n            box_centers=tf.nn.sigmoid(box_centers)\n            confidence=tf.nn.sigmoid(confidence)\n            classes=tf.nn.sigmoid(classes)\n            \n            anchors=tf.tile(anchors, [out_shape[1]*out_shape[2], 1])\n            box_shapes=tf.exp(box_shapes) * tf.cast(anchors,dtype=tf.float32)\n            \n            x=tf.range(out_shape[1],dtype=tf.float32)\n            y=tf.range(out_shape[2],dtype=tf.float32)\n            \n            cx,cy=tf.meshgrid(x,y)\n            cx=tf.reshape(cx,(-1,1))\n            cy=tf.reshape(cy,(-1,1))\n            cxy=tf.concat([cx,cy],axis=-1)\n            cxy=tf.tile(cxy,[1,n_anchors])\n            cxy=tf.reshape(cxy,[1,-1,2])\n            \n            strides=(input_image.shape[1]\/\/out_shape[1] , input_image.shape[2]\/\/out_shape[2])\n            \n            box_centers = (box_centers+cxy)*strides\n            #print(\"i=\",i)\n            #print(box_centers.shape,box_shapes.shape,confidence.shape,classes.shape,\"\\n\\n\")\n            prediction=tf.concat([box_centers,box_shapes,confidence,classes],axis=-1)\n            \n            if scale:\n                out_pred=tf.concat([out_pred,prediction],axis=1)\n            else:\n                out_pred=prediction\n                scale=1\n        \n        #TO KEEP TRACK OF PREVIOUS LAYERS' INPUTS FOR PURPOSES OF 'shortcut' AND 'route' LAYERS\n        outputs[i]=inputs\n        output_filters.append(filters)\n    \n    \n    \n    y_true = [Input(shape=(416\/\/{0:32, 1:16, 2:8}[l], 416\/\/{0:32, 1:16, 2:8}[l], 9\/\/3, len(class_names)+5)) for l in range(3)]\n\n    y_true=[tf.reshape(y,(-1,np.prod(y.shape[1:4]),len(class_names)+5)) for y in y_true]\n    print('PLACEHOLDER FOR y_true OUTPUT: ',y_true)\n    y_op=tf.concat([y for y in y_true],axis=1)\n\n    print(y_op)\n    model_loss = Lambda(yolo_loss, output_shape=(1,), name='yolo_loss', \n                    arguments={'anchors': anchors, 'num_classes': num_classes, 'ignore_thresh': 0.5}) ([*yolmodelv2.output, *y_true])\n    \n    model=Model(input_image, out_pred)\n    \n    model_with_loss=Model([*model.input, *y_true], model_loss)\n    \n    # Freeze the entire model body except last 3 layers if '_FREEZE_BODY'=2, else only first 185 layers (Darknet) if '_FREEZE_BODY'=1\n    num = (185, len(model_with_loss.layers)-3)[_FREEZE_BODY-1]\n    for i in range(num): model_with_loss.layers[i].trainable=False\n    print('Freezing the top {} layers of total {} layers'.format(num, len(model_with_loss.layers)))\n    \n    model_with_loss.summary()\n    return model_with_loss\n\nyolomodelv2=YoloV3Net_2(configfile=config_file, model_size=model_size, n_classes=num_classes)\nprint('Model_Input=',yolomodelv2.input)\nprint('Model_Output=',yolomodelv2.output)\n'''","9f10e080":"### Yolo-v3 network structure:\nThe below function will read the offical _**yolov3.cfg**_ file and generate a list of blocks containing a dictionary of layers. We'll be parsing the config file, retrieving the blocks of layers as per above format, and then stacking up layers.\n\nJust glance at the config file format and this function will be easily understood.\n\nWe're essentially trying to accomplish the following Yolo-v3 network structure:\n\n![Yolo-v3 network structure](https:\/\/www.researchgate.net\/publication\/335865923\/figure\/fig1\/AS:804106595758082@1568725360777\/Structure-detail-of-YOLOv3It-uses-Darknet-53-as-the-backbone-network-and-uses-three.jpg)","e10a35a0":"# RUNNING THE MODEL (USING PIL LIBRARY)\n\nNow, we can begin with model predictions based on the pretrained YOLOv3 model that we loaded.\n\n* Begin by preprocessing the image-file to bring it to expected size - 416x416\n\n* Push the image into the model using yolomodel.predict() function\n\n* Retrieve the boxes and remember, our boxes are predicted values only between 0-1\n\n* Hence, to display the boxes, don't forget to multiply the co-ordinates by original image-size value","5dd819b3":"# QUICK INTRO TO YOLOv3\n\nYOLO fundamental approach for an input image divided into 13x13 grid, and assuming 3 anchors considered:\n![](https:\/\/machinelearningspace.com\/wp-content\/uploads\/2020\/01\/bbox_ok-2.png)\n\nThe actual YOLOv3 implements this kinda detection at 3 scales.\nAt each scale the output tensor (*feature-map*) will be of shape <13, 13, (3)(5+80)>\n\n80 = number of static classnames from COCO.names\n\n\nThe file yolov3.cfg contains all information related to the YOLOv3 architecture and its parameters, whereas the file yolov3.weights contains the convolutional neural network (CNN) parameters of the YOLOv3 pre-trained weights.\n\n### Note- \n> I've implemented the Yolov3 algorithm in Tensorflow v1 with additional code-explainations as well. This notebook serves as a high level overview and implementation, since we've used the official Yolov3 config file and weights, to load the model.","0bc5b67f":"# TESTING MODEL'S SKELETON\nThe following cell attempts to load the pre-trained weights into the model skeleton that we created earlier from the config file.","7b47138b":"# UTILITY FUNCTIONS\n\n### NON-MAX SUPPRESSION\n* Split the input into (x,y,w,h,conf,[classes]) along last axis\n\n* At the crux of this Yolo implementation, non-max suppression is accomplished by using the Tensorflow v2 in-built function\n> tf.image.combined_non_max_suppression()\n\n### LOAD CLASS-NAMES\n* Returns an array of 80 class-names from 'coco.names'\n> Objects that can be detected (ex- dog, cat, person, ...)\n\n### OUTPUT BOXES\n* Re-splits the input feature-map into (x,y,w,h,conf,[classes]) along last axis, and calculates (xmin,ymin) , (xmax,ymax).\n\n* Invokes the above non-max suppression function, and retrieves (boxes,scores,classes,num_valid_detections)\n\n### DRAW BOXES\n* Extract the actual boxes, scores, classes, and number of valid_detections.\n\n    * Create an n-d array of seaborn module's color-palette\n    \n    * Open the Image and link it to a drawable object\n    \n    * For each of our predicted bounding boxes:\n        \n        * Choose that box of 4 co-ordinates and scale their values up by the real image's size\n        \n        * Calculate 't' values of thickness between 0-1 using np.linspace()\n        > for ex.- the 't' values could be [0, 0.25, 0.5, 0.75, 1.0] if thickness=2\n\n        * Draw rectangles using these 't' values to mimic thick dark lines\n        \n        * Print the text using some simple draw functions"}}