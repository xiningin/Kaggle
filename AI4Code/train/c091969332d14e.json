{"cell_type":{"ccc26d10":"code","66e32eaf":"code","da0a27a5":"code","c4a78598":"code","845308af":"code","273bbdaf":"code","7bae9383":"code","835a5928":"code","d8cd5058":"code","ba221bb4":"code","6da0c85f":"code","8866d1b5":"code","5835d930":"code","13a23f07":"code","6a68691d":"code","6314e52c":"code","1c6e7e79":"code","0e1fdefe":"code","fccf0f7e":"code","b11fb49a":"code","bfb2b20f":"code","caa3d342":"code","52c7497d":"code","59887e1e":"code","630a61e6":"code","8e5ad215":"code","a620eacb":"code","8be33954":"code","b25ed5b2":"code","53551a22":"code","7a8d011c":"code","d8a32e3a":"code","acf211a8":"code","ee8e80b4":"code","d3c5a23d":"code","df6ae90d":"code","1f74e365":"code","db065bfc":"markdown","74673844":"markdown","062556b0":"markdown","dac41460":"markdown","fe1c92bf":"markdown","babd4f8e":"markdown","09732b61":"markdown","785c8fb5":"markdown","84d9df5c":"markdown","be63258a":"markdown","b2d821e6":"markdown","fae9bdc1":"markdown","b4c1a448":"markdown","012eb0cc":"markdown","05029750":"markdown","1a318738":"markdown","59e1d647":"markdown","a1703976":"markdown","9730e638":"markdown","6e7a5bde":"markdown","eee0f960":"markdown","4296e007":"markdown","f4ad29f8":"markdown","5a8f8876":"markdown","60997c99":"markdown","7d3e4652":"markdown","326fde14":"markdown","06cc5779":"markdown","e2085f4f":"markdown","51404df1":"markdown","4acfe0ab":"markdown","6f08c66b":"markdown","b4496f9d":"markdown","972320e2":"markdown","02caa2a7":"markdown","9644cc29":"markdown","89380651":"markdown","8c5f0c9a":"markdown","5cda0609":"markdown","e119e33d":"markdown","c2d914d2":"markdown","f29d6866":"markdown"},"source":{"ccc26d10":"# Download YOLOv5\n!git clone https:\/\/github.com\/ultralytics\/yolov5  # clone repo\n%cd yolov5\n\n# Install dependencies\n%pip install -qr requirements.txt  \n\n# change directory\n%cd ..\/\nimport torch\nprint(f\"Setup complete. Using torch {torch.__version__} ({torch.cuda.get_device_properties(0).name if torch.cuda.is_available() else 'CPU'})\")","66e32eaf":"!add-apt-repository ppa:ubuntu-toolchain-r\/test -y\n!apt-get update\n!apt-get upgrade libstdc++6 -y","da0a27a5":"import wandb\nfrom kaggle_secrets import UserSecretsClient\nuser_secrets = UserSecretsClient()\napi_key = user_secrets.get_secret(\"keyme\")\n! wandb login $api_key","c4a78598":"import warnings\nwarnings.filterwarnings(\"ignore\")\n\nfrom itertools import groupby\nimport numpy as np\nfrom tqdm.notebook import tqdm\ntqdm.pandas()\nimport pandas as pd\nimport os\nimport pickle\nimport cv2\nfrom multiprocessing import Pool\nimport matplotlib.pyplot as plt\n# import cupy as cp\nimport ast\nimport glob\nfrom os import listdir\nfrom os.path import isfile, join\nfrom glob import glob\nimport yaml\n\nimport shutil\nfrom shutil import copyfile\nimport sys\n\nfrom joblib import Parallel, delayed\n\n# --- Read data ---\nTRAIN_PATH = '\/kaggle\/input\/tensorflow-great-barrier-reef'","845308af":"def get_bbox(annots):\n    bboxes = [list(annot.values()) for annot in annots]\n    return bboxes\ndef get_path(row):\n    row['image_path'] = f'{TRAIN_PATH}\/train_images\/video_{row.video_id}\/{row.video_frame}.jpg'\n    return row\ndef load_image(image_path):\n    return cv2.cvtColor(cv2.imread(image_path), cv2.COLOR_BGR2RGB)\n\ndef coco2yolo(image_height, image_width, bboxes):\n    \"\"\"\n    coco => [xmin, ymin, w, h]\n    yolo => [xmid, ymid, w, h] (normalized)\n    \"\"\"\n    \n    bboxes = bboxes.copy().astype(float) # otherwise all value will be 0 as voc_pascal dtype is np.int\n    \n    # normolizinig\n    bboxes[..., [0, 2]]= bboxes[..., [0, 2]]\/ image_width\n    bboxes[..., [1, 3]]= bboxes[..., [1, 3]]\/ image_height\n    \n    # converstion (xmin, ymin) => (xmid, ymid)\n    bboxes[..., [0, 1]] = bboxes[..., [0, 1]] + bboxes[..., [2, 3]]\/2\n    \n    return bboxes\n\ndef yolo2coco(image_height, image_width, bboxes):\n    \"\"\"\n    yolo => [xmid, ymid, w, h] (normalized)\n    coco => [xmin, ymin, w, h]\n    \n    \"\"\" \n    bboxes = bboxes.copy().astype(float) # otherwise all value will be 0 as voc_pascal dtype is np.int\n    \n    # denormalizing\n    bboxes[..., [0, 2]]= bboxes[..., [0, 2]]* image_width\n    bboxes[..., [1, 3]]= bboxes[..., [1, 3]]* image_height\n    \n    # converstion (xmid, ymid) => (xmin, ymin) \n    bboxes[..., [0, 1]] = bboxes[..., [0, 1]] - bboxes[..., [2, 3]]\/2\n    \n    return bboxes\ndef plot_one_box(x, img, color=None, label=None, line_thickness=None):\n    # Plots one bounding box on image img\n    tl = line_thickness or round(0.002 * (img.shape[0] + img.shape[1]) \/ 2) + 1  # line\/font thickness\n    color = color or [random.randint(0, 255) for _ in range(3)]\n    c1, c2 = (int(x[0]), int(x[1])), (int(x[2]), int(x[3]))\n    cv2.rectangle(img, c1, c2, color, thickness=tl, lineType=cv2.LINE_AA)\n    if label:\n        tf = max(tl - 1, 1)  # font thickness\n        t_size = cv2.getTextSize(label, 0, fontScale=tl \/ 3, thickness=tf)[0]\n        c2 = c1[0] + t_size[0], c1[1] - t_size[1] - 3\n        cv2.rectangle(img, c1, c2, color, -1, cv2.LINE_AA)  # filled\n        cv2.putText(img, label, (c1[0], c1[1] - 2), 0, tl \/ 3, [0, 0, 255], thickness=tf, lineType=cv2.LINE_AA)\n\n\n\ndef draw_bboxes(img, bboxes, classes, class_ids, colors = None, show_classes = None, bbox_format = 'yolo', class_name = False, line_thickness = 2):  \n    image = img.copy()\n    show_classes = classes if show_classes is None else show_classes\n    colors = (0, 255 ,0) if colors is None else colors\n    \n    if bbox_format == 'yolo':\n        \n        for idx in range(len(bboxes)):  \n            \n            bbox  = bboxes[idx]\n            cls   = classes[idx]\n            cls_id = class_ids[idx]\n            color = colors[cls_id] if type(colors) is list else colors\n            \n            if cls in show_classes:\n            \n                x1 = round(float(bbox[0])*image.shape[1])\n                y1 = round(float(bbox[1])*image.shape[0])\n                w  = round(float(bbox[2])*image.shape[1]\/2) #w\/2 \n                h  = round(float(bbox[3])*image.shape[0]\/2)\n\n                voc_bbox = (x1-w, y1-h, x1+w, y1+h)\n                plot_one_box(voc_bbox, \n                             image,\n                             color = color,\n                             label = cls if class_name else str(get_label(cls)),\n                             line_thickness = line_thickness)\n            \n    elif bbox_format == 'coco':\n        \n        for idx in range(len(bboxes)):  \n            \n            bbox  = bboxes[idx]\n            cls   = classes[idx]\n            cls_id = class_ids[idx]\n            color = colors[cls_id] if type(colors) is list else colors\n            \n            if cls in show_classes:            \n                x1 = int(round(bbox[0]))\n                y1 = int(round(bbox[1]))\n                w  = int(round(bbox[2]))\n                h  = int(round(bbox[3]))\n\n                voc_bbox = (x1, y1, x1+w, y1+h)\n                plot_one_box(voc_bbox, \n                             image,\n                             color = color,\n                             label = cls if class_name else str(cls_id),\n                             line_thickness = line_thickness)\n\n    elif bbox_format == 'voc_pascal':\n        \n        for idx in range(len(bboxes)):  \n            \n            bbox  = bboxes[idx]\n            cls   = classes[idx]\n            cls_id = class_ids[idx]\n            color = colors[cls_id] if type(colors) is list else colors\n            \n            if cls in show_classes: \n                x1 = int(round(bbox[0]))\n                y1 = int(round(bbox[1]))\n                x2 = int(round(bbox[2]))\n                y2 = int(round(bbox[3]))\n                voc_bbox = (x1, y1, x2, y2)\n                plot_one_box(voc_bbox, \n                             image,\n                             color = color,\n                             label = cls if class_name else str(cls_id),\n                             line_thickness = line_thickness)\n    else:\n        raise ValueError('wrong bbox format')\n\n    return image\n\nnp.random.seed(8)\ncolors = (np.random.randint(0, 255), np.random.randint(0, 255), np.random.randint(0, 255))\ncolors=(255,0,0)","273bbdaf":"# Read in the data CSV files\ndf = pd.read_csv(\"\/kaggle\/input\/tensorflow-great-barrier-reef\/train.csv\")\ndf.head(5)","7bae9383":"df[\"NumBBox\"]=df['annotations'].apply(lambda x: str.count(x, 'x'))\ndf.head(5)","835a5928":"print(df[\"NumBBox\"].unique())","d8cd5058":"df_train=df[df[\"NumBBox\"]>0]\ndf_train.sample(2)","ba221bb4":"print(df_train['NumBBox'].sum())","6da0c85f":"df_train['annotations'] = df_train['annotations'].progress_apply(lambda x: ast.literal_eval(x))\ndf_train['bboxes'] = df_train.annotations.progress_apply(get_bbox)\ndf_train.sample(2)","8866d1b5":"df_train[\"Width\"]=1280\ndf_train[\"Height\"]=720\ndf_train.sample(2)","5835d930":"df_train = df_train.progress_apply(get_path, axis=1)\ndf_train.sample(2)","13a23f07":"df_v = df_train[(df_train.NumBBox==13)].sample(2) \nfig,ax = plt.subplots(1,2,figsize=(30,20))\ni=0;\nfor index, row in df_v.iterrows():\n    img           = load_image(row.image_path)\n    image_height  = row.Height\n    image_width   = row.Width\n    bboxes_coco   = np.array(row.bboxes)\n    bboxes_yolo   = coco2yolo(image_height, image_width, bboxes_coco)\n    names         = ['COTS']*len(bboxes_coco)\n    labels        = [0]*len(bboxes_coco)\n    im=draw_bboxes(img = img,\n                           bboxes = bboxes_yolo, \n                           classes = names,\n                           class_ids = labels,\n                           class_name = True, \n                           colors = colors, \n                           bbox_format = 'yolo',\n                           line_thickness = 2)\n    ax[i].imshow(im)\n    ax[i].axis('OFF')\n    i=i+1","6a68691d":"from sklearn.model_selection import GroupKFold\nkf = GroupKFold(n_splits = 5)\ndf_train = df_train.reset_index(drop=True)\ndf_train['fold'] = -1\nfor fold, (train_idx, val_idx) in enumerate(kf.split(df_train, y = df_train.video_id.tolist(), groups=df_train.sequence)):\n    df_train.loc[val_idx, 'fold'] = fold\ndisplay(df_train.fold.value_counts())","6314e52c":"Selected_Fold=1 #0..4\nt_df = df_train.query(\"fold!=@Selected_Fold\")\nv_df = df_train.query(\"fold==@Selected_Fold\")\nprint(t_df['NumBBox'].sum())\nprint(v_df['NumBBox'].sum())\nprint(v_df['NumBBox'].sum()\/t_df['NumBBox'].sum())\nprint(\"##############################\")\nprint(df_train.groupby('video_id').size())\nprint(\"---------------\")\nprint(t_df.groupby('video_id').size())\nprint(\"---------------\")\nprint(v_df.groupby('video_id').size())\n","1c6e7e79":"os.makedirs('COTS\/images\/train', exist_ok=True)\nos.makedirs('COTS\/images\/valid', exist_ok=True)\nos.makedirs('COTS\/labels\/train', exist_ok=True)\nos.makedirs('COTS\/labels\/valid', exist_ok=True)","0e1fdefe":"for i in tqdm(range(len(df_train))):\n    row = df_train.loc[i]\n    if row.fold != Selected_Fold:\n        copyfile(f'{row.image_path}', f'COTS\/images\/train\/{row.image_id}.jpg')\n    else:\n        copyfile(f'{row.image_path}', f'COTS\/images\/valid\/{row.image_id}.jpg') ","fccf0f7e":"list1 = os.listdir('\/kaggle\/working\/COTS\/images\/train') # dir is your directory path\nnumber_files1 = len(list1)\nprint(\"Number of images in .\/COTS\/images\/train folder\",number_files1)\nlist2 = os.listdir('\/kaggle\/working\/COTS\/images\/valid') # dir is your directory path\nnumber_files2 = len(list2)\nprint(\"Number of images in .\/COTS\/images\/valid folder\",number_files2)","b11fb49a":"import yaml\nwith open('\/kaggle\/working\/train.txt', 'w') as f:\n    for path in glob('\/kaggle\/working\/COTS\/images\/train\/*'):\n        f.write(path+'\\n')\n            \nwith open('\/kaggle\/working\/val.txt', 'w') as f:\n    for path in glob('\/kaggle\/working\/COTS\/images\/valid\/*'):\n        f.write(path+'\\n')\n\ndata = dict(\n    train = '\/kaggle\/working\/COTS\/images\/train',\n    val = '\/kaggle\/working\/COTS\/images\/valid',\n    \n    nc    = 1, # number of classes\n    names =  ['cots'] # classes\n    )\n\nwith open('\/kaggle\/working\/yolov5\/data\/data.yaml', 'w') as outfile:\n    yaml.dump(data, outfile, default_flow_style=False)\n\n%cat \/kaggle\/working\/yolov5\/data\/data.yaml","bfb2b20f":"%%writefile \/kaggle\/working\/hyp.yaml\nlr0: 0.01114\nlrf: 0.10357\nmomentum: 0.91893\nweight_decay: 0.00051\nwarmup_epochs: 3.10145\nwarmup_momentum: 0.85195\nwarmup_bias_lr: 0.08903\nbox: 0.05204\ncls: 0.46428\ncls_pw: 0.96079\nobj: 1.02185\nobj_pw: 1.0346\niou_t: 0.2\nanchor_t: 4.367\nfl_gamma: 0.0\nhsv_h: 0.0167\nhsv_s: 0.71065\nhsv_v: 0.39625\ndegrees: 0.0\ntranslate: 0.09844\nscale: 0.48905\nshear: 0.0\nperspective: 0.0\nflipud: 0.0\nfliplr: 0.5\nmosaic: 0.9581\nmixup: 0.0\ncopy_paste: 0.0","caa3d342":"all_bboxes = []\nfor row_idx in tqdm(range(df_train.shape[0])):\n    row = df_train.iloc[row_idx]\n    # Get image\n    image_name = row.image_id\n    image_height = row.Height\n    image_width  = row.Width\n    bboxes_coco  = np.array(row.bboxes).astype(np.float32).copy()\n    num_bbox     = len(bboxes_coco)\n    names        = ['cots']*num_bbox\n    labels       = [0]*num_bbox\n    if row.fold != Selected_Fold:\n        file_name = f'\/kaggle\/working\/COTS\/labels\/train\/{image_name}.txt'\n    else:\n        file_name = f'\/kaggle\/working\/COTS\/labels\/valid\/{image_name}.txt'\n\n    with open(file_name, 'w') as f:\n        bboxes_yolo  = coco2yolo(image_height, image_width, bboxes_coco)\n        bboxes_yolo  = np.clip(bboxes_yolo, 0, 1)\n        all_bboxes.extend(bboxes_yolo)\n        for bbox_idx in range(len(bboxes_yolo)):\n            bb=str(bboxes_yolo[bbox_idx])\n            bb=bb[1:-1]\n            annot = str(str(labels[bbox_idx])) + ' ' + bb + '\\n'\n            annot = ''.join(annot)\n            annot = annot.strip('')\n            f.write(annot)","52c7497d":"list1 = os.listdir('\/kaggle\/working\/COTS\/labels\/train') # dir is your directory path\nnumber_files1 = len(list1)\nprint(\"Number of txt file in .\/COTS\/labels\/train folder\",number_files1)\nlist2 = os.listdir('\/kaggle\/working\/COTS\/labels\/valid') # dir is your directory path\nnumber_files2 = len(list2)\nprint(\"Number of txt file in .\/COTS\/labels\/valid folder\",number_files2)\n","59887e1e":"%cat '\/kaggle\/working\/COTS\/labels\/train\/{list1[10]}'","630a61e6":"%cd yolov5\/","8e5ad215":"BATCH_SIZE =8\nEPOCHS = 15\nIMG_SIZE=768 #1280","a620eacb":"#best_weights = '\/kaggle\/input\/nfl-weights\/yolov5\/kaggle-reef\/exp\/weights\/best.pt' --weights {best_weights} \\\n!python train.py --img {IMG_SIZE} \\\n                 --batch {BATCH_SIZE} \\\n                 --epochs {EPOCHS} \\\n                 --data data.yaml \\\n                 --hyp \/kaggle\/working\/hyp.yaml\\\n                 --weights yolov5x6.pt \\\n                 --project kaggle-Reef ","8be33954":"%cd \"..\/\"\npath = \"COTS\"\nshutil.rmtree(path)","b25ed5b2":"plt.figure(figsize = (20,10))\nplt.axis('off')\nplt.imshow(plt.imread('\/kaggle\/working\/yolov5\/kaggle-Reef\/exp\/labels.jpg'));\n","53551a22":"import matplotlib.pyplot as plt\nplt.figure(figsize = (15, 15))\nplt.axis('off')\nplt.imshow(plt.imread('\/kaggle\/working\/yolov5\/kaggle-Reef\/exp\/train_batch0.jpg'))\n","7a8d011c":"#!ls \/kaggle\/working\/yolov5\/kaggle-NFL\/exp\nplt.figure(figsize=(10,10))\nplt.axis('off')\nplt.imshow(plt.imread('\/kaggle\/working\/yolov5\/kaggle-Reef\/exp\/P_curve.png'));","d8a32e3a":"plt.figure(figsize=(10,10))\nplt.axis('off')\nplt.imshow(plt.imread('\/kaggle\/working\/yolov5\/kaggle-Reef\/exp\/PR_curve.png'));","acf211a8":"plt.figure(figsize=(10,10))\nplt.axis('off')\nplt.imshow(plt.imread('\/kaggle\/working\/yolov5\/kaggle-Reef\/exp\/F1_curve.png'));","ee8e80b4":"plt.figure(figsize=(10,10))\nplt.axis('off')\nplt.imshow(plt.imread('\/kaggle\/working\/yolov5\/kaggle-Reef\/exp\/R_curve.png'));","d3c5a23d":"ig, ax = plt.subplots(3, 2, figsize = (2*5,3*5), constrained_layout = True)\nfor row in range(3):\n    ax[row][0].imshow(plt.imread(f'\/kaggle\/working\/yolov5\/kaggle-Reef\/exp\/val_batch{row}_labels.jpg'))\n    ax[row][0].set_xticks([])\n    ax[row][0].set_yticks([])\n    ax[row][0].set_title(f'\/kaggle\/working\/yolov5\/kaggle-Reef\/exp\/val_batch{row}_labels.jpg', fontsize = 12)\n    \n    ax[row][1].imshow(plt.imread(f'\/kaggle\/working\/yolov5\/kaggle-Reef\/exp\/val_batch{row}_pred.jpg'))\n    ax[row][1].set_xticks([])\n    ax[row][1].set_yticks([])\n    ax[row][1].set_title(f'\/kaggle\/working\/yolov5\/runs\/kaggle-Reef\/val_batch{row}_pred.jpg', fontsize = 12)","df6ae90d":"plt.figure(figsize=(10,10))\nplt.axis('off')\nplt.imshow(plt.imread('\/kaggle\/working\/yolov5\/kaggle-Reef\/exp\/results.png'));","1f74e365":"plt.figure(figsize=(10,10))\nplt.axis('off')\nplt.imshow(plt.imread('\/kaggle\/working\/yolov5\/kaggle-Reef\/exp\/confusion_matrix.png'));","db065bfc":"# \ud83d\ude85 Train with W&B","74673844":"> We have just 4919 images with 11898 BBox, we will use them in training.","062556b0":"#  \u2b07\ufe0f Download YOLOv5\nClone this repo and install requirements.txt dependencies, including Python>=3.8 and PyTorch>=1.7.","dac41460":"# \ud83d\uddbc\ufe0f Visualizing and Results\n\n* Weights & Biases (W&B) is now integrated with YOLOv5 for real-time visualization and cloud logging of training runs. This allows for better run comparison and introspection, as well improved visibility and collaboration among team members.\n\n* During training you will see live updates at https:\/\/wandb.ai, and you can create Detailed Reports of your results using the W&B Reports tool.\n* To see my project on Weights & Biases (W&B) [here](https:\/\/wandb.ai\/ammaralhajali\/kaggle-Reef)","fe1c92bf":"## R Curve","babd4f8e":"![model_comparison.png](attachment:6f64ed0a-fd0e-43de-9d26-77412d6e87cc.png)","09732b61":"1. https:\/\/www.kaggle.com\/awsaf49\/great-barrier-reef-yolov5-train\n","785c8fb5":"### Please if this kernel is useful, <font color='red'>please upvote !!<\/font>","84d9df5c":"# \ud83c\udf08 Visualizing BBoxes","be63258a":"# \ud83c\udf5c Create `Data.YAML` file\n\nThe `data.yaml`, is the dataset configuration file that defines:\n\n1. the dataset root directory and relative paths to train\/val\/test image directories (or paths to *.txt files with image paths).\n1. the number of classes.\n1. a list of class names.\n\n> \ud83d\udccd Note: The `data.yaml` is created in the `yolov5\/data` directory as required. ","b2d821e6":"![91506361-c7965000-e886-11ea-8291-c72b98c25eec.jpg](attachment:812ff98c-03ef-48f5-b171-0c8b3b0fab54.jpg)","fae9bdc1":"# \ud83c\udf6e Loading Data","b4c1a448":"# \ud83c\udf5a Splitting Dataset","012eb0cc":"![1.JPG](attachment:48f84d42-a426-4503-84fb-e53a8e1693c0.JPG)","05029750":"## \ud83c\udf6e Create Labels for YOLOv5\n\nTo label your images,a `.txt` file with the same name of the image,will be created (if no objects in image, no *.txt file is required)\nThe *.txt file specifications are:\n\n* One row per object\n* Each row is class x_center y_center width height format.\n* Box coordinates must be in normalized xywh format (from 0 - 1). If your boxes are in pixels, divide x_center and width by image width, and y_center and height by image height.\n* Class numbers are zero-indexed (start from 0).\n\n> \ud83d\udccd Note: We don't have to remove the images without bounding boxes from the training or validation sets. ","1a318738":"The label file corresponding to the above image contains 2 persons (class 0) and a tie (class 27):","59e1d647":"![2.JPG](attachment:71cbe291-66c2-4a16-9b1d-47c6d2b9b9ba.JPG)","a1703976":"# References","9730e638":"## \ud83c\udf5a Organize Directories\n\nI organized train and val images and labels according to the example below.\n\n```\n\/Kaggle\/working\n    \/COTS\n         \/images\n             \/train\/img0.jpg\n             \/val\n         \/labels\n             \/train\/img0.txt\n             \/val\n    \/yolov5\n```","6e7a5bde":"All training results are saved to runs\/train\/ with incrementing run directories, i.e. runs\/train\/exp2, runs\/train\/exp3 etc. ","eee0f960":"# Path of Images","4296e007":"### Hi kagglers, This is `Training` notebook using `YOLOv5`.\n\n\n### Other notebooks in the competition\n- [Barrier Reef YOLOv5 [Inference]](https:\/\/www.kaggle.com\/ammarnassanalhajali\/barrier-reef-yolov5-inference\/edit)\n\n\n\n### Please if this kernel is useful, <font color='red'>please upvote !!<\/font>","f4ad29f8":"### GT Vs Pred","5a8f8876":"# \ud83d\udcda YOLOv5\nYOLO, \"You Only Look Once\", has a long and succesful history with real time object detection.","60997c99":"# \ud83d\udce6 Select a Model\nSelect a pretrained model to start training from. \n* Here we select YOLOv5s, the smallest and fastest model available.\n* I will try YOLO5s","7d3e4652":"### Class Distribution","326fde14":"### \ud83c\udf58 Hyperparameters","06cc5779":"![10.png](attachment:caf5c201-af01-4c90-b306-3e6e43787992.png)","e2085f4f":"### Batch Image","51404df1":"# Size of Images\n##### \ud83d\udccc Note \n> All images have Width=1280 & Height=720 ","4acfe0ab":"# \ud83d\ude80 Barrier Reef YOLOv5 [Training]","6f08c66b":"# \u2600\ufe0f Importing Libraries","b4496f9d":"\n# BBoxes\n##### \ud83d\udccc Note \n> We can see there are many images without any BBox. ","972320e2":"# \ud83d\udd28 Functions","02caa2a7":"## F1 Curve","9644cc29":"# Removing Files","89380651":"![download.jpg](attachment:07de9c65-7c16-40e7-a821-d5354296394c.jpg)","8c5f0c9a":"# \ud83d\udd28 Weights & Biases\n* Weights & Biases is a set of tools that tracks machine learning experiments, visualizes metrics, and shares results.\n* Weights & Biases is directly integrated into YOLOv5, providing experiment metric tracking, model and dataset versioning, rich model prediction visualization, and more.","5cda0609":"### Confusion Matrix","e119e33d":"### (Loss, Map) Vs Epoch\n","c2d914d2":"## P Curve","f29d6866":"## PR Curve"}}