{"cell_type":{"11a1db9d":"code","1a2c6bf3":"code","6b2931cc":"code","c3b9b20e":"code","357780b2":"code","e45623da":"code","ad6cc8fb":"code","c270f490":"code","798a622e":"code","0d88aabe":"code","af9d8c83":"code","2492a67c":"code","273e27dd":"code","07594f10":"code","5999ca3a":"code","20affce2":"code","f8b3d498":"code","501d3fb6":"code","3a927c63":"code","54a935d9":"code","cc98616f":"code","7e49a3ab":"code","86eb7700":"code","bf7d4abb":"code","9a8e6eb2":"code","4b48f02b":"code","07aeb565":"code","ddb1834a":"code","5108f78f":"code","d457f7d7":"code","44be0931":"code","5b82c55a":"code","1c052491":"code","5716414b":"code","0f134e14":"code","986b4f07":"code","a7e7211f":"code","3ce707c2":"code","38b8f370":"code","d4d0c8bd":"code","595b39d0":"code","e52ab71b":"code","7a90d653":"code","36b29b82":"code","68a08e38":"code","109216fd":"code","62521600":"code","678d05ce":"code","9ff247b3":"code","1e083c3d":"code","ae20ed03":"code","818df003":"code","df798082":"code","868287b5":"code","08ca2cd8":"code","3fe6d5b6":"code","90995b44":"code","bb74078c":"code","83d738ca":"code","b4666d1a":"code","01232a4b":"code","94083a29":"code","4ef4cfa7":"code","aabe9026":"code","628ba901":"code","f8cf3492":"code","f39e4a60":"code","4884c0b9":"code","dfdb21cb":"code","1beb0e68":"code","3d4a8600":"code","a37a2627":"code","1b8f3e6c":"code","27884703":"markdown","9afc99de":"markdown","e7c8cf29":"markdown","0109faf4":"markdown","ccc686f7":"markdown","8ad2577e":"markdown","e269e39c":"markdown","14c7dc7e":"markdown","da9339b1":"markdown","a16ef5c5":"markdown","681b366a":"markdown","f8e8d6b7":"markdown","d6f3b05a":"markdown","73e92a38":"markdown","125806d1":"markdown","8d2b1995":"markdown","2aeaeaa8":"markdown","30f976ce":"markdown","5fcdaab4":"markdown","68ed4f98":"markdown","20361e0b":"markdown","a2470c63":"markdown","0ef19add":"markdown","019c89dc":"markdown","c6b14436":"markdown","88e79ddf":"markdown","131412ce":"markdown","ab893e6f":"markdown","1195aa17":"markdown"},"source":{"11a1db9d":"import os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","1a2c6bf3":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns","6b2931cc":"users = pd.read_csv('\/kaggle\/input\/telecom-users-dataset\/telecom_users.csv')\nusers.head()","c3b9b20e":"#We will remove the columns from the dataset which does not affect the Churn\n\nusers.drop(['Unnamed: 0','customerID'], axis = 1, inplace = True)","357780b2":"users.info()","e45623da":"#We will drop the rows where there is no value in Total Charges Column\n\nfor i in range(len(users)):\n        if users['TotalCharges'][i] == \" \":\n            users.drop(i, inplace = True)","ad6cc8fb":"# Converting the data type of Total Charges column from object to float\n\nusers['TotalCharges'] = users['TotalCharges'].apply(lambda x: float(x))","c270f490":"users[['tenure','MonthlyCharges','TotalCharges']].describe().T","798a622e":"users.columns","0d88aabe":"plt.figure(figsize = (8,6))\nsns.countplot(data = users, x = 'gender', hue = 'Churn')","af9d8c83":"plt.figure(figsize = (8,6))\nsns.countplot(data = users, x = 'SeniorCitizen', hue = 'Churn')","2492a67c":"plt.figure(figsize=(18,6))\nplt.subplot(1,2,1)\nsns.countplot(data = users[users['SeniorCitizen'] == 1], x = 'Partner', hue = 'Churn')\nplt.xlabel('Senior Citizen')\nplt.title('Partner Status')\nplt.subplot(1,2,2)\nsns.countplot(data = users[users['SeniorCitizen'] == 0], x = 'Partner', hue = 'Churn')\nplt.xlabel('Not a Senior Citizen')\nplt.title('Partner Status')","273e27dd":"plt.figure(figsize=(18,6))\nplt.subplot(1,2,1)\nsns.countplot(data = users[users['SeniorCitizen'] == 1], x = 'Dependents', hue = 'Churn')\nplt.xlabel('Senior Citizen')\nplt.title('Dependents Status')\nplt.subplot(1,2,2)\nsns.countplot(data = users[users['SeniorCitizen'] == 0], x = 'Dependents', hue = 'Churn')\nplt.xlabel('Not a Senior Citizen')\nplt.title('Dependents Status')","07594f10":"plt.figure(figsize = (12,8))\nsns.boxplot(y = 'tenure', x = 'SeniorCitizen', data = users, hue = 'Churn')","5999ca3a":"plt.figure(figsize=(18,6))\nplt.subplot(1,2,1)\nsns.countplot(data = users[users['SeniorCitizen'] == 1], x = 'PhoneService', hue = 'Churn')\nplt.xlabel('Senior Citizen')\nplt.title('Phone Service Status')\nplt.subplot(1,2,2)\nsns.countplot(data = users[users['SeniorCitizen'] == 0], x = 'PhoneService', hue = 'Churn')\nplt.xlabel('Not a Senior Citizen')\nplt.title('Phone Service Status')","20affce2":"plt.figure(figsize=(18,6))\nplt.subplot(1,2,1)\nsns.countplot(data = users[users['SeniorCitizen'] == 1], x = 'InternetService', hue = 'Churn')\nplt.xlabel('Senior Citizen')\nplt.title('Internet Service Status')\nplt.subplot(1,2,2)\nsns.countplot(data = users[users['SeniorCitizen'] == 0], x = 'InternetService', hue = 'Churn')\nplt.xlabel('Not a Senior Citizen')\nplt.title('Internet Service Status')","f8b3d498":"plt.figure(figsize = (12,8))\nsns.boxplot(y = 'MonthlyCharges', x = 'SeniorCitizen', data = users, hue = 'Churn')","501d3fb6":"plt.figure(figsize = (12,8))\nsns.boxplot(y = 'TotalCharges', x = 'SeniorCitizen', data = users, hue = 'Churn')","3a927c63":"#Converting Categorical Variables into Numeric numbers\n\nX = pd.get_dummies(data = users, columns=['gender','Partner','Dependents','PhoneService','MultipleLines','InternetService','OnlineSecurity','OnlineBackup','DeviceProtection','TechSupport','StreamingTV','StreamingMovies','Contract','PaperlessBilling','PaymentMethod','Churn'],drop_first=True )","54a935d9":"X.head()","cc98616f":"y = X['Churn_Yes']","7e49a3ab":"X.drop(['Churn_Yes'], axis = 1, inplace = True)","86eb7700":"plt.figure(figsize = (18,10))\nsns.heatmap(X.corr(), cmap = 'magma', annot = True)","bf7d4abb":"#Remove the columns with correlation of 1 to address multicollinearity\nX.drop(['OnlineSecurity_No internet service','OnlineBackup_No internet service','DeviceProtection_No internet service','TechSupport_No internet service','StreamingTV_No internet service','StreamingMovies_No internet service'], axis = 1, inplace = True)","9a8e6eb2":"#Remove the columns with correlation of -1to address multicollinearity\nX.drop(['MultipleLines_No phone service'], axis = 1, inplace = True)","4b48f02b":"plt.figure(figsize = (18,10))\nsns.heatmap(X.corr(), cmap = 'magma', annot = True)","07aeb565":"#Intiating Scaler\nfrom sklearn.preprocessing import StandardScaler\nscaler = StandardScaler()","ddb1834a":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state = 101)","5108f78f":"X_train = scaler.fit_transform(X_train)","d457f7d7":"X_test = scaler.transform(X_test)","44be0931":"from sklearn.linear_model import LogisticRegression\nlogmodel = LogisticRegression()\nlogmodel.fit(X_train,y_train)\nlog_predictions = logmodel.predict(X_test)\n","5b82c55a":"from sklearn.metrics import classification_report, confusion_matrix","1c052491":"# Printing Confusion Matrix\npd.DataFrame(confusion_matrix(y_test,log_predictions))","5716414b":"    #Printing Classification Report\n    print(classification_report(y_test,log_predictions))","0f134e14":"from sklearn.neighbors import KNeighborsClassifier\nknn = KNeighborsClassifier(n_neighbors=10)\nknn.fit(X_train,y_train)\nknn_pred = knn.predict(X_test)","986b4f07":"# Printing Confusion Matrix\npd.DataFrame(confusion_matrix(y_test,knn_pred))","a7e7211f":"print(classification_report(y_test,knn_pred))","3ce707c2":"from sklearn.tree import DecisionTreeClassifier\ndtree = DecisionTreeClassifier()\ndtree.fit(X_train,y_train)\ndtree_predictions = dtree.predict(X_test)","38b8f370":"pd.DataFrame(confusion_matrix(y_test,dtree_predictions))","d4d0c8bd":"print(classification_report(y_test,dtree_predictions))","595b39d0":"from sklearn.ensemble import RandomForestClassifier\nrfc = RandomForestClassifier(n_estimators=500)\nrfc.fit(X_train, y_train)\nrfc_pred = rfc.predict(X_test)","e52ab71b":"#Printing Confusion Matrix\npd.DataFrame(confusion_matrix(y_test,rfc_pred))","7a90d653":"print(classification_report(y_test,rfc_pred))","36b29b82":"from sklearn.svm import SVC\nmodel = SVC()\nmodel.fit(X_train,y_train)\nsvm_predictions = model.predict(X_test)\n","68a08e38":"#Printing Confusion Matrix\npd.DataFrame(confusion_matrix(y_test,svm_predictions))","109216fd":"# Printing Classification Report\nprint(classification_report(y_test,rfc_pred))","62521600":"import tensorflow as tf\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, Activation,Dropout","678d05ce":"y_train = y_train.values\ny_test = y_test.values","9ff247b3":"from tensorflow.keras.callbacks import EarlyStopping","1e083c3d":"early_stop = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=50)","ae20ed03":"model = Sequential()\n\nmodel.add(Dense(units=22,activation='relu'))\nmodel.add(Dropout(0.3))\n\nmodel.add(Dense(units=22,activation='relu'))\nmodel.add(Dropout(0.3))\n\nmodel.add(Dense(units=1,activation='sigmoid'))\n\nmodel.compile(loss='binary_crossentropy', optimizer='adam')","818df003":"model.fit(x=X_train, \n          y=y_train, \n          epochs=600,\n          validation_data=(X_test, y_test), verbose=1,\n          callbacks=[early_stop]\n          )","df798082":"model_loss = pd.DataFrame(model.history.history)","868287b5":"model_loss.plot()","08ca2cd8":"ann_predictions = model.predict_classes(X_test)","3fe6d5b6":"pd.DataFrame(confusion_matrix(y_test,ann_predictions))","90995b44":"print(classification_report(y_test,ann_predictions))","bb74078c":"from imblearn.over_sampling import SMOTE","83d738ca":"smote = SMOTE(sampling_strategy='minority')","b4666d1a":"X_sm, y_sm = smote.fit_resample(X,y)","01232a4b":"y_sm.value_counts()","94083a29":"\nX_train, X_test, y_train, y_test = train_test_split(X_sm, y_sm, test_size=0.2, random_state = 101, stratify = y_sm)","4ef4cfa7":"scaler = StandardScaler()\nX_train = scaler.fit_transform(X_train)\nX_test = scaler.transform(X_test)","aabe9026":"logmodel = LogisticRegression()\nlogmodel.fit(X_train,y_train)\nlog_predictions = logmodel.predict(X_test)\n","628ba901":"# Printing Confusion Matrix\npd.DataFrame(confusion_matrix(y_test,log_predictions))","f8cf3492":"#Printing Classification Report\nprint(classification_report(y_test,log_predictions))","f39e4a60":"y_train = y_train.values\ny_test = y_test.values","4884c0b9":"\nmodel = Sequential()\n\nmodel.add(Dense(units=22,activation='relu'))\nmodel.add(Dropout(0.3))\n\nmodel.add(Dense(units=22,activation='relu'))\nmodel.add(Dropout(0.3))\n\nmodel.add(Dense(units=1,activation='sigmoid'))\n\nmodel.compile(loss='binary_crossentropy', optimizer='adam')","dfdb21cb":"model.fit(x=X_train, \n          y=y_train, \n          epochs=600,\n          validation_data=(X_test, y_test), verbose=1,\n          callbacks=[early_stop]\n          )","1beb0e68":"model_loss = pd.DataFrame(model.history.history)\n\nmodel_loss.plot()","3d4a8600":"ann_predictions = model.predict_classes(X_test)","a37a2627":"pd.DataFrame(confusion_matrix(y_test,ann_predictions))\n","1b8f3e6c":"print(classification_report(y_test,ann_predictions))","27884703":"# **Deep Learning Models**","9afc99de":"# Insight\nChurn is extremely high for Senior Citizen Users. **The company should evaluate why there is such a high churn for this segment.**\n","e7c8cf29":"**Conclusion**\nChurn is almost the same for each of the Gender","0109faf4":"# **Insight**\nHigher tenure users have lower propensity to exit.","ccc686f7":"# > **Deep Learning(Using SMOTE)**\n\nNow let us see if our Deep-Learning model performs better using the balanced data-set.","8ad2577e":"# **Insight**\nFibre Optic users have higher Churn Rate","e269e39c":"# **Comments on SVM Model**\n\nRecall and Accurance both are lower for SVM model as compared to Logistic Regression.","14c7dc7e":"# > **Logistic Regression(Using Smote)**","da9339b1":"# **Insight**\nCounterintutively, users which higher Total Charges have lower churn rate whereas users with lower Total Charges have higher churn rate","a16ef5c5":"**Now that we know that behaviour of Senior Citizens is completely different from others, lets further break-down our analysis for rest of the columns.**","681b366a":"There are 5986 rows and there are no null cells. However, Data tye for Total Charges is Object instead of float. We will change that to float now.","f8e8d6b7":"> # Logistic Regression****","d6f3b05a":"# > SVM Model****","73e92a38":"# **Comments on Logistic Regression**\n\nLogistic model is good at correctly predicting the users which would not leave the company. However, it can only predict around 52% of the users which would leave the company. Overall accurance is 81%","125806d1":"# **Data Pre-Processing**","8d2b1995":"# **Insight**\nChurn of Senior Citizen Users is quite similiar for the Monthly Charges however, for Non Senior Citizen users Churn is higher for users where the Monthly Charges is higher.","2aeaeaa8":"# > **Comments on Random Forest**\nPerformance of Decision Tree is inferior to Logistic regression. Recall and Accuracy both are lower.","30f976ce":"# > **K Nearest Neighbours**","5fcdaab4":"# **Comments on K Nearest Neighbours**\n\nPerformance of K Nearest Neighbours is inferior to Logistic regression.","68ed4f98":"# EDA: Let us explore the churn vis-a-vis each of the columns","20361e0b":"# **Comments on Decision Tree**\nPerformance of Decision Tree is inferior to Logistic regression & K Nearest Neighbour. Recall and Accuracy both are lower.","a2470c63":"# **Insight**\nSenior Citizen users which also have dependents have unsually high churn rate while for Non Senior Citizen users, churn rate is comparatively higher which do not have dependents.","0ef19add":"# **Insight**\nSenior Citizen users which also have partners have unsually high churn rate while for Non Senior Citizen users, churn rate is comparatively higher which do not have partners.","019c89dc":"# > **Logistic Regression(Using Smote) - Conclusion**\n\nUsing of Smote has not increased the overall accuracy substantially, however the recall for value 1 has increased from 59% to 83%, which is a substantial increase and would be more suitable to identify and target the customers which have higher propensity to leave the company.","c6b14436":"# > **Conclusion - 1**\n\nOverall Logistic Regression performed the best. Accurancy level in predicting the customers who would leave the company is only around 50%. However this should give some leads to the company's maangement to take corrective actions for those 50% of the users.","88e79ddf":"# > Random Forest Classifier****","131412ce":"# > **Deep-Learning(Using SMOTE) - Conclusion**\n\nPerformance of the Deep-learning model has substantially improved however performance of logistic regression is marginally better.","ab893e6f":"**SMOTE**\nCurrently, the data that we had used was Imbalanced. Now, lets try to use SMOTE to balance the data and see if we can improve our results","1195aa17":"# > Decision Tree****"}}