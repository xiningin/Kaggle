{"cell_type":{"bd6a9052":"code","6069af0e":"code","f7b1ee75":"code","ef9b0dfc":"code","238ff4e1":"code","5e704313":"code","c5dc61bc":"code","2c0b85e0":"code","0f7fcdd1":"code","9dc6b3db":"code","bab34aea":"code","28910e9c":"code","2aa16d63":"code","6ef6d777":"code","f6ecd657":"code","3ea8ec77":"code","6c42e0b8":"code","6ead040c":"code","13c79d1c":"code","8ab2224e":"code","0feb8408":"code","437077db":"markdown","b7cfcfb8":"markdown","1a65da7a":"markdown","fc414e62":"markdown","ba5c9f36":"markdown","cda848be":"markdown","c801a5ba":"markdown","a516bce0":"markdown","0f7b10d5":"markdown","90841d6b":"markdown"},"source":{"bd6a9052":"#Importing Necessary Libraries\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.metrics import confusion_matrix,plot_confusion_matrix\n\n","6069af0e":"#Importing dataset\ndf = pd.read_csv('..\/input\/gender\/gender_classification_v7.csv')\n","f7b1ee75":"df.head()","ef9b0dfc":"df['gender'].value_counts()","238ff4e1":"df.info()\n","5e704313":"#correlation matrix and the heatmap\nplt.subplots(figsize=(12,5))\ngender_correlation=df.corr()\nsns.heatmap(gender_correlation,annot=True,cmap='RdPu')\nplt.title('Correlation between the variables')\nplt.xticks(rotation=45)\n","c5dc61bc":"sns.lineplot(data=df, x=\"distance_nose_to_lip_long\", y=\"lips_thin\")","2c0b85e0":"sns.lineplot(data=df, x=\"forehead_width_cm\", y=\"forehead_height_cm\")","0f7fcdd1":"sns.lineplot(data=df, x=\"long_hair\", y=\"forehead_width_cm\")\n","9dc6b3db":"males = df.query(\" gender == 'Male' \")\nmales.groupby('nose_wide')['nose_wide'].describe()","bab34aea":"sns.histplot(data = males , x = 'nose_wide')","28910e9c":"females = df.query(\" gender == 'Female' \")\nfemales.groupby('nose_wide')['nose_wide'].describe()","2aa16d63":"sns.histplot(data = females , x = 'nose_wide')","6ef6d777":"x = df.drop('gender', axis=1)\ny = df['gender']","f6ecd657":"from sklearn.model_selection import train_test_split\nx_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.3,random_state=100)","3ea8ec77":"from sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.metrics import accuracy_score, plot_confusion_matrix","6c42e0b8":"knn = KNeighborsClassifier()\nfrom sklearn.model_selection import GridSearchCV\nk_range = list(range(1, 31))\nparam_grid = dict(n_neighbors=k_range)\n\n# defining parameter range\ngrid = GridSearchCV(knn, param_grid, cv=10, scoring='accuracy', return_train_score=False,verbose=1)\n\n# fitting the model for grid search\ngrid_search=grid.fit(x_train, y_train)","6ead040c":"print(grid_search.best_params_)","13c79d1c":"accuracy = grid_search.best_score_ *100\nprint(\"Accuracy for our training dataset with tuning is : {:.2f}%\".format(accuracy))","8ab2224e":"knn = KNeighborsClassifier(n_neighbors=15)\nknn.fit(x, y)\n\ny_test_hat=knn.predict(x_test) \n\ntest_accuracy=accuracy_score(y_test,y_test_hat)*100\n\nprint(\"Accuracy for our testing dataset with tuning is : {:.2f}%\".format(test_accuracy) )","0feb8408":"plot_confusion_matrix(grid,x_test, y_test,values_format='d' )\n","437077db":"**Model fitting with K-cross Validation and GridSearchCV**\n\nK-cross Validation-  \n\n\n\n\nGridSearchCV-","b7cfcfb8":"**Splitting Dataset into Training and Testing set**","1a65da7a":"**Data Preprocessing**","fc414e62":"**Checking Accuracy on Test Data**","ba5c9f36":"**Reading Dataset**","cda848be":"# How KNN Classification Works?\n\n\n\n![](https:\/\/machinelearningknowledge.ai\/wp-content\/uploads\/2021\/07\/Sklearn-KNN-Classifier.gif)\n\n**Important point**\n\nThere is another aspect of the choice of the value of \u2018K\u2019 that can produce different results for different values of K. Hence hyperparameter tuning of K becomes an important role in producing a robust KNN classifier. In Sklearn we can use GridSearchCV to find the best value of K from the range of values.","c801a5ba":"![image.png](attachment:3b993e3e-63df-4f6b-b4ba-bd9050c4e5a1.png)\n\n**It represents the different combinations of Actual VS Predicted values. Let\u2019s define them one by one.**\n\nTP: True Positive: The values which were actually positive and were predicted positive.\n\nFP: False Positive: The values which were actually negative but falsely predicted as positive. Also known as Type I Error.\n\nFN: False Negative: The values which were actually positive but falsely predicted as negative. Also known as Type II Error.\n\nTN: True Negative: The values which were actually negative and were predicted negative.","a516bce0":"**Exploratory Data Analysis**","0f7b10d5":"> About Gender Dataset\n\n1.long_hair : it is 1 if he\/she has long hair or 0 if he\/she haven\u2019t\n\n2.forehead_width_cm : forehead width in cm\n\n3.forehead_height_cm : forehead height in cm\n\n4.nose_long : it is 1 if he\/she has a long nose or 0 if he\/she haven\u2019t\n\n5.nose_wide : it is 1 if he\/she has a wide nose or 0 if he\/she haven\u2019t\n\n6.lips_thin : it is 1 if he\/she has thin lips or 0 if he\/she haven\u2019t\n\n7.distance_nose_to_lip_long: it is 1 if there is a long distance between lips and nose or 0 if there isn\u2019t this long distance between them\n\n8.Gender(Target column): We will use the other 7 features of the dataset in order to make inferences and predictions regarding the gender of any given individual.\n\n","90841d6b":"# We will use a gender dataset to classify as male or female based on facial features with the KNN classifier in Sklearn."}}