{"cell_type":{"1d5eefae":"code","a761f25a":"code","21aad818":"code","9f47af4e":"code","7e273323":"code","f55a4cc8":"code","c8e14d0b":"code","9b1f402a":"code","49a3d781":"code","a662c7be":"code","fd632dca":"code","01228060":"code","7ca7b94e":"code","e2f96e8c":"code","c8b66d13":"code","60a95a4d":"code","925072f8":"code","f0112fd0":"code","1bf06ee9":"code","0d335be8":"code","ce678f71":"code","1341c3e5":"code","6588a498":"code","42dd2f00":"code","65c43552":"code","bd9b9961":"code","168277fe":"code","14c82b73":"code","ecc71b5c":"code","b9a481b7":"code","91254b2a":"code","9579b902":"code","11466a61":"code","7ff97c9c":"code","62d7029a":"code","45e48843":"code","1a3778e2":"code","c995472b":"code","96929c08":"code","04da40f1":"code","6b3237d5":"code","1e167776":"code","0f7f4f8b":"code","2ab80cfa":"code","70407065":"code","3fdf5d54":"code","22ee3c50":"code","2d5f2447":"code","40538ddf":"code","90eb1997":"code","8a4dcfb2":"code","11051e46":"code","da28b0b2":"code","722e5a30":"code","5591b9e1":"code","14739b70":"code","5254548f":"code","ecc7ab54":"code","3ce0d569":"code","2acd9179":"code","489d1c8c":"code","428d8a19":"code","09690500":"code","cf5fdb23":"code","342bf00d":"code","dcca7660":"code","4c3941a6":"code","cef73411":"code","ca3c9498":"code","4283802f":"code","b6ce0f2c":"code","d6e18192":"code","b8b1af84":"code","4feab375":"code","60097ef2":"code","026c29c0":"code","4a0617f6":"code","1f43899b":"code","bc518f60":"code","4f4bebb7":"code","8c63efaa":"code","1e0797be":"code","971b6d3b":"code","8dff3956":"markdown","9cc03ba5":"markdown","4317f33f":"markdown","d450dac6":"markdown","5df19e02":"markdown","1735ca45":"markdown","f79cd49a":"markdown","cfbe1014":"markdown","25483de6":"markdown","40c84bea":"markdown","41888fc9":"markdown","eb59e478":"markdown","eed8e4cc":"markdown","8d06582e":"markdown","7b4b09cf":"markdown","9c4e21e6":"markdown","ed768186":"markdown","5b69fd88":"markdown","f3498014":"markdown","2048fb51":"markdown","c29560a3":"markdown","e08bc2de":"markdown","df5b4bef":"markdown","60011eab":"markdown","d2056688":"markdown","6313bdca":"markdown","a9340550":"markdown","d1cdb6cd":"markdown","9fed47db":"markdown","a2bda389":"markdown"},"source":{"1d5eefae":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","a761f25a":"import numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt","21aad818":"train=pd.read_csv(\"\/kaggle\/input\/nlp-getting-started\/train.csv\")\ntest=pd.read_csv(\"\/kaggle\/input\/nlp-getting-started\/test.csv\")\nsample=pd.read_csv(\"\/kaggle\/input\/nlp-getting-started\/sample_submission.csv\")","9f47af4e":"train.head() ","7e273323":"train.columns","f55a4cc8":"test.head()","c8e14d0b":"test.columns","9b1f402a":"# Dividing the dataset according to the target\ntweetsDisaster = train[train['target']==1]['text']\nprint('<Disaster> : ',tweetsDisaster.values[1] )\n","49a3d781":"tweetsNotDisaster = train[train['target']==0]['text']\nprint('<Not a disaster> : ',tweetsNotDisaster.values[1])","a662c7be":"data_info=pd.DataFrame(train.dtypes).T.rename(index={0:'column type'})\ndata_info=data_info.append(pd.DataFrame(train.isnull().sum()).T.rename(index={0:'null values (nb)'}))\ndata_info=data_info.append(pd.DataFrame(train.isnull().sum()\/train.shape[0]*100).T.\n                         rename(index={0:'null values (%)'}))\ndisplay(data_info)","fd632dca":"data_info=pd.DataFrame(test.dtypes).T.rename(index={0:'column type'})\ndata_info=data_info.append(pd.DataFrame(test.isnull().sum()).T.rename(index={0:'null values (nb)'}))\ndata_info=data_info.append(pd.DataFrame(test.isnull().sum()\/test.shape[0]*100).T.\n                         rename(index={0:'null values (%)'}))\ndisplay(data_info)\n","01228060":"import plotly.express as px\nfig = px.bar(train, x=['Not Real','Real'] , y=train['target'].value_counts(),height=400,template=\"plotly_dark\")\n\nfig.update_layout(\n    autosize=False,\n    width=500,\n    height=500,\n    )\nfig.show()","7ca7b94e":"train['target'].value_counts()","e2f96e8c":"sns.barplot(train['target'].value_counts().index,train['target'].value_counts())","c8b66d13":"from wordcloud import WordCloud,STOPWORDS \nstopwords = set(STOPWORDS) \nstop_word=list(stopwords)+['http','co','https','wa','amp','\u00fb','\u00db','HTTP','HTTPS']\n\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=[26, 8])\nwordcloud1 = WordCloud( background_color='white',stopwords = stop_word,\n                        width=600,\n                        height=400).generate(\" \".join(train[train['target']==1]['text']))\nax1.imshow(wordcloud1)\nax1.axis('off')\nax1.set_title('Disaster Tweets',fontsize=40);\n\nwordcloud2 = WordCloud( background_color='white',stopwords = stop_word,\n                        width=600,\n                        height=400).generate(\" \".join(train[train['target']==0]['text']))\nax2.imshow(wordcloud2)\nax2.axis('off')\nax2.set_title('Non Disaster Tweets',fontsize=40);","60a95a4d":"train['keyword'].unique()","925072f8":"sns.barplot(y=train['keyword'].value_counts()[:20].index,x=train['keyword'].value_counts()[:20],orient='h')","f0112fd0":"#les pays qui contiennet le plus des menaces ordonn\u00e9s\nsns.barplot(y=train['location'].value_counts()[:10].index,x=train['location'].value_counts()[:10],\n            orient='h')","1bf06ee9":"fig,(ax1,ax2)=plt.subplots(1,2,figsize=(10,5))\ntweet_len=train[train['target']==1]['text'].str.len()\nax1.hist(tweet_len,color='red')\nax1.set_title('disaster tweets')\ntweet_len=train[train['target']==0]['text'].str.len()\nax2.hist(tweet_len,color='green')\nax2.set_title('Not disaster tweets')\nfig.suptitle('Characters in tweets')\nplt.show()","0d335be8":"fig,(ax1,ax2)=plt.subplots(1,2,figsize=(10,5))\nword=train[train['target']==1]['text'].str.split().apply(lambda x : [len(i) for i in x])\nsns.distplot(word.map(lambda x: np.mean(x)),ax=ax1,color='red')\nax1.set_title('disaster')\nword=train[train['target']==0]['text'].str.split().apply(lambda x : [len(i) for i in x])\nsns.distplot(word.map(lambda x: np.mean(x)),ax=ax2,color='green')\nax2.set_title('Not disaster')\nfig.suptitle('Average word length in each tweet')","ce678f71":"def create_corpus(target):\n    corpus=[]\n    \n    for x in train[train['target']==target]['text'].str.split():\n        for i in x:\n            corpus.append(i)\n    return corpus","1341c3e5":"from collections import defaultdict\nfrom collections import  Counter\ncorpus1=create_corpus(1)\n\ndic=defaultdict(int)\nfor word in corpus1:\n    if word in corpus1:\n        dic[word]+=1\n\ntop=sorted(dic.items(), key=lambda x:x[1],reverse=True)[:10] \n    \nx,y=zip(*top)\nplt.bar(x,y)","6588a498":"plt.figure(figsize=(10,5))\ncorpus=create_corpus(0)\n\ndic=defaultdict(int)\nimport string\nspecial = string.punctuation\nfor i in (corpus):\n    if i in special:\n        dic[i]+=1\n        \nx,y=zip(*dic.items())\nplt.bar(x,y,color='green')","42dd2f00":"import re\ndef remove_URL(text):\n    url = re.compile(r'https?:\/\/\\S+|www\\.\\S+')\n    return url.sub(r'',text)","65c43552":"def remove_URLs(text):\n    url = re.compile(r'http?:\/\/\\S+|www\\.\\S+')\n    return url.sub(r'',text)","bd9b9961":"train['text'] = train['text'].apply(remove_URL)\ntest['text'] = test['text'].apply(remove_URL)","168277fe":"train['text'] = train['text'].apply(remove_URLs)\ntest['text'] = test['text'].apply(remove_URLs)","14c82b73":"train['text']","ecc71b5c":"def remove_emoji(text):\n    emoji_pattern = re.compile(\"[\"\n                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n                           u\"\\U00002702-\\U000027B0\"\n                           u\"\\U000024C2-\\U0001F251\"\n                           \"]+\", flags=re.UNICODE)\n    return emoji_pattern.sub(r'', text)","b9a481b7":"train['text'] = train['text'].apply(remove_emoji)\ntest['text'] = test['text'].apply(remove_emoji)","91254b2a":"train['text']","9579b902":"contractions = { \n\"ain't\": \"am not\",\n\"aren't\": \"are not\",\n\"can't\": \"cannot\",\n\"can't've\": \"cannot have\",\n\"'cause\": \"because\",\n\"could've\": \"could have\",\n\"couldn't\": \"could not\",\n\"couldn't've\": \"could not have\",\n\"didn't\": \"did not\",\n\"doesn't\": \"does not\",\n\"don't\": \"do not\",\n\"hadn't\": \"had not\",\n\"hadn't've\": \"had not have\",\n\"hasn't\": \"has not\",\n\"haven't\": \"have not\",\n\"he'd\": \"he would\",\n\"he'd've\": \"he would have\",\n\"he'll\": \"he will\",\n\"he's\": \"he is\",\n\"how'd\": \"how did\",\n\"how'll\": \"how will\",\n\"how's\": \"how is\",\n\"i'd\": \"i would\",\n\"i'll\": \"i will\",\n\"i'm\": \"i am\",\n\"i've\": \"i have\",\n\"isn't\": \"is not\",\n\"it'd\": \"it would\",\n\"it'll\": \"it will\",\n\"it's\": \"it is\",\n\"let's\": \"let us\",\n\"ma'am\": \"madam\",\n\"mayn't\": \"may not\",\n\"might've\": \"might have\",\n\"mightn't\": \"might not\",\n\"must've\": \"must have\",\n\"mustn't\": \"must not\",\n\"needn't\": \"need not\",\n\"oughtn't\": \"ought not\",\n\"shan't\": \"shall not\",\n\"sha'n't\": \"shall not\",\n\"she'd\": \"she would\",\n\"she'll\": \"she will\",\n\"she's\": \"she is\",\n\"should've\": \"should have\",\n\"shouldn't\": \"should not\",\n\"that'd\": \"that would\",\n\"that's\": \"that is\",\n\"there'd\": \"there had\",\n\"there's\": \"there is\",\n\"they'd\": \"they would\",\n\"they'll\": \"they will\",\n\"they're\": \"they are\",\n\"they've\": \"they have\",\n\"wasn't\": \"was not\",\n\"we'd\": \"we would\",\n\"we'll\": \"we will\",\n\"we're\": \"we are\",\n\"we've\": \"we have\",\n\"weren't\": \"were not\",\n\"what'll\": \"what will\",\n\"what're\": \"what are\",\n\"what's\": \"what is\",\n\"what've\": \"what have\",\n\"where'd\": \"where did\",\n\"where's\": \"where is\",\n\"who'll\": \"who will\",\n\"who's\": \"who is\",\n\"won't\": \"will not\",\n\"wouldn't\": \"would not\",\n\"you'd\": \"you would\",\n\"you'll\": \"you will\",\n\"you're\": \"you are\",\n\"thx\"   : \"thanks\"\n}","11466a61":"def remove_contractions(text):\n    return contractions[text.lower()] if text.lower() in contractions.keys() else text","7ff97c9c":"train['text']=train['text'].apply(remove_contractions)\ntest['text']=test['text'].apply(remove_contractions)","62d7029a":"train['text']","45e48843":"import string\ndef remove_punct(text):\n    table=str.maketrans('','',string.punctuation)\n    return text.translate(table)","1a3778e2":"train['text'] = train['text'].apply(remove_punct)\ntest['text'] = test['text'].apply(remove_punct)","c995472b":"train['text']","96929c08":"freq = pd.Series(' '.join(train['text']).split()).value_counts()[:20]\nfreq","04da40f1":"freq1 =  pd.Series(' '.join(train \n         ['text']).split()).value_counts()[-20:]\nfreq1","6b3237d5":"import nltk\nfrom nltk.corpus import stopwords \nstop_words = set(stopwords.words('english'))\n\nthislist = ['were', 'the', 'amp', 'dont', 'got', 'know', 'gon', 'na', 'wan', 'like', 'im', 'hers', 'why', 'over', \"'d\",'our', 'these', 'nevertheless', 'its', 'them', 'empty', 'how', 'whereas', 'whether', 'fifteen', 'about', 'four', 'give', 'otherwise', 'move', 'do', 'say', '\u2018ve', 'hence', 'n\u2018t', 'between', 'bottom', 'some', 'against', 'whole', 'i', 'into', 'they', 'already', 'she', 'either', 'an', 'both', 'him', 'due', 'using', 'five', 'across', 'front', 'in', 'off', 'only', 'really', 'twelve', 'twenty', 'show', 'whereupon', '\u2018m', 'n\u2019t', 'himself', '\u2019m', 'from', 'often', 'three', 'various', 'thereupon', 'should', 'put', 'take', 'who', 'above', 'their', 'been', 'towards', 'however', \"n't\", 'her', 'go', 'thereby', 'just', 'yourselves', 'become', 'thru', 'while', 'nowhere', 'neither', 'anyway', 'because', 'ca', 'which', 'moreover', 'forty', 'besides', 'us', 'more', 'third', 'wherein', 'whoever', 'used', 'every', 'whose', 'onto', 'your', 'hereafter', 'itself', 'sometimes', 'name', 'too', 'own', 'somewhere', 'there', 'we', 'you', '\u2019ve', 'ourselves', 'sixty', 'would', 'first', 'must', 'whereafter', 'wherever', 'his', 'around', 'has', 'yours', 'became', 'doing','the', 'below', 'then', 'everyone', 'else', 'any', 'latterly', 'noone', 'part', 'might', \"'ve\", 'becoming', 'same', 'top', 'yourself', 'he', 'each', 'anyone', 'my', 'seeming', 'six', 'the', 'during', 'afterwards', 'throughout', 'formerly', 'seem', 'therefore', 'another', 'keep', 'without', 'being', 'can', 'had', 'per', \"'s\", 'other', 'side', '\u2019s', 'also', 'herself', '\u2019ll', 'eight', 'what', 'please', 'a', 'therein', 'back', 'me', 'never', 'not', 'does', 'enough', 'meanwhile', 'toward', 'even', 'get', 'and', 'it', 'perhaps', 'this', 'regarding', 'somehow', 'cannot', 'anyhow', 'through', 'whenever', 'thereafter', 'rather', 'by', 'still', 'where', 'than', 'made', 'of', 'will', 'within', 'are', 'amongst', 'although', 'former', 'full', 'nobody', 'was', 'to', 'is', 'at', 'hundred', 'all', 'on', 'such', 'after', 'almost', 'most', 'no', 'our', 'see', 'thus', 'upon', \"'ll\", 'whence', 'make', '\u2018s', 'could', 'quite', 'or', 'beyond', 'thence', 'mostly', 'though', 'alone', 'for', 'under', 'seemed', 'until', 'much', 'nine', 'least', 'that', 'nor', 'further', 'themselves', 'whatever', 'whom', 'anywhere', 'myself', 'eleven', 'none', 'with', 'as', 'have', '\u2018ll', \"'m\", 'up', 'if', 'several', 'whereby', 'now', 'always', 'amount', 'done', 'hereupon', 'others', 'may', 'one', 'everything', 'so', 'hereby', 'anything', 'fifty', 'last', 'am', 'beforehand', 'few', 'ever', 'together', 'unless', 'ten', 'behind', 'when', 'those', 'mine', 'everywhere', 'be', 'less', 'nothing', 'something', 'very', \"'re\", 'here', '\u2018re', 'since', 'seems', 'down', 'did', 'before', 'serious', '\u2018d', '\u2019d', 'many', 'call', 'along', 'once', 'herein', 'out', 'namely', 'someone', 'becomes', 'whither', 're', 'two', 'but', 'again', 'elsewhere', 'well', 'next', 'sometime', 'indeed', 'ours', 'yet', '\u2019re', 'via', 'latter', 'except', 'among', 'beside']\nstop_words.update(thislist)","1e167776":"def clean_text(text):\n    '''Make text lower case , removw text in squre backets ,remove links, remove punctuation\n    and remove words containing numbers\n    '''\n    text = text.lower()\n    text = re.sub('\\[.*?\\]','',text)\n    text = re.sub('https?:\/\/\\S+|www\\.\\S+','',text)\n    text = re.sub('<,*?>+','',text)\n    text = re.sub('[%s]' % re.escape(string.punctuation),'',text)\n    text = re.sub('\\n','',text)\n    text = re.sub('\\w*\\d\\w*','',text)\n    return text\n\n# Applying the cleaning function to both test and training datasets\ntrain['text'] = train['text'].apply(lambda x: clean_text(x))\ntest['text'] = test['text'].apply(lambda x: clean_text(x))\n\n# Let's take a look at the updated text\ntrain['text'].head()","0f7f4f8b":"tokenizer = nltk.tokenize.RegexpTokenizer(r'\\w+')\ntrain['text'] = train['text'].apply(lambda x: tokenizer.tokenize(x))\ntest['text'] = test['text'].apply(lambda x: tokenizer.tokenize(x))\ntrain['text'].head()","2ab80cfa":"def remove_stopwords(text):\n    words = [w for w in text if w not in stopwords.words('english')]\n    return words\n\n\ntrain['text'] = train['text'].apply(lambda x : remove_stopwords(x))\ntest['text'] = test['text'].apply(lambda x : remove_stopwords(x))\ntrain.head()","70407065":"train1=pd.read_csv(\"\/kaggle\/input\/nlp-getting-started\/train.csv\")\ntest1=pd.read_csv(\"\/kaggle\/input\/nlp-getting-started\/test.csv\")","3fdf5d54":"from nltk.tokenize import sent_tokenize,word_tokenize\n\ntweets_train = train1['text'].values\ntarget = train1['target'].values\n\ntweets_test = test1['text'].values\n","22ee3c50":"from nltk.stem import WordNetLemmatizer \nlemmatizer = WordNetLemmatizer()\n\nfor i in range(len(tweets_train)):\n    \n    sentences = sent_tokenize(tweets_train[i])\n    \n    word_list = []\n    for sent in sentences:\n        \n        words = word_tokenize(sent)\n        \n        for word in words:\n            if words not in word_list:\n                word_list.append(word)\n                \n    \n    word_list = [lemmatizer.lemmatize(w) for w in word_list if w not in stop_words]\n    \n    tweets_train[i] = ' '.join(w for w in word_list)\n    \nfor i in range(len(tweets_test)):\n    \n    sentences = sent_tokenize(tweets_test[i])\n    \n    word_list = []\n    for sent in sentences:\n        \n        words = word_tokenize(sent)\n        \n        for word in words:\n            \n            if words not in word_list:\n                word_list.append(word)\n                \n    \n    word_list = [lemmatizer.lemmatize(w) for w in word_list if w not in stop_words]\n    \n    tweets_test[i] = ' '.join(w for w in word_list)","2d5f2447":"from nltk.stem import PorterStemmer\nporter = PorterStemmer()\n\nfor i in range(len(tweets_train)):\n    \n    sentences = sent_tokenize(tweets_train[i])\n    \n    word_list = []\n    for sent in sentences:\n        \n        words = word_tokenize(sent)\n        \n        for word in words:\n            \n            if words not in word_list:\n                word_list.append(word)\n                \n    \n    word_list = [porter.stem(w) for w in word_list if w not in stop_words]\n    \n    tweets_train[i] = ' '.join(w for w in word_list)\n\nfor i in range(len(tweets_test)):\n    \n    sentences = sent_tokenize(tweets_test[i])\n    \n    word_list = []\n    for sent in sentences:\n        \n        words = word_tokenize(sent)\n        \n        for word in words:\n            \n            if words not in word_list:\n                word_list.append(word)\n                \n    \n    word_list = [lemmatizer.lemmatize(w) for w in word_list if w not in stop_words]\n    \n    tweets_test[i] = ' '.join(w for w in word_list)    \n    ","40538ddf":"# Combining results\ndef combine_text(list_of_text):\n    combined_text = ' '.join(list_of_text)\n    return combined_text\n\ntrain['text'] = train['text'].apply(lambda x : combine_text(x))\ntest['text'] = test['text'].apply(lambda x : combine_text(x))\ntrain['text']\ntrain.head()","90eb1997":"train.tail()","8a4dcfb2":"from sklearn.feature_extraction.text import CountVectorizer\ncount_vectorizer = CountVectorizer()\ntrain_vectors = count_vectorizer.fit_transform(train['text'])\ntest_vectors = count_vectorizer.transform(test[\"text\"])\n\n## Keeping only non-zero elements to preserve space \nprint(train_vectors[0].todense())","11051e46":"# XG Boost\nimport  xgboost as xgb\nfrom xgboost import XGBClassifier\n\n# Sklearn\nfrom sklearn import model_selection\nfrom sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.metrics import  f1_score\nfrom sklearn import preprocessing ,decomposition, model_selection,metrics,pipeline\nfrom sklearn.model_selection import GridSearchCV","da28b0b2":"tfidf = TfidfVectorizer(min_df=2, max_df=0.5, ngram_range=(1, 2))\ntrain_tfidf = tfidf.fit_transform(train['text'])\ntest_tfidf = tfidf.transform(test[\"text\"])","722e5a30":"# Fitting a simple Logistic Regression on Counts\nclf = LogisticRegression(C=1.0)\nscores = model_selection.cross_val_score(clf, train_vectors, train[\"target\"], cv=5, scoring=\"f1\")\nscores.mean()","5591b9e1":"clf.fit(train_vectors, train[\"target\"])","14739b70":"clf.predict(test_vectors[1])","5254548f":"# Fitting a simple Logistic Regression on TFIDF\nclf_tfidf = LogisticRegression(C=1.0)\nscores = model_selection.cross_val_score(clf_tfidf, train_tfidf, train[\"target\"], cv=5, scoring=\"f1\")\nscores.mean()","ecc7ab54":"# Fitting a simple Naive Bayes on Counts\nclf_NB = MultinomialNB()\nscores = model_selection.cross_val_score(clf_NB, train_vectors, train[\"target\"], cv=5, scoring=\"f1\")\nscores.mean()","3ce0d569":"clf_NB.fit(train_vectors, train[\"target\"])","2acd9179":"# Fitting a simple Naive Bayes on TFIDF\nclf_NB_TFIDF = MultinomialNB()\nscores = model_selection.cross_val_score(clf_NB_TFIDF, train_tfidf, train[\"target\"], cv=5, scoring=\"f1\")\nscores.mean()","489d1c8c":"clf_NB_TFIDF.fit(train_tfidf, train[\"target\"])","428d8a19":"# Fitting XGBoost on Counts\nclf_xgb = xgb.XGBClassifier(max_depth=7, n_estimators=200, colsample_bytree=0.8, \n                        subsample=0.8, nthread=10, learning_rate=0.1)\nscores = model_selection.cross_val_score(clf_xgb, train_vectors, train[\"target\"], cv=5, scoring=\"f1\")\nscores","09690500":"# Fitting XGBoost on TFIDF\nclf_xgb_TFIDF = xgb.XGBClassifier(max_depth=7, n_estimators=200, colsample_bytree=0.8, \n                        subsample=0.8, nthread=10, learning_rate=0.1)\nscores = model_selection.cross_val_score(clf_xgb_TFIDF, train_tfidf, train[\"target\"], cv=5, scoring=\"f1\")\nscores","cf5fdb23":"from sklearn.metrics import confusion_matrix\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import classification_report, accuracy_score, precision_score, recall_score, plot_confusion_matrix\nfrom sklearn.svm import LinearSVC, SVC, NuSVC\nfrom sklearn.linear_model import LogisticRegression, SGDClassifier\nfrom sklearn.naive_bayes import MultinomialNB, BernoulliNB\nimport random","342bf00d":"X = train['text']  \ny = train['target']\ntest_x = test['text']\nX_train, X_test, y_train, y_test = train_test_split(X, y, random_state = 1, shuffle = True)","dcca7660":"count_vectorizer = CountVectorizer(stop_words='english')#, min_df = 0.05, max_df = 0.9)\ncount_train = count_vectorizer.fit_transform(X_train)\ncount_test = count_vectorizer.transform(X_test)\ncount_train_sub = count_vectorizer.transform(X)\ncount_sub = count_vectorizer.transform(test_x)","4c3941a6":"import xgboost as xgb\nxgb_classifier=xgb.XGBClassifier(objective='binary:logistic',learning_rate = 0.1,gamma=0.01,max_depth = 5,booster=\"gbtree\")\nxgb_classifier.fit(count_train ,y_train)\nxgb_pred = xgb_classifier.predict(count_test)\nxgb_score = accuracy_score(y_test,xgb_pred)\nprint('XGBoost Count Score: ',xgb_score)\nxgb_cm = confusion_matrix(y_test, xgb_pred)\nxgb_cm\n","cef73411":"from sklearn import tree\ndt_clf = tree.DecisionTreeClassifier()\ndt_clf.fit(count_train ,y_train)\ndt_pred = dt_clf.predict(count_test)\ndt_score = accuracy_score(y_test,dt_pred)\nprint('Decision Tree Count Score: ',dt_score)\ndt_cm = confusion_matrix(y_test, dt_pred)\ndt_cm\n","ca3c9498":"from sklearn.ensemble import AdaBoostClassifier\nada_clf = AdaBoostClassifier(n_estimators=1000,learning_rate=1,algorithm='SAMME.R')\nada_clf.fit(count_train ,y_train)\nada_pred = ada_clf.predict(count_test)\nada_score = accuracy_score(y_test,ada_pred)\nprint('AdaBoost Decision Tree Count Score: ',ada_score)\nada_cm = confusion_matrix(y_test, ada_pred)\nada_cm","4283802f":"from sklearn.ensemble import RandomForestClassifier\nrf_clf = RandomForestClassifier(max_depth=2, random_state=0)\nrf_clf.fit(count_train ,y_train)\nrf_pred = rf_clf.predict(count_test)\nrf_score = accuracy_score(y_test,rf_pred)\nprint('Random Forest Count Score: ',rf_score)\nrf_cm = confusion_matrix(y_test, rf_pred)\nrf_cm","b6ce0f2c":"count_nb = MultinomialNB()\ncount_nb.fit(count_train ,y_train)\ncount_nb_pred = count_nb.predict(count_test)\ncount_nb_score = accuracy_score(y_test,count_nb_pred)\nprint('MultinomialNaiveBayes Count Score: ', count_nb_score)\ncount_nb_cm = confusion_matrix(y_test, count_nb_pred)\ncount_nb_cm","d6e18192":"count_bnb = BernoulliNB()\ncount_bnb.fit(count_train ,y_train)\ncount_bnb_pred = count_bnb.predict(count_test)\ncount_bnb_score = accuracy_score(y_test,count_bnb_pred)\nprint('BernoulliNaiveBayes Count Score: ', count_bnb_score)\ncount_bnb_cm = confusion_matrix(y_test, count_bnb_pred)\ncount_bnb_cm","b8b1af84":"count_lsvc = LinearSVC()\ncount_lsvc.fit(count_train ,y_train)\ncount_lsvc_pred = count_lsvc.predict(count_test)\ncount_lsvc_score = accuracy_score(y_test,count_lsvc_pred)\nprint('LinearSVC Count Score: ', count_lsvc_score)\ncount_lsvc_cm = confusion_matrix(y_test, count_lsvc_pred)\ncount_lsvc_cm","4feab375":"\ncount_svc = SVC()\ncount_svc.fit(count_train ,y_train)\ncount_svc_pred = count_svc.predict(count_test)\ncount_svc_score = accuracy_score(y_test,count_svc_pred)\nprint('SVC Count Score: ', count_svc_score)\ncount_svc_cm = confusion_matrix(y_test, count_svc_pred)\ncount_svc_cm","60097ef2":"count_nusvc = NuSVC()\ncount_nusvc.fit(count_train ,y_train)\ncount_nusvc_pred = count_nusvc.predict(count_test)\ncount_nusvc_score = accuracy_score(y_test,count_nusvc_pred)\nprint('NuSVC Count Score: ', count_nusvc_score)\ncount_nusvc_cm = confusion_matrix(y_test, count_nusvc_pred)\ncount_nusvc_cm","026c29c0":"count_sgd = SGDClassifier()\ncount_sgd.fit(count_train ,y_train)\ncount_sgd_pred = count_sgd.predict(count_test)\ncount_sgd_score = accuracy_score(y_test,count_sgd_pred)\nprint('SGD Count Score: ', count_sgd_score)\ncount_sgd_cm = confusion_matrix(y_test, count_sgd_pred)\ncount_sgd_cm","4a0617f6":"count_lr = LogisticRegression()\ncount_lr.fit(count_train ,y_train)\ncount_lr_pred = count_lr.predict(count_test)\ncount_lr_score = accuracy_score(y_test,count_lr_pred)\nprint('LogisticRegression Count Score: ', count_lr_score)\ncount_lr_cm = confusion_matrix(y_test, count_lr_pred)\ncount_lr_cm    ","1f43899b":"models = pd.DataFrame({\n    'Model': ['Naive Bayes Multinomiale', 'Naive Bayes Bernoulli',\n              'Linear Support Vector Classification', 'Support vector Classification', 'Nu-Support Vector Classification.', \n              'Stochastic Gradient Decent', 'Logistic Regression','XGBoost','AdaBoost'],\n    'Score': [count_nb_score, count_bnb_score, \n              count_lsvc_score, count_svc_score, count_nusvc_score, \n              count_sgd_score, count_lr_score,xgb_score,ada_score]})\nmodels.sort_values(by=\"Score\",ascending=False)","bc518f60":"from sklearn.metrics import roc_curve, auc","4f4bebb7":"fpr1, tpr1, threshold1 = roc_curve(y_test, count_nb_pred) \nroc_auc1 = auc(fpr1, tpr1)\nfpr2, tpr2, threshold2 = roc_curve(y_test, count_bnb_pred) \nroc_auc2 = auc(fpr2, tpr2)\nfpr3, tpr3, threshold3 = roc_curve(y_test, count_lsvc_pred)\nroc_auc3 = auc(fpr3, tpr3)\nfpr4, tpr4, threshold4 = roc_curve(y_test, count_svc_pred) \nroc_auc4 = auc(fpr4, tpr4)\nfpr5, tpr5, threshold5 = roc_curve(y_test, count_nusvc_pred) \nroc_auc5 = auc(fpr5, tpr5)\nfpr6, tpr6, threshold6 = roc_curve(y_test, count_sgd_pred)\nroc_auc6 = auc(fpr6, tpr6)\nfpr7, tpr7, threshold7 = roc_curve(y_test, count_lr_pred)\nroc_auc7 = auc(fpr7, tpr7)\nfpr8, tpr8, threshold8 = roc_curve(y_test, xgb_pred)\nroc_auc8 = auc(fpr8, tpr8)\nfpr9, tpr9, threshold9 = roc_curve(y_test, ada_pred)\nroc_auc9 = auc(fpr9, tpr9)","8c63efaa":"plt.figure(figsize=(10,10)) \nplt.plot(fpr1, tpr1, color='red', lw=2, label='Naive Bayes Multinomiale (area = %0.2f)'% roc_auc1)\nplt.plot(fpr2, tpr2, color='green', lw=2, label='Naive Bayes Bernoulli (area = %0.2f)'% roc_auc2)\nplt.plot(fpr3, tpr3, color='cyan', lw=2, label='Linear Support Vector Classification (area = %0.2f)'% roc_auc3)\nplt.plot(fpr4, tpr4, color='magenta', lw=2, label='Support vector Classification (area = %0.2f)'% roc_auc4)\nplt.plot(fpr5, tpr5, color='yellow', lw=2, label='Nu-Support Vector Classification (area = %0.2f)'% roc_auc5)\nplt.plot(fpr6, tpr6, color='black', lw=2, label='Stochastic Gradient Decent (area = %0.2f)'% roc_auc6)\nplt.plot(fpr7, tpr7, color='#770080', lw=2, label='Logistic Regression (area = %0.2f)'% roc_auc7)\nplt.plot(fpr8, tpr8, color='pink', lw=2, label='XGBoost Binary:logistic (area = %0.2f)'% roc_auc8)\nplt.plot(fpr9, tpr9, color='blue', lw=2, label='AdaBoost Decision Tree (area = %0.2f)'% roc_auc9)\n\n\nplt.plot([0, 1], [0, 1], color='red', lw=2, linestyle='--') \nplt.xlim([0.0, 1.0]) \nplt.ylim([0.0, 1.05]) \nplt.xlabel('False Positive Rate') \nplt.ylabel('True Positive Rate') \nplt.title('Classifiers ROC curves') \nplt.legend(loc = \"lower right\")\nplt.show()","1e0797be":"#data pour la submission\nsample.head()","971b6d3b":"def submission(model,test_vectors):\n    sample[\"target\"] = model.predict(test_vectors)\n    sample.to_csv(\"submission.csv\", index=False)\n    \ntest_vectors=test_tfidf\nsubmission(clf_NB_TFIDF,test_vectors)\n\nsubFinal=pd.read_csv('submission.csv')\nsubFinal.head()","8dff3956":"**2. Average word length in a tweet :**","9cc03ba5":"> * **Removing contractions**","4317f33f":"* We will begin by doing a basic analysis, for the characters, words and the sentences.","d450dac6":"**Conclusion :**","5df19e02":"* Missing data for test","1735ca45":"> The most frequent words","f79cd49a":"[](http:\/\/)","cfbe1014":"# 1- Exploratory Data Analysis of tweets","25483de6":"# SUBMISSION","40c84bea":"* We have Around 80% missing information in keyword feature\n* We have 34% missing information in location feature.","41888fc9":"> * **Tokenzing the text**","eb59e478":"**4. Analyzing punctuations :**","eed8e4cc":"> * **Removing URL's**","8d06582e":"# Data mining and analysis","7b4b09cf":"### Target variable visualization.","9c4e21e6":"> Uncommon words","ed768186":"> * **Removing Punctuation**","5b69fd88":"* The distribution looks similar for both classes, 120 to 140 characters in a single tweet seems to be the norm for both classes.","f3498014":"__1 is a disaster, while 0 is not a disaster.__","2048fb51":"> * **Removing emojis**","c29560a3":"# 3- Modelisation","e08bc2de":"> * **Lemmatizing and stemming the text**","df5b4bef":"**1. Character analysis**","60011eab":"* The most dominant word is \"THE\".","d2056688":"> * **Removing stopwords**","6313bdca":"This notebook is for the kaggle [Real or Not? NLP with Disaster Tweets](https:\/\/www.kaggle.com\/c\/nlp-getting-started) competition.\n\nThe work has been done by the Vegeta team. The team members are :\n\n* Sarah Abidli\n* Moez Abid\n* Fedi Bayoudh\n* Wissal Bayoudh \n* Montassar Thabti\n* Mohamed Iheb Bousnina","a9340550":"**3. Common stopwords in tweets :**","d1cdb6cd":"# 2-  Data Preprocessing","9fed47db":"# Real or Not? NLP with Disaster Tweets","a2bda389":"*   Missing data for train"}}