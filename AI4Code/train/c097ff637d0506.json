{"cell_type":{"b8e4b103":"code","a2e92dab":"code","0f8f2327":"code","9f6f1b8a":"code","0b79e823":"code","7656d800":"code","faf4f518":"code","b368789f":"code","b40d29c8":"code","54042e1a":"markdown","42556b41":"markdown","3476a646":"markdown","71342071":"markdown","2d3299b3":"markdown","3f9e021d":"markdown","dfcb94b7":"markdown","6b436d42":"markdown","90149aec":"markdown","16746f9f":"markdown","3c247b7e":"markdown","2d357c10":"markdown","2b01c853":"markdown","cff203cd":"markdown","889e8d2b":"markdown","aaa9ce34":"markdown","b738f987":"markdown","7c212b28":"markdown","481a419d":"markdown"},"source":{"b8e4b103":"# Implementing the above Celsius to Fahrenheit example in ML\n\n# Import dependencies\nimport tensorflow as tf\nimport numpy as np\ntf.logging.set_verbosity(tf.logging.ERROR)\n","a2e92dab":"# Set up training data \n\ncelsius_q = np.array([-40, -10, 0, 8, 15, 22, 38], dtype = float)\nfahrenheit_a = np.array([-40, 14, 32, 46, 59, 72, 100], dtype = float)\n\nfor i, c in enumerate(celsius_q):\n    print(\"{} degrees Celsius = {} degrees Fahrenheit\".format(c, fahrenheit_a[i]))","0f8f2327":"# Build a layer\nl0 = tf.keras.layers.Dense(units = 1, input_shape = [1])\n\n# Assemble layers into the model\nmodel = tf.keras.Sequential([l0])","9f6f1b8a":"# Compile the model with loss and optimizer functions\nmodel.compile(loss = \"mean_squared_error\", optimizer =  tf.keras.optimizers.Adam(0.1))","0b79e823":"history = model.fit(celsius_q, fahrenheit_a, epochs=500, verbose=False)\nprint(\"Finished training the model\")","7656d800":"import matplotlib.pyplot as plt\nplt.xlabel('Epoch Number')\nplt.ylabel(\"Loss Magnitude\")\nplt.plot(history.history['loss'])","faf4f518":"print(model.predict([100.0]))","b368789f":"# Finally, let's print the internal variables of the Dense layer\nprint(\"These are the layer variables: {}\".format(l0.get_weights()))","b40d29c8":"l0 = tf.keras.layers.Dense(units=4, input_shape=[1])  \nl1 = tf.keras.layers.Dense(units=4)  \nl2 = tf.keras.layers.Dense(units=1)  \nmodel = tf.keras.Sequential([l0, l1, l2])\nmodel.compile(loss='mean_squared_error', optimizer=tf.keras.optimizers.Adam(0.1))\nmodel.fit(celsius_q, fahrenheit_a, epochs=500, verbose=False)\nprint(\"Finished training the model\")\nprint(model.predict([100.0]))\nprint(\"Model predicts that 100 degrees Celsius is: {} degrees Fahrenheit\".format(model.predict([100.0])))\nprint(\"These are the l0 variables: {}\".format(l0.get_weights()))\nprint(\"These are the l1 variables: {}\".format(l1.get_weights()))\nprint(\"These are the l2 variables: {}\".format(l2.get_weights()))","54042e1a":"Train the model\n\nTrain the model by calling the fit method.\n\nDuring training, the model takes in Celsius values, performs a calculation using the current internal variables (called \"weights\") and outputs values which are meant to be the Fahrenheit equivalent. Since the weights are initially set randomly, the output will not be close to the correct value. The difference between the actual output and the desired output is calculated using the loss function, and the optimizer function directs how the weights should be adjusted.\n\nThis **cycle of calculate, compare, adjust** is controlled by the fit method. The first argument is the inputs, the second argument is the desired outputs. The epochs argument specifies how many times this cycle should be run, and the verbose argument controls how much output the method produces.","42556b41":"####################################################################","3476a646":"Traditional Programming: Input and algorithm are known, we write a function to produce the output\n\nML Programming: Input and output are known, but the algorithm to produce the output is not known.\n\nIn ML, algorithm has to be learned","71342071":"**Loss function** \u2014 A way of measuring how far off predictions are from the desired outcome\n\n**Optimizer function** \u2014 A way of adjusting internal values in order to reduce the loss","2d3299b3":"There are many types of neural network architectures. However, no matter what architecture you choose, the math it contains (what calculations are being performed, and in what order) is not modified during training. Instead, it is the internal variables (\u201cweights\u201d and \u201cbiases\u201d) which are updated during training.\n\ny = [0, 8, 15, 22, 38] \n\nx = [32, 44.4, 59, 71.6, ?]\n\n? = 100.4\n\ny = x(1.8) + 32\n\nF = C(1.8) + 32\n\nFor example, in the Celsius to Fahrenheit conversion problem, the model starts by multiplying the input by some number (the weight) and adding another number (the bias). Training the model involves finding the right values for these variables, not changing from multiplication and addition to some other operation.","3f9e021d":"Use the model to predict values\nNow you have a model that has been trained to learn the relationship between celsius_q and fahrenheit_a. You can use the predict method to have it calculate the Fahrenheit degrees for a previously unknown Celsius degrees.\n\nSo, for example, if the Celsius value is 100, what do you think the Fahrenheit result will be? Take a guess before you run this code.","dfcb94b7":"The training process (happening in **model.fit(...)**) is really about tuning the internal variables of the networks to the best possible values, so that they can map the input to the output. This is achieved through an optimization process called Gradient Descent, which uses Numeric Analysis to find the best possible values to the internal variables of the model.\n\nTo do machine learning, you don't really need to understand these details. But for the curious: gradient descent iteratively adjusts parameters, nudging them in the correct direction a bit at a time until they reach the best values. In this case \u201cbest values\u201d means that nudging them any more would make the model perform worse. The function that measures how good or bad the model is during each iteration is called the \u201closs function\u201d, and the goal of each nudge is to \u201cminimize the loss function.\u201d","6b436d42":"* Technology is a tool to transform education, transportation, health, and so on.\n\n* ML is simpler than programming computers \n","90149aec":"As you can see, this model is also able to predict the corresponding Fahrenheit value really well. But when you look at the variables (weights) in the l0 and l1 layers, they are nothing even close to ~1.8 and ~32. The added complexity hides the \"simple\" form of the conversion equation.","16746f9f":"Feature == Input\n\nLabel == Output\n\nApplications of Deep:\n1. Detecting Skin Cancer as accurate as a certified dermatologist\n2. Self Driving Cars\n3. Automate Emails\n4. YouTube Subtitles","3c247b7e":"####################################################################","2d357c10":"Layer: A collection of nodes connected together within a neural network.\n\nModel: The representation of your neural network\n\nDense and Fully Connected (FC): Each node in one layer is connected to each node in the previous layer.\n\nMSE: Mean squared error, a type of loss function that counts a small number of large discrepancies as worse than a large number of small ones.\n\nGradient Descent: An algorithm that changes the internal variables a bit at a time to gradually reduce the loss function.\n\nOptimizer: A specific implementation of the gradient descent algorithm. (There are many algorithms for this. In this course we will only use the \u201cAdam\u201d Optimizer, which stands for ADAptive with Momentum. It is considered the best-practice optimizer.)\n\nBatch: The set of examples used during training of the neural network\n\nEpoch: A full pass over the entire training dataset\n\nForward pass: The computation of output values from input\n\nBackward pass (backpropagation): The calculation of internal variable adjustments according to the optimizer algorithm, starting from the output layer and working back through each layer to the input.","2b01c853":"**Training** - The act of calculating the current loss of a model and then improving it is precisely. It is not simply memorizing the training examples (**Training and Testing is similar to taking exam in school**). \n\nTensorFlow uses numerical analysis to perform this tuning, and all this complexity is hidden from you.\n\nThe loss function (mean squared error) and the optimizer (Adam) used here are standard for simple models like this one, but many others are available. It is not important to know how these specific functions work at this point.\n\nOne part of the Optimizer you may need to think about when building your own models is the learning rate (0.1 in the code above). This is the step size taken when adjusting values in the model. If the value is too small, it will take too many iterations to train the model. Too large, and accuracy goes down. Finding a good value often involves some trial and error, but the range is usually within 0.001 (default), and 0.1","cff203cd":"The correct answer is 100\u00d71.8+32=212, so our model is doing really well.\n\n**To review**\nWe created a model with a Dense layer\nWe trained it with 3500 examples (7 pairs, over 500 epochs).\nOur model tuned the variables (weights) in the Dense layer until it was able to return the correct Fahrenheit value for any Celsius value. (Remember, 100 Celsius was not part of our training data.)","889e8d2b":"Most problems we want to solve are nonlinear. In these cases, adding ReLU to our Dense layers can help solve the problem.\n\nReLU is a type of activation function. There several of these functions (ReLU, Sigmoid, tanh, ELU), but ReLU is used most commonly and serves as a good default. To build and use models that include ReLU, you don\u2019t have to understand its internals.\n\nFlattening: The process of converting a 2d image into 1d vector\n\nReLU: An activation function that allows a model to solve nonlinear problems\n\nSoftmax: A function that provides probabilities for each possible output class\n\nClassification: A machine learning model used for distinguishing among two or more output categories","aaa9ce34":"The first variable is close to ~1.8 and the second to ~32. These values (1.8 and 32) are the actual variables in the real conversion formula.\n\nThis is really close to the values in the conversion formula. We'll explain this in an upcoming video where we show how a Dense layer works, but for a single neuron with a single input and a single output, the internal math looks the same as the equation for a line, y=mx+b, which has the same form as the conversion equation, f=1.8c+32.\n\nSince the form is the same, the variables should converge on the standard values of 1.8 and 32, which is exactly what happened.\n\nWith additional neurons, additional inputs, and additional outputs, the formula becomes much more complex, but the idea is the same.\n\n**A little experiment**\nJust for fun, what if we created more Dense layers with different units, which therefore also has more variables?","b738f987":"Artificial Intelligence: A field of computer science that aims to make computers achieve human-style intelligence. \n\nIn supervised learning you know what you want to teach the computer, while unsupervised learning is about letting the computer figure out what can be learned.","7c212b28":"Fashion MNIST dataset: 28 by 28 (784 bytes) gray-scale images of clothing, 70000 images","481a419d":"It's surprising that such a simple function (and one composed of two linear pieces) can allow your model to account for non-linearities and interactions so well. But the ReLU function works great in most applications, and it is very widely used as a result."}}