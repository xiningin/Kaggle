{"cell_type":{"fd3c9f6a":"code","b7004146":"code","5ad0e733":"code","f8661d7c":"code","dba643df":"code","5aef3ba1":"code","f358b161":"code","dd5b13ff":"code","f56f08c1":"code","dab25fea":"code","fe255cc3":"markdown","e5d7144e":"markdown"},"source":{"fd3c9f6a":"from kaggle_secrets import UserSecretsClient\nsecret_label = \"wandb\"\nsecret_value = UserSecretsClient().get_secret(secret_label)\n!wandb login $secret_value","b7004146":"import gc\nimport os\nimport random\nimport wandb\nimport math\n\nimport numpy as np\nimport pandas as pd\n\nfrom sklearn.model_selection import GroupKFold\nfrom tqdm.notebook import tqdm\n\nimport torch\nimport torch.nn as nn\nfrom torch.nn import functional as F\nfrom torch.utils.data import Dataset, DataLoader\n\nfrom transformers import AdamW\nfrom transformers import get_cosine_schedule_with_warmup\nfrom sklearn.preprocessing import RobustScaler\n\ndevice = torch.device(\"cuda\")","5ad0e733":"class config:\n    EXP_NAME = \"exp080_conti_rc\"\n    \n    INPUT = \"\/kaggle\/input\/ventilator-pressure-prediction\"\n    OUTPUT = \"\/kaggle\/working\"\n    N_FOLD = 5\n    SEED = 0\n    \n    LR = 5e-3\n    N_EPOCHS = 50\n    EMBED_SIZE = 64\n    HIDDEN_SIZE = 256\n    BS = 512\n    WEIGHT_DECAY = 1e-3\n\n    USE_LAG = 4\n    #CATE_FEATURES = ['R_cate', 'C_cate', 'RC_dot', 'RC_sum']\n    CONT_FEATURES = ['u_in', 'u_out', 'time_step'] + ['u_in_cumsum', 'u_in_cummean', 'area', 'cross', 'cross2'] + ['R_cate', 'C_cate']\n    LAG_FEATURES = ['breath_time']\n    LAG_FEATURES += [f'u_in_lag_{i}' for i in range(1, USE_LAG+1)]\n    #LAG_FEATURES += [f'u_in_lag_{i}_back' for i in range(1, USE_LAG+1)]\n    LAG_FEATURES += [f'u_in_time{i}' for i in range(1, USE_LAG+1)]\n    #LAG_FEATURES += [f'u_in_time{i}_back' for i in range(1, USE_LAG+1)]\n    LAG_FEATURES += [f'u_out_lag_{i}' for i in range(1, USE_LAG+1)]\n    #LAG_FEATURES += [f'u_out_lag_{i}_back' for i in range(1, USE_LAG+1)]\n    #ALL_FEATURES = CATE_FEATURES + CONT_FEATURES + LAG_FEATURES\n    ALL_FEATURES = CONT_FEATURES + LAG_FEATURES\n    \n    NOT_WATCH_PARAM = ['INPUT']","f8661d7c":"def set_seed(seed=config.SEED):\n    random.seed(seed)\n    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = False","dba643df":"class VentilatorDataset(Dataset):\n    \n    def __init__(self, df, label_dic=None):\n        self.dfs = [_df for _, _df in df.groupby(\"breath_id\")]\n        self.label_dic = label_dic\n        \n    def __len__(self):\n        return len(self.dfs)\n    \n    def __getitem__(self, item):\n        df = self.dfs[item]\n        X = df[config.ALL_FEATURES].values\n        y = df['pressure'].values\n        if self.label_dic is None:\n            label = [-1]\n        else:\n            label = [self.label_dic[i] for i in y]\n\n        d = {\n            \"X\": torch.tensor(X).float(),\n            \"y\" : torch.tensor(label).long(),\n        }\n        return d","5aef3ba1":"class VentilatorModel(nn.Module):\n    \n    def __init__(self):\n        super(VentilatorModel, self).__init__()\n        #self.r_emb = nn.Embedding(3, 2, padding_idx=0)\n        #self.c_emb = nn.Embedding(3, 2, padding_idx=0)\n        #self.rc_dot_emb = nn.Embedding(8, 4, padding_idx=0)\n        #self.rc_sum_emb = nn.Embedding(8, 4, padding_idx=0)\n        self.seq_emb = nn.Sequential(\n            #nn.Linear(12+len(config.CONT_FEATURES)+len(config.LAG_FEATURES), config.EMBED_SIZE),\n            nn.Linear(len(config.CONT_FEATURES)+len(config.LAG_FEATURES), config.EMBED_SIZE),\n            nn.LayerNorm(config.EMBED_SIZE),\n        )\n        \n        self.lstm = nn.LSTM(config.EMBED_SIZE, config.HIDDEN_SIZE, batch_first=True, bidirectional=True, dropout=0.0, num_layers=4)\n\n        self.head = nn.Sequential(\n            nn.Linear(config.HIDDEN_SIZE * 2, config.HIDDEN_SIZE * 2),\n            nn.LayerNorm(config.HIDDEN_SIZE * 2),\n            nn.ReLU(),\n            nn.Linear(config.HIDDEN_SIZE * 2, 950),\n        )\n        \n        # Encoder\n        #initrange = 0.1\n        #self.r_emb.weight.data.uniform_(-initrange, initrange)\n        #self.c_emb.weight.data.uniform_(-initrange, initrange)\n        #self.rc_dot_emb.weight.data.uniform_(-initrange, initrange)\n        #self.rc_sum_emb.weight.data.uniform_(-initrange, initrange)\n        \n        # LSTM\n        for n, m in self.named_modules():\n            if isinstance(m, nn.LSTM):\n                print(f'init {m}')\n                for param in m.parameters():\n                    if len(param.shape) >= 2:\n                        nn.init.orthogonal_(param.data)\n                    else:\n                        nn.init.normal_(param.data)\n\n    def forward(self, X, y=None):\n        # embed\n        #bs = X.shape[0]\n        #r_emb = self.r_emb(X[:,:,0].long()).view(bs, 80, -1)\n        #c_emb = self.c_emb(X[:,:,1].long()).view(bs, 80, -1)\n        #rc_dot_emb = self.rc_dot_emb(X[:,:,2].long()).view(bs, 80, -1)\n        #rc_sum_emb = self.rc_sum_emb(X[:,:,3].long()).view(bs, 80, -1)\n        \n        #seq_x = torch.cat((r_emb, c_emb, rc_dot_emb, rc_sum_emb, X[:, :, 4:]), 2)\n        seq_x = X\n        emb_x = self.seq_emb(seq_x)\n        \n        out, _ = self.lstm(emb_x, None) \n        logits = self.head(out)\n\n        if y is None:\n            loss = None\n        else:\n            loss = self.loss_fn(logits, y)\n            \n        return logits, loss\n    \n    def loss_fn(self, y_pred, y_true):\n        loss = nn.CrossEntropyLoss()(y_pred.reshape(-1, 950), y_true.reshape(-1))\n        return loss","f358b161":"def train_loop(model, optimizer, scheduler, loader):\n    losses, lrs = [], []\n    model.train()\n    optimizer.zero_grad()\n    for d in loader:\n        out, loss = model(d['X'].to(device), d['y'].to(device))\n        \n        losses.append(loss.item())\n        step_lr = np.array([param_group[\"lr\"] for param_group in optimizer.param_groups]).mean()\n        lrs.append(step_lr)\n        \n        loss.backward()\n        optimizer.step()\n        optimizer.zero_grad()\n        scheduler.step()\n\n    return np.array(losses).mean(), np.array(lrs).mean()\n\n\ndef valid_loop(model, loader, target_dic_inv):\n    losses, predicts = [], []\n    model.eval()\n    for d in loader:\n        with torch.no_grad():\n            out, loss = model(d['X'].to(device), d['y'].to(device))\n        out = torch.tensor([[target_dic_inv[j.item()] for j in i] for i in out.argmax(2)])\n        losses.append(loss.item())\n        predicts.append(out.cpu())\n\n    return np.array(losses).mean(), torch.vstack(predicts).numpy().reshape(-1)\n\ndef test_loop(model, loader, target_dic_inv):\n    predicts = []\n    model.eval()\n    for d in loader:\n        with torch.no_grad():\n            out, _ = model(d['X'].to(device))\n        out = torch.tensor([[target_dic_inv[j.item()] for j in i] for i in out.argmax(2)])\n        predicts.append(out.cpu())\n\n    return torch.vstack(predicts).numpy().reshape(-1)","dd5b13ff":"def add_feature(df):\n    df['time_delta'] = df.groupby('breath_id')['time_step'].diff().fillna(0)\n    df['delta'] = df['time_delta'] * df['u_in']\n    df['area'] = df.groupby('breath_id')['delta'].cumsum()\n\n    df['cross']= df['u_in']*df['u_out']\n    df['cross2']= df['time_step']*df['u_out']\n    \n    df['u_in_cumsum'] = (df['u_in']).groupby(df['breath_id']).cumsum()\n    df['one'] = 1\n    df['count'] = (df['one']).groupby(df['breath_id']).cumsum()\n    df['u_in_cummean'] =df['u_in_cumsum'] \/ df['count']\n    \n    df = df.drop(['count','one'], axis=1)\n    return df\n\ndef add_lag_feature(df):\n    # https:\/\/www.kaggle.com\/kensit\/improvement-base-on-tensor-bidirect-lstm-0-173\n    for lag in range(1, config.USE_LAG+1):\n        df[f'breath_id_lag{lag}']=df['breath_id'].shift(lag).fillna(0)\n        df[f'breath_id_lag{lag}same']=np.select([df[f'breath_id_lag{lag}']==df['breath_id']], [1], 0)\n\n        # u_in \n        df[f'u_in_lag_{lag}'] = df['u_in'].shift(lag).fillna(0) * df[f'breath_id_lag{lag}same']\n        #df[f'u_in_lag_{lag}_back'] = df['u_in'].shift(-lag).fillna(0) * df[f'breath_id_lag{lag}same']\n        df[f'u_in_time{lag}'] = df['u_in'] - df[f'u_in_lag_{lag}']\n        #df[f'u_in_time{lag}_back'] = df['u_in'] - df[f'u_in_lag_{lag}_back']\n        df[f'u_out_lag_{lag}'] = df['u_out'].shift(lag).fillna(0) * df[f'breath_id_lag{lag}same']\n        #df[f'u_out_lag_{lag}_back'] = df['u_out'].shift(-lag).fillna(0) * df[f'breath_id_lag{lag}same']\n\n    # breath_time\n    df['time_step_lag'] = df['time_step'].shift(1).fillna(0) * df[f'breath_id_lag{lag}same']\n    df['breath_time'] = df['time_step'] - df['time_step_lag']\n\n    drop_columns = ['time_step_lag']\n    drop_columns += [f'breath_id_lag{i}' for i in range(1, config.USE_LAG+1)]\n    drop_columns += [f'breath_id_lag{i}same' for i in range(1, config.USE_LAG+1)]\n    df = df.drop(drop_columns, axis=1)\n\n    # fill na by zero\n    df = df.fillna(0)\n    return df\n\nc_dic = {10: 0, 20: 1, 50:2}\nr_dic = {5: 0, 20: 1, 50:2}\nrc_sum_dic = {v: i for i, v in enumerate([15, 25, 30, 40, 55, 60, 70, 100])}\nrc_dot_dic = {v: i for i, v in enumerate([50, 100, 200, 250, 400, 500, 2500, 1000])}    \n\ndef add_category_features(df):\n    df['C_cate'] = df['C'].map(c_dic)\n    df['R_cate'] = df['R'].map(r_dic)\n    df['RC_sum'] = (df['R'] + df['C']).map(rc_sum_dic)\n    df['RC_dot'] = (df['R'] * df['C']).map(rc_dot_dic)\n    return df\n\nnorm_features = config.CONT_FEATURES + config.LAG_FEATURES\ndef norm_scale(train_df, test_df):\n    scaler = RobustScaler()\n    all_u_in = np.vstack([train_df[norm_features].values, test_df[norm_features].values])\n    scaler.fit(all_u_in)\n    train_df[norm_features] = scaler.transform(train_df[norm_features].values)\n    test_df[norm_features] = scaler.transform(test_df[norm_features].values)\n    return train_df, test_df","f56f08c1":"def main():\n    \n    train_df = pd.read_csv(f\"{config.INPUT}\/train.csv\")\n    test_df = pd.read_csv(f\"{config.INPUT}\/test.csv\")\n    sub_df = pd.read_csv(f\"{config.INPUT}\/sample_submission.csv\")\n    oof = np.zeros(len(train_df))\n    test_preds_lst = []\n\n    target_dic = {v:i for i, v in enumerate(sorted(train_df['pressure'].unique().tolist()))}\n    target_dic_inv = {v: k for k, v in target_dic.items()}\n\n    gkf = GroupKFold(n_splits=config.N_FOLD).split(train_df, train_df.pressure, groups=train_df.breath_id)\n    for fold, (_, valid_idx) in enumerate(gkf):\n        train_df.loc[valid_idx, 'fold'] = fold\n\n    train_df = add_feature(train_df)\n    test_df = add_feature(test_df)\n    train_df = add_lag_feature(train_df)\n    test_df = add_lag_feature(test_df)\n    train_df = add_category_features(train_df)\n    test_df = add_category_features(test_df)\n    train_df, test_df = norm_scale(train_df, test_df)\n\n    test_df['pressure'] = -1\n    test_dset = VentilatorDataset(test_df)\n    test_loader = DataLoader(test_dset, batch_size=config.BS,\n                             pin_memory=True, shuffle=False, drop_last=False, num_workers=os.cpu_count())\n    \n    for fold in range(config.N_FOLD):\n        print(f'Fold-{fold}')\n        train_dset = VentilatorDataset(train_df.query(f\"fold!={fold}\"), target_dic)\n        valid_dset = VentilatorDataset(train_df.query(f\"fold=={fold}\"), target_dic)\n\n        set_seed()\n        train_loader = DataLoader(train_dset, batch_size=config.BS,\n                                  pin_memory=True, shuffle=True, drop_last=True, num_workers=os.cpu_count(),\n                                  worker_init_fn=lambda x: set_seed())\n        valid_loader = DataLoader(valid_dset, batch_size=config.BS,\n                                  pin_memory=True, shuffle=False, drop_last=False, num_workers=os.cpu_count())\n\n        model = VentilatorModel()\n        model.to(device)\n\n        optimizer = AdamW(model.parameters(), lr=config.LR, weight_decay=config.WEIGHT_DECAY)\n        num_train_steps = int(len(train_loader) * config.N_EPOCHS)\n        num_warmup_steps = int(num_train_steps \/ 10)\n        scheduler = get_cosine_schedule_with_warmup(optimizer, num_warmup_steps=num_warmup_steps, num_training_steps=num_train_steps)\n\n        uniqe_exp_name = f\"{config.EXP_NAME}_f{fold}\"\n        wandb.init(project='Ventilator', entity='trtd56', name=uniqe_exp_name, group=config.EXP_NAME)\n        wandb_config = wandb.config\n        wandb_config.fold = fold\n        for k, v in dict(vars(config)).items():\n            if k[:2] == \"__\" or k in config.NOT_WATCH_PARAM:\n                continue\n            wandb_config[k] = v\n        wandb.watch(model)\n        \n        os.makedirs(f'{config.OUTPUT}\/{config.EXP_NAME}', exist_ok=True)\n        model_path = f\"{config.OUTPUT}\/{config.EXP_NAME}\/ventilator_f{fold}_best_model.bin\"\n        \n        valid_best_score = float('inf')\n        valid_best_score_mask = float('inf')\n        for epoch in tqdm(range(config.N_EPOCHS)):\n            train_loss, lrs = train_loop(model, optimizer, scheduler, train_loader)\n            valid_loss, valid_predict = valid_loop(model, valid_loader, target_dic_inv)\n            valid_score = np.abs(valid_predict - train_df.query(f\"fold=={fold}\")['pressure'].values).mean()\n\n            mask = (train_df.query(f\"fold=={fold}\")['u_out'] == 0).values\n            _score = valid_predict - train_df.query(f\"fold=={fold}\")['pressure'].values\n            valid_score_mask = np.abs(_score[mask]).mean()\n\n            if valid_score < valid_best_score:\n                valid_best_score = valid_score\n                torch.save(model.state_dict(), model_path)\n                oof[train_df.query(f\"fold=={fold}\").index.values] = valid_predict\n\n            if valid_score_mask < valid_best_score_mask:\n                valid_best_score_mask = valid_score_mask\n\n            wandb.log({\n                \"train_loss\": train_loss,\n                \"valid_loss\": valid_loss,\n                \"valid_score\": valid_score,\n                \"valid_best_score\": valid_best_score,\n                \"valid_score_mask\": valid_score_mask,\n                \"valid_best_score_mask\": valid_best_score_mask,\n                \"learning_rate\": lrs,\n            })\n            \n            torch.cuda.empty_cache()\n            gc.collect()\n        \n        model.load_state_dict(torch.load(model_path))\n        test_preds = test_loop(model, test_loader, target_dic_inv)\n        test_preds_lst.append(test_preds)\n        \n        sub_df['pressure'] = test_preds\n        sub_df.to_csv(f\"{config.OUTPUT}\/{config.EXP_NAME}\/sub_f{fold}.csv\", index=None)\n\n        train_df['oof'] = oof\n        train_df.to_csv(f\"{config.OUTPUT}\/{config.EXP_NAME}\/oof.csv\", index=None)\n        wandb.finish()\n\n        del model, optimizer, scheduler, train_loader, valid_loader, train_dset, valid_dset\n        torch.cuda.empty_cache()\n        gc.collect()\n    \n    sub_df['pressure'] = np.stack(test_preds_lst).mean(0)\n    sub_df.to_csv(f\"{config.OUTPUT}\/{config.EXP_NAME}\/submission_mean.csv\", index=None)\n\n    sub_df['pressure'] = np.median(np.stack(test_preds_lst), axis=0)\n    sub_df.to_csv(f\"{config.OUTPUT}\/{config.EXP_NAME}\/submission_median.csv\", index=None)\n    \n    # Post Processing: https:\/\/www.kaggle.com\/snnclsr\/a-dummy-approach-to-improve-your-score-postprocess\n    unique_pressures = train_df[\"pressure\"].unique()\n    sorted_pressures = np.sort(unique_pressures)\n    total_pressures_len = len(sorted_pressures)\n\n    def find_nearest(prediction):\n        insert_idx = np.searchsorted(sorted_pressures, prediction)\n        if insert_idx == total_pressures_len:\n            # If the predicted value is bigger than the highest pressure in the train dataset,\n            # return the max value.\n            return sorted_pressures[-1]\n        elif insert_idx == 0:\n            # Same control but for the lower bound.\n            return sorted_pressures[0]\n        lower_val = sorted_pressures[insert_idx - 1]\n        upper_val = sorted_pressures[insert_idx]\n        return lower_val if abs(lower_val - prediction) < abs(upper_val - prediction) else upper_val\n    \n    sub_df = pd.read_csv(f\"{config.OUTPUT}\/{config.EXP_NAME}\/submission_mean.csv\")\n    sub_df[\"pressure\"] = sub_df[\"pressure\"].apply(find_nearest)\n    sub_df.to_csv(f\"{config.OUTPUT}\/{config.EXP_NAME}\/submission_mean_pp.csv\", index=None)\n    \n    sub_df = pd.read_csv(f\"{config.OUTPUT}\/{config.EXP_NAME}\/submission_median.csv\")\n    sub_df[\"pressure\"] = sub_df[\"pressure\"].apply(find_nearest)\n    sub_df.to_csv(f\"{config.OUTPUT}\/{config.EXP_NAME}\/submission_median_pp.csv\", index=None)\n    \n    cv_score = train_df.apply(lambda x: abs(x['oof'] - x['pressure']), axis=1).mean()\n    print(\"CV:\", cv_score)","dab25fea":"if __name__ == \"__main__\":\n    main()","fe255cc3":"Most sharing code train this dataset as a regression task.\n\nBut in this code, I train as a classification task.\n\nI encode the target value pressure to 950 classes and calculate CrossEntropy Loss.","e5d7144e":"# Train classification"}}