{"cell_type":{"b14ab889":"code","e48552b4":"code","8b447b25":"code","2e812511":"code","ece89a16":"code","4c4749df":"code","aae77966":"code","d9af1302":"code","4e5d85b7":"code","100f9187":"code","cc74825c":"code","4235d6d6":"code","51cbfcd7":"code","480926c7":"code","a178e382":"code","c8a67528":"code","a4753a20":"markdown","2cce1e39":"markdown","62dcfbdd":"markdown","8ce1d41e":"markdown","a8f42da2":"markdown","ea9eecb1":"markdown","8055cafc":"markdown","275d86ac":"markdown","57e071df":"markdown","17704087":"markdown","8130d332":"markdown"},"source":{"b14ab889":"%matplotlib inline\nimport os\nimport re\nimport random\nimport string\nimport itertools\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom collections import Counter\n\nimport nltk\nfrom nltk.corpus import stopwords\nfrom nltk.tokenize import word_tokenize\n\nimport torch\nfrom torch import nn\nfrom torch.utils.data import Dataset, DataLoader\nfrom torch.optim.lr_scheduler import CosineAnnealingLR\n\nimport pytorch_lightning as pl\nfrom pytorch_lightning import metrics\nfrom pytorch_lightning.trainer import Trainer\nimport transformers","e48552b4":"class cfg:\n    data_dir = '..\/input\/commonlitreadabilityprize'\n    max_len = 256\n    num_workers = 8\n    epoch = 30\n    batch_size = 8\n    lr = 0.0001\n    seed = 0\n    BERT_MODEL = '..\/input\/huggingface-bert\/bert-base-uncased'","8b447b25":"def seed_everything(seed):\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = True","2e812511":"def text_preprocessing(text):\n    # Delete not-used Word\n    e = re.sub(\"[^a-zA-Z]\", \" \", text)\n    for p in string.punctuation:\n        e = e.replace(p, \" \")\n\n    # Delete Continuous Blank\n    e = re.sub(r\" +\", r\" \", e).strip()\n\n    e = e.lower()\n\n    # Stop Word\n    e = nltk.word_tokenize(e)\n    e = [word for word in e if not word in set(stopwords.words(\"english\"))]\n\n    # Standardize\n    lemma = nltk.WordNetLemmatizer()\n    e = [lemma.lemmatize(word) for word in e]\n    e = \" \".join(e)\n\n    return e","ece89a16":"class LitDataset(Dataset):\n    def __init__(self, df, tokenizer, text_preprocessing_fn=None, max_len=256, phase='train'):\n        self.df = df\n        self.tokenizer = tokenizer\n        self.max_len = max_len\n        self.text_preprocessing_fn = text_preprocessing_fn\n        self.phase = phase\n\n    def __len__(self):\n        return len(self.df)\n\n    def _tokenize(self, text):\n        # Tokenize\n        # BERT Tokenizer\n        out = self.tokenizer.encode_plus(\n            text,\n            None,\n            truncation=True,\n            add_special_tokens=True,\n            max_length=self.max_len,\n            padding='max_length'\n        )\n\n        ids = out['input_ids']\n        mask = out['attention_mask']\n        ttis = out['token_type_ids']\n\n        del out\n\n        return ids, mask, ttis\n\n\n    def __getitem__(self, idx):\n        row = self.df.iloc[idx]\n        text = row['excerpt']\n\n        # Preprocessing\n        if self.text_preprocessing_fn is not None:\n            text = self.text_preprocessing_fn(text)\n\n        # Tokenize & Encode\n        ids, mask, ttis = self._tokenize(text)\n\n        # Concatinate Input\n        inp = {\n            'ids': torch.tensor(ids, dtype=torch.long),\n            'mask': torch.tensor(mask, dtype=torch.long),\n            'ttis': torch.tensor(ttis, dtype=torch.long)\n        }\n\n        if self.phase != 'test':\n            target = torch.tensor(row['target'], dtype=torch.float)\n            return inp, target\n\n        else:\n            ids = row['id']\n            return inp, ids","4c4749df":"# Sanity Check\ndf = pd.read_csv(os.path.join(cfg.data_dir, 'train.csv'))\n\n# BERT Tokenizer\ntokenizer = transformers.AutoTokenizer.from_pretrained(\"..\/input\/bert-base-uncased\")  # Offine\n# tokenizer = transformers.BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)  # OnLine\n\n# Dataset\ndataset = LitDataset(df, text_preprocessing_fn=text_preprocessing, tokenizer=tokenizer, max_len=128)\n\n# Print Outputs\ntext, target = dataset.__getitem__(7)\nprint(text['ids'])\nprint(text['mask'])\nprint(text['ttis'])","aae77966":"class LitDataModule(pl.LightningDataModule):\n    def __init__(self, cfg, tokenizer, text_preprocessing_fn=None):\n        \"\"\"\n        ------------------------------------\n        Parameters\n        cfg: DictConfig\n            Config\n        tokenizer:\n            Pretrained Tokenizer\n        text_preprocessing_fn: func\n            Text Preprocessing Function: str -> str\n        \"\"\"\n        super(LitDataModule, self).__init__()\n        self.cfg = cfg\n        self.tokenizer = tokenizer\n        self.text_preprocessing_fn = text_preprocessing_fn\n\n\n    def prepare_data(self):\n        # Load Data\n        self.train = pd.read_csv(os.path.join(self.cfg.data_dir, 'train.csv'))\n        self.test = pd.read_csv(os.path.join(self.cfg.data_dir, 'test.csv'))\n\n    def setup(self, stage=None):\n        # Split trian valid\n        train_val_rate = 0.8\n        tmp_train = self.train[:int(len(self.train) * train_val_rate)]\n        tmp_val = self.train[int(len(self.train) * train_val_rate):]\n\n\n        self.train_dataset = LitDataset(\n            tmp_train,\n            tokenizer=self.tokenizer,\n            text_preprocessing_fn=self.text_preprocessing_fn,\n            max_len=self.cfg.max_len,\n            phase='train'\n        )\n\n        self.val_dataset = LitDataset(\n            tmp_val,\n            tokenizer=self.tokenizer,\n            text_preprocessing_fn=self.text_preprocessing_fn,\n            max_len=self.cfg.max_len,\n            phase='train'\n        )\n\n        self.test_dataset = LitDataset(\n            self.test,\n            tokenizer=self.tokenizer,\n            text_preprocessing_fn=self.text_preprocessing_fn,\n            max_len=self.cfg.max_len,\n            phase='test'\n        )\n\n    def train_dataloader(self):\n        return DataLoader(self.train_dataset,\n                          batch_size=self.cfg.batch_size,\n                          shuffle=True,\n                          num_workers=self.cfg.num_workers,\n                          pin_memory=True)\n\n\n    def val_dataloader(self):\n        return DataLoader(self.val_dataset,\n                          batch_size=self.cfg.batch_size,\n                          shuffle=False,\n                          num_workers=self.cfg.num_workers,\n                          pin_memory=True)\n\n    def test_dataloader(self):\n        return DataLoader(self.test_dataset,\n                          batch_size=self.cfg.batch_size,\n                          shuffle=False,\n                          num_workers=self.cfg.num_workers,\n                          pin_memory=True)","d9af1302":"# Sanity Check\n# BERT Tokenizer\ntokenizer = transformers.AutoTokenizer.from_pretrained(\"..\/input\/bert-base-uncased\")  # Offine\n# tokenizer = transformers.BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)  # OnLine\n\n# DataModule\ndm = LitDataModule(\n    cfg=cfg,\n    tokenizer=tokenizer,\n    text_preprocessing_fn=text_preprocessing\n)\n\ndm.prepare_data()\ndm.setup()\n\ndataloader = dm.train_dataloader()\n\nbatch, tar = next(iter(dataloader))\n\nprint(batch['ids'].size())\nprint(batch['mask'].size())\nprint(tar)","4e5d85b7":"class LitLightningSystem(pl.LightningModule):\n    def __init__(self, net, cfg, criterion, optimizer, scheduler=None):\n        \"\"\"\n        ------------------------------------\n        Parameters\n        net: torch.nn.Module\n            Model Network\n        cfg: DictConfig\n            Config\n        optimizer: torch.optim\n            Optimizer\n        scheduler: torch.optim.lr_scheduler\n            Learning Rate Scheduler\n        \"\"\"\n        super(LitLightningSystem, self).__init__()\n        self.net = net\n        self.cfg = cfg\n        self.criterion = criterion\n        self.optimizer = optimizer\n        self.scheduler = scheduler\n        self.weight_paths = []\n        self.best_rmse = 1e+9\n        self.losses = []\n        self.rmses = []\n\n    def configure_optimizers(self):\n        if self.scheduler is None:\n            return [self.optimizer], []\n        else:\n            return [self.optimizer], [self.scheduler]\n\n        \n    def forward(self, ids, mask, ttis):\n        output = self.net(ids=ids, mask=mask, token_type_ids=ttis)\n        return output\n\n\n    def training_step(self, batch, batch_idx):\n        inp, target = batch\n        out = self.forward(inp['ids'], inp['mask'], inp['ttis'])\n\n        loss = self.criterion(out.view_as(target), target)\n\n        return {'loss': loss}\n\n\n    def validation_step(self, batch, batch_idx):\n        inp, target = batch\n        out = self.forward(inp['ids'], inp['mask'], inp['ttis'])\n\n        loss = self.criterion(out.view_as(target), target)\n\n        return {'val_loss': loss, 'outputs': out, 'targets': target}\n\n\n    def validation_epoch_end(self, outputs):\n        avg_loss = torch.stack([x['val_loss'] for x in outputs]).mean()\n        res = torch.cat([x['outputs'] for x in outputs]).reshape((-1))\n        targets = torch.cat([x['targets'] for x in outputs]).reshape((-1))\n\n        # RMSE\n        rmse = torch.sqrt(self.criterion(res.view_as(targets), targets))\n\n        # Logging\n        self.losses.append(avg_loss.item())\n        self.rmses.append(rmse.item())\n\n        # Save Weights\n        if rmse.item() < self.best_rmse:\n            filename = 'seed_{}_epoch_{}_loss_{:.3f}_rmse_{:.3f}.pth'.format(\n                self.cfg.seed, self.current_epoch, avg_loss.item(), rmse.item()\n            )\n            torch.save(self.net.state_dict(), filename)\n\n            self.best_rmse = rmse.item()\n\n        return None\n\n\n    def test_step(self, batch, batch_idx):\n        inp, ids = batch\n        out = self.forward(inp['ids'], inp['mask'], inp['ttis'])\n\n        return {'preds': out, 'ids': ids}\n\n\n    def test_epoch_end(self, outputs):\n        preds = torch.cat([x['preds'] for x in outputs]).detach().cpu().numpy()\n        res = pd.DataFrame(preds, columns=['target'])\n\n        ids = [x['ids'] for x in outputs]\n        ids = [list(x) for x in ids]\n        ids = list(itertools.chain.from_iterable(ids))\n\n        res.insert(0, 'id', ids)\n\n        sub_file = 'submission.csv'\n        res.to_csv(sub_file, index=False)\n        self.res = res\n\n        return None","100f9187":"class BERT_Network(nn.Module):\n    def __init__(self, cfg):\n        super(BERT_Network, self).__init__()\n        self.bert = transformers.BertModel.from_pretrained(cfg.BERT_MODEL)\n        self.drop = nn.Dropout(0.5)\n        self.out = nn.Linear(768, 1)\n\n    def forward(self, ids, mask, token_type_ids):\n        _, output = self.bert(ids, attention_mask=mask, token_type_ids=token_type_ids, return_dict=False)\n        output = self.drop(output)\n        output = self.out(output)\n        return output","cc74825c":"# Seed\nseed_everything(cfg.seed)\n\n# BERT Tokenizer\ntokenizer = transformers.AutoTokenizer.from_pretrained(\"..\/input\/bert-base-uncased\")  # Offine\n# tokenizer = transformers.BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)  # OnLine\n\n# Network\nnet = BERT_Network(cfg)\n\n# DataModule\ndm = LitDataModule(\n    cfg=cfg,\n    tokenizer=tokenizer,\n    text_preprocessing_fn=text_preprocessing\n)\n\n# Criterion - MSE (From Pytorch Lightning)\ncriterion = metrics.MeanSquaredError()\n\n# Optimizer & Scheduler\noptimizer = transformers.AdamW(net.parameters(), lr=cfg.lr)\nscheculer = CosineAnnealingLR(optimizer, T_max=cfg.epoch, eta_min=0)\n\n# LightningSystem\nmodel = LitLightningSystem(net, cfg, criterion, optimizer, scheculer)\n\n# Trainer  ------------------------------------------------------\ntrainer = Trainer(\n    logger=None,\n    max_epochs=cfg.epoch,\n    gpus=-1,\n    num_sanity_val_steps=0,\n)","4235d6d6":"# Train\ntrainer.fit(model, datamodule=dm)","51cbfcd7":"# History Plot\nloss_df = pd.DataFrame({\n    'epoch': np.arange(1, len(model.losses) + 1),\n    'loss': model.losses,\n    'rmse': model.rmses\n})\n\nfig = plt.figure(figsize=(16, 6))\nax = fig.add_subplot(111)\n\n# RMSE\nsns.lineplot(x='epoch', y='rmse', data=loss_df, ax=ax)\nax.set_title('RMSE Plot')\n\nplt.show()","480926c7":"# Inference\ntrainer.test(model, datamodule=dm)","a178e382":"model.res","c8a67528":"# Create CSV\nmodel.res.to_csv('submission.csv', index=False)","a4753a20":"## LightningModule","2cce1e39":"## Train","62dcfbdd":"## Config","8ce1d41e":"## Library Install","a8f42da2":"## Utils","ea9eecb1":"## Lightning Data Module","8055cafc":"## Preprocessing\n\nText Preprocessing Function","275d86ac":"## Inference","57e071df":"## Bert Network","17704087":"## Dataset","8130d332":"# CommonLit - Pytorch Lightning & BERT Benchmark\n\n---\n\n## Library\n\n- Pytorch Lightning\n- Transformers (Pretrained BERT)\n\n---"}}