{"cell_type":{"62efa89a":"code","413712fa":"code","a07ce677":"code","74eb09a0":"code","2f6f1b39":"code","2f8b5d85":"code","d746965e":"code","e5dae32c":"code","d1066c13":"code","1f4150aa":"code","4fc0bb1c":"code","dfded064":"code","e38af331":"code","59a7a59b":"code","c340a09b":"code","cedf1b70":"code","e7527aa2":"code","619bd100":"code","19c7a06d":"code","679e9be7":"code","4d824716":"code","3486526a":"code","486fc2dd":"code","1a093d72":"code","6402b363":"code","a8085900":"code","4e434697":"code","42de3dce":"code","f30634ae":"code","609b774a":"code","c8a849a5":"code","7486abd5":"code","c2b44762":"code","1a2a5ff7":"code","5510818d":"code","e3cdcb08":"code","9e418ae1":"code","984889c4":"code","a35fe799":"code","78e36a55":"code","32bbe424":"code","db7a424a":"code","cd818444":"code","f55e5327":"code","7446f035":"code","f88739c0":"code","250cbc66":"code","499f18df":"code","0f0a7ba2":"code","535c2bfa":"markdown","997dfa34":"markdown","120fadbc":"markdown","5e1cfb27":"markdown","e2cf68d0":"markdown","0875792e":"markdown","496c6a20":"markdown","e32212e9":"markdown","64d2bd0c":"markdown","e9e71f86":"markdown","2b8d2df3":"markdown","9e0085f9":"markdown","369f27fe":"markdown","8eadef84":"markdown","5efa7ba8":"markdown","301272c7":"markdown","afb58eb0":"markdown","f6195cae":"markdown","6c6de0b5":"markdown","f195cb47":"markdown","9b99f45b":"markdown","06ba6535":"markdown","1ee27c3e":"markdown","c4dfd6a4":"markdown"},"source":{"62efa89a":"import shutil\nimport sys\n\nimport numpy as np\nfrom scipy import sparse\n\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nimport seaborn as sn\nsn.set()\n\nimport pandas as pd\n\nimport random\nimport tensorflow as tf\n#from tensorflow.contrib.layers import apply_regularization, l2_regularizer #Paquetes eliminados\n\nimport bottleneck as bn\n\nimport os\n","413712fa":"#Cargando datos\nraw_data = pd.read_csv(\"..\/input\/movielens-20m-dataset\/rating.csv\", header = 0)","a07ce677":"raw_data.head()","74eb09a0":"print(\"# Ratings < 0.5: \",sum(raw_data['rating']<0.5))\nprint(\"# Ratings = 0.5: \",sum(raw_data['rating']==0.5))\nprint(\"# Ratings > 3.5: \",sum(raw_data['rating']>3.5))\nprint(\"# Ratings = 4.5: \",sum(raw_data['rating']==4.5))\n\nraw_data = raw_data[raw_data['rating']>3.5]\nraw_data.head()","2f6f1b39":"#Funci\u00f3n que cuenta el n\u00famero de determinados \u00edtems agrupados \nprint(raw_data[['movieId']].groupby('movieId', as_index=False).size)\ndef get_count( data, item ):\n    playcount_groupbyid = data[[item]].groupby(item, as_index=False) #Agrupa por \u00edtems y cuenta el #\u00a0de c\/grupo\n    count = playcount_groupbyid.size()['size']\n    return count","2f8b5d85":"get_count(raw_data, 'userId')","d746965e":"#Funci\u00f3n de tripletas filtradas (datos, conteo de usuarios y conteo de \u00edtems)\ndef filter_triplets( data, min_uc = 5, min_sc = 5 ):\n    #Considerando tripletas solo para \u00edtems que han sido seleccionadas por al menos min_sc usuarios\n    if min_sc > 0:\n        item_count = get_count( data, 'movieId')\n        data = data[ data[ 'movieId' ].isin(item_count.index[ item_count >= min_sc ] ) ]  #i.sin donde see encuentran los valores en el data frame\n    #Considerando tripletas para usuarios que interactuaron con al menos min_uc \u00edtems                                         \n    if min_uc > 0:\n        user_count = get_count( data, 'userId' )\n        data = data[ data[ 'userId' ].isin(user_count.index[ user_count >= min_uc ] ) ]  #i.sin donde see encuentran los valores en el data frame                                    \n    #Actualizando el contador de items y de usuarios despues de los filtros\n    item_count = get_count( data, 'movieId' )\n    user_count = get_count( data, 'userId' )\n    return data, user_count, item_count \n","e5dae32c":"raw_data, user_activity, item_popularity = filter_triplets(raw_data)","d1066c13":"sparsity = 1. * raw_data.shape[0] \/ (user_activity.shape[0] * item_popularity.shape[0])\n\nprint(\"Despu\u00e9s de los filtros, hay %d eventos de %d usuarios y %d pel\u00edculas (\u00edtems) (dispersi\u00f3n: %.3f%%)\" % \n      (raw_data.shape[0], user_activity.shape[0], item_popularity.shape[0], sparsity * 100))","1f4150aa":"print(raw_data.shape)\nprint(item_popularity.shape)\nprint(user_activity.shape)","4fc0bb1c":"#Index \u00fanico de usuario\nunique_uid = user_activity.index\nnp.random.seed(98765)\nidx_perm = np.random.permutation(unique_uid.size)#permutando los indices de los usuarios\nunique_uid = unique_uid[idx_perm]\nprint(unique_uid.size)","dfded064":"n_users = unique_uid.size\nn_heldout_users = 10000\n\ntr_users = unique_uid[:(n_users - n_heldout_users * 2)] #135308-20000 = 115308\n#10000\nvd_users = unique_uid[(n_users - n_heldout_users * 2): (n_users - n_heldout_users)] #115308+10000 =125308 \n#10000\nte_users = unique_uid[(n_users - n_heldout_users):] #125308 + 10000 = 135308","e38af331":"#Seleccionando a todos los usuarios de entrenamiento de nuestros datos\ntrain_plays = raw_data.loc[raw_data['userId'].isin(tr_users)]\n","59a7a59b":"#Pel\u00edculas sin repeticion\nunique_sid = pd.unique( train_plays[ 'movieId'] )","c340a09b":"#Diccionario para usuarios \nprofile2id = dict((pid,i) for (i, pid) in enumerate(unique_uid))\nprint(len(profile2id))\n#Diccionario para pel\u00edculas (enumero los \u00edndices)\nshow2id   = dict((sid, i) for (i, sid) in enumerate(unique_sid))\nprint(len(show2id))","cedf1b70":"DATA_DIR = ''\npro_dir = os.path.join(DATA_DIR, 'pro_sg')\n\nif not os.path.exists(pro_dir):\n    os.makedirs(pro_dir)\n\nwith open(os.path.join(pro_dir, 'unique_sid.txt'), 'w') as f:\n    for sid in unique_sid:\n        f.write('%s\\n' % sid)","e7527aa2":"def split_train_test_proportion(data, test_prop=0.2):\n    data_grouped_by_user = data.groupby('userId') #Agrupando por usuario\n    tr_list, te_list = list(), list() \n\n    np.random.seed(98765)\n\n    for i, (_, group) in enumerate(data_grouped_by_user):\n        n_items_u = len(group) #numero de items por usuario\n\n        if n_items_u >= 5:\n            idx = np.zeros(n_items_u, dtype='bool')\n            idx[np.random.choice(n_items_u, size=int(test_prop * n_items_u), replace=False).astype('int64')] = True\n\n            tr_list.append(group[np.logical_not(idx)])\n            te_list.append(group[idx])\n        else:\n            tr_list.append(group)\n\n        if i % 1000 == 0:\n            print(\"%d users sampled\" % i)\n            sys.stdout.flush()\n\n    data_tr = pd.concat(tr_list)\n    data_te = pd.concat(te_list)\n    \n    return data_tr, data_te","619bd100":"#Ubicaci\u00f3n usuarios para validaci\u00f3n \nvad_plays = raw_data.loc[raw_data['userId'].isin(vd_users)]\n#Ubicaci\u00f3n pel\u00edculas con usuarios de validaci\u00f3n\nvad_plays = vad_plays.loc[vad_plays['movieId'].isin(unique_sid)]","19c7a06d":"#Dividiendo datos de peliculas de usuarios de validaci\u00f3n en train y test data sets\nvad_plays_tr, vad_plays_te = split_train_test_proportion(vad_plays)\n","679e9be7":"#Ubicaci\u00f3n de usuarios en test data\ntest_plays = raw_data.loc[raw_data['userId'].isin(te_users)]\n#Ubicaci\u00f3n de items en test data\ntest_plays = test_plays.loc[test_plays['movieId'].isin(unique_sid)]","4d824716":"#Dividiendo datos de peliculas de usuarios de los datos de prueba (test sets) en train y test data sets\ntest_plays_tr, test_plays_te = split_train_test_proportion(test_plays)\n","3486526a":"def numerize(data):\n    uid = list(map(lambda x: profile2id[x], data['userId']))\n    sid = list(map(lambda x: show2id[x],   data['movieId']))\n    return pd.DataFrame(data={'uid': uid, 'sid': sid}, columns=['uid', 'sid'])","486fc2dd":"train_data = numerize(train_plays)\ntrain_data.to_csv(os.path.join(pro_dir, 'train.csv'), index=False)","1a093d72":"vad_data_tr = numerize(vad_plays_tr)\nvad_data_tr.to_csv(os.path.join(pro_dir, 'validation_tr.csv'), index=False)","6402b363":"vad_data_te = numerize(vad_plays_te)\nvad_data_te.to_csv(os.path.join(pro_dir, 'validation_te.csv'), index=False)","a8085900":"test_data_tr = numerize(test_plays_tr)\ntest_data_tr.to_csv(os.path.join(pro_dir, 'test_tr.csv'), index=False)","4e434697":"test_data_te = numerize(test_plays_te)\ntest_data_te.to_csv(os.path.join(pro_dir, 'test_te.csv'), index=False)","42de3dce":"#Clase de funciones para MULTINOMIAL DENOISING AUTOENCODERS (MULT-DAE)\nclass MultiDAE(object):\n    '''Argumentos:\n    p_dims = dimensi\u00f3n decoder,\n    q_dims = dimensi\u00f3n encoder,\n    lam    = par\u00e1metro de regularizaci\u00f3n\n    lr     = learning rate'''\n    def __init__(self, p_dims, q_dims=None, lam=0.01, lr=1e-3, random_seed=None):\n        self.p_dims = p_dims\n        if q_dims is None: \n            self.q_dims = p_dims[::-1]\n        else:\n            assert q_dims[0]  == p_dims[-1], \"Dimensi\u00f3n de Input y output deben ser iguales para autoencoders\"\n            assert q_dims[-1] == p_dims[0], \"Dimensi\u00f3n latente para los desajustes p- y q- de la red.\"\n            self.q_dims = q_dims\n        self.dims   = self.q_dims + self.p_dims[1:] \n\n        self.lam = lam\n        self.lr  = lr\n        self.random_seed = random_seed\n\n        self.construct_placeholders()\n\n    def construct_placeholders(self):\n        #Para variables de entrenamiento (PLACEHOLDER)\n        tf.compat.v1.disable_eager_execution()\n        self.input_ph     = tf.compat.v1.placeholder( dtype=tf.float32, shape=[None, self.dims[0]])#matriz\n        self.keep_prob_ph = tf.compat.v1.placeholder_with_default( 1.0, shape=None) #Devuelve un valor entre 0 y 1\n    #Representaci\u00f3n latente\n    def forward_pass(self):\n        h = tf.compat.v1.nn.l2_normalize( self.input_ph, 1) #Norma L2 de una matriz... devuelve un vector de la misma longitud que el input\n        #Computes dropout: randomly sets elements to zero to prevent overfitting\n        h = tf.compat.v1.nn.dropout( h, self.keep_prob_ph ) #With probability rate elements of x are set to 0. \n        #The remaining elements are scaled up by 1.0 \/ (1 - rate), so that the expected value is preserved.\n        #(Para evitar overfitting) #construyo vector y \n        for i, (w,b) in enumerate(zip(self.weights, self.biases)):\n            h = tf.compat.v1.matmul(h,w)+b #Representaci\u00f3n latente \n            if i != len(self.weights)-1: #Para la \u00faltima capa\n                h = tf.compat.v1.nn.tanh(h) #Funci\u00f3n de activaci\u00f3n\n        return tf.compat.v1.train.Saver(), h\n\n    def construct_weights(self):\n        #Construyendo weights\n        self.weights = []\n        self.biases  = []\n        #Defino pesos\n        for i, (d_in,d_out) in enumerate(zip(self.dims[:-1],self.dims[1:])): #Enumera dims de encoder y decoder \n            weight_key = \"Weight_{}to_{}\".format(i,i+1) #Nombres\n            bias_key   = \"Bias_{}\".format(i+1)\n            #Creando nuevas variables (weights) con tensorflow\n            #Matriz de pesos\n            self.weights.append(tf.compat.v1.get_variable( name = weight_key, shape = [d_in, d_out],\n                                          initializer = tf.keras.initializers.glorot_normal(seed = self.random_seed)))\n            #Vector de sesgos\n            self.biases.append(tf.compat.v1.get_variable( name = bias_key, shape = [d_out],\n                                          initializer = tf.compat.v1.truncated_normal_initializer(stddev=0.001,\n                                                                                        seed = self.random_seed)))\n            #Agregando resumen estad\u00edstico\n            tf.compat.v1.summary.histogram( weight_key, self.weights[-1])\n            tf.compat.v1.summary.histogram( bias_key, self.biases[-1])\n    def loss(self):\n        with tf.GradientTape() as tape:\n            with tf.compat.v1.Session() as sess:\n                saver, logits = self.forward_pass()\n                log_softmax_var = tf.compat.v1.nn.log_softmax(logits)\n                neg_ll = -tf.compat.v1.reduce_mean(tf.reduce_sum(log_softmax_var * self.input_ph, axis=1))\n                reg_var = tf.compat.v1.nn.l2_loss(self.lam)\n                return neg_ll + 2.0 * reg_var\n        train_op = tf.optimizers.Adam(self.lr).minimize(loss, var_list=[self.weights,self.biases])\n        return train_op\n    def build_graph(self):\n\n        self.construct_weights()\n\n        saver, logits = self.forward_pass()\n        log_softmax_var = tf.compat.v1.nn.log_softmax(logits)\n        neg_ll = -tf.compat.v1.reduce_mean(tf.compat.v1.reduce_sum(log_softmax_var * self.input_ph, axis=1))\n        reg_var = tf.add_n([ tf.nn.l2_loss(v) for v in self.weights ])\n        print(np.shape(self.weights))\n        loss    = neg_ll + 2.0 * reg_var \n        train_op = tf.compat.v1.train.AdamOptimizer(self.lr).minimize(loss)#self.loss()\n        #train_op = tf.optimizers.Adam(self.lr).minimize(loss, var_list=[self.weights,self.biases])\n        # add summary statistics\n        tf.compat.v1.summary.scalar('negative_multi_ll', neg_ll)\n        tf.compat.v1.summary.scalar('loss', loss)\n        merged = tf.compat.v1.summary.merge_all()\n        return saver, logits, loss, train_op, merged","f30634ae":"class MultiVAE(MultiDAE):\n    def construct_placeholders(self):#Construyendo variables\n        super(MultiVAE, self).construct_placeholders() #Esta funci\u00f3n nos permite invocar y \n        #conservar un m\u00e9todo o atributo de una clase padre (primaria) desde una clase hija (secundaria) \n        #sin tener que nombrarla expl\u00edcitamente.\n        # placeholders with default values when scoring\n        self.is_training_ph = tf.compat.v1.placeholder_with_default(0., shape=None)\n        self.anneal_ph = tf.compat.v1.placeholder_with_default(1., shape=None)\n\n    def build_graph(self):\n        self._construct_weights()\n        saver, logits, KL = self.forward_pass()\n        log_softmax_var = tf.compat.v1.nn.log_softmax(logits) #Funci\u00f3n de activaci\u00f3n \n        neg_ll = -tf.compat.v1.reduce_mean(tf.reduce_sum(log_softmax_var*self.input_ph,axis=-1))\n        #Aplicando regularizaci\u00f3n a los pesos\n        reg_var = tf.add_n([ tf.nn.l2_loss(v) for v in (self.weights_q + self.weights_p) ])\n        # tensorflow l2 regularization multiply 0.5 to the l2 norm\n        # multiply 2 so that it is back in the same scale\n        neg_ELBO = neg_ll + self.anneal_ph * KL + 2.0 * reg_var #'''Ecuaci\u00f3n 5  \u00bfPor qu\u00e9 suma esto 2 * reg_var?'''\n        #Optimizando parametros \n        train_op = tf.compat.v1.train.AdamOptimizer(self.lr).minimize(neg_ELBO)\n\n        #Resumen estad\u00edsticos\n        tf.compat.v1.summary.scalar('negative_multi_ll', neg_ll)\n        tf.compat.v1.summary.scalar('KL', KL)\n        tf.compat.v1.summary.scalar('neg_ELBO_train', neg_ELBO)\n        merged = tf.compat.v1.summary.merge_all() #Juntando todos los res\u00famenes estad\u00edsticos\n\n        return saver, logits, neg_ELBO, train_op, merged\n\n\n#Redes\n    def q_graph(self): #encoder\n        mu_q, std_q, KL = None, None, None\n        h = tf.compat.v1.nn.l2_normalize( self.input_ph, 1)\n        h = tf.compat.v1.nn.dropout( h, self.keep_prob_ph )\n        print(h)\n        for i, (w,b) in enumerate(zip(self.weights_q,self.biases_q)): \n            #print(\"W\", w)\n            #print(\"b: \", b)\n            #print(\"Hereee: \", tf.compat.v1.matmul(h,w)) #HERE IS THE PROBLEM \n            h = tf.compat.v1.matmul(h,w) + b\n            \n            if i != len(self.weights_q)-1:\n                h = tf.compat.v1.nn.tanh(h)\n                \n            else: \n                #Se duplica la dimensi\u00f3n de la \u00faltima capa del encoder (q_dims)\n                mu_q     = h[:,:self.q_dims[-1]] #Matriz de medias xiu[:,:dim_out] dim_out del encoder\n                logvar_q = h[:,self.q_dims[-1]:] \n                std_q    = tf.exp(0.5*logvar_q)\n                KL       = tf.compat.v1.reduce_mean(tf.reduce_sum(0.5*(-logvar_q+tf.exp(logvar_q)+mu_q**2-1),\n                                                            axis = 1))\n        return mu_q, std_q, KL\n    def p_graph(self,z): #decoder\n        h = z\n        for i, (w,b) in enumerate(zip(self.weights_p,self.biases_p)):\n            h = tf.compat.v1.matmul(h,w)+b #Matriz de representacion latente\n            if i != len(self.weights_p)-1:\n                h = tf.compat.v1.nn.tanh(h)\n        return h\n    def forward_pass(self): #Latent space\n        #q-network\n        #print(\"here\")\n        mu_q, std_q, KL = self.q_graph()\n        #print(tf.shape(std_q))\n        epsilon         = tf.random.normal(tf.shape(std_q)) #Epsilon aleatorio \n        sampled_z       = mu_q + self.is_training_ph*\\\n            epsilon*std_q #Reparametrizaci\u00f3n\n        #p-network\n        logits = self.p_graph(sampled_z)\n        return tf.compat.v1.train.Saver(), logits, KL #train.Saver guarda y restaura variables\n\n\n    def _construct_weights(self):\n        #Construyendo weights encoder \n        self.weights_q = []\n        self.biases_q  = []\n        for i, (d_in, d_out) in enumerate(zip(self.q_dims[:-1],self.q_dims[1:])):\n            if i == len(self.q_dims[:-1])-1:#pen\u00faltima capa para el conjunto de par\u00e1metros (media y varianza)\n                d_out *= 2\n            weight_key = \"Weight_q_{}_to{}\".format(i,i+1)\n            bias_key   = \"Bias_q_{}\".format(i+1)\n\n            self.weights_q.append(tf.compat.v1.get_variable( name = weight_key, shape = [d_in, d_out],\n                            initializer = tf.keras.initializers.glorot_normal(seed = self.random_seed)))\n            self.biases_q.append(tf.compat.v1.get_variable( name = bias_key, shape = [d_out], \n                            initializer = tf.compat.v1.truncated_normal_initializer(stddev=0.001,\n                                                                                seed = self.random_seed)))\n        #Agregando resumen estad\u00edstico\n        tf.compat.v1.summary.histogram(weight_key, self.weights_q[-1])\n        tf.compat.v1.summary.histogram(bias_key, self.biases_q[-1])\n\n        #Construyendo weights decoder \n        self.weights_p = []\n        self.biases_p  = []\n        for i, (d_in, d_out) in enumerate(zip(self.p_dims[:-1],self.p_dims[1:])):\n            weight_key = \"Weight_p_{}_to_{}\".format(i,i+1)\n            bias_key   = \"Bias_p_{}\".format(i+1)\n\n            self.weights_p.append(tf.compat.v1.get_variable( name = weight_key, shape = [d_in, d_out],\n                            initializer = tf.keras.initializers.glorot_normal(seed = self.random_seed)))\n            self.biases_p.append(tf.compat.v1.get_variable( name = bias_key, shape = [d_out], \n                            initializer = tf.compat.v1.truncated_normal_initializer(stddev=0.001,\n                                                                                seed = self.random_seed)))\n        #Agregando resumen estad\u00edstico\n        tf.compat.v1.summary.histogram(weight_key, self.weights_p[-1])\n        tf.compat.v1.summary.histogram(bias_key, self.biases_p[-1])\n\n        ","609b774a":"unique_sid = list()\n#Datos de pel\u00edculas (\u00edndices)\nwith open(os.path.join(pro_dir, 'unique_sid.txt'), 'r') as f:\n    for line in f:\n        unique_sid.append(line.strip())\n\nn_items = len(unique_sid)","c8a849a5":"#Funci\u00f3n para cargar datos de entrenamiento\ndef load_train_data(file):\n    dat     = pd.read_csv(file)\n    n_users = dat['uid'].max()+1 #usuarios\n    \n    rows    = dat['uid']\n    cols    = dat['sid']\n    data    = sparse.csr_matrix((np.ones_like(rows),(rows, cols)),\n                               dtype = 'float64', shape=(n_users,n_items)) #convirtiendo en matriz sparse\n    return data","7486abd5":"#Cargando datos de entrenamiento \ntrain_data = load_train_data(os.path.join(pro_dir, 'train.csv'))","c2b44762":"type(train_data)","1a2a5ff7":"#Funci\u00f3n para cargar datos de validaci\u00f3n\ndef load_tr_te_data(file_tr,file_te):\n    dat_tr = pd.read_csv(file_tr)\n    dat_te = pd.read_csv(file_te)\n    #Rango de \u00edndices de items\n    start_idx = min(dat_tr['uid'].min(),dat_te['uid'].min())\n    end_idx   = max(dat_tr['uid'].max(),dat_te['uid'].max())\n    \n    rows_train = dat_tr['uid']-start_idx\n    cols_train = dat_tr['sid']\n    \n    rows_test  = dat_te['uid']-start_idx\n    cols_test  = dat_te['sid']\n    \n    data_train = sparse.csr_matrix((np.ones_like(rows_train),(rows_train, cols_train)),\n                               dtype = 'float64', shape=(end_idx-start_idx+1,n_items)) #convirtiendo en matriz sparse\n    data_test  = sparse.csr_matrix((np.ones_like(rows_test),(rows_test, cols_test)),\n                               dtype = 'float64', shape=(end_idx-start_idx+1,n_items)) #convirtiendo en matriz sparse\n    return data_train, data_test","5510818d":"#Cargando datos de validacion\n\nval_data_train,val_data_test = load_tr_te_data(os.path.join(pro_dir,'validation_tr.csv'),\n                                               os.path.join(pro_dir,'validation_te.csv'))","e3cdcb08":"type(val_data_test)","9e418ae1":"train_data = train_data[:10000,:]\nN = train_data.shape[0] #115308 observaciones\nidxlist = range(N) #Rango de indices\n\n#Batch size (entrenamiento)\nbatch_size = 500\nval = np.ceil(float(N)\/batch_size)\nbatches_per_epoch = int(val)#funci\u00f3n techo np.ceil\n\n#Par\u00e1metros de validaci\u00f3n\nN_vad = val_data_train.shape[0] #observaciones\nidxlist_vad = range(N_vad)\n#Batch size para validaci\u00f3n \nbatch_size_vad = 2000\n\n#el n\u00famero total de actualizaciones de gradiente para annealing\ntotal_anneal_steps = 200000\n#Annealing par\u00e1metro (m\u00e1s grande)\nanneal_cap = 0.2\n","984889c4":"\ndef NDCG_binary_at_k_batch(X_pred, heldout_batch, k=100):\n    '''\n    normalized discounted cumulative gain@k for binary relevance\n    ASSUMPTIONS: all the 0's in heldout_data indicate 0 relevance\n    '''\n    batch_users = X_pred.shape[0]\n    idx_topk_part = bn.argpartition(-X_pred, k, axis=1)\n    topk_part = X_pred[np.arange(batch_users)[:, np.newaxis],\n                       idx_topk_part[:, :k]]\n    idx_part = np.argsort(-topk_part, axis=1)\n    # X_pred[np.arange(batch_users)[:, np.newaxis], idx_topk] is the sorted\n    # topk predicted score\n    idx_topk = idx_topk_part[np.arange(batch_users)[:, np.newaxis], idx_part]\n    # build the discount template\n    tp = 1. \/ np.log2(np.arange(2, k + 2))\n\n    DCG = (heldout_batch[np.arange(batch_users)[:, np.newaxis],\n                         idx_topk].toarray() * tp).sum(axis=1)\n    IDCG = np.array([(tp[:min(n, k)]).sum()\n                     for n in heldout_batch.getnnz(axis=1)])\n    return DCG \/ IDCG","a35fe799":"def Recall_at_k_batch(X_pred, heldout_batch, k=100):\n    batch_users = X_pred.shape[0]\n\n    idx = bn.argpartition(-X_pred, k, axis=1)\n    X_pred_binary = np.zeros_like(X_pred, dtype=bool)\n    X_pred_binary[np.arange(batch_users)[:, np.newaxis], idx[:, :k]] = True\n\n    X_true_binary = (heldout_batch > 0).toarray()\n    tmp = (np.logical_and(X_true_binary, X_pred_binary).sum(axis=1)).astype(\n        np.float32)\n    recall = tmp \/ np.minimum(k, X_true_binary.sum(axis=1))\n    return recall","78e36a55":"p_dims = [200, n_items]","32bbe424":"tf.compat.v1.reset_default_graph()\ndae = MultiDAE(p_dims, lam=0.01 \/ batch_size, random_seed=98765)\nsaver, logits_var, loss_var, train_op_var, merged_var = dae.build_graph()\nndcg_var = tf.Variable(0.0)\nndcg_dist_var = tf.compat.v1.placeholder(dtype=tf.float64, shape=None)\nndcg_summary = tf.summary.scalar('ndcg_at_k_validation', ndcg_var)\nndcg_dist_summary = tf.summary.histogram('ndcg_at_k_hist_validation', ndcg_dist_var)\n#merged_valid = tf.compat.v1.summary.merge([ndcg_summary, ndcg_dist_summary])","db7a424a":"ndcg_dist_summary\nprint(logits_var)\n","cd818444":"n_epochs = 200","f55e5327":"ndcgs_vad = []\ni=0\nwith tf.compat.v1.Session() as sess:\n\n    init = tf.compat.v1.global_variables_initializer()\n    sess.run(init)\n\n    best_ndcg = -np.inf\n    \n    for epoch in range(n_epochs):\n        i+=1\n        idxlist = random.sample(list(idxlist), len(list(idxlist)))\n        # train for one epoch\n        for bnum, st_idx in enumerate(range(0, N, batch_size)):\n            end_idx = min(st_idx + batch_size, N)\n            X = train_data[idxlist[st_idx:end_idx]]\n            #print(\"X: \",st_idx,end_idx)\n            #print(type(train_data))\n            if sparse.isspmatrix(X):\n                X = X.toarray()\n            X = X.astype('float32')  \n            #print(X.shape)\n            feed_dict = {dae.input_ph: X, \n                         dae.keep_prob_ph: 0.5} \n            #print(feed_dict)\n            sess.run(train_op_var, feed_dict=feed_dict)\n            if bnum % 100 == 0:\n                summary_train = sess.run(merged_var, feed_dict=feed_dict)\n                #summary_writer.add_summary(summary_train, global_step=epoch * batches_per_epoch + bnum) \n        \n        # compute validation NDCG\n        ndcg_dist = []\n        for bnum, st_idx in enumerate(range(0, N_vad, batch_size_vad)):\n            end_idx = min(st_idx + batch_size_vad, N_vad)\n#            print()\n            X = val_data_train[idxlist_vad[st_idx:end_idx]]\n            #print(\"X\",X)\n            if sparse.isspmatrix(X):\n                X = X.toarray()\n            X = X.astype('float32')\n            #print(type(X))\n            pred_val = sess.run(logits_var, feed_dict={dae.input_ph: X} )\n            # exclude examples from training and validation (if any)\n            pred_val[X.nonzero()] = -np.inf\n            #print(\"Validation: \",NDCG_binary_at_k_batch(pred_val, val_data_test[idxlist_vad[st_idx:end_idx]]))\n            ndcg_dist.append(NDCG_binary_at_k_batch(pred_val, val_data_test[idxlist_vad[st_idx:end_idx]]))\n            #print(\"List: \", ndcg_dist)\n        print(\"Iteraci\u00f3n: \", i )\n        ndcg_dist = np.concatenate(ndcg_dist)\n        ndcg_dist = np.nan_to_num(ndcg_dist)\n        ndcg_ = ndcg_dist.mean()\n        print(\"Mean: \", ndcg_)\n        ndcgs_vad.append(ndcg_)","7446f035":"plt.figure(figsize=(12, 3))\nplt.plot(ndcgs_vad)\nplt.ylabel(\"Validation NDCG@100\")\nplt.xlabel(\"Epochs\")\npass\n\n","f88739c0":"tf.compat.v1.reset_default_graph()\np_dims = [200, 600, n_items]\n#tf.reset_default_graph()\nvae = MultiVAE(p_dims, lam=0.0, random_seed=98765) #Llama a la clase MultiVAE\nsaver, logits_var, loss_var, train_op_var, merged_var = vae.build_graph() #Construccion del grafo\nndcg_var = tf.Variable(0.0)\nndcg_dist_var = tf.compat.v1.placeholder(dtype=tf.float64, shape=None)\nndcg_summary = tf.summary.scalar('ndcg_at_k_validation', ndcg_var)\nndcg_dist_summary = tf.summary.histogram('ndcg_at_k_hist_validation', ndcg_dist_var)\n#merged_valid = tf.compat.v1.summary.merge([ndcg_summary, ndcg_dist_summary])","250cbc66":"n_epochs = 200\n","499f18df":"ndcgs_vad = []\ni=0\nwith tf.compat.v1.Session() as sess:\n\n    init = tf.compat.v1.global_variables_initializer()\n    sess.run(init)\n\n    best_ndcg = -np.inf\n\n    update_count = 0.0\n    \n    for epoch in range(n_epochs):\n        i+=1\n        idxlist = random.sample(list(idxlist), len(list(idxlist)))\n        # train for one epoch\n        for bnum, st_idx in enumerate(range(0, N, batch_size)):\n            end_idx = min(st_idx + batch_size, N)\n            X = train_data[idxlist[st_idx:end_idx]]\n            \n            if sparse.isspmatrix(X):\n                X = X.toarray()\n            X = X.astype('float32')           \n            \n            if total_anneal_steps > 0:\n                anneal = min(anneal_cap, 1. * update_count \/ total_anneal_steps)\n            else:\n                anneal = anneal_cap\n            \n            feed_dict = {vae.input_ph: X, \n                         vae.keep_prob_ph: 0.5, \n                         vae.anneal_ph: anneal,\n                         vae.is_training_ph: 1}        \n            sess.run(train_op_var, feed_dict=feed_dict)\n\n            if bnum % 100 == 0:\n                summary_train = sess.run(merged_var, feed_dict=feed_dict)\n            update_count += 1\n        \n        # compute validation NDCG\n        ndcg_dist = []\n        for bnum, st_idx in enumerate(range(0, N_vad, batch_size_vad)):\n            end_idx = min(st_idx + batch_size_vad, N_vad)\n            X = val_data_train[idxlist_vad[st_idx:end_idx]]\n            if sparse.isspmatrix(X):\n                X = X.toarray()\n            X = X.astype('float32')\n        \n            pred_val = sess.run(logits_var, feed_dict={vae.input_ph: X,vae.keep_prob_ph: 0.5, \n                         vae.anneal_ph: anneal,\n                         vae.is_training_ph: 1} )\n            # exclude examples from training and validation (if any)\n            pred_val[X.nonzero()] = -np.inf\n            ndcg_dist.append(NDCG_binary_at_k_batch(pred_val, val_data_test[idxlist_vad[st_idx:end_idx]]))\n        print(\"Iteraci\u00f3n: \", i )\n        ndcg_dist = np.concatenate(ndcg_dist)\n        ndcg_dist = np.nan_to_num(ndcg_dist)\n        ndcg_ = ndcg_dist.mean()\n        print(\"Mean: \", ndcg_)\n        ndcgs_vad.append(ndcg_)\n        \n","0f0a7ba2":"plt.figure(figsize=(12, 3))\nplt.plot(ndcgs_vad)\nplt.ylabel(\"Validation NDCG@100\")\nplt.xlabel(\"Epochs\")\npass","535c2bfa":"Medida del rendimiento medio del algoritmo:","997dfa34":"Creando train\/validation\/test para usuarios","120fadbc":"Evaluate function: Normalized discounted cumulative gain (NDCG@k) and Recall@k\n\n","5e1cfb27":"Considerando \u00edtems que fueron calificados por al menos 5 usuarios y usuarios que calificaron al menos 5 pel\u00edculas\n\n\n","e2cf68d0":"**Calculando 'dispersi\u00f3n'**\n* Se calcula el numerador de la m\u00e9trica de dispersi\u00f3n contando el n\u00famero total de calificaciones contenidas en la matriz de calificaciones.\n* Se calcula el denominador de la m\u00e9trica de dispersi\u00f3n multiplicando el n\u00famero de usuarios por el n\u00famero de pel\u00edculas en la matriz de clasificaci\u00f3n.\n* Se calcula e imprime la dispersi\u00f3n dividiendo el numerador por el denominador, restando de 1 y multiplicando por 100. Se agrega 1.0 para garantizar que la dispersi\u00f3n se devuelva como un decimal y no como un n\u00famero entero.","0875792e":"Funci\u00f3n para generar datos de entrenamiento y datos de prueba","496c6a20":"**Autoencoders:**\nUn autoencoder toma como input valores $x\\in[0,1]^d$ y primero lo mapea (con un encoder) a una representaci\u00f3n latente $h\\in[0,1]^{d^\\prime}$ mediante un mapeo determinista\n$$h=Wx+b,$$\ndonde $W$ representa a la matriz de pesos y $b$ el vector de sesgos (biases). \n\nLa representaci\u00f3n latente $h$ es mapeada de vuelta (con el decoder) en una reconstrucci\u00f3n $z$ del mismo tama\u00f1o que $x$ mediante una transformaci\u00f3n similar, es decir\n$$z= (W^\\prime h + b^\\prime).$$\n$z$ se debe ver como una predicci\u00f3n de $x$. La matriz de pesos $W^\\prime$ del mapeo inverso puede ser construido por $W^\\prime=W^T$.\n\nBuscamos \n$$\\min_\\theta\\|x-z\\|^2$$\n","e32212e9":"* The *Saver* class adds ops to save and restore variables to and from checkpoints. It also provides convenience methods to run these ops.\n* *tf.contrib.layers.xavier_initializer*: Draws samples from a truncated normal distribution centered on 0 with stddev = sqrt(2 \/ (fan_in + fan_out)) where fan_in is the number of input units in the weight tensor and fan_out is the number of output units in the weight tensor.","64d2bd0c":"# **DEFINICI\u00d3N Y ENTRENAMIENTO DEL MODELO**\n\n**Notaci\u00f3n:**\nSea $u\\in \\{1,...,U\\}$ los \u00edndices de los usuarios e $i\\in\\{1,...,I\\}$ los \u00edndices de \u00edtems. La matriz de interacci\u00f3n usuario-\u00edtem es $X\\in\\mathbb{N}^{U\\times I}$. La matriz de interacci\u00f3n es binarizada.\n\n**Proceso generativo:**\nPara cada usuario $u$, el modelo empieza por muestrear la representaci\u00f3n latente $K$-dimensional $\\mathbf{z}_u$ de una a priori Gaussiana est\u00e1ndar. La representaci\u00f3n latente $\\mathbf{z}_u$ es transformada por una funci\u00f3n no lineal $f_\\theta (\\cdot) \\in \\mathbb{R}^I$ para producir una distribuci\u00f3n de probabilidad sobre $I$ \u00edtems $\\pi (\\mathbf{z}_u)$ del cual el historial de interacci\u00f3n $\\mathbf{x}_u$ se asume que es\n$$\n\\mathbf{z}_{u} \\sim \\mathscr{N}\\left(0, \\mathbf{I}_{K}\\right), \\pi\\left(\\mathbf{z}_{u}\\right) \\propto \\exp \\left\\{f_{\\theta}\\left(\\mathbf{z}_{u}\\right)\\}, \\mathbf{x}_{u} \\sim \\operatorname{Mult}\\left(N_{u}, \\pi\\left(\\mathbf{z}_{u}\\right)\\right)\\right.\n$$\nEl objetivo de Multi-DAE para un solo usuario $u$ es:\n$$\n\\mathscr{L}_{u}(\\theta, \\phi)=\\log p_{\\theta}\\left(\\mathbf{x}_{u} \\mid g_{\\phi}\\left(\\mathbf{x}_{u}\\right)\\right)\n$$\ndonde $g_{\\phi}(\\cdot)$ es la funci\u00f3n \"encoder\" no-lineal.","e9e71f86":"q_dims = dimensi\u00f3n de las capas del encoder \n$\\mu=x_{ui}[:,:qdims[-1]]$, $qdims[-1]$ dimensi\u00f3n de la \u00faltima capa del encoder\n\n$\\log(var)=x_{ui}[:,qdims[-1]:]$\n\n$\\sigma = e^{0.5*\\log(var)}$\n","2b8d2df3":"# Entrenamiento y Validaci\u00f3n de datos, Hiperpar\u00e1metros","9e0085f9":"Hiperpar\u00e1metros de entrenamiento:","369f27fe":"# **Procedimiento de divisi\u00f3n de datos**\n* Seleccionar 10K usuarios como usuarios excluidos, 10K usuarios como usuarios de validaci\u00f3n y el resto de los usuarios para el entrenamiento.\n* Utilizar todos los \u00edtems de los usuarios de entrenamiento como conjunto de \u00edtems.\n* Para cada usuario de validaci\u00f3n y de prueba, submuestrear el 80% como datos plegables y el resto para la predicci\u00f3n.","8eadef84":"**Entrenando un Multi-DAE**","5efa7ba8":"Escribiendo archivos de pel\u00edculas","301272c7":"# Multinomal Denoising Autoencoder Model","afb58eb0":"**Discounted Cumulative Gain (DCG)** es la m\u00e9trica para medir la calidad de la clasificaci\u00f3n. Se utiliza principalmente en problemas de recuperaci\u00f3n de informaci\u00f3n, como medir la eficacia del algoritmo del motor de b\u00fasqueda clasificando los art\u00edculos que muestra de acuerdo con su relevancia en t\u00e9rminos de la palabra clave de b\u00fasqueda.","f6195cae":"Guardando los datos en formato user_index e item_index\n","6c6de0b5":"Visualizaci\u00f3n de datos:","f195cb47":"Cargamos los datos de entrenamiento y de validaci\u00f3n preprocesados ","9b99f45b":"Binarizando datos (Tomando los datos con rating >= 4).\n\nSe tiene una base de datos de calificaciones desde 0.5 a 5.  ","06ba6535":"**Entrenamiento de MultiVAE**\n","1ee27c3e":"La funci\u00f3n objetivo de Multi-VAE$^{PR}$ (ELBO) para un usuario $u$ es:\n$$\n\\mathcal{L}_u(\\theta, \\phi) = \\mathbb{E}_{q_\\phi(z_u | x_u)}[\\log p_\\theta(x_u | z_u)] - \\beta \\cdot KL(q_\\phi(z_u | x_u) \\| p(z_u))\n$$\ndonde $q_\\phi$ es la distribuci\u00f3n variacional aproximada (modelo de inferencia). $\\beta$ es el par\u00e1metro de control adicional. La funci\u00f3n objetivo de un conjunto de datos es el promedio sobre todos los usuarios. Esto se puede entrenar casi de la misma manera que Multi-DAE gracias al truco de la reparametrizaci\u00f3n.\n\nRecordemos que la distancia Kullback-Leiber est\u00e1 definida por \n$$\n\\operatorname{KL}\\left(q\\left(\\mathrm{z}_{u}\\right) \\| p\\left(\\mathrm{z}_{u} \\mid \\mathrm{x}_{u}\\right)\\right)=\\int_{-\\infty}^{\\infty} q\\left(\\mathrm{z}_{u}\\right) \\ln \\frac{q\\left(\\mathrm{z}_{u}\\right)}{p\\left(\\mathrm{z}_{u} \\mid \\mathrm{x}_{u}\\right)} \\mathrm{d} z_{u}\n$$\n\nAsumiendo que tenemos dos distribuciones con par\u00e1metros\n $q(z_u) \\sim N\\left(\\mu_u,diag\\{ \\sigma^{2}_u\\}\\right)$ y $p(z_u) \\sim N(0,I_K)$.\n \n Para agregar un poco m\u00e1s de contexto en t\u00e9rminos de modelos de variables latentes, intentamos ajustar una posterior aproximada a la posterior verdadera minimizando la divergencia KL. Sea $ z_u $ la variable latente, $q(z_u)$ es la distribuci\u00f3n aproximada y $ p(z_u) $ la distribuci\u00f3n a priori. Los par\u00e1metros de $q$ son la salida del encoder.\n\nLa divergencia KL para distribuciones Gaussianas es \n\n$$\n\\begin{aligned}\nK L(q \\| p) &=\\frac{1}{2} \\log (2 \\pi)+\\frac{1}{2}\\left(\\mu^{2}+\\sigma^{2}\\right)-\\frac{1}{2} \\log (2 \\pi)-\\frac{1}{2} \\log \\left(\\sigma^{2}\\right)-\\frac{1}{2} \\\\\n&=-\\frac{1}{2}\\left(1+\\log \\left(\\sigma^{2}\\right)+-\\mu^{2}-\\sigma^{2}\\right)\n\\end{aligned}\n$$\nLa ecuaci\u00f3n anterior se puede generalizar al caso multivariado, sumando sobre todas las dimensiones, i.e.,\n$$\nK L(q \\| p)=-\\frac{1}{2} \\sum_{d=1}^{D}\\left(1+\\log \\left(\\sigma_u^{2}\\right)+-\\mu_u^{2}-\\sigma_u^{2}\\right)\n$$\n\nPara la divergencia KL ver\nhttps:\/\/leenashekhar.github.io\/2019-01-30-KL-Divergence\/","c4dfd6a4":"# Multinomial Variational Autoencoder"}}