{"cell_type":{"1a1c54d1":"code","b5196319":"code","e7b0b55a":"code","16ff6243":"code","79eb83c5":"code","195ea37f":"code","cff5c042":"code","caa58ded":"code","682edb56":"code","6ff428cf":"code","3c8804d3":"code","d7e7a8f0":"code","fa050d9b":"code","d5438d80":"code","c3d67329":"code","9b7991fa":"code","36b862b5":"code","4bbcdbfb":"code","ed4a24d5":"code","af1988ba":"code","8100ba65":"code","0dc2364b":"code","c210d80c":"code","978e9da5":"code","8f97a201":"code","25afac35":"code","9d9d21f2":"code","dc17df0a":"code","5dc462f0":"code","5ccbc848":"code","326f8415":"code","cadd004d":"code","feacfde1":"code","df51f248":"code","97102902":"code","48e6905c":"code","0bc18ee3":"code","6e862034":"code","55d4a8c5":"code","79c6ca7b":"code","029286e8":"code","c24743b7":"code","2f9baf99":"code","f4a9add2":"code","9f20be8f":"code","c08da57f":"code","b98d011e":"code","3e762884":"markdown","f86d1558":"markdown","287a9774":"markdown","87e353d6":"markdown","92cfd752":"markdown","cfcd42f1":"markdown","49416d45":"markdown","4afd9d8f":"markdown","d400743e":"markdown","2ed69ebd":"markdown","e1e951c1":"markdown","74358452":"markdown","a28fc276":"markdown","61803dcb":"markdown","871fbd6e":"markdown","c565c3f9":"markdown","9d508e92":"markdown","35d78c07":"markdown","002297d8":"markdown","4384cf02":"markdown","aa511cfa":"markdown","bec53893":"markdown","cb9e435d":"markdown","5778510d":"markdown","1990031f":"markdown","04f015d7":"markdown","546523ec":"markdown","7fe3f56d":"markdown","617c95ba":"markdown","fc243cfd":"markdown","e7789fb0":"markdown","03cb59c0":"markdown","4e6deb6d":"markdown","8d0ba5be":"markdown","2a839ebd":"markdown","3384e390":"markdown","afde008d":"markdown","a5809c9f":"markdown","2189dd5c":"markdown","5f75c49f":"markdown","43d5677d":"markdown","01d96401":"markdown","f6c0f9d0":"markdown","cf40e34c":"markdown","79122021":"markdown","ce5c0e4a":"markdown","1e9be718":"markdown"},"source":{"1a1c54d1":"from warnings import filterwarnings\nfilterwarnings(\"ignore\")","b5196319":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom matplotlib.colors import ListedColormap\nimport seaborn as sns\nimport statsmodels.api as sm\nimport statsmodels.formula.api as smf\nfrom sklearn.model_selection import train_test_split,cross_val_score,cross_val_predict,ShuffleSplit,GridSearchCV\nfrom sklearn.decomposition import PCA\nfrom sklearn import preprocessing\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import StandardScaler,scale, RobustScaler\nfrom sklearn.metrics import confusion_matrix,accuracy_score, roc_auc_score,roc_curve, classification_report,mean_squared_error,f1_score,recall_score,precision_score\nfrom sklearn.ensemble import RandomForestClassifier,BaseEnsemble,GradientBoostingClassifier\nimport time","e7b0b55a":"pd.set_option('display.max_rows', 1000)\npd.set_option('display.max_columns', 1000)\npd.set_option('display.width', 1000)","16ff6243":"df = pd.read_csv(\"..\/input\/pima-indians-diabetes-database\/diabetes.csv\")\ndf.head()","79eb83c5":"df.shape","195ea37f":"df.describe()","cff5c042":"X = df.drop(\"Outcome\",axis=1)\ny= df[\"Outcome\"] #We will predict Outcome(diabetes) ","caa58ded":"X_train = X.iloc[:600]\nX_test = X.iloc[600:]\ny_train = y[:600]\ny_test = y[600:]\n\nprint(\"X_train Shape: \",X_train.shape)\nprint(\"X_test Shape: \",X_test.shape)\nprint(\"y_train Shape: \",y_train.shape)\nprint(\"y_test Shape: \",y_test.shape)","682edb56":"random_forest = RandomForestClassifier().fit(X_train,y_train)","6ff428cf":"random_forest","3c8804d3":"random_forest.get_params()","d7e7a8f0":"feature_importances =pd.DataFrame({\"Features\":X_train.columns,\n              \"Feature Imporances\": random_forest.feature_importances_}).sort_values(by=\"Feature Imporances\")","fa050d9b":"feature_importances.head()","d5438d80":"plt.figure(figsize=(15,7),dpi=200)\nsns.barplot(data=feature_importances,x=\"Features\",y=\"Feature Imporances\")\nplt.title(\"Feature Imporances\")\nplt.xticks(rotation=90)\nplt.show()","c3d67329":"feature_importances[\"Feature Imporances\"].cumsum()","9b7991fa":"random_forest","36b862b5":"y_pred = random_forest.predict(X_test)","4bbcdbfb":"cm = confusion_matrix(y_test,y_pred)","ed4a24d5":"cm","af1988ba":"print(\"Our Accuracy is: \", (cm[0][0]+cm[1][1])\/(cm[0][0]+cm[1][1]+cm[0][1]+cm[1][0]))","8100ba65":"accuracy_score(y_test,y_pred)","0dc2364b":"recall_score(y_test,y_pred)","c210d80c":"precision_score(y_test,y_pred)","978e9da5":"f1_score(y_test,y_pred)","8f97a201":"print(classification_report(y_test,y_pred))","25afac35":"random_forest","9d9d21f2":"accuracies= cross_val_score(estimator=random_forest,\n                            X=X_train,y=y_train,\n                            cv=10)\nprint(\"Average Accuracy: {:.2f} %\".format(accuracies.mean()*100))\nprint(\"Standart Deviation of Accuracies: {:.2f} %\".format(accuracies.std()*100))","dc17df0a":"random_forest.predict(X_test)[:10]","5dc462f0":"random_forests_params = {\"max_depth\": list(range(1,10)),\n                         \"max_features\":[2,5,7, 8],\n                         \"n_estimators\":[300,500,1000,1700,2000],\n                         \"criterion\":[\"gini\",\"entropy\"]}","5ccbc848":"random_forest_classifier = RandomForestClassifier()\nrandom_forest_cv = GridSearchCV(random_forest_classifier,random_forests_params,cv=9,n_jobs=-1,verbose=2)","326f8415":"start_time = time.time()\n\nrandom_forest_cv.fit(X_train,y_train)\n\nelapsed_time = time.time() - start_time\n\nprint(f\"Elapsed time for Random Forests Classifier cross validation: \"\n      f\"{elapsed_time:.3f} seconds\")","cadd004d":"#best score\nrandom_forest_cv.best_score_","feacfde1":"#best parameters\nrandom_forest_cv.best_params_","df51f248":"random_forest_tuned = RandomForestClassifier(criterion=\"gini\",max_depth=5,max_features=2,\n                                             n_estimators=500).fit(X_train,y_train)","97102902":"random_forest_tuned","48e6905c":"y_pred = random_forest_tuned.predict(X_test)","0bc18ee3":"cm = confusion_matrix(y_test,y_pred)","6e862034":"cm","55d4a8c5":"print(\"Our Accuracy is: \", (cm[0][0]+cm[1][1])\/(cm[0][0]+cm[1][1]+cm[0][1]+cm[1][0]))","79c6ca7b":"accuracy_score(y_test,y_pred)","029286e8":"recall_score(y_test,y_pred)","c24743b7":"precision_score(y_test,y_pred)","2f9baf99":"f1_score(y_test,y_pred)","f4a9add2":"print(classification_report(y_test,y_pred))","9f20be8f":"Importances = pd.DataFrame({\"Importance\":random_forest_tuned.feature_importances_*100},index=X_train.columns)","c08da57f":"Importances.head()","b98d011e":"Importances.sort_values(by=\"Importance\",axis=0,ascending=True).plot(kind=\"barh\",color=\"b\")\nplt.xlabel(\"Feature Importances\")\nplt.ylabel(\"Features\")\nplt.title(\"Feature Importances\");","3e762884":"#### F1 - Score","f86d1558":"Let's look at the importances of features.","287a9774":"For a real world example, we will work with **Pima Indians Diabetes** dataset by UCI Machine Learning as before.\n\nIt can be downloaded [here](https:\/\/www.kaggle.com\/uciml\/pima-indians-diabetes-database).\n\nThis dataset is originally from the National Institute of Diabetes and Digestive and Kidney Diseases. The objective of the dataset is to diagnostically predict whether or not a patient has diabetes, based on certain diagnostic measurements included in the dataset. Several constraints were placed on the selection of these instances from a larger database. In particular, all patients here are females at least 21 years old of Pima Indian heritage.\n\nWe will try to predict whether the patient has diabetes or not.","87e353d6":"Because we are doing a classification case, we will create a **confusion matrix** in order to evaluate out model.","92cfd752":"### Evaluation Metrics","cfcd42f1":"### Prediction","49416d45":"#### Confusion Matrix","4afd9d8f":"- **true positive**: These are cases in which we predicted positive, and they are actually positive.\n- **false positive (Type 1 Error)**: We predicted postive, but they are actually negative. (Also known as a \"Type 1 error.\")\n- **true negative**: We predicted negative, and they are actually negative.\n- **false negative (Type 2 Error)**: We predicted negative, but they are actually postive. (Also known as a \"Type 2 error.\")","d400743e":"Some use-cases:\n\n- Mail classification (spam or not)\n\n- Diagnosis of the sicknesses\n\n- Customer buying prediction (if customer will buy or not)","2ed69ebd":"Recall gives us the answer of this question :\n\n**What proportion of actual positives was identified correctly?**\n\nIt is defined as follows: TP \/ (TP+FN)","e1e951c1":"**Notation**: TP = True Positives, TN = True Negatives, FP = False Positives, and FN = False Negatives.","74358452":"Now we will tune our model with GridSearch. We will tune *criterion*,max_features,*n_estimators* and *max_depth* parameters.","a28fc276":"#### ROC Curve (Receiver Operating Characteristic Curve)","61803dcb":"### Theory","871fbd6e":"Precision gives us the answer of this question : \n\n**What proportion of positive identifications was actually correct?**\n\nIt is defined as follows: TP \/ (TP+FP)","c565c3f9":"#### Precision","9d508e92":"#### Accuracy","35d78c07":"An ROC curve (receiver operating characteristic curve) is a graph showing the performance of a classification model at all classification thresholds. \n\nAn ROC curve plots TP rates vs. FP rares at different classification thresholds. Lowering the classification threshold classifies more items as positive, thus increasing both False Positives and True Positives. The following figure shows a typical ROC curve.\n\n![image.png](attachment:image.png)","002297d8":"#### Recall","4384cf02":"Random Forest is also an example of ensemble learning, in which we combine multiple machine learning algorithms to obtain better predictive performance.\n\nThe random forest algorithm is an extension of the bagging method as it utilizes both bagging and feature randomness to create an uncorrelated forest of decision trees. Feature randomness, also known as feature bagging or \u201cthe random subspace method\u201d, generates a random subset of features, which ensures low correlation among decision trees. This is a key difference between decision trees and random forests. While decision trees consider all the possible feature splits, random forests only select a subset of those features.\n\nLet's try to understand with an example. For example, I want to watch a movie today and I am not sure what to watch. After calling one of my best friends, she recommend a movie to me according to my old preferences that she know. At this point, my old preferences are training set for her. It's a classical decision tree. But if I would get recommendations from my 20 different friends and select most voted movie, that would be **Random Forests**.\n\nRandom forest algorithms have three main hyperparameters, which need to be set before training. These include node size, the number of trees, and the number of features sampled. From there, the random forest classifier can be used to solve for regression or classification problems.","aa511cfa":"- **The Elements of  Statistical Learning** - Trevor Hastie,  Robert Tibshirani, Jerome Friedman -  Data Mining, Inference, and Prediction (Springer Series in Statistics) \n\n- [**What is a Confusion Matrix in Machine Learning?**](https:\/\/machinelearningmastery.com\/confusion-matrix-machine-learning\/)\n\n- [**Classification: Precision and Recall**](https:\/\/developers.google.com\/machine-learning\/crash-course\/classification\/precision-and-recall)\n\n- [**Classification: ROC Curve and AUC**](https:\/\/developers.google.com\/machine-learning\/crash-course\/classification\/roc-and-auc)\n\n- [**AUC-ROC Curve in Machine Learning Clearly Explained**](https:\/\/www.analyticsvidhya.com\/blog\/2020\/06\/auc-roc-curve-machine-learning\/)","bec53893":"### Model Tuning","cb9e435d":"Yet another great quality of Random Forests is that they make it easy to measure the relative *importance of each feature*. Scikit-Learn measures a feature\u2019s importance by looking at how much the tree nodes that use that feature reduce impurity on average (across all trees in the forest). More precisely, it is a weighted average, where each node\u2019s weight is equal to the number of training samples that are associated with it. ","5778510d":"## Classification and Evaluation Metrics","1990031f":"As we say in the theory section, we can see importances of features in Random Forests.","04f015d7":"## Resources","546523ec":"## Importing Libraries","7fe3f56d":"Because we are doing a classification case, we will create a **confusion matrix** in order to evaluate our model.  Confusion matrix is a summary of prediction results on a classification problem. The number of correct and incorrect predictions are summarized with count values and broken down by each class. This is the key to the confusion matrix.","617c95ba":"### Model","fc243cfd":"### Classification","e7789fb0":"Total of ratios should be equal to 1.","03cb59c0":"## Ensemble Learning - Random Forests Classification","4e6deb6d":"![image.png](attachment:image.png)\n\nPhoto is cited by [here](https:\/\/www.google.com\/url?sa=i&url=https%3A%2F%2Ftowardsdatascience.com%2Fconfusion-matrix-for-your-multi-class-machine-learning-model-ff9aa3bf7826&psig=AOvVaw29atdmY9s4wmI-rc0qQZZb&ust=1628435461495000&source=images&cd=vfe&ved=0CAsQjRxqFwoTCKj1g_2Yn_ICFQAAAAAdAAAAABAD).","8d0ba5be":"In order to see all rows and columns, we will increase max display numbers of dataframe.","2a839ebd":"Accuracy is one metric for evaluating classification models. Informally, accuracy is **the fraction of predictions our model got right**.\n\nFormally, accuracy has the following definition: All correct predictions \/ all predictions\n\nFor binary classification, accuracy can also be calculated in terms of positives and negatives as follow: (TP+TN) \/ (TP+FP+FN+TN)","3384e390":"The Area Under the Curve (AUC) is the measure of the ability of a classifier to distinguish between classes and is used as a summary of the ROC curve. The higher the AUC, the better the performance of the model at distinguishing between the positive and negative classes.\n\nThis is an example of AUC:\n\n![image.png](attachment:image.png)","afde008d":"#### AUC (Area under Curve)","a5809c9f":"Now we're going to split our dataset to train and test set. We will choose almost 20% of dataset as test size.","2189dd5c":"Classification is the process of finding or discovering a model or function which helps in separating the data into multiple categorical classes i.e. discrete values. In classification, data is categorized under different labels according to some parameters given in input and then the labels are predicted for the data. \nThe derived mapping function could be demonstrated in the form of \u201cIF-THEN\u201d rules. The classification process deal with the problems where the data can be divided into binary or multiple discrete labels. ","5f75c49f":"![image-2.png](attachment:image-2.png)\n\nPhoto is cited by [here](https:\/\/www.google.com\/url?sa=i&url=https%3A%2F%2Fwww.analyticsvidhya.com%2Fblog%2F2020%2F05%2Fdecision-tree-vs-random-forest-algorithm%2F&psig=AOvVaw2jevf2JFgvEKCBieh5yaHX&ust=1627289101408000&source=images&cd=vfe&ved=0CAsQjRxqFwoTCICY4bvq_fECFQAAAAAdAAAAABAD)","43d5677d":"To compute the points in an ROC curve, we could evaluate a classification model many times with different classification thresholds, but this would be inefficient. Fortunately, there's an efficient, sorting-based algorithm that can provide this information for us, called AUC.","01d96401":"All hyperparameters can be found [here](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.ensemble.RandomForestClassifier.html).","f6c0f9d0":"**Created by Berkay Alan**\n\n**Classification | Random Forest**\n\n**17 January 2022**\n\n**For more Tutorial:** https:\/\/github.com\/berkayalan","cf40e34c":"Another advantage of sampling over the features is that it makes the decision making process more robust to missing data: observations (from the training dataset or not) with missing data can still be regressed or classified based on the trees that take into account only features where data are not missing. Thus, random forest algorithm combines the concepts of bagging and random feature subspace selection to create more robust models.","79122021":"The F1 score can be interpreted as a harmonic mean of the precision and recall, where an F1 score reaches its best value at 1 and worst score at 0. The relative contribution of precision and recall to the F1 score are equal. \n\nThe formula for the F1 score is: 2 * (precision * recall) \/ (precision + recall)","ce5c0e4a":"If you want to see other algorithms such as:\n\n- Logistic Regression (Theory - Model- Tuning)\n\n- Decision Tree Classification (Theory - Model- Tuning)\n\n- Support Vector Machines(SVC) - Linear Kernel (Theory - Model- Tuning)\n\n- Support Vector Machines(SVC) - Radial Basis Kernel (Theory - Model- Tuning)\n\n- Naive Bayes Classification (Theory - Model)\n\n- K - Nearest Neighbors(KNN) (Theory - Model- Tuning)\n\n- XGBoost(Extreme Gradient Boosting) Classification (Theory - Model- Tuning)\n\nPlease visit my [Classification tutorial](https:\/\/github.com\/berkayalan\/Data-Science-Tutorials\/blob\/master\/Classification\/Classification.ipynb)","1e9be718":"Now we will try to tune our model by using **K-Fold Cross Validation**."}}