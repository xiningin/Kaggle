{"cell_type":{"400afbd1":"code","11165d4a":"code","9d209999":"code","69f683b2":"code","2a767d32":"code","f13856be":"code","05737d5d":"code","146344c7":"code","b547fb6e":"code","3a7c4da2":"code","6210b83f":"code","bd7d51e4":"code","69dda06f":"code","78a0ad84":"code","adfbc3a8":"code","5f30a2fe":"code","a65e621c":"code","44db0940":"code","82762408":"code","614dbccc":"code","dd39a0ae":"code","27eaa29c":"code","56bf0599":"code","37fbe3b8":"code","3e188c83":"code","a8a49f56":"code","1b156132":"code","699e542d":"code","a9ab6189":"code","c78439cd":"code","101d6441":"code","ed720c0d":"code","36ee74ca":"code","da289476":"code","f6404c13":"code","127432b8":"code","947dc8b2":"code","f37fa4a5":"code","53b99db8":"code","0872d8ee":"code","6aac1485":"code","e15d5df3":"code","4f5b175a":"code","b0359c62":"code","847327fb":"code","1d5dea72":"code","ef73448c":"code","445a8e7f":"code","cf00007f":"code","c2f6c91b":"code","2f2f7689":"code","3e7b5b22":"code","c828a279":"code","0858d29b":"code","fc0f17ab":"code","6e6be1ac":"code","afce95d9":"code","658aa779":"code","378af970":"code","51acfbc6":"markdown","478b8e9b":"markdown","e1740844":"markdown","d4fae770":"markdown","bb9ffa54":"markdown","ee973d58":"markdown","196a336e":"markdown","485ba3e2":"markdown","9d120203":"markdown","974ba9cc":"markdown","842937b5":"markdown","e2f1d7e0":"markdown","f7ef57cc":"markdown","3c7539eb":"markdown","f6f43a6b":"markdown","9a32f7eb":"markdown","ee6b4f18":"markdown","34c7b3b1":"markdown","ede3d389":"markdown","83f8766c":"markdown","af022561":"markdown","bd620958":"markdown","05da06bc":"markdown","9bf7718a":"markdown","fcbf87be":"markdown","ac5eee7a":"markdown","f4982ea2":"markdown"},"source":{"400afbd1":"import pandas as pd\nimport numpy as np\n\n# DRAGONS\nimport xgboost as xgb\nimport lightgbm as lgb\nimport catboost as cat\n\n# plots\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\n# pandas \/ plt options\npd.options.display.max_columns = 999\nplt.rcParams['figure.figsize'] = (14, 7)\nfont = {'family' : 'verdana',\n        'weight' : 'bold',\n        'size'   : 14}\nplt.rc('font', **font)\n\n# remove warnings\nimport warnings\nwarnings.simplefilter(\"ignore\")\n\n# garbage collector\nimport gc\ngc.enable()","11165d4a":"train = pd.read_csv('..\/input\/create-extracted-json-fields-dataset\/extracted_fields_train.gz', dtype={'date': str, 'fullVisitorId': str, 'sessionId':str, 'visitId': np.int64})\ntest = pd.read_csv('..\/input\/create-extracted-json-fields-dataset\/extracted_fields_test.gz', dtype={'date': str, 'fullVisitorId': str, 'sessionId':str, 'visitId': np.int64})\ntrain.shape, test.shape","9d209999":"train.head()","69f683b2":"train.columns","2a767d32":"# Getting data from leak\ntrain_store_1 = pd.read_csv('..\/input\/exported-google-analytics-data\/Train_external_data.csv', low_memory=False, skiprows=6, dtype={\"Client Id\":'str'})\ntrain_store_2 = pd.read_csv('..\/input\/exported-google-analytics-data\/Train_external_data_2.csv', low_memory=False, skiprows=6, dtype={\"Client Id\":'str'})\ntest_store_1 = pd.read_csv('..\/input\/exported-google-analytics-data\/Test_external_data.csv', low_memory=False, skiprows=6, dtype={\"Client Id\":'str'})\ntest_store_2 = pd.read_csv('..\/input\/exported-google-analytics-data\/Test_external_data_2.csv', low_memory=False, skiprows=6, dtype={\"Client Id\":'str'})","f13856be":"# Getting VisitId from Google Analytics...\nfor df in [train_store_1, train_store_2, test_store_1, test_store_2]:\n    df[\"visitId\"] = df[\"Client Id\"].apply(lambda x: x.split('.', 1)[1]).astype(np.int64)","05737d5d":"# Merge with train\/test data\ntrain = train.merge(pd.concat([train_store_1, train_store_2], sort=False), how=\"left\", on=\"visitId\")\ntest = test.merge(pd.concat([test_store_1, test_store_2], sort=False), how=\"left\", on=\"visitId\")\n\n# Drop Client Id\nfor df in [train, test]:\n    df.drop(\"Client Id\", 1, inplace=True)","146344c7":"train.columns","b547fb6e":"# Cleaning Revenue\nfor df in [train, test]:\n    df[\"Revenue\"].fillna('$', inplace=True)\n    df[\"Revenue\"] = df[\"Revenue\"].apply(lambda x: x.replace('$', '').replace(',', ''))\n    df[\"Revenue\"] = pd.to_numeric(df[\"Revenue\"], errors=\"coerce\")\n    df[\"Revenue\"].fillna(0.0, inplace=True)","3a7c4da2":"for df in [train_store_1, train_store_2, test_store_1, test_store_2]:\n    del df\ngc.collect()","6210b83f":"target_sums = train.groupby(\"fullVisitorId\")[\"totals.transactionRevenue\"].sum().reset_index()","bd7d51e4":"plt.scatter(range(target_sums.shape[0]), np.sort(np.log1p(target_sums[\"totals.transactionRevenue\"].values)))\nplt.xlabel('index')\nplt.ylabel('TransactionRevenue')\nplt.show()","69dda06f":"train.date = pd.to_datetime(train.date, format=\"%Y%m%d\")\ntest.date = pd.to_datetime(test.date, format=\"%Y%m%d\")\ntrain.date.value_counts().sort_index().plot(label=\"train\")\ntest.date.value_counts().sort_index().plot(label=\"test\")\nplt.legend()","78a0ad84":"def drawBars(columnname):\n    sns.barplot(x=\"count\", y=\"index\", hue=\"dataset\",\n        data=pd.melt(pd.concat([train[columnname].value_counts().rename(\"train\"), \n                       test[columnname].value_counts().rename(\"test\")], axis=1, sort=\"False\").reset_index(),\n            id_vars=\"index\", var_name=\"dataset\", value_name=\"count\"))\n\ndrawBars(\"channelGrouping\")","adfbc3a8":"drawBars(\"geoNetwork.continent\")","5f30a2fe":"ids_train = set(train.fullVisitorId.unique())\nids_test = set(test.fullVisitorId.unique())\nprint(\"Unique visitor ids in train:\", len(ids_train))\nprint(\"Unique visitor ids in test:\", len(ids_test))\nprint(\"Common visitors in train and test:\", len(ids_train & ids_test))","a65e621c":"problem = train[train.sessionId.map(train.sessionId.value_counts() == 2)].sort_values([\"sessionId\", 'visitStartTime'])\nproblem.head(10)","44db0940":"(train.visitStartTime == train.visitId).value_counts()","82762408":"train.loc[pd.to_datetime(train.visitStartTime, unit='s') == \"2017-04-25 18:49:35\"].head(8)","614dbccc":"print(\"Train: \", np.bincount(train.visitId.value_counts()))","dd39a0ae":"print(\"test: \", np.bincount(test.visitId.value_counts()))","27eaa29c":"train.visitStartTime = pd.to_datetime(train.visitStartTime, unit='s')\ntest.visitStartTime = pd.to_datetime(test.visitStartTime, unit='s')\ntrain[\"date\"] = train.visitStartTime\ntest[\"date\"] = test.visitStartTime","56bf0599":"train.set_index(\"visitStartTime\", inplace=True)\ntest.set_index(\"visitStartTime\", inplace=True)\ntrain.sort_index(inplace=True)\ntest.sort_index(inplace=True)","37fbe3b8":"def clearRare(columnname, limit = 1000):\n    # you may search for rare categories in train, train&test, or just test\n    #vc = pd.concat([train[columnname], test[columnname]], sort=False).value_counts()\n    vc = test[columnname].value_counts()\n    \n    common = vc > limit\n    common = set(common.index[common].values)\n    print(\"Set\", sum(vc <= limit), columnname, \"categories to 'other';\", end=\" \")\n    \n    train.loc[train[columnname].map(lambda x: x not in common), columnname] = 'other'\n    test.loc[test[columnname].map(lambda x: x not in common), columnname] = 'other'\n    print(\"now there are\", train[columnname].nunique(), \"categories in train\")","3e188c83":"train.fillna(0, inplace=True)\ntest.fillna(0, inplace=True)","a8a49f56":"clearRare(\"device.browser\")\nclearRare(\"device.operatingSystem\")\nclearRare(\"geoNetwork.country\")\nclearRare(\"geoNetwork.city\")\nclearRare(\"geoNetwork.metro\")\nclearRare(\"geoNetwork.networkDomain\")\nclearRare(\"geoNetwork.region\")\nclearRare(\"geoNetwork.subContinent\")\nclearRare(\"trafficSource.adContent\")\nclearRare(\"trafficSource.campaign\")\nclearRare(\"trafficSource.keyword\")\nclearRare(\"trafficSource.medium\")\nclearRare(\"trafficSource.referralPath\")\nclearRare(\"trafficSource.source\")","1b156132":"# Clearing leaked data:\nfor df in [train, test]:\n    df[\"Avg. Session Duration\"][df[\"Avg. Session Duration\"] == 0] = \"00:00:00\"\n    df[\"Avg. Session Duration\"] = df[\"Avg. Session Duration\"].str.split(':').apply(lambda x: int(x[0]) * 60 + int(x[1]))\n    df[\"Bounce Rate\"] = df[\"Bounce Rate\"].astype(str).apply(lambda x: x.replace('%', '')).astype(float)\n    df[\"Goal Conversion Rate\"] = df[\"Goal Conversion Rate\"].astype(str).apply(lambda x: x.replace('%', '')).astype(float)","699e542d":"for df in [train, test]:\n    # remember these features were equal, but not always? May be it means something...\n    df[\"id_incoherence\"] = pd.to_datetime(df.visitId, unit='s') != df.date\n    # remember visitId dublicates?\n    df[\"visitId_dublicates\"] = df.visitId.map(df.visitId.value_counts())\n    # remember session dublicates?\n    df[\"session_dublicates\"] = df.sessionId.map(df.sessionId.value_counts())","a9ab6189":"for df in [train, test]:\n    df['weekday'] = df['date'].dt.dayofweek.astype(object)\n    df['time'] = df['date'].dt.second + df['date'].dt.minute*60 + df['date'].dt.hour*3600\n    #df['month'] = df['date'].dt.month   # it must not be included in features during learning!\n    df['day'] = df['date'].dt.date       # it must not be included in features during learning!","c78439cd":"df = pd.concat([train, test])\ndf.sort_values(['fullVisitorId', 'date'], ascending=True, inplace=True)\ndf['prev_session'] = (df['date'] - df[['fullVisitorId', 'date']].groupby('fullVisitorId')['date'].shift(1)).astype(np.int64) \/\/ 1e9 \/\/ 60 \/\/ 60\ndf['next_session'] = (df['date'] - df[['fullVisitorId', 'date']].groupby('fullVisitorId')['date'].shift(-1)).astype(np.int64) \/\/ 1e9 \/\/ 60 \/\/ 60\ndf.sort_index(inplace=True)\n\ntrain = df[:len(train)]\ntest = df[len(train):]","101d6441":"for df in [train, test]:\n    df['source.country'] = df['trafficSource.source'] + '_' + df['geoNetwork.country']\n    df['campaign.medium'] = df['trafficSource.campaign'] + '_' + df['trafficSource.medium']\n    df['browser.category'] = df['device.browser'] + '_' + df['device.deviceCategory']\n    df['browser.os'] = df['device.browser'] + '_' + df['device.operatingSystem']","ed720c0d":"for df in [train, test]:\n    df['device_deviceCategory_channelGrouping'] = df['device.deviceCategory'] + \"_\" + df['channelGrouping']\n    df['channelGrouping_browser'] = df['device.browser'] + \"_\" + df['channelGrouping']\n    df['channelGrouping_OS'] = df['device.operatingSystem'] + \"_\" + df['channelGrouping']\n    \n    for i in ['geoNetwork.city', 'geoNetwork.continent', 'geoNetwork.country','geoNetwork.metro', 'geoNetwork.networkDomain', 'geoNetwork.region','geoNetwork.subContinent']:\n        for j in ['device.browser','device.deviceCategory', 'device.operatingSystem', 'trafficSource.source']:\n            df[i + \"_\" + j] = df[i] + \"_\" + df[j]\n    \n    df['content.source'] = df['trafficSource.adContent'].astype(str) + \"_\" + df['source.country']\n    df['medium.source'] = df['trafficSource.medium'] + \"_\" + df['source.country']","36ee74ca":"for feature in [\"totals.hits\", \"totals.pageviews\"]:\n    info = pd.concat([train, test], sort=False).groupby(\"fullVisitorId\")[feature].mean()\n    train[\"usermean_\" + feature] = train.fullVisitorId.map(info)\n    test[\"usermean_\" + feature] = test.fullVisitorId.map(info)\n    \nfor feature in [\"visitNumber\"]:\n    info = pd.concat([train, test], sort=False).groupby(\"fullVisitorId\")[feature].max()\n    train[\"usermax_\" + feature] = train.fullVisitorId.map(info)\n    test[\"usermax_\" + feature] = test.fullVisitorId.map(info)","da289476":"excluded = ['date', 'fullVisitorId', 'sessionId', 'totals.transactionRevenue', 'visitId', 'visitStartTime', \n            'month', 'day', 'help']\n\ncat_cols = [f for f in train.columns if (train[f].dtype == 'object' and f not in excluded)]\nreal_cols = [f for f in train.columns if (not f in cat_cols and f not in excluded)]","f6404c13":"train[cat_cols].nunique()","127432b8":"from sklearn.preprocessing import LabelEncoder\nfor col in cat_cols:\n    lbl = LabelEncoder()\n    lbl.fit(list(train[col].values.astype('str')) + list(test[col].values.astype('str')))\n    train[col] = lbl.transform(list(train[col].values.astype('str')))\n    test[col] = lbl.transform(list(test[col].values.astype('str')))","947dc8b2":"for col in real_cols:\n    train[col] = train[col].astype(float)\n    test[col] = test[col].astype(float)","f37fa4a5":"train[real_cols + cat_cols].head()","53b99db8":"for to_del in [\"date\", \"sessionId\", \"visitId\", \"day\"]:\n    del train[to_del]\n    del test[to_del]","0872d8ee":"excluded = ['date', 'fullVisitorId', 'sessionId', 'totals.transactionRevenue', 'visitId', 'visitStartTime', \"month\", \"help\"]\n\ncat_cols = [f for f in train.columns if (train[f].dtype == 'int64' and f not in excluded)]\nreal_cols = [f for f in train.columns if (not f in cat_cols and f not in excluded)]","6aac1485":"from sklearn.metrics import mean_squared_error\ndef score(data, y):\n    validation_res = pd.DataFrame(\n    {\"fullVisitorId\": data[\"fullVisitorId\"].values,\n     \"transactionRevenue\": data[\"totals.transactionRevenue\"].values,\n     \"predictedRevenue\": np.expm1(y)})\n\n    validation_res = validation_res.groupby(\"fullVisitorId\")[\"transactionRevenue\", \"predictedRevenue\"].sum().reset_index()\n    return np.sqrt(mean_squared_error(np.log1p(validation_res[\"transactionRevenue\"].values), \n                                     np.log1p(validation_res[\"predictedRevenue\"].values)))","e15d5df3":"from sklearn.model_selection import GroupKFold\n\nclass KFoldValidation():\n    def __init__(self, data, n_splits=5):\n        unique_vis = np.array(sorted(data['fullVisitorId'].astype(str).unique()))\n        folds = GroupKFold(n_splits)\n        ids = np.arange(data.shape[0])\n        \n        self.fold_ids = []\n        for trn_vis, val_vis in folds.split(X=unique_vis, y=unique_vis, groups=unique_vis):\n            self.fold_ids.append([\n                    ids[data['fullVisitorId'].astype(str).isin(unique_vis[trn_vis])],\n                    ids[data['fullVisitorId'].astype(str).isin(unique_vis[val_vis])]\n                ])\n            \n    def validate(self, train, test, features, model, name=\"\", prepare_stacking=False, \n                 fit_params={\"early_stopping_rounds\": 50, \"verbose\": 100, \"eval_metric\": \"rmse\"}):\n        model.FI = pd.DataFrame(index=features)\n        full_score = 0\n        \n        if prepare_stacking:\n            test[name] = 0\n            train[name] = np.NaN\n        \n        for fold_id, (trn, val) in enumerate(self.fold_ids):\n            devel = train[features].iloc[trn]\n            y_devel = np.log1p(train[\"totals.transactionRevenue\"].iloc[trn])\n            valid = train[features].iloc[val]\n            y_valid = np.log1p(train[\"totals.transactionRevenue\"].iloc[val])\n                       \n            print(\"Fold \", fold_id, \":\")\n            model.fit(devel, y_devel, eval_set=[(valid, y_valid)], **fit_params)\n            \n            if len(model.feature_importances_) == len(features):  # some bugs in catboost?\n                model.FI['fold' + str(fold_id)] = model.feature_importances_ \/ model.feature_importances_.sum()\n\n            predictions = model.predict(valid)\n            predictions[predictions < 0] = 0\n            print(\"Fold \", fold_id, \" error: \", mean_squared_error(y_valid, predictions)**0.5)\n            \n            fold_score = score(train.iloc[val], predictions)\n            full_score += fold_score \/ len(self.fold_ids)\n            print(\"Fold \", fold_id, \" score: \", fold_score)\n            \n            if prepare_stacking:\n                train[name].iloc[val] = predictions\n                \n                test_predictions = model.predict(test[features])\n                test_predictions[test_predictions < 0] = 0\n                test[name] += test_predictions \/ len(self.fold_ids)\n                \n        print(\"Final score: \", full_score)\n        return full_score","4f5b175a":"Kfolder = KFoldValidation(train)","b0359c62":"lgbmodel = lgb.LGBMRegressor(n_estimators=1000, objective=\"regression\", metric=\"rmse\", num_leaves=31, min_child_samples=100,\n                      learning_rate=0.03, bagging_fraction=0.7, feature_fraction=0.5, bagging_frequency=5, \n                      bagging_seed=2019, subsample=.9, colsample_bytree=.9, use_best_model=True)","847327fb":"Kfolder.validate(train, test, real_cols + cat_cols, lgbmodel, \"lgbpred\", prepare_stacking=True)","1d5dea72":"lgbmodel.FI.mean(axis=1).sort_values()[:30].plot(kind=\"barh\")","ef73448c":"def create_user_df(df):\n    agg_data = df[real_cols + cat_cols + ['fullVisitorId']].groupby('fullVisitorId').mean()\n    \n    pred_list = df[['fullVisitorId', 'lgbpred']].groupby('fullVisitorId').apply(lambda visitor_df: list(visitor_df.lgbpred))\\\n        .apply(lambda x: {'pred_'+str(i): pred for i, pred in enumerate(x)})\n    all_predictions = pd.DataFrame(list(pred_list.values), index=agg_data.index)\n    feats = all_predictions.columns\n\n    all_predictions['t_mean'] = all_predictions.mean(axis=1)\n    all_predictions['t_median'] = all_predictions.median(axis=1)   # including t_mean as one of the elements? well, ok\n    all_predictions['t_sum_log'] = all_predictions.sum(axis=1)\n    all_predictions['t_sum_act'] = all_predictions.fillna(0).sum(axis=1)\n    all_predictions['t_nb_sess'] = all_predictions.isnull().sum(axis=1)\n\n    full_data = pd.concat([agg_data, all_predictions], axis=1).astype(float)\n    full_data['fullVisitorId'] = full_data.index\n    del agg_data, all_predictions\n    gc.collect()\n    return full_data","445a8e7f":"user_train = create_user_df(train)\nuser_test = create_user_df(test)","cf00007f":"features = list(user_train.columns)[:-1]  # don't include \"fullVisitorId\"\nuser_train[\"totals.transactionRevenue\"] = train[['fullVisitorId', 'totals.transactionRevenue']].groupby('fullVisitorId').sum()","c2f6c91b":"for f in features:\n    if f not in user_test.columns:\n        user_test[f] = np.nan","2f2f7689":"Kfolder = KFoldValidation(user_train)","3e7b5b22":"lgbmodel = lgb.LGBMRegressor(n_estimators=1000, objective=\"regression\", metric=\"rmse\", num_leaves=31, min_child_samples=100,\n                      learning_rate=0.03, bagging_fraction=0.7, feature_fraction=0.5, bagging_frequency=5, \n                      bagging_seed=2019, subsample=.9, colsample_bytree=.9,\n                            use_best_model=True)","c828a279":"Kfolder.validate(user_train, user_test, features, lgbmodel, name=\"lgbfinal\", prepare_stacking=True)","0858d29b":"xgbmodel = xgb.XGBRegressor(max_depth=22, learning_rate=0.02, n_estimators=1000, \n                                         objective='reg:linear', gamma=1.45, seed=2019, silent=False,\n                                        subsample=0.67, colsample_bytree=0.054, colsample_bylevel=0.50)","fc0f17ab":"Kfolder.validate(user_train, user_test, features, xgbmodel, name=\"xgbfinal\", prepare_stacking=True)","6e6be1ac":"catmodel = cat.CatBoostRegressor(iterations=500, learning_rate=0.2, depth=5, random_seed=2019)","afce95d9":"Kfolder.validate(user_train, user_test, features, catmodel, name=\"catfinal\", prepare_stacking=True,\n                fit_params={\"use_best_model\": True, \"verbose\": 100})","658aa779":"user_train['PredictedLogRevenue'] = 0.4 * user_train[\"lgbfinal\"] + \\\n                                    0.2 * user_train[\"xgbfinal\"] + \\\n                                    0.4 * user_train[\"catfinal\"]\nscore(user_train, user_train.PredictedLogRevenue)","378af970":"user_test['PredictedLogRevenue'] = 0.4 * user_test[\"lgbfinal\"] +  0.4 * user_test[\"catfinal\"] + 0.2 * user_test[\"xgbfinal\"]\nuser_test[['PredictedLogRevenue']].to_csv('leaky submission.csv', index=True)","51acfbc6":"# Preparing validation","478b8e9b":"User-aggregating features:","e1740844":"Comparing categories in train and test:","d4fae770":"Key problem:","bb9ffa54":"Seems to be a serious problem:","ee973d58":"# Features","196a336e":"Function to tell us the score using the metric we actually care about","485ba3e2":"Paired categories from \"teach-lightgbm-to-sum-predictions\" kernel","9d120203":"Cute function to validate and prepare stacking","974ba9cc":"Clearing rare categories and setting 0 to NaNs:","842937b5":"# Preprocessing","e2f1d7e0":"Based on strange things in dataset:","f7ef57cc":"Setting time as index and saving time as feature (for FE purposes only)","3c7539eb":"# Meta-models","f6f43a6b":"# Ensembling dragons","9a32f7eb":"# Encoding features","ee6b4f18":"VisitStartTime seems to be same thing as visitId... yet not always!","34c7b3b1":"Suspicious simultaneous visitors with same visitorId and same visitStartTime:","ede3d389":"# Loading data","83f8766c":"Weird \"double\" sessions:","af022561":"# Looking around","bd620958":"Some pictures to have in mind: target distribution","05da06bc":"![](data:image\/png;base64,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)","9bf7718a":"Basic time features:","fcbf87be":"Looking to future features (from https:\/\/www.kaggle.com\/ashishpatel26\/future-is-here):","ac5eee7a":"Make one user one object:\n* all features are averaged\n* we hope, that categorical features do not change for one user (that's not true :\/ )\n* categoricals labels are averaged (!!!) and are treated as numerical features (o_O)\n* predictions are averaged in multiple ways...","f4982ea2":"# User-level"}}