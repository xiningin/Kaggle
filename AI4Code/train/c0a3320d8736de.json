{"cell_type":{"1f544ff3":"code","eca59209":"code","2ae39a39":"code","faec65b7":"code","e4a0293a":"code","04582909":"code","e1f38b9f":"code","c5acff6b":"code","5fbff71d":"code","0862e16f":"code","feba43ea":"code","f36d506d":"code","8e07321c":"code","62433c71":"code","bff2fc2a":"code","90e9328c":"code","6426eb6a":"code","4c65f8b2":"code","58714ae3":"code","54653a84":"code","9f2ffb77":"code","b051b143":"code","06a69064":"code","46adbc51":"markdown","98c79574":"markdown","5f227c09":"markdown","e40cfa10":"markdown","1d6820f9":"markdown","72b7bbb7":"markdown","ddae8313":"markdown","087cb9b2":"markdown","998cc73e":"markdown","c88857f6":"markdown","df0e462c":"markdown","ed112b1d":"markdown","ef99397c":"markdown","3fd7dc9c":"markdown","cf809504":"markdown","ecc35999":"markdown","bd0befa7":"markdown","34ba36ed":"markdown","ddfb9440":"markdown","cdf976a2":"markdown","fa8b0930":"markdown","5787e144":"markdown","6b4af57c":"markdown","264b0261":"markdown","249c6844":"markdown","5507d363":"markdown","0f99d884":"markdown","8dcc9196":"markdown"},"source":{"1f544ff3":"!pip install deeptables\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport seaborn as sns\nimport matplotlib as plt\nfrom deeptables.models.deeptable import DeepTable, ModelConfig\nfrom tensorflow.keras.utils import plot_model\nimport tensorflow as tf\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","eca59209":"train = pd.read_csv('..\/input\/cat-in-the-dat-ii\/train.csv', index_col='id')\ntest = pd.read_csv('..\/input\/cat-in-the-dat-ii\/test.csv', index_col='id')\ntrain_copy = train.copy()","2ae39a39":"train.head()","faec65b7":"sns.countplot(y = 'target',data = train, palette = 'Set2')","e4a0293a":"train.isnull().sum()","04582909":"test.isnull().sum()","e1f38b9f":"train.isna().sum()*100\/len(train)","c5acff6b":"def heatmap(x, y, size):\n    fig, ax = plt.pyplot.subplots()\n    \n    # Mapping from column names to integer coordinates\n    x_labels = [v for v in sorted(x.unique())]\n    y_labels = [v for v in sorted(y.unique())]\n    x_to_num = {p[1]:p[0] for p in enumerate(x_labels)} \n    y_to_num = {p[1]:p[0] for p in enumerate(y_labels)} \n    \n    size_scale = 500\n    ax.scatter(\n        x=x.map(x_to_num), # Use mapping for x\n        y=y.map(y_to_num), # Use mapping for y\n        s=size * size_scale, # Vector of square sizes, proportional to size parameter\n        marker='s' # Use square as scatterplot marker\n    )\n    \n    # Show column labels on the axes\n    ax.set_xticks([x_to_num[v] for v in x_labels])\n    ax.set_xticklabels(x_labels, rotation=45, horizontalalignment='right')\n    ax.set_yticks([y_to_num[v] for v in y_labels])\n    ax.set_yticklabels(y_labels)\n    \ncorr = train.corr()\ncorr = pd.melt(corr.reset_index(), id_vars='index')\ncorr.columns = ['x', 'y', 'value']\nheatmap(\n    x=corr['x'],\n    y=corr['y'],\n    size=corr['value'].abs()\n)","5fbff71d":"fig, ax = plt.pyplot.subplots(1,5, figsize=(30, 8))\nfor i in range(5): \n    sns.countplot(f'bin_{i}', data= train, ax=ax[i],palette= 'Set2')\n    ax[i].set_ylim([0, 600000])\n    ax[i].set_title(f'bin_{i}', fontsize=15)\nfig.suptitle(\"Binary Feature Distribution (Train Data)\", fontsize=20)\nplt.pyplot.show()","0862e16f":"fig, ax = plt.pyplot.subplots(1,5, figsize=(30, 8))\nfor i in range(5): \n    sns.countplot(f'bin_{i}', data= test, ax=ax[i], alpha=0.7,\n                 order=test[f'bin_{i}'].value_counts().index,palette= 'Set2')\n    ax[i].set_ylim([0, 600000])\n    ax[i].set_title(f'bin_{i}', fontsize=15)\nfig.suptitle(\"Binary Feature Distribution (Test Data)\", fontsize=20)\nplt.pyplot.show()","feba43ea":"fig, ax = plt.pyplot.subplots(1,5, figsize=(30, 8))\nfor i in range(5): \n    sns.countplot(f'bin_{i}', hue='target', data= train, ax=ax[i],palette= 'Set2')\n    ax[i].set_ylim([0, 500000])\n    ax[i].set_title(f'bin_{i}', fontsize=15)\nfig.suptitle(\"Binary Feature Distribution (Train Data)\", fontsize=20)\nplt.pyplot.show()","f36d506d":"num_cols = test.select_dtypes(exclude=['object']).columns\nfig, ax = plt.pyplot.subplots(2,3,figsize=(22,7))\nfor i, col in enumerate(num_cols):\n    plt.pyplot.subplot(2,3,i+1)\n    plt.pyplot.xlabel(col, fontsize=9)\n    sns.kdeplot(train[col].values, bw=0.5,label='Train')\n    sns.kdeplot(test[col].values, bw=0.5,label='Test')\n   \nplt.pyplot.show() ","8e07321c":"target0 = train.loc[train['target'] == 0]\ntarget1 = train.loc[train['target'] == 1]\n\nfig, ax = plt.pyplot.subplots(2,3,figsize=(22,7))\nfor i, col in enumerate(num_cols):\n    plt.pyplot.subplot(2,3,i+1)\n    plt.pyplot.xlabel(col, fontsize=9)\n    sns.kdeplot(target0[col].values, bw=0.5,label='Target: 0')\n    sns.kdeplot(target1[col].values, bw=0.5,label='Target: 1')\n    sns.kdeplot(test[col].values, bw=0.5,label='Test')\n    \nplt.pyplot.show() ","62433c71":"plt.pyplot.figure(figsize=(17, 35)) \nnom_cols = [f'nom_{i}' for i in range(5)]\nfig, ax = plt.pyplot.subplots(2,3,figsize=(22,10))\n\nfor i, col in enumerate(train[nom_cols]): \n    tmp = pd.crosstab(train[col],train['target'], normalize='index') * 100\n    tmp = tmp.reset_index()\n    tmp.rename(columns={0:'No',1:'Yes'}, inplace=True)\n\n    ax = plt.pyplot.subplot(2,3,i+1)\n    sns.countplot(x=col, data=train, order=list(tmp[col].values) , palette='Set2') \n    ax.set_ylabel('Count', fontsize=15) # y axis label\n    ax.set_title(f'{col} Distribution by Target', fontsize=18) # title label\n    ax.set_xlabel(f'{col} values', fontsize=15) # x axis label\n\n    # twinX - to build a second yaxis\n    gt = ax.twinx()\n    gt = sns.pointplot(x=col, y='Yes', data=tmp,\n                           order=list(tmp[col].values),\n                           color='black', legend=False)\n    gt.set_ylim(tmp['Yes'].min()-5,tmp['Yes'].max()*1.1)\n    gt.set_ylabel(\"Target %True(1)\", fontsize=16)\n    sizes=[] # Get highest values in y\n    total = sum([p.get_height() for p in ax.patches])\n    for p in ax.patches: # loop to all objects\n        height = p.get_height()\n        sizes.append(height)\n        ax.text(p.get_x()+p.get_width()\/2.,\n                    height + 2000,\n                    '{:1.2f}%'.format(height\/total*100),\n                    ha=\"center\") \n    ax.set_ylim(0, max(sizes) * 1.15) # set y limit based on highest heights\n\n\nplt.pyplot.subplots_adjust(hspace = 0.5, wspace=.3)\nplt.pyplot.show()","bff2fc2a":"plt.pyplot.figure(figsize=(17, 35)) \nnom_cols = [f'nom_{i}' for i in range(5)]\nfig, ax = plt.pyplot.subplots(2,3,figsize=(22,10))\n\nfor i, col in enumerate(train[nom_cols]): \n    tmp = pd.crosstab(train[col],train['target'], normalize='index') * 100\n    tmp = tmp.reset_index()\n    tmp.rename(columns={0:'No',1:'Yes'}, inplace=True)\n\n    ax = plt.pyplot.subplot(2,3,i+1)\n    sns.countplot(x=col, data=train, order=list(tmp[col].values) , palette='Set2') \n    ax.set_ylabel('Count', fontsize=15) # y axis label\n    ax.set_title(f'{col} Distribution by Target', fontsize=18) # title label\n    ax.set_xlabel(f'{col} values', fontsize=15) # x axis label\n\n    # twinX - to build a second yaxis\n    gt = ax.twinx()\n    gt = sns.pointplot(x=col, y='No', data=tmp,\n                           order=list(tmp[col].values),\n                           color='black', legend=False)\n    gt.set_ylim(tmp['Yes'].min()-5,tmp['No'].max()*1.1)\n    gt.set_ylabel(\"Target %False(0)\", fontsize=16)\n    sizes=[] # Get highest values in y\n    total = sum([p.get_height() for p in ax.patches])\n    for p in ax.patches: # loop to all objects\n        height = p.get_height()\n        sizes.append(height)\n        ax.text(p.get_x()+p.get_width()\/2.,\n                    height + 2000,\n                    '{:1.2f}%'.format(height\/total*100),\n                    ha=\"center\") \n    ax.set_ylim(0, max(sizes) * 1.15) # set y limit based on highest heights\n\n\nplt.pyplot.subplots_adjust(hspace = 0.5, wspace=.3)\nplt.pyplot.show()","90e9328c":"ord_cols = [f'ord_{i}' for i in range(3)]\nplt.pyplot.figure(figsize=(17, 35)) \nfig, ax = plt.pyplot.subplots(1,3,figsize=(22,10))\n\nfor i, col in enumerate(train[ord_cols]): \n    tmp = pd.crosstab(train[col],train['target'], normalize='index') * 100\n    tmp = tmp.reset_index()\n    tmp.rename(columns={0:'No',1:'Yes'}, inplace=True)\n\n    ax = plt.pyplot.subplot(2,3,i+1)\n    sns.countplot(x=col, data=train, order=list(tmp[col].values) , palette='Set2') \n    ax.set_ylabel('Count', fontsize=15) # y axis label\n    ax.set_title(f'{col} Distribution by Target', fontsize=18) # title label\n    ax.set_xlabel(f'{col} values', fontsize=15) # x axis label\n\n    # twinX - to build a second yaxis\n    gt = ax.twinx()\n    gt = sns.pointplot(x=col, y='Yes', data=tmp,\n                           order=list(tmp[col].values),\n                           color='black', legend=False)\n    gt.set_ylim(tmp['Yes'].min()-5,tmp['Yes'].max()*1.1)\n    gt.set_ylabel(\"Target %True(1)\", fontsize=16)\n    sizes=[] # Get highest values in y\n    total = sum([p.get_height() for p in ax.patches])\n    for p in ax.patches: # loop to all objects\n        height = p.get_height()\n        sizes.append(height)\n        ax.text(p.get_x()+p.get_width()\/2.,\n                    height + 2000,\n                    '{:1.2f}%'.format(height\/total*100),\n                    ha=\"center\") \n    ax.set_ylim(0, max(sizes) * 1.15) # set y limit based on highest heights\n\n\nplt.pyplot.subplots_adjust(hspace = 0.5, wspace=.3)\nplt.pyplot.show()","6426eb6a":"plt.pyplot.figure(figsize=(17, 35)) \nord_cols = [f'ord_{i}' for i in range(3)]\nfig, ax = plt.pyplot.subplots(1,3,figsize=(22,10))\n\nfor i, col in enumerate(train[ord_cols]): \n    tmp = pd.crosstab(train[col],train['target'], normalize='index') * 100\n    tmp = tmp.reset_index()\n    tmp.rename(columns={0:'No',1:'Yes'}, inplace=True)\n\n    ax = plt.pyplot.subplot(2,3,i+1)\n    sns.countplot(x=col, data=train, order=list(tmp[col].values) , palette='Set2') \n    ax.set_ylabel('Count', fontsize=15) # y axis label\n    ax.set_title(f'{col} Distribution by Target', fontsize=18) # title label\n    ax.set_xlabel(f'{col} values', fontsize=15) # x axis label\n\n    # twinX - to build a second yaxis\n    gt = ax.twinx()\n    gt = sns.pointplot(x=col, y='No', data=tmp,\n                           order=list(tmp[col].values),\n                           color='black', legend=False)\n    gt.set_ylim(tmp['Yes'].min()-5,tmp['No'].max()*1.1)\n    gt.set_ylabel(\"Target %False(0)\", fontsize=16)\n    sizes=[] # Get highest values in y\n    total = sum([p.get_height() for p in ax.patches])\n    for p in ax.patches: # loop to all objects\n        height = p.get_height()\n        sizes.append(height)\n        ax.text(p.get_x()+p.get_width()\/2.,\n                    height + 2000,\n                    '{:1.2f}%'.format(height\/total*100),\n                    ha=\"center\") \n    ax.set_ylim(0, max(sizes) * 1.15) # set y limit based on highest heights\n\n\nplt.pyplot.subplots_adjust(hspace = 0.5, wspace=.3)\nplt.pyplot.show()","4c65f8b2":"train = train_copy.copy()\n\nord_order = [\n    [1.0, 2.0, 3.0],\n    ['Novice', 'Contributor', 'Expert', 'Master', 'Grandmaster'],\n    ['Freezing', 'Cold', 'Warm', 'Hot', 'Boiling Hot', 'Lava Hot']\n]\n\nfor i in range(1, 3):\n    ord_order_dict = {i : j for j, i in enumerate(ord_order[i])}\n    train[f'ord_{i}_en'] = train[f'ord_{i}'].fillna('NULL').map(ord_order_dict)\n    test[f'ord_{i}_en'] = test[f'ord_{i}'].fillna('NULL').map(ord_order_dict)\n    \nfor i in range(3, 6):\n    ord_order_dict = {i : j for j, i in enumerate(sorted(list(set(list(train[f'ord_{i}'].dropna().unique()) + list(test[f'ord_{i}'].dropna().unique())))))}\n    train[f'ord_{i}_en'] = train[f'ord_{i}'].fillna('NULL').map(ord_order_dict)\n    test[f'ord_{i}_en'] = test[f'ord_{i}'].fillna('NULL').map(ord_order_dict)","58714ae3":"\ncat_cols = [c for c in train.columns if '_en' not in c and c != 'target']\ntrain[cat_cols] = train[cat_cols].astype('category')\ntest[cat_cols] = test[cat_cols].astype('category')","54653a84":"y = train['target']\nX = train\nX.drop(['target'], axis=1, inplace=True)\n\nX_test = test\n#X_test.drop(['id'], axis=1, inplace=True)\nprint(f'X shape: {X.shape}, y shape: {y.shape}, X_test shape: {X_test.shape}')","9f2ffb77":"n_folds=3 #for faster demo, in the competition is 50\nepochs=1 #for faster demo, in the competition is 100\nbatch_size=128","b051b143":"\nconf = ModelConfig(\n    dnn_params={\n        'hidden_units':((300, 0.3, True),(300, 0.3, True),), #hidden_units\n        'dnn_activation':'relu',\n    },\n    fixed_embedding_dim=True,\n    embeddings_output_dim=20,\n    nets =['linear','cin_nets','dnn_nets'],\n    stacking_op = 'add',\n    output_use_bias = False,\n    cin_params={\n       'cross_layer_size': (200, 200),\n       'activation': 'relu',\n       'use_residual': False,\n       'use_bias': True,\n       'direct': True, \n       'reduce_D': False,\n    },\n)\n\ndt = DeepTable(config = conf)\noof_proba, eval_proba, test_prob = dt.fit_cross_validation(\n    X, y, X_eval=None, X_test=X_test, \n    num_folds=n_folds, stratified=False, iterators=None, \n    batch_size=batch_size, epochs=epochs, verbose=1, callbacks=[], n_jobs=1)\n","06a69064":"submission = pd.read_csv('..\/input\/cat-in-the-dat-ii\/sample_submission.csv')\nsubmission['target'] = test_prob\nsubmission.to_csv('submission_linear_dnn_cin_kfold50.csv',index=False)","46adbc51":"Before moving ahead, here are few points about my notebook:\n* This is notebook for the competition : [Categorical Feature Encoding Challend II](https:\/\/www.kaggle.com\/c\/cat-in-the-dat-ii\/overview)\n* I have tried to replicate the first place solution by [Jian Yang](https:\/\/www.kaggle.com\/jackguagua)(thanks to him for sharing his code)\n* I have also added my EDA which I did before modelling\n* If you like this please upvote my notebook :)\n* I welcome suggestions","98c79574":"let us see again for target '0'","5f227c09":"In this kind of features, there are two broad types : one with less cardinality and one with high cardinality. \n\nLow cardinality features :","e40cfa10":"<h2> Our next step is handling Nominal Features <\/h2> ","1d6820f9":"As we can see, it is not an imbalanced dataset. So we don't need to worry about Oversampling or Undersampling.","72b7bbb7":"<h2> Missing values <\/h2>","ddae8313":"<h2>Target Column<\/h2>","087cb9b2":"Let us first see the data and get a basic understanding of the data","998cc73e":"This means many missing values are present. Let us also try to get percentage of missing values of each column.","c88857f6":"<h2> Preprocessing <\/h2>","df0e462c":"<h2> Correlation between numerical features <\/h2>","ed112b1d":"These 3 patterns are almost the same, except from the case of ord_1 and month features, the 1-target class has a slightly different distribution. That means, similar to the train data set, there will be an overwhelming number of 1-target samples compare to the rest. Above graph was made with reference to [Phan Viet Hoang](https:\/\/www.kaggle.com\/warkingleo2000\/first-step-on-kaggle)","ef99397c":"As we can see there is no considerable correlation. Lets move ahead","3fd7dc9c":"This shows that there is similar distribution for both train and test set. Now let us check it with target.","cf809504":"We can see that the distribution of column bin_4 is almost equal in both target values. While bin_0 has the highest difference.","ecc35999":"The train and test set have the same distribution. This means that a model which performs good on training data will perform good on test data too. Let us analyze for different target values too.","bd0befa7":"<h2> Now let us look forward to Ordinal Features <\/h2>","34ba36ed":"The above figure helps us to find few inferences about the data. The most important inference is that the value having maximum values in the data do not have high percentage of true value for the target column.","ddfb9440":"<h2> Now let us work on ML Model <\/h2>","cdf976a2":"One inference that can be found in the above graphs is that the value which is present the least in any column has he highest percentage of '0' in them","fa8b0930":"Analyzing high cardinality features is bit difficult and useless. That is why I prefer to leave them for now","5787e144":"<h1>Welcome to my kernel!<\/h1>","6b4af57c":"<h2> Let us now look into the numerical features <\/h2>","264b0261":"For more understanding let us make the same thing with 0 value of target.","249c6844":"![](https:\/\/miro.medium.com\/max\/1200\/1*pzrsGTuIVROqhz6_pXHgLg.jpeg)","5507d363":"Like nominal features it is the same case with this feature too: low and high cardinality\n\nLet us first take low cardinality features and do the same like above","0f99d884":"Missing values are around 2-3 % for each column which is very less in the dataset. They should not affect the model much","8dcc9196":"<h2> Let us now look into the binary features <\/h2>"}}