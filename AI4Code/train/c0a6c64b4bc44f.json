{"cell_type":{"18292365":"code","7b4a969b":"code","7c073146":"code","101cf7f5":"code","15ec4eb8":"code","e8cf9ff7":"code","e94e475d":"code","25ec7b0e":"code","cfd0ed6b":"code","a424d908":"code","6c0bc2a3":"code","b85d7386":"code","6b6f2279":"code","1f84ab2d":"code","4aa825db":"code","b35bcba4":"code","a9753345":"code","f509c7d3":"code","d7440f3e":"code","16d46ebf":"code","73f1e8d8":"code","9012ebe6":"code","3ed83036":"code","6ad28450":"code","6bbb8024":"code","40085cc0":"markdown","5c9db413":"markdown","7444fa19":"markdown","bfb857fe":"markdown","27cd32df":"markdown","65ed088a":"markdown"},"source":{"18292365":"# Importing Libraries\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\n\nfrom scipy.stats import norm\nfrom sklearn.preprocessing import StandardScaler\nfrom scipy import stats\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\n#for displaying 500 results in pandas dataframe\npd.set_option('display.max_rows', 500)\npd.set_option('display.max_columns', 500)\npd.set_option('display.width', 1000)","7b4a969b":"# File Path\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n","7c073146":"        \ndf = pd.read_csv('\/kaggle\/input\/2020-democratic-primary-endorsements\/endorsements-2020.csv')\ndf.head()","101cf7f5":"#Shape of dataframe\nprint(\" Shape of dataframe: \", df.shape)\n","15ec4eb8":"# names of columns\nprint(df.columns)","e8cf9ff7":"#Datatypes\nprint(df.info())\n","e94e475d":"#Unique values\n\nprint(len(df.position.unique()))\nprint(len(df.city.unique()))","25ec7b0e":"#Let's see how null values look in dataframe\n#Missing data as white lines \nimport missingno as msno\nmsno.matrix(df,color=(0,0.3,0.9))","cfd0ed6b":"#Null values\n\nnull= df.isnull().sum().sort_values(ascending=False)\ntotal =df.shape[0]\npercent_missing= (df.isnull().sum()\/total).sort_values(ascending=False)\n\nmissing_data= pd.concat([null, percent_missing], axis=1, keys=['Total missing', 'Percent missing'])\n\nmissing_data.reset_index(inplace=True)\nmissing_data= missing_data.rename(columns= { \"index\": \" column name\"})\n \nprint (\"Null Values in each column:\\n\", missing_data)\n","a424d908":"#drop columns\n\ndf= df.drop(columns= [\"date\", \"city\", \"body\", \"order\", \"district\", \"source\", \"endorsee\"])","6c0bc2a3":"#Dealing with Null values in endorser party\n\ndf[\"endorser_partyna\"]= df[\"endorser party\"]\n","b85d7386":"df[\"endorser_partyna\"]=df[\"endorser_partyna\"].fillna(\"Missing\")","6b6f2279":"df[\"endorser_partyna\"].value_counts()","1f84ab2d":"df.head()","4aa825db":"sns.boxplot(df[\"points\"])","b35bcba4":"sns.violinplot(x=\"endorser_partyna\", y=\"points\", data=df, size=6)","a9753345":"df.groupby(\"category\").points.sum()","f509c7d3":"df.groupby(\"position\").points.sum()","d7440f3e":"df.groupby(\"endorser_partyna\").points.sum()","16d46ebf":"df.columns","73f1e8d8":"df.groupby([\"category\", \"position\"]).points.sum()","9012ebe6":"df.state.value_counts(normalize= True)","3ed83036":"df.columns","6ad28450":"df.groupby([\"category\", 'endorser_partyna']).points.sum()","6bbb8024":"df.groupby([\"category\", \"position\", 'endorser_partyna']).points.sum()","40085cc0":"### Data Inspection","5c9db413":"### Flags:\n\n#### Date, endorsee,source, district  column has 80% missing data\n#### City is barely having 5% of data\n#### order, order  have 85-90% missing data\n\n#### Date is object, should be datetime\n","7444fa19":"### Loading dataset","bfb857fe":"### Importing Libraries","27cd32df":"## Since we can not impute categorical missing data at this scale- city, body, order, district, source, endorsee, hence dropping the columns (we can use it for viz, but it makes no sense to use incomplete information as it will lead to biased insights)\n\n## Dropping date column too\n","65ed088a":"### Anamoly detection\n#### Working on null values (flags)****"}}