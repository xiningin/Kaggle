{"cell_type":{"d7d7bb73":"code","3b8e22a1":"code","ceaa3ee1":"code","46057adf":"code","1151b9e5":"code","b2158473":"code","be9bdba7":"code","2f2f1cf7":"code","87318f6a":"code","3f516de5":"code","1e1bce27":"code","abeb0107":"code","5d32d48d":"code","4add8bc3":"code","7c258099":"code","79e7c3fa":"code","38d56718":"code","8f778129":"code","ffe7a5f7":"code","18e09a01":"code","fe175a60":"code","78cca8a4":"code","7325a20b":"code","99851d89":"code","5dafa12c":"code","a87bc7b0":"code","9ee36750":"code","477accbf":"code","2eb20116":"code","3cad4d6f":"markdown","fa2fb36c":"markdown","473b11a2":"markdown","cfb56f35":"markdown","04bfdd4b":"markdown"},"source":{"d7d7bb73":"#checking where the files are\nimport os\nos.listdir(\"\/kaggle\/input\/training\")","3b8e22a1":"#importing the rquired libraries\nimport pandas as pd\nimport numpy as np\nimport keras\nfrom tqdm import tqdm\nfrom keras import backend as K","ceaa3ee1":"lookid_data = pd.read_csv(\"\/kaggle\/input\/IdLookupTable.csv\")\nlookid_data.head()","46057adf":"samplesubmission = pd.read_csv(\"\/kaggle\/input\/SampleSubmission.csv\")\nsamplesubmission.head()","1151b9e5":"train = pd.read_csv(\"\/kaggle\/input\/training\/training.csv\")\ntrain.head().T","b2158473":"train.isnull().sum()","be9bdba7":"#filling the nan values\ntrain.fillna(method = 'ffill',inplace = True)","2f2f1cf7":"X = train.Image.values\ndel train['Image']\nY = train.values","87318f6a":"x = []\nfor i in tqdm(X):\n    q = [int(j) for j in i.split()]\n    x.append(q)\nlen(x)","3f516de5":"x = np.array(x)\nx = x.reshape(7049, 96,96,1)\nx  = x\/255.0\nx.shape","1e1bce27":"from sklearn.model_selection import train_test_split as tts\nx_train,x_test,y_train,y_test = tts(x,Y,random_state = 69,test_size = 0.1)","abeb0107":"x_train.shape,x_test.shape,y_train.shape,y_test.shape","5d32d48d":"from keras.layers.advanced_activations import LeakyReLU\nfrom keras.models import Sequential, Model\nfrom keras.layers import Activation, Convolution2D, MaxPooling2D, BatchNormalization, Flatten, Dense, Dropout, Conv2D,MaxPool2D, ZeroPadding2D","4add8bc3":"model = Sequential()\n\nmodel.add(Convolution2D(32, (3,3), padding='same', use_bias=False, input_shape=(96,96,1)))\nmodel.add(LeakyReLU(alpha = 0.1))\nmodel.add(BatchNormalization())\n\nmodel.add(Convolution2D(32, (3,3), padding='same', use_bias=False))\nmodel.add(LeakyReLU(alpha = 0.1))\nmodel.add(BatchNormalization())\nmodel.add(MaxPool2D(pool_size=(2, 2)))\n\nmodel.add(Convolution2D(64, (3,3), padding='same', use_bias=False))\nmodel.add(LeakyReLU(alpha = 0.1))\nmodel.add(BatchNormalization())\n\nmodel.add(Convolution2D(64, (3,3), padding='same', use_bias=False))\nmodel.add(LeakyReLU(alpha = 0.1))\nmodel.add(BatchNormalization())\nmodel.add(MaxPool2D(pool_size=(2, 2)))\n\nmodel.add(Convolution2D(96, (3,3), padding='same', use_bias=False))\nmodel.add(LeakyReLU(alpha = 0.1))\nmodel.add(BatchNormalization())\n\nmodel.add(Convolution2D(96, (3,3), padding='same', use_bias=False))\nmodel.add(LeakyReLU(alpha = 0.1))\nmodel.add(BatchNormalization())\nmodel.add(MaxPool2D(pool_size=(2, 2)))\n\nmodel.add(Convolution2D(128, (3,3),padding='same', use_bias=False))\n# model.add(BatchNormalization())\nmodel.add(LeakyReLU(alpha = 0.1))\nmodel.add(BatchNormalization())\n\nmodel.add(Convolution2D(128, (3,3),padding='same', use_bias=False))\nmodel.add(LeakyReLU(alpha = 0.1))\nmodel.add(BatchNormalization())\nmodel.add(MaxPool2D(pool_size=(2, 2)))\n\nmodel.add(Convolution2D(256, (3,3),padding='same',use_bias=False))\nmodel.add(LeakyReLU(alpha = 0.1))\nmodel.add(BatchNormalization())\n\nmodel.add(Convolution2D(256, (3,3),padding='same',use_bias=False))\nmodel.add(LeakyReLU(alpha = 0.1))\nmodel.add(BatchNormalization())\nmodel.add(MaxPool2D(pool_size=(2, 2)))\n\nmodel.add(Convolution2D(512, (3,3), padding='same', use_bias=False))\nmodel.add(LeakyReLU(alpha = 0.1))\nmodel.add(BatchNormalization())\n\nmodel.add(Convolution2D(512, (3,3), padding='same', use_bias=False))\nmodel.add(LeakyReLU(alpha = 0.1))\nmodel.add(BatchNormalization())\n\n\nmodel.add(Flatten())\nmodel.add(Dense(512,activation='relu'))\nmodel.add(Dropout(0.1))\nmodel.add(Dense(30))\nmodel.summary()","7c258099":"# def root_mean_squared_error(y_true, y_pred):\n#         return K.sqrt(K.mean(K.square(y_pred - y_true)))","79e7c3fa":"model.compile(optimizer = 'adam',loss = 'mean_squared_error', metrics = ['mae','acc'])\nmodel.fit(x_train,y_train,batch_size=256, epochs=50,validation_data=(x_test,y_test))","38d56718":"model.compile(optimizer = 'adam',loss = 'mean_squared_error', metrics = ['mae'])\nmodel.fit(x,Y,batch_size=64, epochs=100)\nmodel.fit(x,Y,batch_size=128, epochs=50)\nmodel.fit(x,Y,batch_size=256, epochs=50)","8f778129":"test = pd.read_csv(\"\/kaggle\/input\/test\/test.csv\")\ntest.head()","ffe7a5f7":"test.isnull().sum()","18e09a01":"test = test.Image.values\nx_t = []\nfor i in tqdm(test):\n    q = [int(j) for j in i.split()]\n    x_t.append(q)\nx_t = np.array(x_t)\nx_t = x_t.reshape(-1, 96,96,1)\nx_t = x_t\/255.0\nx_t.shape","fe175a60":"pred = model.predict(x_t)\npred.shape","78cca8a4":"lookid_list = list(lookid_data['FeatureName'])\nimageID = list(lookid_data['ImageId']-1)\npre_list = list(pred)","7325a20b":"rowid = lookid_data['RowId']\nrowid=list(rowid)","99851d89":"feature = []\nfor f in list(lookid_data['FeatureName']):\n    feature.append(lookid_list.index(f))","5dafa12c":"preded = []\nfor x,y in zip(imageID,feature):\n    preded.append(pre_list[x][y])","a87bc7b0":"rowid = pd.Series(rowid,name = 'RowId')","9ee36750":"loc = pd.Series(preded,name = 'Location')","477accbf":"submission = pd.concat([rowid,loc],axis = 1)","2eb20116":"submission.to_csv('Utkarsh.csv',index = False)","3cad4d6f":"#### Preparing the training data","fa2fb36c":"#### Training on the complete Dataset now","473b11a2":"#### Splitting the data into 90-10 train test split","cfb56f35":"#### Model","04bfdd4b":"#### Predicting for test data"}}