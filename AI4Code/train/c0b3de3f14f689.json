{"cell_type":{"8f80c3ae":"code","32238f62":"code","5fbf325f":"code","9d912910":"code","1bb56bd5":"code","362a7007":"code","8140ec22":"code","b679aac4":"code","00981c1b":"code","72b1f282":"code","88ce954a":"code","a22b91df":"code","c3164b3b":"code","1ea92896":"code","0b54f1d0":"code","e83a5562":"code","d5a8f6a2":"code","de33745d":"code","ae30cfef":"code","84c7bf83":"code","37f960aa":"code","1748b628":"code","4863a9ab":"code","b2fc5595":"markdown","ff8fcd17":"markdown","0aa6fb07":"markdown","d7378bac":"markdown","108729eb":"markdown","fe3eebbb":"markdown","255f3ae9":"markdown","2e28becd":"markdown"},"source":{"8f80c3ae":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","32238f62":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.metrics import confusion_matrix, accuracy_score\nfrom sklearn.model_selection import train_test_split\n\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.models import Sequential\nfrom keras.layers import *\nfrom keras.utils.np_utils import to_categorical\nfrom keras.initializers import Constant\nimport re\n\nimport matplotlib.pyplot as plt\n%matplotlib inline","5fbf325f":"df = pd.read_csv('..\/input\/movie-review-sentiment-analysis-kernels-only\/train.tsv', delimiter='\\t')\ndf = df[['Phrase', 'Sentiment']]\n\npd.set_option('display.max_colwidth', -1)\ndf.head(3)","9d912910":"replace_puncts = {'`': \"'\", '\u2032': \"'\", '\u201c':'\"', '\u201d': '\"', '\u2018': \"'\"}\n\nstrip_chars = [',', '.', '\"', ':', ')', '(', '-', '|', ';', \"'\", '[', ']', '>', '=', '+', '\\\\', '\u2022',  '~', '@', \n '\u00b7', '_', '{', '}', '\u00a9', '^', '\u00ae', '`',  '<', '\u2192', '\u00b0', '\u20ac', '\u2122', '\u203a',  '\u2665', '\u2190', '\u00d7', '\u00a7', '\u2033', '\u2032', '\u00c2', '\u2588', '\u00bd', '\u00e0', '\u2026', \n '\u201c', '\u2605', '\u201d', '\u2013', '\u25cf', '\u00e2', '\u25ba', '\u2212', '\u00a2', '\u00b2', '\u00ac', '\u2591', '\u00b6', '\u2191', '\u00b1', '\u00bf', '\u25be', '\u2550', '\u00a6', '\u2551', '\u2015', '\u00a5', '\u2593', '\u2014', '\u2039', '\u2500', \n '\u2592', '\uff1a', '\u00bc', '\u2295', '\u25bc', '\u25aa', '\u2020', '\u25a0', '\u2019', '\u2580', '\u00a8', '\u2584', '\u266b', '\u2606', '\u00e9', '\u00af', '\u2666', '\u00a4', '\u25b2', '\u00e8', '\u00b8', '\u00be', '\u00c3', '\u22c5', '\u2018', '\u221e', \n '\u2219', '\uff09', '\u2193', '\u3001', '\u2502', '\uff08', '\u00bb', '\uff0c', '\u266a', '\u2569', '\u255a', '\u00b3', '\u30fb', '\u2566', '\u2563', '\u2554', '\u2557', '\u25ac', '\u2764', '\u00ef', '\u00d8', '\u00b9', '\u2264', '\u2021', '\u221a', ]\n\npuncts = ['!', '?', '$', '&', '\/', '%', '#', '*','\u00a3']\n\ndef clean_str(x):\n    x = str(x)\n    \n    x = x.lower()\n    \n    x = re.sub(r\"(https?:\\\/\\\/(?:www\\.|(?!www))[a-zA-Z0-9][a-zA-Z0-9-]+[a-zA-Z0-9]\\.[^\\s]{2,}|www\\.[a-zA-Z0-9][a-zA-Z0-9-]+[a-zA-Z0-9]\\.[^\\s]{2,}|https?:\\\/\\\/(?:www\\.|(?!www))[a-zA-Z0-9]\\.[^\\s]{2,}|www\\.[a-zA-Z0-9]\\.[^\\s]{2,})\", \"url\", x)\n    \n    for k, v in replace_puncts.items():\n        x = x.replace(k, f' {v} ')\n        \n    for punct in strip_chars:\n        x = x.replace(punct, ' ') \n    \n    for punct in puncts:\n        x = x.replace(punct, f' {punct} ')\n        \n    x = x.replace(\" '\", \" \")\n    x = x.replace(\"' \", \" \")\n        \n    return x\n\n\ndf['text'] = df['Phrase'].apply(clean_str)","1bb56bd5":"df_0 = df[df['Sentiment'] == 0].sample(frac=1)\ndf_1 = df[df['Sentiment'] == 1].sample(frac=1)\ndf_2 = df[df['Sentiment'] == 2].sample(frac=1)\ndf_3 = df[df['Sentiment'] == 3].sample(frac=1)\ndf_4 = df[df['Sentiment'] == 4].sample(frac=1)\n\n# we want a balanced set for training against - there are 7072 `0` examples\nsample_size = min(len(df_0), len(df_1), len(df_2), len(df_3), len(df_4))\n\ndata = pd.concat([df_0.head(sample_size), df_1.head(sample_size), df_2.head(sample_size), df_3.head(sample_size), df_4.head(sample_size)]).sample(frac=1)","362a7007":"data['l'] = data['Phrase'].apply(lambda x: len(str(x).split(' ')))\nprint(\"mean length of sentence: \" + str(data.l.mean()))\nprint(\"max length of sentence: \" + str(data.l.max()))\nprint(\"std dev length of sentence: \" + str(data.l.std()))","8140ec22":"# these sentences aren't that long so we may as well use the whole string\nsequence_length = 52","b679aac4":"max_features = 20000 # this is the number of words we care about\n\ntokenizer = Tokenizer(num_words=max_features, split=' ', oov_token='<unw>', filters=' ')\ntokenizer.fit_on_texts(data['Phrase'].values)\n\n# this takes our sentences and replaces each word with an integer\nX = tokenizer.texts_to_sequences(data['Phrase'].values)\n\n# we then pad the sequences so they're all the same length (sequence_length)\nX = pad_sequences(X, sequence_length)\n\ny = pd.get_dummies(data['Sentiment']).values\n\n# lets keep a couple of thousand samples back as a test set\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1)\n\nprint(\"test set size \" + str(len(X_test)))","00981c1b":"embeddings_index = {}\nf = open(os.path.join('..\/input\/glove-global-vectors-for-word-representation', 'glove.6B.100d.txt'))\nfor line in f:\n    values = line.split()\n    word = values[0]\n    coefs = np.asarray(values[1:], dtype='float32')\n    embeddings_index[word] = coefs\nf.close()\n\nprint('Found %s word vectors.' % len(embeddings_index))","72b1f282":"word_index = tokenizer.word_index\nprint('Found %s unique tokens.' % len(word_index))","88ce954a":"num_words = min(max_features, len(word_index)) + 1\nprint(num_words)\n\nembedding_dim = 100\n\n# first create a matrix of zeros, this is our embedding matrix\nembedding_matrix = np.zeros((num_words, embedding_dim))\n\n# for each word in out tokenizer lets try to find that work in our w2v model\nfor word, i in word_index.items():\n    if i > max_features:\n        continue\n    embedding_vector = embeddings_index.get(word)\n    if embedding_vector is not None:\n        # we found the word - add that words vector to the matrix\n        embedding_matrix[i] = embedding_vector\n    else:\n        # doesn't exist, assign a random vector\n        embedding_matrix[i] = np.random.randn(embedding_dim)","a22b91df":"model = Sequential()\nmodel.add(Embedding(num_words,\n                    embedding_dim,\n                    embeddings_initializer=Constant(embedding_matrix),\n                    input_length=sequence_length,\n                    trainable=True))\nmodel.add(SpatialDropout1D(0.2))\nmodel.add(Bidirectional(CuDNNLSTM(64, return_sequences=True)))\nmodel.add(Bidirectional(CuDNNLSTM(32)))\nmodel.add(Dropout(0.25))\nmodel.add(Dense(units=5, activation='softmax'))\nmodel.compile(loss = 'categorical_crossentropy', optimizer='adam',metrics = ['accuracy'])\nprint(model.summary())","c3164b3b":"batch_size = 128\nhistory = model.fit(X_train, y_train, epochs=5, batch_size=batch_size, verbose=1, validation_split=0.1)","1ea92896":"plt.plot(history.history['acc'])\nplt.plot(history.history['val_acc'])\nplt.title('model accuracy')\nplt.ylabel('accuracy')\nplt.xlabel('epoch')\nplt.legend(['train', 'validation'], loc='upper left')\nplt.show()\n\nplt.plot(history.history['loss'])\nplt.plot(history.history['val_loss'])\nplt.title('model loss')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.legend(['train', 'validation'], loc='upper left')\nplt.show()","0b54f1d0":"y_hat = model.predict(X_test)","e83a5562":"accuracy_score(list(map(lambda x: np.argmax(x), y_test)), list(map(lambda x: np.argmax(x), y_hat)))","d5a8f6a2":"conf = confusion_matrix(list(map(lambda x: np.argmax(x), y_test)), list(map(lambda x: np.argmax(x), y_hat)))\nconf","de33745d":"plt.imshow(conf)","ae30cfef":"df_test = pd.read_csv('..\/input\/movie-review-sentiment-analysis-kernels-only\/test.tsv', delimiter='\\t')\n\ndf_test['text'] = df_test['Phrase'].apply(clean_str)","84c7bf83":"x = tokenizer.texts_to_sequences(df_test['text'].values)\nx = pad_sequences(x, sequence_length)\n\ny_hat = model.predict(x)","37f960aa":"df_results = pd.DataFrame(list(zip(df_test['PhraseId'].values, list(map(lambda x: np.argmax(x), y_hat)))), columns=['PhraseId', 'Sentiment'])","1748b628":"df_results.head(3)","4863a9ab":"df_results.to_csv('bilstm.csv', index=False)","b2fc5595":"so how does this perform?","ff8fcd17":"# Bidirectional LSTM in Keras with GloVe embeddings\n\nIn this quick kernel I'm going to use a multilayered bidirectional LSTM to classify text. Rather than using random embeddings for words I'm going to use GloVe embeddings.\n\nThis has the benifit that words which are close to one another are in some sense close in the embedding space.","0aa6fb07":"Here's a tip I learnt the hard way: ideally you should try clean your sentences in exactly the same way that it's been cleaned for the word embeddings. If you don't do this then later when we try to match words up with vectors we won't find a match!\n\nSadly, I'm not sure how the embeddings used in this data source were used, but this seems fairly close (I've also replaced urls with 'url')","d7378bac":"Now our model.\n\nThe important bit here is the `Embedding`, we need to specify the embeddings matrix (the set of vectors representing our words) and for this example I'm going to make them trainable - this means they can be modified during training if something more accurate is found\n\nI'm going to use Keras' `CuDNNLSTM` layer, which is an LSTM implementation ready to work on GPUs (it doesn't run on CPUs). If we're going to stack RNNs it's also important to return the sequences.\n\nKeras makes making this Bidirectional very easy - you just need to wrap the LSTM in Bidirctional!","108729eb":"Now for a bit of fun - lets go through all the words in our tokenizer and find the word embedding (vector) for each.\n\nWhich leaves us with a question - what should we do with the words we can't find? In this example I'm going to give each it's own unique random vector, which we can make trainable later (ie move about)\n\nAnother lesson I learnt the hard way: don't do what I did and give them all the same vector","fe3eebbb":"It's important to have a balanced training set for our classifier, here's a simplistic way to approach this","255f3ae9":"Let's tokenize our text.\n\nNote, there are a couple of details around the `oov_token` that are worth knowing about. Firstly if you don't declare an oov token, Keras will ignore the word, so for example if the word \"brown\" isn't in our tokenizer dictionary\n\n\"the quick brown fox\"\n\nmight become\n\n[1, 312, 21479]\n\nwere 1 -> \"the\", 2 -> \"quick\", 21479 -> \"fox\". \"brown\" isn't here, so it gets ignored\n\nI've not seen any research on this, but ignoring the fact there was a word there feels weird - you can compensate for this with an `oov_token`. If the `oov_token` is 20000, then this would now become `[1, 312, 20000, 21479]`\n\nSecondly, there's a bit of oddity around what keras picks as the `oov_token`. If your corpus has 14,281 unique words, then the `oov_token` will be given the 14,282nd index. That's something you've got to remember when quoting `max_features` later","2e28becd":"Now we need to get the word GloVe embeddings into a format ready to query later.\n\nSide note: I did a bit of experimentation here and found I got slightly better results using the 100d word embeddings for this task and archetecutre. I assume the benifit of using 200d is outweighed by the extra features - this could be an interesting thing to investigate"}}