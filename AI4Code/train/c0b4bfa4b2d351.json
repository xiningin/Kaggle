{"cell_type":{"11732b7c":"code","6894d2ba":"code","b028af32":"code","a13c30b2":"code","8ce3cec5":"code","aa8e9a7b":"code","d0e147fd":"code","64dba29a":"code","4c51153c":"code","ec40d057":"code","7e43ee10":"code","efb64443":"code","bf740f48":"code","73e28291":"code","b35f0da6":"code","b9684fb0":"code","90e7a00f":"code","965f1099":"code","41bb2fcc":"code","ba53e922":"code","dccdb782":"code","4d4ded2c":"code","ada00cd7":"code","4aadaa89":"code","3a1a99fc":"code","d279f3f6":"code","ca5e4806":"code","8e2d664a":"code","bc091f8a":"code","fe0a7dc8":"code","c3a01421":"code","727c8db6":"code","92ed54b5":"code","d81631de":"code","ca1ae5e5":"code","700c4023":"code","23483c6a":"code","3e2235c9":"code","cc64198c":"code","846b08f6":"code","74f952eb":"code","409b664e":"code","06fd53b3":"code","11002989":"code","d7231e15":"code","c6b26ebb":"code","c3f77884":"code","87d9d305":"code","0de0d2c0":"code","782e8e48":"code","7a172233":"code","8488d714":"code","1b318281":"code","30b52323":"code","c0db78a0":"code","5cbef514":"code","1071ae97":"code","5b3b8339":"code","3e243a24":"code","5905ee51":"code","2374a5e8":"code","0438aa47":"code","31ddbf30":"code","4c10b45f":"code","f8ef134a":"code","d4db95b9":"code","3710dfb9":"code","5779e0aa":"code","b8344b1a":"code","e7d8acd2":"code","061b15e4":"code","42c11607":"code","c72efd4f":"code","629a9b2a":"code","8d1448ad":"code","cd2d23d1":"code","6e2bcf66":"code","e50e619d":"code","14813132":"code","3cf82d55":"code","fab5d404":"code","cece31aa":"code","840f40e2":"code","79b6dcf8":"code","33bd5911":"code","5483e1cf":"code","1d903fd8":"code","32f39366":"code","4abee380":"code","538280ad":"code","b8b8e513":"code","afb0f84d":"code","fbd33fe5":"code","10e23752":"code","8c18518d":"code","4ebcf2b0":"code","8e3c71dc":"code","ed7abece":"code","6dea93c1":"code","44d05880":"code","8614b468":"code","6a04f8dd":"code","4f2aa046":"code","0ab7997d":"code","370b33cf":"code","618664e6":"code","25b42f10":"code","5312672e":"code","ed776bdd":"code","47fb520f":"code","b4839326":"code","7ea99f51":"code","7988b937":"code","807050b1":"code","def3c6ea":"code","051fdc9e":"code","4040e84e":"code","da2c282d":"code","7b84c675":"code","1218aafe":"code","d2031c9c":"code","4abecd34":"code","7915ef9d":"code","e759f422":"code","c1553ae2":"code","ee9695ef":"code","fc24760f":"code","f66baef1":"code","c004e923":"code","a170c900":"code","441b3558":"code","29ae96f8":"code","03390f5c":"code","ca0f2f77":"code","69b23942":"code","4db9e9f0":"code","d312f949":"code","a6b13908":"code","34705ec1":"code","597f6cb7":"code","122bebd5":"code","213b6c51":"code","96a64753":"code","1a8b071e":"markdown","266b6c52":"markdown","c0b781e8":"markdown","4fcf230f":"markdown","15459d89":"markdown","5b82bbdc":"markdown","9e2c8feb":"markdown","a6dc90ba":"markdown","55e443b1":"markdown","d2a37e6e":"markdown","d1dda694":"markdown","0b7fb462":"markdown","a851d124":"markdown","0156304f":"markdown","d2b95a68":"markdown","bb5c81d8":"markdown","d25b03e7":"markdown","e31d5464":"markdown","6bf5cc6a":"markdown","71523bd3":"markdown","4e333096":"markdown","9ef37a05":"markdown","ec008d11":"markdown","e978bd0c":"markdown","6ddad884":"markdown","324d6ab4":"markdown","a11ef743":"markdown","d854dbcf":"markdown","9a8e95af":"markdown","c99b6896":"markdown","3d73142d":"markdown","d88347c2":"markdown","d57bf85a":"markdown","3c5e4581":"markdown"},"source":{"11732b7c":"pip install liwc","6894d2ba":"import warnings\nwarnings.filterwarnings('ignore')\nimport pandas as pd\nfrom imblearn.over_sampling import SMOTE\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom matplotlib import pyplot\nimport matplotlib.pyplot as plt\nfrom sklearn.svm import SVC\nfrom sklearn.metrics import classification_report,confusion_matrix\nimport matplotlib.patches as patches\nfrom sklearn.metrics import roc_curve,auc\nimport pickle\n\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport nltk\nfrom sklearn.preprocessing import LabelBinarizer\nfrom nltk.corpus import stopwords\nfrom nltk.stem.porter import PorterStemmer\nfrom nltk.stem import WordNetLemmatizer\nfrom nltk.tokenize import word_tokenize,sent_tokenize\nfrom bs4 import BeautifulSoup\nimport re,string,unicodedata\n\nfrom sklearn.metrics import classification_report,confusion_matrix,accuracy_score\nfrom sklearn.model_selection import train_test_split\nfrom string import punctuation\nfrom nltk import pos_tag\nfrom nltk.corpus import wordnet\n\n\nfrom collections import Counter\nfrom itertools import chain\nfrom collections import Counter\nimport seaborn as sns\nimport joblib\nfrom scipy import interp\nfrom sklearn import model_selection\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.ensemble import RandomForestClassifier\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn import svm\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.model_selection import GridSearchCV, cross_val_score, StratifiedKFold, learning_curve\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport nltk\nimport re\nimport liwc\nimport pandas as pd\nimport numpy as np\nfrom sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer, HashingVectorizer\nfrom nltk.corpus import stopwords\nfrom nltk.stem.porter import PorterStemmer\nimport re\nimport nltk\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.naive_bayes import MultinomialNB,GaussianNB\nimport itertools\nfrom sklearn.metrics import accuracy_score,confusion_matrix,classification_report\nimport warnings\nwarnings.filterwarnings(\"ignore\")\nfrom nltk import ne_chunk, pos_tag, word_tokenize, sent_tokenize","b028af32":"liwcPath = r'..\/input\/liwc2015\/LIWC2015 Dictionary.dic'\nparse, category_names = liwc.load_token_parser(liwcPath)","a13c30b2":"import pandas as pd\ndf1=pd.read_csv(r'..\/input\/fake-dataset\/Fake.csv')\ndf1['label']=0\ndf2=pd.read_csv(r'..\/input\/fake-dataset\/True.csv')\ndf2['label']=1\n\n# 1 data set\ndf3=pd.read_csv(r'..\/input\/dataset2\/test.csv')\ndf3['label']=pd.read_csv(r'..\/input\/lables\/final_sub.csv').label.values\ndf4=pd.read_csv(r'..\/input\/dataset2\/train.csv')\n# 2 dataset \n\n\ndf5=pd.read_csv(r'..\/input\/dataset-1\/data.csv')\n#3 dataset\n\n\n","8ce3cec5":"df_fake_ds=pd.concat([df1,df2]).drop(['title','subject','date'],axis=1)","aa8e9a7b":"df_fake_ds['label']=df_fake_ds['label'].astype(int)","d0e147fd":"df_fake_ds['label'].value_counts()","64dba29a":"df_fake_ds1=pd.concat([df4,df3]).drop(['title','author','id'],axis=1)","4c51153c":"df_fake_ds1['label']=df_fake_ds1['label'].astype(int)","ec40d057":"df_fake_ds1['label'].value_counts()","7e43ee10":"#df_fake_ds1['label'].apply(lambda x: 0 if x == 1 else 1)","efb64443":"df_fake_ds2=df5.drop(['URLs','Headline'],axis=1).rename({'Body':'text','Label':'label'},axis=1)","bf740f48":"df_fake_ds2['label']=df_fake_ds2['label'].astype(int)","73e28291":"df_fake_ds2['label'].value_counts()","b35f0da6":"fake_news_Df=pd.concat([df_fake_ds,df_fake_ds1,df_fake_ds2])","b9684fb0":"fake_news_Df=df_fake_ds","90e7a00f":"fake_news_Df = fake_news_Df[['text','label']].dropna()\nfake_news_Df.reset_index(inplace = True)","965f1099":"fake_news_Df.shape","41bb2fcc":"def wordopt(text):\n    text = text.lower()\n    text = re.sub('\\[.*?\\]', '', text)\n    text = re.sub(\"\\\\W\",\" \",text) \n    text = re.sub('https?:\/\/\\S+|www\\.\\S+', '', text)\n    text = re.sub('<.*?>+', '', text)\n    text = re.sub('[%s]' % re.escape(string.punctuation), '', text)\n    text = re.sub('\\n', '', text)\n    text = re.sub('\\w*\\d\\w*', '', text)    \n    return text","ba53e922":"fake_news_Df['text'] = fake_news_Df.text.apply(lambda x : wordopt(x))","dccdb782":"fake_news_Df['text']","4d4ded2c":"eng_stopwords = nltk.corpus.stopwords.words(\"english\")","ada00cd7":"def remove_eng_stopwords(text):\n    token_text = nltk.word_tokenize(text)\n    remove_stop = [word for word in token_text if word not in eng_stopwords]\n    join_text = ' '.join(remove_stop)\n    return join_text","4aadaa89":"fake_news_Df['text'] = fake_news_Df.text.apply(lambda x : remove_eng_stopwords(x))","3a1a99fc":"fake_news_Df","d279f3f6":"from nltk.stem import WordNetLemmatizer\nfrom sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\nlemm = WordNetLemmatizer()\ndef word_lemmatizer(text):\n    token_text = nltk.word_tokenize(text)\n    remove_stop = [lemm.lemmatize(w) for w in token_text]\n    join_text = ' '.join(remove_stop)\n    return join_text","ca5e4806":"fake_news_Df['text'] = fake_news_Df.text.apply(lambda x : word_lemmatizer(x))","8e2d664a":"fake_news_Df['text']","bc091f8a":"Word_STOPWORDS = [\"e\",\"ma\", \"te\", \"i\",\"it\", \"me\", \"qe\", \"ne\", \"nje\", \"a\", \"per\", \"sh\", \"nga\", \"ka\", \"u\", \"eshte\", \"dhe\", \"shih\", \"nuk\",\n             \"m\", \"dicka\", \"ose\", \"si\", \"shume\", \"etj\", \"se\", \"pa\", \"sipas\", \"s\", \"t\", \"dikujt\", \"dike\", \"mire\", \"vet\",\n             \"bej\", \"ai\", \"vend\", \"prej\", \"ja\", \"duke\", \"tjeter\", \"kur\", \"ia\", \"ku\", \"ta\", \"keq\", \"dy\", \"ben\", \"bere\",\n             \"behet\", \"dickaje\", \"edhe\", \"madhe\", \"la\", \"sa\", \"gjate\", \"zakonisht\", \"pas\", \"veta\", \"mbi\", \"disa\", \"iu\",\n             \"mos\", \"c\", \"para\", \"dikush\", \"gje\", \"be\", \"pak\", \"tek\", \"fare\", \"beri\", \"po\", \"bie\", \"k\", \"do\", \"gjithe\",\n             \"vete\", \"mund\", \"kam\", \"le\", \"jo\", \"beje\", \"tij\", \"kane\", \"ishte\", \"jane\", \"vjen\", \"ate\", \"kete\", \"neper\",\n             \"cdo\", \"na\", \"marre\", \"merr\", \"mori\", \"rri\", \"deri\", \"b\", \"kishte\", \"mban\", \"perpara\", \"tyre\", \"marr\",\n             \"gjitha\", \"as\", \"vetem\", \"nen\", \"here\", \"tjera\", \"tjeret\", \"drejt\", \"qenet\", \"ndonje\", \"nese\", \"jap\",\n             \"merret\", \"rreth\", \"lloj\", \"dot\", \"saj\", \"nder\", \"ndersa\", \"cila\", \"veten\", \"ma\", \"ndaj\", \"mes\", \"ajo\",\n             \"cilen\", \"por\", \"ndermjet\", \"prapa\", \"mi\", \"tere\", \"jam\", \"ashtu\", \"kesaj\", \"tille\", \"behem\", \"cilat\",\n             \"kjo\", \"menjehere\", \"ca\", \"je\", \"aq\", \"aty\", \"prane\", \"ato\", \"pasur\", \"qene\", \"cilin\", \"teper\", \"njera\",\n             \"tej\", \"krejt\", \"kush\", \"bejne\", \"ti\", \"bene\", \"midis\", \"cili\", \"ende\", \"keto\", \"kemi\", \"sic\", \"kryer\",\n             \"cilit\", \"atij\", \"gjithnje\", \"andej\", \"siper\", \"sikur\", \"ketej\", \"ciles\", \"ky\", \"papritur\", \"ua\",\n             \"kryesisht\", \"gjithcka\", \"pasi\", \"kryhet\", \"mjaft\", \"ketij\", \"perbashket\", \"ata\", \"atje\", \"vazhdimisht\",\n             \"kurre\", \"tone\", \"keshtu\", \"une\", \"sapo\", \"rralle\", \"vetes\", \"ishin\", \"afert\", \"tjetren\", \"ketu\", \"cfare\",\n             \"to\", \"anes\", \"jemi\", \"asaj\", \"secila\", \"kundrejt\", \"ketyre\", \"pse\", \"tilla\", \"mua\", \"nepermjet\", \"cilet\",\n             \"ndryshe\", \"kishin\", \"ju\", \"tani\", \"atyre\", \"dic\", \"yne\", \"kudo\", \"sone\", \"sepse\", \"cilave\", \"kem\", \"ty\",\n             \"t'i\", \"nbsp\", \"tha\", \"re\", \"the\",\"jr\",\"t\",\"j\",\"us\",\"mr\",'f','trump','said','would','one','state','president','people','year']\nto_remove = ['\u2022', '!', '\"', '#', '\u201d', '\u201c', '$', '%', '&', '\u2013', '(', ')', '*', '+', ',', '-', '.', '\/', ':',\n             ';', '<', '=', '>', '?', '@', '[', '\\\\', ']', '^', '_', '`', '{', '|', '}', '~', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '\u2026']\nstop = set()\ntext_unknows= Word_STOPWORDS\nstop.update(text_unknows)\nstop.update(to_remove)","fe0a7dc8":"def strip_html(text):\n    soup = BeautifulSoup(text, \"html.parser\")\n    return soup.get_text()\n\n#Removing the square brackets\ndef remove_between_square_brackets(text):\n    return re.sub('\\[[^]]*\\]', '', text)\n# Removing URL's\ndef remove_between_square_brackets(text):\n    return re.sub(r'http\\S+', '', text)\n#Removing the stopwords from text\ndef remove_stopwords(text):\n    final_text = []\n    for i in text.split():\n        if i.strip().lower() not in stop:\n            final_text.append(i.strip())\n    return \" \".join(final_text)\n#Removing the noisy text\ndef denoise_text(text):\n    text = strip_html(text)\n    text = remove_between_square_brackets(text)\n    text = remove_stopwords(text)\n    return text","c3a01421":"fake_news_Df['text'] = fake_news_Df.text.apply(lambda x : denoise_text(x))","727c8db6":"fake_news_Df['text']","92ed54b5":"fake_news_Df['text']=fake_news_Df['text'].astype(str)","d81631de":"import string\ndef punctuation_removal(text):\n    all_list = [char for char in text if char not in string.punctuation]\n    clean_str = ''.join(all_list)\n    return clean_str\nfake_news_Df['text'] = fake_news_Df['text'].apply(punctuation_removal)","ca1ae5e5":"## added preprocessing from https:\/\/www.kaggle.com\/wowfattie\/3rd-place\/data\n##https:\/\/www.kaggle.com\/artgor\/pytorch-approach\ndef sigmoid(x):\n    return 1 \/ (1 + np.exp(-x))\n\n\npuncts = [',', '.', '\"', ':', ')', '(', '-', '!', '?', '|', ';', \"'\", '$', '&', '\/', '[', ']', '>', '%', '=', '#', '*', '+', '\\\\', '\u2022',  '~', '@', '\u00a3',\n '\u00b7', '_', '{', '}', '\u00a9', '^', '\u00ae', '`',  '<', '\u2192', '\u00b0', '\u20ac', '\u2122', '\u203a',  '\u2665', '\u2190', '\u00d7', '\u00a7', '\u2033', '\u2032', '\u00c2', '\u2588', '\u00bd', '\u00e0', '\u2026', '\\n', '\\xa0', '\\t',\n '\u201c', '\u2605', '\u201d', '\u2013', '\u25cf', '\u00e2', '\u25ba', '\u2212', '\u00a2', '\u00b2', '\u00ac', '\u2591', '\u00b6', '\u2191', '\u00b1', '\u00bf', '\u25be', '\u2550', '\u00a6', '\u2551', '\u2015', '\u00a5', '\u2593', '\u2014', '\u2039', '\u2500', '\\u3000', '\\u202f',\n '\u2592', '\uff1a', '\u00bc', '\u2295', '\u25bc', '\u25aa', '\u2020', '\u25a0', '\u2019', '\u2580', '\u00a8', '\u2584', '\u266b', '\u2606', '\u00e9', '\u00af', '\u2666', '\u00a4', '\u25b2', '\u00e8', '\u00b8', '\u00be', '\u00c3', '\u22c5', '\u2018', '\u221e', '\u00ab',\n '\u2219', '\uff09', '\u2193', '\u3001', '\u2502', '\uff08', '\u00bb', '\uff0c', '\u266a', '\u2569', '\u255a', '\u00b3', '\u30fb', '\u2566', '\u2563', '\u2554', '\u2557', '\u25ac', '\u2764', '\u00ef', '\u00d8', '\u00b9', '\u2264', '\u2021', '\u221a', ]\nmispell_dict = {\"aren't\" : \"are not\",\n\"can't\" : \"cannot\",\n\"couldn't\" : \"could not\",\n\"couldn\" : \"could not\",\n\"couldnt\" : \"could not\",\n\"didn't\" : \"did not\",\n\"doesn't\" : \"does not\",\n\"doesnt\" : \"does not\",\n\"don't\" : \"do not\",\n\"hadn't\" : \"had not\",\n\"hasn't\" : \"has not\",\n\"haven't\" : \"have not\",\n\"havent\" : \"have not\",\n\"he'd\" : \"he would\",\n\"he'll\" : \"he will\",\n\"he's\" : \"he is\",\n\"i'd\" : \"I would\",\n\"i'd\" : \"I had\",\n\"i'll\" : \"I will\",\n\"i'm\" : \"I am\",\n\"isn't\" : \"is not\",\n\"it's\" : \"it is\",\n\"it'll\":\"it will\",\n\"i've\" : \"I have\",\n\"let's\" : \"let us\",\n\"mightn't\" : \"might not\",\n\"mustn't\" : \"must not\",\n\"shan't\" : \"shall not\",\n\"she'd\" : \"she would\",\n\"she'll\" : \"she will\",\n\"she's\" : \"she is\",\n\"shouldn't\" : \"should not\",\n\"shouldnt\" : \"should not\",\n\"that's\" : \"that is\",\n\"thats\" : \"that is\",\n\"there's\" : \"there is\",\n\"theres\" : \"there is\",\n\"they'd\" : \"they would\",\n\"they'll\" : \"they will\",\n\"they're\" : \"they are\",\n\"theyre\":  \"they are\",\n\"they've\" : \"they have\",\n\"we'd\" : \"we would\",\n\"we're\" : \"we are\",\n\"weren't\" : \"were not\",\n\"we've\" : \"we have\",\n\"what'll\" : \"what will\",\n\"what're\" : \"what are\",\n\"what's\" : \"what is\",\n\"what've\" : \"what have\",\n\"where's\" : \"where is\",\n\"who'd\" : \"who would\",\n\"who'll\" : \"who will\",\n\"who're\" : \"who are\",\n\"who's\" : \"who is\",\n\"who've\" : \"who have\",\n\"won't\" : \"will not\",\n\"wouldn't\" : \"would not\",\n\"you'd\" : \"you would\",\n\"you'll\" : \"you will\",\n\"you're\" : \"you are\",\n\"you've\" : \"you have\",\n\"'re\": \" are\",\n\"wasn't\": \"was not\",\n\"we'll\":\" will\",\n\"didn't\": \"did not\",\n\"tryin'\":\"trying\"}\n\n\ndef clean_text(x):\n    x = str(x)\n    for punct in puncts:\n        x = x.replace(punct, f' {punct} ')\n    return x\n\n\ndef _get_mispell(mispell_dict):\n    mispell_re = re.compile('(%s)' % '|'.join(mispell_dict.keys()))\n    return mispell_dict, mispell_re\n\n\ndef replace_typical_misspell(text):\n    mispellings, mispellings_re = _get_mispell(mispell_dict)\n\n    def replace(match):\n        return mispellings[match.group(0)]\n\n    return mispellings_re.sub(replace, text)\n\n","700c4023":"fake_news_Df['text']","23483c6a":"fake_news_Df['text']=fake_news_Df['text'].apply(clean_text)","3e2235c9":"fake_news_Df['text']=fake_news_Df['text'].apply(replace_typical_misspell)","cc64198c":"from nltk.corpus import words\ndef check_word_is_in_the_English_dictionary(text):\n    statements=''\n    statements =[word for word in text.split() if word in words.words()]\n    return ' '.join(statements)","846b08f6":"from sklearn.feature_extraction.text import CountVectorizer\ndef get_top_text_ngrams(corpus, n, g):\n    vec = CountVectorizer(ngram_range=(g, g)).fit(corpus)\n    bag_of_words = vec.transform(corpus)\n    sum_words = bag_of_words.sum(axis=0) \n    words_freq = [(word, sum_words[0, idx]) for word, idx in vec.vocabulary_.items()]\n    words_freq =sorted(words_freq, key = lambda x: x[1], reverse=True)\n    return words_freq[:n]","74f952eb":"fake_news_Df[fake_news_Df['label']==0].text","409b664e":"plt.figure(figsize = (16,9))\nmost_common_uni = get_top_text_ngrams(fake_news_Df[fake_news_Df['label']==0].text,10,1)\nmost_common_uni = dict(most_common_uni)\nsns.barplot(x=list(most_common_uni.values()),y=list(most_common_uni.keys()))  ","06fd53b3":"plt.figure(figsize = (16,9))\nmost_common_uni = get_top_text_ngrams(fake_news_Df[fake_news_Df['label']==1].text,10,1)\nmost_common_uni = dict(most_common_uni)\nsns.barplot(x=list(most_common_uni.values()),y=list(most_common_uni.keys()))","11002989":"import re\n\ndef tokenize(text):\n    # you may want to use a smarter tokenizer\n    for match in re.finditer(r'\\w+', text, re.UNICODE):\n        yield match.group(0)","d7231e15":"corpus = []\nwords = []\n\nfor i in range(0,len(fake_news_Df)):\n    review = re.sub('[^a-zA-Z0-9]',' ',fake_news_Df['text'][i])\n    review = review.lower()\n    review = review.split() \n    review = list(category for token in review for category in parse(token))\n    statements = ' '.join(review)\n    corpus.append(statements)\n    words.append(review)","c6b26ebb":"from sklearn.feature_extraction.text import TfidfVectorizer\nvectorizer=TfidfVectorizer(ngram_range=(1, 2),max_df = 0.99,max_features=93)\nX_fit=vectorizer.fit(corpus)","c3f77884":"X_fit","87d9d305":"\nX_transformed=X_fit.transform(corpus)","0de0d2c0":"features = vectorizer.get_feature_names()\ndf_count = pd.DataFrame(X_transformed.toarray(),columns = features)\ndf_count","782e8e48":"# demonstrate data standardization with sklearn\nfrom sklearn.preprocessing import StandardScaler\n# load data\ndata = ...\n# create scaler\nscaler = StandardScaler()\n# fit and transform in one step\nstandardized = scaler.fit_transform(df_count)","7a172233":"x=standardized","8488d714":"y=fake_news_Df['label']","1b318281":"counter = Counter(y)\nprint(counter)","30b52323":"X_train, X_test, y_train, y_test = train_test_split(x, y,  test_size=0.30,random_state=56)","c0db78a0":"from sklearn.ensemble import AdaBoostClassifier\nfrom sklearn.ensemble import BaggingClassifier\nfrom sklearn.ensemble import RandomForestClassifier, VotingClassifier\nfrom sklearn.svm import LinearSVC\nfrom sklearn.linear_model import PassiveAggressiveClassifier\nfrom xgboost import XGBClassifier\ndef get_models():\n    models = dict()\n    models['lr'] = LogisticRegression(solver='liblinear')\n    models['knn'] = KNeighborsClassifier()\n    models['svm'] = LinearSVC()\n    models['NN']=MLPClassifier()\n    models['RF']=RandomForestClassifier()\n    clf1 =LogisticRegression(solver='liblinear')\n    clf2 = RandomForestClassifier()\n    clf3 = KNeighborsClassifier()\n    clf4= LinearSVC()\n    clf5=DecisionTreeClassifier(criterion='entropy',random_state=150,splitter='random')\n    models['VC1']=VotingClassifier(estimators=[('lr', clf1), ('rf', clf2), ('knn', clf3)], weights=[2,1,1],flatten_transform=True)\n    models['VC2']=VotingClassifier(estimators=[('lr', clf1), ('svm', clf4), ('cart', clf5)], weights=[2,1,1],flatten_transform=True)\n    models['BG']=BaggingClassifier(base_estimator=clf5,n_estimators=10, random_state=0)#\n    models['BG1']=BaggingClassifier(base_estimator=clf5,n_estimators=10, random_state=0)\n    models['Ada']=AdaBoostClassifier(algorithm='SAMME.R', base_estimator=None, learning_rate=1,n_estimators=100, random_state=None)\n    models['XGB']=XGBClassifier()\n    return models","5cbef514":"from sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import precision_score, recall_score, f1_score\n\ndef eval_confusion(y_pred, y_true):\n    conf_matrix = confusion_matrix(y_true, y_pred)\n    precision = precision_score(y_true, y_pred)\n    recall = recall_score(y_true, y_pred)\n    f1 = f1_score(y_true, y_pred)\n    return {'matrix': conf_matrix, 'precision': precision, 'recall': recall, 'f1': f1}","1071ae97":"# evaluate each model on the training set\nfor clf_name, clf in get_models().items():\n    clf.fit(X_train, y_train)\n    y_pred = clf.predict(X_test)\n    conf = eval_confusion(y_pred, y_test)\n    print(\"{}:\".format(clf_name))\n    print(conf['matrix'])\n    print(\"precision: {}\".format(conf['precision']))\n    print(\"recall: {}\".format(conf['recall']))\n    print(\"f1-score: {}\".format(conf['f1']))\n    print(\"accuracy: {}\".format(accuracy_score(y_pred, y_test)))\n    print()","5b3b8339":"oversample = SMOTE(random_state = 101)\nx, y = oversample.fit_resample(x, y)","3e243a24":"X_train, X_test, y_train, y_test = train_test_split(x, y,  test_size=0.30,random_state=56)","5905ee51":"# evaluate each model on the training set\n\nfor clf_name, clf in get_models().items():\n    clf.fit(X_train, y_train)\n    y_pred = clf.predict(X_test)\n    conf = eval_confusion(y_pred, y_test)\n    print(\"{}:\".format(clf_name))\n    print(conf['matrix'])\n    print(\"precision: {}\".format(conf['precision']))\n    print(\"recall: {}\".format(conf['recall']))\n    print(\"f1-score: {}\".format(conf['f1']))\n    print(\"accuracy: {}\".format(accuracy_score(y_pred, y_test)))\n    print()\n","2374a5e8":"# I changed this part\n!pip install mlxtend\nimport joblib\nimport sys\nsys.modules['sklearn.externals.joblib'] = joblib\nfrom mlxtend.feature_selection import SequentialFeatureSelector as SFS","0438aa47":"#oversample = SMOTE(random_state = 101)\n#x, y = oversample.fit_resample(df_count,fake_news_Df['label'])","31ddbf30":"X_train, X_test, y_train, y_test = train_test_split(df_count, y, test_size=0.30,random_state=56)","4c10b45f":"X_train","f8ef134a":"\ndef correlation(dataset, threshold):\n    col_corr = set()  # Set of all the names of correlated columns\n    corr_matrix = dataset.corr()\n    for i in range(len(corr_matrix.columns)):\n        for j in range(i):\n            if abs(corr_matrix.iloc[i, j]) > threshold: # we are interested in absolute coeff value\n                colname = corr_matrix.columns[i]  # getting the name of column\n                col_corr.add(colname)\n    return col_corr\n\ncorr_features = correlation(X_train, 0.8)\nprint('correlated features: ', len(set(corr_features)) )","d4db95b9":"\n# removed correlated  features\nX_train.drop(labels=corr_features, axis=1, inplace=True)\nX_test.drop(labels=corr_features, axis=1, inplace=True)\n\nX_train.shape, X_test.shape\n","3710dfb9":"X_train.fillna(0, inplace=True)","5779e0aa":"\n#step forward feature selection\nfrom sklearn.ensemble import RandomForestRegressor\nfrom mlxtend.feature_selection import SequentialFeatureSelector as SFS\nsfs1 = SFS(LogisticRegression(solver='liblinear'), \n           k_features=50, \n           forward=True, \n           floating=False, \n           verbose=2,\n           scoring='r2',\n           cv=3)\nsfs1 = sfs1.fit(np.array(X_train), y_train)","b8344b1a":"sfs1.k_feature_idx_","e7d8acd2":"X_train.columns[list(sfs1.k_feature_idx_)]","061b15e4":"# step backward feature elimination\n'''\nsfs1 = SFS(LogisticRegression(solver='liblinear'), \n           k_features=93, \n           forward=False, \n           floating=False, \n           verbose=2,\n           scoring='r2',\n           cv=3)\n\nsfs1 = sfs1.fit(np.array(X_train), y_train)\n'''","42c11607":"#sfs1.k_feature_idx_","c72efd4f":"#X_train.columns[list(sfs1.k_feature_idx_)]","629a9b2a":"\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.svm import LinearSVC\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import RandomizedSearchCV\nfrom sklearn.model_selection import GridSearchCV\nSVCpipe = LinearSVC()\n\n# Gridsearch to determine the value of C\nparam_grid = {'C':np.arange(0.01,100,10)}\n\nlsvm_randomized_search = RandomizedSearchCV(SVCpipe, param_grid, n_iter=5, scoring='f1', cv=5, verbose=2, random_state=42)\nlsvm_randomized_search.fit(X_train, y_train)\n","8d1448ad":"lsvm_randomized_search.best_estimator_\nLinearSVC(C=80.01)\n","cd2d23d1":"from sklearn.model_selection import GridSearchCV\n\n# fine-tune MLP classifier\nmlp_best = MLPClassifier()\n\nparameters = {'solver': ['lbfgs'], 'max_iter': [1000,1100,1200,1300,1400,1500,1600,1700,1800,1900,2000 ], 'alpha': 10.0 ** -np.arange(1, 10), 'hidden_layer_sizes':np.arange(10, 15), 'random_state':[0,1,2,3,4,5,6,7,8,9]}\n\n\nmlp_grid_search = RandomizedSearchCV(mlp_best, parameters,  n_iter = 100, cv = 5, verbose=2, random_state=35, n_jobs = -1)\nmlp_grid_search.fit(X_train, y_train)","6e2bcf66":"from sklearn.model_selection import RandomizedSearchCV\nimport scipy.stats\n\nlogistic_best = LogisticRegression()\nlogistic_param_dist = {\n    'penalty': ['l1', 'l2', 'elasticnet', 'none'],\n    'C': scipy.stats.reciprocal(20, 1000),\n    'solver' : ['newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga']\n}\n\nlogistic_randomized_search = RandomizedSearchCV(logistic_best, logistic_param_dist, n_iter=5, scoring='f1',\n                                          cv=5, verbose=2, random_state=42)\nlogistic_randomized_search.fit(X_train, y_train)","e50e619d":"#List Hyperparameters that we want to tune.\n\nrandom_param_dist = {\n    'leaf_size': list(range(1,50)),\n    'n_neighbors': list(range(1,30)),\n    'p':[1,2],\n}\n\nknn_best = KNeighborsClassifier()\nknn_f = RandomizedSearchCV(knn_best, random_param_dist, n_iter=5, scoring='f1',\n                                           cv=5, verbose=2, random_state=42)\nknn_f.fit(X_train, y_train)","14813132":"n_estimators = [int(x) for x in np.linspace(start = 1, stop = 20, num = 20)] # number of trees in the random forest\nmax_features = ['auto', 'sqrt'] # number of features in consideration at every split\nmax_depth = [int(x) for x in np.linspace(10, 120, num = 12)] # maximum number of levels allowed in each decision tree\nmin_samples_split = [2, 6, 10] # minimum sample number to split a node\nmin_samples_leaf = [1, 3, 4] # minimum sample number that can be stored in a leaf node\nbootstrap = [True, False] # method used to sample data points\n\nrandom_grid = {'n_estimators': n_estimators,\n\n'max_features': max_features,\n\n'max_depth': max_depth,\n\n'min_samples_split': min_samples_split,\n\n'min_samples_leaf': min_samples_leaf,\n\n'bootstrap': bootstrap}\n\nRandomForest_best = RandomForestClassifier()\nRF_f = RandomizedSearchCV(RandomForest_best, random_grid, n_iter=5, scoring='f1',\n                                           cv=5, verbose=2, random_state=42)\nRF_f.fit(X_train, y_train)","3cf82d55":"knn_f.best_estimator_","fab5d404":"lsvm_randomized_search.best_estimator_","cece31aa":"mlp_grid_search.best_estimator_","840f40e2":"logistic_randomized_search.best_estimator_","79b6dcf8":"RF_f.best_estimator_","33bd5911":"import joblib\nimport os\ndef save_model(model, filename):\n    # create directory if it doesn't exist\n    if not os.path.isdir(MODELS_DIR):\n        os.makedirs(MODELS_DIR)\n        \n    path = os.path.join(MODELS_DIR, filename)\n    joblib.dump(model, path)","5483e1cf":"# get the best estimators\nmlp_best = MLPClassifier(activation='tanh', alpha=0.0001, batch_size='auto', beta_1=0.9,beta_2=0.999, early_stopping=False, epsilon=1e-08,hidden_layer_sizes=(64, 32, 16), learning_rate='constant',learning_rate_init=0.001, max_fun=15000, max_iter=200,momentum=0.9, n_iter_no_change=10, nesterovs_momentum=True,power_t=0.5, random_state=None, shuffle=True, solver='adam',tol=0.0001, validation_fraction=0.1, verbose=False,warm_start=False)\nknn_best = knn_f.best_estimator_#KNeighborsClassifier(leaf_size=29, n_neighbors=8, p=1)\nlogistic_best = logistic_randomized_search.best_estimator_#LogisticRegression(C=86.56900442587761, class_weight=None, dual=False,fit_intercept=True, intercept_scaling=1, l1_ratio=None,max_iter=100, multi_class='auto', n_jobs=None, penalty='l1',random_state=None, solver='liblinear', tol=0.0001, verbose=0,warm_start=False)\nrandom_best = RF_f.best_estimator_\nsvm_best=lsvm_randomized_search.best_estimator_\nMODELS_DIR = 'models'\n# save each model\nsave_model(mlp_best, 'mlp_best.pkl')\nsave_model(knn_best, 'knn_best.pkl')\nsave_model(logistic_best, 'logistic_best.pkl')\nsave_model(random_best, 'random_best.pkl')\nsave_model(svm_best, 'svm_best.pkl')","1d903fd8":"# evaluate each model on the training set\n# get a list of models to evaluate\ndef get_models():\n    models = dict()\n    models['lr'] = logistic_best\n    models['knn'] = knn_best\n    models['svm'] = svm_best\n    models['NN']=mlp_best\n    models['RF']=random_best\n    return models\nfor clf_name, clf in get_models().items():\n    clf.fit(X_train, y_train)\n    y_pred = clf.predict(X_test)\n    conf = eval_confusion(y_pred, y_test)\n    print(\"{}:\".format(clf_name))\n    print(conf['matrix'])\n    print(\"precision: {}\".format(conf['precision']))\n    print(\"recall: {}\".format(conf['recall']))\n    print(\"f1-score: {}\".format(conf['f1']))\n    print(\"accuracy: {}\".format(accuracy_score(y_pred, y_test)))\n    print()\n    print()","32f39366":"X_train, X_test, y_train, y_test = train_test_split(df_count[['achiev', 'achiev power', 'adj affect', 'adj compare', 'adverb',\n       'adverb cogproc', 'affect', 'affect negemo', 'affect posemo', 'anger',\n       'auxverb', 'bio', 'bio health', 'certain', 'cogproc', 'cogproc differ',\n       'cogproc discrep', 'cogproc insight', 'cogproc tentat', 'drives',\n       'drives reward', 'focusfuture', 'focuspast', 'focuspast relativ',\n       'focuspresent', 'focuspresent relativ', 'function', 'function prep',\n       'hear', 'home', 'leisure', 'money', 'number', 'percept', 'quant',\n       'relativ', 'relig', 'social drives', 'social percept', 'space relativ',\n       'time relativ', 'time verb', 'verb', 'verb affect', 'verb cogproc',\n       'verb drives', 'verb focuspast', 'verb focuspresent', 'verb social',\n       'work drives']], y, test_size=0.30,random_state=56)","4abee380":"# evaluate each model on the training set\n# get a list of models to evaluate\ndef get_models():\n    models = dict()\n    models['lr'] = logistic_best\n    models['knn'] = knn_best\n    models['svm'] = svm_best\n    models['NN']=mlp_best\n    models['RF']=random_best\n    return models\nfor clf_name, clf in get_models().items():\n    clf.fit(X_train, y_train)\n    y_pred = clf.predict(X_test)\n    conf = eval_confusion(y_pred, y_test)\n    print(\"{}:\".format(clf_name))\n    print(conf['matrix'])\n    print(\"precision: {}\".format(conf['precision']))\n    print(\"recall: {}\".format(conf['recall']))\n    print(\"f1-score: {}\".format(conf['f1']))\n    print(\"accuracy: {}\".format(accuracy_score(y_pred, y_test)))\n    print()\n    print()","538280ad":"from tensorflow.keras.layers import Embedding\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom tensorflow.keras.preprocessing.text import one_hot\nfrom tensorflow.keras.layers import LSTM, Dense\nfrom tensorflow.keras.layers import Embedding,LSTM,Dense,Dropout\nfrom tensorflow.keras.preprocessing import text, sequence\nfrom tensorflow.keras.callbacks import ReduceLROnPlateau\nfrom tensorflow.keras.optimizers import Adam","b8b8e513":"x_train,x_test,y_train,y_test = train_test_split(df_count,y,random_state = 0)\nmax_features = 10000\nmaxlen = 300","afb0f84d":"tokenizer = text.Tokenizer(num_words=max_features)\ntokenizer.fit_on_texts(x_train)\ntokenized_train = tokenizer.texts_to_sequences(x_train)\nx_train = sequence.pad_sequences(tokenized_train, maxlen=maxlen)","fbd33fe5":"tokenized_test = tokenizer.texts_to_sequences(x_test)\nX_test = sequence.pad_sequences(tokenized_test, maxlen=maxlen)","10e23752":"EMBEDDING_FILE = r'..\/input\/glove6b300dtxt\/glove.6B.300d.txt'\ndef get_coefs(word, *arr): \n    return word, np.asarray(arr, dtype='float32')\nembeddings_index = dict(get_coefs(*o.rstrip().rsplit(' ')) for o in open(EMBEDDING_FILE))","8c18518d":"all_embs = np.stack(embeddings_index.values())\nemb_mean,emb_std = all_embs.mean(), all_embs.std()\nembed_size = all_embs.shape[1]\n\nword_index = tokenizer.word_index\nnb_words = min(max_features, len(word_index))\n#change below line if computing normal stats is too slow\nembedding_matrix = embedding_matrix = np.random.normal(emb_mean, emb_std, (nb_words, embed_size))\nfor word, i in word_index.items():\n    if i >= max_features: continue\n    embedding_vector = embeddings_index.get(word)\n    if embedding_vector is not None: embedding_matrix[i] = embedding_vector","4ebcf2b0":"batch_size = 256\nepochs = 10\nembed_size = 100","8e3c71dc":"learning_rate_reduction = ReduceLROnPlateau(monitor='val_accuracy', patience = 2, verbose=1,factor=0.5, min_lr=0.00001)","ed7abece":"#Defining Neural Network\nmodel = Sequential()\n#Non-trainable embeddidng layer\nmodel.add(Embedding(max_features, output_dim=embed_size, input_length=maxlen, trainable=False))\n#LSTM \nmodel.add(LSTM(units=128 , return_sequences = True , recurrent_dropout = 0.25 , dropout = 0.25))\nmodel.add(LSTM(units=64 , recurrent_dropout = 0.1 , dropout = 0.1))\nmodel.add(Dense(units = 32 , activation = 'relu'))\nmodel.add(Dense(1, activation='sigmoid'))\nmodel.compile(optimizer=Adam(lr = 0.01), loss='binary_crossentropy', metrics=['accuracy'])","6dea93c1":"model.summary()","44d05880":"y_test","8614b468":"history = model.fit(x_train, y_train, batch_size = batch_size , validation_data = (X_test,y_test) , epochs = epochs , callbacks = [learning_rate_reduction])","6a04f8dd":"# one hot representation of words\noh_repr = [one_hot(words, 5000) for words in corpus]","4f2aa046":"# taking sentences length as 400\nsent_length = 1000\n# padding\nembedded_doc = pad_sequences(df_count.values, padding='pre', maxlen=sent_length)\nembedded_doc[:5]","0ab7997d":"# initializing model\nmodel = Sequential()\n# adding embedding layer\nmodel.add(Embedding(5000, 100,input_length=sent_length))\nmodel.add(LSTM(units=128 , return_sequences = True , recurrent_dropout = 0.25 , dropout = 0.25))\nmodel.add(LSTM(units=64 , recurrent_dropout = 0.1 , dropout = 0.1))\nmodel.add(Dense(units = 32 , activation = 'relu'))\nmodel.add(Dense(1, activation='sigmoid'))\nmodel.compile(optimizer=Adam(lr = 0.01), loss='binary_crossentropy', metrics=['accuracy'])\nmodel.summary()","370b33cf":"from sklearn.model_selection import train_test_split\nx = np.array(embedded_doc)\ny = fake_news_Df['label']\n\n# splitting the dataset into train and test\ntrain_x, test_x, train_y, test_y = train_test_split(x, y, \n                                                    test_size=0.30, random_state=56)","618664e6":"# fitting the model\nhistory = model.fit(train_x, train_y, validation_data=(test_x, test_y), epochs=20, batch_size=64)","25b42f10":"def plot_loss_epochs(history):\n    epochs = np.arange(1,len(history.history['accuracy']) + 1,1)\n    train_acc = history.history['accuracy']\n    train_loss = history.history['loss']\n    val_acc = history.history['val_accuracy']\n    val_loss = history.history['val_loss']\n\n    fig , ax = plt.subplots(1,2, figsize=(7,3))\n    ax[0].plot(epochs , train_acc , '.-' , label = 'Train Accuracy')\n    ax[0].plot(epochs , val_acc , '.-' , label = 'Validation Accuracy')\n    ax[0].set_title('Train & Validation Accuracy')\n    ax[0].legend()\n    ax[0].set_xlabel(\"Epochs\")\n    ax[0].set_ylabel(\"Accuracy\")\n\n    ax[1].plot(epochs , train_loss , '.-' , label = 'Train Loss')\n    ax[1].plot(epochs , val_loss , '.-' , label = 'Validation Loss')\n    ax[1].set_title('Train & Validation Loss')\n    ax[1].legend()\n    ax[1].set_xlabel(\"Epochs\")\n    ax[1].set_ylabel(\"Loss\")\n    fig.tight_layout()\n    fig.show()\n    \nplot_loss_epochs(history)","5312672e":"model.save(\"fakenews_new_full_verison.h5\")","ed776bdd":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\nimport matplotlib.pyplot as plt #  plotting and data visualization\nimport seaborn as sns # improve visuals\nsns.set() # Set as default style\n\nimport string # python library\nimport re # regex library\n\nfrom gensim.parsing.preprocessing import preprocess_string, strip_tags, strip_punctuation, strip_multiple_whitespaces, strip_numeric, remove_stopwords, strip_short # Preprocesssing\nfrom gensim.models import Word2Vec # Word2vec\n\nfrom sklearn import cluster # Kmeans clustering\nfrom sklearn import metrics # Metrics for evaluation\nfrom sklearn.decomposition import PCA #PCA\nfrom sklearn.manifold import TSNE #TSNE","47fb520f":"#https:\/\/www.kaggle.com\/nasirkhalid24\/unsupervised-k-means-clustering-fake-news-87\/notebook","b4839326":"# Training for 2 clusters (Fake and Real)\nkmeans = cluster.KMeans(n_clusters=2, verbose=1,init='k-means++')\n\n# Fit predict will return labels\nclustered = kmeans.fit_predict(X_transformed)","7ea99f51":"testing_df = {'Sentence': fake_news_Df['text'], 'Labels': fake_news_Df['label'], 'Prediction': clustered}\ntesting_df = pd.DataFrame(data=testing_df)\n\ntesting_df.head(10)","7988b937":"correct = 0\nincorrect = 0\nfor index, row in testing_df.iterrows():\n    if row['Labels'] == row['Prediction']:\n        correct += 1\n    else:\n        incorrect += 1\n        \nprint(\"Correctly clustered news: \" + str((correct*100)\/(correct+incorrect)) + \"%\")","807050b1":"# PCA of sentence vectors\n#pca = TruncatedSVD(n_components=2)\n#pca_result = pca.fit_transform(x)\n\n#PCA_df = pd.DataFrame(pca_result)\n#PCA_df['cluster'] = clustered\n#PCA_df.columns = ['x1','x2','cluster']","def3c6ea":"# T-SNE\n#tsne = TSNE(n_components=2)\n#tsne_result = tsne.fit_transform(pca_result)\n\n#TSNE_df = pd.DataFrame(tsne_result)\n#TSNE_df['cluster'] = clustered\n#TSNE_df.columns = ['x1','x2','cluster']","051fdc9e":"#TSNE_df","4040e84e":"# Plots\n#fig, ax = plt.subplots(1, 2, figsize=(12,6))\n#sns.scatterplot(data=PCA_df,x='x1',y='x2',hue='cluster',legend=\"full\",alpha=0.5,ax=ax[1])\n#sns.scatterplot(data=TSNE_df,x='x1',y='x2',hue='cluster',legend=\"full\",alpha=0.5,ax=ax[0])\n#ax[0].set_title('Visualized on TSNE')\n#ax[1].set_title('Visualized on PCA')","da2c282d":"# Modeling step Test differents algorithms\nfrom sklearn.model_selection import KFold, cross_val_score\nkfolds = KFold(n_splits=6, shuffle=True)\nclassifiers = []\nclassifiers.append(logistic_best)\nclassifiers.append(knn_best)\nclassifiers.append(svm_best)\nclassifiers.append(mlp_best)\nclassifiers.append(random_best)\n\ncv_results = []\nfor classifier in classifiers :\n    cv_results.append(cross_val_score(classifier,x, y , scoring = \"accuracy\", cv=kfolds,n_jobs= 4))\n\ncv_means = []\ncv_std = []\nfor cv_result in cv_results:\n    cv_means.append(cv_result.mean())\n    cv_std.append(cv_result.std())\n\ncv_res = pd.DataFrame({\"CrossValMeans\":cv_means,\"CrossValerrors\": cv_std,\"Algorithm\":[\"LogisticRegression\",\"KNeighborsClassifier\",\"LinearSVC\",\"MLPClassifier\",\"RandomForestClassifier \"]})\n\ng = sns.barplot(\"CrossValMeans\",\"Algorithm\",data = cv_res, palette=\"Set3\",orient = \"h\",**{'xerr':cv_std})\ng.set_xlabel(\"Mean Accuracy\")\ng = g.set_title(\"Cross validation scores\")","7b84c675":"import numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\n\nfrom sklearn import datasets\n\nfrom sklearn.decomposition import (PCA, IncrementalPCA,\n                                   KernelPCA, TruncatedSVD,\n                                   FastICA, MiniBatchDictionaryLearning,\n                                   SparsePCA)\n\nfrom sklearn.manifold import (Isomap,\n                              LocallyLinearEmbedding)\n\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n\nfrom sklearn.random_projection import (GaussianRandomProjection,\n                                       SparseRandomProjection)\n\nfrom sklearn.neighbors import (KNeighborsClassifier,\n                               NeighborhoodComponentsAnalysis)\n                               \nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split\n","1218aafe":"from sklearn.model_selection import cross_val_score\nfrom numpy import mean\nfrom sklearn.pipeline import Pipeline\n\nresultts=[]\nn_components =[i for i in range(1,x.shape[1])]\nfor i in n_components:\n    steps=[('SVD',TruncatedSVD(n_components=i)),('rf',random_best)]\n    print(i)\n    model=Pipeline(steps=steps)\n    n_scores=cross_val_score(model,x,y,scoring='f1',cv=5)\n    resultts.append(mean(n_scores))","d2031c9c":"n_components_=resultts.index(max(resultts))","4abecd34":"X_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.30)","7915ef9d":"'''\ntsvd = TruncatedSVD(n_components=59)\nX_train = tsvd.fit_transform(X_train)\nX_test = tsvd.transform(X_test)\n'''","e759f422":"'''\ndef get_models():\n    models = dict()\n    models['knn'] = KNeighborsClassifier(leaf_size=29, n_neighbors=8, p=1)\n    return models\n'''","c1553ae2":"'''\nfor clf_name, clf in get_models().items():\n    clf.fit(X_train, y_train)\n    y_pred = clf.predict(X_test)\n    conf = eval_confusion(y_pred, y_test)\n    print(\"{}:\".format(clf_name))\n    print(conf['matrix'])\n    print(\"precision: {}\".format(conf['precision']))\n    print(\"recall: {}\".format(conf['recall']))\n    print(\"f1-score: {}\".format(conf['f1']))\n    print(\"accuracy: {}\".format(accuracy_score(y_pred, y_test)))\n    print()\n'''","ee9695ef":"df_test=pd.read_csv(r'..\/input\/dataset2\/test.csv')\ndf_test=df_test.drop(['id','title','author'],axis=1)","fc24760f":"df_test['text']","f66baef1":"df_test['text']=df_test['text'].astype(str)","c004e923":"vectorizer=TfidfVectorizer(ngram_range=(1, 2),max_df = 0.99)\ndef clean_test_data(df):\n    df['text'] = df.text.apply(lambda x : wordopt(x))\n    \ndef make_corpus_from_text(df):\n    corpus = []\n    words = []\n    for i in df.text:\n        gettysburg_tokens = tokenize(i.lower())\n        review = [category for token in gettysburg_tokens for category in parse(token)]\n        statements = ' '.join(review)\n        corpus.append(statements)\n    return corpus\ndef vectorizer_text(corpus):\n    X_fit=vectorizer.fit(corpus)\n    X_transformed=X_fit.transform(corpus)\n    return X_transformed\ndef visual_transform_data(X_transformed):\n    features = vectorizer.get_feature_names()\n    df_count = pd.DataFrame(X_transformed.toarray(),columns = features)\n    return  df_count","a170c900":"clean_test_data(df_test)","441b3558":"df_test['text']","29ae96f8":"corpus=make_corpus_from_text(df_test)","03390f5c":"X_transformed=vectorizer_text(corpus)","ca0f2f77":"df=visual_transform_data(X_transformed)","69b23942":"df=df[['adverb function', 'affect negemo', 'affiliation focuspresent',\n       'article percept', 'bio relativ', 'cause drives', 'cause work',\n       'female verb', 'focuspast work', 'focuspresent death',\n       'focuspresent percept', 'hear', 'hear focuspresent', 'informal',\n       'interrog verb', 'ipron function', 'ipron social', 'leisure informal',\n       'male drives', 'money drives', 'money percept', 'netspeak drives',\n       'percept see', 'power death', 'power relig', 'power verb',\n       'prep compare', 'sexual informal', 'social informal', 'space relativ',\n       'work percept', 'you informal']]","4db9e9f0":"df","d312f949":"filename = r'.\/models\/random_best.pkl'\n\nX= X_transformed\n\n# load the model from disk\nloaded_model = joblib.load(filename)\n\nresult = loaded_model.predict(df)","a6b13908":"df_99_test=pd.read_csv(r'..\/input\/lables\/final_sub.csv')\ndf_99_test=df_99_test.drop('id',axis=1)","34705ec1":"df_final=pd.DataFrame(list(result),list(df_99_test['label']),columns = ['gecek'])","597f6cb7":"correct = 0\nincorrect = 0\nfor index, row in df_final.iterrows():\n    if index == row['gecek']:\n        correct += 1\n    else:\n        incorrect += 1\n        \nprint(\"Correctly clustered news: \" + str((correct*100)\/(correct+incorrect)) + \"%\")","122bebd5":"correct = 0\nincorrect = 0\nfor index, row in testing_df.iterrows():\n    if row['Labels'] == row['Prediction']:\n        correct += 1\n    else:\n        incorrect += 1\n        \nprint(\"Correctly clustered news: \" + str((correct*100)\/(correct+incorrect)) + \"%\")","213b6c51":"#https:\/\/www.kaggle.com\/klmsathishkumar\/validate-the-news-here-lstm-90-accuracy\n#https:\/\/www.kaggle.com\/snanilim\/100-accuracy-is-this-title-fake-or-real\/notebook#N-Gram-Analysis\n#https:\/\/www.kaggle.com\/atishadhikari\/fake-news-cleaning-word2vec-lstm-99-accuracy","96a64753":"#https:\/\/www.kaggle.com\/quentinfu\/word2vec-and-lstm-98-accuracy\n#https:\/\/www.kaggle.com\/esraamohamedahmed\/fake-news-detection\n#https:\/\/www.kaggle.com\/ashishkumarbehera\/fake-news-classifier    \n#https:\/\/www.kaggle.com\/nileshsuryavanshi\/fake-news-detection-using-lstm","1a8b071e":"### Stopword Removal","266b6c52":"1. https:\/\/www.kaggle.com\/c\/fake-news\n2. https:\/\/www.kaggle.com\/jruvika\/fake-news-detection\n3. https:\/\/www.kaggle.com\/clmentbisaillon\/fake-and-real-news-dataset","c0b781e8":"#### dataset links\n","4fcf230f":"# Wrapper Methods","15459d89":"# cross validation ","5b82bbdc":"# Test my model","9e2c8feb":"## Using LIWC dictionary to extract features","a6dc90ba":"# Correcting Words","55e443b1":"# Read LIWC dictionary ","d2a37e6e":"# hyperparameter","d1dda694":"Ref:https:\/\/www.google.com.tr\/url?sa=i&url=https%3A%2F%2Fwww.datacamp.com%2Fcommunity%2Ftutorials%2Fstemming-lemmatization-python&psig=AOvVaw0MT_p4g76LhbGwtIbyPBve&ust=1640183270141000&source=images&cd=vfe&ved=0CAsQjRxqFwoTCJi7qPyM9fQCFQAAAAAdAAAAABAD","0b7fb462":"# balance data","a851d124":"## Lemmatization","0156304f":"![image.png](attachment:13697e45-a8a9-4398-8a59-c521d54e6653.png)","d2b95a68":"### The lexicon data can be acquired (purchased) from liwc.net.\n\n","bb5c81d8":"# read csv ","d25b03e7":"### t-da\u011f\u0131t\u0131lm\u0131\u015f stokastik kom\u015fu g\u00f6mme, her veri noktas\u0131na iki veya \u00fc\u00e7 boyutlu bir harita i\u00e7inde bir konum vererek y\u00fcksek boyutlu verileri g\u00f6rselle\u015ftirmek i\u00e7in istatistiksel bir y\u00f6ntemdir.","e31d5464":"# After featurs selection","6bf5cc6a":"# LSTM","71523bd3":"# Split data to train and test","4e333096":"### t-da\u011f\u0131t\u0131lm\u0131\u015f stokastik kom\u015fu g\u00f6mme, her veri noktas\u0131na iki veya \u00fc\u00e7 boyutlu bir harita i\u00e7inde bir konum vererek y\u00fcksek boyutlu verileri g\u00f6rselle\u015ftirmek i\u00e7in istatistiksel bir y\u00f6ntemdir.","9ef37a05":"## Lemmatization: Lemmatization kelimeleri morfolojik olarak inceler. Bir \u00f6rnek olarak: \u201cGidiyorlar\u201d gitmek fiilinin \u00fc\u00e7\u00fcnc\u00fc \u00e7o\u011ful \u015fahs\u0131n\u0131n geni\u015f zamanda \u00e7ekiminden olu\u015fur. Burada kelimenin \u00e7ekimlenmemi\u015f ilk haline lemma denir, bu \u00f6rnekte gitmek bir lemmad\u0131r. Lemmatization algoritmalar\u0131 \u00e7al\u0131\u015fmak i\u00e7in bir s\u00f6zl\u00fc\u011fe ihtiya\u00e7 duyar. Ayn\u0131 \u015fekilde ingilizcede bir \u00f6rnek verirsek \u201cFeeds\u201d, feed fiilinin \u00fc\u00e7\u00fcnc\u00fc tekil \u015fahs\u0131n\u0131n geni\u015f zamanda \u00e7ekimlenmi\u015f halidir.\n## NLTK k\u00fct\u00fcphanesinde WordNetLemmatizer\u2019la kelimelerin lemma\u2019lar\u0131 bulunabilir.","ec008d11":"# processing data","e978bd0c":"Ref: https:\/\/mcrc.journalism.wisc.edu\/files\/2018\/04\/Manual_LIWC.pdf","6ddad884":"Ref:https:\/\/www.google.com.tr\/url?sa=i&url=https%3A%2F%2Fwww.geeksforgeeks.org%2Fremoving-stop-words-nltk-python%2F&psig=AOvVaw2BVMU7sjLZuP8gjAVe52T9&ust=1640182574752000&source=images&cd=vfe&ved=0CAwQjhxqFwoTCKiB_q-K9fQCFQAAAAAdAAAAABAD","324d6ab4":"# Unsupervised - K Means Clustering\n\n","a11ef743":"## TfidfVectorizer method ","d854dbcf":"# PCA","9a8e95af":"# check the word","c99b6896":"https:\/\/www.kaggle.com\/prashant111\/comprehensive-guide-on-feature-selection","3d73142d":"RF:https:\/\/ichi.pro\/tr\/tf-idf-vectorizer-scikit-learn-241618289070362","d88347c2":"#### BaggingClassifier\n### Bagging meta-estimator, hem s\u0131n\u0131fland\u0131rma (BaggingClassifier) hem de regresyon (BaggingRegressor) problemleri i\u00e7in kullan\u0131labilen bir algoritmad\u0131r. Tahmin yapmak i\u00e7in tipik torbalama tekni\u011fini izler. Torbalama meta-estimator algoritmas\u0131 i\u00e7in ad\u0131mlar \u015funlard\u0131r: Rastgele alt k\u00fcmeleri orijinal veri k\u00fcmesinden olu\u015fturulur.","d57bf85a":"# classifier ","3c5e4581":"Ref:https:\/\/www.google.com.tr\/url?sa=i&url=https%3A%2F%2Fwww.extendoffice.com%2Fdocuments%2Fexcel%2F3296-excel-remove-all-punctuation.html&psig=AOvVaw3MbjzjUgx2zaamqjDQHxhd&ust=1640183372787000&source=images&cd=vfe&ved=0CAwQjhxqFwoTCPCB5KyN9fQCFQAAAAAdAAAAABAD"}}