{"cell_type":{"96aa1908":"code","eda2e039":"code","fb3aa9e6":"code","91763691":"code","8e7e7442":"code","75b49041":"code","f9740c71":"code","1453a84b":"code","924eb48c":"code","a49922c1":"code","fa0b6dd1":"code","d3d20cb5":"code","8890f712":"code","7d18a4d6":"code","134707ed":"code","24f1393a":"code","b794f945":"code","7927386a":"code","482bf191":"code","cb5ba3a5":"code","10f8b02b":"code","4ce7f4fb":"code","0dff5ce8":"code","3d3ea2b7":"code","ea2ec511":"code","c292da5c":"code","2a98b11d":"code","b442655b":"code","f22da8bc":"code","cccccf74":"code","2649921d":"code","6d0e6cac":"markdown","30c74d45":"markdown","545c054a":"markdown","908563e8":"markdown","e9e3e0a4":"markdown","c1b466d7":"markdown","d382986f":"markdown","0e631a4e":"markdown"},"source":{"96aa1908":"!nvidia-smi","eda2e039":"!pip install tensorflow-gpu","fb3aa9e6":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler, MinMaxScaler\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras.layers import LSTM, Dense, RepeatVector, Dropout, TimeDistributed\nimport seaborn as sns\n\nimport warnings\nwarnings.simplefilter('ignore')\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n","91763691":"df = pd.read_csv(\"\/kaggle\/input\/sp500-daily-19862018\/spx.csv\", parse_dates=['date'], index_col='date')","8e7e7442":"df.plot(figsize=(14,8))\nplt.show()","75b49041":"df.info()","f9740c71":"df.describe()","1453a84b":"### Using 95% as training data\n\n# We'll look back 30 days of historical data to learn past trend. \n# Setting shuffle to False to retain the time series\nTIMESTEPS = 30            \n\ntrain_data, test_data = train_test_split(df, train_size=0.95, shuffle=False)\ntrain_data.sort_index(inplace=True)\ntest_data.sort_index(inplace=True)\ntrain_data.shape, test_data.shape","924eb48c":"train_data","a49922c1":"train_data.info()","fa0b6dd1":"test_data.info()","d3d20cb5":"def getScaledData(method='standard', train_df=None, test_df=None, feature_col='feature'):\n    if method == 'standard':\n        scaler = StandardScaler()\n    else:\n        scaler = MinMaxScaler()\n    scaler = scaler.fit(train_df[[feature_col]])\n    train_df['scaled_'+feature_col] = scaler.transform(train_df[[feature_col]])\n    test_df['scaled_'+feature_col] = scaler.transform(test_df[[feature_col]])\n    return train_df, test_df, scaler\n    \ndef createDataset(df, lookback=30, feature_col=None):\n    data_x, data_y = [], []\n    for i in range(lookback, len(df)):\n        data_x.append(df.iloc[i-lookback:i][[feature_col]].values)\n        data_y.append(df.iloc[i][feature_col])\n    data_x = np.array(data_x)\n    data_y = np.array(data_y)\n    return data_x, data_y","8890f712":"train_df, test_df, scaler = getScaledData('standard', train_data, test_data, 'close')\ntrain_df.shape, test_df.shape","7d18a4d6":"train_df['scaled_close'].plot(figsize=(14,8))\nplt.show()","134707ed":"train_x, train_y = createDataset(train_df, TIMESTEPS, 'scaled_close')\ntest_x, test_y = createDataset(test_df, TIMESTEPS, 'scaled_close')","24f1393a":"train_x.shape, train_y.shape, test_x.shape, test_y.shape","b794f945":"\nLSTM_units = 64\nmodel = keras.Sequential()\nmodel.add(LSTM(LSTM_units, input_shape=(train_x.shape[1], train_x.shape[2]), return_sequences=False,name='encoder_lstm'\n              ))\nmodel.add(Dropout(0.2, name='encoder_dropout'))\nmodel.add(RepeatVector(train_x.shape[1], name='decoder_repeater'))\nmodel.add(LSTM(LSTM_units, return_sequences=True, name='decoder_lstm'))\nmodel.add(Dropout(rate=0.2, name='decoder_dropout'))\nmodel.add(TimeDistributed(Dense(train_x.shape[2],name='decoder_dense_output')))\n\nmodel.compile(loss='mae', optimizer='adam')","7927386a":"model.summary()","482bf191":"%time history = model.fit(train_x, train_x, epochs=10, batch_size=32, validation_split=0.1, shuffle=False)","cb5ba3a5":"plt.plot(history.history['loss'], label='training_loss')\nplt.plot(history.history['val_loss'], label='validation_loss')\nplt.legend()\nplt.show()","10f8b02b":"reconstructed = model.predict(train_x)\nreconstructed.shape, train_x.shape","4ce7f4fb":"# Reconstruction error - MAE for each sample\n\nmae_loss = np.mean(np.abs(reconstructed - train_x), axis=1)\nmae_loss.shape","0dff5ce8":"sns.distplot(mae_loss[:,0])\nplt.show()","3d3ea2b7":"THRESHOLD = 0.65","ea2ec511":"test_reconstruction = model.predict(test_x)\ntest_reconstruction.shape","c292da5c":"# MAE for reconstruction on test data\ntest_mae_loss = np.mean(np.abs(test_x - test_reconstruction), axis=1)\ntest_mae_loss.shape","2a98b11d":"test_df.info()","b442655b":"# Setting index after N timesteps from past in test_df\nanomaly_results_df = test_df[TIMESTEPS:][['close', 'scaled_close']].copy()\nanomaly_results_df.index = test_df[TIMESTEPS:].index\n\n# Including reconstructed predictions\nanomaly_results_df['deviation'] = test_mae_loss\nanomaly_results_df['threshold'] = THRESHOLD\nanomaly_results_df['anomaly'] = anomaly_results_df['deviation'].apply(lambda dev: 1 if dev > THRESHOLD else 0)\n\n\nanomalies = anomaly_results_df[anomaly_results_df['anomaly'] == 1]\nanomalies.shape","f22da8bc":"anomaly_results_df['anomaly'].plot(kind='hist')\nplt.show()","cccccf74":"anomaly_results_df[['deviation', 'threshold']].plot(figsize=(14, 6))\nplt.show()","2649921d":"anomaly_results_df[['close']].plot(figsize=(14, 6))\nsns.scatterplot(anomalies.index, anomalies['close'],label='anomaly',color='red')\nplt.show()","6d0e6cac":"1. ### Model configuration & training \n\nEach LSTM unit cell has an internal state called as cell state and an output called as hidden state.\nWe set return sequences to true to return hidden state for each timestep. This is set to true when \nstacking multiple LSTM layers where each LSTM layer receives a 3-dimensional input sequence or while returning a sequence of outputs.\nWe'll be using Timedistributed Layer to wrap output of dense layer for every timestep to return an output sequence.\n\nWe use RepeatVector to repeat our vector output returned by last layer in encoder LSTM. This vector is repeated TIMESTEPS time since the 1st layer in the decoder - decoder_lstm requires a 3-D input compressed sequence.\n","30c74d45":"The LSTM autoencoder will get train_x as input and will return an output with the same shape that will be compared with this input.","545c054a":"### Finding Anomalies\n\nPlotting the distribution of error for train set to set a threshold for reconstruction error beyond which the input record will be labelled as anomaly.","908563e8":"Setting a threshold to label anomalies","e9e3e0a4":"We will fit a separate scaler for training (& validation set) and test set.\nWe are assuming that the data used in training is normal with no anomalies and hence will fit a scaler from training dataset and will extract a subset of data as validation. Since validation data is also normal, this will be used for validation during training process.","c1b466d7":"### Data Preparation","d382986f":"### Observing the anomalies","0e631a4e":"#### References\n\n* https:\/\/machinelearningmastery.com\/return-sequences-and-return-states-for-lstms-in-keras\/\n* https:\/\/www.curiousily.com\/posts\/anomaly-detection-in-time-series-with-lstms-using-keras-in-python\/\n* https:\/\/towardsdatascience.com\/step-by-step-understanding-lstm-autoencoder-layers-ffab055b6352"}}