{"cell_type":{"08b07a40":"code","966bba8a":"code","9e1659b7":"code","037ee94a":"code","c45182f0":"code","bf1c4b36":"code","ba4b8812":"code","88509038":"code","a4f4fa56":"code","ed16bd7b":"code","31fc7c32":"code","43b169a3":"code","94c075b9":"code","e588a721":"code","685a961b":"code","29a0503e":"code","3ee92ff3":"code","773ff8bb":"code","3977d18f":"code","b4bef7a2":"code","3322a0c2":"code","2dd941c7":"code","2b452c86":"code","5871ba48":"code","5b287ce4":"code","74d58d70":"code","a9132d06":"code","fa1d8ece":"code","38a5b9f2":"code","bda4e531":"code","af26fd8b":"code","99eb1e0b":"code","288e2b5a":"markdown","a7d7f16c":"markdown","1d36335b":"markdown","e4a4cac6":"markdown","4b8d9940":"markdown","5d037bd1":"markdown","6d47c0d6":"markdown","1628d2ba":"markdown","86866796":"markdown","7f2238e4":"markdown","18a82d55":"markdown","422eb8fc":"markdown","3ca74f77":"markdown","8b9b13d7":"markdown","24668266":"markdown","499546cc":"markdown","a4813609":"markdown","860de436":"markdown","193bf07b":"markdown","afaa4dc1":"markdown","0fc1eaa1":"markdown","5277c083":"markdown","e91b159d":"markdown","2b75d02e":"markdown","fe372097":"markdown","7ea255a5":"markdown","32b78452":"markdown"},"source":{"08b07a40":"import pandas as pd\nimport numpy as np \nimport seaborn as sb\nfrom IPython.display import display\nimport matplotlib.pyplot as plt\ndata = pd.read_csv(\"..\/input\/Churn_Modelling.csv\")\noverview = data.head(10)\ntarget = data['Exited']\ndisplay(overview)","966bba8a":"X = data.iloc[:,3:13]\ntarget = np.array(target)\ndisplay(X.head())","9e1659b7":"target_0 = data[data['Exited'] == 0]['Exited'].count()\ntarget_1 = data[data['Exited']== 1]['Exited'].count()\nprint(target_0,target_1)","037ee94a":"%matplotlib inline\nlabels = [0,1]\nplt.bar(labels[0],target_0, width=0.1,color = 'red',edgecolor='yellow')\nplt.bar(labels[1],target_1,width=0.1,color = 'black',edgecolor='yellow')\nplt.legend()","c45182f0":"data.info()","bf1c4b36":"data.describe()","ba4b8812":"fig,axis = plt.subplots(figsize=(8,6))\naxis = sb.heatmap(data=data.corr(method='pearson',min_periods=1),annot=True,cmap=\"YlGnBu\")","88509038":"from itertools import chain\ncountmale = data[data['Gender']=='Male']['Gender'].count()\ncountfemale=data[data['Gender']=='Female']['Gender'].count()    \nfig,aix = plt.subplots(figsize=(8,6))\n#print(countmale)\n#print(countfemale)\naix = sb.countplot(hue='Exited',y='Geography',data=data)","a4f4fa56":"cal= data[data['IsActiveMember']==1].count()\ncal2 = data[data['Exited']==1].count()\nave = (cal2\/(cal+cal2))*100\nva= '%.1f '  % ave[1]\nprint(va+'%')","ed16bd7b":"age = np.array(data['Age'])\nfig,axis = plt.subplots(figsize=(8,6))\naxis = sb.distplot(age,kde=False,bins=200)","31fc7c32":"axis = sb.jointplot(x='Age',y='Exited',data = data)","43b169a3":"g = sb.FacetGrid(data,hue = 'Exited')\n(g.map(plt.hist,'Age',edgecolor=\"w\").add_legend())","94c075b9":"array1 = np.array(data['IsActiveMember'])\narray2 = np.array(data['Exited'])\nindex = len(array1)\ncount = 0\nfor i in range(index):\n    if(array1[i]==1 and array2[i]==1):\n        count +=1\nprint(count)","e588a721":"France = float(data[data['Geography']=='France']['Geography'].count())\nSpain = float(data[data['Geography']=='Spain']['Geography'].count())\nGermany = float(data[data['Geography']=='Germany']['Geography'].count())\nprint(France+Spain+Germany)","685a961b":"import plotly.plotly as py\nimport plotly.graph_objs as go \nfrom plotly.offline import download_plotlyjs,init_notebook_mode,plot,iplot\ninit_notebook_mode(connected=True)\n\n\ndata = dict(type='choropleth',\n           locations=['ESP','FRA','DEU'],\n           colorscale='YlGnBu',\n           text = ['Spain','France','Germany'],\n           z=[France,Spain,Germany],\n           colorbar={'title':'number in each geography'})\nlayout = dict(title='Counting the numbers of each nationality',\n              geo=dict(showframe=False,projection={'type':'natural earth'}))\nchoromap = go.Figure(data=[data],layout=layout)\n","29a0503e":"iplot(choromap)","3ee92ff3":"from sklearn.preprocessing import LabelEncoder,OneHotEncoder\nlabel = LabelEncoder()\nX['Geography'] = label.fit_transform(X['Geography'])\nX['Gender'] = label.fit_transform(X['Gender'])\nprint(X['Gender'].head(7))","773ff8bb":"onehotencoding = OneHotEncoder(categorical_features = [1])\nX = onehotencoding.fit_transform(X).toarray()\nprint(X)","3977d18f":"from sklearn.model_selection import train_test_split\n\ntrain_x,test_x,train_y,test_y = train_test_split(X,target,test_size=0.25,random_state=42)","b4bef7a2":"from imblearn.over_sampling import RandomOverSampler\nfrom imblearn.under_sampling import ClusterCentroids\nfrom imblearn.combine import SMOTEENN\nfrom collections import Counter\ngo = RandomOverSampler(random_state=42)\ntrain_x_resample,train_y_resample = go.fit_resample(train_x,train_y)\n# before resampling the number of each catogorical\nprint(Counter(train_y).items())\n# After resampling the number of each catogorical\nprint(Counter(train_y_resample).items())\n# now let use under sampling \ngo1 = ClusterCentroids(random_state=0)\ntrain_x_resample1,train_y_resample1 = go1.fit_resample(train_x,train_y)\n# before resampling the number of each catogorical\nprint(Counter(train_y).items())\n# After resampling the number of each catogorical\nprint(Counter(train_y_resample1).items())\n# now let combine two over and under resample\ngo2 = SMOTEENN(random_state=0) \ntrain_x_resample2,train_y_resample2 = go2.fit_resample(train_x,train_y)\n# before resampling the number of each catogorical\nprint(Counter(train_y).items())\n# After resampling the number of each catogorical\nprint(Counter(train_y_resample2).items())","3322a0c2":"from sklearn.metrics import accuracy_score,recall_score,f1_score,cohen_kappa_score,precision_score\nfrom time import *\ndef choose_best(model, train_x , train_y , test_x , test_y):\n    result = {}\n    \n    #for calculate time of fitting data\n    start = time()\n    model.fit(train_x,train_y)\n    end = time()\n    result['train_time'] = end-start\n    \n    #for prediction\n    \n    start = time()\n    test_y_new = model.predict(test_x)\n    train_y_new = model.predict(train_x)\n    end = time()\n    \n    result[\"prediction_time\"] = end - start\n    \n    result['acc_prediction_train'] = accuracy_score(train_y,train_y_new)\n    result['recall_prediction_train'] = recall_score(train_y,train_y_new)\n    result['f1_score_test'] = f1_score(test_y,test_y_new)\n    result['recall_prediction_test'] = recall_score(test_y,test_y_new)\n    result['cohen_kappa_score'] = cohen_kappa_score(test_y,test_y_new)\n    result['precision_score'] = precision_score(test_y,test_y_new)\n    print('name of model {}'.format(model))\n    \n    return result\n    ","2dd941c7":"from sklearn.linear_model import LogisticRegression\n\nclassifier_1 = LogisticRegression(random_state = 42,solver='lbfgs')\nvalues1 = choose_best(classifier_1,train_x_resample,train_y_resample,test_x,test_y)\nvalues1n = choose_best(classifier_1,train_x_resample1,train_y_resample1,test_x,test_y)\nvalues1nn = choose_best(classifier_1,train_x_resample2,train_y_resample2,test_x,test_y)","2b452c86":"from sklearn.ensemble import AdaBoostClassifier\n\nclassifier_2 = AdaBoostClassifier(random_state=42)\nvalues2 = choose_best(classifier_2,train_x_resample,train_y_resample,test_x,test_y)\nvalues2n = choose_best(classifier_2,train_x_resample1,train_y_resample1,test_x,test_y)\nvalues2nn = choose_best(classifier_2,train_x_resample2,train_y_resample2,test_x,test_y)","5871ba48":"from sklearn.ensemble import GradientBoostingClassifier\n\nclassifier_3 = GradientBoostingClassifier()\nvalues3 = choose_best(classifier_3,train_x_resample,train_y_resample,test_x,test_y)\nvalues3n = choose_best(classifier_3,train_x_resample1,train_y_resample1,test_x,test_y)\nvalues3nn = choose_best(classifier_3,train_x_resample2,train_y_resample2,test_x,test_y)","5b287ce4":"from sklearn.ensemble import RandomForestClassifier\nclassifier_4 = RandomForestClassifier(n_estimators=100) #warning 10 to 100\nvalues4 = choose_best(classifier_4,train_x_resample,train_y_resample,test_x,test_y)\nvalues4n = choose_best(classifier_4,train_x_resample1,train_y_resample1,test_x,test_y)\nvalues4nn = choose_best(classifier_4,train_x_resample2,train_y_resample2,test_x,test_y)","74d58d70":"moduels = pd.DataFrame({'name_model':[\"logistic regression\",\"adaboost\",\"gradient boost\",\"random forest\"],\\\n                       'accuracy_training':[values1[\"acc_prediction_train\"],values2['acc_prediction_train'],values3['acc_prediction_train'],values4['acc_prediction_train']],\\\n                       \"recall_testing\":[values1[\"recall_prediction_test\"],values2[\"recall_prediction_test\"],values3[\"recall_prediction_test\"],values4[\"recall_prediction_test\"]],\\\n                        \"f1_score\":[values1[\"f1_score_test\"],values2[\"f1_score_test\"],values3[\"f1_score_test\"],values4[\"f1_score_test\"]],\\\n                        \"precision_test\":[values1[\"precision_score\"],values2[\"precision_score\"],values3[\"precision_score\"],values4[\"precision_score\"]],\\\n                        \"kappa_score\":[values1[\"cohen_kappa_score\"],values2[\"cohen_kappa_score\"],values3[\"cohen_kappa_score\"],values4[\"cohen_kappa_score\"]],\\\n                        \"timing_train\":[values1[\"train_time\"],values2[\"train_time\"],values3[\"train_time\"],values4[\"train_time\"]],\\\n                       \"timing_test\":[values1[\"prediction_time\"],values2[\"prediction_time\"],values3[\"prediction_time\"],values4[\"prediction_time\"]]})\nmoduels.sort_values(by =[\"f1_score\"], ascending = False)","a9132d06":"moduels = pd.DataFrame({'name_model':[\"logistic regression\",\"adaboost\",\"gradient boost\",\"random forest\"],\\\n                       'accuracy_training':[values1n[\"acc_prediction_train\"],values2n['acc_prediction_train'],values3n['acc_prediction_train'],values4n['acc_prediction_train']],\\\n                       \"recall_testing\":[values1n[\"recall_prediction_test\"],values2n[\"recall_prediction_test\"],values3n[\"recall_prediction_test\"],values4n[\"recall_prediction_test\"]],\\\n                        \"f1_score\":[values1n[\"f1_score_test\"],values2n[\"f1_score_test\"],values3n[\"f1_score_test\"],values4n[\"f1_score_test\"]],\\\n                        \"kappa_score\":[values1n[\"cohen_kappa_score\"],values2n[\"cohen_kappa_score\"],values3n[\"cohen_kappa_score\"],values4n[\"cohen_kappa_score\"]],\\\n                        \"precision_test\":[values1n[\"precision_score\"],values2n[\"precision_score\"],values3n[\"precision_score\"],values4n[\"precision_score\"]],\\\n                        \"timing_train\":[values1n[\"train_time\"],values2n[\"train_time\"],values3n[\"train_time\"],values4n[\"train_time\"]],\\\n                       \"timing_test\":[values1n[\"prediction_time\"],values2n[\"prediction_time\"],values3n[\"prediction_time\"],values4n[\"prediction_time\"]]})\nmoduels.sort_values(by =[\"f1_score\"], ascending = False)","fa1d8ece":"moduels = pd.DataFrame({'name_model':[\"logistic regression\",\"adaboost\",\"gradient boost\",\"random forest\"],\\\n                       'accuracy_training':[values1nn[\"acc_prediction_train\"],values2nn['acc_prediction_train'],values3nn['acc_prediction_train'],values4nn['acc_prediction_train']],\\\n                       \"recall_testing\":[values1nn[\"recall_prediction_test\"],values2nn[\"recall_prediction_test\"],values3nn[\"recall_prediction_test\"],values4nn[\"recall_prediction_test\"]],\\\n                        \"f1_score\":[values1nn[\"f1_score_test\"],values2nn[\"f1_score_test\"],values3nn[\"f1_score_test\"],values4nn[\"f1_score_test\"]],\\\n                        \"kappa_score\":[values1nn[\"cohen_kappa_score\"],values2nn[\"cohen_kappa_score\"],values3nn[\"cohen_kappa_score\"],values4nn[\"cohen_kappa_score\"]],\\\n                        \"precision_test\":[values1nn[\"precision_score\"],values2nn[\"precision_score\"],values3nn[\"precision_score\"],values4nn[\"precision_score\"]],\\\n                        \"timing_train\":[values1nn[\"train_time\"],values2nn[\"train_time\"],values3nn[\"train_time\"],values4nn[\"train_time\"]],\\\n                       \"timing_test\":[values1nn[\"prediction_time\"],values2nn[\"prediction_time\"],values3nn[\"prediction_time\"],values4nn[\"prediction_time\"]]})\nmoduels.sort_values(by =[\"f1_score\"], ascending = False)","38a5b9f2":"from sklearn.model_selection import GridSearchCV\nfrom sklearn.metrics import fbeta_score,make_scorer,classification_report,confusion_matrix,roc_auc_score\nparameters = [{'loss':['deviance'],'learning_rate':[0.1,0.2,0.3,0.4],'n_estimators':[50,100],\n              'max_depth':[3,6,10,15]}]\nscorer = make_scorer(fbeta_score,beta=0.5)\ngrid_search =  GridSearchCV(estimator = classifier_3, param_grid  = parameters, scoring = scorer ,cv = 5)\ngrid_fit = grid_search.fit(train_x_resample,train_y_resample)\nbest_accuracy = grid_fit.best_score_\nbest_para = grid_fit.best_params_\nbest_clas = grid_fit.best_estimator_\nprdict_y  = best_clas.predict(test_x)\nscore = fbeta_score(test_y,prdict_y,beta=0.5)\nprint(best_accuracy,best_para,score)","bda4e531":"confusionMatrix = confusion_matrix(test_y,prdict_y)\nsb.heatmap(confusionMatrix,annot=True,fmt='d')","af26fd8b":"print(classification_report(test_y,prdict_y))","99eb1e0b":"roc = roc_auc_score(test_y,prdict_y)\nprint(roc)","288e2b5a":"we are removing this features('RowNumber','CustomerId','Surname') this features do not effect with prediction.","a7d7f16c":"based on kappa score and f1 score that is best choose to evalute the best model in imbalanced dataset, we arre choose the gradient boost as the classifier for this problem and we are choose the resampling method number **3** that is combination of under_sampler and oversampler.","1d36335b":"In this problem we faced on more challenge and beat it, first problem is imbalanced dataset and we define three methods of resampling:\n- overSampling: In this method add samples in lower class and make it equal to class2, the disadantage is : we can make overfitting to our model.\n- underSampling: In this method add to lower class more samples repeted samples to lower class to be equaled to another class disadventage:that we are lossing more important information.\n- combination oversampling and undersampling(the best): This method is the best one because it combine the advantage of first method and advantage second method add and remove samples to be nearly equal to each other.\n#### then\nwe choose the classifier and feed our resampling data to fitting and training on it, then we choose the higher one based on metrics and we choose metrics specified to deal with imbalanced data like: f1 score, kappa score, precision, recall then we create grid search to selected most powerful parameters to classifier.then create confusion and roc curve to see the final result.","e4a4cac6":"### Resampling Training Data","4b8d9940":"in this section we are making to features preprocessing to be acceptable to feed it to classifier. and in this section below in create label encoder to geography and gender to be numbers that each number refer to name that can not accepted to classifier, example gender have two types female and male this function **LabelEncoder** make this two types in encode like 1 refer to male and 0 refer to female. if we have more types the number is increased. ","5d037bd1":"now we are going to see the important column and the more powerfull column 'geography'. and i want to visualize this column with **plotly**, because it interactive visualization library.","6d47c0d6":"**Problem in Data**","1628d2ba":"we see that our data is imbalanced, because we have 80% of zeros and we have 20% of ones that may make problems with predication.","86866796":"we are seeing that the clients is more between 30 to 50 we are having more clients in this range.","7f2238e4":"# with resampling, we get higher accuracy\n\n### Churn prediction model ","18a82d55":"This Dataframe show the result of each classifiers with resampling using : **Under_Sampling** method. ","422eb8fc":"In this section we are going to load data with pandas and explore data by visulization it with seaborn and matplotlib","3ca74f77":"### Data Preprocessing","8b9b13d7":"This Dataframe show the result of each classifiers with resampling using : **Combination of twos** method. ","24668266":"### visualization","499546cc":"### Splitting Data","a4813609":"### Tunning the parameters","860de436":"### Exploring the data and visualization","193bf07b":"In previous section we are creating code to count all matched 1 with another 1 in exited column ,that is is activate user is effects on the output 735.  ","afaa4dc1":"### Classifiers and Evaluation","0fc1eaa1":"the Data is clear we do not have null values or another types of noise in dataset.","5277c083":"### Conclusion","e91b159d":"This Dataframe show the result of each classifiers with resampling using : **over_Sampling** method. ","2b75d02e":"**sections**:\n* Exploring the data and visualization\n* Data preprocessing\n* Splitting Data\n* Resampling Training Data\n* Classifiers and Evaluation\n* Tunning the parameters\n* Conclusion\n","fe372097":"we have imbalanced data that mean we have the number of samples for class1 is more than class2, the solution for this problem is resampling data and we are resampling data by oversampling it. **Over_sampling** mean that i will generate new samples and add it to the low class that is low than another class.","7ea255a5":"we are computing the pairwise correlation between columns because we are having another types in data and we want numbers columns the plot this value to the heatmap to see the correlation.","32b78452":"you can easy interact with earth to see the numbers of clients in each country."}}