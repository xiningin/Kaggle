{"cell_type":{"391370c8":"code","742b0d78":"code","2dbdb998":"code","122b0f5b":"code","e5b3fe01":"code","2c1937f3":"code","6546e824":"code","6116f372":"code","53a1a4d7":"code","e22f5c54":"code","84653d4e":"code","8a707044":"code","1b12471a":"code","a498fd61":"code","a90f7dea":"code","e6997053":"code","d0379b36":"code","59de3bde":"code","36e5b18e":"code","ec6ba369":"code","73070af2":"code","481748a2":"code","9dca1b98":"code","f76997a5":"code","dd942f4d":"code","ed772f06":"code","085c561b":"code","b20a35de":"code","67201c5a":"code","97263965":"code","7f9bf58a":"code","a376f1ba":"code","d4b794b8":"code","8f697950":"code","b8af6be9":"markdown","9010cb59":"markdown","37de4be5":"markdown","83b516ad":"markdown","2d13b099":"markdown","e04bef85":"markdown"},"source":{"391370c8":"import os\ntrain_dir='\/kaggle\/input\/depthwise-conv-adv\/data_aug\/train\/content\/data\/content\/FINAL_AUG_DATA\/Train\/'\ntest_dir='\/kaggle\/input\/depthwise-conv-adv\/data_aug\/val\/content\/data\/content\/FINAL_AUG_DATA\/Val\/'\nos.listdir(train_dir)","742b0d78":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","2dbdb998":"# %run -i '..\/input\/gdown-package\/Gdown.txt'","122b0f5b":"import warnings\nwarnings.filterwarnings('ignore')\nfrom tensorflow.keras.models import load_model\n!pip install albumentations > \/dev\/null\n# !pip install -U efficientnet==0.0.4\n!pip install -U segmentation-models\n\nimport numpy as np\nimport pandas as pd\nimport gc\nimport keras\n\nimport matplotlib.pyplot as plt\nplt.style.use('seaborn-white')\nimport seaborn as sns\nsns.set_style(\"white\")\n\nfrom sklearn.model_selection import train_test_split,StratifiedKFold\n\nfrom skimage.transform import resize\nimport tensorflow as tf\nimport keras.backend as K\nfrom keras.losses import binary_crossentropy\n\nfrom keras.preprocessing.image import load_img\nfrom keras import Model\nfrom keras.callbacks import  ModelCheckpoint\nfrom keras.layers import Input, Conv2D, Conv2DTranspose, MaxPooling2D, concatenate, Dropout,BatchNormalization\nfrom keras.layers import Conv2D, Concatenate, MaxPooling2D\nfrom keras.layers import UpSampling2D, Dropout, BatchNormalization\nfrom tqdm import tqdm_notebook\nfrom keras import initializers\nfrom keras import regularizers\nfrom keras import constraints\nfrom keras.utils import conv_utils\nfrom keras.utils.data_utils import get_file\nfrom keras.engine.topology import get_source_inputs\nfrom keras.engine import InputSpec\nfrom keras import backend as K\nfrom keras.layers import LeakyReLU\nfrom keras.layers import ZeroPadding2D\nfrom keras.losses import binary_crossentropy\nimport keras.callbacks as callbacks\nfrom keras.callbacks import Callback\nfrom keras.applications.xception import Xception\nfrom keras.layers import multiply\n\n\nfrom keras import optimizers\nfrom keras.legacy import interfaces\nfrom keras.utils.generic_utils import get_custom_objects\n\nfrom keras.engine.topology import Input\nfrom keras.engine.training import Model\nfrom keras.layers.convolutional import Conv2D, UpSampling2D, Conv2DTranspose\nfrom keras.layers.core import Activation, SpatialDropout2D\nfrom keras.layers.merge import concatenate\nfrom keras.layers.normalization import BatchNormalization\nfrom keras.layers.pooling import MaxPooling2D\nfrom keras.layers import Input,Dropout,BatchNormalization,Activation,Add\nfrom keras.regularizers import l2\nfrom keras.layers.core import Dense, Lambda\nfrom keras.layers.merge import concatenate, add\nfrom keras.layers import GlobalAveragePooling2D, Reshape, Dense, multiply, Permute\nfrom keras.optimizers import SGD,Adam\nfrom keras.models import Model\nfrom keras.preprocessing.image import ImageDataGenerator\n\nimport glob\nimport shutil\nimport os\nimport random\nfrom PIL import Image\nimport cv2\n\nseed = 10\nnp.random.seed(seed)\nrandom.seed(seed)\nos.environ['PYTHONHASHSEED'] = str(seed)\nnp.random.seed(seed)\n# tf.set_random_seed(seed)\n    \n%matplotlib inline","e5b3fe01":"def dice_loss(y_true,y_pred):   \n    smooth = 1\n    y_true_f = K.flatten(y_true)\n    y_pred_f = K.flatten(y_pred)\n    intersection = K.sum(y_true_f * y_pred_f)\n    dice_coeff = (intersection * 2 + smooth) \/ (K.sum(y_true_f) + K.sum(y_pred_f) + smooth)\n    return 1 - dice_coeff\ndef bce_dice_loss(y_true,y_pred):\n    return binary_crossentropy(y_true,y_pred) + dice_loss(y_true,y_pred)","2c1937f3":"def get_iou_vector(A,B):\n    batch_size = A.shape[0]\n    metric = 0.0\n    for i in range(batch_size):\n        t,p = A[i],B[i]\n        intersection = np.sum(t * p)\n        true = np.sum(t)\n        pred = np.sum(p)\n        \n        if(true == 0):\n            metric += (pred == 0)\n            \n        union = true + pred - intersection\n        iou = intersection \/ union\n        iou = np.floor(max(0,(iou - 0.45) * 20)) \/ 10\n        metric += iou\n    return metric \/ batch_size\ndef iou_metric(label,pred):\n    return tf.compat.v1.py_func(get_iou_vector,[label,pred > 0.5],tf.float64)","6546e824":"dependencies = {\n    'iou_metric': iou_metric,\n    'bce_dice_loss':bce_dice_loss\n}\n","6116f372":"model = load_model('..\/input\/lung-segmentation-unet\/best_model.h5',custom_objects=dependencies)","53a1a4d7":"from tensorflow import keras\nimport cv2\nimport matplotlib.pyplot as plt","e22f5c54":"img = cv2.imread('..\/input\/covid19-radiography-database\/COVID-19 Radiography Database\/NORMAL\/NORMAL (670).png')\nimg1 = cv2.cvtColor(img,cv2.COLOR_BGR2RGB)\nplt.imshow(img1)\n\n# img.shape\n# plt.imshow(img,cmap='gray')","84653d4e":"img = np.reshape(img,(1,1024,1024,3))\nprediction=model.predict(img)","8a707044":"type(prediction)","1b12471a":"pred_img = np.reshape(prediction,(1024, 1024)) \n","a498fd61":"fig , (ax1,ax2) = plt.subplots(1,2)\nax1.imshow(pred_img)\nax2.imshow(img1)\nax1.legend()\nax2.legend()","a90f7dea":"ret,thresh = cv2.threshold(pred_img,0.21205534,1.0,cv2.THRESH_BINARY)\nplt.imshow(thresh)","e6997053":"print(np.unique(thresh))","d0379b36":"arr = thresh.astype('int8') ","59de3bde":"show_masked_img = cv2.bitwise_and(img1,img1,mask = arr)","36e5b18e":"plt.imshow(show_masked_img)","ec6ba369":"import gc\ngc.collect()","73070af2":"DIR_covid = '..\/input\/covid19-radiography-database\/COVID-19 Radiography Database\/COVID-19'\nDIR_normal = '..\/input\/covid19-radiography-database\/COVID-19 Radiography Database\/NORMAL'\nDIR_pneumonia = '..\/input\/covid19-radiography-database\/COVID-19 Radiography Database\/Viral Pneumonia'\n\nfile_covid  = os.listdir(DIR_covid)\nfile_normal  = os.listdir(DIR_normal)\nfile_pneumonia  = os.listdir(DIR_pneumonia)\n\nprint(len(file_covid))\nprint(len(file_pneumonia))\nprint(len(file_normal))\n","481748a2":"list_covid = []\nlist_normal = []\nlist_pneumonia = []\n\nfor i in file_covid:\n    lists1 = os.path.join(DIR_covid,i)\n    list_covid.append(lists1)\n\n\nfor j in file_normal:\n    lists2 = os.path.join(DIR_normal,j)\n    list_normal.append(lists2)\n    \n\nfor k in file_pneumonia:\n    lists3 = os.path.join(DIR_pneumonia,k)\n    list_pneumonia.append(lists3)\n    \n    \n    \n","9dca1b98":"main_list = list_covid+list_normal+list_pneumonia\nmain_list[0]","f76997a5":"from tqdm import tqdm","dd942f4d":"IMG_SIZE = 224\nimg = []\n\nfor i in tqdm(main_list):\n    pic = cv2.imread('..\/input\/covid19-radiography-database\/COVID-19 Radiography Database\/COVID-19\/COVID-19(219).png')\n    pic1 = cv2.cvtColor(pic,cv2.COLOR_BGR2RGB)\n    \n#   plt.imshow(pic1)\n    \n    pic = np.reshape(pic,(1,1024,1024,3))\n    pred = model.predict(pic)\n    predicted_img = np.reshape(pred,(1024, 1024)) \n    \n#   plt.imshow(predicted_img)\n    \n    ret1,thresh1 = cv2.threshold(predicted_img,predicted_img.mean(),1.0,cv2.THRESH_BINARY)\n    \n#   plt.imshow(thresh1)\n    \n    arr1 = thresh1.astype('int8') \n    segmented_img = cv2.bitwise_and(pic1,pic1,mask = arr1)\n    \n#   plt.imshow(segmented_img)\n\n    img.append(segmented_img)\n\n\n    \n\n","ed772f06":"gc.collect()","085c561b":"# pic = cv2.imread('..\/input\/covid19-radiography-database\/COVID-19 Radiography Database\/COVID-19\/COVID-19(219).png')\n# pic1 = cv2.cvtColor(pic,cv2.COLOR_BGR2RGB)\n# # plt.imshow(pic1)\n# pic = np.reshape(pic,(1,1024,1024,3))\n# pred = model.predict(pic)\n# predicted_img = np.reshape(pred,(1024, 1024)) \n# # plt.imshow(predicted_img)\n# ret1,thresh1 = cv2.threshold(predicted_img,predicted_img.mean(),1.0,cv2.THRESH_BINARY)\n# # plt.imshow(thresh1)\n# arr1 = thresh1.astype('int8') \n# segmented_img = cv2.bitwise_and(pic1,pic1,mask = arr1)\n# plt.imshow(segmented_img)\n\n","b20a35de":"covid_label = []\nnormal_label = []\npneumonia_label = []\n\nfor i in range(219):\n    zero = 0\n    covid_label.append(zero)\n    \nfor i in range(1341):\n    one = 1\n    normal_label.append(one)\n    \nfor i in range(1345):\n    two = 2\n    pneumonia_label.append(two)\n    ","67201c5a":"main_label = covid_label + normal_label + pneumonia_label","97263965":"train_imgs = np.asarray(img)\nprint(train_imgs.shape)\ntrain_labels =  np.asarray(main_label)","7f9bf58a":"BATCH_SIZE = 64\nSEED = 42\nEPOCHS = 100\nx, y, z = 224, 224, 3\ninputShape = (x, y, z)\nNUM_CLASSES = 1","a376f1ba":"!pip install gdown","d4b794b8":"!\/opt\/conda\/bin\/python3.7 -m pip install --upgrade pip","8f697950":"!pip install gdown\nimport gdown\n\ntrain_url = 'https:\/\/drive.google.com\/uc?id=1-4WfgSQLMIMxl-vrqDjZTMiwb1w3qiJq'\nval_url = 'https:\/\/drive.google.com\/uc?id=1w9CuqPi3DbvbCN9DFGwLPeyYsXJvbjzk'\n#the problem is that the whole file train+val is too big for kaggle!\n#as kaggle gives 4.9gb of stage and the dataset is 2.8 gb\n#since we are using zipped file first.\n#https:\/\/drive.google.com\/open?id=1-4WfgSQLMIMxl-vrqDjZTMiwb1w3qiJq --training\n#https:\/\/drive.google.com\/open?id=1w9CuqPi3DbvbCN9DFGwLPeyYsXJvbjzk --validation\noutput_train = 'dataset_train.zip'\noutput_val = 'dataset_val.zip'\ngdown.download(train_url, output_train, quiet=False)\nos.remove(file_name)\n\nimport os\ndef create_dir(dir):\n  if not os.path.exists(dir):\n    os.makedirs(dir)\n    print(\"Created Directory : \", dir)\n    return\ncreate_dir(\"\/kaggle\/working\/data_aug\/train\")\n!unzip -q \/kaggle\/working\/dataset_train.zip -d \/kaggle\/working\/data_aug\/train\n\nos.listdir(\"\/kaggle\/working\/\")\n\nfile_name=\"\/kaggle\/working\/dataset_train.zip\"\nos.remove(file_name)\n\nos.listdir(\"\/kaggle\/working\/\")\n\ngdown.download(val_url, output_val, quiet=False)\n\nos.listdir(\"\/kaggle\/working\/\")\n\n\ncreate_dir(\"\/kaggle\/working\/data_aug\/val\")\n!unzip -q \/kaggle\/working\/dataset_val.zip -d \/kaggle\/working\/data_aug\/val\n\n\nos.listdir(\"\/kaggle\/working\/data_aug\")\nfile_name=\"\/kaggle\/working\/dataset_val.zip\"\nos.remove(file_name)\n\n\nimport os\n\"\"\"\nfor dirname, _, filenames in os.walk('\/kaggle\/working'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\"\"\"","b8af6be9":"### Prepairing the segmented data:","9010cb59":"### Segmenting the Lungs from the normal image using the thresholded image\n","37de4be5":"### Loading an Image for prediction","83b516ad":"### visualising the O\/P","2d13b099":"### Reshapeing the predicted image for visualization","e04bef85":"### Applying thresholding for converting the predicted image into a Binary image:"}}