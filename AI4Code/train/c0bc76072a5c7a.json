{"cell_type":{"9fe84e16":"code","58f39107":"code","9da172b4":"code","2bfc82df":"code","a181b3cb":"code","7bd4de52":"code","cfb07e7f":"code","4ddb2132":"code","b01f9668":"code","5cdaaf7d":"code","05d47ff2":"code","0637935b":"code","cd27c7ad":"code","9cc06244":"code","3a4efb5a":"code","0e9fce2a":"code","993f4ee0":"code","8fdd8b34":"code","f056b528":"code","06f4f51b":"code","17ce130d":"code","bddd90f8":"code","67a2b88a":"code","50cb653d":"code","ace54035":"code","99525664":"code","2603cd50":"code","23ed84ec":"code","859bed34":"code","df019118":"code","ce454c2f":"code","89db8ac2":"code","0dfcff00":"code","5bea8bc7":"code","c00735c6":"code","ff46d11b":"code","4ad303d6":"code","5a5dae61":"code","9eb0b1ac":"code","4fdff6ea":"code","a32355ba":"markdown","9e86646f":"markdown","1254ee4f":"markdown","81dceb4a":"markdown","97dddc72":"markdown","bb8b13f4":"markdown","f347bac6":"markdown","35d23858":"markdown","6fd8c54e":"markdown","4b407a12":"markdown","260c662c":"markdown","a9d52afe":"markdown","b6cd1865":"markdown","30bc66af":"markdown","744eda7f":"markdown"},"source":{"9fe84e16":"import numpy as np\nfrom tensorflow import keras \n\nxs = np.array([-1.0, 0.0, 1.0, 2.0, 3.0, 4.0], dtype=float)\nys = np.array([-2.0, 1.0, 4.0, 7.0, 10.0, 13.0], dtype=float)","58f39107":"model1 = keras.Sequential(keras.layers.Dense(units=1, input_shape=[1]))\n\nmodel1.compile(loss='mean_squared_error', optimizer='sgd', metrics='mean_squared_error')","9da172b4":"model1.fit(x=xs, y=ys, epochs=50, verbose=0)","2bfc82df":"model1.evaluate([10], [31])","a181b3cb":"model1.predict([10])","7bd4de52":"from tensorflow import keras\nprint(keras.__version__)\n\nmnist = keras.datasets.fashion_mnist\n\n(training_images, training_labels), (test_images, test_labels) = mnist.load_data()\n\ntraining_images  = training_images \/ 255.0\ntest_images = test_images \/ 255.0","cfb07e7f":"model2 = keras.Sequential([keras.layers.Flatten(),\n                         keras.layers.Dense(128, activation='relu'),\n                         keras.layers.Dense(10, activation='softmax')])\n\nmodel2.compile(loss='sparse_categorical_crossentropy',\n              optimizer=keras.optimizers.Adam(),\n              metrics='accuracy')","4ddb2132":"class myCallback(keras.callbacks.Callback):\n    \n    def on_epoch_end(self, epoch, logs={}):\n        if logs['accuracy'] > 0.87:\n            print(f\"Accuracy is {logs['accuracy']} so cancelling training!\")\n            self.model.stop_training = True\n            \ncallback = myCallback()","b01f9668":"model2.fit(training_images, training_labels, epochs=5, callbacks=callback)","5cdaaf7d":"model2.evaluate(test_images, test_labels)","05d47ff2":"classifications = model2.predict(test_images)\nprint(classifications[0])\nprint(test_labels[0])","0637935b":"import cv2\nimport numpy as np\nfrom scipy import misc\ni = misc.ascent()","cd27c7ad":"import matplotlib.pyplot as plt\nplt.grid(False)\nplt.gray()\nplt.axis('off')\nplt.imshow(i)\nplt.show()","9cc06244":"i_transformed = np.copy(i)\nsize_x = i_transformed.shape[0]\nsize_y = i_transformed.shape[1]","3a4efb5a":"# This filter detects edges nicely\n# It creates a convolution that only passes through sharp edges and straight\n# lines.\n\n#Experiment with different values for fun effects.\n#filter = [ [0, 1, 0], [1, -4, 1], [0, 1, 0]]\n\n# A couple more filters to try for fun!\nfilter = [ [-1, -2, -1], [0, 0, 0], [1, 2, 1]]\n#filter = [ [-1, 0, 1], [-2, 0, 2], [-1, 0, 1]]\n\n# If all the digits in the filter don't add up to 0 or 1, you \n# should probably do a weight to get it to do so\n# so, for example, if your weights are 1,1,1 1,2,1 1,1,1\n# They add up to 10, so you would set a weight of .1 if you want to normalize them\nweight = 1","0e9fce2a":"for x in range(1,size_x-1):\n    for y in range(1,size_y-1):\n        convolution = 0.0\n        convolution = convolution + (i[x - 1, y-1] * filter[0][0])\n        convolution = convolution + (i[x, y-1] * filter[0][1])\n        convolution = convolution + (i[x + 1, y-1] * filter[0][2])\n        convolution = convolution + (i[x-1, y] * filter[1][0])\n        convolution = convolution + (i[x, y] * filter[1][1])\n        convolution = convolution + (i[x+1, y] * filter[1][2])\n        convolution = convolution + (i[x-1, y+1] * filter[2][0])\n        convolution = convolution + (i[x, y+1] * filter[2][1])\n        convolution = convolution + (i[x+1, y+1] * filter[2][2])\n        convolution = convolution * weight\n        if(convolution<0):\n            convolution=0\n        if(convolution>255):\n            convolution=255\n        i_transformed[x, y] = convolution","993f4ee0":"# Plot the image. Note the size of the axes -- they are 512 by 512\nplt.gray()\nplt.grid(False)\nplt.imshow(i_transformed)\n#plt.axis('off')\nplt.show()   ","8fdd8b34":"new_x = int(size_x\/2)\nnew_y = int(size_y\/2)\nnewImage = np.zeros((new_x, new_y))\nfor x in range(0, size_x, 2):\n    for y in range(0, size_y, 2):\n        pixels = []\n        pixels.append(i_transformed[x, y])\n        pixels.append(i_transformed[x+1, y])\n        pixels.append(i_transformed[x, y+1])\n        pixels.append(i_transformed[x+1, y+1])\n        pixels.sort(reverse=True)\n        newImage[int(x\/2),int(y\/2)] = pixels[0]\n\n# Plot the image. Note the size of the axes -- now 256 pixels instead of 512\nplt.gray()\nplt.grid(False)\nplt.imshow(newImage)\n#plt.axis('off')\nplt.show()      ","f056b528":"import tensorflow as tf\nprint(tf.__version__)\nmnist = tf.keras.datasets.fashion_mnist\n(training_images, training_labels), (test_images, test_labels) = mnist.load_data()\ntraining_images=training_images.reshape(60000, 28, 28, 1)\ntraining_images=training_images \/ 255.0\ntest_images = test_images.reshape(10000, 28, 28, 1)\ntest_images=test_images \/ 255.0","06f4f51b":"model = tf.keras.models.Sequential([\n  tf.keras.layers.Conv2D(64, (3, 3), activation='relu', input_shape=(28, 28, 1)),\n  tf.keras.layers.MaxPooling2D(2, 2),\n  tf.keras.layers.Conv2D(64, (3, 3), activation='relu'),\n  tf.keras.layers.MaxPooling2D(2,2),\n  tf.keras.layers.Flatten(),\n  tf.keras.layers.Dense(128, activation='relu'),\n  tf.keras.layers.Dense(10, activation='softmax')\n])\nmodel.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\nmodel.summary()","17ce130d":"model.fit(training_images, training_labels, epochs=5)","bddd90f8":"test_loss, test_accuracy = model.evaluate(test_images, test_labels)\nprint ('Test loss: {}, Test accuracy: {}'.format(test_loss, test_accuracy*100))","67a2b88a":"import matplotlib.pyplot as plt\nf, axarr = plt.subplots(3,4)\nFIRST_IMAGE=0\nSECOND_IMAGE=23\nTHIRD_IMAGE=28\nCONVOLUTION_NUMBER = 6\nfrom tensorflow.keras import models\nlayer_outputs = [layer.output for layer in model.layers]\nactivation_model = tf.keras.models.Model(inputs = model.input, outputs = layer_outputs)\nfor x in range(0,4):\n    f1 = activation_model.predict(test_images[FIRST_IMAGE].reshape(1, 28, 28, 1))[x]\n    axarr[0,x].imshow(f1[0, : , :, CONVOLUTION_NUMBER], cmap='inferno')\n    axarr[0,x].grid(False)\n    f2 = activation_model.predict(test_images[SECOND_IMAGE].reshape(1, 28, 28, 1))[x]\n    axarr[1,x].imshow(f2[0, : , :, CONVOLUTION_NUMBER], cmap='inferno')\n    axarr[1,x].grid(False)\n    f3 = activation_model.predict(test_images[THIRD_IMAGE].reshape(1, 28, 28, 1))[x]\n    axarr[2,x].imshow(f3[0, : , :, CONVOLUTION_NUMBER], cmap='inferno')\n    axarr[2,x].grid(False)\n","50cb653d":"!wget --no-check-certificate https:\/\/storage.googleapis.com\/laurencemoroney-blog.appspot.com\/horse-or-human.zip  -O \/tmp\/horse-or-human.zip","ace54035":"import os\nimport zipfile\n \nlocal_zip = '\/tmp\/horse-or-human.zip'\nzip_ref = zipfile.ZipFile(local_zip, 'r')\nzip_ref.extractall('\/tmp\/horse-or-human')\nzip_ref.close()\n\n# Directory with our training horse pictures\ntrain_horse_dir = os.path.join('\/tmp\/horse-or-human\/horses')\n \n# Directory with our training human pictures\ntrain_human_dir = os.path.join('\/tmp\/horse-or-human\/humans')\n\ntrain_horse_names = os.listdir(train_horse_dir)\nprint(train_horse_names[:10])\ntrain_human_names = os.listdir(train_human_dir)\nprint(train_human_names[:10])\n\nprint('total training horse images:', len(os.listdir(train_horse_dir)))\nprint('total training human images:', len(os.listdir(train_human_dir)))","99525664":"%matplotlib inline\n \nimport matplotlib.pyplot as plt\nimport matplotlib.image as mpimg\n \n# Parameters for our graph; we'll output images in a 4x4 configuration\nnrows = 4\nncols = 4\n \n# Index for iterating over images\npic_index = 0\n\n# Set up matplotlib fig, and size it to fit 4x4 pics\nfig = plt.gcf()\nfig.set_size_inches(ncols * 4, nrows * 4)\n \npic_index += 8\nnext_horse_pix = [os.path.join(train_horse_dir, fname) \n                for fname in train_horse_names[pic_index-8:pic_index]]\nnext_human_pix = [os.path.join(train_human_dir, fname) \n                for fname in train_human_names[pic_index-8:pic_index]]\n \nfor i, img_path in enumerate(next_horse_pix+next_human_pix):\n    # Set up subplot; subplot indices start at 1\n    sp = plt.subplot(nrows, ncols, i + 1)\n    sp.axis('Off') # Don't show axes (or gridlines)\n\n    img = mpimg.imread(img_path)\n    plt.imshow(img)\n\nplt.show()\n","2603cd50":"import tensorflow as tf\nfrom tensorflow.keras.optimizers import RMSprop\n\nmodel = tf.keras.models.Sequential([\n    # Note the input shape is the desired size of the image 300x300 with 3 bytes color\n    # This is the first convolution\n    tf.keras.layers.Conv2D(16, (3,3), activation='relu', input_shape=(300, 300, 3)),\n    tf.keras.layers.MaxPooling2D(2, 2),\n    # The second convolution\n    tf.keras.layers.Conv2D(32, (3,3), activation='relu'),\n    tf.keras.layers.MaxPooling2D(2,2),\n    # The third convolution\n    tf.keras.layers.Conv2D(64, (3,3), activation='relu'),\n    tf.keras.layers.MaxPooling2D(2,2),\n    # The fourth convolution\n    tf.keras.layers.Conv2D(64, (3,3), activation='relu'),\n    tf.keras.layers.MaxPooling2D(2,2),\n    # The fifth convolution\n    tf.keras.layers.Conv2D(64, (3,3), activation='relu'),\n    tf.keras.layers.MaxPooling2D(2,2),\n    # Flatten the results to feed into a DNN\n    tf.keras.layers.Flatten(),\n    # 512 neuron hidden layer\n    tf.keras.layers.Dense(512, activation='relu'),\n    # Only 1 output neuron. It will contain a value from 0-1 where 0 for 1 class ('horses') and 1 for the other ('humans')\n    tf.keras.layers.Dense(1, activation='sigmoid')\n])\nmodel.compile(loss='binary_crossentropy',\n              optimizer=RMSprop(lr=0.001),\n              metrics=['acc'])\n\nmodel.summary()","23ed84ec":"from tensorflow.keras.preprocessing.image import ImageDataGenerator\n \n# All images will be rescaled by 1.\/255\ntrain_datagen = ImageDataGenerator(rescale=1.\/255)\n \n# Flow training images in batches of 128 using train_datagen generator\ntrain_generator = train_datagen.flow_from_directory(\n        '\/tmp\/horse-or-human\/',  # This is the source directory for training images\n        target_size=(300, 300),  # All images will be resized to 150x150\n        batch_size=128,\n        # Since we use binary_crossentropy loss, we need binary labels\n        class_mode='binary')","859bed34":"history = model.fit(\n      train_generator,\n      steps_per_epoch=8,  \n      epochs=15,\n      verbose=1)","df019118":"# import numpy as np\n# from google.colab import files\n# from keras.preprocessing import image\n \n# uploaded = files.upload()\n \n# for fn in uploaded.keys():\n \n#     # predicting images\n#     path = '\/content\/' + fn\n#     img = image.load_img(path, target_size=(300, 300))\n#     x = image.img_to_array(img)\n#     x = np.expand_dims(x, axis=0)\n\n#     images = np.vstack([x])\n#     classes = model.predict(images, batch_size=10)\n#     print(classes[0])\n#     if classes[0]>0.5:\n#         print(fn + \" is a human\")\n#     else:\n#         print(fn + \" is a horse\")","ce454c2f":"import numpy as np\nimport random\nfrom tensorflow.keras.preprocessing.image import img_to_array, load_img\n \n# Let's define a new Model that will take an image as input, and will output\n# intermediate representations for all layers in the previous model after\n# the first.\nsuccessive_outputs = [layer.output for layer in model.layers[1:]]\n#visualization_model = Model(img_input, successive_outputs)\nvisualization_model = tf.keras.models.Model(inputs = model.input, outputs = successive_outputs)\n# Let's prepare a random input image from the training set.\nhorse_img_files = [os.path.join(train_horse_dir, f) for f in train_horse_names]\nhuman_img_files = [os.path.join(train_human_dir, f) for f in train_human_names]\nimg_path = random.choice(horse_img_files + human_img_files)\n \nimg = load_img(img_path, target_size=(300, 300))  # this is a PIL image\nx = img_to_array(img)  # Numpy array with shape (150, 150, 3)\nx = x.reshape((1,) + x.shape)  # Numpy array with shape (1, 150, 150, 3)\n \n# Rescale by 1\/255\nx \/= 255\n \n# Let's run our image through our network, thus obtaining all\n# intermediate representations for this image.\nsuccessive_feature_maps = visualization_model.predict(x)\n \n# These are the names of the layers, so can have them as part of our plot\nlayer_names = [layer.name for layer in model.layers]\n \n# Now let's display our representations\nfor layer_name, feature_map in zip(layer_names, successive_feature_maps):\n    if len(feature_map.shape) == 4:\n        # Just do this for the conv \/ maxpool layers, not the fully-connected layers\n        n_features = feature_map.shape[-1]  # number of features in feature map\n        # The feature map has shape (1, size, size, n_features)\n        size = feature_map.shape[1]\n        # We will tile our images in this matrix\n        display_grid = np.zeros((size, size * n_features))\n        for i in range(n_features):\n            # Postprocess the feature to make it visually palatable\n            x = feature_map[0, :, :, i]\n            x -= x.mean()\n            if x.std()>0:\n                x \/= x.std()\n            x *= 64\n            x += 128\n            x = np.clip(x, 0, 255).astype('uint8')\n            # We'll tile each filter into this big horizontal grid\n            display_grid[:, i * size : (i + 1) * size] = x\n        # Display the grid\n        scale = 20. \/ n_features\n        plt.figure(figsize=(scale * n_features, scale))\n        plt.title(layer_name)\n        plt.grid(False)\n        plt.imshow(display_grid, aspect='auto', cmap='viridis')","89db8ac2":"import os\nimport zipfile\nimport random\nimport tensorflow as tf\nfrom tensorflow.keras.optimizers import RMSprop\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom shutil import copyfile","0dfcff00":"# If the URL doesn't work, visit https:\/\/www.microsoft.com\/en-us\/download\/confirmation.aspx?id=54765\n# And right click on the 'Download Manually' link to get a new URL to the dataset \n# Note: This is a very large dataset and will take time to download\n!wget --no-check-certificate \"https:\/\/download.microsoft.com\/download\/3\/E\/1\/3E1C3F21-ECDB-4869-8368-6DEBA77B919F\/kagglecatsanddogs_3367a.zip\" -O \"\/tmp\/cats-and-dogs.zip\"\nlocal_zip = '\/tmp\/cats-and-dogs.zip'\nzip_ref   = zipfile.ZipFile(local_zip, 'r')\nzip_ref.extractall('\/tmp')\nzip_ref.close()\nprint(len(os.listdir('\/tmp\/PetImages\/Cat\/')))\nprint(len(os.listdir('\/tmp\/PetImages\/Dog\/'))) \n# Expected Output:\n# 12501\n# 12501","5bea8bc7":"try:\n    os.mkdir('\/tmp\/cats-v-dogs')\n    os.mkdir('\/tmp\/cats-v-dogs\/training')\n    os.mkdir('\/tmp\/cats-v-dogs\/testing')\n    os.mkdir('\/tmp\/cats-v-dogs\/training\/cats')\n    os.mkdir('\/tmp\/cats-v-dogs\/training\/dogs')\n    os.mkdir('\/tmp\/cats-v-dogs\/testing\/cats')\n    os.mkdir('\/tmp\/cats-v-dogs\/testing\/dogs')\nexcept OSError:\n    pass\n\ndef split_data(SOURCE, TRAINING, TESTING, SPLIT_SIZE):\n    files = []\n    for filename in os.listdir(SOURCE):\n        file = SOURCE + filename\n        if os.path.getsize(file) > 0:\n            files.append(filename)\n        else:\n            print(filename + \" is zero length, so ignoring.\")\n \n    training_length = int(len(files) * SPLIT_SIZE)\n    testing_length = int(len(files) - training_length)\n    shuffled_set = random.sample(files, len(files))\n    training_set = shuffled_set[0:training_length]\n    testing_set = shuffled_set[:testing_length]\n \n    for filename in training_set:\n        this_file = SOURCE + filename\n        destination = TRAINING + filename\n        copyfile(this_file, destination)\n \n    for filename in testing_set:\n        this_file = SOURCE + filename\n        destination = TESTING + filename\n        copyfile(this_file, destination)\n \n \nCAT_SOURCE_DIR = \"\/tmp\/PetImages\/Cat\/\"\nTRAINING_CATS_DIR = \"\/tmp\/cats-v-dogs\/training\/cats\/\"\nTESTING_CATS_DIR = \"\/tmp\/cats-v-dogs\/testing\/cats\/\"\nDOG_SOURCE_DIR = \"\/tmp\/PetImages\/Dog\/\"\nTRAINING_DOGS_DIR = \"\/tmp\/cats-v-dogs\/training\/dogs\/\"\nTESTING_DOGS_DIR = \"\/tmp\/cats-v-dogs\/testing\/dogs\/\"\n \nsplit_size = .9\nsplit_data(CAT_SOURCE_DIR, TRAINING_CATS_DIR, TESTING_CATS_DIR, split_size)\nsplit_data(DOG_SOURCE_DIR, TRAINING_DOGS_DIR, TESTING_DOGS_DIR, split_size)\n# Expected output\n# 666.jpg is zero length, so ignoring\n# 11702.jpg is zero length, so ignoring","c00735c6":"print(len(os.listdir('\/tmp\/cats-v-dogs\/training\/cats\/')))\nprint(len(os.listdir('\/tmp\/cats-v-dogs\/training\/dogs\/')))\nprint(len(os.listdir('\/tmp\/cats-v-dogs\/testing\/cats\/')))\nprint(len(os.listdir('\/tmp\/cats-v-dogs\/testing\/dogs\/')))\n# Expected output:\n# 11250\n# 11250\n# 1250\n# 1250","ff46d11b":"model = tf.keras.models.Sequential([\n    tf.keras.layers.Conv2D(16, (3, 3), activation='relu', input_shape=(150, 150, 3)),\n    tf.keras.layers.MaxPooling2D(2, 2),\n    tf.keras.layers.Conv2D(32, (3, 3), activation='relu'),\n    tf.keras.layers.MaxPooling2D(2, 2),\n    tf.keras.layers.Conv2D(64, (3, 3), activation='relu'),\n    tf.keras.layers.MaxPooling2D(2, 2),\n    tf.keras.layers.Flatten(),\n    tf.keras.layers.Dense(512, activation='relu'),\n    tf.keras.layers.Dense(1, activation='sigmoid')\n]) \nmodel.compile(optimizer=RMSprop(lr=0.001), loss='binary_crossentropy', metrics=['accuracy'])","4ad303d6":"TRAINING_DIR = \"\/tmp\/cats-v-dogs\/training\/\"\ntrain_datagen = ImageDataGenerator(rescale=1.0\/255.)\ntrain_generator = train_datagen.flow_from_directory(TRAINING_DIR,\n                                                    batch_size=100,\n                                                    class_mode='binary',\n                                                    target_size=(150, 150))\n \nVALIDATION_DIR = \"\/tmp\/cats-v-dogs\/testing\/\"\nvalidation_datagen = ImageDataGenerator(rescale=1.0\/255.)\nvalidation_generator = validation_datagen.flow_from_directory(VALIDATION_DIR,\n                                                              batch_size=100,\n                                                              class_mode='binary',\n                                                              target_size=(150, 150))\n \n# Expected Output:\n# Found 22498 images belonging to 2 classes.\n# Found 2500 images belonging to 2 classes.","5a5dae61":"# Note that this may take some time.\nhistory = model.fit_generator(train_generator,\n                              epochs=15,\n                              verbose=1,\n                              validation_data=validation_generator)","9eb0b1ac":"%matplotlib inline\nimport matplotlib.image  as mpimg\nimport matplotlib.pyplot as plt\n#-----------------------------------------------------------\n# Retrieve a list of list results on training and test data\n# sets for each training epoch\n#-----------------------------------------------------------\nacc=history.history['accuracy']\nval_acc=history.history['val_accuracy']\nloss=history.history['loss']\nval_loss=history.history['val_loss']\n \nepochs=range(len(acc)) # Get number of epochs\n \n#------------------------------------------------\n# Plot training and validation accuracy per epoch\n#------------------------------------------------\nplt.plot(epochs, acc, 'r', \"Training Accuracy\")\nplt.plot(epochs, val_acc, 'b', \"Validation Accuracy\")\nplt.title('Training and validation accuracy')\nplt.figure()\n \n#------------------------------------------------\n# Plot training and validation loss per epoch\n#------------------------------------------------\nplt.plot(epochs, loss, 'r', \"Training Loss\")\nplt.plot(epochs, val_loss, 'b', \"Validation Loss\")\nplt.figure()","4fdff6ea":"# # Here's a codeblock just for fun. You should be able to upload an image here \n# # and have it classified without crashing\n# import numpy as np\n# from google.colab import files\n# from keras.preprocessing import image\n \n# uploaded = files.upload()\n \n# for fn in uploaded.keys():\n \n#     # predicting images\n#     path = '\/content\/' + fn\n#     img = image.load_img(path, target_size=(150, 150))\n#     x = image.img_to_array(img)\n#     x = np.expand_dims(x, axis=0)\n\n#     images = np.vstack([x])\n#     classes = model.predict(images, batch_size=10)\n#     print(classes[0])\n#     if classes[0]>0.5:\n#         print(fn + \" is a dog\")\n#     else:\n#         print(fn + \" is a cat\")","a32355ba":"<a id=\"section-three\"><\/a>\n## Introduction to Convolutions","9e86646f":"<a id=\"section-one\"><\/a>\n## The Hello World of Machine Learning","1254ee4f":"![obraz.png](attachment:05f3bbdc-fc4d-40ea-a25a-4944d3934837.png)","81dceb4a":"***\n\n\ud83e\udd14 Difference between Dense and Activation layer in Keras\nhttps:\/\/stackoverflow.com\/questions\/40866124\/difference-between-dense-and-activation-layer-in-keras\n\n\ud83e\udd14 What is the difference between sparse_categorical_crossentropy and categorical_crossentropy?\nhttps:\/\/stackoverflow.com\/questions\/58565394\/what-is-the-difference-between-sparse-categorical-crossentropy-and-categorical-c\n\n***","97dddc72":"***   \n* `Sequential` defines a sequence of layers in the neural network.\n* `Flatten` takes a square and turns it into a one-dimensional vector.\n* `Dense` adds a layer of neurons.\n* `Activation functions` tell each layer of neurons what to do. There are lots of options, but use these for now:\n* `Relu` effectively means that if X is greater than 0 return X, else return 0. It only passes values of 0 or greater to the next layer in the network.\n* `Softmax` takes a set of values, and effectively picks the biggest one. For example, if the output of the last layer looks like [0.1, 0.1, 0.05, 0.1, 9.5, 0.1, 0.05, 0.05, 0.05], then it saves you from having to sort for the largest value\u2014it returns [0,0,0,0,1,0,0,0,0].\n***","bb8b13f4":"### Exercises\n#### Exercise 1\n\nTry editing the convolutions. Change the number of convolutions from 32 to either 16 or 64. What impact does that have on accuracy and training time?\n#### Exercise 2\n\nRemove the final convolution. What impact does that have on accuracy or training time?\n#### Exercise 3\n\nAdd more convolutions. What impact does that have?\n#### Exercise 4\n\nRemove all convolutions but the first. What impact does that have? Experiment with it.","f347bac6":"# Program neural networks with TensorFlow\n\nThis is a notebook with content from https:\/\/developers.google.com\/learn\/pathways\/tensorflow with some additional links I found interesting during the completion of the course. Completing all the lessons on [Google Developers platform](https:\/\/developers.google.com\/) will earn you a fabulous badge! \ud83c\udf96\n\n***\n> Learn everything that you need to know to demystify machine learning, from the first principles in the new programming paradigm to creating convolutional neural networks for advanced image recognition and classification that solve common computer-vision problems.\n\n\n* [The Hello World of Machine Learning](#section-one)\n* [Introduction to Computer Vision](#section-two)\n* [Introduction to Convolutions](#section-three)\n* [Convolutional Neural Networks (CNNs)](#section-four)\n* [Complex Images](#section-five)\n* [Use CNNS with larger datasets](#section-six)","35d23858":"<a id=\"section-six\"><\/a>\n## Use CNNS with larger datasets","6fd8c54e":"#### Model","4b407a12":"<a id=\"section-five\"><\/a>\n## Complex Images","260c662c":"<a id=\"section-two\"><\/a>\n## Introduction to Computer Vision","a9d52afe":"> **Note:** In this case, using the RMSprop optimization algorithm is preferable to stochastic gradient descent (SGD) because RMSprop automates learning-rate tuning for you. (Other optimizers, such as Adam and Adagrad, also automatically adapt the learning rate during training and would work equally well here.)","b6cd1865":"*** \n\n\ud83e\udd14 Why should we normalize data for deep learning in Keras?\nhttps:\/\/stackoverflow.com\/questions\/48284427\/why-should-we-normalize-data-for-deep-learning-in-keras","30bc66af":"<a id=\"section-four\"><\/a>\n## Convolutional Neural Networks (CNNs)","744eda7f":"***\n\n\ud83e\udd14 What is \u201cmetrics\u201d in Keras?\nhttps:\/\/stackoverflow.com\/questions\/47302085\/what-is-metrics-in-keras\/47306502#47306502\n\n\ud83e\udd14 Loss function and evaluation metric\nhttps:\/\/stats.stackexchange.com\/questions\/379264\/loss-function-and-evaluation-metric\n\n***"}}