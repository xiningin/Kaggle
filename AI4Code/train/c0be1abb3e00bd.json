{"cell_type":{"bab5c84d":"code","0df91f8f":"code","b7cd364c":"code","e3a90943":"code","203b211b":"code","9f5de9d8":"code","9cee8c3f":"code","35f3c10a":"code","0b57b7ee":"code","758008b6":"code","b9a85a74":"code","ae50d1a8":"code","bf7add9e":"code","d5339f03":"code","ca71dcab":"code","1e35a2d9":"code","c788bf28":"code","489ee11c":"code","ee577c15":"code","cd3e2891":"code","7765e340":"markdown","b336a7cf":"markdown","d368b84a":"markdown"},"source":{"bab5c84d":"import matplotlib.pyplot as plt\nimport tensorflow as tf\nfrom tensorflow import keras\nimport numpy as np","0df91f8f":"mnist = keras.datasets.fashion_mnist.load_data()","b7cd364c":"X, y = mnist[0], mnist[1]\nX_train, y_train, X_test, y_test = X[0], X[1], y[0], y[1]","e3a90943":"np.random.seed(42)\ntf.random.set_seed(42)\nkeras.backend.clear_session()\n\nX_train, X_valid, y_train, y_valid = X_train[:55000], X_train[55000:], y_train[:55000], y_train[55000:]","203b211b":"print(\"Shape of X_train:\",X_train.shape)\nprint(\"Shape of y_train:\",y_train.shape)\nprint(\"\\nShape of X_valid:\",X_valid.shape)\nprint(\"Shape of y_valid:\",y_valid.shape)\nprint(\"\\nShape of X_test:\",X_test.shape)\nprint(\"Shape of y_test:\",y_test.shape)","9f5de9d8":"train_set = tf.data.Dataset.from_tensor_slices((X_train, y_train)).shuffle(len(X_train))\nvalid_set = tf.data.Dataset.from_tensor_slices((X_valid, y_valid))\ntest_set = tf.data.Dataset.from_tensor_slices((X_test, y_test))","9cee8c3f":"BytesList = tf.train.BytesList\nFloatList = tf.train.FloatList\nInt64List = tf.train.Int64List\nFeature = tf.train.Feature\nFeatures = tf.train.Features\nExample = tf.train.Example","35f3c10a":"def create_example(image, label):\n    image_data = tf.io.serialize_tensor(image)\n    example = Example(\n        features=Features(\n            feature={\n                \"image\": Feature(bytes_list=BytesList(value=[image_data.numpy()])),\n                \"label\": Feature(int64_list=Int64List(value=[label]))\n            }\n        )\n    )\n    return example","0b57b7ee":"for image, label in train_set.take(1):\n    example = create_example(image, label)\n    print(example)","758008b6":"def create_tf_record(set_, filename):\n    with tf.io.TFRecordWriter(\"%s.tfrecord\" %filename) as f:\n        for image, label in set_:\n            example = create_example(image, label)\n            f.write(example.SerializeToString())","b9a85a74":"create_tf_record(train_set, \"train_data\")","ae50d1a8":"create_tf_record(valid_set, \"valid_data\")","bf7add9e":"create_tf_record(test_set, \"test_data\")","d5339f03":"def preprocess(tfrecord):\n    feature_descriptions = {\n        \"image\": tf.io.FixedLenFeature([], tf.string, default_value=\"\"),\n        \"label\": tf.io.FixedLenFeature([], tf.int64, default_value=-1)\n    }\n    example = tf.io.parse_single_example(tfrecord, feature_descriptions)\n    image = tf.io.parse_tensor(example[\"image\"], out_type=tf.uint8)\n    #image = tf.io.decode_jpeg(example[\"image\"])\n    image = tf.reshape(image, shape=[28, 28])\n    return image, example[\"label\"]\n\ndef mnist_dataset(filepaths, n_read_threads=5, shuffle_buffer_size=None,\n                  n_parse_threads=5, batch_size=32, cache=True):\n    dataset = tf.data.TFRecordDataset(filepaths,\n                                      num_parallel_reads=n_read_threads)\n    if cache:\n        dataset = dataset.cache()\n    if shuffle_buffer_size:\n        dataset = dataset.shuffle(shuffle_buffer_size)\n    dataset = dataset.map(preprocess, num_parallel_calls=n_parse_threads)\n    dataset = dataset.batch(batch_size)\n    return dataset.prefetch(1)","ca71dcab":"train_set = mnist_dataset(\"train_data.tfrecord\")\nvalid_set = mnist_dataset(\"valid_data.tfrecord\")\ntest_set = mnist_dataset(\"test_data.tfrecord\")","1e35a2d9":"for X, y in train_set.take(1):\n    for i in range(5):\n        plt.subplot(1, 5, i + 1)\n        plt.imshow(X[i].numpy(), cmap=\"binary\")\n        plt.axis(\"off\")\n        plt.title(str(y[i].numpy()))","c788bf28":"norm_layer = keras.layers.Normalization(input_shape=[28, 28])\n\nsample_image_batches = train_set.take(100).map(lambda image, label: image)\nsample_images = np.concatenate(list(sample_image_batches.as_numpy_iterator()),\n                               axis=0).astype(np.float32)\n\nnorm_layer.adapt(sample_images)","489ee11c":"model = keras.Sequential([\n    norm_layer,\n    keras.layers.Flatten(),\n    keras.layers.Dense(100, activation=\"relu\"),\n    keras.layers.Dense(10, activation=\"softmax\")\n]) \nmodel.compile(loss=\"sparse_categorical_crossentropy\",\n              optimizer=\"nadam\", metrics=[\"accuracy\"])","ee577c15":"import os\nfrom datetime import datetime\n\nlogs = os.path.join(os.curdir, \"logs\",\n                    \"run_\" + datetime.now().strftime(\"%Y%m%d-%H%M%S\"))\n\ntensorboard_cb = keras.callbacks.TensorBoard(log_dir=logs,\n                                             histogram_freq=1, profile_batch=10)\nmodel.fit(train_set, epochs=5, validation_data=valid_set,\n          callbacks=[tensorboard_cb])                           ","cd3e2891":"%tensorboard --logdir=.\/logs --port=6006","7765e340":"# How to Read Data Efficiently with TensorFlow?","b336a7cf":"### Dependencies","d368b84a":"# Preprocessing and Training our Model"}}