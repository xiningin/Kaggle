{"cell_type":{"1f47fe5d":"code","f7df54aa":"code","5dc0fc79":"code","eb4765fc":"code","f6e160e7":"code","6dfb30cb":"code","2c12f0cd":"code","1bfb4f2a":"code","14b573e2":"code","f7f10811":"code","afee510c":"code","04de7e25":"code","445fc511":"code","8275c855":"code","5cbf6065":"code","6aeb4c88":"code","3cc8c59d":"code","cb3a97e4":"code","9fea1169":"code","f16cf997":"code","c2c8cf99":"code","1fb36382":"code","239427f4":"code","46202582":"code","3c16c9a2":"code","3e593d91":"code","1853b2bf":"code","ebfa3f2e":"code","d75c279a":"code","c0cc4bf8":"markdown","f60b6aff":"markdown","4a028687":"markdown","1da57a3c":"markdown"},"source":{"1f47fe5d":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\n!pip install -i https:\/\/test.pypi.org\/simple\/  litemort==0.1.7\nfrom LiteMORT import *\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport gc\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","f7df54aa":"pd.set_option('display.max_rows', 500)\npd.set_option('display.max_columns', 100)\n\nfrom itertools import product\nfrom sklearn.preprocessing import LabelEncoder\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nfrom xgboost import XGBRegressor\nfrom xgboost import plot_importance\n\ndef plot_features(booster, figsize):    \n    fig, ax = plt.subplots(1,1,figsize=figsize)\n    return plot_importance(booster=booster, ax=ax)\n\nimport time\nimport sys\nimport gc\nimport pickle","5dc0fc79":"item_categories = pd.read_csv(\"..\/input\/competitive-data-science-predict-future-sales\/item_categories.csv\")\nitems = pd.read_csv(\"..\/input\/competitive-data-science-predict-future-sales\/items.csv\")\nsales_train = pd.read_csv(\"..\/input\/competitive-data-science-predict-future-sales\/sales_train.csv\")\nsample_submission = pd.read_csv(\"..\/input\/competitive-data-science-predict-future-sales\/sample_submission.csv\")\nshops = pd.read_csv(\"..\/input\/competitive-data-science-predict-future-sales\/shops.csv\")\ntest = pd.read_csv(\"..\/input\/competitive-data-science-predict-future-sales\/test.csv\")","eb4765fc":"sales_train['item_price'].max()","f6e160e7":"plt.figure(figsize=(10,4))\nplt.xlim(-100, 3000)\nsns.boxplot(x=sales_train.item_cnt_day)\n\nplt.figure(figsize=(10,4))\nplt.xlim(sales_train.item_price.min(), sales_train.item_price.max()*1.1)\nsns.boxplot(x=sales_train.item_price)","6dfb30cb":"sales_train = sales_train[sales_train.item_price<100000]\nsales_train = sales_train[sales_train.item_cnt_day<1001]","2c12f0cd":"median = sales_train[(sales_train.shop_id==32)&(sales_train.item_id==2973)&(sales_train.date_block_num==4)&(sales_train.item_price>0)].item_price.median()\nsales_train.loc[sales_train.item_price<0, 'item_price'] = median","1bfb4f2a":"# \u042f\u043a\u0443\u0442\u0441\u043a \u041e\u0440\u0434\u0436\u043e\u043d\u0438\u043a\u0438\u0434\u0437\u0435, 56\nsales_train.loc[sales_train.shop_id == 0, 'shop_id'] = 57\ntest.loc[test.shop_id == 0, 'shop_id'] = 57\n# \u042f\u043a\u0443\u0442\u0441\u043a \u0422\u0426 \"\u0426\u0435\u043d\u0442\u0440\u0430\u043b\u044c\u043d\u044b\u0439\"\nsales_train.loc[sales_train.shop_id == 1, 'shop_id'] = 58\ntest.loc[test.shop_id == 1, 'shop_id'] = 58\n# \u0416\u0443\u043a\u043e\u0432\u0441\u043a\u0438\u0439 \u0443\u043b. \u0427\u043a\u0430\u043b\u043e\u0432\u0430 39\u043c\u00b2\nsales_train.loc[sales_train.shop_id == 10, 'shop_id'] = 11\ntest.loc[test.shop_id == 10, 'shop_id'] = 11","14b573e2":"shops.loc[shops.shop_name == '\u0421\u0435\u0440\u0433\u0438\u0435\u0432 \u041f\u043e\u0441\u0430\u0434 \u0422\u0426 \"7\u042f\"', 'shop_name'] = '\u0421\u0435\u0440\u0433\u0438\u0435\u0432\u041f\u043e\u0441\u0430\u0434 \u0422\u0426 \"7\u042f\"'\nshops['city'] = shops['shop_name'].str.split(' ').map(lambda x: x[0])\nshops.loc[shops.city == '!\u042f\u043a\u0443\u0442\u0441\u043a', 'city'] = '\u042f\u043a\u0443\u0442\u0441\u043a'\nshops['city_code'] = LabelEncoder().fit_transform(shops['city'])\nshops = shops[['shop_id','city_code']]\n\nitem_categories['split'] = item_categories['item_category_name'].str.split('-')\nitem_categories['type'] = item_categories['split'].map(lambda x: x[0].strip())\nitem_categories['type_code'] = LabelEncoder().fit_transform(item_categories['type'])\n# if subtype is nan then type\nitem_categories['subtype'] = item_categories['split'].map(lambda x: x[1].strip() if len(x) > 1 else x[0].strip())\nitem_categories['subtype_code'] = LabelEncoder().fit_transform(item_categories['subtype'])\nitem_categories = item_categories[['item_category_id','type_code', 'subtype_code']]\n\nitems = pd.merge(items, item_categories, on=['item_category_id'], how='left')\nsales_train = pd.merge(sales_train, shops, on=['shop_id'], how='left')\nsales_train = pd.merge(sales_train, items, on=['item_id'], how='left')\ntest = pd.merge(test, shops, on=['shop_id'], how='left')\ntest = pd.merge(test, items, on=['item_id'], how='left')\n\nsales_train.drop(['item_name'], axis=1, inplace=True)\ntest.drop(['item_name'], axis=1, inplace=True)\ndel shops\ndel item_categories\ndel items\ngc.collect()","f7f10811":"train = sales_train.drop(['date'], axis = 1)\ngrouped_train = train.groupby(['date_block_num', 'shop_id', 'item_id', 'item_price'], \n                              as_index=False).sum()\ngrouped_train['total_sales'] = grouped_train['item_price'] * grouped_train['item_cnt_day']\nsales_train.shape, grouped_train.shape","afee510c":"del sales_train\ndel train\ngc.collect()","04de7e25":"train_in_test = grouped_train[grouped_train['item_id'].isin(test['item_id'])]","445fc511":"del grouped_train\ngc.collect()","8275c855":"def downcast_dtypes(df):\n    float_cols = [c for c in df if df[c].dtype == \"float64\"]\n    int_cols = [c for c in df if df[c].dtype in [\"int64\", \"int32\", \"int16\"]]\n    df[float_cols] = df[float_cols].astype(np.float32)\n    df[int_cols] = df[int_cols].astype(np.int8)\n    return df","5cbf6065":"downcast_train = downcast_dtypes(train_in_test)\ndowncast_test = downcast_dtypes(test)","6aeb4c88":"del train_in_test\ngc.collect()","3cc8c59d":"downcast_train.shape, downcast_test.shape","cb3a97e4":"downcast_train.info()","9fea1169":"train_stats = downcast_train.describe()\ntrain_stats = train_stats.transpose()\ndef norm(x):\n  return (x - train_stats['mean']) \/ train_stats['std']\nnormed_train_data = norm(downcast_train)\nnormed_test_data = norm(downcast_test)\nnormed_test_data=normed_test_data.drop(['ID', 'date_block_num', 'item_cnt_day', 'item_price', 'total_sales'], axis=1)","f16cf997":"normed_train_data['date_block_num'].unique()","c2c8cf99":"normed_train_data.head()","1fb36382":"normed_test_data.head()","239427f4":"X_train = normed_train_data[normed_train_data.date_block_num < 1.4].drop(['item_price', 'item_cnt_day', 'total_sales'], axis=1)\nY_train = normed_train_data[normed_train_data.date_block_num < 1.4]['total_sales']\nX_valid = normed_train_data[normed_train_data.date_block_num > 1.4].drop(['item_price', 'item_cnt_day', 'total_sales'], axis=1)\nY_valid = normed_train_data[normed_train_data.date_block_num > 1.4]['total_sales']\nX_test = normed_test_data[['shop_id', 'item_id', 'city_code', 'item_category_id', 'type_code', 'subtype_code']]\nX_test.insert(0, 'date_block_num', 1.5)","46202582":"del downcast_train\ndel downcast_test\ngc.collect()","3c16c9a2":"X_train.shape","3e593d91":"X_train.fillna(0, inplace=True)\nY_train.fillna(0, inplace=True)\nX_test.fillna(0, inplace=True)","1853b2bf":"params={'num_leaves': 550,   \n        'n_estimators':1000,\n        'early_stopping_rounds':20,\n        'feature_fraction': 1,     \n        'bagging_fraction': 1,\n        'max_bin': 512,\n        'max_depth': 10,\n        'min_child_weight': 300,    #'min_data_in_leaf': 300,\n        'learning_rate': 0.1,\n        'objective': 'regression',\n        'boosting_type': 'gbdt',\n        'verbose': 1,\n        'metric': {'rmse'}\n}\n\nprint(f\"Call LiteMORT... \")    \nt0=time.time()\nmodel = LiteMORT(params).fit(X_train,Y_train,eval_set=[(X_valid, Y_valid)])\nprint(f\"LiteMORT......OK time={time.time()-t0:.4g} model={model}\")\n\n#Y_pred = model.predict(X_valid).clip(0, 20)\n#score = np.sqrt(mean_squared_error(Y_pred, Y_valid))\n#Y_test = model.predict(X_test).clip(0, 20)\n#print(f\"score={score}\")","ebfa3f2e":"#model = XGBRegressor(\n#    max_depth=8,\n#    n_estimators=1000,\n#    min_child_weight=300,\n#    colsample_bytree=0.8, \n#    subsample=0.8, \n#    eta=0.3,    \n#    seed=42)\n\n#model.fit(\n#    X_train, \n#    Y_train, \n#    eval_metric=\"rmse\", \n#    eval_set=[(X_train, Y_train), (X_valid, Y_valid)], \n#    verbose=True, \n#    early_stopping_rounds = 10)","d75c279a":"from sklearn.metrics import mean_squared_error\n\nY_pred = model.predict(X_valid).clip(0, 20)\nY_test = model.predict(X_test).clip(0, 20)\nscore = np.sqrt(mean_squared_error(Y_pred, Y_valid))\nprint(score)\nsubmission = pd.DataFrame({\n    \"ID\": test.index, \n    \"item_cnt_month\": Y_test\n})\nsubmission.to_csv('xgb_submission.csv', index=False)","c0cc4bf8":"The first thing I thought was grouping the sales_train products by month and shop_id, considering that date_block_num is a number representing month. I decided to drop the date column, does not seems important, because the date is for a day and I need for month.\n\nAnd it looks like I need to multiply item_price by item_cnt_day to get total sales.","f60b6aff":"Remove Outliers","4a028687":"There is one item with price below zero. Fill it with median.","1da57a3c":"Several shops are duplicates of each other (according to its name). Fix train and test set."}}