{"cell_type":{"4a5d7ab2":"code","63ba7899":"code","cbbb5738":"code","bf741928":"code","d3bd695b":"code","57b510bb":"code","41baf4cf":"code","43090213":"code","973c1a7f":"code","dbcf1562":"code","6d249068":"code","60b0a577":"code","e75e7a5e":"code","44314250":"code","2221d2a6":"code","d0a10d02":"code","9f641a7e":"code","ffd2ce2a":"code","2a457488":"code","9be34318":"markdown","36c1c774":"markdown","cf9a0231":"markdown","1faf9ce3":"markdown","bc32ca78":"markdown","bbf31505":"markdown","129043de":"markdown","d664f2aa":"markdown","96f4dd0d":"markdown","e44b8c4f":"markdown","55316747":"markdown","3b9b01ee":"markdown","8cbfc7d2":"markdown","b46b4b87":"markdown","0c83109e":"markdown","9de09b35":"markdown","8663313b":"markdown","005c2e76":"markdown"},"source":{"4a5d7ab2":"\nimport numpy as np \nimport pandas as pd \nimport seaborn as sb \nimport matplotlib.pyplot as plt \n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current sesson","63ba7899":"# Loading  all the datasets \nratings=pd.read_csv('\/kaggle\/input\/the-movies-dataset\/ratings.csv')\ncredits=pd.read_csv('\/kaggle\/input\/the-movies-dataset\/credits.csv')\nmetadata=pd.read_csv('\/kaggle\/input\/the-movies-dataset\/movies_metadata.csv')\nlinks=pd.read_csv('\/kaggle\/input\/the-movies-dataset\/links.csv')\n","cbbb5738":"df = metadata.copy()\n#show the head \ndf.head()","bf741928":"#get the shape of the dataframe\ndf.shape","d3bd695b":"#get some informations about the dataset\ndf.info()","57b510bb":"#there are NaN values in some features down here, we will deal with it later. \ndf.isnull().sum()","41baf4cf":"#get the features of the datframe \ncols= list(df.columns)\nprint(cols)","43090213":"#compute the parameters above:\n\n## Calculate mean of vote average column\nC = df['vote_average'].mean()\n##Compute the minimum number of votes required to be in the chart, m\nm = df['vote_count'].quantile(0.90)\n#compute the avergage rating of the movie \nR = df['vote_average']\n#compute vote count of the movie .\nv= df['vote_count']\n\n#define a function to compute the scores \ndef get_weighting_rate(df,C=C,m=m):\n    \n    R = df['vote_average']\n    v= df['vote_count']\n    wr =  (v\/(v+m) * R) + (m\/(m+v) * C)\n    return wr\n\n#filtering the movies to be scored \nm_movies = df.loc[df['vote_count'] >= m]\n# Compute the score for every movie in the dataset \n\nm_movies['score']=m_movies.apply(get_weighting_rate,axis=1)\n\n#show the movie title and its score  \nm_movies[['title','score']].head(10)\n    ","973c1a7f":"#now sort the movies by thier score \nm_movies = m_movies.sort_values('score', ascending=False)\n#show the top 20 scored movies \nm_movies[['title','vote_count','vote_average','score']].head(20) ","dbcf1562":"\n#plot the top twenty (you can top whatever you want) movies as bar plot \ntopten_scores=m_movies['score'].head(20) \n#get the titles \ntopten_titles=m_movies['title'].head(20) \n#plot the top twenty movies \nsb.set_style('whitegrid')\nplt.figure(figsize=(12,8))\nplt.barh(topten_titles,topten_scores, align='center',color='#C6870A')\nplt.gca().invert_yaxis()\nplt.xlabel(\" Movie Scores\")\nplt.title(\"Top Ten Movies\")","6d249068":"#Print plot overviews of the first 5 movies.\nplots= df['overview']\nplots.head()","60b0a577":"plots.shape","e75e7a5e":"plots=plots[:20000]\nplots.shape","44314250":"#import TF-IDF Vectorizer\nfrom sklearn.feature_extraction.text import TfidfVectorizer\n\n#Define a TF-IDF Vectorizer Object. Remove all english stop words. \ntfidf = TfidfVectorizer(stop_words='english')\n\n#Imute NaN with an empty string\nplots =plots.fillna('')\n\n#Construct the required TF-IDF matrix by fitting and transforming the data\ntfidf_matrix = tfidf.fit_transform(plots)\n\n#Output the shape of tfidf_matrix\ntfidf_matrix.shape","2221d2a6":"# Import linear_kernel\nfrom sklearn.metrics.pairwise import linear_kernel\n\n# Compute the cosine similarity matrix\ncos_sim = linear_kernel(tfidf_matrix, tfidf_matrix)\n#show the shape of cos_sim matrix \ncos_sim.shape","d0a10d02":"#Construct a reverse map of indices and movie titles\nindices = pd.Series(df[:20000].index, index=df['title'][:20000]).drop_duplicates()\n#show the some indices \nindices[:10]\n","9f641a7e":"# Function that takes in movie title as input and outputs most similar movies\ndef get_recommendations(title, cos_sim=cos_sim):\n    # Get the index of the movie that matches the title\n    index = indices[title]\n\n    # Get the pairwsie similarity scores of all movies with that movie\n    sim_scores = list(enumerate(cos_sim[index]))\n\n    # Sort the movies based on the similarity scores\n    sim_scores = sorted(sim_scores, key=lambda x: x[1], reverse=True)\n\n    # Get the scores of the 10 most similar movies\n    sim_scores = sim_scores[1:11]\n\n    # Get the movie indices\n    movie_indices = [i[0] for i in sim_scores]\n\n    # Return the top 10 most similar movies\n    return df['title'].iloc[movie_indices]","ffd2ce2a":"#get the recommandations of movies similar to the Dark Knight\nget_recommendations('The Dark Knight Rises')","2a457488":"#get the recommandations of movies similar to the Godfather\nget_recommendations('The Godfather')","9be34318":"# \" Film Lovers are Sick People \"<br><br> Fran\u00e7ois Truffaut","36c1c774":"#### 2. Compute the score for every movie:","cf9a0231":"A simple recommender system is a process in which we make recommendations of the top items based on a specific metric or score. In this section, we will build a simplified clone of IMDB Top 250 Movies using metadata collected from IMDB.\n\nThe following are the steps involved:\n\n* Choose the metric\/score to rate movies on.\n\n* Compute the score for every movie.\n\n* Sort the movies based on the score and output the top results.","1faf9ce3":"The next step is to define a function that recommends movies based on its content.Meaning, a functio that outputs a list of movies that are similiar in the synopsis. So let's follow the folowwong steps:\n\n1. Get the index of the movie given its title.\n\n2. Get the list of cosine similarity scores for that particular movie with all movies. Convert it into a list of tuples where the first element is its position, and the second is the similarity score.\n\n3. Sort the  list of tuples based on the similarity scores\n\n4. Get the top 10 elements of this list. Ignore the first element as it refers to the movie itself.\n\n5. Return the titles corresponding to the indices of the top elements.","bc32ca78":"From the tfidf matrix above, we have 75,827 different vocabularies(words) in  20,000 movies. Now we will  compute the similarity score. There are several similarity metrics that you can use for this, such as the manhattan, euclidean, the Pearson, and the cosine similarity scores. \n\nFor our case, we will use cosine similarity to calculate a numeric quantity that denotes the similarity between two movies. You use the cosine similarity score since it is independent of magnitude and is relatively easy and fast to calculate. Mathematically,cosine similirity  is defined by the formula:<br>\n![](data:image\/png;base64,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)\n\n\nSince we  used the TF-IDF vectorizer, calculating the dot product between each vector will directly give you the cosine similarity score. Therefore, we will use sklearn's linear_kernel() instead of cosine_similarities() since it is faster.\n\nThis would return a matrix of shape 45466x45466, which means each movie overview cosine similarity score with every other movie overview. Hence, each movie will be a 1x45466 column vector where each column will be a similarity score with each movie.","bbf31505":"Based on Natural Language Processing techniques, we need to extract some  of features from the plot texts  before we can compute the similarity  between them. To put it simply, it is not possible to compute the similarity between any two overviews in their raw forms. So it is very essensial to preprocess our plot descriptions before feeding the synopsis texts to a TF-IDF vectorizer. To do this, we need to compute the word vectors of each overview or document, as it will be called from now on.<br><br>\n\nTF-IDF score is the frequency of a word occurring in a document, down-weighted by the number of documents in which it occurs. This is done to reduce the importance of words that frequently occur in plot overviews and, therefore, their significance in computing the final similarity score.\n\nIn scikit-learn library, there is a  built-in TfIdfVectorizer class that computes the TF-IDF matrix. So the processing will follow these steps :<br>\n* A. Import the Tfidf vectorizer from scikit-learn;\n* B. Preprocess the plots and remove the stopwords\n* C. Impute empty or non relevant values with a blanks;\n* D. Construct the TF-IDF matrix on the data.","129043de":"### A Little Overview about Data","d664f2aa":"In the Next step , we will define  a function that takes a movie title as an argument and returns a list of the 10 most similar movies based on their plots. First,  we will need to reverse mapping of movie titles and DataFrame indices. In other words, you need a mechanism to identify the index of a movie in your metadata DataFrame, given its title.","96f4dd0d":"* The dataset files contain metadata for all 45,000 movies listed in the Full MovieLens Dataset. It consists of movies released on or before July 2017. This dataset captures feature points like cast, crew, plot keywords, budget, revenue, posters, release dates, languages, production companies, countries, TMDB vote counts, and vote averages.\n* The dataframe contains information about ~45,000 movies featured in the Full MovieLens dataset. Features include posters, backdrops, budget, genre, revenue, release dates, languages, production countries, and companies.","e44b8c4f":"![](https:\/\/www.24hourmoviemarathon.com\/wp-content\/uploads\/2019\/09\/image-11-726x510.png?da238f&da238f)","55316747":"### IMDB Rating Based Recommandation System","3b9b01ee":"Because the memory resources couldn't handle processing this big size of the movies dataset, we will reduce its length to 20000 rows only.\n","8cbfc7d2":"### Content Based Recommandation System","b46b4b87":"As you can notice from the recommanded list above, our system returns the top ten movies that are very related to gangsters and mobs plots, since the movie that we based our recommandations on belongs to this gangster and crime movie genre. ","0c83109e":"## Don't Forget to Upvote if you Like the NoteBook!!","9de09b35":"#### 1. Choose the ranking score for the Movies:  \nTo do so, we will define the IMDB formula to use for computing the weighting rate as the score of a movie: <br><br>\nWR == (v\/(v+m) * R) + (m\/(m+v) * C)<br><br>\nwith:<br><br>\nv is the number of votes for the movie. <br>\n\nm is the minimum votes required to be listed in the chart. <br>\n\nR is the average rating of the movie. <br>\n\nC is the mean vote across the whole report.<br>\n\n","8663313b":"You can see that from the synopsis based recommendations list above that our system gets the movies with similar plot descriptions, the quality of recommendations is not that great. But the recommander could be improved by trying other techniques in computing similarity or do more preprocessing in the texts. In the example above, \"The Dark Knight Rises\" returns all Batman movies while it is more likely that the people who liked that movie are more inclined to enjoy other Christopher Nolan movies. ","005c2e76":"In this part of the tutorial, we will be building a system that recommends movies which are similar to a particular movie considering its synopsis or plot description. To make this happen, we will compute pairwise cosine similarity scores for all movies based on their synopsis descriptions and recommend movies based on that similarity score threshold.<br>\nThe plot overview is available to you as the overview feature in your metadata dataset. Let's inspect the plots of a few movies"}}