{"cell_type":{"25cba388":"code","8a908165":"code","31a66db9":"code","fb71bfbf":"code","3c3c5245":"code","60e83fb1":"code","5f795a1a":"code","89e6c3de":"code","05a66b34":"code","0800e118":"code","21dacfca":"code","aad1279e":"code","3e1dfe33":"code","5dab4ccb":"code","beb49661":"code","c2ace9b4":"code","24cbd45f":"code","82ecde98":"code","33c90990":"code","319d93e6":"code","38a22f98":"code","1a00b139":"code","01b9e83e":"code","a0929c1a":"code","43ece268":"code","e14a2f02":"code","a69548f7":"code","235aa277":"code","1f0c9708":"code","d15133a7":"code","59a2b186":"code","3bab9527":"code","a6e7dc05":"code","a9561ac7":"code","e8323f5d":"code","0ae4494f":"code","6975316b":"code","1116a5fb":"code","b48fe712":"code","131e7015":"code","f9c5a0ce":"code","942db46f":"code","c7124b99":"code","90280635":"code","8f2d7141":"code","9c2f7f51":"code","e0cedad7":"code","4407e35d":"markdown","7fb4692d":"markdown","1565b0e2":"markdown","468fcf2c":"markdown","d788ae43":"markdown","81738e70":"markdown","c943d848":"markdown","44c70e0c":"markdown","241c6e76":"markdown","59561f13":"markdown","fc9c9498":"markdown","a8e1eebb":"markdown","ec78a844":"markdown","5d7a6842":"markdown","26bfc6ee":"markdown","fb97e8fe":"markdown","8dec956f":"markdown","83eee23a":"markdown","12bb25ed":"markdown","ec654886":"markdown","8ac3c70a":"markdown","8ccc691f":"markdown"},"source":{"25cba388":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\ntrain_dir = '..\/input\/restaurant-revenue-prediction\/train.csv.zip'\ntest_dir = '..\/input\/restaurant-revenue-prediction\/test.csv.zip'\n\nimport zipfile\nwith zipfile.ZipFile(train_dir,\"r\") as z:\n    z.extractall('.')\n\nwith zipfile.ZipFile(test_dir,\"r\") as z:\n    z.extractall('.')","8a908165":"from datetime import datetime","31a66db9":"# load the train and test dataframe\ntrain_df=pd.read_csv('.\/train.csv')\ntest_df=pd.read_csv('.\/test.csv')\n\n# look at the information of the train_df\ndisplay(train_df.head())\ndisplay(train_df.info())","fb71bfbf":"# select the columns convert to [datetime] object in Python\ndef time_feature_convert(df,time_features,time_format):\n    df[time_features]=df[time_features].apply(\n    pd.to_datetime,\n    format=time_format)\n    print(f'Feature Type Conversion Informatoin: {(df[time_features]).dtypes}\\n')\n    return df\n\n# transfer the cols in train and test df     \ntime_feature_convert(df=train_df,\n                     time_features=['Open Date'],\n                     time_format='%m\/%d\/%Y')\ntime_feature_convert(df=test_df,\n                     time_features=['Open Date'],\n                     time_format='%m\/%d\/%Y')\npass","3c3c5245":"# select the columns convert to [str] object in Python\ndef str_feature_convert(df,str_features):\n    df[str_features]=df[str_features].astype(str)\n    print(f'Feature Type Conversion Informatoin: {(df[str_features]).dtypes}\\n')\n    return df\n\n# transfer the cols in train and test df     \nstr_feature_convert(df=train_df,\n                     str_features=['Id'])\nstr_feature_convert(df=test_df,\n                     str_features=['Id'])\npass","60e83fb1":"# now check the df datatypes\ndisplay(train_df.info())\ndisplay(train_df.head())","5f795a1a":"# indicate the cols that need to be dropped\ndrop_features=['Id','Open Date']\n\n# drop the selected cols\ntrain_df.drop(drop_features,axis=1,inplace=True)\ntest_df.drop(drop_features,axis=1,inplace=True)","89e6c3de":"train_df","05a66b34":"pip install dataprep","0800e118":"from dataprep.datasets import get_dataset_names\nfrom dataprep.datasets import load_dataset\nfrom dataprep.eda import create_report,plot,plot_missing\nimport scipy.stats as stats","21dacfca":"# function for overall statistical report\ndef overall_stat(df):\n    # display the overall stat report\n    display(plot(df, display=['Stats', 'Insights']))\n    # display(df.info())\n\n    # store and display the numerical and nonn-numerical cols in df\n    num_cols=list(df.select_dtypes(include=['number']).columns)\n    non_num_cols=list((set(df.columns)-set(num_cols)))\n\n    print(f'Num cols = {num_cols}')\n    print(f'Non-num cols = {non_num_cols}')","aad1279e":"# display the overall stats\noverall_stat(train_df)","3e1dfe33":"# define the interest feature you want to explore\ninter_features='revenue'\n\n# define the function for univariate analysis\ndef num_uni_analysis(df,inter_features):\n    display(plot(df,inter_features,display=['Stats','KDE Plot','Normal Q-Q Plot','Box Plot']))\n    skewness=df[inter_features].skew()\n    kurtosis=df[inter_features].kurtosis()\n    print(f'-The Skewness = {skewness}')\n    if abs(skewness)<1:\n        print(f'The [{inter_features}] distribution is nearly normal')\n    elif skewness>1:\n        print(f'The [{inter_features}] distribution is right skewed ')\n    else:\n        print(f'The [{inter_features}] distribution is left skewed ')\n    print(f'-The Kurtosis = {kurtosis}')","5dab4ccb":"# display the univariate analysis result for feature [revenue]\nnum_uni_analysis(train_df,inter_features)","beb49661":"# define the function for univariate analysis\ndef cat_uni_analysis(df,inter_features):\n    print(f'The Non-Numerical Column You Choose is: [{inter_features}]\\n')\n    display(plot(df,inter_features,display=['Stats','Pie Chart','Value Table']))","c2ace9b4":"# display the univariate categorical analysis result\ncat_uni_analysis(train_df,inter_features='Type')\ncat_uni_analysis(train_df,inter_features='City')\ncat_uni_analysis(train_df,inter_features='City Group')","24cbd45f":"import matplotlib.pyplot as plt\nimport seaborn as sns","82ecde98":"# overall num-num relationship: correlation heatmap\ndef heatmap(df,figsize):\n    fig, axs=plt.subplots(figsize=figsize)\n    sns.heatmap(df.corr(),annot=True, linewidths=.7,cmap='coolwarm',fmt='.1f',ax=axs)\n    \n# display the overall correlation heatmap\nheatmap(df=train_df,figsize=(25,25))","33c90990":"pip install scikit-posthocs","319d93e6":"# this is for the Kruskal test used for categorical-numerical relationship\nimport scikit_posthocs as sp ","38a22f98":"# categoircal-numerical relationship (cat_feature - target)\n\ndef cat_num_relationship(df,cat_col,num_col):\n    # visualization\n    print(f'[{cat_col}] --- [{num_col}] relationship')\n    display(plot(df,num_col,cat_col))\n    \n    # hypothesis testing for catgorical-numerical relationship (Kruskal test)\n    pc = sp.posthoc_conover(df, val_col=num_col, group_col=cat_col,p_adjust = 'holm')\n    # visualization of the heatmap\n    heatmap_args = {'linewidths': 0.25, 'linecolor': '0.5', 'square': True, 'cbar_ax_bbox': [0.80, 0.35, 0.04, 0.3]}\n\n    # plot\n    fig, ax = plt.subplots(ncols=1)\n    fig.suptitle('Significance Plot')\n    sp.sign_plot(pc,**heatmap_args,ax=ax) \n    fig.show()\n","1a00b139":"cat_num_relationship(df=train_df,\n                    cat_col='City',\n                    num_col='revenue')\n\ncat_num_relationship(df=train_df,\n                    cat_col='Type',\n                    num_col='revenue')\n\ncat_num_relationship(df=train_df,\n                    cat_col='City Group',\n                    num_col='revenue')","01b9e83e":"from scipy.stats import chi2_contingency","a0929c1a":"# categoircal-categorical relationship\n\ndef cat_cat_relationship(df,cat_col_1,cat_col_2):\n    # visualization\n    plot(df,cat_col_1,\n         cat_col_2,\n         display=['Stacked Bar Chart','Heat Map'])\n    \n    # Chi-square test\n    \n    # 1st step convert the data into a contingency table with frequencies\n    chi_contigency=pd.crosstab(df[cat_col_1],df[cat_col_2])\n    print(f'Selected cols [{cat_col_1}] and [{cat_col_2}]')\n    print('chi2-contingency table')\n    display(chi_contigency)\n    \n    # 2nd step: Chi-square test of independence.\n    c, p, dof, expected = chi2_contingency(chi_contigency)\n    if p<0.05:\n      print('Reject Null Hypothesis')\n      print(f'The:\\n [{cat_col_1}],[{cat_col_2}] are not independent\\n')\n    else:\n      print('Fail to Reject Null Hypothesis')\n      print(f'The:\\n [{cat_col_1}],[{cat_col_2}] are independent\\n') \n    print(f'The P-value = {p}')\n","43ece268":"# display the result\ncat_cat_relationship(df=train_df,\n                    cat_col_1='City Group',\n                    cat_col_2='Type')\n","e14a2f02":"from sklearn import preprocessing","a69548f7":"train_df.head()","235aa277":"# define the function for label or one-hot encoding\ndef label_encode_transform(df,cols):\n    cols=cols\n    le = preprocessing.LabelEncoder()\n    df[cols]=df[cols].apply(le.fit_transform)\n    return df\n    \ndef onehot_encode_transform(df,cols):\n    cols=cols\n    df=pd.get_dummies(df,columns=cols)\n    return df","1f0c9708":"train_df_encode=label_encode_transform(df=train_df,\n                        cols=['City'])\ntrain_df_encode=onehot_encode_transform(df=train_df_encode,\n                        cols=['City Group','Type'])\n\ntest_df_encode=label_encode_transform(df=test_df,\n                        cols=['City'])\ntest_df_encode=onehot_encode_transform(df=test_df_encode,\n                        cols=['City Group','Type'])","d15133a7":"train_df_encode.info()","59a2b186":"from sklearn.linear_model import LassoCV\nfrom sklearn.linear_model import Lasso\nfrom sklearn.feature_selection import RFECV\nfrom sklearn.metrics import mean_squared_error, roc_auc_score","3bab9527":"# seperate the source and the target variables\nfeature_cols = [x for x in train_df_encode.columns if x != 'revenue']\nX_train = train_df_encode[feature_cols]\ny_train = train_df_encode['revenue']\n\nX_test  = test_df_encode[feature_cols]\n","a6e7dc05":"from sklearn.preprocessing import RobustScaler\nscaler = RobustScaler()\n\nX_train_scaled= scaler.fit_transform(X_train)\nX_test_scaled = scaler.transform (X_test)","a9561ac7":"model= Lasso()\nfeature_selector = RFECV(model,\n                         scoring='neg_mean_squared_error',\n                         cv=3)\nfeature_selector.fit(X_train_scaled,y_train)","e8323f5d":"feature_selector.n_features_","0ae4494f":"# length check\nlen(feature_selector.support_)==len(X_train.columns)\n\n# store the selected features\nselect_feature=pd.DataFrame(\n    {'tf':feature_selector.support_,\n    'feature':X_train.columns})\n\nselected_feature=list(select_feature.loc[select_feature['tf']==True]['feature'])\nfeature_coef=feature_selector.estimator_.coef_","6975316b":"# visualize the selected feature and coefficient\nax=sns.barplot(y=selected_feature,x=feature_coef)\nax.set_title('Selected Feature Coefficient Plot')\nprint(f'Selected features are: {selected_feature}')","1116a5fb":"# Visualization\nfig,axs=plt.subplots(ncols=1,figsize=(15,5))\nfig.suptitle('Number of features --- RMSE')\nsns.lineplot(range(1,len(feature_selector.grid_scores_)+1),\n             feature_selector.grid_scores_,\n             marker='o',\n             ax=axs)\n","b48fe712":"# select the features from the dataset (3 features)\nX_train_fs=X_train[selected_feature]\nX_test_fs=X_test[selected_feature]\n\nX_train_fs_scaled= scaler.fit_transform(X_train_fs)\nX_test_fs_scaled = scaler.transform (X_test_fs)","131e7015":"pip install xgboost","f9c5a0ce":"pip install lightgbm","942db46f":"from sklearn.linear_model import LassoCV\nfrom sklearn.linear_model import RidgeCV\nfrom sklearn.linear_model import ElasticNetCV\nimport lightgbm as lgb\nimport xgboost as xgb\n\nfrom sklearn.metrics import mean_squared_error","c7124b99":"cv=3\n\n# fit the Models\nlassoCV=LassoCV(cv=cv).fit(X_train_fs_scaled,y_train)\nridgeCV=RidgeCV(cv=cv).fit(X_train_fs_scaled,y_train)\nelasticnetCV=ElasticNetCV(cv=cv).fit(X_train_fs_scaled,y_train)\nlightgbm=lgb.LGBMRegressor().fit(X_train_fs_scaled,y_train)\nxgboost=xgb.XGBRegressor().fit(X_train_fs_scaled,y_train)\n\n# generate the prediction for train dataset\nlasso_train_pred=lassoCV.predict(X_train_fs_scaled)\nridge_train_pred=ridgeCV.predict(X_train_fs_scaled)\nelasticnet_train_pred=elasticnetCV.predict(X_train_fs_scaled)\nlgbm_train_pred=lightgbm.predict(X_train_fs_scaled)\nxgb_train_pred=xgboost.predict(X_train_fs_scaled)\n\n# generate RMSE for each models\nlasso_RMSE= np.sqrt(mean_squared_error(y_train, lasso_train_pred))\nridge_RMSE= np.sqrt(mean_squared_error(y_train, ridge_train_pred))\nelasticnet_RMSE= np.sqrt(mean_squared_error(y_train, elasticnet_train_pred))\nlgbm_RMSE= np.sqrt(mean_squared_error(y_train, lgbm_train_pred))\nxgb_RMSE= np.sqrt(mean_squared_error(y_train, xgb_train_pred))","90280635":"model_list=['Lasso','Ridge','ElasticNet','LGBM','XGBoost']\nrmse_list=[lasso_RMSE,ridge_RMSE,elasticnet_RMSE,lgbm_RMSE,xgb_RMSE]\n\n# plot the RMSE for each model\nax=sns.barplot(y=model_list,x=rmse_list)\nax.set_title('Model RMSE Result')\n\n# print the result RMSE number\nprint(f' lasso={lasso_RMSE} \\n ridge = {ridge_RMSE}\\n Elastic_Net = {elasticnet_RMSE}\\n LGBM = {lgbm_RMSE}\\n XGBoost= {xgb_RMSE}\\n')","8f2d7141":"# generate prediction for test dataset\nxgb_test_pred=xgboost.predict(X_test_fs_scaled)","9c2f7f51":"# store the result\nsubmission_df=pd.DataFrame(\n{'Id':test_df.index,\n'Prediction':xgb_test_pred}\n)","e0cedad7":"submission_df.to_csv('submission_dcx.csv',index=False)","4407e35d":"<H1> 6. Modelling","7fb4692d":"<h3> 3.2.2 Categorical Feature Univaraite Analysis","1565b0e2":"<h2> 2.2 Drop some irrelevant features (such as ID)","468fcf2c":"<h3> 3.2.1 Numerical Feature Univariate Analysis","d788ae43":"<h1> 4. Label Encoding","81738e70":"<h2> 2.1 convert some features data type\n    \n<li> convert some features into [datetime] datatypes\n<li> convert some int\/float to [str] datatypes\n    ","c943d848":"<h1> 0. unzip the .csv dataset","44c70e0c":"**Therefore here i choose the XGBoost as the final model for prediction**","241c6e76":"<h1> 1. Load the Dataset","59561f13":"<h2> 3.2 Univariate Analysis","fc9c9498":"<h3> 3.3.1 Overall num-num relationship (corr heatmap)","a8e1eebb":"<h2> 3.3 Bivariate Analysis","ec78a844":"<h3> 3.3.3 Categorical-Categorical Relationship (1 vs 1)","5d7a6842":"<h1> 2. Pre-Process the Dataset","26bfc6ee":"<h1> 5. Feature Selection","fb97e8fe":"**As can be seen above, for train_df:**\n<li> 137 rows\n<li> no missing value\n<li> 41 features, 1 target (numerical)","8dec956f":"![](https:\/\/upload.wikimedia.org\/wikipedia\/commons\/thumb\/c\/cc\/Relationship_between_mean_and_median_under_different_skewness.png\/434px-Relationship_between_mean_and_median_under_different_skewness.png)","83eee23a":"<h1> Restaurant Revenue Regression\n    \n<h4> Data fields\n    \n* **Id** : Restaurant id. \n* **Open Date** : opening date for a restaurant\n* **City** : City that the restaurant is in. Note that there are unicode in the names. \n* **City Group**: Type of the city. Big cities, or Other. \n* **Type**: Type of the restaurant. FC: Food Court, IL: Inline, DT: Drive Thru, MB: Mobile\n* **P1, P2 - P37**: There are three categories of these obfuscated data. Demographic data are gathered from third party providers with GIS systems. These include population in any given area, age and gender distribution, development scales. Real estate data mainly relate to the m2 of the location, front facade of the location, car park availability. Commercial data mainly include the existence of points of interest including schools, banks, other QSR operators.\n* **Revenue**: The revenue column indicates a (transformed) revenue of the restaurant in a given year and is the target of predictive analysis. Please note that the values are transformed so they don't mean real dollar values. ","12bb25ed":"<h2> 3.1 Overall Statistics","ec654886":"<h1> 3. EDA","8ac3c70a":"* Regularization method (Lasso, Ridge, Elastic Net)\n* Tree based model (XGBoost etc)","8ccc691f":"<h3> 3.3.2 Categorical - Numerical relationship (1 vs 1)"}}