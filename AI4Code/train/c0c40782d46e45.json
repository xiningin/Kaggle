{"cell_type":{"14726cc5":"code","86c0e140":"code","a20ccc75":"code","f27095ed":"code","33509851":"code","49c5a4d0":"code","fee8aab4":"code","20088ff5":"code","0b7be0ef":"code","3e816e67":"code","f6502c0e":"code","8362cf20":"code","37663ba1":"code","c3fa95a1":"code","b14952b8":"code","6f802e5e":"code","f9e9f194":"markdown","d0273c26":"markdown","2e7b8bd8":"markdown","ef0ace5a":"markdown","65675e01":"markdown","1c74ca4d":"markdown","0566c0ab":"markdown"},"source":{"14726cc5":"import numpy as np\nimport pandas as pd\nimport tensorflow as tf\nfrom skimage import io \nfrom skimage import transform\nfrom matplotlib import pyplot as plt\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\ndat_path = '..\/input'\nprint(os.listdir(dat_path))\n\n# Any results you write to the current directory are saved as output.","86c0e140":"train_img_path = dat_path + '\/train'\ntest_img_path = dat_path + '\/test'\nlabel_path = dat_path + '\/train.csv' \n\n#Data\nlabDat = pd.read_csv(label_path)\n\nSplitRatio = 0.1 #Ratio to keep as validation and test data\ndatLen = len(labDat)\n\ntrainDat = labDat[ : int(datLen * (1 - 2 * SplitRatio))]\nvalidDat = labDat[int(datLen * (1 - 2 * SplitRatio)) : int(datLen * (1 - SplitRatio))]\ntestDat = labDat[int(datLen * (1 - SplitRatio)) : ]\n\nprint('elements : ' + str(len(labDat)))\nprint('training : ' + str(len(trainDat)))\nprint('validation : ' + str(len(validDat)))\nprint('test : ' + str(len(testDat)))","a20ccc75":"#number of labels\nnum_labels = 28","f27095ed":"class datGen():\n    \n    def make_batch(dat, path, batch_size, ind = 0, offset = 0, n_labels = 28, aug_chance = 0.3):\n        I = np.identity(n_labels)\n        imgs = []\n        labels = []\n        rnd_indexes = np.array(range(batch_size)) + ind * batch_size + offset\n        for index in rnd_indexes:\n            imgs.append(datGen.fetch_img('%s\/%s' % (path, dat.loc[index][0]), aug_chance))\n            labels.append(\n                np.sum(I[np.array(dat.loc[index][1].split()).astype(np.int)], axis = 0) \n                ) # encodes labels into one-hot vectors and sums all the labels\n        # returns normalized and flattened images along with the labels\n        return np.reshape(np.array(imgs) \/ 256, [batch_size, 512 * 512 * 4]), np.array(labels)\n\n    def fetch_img(path, aug_chance):\n        img = []\n        colors = ['red', 'green', 'blue', 'yellow']\n        r = np.random.uniform()\n        angle = 0\n        if r < aug_chance:\n            angle = np.random.uniform(90)\n        for color in colors :\n            img.append(datGen.augment(io.imread('%s_%s.png' % (path, color)), angle = angle))\n        return np.stack(img, -1)\n    \n    def augment(img, angle = 0):\n        if angle == 0:\n            return img\n        return transform.rotate(img, angle)","33509851":"elements = 3\nex_dat = datGen.make_batch(labDat, train_img_path, elements, n_labels = num_labels, ind = 5, aug_chance = 0)\nfig, ax = plt.subplots(elements, 4, sharex = 'col', sharey = 'row', figsize = [15,10], dpi = 150)\nfor ind, i in enumerate(ex_dat[0]):\n    img = np.reshape(i, [512, 512, 4])\n    ax[ind, 0].imshow(img[ : , : , 0], cmap = 'gnuplot')\n    ax[ind, 1].imshow(img[ : , : , 1], cmap = 'hot')\n    ax[ind, 2].imshow(img[ : , : , 2], cmap = 'magma')\n    ax[ind, 3].imshow(img[ : , : , 3], cmap = 'terrain')","49c5a4d0":"class ResNet():\n    \n    def resBlock(inp, out_space):\n        rb1 = tf.layers.conv2d(inp, filters = out_space, kernel_size = 3, strides = 1, padding = 'same')\n        rb2 = tf.layers.conv2d(rb1, filters = out_space, kernel_size = 3, strides = 1, padding = 'same')\n        return tf.nn.relu(rb2 + inp)\n    \n    def __init__(self, input_size, out_space, res = 512, l_rate = 0.0001, beta = 0.09):\n        # Placeholders\n        self.inputVec = tf.placeholder(dtype = tf.float32, shape = [None, input_size])\n        self.labels = tf.placeholder(dtype = tf.float32, shape = [None, out_space])\n        \n        # Convolutional layers\n        self.X = tf.reshape(self.inputVec, shape = [-1, res, res, 4])\n        self.cnv1 = tf.layers.conv2d(self.X, filters = 64, kernel_size = 7, strides = 2, padding = 'same')\n        self.pool1 = tf.layers.max_pooling2d(self.cnv1, pool_size = 3, strides = 2, padding = 'same')\n        \n        self.res1 = ResNet.resBlock(self.pool1, 64)\n        self.res2 = ResNet.resBlock(self.res1, 64)\n        self.res3 = ResNet.resBlock(self.res2, 64)\n        \n        self.cnv2 = tf.layers.conv2d(self.res3, filters = 128, kernel_size = 3, strides = 2, padding = 'same')\n        self.cnv2 = tf.layers.batch_normalization(self.cnv2)\n        self.cnv3 = tf.layers.conv2d(self.cnv2, filters = 128, kernel_size = 3, strides = 1, padding = 'same')\n        self.cnv3 = tf.layers.dropout(self.cnv3)\n        \n        self.res4 = ResNet.resBlock(self.cnv3, 128)\n        \n        self.cnv4 = tf.layers.conv2d(self.res4, filters = 256, kernel_size = 3, strides = 2, padding = 'same')\n        self.cnv4 = tf.layers.batch_normalization(self.cnv4)\n        self.cnv5 = tf.layers.conv2d(self.cnv4, filters = 256, kernel_size = 3, strides = 1, padding = 'same')\n        self.cnv5 = tf.layers.dropout(self.cnv5)\n        \n        self.res5 = ResNet.resBlock(self.cnv5, 256)\n        self.res6 = ResNet.resBlock(self.res5, 256)\n        \n        self.cnv6 = tf.layers.conv2d(self.res6, filters = 512, kernel_size = 3, strides = 2, padding = 'same')\n        self.cnv6 = tf.layers.batch_normalization(self.cnv6)\n        self.cnv7 = tf.layers.conv2d(self.cnv6, filters = 512, kernel_size = 3, strides = 2, padding = 'same')\n        self.cnv7 = tf.layers.dropout(self.cnv7)\n        \n        self.res7 = ResNet.resBlock(self.cnv7, 512)\n        self.res8 = ResNet.resBlock(self.res7, 512)\n        \n        self.pool2 = tf.layers.average_pooling2d(self.res8, pool_size = 3, strides = 2, padding = 'same')\n        \n        # Fully connected layer\n        self.fc1 = tf.layers.flatten(self.pool2)\n        xavier_init = tf.contrib.layers.xavier_initializer()\n        self.fc1W = tf.Variable(xavier_init([8192, out_space]))\n        \n        self.logits = tf.matmul(self.fc1, self.fc1W)\n        self.out = tf.sigmoid(self.logits)\n        \n        # Loss\n        reg_losses = tf.get_collection(tf.GraphKeys.REGULARIZATION_LOSSES)\n        self.loss = tf.reduce_mean(tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(labels = self.labels, logits = self.logits)) + beta * sum(reg_losses))\n        self.optimizer = tf.train.AdamOptimizer(learning_rate = l_rate)\n        self.updateOp = self.optimizer.minimize(self.loss)","fee8aab4":"#Hyperparameters \nbatch_size = 40\nlearning_rate  = 0.0001\nepochs = 10\n\n#Create ops\nClassifier = ResNet(512 * 512 * 4, 28, l_rate = learning_rate)\ninit = tf.global_variables_initializer()\nsaver = tf.train.Saver(tf.trainable_variables())\n\n#Session\nsess = tf.InteractiveSession()\ntf.reset_default_graph()\nsess.run(init)\n\nif not os.path.exists('..\/checkpoints'):\n    os.mkdir('..\/checkpoints')\ntry:\n    saver.restore(sess, '..\/checkpoints\/test_model.ckpt')\n    print('...model loaded from previous checkpoint')\nexcept:\n    pass","20088ff5":"tr_losses = []\nvl_losses = []\nfor epoch in range(epochs):\n    temp_t_loss = []\n    temp_v_loss = []\n    for i in range(int(datLen * (1 - 2 * SplitRatio)) \/\/ batch_size):\n        trDat = datGen.make_batch(trainDat, train_img_path, batch_size = batch_size, ind = i)\n        tr_loss, _ = sess.run([Classifier.loss, Classifier.updateOp], feed_dict = {Classifier.inputVec : trDat[0], Classifier.labels : trDat[1]})\n        saver.save(sess, '..\/checkpoints\/test_model.ckpt')\n        \n        if (i < int(datLen * SplitRatio) \/\/ batch_size):\n            valDat = datGen.make_batch(validDat, train_img_path, batch_size = batch_size, ind = i, offset = int(datLen * (1 - 2 * SplitRatio)))\n            vl_loss = sess.run([Classifier.loss], feed_dict = {Classifier.inputVec : valDat[0], Classifier.labels : valDat[1]})\n            temp_v_loss.append(vl_loss)\n        temp_t_loss.append(tr_loss)\n        \n        if (i % ((datLen \/\/ batch_size) \/\/ 20) == 0):            \n            print('[epoch : %i , iter : %i] tr_loss : %f' % (epoch, i, tr_loss))\n            \n    #print training and validation loss \n    mean_t_loss = np.mean(temp_t_loss)\n    mean_v_loss = np.mean(temp_v_loss)\n    \n    tr_losses.append(mean_t_loss)\n    vl_losses.append(mean_v_loss)\n    \n    print('[epoch : %i] tr_loss : %f, vl_loss : %f' % (epoch, mean_t_loss, mean_v_loss))\n    \nsess.close()","0b7be0ef":"plt.plot(tr_losses, label = 'training loss')\nplt.plot(vl_losses, '--', label = 'validation loss')\nplt.legend(loc = 'upper right')\nplt.xlabel('epoch')\nplt.ylabel('loss')\nplt.show()","3e816e67":"def infer(inp_vector):\n    out = sess.run(Classifier.out, feed_dict = {Classifier.inputVec : inp_vector})\n    labels = []\n    for element in out:\n        label = np.squeeze(np.argwhere(element > 0.2)).astype(np.str)\n        if label.size > 1:\n            label = ' '.join(label)\n        else :\n            label = str(label)\n        labels.append(label)\n    return labels\n    ","f6502c0e":"inDat = datGen.make_batch(trainDat, train_img_path, batch_size = 4)\nx = infer(inDat[0])\ny = ['A', 'B', 'C', 'D']\nlist(zip(y,x))","8362cf20":"#write to CSV\nl = list(zip(y, x))\npd.DataFrame(l, columns = list('ky'))","37663ba1":"import copy\ntestDF = copy.deepcopy(labDat[:200])\nlist(testDF.iloc[:,0])","c3fa95a1":"class DFrameOps:\n    \n    def mkFrame():\n        return pd.DataFrame({'Id' : [], 'Target' : []})\n    \n    def append(df, Id, Target):\n        tempDF = pd.DataFrame(list(zip(Id, Target)), columns = ['Id', 'Target'])\n        return df.append(tempDF)\n    \n    def resetIndex(df):\n        return df.set_index(np.array(list(range(len(df)))))\n        ","b14952b8":"tdf = DFrameOps.mkFrame()\ninf_batch_size = 50\nfor i in range(len(testDF) \/\/ inf_batch_size):\n    infDat, _ = datGen.make_batch(testDF, train_img_path, inf_batch_size, aug_chance = 0)\n    infTarget = infer(infDat)\n    infIds = testDF.iloc[i * inf_batch_size:(i + 1) * inf_batch_size, 0]\n    tdf = DFrameOps.append(tdf, infIds, infTarget)\n    \ntdf = DFrameOps.resetIndex(tdf)\ntdf[:10]\n    \n","6f802e5e":"testDF[:10]","f9e9f194":"## Inference","d0273c26":"## The network \nThe architecture we're using is a modified version of  [ arXiv:1512.03385v1 [cs.CV]](https:\/\/arxiv.org\/abs\/1512.03385) with fewer residual blocks. ![A residual block](https:\/\/cdn-images-1.medium.com\/max\/987\/1*pUyst_ciesOz_LUg0HocYg.png) an example residual block\n\n#### Activation : sigmoid (As we have a multi-label classification problem)\n","2e7b8bd8":"Let's look at a few elements from our dataset","ef0ace5a":"## Training the network","65675e01":"### **Import libraries**","1c74ca4d":"### Loading data\nData is loaded onto the memory in batches to prevent leaks\n#### params:\n###### dat : dataframe consisting of filenames and the corresponding labels.\n###### path : path to folder having the images of the dataset.\n###### batch_size : number of images to load at a time.\n###### ind : (ind * batch_size) to offset by\n###### offset : elements to offset by\n###### n_labels : number of possible labels \n###### aug_chance : probability of data getting augmented\n","0566c0ab":"### Training loop"}}