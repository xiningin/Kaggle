{"cell_type":{"a1df0cc9":"code","2354611d":"code","7adb3fc1":"code","68485190":"code","25576f1f":"code","d7e4719a":"code","15e7c5f9":"code","af4e1334":"code","455fbeb2":"code","fde77bcb":"code","b391c694":"code","9ce89c50":"markdown"},"source":{"a1df0cc9":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","2354611d":"# import library to be used\nimport pandas as pd\nimport numpy as np\nimport random\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport tensorflow as tf\nfrom sklearn.model_selection import train_test_split\nimport time\nimport warnings\nimport plotly.express as px\nwarnings.filterwarnings(\"ignore\")\n\n#Common Model Algorithms\nfrom sklearn import svm, tree, linear_model, neighbors, naive_bayes, ensemble, discriminant_analysis, gaussian_process\nfrom xgboost import XGBClassifier\n\n#Common Model Helpers\nfrom sklearn.preprocessing import OneHotEncoder, LabelEncoder\nfrom sklearn import feature_selection\nfrom sklearn import model_selection\nfrom sklearn import metrics\n%matplotlib inline","7adb3fc1":"# importing data\ntrain_df = pd.read_csv('\/kaggle\/input\/digit-recognizer\/train.csv')\ntest_df = pd.read_csv('\/kaggle\/input\/digit-recognizer\/test.csv')\nsample_submission_df = pd.read_csv('\/kaggle\/input\/digit-recognizer\/sample_submission.csv')\nprint(\"Train dataset: \",train_df.shape)\nprint(\"Test dataset: \",test_df.shape)","68485190":"# check if null\/na data exists in observations. also, since the data description says the data is between 0 to 255, also count the data if there is any data < 0 and > 255\n\n# Data for isnull, isna checks\npixelx = list(train_df.columns)\nisna = train_df[pixelx].isna().sum()\nisnull = train_df[pixelx].isnull().sum()\n[print(i) for i in isna if i > 0] # output is [], so.. none of the data is isna\n[print(i) for i in isnull if i > 0] # output is [], so.. none of the data is isnull\n\n# Data for checking any < 0 and > 255\ncheck_lesser_than_0 = train_df[train_df[pixelx] < 0].value_counts() # output is [], so... non of the data is lesser than 0\ncheck_greater_than_255= train_df[train_df[pixelx] < 255].value_counts() # output is [], so... non of the data is greater than 255\n","25576f1f":"# parting target vs predictors\ntrain_pred_df = train_df[list(train_df.columns.drop('label'))] # predictor variables of training dataset\ntrain_resp_df = train_df['label'] # response variable of training dataset\ntest_pred_df = test_df # test dataset does not have any labels. we need to predict one\n\n# Normalization of predictor variables. - why do we normalize? - for optimization purposes\ntrain_pred_df = train_pred_df\/np.max(np.max(train_pred_df))\ntest_pred_df = test_pred_df\/np.max(np.max(test_pred_df))\ntrain_pred_df_visuals = train_pred_df.values.reshape(-1,28,28,1)\ntest_pred_df_visuals = test_pred_df.values.reshape(-1,28,28,1)","d7e4719a":"# Visualization on some of the data. We know that it is having a 28 by 28, and only white and black - not RGB. canal = 1\ndf_by_numbers = []\nrand_pick = []\nfor i in range(0,10):\n    df_by_numbers.append(train_df[train_df['label']==i])\n    df_by_numbers[i].reset_index(inplace=True)\n    df_by_numbers[i].drop(columns=['index','label'],inplace=True)\n    df_by_numbers[i] = df_by_numbers[i].values.reshape(-1,28,28,1)\n    rand_pick.append([random.randint(0,len(df_by_numbers[i])) for j in range(0,10)])\n\ndf_by_numbers[0].shape\nrand_pick\n\nfig, ax = plt.subplots(10,10,figsize=(20,20))\n\nfor i in range(0,10):\n    for j in range(0,10):\n        ax[i,j].imshow(df_by_numbers[i][rand_pick[i][j]][:,:,0], cmap='Greys')\n        ax[i,j].set_title(str(i)+', '+str(j)+'th num')\n\n# plt.imshow(train_pred_df_visuals[n][:,:,0])\n# print(train_resp_df[n])","15e7c5f9":"num_of_values = train_resp_df.value_counts()\nnum_of_values.sort_index()\nfig = px.bar(num_of_values,x=num_of_values.index,y='label',color=num_of_values.index)\n\nfig.update_layout(xaxis = dict(tickvals = [0,1,2,3,4,5,6,7,8,9]))\n\nfig.show()","af4e1334":"# train test split \nx_train, x_test, y_train, y_test = train_test_split(train_pred_df,train_resp_df,test_size=0.33,random_state=42)\nprint('train data shape: ',x_train.shape)\nprint('test data shape: ',x_test.shape)","455fbeb2":"#Machine Learning Algorithm (MLA) Selection and Initialization - sklearn available preset classification algorithm lists\nEnsemble_MLA = [\n     #Ensemble Methods\n#     ensemble.AdaBoostClassifier(),\n#     ensemble.BaggingClassifier(),\n    ensemble.ExtraTreesClassifier()]\n#     ensemble.GradientBoostingClassifier(),\n#     ensemble.RandomForestClassifier()]\n# Gaussian_MLA = [\n#     #Gaussian Processes\n#     gaussian_process.GaussianProcessClassifier()]\n# GLM_MLA = [\n#     #Generalized Linear Model, GLM\n#     linear_model.LogisticRegressionCV(),\n#     linear_model.PassiveAggressiveClassifier(),\n#     linear_model.RidgeClassifierCV(),\n#     linear_model.SGDClassifier(),\n#     linear_model.Perceptron()]\n# Bayes_MLA = [\n#     #Navies Bayes\n#     naive_bayes.BernoulliNB(),\n#     naive_bayes.GaussianNB()]\n# KNN_MLA = [\n#     #Nearest Neighbor\n#     neighbors.KNeighborsClassifier()]\n# SVM_MLA = [\n#     #SVM\n#     svm.SVC(probability=True),\n#     svm.NuSVC(probability=True),\n#     svm.LinearSVC()]\n# TREE_MLA = [\n#     #Trees    \n#     tree.DecisionTreeClassifier(),\n#     tree.ExtraTreeClassifier()]\n# DA_MLA = [\n#     #Discriminant Analysis\n#     discriminant_analysis.LinearDiscriminantAnalysis(),\n#     discriminant_analysis.QuadraticDiscriminantAnalysis()]\n\n# XGB_MLA = [\n#     #xgboost\n#     XGBClassifier()]","fde77bcb":"#create table to compare MLA metrics\nAlgorithm_Perf = ['MLA Name', 'MLA Train Accuracy Mean', 'MLA Test Accuracy Mean']\nClassifier_Alg = pd.DataFrame(columns = Algorithm_Perf)\n\n#create table to compare MLA predictions\nMLA_predict = train_resp_df\n\nalg = Ensemble_MLA[0]\n\n#set name and parameters\nalg_name = alg.__class__.__name__\nClassifier_Alg.loc[0, 'MLA Name'] = alg_name\n\n#score model with cross validation: http:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.model_selection.cross_validate.html#sklearn.model_selection.cross_validate\ncv_results = model_selection.cross_validate(alg, x_train, y_train, return_train_score=True)\n\nClassifier_Alg.loc[0, 'MLA Train Accuracy Mean'] = cv_results['train_score'].mean()\nClassifier_Alg.loc[0, 'MLA Test Accuracy Mean'] = cv_results['test_score'].mean()   \n\nalg.fit(train_pred_df, train_resp_df)\nprediction = alg.predict(test_pred_df)\nsubmitted = pd.DataFrame(prediction,columns = ['Label'])\n\nsubmitted.index.name = \"ImageId\"\n\nEnsemble = Classifier_Alg\nEnsemble\n\nsubmitted.to_csv('submitted.csv', encoding='utf-8')","b391c694":"Ensemble","9ce89c50":"# Extra Tree classifier - sklearn ensemble extratreeclassifier"}}