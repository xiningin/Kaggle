{"cell_type":{"0b574f36":"code","d60f1020":"code","d7c0c6cf":"code","cc309e03":"code","6f2931dd":"code","e67cc215":"code","e9569225":"code","74c073a3":"code","39360a3d":"code","b2abf337":"code","49d42be7":"code","28a05a3c":"code","8c2ccdf0":"code","48695214":"code","84272d61":"code","1af43cb2":"code","1ffe22b0":"code","65a8ed38":"code","ce615553":"code","18a13376":"code","f77af00a":"code","b0f9cc1f":"code","6ef79ea1":"code","05ef6d5e":"code","4f21787e":"code","09e903fc":"code","19ed37ce":"code","fa24a05b":"code","47947391":"code","3fabcf34":"code","2d96c7e9":"markdown","13f49c14":"markdown","9f75232f":"markdown","c68dbb23":"markdown","6f22378d":"markdown","9f6c4b4f":"markdown","0b37764f":"markdown","420c2d35":"markdown","8aaba01b":"markdown","c48c3cfb":"markdown","99a62767":"markdown"},"source":{"0b574f36":"# import of packages\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n","d60f1020":"import plotly.express as px","d7c0c6cf":"import nltk\nfrom nltk.tokenize import word_tokenize\nfrom nltk.corpus import stopwords\n\nfrom IPython.display import HTML\n\nfrom nltk.sentiment.vader import SentimentIntensityAnalyzer\nsid = SentimentIntensityAnalyzer()\n\nfrom sklearn.feature_extraction.text import CountVectorizer","cc309e03":"pip install NRClex","6f2931dd":"from collections import defaultdict\nfrom pathlib import Path\nimport os\n","e67cc215":"# https:\/\/stackoverflow.com\/questions\/58467374\/loading-a-multiple-txt-files-in-to-python-as-dataframe\n\ntex_dir_path  = \"..\/input\/donald-trumps-rallies\"\nresults = defaultdict(list)\n\nfor file in Path(tex_dir_path).iterdir():\n    with open(file, \"r\") as file_open:\n            results[\"text\"].append(file_open.read())\n        \ndf = pd.DataFrame(results)","e9569225":"df.head() ","74c073a3":"# checking for blanks\n\nblanks =[]\n\n# index, text\nfor i,it in df.itertuples():\n    if it.isspace():\n        blanks.append(i)\n        \n        \nprint(blanks)","39360a3d":"# packages for text preprocesing\n\nimport re\nimport nltk\nimport spacy\nimport string","b2abf337":"# text to lower case\n\ndf['text'] = df['text'].str.lower()","49d42be7":"# Removing Punctuation\n\npunk_Remove = string.punctuation\n\ndef del_punk(text):\n    \"\"\"\n    function to remove the Punctuation\n    \"\"\"\n    return text.translate(str.maketrans('','',punk_Remove))\n\ndf['text'] = df['text'].apply(lambda T: del_punk(T))\ndf.head(2)","28a05a3c":"from nltk.corpus import stopwords\n\nFre_word = set(stopwords.words('english'))\n\ndef del_stopwords(text):\n    \"\"\"\n    function to remove the stopwords\n    \"\"\"\n    return \" \".join([word for word in str(text).split() if word not in Fre_word])\n\ndf['text'] = df['text'].apply(lambda T: del_stopwords(T))\n\ndf.head(2)","8c2ccdf0":"# most frequent words\n\nfrom collections import Counter\n\ncount = Counter()\n\nfor text in df['text'].values:\n    for word in text.split():\n        count[word] = count[word] + 1\n\ncount.most_common(10)","48695214":"\nFreq_word = set([i for (i, ic) in count.most_common(10)])\n\n\ndef del_freq_word(text):\n    \"\"\"\n    This function will remove the frequent words\n    \n    \"\"\"\n    return \" \".join([word for word in str(text).split() if word not in Freq_word])\n\ndf['text'] = df['text'].apply(lambda t: del_freq_word(t))\n\ndf.head(2)","84272d61":"# Removal of Rare words\n\nrare_word = 10\n\nRarewords = set([i for (i, ic) in count.most_common()[:-rare_word:-1]])\n\ndef del_rarewords(text):\n    \"\"\"\n    function to remove the rare words\n    \"\"\"\n    return \" \".join([word for word in str(text).split() if word not in Rarewords])\n\n\n\ndf['text'] = df['text'].apply(lambda T: del_rarewords(T))","1af43cb2":"# Lemmatization (turn the words to base form for example: cars, car's should become car)\n\nfrom nltk.corpus import wordnet\nfrom nltk.stem import WordNetLemmatizer\n\nlemmatizer = WordNetLemmatizer()\nwordnet_m = {\"N\":wordnet.NOUN, \"V\":wordnet.VERB, \"J\":wordnet.ADJ, \"R\":wordnet.ADV}\n\n\ndef lemmat_pos_word(text):\n    \n    pos_tagger_text = nltk.pos_tag(text.split())\n    \n    return \" \".join([lemmatizer.lemmatize(word, wordnet_m.get(pos[0],wordnet.NOUN)) for word,pos in pos_tagger_text])\n\n\n\ndf['lemma_text'] = df['text'].apply(lambda T : lemmat_pos_word(T))\n\ndf.head(2)","1ffe22b0":"df['scores'] = df['lemma_text'].apply(lambda review: sid.polarity_scores(review))\n\ndf['compound']  = df['scores'].apply(lambda score_dict: score_dict['compound'])\n\ndf['Negative']  = df['scores'].apply(lambda score_dict: score_dict['neg'])\n\ndf['Neutral']  = df['scores'].apply(lambda score_dict: score_dict['neu'])\n\ndf['Positive']  = df['scores'].apply(lambda score_dict: score_dict['pos'])","65a8ed38":"df['number_of_words'] = df['text'].str.split().apply(lambda x: len(x))","ce615553":"df.head()","18a13376":"# n-gram analysis function\n\ndef text_ngrams(corpus, n, g):\n    vec = CountVectorizer(ngram_range=(g, g)).fit(corpus)\n    bag_of_words = vec.transform(corpus)\n    sum_words = bag_of_words.sum(axis=0) \n    words_freq = [(word, sum_words[0, idx]) for word, idx in vec.vocabulary_.items()]\n    words_freq =sorted(words_freq, key = lambda x: x[1], reverse=True)\n    return words_freq[:n]","f77af00a":"freq_word = count.most_common(20)\n\nfreq_word_plot1 = pd.Series( (v[0] for v in freq_word) )\nfreq_word_plot2 = pd.Series( (v[1] for v in freq_word) )\n\n\ndata_freq = pd.DataFrame({'Words':freq_word_plot1,'Count':freq_word_plot2})\ndata_freq = data_freq.sort_values('Count')\n\n\n#plot\nplt.figure(figsize = (12,6))\nsns.set_style('darkgrid')\nsns.set_context(context='notebook',font_scale=1.5)\nsns.barplot(data=data_freq,y='Words',x='Count');\nplt.title('Top 20 Frequent words in the Speech');\nplt.tight_layout();","b0f9cc1f":"from wordcloud import WordCloud, STOPWORDS\n\ncomment_words = '' \nstopwords = set(STOPWORDS) \n  \n# iterate through the csv file \nfor val in df['lemma_text']: \n      \n    # typecaste each val to string \n    val = str(val) \n  \n    # split the value \n    tokens = val.split() \n      \n    # Converts each token into lowercase \n    for i in range(len(tokens)): \n        tokens[i] = tokens[i].lower() \n      \n    comment_words += \" \".join(tokens)+\" \"\n  \nwordcloud = WordCloud(width = 1900, height = 1500, \n                background_color ='white', \n                stopwords = stopwords, \n                min_font_size = 10).generate(comment_words) \n  \n# plot the WordCloud image                        \nplt.figure(figsize = (8, 8), facecolor = '#7FFFD4') \nplt.imshow(wordcloud) \nplt.axis(\"off\") \nplt.tight_layout(pad = 0) \nplt.title('high frequency words in Speech')  \nplt.show() ","6ef79ea1":"plt.figure(figsize = (10,6))\nsns.set_style('darkgrid')\nsns.set_context(context='notebook',font_scale=1.5)\nUnigram_word = text_ngrams(df.lemma_text,10,1)\nUnigram_word = dict(Unigram_word)\nsns.barplot(x=list(Unigram_word.values()),y=list(Unigram_word.keys()),palette = 'rainbow');\nplt.title('Unigram Analysis');","05ef6d5e":"plt.figure(figsize = (10,6))\nsns.set_style('darkgrid')\nsns.set_context(context='notebook',font_scale=1.5)\nUnigram_word = text_ngrams(df.lemma_text,10,2)\nUnigram_word = dict(Unigram_word)\nsns.barplot(x=list(Unigram_word.values()),y=list(Unigram_word.keys()),palette = 'rainbow');\nplt.title('bigram Analysis');","4f21787e":"plt.figure(figsize = (10,6))\nsns.set_style('darkgrid')\nsns.set_context(context='notebook',font_scale=1.5)\nUnigram_word = text_ngrams(df.lemma_text,10,3)\nUnigram_word = dict(Unigram_word)\nsns.barplot(x=list(Unigram_word.values()),y=list(Unigram_word.keys()),palette = 'rainbow');\nplt.title('Trigram Analysis');","09e903fc":"# data for plot\ndf_sen = df[['Negative','Neutral', 'Positive']]\ndf_sen_avg = df_sen.mean().reset_index()\ndf_sen_avg.columns = ['sentiments','score']\n","19ed37ce":"# plot\nplt.figure(figsize=(10,6))\nsns.set_style('darkgrid')\nsns.set_context(context='notebook',font_scale=1.5)\nsns.barplot(y='score', x='sentiments', data=df_sen_avg)\nplt.title('Sentiments in Speech');","fa24a05b":"from nrclex import NRCLex\n\n\ndef Review_emo(word):\n    \"\"\"\n    Function to convert the raw data to utf-8 formate\n    \n    * convert to data Frame\n    \n    \"\"\"\n    \n    word = str([cell.encode('utf-8') for cell in word])# to convert the text into utf-8 unicode\n    str_text = NRCLex(word) \n    str_text = str_text.raw_emotion_scores\n    str_text = pd.DataFrame(str_text,index=[0])\n    str_text = pd.melt(str_text)\n    str_text.columns = ('Emotions','Count')\n    str_text = str_text.sort_values('Count')\n    \n    return str_text","47947391":"rating_clean = Review_emo(df['lemma_text'])","3fabcf34":"plt.figure(figsize=(10,6))\nsns.set_style('darkgrid')\nsns.set_context(context='notebook',font_scale=1.5)\nsns.barplot(y='Emotions',x='Count',data=rating_clean[0:8],)\nplt.title('Emotional affects in Speech');","2d96c7e9":"### upvote if you like my work","13f49c14":"### Thanks!","9f75232f":"# NRClex\n\n#### Emotional affects measured include the following:\n\n##### fear, anger, anticipation, trust, surprise, positive, egative, sadness, disgust and joy","c68dbb23":"# EDA","6f22378d":"![1_Xj8-Jpi5TppZHA8dFRml6A.jpg](attachment:1_Xj8-Jpi5TppZHA8dFRml6A.jpg)![](http:\/\/)","9f6c4b4f":"# Index\n\n* <a href=\"#Data-import\">Data import<\/a>\n* <a href=\"#Text-preprocessing\">Text preprocessing<\/a>\n* <a href=\"#EDA\">EDA<\/a>\n* <a href=\"#Sentiment-analysis\">Sentiment analysis<\/a>\n","0b37764f":"# Sentiment analysis","420c2d35":"# Data import\n","8aaba01b":"# Trump_speech_Sentiment_Analysis\n","c48c3cfb":"# Text preprocessing\n\n* Removing stop words(stop word are extremely common words \u201cthe\u201d, \u201cis\u201d and \u201cand\u201d)\n\n* Normalization of words (USA has more foams like U.S., United States and so we eliminate data redundancy(repetition)\n\n* Lemmatization (turn the words to base form for example: cars, car's should become car)","99a62767":"# VADER\n#### A SentimentAnalyzer is a tool to implement and facilitate Sentiment Analysis tasks using NLTK features and classifiers, especially for teaching and demonstrative purposes."}}