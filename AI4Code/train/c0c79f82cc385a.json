{"cell_type":{"c8f14d7e":"code","5ff50512":"code","336f6e63":"code","f6bce222":"code","594a0525":"code","40636a10":"code","91508ccd":"code","b556f513":"code","9c9bd498":"code","57f0deee":"code","ccc4384f":"code","8a68ca63":"code","e8c1ea58":"code","76607bed":"code","e353e908":"code","861ebd66":"code","85e26ed1":"code","5a2d1b2a":"code","bafd5bdb":"code","d6aa5027":"code","5cc61f58":"code","ee6639be":"code","a9c58757":"code","465b196a":"code","a8e5aabe":"code","cd51e8bf":"code","acdb0376":"code","0405f688":"code","093504a1":"code","93582db3":"code","29151579":"code","5b123240":"markdown","231012b0":"markdown"},"source":{"c8f14d7e":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","5ff50512":"review = pd.read_csv(\"\/kaggle\/input\/IMDB Dataset.csv\")\nreview.tail()","336f6e63":"import tensorflow as tf","f6bce222":"raw_comments = review['review'].tolist()\n","594a0525":"import re\nSTOPWORDS = '<.*?>|&([a-z0-9]+|#[0-9]{1,6}|#x[0-9a-f]{1,6});'\ncleanwords = re.compile(STOPWORDS)","40636a10":"vocab_size = 1000\nall_comments = []\nall_comments_set = set()\nfor comments in raw_comments:\n    #Limit the vocab size to vovab_size\n    if(len(all_comments_set) > vocab_size):\n        break\n    comments = re.sub(cleanwords,' ',comments)\n    temp = comments.split()\n    all_comments.append(temp)\n    all_comments_set.update(temp) #for adding a list to set","91508ccd":"vocab_size = len(all_comments_set)\n","b556f513":"print(len(all_comments_set))\nword_to_int_dict = {}\nint_to_word_dict = {}\n\nfor i,word in enumerate(all_comments_set):\n    word_to_int_dict[word] = i\n    int_to_word_dict[i] = word","9c9bd498":"print(word_to_int_dict['One'])\nprint(int_to_word_dict[word_to_int_dict['One']])\nvocab_size = len(int_to_word_dict)\nvocab_size","57f0deee":"#The below function will create a oneHot Representation of all words in dictonary\nlen_dict = len(int_to_word_dict)\noneHot = np.zeros((len_dict,len_dict))\nfor key in int_to_word_dict:\n    oneHot[key][key] = 1\n","ccc4384f":"oneHot[2]","8a68ca63":"#let's use a window size of last 3 words to create the context\/target pairs\ncontext_target_pair = []\nfor raw_comments in all_comments:\n    n = len(raw_comments)\n    for i in range(0,n-2):\n        print(\"adding pair :\",raw_comments[i],raw_comments[i+1])\n        pair=(raw_comments[i],raw_comments[i+1])\n        context_target_pair.append(pair)","e8c1ea58":"pair_list = []\n\nfor i in range(0,len(context_target_pair)):\n    word1 = context_target_pair[i][0]\n    value1 = word_to_int_dict[word1]\n    word2 = context_target_pair[i][1]\n    value2 = word_to_int_dict[word2]\n    #print(word1,word2)\n    pair_list.append((value1,value2,1))\n\nprint(len(pair_list))\nprint(pair_list[0:2])\n","76607bed":"#let's use Negative Sampling approach to create word embeddings. This means that for every X,Y pair we need to add K invalid X,Y pairs\nK=4\nimport random\n\ndatalen = len(pair_list)\nfor i in range(0,datalen):\n    for j in range(0,K):\n        rand_int = random.randint(0,vocab_size)\n        pair_list.append((value1,value2,0))\n\nprint(len(pair_list))\nm = len(pair_list)","e353e908":"random.shuffle(pair_list)\npair_list[0:20]","861ebd66":"X_temp = []\nY_temp = []\nX_target_temp = []\nfor i in range(0,m):\n    X_temp.append(oneHot[pair_list[i][0]])\n    Y_temp.append(pair_list[i][2])\n    X_target_temp.append(oneHot[pair_list[i][1]])\n\n#X = pd.DataFrame(X_train,columns=['x1','x2','valid'],dtype='float32')\nX_temp[1:5]","85e26ed1":"X_train = pd.DataFrame(X_temp,dtype='float32')\nX_target_train = pd.DataFrame(X_target_temp,dtype='float32')\n\nY_train = pd.DataFrame(Y_temp,columns=['target'],dtype='float32')","5a2d1b2a":"def create_placeholders(n_x):\n    X1 = tf.placeholder(tf.float32,name='X',shape=(None,n_x))\n    Y1 = tf.placeholder(tf.float32,name='Y',shape=(None))\n    X1_target = tf.placeholder(tf.float32,name='X_target',shape=(None,n_x))\n\n    return X1,Y1,X1_target\n\nEMBEDDING_DIM = 300 \n\ndef get_minibatch(X,Y,X_target,batch_size):\n    m,n = X.shape\n    total_batch = (np.int32)(m\/batch_size)\n    left_over = (np.int32)(m%batch_size)\n\n    minibatches = []\n    start=0\n    for i in range(0,total_batch):\n        end=start+batch_size\n        X_temp = X[start:end]\n        X_target_temp = X_target[start:end]\n        Y_temp = Y[start:end]\n        minibatches.append((X_temp,X_target_temp,Y_temp))\n        start = end\n    \n    X_temp = X[start:m]\n    X_target_temp = X_target[start:m]\n    Y_temp = Y[start:m]\n    minibatches.append((X_temp,X_target_temp,Y_temp))\n    \n    return minibatches\n\ndef init_params(n_x):\n\n    Xinitializer = tf.contrib.layers.xavier_initializer(dtype=tf.dtypes.float32)\n    #xavier_initializer_conv2d is designed to keep the scale of the gradients roughly the same in all layers\n    W1 = tf.Variable(Xinitializer(shape=(EMBEDDING_DIM,vocab_size)))\n    W2 = tf.Variable(Xinitializer(shape=(vocab_size,EMBEDDING_DIM)))\n    W3 = tf.Variable(Xinitializer(shape=(vocab_size,1)))\n\n    b1 = tf.Variable(Xinitializer(shape=(EMBEDDING_DIM,1)))\n    b2 = tf.Variable(Xinitializer(shape=(vocab_size,1)))\n    b3 = tf.Variable(Xinitializer(shape=(1,1)))\n\n    parameters = {\n        'W1' : W1,\n        'W2' : W2,\n        'W3' : W3,\n        'b1' : b1,\n        'b2' : b2,\n        'b3' : b3\n    }\n    \n    return parameters\n\ndef fwd_move(X,Y,X_target,parameters):\n    \n    W1 = parameters['W1']\n    W2 = parameters['W2']\n    b1 = parameters['b1']\n    b2 = parameters['b2']\n    W3 = parameters['W3']\n    b3 = parameters['b3']\n\n\n    print(\"W1 shape is :\",W1.shape)\n    print(\"X shape is :\",X.shape)\n    z1 = tf.matmul(W1,tf.transpose(X)) + b1\n    print(\"z1 shape is :\",z1.shape)\n\n    #We also need to make sure that input at is normalized. For Now we are leaving it\n    a1 = tf.nn.relu(z1)\n    print(\"a1 shape is :\",a1.shape)\n    print(\"W2 shape is :\",W2.shape)\n    print(\"b2 shape is :\",b2.shape)\n\n    z2 = tf.matmul(W2,a1) + b2\n    print(\"z2 shape is :\",z2.shape)\n    a2 = tf.nn.relu(z2)\n\n    #We have a2 here which represents a vector of vocab size binary clasification problems\n    z3 = tf.matmul(X_target,W3) + b3\n    print(\"z3 shape is :\",z3.shape)\n\n    return z3,parameters\n\ndef sigmoid_cost(z,Y):\n\n    logit = z\n    label=Y_train\n    print(\"Logit Shape :\",logit.shape)\n    print(\"Label Shape :\",label.shape)\n    cost = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=logit,labels=label))\n    print(\"Inside Cost function Cost is: \",cost)\n    return cost","bafd5bdb":"def sigmoid(z):\n    return (1\/(1+np.exp(-z)))","d6aa5027":"print(X_target_train.shape)","5cc61f58":"sess = tf.Session()\ndef model(X_train,X_tatget_train,Y_train,num_epochs=1500,minibatch_size=32,learning_rate=0.001):\n    m,n_x = X_train.shape\n    parameters = init_params(n_x)\n    X1,Y1,X_target1 = create_placeholders(n_x)\n    z,params = fwd_move(X_train,Y_train,X_tatget_train,parameters)\n    cost = sigmoid_cost(z,Y_train)\n    optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate).minimize(cost)\n    total_batch = (np.int16)(m\/minibatch_size)\n    init_op = tf.initialize_all_variables()\n    init = tf.global_variables_initializer()\n    #Graph for tensorflow\n    writer = tf.summary.FileWriter('C:\/Users\/sacgupt5\/Documents\/AI\/graphs', sess.graph)\n    sess.run(init_op)\n    sess.run(init)\n    sess.run(parameters)\n    #sess.run(print(\"Shape of W1 is :\",W1.shape))\n    minibatches = get_minibatch(X_train,Y_train,X_target_train,minibatch_size)\n    for epoch in range (0,num_epochs):\n        #print(\"Epoch is :\", epoch)\n        minibatch_cost = 0\n        epoch_minibatch_cost = 0\n\n        for i in range (0,len(minibatches)):\n            X_mini,X_target_mini,Y_mini = minibatches[i]\n            #print(\"Minibatch Shape is \",X1.shape,Y1.shape)\n            #z1,params = sess.run(fwd_move(X1,X_target1,Y1,parameters,feed_dict={X: X_mini, Y: Y_mini,X_target: X_target_mini,parameters:parameters}))\n            _ , epoch_minibatch_cost = sess.run([optimizer, cost], feed_dict={X1: X_mini, Y1: Y_mini,X_target1: X_target_mini})\n            #_ , epoch_minibatch_cost = sess.run([optimizer, cost], feed_dict={z:z1,Y: Y_mini})\n            minibatch_cost = epoch_minibatch_cost + minibatch_cost\n        epoch_cost = minibatch_cost\/total_batch\n\n        if(epoch % 100 == 0):\n            print(\"Epoch Error is \",epoch_cost)\n\n    print(\"Final Error is :\",epoch_cost)\n\n    #z_test,params = sess.run(fwd_move(X_test,Y_test,parameters))\n    #predicted_cost = (sigmoid(z_test))\n    #accuracy = tf.metrics.accuracy((predicted_cost),Y_test)\n    #print(\"Accuracy is : \", accuracy)\n    return parameters","ee6639be":"parameters = model(X_train,X_target_train,Y_train,num_epochs=400,minibatch_size=32,learning_rate=0.001)","a9c58757":"W1 = parameters['W1']\nb1 = parameters['b1']\nprint(sess.run(W1))\nprint(sess.run(b1))\nvector = sess.run(W1+b1)\n\n","465b196a":"temp = np.array(vector)\nEmbed = np.transpose(temp)","a8e5aabe":"Embed.shape","cd51e8bf":"def cosine_similarity(u, v):\n    \"\"\"\n    Cosine similarity reflects the degree of similariy between u and v\n        \n    Arguments:\n        u -- a word vector of shape (n,)          \n        v -- a word vector of shape (n,)\n\n    Returns:\n        cosine_similarity -- the cosine similarity between u and v defined by the formula above.\n    \"\"\"\n\n    distance = 0.0\n    \n    dot = np.dot(u,v)\n    norm_u = np.linalg.norm(u)\n    \n    norm_v = np.linalg.norm(v)\n    cosine_similarity = ((dot)\/(norm_u * norm_v))\n\n    return cosine_similarity","acdb0376":"out = Embed[word_to_int_dict['out']]\nJake  = Embed[word_to_int_dict['Jake']]\nwonderful = Embed[word_to_int_dict['wonderful']]\nlittle = Embed[word_to_int_dict['little']]\nprint(\"cosine_similarity(out,Jake) = \", cosine_similarity(out, Jake))\nprint(\"cosine_similarity(wonderful,little) = \", cosine_similarity(wonderful, little))","0405f688":"watching = Embed[word_to_int_dict['watching']]\nprint(\"cosine_similarity(wonderful,with) = \", cosine_similarity(watching,wonderful))\n","093504a1":"def euclidean_dist(vec1, vec2):\n    return np.sqrt(np.sum((vec1-vec2)**2))\n\ndef find_closest(word_index, vectors):\n    min_dist = 10000 # to act like positive infinity\n    min_index = -1\n    query_vector = vectors[word_index]\n    for index, vector in enumerate(vectors):\n        if euclidean_dist(vector, query_vector) < min_dist and not np.array_equal(vector, query_vector):\n            min_dist = euclidean_dist(vector, query_vector)\n            min_index = index\n    return min_index","93582db3":"print(int_to_word_dict[find_closest(word_to_int_dict['with'],Embed)])\nprint(int_to_word_dict[find_closest(word_to_int_dict['out'],Embed)])","29151579":"#print(int_to_word_dict['out'])\nprint(int_to_word_dict[find_closest(word_to_int_dict['wonderful'],Embed)])","5b123240":"The below code will create word Embeddings","231012b0":"Testing the code****\n**CosineSimilarity(u, v)=(u.v)\/(||u||.||v||)=cos(\u03b8)**"}}