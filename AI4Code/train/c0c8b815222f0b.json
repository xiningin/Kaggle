{"cell_type":{"d570e92d":"code","b309f292":"code","26bef3b4":"code","c9e83500":"code","2294f527":"code","e2145aec":"code","a6f304c8":"code","9f67914b":"code","101a4be4":"code","d6ab1d84":"code","8b643c4f":"code","28b63aaf":"code","7ab379f1":"code","3acbea6e":"code","0eade8ef":"code","957de9c4":"code","c853322b":"code","d4f4e11c":"code","5f22b15c":"code","a0058d61":"code","cc6f0b7a":"code","83cf7f29":"code","f5ae0334":"code","5af4dfa7":"markdown","b4bc1726":"markdown","d367e185":"markdown","6b14ac91":"markdown","6cf269ad":"markdown"},"source":{"d570e92d":"import pandas as pd \n\npath = '\/kaggle\/input\/optiver-realized-volatility-prediction\/'\nfile_path_bk = path + ('book_train.parquet\/stock_id=0')\nfile_path_tr = path + ('trade_train.parquet\/stock_id=0')\n\nbook_example = pd.read_parquet(file_path_bk)\ntrade_example = pd.read_parquet(file_path_tr)\ntarget_example = pd.read_csv (path + 'train.csv')","b309f292":"# this is only for stock_id == 0 \nbook_example.head()","26bef3b4":"# this is only for stock_id == 0 \ntrade_example.head()","c9e83500":"# My model will forecast 'target' or the next 10 minutes window of realized volatility \ntarget_example.head()","2294f527":"# There are 112 different stocks \ntarget_example['stock_id'].unique()","e2145aec":"import numpy as np \nimport pandas as pd \nimport os\nimport glob\nfrom sklearn.model_selection import KFold\nfrom sklearn.cluster import KMeans\nimport lightgbm as lgb\nfrom joblib import Parallel, delayed\nimport matplotlib.pyplot as plt\nimport seaborn as sns","a6f304c8":"path = '\/kaggle\/input\/optiver-realized-volatility-prediction\/'\nfile_path_bk = path + ('book_train.parquet\/stock_id=0')\nfile_path_tr = path + ('trade_train.parquet\/stock_id=0')","9f67914b":"def wap (df_sec, n = 1, cross = True):\n    a,b = 'ask' , 'bid'\n    if cross : return (df_sec[b+'_price' + str (n)] * df_sec[a+'_size'+ str (n)] + df_sec[a+'_price'+ str (n)] * df_sec[b+'_size'+ str (n)])\/(df_sec[b+'_size'+ str (n)] + df_sec[a+'_size'+ str (n)])\n    else : return (df_sec[a+'_price' + str (n)] * df_sec[a+'_size'+ str (n)] + df_sec[b+'_price'+ str (n)] * df_sec[b+'_size'+ str (n)])\/(df_sec[b+'_size'+ str (n)] + df_sec[a+'_size'+ str (n)])\n    \ndef log_return(series):\n    return np.log(series).diff()\n\ndef count_unique(series):\n    return len(np.unique(series))\n\ndef realized_volatility(series):\n    return np.sqrt(np.sum(series**2))\n\ndef high_low_spread(series):\n    return np.max(series) - np.min(series)\ndef high_low_7525_spread(series):\n    return np.percentile(series,75) - np.percentile(series,25)\n\ndef ser_diff(series):\n    return series.diff()\n\ndef sum_above_mean (series) :\n    return np.sum (series > np.mean(series))\ndef sum_below_mean (series) :\n    return np.sum (series < np.mean(series))\n\ndef sum_above_zero (series) :\n    return np.sum ( series > 0 )\ndef sum_below_zero (series) :\n    return np.sum ( series < 0 )\n\ndef std_above_zero (series) :\n    return np.std ( series > 0 )\ndef std_below_zero (series) :\n    return np.std ( series < 0 )\n\ndef sum_sqrt (series) :\n    return np.sqrt(1\/np.sum(series))\n\ndef vwap (price_size ,size) :\n    return np.cumsum (price_size) \/ np.cumsum (size)\n\ndef mean_diff (series) :\n    return np.mean (series.diff())","101a4be4":"def process_bk (file_path) :\n    df_sec = pd.read_parquet (file_path)\n    \n    # adding feature \n    df_sec['wap1'] = wap(df_sec)\n    df_sec['wap2'] = wap(df_sec,2)\n    df_sec['wap1diff'] = df_sec['wap1'].diff()\n    df_sec['log_wap1'] = df_sec.groupby(['time_id'])['wap1'].apply(log_return)\n    df_sec['log_wap2'] = df_sec.groupby(['time_id'])['wap2'].apply(log_return)\n    df_sec['wap_spread'] = abs(df_sec['wap1'] - df_sec['wap2'])\n    df_sec ['bid_spread'] = df_sec ['bid_price1'] - df_sec ['bid_price2']\n    df_sec ['ask_spread'] = df_sec ['ask_price2'] - df_sec ['ask_price1']\n    df_sec ['price_spread1'] = (df_sec ['ask_price1'] - df_sec ['bid_price1'])\/ (df_sec ['ask_price1'] + df_sec ['bid_price1'])\n    df_sec ['price_spread2'] = (df_sec ['ask_price2'] - df_sec ['bid_price2'])\/ (df_sec ['ask_price2'] + df_sec ['bid_price2'])\n    df_sec ['volume_bid'] = df_sec ['bid_size1'] + df_sec ['bid_size2']\n    df_sec ['volume_ask'] = df_sec ['ask_size1'] + df_sec ['ask_size2']\n    df_sec ['volume_total'] = df_sec ['bid_size1'] + df_sec ['ask_size1'] + df_sec ['bid_size2'] + df_sec ['ask_size2']\n    df_sec ['volume_spread'] = abs(df_sec ['bid_size1'] + df_sec ['bid_size2'] - df_sec ['ask_size1'] - df_sec ['ask_size2'])\n    df_sec ['volume_total_1'] = df_sec ['bid_size1'] + df_sec ['ask_size1']\n    df_sec ['volume_spread_1'] = (df_sec ['bid_size1'] - df_sec ['ask_size1']) \/ (df_sec ['bid_size1'] + df_sec ['ask_size1'])\n    df_sec ['bid_ask_spread'] = abs(df_sec ['bid_spread'] - df_sec ['ask_spread'])\n    df_sec['wap3'] = wap(df_sec,n=1,cross = False)\n    df_sec['wap4'] = wap(df_sec,n=2,cross = False)\n    df_sec['log_wap3'] = df_sec.groupby(['time_id'])['wap3'].apply(log_return)\n    df_sec['log_wap4'] = df_sec.groupby(['time_id'])['wap4'].apply(log_return)\n    \n    # dictionary\n    feature_dict1 = {\n                     'wap1diff' : [np.mean],\n                     'log_wap1' : [realized_volatility, np.mean, np.std],\n                     'log_wap2' : [realized_volatility, np.mean, np.std],\n                     'wap_spread' : [np.mean, np.std],\n                     'bid_spread' : [np.mean, np.std],\n                     'ask_spread' : [np.mean, np.std],\n                     'price_spread1': [np.mean, np.std],\n                     'price_spread2': [np.mean, np.std],\n                     'volume_bid': [np.mean],\n                     'volume_ask': [np.mean],\n                     'volume_total': [np.mean],\n                     'volume_spread': [np.mean],\n                     'volume_total_1': [np.mean],\n                     'volume_spread_1': [np.mean, np.std],\n                     'bid_ask_spread' : [np.mean],\n                     'log_wap3' :[realized_volatility],\n                     'log_wap4' : [realized_volatility]\n                    }\n    feature_dict2 = {'log_wap1' : [realized_volatility, np.mean, np.std],\n                   'log_wap2' : [realized_volatility, np.mean, np.std],\n                    'price_spread1': [np.mean, np.std],\n                    'volume_spread': [np.mean],\n                    'volume_total_1': [np.mean],\n                     'log_wap3' :[realized_volatility],\n                     'log_wap4' : [realized_volatility]\n                    }\n    feature_dict3 = {'log_wap1' : [realized_volatility],\n                   'log_wap2' : [realized_volatility]\n                    }\n    \n    df = df_sec.groupby (['time_id']).agg(feature_dict1).reset_index()\n    df.columns = ['_'.join(col) for col in df.columns]\n    df = df.add_prefix('bk_')\n    df['row_id'] = df['bk_time_id_'].apply(lambda x: file_path[ file_path.find('=') + 1: ] + f'-{x}')\n    for window in [200,400] :\n        df_temp = df_sec[df_sec['seconds_in_bucket'] > window].groupby (['time_id']).agg(feature_dict2).reset_index()\n        df_temp.columns = ['_'.join(col) for col in df_temp.columns]\n        df_temp = df_temp.add_prefix('bk_')\n        df_temp = df_temp.add_suffix('_' + str(window))\n        df_temp['row_id'] = df_temp['bk_time_id__' + str(window)].apply(lambda x: file_path[ file_path.find('=') + 1: ] + f'-{x}')\n        df = df.merge (df_temp, how = 'left', on = 'row_id')\n    for window in [100,300,500] :\n        df_temp = df_sec[df_sec['seconds_in_bucket'] > window].groupby (['time_id']).agg(feature_dict3).reset_index()\n        df_temp.columns = ['_'.join(col) for col in df_temp.columns]\n        df_temp = df_temp.add_prefix('bk_')\n        df_temp = df_temp.add_suffix('_' + str(window))\n        df_temp['row_id'] = df_temp['bk_time_id__' + str(window)].apply(lambda x: file_path[ file_path.find('=') + 1: ] + f'-{x}')\n        df = df.merge (df_temp, how = 'left', on = 'row_id')\n    df.drop(['bk_time_id_','bk_time_id__100','bk_time_id__200','bk_time_id__300','bk_time_id__400','bk_time_id__500' ], axis = 1, inplace =True)\n    return df","d6ab1d84":"def process_tr (file_path) :\n    df_sec = pd.read_parquet (file_path)\n    \n    df_sec['log_return'] = df_sec.groupby(['time_id'])['price'].apply(log_return)\n    df_sec['log_return2'] = df_sec.groupby(['time_id'])['log_return'].apply(ser_diff)\n    df_sec['price_size'] = df_sec['price'] * df_sec['size']\n    df_sec['vwap'] = df_sec.groupby(['time_id'])['price_size'].cumsum() \/ df_sec.groupby(['time_id'])['size'].cumsum()\n    df_sec['vwap_log_return'] = df_sec.groupby(['time_id'])['vwap'].apply(log_return)\n    \n    feature_dict1 = {'seconds_in_bucket':[count_unique],\n                     'price' :[high_low_spread],\n                     'size':[np.sum, np.std, high_low_7525_spread, sum_sqrt],\n                     'order_count':[np.sum, np.std,sum_sqrt],\n                     'log_return':[realized_volatility, \n                                   high_low_spread,sum_above_zero,sum_below_zero,\n                                   std_above_zero,std_below_zero],\n                     'log_return2':[np.mean],\n                     'price_size' : [np.mean],\n                     'vwap_log_return' : [realized_volatility, np.std]\n                    }\n    feature_dict2 = {'log_return':[realized_volatility, np.mean, high_low_spread],\n                     'log_return2':[np.std, np.mean],\n                     'vwap' : [np.std],\n                     'vwap_log_return' : [realized_volatility, high_low_spread]\n                    }\n    feature_dict3 = {'log_return':[realized_volatility],\n                     'vwap_log_return' : [realized_volatility]\n                    }\n    df = df_sec.groupby (['time_id']).agg(feature_dict1).reset_index()\n    df.columns = ['_'.join(col) for col in df.columns]\n    df = df.add_prefix('tr_')\n    df['row_id'] = df['tr_time_id_'].apply(lambda x: file_path[ file_path.find('=') + 1: ] + f'-{x}')\n    for window in [300,450] :\n        df_temp = df_sec[df_sec['seconds_in_bucket'] > window].groupby (['time_id']).agg(feature_dict2).reset_index()\n        df_temp.columns = ['_'.join(col) for col in df_temp.columns]\n        df_temp = df_temp.add_prefix('tr_')\n        df_temp = df_temp.add_suffix('_' + str(window))\n        df_temp['row_id'] = df_temp['tr_time_id__' + str(window)].apply(lambda x: file_path[ file_path.find('=') + 1: ] + f'-{x}')\n        df = df.merge (df_temp, how = 'left', on = 'row_id')\n    for window in [200,400,500] :\n        df_temp = df_sec[df_sec['seconds_in_bucket'] > window].groupby (['time_id']).agg(feature_dict3).reset_index()\n        df_temp.columns = ['_'.join(col) for col in df_temp.columns]\n        df_temp = df_temp.add_prefix('tr_')\n        df_temp = df_temp.add_suffix('_' + str(window))\n        df_temp['row_id'] = df_temp['tr_time_id__' + str(window)].apply(lambda x: file_path[ file_path.find('=') + 1: ] + f'-{x}')\n        df = df.merge (df_temp, how = 'left', on = 'row_id')\n    df.drop(['tr_time_id_', 'tr_time_id__200','tr_time_id__300','tr_time_id__400', 'tr_time_id__450','tr_time_id__500' ], axis = 1, inplace =True)\n    return df","8b643c4f":"def process_all (stock_id_list, is_train = True) :\n    path = '\/kaggle\/input\/optiver-realized-volatility-prediction\/'\n    \n    df = pd.DataFrame()\n    \n    def process (stock_id) :\n        if is_train :\n            path_bk = path + f'book_train.parquet\/stock_id={stock_id}' \n            path_tr = path + f'trade_train.parquet\/stock_id={stock_id}' \n        else :\n            path_bk = path + f'book_test.parquet\/stock_id={stock_id}' \n            path_tr = path + f'trade_test.parquet\/stock_id={stock_id}'\n            \n        df_temp = pd.merge (process_bk (path_bk), process_tr (path_tr), how = 'left', on = 'row_id')\n        return pd.concat([df,df_temp])\n    \n    df = Parallel(n_jobs=-1, verbose=1)(\n        delayed(process)(stock_id) for stock_id in stock_id_list\n        )\n\n    df =  pd.concat(df,ignore_index = True)\n    \n    if is_train : \n        target = pd.read_csv (path + 'train.csv')\n        target.insert (2,'row_id',target['stock_id'].astype (str) + '-' + target['time_id'].astype(str))\n        df = target.merge (df, how = 'left', on = 'row_id')\n    \n    else : \n        test = pd.read_csv (path + 'test.csv')\n        df = test.merge (df, how = 'left', on = 'row_id')\n    \n    return df ","28b63aaf":"def adding_more_feature (df) : # do this after process all\n    cols = ['bk_log_wap1_realized_volatility', 'bk_log_wap2_realized_volatility',\n            'bk_log_wap1_realized_volatility_500','bk_log_wap2_realized_volatility_500',\n            'tr_log_return_realized_volatility','tr_vwap_log_return_realized_volatility',\n            'tr_log_return_realized_volatility_300','tr_vwap_log_return_realized_volatility_300',\n            'tr_log_return_realized_volatility_400','tr_vwap_log_return_realized_volatility_400',\n            'tr_log_return_realized_volatility_500','tr_vwap_log_return_realized_volatility_500'\n           ]\n    categories = ['time_id']\n\n    for cat in categories :\n        df_temp = df.groupby([cat])[cols].agg([np.nanmean, np.nanstd, high_low_spread, high_low_7525_spread]).reset_index()\n        df_temp.columns = ['_'.join(col) for col in df_temp.columns]\n        df_temp = df_temp.add_prefix(cat + '_')\n        df = pd.merge (df,df_temp,how = 'left', left_on = cat, right_on = (cat + '_')*2 )\n        df.drop([(cat + '_')*2],axis = 1,inplace = True)\n    \n    return df","7ab379f1":"def clustering ( n_c = 7) : # n_c is number of cluster \n    path = '\/kaggle\/input\/optiver-realized-volatility-prediction\/'\n    df_cluster = pd.read_csv (path +'train.csv')\n\n        \n    df_cluster = df_cluster.pivot(index='time_id', columns='stock_id', values='target')\n\n    corr = df_cluster.corr()\n    kmeans = KMeans(n_clusters=n_c, random_state=0).fit(corr.values)\n\n    cluster_list = [ ]\n    for n in range(n_c):\n        cluster_list.append ( [ (x-1) for x in ( (corr.index+1)*(kmeans.labels_ == n)) if x > 0] )\n    return cluster_list","3acbea6e":"def adding_clustering_feature (df, clustering_list , cluster_ids = []):\n    \n    if not cluster_ids : cluster_ids = [i for i in range (len (clustering_list))]\n    df_cluster = pd.DataFrame()\n    \n    for i,cl in enumerate (clustering_list):\n        if i not in cluster_ids : continue \n        df_ini = pd.DataFrame({'time_id' : df['time_id'].unique()})\n        df_temp = df.loc[df['stock_id'].isin(cl)]\n        df_temp = df_temp.groupby(['time_id']).agg(np.nanmean).reset_index()\n        df_temp = pd.merge(df_ini, df_temp, how = 'left', on = 'time_id')\n        df_temp.insert (2,'cluster_id', 'cluster_' + str(i))\n        df_temp.drop(['stock_id'], axis = 1, inplace =True)\n        df_cluster = pd.concat ([df_cluster,df_temp])\n    \n    df_cluster.reset_index( drop = True, inplace = True)\n    df_cluster = df_cluster.pivot (index='time_id', columns='cluster_id')\n    df_cluster.columns = [\"_\".join(x) for x in df_cluster.columns]\n    df_cluster.reset_index (inplace = True)\n    \n    def cluster_selection (cl) :\n        selections = cl[\n            cl.columns[cl.columns == 'time_id'] |\n            cl.columns[cl.columns.str.contains('bk_wap1diff_mean_cluster_')] |\n            cl.columns[cl.columns.str.contains('bk_log_wap1_realized_volatility_cluster_')] |\n            cl.columns[cl.columns.str.contains('bk_log_wap2_std_cluster_')] |\n            cl.columns[cl.columns.str.contains('bk_wap_spread_mean_cluster_')] |\n            cl.columns[cl.columns.str.contains('bk_bid_spread_std_cluster_')] |\n            cl.columns[cl.columns.str.contains('bk_ask_spread_std_cluster_')] |\n            cl.columns[cl.columns.str.contains('bk_price_spread1_mean_cluster_')] |\n            cl.columns[cl.columns.str.contains('bk_price_spread2_mean_cluster_')] |\n            cl.columns[cl.columns.str.contains('bk_volume_total_mean_cluster_')] |\n            cl.columns[cl.columns.str.contains('bk_volume_spread_mean_cluster_')] |\n            cl.columns[cl.columns.str.contains('bk_volume_spread_1_mean_cluster_')] |\n            \n            cl.columns[cl.columns.str.contains('tr_seconds_in_bucket_count_unique_cluster_')] |\n            cl.columns[cl.columns.str.contains('tr_price_high_low_spread_cluster_')] |\n            cl.columns[cl.columns.str.contains('tr_size_sum_cluster_')] |\n            cl.columns[cl.columns.str.contains('tr_size_sum_sqrt_cluster_')] |\n            cl.columns[cl.columns.str.contains('tr_order_count_sum_cluster_')] |\n            cl.columns[cl.columns.str.contains('tr_order_count_sum_sqrt_cluster_')] |\n            cl.columns[cl.columns.str.contains('tr_log_return_realized_volatility_cluster_')] |\n            cl.columns[cl.columns.str.contains('tr_log_return2_mean_cluster_')] |\n            cl.columns[cl.columns.str.contains('tr_price_size_mean_cluster_')] |\n            cl.columns[cl.columns.str.contains('tr_vwap_log_return_realized_volatility_cluster_')]\n        ]\n        return selections\n    \n    df_cluster = cluster_selection(df_cluster)\n    \n    df = pd.merge (df,df_cluster, how = 'left', on = 'time_id')\n    \n    return df","0eade8ef":"# features \n\ntrain = pd.read_csv (path + 'train.csv')\ntest = pd.read_csv (path + 'test.csv')\nstock_id_list_train = train.stock_id.unique()\nstock_id_list_test = test.stock_id.unique()\n\ndf = process_all (stock_id_list_train)\ndf_test = process_all (stock_id_list_test, is_train = False)\n\n#df = process_all ([0])\n#df_test = process_all ([0], is_train = False)\n\ndf = adding_more_feature(df)\ndf_test = adding_more_feature(df_test)\n\nclustering_list = clustering(5)\n\ndf = adding_clustering_feature(df, clustering_list)\ndf_test = adding_clustering_feature(df_test, clustering_list)","957de9c4":"# Model\ndef rmspe(y_true, y_pred):\n    return np.sqrt(np.mean(np.square((y_true - y_pred) \/ y_true)))\n\ndef feval_rmspe(y_pred, lgb_train):\n    y_true = lgb_train.get_label()\n    return 'RMSPE', rmspe(y_true, y_pred), False\n\ndef train_and_evaluate_lgb(train, test, params):\n    \n    features = [col for col in train.columns if col not in {\"time_id\", \"target\", \"row_id\"}]\n    y = train['target']\n    \n    oof_predictions = np.zeros(train.shape[0])\n    test_predictions = np.zeros(test.shape[0])\n    \n    \n    kfold = KFold(n_splits = 5, random_state = 2021, shuffle = True)\n    \n    for fold, (trn_ind, val_ind) in enumerate(kfold.split(train)):\n        print(f'Training fold {fold + 1}')\n        x_train, x_val = train.iloc[trn_ind], train.iloc[val_ind]\n        y_train, y_val = y.iloc[trn_ind], y.iloc[val_ind]\n        \n        train_weights = 1 \/ np.square(y_train)\n        val_weights = 1 \/ np.square(y_val)\n        train_dataset = lgb.Dataset(x_train[features], y_train, weight = train_weights)\n        val_dataset = lgb.Dataset(x_val[features], y_val, weight = val_weights)\n        \n        model = lgb.train(params = params,\n                          train_set = train_dataset, \n                          valid_sets = [train_dataset, val_dataset], \n                          verbose_eval = 250,\n                          feval = feval_rmspe)\n        \n        oof_predictions[val_ind] = model.predict(x_val[features])\n        test_predictions += model.predict(test[features]) \/ 5\n        \n    rmspe_score = rmspe(y, oof_predictions)\n    print(f'Our out of folds RMSPE is {rmspe_score}')\n    lgb.plot_importance(model,max_num_features=20)\n    \n    return test_predictions, oof_predictions, model","c853322b":"seed1=2021\nparams1 = {\n    'objective': 'rmse',\n    'boosting_type': 'gbdt',\n    'max_depth': -1,\n    'max_bin':100,\n    'min_data_in_leaf':500,\n    'learning_rate': 0.05,\n    'subsample': 0.72,\n    'subsample_freq': 4,\n    'feature_fraction': 0.5,\n    'lambda_l1': 0.5,\n    'lambda_l2': 1.0,\n    'categorical_column':[0],\n    'seed':seed1,\n    'feature_fraction_seed': seed1,\n    'bagging_seed': seed1,\n    'drop_seed': seed1,\n    'data_random_seed': seed1,\n    'n_jobs':-1,\n    'verbose': -1,\n    'num_boost_round':1400,\n    'early_stopping_rounds' : 30\n\n}\n\nseed2=1111\nparams2 = {\n    'objective': 'rmse',\n    'boosting_type': 'gbdt',\n    'max_depth': -1,\n    'max_bin':100,\n    'min_data_in_leaf':500,\n    'learning_rate': 0.05,\n    'subsample': 0.72,\n    'subsample_freq': 4,\n    'feature_fraction': 0.5,\n    'lambda_l1': 0.5,\n    'lambda_l2': 1.0,\n    'categorical_column':[0],\n    'seed':seed2,\n    'feature_fraction_seed': seed2,\n    'bagging_seed': seed2,\n    'drop_seed': seed2,\n    'data_random_seed': seed2,\n    'n_jobs':-1,\n    'verbose': -1,\n    'num_boost_round':1200,\n    'early_stopping_rounds' : 50\n}","d4f4e11c":"# Training \npredictions_lgb1, oof_predictions1, model1= train_and_evaluate_lgb(df, df_test,params1)\npredictions_lgb2, oof_predictions2, model2= train_and_evaluate_lgb(df, df_test,params2)","5f22b15c":"def feature_importance (x,model):\n    num = 20 \n    fig_size = (40, 20)\n    feature_imp = pd.DataFrame({'Feature':x.columns,'Value':model.feature_importance()})\n    plt.figure(figsize=fig_size)\n    sns.set(font_scale = 5)\n    sns.barplot(x=\"Value\", y=\"Feature\", data=feature_imp.sort_values(by=\"Value\", \n                                                        ascending=False)[0:num])\n    plt.show ()\n    return feature_imp","a0058d61":"fi1 = feature_importance(df.drop(columns=['target','row_id','time_id'],axis = 1), model1)\nfi2 = feature_importance(df.drop(columns=['target','row_id','time_id'],axis = 1), model2)","cc6f0b7a":"# Prediction consists 2 model\npreds = []\nfor i in range (30,71) :\n    i = i\/100\n    series = (oof_predictions1 * i) + (oof_predictions2 * (1-i))\n    preds.append (rmspe (df['target'],series) )\n\npercentage = (30 + preds.index (min(preds))) \/ 100","83cf7f29":"# Prediction \nprediction = (predictions_lgb1*percentage) + (predictions_lgb2*(1-percentage) )\nsubmission = pd.concat ([df_test['row_id'],pd.Series (prediction)], axis = 1)\nsubmission.columns = ['row_id','target']\nsubmission","f5ae0334":"submission.to_csv('submission.csv',index = False)","5af4dfa7":"# RAW DATA \n#### Order Book, Execute Trade, and Target\n","b4bc1726":"# Project Description \n\nIn this competition, **I built a model that predicted short-term volatility for hundreds of stocks across different sectors**. Optiver provided hundreds of millions of rows of highly granular financial data that is used to forecast volatility over 10 minutes periods. My model will be evaluated against real market data collected in the three-month evaluation period after training period (evaluation period ends on January 10 2022).","d367e185":"# Data\n\n#### Train Data - available for us\nMy model is designed utilizing Train data. Train data includes order book snapshots and executed trades with one second resolution. Order book snapshot provides top two levels of ask and bid. Executed trades contain data on trades that actually executed. \n\n#### Test Data - not available for us \nI had 5 submissions each day to test my model against the test data. My model is evaluated based on RMPSE  ( Root Mean Square Percentage Error ). The best submission I had was 20.78%\n\n#### Evaluation Data - not available for us \nThis data is collected after the training period is over. The goal for this competition was to have minimum RMPSE against evaluation data.\n","6b14ac91":"#### Challenges\n* Time IDs are not necessarily sequential\n* We only given the stock_id, not the real ticker\n* All of the price have already been normalize\n* The normalization of the price happened in every time_id which mean same stock_id with the same price but different time_id, possibly have different un-normalize price ","6cf269ad":"# Features and Model  \n#### Features\nFrom the raw data, I compute multiple features that help to predict future volatility. In the book data, I add WAP or weighted average price, log return, spread and volume total. In the Trade Data, I add VWAP or volume weighted average price, log return and price volume. After that, I grouped those features by time_id based on multiple functions. All of this steps was done in process_bk and process_tr function \n\n#### Clustering\n1. with K-Means, I clustered the stock based on their target. And then I selected multiple features to calculate its mean based on their cluster. This steps was done in clustering function and adding_clustering_feature function \n2. I selected multiple features, grouped them based on their time_id, and applied multiple functions. This step was done in adding_more_feature function \n\n#### Model\nI chose Light Gradient Boost because it is very fast, and high performance. In some cases, Light GBM produces better accuracy than XGBoost and other boosting algorithms.\n\n"}}