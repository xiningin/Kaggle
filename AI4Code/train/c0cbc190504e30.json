{"cell_type":{"ea06f238":"code","9c9f03b2":"code","b3f3131a":"code","58068e4a":"code","a13375e6":"code","d248828d":"code","0a1f5913":"code","a712fb38":"code","cf462637":"code","9e7f1c30":"code","ec2715f3":"code","0db590a6":"code","9f950555":"code","e92dd8fe":"code","19b1f843":"code","e7f53386":"code","82ac025f":"code","9bdfdb6f":"code","f80f35a9":"code","8daf5256":"code","1a15f334":"code","b0bb2d2a":"code","ecedd3a0":"markdown","548bd6aa":"markdown","4e6c4d48":"markdown","8b67dc34":"markdown","448877eb":"markdown","3fcaaefc":"markdown","82f782c2":"markdown","6e1c3708":"markdown","976fe188":"markdown","8a9def7e":"markdown","b4a608b6":"markdown","305d079d":"markdown","6a821ad8":"markdown","e852abfa":"markdown","5689835c":"markdown","59c6d2b8":"markdown"},"source":{"ea06f238":"# Importing necessary libraries.\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport cv2\nfrom tqdm.notebook import tqdm\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Input, Dense, Dropout, Flatten, Conv2D, MaxPool2D\nimport seaborn as sns\nimport tensorflow.keras.backend as K\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import f1_score \nfrom tensorflow.keras import optimizers\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nsns.set()","9c9f03b2":"train_datagen = ImageDataGenerator(rescale=1.\/255,\n                                   shear_range=0.3,\n                                   zoom_range=0.3,\n                                   width_shift_range=0.3,\n                                   height_shift_range=0.3)\n\ntest_datagen = ImageDataGenerator(rescale=1.\/255)\n\ntrain_generator = train_datagen.flow_from_directory('..\/input\/hand-gesture-to-emoji-data\/emoji_data\/data\/Train',\n                                                    target_size=(150, 150),\n                                                    batch_size=2,\n                                                    color_mode='grayscale',\n                                                    class_mode=\"sparse\",\n                                                    shuffle=True)\n\nvalidation_generator = test_datagen.flow_from_directory('..\/input\/hand-gesture-to-emoji-data\/emoji_data\/data\/Valid',\n                                                        target_size=(150, 150),\n                                                        batch_size=2,\n                                                        color_mode='grayscale',\n                                                        class_mode=\"sparse\")","b3f3131a":"# Metrics for checking the model performance while training\nimport tensorflow as tf\ndef f1score(y, y_pred):\n  return f1_score(y, tf.math.argmax(y_pred, axis=1), average='micro')\n\ndef custom_f1score(y, y_pred):\n  return tf.py_function(f1score, (y, y_pred), tf.double)","58068e4a":"# Callback for stopping the learning if the model reaches a f1-score > 0.999 on the CV\/test data.\nclass stop_training_callback(tf.keras.callbacks.Callback):\n  def on_epoch_end(self, epoch, logs={}):\n    if(logs.get('val_custom_f1score') > 0.98 and logs.get('custom_f1score') > 0.98):\n      self.model.stop_training = True","a13375e6":"# Model architecture.\nK.clear_session()\nip = Input(shape = (150,150,1))\nz = Conv2D(filters = 32, kernel_size = (64,64), padding='same', input_shape = (150,150,1), activation='relu')(ip)\nz = Conv2D(filters = 64, kernel_size = (16,16), padding='same', input_shape = (150,150,1), activation='relu')(z)\nz = Conv2D(filters = 128, kernel_size = (8,8), padding='same', input_shape = (150,150,1), activation='relu')(z)\nz = MaxPool2D(pool_size = (4,4))(z)\nz = Flatten()(z)\nz = Dense(32, activation='relu')(z)\nop = Dense(5, activation='softmax')(z)\nmodel = Model(inputs=ip, outputs=op)\nmodel.compile(loss='sparse_categorical_crossentropy', optimizer=optimizers.Adam(lr=0.00001), metrics=[custom_f1score])","d248828d":"model.summary()","0a1f5913":"callbacks = [stop_training_callback()]\n# model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=10, verbose=1, callbacks=callbacks)\nmodel.fit(train_generator,\n          epochs=10,\n          validation_data=validation_generator,\n          callbacks=callbacks,\n          verbose=1)","a712fb38":"test_generator = test_datagen.flow_from_directory('..\/input\/hand-gesture-to-emoji-data\/emoji_data\/data\/Valid',\n                                                    target_size=(150, 150),\n                                                    batch_size=1,\n                                                    color_mode='grayscale',\n                                                    class_mode=\"sparse\")","cf462637":"num = 0\ny_pred = []\ny_true = []\nfor img, y_actual in test_generator:\n    if num==600:\n        break\n    pred_label = model.predict(img).argmax()\n    y_pred.append(pred_label)\n    y_true.append(y_actual[0])\n    num+=1","9e7f1c30":"# Confusion matrix to check the misclassified points in test\/CV data.\ncm = confusion_matrix(y_true, y_pred)\ndf_cm = pd.DataFrame(cm, index=np.arange(5), columns=np.arange(5))\nplt.figure(figsize = (10,7))\nsns.heatmap(df_cm, annot=True)\nplt.xlabel('predicted class labels')\nplt.ylabel('actual class labels')\nplt.title('confusion matrix for first 600 images in validation data')\nplt.show()","ec2715f3":"# sample images from test\/CV data and their actual and predicted class labels.\nplt.figure(figsize=(16,16))\nfor i,j in enumerate(test_generator):\n    if i==16:\n        break\n    pred_label = model.predict(j[0]).argmax()\n    actual_label = j[1][0]\n    plt.subplot(4,4,i+1)\n    img = j[0][0][:,:,0]\n    plt.imshow(img, cmap='gray')\n    plt.title(f'actual: {actual_label} | predicted: {pred_label}')\n    plt.axis('off')\nplt.show()","0db590a6":"model.save_weights('model_weights.h5')","9f950555":"# importing necessary libraries\nimport cv2\nimport time\nimport numpy as np\nimport tensorflow.keras.backend as K\nfrom sklearn.metrics import f1_score\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.optimizers import RMSprop\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPool2D, Input\nfrom tensorflow.keras import optimizers\nimport tensorflow as tf","e92dd8fe":"# Defining the model architecture.\ndef create_model():\n    K.clear_session()\n    ip = Input(shape = (150,150,1))\n    z = Conv2D(filters = 32, kernel_size = (64,64), padding='same', input_shape = (150,150,1), activation='relu')(ip)\n    z = Conv2D(filters = 64, kernel_size = (16,16), padding='same', input_shape = (150,150,1), activation='relu')(z)\n    z = Conv2D(filters = 128, kernel_size = (8,8), padding='same', input_shape = (150,150,1), activation='relu')(z)\n    z = MaxPool2D(pool_size = (4,4))(z)\n    z = Flatten()(z)\n    z = Dense(32, activation='relu')(z)\n    op = Dense(5, activation='softmax')(z)\n    model = Model(inputs=ip, outputs=op)\n    return model\n\n# Loading the pretrained weights.\nmodel = create_model()\nmodel.load_weights('model_weights.h5')","19b1f843":"# Detecting the hand using YOLOv3 or HAAR cascade.\ndef hand_detect(img, type='haar'):\n\n    cv2.imshow('output', img)\n    key = cv2.waitKey(20)\n    if key == 27:  # exit on ESC\n        cv2.destroyAllWindows()\n\n    hand_img = img.copy()\n    if type=='yolo':\n        width, height, inference_time, results = yolo.inference(img) # getting the hand inferences using YOLOv3.\n    elif type=='haar':\n        results = cascade.detectMultiScale(hand_img, scaleFactor=1.3, minNeighbors=7) # getting the hand inferences using HAAR cascade.\n\n    if len(results) > 0:\n        if type=='yolo':\n            _,_,_, x, y, w, h = results[0] # returning the hand location.\n            return x,y,150,150\n        elif type=='haar':\n            x, y, w, h = results[0] # returning the hand location.\n            return x-50,y-70,150,150\n    else:\n        return []","e7f53386":"# Bringing all together.\ndef main():\n    roi = []\n    # Initial loop for detecting the fist\/palm and defining a ROI.\n    while(len(roi) == 0):\n        ret, frame = cap.read()\n        frame = cv2.flip(frame, 1)\n        roi = hand_detect(frame)\n\n    cv2.destroyAllWindows()\n    ret = tracker.init(frame, roi) # initializing the tracker using ROI.\n    cv2.destroyAllWindows()\n\n    c = 0\n    d = c+1\n\n    while True:\n        ret, frame = cap.read() # getting the input frames\n        frame = cv2.flip(frame, 1)\n        success, roi = tracker.update(frame) # tracking the ROI in the new input frame\n        (x,y,w,h) = tuple(map(int, roi)) # defining the ROI box's coordinates.\n        if success:\n            pt1 = (x,y)\n            pt2 = (x+w, y+h)\n            square  = frame[y:y+h, x:x+w] # extracting the image(hand gesture) inside ROI.\n            gray = cv2.cvtColor(square, cv2.COLOR_BGR2GRAY) # converting the image to grayscale for contour detection.\n            gray_ = cv2.medianBlur(gray, 7) # applying blur to reduce the salt and pepper noise.\n            hand = contours(gray, th=150)[0] # converting the grayscale image to binary.\n            im = np.array([hand]) # adding a last dimension to the image array to make it model input compatible.\n            im = im.reshape(-1,150,150,1) # reshaping the image to be perfectly model compatible.\n            result = pred(im) # getting the class label for the image from pretrained model.\n            \n            cv2.rectangle(frame, pt1, pt2, (255,255,0), 3) # Drawing a rectangle around the detected ROI.\n            emo = icon(result) # Getting the emoji corresponding to the predicted class label.\n\n            frame_copy = paste(frame, emo) # Pasting the emoji image over the ROI in input frame\n            # Displaying the extracted ROI (what the model sees for prediction) on the top left corner.\n            (a,b) = hand.shape\n            for i in range(3):\n                hand = hand*255\n                frame_copy[0:a, 0:b, i] = hand\n\n        # retry the above steps if the tracker fails to detect a hand.\n        else:\n            cv2.putText(frame, 'Failed to detect object', (100,200), cv2.FONT_HERSHEY_SIMPLEX, 2, 255)\n            cv2.imshow('output', frame_copy)\n            roi = []\n\n            while(len(roi) == 0):\n                ret, frame = cap.read()\n                frame = cv2.flip(frame, 1)\n                roi = hand_detect(frame)\n            continue\n        cv2.imshow('output', frame_copy)\n        if cv2.waitKey(1) & 0xFF == 27:\n            break\n\n    cap.release()\n    cv2.destroyAllWindows()","82ac025f":"# This function defines the contour of the hand gesture and converts the image to binary.\ndef contours(diff, th=100):\n    _ , thresholded = cv2.threshold(diff, th, 255, cv2.THRESH_BINARY_INV)\n    contours, hierarchy = cv2.findContours(thresholded.copy(), cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n    if len(contours) == 0:\n        return None\n    else:\n        hand_segment = max(contours, key=cv2.contourArea)\n    return (thresholded, hand_segment)","9bdfdb6f":"# Predicting the class label of the hand gesture in ROI.\ndef pred(img):\n    return model.predict(img).argmax(axis=1)[0]","f80f35a9":"# Loading the 5 emoji images into a list\nemoji = []\nfor i in range(1,6):\n    img = cv2.imread(f'.\/emoji\/{i}.png', -1)\n    emoji.append(img)","8daf5256":"# This function returns the file path to the emoji picture.\ndef icon(x, e=emoji):\n    return e[x - 1]","1a15f334":"# Pasting the emoji image over the ROI.\ndef paste(frame, s_img, pt1, pt2):\n    x, y = pt1\n    x_w, y_h = pt2\n    n = min(x_w-x, y_h-y)\n    s_img = cv2.resize(s_img, dsize=(n,n))\n    l_img = frame\n    (x_offset,y_offset,_) = (frame.shape)\n    (y_offset,x_offset) = (x_offset\/\/2 - 72, y_offset\/\/2 - 72)\n    y1, y2 = y_offset, y_offset + s_img.shape[0]\n    x1, x2 = x_offset, x_offset + s_img.shape[1]\n\n    alpha_s = s_img[:, :, 3] \/ 255.0\n    alpha_l = 1.0 - alpha_s\n\n    for c in range(0, 3):\n        l_img[y:y_h, x:x_w, c] = (alpha_s * s_img[:, :, c] + alpha_l * l_img[y:y_h, x:x_w, c])\n        \n    return l_img","b0bb2d2a":"# Defining the hand detection methods haar cascads and YOLO.\ncascade = cv2.CascadeClassifier('haarcascade\/fist.xml')\nyolo = YOLO(\".\/models\/cross-hands.cfg\", \".\/models\/cross-hands.weights\", [\"hand\"])\n# Defining multiple tracking objects for tracking the hand.\ntk = [cv2.TrackerBoosting_create(), cv2.TrackerMIL_create(), cv2.TrackerKCF_create(), cv2.TrackerTLD_create(), cv2.TrackerCSRT_create()]\ntracker = tk[2]\ncap = cv2.VideoCapture(0)\nmain()","ecedd3a0":"## Preparing the data\n### Now we'll convert the data into a numpy array, split into train and test\/CV sets and reshape it to make it model compatible.","548bd6aa":"### In the dataset we have around 3000 images for each of the 5 class labels.\n### Let's take a look at the emoji images that correspond to the 5 class lables:\n<img src=\"https:\/\/i.ibb.co\/B46MbgQ\/Screen-Shot-2020-08-15-at-3-38-40-AM.png\" alt=\"Screen-Shot-2020-08-15-at-3-38-40-AM\" border=\"0\">","4e6c4d48":"## About the data:\n### Since this is a 5 class classification problem, we need data for each of the 5 classes to train our model. To make things even simpler and fast, we'll be using binary images for training. Let's take a look at some of the images from each of the 5 classes that we'll be using.\n<img src=\"https:\/\/i.ibb.co\/6vmhbCD\/download-2.png\" alt=\"download-2\" border=\"0\">","8b67dc34":"## Collecting the data\n#### If we take a look at the file system of the images, inside the data folder, there are 5 folders from class_1 to class_5 and inside each folder we have 1460 images from class_i_1 to class_i_1460. The images are named in a format accepted by the keras ImageDataGenerator library. Eg. For an image with name class_x_y.jpg 'x' corresponds the class that image belongs to and 'y' corresponds to the image number in that class. So, in our case, x will range from 1 to 5 and y will range from 1 to 1460.\n<img src=\"https:\/\/i.ibb.co\/X5vwbjZ\/Screen-Shot-2020-08-15-at-12-56-54-AM.png\" alt=\"Screen-Shot-2020-08-15-at-12-56-54-AM\" border=\"0\">\n\n#### I'm using ImageDataGenerator because it provides multiple image augmentation functions like rotation, zoom, width and height shift etc. And since we have only 1460 images to begin with, we should perform some image augmentation.\n#### You can read more about ImageDataGenerator using the links below:\n[Keras preprocessing documentation](https:\/\/keras.io\/api\/preprocessing\/image\/)\n\n[Creating a dataset for ImageDataGenerator](https:\/\/medium.com\/@vijayabhaskar96\/tutorial-image-classification-with-keras-flow-from-directory-and-generators-95f75ebe5720)\n\nFind the code for generating the data on my github repo using [this](https:\/\/github.com\/SarthakV7\/AI-Hand-gesture-emoji-detection\/blob\/master\/data_creator.py) link","448877eb":"### A few days back, I saw this cool app on the internet that would look at your hand using the webcam and based on the palm gesture, it tries to come up with a similar emoji.\n### The idea was simple yet amusing to watch. Next thing I thought was why not code something similar since it would be a decent hands-on project for anyone beginning with deeplearning.","3fcaaefc":"## Training the CNN Model.","82f782c2":"## Saving the model weights so that we can directly use the pretrained model in part 2.","6e1c3708":"## Let's begin with the part 1 of this project. We'll go through 5 steps in this part:\n* Collecting the data.\n* Preparing the data.\n* Designing the CNN architecture and defining the evaluation metrics.\n* Training the CNN Model.\n* Analysing the model performance.\n* Saving the model weights so that we can directly use the pretrained model in part 2.\n\n### Let's begin by importing the necessary libraries.","976fe188":"### Thanks for reading. Hope you found this project useful! \n### Find all the files on my github repo using [this link](https:\/\/github.com\/SarthakV7\/AI-Hand-gesture-emoji-detection).","8a9def7e":"## YOLO vs HAAR cascade\n#### YOLO is much accurate and robust than haar cascade but it takes much more time than haar cascade to process the image.  Whereas Haar cascade being faster, has its own drawback. It sometimes misses to detect the fist or palm im low lighting condition. So, if you are running this code on a micro-controller or a low end PC, Haar cascade would be a good choice.","b4a608b6":"#### The codeblock below can be executed to translate the hand gesture to emoji in realtime.","305d079d":"## Let's begin with the part 2 of this project. We'll go through 7 steps in this part:\n* Taking the input frame using the user's webcam.\n* Searching for a fist in the input frame using pre-trained YOLOv3.\n* If we find a fist, we'll localize it by defining a box around it. The box will be our region of interest (ROI).\n* We'll also add a tracking functionality for the ROI so that it keeps the fist\/palm as the ROI even if the hand moves anywhere inside the frame.\n* Next we'll extract the image inside the ROI from the complete image and feed this segmented image to the pretrained model (from part 1).\n* The next step would be to get the predicted class label from the pretrained model and replace the ROI with the emoji associated with the predicted class.\n* Finally, we'll repeat the above steps in a loop so that the processes keep on repeating on the input image frames coming from the video.\n\n### We'll start by importing libraries and defining some functions.","6a821ad8":"## Designing the CNN architecture and defining the evaluation metrics.","e852abfa":"## Project overview: I've divided the project into 2 parts:\n### 1. Part 1: In this part we'll look at how to design and train a CNN that can predict the input image of the hand gesture and predict the class label.\n### 2. Part 2: In this part we'll take the input from user and predict the hand gesture in realtime. The steps are:\n- Capturing image stream (video) from the user's webcam.\n- Detecting the hand\n- Segmenting out the hand gesture image\n- Predicting the class label of the hand gesture\n- Replacing the hand gesture from the original input with the emoji picture that corresponds to the predicted class.\n","5689835c":"## Analyzing the model performance.","59c6d2b8":"<img src=\"https:\/\/i.ibb.co\/7nwYnCB\/Screen-Shot-2020-08-14-at-4-35-56-AM.png\" alt=\"Screen-Shot-2020-08-14-at-4-35-56-AM\" border=\"0\" \/>"}}