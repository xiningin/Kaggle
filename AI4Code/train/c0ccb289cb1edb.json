{"cell_type":{"8ebbef2e":"code","e55b6a50":"code","a2e92bb3":"code","23190634":"code","f1c3c3ce":"code","dffef176":"code","8dbe1392":"code","a11cfdb3":"code","8b4b9560":"code","ffda2686":"code","774eec5f":"code","fb59b3f4":"code","6c95d4b6":"code","fc86ed68":"code","61238976":"code","055c45d6":"code","902e3313":"code","7eb574a6":"code","35cd546e":"code","debbc556":"code","b81795d8":"code","88b7ad02":"code","c712de1d":"code","09a44d14":"code","35da23db":"code","616285a3":"code","2bcc6fb5":"code","7c5e2355":"code","a724612a":"code","62496238":"code","9b0b21ca":"code","4cfcaa86":"code","d6dc33d9":"code","a3b0ae1c":"code","eb503948":"code","656307ef":"code","ca776d30":"code","61d4fa91":"code","98258d83":"code","32335f12":"code","de3a680c":"code","258da4dd":"code","07e33e2b":"code","dfca2e44":"code","8f050068":"code","b7ca2ec2":"code","65ae28ff":"code","14f6f557":"code","a00405fb":"code","84ced3f4":"code","927d5f52":"code","20d8d609":"code","d52cad9c":"code","b06224ee":"code","653b5cf2":"code","d85cbce6":"code","1d9d55bf":"code","5c17bd93":"code","35e8226c":"code","726de10e":"code","aa15e342":"code","625471d9":"code","27c06107":"code","ccb7abad":"code","3dba63be":"code","75bfa1cc":"code","7cc92ce4":"code","e2ef526c":"code","f145b846":"code","7aa9c0a7":"code","f27df0e1":"code","08b0d082":"code","fddbe9eb":"code","7dfa3475":"code","de61be6b":"code","c2b903a8":"code","e300e527":"code","4f170e5b":"code","f2b44a1d":"code","23e97422":"markdown","99b062c1":"markdown","7cc91680":"markdown","94dd388d":"markdown","85f13046":"markdown","d2dfa2ae":"markdown","84f7574a":"markdown","0bfdfdea":"markdown","5f7704e5":"markdown","8a28ebcc":"markdown","5b42fc29":"markdown","258e1ba1":"markdown","dfe7fa17":"markdown","2994b733":"markdown","68ead5a3":"markdown","f3f68e40":"markdown","d5ebe166":"markdown","7262960b":"markdown","7fc01685":"markdown","74e7ed8e":"markdown","91bcc967":"markdown","f39cd24c":"markdown","d8336c36":"markdown","d00d0f18":"markdown","333dde45":"markdown","add05dd2":"markdown","ccdb07e1":"markdown","0e29607a":"markdown","6bfdfcc2":"markdown","42252b9d":"markdown","60756146":"markdown"},"source":{"8ebbef2e":"import pandas as pd\nimport numpy as np\nimport plotly.express as px\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport warnings\nwarnings.filterwarnings(\"ignore\")","e55b6a50":"df=pd.read_csv('..\/input\/red-wine-quality-cortez-et-al-2009\/winequality-red.csv')","a2e92bb3":"data_og=df.copy()","23190634":"df.head()","f1c3c3ce":"df.isna().sum()","dffef176":"np.unique(df['quality'],return_counts=True)","8dbe1392":"df['quality']=df['quality'].apply(lambda x:0 if x<=6 else 1)","a11cfdb3":"count=np.unique(df['quality'],return_counts=True)\ncount","8b4b9560":"max(count[1][0]\/count[1].sum()*100,count[1][1]\/count[1].sum()*100)","ffda2686":"def graph(data,x,hist=False):\n    if hist:\n        px.histogram(data, x=x,width=600,height=600,color_discrete_sequence=px.colors.qualitative.Dark2).show()\n    px.box(data, x=x,width=600,height=600,color_discrete_sequence=px.colors.qualitative.Dark2).show()","774eec5f":"graph(df,'fixed acidity',hist=True)","fb59b3f4":"df['fixed acidity']=df['fixed acidity'].apply(lambda x:12.5 if x>=12.5 else x)","6c95d4b6":"graph(df,'fixed acidity',hist=False)","fc86ed68":"graph(df,'volatile acidity',hist=True)","61238976":"df['volatile acidity']=df['volatile acidity'].apply(lambda x:1.02 if x>=1.02 else x)","055c45d6":"graph(df,'volatile acidity',hist=False)","902e3313":"graph(df,'citric acid',hist=True)","7eb574a6":"df['citric acid']=df['citric acid'].apply(lambda x:0.79 if x>=0.79 else x)","35cd546e":"graph(df,'citric acid',hist=False)","debbc556":"graph(df,'residual sugar',hist=True)","b81795d8":"df['residual sugar']=df['residual sugar'].apply(lambda x:3.7 if x>3.7 else x)","88b7ad02":"graph(df,'residual sugar',hist=False)","c712de1d":"graph(df,'chlorides',hist=True)","09a44d14":"df['chlorides']=df['chlorides'].apply(lambda x:0.041 if x<0.041 else x)\ndf['chlorides']=df['chlorides'].apply(lambda x:0.12 if x>0.12 else x)","35da23db":"graph(df,'chlorides',hist=False)","616285a3":"graph(df,'free sulfur dioxide',hist=True)","2bcc6fb5":"df['free sulfur dioxide']=df['free sulfur dioxide'].apply(lambda x:42 if x>42 else x)","7c5e2355":"graph(df,'free sulfur dioxide',hist=False)","a724612a":"graph(df,'total sulfur dioxide',hist=True)","62496238":"df['total sulfur dioxide']=df['total sulfur dioxide'].apply(lambda x:122 if x>122 else x)","9b0b21ca":"graph(df,'total sulfur dioxide',hist=False)","4cfcaa86":"graph(df,'density',hist=True)","d6dc33d9":"df['density']=df['density'].apply(lambda x:0.99235 if x<=0.99235 else x)\ndf['density']=df['density'].apply(lambda x:1.001 if x>=1.001 else x)","a3b0ae1c":"graph(df,'density',hist=False)","eb503948":"graph(df,'fixed acidity',hist=True)","656307ef":"df['fixed acidity']=df['fixed acidity'].apply(lambda x:12.3 if x>=12.3 else x)","ca776d30":"graph(df,'fixed acidity',hist=False)","61d4fa91":"graph(df,'pH',hist=True)","98258d83":"df['pH']=df['pH'].apply(lambda x:2.92 if x<=2.92 else x)\ndf['pH']=df['pH'].apply(lambda x:3.69 if x>=3.69 else x)","32335f12":"graph(df,'pH',hist=False)","de3a680c":"graph(df,'sulphates',hist=True)","258da4dd":"df['sulphates']=df['sulphates'].apply(lambda x:1.01 if x>=1.01 else x)","07e33e2b":"graph(df,'sulphates',hist=False)","dfca2e44":"graph(df,'alcohol',hist=True)","8f050068":"df['alcohol']=df['alcohol'].apply(lambda x:13.56 if x>=13.56 else x)","b7ca2ec2":"graph(df,'alcohol',hist=False)","65ae28ff":"plt.rcParams.update({'font.size': 18})\nsns.pairplot(df, hue=\"quality\", diag_kind=\"hist\",corner=True)","14f6f557":"plt.figure(figsize=(10,10))\nplt.rcParams.update({'font.size': 14})\nsns.heatmap(df.corr(),annot=True,fmt='0.2f',linewidth=1)","a00405fb":"from sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split, GridSearchCV","84ced3f4":"X=df.drop('quality',axis=1)\ny=df[\"quality\"]","927d5f52":"X.isna().sum()","20d8d609":"X_train, X_test, y_train, y_test = train_test_split(X,y , test_size = 0.2, random_state = 42,stratify=y)","d52cad9c":"sc=StandardScaler()","b06224ee":"X_train_sc=sc.fit_transform(X_train)\nX_test_sc=sc.transform(X_test)","653b5cf2":"from sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC\nfrom sklearn.tree import DecisionTreeClassifier\nfrom xgboost import XGBClassifier\n\nfrom sklearn.ensemble import RandomForestClassifier\nimport time","d85cbce6":"from sklearn.metrics import accuracy_score\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import f1_score\nfrom sklearn.metrics import precision_score\nfrom sklearn.metrics import recall_score\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.metrics import classification_report","1d9d55bf":"rfc = RandomForestClassifier(n_estimators=200)\nrfc.fit(X_train_sc, y_train)\npred_rfc = rfc.predict(X_test_sc)","5c17bd93":"print(classification_report(y_test, pred_rfc))","35e8226c":"# Machine learing models with hypertuning parameters to find best results\nmodel_params = [\n(LogisticRegression(),[{'C':[0.25,0.5,0.75,1],'random_state':[0]}]), \n(KNeighborsClassifier(),[{'n_neighbors':[4,5,7,8,10], 'metric': ['euclidean', 'manhattan', 'chebyshev', 'minkowski']}]), \n(SVC( probability = True),[{'C':[0.25,0.5,0.75,1],'kernel':['linear', 'rbf'],'random_state':[0]}]), \n(DecisionTreeClassifier(),[{'criterion':['gini','entropy'],'random_state':[0]}]), \n(RandomForestClassifier(),[{'n_estimators':[100,150,200],'criterion':['gini','entropy'],'random_state':[0]}]), \n(XGBClassifier(), [{'learning_rate': [0.01, 0.05, 0.1], 'eval_metric': ['error']}])\n]","726de10e":"result_record=pd.DataFrame(columns=['Algorithm','Accuracy (train)','Accuracy (test)','Recall','Precision','F1 score','Roc Auc score'])","aa15e342":"def run_model(x_train,x_test,y_train, y_test):\n    start_time=time.time()\n    for i,j in model_params:\n        t1=time.time()\n        grid = GridSearchCV(estimator=i,param_grid = j, scoring = 'accuracy',cv = 5)  # Grid search function\n        grid.fit(x_train, y_train)                           # Training model on train data\n        predict_y=grid.predict(x_test)                       # Predicting test data \n        predict_probability=grid.predict_proba(x_test)       # Taking probability scores to calculate AUC score\n        \n        best_param = grid.best_params_                       # Best paramter where highest accuracy was obtained \n        train_accuracy = grid.best_score_\n        \n        # Different metrics to check performance of model\n        test_accuracy=accuracy_score(y_test,predict_y)\n        test_f1_score=f1_score(y_test,predict_y, average='micro')\n        test_precision=precision_score(y_test,predict_y, average='micro')\n        test_recall=recall_score(y_test,predict_y, average='micro')\n        test_auc_roc=roc_auc_score(y_test,predict_probability[:,1])\n        \n        print(str(i)+':\\nBest Parameters : ',best_param)\n        print('Train Accuracy : '+str(round(train_accuracy,2)*100)+' % ')\n        print('Test accuracy : '+str(round(test_accuracy,2)*100)+' % ')\n        print('F1 score : '+str(round(test_f1_score,2)*100)+' % ')\n        print('Precision : '+str(round(test_precision,2)*100)+' % ')\n        print('Recall : '+str(round(test_recall,2)*100)+' % ')\n        print('Auc Roc score : '+str(round(test_auc_roc,2)*100)+' % ')\n        \n        result_record.loc[len(result_record.index)]= [i,round(train_accuracy,2)*100,round(test_accuracy,2)*100,round(test_recall,2)*100,round(test_precision,2)*100,round(test_f1_score,2)*100,round(test_auc_roc,2)*100] \n        \n        t2=time.time()-t1\n        print(str(i)+' finished in '+str(t2)+' seconds')\n        print('='*50)\n    final_time=time.time()-start_time\n    print('All operations are finished in '+str(final_time)+' seconds')","625471d9":"run_model(X_train_sc, X_test_sc, y_train, y_test)  # One function to run all models","27c06107":"from imblearn.over_sampling import SMOTE","ccb7abad":"smote=SMOTE()","3dba63be":"df.columns","75bfa1cc":"df.quality.value_counts()","7cc92ce4":"x_smote,y_smote=smote.fit_resample(df.drop('quality',axis=1),df['quality'])","e2ef526c":"y_smote.value_counts()","f145b846":"X_train, X_test, y_train, y_test = train_test_split(x_smote, y_smote, test_size=0.25)","7aa9c0a7":"X_train_sm=sc.fit_transform(X_train)\nX_test_sm=sc.transform(X_test)","f27df0e1":"run_model(X_train_sm, X_test_sm, y_train, y_test)","08b0d082":"from imblearn.under_sampling import ClusterCentroids","fddbe9eb":"y.value_counts()","7dfa3475":"cc=ClusterCentroids()\nx_cc,y_cc=cc.fit_resample(X,y)","de61be6b":"y_cc.value_counts()","c2b903a8":"X_train_cc, X_test_cc, y_train, y_test = train_test_split(x_cc, y_cc, test_size=0.20)","e300e527":"X_train_sm=sc.fit_transform(X_train_cc)\nX_test_sm=sc.transform(X_test_cc)","4f170e5b":"run_model(X_train_sm, X_test_sm, y_train, y_test)","f2b44a1d":"plt.figure(figsize = (25,15), dpi = 60)\nplt.subplot(3,1,1)\nlabels=['LogisticRegression','KNeighborsClassifier','SVC','DecisionTreeClassifier','RandomForestClassifier','XGBClassifier']\nax=sns.heatmap(result_record.iloc[:6,1:],annot=True,yticklabels=labels,fmt='.1f')\nax.xaxis.tick_top()\nplt.title('Unbalanced Data')\n\nplt.subplot(3,1,2)\nlabels=['LogisticRegression','KNeighborsClassifier','SVC','DecisionTreeClassifier','RandomForestClassifier','XGBClassifier']\nax=sns.heatmap(result_record.iloc[6:12,1:],annot=True,yticklabels=labels,xticklabels=False,fmt='.1f')\n# ax.xaxis.tick_top()\nplt.title('SMOTE (Upsampling)')\n\nplt.subplot(3,1,3)\nlabels=['LogisticRegression','KNeighborsClassifier','SVC','DecisionTreeClassifier','RandomForestClassifier','XGBClassifier']\nax=sns.heatmap(result_record.iloc[12:,1:],annot=True,yticklabels=labels,xticklabels=False,fmt='.1f')\nax.xaxis.tick_top()\nplt.title('Clustercentroids (Downsampling)')\nplt.show()","23e97422":"### Since there are no missing values, we dont have to handle them","99b062c1":"---\nalcohol","7cc91680":"---\nchlorides","94dd388d":"#### We are recording every optimized result, so we can compare at last, which model is performing better on data","85f13046":"### After preprocessing we will split data and transform data into StandardScaler to make data scale neutral","d2dfa2ae":"We have very less outliers, left part of box plot needs some attention so we will manipulate some values of left side, outliers to the right, we will consider them","84f7574a":"---\ncitric acid","0bfdfdea":"## Correlation matrix","5f7704e5":"---","8a28ebcc":"---\nresidual sugar","5b42fc29":"---\n## Univariate and Bivariate analysis","258e1ba1":"---","dfe7fa17":"Label count after modification","2994b733":"### It is easy to apply machine learning models but manually hypertuning them to obtain best result is a lot of time consuming process<br>\n### Luckily we have Grid Search technique to do many things dynamically, which will save our time and efforts","68ead5a3":"### Observations made from scatter plot are true\n* 'free sulfur dioxide' and 'total sulfur dioxide' have good corelation, but not strong corelation, so we cannot drop any feature among them\n* 'density' and 'fixed acidity'\n\n### New Discoveries\n* 'pH' and 'fixed acidity' also have good corelation but not strong corelation","f3f68e40":"### This is traditional way of applying machine learning","d5ebe166":"free sulfur dioxide","7262960b":"Any model will give alteast 86% AUC score <br>\nData is highly imbalance which is a concern, as we know balancing techniques, so we can handle them afterwards <br>\nMaking functions for histogram and box plot, it will save time<br>","7fc01685":"### Smote","74e7ed8e":"# Observations:\n#### There no missing values, so no need for imputation\n#### Making every feature data distribution to normal distribution with skewness value near to 0\n#### Handling outlier\n#### Univariate and Bivariate analysis\n#### Dynmaic functions to create model and optimize them in one step, which results in lot of time saving\n#### As Data was highly imbalanced, It was oversapled and undersampled to check models performance\n#### Undersampled models may overfit sometimes\n#### Results of over sampled data and original ie unbalanced data are good, RandomforestClassifier and XGBClassifier performed good on both dataset\n## Feel free to download notebook and do experiments on it\n## Comments if you find something inapprpraite and will improve accordingly\n## Upvote if you find this notebook usefull\n","91bcc967":"---\npH","f39cd24c":"### Undersampling","d8336c36":"#### We already know wine with quality score 7 or above that is good quality wine so, we will modify data accordingly","d00d0f18":"Above plot show univariate analysis ie Histogram and bivariate analysis ie scatter plot<br>\nAfter observing univariate analysis(Only diagonal plots) clearly there is no linear relationship to seperate data<br>\nIn bivariate analysis we have made some conclusions described below<br>\n'free sulfur dioxide' and 'total sulfur dioxide' is having corelation if you observe scatterplot<br>\n'density' and 'fixed acidity' is also corelated but they are not strongly correlated","333dde45":"---\ndensity","add05dd2":"#### It is ok to have some outliers which are nearby in box plot, If outliers are extreme or far away from upper whisker or lower whisker, they needed to be handle immediately<br>\n---\nvolatile acidity","ccdb07e1":"---\ntotal sulfur dioxide","0e29607a":"---\nsulphates","6bfdfcc2":"### Labels count","42252b9d":"# Missing values\n","60756146":"#### Histogram and box plot of 'fixed acidity' feature"}}