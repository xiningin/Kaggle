{"cell_type":{"d60e041b":"code","f75f6ad0":"code","bea375da":"code","39655623":"code","f77ecd89":"code","dc746869":"code","a86cd7ef":"code","b8f6028b":"code","57986844":"code","dd7fcdb7":"code","3896ff1b":"code","0441bf26":"code","a2a944a7":"code","1b9d080d":"code","7d265a11":"code","fc932674":"code","0a3366a6":"code","165d5635":"code","05c82fe2":"code","12c6eb54":"code","476fb22e":"code","e7be44dc":"code","e0f60f56":"code","402127a1":"code","c2d2cc4f":"code","1e70603d":"code","b9406eca":"code","4bf432e5":"code","57d57363":"code","9da66984":"code","923a3146":"code","edf8b874":"code","a325b4b5":"code","b4b88594":"markdown","51fc6a56":"markdown","3e335b42":"markdown","ffd576ac":"markdown","1932b602":"markdown","17b07780":"markdown","f96654b8":"markdown","462a4e82":"markdown","08433413":"markdown","91e43452":"markdown","98adc20a":"markdown","d6e11fbb":"markdown","434413e0":"markdown","9d61dd03":"markdown","c6d63e5f":"markdown","a5d2f7fe":"markdown","ed52f46a":"markdown"},"source":{"d60e041b":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","f75f6ad0":"import matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score, confusion_matrix, f1_score, classification_report, plot_roc_curve\nfrom sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import RobustScaler\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.svm import SVC\n\nimport plotly.express as px\nsns.set_theme(style='darkgrid')\n","bea375da":"df = pd.read_csv('\/kaggle\/input\/heart-attack-analysis-prediction-dataset\/heart.csv')\ndf.head()","39655623":"df.info()","f77ecd89":"df.isnull().sum()","dc746869":"df.describe()\n","a86cd7ef":"# check for how many womens are prone to heart-attack\nwomen_stroke = df.loc[df.sex == 0]['output']\nwomen_stroke_percentage = sum(women_stroke)\/len(women_stroke)\nprint('The % of womens prone to heart-attack: {}%'.format(women_stroke_percentage*100))","b8f6028b":"women = df[df['sex']==0]\nmen = df[df['sex']==1]\n","57986844":"def cp_density(women, men):\n    plt.figure(figsize=(16,6))\n    sns.kdeplot(women['cp'], label = 'Women cp values', shade = True, color='red')\n    sns.kdeplot(men['cp'], label = 'men cp values', shade = True, color = 'blue')\n    plt.legend()\n    plt.title('Cp values comparision', weight='bold')\n    plt.xlabel('Cp values')\n    \ndef resting_blood_pressure_comparision(women, men):\n    plt.figure(figsize=(16,6))\n    sns.kdeplot(women['trtbps'], label = 'Women rbp values', shade = True, color='red')\n    sns.kdeplot(men['trtbps'], label = 'men rbp values', shade = True, color = 'blue')\n    plt.title('Resting blood pressure values comparision', weight='bold')\n    plt.legend()\n    plt.xlabel('trtbps values')\n    \ndef cholestarol(women, men):\n    plt.figure(figsize=(16,6))\n    sns.kdeplot(women['chol'], label = 'Women cholestoral values', shade = True, color='green')\n    sns.kdeplot(men['chol'], label = 'men cholestoral values', shade = True, color = 'black')\n    plt.title('Cholestarol values comparision', weight='bold')\n    plt.legend()\n    plt.xlabel('Cholestral values')\n    \ndef resting_electrocardiographic(women, men):\n    plt.figure(figsize=(16,6))\n    sns.kdeplot(women['restecg'], label = 'Women resting electrocardiographic values', shade = True, color='green')\n    sns.kdeplot(men['restecg'], label = 'men resting electrocardiographic values', shade = True, color = 'black')\n    plt.title('resting electrocardiographic values comparision', weight='bold')\n    plt.legend()\n    plt.xlabel('resting electrocardiographic values')\n\ndef maximum_heartrate_achieved(women, men):\n    plt.figure(figsize=(16,6))\n    sns.kdeplot(women['thalachh'], label = 'Women cp values', shade = True, color='black')\n    sns.kdeplot(men['thalachh'], label = 'men cp values', shade = True, color = 'yellow')\n    plt.title('Maximum heart-rate achieved values comparision', weight='bold')\n    plt.legend()\n    plt.xlabel('Maximum heart-rate achieved values')\n    ","dd7fcdb7":"cp_density(women, men)","3896ff1b":"resting_blood_pressure_comparision(women, men)","0441bf26":"cholestarol(women, men)","a2a944a7":"resting_electrocardiographic(women, men)","1b9d080d":"maximum_heartrate_achieved(women, men)","7d265a11":"plt.figure(figsize=(10,10))\nsns.pairplot(df, hue='output')","fc932674":"df.head()","0a3366a6":"def eda_for_several_attributes(data):\n    features = ['slp','thalachh','age','caa']\n    for var in features:\n        plt.figure(figsize=(16,6))\n        g = sns.scatterplot(data = data, x = 'oldpeak', y = data[var], hue = 'output')\n        plt.title(var, weight='bold')\n        plt.grid(linestyle = '--', axis = 'y')\n        plt.show()\n            \neda_for_several_attributes(df)","165d5635":"# check for the age and sex\ndef check_the_risk(data):\n    for var in ['sex','age']:\n        plt.figure(figsize=(16,8))\n        g = sns.countplot(data=data, x = data[var], hue = data.output, palette='OrRd')\n        plt.grid(linestyle = '--', axis='y')\n        plt.title(var)\n        \n        for i in ['top','left']:\n            g.spines[i].set_visible(False)\n            \ncheck_the_risk(df)","05c82fe2":"def find_correlational_map(data):\n    plt.figure(figsize=(16,12))\n    sns.heatmap(data.corr(), annot=True, cmap='OrRd')\n    plt.title('Correlational Map', weight='bold')\n    print('---'*50)\n    print(data.corr().output.sort_values(ascending = False))\n    plt.tight_layout()\n    \nfind_correlational_map(df)","12c6eb54":"df.head()","476fb22e":"X = df.iloc[:,:-1].values\ny = df.iloc[:,-1].values","e7be44dc":"X.shape, y.shape","e0f60f56":"# Let's split the date into train and test\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 0)\nX_train.shape, X_test.shape","402127a1":"y_train.shape, y_test.shape","c2d2cc4f":"# let's create a pipline \npipeline = make_pipeline(RobustScaler()) # creating pipeline for model building\n\nLR = make_pipeline(pipeline, LogisticRegression(random_state=0)) # LogisticRegression pipeline\nDT = make_pipeline(pipeline, DecisionTreeClassifier(random_state=0)) # DecisionTree Classifier pipeline\nRF = make_pipeline(pipeline, RandomForestClassifier(random_state=0)) # RandomForest Classifier pipeline\nAC = make_pipeline(pipeline, AdaBoostClassifier(random_state=0)) # Adaboost Classifier pipeline\nNB = make_pipeline(pipeline, GaussianNB()) # Naive bayes pipeline\nKN = make_pipeline(pipeline, KNeighborsClassifier()) # KNeighbor pipeline\nSV = make_pipeline(pipeline, SVC(random_state=0)) # Support vector pipeline","1e70603d":"# creating model_dict\nmodel_dictionary = {\n    'Logistic_Regression':LR,\n    'DecisionTree_Classifier':DT,\n    'RandomForest_classifier':RF,\n    'Adaboost_Classifier':AC,\n    'Naivebayes_Classifier':NB,\n    'KNeighbors_classifier':KN,\n    'Support_Vector':SV\n}","b9406eca":"print(model_dictionary)","4bf432e5":"# define a function to fit the model and return it's accuracy, classification report and confusion matrix\ndef model_fitting(model):\n    model.fit(X_train, y_train)\n    y_pred = model.predict(X_test)\n    print('The accuracy score of the model is: {}%'.format(accuracy_score(y_test, y_pred)* 100))\n    print('-----'*20)\n    print(confusion_matrix(y_test, y_pred))\n    print(classification_report(y_test, y_pred))","57d57363":"for name, model in model_dictionary.items():\n    print('---'*10)\n    print(name)\n    model_fitting(model)","9da66984":"model = AdaBoostClassifier(random_state=0)\nmodel.fit(X_train, y_train)","923a3146":"y_pred = model.predict(X_test)\naccuracy_score(y_test, y_pred)","edf8b874":"def find_confusion_matrix(y_test, y_pred):\n    cm = confusion_matrix(y_test, y_pred)\n    sns.heatmap(cm, annot=True, cmap='OrRd')\n    plt.title('Confusion Matrix', weight='bold')\n    print(classification_report(y_test, y_pred))\n    plot_roc_curve(model, X_test, y_test)\n    \n    \nfind_confusion_matrix(y_test, y_pred)","a325b4b5":"print('The accuracy of the model is: {}%'.format(round(accuracy_score(y_test, y_pred)*100, 2)))","b4b88594":"As we can see:\n* Ada boost has got 90% accuracy with only 6 misclassified classes.\n* It' has a precision of 0.86 for classes 0 and 0.94 for classes 1, which is better than all other algorithms.","51fc6a56":"There are lot of women's who have achived maximum heart-rate than men.","3e335b42":"* The less maximum heart-rate achieved the less chances of heart-attack.\n* The slp value of 1 shows feable chances of heart-attack.","ffd576ac":"* Men's Resting blood pressure values has the probable density of 120.\n* Women's Resting blood pressure values has the probable density of 135.","1932b602":"* As we can see here that, cholestoral values of range 200 - 270 falls majorly under womens category. \n* It's interesting that only the men's are showing values above 350.","17b07780":"Let's use Adaboost Model","f96654b8":"Check for the following relationship;\n* cp\n* trtbps\n* chol\n* restecg\n* thalachh\n* oldpeak","462a4e82":"# **EDA (Exploratory Data Analysis)**","08433413":"# **Model Building**","91e43452":"# **Data Splitting**","98adc20a":"Surprisingly there are 75% of the women are prone to heart-attack","d6e11fbb":"There is a spike in for heart-attack at the age of 41, 44, 51, 52,54.","434413e0":"# **Selecting The Best Model**","9d61dd03":"# **THANK YOOOOUU!!!!!!!!!**","c6d63e5f":"# **Correlational Map**","a5d2f7fe":"* Predominently women shows probable or definite left ventricular hypertrophy by Estes.","ed52f46a":"* This shows that most of the men's are free from heart-attacks.\n* Both the genders show's spike for non-anginal pain. Women has the great tendency for non-anginal chest pain."}}