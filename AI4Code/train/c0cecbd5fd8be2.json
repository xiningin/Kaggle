{"cell_type":{"bfa49179":"code","a54bb339":"code","28320bf9":"code","87551675":"code","481b9130":"code","f51cb2a3":"code","9f0eee1a":"code","be6b4659":"code","c5796a2f":"code","9584ee5e":"code","b6d53310":"code","c7b565f6":"code","13eee526":"code","60a1b70b":"code","934a32f6":"code","08f87d34":"code","4027cb90":"code","19d74eac":"code","f282396e":"code","c550df1a":"code","c23be7b6":"code","b7463714":"code","1cab7538":"code","ca152b0b":"code","a7c2d2a8":"code","44d2418b":"code","9f268066":"code","75069f69":"code","5e934707":"code","0a82eb82":"code","d879a574":"code","ecc67d80":"code","e5a1520b":"code","e88cdc51":"code","e5a019d9":"code","5872e475":"code","551579ba":"code","6d4c6a05":"code","8cdc903b":"code","ff723734":"code","1655d291":"code","ad1ae7d4":"code","7f8d516d":"code","8d3c5fd7":"code","a207b783":"code","d207a5dd":"code","24fec5d6":"code","5016be15":"code","fa16ad1a":"code","43673942":"code","e469cda7":"code","af9ebac7":"code","0462a098":"code","ab7d5a0c":"code","74e99647":"markdown","dc944479":"markdown","44208f24":"markdown","a04db383":"markdown","9600e8bc":"markdown","e65a6e6c":"markdown","d78c5328":"markdown","3c91802a":"markdown","cde88661":"markdown","ab0f2f31":"markdown","654c5e43":"markdown","4d8df823":"markdown","35564674":"markdown","8067654b":"markdown","09aa3382":"markdown","393a5de6":"markdown","1dd39742":"markdown","d6f3549a":"markdown","af87b4e3":"markdown","0d73f0fd":"markdown"},"source":{"bfa49179":"import pandas as pd\nimport numpy as np \nimport matplotlib.pyplot as plt\nimport matplotlib.dates as mdates\nimport seaborn as sns\n\nfrom sklearn.linear_model import LinearRegression,HuberRegressor,Ridge,TweedieRegressor\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.preprocessing import StandardScaler, RobustScaler, MinMaxScaler\n\nimport dateutil.easter as easter\n\nimport datetime\nimport optuna\nimport math","a54bb339":"#Holidays\nHOLIDAYS = False     \nNEXT_HOLIDAY = False  \n\nSEASONS = True \nWEATHER = True \n\nLAG_FEATURES = False\n\nPOST_PROCESSING = False\nMODEL_TYPE = \"Ridge Regression\"\n\nVAL_SPLIT = \"2017-12-31\" #\"2018-05-31\"","28320bf9":"EPOCHS = 10000    \nEARLY_STOPPING = 30\nDEVICE = \"cpu\"\n\nSCALER_NAME = \"MinMaxScaler\"  #None MinMax Standard\nSCALER = MinMaxScaler()  #MinMaxScaler StandardScaler","87551675":"train_df = pd.read_csv(\"..\/input\/tabular-playground-series-jan-2022\/train.csv\",index_col = 0)\ntest_df = pd.read_csv(\"..\/input\/tabular-playground-series-jan-2022\/test.csv\",index_col = 0)\nsub = pd.read_csv(\"..\/input\/tabular-playground-series-jan-2022\/sample_submission.csv\",index_col = 0)\ngdp_df = pd.read_csv('..\/input\/gdp-20152019-finland-norway-and-sweden\/GDP_data_2015_to_2019_Finland_Norway_Sweden.csv')\ngdp_df.set_index('year', inplace=True)\n\nif HOLIDAYS:\n    holidays = pd.read_csv(\"..\/input\/holidays-finland-norway-sweden-20152019\/Holidays_Finland_Norway_Sweden_2015-2019.csv\",usecols = [\"Date\",\"Country\",\"Name\"]                      )\n    holidays.rename(columns = {\"Date\":\"date\",\"Country\":\"country\",\"Name\":\"holiday\"},inplace= True)\n    holidays[\"holiday\"]= 1\n    holidays[\"holiday\"]= holidays[\"holiday\"].astype(\"int32\")\n    holidays[\"date\"] = pd.to_datetime(holidays[\"date\"])","481b9130":"df_weather = pd.read_csv('..\/input\/finland-norway-and-sweden-weather-data-20152019\/nordics_weather.csv', parse_dates=['date'])","f51cb2a3":"#Make date\ntrain_df[\"date\"] = pd.to_datetime(train_df[\"date\"])\ntest_df[\"date\"] = pd.to_datetime(test_df[\"date\"])","9f0eee1a":"train_df.head()","be6b4659":"# Feature engineering\ndef engineer(df):\n    \"\"\"Return a new dataframe with the engineered features\"\"\"\n    \n    def get_gdp(row):\n        country = 'GDP_' + row.country\n        return gdp_df.loc[row.date.year, country]\n      \n    \n    new_df = pd.DataFrame({'gdp': np.log(df.apply(get_gdp, axis=1)),\n                           'wd4': df.date.dt.weekday == 4, # Friday\n                           'wd56': df.date.dt.weekday >= 5, # Saturday and Sunday\n                          })\n\n    #new_df['daysinmonth'] = df['date'].dt.days_in_month         \n    \n    # One-hot encoding (no need to encode the last categories)\n    for country in ['Finland', 'Norway']:\n        new_df[country] = df.country == country\n    new_df['KaggleRama'] = df.store == 'KaggleRama'\n    for product in ['Kaggle Mug', 'Kaggle Hat']:\n        new_df[product] = df['product'] == product\n        \n    # Seasonal variations (Fourier series)\n    # The three products have different seasonal patterns\n    \n    dayofyear = df.date.dt.dayofyear\n    for k in range(1, 3):\n        new_df[f'sin{k}'] = np.sin(dayofyear \/ 365 * 2 * math.pi * k)\n        new_df[f'cos{k}'] = np.cos(dayofyear \/ 365 * 2 * math.pi * k)\n        new_df[f'mug_sin{k}'] = new_df[f'sin{k}'] * new_df['Kaggle Mug']\n        new_df[f'mug_cos{k}'] = new_df[f'cos{k}'] * new_df['Kaggle Mug']\n        new_df[f'hat_sin{k}'] = new_df[f'sin{k}'] * new_df['Kaggle Hat']\n        new_df[f'hat_cos{k}'] = new_df[f'cos{k}'] * new_df['Kaggle Hat']\n\n    return new_df\n#train = engineer(train_df)","c5796a2f":"# Feature engineering for holidays\ndef engineer_more(df):\n    \"\"\"Return a new dataframe with more engineered features\"\"\"\n    new_df = engineer(df)\n\n    # End of year\n    new_df = pd.concat([new_df,\n                        pd.DataFrame({f\"dec{d}\":\n                                      (df.date.dt.month == 12) & (df.date.dt.day == d)\n                                      for d in range(24, 32)}),\n                        pd.DataFrame({f\"n-dec{d}\":\n                                      (df.date.dt.month == 12) & (df.date.dt.day == d) & (df.country == 'Norway')\n                                      for d in range(24, 32)}),\n                        pd.DataFrame({f\"f-jan{d}\":\n                                      (df.date.dt.month == 1) & (df.date.dt.day == d) & (df.country == 'Finland')\n                                      for d in range(1, 14)}),\n                        pd.DataFrame({f\"jan{d}\":\n                                      (df.date.dt.month == 1) & (df.date.dt.day == d) & (df.country == 'Norway')\n                                      for d in range(1, 10)}),\n                        pd.DataFrame({f\"s-jan{d}\":\n                                      (df.date.dt.month == 1) & (df.date.dt.day == d) & (df.country == 'Sweden')\n                                      for d in range(1, 15)})],\n                       axis=1)\n    \n    # May\n    new_df = pd.concat([new_df,\n                        pd.DataFrame({f\"may{d}\":\n                                      (df.date.dt.month == 5) & (df.date.dt.day == d) \n                                      for d in list(range(1, 10))}), #  + list(range(17, 25))\n                        pd.DataFrame({f\"may{d}\":\n                                      (df.date.dt.month == 5) & (df.date.dt.day == d) & (df.country == 'Norway')\n                                      for d in list(range(19, 26))})],\n                       axis=1)\n    \n    # June and July\n    new_df = pd.concat([new_df,\n                        pd.DataFrame({f\"june{d}\":\n                                      (df.date.dt.month == 6) & (df.date.dt.day == d) & (df.country == 'Sweden')\n                                      for d in list(range(8, 14))}),\n                        #pd.DataFrame({f\"june{d}\":\n                        #              (df.date.dt.month == 6) & (df.date.dt.day == d) & (df.country == 'Norway')\n                        #              for d in list(range(22, 31))}),\n                        #pd.DataFrame({f\"july{d}\":\n                        #              (df.date.dt.month == 7) & (df.date.dt.day == d) & (df.country == 'Norway')\n                        #              for d in list(range(1, 3))})],\n                       ],\n                       axis=1)\n    \n    # Last Wednesday of June\n    wed_june_date = df.date.dt.year.map({2015: pd.Timestamp(('2015-06-24')),\n                                         2016: pd.Timestamp(('2016-06-29')),\n                                         2017: pd.Timestamp(('2017-06-28')),\n                                         2018: pd.Timestamp(('2018-06-27')),\n                                         2019: pd.Timestamp(('2019-06-26'))})\n    new_df = pd.concat([new_df,\n                        pd.DataFrame({f\"wed_june{d}\": \n                                      (df.date - wed_june_date == np.timedelta64(d, \"D\")) & (df.country != 'Norway')\n                                      for d in list(range(-4, 6))})],\n                       axis=1)\n    \n    # First Sunday of November\n    sun_nov_date = df.date.dt.year.map({2015: pd.Timestamp(('2015-11-1')),\n                                         2016: pd.Timestamp(('2016-11-6')),\n                                         2017: pd.Timestamp(('2017-11-5')),\n                                         2018: pd.Timestamp(('2018-11-4')),\n                                         2019: pd.Timestamp(('2019-11-3'))})\n    new_df = pd.concat([new_df,\n                        pd.DataFrame({f\"sun_nov{d}\": \n                                      (df.date - sun_nov_date == np.timedelta64(d, \"D\")) & (df.country != 'Norway')\n                                      for d in list(range(0, 9))})],\n                       axis=1)\n    \n    # First half of December (Independence Day of Finland, 6th of December)\n    new_df = pd.concat([new_df,\n                        pd.DataFrame({f\"dec{d}\":\n                                      (df.date.dt.month == 12) & (df.date.dt.day == d) & (df.country == 'Finland')\n                                      for d in list(range(6, 14))})],\n                       axis=1)\n\n    # Easter\n    easter_date = df.date.apply(lambda date: pd.Timestamp(easter.easter(date.year)))\n    new_df = pd.concat([new_df,\n                        pd.DataFrame({f\"easter{d}\": \n                                      (df.date - easter_date == np.timedelta64(d, \"D\"))\n                                      for d in list(range(-2, 11)) + list(range(40, 48)) + list(range(50, 59))})],\n                       axis=1)\n    \n    #new_df = pd.get_dummies(new_df)\n\n    return new_df.astype(np.float64)\n\ntrain = engineer_more(train_df)\n\ntrain['num_sold'] = train_df.num_sold.astype(np.float32)\ntest = engineer_more(test_df)\n\n#features = list(test.columns)\n#print(features)\n\ntest['date'] = test_df.date\ntrain['date'] = train_df.date","9584ee5e":"train[[\"store\",\"product\",\"country\"]]= train_df[[\"store\",\"product\",\"country\"]]\ntest[[\"store\",\"product\",\"country\"]]= test_df[[\"store\",\"product\",\"country\"]]","b6d53310":"if SEASONS:\n    \n    print(\"Adding Seasons \")\n    seasons = [1, 1, 2, 2, 2, 3, 3, 3, 4, 4, 4, 1]\n\n    month_to_season = dict(zip(range(1,13), seasons))\n\n    train[\"season\"] = train[\"date\"].dt.month.map(month_to_season)\n    test[\"season\"] = test[\"date\"].dt.month.map(month_to_season)","c7b565f6":"if WEATHER:\n    w_feats = ['country', 'date', 'tavg','precipitation']\n    #w_feats = ['country', 'date', 'precipitation', 'snow_depth', 'tavg', 'tmax','tmin']\n    print(\"Adding weather\")\n    train = train.merge(df_weather[w_feats], on=['date', 'country'], how='left')\n    train.index = train_df.index \n    test = test.merge(df_weather[w_feats], on=['date', 'country'], how='left')\n    test.index = test_df.index ","13eee526":"train.head()","60a1b70b":"def public_hols(df):\n    df = pd.merge(df, holidays, how='left', on=['date', 'country'])\n    df.fillna(value = 0,inplace=True)\n    return df","934a32f6":"if HOLIDAYS:\n    train = public_hols(train)\n    test = public_hols(test)\n    test.index = test_df.index ","08f87d34":"def next_holiday(x):\n    i=1\n    while sum(holidays[\"date\"] == pd.Timestamp(x) + pd.DateOffset(days=i)) ==0:\n        i+=1\n        if i >200:\n            i=0\n            break\n            break\n    return i\n\nif NEXT_HOLIDAY:\n    holidays[\"date\"] = pd.to_datetime(holidays[\"date\"])\n    train[\"to_holiday\"] = train[\"date\"].apply(lambda x : next_holiday(x))\n    test[\"to_holiday\"] = test[\"date\"].apply(lambda x : next_holiday(x))","4027cb90":"def SMAPE(y_true, y_pred):\n    denominator = (y_true + np.abs(y_pred)) \/ 200.0\n    diff = np.abs(y_true - y_pred) \/ denominator\n    diff[denominator == 0] = 0.0\n    return np.mean(diff)","19d74eac":"def create_lag(DAYS,df):\n    df[f\"shift{DAYS}\"] = df.groupby([\"store\",\"product\",\"country\"])[\"num_sold\"].shift(DAYS,fill_value = 0)\n    return df","f282396e":"def rolling_mean_std(day_roll_list, df):\n    shift_days = day_roll_list[0]\n    roll_window = day_roll_list[1]\n    col_name = 'rolling_'+str(shift_days)+'_'+str(roll_window)\n    df[col_name+\"_mean\"] = df.groupby([\"store\",\"product\",\"country\"])[\"num_sold\"].shift(shift_days,fill_value=0).rolling(roll_window).mean()\n    df[col_name+\"_std\"] = df.groupby([\"store\",\"product\",\"country\"])[\"num_sold\"].shift(shift_days,fill_value=0).rolling(roll_window).std()\n    \n    return df.fillna(0,inplace = True)","c550df1a":"def day_roll(df,day_shift,roll_window):\n    #Shift values and rolling mean\n    for day in days_shift:\n        pass\n        #create_lag(day,df)\n    for day, window in ROLLS_SPLIT:\n        rolling_mean_std([day,window],df)\n\n    return df","c23be7b6":"days_shift = [1,7,14, 30]\nroll_window = [7,14,30]","b7463714":"if LAG_FEATURES:\n    print(\"Running Lag features\")\n    ROLLS_SPLIT = []\n    # shift days\n    for i in days_shift:\n        #rolling window\n        for j in roll_window:\n            ROLLS_SPLIT.append([i,j])\n\n        train = day_roll(train,days_shift,roll_window)","1cab7538":"features_base= list(test.columns)\nfeatures= list(train.columns)\n\n\nfor feat in [features_base, features]:\n    feat.remove(\"store\")\n    feat.remove(\"product\")\n    feat.remove(\"country\")\n    feat.remove(\"date\")\n\nfeatures.remove(\"num_sold\")\n\nprint(features_base)\nprint()\nprint(features)","ca152b0b":"X = train[features_base]\ny= train[\"num_sold\"]\n\nX_train = train[train[\"date\"]<=VAL_SPLIT][features]\nX_test = train[train[\"date\"]>VAL_SPLIT][features]\n\ny_train = train[train[\"date\"]<=VAL_SPLIT][\"num_sold\"]\ny_test = train[train[\"date\"]>VAL_SPLIT][\"num_sold\"]","a7c2d2a8":"def scale_data(X_train, X_test= None, test=None,):\n     \n    scaler= SCALER\n    \n    #this can be X or X_train \n    X_train_s = scaler.fit_transform(X_train)\n\n    if X_test is None: #full train \n        test_s = scaler.transform(test)\n        return X_train_s, test_s\n    \n    else: # validation \n        X_test_s = scaler.transform(X_test)\n    \n    return   X_train_s , X_test_s","44d2418b":"def fit_model(X,y,test = None, X_test = None,y_test= None):\n    \n    model = Ridge(max_iter=EPOCHS)\n\n    \n    if X_test is not None: #validation prediction \n        X_train_s , X_test_s = scale_data(X, X_test)\n        model.fit(X_train_s,np.log1p(y))\n        preds = np.expm1(model.predict(X_test_s))\n        \n        smape = SMAPE(y_test,preds)\n        \n        return preds, model, smape\n        \n    else:\n        X_s, test_s = scale_data(X, test)\n        \n        model.fit(X_s,np.log1p(y))\n        preds = np.expm1(model.predict(test_s))\n        \n        return preds, model","9f268066":"val_predictions , model ,smape = fit_model(X= X_train,y = y_train,test= None, X_test = X_test,y_test = y_test)","75069f69":"print(\"SMAPE :\",smape )\nprint(f\"\\n EPOCHS: {EPOCHS}\")\nprint(f\"\\n SCALER: {SCALER_NAME}\")\nprint(f\"\\n POST_PROCESSING: {POST_PROCESSING}\")","5e934707":"residuals = y_test - val_predictions\nplt.figure(figsize = (20,7))\nplt.scatter(y_test,val_predictions)\nplt.title(\"\")\nplt.show()","0a82eb82":"# fit on full dataset\nonesplit_preds , model = fit_model(X,y,test[features])","d879a574":"sub_base  = sub.copy(deep = True)\nsub_base_full  = sub.copy(deep = True)\nsub_base[\"num_sold\"] =  val_predictions\nsub_base_full[\"num_sold\"] =  onesplit_preds","ecc67d80":"sub_base_full","e5a1520b":"FREQUENCY = 1 #prediction period\n\nstart_date = min(test[\"date\"]) \nend_date = max(test[\"date\"])","e88cdc51":"def multi_step_recursive(start_date, end_date, freq, sub, train_i, test_i):\n    delta = pd.DateOffset(days = freq)\n\n    all_df = pd.concat([train_i.assign(ds=\"a\"),test_i.assign(ds=\"b\")],axis =0)\n    \n    #Shift values and rolling \n    if LAG_FEATURES:\n        all_df = day_roll(all_df,days_shift,roll_window)\n\n    while start_date <= end_date:\n\n        #Select slice to predict\n        test_split = all_df [  (all_df[\"date\"]>= start_date ) & (all_df[\"date\"]< start_date+delta) ][features]\n\n        X = all_df[ all_df[\"date\"]< start_date][features]\n        y = all_df[ all_df[\"date\"]< start_date][\"num_sold\"]\n        \n        #predict 1 timeframe - full data\n        one_period_preds , model = fit_model(X,y,test_split[features])\n\n        #Add prediction test data to X and preds to y\n        test_split[\"num_sold\"] = one_period_preds\n        all_df.loc[test_split.index, \"num_sold\"]  = test_split[\"num_sold\"]\n\n        sub.loc[test_split.index , \"num_sold\"] = one_period_preds\n        \n        #Shift values and rolling \n        if LAG_FEATURES:\n            all_df = day_roll(all_df,days_shift,roll_window)\n\n        #update start date\n        start_date += delta\n    \n    #val prediction\n    X_train = train_i[train_i[\"date\"]<=VAL_SPLIT][features]\n    X_test = train_i[train_i[\"date\"]>VAL_SPLIT][features]\n    y_train = train_i[train_i[\"date\"]<=VAL_SPLIT][\"num_sold\"]\n    y_test = train_i[train_i[\"date\"]>VAL_SPLIT][\"num_sold\"]\n\n    val_predictions , model ,smape = fit_model(X= X_train,y = y_train,test= None, X_test = X_test,y_test = y_test)\n    print(\"SMAPE:\",smape)\n    \n    return sub ,smape","e5a019d9":"sub_recursive , smape = multi_step_recursive(start_date, end_date, FREQUENCY, sub.copy(deep=True), train, test)","5872e475":"sub_recursive","551579ba":"import pandas as pd\npd.options.mode.chained_assignment = None  # default='warn'","6d4c6a05":"def split_models(split_on, sub_df, train ,test):    \n\n    split_smape=0\n    \n    # split training on product\/ store\/ country\n    for split in train[split_on].unique():\n        print(f\"\\nPredicting for {split_on} {split}\")\n\n        train_split= train[train[split_on] ==split]\n        test_split =test[test[split_on] ==split]\n        \n        #train on Full dataset\n        final_predictions , smape = multi_step_recursive(start_date, end_date, FREQUENCY, sub.copy(deep=True), train_split, test_split)\n        split_smape += smape\/train[split_on].nunique()\n        \n        sub_df.loc[test_split.index,\"num_sold\"] = final_predictions[\"num_sold\"]\n\n    print(f\"\\n Final mean smape:\",split_smape)\n    \n    return split_smape, sub_df, model","8cdc903b":"store_smape, sub_store, model = split_models(\"store\", sub.copy(deep=True), train ,test)\n#sub_store","ff723734":"product_smape, sub_product, model = split_models(\"product\", sub.copy(deep=True), train ,test)\n#sub_product","1655d291":"country_smape, sub_country,model = split_models(\"country\", sub.copy(deep=True), train ,test)\n#sub_country","ad1ae7d4":"import itertools\nall_splits = list(itertools.product(['KaggleMart', 'KaggleRama'],['Kaggle Mug', 'Kaggle Hat', 'Kaggle Sticker'],['Finland', 'Norway', 'Sweden']))","7f8d516d":"def split_models_ALL(split_on, sub_df):    \n\n    split_smape=0\n    split_dict = {}\n\n    # split training on product\/ store\/ country\n    for idx ,split in enumerate(split_on):\n        print(f\"\\nPredicting for store: {split[0]}, product: {split[1]}, country: {split[2]} \")\n\n        train_split= train[ (train[\"store\"] == split[0]) & (train[\"product\"] == split[1]) & (train[\"country\"] == split[2])]\n        test_split =test[ (test[\"store\"] == split[0]) & (test[\"product\"] == split[1]) & (test[\"country\"] == split[2])]\n\n        X_train = train_split[train_split[\"date\"]<=VAL_SPLIT][features]\n        X_test = train_split[train_split[\"date\"]>VAL_SPLIT][features]\n        y_train= train_split[train_split[\"date\"]<=VAL_SPLIT][\"num_sold\"]\n        y_test= train_split[train_split[\"date\"]>VAL_SPLIT][\"num_sold\"]\n\n        #run model for each split type\n        val_predictions , model ,smape = fit_model(X_train,y_train,test_split[features], X_test,y_test)\n\n        split_smape += smape\/len(all_splits)\n        split_dict[split] = smape\n\n        #train on Full dataset\n        #final_predictions , model = fit_model(train_split[features]  ,train_split[\"num_sold\"]  ,  test_split[features])\n        final_predictions , smape = multi_step_recursive(start_date, end_date, FREQUENCY, sub.copy(deep=True), train_split, test_split)\n        sub_df.loc[test_split.index,\"num_sold\"] = final_predictions[\"num_sold\"]\n                \n\n    print(f\"\\n final all_split smape:\",split_smape)\n    \n    return split_smape, sub_df , split_dict","8d3c5fd7":"smape_all, sub_all, split_dict = split_models_ALL(all_splits, sub.copy(deep=True))","a207b783":"split_dict","d207a5dd":"sub_all","24fec5d6":"if POST_PROCESSING: \n    dec = sub_recursive % 1\n    to_round = (dec<=0.2)|(dec>=0.8)\n    sub_recursive[to_round] = np.round(sub_recursive[to_round])","5016be15":"sub_base[\"num_sold\"]= sub_base[\"num_sold\"].round()\nsub_base_full[\"num_sold\"]= sub_base_full[\"num_sold\"].round()\nsub_recursive[\"num_sold\"]= sub_recursive[\"num_sold\"].round()\nsub_store[\"num_sold\"]= sub_store[\"num_sold\"].round()\nsub_product[\"num_sold\"]= sub_product[\"num_sold\"].round()\nsub_country[\"num_sold\"]= sub_country[\"num_sold\"].round()\nsub_all[\"num_sold\"]= sub_all[\"num_sold\"].round()\n\nsub_base.to_csv(\"submission_base.csv\")\nsub_base_full.to_csv(\"submission_base_full.csv\")\n\nsub_recursive.to_csv(\"submission_recursive.csv\")\n\nsub_store.to_csv(\"submission_store.csv\")\nsub_product.to_csv(\"submission_product.csv\")\nsub_country.to_csv(\"submission_country.csv\")\nsub_all.to_csv(\"submission_all.csv\")","fa16ad1a":"plt.figure(figsize=(25,10))\nsns.lineplot(data = train[train[\"date\"]>=VAL_SPLIT], x= \"date\" , y = \"num_sold\", label =\"actual\" ,ci=None)\nsns.lineplot(data = sub_base, x= test[\"date\"] , y = \"num_sold\", label =\"Base prediction\" ,ci=None)\nsns.lineplot(data = sub_base_full, x= test[\"date\"] , y = \"num_sold\", label =\"Base full train prediction\" ,ci=None)\nsns.lineplot(data = sub_recursive, x= test[\"date\"] , y = \"num_sold\", label =\"Mutistep recursive prediction\" ,ci=None)\n\nplt.title(\"Recursive vs Baseline\")\nplt.show()","43673942":"plt.figure(figsize=(25,10))\n#sns.lineplot(data = train[train[\"date\"]>=VAL_SPLIT], x= \"date\" , y = \"num_sold\", label =\"actual\" ,ci=None)\nsns.lineplot(data = sub_base, x= test[\"date\"] , y = \"num_sold\", label =\"Baseline prediction\" ,ci=None)\nsns.lineplot(data = sub_store,x = test[\"date\"] , y = \"num_sold\", label =\"Store recursive\" ,ci=None)\nsns.lineplot(data = sub_product, x= test[\"date\"] , y = \"num_sold\", label =\"Product recursive\" ,ci=None)\nsns.lineplot(data = sub_country, x= test[\"date\"] , y = \"num_sold\", label =\"Country recursive\" ,ci=None)\nsns.lineplot(data = sub_all, x= test[\"date\"] , y = \"num_sold\", label =\"ALL recursive\" ,ci=None)\n\nplt.axvline(pd.to_datetime(\"2019-04-21\"),label= \"easter\",  c = \"r\", linestyle=\"--\")\nplt.text(x =pd.to_datetime(\"2019-04-21\") ,y = 0,s ='EASTER',rotation=90)\nplt.title(\"Split recursive predctions \")\nplt.show()","e469cda7":"plt.figure(figsize=(25,10))\nsns.lineplot(data = sub_recursive, x= test[\"date\"] , y = \"num_sold\", label =\"Mutistep recursive\" ,ci=None)\nsns.lineplot(data = sub_country, x= test[\"date\"] , y = \"num_sold\", label =\"Country recursive\" ,ci=None)\nsns.lineplot(data = sub_store,x = test[\"date\"] , y = \"num_sold\", label =\"Store recursive\" ,ci=None)\nplt.title(\" Baseline Muti-step vs Store Multi-step\")\nplt.show()","af9ebac7":"fig,ax = plt.subplots(2,1, figsize=(25,15))\n\n\n#sns.lineplot(ax=ax[0],data = train[train[\"date\"]>=VAL_SPLIT], x= \"date\" , y = \"num_sold\", label =\"actual\" ,ci=None)\nsns.lineplot(ax=ax[0],data = sub_base_full, x= test[\"date\"] , y = \"num_sold\", label =\"Baseline full train prediction\" ,ci=None)\nsns.lineplot(ax=ax[0],data = sub_recursive, x= test[\"date\"] , y = \"num_sold\", label =\"Mutistep recursive prediction\" ,ci=None)\nax[0].set_title(f\"Baseline vs Recursive Baseline\")\n\nres_base_rec = sub_base_full[\"num_sold\"] - sub_recursive[\"num_sold\"]\nsns.lineplot(ax=ax[1], y = res_base_rec,  x= test[\"date\"] ,  label =\"Residuals\",ci=None )\nax[1].set_title(f\"Residuals baseline - recursive\")\n\nplt.show()","0462a098":"fig,ax = plt.subplots(2,1, figsize=(25,15))\n\nsns.lineplot(ax=ax[0],data = sub_recursive, x= test[\"date\"] , y = \"num_sold\", label =\"Mutistep recursive\" ,ci=None)\nsns.lineplot(ax=ax[0],data = sub_store,x = test[\"date\"] , y = \"num_sold\", label =\"Store recursive\" ,ci=None)\nax[0].set_title(\" Baseline recursive vs Store split recursive\")\n\nbase_store = sub_recursive[\"num_sold\"] -  sub_store[\"num_sold\"] \nsns.lineplot(ax=ax[1], y = base_store,  x= test[\"date\"] ,  label =\"Residuals\",ci=None )\nax[1].set_title(f\"Recursive baseline - Store split\")\n\nplt.show()","ab7d5a0c":"fig,ax = plt.subplots(2,1, figsize=(25,15))\n\nsns.lineplot(ax=ax[0],data = sub_recursive, x= test[\"date\"] , y = \"num_sold\", label =\"Mutistep recursive\" ,ci=None)\nsns.lineplot(ax=ax[0],data = sub_all,x = test[\"date\"] , y = \"num_sold\", label =\"ALL recursive\" ,ci=None)\nax[0].set_title(\" Baseline Muti-step vs ALL Multi-step\")\n\nbase_store = sub_recursive[\"num_sold\"] -  sub_all[\"num_sold\"] \nsns.lineplot(ax=ax[1], y = base_store,  x= test[\"date\"] ,  label =\"Residuals\",ci=None )\nax[1].set_title(f\"Recursive baseline - ALL Split\")\n\nplt.show()","74e99647":"obj is the objective function of the algorithm, i.e. what it's trying to maximize or minimize, e.g. \"regression\" means it's minimizing squared residuals.\n\nMetric and eval are essentially the same. They are used for Early stopping ","dc944479":"# All Split ","44208f24":"# Libraries","a04db383":"##  Residuals comparison ","9600e8bc":"## Set Features","e65a6e6c":"**Notes:**\n\n* best----- Kaggle Hat (product)  ,   Norway (Country) , Sweden (country)\n* good----- all store splits \n* bad------ Finland (Country)-worst   -- Kaggle Sticker(product) --KaggleMug(product) \n\n#### summary\n* Country split is good  but Finland is bad \\\n* Try focus on Finland and see where stickers fits in ","d78c5328":"# Shift Days","3c91802a":"# Functions ","cde88661":"Thanks to [ambrosm](https:\/\/www.kaggle.com\/anirudhg15) \\\nFor this amazing feature engineering\nhttps:\/\/www.kaggle.com\/ambrosm\/tpsjan22-03-linear-model\/notebook#More-feature-engineering-(advanced-model)","ab0f2f31":"# Lag Features ","654c5e43":"## Run on Full training data ","4d8df823":"**Note**: \n* Store is less than full recursive ","35564674":"# Split and Scale","8067654b":"# Load Data","09aa3382":"# Multi - Step Recursive \nWe will loop through a time period (days), predict the data and append to the training data for re-training \\\nThis will continue till the end of test ","393a5de6":"# Post Processing & Submission ","1dd39742":"# Run model","d6f3549a":"# Split on Store, Product , Country","af87b4e3":"Targeted rounding \nhttps:\/\/www.kaggle.com\/c\/petfinder-pawpularity-score\/discussion\/300992","0d73f0fd":"# Training Visualization"}}