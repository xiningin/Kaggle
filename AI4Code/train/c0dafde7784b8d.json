{"cell_type":{"3c00c3b5":"code","2d3dcef6":"code","b8eac51f":"code","548a4697":"code","3e4de7e9":"code","6ce2e2d5":"code","6182d896":"code","11e9dd47":"code","18afb08b":"code","fffd218f":"code","01d2e969":"code","5b6fe73c":"code","1b0544c7":"code","6b0b8e1e":"code","b8ed8356":"code","63781986":"code","e20d081c":"code","806cd9ef":"code","bbaf6283":"code","f32ddfcd":"code","d001feef":"code","c69aaba5":"code","71b917d4":"code","56305372":"code","2f689719":"code","9cc1d026":"code","290650ec":"code","ceecf6cc":"code","5770883b":"code","be8fbaec":"code","e7041272":"code","80b41364":"code","c5bbd66c":"code","edfd41c8":"code","82e8728a":"code","4bd606de":"code","667a4f89":"code","14c3d874":"code","e6ca146a":"code","610a10df":"code","9ed6e215":"code","e1510258":"code","49b8b563":"code","548fe84c":"code","7808a0bf":"code","5b3083c0":"code","434fa018":"code","0a04a5a9":"code","bf5b8040":"code","e6f15320":"code","cd0fe4fe":"code","24a4423d":"code","7f82d07d":"code","98a68b37":"code","29ef5a29":"code","f8094307":"code","dbb90194":"code","e4e4d866":"code","92924a4a":"code","f2ddef8f":"code","2a6e45b3":"code","3c02549e":"code","5b470313":"code","85d07b10":"code","a0eada87":"code","71cb878e":"code","35316914":"code","7bf73ee9":"code","0adb097e":"code","ad390167":"code","ca3dd553":"code","c4762495":"code","37196099":"code","f67088ac":"code","70a55bb6":"code","75f11279":"code","ec7d90ef":"code","f0a8007c":"code","6387498b":"code","68301a70":"code","30e05dc4":"code","ba0d9b78":"code","d3fa7532":"code","ed2a38b7":"code","064fb174":"code","e57962cb":"markdown","493f2ae1":"markdown","4703aef6":"markdown","73a4be05":"markdown","23876f6d":"markdown","27e7432d":"markdown","5849368d":"markdown","87233f91":"markdown","0ff10748":"markdown","09e7efa2":"markdown","027b9449":"markdown","6069060c":"markdown","804175cb":"markdown","e19d2163":"markdown","178704b5":"markdown","14f933ab":"markdown","57692e35":"markdown","7e11d676":"markdown","c876a543":"markdown","d99a79a1":"markdown","5b957c8b":"markdown","546c6e0a":"markdown","8bcf74fc":"markdown","8ca0ffe0":"markdown","b3466101":"markdown","1611b4d9":"markdown","5a00864f":"markdown","14829d74":"markdown","b073b0a0":"markdown","7d5dbb39":"markdown","548a3030":"markdown","de028dae":"markdown","01b8b985":"markdown","e14f105c":"markdown","b4cb3159":"markdown","eaf722e0":"markdown","f4f1a137":"markdown","b0661bbc":"markdown","f642e946":"markdown","1d3580f7":"markdown"},"source":{"3c00c3b5":"import collections\nimport unidecode\nimport re\nimport string\n\n!pip install pyspellchecker\nfrom spellchecker import SpellChecker\n\nimport numpy as np \nimport pandas as pd\n\nimport matplotlib\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport warnings\nimport gc, math\nfrom tqdm import tqdm\nimport pickle\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")","2d3dcef6":"from sklearn.base import BaseEstimator\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split, StratifiedKFold\nfrom sklearn.metrics import roc_auc_score, f1_score\nfrom sklearn.preprocessing import StandardScaler, MinMaxScaler, LabelEncoder\nfrom sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.decomposition import TruncatedSVD\nfrom sklearn.metrics import roc_auc_score, roc_curve","b8eac51f":"import nltk\nfrom nltk.tokenize import WordPunctTokenizer\nfrom nltk.corpus import stopwords\nfrom wordcloud import WordCloud","548a4697":"pd.set_option('display.max_rows', 500)\npd.set_option('display.max_columns', 500)\npd.set_option('display.width', 1000)\npd.set_option('use_inf_as_na', True)\n\nwarnings.simplefilter('ignore')\nmatplotlib.rcParams['figure.dpi'] = 100\nsns.set()\n%matplotlib inline","3e4de7e9":"%%time\nfolder = '..\/input\/nlp-getting-started\/'\ntrain_df = pd.read_csv(folder + 'train.csv')\ntest_df = pd.read_csv(folder + 'test.csv')\nsub_df = pd.read_csv(folder + 'sample_submission.csv')","6ce2e2d5":"print('train_df')\nprint('train: ', train_df.shape)\nprint('test_df')\nprint('test: ', test_df.shape)\nprint('sub_df')\nprint('sub_df: ', sub_df.shape)","6182d896":"train_df.head()","11e9dd47":"test_df.head()","18afb08b":"train_df.drop(['id'], axis=1, inplace=True)\ntest_df.drop(['id'], axis=1, inplace=True)","fffd218f":"train_df.describe(include=['O'])","01d2e969":"test_df.describe(include=['O'])","5b6fe73c":"sub_df.head()","1b0544c7":"def missing_values_table(df, info=True):\n        mis_val = df.isnull().sum()\n        mis_val_percent = 100 * df.isnull().sum() \/ len(df)\n        mis_val_table = pd.concat([mis_val, mis_val_percent], axis=1)\n        mis_val_table_ren_columns = mis_val_table.rename(\n        columns = {0 : 'Missing Values', 1 : '% of Total Values'})\n        mis_val_table_ren_columns = mis_val_table_ren_columns[\n            mis_val_table_ren_columns.iloc[:,1] != 0].sort_values(\n        '% of Total Values', ascending=False).round(1)\n        if info:\n            print (\"Your selected dataframe has \" + str(df.shape[1]) + \" columns.\\n\"      \n                \"There are \" + str(mis_val_table_ren_columns.shape[0]) +\n                  \" columns that have missing values.\")\n        return mis_val_table_ren_columns","6b0b8e1e":"print('Missing columns in train: ')\nmissing_values_table(train_df)","b8ed8356":"print('Missing columns in test: ')\nmissing_values_table(test_df)","63781986":"for df_ in (train_df, test_df):\n    df_['location'] = df_['location'].fillna('?')\n    df_['keyword'] = df_['keyword'].fillna('?')","e20d081c":"class MeanEncoding(BaseEstimator):\n    \"\"\"   In Mean Encoding we take the number \n    of labels into account along with the target variable \n    to encode the labels into machine comprehensible values    \"\"\"\n    \n    def __init__(self, feature, C=0.1):\n        self.C = C\n        self.feature = feature\n        \n    def fit(self, X_train, y_train):\n        \n        df = pd.DataFrame({'feature': X_train[self.feature], 'target': y_train}).dropna()\n        \n        self.global_mean = df.target.mean()\n        mean = df.groupby('feature').target.mean()\n        size = df.groupby('feature').target.size()\n        \n        self.encoding = (self.global_mean * self.C + mean * size) \/ (self.C + size)\n    \n    def transform(self, X_test):\n        \n        X_test[self.feature] = X_test[self.feature].map(self.encoding).fillna(self.global_mean).values\n        \n        return X_test\n    \n    def fit_transform(self, X_train, y_train):\n        \n        df = pd.DataFrame({'feature': X_train[self.feature], 'target': y_train}).dropna()\n        \n        self.global_mean = df.target.mean()\n        mean = df.groupby('feature').target.mean()\n        size = df.groupby('feature').target.size()\n        self.encoding = (self.global_mean * self.C + mean * size) \/ (self.C + size)\n        \n        X_train[self.feature] = X_train[self.feature].map(self.encoding).fillna(self.global_mean).values\n        \n        return X_train","806cd9ef":"for f in ['location', 'keyword']:\n    me = MeanEncoding(f, C=0.01*len(train_df[f].unique()))\n    me.fit(train_df, train_df['target'])\n    train_df = me.transform(train_df)\n    test_df = me.transform(test_df)","bbaf6283":"train_df.tail()","f32ddfcd":"test_df.tail()","d001feef":"def link_flg(string):\n    if 'http' in string.lower():\n        return 1\n    else:\n        return 0\n\nfor df_ in (train_df, test_df):\n    df_['link_flg'] = df_['text'].apply(link_flg)\n\ntmp = train_df.groupby('link_flg').agg('mean')['target'].reset_index()\nplt.figure(figsize=(8,5))\nfig = sns.barplot(x=tmp['link_flg'], y=tmp['target'], palette=\"husl\")","c69aaba5":"def remove_accented_chars(text):\n    text = unidecode.unidecode(text)\n    return text\n\ndef remove_html(text):\n    html=re.compile(r'<.*?>')\n    return html.sub(r'',text)\n\ndef remove_url(text):\n    url = re.compile(r'https?:\/\/\\S+|www\\.\\S+')\n    return url.sub(r'',text)\n\ndef text_preprocess(text):\n    text = remove_accented_chars(text)\n    text = remove_html(text)\n    text = remove_url(text)\n    return text","71b917d4":"train_df['text'] = train_df['text'].apply(text_preprocess)\ntest_df['text'] = test_df['text'].apply(text_preprocess)","56305372":"nm = 'cnt_len'\ndef cnt_len(string) -> int:\n    return len(string)\n\nfor df_ in (train_df, test_df):\n    df_[nm] = df_['text'].apply(cnt_len)\n\nplt.figure(figsize=(14,5))\nsns.distplot(train_df[nm], hist=False, rug=True, label=\"train\");\nsns.distplot(test_df[nm], hist=False, rug=True, label=\"test\");\nplt.legend();","2f689719":"tmp = train_df.groupby(nm).agg('mean')['target'].reset_index()\nplt.figure(figsize=(48,15))\nfig = sns.barplot(x=tmp[nm], y=tmp['target'], palette=\"husl\")","9cc1d026":"nm = 'cnt_users'\ndef cnt_users(string) -> int:\n    return sum(list(map(lambda s: 1 if s == '@' else 0, string)))\n\nfor df_ in (train_df, test_df):\n    df_[nm] = df_['text'].apply(cnt_users)\n\nplt.figure(figsize=(14,5))\nsns.distplot(train_df[nm], hist=False, rug=True, label=\"train\");\nsns.distplot(test_df[nm], hist=False, rug=True, label=\"test\");\nplt.legend();","290650ec":"tmp = train_df.groupby(nm).agg('mean')['target'].reset_index()\nplt.figure(figsize=(14,5))\nfig = sns.barplot(x=tmp[nm], y=tmp['target'], palette=\"husl\")","ceecf6cc":"nm = 'cnt_hashtags'\ndef cnt_hashtags(string) -> int:\n    return sum(list(map(lambda s: 1 if s == '#' else 0, string)))\n\nfor df_ in (train_df, test_df):\n    df_[nm] = df_['text'].apply(cnt_hashtags)","5770883b":"tmp = train_df.groupby(nm).agg('mean')['target'].reset_index()\nplt.figure(figsize=(14,5))\nfig = sns.barplot(x=tmp[nm], y=tmp['target'], palette=\"husl\")","be8fbaec":"punctuation = ['.',',',':',';','!','?','(',')','\/','\\\\','|','\\\"','\\'','-', '\u00ab', '\u00bb']\nnm = 'cnt_punctuation'\n\ndef cnt_punctuation(string) -> int:\n    return sum(list(map(lambda s: 1 if s in punctuation else 0, string)))\n\nfor df_ in (train_df, test_df):\n    df_[nm] = df_['text'].apply(cnt_punctuation)\n\nplt.figure(figsize=(14,5))\nsns.distplot(train_df[nm], hist=False, rug=True, label=\"train\");\nsns.distplot(test_df[nm], hist=False, rug=True, label=\"test\");\nplt.legend();","e7041272":"tmp = train_df.groupby(nm).agg('mean')['target'].reset_index()\nplt.figure(figsize=(14,5))\nfig = sns.barplot(x=tmp[nm], y=tmp['target'], palette=\"husl\")","80b41364":"nm = 'cnt_whitespace'\ndef cnt_whitespace(string) -> int:\n    return sum(list(map(lambda s: 1 if s == ' ' else 0, string)))\n\nfor df_ in (train_df, test_df):\n    df_[nm] = df_['text'].apply(cnt_whitespace)\n\nplt.figure(figsize=(14,5))\nsns.distplot(train_df[nm], hist=False, rug=True, label=\"train\");\nsns.distplot(test_df[nm], hist=False, rug=True, label=\"test\");\nplt.legend();","c5bbd66c":"tmp = train_df.groupby(nm).agg('mean')['target'].reset_index()\nplt.figure(figsize=(14,5))\nfig = sns.barplot(x=tmp[nm], y=tmp['target'], palette=\"husl\")","edfd41c8":"numeral = '1234567890'\nnm = 'cnt_numeral'\n\ndef cnt_numeral(string) -> int:\n    return sum(list(map(lambda s: 1 if s in numeral else 0, string)))\n\nfor df_ in (train_df, test_df):\n    df_[nm] = df_['text'].apply(cnt_numeral)\n\nplt.figure(figsize=(14,5))\nsns.distplot(train_df[nm], hist=False, rug=True, label=\"train\");\nsns.distplot(test_df[nm], hist=False, rug=True, label=\"test\");\nplt.legend();","82e8728a":"tmp = train_df.groupby(nm).agg('mean')['target'].reset_index()\nplt.figure(figsize=(14,5))\nfig = sns.barplot(x=tmp[nm], y=tmp['target'], palette=\"husl\")","4bd606de":"eng = 'abcdefghijklmnopqrstuvwxyz'\nnm = 'cnt_others_chars'\ndef cnt_others_chars(string) -> int:\n    return sum(list(map(lambda s: 1 if s != ' ' and s not in punctuation and s not in numeral and \n                        s != '#' and s != '@' and s not in eng and s not in eng.upper() else 0, string)))\n\nfor df_ in (train_df, test_df):\n    df_[nm] = df_['text'].apply(cnt_others_chars)\n","667a4f89":"tmp = train_df.groupby(nm).agg('mean').reset_index()\nplt.figure(figsize=(18,5))\nfig = sns.barplot(x=tmp[nm], y=tmp['target'], palette=\"husl\")","14c3d874":"nm = 'cnt_upper'\ndef cnt_upper(string) -> int:\n    return sum(list(map(lambda s: 1 if s.isupper() else 0, string)))\n\nfor df_ in (train_df, test_df):\n    df_[nm] = df_['text'].apply(cnt_upper)\n\nplt.figure(figsize=(14,5))\nsns.distplot(train_df[nm], hist=False, rug=True, label=\"train\");\nsns.distplot(test_df[nm], hist=False, rug=True, label=\"test\");\nplt.legend();","e6ca146a":"tmp = train_df.groupby(nm).agg('mean').reset_index()\nplt.figure(figsize=(18,5))\nfig = sns.barplot(x=tmp[nm], y=tmp['target'], palette=\"husl\")","610a10df":"nm = 'cnt_exclamatory'\n\ndef cnt_exclamatory(string) -> int:\n    return sum(list(map(lambda s: 1 if s == '!' else 0, string)))\n\nfor df_ in (train_df, test_df):\n    df_[nm] = df_['text'].apply(cnt_exclamatory)\n","9ed6e215":"tmp = train_df.groupby(nm).agg('mean')['target'].reset_index()\nplt.figure(figsize=(14,5))\nfig = sns.barplot(x=tmp[nm], y=tmp['target'], palette=\"husl\")","e1510258":"def words_len_stats(string) -> list:\n    tmp = np.array(list(map(lambda s: len(s), string.split())))\n    return pd.Series([np.median(tmp), np.min(tmp), np.max(tmp), np.std(tmp)])\n    \nfor df_ in (train_df, test_df):\n    df_[['word_len_median', 'word_len_min', 'word_len_max', 'word_len_std', ]] = df_['text'].apply(words_len_stats)\n\nfor c in ['word_len_median', 'word_len_min', 'word_len_max', 'word_len_std', ]:\n    plt.figure(figsize=(14,5))\n    sns.distplot(train_df[c], hist=False, rug=True, label=\"train\");\n    sns.distplot(test_df[c], hist=False, rug=True, label=\"test\");\n    plt.legend()\n    plt.show();","49b8b563":"for c in ['word_len_median', 'word_len_min', 'word_len_max']:\n    tmp = train_df.groupby(c).agg('mean')['target'].reset_index()\n    plt.figure(figsize=(14,5))\n    fig = sns.barplot(x=tmp[c], y=tmp['target'], palette=\"husl\")\n    plt.show();","548fe84c":"train_df.head()","7808a0bf":"train_df['cnt_punct_num_div_len'] = (train_df['cnt_punctuation'] + train_df['cnt_numeral'])**2 \/ train_df['cnt_len']\ntrain_df['cnt_users_hasht_div_len'] = (train_df['cnt_users'] + train_df['cnt_hashtags'])**2 \/ train_df['cnt_len']\ntrain_df['cnt_upper_div_len'] = train_df['cnt_upper']**2 \/ train_df['cnt_len']\ntrain_df['cnt_whitespace_div_len'] = train_df['cnt_whitespace']**2 \/ train_df['cnt_len']\ntrain_df['cnt_len_median_div_len'] = train_df['word_len_median']**2 \/ train_df['cnt_len']\ntrain_df['cnt_len_min_div_len'] = train_df['word_len_min']**2 \/ train_df['cnt_len']\ntrain_df['cnt_len_max_div_len'] = train_df['word_len_max']**2 \/ train_df['cnt_len'] \ntrain_df['cnt_len_std_div_len'] = train_df['word_len_std']**2 \/ train_df['cnt_len'] \n\ntest_df['cnt_punct_num_div_len'] = (test_df['cnt_punctuation'] + test_df['cnt_numeral'])**2 \/ test_df['cnt_len']\ntest_df['cnt_users_hasht_div_len'] = (test_df['cnt_users'] + test_df['cnt_hashtags'])**2 \/ test_df['cnt_len']\ntest_df['cnt_upper_div_len'] = test_df['cnt_upper']**2 \/ test_df['cnt_len']\ntest_df['cnt_whitespace_div_len'] = test_df['cnt_whitespace']**2 \/ test_df['cnt_len']\ntest_df['cnt_len_median_div_len'] = test_df['word_len_median']**2 \/ test_df['cnt_len']\ntest_df['cnt_len_min_div_len'] = test_df['word_len_min']**2 \/ test_df['cnt_len']\ntest_df['cnt_len_max_div_len'] = test_df['word_len_max']**2 \/ test_df['cnt_len']\ntest_df['cnt_len_std_div_len'] = test_df['word_len_std']**2 \/ test_df['cnt_len'] ","5b3083c0":"abbreviations = {\n    \"$\" : \" dollar \", \"\u20ac\" : \" euro \", \"4ao\" : \"for adults only\", \"a.m\" : \"before midday\", \n    \"a3\" : \"anytime anywhere anyplace\", \"aamof\" : \"as a matter of fact\", \"acct\" : \"account\", \n    \"adih\" : \"another day in hell\", \"afaic\" : \"as far as i am concerned\", \"ave.\" : \"avenue\",\n    \"afaict\" : \"as far as i can tell\", \"afaik\" : \"as far as i know\", \n    \"afair\" : \"as far as i remember\", \"afk\" : \"away from keyboard\", \"app\" : \"application\", \n    \"approx\" : \"approximately\", \"apps\" : \"applications\",\"atk\" : \"at the keyboard\", \n    \"asap\" : \"as soon as possible\", \"asl\" : \"age, sex, location\", \"ayor\" : \"at your own risk\", \n    \"aymm\" : \"are you my mother\", \"b&b\" : \"bed and breakfast\", \"b+b\" : \"bed and breakfast\",\n    \"b.c\" : \"before christ\", \"b2b\" : \"business to business\", \"b2c\" : \"business to customer\", \n    \"b4\" : \"before\", \"b4n\" : \"bye for now\", \"b@u\" : \"back at you\"\n    , \"bae\" : \"before anyone else\", \"bbc\" : \"british broadcasting corporation\", \n    \"bak\" : \"back at keyboard\", \"bbbg\" : \"bye bye be good\", \"be4\" : \"before\", \n    \"bbias\" : \"be back in a second\", \"bbl\" : \"be back later\", \"bbs\" : \"be back soon\",\n    \"bfn\" : \"bye for now\", \"blvd\" : \"boulevard\", \"bout\" : \"about\", \"brb\" : \"be right back\", \n    \"bros\" : \"brothers\", \"brt\" : \"be right there\", \"bsaaw\" : \"big smile and a wink\",\n    \"btw\" : \"by the way\", \"bwl\" : \"bursting with laughter\", \"c\/o\" : \"care of\", \n    \"cet\" : \"central european time\", \"cf\" : \"compare\", \"cia\" : \"central intelligence agency\", \n    \"csl\" : \"can not stop laughing\", \"cu\" : \"see you\", \"cul8r\" : \"see you later\", \n    \"cv\" : \"curriculum vitae\", \"cwot\" : \"complete waste of time\", \"cya\" : \"see you\",\n    \"cyt\" : \"see you tomorrow\", \"dae\" : \"does anyone else\", \n    \"dbmib\" : \"do not bother me i am busy\",\n    \"diy\" : \"do it yourself\", \"dm\" : \"direct message\", \"dwh\" : \"during work hours\", \n    \"e123\" : \"easy as one two three\", \"eet\" : \"eastern european time\", \"eg\" : \"example\", \n    \"embm\" : \"early morning business meeting\", \"encl\" : \"enclosed\", \"encl.\" : \"enclosed\", \n    \"etc\" : \"and so on\", \"faq\" : \"frequently asked questions\", \"fawc\" : \"for anyone who cares\",\n    \"fb\" : \"facebook\", \"fc\" : \"fingers crossed\", \"fig\" : \"figure\", \n    \"fimh\" : \"forever in my heart\", \n    \"ft.\" : \"feet\", \"ft\" : \"featuring\", \"ftl\" : \"for the loss\", \"ftw\" : \"for the win\", \n    \"fwiw\" : \"for what it is worth\", \"fyi\" : \"for your information\", \"g9\" : \"genius\", \n    \"gahoy\" : \"get a hold of yourself\", \"gal\" : \"get a life\", \"gfn\" : \"gone for now\", \n    \"gg\" : \"good game\", \"gl\" : \"good luck\", \"glhf\" : \"good luck have fun\", \n    \"gmta\" : \"great minds think alike\", \"gn\" : \"good night\", \n    \"g.o.a.t\" : \"greatest of all time\", \n    \"goat\" : \"greatest of all time\", \"goi\" : \"get over it\", \"gmt\" : \"greenwich mean time\", \n    \"gcse\" : \"general certificate of secondary education\", \"gps\" : \"global positioning system\", \n    \"gr8\" : \"great\", \"gratz\" : \"congratulations\", \"gyal\" : \"girl\", \"h&c\" : \"hot and cold\",\n    \"hp\" : \"horsepower\", \"hr\" : \"hour\", \"hrh\" : \"his royal highness\", \"ht\" : \"height\", \n    \"ibrb\" : \"i will be right back\", \"ic\" : \"i see\", \"icq\" : \"i seek you\", \n    \"icymi\" : \"in case you missed it\", \"idc\" : \"i do not care\", \n    \"idgadf\" : \"i do not give a damn fuck\", \"i.e\" : \"that is\", \n    \"idgaf\" : \"i do not give a fuck\", \"idk\" : \"i do not know\", \"ie\" : \"that is\",\n    \"ifyp\" : \"i feel your pain\", \"IG\" : \"instagram\", \"iirc\" : \"if i remember correctly\",\n    \"ilu\" : \"i love you\", \"ily\" : \"i love you\", \"imho\" : \"in my humble opinion\", \n    \"imo\" : \"in my opinion\", \"imu\" : \"i miss you\", \"iow\" : \"in other words\", \n    \"irl\" : \"in real life\", \n    \"j4f\" : \"just for fun\", \"jic\" : \"just in case\", \"jk\" : \"just kidding\",\n    \"jsyk\" : \"just so you know\", \"l8r\" : \"later\", \"lb\" : \"pound\", \"lbs\" : \"pounds\", \n    \"ldr\" : \"long distance relationship\", \"lmao\" : \"laugh my ass off\", \n    \"lmfao\" : \"laugh my fucking ass off\", \"lol\" : \"laughing out loud\", \"ltd\" : \"limited\",\n    \"ltns\" : \"long time no see\", \"m8\" : \"mate\", \"mf\" : \"motherfucker\", \"mfs\" : \"motherfuckers\", \n    \"mfw\" : \"my face when\", \"mofo\" : \"motherfucker\", \"mph\" : \"miles per hour\", \"mr\" : \"mister\", \n    \"mrw\" : \"my reaction when\", \"ms\" : \"miss\", \"mte\" : \"my thoughts exactly\", \n    \"nagi\" : \"not a good idea\", \n    \"nbc\" : \"national broadcasting company\", \"nbd\" : \"not big deal\", \"nfs\" : \"not for sale\", \n    \"ngl\" : \"not going to lie\", \"nhs\" : \"national health service\", \n    \"nrn\" : \"no reply necessary\", \n    \"nsfl\" : \"not safe for life\", \"nsfw\" : \"not safe for work\", \"nth\" : \"nice to have\",\n    \"nvr\" : \"never\", \"nyc\" : \"new york city\", \"oc\" : \"original content\", \"og\" : \"original\", \n    \"ohp\" : \"overhead projector\", \"oic\" : \"oh i see\", \n    \"omdb\" : \"over my dead body\", \"omg\" : \"oh my god\", \n    \"omw\" : \"on my way\", \"p.a\" : \"per annum\", \"p.m\" : \"after midday\",  \"pm\" : \"prime minister\", \n    \"poc\" : \"people of color\", \"pov\" : \"point of view\", \"pp\" : \"pages\", \"ppl\" : \"people\", \n    \"prw\" : \"parents are watching\", \"ps\" : \"postscript\", \n    \"pt\" : \"point\", \"ptb\" : \"please text back\",\n    \"pto\" : \"please turn over\", \"qpsa\" : \"what happens\", \"ratchet\" : \"rude\", \n    \"rbtl\" : \"read between the lines\", \"rlrt\" : \"real life retweet\", \n    \"rofl\" : \"rolling on the floor laughing\", \"ruok\" : \"are you ok\",\n    \"roflol\" : \"rolling on the floor laughing out loud\", \"rt\" : \"retweet\",  \n    \"rotflmao\" : \"rolling on the floor laughing my ass off\", \"sfw\" : \"safe for work\",\n    \"sk8\" : \"skate\", \"smh\" : \"shake my head\", \"sq\" : \"square\", \"srsly\" : \"seriously\", \n    \"ssdd\" : \"same stuff different day\", \"tbh\" : \"to be honest\", \"tbs\" : \"tablespooful\", \n    \"tbsp\" : \"tablespooful\", \"tfw\" : \"that feeling when\", \"thks\" : \"thank you\",\n    \"tho\" : \"though\", \"thx\" : \"thank you\", \"tia\" : \"thanks in advance\", \n    \"til\" : \"today i learned\", \"tmb\" : \"tweet me back\",\n    \"tl;dr\" : \"too long i did not read\", \"tldr\" : \"too long i did not read\",  \n    \"tntl\" : \"trying not to laugh\", \"ttyl\" : \"talk to you later\", \n    \"u\" : \"you\", \"u2\" : \"you too\", \n    \"u4e\" : \"yours for ever\", \"utc\" : \"coordinated universal time\", \n    \"w\/\" : \"with\", \"w\/o\" : \"without\", \n    \"w8\" : \"wait\", \"wassup\" : \"what is up\", \"wb\" : \"welcome back\", \"wtf\" : \"what the fuck\", \n    \"wtg\" : \"way to go\", \"wtpa\" : \"where the party at\", \"wuf\" : \"where are you from\", \n    \"wuzup\" : \"what is up\", \"wywh\" : \"wish you were here\", \n    \"yd\" : \"yard\", \"ygtr\" : \"you got that right\", \n    \"ynk\" : \"you never know\", \"zzz\" : \"sleeping bored and tired\" }","434fa018":"def convert_abbrev(word):\n    return abbreviations[word.lower()] if word.lower() in abbreviations.keys() else word\n\ndef convert_abbrev_in_text(text):\n    tokenizer = WordPunctTokenizer()\n    tokens = tokenizer.tokenize(text)\n    tokens = [convert_abbrev(word) for word in tokens]\n    text = ' '.join(tokens)\n    return text\n\ntrain_df[\"text\"] = train_df[\"text\"].apply(lambda x: convert_abbrev_in_text(x))\ntest_df[\"text\"] = test_df[\"text\"].apply(lambda x: convert_abbrev_in_text(x))","0a04a5a9":"def remove_emoji(text):\n    emoji_pattern = re.compile(\"[\"\n                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n                           u\"\\U00002702-\\U000027B0\"\n                           u\"\\U000024C2-\\U0001F251\"\n                           \"]+\", flags=re.UNICODE)\n    return emoji_pattern.sub(r'', text)\n\ndef remove_punct(text):\n    table=str.maketrans('','',string.punctuation)\n    return text.translate(table)\n\nspell = SpellChecker()\ndef correct_spellings(text):\n    corrected_text = []\n    misspelled_words = spell.unknown(text.split())\n    for word in text.split():\n        if word in misspelled_words:\n            corrected_text.append(spell.correction(word))\n        else:\n            corrected_text.append(word)\n    return \" \".join(corrected_text)\n\n# Now compact all the normalization function calls into a single function\ndef normalization(text):\n    text = remove_emoji(text)\n    text = remove_punct(text)\n    text = correct_spellings(text)\n    return text","bf5b8040":"%%time\ntrain_df['text'] = train_df['text'].apply(normalization)\ntest_df['text'] = test_df['text'].apply(normalization)","e6f15320":"tokenizer = WordPunctTokenizer()\npreprocess = lambda text: ' '.join(tokenizer.tokenize(text.lower()))\n\ntext = 'How to be a grown-up at work: replace \"tweet you\" with \"Ok, great!!!\".'\nprint(\"before:\", text,)\nprint(\"after:\", preprocess(text),)","cd0fe4fe":"nltk.download('stopwords')","24a4423d":"stop_words = set(stopwords.words('english'))\nstop_words_filter = lambda text: ' '.join([t for t in text.split() if t not in stop_words])\nprint(\"before:\", text,)\nprint(\"after:\", stop_words_filter(preprocess(text)),)","7f82d07d":"%%time\ntexts_train = list(map(lambda text: stop_words_filter(preprocess(text)), train_df['text']))\ntexts_test = list(map(lambda text: stop_words_filter(preprocess(text)), test_df['text']))\nassert len(texts_train) == len(train_df)\nassert len(texts_test) == len(test_df)","98a68b37":"import collections\nk = 3750\nc = collections.Counter()\n\nfor sentence in texts_train:\n    for word in tokenizer.tokenize(sentence):\n        c[word] += 1\n\nbow_vocabulary = list([i[0] for i in c.most_common(k)])\nprint('example features:', sorted(bow_vocabulary)[::500])","29ef5a29":"def text_to_bow(text) -> np.array:\n    \"\"\" convert text string to an array of token counts. Use bow_vocabulary. \"\"\"\n    bow = bow_vocabulary\n    tmp = []\n    for ch_bow in bow:\n        val = 0\n        for ttk in tokenizer.tokenize(text):\n            if ch_bow == ttk:\n                val += 1\n        tmp.append(val)\n    \n    return np.array(tmp, 'float32')","f8094307":"%%time\nX_train_bow = np.stack(list(map(text_to_bow, texts_train)))\nX_test_bow = np.stack(list(map(text_to_bow, texts_test)))","dbb90194":"X_train_bow[0:5]","e4e4d866":"#convert country names to dictionary with values and its occurences\nwordcloud = WordCloud(width = 1000, height = 500).generate_from_frequencies(c)\n\nplt.figure(figsize=(15,8))\nplt.imshow(wordcloud)\nplt.axis(\"off\")\nplt.show()","92924a4a":"def tf_idf_calc(txt, bow: list) -> dict:\n    d = {}\n    n_txt = len(txt)\n    for word in bow:\n        cnt = 0\n        for sent in txt:\n            if word in set(tokenizer.tokenize(sent)):\n                cnt += 1\n        d[word] = np.log(n_txt \/ (cnt + 1))\n    return d","f2ddef8f":"%%time\ntf_idf_dict_train = tf_idf_calc(texts_train, bow_vocabulary)","2a6e45b3":"def tf_idf(df, d) -> list:\n    df_ = []\n    for bag in df:\n        tmp = []\n        for n_word in range(len(bag)):\n            tmp.append(bag[n_word] * d[bow_vocabulary[n_word]])\n        df_.append(tmp)\n    return df_","3c02549e":"%%time\nX_train_tf_idf = pd.DataFrame(tf_idf(X_train_bow, tf_idf_dict_train))\nX_test_tf_idf = pd.DataFrame(tf_idf(X_test_bow, tf_idf_dict_train))","5b470313":"y = train_df['target']\ntrain_df.drop(['target'], axis=1, inplace=True)","85d07b10":"y.value_counts()","a0eada87":"y.value_counts(normalize=True)","71cb878e":"y.hist(bins=y.nunique());","35316914":"X_train_tf_idf.shape","7bf73ee9":"%%time\ncols = X_train_tf_idf.shape[1]\n# SVD step\nstep = 10\n# different between train and test penalty power:\np = 0.8\n\nskf = StratifiedKFold(n_splits=3, shuffle=True, random_state=777)\nw=[]\n\nfor n_svd in range(X_train_tf_idf.shape[1]-(step*300), 0, -step):\n    tmp_w = []\n    for train_index, val_index in skf.split(X_train_tf_idf, y):\n        x_train, x_valid = X_train_tf_idf.iloc[train_index, :], X_train_tf_idf.iloc[val_index, :]\n        y_train, y_valid = y[train_index], y[val_index]\n        \n        svd = TruncatedSVD(n_components=n_svd).fit(x_train)\n        x_train_svd = svd.transform(x_train)\n        x_valid_svd = svd.transform(x_valid)\n        \n        tf_idf_model = LogisticRegression().fit(x_train_svd, y_train)\n        svd_train = f1_score(y_train, tf_idf_model.predict(x_train_svd))\n        svd_valid = f1_score(y_valid, tf_idf_model.predict(x_valid_svd))\n        tmp_w.append([svd_train, svd_valid])\n        \n    mn = np.mean(tmp_w, axis=0)\n    train = mn[0]\n    test = mn[1]\n    w.append([n_svd, train, test, np.power(np.square(train-test), p), +test-np.power(np.square(train-test), p)])","0adb097e":"svd_valid = pd.DataFrame(w, columns=['n_svd', 'train', 'test', 'diff', 'result'])","ad390167":"svd_valid","ca3dd553":"svd_valid[svd_valid.index == svd_valid['result'].argmax()]","c4762495":"n_svd = svd_valid[svd_valid.index == svd_valid['result'].argmax()].iloc[0, 0]","37196099":"%%time\nsvd = TruncatedSVD(n_components=n_svd).fit(X_train_tf_idf)\nX_train_svd = svd.transform(X_train_tf_idf)\nX_test_svd = svd.transform(X_test_tf_idf)","f67088ac":"train_df.drop('text', inplace=True, axis=1)\ntest_df.drop('text', inplace=True, axis=1)","70a55bb6":"train_corr = train_df.corr()\n# plot the heatmap and annotation on it\nfig, ax = plt.subplots(figsize=(16,16))\nsns.heatmap(train_corr, xticklabels=train_corr.columns, yticklabels=train_corr.columns, annot=True, ax=ax);","75f11279":"test_corr = test_df.corr()\n# plot the heatmap and annotation on it\nfig, ax = plt.subplots(figsize=(16,16))\nsns.heatmap(test_corr, xticklabels=test_corr.columns, yticklabels=test_corr.columns, annot=True, ax=ax);","ec7d90ef":"drop_cols = ['cnt_len', 'cnt_hashtags', 'cnt_whitespace', 'cnt_upper', 'word_len_min', 'word_len_max', 'word_len_std',]","f0a8007c":"train_df[['svd_'+str(c) for c in range(X_train_svd.shape[1])]] = pd.DataFrame(X_train_svd)\ntest_df[['svd_'+str(c) for c in range(X_test_svd.shape[1])]] = pd.DataFrame(X_test_svd)","6387498b":"train_df.head()","68301a70":"test_df.head()","30e05dc4":"train_df.shape, test_df.shape","ba0d9b78":"%%time\ntrain_scores=[]\ntest_scores=[]\n\nskf_1 = StratifiedKFold(n_splits=3, shuffle=True, random_state=777)\n\nlr_grid = {\"C\": [10, 1, 0.5, 0.1, 0.075, 0.06, 0.05, 0.04, 0.35, 0.25, 0.01, 0.005, 0.001], \"penalty\":['l1', 'l2']}\n\n\nfor no, (train_index_1, val_index_1) in enumerate(skf_1.split(train_df, y)):\n    train_df_ = train_df.drop(drop_cols, axis=1).copy()\n    test_df_ = test_df.drop(drop_cols, axis=1).copy()\n    x_train_1, x_valid_1 = train_df_.iloc[train_index_1, :], train_df_.iloc[val_index_1, :]\n    y_train_1, y_valid_1 = y[train_index_1], y[val_index_1]\n    \n    logreg=LogisticRegression()\n    logreg_cv=GridSearchCV(logreg, lr_grid, cv=3, verbose=False, scoring='f1', n_jobs=-1)\n    logreg_cv.fit(x_train_1, y_train_1)\n    logreg_model = LogisticRegression(**logreg_cv.best_params_).fit(x_train_1, y_train_1)\n    train_pred = logreg_model.predict_proba(train_df_)[:, 1]\n    test_pred = logreg_model.predict_proba(test_df_)[:, 1]\n    train_scores.append(train_pred)\n    test_scores.append(test_pred)\n    print('Fold Log: ', no, 'CV F1: ', logreg_cv.best_score_, 'Valid F1: ', f1_score(y_valid_1, logreg_model.predict(x_valid_1)), \n          'Best params: ', logreg_cv.best_params_)","d3fa7532":"sub_df['target'] = list(map(lambda x: 1 if x>=0.5 else 0, np.mean(test_scores, axis=0)))","ed2a38b7":"sub_df['target'].mean()","064fb174":"sub_df.to_csv('submission.csv', index=False)","e57962cb":"# Simple EDA","493f2ae1":"Thank you for attention!","4703aef6":"# TEXT Normalization\nDefinition: The jargon used in coloquial language, specially in social media, leads to either content not useful for NLP or to different versions of the same sentence but written in alternative ways. For example: punctuation marks, emojis, contractions, URLs, grammar errors...\n\nTo normalize data and reduce noise, we will apply several transformations:\n\n- Remove emojis. Despite emojis are related to sentiments, they are abused and used in any type of tweet.\n- Remove punctuation. All punctuation marks are deleted.\n- Spell checking. Substitute wrong sentences (Am gona ned you) to correct ones (I am going to need you).\n\n\nDisclaimer: the origin of several of the functions used in this subsection come from the awesome kernel of Shahules786; https:\/\/www.kaggle.com\/shahules\/basic-eda-cleaning-and-glove. Please check it out for reckon.","73a4be05":"# Some additional features\n* with div tweet diff length","23876f6d":"# Manual tf-idf without SKLEARN\n- Calculate bow vocabulary","27e7432d":"### Fill it","5849368d":"# Count users by \"@\"","87233f91":"# Words length stats in tweet:\n- median len\n- min len\n- max len\n- std len","0ff10748":"### At this stage we choose the optimal decomposition. We introduce the metric for this step:\n\n$$ penalty = { [(F_1(train) - F_1(test))^2]^P } $$\n$$ bestSVD = { argmax[F_1(test) - penalty ] } $$\n","09e7efa2":"## Options","027b9449":"# Link Flag","6069060c":"# Count words by whitespaces","804175cb":"# Predict & submit","e19d2163":"# Count hashtags by \"#\"","178704b5":"# Blend it","14f933ab":"# Count exclamatory in tweets","57692e35":"# Text preprocessing with NLTK\n- WordPunctTokenizer\n- stopwords delete","7e11d676":"# Drop columns with high correlation","c876a543":"# Text preprocess for counts\n- Remove accents. Instead of receiving accented characters (latt\u00e9) we just write plain letters.\n- Remove URL. Most tweets may include links and such, it may be good to clean them.\n- Remove html. Scratched data usually contains headers and marks (<br\/), this should be removed.","d99a79a1":"## Check some Null's","5b957c8b":"## Import","546c6e0a":"### WordCloud for tweets","8bcf74fc":"# Count other symbols","8ca0ffe0":"## This is my first public Notebook. Please, do an upvote)\n\n# Competition Description\n\n* https:\/\/www.kaggle.com\/c\/nlp-getting-started\n\nIn this competition, you\u2019re challenged to build a machine learning model that predicts which Tweets are about real disasters and which one\u2019s aren\u2019t. You\u2019ll have access to a dataset of 10,000 tweets that were hand classified. If this is your first time working on an NLP problem, we've created a quick tutorial to get you up and running.\n","b3466101":"# Stack it","1611b4d9":"# Count punctuation by:\n## \".\", \",\", \":\", \";\", \"!\", \"?\", \"(\", \")\", \"\/\", \"\\\\\", \"|\", \"\\\"\", \"\\\"\", \"-\",  \"\u00ab\",  \"\u00bb\"","5a00864f":"# Count other upper","14829d74":"### Preprocess each tweet in train and test:","b073b0a0":"# Words features:\n1. build a vocabulary of frequent words (use train data only)\n2. for each training sample, count the number of times a word occurs in it (for each word in vocabulary).\n3. consider this count a feature with tf-idf for some classifier","7d5dbb39":"# Count numeral in tweets","548a3030":"# SMS Slang clear\n- https:\/\/github.com\/rishabhverma17\/sms_slang_translator\/blob\/master\/slang.txt\n- https:\/\/www.kaggle.com\/rftexas\/text-only-kfold-bert","de028dae":"# Check it","01b8b985":"### Best number Truncated SVD is:","e14f105c":"# Truncated SVD for logistic regression ","b4cb3159":"# Get target","eaf722e0":"# Competition metric\nSubmissions are evaluated using F1 between the predicted and expected answers.\n\nF1 is calculated as follows:\n$$ F_1 = {2 * {precision*recall \\over precision+recall}} $$\n\n\nand:\n\n$$ precision = {TP \\over TP+FP} $$\n\n$$ recall = {TP \\over TP+FN} $$\n\n\n* True Positive [TP] = your prediction is 1, and the ground truth is also 1 - you predicted a positive and that's true!\n* False Positive [FP] = your prediction is 1, and the ground truth is 0 - you predicted a positive, and that's false.\n* False Negative [FN] = your prediction is 0, and the ground truth is 1 - you predicted a negative, and that's false.","f4f1a137":"## TF-IDF features\n\nNot all words are equally useful. One can prioritize rare words and downscale words like \"and\"\/\"or\" by using __tf-idf features__. This abbreviation stands for __text frequency\/inverse document frequence__ and means exactly that:\n\n$$ feature_i = { Count(word_i \\in x) \\times { log {N \\over Count(word_i \\in D) + \\alpha} }} $$\n\n\n, where x is a single text, D is your dataset (a collection of texts), N is a total number of documents and $\\alpha$ is a smoothing hyperparameter (typically 1). \nAnd $Count(word_i \\in D)$ is the number of documents where $word_i$ appears.\n\nIt may also be a good idea to normalize each data sample after computing tf-idf features.","b0661bbc":"# Count len of tweets","f642e946":"### Transform tf-idf  matrix with Truncated SVD","1d3580f7":"# Make mean target encoding for categorical feature\n\nLet us consider the above table (A simple binary classification). \n\n$$ MeanTargetEnc_i = {((GlobalMean * C) + (Mean_i * Size)) \\over (C + Size)} $$\n\nInstead of finding the mean of the targets, we can also focus on median and other statistical correlations\u2026.These are broadly called target encodings"}}