{"cell_type":{"4adc8e8a":"code","a0a1b5c5":"code","1c17defe":"code","22fe30db":"code","d831dc42":"code","f144fde3":"code","f88b4f08":"code","3a28f211":"code","a970cf3d":"code","1e0529b2":"code","a77f3971":"code","84b970d0":"code","72be74a4":"code","d0765255":"code","34a8079e":"code","6868c30e":"code","a726315c":"code","26004d73":"code","6e28cdfd":"code","4a78f3b4":"markdown"},"source":{"4adc8e8a":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","a0a1b5c5":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\nfrom sklearn.svm import SVC\nfrom sklearn.model_selection import train_test_split,cross_val_score\nfrom sklearn.metrics import accuracy_score,confusion_matrix,classification_report\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.linear_model import SGDClassifier\nimport warnings\nwarnings.filterwarnings('always') \n","1c17defe":"filepath = \"..\/input\/phishing-website-detector\/phishing.csv\"\ndata = pd.read_csv(filepath)\ndata.head()","22fe30db":"data.info()","d831dc42":"data.isnull().sum()","f144fde3":"X = data.drop(columns=\"class\")\nX","f88b4f08":"Y=data[\"class\"]\nY=pd.DataFrame(Y)\nY.head()","3a28f211":"train_X,test_X,train_Y,test_Y = train_test_split(X,Y,test_size=0.3,random_state=2)","a970cf3d":"print(train_X.shape)\nprint(train_Y.shape)\nprint(test_X.shape)\nprint(test_Y.shape)","1e0529b2":"corr = data.corr()\nfig,ax= plt.subplots(figsize=(20,20))\nsns.heatmap(corr,annot=True,linewidth=2.5,ax=ax)","a77f3971":"\nlg=LogisticRegression()\nmodel1=lg.fit(train_X,train_Y)\nlg_predict = lg.predict(test_X)\nacc_lg=accuracy_score(lg_predict,test_Y)\nprint(acc_lg)\nprint(classification_report(lg_predict,test_Y))\ncon  = confusion_matrix(lg_predict,test_Y)\nsns.heatmap(con,annot=True,fmt='.2f')","84b970d0":"svc=SVC()\nmodel2=svc.fit(train_X,train_Y)\nsvc_predict = svc.predict(test_X)\nacc_svc = accuracy_score(test_Y, svc_predict)\nprint(acc_svc)\nprint(classification_report(svc_predict,test_Y))\ncon  = confusion_matrix(svc_predict,test_Y)\nsns.heatmap(con,annot=True,fmt='.2f')","72be74a4":"gbc = GradientBoostingClassifier()\nmodel3=gbc.fit(train_X,train_Y)\ngbc_predict = gbc.predict(test_X)\nacc_gbc = accuracy_score(test_Y, gbc_predict)\nprint(acc_gbc)\nprint(classification_report(gbc_predict,test_Y))\ncon  = confusion_matrix(gbc_predict,test_Y)\nsns.heatmap(con,annot=True,fmt='.2f')\n","d0765255":"tree = DecisionTreeClassifier()\nmodel5=tree.fit(train_X,train_Y)\ntree_predict = tree.predict(test_X)\nacc_tree = accuracy_score(test_Y, tree_predict)\nprint(acc_tree)\nprint(classification_report(tree_predict,test_Y))\ncon  = confusion_matrix(tree_predict,test_Y)\nsns.heatmap(con,annot=True,fmt='.2f')\n","34a8079e":"forest = RandomForestClassifier()\nmodel6 = forest.fit(train_X,train_Y)\nforest_predict = forest.predict(test_X)\nacc_forest = accuracy_score(test_Y, forest_predict)\nprint(acc_forest)\nprint(classification_report(forest_predict,test_Y))\ncon  = confusion_matrix(forest_predict,test_Y)\nsns.heatmap(con,annot=True,fmt='.2f')","6868c30e":"knn=KNeighborsClassifier()\nmodel7=knn.fit(train_X,train_Y)\nknn_predict = knn.predict(test_X)\nacc_knn = accuracy_score(test_Y, knn_predict)\nprint(acc_knn)\nprint(classification_report(knn_predict,test_Y))\ncon  = confusion_matrix(knn_predict,test_Y)\nsns.heatmap(con,annot=True,fmt='.2f')","a726315c":"sgd =SGDClassifier()\nmodel8=sgd.fit(train_X,train_Y)\nsgd_predict = sgd.predict(test_X)\nacc_sgd = accuracy_score(test_Y, sgd_predict)\nprint(acc_sgd)\nprint(classification_report(sgd_predict,test_Y))\ncon  = confusion_matrix(sgd_predict,test_Y)\nsns.heatmap(con,annot=True,fmt='.2f')","26004d73":"print('Logistic Regression Accuracy:',round(acc_lg*100,2))\nprint('K-Nearest Neighbour Accuracy:',round(acc_knn*100,2))\nprint('Decision Tree Classifier Accuracy:',round(acc_tree*100,2))\nprint('Random Forest Classifier Accuracy:',round(acc_forest*100,2))\nprint('support Vector Machine Accuracy:',round(acc_svc*100,2))\nprint('GradientBoost Classifier Accuracy:',round(acc_gbc*100,2))\nprint('SGD Accuracy:',round(acc_sgd*100,2))","6e28cdfd":"from sklearn.model_selection import learning_curve\ndef plot_learning_curve(estimator,title,X,y,ylim=None,cv=None,n_jobs=-1,train_sizes=np.linspace(.1, 1.0, 5)):\n    plt.figure()\n    plt.title(title)\n    if ylim is not None:\n        plt.ylim(*ylim)\n    plt.xlabel(\"Training Examples\")\n    plt.ylabel(\"Score\")\n    train_sizes, train_scores, test_scores = learning_curve(\n        estimator, X, y, cv=cv, n_jobs=n_jobs, train_sizes=train_sizes)\n    train_scores_mean = np.mean(train_scores, axis=1)\n    train_scores_std = np.std(train_scores, axis=1)\n    test_scores_mean = np.mean(test_scores, axis=1)\n    test_scores_std = np.std(test_scores, axis=1)\n    plt.grid()\n\n    plt.fill_between(train_sizes, train_scores_mean - train_scores_std,\n                     train_scores_mean + train_scores_std, alpha=0.1,\n                     color=\"r\")\n    plt.fill_between(train_sizes, test_scores_mean - test_scores_std,\n                     test_scores_mean + test_scores_std, alpha=0.1, color=\"b\")\n    plt.plot(train_sizes, train_scores_mean, 'o-', color=\"r\",\n             label=\"Training score\")\n    plt.plot(train_sizes, test_scores_mean, 'o-', color=\"b\",\n             label=\"Cross-validation score\")\n\n    plt.legend(loc=\"best\")\n    return plt\n\ng = plot_learning_curve(model1,\" Logistic Regression learning curves\",train_X,train_Y)\ng = plot_learning_curve(model2,\" SVC learning curves\",train_X,train_Y)\ng = plot_learning_curve(model3,\" GradientBoost Classifier learning curves\",train_X,train_Y)\ng = plot_learning_curve(model8,\" SGD learning curves\",train_X,train_Y)\ng = plot_learning_curve(model5,\" Decision Tree Classifier learning curves\",train_X,train_Y)\ng = plot_learning_curve(model6,\" Random Forest Classifier learning curves\",train_X,train_Y)\ng = plot_learning_curve(model7,\" KNN learning curves\",train_X,train_Y)\n","4a78f3b4":"Random Forest Classifier has highest accuracy of 97.10%"}}