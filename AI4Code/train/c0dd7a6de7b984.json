{"cell_type":{"e4fbfb83":"code","89ef40c7":"code","f39aafe5":"code","f4f9d2bd":"code","091e9b4e":"code","aee51c4d":"code","1a2e70bb":"code","c1bb8f6d":"code","c81567ab":"code","feb52d18":"code","8a8418c4":"code","889d73a4":"code","121dbea2":"code","2d20fcff":"code","0417c6aa":"code","d665c280":"code","a0b24119":"code","4096dd43":"code","510dbda6":"code","c6574e93":"code","47c9af0e":"code","e716164b":"code","103a384e":"code","af71891d":"code","e36b20be":"code","2cd0752b":"code","52b96428":"code","24ae9d58":"code","1b00c4cc":"code","c6137eae":"code","cb4d84db":"code","5634de71":"code","0340161a":"code","249b5743":"code","2f8feed1":"code","dd4eb3cc":"code","53634e03":"code","8d8c3acf":"code","244c1806":"code","7aa8aac3":"code","05dcc19a":"markdown","85d3c29e":"markdown","3a5e99b3":"markdown","d2766e2a":"markdown","02b0599d":"markdown","eca7bda9":"markdown","9f3da5e1":"markdown","8c07dce6":"markdown","57498e7a":"markdown","36b8e475":"markdown","d9de1361":"markdown","b28f7ff2":"markdown","a882928f":"markdown","e7c4fe26":"markdown","0d9e08f1":"markdown","c005860a":"markdown","b8d26baf":"markdown","8359c0a4":"markdown","2ad4af83":"markdown","45f627c9":"markdown","8e4af87c":"markdown","3e0c1f40":"markdown","24d2d49f":"markdown","6f3e1dd1":"markdown"},"source":{"e4fbfb83":"import numpy as np\nimport pandas as pd\nimport os\nimport random, re, math\nimport tensorflow as tf, tensorflow.keras.backend as K\nimport tensorflow_addons as tfa\nfrom tensorflow.keras.layers import Dense\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras import optimizers\nfrom kaggle_datasets import KaggleDatasets\nfrom tensorflow.keras.models import Sequential\nimport tensorflow.keras.layers as L\nfrom tensorflow.keras.applications import ResNet152V2, InceptionResNetV2, InceptionV3, Xception, VGG19\nfrom tensorflow.keras.layers import Dense, GlobalAveragePooling2D,GlobalMaxPooling2D\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom sklearn.model_selection import train_test_split\nfrom tensorflow.keras.callbacks import ReduceLROnPlateau , EarlyStopping , ModelCheckpoint , LearningRateScheduler\n#from keras import regularizers\n\nimport matplotlib.pyplot as plt\n\n!pip install efficientnet\nimport efficientnet.tfkeras as efn","89ef40c7":"AUTO = tf.data.experimental.AUTOTUNE\ntry:\n    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n    print('Running on TPU ', tpu.master())\nexcept ValueError:\n    tpu = None\n\nif tpu:\n    tf.config.experimental_connect_to_cluster(tpu)\n    tf.tpu.experimental.initialize_tpu_system(tpu)\n    strategy = tf.distribute.experimental.TPUStrategy(tpu)\nelse:\n    strategy = tf.distribute.get_strategy()\n\nprint(\"REPLICAS: \", strategy.num_replicas_in_sync)\n","f39aafe5":"GCS_DS_PATH = KaggleDatasets().get_gcs_path('isic-2019-train')","f4f9d2bd":"train = pd.read_csv('..\/input\/csv-isic\/ISIC_2019_Training_GroundTruth.csv')","091e9b4e":"train","aee51c4d":"train_paths = train.image.apply(lambda x: GCS_DS_PATH+ 'ISIC_2019_Training_Input\/' + x+'.jpg').values\n#train_labels = train.target.values","1a2e70bb":"train.head(10)","c1bb8f6d":"#train=train.drop(columns=['D','H','A','O','G'],axis=1)","c81567ab":"#train=train[((train['N']== 1) | (train['C'] == 1)| (train['M'] == 1))]","feb52d18":"#train","8a8418c4":"train,valid = train_test_split(train,test_size = 0.2,random_state = 42)","889d73a4":"BATCH_SIZE = 8* strategy.num_replicas_in_sync\nimg_size = 512\nEPOCHS = 100\nSEED = 42","121dbea2":"def decode_image(filename, label=None, image_size=(img_size,img_size)):\n    bits = tf.io.read_file(filename)\n    image = tf.image.decode_jpeg(bits, channels=3) \n    image = tf.image.resize(image, image_size)\n    image = tf.cast(image, tf.float32)\n    image = tf.image.per_image_standardization(image)\n    if label is None:\n        return image\n    else:\n        return image, label\n    \ndef preprocess(df,test=False):\n    paths = df.image.apply(lambda x: GCS_DS_PATH+ '\/ISIC_2019_Training_Input\/' + x+'.jpg').values\n    labels = df.loc[:, ['MEL','NV','BCC','AK','BKL','DF','VASC','SCC']].values\n    if test==False:\n        return paths,labels\n    else:\n        return paths\n    \ndef data_augment(image, label=None, seed=SEED):\n    image = tf.image.random_flip_left_right(image, seed=seed)\n    image = tf.image.random_flip_up_down(image, seed=seed)\n           \n    if label is None:\n        return image\n    else:\n        return image, label","2d20fcff":"ROT_ = 180.0\nSHR_ = 2.0\nHZOOM_ = 8.0\nWZOOM_ = 8.0\nHSHIFT_ = 8.0\nWSHIFT_ = 8.0","0417c6aa":"def get_mat(rotation, shear, height_zoom, width_zoom, height_shift, width_shift):\n    rotation = math.pi * rotation \/ 180.\n    shear = math.pi * shear \/ 180.\n\n    c1 = tf.math.cos(rotation)\n    s1 = tf.math.sin(rotation)\n    one = tf.constant([1],dtype='float32')\n    zero = tf.constant([0],dtype='float32')\n    rotation_matrix = tf.reshape( tf.concat([c1,s1,zero, -s1,c1,zero, zero,zero,one],axis=0),[3,3] )\n\n    c2 = tf.math.cos(shear)\n    s2 = tf.math.sin(shear)\n    \n    shear_matrix = tf.reshape( tf.concat([one,s2,zero, zero,c2,zero, zero,zero,one],axis=0),[3,3] )    \n    zoom_matrix = tf.reshape( tf.concat([one\/height_zoom,zero,zero, zero,one\/width_zoom,zero, zero,zero,one],axis=0),[3,3] )\n    shift_matrix = tf.reshape( tf.concat([one,zero,height_shift, zero,one,width_shift, zero,zero,one],axis=0),[3,3] )\n    \n    return K.dot(K.dot(rotation_matrix, shear_matrix), K.dot(zoom_matrix, shift_matrix))\n","d665c280":"def transform(image,label=None):\n    DIM = img_size\n    XDIM = DIM%2 \n    \n    rot = 15. * tf.random.normal([1],dtype='float32')\n    shr = 5. * tf.random.normal([1],dtype='float32') \n    h_zoom = 1.0 + tf.random.normal([1],dtype='float32')\/10.\n    w_zoom = 1.0 + tf.random.normal([1],dtype='float32')\/10.\n    h_shift = 8. * tf.random.normal([1],dtype='float32') \n    w_shift = 8. * tf.random.normal([1],dtype='float32') \n  \n    m = get_mat(rot,shr,h_zoom,w_zoom,h_shift,w_shift) \n\n    x = tf.repeat( tf.range(DIM\/\/2,-DIM\/\/2,-1), DIM )\n    y = tf.tile( tf.range(-DIM\/\/2,DIM\/\/2),[DIM] )\n    z = tf.ones([DIM*DIM],dtype='int32')\n    idx = tf.stack( [x,y,z] )\n    \n    idx2 = K.dot(m,tf.cast(idx,dtype='float32'))\n    idx2 = K.cast(idx2,dtype='int32')\n    idx2 = K.clip(idx2,-DIM\/\/2+XDIM+1,DIM\/\/2)\n              \n    idx3 = tf.stack( [DIM\/\/2-idx2[0,], DIM\/\/2-1+idx2[1,]] )\n    d = tf.gather_nd(image,tf.transpose(idx3))\n    \n    if label is None:\n        return tf.reshape(d,[DIM,DIM,3])\n    else:\n        return tf.reshape(d,[DIM,DIM,3]),label\n","a0b24119":"train_dataset = (tf.data.Dataset\n    .from_tensor_slices(preprocess(train))\n    .map(decode_image, num_parallel_calls=AUTO)\n    .map(data_augment, num_parallel_calls=AUTO)\n    .map(transform,num_parallel_calls=AUTO)\n    .shuffle(SEED)\n    .batch(BATCH_SIZE)\n    .repeat()\n    .prefetch(AUTO))","4096dd43":"test_dataset= (tf.data.Dataset\n    .from_tensor_slices(preprocess(valid))\n    .map(decode_image, num_parallel_calls=AUTO)\n    .batch(BATCH_SIZE)\n    .cache()\n    .prefetch(AUTO))","510dbda6":"LR_START = 0.00001\nLR_MAX = 0.00005 * strategy.num_replicas_in_sync\nLR_MIN = 0.00001\nLR_RAMPUP_EPOCHS = 5\nLR_SUSTAIN_EPOCHS = 0\nLR_EXP_DECAY = .8\n\ndef lrfn(epoch):\n    if epoch < LR_RAMPUP_EPOCHS:\n        lr = (LR_MAX - LR_START) \/ LR_RAMPUP_EPOCHS * epoch + LR_START\n    elif epoch < LR_RAMPUP_EPOCHS + LR_SUSTAIN_EPOCHS:\n        lr = LR_MAX\n    else:\n        lr = (LR_MAX - LR_MIN) * LR_EXP_DECAY**(epoch - LR_RAMPUP_EPOCHS - LR_SUSTAIN_EPOCHS) + LR_MIN\n    return lr\n    \nlr_callback = tf.keras.callbacks.LearningRateScheduler(lrfn, verbose=True)\n\nrng = [i for i in range(EPOCHS)]\ny = [lrfn(x) for x in rng]\nplt.plot(rng, y)\nprint(\"Learning rate schedule: {:.3g} to {:.3g} to {:.3g}\".format(y[0], max(y), y[-1]))","c6574e93":"def categorical_focal_loss(gamma=2., alpha=.25):\n    def categorical_focal_loss_fixed(y_true, y_pred):\n        y_pred \/= K.sum(y_pred, axis=-1, keepdims=True)\n        epsilon = K.epsilon()\n        y_pred = K.clip(y_pred, epsilon, 1. - epsilon)\n        cross_entropy = -y_true * K.log(y_pred)\n        loss = alpha * K.pow(1 - y_pred, gamma) * cross_entropy\n        return K.sum(loss, axis=1)\n    return categorical_focal_loss_fixed","47c9af0e":"with strategy.scope():\n    enet = efn.EfficientNetB7(input_shape=(img_size, img_size, 3),weights='noisy-student',include_top=False)","e716164b":"with strategy.scope():\n    for layer in  enet.layers:\n        layer.trainable = False","103a384e":"with strategy.scope():\n    ef7 =tf.keras.Sequential()\n    ef7.add(enet)\n    ef7.add(tf.keras.layers.GlobalAveragePooling2D())\n    ef7.add(tf.keras.layers.Flatten())\n\n    ef7.add(tf.keras.layers.Dense(4096,activation='relu'))\n    ef7.add(tf.keras.layers.BatchNormalization())\n    ef7.add(tf.keras.layers.LeakyReLU())\n    ef7.add(tf.keras.layers.Dropout(0.55))\n    \n    ef7.add(tf.keras.layers.Dense(2048,activation='relu'))\n    ef7.add(tf.keras.layers.BatchNormalization())\n    ef7.add(tf.keras.layers.LeakyReLU())\n    ef7.add(tf.keras.layers.Dropout(0.45))\n\n    ef7.add(tf.keras.layers.Dense(1024,activation='relu'))\n    ef7.add(tf.keras.layers.BatchNormalization())\n    ef7.add(tf.keras.layers.LeakyReLU())\n    ef7.add(tf.keras.layers.Dropout(0.35))\n    \n    \n    ef7.add(tf.keras.layers.Dense(8,activation='softmax'))\n    ef7.compile(\n                optimizer=tf.optimizers.Adam(lr=0.0001),\n                loss=categorical_focal_loss(gamma=2., alpha=.25),\n                metrics=['categorical_accuracy',\n                        tf.keras.metrics.Recall(),\n                        tf.keras.metrics.Precision(),   \n                        tf.keras.metrics.AUC(),\n                        tfa.metrics.F1Score(num_classes=8, average=\"macro\")\n                       ])\n","af71891d":"h7=ef7.fit(\n    train_dataset,\n    steps_per_epoch=len(train) \/\/ (BATCH_SIZE),\n    callbacks=[lr_callback],\n    verbose=1,\n    epochs=EPOCHS)","e36b20be":"import seaborn as sns\nsns.set()\nfig = plt.figure(0, (12, 4))\n\nax = plt.subplot(1, 2, 1)\nsns.lineplot(h7.epoch,h7.history['categorical_accuracy'], label = 'train')\nplt.title('Accuracy')\nplt.tight_layout()\n\nax = plt.subplot(1, 2, 2)\nsns.lineplot(h7.epoch,h7.history['loss'], label = 'train')\nplt.title('Loss')\nplt.tight_layout()\nplt.show()","2cd0752b":"ef7.evaluate(test_dataset)","52b96428":"from sklearn.metrics import confusion_matrix\nclasses=['MEL','NV','BCC','AK','BKL','DF','VASC','SCC']\nY_pred = ef7.predict(test_dataset)\ntrue_classes = valid.loc[:, ['MEL','NV','BCC','AK','BKL','DF','VASC','SCC']].values\nprint('Confusion Matrix')\ncm=confusion_matrix(true_classes.argmax(axis=1),Y_pred.argmax(axis=1))\ncm","24ae9d58":"import seaborn as sns\nsns.set_style(\"darkgrid\")\nimport itertools\ndef plot_confusion_matrix(cm, classes, normalize=False, cmap=plt.cm.Oranges):\n    plt.figure(figsize=(16,16))\n    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n    #plt.title(title)\n    #plt.colorbar()\n    tick_marks = np.arange(len(classes))\n    plt.xticks(tick_marks, classes, rotation=45)\n    plt.yticks(tick_marks, classes)\n    if normalize:\n        cm = cm.astype('float') \/ cm.sum(axis=1)[:, np.newaxis]\n        cm = np.around(cm, decimals=2)\n        cm[np.isnan(cm)] = 0.0\n        print(\"Normalized confusion matrix\")\n    else:\n        print('Confusion matrix, without normalization')\n    thresh = cm.max() \/ 2.\n    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n        plt.text(j, i, cm[i, j],\n                 horizontalalignment=\"center\",\n                 color=\"white\" if cm[i, j] > thresh else \"black\")\n    plt.tight_layout()\n    plt.ylabel('True label')\n    plt.xlabel('Predicted label')","1b00c4cc":"plot_confusion_matrix(cm,classes)","c6137eae":"def calculate_sensitivity_specificity(y_test, y_pred_test):\n    actual_pos = y_test == 1\n    actual_neg = y_test == 0\n    \n    true_pos = (y_pred_test == 1) & (actual_pos)\n    false_pos = (y_pred_test == 1) & (actual_neg)\n    true_neg = (y_pred_test == 0) & (actual_neg)\n    false_neg = (y_pred_test == 0) & (actual_pos)\n    \n    # Calculate sensitivity and specificity\n    sensitivity = np.sum(true_pos) \/ np.sum(actual_pos)\n    specificity = np.sum(true_neg) \/ np.sum(actual_neg)\n    \n    return sensitivity, specificity","cb4d84db":"sensitivity, specificity= calculate_sensitivity_specificity(true_classes.argmax(axis=1),Y_pred.argmax(axis=1))\nprint ('Sensitivity:', sensitivity)\nprint ('Specificity:', specificity)","5634de71":"with strategy.scope():\n    DenseNet201 = tf.keras.applications.DenseNet201(input_shape=(512, 512, 3), weights='imagenet', include_top=False)","0340161a":"with strategy.scope():\n    for layer in  DenseNet201.layers:\n        layer.trainable = False","249b5743":"with strategy.scope():\n    model_D201=tf.keras.Sequential()\n    model_D201.add(DenseNet201)\n    model_D201.add(tf.keras.layers.GlobalAveragePooling2D())\n    model_D201.add(tf.keras.layers.Flatten())\n\n    model_D201.add(tf.keras.layers.Dense(4096,activation='relu'))\n    model_D201.add(tf.keras.layers.BatchNormalization())\n    model_D201.add(tf.keras.layers.LeakyReLU())\n    model_D201.add(tf.keras.layers.Dropout(0.55))\n    \n    model_D201.add(tf.keras.layers.Dense(2048,activation='relu'))\n    model_D201.add(tf.keras.layers.BatchNormalization())\n    model_D201.add(tf.keras.layers.LeakyReLU())\n    model_D201.add(tf.keras.layers.Dropout(0.45))\n    \n    model_D201.add(tf.keras.layers.Dense(1024,activation='relu'))\n    model_D201.add(tf.keras.layers.BatchNormalization())\n    model_D201.add(tf.keras.layers.LeakyReLU())\n    model_D201.add(tf.keras.layers.Dropout(0.35))\n    \n    \n    model_D201.add(tf.keras.layers.Dense(8,activation='softmax'))\n    model_D201.compile(\n                optimizer=tf.optimizers.Adam(lr=0.0001),\n                loss=categorical_focal_loss(gamma=2., alpha=.25),\n                metrics=['categorical_accuracy',\n                        tf.keras.metrics.Recall(),\n                        tf.keras.metrics.Precision(),   \n                        tf.keras.metrics.AUC(),\n                        tfa.metrics.F1Score(num_classes=8, average=\"macro\")\n                       ])\n","2f8feed1":"D_201=model_D201.fit(\n    train_dataset, \n    steps_per_epoch=train_labels.shape[0] \/\/ BATCH_SIZE,\n    callbacks=[lr_callback],\n    epochs=EPOCHS)\n    #validation_data=test_dataset)","dd4eb3cc":"import seaborn as sns\nsns.set()\nfig = plt.figure(0, (12, 4))\n\nax = plt.subplot(1, 2, 1)\nsns.lineplot(D_201.epoch,D_201.history['categorical_accuracy'], label = 'train')\nplt.title('Accuracy')\nplt.tight_layout()\n\nax = plt.subplot(1, 2, 2)\nsns.lineplot(D_201.epoch,D_201.history['loss'], label = 'train')\nplt.title('Loss')\nplt.tight_layout()\nplt.show()","53634e03":"model_D201.evaluate(test_dataset)","8d8c3acf":"from sklearn.metrics import confusion_matrix\nclasses=['MEL','NV','BCC','AK','BKL','DF','VASC','SCC']\nY_pred = ef7.predict(test_dataset)\ntrue_classes = valid.loc[:, ['MEL','NV','BCC','AK','BKL','DF','VASC','SCC']].values\nprint('Confusion Matrix')\ncm=confusion_matrix(true_classes.argmax(axis=1),Y_pred.argmax(axis=1))\ncm","244c1806":"plot_confusion_matrix(cm,classes)","7aa8aac3":"sensitivity, specificity= calculate_sensitivity_specificity(true_classes.argmax(axis=1),Y_pred.argmax(axis=1))\nprint ('Sensitivity:', sensitivity)\nprint ('Specificity:', specificity)","05dcc19a":"## 3 Hyperparam\u00e8tre","85d3c29e":"# 10 Entra\u00eenement","3a5e99b3":"## 11. Test et \u00e9valuation","d2766e2a":"## 2. Pr\u00e9paration de la base de donn\u00e9es\n\n### 2.1 importer les biblioth\u00e8ques n\u00e9cessaires","02b0599d":"<p>\n<h1><center> EfficientNetB7 <\/center><\/h1>\n<center><img src='https:\/\/1.bp.blogspot.com\/-DjZT_TLYZok\/XO3BYqpxCJI\/AAAAAAAAEKM\/BvV53klXaTUuQHCkOXZZGywRMdU9v9T_wCLcBGAs\/s1600\/image2.png' height=350><\/center>\n<p>","eca7bda9":"# 1. Entra\u00eenement","9f3da5e1":"## 5. Augmentation","8c07dce6":"## 2.Affichage des courbes (acc,loss)","57498e7a":"## 1.Objectifs\n\n* L\u2019objectif ici est de d\u00e9velopper un r\u00e9seau CNN en se basant sur des architecture diff\u00e9rentes (EfficientNetB7,DenseNet201,EfficientNetB7+Att,DenseNet201+Att) pour la classification de deux maladie (Normal VS Cataract VS Glaucoma VS Myopia)","36b8e475":"### 2.3 Normal VS Cataract VS Glaucoma VS Myopia","d9de1361":"## 12. Matrice de confusion","b28f7ff2":"## 2.2 Configuration de tpu","a882928f":"<p>\n<h1><center> DenseNet201 <\/center><\/h1>\n<center><img src='https:\/\/oi.readthedocs.io\/en\/latest\/_images\/cnn_vs_resnet_vs_densenet.png' height=20><\/center>\n<p>\n","e7c4fe26":"## 2.4 Diviser notre dataset en 80% l'entra\u00eenement et 20% pour le test","0d9e08f1":"<center><img src='https:\/\/www.sante-sur-le-net.com\/wp-content\/uploads\/2017\/06\/conjonctivite-schema.jpg' height=350><\/center>\n<p>\n<h1><center> Normal VS Cataract VS Glaucoma VS Myopia <\/center><\/h1>","c005860a":"## 11. Affichage des courbes (acc,loss)","b8d26baf":"## 8. Fonction du taux d'apprentissage","8359c0a4":"## 6.Cr\u00e9ation d'un g\u00e9n\u00e9rateur pour l'ensemble de donn\u00e9es d'entra\u00eenement ","2ad4af83":"## 4. Matrice de confusion","45f627c9":"## 3. Test et \u00e9valuation","8e4af87c":"## 7. Cr\u00e9ation d'un g\u00e9n\u00e9rateur pour l'ensemble de donn\u00e9es de test","3e0c1f40":"## 13. specificity et sensitivity","24d2d49f":"## 9. Fonction de perte","6f3e1dd1":"## 4. Pr\u00e9traitement des donn\u00e9es"}}