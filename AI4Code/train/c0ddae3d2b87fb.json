{"cell_type":{"7ab862aa":"code","8d3d5dab":"code","125efcf3":"code","fead826f":"code","d9480181":"code","b6371e3f":"code","9fc78ab5":"code","f51a773c":"code","a9947dca":"code","cd778b83":"code","974aa58b":"code","f276213e":"code","fa724d7d":"code","0e3a6bf9":"code","76aca17f":"code","342218bb":"code","b6562466":"code","4296a518":"code","3e507070":"code","acfd593b":"code","028e3200":"code","2c35c0df":"code","0556f541":"code","d10904d4":"code","1304f2a1":"code","7d2ef213":"code","691297e1":"code","e1d8aa0f":"code","57e05b12":"code","5a1140a3":"code","93aec79d":"code","5ec18062":"code","e8b1e55a":"code","b3e08ff9":"code","61f7057e":"code","4266f1e8":"code","ca3f1442":"code","67f1e25d":"code","ace25d4a":"code","708f9848":"code","007deba9":"code","cd77c729":"code","e6df6487":"code","9325f5f8":"code","2a035ad7":"code","34ab3019":"code","05d9de45":"code","beaae655":"code","3ed3fe1d":"markdown","4ec401c5":"markdown","b5a4f2e4":"markdown","18abc609":"markdown","04e445d0":"markdown","340f213e":"markdown"},"source":{"7ab862aa":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport seaborn as sns \nimport matplotlib.pyplot as plt\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","8d3d5dab":"train = pd.read_csv(\"\/kaggle\/input\/titanic\/train.csv\")\ntest = pd.read_csv(\"\/kaggle\/input\/titanic\/test.csv\")","125efcf3":"train[\"train_test\"] = 1\ntest['train_test'] = 0\ntest['Survived'] = np.NaN\nall_data = pd.concat([train,test])\n\n%matplotlib inline\nall_data.columns","fead826f":"#quick look at our data types & null counts \ntrain.info()","d9480181":"train.describe()","b6371e3f":"train.describe().columns","9fc78ab5":"# look at numeric and categorical values separately \ndf_num = train[['Age','SibSp','Parch','Fare']]\ndf_cat = train[['Survived','Pclass','Sex','Ticket','Cabin','Embarked']]","f51a773c":"#distributions for all numeric variables \nfor i in df_num.columns:\n    plt.hist(df_num[i])\n    plt.title(i)\n    plt.show()","a9947dca":"print(df_num.corr())\nsns.heatmap(df_num.corr())","cd778b83":"# compare survival rate across Age, SibSp, Parch, and Fare \npd.pivot_table(train, index = 'Survived', values = ['Age','SibSp','Parch','Fare'])","974aa58b":"#distributions for all categorical variables \nfor i in df_cat.columns:\n    sns.barplot(df_cat[i].value_counts().index, df_cat[i].value_counts()).set_title(i)\n    plt.show()","f276213e":"# Comparing survival and each of these categorical variables \nprint(pd.pivot_table(train, index = 'Survived', columns = 'Pclass', values = 'Ticket' ,aggfunc ='count'))\nprint()\nprint(pd.pivot_table(train, index = 'Survived', columns = 'Sex', values = 'Ticket' ,aggfunc ='count'))\nprint()\nprint(pd.pivot_table(train, index = 'Survived', columns = 'Embarked', values = 'Ticket' ,aggfunc ='count'))","fa724d7d":"df_cat.Cabin\ntrain['cabin_multiple'] = train.Cabin.apply(lambda x: 0 if pd.isna(x) else len(x.split(' ')))\n# after looking at this, we may want to look at cabin by letter or by number. Let's create some categories for this \n# letters \n# multiple letters \ntrain['cabin_multiple'].value_counts()","0e3a6bf9":"pd.pivot_table(train, index = 'Survived', columns = 'cabin_multiple', values = 'Ticket' ,aggfunc ='count')","76aca17f":"#creates categories based on the cabin letter (n stands for null)\n#in this case we will treat null values like it's own category\n\ntrain['cabin_adv'] = train.Cabin.apply(lambda x: str(x)[0])","342218bb":"#comparing surivial rate by cabin\nprint(train.cabin_adv.value_counts())\npd.pivot_table(train,index='Survived',columns='cabin_adv', values = 'Name', aggfunc='count')","b6562466":"#understand ticket values better \n#numeric vs non numeric \ntrain['numeric_ticket'] = train.Ticket.apply(lambda x: 1 if x.isnumeric() else 0)\ntrain['ticket_letters'] = train.Ticket.apply(lambda x: ''.join(x.split(' ')[:-1]).replace('.','').replace('\/','').lower() if len(x.split(' ')[:-1]) >0 else 0)","4296a518":"train['numeric_ticket'].value_counts()","3e507070":"#lets us view all rows in dataframe through scrolling. (This is for convenience)\npd.set_option(\"max_rows\", None)\ntrain['ticket_letters'].value_counts()","acfd593b":"#difference in numeric vs non-numeric tickets in survival rate \npd.pivot_table(train,index='Survived',columns='numeric_ticket', values = 'Ticket', aggfunc='count')","028e3200":"#survival rate across different tyicket types \npd.pivot_table(train,index='Survived',columns='ticket_letters', values = 'Ticket', aggfunc='count')","2c35c0df":"#feature engineering on person's title \ntrain.Name.head(50)\ntrain['name_title'] = train.Name.apply(lambda x: x.split(',')[1].split('.')[0].strip())\n#mr., ms., master. etc","0556f541":"train[\"name_title\"].value_counts()","d10904d4":"#create all categorical variables that we did above for both train and test sets \nall_data['cabin_multiple'] = all_data.Cabin.apply(lambda x: 0 if pd.isna(x) else len(x.split(' ')))\nall_data['cabin_adv'] = all_data.Cabin.apply(lambda x: str(x)[0])\nall_data['numeric_ticket'] = all_data.Ticket.apply(lambda x: 1 if x.isnumeric() else 0)\nall_data['ticket_letters'] = all_data.Ticket.apply(lambda x: ''.join(x.split(' ')[:-1]).replace('.','').replace('\/','').lower() if len(x.split(' ')[:-1]) >0 else 0)\nall_data['name_title'] = all_data.Name.apply(lambda x: x.split(',')[1].split('.')[0].strip())\n\n#impute nulls for continuous data \n#all_data.Age = all_data.Age.fillna(train.Age.mean())\nall_data.Age = all_data.Age.fillna(train.Age.median())\n#all_data.Fare = all_data.Fare.fillna(train.Fare.mean())\nall_data.Fare = all_data.Fare.fillna(train.Fare.median())\n\n#drop null 'embarked' rows. Only 2 instances of this in train and 0 in test \nall_data.dropna(subset=['Embarked'],inplace = True)\n\n#tried log norm of sibsp (not used)\nall_data['norm_sibsp'] = np.log(all_data.SibSp+1)\nall_data['norm_sibsp'].hist()\n\n# log norm of fare (used)\nall_data['norm_fare'] = np.log(all_data.Fare+1)\nall_data['norm_fare'].hist()\n\n# converted fare to category for pd.get_dummies()\nall_data.Pclass = all_data.Pclass.astype(str)\n\n#created dummy variables from categories (also can use OneHotEncoder)\nall_dummies = pd.get_dummies(all_data[['Pclass','Sex','Age','SibSp','Parch','norm_fare','Embarked','cabin_adv','cabin_multiple','numeric_ticket','name_title','train_test']])\n\n#Split to train test again\nX_train = all_dummies[all_dummies.train_test == 1].drop(['train_test'], axis =1)\nX_test = all_dummies[all_dummies.train_test == 0].drop(['train_test'], axis =1)\n\n\ny_train = all_data[all_data.train_test==1].Survived\ny_train.shape","1304f2a1":"# Scale data \nfrom sklearn.preprocessing import StandardScaler\nscale = StandardScaler()\nall_dummies_scaled = all_dummies.copy()\nall_dummies_scaled[['Age','SibSp','Parch','norm_fare']]= scale.fit_transform(all_dummies_scaled[['Age','SibSp','Parch','norm_fare']])\nall_dummies_scaled\n\nX_train_scaled = all_dummies_scaled[all_dummies_scaled.train_test == 1].drop(['train_test'], axis =1)\nX_test_scaled = all_dummies_scaled[all_dummies_scaled.train_test == 0].drop(['train_test'], axis =1)\n\ny_train = all_data[all_data.train_test==1].Survived","7d2ef213":"from sklearn.model_selection import cross_val_score\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.svm import SVC\nfrom xgboost import XGBClassifier\nfrom catboost import CatBoostClassifier\nfrom sklearn.model_selection import train_test_split","691297e1":"svc = SVC(probability = True)\ncv = cross_val_score(svc,X_train_scaled,y_train,cv=5)\nprint(cv)\nprint(cv.mean())","e1d8aa0f":"rf = RandomForestClassifier(random_state = 1)\ncv = cross_val_score(rf,X_train_scaled,y_train,cv=5)\nprint(cv)\nprint(cv.mean())","57e05b12":"xgb = XGBClassifier(random_state =1)\ncv = cross_val_score(xgb,X_train_scaled,y_train,cv=5)\nprint(cv)\nprint(cv.mean())","5a1140a3":"X_train, X_valid, y_train1, y_valid = train_test_split(X_train_scaled, y_train, test_size=0.25, random_state=30)","93aec79d":"params = {'loss_function':'Logloss', \n          'eval_metric':'AUC', \n          'verbose': 200, \n          'random_seed': 30\n         }\ncbc_1 = CatBoostClassifier(**params)\ncbc_1.fit(X_train, y_train1, eval_set=(X_valid, y_valid), use_best_model=True, plot=True);","5ec18062":"cv = cross_val_score(cbc_1,X_train_scaled,y_train,cv=5)\nprint(cv.mean())","e8b1e55a":"from sklearn.ensemble import VotingClassifier\nvoting_clf = VotingClassifier(estimators = [('rf',rf),('svc',svc),('xgb',xgb), ('cbc_1', cbc_1)], voting = 'soft') ","b3e08ff9":"cv = cross_val_score(voting_clf,X_train_scaled,y_train,cv=5)\nprint(cv)\nprint(cv.mean())","61f7057e":"voting_clf.fit(X_train_scaled,y_train)\ny_hat_base_vc = voting_clf.predict(X_test_scaled).astype(int)\nbasic_submission = {'PassengerId': test.PassengerId, 'Survived': y_hat_base_vc}\nbase_submission = pd.DataFrame(data=basic_submission)\nbase_submission.to_csv('\/kaggle\/working\/base_submission.csv', index=False)","4266f1e8":"from sklearn.model_selection import GridSearchCV \nfrom sklearn.model_selection import RandomizedSearchCV ","ca3f1442":"#simple performance reporting function\ndef clf_performance(classifier, model_name):\n    print(model_name)\n    print('Best Score: ' + str(classifier.best_score_))\n    print('Best Parameters: ' + str(classifier.best_params_))","67f1e25d":"svc = SVC(probability = True)\nparam_grid = tuned_parameters = [{'kernel': ['rbf'], 'gamma': [.1,.5,1,2,5,10],\n                                  'C': [.1, 1, 10, 100, 1000]},\n                                 {'kernel': ['linear'], 'C': [.1, 1, 10, 100, 1000]},\n                                 {'kernel': ['poly'], 'degree' : [2,3,4,5], 'C': [.1, 1, 10, 100, 1000]}]\nclf_svc = GridSearchCV(svc, param_grid = param_grid, cv = 5, verbose = True, n_jobs = -1)\nbest_clf_svc = clf_svc.fit(X_train_scaled,y_train)\nclf_performance(best_clf_svc,'SVC')\n","ace25d4a":"cbc_2 = CatBoostClassifier(random_state = 1)\n\nparam_grid = {\n    'n_estimators': [20, 50, 100, 250, 500,1000],\n    'colsample_bytree': [0.2, 0.5, 0.7, 0.8, 1],\n    'max_depth': [2, 5, 10, 15, 20, 25, None],\n    'learning_rate':[.01,0.1,0.2,0.3,0.5, 0.7, 0.9],\n    'min_child_weight':[0,.01,0.1,1,10,100],\n    \n}\n\n#clf_xgb = GridSearchCV(xgb, param_grid = param_grid, cv = 5, verbose = True, n_jobs = -1)\n#best_clf_xgb = clf_xgb.fit(X_train_scaled,y_train)\n#clf_performance(best_clf_xgb,'XGB')\nclf_cbc2_rnd = RandomizedSearchCV(cbc_2, param_distributions = param_grid, n_iter = 1000, cv = 5, verbose = True, n_jobs = -1)\nbest_clf_cbc2_rnd = clf_cbc2_rnd.fit(X_train_scaled,y_train)\nclf_performance(best_clf_cbc2_rnd,'CBC')","708f9848":"y_hat_cbc = best_clf_cbc2_rnd.best_estimator_.predict(X_test_scaled).astype(int)\ncbc_submission = {'PassengerId': test.PassengerId, 'Survived': y_hat_cbc}\nsubmission_cbc = pd.DataFrame(data=cbc_submission)\nsubmission_cbc.to_csv('cbc_submission.csv', index=False)","007deba9":"xgb = XGBClassifier(random_state = 1)\n\nparam_grid = {\n    'n_estimators': [20, 50, 100, 250, 500,1000],\n    'colsample_bytree': [0.2, 0.5, 0.7, 0.8, 1],\n    'max_depth': [2, 5, 10, 15, 20, 25, None],\n    'reg_alpha': [0, 0.5, 1],\n    'reg_lambda': [1, 1.5, 2],\n    'subsample': [0.5,0.6,0.7, 0.8, 0.9],\n    'learning_rate':[.01,0.1,0.2,0.3,0.5, 0.7, 0.9],\n    'gamma':[0,.01,.1,1,10,100],\n    'min_child_weight':[0,.01,0.1,1,10,100],\n    'sampling_method': ['uniform', 'gradient_based']\n}\n\n#clf_xgb = GridSearchCV(xgb, param_grid = param_grid, cv = 5, verbose = True, n_jobs = -1)\n#best_clf_xgb = clf_xgb.fit(X_train_scaled,y_train)\n#clf_performance(best_clf_xgb,'XGB')\nclf_xgb_rnd = RandomizedSearchCV(xgb, param_distributions = param_grid, n_iter = 1000, cv = 5, verbose = True, n_jobs = -1)\nbest_clf_xgb_rnd = clf_xgb_rnd.fit(X_train_scaled,y_train)\nclf_performance(best_clf_xgb_rnd,'XGB')","cd77c729":"rf = RandomForestClassifier(random_state = 1)\nparam_grid =  {'n_estimators': [400,450,500,550],\n               'criterion':['gini','entropy'],\n                                  'bootstrap': [True],\n                                  'max_depth': [15, 20, 25],\n                                  'max_features': ['auto','sqrt', 10],\n                                  'min_samples_leaf': [2,3],\n                                  'min_samples_split': [2,3]}\n                                  \nclf_rf = GridSearchCV(rf, param_grid = param_grid, cv = 5, verbose = True, n_jobs = -1)\nbest_clf_rf = clf_rf.fit(X_train_scaled,y_train)\nclf_performance(best_clf_rf,'Random Forest')","e6df6487":"best_svc = best_clf_svc.best_estimator_\nbest_rf = best_clf_rf.best_estimator_\nbest_xgb = best_clf_xgb.best_estimator_\nbest_cbc = best_clf_cbc2_rnd.best_estimator_\n\nvoting_clf_hard = VotingClassifier(estimators = [('xgb',best_xgb),('rf',best_rf),('svc',best_svc)], voting = 'hard') \nvoting_clf_soft = VotingClassifier(estimators = [('xgb',best_xgb),('rf',best_rf),('svc',best_svc)], voting = 'soft') \nvoting_clf_cbc = VotingClassifier(estimators = [('xgb',best_xgb),('rf',best_rf),('svc',best_svc), ('cbc', best_cbc)], voting = 'soft')\n\nprint('voting_clf_hard :',cross_val_score(voting_clf_hard,X_train_scaled,y_train,cv=5))\nprint('voting_clf_hard mean :',cross_val_score(voting_clf_hard,X_train_scaled,y_train,cv=5).mean())\n\nprint('voting_clf_soft :',cross_val_score(voting_clf_soft,X_train_scaled,y_train,cv=5))\nprint('voting_clf_soft mean :',cross_val_score(voting_clf_soft,X_train_scaled,y_train,cv=5).mean())\n\nprint('voting_clf_cbc :',cross_val_score(voting_clf_cbc,X_train_scaled,y_train,cv=5))\nprint('voting_clf_cbc mean :',cross_val_score(voting_clf_cbc,X_train_scaled,y_train,cv=5).mean())","9325f5f8":"#Make Predictions \nvoting_clf_hard.fit(X_train_scaled, y_train)\nvoting_clf_soft.fit(X_train_scaled, y_train)\nvoting_clf_cbc.fit(X_train_scaled, y_train)\n\nbest_xgb.fit(X_train_scaled, y_train)\ny_hat_vc_hard = voting_clf_hard.predict(X_test_scaled).astype(int)\ny_hat_xgb = best_xgb.predict(X_test_scaled).astype(int)\ny_hat_vc_soft =  voting_clf_soft.predict(X_test_scaled).astype(int)\ny_hat_vc_cbc = voting_clf_cbc.predict(X_test_scaled).astype(int)","2a035ad7":"#convert output to dataframe \nfinal_data = {'PassengerId': test.PassengerId, 'Survived': y_hat_xgb}\nsubmission = pd.DataFrame(data=final_data)\n\nfinal_data_2 = {'PassengerId': test.PassengerId, 'Survived': y_hat_vc_hard}\nsubmission_2 = pd.DataFrame(data=final_data_2)\n\nfinal_data_3 = {'PassengerId': test.PassengerId, 'Survived': y_hat_vc_soft}\nsubmission_3 = pd.DataFrame(data=final_data_3)\n\nfinal_data_5 = {'PassengerId': test.PassengerId, 'Survived': y_hat_vc_cbc}\nsubmission_5 = pd.DataFrame(data=final_data_5)\n\nfinal_data_comp = {'PassengerId': test.PassengerId, 'Survived_vc_hard': y_hat_vc_hard, 'Survived_xgb': y_hat_xgb, 'Survived_vc_soft' : y_hat_vc_soft, 'Survived_vc_xgb' : y_hat_vc_cbc}\ncomparison = pd.DataFrame(data=final_data_comp)","34ab3019":"#track differences between outputs \ncomparison['difference_xgb_vc_hard'] = comparison.apply(lambda x: 1 if x.Survived_vc_hard != x.Survived_xgb else 0, axis =1)\ncomparison['difference_soft_hard'] = comparison.apply(lambda x: 1 if x.Survived_vc_hard != x.Survived_vc_soft else 0, axis =1)\ncomparison['difference_soft_xgb'] = comparison.apply(lambda x: 1 if x.Survived_vc_soft != x.Survived_xgb else 0, axis=1)","05d9de45":"comparison.difference_soft_hard.value_counts()","beaae655":"#prepare submission files \nsubmission.to_csv('submission_xgb.csv', index =False)\nsubmission_2.to_csv('submission_vc_hard.csv',index=False)\nsubmission_3.to_csv('submission_vc_soft.csv', index=False)\nsubmission_5.to_csv('submission_vc_cbc.csv', index=False)","3ed3fe1d":"Tuning models","4ec401c5":"Additional Ensembling approach","b5a4f2e4":"Feature Engineering","18abc609":"Data Exploration","04e445d0":"Training a catboost model","340f213e":"Data Processing for Model"}}