{"cell_type":{"ed73851a":"code","e981ffdb":"code","992de9be":"code","bcc9f0e9":"code","6aed50b5":"code","d826dc01":"code","1ea23cda":"code","89a807f5":"code","1e5a0e5b":"code","7ee4583c":"code","70a41c19":"code","2a420022":"code","25c9b5b5":"code","93e31b47":"code","ff1f3762":"code","9438cb82":"code","d7c7c3d5":"code","92f13cb5":"code","ed41a6e7":"code","7339e67c":"code","d60b2f59":"code","9d6b5c5c":"code","f8358e03":"code","06cdf35a":"code","e4e6621b":"code","9e29203d":"code","a81e896f":"code","7d65c1b7":"code","bb68dfbd":"code","c6ca4e91":"code","1a33b407":"code","8381a14e":"code","fc73b195":"code","6bffc3f0":"code","d321092a":"code","8a556bda":"code","7c1427dd":"code","16dc84ab":"code","2fa6fca5":"code","fe2976d8":"code","d275e4a6":"code","176456a9":"code","2cd9f0d0":"code","0813d905":"code","5d867fe8":"code","9749b9d6":"code","5f0f84ef":"code","0422b631":"code","864c3b58":"code","b5f3610a":"code","1bf9a343":"markdown","b7ed5857":"markdown","81cb6970":"markdown","0e79c822":"markdown","0dd78dc2":"markdown","75da0e6d":"markdown","9f00ddc1":"markdown"},"source":{"ed73851a":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","e981ffdb":"df = pd.read_csv('..\/input\/persian-artists-on-spotfiy\/Spotfiy_Persian_Artists.csv', encoding='utf8')\npd.set_option('display.max_columns', None)\ndf.head()","992de9be":"df.isnull().sum()","bcc9f0e9":"!pip install pycomp","6aed50b5":"# Importing libraries\nfrom pycomp.viz.insights import *\n# Explicit rate\n\nexplicit_colors = ['chartreuse', 'magenta']\nplot_donut_chart(df=df, col='explicit', colors=explicit_colors, figsize=(12, 12),\n                 title='Persian Artists by Explicity')","d826dc01":"# Countplot for ethnicity\nkey_name_colors = ['chartreuse', 'coral', 'magenta', 'dodgerblue', 'darkgreen', 'darkorchid']\n\nplot_countplot(df=df, col='key_name', palette=key_name_colors, \n               title='Persian Artists by key name')","1ea23cda":"# Data overview\ndata_overview(df=df)","89a807f5":"#Calculating the number of songs by each of the artists\nprint(df.groupby('artist_name').size())\npopular_artist=df.groupby('artist_name').size()\nprint(popular_artist)\nartist_list=df['artist_name'].values.tolist()","1e5a0e5b":"pd.set_option('precision', 3)\ndf.describe()","7ee4583c":"from scipy import stats\n\n#Finding out the skew for each attribute\nskew=df.skew()\nprint(skew)\n# Removing the skew by using the boxcox transformations\ntransform=np.asarray(df[['liveness']].values)\ndf_transform = stats.boxcox(transform)[0]\n#Plotting a histogram to show the difference \nplt.hist(df['liveness'],bins=10) #original data\nplt.show()\nplt.hist(df_transform,bins=10) #corrected skew data\nplt.show()","70a41c19":"pd.set_option('display.width', 100)\npd.set_option('precision', 3)\ncorrelation=df.corr(method='spearman')\nprint(correlation)","2a420022":"# heatmap of the correlation \nplt.figure(figsize=(10,10))\nplt.title('Correlation heatmap')\nsns.heatmap(correlation,annot=True,vmin=-1,vmax=1,cmap=\"GnBu_r\",center=1)","25c9b5b5":"fig = plt.figure(figsize = (15,7))\ndf.groupby('artist_name')['track_number'].agg(len).sort_values(ascending = False).plot(kind = 'bar')\nplt.xlabel('Artist Name', fontsize = 20)\nplt.ylabel('Count of songs', fontsize = 20)\nplt.title('Artist Name vs Count of songs', fontsize = 30)","93e31b47":"# Analysing the relationship between energy and loudness\nfig=plt.subplots(figsize=(10,10))\nsns.regplot(x='energy',y='loudness',data=df,color='purple')","ff1f3762":"fig=plt.subplots(figsize=(10,10))\nplt.title('Dependence between Energy and Popularity')\nsns.regplot(x='energy', y='popularity',\n            ci=None, data=df)\nsns.kdeplot(df.energy,df.popularity)","9438cb82":"df.plot(kind='box', subplots=True)\nplt.gcf().set_size_inches(30,30)\nplt.show()","d7c7c3d5":"import squarify as sq\n\nplt.figure(figsize=(14,8))\nsq.plot(sizes=df.key_name.value_counts(), label=df[\"key_name\"].unique(), alpha=.8 )\nplt.axis('off')\nplt.show()","92f13cb5":"#Pie charts \nlabels = df.artist_name.value_counts().index\nsizes = df.artist_name.value_counts().values\ncolors = ['red', 'yellowgreen', 'lightcoral', 'lightskyblue','cyan', 'green', 'black','yellow']\nplt.figure(figsize = (10,10))\nplt.pie(sizes, labels=labels, colors=colors)\nautopct=('%1.1f%%')\nplt.axis('equal')\nplt.show()","ed41a6e7":"# categorical features with missing values\ncategorical_nan = [feature for feature in df.columns if df[feature].isna().sum()>0 and df[feature].dtypes=='O']\nprint(categorical_nan)","7339e67c":"# replacing missing values in categorical features\nfor feature in categorical_nan:\n    df[feature] = df[feature].fillna('None')","d60b2f59":"df[categorical_nan].isna().sum()","9d6b5c5c":"# Lets first handle numerical features with nan value\nnumerical_nan = [feature for feature in df.columns if df[feature].isna().sum()>1 and df[feature].dtypes!='O']\nnumerical_nan","f8358e03":"df[numerical_nan].isna().sum()","06cdf35a":"## Replacing the numerical Missing Values\n\nfor feature in numerical_nan:\n    ## We will replace by using median since there are outliers\n    median_value=df[feature].median()\n    \n    df[feature].fillna(median_value,inplace=True)\n    \ndf[numerical_nan].isnull().sum()","e4e6621b":"from sklearn.preprocessing import LabelEncoder\n\n#fill in mean for floats\nfor c in df.columns:\n    if df[c].dtype=='float16' or  df[c].dtype=='float32' or  df[c].dtype=='float64':\n        df[c].fillna(df[c].mean())\n\n#fill in -999 for categoricals\ndf = df.fillna(-999)\n# Label Encoding\nfor f in df.columns:\n    if df[f].dtype=='object': \n        lbl = LabelEncoder()\n        lbl.fit(list(df[f].values))\n        df[f] = lbl.transform(list(df[f].values))\n        \nprint('Labelling done.')","9e29203d":"df = pd.get_dummies(df)","a81e896f":"import warnings\nwarnings.filterwarnings(\"ignore\")\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.preprocessing import MinMaxScaler,LabelEncoder\nfrom sklearn.model_selection import train_test_split,cross_val_score, KFold\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.naive_bayes import GaussianNB, MultinomialNB,BernoulliNB\nfrom sklearn.svm import LinearSVC, SVC\nfrom sklearn import metrics\nfrom sklearn.metrics import confusion_matrix, classification_report\n%matplotlib inline","7d65c1b7":"#Linear regression, first create test and train dataset\nx=df.loc[:,['energy','danceability','speechiness','loudness','acousticness']].values\ny=df.loc[:,'popularity'].values","bb68dfbd":"# Creating a test and training dataset\nX_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.30)","c6ca4e91":"# Linear regression\nregressor = LinearRegression()\nregressor.fit(X_train, y_train)\nprint(regressor.intercept_)\nprint(regressor.coef_)","1a33b407":"#Displaying the difference between the actual and the predicted\ny_pred = regressor.predict(X_test)\ndf_output = pd.DataFrame({'Actual': y_test, 'Predicted': y_pred})\nprint(df_output)","8381a14e":"#Checking the accuracy of Linear Regression\nprint('Mean Absolute Error:', metrics.mean_absolute_error(y_test, y_pred))\nprint('Mean Squared Error:', metrics.mean_squared_error(y_test, y_pred))\nprint('Root Mean Squared Error:', np.sqrt(metrics.mean_squared_error(y_test, y_pred)))","fc73b195":"plt.figure(figsize=(10,10))\nplt.plot(y_pred,y_test,color='black',linestyle='dashed',marker='*',markerfacecolor='red',markersize=10)\nplt.title('Error analysis')\nplt.xlabel('Predicted values')\nplt.ylabel('Test values')","6bffc3f0":"# Cross validation score\nx=df.loc[:,['energy','danceability']].values\ny=df.loc[:,'popularity'].values\nregressor=LinearRegression()\nmse=cross_val_score(regressor,X_train,y_train,scoring='neg_mean_squared_error',cv=5)\nmse_mean=np.mean(mse)\nprint(mse_mean)\ndiff=metrics.mean_squared_error(y_test, y_pred)-abs(mse_mean)\nprint(diff)","d321092a":"x=df.loc[:,['artist_name']].values\ny=df.loc[:,'popularity'].values","8a556bda":"# Label encoding of features\n#x.shape\n#encoder=LabelEncoder()\n#x = encoder.fit_transform(x)\n#x=pd.DataFrame(x)\n#x","7c1427dd":"# Label Encoding of target\n#Encoder_y=LabelEncoder()\n#Y = Encoder_y.fit_transform(y)\n#Y=pd.DataFrame(Y)\n#Y","16dc84ab":"x_train,x_test,y_train,y_test = train_test_split(x,y,test_size = 0.3,random_state = 1)\n\n#Scaling\nfrom sklearn.preprocessing import StandardScaler\nsc=StandardScaler()\nsc.fit(x_train)\nx_train=sc.transform(x_train)\nx_test=sc.transform(x_test)","2fa6fca5":"# KNN Classification\n# sorted(sklearn.neighbors.VALID_METRICS['brute'])\nknn = KNeighborsClassifier(n_neighbors = 17)\nknn.fit(x_train,y_train)\ny_pred=knn.predict(x_test)","fe2976d8":"error=[]\nfor i in range(1,30):\n    knn=KNeighborsClassifier(n_neighbors=i)\n    knn.fit(X_train,y_train)\n    pred_i=knn.predict(X_test)\n    error.append(np.mean(pred_i!=y_test))","d275e4a6":"plt.figure(figsize=(10,10))\nplt.plot(range(1,30),error,color='black',marker='o',markerfacecolor='cyan',markersize=10)\nplt.title('Error Rate K value')\nplt.xlabel('K Value')\nplt.ylabel('Mean error')","176456a9":"x=df.loc[:,['energy','loudness','danceability','liveness', 'acousticness']].values\ny=df.loc[:,'popularity'].values","2cd9f0d0":"# Creating a test and training dataset\nX_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.30)","0813d905":"gnb = GaussianNB()\ngnb.fit(X_train, y_train)\ny_pred=gnb.predict(X_test)\ndf_output = pd.DataFrame({'Actual': y_test, 'Predicted': y_pred})\nprint(df_output)","5d867fe8":"# Testing the accuracy of Naive Bayes \nscores=cross_val_score(gnb,X_train,y_train,scoring='accuracy',cv=3).mean()*100\nprint(scores)","9749b9d6":"x=df.loc[:,['energy','loudness','danceability','liveness', 'acousticness']].values\ny=df.loc[:,'popularity'].values","5f0f84ef":"X_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.30)","0422b631":"# Linear SVM model \nLinSVC = LinearSVC(penalty='l2', loss='squared_hinge', dual=True)\nLinSVC.fit(X_train, y_train)\ny_pred=gnb.predict(X_test)\ndf_output = pd.DataFrame({'Actual': y_test, 'Predicted': y_pred})\nprint(df_output)","864c3b58":"# Testing the accuracy\nscores=cross_val_score(LinSVC,X_train,y_train,scoring='accuracy',cv=3).mean()*100\nprint(scores)","b5f3610a":"#Code by Olga Belitskaya https:\/\/www.kaggle.com\/olgabelitskaya\/sequential-data\/comments\nfrom IPython.display import display,HTML\nc1,c2,f1,f2,fs1,fs2=\\\n'#eb3434','#eb3446','Akronim','Smokum',30,15\ndef dhtml(string,fontcolor=c1,font=f1,fontsize=fs1):\n    display(HTML(\"\"\"<style>\n    @import 'https:\/\/fonts.googleapis.com\/css?family=\"\"\"\\\n    +font+\"\"\"&effect=3d-float';<\/style>\n    <h1 class='font-effect-3d-float' style='font-family:\"\"\"+\\\n    font+\"\"\"; color:\"\"\"+fontcolor+\"\"\"; font-size:\"\"\"+\\\n    str(fontsize)+\"\"\"px;'>%s<\/h1>\"\"\"%string))\n    \n    \ndhtml('Be patient. Mar\u00edlia Prata, @mpwolke was Here.' )","1bf9a343":"I've already Encoded some snippets above.","b7ed5857":"#Codes by Arpita Gupta https:\/\/www.kaggle.com\/arpita28\/analysis-of-spotify-trends","81cb6970":"![](https:\/\/cdn.playlists.net\/images\/playlists\/image\/medium\/44f66f1ca1fd95d11fbbff43a01e9a5a.jpg)playlists.net","0e79c822":"ValueError: Data must be 1-dimensional. How can I fix that?By giving np.ones a single value?","0dd78dc2":"Again T,T,S?  Really Arpita?","75da0e6d":"Train, Test Split One more Time???","9f00ddc1":"#Pycomp by Thiago Panini https:\/\/www.kaggle.com\/thiagopanini\/pycomp-predicting-survival-on-titanic-disaster\/notebook"}}