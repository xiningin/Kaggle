{"cell_type":{"a51f5049":"code","b3de44d0":"code","dcd86d82":"code","ef23478b":"code","2c5b0fd5":"code","71b7750a":"code","6e0f2e9f":"code","c41556a4":"code","c4a14b94":"code","bf81738b":"code","938ab589":"code","62931b31":"code","d83ea457":"code","6fd15d49":"code","bb337f39":"code","b7101461":"code","6564b355":"code","0c714610":"code","dbc91f2f":"code","a17b227e":"code","c60588be":"code","ccb2eb5a":"code","cd24934a":"code","e4786c05":"code","8b567fce":"code","465afef9":"code","109245ff":"code","2167d142":"code","5a6b2cd5":"code","574cb65e":"code","7975d5a2":"code","02c7b27c":"code","0ec32ffd":"code","53b6b63f":"code","a22752d6":"code","145b77c3":"code","99258773":"code","c1a813fa":"code","63a71228":"code","7c2a9914":"code","b47ae5d6":"code","5583cd9c":"markdown","74a4c3d0":"markdown","744eaab1":"markdown","a5ea6e96":"markdown","ce2fe16b":"markdown","d06b41dd":"markdown","3de2c8dc":"markdown","08cf8fe0":"markdown","3a0091d3":"markdown","27225cdd":"markdown","1c8abc54":"markdown","bc23b58c":"markdown","8315ec95":"markdown","e575d90a":"markdown","625b42cd":"markdown","a38824c1":"markdown","2dfd6e55":"markdown","009abb8c":"markdown","6c0ef685":"markdown","c79d8d9d":"markdown","9177b501":"markdown"},"source":{"a51f5049":"import re\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom scipy import stats\nfrom scipy.stats import norm\nimport warnings\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.ensemble import RandomForestRegressor\nimport tensorflow as tf\n%matplotlib inline\nwarnings.filterwarnings('ignore')\ndata = pd.read_csv('..\/input\/2015_Air_quality_in_northern_Taiwan.csv')","b3de44d0":"# We want to predict PM2.5, so we delete the rows with NA in 'PM2.5'\ndata = data[data['PM2.5'].notna()]\ndata.info()","dcd86d82":"(data.isna().sum() \/ len(data)).sort_values(ascending=False)","ef23478b":"plt.figure(figsize=(20,5))\nplt.title('The NA ratio in each column')\nplt.xticks(rotation='vertical')\nplt.plot([0,22],[0.5,0.5],'g:')\nplt.plot((data.isna().sum() \/ len(data)).sort_values(ascending=False).index,\n         (data.isna().sum() \/ len(data)).sort_values(ascending=False).values,'-',label=r'$NA \\ ratio = \\frac{counts \\ of \\ NA}{Total \\ row \\ of \\ NA}$')\nplt.annotate('A notable shrink', xy=(6.5, 0.15), xytext=(3, 0.6),fontsize='x-large',\n            arrowprops={'facecolor':'black','shrink':1.0}\n            )\nplt.xlim(0,22)\n_ = plt.legend(fontsize='x-large')","2c5b0fd5":"data['UVB'].value_counts()","71b7750a":"data.drop(['UVB','RAIN_COND','PH_RAIN'],axis=1,inplace=True)","6e0f2e9f":"data[data['CO'].isna()].head()","c41556a4":"plt.figure(figsize=(20,5))\nplt.subplot(1,2,1)\nplt.title('NA counts in each row')\nplt.plot(data.isna().sum(axis=1).value_counts().sort_index())\nplt.xlabel('Number of NA in each row')\nplt.ylabel('Counts')\nplt.xlim(0)\nplt.ylim(0)\n\nplt.subplot(1,2,2)\nplt.title('Accumulated NA counts in each row')\nplt.plot(data.isna().sum(axis=1).value_counts().sort_index().cumsum())\nplt.annotate('Exclude rows with more than 10 NAs \\n will last  215716 rows', xy=(10, 210000), xytext=(10, 160000),fontsize='medium',\n            arrowprops={'facecolor':'black','shrink':1.0})\nplt.annotate('Exclude rows with more than 3 NAs \\n will last 180129 rows', xy=(3, 170000), xytext=(3, 100000),fontsize='medium',\n            arrowprops={'facecolor':'black','shrink':1.0})\nplt.xlabel('Accumulated Number of NA in each row')\nplt.ylabel('Accumulated Counts')\nplt.xlim(0)\n_= plt.ylim(0)","c4a14b94":"data = data.dropna(thresh=17) # 17 = len(data.columns) - 3","bf81738b":"def numeric(row):\n    try:\n        if np.isnan(row):\n            return\n        else:\n            row =str(row)\n            return float(row.replace('x','').replace('#','').replace('*',''))\n    except TypeError:\n        row =str(row)\n        return float(row.replace('x','').replace('#','').replace('*',''))","938ab589":"data['WS_HR'] = data['WS_HR'].apply(numeric)\nprint(data['WS_HR'].describe())\nprint('\\nThe skewness:',data['WS_HR'].skew())\nprint('Right skewed') if data['WS_HR'].skew()>0 else print('Left skewed')","62931b31":"plt.figure(figsize=(20,5))\nplt.subplot(1,2,1)\nplt.plot([data['WS_HR'].median(),data['WS_HR'].median()],[0,7000],'g:',label='median={0:.2f}'.format(data['WS_HR'].median()))\nplt.plot([data['WS_HR'].mean(),data['WS_HR'].mean()],[0,7000],'r:',label='mean   ={0:.2f}'.format(data['WS_HR'].mean()))\nplt.plot([data['WS_HR'].mode(),data['WS_HR'].mode()],[0,7000],'y:',label='mode   ={}'.format(data['WS_HR'].mode()[0]))\nplt.plot(data['WS_HR'].value_counts().sort_index(),label='distribution')\nplt.legend(loc='upper right')\nplt.xlim(0)\nplt.ylim(0)\nplt.subplot(1,2,2)\n_=sns.boxplot(data['WS_HR'])","d83ea457":"data['WS_HR'].fillna(value=data['WS_HR'].median(),inplace=True)","6fd15d49":"for col in ['NO2','NO','NOx','PM10','CO','O3','AMB_TEMP','SO2','WD_HR','RH','WIND_DIREC', 'WIND_SPEED','PM2.5']:\n    data[col]=data[col].apply(numeric)\n    data[col].fillna(value=data[col].median(),inplace=True)\ndata['RAINFALL'] = data['RAINFALL'].apply(lambda x:0 if x=='NR' else x).apply(numeric)","bb337f39":"print((data['CH4'].notna() & data['NMHC'].notna() & data['THC'].isna()).value_counts())\nprint('\\n')\nprint(data['THC'].apply(numeric).describe())\nprint('\\nMode :',data['THC'].apply(numeric).mode())\nprint('\\nKurtosis is ',data['THC'].apply(numeric).kurt(),'>3, it is leptokurtic')","b7101461":"plt.figure(figsize=(20,5))\nplt.subplot(1,2,1)\nplt.plot(data['THC'].apply(numeric).value_counts().sort_index(),label='distribution')\nplt.plot([data['THC'].apply(numeric).mean()]*2,[0,17500],'g:',label='mean   ={0:.2f}'.format(data['THC'].apply(numeric).mean()))\nplt.plot([data['THC'].apply(numeric).median()]*2,[0,17500],'r:',label='median={0:.2f}'.format(data['THC'].apply(numeric).median()))\nplt.plot([data['THC'].apply(numeric).mode()[0]]*2,[0,17500],'y:',label='mode   ={0:.2f}'.format(data['THC'].apply(numeric).mode()[0]))\nplt.legend()\n\nplt.subplot(1,2,2)\nsns.boxplot(data['THC'].apply(numeric).value_counts().sort_index())","6564b355":"data.drop(['CH4','NMHC','THC'],axis=1,inplace=True)","0c714610":"data['year'] = pd.to_datetime(data['time']).dt.year\ndata['month'] = pd.to_datetime(data['time']).dt.month\ndata['day'] = pd.to_datetime(data['time']).dt.day\ndata['hour'] = pd.to_datetime(data['time']).dt.hour\n# data.drop('time',axis=1,inplace=True)","dbc91f2f":"plt.figure(figsize=(20,5))\nplt.subplot(1,3,1)\nplt.title('AVG. PM2.5 in each month')\nplt.plot(data.groupby('month').mean()['PM2.5'])\nplt.xlim(1,12)\nplt.xlabel('month')\nplt.ylabel('PM2.5')\n\nplt.subplot(1,3,2)\nplt.title('AVG. PM2.5 in each hour')\nplt.plot(data.groupby('hour').mean()['PM2.5'])\nplt.xlim(0,23)\nplt.xlabel('hour')\nplt.ylabel('PM2.5')\n\nplt.subplot(1,3,3)\nplt.title('AVG. PM2.5 in each station')\nplt.bar(data.groupby('station').mean().index,data.groupby('station').mean()['PM2.5'])\nplt.xticks(rotation='vertical')\nplt.xlabel('station')\n_=plt.ylabel('PM2.5')","a17b227e":"continous_columns=['AMB_TEMP', 'CO', 'NO', 'NO2', 'NOx', 'O3', 'PM10', 'PM2.5', 'RAINFALL','RH', 'SO2', 'WD_HR', 'WIND_DIREC', 'WIND_SPEED', 'WS_HR']\ndiscrete_columns=['station','year','month','day','hour']","c60588be":"plt.figure(figsize=(20,60))\nfor number,col in enumerate(continous_columns):\n    plt.subplot(15,2,number*2+1)\n    sns.distplot(data[col],fit=norm)\n    plt.subplot(15,2,number*2+2)\n    res = stats.probplot(data[col],plot=plt)","ccb2eb5a":"plt.figure(figsize=(20,20))\nfor number,col in enumerate(['station','month','day','hour']):\n    plt.subplot(4,2,number*2+1)\n    plt.xticks(rotation='vertical')\n    sns.boxplot(data[col],data['PM2.5'])\n    plt.subplot(4,2,number*2+2)\n    plt.title('With log')\n    plt.xticks(rotation='vertical')\n    sns.boxplot(data[col],np.log(data['PM2.5']))","cd24934a":"# # This will take long time to plot\n# plt.figure(figsize=(50,50))\n# sns.set(style='darkgrid')\n# fig=sns.pairplot(data[continous_columns+['station']],hue='station')","e4786c05":"plt.figure(figsize=(20,20))\nhm=sns.heatmap(data[continous_columns].corr().values,annot=True,\n               xticklabels=continous_columns,\n               yticklabels=continous_columns,\n               cmap='Reds')","8b567fce":"abs(data[continous_columns].corr()['PM2.5']).sort_values(ascending=False) # absolute number","465afef9":"data_Banqiao = data[data['station']=='Banqiao'].sort_values(by=['year','month','day','hour'])","109245ff":"for col in ['AMB_TEMP', 'CO', 'NO', 'NO2', 'NOx', 'O3', 'PM10','RAINFALL', 'RH', 'SO2', 'WD_HR']:\n    stdscl = StandardScaler()\n    data_Banqiao[col] = stdscl.fit_transform(data_Banqiao[col].values.reshape(-1,1))","2167d142":"data_Banqiao['next_hour_pm2.5'] = data_Banqiao['PM2.5'].shift(-1)\ndata_Banqiao.drop(8759,inplace=True)\ny_train = data_Banqiao[data_Banqiao['month'] != 12].loc[:,'next_hour_pm2.5'].values\nx_train = data_Banqiao[data_Banqiao['month'] != 12].loc[:,'AMB_TEMP':'hour'].values\ny_test = data_Banqiao[data_Banqiao['month'] == 12].loc[:,'next_hour_pm2.5'].values\nx_test = data_Banqiao[data_Banqiao['month'] == 12].loc[:,'AMB_TEMP':'hour'].values","5a6b2cd5":"# Need to check if it's suitable, because it will interrupt the continuity of time, e.g. is it proper to use time T+1 and Time T-1 to predirct Time T? Maybe not, but grid search is doing so. Therefore, I think maybe it's not fair to compare this RF to LSTM.\ng_search = GridSearchCV(RandomForestRegressor(n_jobs=-1),param_grid={'n_estimators':[5,10,20],'max_features':[5,10],'max_depth':[10,20,30]},scoring='neg_mean_squared_error',n_jobs=-1,cv=2)\ng_search.fit(x_train,y_train)\ny_rf_pred = g_search.best_estimator_.predict(x_test)\npd.DataFrame(g_search.cv_results_)","574cb65e":"plt.figure(figsize=(20,5))\nplt.plot(y_rf_pred,label='prediction')\nplt.plot(y_test,label='real data')\nplt.xlim(0,737)\nplt.ylim(0,)\n_ =plt.legend(loc='upper left')\nprint('MSE',mean_squared_error(y_test, y_rf_pred))","7975d5a2":"device_name = tf.test.gpu_device_name()\nif device_name != '\/device:GPU:0':\n  raise SystemError('GPU device not found')\nprint('Found GPU at: {}'.format(device_name))\nimport timeit\n\n# See https:\/\/www.tensorflow.org\/tutorials\/using_gpu#allowing_gpu_memory_growth\nconfig = tf.ConfigProto()\nconfig.gpu_options.allow_growth = True\n\nwith tf.device('\/cpu:0'):\n  random_image_cpu = tf.random_normal((100, 100, 100, 3))\n  net_cpu = tf.layers.conv2d(random_image_cpu, 32, 7)\n  net_cpu = tf.reduce_sum(net_cpu)\n\nwith tf.device('\/gpu:0'):\n  random_image_gpu = tf.random_normal((100, 100, 100, 3))\n  net_gpu = tf.layers.conv2d(random_image_gpu, 32, 7)\n  net_gpu = tf.reduce_sum(net_gpu)\n\nwith tf.Session(config=config) as sess:\n    tf.global_variables_initializer().run()\n    def cpu():\n      sess.run(net_cpu)\n    def gpu():\n      sess.run(net_gpu)\n    \n    print('Time (s) to convolve 32x7x7x3 filter over random 100x100x100x3 images '\n          '(batch x height x width x channel). Sum of ten runs.')\n    print('CPU (s):')\n    cpu_time = timeit.timeit('cpu()', number=10, setup=\"from __main__ import cpu\")\n    print(cpu_time)\n    print('GPU (s):')\n    gpu_time = timeit.timeit('gpu()', number=10, setup=\"from __main__ import gpu\")\n    print(gpu_time)\n    print('GPU speedup over CPU: {}x'.format(int(cpu_time\/gpu_time)))","02c7b27c":"n_steps = 1\nn_inputs = 19\nn_outputs = 1\nn_units = [100, 100,100]\nlearning_rate= 0.5 #Adam perform learning rate decay \nn_epochs = 100\nn_iterations = n_epochs * len(x_train)\nwith tf.device('\/gpu:0'):\n    tf.reset_default_graph()\n    x = tf.placeholder(tf.float32,[None,n_steps,n_inputs])\n    y = tf.placeholder(tf.float32,[None,n_steps,n_outputs])\n\n    cells = [tf.contrib.rnn.LSTMCell(num_units=n) for n in n_units]\n    cells = tf.contrib.rnn.MultiRNNCell(cells)\n    cells = tf.contrib.rnn.DropoutWrapper(cells,0.5)\n    cells = tf.contrib.rnn.OutputProjectionWrapper(cells, output_size=n_outputs)\n    outputs, states = tf.nn.dynamic_rnn(cells, x, dtype=tf.float32)\n    loss = tf.reduce_mean(tf.square(outputs - y))\n    training_op = tf.train.AdamOptimizer(learning_rate).minimize(loss)","0ec32ffd":"from IPython.display import clear_output, Image, display, HTML\n\ndef strip_consts(graph_def, max_const_size=32):\n    \"\"\"Strip large constant values from graph_def.\"\"\"\n    strip_def = tf.GraphDef()\n    for n0 in graph_def.node:\n        n = strip_def.node.add() \n        n.MergeFrom(n0)\n        if n.op == 'Const':\n            tensor = n.attr['value'].tensor\n            size = len(tensor.tensor_content)\n            if size > max_const_size:\n                tensor.tensor_content = \"<stripped %d bytes>\"%size\n    return strip_def\n\ndef show_graph(graph_def, max_const_size=32):\n    \"\"\"Visualize TensorFlow graph.\"\"\"\n    if hasattr(graph_def, 'as_graph_def'):\n        graph_def = graph_def.as_graph_def()\n    strip_def = strip_consts(graph_def, max_const_size=max_const_size)\n    code = \"\"\"\n        <script>\n          function load() {{\n            document.getElementById(\"{id}\").pbtxt = {data};\n          }}\n        <\/script>\n        <link rel=\"import\" href=\"https:\/\/tensorboard.appspot.com\/tf-graph-basic.build.html\" onload=load()>\n        <div style=\"height:600px\">\n          <tf-graph-basic id=\"{id}\"><\/tf-graph-basic>\n        <\/div>\n    \"\"\".format(data=repr(str(strip_def)), id='graph'+str(np.random.rand()))\n\n    iframe = \"\"\"\n        <iframe seamless style=\"width:1200px;height:620px;border:0\" srcdoc=\"{}\"><\/iframe>\n    \"\"\".format(code.replace('\"', '&quot;'))\n    display(HTML(iframe))\nshow_graph(tf.get_default_graph())","53b6b63f":"config = tf.ConfigProto()\nconfig.gpu_options.allow_growth = True\nwith tf.Session(config=config) as sess:\n    tf.global_variables_initializer().run()\n    for iteration in range(n_iterations):\n        index = iteration%len(x_train)\n        x_batch = x_train[index,:].reshape(-1,n_steps,n_inputs)\n        y_batch = y_train[index].reshape(-1,n_steps,n_outputs)\n        sess.run(training_op, feed_dict={x:x_batch,y:y_batch})\n        output_val=outputs.eval(feed_dict={x:x_batch,y:y_batch})\n        if iteration % len(x_train)==0:\n            loss_val = loss.eval(feed_dict={x:x_batch,y:y_batch})\n            if loss_val <= 300: #early break\n                break\n    y_lstm_pred = outputs.eval(feed_dict={x:x_test.reshape(-1,n_steps,n_inputs)})\n    y_lstm_pred = [i.reshape(1)[0] for i in y_lstm_pred]","a22752d6":"plt.figure(figsize=(20,5))\nplt.plot(y_lstm_pred,label='LSTM',)\nplt.plot(y_rf_pred.reshape(-1),label='RandomForest')\nplt.plot(y_test,label='Real data')\nplt.xlabel('Hour start from 2015\/12\/1 0:00  to  2015\/12\/31 22:00 ')\nplt.ylabel('PM2.5')\nplt.xlim(0,737)\nplt.ylim(0,)\n_=plt.legend(loc='upper left')\nprint('Random Forest MSE',mean_squared_error(y_test, y_rf_pred))\nprint('LSTM MSE:',mean_squared_error(y_test, y_lstm_pred))","145b77c3":"plt.figure(figsize=(100,50))\nfor i in range(1,13):\n    y_train = data_Banqiao[data_Banqiao['month'] != i].loc[:,'next_hour_pm2.5'].values\n    x_train = data_Banqiao[data_Banqiao['month'] != i].loc[:,'AMB_TEMP':'hour'].values\n    y_test = data_Banqiao[data_Banqiao['month'] == i].loc[:,'next_hour_pm2.5'].values\n    x_test = data_Banqiao[data_Banqiao['month'] == i].loc[:,'AMB_TEMP':'hour'].values\n    est = RandomForestRegressor(max_depth=10, max_features=10, n_estimators=20,n_jobs=-1)    \n    y_rf_pred = est.fit(x_train,y_train).predict(x_test)\n    plt.subplot(4,3,i)\n    plt.title(f'Month {i}')\n    plt.plot(y_rf_pred,label='prediction')\n    plt.plot(y_test,label='real data')\n    plt.plot(lable=mean_squared_error(y_test, y_rf_pred))\n    plt.xlim(0,)\n    plt.ylim(0,)\n    _ =plt.legend(loc='upper left')\n    annotation_location = max(max(y_test), max(y_rf_pred))*0.7\n    plt.annotate('Month {0} MSE: {1:.2f}'.format(i, mean_squared_error(y_test, y_rf_pred)), \n             xy=(10, annotation_location), xytext=(10, annotation_location))","99258773":"# https:\/\/e-service.cwb.gov.tw\/wdps\/obs\/state.htm \nBanqiao = {'name':'Banqiao','longitude':121.4420,'latitude':24.9976}\nDayuan = {'name':'Dayuan','longitude':121.2260,'latitude':25.0478}\nGuanyin = {'name':'Guanyin','longitude':121.1533,'latitude':25.0271}\nKeelung = {'name':'Keelung','longitude':121.7405,'latitude':25.1333}\nLinkou = {'name':'Linkou','longitude':121.3808,'latitude':25.0723}\nLongtan = {'name':'Longtan','longitude':121.2214,'latitude':24.8701}\nPingzhen = {'name':'Pingzhen','longitude':121.2146,'latitude':24.8975}\nShilin = {'name':'Shilin','longitude':121.5030,'latitude':25.0903}\nSongshan = {'name':'Songshan','longitude':121.5504,'latitude':25.0487}\nTaoyuan = {'name':'Taoyuan','longitude':121.3232,'latitude':24.9924}\nTucheng = {'name':'Tucheng','longitude':121.4452,'latitude':24.9732}\nXinzhuang = {'name':'Xinzhuang','longitude':121.4468,'latitude':25.0515}\nXizhi = {'name':'Xizhi','longitude':121.6588,'latitude':25.0669}\nYonghe = {'name':'Yonghe','longitude':121.5081,'latitude':25.0113}\nZhongli = {'name':'Zhongli','longitude':121.2564,'latitude':24.9777}\nall_stations= [Banqiao,Dayuan,Guanyin,Keelung,Linkou,Longtan,Pingzhen,Shilin,Songshan,Taoyuan,Tucheng,Xinzhuang,Xizhi,Zhongli]","c1a813fa":"import folium","63a71228":"m = folium.Map(location=[25.0, 121.5],zoom_start=10)\nfor station in all_stations:\n    name = station['name']\n    lat = station['latitude']\n    lon = station['longitude']\n    folium.Marker([lat,lon],popup=name).add_to(m)\nprint('Welcome to Taiwan!')\nm","7c2a9914":"plt.figure(figsize=(20,5))\nplt.title('Simpler map')\nplt.xlabel('longitude')\nplt.ylabel('latitude')\nfor station in all_stations:\n    plt.scatter(station['longitude'],station['latitude'])\n    plt.annotate(s=station['name'], xy=(station['longitude'],station['latitude']))","b47ae5d6":"def all_stations_plot(month,day):\n    plt.figure(figsize=(50,25))\n    plt.title('month {} day {}'.format(month,day))\n    for station in data['station'].unique():\n        plt.plot(data[(data['station']==station)&(data['month'] == month)&(data['day'] == day)]['PM2.5'].values,label=station)\n        plt.legend(loc='upper right',prop={'size': 20})\n    plt.xticks(np.arange(0,25,1))\n    _=plt.xlim(0,23)\n    plt.show()\nfor i in range(1,32):\n    all_stations_plot(1,i)","5583cd9c":"Like k-fold, we can do some monthly-fold, use all month exclude $i$ to train, and use month $i$ to test it. ","74a4c3d0":"Unfortunately there is no NA we can fill back. THC mainly distributes from 1.9(25%) to 2.2(75%).\n\nAlthough this leptokurtic distribution  might be promising we impute the NA row with median and build a fine data, but  in my preference, I will just exclude these variables, one reason is that the redudancy will increase computation cost.","744eaab1":"That's impressived.  Just can't wait to use tensorflow !!\n![release the kraken](https:\/\/imgur.com\/wJiKf1b.jpg)\n[And recently Google release TPU free trial on colab, theoretically it can run much more faster than GPU.](https:\/\/colab.research.google.com\/github\/tensorflow\/tpu\/blob\/master\/tools\/colab\/shakespeare_with_tpu_and_keras.ipynb)\n\nBack to LSTM.\n\n[For the power of RNN, please have a look at Andrej Karpathy blog](http:\/\/karpathy.github.io\/2015\/05\/21\/rnn-effectiveness\/)\n\n[For how LSTM works, please have a look at colah's blog Blog](https:\/\/colah.github.io\/posts\/2015-08-Understanding-LSTMs\/)\n\n\n\n\n![Common RNN](https:\/\/imgur.com\/USXFzHV.jpg)\n> Each rectangle is a vector and arrows represent functions (e.g. matrix multiply). Input vectors are in red, output vectors are in blue and green vectors hold the RNN's state (more on this soon). From left to right: (1) Vanilla mode of processing without RNN, from fixed-sized input to fixed-sized output (e.g. image classification). (2) Sequence output (e.g. image captioning takes an image and outputs a sentence of words). (3) Sequence input (e.g. sentiment analysis where a given sentence is classified as expressing positive or negative sentiment). (4) Sequence input and sequence output (e.g. Machine Translation: an RNN reads a sentence in English and then outputs a sentence in French). (5) Synced sequence input and output (e.g. video classification where we wish to label each frame of the video). Notice that in every case are no pre-specified constraints on the lengths sequences because the recurrent transformation (green) is fixed and can be applied as many times as we like.\n\n(From Andrej Karpathy blog)\n\nWe take One vector to predict another One vector, aka next hour PM2.5, so \"n_outputs\"  is one. And We since we have lager n_units size of inputs ,we also apply dropout to force all the neuron participating training.\n\nFor the tuning, I use default hyperparameter. [Klaus Greff, Rupesh K. Srivastava, Jan Koutn\u00b4\u0131k, Bas R. Steunebrink, Jurgen Schmidhuber(2017), LSTM: A Search Space Odyssey](https:\/\/arxiv.org\/pdf\/1503.04069.pdf)\n\nNIG: No Input Gate\n\nNFG: No Forget Gate\n\nNOG: No Output Gate\n\nNIAF: No Input Activation Function\n\nNOAF: No Output Activation Function\n\nCIFG: Coupled Input and Forget Gate\n\nNP: No Peepholes\n\nFGR: Full Gate Recurrence\n![Odyssey](https:\/\/imgur.com\/FegnNXg.jpg)\n> This paper reports the results of a large scale study on\nvariants of the LSTM architecture. We conclude that the\nmost commonly used LSTM architecture (vanilla LSTM)\nperforms reasonably well on various datasets. None of the eight\ninvestigated modifications significantly improves performance.\nHowever, certain modifications such as coupling the input and \nforget gates (CIFG) or removing peephole connections (NP)\nsimplified LSTMs in our experiments without significantly\ndecreasing performance. These two variants are also attractive\nbecause they reduce the number of parameters and the\ncomputational cost of the LSTM.\n","a5ea6e96":"\n# What we are looking for in this dataset?(Why we analysis this dataset?) #\n\n*(Note:  After  the first edition of  this kernel was committed, some improvement thoughts had appeared in my mind. But I am busy working on project recently. Please  feel free to fork and modify this kernel or leave a message below, thank you!)*\n\n![credit:yahoo news,  https:\/\/tw.news.yahoo.com\/pm2-5%E5%81%8F%E9%AB%98-%E5%9C%8B%E5%81%A5%E7%BD%B2%E7%B1%B2-%E6%88%B6%E5%A4%96%E6%B4%BB%E5%8B%95%E5%AE%9C%E6%B8%9B%E5%B0%91-103036186.html](https:\/\/imgur.com\/2ce0M3x.jpg)\nRecently, the pollution of PM2.5 in northern Taiwn has became more severe. It has decrease the life expectancy in Asia by 2 years, and people in Taiwan are warning not to exercise outside off works. Let's have a look at the news.\n\n[\u7814\u7a76\uff1aPM2.5\u7a7a\u6c61\u7e2e\u77ed\u4eba\u985e\u9810\u671f\u58fd\u547d \u4e9e\u6d32\u5f71\u97ff\u6700\u5287](http:\/\/news.ltn.com.tw\/news\/world\/breakingnews\/2530451)\n> \u5fb7\u5dde\u5927\u5b78\u5967\u65af\u6c40\u5206\u6821\u79d1\u5b78\u5718\u968a\u5148\u5206\u6790\u7a7a\u6c61\u4ee5\u5916\u7684\u75be\u75c5\u3001\u50b7\u5bb3\u548c\u76f8\u95dc\u55aa\u547d\u98a8\u96aa\u56e0\u7d20\u5c0e\u81f4\u7684\u6b7b\u4ea1\u7387\uff0c\u518d\u6aa2\u8996\u5168\u7403185\u500b\u66b4\u9732\u5728PM2.5\u7576\u4e2d\u7684\u570b\u5bb6\u4e4b\u72c0\u6cc1\uff0c\u63a2\u7a76\u6bcf\u500b\u570b\u5bb6\u7684\u9810\u671f\u58fd\u547d\u8207\u7a7a\u6c61\u4e4b\u9593\u7684\u95dc\u806f\uff0c\u767c\u73fe\u5168\u7403\u4eba\u985e\u7684\u9810\u671f\u58fd\u547d\u5e73\u5747\u77ed\u5c112\u5e74\uff0c\u5728\u82f1\u7f8e\u5730\u5340\u4eba\u5011\u7684\u5e73\u5747\u58fd\u547d\u5c11\u4e864\u500b\u6708\u3002\n> \u7814\u7a76\u6307\u51fa\uff0c\u4e9e\u6d32\u3001\u975e\u5dde\u8207\u4e2d\u6771\u5730\u5340\u7684\u7a7a\u6c61\u554f\u984c\u56b4\u91cd\uff0c\u5b5f\u52a0\u62c9\u4eba\u6c11\u9810\u671f\u58fd\u547d\u6e1b\u5c111.87\u5e74\uff0c\u57c3\u53ca\u4eba\u6c11\u5247\u5c11\u63891.85\u5e74\uff0c\u5df4\u57fa\u65af\u5766\u4eba\u6c11\u77ed\u5c111.56\u5e74\uff0c\u5370\u5ea6\u6c11\u773e\u5c11\u63891.53\u5e74\uff0c\u963f\u62c9\u4f2f\u4eba\u6c11\u5c11\u4e861.48\u5e74\uff0c\u5948\u53ca\u5229\u4e9e\u6c11\u773e\u77ed\u5c111.28\u5e74\uff0c\u4e2d\u570b\u4eba\u6c11\u5247\u662f\u5e73\u5747\u5c11\u4e861.25\u5e74\u7684\u58fd\u547d\u3002\n\n(Translated to English by me)\n>Science team of University of Texas at Austin analyzes the mortality caused by diseases, injuries rather than air pollution, and then view the status of 185 countries exposed to PM2.5. They analysised the relationship of  the air pollution with life expectancy  of each country. The correlation between air pollution and life expectancy is  found that the life expectancy of human beings worldwide is 2 years shorter averagely, and the average life expectancy in the British and America is 4 months shorter.\nThe study also pointed out that the air pollution problem in Asia, Africa and the Middle East is extremely severe. The life expectancy of the Bangladeshi is decrease by 1.87 years, the Egypt is 1.85 years shorter, the Pakistan  is 1.56 years shorter, the India is 1.53 years shorter, and the Arab people is 1.48 shorter, and China is 1.25 years shorter.\n\n[\u4e0b\u73ed\u5f8c\u6236\u5916\u904b\u52d5\u597d\u55ce\uff1f\u5fc3\u81df\u79d1\u540d\u91ab\uff1a\u4e0d\u53ef\u4ee5\uff01](https:\/\/health.udn.com\/health\/story\/5978\/3364185)\n> \u6839\u64da\u7f8e\u570b\u5fc3\u81df\u5b78\u67032010\u5e74\u7684\u5171\u8b58\u5831\u544a\uff0cPM2.5\u6bcf\u589e\u52a010\u500b\u55ae\u4f4d\uff0c\u7e3d\u6b7b\u4ea1\u7387\u7d04\u589e\u52a015\uff05\uff0c\u5fc3\u80ba\u75be\u75c5\u6b7b\u4ea1\u7387\u7d04\u589e\u52a015\uff05\uff0c\u5fc3\u8840\u7ba1\u75be\u75c5\u6b7b\u4ea1\u7387\u589e\u52a010\u81f315\uff05\uff0c\u7f3a\u8840\u6027\u5fc3\u81df\u75c5\u6b7b\u4ea1\u7387\u589e\u52a015\u81f320\uff05\u3002\n>\u70ba\u4f55\u7a7a\u6c23\u6c59\u67d3\u6703\u50b7\u5bb3\u5fc3\u8840\u7ba1\uff1f\u6d2a\u60e0\u98a8\u6307\u51fa\uff0c\u5fc3\u808c\u6897\u585e\u597d\u6bd4\u571f\u77f3\u6d41\uff0c\u571f\u77f3\u6d41\u901a\u5e38\u662f\u5728\u98b1\u98a8\u8c6a\u96e8\u5f8c\u8f03\u5bb9\u6613\u767c\u751f\uff0c\u5c0d\u5fc3\u8840\u7ba1\u800c\u8a00\uff0c\u7a7a\u6c23\u6c59\u67d3\u3001\u58d3\u529b\u548c\u7dca\u5f35\u7b49\u90fd\u662f\u98b1\u98a8\u8c6a\u96e8\u3002\u7531\u65bcPM2.5\u662f\u7a7a\u6c23\u6c59\u67d3\u7684\u4e00\u7a2e\uff0c\u5b83\u975e\u5e38\u5fae\u5c0f\uff0c\u6703\u96a8\u8457\u547c\u5438\u9032\u5165\u80ba\u6ce1\u518d\u6df1\u5165\u8840\u6db2\uff0c\u7576\u4eba\u66b4\u9732\u5728PM2.5\u4e4b\u4e0b\u5bb9\u6613\u5347\u9ad8\u8840\u58d3\uff0c\u63d0\u9ad8\u767c\u708e\u53cd\u61c9\uff0c\u5c0e\u81f4\u8840\u7ba1\u62bd\u7b4b\u3001\u7834\u88c2\uff0c\u7522\u751f\u8840\u6813\uff0c\u9032\u800c\u5bb9\u6613\u767c\u751f\u5fc3\u81df\u75c5\u3002\n\n\n>Is it good for outdoor sports after work?  Cardiology doctor:definitely not! (WIP: (Translated to English by me))\n\nOK, Let's get started.","ce2fe16b":"To be continued.\n\nFor more recent information about PM2.5 in Tawian please check [\u53f0\u7063\u5373\u6642 PM2.5](https:\/\/www.taiwanstat.com\/realtime\/pm2.5\/). This issue should be \nget more attention.  And when summer ends, the situation getting worse.\n![monthly](https:\/\/imgur.com\/2SaVuAT.jpg)\n\n![wic](https:\/\/imgur.com\/QXkcSXD.jpg)\n\nTo do list\n- [ ] Adjust LSTM for better performance. The dependency of prior step's output  is too much, make the prediction varies too little, became a steady line compared to RF. Maybe using Attetion mechanism.(I just believe it can performance better)\n- [ ] Optimize run time. Padding zero for deleted rows and processing with larger batch size. Try coupling the input and forget gates (CIFG) or removing peephole connections (NP)\n- [ ] Take location into consideration. Assume the time now is $t=0$ , make features such as 'Dayuan-1' repesenting PM2.5 of Dayuan at $t=-1$, 'Dayuan-2' repesenting PM2.5 of Dayuan at $t=-2$, this maybe help predicting  $t=1$ at Banqiao.\n\n   t=-2(2 hours ago)           t=-1(1 hour ago)            t=0(now)            t=1(1 hour latter)\n\n- [ ] Predict further future. 3 hours later, 12 hours, 1 day?\n- [ ] Tranlate some Chinese news into English.","d06b41dd":"\nIf we sort the counts of NA in each columns, we can get a graph below. We can see there is a notable shrink from 'THC' to 'WS_HR', droping from over 0.5 to below 0.5 , after that, the NA ratio of each column becoms much more stable. Based on some empirical approachs, it's fine to just impute, fill NAs and use the columns after 'WS_HR'. But we will have a look of columns before that just in case we miss some useful information.\n\nMy strategy here is dealing with the column with most and least NAs first, and come back to decide to how to deal with the columns with about 0.5 NA ratio.","3de2c8dc":"The mode of this columns is 0. According to [\u4e2d\u592e\u6c23\u8c61\u5c40\u6b77\u53f2\u8cc7\u6599](https:\/\/www.cwb.gov.tw\/V7\/observe\/UVI\/UVI_Hist.htm), it's possible that we have 0 in UVB. \n\n![2013\/09](https:\/\/i.imgur.com\/43BbLMD.jpg)\n\n\nHowever, the historical data also shows that instead of actually being 0, it's more likely that the 0 is missing value. Therefore, instead of making use of this column, ","08cf8fe0":"Is it possible to use the data from last hour to predict the pm2.5 air quality now?\n\ne.g. \n\nuse data from 2015\/1\/1\/0:00\n\nto predict PM2.5 of 2015\/1\/1\/1:00\n\nSince this is a time-series problem, instead of using  sklearn.model_selection.train_test_split for making test data, I will choose December for testing data.(737 rows)\nFrom 2015\/12\/1 0:00  to  2015\/12\/31 22:00","3a0091d3":"Let's have a look with columns 'CO' which have 0.000885 NA raio. Obivously, if 'CO' is NA usaully  other columns will be NA too.","27225cdd":"# Now, let's take location into consideration.","1c8abc54":"Let's have a look in tensorflow and LSTM","bc23b58c":"The counts of NR in  'RAIN_COND', 'PH_RAIN' are both 30455(81.48%). If NR stands for 'no record', we should better drop these columns.","8315ec95":"So far, it matches with [this news(PM2.5\u9054\u300c\u975e\u5e38\u9ad8\u300d \u6843\u5712\u3001\u5e73\u93ae\u548c\u9f8d\u6f6d\u6c11\u773e\u5c11\u5916\u51fa)](http:\/\/news.ltn.com.tw\/news\/life\/breakingnews\/1541461)\n> (2015-12-16) ...\u53d7\u5883\u5916\u6c59\u67d3\u7269\u5f71\u97ff\uff0c\u76ee\u524d\u6843\u5712\u3001\u5e73\u93ae\u548c\u9f8d\u6f6d\u6e2c\u5f97\u7d30\u61f8\u6d6e\u5fae\u7c92\uff08PM2.5\uff09\u6307\u6a19\u5df2\u7d93\u9054\u300c\u975e\u5e38\u9ad8\u300d\u7b49\u7d1a\uff0c\u5927\u5712\u548c\u4e2d\u58e2\u4e5f\u9054\u300c\u9ad8\u300d\u7684\u7b49\u7d1a\uff0c\u7a7a\u6c23\u54c1\u8cea\u6975\u5dee...","e575d90a":"We got some annotation in the numeric column like, 1.7#,9.6#,2.3*,2.5x, we want to clean that.","625b42cd":"It's time to have a closer look at the column 'NHMC', 'THC', 'CH4'.\n \n| column | NA Ratio |\n|--------|----------|\n| NMHC   | 0.568776 |\n| THC    | 0.568071 |\n| CH4    | 0.568071 |\n\n\n***NMHC***\n\nAccoding to [wikipedia](https:\/\/en.wikipedia.org\/wiki\/Non-methane_volatile_organic_compound), NHMC is 'non-methane hydrocarbons' (\u975e\u7532\u70f7\u70f4,\u6307\u9664\u7532\u70f7\u4ee5\u5916\u6240\u6709\u7684\u7e3d\u7a31)\n> An important subset of NMVOCs are the non-methane hydrocarbons (NMHCs). Methane is excluded in air-pollution contexts because it is not harmful. Its low reactivity and thus long lifetime in the atmosphere, however, makes it an important greenhouse gas.\n\n(\u662f\u6709\u6a5f\u5316\u5408\u7269\u7684\u4e00\u7a2e\uff0c\u53ea\u7531\u78b3\u548c\u6c2b\u7d44\u6210\u3002\u70f4\u985e\u5305\u62ec\u4e86\u70f7\u70f4\u3001\u70ef\u70f4\u3001\u7094\u70f4\u3001\u74b0\u70f4\u53ca\u82b3\u70f4\uff0c\u662f\u8a31\u591a\u5176\u4ed6\u6709\u6a5f\u5316\u5408\u7269\u7684\u57fa\u9ad4\u3002\u975e\u7532\u70f7\u70f4\uff0c\u4e3b\u8981\u5305\u62ec\u70f7\u70f4\u3001\u70ef\u70f4\u3001\u82b3\u9999\u70f4\u548c\u542b\u6c27\u70f4\u7b49\u7d44\u5206\uff0c\u70ba\u5149\u5316\u5b78\u7159\u9727\u5f62\u6210\u7684\u4e3b\u8981\u7269\u8cea\uff0c\u5c0d\u5927\u6c23\u6c61\u67d3\u6709\u91cd\u8981\u7684\u5f71\u97ff\u3002\u56e0\u70ba\u7532\u70f7\u76f8\u5c0d\u8f03\u7a69\u5b9a\uff0c\u6545\u8a0e\u8ad6\u78b3\u6c2b\u5316\u5408\u7269\u7684\u6c61\u67d3\u6642\u4e0d\u5305\u62ec\u5728\u5167\u3002)\n\n\n\n***THC*** total hydrocarbons \u7e3d\u70f4\n\n***CH4*** Methane \u7532\u70f7\n\nSome we can make a assumption :\n\n$THC = NMHC + CH4$\n\nLet's see if there is chance we  can fill NAs by adding 'NMHC', 'CH4' back. ","a38824c1":"# EDA again with more detail with univariate and biavariate analysis#\n## univariate analysis##\n","2dfd6e55":"If we make the NA threshold higher, it will be more abundant  but less clean, vice versa. It's a trade-off.  Judging by the slope, it will be a nice a balance of abundance and cleanliness  if we set the threshold be 3, but no more less further.","009abb8c":" It will be efficient if we know what will it be, if we filter the rows by counts of NAs.","6c0ef685":"Let's have a look of UVB first.\n\nAccoding to [wikipedia](https:\/\/en.wikipedia.org\/wiki\/Ultraviolet)\n\n> Ultraviolet (UV) is electromagnetic radiation with a wavelength from 10 nm to 400 nm, shorter than that of visible light but longer than X-rays. UV radiation is present in sunlight constituting about 10% of the total light output of the Sun. It is also produced by electric arcs and specialized lights, such as mercury-vapor lamps, tanning lamps, and black lights. Although long-wavelength ultraviolet is not considered an ionizing radiation because its photons lack the energy to ionize atoms, it can cause chemical reactions and causes many substances to glow or fluoresce. Consequently, the chemical and biological effects of UV are greater than simple heating effects, and many practical applications of UV radiation derive from its interactions with organic molecules.\n\n\nUVB seems to be a potential variable that will have influence on PM 2.5. But somehow it has too much NAs. Let's see what happend to this columns.","c79d8d9d":"Let's  take Banqiao station for model building example.","9177b501":"# EDA & Preprocessing #\nThis is a data set with 218640 smaples and 23 variables. Knowing our data before starting building model can save hours of pain.\n\n## The NAs##"}}