{"cell_type":{"88f85e88":"code","f5cb99f0":"code","0f3f0b44":"code","60d6da15":"code","1aaa43ef":"code","ec29abc9":"code","0ca9291c":"code","806da56f":"code","4bf6e9e2":"code","39398c02":"code","c36f2fb5":"code","962e918a":"code","63677e2d":"code","7e5abd9d":"code","e6d96911":"code","57979a06":"code","4b45b2a1":"code","4af15517":"markdown","caa3c9c8":"markdown","53bb8ef5":"markdown","4915a29c":"markdown"},"source":{"88f85e88":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport numpy.matlib\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\nimport gc\nimport sys\n\n# Any results you write to the current directory are saved as output.\n\nimport keras\nfrom keras.models import Sequential, Model\nfrom keras.layers import Dense, Dropout, Input, Add, concatenate\nfrom keras.optimizers import SGD\nfrom  keras.losses import categorical_crossentropy\nfrom keras.regularizers import l2\nkeras.backend.set_floatx('float32')\n\nfrom xgboost import XGBClassifier\n\nfrom kaggle.competitions import nflrush","f5cb99f0":"def error_correcting_codes(df):\n    df = df.replace('BLT', 'BAL')\n    df = df.replace('HST', 'HOU')\n    df = df.replace('ARZ', 'ARI')\n    df = df.replace('CLV', 'CLE')\n    return df\n  \n\ndef organize_positions(df):\n    return (df.loc[(df['PossessionTeam']==df['HomeTeamAbbr'])&(df['Team']=='away') | (df['PossessionTeam']==df['VisitorTeamAbbr'])&(df['Team']=='home')].copy().reset_index(),\n      df.loc[((df['PossessionTeam']==df['HomeTeamAbbr'])&(df['Team']=='home') | (df['PossessionTeam']==df['VisitorTeamAbbr'])&(df['Team']=='away'))&(df['NflId']!=df['NflIdRusher'])].copy().reset_index(),\n      df.loc[df['NflId']==df['NflIdRusher']].copy().reset_index())\n    \n\ndef doubledown(X, doublings=1):\n    np.random.seed(3)\n    for w in range(doublings):\n        X_dupe2 = np.concatenate((X.copy(), X.copy()), axis=0)\n        for i in range(X.shape[0]):\n            X_dupe2[2*i, :] = X[i, :]\n            X_dupe2[2*i+1, :] = X[i, :]\n        X = X_dupe2\n\n    return X\n\n\ndef physics_init(df):\n    way = -2*(df['PlayDirection']=='left') + 1\n    theta = way*df['Dir']*np.pi\/180\n    df['X'] = (df['PlayDirection']=='right')*df['X'] + (df['PlayDirection']=='left')*(120 - df['X'])\n    df['Sx'] = np.sin(theta)*df['S']\n    df['Sy'] = np.cos(theta)*df['S']\n    df['Ax'] = np.sin(theta)*df['A']\n    df['Ay'] = np.cos(theta)*df['A']\n    df['EquivYardLine'] = (df['PossessionTeam']==df['FieldPosition'])*(df['YardLine']+10) + (df['PossessionTeam']!=df['FieldPosition'])*(110-df['YardLine'])\n\n    defn, off, RBs = organize_positions(df)\n\n    defn['X'] -= RBs.loc[[i\/\/11 for i in defn.index], 'X'].values\n    defn['Y'] -= RBs.loc[[i\/\/11 for i in defn.index], 'Y'].values\n    defn = defn.loc[:, ('PlayId', 'PlayerWeight', 'Sx', 'Sy', 'Ax', 'Ay', 'X', 'Y')]\n    defn.fillna(0, inplace=True)\n    defn['Infl'] = defn['PlayerWeight']\/(np.square(defn['X']) + np.square(defn['Y']))**0.5\n    defn['AngularMomentum'] = -defn['PlayerWeight']*(defn['X']*defn['Sx'] + defn['Y']*defn['Sy'])\/(np.square(defn['X']) + np.square(defn['Y']))\n\n    off['X'] -= RBs.loc[[i\/\/10 for i in off.index], 'X'].values\n    off['Y'] -= RBs.loc[[i\/\/10 for i in off.index], 'Y'].values\n    off = off.loc[:, ('PlayId', 'PlayerWeight', 'Sx', 'Sy', 'Ax', 'Ay', 'X', 'Y')]\n    off.fillna(0, inplace=True)\n    off['Infl'] = off['PlayerWeight']\/(np.square(off['X']) + np.square(off['Y']))**0.5\n    off['AngularMomentum'] = -off['PlayerWeight']*(off['X']*off['Sx'] + off['Y']*off['Sy'])\/(np.square(off['X']) + np.square(off['Y']))\n\n    RBs['YardsBehindScrimmage'] = RBs['EquivYardLine'] - RBs['X']\n    RBs['X'] = 0\n    RBs['Y'] = 0\n    RBs = RBs.loc[:, ('PlayId', 'YardsBehindScrimmage', 'PlayerWeight', 'Sx', 'Sy', 'Ax', 'Ay', 'X', 'Y')]\n    RBs.fillna(0, inplace=True)\n    \n    return defn, off, RBs\n\n\ndef action(defn, off, RBs, timestep=0.1):\n    t = 0.0\n    while t<timestep:\n        for X in (defn, off, RBs):\n            X['X'] += X['Sx']*0.01 +X['Ax']*0.01**2\/2\n            X['Y'] += X['Sy']*0.01 +X['Ay']*0.01**2\/2\n            X['Sx'] += X['Ax']*0.01\n            X['Sy'] += X['Ay']*0.01\n            X['Ax'] *= 0.99\n            X['Ay'] *= 0.99\n        t += 0.01\n\n        defn['X'] -= RBs.loc[[i\/\/11 for i in defn.index], 'X'].values\n        defn['Y'] -= RBs.loc[[i\/\/11 for i in defn.index], 'Y'].values\n        defn['Infl'] = defn['PlayerWeight']\/(np.square(defn['X']) + np.square(defn['Y']))**0.5\n        defn['AngularMomentum'] = -defn['PlayerWeight']*(defn['X']*defn['Sx'] + defn['Y']*defn['Sy'])\/(np.square(defn['X']) + np.square(defn['Y']))\n\n        off['X'] -= RBs.loc[[i\/\/10 for i in off.index], 'X'].values\n        off['Y'] -= RBs.loc[[i\/\/10 for i in off.index], 'Y'].values\n        off['Infl'] = off['PlayerWeight']\/(np.square(off['X']) + np.square(off['Y']))**0.5\n        off['AngularMomentum'] = -off['PlayerWeight']*(off['X']*off['Sx'] + off['Y']*off['Sy'])\/(np.square(off['X']) + np.square(off['Y']))\n\n        RBs['X'] = 0\n        RBs['Y'] = 0\n\n    return defn, off, RBs\n\n\ndef physics_doubledown(X, doublings, width):\n    np.random.seed(3)\n    for w in range(doublings):\n        X_dupe = X.copy()\n        X_dupe2 = np.concatenate((X.copy(), X.copy()), axis=0)\n        numpy_sucks = np.arange(11)\n        np.random.shuffle(numpy_sucks)\n        for (i,j) in enumerate(numpy_sucks):\n            X_dupe[:, width*i:width*i+width] = X[:, width*j:width*j+width]\n        numpy_sucks = np.arange(10)\n        np.random.shuffle(numpy_sucks)\n        for (i,j) in enumerate(numpy_sucks):\n            X_dupe[:, width*(i+11):width*(i+12)] = X[:, width*(j+11):width*(j+12)]\n        for i in range(X.shape[0]):\n            X_dupe2[2*i, :] = X[i, :]\n            X_dupe2[2*i+1, :] = X_dupe[i, :]\n        X = X_dupe2\n\n    return X\n\n\ndef generate_physics(df, forward_action=0, timestep=0.1, doublings=0):\n    d, o, r = physics_init(df)\n    df = None\n\n    defn = [d.copy()]\n    off = [o.copy()]\n    RBs = [r.copy()]\n\n    for a in range(forward_action):\n        d, o, r = action(d, o, r, timestep)\n        defn.append(d.copy())\n        off.append(o.copy())\n        RBs.append(r.copy())\n    d, o, r = None, None, None\n\n    for X in (defn, off, RBs):\n        for i in range(len(defn)):\n            X[i]['Px'] = X[i]['Sx']*X[i]['PlayerWeight']\n            X[i]['Py'] = X[i]['Sy']*X[i]['PlayerWeight']\n            X[i]['Fx'] = X[i]['Ax']*X[i]['PlayerWeight']\n            X[i]['Fy'] = X[i]['Ay']*X[i]['PlayerWeight']\n\n    for i in range(len(defn)):\n        if i==0:\n            bigD = defn[i].loc[:, ('Px', 'Py', 'X', 'Y', 'Infl', 'AngularMomentum')].astype(np.float32)\n            bigO = off[i].loc[:, ('Px', 'Py', 'X', 'Y', 'Infl', 'AngularMomentum')].astype(np.float32)\n            backs = RBs[i].loc[:, ('YardsBehindScrimmage', 'Px', 'Py', 'Fx', 'Fy')].astype(np.float32)\n            if bigD.shape[0]==0 | bigO.shape[0]==0:\n                bigD = pd.DataFrame(data=np.random.randn(11,6), columns=('Px', 'Py', 'X', 'Y', 'Infl', 'AngularMomentum'))\n                bigO = pd.DataFrame(data=np.random.randn(10,6), columns=('Px', 'Py', 'X', 'Y', 'Infl', 'AngularMomentum'))\n            inst = np.concatenate((np.reshape(bigD.copy().values, (bigD.shape[0]\/\/11, 11*6)), \n                                            np.reshape(bigO.copy().values, (bigO.shape[0]\/\/10, 10*6)), \n                                            backs.copy().values), axis=1)\n            summary = physics_doubledown(inst, doublings, 6)\n        else:\n            bigD = defn[i].loc[:, ('Infl', 'AngularMomentum')].astype(np.float32)\n            bigO = off[i].loc[:, ('Infl', 'AngularMomentum')].astype(np.float32)\n            if bigD.shape[0]==0 | bigO.shape[0]==0:\n                bigD = pd.DataFrame(data=np.random.randn(11,2), columns=('Infl', 'AngularMomentum'))\n                bigO = pd.DataFrame(data=np.random.randn(10,2), columns=('Infl', 'AngularMomentum'))\n            inst = np.concatenate((np.reshape(bigD.copy().values, (bigD.shape[0]\/\/11, 11*2)), \n                                            np.reshape(bigO.copy().values, (bigO.shape[0]\/\/10, 10*2))), axis=1)\n            summary = np.concatenate((summary, physics_doubledown(inst, doublings, 2)), axis=1)\n    defn, off, RBs = None, None, None\n\n    return summary\n\n\ndef generate_physics_II(df):\n    defn, off, RBs = physics_init(df)\n\n    bigD = defn[['PlayerWeight', 'X', 'Y', 'Sx', 'Sy', 'Ax', 'Ay']].astype(np.float32)\n    bigO = off[['PlayerWeight', 'X', 'Y', 'Sx', 'Sy', 'Ax', 'Ay']].astype(np.float32)\n    backs = RBs[['YardsBehindScrimmage', 'PlayerWeight', 'Sx', 'Sy', 'Ax', 'Ay']].astype(np.float32)\n    if bigD.shape[0]==0 | bigO.shape[0]==0:\n        bigD = pd.DataFrame(data=np.random.randn(11,7), columns=('PlayerWeight', 'X', 'Y', 'Sx', 'Sy', 'Ax', 'Ay'))\n        bigO = pd.DataFrame(data=np.random.randn(10,7), columns=('PlayerWeight', 'X', 'Y', 'Sx', 'Sy', 'Ax', 'Ay'))\n    summary = np.concatenate((backs.copy().values,\n                             np.reshape(bigD.copy().values, (bigD.shape[0]\/\/11, 11*7)), \n                              np.reshape(bigO.copy().values, (bigO.shape[0]\/\/10, 10*7))), axis=1)\n    defn, off, RBs = None, None, None\n    bigD, bigO, backs = None, None, None\n\n    return summary\n\n\ndef down_situation(df):\n    X = df.loc[::22, ('Down', 'Distance')].copy().astype(np.float32)\n    X.fillna(-1, inplace=True)\n    framer = pd.DataFrame(columns=(1.0, 2.0, 3.0, 4.0, 'Distance'), dtype=np.float32)\n    concatenation = pd.concat((pd.get_dummies(X['Down']), X['Distance']), axis=1, join='outer')\n    concatenation = pd.concat((framer, concatenation), axis=0, join='outer')\n    concatenation.fillna(0, inplace=True)\n    return concatenation.values\n\n\ndef RB_wins_it(df):\n    X = df.loc[::22, ('NflIdRusher')]\n    framer = pd.DataFrame(columns=baxxx, dtype=np.float32)\n    OHE = pd.get_dummies(X)\n    concatenation = pd.concat((framer, OHE.loc[:, :framer.shape[1]]), axis=0, join='outer')\n    concatenation.fillna(0, inplace=True)\n    return concatenation.values.astype(np.float32)\n\n\ndef stats(array):\n    return array.mean(axis=0), array.std(axis=0)\n\n\ndef stats_II(X):\n    means = X[:, :11].mean(axis=0)\n    deviants = X[:, :11].std(axis=0)\n    for start, step, amt in [(11,7,11), (88,7,10)]: #D and O\n        repmeans = []\n        repdevs = []\n        for i in range(step):\n            repmeans.append(X[:, start+i:start+step*amt:step].mean())\n            repdevs.append(X[:, start+i:start+step*amt:step].std())\n        for j in range(amt):\n            means = np.concatenate((means, repmeans))\n            deviants = np.concatenate((deviants, repdevs))\n    means = np.concatenate((means, [0 for i in range(158, 529)]), axis=0)\n    deviants = np.concatenate((deviants, [1 for i in range(158, 529)]), axis=0)\n    return means, deviants\n\n\ndef normalize(X, mn, stand):\n    return (X - mn) \/ stand\n\n\ndef generate_yardudge(df):\n    Y = np.zeros((df.shape[0]\/\/22, 199))\n    for (i, yerds) in enumerate(df['Yards'][::22]):\n        Y[i, yerds+99] = 1\n    return Y.astype(np.float32)","0f3f0b44":"# global meen_I, sigma_I, PCs, doublings, forward_actions, timestep\n\n# train_df = pd.read_csv('\/kaggle\/input\/nfl-big-data-bowl-2020\/train.csv', low_memory=False)\n\n# doublings = 5\n# forward_actions = 8\n# timestep = 0.1\n\n# Y = doubledown(generate_yardudge(train_df), doublings)\n\n# train_df = error_correcting_codes(train_df)\n# X = np.concatenate(\n#                 (doubledown(down_situation(train_df), doublings), \n#                  generate_physics(train_df, forward_actions, timestep, doublings)), \n#             axis=1)\n# train_df = None\n\n# meen_I, sigma_I = stats(X)\n# X = normalize(X, meen_I, sigma_I)\n\n# PCs = np.linalg.eig(np.dot(np.transpose(X), X))[1]\n# X = np.dot(X, PCs)\n\n# gc.collect()","60d6da15":"# stuff = [0.01541, 0.00000173, 0.8858, 0.3867, 0.6044, 14, 180, 1024, 512, 256]\n\n# learning_rate = stuff[0]\n# decay = stuff[1]\n# beta = stuff[2]\n# clipse = stuff[3]\n# dropout_rate = stuff[4]\n# epochs = stuff[5]\n# batch_size = stuff[6]\n# hidden_layer1_size = int(round(stuff[7]))\n# hidden_layer2_size = int(round(stuff[8]))\n# hidden_layer3_size = int(round(stuff[9]))\n\n# np.random.seed(1729)\n# model_NN1 = Sequential()\n# model_NN1.add(Dense(units=hidden_layer1_size, activation='relu'))\n# model_NN1.add(Dropout(dropout_rate))\n# model_NN1.add(Dense(units=hidden_layer2_size, activation='relu'))\n# model_NN1.add(Dropout(dropout_rate))\n# model_NN1.add(Dense(units=hidden_layer3_size, activation='relu'))\n# model_NN1.add(Dropout(dropout_rate))\n# model_NN1.add(Dense(units=199, activation='softmax'))\n# model_NN1.compile(loss=categorical_crossentropy, optimizer=SGD(lr=learning_rate, decay=decay, momentum=beta, nesterov=True, clipnorm=clipse))\n# model_NN1.fit(X, Y, epochs=epochs, batch_size=batch_size) #,  validation_split=0.1)","1aaa43ef":"# test_X = X[20853*2**doublings:]\n# P_NN1_partsy = model_NN1.predict(test_X)\n# P_NN1 = None\n# for i in range(P_NN1_partsy.shape[0]\/\/2**doublings):\n#     if P_NN1 is None:\n#         P_NN1 = np.reshape(np.mean(P_NN1_partsy[2**doublings*i:2**doublings*(i+1), :], axis=0), (1, 199))\n#     else:\n#         P_NN1 = np.concatenate((P_NN1, np.reshape(np.mean(P_NN1_partsy[2**doublings*i:2**doublings*(i+1), :], axis=0), (1, 199))), axis=0)","ec29abc9":"# X, Y = None, None\n# gc.collect()","0ca9291c":"global meen_II, sigma,_II, baxxx\n\ntrain_df = pd.read_csv('\/kaggle\/input\/nfl-big-data-bowl-2020\/train.csv', low_memory=False)\n\nbaxxx = train_df['NflIdRusher'].unique()\n\nY = generate_yardudge(train_df)\n\ntrain_df = error_correcting_codes(train_df)\n\nX = np.concatenate((down_situation(train_df), \n                    generate_physics_II(train_df),\n                    RB_wins_it(train_df)), axis=1)\ntrain_df = None\n\nmeen_II, sigma_II = stats_II(X)\nX = normalize(X, meen_II, sigma_II)\n\ngc.collect()","806da56f":"stuff = [4.81725572e-02, 1.48089908e-05, 9.57455511e-01, 1.58078791e+00,\n       9.93707682e-02, 5.04421303e-01, 6.51849665e+01, 9.68255755e+02,\n       5.14218542e+02, 5.09462973e+02, 5.17812944e+02, 8.04146527e+02,\n       1.64480385e+02, 9.80945456e+02, 8.50852251e+01, 1.03668046e+03,\n       6.57080119e+01]\n\nlearning_rate = stuff[0]\nlr_decay =stuff[1]\nbeta = stuff[2]\nclipse = stuff[3]\ndrop_rate1 = stuff[4]\ndrop_rate2 = stuff[5]\nepochs = int(round(stuff[6]))\nbatch_size = int(round(stuff[7]))\nhidden_layer1_nodes = int(round(stuff[8]))\nhidden_layer2_nodes = int(round(stuff[9]))\nhidden_layer3_nodes = int(round(stuff[10]))\nfunction_layer1_nodes = int(round(stuff[11]))\nfunction_layer2_nodes = int(round(stuff[12]))\nfunction_layer3_nodes = int(round(stuff[13]))\nfunction_layer4_nodes = int(round(stuff[14]))\nfunction_layer5_nodes = int(round(stuff[15]))\nfunction_layer6_nodes = int(round(stuff[16]))\n\nipt = [Input(shape=(11,))] + [Input(shape=(7,)) for i in range(11)] + \\\n[Input(shape=(7,)) for i in range(10)] + [Input(shape=(baxxx.shape[0],))]\n\nnp.random.seed(314159)\n\nexpanse = Dense(function_layer1_nodes, activation='relu') \ncollapsor = Dense(function_layer2_nodes, activation='linear')\nexpanse2 = Dense(function_layer3_nodes, activation='relu')\ncollapsor2 = Dense(function_layer4_nodes, activation='linear')\nfaux_embedding_expanse = Dense(function_layer5_nodes, activation='relu') \nfaux_embedding_collapsor = Dense(function_layer6_nodes, activation='linear') \nD = Add()([collapsor(expanse(ipt[i])) for i in range(1,12)])\nO = Add()([collapsor(expanse(ipt[i])) for i in range(12,22)])\nR = faux_embedding_collapsor(faux_embedding_expanse(ipt[22]))\np = concatenate([ipt[0], D, O, R], axis=-1)\nx = Dropout(drop_rate1)(p)\nx = Dense(hidden_layer1_nodes, activation='relu')(x)\nx = Dropout(drop_rate2)(x)\nx = Dense(hidden_layer2_nodes, activation='relu')(x)\nx = Dropout(drop_rate2)(x)\nx = Dense(hidden_layer3_nodes, activation='relu')(x)\nx = Dropout(drop_rate2)(x)\noutput = Dense(199, activation='softmax')(x)\n\nmodel_NN2 = Model(inputs=ipt, outputs=output)\n\nmodel_NN2.compile(optimizer=SGD(lr=learning_rate, decay=lr_decay, momentum=beta, nesterov=True, clipnorm=clipse), loss=categorical_crossentropy)\nmodel_NN2.fit([X[:,:11]]+[X[:,7*i+11:7*i+18] for i in range(0,11)]+\n                   [X[:,7*i+11:7*i+18] for i in range(11,21)]+[X[:,158:]],\n                   Y, epochs=epochs, batch_size=batch_size) #, validation_split=0.1)\n\ngc.collect()","4bf6e9e2":"# P_NN2 = model_NN2.predict([X[20853:,:11]]+[X[20853:,7*i+11:7*i+18] for i in range(0,11)]+\n#                        [X[20853:,7*i+11:7*i+18] for i in range(11,21)]+[X[20853:,158:]])","39398c02":"# global meen_III, sigma_III, PCs_III, forward_actions, timestep\n\n# forward_actions = 8\n# timestep = 0.1\n\n# train_df = pd.read_csv('\/kaggle\/input\/nfl-big-data-bowl-2020\/train.csv', low_memory=False)\n\n# Y = generate_yardudge(train_df)\n\n# train_df = error_correcting_codes(train_df)\n# X = np.concatenate(\n#                 (down_situation(train_df), \n#                  generate_physics(train_df, forward_actions, timestep)), \n#             axis=1)\n# train_df = None\n\n# meen_III, sigma_III = stats(X)\n# X = normalize(X, meen_III, sigma_III)\n\n# PCs_III = np.linalg.eig(np.dot(np.transpose(X), X))[1]\n# X = np.dot(X, PCs_III)\n\n# gc.collect()","c36f2fb5":"Yxgb = np.argmax(Y, axis=1) - 99\nmodel_xgb1 = XGBClassifier(objective='multi:softmax', seed=27182818, max_depth=1)\nmodel_xgb1.fit(X, Yxgb)","962e918a":"# offset = int((modelo.predict(X[20853:20854,:]) - np.argmax(modelo.predict_proba(X[20853:20854, :]), axis=1))[0])\n# P_xgb1 = modelo.predict_proba(X[20853:, :])\n# P_xgb1 = np.concatenate((np.zeros(Y.shape[0], 99+offset), P_xgb1, np.ones(Y.shape[0], 100-P_xgb1.shape[1]-offset)), axis=1)\n# P_xgb1.shape","63677e2d":"# def CRPS(P, Y):\n#     cucumber = np.zeros(P.shape[0])\n#     C = np.zeros(P.shape)\n#     for i in range(1, P.shape[1]):\n#         cucumber = cucumber + np.maximum(P[:, i], 0)\n#         C[:, i] = np.minimum(cucumber, 1)\n#     N = np.matlib.repmat(np.reshape([i for i in range(-99,100)], (1, 199)), P.shape[0], 1)\n#     Ynot = np.matlib.repmat(Y, 1, 199)\n#     return np.sum(np.square(C - (N>=Y)))\/199\/C.shape[0]","7e5abd9d":"# best = 1000\n# optimo = None\n# for i in np.arange(0,1.01,0.01):\n#     junk = i*P_NN2 + (1-i)*P_xgb1\n#     noo = CRPS(junk, Y[20853:, :])\n#     if noo < best:\n#         best = noo\n#         optimo = (i, j, 1-i-j)\n# print(optimo)","e6d96911":"# create an nfl environment\nenv = nflrush.make_env()","57979a06":"def make_my_predictions(model1, model2, model3, test_df, sample_predictions):\n    test_df = error_correcting_codes(test_df)\n#     # Neural Network I\n\n#     X_downer = doubledown(down_situation(test_df.copy()), doublings)\n#     X_phys = generate_physics(test_df.copy(), forward_actions, timestep, doublings)\n#     X = np.concatenate((X_downer, X_phys), axis=1)\n#     X = normalize(X, meen_I, sigma_I)\n#     X = np.dot(X, PCs)\n#     my_predictions_I = np.mean(model1.predict(X), axis=0)\n\n    # Neural Network II\n    X = np.concatenate((down_situation(test_df.copy()), \n                    generate_physics_II(test_df.copy()),\n                    RB_wins_it(test_df.copy())), axis=1)\n    X = normalize(X, meen_II, sigma_II)\n    my_predictions_II = model2.predict([X[:,:11]]+[X[:,7*i+11:7*i+18] for i in range(0,11)]+\n                   [X[:,7*i+11:7*i+18] for i in range(11,21)]+[X[:,158:]])\n    \n    # XGB I\n#     X = np.concatenate(\n#                 (down_situation(test_df.copy()), \n#                  generate_physics(test_df.copy(), forward_actions, timestep)), \n#             axis=1)\n#     X = normalize(X, meen_III, sigma_III)\n#     X = np.dot(X, PCs_III)\n    offset = int((model3.predict(X) - np.argmax(model3.predict_proba(X), axis=1))[0])\n    my_predictions_III = model3.predict_proba(X)\n    my_predictions_III = np.concatenate((np.zeros((1, 99+offset)), my_predictions_III, np.ones((1, 100-my_predictions_III.shape[1]-offset))), axis=1)\n\n    # Ensemble\n#     my_predictions = 0.8*(0.62*my_predictions_I + 0.38*my_predictions_II) + 0.2*my_predictions_III\n    my_predictions = np.reshape(0.62*my_predictions_II + 0.38*my_predictions_III, (199,0))\n#     my_predictions = np.reshape(my_predictions_III, (199,0))\n    \n    \n    cucumber = 0\n    for i in range(1, my_predictions.shape[0]-1):\n        cucumber += np.maximum(my_predictions[i], 0)\n        sample_predictions.iloc[0, i] = np.minimum(cucumber, 1)\n\n    return sample_predictions","4b45b2a1":"for (test_df, sample_prediction_df) in env.iter_test():\n    predictions_df = make_my_predictions(model_xgb1, model_NN2, model_xgb1, test_df, sample_prediction_df)\n    env.predict(predictions_df)\n        \nenv.write_submission_file()","4af15517":"# Ensembling Studies","caa3c9c8":"# XGBoost I","53bb8ef5":"# Neural Network II","4915a29c":"# Neural Network I"}}