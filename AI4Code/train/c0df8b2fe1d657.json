{"cell_type":{"d24e1bc2":"code","925dca12":"code","1ff4fe7e":"code","5ee131dc":"code","6fc893ec":"code","87e1d12a":"code","30799d19":"code","d3225eb9":"code","bf3baa22":"code","2bca9739":"code","57f7e374":"markdown"},"source":{"d24e1bc2":"\"\"\"\nCreated on Fri Oct  5 15:03:37 2018\n\n@author: alex\n\"\"\"\n# test on\n#iris\n#MNIST\n\n\nfrom keras import losses, models, optimizers\nfrom keras.models import Sequential\nfrom keras.wrappers.scikit_learn import KerasRegressor\nfrom keras.activations import relu, softmax\nfrom keras.callbacks import (EarlyStopping, LearningRateScheduler,\n                             ModelCheckpoint, TensorBoard, ReduceLROnPlateau)\nfrom keras.layers import (Convolution2D, Dense, Dropout, GlobalAveragePooling2D, \n                              GlobalMaxPool2D, Input, MaxPool2D, concatenate, Activation,  \n                              MaxPooling2D,Flatten,BatchNormalization, Conv2D,AveragePooling2D)\nfrom keras.utils import Sequence, to_categorical\nfrom sklearn.datasets import load_iris \nfrom sklearn.datasets import make_moons \nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.tree import export_graphviz\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import KFold\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.pipeline import Pipeline\nfrom matplotlib import pyplot as plt\nimport numpy as np\nimport seaborn as sns\nimport pandas as pd\n","925dca12":"# Moons classification\ndata_moons = make_moons(n_samples=100, shuffle=True, noise=0.1, random_state=1)\nd = {'x': data_moons[0][:,0], 'y': data_moons[0][:,1], 'target': data_moons[1]}\nmoons = pd.DataFrame(d)\nsns.pairplot(moons, hue=\"target\")\nprint('moons.x',moons.x)\nprint('moons.y',moons.y)\nprint('moons.target', moons.target)\n","1ff4fe7e":"data_iris = load_iris()\nx = pd.DataFrame(data_iris.data)\ny = data_iris.target\nclass_names = data_iris.target_names\n# \u0421\u0442\u0432\u043e\u0440\u0435\u043d\u043d\u044f \u043e\u0431'\u0454\u043a\u0442\u0430 figure\nsns.pairplot(x)","5ee131dc":"moonsX_train, moonsX_test, moonsY_train, moonsY_test = train_test_split(data_moons[0], data_moons[1], \n                                                    test_size=0.4, random_state=0)    \n\nirisX_train, irisX_test, irisY_train, irisY_test = train_test_split(data_iris.data, data_iris.target, \n                                                    test_size=0.4, random_state=0)    \n","6fc893ec":"print('data_moons',np.shape(data_moons[0]))\nprint('moonsX_train',np.shape(moonsX_train))","87e1d12a":"print('data_moons',np.shape(data_moons[1]))\nprint('moonsX_train',np.shape(moonsY_train))","30799d19":"print(irisY_train)","d3225eb9":"# \u041f\u043e\u0431\u0443\u0434\u043e\u0432\u0430 \u043c\u043e\u0434\u0435\u043b\u0456 \u043b\u043e\u0433\u0456\u0441\u0442\u0438\u0447\u043d\u043e\u0457 \u0440\u0435\u0433\u0440\u0435\u0441\u0456\u0457\n# moons \nclf = LogisticRegression(random_state=0, solver='lbfgs', multi_class='ovr')\nclf = clf.fit(moonsX_train, moonsY_train)\nmoonsY_pred = clf.predict(moonsX_test)\nlog_regr_score1 = clf.score(moonsX_test, moonsY_test)\nprint('log_regr_score ',log_regr_score1)\nprint(confusion_matrix(moonsY_test, moonsY_pred))\n\nclf = LogisticRegression(random_state=0, solver='lbfgs', multi_class='multinomial')\nclf = clf.fit(irisX_train, irisY_train)\nirisY_pred = clf.predict(irisX_test)\nlog_regr_score2 = clf.score(irisX_test, irisY_test)\nprint('log_regr_score ',log_regr_score2)\nprint(confusion_matrix(irisY_test, irisY_pred))\n","bf3baa22":"# Decision Tree\n\nclf = DecisionTreeClassifier(max_depth=10)\nclf = clf.fit(moonsX_train, moonsY_train)\nmoonsY_pred = clf.predict(moonsX_test)\ndec_tree_score1 = clf.score(moonsX_test, moonsY_test)\nprint('dec_tree_score ',dec_tree_score1)\nconfusion_matrix(moonsY_test, moonsY_pred)\n\nclf = DecisionTreeClassifier(max_depth=10)\nclf = clf.fit(irisX_train, irisY_train)\nirisY_pred = clf.predict(irisX_test)\ndec_tree_score2 = clf.score(irisX_test, irisY_test)\nprint('dec_tree_score ',dec_tree_score2)\nconfusion_matrix(irisY_test, irisY_pred)\n# \u0433\u0440\u0430\u0444\u0456\u0447\u043d\u0435 \u0437\u043e\u0431\u0440\u0430\u0436\u0435\u043d\u043d\u044f \u0434\u0435\u0440\u0435\u0432\u0430 \u0440\u0456\u0448\u0435\u043d\u044c\nexport_graphviz(clf, out_file='tree.dot')\n# \u0434\u043b\u044f \u0456\u043d\u0441\u0442\u0430\u043b\u044f\u0446\u0456\u0457 \u0431\u0456\u0431\u043b\u0456\u043e\u0442\u0435\u043a\u0438 python-graphviz:\n# ..\/>conda install python-graphviz\n# ..\/>dot -Tpng filename.dot -o filename.png\n","2bca9739":"\n# RandomForestClassifier\n\nclf = RandomForestClassifier(n_estimators=100, max_depth=2,random_state=0)\nclf = clf.fit(moonsX_train, moonsY_train)\nmoonsY_pred = clf.predict(moonsX_test)\nrand_frst_score1 = clf.score(moonsX_test, moonsY_test)\nprint('dec_tree_score ',rand_frst_score1)\nconfusion_matrix(moonsY_test, moonsY_pred)\n\nclf = RandomForestClassifier(n_estimators=100, max_depth=2,random_state=0)\nclf = clf.fit(irisX_train, irisY_train)\nirisY_pred = clf.predict(irisX_test)\nrand_frst_score2 = clf.score(irisX_test, irisY_test)\nprint('dec_tree_score ',rand_frst_score2)\nconfusion_matrix(irisY_test, irisY_pred)\n\n# GradientBoostingClassifierx \n\nclf = RandomForestClassifier()\nclf = clf.fit(moonsX_train, moonsY_train)\nmoonsY_pred = clf.predict(moonsX_test)\ngrand_boost_score1 = clf.score(moonsX_test, moonsY_test)\nprint('dec_tree_score ',grand_boost_score1)\nconfusion_matrix(moonsY_test, moonsY_pred)\n\nclf = RandomForestClassifier()\nclf = clf.fit(irisX_train, irisY_train)\nirisY_pred = clf.predict(irisX_test)\ngrand_boost_score2 = clf.score(irisX_test, irisY_test)\nprint('dec_tree_score ',grand_boost_score2)\nconfusion_matrix(irisY_test, irisY_pred)\n\n# KNeighborsClassifier \n\nclf = KNeighborsClassifier(n_neighbors=3)\nclf = clf.fit(moonsX_train, moonsY_train)\nmoonsY_pred = clf.predict(moonsX_test)\nKNbrs_score1 = clf.score(moonsX_test, moonsY_test)\nprint('KNbrs_score ',grand_boost_score1)\nconfusion_matrix(moonsY_test, moonsY_pred)\n\nclf = KNeighborsClassifier(n_neighbors=3)\nclf = clf.fit(irisX_train, irisY_train)\nirisY_pred = clf.predict(irisX_test)\nKNbrs_score2 = clf.score(irisX_test, irisY_test)\nprint('KNbrs_score ',grand_boost_score2)\nconfusion_matrix(irisY_test, irisY_pred)\n\n# Support vector machine\n\nclf = SVC(gamma='auto')\nclf = clf.fit(moonsX_train, moonsY_train)\nmoonsY_pred = clf.predict(moonsX_test)\nKNbrs_score1 = clf.score(moonsX_test, moonsY_test)\nprint('KNbrs_score ',grand_boost_score1)\nconfusion_matrix(moonsY_test, moonsY_pred)\n\nclf = SVC(gamma='auto')\nclf = clf.fit(irisX_train, irisY_train)\nirisY_pred = clf.predict(irisX_test)\nKNbrs_score2 = clf.score(irisX_test, irisY_test)\nprint('KNbrs_score ',grand_boost_score2)\nconfusion_matrix(irisY_test, irisY_pred)\n\n# Naive Bayes\n\nclf = GaussianNB()\nclf = clf.fit(moonsX_train, moonsY_train)\nmoonsY_pred = clf.predict(moonsX_test)\nNB_score1 = clf.score(moonsX_test, moonsY_test)\nprint('NB_score ',NB_score1)\nconfusion_matrix(moonsY_test, moonsY_pred)\n\nclf = GaussianNB()\nclf = clf.fit(irisX_train, irisY_train)\nirisY_pred = clf.predict(irisX_test)\nNB_score2 = clf.score(irisX_test, irisY_test)\nprint('NB_score ',NB_score2)\nconfusion_matrix(irisY_test, irisY_pred)\n\n# MLPClassifier\n\nclf = MLPClassifier(hidden_layer_sizes=(100,100,100), max_iter=500, alpha=0.0001,\n                     solver='sgd', verbose=10,  random_state=21,tol=0.000000001)\nclf = clf.fit(moonsX_train, moonsY_train)\nmoonsY_pred = clf.predict(moonsX_test)\nmlp_score1 = clf.score(moonsX_test, moonsY_test)\nprint('NB_score ',mlp_score1)\nconfusion_matrix(moonsY_test, moonsY_pred)\n\n\nclf = MLPClassifier(hidden_layer_sizes=(100,100,100), max_iter=500, alpha=0.0001,\n                     solver='sgd', verbose=10,  random_state=21,tol=0.000000001)\nclf = clf.fit(irisX_train, irisY_train)\nirisY_pred = clf.predict(irisX_test)\nmlp_score2 = clf.score(irisX_test, irisY_test)\nprint('MLP_score ',mlp_score2)\nconfusion_matrix(irisY_test, irisY_pred)\n\n### Deep Neural Net with Keras\n\nkernel_initializer='lecun_uniform'\nbias_initializer='zeros'\nkernel_regularizer=None\nactivation = \"selu\"\nnb_epoch = 300 # \u041a\u0456\u043b\u044c\u043a\u0456\u0441\u0442\u044c \u0435\u043f\u043e\u0445 \u043d\u0430\u0432\u0447\u0430\u043d\u043d\u044f\nalpha_zero = 0.001 # \u041a\u043e\u0435\u0444\u0456\u0446\u0456\u0454\u043d\u0442 \u0448\u0432\u0438\u0434\u043a\u043e\u0441\u0442\u0456 \u043d\u0430\u0432\u0447\u0430\u043d\u043d\u044f\nbatch_size = 5\n\nirisY_train = to_categorical(irisY_train, num_classes=3)\nirisY_test = to_categorical(irisY_test, num_classes=3)\n\nmodel = Sequential()\n############ \u0414\u043e\u0434\u0430\u0432\u0430\u043d\u043d\u044f \u043f\u043e\u0432\u043d\u043e\u0437\u0432'\u044f\u0437\u043d\u043e\u0433\u043e \u0448\u0430\u0440\u0443 \n#model.add(Flatten())\nmodel.add(Dense(20, input_dim =4  , activation = activation))\nmodel.add(Dropout(0.4))\nmodel.add(Dense(15, activation = activation))\nmodel.add(Dropout(0.4))\nmodel.add(Dense(10, activation = activation))\nmodel.add(Dropout(0.4))\nmodel.add(Dense(5, activation = activation))\nmodel.add(Dense(3,kernel_initializer=kernel_initializer,\n                bias_initializer=bias_initializer, activation = \"softmax\"))\n############ \u041a\u043e\u043c\u043f\u0456\u043b\u044f\u0446\u0456\u044f \u043c\u043e\u0434\u0435\u043b\u0456\noptimizer = optimizers.Nadam(lr=alpha_zero, beta_1=0.9, beta_2=0.999, \n                             epsilon=None, schedule_decay=0.004)\nmodel.compile(loss = \"categorical_crossentropy\", optimizer = optimizer, \n              metrics = [\"accuracy\"])\nhistory = model.fit(irisX_train, irisY_train, batch_size = batch_size, \n                    epochs = nb_epoch, verbose=2, validation_data = (irisX_test, irisY_test))\nscore = model.evaluate(irisX_test, irisY_test,verbose = 0)\nprint(\"score for MLP is \", score[1])\nmodel.summary()\n\n# list all data in history\nprint(history.history.keys())\n# summarize history for accuracy\nplt.plot(history.history['acc'], 'k-')\nplt.plot(history.history['val_acc'], 'k:')\nplt.title('\u0422\u043e\u0447\u043d\u0456\u0441\u0442\u044c \u043c\u043e\u0434\u0435\u043b\u0456')\nplt.ylabel('\u0442\u043e\u0447\u043d\u0456\u0441\u0442\u044c')\nplt.xlabel('\u0435\u043f\u043e\u0445\u0438')\nplt.legend(['\u0442\u0440\u0435\u043d\u0443\u0432\u0430\u043d\u043d\u044f', '\u0442\u0435\u0441\u0442'], loc='upper left')\nplt.show()\n# summarize history for loss\nplt.plot(history.history['loss'], 'k-')\nplt.plot(history.history['val_loss'], 'k:')\nplt.title('\u041f\u043e\u0445\u0438\u0431\u043a\u0430 \u043c\u043e\u0434\u0435\u043b\u0456')\nplt.ylabel('\u043f\u043e\u0445\u0438\u0431\u043a\u0430')\nplt.xlabel('\u0435\u043f\u043e\u0445\u0438')\nplt.legend(['\u0442\u0440\u0435\u043d\u0443\u0432\u0430\u043d\u043d\u044f', '\u0442\u0435\u0441\u0442'], loc='upper left')\nplt.show()\n","57f7e374":"\u041f\u043e\u0434\u0433\u043e\u0442\u043e\u0432\u043a\u0430 \u043d\u0430\u0431\u043e\u0440\u043e\u0432 \u043e\u0431\u0443\u0447\u0435\u043d\u0438\u044f\\\u0442\u0435\u0441\u0442\u0438\u0440\u043e\u0432\u0430\u043d\u0438\u044f"}}