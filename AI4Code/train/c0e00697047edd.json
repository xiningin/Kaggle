{"cell_type":{"ca6e5881":"code","7a040705":"code","66e3dee4":"code","5568ef01":"code","0a08df05":"code","59d93fd3":"code","04d412b8":"code","5066a240":"code","98df467a":"code","e567af76":"code","2b6401de":"code","dc17c2be":"code","b78e6430":"code","63c53fc6":"code","478cf341":"code","9162b5eb":"code","37b965ef":"code","0ac4c1a1":"code","8affdaf2":"code","2e2e95cb":"code","d8126b9f":"code","b8340ec2":"code","fcb57133":"code","9acf5e55":"code","6de9c6e0":"code","5c9b181a":"code","62474405":"code","6bd74dd5":"code","56ff7105":"code","0e961987":"code","1c55ee37":"code","ec2711fa":"code","7d7b5cdb":"code","a59c2b97":"code","ff83f64c":"code","0b3fc1f1":"markdown","082ed705":"markdown","310036b8":"markdown","d34f364e":"markdown","83a51b97":"markdown","1b071365":"markdown","fa105cff":"markdown","ad8ad809":"markdown","f75253f6":"markdown","759b0f62":"markdown","8679dabc":"markdown","5c4eeb66":"markdown","ed21a2ff":"markdown","f7b19d64":"markdown","7afa9822":"markdown","ae8205f8":"markdown","12b1da6e":"markdown","144664b5":"markdown","5ab78de5":"markdown","d0db42c4":"markdown"},"source":{"ca6e5881":"%matplotlib inline\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nimport numpy as np\nimport matplotlib\nimport matplotlib.pyplot as plt","7a040705":"from sklearn.datasets import make_blobs\n\nX, y = make_blobs(n_features=2, centers=3, n_samples=500,\n                  random_state=42)","66e3dee4":"X.shape","5568ef01":"plt.figure()\nplt.scatter(X[:, 0], X[:, 1])\nplt.show()","0a08df05":"from sklearn.neighbors.kde import KernelDensity\n\n# Estimate density with a Gaussian kernel density estimator\nkde = KernelDensity(kernel='gaussian')\nkde = kde.fit(X)\nkde","59d93fd3":"kde_X = kde.score_samples(X)\nprint(kde_X.shape)  # contains the log-likelihood of the data. The smaller it is the rarer is the sample","04d412b8":"from scipy.stats.mstats import mquantiles\nalpha_set = 0.95\ntau_kde = mquantiles(kde_X, 1. - alpha_set)","5066a240":"n_samples, n_features = X.shape\nX_range = np.zeros((n_features, 2))\nX_range[:, 0] = np.min(X, axis=0) - 1.\nX_range[:, 1] = np.max(X, axis=0) + 1.\n\nh = 0.1  # step size of the mesh\nx_min, x_max = X_range[0]\ny_min, y_max = X_range[1]\nxx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n                     np.arange(y_min, y_max, h))\n\ngrid = np.c_[xx.ravel(), yy.ravel()]","98df467a":"Z_kde = kde.score_samples(grid)\nZ_kde = Z_kde.reshape(xx.shape)\n\nplt.figure()\nc_0 = plt.contour(xx, yy, Z_kde, levels=tau_kde, colors='red', linewidths=3)\nplt.clabel(c_0, inline=1, fontsize=15, fmt={tau_kde[0]: str(alpha_set)})\nplt.scatter(X[:, 0], X[:, 1])\nplt.show()","e567af76":"from sklearn.svm import OneClassSVM","2b6401de":"nu = 0.05  # theory says it should be an upper bound of the fraction of outliers\nocsvm = OneClassSVM(kernel='rbf', gamma=0.05, nu=nu)\nocsvm.fit(X)","dc17c2be":"X_outliers = X[ocsvm.predict(X) == -1]","b78e6430":"Z_ocsvm = ocsvm.decision_function(grid)\nZ_ocsvm = Z_ocsvm.reshape(xx.shape)\n\nplt.figure()\nc_0 = plt.contour(xx, yy, Z_ocsvm, levels=[0], colors='red', linewidths=3)\nplt.clabel(c_0, inline=1, fontsize=15, fmt={0: str(alpha_set)})\nplt.scatter(X[:, 0], X[:, 1])\nplt.scatter(X_outliers[:, 0], X_outliers[:, 1], color='red')\nplt.show()","63c53fc6":"X_SV = X[ocsvm.support_]\nn_SV = len(X_SV)\nn_outliers = len(X_outliers)\n\nprint('{0:.2f} <= {1:.2f} <= {2:.2f}?'.format(1.\/n_samples*n_outliers, nu, 1.\/n_samples*n_SV))","478cf341":"plt.figure()\nplt.contourf(xx, yy, Z_ocsvm, 10, cmap=plt.cm.Blues_r)\nplt.scatter(X[:, 0], X[:, 1], s=1.)\nplt.scatter(X_SV[:, 0], X_SV[:, 1], color='orange')\nplt.show()","9162b5eb":"# %load solutions\/22_A-anomaly_ocsvm_gamma.py\n\nnu = 0.05  # theory says it should be an upper bound of the fraction of outliers\n\nfor gamma in [0.001, 1.]:\n    ocsvm = OneClassSVM(kernel='rbf', gamma=gamma, nu=nu)\n    ocsvm.fit(X)\n\n    Z_ocsvm = ocsvm.decision_function(grid)\n    Z_ocsvm = Z_ocsvm.reshape(xx.shape)\n\n    plt.figure()\n    c_0 = plt.contour(xx, yy, Z_ocsvm, levels=[0], colors='red', linewidths=3)\n    plt.clabel(c_0, inline=1, fontsize=15, fmt={0: str(alpha_set)})\n    plt.scatter(X[:, 0], X[:, 1])\n    plt.scatter(X_outliers[:, 0], X_outliers[:, 1], color='red')\n    plt.legend()\n    plt.show()\n","37b965ef":"from sklearn.ensemble import IsolationForest","0ac4c1a1":"iforest = IsolationForest(n_estimators=300, contamination=0.10)\niforest = iforest.fit(X)","8affdaf2":"### Z_iforest = iforest.decision_function(grid)\nZ_iforest = Z_iforest.reshape(xx.shape)\n\nplt.figure()\nc_0 = plt.contour(xx, yy, Z_iforest,\n                  levels=[iforest.threshold_],\n                  colors='red', linewidths=3)\nplt.clabel(c_0, inline=1, fontsize=15,\n           fmt={iforest.threshold_: str(alpha_set)})\nplt.scatter(X[:, 0], X[:, 1], s=1.)\nplt.show()","2e2e95cb":"# %load solutions\/22_B-anomaly_iforest_n_trees.py\nfor n_estimators in [1, 10, 50, 100]:\n    iforest = IsolationForest(n_estimators=n_estimators, contamination=0.10)\n    iforest = iforest.fit(X)\n\n    Z_iforest = iforest.decision_function(grid)\n    Z_iforest = Z_iforest.reshape(xx.shape)\n\n    plt.figure()\n    c_0 = plt.contour(xx, yy, Z_iforest,\n                      levels=[iforest.threshold_],\n                      colors='red', linewidths=3)\n    plt.clabel(c_0, inline=1, fontsize=15,\n               fmt={iforest.threshold_: str(alpha_set)})\n    plt.scatter(X[:, 0], X[:, 1], s=1.)\n    plt.legend()\n    plt.show()","d8126b9f":"from sklearn.datasets import load_digits\ndigits = load_digits()","b8340ec2":"images = digits.images\nlabels = digits.target\nimages.shape","fcb57133":"i = 102\n\nplt.figure(figsize=(2, 2))\nplt.title('{0}'.format(labels[i]))\nplt.axis('off')\nplt.imshow(images[i], cmap=plt.cm.gray_r, interpolation='nearest')\nplt.show()","9acf5e55":"n_samples = len(digits.images)\ndata = digits.images.reshape((n_samples, -1))","6de9c6e0":"data.shape","5c9b181a":"X = data\ny = digits.target","62474405":"X.shape","6bd74dd5":"X_5 = X[y == 5]","56ff7105":"X_5.shape","0e961987":"fig, axes = plt.subplots(1, 5, figsize=(10, 4))\nfor ax, x in zip(axes, X_5[:5]):\n    img = x.reshape(8, 8)\n    ax.imshow(img, cmap=plt.cm.gray_r, interpolation='nearest')\n    ax.axis('off')","1c55ee37":"from sklearn.ensemble import IsolationForest\niforest = IsolationForest(contamination=0.05)\niforest = iforest.fit(X_5)\n","ec2711fa":"iforest_X = iforest.decision_function(X_5)\nplt.hist(iforest_X);","7d7b5cdb":"X_strong_inliers = X_5[np.argsort(iforest_X)[-10:]]\n\nfig, axes = plt.subplots(2, 5, figsize=(10, 5))\n\nfor i, ax in zip(range(len(X_strong_inliers)), axes.ravel()):\n    ax.imshow(X_strong_inliers[i].reshape((8, 8)),\n               cmap=plt.cm.gray_r, interpolation='nearest')\n    ax.axis('off')","a59c2b97":"fig, axes = plt.subplots(2, 5, figsize=(10, 5))\n\nX_outliers = X_5[iforest.predict(X_5) == -1]\n\nfor i, ax in zip(range(len(X_outliers)), axes.ravel()):\n    ax.imshow(X_outliers[i].reshape((8, 8)),\n               cmap=plt.cm.gray_r, interpolation='nearest')\n    ax.axis('off')","ff83f64c":"# %load solutions\/22_C-anomaly_digits.py\nk = 1  # change to see other numbers\n\nX_k = X[y == k]\n\niforest = IsolationForest(contamination=0.05)\niforest = iforest.fit(X_k)\niforest_X = iforest.decision_function(X_k)\n\nX_strong_outliers = X_k[np.argsort(iforest_X)[:10]]\n\nfig, axes = plt.subplots(2, 5, figsize=(10, 5))\n\nfor i, ax in zip(range(len(X_strong_outliers)), axes.ravel()):\n    ax.imshow(X_strong_outliers[i].reshape((8, 8)),\n               cmap=plt.cm.gray_r, interpolation='nearest')\n    ax.axis('off')\n","0b3fc1f1":"## Isolation Forest\n\nIsolation Forest is an anomaly detection algorithm based on trees. The algorithm builds a number of random trees and the rationale is that if a sample is isolated it should alone in a leaf after very few random splits. Isolation Forest builds a score of abnormality based the depth of the tree at which samples end up.","082ed705":"## Anomaly detection with density estimation","310036b8":"1. Let's use IsolationForest to find the top 5% most abnormal images.\n2. Let's plot them !","d34f364e":"Compute the level of \"abnormality\" with `iforest.decision_function`. The lower, the more abnormal.","83a51b97":"# Anomaly detection\n\nAnomaly detection is a machine learning task that consists in spotting so-called outliers.\n\n\u201cAn outlier is an observation in a data set which appears to be inconsistent with the remainder of that set of data.\u201d\nJohnson 1992\n\n\u201cAn outlier is an observation which deviates so much from the other observations as to arouse suspicions that it was generated by a different mechanism.\u201d\n  Outlier\/Anomaly\nHawkins 1980\n\n### Types of anomaly detection setups\n\n- Supervised AD\n    - Labels available for both normal data and anomalies\n    - Similar to rare class mining \/ imbalanced classification\n- Semi-supervised AD (Novelty Detection)\n    - Only normal data available to train\n    - The algorithm learns on normal data only\n- Unsupervised AD (Outlier Detection)\n    - no labels, training set = normal + abnormal data\n    - Assumption: anomalies are very rare","1b071365":"### Generating the data set","fa105cff":"## now with One-Class SVM","ad8ad809":"Let's focus on digit 5.","f75253f6":"<div class=\"alert alert-success\">\n    <b>EXERCISE<\/b>:\n     <ul>\n      <li>\n      Rerun the same analysis with all the other digits\n      <\/li>\n    <\/ul>\n<\/div>","759b0f62":"The digits data set consists in images (8 x 8) of digits.","8679dabc":"<div class=\"alert alert-success\">\n    <b>EXERCISE<\/b>:\n     <ul>\n      <li>\n      **Change** the `gamma` parameter and see it's influence on the smoothness of the decision function.\n      <\/li>\n    <\/ul>\n<\/div>","5c4eeb66":"Only the support vectors are involved in the decision function of the One-Class SVM.\n\n1. Plot the level sets of the One-Class SVM decision function as we did for the true density.\n2. Emphasize the Support vectors.","ed21a2ff":"Let's first get familiar with different unsupervised anomaly detection approaches and algorithms. In order to visualise the output of the different algorithms we consider a toy data set consisting in a two-dimensional Gaussian mixture.","f7b19d64":"### Support vectors - Outliers\n\nThe so-called support vectors of the one-class SVM form the outliers","7afa9822":"Let's plot the strongest outliers","ae8205f8":"The problem of density based estimation is that they tend to become inefficient when the dimensionality of the data increase. It's the so-called curse of dimensionality that affects particularly density estimation algorithms. The one-class SVM algorithm can be used in such cases.","12b1da6e":"<div class=\"alert alert-success\">\n    <b>EXERCISE<\/b>:\n     <ul>\n      <li>\n      Illustrate graphically the influence of the number of trees on the smoothness of the decision function?\n      <\/li>\n    <\/ul>\n<\/div>","144664b5":"# Illustration on Digits data set\n\n\nWe will now apply the IsolationForest algorithm to spot digits written in an unconventional way.","5ab78de5":"Let's plot the strongest inliers","d0db42c4":"To use the images as a training set we need to flatten the images."}}