{"cell_type":{"341088d8":"code","d0957fac":"code","e7cbcf9e":"code","c84048b7":"code","cb52ca80":"code","9319a692":"code","ea34d85e":"code","a849708d":"code","7418792a":"markdown","08fe981d":"markdown","4d069d05":"markdown","d0424cfd":"markdown","8bf0195b":"markdown","915515f2":"markdown"},"source":{"341088d8":"!pip install -qU 'git+https:\/\/github.com\/PyTorchLightning\/lightning-flash.git#egg=lightning-flash[audio,image]'","d0957fac":"import os\nimport functools\nfrom pathlib import Path\n\nimport torch\nfrom torch import nn\nimport numpy as np\nfrom torch.nn import functional as F\nfrom torchmetrics import Accuracy, F1, Recall, Precision, MeanAbsoluteError\nimport torchvision\n\nimport flash\nfrom flash.audio import AudioClassificationData\nfrom flash.core.finetuning import FreezeUnfreeze\nfrom flash.image import ImageClassifier\n\nfrom flash.core.data.transforms import ApplyToKeys, merge_transforms\nfrom flash.audio.classification.transforms import default_transforms\n\nimport albumentations\n\n\nPATH_DATASET = Path(\"\/kaggle\/input\/seti-breakthrough-listen\")","e7cbcf9e":"def resolver(root, id):\n    return os.path.join(root, id[0], f\"{id}.npy\")\n\ndef preprocess(array):\n    array = np.vstack(array).transpose((1, 0))\n    return array.astype(np.float32)\n\ndef mixup(batch, alpha=1.0):\n    images = batch[\"input\"]\n    targets = batch[\"target\"].float().unsqueeze(1)\n    \n    lam = np.random.beta(alpha, alpha)\n    perm = torch.randperm(images.size(0))\n    \n    new_images = images * lam + images[perm] * (1 - lam)\n    new_targets = targets * lam + targets[perm] * (1 - lam)\n    batch[\"input\"] = new_images\n    batch[\"target\"] = new_targets\n    return batch\n\ndef convert_val_targets(batch):\n    batch[\"target\"] = batch[\"target\"].float().unsqueeze(1)\n    return batch\n\n\nclass AlbumentationsAdapter(nn.Module):\n    def __init__(self, transform):\n        super().__init__()\n        self.transform = transform\n        \n    def forward(self, x):\n        return self.transform(image=x)[\"image\"]\n\ntrain_transform = {\n    \"pre_tensor_transform\": nn.Sequential(\n        ApplyToKeys(\"input\", preprocess),\n        ApplyToKeys(\n            \"input\",\n            AlbumentationsAdapter(albumentations.Compose([\n                albumentations.Resize(340, 340),\n                #albumentations.HorizontalFlip(p=0.5),\n                #albumentations.VerticalFlip(p=0.5),\n                albumentations.ShiftScaleRotate(\n                    shift_limit=0.1,\n                    scale_limit=0.15,\n                    rotate_limit=20,\n                    p=0.5,\n                ),\n                albumentations.Resize(224, 224),\n                albumentations.RandomBrightness(limit=0.6, p=0.5),\n            ]))\n        ),\n    ),\n    \"to_tensor_transform\": nn.Sequential(\n        ApplyToKeys(\"input\", torchvision.transforms.ToTensor()),\n        ApplyToKeys(\"target\", torch.as_tensor),\n    ),\n    \"per_batch_transform\": mixup,\n}\n\nval_transform = {\n    \"pre_tensor_transform\": nn.Sequential(\n        ApplyToKeys(\"input\", preprocess),\n        ApplyToKeys(\"input\", AlbumentationsAdapter(albumentations.Resize(224, 224))),\n    ),\n    \"to_tensor_transform\": nn.Sequential(\n        ApplyToKeys(\"input\", torchvision.transforms.ToTensor()),\n        ApplyToKeys(\"target\", torch.as_tensor),\n    ),\n    \"per_batch_transform\": convert_val_targets,\n}\n\ndatamodule = AudioClassificationData.from_csv(\n    input_field=\"id\",\n    target_fields=\"target\",\n    train_file=str(PATH_DATASET \/ \"train_labels.csv\"),\n    train_images_root=str(PATH_DATASET \/ \"train\"),\n    train_resolver=resolver,\n    train_transform=train_transform,\n    val_resolver=resolver,\n    val_transform=val_transform,\n    batch_size=64,\n    num_workers=os.cpu_count(),\n    val_split=0.1,\n)","c84048b7":"model = ImageClassifier(\n    backbone=\"efficientnet_b3\",\n    backbone_kwargs={\"in_chans\": 1},\n    metrics=[MeanAbsoluteError()],\n    pretrained=True,\n    num_classes=1,\n    learning_rate=1e-5,\n    loss_fn=nn.BCEWithLogitsLoss(),\n)","cb52ca80":"import pytorch_lightning as pl\nlogger = pl.loggers.CSVLogger(save_dir='logs\/')\n\ntrainer = flash.Trainer(\n    max_epochs=10,\n    gpus=torch.cuda.device_count(),\n    logger=logger,\n    val_check_interval=0.5,\n    precision=16,\n)\ntrainer.fit(model, datamodule=datamodule)\n\ntrainer.save_checkpoint(\"audio_classification_model.pt\")","9319a692":"import pandas as pd\n\nmetrics = pd.read_csv(f'{trainer.logger.log_dir}\/metrics.csv')\ndisplay(metrics.head())\n\naggreg_metrics = []\nagg_col = \"epoch\"\nfor i, dfg in metrics.groupby(agg_col):\n    agg = dict(dfg.mean())\n    agg[agg_col] = i\n    aggreg_metrics.append(agg)\n\ndf_metrics = pd.DataFrame(aggreg_metrics)\ndf_metrics[['train_bcewithlogitsloss_step']].plot(grid=True, legend=True, xlabel=agg_col)\ndf_metrics[['train_meanabsoluteerror_step', 'val_meanabsoluteerror']].plot(grid=True, legend=True, xlabel=agg_col)\n# df_metrics[['train_f1_step', 'train_recall_step', 'train_precision_step', 'val_f1', 'val_recall', 'val_precision']].plot(grid=True, legend=True, xlabel=agg_col)","ea34d85e":"import torch\n\nfrom flash.core.data.process import Serializer\n\n\nclass IdPredictionSerializer(Serializer):\n\n    def serialize(self, sample):\n        preds = sample[\"preds\"]\n        preds = torch.tensor(preds).to(torch.float)\n        preds = preds.sigmoid().item()\n        filepath = sample[\"metadata\"][\"filepath\"]\n        file_id = os.path.basename(filepath).split(\".\")[0]\n        return {\"id\": file_id, \"target\": preds}","a849708d":"import glob\nfrom tqdm import tqdm\nfrom itertools import chain\n\nfound_npy = glob.glob(str(PATH_DATASET \/ \"test\" \/ \"*\" \/ \"*.npy\"))\n\npredict_transform = {\n    \"pre_tensor_transform\": nn.Sequential(\n        ApplyToKeys(\"input\", preprocess),\n        ApplyToKeys(\"input\", AlbumentationsAdapter(albumentations.Resize(224, 224))),\n    ),\n    \"to_tensor_transform\": ApplyToKeys(\"input\", torchvision.transforms.ToTensor()),\n}\n\ndatamodule = AudioClassificationData.from_files(\n    predict_files=found_npy,\n    predict_transform=predict_transform,\n    batch_size=1024,\n    num_workers=os.cpu_count(),\n)\n\nmodel.serializer = IdPredictionSerializer()\n\nsubmission = trainer.predict(model, datamodule=datamodule)\n\nsubmission = list(chain.from_iterable(submission))\npd.DataFrame(submission).set_index(\"id\").to_csv(\"submission.csv\")","7418792a":"### Training\n\nit is based on standard [Lightning](https:\/\/pytorch-lightning.readthedocs.io\/en\/stable) trainer","08fe981d":"### Define Flash DataModule\n\nIn this section we need to define our datast used later for classification.","4d069d05":"### Define Classif. model\n\nFlash offers rich collection od backbones for image classification and simple plug-in integration with [TorchMetrics](https:\/\/torchmetrics.readthedocs.io\/en\/stable\/)","d0424cfd":"## Training with Flash\n\n[Flash](https:\/\/lightning-flash.readthedocs.io\/en\/stable) is a framework of tasks for fast prototyping, baselining, finetuning and solving business and scientific problems with deep learning. It is focused on:\n\n- Predictions\n- Finetuning\n- Task-based training\n\nIt is built for data scientists, machine learning practitioners, and applied researchers.","8bf0195b":"### Logged stats\n\nLet's check some training statistic, how los and defined metrics evolved over time\/epochs","915515f2":"## Predictions\n\nWith trained model we just need run inference to prepeare dibission file"}}