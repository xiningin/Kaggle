{"cell_type":{"a6bbc88a":"code","1af93f6a":"code","e2c1bf5e":"code","7736bc07":"code","53ac6e3e":"code","c0772218":"code","e8b6b149":"code","7beb3416":"code","a3f08493":"code","7d5ddb0c":"code","51031c74":"code","ad8ac6b5":"code","e3753eb1":"code","0720ef6f":"code","490585c5":"code","192a7033":"code","24f8576e":"code","723183d0":"code","9da7126e":"code","8ea98a9a":"code","263282c9":"code","be4cee31":"code","16a73a1b":"code","e4afa339":"code","6a49a6dc":"code","009f0298":"code","b9790a82":"code","aab1f07e":"code","47a22dc7":"code","45c292f6":"markdown","62eb8423":"markdown","04319097":"markdown","7c8f1bed":"markdown","2a24ac51":"markdown","345049b6":"markdown","e9ed1b69":"markdown","91b55e20":"markdown","c82987c5":"markdown"},"source":{"a6bbc88a":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","1af93f6a":"train = pd.read_csv(\"\/kaggle\/input\/titanic\/train.csv\")\ntest = pd.read_csv(\"\/kaggle\/input\/titanic\/test.csv\")","e2c1bf5e":"train.head()","7736bc07":"train.describe()","53ac6e3e":"train.info()","c0772218":"train.isnull().sum()","e8b6b149":"train['Embarked'].value_counts()","7beb3416":"train['Embarked'].fillna('S',inplace = True)\ntest['Embarked'].fillna('S',inplace = True)\ntrain['Age'].fillna(train['Age'].mean(),inplace = True)\ntest['Age'].fillna(test['Age'].mean(),inplace = True)\nprint(train.head())\nprint(train.info())\nprint(train['Embarked'].value_counts)\nprint(train.isnull().sum())","a3f08493":"a = {'male' : 0 , 'female' : 1}\ne = {'S' : 0 , 'Q' : 1 , 'C' : 1 }\ntrain.replace({'Sex' : a , 'Embarked' : e},inplace=True)\n#test.replace({'Sex' : a , 'Embarked' : e},inplace=True)\nprint(test.head())\ntrain.head()","7d5ddb0c":"import seaborn as sns\nt = train.drop(['PassengerId','Ticket','Cabin','Name'],axis = 1)\nc = abs(t.corr())\nsns.heatmap(c,annot = True)","51031c74":"import seaborn as sns\nt = train.drop(['PassengerId','Ticket','Cabin','Name'],axis = 1)\nc = (t.corr())\ny = train['Survived']\nsns.heatmap(c,annot = True)","ad8ac6b5":"sns.countplot(x = 'Survived', data = train)","e3753eb1":"sns.countplot(x = 'Survived', hue = 'Sex', data = train)","0720ef6f":"sns.countplot(x = 'Survived', hue = 'Pclass', data = train)","490585c5":"train['Fare'].hist(bins = 50)","192a7033":"sns.countplot(x = 'Survived', hue = 'SibSp', data = train)","24f8576e":"sns.countplot(x = 'Survived', hue = 'Parch', data = train)","723183d0":"#Import models from scikit learn module: \nfrom sklearn.linear_model import LogisticRegression   \nfrom sklearn.model_selection import KFold \nfrom sklearn.ensemble import RandomForestClassifier \nfrom sklearn import metrics","9da7126e":"def classification_model(model, data, predictors, outcome):  \n    #Fit the model:  \n    model.fit(data[predictors],data[outcome])    \n    #Make predictions on training set:  \n    predictions = model.predict(data[predictors])    \n    #Print accuracy  \n    accuracy = metrics.accuracy_score(predictions,data[outcome])  \n    print(\"Accuracy : %s\" % \"{0:.3%}\".format(accuracy))\n    #Perform k-fold cross-validation with 5 folds  \n    kf = KFold(5,shuffle=True)  \n    error = []  \n    for train, test in kf.split(data):\n        # Filter training data    \n        train_predictors = (data[predictors].iloc[train,:])        \n        # The target we're using to train the algorithm.    \n        train_target = data[outcome].iloc[train]        \n        # Training the algorithm using the predictors and target.    \n        model.fit(train_predictors, train_target)\n        #Record error from each cross-validation run    \n        error.append(model.score(data[predictors].iloc[test,:], data[outcome].iloc[test]))\n     \n    print(\"Cross-Validation Score : %s\" % \"{0:.3%}\".format(np.mean(error))) \n    # %s is placeholder for data from format, next % is used to conert it into percentage\n    #.3% is no. of decimals\n    return model","8ea98a9a":"a = {'male' : 0 , 'female' : 1}\ne = {'S' : 0 , 'Q' : 1 , 'C' : 1 }\ntest.replace({'Sex' : a , 'Embarked' : e},inplace=True)\ntest.head()","263282c9":"\ntest['Age'].fillna(test['Age'].median(),inplace = True)\ntest['Fare'].fillna(test['Fare'].median(),inplace = True)\nt = test.drop(['PassengerId','Ticket','Cabin','Name'],axis = 1)\nt","be4cee31":"output = 'Survived'\nmodel = RandomForestClassifier()\npredict = ['Sex','Parch','SibSp','Fare','Age','Embarked']\nclassification_model(model,train,predict,output)\nm = classification_model(model,train,predict,output)\na = m.predict(t[predict])\na\n#'Age','Parch','SibSp',","16a73a1b":"my_submission = pd.DataFrame({'PassengerId': test['PassengerId'], 'Survived': a})\n# you could use any filename. We choose submission here\nmy_submission.to_csv('submission.csv', index=False)","e4afa339":"test.info()\n","6a49a6dc":"test.describe()\n","009f0298":"from sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import GridSearchCV","b9790a82":"\noutput = 'Survived'\nmodel = RandomForestClassifier()\npredict = ['Sex','Parch','SibSp','Fare','Age','Embarked']\nparam =  {  'n_estimators': [200, 500],    'max_features': ['auto', 'sqrt', 'log2'],    'max_depth' : [4,5,6,7,8],\n    'criterion' :['gini', 'entropy']\n}\nmod  = GridSearchCV(estimator=model, param_grid=param, cv= 5)\nmod.fit(train[predict], train[output])\n","aab1f07e":"mod.best_params_","47a22dc7":"output = 'Survived'\nmodel = RandomForestClassifier(criterion= 'entropy', max_depth= 5,max_features= 'auto',n_estimators= 500)\npredict = ['Sex','Parch','SibSp','Fare','Age','Embarked','Pclass']\nclassification_model(model,train,predict,output)\nm = classification_model(model,train,predict,output)\na = m.predict(t[predict])\na","45c292f6":"From the above plot we can infer that of the people who died,most were 'male'. For those who survived, 'female' were more than 'male' class. ","62eb8423":"As we can see that **Cabin** has 687 missing values out of a total of 891 values. That is around 77% of the data missing. So, we can not conclude anything about the missing data. Hence, it'd be better to avoid the Cabin column. \n\nThe **Embarked** column has 2 missing values. From its analysis, we can see that 'S' has the highest frequency. We also find that if we make the missing values equal to either of 'S','C' or 'Q', it won't make a huge difference to our analysis and model. So,I decided to replace the null values with the mode, which is 'S'.\n\nNext, we have missing values in the **Age** column. From the 5-Number Summary of the Age column, we get to know that Age has a normal distribution because the mean = 29.699 is very close to the median = 28.0. We have 177 missing values in Age. Age can gave a vital role in determining whether a person survived or not. We have two options to replace the missing values. One, we can replace them with the mean of the column or two, we can replace them with the median of the column. As it seems, we can replace them with either mean or median and it won't make a huge difference. I decided to replace the null values with the median, as median is much more robust to outliers than mean.","04319097":"From the two plots above, we can conclude that there is no clear correlation between 'SibSp' and 'Survived' and between 'Parch' and 'Survived'. Now, both 'SibSp' and 'Parch' are blurred indicators of the Age of a certain person. Hence, we can conclude the following - \n* Either of 'SibSp' and 'Parch' or 'Age' ,must be present to make accurate predictions.\n* However, if we have all the three columns, we might get a slightly greater accuracy as we have more features then. The increase in accuracy, intuitvely, won't be huge just because there is no strong correlation between either of the three columns and 'Survived'\n* Moreover, if we use all the features, there remains a possibility of getting an overfitted model. ","7c8f1bed":"output = 'Survived'\nmodel = RandomForestClassifier()\npredict = ['Pclass','Sex', 'Age','SibSp','Parch','Fare']\nm = classification_model(model,train,predict,output)\nm.predict(t[predict])\n#len(m.predict(t[predict]))","2a24ac51":"From the above plot of the column 'Survived', we can see tha most people(>400) did not survive. Now, let us have an even cleaner visualization on the basis of the 'Sex' column","345049b6":"output = 'Survived'\nmodel = RandomForestClassifier()\npredict = ['Pclass','Sex', 'Age','Fare']\nclassification_model(model,train,predict,output)\nm = classification_model(model,train,predict,output)\nm.predict(t[predict])","e9ed1b69":"The Pclass column denotes the class of people aboard the Titanic from a monetary point of view. We can see that of the people who died, most were from class 3. Similarly, of those who survived, most were from class 1. Hence, there seems to be some correlation of Pclass with Survived. This justifies the value of correlation(=-0.34) which we got from the heatmap. ","91b55e20":"Now we will move forward with some visualization and data exploration. We have to visualize the data to find the correlation of the different columns with each other and specifically the 'Survive' column.","c82987c5":"output = 'Survived'\nmodel = RandomForestClassifier()\npredict = ['Pclass','Sex', 'Fare']\nclassification_model(model,train,predict,output)\nm = classification_model(model,train,predict,output)\nm.predict(t[predict])"}}