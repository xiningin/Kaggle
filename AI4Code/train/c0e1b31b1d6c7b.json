{"cell_type":{"9afd1f23":"code","719a4ff1":"code","487a32a0":"code","bca58fef":"code","628f128c":"code","0120a348":"code","1d236590":"code","ceaab311":"code","9208c7c0":"code","2aff0cd8":"code","3f42a28c":"code","d9d665db":"code","237ed9f7":"code","47b82f61":"code","9c44c93a":"code","1140a034":"code","c4e4f28e":"code","36f9fe7a":"code","c95c7801":"code","1084de29":"code","c96a4c9d":"code","1cceef57":"code","7afe28a9":"code","08828f6e":"code","b695cfcb":"code","b884007d":"code","108c2123":"code","d81fb7d1":"code","1754bf41":"code","0d10e561":"code","bab7bb3b":"code","e9d043f1":"code","53983435":"code","b0b01472":"code","823aeb1d":"code","01a9749a":"code","2e9e00a6":"code","5b341b81":"code","d45c3e35":"code","4fed2373":"code","b904e869":"code","675bcd00":"code","a4ae417b":"code","207a1242":"code","f88be4d7":"code","06d8ed38":"markdown","4422b9f6":"markdown","cafc1a91":"markdown","c60cdd8a":"markdown","bd43a7c9":"markdown","4471f4a5":"markdown","4eed8cad":"markdown","a4190f4b":"markdown","a73b21cb":"markdown","88ffe2d7":"markdown","5b5d18af":"markdown","4c602553":"markdown","e856b95d":"markdown","5fed522a":"markdown","28d04908":"markdown","6aaf1893":"markdown","14c3102e":"markdown","3871220e":"markdown","71eba63f":"markdown","be124c12":"markdown","4d0c89b0":"markdown","23e5954e":"markdown","1a0bd590":"markdown","92b08826":"markdown","5f7df41a":"markdown","ec9f0c01":"markdown","7444d2a2":"markdown","13482dfb":"markdown","32ed393a":"markdown","27386111":"markdown","889f73a8":"markdown","03e76ab5":"markdown","2db701fa":"markdown","d215aef5":"markdown","3a009e66":"markdown","7a47364e":"markdown","19c947b8":"markdown","fbd05f46":"markdown","8cefe1d4":"markdown","53deaf9c":"markdown","6164454e":"markdown","5e685a0e":"markdown","421ee5fe":"markdown","f83d39f8":"markdown","92cf5745":"markdown"},"source":{"9afd1f23":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt # this is used for the plot the graph \nimport seaborn as sns # used for plot interactive graph.\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import average_precision_score\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.metrics import precision_recall_curve\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.metrics import roc_curve\nimport eli5 \nfrom eli5.sklearn import PermutationImportance\nimport shap \nfrom pdpbox import pdp, info_plots \nfrom sklearn.metrics import f1_score\nfrom sklearn.metrics import auc\nfrom sklearn.svm import SVC\nnp.random.seed(123)\nfrom sklearn.tree import export_graphviz\n\npd.options.mode.chained_assignment = None \n%matplotlib inline","719a4ff1":"df = pd.read_csv('\/kaggle\/input\/health-care-data-set-on-heart-attack-possibility\/heart.csv')\ndf.head(5)","487a32a0":"df.describe()","bca58fef":"df.isnull().sum()","628f128c":"import seaborn as sns\nplt.figure(figsize=(10,8), dpi= 80)\nsns.heatmap(df.corr(), cmap='RdYlGn', center=0)\n\n# Decorations\nplt.xticks(fontsize=12)\nplt.yticks(fontsize=12)\nplt.show()","0120a348":"sns.distplot(df['target'],rug=True)\nplt.show()","1d236590":"import plotly\nimport plotly.graph_objs as go\nfrom plotly.offline import init_notebook_mode, iplot\ninit_notebook_mode(connected=True)\ncol = \"target\"\ncolors = ['gold', 'blue']\ngrouped = df[col].value_counts().reset_index()\ngrouped = grouped.rename(columns = {col : \"count\", \"index\" : col})\n\n## plot\n#trace = go.Pie(labels=grouped[col], values=grouped['count'], pull=[0.05, 0])\ntrace = go.Pie(labels=grouped[col], values=grouped['count'], pull=[0.05, 0],\n               marker=dict(colors=colors, line=dict(color='#000000', width=2)))\nlayout = {'title': 'Target(0 = No, 1 = Yes)'}\nfig = go.Figure(data = [trace], layout = layout)\niplot(fig)\n","ceaab311":"sns.distplot(df['sex'],rug=True)\nplt.show()","9208c7c0":"from plotly.offline import init_notebook_mode, iplot\ninit_notebook_mode(connected=True)\ncol = \"sex\"\ngrouped = df[col].value_counts().reset_index()\ngrouped = grouped.rename(columns = {col : \"count\", \"index\" : col})\n\n## plot\ntrace = go.Pie(labels=grouped[col], values=grouped['count'], pull=[0.05, 0])\nlayout = {'title': 'Male(1), Female(0)'}\nfig = go.Figure(data = [trace], layout = layout)\niplot(fig)","2aff0cd8":"plt.style.use('fivethirtyeight')\nplt.figure(figsize=(8,5))\nsns.countplot(x=df.target,hue=df.sex)\nplt.legend(labels=['Female', 'Male'])","3f42a28c":"dy=pd.DataFrame(df.groupby('sex')['target'].mean().reset_index().values,\n                    columns=[\"gender\",\"target1\"])\ndy.head()","d9d665db":"sns.barplot(dy.gender,dy.target1)\nplt.ylabel('rate of heart attack')\nplt.title('0=Female, 1=Male')","237ed9f7":"sns.distplot(df['cp'],rug=True)\nplt.show()","47b82f61":"content=df['cp'].value_counts().to_frame().reset_index().rename(columns={'index':'c1','C1':'count'})\n#content\nfig = go.Figure([go.Pie(labels=content['c1'], values=content['cp']\n                        ,hole=0.3)])  # can change the size of hole \n\nfig.update_traces(hoverinfo='label+percent', textinfo='percent', textfont_size=15)\nfig.update_layout(title=\"Chest Pain Types\",title_x=0.5)\nfig.show()","9c44c93a":"plt.style.use('fivethirtyeight')\nplt.figure(figsize=(8,5))\nsns.countplot(x=df.target,hue=df.cp)\nplt.legend(labels=['0: typical angina', '1: atypical angina','2: non-anginal pain','3: asymptomatic'])","1140a034":"dy=pd.DataFrame(df.groupby('cp')['target'].mean().reset_index().values,\n                    columns=[\"chest_pain\",\"target2\"])\ndy.head()","c4e4f28e":"sns.barplot(dy.chest_pain,dy.target2)\nplt.ylabel('rate of heart attack')","36f9fe7a":"sns.distplot(df['thalach'],rug=True)\nplt.show()","c95c7801":"col='thalach'\nd1=df[df['target']==0]\nd2=df[df['target']==1]\nv1=d1[col].value_counts().reset_index()\nv1=v1.rename(columns={col:'count','index':col})\nv1['percent']=v1['count'].apply(lambda x : 100*x\/sum(v1['count']))\nv1=v1.sort_values(col)\nv2=d2[col].value_counts().reset_index()\nv2=v2.rename(columns={col:'count','index':col})\nv2['percent']=v2['count'].apply(lambda x : 100*x\/sum(v2['count']))\nv2=v2.sort_values(col)\ntrace1 = go.Scatter(x=v1[col], y=v1[\"count\"], name=0,mode='lines+markers')\ntrace2 = go.Scatter(x=v2[col], y=v2[\"count\"], name=1,mode='lines+markers')\ndata = [trace1, trace2]\nlayout={'title':\"target over the person's maximum heart rate achieved\",'xaxis':{'title':\"Thalach\"}}\nfig = go.Figure(data, layout=layout)\niplot(fig)","1084de29":"col='fbs'\nd1=df[df['target']==0]\nd2=df[df['target']==1]\nv1=d1[col].value_counts().reset_index()\nv1=v1.rename(columns={col:'count','index':col})\nv1['percent']=v1['count'].apply(lambda x : 100*x\/sum(v1['count']))\nv1=v1.sort_values(col)\nv2=d2[col].value_counts().reset_index()\nv2=v2.rename(columns={col:'count','index':col})\nv2['percent']=v2['count'].apply(lambda x : 100*x\/sum(v2['count']))\nv2=v2.sort_values(col)\ntrace1 = go.Bar(x=v1[col], y=v1[\"count\"], name=0, marker=dict(color=\"#a678de\"))\ntrace2 = go.Bar(x=v2[col], y=v2[\"count\"], name=1, marker=dict(color=\"#6ad49b\"))\ndata = [trace1, trace2]\nlayout={'title':\"target over the person's fasting blood sugar \",'xaxis':{'title':\"fbs(> 120 mg\/dl, 1 = true; 0 = false)\"}}\n#layout = go.Layout(title=\"Content added over the years\", legend=dict(x=0.1, y=1.1, orientation=\"h\"))\nfig = go.Figure(data, layout=layout)\niplot(fig)","c96a4c9d":"dy=pd.DataFrame(df.groupby('fbs')['target'].mean().reset_index().values,\n                    columns=[\"fbs\",\"target3\"])\nprint(dy.head())\nsns.barplot(dy.fbs,dy.target3)\nplt.ylabel('rate of heart attack')\n#plt.title('0=Female, 1=Male')","1cceef57":"col='age'\nd1=df[df['target']==0]\nd2=df[df['target']==1]\nv1=d1[col].value_counts().reset_index()\nv1=v1.rename(columns={col:'count','index':col})\nv1['percent']=v1['count'].apply(lambda x : 100*x\/sum(v1['count']))\nv1=v1.sort_values(col)\nv2=d2[col].value_counts().reset_index()\nv2=v2.rename(columns={col:'count','index':col})\nv2['percent']=v2['count'].apply(lambda x : 100*x\/sum(v2['count']))\nv2=v2.sort_values(col)\ntrace1 = go.Scatter(x=v1[col], y=v1[\"count\"], name=0, marker=dict(color=\"blue\"),mode='lines+markers')\ntrace2 = go.Scatter(x=v2[col], y=v2[\"count\"], name=1, marker=dict(color=\"red\"),mode='lines+markers')\ndata = [trace1, trace2]\nlayout={'title':\"Target With Respect to age\",'xaxis':{'title':\"Age\"}}\nfig = go.Figure(data, layout=layout)\niplot(fig)","7afe28a9":"#df=df.dropna()\n#X = df.drop(['target'], axis = 1)\n#y = df.target.values\n#from sklearn.model_selection import train_test_split\n#X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state=0)\n#from sklearn.preprocessing import StandardScaler\n#sc_X=StandardScaler()\n#X_train=sc_X.fit_transform(X_train)\n#X_test=sc_X.transform(X_test)","08828f6e":"# Mean Encoding\ncumsum = df.groupby('sex')['target'].cumsum() - df['target']\ncumcnt = df.groupby('sex').cumcount()\ndf['sex'] = cumsum\/cumcnt\ndf=df.dropna()\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(df.drop('target', 1), \n                                                    df['target'], test_size = .2, random_state=10)","b695cfcb":"import xgboost as xgb\nfrom xgboost import XGBClassifier\nalg = XGBClassifier(learning_rate=0.01, n_estimators=2000, max_depth=8,\n                        min_child_weight=0, gamma=0, subsample=0.52, colsample_bytree=0.6,\n                        objective='binary:logistic', nthread=4, scale_pos_weight=1, \n                    seed=27, reg_alpha=5, reg_lambda=2, booster='gbtree',\n            n_jobs=-1, max_delta_step=0, colsample_bylevel=0.6, colsample_bynode=0.6)\nalg.fit(X_train, y_train)\nprint('train accuracy',alg.score(X_train, y_train))\nprint('test accuracy',alg.score(X_test,y_test))","b884007d":"import scikitplot as skplt\nxgb_prob = alg.predict_proba(X_test)","108c2123":"skplt.metrics.plot_roc(y_test, xgb_prob)\nskplt.metrics.plot_ks_statistic(y_test, xgb_prob)","d81fb7d1":"probas_list1 = [alg.predict_proba(X_test)]\nxy=['xgb']\nskplt.metrics.plot_calibration_curve(y_test,\n                                      probas_list1,\n                                    xy)","1754bf41":"from yellowbrick.classifier import ClassificationReport,ConfusionMatrix\nclasses=[0,1]\nvisualizer = ClassificationReport(alg, classes=classes)\n#visualizer.fit(X_train, y_train)  \nvisualizer.score(X_test, y_test)  \ng = visualizer.poof()","0d10e561":"from sklearn.ensemble import RandomForestClassifier\nrf = RandomForestClassifier(n_estimators=600,random_state=0, n_jobs= -1)\nrf = rf.fit(X_train, y_train)\nprint('train accuracy',rf.score(X_train, y_train))\nprint('test accuracy',rf.score(X_test,y_test))","bab7bb3b":"import scikitplot as skplt\nrdf_prob = rf.predict_proba(X_test)","e9d043f1":"skplt.metrics.plot_roc(y_test, rdf_prob)\nskplt.metrics.plot_ks_statistic(y_test, rdf_prob)\nplt.show()","53983435":"probas_list1 = [rf.predict_proba(X_test)]\nxy=['rdf']\nskplt.metrics.plot_calibration_curve(y_test,\n                                      probas_list1,\n                                    xy)","b0b01472":"from yellowbrick.classifier import ClassificationReport,ConfusionMatrix\nclasses=[0,1]\nvisualizer = ClassificationReport(rf, classes=classes)\n#visualizer.fit(X_train, y_train)  \nvisualizer.score(X_test, y_test)  \ng = visualizer.poof()","823aeb1d":"X_train, X_test, y_train, y_test = train_test_split(df.drop('target', 1), df['target'], test_size = .2, random_state=10) #split the data\nestimator = rf.estimators_[1]\nfeature_names = [i for i in X_train.columns]\n\ny_train_str = y_train.astype('str')\ny_train_str[y_train_str == '0'] = 'no disease'\ny_train_str[y_train_str == '1'] = 'disease'\ny_train_str = y_train_str.values","01a9749a":"\nexport_graphviz(estimator, out_file='tree.dot', \n                feature_names = feature_names,\n                class_names = y_train_str,\n                rounded = True, proportion = True, \n                label='root',\n                precision = 2, filled = True)\n\nfrom subprocess import call\ncall(['dot', '-Tpng', 'tree.dot', '-o', 'tree.png', '-Gdpi=600'])\n\nfrom IPython.display import Image\nImage(filename = 'tree.png')","2e9e00a6":"perm = PermutationImportance(rf, random_state=1).fit(X_test, y_test)\neli5.show_weights(perm, feature_names = X_test.columns.tolist())","5b341b81":"base_features = df.columns.values.tolist()\nbase_features.remove('target')\n\nfeat_name = 'cp'\npdp_dist = pdp.pdp_isolate(model=rf, dataset=X_test, model_features=base_features, feature=feat_name)\n\npdp.pdp_plot(pdp_dist, feat_name)\nplt.show()","d45c3e35":"feat_name = 'ca'\npdp_dist = pdp.pdp_isolate(model=rf, dataset=X_test, model_features=base_features, feature=feat_name)\n\npdp.pdp_plot(pdp_dist, feat_name)\nplt.show()","4fed2373":"feat_name = 'oldpeak'\npdp_dist = pdp.pdp_isolate(model=rf, dataset=X_test, model_features=base_features, feature=feat_name)\n\npdp.pdp_plot(pdp_dist, feat_name)\nplt.show()","b904e869":"feat_name = 'exang'\npdp_dist = pdp.pdp_isolate(model=rf, dataset=X_test, model_features=base_features, feature=feat_name)\n\npdp.pdp_plot(pdp_dist, feat_name)\nplt.show()","675bcd00":"feat_name = 'fbs'\npdp_dist = pdp.pdp_isolate(model=rf, dataset=X_test, model_features=base_features, feature=feat_name)\n\npdp.pdp_plot(pdp_dist, feat_name)\nplt.show()","a4ae417b":"explainer = shap.TreeExplainer(rf)\nshap_values = explainer.shap_values(X_test)\n\nshap.summary_plot(shap_values[1], X_test, plot_type=\"bar\")","207a1242":"shap.summary_plot(shap_values[1], X_test)","f88be4d7":"#shap_values = explainer.shap_values(X_train.iloc[:50])\n#shap.initjs()\n#shap.force_plot(explainer.expected_value[1], shap_values[1], X_test.iloc[:50])","06d8ed38":"### Explaination","4422b9f6":"* Cardiovascular disease is the leading cause of death in women in Australia with 90% of women having one risk factor. \n* The causes including high blood pressure, high cholesterol, smoking, diabetes, weight and family history are discussed.\n* A woman's risk also goes up if she's had a miscarriage or had her ovaries or uterus removed.\n*  Women's hearts are affected by stress and depression more than men's. Depression makes it difficult to maintain a healthy lifestyle.","cafc1a91":"The person having the heart rate over 140 is more likely to have heart disease, therefor we conclude that we have to check our heart rate monthly if its over the thalach 140 then we have to concult the doctor and much concious to the health.","c60cdd8a":"## Symptoms","bd43a7c9":"It's a clean, easy to understand set of data. However, the meaning of some of the column headers are not obvious. Here's what they mean,\n\n1.age: The person's age in years\n\n2.sex: The person's sex (1 = male, 0 = female)\n\n3.cp: The chest pain experienced (Value 0: typical angina, Value 1: atypical angina, Value 2: non-anginal pain, Value 3: asymptomatic)\n\n4.trestbps: The person's resting blood pressure (mm Hg on admission to the hospital)\n\n5.chol: The person's cholesterol measurement in mg\/dl\n\n6.fbs: The person's fasting blood sugar (> 120 mg\/dl, 1 = true; 0 = false)\n\n7.restecg: Resting electrocardiographic measurement (0 = normal, 1 = having ST-T wave abnormality, 2 = showing probable or definite left ventricular hypertrophy by Estes' criteria)\n\n8.thalach: The person's maximum heart rate achieved\n\n9.exang: Exercise induced angina (1 = yes; 0 = no)\n\n10.oldpeak: ST depression induced by exercise relative to rest ('ST' relates to positions on the ECG plot. See more here)\n\n11.slope: the slope of the peak exercise ST segment (Value 1: upsloping, Value 2: flat, Value 3: downsloping)\n\n12.ca: The number of major vessels (0-3)\n\n13.thal: A blood disorder called thalassemia (3 = normal; 6 = fixed defect; 7 = reversable defect)\n\n14.target: Heart disease (0 = no, 1 = yes)\n","4471f4a5":"In the above plot we see that 27.2% persons having chaist pain type 0, 82% having chaist pain type 1, 79.3% having chaist pain type 2 and 69.5% having chaist pain type 3. These person have heart disease, from this data we observe that those who have chaist pain type 1 and chaist pain type 2 is more likely to affected by heart disease.","4eed8cad":"Looking at information of heart disease risk factors led me to the following: high cholesterol, high blood pressure, diabetes, weight, family history and smoking 3. According to another source 4, the major factors that can't be changed are: increasing age, male gender and heredity. Note that thalassemia, one of the variables in this dataset, is heredity. Major factors that can be modified are: Smoking, high cholesterol, high blood pressure, physical inactivity, and being overweight and having diabetes. Other factors include stress, alcohol and poor diet\/nutrition.","a4190f4b":"<font size=\"+3\" color=blue ><b> <center><u>Preventation is Better Than Cure<\/u><\/center><\/b><\/font>\n ","a73b21cb":"##### from the above plot we see that the rate of heart disease in females 75% have more in comprission of male 45%. ","88ffe2d7":"### Women are 4 times more likely to die from heart disease than breast cancer","5b5d18af":"there are no missing data in this dataset","4c602553":"* Chest pain, chest tightness, chest pressure and chest discomfort (angina)\n* Shortness of breath\n* Pain, numbness, weakness or coldness in your legs or arms if the blood vessels in those parts of your body are narrowed\n* Pain in the neck, jaw, throat, upper abdomen or back.\n* Heart failure is also an outcome of heart disease, and breathlessness can occur when the heart becomes too weak to circulate blood.\n* Some heart conditions occur with no symptoms at all, especially in older adults and individuals with\u00a0diabetes.","e856b95d":"## Loading appropriate libraries","5fed522a":"<img src=\"https:\/\/www.narayanahealth.org\/sites\/default\/files\/nh-campaigns\/heart-rhythm\/images\/banner2.jpg\" style=\"width: 950px;\"\/>","28d04908":"## Data Visulization","6aaf1893":"### THALACH","14c3102e":"Describe function is a function that allows analysis between the numerical values contained in the data set. Using this function count, mean, std, min, max, 25%, 50%, 75%.\nAs seen in this section, most values are generally categorized. This means that we need to integrate other values into this situation. These; age, trestbps, chol, thalach","3871220e":"From this plot we identify that person having age between 40 to 65 year is more likely to affected by heart disease, therefor person having the age in this range have to more concisous about there health. ","71eba63f":"## Cause of Heart Attack","be124c12":"1.  Excess weight, especially around the stomach area, increases a woman's risk of developing cardiovascular disease and lack of physical activity makes it worse.\n2. Diabetes causes damage to blood vessels so diabetes is a major factor in developing cardiovascular disease.\n3. Unhealthy foods, lack of exercise, lead to heart disease. So can high blood pressure, infections, and birth defects.\n4. Smoking is one of the biggest causes of cardiovascular disease.\n5. Just a few cigarettes a day can damage the blood vessels and reduce the amount of oxygen available in our blood.\n*  But other things might surprise you.","4d0c89b0":"#### Eli5 Values\n> **Permutation importance is the first tool for understanding a machine-learning model individual variables in the validation data (after a model has been fit), and seeing the effect on accuracy.** ","23e5954e":"StandardScaler will transform your data such that its distribution will have a mean value 0 and standard deviation of 1. Given the distribution of the data, each value in the dataset will have the sample mean value subtracted, and then divided by the standard deviation of the whole dataset.","1a0bd590":"### CHAIST PAIN","92b08826":"## Preventions","5f7df41a":"#### Loading the Data","ec9f0c01":"The 51.2% person having the fasting blood sugar rate over 120 mg\/dl and 55% person having the fasting blood sugar rate below 120 mg\/dl is affected by heart disease.","7444d2a2":"#### AGE","13482dfb":"<font size=\"+3\" color=purple ><b>Please Upvote my kernel if you think it is helpful.<\/b><\/font>","32ed393a":"* Quit smoking.\n* Control other health conditions, such as high blood pressure, high cholesterol and diabetes.\n* Exercise at least 30 minutes a day on most days of the week.\n* Eat a diet that's low in salt and saturated fat.\n* Maintain a healthy weight.\n* Reduce and manage stress.\n* Practice good hygiene.","27386111":"### TARGET","889f73a8":"### SEX","03e76ab5":"In the heart disease UCI dataset only 31.7% are female and rest are male. we have find that either male or female which have likely to heart disease.","2db701fa":"### 1.XGBoost\n\n> XGBoost is a decision-tree-based ensemble Machine Learning algorithm that uses a gradient boosting framework. In prediction problems involving unstructured data (images, text, etc.) ... A wide range of applications: Can be used to solve regression, classification, ranking, and user-defined prediction problems.","d215aef5":"#### Training & Testing of Model","3a009e66":"Now,I will check null on all data and If data has null, I will sum of null data's. In this way, how many missing data is in the data.\n","7a47364e":"### 2.Random Forest\n\n> The random forest is a classification algorithm consisting of many decisions trees. It uses bagging and feature randomness when building each individual tree to try to create an uncorrelated forest of trees whose prediction by committee is more accurate than that of any individual tree","19c947b8":"### Partial Dependence Plot\n\n> These plots vary a single variable in a single row across a range of values and see what effect it has on the outcome. It does this for several rows and plots the average effect. Let's take a look at the 'num_major_vessels' variable, which was at the top of the permutation importance list,","fbd05f46":"### Summary\nWe started with the data exploration where we got a feeling for the dataset, checked about missing data and learned which features are important. During this process we used Plotly, seaborn and matplotlib to do the visualizations. During the data preprocessing part, we converted features into numeric ones, grouped values into categories and created a few new features. Afterwards we started training machine learning models, and applied cross validation on it. Of course there is still room for improvement, like doing a more extensive feature engineering, by comparing and plotting the features against each other and identifying and removing the noisy features. You could also do some ensemble learning.Lastly, we looked at it\u2019s confusion matrix and computed the models precision.","8cefe1d4":"from the above plot we see that 45.5% people does not have disease and 54.5% have heart disease.","53deaf9c":"#### signs of a heart attack.\n\n* Nausea and cold sweats.\n\n* Chest Pain, Pressure, Fullness, or Discomfort. \n\n* Discomfort in other areas of your body. \n\n* Difficulty breathing and dizziness.","6164454e":"\n## Plots that are Used for showing the Classification Results\n\n#### ROC curve\n* > A receiver operating characteristic curve, or ROC curve, is a graphical plot that illustrates the diagnostic ability of a binary classifier system as its discrimination threshold is varied.\n* > A precision-recall curve is a plot of the precision (y-axis) and the recall (x-axis) for different thresholds, much like the ROC curve. A no-skill classifier is one that cannot discriminate between the classes and would predict a random class or a constant class in all cases\n#### Lift curve\n> A lift curve shows the ratio of a model to a random guess ('model cumulative sum' \/ 'random guess' from above). Cumulative gains charts are a bit mor. A lift curve is a way of visualizing the performance of a classification model. Lift curves are closely related to, and frequently confused with, cumulative gains charts\n\n#### Calibration curve\n> In analytical chemistry, a calibration curve, also known as a standard curve, is a general method for determining the concentration of a substance in an unknown sample by comparing the unknown to a set of standard samples of known\n\n#### KS plot\n> Kolmogorov-Smirnov chart measures performance of classification models. K-S is a measure of the degree of separation between the positive and negative distributions.\n\n#### Cumulative gain plot\n> The cumulative gains chart shows the percentage of the overall number of cases in a given category \"gained\" by targeting a percentage of the total number of cases.","5e685a0e":"## Creating Different Machine Learning Model ","421ee5fe":"from theabove corelation plot we see that cp(chest pain),thalch and slope are highly corelated with the target.","f83d39f8":"<font size=\"+3\" color=red ><b> <center><u>Introduction<\/u><\/center><\/b><\/font>\n \n\nThis dataset gives a number of variables along with a target condition of having or not having heart disease. It contains 76 attributes, but all published experiments refer to using a subset of 14 of them. In particular, the Cleveland database is the only one that has been used by ML researchers to this date. The \"goal\" field refers to the presence of heart disease in the patient.\n\n> **In addition, we will analyze for this dataset. We will use a wide range of tools for this part. If there's value in there, we'il do it there. Finally, machine learning algorithms are estimated.**","92cf5745":"### FASTING BLOOD SUGAR"}}