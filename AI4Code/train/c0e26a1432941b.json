{"cell_type":{"031ab5a7":"code","1de90a77":"code","f1262708":"code","858ab33d":"code","731909af":"code","5360d110":"code","64351785":"code","087977c6":"code","69b9dbdc":"code","ba3f7cbb":"code","e1facd54":"code","0b4628f0":"code","77e93f6f":"code","4e1ec767":"markdown","93309741":"markdown","1e78dae7":"markdown","544b73d6":"markdown","71d17a9e":"markdown","f2183916":"markdown","17eaebd8":"markdown","8406548b":"markdown"},"source":{"031ab5a7":"#Import Libraries\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport tensorflow as tf\nimport keras\nfrom tensorflow.keras import datasets,models,layers\n","1de90a77":"\n# Adding TF Cifar10 Data ..\nfrom keras.datasets import cifar10\n(X_train, Y_train), (X_test, Y_test) = cifar10.load_data()\n\n","f1262708":"# Drawing sample . \nplt.imshow(X_train[42])","858ab33d":"# Normalize the data.\nX_train = X_train.astype('float32')\nX_test = X_test.astype('float32')\nX_train \/= 255.0\nX_test \/= 255.0","731909af":"from sklearn.model_selection import train_test_split\nX_train, X_val, Y_train, Y_val = train_test_split(X_train, Y_train, test_size = 0.2,shuffle = True)","5360d110":"from sklearn.preprocessing import OneHotEncoder\nencoder = OneHotEncoder()\nencoder.fit(Y_train)\nY_train = encoder.transform(Y_train).toarray()\nY_test = encoder.transform(Y_test).toarray()\nY_val =  encoder.transform(Y_val).toarray()","64351785":"from keras.preprocessing.image import ImageDataGenerator\naug = ImageDataGenerator(horizontal_flip=True, width_shift_range=0.05,\n                             height_shift_range=0.05)\naug.fit(X_train)","087977c6":"\"\"\"\nResNet-18\nReference:\n[1] K. He et al. Deep Residual Learning for Image Recognition. CVPR, 2016\n[2] K. He, X. Zhang, S. Ren, and J. Sun. Delving deep into rectifiers:\nSurpassing human-level performance on imagenet classification. In\nICCV, 2015.\n\"\"\"\n\n\nfrom keras.callbacks import EarlyStopping\nfrom keras.layers import Dense, Conv2D,  MaxPool2D, Flatten, GlobalAveragePooling2D,  BatchNormalization, Layer, Add\nfrom keras.models import Sequential\nfrom keras.models import Model\nimport tensorflow as tf\n\n\nclass ResnetBlock(Model):\n    \"\"\"\n    A standard resnet block.\n    \"\"\"\n\n    def __init__(self, channels: int, down_sample=False):\n        \"\"\"\n        channels: same as number of convolution kernels\n        \"\"\"\n        super().__init__()\n\n        self.__channels = channels\n        self.__down_sample = down_sample\n        self.__strides = [2, 1] if down_sample else [1, 1]\n\n        KERNEL_SIZE = (3, 3)\n        # use He initialization, instead of Xavier (a.k.a 'glorot_uniform' in Keras), as suggested in [2]\n        INIT_SCHEME = \"he_normal\"\n\n        self.conv_1 = Conv2D(self.__channels, strides=self.__strides[0],\n                             kernel_size=KERNEL_SIZE, padding=\"same\", kernel_initializer=INIT_SCHEME)\n        self.bn_1 = BatchNormalization()\n        self.conv_2 = Conv2D(self.__channels, strides=self.__strides[1],\n                             kernel_size=KERNEL_SIZE, padding=\"same\", kernel_initializer=INIT_SCHEME)\n        self.bn_2 = BatchNormalization()\n        self.merge = Add()\n\n        if self.__down_sample:\n            # perform down sampling using stride of 2, according to [1].\n            self.res_conv = Conv2D(\n                self.__channels, strides=2, kernel_size=(1, 1), kernel_initializer=INIT_SCHEME, padding=\"same\")\n            self.res_bn = BatchNormalization()\n\n    def call(self, inputs):\n        res = inputs\n\n        x = self.conv_1(inputs)\n        x = self.bn_1(x)\n        x = tf.nn.relu(x)\n        x = self.conv_2(x)\n        x = self.bn_2(x)\n\n        if self.__down_sample:\n            res = self.res_conv(res)\n            res = self.res_bn(res)\n\n        # if not perform down sample, then add a shortcut directly\n        x = self.merge([x, res])\n        out = tf.nn.relu(x)\n        return out\n\n\nclass ResNet18(Model):\n\n    def __init__(self, num_classes, **kwargs):\n        \"\"\"\n            num_classes: number of classes in specific classification task.\n        \"\"\"\n        super().__init__(**kwargs)\n        self.conv_1 = Conv2D(64, (7, 7), strides=2,\n                             padding=\"same\", kernel_initializer=\"he_normal\")\n        self.init_bn = BatchNormalization()\n        self.pool_2 = MaxPool2D(pool_size=(2, 2), strides=2, padding=\"same\")\n        self.res_1_1 = ResnetBlock(64)\n        self.res_1_2 = ResnetBlock(64)\n        self.res_2_1 = ResnetBlock(128, down_sample=True)\n        self.res_2_2 = ResnetBlock(128)\n        self.res_3_1 = ResnetBlock(256, down_sample=True)\n        self.res_3_2 = ResnetBlock(256)\n        self.res_4_1 = ResnetBlock(512, down_sample=True)\n        self.res_4_2 = ResnetBlock(512)\n        self.avg_pool = GlobalAveragePooling2D()\n        self.flat = Flatten()\n        self.fc = Dense(num_classes, activation=\"softmax\")\n\n    def call(self, inputs):\n        out = self.conv_1(inputs)\n        out = self.init_bn(out)\n        out = tf.nn.relu(out)\n        out = self.pool_2(out)\n        for res_block in [self.res_1_1, self.res_1_2, self.res_2_1, self.res_2_2, self.res_3_1, self.res_3_2, self.res_4_1, self.res_4_2]:\n            out = res_block(out)\n        out = self.avg_pool(out)\n        out = self.flat(out)\n        out = self.fc(out)\n        return out\n\n","69b9dbdc":"model = ResNet18(10)\nmodel.build(input_shape = (None,32,32,3))\n#use categorical_crossentropy since the label is one-hot encoded\nfrom keras.optimizers import SGD\n# opt = SGD(learning_rate=0.1,momentum=0.9,decay = 1e-04) #parameters suggested by He [1]\nmodel.compile(optimizer = \"adam\",loss='categorical_crossentropy', metrics=[\"accuracy\"]) \nmodel.summary()\n","ba3f7cbb":"from keras.callbacks import EarlyStopping\n\nes = EarlyStopping(patience= 8, restore_best_weights=True, monitor=\"val_acc\")\n#I did not use cross validation, so the validate performance is not accurate.\nSTEPS = len(X_train) \/ 256\nhistory = model.fit(aug.flow(X_train,Y_train,batch_size = 256), steps_per_epoch=STEPS, batch_size = 256, epochs=50, validation_data=(X_train, Y_train),callbacks=[es])\n\n","e1facd54":"\ndef plotmodelhistory(history): \n    fig, axs = plt.subplots(1,2,figsize=(15,5)) \n    # summarize history for accuracy\n    axs[0].plot(history.history['accuracy']) \n    axs[0].plot(history.history['val_accuracy']) \n    axs[0].set_title('Model Accuracy')\n    axs[0].set_ylabel('Accuracy') \n    axs[0].set_xlabel('Epoch')\n    \n    axs[0].legend(['train', 'validate'], loc='upper left')\n    # summarize history for loss\n    axs[1].plot(history.history['loss']) \n    axs[1].plot(history.history['val_loss']) \n    axs[1].set_title('Model Loss')\n    axs[1].set_ylabel('Loss') \n    axs[1].set_xlabel('Epoch')\n    axs[1].legend(['train', 'validate'], loc='upper left')\n    plt.show()\n\n# list all data in history\nprint(history.history.keys())\nplotmodelhistory(history)\n","0b4628f0":"## Evaluation\n\nModelLoss, ModelAccuracy = model.evaluate(X_train, Y_train)\n\nprint('Model Loss is {}'.format(ModelLoss))\nprint('Model Accuracy is {}'.format(ModelAccuracy))","77e93f6f":"# sub = pd.read_csv(\"..\/input\/cifar-10\/trainLabels.csv\")\n\n# Y_pred = np.argmax(model.predict(X_test),axis = 1) \n# #convert to string label\n# labels = ['Airplane', 'Automobile', 'Bird', 'Cat', 'Deer', 'Dog', 'Frog', 'Horse', 'Ship', 'Truck']\n# str_pred = [labels[i] for i in Y_pred]\n# sub[\"label\"] = pd.DataFrame(str_pred)\n","4e1ec767":"## data augmentation","93309741":"In this note book I will try to implement ResNet-18 using Keras and compare my implementation with the standard implementation provided in keras.application","1e78dae7":"Codes below are taken from my [Github](https:\/\/github.com\/songrise\/CNN_Keras)","544b73d6":"## train curve","71d17a9e":"## preprocess","f2183916":"# Implement ResNet-18 model","17eaebd8":"##  Prediction","8406548b":"# Implementing ResNet-18 Using Keras"}}