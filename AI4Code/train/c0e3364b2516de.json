{"cell_type":{"b76c4c62":"code","22ff3dad":"code","d89d453a":"code","97e75fbd":"code","af6edc46":"code","298f1fcd":"code","c71e2aed":"code","1a88941b":"code","538b5cd5":"code","8dbe75e7":"code","24bcbf68":"code","6ef676fb":"code","66116fb1":"code","e10fa1ef":"code","b1205b25":"code","a9d97e58":"code","1a74226a":"code","ec46efaa":"code","67e8d1a0":"code","67729901":"code","30d7b0ef":"code","2f44a66c":"code","a25995ee":"code","8dcbbc58":"code","3889d7cd":"code","2d4ea476":"code","cf0bec2c":"code","e85041e6":"code","81a3feb7":"code","7e676666":"code","8ed35bdd":"code","d58b40fe":"code","5682be07":"code","c4270f8c":"code","e5df1b20":"code","8b7b2904":"code","25eb6f32":"code","f8c62098":"code","3646d5bb":"code","22260f38":"code","61b190a0":"code","f9f54438":"code","1f8ba4b2":"code","027544fa":"code","6bdd10b7":"code","5d564ff7":"code","6829be3c":"code","4be6e538":"code","7e8b92f8":"code","a169580f":"code","44abe4a3":"code","e8a7ee35":"code","f4bcc5dc":"code","18661b8f":"code","982ffc68":"code","1b007ed0":"markdown","cba88b7f":"markdown","9f1013ac":"markdown","a0e6a19e":"markdown","b4324f9a":"markdown","8ba01a11":"markdown","5eab6105":"markdown","a312afa5":"markdown","0bc08f71":"markdown","4630bbd7":"markdown","e1b9ee4f":"markdown","49a072db":"markdown","5739d887":"markdown","2e6ea533":"markdown","664549f8":"markdown","37ed3c07":"markdown","8890cb81":"markdown","fa14cef6":"markdown","016fd805":"markdown","e1abef82":"markdown","3edbd273":"markdown"},"source":{"b76c4c62":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","22ff3dad":"import pandas as pd\nimport numpy as np\n\nsales_train = pd.read_csv('\/kaggle\/input\/competitive-data-science-predict-future-sales\/sales_train.csv')\nitems = pd.read_csv('\/kaggle\/input\/competitive-data-science-predict-future-sales\/items.csv')\nshops = pd.read_csv('\/kaggle\/input\/competitive-data-science-predict-future-sales\/shops.csv')\nitem_categories = pd.read_csv('\/kaggle\/input\/competitive-data-science-predict-future-sales\/item_categories.csv')\ntest = pd.read_csv('\/kaggle\/input\/competitive-data-science-predict-future-sales\/test.csv')\nsample_submission = pd.read_csv('\/kaggle\/input\/competitive-data-science-predict-future-sales\/sample_submission.csv')","d89d453a":"sales_train.head()","97e75fbd":"sales_train[sales_train['date_block_num'] == 33].sort_values(by='item_cnt_day', ascending=False)","af6edc46":"sales_train[sales_train['item_id'] == 11373].sort_values(['item_price'])","298f1fcd":"sales_train['item_price'][2909818] = np.nan\nsales_train['item_cnt_day'][2909818] = np.nan\n\nsales_train['item_price'][2909818] = sales_train[\n    (sales_train['shop_id'] ==12) & \n    (sales_train['item_id'] == 11373) &\n    (sales_train['date_block_num'] == 33)]['item_price'].median()\n\nsales_train['item_cnt_day'][2909818] = round(sales_train[(sales_train['shop_id'] ==12) &\n                                                         (sales_train['item_id'] == 11373) &\n                                                         (sales_train['date_block_num'] == 33)]['item_cnt_day'].median())","c71e2aed":"sales_train[sales_train['shop_id'] == 12].sort_values(by='item_price', ascending=False)","1a88941b":"sales_train[sales_train['item_id'] == 6066].sort_values(['item_price'])","538b5cd5":"sales_train[sales_train['item_id'] == 11365].sort_values(by='item_price', ascending=False)","8dbe75e7":"sales_train['item_price'][885138] = np.nan\nsales_train['item_price'][885138] = sales_train[(sales_train['item_id'] == 11365) &\n                                              (sales_train['shop_id'] ==12) &\n                                              (sales_train['date_block_num'] == 8)]['item_price'].median()","24bcbf68":"import matplotlib.pyplot as plt\nimport seaborn as sns\n\nplt.figure(figsize=(10,4))\nplt.xlim(-100, 3000)\n\nflierprops = dict(marker='o', markerfacecolor='purple', markersize=6,\n                  linestyle='none', markeredgecolor='black')\nsns.boxplot(x=sales_train.item_cnt_day, flierprops=flierprops)\n\nplt.figure(figsize=(10,4))\nplt.xlim(sales_train.item_price.min(), sales_train.item_price.max()*1.1)\n\nsns.boxplot(x=sales_train.item_price, flierprops=flierprops)","6ef676fb":"sales_train = sales_train[(sales_train.item_price < 300000 )& (sales_train.item_cnt_day < 1000)]","66116fb1":"sales_train = sales_train[sales_train.item_price > 0].reset_index(drop = True)\nsales_train.loc[sales_train.item_cnt_day < 1, \"item_cnt_day\"] = 0","e10fa1ef":"sales_train.loc[sales_train.shop_id == 0, 'shop_id'] = 57\ntest.loc[test.shop_id == 0, 'shop_id'] = 57\n\nsales_train.loc[sales_train.shop_id == 1, 'shop_id'] = 58\ntest.loc[test.shop_id == 1, 'shop_id'] = 58\n\nsales_train.loc[sales_train.shop_id == 10, 'shop_id'] = 11\ntest.loc[test.shop_id == 10, 'shop_id'] = 11","b1205b25":"in_train = np.sort(sales_train['shop_id'].unique())\nin_test = np.sort(test['shop_id'].unique())\n\nprint('list shop_id are in train set but are not in test set')\nfor i in in_train:\n    if i not in in_test:\n        print(i, end=', ')","a9d97e58":"print('Before:', sales_train.shape)","1a74226a":"sales_train = sales_train.merge(test[['shop_id']].drop_duplicates(), how='inner')\n\nsales_train['date'] = pd.to_datetime(sales_train['date'], format = '%d.%m.%Y')","ec46efaa":"print('After:', sales_train.shape)","67e8d1a0":"from itertools import product\n\ngrid = []\nfor block_num in sales_train['date_block_num'].unique():\n    cur_shops = sales_train[sales_train['date_block_num'] == block_num]['shop_id'].unique()\n    cur_items = sales_train[sales_train['date_block_num'] == block_num]['item_id'].unique()\n    grid.append(np.array(list(product(*[cur_shops, cur_items, [block_num]])),dtype='int32'))\n\nidx_columns = ['shop_id', 'item_id', 'date_block_num']\ngrid = pd.DataFrame(np.vstack(grid), columns = idx_columns, dtype = np.int32)\ngrid.shape","67729901":"sales_train['item_cnt_day'] = sales_train['item_cnt_day'].clip(0,20)\n\ngb_cnt = sales_train.groupby(idx_columns)['item_cnt_day'].agg(['sum']).reset_index().rename(columns = {'sum': 'item_cnt_month'})\ngb_cnt['item_cnt_month'] = gb_cnt['item_cnt_month'].clip(0,20).astype(np.int)","30d7b0ef":"train = pd.merge(grid, gb_cnt, how='left', on=idx_columns).fillna(0)\ntrain['item_cnt_month'] = train['item_cnt_month'].astype(int)","2f44a66c":"train.info()","a25995ee":"cur_dtypes = [i for i in train if train[i].dtype in ['int32', 'int64']]\ntrain[cur_dtypes] = train[cur_dtypes].astype(np.int16)\ntrain.info()","8dcbbc58":"train.sort_values(['date_block_num','shop_id','item_id'], inplace=True)\ntrain.head()","3889d7cd":"print(sales_train['item_cnt_day'].sum())\nprint(train['item_cnt_month'].sum())\nprint(gb_cnt['item_cnt_month'].sum())","2d4ea476":"train = train.merge(items[['item_id', 'item_category_id']], on = ['item_id'], how = 'left')\ntest = test.merge(items[['item_id', 'item_category_id']], on = ['item_id'], how = 'left')","cf0bec2c":"categ_translated = pd.read_csv('..\/input\/predict-future-sales-english\/item_categories-translated.csv')\n\nitem_categories = item_categories.merge(categ_translated[['item_category_name_translated', 'item_category_id']], on=['item_category_id'], how='left')","e85041e6":"for idx, name in enumerate(item_categories.item_category_name_translated):\n    print(idx, name)","81a3feb7":"list_categ = list(item_categories.item_category_name_translated)\n\nfor i in range(1, 8):\n    list_categ[i] = 'Accessories'\nfor i in range(10, 18):\n    list_categ[i] = 'Game Consoles'\nfor i in range(18, 25):\n    list_categ[i] = 'Games'\nlist_categ[25] = 'Accessories for games'\nfor i in range(26, 28):\n    list_categ[i] = 'Phone Games'\nfor i in range(28, 32):\n    list_categ[i] = 'PC Games'\nfor i in range(32, 37):\n    list_categ[i] = 'Cards'\nfor i in range(37, 42):\n    list_categ[i] = 'Cinema'\nfor i in range(42, 55):\n    list_categ[i] = 'Books'\nfor i in range(55, 61):\n    list_categ[i] = 'Music'\nfor i in range(61, 73):\n    list_categ[i] = 'Gifts'\nfor i in range(73, 79):\n    list_categ[i] = 'Programs'\nfor i in range(79, 81):\n    list_categ[i] = 'Office'\nfor i in range(81, 83):\n    list_categ[i] = 'Clean'","7e676666":"from sklearn import preprocessing\n\nlb_encode = preprocessing.LabelEncoder()\nitem_categories['item_category_id_fixed'] = lb_encode.fit_transform(list_categ)\n\ntrain = train.merge(item_categories[['item_category_id_fixed', 'item_category_id']], on = ['item_category_id'], how = 'left')\n\ntest = test.merge(item_categories[['item_category_id_fixed', 'item_category_id']], on = ['item_category_id'], how = 'left')","8ed35bdd":"train.head()","d58b40fe":"from tqdm import tqdm\nfrom sklearn.model_selection import KFold\n\nglobal_mean =  train['item_cnt_month'].mean()\ny_train = train['item_cnt_month'].values\n\nmean_encoded_col = ['shop_id', 'item_id', 'item_category_id', 'item_category_id_fixed']\n\nfor col in tqdm(mean_encoded_col):\n    col_train = train[[col] + ['item_cnt_month']]\n    corr_coefs = pd.DataFrame(columns = ['Cor'])\n    # Mean encoding KFold scheme\n    kf = KFold(n_splits = 5, shuffle = False, random_state = 0)\n    \n    col_train[col + '_cnt_month_mean_KFold'] = global_mean\n    for idx_train, idx_val in kf.split(col_train):\n        X_train, X_val = col_train.iloc[idx_train], col_train.iloc[idx_val]\n        means = X_val[col].map(X_train.groupby(by=col)['item_cnt_month'].mean())\n        X_val[col + '_cnt_month_mean_KFold'] = means\n        col_train.iloc[idx_val] = X_val\n        print(X_val.head(5))\n    col_train.fillna(global_mean, inplace=True)\n    corr_coefs.loc[col + '_cnt_month_mean_KFold'] = np.corrcoef(y_train, col_train[col + '_cnt_month_mean_KFold'])[0][1]\n    \n    # Mean encodings - Leave-one-out scheme\n    item_id_target_sum = col_train.groupby(col)['item_cnt_month'].sum()\n    item_id_target_count = col_train.groupby(col)['item_cnt_month'].count()\n    col_train[col + '_cnt_month_sum'] = col_train[col].map(item_id_target_sum)\n    col_train[col + '_cnt_month_count'] = col_train[col].map(item_id_target_count)\n    col_train[col + '_target_mean_LOO'] = (col_train[col + '_cnt_month_sum'] - col_train['item_cnt_month']) \/ (col_train[col + '_cnt_month_count'] - 1)\n    col_train.fillna(global_mean, inplace = True)\n    corr_coefs.loc[col + '_target_mean_LOO'] = np.corrcoef(y_train, col_train[col + '_target_mean_LOO'])[0][1]\n    \n    # Mean encodings - Smoothing\n    item_id_target_mean = col_train.groupby(col)['item_cnt_month'].mean()\n    item_id_target_count = col_train.groupby(col)['item_cnt_month'].count()\n    col_train[col + '_cnt_month_mean'] = col_train[col].map(item_id_target_mean)\n    col_train[col + '_cnt_month_count'] = col_train[col].map(item_id_target_count)\n    alpha = 100\n    col_train[col + '_cnt_month_mean_Smooth'] = (col_train[col + '_cnt_month_mean'] *  col_train[col + '_cnt_month_count'] + global_mean * alpha) \/ (alpha + col_train[col + '_cnt_month_count'])\n    col_train[col + '_cnt_month_mean_Smooth'].fillna(global_mean, inplace=True)\n    corr_coefs.loc[col + '_cnt_month_mean_Smooth'] = np.corrcoef(y_train, col_train[col + '_cnt_month_mean_Smooth'])[0][1]\n    \n    # 3.1.4 Mean encodings - Expanding mean scheme\n    cumsum = col_train.groupby(col)['item_cnt_month'].cumsum() - col_train['item_cnt_month']\n    sumcnt = col_train.groupby(col).cumcount()\n    col_train[col + '_cnt_month_mean_Expanding'] = cumsum \/ sumcnt\n    col_train[col + '_cnt_month_mean_Expanding'].fillna(global_mean, inplace=True)\n    corr_coefs.loc[col + '_cnt_month_mean_Expanding'] = np.corrcoef(y_train, col_train[col + '_cnt_month_mean_Expanding'])[0][1]\n    train = pd.concat([train, col_train[corr_coefs['Cor'].idxmax()]], axis = 1)\n    print(corr_coefs.sort_values('Cor'))","5682be07":"# Nov, 2015\ntest['date_block_num'] = 34\n\nall_data = pd.concat([train, test], axis = 0)\nall_data = all_data.drop(columns = ['ID'])","c4270f8c":"float_col = [f for f in all_data if all_data[f].dtype == 'float64']\nint_col = [i for i in all_data if all_data[i].dtype == 'int64']\n\nall_data[float_col] = all_data[float_col].astype(np.float32)\nall_data[int_col] = all_data[int_col].astype(np.int16)","e5df1b20":"index_cols = ['shop_id', 'item_id', 'item_category_id', 'item_category_id_fixed', 'date_block_num']\ncols_to_rename = list(all_data.columns.difference(index_cols))\n\nshift_range = [1, 2, 3, 4, 12]\nfor month_shift in tqdm(shift_range):\n    train_shift = all_data[index_cols + cols_to_rename].copy()\n    train_shift['date_block_num'] = train_shift['date_block_num'] + month_shift\n    \n    foo = lambda x: '{}_lag_{}'.format(x, month_shift) if x in cols_to_rename else x\n    train_shift = train_shift.rename(columns=foo)\n    \n    all_data = pd.merge(all_data, train_shift, on=index_cols, how='left').fillna(0)\ndel train_shift","8b7b2904":"# Don't use old data from year 2013\nall_data = all_data[all_data['date_block_num'] >= 12]\n\nlag_cols = [col for col in all_data.columns if col[-1] in [str(item) for item in shift_range]]\n\nfloat_col = [f for f in all_data if all_data[f].dtype == 'float64']\nint_col = [i for i in all_data if all_data[i].dtype in ['int64', 'int32']]\n\nall_data[float_col] = all_data[float_col].astype(np.float32)\nall_data[int_col] = all_data[int_col].astype(np.int16)","25eb6f32":"dates_train = sales_train[['date', 'date_block_num']].drop_duplicates()\ndates_test = dates_train[dates_train['date_block_num'] == 34-12]\ndates_test['date_block_num'] = 34\ndates_test['date'] = dates_test['date'] + pd.DateOffset(years=1)\ndates_all = pd.concat([dates_train, dates_test])\n\n\ndates_all['dow'] = dates_all['date'].dt.dayofweek\ndates_all['year'] = dates_all['date'].dt.year\ndates_all['month'] = dates_all['date'].dt.month\n\ndates_all = pd.get_dummies(dates_all, columns=['dow'])\n\ndow_col = ['dow_' + str(x) for x in range(7)]\n\ndate_features = dates_all.groupby(['year', 'month', 'date_block_num'])[dow_col].agg('sum').reset_index()\ndate_features['days_of_month'] = date_features[dow_col].sum(axis=1)\ndate_features['year'] = date_features['year'] - 2013\ndate_features = date_features[['month', 'year', 'days_of_month', 'date_block_num']]\n\nall_data = all_data.merge(date_features, on = 'date_block_num', how = 'left')\n\ndate_columns = date_features.columns.difference(set(index_cols))","f8c62098":"from sklearn.preprocessing import StandardScaler\n\ntrain = all_data[all_data['date_block_num']!= all_data['date_block_num'].max()]\ntest = all_data[all_data['date_block_num']== all_data['date_block_num'].max()]\nsc = StandardScaler()\n\nto_drop_cols = ['date_block_num']\nfeature_columns = list(set(lag_cols + index_cols + list(date_columns)).difference(to_drop_cols))\n\ntrain[feature_columns] = sc.fit_transform(train[feature_columns])\n\ntest[feature_columns] = sc.transform(test[feature_columns])\n\nall_data = pd.concat([train, test], axis = 0)","3646d5bb":"float_col = [f for f in all_data if all_data[f].dtype == 'float64']\nint_col = [i for i in all_data if all_data[i].dtype in ['int64', 'int32']]\n\nall_data[float_col] = all_data[float_col].astype(np.float32)\nall_data[int_col] = all_data[int_col].astype(np.int16)","22260f38":"dates = all_data['date_block_num']\nprint('Test `date_block_num` is %d' % dates.max())\nprint(len(feature_columns))","61b190a0":"all_data.info()","f9f54438":"all_data[all_data[\"date_block_num\"]==34].shape","1f8ba4b2":"X_test.shape","027544fa":"X_train = all_data[all_data.date_block_num < 33].drop(['item_cnt_month'], axis=1)\nY_train = all_data[all_data.date_block_num < 33]['item_cnt_month']\nX_valid = all_data[all_data.date_block_num == 33].drop(['item_cnt_month'], axis=1)\nY_valid = all_data[all_data.date_block_num == 33]['item_cnt_month']\nX_test = all_data[all_data.date_block_num == 34].drop(['item_cnt_month'], axis=1)","6bdd10b7":"Y_train = Y_train.clip(0, 20)\nY_valid = Y_valid.clip(0, 20)","5d564ff7":"from xgboost import XGBRegressor\nfrom matplotlib.pylab import rcParams\n\nmodel = XGBRegressor(max_depth=10, n_estimators=1000, min_child_weight=0.5, colsample_bytree=0.8, subsample=0.8, eta=0.1, seed=42)\n\nmodel.fit(X_train, Y_train, eval_metric=\"rmse\", eval_set=[(X_train, Y_train), (X_valid, Y_valid)], verbose=True, early_stopping_rounds = 20)","6829be3c":"test_1 = pd.read_csv('\/kaggle\/input\/competitive-data-science-predict-future-sales\/test.csv')","4be6e538":"Y_pred = model.predict(X_valid).clip(0, 20)\nY_test = model.predict(X_test).clip(0, 20)\n\nsubmission = pd.DataFrame({'ID': test_1.index, 'item_cnt_month': Y_test})\nsubmission.to_csv('xgb_submission.csv', index=False)","7e8b92f8":"from xgboost import plot_importance\n\ndef plot_features(booster, figsize):    \n    fig, ax = plt.subplots(1,1,figsize=figsize)\n    return plot_importance(booster=booster, ax=ax)\n\nplot_features(model, (10,14))","a169580f":"from sklearn.metrics import mean_squared_error\nfrom math import sqrt\nscoringMethod = 'r2'\nnum_first_level_models = 3\n\n# Train meta-features M = 15 (12 + 15 = 27)\nmonths_to_generate_meta_features = range(27,last_block +1)\nmask = dates.isin(months_to_generate_meta_features)\n\ntarget = 'item_cnt_month'\n\ny_all_level2 = all_data[target][mask].values\n\nX_all_level2 = np.zeros([y_all_level2.shape[0], num_first_level_models])","44abe4a3":"from sklearn.linear_model import LinearRegression, SGDRegressor\nfrom xgboost import XGBRegressor\nimport lightgbm as lgb\nfrom keras.models import Sequential\nfrom keras.layers import Dense\nfrom keras.wrappers.scikit_learn import KerasRegressor\n\nslice_start = 0\nTarget = 'item_cnt_month'\n\nfor cur_block_num in tqdm(months_to_generate_meta_features):\n    print('-' * 50)\n    print('Start training for month%d'% cur_block_num)\n\n    cur_X_train = all_data.loc[dates <  cur_block_num][feature_columns]\n    cur_X_test =  all_data.loc[dates == cur_block_num][feature_columns]\n\n    cur_y_train = all_data.loc[dates <  cur_block_num, Target].values\n    cur_y_test =  all_data.loc[dates == cur_block_num, Target].values\n\n    # Create Numpy arrays of train, test and target dataframes to feed into models\n    train_x = cur_X_train.values\n    train_y = cur_y_train.ravel()\n    test_x = cur_X_test.values\n    test_y = cur_y_test.ravel()\n\n    preds = []\n\n    sgdr= SGDRegressor(penalty = 'l2', random_state = 0 )\n    lgb_params = {'feature_fraction': 0.75, 'metric': 'rmse', 'nthread':1, 'min_data_in_leaf': 2**7,\n                  'bagging_fraction': 0.75, 'learning_rate': 0.03, 'objective': 'mse', 'bagging_seed': 2**7,\n                  'num_leaves': 2**7, 'bagging_freq':1, 'verbose':0 }\n\n    estimators = [sgdr]\n    \n    for estimator in estimators:\n        print('Training Model %d: %s'%(len(preds), estimator.__class__.__name__))\n        estimator.fit(train_x, train_y)\n        pred_test = estimator.predict(test_x)\n        preds.append(pred_test)\n\n        # pred_train = estimator.predict(train_x)\n        # print('Train RMSE for %s is %f' % (estimator.__class__.__name__, sqrt(mean_squared_error(cur_y_train, pred_train))))\n        print('Test RMSE for %s is %f' % (estimator.__class__.__name__, sqrt(mean_squared_error(cur_y_test, pred_test))))\n\n    print('Training Model %d: %s'%(len(preds), 'lightgbm'))\n    estimator = lgb.train(lgb_params, lgb.Dataset(train_x, label=train_y), 300)\n    pred_test = estimator.predict(test_x)\n    preds.append(pred_test)\n    \n    # pred_train = estimator.predict(train_x)\n    # print('Train RMSE for %s is %f' % ('lightgbm', sqrt(mean_squared_error(cur_y_train, pred_train))))\n    print('Test RMSE for %s is %f' % ('lightgbm', sqrt(mean_squared_error(cur_y_test, pred_test))))\n\n    print('Training Model %d: %s'%(len(preds), 'keras'))\n\n    def baseline_model():\n        model = Sequential()\n        model.add(Dense(20, input_dim=train_x.shape[1], kernel_initializer='uniform', activation='softplus'))\n        model.add(Dense(1, kernel_initializer='uniform', activation = 'relu'))\n        \n        # Compile model\n        model.compile(loss='mse', optimizer='Nadam', metrics=['mse'])\n        # model.compile(loss='mean_squared_error', optimizer='adam')\n        return model\n\n    estimator = KerasRegressor(build_fn=baseline_model, verbose=1, epochs=5, batch_size = 55000)\n    estimator.fit(train_x, train_y)\n    pred_test = estimator.predict(test_x)\n    preds.append(pred_test)\n\n\n    slice_end = slice_start + cur_X_test.shape[0]\n\n    X_all_level2[ slice_start : slice_end , :] = np.c_[preds].transpose()\n\n    slice_start = slice_end","e8a7ee35":"test_nrow = len(preds[0])\nX_train_level2 = X_all_level2[ : -test_nrow, :]\nX_test_level2 = X_all_level2[ -test_nrow: , :]\ny_train_level2 = y_all_level2[ : -test_nrow]\ny_test_level2 = y_all_level2[ -test_nrow : ]","f4bcc5dc":"from sklearn.linear_model import LinearRegression, SGDRegressor\n\npred_list = {}\n\nlr = LinearRegression()\nlr.fit(X_train_level2, y_train_level2)\n\ntest_preds_lr_stacking = lr.predict(X_test_level2)\ntrain_preds_lr_stacking = lr.predict(X_train_level2)\n\nprint('Train R-squared for %s is %f' %('train_preds_lr_stacking', sqrt(mean_squared_error(y_train_level2, train_preds_lr_stacking))))\n\npred_list['test_preds_lr_stacking'] = test_preds_lr_stacking","18661b8f":"sgdr= SGDRegressor(penalty = 'l2', random_state = 0 )\n\nsgdr.fit(X_train_level2, y_train_level2)\n\ntest_preds_sgdr_stacking = sgdr.predict(X_test_level2)\ntrain_preds_sgdr_stacking = sgdr.predict(X_train_level2)\n\nprint('Train R-squared for %s is %f' %('train_preds_lr_stacking', sqrt(mean_squared_error(y_train_level2, train_preds_sgdr_stacking))))\n\npred_list['test_preds_sgdr_stacking'] = test_preds_sgdr_stacking","982ffc68":"submission = sample_submission\nsubmission_path = '.\/'\n\nfor pred_ver in ['lr_stacking', 'sgdr_stacking']:\n    print(pred_list['test_preds_' + pred_ver].clip(0,20).mean())\n    \n    submission['item_cnt_month'] = pred_list['test_preds_' + pred_ver].clip(0,20)\n    \n    submission[['ID', 'item_cnt_month']].to_csv('{0}\/{1}.csv'.format(submission_path, pred_ver), index = False)","1b007ed0":"Something wrong with first row(2909818). Let check it","cba88b7f":"Split train and test","9f1013ac":"I using item_categories-translated by [**deargle**](https:\/\/www.kaggle.com\/c\/competitive-data-science-predict-future-sales\/discussion\/54949)","a0e6a19e":"# 4. Submision","b4324f9a":"## Remove outliers","8ba01a11":"### Aggregate data","5eab6105":"Add item\/shop pair mean-encodings","a312afa5":"Sort train data","0bc08f71":"in Evalution :\"*Submissions are evaluated by root mean squared error (RMSE). True target values are clipped into [0,20] range.\"* So, clip\\[0,20\\] in train and test set","4630bbd7":"# 3. Model","e1b9ee4f":"### Feature Engineering ","49a072db":"## Droping all the shop are in sales_train but are not in test","5739d887":"Scale feature columns","2e6ea533":"For every month we create a grid from all shops\/items combinations from that month","664549f8":"Almost item_id 11373 sold by shop_id 12. I will check that shop ~~","37ed3c07":"Using Oct, 2015 as importance feature to predict Nov, 2015 sold.<br>\ndate_block_num of Oct,2015 is 33.","8890cb81":"### Encode item_category_name","fa14cef6":"# 2. Data Preprocessing","016fd805":"Creating date feature","e1abef82":"# 1. Read data","3edbd273":"join aggregated data to the grid"}}