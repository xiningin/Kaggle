{"cell_type":{"10dab812":"code","10a6c51a":"code","7f085ffe":"code","bf2c0146":"code","82b0be91":"code","b32e0eb0":"code","fb424602":"code","3a1f19ab":"code","30505b27":"code","100381c0":"code","55be7fb1":"code","15b83cc7":"code","eb77438b":"code","edc7cec1":"code","9679dde9":"code","8de14573":"code","c6b2608e":"code","4da4d22d":"code","32246d4c":"code","cda3dfbf":"code","6c74d4a8":"code","326248b6":"code","413a7a2a":"code","5c7f4c1b":"code","33fb3393":"code","1324419e":"code","3507f538":"code","9b27fb6d":"code","65c7af67":"code","ac8f880f":"code","bdb5a333":"code","30f5e0c8":"code","9034c81d":"code","992f3597":"markdown","668700c8":"markdown","cf53fe38":"markdown","918734ac":"markdown","2b57e905":"markdown"},"source":{"10dab812":"# more common imports\nimport pandas as pd\nimport numpy as np\nfrom collections import Counter\nimport re\n\n# languange processing imports\nimport nltk\nfrom gensim.corpora import Dictionary\n# preprocessing imports\nfrom sklearn.preprocessing import LabelEncoder\n\n# model imports\nfrom gensim.models.ldamulticore import LdaMulticore\nfrom gensim.models.word2vec import Word2Vec\nfrom sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\nfrom sklearn.linear_model import LogisticRegression\n# hyperparameter training imports\nfrom sklearn.model_selection import GridSearchCV","10a6c51a":"# visualization imports\nfrom IPython.display import display\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport matplotlib.image as mpimg\nimport base64\nimport io\n%matplotlib inline\nsns.set()  # defines the style of the plots to be seaborn style","7f085ffe":"train_data = pd.read_csv('data.csv')","bf2c0146":"train_data.head()","82b0be91":"document_lengths = np.array(list(map(len, train_data.text.str.split(' '))))\n\nprint(\"The average number of words in a document is: {}.\".format(np.mean(document_lengths)))\nprint(\"The minimum number of words in a document is: {}.\".format(min(document_lengths)))\nprint(\"The maximum number of words in a document is: {}.\".format(max(document_lengths)))","b32e0eb0":"fig, ax = plt.subplots(figsize=(15,6))\n\nax.set_title(\"Distribution of number of words\", fontsize=16)\nax.set_xlabel(\"Number of words\")\nsns.distplot(document_lengths, bins=50, ax=ax);","fb424602":"print(\"There are {} documents with over 150 words.\".format(sum(document_lengths > 450)))\n\nshorter_documents = document_lengths[document_lengths <= 450]","3a1f19ab":"fig, ax = plt.subplots(figsize=(15,6))\n\nax.set_title(\"Distribution of number of words\", fontsize=16)\nax.set_xlabel(\"Number of words\")\nsns.distplot(shorter_documents, bins=50, ax=ax);","30505b27":"# find and remove non-ascii words\n# I stored our special word in a variable for later use\nour_special_word = 'qwerty'\n\ndef remove_ascii_words(df):\n    \"\"\" removes non-ascii characters from the 'texts' column in df.\n    It returns the words containig non-ascii characers.\n    \"\"\"\n    non_ascii_words = []\n    for i in range(len(df)):\n        for word in df.loc[i, 'text'].split(' '):\n            if any([ord(character) >= 128 for character in word]):\n                non_ascii_words.append(word)\n                df.loc[i, 'text'] = df.loc[i, 'text'].replace(word, our_special_word)\n    return non_ascii_words\n\nnon_ascii_words = remove_ascii_words(train_data)\n\nprint(\"Replaced {} words with characters with an ordinal >= 128 in the train data.\".format(\n    len(non_ascii_words)))","100381c0":"def get_good_tokens(sentence):\n    replaced_punctation = list(map(lambda token: re.sub('[^0-9A-Za-z!?]+', '', token), sentence))\n    removed_punctation = list(filter(lambda token: token, replaced_punctation))\n    return removed_punctation","55be7fb1":"# Here we get transform the documents into sentences for the word2vecmodel\n# we made a function such that later on when we make the submission, we don't need to write duplicate code\ndef w2v_preprocessing(df):\n    \"\"\" All the preprocessing steps for word2vec are done in this function.\n    All mutations are done on the dataframe itself. So this function returns\n    nothing.\n    \"\"\"\n    df['text'] = df.text.str.lower()\n    df['document_sentences'] = df.text.str.split('.')  # split texts into individual sentences\n    df['tokenized_sentences'] = list(map(lambda sentences:\n                                         list(map(nltk.word_tokenize, sentences)),\n                                         df.document_sentences))  # tokenize sentences\n    df['tokenized_sentences'] = list(map(lambda sentences:\n                                         list(map(get_good_tokens, sentences)),\n                                         df.tokenized_sentences))  # remove unwanted characters\n    df['tokenized_sentences'] = list(map(lambda sentences:\n                                         list(filter(lambda lst: lst, sentences)),\n                                         df.tokenized_sentences))  # remove empty lists\n\nw2v_preprocessing(train_data)","15b83cc7":"def lda_get_good_tokens(df):\n    df['text'] = df.text.str.lower()\n    df['tokenized_text'] = list(map(nltk.word_tokenize, df.text))\n    df['tokenized_text'] = list(map(get_good_tokens, df.tokenized_text))\n\nlda_get_good_tokens(train_data)","eb77438b":"tokenized_only_dict = Counter(np.concatenate(train_data.tokenized_text.values))\n\ntokenized_only_df = pd.DataFrame.from_dict(tokenized_only_dict, orient='index')\ntokenized_only_df.rename(columns={0: 'count'}, inplace=True)","edc7cec1":"tokenized_only_df.sort_values('count', ascending=False, inplace=True)","9679dde9":"# I made a function out of this since I will use it again later on \ndef word_frequency_barplot(df, nr_top_words=50):\n    \"\"\" df should have a column named count.\n    \"\"\"\n    fig, ax = plt.subplots(1,1,figsize=(20,5))\n\n    sns.barplot(list(range(nr_top_words)), df['count'].values[:nr_top_words], palette='hls', ax=ax)\n\n    ax.set_xticks(list(range(nr_top_words)))\n    ax.set_xticklabels(df.index[:nr_top_words], fontsize=14, rotation=90)\n    return ax\n    \nax = word_frequency_barplot(tokenized_only_df)\nax.set_title(\"Word Frequencies\", fontsize=16);","8de14573":"def remove_stopwords(df):\n    \"\"\" Removes stopwords based on a known set of stopwords\n    available in the nltk package. In addition, we include our\n    made up word in here.\n    \"\"\"\n    # Luckily nltk already has a set of stopwords that we can remove from the texts.\n    stopwords = nltk.corpus.stopwords.words('english')\n    # we'll add our own special word in here 'qwerty'\n    stopwords.append(our_special_word)\n\n    df['stopwords_removed'] = list(map(lambda doc:\n                                       [word for word in doc if word not in stopwords],\n                                       df['tokenized_text']))\n\nremove_stopwords(train_data)","c6b2608e":"def stem_words(df):\n    lemm = nltk.stem.WordNetLemmatizer()\n    df['lemmatized_text'] = list(map(lambda sentence:\n                                     list(map(lemm.lemmatize, sentence)),\n                                     df.stopwords_removed))\n\n    p_stemmer = nltk.stem.porter.PorterStemmer()\n    df['stemmed_text'] = list(map(lambda sentence:\n                                  list(map(p_stemmer.stem, sentence)),\n                                  df.lemmatized_text))\n\nstem_words(train_data)","4da4d22d":"dictionary = Dictionary(documents=train_data.stemmed_text.values)\n\nprint(\"Found {} words.\".format(len(dictionary.values())))","32246d4c":"dictionary.filter_extremes(no_above=0.8, no_below=3)\n\ndictionary.compactify()  # Reindexes the remaining words after filtering\nprint(\"Left with {} words.\".format(len(dictionary.values())))","cda3dfbf":"#Make a BOW for every document\ndef document_to_bow(df):\n    df['bow'] = list(map(lambda doc: dictionary.doc2bow(doc), df.stemmed_text))\n    \ndocument_to_bow(train_data)","6c74d4a8":"# we make a function such that later on when we make the submission, we don't need to write duplicate code\ndef lda_preprocessing(df):\n    \"\"\" All the preprocessing steps for LDA are combined in this function.\n    All mutations are done on the dataframe itself. So this function returns\n    nothing.\n    \"\"\"\n    lda_get_good_tokens(df)\n    remove_stopwords(df)\n    stem_words(df)\n    document_to_bow(df)","326248b6":"cleansed_words_df = pd.DataFrame.from_dict(dictionary.token2id, orient='index')\ncleansed_words_df.rename(columns={0: 'id'}, inplace=True)\n\ncleansed_words_df['count'] = list(map(lambda id_: dictionary.dfs.get(id_), cleansed_words_df.id))\ndel cleansed_words_df['id']","413a7a2a":"cleansed_words_df.sort_values('count', ascending=False, inplace=True)","5c7f4c1b":"ax = word_frequency_barplot(cleansed_words_df)\nax.set_title(\"Document Frequencies (Number of documents a word appears in)\", fontsize=16);","33fb3393":"corpus = train_data.bow","1324419e":"%%time\nnum_topics = 5\n#A multicore approach to decrease training time\nLDAmodel = LdaMulticore(corpus=corpus,\n                        id2word=dictionary,\n                        num_topics=num_topics,\n                        workers=4,\n                        chunksize=4000,\n                        passes=7,\n                        alpha=0.91,\n                        eta=0.31)","3507f538":"word_dict = {}\ntopics = LDAmodel.show_topics(5, 25)\nword_dict = {'Topic '+str(i):[x.split('*') for x in words.split('+')] for i, words in LDAmodel.show_topics(5, 25)}\ntopic_df = pd.DataFrame.from_dict(word_dict)\ntopic_df","9b27fb6d":"def get_most_probable_topic(bow):\n    abc= LDAmodel.get_document_topics(bow)\n    abc.sort(key=lambda x:x[1], reverse=True)\n    return abc[0][0]","65c7af67":"train_data['topic'] = train_data['bow'].apply(lambda x: get_most_probable_topic(x))","ac8f880f":"submission = train_data[['Id', 'topic']]","bdb5a333":"submission.replace({'topic': {0: \"sports_news\",\n                                1: \"Automobiles\",\n                                2: \"glassdoor_reviews\",\n                                3: \"room_rentals\",\n                                4: \"tech_news\"}}, inplace = True)","30f5e0c8":"# submission.to_csv('submission1.csv', index=False)","9034c81d":"submission","992f3597":"## Cleaning and Feature Creation","668700c8":"Using parameters from the previously tuned model.","cf53fe38":"## EDA","918734ac":"# Classification","2b57e905":"## Model Training"}}