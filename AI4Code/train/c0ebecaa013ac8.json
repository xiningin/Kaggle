{"cell_type":{"c95fb1a6":"code","8f45cb06":"code","18b11df8":"code","252c7f8a":"code","57b3ca1d":"code","29c21c5a":"code","3cd8c611":"code","c4d901ec":"code","822ef024":"code","e4bf2f0c":"code","1f97a884":"code","67a76bcc":"code","789c4fd3":"code","101f2d04":"code","34ac1fd9":"code","68a7e39c":"code","fd3b94fb":"markdown","8438ff9f":"markdown","ab37f816":"markdown","13a251d8":"markdown","dfd516e4":"markdown","3779203c":"markdown","019a2e65":"markdown","6e18c51d":"markdown","9875faf7":"markdown"},"source":{"c95fb1a6":"%matplotlib inline\nfrom fastai.basics import *","8f45cb06":"n=100","18b11df8":"x = torch.ones(n,2) \nx[:,0].uniform_(-1.,1)\nx[:5]","252c7f8a":"a = tensor(3.,2); a","57b3ca1d":"y = x@a + torch.rand(n)","29c21c5a":"plt.scatter(x[:,0], y);","3cd8c611":"def mse(y_hat, y): return ((y_hat-y)**2).mean()","c4d901ec":"a = tensor(-1.,1)","822ef024":"y_hat = x@a\nmse(y_hat, y)","e4bf2f0c":"plt.scatter(x[:,0],y)\nplt.scatter(x[:,0],y_hat);","1f97a884":"a = nn.Parameter(a); a","67a76bcc":"def update():\n    y_hat = x@a\n    loss = mse(y, y_hat)\n    if t % 10 == 0: print(loss)\n    loss.backward()\n    with torch.no_grad():\n        a.sub_(lr * a.grad)\n        a.grad.zero_()","789c4fd3":"lr = 1e-1\nfor t in range(100): update()","101f2d04":"plt.scatter(x[:,0],y)\nplt.scatter(x[:,0],x@a);\n","34ac1fd9":"from matplotlib import animation, rc\nrc('animation', html='jshtml')","68a7e39c":"a = nn.Parameter(tensor(-1.,1))\n\nfig = plt.figure()\nplt.scatter(x[:,0], y, c='orange')\nline, = plt.plot(x[:,0], x@a)\nplt.close()\n\ndef animate(i):\n    update()\n    line.set_ydata(x@a)\n    return line,\n\nanimation.FuncAnimation(fig, animate, np.arange(0, 100), interval=20)","fd3b94fb":"[Lesson Video Link](https:\/\/course.fast.ai\/videos\/?lesson=2)\n\n[Lesson resources and updates](https:\/\/forums.fast.ai\/t\/lesson-2-official-resources-and-updates\/28630)\n\n[Lesson chat](https:\/\/forums.fast.ai\/t\/lesson-2-chat\/28722)\n\n[Further discussion thread](https:\/\/forums.fast.ai\/t\/lesson-2-further-discussion\/28706)\n\nNote: This is a mirror of the FastAI Lesson 2 Nb. \nPlease thank the amazing team behind fast.ai for creating these, I've merely created a mirror of the same here\nFor complete info on the course, visit course.fast.ai","8438ff9f":"## Linear Regression problem\nThe goal of linear regression is to fit a line to a set of points.","ab37f816":"In practice, we don't calculate on the whole file at once, but we use mini-batches.\n\n## Vocab\n\n -  Learning rate\n -  Epoch\n -  Minibatch\n -  SGD\n -  Model \/ Architecture\n -  Parameters\n -  Loss function\n\nFor classification problems, we use *cross entropy loss*, also known as *negative log likelihood loss*. This penalizes incorrect confident predictions, and correct unconfident predictions.","13a251d8":"You want to find **parameters** (weights) a such that you minimize the error between the points and the line x@a. Note that here a is unknown. For a regression problem the most common error function or loss function is the **mean squared error**.","dfd516e4":"## **Animate it!**","3779203c":"\nSo far we have specified the *model* (linear regression) and the *evaluation criteria* (or loss function). Now we need to handle *optimization*; that is, how do we find the best values for a? How do we find the best *fitting* linear regression.\n","019a2e65":"## Gradient Descent\n\nWe would like to find the values of a that minimize mse_loss.\n\n**Gradient descent** is an algorithm that minimizes functions. Given a function defined by a set of parameters, gradient descent starts with an initial set of parameter values and iteratively moves toward a set of parameter values that minimize the function. This iterative minimization is achieved by taking steps in the negative direction of the function gradient.\n\nHere is gradient descent implemented in [PyTorch](https:\/\/pytorch.org).","6e18c51d":"Suppose we believe a = (-1.0,1.0) then we can compute y_hat which is our prediction and then compute our error.","9875faf7":"In this part of the lecture we explain Stochastic Gradient Descent (SGD) which is an **optimization** method commonly used in neural networks. We will illustrate the concepts with concrete examples."}}