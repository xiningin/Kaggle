{"cell_type":{"4bc131a3":"code","b5368a7f":"code","ef674e96":"code","9bf9fad0":"code","25b575b5":"code","86c08ab0":"code","03ceb2de":"code","333fd3bb":"code","32cab4b0":"code","f3322e68":"code","b8f838ac":"code","e7762c94":"code","6f84625c":"code","d8d50d41":"code","91056b2d":"code","6f20fa37":"code","00f292f8":"code","7a123626":"code","114f810a":"code","8cea37bb":"code","e640b47d":"code","99379555":"code","45dc366b":"code","c845c180":"code","b91b8745":"code","375f67bd":"code","d64611fa":"code","be420e09":"code","1c58541a":"code","434760c8":"code","71063a0c":"code","f00b46f3":"code","ef65bbd9":"code","cabc1b80":"code","3ea596a2":"code","4ff76874":"code","7bd45e92":"code","f15f2fb2":"code","03921dd8":"code","05850545":"code","67f55c92":"code","23461aa0":"code","87f1aa7f":"code","28e08b1e":"code","5228eb19":"code","90d92bdf":"code","64776b51":"code","a626ef2b":"code","0b77e32d":"code","8a281557":"code","fea178f5":"code","2ef26b93":"code","fd8b7663":"code","3bb06cde":"code","3f25ab6a":"code","d35086de":"code","e9ab1ca1":"markdown","a1317324":"markdown","3186198e":"markdown","92df26ed":"markdown","02376bc3":"markdown","ef136912":"markdown","fee8874b":"markdown","475b2250":"markdown","229fdf6d":"markdown","8afd0558":"markdown","91153e5e":"markdown","304407de":"markdown","aa60d16f":"markdown","6805f604":"markdown","7ff86e41":"markdown","a71188e2":"markdown","62f8368c":"markdown","7933f255":"markdown","4f7be17a":"markdown","6174319c":"markdown","619c3ac8":"markdown","724ab942":"markdown","3630df99":"markdown","76c74844":"markdown","3b67acce":"markdown","0b3e8768":"markdown","eaa2c3bc":"markdown","afce55a7":"markdown","295677e4":"markdown","43e9b9c1":"markdown","3f84a3f8":"markdown","1059384e":"markdown","9bd92f2c":"markdown","fbfb63ea":"markdown","dc14ecc1":"markdown","860714e5":"markdown","bb55f273":"markdown","fa5852de":"markdown","ee99296a":"markdown"},"source":{"4bc131a3":"import json\nfrom tqdm.notebook import tqdm\n\nimport numpy as np \nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nimport tensorflow as tf\nfrom tensorflow.keras.layers import LSTM, Bidirectional, GlobalMaxPooling1D, SpatialDropout1D, Dense, Dropout, Input, concatenate, Conv1D, Activation, Flatten\n\nfrom nltk.corpus import stopwords\nimport re","b5368a7f":"# data to load\nNUM_OF_TRAIN_QUESTIONS = 1000\nNUM_OF_VAL_QUESTIONS = 1050\nSAMPLE_RATE = 15\nTRAIN_PATH = '..\/input\/tensorflow2-question-answering\/simplified-nq-train.jsonl'\n\n# TOKENIZATION\nFILTERS = '!\"#$%&()*+,-.\/:;<=>?@[\\\\]^_`{|}~\\t\\n'\nLOWER_CASE = True\nMAX_LEN = 300\n\n# long answer model parameters\nEPOCHS = 40\nBATCH_SIZE = 64\nEMBED_SIZE = 100\nCLASS_WEIGHTS = {0: 0.5, 1: 5.}\n\n# short answer model parameters\nSHORT_EPOCHS = 80\nSHORT_BATCH_SIZE = 32\nSHORT_EMBED_SIZE = 200","ef674e96":"def get_line_of_data(file):\n    line = file.readline()\n    line = json.loads(line)\n    \n    return line\n\n\ndef get_question_and_document(line):\n    question = line['question_text']\n    text = line['document_text'].split(' ')\n    annotations = line['annotations'][0]\n    \n    return question, text, annotations\n                \n                \ndef get_long_candidate(i, annotations, candidate):\n    # check if this candidate is the correct answer\n    if i == annotations['long_answer']['candidate_index']:\n        label = True\n    else:\n        label = False\n\n    # get place where long answer starts and ends in the document text\n    long_start = candidate['start_token']\n    long_end = candidate['end_token']\n    \n    return label, long_start, long_end\n\n\ndef form_data_row(question, label, text, long_start, long_end):\n    row = {\n        'question': question,\n        'long_answer': ' '.join(text[long_start:long_end]),\n        'is_long_answer': label,\n    }\n    \n    return row\n\n\ndef load_data(file_path, questions_start, questions_end):\n    rows = []\n    \n    with open(file_path) as file:\n\n        for i in tqdm(range(questions_start, questions_end)):\n            line = get_line_of_data(file)\n            question, text, annotations = get_question_and_document(line)\n\n            for i, candidate in enumerate(line['long_answer_candidates']):\n                label, long_start, long_end = get_long_candidate(i, annotations, candidate)\n\n                if label == True or (i % SAMPLE_RATE == 0):\n                    rows.append(\n                        form_data_row(question, label, text, long_start, long_end)\n                    )\n        \n    return pd.DataFrame(rows)","9bf9fad0":"train_df = load_data(TRAIN_PATH, 0, NUM_OF_TRAIN_QUESTIONS)\nval_df = load_data(TRAIN_PATH, NUM_OF_TRAIN_QUESTIONS, NUM_OF_VAL_QUESTIONS)","25b575b5":"train_df.head(5)","86c08ab0":"def remove_stopwords(sentence):\n    words = sentence.split()\n    words = [word for word in words if word not in stopwords.words('english')]\n    \n    return ' '.join(words)\n\n\ndef remove_html(sentence):\n    html = re.compile(r'<.*?>')\n    return html.sub(r'', sentence)\n\n\ndef clean_df(df):\n    df['long_answer'] = df['long_answer'].apply(lambda x : remove_stopwords(x))\n    df['long_answer'] = df['long_answer'].apply(lambda x : remove_html(x))\n\n    df['question'] = df['question'].apply(lambda x : remove_stopwords(x))\n    df['question'] = df['question'].apply(lambda x : remove_html(x))\n    \n    return df","03ceb2de":"train_df = clean_df(train_df)\nval_df = clean_df(val_df)","333fd3bb":"train_df.head(5)","32cab4b0":"def define_tokenizer(df_series):\n    sentences = pd.concat(df_series)\n    \n    tokenizer = tf.keras.preprocessing.text.Tokenizer(\n        filters=FILTERS, \n        lower=LOWER_CASE\n    )\n    tokenizer.fit_on_texts(sentences)\n    \n    return tokenizer\n\n    \ndef encode(sentences, tokenizer):\n    encoded_sentences = tokenizer.texts_to_sequences(sentences)\n    \n    encoded_sentences = tf.keras.preprocessing.sequence.pad_sequences(\n        encoded_sentences, \n        padding='post',\n        maxlen=MAX_LEN\n    )\n    \n    return encoded_sentences","f3322e68":"tokenizer = define_tokenizer([\n    train_df.long_answer, \n    train_df.question,\n    val_df.long_answer, \n    val_df.question\n])","b8f838ac":"tokenizer.word_index['tracy']","e7762c94":"train_long_answers = encode(train_df['long_answer'].values, tokenizer)\ntrain_questions = encode(train_df['question'].values, tokenizer)\n\nval_long_answers = encode(val_df['long_answer'].values, tokenizer)\nval_questions = encode(val_df['question'].values, tokenizer)","6f84625c":"train_long_answers[0]","d8d50d41":"train_labels = train_df.is_long_answer.astype(int).values\nval_labels = val_df.is_long_answer.astype(int).values","91056b2d":"train_labels","6f20fa37":"embedding_dict = {}\n\nwith open('..\/input\/glove-global-vectors-for-word-representation\/glove.6B.' + str(EMBED_SIZE) + 'd.txt','r') as f:\n    for line in f:\n        values = line.split()\n        word = values[0]\n        vectors = np.asarray(values[1:],'float32')\n        embedding_dict[word] = vectors\n        \nf.close()","00f292f8":"num_words = len(tokenizer.word_index) + 1\nembedding_matrix = np.zeros((num_words, EMBED_SIZE))\n\nfor word, i in tokenizer.word_index.items():\n    if i > num_words:\n        continue\n    \n    emb_vec = embedding_dict.get(word)\n    \n    if emb_vec is not None:\n        embedding_matrix[i] = emb_vec","7a123626":"embedding = tf.keras.layers.Embedding(\n    len(tokenizer.word_index) + 1,\n    EMBED_SIZE,\n    embeddings_initializer = tf.keras.initializers.Constant(embedding_matrix),\n    trainable = False\n)","114f810a":"# question encoding\nquestion_input = Input(shape=(None,))\nquestion_x = embedding(question_input)\nquestion_x = SpatialDropout1D(0.2)(question_x)\nquestion_x = Bidirectional(LSTM(100, return_sequences=True))(question_x)\nquestion_x = GlobalMaxPooling1D()(question_x)\n\n# answer encoding\nanswer_input = Input(shape=(None,))\nanswer_x = embedding(answer_input)\nanswer_x = SpatialDropout1D(0.2)(answer_x)\nanswer_x = Bidirectional(LSTM(150, return_sequences=True))(answer_x)\nanswer_x = GlobalMaxPooling1D()(answer_x)\n\n# classification\ncombined_x = concatenate([question_x, answer_x])\ncombined_x = Dense(300, activation='relu')(combined_x)\ncombined_x = Dropout(0.5)(combined_x)\ncombined_x = Dense(300, activation='relu')(combined_x)\ncombined_x = Dropout(0.5)(combined_x)\noutput = Dense(1, activation='sigmoid')(combined_x)\n\n# combine model parts into one\nmodel = tf.keras.models.Model(inputs=[answer_input, question_input], outputs=output)","8cea37bb":"model.compile(\n    loss='binary_crossentropy', \n    optimizer='adam',\n    metrics=['BinaryAccuracy', 'Recall', 'Precision']\n)","e640b47d":"callbacks = [\n    tf.keras.callbacks.ReduceLROnPlateau(monitor='loss', patience=2, verbose=1),\n    tf.keras.callbacks.EarlyStopping(monitor='loss', patience=5, verbose=1),\n]","99379555":"history = model.fit(\n    x = [train_long_answers, train_questions], \n    y = train_labels,\n    validation_data = (\n        [val_long_answers, val_questions], \n        val_labels\n    ),\n    epochs = EPOCHS,\n    callbacks = callbacks,\n    class_weight = CLASS_WEIGHTS,\n    batch_size = BATCH_SIZE,\n    shuffle = True\n)","45dc366b":"fig, ax = plt.subplots(1, 2, figsize=(15, 5))\n\nax[0].set_title('Training Loss')\nax[0].plot(history.history['loss'])\n\nax[1].set_title('Validation Loss')\nax[1].plot(history.history['val_loss'])","c845c180":"fig, ax = plt.subplots(3, 2, figsize=(15, 10))\n\nax[0,0].set_title('Training Accuracy')\nax[0,0].plot(history.history['binary_accuracy'])\n\nax[0,1].set_title('Validation Accuracy')\nax[0,1].plot(history.history['val_binary_accuracy'])\n\nax[1,0].set_title('Training Recall')\nax[1,0].plot(history.history['recall'])\n\nax[1,1].set_title('Validation Recall')\nax[1,1].plot(history.history['val_recall'])\n\nax[2,0].set_title('Training Precision')\nax[2,0].plot(history.history['precision'])\n\nax[2,1].set_title('Validation Precision')\nax[2,1].plot(history.history['val_precision'])","b91b8745":"print('Epochs: {0}'.format(\n    len(history.history['loss'])\n))","375f67bd":"recall = history.history['recall'][-1]\nprecision = history.history['precision'][-1]\n\nprint('Train F1 score: {0:.4f}'.format(\n    2 * (precision * recall) \/ (precision + recall)\n))","d64611fa":"recall = history.history['val_recall'][-1]\nprecision = history.history['val_precision'][-1]\n\nprint('Validation F1 score: {0:.4f}'.format(\n    2 * (precision * recall) \/ (precision + recall)\n))","be420e09":"def test_question(question, positive, negative):\n    sentences = [question, positive, negative]\n    \n    for i in range(3):\n        sentences[i] = remove_stopwords(sentences[i])\n        sentences[i] = remove_html(sentences[i])\n    \n    sentences = encode(sentences, tokenizer)\n    \n    predictions = model.predict(\n        [np.expand_dims(sentences[1], axis=0), np.expand_dims(sentences[0], axis=0)]\n    )\n\n    print('Positive: {0:.2f}'.format(predictions[0][0]))\n\n    predictions = model.predict(\n        [np.expand_dims(sentences[2], axis=0), np.expand_dims(sentences[0], axis=0)]\n    )\n\n    print('Negative: {0:.2f}'.format(predictions[0][0]))","1c58541a":"question = 'which is the most common use of opt-in e-mail marketing'\n\npositive = \"<P> A common example of permission marketing is a newsletter sent to an advertising firm 's customers . Such newsletters inform customers of upcoming events or promotions , or new products . In this type of advertising , a company that wants to send a newsletter to their customers may ask them at the point of purchase if they would like to receive the newsletter . <\/P>\"\n\nnegative = '<P> Email marketing has evolved rapidly alongside the technological growth of the 21st century . Prior to this growth , when emails were novelties to the majority of customers , email marketing was not as effective . In 1978 , Gary Thuerk of Digital Equipment Corporation ( DEC ) sent out the first mass email to approximately 400 potential clients via the Advanced Research Projects Agency Network ( ARPANET ) . This email resulted in $13 million worth of sales in DEC products , and highlighted the potential of marketing through mass emails . However , as email marketing developed as an effective means of direct communication , users began blocking out content from emails with filters and blocking programs . In order to effectively communicate a message through email , marketers had to develop a way of pushing content through to the end user , without being cut out by automatic filters and spam removing software . This resulted in the birth of triggered marketing emails , which are sent to specific users based on their tracked online browsing patterns . <\/P>'","434760c8":"test_question(question, positive, negative)","71063a0c":"question = 'how i.met your mother who is the mother'\n\npositive = \"<P> Tracy McConnell , better known as `` The Mother '' , is the title character from the CBS television sitcom How I Met Your Mother . The show , narrated by Future Ted , tells the story of how Ted Mosby met The Mother . Tracy McConnell appears in 8 episodes from `` Lucky Penny '' to `` The Time Travelers '' as an unseen character ; she was first seen fully in `` Something New '' and was promoted to a main character in season 9 . The Mother is played by Cristin Milioti . <\/P>\"\n\nnegative = \"<P> In `` Bass Player Wanted '' , the Mother picks up a hitchhiking Marshall , carrying his son Marvin , on her way to Farhampton Inn . On their way , it is revealed that the Mother is a bass player in the band , that is scheduled to play at the wedding reception . But the band 's leader , Darren , forced her to quit . The Mother ultimately decides to confront Darren and retake the band . She ends up alone at the bar , and while practicing a speech to give Darren , Darren walks up to her furious the groom 's best man punched him for `` no reason . '' Amused by this , the Mother laughs , and Darren quits the band in anger . <\/P>\"","f00b46f3":"test_question(question, positive, negative)","ef65bbd9":"question = 'how i met your mother who is the mother'\ntest_question(question, positive, negative)","cabc1b80":"question = 'who is tracy mcconnell'\ntest_question(question, positive, negative)","3ea596a2":"def get_short_answer(annotations, long_start, long_end):\n    if len(annotations['short_answers']) > 0:\n        short_start = annotations['short_answers'][0]['start_token']\n        short_end = annotations['short_answers'][0]['end_token']\n        \n        short_start = short_start - long_start\n        short_end = short_end - long_start\n        \n        return short_start, short_end\n    else:\n        return 0, 0\n    \n\ndef form_short_data_row(question, text, long_start, long_end, short_start, short_end):\n    long_answer = ' '.join(text[long_start:long_end])\n    short_answer = ' '.join(long_answer.split(' ')[short_start:short_end])\n    \n    row = {\n        'question': question,\n        'long_answer': long_answer,\n        'short_answer': short_answer,\n        'short_start': short_start,\n        'short_end': short_end\n    }\n    \n    return row\n\n\ndef load_short_data(file_path, questions_start, questions_end):\n    rows = []\n    \n    with open(file_path) as file:\n\n        for i in tqdm(range(questions_start, questions_end)):\n            line = get_line_of_data(file)\n            question, text, annotations = get_question_and_document(line)\n\n            for i, candidate in enumerate(line['long_answer_candidates']):\n                label, long_start, long_end = get_long_candidate(i, annotations, candidate)\n\n                if label == True:\n                    short_start, short_end = get_short_answer(annotations, long_start, long_end)\n                    \n                    rows.append(\n                        form_short_data_row(question, text, long_start, long_end, short_start, short_end)\n                    )\n        \n    return pd.DataFrame(rows)","4ff76874":"train_short_df = load_short_data(TRAIN_PATH, 0, NUM_OF_TRAIN_QUESTIONS)\nval_short_df = load_short_data(TRAIN_PATH, NUM_OF_TRAIN_QUESTIONS, NUM_OF_VAL_QUESTIONS)","7bd45e92":"train_short_df.head()","f15f2fb2":"train_long_answers = encode(train_short_df['long_answer'].values, tokenizer)\ntrain_questions = encode(train_short_df['question'].values, tokenizer)\n\nval_long_answers = encode(val_short_df['long_answer'].values, tokenizer)\nval_questions = encode(val_short_df['question'].values, tokenizer)","03921dd8":"def form_short_labels(df, sentence_length):\n    start_labels = np.zeros((len(df), sentence_length))\n    end_labels = np.zeros((len(df), sentence_length))\n\n    for i in range(len(df)):\n        start = df.loc[i].short_start\n        end = df.loc[i].short_end\n\n        if start < 300 and end < 300:\n            start_labels[i, start] = 1\n            end_labels[i, end] = 1\n        else:\n            continue\n    \n    return start_labels, end_labels\n\n\ntrain_start_labels, train_end_labels = form_short_labels(train_short_df, MAX_LEN)\nval_start_labels, val_end_labels = form_short_labels(val_short_df, MAX_LEN)","05850545":"print(train_short_df.loc[0].long_answer)\nprint('Start index: {0}'.format(train_start_labels[0]))\nprint('End index: {0}'.format(train_end_labels[0]))","67f55c92":"# load from file\nembedding_dict = {}\n\nwith open('..\/input\/glove-global-vectors-for-word-representation\/glove.6B.' + str(SHORT_EMBED_SIZE) + 'd.txt','r') as f:\n    for line in f:\n        values = line.split()\n        word = values[0]\n        vectors = np.asarray(values[1:],'float32')\n        embedding_dict[word] = vectors\n        \nf.close()\n\n# write to matrix\nnum_words = len(tokenizer.word_index) + 1\nembedding_matrix = np.zeros((num_words, SHORT_EMBED_SIZE))\n\nfor word, i in tokenizer.word_index.items():\n    if i > num_words:\n        continue\n    \n    emb_vec = embedding_dict.get(word)\n    \n    if emb_vec is not None:\n        embedding_matrix[i] = emb_vec\n        \n# load as tensorflow embedding\nembedding = tf.keras.layers.Embedding(\n    len(tokenizer.word_index) + 1,\n    SHORT_EMBED_SIZE,\n    embeddings_initializer = tf.keras.initializers.Constant(embedding_matrix),\n    trainable = False\n)","23461aa0":"# encode question\nquestion_input = Input(shape=(None,))\nquestion_x = embedding(question_input)\nquestion_x = SpatialDropout1D(0.2)(question_x)\nquestion_x = Bidirectional(LSTM(200, return_sequences=True))(question_x)\nquestion_x = Bidirectional(LSTM(100, return_sequences=True))(question_x)\n\n# encode answer\nanswer_input = Input(shape=(None,))\nanswer_x = embedding(answer_input)\nanswer_x = SpatialDropout1D(0.2)(answer_x)\nanswer_x = Bidirectional(LSTM(250, return_sequences=True))(answer_x)\nanswer_x = Bidirectional(LSTM(150, return_sequences=True))(answer_x)\n\n# merge the encodings\ncombined_x = concatenate([question_x, answer_x])\n\n# predict start index\nstart_x = Dropout(0.1)(combined_x) \nstart_x = Conv1D(1,1)(start_x)\nstart_x = Flatten()(start_x)\nstart_x = Activation('softmax', name='start_token_out')(start_x)\n\n# predict end index\nend_x = Dropout(0.1)(combined_x) \nend_x = Conv1D(1,1)(end_x)\nend_x = Flatten()(end_x)\nend_x = Activation('softmax', name='end_token_out')(end_x)\n\n# merge the parts into one model\nshort_model = tf.keras.models.Model(inputs=[answer_input, question_input], outputs=[start_x, end_x])","87f1aa7f":"short_model.compile(\n    loss='categorical_crossentropy', \n    optimizer='adam',\n    metrics=['categorical_accuracy', 'Recall', 'Precision']\n)","28e08b1e":"callbacks = [\n    tf.keras.callbacks.ReduceLROnPlateau(monitor='loss', patience=3, verbose=1),\n    tf.keras.callbacks.EarlyStopping(monitor='loss', patience=5, verbose=1),\n]","5228eb19":"history = short_model.fit(\n    x = [train_long_answers, train_questions], \n    y = [train_start_labels, train_end_labels],\n    validation_data = (\n        [val_long_answers, val_questions], \n        [val_start_labels, val_end_labels]\n    ),\n    epochs = SHORT_EPOCHS,\n    callbacks = callbacks,\n    batch_size = SHORT_BATCH_SIZE,\n    shuffle = True\n)","90d92bdf":"print('Epoch: {0}'.format(len(history.history['loss'])))\nprint('Loss: {0}'.format(history.history['loss'][-1]))","64776b51":"fig, ax = plt.subplots(1, 2, figsize=(15, 5))\n\nax[0].set_title('Training Loss')\nax[0].plot(history.history['loss'])\n\nax[1].set_title('Validation Loss')\nax[1].plot(history.history['val_loss'])","a626ef2b":"fig, ax = plt.subplots(3, 2, figsize=(15, 10))\n\nfig.suptitle('Start Token')\n\nax[0,0].set_title('Training Accuracy')\nax[0,0].plot(history.history['start_token_out_categorical_accuracy'])\n\nax[0,1].set_title('Validation Accuracy')\nax[0,1].plot(history.history['val_start_token_out_categorical_accuracy'])\n\nax[1,0].set_title('Training Recall')\nax[1,0].plot(history.history['start_token_out_recall'])\n\nax[1,1].set_title('Validation Recall')\nax[1,1].plot(history.history['val_start_token_out_recall'])\n\nax[2,0].set_title('Training Precision')\nax[2,0].plot(history.history['start_token_out_precision'])\n\nax[2,1].set_title('Validation Precision')\nax[2,1].plot(history.history['val_start_token_out_precision'])","0b77e32d":"fig, ax = plt.subplots(3, 2, figsize=(15, 10))\n\nfig.suptitle('End Token')\n\nax[0,0].set_title('Training Accuracy')\nax[0,0].plot(history.history['end_token_out_categorical_accuracy'])\n\nax[0,1].set_title('Validation Accuracy')\nax[0,1].plot(history.history['val_end_token_out_categorical_accuracy'])\n\nax[1,0].set_title('Training Recall')\nax[1,0].plot(history.history['end_token_out_recall_1'])\n\nax[1,1].set_title('Validation Recall')\nax[1,1].plot(history.history['val_end_token_out_recall_1'])\n\nax[2,0].set_title('Training Precision')\nax[2,0].plot(history.history['end_token_out_precision_1'])\n\nax[2,1].set_title('Validation Precision')\nax[2,1].plot(history.history['val_end_token_out_precision_1'])","8a281557":"accuracy = history.history['start_token_out_categorical_accuracy'][-1]\nrecall = history.history['start_token_out_recall'][-1]\nprecision = history.history['start_token_out_precision'][-1]\n\n\nprint('Training')\nprint('Start token accuracy: {0}'.format(accuracy))\nprint('Start token recall: {0}'.format(recall))\nprint('Start token precision: {0}'.format(precision))\nprint('Start token F1 score: {0:.4f}'.format(\n    2 * (precision * recall) \/ (precision + recall)\n))\n\naccuracy = history.history['end_token_out_categorical_accuracy'][-1]\nrecall = history.history['end_token_out_recall_1'][-1]\nprecision = history.history['end_token_out_precision_1'][-1]\n\nprint('End token accuracy: {0}'.format(accuracy))\nprint('End token recall: {0}'.format(recall))\nprint('End token precision: {0}'.format(precision))\nprint('End token F1 score: {0:.4f}'.format(\n    2 * (precision * recall) \/ (precision + recall)\n))","fea178f5":"accuracy = history.history['val_start_token_out_categorical_accuracy'][-1]\nrecall = history.history['val_start_token_out_recall'][-1]\nprecision = history.history['val_start_token_out_precision'][-1]\n\n\nprint('Validation')\nprint('Start token accuracy: {0}'.format(accuracy))\nprint('Start token recall: {0}'.format(recall))\nprint('Start token precision: {0}'.format(precision))\nprint('Start token F1 score: {0:.4f}'.format(\n    2 * (precision * recall) \/ (precision + recall)\n))\n\naccuracy = history.history['val_end_token_out_categorical_accuracy'][-1]\nrecall = history.history['val_end_token_out_recall_1'][-1]\nprecision = history.history['val_end_token_out_precision_1'][-1]\n\nprint('End token accuracy: {0}'.format(accuracy))\nprint('End token recall: {0}'.format(recall))\nprint('End token precision: {0}'.format(precision))\nprint('End token F1 score: {0:.4f}'.format(\n    2 * (precision * recall) \/ (precision + recall)\n))","2ef26b93":"def test_short_answer(question, long_answer):\n    sentences = [long_answer, question]\n    \n    sentences = encode(sentences, tokenizer)\n    \n    predictions = short_model.predict(\n        [np.expand_dims(sentences[0], axis=0), np.expand_dims(sentences[1], axis=0)]\n    )\n    \n    predictions = np.array(predictions)\n    \n    pred_start = np.argmax(predictions[0,0])\n    pred_end = np.argmax(predictions[1,0])\n    pred_string = ' '.join(long_answer.split(' ')[pred_start:pred_end])\n\n    return pred_start, pred_end, pred_string","fd8b7663":"question = 'which is the most common use of opt-in e-mail marketing'\nlong_answer = \"<P> A common example of permission marketing is a newsletter sent to an advertising firm 's customers . Such newsletters inform customers of upcoming events or promotions , or new products . In this type of advertising , a company that wants to send a newsletter to their customers may ask them at the point of purchase if they would like to receive the newsletter . <\/P>\"","3bb06cde":"start, end, short_answer = test_short_answer(question, long_answer)\n\nprint('Start token: ' + str(start))\nprint('End token: ' + str(end))\nprint('Answer: ' + short_answer)","3f25ab6a":"question = 'how i.met your mother who is the mother'\nlong_answer = \"<P> Tracy McConnell , better known as `` The Mother '' , is the title character from the CBS television sitcom How I Met Your Mother . The show , narrated by Future Ted , tells the story of how Ted Mosby met The Mother . Tracy McConnell appears in 8 episodes from `` Lucky Penny '' to `` The Time Travelers '' as an unseen character ; she was first seen fully in `` Something New '' and was promoted to a main character in season 9 . The Mother is played by Cristin Milioti . <\/P>\"","d35086de":"start, end, short_answer = test_short_answer(question, long_answer)\n\nprint('Start token: ' + str(start))\nprint('End token: ' + str(end))\nprint('Answer: ' + short_answer)","e9ab1ca1":"### Evaluate\n\nAs this model has two outputs there are more metrics to display this time. The only one that is the same is the loss for the model.","a1317324":"Not bad. Even when the question is phrased in a very different way i.e. ask who Tracy is rather than who the mother is, the model still knows I am asking about the mother in \"How I met your mother\" and returns the right answer.","3186198e":"The model itself looks similar to last time. There are three parts where the first two encode the question and the answer. They feed into the classification part of the model that then predicts the start and the end indexes of the short answer. The two encoding parts are much the same as the encoding parts in the last model. Only the classification part has changed which uses convolutional layers instead of dense ones and outputs two arrays instead of one.\n\n![Screenshot%202020-07-15%20at%2009.42.19.png](attachment:Screenshot%202020-07-15%20at%2009.42.19.png)","92df26ed":"### Define model\n\nFor this challenge I'll define one of the simpler models that can be used. For bleeding edge results BERT is probably the best model to use. However this works for now and is something that helped me understand the fundamentals of how these types of models work.\n\nThe model works by inserting one question and one potential answer as a pair. The model then outputs a probability telling us the what extent the potential answer is in fact, the correct answer for this question. We could run this model against all potential answers for a question and use the highest probability  to determine which answer is correct.\n\nWith this in mind, the model I'll use has three parts to it:\n\n- **Question encoding:** This part of the model encodes the question. That is it uses the embeddings discussed earlier and a bidirectional LSTM layer to determine the meaning of the sentence. This outputs an encoded array representing the sentence in a way that the model can understand its meaning.\n- **Answer encoding:** This is the same the question encoding part except it does this for a potential answer.\n- **Classification:** This takes the sentence encodings from the two encoding parts of the model, compares them using dense layers and outputs a probability regarding how close the potential answer is to the true answer to this question.\n\n![Screenshot%202020-07-15%20at%2009.41.21.png](attachment:Screenshot%202020-07-15%20at%2009.41.21.png)","02376bc3":"### Import GloVe embedding\n\nWith the data loaded and pre-processed I can now define the model. Natural language models rely on an embedding layer to provide meaning to the words in a sentence. I won't go into all the detail here but I like to think about embeddings is as a grid of the words the model understands alongside different meanings:\n\n|             | tracy | excited | mother |\n|-------------|-------|---------|--------|\n| male\/female | 0.8   | 0.5     | 0.95   |\n| happy\/sad   | 0.5   | 0.9     | 0.6    |","ef136912":"The final piece of pre-processing to be done is to create an array of binary labels for the model. The labels were boolean values in the model so they have simply been converted to 0-1 integers as the model prefers them that way.","fee8874b":"I've always been impressed when I ask Google the same question but slightly differently and it still understands it as the same question. Let's see if this model can do this.","475b2250":"I initially found natural language models tricky to get started with especially once I moved on from starter tasks like sentiment analysis. I wanted to share this notebook to show how some common and slightly more complex language tasks could be approached. In this notebook I have used Googles question answering dataset to show how to match questions to answers and how to extract shorter answers from longer ones.","229fdf6d":"So this gives me a dataset with two features (question and long_answer) and a binary label (is_long_answer) to predict. This simplifies the challenge into a binary classification task where the model is given a question and a possible answer and returns a probability telling us whether this is the right answer or not.","8afd0558":"I've increased the patience of the reduce learning plateau call","91153e5e":"And here are the metrics of the end token output.","304407de":"Now that the data is clean it can be converted to tokens. This assigns a numeric index to each unique word in the dataset so that an array of integers (representing a sentence of words) can be fed into the model instead of a string. I've included an example below of how a sentence may look like after tokenisation.\n\n> ['how', 'i', 'met', 'your', 'mother', 'who', 'is', 'the', 'mother']\n\n> [2018, 44, 1275, 932, 312, 3293, 1164, 1, 312]\n\nIt's worth noting that I also pad sentences so that they are all the same length (which I set to an arbitary 300 words). If a sentence is less than this maximum length they get padded with 0 tokens to get them to 300 words. The model needs the sentences to be the same length but is also taught to ignore the 0 tokens. This is actually a good hyper-parameter to tweak when trying to improve the model.\n\nI'll use the words in loaded questions and answers then to define a tensorflow keras tokenizer. ","aa60d16f":"Let's see an example. Here the correct short answer is \"a newsletter sent to an advertising firm's customers\". If you are counting the words in the sentence to see if they match the label, be sure to include the html tag in the count.","6805f604":"Usually I would clean the text here but it can get a little confusing when trying to extract text if sentences have parts removed from them. As such they have been maintained. However I have kept the tokeniser used in the first challenge of this notebook which I use here to encode the dataset.","7ff86e41":"I'll also print out the number of epochs the model trained for to see if early stopping took place.","a71188e2":"Finally the model can begin training. One extra hyper-parameter to mention here is the class weights. This dataset contains far more wrong answers (label 0) than correct answers (label 1). While I want the model to know when it sees a wrong answer, I'd like it to focus more on the correct answers that it sees. As such I have weighted the correct answers far more heavily than the wrong answers.\n\nOn GPU 40 epochs should take around 8 minutes. On CPU it will take about an hour and a half.","62f8368c":"Finally here's a list of all the final metrics for this model.","7933f255":"## Evaluate\n\nTo evaluate the model there are a lot of the usual quantitative methods. A good start is the loss curve across both the training and validation datasets. If they are both trending down the model is learning something.","4f7be17a":"In the above example you can see some sample words in the header with some meanings in the index column. The numbers represent how far that word is one meaning or another. So the word 'mother' for example is a higher number in the first meaning category telling the model that it is more female than masculine while its neutral number in the second category tells the model that it doesn't hold much in the way of happy or sad sentiment. Of course those meaning categories are black box categories in the real embeddings and the human eye would never be able to understand what they really mean. However it is these numbers that help the model understand words as we do.\n\nIt usually takes a very long time to train a good embedding. As the meaning of words rarely change from one document to another (assuming they are in the same language) it is a good idea to load a pre-trained embedding. I'll use the popular GloVe embeddings for this challenge. GloVe can be loaded with either 50, 100 or 200 dimensions (what I referred to as menaing categories just above). More dimensions tend to mean better accuracy but also longer training times. I have opted for 100 dimensions here.","6174319c":"And once again, set off the training.","619c3ac8":"# Final thoughts\n\nIt took me a few days to work out how these models work but it turns out that it's not difficult to get language models like these working to some extent. If these models were to be made competition ready though I would endeavour to use the BERT model (or an equivalent such as roberta) and apply far more pre-processing to the text before it goes into the model. I would also increase how much data the model is trained on as it is likely to be overfit with so few examples fed into it. In the meantime I hope this helps anyone looking to get started with natural language tasks such as these.","724ab942":"Let's take a look at what a sentence that is ready to be inputted into the model looks like.","3630df99":"### Clean and tokenize the text\n\nWith the raw data loaded I'll clean up the data to make it easier for the model to train on. There are so many things that can be done here to improve a model. To keep this notebook short I have only removed html tags and stopwords (e.g. \"a\", \"the\" and \"are\"). There are however many more that I could have used to improve this model which can be found in notebooks like this [one](https:\/\/www.kaggle.com\/raenish\/cheatsheet-text-helper-functions).","76c74844":"### Load data\n\nThe data for this task comes in a rather complex form. The raw data is in a json file with each line of json containing the following:\n\n- **question_text:** A string representing a question asked on Google.\n- **document_text:** A string containing the html of the wikipedia page relevant to the question.\n- **annotations\/long_answer:** A json object containing the start and end index of the correct answer substring (substring of the document_text) \n- **annotations\/short_answers:** An array of the possible short answers. Each short answer is represented as the start and end index of the substring that is within document_text).\n- **long_answer_candidates:** An array of the possible long answers to the questions. These are basically the paragraphs within the wikipedia page.\n\nI've got a cell of code hidden below that reads a number of rows from the file (specified in the hyper-parameters at the top of this notebook) and puts them into a pandas dataframe. The code is a little long so I have hidden the cell so you can just see the dataframe that I will be working with for the rest of the task.","3b67acce":"# Matching questions to long answers\n\nThe first task I'll approach in this notebook is how to find answers that are relevant to the question asked. So if someone typed the following question into Google search:\n\n> \"how i.met your mother who is the mother\"\n\nHow would we find the paragraph in wikipedia that contains the answer? For example, if we had the two following potential answers from a wikipedia page how would a machine recognise that the first paragraph is more likely to contain the answer than the second one:\n\n> \"Tracy McConnell, better known as 'The Mother', is the title character from the CBS television sitcom How I Met Your Mother. The show, narrated by Future Ted, tells the story of how Ted Mosby met The Mother. Tracy McConnell appears in 8 episodes from 'Lucky Penny' to 'The Time Travelers' as an unseen character; she was first seen fully in 'Something New' and was promoted to a main character in season 9. The Mother is played by Cristin Milioti.\"\n\n> \"In 'Bass Player Wanted', the Mother picks up a hitchhiking Marshall, carrying his son Marvin, on her way to Farhampton Inn. On their way, it is revealed that the Mother is a bass player in the band, that is scheduled to play at the wedding reception. But the band's leader, Darren, forced her to quit. The Mother ultimately decides to confront Darren and retake the band. She ends up alone at the bar, and while practicing a speech to give Darren, Darren walks up to her furious the groom's best man punched him for no reason. Amused by this, the Mother laughs, and Darren quits the band in anger.\"\n\nBefore I get into how to model this I will do the usual pre-processing to get the data ready.","0b3e8768":"The final metric I'll use is F1 score. This is a combination of precision and recall and the metric the competition (now closed) used to measure the quality of models. It's thus a good single metric that encompasses the quality of the model.","eaa2c3bc":"# Text extraction\n\nWith the correct long answer matched to a question, the next challenge is to extract the short answer from a long answer. So if the correct answer is a paragraph about who the mother in \"How I met your mother\" is:\n\n> \"Tracy McConnell, better known as 'The Mother', is the title character from the CBS television sitcom How I Met Your Mother. The show, narrated by Future Ted, tells the story of how Ted Mosby met The Mother. Tracy McConnell appears in 8 episodes from 'Lucky Penny' to 'The Time Travelers' as an unseen character; she was first seen fully in 'Something New' and was promoted to a main character in season 9. The Mother is played by Cristin Milioti.\"\n\nThe correct short answer would be the mothers name:\n\n> \"Tracy McConnell\"\n\nThe approach to this challenge isn't too different from the question, answer matching model above. The pre-processing is much the same. So first I'll load the dataset filtering out the wrong long answers and extracting the start and end index of the short answer within the long answer. The start and end index are the indexes of the words in the long answer that make up the short answer. So if we use the same example answer above, the start and end index would 0 and 1 as the short answer begins on the first word of the long answer and ends of the second word.\n\nI've hidden the code for transforming the json file into a pandas dataset again.","afce55a7":"To make it easier to experiement with hyper-parameter tweaking experiments I have added them to this notebook as globals here.","295677e4":"Here's a sample question with two answers from the training dataset. Let's see how the model does with them.","43e9b9c1":"With the tokenizer defined I can encode the loaded questions and answers. It is easier to input the sentences into the model as numpy arrays so I have outputted the encoded sentences as numpy arrays of shape (m, 300) where m is the number of sentences in the array and 300 represents the maximum length of all the sentences.","3f84a3f8":"Quantitative methods are a great way to see the whole models quality in one number. However I always find it invaluable to look at some of the results the model produces. This helps me validate the metrics and see exactly what users would see if this model were to be deployed to an application. To this end I have defined a function that takes a question, the right answer (positive) and a wrong answer (negative) and returns the probability for each of the two answers. I'm expecting to see a high probability for the right answer and low probability for the wrong answer. The greater the difference between the numbers, the better.","1059384e":"Like last time, let's finsih off with a look at how the model performs on some examples.","9bd92f2c":"The same can be done with the other metrics, accuracy, recall and precision. This time I'm hoping the curves trend upwards.","fbfb63ea":"I'll also define a few callbacks. The first helps the model to avoid plateauing as it approaches zero loss. I've also added early stopping in case the model reaches the optimum before it has finished its final epoch.","dc14ecc1":"The labels look a little different this time. The model will be outputting two arrays (for for the start index and another for the end index) 300 elements in length (to represent the 300 words in a padded sentence). All but one of the elements in the arrays will be a zero value. The one value of the arrays that is a value of one represents the  models predicted start or end of a short answer.","860714e5":"With the data ready to go I'll begin defining the model. The below code imports the glove embeddings again and the code is much the same as last time I used it in this notebook. However this model requires more dimensions (200) to be effective meaning I couldn't just re-use the embeddings from the question, answer model.","bb55f273":"The model will be optimised with adam and use categorical cross entropy as a loss function. Like last time, accuracy, recall and precision will be used to measure the models quality.","fa5852de":"While this model has quite a few moing parts, at its heart it is just a binary classification model. This means I can make use of the easy and well tested loss function that is binary cross-entropy. I'll also use adam to optimise the model.","ee99296a":"This model has accuracy, recall and precision metrics for both the start and end index outputs. This is further split by training and validation. Here are the metrics for the start token output of the model."}}