{"cell_type":{"927bfc7e":"code","06bb1ae0":"code","91d0c782":"code","c932fec8":"code","53a8eacf":"code","8288b7a7":"code","5b0ebfbb":"code","8e72b769":"code","dfbdbc31":"code","a729e38c":"code","6e799056":"code","75f44efc":"code","131fcd39":"code","04994b30":"code","d6396cdc":"code","223eb1c0":"code","0add2941":"code","6345565b":"code","5e33a54e":"code","05709740":"code","3f6887c2":"code","e5d33d04":"code","2afc18fc":"code","1475f2b7":"code","0ccbbe6c":"code","e052b886":"code","c7ced29e":"code","da8a12ce":"code","0f825567":"code","200409d7":"code","b9f51643":"code","9f3f0d13":"code","e499573d":"code","52822e5e":"code","5d60b493":"code","77d0a9be":"code","c7bbfff7":"code","92759c50":"code","195d42e5":"code","67e8cbf5":"code","724068f6":"code","8041532c":"code","3239222b":"code","e0d929cc":"code","6e898530":"code","600ad2b8":"code","fc4ca8e1":"code","9c9337ed":"code","b7647ad3":"code","9f665187":"code","b3887958":"code","a4721e4f":"markdown"},"source":{"927bfc7e":"import nltk\nnltk.download(\"stopwords\")","06bb1ae0":"!pip install delayed","91d0c782":"!pip install scikit-multilearn","c932fec8":"!pip install scikit-learn==0.24.2","53a8eacf":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom matplotlib import rcParams\nimport seaborn as sns\nimport warnings\nwarnings.simplefilter(\"ignore\")\nfrom sklearn.model_selection import train_test_split\nfrom sklearn import metrics\nimport csv\nimport plotly.graph_objs as go\nfrom plotly.offline import iplot, init_notebook_mode\nimport re\ninit_notebook_mode(connected=True)\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.metrics import accuracy_score\nfrom nltk.corpus import stopwords\nstop_words = set(stopwords.words('english'))\nfrom sklearn.svm import LinearSVC\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.preprocessing import LabelEncoder\nrcParams['figure.figsize'] = 12,8","8288b7a7":"!wget https:\/\/s3-ap-southeast-1.amazonaws.com\/he-public-data\/dataset52a7b21.zip","5b0ebfbb":"!unzip dataset52a7b21.zip","8e72b769":"train_df=pd.read_csv('dataset\/train.csv',escapechar=\"\\\\\",quoting=csv.QUOTE_NONE)\ntest_df=pd.read_csv('dataset\/test.csv',escapechar=\"\\\\\",quoting=csv.QUOTE_NONE)","dfbdbc31":"train_df.head()","a729e38c":"train_df.isna().sum()","6e799056":"train_df['TITLE']=train_df['TITLE'].fillna(\"\")\ntrain_df['DESCRIPTION']=train_df['DESCRIPTION'].fillna(\"\")\ntrain_df['BULLET_POINTS']=train_df['BULLET_POINTS'].fillna(\"\")\ntest_df['TITLE']=test_df['TITLE'].fillna(\"\")\ntest_df['DESCRIPTION']=test_df['DESCRIPTION'].fillna(\"\")\ntest_df['BULLET_POINTS']=test_df['BULLET_POINTS'].fillna(\"\")","75f44efc":"train_df[\"BRAND\"].value_counts()","131fcd39":"train_df['BRAND']=train_df['BRAND'].fillna('Generic')\ntest_df['BRAND']=test_df['BRAND'].fillna('Generic')","04994b30":"train_df.head()","d6396cdc":"train_df.tail()","223eb1c0":"train_df.isna().sum()","0add2941":"test_df.isna().sum()","6345565b":"train_df.describe()","5e33a54e":"g = sns.countplot('BROWSE_NODE_ID', data=train_df, palette=\"Set1\",order = train_df['BROWSE_NODE_ID'].value_counts().head(10).index)\ng.set_xticklabels(g.get_xticklabels(),rotation=45)\ng.set_title(\"Counting the browse node wise data\", fontsize=15)\ng.set_xlabel(\"\", fontsize=12)\ng.set_ylabel(\"Count\", fontsize=12)\n\nplt.show()","05709740":"from wordcloud import WordCloud,STOPWORDS\nplt.figure(figsize=(20,10))\n# clean\ntext = train_df.TITLE.values\ncloud_brand = WordCloud(\n                          stopwords=STOPWORDS,\n                          background_color='black',\n                          collocations=False,\n                          width=2500,\n                          height=1800\n                         ).generate(\" \".join(text))\nplt.axis('off')\nplt.title(\"Title\",fontsize=40)\nplt.imshow(cloud_brand)","3f6887c2":"from wordcloud import WordCloud,STOPWORDS\nplt.figure(figsize=(20,10))\n# clean\ntext = train_df.DESCRIPTION.values\ncloud_brand = WordCloud(\n                          stopwords=STOPWORDS,\n                          background_color='black',\n                          collocations=False,\n                          width=2500,\n                          height=1800\n                         ).generate(\" \".join(text))\nplt.axis('off')\nplt.title(\"Description\",fontsize=40)\nplt.imshow(cloud_brand)","e5d33d04":"train_df=train_df[:10000]","2afc18fc":"train_df['INFO'] = train_df['DESCRIPTION'] + ' ' + train_df['TITLE']\ntest_df['INFO'] = test_df['DESCRIPTION'] + ' ' + test_df['TITLE']","1475f2b7":"train_df.head()","0ccbbe6c":"\"\"\"labelencoder = LabelEncoder()\ntrain_df['BRAND'] = labelencoder.fit_transform(train_df['BRAND'].astype(str))\ntrain_df\"\"\"","e052b886":"# function to clean data\nimport string\nimport itertools \nimport re\nfrom nltk.stem import WordNetLemmatizer\nfrom nltk.corpus import stopwords\nfrom string import punctuation\n\nstop_words = set(stopwords.words('english'))\n\ndef cleanData(text, lowercase = False, remove_stops = False, stemming = False, lemmatization = False):\n    txt = str(text)\n    \n    # Replace apostrophes with standard lexicons\n    txt = txt.replace(\"isn't\", \"is not\")\n    txt = txt.replace(\"aren't\", \"are not\")\n    txt = txt.replace(\"ain't\", \"am not\")\n    txt = txt.replace(\"won't\", \"will not\")\n    txt = txt.replace(\"didn't\", \"did not\")\n    txt = txt.replace(\"shan't\", \"shall not\")\n    txt = txt.replace(\"haven't\", \"have not\")\n    txt = txt.replace(\"hadn't\", \"had not\")\n    txt = txt.replace(\"hasn't\", \"has not\")\n    txt = txt.replace(\"don't\", \"do not\")\n    txt = txt.replace(\"wasn't\", \"was not\")\n    txt = txt.replace(\"weren't\", \"were not\")\n    txt = txt.replace(\"doesn't\", \"does not\")\n    txt = txt.replace(\"'s\", \" is\")\n    txt = txt.replace(\"'re\", \" are\")\n    txt = txt.replace(\"'m\", \" am\")\n    txt = txt.replace(\"'d\", \" would\")\n    txt = txt.replace(\"'ll\", \" will\")\n    txt = txt.replace(\"--th\", \" \")\n    \n    # More cleaning\n    txt = re.sub(r\"alot\", \"a lot\", txt)\n    txt = re.sub(r\"what's\", \"\", txt)\n    txt = re.sub(r\"What's\", \"\", txt)\n    \n    \n    # Remove urls and emails\n    txt = re.sub(r'^https?:\\\/\\\/.*[\\r\\n]*', ' ', txt, flags=re.MULTILINE)\n    txt = re.sub(r'[\\w\\.-]+@[\\w\\.-]+', ' ', txt, flags=re.MULTILINE)\n    \n    # Replace words like sooooooo with so\n    txt = ''.join(''.join(s)[:2] for _, s in itertools.groupby(txt))\n    \n    # Remove punctuation from text\n    txt = ''.join([c for c in text if c not in punctuation])\n    \n    # Remove all symbols\n    txt = re.sub(r'[^A-Za-z\\s]',r' ',txt)\n    txt = re.sub(r'\\n',r' ',txt)\n    \n    if lowercase:\n        txt = \" \".join([w.lower() for w in txt.split()])\n        \n    if remove_stops:\n        txt = \" \".join([w for w in txt.split() if w not in stop_words])\n        \n    if stemming:\n        st = PorterStemmer()\n        txt = \" \".join([st.stem(w) for w in txt.split()])\n    \n    if lemmatization:\n        wordnet_lemmatizer = WordNetLemmatizer()\n        txt = \" \".join([wordnet_lemmatizer.lemmatize(w, pos='v') for w in txt.split()])\n\n    return txt","c7ced29e":"# clean info\ntrain_df['INFO'] = train_df['INFO'].map(lambda x: cleanData(x, lowercase=True, remove_stops=True, stemming=False, lemmatization = False))\ntest_df['INFO'] = test_df['INFO'].map(lambda x: cleanData(x, lowercase=True, remove_stops=True, stemming=False, lemmatization = False))","da8a12ce":"import pickle\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.models import Sequential, Model\nfrom keras.layers import Activation, Dense, Dropout\nfrom sklearn.preprocessing import LabelBinarizer\nimport sklearn.datasets as skds\nimport itertools\nfrom sklearn.metrics import confusion_matrix\n# For reproducibility\nnp.random.seed(1237)","0f825567":"#x_train, x_test, y_train, y_test = train_test_split(train_df.drop(['BROWSE_NODE_ID','TITLE','DESCRIPTION','BULLET_POINTS'], axis=1), train_df['BROWSE_NODE_ID'], random_state=42,test_size=0.2)","200409d7":"#len(x_train),len(x_test)","b9f51643":"# 20 news groups\nnum_labels = 2118\nvocab_size = 15000\nbatch_size = 100\nnum_epochs = 30\n\n# lets take 80% data as training and remaining 20% for test.\ntrain_size = int(len(train_df) * .8)\n\ntrain_brand = train_df['INFO'][:train_size]\ntrain_node = train_df['BROWSE_NODE_ID'][:train_size]\n\ntest_brand = train_df['INFO'][train_size:]\ntest_node = train_df['BROWSE_NODE_ID'][train_size:]\n\n# define Tokenizer with Vocab Size\ntokenizer = Tokenizer(num_words=vocab_size)\ntokenizer.fit_on_texts(train_brand)\n\nx_train = tokenizer.texts_to_matrix(train_brand, mode='tfidf')\nx_test = tokenizer.texts_to_matrix(test_brand, mode='tfidf')\n\nencoder = LabelBinarizer()\nencoder.fit(train_node)\ny_train = encoder.transform(train_node)\ny_test = encoder.transform(test_node)","9f3f0d13":"len(x_train),len(x_test)","e499573d":"x_train.shape,y_train.shape","52822e5e":"#let us build a basic model\nmodel = Sequential()\nmodel.add(Dense(3600, input_shape=(vocab_size,)))\nmodel.add(Activation('relu'))\nmodel.add(Dropout(0.2))\nmodel.add(Dense(3600))\nmodel.add(Activation('relu'))\nmodel.add(Dropout(0.2))\nmodel.add(Dense(num_labels))\nmodel.add(Activation('softmax'))\nmodel.summary()\n\nmodel.compile(loss='categorical_crossentropy',\n              optimizer='adam',\n              metrics=['accuracy'])","5d60b493":"num_epochs =10\nbatch_size = 128\nhistory = model.fit(x_train, y_train,\n                    batch_size=batch_size,\n                    epochs=num_epochs,\n                    verbose=2,\n                    validation_split=0.2)","77d0a9be":"score, acc = model.evaluate(x_test, y_test,\n                       batch_size=batch_size, verbose=2)\n\nprint('Test accuracy:', acc)","c7bbfff7":"from imblearn.over_sampling import RandomOverSampler \nros = RandomOverSampler(sampling_strategy='minority')\n\nX_sm, y_sm = ros.fit_resample(x_train, y_train)\nprint(X_sm.shape, y_sm.shape)","92759c50":"y_train_labels = np.argmax(y_train, axis =1)\ny_train_labels","195d42e5":"y_test_labels = np.argmax(y_test, axis =1)\ny_test_labels","67e8cbf5":"from sklearn.utils import class_weight\nclass_weight = class_weight.compute_class_weight('balanced' ,np.unique(y_train_labels) ,y_train_labels)\nclass_weight = {l:c for l,c in zip(np.unique(y_train_labels), class_weight)}\nnum_epochs =10\nbatch_size = 128\nhistory = model.fit(X_sm, y_sm,\n                    batch_size=batch_size,\n                    epochs=num_epochs,\n                    verbose=2,\n                    class_weight=class_weight,\n                    validation_split=0.2)","724068f6":"from imblearn.under_sampling import RandomUnderSampler \nros = RandomUnderSampler(sampling_strategy='majority')\n\nX_sm, y_sm = ros.fit_resample(x_train, y_train)\nprint(X_sm.shape, y_sm.shape)","8041532c":"from sklearn.utils import class_weight\nclass_weight = class_weight.compute_class_weight('balanced' ,np.unique(y_train_labels) ,y_train_labels)\nclass_weight = {l:c for l,c in zip(np.unique(y_train_labels), class_weight)}\nnum_epochs =10\nbatch_size = 128\nhistory = model.fit(X_sm, y_sm,\n                    batch_size=batch_size,\n                    epochs=num_epochs,\n                    verbose=2,\n                    class_weight=class_weight,\n                    validation_split=0.2)","3239222b":"model.save(\"mlModel.h5\")","e0d929cc":"import tensorflow as tf","6e898530":"model = tf.keras.models.load_model(\".\/mlModel.h5\")","600ad2b8":"test_df.head()","fc4ca8e1":"tokenizer = Tokenizer(num_words=15000)\ntokenizer.fit_on_texts(test_df['INFO'])","9c9337ed":"test_x = tokenizer.texts_to_matrix(test_df['INFO'], mode='tfidf')","b7647ad3":"from tensorflow.compat.v1.keras.backend import set_session\nconfig = tf.compat.v1.ConfigProto()\nconfig.gpu_options.allow_growth = True  # dynamically grow the memory used on the GPU\nconfig.log_device_placement = True  # to log device placement (on which device the operation ran)\nsess = tf.compat.v1.Session(config=config)\nset_session(sess)","9f665187":"result = pd.DataFrame()\nresult['PRODUCT_ID'] = test_df.index\nresult['BROWSE_NODE_ID'] = model.predict_classes(test_x,batch_size=10,verbose=2)\nresult.to_csv(\"submission.csv\",index=False)","b3887958":"\"\"\"from google.colab import files\nfiles.download('submission.csv') \"\"\"","a4721e4f":"# Predicting On Test Data"}}