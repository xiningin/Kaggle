{"cell_type":{"80711edc":"code","88a2c0f7":"code","5edd795d":"code","262caba7":"code","8560dd4e":"code","658c00ee":"code","acd5fdd4":"code","68d3b272":"code","0f907f09":"code","1b62dbf1":"code","43172edd":"code","eb51b7ca":"code","8d6f51ee":"code","8bff3bdd":"code","62114cc3":"code","50818e81":"code","f72c207f":"code","38b3a9b8":"code","efc7131a":"code","342b7593":"code","ac0cb54e":"code","094c9501":"code","c495fbc0":"code","922884d5":"code","9e5d3e2e":"code","1bf7fffb":"code","b681d2cc":"code","387be10e":"code","20379b96":"code","5ee2150f":"code","a1022335":"code","169b6f01":"code","b1c6bbee":"code","ea454e2e":"code","dfb2cf83":"code","c0b075fa":"code","adc1fcdb":"code","0fe800d2":"code","bde32ae9":"code","4c315aad":"code","e2aa64ab":"code","13092155":"markdown","72deda71":"markdown","ec26ba9d":"markdown","b8d03e13":"markdown","9c42d1df":"markdown","90e3930d":"markdown","b1d4f2c4":"markdown","c347ede5":"markdown","e2090ff1":"markdown","f6f22ab9":"markdown","18bee487":"markdown","8fd8330c":"markdown","1396f2dc":"markdown","0d4f873b":"markdown","5a7c3d11":"markdown","c0e196e7":"markdown","76d4e64f":"markdown","68edaf2f":"markdown"},"source":{"80711edc":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","88a2c0f7":"#reading my training data \ntraining_data_path=\"..\/input\/titanic\/train.csv\"\n#reading my data from the given path and storing it in the dataframe called \"titanic_traindata\"\ntitanic_train=pd.read_csv(training_data_path)\n\n#printing the data frame,just some values\ntitanic_train.head()","5edd795d":"# Remove rows with missing target, separate target from predictors\n#titanic_train.dropna(axis=0, subset=['Survived'], inplace=True)\n#y = titanic_train.Survived\n#titanic_train.drop(['Survived'], axis=1, inplace=True)","262caba7":"#seeing the distributions\n%matplotlib inline \nimport matplotlib.pyplot as plt\ntitanic_train.hist(bins=50, figsize=(20,15))\nplt.show()","8560dd4e":"#taking a look at Pearson Correlation for any sign of multicollinearity\ncorr_matrix = titanic_train.corr()\ncorr_matrix[\"Survived\"].sort_values(ascending=False)","658c00ee":"#for checking correlations\nfrom pandas.plotting import scatter_matrix\nattributes = [\"Survived\", \"Pclass\", \"Age\",\"SibSp\",\"Parch\"]\nscatter_matrix(titanic_train[attributes], figsize=(12, 8))\n","acd5fdd4":"#checking for missing values:\ntitanic_train.info()\n","68d3b272":"\n#Q1:what is the survival ratio?\n#we are going to use a count plot to dive into the Survivor columns\nsns.set_style('whitegrid')\nsns.countplot(x='Survived',data=titanic_train,palette='rainbow')","0f907f09":"#Question2 : The survival rate wrt male and female\n#sns.set_style('whitegrid')\nsns.countplot(x='Survived',hue='Sex',data=titanic_train,palette='rainbow')\n","1b62dbf1":"#Question 3 : what was the survival rate wrt to passenger class?\nsns.set_style('whitegrid')\nsns.countplot(x='Survived',hue='Pclass',data=titanic_train,palette='rainbow')","43172edd":"#Question 4: What was the survival rate in different age classes\n\n# representing the age ranges in form of count plots.\ntitanic_train['age_group']=pd.cut(titanic_train.Age,bins=[0,10,20,30,40,50,60,70,80], labels=['0-10','10-20','20-39','30-40','40-50','50-60','60-70','70-80'])\nsns.set_style('whitegrid')\nsns.countplot(x='age_group',hue='Survived',data=titanic_train,palette='rainbow')","eb51b7ca":"titanic_train=titanic_train.drop(\"age_group\",axis=1)","8d6f51ee":"\n# Remove rows with missing target, separate target from predictors\ntitanic_train.dropna(axis=0, subset=['Survived'], inplace=True)\ntitanic_trainX=titanic_train.drop(\"Survived\",axis=1)\ntitanic_train_labels=titanic_train[\"Survived\"].copy()","8bff3bdd":"from sklearn.compose import ColumnTransformer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.preprocessing import StandardScaler\n","62114cc3":"#taking care of the null values\n#filtering the numeric attributes\nnumerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\nnumerical_cols= list(titanic_trainX.select_dtypes(include=numerics))\n#print(numerical_cols)\n\n#creating a numerical transformer for our pipeline purpose\nnumerical_transformer = Pipeline([\n ('imputer', SimpleImputer(strategy=\"median\")),\n ('std_scaler', StandardScaler()),\n ])\n","50818e81":"#checking the value counts for every categorical attribute\ns = (titanic_trainX.dtypes == 'object')\nobject_cols = list(s[s].index)\nprint(object_cols)","f72c207f":"titanic_train[object_cols].nunique()","38b3a9b8":"#creating a categorical pipeline\ncategorical_transformer = Pipeline(steps=[\n    ('imputer', SimpleImputer(strategy='most_frequent')),\n    ('onehot', OneHotEncoder(handle_unknown='ignore',sparse=False,categories=\"auto\"))\n])","efc7131a":"full_pipeline = ColumnTransformer([\n (\"num\", numerical_transformer, numerical_cols),\n (\"cat\",categorical_transformer ,object_cols),\n ])\ntitanic_prepared= full_pipeline.fit_transform(titanic_trainX)\ntitanic_prepared","342b7593":"from sklearn.model_selection import cross_val_score\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn import tree\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.svm import SVC\n","ac0cb54e":"gnb = GaussianNB()\ncv = cross_val_score(gnb,titanic_prepared,titanic_train_labels,cv=5)\nprint(cv)\nprint(cv.mean())","094c9501":"lr = LogisticRegression(max_iter = 2000)\ncv = cross_val_score(lr,titanic_prepared,titanic_train_labels,cv=5)\nprint(cv)\nprint(cv.mean())","c495fbc0":"dt = tree.DecisionTreeClassifier(random_state = 1)\ncv = cross_val_score(dt,titanic_prepared,titanic_train_labels,cv=5)\nprint(cv)\nprint(cv.mean())","922884d5":"knn = KNeighborsClassifier()\ncv = cross_val_score(knn,titanic_prepared,titanic_train_labels,cv=5)\nprint(cv)\nprint(cv.mean())","9e5d3e2e":"rf = RandomForestClassifier(random_state = 1)\ncv = cross_val_score(rf,titanic_prepared,titanic_train_labels,cv=5)\nprint(cv)\nprint(cv.mean())","1bf7fffb":"svc = SVC(probability = True)\ncv = cross_val_score(svc,titanic_prepared,titanic_train_labels,cv=5)\nprint(cv)\nprint(cv.mean())","b681d2cc":"from xgboost import XGBClassifier\nxgb = XGBClassifier(random_state =1)\ncv = cross_val_score(xgb,titanic_prepared,titanic_train_labels,cv=5)\nprint(cv)\nprint(cv.mean())","387be10e":"from sklearn.ensemble import VotingClassifier\nvoting_clf = VotingClassifier(estimators = [('lr',lr),('knn',knn),('rf',rf),('gnb',gnb),('svc',svc),('xgb',xgb)], voting = 'soft') \ncv = cross_val_score(voting_clf,titanic_prepared,titanic_train_labels,cv=5)\nprint(cv)\nprint(cv.mean())","20379b96":"from sklearn.model_selection import GridSearchCV \nfrom sklearn.model_selection import RandomizedSearchCV ","5ee2150f":"def clf_performance(classifier,model_name):\n    print(model_name)\n    print('Best score:'+str(classifier.best_score_))\n    print('Best Parameters: ' + str(classifier.best_params_))","a1022335":"lr = LogisticRegression()\nparam_grid = {'max_iter' : [2000],\n              'penalty' : ['l1', 'l2'],\n              'C' : [100, 10, 1.0, 0.1, 0.01],\n              'solver' : ['newton-cg','lbfgs']}\n\nclf_lr = GridSearchCV(lr, param_grid = param_grid, cv = 5, verbose = True, n_jobs = -1)\nbest_clf_lr = clf_lr.fit(titanic_prepared,titanic_train_labels)\nclf_performance(best_clf_lr,'Logistic Regression')","169b6f01":"knn = KNeighborsClassifier()\nparam_grid = {'n_neighbors' : np.arange(1,22,1),\n              'weights' : ['uniform', 'distance'],\n              'algorithm' : ['auto', 'ball_tree','kd_tree'],\n              'metric' : ['euclidean', 'manhattan', 'minkowski']\n              }\nclf_knn = GridSearchCV(knn, param_grid = param_grid, cv = 5, verbose = True, n_jobs = -1)\nbest_clf_knn = clf_knn.fit(titanic_prepared,titanic_train_labels)\nclf_performance(best_clf_knn,'KNN')","b1c6bbee":"svc = SVC(probability = True)\nparam_grid = tuned_parameters = [{'kernel': ['rbf', 'linear', 'sigmoid'],\n                                  'C': [.1, 1, 10, 100, 1000]},\n                                 {'kernel': ['poly'], 'degree' : [2,3,4,5], 'C': [.1, 1, 10, 100, 1000]}]\nclf_svc = GridSearchCV(svc, param_grid = param_grid, cv = 5, verbose = True, n_jobs = -1)\nbest_clf_svc = clf_svc.fit(titanic_prepared,titanic_train_labels)\nclf_performance(best_clf_svc,'SVC')","ea454e2e":"rf = RandomForestClassifier(random_state = 1)\nparam_grid =  {'n_estimators': [100,500,1000], \n                                  'bootstrap': [True,False],\n                                  'max_depth': [3,5,10,20,50,75,100,None],\n                                  'max_features': ['auto','sqrt'],\n                                  'min_samples_leaf': [1,2,4,10],\n                                  'min_samples_split': [2,5,10]}\n                                  \nclf_rf_rnd = RandomizedSearchCV(rf, param_distributions = param_grid, n_iter = 100, cv = 5, verbose = True, n_jobs = 6)\nbest_clf_rf = clf_rf_rnd.fit(titanic_prepared,titanic_train_labels)\nclf_performance(best_clf_rf,'Random Forest')","dfb2cf83":"\nvoting_clf.fit(titanic_prepared,titanic_train_labels)","c0b075fa":"#reading my training data \ntest_data_path=\"..\/input\/titanic\/test.csv\"\n#reading my data from the given path and storing it in the dataframe called \"titanic_traindata\"\ntitanic_test=pd.read_csv(test_data_path)","adc1fcdb":"\ntest_prepared= full_pipeline.transform(titanic_test)\ntest_prepared.shape\n","0fe800d2":"titanic_prepared.shape","bde32ae9":"my_pred=voting_clf.predict(test_prepared)","4c315aad":"final_data = {'PassengerId': titanic_test.PassengerId, 'Survived': my_pred}\nsubmission_2 = pd.DataFrame(data=final_data)\nsubmission_2.to_csv('submission_2.csv', index =False)","e2aa64ab":"\n#for j in ['Name','Sex','Ticket','Embarked']:\n  #  print(j)\n  #  print(sorted(titanic_train[j].unique())) # sort in alphabetical order","13092155":"# Titanic Survival Prediction \n**Task**: In this baisc beginner project, we'll be working on the titanic dataset and we're going to focus on the initial data preprocessing then EDA, and then finally we're going to train a model and fine tune it's performance.\nSome basic points I'll try to keep in mind:\n* Understand nature of the data \n* Histograms and boxplots\n* Value counts\n* Missing data\n* Correlation between the metrics\n* Explore interesting themes\n* Wealthy survive?\n* By location\n* Age scatterplot with ticket price\n* Young and wealthy Variable?\n* Total spent?\n* Feature engineering\n* preprocess data together or use a transformer?\n* use label for train and test\n* Scaling?\n* Model Baseline\n* Model comparison with CV\n\n","72deda71":"Taking a look at the scattermix plot, there are no interesting \"linear\" correlations found among the numerical attributes.","ec26ba9d":"hmm having a voting classifier did improve the results. i need to check if it's overfitting?\n","b8d03e13":"We'll be scaling our numerical features since Machine Learning algorithms don\u2019t perform well when\nthe input numerical attributes have very different scales.","9c42d1df":"# Model Building (Baseline validation on models):\nSo now i would like to see various  models performance with default parameters. The models are evaluated using CrossValidation. Next, we tried out an ensemble technique for encorporating all these default model results and giving out an average result ( Voting Classifier, good for generalizing)","90e3930d":"We have 5 integer features(PassengerId,Survived,Pclass,SibSp,Parch) and  2 floats(Age,Fare).\nWe have 5 categorical features: 1) Embarked, 2) Cabin , 3)Ticket,  4)Sex, 5)Name. \n\n\nAge and Fare are a continous discrete variable of float  with 64 placings. While the rest are discrete quantative int attributes.\n\nThe attribute 'cabin' has the most of the null values; we'll either drop it or consider to do some transformation over it.\nThen we'll have to deal the Age's null value, We cannot drop it because it is an important feature. There are also two records missing in the Embarked feature which we'll drop.","b1d4f2c4":"# Data Preprocessing","c347ede5":"# Loading the Data","e2090ff1":"Now we will see the frequency of the categorical features, and we'll check for the data entries for any structural errors or typos","f6f22ab9":"Regarding the Attributes: ![image.png](attachment:3750379a-193c-4f92-b05b-d3354eabd677.png)\n**notes for attributes**\n* pclass: A proxy for socio-economic status (SES)\n1. 1st = Upper\n2. 2nd = Middle\n3. 3rd = Lower\n\n* age: Age is fractional if less than 1. If the age is estimated, is it in the form of xx.5\n\n* sibsp: The dataset defines family relations in this way...\n* Sibling = brother, sister, stepbrother, stepsister\n* Spouse = husband, wife (mistresses and fianc\u00e9s were ignored)\n\n* parch: The dataset defines family relations in this way...\n* Parent = mother, father\n* Child = daughter, son, stepdaughter, stepson\n* Some children travelled only with a nanny, therefore parch=0 for them.","18bee487":"0= not survived, 1=survived; you can view the description on the kaggle dataset","8fd8330c":"#diving into some Questions:","1396f2dc":"# The EDA process","0d4f873b":"# Fine Tuning your Model\nNow after the baselines, we wil try to improve individual model accuracy\n","5a7c3d11":"first we are going to separate the predictors and the labels, we drop out the rows where we didn't had null labels i.e.'Survived':","c0e196e7":"now we're going to use voting classifiers. \nthey are good for generalising the overall output. the 'soft' parameter means that it is encorporatiing all the model's probabilities and averaging them","76d4e64f":"The correlation coefficient ranges from \u20131 to 1. When it is close to 1, it means that\nthere is a strong positive correlation; for example, the median house value tends to go\nup when the median income goes up. When the coefficient is close to \u20131, it means\nthat there is a strong negative correlation; (i.e., prices have a slight tendency to\ngo down when you go north). Finally, coefficients close to zero mean that there is no\nlinear correlation. \n\nthis method completely misses out non linear relationships","68edaf2f":"> Data preprocessing is a data mining technique that transforms raw data into an understandable format. This process has four main stages \u2013 data cleaning, data integration, data transformation, and data reduction.\n\n>Data cleaning will filter, detect, and handle dirty data to ensure quality data and quality analysis results. In this case, there may be noises of impossible and extreme values and outliers, and missing values. The errors may include inconsistent data and redundant attributes and data."}}