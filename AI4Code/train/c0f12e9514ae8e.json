{"cell_type":{"50077d66":"code","c605e55a":"code","55d046b7":"code","b8b4f1bc":"code","5e7f6c1d":"code","652093ad":"code","be7c7ce8":"code","64a68c0a":"markdown","f871a116":"markdown","9abb611d":"markdown","fa635c5a":"markdown","a0c67589":"markdown","1be4e302":"markdown","d382c92e":"markdown","3f0be041":"markdown","6253de6f":"markdown"},"source":{"50077d66":"## Imports\nimport pandas as pd\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader, random_split\nfrom torchtext.data.utils import get_tokenizer\nfrom torchtext.vocab import GloVe\nfrom collections import Counter\nfrom torch.nn.utils.rnn import pad_sequence\nimport pickle\n\n## Read data\ntrain = pd.read_csv('..\/input\/commonlitreadabilityprize\/train.csv')\ntest = pd.read_csv('..\/input\/commonlitreadabilityprize\/test.csv')\ntrain = train.rename(columns={'id':'ids'})\ntest = test.rename(columns={'id':'ids'})\nwith open('..\/input\/pickled-glove840b300d-for-10sec-loading\/glove.840B.300d.pkl', 'rb') as inglove:\n    glove = pickle.load(inglove)\n\n## Set device\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\n## Seed\ntorch.manual_seed(43)\ntorch.cuda.manual_seed(43)\nnp.random.seed(43)","c605e55a":"## Hyperparameters\nn_layers = 1\nlearning_rate = 1e-4\nn_epochs = 6\ntokenizer = get_tokenizer('spacy')\nbatch_size = 4                       # equivalent to n_observations\ninput_size = embedding_dim = 300     # also equivalent to n_features, vocab_size \nhidden_size = 50\nmax_len= train['excerpt'].apply(lambda x: len(tokenizer(x))).max()\nseq_len = max_len   # with ^, the second dimension of an observation","55d046b7":"def tokenize_and_vectorize(excerpt: str):\n    tokenized = tokenizer(excerpt)\n    vectorized_excerpt = []\n    for token in tokenized:\n        try:\n            vectorized = torch.tensor(glove[token])\n        except KeyError:\n            vectorized = torch.zeros(embedding_dim)\n        vectorized_excerpt.append(vectorized)\n    return torch.stack(vectorized_excerpt)\n\ntrain['inputs'] = train['excerpt'].apply(tokenize_and_vectorize)\ntest['inputs'] = test['excerpt'].apply(tokenize_and_vectorize)","b8b4f1bc":"# Batch data\nclass CLDataset(Dataset):\n    def __init__(self, df): \n        self.df = df\n\n    def __len__(self):                   \n        return self.df.shape[0]\n\n    def __getitem__(self, idx):          \n        column_map = {v:k for (k,v) in list(enumerate(train.columns))}\n        X = self.df.iloc[idx, column_map['inputs']]\n        y = self.df.iloc[idx, column_map['target']]\n        return X, y\n\nclass CLTestDataset(Dataset):\n    def __init__(self, df): \n        self.df = df\n\n    def __len__(self):                   \n        return self.df.shape[0]\n\n    def __getitem__(self, idx):          \n        column_map = {v:k for (k,v) in list(enumerate(test.columns))}\n        X = self.df.iloc[idx, column_map['inputs']]\n        y = torch.zeros((1))\n        return X, y\n\ndef collate_fn(batch): \n    X, y = zip(*batch)\n    X = pad_sequence(X, batch_first=True).to(device)\n    y = torch.tensor(y).type(torch.float).to(device)\n    return X, y\n\ndef collate_fn_test(batch):\n    X, y = zip(*batch)\n    X = pad_sequence(X, batch_first=True).to(device)\n    y = torch.tensor(y).type(torch.float).to(device)\n    return X, y\n\n\n# Load data\ntrain_set = CLDataset(train)\npartition_lengths = [round(len(train_set)*.8), round(len(train_set)*.2)]\ntrain_set, val_set = random_split(train_set, partition_lengths)\n\ntrain_loader = DataLoader(dataset=train_set, \n                          batch_size=batch_size,\n                          collate_fn=collate_fn,\n                          shuffle=True)\n                          \nval_loader =  DataLoader(dataset=val_set, \n                         batch_size=batch_size,\n                         collate_fn=collate_fn, \n                         shuffle=True)\n\ntest_set = CLTestDataset(test)\ntest_loader =  DataLoader(dataset=test_set, \n                          batch_size=1,\n                          collate_fn=collate_fn_test,\n                          shuffle=False)","5e7f6c1d":"## Create network\nclass myRNN(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.i_h = nn.Linear(input_size,  hidden_size)\n        self.h_h = nn.Linear(hidden_size, hidden_size)\n        self.h_o = nn.Linear(hidden_size, 1)\n        \n    def forward(self, x):\n        # initialize hidden_state \n        self.h   = torch.zeros((1,hidden_size)).to(device)\n        time_steps = x.shape[1]     \n        for t in range(time_steps): \n            word_vector = x[:, t, :]  # x_t\n            # prev hidden_state + input word\n            self.h = self.h + self.i_h(word_vector)\n            # multiply by hidden weights, activate\n            self.h = F.relu(self.h_h(self.h)) \n        \n        out = self.h_o(self.h).squeeze(dim=1) \n        self.h = self.h.detach()\n    \n        return out","652093ad":"## Initialize network\nmodel = myRNN().to(device)\n\n## Loss and optimizer\ncriterion = nn.MSELoss()\noptimizer = torch.optim.Adam(\n    [param for param in model.parameters() if param.requires_grad == True],\n    lr=learning_rate)\n## Train network\nfor epoch in range(n_epochs):\n    \n    # Training\n    train_loss = 0.0\n    model.train()\n    model.i_h.requires_grad_ = False\n    for batch_idx, (inputs, labels) in enumerate(train_loader):       \n        outputs = model(inputs)\n        \n        optimizer.zero_grad()\n        loss = criterion(outputs, labels) \n        loss.backward()\n        optimizer.step()\n\n        train_loss += loss.item()\n\n    # Validation\n    val_loss = 0.0\n    model.eval()\n    for batch_idx, (inputs, labels) in enumerate(val_loader):\n       \n        outputs = model(inputs)\n        \n        optimizer.zero_grad()\n        loss = criterion(outputs, labels) \n        val_loss += loss.item()\n\n    print(f\"Epoch:      {epoch}   \"                         \\\n          f\"Train Loss: {train_loss\/len(train_loader)}    \"  \\\n          f\"Valid Loss: {val_loss\/len(val_loader)}\" )","be7c7ce8":"preds = [] \nwith torch.no_grad():\n    for _, (inputs, _) in enumerate(test_loader):\n        outputs = model(inputs)\n        preds.append(outputs.squeeze(-1).cpu().numpy())\n\nsubmission = pd.read_csv('..\/input\/commonlitreadabilityprize\/sample_submission.csv')\nsubmission['target'] = preds\nsubmission.to_csv(\"submission.csv\",index = False)","64a68c0a":"## Training  \n- We're using static word embeddings pretrained on a huge corpus\n- Our training data is tiny by comparison \n- So we *definitely* want to **freeze the weights** on our input_to_hidden layer.  \n\nThere are several ways to freeze a layer in pytorch. The two step process below is common.\n1. Set `requires_grad_` to `False` for the input layer.\n2. Tell our optimizer to ignore any layers configured as such.","f871a116":"## Dependencies","9abb611d":"## Hyperparameters","fa635c5a":"In **step 2** we perform the matrix multiplication W_ih @ X. From the view of an individual word vector x_t, that word vector is 'dotted' by each row of the W_ih weight matrix.\n\n**Step 3** shows the previous hidden state being produced in the previous time step and then passed in to be added to the 50-dim transformed x_t in **step 4**.  \n\n**Step 5** applies the activation function.\n\n**Step 6** Note that we only use the output y_hat for the final iteration of the loop, after all of the time steps are completed (indicated by the ellipsis ...) . This is our (1, 1) float, which we use to calculate the loss. This makes our network a *sequence-to-vector* RNN. Other scenarios might use the outputs y_t from each iteration of the loop - these would be *sequence-to-sequence* RNNs.\n   \n  \n**In summary, to 'Roll our own' RNN we just:**\n1. lay out linear layers, one for each token in our sequence\n2. 'roll up' the layers by looping over that single layer. ","a0c67589":"## Rolling up a deep net\n\nEach`forward` pass of `myRNN()` below has a loop that processes a sequence (actually a batch of them), one word at a time - so it is in effect a 200+ layer Neural Network w.r.t. to the individual words - one layer for each word. Typically each layer is represented as a *timestep* - see the right side of the image below. (Here, S_t represents the `hidden_state` passed forward in time, and X_t is an input word):\n\n\n![image.png](attachment:fa8c6311-cd2c-466e-bf35-d8633251242b.png)  \nhttps:\/\/www.kdnuggets.com\/2015\/10\/recurrent-neural-networks-tutorial.html  \n\nWhat makes it manageable is that we're reusing the same `Linear` (or `Dense`) layer over and over again - the weight tensor of that layer is only updated once per pass. From this perspective, it's really just a 1-hidden-layer Neural Network. This is the view seen on the left of the image.\n\nThere are two 'inputs' to think about:\n1. The input to the forward pass is a single text document - one of our 'excerpts'. As mentioned in the Transform section above, **each of our input excerpts is represented as a 2-d tensor of shape (<=260, 300)** - The exact size of the sequence_length axis is variable between batches, but uniform within a batch (after padding).  \n\n2. The input to the 'RNN layer' - the loop - is a single (1, 300) word vector sliced off of our 2-d input tensor - shown as **step 1** in this sketch:\n\n\n\n![image.png](attachment:0504d160-4a45-41db-b7d4-8b96e0bed4bf.png)\n ","1be4e302":"## Inference","d382c92e":"## Transform\n\n\n\n\n\n\n\n\n\n\n![excerpt_to_batch.png](attachment:d89d93aa-25da-42b0-b0a2-925454413d15.png)\n\n\n\n\n\n\n\n\n\n1. Each excerpt is first tokenized.\n2. Then we look up the 300-d GloVe word-vector for each token\n3. After substituting the appropriate word-vector for each token, the transformed excerpts are stacked into a 2-d `tensor` to be fed into the network.\n4. The 2-d tensors are batched by a Dataloader","3f0be041":"## Motivation\nI'm fairly new to both Pytorch and Text Classification (and for that matter, training Neural Networks). Goals:\n1. Build an understanding and intuition around how RNNs work by implementing one using only `Linear` layers in Pytorch.\n2. Get comfortable with a Pytorch workflow before tackling a more sophisticated architecture (ahem Transformers...).  Using static word embeddings is an easier starting point, even if they are being supplanted by dynamic embeddings.\n3. Start contributing to Kaggle.\n\nI found several sources helpful in implementing an RNN as a looped `Linear` layer. \n\nIn particular my code for the rnn loop is based on this discussion:  \nhttps:\/\/github.com\/fastai\/fastbook\/blob\/master\/12_nlp_dive.ipynb  \nJeremy Howard, Sylvain Gugger  \n\nBut I didn't really understand the implementation until I looked at it from the matrix multiplication view, as in:   \nhttps:\/\/gist.github.com\/karpathy\/587454dc0146a6ae21fc  \nAndrej Karpathy\n\nI also found these two notebooks by Kaggler Leigh useful for organizing Pytorch code:  \nhttps:\/\/www.kaggle.com\/leighplt\/simple-recurrent-model-pytorch  \nhttps:\/\/www.kaggle.com\/leighplt\/transformers-cv-train-inference-pytorch\n\nIf you see something, say something in the comments! And **if it's useful, please upvote**!","6253de6f":"## Batch and Load"}}