{"cell_type":{"3e18b83f":"code","775aae5b":"code","2a9e1900":"code","330b00bb":"code","24b95397":"code","66c303ce":"code","4f11b653":"code","e9f6deaf":"code","4f49162d":"code","79c96b44":"code","4b8f86b7":"markdown","83bdf391":"markdown","05a5ce06":"markdown","381e24f5":"markdown","b7ee6f67":"markdown","1f8b2941":"markdown","9d5b3baa":"markdown","de42e607":"markdown","96d99bc9":"markdown","f8ed1219":"markdown","fb161515":"markdown","81cf2fbe":"markdown","085aaffc":"markdown","b7be54f0":"markdown","4a6f7c25":"markdown","aadf732b":"markdown","37648acb":"markdown","58a672af":"markdown","2a122657":"markdown","6c9ab3cd":"markdown","bd1f176b":"markdown","c6f1ab1a":"markdown","34d0414b":"markdown","9acd672a":"markdown","cf4886e7":"markdown","7cc4e3c7":"markdown","59a2dd4d":"markdown","bd28bea4":"markdown","10b75a05":"markdown","b7ceef79":"markdown","7ddbe39e":"markdown","b3024292":"markdown","faaa89d5":"markdown","a27260cf":"markdown","b6fe48cc":"markdown","6a9326c2":"markdown","d3d72a81":"markdown","0e01c2bf":"markdown","0a1e4c24":"markdown","90a245b9":"markdown","6559969c":"markdown"},"source":{"3e18b83f":"# from sklearn.model_selection import GridSearchCV\nfrom ray.tune.sklearn import TuneGridSearchCV\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import SGDClassifier\nfrom sklearn.datasets import load_iris\nimport numpy as np\n\n\niris = load_iris()\nX = iris.data\ny = iris.target\n\nx_train, x_test, y_train, y_test = train_test_split(X,y,test_size = 0.3,random_state = 14)\n\n# Example parameters to tune from SGDClassifier\nparameter_grid = {\"alpha\": [1e-4, 1e-1, 1], \"epsilon\": [0.01, 0.1]}\n\ntune_search = TuneGridSearchCV(SGDClassifier(),parameter_grid,early_stopping=True,max_iters=10)\n\ntune_search.fit(x_train, y_train)\nprint(tune_search.best_score)\nprint(tune_search.best_params_)","775aae5b":"# !pip install optuna","2a9e1900":"import optuna\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.model_selection import KFold , cross_val_score\nfrom sklearn.datasets import load_iris\n\niris = load_iris()\nX = iris.data\ny = iris.target\n\nfrom sklearn.model_selection import train_test_split\nX_train,X_test,y_train,y_test = train_test_split(X,y,test_size = 0.3,random_state = 14)\n\ndef objective(trial):\n\n    optimizer = trial.suggest_categorical('algorithm', ['auto','ball_tree','kd_tree','brute'])\n    rf_max_depth = trial.suggest_int(\"k_n_neighbors\", 2, 10, log=True)\n    knn = KNeighborsClassifier(n_neighbors=rf_max_depth,algorithm=optimizer)\n\n    score = cross_val_score(knn, X_train,y_train, n_jobs=-1, cv=3)\n    accuracy = score.mean()\n    return accuracy\n\n\nif __name__ == \"__main__\":\n    study = optuna.create_study(direction=\"maximize\")\n    study.optimize(objective, n_trials=10)\n    print(study.best_trial)\n    \n#best parameter combination\nstudy.best_params\n\n#score achieved with best parameter combination\nstudy.best_value","330b00bb":"from hyperopt import fmin, tpe, hp, STATUS_OK, Trials , space_eval\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.datasets import load_iris\n\niris = load_iris()\nX = iris.data\ny = iris.target\n\ndef hyperopt_train_test(params):\n    clf = RandomForestClassifier(**params)\n    return cross_val_score(clf, X, y).mean()\n\nspace = {\n    'max_depth': hp.choice('max_depth', range(1,20)),\n    'max_features': hp.choice('max_features', range(1,5)),\n    'n_estimators': hp.choice('n_estimators', range(1,20)),\n    'criterion': hp.choice('criterion', [\"gini\", \"entropy\"])\n            }\nbest = 0\ndef f(params):\n    global best\n    acc = hyperopt_train_test(params)\n    if acc > best:\n      best = acc\n      print( 'new best:', best, params)\n    return {'loss': -acc, 'status': STATUS_OK}\ntrials = Trials()\nbest = fmin(f, space, algo=tpe.suggest, max_evals=300, trials=trials)\nprint(best)","24b95397":"# !pip install parameter-sherpa","66c303ce":"from sklearn.datasets import load_breast_cancer\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import cross_val_score\nimport time\nimport sherpa\nimport sherpa.algorithms.bayesian_optimization as bayesian_optimization\n\n\nparameters = [sherpa.Discrete('n_estimators', [2, 50]),\n              sherpa.Choice('criterion', ['gini', 'entropy']),\n              sherpa.Continuous('max_features', [0.1, 0.9])]\n\nalgorithm = bayesian_optimization.GPyOpt(max_concurrent=1,\n                                         model_type='GP_MCMC',\n                                         acquisition_type='EI_MCMC',\n                                         max_num_trials=10)\n\nX, y = load_breast_cancer(return_X_y=True)\nstudy = sherpa.Study(parameters=parameters,\n                     algorithm=algorithm,\n                     lower_is_better=False)\n\nfor trial in study:\n    print(\"Trial \", trial.id, \" with parameters \", trial.parameters)\n    clf = RandomForestClassifier(criterion=trial.parameters['criterion'],\n                                 max_features=trial.parameters['max_features'],\n                                 n_estimators=trial.parameters['n_estimators'],\n                                 random_state=0)\n    scores = cross_val_score(clf, X, y, cv=5)\n    print(\"Score: \", scores.mean())\n    study.add_observation(trial, iteration=1, objective=scores.mean())\n    study.finalize(trial)\nprint(study.get_best_result())","4f11b653":"# !pip install scikit-optimize","e9f6deaf":"from skopt import BayesSearchCV\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\n# parameter ranges are specified by one of below\nfrom skopt.space import Real, Categorical, Integer\n\nknn = KNeighborsClassifier()\n#defining hyper-parameter grid\ngrid_param = { 'n_neighbors' : list(range(2,11)) , \n              'algorithm' : ['auto','ball_tree','kd_tree','brute'] }\n\n#initializing Bayesian Search\nBayes = BayesSearchCV(knn , grid_param , n_iter=30 , random_state=14)\nBayes.fit(X_train,y_train)\n\n#best parameter combination\nprint(f'Best parameter combination : {Bayes.best_params_}')\n\n#score achieved with best parameter combination\nprint(f'Best Score : {Bayes.best_score_}')\n\n#all combinations of hyperparameters\n# print(Bayes.cv_results_['params'])\n\n#average scores of cross-validation\n# Bayes.cv_results_['mean_test_score']","4f49162d":"!pip install gpyopt","79c96b44":"import GPy\nimport GPyOpt\nfrom GPyOpt.methods import BayesianOptimization\nfrom sklearn.model_selection import train_test_split\nfrom sklearn import svm\nfrom sklearn.datasets import load_iris\nfrom scipy.stats import uniform\nfrom xgboost import XGBRegressor\nimport numpy as np\n\niris = load_iris()\nX = iris.data\ny = iris.target\n\nx_train, x_test, y_train, y_test = train_test_split(X,y,test_size = 0.3,random_state = 14)\n\nbds = [{'name': 'learning_rate', 'type': 'continuous', 'domain': (0, 1)},\n        {'name': 'gamma', 'type': 'continuous', 'domain': (0, 5)},\n        {'name': 'max_depth', 'type': 'discrete', 'domain': (1, 50)}]\n\n# Optimization objective \ndef cv_score(parameters):\n    parameters = parameters[0]\n    score = cross_val_score(\n                XGBRegressor(learning_rate=parameters[0],\n                              gamma=int(parameters[1]),\n                              max_depth=int(parameters[2])), \n                X, y, scoring='neg_mean_squared_error').mean()\n    score = np.array(score)\n    return score\n\noptimizer = GPyOpt.methods.BayesianOptimization(f = cv_score,            # function to optimize       \n                                          domain = bds,         # box-constraints of the problem\n                                          acquisition_type ='LCB',       # LCB acquisition\n                                          acquisition_weight = 0.1)   # Exploration exploitation\n\nx_best = np.exp(optimizer.X[np.argmin(optimizer.Y)])\nprint(\"Best parameters: learning_rate=\"+str(x_best[0])+\",gamma=\"+str(x_best[1])+\",max_depth=\"+str(x_best[2]))\n","4b8f86b7":"**Installation**: pip install mlmachine","83bdf391":"### Key Features\n1. Easy-to-use: Polyaxon\u2019s Optimization Engine is a built-in service and can be used easily by adding a matrix section to your operations, you can run hyperparameter tuning using the CLI, client, and the dashboard.\n2. Scalability: Tuning hyperparameters or neural architectures requires leveraging a large amount of computation resources, using Polyaxon you can run hundreds of trials in parallel and intuitively track their progress.\n3. Flexibility: Besides the rich built-in algorithms, Polyaxon allows users to customize various hyperparameter tuning algorithms, neural architecture search algorithms, early stopping algorithms, etc.\n4. Efficiency: We are intensively working on more efficient model tuning from both system-level and algorithm level. For example, leveraging early feedback to speedup tuning procedure.","05a5ce06":"[GPyOpt](https:\/\/github.com\/SheffieldML\/GPyOpt) is a tool for optimization (minimization) of black-box functions using Gaussian processes. It has been implemented in Python by the [group of Machine Learning](http:\/\/ml.dcs.shef.ac.uk\/sitran\/) (at SITraN) of the University of Sheffield.[GPyOpt](https:\/\/github.com\/SheffieldML\/GPy) is based on GPy, a library for Gaussian process modeling in Python. It can handle large data sets via sparse Gaussian process models.","381e24f5":"# 9. Scikit-Optimize","b7ee6f67":"# Thank you for reading!\nAny feedback and comments are, greatly appreciated!","1f8b2941":"[mlmachine](https:\/\/github.com\/petersontylerd\/mlmachine#Installation) is a Python package that facilitates clean and organized notebook-based machine learning experimentation and accomplishes many key aspects of the experimentation life cycle.<br><br>\nmlmachine performs [Hyperparameter Tuning with Bayesian Optimization](https:\/\/nbviewer.jupyter.org\/github\/petersontylerd\/mlmachine\/blob\/master\/notebooks\/mlmachine_part_4.ipynb) on multiple estimators in one shot and includes functionality for visualizing model performance and parameter selections.<br>","9d5b3baa":"**Hyper-parameters** are the parameters used to control the behavior of the algorithm while building the model. These parameters cannot be learned from the regular training process. They need to be assigned before training the model.<br><br>\nExample: **n_neighbors** (KNN), **kernel** (SVC) , **max_depth & criterion** (Decision Tree Classifier) etc.<br>\n","de42e607":"# 2. Optuna","96d99bc9":"**Installation**: pip install -U polyaxon","f8ed1219":"# 1. Ray-Tune","fb161515":"[Hyperopt](https:\/\/github.com\/hyperopt\/hyperopt) is a Python library for serial and parallel optimization over awkward search spaces, which may include real-valued, discrete, and conditional dimensions.<br>\n**Hyperopt currently it supports three algorithms :**\n* [Random Search](http:\/\/www.jmlr.org\/papers\/v13\/bergstra12a.html?source=post_page---------------------------)\n* [Tree of Parzen Estimators (TPE)](https:\/\/papers.nips.cc\/paper\/4443-algorithms-for-hyper-parameter-optimization.pdf)\n* [Adaptive TPE](https:\/\/www.electricbrain.io\/blog\/learning-to-optimize)\n<br>\n\n### Key Features\n1. Search space **(you can create very complex parameter spaces)**\n2. Persisting and restarting **(you can save important information and later load and then resume the optimization process)**\n3. Speed and Parallelization **(you can distribute your computation over a cluster of machines)**","81cf2fbe":"**Installation**: pip install scikit-optimize","085aaffc":"**It provides:**\n1. hyperparameter optimization for machine learning researchers\n2. a choice of hyperparameter optimization algorithms\n3. parallel computation that can be fitted to the user\u2019s needs\n4. a live dashboard for the exploratory analysis of results.","b7be54f0":"# 8. SHERPA","4a6f7c25":"# 10. GPyOpt","aadf732b":"**Installation**: pip install parameter-sherpa","37648acb":"**Installation**: pip install talos","58a672af":"1. [Talos User Manul](https:\/\/autonomio.github.io\/docs_talos\/#introduction)\n2. [ Smart Hyperparameter Optimization of any Deep Learning model using TPU and Talos](https:\/\/towardsdatascience.com\/smart-hyperparameter-optimization-of-any-deep-learning-model-using-tpu-and-talos-9eb48d09d637)","2a122657":"**Installation**: pip install optuna","6c9ab3cd":"# Introduction","bd1f176b":"**Hyperparameter optimization or tuning** in machine learning is the process of selecting the best combination of hyper-parameters that deliver the best performance.<br><br>\nVarious automatic optimization techniques exist, and each has its own strengths and drawbacks when applied to different types of problems.<br>\nExample: Grid Search, Random Search, Bayesian Search, etc.<br>\nScikit-learn is one of the frameworks we could use for Hyperparameter optimization, but there are other frameworks that could even perform better.<br>\n1. Ray-Tune\n2. Optuna\n3. Hyperopt\n4. mlmachine\n5. Polyaxon\n6. BayesianOptimization\n7. Talos\n8. SHERPA\n9. Scikit-Optimize\n10. GPyOpt\n","c6f1ab1a":"# 5. Polyaxon","34d0414b":"### Key Features\n\n1. Single-line optimize-to-predict pipeline *talos.Scan(x, y, model, params).predict(x_test, y_test)*\n2. Automated hyperparameter optimization\n3. Model generalization evaluator\n4. Experiment analytics\n5. Pseudo, Quasi, and Quantum Random search options\n6. Grid search\n7. Probabilistic optimizers\n8. Single file custom optimization strategies","9acd672a":"# 4. mlmachine","cf4886e7":"[Tune](https:\/\/docs.ray.io\/en\/latest\/tune\/index.html) is a Python library for experiment execution and hyperparameter tuning at any scale.[[GitHub](https:\/\/github.com\/ray-project\/tune-sklearn)]<br>\n### Key Features<br>\n1. Launch a multi-node [distributed hyperparameter sweep](https:\/\/docs.ray.io\/en\/latest\/tune\/tutorials\/tune-distributed.html#tune-distributed) in less than ten lines of code.\n2. Supports any machine learning framework, [including PyTorch, XGBoost, MXNet, and Keras.](https:\/\/docs.ray.io\/en\/latest\/tune\/tutorials\/overview.html#tune-guides)\n3. Choose among the state of the art algorithms such as [Population Based Training (PBT)](https:\/\/docs.ray.io\/en\/latest\/tune\/api_docs\/schedulers.html#tune-scheduler-pbt), [BayesOptSearch](https:\/\/docs.ray.io\/en\/latest\/tune\/api_docs\/suggestion.html#bayesopt), [HyperBand\/ASHA](https:\/\/docs.ray.io\/en\/latest\/tune\/api_docs\/schedulers.html#tune-scheduler-hyperband).\n4. Tune\u2019s Search Algorithms are wrappers around open-source optimization [libraries](https:\/\/docs.ray.io\/en\/latest\/tune\/api_docs\/suggestion.html) such as HyperOpt, SigOpt, Dragonfly, and Facebook Ax.\n5. Automatically visualize results with TensorBoard.","7cc4e3c7":"# 7. Talos","59a2dd4d":"**Installation**: pip install gpyopt","bd28bea4":"### Key Features\n\n1. [Bayesian optimization with arbitrary restrictions](https:\/\/nbviewer.jupyter.org\/github\/SheffieldML\/GPyOpt\/blob\/master\/manual\/GPyOpt_constrained_optimization.ipynb)\n2. [Parallel Bayesian optimization](https:\/\/nbviewer.jupyter.org\/github\/SheffieldML\/GPyOpt\/blob\/master\/manual\/GPyOpt_parallel_optimization.ipynb)\n3. [Mixing different types of variables](https:\/\/nbviewer.jupyter.org\/github\/SheffieldML\/GPyOpt\/blob\/master\/manual\/GPyOpt_mixed_domain.ipynb)\n4. [Tuning scikit-learn models](https:\/\/nbviewer.jupyter.org\/github\/SheffieldML\/GPyOpt\/blob\/master\/manual\/GPyOpt_scikitlearn.ipynb)\n5. [Integrating the model hyper parameters](https:\/\/nbviewer.jupyter.org\/github\/SheffieldML\/GPyOpt\/blob\/master\/manual\/GPyOpt_integrating_model_hyperparameters.ipynb)\n6. [External objective evaluation](https:\/\/nbviewer.jupyter.org\/github\/SheffieldML\/GPyOpt\/blob\/master\/manual\/GPyOpt_external_objective_evaluation.ipynb)","10b75a05":"**Installation**: pip install bayesian-optimization","b7ceef79":"[Polyaxon](https:\/\/polyaxon.com\/docs\/automation\/optimization-engine\/) is a platform for building, training, and monitoring large scale deep learning applications. It makes a system to solve reproducibility, automation, and scalability for machine learning applications.<br>\n\nThe way Polyaxon performs hyperparameter tuning is by providing a selection of customizable search algorithms. Polyaxon supports both simple approaches such as *random search* and *grid search*, and provides a simple interface for advanced approaches, such as *Hyperband* and *Bayesian Optimization*, it also integrates with tools such as *Hyperopt*, and provides an interface for running custom iterative processes. All these search algorithms run in an asynchronous way, and support concurrency and routing to leverage your cluster(s)\u2019s resources to the maximum.","7ddbe39e":"[Optuna](https:\/\/optuna.readthedocs.io\/en\/stable\/) is an automatic hyperparameter optimization software framework, particularly designed for machine learning.<br>\n\n### Key Features<br>\n\n1. [Easy parallelization](https:\/\/optuna.readthedocs.io\/en\/stable\/tutorial\/004_distributed.html)\n2. [Quick visualization](https:\/\/optuna.readthedocs.io\/en\/stable\/reference\/visualization.html)\n3. [Efficient optimization algorithms](https:\/\/optuna.readthedocs.io\/en\/stable\/tutorial\/007_pruning.html)\n4. [Lightweight, versatile, and platform-agnostic architecture](https:\/\/optuna.readthedocs.io\/en\/stable\/tutorial\/001_first.html)\n5. [Pythonic search spaces](https:\/\/optuna.readthedocs.io\/en\/stable\/tutorial\/002_configurations.html)\n","b3024292":"[An Introductory Example of Bayesian Optimization in Python with Hyperopt](https:\/\/towardsdatascience.com\/an-introductory-example-of-bayesian-optimization-in-python-with-hyperopt-aae40fff4ff0)\n","faaa89d5":"[mlmachine - Hyperparameter Tuning with Bayesian Optimization](https:\/\/towardsdatascience.com\/mlmachine-hyperparameter-tuning-with-bayesian-optimization-2de81472e6d)","a27260cf":"[SHERPA](https:\/\/parameter-sherpa.readthedocs.io\/en\/latest\/) is a Python library for hyperparameter tuning of machine learning models.","b6fe48cc":"# 3. Hyperopt","6a9326c2":"[Scikit-Optimize](https:\/\/scikit-optimize.github.io\/stable\/user_guide.html), or skopt, is a simple and efficient library to minimize (very) expensive and noisy black-box functions. It implements several methods for sequential model-based optimization. skopt aims to be accessible and easy to use in many contexts. Scikit-Optimize provides support for tuning the hyperparameters of ML algorithms offered by the scikit-learn library, so-called hyperparameter optimization.<br>\n\nThe library is built on top of NumPy, SciPy and Scikit-Learn.","d3d72a81":"# 6. Bayesian Optimization","0e01c2bf":"[Talos](https:\/\/github.com\/autonomio\/talos) radically changes the ordinary Keras workflow by fully automating hyperparameter tuning and model evaluation. Talos exposes Keras functionality entirely and there is no new syntax or templates to learn.","0a1e4c24":"[Bayesian Optimization](https:\/\/github.com\/fmfn\/BayesianOptimization) is another framework that is a pure Python implementation of Bayesian global optimization with Gaussian processes. This is a constrained global optimization package built upon Bayesian inference and Gaussian process, that attempts to find the maximum value of an unknown function in as few iterations as possible. This technique is particularly suited for the optimization of high-cost functions, situations where the balance between exploration and exploitation is important.","90a245b9":"**Installation**: pip install hyperopt","6559969c":"**Installation**: pip install ray[tune] tune-sklearn"}}