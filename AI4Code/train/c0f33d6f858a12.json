{"cell_type":{"afc60490":"code","83949012":"code","e57a2fd1":"code","4f2c439f":"code","fe53dd95":"code","51fe0092":"code","f984bb5d":"code","247f1497":"code","283e083a":"code","eb348cee":"code","6c35c7cc":"code","3244fb21":"code","1d407ca6":"code","481381f1":"code","a9374465":"code","fac7cbc8":"code","9905e435":"code","b17b6fec":"code","bf7424f7":"code","c2495f59":"code","8490a05f":"code","2292d51e":"code","c577a405":"code","5120c9df":"code","0ef47203":"code","90ae0524":"code","4a156f61":"code","c071ad8b":"code","d94fe87e":"code","7668f306":"code","d8ac790f":"code","d6504986":"code","f0d90715":"code","380f7454":"code","14b07b07":"code","2dd80638":"code","bd3c8790":"code","d6002364":"code","d8e1d1ed":"code","252bc4c6":"code","93eb4adb":"code","ad7297ef":"code","8d76182e":"code","0e20b894":"code","5c253e0f":"markdown","6a8b3114":"markdown","e23ef6eb":"markdown","8fed2d79":"markdown","14ce9b1b":"markdown","c48d493b":"markdown","5ea2a18a":"markdown","17deab2c":"markdown","53c3cffb":"markdown","3c984594":"markdown","98f2f22e":"markdown","23540360":"markdown","b23c8e6c":"markdown","6570f685":"markdown","6cabf957":"markdown","17e7f785":"markdown","2f823b55":"markdown","9d481284":"markdown","972b55b6":"markdown","1cda5baf":"markdown","af7b175d":"markdown","e9629d6f":"markdown","d45155c5":"markdown","13c008b1":"markdown","9321c688":"markdown","54d87dd2":"markdown","3a187ec5":"markdown","02119a9e":"markdown","8c825f5e":"markdown","a26cf0b3":"markdown","75814e0e":"markdown"},"source":{"afc60490":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","83949012":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","e57a2fd1":"df=pd.read_csv(\"\/kaggle\/input\/used-car-dataset-ford-and-mercedes\/audi.csv\")\ndf.head(20)","4f2c439f":"df.describe()","fe53dd95":"import matplotlib.pyplot as plt\nresult=df.groupby(\"year\")[\"price\"].mean()\nprint(result)\nplt.figure(figsize=(12,6))\n\nplt.plot(result,color='blue')  \nplt.xlabel('Year')\nplt.ylabel('Price')\nplt.show()","51fe0092":"df.info()\ndf.head(5)","f984bb5d":"df.groupby('transmission').agg({'transmission':'count'})","247f1497":"import seaborn as sns\nplt.figure(figsize = (15,8))\nsns.countplot(df['transmission'].astype('str').sort_values())\nplt.show()","283e083a":"print('number of models:',len(df['model'].unique()))\ndf['model'].value_counts()","eb348cee":"import seaborn as sns\nplt.figure(figsize = (15,8))\nsns.countplot(df['model'].astype('str').sort_values())\nplt.show()","6c35c7cc":"df['fuelType'].value_counts()","3244fb21":"import seaborn as sns\nplt.figure(figsize = (10,8))\nsns.histplot(df['fuelType'].astype('str').sort_values())\nplt.show()","1d407ca6":"df['engineSize'].value_counts()","481381f1":"import seaborn as sns\nplt.figure(figsize = (15,8))\nsns.histplot(df['engineSize'].astype('str').sort_values())\nplt.show()","a9374465":"df['year'].value_counts()","fac7cbc8":"import seaborn as sns\nplt.figure(figsize = (10,9))\nsns.histplot(df['year'].astype('str').sort_values())\nplt.show()\n","9905e435":"df.isnull().sum()","b17b6fec":"df=df.dropna(subset=[\"model\",\"price\",\"year\"],axis=0)\ndf.head()\ndf.count()","bf7424f7":"df2 = df.copy()\ndf2 = df2[['model','year','transmission','mileage','fuelType','tax','mpg','engineSize','price']]\ndf2.head(3)","c2495f59":"data_audi_D = pd.get_dummies(df2)\ndata_audi_D.head(3)","8490a05f":"from sklearn.preprocessing import OrdinalEncoder\n\nencoder = OrdinalEncoder()\ndf['model_code'] = encoder.fit_transform(df[['model']])\ndf[['model','model_code']].head(10)","2292d51e":"df.head(10)","c577a405":"cat_cols = ['model', 'transmission', 'fuelType']","5120c9df":"from sklearn.preprocessing import OneHotEncoder\n\nOHE = OneHotEncoder(handle_unknown = 'ignore', sparse = False)\n\ndf_OHE = pd.DataFrame(OHE.fit_transform(df[cat_cols]))\n\ndf_OHE.index = df.index\n\ndf_num = df.drop(cat_cols, axis=1)\n\ndf_OHE_concat = pd.concat([df_num, df_OHE], axis=1)\n\n\n\n\ndf_OHE_concat.head()\n","0ef47203":"df_onehot = pd.get_dummies(df,columns=['model', 'transmission','fuelType'])","90ae0524":"df.info()","4a156f61":"df.iloc[:,8].unique()","c071ad8b":"df.head(10)","d94fe87e":"nom_cols = [1,5,8]#-------> OneHot Encoder\n\nord_cols = [0,9] #------> Ordinal Encoder\n\n\n","7668f306":"from sklearn.preprocessing import OneHotEncoder,OrdinalEncoder,PowerTransformer\nfrom sklearn.compose import make_column_transformer\n\ntransformer = make_column_transformer((OneHotEncoder(sparse=False, handle_unknown = 'ignore'),nom_cols),\n                                      (OrdinalEncoder(),ord_cols))\n\ntransformer","d8ac790f":"transformer.fit_transform(df)","d6504986":"from sklearn.linear_model import LinearRegression\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn import set_config\nset_config(display='diagram')\n\nmodel = LinearRegression()\n\npipe = make_pipeline(transformer, model)\n\npipe","f0d90715":"\n\nX = df_OHE_concat.drop('price', axis=1)\ny = df['price']\n\nfrom sklearn.model_selection import train_test_split\n\nX_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.3)\n\nprint(X_train.shape)\nprint(X_val.shape)\nprint(y_train.shape)\nprint(y_val.shape)\n\n","380f7454":"from sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import mean_absolute_error as mae\nlr = LinearRegression()\n\nlr.fit(X_train, y_train)\nlr_pred = lr.predict(X_val)\nlr_mae = mae(lr_pred, y_val)\nprint('MAE of Linear Regression is: {:,.0f}'.format(lr_mae))","14b07b07":"from sklearn.neighbors import KNeighborsRegressor\nfrom sklearn.model_selection import GridSearchCV\n\nknn = KNeighborsRegressor()\nparams = {'n_neighbors':[2,3,4,5,6,7,8,9]}\n\nmodel = GridSearchCV(knn, params, cv=5)\nmodel.fit(X_train, y_train)\nmodel.best_params_","2dd80638":"kn = KNeighborsRegressor(n_neighbors=4)\n\nkn.fit(X_train, y_train)\nkn_pred = kn.predict(X_val)\nkn_mae = mae(kn_pred, y_val)\nprint('MAE of KNeighbors is: {:,.0f}'.format(kn_mae))","bd3c8790":"\nprint(lr.score(X_val, y_val)*100)\nprint(kn.score(X_val, y_val)*100)","d6002364":"data_audi_D = pd.get_dummies(df2)\ndata_audi_D.head(3)","d8e1d1ed":"from sklearn.preprocessing import MinMaxScaler \nMinMaxScaler = MinMaxScaler() \ndata_audi_D_Scaled = MinMaxScaler.fit_transform(data_audi_D)\ndata_audi_D_Scaled = pd.DataFrame(data_audi_D_Scaled, columns = data_audi_D.columns)\ndata_audi_D_Scaled.head(3)","252bc4c6":"X_train, X_test, y_train, y_test = train_test_split(data_audi_D_Scaled.drop(columns = ['price']),\n                                                    data_audi_D_Scaled[['price']],\n                                                    test_size = 0.2, random_state = 0)\nprint(X_train.shape)\nprint(y_train.shape)\nprint(X_test.shape)\nprint(y_test.shape)","93eb4adb":"X = df_onehot.drop(['price'],axis=1)\ny = df_onehot['price']\nX_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.2,random_state=25)","ad7297ef":"from sklearn.feature_selection import SelectKBest, f_regression\ncolumn_names = data_audi_D_Scaled.drop(columns = ['price']).columns\n\nno_of_features = []\nr_squared_train = []\nr_squared_test = []\n\nfor k in range(3, 35, 2): # From 3 to 35 variables (every single one)\n    selector = SelectKBest(f_regression, k = k)\n    X_train_transformed = selector.fit_transform(X_train, y_train)\n    X_test_transformed = selector.transform(X_test)\n    regressor = LinearRegression()\n    regressor.fit(X_train_transformed, y_train)\n    no_of_features.append(k)\n    r_squared_train.append(regressor.score(X_train_transformed, y_train))\n    r_squared_test.append(regressor.score(X_test_transformed, y_test))\n    \nsns.lineplot(x = no_of_features, y = r_squared_train, legend = 'full')\nsns.lineplot(x = no_of_features, y = r_squared_test, legend = 'full')\nplt.show()","8d76182e":"selector = SelectKBest(f_regression, k = 27)\nX_train_transformed = selector.fit_transform(X_train, y_train)\nX_test_transformed = selector.transform(X_test)\ncolumn_names","0e20b894":"\nregressor = LinearRegression()\nregressor.fit(X_train,y_train)\nregressor.score(X,y)\nresults = X_test.copy()\nresults[\"predicted\"] = regressor.predict(X_test)\nresults[\"actual\"]= y_test\nresults = results[['predicted', 'actual']]\nresults['predicted'] = results['predicted'].round(2)\nresults","5c253e0f":"* We get a stable curve of 27, so that is the number of vairables we are using.","6a8b3114":"The dataset has:\n\n* 5 numerical columns: Price, mileage, tax, mpg and EngineSize.\n* 3 categorical columns: Model, transmission and FuelType\n* 1 date column: year","e23ef6eb":"* preparing data","8fed2d79":"* Linear Regression","14ce9b1b":"* Here Audi A3 model stands first in sales.","c48d493b":"**Ordinal Encoding**","5ea2a18a":"* Here the Manual models are the most selling ones.","17deab2c":" **To find which transmission models are sold the most**","53c3cffb":"* KNeighbours","3c984594":"# Analytics\n","98f2f22e":"**Which fuel type cars are more in demand ?.**","23540360":"\n\n* Since ther are 40 variables in the dataset after the one hot encoding, I am using SelectKBest option from sklearn to select the best features from the dataset for applying the regression.\n\n* For this, I am executing the SelectKBest() on f_regression by taking into consideration from 3 variables to 40 variables to see where we get the best score.\n","b23c8e6c":"# **IMPORTING DATA**","6570f685":"* # Linear Regression and KNeighbours score","6cabf957":"**To Plot price changes over the years with matplot.**","17e7f785":"# Feature selection","2f823b55":"* Here diesel fuel type cars are more in demand","9d481284":" **Which Year had the most sales ?.**","972b55b6":" **Which Engine size is more in demand ?.**","1cda5baf":"* Year 2019 had the most sales, 2020 it stumbled down","af7b175d":"* Here engineSize 2.0 is more in demand","e9629d6f":"* Selecting the best features with selectKbest and f_regression","d45155c5":"# Predicted and actual values","13c008b1":"# Selecting Algorithm and creating pipeline","9321c688":"# **DATA PRE PROCESSING**","54d87dd2":"* First we have to scale the values\n","3a187ec5":"# One hot encoder","02119a9e":" **Which model stands first in sales.**","8c825f5e":" **To find sum of NaN in the data set of each rows**\n*  **To Drop those NaN values**","a26cf0b3":"# Testing and splitting","75814e0e":" ****As we can see above,there are a lot of data from different data files\n*    >Each file represent different brand. But, when you see the location of these files, you can see that the name of the files are also the brand. \n*    >First we are comparring the prices of Audi cars according to their features      \n*    >First step to the process is the pandas for read command."}}