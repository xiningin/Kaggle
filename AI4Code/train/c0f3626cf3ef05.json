{"cell_type":{"2cea93ec":"code","d6bf913e":"code","fa54b0e0":"code","069cfb75":"code","e1c761a8":"code","a6422842":"code","ea2fd757":"code","087c5478":"code","8af128f0":"code","acbe55b2":"code","24fe0329":"code","614383a6":"code","f19e7ba7":"code","67452f04":"code","27c42adc":"code","d288e805":"code","4ad0f6e8":"code","14390fb1":"code","c76824de":"code","ddae11e6":"code","0e0b92ec":"code","c3fc9457":"code","454ff367":"code","aa89c05f":"code","d3589a79":"code","e74cb668":"code","87a64fe3":"code","62514fd8":"code","c0d1956c":"code","038ed6f2":"code","48434c29":"code","efb664f5":"code","320ad2f6":"code","10429e2b":"code","9880126a":"code","c116d4a2":"markdown","7e61ae99":"markdown","756b167c":"markdown","f1dd388c":"markdown","575628cc":"markdown","385c17db":"markdown","340063ea":"markdown","4453d689":"markdown","a2952fdc":"markdown","0a48a678":"markdown","d9562575":"markdown","0c94b865":"markdown","7930e23e":"markdown","b3a3cf81":"markdown","89f452ef":"markdown","c40e124c":"markdown","26e1589c":"markdown","1b4dd9d7":"markdown","21f5a6ae":"markdown","3f09e107":"markdown","55487074":"markdown","0d739b0d":"markdown","d69a4d35":"markdown","8ecfc75d":"markdown","3273c55c":"markdown","bda0b209":"markdown","6d8795b5":"markdown","6acde3fb":"markdown","ed318160":"markdown","f2ff14c8":"markdown","955cb725":"markdown","9e8c7d8a":"markdown","86c486e9":"markdown","49fe0a47":"markdown","6fec0929":"markdown"},"source":{"2cea93ec":"import numpy as np #used for mathematical calculations\r\nimport pandas as pd #used for data structuring and processing\r\nfrom sklearn.preprocessing import StandardScaler #will be used to normalize the input data for the AI (AI's like normalized data :))\r\nimport matplotlib.pyplot as plt # used for plotting data\r\nimport seaborn as sns # my perfered library for normalizing counts in bar plots\r\n","d6bf913e":"# Load the CSV files into Pandas data sets\ntrain_data = pd.read_csv(\"..\/input\/titanic\/train.csv\")\ntest_data = pd.read_csv(\"..\/input\/titanic\/test.csv\")","fa54b0e0":"train_data.head()","069cfb75":"test_data.head()","e1c761a8":"sns.barplot(x=\"Sex\",y=\"Survived\", data=train_data) #this type of plot gives survival rate normalized for each sex as a population","a6422842":"train_data[\"Age\"]=train_data[\"Age\"].fillna(-0.5) #fill na with -.5, I will deal with these later for the AI\nbins=[-1,0,5,12,18,24,35,60,np.inf] #setup the bins, basically defined by normal age groups\nlabels=['unknown','Baby','Child','Teenager','Student','Young Adult','Adult','Older'] #define X axis labels\ntrain_data['AgeGroup']=pd.cut(train_data[\"Age\"],bins,labels=labels) #Create new column AgeGroup but cutting and then insert the labels by Bin\n\n#now plot new column\nsns.barplot(x=\"AgeGroup\",y=\"Survived\",data=train_data)\nplt.show()","ea2fd757":"sns.barplot(x=\"Pclass\",y=\"Survived\",data=train_data)\r\nplt.show()","087c5478":"sns.barplot(x=\"SibSp\",y=\"Survived\",data=train_data)\r\nplt.show()","8af128f0":"bins=[0,10,20,30,40,50,60,np.inf] #setup the bins for fare\nlabels=[\"0\",\"10\",\"20\",\"30\",\"40\",\"50\",\">60\"]\ntrain_data['FareGroup']=pd.cut(train_data[\"Fare\"],bins,labels=labels) #Create new column FareGroup by cutting and then inserting the labels by Bin\nsns.barplot(x=\"FareGroup\",y=\"Survived\",data=train_data)\nplt.show()","acbe55b2":"print(\"Checking N\/A in the training data\")\ntrain_data.isna().sum()","24fe0329":"print(\"Checking N\/A in the test data\")\ntest_data.isna().sum()","614383a6":"#Drop the unwanted columns from the training data set\ntrain_data=train_data.drop(['PassengerId','Name','Ticket','Cabin','Embarked','AgeGroup','FareGroup'],axis=1)\ntrain_data.head()","f19e7ba7":"#Drop unwanted columns from the test data set\ntest_data=test_data.drop(['Name','Ticket','Cabin','Embarked'],axis=1)\ntest_data.head()","67452f04":"#first determine the median age from the given sets\n#check median age by sex\nmed_female=test_data.loc[(test_data['Sex']=='female'),'Age'].median()\nprint(med_female)\nmed_male=test_data.loc[(test_data['Sex']=='male'),'Age'].median()\nprint(med_male)\n","27c42adc":"#clean up the test data\ntest_data[\"Age\"]=test_data[\"Age\"].fillna(27)\n#clean up the training data\ntrain_data[\"Age\"]=train_data[\"Age\"].replace(-.5,27) #We grouped all the NAs as an age of -.5 earlier","d288e805":"# check for the median fare value\nmed_fare=test_data['Fare'].median()\nprint(med_fare)","4ad0f6e8":"# replace the missing test data fare with 14.45\ntest_data['Fare']=test_data['Fare'].fillna(14.45)\ntest_data.head()","14390fb1":"#swap male and female for 0 and 1 and inspect the data types to ensure inputs are numerical\ntrain_data['Sex']=train_data['Sex'].replace(['male','female'],[0,1])\ntrain_data.head()\nprint(\"Train Data \\n \\n\" + str(train_data.dtypes))\ntest_data['Sex']=test_data['Sex'].replace(['male','female'],[0,1])\nprint(\"Test Data \\n \\n\" + str(test_data.dtypes))","c76824de":"#for training, X is the selected features so we need to drop the predicted outcome, which will be the Y value for training purposes\nx_train=train_data.drop(['Survived'], axis=1)\ny_train=train_data['Survived']","ddae11e6":"#inspect x data, always good to double check\nx_train.head()","0e0b92ec":"#transform the y training data into a 1D array of the same shape as the X input data\ny_train=y_train.values.reshape(x_train.shape[0],1) #this step is important as it will reshape the Y values into a 1D array for the models input\n","c3fc9457":"sc=StandardScaler() #this function will scale the X features based on the max and min of the range\n# another option is to use the MinMaxScaler\nsc.fit(x_train)\nx=sc.transform(x_train)\n#\nprint(x)","454ff367":"class SkyNet():\n    '''\n    A two layer neural network\n    '''\n        # Here is where layers and nodes are initialized \n        #[number of inputs, number of perceptrons in the first layer, size of final layer]\n        #For this exersise the number of perceptons in the intermediate layer is the average of the input layer and output layer\n        # so we do a 6 4 1 layer setup\n    def __init__(self, layers=[6,4,1], learning_rate=0.001, iterations=100):\n        self.params = {}\n        self.learning_rate = learning_rate\n        self.iterations = iterations\n        self.loss = []\n        self.sample_size = None\n        self.layers = layers\n        self.X = None\n        self.y = None\n         \n        #in defining weights and biases we need to modify the below code based on the number of layers\n        # Since the input layer isn't part of the count, we have 2 weights and 2 bias values\n        # if we add another hidden layer then we need a W3 and b3\n        \n    def init_weights(self):\n        '''\n        Initialize the weights from a random normal distribution\n        '''\n        np.random.seed(1) # Seed the random number generator\n        self.params[\"W1\"] = np.random.randn(self.layers[0], self.layers[1]) #so we need 6 random weights for the 4 nodes in the first hidden layer \n        self.params['b1']  =np.random.randn(self.layers[1],) # A bias for each node on the first hidden layer\n        self.params['W2'] = np.random.randn(self.layers[1],self.layers[2]) \n        self.params['b2'] = np.random.randn(self.layers[2],)\n      \n    # this is the output layer function from each perceptetron\n    def relu(self,Z):\n        '''\n        The ReLu activation function is to performs a threshold\n        operation to each input element where values less \n        than zero are set to zero.\n        '''\n        return np.maximum(0,Z)\n    \n    # this is the derivative of the output layer function\n    def dRelu(self, x):\n        x[x<=0] = 0\n        x[x>0] = 1\n        return x\n    \n    def eta(self, x):\n      ETA = 0.0000000001\n      return np.maximum(x, ETA)\n\n    # This is the sigmoid function which is applied to our output layer\n    def sigmoid(self,Z):\n        '''\n        The sigmoid function takes in real numbers in any range and \n        squashes it to a real-valued output between 0 and 1.\n        '''\n        return 1\/(1+np.exp(-Z))\n\n    def entropy_loss(self,y, yhat):\n        nsample = len(y)\n        yhat_inv = 1.0 - yhat\n        y_inv = 1.0 - y\n        yhat = self.eta(yhat) ## clips value to avoid NaNs in log\n        yhat_inv = self.eta(yhat_inv) \n        loss = -1\/nsample * (np.sum(np.multiply(np.log(yhat), y) + np.multiply((y_inv), np.log(yhat_inv))))\n        return loss\n\n        # this will run our X input through the algorithm\n        #for each additional layer you need to add additional lines of code\n    def forward_propagation(self):\n        '''\n        Performs the forward propagation\n        '''\n        \n        Z1 = self.X.dot(self.params['W1']) + self.params['b1']\n        A1 = self.relu(Z1) #output of the layer\n        Z2 = A1.dot(self.params['W2']) + self.params['b2'] #if not the final layer then add a Z3 and A2\n        yhat = self.sigmoid(Z2) #final output layer result\n        loss = self.entropy_loss(self.y,yhat) #loss for back propigation\n\n        # save calculated parameters     \n        self.params['Z1'] = Z1\n        self.params['Z2'] = Z2\n        self.params['A1'] = A1\n\n        return yhat,loss\n    \n    # this will calulate the loss using the forward function derivative\n    # It will need to be updated based on used functions of the activation layer and number of layers\n    def back_propagation(self,yhat):\n        '''\n        Computes the derivatives and update weights and bias according.\n        '''\n        y_inv = 1 - self.y\n        yhat_inv = 1 - yhat\n\n        dl_wrt_yhat = np.divide(y_inv, self.eta(yhat_inv)) - np.divide(self.y, self.eta(yhat))\n        dl_wrt_sig = yhat * (yhat_inv)\n        dl_wrt_z2 = dl_wrt_yhat * dl_wrt_sig\n\n        dl_wrt_A1 = dl_wrt_z2.dot(self.params['W2'].T)\n        dl_wrt_w2 = self.params['A1'].T.dot(dl_wrt_z2)\n        dl_wrt_b2 = np.sum(dl_wrt_z2, axis=0, keepdims=True)\n\n        dl_wrt_z1 = dl_wrt_A1 * self.dRelu(self.params['Z1'])\n        dl_wrt_w1 = self.X.T.dot(dl_wrt_z1)\n        dl_wrt_b1 = np.sum(dl_wrt_z1, axis=0, keepdims=True)\n\n        #update the weights and bias\n        self.params['W1'] = self.params['W1'] - self.learning_rate * dl_wrt_w1\n        self.params['W2'] = self.params['W2'] - self.learning_rate * dl_wrt_w2\n        self.params['b1'] = self.params['b1'] - self.learning_rate * dl_wrt_b1\n        self.params['b2'] = self.params['b2'] - self.learning_rate * dl_wrt_b2\n\n    def fit(self, X, y):\n        '''\n        Trains the neural network using the specified data and labels\n        '''\n        self.X = X\n        self.y = y\n        self.init_weights() #initialize weights and bias\n\n\n        for i in range(self.iterations):\n            yhat, loss = self.forward_propagation()\n            self.back_propagation(yhat)\n            self.loss.append(loss)\n    \n    #This needs updated as layers are added\n    def predict(self, X):\n        '''\n        Predicts on a test data\n        '''\n        Z1 = X.dot(self.params['W1']) + self.params['b1']\n        A1 = self.relu(Z1)\n        Z2 = A1.dot(self.params['W2']) + self.params['b2']\n        pred = self.sigmoid(Z2)\n        return np.round(pred) \n\n    def acc(self, y, yhat):\n        '''\n        Calculates the accuracy between the predicted values and the truth labels\n        '''\n        acc = int(sum(y == yhat) \/ len(y) * 100)\n        return acc\n\n\n    def plot_loss(self):\n        '''\n        Plots the loss curve\n        '''\n        plt.plot(self.loss)\n        plt.xlabel(\"Iteration\")\n        plt.ylabel(\"logloss\")\n        plt.title(\"Loss curve for training\")\n        plt.show()  ","aa89c05f":"nn=SkyNet() #initialize\nnn.fit(x,y_train) #train the AI","d3589a79":"nn.plot_loss()","e74cb668":"train_pred=nn.predict(x)\nprint(\"Training Accuracy is {}\".format(nn.acc(y_train,train_pred)))","87a64fe3":"#First, drop the passengerId from the test data\ntest_data_Pid=list(test_data['PassengerId'])\ntest_data=test_data.drop(['PassengerId'], axis=1)\n#Standardize the test data input\nsc.fit(test_data)\nx_test=sc.transform(test_data)","62514fd8":"print(x_test) #double check that we have a clean normalized input\nsolution=nn.predict(x_test)# now give us a solution to the test data","c0d1956c":"test_data_Pid=np.array(test_data_Pid) #for the final output we need to make a 1D array for passenger ID that will combine with Survived\ntest_data_Pid=test_data_Pid.ravel() #this step will format the column into the proper 1D array for the Pandas library\nsolution=np.array(solution)\nsolution=solution.ravel()\nsolution_1=solution.astype(int) # switch to integer or the submission will fail\noutput = pd.DataFrame({'PassengerId': test_data_Pid, 'Survived': solution_1})\nprint(output)","038ed6f2":"output.to_csv('submission.csv', index=False)\npd.read_csv('submission.csv')","48434c29":"# Import the needed libaries\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, Activation\nfrom tensorflow.keras.optimizers import Adam\n","efb664f5":"# Create the model\nKmodel=Sequential()","320ad2f6":"# Create the hidden layer and the output layer\nKmodel.add(Dense(4, input_shape=(6,))) # This is the 4 node, 6 input setup used in the above network\nKmodel.add(Dense(1, activation='sigmoid')) # Create the finl output layer.\n\n# compile the model using crossentropy and accuray for metrics to be consistent with the above model.\n\nopt=Adam(learning_rate=.001)\nKmodel.compile(optimizer=opt, loss=['binary_crossentropy'], metrics=['accuracy']) \n","10429e2b":"# fit and train the model\nKmodel.fit(x,y_train, epochs=100, verbose=0)\n","9880126a":"# evaluate the training accuracy of the model\ntrain_acc = Kmodel.evaluate(x, y_train, verbose=0)[1]\nprint(\"Train accuracy of keras neural network: {}\".format(round((train_acc * 100), 2)))","c116d4a2":"<H2 align=\"center\">4. Modify the data sets for input into the AI<H2>","7e61ae99":"<H3 align=\"center\"> Load the CSV files using Pandas for Data Structure <\/H2>","756b167c":"### It looks like family size is a good feature to include as well.\n\n<H2 align=\"center\"> Checking Fare ranges <\/H2>","f1dd388c":"So I now have confirmed that all the features are numerical. The next step is to prepare the data into the proper X and Y formats needed by the AI.\nFirst we need to seperate the X and Y values from the training data. The Y value is the target or prediction as a function of the input features or X. In the case of the training set, the Y is a known value. It is also important to format the X input and Y training data as simple arrays and the X input data should be scaled prior to fitting and predicting. ","575628cc":"#### The self implemented Neural Network model has a better training accuracy than the keras model. How well it stacks up on the test data is a good indicator on performance. ","385c17db":"<H2 align=\"center\"> Checking Family size <\/H2>","340063ea":"#### Now fix the missing fare value in the training data","4453d689":"### Higher fare rates have a better survival rate as well so it will be included as an input\n\n<H2 align=\"center\"> Next I will inspect data integrity <\/H2>","a2952fdc":"First will be the basic inspection of the datasets using .head() ","0a48a678":"#### Now its time to make a prediction using the test data set.","d9562575":"### We can see there is a signifacant correlation between sex and survival, so I will include sex as an input to our AI\n<H3 align=\"center\"> Next lets check age groupings..... <\/H3>","0c94b865":"<H3 align=\"center\"> Now we have a dataset with the features I want to input into the AI. However, the AI only takes mathematical inputs and not text. So it is neccesarry to modify the Sex column from text to a binary numerical value. <H3>","7930e23e":"Inspection reveals that the test data is missing the Survived column which is expected since the test data is what we will use to make our final prediction. We will take this data set and feed it to the AI to determine who will and will not survive. We also want to determine what data will give us a good input to train and predict results. To accomplish this, I will use exploratory data analys to examine the features that I believe to be most relevant","b3a3cf81":"<H3 align=\"center\">Y = f(x) <H3>","89f452ef":"### First class passengers seem to do better than 3rd class (no surprise) so I'll add this as an input to the AI\n","c40e124c":"<H3 align=\"left\"> I am going to make a set of histograms as a method of inspection between various features and survival <\/H3>\n<H3 align=\"center\"> First will be sex vs. survival <\/H3>\n","26e1589c":"<H2 align = \"center\"> Preface <\/H2>\n\nThis is my first attempt at performing a machine learning operation on a dataset. I am new to the area of Data Analytics and Data Science but am an engineer that is familiar with the mathematical concepts behind the analysis. My coding skills in python are at a novice level but in the past have worked in C++ and Java. I also wanted to develop a better understanding of how a Neural Network functions on a fundamental level. So I chose to use a line by line implementation published by  <a href=\"(http:\/\/heartbeat.comet.ml\/building-a-neural-network-from-scratch-using-python-part-1-6d399df8d432\"> Rising Odegua Medium at heartbeat.comet.ml<\/a>","1b4dd9d7":"#### Since a lot of data is missing in the Cabin column, I'll drop that one. I will also drop ticket numbers, names, and embarked since they have less relevancy to survival.  Finally, I will want to replace the unknown ages with the median age of the set so that we can use age as an input and fix the missing fare value in the test data.","21f5a6ae":"<H2 align=\"center\"> 2. Determine what features will be inputs to the AI using EDA","3f09e107":"<H2 align=\"center\"> Step 1. Load the needed libraries <\/H2>","55487074":"#### First, as part of cleaning the data, we need to deal with the missing ages in both sets. A typical approach would be to find the median age and then use that to replace the NAs. It will also be neccessary to convert the Sex column into a binary set.","0d739b0d":"#### Now all the features are numeric","d69a4d35":"### So by visual inspection, Teens and under have a slightly better chance to survive which seems like a relevant input\n","8ecfc75d":"#### Next we can check the loss plot from the learning session and calulate the accuracy of the predictions.","3273c55c":"<H2 align=\"center\">3. Clean and modify the data<\/H2>","bda0b209":"<figure class=\"content-lead-image grid-twelve large-grid-eleven\">\n  <div class=\"placeholder-container\" style=\"--aspect-ratio-percent:75.05399568034558%;background-color:#2c354b\"><img alt=\"A 3D digital illustration of connected points in the shape of a brain.\" class=\"lazyloaded\" src=\"https:\/\/images.theconversation.com\/files\/374303\/original\/file-20201210-18-elk4m.jpg?ixlib=rb-1.1.0&amp;rect=0%2C22%2C7500%2C5591&amp;q=45&amp;auto=format&amp;w=926&amp;fit=clip\" data-id=\"374303\" itemprop=\"image\"><\/div>\n    <figcaption>\n      Neural networks try to simulate the brain by processing data through layers of artificial neurons.\n      <span class=\"attribution\"><a class=\"source\" href=\"https:\/\/www.gettyimages.com\/detail\/photo\/artificial-intelligence-concept-royalty-free-image\/1186776025\">MF3d \/ E+ via Getty Images<\/a><\/span>\n    <\/figcaption>\n<\/figure>","6d8795b5":"<H1 align=\"center\">Titanic Challenge submission from a novice<\/H1>\n\n<H2 align = \"center\">\n<img src=\"https:\/\/c8.alamy.com\/comp\/2C21APK\/titanic-1912-vintage-advertising-poster-white-star-line-titanic-olympic-cruise-ship-steamers-poster-the-largest-steamers-in-the-world-olympic-45000-tons-titanic-45000-tons-to-new-york-from-southampton-cherbourg-queenstown-to-boston-from-liverpool-queenstown-freight-and-passage-apply-thomas-cook-son-2C21APK.jpg\" width=\"300\" height=\"400\" align=\"middle\">\n<\/H2>\n","6acde3fb":"#### Now we format the results for submission","ed318160":"<H2 align=\"center\"> How about passenger class?<\/H2>","f2ff14c8":"It looks like 27 is a good choice to replace the missing ages in both data sets","955cb725":"<H2 align=\"center\"> Step 5. Define the neural network algorithm <\/H2>\n<H3 align=\"center\"> This code comes from Rising Odegua at heartbeat.comet.ml. I modified it slightly to accomidate my feature numbers. I also added some extra comments.<\/H3>\n<div class=\"ephox-summary-card\" style=\"max-width: 650px;\" data-ephox-embed-iri=\"https:\/\/heartbeat.comet.ml\/building-a-neural-network-from-scratch-using-python-part-1-6d399df8d432\"><a class=\"ephox-summary-card-link-thumbnail\" href=\"https:\/\/heartbeat.comet.ml\/building-a-neural-network-from-scratch-using-python-part-1-6d399df8d432\"> <img class=\"ephox-summary-card-thumbnail\" src=\"https:\/\/miro.medium.com\/max\/1200\/1*NUil-wyDtAKmoD-HjWXBLw.jpeg\" \/> <\/a> <a class=\"ephox-summary-card-link\" href=\"https:\/\/heartbeat.comet.ml\/building-a-neural-network-from-scratch-using-python-part-1-6d399df8d432\"> <span class=\"ephox-summary-card-title\">Building a Neural Network From Scratch Using Python (Part 1)<\/span> <span class=\"ephox-summary-card-description\">Write every line of code and understand why it works<\/span> <span class=\"ephox-summary-card-author\">Rising Odegua<\/span> <span class=\"ephox-summary-card-website\">Medium<\/span> <\/a><\/div>\n\nBasic Neural Network Model for this exercise:\n1. There are 6 features or inputs to the input Layer\n2. We will standardize the training data set first\n3. We will use one input layer, one hidden layer, and one output layer\n4. We will use ReLU as an activation function for our network hidden layer\n5. We will use the sigmoid Function for the output layer. This is becuase we are looking for a binary output or probability between 0 and 1.\n6. For the loss function for back propigation we will use the cross entropy function, this function work well for the simple classification of survived or not survived.","9e8c7d8a":"<H2 align=\"center\"> Normalize the input (X) ","86c486e9":"<H2 align=\"center\"> My strategy <\/H2>\n\nThe goal of this challenge is to train a simple AI to make a prediction of survived or did not survive from the Titanic Disaster after using a sample set for training.\n\nBasic outline of my process is as follows:\n1. Load up the needed Python libraries for the analysis.\n2. From the training data set, determine which attributes or \"Features\" have a good correlation to surviving the Titanic Disaster.\n3. Clean or modify the test data to use only the relevant features for training the AI (Neural network).\n4. Modify the training and test data sets for insertation in to the AI for learning, this includes formatting text into numerical types (The AI wants only numbers as an input) and properly formatting the X (input) and Y (prediction) values.\n5. Define an algorithm for the desired neural network that will learn and then predict.\n6. Train the defined AI and then ask the AI to give a prediction from the test set.\n7. Compare results to keras tensor algorithm.","49fe0a47":"<H2 align=\"center\"> 7. Make a Comparison prediction with Keras from Tensorflow<\/H2>\n\n#### We want the Keras model to be similar to the above defined Neural Network. So for attributes we will use:\n\n- We will use the Sequential model vice the functional model.\n- We will use one input layer, one hiden layer, and one output layer.\n- We will use ReLU as an activation function for our network hidden layer.\n- We will use the sigmoid Function for the output layer. \n- We will use the Adam optimizer since it is a common and reliable method \n  See the following article for a good description (https:\/\/machinelearningmastery.com\/adam-optimization-algorithm-for-deep-learning\/)\n","6fec0929":"<H2 align=\"center\"> 6. Train and test the model to make a prediction<\/H2>"}}