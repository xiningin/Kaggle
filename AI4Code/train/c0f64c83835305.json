{"cell_type":{"b5765a9b":"code","f50cc743":"code","485824e0":"code","1fcbddb0":"code","7141924e":"code","524fc0e0":"code","48e68774":"code","27055ec4":"code","2355b2d4":"code","a2b8ba9c":"code","14629145":"code","537b1c6b":"code","b53d3a43":"code","840cd7f8":"code","97fb31e5":"code","b9942415":"code","c10c48ce":"code","48c0bbb6":"code","b199abec":"code","af582839":"code","f099339c":"code","fdc55b97":"code","06cf1f7b":"code","e89584d7":"code","41ab5c61":"code","43b6dba5":"code","5c6168f9":"code","1bc20e33":"code","9b494672":"code","bfc96ff6":"code","0254317a":"code","fc11c331":"code","947124e3":"code","e6be61b6":"code","d98277c4":"code","30925628":"code","6b33de04":"code","43544f7d":"code","ae7df247":"code","b074531d":"code","e2dac612":"code","f9982cbc":"code","afb4a127":"code","5abe3d4b":"code","7c58ab92":"code","915a961e":"code","7517dff3":"code","da4df911":"code","c9a5d264":"code","93a7cbed":"markdown","4882d556":"markdown","c7557f9a":"markdown","941f6f82":"markdown","f431831f":"markdown","08b4c9d0":"markdown","32fdca88":"markdown","7864d3bf":"markdown","59ba9e7a":"markdown","be66f9c9":"markdown","0abd5c4a":"markdown","d435ea3b":"markdown","3c9f12c5":"markdown","5499627e":"markdown","75d26916":"markdown","2759bf27":"markdown","386fc044":"markdown","32e7b693":"markdown","1fae2886":"markdown","c962e99e":"markdown","b8ed02f6":"markdown","88b8af17":"markdown","4ef02ba6":"markdown","867ab105":"markdown","a26c3d25":"markdown","0f4f2cb6":"markdown","05131b9f":"markdown","24023c34":"markdown","871ed40e":"markdown","54248bf2":"markdown","847188ba":"markdown","46c0392c":"markdown","72bc20d3":"markdown"},"source":{"b5765a9b":"! wget https:\/\/raw.githubusercontent.com\/hse-aml\/natural-language-processing\/master\/setup_google_colab.py -O setup_google_colab.py\nimport setup_google_colab\n# please, uncomment the week you're working on\nsetup_google_colab.setup_week1()  \n# setup_google_colab.setup_week2()\n# setup_google_colab.setup_week3()\n# setup_google_colab.setup_week4()\n# setup_google_colab.setup_project()\n# setup_google_colab.setup_honor()","f50cc743":"import sys\nsys.path.append(\"..\")\nfrom common.download_utils import download_week1_resources\ndownload_week1_resources()","485824e0":"from grader import Grader","1fcbddb0":"grader = Grader()","7141924e":"import nltk\nnltk.download('stopwords')\nfrom nltk.corpus import stopwords","524fc0e0":"from ast import literal_eval\nimport pandas as pd\nimport numpy as np","48e68774":"def read_data(filename):\n    data = pd.read_csv(filename, sep='\\t')\n    data['tags'] = data['tags'].apply(literal_eval)\n    return data","27055ec4":"train = read_data('\/kaggle\/working\/data\/train.tsv')\nvalidation = read_data('\/kaggle\/working\/data\/validation.tsv')\ntest = pd.read_csv('\/kaggle\/working\/data\/test.tsv', sep='\\t')","2355b2d4":"train.head()","a2b8ba9c":"X_train, y_train = train['title'].values, train['tags'].values\nX_val, y_val = validation['title'].values, validation['tags'].values\nX_test = test['title'].values","14629145":"import re","537b1c6b":"REPLACE_BY_SPACE_RE = re.compile('[\/(){}\\[\\]\\|@,;]')\nBAD_SYMBOLS_RE = re.compile('[^0-9a-z #+_]')\nSTOPWORDS = set(stopwords.words('english'))\n\ndef text_prepare(text):\n    \"\"\"\n        text: a string\n        \n        return: modified initial string\n    \"\"\"\n    text = text.lower()\n    text = re.sub(REPLACE_BY_SPACE_RE,\" \",text)\n    text = re.sub(BAD_SYMBOLS_RE,\"\",text)\n    # text = # delete symbols which are in BAD_SYMBOLS_RE from text\n    # text = # delete stopwords from text\n    # for word in STOPWORDS:\n    text = text.split()\n    text = ' '.join(word for word in text if word not in STOPWORDS)\n    return text","b53d3a43":"text_prepare(\"all isGSJEGJ mera naam amogh hai ]]]\")","840cd7f8":"def test_text_prepare():\n    examples = [\"SQL Server - any equivalent of Excel's CHOOSE function?\",\n                \"How to free c++ memory vector<int> * arr?\"]\n    answers = [\"sql server equivalent excels choose function\", \n               \"free c++ memory vectorint arr\"]\n    for ex, ans in zip(examples, answers):\n        if text_prepare(ex) != ans:\n            return \"Wrong answer for the case: '%s'\" % ex\n    return 'Basic tests are passed.'","97fb31e5":"print(test_text_prepare())","b9942415":"prepared_questions = []\nfor line in open('data\/text_prepare_tests.tsv', encoding='utf-8'):\n    line = text_prepare(line.strip())\n    prepared_questions.append(line)\ntext_prepare_results = '\\n'.join(prepared_questions)\n\ngrader.submit_tag('TextPrepare', text_prepare_results)","c10c48ce":"X_train = [text_prepare(x) for x in X_train]\nX_val = [text_prepare(x) for x in X_val]\nX_test = [text_prepare(x) for x in X_test]","48c0bbb6":"X_train","b199abec":"# Dictionary of all tags from train corpus with their counts.\ntags_counts = {}\n# Dictionary of all words from train corpus with their counts.\nwords_counts = {}\n\nfor l in X_train:\n    for word in l.split():\n        curr = words_counts.pop(word,0)\n        words_counts[word] = curr+1\nfor j in y_train:\n    for tag in j:\n        curr = tags_counts.pop(tag,0)\n        tags_counts[tag] = curr+1\n\n######################################\n######### YOUR CODE HERE #############\n######################################","af582839":"sorted(tags_counts.items(), key=lambda item: item[1], reverse=True)[:3]","f099339c":"most_common_tags = sorted(tags_counts.items(), key=lambda x: x[1], reverse=True)[:3]\nmost_common_words = sorted(words_counts.items(), key=lambda x: x[1], reverse=True)[:3]\n\ngrader.submit_tag('WordsTagsCount', '%s\\n%s' % (','.join(tag for tag, _ in most_common_tags), \n                                                ','.join(word for word, _ in most_common_words)))","fdc55b97":"most_common_tags","06cf1f7b":"most_common_words","e89584d7":"DICT_SIZE = 5000\nWORDS_TO_INDEX = sorted(words_counts.items(), key=lambda x: x[1], reverse=True)[:DICT_SIZE]\nw = {}\nind_w = {}\nfor i in range(len(WORDS_TO_INDEX)):\n    w[WORDS_TO_INDEX[i][0]] = i\n    ind_w[i] = WORDS_TO_INDEX[i][0]","41ab5c61":"# DICT_SIZE = 5000\nWORDS_TO_INDEX = w \nINDEX_TO_WORDS = ind_w\nALL_WORDS = WORDS_TO_INDEX.keys()\n\ndef my_bag_of_words(text, words_to_index, dict_size):\n    \"\"\"\n        text: a string\n        dict_size: size of the dictionary\n        \n        return a vector which is a bag-of-words representation of 'text'\n    \"\"\"\n    result_vector = np.zeros(dict_size)\n    vec = text.split()\n    for word in vec:\n#         print(word,result_vector)\n        if word in words_to_index:\n            result_vector[words_to_index[word]] = 1\n    return result_vector","43b6dba5":"def test_my_bag_of_words():\n    words_to_index = {'hi': 0, 'you': 1, 'me': 2, 'are': 3}\n    examples = ['hi how are you']\n    answers = [[1, 1, 0, 1]]\n    for ex, ans in zip(examples, answers):\n        if (my_bag_of_words(ex, words_to_index, 4) != ans).any():\n            return \"Wrong answer for the case: '%s'\" % ex\n    return 'Basic tests are passed.'","5c6168f9":"print(test_my_bag_of_words())","1bc20e33":"from scipy import sparse as sp_sparse","9b494672":"X_train_mybag = sp_sparse.vstack([sp_sparse.csr_matrix(my_bag_of_words(text, WORDS_TO_INDEX, DICT_SIZE)) for text in X_train])\nX_val_mybag = sp_sparse.vstack([sp_sparse.csr_matrix(my_bag_of_words(text, WORDS_TO_INDEX, DICT_SIZE)) for text in X_val])\nX_test_mybag = sp_sparse.vstack([sp_sparse.csr_matrix(my_bag_of_words(text, WORDS_TO_INDEX, DICT_SIZE)) for text in X_test])\nprint('X_train shape ', X_train_mybag.shape)\nprint('X_val shape ', X_val_mybag.shape)\nprint('X_test shape ', X_test_mybag.shape)","bfc96ff6":"row = X_train_mybag[10].toarray()[0]\n# count = \nnon_zero_elements_count = int(sum(row)) \n\ngrader.submit_tag('BagOfWords', str(non_zero_elements_count))","0254317a":"from sklearn.feature_extraction.text import TfidfVectorizer","fc11c331":"def tfidf_features(X_train, X_val, X_test):\n    \"\"\"\n        X_train, X_val, X_test \u2014 samples        \n        return TF-IDF vectorized representation of each sample and vocabulary\n    \"\"\"\n    # Create TF-IDF vectorizer with a proper parameters choice\n    # Fit the vectorizer on the train set\n    # Transform the train, test, and val sets and return the result\n    \n    \n    tfidf_vectorizer = TfidfVectorizer( token_pattern=\"(\\S+)\",min_df = 5, max_df = .9, ngram_range=(1,2))\n\n    X_train = tfidf_vectorizer.fit_transform(X_train)\n    X_val = tfidf_vectorizer.transform(X_val)\n    X_test = tfidf_vectorizer.transform(X_test)\n    return X_train, X_val, X_test, tfidf_vectorizer.vocabulary_","947124e3":"X_train_tfidf, X_val_tfidf, X_test_tfidf, tfidf_vocab = tfidf_features(X_train, X_val, X_test)\ntfidf_reversed_vocab = {i:word for word,i in tfidf_vocab.items()}","e6be61b6":"for word in [\"c++\", \"c#\"]:\n    if word in tfidf_vocab:\n        print(True)\n    else:\n        print(False)","d98277c4":"from sklearn.preprocessing import MultiLabelBinarizer","30925628":"mlb = MultiLabelBinarizer(classes=sorted(tags_counts.keys()))\ny_train = mlb.fit_transform(y_train)\ny_val = mlb.fit_transform(y_val)","6b33de04":"from sklearn.multiclass import OneVsRestClassifier\nfrom sklearn.linear_model import LogisticRegression, RidgeClassifier","43544f7d":"def train_classifier(X_train, y_train):\n    \"\"\"\n      X_train, y_train \u2014 training data\n      \n      return: trained classifier\n    \"\"\"\n    logreg = LogisticRegression()\n    one_v_rest = OneVsRestClassifier(LogisticRegression(max_iter=2000)).fit(X_train,y_train)\n#     onev1 = OneVsRestClassifier(RidgeClassifier()).fit(X_train,y_train)\n    return one_v_rest","ae7df247":"classifier_mybag = train_classifier(X_train_mybag, y_train)\nclassifier_tfidf = train_classifier(X_train_tfidf, y_train)","b074531d":"y_val_predicted_labels_mybag = classifier_mybag.predict(X_val_mybag)\ny_val_predicted_scores_mybag = classifier_mybag.decision_function(X_val_mybag)\n\ny_val_predicted_labels_tfidf = classifier_tfidf.predict(X_val_tfidf)\ny_val_predicted_scores_tfidf = classifier_tfidf.decision_function(X_val_tfidf)","e2dac612":"y_val_pred_inversed = mlb.inverse_transform(y_val_predicted_labels_tfidf)\ny_val_inversed = mlb.inverse_transform(y_val)\nfor i in range(3):\n    print('Title:\\t{}\\nTrue labels:\\t{}\\nPredicted labels:\\t{}\\n\\n'.format(\n        X_val[i],\n        ','.join(y_val_inversed[i]),\n        ','.join(y_val_pred_inversed[i])\n    ))","f9982cbc":"from sklearn.metrics import accuracy_score\nfrom sklearn.metrics import f1_score\nfrom sklearn.metrics import roc_auc_score \nfrom sklearn.metrics import average_precision_score\nfrom sklearn.metrics import recall_score\n","afb4a127":"def print_evaluation_scores(y_val, predicted):\n    print(\"Accuracy score :\",accuracy_score(y_true=y_val,y_pred=predicted)) \n    print(\"F1 averaged score :\",np.mean(np.array(f1_score(y_true=y_val, y_pred=predicted,average = None))))\n    print(\"Precision score :\",average_precision_score(y_true=y_val, y_score=predicted))","5abe3d4b":"print('Bag-of-words')\nprint_evaluation_scores(y_val, y_val_predicted_labels_mybag)\nprint('Tfidf')\nprint_evaluation_scores(y_val, y_val_predicted_labels_tfidf)","7c58ab92":"from metrics import roc_auc\n%matplotlib inline","915a961e":"n_classes = len(tags_counts)\nroc_auc(y_val, y_val_predicted_scores_mybag, n_classes)","7517dff3":"n_classes = len(tags_counts)\nroc_auc(y_val, y_val_predicted_scores_tfidf, n_classes)","da4df911":"# classifier_mybag = train_classifier(X_train_mybag, y_train)\n# classifier_tfidf = train_classifier(X_train_tfidf, y_train)\n\ny_val_predicted_labels_mybag = classifier_mybag.predict(X_val_mybag)\ny_val_predicted_scores_mybag = classifier_mybag.decision_function(X_val_mybag)\n\ny_val_predicted_labels_tfidf = classifier_tfidf.predict(X_val_tfidf)\ny_val_predicted_scores_tfidf = classifier_tfidf.decision_function(X_val_tfidf)","c9a5d264":"test_predictions = classifier_tfidf.predict(X_test_tfidf)\ntest_pred_inversed = mlb.inverse_transform(test_predictions)\n\ntest_predictions_for_submission = '\\n'.join('%i\\t%s' % (i, ','.join(row)) for i, row in enumerate(test_pred_inversed))\ngrader.submit_tag('MultilabelClassification', test_predictions_for_submission)","93a7cbed":"Implement the function *print_evaluation_scores* which calculates and prints to stdout:\n - *accuracy*\n - *F1-score macro\/micro\/weighted*\n - *Precision macro\/micro\/weighted*","4882d556":"Implement the function *train_classifier* for training a classifier. In this task we suggest to use One-vs-Rest approach, which is implemented in [OneVsRestClassifier](http:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.multiclass.OneVsRestClassifier.html) class. In this approach *k* classifiers (= number of tags) are trained. As a basic classifier, use [LogisticRegression](http:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.linear_model.LogisticRegression.html). It is one of the simplest methods, but often it performs good enough in text classification tasks. It might take some time, because a number of classifiers to train is large.","c7557f9a":"As you might notice, we transform the data to sparse representation, to store the useful information efficiently. There are many [types](https:\/\/docs.scipy.org\/doc\/scipy\/reference\/sparse.html) of such representations, however sklearn algorithms can work only with [csr](https:\/\/docs.scipy.org\/doc\/scipy\/reference\/generated\/scipy.sparse.csr_matrix.html#scipy.sparse.csr_matrix) matrix, so we will use this one.","941f6f82":"You might also want to plot some generalization of the [ROC curve](http:\/\/scikit-learn.org\/stable\/modules\/model_evaluation.html#receiver-operating-characteristic-roc) for the case of multi-label classification. Provided function *roc_auc* can make it for you. The input parameters of this function are:\n - true labels\n - decision functions scores\n - number of classes","f431831f":"Train the classifiers for different data transformations: *bag-of-words* and *tf-idf*.","08b4c9d0":"In this assignment you will learn how to predict tags for posts from [StackOverflow](https:\/\/stackoverflow.com). To solve this task you will use multilabel classification approach.\n\n### Libraries\n\nIn this task you will need the following libraries:\n- [Numpy](http:\/\/www.numpy.org) \u2014 a package for scientific computing.\n- [Pandas](https:\/\/pandas.pydata.org) \u2014 a library providing high-performance, easy-to-use data structures and data analysis tools for the Python\n- [scikit-learn](http:\/\/scikit-learn.org\/stable\/index.html) \u2014 a tool for data mining and data analysis.\n- [NLTK](http:\/\/www.nltk.org) \u2014 a platform to work with natural language.","32fdca88":"**Task 3 (BagOfWords).** For the 11th row in *X_train_mybag* find how many non-zero elements it has. In this task the answer (variable *non_zero_elements_count*) should be an integer number, e.g. 20.","7864d3bf":"**Task 4 (MultilabelClassification).** Once we have the evaluation set up, we suggest that you experiment a bit with training your classifiers. We will use *F1-score weighted* as an evaluation metric. Our recommendation:\n- compare the quality of the bag-of-words and TF-IDF approaches and chose one of them.\n- for the chosen one, try *L1* and *L2*-regularization techniques in Logistic Regression with different coefficients (e.g. C equal to 0.1, 1, 10, 100).\n\nYou also could try other improvements of the preprocessing \/ model, if you want. ","59ba9e7a":"When you are happy with the quality, create predictions for *test* set, which you will submit to Coursera.","be66f9c9":"# Predict tags on StackOverflow with linear models","0abd5c4a":"Now apply the implemented function to all samples (this might take up to a minute):","d435ea3b":"For this and most of the following assignments you will need to use a list of stop words. It can be downloaded from *nltk*:","3c9f12c5":"### MultiLabel classifier\n\nAs we have noticed before, in this task each example can have multiple tags. To deal with such kind of prediction, we need to transform labels in a binary form and the prediction will be a mask of 0s and 1s. For this purpose it is convenient to use [MultiLabelBinarizer](http:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.preprocessing.MultiLabelBinarizer.html) from *sklearn*.","5499627e":"### Text preprocessing","75d26916":"In this task you will deal with a dataset of post titles from StackOverflow. You are provided a split to 3 sets: *train*, *validation* and *test*. All corpora (except for *test*) contain titles of the posts and corresponding tags (100 tags are available). The *test* set is provided for Coursera's grading and doesn't contain answers. Upload the corpora using *pandas* and look at the data:","2759bf27":"Now we can preprocess the titles using function *text_prepare* and  making sure that the headers don't have bad symbols:","386fc044":"Once you have done text preprocessing, always have a look at the results. Be very careful at this step, because the performance of future models will drastically depend on it. \n\nIn this case, check whether you have c++ or c# in your vocabulary, as they are obviously important tokens in our tags prediction task:","32e7b693":"### Grading\nWe will create a grader instance below and use it to collect your answers. Note that these outputs will be stored locally inside grader and will be uploaded to platform only after running submitting function in the last part of this assignment. If you want to make partial submission, you can run that cell any time you want.","1fae2886":"For a more comfortable usage, initialize *X_train*, *X_val*, *X_test*, *y_train*, *y_val*.","c962e99e":"Run your implementation for questions from file *text_prepare_tests.tsv* to earn the points.","b8ed02f6":"We are assuming that *tags_counts* and *words_counts* are dictionaries like `{'some_word_or_tag': frequency}`. After applying the sorting procedure, results will be look like this: `[('most_popular_word_or_tag', frequency), ('less_popular_word_or_tag', frequency), ...]`. The grader gets the results in the following format (two comma-separated strings with line break):\n\n    tag1,tag2,tag3\n    word1,word2,word3\n\nPay attention that in this assignment you should not submit frequencies or some additional information.","88b8af17":"Now, use this transormation for the data and check again.","4ef02ba6":"Now take a look at how classifier, which uses TF-IDF, works for a few examples:","867ab105":"#### TF-IDF\n\nThe second approach extends the bag-of-words framework by taking into account total frequencies of words in the corpora. It helps to penalize too frequent words and provide better features space. \n\nImplement function *tfidf_features* using class [TfidfVectorizer](http:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.feature_extraction.text.TfidfVectorizer.html) from *scikit-learn*. Use *train* corpus to train a vectorizer. Don't forget to take a look into the arguments that you can pass to it. We suggest that you filter out too rare words (occur less than in 5 titles) and too frequent words (occur more than in 90% of the titles). Also, use bigrams along with unigrams in your vocabulary. ","a26c3d25":"### Evaluation\n\nTo evaluate the results we will use several classification metrics:\n - [Accuracy](http:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.metrics.accuracy_score.html)\n - [F1-score](http:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.metrics.f1_score.html)\n - [Area under ROC-curve](http:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.metrics.roc_auc_score.html)\n - [Area under precision-recall curve](http:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.metrics.average_precision_score.html#sklearn.metrics.average_precision_score) \n \nMake sure you are familiar with all of them. How would you expect the things work for the multi-label scenario? Read about micro\/macro\/weighted averaging following the sklearn links provided above.","0f4f2cb6":"Now you can create predictions for the data. You will need two types of predictions: labels and scores.","05131b9f":"If you can't find it, we need to understand how did it happen that we lost them? It happened during the built-in tokenization of TfidfVectorizer. Luckily, we can influence on this process. Get back to the function above and use '(\\S+)' regexp as a *token_pattern* in the constructor of the vectorizer.  ","24023c34":"One of the most known difficulties when working with natural data is that it's unstructured. For example, if you use it \"as is\" and extract tokens just by splitting the titles by whitespaces, you will see that there are many \"weird\" tokens like *3.5?*, *\"Flip*, etc. To prevent the problems, it's usually useful to prepare the data somehow. In this task you'll write a function, which will be also used in the other assignments. \n\n**Task 1 (TextPrepare).** Implement the function *text_prepare* following the instructions. After that, run the function *test_text_prepare* to test it on tiny cases and submit it to Coursera.","871ed40e":"For each tag and for each word calculate how many times they occur in the train corpus. \n\n**Task 2 (WordsTagsCount).** Find 3 most popular tags and 3 most popular words in the train data and submit the results to earn the points.","54248bf2":"Now, we would need to compare the results of different predictions, e.g. to see whether TF-IDF transformation helps or to try different regularization techniques in logistic regression. For all these experiments, we need to setup evaluation procedure. ","847188ba":"As you can see, *title* column contains titles of the posts and *tags* column contains the tags. It could be noticed that a number of tags for a post is not fixed and could be as many as necessary.","46c0392c":"### Transforming text to a vector\n\nMachine Learning algorithms work with numeric data and we cannot use the provided text data \"as is\". There are many ways to transform text data to numeric vectors. In this task you will try to use two of them.\n\n#### Bag of words\n\nOne of the well-known approaches is a *bag-of-words* representation. To create this transformation, follow the steps:\n1. Find *N* most popular words in train corpus and numerate them. Now we have a dictionary of the most popular words.\n2. For each title in the corpora create a zero vector with the dimension equals to *N*.\n3. For each text in the corpora iterate over words which are in the dictionary and increase by 1 the corresponding coordinate.\n\nLet's try to do it for a toy example. Imagine that we have *N* = 4 and the list of the most popular words is \n\n    ['hi', 'you', 'me', 'are']\n\nThen we need to numerate them, for example, like this: \n\n    {'hi': 0, 'you': 1, 'me': 2, 'are': 3}\n\nAnd we have the text, which we want to transform to the vector:\n\n    'hi how are you'\n\nFor this text we create a corresponding zero vector \n\n    [0, 0, 0, 0]\n    \nAnd iterate over all words, and if the word is in the dictionary, we increase the value of the corresponding position in the vector:\n\n    'hi':  [1, 0, 0, 0]\n    'how': [1, 0, 0, 0] # word 'how' is not in our dictionary\n    'are': [1, 0, 0, 1]\n    'you': [1, 1, 0, 1]\n\nThe resulting vector will be \n\n    [1, 1, 0, 1]\n   \nImplement the described encoding in the function *my_bag_of_words* with the size of the dictionary equals to 5000. To find the most common words use train data. You can test your code using the function *test_my_bag_of_words*.","72bc20d3":"### Data\n\nThe following cell will download all data required for this assignment into the folder `week1\/data`."}}