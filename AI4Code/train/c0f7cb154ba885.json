{"cell_type":{"333eebe6":"code","255b90ca":"code","b526fe21":"code","ee95e311":"code","f1349de6":"code","6dc60c25":"code","fffa4cdb":"code","c2fd291b":"code","13e0a1b8":"code","6aae2a07":"code","e37283c4":"code","400c455e":"code","d4275566":"code","ca888bee":"code","d1e0d20c":"markdown","6eb72b73":"markdown","aacad1ec":"markdown","da486090":"markdown","239b4ff1":"markdown","99571067":"markdown","2efb44c4":"markdown","a0af9abe":"markdown","a470b24f":"markdown","ffa4d027":"markdown","f4df7d69":"markdown"},"source":{"333eebe6":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport itertools\nimport os\n\nfrom sklearn.metrics import mean_squared_error as mse\nfrom sklearn.metrics import mean_absolute_error as mae\n\nimport seaborn as sns\nsns.set(font_scale=1.4)\n\nimport matplotlib.pyplot as plt\n\ndef pl(nr=1, nc=1,fs1=20,fs2=7):\n    fig,axes=plt.subplots(nrows=nr, ncols=nc, figsize=(fs1, fs2))\n    return fig, axes","255b90ca":"PATH = '\/kaggle\/input\/tabular-playground-series-jan-2021\/'\ntrain = pd.read_csv(PATH+'train.csv')\ntest = pd.read_csv(PATH+'test.csv')\nsample_submission = pd.read_csv(PATH+'sample_submission.csv')\n\nFT_COLS = [x for x in train.columns if 'cont' in x]\nLABEL='target'","b526fe21":"#highlight outliers\ntrain['outlier_filter'] = np.where(train[LABEL]<4, True, False)\nprint('# outliers', sum(train['outlier_filter']))\n\nol_filt = ~train['outlier_filter']","ee95e311":"NSUBS=4\n\nSUBMISSION_DESCR = ['LGBM','XGB','KERAS','KERAS FEWER FTS',]\ncolors=['Blue','Green','Red', 'Pink',]\n\nSUBMISSION_PATHS = ['\/kaggle\/input\/jan21-lgbm-submission\/',\n                   '\/kaggle\/input\/jan21-tabular-xgb-sub\/',\n                    '\/kaggle\/input\/jan21-tabplayground-nn1-output\/',\n                    '\/kaggle\/input\/jan21-tabplayground-nn2-output\/',                   \n                   ]","f1349de6":"f,a=pl(nr=1,nc=2,fs1=18,fs2=5)\n\noof_list = []\nsubmission_list = []\n\nsns.histplot(train[LABEL], color='Black', alpha=0.5, ax=a[0])\nsns.histplot(train[LABEL], color='Black', alpha=0.5, ax=a[1])\n\nfor count,sp in enumerate(SUBMISSION_PATHS):\n    oof_list+=[pd.read_csv(SUBMISSION_PATHS[count]+'oof_predictions.csv')['oof_prediction'].values]\n    submission_list+=[pd.read_csv(SUBMISSION_PATHS[count]+'submission.csv')['target'].values]\n    \nfor count, w in enumerate(submission_list):    \n    sns.histplot(oof_list[count], color=colors[count], alpha=0.5, ax=a[0])\n    sns.histplot(submission_list[count], color=colors[count], alpha=0.3, ax=a[1])\n    \na[0].set_title('Train Labels & OOFs')\na[1].set_title('Train Labels & Test Predictions')\na[0].set_xlim(6,10)\na[1].set_xlim(6,10)\n\na[0].legend(['LABEL','LGBM','XGB','KERAS','KERAS2',], facecolor='White')\na[1].legend(['LABEL','LGBM - TEST PRD.','XGB - TEST PRD.','KERAS - TEST PRD.', 'KERAS2 - TEST PRD.',], facecolor='White')\n\nplt.tight_layout()","6dc60c25":"#check min and max predictions\nprint('min and max predictions')\nfor count, w in enumerate(submission_list): \n    print(SUBMISSION_DESCR[count],submission_list[count].min(), submission_list[count].max())","fffa4cdb":"weights_range = [0.001  , 0.05, 0.1 , 0.15, 0.2 , 0.25, 0.3 , 0.35, 0.4 , 0.45, 0.5 ,\n       0.55, 0.6 , 0.65, 0.7 , 0.75, 0.8 , 0.85, 0.9 , 0.95, 0.999]","c2fd291b":"%%time\n#create placeholder for results table\noutput_wts = np.zeros((194481,NSUBS+1))\nj=0\nimport itertools\nfor a,b,c,d in itertools.product(weights_range, repeat=NSUBS):\n    #get combination of weights, sum to 100%\n    sum_w = np.array([a,b,c,d]).sum()\n    wts = np.array([a,b,c,d]) \/ sum_w\n    \n    #get oof combination for weights\n    final_oof_preds = np.zeros((len(train),))\n\n    for count, w in enumerate(oof_list):\n        final_oof_preds+=oof_list[count] * wts[count]\n    \n    #get error and put into output table\n    output_wts[j,NSUBS] = np.sqrt(mse(train[LABEL], final_oof_preds))\n    \n    #record the associated weights\n    output_wts[j,0:NSUBS] = wts\n    \n    j+=1","13e0a1b8":"output_wts = pd.DataFrame(columns=['wt_lgbm','wt_xgb','wt_keras','wt_keras2', 'oof_error'],data=output_wts)\n\nf,a=pl(nc=NSUBS,fs1=20)\n\nfor count,c in enumerate(['wt_lgbm','wt_xgb','wt_keras','wt_keras2', ]):\n    a[count].scatter(x=output_wts[c],y=output_wts['oof_error'],color=colors[count])\n    a[count].set_title(SUBMISSION_DESCR[count])\n    a[count].set_xlabel('blending weight')\n    a[count].set_ylabel('oof_score')\n    \nplt.tight_layout()","6aae2a07":"output_wts = output_wts.sort_values('oof_error').reset_index(drop=True)\noutput_wts.head(10)","e37283c4":"#select weights\n\nselected_wts = output_wts.loc[0:200, ['wt_lgbm','wt_xgb','wt_keras','wt_keras2', ]].mean(axis=0)\nprint(selected_wts.sum())\nselected_wts","400c455e":"final_test_preds = np.zeros((len(sample_submission),))\nfinal_oof_preds = np.zeros((len(train),))\n\nfor count, s in enumerate(submission_list):\n    final_test_preds+=submission_list[count] * selected_wts.values[count]\n    final_oof_preds+=oof_list[count] * selected_wts[count]\n    \nprint('final CV error', np.sqrt(mse(train[LABEL], final_oof_preds)))","d4275566":"f,a=pl(nr=1,nc=2,fs1=15,fs2=5)\n\n\nsns.kdeplot(train[LABEL], color='Black', alpha=0.5, ax=a[0])\nsns.kdeplot(train[LABEL], color='Black', alpha=0.5, ax=a[1])\nsns.kdeplot(final_test_preds, color='Green', alpha=0.5, ax=a[1])\n    \na[0].set_title('Train Labels')\na[1].set_title('Train Labels & Test Predictions')\na[0].set_xlim(4.5,10)\na[1].set_xlim(4.5,10)\n\na[1].legend(['LABEL','Test Predictions'], facecolor='White')\n\nplt.tight_layout()","ca888bee":"sample_submission['target'] = final_test_preds\nsample_submission.to_csv('submission.csv', index=False)\nsample_submission.head(5)","d1e0d20c":"# Look at outcomes","6eb72b73":"# Check CV outcome depending on weighting of model","aacad1ec":"Note: this notebook & NN notebooks have been tidied from original submission. Score is the same.","da486090":"**Please note: I took public notebook parameters for running my LGBM and XGB. Therefore thanks to original authors for work on the tree model parameters **\n\nhttps:\/\/www.kaggle.com\/hamditarek\/tabular-playground-series-xgboost-lightgbm\n\n\nhttps:\/\/www.kaggle.com\/hamzaghanmi\/xgboost-hyperparameter-tuning-using-optuna","239b4ff1":"Weights suggest approx 40% keras total across 2 models, 40% LGBM, 20% XGB","99571067":"NN Models\n\nV1 with more features\n\nhttps:\/\/www.kaggle.com\/davidedwards1\/jan21-tabplayground-nn-final-more-features\n\n\nV2 with less features\n\nhttps:\/\/www.kaggle.com\/davidedwards1\/jan21-tabplayground-nn-final-fewer-features","2efb44c4":"Note: suspect my CV is somewhat underestimated due to label encoding etc across CV folds.","a0af9abe":"# Links to notebooks for submissions to be blended","a470b24f":"# Prediction Distributions","ffa4d027":"# Load Inputs (OOF, Test predictions)","f4df7d69":"# Estimate final CV error and create mix \/ submission"}}