{"cell_type":{"745a3164":"code","f6b2472c":"code","97cb477a":"code","420e9269":"code","4385ab3a":"code","20a70d03":"code","ee926203":"code","623287e2":"code","9f834c28":"code","8be138ba":"code","8d39836f":"code","3be544ab":"code","901df211":"code","256970cf":"code","76fbd00a":"code","5aa5a40f":"code","09562f06":"code","28d6855d":"code","ea43dd37":"code","ef0873ca":"code","d8283044":"code","850fdd16":"code","afb00f5e":"code","26334b3d":"code","dd4a2900":"code","b12ddfeb":"code","2e2bc19b":"code","2353244e":"code","aadec690":"code","9bbaaa2e":"code","4a67cbf3":"code","433442d9":"code","a86bc091":"code","8f528ebc":"code","17904a7e":"code","b8e96425":"code","0a2d8fd9":"code","cf6b588f":"code","422abd54":"code","02cc8969":"code","3e7d66ef":"code","835b7ffa":"code","3cf0a5ce":"code","7c2c0d8c":"code","db5c4758":"code","96f4159d":"code","03a51b45":"code","27709d5e":"code","f3de6cae":"code","5abd3f2a":"code","1367ff1d":"code","91a36a64":"code","1ad1bfff":"code","9288a356":"code","7b8b2e5e":"code","98c4f79c":"code","1f6bb8eb":"code","10d53948":"markdown","fd0c7176":"markdown","d89af5df":"markdown","bacd4ea1":"markdown","274e34b4":"markdown","38ff0690":"markdown","8409601d":"markdown","657e982b":"markdown","81c74e9e":"markdown","ecd658ab":"markdown","fb28e5d4":"markdown","a92eb2a8":"markdown","a14251a8":"markdown","b67e6b9c":"markdown","0c27f67b":"markdown","b3fb637a":"markdown","f6194321":"markdown","0280e19d":"markdown","34d9d95b":"markdown","06a73375":"markdown"},"source":{"745a3164":"!pip install seaborn==0.11\n# \ub77c\uc774\ube0c\ub7ec\ub9ac \uc5c5\uadf8\ub808\uc774\ub4dc\n# \uc5c5\uadf8\ub808\uc774\ub4dc \ud6c4\uc5d0\ub294 \uc138\uc158 \uc7ac\uc2dc\uc791\ud574\uc57c \ud568","f6b2472c":"# data \ub77c\uc774\ube0c\ub7ec\ub9ac\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# \uc2dc\uac01\ud654 \ub77c\uc774\ube0c\ub7ec\ub9ac\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nprint(sns.__version__)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n","97cb477a":"train = pd.read_csv('\/kaggle\/input\/kakr-4th-competition\/train.csv')\ntrain.head() # \uc55e\ucabd \ub370\uc774\ud130 \ucd9c\ub825\ntrain.tail() # \ub4a4\ucabd \ub370\uc774\ud130 \ucd9c\ub825\ntrain.info() # data frame\ucd9c\ub825\n#non-null \ub370\uc774\ud130 \uac1c\uc218, data type","420e9269":"import missingno as msno # null \ub370\uc774\ud130 \ud655\uc778 \ub77c\uc774\ube0c\ub7ec\ub9ac\n# https:\/\/github.com\/ResidentMario\/missingno\nmsno.matrix(train)","4385ab3a":"train['workclass'].value_counts() # feature \uac01 \uad6c\uc131 \uc694\uc18c\uc758 \uac1c\uc218 \ud655\uc778","20a70d03":"sns.countplot(data = train, x = 'workclass') # data \ubd84\ud3ec \uc2dc\uac01\ud654\nsns.countplot(data = train, x = 'race', hue = 'sex') # hue : \ub2e4\ub978 feat \uae30\uc900\uc73c\ub85c \ubd84\ub958","ee926203":"train.columns # feature \ud655\uc778\ntrain.describe() # \ub370\uc774\ud130 \uc694\uc57d","623287e2":"# \ub098\uc774 \ubd84\ud3ec\ub97c \ud788\uc2a4\ud1a0\uadf8\ub7a8\uc73c\ub85c\nfig, ax = plt.subplots(1,1,figsize = (12,5))\nax.hist(train['age'], width = 5, edgecolor = 'r')\nax.set_title('age histogram', fontweight = 'bold')\nplt.show()","9f834c28":"# \ub098\uc774 \ubd84\ud3ec\ub97c kdeplot\uc73c\ub85c\nsns.kdeplot(data = train, x = 'age')","8be138ba":"# \ucd9c\ub825\uac12(income)\uc744 text\uc5d0\uc11c \uc22b\uc790\uac12(0, 1)\ub85c \ubc14\uafb8\uae30\ntrain['income'] = (train['income'] == '>50K').astype('int') # astype : T\/F\ub97c 0\/1\ub85c \ucd9c\ub825\nprint(train['income'])","8d39836f":"train.groupby(['race', 'sex'])['income'].mean()","3be544ab":"# pandas\ub85c pivot table\npd.pivot_table(train,\n                      columns = 'sex',\n                      index = 'race',\n                      values = 'income',\n                      aggfunc = 'sum')","901df211":"sns.boxplot(data = train, x = 'race', y = 'age', hue = 'sex') \nsns.violinplot(data = train, x = 'race', y = 'age', hue = 'sex') ","256970cf":"corr = train.corr()\ncorr.style.background_gradient()\ncorr[['income']].style.background_gradient()\n# \uba54\uc18c\ub4dc \uc801\uc6a9\uc2dc \ub300\uad04\ud638 \ub450\uac1c\ub85c","76fbd00a":"sns.heatmap(corr, square = True, linecolor = 'white', linewidth = 0.3,\n            cmap = 'coolwarm', vmax = 1.0, vmin = -1.0)","5aa5a40f":"PATH = '..\/input\/kakr-4th-competition\/'\ntrain = pd.read_csv(PATH + 'train.csv')\ntest  = pd.read_csv(PATH + 'test.csv')\n\ntrain.info() # data frame\ucd9c\ub825","09562f06":"print(\"\\n########head########\\n\")\nprint(train.head())\nprint(\"\\n########tail########\\n\")\nprint(train.tail())\n\n# \ub370\uc774\ud130 \uc911 \uc77c\ubd80\ub97c \ubb34\uc791\uc704 \ucd94\ucd9c\nprint(\"\\n########sample########\\n\")\nprint(train.sample(10))\n# \ud1b5\uacc4\uc801 \uc694\uc57d\nprint(\"\\n########describe########\\n\")\nprint(train.describe())","28d6855d":"# data\ub294 pandas\uc758 dataframe\uc774\uace0, data.column, data['column']\uc740 pandas\uc758 series \uc790\ub8cc\ud615\uc784\uc744 \uc720\uc758\n# data.column\uc740 \uac00\ub054 \uc0ac\uc804\uc5d0 \uc815\uc758\ub41c \uac12\uc774\ub791 \uacb9\uce60 \uc218 \uc788\uc74c\n# data['column']\uc740 \uadf8 \ub4a4\uc758 \uba54\uc18c\ub4dc \ud78c\ud2b8 \ub4f1\uc774 \uc548\ub098\uc634 - \ucde8\uc0ac\uc120\ud0dd!\n\n# data.column : \ud574\ub2f9 \ub370\uc774\ud130\uc758 column\uac12 \ucd9c\ub825\nprint(\"\\n#######native_country########\\n\")\nprint(train.native_country)\nprint(\"\\n########native_country_groupby########\\n\")\nprint(train.native_country.value_counts()) #? : \uac12 \uc5c6\ub2e4\nprint(\"\\n########workclass_groupby##########\\n\")\nprint(train.workclass.value_counts())","ea43dd37":"print(\"\\n########capital_gain_99%###########\\n\")\nprint(train.capital_gain.describe(percentiles = [.99]))\n# .describe[percentiles = float] : \ud2b9\uc815 %\ubd84\uc704 \uac12 \ud655\uc778\n\nprint(\"\\n#########capital_gain_loc############\\n\")\nprint(train.loc[train.capital_gain == 99999, :])\n# loc : \ub370\uc774\ud130\uc758 [\uc870\uac74, \ubc94\uc704]\ub85c \uc120\ud0dd\ud558\uc5ec \ucd9c\ub825\n\nprint(\"\\n#########White race data############\\n\")\nprint(train.loc[train.race == 'White', :])","ef0873ca":"# income\uc744 \ubd80\ub4f1\ud638 text\uc5d0\uc11c \uac12\uc73c\ub85c \ubc14\uafb8\uae30\nprint(\"\\n############## income : >50K, <=50K to 0, 1#################\\n\")\ntrain.income = train.income.map(lambda x : int(x == '>50K') )\nprint(train.income.value_counts())\n\n# 50K \ub118\ub294 \uc0ac\ub78c \uc218\uac00 19744 \/ \uadf8\ub807\uc9c0 \uc54a\uc740 \uc218\uac00 6305\uba85\n\n# \ubcf5\uc218\uac1c\uc758 column\uac19\uc774 \uc5f0\uc0b0\ud558\uae30\nprint(\"\\n##########apply method example###############\\n\")\nprint(train.apply(lambda x : x['age'] + x['hours_per_week'], axis=1))","d8283044":"# pandas\uc758 plot \uae30\ub2a5\nprint(\"\\n##########pandas plot#############\\n\")\nprint(train.age.plot(kind = 'box'))","850fdd16":"# ?\uac00 \uc788\ub294 \ub370\uc774\ud130 \ucd9c\ub825\ud574\ubcf4\uae30\nprint(\"\\n###########data having ? value#############\\n\")\nprint(train[train.apply(lambda x:\"?\" in list(x), axis = 1)])\nprint(\"\\n###########both workclass is ? and occupation is ?###########\\n\")\nprint(train[(train.workclass == '?') & (train.occupation == '?')])","afb00f5e":"# Null Check\ud558\uae30\nprint(train.info)","26334b3d":"print('\\n########capital gain outliar###########\\n')\nprint(train.loc[train.capital_gain == 99999, 'income'], sum(train.loc[train.capital_gain == 99999, 'income']))\n# capital gain\uc774 99999\uba74 income\uc774 >50K\ub85c \ub098\ud0c0\ub098\ub294 \uac83\uc744 \ubcfc \uc218 \uc788\uc74c\n\nprint('\\n########capital gain outliar###########\\n')\nprint(train[train.capital_gain < 99999].sort_values('capital_gain', ascending = False))","dd4a2900":"print(\"\\n#######log capital gain###############\\n\")\nprint(train.capital_gain.map(lambda x:np.log(x, where = (x!=0))).describe())","b12ddfeb":"from sklearn.preprocessing import MinMaxScaler, StandardScaler\nmm_scaler = MinMaxScaler()\nst_scaler = StandardScaler()\n\n# fit_transform vs transform\ntrain['MM_fnlwgt'] = mm_scaler.fit_transform(train['fnlwgt'].values.reshape(-1,1))\ntest['MM_fnlwgt'] = mm_scaler.transform(test['fnlwgt'].values.reshape(-1,1))\n\ntrain['MM_age'] = mm_scaler.fit_transform(train['age'].values.reshape(-1,1))\ntest['MM_age'] = mm_scaler.transform(test['age'].values.reshape(-1,1))\n\ntrain['ST_fnlwgt'] = st_scaler.fit_transform(train['fnlwgt'].values.reshape(-1,1))\ntest['ST_fnlwgt'] = st_scaler.transform(test['fnlwgt'].values.reshape(-1,1))\n\ntrain['ST_age'] = st_scaler.fit_transform(train['age'].values.reshape(-1,1))\ntest['ST_age'] = st_scaler.transform(test['age'].values.reshape(-1,1))\n\nprint('\\n############scaled train################\\n')\nprint(train[['MM_fnlwgt','MM_age', 'ST_fnlwgt', 'ST_age']])\n\nprint('\\n############scaled test################\\n')\nprint(test[['MM_fnlwgt','MM_age', 'ST_fnlwgt', 'ST_age']])","2e2bc19b":"print(train)","2353244e":"from sklearn.preprocessing import OneHotEncoder, LabelEncoder\nOHE = OneHotEncoder()\nLE = LabelEncoder()\n\n# onehot encoding\nohe_workclass = OHE.fit_transform(train['workclass'].values.reshape(-1,1))\n\nOHE.get_feature_names(['workclass'])\nohe_workclass = pd.DataFrame(ohe_workclass.toarray(), \n             columns= OHE.get_feature_names(['workclass']))\nprint(\"\\n########### onehot encoding - workclass##############\\n\")\nprint(ohe_workclass)\n\n# \uc6d0\ub798 dataset\uc5d0 \ud569\uce58\uae30\nprint(\"\\n######### concat onehot encoded feature ans train data ##########\\n\")\ntrain = pd.concat([train, ohe_workclass], axis = 1)\nprint(train)","aadec690":"# onehot encoding \uc77c\uad04 \uc801\uc6a9\ud558\uae30\nprint(pd.get_dummies(train))","9bbaaa2e":"print(\"\\n############Lable Encoder#############\\n\")\nprint(LE.fit_transform(train['workclass'].values.reshape(-1,1)))\n\n# Label Encoding \uc21c\uc11c \uc9c1\uc811 \uc9c0\uc815\ud558\uae30\nprint(\"\\n##########mapped label encoding############\\n\")\nworkclass_to_label = dict(zip(train['workclass'].unique(), list(range(10))))\nprint(train['workclass'].map(workclass_to_label))","4a67cbf3":"from sklearn.decomposition import PCA\npca = PCA(n_components = 60, svd_solver = 'full')\n\ndummied = pd.get_dummies(train).drop(columns=['id', 'income'])\nX_train_std = st_scaler.fit_transform(dummied)\nX_train_pca = pca.fit_transform(X_train_std)\n\nprint(\"\\n####### pca ##########\\n\")\nprint(X_train_pca)\n\nX_train_res = np.dot(X_train_std - X_train_std.mean(axis=0), pca.components_.T)\n\nprint('\\n#######feature domain before \/ after##########\\n')\nprint(dummied.shape)\nprint(X_train_res.shape)","433442d9":"# train, test data \ubd88\ub7ec\uc624\uae30\ntrain = pd.read_csv(\"\/kaggle\/input\/kakr-4th-competition\/train.csv\")\ntest = pd.read_csv(\"\/kaggle\/input\/kakr-4th-competition\/test.csv\")\nprint('train head')\nprint(train.head())\n\n# income \ubcc4\ub3c4\ub85c \ud560\ub2f9\nlabel = train['income']\ndel train['income']\n# label \uc778\ucf54\ub529 : map\ud568\uc218\uc640 \ub78c\ub2e4\uc2dd\uc73c\ub85c data \uad6c\uc131 \uc694\uc18c \ubaa8\ub450\uc5d0 \uac01 \ud568\uc218 \uc5f0\uc0b0 \uc801\uc6a9\nlabel = label.map(lambda x:1 if x == '>50K' else 0)\nprint('\\n\\nlabel encoded')\nprint(label)","a86bc091":"# data \uc911 id column\uc740 index\uc640 \uac19\uc740 \uc758\ubbf8\n# \uacb0\uacfc\uac12\uc5d0 \uc601\ud5a5\uc744 \uc8fc\uc9c0 \uc54a\uc744 \uac83\uc774\ubbc0\ub85c \ubbf8\ub9ac \uc81c\uac70\ndel train['id']\ndel test['id']\n\n# \ub370\uc774\ud130\ub97c \uc548\uc804\ud558\uac8c \ucc98\ub9ac\ud558\uae30 \uc704\ud574 \uc784\uc2dc\ub85c \ubcf5\uc0ac\ntmp_train = train.copy()\ntmp_test = test.copy()","8f528ebc":"print(train.info())","17904a7e":"# \uacb0\uce21\uce58 \ucc98\ub9ac - \ucd5c\ube48\uac12(mode)\uc73c\ub85c\n# workclass, occupation, native_country\ub294 object \uac1d\uccb4\uc774\uba74\uc11c \uacb0\uce21\uce58\uac00 \uc874\uc7ac\nnan_column = ['workclass', 'occupation', 'native_country']\nfor c in nan_column:\n    tmp_train.loc[train[c] == '?', c] = train[c].mode()[0]\n    tmp_test.loc[test[c] == '?', c] = test[c].mode()[0]","b8e96425":"tmp_train['capital_gain'].plot.hist()\n\n# capital-gain, capital-loss\uc758 \uac12 \ud3b8\ud5a5\uc744 \ub85c\uadf8\ud568\uc218\ub85c \ubcf4\uc644\ntmp_train['log_capital_gain'] = train['capital_gain'].map(lambda x:np.log(x) if x!= 0 else 0)\ntmp_test['log_capital_gain'] = test['capital_gain'].map(lambda x:np.log(x) if x!= 0 else 0)\ntmp_train['log_capital_gain'].plot.hist()\n\ntmp_train['capital_loss'].plot.hist()\n\n# capital-gain, capital-loss\uc758 \uac12 \ud3b8\ud5a5\uc744 \ub85c\uadf8\ud568\uc218\ub85c \ubcf4\uc644\ntmp_train['log_capital_loss'] = train['capital_loss'].map(lambda x:np.log(x) if x!= 0 else 0)\ntmp_test['log_capital_loss'] = test['capital_loss'].map(lambda x:np.log(x) if x!= 0 else 0)\ntmp_train['log_capital_loss'].plot.hist()","0a2d8fd9":"# \uae30\uc874 \ub370\uc774\ud130 \uc0ad\uc81c\ndel tmp_train['capital_gain']\ndel tmp_train['capital_loss']\ndel tmp_test['capital_gain']\ndel tmp_test['capital_loss']","cf6b588f":"from sklearn.model_selection import train_test_split\n\ntmp_train, tmp_valid, y_train, y_valid = train_test_split(tmp_train, label,\n                                                        test_size = 0.3,\n                                                         random_state = 2020,\n                                                         shuffle = True,\n                                                         stratify = label\n                                                         )","422abd54":"# \uc778\ub371\uc2a4 \ucd08\uae30\ud654 : shuffle, random seed\uc5d0 \uc758\ud574 \uc11e\uc778 \uc778\ub371\uc2a4\ub97c \uc815\ub9ac\n# concatenate \uc5f0\uc0b0 \uc2dc \uc778\ub371\uc2a4\uac00 \uc815\ub82c\ub418\uc5b4\uc788\uc9c0 \uc54a\uc73c\uba74 \uc5f0\uc0b0\uc774 \uaf2c\uc77c \uc218 \uc788\uc5b4 \uc815\ub9ac\ud574\uc8fc\ub294\uac8c \uc88b\uc74c\n\n# reset_index(drop = ) : index \ucd08\uae30\ud654 \/ drop = True - \uc774\uc804 \uc778\ub371\uc2a4\ub97c \uc81c\uac70\ntmp_train = tmp_train.reset_index(drop = True)\ntmp_valid = tmp_valid.reset_index(drop = True)\ntmp_test = tmp_test.reset_index(drop = True)","02cc8969":"# \uc2a4\ucf00\uc77c\ub9c1\ncat_columns = [c for c, t in zip(tmp_train.dtypes.index, tmp_train.dtypes) if t == 'O'] \nnum_columns = [c for c in tmp_train.columns if c not in cat_columns]\n\nprint('\ubc94\uc8fc\ud615 \ubcc0\uc218: \\n{}\\n\\n \uc218\uce58\ud615 \ubcc0\uc218: \\n{}\\n'.format(cat_columns, num_columns))\n\nfrom sklearn.preprocessing import StandardScaler\n\nscaler = StandardScaler()\ntmp_train[num_columns] = scaler.fit_transform(tmp_train[num_columns])\ntmp_valid[num_columns] = scaler.transform(tmp_valid[num_columns])\ntmp_test[num_columns]  = scaler.transform(tmp_test[num_columns])","3e7d66ef":"# \ubc94\uc8fc\ud615 \ubcc0\uc218 : one-hot encoding\nfrom sklearn.preprocessing import OneHotEncoder\n\ntmp_data = pd.concat([tmp_train, tmp_valid, tmp_test])\n\nohe = OneHotEncoder(sparse = False)\nohe.fit(tmp_data[cat_columns])\n\n# ohe \uc5f0\uc0b0\uc73c\ub85c \uc0dd\uc131\ub41c column\uc744 \ub9ac\uc2a4\ud2b8\ub85c \ubcc0\ud658\nohe_col = list()\n\nfor c in ohe.categories_:\n    ohe_col += c.tolist()","835b7ffa":"new_train_category = pd.DataFrame(ohe.transform(tmp_train[cat_columns])\n                                  , columns=ohe_col)\n\nnew_valid_category = pd.DataFrame(ohe.transform(tmp_valid[cat_columns])\n                                  , columns=ohe_col)\n\nnew_test_category  = pd.DataFrame(ohe.transform(tmp_test[cat_columns])\n                                  , columns=ohe_col)","3cf0a5ce":"# concat\uc758 axis : \uc5f0\uacb0\ud560 \ub54c \uc5b4\ub290 \ucd95\uc744 \uae30\uc900\uc73c\ub85c \ud560\uc9c0\ub97c \uacb0\uc815\n# data.shape = (a, b, ...)\uc77c \ub54c\n# axis = 0 : a\ub97c \uae30\uc900 \/ 1 : b\ub97c \uae30\uc900 \/ ...\ntmp_train = pd.concat([tmp_train, new_train_category], axis = 1)\ntmp_valid = pd.concat([tmp_valid, new_valid_category], axis = 1)\ntmp_test = pd.concat([tmp_test, new_test_category], axis = 1)\n\n# \uae30\uc874 \ubc94\uc8fc\ud615 column \uc81c\uac70\ntmp_train = tmp_train.drop(columns=cat_columns)\ntmp_valid = tmp_valid.drop(columns=cat_columns)\ntmp_test = tmp_test.drop(columns=cat_columns)","7c2c0d8c":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC # Support Vector Machine\nfrom sklearn.ensemble import RandomForestClassifier\n\nfrom xgboost import XGBClassifier\nfrom lightgbm import LGBMClassifier\n\n# \uc774\ubc88 \ub300\ud68c\uc758 valuation : f1_score\ubc29\uc2dd\nfrom sklearn.metrics import f1_score","db5c4758":"tmp_y_train = y_train\ntmp_y_valid = y_valid","96f4159d":"lr = LogisticRegression()\nlr.fit(tmp_train, tmp_y_train)\n\ny_pred = lr.predict(tmp_valid)\nprint(f\"Logistic Regression f1 score : {f1_score(tmp_y_valid, y_pred, average = 'micro')}\")","03a51b45":"svc = SVC()\nsvc.fit(tmp_train, tmp_y_train)\n\ny_pred = svc.predict(tmp_valid)\nprint(f\"SVC f1 score : {f1_score(tmp_y_valid, y_pred, average = 'micro')}\")","27709d5e":"rf = RandomForestClassifier()\nrf.fit(tmp_train, tmp_y_train)\n\ny_pred = rf.predict(tmp_valid)\nprint(f\"RandomForest F1 Score: {f1_score(tmp_y_valid, y_pred, average='micro')}\")","f3de6cae":"xgb = XGBClassifier(tree_method='gpu_hist')\nxgb.fit(tmp_train, tmp_y_train)\n\ny_pred = xgb.predict(tmp_valid)\nprint(f\"XGBoost F1 Score: {f1_score(tmp_y_valid, y_pred, average='micro')}\")","5abd3f2a":"lgb = LGBMClassifier(tree_method='gpu_hist')\nlgb.fit(tmp_train, tmp_y_train)\n\ny_pred = lgb.predict(tmp_valid)\nprint(f\"LightGBM F1 Score: {f1_score(tmp_y_valid, y_pred, average='micro')}\")","1367ff1d":"# \uae30\uc874 \ubaa8\ub378 \ud559\uc2b5\uc5d0\uc11c\ub294 \uc804\ucc98\ub9ac\ub97c \ubbf8\ub9ac \ud574\ub3c4 \ub418\ub098,\n# K-fold\uc758 \uacbd\uc6b0 \ub9e4\ubc88 \uc804\ucc98\ub9ac\uac00 \ud544\uc694\ud558\ubbc0\ub85c \uc774\ub97c \ud568\uc218\ub85c \ub9cc\ub4e4\uac8c \ub428\ndef preprocess(x_train, x_valid, x_test):\n    tmp_x_train = x_train.copy()\n    tmp_x_valid = x_valid.copy()\n    tmp_x_test  = x_test.copy()\n    \n    tmp_x_train = tmp_x_train.reset_index(drop=True)\n    tmp_x_valid = tmp_x_valid.reset_index(drop=True)\n    tmp_x_test  = tmp_x_test.reset_index(drop=True)\n    \n    nan_column = ['workclass', 'occupation', 'native_country']\n    for c in nan_column:\n        tmp_x_train.loc[tmp_x_train[c] == '?', c] = tmp_x_train[c].mode()[0]\n        tmp_x_valid.loc[tmp_x_valid[c] == '?', c] = tmp_x_valid[c].mode()[0]\n        tmp_x_test.loc[tmp_x_test[c]   == '?', c] = tmp_x_test[c].mode()[0]\n    \n    tmp_x_train['log_capital_loss'] = tmp_x_train['capital_loss'].map(lambda x : np.log(x) if x != 0 else 0)\n    tmp_x_valid['log_capital_loss'] = tmp_x_valid['capital_loss'].map(lambda x : np.log(x) if x != 0 else 0)\n    tmp_x_test['log_capital_loss'] = tmp_x_test['capital_loss'].map(lambda x : np.log(x) if x != 0 else 0)\n    \n    tmp_x_train['log_capital_gain'] = tmp_x_train['capital_gain'].map(lambda x : np.log(x) if x != 0 else 0)\n    tmp_x_valid['log_capital_gain'] = tmp_x_valid['capital_gain'].map(lambda x : np.log(x) if x != 0 else 0)\n    tmp_x_test['log_capital_gain'] = tmp_x_test['capital_gain'].map(lambda x : np.log(x) if x != 0 else 0)\n    \n    tmp_x_train = tmp_x_train.drop(columns=['capital_loss', 'capital_gain'])\n    tmp_x_valid = tmp_x_valid.drop(columns=['capital_loss', 'capital_gain'])\n    tmp_x_test  = tmp_x_test.drop(columns=['capital_loss', 'capital_gain'])\n    \n    scaler = StandardScaler()\n    tmp_x_train[num_columns] = scaler.fit_transform(tmp_x_train[num_columns])\n    tmp_x_valid[num_columns] = scaler.transform(tmp_x_valid[num_columns])\n    tmp_x_test[num_columns]  = scaler.transform(tmp_x_test[num_columns])\n    \n    tmp_all = pd.concat([tmp_x_train, tmp_x_valid, tmp_x_test])\n\n    ohe = OneHotEncoder(sparse=False)\n    ohe.fit(tmp_all[cat_columns])\n    \n    ohe_columns = list()\n    for lst in ohe.categories_:\n        ohe_columns += lst.tolist()\n    \n    tmp_train_cat = pd.DataFrame(ohe.transform(tmp_x_train[cat_columns]), columns=ohe_columns)\n    tmp_valid_cat = pd.DataFrame(ohe.transform(tmp_x_valid[cat_columns]), columns=ohe_columns)\n    tmp_test_cat  = pd.DataFrame(ohe.transform(tmp_x_test[cat_columns]), columns=ohe_columns)\n    \n    tmp_x_train = pd.concat([tmp_x_train, tmp_train_cat], axis=1)\n    tmp_x_valid = pd.concat([tmp_x_valid, tmp_valid_cat], axis=1)\n    tmp_x_test = pd.concat([tmp_x_test, tmp_test_cat], axis=1)\n\n    tmp_x_train = tmp_x_train.drop(columns=cat_columns)\n    tmp_x_valid = tmp_x_valid.drop(columns=cat_columns)\n    tmp_x_test = tmp_x_test.drop(columns=cat_columns)\n    \n    return tmp_x_train.values, tmp_x_valid.values, tmp_x_test.values","91a36a64":"def xgb_f1(y, t, threshold=0.5):\n    t = t.get_label()\n    y_bin = (y > threshold).astype(int) \n    return 'f1',f1_score(t, y_bin, average='micro')","1ad1bfff":"from sklearn.model_selection import KFold, StratifiedKFold\n# KFold : \ub370\uc774\ud130\ub97c \ub3d9\uc77c \ube44\uc728\ub85c \ub098\ub214\n# StratifiedKFold : \ub370\uc774\ud130\ub97c \ud074\ub798\uc2a4 \ube44\uc728\uc5d0 \ub9de\ucd94\uc5b4 \ub098\ub214\n\nn_split = 5\nskf = StratifiedKFold(n_splits = n_split, shuffle = True)","9288a356":"score = list()\noof_pred = np.zeros((test.shape[0], )) # data \ud589 \uac1c\uc218\n\nfor i, (train_idx, val_idx) in enumerate(skf.split(train, label)): # dataset\uc744 Stratified K-Fold \ubd84\ud560\n    # i : iteration\n    # train_idx, val_idx : skf.split\uc73c\ub85c \ubc18\ud658\ub41c fold data index\n    x_train, y_train = train.iloc[train_idx, :], label[train_idx]\n    x_valid, y_valid = train.iloc[val_idx, :], label[val_idx]\n    \n    # \uc804\ucc98\ub9ac\n    x_train, x_valid, x_test = preprocess(x_train, x_valid, test)\n    \n    # \ubaa8\ub378 \uc815\uc758\n    clf = XGBClassifier(tree_method='gpu_hist')\n    \n    # \ubaa8\ub378 \ud559\uc2b5\n    clf.fit(x_train, y_train,\n            eval_set = [[x_valid, y_valid]], \n            eval_metric = xgb_f1,        \n            early_stopping_rounds = 100,\n            verbose = 100,  )\n\n    # \ud6c8\ub828, \uac80\uc99d \ub370\uc774\ud130 Log Loss \ud655\uc778\n    train_f1_score = f1_score(y_train, clf.predict(x_train), average='micro')\n    val_f1_score = f1_score(y_valid, clf.predict(x_valid), average='micro')\n    print('{} Fold, train f1_score : {:.4f}4, validation f1_score : {:.4f}\\n'.format(i, train_f1_score, val_f1_score))\n    \n    score.append(val_f1_score)\n    \n    # out of fold \uc559\uc0c1\ube14 \uad6c\ud604\n    # \uc804\uccb4 \ud655\ub960\uc758 mean\uac12 \uacc4\uc0b0\uc744 \uc704\ud574 split\uc73c\ub85c \ub098\ub208 \ud6c4 \ud569\uc0b0\n    # predict_proba\uc5d0\ub294 0 \ud639\uc740 1\uc774 \uc800\uc7a5 - 1\ubc88 column\uac12\uc73c\ub85c '\ucc38'\uc778 \uacbd\uc6b0\ub97c \ucd94\ucd9c\ud574\ub0c4\n    oof_pred += clf.predict_proba(x_test)[:, 1] \/ n_split\n     \n# \uad50\ucc28 \uac80\uc99d F1 Score \ud3c9\uade0 \uacc4\uc0b0\ud558\uae30\nprint('Cross Validation Score : {:.4f}'.format(np.mean(score)))","7b8b2e5e":"score = list()\noof_pred = np.zeros((test.shape[0], )) # data \ud589 \uac1c\uc218\n\nfor i, (train_idx, val_idx) in enumerate(skf.split(train, label)): # dataset\uc744 Stratified K-Fold \ubd84\ud560\n    # i : iteration\n    # train_idx, val_idx : skf.split\uc73c\ub85c \ubc18\ud658\ub41c fold data index\n    x_train, y_train = train.iloc[train_idx, :], label[train_idx]\n    x_valid, y_valid = train.iloc[val_idx, :], label[val_idx]\n    \n    # \uc804\ucc98\ub9ac\n    x_train, x_valid, x_test = preprocess(x_train, x_valid, test)\n    \n    # \ubaa8\ub378 \uc815\uc758\n    clf = LogisticRegression(solver='liblinear', max_iter=1000)\n    \n    # \ubaa8\ub378 \ud559\uc2b5\n    clf.fit(x_train, y_train)\n\n    # \ud6c8\ub828, \uac80\uc99d \ub370\uc774\ud130 Log Loss \ud655\uc778\n    train_f1_score = f1_score(y_train, clf.predict(x_train), average='micro')\n    val_f1_score = f1_score(y_valid, clf.predict(x_valid), average='micro')\n    print('{} Fold, train f1_score : {:.4f}4, validation f1_score : {:.4f}\\n'.format(i, train_f1_score, val_f1_score))\n    \n    score.append(val_f1_score)\n    \n    # out of fold \uc559\uc0c1\ube14 \uad6c\ud604\n    # \uc804\uccb4 \ud655\ub960\uc758 mean\uac12 \uacc4\uc0b0\uc744 \uc704\ud574 split\uc73c\ub85c \ub098\ub208 \ud6c4 \ud569\uc0b0\n    # predict_proba\uc5d0\ub294 0 \ud639\uc740 1\uc774 \uc800\uc7a5 - 1\ubc88 column\uac12\uc73c\ub85c '\ucc38'\uc778 \uacbd\uc6b0\ub97c \ucd94\ucd9c\ud574\ub0c4\n    oof_pred += clf.predict_proba(x_test)[:, 1] \/ n_split\n     \n# \uad50\ucc28 \uac80\uc99d F1 Score \ud3c9\uade0 \uacc4\uc0b0\ud558\uae30\nprint('Cross Validation Score : {:.4f}'.format(np.mean(score)))","98c4f79c":"score = list()\noof_pred = np.zeros((test.shape[0], )) # data \ud589 \uac1c\uc218\n\nfor i, (train_idx, val_idx) in enumerate(skf.split(train, label)): # dataset\uc744 Stratified K-Fold \ubd84\ud560\n    # i : iteration\n    # train_idx, val_idx : skf.split\uc73c\ub85c \ubc18\ud658\ub41c fold data index\n    x_train, y_train = train.iloc[train_idx, :], label[train_idx]\n    x_valid, y_valid = train.iloc[val_idx, :], label[val_idx]\n    \n    # \uc804\ucc98\ub9ac\n    x_train, x_valid, x_test = preprocess(x_train, x_valid, test)\n    \n    # \ubaa8\ub378 \uc815\uc758\n    clf = LGBMClassifier(tree_method='gpu_hist')\n    \n    # \ubaa8\ub378 \ud559\uc2b5\n    clf.fit(x_train, y_train,\n            eval_set = [[x_valid, y_valid]], \n            eval_metric = 'logloss',        \n            early_stopping_rounds = 100,\n            verbose = 100, )\n\n    # \ud6c8\ub828, \uac80\uc99d \ub370\uc774\ud130 Log Loss \ud655\uc778\n    train_f1_score = f1_score(y_train, clf.predict(x_train), average='micro')\n    val_f1_score = f1_score(y_valid, clf.predict(x_valid), average='micro')\n    print('{} Fold, train f1_score : {:.4f}4, validation f1_score : {:.4f}\\n'.format(i, train_f1_score, val_f1_score))\n    \n    score.append(val_f1_score)\n    \n    # out of fold \uc559\uc0c1\ube14 \uad6c\ud604\n    # \uc804\uccb4 \ud655\ub960\uc758 mean\uac12 \uacc4\uc0b0\uc744 \uc704\ud574 split\uc73c\ub85c \ub098\ub208 \ud6c4 \ud569\uc0b0\n    # predict_proba\uc5d0\ub294 0 \ud639\uc740 1\uc774 \uc800\uc7a5 - 1\ubc88 column\uac12\uc73c\ub85c '\ucc38'\uc778 \uacbd\uc6b0\ub97c \ucd94\ucd9c\ud574\ub0c4\n    oof_pred += clf.predict_proba(x_test)[:, 1] \/ n_split\n     \n# \uad50\ucc28 \uac80\uc99d F1 Score \ud3c9\uade0 \uacc4\uc0b0\ud558\uae30\nprint('Cross Validation Score : {:.4f}'.format(np.mean(score)))","1f6bb8eb":"submit = pd.read_csv(\"\/kaggle\/input\/kakr-4th-competition\/sample_submission.csv\")\nsubmit.loc[:, 'prediction'] = (oof_pred > 0.5).astype(int)\nsubmit.head()\nsubmit.to_csv('submission.csv', index=False)","10d53948":"# PCA","fd0c7176":"# \uc774\uc0c1\uce58 \ucc98\ub9ac\ud574\ubcf4\uae30","d89af5df":"train_dtype = {\n    'id' : np.int32,\n    ...\n}\n- dictionary\ub97c \uc774\uc6a9\ud55c data type\uc744 \uc9c0\uc815 \uac00\ub2a5.\n    - \uc2e4\uc81c \ub370\uc774\ud130\uc758 \ubc94\uc704\ubcf4\ub2e4 read_csv\ub294 \ucd5c\ub300 \ubc94\uc704\ub85c \ubd88\ub7ec\uc624\uae30 \ub54c\ubb38\uc5d0,\n\uc774\ub7ec\ud55c \ubc29\uc2dd\uc73c\ub85c dtype\uc744 \uc9c0\uc815\ud574\uc8fc\uba74 \uba54\ubaa8\ub9ac \uc808\uc57d\uc774 \uac00\ub2a5\n\n- \ub2e8\uc810 : overflow \ubb38\uc81c \uc704\ud5d8\n","bacd4ea1":"# \uc0c1\uad00\uacc4\uc218 \uc2dc\uac01\ud654\ud558\uae30","274e34b4":"# Boxplot \uadf8\ub9ac\uae30","38ff0690":"# K-Fold Cross Validation","8409601d":"# NULL check\uad00\ub828 \ud301\n- .info method\ub85c \ubcfc \ub54c, dtype\uc774 float\uac00 \uc544\ub2d0 \uac83 \uac19\uc740\ub370 float\ub85c \ub098\ud0c0\ub098\uba74 \ub300\ubd80\ubd84 NULL\uac12\uc744 \uac16\ub294\ub2e4.","657e982b":"# 2\uc8fc\ucc28 - Feature Engineering","81c74e9e":"# ex.\ub098\uc774\n- \uc911\uac04\uac12\uc740 37\uc138\n- \ucd5c\ub300 90\uc138, \uc57d 3~40\ub300\ub85c \ud3b8\ud5a5\ub41c \ub098\uc774 \ub370\uc774\ud130\uc784\uc744 \uc54c \uc218 \uc788\ub2e4.","ecd658ab":"# Pivot table \ub9cc\ub4e4\uae30","fb28e5d4":"# 3\uc8fc\ucc28 - \ubaa8\ub378\uacfc \uc559\uc0c1\ube14","a92eb2a8":"* id\n* age : \ub098\uc774\n* workclass : \uace0\uc6a9 \ud615\ud0dc\n* fnlwgt : \uc0ac\ub78c \ub300\ud45c\uc131\uc744 \ub098\ud0c0\ub0b4\ub294 \uac00\uc911\uce58 (final weight\uc758 \uc57d\uc790)\n* education : \uad50\uc721 \uc218\uc900\n* education_num : \uad50\uc721 \uc218\uc900 \uc218\uce58\n* marital_status: \uacb0\ud63c \uc0c1\ud0dc\n* occupation : \uc5c5\uc885\n* relationship : \uac00\uc871 \uad00\uacc4\n* race : \uc778\uc885\n* sex : \uc131\ubcc4\n* capital_gain : \uc591\ub3c4 \uc18c\ub4dd\n* capital_loss : \uc591\ub3c4 \uc190\uc2e4\n* hours_per_week : \uc8fc\ub2f9 \uadfc\ubb34 \uc2dc\uac04\n* native_country : \uad6d\uc801\n* income : \uc218\uc775 (\uc608\uce21\ud574\uc57c \ud558\ub294 \uac12)\n    * \\>50K : 1\n    * <=50K : 0","a14251a8":"# \ud559\uc2b5 \ubaa8\ub378 \ub9cc\ub4e4\uae30","b67e6b9c":"## \ubc94\uc8fc\ud615 \ubcc0\uc218 : one-hot encoding\n|column|\n|--|\n|a|\n|b|\n|c|\n\n\ub300\uc2e0\n\n|a|b|c|\n|--|--|--|\n|1|0|0|\n|0|1|0|\n|0|0|1|\n\n\ud615\ud0dc\ub85c \ub9cc\ub4e0\ub2e4.","0c27f67b":"# 1\uc8fc\ucc28 \uc2e4\uc2b5 - Data EDA \/ \ubd84\uc11d\ud558\uae30","b3fb637a":"# \ub370\uc774\ud130 \ucabc\uac1c\uae30\n- train data : \ubaa8\ub378 \ud559\uc2b5\uc744 \uc704\ud55c \ub370\uc774\ud130\n- valid data : \ud559\uc2b5 \ubaa8\ub378\uc758 \uc131\ub2a5 \uac80\uc99d (train data \uc911 \uc77c\ubd80\ub97c \ubd84\ub9ac)\n- test data : \uc608\uce21\ud560 \ub370\uc774\ud130\n\n- train_test_split( data, label, test_size, random_state, shuffle, stratify )\n    - test_size(float) : \ube44\uc728 \uc9c0\uc815\n    - random_state(int) : \ub79c\ub364 seed\uac12 \uc9c0\uc815 (\uc9c0\uc815\ud558\uc9c0 \uc54a\uc744 \uc2dc \ub2ec\ub77c\uc9d0)\n    - shuffle(bool) : \ub370\uc774\ud130 \ubd84\ub9ac \uc2dc \uc11e\uc744\uc9c0 \uc720\ubb34\n    - stratify(array) : \ucabc\uac1c\uae30 \uc804\/\ud6c4 \ud074\ub798\uc2a4 \ube44\uc728\uc744 \uc720\uc9c0\ud558\uae30 \uc704\ud574 \uc124\uc815","f6194321":"# Feature Engineering","0280e19d":"# Tips\n- \uc0c8\ub85c\uc6b4 feature\ub97c \ub9cc\ub4e4\uc5b4\ubcf4\uc790\n- \uacfc\ud3b8\ud5a5\ub41c feature\ub77c\uba74 T\/F\ub85c \ubd84\ub958\ud574\uc11c \uc801\uc6a9\ud574\ubcf4\uc790\n- \uc5ec\ub7ec\uac1c\uc758 \ubaa8\ub378\uc744 \ub9cc\ub4e4 \ub54c \ud2b9\uc815 feature \uc911\uc2ec \/ Outliar \uc608\uce21 \ub4f1\uc73c\ub85c ","34d9d95b":"# Scaling","06a73375":"# pivot table\ubd84\uc11d\n- \ubc31\uc778 \ub0a8\uc131\uc758 31%\uac00 50K \uc774\uc0c1\uc758 income\uc744 \ubcf4\uc778\ub2e4."}}