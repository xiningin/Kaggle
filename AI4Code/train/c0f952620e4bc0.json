{"cell_type":{"0fedb44a":"code","f23899d9":"code","bffe4890":"code","b588feaa":"code","996a3585":"code","34cf94d2":"code","a2bbd08b":"code","643dce4b":"code","e181ec9d":"code","b6877072":"code","394d1394":"code","2603d65d":"code","915457d5":"code","5d0ebbc2":"code","6b1b6450":"code","e2430228":"code","8d069a99":"code","8202d8e9":"code","b3bb3526":"code","688794b1":"code","abcacd26":"code","5c21ae57":"markdown","30eda0ae":"markdown","20461396":"markdown","36ff3559":"markdown","87c24d5b":"markdown","2fc1538c":"markdown","0b6aad75":"markdown","67eae347":"markdown"},"source":{"0fedb44a":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","f23899d9":"import pandas as pd\nimport numpy as np\nimport os\n\ndf_train=pd.read_csv('..\/input\/bank-notes\/train.csv')\ndf_test=pd.read_csv('..\/input\/bank-notes\/test.csv')\ndf_train.head()","bffe4890":"df_test.isnull().sum(axis=0)","b588feaa":"X_train=df_train[df_train.loc[:,df_train.columns!= 'Class'].columns]\ny_train=df_train['Class']\nX_test=df_test\n","996a3585":"from sklearn.preprocessing import MinMaxScaler\n\nscaler=MinMaxScaler()\nX_train_scaled=scaler.fit_transform(X_train.values)\nX_test_scaled=scaler.fit_transform(X_test.values)\n\n","34cf94d2":"X_train=X_train_scaled\nX_test=X_test_scaled","a2bbd08b":"from sklearn import ensemble\nfrom sklearn.model_selection import GridSearchCV\nclf=ensemble.RandomForestClassifier()\n\nparams = {\n    'bootstrap': [False],\n    'max_depth': [5, 10],\n    'max_features': ['auto'],\n    'min_samples_leaf': [1, 2, 3],\n    'min_samples_split': [2, 4],\n    'n_estimators': [100, 250]}\n\ngsv = GridSearchCV(clf, params, cv=3, n_jobs=-1, scoring='accuracy')\ngsv.fit(X_train, y_train)","643dce4b":"predictions=gsv.best_estimator_.predict(X_test)\n","e181ec9d":"gsv.best_estimator_","b6877072":"predictions","394d1394":"len(predictions)","2603d65d":"X_test_rescaled=scaler.inverse_transform(X_test)\nX_test_rescaled","915457d5":"df_preds = pd.DataFrame(data=X_test_rescaled, columns=[\"Variance\", \"Skewness\", \"Curtosis\",\"Entropy\"])\ndf_preds_class=pd.DataFrame(data=predictions, columns=[\"Class\"])\n","5d0ebbc2":"df_final_preds=df_preds.join(df_preds_class)","6b1b6450":"df_final_preds.to_csv('predictions.csv',header=False, index=False)","e2430228":"import xgboost as xgb\nfrom sklearn import metrics, model_selection","8d069a99":"# function to run the xgboost model #\ndef runXGB(train_X, train_y, test_X, test_y=None, feature_names=None, seed_val=0):\n        params = {}\n        params[\"objective\"] = \"binary:logistic\"\n        params['eval_metric'] = 'logloss'\n        params[\"eta\"] = 0.05\n        params[\"subsample\"] = 0.7\n        params[\"min_child_weight\"] = 10\n        params[\"colsample_bytree\"] = 0.7\n        params[\"max_depth\"] = 8\n#        params[\"silent\"] = 1\n        params[\"seed\"] = seed_val\n        num_rounds = 100\n        plst = list(params.items())\n        xgtrain = xgb.DMatrix(train_X, label=train_y)\n\n        if test_y is not None:\n                xgtest = xgb.DMatrix(test_X, label=test_y)\n                watchlist = [ (xgtrain,'train'), (xgtest, 'test') ]\n                model = xgb.train(plst, xgtrain, num_rounds, watchlist, early_stopping_rounds=50, verbose_eval=10)\n        else:\n                xgtest = xgb.DMatrix(test_X)\n                model = xgb.train(plst, xgtrain, num_rounds)\n\n        pred_test_y = model.predict(xgtest)\n        return pred_test_y","8202d8e9":"pred = runXGB(X_train, y_train, X_test)","b3bb3526":"cutoff = 0.2\npred[pred>=cutoff] = 1\npred[pred<cutoff] = 0\n","688794b1":"pred","abcacd26":"#from sklearn.metrics import accuracy_score\n#accuracy = accuracy_score(y_test, pred)\n#print(\"Accuracy: %.2f%%\" % (accuracy * 100.0))","5c21ae57":"Applied MinMax scaler","30eda0ae":"**This is part of a hackathon I participated. Consdering the timelimit, I tried with Random Forest Classification. I am yet to explore multiple models**","20461396":"No null records. All the values are numeric. We can only perform scaling to increase effciency","36ff3559":"Implementing Random Forest Classifier","87c24d5b":"Merge the output with rescaled values and output the vlaues to a csv file","2fc1538c":"Rescaling the scaled values","0b6aad75":"I cannot find the accuracy since we do not have a y_test dataframe.\n","67eae347":"#implementing XGBoost for prediction"}}