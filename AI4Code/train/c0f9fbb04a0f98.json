{"cell_type":{"4da1077b":"code","f304b47a":"code","2e7b8706":"code","6c67d674":"code","8caee3f7":"code","2e04e6fb":"code","4c84ec7e":"code","a5c46914":"code","d4f55e3c":"code","6e44552a":"code","a4a97c25":"code","a86012df":"markdown","0e1fd2e5":"markdown","f89fec1f":"markdown","feacc0e3":"markdown","6331a44e":"markdown","bf1af968":"markdown","74d466c7":"markdown","71cf00d3":"markdown"},"source":{"4da1077b":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nfrom sklearn.feature_selection import VarianceThreshold\nfrom sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.metrics import roc_auc_score\n\n# read in and split data\ntrain = pd.read_csv('..\/input\/train.csv')\ntest = pd.read_csv('..\/input\/test.csv')","f304b47a":"drop = [\"id\", \"target\", \"wheezy-copper-turtle-magic\"]\nfeature_cols = [ c for c in train.columns if c not in drop ]\n\nskf = StratifiedKFold(n_splits=11, random_state=42)\nclf = QuadraticDiscriminantAnalysis(1.0, store_covariances=False)\n\n# prep result dataframe\nsub = test[[\"id\"]].copy()\nsub[\"target\"] = None\nnum_sets = train['wheezy-copper-turtle-magic'].max() + 1\n\ntrain_preds = np.zeros(train.shape[0])\npreds = np.zeros(test.shape[0])\n\nfor i in range(num_sets):\n    train_data = train[train['wheezy-copper-turtle-magic'] == i]\n    test_data = test[test['wheezy-copper-turtle-magic'] == i]\n    \n    data = pd.concat([train_data[feature_cols], test_data[feature_cols]])\n\n    vt = VarianceThreshold(threshold=1.5).fit(data)\n    \n    slim_train_features = vt.transform(train_data[feature_cols])\n    slim_test_features = vt.transform(test_data[feature_cols])\n\n    for train_index, test_index in skf.split(slim_train_features, train_data['target']):\n        clf.fit(slim_train_features[train_index, :], train_data.iloc[train_index]['target'])\n        train_preds[train_data.index[test_index]] += clf.predict_proba(slim_train_features[test_index, :])[:, 1]\n        \n        preds[test_data.index] += clf.predict_proba(slim_test_features)[:, 1] \/ skf.n_splits\n        \nprint(\"current AUC: \", roc_auc_score(train['target'], train_preds))\nsub[\"target\"] = preds","2e7b8706":"sub[[\"id\", \"target\"]].to_csv(\"qda_submission_v17.csv\", index=False)","6c67d674":"scores = []\nstraglers = []\nfor i in range(num_sets):\n    idx = train[train['wheezy-copper-turtle-magic'] == i].index\n    s = roc_auc_score(train.loc[idx, 'target'], train_preds[idx])\n    \n    if s < 0.95:\n        print(i, s)\n        straglers.append(i)\n        \n    scores.append(s)\nprint(\"total straglers: \", len(straglers))","8caee3f7":"import matplotlib.gridspec as gridspec\nimport matplotlib\n\nj = 197\ncolors = [\"#fbd808\", \"#f9530b\"]\n\ntrain_features = train[train['wheezy-copper-turtle-magic'] == j].drop(['target', 'id', 'wheezy-copper-turtle-magic'], axis=1)\ntrain_label = train[train['wheezy-copper-turtle-magic'] == j]['target']\nvt = VarianceThreshold(threshold=1.5)\ntrain_features_slim = vt.fit_transform(train_features)\n\nrows = int(train_features_slim.shape[1] \/ 4)\ncols = 4\n\nfig, ax = plt.subplots(nrows=rows, ncols=cols, squeeze=False, figsize=(24, 36))\nfig.subplots_adjust(hspace=0.5)\n\nc = 0\nfor row in ax:\n    for col in row:\n        c += 1\n        col.scatter(np.arange(train_features_slim.shape[0]), train_features_slim[:, c], c=train_label, cmap=matplotlib.colors.ListedColormap(colors));\n        col.title.set_text(\"feature: {0}\".format(c))  ","2e04e6fb":"# for c in np.arange(train_features_slim.shape[1]):\nimport itertools\nc_features = train_features_slim.shape[1]\nfor n, m in list(itertools.combinations(np.arange(train_features_slim.shape[1]), 2))[c_features*3:c_features*4]:\n    plt.figure()\n    plt.scatter(train_features_slim[:, n], train_features_slim[:, m], c=train_label, cmap=matplotlib.colors.ListedColormap(colors));\n    plt.title(\"feature: {0} v {1}\".format(n, m))\n    plt.plot()","4c84ec7e":"from mpl_toolkits.mplot3d import Axes3D\nfig = plt.figure(figsize=(20, 15))\nax = fig.add_subplot(111, projection='3d')\n\ncombos = itertools.combinations(np.arange(0, train_features_slim.shape[1]), 3)\nfor xi, yi, zi in list(combos)[0:10]:\n    if xi != yi or zi != yi or xi != zi:\n        ax.scatter(train_features_slim[:, xi], train_features_slim[:, yi], train_features_slim[:, zi], c=train_label, cmap=matplotlib.colors.ListedColormap(colors));\nax.set_xlabel('X axis')\nax.set_ylabel('Y axis')\nax.set_zlabel('Z axis');","a5c46914":"x = train_features_slim[:, 1] # first feature\ny = train_features_slim[:, 2] # second feature\n\nd = 0.1\nt_x1, t_x4 = np.quantile(x, [d, 1-d])\nt_y1, t_y4 = np.quantile(y, [d, 1-d])\nidx = np.where(\n    ((x < t_x1) & (y < t_y1)) | ((x > t_x4) & (y > t_y4)) \n)\n\nfig = plt.figure(figsize=(20, 15))\nax = fig.add_subplot(111, projection='3d')\n\nfor j in range(2, train_features_slim.shape[1]):\n    z = train_features_slim[:, j]\n    x_slim = x[idx]\n    y_slim = y[idx]\n    z_slim = z[idx]\n\n    ax.scatter(x_slim, y_slim, z_slim, c=train_label.iloc[idx], cmap=matplotlib.colors.ListedColormap(colors));\nax.set_xlabel('X axis')\nax.set_ylabel('Y axis')\nax.set_zlabel('Z axis');","d4f55e3c":"from sklearn.tree import DecisionTreeClassifier\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.naive_bayes import GaussianNB\n\n# use LDA for outlier classification\nclf = QuadraticDiscriminantAnalysis(0.5, store_covariances=False)\n\nskf = StratifiedKFold(n_splits=3, random_state=42)\n\ndef cutoffs(x, y):\n    d = 0.15\n    t_x1, t_x4 = np.quantile(x, [d, 1-d])\n    t_y1, t_y4 = np.quantile(y, [d, 1-d])\n    idx = np.where(\n        ((x < t_x1) & (y < t_y1)) | ((x > t_x4) & (y > t_y4)) \n    )\n    idx_other = np.where(\n        ((x >= t_x1) & (y >= t_y1)) | ((x <= t_x4) & (y <= t_y4))\n    )\n    return idx, idx_other\n\nedge_train_preds = np.zeros(train_features_slim.shape[0])\nedge_test_preds = np.zeros(train_features_slim.shape[0])\n\npreds = np.zeros(test.shape[0])\n\nfor j in straglers:\n# for j in range(num_sets):\n    scores = []\n    \n    test_data = test[test['wheezy-copper-turtle-magic'] == j]\n\n    train_features = train[train['wheezy-copper-turtle-magic'] == j].drop(['target', 'id', 'wheezy-copper-turtle-magic'], axis=1)\n    train_label = train[train['wheezy-copper-turtle-magic'] == j]['target']\n    train_preds_j = np.zeros(train_features.shape[0])\n    train_times_j = np.zeros(train_features.shape[0]) # keeps track of the number of times an index shows up in sample\n\n    vt = VarianceThreshold(threshold=1.5)\n    train_features_slim = vt.fit_transform(train_features)\n    test_features_slim = vt.transform(test_data[feature_cols])\n\n    for f in range(train_features_slim.shape[1] - 1):\n        idx_train, idx_train_other = cutoffs(train_features_slim[:, 0], train_features_slim[:, 1])\n        idx_test_other, idx_test = cutoffs(test_features_slim[:, 0], test_features_slim[:, 1])\n\n        for train_index, test_index in skf.split(train_features_slim[idx_train_other], train_label.iloc[idx_train_other]):\n        \n            x_train = train_features_slim[idx_train_other[0][train_index]]\n            x_test = train_features_slim[idx_train_other[0][test_index]]\n            y_train = train_label.iloc[idx_train_other].iloc[train_index]\n            y_test = train_label.iloc[idx_train_other].iloc[test_index]\n        \n            if len(set(y_train)) > 1:\n                clf.fit(x_train, y_train)\n                probs = clf.predict_proba(x_test)[:, 1]\n                scores.append(roc_auc_score(y_test, probs))\n            \n            train_times_j[idx_train_other[0][test_index]] += 1 \n            train_preds_j[idx_train_other[0][test_index]] += probs \n            \n            preds[test_data.index[idx_test[0]]] += clf.predict_proba(test_features_slim[idx_test[0]])[:, 1] \/ (skf.n_splits * (train_features_slim.shape[1] - 1))","6e44552a":"assert len(preds) == len(sub)\n\nsub2 = sub.copy()\nidx = np.where(preds != 0)[0]\n# for how many did we predict the same?\nprint(preds[idx].shape)\nprint(np.where(np.rint(sub[\"target\"].loc[idx]) == np.rint(preds[idx]))[0].shape)\n\n# what's the different in previous sums to current?\nprint(np.sum(preds[idx]))\nprint(np.sum(sub[\"target\"].loc[idx]))\n\n# what's the total summation impact on the predictions?\nsub2.loc[idx, \"target\"] = (sub2.loc[idx, \"target\"] + preds[idx]) \/ 2\nprint(sub2[\"target\"].sum())\nprint(sub[\"target\"].sum())","a4a97c25":"sub2[[\"id\", \"target\"]].to_csv(\"qda_submission_v16.csv\", index=False)","a86012df":"That wasn't very fruitful, let's looks at interactions between 3 features at once. ","0e1fd2e5":"Maybe this is something we can exploit? We can use the QDA model as our base, and then add tweak obvervations like those above.","f89fec1f":"## Current approach","feacc0e3":"Look at that saw score for <strong>197<\/strong>. As an initial test, are there any single features that look like good predictors in this subset?","6331a44e":"# Focus on the stragglers\n#### I explore modeling outliers exclusively, and modeling without outliers \u2014 neither improved my score.\n---\nSo far my highest LB score is 0.9659, a pathetic comparison to the 0.97X at the top of the LB.  \n\nIn order to make progress, I'm going to focus on data subsets that had the lowest CV score. ","bf1af968":"It looks like for the outlier values of <em>x, y<\/em> there is a clear relationship between the points, and the labels. Let's take a look at only those. We can filter for <em>x, y<\/em> pairs that show up in the top and bottom deciles.","74d466c7":"None of the single features seem to standout. \n\nWhat about feature combinations? ","71cf00d3":"Let's take a look at some of the lower performing subsets"}}