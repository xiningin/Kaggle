{"cell_type":{"79495af7":"code","d7163533":"code","79dcae67":"code","53e4ff56":"code","0818d04c":"code","3de6aad5":"code","389faf5a":"code","f1b8df47":"code","d9865e8a":"code","60a931af":"code","c6161012":"code","a5ef8eb9":"code","6de42c2e":"markdown","0575bc65":"markdown","b400ff81":"markdown","c9641d27":"markdown","70006a31":"markdown","bb600591":"markdown","f7096914":"markdown","f3fc684c":"markdown","9e5e7925":"markdown","a85fe330":"markdown","6557b300":"markdown","3daa33ab":"markdown","f5be9c1e":"markdown","e5963573":"markdown","46ea6960":"markdown","86afeaa4":"markdown"},"source":{"79495af7":"# import necessary packages\n!pip install apyori\nimport requests\nimport numpy as np \nimport pandas as pd\nimport plotly.express as px\nimport plotly.graph_objects as go\nimport matplotlib.pyplot as plt\nfrom PIL import Image\nfrom io import BytesIO\nfrom apyori import apriori\nfrom collections import Counter\nfrom wordcloud import WordCloud","d7163533":"# load dataset with ignoring the header because the 1st row is also a purchase history entry\npurchase_history = pd.read_csv(\"\/kaggle\/input\/market-basket-optimization\/Market_Basket_Optimisation.csv\", \n                               header = None)\npurchase_history","79dcae67":"# flatten the dataframe to a list of all the purchased items including the nan values\npurchased_items = purchase_history.values.flatten()\nprint(\"Length of the list:\", len(purchased_items))\npurchased_items","53e4ff56":"# only preserve the non-nan values using for loop because np.isnan() doesn't support mixed types\n# of string float\npurchased_items_processed = []\nfor item in purchased_items:\n    if type(item) == str:\n        purchased_items_processed.append(item)\nprint(\"Length of the processed list:\", len(purchased_items_processed))","0818d04c":"# create product counts using Counter() from collections package\nproduct_counts = Counter(purchased_items_processed)","3de6aad5":"# load masking from remote URL for displaying wordcloud based on item frequency\nicon_path = \"https:\/\/img.flaticon.com\/icons\/png\/512\/25\/25619.png?size=1200x600f&pad=10,10,10,10&ext=png&bg=FFFFFFFF\"\nmasking_layer = np.array(Image.open(BytesIO(requests.get(icon_path).content)))\nwordcloud = WordCloud(width = 1000, \n                      height = 500,\n                      background_color = 'white',\n                      max_font_size = 90,\n                      mask = masking_layer,\n                      random_state = 9).generate_from_frequencies(product_counts)\nplt.figure(figsize=(20,10))\nplt.axis(\"off\")\nplt.imshow(wordcloud);","389faf5a":"# sort two list together based on item count with descending order\nproduct_count, product_label = zip(*sorted(zip(product_counts.values(), product_counts.keys()), \n                                           reverse=True)) \n\n# bar chart visualization of the item frequency\nfig = go.Figure([go.Bar(x=product_label[:25], y=product_count[:25], text=product_count[:25])])\nfig.update_layout(title_text='Top 25 purchased items in a week')\nfig.show()","f1b8df47":"# histogram of the purchased item frequencies\nfig = go.Figure([go.Box(x=product_count, boxpoints='all', name=\"Purchase Count\")])\nfig.update_layout(title_text='Item Purchased Count')\nfig.show()","d9865e8a":"# remove nan for each purchase history\nnon_na_entries = []\nfor entry in purchase_history.values:\n    non_na_entry = []\n    for item in entry:\n        if type(item) == str:\n            non_na_entry.append(item)\n    non_na_entries.append(non_na_entry)","60a931af":"# parameters I chose; feel free to change based on your preference and what's your interest\nmin_support = 0.01 # find items having purchase probability of at least 0.01\nmin_confidence = 0.10 # find co-occurred itemset with co-occurrence probability of 0.10\nmin_lift = 2 # find a list of itemset A and B that's at least 2 times more likely to bought together than B alone\nmin_length = 2 # find a list of items with length of at least 2\nassociation_rules = apriori(non_na_entries, \n                            min_support=min_support,\n                            min_confidence=min_confidence,\n                            min_lift=min_lift,\n                            min_length=min_length) \nassociation_results = list(association_rules)\nprint(\"Total number of itemsets mined:\", len(association_results))","c6161012":"# example output of the 1st itemset\nassociation_results[0]","a5ef8eb9":"# helper function for extracting data for creating dataframe\ndef process_itemset(itemset):\n    # get max lift rule\n    max_rule_idx = np.argmax([rules.lift for rules in itemset.ordered_statistics])\n    max_rule = itemset.ordered_statistics[max_rule_idx]\n    \n    # return itemset and stats with the chosen rule\n    return {\n        \"frequent_itemset\": list(itemset.items),\n        \"support\": round(itemset.support, 6),\n        \"rule\": str(list(max_rule.items_base)) + \" -> \" + str(list(max_rule.items_add)),\n        \"confidence\": round(max_rule.confidence, 6),\n        \"lift\": round(max_rule.lift, 6)\n    }\n\n# create dataframe for the frequent itemset obtained from apriori algorithm\nitemset_df = pd.DataFrame([process_itemset(itemset) for itemset in association_results])\nitemset_df.sort_values([\"support\", \"confidence\", \"lift\"], ascending=False).head(5)","6de42c2e":"#### 3.1 Main concepts in Apriori simply explained:\n\nAssociation rule learning typically does not consider the order of items either within a transaction or across transactions.\n\nAn **association rule** commonly represented as \"A,B \u2192 C\", indicates C will likely to appear when A and B appears together.  \nAn **itemset** is a collection of items as {A,B,C}, and it is frequent if its items tend to co-occur. An itemset can have multiple association rule generated with matching criteria specified to the Apriori algorithm.\n\n1. **Support(A)**: the item popularity calculated by finding number of purchase containing the item divided by total purchases in the store.\n2. **Confidence(A \u2192 B)**: the likelihood of an item B is also bought when item A is bought. It's the number of purchases containing both item A and item B divided by the total number of purchases of item A. Or just support(A and B) \/ support(A)\n3. **Lift(A \u2192 B) = Confidence(A \u2192 B) \/ Support(B)**: the increase in the ratio of sale of B when A is sold. We are basically calculating the ratio of two different likelihoods. \n\n#### Interpretations of the 3 concepts:  \n**Support** tells us what's the probability of a single item being purchased in the store.  \n**Confidence** tells us how likely item A and item B are purchased together.  \n**Lift** tells us that the likelihood of buying A and B together is how many times more than the likelihood of just buying B. A lift value of greater than 1 means high association between two products; products A and B are more likely to be bought together. When the lift of A -> B is 1, it tells us it's unlikely the two products are associated in the purchased history. Higher lift values tells us the greater chances of customer preference to buy B if the customer has already bought A. It's a good indicator whether you should consider about cross merchadising strategy in product placement.\n\n#### A general explanation of how the algorithm construct sets of items satisfiying a given min support and min confidence value:\n\nApriori uses breadth-first search strategy to count the support of itemsets and uses a candidate generation function which exploits the downward closure property of support..\n\n1. Find all the purchased item subsets with **support** value higher than minimum support threshold.\n2. Select all the rules from the subsets with **confidence** value higher than minimum confidence threshold.\n3. Order the rules by descending order of **Lift**.\n\nsource: https:\/\/en.wikipedia.org\/wiki\/Association_rule_learning","0575bc65":"#### Interpretation of the apriori algorithm association rule learning output:\n\nEach itemset representation has at least one rule matched the specified criteria of support > 1%, confidence > 10%, and lift > 2.\n\nFrom the top 5 matching rules evaluated on the support, confidence, and lift value, the itemset of {\"group beef\", \"spaghetti\"} are the most commonly purchased combination matched our requirement for confidence of 20% and lift > 2.  \n\nBased on the most common itemset of {\"ground beef\", \"spaghetti\"}'s rule of {'spaghetti'} -> {'ground beef'}, the confidence of \t{'spaghetti'} -> {'ground beef'} is 0.225115 or 22.5%. This confidence implies that out of all the purchases that contain spaghetti, 22.5% of the purchases also included ground beef. \n\nThe lift of the most purchased item combination is 2.29. This indicates that groud beef is 2.29 times more likely to be bought by the customers who buy spaghetti compared to the default likelihood of the sales of groud beef alone.\n\n#### Suggestions based on the model output:\n\nThe store owner can consider creating product placement strategies with the above itemsets learned from the apriori algorithm to sell products together because they are more likely to be bought together than single products alone. For example, putting \"herb & pepper\" next to ground beef location to see what will happen to the sale.","b400ff81":"### 2. Data Preprocessing for visualizations","c9641d27":"#### After obtaining the sorted item frequency together with the item label, the bar chart's output further displayed more detailed statistics of the item count. The top 5 purchased items are \"mineral water\", \"eggs\", \"spaghetti\", \"french fries\", \"chocolate\"\n","70006a31":"#### From the above masked wordcloud generated using item frequency , \"mineral water\" stands out to the audience and we can see \"eggs\", \"spaghetti\", \"french fries\", and \"chocolate\" are pretty popular items in the store in one week of store ","bb600591":"### 1. Dataset Description","f7096914":"#### 3.2 Apply Apriori algoirthm to the grocery purchase dataset","f3fc684c":"#### Now, we have a list of items purchased in a week in the grocery store without the NaN values.  \n#### I will visualize the frequency of the items with bar plot and wordcloud for illustration.","9e5e7925":"![image.png](attachment:b533937d-29dc-4693-8b65-547189f2562d.png)\n\n### The Idea of Cross merchandising:\n\n#### The idea of cross merchandising is a practice of placing complimentary goods together that consumers tend to buy together to grow basket size and drive impulse purchases. Common examples include salsa and tortilla chips (<u>see example photo below<\/u>), pasta and tomato sauce, Halloween costume and candy. \n\n![image.png](attachment:41032bb0-6557-4d4f-b6bb-80b807cd678e.png)  \nsource: Pinterest \n\n#### If a product goes well with other products in the store, it would be a great idea to adjust the product placement in the store by co-branding the two products together in the store. The practice of cross merchandising is not limited to traditional physical store product displacement. It can also be applied to online store and online marketing strategies for displaying or selling products together.\n\n#### In this notebook, I will explore the grocery purchase history data with puchased items recorded and apply Apriori algorithm to conduct unsupervised assoication rule learning to identify underlying relations between different purchased items. ","a85fe330":"### 3. Unsupervised association rule learning with Apriori Algorithm","6557b300":"#### My advice on choosing your thresholds:\n#### Know your upper bound of support beforehand by finding the max support of single item. Here, the max support you can get is 1788 \/ 7501 = 0.238. See bar chart above for reference. You don't want go above that number.\n#### Otherwise, you will not obtain any results because your threshold is too strict","3daa33ab":"#### As you can see above, the output returns all the possible rules matching the given mininium thresholds of itemset support of 0.01, rule confidence of 0.10, and rule lift of 2. The itemset doesn't have direction of which one is added first, but the rule does to compare whether adding an item will increase the ratio of sales. \n\n#### I want to only keep the rule having the max lift for explanation instead of storing all the rules of a given itemset in a dataframe. Now, let me write a helper function to extract only the rule with max lift among all the rules available given a itemset mined using Apriori algorithm.","f5be9c1e":"#### Thanks for reading all the way to here. If you like my kernel, please give it an upvote and leave a comment. ","e5963573":"#### The histogram shows that purchased counts are centered on left and the 50% of the item purchase count is less than 117 times a week.\n#### Now, let's explore what can Apriori algorithm tell us about the associations between products.","46ea6960":"```\n  _______ _                 _     __     __            __             _____                _ _             \n |__   __| |               | |    \\ \\   \/ \/           \/ _|           |  __ \\              | (_)            \n    | |  | |__   __ _ _ __ | | __  \\ \\_\/ \/__  _   _  | |_ ___  _ __  | |__) |___  __ _  __| |_ _ __   __ _ \n    | |  | '_ \\ \/ _` | '_ \\| |\/ \/   \\   \/ _ \\| | | | |  _\/ _ \\| '__| |  _  \/\/ _ \\\/ _` |\/ _` | | '_ \\ \/ _` |\n    | |  | | | | (_| | | | |   <     | | (_) | |_| | | || (_) | |    | | \\ \\  __\/ (_| | (_| | | | | | (_| |\n    |_|  |_| |_|\\__,_|_| |_|_|\\_\\    |_|\\___\/ \\__,_| |_| \\___\/|_|    |_|  \\_\\___|\\__,_|\\__,_|_|_| |_|\\__, |\n                                                                                                      __\/ |\n                                                                                                     |___\/ \n```","86afeaa4":"#### The above displayed data contiains 7501 customer purchase history of a French grocery store in a week. The items in each row together represent what a purchase history look like. NaN is padded for creating a dataframe with consistent column size."}}