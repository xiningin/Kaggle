{"cell_type":{"c7410c07":"code","e814575b":"code","957a7d36":"code","466ffbe3":"code","fd1b2bd7":"code","d718d58e":"code","205d0fe6":"code","1b2e0f28":"code","47e408ad":"code","66891534":"code","1dd4ace4":"code","3f083e17":"code","8b97cafb":"code","1903da60":"code","00058be8":"code","f9fdd681":"code","29ee6373":"code","b292af81":"code","3f66efce":"code","031ade13":"code","242a3daf":"code","026052f5":"code","29bc9cb4":"markdown","fbdd6e14":"markdown","cb02dd8e":"markdown","00671690":"markdown","d4377855":"markdown","420b8a67":"markdown","0bf4057a":"markdown","77c5889d":"markdown"},"source":{"c7410c07":"#remove \" > \/dev\/null 2>&1\" to see what is going on under the hood\n!pip install gym pyvirtualdisplay > \/dev\/null 2>&1\n!apt-get update  > \/dev\/null 2>&1\n!apt-get install -y xvfb python-opengl ffmpeg  > \/dev\/null 2>&1","e814575b":"import gym\nfrom gym import logger as gymlogger\nfrom gym.wrappers import Monitor\ngymlogger.set_level(40) #error only\nimport tensorflow as tf\nimport numpy as np\nimport random\nimport matplotlib\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport math\nimport glob\nimport io\nimport base64\nfrom IPython.display import HTML\nfrom IPython import display as ipythondisplay","957a7d36":"from pyvirtualdisplay import Display\ndisplay = Display(visible=0, size=(1400, 900))\ndisplay.start()","466ffbe3":"def show_video():\n  mp4list = glob.glob('video\/*.mp4')\n  if len(mp4list) > 0:\n    mp4 = mp4list[0]\n    video = io.open(mp4, 'r+b').read()\n    encoded = base64.b64encode(video)\n    ipythondisplay.display(HTML(data='''<video alt=\"test\" autoplay \n                loop controls style=\"height: 400px;\">\n                <source src=\"data:video\/mp4;base64,{0}\" type=\"video\/mp4\" \/>\n             <\/video>'''.format(encoded.decode('ascii'))))\n  else: \n    print(\"Could not find video\")\n    \n\ndef wrap_env(env):\n  env = Monitor(env, '.\/video', force=True)\n  return env","fd1b2bd7":"# Install CarRacing environment (in Box2D)\n!pip install Box2D","d718d58e":"env = wrap_env(gym.make(\"CarRacing-v0\"))","205d0fe6":"# Test Environment and Show Videos\nobservation = env.reset()\nwhile True:\n    env.render()\n    action = env.action_space.sample() \n    observation, reward, done, info = env.step(action)         \n    if done: \n      break;\nenv.close()\nshow_video()","1b2e0f28":"# Import Pytorch\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom torch.distributions import Beta\nfrom torch.utils.data.sampler import BatchSampler, SubsetRandomSampler\nimport time\nfrom collections import deque","47e408ad":"class Net(nn.Module):\n    \"\"\"\n    Convolutional Neural Network for PPO\n    \"\"\"\n\n    def __init__(self, img_stack):\n        super(Net, self).__init__()\n        self.cnn_base = nn.Sequential(  # input shape (4, 96, 96)\n            nn.Conv2d(img_stack, 8, kernel_size=4, stride=2),\n            nn.ReLU(),  # activation\n            nn.Conv2d(8, 16, kernel_size=3, stride=2),  # (8, 47, 47)\n            nn.ReLU(),  # activation\n            nn.Conv2d(16, 32, kernel_size=3, stride=2),  # (16, 23, 23)\n            nn.ReLU(),  # activation\n            nn.Conv2d(32, 64, kernel_size=3, stride=2),  # (32, 11, 11)\n            nn.ReLU(),  # activation\n            nn.Conv2d(64, 128, kernel_size=3, stride=1),  # (64, 5, 5)\n            nn.ReLU(),  # activation\n            nn.Conv2d(128, 256, kernel_size=3, stride=1),  # (128, 3, 3)\n            nn.ReLU(),  # activation\n        )  # output shape (256, 1, 1)\n        self.v = nn.Sequential(nn.Linear(256, 100), nn.ReLU(), nn.Linear(100, 1))\n        self.fc = nn.Sequential(nn.Linear(256, 100), nn.ReLU())\n        self.alpha_head = nn.Sequential(nn.Linear(100, 3), nn.Softplus())\n        self.beta_head = nn.Sequential(nn.Linear(100, 3), nn.Softplus())\n        self.apply(self._weights_init)\n\n    @staticmethod\n    def _weights_init(m):\n        if isinstance(m, nn.Conv2d):\n            nn.init.xavier_uniform_(m.weight, gain=nn.init.calculate_gain('relu'))\n            nn.init.constant_(m.bias, 0.1)\n\n    def forward(self, x):\n        x = self.cnn_base(x)\n        x = x.view(-1, 256)\n        v = self.v(x)\n        x = self.fc(x)\n        alpha = self.alpha_head(x) + 1\n        beta = self.beta_head(x) + 1\n\n        return (alpha, beta), v","66891534":"img_stack=4\n\ntransition = np.dtype([('s', np.float64, (img_stack, 96, 96)), \n                       ('a', np.float64, (3,)), ('a_logp', np.float64),\n                       ('r', np.float64), ('s_', np.float64, (img_stack, 96, 96))])\n\nGAMMA=0.99\nEPOCH= 8 # beter than 10\nMAX_SIZE = 2000 ## CUDA out of mem for max_size=10000\nBATCH=128 \nEPS=0.1\nLEARNING_RATE = 0.001 # bettr than 0.005 or 0.002 \n\nclass Agent():\n    \"\"\" Agent for training \"\"\"\n    \n    def __init__(self, device):\n        self.training_step = 0\n        self.net = Net(img_stack).double().to(device)\n        self.buffer = np.empty(MAX_SIZE, dtype=transition)\n        self.counter = 0\n        self.device = device\n        \n        self.optimizer = optim.Adam(self.net.parameters(), lr=LEARNING_RATE)  ## lr=1e-3\n\n    def select_action(self, state):\n        state = torch.from_numpy(state).double().to(self.device).unsqueeze(0)\n        \n        with torch.no_grad():\n            alpha, beta = self.net(state)[0]\n        dist = Beta(alpha, beta)\n        action = dist.sample()\n        a_logp = dist.log_prob(action).sum(dim=1)\n\n        action = action.squeeze().cpu().numpy()\n        a_logp = a_logp.item()\n        return action, a_logp\n\n\n    def store(self, transition):\n        self.buffer[self.counter] = transition\n        self.counter += 1\n        if self.counter == MAX_SIZE:\n            self.counter = 0\n            return True\n        else:\n            return False\n\n    def update(self):\n        self.training_step += 1\n\n        s = torch.tensor(self.buffer['s'], dtype=torch.double).to(self.device)\n        a = torch.tensor(self.buffer['a'], dtype=torch.double).to(self.device)\n        r = torch.tensor(self.buffer['r'], dtype=torch.double).to(self.device).view(-1, 1)\n        next_s = torch.tensor(self.buffer['s_'], dtype=torch.double).to(self.device)\n\n        old_a_logp = torch.tensor(self.buffer['a_logp'], dtype=torch.double).to(self.device).view(-1, 1)\n\n        with torch.no_grad():\n            target_v = r + GAMMA * self.net(next_s)[1]\n            adv = target_v - self.net(s)[1]\n            # adv = (adv - adv.mean()) \/ (adv.std() + 1e-8)\n\n        for _ in range(EPOCH):\n            for index in BatchSampler(SubsetRandomSampler(range(MAX_SIZE)), BATCH, False):\n\n                alpha, beta = self.net(s[index])[0]\n                dist = Beta(alpha, beta)\n                a_logp = dist.log_prob(a[index]).sum(dim=1, keepdim=True)\n                ratio = torch.exp(a_logp - old_a_logp[index])\n\n                surr1 = ratio * adv[index]\n                \n                # clipped function\n                surr2 = torch.clamp(ratio, 1.0 - EPS, 1.0 + EPS) * adv[index]\n                action_loss = -torch.min(surr1, surr2).mean()\n                value_loss = F.smooth_l1_loss(self.net(s[index])[1], target_v[index])\n                loss = action_loss + 2. * value_loss\n\n                self.optimizer.zero_grad()\n                loss.backward()\n                self.optimizer.step()","1dd4ace4":"# Initializing Training Environment\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint('device: ', device)\n\nseed = 0 \ntorch.manual_seed(seed)\ntorch.cuda.manual_seed(seed)\nnp.random.seed(seed)\n\naction_repeat = 10\nenv = gym.make('CarRacing-v0', verbose=0)\nstate = env.reset()\nprint('env.action_space.shape: ', env.action_space.shape)\nreward_threshold = env.spec.reward_threshold\nprint('reward_threshold', reward_threshold)","3f083e17":"# show what a preprocessed image looks like\nframe, _, _, _ = env.step(np.array([2., 1., 1.]))\n\nprint('frame.shape: ', frame.shape)\nplt.subplot(1,2,1)\nplt.imshow(frame)\nplt.title('original image')\n\n#-------------------------------#\n\ndef rgb2gray(rgb, norm=True):\n        # rgb image -> gray [0, 1]\n    gray = np.dot(rgb[..., :], [0.299, 0.587, 0.114])\n    if norm:\n        # normalize\n        gray = gray \/ 128. - 1.\n    return gray\n\nimg_gray = rgb2gray(frame)\n\n#-------------------------------# \nplt.subplot(1,2,2)\nplt.title('preprocessed image')\n\nprint('img.shape: ', img_gray.shape)\n\n# 96 x 96 black and white image\nplt.imshow(img_gray, cmap='Greys')\nplt.show()","8b97cafb":"class Wrapper():\n    \"\"\"\n    Environment wrapper for CarRacing \n    \"\"\"\n\n    def __init__(self, env):\n        self.env = env  \n\n    def reset(self):\n        self.counter = 0\n        self.av_r = self.reward_memory()\n\n        self.die = False\n        img_rgb = env.reset()\n        img_gray = rgb2gray(img_rgb)\n        self.stack = [img_gray] * img_stack  # four frames for decision\n        return np.array(self.stack)\n\n    def step(self, action):\n        total_reward = 0\n        for i in range(action_repeat):\n            img_rgb, reward, die, _ = env.step(action)\n            # don't penalize \"die state\"\n            if die:\n                reward += 100\n            # green penalty\n            if np.mean(img_rgb[:, :, 1]) > 185.0:\n                reward -= 0.05\n            total_reward += reward\n            # if no reward recently, end the episode\n            done = True if self.av_r(reward) <= -0.1 else False\n            if done or die:\n                break\n        img_gray = rgb2gray(img_rgb)\n        self.stack.pop(0)\n        self.stack.append(img_gray)\n        assert len(self.stack) == img_stack\n        return np.array(self.stack), total_reward, done, die\n\n\n    @staticmethod\n    def reward_memory():\n        # record reward for last 100 steps\n        count = 0\n        length = 100\n        history = np.zeros(length)\n\n        def memory(reward):\n            nonlocal count\n            history[count] = reward\n            count = (count + 1) % length\n            return np.mean(history)\n\n        return memory","1903da60":"def save(agent, directory, filename, suffix):\n    torch.save(agent.net.state_dict(), '%s\/%s_%s.pth' % (directory, filename, suffix))","00058be8":"def ppo_train(n_episodes=500, save_every=100):\n    \n    scores_deque = deque(maxlen=100)\n    scores_array = []\n    avg_scores_array = []    \n\n    timestep_after_last_save = 0\n    \n    time_start = time.time()\n\n    running_score = 0\n    state = env_wrap.reset()\n    \n    i_lim = 0\n    \n    for i_episode in range(n_episodes):\n        \n        timestep = 0\n        total_reward = 0\n        \n        ## score = 0\n        state = env_wrap.reset()\n\n        while True:    \n            \n            action, a_logp = agent.select_action(state)\n            next_state, reward, done, die = env_wrap.step( \n                action * np.array([2., 1., 1.]) + np.array([-1., 0., 0.]))\n\n            if agent.store((state, action, a_logp, reward, next_state)):\n                print('updating')\n                agent.update()\n            \n            total_reward += reward\n            state = next_state\n            \n            timestep += 1  \n            timestep_after_last_save += 1\n            \n            if done or die:\n                break\n                \n        running_score = running_score * 0.99 + total_reward * 0.01\n\n        scores_deque.append(total_reward)\n        scores_array.append(total_reward)\n\n        avg_score = np.mean(scores_deque)\n        avg_scores_array.append(avg_score)\n        \n        s = (int)(time.time() - time_start)        \n        print('Ep. {}, Ep.Timesteps {}, Score: {:.2f}, Avg.Score: {:.2f}, Run.Score {:.2f}, \\\nTime: {:02}:{:02}:{:02} '\\\n            .format(i_episode, timestep, \\\n                    total_reward, avg_score, running_score, s\/\/3600, s%3600\/\/60, s%60))  \n       \n        \n        # Save episode is equal to \"save_every\" timesteps\n        if i_episode+1 % save_every == 0:\n\n            suf = str(i_episode)\n            save(agent, '', 'model_weights', suf)\n            \n        if np.mean(scores_deque) > reward_threshold:\n            print(\"Solved environment! Running score is {:.2f}, Avg.Score: {:.2f} !\" \\\n                  .format(running_score, avg_score))\n            break\n            \n    return scores_array, avg_scores_array    \n            \n","f9fdd681":"agent = Agent(device)\n\nenv_wrap = Wrapper(env)\n\nNUM_EPISODES = 300\n\nscores, avg_scores  = ppo_train(NUM_EPISODES)\n# Save latest model. We'll use it for testing\nsave(agent, '.', 'model_weights', 'latest')","29ee6373":"%matplotlib inline\n\nprint('length of scores: ', len(scores), ', len of avg_scores: ', len(avg_scores))\n\nfig = plt.figure()\nax = fig.add_subplot(111)\nplt.plot(np.arange(1, len(scores)+1), scores, label=\"Score\")\nplt.plot(np.arange(1, len(avg_scores)+1), avg_scores, label=\"Avg on 100 episodes\")\nplt.legend(bbox_to_anchor=(1.05, 1)) \nplt.ylabel('Score')\nplt.xlabel('Episodes #')\nplt.show()","b292af81":"def load(agent, directory, filename):\n    agent.net.load_state_dict(torch.load(os.path.join(directory,filename)))","3f66efce":"def play(env, agent, n_episodes):\n    state = env_wrap.reset()\n    \n    scores_deque = deque(maxlen=n_episodes)\n    scores = []\n    \n    for i_episode in range(1, n_episodes+1):\n        state = env_wrap.reset()        \n        score = 0\n        \n        time_start = time.time()\n        \n        while True:\n            action, a_logp = agent.select_action(state)\n            env.render()\n            next_state, reward, done, die = env_wrap.step( \\\n                action * np.array([2., 1., 1.]) + np.array([-1., 0., 0.]))\n\n            state = next_state\n            score += reward\n            \n            if done or die:\n                break \n\n        s = (int)(time.time() - time_start)\n        \n        scores_deque.append(score)\n        scores.append(score)\n\n        print('Episode {}\\tAverage Score: {:.2f},\\tScore: {:.2f} \\tTime: {:02}:{:02}:{:02}'\\\n                  .format(i_episode, np.mean(scores_deque), score, s\/\/3600, s%3600\/\/60, s%60))\n    return np.mean(scores_deque)","031ade13":"# We use the average score of 10 episodes as result, Don't change n_episodes!!!\ntotal_avg_reward = play(env, agent, n_episodes=10)","242a3daf":"# Print results in CSV format and upload to Kaggle\nwith open('rewards.csv', 'w') as f:\n    f.write('Id,Predicted\\n')\n    f.write('CarRacing_public,{}\\n'.format(total_avg_reward))\n    f.write('CarRacing_private,{}\\n'.format(total_avg_reward))\n\n# Download your results!\nfrom IPython.display import FileLink\nFileLink('rewards.csv')","026052f5":"import os\nenv_test = Monitor(gym.make(\"CarRacing-v0\"), '.\/video', force=True)\nenv_test.reset()\nenv_wrap = Wrapper(env_test)\nload(agent, '', 'model_weights_latest.pth')\nplay(env_test, agent, n_episodes=1)\nwhile True:\n    env_test.render()\n    action = env_test.action_space.sample() \n    observation, reward, done, info = env_test.step(action)         \n    if done: \n      break;\nenv_test.close()\nshow_video()","29bc9cb4":"### Preprocessing CarRacing Screenshots","fbdd6e14":"### Video Display Functions","cb02dd8e":"# NTUT DRL Homework 2: CarRacing\n\nCarRacing is the continuous control task learning from pixels, a top-down racing environment. State consists of 96x96 pixels. Reward is -0.1 every frame and +1000\/N for every track tile visited, where N is the total number of tiles in track. For example, if you have finished in 732 frames, your reward is 1000 - 0.1*732 = 926.8 points. CarRacing-v0 defines \"solving\" as getting average reward of 900 over 100 consecutive trials.\nThis example notebook refers to the github:\n\nhttps:\/\/github.com\/Rafael1s\/Deep-Reinforcement-Learning-Udacity\/tree\/master\/CarRacing-From-Pixels-PPO","00671690":"### Training Agent","d4377855":"### Initializing Training Environment","420b8a67":"### Install video rendering dependancies, which takes around 45 seconds","0bf4057a":"## Show Your Training Result","77c5889d":"### Define Wrapper for Our Environment"}}