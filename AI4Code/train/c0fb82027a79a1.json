{"cell_type":{"d4d897a8":"code","95391e77":"code","0661087a":"code","8265e578":"code","4c5a1f9e":"code","77b292a0":"code","57d3f222":"code","17142003":"code","6dbf855d":"markdown","22a00136":"markdown","d49b59d6":"markdown","837d6e6b":"markdown","466d62a6":"markdown","a37c971e":"markdown","26fc11eb":"markdown","7006504c":"markdown","b27fad10":"markdown"},"source":{"d4d897a8":"from sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.multiclass import OneVsRestClassifier\nfrom sklearn.preprocessing import LabelEncoder\nimport pandas as pd\nimport numpy as np\nimport json\nfrom timeit import default_timer as timer","95391e77":"%%time\n\ndef read_dataset(path):\n    return json.load(open(path)) \n\ntrain = read_dataset('..\/input\/whats-cooking-kernels-only\/train.json')\ntest = read_dataset('..\/input\/whats-cooking-kernels-only\/test.json')\n\ndef generate_text(data):\n    text_data = [\" \".join(doc['ingredients']).lower() for doc in data]\n    return text_data \n\ntrain_text = generate_text(train)\ntest_text = generate_text(test)\ntarget = [doc['cuisine'] for doc in train]\n\ntfidf = TfidfVectorizer(binary=True)\ndef tfidf_features(txt, flag):\n    if flag == \"train\":\n        x = tfidf.fit_transform(txt)\n    else:\n        x = tfidf.transform(txt)\n    x = x.astype('float32')\n    return x \nX = tfidf_features(train_text, flag=\"train\")\nX_test = tfidf_features(test_text, flag=\"test\")\n\n# Label Encoding - Target \nlb = LabelEncoder()\ny = lb.fit_transform(target)","0661087a":"def train_predict():\n    from sklearn.svm import SVC\n    classifier = SVC(C=100, # penalty parameter\n                     kernel='rbf', # kernel type, rbf working fine here\n                     degree=3, # default value\n                     gamma=1, # kernel coefficient\n                     coef0=1, # change to 1 from default value of 0.0\n                     shrinking=True, # using shrinking heuristics\n                     tol=0.001, # stopping criterion tolerance \n                     probability=False, # no need to enable probability estimates\n                     cache_size=200, # 200 MB cache size\n                     class_weight=None, # all classes are treated equally \n                     verbose=False, # print the logs \n                     max_iter=-1, # no limit, let it run\n                     random_state=0)\n    model = OneVsRestClassifier(classifier, n_jobs=4)\n    model.fit(X, y)\n    y_test = model.predict(X_test)\n    return lb.inverse_transform(y_test)","8265e578":"%%time\ny_pred_original = train_predict()","4c5a1f9e":"!pip install scikit-learn-intelex --progress-bar off >> \/tmp\/pip_sklearnex.log","77b292a0":"from sklearnex import patch_sklearn\npatch_sklearn()","57d3f222":"%%time\ny_pred_oprimized = train_predict()","17142003":"test_id = [doc['id'] for doc in test]\nsub = pd.DataFrame({'id': test_id, 'cuisine': y_pred_oprimized}, columns=['id', 'cuisine'])\nsub.to_csv('submission.csv', index=False)","6dbf855d":"To get optimizations, patch scikit-learn using Intel(R) Extension:","22a00136":"![image.png](attachment:image.png)","d49b59d6":"The training of the SVM model took almost **30 minutes**. Let's try to use scikit-learn-intelex. First, download it: ","837d6e6b":"This time, the training took a **little over two minutes**, which saved us almost **28 minutes** Let\u2019s make sure that the quality has not changed:","466d62a6":"Save the result. Now the search for the perfect model has accelerated significantly.","a37c971e":"Let\u2019s run the same code to train the SVM model: ","26fc11eb":"Let's take the training and predict into a separate function:","7006504c":"With scikit-learn-intelex patching you can:\n\n- Use your scikit-learn code for training and inference without modification.\n- Train and predict scikit-learn models **up to 15 times faster**.\n- Get the same quality of predictions as other tested frameworks.\n\n*Please, upvote if you like.*","b27fad10":"# Optimizing Kaggle kernels using Intel(R) Extension for Scikit-learn*\n\nFor classical machine learning algorithms, we often use the most popular Python library, [scikit-learn](https:\/\/scikit-learn.org\/stable\/). We use it to fit models and search for optimal parameters, but\u202fscikit-learn\u202fsometimes works for hours, if not days. Speeding up this process is something anyone who uses scikit-learn would be interested in.\n\nI want to show you how to get results faster without changing the code. To do this, we will use another Python library,\u202f\u202f**[scikit-learn-intelex](https:\/\/github.com\/intel\/scikit-learn-intelex)**. It accelerates scikit-learn and does not require you changing the code written for scikit-learn.\n\nI will use a Kaggle [notebook](https:\/\/www.kaggle.com\/shivamb\/tf-idf-with-ovr-svm-what-s-cooking) in which the training of the Support Vector Machine model executed in over **30 minutes**. \n\n## Problem Statement: SVM script for multiclass classification \n\n*What's Cooking:* Tf-IDF with One-vs-Rest Support Vector Machine (SVM) Model\n\n*Goal:* Use recipe ingredients to categorize the cuisine\n\n*Input:* Text Data (Ingredients for a Cuisine)\n\n*Output:* Single Class (Cuisine Class)\n\n*Author:* [sban](https:\/\/www.kaggle.com\/shivamb)\n\n*Created date:* 26 June 2018"}}