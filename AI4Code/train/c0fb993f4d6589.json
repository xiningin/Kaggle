{"cell_type":{"1716a5d1":"code","62b7cd92":"code","c84d1ba2":"code","57970ac9":"code","bd843dd5":"code","65c54965":"code","da6167d0":"code","76e10559":"code","e0c70ee7":"code","a44bf518":"code","1e154b7b":"code","2b01b0f7":"code","8e9e12dc":"code","9e3d20f3":"code","58bbe429":"code","7b357484":"code","06431a03":"code","bca3f6e8":"code","e099c283":"code","1e5e9b52":"code","73a637c6":"code","f13b3610":"code","0e350cf2":"code","471f84d6":"markdown","8f435c7d":"markdown","6f6cbe80":"markdown","00dcbbea":"markdown","209594e5":"markdown","db25baae":"markdown","b3a3abf8":"markdown","636ce68a":"markdown","eed78354":"markdown","7fd54c4f":"markdown","ebcd66a5":"markdown","c88ef6ed":"markdown","229df612":"markdown","8afe74c5":"markdown","2603a4f8":"markdown","38e23fd0":"markdown","1f460715":"markdown"},"source":{"1716a5d1":"import numpy as np #linear algebra\nimport pandas as pd #data processing\n\nimport matplotlib.pyplot as plt #data visualization\nimport seaborn as sns #data visualization\n\nimport warnings\nwarnings.filterwarnings(\"ignore\") #to ignore the warnings\n\n#for model building\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.ensemble import AdaBoostClassifier\nimport xgboost as xgb","62b7cd92":"# Loading the data\ndf = pd.read_csv('\/kaggle\/input\/breast-cancer-wisconsin-data\/data.csv')","c84d1ba2":"df.head()","57970ac9":"df.info()","bd843dd5":"df.drop(['Unnamed: 32','id'], axis = 1 , inplace=True)\ndf.columns","65c54965":"df.diagnosis.replace({\"M\":1,\"B\":0},inplace=True)\ndf.diagnosis.unique()","da6167d0":"corr = df.corr()\nplt.figure(figsize=(20,20))\nsns.heatmap(df.corr(), cmap='YlGnBu', annot = True)\nplt.title(\"Correlation Map\", fontweight = \"bold\", fontsize=16)","76e10559":"corr[abs(corr['diagnosis']) > 0.59].index","e0c70ee7":"df.drop('diagnosis', axis=1).corrwith(df.diagnosis).plot(kind='bar', grid=True, figsize=(12, 10), title=\"Correlation with target\",color=\"green\");","a44bf518":"corr_matrix = df.corr()\nthreshold = 0.60 \nfiltre = np.abs(corr_matrix[\"diagnosis\"]) > threshold\ncorr_features = corr_matrix.columns[filtre].tolist()\nsns.clustermap(df[corr_features].corr(), annot = True, cmap=\"YlGnBu\")\nplt.title(\"Correlation Between Features w Corr Theshold 0.60\", fontweight = \"bold\", fontsize=16)\nplt.show()","1e154b7b":"#pair plot\nsns.pairplot(df[corr_features], diag_kind = \"kde\", markers = \"+\", hue = \"diagnosis\", palette='viridis')\nplt.show()","2b01b0f7":"from sklearn.neighbors import LocalOutlierFactor\n\n# split the data to X and y before Local Outlier Factorization\n\ny=df[\"diagnosis\"]\nX=df.drop([\"diagnosis\"],axis=1)\ncolumns= df.columns.tolist()","8e9e12dc":"lof= LocalOutlierFactor()\ny_pred=lof.fit_predict(X)\ny_pred[0:30]\n#  1 = inlier\n# -1 = outlier","9e3d20f3":"x_score= lof.negative_outlier_factor_\noutlier_score= pd.DataFrame()\noutlier_score[\"score\"]=x_score\n\nlofthreshold= -2.5\nloffilter= outlier_score[\"score\"]< lofthreshold\noutlier_index= outlier_score[loffilter].index.tolist()","58bbe429":"plt.figure(figsize=(12,8.))\nplt.scatter(X.iloc[outlier_index,0],X.iloc[outlier_index,4],color=\"blue\",s=50,label=\"outliers\")\nplt.scatter(X.iloc[:,0],X.iloc[:,4],color=\"k\",s=3,label=\"Data Points\")\n\nradius=(x_score.max()- x_score)\/(x_score.max()-x_score.min())\noutlier_score[\"radius\"]=radius\nplt.scatter(X.iloc[:,0],X.iloc[:,4],s=1000*radius,edgecolors=\"r\",facecolors=\"none\",label=\"outlier scores\")\nplt.legend();","7b357484":"X= X.drop(outlier_index)\ny= y.drop(outlier_index).values","06431a03":"from sklearn.model_selection import train_test_split\nfrom sklearn.neighbors import NeighborhoodComponentsAnalysis\nX_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.3,random_state=42)\n\n# Dont fit the scaler while standardizate X_test !\nscaler = StandardScaler()\nX_train=scaler.fit_transform(X_train)\nX_test=scaler.transform(X_test)","bca3f6e8":"key = ['LogisticRegression','KNeighborsClassifier','SVC','DecisionTreeClassifier','RandomForestClassifier','GradientBoostingClassifier','AdaBoostClassifier','XGBClassifier']\nvalue = [LogisticRegression(), KNeighborsClassifier(n_neighbors = 2, weights ='uniform'), SVC(kernel=\"rbf\",random_state=15), DecisionTreeClassifier(random_state=10), RandomForestClassifier(n_estimators=60, random_state=0), GradientBoostingClassifier(random_state=20), AdaBoostClassifier(), xgb.XGBClassifier(random_state=0,booster=\"gbtree\")]\nmodels = dict(zip(key,value))\nmodels","e099c283":"predicted =[]\nfor name,algo in models.items():\n    model=algo\n    model.fit(X_train,y_train)\n    predict = model.predict(X_test)\n    acc = accuracy_score(y_test, predict)\n    predicted.append(acc)\n    print(name,acc)","1e5e9b52":"plt.figure(figsize = (10,5))\nsns.barplot(x = predicted, y = key, palette='pastel')\nplt.title(\"Plotting the Model Accuracies\", fontsize=16, fontweight=\"bold\")","73a637c6":"lr = LogisticRegression(solver='lbfgs', max_iter=10000)\nrs = []\nacc = []\nfor i in range(1,25,1):\n    X_train,X_test,y_train,y_test = train_test_split(X, y, test_size = 0.2, random_state = i)    \n    model_lr_rs = lr.fit(X_train, y_train)\n    predict_values_lr_rs = model_lr_rs.predict(X_test)\n    acc.append(accuracy_score(y_test, predict_values_lr_rs))\n    rs.append(i)","f13b3610":"plt.figure(figsize=(10,10))\nplt.plot(rs, acc, color ='red')\n\nfor i in range(len(rs)):\n    print(rs[i],acc[i])\n","0e350cf2":"for i in range(0,24):\n    if acc[i] > 0.98:\n        print(acc[i])","471f84d6":"<h3 style = \"font-family: Comic Sans MS;background-color:#ff99cc\t\"> Observations: <\/h3>\nThe following columns are the one's that show the greatest correlation with our diagnosis column. There are two things that can be done. \n* We can either use only the columns which have greatest correlation, or we can continue to use all the columns.\n* I will be using all these columns to predict our result\n* You can eliminate a few and see if the accuracy improves!","8f435c7d":"![image.png](attachment:a530fc48-7806-483c-9049-9b86635fd7bd.png)","6f6cbe80":"<h3 style = \"font-family: Comic Sans MS;background-color:#ff99cc\t\"> Observations: <\/h3>\nIn order to conduct our analysis easily, we have converted the target column as:\n\n* Malignant - 1\n* Benignant - 0","00dcbbea":"<h3 style = \"font-family: Comic Sans MS;background-color:#ff99cc\t\"> Observations: <\/h3>\nWe would need to eliminate the outliers so that it does not affects our model's accuracy. Let us see if there are any outliers present in the dataset!","209594e5":"From the above figure we can see that our model touches somewhere around 99% between (20-25). Let's try to see where exactly does this happen!","db25baae":"<h3 style = \"font-family: Comic Sans MS;background-color:#ff99cc\t\"> Observations: <\/h3>\n\n* Only the 'diagnosis' column, which we have to predict is of object datatype.\n* There's only ID column of int type. We will probably drop it anyway.\n* There are a total of 31 columns which are of float datatype.","b3a3abf8":"**Summary:** We used a total of 8 models in order to achieve our final result. \n\n\n    * LogisticRegression 99.12 %\n    \n    * KNeighborsClassifier 95.32 %\n    \n    * SVC 96.50 %\n    \n    * DecisionTreeClassifier 90.05 %\n    \n    * RandomForestClassifier 96.50 %\n    \n    * GradientBoostingClassifier 97.67 %\n    \n    * AdaBoostClassifier 96 %\n    \n    * XGBClassifier 97.67 %\n","636ce68a":"<h3 style = \"font-family: Comic Sans MS;background-color:#ff99cc\t\"> Observations: <\/h3>\nLooks wonderful, isn't it! \n* There are only a handful of columns that show negative correlation with the 'diagnosis column'\n* Around half of our columns are more than 50% positively correlated to diagnosis column.\n\nWe have to select which of the attributes we want to use in building our model!\n","eed78354":"<h3 style = \"font-family: Comic Sans MS;background-color:#ff99cc\t\"> Observations: <\/h3>\nAfter dropping the two columns, we are now left with 31 columns. Let us see how well do they correlate with the diagnosis column. ","7fd54c4f":"\n![image.png](attachment:efa5748c-1e3b-44c4-a93d-f37f0421fcc1.png)","ebcd66a5":"**About The Local Outlier Factor**\n\nThe Local Outlier Factor (LOF) algorithm is an unsupervised anomaly detection method which computes the local density deviation of a given data point with respect to its neighbors. It considers as outliers the samples that have a substantially lower density than their neighbors.","c88ef6ed":"**<h3 style = \"font-family: Comic Sans MS;background-color:#ff99cc\t\"> Observations: <\/h3>\n:** \nSo there are 3 columns that have outliers, lets plot them and check them out!","229df612":"<h3 style = \"font-family: Comic Sans MS;background-color:#ff99cc\t\"> Observations: <\/h3>\nWe can clearly see that all our models perform with more than 90% accuracy where DecisionTreeClassifier has the lowest of 90.058% and LogisticRegression has the highest of 98.25% accuracy.\n\nLet us see if we can further improve the accuracy of our model by adding a few changes to it!\n","8afe74c5":"**About the Dataset:**\n<h1 style = \"font-family: Comic Sans MS;background-color:#ff99cc\t\"> About this Kernel <\/h1>\n\n\n* The Breast Cancer datasets is available UCI machine learning repository maintained by the University of California, Irvine. \n* The dataset contains 569 samples of malignant and benign tumor cells.\n\n* The first two columns in the dataset store the unique ID numbers of the samples and the corresponding diagnosis (M=malignant, B=benign), respectively. \n* The columns 3-32 contain 30 real-value features that have been computed from digitized images of the cell nuclei, which can be used to build a model to predict whether a tumor is benign or malignant.\n\n    * 1= Malignant (Cancerous) - Present (M)\n    * 0= Benign (Not Cancerous) -Absent (B)\n    \n    \n    \n**Ten real-valued features are computed for each cell nucleus:**\n\n\n* radius (mean of distances from center to points on the perimeter)\n* texture (standard deviation of gray-scale values)\n* perimeter\n* area\n* smoothness (local variation in radius lengths)\n* compactness (perimeter^2 \/ area - 1.0)\n* concavity (severity of concave portions of the contour)\n* concave points (number of concave portions of the contour)\n* symmetry\n* fractal dimension (\"coastline approximation\" - 1)\n\nThe mean, standard error and \"worst\" or largest (mean of the three largest values) of these features were computed for each image, resulting in 30 features. For instance, field 3 is Mean Radius, field\n13 is Radius SE, field 23 is Worst Radius.\n\n    * All feature values are recoded with four significant digits.\n\n    * Missing attribute values: none\n\n> Class distribution: 357 benign, 212 malignant","2603a4f8":"<h3 style = \"font-family: Comic Sans MS;background-color:#ff99cc\t\"> Observations: <\/h3>\nFrom the above plot we may conclude that highest accuracy is achieved at 20th iteration.\n\nWohhoo, We have finally built a model with an accuracy of 99.12%!","38e23fd0":"> *Thanks a lot for checking this out till the end!*","1f460715":"<h3 style = \"font-family: Comic Sans MS;background-color:#ff99cc\t\"> Observations: <\/h3>\n\n* The last column named \"Unaname: 32\" seems like an erronous coloumn in our dataset. We might probably just drop it.\n* Most of the columns seem to have a numeric entry. This would save our time from mapping the variables.\n* The ID column would not help us contributing to predict about the cancer. We might as well drop it."}}