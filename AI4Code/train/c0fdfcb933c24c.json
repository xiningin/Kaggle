{"cell_type":{"06ce0365":"code","79331cc6":"code","99a1fd46":"code","c66ae209":"code","bf0c8f8e":"code","678de6dc":"code","6383a459":"code","536ecba4":"code","4444bb7b":"code","ac20d3ab":"code","a25cc0cb":"code","9299bd2a":"code","528fb8c5":"code","f0e5d54a":"code","4bd3b102":"code","d450124e":"code","702b2a92":"code","b6d5f3fc":"code","1ac5c3ee":"code","3fc1a722":"code","4f1bfaf1":"code","a205f359":"code","a7634b32":"code","ffea9c8e":"code","95468865":"code","135c5dee":"code","051aaa62":"code","8186f7b4":"code","16bed126":"code","4d41eec3":"code","fa23ea59":"code","cc56a177":"code","44078149":"code","11d612a7":"code","9ef0951d":"code","2d2e3e04":"code","a53e0d2a":"code","2cebc0d5":"code","5e941483":"code","edfd1f04":"code","f4b109d8":"code","aa5dd9d7":"code","fd183eec":"code","9c666995":"code","f61cc1da":"code","3f544e97":"code","30cd2087":"code","90bd7694":"code","f8e3d325":"code","b32a596b":"code","f82f1cac":"code","01875377":"code","177ad673":"code","a1af3a8b":"code","5dea1fbe":"code","95e7c23d":"code","1f821545":"code","75b82848":"code","e7097854":"code","c7d4221a":"code","46abdce9":"code","dcc85b86":"code","9fb0f916":"code","e990e104":"code","0d19356f":"code","14213dc4":"code","d1738144":"code","1cb966f0":"code","290402fe":"code","fa69749a":"markdown","f5b3718e":"markdown","0f2d919c":"markdown","f3273b3e":"markdown","fd631ea1":"markdown","29513c94":"markdown","c6bb9230":"markdown","fcb1126d":"markdown","19185bc6":"markdown","9e3cbd59":"markdown","ec874c19":"markdown","7b6b7a1e":"markdown","31f9d593":"markdown","769c6304":"markdown","d3376040":"markdown","2f23acff":"markdown","d931ff85":"markdown"},"source":{"06ce0365":"import numpy as np\nimport pandas as pd\nfrom scipy.stats import describe\npd.options.display.max_columns = 12\npd.options.display.max_rows = 24","79331cc6":"# disable warnings in Anaconda\nimport warnings\n\nwarnings.simplefilter('ignore')","99a1fd46":"# plots inisde jupyter notebook\n%matplotlib inline\nimport matplotlib.pyplot as plt","c66ae209":"import seaborn as sns\nsns.set()","bf0c8f8e":"# use svg for all plots within inline backend\n%config InlineBackend.figure_format = 'svg'","678de6dc":"# increase default plot size\nfrom pylab import rcParams\nrcParams['figure.figsize'] = 5, 4","6383a459":"import pandas as pd\npd.options.display.max_columns = 12\npd.options.display.max_rows = 24\n\n# disable warnings in Anaconda\nimport warnings\n\nwarnings.simplefilter('ignore')\n\n# plots inisde jupyter notebook\n%matplotlib inline\nimport matplotlib.pyplot as plt\n\nimport seaborn as sns\nsns.set()\n\n# use svg for all plots within inline backend\n%config InlineBackend.figure_format = 'svg'\n\n# increase default plot size\nfrom pylab import rcParams\nrcParams['figure.figsize'] = 5, 4","536ecba4":"df_train = pd.read_csv('..\/input\/train.csv')","4444bb7b":"df_train.head()","ac20d3ab":"df_train['date'] = pd.to_datetime(df_train['date'])\ndf_train.index = pd.DatetimeIndex(df_train['date'])\ndf_train.drop('date', axis=1, inplace=True)","a25cc0cb":"df_train.info()","9299bd2a":"from itertools import product, starmap\n\n\ndef storeitems():\n    return product(range(1,51), range(1,11))\n\n\ndef storeitems_column_names():\n    return list(starmap(lambda i,s: f'item_{i}_store_{s}_sales', storeitems()))\n\n\ndef sales_by_storeitem(df):\n    ret = pd.DataFrame(index=df.index.unique())\n    for i, s in storeitems():\n        ret[f'item_{i}_store_{s}_sales'] = df[(df['item'] == i) & (df['store'] == s)]['sales'].values\n    return ret","528fb8c5":"df_train = sales_by_storeitem(df_train)","f0e5d54a":"df_train.info()","4bd3b102":"# load data\ndf_test = pd.read_csv('..\/input\/test.csv')\ndf_test.head()","d450124e":"# strings to dates\ndf_test['date'] = pd.to_datetime(df_test['date'])\ndf_test.index = pd.DatetimeIndex(df_test['date'])\ndf_test.drop('date', axis=1, inplace=True)\ndf_test.info()","702b2a92":"# mock sales to use same transformations as in df_train\ndf_test['sales'] = np.zeros(df_test.shape[0])\ndf_test = sales_by_storeitem(df_test)\ndf_test.info()","b6d5f3fc":"# make sure all column names are the same and in the same order\ncol_names = list(zip(df_test.columns, df_train.columns))\nfor cn in col_names:\n    assert cn[0] == cn[1]","1ac5c3ee":"df_test['is_test'] = np.repeat(True, df_test.shape[0])\ndf_train['is_test'] = np.repeat(False, df_train.shape[0])\ndf_total = pd.concat([df_train, df_test])\ndf_total.info()","3fc1a722":"weekday_df = pd.get_dummies(df_total.index.weekday, prefix='weekday')\nweekday_df.index = df_total.index\nweekday_df.head()","4f1bfaf1":"month_df = pd.get_dummies(df_total.index.month, prefix='month')\nmonth_df.index =  df_total.index\nmonth_df.head()","a205f359":"df_total = pd.concat([weekday_df, month_df, df_total], axis=1)\ndf_total.info()","a7634b32":"assert df_total.isna().any().any() == False","ffea9c8e":"def shift_series(series, days):\n    return series.transform(lambda x: x.shift(days))\n\n\ndef shift_series_in_df(df, series_names=[], days_delta=90):\n    \"\"\"\n    Shift columns in df with names in series_names by days_delta.\n    \n    Negative days_delta will prepend future values to current date,\n    positive days_delta wil prepend past values to current date.\n    \"\"\"\n    ret = pd.DataFrame(index=df.index.copy())\n    str_sgn = 'future' if np.sign(days_delta) < 0 else 'past'\n    for sn in series_names:\n        ret[f'{sn}_{str_sgn}_{np.abs(days_delta)}'] = shift_series(df[sn], days_delta)\n    return ret\n\n    \ndef stack_shifted_sales(df, days_delta=90):\n    names = storeitems_column_names()\n    dfs = [df.copy()]\n    abs_range = range(1, days_delta+1) if days_delta > 0 else range(days_delta, 0)\n    for day_offset in abs_range:\n        delta = -day_offset\n        shifted = shift_series_in_df(df, series_names=names, days_delta=delta)\n        dfs.append(shifted)\n    return pd.concat(dfs, axis=1, copy=False)","95468865":"df_total = stack_shifted_sales(df_total, days_delta=-1)","135c5dee":"df_total = df_total.dropna()  # this should ONLY remove 1st row\ndf_total.info()","051aaa62":"# make sure stacked and standard sales columns appear in the same order:\nsales_cols = [col for col in df_total.columns if '_sales' in col and '_sales_' not in col]\nstacked_sales_cols = [col for col in df_total.columns if '_sales_' in col]\nother_cols = [col for col in df_total.columns if col not in set(sales_cols) and col not in set(stacked_sales_cols)]\n\nsales_cols = sorted(sales_cols)\nstacked_sales_cols = sorted(stacked_sales_cols)\n\nnew_cols = other_cols + stacked_sales_cols + sales_cols","8186f7b4":"df_total = df_total.reindex(columns=new_cols)","16bed126":"df_total.head()","4d41eec3":"df_total.tail()","fa23ea59":"df_total.describe()","cc56a177":"assert df_total.isna().any().any() == False","44078149":"from sklearn.preprocessing import MinMaxScaler\nfrom sklearn.model_selection import train_test_split","11d612a7":"cols_to_scale = [col for col in df_total.columns if 'weekday' not in col and 'month' not in col]","9ef0951d":"scaler = MinMaxScaler(feature_range=(0,1))\nscaled_cols = scaler.fit_transform(df_total[cols_to_scale])\ndf_total[cols_to_scale] = scaled_cols\ndf_total.head()","2d2e3e04":"df_total.describe()","a53e0d2a":"df_train = df_total[df_total['is_test'] == False].drop('is_test', axis=1)\ndf_test = df_total[df_total['is_test'] == True].drop('is_test', axis=1)","2cebc0d5":"df_train.info()","5e941483":"df_test.info()","edfd1f04":"X_cols_stacked = [col for col in df_train.columns if '_past_' in col]\nX_cols_caldata = [col for col in df_train.columns if 'weekday_' in col or 'month_' in col]\nX_cols = X_cols_stacked + X_cols_caldata\n\nX = df_train[X_cols]","f4b109d8":"X_colset = set(X_cols)\ny_cols = [col for col in df_train.columns if col not in X_colset]\n\ny = df_train[y_cols]","aa5dd9d7":"X.info()","fd183eec":"y.info()","9c666995":"# split values to train and test, use np arrays to allow reshaping\nX_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.2, shuffle=False)","f61cc1da":"# reshape inputs to be 3d, as in: https:\/\/machinelearningmastery.com\/multivariate-time-series-forecasting-lstms-keras\/\nX_train_vals = X_train.values.reshape((X_train.shape[0], 1, X_train.shape[1]))\nX_valid_vals = X_valid.values.reshape((X_valid.shape[0], 1, X_valid.shape[1]))","3f544e97":"from keras.models import Sequential, Model\nfrom keras.layers import *","30cd2087":"# model alternative 2 - double with conv1ds TODO\n# https:\/\/arxiv.org\/pdf\/1709.05206.pdf\ndef build_model():\n    inputs = Input(shape=(X_train_vals.shape[1], X_train_vals.shape[2]))\n    # top pipeline\n    top_lstm = LSTM(500, return_sequences=True)(inputs)\n    top_dense = Dense(500, activation='relu')(top_lstm)\n    # bottom pipeline\n    bottom_dense = Dense(500)(inputs)\n    bottom_conv1 = Conv1D(\n        500, \n        kernel_size=1,\n        input_shape=(X_train_vals.shape[1], X_train_vals.shape[2])\n    )(bottom_dense)\n    bottom_conv2 = Conv1D(\n        1000,\n        kernel_size=50,\n        padding='same',\n        activation='relu'\n    )(bottom_conv1)\n    bottom_conv3 = Conv1D(\n        500,\n        kernel_size=10,\n        padding='same',\n        activation='relu'\n    )(bottom_conv2)\n    bottom_pooling = AvgPool1D(\n        pool_size=10, \n        padding='same'\n    )(bottom_conv3)\n#     bottom_reshape = Reshape(\n#         target_shape=[500]\n#     )(bottom_conv3)\n    # concat output\n    final_concat = Concatenate()([top_dense, bottom_pooling])\n    final_lstm = LSTM(1000, dropout=0.2)(final_concat)\n    final_dense = Dense(500)(final_lstm)\n    # compile and return\n    model = Model(inputs=inputs, outputs=final_dense)\n    model.compile(loss='mean_absolute_error', optimizer='adam', metrics=['mape'])\n    return model\n\nmodel = build_model()","90bd7694":"history = model.fit(\n    X_train_vals, \n    y_train.values, \n    epochs=130, \n    batch_size=70,\n    validation_data=(X_valid_vals, y_valid.values),\n    verbose=2,\n    shuffle=False\n)","f8e3d325":"# plot history\nplt.plot(history.history['loss'], label='train')\nplt.plot(history.history['val_loss'], label='test')\nplt.legend()\nplt.show()","b32a596b":"X_valid.shape","f82f1cac":"y_valid.shape","01875377":"def model_eval(model, X_test, y_test, log_all=False):\n    \"\"\"\n    Model must have #predict method.\n    X_test, y_test - instances of pd.DataFrame (normal, not reshaped for LSTM !!!)\n    \n    Note that this function assumes that sales columns for previous values appear \n    in the same order as sales columns for current values.\n    \"\"\"\n    # prepare data\n    sales_x_cols = [col for col in X_test.columns if 'sales' in col]\n    sales_x_idxs = [X_test.columns.get_loc(col) for col in sales_x_cols]\n    sales_y_cols = [col for col in y_test.columns if 'sales' in col]\n    sales_y_idxs = [y_test.columns.get_loc(col) for col in sales_y_cols]\n    n_samples = y_test.shape[0]\n    y_pred = np.zeros(y_test.shape)\n    # iterate\n    x_next = X_test.iloc[0].values\n    for i in range(0, n_samples):\n        if log_all:\n            print('[x]', x_next)\n        x_arr = np.array([x_next])\n        x_arr = x_arr.reshape(x_arr.shape[0], 1, x_arr.shape[1])\n        y_pred[i] = model.predict(x_arr)[0]\n        try:\n            x_next = X_test.iloc[i+1].values\n            x_next[sales_x_idxs] = y_pred[i][sales_y_idxs]\n        except IndexError:\n            pass  # this happens on last iteration, and x_next does not matter anymore\n    return y_pred, y_test.values\n\ndef vector_smape(y_pred, y_real):\n    nom = np.abs(y_pred-y_real)\n    denom = (np.abs(y_pred) + np.abs(y_real)) \/ 2\n    results = nom \/ denom\n    return 100*np.mean(results)  # in percent, same as at kaggle","177ad673":"X_valid, y_valid = X_valid.head(90), y_valid.head(90)","a1af3a8b":"y_pred, y_real = model_eval(model, X_valid, y_valid)","5dea1fbe":"def unscale(y_arr, scaler, template_df, toint=False):\n    \"\"\"\n    Unscale array y_arr of model predictions, based on a scaler fitted \n    to template_df.\n    \"\"\"\n    tmp = template_df.copy()\n    tmp[y_cols] = pd.DataFrame(y_arr, index=tmp.index)\n    tmp[cols_to_scale] = scaler.inverse_transform(tmp[cols_to_scale])\n    if toint:\n        return tmp[y_cols].astype(int)\n    return tmp[y_cols]","95e7c23d":"template_df = pd.concat([X_valid, y_valid], axis=1)\ntemplate_df['is_test'] = np.repeat(True, template_df.shape[0])\n\npred = unscale(y_pred, scaler, template_df, toint=True)\nreal = unscale(y_real, scaler, template_df, toint=True)","1f821545":"smapes = [vector_smape(pred[col], real[col]) for col in pred.columns]","75b82848":"sns.distplot(smapes)","e7097854":"describe(smapes)","c7d4221a":"store, item = np.random.randint(1,11), np.random.randint(1,51)\nrandom_storeitem_col = f'item_{item}_store_{store}_sales'","46abdce9":"plot_lengths = [7, 30, 60, 365]\n\nfor pl in plot_lengths:\n    plt.plot(pred[random_storeitem_col].values[:pl], label='predicted')\n    plt.plot(real[random_storeitem_col].values[:pl], label='real')\n    plt.legend()\n    plt.show()","dcc85b86":"# make sure 1st row has correctly stacked sales\ndf_test[stacked_sales_cols].head(2)","9fb0f916":"# split to X and y\nX_test, y_test = df_test[X_cols], df_test[y_cols]","e990e104":"# y_test is basically blank, but allows us to use the same function\ny_test_pred, _ = model_eval(model, X_test, y_test)","0d19356f":"test_template_df = pd.concat([X_test, y_test], axis=1)\ntest_template_df['is_test'] = np.repeat(True, test_template_df.shape[0])\n\ntest_pred = unscale(y_test_pred, scaler, test_template_df, toint=True)","14213dc4":"test_pred.head()","d1738144":"plt.plot(test_pred['item_1_store_1_sales'].values)\nplt.show()","1cb966f0":"result = np.zeros(45000, dtype=np.int)\nfor i, s in storeitems():\n    slice_start_idx = 90*10*(i-1) + 90*(s-1)\n    slice_end_idx = slice_start_idx + 90\n    col_name = f'item_{i}_store_{s}_sales'\n    result[slice_start_idx:slice_end_idx] = test_pred[col_name].values\nresult = pd.DataFrame(result, columns=['sales'])\nresult.index.name = 'id'\nresult.head()","290402fe":"result.to_csv('basic_lstm.csv')","fa69749a":"Prepare data for calculating SMAPE scores:","f5b3718e":"## Shift sales \nDoing this on combined dataset allows us to have first value for test set already calculated.","0f2d919c":"Calculate SMAPE for each of the items","f3273b3e":"## Exporting to Kaggle\nTrain model on test set (already loaded and formatted) and save results in Kaggle format.","fd631ea1":"## 1hot encoding weekdays and months","29513c94":"## Shape for Keras LSTM","c6bb9230":"## Calculate SMAPE for the model\nSMAPE is calculated on a validation set for the model","fcb1126d":"## Combine test and train datasets","19185bc6":"## Sales for each storeitem","9e3cbd59":"# Load data","ec874c19":"## Split to X and y (for training)","7b6b7a1e":"## Split back to train and test set\n`df_total` will be still available - it will be necessary to reverse scaling on output data (sales predictions from the model)","31f9d593":"## Plot sample prediction","769c6304":"## Convert strings to dates","d3376040":"## Scaling\nWith combined datasets and shifted sales, we can now correctly min-max scale all data.","2f23acff":"## Training LSTM\nUsing features for all storeitems (stacked sales from previous day) to predict sales for one storeitem (sales for current day).\n\n500 features, 1826 samples","d931ff85":"## Test data\nLoading test data will allow  us to have correct scaling on entire set, and perform all necessary transformation on combined data."}}