{"cell_type":{"0f420bfe":"code","197d2750":"code","e5c2a9da":"code","202eca80":"code","66b583c0":"code","bb27b837":"code","414ceef8":"code","84e60a34":"code","6c14ea56":"code","700577a6":"code","96f72c16":"code","0c9b3067":"code","3830a36f":"code","b2f67ea5":"code","8c4e2251":"code","1aa6b427":"code","f345790e":"code","cd075eb8":"code","a265af13":"code","5c249ca9":"code","d3c29101":"code","455c0603":"code","6746e292":"code","906b0551":"code","beca8cfe":"code","8ad14b68":"code","dcfcf8e9":"code","25f0e877":"code","9df17bc4":"code","c1649eee":"code","89026a85":"code","58322e48":"code","272ac439":"code","aa6ae08a":"code","428c35db":"code","8c7ad22f":"code","397619a2":"code","2cfce730":"code","97e54685":"code","1ac1a877":"code","6e3139d4":"code","9c7f134e":"code","de54e982":"code","8956d0ce":"code","6e5ea053":"code","d2da921b":"code","d87e29ec":"code","845747de":"code","afb33bf7":"code","3ac56c7a":"code","26b9780b":"code","49fd2e19":"code","1d96b2a9":"code","8f6256f2":"code","dca48f2b":"code","d658e6ed":"code","7dcb6866":"code","359d96fb":"code","4315e5e1":"code","ff5820a6":"code","41c7549a":"code","c0156ef9":"code","c06353c5":"code","4a6422fc":"code","cd508282":"code","e17ca334":"code","6bc9c125":"code","eb269ef8":"code","95a91902":"code","71a04fad":"code","1ede801e":"code","0368ebae":"code","e35b6788":"code","5a4a0fc7":"code","4825f1e5":"code","9e220b59":"code","ba897fce":"code","7823dd28":"code","6b805ea0":"markdown","ac4b0e68":"markdown","d7c9bd69":"markdown","d55a3c66":"markdown","913a8573":"markdown","d18eab9e":"markdown","c7ff76a2":"markdown","7bc0f178":"markdown","03b6c363":"markdown","e66e0bd5":"markdown","e1e6021f":"markdown","3fbb7e3d":"markdown","7a45947d":"markdown","b87ecf8b":"markdown","955a38c1":"markdown","3005e7ca":"markdown","1edb9ca6":"markdown","81f303fb":"markdown","6fa4cf07":"markdown","de9262dc":"markdown","f6f1a55a":"markdown","20c3962e":"markdown","efc85572":"markdown","8c97d132":"markdown","69c3473b":"markdown","80db2061":"markdown","e2359e73":"markdown","ef831422":"markdown","66f6f4aa":"markdown","11b698a5":"markdown","0cc5bfe9":"markdown","214922ad":"markdown","df2680dc":"markdown","37d8e736":"markdown","ca47596e":"markdown","e36f5db1":"markdown","0c228f79":"markdown","78e96078":"markdown","70b46435":"markdown","6d056ff4":"markdown","0636dd85":"markdown","6d1d3cf7":"markdown","96347729":"markdown","ba162060":"markdown","8c1dcd62":"markdown","c8ec2279":"markdown","d8910167":"markdown","8b0e101d":"markdown","aa5b8904":"markdown","dbcc9d07":"markdown","7e9e4c87":"markdown","d5ea60d2":"markdown","7cd73c5b":"markdown","1c729bb3":"markdown","a6afe497":"markdown","0ce7e39f":"markdown","c392eb14":"markdown","79dcbf8d":"markdown","28cb24e7":"markdown","9fe8532f":"markdown","4eca1df7":"markdown","ad91f18b":"markdown","677883e7":"markdown","8bc5d067":"markdown","e03c9a22":"markdown","58cb9527":"markdown","42926cc7":"markdown","4f3baf6c":"markdown"},"source":{"0f420bfe":"import pandas as pd # data analytical library\nimport numpy as np #fast linear algebra\nimport matplotlib.pyplot as plt #visualization\nimport seaborn as sns #statistical visualization","197d2750":"df=pd.read_csv(\"..\/input\/machine-learning-24-hrs-hackathon\/train_SJC.csv\")","e5c2a9da":"df.head()","202eca80":"df=df.rename(columns={\"Unnamed: 0\":\"ClaimNumber\",\"Unnamed: 1\":\"DateTimeOfAccident\",\"Unnamed: 3\":\"Age\",\"Unnamed: 4\":\"Gender\",\n                      \"Unnamed: 5\":\"MaritalStatus\",\"Unnamed: 6\":\"DependentChildren\",\"Unnamed: 8\":\"WeeklyWages\",\n                      \"Unnamed: 9\":\"PartTimeFullTime\",\"Unnamed: 10\":\"HoursWorkedPerWeek\",\"Unnamed: 12\":\"ClaimDescription\",\n                      \"Unnamed: 13\":\"InitialIncurredCalimsCost\",\"Unnamed: 14\":'UltimateIncurredClaimCost'},inplace=False)","66b583c0":"df=df.drop(df.index[0])\ndf.head()","bb27b837":"df.describe() # displays only for numeric columns","414ceef8":"df.shape # the dataframe has 36176 rows and 17 columns","84e60a34":"df.info()","6c14ea56":"df.dtypes # few numeric datatypes are classified as string","700577a6":"df['Age'] = pd.to_numeric(df['Age'])\ndf['DependentChildren'] = pd.to_numeric(df['DependentChildren'])\ndf['WeeklyWages'] = df['WeeklyWages'].astype('float64')\ndf['HoursWorkedPerWeek'] = df['HoursWorkedPerWeek'].astype('float64')\ndf['InitialIncurredCalimsCost'] = pd.to_numeric(df['InitialIncurredCalimsCost'])\ndf['UltimateIncurredClaimCost'] = pd.to_numeric(df['UltimateIncurredClaimCost'])","96f72c16":"df.dtypes","0c9b3067":"df.isnull().sum()","3830a36f":"df['MaritalStatus'].value_counts() # categorical","b2f67ea5":"df['WeeklyWages'].value_counts() # continuous","8c4e2251":"df['HoursWorkedPerWeek'].value_counts() # continuous","1aa6b427":"df['MaritalStatus'] = df['MaritalStatus'].fillna('U')","f345790e":"sns.distplot(df['HoursWorkedPerWeek']) # Mean becuase data is not skewed","cd075eb8":"df['HoursWorkedPerWeek']=df['HoursWorkedPerWeek'].fillna(df['HoursWorkedPerWeek'].mean())","a265af13":"sns.distplot(df['WeeklyWages']) # Median because data is skewed slightly","5c249ca9":"df['WeeklyWages']=df['WeeklyWages'].fillna(df['WeeklyWages'].median())","d3c29101":"df.isnull().sum()","455c0603":"df.dtypes","6746e292":"df['DateTimeOfAccident']","906b0551":"df['DateTimeOfAccident']=pd.to_datetime(df['DateTimeOfAccident'])","beca8cfe":"df.dtypes ","8ad14b68":"df['YearOfAccident']=pd.to_datetime(df['DateTimeOfAccident']).dt.year","dcfcf8e9":"df['YearOfAccident']","25f0e877":"df.head() ","9df17bc4":"df['Age_Bin']=pd.cut(df['Age'],bins=3)\ndf['Age_Bin']","c1649eee":"df['Age_Bin']=pd.cut(df['Age'],bins=[12,35,57,80] , labels=['Youth','Adult','Senior_Citizen'])\ndf['Age_Bin']","89026a85":"df['Rating'] = pd.cut(df['InitialIncurredCalimsCost'],bins=[0,3500,7000,9500,12000,18000],labels=['1','2','3','4','5'])\ndf['Rating']","58322e48":"cat_data=df.select_dtypes(include=object)\ncat_data.columns","272ac439":"num_data=df.select_dtypes(exclude=object)\nnum_data.columns","aa6ae08a":"num_data.describe()","428c35db":"cat_data.describe()","8c7ad22f":"df.apply(pd.Series.nunique)","397619a2":"univariate=['Age_Bin','Gender','MaritalStatus','DependentChildren','DependentsOther','PartTimeFullTime','DaysWorkedPerWeek']\nfor col in univariate:\n    plt.figure(figsize = (6,4))\n    sns.countplot(x=col,data =df)","2cfce730":"df['UltimateIncurredClaimCost'].hist(bins=25)\nplt.show()\nsns.distplot(df['UltimateIncurredClaimCost'],color='g')\nplt.show()\nsns.boxplot(df['UltimateIncurredClaimCost'],color='r')","97e54685":"log_UltimateIncurredClaimCost=np.log(df['UltimateIncurredClaimCost'])","1ac1a877":"sns.boxplot(x = 'Gender', y = log_UltimateIncurredClaimCost,  data =df)","6e3139d4":"sns.barplot(x='PartTimeFullTime',y='UltimateIncurredClaimCost',data=df)","9c7f134e":"sns.barplot(x='DaysWorkedPerWeek',y='UltimateIncurredClaimCost',data=df)","de54e982":"sns.barplot(x='Age_Bin',y='UltimateIncurredClaimCost',data=df)","8956d0ce":"plt.figure(figsize=(15,6))\nsns.barplot(x='YearOfAccident',y='UltimateIncurredClaimCost',data=df)","6e5ea053":"sns.pairplot(df)","d2da921b":"corrmat = df.corr()\ntop_corr_features = corrmat.index\nplt.figure(figsize=(20,20))\n#plot heat map\ng=sns.heatmap(df[top_corr_features].corr(),annot=True,cmap=\"RdYlGn\")","d87e29ec":"plt.figure(figsize = (10,10))\nsns.heatmap(df.corr()[['UltimateIncurredClaimCost']] ,annot = True)","845747de":"df.plot.box(figsize=(25,6))","afb33bf7":"for i in range(4):\n\n    limit=3*df['InitialIncurredCalimsCost'].std()\n\n    lower_limit=df['InitialIncurredCalimsCost'].mean()-limit\n    upper_limit=df['InitialIncurredCalimsCost'].mean()+limit\n\n    df=df[(df['InitialIncurredCalimsCost']>lower_limit)&(df['InitialIncurredCalimsCost']<upper_limit)]\n\n    limit=3*df['UltimateIncurredClaimCost'].std()\n\n    lower_limit=df['UltimateIncurredClaimCost'].mean()-limit\n    upper_limit=df['UltimateIncurredClaimCost'].mean()+limit\n\n    df=df[(df['UltimateIncurredClaimCost']>lower_limit)&(df['UltimateIncurredClaimCost']<upper_limit)]","3ac56c7a":"df.plot.box(figsize=(25,6))","26b9780b":"response=df['UltimateIncurredClaimCost']\nresponse ","49fd2e19":"features=['ClaimNumber', 'DateReported', 'Age', 'Gender',\n          'MaritalStatus', 'DependentChildren', 'DependentsOther', 'WeeklyWages',\n          'PartTimeFullTime', 'HoursWorkedPerWeek', 'DaysWorkedPerWeek',\n          'ClaimDescription', 'InitialIncurredCalimsCost']\npred=df[features]\npred","1d96b2a9":"def linear_regression(data):\n    \n    from sklearn.linear_model import LinearRegression\n    import sklearn.preprocessing as pre\n    from sklearn.preprocessing import LabelEncoder\n    label_en=LabelEncoder()\n    from sklearn.model_selection import train_test_split\n    from sklearn.metrics import mean_squared_error\n    \n    assignment_dict=[]\n    \n    X_scale=data.drop(['UltimateIncurredClaimCost'],axis='columns')\n    cat_df=X_scale.select_dtypes(exclude=[float,int]).columns\n    for i in cat_df :\n        X_scale[str(i)]=label_en.fit_transform(X_scale[str(i)])\n    X_scale=X_scale.apply(pre.minmax_scale)\n    Y=data['UltimateIncurredClaimCost']\n    \n    \n    X=X_scale.copy()\n    \n    n=len(X_scale.columns)\n    \n    upper_index=3\n    lower_index=0\n    \n    while upper_index<=n:\n        \n        X_scale=X.drop(X.columns[lower_index:upper_index],axis='columns')\n        x_train,x_test,y_train,y_test=train_test_split(X_scale,Y,test_size=0.3,random_state=1234456)\n        dropped_cols=X.columns[lower_index:upper_index]\n        \n        glm=LinearRegression()\n        glm.fit(x_train,y_train)\n        y_pred=glm.predict(x_test)\n        \n        \n\n        assignment_dict.append({\n\n            'features':X_scale.columns,\n            'Number of features':len(X_scale.columns),\n            'dropped_features':dropped_cols,\n            'train_score':glm.score(x_train,y_train),\n            'test_score':glm.score(x_test,y_test),\n            'rmse' : mean_squared_error(y_test, y_pred, squared=False)\n        })\n\n        \n        \n        upper_index+=3\n        lower_index+=3\n    \n    dataframe=pd.DataFrame(assignment_dict)\n    assignment=dataframe.style.set_caption(\"Train and test scores for different features\")\n    pd.set_option('display.max_colwidth', None)\n    \n\n    return assignment\n\nlinear_regression(df)","8f6256f2":"def random_forest_regression(data):\n    \n    from sklearn.metrics import mean_squared_error\n    from sklearn.ensemble import RandomForestRegressor\n    import sklearn.preprocessing as pre\n    from sklearn.preprocessing import LabelEncoder\n    label_en=LabelEncoder()\n    from sklearn.model_selection import train_test_split\n    \n    assignment_dict=[]\n    \n    X_scale=data.drop(['UltimateIncurredClaimCost'],axis='columns')\n    cat_df=X_scale.select_dtypes(exclude=[float,int]).columns\n    for i in cat_df :\n        X_scale[str(i)]=label_en.fit_transform(X_scale[str(i)])\n    X_scale=X_scale.apply(pre.minmax_scale)\n    Y=data['UltimateIncurredClaimCost']\n    \n    \n    X=X_scale.copy()\n    \n    n=len(X_scale.columns)\n    \n    upper_index=3\n    lower_index=0\n    \n    while upper_index<=n:\n        \n        X_scale=X.drop(X.columns[lower_index:upper_index],axis='columns')\n        x_train,x_test,y_train,y_test=train_test_split(X_scale,Y,test_size=0.3,random_state=1234456)\n        dropped_cols=X.columns[lower_index:upper_index]\n        \n        rfr= RandomForestRegressor()\n        rfr.fit(x_train,y_train)\n        y_pred=rfr.predict(x_test)\n        \n        \n\n        assignment_dict.append({\n\n            'features':X_scale.columns,\n            'Number of features':len(X_scale.columns),\n            'dropped_features':dropped_cols,\n            'train_score':rfr.score(x_train,y_train),\n            'test_score':rfr.score(x_test,y_test),\n            'rmse' : mean_squared_error(y_test, y_pred, squared=False)\n        })\n\n        \n        \n        upper_index+=3\n        lower_index+=3\n    \n    dataframe=pd.DataFrame(assignment_dict)\n    assignment=dataframe.style.set_caption(\"Train and test scores for different features\")\n    pd.set_option('display.max_colwidth', None)\n    \n\n    return assignment\nrandom_forest_regression(df)","dca48f2b":"import sklearn.preprocessing as pre\nle=pre.LabelEncoder()","d658e6ed":"df_copy=df","7dcb6866":"for x in df.select_dtypes(include='object').columns:\n    df[x]=le.fit_transform(df[x])\n","359d96fb":"df.head()","4315e5e1":"response=df['UltimateIncurredClaimCost']\nresponse ","ff5820a6":"imp_features=['ClaimNumber', 'DateReported', 'Age','MaritalStatus','WeeklyWages','HoursWorkedPerWeek', \n             'ClaimDescription', 'InitialIncurredCalimsCost']\npredictor=df[imp_features]\npredictor","41c7549a":"df_scale=pre.minmax_scale(predictor)\ndf_scale=pd.DataFrame(df_scale,columns=imp_features)\ndf_scale","c0156ef9":"from sklearn.model_selection import train_test_split\nx_train, x_test, y_train, y_test = train_test_split(predictor, response, test_size=0.3, random_state=0)","c06353c5":"from sklearn.ensemble import RandomForestRegressor","4a6422fc":"rfr=RandomForestRegressor()","cd508282":"rfr.fit(x_train,y_train)\ny_pred=rfr.predict(x_test)\ny_pred","e17ca334":"from sklearn.metrics import mean_squared_error\nmean_squared_error(y_test, y_pred, squared=False)","6bc9c125":"test_data=pd.read_csv(\"..\/input\/machine-learning-24-hrs-hackathon\/Test_SJC.csv\",usecols=imp_features)\ntest_data.head()","eb269ef8":" test_data.isnull().sum()","95a91902":"test_data.dtypes","71a04fad":"df_scale.head()","1ede801e":"test_data.columns","0368ebae":" test_data['MaritalStatus'] = test_data['MaritalStatus'].fillna('U')","e35b6788":"label_encoder=pre.LabelEncoder()\ntest_data['ClaimNumber']=label_encoder.fit_transform(test_data['ClaimNumber'])\ntest_data['ClaimNumber']=label_encoder.fit_transform(test_data['ClaimNumber'])\ntest_data['DateReported']=label_encoder.fit_transform(test_data['DateReported'])\ntest_data['MaritalStatus']=label_encoder.fit_transform(test_data['MaritalStatus'])\ntest_data['ClaimDescription']=label_encoder.fit_transform(test_data['ClaimDescription'])","5a4a0fc7":"def test_pre(data):\n   \n    import sklearn.preprocessing as pre\n    from sklearn.preprocessing import minmax_scale\n    label_encoder=pre.LabelEncoder()\n    data=data.apply(minmax_scale)\n    data['MaritalStatus']=label_encoder.fit_transform(data['MaritalStatus'])\n    data['ClaimDescription']=label_encoder.fit_transform(data['ClaimDescription'])\n    return data\n    ","4825f1e5":"test=test_pre(test_data)","9e220b59":"y_pred=rfr.predict(test)","ba897fce":"result=y_pred","7823dd28":"def submission(result):\n    submission = pd.read_csv(\"..\/input\/machine-learning-24-hrs-hackathon\/sample_submission.csv\")\n    submission = submission.drop('UltimateIncurredClaimCost',axis=1)\n    submission['UltimateIncurredClaimCost'] = result\n    #Writing the file\n    submission.to_csv(\"submit.csv\",index=False)\n    \nsubmission(result)","6b805ea0":"##### Performing mean imputation on HoursWorkedPerWeek(continuous) data","ac4b0e68":"## Import libraries","d7c9bd69":"### Min Max scaling","d55a3c66":"#### To find the shape\/size of the data","913a8573":"##### Since it takes more time to run,I have made it markdown\n\n#### It gives very high RMSE value hence ignored","d18eab9e":"#### Transforming DateTimeOfAccident to datetime datatype","c7ff76a2":"##### From the above 3 plots , namely histogram, distplot and boxplot we can infer the target variable is highly poitively skewed","7bc0f178":"##### people working 6 days a week have got higher insurance","03b6c363":"The results given from considering the parameters obtained from the above Randomized search Cv were not better, hence ignored. ","e66e0bd5":"# SVR","e1e6021f":"#### checking the type of data of missing value columns","3fbb7e3d":"##### Classifying the missing WeeklyWages with median","7a45947d":"#### To check the datatypes of the features","b87ecf8b":"Chose Random Forest as it gives lower RMSE score","955a38c1":"##### As the year increases the amount given as insurance also increases","3005e7ca":"def support_vector_regression(data):\n    \n    from sklearn.svm import SVR\n    import sklearn.preprocessing as pre\n    from sklearn.preprocessing import LabelEncoder\n    label_en=LabelEncoder()\n    from sklearn.model_selection import train_test_split\n    \n    assignment_dict=[]\n    \n    X_scale=data.drop(['UltimateIncurredClaimCost'],axis='columns')\n    cat_df=X_scale.select_dtypes(exclude=[float,int]).columns\n    for i in cat_df :\n        X_scale[str(i)]=label_en.fit_transform(X_scale[str(i)])\n    X_scale=X_scale.apply(pre.minmax_scale)\n    Y=data['UltimateIncurredClaimCost']\n    \n    \n    X=X_scale.copy()\n    \n    n=len(X_scale.columns)\n    \n    upper_index=3\n    lower_index=0\n    \n    while upper_index<=n:\n        \n        X_scale=X.drop(X.columns[lower_index:upper_index],axis='columns')\n        x_train,x_test,y_train,y_test=train_test_split(X_scale,Y,test_size=0.3,random_state=1234456)\n        dropped_cols=X.columns[lower_index:upper_index]\n        \n        regressor = SVR(kernel = 'rbf')\n        regressor.fit(x_train, y_train)\n        \n        \n        \n\n        assignment_dict.append({\n\n            'features':X_scale.columns,\n            'Number of features':len(X_scale.columns),\n            'dropped_features':dropped_cols,\n            'train_score':regressor.score(x_train,y_train),\n            'test_score':regressor.score(x_test,y_test)\n        })\n\n        \n        \n        upper_index+=3\n        lower_index+=3\n    \n    dataframe=pd.DataFrame(assignment_dict)\n    assignment=dataframe.style.set_caption(\"Train and test scores for different features\")\n    pd.set_option('display.max_colwidth', None)\n    \n\n    return assignment\nsupport_vector_regression(df)","1edb9ca6":"#### Statistical Analysis","81f303fb":"print(model.feature_importances_)\n\nfeat_importances = pd.Series(model.feature_importances_, index=predictor.columns)\n\nfeat_importances.nlargest(8).plot(kind='barh')\n\nplt.show()","6fa4cf07":"## Random forest","de9262dc":"##### To check if the datatypes are changed","f6f1a55a":"##### Average incurred claims cost female are the higer amount than male","20c3962e":"##### Analysing basic descriptions of the dataset","efc85572":"Choosing ['ClaimNumber', 'DateTimeOfAccident', 'DateReported', 'Age', 'Gender', 'MaritalStatus', 'DependentChildren', 'DependentsOther', 'WeeklyWages', 'ClaimDescription', 'InitialIncurredCalimsCost', 'YearOfAccident', 'Age_Bin'] gives better score of {0.686777,0.692278} for train and test scores with rmse value 1897.786500","8c97d132":"## To optimize the Random forest regressor","69c3473b":"#### Treatment","80db2061":"##### Retaining only the year of accident from date-time","e2359e73":"##### Verify if there are no missing values","ef831422":"#### Descriptive statistics using describe function","66f6f4aa":"## Model Building","11b698a5":"# Randomized Search CV\n#Number of trees in random forest\nn_estimators = [int(x) for x in np.linspace(start = 100, stop = 1200, num = 12)]\n\n#Number of features to consider at every split\nmax_features = ['auto', 'sqrt']\n\n#Maximum number of levels in tree\nmax_depth = [int(x) for x in np.linspace(5, 30, num = 6)]\n\n#Minimum number of samples required to split a node\nmin_samples_split = [2, 5, 10, 15, 100]\n\n#Minimum number of samples required at each leaf node\nmin_samples_leaf = [1, 2, 5, 10]\n\n### Create the random grid\nrandom_grid = {'n_estimators': n_estimators,\n               'max_features': max_features,\n               'max_depth': max_depth,\n               'min_samples_split': min_samples_split,\n               'min_samples_leaf': min_samples_leaf}\n\nprint(random_grid)\n\nrf = RandomForestRegressor()\n\nfrom sklearn.model_selection import RandomizedSearchCV\n\nrf_random = RandomizedSearchCV(estimator = rf, param_distributions = random_grid,scoring='neg_mean_squared_error', n_iter = 5, cv = 5, verbose=2, random_state=42, n_jobs = 1)\n\n### Fitting the model and finding best parameters and score\n\nrf_random.fit(x_train,y_train)\n\nrf_random.best_params_\n\nrf_random.best_score_","0cc5bfe9":"##### Viewing the distribution of target variable using dist plot","214922ad":"##### Since the headers are not named in most cases, renaming it using dictionary","df2680dc":"##### Biining InitialIncurredCalimsCost into Rankings based on the cost to show the value of it","37d8e736":"### Finding which columns have missing values","ca47596e":"# Reading and testing the built model on test data","e36f5db1":"##### Converting some features that are classified as object data type to numeric for better analysis","0c228f79":"## Reading the dataset","78e96078":"from sklearn.ensemble import ExtraTreesRegressor\n\nimport matplotlib.pyplot as plt\n\nmodel = ExtraTreesRegressor()\n\nmodel.fit(pred,response)","70b46435":"##### Header column is repeated in the first row, hence remove it using drop function and display first 5 rows","6d056ff4":"## Model","0636dd85":"##### to check if the yearOfAccident column is created","6d1d3cf7":"## Missing value analysis and treatment","96347729":"##### Part time workers have got higher insurance","ba162060":"##### Separating the response and predictor variables","8c1dcd62":"Choosing ['ClaimNumber', 'DateTimeOfAccident', 'DateReported', 'Age', 'Gender', 'MaritalStatus', 'DependentChildren', 'DependentsOther', 'WeeklyWages', 'ClaimDescription', 'InitialIncurredCalimsCost', 'YearOfAccident', 'Age_Bin'] gives better score of {0.963530,0.749708} for train and test scores with rmse value 1662.737423","c8ec2279":"##### From the above plots we can infer the following\n1. Youth have applied the most for insurance\n2. Number of Males who applied for insurance are way higher than females\n3. The marital status for most of the insurance holders are single\n4. Most of them do not have children or dependencies\n5. full time workers have applied more for the policy\n6. Among those who applied, most of them work 5 days a week\n","d8910167":"### Feature importance","8b0e101d":"## Submitting the code and writing it in the form of csv","aa5b8904":"##### Binning the age column","dbcc9d07":"##### Split train and test","7e9e4c87":"### Univariate Analysis","d5ea60d2":"## Linear Regression","7cd73c5b":"Bar plots","1c729bb3":"##### Classifying the missing marital status as unknown","a6afe497":"### Outlier Analysis and treatment","0ce7e39f":"### Bivariate Analysis","c392eb14":"##### From the above 3 plots we can infer that the most important feature responsible for predicting the ultimateIncurredClainCost is  InitiaIncurredClaimCost ","79dcbf8d":"#### To find a concise summary of the dataframe using info function","28cb24e7":"#### Encoding the categorical variables to numeric using LabelEncoder","9fe8532f":"## Exploratory Data Analysis","4eca1df7":"## Pre-processing","ad91f18b":"##### Senior citizens have got highest insurance, as the age increases the insurance also increases","677883e7":"#### Binning","8bc5d067":"##### To display first 5 rows of the dataset","e03c9a22":"## Data Tranformation","58cb9527":"# OVERVIEW OF THE PROJECT\nI will give a brief outline of my approach towards solving the given problem statement.\n\n**Problem Statement** : to predict Workers Compensation claims using the realistic data\nI figured out that it is a regression problem by looking at the data and the problem statement.With this in mind,I approched solving the problem in the following way.\n### Approach :\n1. Pre-processing:\n   a) Initially I imported the required linbraries, loaded the data to find out how it looks like and did some processing to name the headers properly. \n   b) In order to understand basic description of the data, I found out the summary, shape, datatypes of the data.\n   c) found out the missing values and treated them in required manner.\n   d) transformed the data-time columns, changed the datatypes of few columns and binned few columns to make better analysis\n\n2. Exploratory Data Analysis:\n   a) Started the EDA with basic statistical analysis by separating the numeric and categorical variables\n   b) Created charts in 3 sections : Univariate analysis, bivariate analysis and Multivariate analysis and drew various insights of the data\n\n3. Outlier Analysis: \n   In order take care of the outliers that might affect the working of the model, with the help of boxplot analysed which columns have outliers and treated them by writing a function.\n\n4. Model Building: \n   a)In order to select the model I built 3 models namely Linear Regression, Support Vector Regressor and Random Forest Regressor. \n   b)After building these models by writing a single function that can encode, scale, build the model, find rmse value, and the best parameters to get the best score, I concluded that Random Forest Regressor performs better with lower RMSE value.\n   c) After selecting the model as Random Forest Regressor, I decided to optimize the model. Using feature importance, I selected the featured, performed Label encoding and min max scaling, tried randomized search CV to get the best parameters( but did not use it as it did not give good results) and finally obtained the Rmse score\n   \n5. Testing the model:\n   After training the model on the train data, I tested it on the given test data by preprocessing the test data and predicting the target variable using the built model.\n\n","42926cc7":"### Multivariate analysis","4f3baf6c":"##### Analysing the count of different discrete freautures using count plot"}}