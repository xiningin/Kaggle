{"cell_type":{"686bfed8":"code","05981908":"code","8b59b862":"code","a0399645":"code","8ad8daf9":"code","977a8952":"code","877cf373":"code","76a3ebe3":"code","7605b708":"code","a0d18481":"code","1e341d7b":"code","b9b34749":"code","63593fda":"code","89f24183":"code","8fd8bd32":"code","fa791cef":"code","fd1a3e7a":"code","431f1bd2":"code","a1bf814c":"code","8f8a0341":"code","74be7b18":"code","9f85b2d2":"code","aa50b105":"code","9f6ce38a":"markdown","2d5abe3a":"markdown","b78a23b7":"markdown","0ec45978":"markdown","f062d697":"markdown","1a34bb13":"markdown","7567780d":"markdown","f5cbcb56":"markdown"},"source":{"686bfed8":"#===========================================================\n# Library\n#===========================================================\nimport os\nimport gc\nfrom logging import getLogger, INFO, StreamHandler, FileHandler, Formatter\nfrom contextlib import contextmanager\nimport time\n\nimport numpy as np\nimport pandas as pd\nimport scipy as sp\nimport random\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom functools import partial\n\nfrom sklearn.model_selection import StratifiedKFold, GroupKFold, KFold\nfrom sklearn import preprocessing\nimport category_encoders as ce\nfrom sklearn.metrics import mean_squared_error\n\nimport torch\n\nimport lightgbm as lgb\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")","05981908":"os.listdir('..\/input\/trends-assessment-prediction\/')","8b59b862":"#===========================================================\n# Utils\n#===========================================================\ndef get_logger(filename='log'):\n    logger = getLogger(__name__)\n    logger.setLevel(INFO)\n    handler1 = StreamHandler()\n    handler1.setFormatter(Formatter(\"%(message)s\"))\n    handler2 = FileHandler(filename=f\"{filename}.log\")\n    handler2.setFormatter(Formatter(\"%(message)s\"))\n    logger.addHandler(handler1)\n    logger.addHandler(handler2)\n    return logger\n\nlogger = get_logger()\n\n\n@contextmanager\ndef timer(name):\n    t0 = time.time()\n    yield\n    logger.info(f'[{name}] done in {time.time() - t0:.0f} s')\n\n\ndef seed_everything(seed=777):\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True\n\n    \ndef load_df(path, df_name, debug=False):\n    if path.split('.')[-1]=='csv':\n        df = pd.read_csv(path)\n        if debug:\n            df = pd.read_csv(path, nrows=1000)\n    elif path.split('.')[-1]=='pkl':\n        df = pd.read_pickle(path)\n    if logger==None:\n        print(f\"{df_name} shape \/ {df.shape} \")\n    else:\n        logger.info(f\"{df_name} shape \/ {df.shape} \")\n    return df","a0399645":"#===========================================================\n# Config\n#===========================================================\nOUTPUT_DICT = ''\n\nID = 'Id'\nTARGET_COLS = ['age', 'domain1_var1', 'domain1_var2', 'domain2_var1', 'domain2_var2']\nSEED = 42\nseed_everything(seed=SEED)\n\nN_FOLD = 5","8ad8daf9":"train = pd.read_csv('..\/input\/trends-assessment-prediction\/train_scores.csv', dtype={'Id':str})\\\n            .dropna().reset_index(drop=True) # to make things easy\nreveal_ID = pd.read_csv('..\/input\/trends-assessment-prediction\/reveal_ID_site2.csv', dtype={'Id':str})\nICN_numbers = pd.read_csv('..\/input\/trends-assessment-prediction\/ICN_numbers.csv')\nloading = pd.read_csv('..\/input\/trends-assessment-prediction\/loading.csv', dtype={'Id':str})\nfnc = pd.read_csv('..\/input\/trends-assessment-prediction\/fnc.csv', dtype={'Id':str})\nsample_submission = pd.read_csv('..\/input\/trends-assessment-prediction\/sample_submission.csv', dtype={'Id':str})","977a8952":"train.head()","877cf373":"reveal_ID.head()","76a3ebe3":"ICN_numbers.head()","7605b708":"loading.head()","a0d18481":"fnc.head()","1e341d7b":"sample_submission.head()","b9b34749":"sample_submission['ID_num'] = sample_submission[ID].apply(lambda x: int(x.split('_')[0]))\ntest = pd.DataFrame({ID: sample_submission['ID_num'].unique().astype(str)})\ndel sample_submission['ID_num']; gc.collect()\ntest.head()","63593fda":"# merge\ntrain = train.merge(loading, on=ID, how='left')\ntrain = train.merge(fnc, on=ID, how='left')\ntrain.head()","89f24183":"# merge\ntest = test.merge(loading, on=ID, how='left')\ntest = test.merge(fnc, on=ID, how='left')\ntest.head()","8fd8bd32":"folds = train[[ID]+TARGET_COLS].copy()\nFold = KFold(n_splits=N_FOLD, shuffle=True, random_state=SEED)\nfor n, (train_index, val_index) in enumerate(Fold.split(folds, folds[TARGET_COLS])):\n    folds.loc[val_index, 'fold'] = int(n)\nfolds['fold'] = folds['fold'].astype(int)\nfolds.head()","fa791cef":"#===========================================================\n# model\n#===========================================================\ndef run_single_lightgbm(param, train_df, test_df, folds, features, target, fold_num=0, categorical=[]):\n    \n    trn_idx = folds[folds.fold != fold_num].index\n    val_idx = folds[folds.fold == fold_num].index\n    logger.info(f'len(trn_idx) : {len(trn_idx)}')\n    logger.info(f'len(val_idx) : {len(val_idx)}')\n    \n    if categorical == []:\n        trn_data = lgb.Dataset(train_df.iloc[trn_idx][features],\n                               label=target.iloc[trn_idx])\n        val_data = lgb.Dataset(train_df.iloc[val_idx][features],\n                               label=target.iloc[val_idx])\n    else:\n        trn_data = lgb.Dataset(train_df.iloc[trn_idx][features],\n                               label=target.iloc[trn_idx],\n                               categorical_feature=categorical)\n        val_data = lgb.Dataset(train_df.iloc[val_idx][features],\n                               label=target.iloc[val_idx],\n                               categorical_feature=categorical)\n\n    oof = np.zeros(len(train_df))\n    predictions = np.zeros(len(test_df))\n\n    num_round = 10000\n\n    clf = lgb.train(param,\n                    trn_data,\n                    num_round,\n                    valid_sets=[trn_data, val_data],\n                    verbose_eval=100,\n                    early_stopping_rounds=100)\n\n    oof[val_idx] = clf.predict(train_df.iloc[val_idx][features], num_iteration=clf.best_iteration)\n\n    fold_importance_df = pd.DataFrame()\n    fold_importance_df[\"Feature\"] = features\n    fold_importance_df[\"importance\"] = clf.feature_importance(importance_type='gain')\n    fold_importance_df[\"fold\"] = fold_num\n\n    predictions += clf.predict(test_df[features], num_iteration=clf.best_iteration)\n    \n    # RMSE\n    logger.info(\"fold{} RMSE score: {:<8.5f}\".format(fold_num, np.sqrt(mean_squared_error(target[val_idx], oof[val_idx]))))\n    \n    return oof, predictions, fold_importance_df\n\n\ndef run_kfold_lightgbm(param, train, test, folds, features, target, n_fold=5, categorical=[]):\n    \n    logger.info(f\"================================= {n_fold}fold lightgbm =================================\")\n    \n    oof = np.zeros(len(train))\n    predictions = np.zeros(len(test))\n    feature_importance_df = pd.DataFrame()\n\n    for fold_ in range(n_fold):\n        print(\"Fold {}\".format(fold_))\n        _oof, _predictions, fold_importance_df = run_single_lightgbm(param,\n                                                                     train,\n                                                                     test,\n                                                                     folds,\n                                                                     features,\n                                                                     target,\n                                                                     fold_num=fold_,\n                                                                     categorical=categorical)\n        feature_importance_df = pd.concat([feature_importance_df, fold_importance_df], axis=0)\n        oof += _oof\n        predictions += _predictions \/ n_fold\n\n    # RMSE\n    logger.info(\"CV RMSE score: {:<8.5f}\".format(np.sqrt(mean_squared_error(target, oof))))\n\n    logger.info(f\"=========================================================================================\")\n    \n    return feature_importance_df, predictions, oof\n\n    \ndef show_feature_importance(feature_importance_df, name):\n    cols = (feature_importance_df[[\"Feature\", \"importance\"]]\n            .groupby(\"Feature\")\n            .mean()\n            .sort_values(by=\"importance\", ascending=False)[:50].index)\n    best_features = feature_importance_df.loc[feature_importance_df.Feature.isin(cols)]\n\n    plt.figure(figsize=(8, 16))\n    sns.barplot(x=\"importance\", y=\"Feature\", data=best_features.sort_values(by=\"importance\", ascending=False))\n    plt.title('Features importance (averaged\/folds)')\n    plt.tight_layout()\n    plt.savefig(OUTPUT_DICT+f'feature_importance_{name}.png')","fd1a3e7a":"prediction_dict = {}\noof_dict = {}\n\nfor TARGET in TARGET_COLS:\n    \n    logger.info(f'### LGB for {TARGET} ###')\n\n    target = train[TARGET]\n    test[TARGET] = np.nan\n\n    # features\n    cat_features = []\n    num_features = [c for c in test.columns if (test.dtypes[c] != 'object') & (c not in cat_features)]\n    features = num_features + cat_features\n    drop_features = [ID] + TARGET_COLS\n    features = [c for c in features if c not in drop_features]\n\n    if cat_features:\n        ce_oe = ce.OrdinalEncoder(cols=cat_features, handle_unknown='impute')\n        ce_oe.fit(train)\n        train = ce_oe.transform(train)\n        test = ce_oe.transform(test)\n        \n    lgb_param = {'objective': 'regression',\n             'metric': 'rmse',\n             'boosting_type': 'gbdt',\n             'learning_rate': 0.03,\n             'seed': SEED,\n             'max_depth': -1,\n             'verbosity': -1,\n            }\n\n    feature_importance_df, predictions, oof = run_kfold_lightgbm(lgb_param, train, test, folds, features, target, \n                                                                 n_fold=N_FOLD, categorical=cat_features)\n    \n    prediction_dict[TARGET] = predictions\n    oof_dict[TARGET] = oof\n    \n    show_feature_importance(feature_importance_df, TARGET)","431f1bd2":"oof_df = pd.DataFrame()\n\nfor TARGET in TARGET_COLS:\n    oof_df[TARGET] = oof_dict[TARGET]","a1bf814c":"# https:\/\/www.kaggle.com\/akurmukov\/trends-starter-rf-0-168-lb-metric\n\ndef lb_metric(y_true, y_pred):\n    '''Computes lb metric, both y_true and y_pred should be DataFrames of shape n x 5'''\n    y_true = y_true[['age', 'domain1_var1', 'domain1_var2','domain2_var1','domain2_var2']]\n    y_pred = y_pred[['age', 'domain1_var1', 'domain1_var2','domain2_var1','domain2_var2']]\n    weights = np.array([.3, .175, .175, .175, .175])\n    return np.sum(weights * np.abs(y_pred.values - y_true.values).sum(axis=0) \/ y_true.values.sum(axis=0))","8f8a0341":"score = lb_metric(train, oof_df)\nlogger.info(f'Local Score: {score}')","74be7b18":"sample_submission.head()","9f85b2d2":"pred_df = pd.DataFrame()\n\nfor TARGET in TARGET_COLS:\n    tmp = pd.DataFrame()\n    tmp[ID] = [f'{c}_{TARGET}' for c in test[ID].values]\n    tmp['Predicted'] = prediction_dict[TARGET]\n    pred_df = pd.concat([pred_df, tmp])\n\nprint(pred_df.shape)\nprint(sample_submission.shape)\n\npred_df.head()","aa50b105":"submission = sample_submission.drop(columns='Predicted').merge(pred_df, on=ID, how='left')\nprint(submission.shape)\nsubmission.to_csv('submission.csv', index=False)\nsubmission.head()","9f6ce38a":"# Submission","2d5abe3a":"# Library","b78a23b7":"# Data Loading","0ec45978":"# Utils","f062d697":"# Config","1a34bb13":"# MODEL","7567780d":"# Prepare folds","f5cbcb56":"# FE"}}