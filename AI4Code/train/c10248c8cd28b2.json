{"cell_type":{"e58dd78e":"code","2856b08c":"code","83b00152":"code","fa88f5fc":"code","d1e29379":"code","07d0ea66":"code","3a35ec27":"code","ec7e0d6b":"code","e2b0624e":"code","0eb7d955":"code","faf0bb1f":"code","c8ccd6c0":"code","ba5a07b8":"code","0db2cd0e":"code","42bea600":"code","d7fad0f8":"code","f8f3de60":"code","4c683b02":"code","f72230d7":"code","4bed18c9":"code","4de9b518":"code","baf956fe":"code","41358940":"code","1f5b77b2":"markdown","b1f2b613":"markdown","f2ebc188":"markdown","0a0e8ba0":"markdown","78e982ca":"markdown","af6ce162":"markdown","c0f1ee76":"markdown","260bd34b":"markdown","df19f77e":"markdown","9a726d49":"markdown","d681eff1":"markdown","7a73d47b":"markdown","5f52e365":"markdown","6b1495fd":"markdown","c3014f2b":"markdown"},"source":{"e58dd78e":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n","2856b08c":"import numpy as np\nimport pandas as pd\nimport os\n\nimport matplotlib.pyplot as plt\n%matplotlib inline\nfrom tqdm import tqdm_notebook\npd.options.display.precision = 15\n\nimport time\nimport datetime\nimport gc\nimport seaborn as sns\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nimport json\n# import altair as alt\nimport matplotlib.pyplot as plt\n%matplotlib inline\n# alt.renderers.enable('notebook')\n\nimport plotly.graph_objs as go\n# import plotly.plotly as py\nimport plotly.offline as pyo\nfrom plotly import tools\nfrom plotly.offline import download_plotlyjs, init_notebook_mode, plot, iplot\nimport plotly_express as px\ninit_notebook_mode(connected=True)\nfrom matplotlib import cm\nfrom matplotlib import pyplot as plt\nimport seaborn as sns\n\nfrom sklearn.decomposition import PCA\nfrom sklearn.cluster import KMeans\n\nimport plotly_express as px","83b00152":"folder_path = '..\/input\/ieee-fraud-detection\/\/'\ntrain_identity = pd.read_csv(f'{folder_path}train_identity.csv')\ntrain_transaction = pd.read_csv(f'{folder_path}train_transaction.csv')\n# test_identity = pd.read_csv(f'{folder_path}test_identity.csv')\n# test_transaction = pd.read_csv(f'{folder_path}test_transaction.csv')\nsub = pd.read_csv(f'{folder_path}sample_submission.csv')\n# let's combine the data and work with the whole dataset\n# I will save this for later\n# train = pd.merge(train_transaction, train_identity, on='TransactionID', how='left')\n# test = pd.merge(test_transaction, test_identity, on='TransactionID', how='left')","fa88f5fc":"print(f'Train Transaction dataset has {train_transaction.shape[0]} rows and {train_transaction.shape[1]} columns.')","d1e29379":"train_transaction = train_transaction.sample(n=10000)\ntrain_transaction.head()","07d0ea66":"print(f'Train Transaction dataset has {train_transaction.shape[0]} rows and {train_transaction.shape[1]} columns.')","3a35ec27":"def make_day_feature(df, offset=0, tname='TransactionDT'):\n    \"\"\"\n    Creates a day of the week feature, encoded as 0-6. \n    \n    Parameters:\n    -----------\n    df : pd.DataFrame\n        df to manipulate.\n    offset : float (default=0)\n        offset (in days) to shift the start\/end of a day.\n    tname : str\n        Name of the time column in df.\n    \"\"\"\n    # found a good offset is 0.58\n    days = df[tname] \/ (3600*24)        \n    encoded_days = np.floor(days-1+offset) % 7\n    return encoded_days\n\ndef make_hour_feature(df, tname='TransactionDT'):\n    \"\"\"\n    Creates an hour of the day feature, encoded as 0-23. \n    \n    Parameters:\n    -----------\n    df : pd.DataFrame\n        df to manipulate.\n    tname : str\n        Name of the time column in df.\n    \"\"\"\n    hours = df[tname] \/ (3600)        \n    encoded_hours = np.floor(hours) % 24\n    return encoded_hours\n\nSTART_DATE = '2017-12-01'\nstartdate = datetime.datetime.strptime(START_DATE, '%Y-%m-%d')","ec7e0d6b":"train_transaction['TransactionDateTime'] = train_transaction['TransactionDT'].apply(lambda x: (startdate + datetime.timedelta(seconds = x)))\ntrain_transaction['TransactionDate'] = [x.date() for x in train_transaction['TransactionDateTime']]\ntrain_transaction['TransactionHour'] = train_transaction.TransactionDT \/\/ 3600\ntrain_transaction['TransactionHourOfDay'] = train_transaction['TransactionHour'] % 24\ntrain_transaction['TransactionDay'] = train_transaction.TransactionDT \/\/ (3600 * 24)","e2b0624e":"train_transaction.head(10)","0eb7d955":"trx_colnames = train_transaction.columns\ntrx_colnames_core_num = ['isFraud', 'TransactionAmt', 'card1','card2', 'card3','card5']\ntrx_colnames_core_cat = ['ProductCD', 'card4', 'card6', 'P_emaildomain', 'R_emaildomain']\ntrx_colnames_C = [c for c in trx_colnames if c.startswith(\"C\") ]\ntrx_colnames_V = [c for c in trx_colnames if c.startswith(\"V\") ]\ntrx_colnames_M = [c for c in trx_colnames if c.startswith(\"M\") ]","faf0bb1f":"agg_dict = {}\nfor col in trx_colnames_core_num:\n    agg_dict[col] = ['mean','sum']\ntrain_trx_hour = train_transaction.groupby(['TransactionHour']).agg(agg_dict).reset_index()\ntrain_trx_hour.columns = ['_'.join(col).strip() for col in train_trx_hour.columns.values]\ntrain_trx_hour.head()","c8ccd6c0":"import math\ndf = train_trx_hour\nplotted_columns = train_trx_hour.columns[1:]\nlencols = len(plotted_columns)\nfig, axes = plt.subplots(math.ceil(lencols\/\/3),3,figsize=(12,lencols))\nfor i, metrics in enumerate(plotted_columns):\n    df.plot(x='TransactionHour_',y=metrics,title=metrics + \" by Hour\",ax=axes[i\/\/3,i%3])\nplt.tight_layout()\nplt.suptitle(\"Core Metrics on Hourly Basis\",y=\"1.05\")\nfig.show()","ba5a07b8":"agg_dict = {}\nfor col in trx_colnames_core_num:\n    agg_dict[col] = ['mean','sum']\ntrain_trx_date = train_transaction.groupby(['TransactionDate']).agg(agg_dict).reset_index()\ntrain_trx_date.columns = ['_'.join(col).strip() for col in train_trx_date.columns.values]\ntrain_trx_date.head()","0db2cd0e":"pd.plotting.register_matplotlib_converters()\nimport math\ndf = train_trx_date\nplotted_columns = df.columns[1:]\nlencols = len(plotted_columns)\nfig, axes = plt.subplots(math.ceil(lencols\/\/3),3,figsize=(16,lencols))\nfor i, metrics in enumerate(plotted_columns):\n    df.plot(x='TransactionDate_',y=metrics,title=metrics + \" by Date\",ax=axes[i\/\/3,i%3])\nplt.tight_layout()\nplt.suptitle(\"Core Metrics on Date Basis\",y=\"1.05\")\nfig.show()","42bea600":"from fbprophet import Prophet\nprophet = Prophet()\n\ndf = train_trx_date[['TransactionDate_','isFraud_sum']]\ndf.columns = ['ds','y']\nprophet = Prophet()\nprophet.fit(df)\nforecast = prophet.predict(df)","d7fad0f8":"from fbprophet import Prophet\n\ndef plot_forecast(df_input,metrics):\n    prophet = Prophet()\n    df = df_input[['TransactionDate_',metrics]]\n    df.columns = ['ds','y']\n    prophet.fit(df)\n    forecast = prophet.predict(df)\n    fig,ax = plt.subplots(1,3,figsize=(20,5),sharey=True)\n    forecast.weekly[:7].plot(ax=ax[0])\n    ax[0].set_title(\"weekly component\")\n    forecast.trend.plot(ax=ax[1])\n    ax[1].set_title(\"trend component\")\n#     ax[1].xticks(forecast.ds)\n    ax[2].plot(forecast.yhat)\n    ax[2].plot(df.y)\n    ax[2].set_title(\"comparing fitted vs. actual\")\n    plt.suptitle(metrics + ' based on: Day Of Week, Long-term Trend, Seasonality vs. Actual')","f8f3de60":"for metrics in plotted_columns:\n    plot_forecast(train_trx_date,metrics)","4c683b02":"agg_dict = {}\nfor col in trx_colnames_C:\n    agg_dict[col] = ['mean','sum']\ntrain_trx_date = train_transaction.groupby(['TransactionDate']).agg(agg_dict).reset_index()\ntrain_trx_date.columns = ['_'.join(col).strip() for col in train_trx_date.columns.values]\ntrain_trx_date.head()","f72230d7":"import math\npd.plotting.register_matplotlib_converters()\ndf = train_trx_date\nplotted_columns = df.columns[1:]\nlencols = len(plotted_columns)\nfig, axes = plt.subplots(math.ceil(lencols\/\/3)+1,3,figsize=(16,lencols))\nfor i, metrics in enumerate(plotted_columns):\n    df.plot(x='TransactionDate_',y=metrics,title=metrics + \" by Date\",ax=axes[i\/\/3,i%3])\nplt.tight_layout()\nplt.suptitle(\"Core Metrics on Date Basis\",y=\"1.05\")\nfig.show()","4bed18c9":"import datetime\nt = datetime.date(2018,1,1)\ntrain_trx_date2 = train_trx_date[train_trx_date['TransactionDate_']>t].reset_index(drop=True)\ntrain_trx_date2.head()","4de9b518":"for metrics in plotted_columns:\n    plot_forecast(train_trx_date2,metrics)","baf956fe":"def percentile(n):\n    def percentile_(x):\n        return np.percentile(x, n)\n    percentile_.__name__ = 'percentile_%s' % n\n    return percentile_\n\nagg_dict = {}\nfor col in trx_colnames_core_num:\n    agg_dict[col] = ['mean','sum']\ntrain_trx_HOD = train_transaction.groupby(['TransactionHourOfDay']).agg(agg_dict).reset_index()\ntrain_trx_HOD.columns = ['_'.join(col).strip() for col in train_trx_HOD.columns.values]\ntrain_trx_HOD.head()\n\nagg_dict = {}\nfor col in trx_colnames_core_num:\n    agg_dict[col] = [percentile(25),'median',percentile(75)]\ntrain_trx_HOD2 = train_transaction.groupby(['TransactionHourOfDay']).agg(agg_dict).reset_index()\ntrain_trx_HOD2.columns = ['_'.join(col).strip() for col in train_trx_HOD2.columns.values]\ntrain_trx_HOD2.head()","41358940":"pd.plotting.register_matplotlib_converters()\nimport math\ndf = train_trx_HOD\nplotted_columns = df.columns[1:]\nlencols = len(plotted_columns)\nfig, axes = plt.subplots(math.ceil(lencols\/\/2),2,figsize=(16,lencols))\nfor i, metrics in enumerate(plotted_columns):\n    df.plot(x='TransactionHourOfDay_',y=metrics,title=metrics + \" by HourOfDay\",ax=axes[i\/\/2,i%2])\nplt.tight_layout()\nplt.suptitle(\"Core Metrics by HourOfDay\",y=\"1.05\")\nfig.show()","1f5b77b2":"I will only be taking the time series for data after 1-Jan, since the holiday will really skew the decomposition. Alternatively, I can add in the holiday date into Prophet specification, but I'm not doing that for now","b1f2b613":"# C Metrics - Hour of Day Analysis","f2ebc188":"### Some interesting observations:\n* Card5 mean have a common cap at $225, and the spiky variation tend to be on the negative direction. So card5 is probably a \"Maximum Balance\" \/ \"Remaining Balance\" type of variables\n* Card3 mean has a common band between 150 and 185\n* Card1 and Card2 mean hugely varied, and they're at different scales","0a0e8ba0":"# Overview\n\nThis notebook will focus on time series EDA on the variables in our dataset. While the original data table is per transaaction basis, we can derive hourly \/ daily summary of is_fraud and other predictive variables. This has two purposes: \n* (1) Better understanding of underlying variables \n* (2) Use time series metrics as additional features for our prediction model","78e982ca":"# C Metrics - Seasonal decomposition","af6ce162":"# Core Metrics - Hourly Time Series","c0f1ee76":"# Time Series EDA based on: ","260bd34b":"I also sample down the dataset since this is an EDA and it will accelerate data processing and visualization","df19f77e":"#### The spike is clearly the Christmas holiday (25-Dec). Probably removing those holiday dates would help to generalize the training data. I do wonder what these C variables could be, since some of them have very low mean (<10) in all of the days, except during the holiday\n\n#### Some of these variables also demonstrate spikes on several dates. C3 is the most prominent one, because it is usually 0, except for a couple of days. It is probably some kind of indicator of rare event.","9a726d49":"# Loading data","d681eff1":"# Core Metrics - Seasonal Decomposition using Prophet\n\nNext, I will do seasonal decomposition using Facebook's prophet library. Because the data is not yet 2 years, we couldn't extract yearly seasonality. Nonetheless, we are still able to extract the DayOfWeek seasonality, which is quite significant in some metrics. ","7a73d47b":"# Define groups of metrics","5f52e365":"# Create datetime feature\n\nI use some codes from the following notebooks which provides a handy datetime feature creation. Based on their EDA, the notebook also found that the datetime is likely starting at 1-Dec-2017\n* https:\/\/www.kaggle.com\/fchmiel\/day-and-time-powerful-predictive-feature\n* https:\/\/www.kaggle.com\/kevinbonnes\/transactiondt-starting-at-2017-12-01","6b1495fd":"# C Metrics - Daily time series ","c3014f2b":"# Core Metrics - Daily time series "}}