{"cell_type":{"a8c3b5d4":"code","cb7d66e8":"code","903f159b":"code","3c0d1636":"code","947ed36b":"code","99eca153":"code","4560c3b7":"code","14fda1d5":"code","f328e933":"code","85fe7a8d":"code","3e674ac4":"code","0b520fcf":"code","49118be9":"code","9ab2feee":"code","56114c18":"code","1f805754":"code","2308cc1c":"markdown","c60564a7":"markdown","053e0433":"markdown","987d4493":"markdown","2410abe3":"markdown","90929b5c":"markdown"},"source":{"a8c3b5d4":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","cb7d66e8":"import matplotlib.pyplot as plt\nimport tensorflow as tf\nimport math","903f159b":"def get_data(SAMPLE=1000):\n    x_data = np.float32(np.random.uniform(-10.5, 10.5, (1, SAMPLE))).T\n    r_data = np.float32(np.random.normal(size=(SAMPLE, 1))) \n    y_data = np.float32(7.0*np.sin(0.75*x_data) + 0.5*x_data + r_data)\n    return x_data, y_data\n\ndef plot(x,y):\n    plt.figure(figsize=(8, 8))\n    plt.plot(x, y, 'ro', alpha=0.3)\n    plt.show()\n\nx,y = get_data(3500)\nplot(x, y)","3c0d1636":"print(x.shape, y.shape)","947ed36b":"import keras.backend as K\n\ndef square_loss(y, y_pred):\n    loss = K.sum(K.square(y-y_pred))\n    return loss \n    \n\ndef build_model():\n    model = tf.keras.Sequential([\n        tf.keras.layers.Dense(20, activation='tanh', input_shape=(1,)),\n        tf.keras.layers.Dense(1)\n    ])\n    model.compile(\n         loss=square_loss, \n         optimizer=tf.keras.optimizers.Adam()\n    )\n    return model\n\nmodel1 = build_model()\nmodel1.summary()","99eca153":"import tensorflow as tf\nclass LoggerEpochEnd(tf.keras.callbacks.Callback):\n    def __init__(self, display):\n        self.seen = 0\n        self.display = display\n    \n    def on_epoch_end(self, epoch, logs={}):\n        self.seen += 1\n        if self.seen%self.display == 0 :\n            self.seen = 1\n            print('Epochs: {} Loss:{}'.format(epoch, logs.get('loss')))     \n            \nlogger = LoggerEpochEnd(2000)","4560c3b7":"# train the model1 for x,y and predict for some random sample\nprint(\"Training model...\")\nh1 = model1.fit(x,y, epochs=1000, verbose=0, callbacks=[logger])\n\nx_test,_ = get_data(500)\ny_pred = model1.predict(x_test)\n\ndef plot_multiple(x_data,y_data,x_test,y_test):\n    plt.plot(x_data, y_data, 'ro', x_test, y_test, 'bo', alpha=0.3)\n    plt.show()\n    \nplot_multiple(x,y, x_test, y_pred)","14fda1d5":"# plot the new graph\nplot(y, x)","f328e933":"# create a new model and train it on this graph\nprint(\"Training model...\")\nmodel2 = build_model()\nh2 = model2.fit(y, x, epochs=2000, verbose=0)\n\n_,y_test = get_data(500)\nx_pred = model2.predict(y_test)\n\nplot_multiple(y,x, y_test, x_pred)","85fe7a8d":"import tensorflow as tf\nimport tensorflow.keras.backend as K\nHIDDEN = 20\nKMIX = 24\nNOUT = KMIX * 3 \n\ndef build_mdn_model():\n    inputs = tf.keras.layers.Input(shape=(1,))\n    outputs = tf.keras.layers.Dense(NOUT, activation='tanh')(inputs)\n    # input - x\n    # output [PI, Std Dev, Mean]\n    model = tf.keras.Model(inputs=inputs, outputs=outputs)\n    return model\n\nmodel_mdn = build_mdn_model()\nmodel_mdn.summary()","3e674ac4":"import math\n\n#oneDivSqrt2PI = 1 \/ math.sqrt(2*math.pi)\n\ndef extract_bayesians(outputs):\n    pi = outputs[: , : KMIX ]\n    std = outputs[: , KMIX : KMIX*2 ]\n    mean =  outputs[: , KMIX*2 : ]\n    \n    # use softmax to normalize pi into prob distribution\n    max_pi = K.max(pi, axis=1, keepdims=True)\n    pi = pi - max_pi\n    pi = K.exp(pi)\n    norm_pi = 1 \/ K.sum(pi, axis=1, keepdims=True)\n    pi = norm_pi * pi\n    \n    # exponent standard deviation\n    # mean remains the same\n    std = K.exp(std)\n    \n    return pi, std, mean\n\ndef keras_normal(y, mean, std):\n    # normalize distribution\n    result = y - mean\n    result = result * ( 1 \/ std )\n    result = - K.square(result)\/2\n    result = (K.exp(result) * (1 \/ std) ) \n    return result\n    \ndef get_loss_func(pi, std, mean, y):\n    result = keras_normal(y, mean, std)\n    result = result * pi\n    result = K.sum(result, axis=1, keepdims=True)\n    result = -K.log(result)\n    return K.mean(result)\n\ndef mdn_loss(y_true, y_pred):\n    pi, std, mean = extract_bayesians(y_pred)\n    return get_loss_func(pi, std, mean, y_true)","0b520fcf":"model_mdn.compile(loss=mdn_loss, optimizer='adam')\nepochs = 25000\nhistory = model_mdn.fit(y,x, epochs = epochs, verbose = 0)","49118be9":"plt.plot(history.history['loss'])\nplt.xlabel('Epochs')\nplt.ylabel('Loss')\nplt.show()","9ab2feee":"# save the module for re-use\nmodel_mdn.save('mdn.h5')","56114c18":"def get_pi_index(x, pdf):\n    N = pdf.shape[0]\n    accum = 0\n    for i in range(N):\n        accum += pdf[i]\n        if accum >= x:\n            return i\n    print(\"Error sampling ensemble\")\n    return -1\n\n\ndef generate_ensemble(samples, pi, std, mean, M = 10):\n    size = samples.shape[0]\n    result = np.random.rand(size, M) # initialize random [0, 1]\n    rn = np.random.randn(size, M) # normal matrix\n    \n    l_mean, l_std, idx = 0, 0 ,0\n    for j in range(M):\n         for i in range(size):\n                idx = get_pi_index(result[i, j], pi[i])\n                l_mean = mean[i, idx]\n                l_std = std[i, idx]\n                result[i, j] = l_mean + rn[i, j]*l_std\n    return result\n    \ndef generate_distribution(samples):\n    # get the result and its bayesian data\n    outputs = model_mdn.predict(samples)\n    pi, std, mean = extract_bayesians(outputs)\n    x_test = generate_ensemble(samples, pi, std ,mean)\n    return x_test ","1f805754":"x_pred = generate_distribution(y_test)\nplot_multiple(y,x, y_test, x_pred)","2308cc1c":"We will ceate a simple neural network to fit the above data with following representaion.\n\n![Network Definition](https:\/\/blog.otoro.net\/wp-content\/ql-cache\/quicklatex.com-8cb4794ce56d1721055cf89b46945305_l3.png)\n\nWe will use the L2(Sum of Squared Difference between actual and predicted value) as the loss function.\n\n![L2 Loss](https:\/\/s3.ap-south-1.amazonaws.com\/afteracademy-server-uploads\/l2-loss-function.png)\n\nThe optimizer used for updating the weights is the Adam optimizer. Look into the documentation to learn more about it.","c60564a7":"Following expression is used to generate the data samples for training.\n\n![Sample](https:\/\/blog.otoro.net\/wp-content\/ql-cache\/quicklatex.com-82fe82d65e94656edfb1a8377a410bd6_l3.png)","053e0433":"model1 fits the data perfectly. We will now take the interchange the roles of x, y and try to fit our model on this network.\n\n![New Strucutre](https:\/\/blog.otoro.net\/wp-content\/ql-cache\/quicklatex.com-1b613f8b4f2878a467364d6a1cfef1bd_l3.png)","987d4493":"Clearly the model cannot to fit this data. This is because, there are multiple outputs for one input and currently the model is only predicting one output for one input. In the next section we will create a Mixture Density Network that can predict multiple set of ouputs for one input. \n\nWe will predict the 3 results i.e mean, standard deviation and weights of the models.\n\n![PDF](https:\/\/blog.otoro.net\/wp-content\/ql-cache\/quicklatex.com-191be2bacb500cca5ef2d760c786be4b_l3.png)\n\n![Results](https:\/\/blog.otoro.net\/wp-content\/ql-cache\/quicklatex.com-ed28650f5c090d2e37f4f64f61336fdb_l3.png)","2410abe3":"We use the following loss function \n\n![Loss](https:\/\/blog.otoro.net\/wp-content\/ql-cache\/quicklatex.com-69b07f5996898963d10ff73e13f45f7b_l3.png)","90929b5c":"Code and Images are taken from - [MDN blog](https:\/\/blog.otoro.net\/2015\/11\/24\/mixture-density-networks-with-tensorflow\/)."}}