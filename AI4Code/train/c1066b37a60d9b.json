{"cell_type":{"0802a03e":"code","fdff7514":"code","42c37a2d":"code","4a4092aa":"code","ecfbcd11":"code","e6ddd6e3":"code","8b36f23d":"code","9ad2715e":"code","cf53dac8":"code","3b427564":"code","556b6fec":"code","ecfff38c":"code","48d83c35":"code","e0aa71a5":"code","520d5579":"code","3cb9839e":"code","26f828f5":"code","e5eb75cd":"code","ae604a90":"code","2f9a6672":"code","136a8174":"code","b531cea4":"code","1a03459d":"code","b950ed65":"code","ec89fcb7":"code","8f41d178":"code","66c251ab":"code","2cbe346b":"code","a2404fcf":"code","a57e643b":"code","aa9d15f1":"code","a35083e6":"code","d1b4a5ae":"code","24f61a7c":"code","617f38e6":"code","a56908eb":"markdown","88761a52":"markdown","9cc1fb61":"markdown","004dda95":"markdown","e45c488c":"markdown","370e4d2f":"markdown","5427f686":"markdown","a33351b3":"markdown","b8dc612a":"markdown","04177699":"markdown"},"source":{"0802a03e":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","fdff7514":"pd.set_option('display.max_columns', None)\npd.set_option('display.max_rows', None)\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nimport matplotlib.pyplot as plt\nimport keras\nimport statsmodels.api as sm","42c37a2d":"## Wont need this buuutt in casee !!!\nsample_submission = pd.read_csv(\"..\/input\/mlb-player-digital-engagement-forecasting\/example_sample_submission.csv\")\n# players = pd.read_csv(\"..\/input\/mlb-player-digital-engagement-forecasting\/players.csv\")\n# seasons = pd.read_csv(\"..\/input\/mlb-player-digital-engagement-forecasting\/seasons.csv\")\n# awards = pd.read_csv(\"..\/input\/mlb-player-digital-engagement-forecasting\/awards.csv\")\n# teams = pd.read_csv(\"..\/input\/mlb-player-digital-engagement-forecasting\/teams.csv\")\n# train = pd.read_csv(\"..\/input\/mlb-player-digital-engagement-forecasting\/train.csv\")\n# example_test = pd.read_csv(\"..\/input\/mlb-player-digital-engagement-forecasting\/example_test.csv\")\n\n## Merged data !!\ndata = pd.read_pickle('..\/input\/mlb-player-digital-engagement-merged-data\/player_engagement_with_info.pkl')","4a4092aa":"data.head(2)","ecfbcd11":"## Looking at individual player \ud83c\udfad !!\n\nplayer_1 = data[data['playerId']== 628317]\n\nplayer_1.head()","e6ddd6e3":"## Null percentage !! \ud83d\udc80 \ud83d\udc80 \ud83d\udc80 !!\n## most columns have null values and its not because of source issue, ist trully because the individual haven't score or performed !!\nplayer_1.isnull().sum()\/player_1.shape[0]","8b36f23d":"## Filling missing values !! to thoesss columns whicch seem relevant to MEE for any type of player  , it can change for you \ud83e\udd17 \ud83e\udd17 \ud83e\udd17\n\n# lossStreakTeam :-\n## lenght of losses by it's team, there it's null place cane filled with '0.'\n# player_1[~player_1['lossStreakTeam'].isnull()].head()\ndata['lossStreakTeam'].fillna(0.0, inplace=True)\n\n# winStreakTeam :-\n## lenght of win by it's team, there it's null place cane filled with '0.'\n# player_1[~player_1['winStreakTeam'].isnull()].head()\ndata['winStreakTeam'].fillna(0.0, inplace=True)\n\n# winPctTeam :-\n## lenght of winPctTeam by it's team, there it's null place cane filled with '0.'\n# player_1[~player_1['winPctTeam'].isnull()]['winPctTeam'].describe()\ndata['winPctTeam'].fillna(0.0, inplace=True)\n\n# wildCardRankTeam :-\n## wildCardRankTeam by it's team, there it's null place cane filled with '0.'\n# player_1[~player_1['wildCardRankTeam'].isnull()].head()\ndata['wildCardRankTeam'].fillna(0.0, inplace=True)\n\n# leagueRankTeam :-\n## leagueRankTeam by it's team, there it's null place cane filled with '0.'\n# player_1[~player_1['leagueRankTeam'].isnull()].head()\ndata['leagueRankTeam'].fillna(0.0, inplace=True)\n\n# divisionRankTeam :-\n## divisionRankTeam by it's team, there it's null place cane filled with '0.'\n# player_1[~player_1['divisionRankTeam'].isnull()].head()\ndata['divisionRankTeam'].fillna(0.0, inplace=True)\n\n# leagueRankTeam :-\n## leagueRankTeam by it's team, there it's null place cane filled with '0.'\n# player_1[~player_1['wildCardRankTeam'].isnull()].head()\ndata['winStreakTeam'].fillna(0.0, inplace=True)\n\n# noHitter :-\n## noHitter , there it's null place cane filled with '0.'\n# player_1[~player_1['noHitter'].isnull()].head()\ndata['noHitter'].fillna(0.0, inplace=True)\n\n# blownSaves :-\n## blownSaves Binary, 1 if credited with blown save, 0 will signify no saves in place of nan !!!\n# player_1[~player_1['blownSaves'].isnull()].head()\ndata['blownSaves'].fillna(0.0, inplace=True)\n\n# saves :-\n## saves Binary, 1 if credited with save, 0 will signify no saves in place of nan !!!\n# player_1[~player_1['saves'].isnull()].head()\ndata['saves'].fillna(0.0, inplace=True)\n\n# battersFaced :-\n## battersFaced game total batter faced !!, 0 will signify no batter faced\n# player_1[~player_1['battersFaced'].isnull()].head()\ndata['battersFaced'].fillna(0.0, inplace=True)\n\n# earnedRuns :-\n## Game total earned runs allowed.\n# player_1[~player_1['earnedRuns'].isnull()].head()\ndata['earnedRuns'].fillna(0.0, inplace=True)\n\n# caughtStealing :-\n## caughtStealing game total caught Stealing\n# player_1[~player_1['caughtStealing'].isnull()].head()\ndata['caughtStealing'].fillna(0.0, inplace=True)\n\n# strikeOuts :-\n## Game total strike outs.\n# player_1[~player_1['strikeOuts'].isnull()].head()\ndata['strikeOuts'].fillna(0.0, inplace=True)\n\n# homeRuns :-\n## Game total homeRuns.\n# player_1[~player_1['homeRuns'].isnull()].head()\ndata['homeRuns'].fillna(0.0, inplace=True)\n\n# runsScored :-\n## Game total runscored.\n# player_1[~player_1['runsScored'].isnull()].head()\ndata['runsScored'].fillna(0.0, inplace=True)","9ad2715e":"## Removing all irrelevant or incompatable ( ** according to me ** ) columns having NAN \ud83d\ude2d \ud83d\ude2d !!\ndata.dropna(axis=1, inplace=True)\n\nplayer_1 = data[data['playerId']== 628317]\nplayer_1.head()","cf53dac8":"## Removing all constant, quasi constant columsn or dublicate columns\n\n## Constant columns for specific players ~ most probably will stay constant wrt other players \ud83d\ude30 \ud83d\ude30 \ud83d\ude30 \ud83d\ude30!!\n# playerId : wont be removed !!\n# playerName\n# DOB\n# birthCity\n# birthCountry\n# primaryPositionName\n# homeRuns\n# caughtStealing\n# blownSaves\n# noHitter\n# inSeason: 2\n# year: 4\n\n\nfor col in player_1.columns:\n#     print(col, player_1[col].nunique())\n    if player_1[col].nunique()==1:\n        if col != 'playerId':\n            data.drop(col, axis=1, inplace =True)\n            \n## Updating the individual player's data !!            \nplayer_1 = data[data['playerId']== 628317]\nplayer_1.head()","3b427564":"## Removing dublicate columns !!\n## dailyDataDate\n## engagementMetricsDate\ndata.drop(['dailyDataDate','engagementMetricsDate'], axis=1, inplace =True)\n\n## string columns to integer !!\n## inSeason\n## seasonPart\n\n# Get one hot encoding of columns inSeason & seasonPart\none_hot_inSeason = pd.get_dummies(data['inSeason'])\none_hot_seasonPart = pd.get_dummies(data['seasonPart'])\n# Drop column B as it is now encoded\ndata = data.drop(['inSeason','seasonPart'],axis = 1)\n# Join the encoded df\ndata = data.join(one_hot_inSeason)\ndata = data.join(one_hot_seasonPart)\n\n## Updating the individual player's data !!            \nplayer_1 = data[data['playerId']== 628317]\nplayer_1.head()","556b6fec":"## Saved for later usage and since we know !! submission is kinda weird in the competition !!\n# data.to_pickle(\"mlb_data_cleaned.pkl\")\n# ##\n# data = pd.read_pickle('..\/input\/mlb-player-digital-engagement-merged-data\/player_engagement_with_info_cleaned.pkl')","ecfff38c":"## Lets Start with IDK maybe EDA or visual of a single player,\ud83e\udd23 \ud83e\udd23 \ud83e\udd23 \ud83e\udd23 !!!\n## Updating the individual player's data !!            \nplayer_1 = data[data['playerId']== 628317]\nplayer_1.head()","48d83c35":"## Date Range !!\n## 2 year 5 months !!\nprint('Date range : '+ str(player_1['date'].min()) +' to ', player_1['date'].max())\n","e0aa71a5":"## Visualizing plots of first 5 players !!\n\nfor num, ID in enumerate(data['playerId'].unique()):\n    if num > 2:\n        break\n    player = data[data['playerId']== ID]\n    ## Visualizinggg targets !!\n    player.set_index('date', inplace=True)\n    player['target1'].plot.line(mark_right=False, figsize=(20,5), grid=True)\n    player['target2'].plot.line(mark_right=False, figsize=(20,5), grid=True)\n    player['target3'].plot.line(mark_right=False, figsize=(20,5), grid=True)\n    player['target4'].plot.line(mark_right=False, figsize=(20,5), grid=True)\n    plt.legend(loc=\"upper left\")\n    plt.show()\n","520d5579":"## We can sseee SEASONALITY !!!!!!!!","3cb9839e":"## lets check till what date we need to forecast !! \n\n# During the Training phase of the competition, this unseen test set is comprised of data for the month of \n# May 2021 and the set of active players this year.\n# During the Evaluation phase, the test set will be a future in-season range of approximately one month.\n\n# sample_submission = pd.read_csv(\"..\/input\/mlb-player-digital-engagement-forecasting\/example_sample_submission.csv\")\n\n# sample_submission['playerId'] = sample_submission['date_playerId'].str.rsplit('_').apply(lambda x: int(x[-1]))\n# sample_submission['date'] = sample_submission['date'].apply(lambda x:pd.to_datetime(pd.Series([str(x)])))\n# sample_submission.head()","26f828f5":"## lets seee what correlats the most with the targets : \nplayer_1 = data[data['playerId']== 628317]\n\n## HMMMMMMMMMMM ~~~\nplayer_1[player_1.columns[:]].corr()['target2'][:].sort_values(ascending=False)","e5eb75cd":"# The Seasonal Autoregressive Integrated Moving-Average with Exogenous Regressors (SARIMAX) is an extension of the SARIMA model that also includes the modeling of exogenous variables.\n# Trying SARIMAX !!\nplot_df = player_1.set_index(['date'])['target1']\n\n# Using the \u201csm.tsa.seasonal_decompose\u201d command from the pylab library we can decompose the time-series into \n# three distinct components: trend, seasonality, and noise.\n\nfrom pylab import rcParams\nimport itertools\n\np = d = q = range(0, 2)\npdq = list(itertools.product(p, d, q))\nseasonal_pdq = [(x[0], x[1], x[2], 12) for x in list(itertools.product(p, d, q))]\n\nfor param in pdq:\n    for param_seasonal in seasonal_pdq:\n        try:\n            mod = sm.tsa.statespace.SARIMAX(y,order=param,seasonal_order=param_seasonal,enforce_stationarity=False,enforce_invertibility=False)\n            results = mod.fit()\n            print('ARIMA{}x{}12 - AIC:{}'.format(param,param_seasonal,results.aic))\n        except: \n            continue\n            \nmod = sm.tsa.statespace.SARIMAX(plot_df,\n                                order=(0, 0, 1),\n                                seasonal_order=(1, 1, 1, 12),\n                                enforce_stationarity=False,\n                                enforce_invertibility=False)\nresults = mod.fit()\npred_uc = results.get_forecast(steps=30)\n\nforecast = pred_uc.predicted_mean\n\nforecast_data = plot_df.append(forecast)\n# forecast_data = forecast_data.to_frame('target1')\n\nforecast_data[-180:].plot.line(mark_right=False, figsize=(20,5), grid=True)\nplt.legend(loc=\"upper left\")\nplt.show()","ae604a90":"# The Autoregressive Integrated Moving Average (ARIMA) method models the next step in the sequence as a linear function \n# of the differenced observations and residual errors at prior time steps.\n\n# ARIMA example\n\nfrom statsmodels.tsa.arima.model import ARIMA\nfrom random import random\n# contrived dataset\ndata_ply = player_1.set_index(['date'])['target1']\n# fit model\nmodel = ARIMA(data_ply, order=(1, 1, 1))\nmodel_fit = model.fit()\n# make prediction\nyhat = model_fit.predict(len(data_ply), len(data_ply)+30, typ='levels')\n# print(yhat)\n\nforecast_data_ply = plot_df.append(yhat)\n# forecast_data_ply = forecast_data_ply.to_frame('target1')\n\nforecast_data_ply[-180:].plot.line(mark_right=False, figsize=(20,5), grid=True)\nplt.legend(loc=\"upper left\")\nplt.show()","2f9a6672":"# Statistical approaches to Forecast : Moving Average\nimport datetime\n# plot_df = player_1.set_index(['date'])['target1']\nseries = player_1['target1']\ntime = player_1['date']\n\nsplit_time = time.shape[0]-365\ntime_train = time[:split_time]\nx_train = series[:split_time]\ntime_valid = time[split_time:]\nx_valid = series[split_time:]\n\ndef plot_series(time, series, format=\"-\", start=0, end=None):\n    plt.plot(time[start:end], series[start:end], format)\n    plt.xlabel(\"Time\")\n    plt.ylabel(\"Value\")\n    plt.grid(True)\n    \ndef moving_average_forecast(series, window_size):\n    \"\"\"Forecasts the mean of the last few values.\n     If window_size=1, then this is equivalent to naive forecast\"\"\"\n    forecast = []\n    for time in range(len(series) - window_size):\n        forecast.append(series[time:time + window_size].mean())\n    return np.array(forecast)\n\nmoving_avg = moving_average_forecast(series,50)[split_time - 50:]\n\nforecast_data_ply = plot_df.append((pd.DataFrame(moving_avg)).set_index(\n    pd.Series([(pd.to_datetime('2021-05-01') + datetime.timedelta(days=i)).strftime(\"%Y-%m-%d\") for i in range(365)]))[0])\n\nforecast_data_ply[:].plot.line(mark_right=False, figsize=(20,5), grid=True)\nplt.legend(loc=\"upper left\")\nplt.show()\n\n# plt.figure(figsize=(10, 6))\n# plot_series(time_valid, x_valid)\n# plot_series(time_valid, moving_avg)\n\n# # print(keras.metrics.mean_squared_error(x_valid, moving_avg).numpy())\n# # print(keras.metrics.mean_absolute_error(x_valid, moving_avg).numpy())","136a8174":"# The Seasonal Autoregressive Integrated Moving Average (SARIMA) method models the next step in the sequence as a linear function \n# of the differenced observations, errors, differenced seasonal observations, and seasonal errors at prior time steps.\n# SARIMA example\nfrom statsmodels.tsa.statespace.sarimax import SARIMAX\nfrom random import random\n# contrived data_plyset\ndata_ply = player_1.set_index(['date'])['target1']\n# fit model\nmodel = SARIMAX(data_ply, order=(1,1, 1), seasonal_order=(0, 0, 0, 0))\nmodel_fit = model.fit(disp=False)\n# make prediction\nyhat = model_fit.predict(len(data_ply), len(data_ply)+30)\n\nforecast_data_ply = plot_df.append(yhat)\n# forecast_data_ply = forecast_data_ply.to_frame('target1')\n\nforecast_data_ply[-410:].plot.line(mark_right=False, figsize=(20,5), grid=True)\nplt.legend(loc=\"upper left\")\nplt.show()","b531cea4":"# Holt Winter\u2019s Exponential Smoothing (HWES)\n\n# The Holt Winter\u2019s Exponential Smoothing (HWES) also called the Triple Exponential Smoothing method models \n# the next time step as an exponentially weighted linear function of observations at prior time steps, \n# taking trends and seasonality into account.\n\n# The method is suitable for univariate time series with trend and\/or seasonal components.\n\nfrom statsmodels.tsa.holtwinters import ExponentialSmoothing\nfrom random import random\n\nplot_df = player_1.set_index(['date'])['target1']\ndata_ply = player_1.set_index(['date'])['target1']\n# contrived data_plyset\n# data_ply = [x + random() for x in range(1, 100)]\n# fit model\nmodel = ExponentialSmoothing(data_ply ,seasonal_periods=7 ,trend='add', seasonal='add',)\nmodel_fit = model.fit()\n# make prediction\nyhat = model_fit.predict(len(data_ply), len(data_ply)+30)\nforecast_data_ply = plot_df.append(yhat)\n# forecast_data_ply = forecast_data_ply.to_frame('target1')\n\nforecast_data_ply[-180:].plot.line(mark_right=False, figsize=(20,5), grid=True)\nplt.legend(loc=\"upper left\")\nplt.show()","1a03459d":"# The Simple Exponential Smoothing (SES) method models the next time step as an exponentially weighted linear function \n# of observations at prior time steps.\n\nfrom statsmodels.tsa.holtwinters import SimpleExpSmoothing\nfrom random import random\n# contrived data_plyset\ndata_ply = player_1.set_index(['date'])['target1']\n# fit model\nmodel = SimpleExpSmoothing(data_ply)\nmodel_fit = model.fit()\n# make prediction\nyhat = model_fit.predict(len(data_ply), len(data_ply)+30)\nforecast_data_ply = plot_df.append(yhat)\n# forecast_data_ply = forecast_data_ply.to_frame('target1')\n\nforecast_data_ply[-180:].plot.line(mark_right=False, figsize=(20,5), grid=True)\nplt.legend(loc=\"upper left\")\nplt.show()","b950ed65":"#### Hmmmmmmmmmmmmm : looks like non of them did well !! LOL \ud83e\udd12 \ud83d\ude2d\n\ndata = pd.read_pickle('..\/input\/mlb-player-digital-engagement-merged-data\/player_engagement_with_info_cleaned.pkl')","ec89fcb7":"## getting mean of each player based on there target inform of list !!\nimport random\n\nmean_player = {}\nfor Id in data['playerId'].unique().tolist():\n    mean_player[Id] = {}\n    mean_player[Id]['target1'] = random.uniform(data[data['playerId']== Id]['target1'].quantile(0.25),\n                     data[data['playerId']== Id]['target1'].quantile(0.5))\n    mean_player[Id]['target2'] = random.uniform(data[data['playerId']== Id]['target2'].quantile(0.25),\n                     data[data['playerId']== Id]['target2'].quantile(0.5))\n    mean_player[Id]['target3'] = random.uniform(data[data['playerId']== Id]['target3'].quantile(0.25),\n                     data[data['playerId']== Id]['target3'].quantile(0.5))\n    mean_player[Id]['target4'] = random.uniform(data[data['playerId']== Id]['target4'].quantile(0.25),\n                     data[data['playerId']== Id]['target4'].quantile(0.5))\n\nmean_player[656669]\n\n# mean_player = {}\n# for Id in data['playerId'].unique().tolist():\n#     mean_player[Id] = {}\n#     mean_player[Id]['target1'] = data[data['playerId']== Id]['target1'].quantile(0.75)\n#     mean_player[Id]['target2'] = data[data['playerId']== Id]['target2'].quantile(0.75)\n#     mean_player[Id]['target3'] = data[data['playerId']== Id]['target3'].quantile(0.75)\n#     mean_player[Id]['target4'] = data[data['playerId']== Id]['target4'].quantile(0.75)\n\n# mean_player[656669]","8f41d178":"len(mean_player), data['playerId'].nunique()","66c251ab":"# # COMMENTING SINCE I HAVE SAVED THE OUTPUT !!\n\n# sub_dict = {}\n# ext_date = pd.Series([(pd.to_datetime('2021-05-01') + datetime.timedelta(days=i)).strftime(\"%Y-%m-%d\") for i in range(90)])\n# split_time = 851 # player_1['date'].shape[0]-365\n\n# no_days = 90\n\n# for player in data['playerId'].unique():\n#     player_data = data[data['playerId']==player][['target1', 'target2', 'target3', 'target4','year','date']]\n#     player_data_cut = player_data[player_data['date']>'2021-04-20']\n    \n#     ## we know the avilable data ranges from :2018-01-01 to  2021-04-30\n    \n#     model_fit = ExponentialSmoothing(player_data['target1']).fit()\n#     target1_df = pd.DataFrame({'target1': model_fit.predict(player_1.shape[0], player_1.shape[0]+no_days)}).reset_index(drop=True)\n#     target1_df['date'] = ext_date\n#     target1_df = player_data_cut[['date','target1']].append(target1_df).reset_index(drop=True)\n    \n#     model_fit = ExponentialSmoothing(player_data['target2']).fit()\n#     target2_df = pd.DataFrame({'target2': model_fit.predict(player_1.shape[0], player_1.shape[0]+no_days)}).reset_index(drop=True)\n#     target2_df['date'] = ext_date\n#     target2_df = player_data_cut[['date','target2']].append(target2_df).reset_index(drop=True)\n    \n#     model_fit = ExponentialSmoothing(player_data['target3']).fit()\n#     target3_df = pd.DataFrame({'target3': model_fit.predict(player_1.shape[0], player_1.shape[0]+no_days)}).reset_index(drop=True)\n#     target3_df['date'] = ext_date\n#     target3_df = player_data_cut[['date','target3']].append(target3_df).reset_index(drop=True)\n    \n#     model_fit = ExponentialSmoothing(player_data['target4']).fit()\n#     target4_df = pd.DataFrame({'target4': model_fit.predict(player_1.shape[0], player_1.shape[0]+no_days)}).reset_index(drop=True)\n#     target4_df['date'] = ext_date\n#     target4_df = player_data_cut[['date','target4']].append(target4_df).reset_index(drop=True)\n    \n#     # Lets make dict which will help us in this weird submission style \ud83e\udd2c \ud83e\udd2c \ud83e\udd2c !!\n#     sub_dict[player] = {}\n#     for date in target1_df['date'][:-1]:\n#         sub_dict[player][date] ={}\n#         sub_dict[player][date]['target1'] = target1_df[target1_df['date']==date]['target1'].values[0]\n#         sub_dict[player][date]['target2'] = target2_df[target1_df['date']==date]['target2'].values[0]\n#         sub_dict[player][date]['target3'] = target3_df[target1_df['date']==date]['target3'].values[0]\n#         sub_dict[player][date]['target4'] = target4_df[target1_df['date']==date]['target4'].values[0]\n        ","2cbe346b":"# sub_dict[596071]['2021-04-30']","a2404fcf":"# import numpy as np\n\n# Save\n# np.save('sub_dict_ExponentialSmoothing.npy', sub_dict) \n\n# # Load\n# sub_dict = np.load('..\/input\/mlb-player-digital-engagement-merged-data\/sub_dict_ExponentialSmoothing.npy',allow_pickle='TRUE').item()\n# sub_dict[628317]['2021-05-01']\n","a57e643b":"## Submission !! \ud83d\ude07 \ud83d\ude07 \ud83d\ude07 ##\n## Function to create processed data that will be submitted !!\n\n# def process_pred(data):\n#     data = data.reset_index()\n#     data['date_formated'] = data['date'].apply(lambda x:pd.to_datetime(pd.Series([str(x)])))\n#     data['date_formated'] = data['date_formated'].apply(lambda x: x.strftime(\"%Y-%m-%d\") )\n    \n#     data['playerId'] = data['date_playerId'].str.rsplit('_').apply(lambda x: int(x[-1]))\n\n#     for player, date in zip(data['playerId'], data['date_formated']):\n#         indexes = (data[(data['playerId'] == player) & (data['date_formated'] == date)].index).tolist()\n        \n#         data.loc[indexes,'target1'] = sub_dict[player][date]['target1']\n#         data.loc[indexes,'target2'] = sub_dict[player][date]['target2']\n#         data.loc[indexes,'target3'] = sub_dict[player][date]['target3']\n#         data.loc[indexes,'target4'] = sub_dict[player][date]['target4']\n    \n#     data = data.set_index('date', drop = True)\n#     data = data.drop(['playerId','date_formated'], axis=1)\n#     return data\n\ndef process_pred(data):\n    data = data.reset_index()\n    data['playerId'] = data['date_playerId'].str.rsplit('_').apply(lambda x: int(x[-1]))\n\n    for ply in data['playerId'].unique().tolist():\n        indexes = (data[data['playerId'] == ply].index).tolist()\n        data.loc[indexes,'target1'] = mean_player[ply]['target1']\n        data.loc[indexes,'target2'] = mean_player[ply]['target2']\n        data.loc[indexes,'target3'] = mean_player[ply]['target3']\n        data.loc[indexes,'target4'] = mean_player[ply]['target4']\n    \n    data = data.set_index('date', drop = True)\n    data = data.drop(['playerId'], axis=1)\n    return data","aa9d15f1":"## Checking the process_pred function !!\nsample_submission = pd.read_csv(\"..\/input\/mlb-player-digital-engagement-forecasting\/example_sample_submission.csv\")\n\nprocess_pred(sample_submission.set_index('date', drop = True)).head()","a35083e6":"## Shitty submission typeee !!\n\nimport sys\nif 'kaggle_secrets' in sys.modules:  # only run while on Kaggle\n    import mlb","d1b4a5ae":"## Execute this only for one !!\n\nenv = mlb.make_env()\niter_test = env.iter_test()","24f61a7c":"## Hereeee wee Ggooooooooo !!!!! \ud83c\udf0a \ud83d\ude80 \ud83d\ude80 \ud83d\ude80\n\nfor (test_df, sample_prediction_df) in iter_test:\n    sample_prediction_df = process_pred(sample_prediction_df)\n    env.predict(sample_prediction_df)","617f38e6":"sample_prediction_df.head()","a56908eb":"## Classical Time Series Forecasting Methods !!","88761a52":"## 5. Holt Winter\u2019s Exponential Smoothing (HWES)","9cc1fb61":"## 2. The Autoregressive Integrated Moving Average (ARIMA) ","004dda95":"####                                                        Lets try \n\n<img src=\"https:\/\/i.redd.it\/xwgslh4ioqq61.jpg\" width=\"500\" class=\"center\"\/>","e45c488c":"## Heyyy Everyyoneeee !! *** pleasee UPVOTE if you liked it OR if you have a good heart *** \ud83e\udd7a \ud83e\udd7a \ud83e\udd7a\n\n<img src=\"https:\/\/ih1.redbubble.net\/image.1600796187.0360\/mp,840x830,matte,f8f8f8,t-pad,1000x1000,f8f8f8.jpg\" width=\"600\" class=\"center\"\/>","370e4d2f":"## 4. The Seasonal Autoregressive Integrated Moving-Average (SARIMA)","5427f686":"## 1. The Seasonal Autoregressive Integrated Moving-Average with Exogenous Regressors (SARIMAX)","a33351b3":"### Since you have reached this far -- Upvoteeee !! yeah i am a simp for upvotes <3","b8dc612a":"## 6. Simple Exponential Smoothing (SES)","04177699":"## 3. Moving Average"}}