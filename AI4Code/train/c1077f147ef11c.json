{"cell_type":{"464b1c2a":"code","5c58fbae":"code","ec7afaf1":"code","1243aba9":"code","b30cfe3f":"code","5c7d5317":"code","90d4a229":"code","fef6fbe0":"code","ef60076a":"code","dbff7974":"code","b4e6aaa1":"code","941cd51e":"code","a3a2aad5":"code","33512f63":"code","2f755699":"code","ef0c6821":"code","4d74ab27":"code","2c7bbd3d":"code","18b24758":"code","8e0393e4":"code","356ef7b1":"markdown","793ffd94":"markdown","06f1998b":"markdown","fa558807":"markdown","5f7e7a19":"markdown","b542b360":"markdown","47f80e2b":"markdown","1e1238ba":"markdown","3569e41e":"markdown","ce166cc9":"markdown","75b16578":"markdown","02864375":"markdown","c9b05a29":"markdown","176ba3ed":"markdown"},"source":{"464b1c2a":"import pandas as pd\nfrom nltk.tokenize import word_tokenize\nfrom nltk.corpus import stopwords\nimport string\n\ntrain_data = pd.read_csv('..\/input\/news-headline\/train.csv', names=['news','sentiment'], header=None)\nval_data = pd.read_csv('..\/input\/news-headline\/validation.csv', names=['news','sentiment'], header=None)\ntest_data = pd.read_csv('..\/input\/news-headline\/test.csv', names=['news','sentiment'], header=None)\n\nstop_words = set(stopwords.words('english'))\nstop_words -= {\n    \"don\", \"hadn't\", \"hasn\", \"didn't\", \"aren't\", \"needn't\", \"couldn't\", \n    \"haven\", \"down\", \"weren't\", \"weren\", \"doesn\", \"couldn\", \"don't\", \n    \"mightn't\", \"hasn't\", \"off\", \"wasn\", \"mustn\", \"hadn\", \"didn\", \"up\",\n    \"shan't\", \"isn\", \"mightn\", \"mustn't\", \"shouldn't\", \"doesn't\", \"no\", \n    \"not\", \"haven't\", \"wouldn\", \"won't\", \"wouldn't\", \"shouldn\", \"aren\", \n    \"isn't\", \"wasn't\", \"ain\", \"under\", \"needn\", \"shan\", \"above\"\n}\n\ndef clean_text(text):\n    words = word_tokenize(text)\n    words = [w.lower() for w in words]\n    \n    translator = str.maketrans('', '', string.punctuation)\n    words = [w.translate(translator) for w in words]\n    \n    words = [w for w in words if w.isalpha()]\n    words = [w for w in words if w not in stop_words]\n    return ' '.join(words)\n    \ntrain_data['news'] = train_data['news'].apply(clean_text)\nval_data['news'] = val_data['news'].apply(clean_text)\ntest_data['news'] = test_data['news'].apply(clean_text)","5c58fbae":"# negative: 0, neutral: 1, positive: 2\ntrain_data['sentiment'] += 1\nval_data['sentiment'] += 1\ntest_data['sentiment'] += 1","ec7afaf1":"train_data.head()","1243aba9":"import numpy as np\n\nembeddings_index = dict()\nembedding_dim = 300\n\nwith open('..\/input\/glove42b300dtxt\/glove.42B.300d.txt') as f:\n    for line in f:\n        word, coefs = line.split(maxsplit=1)\n        coefs = np.fromstring(coefs, \"f\", sep=\" \")\n        embeddings_index[word] = coefs\n\nprint(f'Loaded {len(embeddings_index)} word vectors.')","b30cfe3f":"from keras.preprocessing.text import Tokenizer\n\ntokenizer = Tokenizer()\ntokenizer.fit_on_texts(train_data['news'])\nword_index = tokenizer.word_index\nvocab_size = len(word_index) + 1\nprint(f'Vocabulary Size: {vocab_size} words')","5c7d5317":"hits = 0\nmisses = 0\n\nembedding_matrix = np.zeros((vocab_size, embedding_dim))\n\nfor word, i in word_index.items():\n    embedding_vector = embeddings_index.get(word)\n\n    if embedding_vector is not None:\n        embedding_matrix[i] = embedding_vector\n        hits += 1\n    else:\n        misses += 1\n\nprint(f'Converted {hits} words ({misses} misses)')","90d4a229":"from tensorflow.keras.preprocessing.sequence import pad_sequences\n\ntrain_sequences = tokenizer.texts_to_sequences(train_data['news'])\navg_words = sum([len(ts) for ts in train_sequences])\/len(train_sequences)\nmax_seq_len = len(max(train_sequences, key=len)) \ntrain_sequences = pad_sequences(train_sequences, maxlen=max_seq_len, padding='post')    \n\nx_train = train_sequences\ny_train = train_data['sentiment'].to_numpy()\n\nprint(f'Average words per sentence: {avg_words:.2f} words')\nprint(f'Maximum words: {max_seq_len}')","fef6fbe0":"val_sequences = tokenizer.texts_to_sequences(val_data['news'])\nval_sequences = pad_sequences(val_sequences, maxlen=max_seq_len, padding='post')\n\nx_val = val_sequences\ny_val = val_data['sentiment'].to_numpy()","ef60076a":"test_sequences = tokenizer.texts_to_sequences(test_data['news'])\ntest_sequences = pad_sequences(test_sequences, maxlen=max_seq_len, padding='post')\n\nx_test = test_sequences\ny_test = test_data['sentiment'].to_numpy()","dbff7974":"from tensorflow import keras\nfrom tensorflow.keras.layers import Embedding\nfrom tensorflow.keras import layers\n\ninputs = keras.Input(shape=(max_seq_len,))\nx = Embedding(\n    input_dim=vocab_size,\n    output_dim=embedding_dim,\n    embeddings_initializer=keras.initializers.Constant(embedding_matrix),\n    mask_zero=True,\n    trainable=False,\n)(inputs)\nx = layers.Bidirectional(layers.LSTM(\n    8, \n    return_sequences=True,\n    dropout=0.2,\n    recurrent_dropout=0.1,\n))(x)\nx = layers.Bidirectional(layers.LSTM(\n    4,\n    dropout=0.2,\n    recurrent_dropout=0.1,\n))(x)\noutputs = layers.Dense(3, activation='softmax')(x)\n\nmodel = keras.Model(inputs, outputs)\nmodel.summary()","b4e6aaa1":"from keras.utils.vis_utils import plot_model\n\nplot_model(model, to_file='model_plot.png', show_shapes=True, rankdir='LR')","941cd51e":"from keras.callbacks import Callback\n\nclass Histories(Callback):\n    \n    def on_train_begin(self,logs={}):\n        self.accs = []\n        self.losses = []\n\n    def on_batch_end(self, batch, logs={}):\n        self.accs.append(logs.get('accuracy'))\n        self.losses.append(logs.get('loss'))\n\nhistories = Histories()","a3a2aad5":"model.compile(\n    optimizer=keras.optimizers.Adam(learning_rate=5E-3),\n    loss='sparse_categorical_crossentropy', \n    metrics=['accuracy'],\n)\n\nbatch_size=256\nhistory = model.fit(\n    x_train,\n    y_train,\n    validation_data=(x_val,y_val),\n    batch_size=batch_size,\n    epochs=20,\n    callbacks=[histories]\n)","33512f63":"train_accuracy = history.history['accuracy'][-1] * 100\ntrain_loss = history.history['loss'][-1]\nval_accuracy = history.history['val_accuracy'][-1] * 100\nval_loss = history.history['val_loss'][-1]\n\nprint(f'Training Accuracy: {train_accuracy:.2f}%')\nprint(f'Traning Loss: {train_loss:.4f}')\nprint(f'Validation Accuracy: {val_accuracy:.2f}%')\nprint(f'Validation Loss: {val_loss:.4f}')","2f755699":"import matplotlib.pyplot as plt\n\ntrain_accs = [y*100 for y in histories.accs]\ntrain_losses = [y for y in histories.losses]\n\nval_accs = [y*100 for y in history.history['val_accuracy']]\nval_accs.insert(0, 0)\n\nval_losses = [y for y in history.history['val_loss']]\nval_losses.insert(0, 0)\n\nfig, ax1 = plt.subplots(figsize=(9,7))\nax1.set_xlabel('Iteration')\nax1.set_ylabel('Accuracy (%)')\n\nxlim = (0, len(train_accs))\nxticks = [xt for xt in range(xlim[0], xlim[1], 1000)]\nxticks.append(xlim[1])\nax1.set_xlim(xlim)\nax1.set_xticks(xticks)\n\nx_train = [i for i in range(xlim[0], xlim[1], 1)]\nx_val = [i for i in range(xlim[0], xlim[1], round(xlim[1]\/20))]\nx_val.append(xlim[1])\n\nax1.plot(x_train, train_accs, 'b', label='Training Accuracy')\nax1.plot(x_val, val_accs, 'r', label='Validation Accuracy')\n\nax2 = ax1.twinx()\nax2.set_ylabel('Loss')\n\nax2.plot(x_train, train_losses, 'g', label='Training Loss')\nax2.plot(x_val, val_losses, 'm', label='Validation Loss')\n\nfig.tight_layout()\nplt.title('Accuracy and Loss Curves of Bidirectional LSTM Model', fontweight='bold')\nfig.legend(loc=\"center right\", bbox_to_anchor=(1,0.7), bbox_transform=ax1.transAxes)\nplt.savefig('performance.svg')\nplt.show()","ef0c6821":"model.save('bi_lstm_model.h5')","4d74ab27":"# model = keras.models.load_model('.\/bi_lstm_model.h5')","2c7bbd3d":"test_loss, test_accuracy = model.evaluate(\n    x_test,\n    y_test,\n    batch_size=batch_size,\n    verbose=0\n)\n\nprint(f'Test Accuracy: {test_accuracy*100:.2f}%')\nprint(f'Test Loss: {test_loss:.5f}')","18b24758":"from sklearn.metrics import classification_report\n\ny_pred = model.predict(x_test, batch_size=batch_size, verbose=1)\ny_pred = np.argmax(y_pred, axis=1)\n\nsentiments = ['Negative', 'Neutral', 'Positive']\nprint(classification_report(y_test, y_pred, target_names=sentiments))","8e0393e4":"from sklearn.metrics import ConfusionMatrixDisplay\nfrom sklearn.metrics import confusion_matrix\n\ncm = confusion_matrix(y_test, y_pred)\ncmd = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=sentiments)\ncmd.plot(cmap=plt.cm.Blues)\ncmd.ax_.set(xlabel='Predicted Sentiments', ylabel='True Sentiments')\n\nplt.title('Confusion Matrix of Bidirectional LSTM Model', fontweight='bold')\nplt.savefig('cm.svg')\nplt.show()","356ef7b1":"### Configure Callbacks for Plotting Model Performance","793ffd94":"### Initialize Tokenizer","06f1998b":"### Train the Model","fa558807":"###  Visualize the Model","5f7e7a19":"### Create Embedding Matrix for News Dataset","b542b360":"### Data Preparation","47f80e2b":"### Plot Accuracy and Loss of the Model","1e1238ba":"### Read News Dataset & Text Cleaning","3569e41e":"### Build the Model","ce166cc9":"### Map Values of Sentiments to Positive Numbers","75b16578":"### Save the Model","02864375":"### Load the Model","c9b05a29":"### Evaluate the Model","176ba3ed":"### Read Pretrained Embeddings"}}