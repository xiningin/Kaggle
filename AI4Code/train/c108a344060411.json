{"cell_type":{"ee90b505":"code","c2c24249":"code","cde0a07d":"code","666797e9":"code","c83c70bc":"code","33eb70b9":"code","4afc310c":"code","0f993d8f":"code","b384f9fb":"code","40b7885d":"code","df77c71d":"code","ac912328":"code","ff191a99":"code","29c829ce":"code","f3b66cae":"code","4e7facdf":"code","f1aac082":"code","8c8e514d":"code","eb7d8e93":"code","7807c196":"code","39b366c0":"code","bb193b3d":"code","73d3993f":"code","78170427":"code","f1c11ed5":"code","3dda76e3":"code","bf581387":"code","95d8b685":"code","d45c85af":"code","d62238bb":"code","9c362119":"code","594e67ad":"code","b7f211fd":"code","48cb9ea5":"code","2e2adc52":"code","27fad1cc":"code","33d7bc4e":"code","784d55b5":"code","29b706a8":"code","50dd6688":"code","58b8cff2":"code","f028e7d9":"code","b2c16961":"code","c9632e04":"markdown","7a32c008":"markdown","d39ed578":"markdown","2295b975":"markdown","86b15ec2":"markdown","3d6f9c51":"markdown","8a56dafb":"markdown","826e531a":"markdown","0805a642":"markdown","fd64084a":"markdown","a8422480":"markdown","666e83bf":"markdown","c0e47fd1":"markdown","0a734b99":"markdown"},"source":{"ee90b505":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","c2c24249":"train = pd.read_csv(\"..\/input\/titanic\/train.csv\")\ntest = pd.read_csv(\"..\/input\/titanic\/test.csv\")\nsubmission = pd.read_csv(\"..\/input\/titanic\/gender_submission.csv\")","cde0a07d":"train.head()","666797e9":"print('Train data shape is: ', train.shape)\nprint('Test data shape is: ', test.shape)","c83c70bc":"plt.subplots(0,0, figsize = (15,3))\ntrain.isnull().mean().sort_values(ascending = False).plot.bar(color = 'grey')\nplt.axhline(y=0.1, color='r', linestyle='-')\nplt.title('Missing values average per columns in TRAIN data', fontsize = 20)\nplt.show()\n\nplt.subplots(1,0, figsize = (15,3))\ntest.isnull().mean().sort_values(ascending = False).plot.bar(color = 'grey')\nplt.axhline(y=0.1, color='r', linestyle='-')\nplt.title('Missing values average per columns in TEST data', fontsize = 20)\nplt.show()","33eb70b9":"fig = plt.figure(figsize = (15,10))\n\nax1 = plt.subplot2grid((2,3),(0,0))\nsns.countplot(x = 'Survived', data = train)\nplt.title('Survived')\n\nax1 = plt.subplot2grid((2,3),(0,1))\nsns.countplot(x = 'Sex',hue = 'Survived',data = train)\nplt.title('Sex')\n\nax1 = plt.subplot2grid((2,3),(0,2))\nsns.countplot(x = 'SibSp',hue = 'Survived', data = train)\nplt.title('SibSp')\n\nax1 = plt.subplot2grid((2,3),(1,0))\nsns.countplot(x = 'Parch',hue = 'Survived', data = train)\nplt.title('Parch')\n\nax1 = plt.subplot2grid((2,3),(1,1))\nsns.countplot(x = 'Pclass',hue = 'Survived', data = train)\nplt.title('Pclass')\n\nax1 = plt.subplot2grid((2,3),(0,1))\nsns.countplot(x = 'Embarked',hue = 'Survived', data = train)\nplt.title('Embarked')","4afc310c":"df = train.drop('Survived',axis = 1).append(test)","0f993d8f":"df['Cabin'] = df['Cabin'].str[0]\ndf['Cabin'] = df['Cabin'].fillna(\"noCabin\")\ndf['Cabin'] = df['Cabin'].replace(\"T\",\"C\")\n#mapping = {'noCabin':0,'G':1,'F':2,'E':3,'D':4,'C':5,'B':6,'A':7}\n#df['Cabin'] = df['Cabin'].map(mapping)\n\ndf['Embarked'] = df['Embarked'].fillna(\"S\")\n\n#mapping = {'C':3,'Q':2,'S':1}\n#df['Embarked'] = df['Embarked'].map(mapping)","b384f9fb":"sns.countplot(x = 'Cabin', data = df)\nplt.title('Cabin barplot')","40b7885d":"df['FamilySize'] = df['SibSp'] + df['Parch'] + 1","df77c71d":"#df['Size'] = df['FamilySize']\n#df['Size'] = df['Size'].replace(1,1)\n#df['Size'] = df['Size'].replace([2,3,4],2)\n#df['Size'] = df['Size'].replace([5,6,7],3)\n#df['Size'] = df['Size'].replace([8,10,11], 4)\ndf['IsSolo'] = 0\ndf['IsSolo'][df['FamilySize'] == 1] = 1\ndf['SmallGroup'] = 0\ndf['SmallGroup'][df['FamilySize'].isin([2,3,4])] = 1","ac912328":"df.head()","ff191a99":"df['Title'] = df['Name'].str.replace('(.*, )|(\\..*)',\"\")","29c829ce":"df['Title'] = df['Title'].replace(['Don','Mme','Lady','Mlle','Dona','Miss','Ms'],'lady')\ndf['Title'] = df['Title'].replace(['Rev','Dr','Major','Sir','Col','Capt','Capt','the Countess','Jonkheer'],'other')\ndf['Title'].unique().tolist()","f3b66cae":"# fillna of age\nfor tit in df['Title'].unique().tolist():\n    df['Age'][df['Title'] == tit] = df['Age'][df['Title'] == tit].fillna(df['Age'][df['Title'] == tit].mean())","4e7facdf":"sns.set_style(\"whitegrid\")\nfig = plt.figure(figsize = (12,6))\n\nax1 = plt.subplot2grid((1,2),(0,0))\nsns.boxplot(x = 'Title',y='Age',data = df)\nplt.title('Age boxplot by title')\n\nax1 = plt.subplot2grid((1,2),(0,1))\nplt.hist(df['Age'])\nplt.title('Histogram of Age')\nplt.xlabel('Age')\nplt.ylabel('frequency')","f1aac082":"#df['Title'][(df['Title'] == 'lady') & (df['Age'] < 10)] = 'Master'\n#df['Title'][(df['Title'] == 'Mr') & (df['Age'] < 15)] = 'Master'\n#mapping = {'other':0,'Mr':1,'lady':2,'Master':3}\n#df['Title'] = df['Title'].map(mapping)","8c8e514d":"df.Sex = df.Sex.replace('male','0')\ndf.Sex = df.Sex.replace('female','1')\ndf.Sex = df.Sex.astype(int)","eb7d8e93":"#df['Pclass'] = df['Pclass'].astype(object)","7807c196":"df['Fare'].plot(kind = 'hist')\nplt.title('Fare histogram')\nplt.xlabel('Fare')","39b366c0":"tickets = set(df['Ticket'])\nticket_numbers = []\nfares = []\nfor ticket in tickets:\n    ticket_numbers.append(len(df[df['Ticket'] == ticket]))\n    fares.append(df['Fare'][df['Ticket'] == ticket].mean())","bb193b3d":"tickets_sum = pd.DataFrame()\ntickets_sum['Ticket'] = list(tickets)\ntickets_sum['Count'] = ticket_numbers\ntickets_sum['Fare'] = fares\ntickets_sum['FarePP'] = tickets_sum['Fare']\/tickets_sum['Count']\ntickets_sum = tickets_sum.drop(['Count','Fare'], axis = 1)\n\n# concat to df\ndf = pd.merge(df, tickets_sum,'left','Ticket')","73d3993f":"tickets_sum.head()","78170427":"df['FarePP'] = df['FarePP'].fillna(df['FarePP'].median())","f1c11ed5":"df.info()","3dda76e3":"df_2 = df.loc[:,['Pclass','Age',\n                 #'Cabin',\n                 'Embarked',\n                 #'FamilySize',\n                 'IsSolo','SmallGroup',\n                 'Title','FarePP']]\n\ndf_2 = pd.get_dummies(df_2)\ntrain_x = df_2.iloc[:891,:]\ntest_x = df_2.iloc[891:len(df)+1,:]\n\n#train_x.to_csv('train_x.csv', index = False)\n#test_x.to_csv('test_x.csv', index = False)\n\nprint('train: ',train_x.shape)\nprint('test: ',test_x.shape)","bf581387":"train_y = train['Survived']","95d8b685":"from sklearn.model_selection import GridSearchCV\nfrom sklearn.metrics import accuracy_score, roc_curve, auc, confusion_matrix, roc_auc_score\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import StratifiedShuffleSplit, cross_val_score\nfrom sklearn import tree\nfrom sklearn.tree import DecisionTreeClassifier","d45c85af":"submission.head()","d62238bb":"from sklearn.ensemble import RandomForestClassifier\n\n# split to training data to 2 parts: training and validation\nx_train, x_valid, y_train, y_valid = train_test_split(train_x, train_y, test_size = 0.2,random_state=123)\n\nmodels = []\nbag = 10\ny_test_bags = np.zeros((test_x.shape[0]))\n\nfor i in range(bag):\n    params = {'n_estimators':[100,300],\n             'max_depth': [2,3,4,5],\n             'max_features':[3,5,7]}\n    rf = RandomForestClassifier(random_state = i)\n    grid = GridSearchCV(rf, params,cv=5)\n    grid.fit(x_train, y_train)\n    model = grid.best_estimator_\n    models.append(model)\n    \n    y_hat = model.predict(x_valid)\n    y_hat_test = model.predict_proba(test_x)[:,1]\n    y_test_bags += y_hat_test\n    \n    score = accuracy_score(y_valid, y_hat)\n    print('Seed: {} - AUC: {}'.format(i,score))\n\nfinal_test_y = y_test_bags\/bag","9c362119":"submission['Survived'] = np.where(final_test_y > 0.5,1,0)\nsubmission.to_csv('submission.csv',index=False)","594e67ad":"from xgboost import XGBClassifier\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.metrics import accuracy_score\n\nparam_grid = [{'max_depth':[3,4,5],\n              'min_child_weight':[1,2,3],\n              'n_estimators':[100,200],\n              'learning_rate':[0.1,0.01]}] #set of trial values for min_child_weight\ni=1\nkf = StratifiedKFold(n_splits=10,random_state=1,shuffle=True)\n\n# for each fold in kfold:\nfor train_index,test_index in kf.split(train_x, train_y):\n    print('\\n{} of kfold {}'.format(i,kf.n_splits))\n     \n    # split to train and test\n    x_train, x_valid = train_x.loc[train_index],train_x.loc[test_index]\n    y_train, y_valid = train_y[train_index],train_y[test_index]\n    \n    # GridSearch\n    model = GridSearchCV(XGBClassifier(), param_grid, cv=10, scoring= 'accuracy')\n    model.fit(x_train, y_train)\n    print(model.best_params_)\n    y_hat = model.predict(x_valid)\n    print('accuracy_score',accuracy_score(y_valid,y_hat))\n    i+=1","b7f211fd":"op=pd.DataFrame(data={'PassengerId':test['PassengerId'],'Survived':model.predict(test_x)})\nop.to_csv('KFold_XGB_GridSearchCV_submission.csv',index=False)","48cb9ea5":"from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier, ExtraTreesClassifier, VotingClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.svm import SVC","2e2adc52":"#ExtraTrees \nExtC = ExtraTreesClassifier()\n\n## Search grid for optimal parameters\nex_param_grid = {\"max_depth\": [None],\n              \"max_features\": [1, 3, 10],\n              \"min_samples_split\": [2, 3, 10],\n              \"min_samples_leaf\": [1, 3, 10],\n              \"bootstrap\": [False],\n              \"n_estimators\" :[100,300],\n              \"criterion\": [\"gini\"]}\n\n# define kfold cv\nkfold = StratifiedKFold(n_splits=10,random_state=1,shuffle=True)\n\ngsExtC = GridSearchCV(ExtC,param_grid = ex_param_grid, cv=kfold, scoring=\"accuracy\", n_jobs= 4, verbose = 1)\n\ngsExtC.fit(train_x, train_y)\n\nExtC_best = gsExtC.best_estimator_\n\n# Best score\ngsExtC.best_score_","27fad1cc":"# RFC Parameters tunning \nRFC = RandomForestClassifier()\n\n\n## Search grid for optimal parameters\nrf_param_grid = {\"max_depth\": [None],\n              \"max_features\": [1, 3, 10],\n              \"min_samples_split\": [2, 3, 10],\n              \"min_samples_leaf\": [1, 3, 10],\n              \"bootstrap\": [False],\n              \"n_estimators\" :[100,300],\n              \"criterion\": [\"gini\"]}\n\n\ngsRFC = GridSearchCV(RFC,param_grid = rf_param_grid, cv=kfold, scoring=\"accuracy\", n_jobs= 4, verbose = 1)\n\ngsRFC.fit(train_x, train_y)\n\nRFC_best = gsRFC.best_estimator_\n\n# Best score\ngsRFC.best_score_","33d7bc4e":"submission['Survived'] = gsRFC.predict(test_x)\nsubmission.to_csv(\"submission_rf.csv\",index=False)","784d55b5":"# Gradient boosting tunning\n\nGBC = GradientBoostingClassifier()\ngb_param_grid = {'loss' : [\"deviance\"],\n              'n_estimators' : [100,200,300],\n              'learning_rate': [0.1, 0.05, 0.01],\n              'max_depth': [4, 8],\n              'min_samples_leaf': [100,150],\n              'max_features': [0.3, 0.1] \n              }\n\ngsGBC = GridSearchCV(GBC,param_grid = gb_param_grid, cv=kfold, scoring=\"accuracy\", n_jobs= 4, verbose = 1)\n\ngsGBC.fit(train_x, train_y)\n\nGBC_best = gsGBC.best_estimator_\n\n# Best score\ngsGBC.best_score_","29b706a8":"submission['Survived'] = gsGBC.predict(test_x)\nsubmission.to_csv(\"submission_GBC.csv\",index=False)","50dd6688":"### SVC classifier\nSVMC = SVC(probability=True)\nsvc_param_grid = {'kernel': ['rbf'], \n                  'gamma': [ 0.001, 0.01, 0.1, 1],\n                  'C': [1, 10, 50, 100,200,300, 1000]}\n\ngsSVMC = GridSearchCV(SVMC,param_grid = svc_param_grid, cv=kfold, scoring=\"accuracy\", n_jobs= 4, verbose = 1)\n\ngsSVMC.fit(train_x, train_y)\n\nSVMC_best = gsSVMC.best_estimator_\n\n# Best score\ngsSVMC.best_score_","58b8cff2":"from sklearn.model_selection import learning_curve\ndef plot_learning_curve(estimator, title, X, y, ylim=None, cv=None,\n                        n_jobs=-1, train_sizes=np.linspace(.1, 1.0, 5)):\n    \"\"\"Generate a simple plot of the test and training learning curve\"\"\"\n    plt.figure()\n    plt.title(title)\n    if ylim is not None:\n        plt.ylim(*ylim)\n    plt.xlabel(\"Training examples\")\n    plt.ylabel(\"Score\")\n    train_sizes, train_scores, test_scores = learning_curve(\n        estimator, X, y, cv=cv, n_jobs=n_jobs, train_sizes=train_sizes)\n    train_scores_mean = np.mean(train_scores, axis=1)\n    train_scores_std = np.std(train_scores, axis=1)\n    test_scores_mean = np.mean(test_scores, axis=1)\n    test_scores_std = np.std(test_scores, axis=1)\n    plt.grid()\n\n    plt.fill_between(train_sizes, train_scores_mean - train_scores_std,\n                     train_scores_mean + train_scores_std, alpha=0.1,\n                     color=\"r\")\n    plt.fill_between(train_sizes, test_scores_mean - test_scores_std,\n                     test_scores_mean + test_scores_std, alpha=0.1, color=\"g\")\n    plt.plot(train_sizes, train_scores_mean, 'o-', color=\"r\",\n             label=\"Training score\")\n    plt.plot(train_sizes, test_scores_mean, 'o-', color=\"g\",\n             label=\"Cross-validation score\")\n\n    plt.legend(loc=\"best\")\n    return plt\n\ng = plot_learning_curve(gsRFC.best_estimator_,\"RF mearning curves\",train_x, train_y,cv=kfold)\ng = plot_learning_curve(gsExtC.best_estimator_,\"ExtraTrees learning curves\",train_x, train_y,cv=kfold)\ng = plot_learning_curve(gsSVMC.best_estimator_,\"SVC learning curves\",train_x, train_y,cv=kfold)\ng = plot_learning_curve(gsGBC.best_estimator_,\"GradientBoosting learning curves\",train_x, train_y,cv=kfold)","f028e7d9":"votingC = VotingClassifier(estimators=[('rfc', RFC_best), ('extc', ExtC_best),\n('svc', SVMC_best), ('gbc',GBC_best)], voting='soft', n_jobs=4)\n\nvotingC = votingC.fit(train_x, train_y)","b2c16961":"submission['Survived'] = votingC.predict(test_x)\nsubmission.to_csv(\"ensemble_voting.csv\",index=False)","c9632e04":"## **Data visualization**","7a32c008":"**Architecture:**\n\n* For each i in range(10):\n    \n    * seed = i\n    \n    * GridSearchCV for RandomForest()","d39ed578":"**Plot learning curve**","2295b975":"NA value:","86b15ec2":"## **Average bagging**","3d6f9c51":"# **Modeling**","8a56dafb":"## **Data overview**","826e531a":"# **Import data**","0805a642":"# **Feature selection**","fd64084a":"## **KFold**","a8422480":"Combine train & test data to pre-processing","666e83bf":"**Ensemble by Voting**","c0e47fd1":"## **Stacking**","0a734b99":"# **Data exploratory analysis**"}}