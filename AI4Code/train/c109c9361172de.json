{"cell_type":{"cca4060d":"code","dec4bb16":"code","4110ec06":"code","93c37afa":"code","34713fa7":"code","b7facd7f":"code","d7e2b1c3":"code","d59b1e28":"code","bc1c6590":"code","83afbbec":"code","1a4f62a5":"code","15881373":"code","92e88411":"markdown","f8ed2b9e":"markdown","0f8b461b":"markdown","f0cf10db":"markdown","3308d06e":"markdown","6837ada8":"markdown","9984d7c9":"markdown","116271ac":"markdown","0cd468a6":"markdown"},"source":{"cca4060d":"# \ubd84\uc11d \uae30\ubcf8 \ub3c4\uad6c\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport gc\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.decomposition import PCA\nfrom sklearn.cluster import KMeans\nimport xgboost as xgb\nimport lightgbm as lgb\nfrom sklearn.ensemble import GradientBoostingRegressor\nfrom sklearn.model_selection import GridSearchCV\nfrom lightgbm import LGBMRegressor\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import KFold\n%matplotlib inline\n","dec4bb16":"\ndef haversine_array(lat2, lng2):\n    lat1, lng1 = 47.63, -122.22\n    lat1, lng1, lat2, lng2 = map(np.radians, (lat1, lng1, lat2, lng2))\n    AVG_EARTH_RADIUS = 6371  # in km\n    lat = lat2 - lat1\n    lng = lng2 - lng1\n    d = np.sin(lat * 0.5) ** 2 + np.cos(lat1) * np.cos(lat2) * np.sin(lng * 0.5) ** 2\n    h = 2 * AVG_EARTH_RADIUS * np.arcsin(np.sqrt(d))\n    return h\n\ndef rmse_exp(predictions, dmat):\n    labels = dmat.get_label()\n    diffs = np.expm1(predictions) - np.expm1(labels)\n    squared_diffs = np.square(diffs)\n    avg = np.mean(squared_diffs)\n    return ('rmse_exp', np.sqrt(avg))\n\ndef print_best_params(model, params):\n    grid_model = GridSearchCV(\n        model, \n        param_grid = params,\n        scoring='neg_mean_squared_error',\n        cv=5,\n        n_jobs=-1\n    )\n\n    grid_model.fit(X_train, y_train)\n    rmse = np.sqrt(-1*grid_model.best_score_)\n    print(\n        '{0} 5 CV \uc2dc \ucd5c\uc801 \ud3c9\uade0 RMSE \uac12 {1}, \ucd5c\uc801 alpha:{2}'.format(model.__class__.__name__, np.round(rmse, 6), grid_model.best_params_))\n    return grid_model.best_estimator_\n\ndef zipcode_groupby(train, test, group_col, colname, agg_method) :\n    new_colname = 'price_per'+'_'+colname\n    #new_colname2 = colname+'mean'\n    \n    train[new_colname] = train['price']\/train[colname]\n    price_per_temp = train.groupby([group_col])[new_colname].agg(agg_method)\n    price_per_temp.columns = ['{}_{}'.format(new_colname, m) for m in agg_method]\n    price_per_temp = price_per_temp.reset_index()\n    #price_per_temp.rename(columns={'mean':new_colname2}, inplace=True)\n    train = pd.merge(train, price_per_temp, how='left', on=group_col)\n    test = pd.merge(test, price_per_temp, how='left', on=group_col)\n    \n    del train[new_colname]\n    \n    return train, test\n\ndef groupby_helper(df, group_col, target_col, agg_method, prefix_param=None):\n    try:\n        prefix = get_prefix(group_col, target_col, prefix_param)\n        print(group_col, target_col, agg_method)\n        group_df = df.groupby(group_col)[target_col].agg(agg_method)\n        group_df.columns = ['{}_{}'.format(prefix, m) for m in agg_method]\n    except BaseException as e:\n        print(e)\n    return group_df.reset_index()\n\ndef get_prefix(group_col, target_col, prefix=None):\n    if isinstance(group_col, list) is True:\n        g = '_'.join(group_col)\n    else:\n        g = group_col\n    if isinstance(target_col, list) is True:\n        t = '_'.join(target_col)\n    else:\n        t = target_col\n    if prefix is not None:\n        return prefix + '_' + g + '_' + t\n    return g + '_' + t\n\ndef category_feature_distribution(train, col, target='price'):\n    fig, ax = plt.subplots(1, 2, figsize=(16,4))\n    \n    for c in sorted(train[col].unique()):\n        sns.distplot(train.loc[train[col]==c, target], ax=ax[0])\n    ax[0].legend(sorted(train[col].unique()))\n    ax[0].set_title(f'{col} {target} distribution')\n\n    sns.boxplot(x=col, y=target, data=train, ax=ax[1])\n    ax[1].set_title(f'{col} vs {target}')\n    \n    plt.show()\n    \ndef haversine_array_new(lat1_raw, lng1_raw, lat2, lng2):\n    lat1, lng1 = lat1_raw, lng1_raw\n    lat1, lng1, lat2, lng2 = map(np.radians, (lat1, lng1, lat2, lng2))\n    AVG_EARTH_RADIUS = 6371  # in km\n    lat = lat2 - lat1\n    lng = lng2 - lng1\n    d = np.sin(lat * 0.5) ** 2 + np.cos(lat1) * np.cos(lat2) * np.sin(lng * 0.5) ** 2\n    h = 2 * AVG_EARTH_RADIUS * np.arcsin(np.sqrt(d))\n    return h\n\ndef haversine_array_low(lat2, lng2):\n    lat1, lng1 = 47.382, -122.247\n    lat1, lng1, lat2, lng2 = map(np.radians, (lat1, lng1, lat2, lng2))\n    AVG_EARTH_RADIUS = 6371  # in km\n    lat = lat2 - lat1\n    lng = lng2 - lng1\n    d = np.sin(lat * 0.5) ** 2 + np.cos(lat1) * np.cos(lat2) * np.sin(lng * 0.5) ** 2\n    h = 2 * AVG_EARTH_RADIUS * np.arcsin(np.sqrt(d))\n    return h","4110ec06":"train = pd.read_csv('..\/input\/train.csv')\ntest = pd.read_csv('..\/input\/test.csv')\ndata = pd.merge(train, test, how='outer')\n\ntrain_raw = train\ntrain_len = len(train)\n\ndata['min_haver'] = np.nan\nfor i in range(len(data)) :\n    temp_lat, temp_lng = data[['lat', 'long']].loc[i,'lat'], data[['lat', 'long']].loc[i,'long']\n    temp_all_df = data.drop([i], 0)\n    temp_coord_df = haversine_array_new(temp_lat, temp_lng, temp_all_df['lat'], temp_all_df['long'])\n    temp_min = temp_coord_df.min()\n    data.loc[i,'min_haver'] = temp_min    ","93c37afa":"######## zipcode \ub77c\ubca8\ub9c1\ntrain = data.iloc[:train_len,:]\ntest = data.iloc[train_len:,:]\n\nfor df in [train, test] :\n    #df['zip_1'] = df['zipcode'].apply(lambda x : str(x)[2]).astype(int)\n    df['zip_12'] = df['zipcode'].apply(lambda x : str(x)[2:4]).astype(int)\n    #df['zip_2'] = df['zipcode'].apply(lambda x : str(x)[3]).astype(int)\n    #df['zip_23'] = df['zipcode'].apply(lambda x : str(x)[3:5]).astype(int)\n    #df['zip_3'] = df['zipcode'].apply(lambda x : str(x)[4]).astype(int)\n\nle = LabelEncoder()\n\nle.fit(train['zipcode'])\nle.fit(test['zipcode'])\n\ntrain['zipcode'] = le.transform(train['zipcode'])\ntest['zipcode'] = le.transform(test['zipcode'])\n\ntrain['sqft_total_size'] = train['sqft_above'] + train['sqft_basement'] # \ucd1d \uc8fc\uac70 \uba74\uc801    \ntest['sqft_total_size'] = test['sqft_above'] + test['sqft_basement']\n\n####### zipcode_groupby \ub2e8\uac00 \ubcc0\uc218 \uc0dd\uc131\ntrain['price_per_land_area'] = train['price'] \/ (train['sqft_living'])\nprice_per_ft = train.groupby(['zipcode'])['price_per_land_area'].agg({'mean', 'std', 'count'}).reset_index()\ntrain = pd.merge(train, price_per_ft, how='left', on='zipcode')\ntest = pd.merge(test, price_per_ft, how='left', on='zipcode')\ndel train['price_per_land_area']\n\n####### train, test set \uc9c0\uc815\n\nX_train = train.drop(['id', 'price'], 1)\ny_train = train['price']\ny_train = np.log1p(y_train)\nX_test = test.drop(['id', 'price'], axis=1)\n\n####### KMeans Clustering\nkm_n = 120\nkm = KMeans(n_clusters=km_n, random_state=2019)\nkm.fit(X_train[['lat', 'long']])#\n\n######## \uc774\uc678 Feature Engineering\nfor df in [X_train, X_test]:\n    df['date(new)'] = df['date'].apply(lambda x: int(x[4:8])+800 if x[:4] == '2015' else int(x[4:8])-400) # \ub0a0\uc9dc \uc904 \uc138\uc6b0\uae30\n    df['how_old'] = df['date'].apply(lambda x: x[:4]).astype(int) - df[['yr_built', 'yr_renovated']].max(axis=1) # \uc5bc\ub9c8\ub098 \ub410\ub294\uc9c0 \uc5f0\uc2dd\n    df['yr_built'] = df['yr_built'] - 1900 # \uac74\ucd95\ub144\ub3c4 1900\ub144\ub3c4\ub85c\ubd80\ud130 \uc5bc\ub9c8\ub098 \ub410\ub294\uc9c0 \n    \n    # sqft \uad00\ub828\n    df['sqft_diff'] = df['sqft_living15'] - df['sqft_living']\n    df['sqft_living_lot_diff'] = df['sqft_lot'] - df['sqft_living']\n    #df['sqft_living_lot_div'] = df['sqft_living'] \/ df['sqft_lot']\n    del df['sqft_lot15'], df['yr_renovated'], df['sqft_lot'],df['date']#, df['waterfront']#, df['view'], df['condition']\n    \n    #df['sqft_living_all'] = df['sqft_living'] + df['sqft_living15']\n    #df['living_div'] = df['sqft_living15'] \/ df['sqft_living_all']    \n    \n    # \ubc29 \uad00\ub828\n    df['sqft_bedrooms'] = df['sqft_total_size'] \/ (df['bedrooms'] + 1) # \ubc29\ud558\ub098\ub2f9 \uba74\uc801 : bedrooms\uac00 0\uc778 \uc790\ub8cc \uaf64 \uc788\uc74c \n    \n    \n    # KMeans \ud074\ub7ec\uc2a4\ud130\ub9c1\n    km_col_name = 'km'+ '_' + str(km_n)\n    df[km_col_name] = km.predict(df[['lat', 'long']])\n    \n    # \ub808\ubca8 \uad00\ub828\n    df['sum_level'] = df['grade'] + df['view'] + df['condition'] # \ub4f1\uae09 \ucd1d\ud569\n    df['multi_level'] = df['grade'] * (df['view']+1) * df['condition']\n    \n    df['condition_2'] = df['condition'].apply(lambda x : 0 if x < 3 else x)\n    df['sum_level_2'] = df['view'] + df['condition_2'] + df['grade']\n    \n    del df['condition_2']\n    \n    df['low_cond']=df['condition'].apply(lambda x : 0 if x <= 2 else 1)\n    df['low_view'] = df['view'].apply(lambda x : 0 if x == 0 else 1)\n    df['low_bath'] = df['bathrooms'].apply(lambda x: 0 if x < 1 else 1)\n    df['low_bed'] = df['bedrooms'].apply(lambda x : 0 if x ==1 else 1)\n    df['low_grade'] = df['grade'].apply(lambda x : 0 if x <= 6 else 1)\n    df['low_all'] = (df['low_cond'] + df['low_view']+df['low_bath'] + df['low_bed'] + df['low_grade']) + df['waterfront']\n    \n    del df['low_cond'], df['low_view'], df['low_bath'], df['low_bed'], df['low_grade']\n   \n    # \uac70\ub9ac \uad00\ub828\n    df['haversine_dist']= haversine_array(df['lat'], df['long']) # \uc911\uc2ec\uac00\ub85c\ubd80\ud130\uc758 \uac70\ub9ac (haversine_dist)\n    df['haversine_dist_low'] = haversine_array_low(df['lat'], df['long'])\n    #df['min_haver_multi'] = df['min_haver'] * df['haversine_dist'] \n\n# \ub85c\uadf8\ud654\nfor i in ['sqft_living', 'sqft_above', 'sqft_basement', 'sqft_living15', 'sqft_total_size', 'sqft_bedrooms'] :\n    X_train[i] = np.log1p(X_train[i])\n    X_test[i] = np.log1p(X_test[i])\n    \nX_train = X_train.drop([13522, 4123],0)\ny_train = y_train.drop([13522, 4123],0)\n\nX_train = X_train.reset_index(drop=True)\ny_train = y_train.reset_index(drop=True)\n\nprint(len(X_train.columns), X_train.columns)\nprint([item for item in X_train.columns if item not in train_raw.columns])","34713fa7":"X_train['price'] = y_train\nkm2 = KMeans(n_clusters=120, random_state=2019)\nkm2.fit(X_train[['price', 'km_120']])\nX_train['km_pre'] = km2.predict(X_train[['price', 'km_120']])\nX_train.drop('price', 1, inplace=True)\nX_train_km_pre = X_train[['km_120', 'km_pre']].groupby('km_120')['km_pre'].mean().reset_index()\nX_test = X_test.merge(X_train_km_pre, how='left', on='km_120')","b7facd7f":"plt.figure(figsize=(10,7))\nplt.scatter(data['long'], data['lat'], s=5, c=np.log1p(data['price']), cmap='coolwarm')\nplt.yticks(np.arange(47, 47.81, 0.05))\nplt.xticks(np.arange(-122.6, -121.2, 0.15))\nplt.grid(color='#BDBDBD', linestyle='-', linewidth=0.5)\nplt.colorbar()\nplt.scatter(-122.22, 47.63, color='green')\nplt.scatter(-122.247, 47.382, color='purple')","d7e2b1c3":"X_train['price'] = y_train\nf, ax =  plt.subplots(2,2, figsize=(20, 10))\nsns.boxplot(x='sum_level', y='price', data=X_train, ax=ax[0,0])\nsns.boxplot(x='sum_level_2', y='price', data=X_train, ax=ax[0,1])\nsns.boxplot(x='bathrooms', y='price', data=X_train, ax=ax[1,0])\nsns.boxplot(x='low_all', y='price', data=X_train, ax=ax[1,1])","d59b1e28":"plt.figure(figsize=(10,7))\nplt.scatter(X_train['long'], X_train['lat'], s=5, c=X_train['km_120'])\nplt.yticks(np.arange(47, 47.81, 0.05))\nplt.xticks(np.arange(-122.6, -121.2, 0.15))\nplt.grid(color='#BDBDBD', linestyle='-', linewidth=0.5)\nplt.colorbar()","bc1c6590":"X_train.drop('price', 1, inplace=True)","83afbbec":"%%time\ndtrain = lgb.Dataset(X_train, label=y_train)\ndtest  = lgb.Dataset(X_test)\n\nlgb_params = {\n    'boosting_type': 'gbdt',\n    'objective':'regression',\n    'num_leave' : 1,\n    'learning_rate' : 0.03,\n    'max_depth' : 6,\n    'colsample_bytree' : 0.4,\n    'subsample' : 0.4,\n    'max_bin' : 80,\n    'gpu_id':0,         \n    'tree_method':'gpu_hist',\n    'predictor':'gpu_predictor',\n    'refit':True,\n    'metric' : 'rmse',\n    'seed' : 2019\n}\n\ncv_lgb_output = lgb.cv(lgb_params, dtrain, num_boost_round=5000, nfold=5, early_stopping_rounds=200, verbose_eval=100,stratified=False)\n\nprint('best_num_rounds :',len(cv_lgb_output['rmse-mean']))\nprint('best_cv_score :', cv_lgb_output['rmse-mean'][-1])\n\nbest_num_rounds = len(cv_lgb_output['rmse-mean'])\n\nmodel_lgb = lgb.train(lgb_params, dtrain, num_boost_round=best_num_rounds)\nlgb_pred_log = model_lgb.predict(X_test)\nlgb_pred = np.expm1(lgb_pred_log)\n","1a4f62a5":"xgb_params = {\n    'eta': 0.02,\n    'max_depth': 6,\n    'subsample': 0.8,\n    'colsample_bytree': 0.4,\n    'objective': 'reg:linear',    \n    'eval_metric': 'rmse',        \n    'silent': True,               \n    'seed' : 1984\n}\n\n\n\n# transform\ndtrain = xgb.DMatrix(X_train, y_train)\ndtest = xgb.DMatrix(X_test)\n\n# cross validation\ncv_output = xgb.cv(xgb_params,\n                   dtrain,                        \n                   num_boost_round=5000,         \n                   early_stopping_rounds=150,    \n                   nfold=5,                      \n                   verbose_eval=100,             \n                   feval=rmse_exp,               \n                   maximize=False,\n                   show_stdv=False,              \n                   )\n\n# scoring\nbest_rounds = cv_output.index.size\nscore = round(cv_output.iloc[-1]['test-rmse_exp-mean'], 2)\n\nprint(f'\\nBest Rounds: {best_rounds}')\nprint(f'Best Score: {score}')\n\nmodel = xgb.train(xgb_params, dtrain, num_boost_round=best_rounds)\nxgb_pred_log = model.predict(dtest)\nxgb_pred = np.expm1(xgb_pred_log)\n\n# plotting\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14,5))\ncv_output[['train-rmse-mean', 'test-rmse-mean']].plot(ax=ax1)\nax1.set_title('RMSE_log', fontsize=20)\ncv_output[['train-rmse_exp-mean', 'test-rmse_exp-mean']].plot(ax=ax2)\nax2.set_title('RMSE', fontsize=20)\n\nplt.show()\n","15881373":"ensemble_pred = np.vstack([lgb_pred, xgb_pred]).mean(0)\nsample_submission = pd.read_csv('..\/input\/sample_submission.csv')\nsubmission = pd.DataFrame(data = {'id': test['id'], 'price': ensemble_pred})\nsubmission.to_csv('submission_ensemble.csv', index=False)","92e88411":"* \uc704\ub97c \ubcf4\uc2dc\uba74, \uc81c\uac00 \uae30\uc900\uc810\uc73c\ub85c \uc4f4 \uc810\ub4e4\uc774 \ucd08\ub85d\uc0c9\uacfc \ubcf4\ub77c\uc0c9\uc73c\ub85c \ubcf4\uc785\ub2c8\ub2e4. \n* \uc800\ub294 \uc81c\uac00 \ubcf4\uae30\uc5d0 \uac00\uc7a5 \ube68\uac04\uc0c9\uacfc \ud30c\ub780\uc0c9(\uac00\uaca9\uc774 \ub192\uace0 \ub0ae\uc740) \uc810\uc774 \ub9ce\uc774 \uc788\ub294 \uacf3\uc758 \uc911\uac04\uc758 lat, long\uc744 \uae30\uc900\uc810\uc73c\ub85c \uc0bc\uc558\uc2b5\ub2c8\ub2e4.\n* \uac00\uaca9\uc774 \ub0ae\uc740 \uacf3\uc73c\ub85c\ubd80\ud130 \uac70\ub9ac\ub294 \ubaa8\ub974\uaca0\uc9c0\ub9cc \uac00\uaca9\uc774 \ub192\uc740 \uac83\uc73c\ub85c\ubd80\ud130 \uac70\ub9ac\ub294 \uc810\uc218 \uc0c1\uc2b9\uc5d0 \ub9ce\uc740 \uae30\uc5ec\ub97c \ud588\uc2b5\ub2c8\ub2e4.\n* \uc774 \ucee4\ub110\uc744 \ucc38\uace0\ud558\uc2dc\uba74 \ub354 \ub9ce\uc740 \uc815\ubcf4\ub97c \uc5bb\uc73c\uc2e4 \uc218 \uc788\uc2b5\ub2c8\ub2e4. https:\/\/www.kaggle.com\/gaborfodor\/from-eda-to-the-top-lb-0-367","f8ed2b9e":"> sum_level_2, low_all","0f8b461b":"> 1) haversine_dist, haversine_dist_low","f0cf10db":"\ub9c8\uc9c0\ub9c9\uae4c\uc9c0 \ubc30\uc6b8 \uc810\uc774 \ub9ce\uc740 \ub300\ud68c\uc785\ub2c8\ub2e4. \uce90\uae00 \ub2e8\ud1a1\ubc29\uc5d0\ub3c4 \ud55c\ubc88 \uc598\uae30\uac00 \ub098\uc628\uc801\uc774 \uc788\ub294 \uac83 \uac19\uc2b5\ub2c8\ub2e4\ub9cc, <br>\nfeature \uac04\uc758 \uc21c\uc11c\ub9cc \ubc14\ub00c\uc5b4\ub3c4 \uacb0\uacfc\uac12\uc774 \uc870\uae08 \ub2e4\ub974\uac8c \ub098\uc624\ub294 \uac70 \uac19\uc2b5\ub2c8\ub2e4. <br>\n\uc81c Leaderboard\uc0c1\uc758 \uc810\uc218\uac00 \ub098\uc628 Feature\uac00 \uc815\ud655\ud788 \uae30\uc5b5\uc774 \uc548\ub098\uc11c ('haversine_dist_low'\ud3ec\ud568\uc5ec\ubd80\uac00 \uc815\ud655\ud558\uc9c0 \uc54a\uc74c)\n\uba87\ubc88 \ub2e4\uc2dc \ub3cc\ub824\ubd24\uc74c\uc5d0\ub3c4 \uc644\uc804\ud788 \ub611\uac19\uc740 \uc810\uc218\ub294 \ub098\uc624\uc9c0 \uc54a\uc558\uc2b5\ub2c8\ub2e4.\n\uadf8\ub798\uc11c \uba3c\uc800 \uc591\ud574\ub97c \uad6c\ud558\uace0 \uc2f6\uc740 \uc810\uc740 \uc81c \ud604\uc7ac Leaderboard \uc0c1\uc758 score\ub294 104991.88329\uc774\uc9c0\ub9cc, \uc544\ub798 \ucee4\ub110\uc744 \ub3cc\ub9ac\uc2dc\uba74, <br>\nPrivate score \uae30\uc900\uc73c\ub85c 'haversine_dist_low'\ub97c \ube7c\uc2dc\uba74, 105438.35505<br>\n\ud3ec\ud568\ud574\uc11c \ub3cc\ub9ac\uc2dc\uba74, 104252.30328\uc774 \ub098\uc624\uac8c \ub429\ub2c8\ub2e4. <br>\n\uae30\ub85d\uc744 \uc815\ud655\ud558\uac8c \ud574\ub1a8\uc5b4\uc57c \ud588\ub294\ub370, \uadf8\ub7ec\uc9c0 \ubabb\ud55c \uc810 \uc591\ud574 \ubd80\ud0c1\ub4dc\ub9bd\ub2c8\ub2e4. <br>\n(\uc9c0\uae08 \ucee4\ub110\ub85c \ub3cc\ub9b0 \uc810\uc218\ub294 104747.22031\uac00 \ub098\uc624\uac8c \ub418\ub124\uc694.\uc65c \uadf8\ub7f0\uc9c0 \uc798 \ubaa8\ub974\uaca0\uc2b5\ub2c8\ub2e4;)\n<br><br>\n\uc800\ub294 \uc544\ub798 \ucee4\ub110\ub4e4\uc744 \ucc38\uace0\ud558\uc5ec \uc9c4\ud589\ud558\uc600\uc2b5\ub2c8\ub2e4. <br>\n\uc774\uc678 \uc800\uc5d0\uac8c \ub9ce\uc740 \uac00\ub974\uce68\uc744 \uc8fc\uc2e0 \ubaa8\ub4e0 \ubd84\ub4e4\uaed8 \uac10\uc0ac\ub9d0\uc500\ub4dc\ub9bd\ub2c8\ub2e4!!<br>\n\n* https:\/\/www.kaggle.com\/dhznsdl\/house-price-regression-python\n* https:\/\/www.kaggle.com\/kcs93023\/2019-ml-month-2nd-baseline\n* https:\/\/www.kaggle.com\/chocozzz\/house-price-prediction-eda-updated-2019-03-12\n* https:\/\/www.kaggle.com\/yeonmin\/default-eda-stacking-introduction\n* https:\/\/www.kaggle.com\/ivoryrabbit\/a-note-on-using-a-single-model-xgboost\n* https:\/\/www.kaggle.com\/tmheo74\/geo-data-eda-and-feature-engineering","3308d06e":">** \uc2e0\uaddc Feature \uc124\uba85**\n* min_haver : \uc774\uc6c3 \uc911 \uac00\uc7a5 \uac00\uae4c\uc6b4 \uacf3\uacfc\uc758 \uac70\ub9ac (\ud5c8\ud0dc\uba85\ub2d8\uc758 \ucee4\ub110 \ucc38\uace0)\n* zip_12 : zipcode \uc911 98\uc744 \uc81c\uc678\ud55c 1,2\ubc88\uc9f8 \uc22b\uc790 (\ud5c8\ud0dc\uba85\ub2d8\uc758 \ucee4\ub110 \ucc38\uace0)\n* sqft_total_size : sqft_above + sqft_basement\n* std, mean, count : std, mean\uc740 zipcode\ubcc4 price\/sqft_total_size\uc758 \ud45c\uc900\ud3b8\ucc28, \ud3c9\uade0, count\ub294 zipcode\ubcc4 count \ud69f\uc218\n* date(new), how_old : (DongGyu Lee\ub2d8\uc758 \ucee4\ub110 \ucc38\uace0)\n* sqft_diff : sqft_living15 - sqft_living\n* sqft_living_lot_diff : sqft_lot - sqft_living\n* sqft_bedrooms : sqft_living \/ (sqft_bedrooms + 1)\n* sum_level : grade + view + condition\n* multi_level : grade * view * condition\n* sum_level_2 : view + condition(3\ubbf8\ub9cc\uc778 \uac83\uc740 0\uc73c\ub85c \uce58\ud658) + grade\n* low_all : condtion(2\uc774\ud558\ub294 0, \ub098\uba38\uc9c0\ub294 1) + view(0\uc778\uac83\uc740 0 \ub098\uba38\uc9c0\ub294 1) + bath(1\ubbf8\ub9cc\uc740 0 \ub098\uba38\uc9c0\ub294 1) + bed(1\uc774\uba74 0 \ub098\uba38\uc9c0\ub294 1) + grade(6\uc774\ud558\uba74 0, \ub098\uba38\uc9c0\ub294 1) + waterfront\n* haversine_dist : 47.63, -122.22\uc73c\ub85c\ubd80\ud130 \uac70\ub9ac\n* haversine_dist_low : 47.382, -122.247\n* km_120 : 120\uac1c\ub85c KMeans \ud074\ub7ec\uc2a4\ud130\ub9c1 lat, long\n* km_pre : price\uc640 km_120 KMeans \ud074\ub7ec\uc2a4\ud130\ub9c1 \ud6c4 test set\uc5d0\ub294 merge","6837ada8":"multi_level\uae4c\uc9c0\ub294 \ub300\ubd80\ubd84 \uc544\uc2e4\uac70\ub77c \uc0dd\uac01\ud569\ub2c8\ub2e4. \uadf8 \uc774\ud6c4 \ub098\uc624\ub294 \uac83\ub4e4\uc5d0 \ub300\ud574 \uc2dc\uac01\ud654\ub97c \uc9c4\ud589\ud558\uace0\uc790 \ud569\ub2c8\ub2e4.","9984d7c9":"* \uc800\ub294 \uc774\ubc88 \ub300\ud68c \ub0b4\ub0b4 oof\ub97c \ud1b5\ud574\uc11c \uc81c\uac00 \uc124\uacc4\ud558\ub294 \ubaa8\ub378\uc774 log \ubcc0\ud658\ud6c4 12\ubbf8\ub9cc \uac00\uaca9\uc744 \uc81c\ub300\ub85c \uc608\uce21\ud558\uc9c0 \ubabb \ud55c\ub2e4\ub294 \uac78 \uc54c\uac8c\ub410\uc2b5\ub2c8\ub2e4.\n* \uc774\uc5d0 \uc5b4\ub5bb\uac8c \ud558\uba74, 12\ubbf8\ub9cc \uac00\uaca9\uc758 \uc9d1\uac12\uc744 \uc608\uce21\ud560\uc9c0 \uace0\ubbfc\ud558\uba70 \ub9cc\ub4e0 feature\uac00 sum_level_2\uc640 low_all\uc785\ub2c8\ub2e4.\n* \uc810\uc218\uc0c1\uc2b9\uc5d0 \uadf8\ub807\uac8c\uae4c\uc9c0 \ud070 \uae30\uc5ec\ub97c \ud588\ub294\uc9c0\ub294 \uc758\ubb38\uc774\ub124\uc694;","116271ac":"- \uc800\ub294 \ud5c8\ud0dc\uba85\ub2d8\uc758 \ucee4\ub110\uacfc \ub2ec\ub9ac \uc880 \ub354 \ub9ce\uc740 \ud074\ub7ec\uc2a4\ud130\ub85c \ub098\ub220\ubcf4\uc558\uc2b5\ub2c8\ub2e4 (120)\n- \uadf8\ub9ac\uace0 \uc774\ub97c \ubc14\ud0d5\uc73c\ub85c km120\uacfc price\ub97c kmeans \ud6c4 X_test\uc5d0 km120 \uae30\uc900\uc73c\ub85c merge\ub97c \ud574\ubcf4\uc558\uc2b5\ub2c8\ub2e4","0cd468a6":"> km_120"}}