{"cell_type":{"384569a4":"code","c111cab7":"code","fa9bad60":"code","5e094612":"code","99efaaef":"code","ee4b833d":"code","8ce95c3d":"code","5438d907":"code","48194eb4":"code","6f3ecdc5":"code","39b596be":"code","abfaed21":"code","173068db":"code","d3e60774":"code","86361637":"code","13cb6bc4":"markdown","80a19489":"markdown","d925a692":"markdown","876a3727":"markdown","09fc3c16":"markdown","838d4a2e":"markdown","57149f9d":"markdown"},"source":{"384569a4":"import pandas as pd \nimport numpy as np \nimport matplotlib.pyplot as plt \nimport wandb \nfrom kaggle_secrets import UserSecretsClient\nimport os, pickle, requests, json \nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.preprocessing import MinMaxScaler \nfrom sklearn.model_selection import GroupKFold\n\nimport tensorflow as tf \nfrom tensorflow.keras.layers import Dense, Dropout, LSTM , Bidirectional, Input, Flatten\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.callbacks import EarlyStopping\nfrom tensorflow.keras.callbacks import LearningRateScheduler\nfrom tensorflow.keras.optimizers.schedules import ExponentialDecay\n","c111cab7":"config = dict(\n    competition = \"covid19-transition\", \n    n_fold = 1, \n    infra = \"kaggle\",\n    epoch = 200 ,\n    optimizer = \"Adam\", \n    lr = 0.0002, \n    model_name = \"bidirectional-lstm\", \n    frame_work = \"tensorflow\", \n    device = \"cpu\", \n    ealry_stopping_rounds = 30, \n    batch_size = 12, \n    train = True, \n    inference = True, \n    debug = False, \n    seed = 42, \n    verbose = 100, \n    type = \"train\", \n    input_shape = (272, 1)\n)","fa9bad60":"user_secrets = UserSecretsClient()\nurl = user_secrets.get_secret(\"WEB_HOOK_URL\") \n\nuser_secrets = UserSecretsClient()\napi = user_secrets.get_secret(\"wandb_api\")\n\nwandb.login(key=api)\n\nrun = wandb.init(\n    project = config[\"competition\"], \n    name = config[\"model_name\"], \n    config = config, \n    group = config[\"model_name\"], \n    job_type = config[\"type\"]\n)\n\ndef slack(txt):\n    requests.post(url, data=json.dumps({\n        \"username\": \"kaggle\", \n        \"text\": txt \n    }))","5e094612":"train = pd.read_csv(\"..\/input\/covid19-japan-time-series-transition\/step1\/train.csv\")\ntrain[\"date\"] = pd.to_datetime(train.date)\ntrain = train[train.date >= \"2021-01-01\" ]\ntrain = train.fillna(0)\ntrain = train.reset_index(drop=True)\ntrain.head()","99efaaef":"\n'''\nSince we want to divide into batches in each prefecture, we need to have the same number. \nCheck the number here and you can see that there is no problem.\n'''\n\ntrain.groupby(\"prefecture\").size().to_frame()","ee4b833d":"df = train.pivot_table(values=\"newly_confirmed\", columns=\"date\", index=\"prefecture\")\ndf.head()","8ce95c3d":"def scaler(tr, va, tes):\n    s = MinMaxScaler(feature_range=(0.0, 1.0))\n    return s.fit_transform(tr), s.transform(va), s.transform(tes)\n\n\ndef train_test_splits(df):\n    n_span = 274 \n    # create trianing dataset \n    x_train = df.iloc[:, :n_span-2] # (47, 272)\n    x_val = df.iloc[:, 1:n_span-1]\n    x_test = df.iloc[:, 2:n_span]\n    # newly confirmed scaler \n    x_train, x_val, x_test = scaler(x_train, x_val, x_test)\n    # create lable dataset \n    y_train = df.iloc[:, n_span-2] # (47, )\n    y_val = df.iloc[:, n_span-1]\n    # (batch, time-step, 1)\n    x_train = x_train.reshape(47, -1, 1)\n    x_val = x_val.reshape(47, -1, 1)\n    x_test = x_test.reshape(47, -1, 1)\n    y_train = y_train.values.reshape(47, -1)\n    y_val = y_val.values.reshape(47, -1)\n    \n    return x_train, x_val, x_test, y_train, y_val \n","5438d907":"x_train, x_val, x_test, y_train, y_val = train_test_splits(df)","48194eb4":"\n'''\nprefecture == batch \n'''\n\nprint(f\"input shape: {x_train.shape}\")\nprint(f\"output shape: {y_train.shape}\")","6f3ecdc5":"def build_model(input_shape=config[\"input_shape\"]):\n    model = Sequential()\n    model.add(Input(shape=input_shape))\n    model.add(Bidirectional(LSTM(300, return_sequences=True)))\n    model.add(Bidirectional(LSTM(250, return_sequences=True)))    \n    model.add(Bidirectional(LSTM(150, return_sequences=True)))    \n    model.add(Bidirectional(LSTM(100, return_sequences=True)))    \n    model.add(Flatten())\n    model.add(Dense(50, activation=\"selu\"))\n    model.add(Dense(1))\n    model.compile(optimizer=\"adam\", loss=\"mae\")\n    return model \n\n\ndef mae(pred, corr):\n    return mean_squared_error(pred, corr, squared=False)\n\ndef checkpoint(model):\n    os.makedirs(\"models\/\", exist_ok=True)\n    model.save(\"models\/lstm.h5\")\n    print(\"success saving model.\")\n    \ndef logger(pred, corr):\n    wandb.log({\"mae\": mae(pred, corr)})\n    ","39b596be":"\ndef train_fn(x_train, x_val, x_test, y_train, y_val):\n    if config[\"device\"] == \"tpu\":\n        tpu = tf.distribute.cluster_resolver.TPUClusterResolver.connect()\n        tpu_strategy = tf.distribute.experimental.TPUStrategy(tpu)\n\n    scheduler = ExponentialDecay(config[\"lr\"], \n                                 400*((train.shape[0]*0.8)\/config[\"batch_size\"]), \n                                 1e-5)\n    lr = LearningRateScheduler(scheduler, verbose=1)\n    es = EarlyStopping(monitor=\"val_loss\",\n                       patience=15,\n                       verbose=1, \n                       mode=\"min\",\n                       restore_best_weights=True)\n    model = build_model()\n    history = model.fit(x_train,\n                          y_train, \n                         validation_data=(x_val, y_val), \n                         epochs=config[\"epoch\"],\n                         batch_size=config[\"batch_size\"], \n                         callbacks=[lr, es])\n    pred_v = model.predict(x_val).flatten()\n    pred_t = model.predict(x_test).flatten()\n    \n    checkpoint(model)\n    logger(pred_v, y_val.ravel())\n    del model \n    return history, pred_t ","abfaed21":"history, pred_t = train_fn(x_train, x_val, x_test, y_train, y_val)","173068db":"def show_metrics(history):\n    fig, axes = plt.subplots(1, 2, figsize=(15, 6))\n    ax = axes.ravel()\n    \n    loss = history.history[\"loss\"]\n    val_loss = history.history[\"val_loss\"]\n    lr = history.history[\"lr\"]\n    \n    ax[0].plot(np.arange(len(loss)).tolist(), loss, c=\"r\")\n    ax[0].plot(np.arange(len(val_loss)).tolist(), val_loss, c=\"b\")\n    ax[0].set_title(\"Loss\")\n    ax[0].set_xticks(np.arange(len(val_loss)))\n    \n    ax[1].plot(np.arange(len(lr)).tolist(), lr, c=\"pink\")\n    ax[1].set_title(\"learning Rate\")\n    ax[1].set_xticks(np.arange(len(val_loss)))\n\n    plt.tight_layout()","d3e60774":"show_metrics(history)","86361637":"pref = train.prefecture.unique()\nresult = pd.DataFrame({\"prefecture\": pref, \"newly_confirmed\": pred_t})\n\nsub = pd.read_csv(\"..\/input\/covid19-japan-time-series-transition\/step1\/sample_submission.csv\", usecols=[\"prefecture\", \"date\"])\nsub = pd.merge(sub, result, how=\"left\", on=\"prefecture\")\nsub.to_csv(\"submisson.csv\", index=False)\n\nslack(\"done.\")","13cb6bc4":"# Const ","80a19489":"# Validation split","d925a692":"# Tools ","876a3727":"# Submit ","09fc3c16":"# Dataset ","838d4a2e":"# Model ","57149f9d":"# Train "}}