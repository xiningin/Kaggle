{"cell_type":{"1562d274":"code","7dd1a7cf":"code","82ef6375":"code","beb4e9b3":"code","23df04b0":"code","ea743834":"code","09628112":"code","c083dc7b":"code","21def9a8":"code","513b741e":"code","2484a9ef":"code","620d2b7a":"code","11fd2aae":"code","925b294f":"code","fc418355":"code","4645d797":"code","6fe465bc":"code","c0e18a69":"code","4e0e7816":"code","63bee383":"code","d1bdf720":"code","23b17895":"code","895aa5c0":"code","28e9d474":"code","77b8327e":"code","8485538e":"code","a50f7654":"code","4c542614":"code","267bf56c":"code","d7ba94e0":"code","338bb63f":"code","d154eae2":"code","e5ebafd6":"code","581735b4":"code","b2c56acf":"code","f3f3b0dd":"code","79ce5fb0":"code","ce80109f":"code","6242da97":"code","079da3cb":"code","6dba148a":"code","ec31ef76":"code","f6c57d6c":"code","b22332ae":"code","4caac364":"code","d8506dd3":"code","e18ea554":"code","a3cea979":"markdown","35678f91":"markdown","af3bc8ff":"markdown","fb2670b9":"markdown","a436d584":"markdown","9a7f7921":"markdown","82cc2ebf":"markdown","701beac6":"markdown","c3b5784a":"markdown","985ffd9d":"markdown","0015c3d8":"markdown","a2f5f539":"markdown","688837da":"markdown","fdf2f63c":"markdown","35b5ea64":"markdown","4ab1be92":"markdown","f0eb4834":"markdown","ae26b4d6":"markdown","861dd185":"markdown","40736f92":"markdown","21dbc601":"markdown","0fc35c3d":"markdown","ad511421":"markdown","2aeb915c":"markdown","8835cec3":"markdown","637bfdd2":"markdown","3380cff2":"markdown","bc5450c2":"markdown","394e2e4b":"markdown","14624c00":"markdown","dc067c62":"markdown","0134c8e3":"markdown","06465394":"markdown","019ada2b":"markdown","1785a54e":"markdown","5dfd52f4":"markdown","d8b244bd":"markdown","b90375ac":"markdown","be5e6d77":"markdown","82349f7f":"markdown","4abc7045":"markdown","70395022":"markdown","fbdf85db":"markdown","aad54367":"markdown","830b418c":"markdown","6c36f2c1":"markdown","bc952bdc":"markdown","deaab518":"markdown","e49e1b89":"markdown","2d383014":"markdown","6cba14d2":"markdown","f63fc3eb":"markdown","69584d48":"markdown","54b145c1":"markdown","67c1a981":"markdown","c34c8521":"markdown"},"source":{"1562d274":"%%HTML\n<style type=\"text\/css\">\n\ndiv.h2 {\n    background-color: #E87D23; \n    color: white; \n    padding: 5px; \n    padding-right: 300px; \n    font-size: 30px; \n    max-width: 1500px; \n    margin-top: 2px;\n    margin-bottom: 10px;\n}\n<\/style>","7dd1a7cf":"# installing some cool stuff\n\n! pip install dexplot","82ef6375":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport collections as col\n\nimport dexplot as dxp\nimport plotly.express as px\nimport plotly.graph_objs as go\nfrom plotly.offline import iplot\nfrom plotly.subplots import make_subplots\n\nimport re\nimport string\nimport nltk\nnltk.download('stopwords')\nfrom nltk.corpus import stopwords\nfrom wordcloud import WordCloud\nfrom nltk.tokenize import sent_tokenize, word_tokenize\nfrom nltk.util import ngrams\nfrom sklearn.feature_extraction.text import CountVectorizer\n\nfrom sklearn.metrics.pairwise import cosine_similarity\nimport networkx as nx\n\nimport os\n\nfrom IPython.display import HTML","beb4e9b3":"# loading data\n\ntry:\n    home = '\/kaggle\/input\/chai-time-data-science'\n    epi = pd.read_csv(os.path.join(home, \"Episodes.csv\"))\n    descr = pd.read_csv(os.path.join(home, \"Description.csv\"))\n    y_thumb = pd.read_csv(os.path.join(home, \"YouTube Thumbnail Types.csv\"))\n    a_thumb = pd.read_csv(os.path.join(home, \"Anchor Thumbnail Types.csv\"))\nexcept:\n    print(\"File names have been changed. Have a look at the dataset home page.\")","23df04b0":"# integrate episodes and thumbnail types datasets\n\nepi_y_thumb = epi.merge(\n    y_thumb, left_on=\"youtube_thumbnail_type\", right_on=\"youtube_thumbnail_type\"\n)\nepi_y_thumb[\"recording_date\"] = pd.to_datetime(epi_y_thumb[\"recording_date\"])\nepi_y_thumb[\"release_date\"] = pd.to_datetime(epi_y_thumb[\"release_date\"])","ea743834":"# visualizing the change in thumbnail types over the episodes\n\ndxp.count(\n    val=\"release_date\",\n    data=epi_y_thumb,\n    split=\"description\",\n    orientation=\"h\",\n    stacked=True,\n    figsize=(12, 24),\n    xlabel=\"Number of episodes\",\n)\n","09628112":"# all 9 episodes released on 7th March 2020\n\nprint(\"All 9 episodes released on 7th March 2020 =>\")\nprint()\nfor episode_name in list(epi[epi['release_date'] == '2020-03-07']['episode_name']):\n    print(episode_name)","c083dc7b":"# utility functions to visualize data\n\n\ndef across_epi_plot(df, feature, ind=\"release_date\", title=\"You forgot the title\"):\n    \"\"\"\n    Plots an interactive line plot depicting the values of each episode\n    across `feature` in the data\n    -----\n    \n    df: The dataframe\n    feature: The feature name from the relevant dataset\n    ind: The independent feature across which the values were shown\n         `release_date` was the default\n    title: Title of the plot\n    \"\"\"\n\n    # find the mean value\n    mean_val = round(df[feature].mean(), 2)\n\n    # set the plot title\n    plt_title = title + \" (Mean=\" + str(mean_val) + \")\"\n\n    # plot the graph\n    fig = px.line(df, x=ind, y=feature, hover_data=[\"episode_id\", \"episode_name\"])\n    fig.update_traces(mode=\"lines+markers\", line_color=\"#e87d23\")\n    fig.update_layout(\n        hoverlabel=dict(bgcolor=\"white\", font_size=12, font_family=\"Rockwell\")\n    )\n    fig.add_shape(\n        # add a horizontal line\n        type=\"line\",\n        x0=\"2019-07-19\",\n        y0=mean_val,\n        x1=\"2020-06-20\",\n        y1=mean_val,\n        line=dict(color=\"black\", width=2, dash=\"dash\"),\n    )\n\n    # template enhancement\n    fig.update_layout(\n        template=\"ggplot2\",\n        title={\n            \"text\": plt_title,\n            \"font\": {\"family\": \"Rockwell\", \"size\": 20},\n            \"xanchor\": \"center\",\n            \"yanchor\": \"top\",\n        },\n        xaxis=dict(range=[\"2019-07-19\", \"2020-06-20\"]),\n    )\n\n    # show plot\n    fig.show()\n\n\ndef across_epi_plot2(df, feature, ind=\"release_date\", title=\"You forgot the title\"):\n    \"\"\"\n    Plots an interactive line plot depicting the values of each episode\n    across `feature` in the data\n    -----\n    \n    df: The dataframe\n    feature: The feature name from the relevant dataset\n    ind: The independent feature across which the values were shown\n         `release_date` was the default\n    title: Title of the plot\n    \"\"\"\n\n    # find the mean value\n    mean_val = round(df[feature].mean(), 2)\n\n    # set the plot title\n    plt_title = title + \" (Mean=\" + str(mean_val) + \")\"\n\n    # plot the graph\n    fig = px.line(df, x=ind, y=feature)\n    fig.update_traces(mode=\"lines+markers\", line_color=\"#e87d23\")\n    fig.update_layout(\n        hoverlabel=dict(bgcolor=\"white\", font_size=12, font_family=\"Rockwell\")\n    )\n    fig.add_shape(\n        # add a horizontal line\n        type=\"line\",\n        x0=\"2019-07-19\",\n        y0=mean_val,\n        x1=\"2020-06-20\",\n        y1=mean_val,\n        line=dict(color=\"black\", width=2, dash=\"dash\"),\n    )\n\n    # template enhancement\n    fig.update_layout(\n        template=\"ggplot2\",\n        title={\n            \"text\": plt_title,\n            \"font\": {\"family\": \"Rockwell\", \"size\": 20},\n            \"xanchor\": \"center\",\n            \"yanchor\": \"top\",\n        },\n        xaxis=dict(range=[\"2019-07-19\", \"2020-06-20\"]),\n    )\n\n    # show plot\n    fig.show()\n\n\ndef comp_bin_plot(\n    df,\n    f1,\n    f2,\n    l1,\n    l2,\n    title=\"You forgot the title\",\n    xtitle=\"You forgot\",\n    ytitle=\"You forgot\",\n):\n    \"\"\"\n    Plots an interactive line plot depicting the values of each episode\n    across features `f1` and `f2` in the data\n    -----\n    \n    df: Dataframe\n    f1: First feature\n    f2: Second feature\n    l1: Label 1\n    l2: Label 2\n    title: Title of the plot\n    \"\"\"\n\n    # find the means\n    m1 = round(df[f1].mean(), 2)\n    m2 = round(df[f2].mean(), 2)\n\n    # set the plot title\n    plt_title = (\n        title\n        + \" (\"\n        + l1\n        + \"_Mean=\"\n        + str(m1)\n        + \")\"\n        + \" (\"\n        + l2\n        + \"_Mean=\"\n        + str(m2)\n        + \")\"\n    )\n\n    # plot the graph\n    fig = go.Figure()\n    fig.add_trace(go.Scatter(x=df[\"episode_id\"], y=df[f1], mode=\"lines\", name=l1))\n    fig.add_trace(go.Scatter(x=df[\"episode_id\"], y=df[f2], mode=\"lines\", name=l2))\n    fig.update_traces(mode=\"lines+markers\")\n    fig.update_layout(\n        hoverlabel=dict(bgcolor=\"white\", font_size=12, font_family=\"Rockwell\")\n    )\n\n    # template enhancement\n    fig.update_layout(\n        template=\"ggplot2\",\n        title={\n            \"text\": plt_title,\n            \"font\": {\"family\": \"Rockwell\", \"size\": 20},\n            \"xanchor\": \"center\",\n            \"yanchor\": \"top\",\n        },\n        xaxis_title=xtitle,\n        yaxis_title=ytitle,\n        xaxis=dict(range=[\"2019-07-19\", \"2020-06-20\"]),\n    )\n\n    # show plot\n    fig.show()\n\n\ndef plot_corr(df, feature, title=\"You forgot the title\"):\n    \"\"\"\n    Plot correlations of appropriate features against `feature`\n    as a bar plot\n    -----\n    \n    df: The dataframe\n    feature: The feature whose correlation with other features\n             is under consideration\n    title: The title of the plot\n    \"\"\"\n\n    # features to exclude\n    ignore_list = [\n        \"youtube_thumbnail_type\",\n        \"anchor_thumbnail_type\",\n        \"anchor_plays\",\n        \"spotify_starts\",\n        \"spotify_streams\",\n        \"spotify_listeners\",\n        \"apple_listeners\",\n        \"apple_listened_hours\",\n        \"apple_avg_listen_duration\",\n    ]  # audio data is not considered here\n    ignore_list.append(\n        feature\n    )  # self-correlation is not necessary; always going to be 1\n\n    # prepare some correlation data\n    sub_corr = pd.DataFrame(df.corr()[feature])\n    sub_corr.columns = [\"Correlation\"]\n    sub_corr[\"Impact\"] = sub_corr[\"Correlation\"]\n    sub_corr = sub_corr.drop(ignore_list)\n\n    # plot the graph\n    fig = px.bar(\n        sub_corr,\n        x=sub_corr.index,\n        y=\"Correlation\",\n        color=\"Correlation\",\n        color_continuous_scale=\"oranges\",\n    )\n\n    # template enhancement\n    fig.update_layout(\n        template=\"plotly_dark\",\n        title={\n            \"text\": title,\n            \"font\": {\"family\": \"Rockwell\", \"size\": 20},\n            \"yanchor\": \"top\",\n        },\n        xaxis_title=\"\",\n    )\n\n    fig.show()\n","21def9a8":"# youtube_ctr across episodes\n\nacross_epi_plot(epi, 'youtube_ctr', title='CTR across episodes')","513b741e":"# corr with youtube_ctr\n\nplot_corr(epi, 'youtube_ctr', 'Correlations with CTR')","2484a9ef":"# thumbnail's effect on ctr\n\n\n# create the box plot\nfig = px.box(epi_y_thumb, x=\"youtube_ctr\", y=\"description\")\n\n# enhance markers\nfig.update_traces(line_color=\"#e87d23\", marker=dict(color=\"#e87d23\", size=10))\n\n# add annotations\nannotations = []\n\n# custom branding\nannotations.append(\n    dict(\n        xref=\"x\",\n        yref=\"y\",\n        x=8,\n        y=\"Custom image with CTDS branding, Title and Tags\",\n        xanchor=\"right\",\n        yanchor=\"top\",\n        text=(\n            \"Mean CTR: \"\n            + str(\n                round(\n                    epi_y_thumb[\n                        epi_y_thumb[\"description\"]\n                        == \"Custom image with CTDS branding, Title and Tags\"\n                    ][\"youtube_ctr\"].mean(),\n                    2,\n                )\n            )\n        ),\n        font=dict(family=\"Rockwell\", size=15, color=\"black\"),\n        showarrow=False,\n    )\n)\n\n# mini series\nannotations.append(\n    dict(\n        xref=\"x\",\n        yref=\"y\",\n        x=8,\n        y=\"Mini Series: Custom Image with annotations\",\n        xanchor=\"right\",\n        yanchor=\"top\",\n        text=(\n            \"Mean CTR: \"\n            + str(\n                round(\n                    epi_y_thumb[\n                        epi_y_thumb[\"description\"]\n                        == \"Mini Series: Custom Image with annotations\"\n                    ][\"youtube_ctr\"].mean(),\n                    2,\n                )\n            )\n        ),\n        font=dict(family=\"Rockwell\", size=15, color=\"black\"),\n        showarrow=False,\n        borderpad=2,\n    )\n)\n\n# youtube default\nannotations.append(\n    dict(\n        xref=\"x\",\n        yref=\"y\",\n        x=8,\n        y=\"YouTube default image\",\n        xanchor=\"right\",\n        yanchor=\"top\",\n        text=(\n            \"Mean CTR: \"\n            + str(\n                round(\n                    epi_y_thumb[epi_y_thumb[\"description\"] == \"YouTube default image\"][\n                        \"youtube_ctr\"\n                    ].mean(),\n                    2,\n                )\n            )\n        ),\n        font=dict(family=\"Rockwell\", size=15, color=\"black\"),\n        showarrow=False,\n    )\n)\n\n# youtube deafult with custom annotation\nannotations.append(\n    dict(\n        xref=\"x\",\n        yref=\"y\",\n        x=8,\n        y=\"YouTube default image with custom annotation\",\n        xanchor=\"right\",\n        yanchor=\"top\",\n        text=(\n            \"Mean CTR: \"\n            + str(\n                round(\n                    epi_y_thumb[\n                        epi_y_thumb[\"description\"]\n                        == \"YouTube default image with custom annotation\"\n                    ][\"youtube_ctr\"].mean(),\n                    2,\n                )\n            )\n        ),\n        font=dict(family=\"Rockwell\", size=15, color=\"black\"),\n        showarrow=False,\n    )\n)\n\n# enhance template\nfig.update_layout(\n    annotations=annotations,\n    template=\"ggplot2\",\n    title={\n        \"text\": \"The Effect of the Thumbnail type on CTR\",\n        \"font\": {\"family\": \"Rockwell\", \"size\": 20},\n        \"yanchor\": \"top\",\n    },\n    xaxis_title=\"YouTube CTR\",\n    yaxis_title=\"\",\n)\n\n# show plot\nfig.show()","620d2b7a":"# feature engineering average watch percentage\n\nepi['youtube_avg_watch_percentage'] = epi['youtube_avg_watch_duration'] \/ epi['episode_duration']","11fd2aae":"# youtube_avg_watch_percentage across episodes\n\nacross_epi_plot(epi, 'youtube_avg_watch_percentage', title='Average watch percentage across episodes')","925b294f":"# corr with youtube_ctr\n\nplot_corr(epi, 'youtube_ctr', 'Correlations with CTR(New feature added)')","fc418355":"# youtube_watch_hours across episodes\n\nacross_epi_plot(epi, 'youtube_watch_hours', title='YouTube watch hours across episodes')","4645d797":"# ignore episode 27\n\nepi_sans_27 = epi[~(epi['episode_id']=='E27')]","6fe465bc":"# Category-wise comparison of youtube watch hours\n\n# create the dataset\ncat_wise = epi_sans_27.groupby([\"category\"], as_index=False).agg(\n    {\"youtube_watch_hours\": \"mean\"}\n)\ncat_wise.columns = [\"category\", \"youtube_watch_hours_avg\"]\n\n# plot the bar chart\nfig = px.bar(cat_wise, x=\"category\", y=\"youtube_watch_hours_avg\", color=\"category\")\n\n# enhance template\nfig.update_layout(\n    template=\"ggplot2\",\n    title={\n        \"text\": \"Avg. watch hours per category\",\n        \"font\": {\"family\": \"Rockwell\", \"size\": 20},\n        \"xanchor\": \"center\",\n        \"yanchor\": \"top\",\n    },\n)\n\n# show plot\nfig.show()","c0e18a69":"# corr with youtube_watch_hours\n\nplot_corr(epi_sans_27, 'youtube_watch_hours', 'Correlations with Watch Hours')","4e0e7816":"# Average watch percent vs watch hours\n\n# plot the scatter plot\nfig = px.scatter(\n    epi_sans_27,\n    x=\"youtube_watch_hours\",\n    y=\"youtube_avg_watch_percentage\",\n    color=\"category\",\n    hover_data=[\"episode_id\"],\n    opacity=0.90,\n)\n\n# enhance markers\nfig.update_traces(\n    marker=dict(size=10, line=dict(width=2, color=\"DarkSlateGrey\")),\n    selector=dict(mode=\"markers\"),\n)\n\n# add annotations\nannotations = []\nannotations.append(\n    dict(\n        xref=\"paper\",\n        yref=\"paper\",\n        x=0.9,\n        y=-0.1,\n        xanchor=\"center\",\n        yanchor=\"top\",\n        text=\"*Each data point represents \" + \"an episode\",\n        font=dict(family=\"Rockwell\", size=12, color=\"grey\"),\n        showarrow=False,\n    )\n)\n\n# enhance template\nfig.update_layout(\n    annotations=annotations,\n    template=\"ggplot2\",\n    title={\n        \"text\": \"Average Watch Percentage vs Watch Hours\",\n        \"font\": {\"family\": \"Rockwell\", \"size\": 20},\n        \"xanchor\": \"center\",\n        \"yanchor\": \"top\",\n    },\n    xaxis_title=\"Total watch hours\",\n    yaxis_title=\"Average watch percentage\",\n)\n\n# show plot\nfig.show()","63bee383":"# youtube_subscribers across episodes\n\nacross_epi_plot(epi, 'youtube_subscribers', title='New subscribers across episodes')","d1bdf720":"# feature engineer the transcript features\n\n\ndef conv_to_sec(x):\n    \"\"\" Time to seconds \"\"\"\n\n    t_list = x.split(\":\")\n    if len(t_list) == 2:\n        m = t_list[0]\n        s = t_list[1]\n        time = int(m) * 60 + int(s)\n    else:\n        h = t_list[0]\n        m = t_list[1]\n        s = t_list[2]\n        time = int(h) * 60 * 60 + int(m) * 60 + int(s)\n    return time\n\n\ndef get_durations(nums, size):\n    \"\"\" Get durations i.e the time for which each speaker spoke continuously \"\"\"\n\n    diffs = []\n    for i in range(size - 1):\n        diffs.append(nums[i + 1] - nums[i])\n    diffs.append(30)  # standard value for all end of the episode CFA by Sanyam\n    return diffs\n\n\ndef transform_transcript(sub, episode_id):\n    \"\"\" Transform the transcript of the given episode \"\"\"\n\n    # create the time second feature that converts the time into the unified qty. of seconds\n    sub[\"Time_sec\"] = sub[\"Time\"].apply(conv_to_sec)\n\n    # get durations\n    sub[\"Duration\"] = get_durations(sub[\"Time_sec\"], sub.shape[0])\n\n    # providing an identity to each transcript\n    sub[\"Episode_ID\"] = episode_id\n    sub = sub[[\"Episode_ID\", \"Time\", \"Time_sec\", \"Duration\", \"Speaker\", \"Text\"]]\n\n    return sub\n\n\ndef combine_transcripts(sub_dir):\n    \"\"\" Combine all the 75 transcripts of the ML Heroes Interviews together as one dataframe \"\"\"\n\n    episodes = []\n    for i in range(1, 76):\n        file = \"E\" + str(i) + \".csv\"\n        try:\n            sub_epi = pd.read_csv(os.path.join(sub_dir, file))\n            sub_epi = transform_transcript(sub_epi, (\"E\" + str(i)))\n            episodes.append(sub_epi)\n        except:\n            continue\n    return pd.concat(episodes, ignore_index=True)\n\n\n# create the combined transcript dataset\nsub_dir = \"..\/input\/chai-time-data-science\/Cleaned Subtitles\"\ntranscripts = combine_transcripts(sub_dir)\n\n# display first 10 instances\ntranscripts.head(10)\n","23b17895":"# generating some unusual features - Speech Speed Patterns\n\n\ndef host_speaking_speed(epi):\n    \"\"\" Return the speaking speed of the host in words per minute \"\"\"\n\n    speeds = []\n\n    df = transcripts[transcripts[\"Episode_ID\"] == epi]\n    df = df[df[\"Speaker\"] == \"Sanyam Bhutani\"].reset_index()\n    df = df[\n        1 : (df.shape[0] - 1)\n    ]  # the first and last dialogues are standard for all episodes\n\n    for i in range(df.shape[0]):\n        if df[\"Duration\"].iloc[i] != 0:\n            speeds.append(len(df[\"Text\"].iloc[i].split()) \/ df[\"Duration\"].iloc[i])\n        else:\n            continue\n    return round(np.mean(speeds), 2)\n\n\ndef host_avg_speaking_speed():\n    \"\"\" Returns a dataframe of Sanyam's average speaking speed across episodes \"\"\"\n\n    avg_speeds = []\n    for epi in list(transcripts[\"Episode_ID\"].unique()):\n        avg_speeds.append(host_speaking_speed(epi))\n    return pd.DataFrame(\n        {\n            \"episode_id\": list(transcripts[\"Episode_ID\"].unique()),\n            \"host_avg_spk_speed\": avg_speeds,\n        }\n    )\n\n\ndef guest_speaking_speed(epi):\n    \"\"\" Return the speaking speed of the host in words per minute \"\"\"\n\n    speeds = []\n\n    df = transcripts[transcripts[\"Episode_ID\"] == epi]\n    df = df[df[\"Speaker\"] != \"Sanyam Bhutani\"].reset_index()\n\n    # episode 69 was the host AMA session. So, there was no guest here => np.nan() will be added in place of guest speed here\n    for i in range(df.shape[0]):\n        if df[\"Duration\"].iloc[i] != 0:\n            speeds.append(len(df[\"Text\"].iloc[i].split()) \/ df[\"Duration\"].iloc[i])\n        else:\n            continue\n    return round(np.mean(speeds), 2)\n\n\ndef guest_avg_speaking_speed():\n    \"\"\" Returns a dataframe of Sanyam's average speaking speed across episodes \"\"\"\n\n    avg_speeds = []\n    for epi in list(transcripts[\"Episode_ID\"].unique()):\n        avg_speeds.append(guest_speaking_speed(epi))\n    return pd.DataFrame(\n        {\n            \"episode_id\": list(transcripts[\"Episode_ID\"].unique()),\n            \"guest_avg_spk_speed\": avg_speeds,\n        }\n    )\n\n\n# create the dataset\nspk_speed = pd.merge(\n    host_avg_speaking_speed(), guest_avg_speaking_speed(), on=\"episode_id\"\n)\n\n# feature engineer \"both_avg\"\nspk_speed[\"both_avg\"] = round(\n    (spk_speed[\"host_avg_spk_speed\"] + spk_speed[\"guest_avg_spk_speed\"]) \/ 2, 2\n)\n\n# feature engineer \"diff\"\nspk_speed[\"diff\"] = round(\n    (spk_speed[\"host_avg_spk_speed\"] - spk_speed[\"guest_avg_spk_speed\"]), 2\n)\n\n# display first 5 instances\nspk_speed.head(5)\n","895aa5c0":"# vocal speed comparison\n\ncomp_bin_plot(\n    spk_speed,\n    \"host_avg_spk_speed\",\n    \"guest_avg_spk_speed\",\n    \"Host\",\n    \"Guest\",\n    title=\"Comparing vocal speeds\",\n    xtitle=\"Episodes\",\n    ytitle=\"Words per second(WPS)\",\n)\n","28e9d474":"# merge spisodes and spk_speed\n\nepi_spk_speed = pd.merge(spk_speed, epi, on='episode_id')","77b8327e":"# corr with youtube_ctr\n\nplot_corr(epi_spk_speed, 'youtube_subscribers', 'Correlations with Subscribers')","8485538e":"# Are comparable rates of speaking more preferred by subscribers?\n\n# plot the scatter plot\nfig = px.scatter(\n    epi_spk_speed, x=\"diff\", y=\"youtube_subscribers\", hover_data=[\"episode_id\"]\n)\n\n# add vertical boundaries to separate into regions\nfig.add_shape(\n    # Line Vertical\n    type=\"line\",\n    x0=-0.5,\n    y0=-10,\n    x1=-0.5,\n    y1=160,\n    line=dict(color=\"black\", width=2, dash=\"solid\",),\n)\nfig.add_shape(\n    # Line Vertical\n    type=\"line\",\n    x0=0.5,\n    y0=-10,\n    x1=0.5,\n    y1=160,\n    line=dict(color=\"black\", width=2, dash=\"solid\",),\n)\n\n# enhance markers\nfig.update_traces(\n    marker=dict(size=10, line=dict(width=2, color=\"DarkSlateGrey\"), color=\"#e87d23\"),\n    selector=dict(mode=\"markers\"),\n)\n\n# add annotations\nannotations = []\nannotations.append(\n    dict(\n        xref=\"paper\",\n        yref=\"paper\",\n        x=0.9,\n        y=-0.1,\n        xanchor=\"center\",\n        yanchor=\"top\",\n        text=\"*Each data point represents \" + \"an episode\",\n        font=dict(family=\"Rockwell\", size=12, color=\"grey\"),\n        showarrow=False,\n    )\n)\n\nannotations.append(\n    dict(\n        xref=\"x\",\n        yref=\"y\",\n        x=-1.2,\n        y=140,\n        xanchor=\"left\",\n        yanchor=\"top\",\n        text=(\"I\"),\n        font=dict(family=\"Rockwell\", size=15, color=\"black\"),\n        showarrow=False,\n        borderpad=4,\n    )\n)\n\nannotations.append(\n    dict(\n        xref=\"x\",\n        yref=\"y\",\n        x=-0.2,\n        y=140,\n        xanchor=\"right\",\n        yanchor=\"top\",\n        text=(\"II\"),\n        font=dict(family=\"Rockwell\", size=15, color=\"black\"),\n        showarrow=False,\n        borderpad=4,\n    )\n)\n\nannotations.append(\n    dict(\n        xref=\"x\",\n        yref=\"y\",\n        x=0.8,\n        y=140,\n        xanchor=\"right\",\n        yanchor=\"top\",\n        text=(\"III\"),\n        font=dict(family=\"Rockwell\", size=15, color=\"black\"),\n        showarrow=False,\n        borderpad=4,\n    )\n)\n\n# enhance template\nfig.update_layout(\n    annotations=annotations,\n    template=\"ggplot2\",\n    title={\n        \"text\": \"Are comparable rates of speaking more preferred by subscribers?\",\n        \"font\": {\"family\": \"Rockwell\", \"size\": 20},\n        \"xanchor\": \"center\",\n        \"yanchor\": \"top\",\n    },\n    xaxis_title=\"Difference in WPS between host and guest\",\n    yaxis_title=\"Number of new subscribers\",\n    hoverlabel=dict(bgcolor=\"white\", font_size=12, font_family=\"Rockwell\"),\n)\n\n# show plot\nfig.show()","a50f7654":"# This cell has been completely borrowed from Parul Pandey's starter kernel; hope that is okay!\n\n\ndef clean_text(text):\n    \"\"\"\n    Preliminary cleaning operations\n    \"\"\"\n\n    text = text.lower()\n    # text = re.sub('\\[.*?\\]', '', text)\n    text = re.sub(\"https?:\/\/\\S+|www\\.\\S+\", \"\", text)\n    # text = re.sub('<.*?>+', '', text)\n    # text = re.sub('[%s]' % re.escape(string.punctuation), '', text)\n    text = re.sub(\"\\n\", \"\", text)\n    text = re.sub(\"\\w*\\d\\w*\", \"\", text)\n    return text\n\n\ndef text_preprocessing(text):\n    \"\"\"\n    Cleaning and parsing the text.\n    \"\"\"\n\n    tokenizer = nltk.tokenize.RegexpTokenizer(r\"\\w+\")\n    nopunc = clean_text(text)\n    tokenized_text = tokenizer.tokenize(nopunc)\n    remove_stopwords = [\n        w for w in tokenized_text if w not in stopwords.words(\"english\")\n    ]\n    combined_text = \" \".join(remove_stopwords)\n    return combined_text\n\n\ndef remove_stopwords(sen):\n    \"\"\"\n    Remove stopwords\n    \"\"\"\n\n    sen_new = \" \".join([i for i in sen if i not in stopwords.words(\"english\")])\n    return sen_new\n","4c542614":"# Make a dataset aggregating the content for each episode (only the guest)\n\n\ndef aggregate_trans(epi):\n    \"\"\" Aggregate Transcripts for a whole episode \"\"\"\n\n    trans_epi = transcripts[transcripts[\"Episode_ID\"] == epi]\n    trans_epi = trans_epi[trans_epi[\"Speaker\"] != \"Sanyam Bhutani\"]\n    return \" \".join(trans_epi[\"Text\"])\n\n\n# initialise empty data structures\ntrans_dict = {}\neid = []\ncontent = []\n\n# episode id\nfor ep in list(transcripts[\"Episode_ID\"].unique()):\n    eid.append(ep)\ntrans_dict[\"episode_id\"] = eid\n\n# aggregate content per episode\nfor ep in list(transcripts[\"Episode_ID\"].unique()):\n    content.append(aggregate_trans(ep))\ntrans_dict[\"content\"] = content\n\n# create the dataframe\nepi_content = pd.DataFrame(trans_dict)\n\n# cleaning up the transcripts (for summarization)\nepi_content[\"cleaned\"] = epi_content[\"content\"].apply(clean_text)\nepi_content[\"preprocessed\"] = epi_content[\"cleaned\"].apply(text_preprocessing)\n\n# display first 10 instances\nepi_content.head(10)","267bf56c":"# binning number of subscribers as well as number of likes\n\n\ndef make_sub_cat(x):\n    \"\"\"\n    Bin into categories based on subscribers\n    -----\n    \n    > Based on the 25th, 50th and 75th quantiles\n    \"\"\"\n\n    if x <= 2:\n        return \"very low\"\n    elif (x > 2) and (x <= 6):\n        return \"low\"\n    elif (x > 6) and (x <= 16):\n        return \"medium\"\n    return \"high\"\n\n\ndef make_like_cat(x):\n    \"\"\"\n    Bin into categories based on likes\n    -----\n    \n    > Based on the 25th, 50th and 75th quantiles\n    \"\"\"\n\n    if x <= 4:\n        return \"very low\"\n    elif (x > 4) and (x <= 13):\n        return \"low\"\n    elif (x > 13) and (x <= 24):\n        return \"medium\"\n    return \"high\"\n\n\nepi[\"sub_cat\"] = epi[\"youtube_subscribers\"].apply(make_sub_cat)\nepi[\"like_cat\"] = epi[\"youtube_likes\"].apply(make_like_cat)","d7ba94e0":"# make data for subscribers\n\nsub_data = pd.merge(epi_content, epi[['episode_id', 'sub_cat']], on='episode_id')\nsub_data = sub_data[sub_data['episode_id']!='E69'] # episode 69 was just the AMA\n\nprint(\"Dataset used after binning\")\nsub_data.head()","338bb63f":"# calculate average sentence length\n\n\ndef get_avg_sent_length(text):\n    \"\"\"\n    Returns the average sentence length for the given guest\n    \"\"\"\n\n    sentences = sent_tokenize(text)\n    avg_len = sum(len(sentence.split()) for sentence in sentences) \/ len(sentences)\n\n    return avg_len\n\n\n# feature engineer \"avg_sent_length\"\nsub_data[\"avg_sent_length\"] = sub_data[\"content\"].apply(get_avg_sent_length)\n\n# set the seaborn plotting style\nsns.set_style(\"darkgrid\")\nsns.set(font_scale=1)\n\n# set figure size\nplt.figure(figsize=(10, 6))\n\n# plot violin plot\nsns.violinplot(\n    y=\"avg_sent_length\",\n    data=sub_data,\n    x=\"sub_cat\",\n    order=[\"very low\", \"low\", \"medium\", \"high\"],\n    palette=\"Reds\",\n)\n\n# plot swarm plot\nsns.swarmplot(\n    y=\"avg_sent_length\",\n    data=sub_data,\n    x=\"sub_cat\",\n    order=[\"very low\", \"low\", \"medium\", \"high\"],\n    color=\"black\",\n)\n\n# re-assign axes labels\nplt.xlabel(\"\\nCategory of Episodes w.r.t Subscribers\", fontsize=12)\nplt.ylabel(\"Average Length of a Sentence by the Guest\\n\", fontsize=12)\n\n# add title\nplt.title(\n    \"Does the community prefer to subscribe when guests speak in shorter sentences?\\n\",\n    fontsize=15,\n)\n\n# show plot\nplt.show()\n","d154eae2":"# generating custom word clouds per category\n\n\ndef cloudy_words(episode, title=\"You forgot to name it\"):\n    \"\"\"\n    Generate word clouds based on the guest's speech in the episode\n    -----\n    \n    Can help identify a guest's main driving points behind his\/her journey and\n    also provide an insight into the key ideas they stress on for the listeners\n    \"\"\"\n\n    temp = epi_content[epi_content[\"episode_id\"] == episode]\n    word_cloud = WordCloud(\n        width=1600,\n        height=800,\n        colormap=\"YlOrBr\",\n        margin=5,\n        stopwords={\n            \"thing\",\n            \"really\",\n            \"people\",\n            \"going\",\n            \"actually\",\n            \"definitely\",\n            \"something\",\n            \"think\",\n            \"little\",\n            \"basically\",\n            \"could\",\n            \"should\",\n            \"maybe\",\n            \"would\",\n            \"still\",\n            \"guess\",\n            \"exactly\",\n            \"right\",\n            \"different\",\n            \"example\",\n            \"always\",\n            \"everything\",\n        },\n        max_words=200,  \n        min_word_length=5,  \n        max_font_size=150,\n        min_font_size=20,  \n        background_color=\"#E87D23\",\n    ).generate(\" \".join(temp[\"preprocessed\"]))\n\n    plt.figure(figsize=(10, 16))\n    plt.imshow(word_cloud, interpolation=\"gaussian\")\n    plt.title(title)\n    plt.axis(\"off\")\n    plt.show()\n\n\ndef cloudy_words_for_group(df, title=\"You forgot to name it\"):\n    \"\"\"\n    Generate word clouds based on the guest's speech in a group of episodes\n    -----\n    \n    A good way to use this would be to identify if a common group of speakers(researchers\/kagglers)\n    focus on specific points\n    \"\"\"\n\n    word_cloud = WordCloud(\n        width=1000,\n        height=500,\n        colormap=\"YlOrBr\",\n        margin=5,\n        stopwords={\n            \"thing\",\n            \"really\",\n            \"people\",\n            \"going\",\n            \"actually\",\n            \"definitely\",\n            \"something\",\n            \"think\",\n            \"little\",\n            \"basically\",\n            \"could\",\n            \"should\",\n            \"maybe\",\n            \"would\",\n            \"still\",\n            \"guess\",\n            \"exactly\",\n            \"right\",\n            \"different\",\n            \"example\",\n            \"always\",\n            \"everything\",\n            \"model\",\n            \"stuff\",\n            \"thank\",\n            \"you\",\n            \"like\",\n            \"lot\",\n            \"kind\",\n            \"know\",\n            \"yeah\",\n        },\n        max_words=200,  \n        min_word_length=5,  \n        max_font_size=150,\n        min_font_size=20, \n        background_color=\"black\",\n    ).generate(\" \".join(df[\"preprocessed\"]))\n\n    plt.figure(figsize=(10, 16))\n    plt.imshow(word_cloud, interpolation=\"gaussian\")\n    title = (\n        \"Main words spoken by the guest in episodes with \"\n        + title\n        + \" new subscribers\\n\"\n    )\n    plt.title(title, fontsize=15)\n    plt.axis(\"off\")\n    plt.show()\n\n\ndef generate_clouds():\n    \"\"\"\n    Generate word clouds for each of the episodes based on subscriber-based grouping\n    \"\"\"\n\n    cloudy_words_for_group(\n        sub_data[sub_data[\"sub_cat\"] == \"very low\"], title=\"very low\"\n    )\n    cloudy_words_for_group(sub_data[sub_data[\"sub_cat\"] == \"low\"], title=\"low\")\n    cloudy_words_for_group(sub_data[sub_data[\"sub_cat\"] == \"medium\"], title=\"medium\")\n    cloudy_words_for_group(sub_data[sub_data[\"sub_cat\"] == \"high\"], title=\"high\")","e5ebafd6":"# generate the wordclouds\n\ngenerate_clouds()","581735b4":"# creating a feature to support plotting with hue\n\nmost_loved = epi[['episode_id', 'episode_name', 'youtube_subscribers', 'youtube_likes']]\nmost_loved['hue'] = 'normal'\nmost_loved.iloc[1,4] = \"outlier\"\nmost_loved.iloc[27,4] = \"outlier\"\nmost_loved.iloc[58,4] = \"outlier\"","b2c56acf":"# Likes vs Subscribers scatter\n\n# plot the scatter plot\nfig = px.scatter(\n    most_loved,\n    x=\"youtube_subscribers\",\n    y=\"youtube_likes\",\n    opacity=0.80,\n    color=\"hue\",\n    hover_data={\n        \"episode_id\": True,\n        \"episode_name\": True,\n        \"youtube_likes\": True,\n        \"youtube_subscribers\": True,\n        \"hue\": False,\n    },\n)\n\n# enhance markers\nfig.update_traces(\n    marker=dict(size=10, line=dict(width=2, color=\"DarkSlateGrey\"),),\n    selector=dict(mode=\"markers\"),\n)\n\n# add annotations\nannotations = []\nannotations.append(\n    dict(\n        xref=\"paper\",\n        yref=\"paper\",\n        x=0.9,\n        y=-0.1,\n        xanchor=\"center\",\n        yanchor=\"top\",\n        text=\"*Each data point represents \" + \"an episode\",\n        font=dict(family=\"Rockwell\", size=12, color=\"grey\"),\n        showarrow=False,\n    )\n)\n\n# enhance template\nfig.update_layout(\n    annotations=annotations,\n    template=\"ggplot2\",\n    title={\n        \"text\": \"Outliers - The Most \ud83d\udc97 Episodes of CTDS\",\n        \"font\": {\"family\": \"Rockwell\", \"size\": 20},\n        \"xanchor\": \"center\",\n        \"yanchor\": \"top\",\n    },\n    xaxis_title=\"Number of Subscribers\",\n    yaxis_title=\"Number of Likes\",\n    hoverlabel=dict(bgcolor=\"white\", font_size=12, font_family=\"Rockwell\"),\n    showlegend=False,\n)\n\n# show plot\nfig.show()","f3f3b0dd":"# Download GloVe embeddings\n!wget http:\/\/nlp.stanford.edu\/data\/glove.6B.zip\n!unzip glove*.zip\n\n# Extract word vectors from GloVe embeddings\nword_embeddings = {}\nf = open('glove.6B.100d.txt', encoding='utf-8')\nfor line in f:\n    values = line.split()\n    word = values[0]\n    coefs = np.asarray(values[1:], dtype='float32')\n    word_embeddings[word] = coefs\nf.close()","79ce5fb0":"# Code to get summaries\n\n\ndef prep_sim_matrix(sentences, sentence_vectors):\n    \"\"\"\n    Prepares a cosine similarity matrix\n    \"\"\"\n\n    sim_mat = np.zeros([len(sentences), len(sentences)])\n    for i in range(len(sentences)):\n        for j in range(len(sentences)):\n            if i != j:\n                sim_mat[i][j] = cosine_similarity(\n                    sentence_vectors[i].reshape(1, 100),\n                    sentence_vectors[j].reshape(1, 100),\n                )[0, 0]\n    return sim_mat\n\n\ndef vectorize(sentences):\n    \"\"\"\n    Convert text data into numbers\/vectors\n    \"\"\"\n\n    sentence_vectors = []  # initialise as empty\n    for i in sentences:\n        if len(i) != 0:\n            v = sum([word_embeddings.get(w, np.zeros((100,))) for w in i.split()]) \/ (\n                len(i.split()) + 0.001\n            )\n        else:\n            v = np.zeros((100,))\n        sentence_vectors.append(v)\n    return sentence_vectors\n\n\ndef generate_sentences(episode):\n    \"\"\"\n    Generate sentences for a given episode\n    \"\"\"\n\n    temp = epi_content[epi_content[\"episode_id\"] == episode]\n\n    sentences = []\n    for s in temp[\"cleaned\"]:\n        sentences.append(sent_tokenize(s))\n    sentences = [y for x in sentences for y in x]\n\n    return sentences\n\n\ndef get_summary(episode):\n    \"\"\"\n    Get an N sentence (by default, N is 20) summary for a given episode\n    \"\"\"\n\n    n_sent = 20  # tweak this paramater if you want more sentences in the summary\n\n    # break the transcript to sentences\n    sentences = generate_sentences(episode)\n    \n    # vectorize\n    sent_vectors = vectorize(sentences)\n    \n    # cosine similarity matrix\n    sim_mat = prep_sim_matrix(sentences, sent_vectors)\n\n    # Summarization\n    nx_graph = nx.from_numpy_array(sim_mat)\n    scores = nx.pagerank(nx_graph)\n    ranked_sentences = sorted(\n        ((scores[i], s) for i, s in enumerate(sentences)), reverse=True\n    )\n    \n    # display it\n    print(\"SUMMARY FOR \" + episode + \"=>\")\n    for i in range(n_sent):\n        print(ranked_sentences[i][1])\n        print(\"xxxxxxxxxx\")\n    print(\"--------------------\")","ce80109f":"# episode 1 summary\n\nget_summary('E1')","6242da97":"# episode 27 summary\n\nget_summary('E27')","079da3cb":"# episode 49 summary\n\nget_summary('E49')","6dba148a":"HTML('<div class=\"canva-embed\" data-design-id=\"DAEBxI13Mww\" data-height-ratio=\"0.5625\" style=\"padding:56.2500% 5px 5px 5px;background:rgba(0,0,0,0.03);border-radius:8px;\"><\/div><script async src=\"https:&#x2F;&#x2F;sdk.canva.com&#x2F;v1&#x2F;embed.js\"><\/script><a href=\"https:&#x2F;&#x2F;www.canva.com&#x2F;design&#x2F;DAEBxI13Mww&#x2F;view?utm_content=DAEBxI13Mww&amp;utm_campaign=designshare&amp;utm_medium=embeds&amp;utm_source=link\" target=\"_blank\" rel=\"noopener\">A Thematic Analysis of the Most Loved Episodes of CTDS<\/a> by Ramshankar Yadhunath')","ec31ef76":"# the host's wordcloud\n\n# subset the dataset to include only the host's part\nepi_host = transcripts[transcripts[\"Speaker\"] == \"Sanyam Bhutani\"]\n\n# create the wordcloud\nword_cloud = WordCloud(\n    width=1600,\n    height=800,\n    colormap=\"YlOrBr\",\n    margin=5,\n    max_words=100,  # Maximum numbers of words we want to see\n    min_word_length=4,  # Minimum numbers of letters of each word to be part of the cloud\n    max_font_size=150,\n    min_font_size=20,  # Font size range\n    background_color=\"black\",\n).generate(\" \".join(epi_host[\"Text\"]))\n\n# set the figure size\nplt.figure(figsize=(10, 16))\n\n# set the title\nplt.title(\"The host's most used 100 words\", fontsize=20)\n\n# display the plot\nplt.imshow(word_cloud, interpolation=\"gaussian\")\nplt.axis(\"off\")\nplt.show()","f6c57d6c":"# creating a new dataset to capture trends in Q and A\n\n\ndef count_questions(text):\n    \"\"\"\n    Returns the number of question marks in the 'text'\n    \"\"\"\n\n    return text.count(\"?\")\n\n\ndef ques_ratio(df, n_ques):\n    \"\"\"\n    Returns the ratio of number of questions \/ number of interactions\n    by the host\n    -----\n    \n    > An interaction is counted when the host talks\n    > A question is counted everytime the host asks a question, irrespective\n      of whether it is the same question asked in a different form\n    > Can be greater than 1\n    \"\"\"\n\n    return n_ques \/ df.shape[0]\n\n\ndef times_like_used(text):\n    \"\"\"\n    Returns the number of times the host uses the word 'like'\n    \"\"\"\n\n    return text.count(\"like\")\n\n\ndef count_I(text):\n    \"\"\"\n    Returns the number of times the host talks in\n    first person\n    -----\n    > I think\n    > I'll\n    > I'm\n    \"\"\"\n\n    i_sp = text.count(\"I \")\n    i_ap = text.count(\"I'\")\n    return i_sp + i_ap\n\n\n# code to make the dataframe\nepisodes = []\nnum_questions = []\ninteractions = []\nques_ratios = []\nnum_like_used = []\nfp_used = []\n\nfor ep in list(transcripts[\"Episode_ID\"].unique()):\n\n    if ep == \"E69\":\n        continue\n    else:\n\n        epi_t = transcripts[transcripts[\"Episode_ID\"] == ep]\n        epi_host = epi_t[epi_t[\"Speaker\"] == \"Sanyam Bhutani\"]\n        text = \" \".join(epi_host[\"Text\"])\n\n        episodes.append(ep)\n        num_questions.append(count_questions(text))\n        interactions.append(epi_host.shape[0])\n        ques_ratios.append(round(ques_ratio(epi_host, count_questions(text)), 2))\n        num_like_used.append(times_like_used(text))\n        fp_used.append(count_I(text))\n        \n# make the dataframe\nhost_pre = pd.DataFrame(\n    {\n        \"episode_id\": episodes,\n        \"num_questions_by_host\": num_questions,\n        \"interactions\": interactions,\n        \"ques_ratio\": ques_ratios,\n        \"num_like_used\": num_like_used,\n        \"first_person_usage\": fp_used,\n    }\n)\n\n# display first 10 rows\nhost_pre.head()","b22332ae":"# num_questions_by_host across episodes\n\nacross_epi_plot2(\n    host_pre,\n    \"num_questions_by_host\",\n    ind=\"episode_id\",\n    title=\"#Questions asked across episodes\",\n)","4caac364":"# ques_ratio across episodes\n\nacross_epi_plot2(\n    host_pre, \"ques_ratio\", ind=\"episode_id\", title=\"Ques. ratio across episodes\"\n)","d8506dd3":"# num_like_used across episodes\n\nacross_epi_plot2(\n    host_pre,\n    \"num_like_used\",\n    ind=\"episode_id\",\n    title=\"#Like used by the host across episodes\",\n)","e18ea554":"# num_questions_by_host across episodes\n\nacross_epi_plot2(\n    host_pre,\n    \"first_person_usage\",\n    ind=\"episode_id\",\n    title=\"#Usage of first person by the host\",\n)","a3cea979":"## The trends of the host's presentation and Q&A - Affects the subscribers?\n\nThis section is somewhat of a \"bold\" attempt to analyse the host's presentation and Q&A patterns across the episodes. I will also try to identify any visible patterns in these that could affect the subscriber count per episode.\n\nBeneath, there is a word cloud that briefly summarizes the 100 most common words used by the host. \n> Only words with more than 4 characters are being considered.","35678f91":"# What makes a person subscribe after watching a video \u2753\n\nSubscribers are the unsung heroes of any YouTube channel. They are pivotal to the success of a channel because a subscriber spends almost 2x the time watching a channel's videos than a non-subsriber. Also, subscribers are the first to be notified when the channel publishes a new video. So, if there are 2 channels C1 and C2 with a subsriber count of 200 and 2000 => When a video is published by both channels, a lot more people know about it for C2 when compared to C1.  \n\n[Discovery of a channel is driven by watch time now in YouTube](https:\/\/creatoracademy.youtube.com\/page\/lesson\/subscriber-advantage#strategies-zippy-link-2).  \n> More subscribers => More watch time => Greater discovery\n\n<center><img src=\"https:\/\/github.com\/ry05\/ctds-analysis\/blob\/master\/graphics\/subscribe.png?raw=true\" width=\"900\" height=\"700\"><\/center>\n<center><i>Credits: Author<\/i><\/center>\n<br>\n\nSo, in this section let's have a look at the factors that make people subscribe to a CTDS episode.  \n\n**NOTE:**  \nWhile the previous sections were about a viewer clicking on the video and watching it, this section(as specified already) is based strongly on why someone will subscribe to the channel. This calls for an analysis on the wonderful transcript data provided by the CTDS team.\n\n\ud83d\udd0d **WHAT METRIC DEFINES SUCCESS HERE**\n\nThere is only one metric that defines success here:\n- youtube_subscribers : The number of new subsribers the channel has got from that episode\n\n\ud83d\udcda **WHAT DO WE KNOW OR BACKGROUND KNOWLEDGE**\n\n- A person subsribes when they find that the content they saw is worth their time and helpful to them.\n- Also, not everybody who liked a video subscribes. We need to factor in the human component of doubt and skepticism. Subscribers **do not indicate how much the video was liked**, it only indicates **how many viewers felt they needed more of this content**","af3bc8ff":"Since I am a beginner into the data science field, these 5 themes discussed in these episodes trigger a very familiar feeling in me. ***These are the concepts a beginner in data science really needs.*** A fancy course does just the part of teaching some code. But, to be a data scientist it takes more than just packages. It requires an understanding of the data and its domain, it requires learning from the community, it requires knowing the right conduct to engage in these data spaces and it also does help to know the practices that help real-world practitoners get their work done!  \n\nTherefore, it looks like the most loyal supporters of CTDS are beginners in the data science discipline and they absolutely do not hold back from showering their love onto the episodes that provide them with much needed tools to be better data people!","fb2670b9":"## How has the CTR been across all episodes?","a436d584":"<a id='ref'><\/a>\n<div class=\"h2\">References<\/div>\n\n1. [A bit about YouTube Analytics](https:\/\/blog.hubspot.com\/marketing\/youtube-analytics)\n2. [Parul Pandey's Guide Notebook](https:\/\/www.kaggle.com\/parulpandey\/how-to-explore-the-ctds-show-data)\n3. [Best Practices for Analytics Reporting](https:\/\/www.kaggle.com\/jpmiller\/some-best-practices-for-analytics-reporting)\n4. [A detailed explanation cum implementation of the TextRank algorithm](https:\/\/www.analyticsvidhya.com\/blog\/2018\/11\/introduction-text-summarization-textrank-python\/)\n5. [Natural Language Processing EDA](https:\/\/neptune.ai\/blog\/exploratory-data-analysis-natural-language-processing-tools)\n6. [Cole Nussbaumer Knaflic: \"Storytelling with Data\" | Talks at Google](https:\/\/www.youtube.com\/watch?v=8EMW7io4rSI&feature=youtu.be)\n7. [Meg Risdal's Utility Kernel](https:\/\/www.kaggle.com\/mrisdal\/anthony-in-a-kernel\/comments)\n8. [Rachael Tatman's Kernel on writing professional data science code](https:\/\/www.kaggle.com\/rtatman\/six-steps-to-more-professional-data-science-code)","9a7f7921":"\ud83d\udd0d **ABOUT THE NEW DATASET - EPI_CONTENT**\n\n- Contains the contents spoken by the guest\n- There are the raw versions, cleaned versions and also the tokenized versions\n- This dataset could be used to:\n    - Understand the kind of content that gets more subscriptions than others\n    - Generate summaries for episodes","82cc2ebf":"*Correlation on this seems like a dead-end :(*  ","701beac6":"As it is evident from the scatterplot above, 3 episodes lie far away from the rest of the episodes. These 3 episodes are represented in red and include episode E1(Abhishek Thakur), episode E27(Jeremy Howard) and episode E49(Parul Pandey). These episodes received the most likes and subscribers when compared to the other episodes on the show. *E27 is more or less in a different galaxy ;)*  \n\nJokes apart, it is now important to understand the content in these episodes. It will be more suitable to have a summary of these episodes' to review the key points of discussion.  \n\nThis summary is provided in the next section using the TextRank algorithm.","c3b5784a":"\ud83d\udca1 **INSIGHTS**  \n- The most an episode has been watched at a stretch on average by viewers was the inaugural announcement episode of CTDS at 52.2%\n    - This makes perfect sense as this inaugural episode was only a couple of minutes long!\n- On average, a CTDS episode was watched upto 13% of the episode per view\n- The least of this metric was 1.8% that came in episode 46. This probably could have been because the topic looked very specific to a particular group of practioners interested in ML in Classical Japanese Literature.\n    - However, the reason was quite different. On checking the youtube version of this episode, it became apparent to me that this episode on youtube was just an intro to the talk and the talk was available only as audio.\n    - This was probably the main reason why this episode(intro) had such a low average percentage watched value.\n- But, there is a surprise! The mini CTDS episodes(released on March 7) that had been amongst the bottom-most when compared using CTR, perform exceptionally well if we take the average percentage watched into consideration(w.r.t other episodes). \n    - This means that inspite of not having too many fans to click on these episodes, the ones who do click are very diligent and loyal to the fast.ai curriculum\n    - Also, hovering over these episodes will let you understand the kind of topics that had a high average percentage watched value\n        - M4 leads => Shows that most learners were super hooked onto the concepts of NLP and tabular data considered in this episode\n        - M7 is a surprise second => If you recall from the CTR graph, M7 was the lowest in terms of CTR. But here, it is second. This shows that \"Topics like GANs or the U-Net or ResNet are not popular choices, they are learnt and coveted by a specific audience\"\n    - So, the mini-CTDS episodes might not be **everybody's cup of chai**, but **those who like it, really love it!**","985ffd9d":"\ud83d\udca1 **INSIGHTS**  \n- There are no significant strong correlations between CTR and any other quantitiative youtube metric\n- The highest is the correlation between youtube_impression_views and youtube_ctr which is an *obvious case*","0015c3d8":"<a id='dk'><\/a>\n<div class=\"h2\">Domain Knowledge<\/div>\n\nHistorically speaking, podcasts date back to the 1980s when they were known by the term \"audio blogging\", but severly handicapped due to the lack of a means to distribute content <a href=\"https:\/\/brandastic.com\/blog\/why-are-podcasts-so-popular\/\">(Source)<\/a>. With the advent of the internet, things became a lot easier and now podcasts are rampant. They are an effective way to reach a wide audience, grow a community and even advertise products.\n<br>\n<br>\nInspite of the large amount of courses and certifications emerging for data science, I personally feel that the best learning comes through interactions with the community - people who are already in the field or people who are with you. Discussions and rounded perspectives lead to more comprehensive, wholesome ideas and will at the end help become more skilled in the given field. Chai Time Data Science offers this very opportunity where the guests on the show are often erudite practitioners with experience on their side. And most of their stories are both inspiring and highly useful to learning at the same time.\n<br>\n<br>\nHowever, growing the audience is important for CTDS. There are 2 primary reasons why I feel this should happen.\n<ul>\n    <li>To help broaden the community watching CTDS => The more people watch it, the more they will talk about it, the more it's popularity will spread and finally reach aspiring data scientists who could be benefitted from the show.<\/li>\n    <li>To create a new CTDS community => CTDS offers an in-depth perspective of the field, where guests range from almost every area. A community built along these lines would be an absolute haven for a beginner(like me) to get into the field.<\/li>\n<\/ul>","a2f5f539":"\ud83d\udca1 **INSIGHTS**  \n- Episode 27 has contributed the highest number of subscribers for the channel(139). This seems intuitive as episode 27 also had the largest share of watch hours amongst all episodes. So, more the subscribers, more the episode is distributed, more times it is discovered and probably, more times it is watched!\n- On an average, around 12 people subscribe for an episode. But, averages are misleading!\n    - 28 episodes have less than 5 subscribers out of which 10 have 0 subscribers\n    - Only 26 episodes(30.5%) have subscribers more than 12(the mean)\n    - So, there clearly is a distinction in how people subscribe to the CTDS channel - **Not all episodes are equal in the eyes of prospective subscribers**\n\nNow, with such high quality and specific content being featured on the show its hard to think that people *did not like the content*. Therefore, we could look at this from another angle : People subscribe to the channel after episode E if they think they will be helped by what they saw on E. Since CTDS interviews are discrete in nature, judging a future video's content by the present content does not make much sense - Ideally, if people like the format, they should be subscribing.  \n\nBut contrary to popular opinion, *we tend to always judge a book by its cover*. And if it is our first CTDS episode, this episode is the cover of the book called CTDS. So, if the content of that episode E is not appealing to *our interests*, we would not care to subscribe to the channel *hoping that future content will be what we want*. Hence, analysing the subscribers of a video does not talk about the quality of the content, but rather talks about the mindset and interest towards the content of the community watching it!     ","688837da":"## What determines the watch hours on an episode?\n\nGiven the visualization above, episode 27 looks like an outlier(a very pleasant one to have nevertheless), and hence I will be ignoring it for the analysis on watch hours on YouTube.","fdf2f63c":"In order to understand the patterns of the host's interactions with the guests, I am creating a new dataset here(yes, again).","35b5ea64":"\ud83d\udca1 **INSIGHTS**  \n- The above boxplot shows that the type of thumbnail chosen *might* have an effect on the CTR of a video\n- Looking at the means, 'Custom image with CTDS branding, Title and Tags' type thumbnail leads the pack with a mean of 3.12 which is significantly higher than the mean CTRs of other thumbnail types\n\n\u2753 **QUESTION- Is the 'Custom image with CTDS branding...' type thumbnail that revolutionary a change for CTDS?**\n- It *might or might not be*. \n- We know from our earlier visualization that all episodes with this thumbnail type were released only after 26th April 2020 and are therefore, the latest set of videos to be released. \n- Also, given how much CTDS has grown in popularity, this increase in CTR for these episodes might also be attributed to the channel being more popular now when compared to its initial stages\n- But, atleast the new branding added to the thumbnail has not had a negative impact ;) And inspite of my skepticism here, it is more promising to go with the theory that **Adding a custom image with CTDS branding, title and tags has lead to an overall better CTR for videos**","4ab1be92":"## How many subscribers did each episode contribute to the channel?","f0eb4834":"## CTR correlation with other YouTube metrics","ae26b4d6":"\ud83d\udca1 **INSIGHTS**  \n- The average watch hours for a CTDS episode is 50.84 hours. But, the average is skewed by the watch hours of episode 27 with Jeremy Howard. Leaving out this episode, the average watch time of a CTDS episode is at around 43 hours\n- The second most watched episode is episode 33 with Gilberto Titericz\n- We just saw above how the mini-CTDS episodes featuring tips on doing the fast.ai course has such a specific, diligent viewership. With that in place, it seems rather intuitive to understand why episode 27 had such a significantly large value for watch hours!\n    - Also, as we will know later, episode 27 also had the most number of subscribers. And increasing subscribers and watch hours positively impact each other.\n        - The more subscribers it gets and the more it is watched, the more it will be distributed and the more it will be watched again.\n    - This gargantuan, disproportionate affection towards episode 27 signifies the importance the fast.ai family has in the data science community.\n    - Going a little deeper, we can understand that fast.ai is *not our traditional course*. It's based on a top-down learning approach and encourages us to build and then learn why something works the way it does. So, **a reception to fast.ai on such a grand level echoes the growing support of the community towards learning by doing, rather than learning by theory.**\n    - Infact, we might even be looking at a paradigm shift here in the data science learning process amongst aspiring data scientists as well as industry practitioners who are looking to break into the field.","861dd185":"\ud83d\udd0d **ABOUT THE NEW DATASET - TRANSCRIPTS**\n- Our main dataset has 7649 rows; the distribution of rows is non-uniform across episodes\n- There is no transcript available for Episode 4. So, we have 74 episodes' transcripts in this dataframe\n- And episode 69 was Sanyam's AMA, his gift to the community on his b'day ;) (May 27th). So, the transcripts here only have his dialogues(monologue!)\n- I have introduced 3 main features as of now:\n    - Episode_ID => To understand which episode the dialogue belongs to\n    - Time_sec => Unify the time format (seconds)\n    - Duration => Duration of each continuous speech by a speaker before the other speaker speaks (seconds)\n- Also, I have added a 30 second duration at the end for the last call for action that the host makes at the end of each episode","40736f92":"<a id='mt'><\/a>\n<div class=\"h2\">Methodology<\/div>\n<br>\n\nThe methodology adopted by me for this analysis is fairly simple to understand and guided by the initative of <strong>data exploration<\/strong>. Hence, translating my finds into a narrative for the stakeholder(in this case, the host of the show) has been of utmost importance. It was also necessary to segregate the results that were discovered into those useful for the host and those that were not that useful. Useful insights are insights that can help the host understand a new facet of the show's reception amongst the broad data science community as well as help the host plan future actions aligned with the insights to widen the audience of the podcast on YouTube.\n<br>\n<br>\nThe diagrammatical representation of the methodology is represented in the figure beneath.\n\n<center><img src=\"https:\/\/github.com\/ry05\/ctds-analysis\/blob\/master\/graphics\/methodology.png?raw=true\" width=\"900\" height=\"700\"><\/center>\n<center><i>Credits: Author<\/i><\/center>\n<br>\n\n<strong>NOTE:<\/strong><br>\nSince each episode deals with a new interview with a new practitioner, special care has been taken to not involve any comparisons amongst the guests and their content on the basis of their social, ethnic, gender or professional distinctions. I am not an expert in the ethical considerations of data science, but personally I felt that such comparisons could lessen the importance the <strong>collective, connected<\/strong> community has in the data space. If by any chance I have made a harming comparison out of my naivety, I sincerely apologize to the concerned reader.","21dbc601":"## Does clicking on the video mean that a person will watch the video?\n\nThis is a very important insight to have. If an episode has a high CTR and a low average percentage of video watched, that means the viewers didn't find the content compelling(or maybe they just had a whole different set of expectations).  \n\nIn order to check this, I have engineered a new feature *youtube_avg_watch_percentage* which is the average percentage of an episode watched by people.\n\n> youtube_avg_watch_percentage = youtube_avg_watch_duration \/ episode_duration","0fc35c3d":"### Summary of Episode 27\n\nThis is a 20-line summary of the knowledge shared by Jeremy Howard in Episode E27 of CTDS.","ad511421":"\ud83d\udca1 **INSIGHTS**  \n- On an average, CTDS episodes have a CTR of 2.62\n- The highest CTR rate of 8.46 was achieved for episode 19, released on October 19 (wow, that rhymes)\n- 41 of the 85 videos(almost 50%) have CTRs above the average\n- Of all the episodes released as mini CTDS on 7th March, **only 2** of these have CTRs above the average CTR.\n\n\u2753 **QUESTION- How is the CTR for episode 19 at such a staggering high when compared to other episodes?**\n- The answer to this is a hypothesis, but is backed up by my understanding from the view of an aspiring data scientist ;)\n- The topic of the episode is about MOOCs and ML interviews\n    - Everybody is picking up a MOOC to learn data science currently!\n    - Also, the understanding of what an interview for a job in the data science and ML field will be like is a hot point for beginners to be interested in\n- Considering these 2 key factors, I believe these justify why episode 19 has such a high CTR when compared to other episodes!\n\n\u2753 **QUESTION- Was it a bad idea to release all the mini-CTDS episodes on the same day?**\n- The very first of this series M0 had the highest CTR of 3.67, followed by episode M1 with a CTR of 3.51\n- But the real drop is seen when the graph shows us that all the other episodes have CTR's much below the average CTR of a CTDS episode\n- M3 has the third highest CTR amongst the mini-series. It is a value of 2.13 which is significantly lower than M1's 3.51\n- Very surprisingly, M8 has a higher CTR than episodes M5, M6 and M7!\n    - M8 is the last episode and it talks about future steps after fast.ai\n    - Seems like a lot of the viewers were more interested in **knowing what next** before **doing it**\n    - The jumbled up order of the episodes which were released together indicates how the viewers were not very linear about watching these episodes\n- It seems a bit too early to pass a verdict. But as of now, it looks as though the mini-CTDS initiative didn't hit the sweet spot with the whole data community.\n- Had they been released separately, there would have been more spotlight for each episode on its own.\n\n**P.S : Read ahead for a surprise to this in a later section!**","2aeb915c":"> Episode 69 has been ignored because that episode was the AMA episode. So, the questions were not necessarily asked \"by the host\"","8835cec3":"### Summary of Episode 49\n\nThis is a 20-line summary of the knowledge shared by Parul Pandey in Episode E49 of CTDS.","637bfdd2":"## How has the format of the thumbnail changed over the episodes?","3380cff2":"\ud83d\udca1 **INSIGHTS**  \n- There is a negative correlation between CTR and average percentage of the episode viewed - So, even if a viewer clicks the thumbnail, there is no guarantee that he or she will watch the episode for long\n- A low percentage of video watched indicates that the viewers do not sit through the whole video. In fact, on average a CTDS episode is watched only upto 13% of its watch time by the average viewer...but that's not a completely negative sign\n- This is because there is a possibility that **the same viewer watches parts of an episode at different times**. Also, the average is not a very reliable metric - There maybe users who are absolutely not interested and just click on the thumbnail because they want to just check the video out. So, these clicks do not really mean anything.","bc5450c2":"With these new-found transcript features, we can introduce a new set of attributes - **Speaking speeds**. The number of words per each instance divided by the duration(seconds) to speak these by the speaker gives the speaking speed. The following dataset depicts the first 5 instances of this newly formed dataset.  \n\n> You could ignore the warning generated beneath","394e2e4b":"> After having discovered a few insights based on the average percentage viewed of an episode on average by an average viewer and after realising that it is not as high as we would like it to be, it is time to analyse, **\"What makes a viewer keep watching a video after they click its thumbnail\"** ?","14624c00":"\ud83d\udca1 **INSIGHTS**  \n- On average, the host tends to speak faster than his guests! This is quite normal considering how excited the host is with each interview(and as he has revealed on more than one occassion). Also, since the guests have a lot of thinking to do when answering the questions asked, there would be some time factored into this.\n- There is a gap for guest's speed on episode 69 as this was Sanyam's AMA session on his birthday(May 27)\n- Usually the host speaks faster, as is seen from the pink line being above the olive line on most occassions, but there are exceptions:\n    - The most notable exception would be episode 62(Pablo Samuel Castro, on ML Research), where the guest had a WPS measure of  greater than 1 word more than the host! Hmm, maybe Sanyam could let us know the story behind this ;)\n    - Other significant differences are in episode 37 (Anthony Goldbloom on Kaggle) and episode 57(Mark Landry on AutoML)","dc067c62":"\ud83d\udca1 **INSIGHTS**  \n- In the above graph, there are 3 regions:\n    - Region I contains the episodes where the host was speaking atleast 0.5 WPS(words per second) slower than the guest\n    - Region III contains the episodes where the host was speaking atleast 0.5 WPS(words per second) faster than the guest\n    - Region II contains all other episodes\n- Most points obviously lie in region II, but what is to be noted here is that the best performing episodes(i.e number of subscribers per episode) also lie in this region.\n- Region I has only 4 episodes and none of the episodes have received more than 20 subscribers.\n- On the other extreme region III has a lot more episodes, but here too only 2 episodes have had more than 20 subscribers\n- Region II has 12 episodes with more than 20 new subscribers and even if we do not consider the episode 27 which is an outlier, the figures are still pretty good!\n- Now, region II also has several episodes that do not perform as good as the others. Hence, we *cannot be definite that any episode in region II will do well in terms of generating new subscribers*. However, we must also acknowledge that this **could** also be a contributing factor to the larger number of subscribers.\n\n**But why is this argument to be taken with a pinch of salt?**  \nOne reason - The host never speaks as much as the guest. So, technically this must not make a difference to the viewer.","0134c8e3":"\ud83d\udca1 **INSIGHTS**  \n- The very first CTDS episode was on 21st July 2019 and used a \"Youtube default image with custom annotation\" thumbnail\n- Till 23rd April 2020, the thumbnails used shuffle between \"Youtube default image\" or \"Youtube default image with custom annotation\" in a ratio of over 5:1, respectively. \n- The only exception in this period is on 7th March 2020 when the host published 9 episodes. All of these were of the format \"Mini Series\"\n- The \"new\" CTDS format since 26th April 2020 has had custom image branding with the [CTDS logo](https:\/\/chaitimedatascience.com\/content\/images\/2020\/07\/ctds-1.png)\n\n\u2753 **QUESTION- Why was 7th March, 2020 an exception?**  \n\n- On this day, the host published 9 episodes\n- In the words of the host each of these episodes were meant to be mini-chai time data science episodes that acted as supplementary material for the [fast.ai](https:\/\/www.fast.ai\/) Part 1 course. \n- The host was joined by [Robert Bracco](https:\/\/twitter.com\/MadeUpMasters) where the main point of discussion was the advice provided by [Jeremy Howard](https:\/\/twitter.com\/jeremyphoward) to do the fast.ai lectures. The point of their discussions are summed up in [this thread](https:\/\/forums.fast.ai\/t\/things-jeremy-says-to-do\/36682).\n- All the 9 episodes were probably released together as they fit in together as a mini CTDS episode","06465394":"\ud83d\udca1 **INSIGHTS**  \n- YouTube watch hours have very strong positive correlations(>=0.8) with\n    - Views that result from sources other than YouTube or Non-impression views\n    - YouTube views\n    - Likes on an episode\n    - New subscribers that have joined the channel when they were watching this episode\n- YouTube watch hours have strong positive correlations(<0.8 and >0.5) with\n    - Views that result from YouTube or Impression views\n    - Comments on an episode (This can also be because more the community watches an episode, more there is a chance for comments)\n- YouTube watch hours has a negative correlation with the average watch percentage of an episode. This means that even if an episode E is watched more by the community in terms of the hours watched, it necessarily does \"not mean people are watching the episode in depth\".\n    - For example, if there are 100 viewers such that each of them watch an episode E for 6 mins each, we can calculate the watch hours to be 10 hours\n    - But, what if the duration of episode E is 1 hour? This means that the average percentage viewed of episode E is 10%\n    - This part has been discussed in the previous section","019ada2b":"## What content do the viewers most subscribe for?\n\nThe number of new subscribers per episode is a metric that is continuous in nature. In order to have an understanding of the content the community subscribes for, binning the number of subscribers per episode will be a good alternative. In order to bin, I use the 25th, 50th and 75th quantiles as checkpoints to bin all the episodes into one of 4 categories:\n- Very low subscribers [0-2]\n- Low subscribers (2,6]\n- Medium subscribers (6,16]\n- High subscribers (16,139]\n\n<center><img src=\"https:\/\/github.com\/ry05\/ctds-analysis\/blob\/master\/graphics\/subscriber-binning.png?raw=true\" width=\"900\" height=\"700\"><\/center>\n<center><i>Credits: Author<\/i><\/center>\n<br>","1785a54e":"\ud83d\udca1 **INSIGHTS**\n- The host asks about 30 questions on average per episode\n- The most questions asked by the host was in episode 35 featuring Rohan Rao\n- Episode 74 has registered no questions as per the transcript provided\n- A *question ratio* provides an estimate of how many questions the host asks per interaction in the interview. One interaction is one continuous unit for which the host speaks.\n    - If this ratio is 2, it means that for every 1 interaction the host asks 2 questions on an average in that episode\n    - If this ratio is more than 1, it could possible indicate a highly curious host!\n    - The average ratio was at 0.67 => On an average the host asked one question in every 2 interactions on the channel\n    - In episode 71 featuring Martin Henze, the first kernels GM, this ratio was at 1.32 => For every 3 interactions with Mr. Henze, the host asked about 4 questions\n- On a random read through the transcripts, it seemed as though the host had a habit of excessively using 'like' in his interactions. Now, 'like' is a very common filler word and a lot of us use it very often because it's a word that can be a part of any sentence! But, filler words are usually discouraged in common speech and it looks like *the host had put some real effort in bringing that to control*\n    - On an average, the host says 'like' 17-18 times per episode\n    - In episode 11, featuring Christine Payne the host used 'like' a staggering 125 times!\n    - From episode 16, the usage of 'like' has been mostly confined to below the mean figure, except slight exceptions\n    - However, in episode 49 with Parul Pandey, a big exception occured with the host registering 65 like-usages\n- The host is someone who talks so much out of his experience and that's what makes CTDS so engaging to watch(atleast for me). This is evident with the signifciant amount of first person usages he brings about at an average of 52 'I' words per episode.\n    - In episode 63, the host uses a whopping 510 'I' words! But, that's because this episode was a conversation\/call between the host and the guest, Robert Bracco.\n    \n  \n> There exist no direct correlations between these new features and the number of subscribers per episode.","5dfd52f4":"# What makes a person click a video's thumbnail \u2753\n\nAs soon as any visitor enters the Youtube Homepage, the first thing they see is a screen with thumbnails. There are several thumbnails, most of which relate to the kind of content the particular visitor has *watched in the past*. The first hurdle a CTDS video must cross over is **getting a user X to click on a video's thumbnail**. But, what makes a user want to click on the thumbnail of a video?\n\n<center><img src=\"https:\/\/github.com\/ry05\/ctds-analysis\/blob\/master\/graphics\/yt-homepage.png?raw=true\" width=\"900\" height=\"700\"><\/center>\n<center><i>Credits: Author<\/i><\/center>\n<br>\n\nFrom our experience on YouTube(if you don't have any, I recommend you try) we know that a visitor can see the following features of the thumbnail:\n- The type of it\n- The topic to be discussed, the ML hero and other info\n- The duration of the episode\n\nAt this juncture, let's hypothesize that these 3 features that are visible to a visitor play an important role in whether the visitor chooses to click a thumbnail or not. We will see if they hold good. \n\n\n\ud83d\udd0d **WHAT METRIC DEFINES SUCCESS HERE**\n\nIn order to evaluate the success rate of whether a visitor will click on a thumbnail or not, the click through rate(CTR) will be used. The CTR measures the ability of a video to prompt people to watch it after seeing it on their homepage, recommendation section or trending section. \n\n> CTR = Number of Clicks \/ Number of Impressions * 100\n\n\ud83d\udcda **WHAT DO WE KNOW OR BACKGROUND KNOWLEDGE**\n\n**A high CTR for a video indicates that the viewers saw something in its thumbnail that compelled them to click on it**\n\n\u2757 **NOTE**\n\nEvery user does have a different purpose. So, it's not that simple to generalize all episodes. However, the idea here is not to compare videos, but to identify if the click through rate is determined by *what the visitor sees* on the thumbnail.","d8b244bd":"## Generating 20-line summaries for the 3 most loved episodes\n\nIf we have quick summaries for the CTDS episodes, we could probably understand the content that makes viewers subscribe to the channel and the content that does not. A quick summary will also be easier for the end user to comprehend the contents of the episode before watching it, and then decide if it is something they would like to watch or not. \n\n> ***A short 20-line summary of an episode with hundreds of lines in the transcripts defnitely loses out on a lot of detail***. However, for the purpose of the analysis performed in this section, a 20-line summary was useful. It provides a good assortment of valuable content the guests in these 3 episodes had for the community. \n\n\n***NOTE:***  \nThis is where I learnt about something called [TextRank](https:\/\/web.eecs.umich.edu\/~mihalcea\/papers\/mihalcea.emnlp04.pdf), an extractive summary generation algorithm built on top of Google's PageRank. To understand its crux in under 3 minutes, I would suggest [this video](https:\/\/www.speechtechmag.com\/Articles\/ReadArticle.aspx?ArticleID=127336). I am using the TextRank algorithm to pick up 20-line summaries from an episode. Not witholding any credit where it is due, I have used most of the code to do this from [here](https:\/\/www.analyticsvidhya.com\/blog\/2018\/11\/introduction-text-summarization-textrank-python\/) and tweaked it at will, including modularizing it to suit my requirements.","b90375ac":"<center><h1>Amplifying the Impact of the CTDS Shows on YouTube<\/h1><\/center>\n<center><i>Dissecting the \"success\" of the podcast's YouTube episodes and towards formulating a strategy to reach a wider audience <\/i><\/center>\n<br>\n\n[Chai Time Data Science(CTDS)](https:\/\/chaitimedatascience.com\/), the no-filter podcast for data science, brought to the community by Sanyam Bhutani provides interviews with kagglers, industry practitioners and researchers. The podcast's content is widely distributed over multiple platforms(4 of which are provided in the given [ctds.show dataset](https:\/\/www.kaggle.com\/rohanrao\/chai-time-data-science)). Out of the around 10 different platforms the CTDS episodes are hosted on, YouTube is the only platform where CTDS is provided in it's video format.  \n\n<center><img src=\"https:\/\/chaitimedatascience.com\/content\/images\/2020\/07\/ctds-1.png\" width=\"700\" height=\"500\"><\/center>\n<center><i><a href=\"https:\/\/chaitimedatascience.com\/\">Credits<\/a><\/i><\/center>\n<br>\n\n<div class=\"h2\">Table of Contents<\/div>\n<ul>\n    <li><a href=\"#in\">Introduction<\/a><\/li>\n    <li><a href=\"#mpi\">Main Points of Interest<\/a><\/li>\n    <li><a href=\"#dk\">Domain Knowledge<\/a><\/li>\n    <li><a href=\"#mt\">Methodology<\/a><\/li>\n    <li><a href=\"#an\">The Analysis<\/a><\/li>\n    <li><a href=\"#ack\">Acknowledgement<\/a><\/li>\n    <li><a href=\"#ref\">References<\/a><\/li>\n<\/ul>","be5e6d77":"<a id='in'><\/a>\n<div class=\"h2\">Introduction<\/div>\nWhile most followers of the podcast identify with the audio platforms more, it is to be realised that podcasts are less engaging than video. <a href=\"https:\/\/makermag.com\/podcasts-vs-youtube\/\">Video captures the senses of both hearing and sight while a podcast only caters to the former<\/a>. Also if people are watching a video, we know for sure that more often than not they are paying attention to it. But, as far as podcasts are concerned, you could be watching them at any time. That is even when you are not mentally engaged with the content provided(for example, you could listen to them while doing a chore). This might cause a listener to miss out on key points discussed.  \n<br>\n<br>\nHence, it seems a little more beneficial to analyse the data of CTDS generated from YouTube to see if there are patterns that could possibly provide info on the community as a whole, and if possible provide data-backed suggestions to the host to expand the community watching CTDS on YouTube. ","82349f7f":"<a id='mpi'><\/a>\n<div class=\"h2\">Main Points of Interest<\/div>\n\nThis report is the result of my analysis into the constituents of a \"successful\" CTDS episode. Success is a subjective term and hence, it is defined differently depending upon the question under consideration. However, it is my perception that in general, a CTDS episode can be called successful if it is\n- Visited by a large number of users\n- Watched by these visitors i.e not just \"click and skip to next video\"\n- Able to contribute new subscribers to the CTDS channel\n\nI propose a couple of plans of action that the host could take to make CTDS reach a wider audience =>\n1. ***Have more mini-CTDS series, but do not release them on the same day =>*** Inspite of being clicked on by only very few people, these episodes were highly watched by those who clicked on them. Hence, mini-CTDS episodes could help build specific audience interested in the topic of the mini-series. However, releasing them on the same day takes the spotlight of all episodes. Viewers tend to see the first 1 or 2 and then abort. So, releasing them on separate days will help place more spotlight on each episode.\n2. ***Release more episodes that cater specifically to the budding data science community to generate more subscribers for the channel =>*** It was observed that the most well-recevied episodes on the show catered very specifically to data science aspirants or beginners. These episodes talked about the perspectives on community-driven development, responsibilities of practitioners, career advice to aspirants, domain-specific dependence of data science and tips on application-based learning. The more people love episodes, the more they subscribe and the reach of CTDS grows wider.\n\nSeveral other insights have been provided in this report. However, I leave it to the host's discretion to identify which of those are insights he could act upon.  ","4abc7045":"Before we start with this, let's aggregate all the transcripts together and create one main dataset with a few additional features that could help in analysis.","70395022":"<a id='an'><\/a>\n<div class=\"h2\">The Analysis<\/div>\n\n\nThis section outlines the analysis performed as a part of the notebook.","fbdf85db":"**Yes, those long lines of text are difficult to read!** Let me simplify things.  \n\nThe slide deck below provides an in-depth explanation of how **Qualitative Analysis** can be used to unearth patterns from these 20-line summaries of the 3 most loved episodes on CTDS.  ","aad54367":"The transcripts dataset has been created(from before), but each episode has many rows as each continuous dialogue from the speaker is recorded. So, let's aggregate these in order to get a unified transcript for an individual episode.  \n\nThe first 10 rows of this new dataset is as follows:","830b418c":"\ud83d\udca1 **INSIGHTS**\n- The largest(most used) word across episodes in each of the respective categories are\n    - Very Low : first\n    - Low : kaggle\n    - Medium : problem\n    - High : competition\n- Of course, these words are used throughout each category and an analysis based solely on thier frequencies will not make much sense. A single interview can skew the whole analysis in favour of a given word.\n- Yet, there are a few finds that cannot go unnoticed\n    - \"course\" which is a very prominent word in the very low and low categories, is suddenly diminished largely in the medium and high categories. Remember the mini-CTDS series focused on the fast.ai course tips? They had very low subscribers. That could be the reason for this.\n    - \"machine learning\" which can be missed easily in the very low and low wordclouds is suddenly very much viewable in the medium and high word clouds","6c36f2c1":"\ud83d\udca1 **INSIGHTS**  \n- All 4 different categories have roughly around the same mean when it comes to the average length of a sentence spoken by guests in episodes corresponding to the relevant category.\n- The violin plot for category High(episodes with a high number of new subscribers) stands out very distinctly from the other 3 plots\n    - It lies considerably lower than the other plots indicating how there is a chance that the episodes that were subscribed to by the viewers had guests who spoke in shorter sentences on average.","bc952bdc":"\ud83d\udca1 **INSIGHT**  \n- On an average, episodes of category \"Kaggle\" are watched more in terms of watch hours than other categories","deaab518":"# What makes a person persist in watching a video \u2753\n\nOn opening the video, not every viewer sits through the the whole video on YouTube. Similarly, in CTDS episodes too this can be observed. But, what makes viewers watch more of some videos and less of others? Could these finds be used to make CTDS episodes more engaging?  \n\n<center><img src=\"https:\/\/github.com\/ry05\/ctds-analysis\/blob\/master\/graphics\/persist-watch.png?raw=true\" width=\"900\" height=\"700\"><\/center>\n<center><i>Credits: Author<\/i><\/center>\n<br>\n\n**Why is it necessary to analyse the patterns of viewers in watching a video?**  \n- Will help understand what topics interest viewers more\n- Will help identify the indicators that will determine the percentage of an episode that viewers would watch on average\n- [Discovery of a channel is driven by watch time now in YouTube](https:\/\/creatoracademy.youtube.com\/page\/lesson\/subscriber-advantage#strategies-zippy-link-2). So, if the watch time for an episode is high, more people end up discovering it.\n\n\n\ud83d\udd0d **WHAT METRIC DEFINES SUCCESS HERE**\n\nThere are 3 features that will act as our metrics in this case:\n- youtube_watch_hours : Total hoursof watch time for an episode\n- youtube_avg_watch_duration : Average time(seconds) for which the episode was viewed for each view\n- youtube_avg_watch_percentage : Average percentage of an episode that was viewed for each view\n\n> youtube_avg_watch_percentage = youtube_avg_watch_duration \/ episode_duration\n\n\ud83d\udcda **WHAT DO WE KNOW OR BACKGROUND KNOWLEDGE**\n\n- The ability of an episode to engage it's viewers for a longer duration in one sitting is directly proportional to the value of the metrics above","e49e1b89":"## Are there apparent relations between the content of an episode and its delivery w.r.t to number of subscribers?\n\nIn this section I shall take the liberty of engineering a few new features that may or may not help, nonetheless could be new vantage points to look at the probem at hand, before we step into NLP territory.","2d383014":"### Summary of Episode 1\n\nThis is a 20-line summary of the knowledge shared by Abhishek Thakur in Episode E1 of CTDS.","6cba14d2":"<a id='ack'><\/a>\n<div class=\"h2\">Acknowledgement<\/div>\n\nI extend my gratitude to the CTDS team for making this dataset public and hosting this competition. It's been a really good exposure for me on a personal level. I would also like to thank [@parulpandey](https:\/\/www.kaggle.com\/parulpandey) and [@jpmiller](https:\/\/www.kaggle.com\/jpmiller) for providing their experience through their kernels that really helped me put this together.","f63fc3eb":"\ud83d\udd0d **ABOUT THE NEW DATASET - HOST_PRE**\n\n- Contains data about a few numerical quantities to describe the host's interactions\n- Features\n    - **episode_id** : Episode ID\n    - **num_questions_by_host** : Number of questions asked by the host(also includes questions that were asked in different formats and even follow up questions as well)\n    - **interactions** : Number of times the host spoke\/interacted\n    - **ques_ratio** : num_questions_by_host \/ interactions\n    - **num_like_used** : Number of times the host used the word 'like'\n    - **first_person_usage**: Number of times the host uses words like 'I' or 'I'm' or 'I'll'","69584d48":"\ud83d\udca1 **INSIGHTS**  \n- The top left cluster of blue circles represent the mini-CTDS series and the inaugural episode\n    - In spite of having the about the **lowest** watch hours on the channel, they have the **highest** average watch percentage of all videos on the channel\n    - This signifies a very important characteristic of the community : **\"Viewers preferred watching at a stretch, more of these episodes talking about the popular fast.ai course than other episodes on CTDS\"**\n- The very first episode, the **introductory one had the highest average watch percentage**. This makes sense as this was the shortest episode on the show at around 2.5 minutes\n- It can also be seen that the first **5 most-watched episodes** in terms of watch hours come from the **Kaggle** category\n- Research-related episodes **do not tend to be watched** a lot.\n    - This might be because research is an esoteric concept.\n    - It can't be ubiquitously understood by or interest every viewer.\n    - Also, till a year back the term \"research\" used to freak me out. So, maybe there are others out there who share that fear :)","54b145c1":"## Which episodes of CTDS received the most \ud83d\udc97?\n\nThe global data science community is a wonderful bunch of people. They are always looking to share more and talk about their experiences in the field. Some of these practitioners are highly regarded in the data space and their words of wisdom form fulcrum points in the data science journeys of several others.  \n\nOn YouTube, we tend to subscribe to a channel if we think the content it releases is worth our time and useful for us to grow as individuals. We also tend to click on the like icon of videos if we genuinely like and appreciate the content. *We might forget to give a like to a video we liked. But, we never give a like to content we don't appreciate.*  \n\nTherefore, it is natural for the **number of likes** on a video and the **new subscribers it contributes** to have a very **strong positive correlation**. Putting this to use for CTDS, we can identify the most loved episodes of CTDS.  \n\n*Episodes that have received a high number of likes as well as new subscribers are arguably the most favoured episodes of the broad data science community.* \n\nAlso, analysing these episodes individually will help us get a general idea of what really interests the CTDS fan following.","67c1a981":"## Which episodes are the most watched in terms of Youtube Watch Hours?","c34c8521":"\ud83d\udd0d **ABOUT THE NEW DATASET - SPK_SPEED**\n\n- host_avg_spk_speed : The average number of words said by the host per second\n- guest_avg_spk_speed : The average number of words said by the speaker per second\n- both_avg : The average of bothe host's and guest's speak speeds\n- diff : The difference between the host's and guest's speak speeds\n    - If -ve, host speaks slower than guest\n    - If +ve, host speaks faster than guest"}}