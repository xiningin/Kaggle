{"cell_type":{"2961aada":"code","2a1e7087":"code","00c71028":"code","e5138859":"code","74bf7d29":"code","21096b73":"code","f52882e8":"code","dae2b5cc":"code","7565ce87":"code","00462dd9":"code","f3754fda":"code","2549df19":"code","b3118bdb":"code","2efa6cb0":"code","aa27967a":"code","b4a08303":"code","d0073695":"code","85a66d5b":"code","299ae4d9":"code","1081f0db":"code","140e9e76":"code","821835d5":"code","1c40d7c6":"markdown","94a27477":"markdown","1d059bbe":"markdown","f1400131":"markdown","fc275bf7":"markdown","4320f7d4":"markdown"},"source":{"2961aada":"!pip install category_encoders","2a1e7087":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split, GridSearchCV\nfrom sklearn.preprocessing import StandardScaler, RobustScaler, PowerTransformer, PolynomialFeatures\nfrom sklearn.metrics import precision_score, r2_score, mean_squared_error, classification_report, confusion_matrix, mean_absolute_error, recall_score, accuracy_score\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.feature_selection import SelectKBest, f_regression, f_classif\nimport seaborn as sns\nimport category_encoders as ce\nimport xgboost as xgb\nimport os\n\nprint(os.listdir(\"..\/input\"))","00c71028":"\n#importing the training features, training labels, and test\ndf1 = pd.read_csv('..\/input\/kaggle-comp-proj-2\/train_features.csv')\ndf2 = pd.read_csv('..\/input\/kaggle-comp-proj-2\/train_labels.csv')\ntestdf = pd.read_csv('..\/input\/kaggle-comp-proj-2\/test_features.csv')\n\n","e5138859":"#joining the labels and features just so i dont get confused with my numbers\ndf = df1.join(df2, rsuffix = 'status_group')\n\n#pulling out the ids from the test df in order to reattach them to the predictions later\nid = pd.DataFrame(testdf['id'])\n\ndf.head()","74bf7d29":"print(df.shape)\nprint(df.isna().sum())\n","21096b73":"nan_values_list = ['Not Known', 'Unknown', 'None', 'Not known', 'not known',\n                  '-', 'unknown', 'Unknown Installer', '##', 'none', '0']\n\ndf = df.replace(nan_values_list, np.nan)\ntestdf = testdf.replace(nan_values_list, np.nan)\n\ndf = df.replace(np.nan, 'unknown')\ntestdf = testdf.replace(np.nan, 'unknown')","f52882e8":"'''because certain columns seem unimportant, redundant, \nor missing most of their values, \nI will drop them in this cell, to be modified as needed later'''\ndf2 = df.drop(['scheme_name', 'public_meeting', 'payment_type', 'region', 'idstatus_group', 'recorded_by', 'latitude', 'longitude'],axis = 1)\ntestdf = testdf.drop(['scheme_name', 'public_meeting', 'payment_type', 'region', 'recorded_by', 'latitude', 'longitude'],axis = 1)","dae2b5cc":"#this will get us the error from just guessing the most often classifications\nyencode = df['status_group'].replace({'functional':2, 'functional needs repair':1, 'non functional':0})\nyencode = pd.DataFrame(yencode)\nu = yencode.status_group.mean()\nbaseline = [u] * len(yencode.status_group)  \nprint(mean_absolute_error(yencode.status_group, baseline))\nmaj_classification = yencode.status_group.mode()\ny_pred = np.full(shape=yencode['status_group'].shape, fill_value=maj_classification)\nprint(recall_score(yencode['status_group'], y_pred, average = 'micro'))","7565ce87":"#latitude and longitude arent too telling by themselves, but together they give a unique position\ndf2['latlong'] = abs(df['latitude'].round(2)) + abs(df['longitude'].round(2))\ntestdf['latlong'] = abs(df['latitude'].round(2)) + abs(df['longitude'].round(2))\n\n#I see two values that would really ebnefit from a bin, and those are \n#population and construction year, lets do it!\nyear_bins = [-1, 1980, 1990, 2000, 2010, 2020]\nyear_labels = [1, 2, 3, 4, 5]\ndf2['year_made'] = pd.cut(df2['construction_year'], bins = year_bins, labels = year_labels)\ntestdf['year_made'] = pd.cut(testdf['construction_year'], bins = year_bins, labels = year_labels)\n\npop_bins = [-1, 10, 20, 100, 250, 1000, 5000, 10000, 100000]\npop_labels = [1, 2, 3, 4, 5, 6, 7, 8]\ndf2['pop'] = pd.cut(df2['population'], bins = pop_bins, labels = pop_labels)\ntestdf['pop'] = pd.cut(testdf['population'], bins = pop_bins, labels = pop_labels)","00462dd9":"#alright, whats it looking like?  Numbers, awayyyyyy~\nprint(df2.nunique())\ndf2.describe(include = 'all')","f3754fda":"df2['wpt_name'].value_counts()\n# i wanted to reduce the number of unique variables in each column, and it looks like a jackpot!\n#a bunch of these look similar to each other, im sure theres some regex for it!...\n","2549df19":"#...and heres my version! \n\n\ndf2['wpt_name'] = df2.wpt_name.str.replace(r'((?i)^.*clinic.*$)', 'health')\ndf2['wpt_name'] = df2.wpt_name.str.replace(r'((?i)^.*hospital.*$)', 'health')\ndf2['wpt_name'] = df2.wpt_name.str.replace(r'((?i)^.*zahanati.*$)', 'health')\ndf2['wpt_name'] = df2.wpt_name.str.replace(r'((?i)^.*health.*$)', 'health')\ndf2['wpt_name'] = df2.wpt_name.str.replace(r'((?i)^.*secondary.*$)', 'school')\ndf2['wpt_name'] = df2.wpt_name.str.replace(r'((?i)^.*school.*$)', 'school')\ndf2['wpt_name'] = df2.wpt_name.str.replace(r'((?i)^.*shule.*$)', 'school')\ndf2['wpt_name'] = df2.wpt_name.str.replace(r'((?i)^.*sekondari.*$)', 'school')\ndf2['wpt_name'] = df2.wpt_name.str.replace(r'((?i)^.*msingi.*$)', 'school')\ndf2['wpt_name'] = df2.wpt_name.str.replace(r'((?i)^.*primary.*$)', 'school')\ntestdf['wpt_name'] = testdf.wpt_name.str.replace(r'((?i)^.*secondary.*$)', 'school')\ntestdf['wpt_name'] = testdf.wpt_name.str.replace(r'((?i)^.*school.*$)', 'school')\ntestdf['wpt_name'] = testdf.wpt_name.str.replace(r'((?i)^.*shule.*$)', 'school')\ntestdf['wpt_name'] = testdf.wpt_name.str.replace(r'((?i)^.*sekondari.*$)', 'school')\ntestdf['wpt_name'] = testdf.wpt_name.str.replace(r'((?i)^.*msingi.*$)', 'school')\ntestdf['wpt_name'] = testdf.wpt_name.str.replace(r'((?i)^.*primary.*$)', 'school')\ntestdf['wpt_name'] = testdf.wpt_name.str.replace(r'((?i)^.*clinic.*$)', 'health')\ntestdf['wpt_name'] = testdf.wpt_name.str.replace(r'((?i)^.*hospital.*$)', 'health')\ntestdf['wpt_name'] = testdf.wpt_name.str.replace(r'((?i)^.*zahanati.*$)', 'health')\ntestdf['wpt_name'] = testdf.wpt_name.str.replace(r'((?i)^.*health.*$)', 'health')\ndf2['wpt_name'] = df2.wpt_name.str.replace(r'((?i)^.*ccm.*$)', 'official')\ndf2['wpt_name'] = df2.wpt_name.str.replace(r'((?i)^.*office.*$)', 'official')\ndf2['wpt_name'] = df2.wpt_name.str.replace(r'((?i)^.*kijiji.*$)', 'official')\ndf2['wpt_name'] = df2.wpt_name.str.replace(r'((?i)^.*ofis.*$)', 'official')\ndf2['wpt_name'] = df2.wpt_name.str.replace(r'((?i)^.*idara.*$)', 'offical')\ndf2['wpt_name'] = df2.wpt_name.str.replace(r'((?i)^.*maziwa.*$)', 'farm')\ndf2['wpt_name'] = df2.wpt_name.str.replace(r'((?i)^.*farm.*$)', 'farm')\ndf2['wpt_name'] = df2.wpt_name.str.replace(r'((?i)^.*maji.*$)', 'pump')\ndf2['wpt_name'] = df2.wpt_name.str.replace(r'((?i)^.*water.*$)', 'pump')\ndf2['wpt_name'] = df2.wpt_name.str.replace(r'((?i)^.*pump house.*$)', 'pump')\ndf2['wpt_name'] = df2.wpt_name.str.replace(r'((?i)^.*pump.*$)', 'pump')\ndf2['wpt_name'] = df2.wpt_name.str.replace(r'((?i)^.*bombani.*$)', 'pump')\ndf2['wpt_name'] = df2.wpt_name.str.replace(r'((?i)^.*center.*$)', 'center')\ndf2['wpt_name'] = df2.wpt_name.str.replace(r'((?i)^.*madukani.*$)', 'center')\ndf2['wpt_name'] = df2.wpt_name.str.replace(r'((?i)^.*sokoni.*$)', 'center')\ndf2['wpt_name'] = df2.wpt_name.str.replace(r'((?i)^.*market.*$)', 'center')\ndf2['wpt_name'] = df2.wpt_name.str.replace(r'((?i)^.*kwa.*$)', 'name')\ntestdf['wpt_name'] = testdf.wpt_name.str.replace(r'((?i)^.*ccm.*$)', 'official')\ntestdf['wpt_name'] = testdf.wpt_name.str.replace(r'((?i)^.*office.*$)', 'official')\ntestdf['wpt_name'] = testdf.wpt_name.str.replace(r'((?i)^.*kijiji.*$)', 'official')\ntestdf['wpt_name'] = testdf.wpt_name.str.replace(r'((?i)^.*ofis.*$)', 'official')\ntestdf['wpt_name'] = testdf.wpt_name.str.replace(r'((?i)^.*idara.*$)', 'offical')\ntestdf['wpt_name'] = testdf.wpt_name.str.replace(r'((?i)^.*maziwa.*$)', 'farm')\ntestdf['wpt_name'] = testdf.wpt_name.str.replace(r'((?i)^.*farm.*$)', 'farm')\ntestdf['wpt_name'] = testdf.wpt_name.str.replace(r'((?i)^.*maji.*$)', 'pump')\ntestdf['wpt_name'] = testdf.wpt_name.str.replace(r'((?i)^.*water.*$)', 'pump')\ntestdf['wpt_name'] = testdf.wpt_name.str.replace(r'((?i)^.*pump house.*$)', 'pump')\ntestdf['wpt_name'] = testdf.wpt_name.str.replace(r'((?i)^.*pump.*$)', 'pump')\ntestdf['wpt_name'] = testdf.wpt_name.str.replace(r'((?i)^.*bombani.*$)', 'pump')\ntestdf['wpt_name'] = testdf.wpt_name.str.replace(r'((?i)^.*center.*$)', 'center')\ntestdf['wpt_name'] = testdf.wpt_name.str.replace(r'((?i)^.*madukani.*$)', 'center')\ntestdf['wpt_name'] = testdf.wpt_name.str.replace(r'((?i)^.*sokoni.*$)', 'center')\ntestdf['wpt_name'] = testdf.wpt_name.str.replace(r'((?i)^.*market.*$)', 'center')\ntestdf['wpt_name'] = testdf.wpt_name.str.replace(r'((?i)^.*kwa.*$)', 'name')\ndf2['wpt_name'] = df2.wpt_name.str.replace(r'((?i)^.*none.*$)', 'other')\ntestdf['wpt_name'] = testdf.wpt_name.str.replace(r'((?i)^.*none.*$)', 'other')\n\n#and then removing anything with less than 100 entries\nvalue_counts = df2['wpt_name'].value_counts()\nto_remove = value_counts[value_counts <= 100].index\ndf2['wpt_name'].replace(to_remove, 'other', inplace=True)\nvalue_counts = testdf['wpt_name'].value_counts()\nto_remove = value_counts[value_counts <= 100].index\ntestdf['wpt_name'].replace(to_remove, 'other', inplace=True)\n\n","b3118bdb":"df2['wpt_name'].nunique()\n#a little better","2efa6cb0":"#so what are we working with here\ntestdf.nunique()","aa27967a":"#ok lga seems important, lets go ahead and get that one manageable next\n\nseries = df2['lga'].copy()\nseries[series.str.contains('Rural')] = 'rural'\nseries[series.str.contains('Urban')] = 'urban'\nother_flag = series.str.contains('rural') | series.str.contains('urban')\nother_flag = other_flag == False\nseries[other_flag] = 'other'\n\ndf2['lga'] = series\n\nseries = testdf['lga'].copy()\nseries[series.str.contains('Rural')] = 'rural'\nseries[series.str.contains('Urban')] = 'urban'\nother_flag = series.str.contains('rural') | series.str.contains('urban')\nother_flag = other_flag == False\nseries[other_flag] = 'other'\n\ntestdf['lga'] = series","b4a08303":"#population number is a string apparently??...\ndf2['population'] = df2['population'].astype(int)\ntestdf['population'] = testdf['population'].astype(int)\n#not anymore!  Nice try, python, but you cant beat me with simple string!","d0073695":"#i want to make two new columns, one which shows the date \n#recorded as an integer, for manipulating purposes..\n#the other is a column that takes the recorded date and\n#the year and makes an approx. years in operation column\ndf2['2011'] = df2['date_recorded'].str.contains('2011', na=False, regex=True)\ndf2['2012'] = df2['date_recorded'].str.contains('2012', na=False, regex=True)\ndf2['2013'] = df2['date_recorded'].str.contains('2013', na=False, regex=True)\n\ntestdf['2011'] = testdf['date_recorded'].str.contains('2011', na=False, regex=True)\ntestdf['2012'] = testdf['date_recorded'].str.contains('2012', na=False, regex=True)\ntestdf['2013'] = testdf['date_recorded'].str.contains('2013', na=False, regex=True)\n\ndf2['2011'] = df2['2011'].replace(True, 2011)\ndf2['2012'] = df2['2012'].replace(True, 2012)\ndf2['2013'] = df2['2013'].replace(True, 2013)\n\ntestdf['2011'] = testdf['2011'].replace(True, 2011)\ntestdf['2012'] = testdf['2012'].replace(True, 2012)\ntestdf['2013'] = testdf['2013'].replace(True, 2013)\n\nlarger = lambda s1, s2: s1 if s1.sum() > s2.sum() else s2\n\ndf2['yrs_intermediate'] = df2['2011'].combine(df2['2012'], larger)\ndf2['yrs_intermediate'] = df2['yrs_intermediate'].combine(df2['2013'], larger)\ndf2['yrs in operation'] = df2['yrs_intermediate'] - df2['construction_year']\n\ntestdf['yrs_intermediate'] = testdf['2011'].combine(testdf['2012'], larger)\ntestdf['yrs_intermediate'] = testdf['yrs_intermediate'].combine(testdf['2013'], larger)\ntestdf['yrs in operation'] = testdf['yrs_intermediate'] - testdf['construction_year']\n\ndf2.head()","85a66d5b":"# But first!  Targets and features separated, plz and ty!\nX = df2.drop(['status_group', 'funder', 'installer', 'subvillage', 'construction_year', 'num_private',\n             'extraction_type_group', 'quantity_group', 'source_class', 'source_type', 'subvillage',\n             'permit', 'date_recorded', '2011', '2012', '2013', 'id', 'waterpoint_type_group',\n             'amount_tsh', 'management_group', 'district_code', 'quality_group',\n             'extraction_type_class'], axis = 1)\ny = df2['status_group']\n\nXtdf = testdf.drop(['funder', 'installer', 'subvillage', 'construction_year', 'num_private',\n                   'extraction_type_group', 'quantity_group', 'source_class', 'source_type',\n                   'subvillage', 'permit', 'date_recorded', '2011', '2012', '2013', 'id', 'waterpoint_type_group',\n                   'amount_tsh', 'management_group', 'district_code', 'quality_group',\n                   'extraction_type_class'], axis = 1)\n\nX_train, X_test, y_train, y_test = train_test_split(X, y)","299ae4d9":"#After trying a bunch of pipelines, i settled on\nfrom sklearn.ensemble import RandomForestClassifier\npipelinef = make_pipeline(\n    ce.OneHotEncoder(use_cat_names=True),\n    RobustScaler(),\n    RandomForestClassifier(n_estimators = 1000)\n)\n#possibly the others would work with more feature engineering, but i got lower scores with them\n#and i spent half the week looking up regex :,D","1081f0db":"pipelinef.fit(X_train, y_train)\ny_pred = pipelinef.predict(X_test)\naccuracy_score(y_test, y_pred)\n#yay thats not horrible~","140e9e76":"#convoluted but it works!\n#turning my pipeline prediction into a DF then joining it up with the index to make a dec file\nsubby = pipelinef.predict(Xtdf)\nprint(subby.shape)\nsubby = pd.DataFrame(subby)\nsubs = id.join(subby, rsuffix = '0')\nsubs = subs.rename(index=str, columns={0:'status_group'})\nprint(subs.shape)\nsubs.head()","821835d5":"subs.to_csv('forest_for_the_trees.csv', index=False)\n#alright this was my best performing models and features out of many more, now get this out of my face!!","1c40d7c6":"Now that we have our baseslines, its time to look for some features that might go together","94a27477":"after a little searching we find some values that stand in for unknowns, so lets go ahead and take care of those","1d059bbe":"Ok, time for some baselines","f1400131":"Imports and data ","fc275bf7":"after the inital imports, we need to explore what data we have here.","4320f7d4":"Now lets put it through some pipes and see what squeezes through!!"}}