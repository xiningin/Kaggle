{"cell_type":{"c1a4853d":"code","c8d6c99e":"code","9e42979f":"code","76af0520":"code","23916a6b":"code","ffb77400":"code","9147b19d":"code","be41b8af":"code","e4db0168":"code","ad66f1d4":"code","49f4c22b":"code","e9b83f30":"code","9891e2ec":"code","2dcfa831":"code","89cc1d18":"code","77e2d54d":"code","4df4c99c":"code","7e795196":"code","80fe226f":"code","c95f8b23":"code","d7a10750":"code","06fd5d9e":"code","ea0387c2":"code","aed58250":"code","cb8b8573":"code","840a55a7":"code","f7f91f59":"code","c025d77d":"code","d7febdd0":"code","a76fb175":"code","f21cca21":"code","5de5db9f":"code","9f8a4c8e":"code","f4fd6c23":"markdown","92ea47fa":"markdown","aafa2383":"markdown","f4f7267a":"markdown","329ea72b":"markdown","e952548a":"markdown","76513a69":"markdown","afce66f8":"markdown","d7eb9842":"markdown"},"source":{"c1a4853d":"import datetime\nimport gc\nimport numpy as np\nimport os\nimport operator\nimport pandas as pd\nfrom tqdm import tqdm\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom scipy.stats import describe\n%matplotlib inline\n\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.metrics import f1_score, mean_squared_error\nfrom sklearn.model_selection import KFold, RepeatedKFold, GroupKFold\nfrom imblearn.under_sampling import RandomUnderSampler\nimport lightgbm as lgb\nimport xgboost as xgb","c8d6c99e":"train_df = pd.read_csv('..\/input\/train.csv')\ntrain_df.head()","9e42979f":"test_df = pd.read_csv('..\/input\/test.csv')","76af0520":"print(\"Costa Rican Household Poverty Level Prediction -  rows:\",train_df.shape[0],\" columns:\", train_df.shape[1])\nprint(\"Costa Rican Household Poverty Level Prediction -  rows:\",test_df.shape[0],\" columns:\", test_df.shape[1])","23916a6b":"test_df.head()","ffb77400":"train_df.isnull().values.any()","9147b19d":"test_df.isnull().values.any()","be41b8af":"train_df.info()","e4db0168":"train_df.isnull().values.sum(axis=0)","ad66f1d4":"train_df_describe = train_df.describe()\ntrain_df_describe","49f4c22b":"test_df_describe = test_df.describe()\ntest_df_describe","e9b83f30":"test_df.isnull().values.sum(axis=0)","9891e2ec":"plt.figure(figsize=(12, 5))\nplt.hist(train_df.Target.values, bins=4)\nplt.title('Histogram - target counts')\nplt.xlabel('Count')\nplt.ylabel('Target')\nplt.show()","2dcfa831":"plt.title(\"Distribution of Target\")\nsns.distplot(train_df['Target'].dropna(),color='blue', kde=True,bins=100)\nplt.show()","89cc1d18":"sns.set_style(\"whitegrid\")\nax = sns.violinplot(x=train_df.Target.values)\nplt.show()","77e2d54d":"plt.title(\"Distribution of log(target)\")\nsns.distplot(np.log1p(train_df['Target']).dropna(),color='blue', kde=True,bins=100)\nplt.show()","4df4c99c":"sns.set_style(\"whitegrid\")\nax = sns.violinplot(x=np.log(1+train_df.Target.values))\nplt.show()","7e795196":"np.unique(train_df.Target.values)","80fe226f":"columns_to_use = train_df.columns[1:-1]","c95f8b23":"columns_to_use","d7a10750":"y = train_df['Target'].values-1","06fd5d9e":"train_test_df = pd.concat([train_df[columns_to_use], test_df[columns_to_use]], axis=0)\n# extract columns which data type is object\nobject_cols = [f_ for f_ in train_test_df.columns if train_test_df[f_].dtype == 'object']","ea0387c2":"# labeling\nfor col in object_cols:\n    le = LabelEncoder()\n    print(col)\n    le.fit(train_test_df[col].astype(str))\n    train_df[col] = le.transform(train_df[col].astype(str))\n    test_df[col] = le.transform(test_df[col].astype(str))\ndel le","aed58250":"def dprint(*args, **kwargs):\n    print(\"[{}] \".format(datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M\")) + \\\n        \" \".join(map(str,args)), **kwargs)\n\nid_name = 'Id'\ntarget_name = 'Target'\n\ndf_all = pd.concat([train_df, test_df], axis=0)\ncols = [f_ for f_ in df_all.columns if df_all[f_].dtype == 'object' and f_ != id_name]\nprint(cols)\n\nfor c in tqdm(cols):\n    le = preprocessing.LabelEncoder()\n    le.fit(df_all[c].astype(str))\n    train_df[c] = le.transform(train_df[c].astype(str))\n    test_df[c] = le.transform(test[c].astype(str))\n\n    del le\ngc.collect()\n\ndef extract_features(df):\n    df['bedrooms_to_rooms'] = df['bedrooms']\/df['rooms']\n    df['rent_to_rooms'] = df['v2a1']\/df['rooms']\n    df['tamhog_to_rooms'] = df['tamhog']\/df['rooms']\n\nextract_features(train_df)\nextract_features(test_df)","cb8b8573":"labels = []\nvalues = []\nfor col in train_df.columns:\n    if col not in [\"Id\", \"Target\"]:\n        labels.append(col)\n        values.append(np.corrcoef(train_df[col].values, train_df[\"Target\"].values)[0,1])\ncorr_df = pd.DataFrame({'columns_labels':labels, 'corr_values':values})\ncorr_df = corr_df.sort_values(by='corr_values')\n \ncorr_df = corr_df[(corr_df['corr_values']>0.20) | (corr_df['corr_values']<-0.20)]\nind = np.arange(corr_df.shape[0])\nwidth = 0.9\nfig, ax = plt.subplots(figsize=(10,6))\nrects = ax.barh(ind, np.array(corr_df.corr_values.values), color='black')\nax.set_yticks(ind)\nax.set_yticklabels(corr_df.columns_labels.values, rotation='horizontal')\nax.set_xlabel(\"Correlation coefficient\")\nax.set_title(\"Correlation coefficient of the variables\")\nplt.show()","840a55a7":"temp_df = train_df[corr_df.columns_labels.tolist()]\ncorrmat = temp_df.corr(method='pearson')\nf, ax = plt.subplots(figsize=(12, 12))\nsns.heatmap(corrmat, vmax=1., square=True, cmap=plt.cm.BrBG)\nplt.title(\"Important variables correlation map\", fontsize=15)\nplt.show()","f7f91f59":"cnt = 0\np_buf = []\nn_splits = 20\nn_repeats = 1\nkf = RepeatedKFold(\n    n_splits=n_splits, \n    n_repeats=n_repeats, \n    random_state=None)\nerr_buf = []   \n\ncols_to_drop = [\n    id_name, \n    target_name,\n]\nX = train_df.drop(cols_to_drop, axis=1, errors='ignore')\nfeature_names = list(X.columns)\nX = X.fillna(0)\nX = X.values\ny = train_df[target_name].values\n\nclasses = np.unique(y)\ndprint('Number of classes: {}'.format(len(classes)))\nc2i = {}\ni2c = {}\nfor i, c in enumerate(classes):\n    c2i[c] = i\n    i2c[i] = c\n\ny_le = np.array([c2i[c] for c in y])\n\nX_test = test_df.drop(cols_to_drop, axis=1, errors='ignore')\nX_test = X_test.fillna(0)\nX_test = X_test.values\nid_test = test_df[id_name].values\n\ndprint(X.shape, y.shape)\ndprint(X_test.shape)\n\nn_features = X.shape[1]\n\nlgb_params = {\n    'boosting_type': 'gbdt',\n    'objective': 'multiclass',\n    'metric': 'multi_logloss',\n    'max_depth': -1,\n    'num_leaves': 14,\n    'learning_rate': 0.1,\n    'feature_fraction': 0.85,\n    'bagging_fraction': 0.85,\n    'bagging_freq': 5,\n    'verbose': -1,\n    'num_threads': 8,\n    'colsample_bytree': 0.89,\n    'min_child_samples': 90,\n    'subsample': 0.96,\n    'lambda_l2': 1.0,\n    'min_gain_to_split': 0,\n    'num_class': len(np.unique(y)),\n}","c025d77d":"sampler = RandomUnderSampler(random_state=314)\nX, y = sampler.fit_sample(X, y)\ny_le = np.array([c2i[c] for c in y])\n\nfor train_index, valid_index in kf.split(X, y):\n    print('Fold {}\/{}*{}'.format(cnt + 1, n_splits, n_repeats))\n    params = lgb_params.copy() \n\n    lgb_train = lgb.Dataset(\n        X[train_index], \n        y_le[train_index], \n        feature_name=feature_names,\n        )\n    lgb_train.raw_data = None\n\n    lgb_valid = lgb.Dataset(\n        X[valid_index], \n        y_le[valid_index],\n        feature_name=feature_names,\n        )\n    lgb_valid.raw_data = None\n\n    model = lgb.train(\n        params,\n        lgb_train,\n        num_boost_round=99999,\n        valid_sets=[lgb_train, lgb_valid],\n        early_stopping_rounds=400, \n        verbose_eval=100, \n    )\n\n    if cnt == 0:\n        importance = model.feature_importance()\n        model_fnames = model.feature_name()\n        tuples = sorted(zip(model_fnames, importance), key=lambda x: x[1])[::-1]\n        tuples = [x for x in tuples if x[1] > 0]\n        print('Important features:')\n        for i in range(10):\n            if i < len(tuples):\n                print(i, tuples[i])\n            else:\n                break\n\n        del importance, model_fnames, tuples\n\n    p = model.predict(X[valid_index], num_iteration=model.best_iteration)\n\n    err = f1_score(y_le[valid_index], np.argmax(p, axis=1), average='macro')\n\n    dprint('{} F1: {}'.format(cnt + 1, err))\n\n    p = model.predict(X_test, num_iteration=model.best_iteration)\n    if len(p_buf) == 0:\n        p_buf = np.array(p, dtype=np.float16)\n    else:\n        p_buf += np.array(p, dtype=np.float16)\n    err_buf.append(err)\n\n    cnt += 1\n\n    del model, lgb_train, lgb_valid, p\n    gc.collect","d7febdd0":"err_mean = np.mean(err_buf)\nerr_std = np.std(err_buf)\nprint('F1 = {:.6f} +\/- {:.6f}'.format(err_mean, err_std))\npreds = p_buf\/cnt","a76fb175":"print(preds)\npreds = np.argmax(preds, axis = 1) +1\npreds","f21cca21":"sample_submission  = pd.read_csv(\"..\/input\/sample_submission.csv\")\nsample_submission.head()","5de5db9f":"sample_submission['Target'] = preds\nsample_submission.to_csv('submission_{:.6f}.csv'.format(err_mean), index=False)\nsample_submission.head()","9f8a4c8e":"np.mean(preds)","f4fd6c23":"## Predictive Model","92ea47fa":"### Import Libraries","aafa2383":"## Identifying features that are highly correlated with target","f4f7267a":"## Shape of the data","329ea72b":"## Distribution of Target Variable","e952548a":"### Violin distribution of target","76513a69":"## Correlation matrix of the most highly correlated features","afce66f8":"## Missing Values","d7eb9842":"# Beginner's Tutorial to Costa Rican Household Poverty Level Prediction\n![](https:\/\/www.habitatforhumanity.org.uk\/wp-content\/uploads\/2017\/10\/Housing-poverty-Costa-Rica--1200x600-c-default.jpg)\n![](https:\/\/www.habitatforhumanity.org.uk\/wp-content\/uploads\/2017\/10\/where-we-work-costa-rica--800x400-c-default.jpg)\n ## How we address housing poverty in Costa Rica"}}