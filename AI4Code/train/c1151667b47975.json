{"cell_type":{"80814e8e":"code","870084d7":"code","c4ea4886":"code","6a5de719":"code","ea3bec0a":"code","77de712d":"code","131d4b8f":"code","7f5d92ca":"code","102e9c3d":"code","cdc1e918":"code","dff35056":"code","4d796314":"code","a9e198b5":"code","ae5217bc":"code","8e568aa3":"code","047880e0":"code","e7054ca2":"code","f093d68c":"code","d08d1746":"code","06f674d1":"code","45003662":"code","d7738859":"code","4edb0461":"code","ef51e3ae":"markdown","4d644d00":"markdown","a9531ab3":"markdown","17c028a9":"markdown","e3a0749e":"markdown","147a1b18":"markdown","0636d11b":"markdown","486d8738":"markdown","dbc31adc":"markdown","9e6e4c6e":"markdown","b04a910c":"markdown"},"source":{"80814e8e":"import sys\n!cp ..\/input\/rapids\/rapids.0.18.0 \/opt\/conda\/envs\/rapids.tar.gz\n!cd \/opt\/conda\/envs\/ && tar -xzvf rapids.tar.gz > \/dev\/null\nsys.path = [\"\/opt\/conda\/envs\/rapids\/lib\/python3.7\/site-packages\"] + sys.path\nsys.path = [\"\/opt\/conda\/envs\/rapids\/lib\/python3.7\"] + sys.path\nsys.path = [\"\/opt\/conda\/envs\/rapids\/lib\"] + sys.path \n!cp \/opt\/conda\/envs\/rapids\/lib\/libxgboost.so \/opt\/conda\/lib\/","870084d7":"import cudf as pd # pandas on GPU\nimport cupy as np # numpy on GPU\nfrom cuml.decomposition import PCA # scikit-learn on GPU\n!pip install sentence-transformers\nfrom sentence_transformers import SentenceTransformer # PyTorch supported\nimport gc\nimport torch\nimport matplotlib.pyplot as plt\nimport matplotlib as mpl","c4ea4886":"df_train = pd.read_csv('..\/input\/commonlitreadabilityprize\/train.csv')\ndf_test  = pd.read_csv('..\/input\/commonlitreadabilityprize\/test.csv')","6a5de719":"df_train.head()","ea3bec0a":"df_train.shape","77de712d":"n0 = df_train.shape[0]\n# text is upper case in source data, but models are lower case\n# txt = [sent3.lower() for df in (df_train, df_test) for t in df.excerpt.to_array() for sent in t.split('\\n') for sent2 in sent.split('.') for sent3 in sent2.split(';')]\ntxt = [t.lower() for df in (df_train, df_test) for t in df.excerpt.to_array()]\nids = [id_ for df in (df_train, df_test) for id_ in df.id.to_array() ]\ntxt[:3]","131d4b8f":"IDS = []\nTXT = []\nfor t, id_ in zip(txt, ids):\n    sent = [w.replace('\\\\', '') for w in t.split('\\n')]\n    TXT += sent\n    for _ in range(len(sent)):\n        IDS.append(id_)\nTXT[:3]","7f5d92ca":"txt = []\nids = []\nfor t, id_ in zip(TXT, IDS):\n    sent = [t]\n    for char in ['...', '.', ';', '!', '?', '\"']:\n        sent = [w for t_ in sent for w in t_.split(char)]\n    sent = list(filter(lambda w: len(w)>1, sent))\n    txt += sent\n    for _ in range(len(sent)):\n        ids.append(id_)","102e9c3d":"# txt, ids","cdc1e918":"if torch.cuda.is_available(): # check if GPU enabled kernel\"\n    print('Cuda !')","dff35056":"model = SentenceTransformer('paraphrase-distilroberta-base-v1', device='cuda')\nprint(f'Initial sequence length in paraphrase distilroberta : {model.max_seq_length}')\nprint(f'First sentence : {txt[0]}\\nCorresponding tokens : {model.tokenizer(txt[0])}')\nprint(f\"Maximal sequence length in our text data : {max([len(model.tokenizer(t)['input_ids']) for t in txt])}\")\nmodel.max_seq_length = 150\nprint(f'Resized sequence length in paraphrase distilroberta : {model.max_seq_length}')","4d796314":"txt_encoded = np.array(model.encode(txt, normalize_embeddings=True))\ntxt_encoded.shape","a9e198b5":"plt.hist(np.var(txt_encoded, axis=0).get(), bins=100)\nplt.title('Variance on the 768 Embedding Coordinates')\nplt.show()","ae5217bc":"train_ids = [i for i in ids if i in df_train.id.to_pandas().values]","8e568aa3":"n0 = len(train_ids)","047880e0":"x_train, x_test = txt_encoded[:n0, :], txt_encoded[n0:, :]\nx_train.shape","e7054ca2":"targets = np.array([df_train.loc[df_train.id==i, 'target'].values[0, 0] for i in train_ids])\ntargets","f093d68c":"import torch\nfrom torch.autograd import Variable\nimport torch.nn.functional as F\nimport torch.utils.data as Data\n\ntorch.manual_seed(1)    # reproducible\n\nnet = torch.nn.Sequential(\n        torch.nn.Linear(768, 200),\n        torch.nn.LeakyReLU(),\n        torch.nn.Linear(200, 200),\n        torch.nn.LeakyReLU(),\n        torch.nn.Linear(200, 100),\n        torch.nn.LeakyReLU(),\n        torch.nn.Linear(100, 1),\n    )\nnet.cuda()\n\noptimizer = torch.optim.Adam(net.parameters(), lr=0.001)\nloss_func = torch.nn.MSELoss()  # this is for regression mean squared loss\n\nBATCH_SIZE = 64\nEPOCH = 10\n\ntorch_dataset = Data.TensorDataset(torch.tensor(x_train, device='cuda'), torch.tensor(targets, device='cuda'))","d08d1746":"from tqdm.notebook import tqdm\nloader = Data.DataLoader(\n    dataset=torch_dataset, \n    batch_size=BATCH_SIZE, \n    shuffle=True)\n# start training\nl = []\nfor epoch in tqdm(range(EPOCH)):\n    total_loss = 0\n    for step, (b_x, b_y) in enumerate(loader): # for each training step\n        \n#         b_x = Variable(batch_x)\n#         b_y = Variable(batch_y)\n\n        prediction = net(b_x)     # input x and predict based on x\n\n        loss = loss_func(prediction.float(), b_y.float())     # must be (1. nn output, 2. target)\n        with torch.no_grad():\n            total_loss += loss.item() \/ len(loader)\n        optimizer.zero_grad()   # clear gradients for next train\n        loss.backward()         # backpropagation, compute gradients\n        optimizer.step()        # apply gradients\n    l.append(total_loss)\nplt.plot(l)\nplt.show()","06f674d1":"pred = net(torch.tensor(x_test, device='cuda'))\npred","45003662":"sub = pd.read_csv('..\/input\/commonlitreadabilityprize\/sample_submission.csv')\nsub.head()","d7738859":"test_ids = [i for i in ids if i in df_test.id.to_pandas().values]\nsub = pd.DataFrame({'id':test_ids, 'target':pred.detach().cpu()}).groupby('id').mean()\nsub.head()","4edb0461":"sub.to_csv('submission.csv', index=True)","ef51e3ae":"That's not great... Predicts almost a constant ! We'll have to improve that !","4d644d00":"### Installing RAPIDS and other requirements","a9531ab3":"# Simple predictive neuron","17c028a9":"We will concatenate text data from train and test database in order to process them globally.","e3a0749e":"Transformers are a very efficient way of getting optimal text embeddings.\nI will compute raw sentence embeddings based on the paraphrase-trained DistilRoberta. You can see more on this model [here](https:\/\/github.com\/UKPLab\/sentence-transformers) or [here](https:\/\/www.sbert.net).","147a1b18":"### Model Preparation","0636d11b":"[RAPIDS](https:\/\/rapids.ai) enable you to perform every numpy, pandas or sklearn manipulation & modeling, entirely on GPU for higher performance.","486d8738":"### Raw Roberta embeddings","dbc31adc":"# Sentence Embeddings for Regression with RAPIDS","9e6e4c6e":"## Summary :\n1. Obtaining Sentence Embeddings from Transformers\n2. Using it for Regression","b04a910c":"### Reading and Processing Text Data"}}