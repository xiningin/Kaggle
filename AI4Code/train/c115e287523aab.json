{"cell_type":{"94c1c544":"code","873c5e21":"code","8656137d":"code","b0f45e2b":"code","b1e1a9ed":"code","defbece4":"code","2619d638":"code","e372d4e4":"code","b86b3cd2":"code","05021094":"code","063d2567":"code","4a6913c2":"code","47b6493f":"code","fefe7e3d":"code","8470995f":"code","cf842146":"code","99ef6bd1":"code","69a83aff":"code","1330ae1a":"code","053bf4fb":"code","cf665456":"code","ad72d2d7":"code","67878b86":"code","58395a02":"code","8bfd3078":"code","d26fd6c1":"code","260d56f9":"code","6dbf1608":"code","492f3b37":"code","7c475c52":"code","3881ed38":"code","8268d5db":"markdown","dd9ed69d":"markdown","0bc798af":"markdown","3f1a737b":"markdown","90896829":"markdown","3a27834a":"markdown","40af5077":"markdown","47b04242":"markdown","8ee75e12":"markdown","169a0072":"markdown","e3765dfa":"markdown","40825f76":"markdown","157a76fa":"markdown","24043ef7":"markdown","6edc446b":"markdown","c7c6b93a":"markdown","70ae2180":"markdown","f86a81f1":"markdown","9f12256c":"markdown","185fd888":"markdown","10e401b2":"markdown","27cd4382":"markdown","e49c3c5c":"markdown","e6c2fce1":"markdown","cef696c1":"markdown","c16324c6":"markdown","4353e393":"markdown","fe0ff487":"markdown","e6d3b1ee":"markdown","7d16222f":"markdown","0baadf13":"markdown","1a13557d":"markdown","a231698c":"markdown","bae59788":"markdown"},"source":{"94c1c544":"!pip install -q efficientnet >> \/dev\/null\n!pip install -q imagesize\n!pip install -qU wandb","873c5e21":"import pandas as pd, numpy as np, random,os, shutil\nimport tensorflow as tf, re, math\nimport tensorflow.keras.backend as K\nimport efficientnet.tfkeras as efn\nimport sklearn\nimport matplotlib.pyplot as plt\nimport tensorflow_addons as tfa\nimport imagesize\nimport wandb\nimport yaml\nfrom IPython import display as ipd\n\nfrom glob import glob\nfrom tqdm.notebook import tqdm\nfrom kaggle_datasets import KaggleDatasets\nfrom sklearn.model_selection import KFold, StratifiedKFold, GroupKFold\nfrom sklearn.metrics import roc_auc_score","8656137d":"print('np:', np.__version__)\nprint('pd:', pd.__version__)\nprint('sklearn:', sklearn.__version__)\nprint('tf:',tf.__version__)\nprint('tfa:', tfa.__version__)\nprint('w&b:', wandb.__version__)","b0f45e2b":"import wandb\n\ntry:\n    from kaggle_secrets import UserSecretsClient\n    user_secrets = UserSecretsClient()\n    api_key = user_secrets.get_secret(\"WANDB\")\n    wandb.login(key=api_key)\n    anonymous = None\nexcept:\n    anonymous = \"must\"\n    print('To use your W&B account,\\nGo to Add-ons -> Secrets and provide your W&B access token. Use the Label name as WANDB. \\nGet your W&B access token from here: https:\/\/wandb.ai\/authorize')","b1e1a9ed":"class CFG:\n    wandb         = True\n    competition   = 'petfinder' \n    _wandb_kernel = 'awsaf49'\n    debug         = False\n    exp_name      ='effnetb6-baseline' # name of the experiment, folds will be grouped using 'exp_name'\n    \n    # USE verbose=0 for silent, vebose=1 for interactive, verbose=2 for commit\n    verbose      = 1 if debug else 0\n    display_plot = True\n\n    device     = \"TPU\" #or \"GPU\"\n\n    model_name = 'efficientnet_b6'\n\n    # USE DIFFERENT SEED FOR DIFFERENT STRATIFIED KFOLD\n    seed = 42\n\n    # NUMBER OF FOLDS. USE 2, 5, 10\n    folds = 5\n    \n    # FOLDS TO TRAIN\n    selected_folds = [0, 1, 2, 3, 4]\n\n    # IMAGE SIZE\n    img_size = [512, 512]\n\n    # BATCH SIZE AND EPOCHS\n    batch_size  = 32\n    epochs      = 12\n\n    # LOSS\n    loss      = 'RMSE'\n    optimizer = 'Adam'\n\n    # CFG.augmentATION\n    augment   = True\n    transform = False\n\n    # TRANSFORMATION\n    fill_mode = 'nearest'\n    rot    = 10.0\n    shr    = 5.0\n    hzoom  = 30.0\n    wzoom  = 30.0\n    hshift = 30.0\n    wshift = 30.0\n\n    # FLIP\n    hflip = True\n    vflip = False\n\n    # CLIP [0, 1]\n    clip = False\n\n    # LEARNING RATE SCHEDULER\n    scheduler   = 'exp' # Cosine\n\n    # Dropout\n    drop_prob   = 0.75\n    drop_cnt    = 10\n    drop_size   = 0.05\n\n    #bri, contrast\n    sat  = [0.7, 1.3]\n    cont = [0.8, 1.2]\n    bri  =  0.15\n    hue  = 0.05\n\n    # TEST TIME CFG.augmentATION STEPS\n    tta = 1\n    \n    tab_cols    = ['Subject Focus', 'Eyes', 'Face', 'Near', 'Action', 'Accessory',\n                   'Group', 'Collage', 'Human', 'Occlusion', 'Info', 'Blur']\n    target_col  = ['Pawpularity']","defbece4":"def seeding(SEED):\n    np.random.seed(SEED)\n    random.seed(SEED)\n    os.environ['PYTHONHASHSEED'] = str(SEED)\n    os.environ['TF_CUDNN_DETERMINISTIC'] = str(SEED)\n    tf.random.set_seed(SEED)\n    print('seeding done!!!')\nseeding(CFG.seed)","2619d638":"if CFG.device == \"TPU\":\n    print(\"connecting to TPU...\")\n    try:\n        tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n        print('Running on TPU ', tpu.master())\n    except ValueError:\n        print(\"Could not connect to TPU\")\n        tpu = None\n\n    if tpu:\n        try:\n            print(\"initializing  TPU ...\")\n            tf.config.experimental_connect_to_cluster(tpu)\n            tf.tpu.experimental.initialize_tpu_system(tpu)\n            strategy = tf.distribute.experimental.TPUStrategy(tpu)\n            print(\"TPU initialized\")\n        except _:\n            print(\"failed to initialize TPU\")\n    else:\n        CFG.device = \"GPU\"\n\nif CFG.device != \"TPU\":\n    print(\"Using default strategy for CPU and single GPU\")\n    strategy = tf.distribute.get_strategy()\n\nif CFG.device == \"GPU\":\n    print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))\n    \n\nAUTO     = tf.data.experimental.AUTOTUNE\nREPLICAS = strategy.num_replicas_in_sync\nprint(f'REPLICAS: {REPLICAS}')","e372d4e4":"BASE_PATH = '\/kaggle\/input\/petfinder-pawpularity-score'\nGCS_PATH  = KaggleDatasets().get_gcs_path('petfinder-pawpularity-score')","b86b3cd2":"def get_imgsize(row):\n    width, height = imagesize.get(row['image_path'].replace(GCS_PATH, BASE_PATH))\n    row['width']  = width\n    row['height'] = height\n    return row","05021094":"# Train Data\ndf = pd.read_csv('..\/input\/petfinder-pawpularity-score\/train.csv')\ndf['image_path'] = GCS_PATH + '\/train\/' + df.Id + '.jpg'\ntqdm.pandas(desc='train')\ndf = df.progress_apply(get_imgsize, axis=1)\ndisplay(df.head(2))\n\n# Test Data\ntest_df  = pd.read_csv('..\/input\/petfinder-pawpularity-score\/test.csv')\ntest_df['image_path'] = GCS_PATH + '\/test\/' + test_df.Id + '.jpg'\ntqdm.pandas(desc='test')\ntest_df = test_df.progress_apply(get_imgsize, axis=1)\n\ndisplay(test_df.head(2))","063d2567":"print('train_files:',df.shape[0])\nprint('test_files:',test_df.shape[0])","4a6913c2":"from pandas_profiling import ProfileReport\ntrain_profile = ProfileReport(df, title=\"Train Data\")\ntest_profile  = ProfileReport(test_df, title=\"Test Data\")","47b6493f":"display(train_profile)","fefe7e3d":"display(test_profile)","8470995f":"num_bins = int(np.floor(1 + np.log2(len(df))))\ndf[\"bins\"] = pd.cut(df[CFG.target_col].values.reshape(-1), bins=num_bins, labels=False)\n\nskf = StratifiedKFold(n_splits=CFG.folds, shuffle=True, random_state=CFG.seed)\nfor fold, (train_idx, val_idx) in enumerate(skf.split(df, df[\"bins\"])):\n    df.loc[val_idx, 'fold'] = fold\ndisplay(df.groupby(['fold', \"bins\"]).size())","cf842146":"def get_mat(shear, height_zoom, width_zoom, height_shift, width_shift):\n    # returns 3x3 transformmatrix which transforms indicies\n        \n    # CONVERT DEGREES TO RADIANS\n    #rotation = math.pi * rotation \/ 180.\n    shear    = math.pi * shear    \/ 180.\n\n    def get_3x3_mat(lst):\n        return tf.reshape(tf.concat([lst],axis=0), [3,3])\n    \n    # ROTATION MATRIX\n#     c1   = tf.math.cos(rotation)\n#     s1   = tf.math.sin(rotation)\n    one  = tf.constant([1],dtype='float32')\n    zero = tf.constant([0],dtype='float32')\n    \n#     rotation_matrix = get_3x3_mat([c1,   s1,   zero, \n#                                    -s1,  c1,   zero, \n#                                    zero, zero, one])    \n    # SHEAR MATRIX\n    c2 = tf.math.cos(shear)\n    s2 = tf.math.sin(shear)    \n    \n    shear_matrix = get_3x3_mat([one,  s2,   zero, \n                               zero, c2,   zero, \n                                zero, zero, one])        \n    # ZOOM MATRIX\n    zoom_matrix = get_3x3_mat([one\/height_zoom, zero,           zero, \n                               zero,            one\/width_zoom, zero, \n                               zero,            zero,           one])    \n    # SHIFT MATRIX\n    shift_matrix = get_3x3_mat([one,  zero, height_shift, \n                                zero, one,  width_shift, \n                                zero, zero, one])\n    \n\n    return  K.dot(shear_matrix,K.dot(zoom_matrix, shift_matrix)) #K.dot(K.dot(rotation_matrix, shear_matrix), K.dot(zoom_matrix, shift_matrix))                  \n\ndef transform(image, DIM=CFG.img_size):#[rot,shr,h_zoom,w_zoom,h_shift,w_shift]):\n    if DIM[0]!=DIM[1]:\n        pad = (DIM[0]-DIM[1])\/\/2\n        image = tf.pad(image, [[0, 0], [pad, pad+1],[0, 0]])\n        \n    NEW_DIM = DIM[0]\n    \n    rot = CFG.rot * tf.random.normal([1], dtype='float32')\n    shr = CFG.shr * tf.random.normal([1], dtype='float32') \n    h_zoom = 1.0 + tf.random.normal([1], dtype='float32') \/ CFG.hzoom\n    w_zoom = 1.0 + tf.random.normal([1], dtype='float32') \/ CFG.wzoom\n    h_shift = CFG.hshift * tf.random.normal([1], dtype='float32') \n    w_shift = CFG.wshift * tf.random.normal([1], dtype='float32') \n    \n    transformation_matrix=tf.linalg.inv(get_mat(shr,h_zoom,w_zoom,h_shift,w_shift))\n    \n    flat_tensor=tfa.image.transform_ops.matrices_to_flat_transforms(transformation_matrix)\n    \n    image=tfa.image.transform(image,flat_tensor, fill_mode=CFG.fill_mode)\n    \n    rotation = math.pi * rot \/ 180.\n    \n    image=tfa.image.rotate(image,-rotation, fill_mode=CFG.fill_mode)\n    \n    if DIM[0]!=DIM[1]:\n        image=tf.reshape(image, [NEW_DIM, NEW_DIM,3])\n        image = image[:, pad:DIM[1]+pad,:]\n    image = tf.reshape(image, [*DIM, 3])    \n    return image\n\ndef dropout(image,DIM=CFG.img_size, PROBABILITY = 0.6, CT = 5, SZ = 0.1):\n    # input image - is one image of size [dim,dim,3] not a batch of [b,dim,dim,3]\n    # output - image with CT squares of side size SZ*DIM removed\n    \n    # DO DROPOUT WITH PROBABILITY DEFINED ABOVE\n    P = tf.cast( tf.random.uniform([],0,1)<PROBABILITY, tf.int32)\n    if (P==0)|(CT==0)|(SZ==0): \n        return image\n    \n    for k in range(CT):\n        # CHOOSE RANDOM LOCATION\n        x = tf.cast( tf.random.uniform([],0,DIM[1]),tf.int32)\n        y = tf.cast( tf.random.uniform([],0,DIM[0]),tf.int32)\n        # COMPUTE SQUARE \n        WIDTH = tf.cast( SZ*min(DIM),tf.int32) * P\n        ya = tf.math.maximum(0,y-WIDTH\/\/2)\n        yb = tf.math.minimum(DIM[0],y+WIDTH\/\/2)\n        xa = tf.math.maximum(0,x-WIDTH\/\/2)\n        xb = tf.math.minimum(DIM[1],x+WIDTH\/\/2)\n        # DROPOUT IMAGE\n        one = image[ya:yb,0:xa,:]\n        two = tf.zeros([yb-ya,xb-xa,3], dtype = image.dtype) \n        three = image[ya:yb,xb:DIM[1],:]\n        middle = tf.concat([one,two,three],axis=1)\n        image = tf.concat([image[0:ya,:,:],middle,image[yb:DIM[0],:,:]],axis=0)\n        image = tf.reshape(image,[*DIM,3])\n\n#     image = tf.reshape(image,[*DIM,3])\n    return image","99ef6bd1":"def build_decoder(with_labels=True, target_size=CFG.img_size, ext='jpg'):\n    def decode(path):\n        file_bytes = tf.io.read_file(path)\n        if ext == 'png':\n            img = tf.image.decode_png(file_bytes, channels=3)\n        elif ext in ['jpg', 'jpeg']:\n            img = tf.image.decode_jpeg(file_bytes, channels=3)\n        else:\n            raise ValueError(\"Image extension not supported\")\n\n        img = tf.image.resize(img, target_size)\n        img = tf.cast(img, tf.float32) \/ 255.0\n        img = tf.reshape(img, [*target_size, 3])\n\n        return img\n    \n    def decode_with_labels(path, label):\n        return decode(path), tf.cast(label, tf.float32)\n    \n    return decode_with_labels if with_labels else decode\n\n\ndef build_augmenter(with_labels=True, dim=CFG.img_size):\n    def augment(img, dim=dim):\n        img = transform(img,DIM=dim) if CFG.transform else img\n        img = tf.image.random_flip_left_right(img) if CFG.hflip else img\n        img = tf.image.random_flip_up_down(img) if CFG.vflip else img\n        img = tf.image.random_hue(img, CFG.hue)\n        img = tf.image.random_saturation(img, CFG.sat[0], CFG.sat[1])\n        img = tf.image.random_contrast(img, CFG.cont[0], CFG.cont[1])\n        img = tf.image.random_brightness(img, CFG.bri)\n        img = dropout(img, DIM=dim, PROBABILITY = CFG.drop_prob, CT = CFG.drop_cnt, SZ = CFG.drop_size)\n        img = tf.clip_by_value(img, 0, 1)  if CFG.clip else img         \n        img = tf.reshape(img, [*dim, 3])\n        return img\n    \n    def augment_with_labels(img, label):    \n        return augment(img), label\n    \n    return augment_with_labels if with_labels else augment\n\n\ndef build_dataset(paths, labels=None, batch_size=32, cache=True,\n                  decode_fn=None, augment_fn=None,\n                  augment=True, repeat=True, shuffle=1024, \n                  cache_dir=\"\", drop_remainder=False):\n    if cache_dir != \"\" and cache is True:\n        os.makedirs(cache_dir, exist_ok=True)\n    \n    if decode_fn is None:\n        decode_fn = build_decoder(labels is not None)\n    \n    if augment_fn is None:\n        augment_fn = build_augmenter(labels is not None)\n    \n    AUTO = tf.data.experimental.AUTOTUNE\n    slices = paths if labels is None else (paths, labels)\n    \n    ds = tf.data.Dataset.from_tensor_slices(slices)\n    ds = ds.map(decode_fn, num_parallel_calls=AUTO)\n    ds = ds.cache(cache_dir) if cache else ds\n    ds = ds.repeat() if repeat else ds\n    if shuffle: \n        ds = ds.shuffle(shuffle, seed=CFG.seed)\n        opt = tf.data.Options()\n        opt.experimental_deterministic = False\n        ds = ds.with_options(opt)\n    ds = ds.map(augment_fn, num_parallel_calls=AUTO) if augment else ds\n    ds = ds.batch(batch_size, drop_remainder=drop_remainder)\n    ds = ds.prefetch(AUTO)\n    return ds","69a83aff":"def display_batch(batch, size=2):\n    imgs, tars = batch\n    plt.figure(figsize=(size*5, 5))\n    for img_idx in range(size):\n        plt.subplot(1, size, img_idx+1)\n        plt.title(f'{CFG.target_col[0]}: {tars[img_idx].numpy()[0]}', fontsize=15)\n        plt.imshow(imgs[img_idx,:, :, :])\n        plt.xticks([])\n        plt.yticks([])\n    plt.tight_layout()\n    plt.show() ","1330ae1a":"fold = 0\nfold_df = df.query('fold==@fold')[:1000]\npaths  = fold_df.image_path.tolist()\nlabels = fold_df[CFG.target_col].values\nds = build_dataset(paths, labels, cache=False, batch_size=CFG.batch_size*REPLICAS,\n                   repeat=True, shuffle=True, augment=True)\nds = ds.unbatch().batch(20)\nbatch = next(iter(ds))\ndisplay_batch(batch, 5);","053bf4fb":"def RMSE(y_true, y_pred):\n    loss = tf.math.sqrt(tf.math.reduce_mean(tf.math.square(tf.subtract(y_true, y_pred))))\n    return loss","cf665456":"import efficientnet.tfkeras as efn\n\nname2effnet = {\n    'efficientnet_b0': efn.EfficientNetB0,\n    'efficientnet_b1': efn.EfficientNetB1,\n    'efficientnet_b2': efn.EfficientNetB2,\n    'efficientnet_b3': efn.EfficientNetB3,\n    'efficientnet_b4': efn.EfficientNetB4,\n    'efficientnet_b5': efn.EfficientNetB5,\n    'efficientnet_b6': efn.EfficientNetB6,\n    'efficientnet_b7': efn.EfficientNetB7,\n}\n\ndef build_model(model_name=CFG.model_name, DIM=CFG.img_size[0], compile_model=True, include_top=False):       \n    base = name2effnet[model_name](input_shape=(DIM, DIM, 3),\n                                  include_top=include_top,\n                                   weights='imagenet',\n                                  )\n    inp = base.inputs\n    out = base.output\n    out = tf.keras.layers.GlobalAveragePooling2D()(out)\n    out = tf.keras.layers.Dense(64,activation='selu')(out)\n    out = tf.keras.layers.Dense(1)(out)\n    model = tf.keras.Model(inputs=inp, outputs=out)\n    if compile_model:\n        #optimizer\n        opt = tf.keras.optimizers.Adam(learning_rate=0.001)\n        #loss\n        loss = RMSE\n        #metric\n        rmse = tf.keras.metrics.RootMeanSquaredError(name='rmse')\n        model.compile(optimizer=opt,\n                      loss=loss,\n                      metrics=[rmse])\n    return model","ad72d2d7":"tmp = build_model(CFG.model_name, DIM=CFG.img_size[0], compile_model=True)","67878b86":"def get_lr_callback(batch_size=8, plot=False):\n    lr_start   = 0.000005\n    lr_max     = 0.00000125 * REPLICAS * batch_size\n    lr_min     = 0.000001\n    lr_ramp_ep = 5\n    lr_sus_ep  = 0\n    lr_decay   = 0.8\n   \n    def lrfn(epoch):\n        if epoch < lr_ramp_ep:\n            lr = (lr_max - lr_start) \/ lr_ramp_ep * epoch + lr_start\n            \n        elif epoch < lr_ramp_ep + lr_sus_ep:\n            lr = lr_max\n            \n        elif CFG.scheduler=='exp':\n            lr = (lr_max - lr_min) * lr_decay**(epoch - lr_ramp_ep - lr_sus_ep) + lr_min\n            \n        elif CFG.scheduler=='cosine':\n            decay_total_epochs = CFG.epochs - lr_ramp_ep - lr_sus_ep + 3\n            decay_epoch_index = epoch - lr_ramp_ep - lr_sus_ep\n            phase = math.pi * decay_epoch_index \/ decay_total_epochs\n            cosine_decay = 0.5 * (1 + math.cos(phase))\n            lr = (lr_max - lr_min) * cosine_decay + lr_min\n        return lr\n    if plot:\n        plt.figure(figsize=(10,5))\n        plt.plot(np.arange(CFG.epochs), [lrfn(epoch) for epoch in np.arange(CFG.epochs)], marker='o')\n        plt.xlabel('epoch'); plt.ylabel('learnig rate')\n        plt.title('Learning Rate Scheduler')\n        plt.show()\n\n    lr_callback = tf.keras.callbacks.LearningRateScheduler(lrfn, verbose=False)\n    return lr_callback\n\n_=get_lr_callback(CFG.batch_size, plot=True )","58395a02":"import matplotlib.cm as cm, cv2\nfrom tensorflow import keras\n\ndef gen_gradcam_heatmap(img, model, last_conv_layer_name='top_conv', pred_index=0):\n    # First, we create a model that maps the input image to the activations\n    # of the last conv layer as well as the output predictions\n    grad_model = tf.keras.models.Model(\n        [model.inputs], [model.get_layer('top_conv').output, model.output]\n    )\n\n    # Then, we compute the gradient of the top predicted class for our input image\n    # with respect to the activations of the last conv layer\n    with tf.GradientTape() as tape:\n        last_conv_layer_output, preds = grad_model(img)\n        if pred_index is None:\n            pred_index = tf.argmax(preds[0])\n        class_channel = preds[:, pred_index]\n\n    # This is the gradient of the output neuron (top predicted or chosen)\n    # with regard to the output feature map of the last conv layer\n    grads = tape.gradient(class_channel, last_conv_layer_output)\n\n    # This is a vector where each entry is the mean intensity of the gradient\n    # over a specific feature map channel\n    pooled_grads = tf.reduce_mean(grads, axis=(0, 1, 2))\n\n    # We multiply each channel in the feature map array\n    # by \"how important this channel is\" with regard to the top predicted class\n    # then sum all the channels to obtain the heatmap class activation\n    last_conv_layer_output = last_conv_layer_output[0]\n    heatmap = last_conv_layer_output @ pooled_grads[..., tf.newaxis]\n    heatmap = tf.squeeze(heatmap)\n\n    # For visualization purpose, we will also normalize the heatmap between 0 & 1\n    heatmap = tf.maximum(heatmap, 0) \/ tf.math.reduce_max(heatmap)\n    return heatmap.numpy()\n\ndef get_gradcam(img, model,  alpha=0.4, show=False):\n\n    heatmap = gen_gradcam_heatmap(img, model, last_conv_layer_name='top_conv', pred_index=0)\n    img     = img[0]\n    # Rescale heatmap to a range 0-255\n    heatmap = np.uint8(255 * heatmap)\n\n    # Use jet colormap to colorize heatmap\n    jet = cm.get_cmap(\"jet\")\n\n    # Use RGB values of the colormap\n    jet_colors  = jet(np.arange(256))[:, :3]\n    jet_heatmap = jet_colors[heatmap]\n\n    # Create an image with RGB colorized heatmap\n    jet_heatmap = cv2.resize(jet_heatmap, dsize=(img.shape[1], img.shape[0]))\n\n    # Superimpose the heatmap on original image\n    superimposed_img = jet_heatmap * alpha + img\n    superimposed_img = keras.preprocessing.image.array_to_img(superimposed_img)\n#     superimposed_img = np.uint8(superimposed_img*255.0)\n\n    # Display Grad CAM\n    if show:\n        plt.imshow(superimposed_img)\n        \n    return superimposed_img","8bfd3078":"if CFG.wandb:\n    def wandb_init(fold):\n        config = {k:v for k,v in dict(vars(CFG)).items() if '__' not in k}\n        yaml.dump(config, open(f'\/kaggle\/working\/config fold-{fold}.yaml', 'w'),)\n        config = yaml.load(open(f'\/kaggle\/working\/config fold-{fold}.yaml', 'r'), Loader=yaml.FullLoader)\n        run    = wandb.init(project=\"petfinder-public\",\n                   name=f\"fold-{fold}|dim-{CFG.img_size[0]}|model-{CFG.model_name}\",\n                   config=config,\n                   anonymous=anonymous,\n                   group=CFG.exp_name\n                        )\n        return run\n    \ndef log_wandb(fold):\n    \"log best result and grad-cam for error analysis\"\n    \n    valid_df = df.loc[df.fold==fold].copy()\n    valid_df['pred'] = oof_pred[fold].reshape(-1)\n    valid_df['diff'] =  abs(valid_df.Pawpularity - valid_df.pred)\n    valid_df    = valid_df[valid_df.fold == fold].reset_index(drop=True)\n    vali_df     = valid_df.sort_values(by='diff', ascending=False)\n    \n    noimg_cols  = ['Id', 'fold', 'Subject Focus','Eyes','Face','Near','Action','Accessory','Group',\n                    'Collage','Human','Occlusion','Info','Blur',\n                   'Pawpularity', 'pred', 'diff']\n    # select top and worst 10 cases\n    gradcam_df  = pd.concat((valid_df.head(10), valid_df.tail(10)), axis=0)\n    gradcam_ds  = build_dataset(gradcam_df.image_path, labels=None, cache=False, batch_size=1,\n                   repeat=False, shuffle=False, augment=False)\n    data = []\n    for idx, img in enumerate(gradcam_ds):\n        gradcam = get_gradcam(img, model)\n        row = gradcam_df[noimg_cols].iloc[idx].tolist()\n        data+=[[*row, wandb.Image(img.numpy()[0]), wandb.Image(gradcam)]]\n    wandb_table = wandb.Table(data=data, columns=[*noimg_cols,'image', 'gradcam'])\n    wandb.log({'best_rmse':oof_val[-1], \n               'best_rmse_tta':rmse,\n               'best_epoch':np.argmin(history.history['val_rmse']),\n               'viz_table':wandb_table})","d26fd6c1":"oof_pred = []; oof_tar = []; oof_val = []; oof_ids = []; oof_folds = []\npreds = np.zeros((test_df.shape[0],1))\n\nfor fold in np.arange(CFG.folds):\n    if fold not in CFG.selected_folds:\n        continue\n    if CFG.wandb:\n        run = wandb_init(fold)\n        WandbCallback = wandb.keras.WandbCallback(save_model=False)\n    if CFG.device=='TPU':\n        if tpu: tf.tpu.experimental.initialize_tpu_system(tpu)\n            \n    # TRAIN AND VALID DATAFRAME\n    train_df = df.query(\"fold!=@fold\")\n    valid_df = df.query(\"fold==@fold\")\n    \n    # CREATE TRAIN AND VALIDATION SUBSETS\n    train_paths = train_df.image_path.values; train_labels = train_df[CFG.target_col].values.astype(np.float32)\n    valid_paths = valid_df.image_path.values;   valid_labels   = valid_df[CFG.target_col].values.astype(np.float32)\n    test_paths  = test_df.image_path.values\n    \n    # SHUFFLE IMAGE AND LABELS\n    index = np.arange(len(train_paths))\n    np.random.shuffle(index)\n    train_paths  = train_paths[index]\n    train_labels = train_labels[index]\n    \n    if CFG.debug:\n        train_paths = train_paths[:2000]; train_labels = train_labels[:2000]\n        valid_paths = valid_paths[:1000]; valid_labels = valid_labels[:1000]\n    \n    print('#'*25); print('#### FOLD',fold)\n    print('#### IMAGE_SIZE: (%i, %i) | MODEL_NAME: %s | BATCH_SIZE: %i'%\n          (CFG.img_size[0],CFG.img_size[1],CFG.model_name,CFG.batch_size*REPLICAS))\n    train_images = len(train_paths)\n    val_images   = len(valid_paths)\n    if CFG.wandb:\n        wandb.log({'num_train':train_images,\n                   'num_valid':val_images})\n    print('#### NUM_TRAIN %i | NUM_VALID: %i'%(train_images, val_images))\n    \n    # BUILD MODEL\n    K.clear_session()\n    with strategy.scope():\n        model = build_model(CFG.model_name, DIM=CFG.img_size[0], compile_model=True)\n\n    # DATASET\n    train_ds = build_dataset(train_paths, train_labels, cache=True, batch_size=CFG.batch_size*REPLICAS,\n                   repeat=True, shuffle=True, augment=CFG.augment)\n    val_ds   = build_dataset(valid_paths, valid_labels, cache=True, batch_size=CFG.batch_size*REPLICAS,\n                   repeat=False, shuffle=False, augment=False)\n    \n    print('#'*25)   \n    # SAVE BEST MODEL EACH FOLD\n    sv = tf.keras.callbacks.ModelCheckpoint(\n        'fold-%i.h5'%fold, monitor='val_rmse', verbose=CFG.verbose, save_best_only=True,\n        save_weights_only=False, mode='min', save_freq='epoch')\n    callbacks = [sv,get_lr_callback(CFG.batch_size)]\n    if CFG.wandb:\n        callbacks.append(WandbCallback)\n    # TRAIN\n    print('Training...')\n    history = model.fit(\n        train_ds, \n        epochs=CFG.epochs if not CFG.debug else 2, \n        callbacks = callbacks, \n        steps_per_epoch=len(train_paths)\/CFG.batch_size\/\/REPLICAS,\n        validation_data=val_ds, \n        #class_weight = {0:1,1:2},\n        verbose=CFG.verbose\n    )\n    \n    # Loading best model for inference\n    print('Loading best model...')\n    model.load_weights('fold-%i.h5'%fold)  \n    \n    # PREDICT OOF USING TTA\n    print('Predicting OOF with TTA...')\n    ds_valid = build_dataset(valid_paths, labels=None, cache=False, batch_size=CFG.batch_size*REPLICAS*2,\n                   repeat=True, shuffle=False, augment=True if CFG.tta>1 else False)\n    ct_valid = len(valid_paths); STEPS = CFG.tta * ct_valid\/CFG.batch_size\/2\/REPLICAS\n    pred = model.predict(ds_valid,steps=STEPS,verbose=CFG.verbose)[:CFG.tta*ct_valid,] \n    oof_pred.append(np.mean(pred.reshape((ct_valid,-1,CFG.tta),order='F'),axis=-1) )                 \n    \n    # GET OOF TARGETS AND idS\n    oof_tar.append(valid_df[CFG.target_col].values[:(1000 if CFG.debug else len(valid_df))])\n    oof_folds.append(np.ones_like(oof_tar[-1],dtype='int8')*fold )\n    oof_ids.append(valid_df.Id.values)\n    \n    # PREDICT TEST USING TTA\n    print('Predicting Test with TTA...')\n    ds_test = build_dataset(test_paths, labels=None, cache=True, \n                    batch_size=(CFG.batch_size*2 if len(test_df)>8 else 1)*REPLICAS,\n                   repeat=True, shuffle=False, augment=True if CFG.tta>1 else False)\n    ct_test = len(test_paths); STEPS = 1 if len(test_df)<=8 else (CFG.tta * ct_test\/CFG.batch_size\/2\/REPLICAS)\n    pred = model.predict(ds_test,steps=STEPS,verbose=CFG.verbose)[:CFG.tta*ct_test,] \n    preds[:ct_test, :] += np.mean(pred.reshape((ct_test,-1,CFG.tta),order='F'),axis=-1) \/ CFG.folds # not meaningful for DIBUG = True\n    \n    # REPORT RESULTS\n    y_true =oof_tar[-1]; y_pred = oof_pred[-1]\n    rmse   =RMSE(y_true.astype(np.float32),y_pred).numpy()\n    oof_val.append(np.min( history.history['val_rmse'] ))\n    print('#### FOLD %i OOF RMSE without TTA = %.3f, with TTA = %.3f'%(fold,oof_val[-1],rmse))\n    \n    if CFG.wandb:\n        log_wandb(fold) # log result to wandb\n        wandb.run.finish() # finish the run\n        display(ipd.IFrame(run.url, width=1080, height=720)) # show wandb dashboard","260d56f9":"# COMPUTE OVERALL OOF RMSE\noof = np.concatenate(oof_pred); true = np.concatenate(oof_tar);\nids = np.concatenate(oof_ids); folds = np.concatenate(oof_folds)\nrmse = RMSE(true.astype(np.float32),oof)\nprint('Overall OOF RMSE with TTA = %.3f'%rmse)","6dbf1608":"# SAVE OOF TO DISK\ncolumns = ['Id', 'fold', 'true', 'pred']\ndf_oof = pd.DataFrame(np.concatenate([ids[:,None], folds[:, 0:1], true, oof], axis=1), columns=columns)\ndf_oof.to_csv('oof.csv',index=False)\ndf_oof.head()","492f3b37":"import seaborn as sns\nsns.set(style='dark')\n\nplt.figure(figsize=(10*2,6))\n\nplt.subplot(1, 2, 1)\nsns.kdeplot(x=train_df[CFG.target_col[0]], color='b',shade=True);\nsns.kdeplot(x=df_oof.pred.values, color='r',shade=True);\nplt.grid('ON')\nplt.xlabel(CFG.target_col[0]);plt.ylabel('freq');plt.title('KDE')\nplt.legend(['train', 'oof'])\n\nplt.subplot(1, 2, 2)\nsns.histplot(x=train_df[CFG.target_col[0]], color='b');\nsns.histplot(x=df_oof.pred.values, color='r');\nplt.grid('ON')\nplt.xlabel(CFG.target_col[0]);plt.ylabel('freq');plt.title('Histogram')\nplt.legend(['train', 'oof'])\n\nplt.tight_layout()\nplt.show()","7c475c52":"pred_df = pd.DataFrame({'Id':test_df.Id,\n                        'Pawpularity':preds.reshape(-1)})\nsub_df = pd.read_csv('\/kaggle\/input\/petfinder-pawpularity-score\/sample_submission.csv')\ndel sub_df['Pawpularity']\nsub_df = sub_df.merge(pred_df, on='Id', how='left')\nsub_df.to_csv('submission.csv',index=False)\nsub_df.head(2)","3881ed38":"!rm -r \/kaggle\/working\/wandb","8268d5db":"## Train-Test Ditribution","dd9ed69d":"# Calculate OOF Score","0bc798af":"## Visualization\n* Check if augmentation is working properly or not.","3f1a737b":"# TPU Configs","90896829":"# Install Libraries","3a27834a":"# Data Split\n* Data is splited using **Pawpularity** distrubtion.\n","40af5077":"# Light EDA\nIf you're too lazy to do **EDA** by yourself. Then definitely this library is for you. You can use **Pandas-Profiling** to do bunch of **EDA** with vey few lines of code. \ud83d\ude09","47b04242":"# Overview:\n\n## Augmentations:\n* Random - Horizontal Flip\n* Random - Brightness, Contrast, Hue, Saturation\n* Coarse Dropout\n\n<img src=\"https:\/\/i.ibb.co\/XC4dBJs\/results-39-0.png\" alt=\"results-39-0\" border=\"0\">\n\n## WandB Integration:\n* You can track your training using **wandb**\n* It's very easy to compare model's performance using **wandb**.\n\n<img src=\"https:\/\/i.ibb.co\/KKxF8Fj\/wandb-result.png\" alt=\"wandb-result\" border=\"0\">\n\n## Grad-CAM:\n* You can use **Grad-CAM** to interpret the results\n\n<img src=\"https:\/\/i.ibb.co\/WPwNcc9\/wandb-grad-cam.png\" alt=\"wandb-grad-cam\" border=\"0\">\n\n## Train Vs OOF Distribution:\n\n<img src=\"https:\/\/i.ibb.co\/Yy4NXvW\/results-58-0.png\" alt=\"results-58-0\" border=\"0\">","8ee75e12":"# Submission\nThis notebook can't be used for submission as this noteboook uses **TPU** which requires `internet access`.","169a0072":"# **Wandb** Logger\nLog:\n* Best Score\n* Grad-CAM","e3765dfa":"# Import Libraries","40825f76":"# [PetFinder.my - Pawpularity Contest](https:\/\/www.kaggle.com\/c\/petfinder-pawpularity-score)\n> Predict the popularity of shelter pet photos\n\n![](https:\/\/storage.googleapis.com\/kaggle-competitions\/kaggle\/25383\/logos\/header.png)","157a76fa":"# Version Check","24043ef7":"# Build Model\n","6edc446b":"<img src=\"https:\/\/camo.githubusercontent.com\/dd842f7b0be57140e68b2ab9cb007992acd131c48284eaf6b1aca758bfea358b\/68747470733a2f2f692e696d6775722e636f6d2f52557469567a482e706e67\" width=\"400\" alt=\"Weights & Biases\" \/>\n\n<span style=\"color: #000508; font-family: Segoe UI; font-size: 1.2em; font-weight: 300;\"> Weights & Biases (W&B) is MLOps platform for tracking our experiemnts. We can use it to Build better models faster with experiment tracking, dataset versioning, and model management<\/span>\n\n<span style=\"color: #000508; font-family: Segoe UI; font-size: 1.2em; font-weight: 300;\">Some of the cool features of **W&B**:<\/span>\n\n* <span style=\"color: #000508; font-family: Segoe UI; font-size: 1.2em; font-weight: 300;\">Track, compare, and visualize ML experiments<br><\/span>\n* <span style=\"color: #000508; font-family: Segoe UI; font-size: 1.2em; font-weight: 300;\">Get live metrics, terminal logs, and system stats streamed to the centralized dashboard.<br><\/span>\n* <span style=\"color: #000508; font-family: Segoe UI; font-size: 1.2em; font-weight: 300;\">Explain how your model works, show graphs of how model versions improved, discuss bugs, and demonstrate progress towards milestones.<br><\/span>","c7c6b93a":"# Meta Data","70ae2180":"# Notebooks:\n* train: [[TF] PetFinder: Image [TPU][Train] \ud83d\udc36](https:\/\/www.kaggle.com\/awsaf49\/tf-petfinder-image-tpu-train)\n* infer: [[TF] PetFinder: Image [TPU][Infer] \ud83d\udc36](https:\/\/www.kaggle.com\/awsaf49\/tf-petfinder-image-tpu-infer)\n","f86a81f1":"## Data Pipeline\n* Reads the raw file and then decodes it to tf.Tensor\n* Resizes the image in desired size\n* Chages the datatype to **float32**\n* Caches the Data for boosting up the speed.\n* Uses Augmentations to reduce overfitting and make model more robust.\n* Finally, splits the data into batches.\n","9f12256c":"# GCS Path for TPU\n* TPU requires **GCS** path. Luckily Kaggle Provides that for us :)","185fd888":"## Test","10e401b2":"# Pawpularity Distribution of OOF & Train \nCheck **Pawpularity** distribution of `train` and `oof`. ","27cd4382":"# Train Model\n* Cross-Validation: 5 fold\n* **WandB** dashboard is shown end of the each fold. So we don't need to plot anything. We can select best model from here.","e49c3c5c":"## Model Check","e6c2fce1":"# Loss Function\nLoss Function for this competition is **RMSE: Root Mean Squared Error**\n$$\\textrm{RMSE} = \\sqrt{\\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2}$$\n\nwhere $\\hat{y}_i$ is the **predicted** value and $y_i$ is the **original** value for each instance $i$. Check out this blog, [What does RMSE really mean?](https:\/\/towardsdatascience.com\/what-does-rmse-really-mean-806b65f2e48e) for more details","cef696c1":"## Train","c16324c6":"# Set Seed for Reproducibility","4353e393":"# Learning-Rate Scheduler","fe0ff487":"# Remove Files","e6d3b1ee":"# Content:\n* Install Libraries.\n* Import Libraries.\n* Libraries Version Check\n* Wandb\n* Configuration.\n* Set Seed for Reproducibility.\n* TPU Configs.\n* GCS Path for TPU.\n* Meta Data.\n* Train-Test Distrubution\n* EDA\n    * Train.\n    * Test.\n* Data Split.\n* Data Augmentation.\n* Data Pipeline.\n* Visualization.\n* Loss Function.\n* Build Model.\n* Learning-Rate Scheduler.\n* Grad-CAM Helper\n* Wandb Logger\n* Train Model\n* Calculate OOF Scorej\n* Pawpularity Distrubtion of Train & OOF\n\n\n\n","7d16222f":"# Data Augmentation\nUsed simple augmentations, some of them may hurt the model.\n* RandomFlip (Left-Right)\n* No Rotation\n* RandomBrightness\n* RndomContrast\n* Shear\n* Zoom\n* Coarsee Dropout\/Cutout","0baadf13":"# Idea:\n* Basic idea of this notebook is to use only **Image** Feature.\n* Tabular data will be merged on later Notebooks. \n* **Wandb** is integrated hence we can use this notebook to track which experiemnt is peforming better and also do error analysis using **Grad-CAM** at the end.\n","1a13557d":"# Grad-CAM\nGradient-weighted Class Activation Mapping (Grad-CAM), uses the class-specific gradient information flowing into the final convolutional layer of a CNN to produce a coarse localization map of the important regions in the image. Grad-CAM is a strict generalization of the Class Activation Mapping. Grad-CAM provides visual explanations to better understand image classification problems.\n\n<img src=\"http:\/\/gradcam.cloudcv.org\/static\/images\/network.png\" width=\"800\">","a231698c":"# Configuration","bae59788":"# Wandb"}}