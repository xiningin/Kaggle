{"cell_type":{"682a4e76":"code","d880f674":"code","3f7ea393":"code","ecb58e68":"code","d4b27495":"code","b04487a6":"code","e6e4fe7f":"code","31d19399":"code","2f4ce80a":"code","6d978c15":"code","ba588f11":"code","db865535":"code","cc24b1d5":"code","c0256199":"code","124cafb5":"code","4a0c2dd1":"code","8f359c87":"code","f435a091":"code","18747454":"code","49b6ce00":"code","544bdf16":"code","825c25b0":"markdown","ec55781d":"markdown","22c6da7c":"markdown","0277a2d7":"markdown","52f9b827":"markdown","4eb50073":"markdown"},"source":{"682a4e76":"import os\nif not os.path.exists(\"..\/input\/train.csv\"):\n    os.symlink(\"..\/input\/home-data-for-ml-course\/train.csv\", \"..\/input\/train.csv\")  \n    os.symlink(\"..\/input\/home-data-for-ml-course\/test.csv\", \"..\/input\/test.csv\") \nprint(\"Setup Complete\")","d880f674":"import pandas as pd\nfrom sklearn.model_selection import train_test_split\n\n\nX_full = pd.read_csv('..\/input\/train.csv', index_col='Id')\nX_test_full = pd.read_csv('..\/input\/test.csv', index_col='Id')\n\n\n\nX = X_full.select_dtypes(exclude=['object'])\nX_test = X_test_full.select_dtypes(exclude=['object'])\n","3f7ea393":"\ncols_with_missing_value = [col for col in X.columns if X[col].isnull().any()]\nprint(cols_with_missing_value)\ncols_with_all = [col for col in X.columns if not X[col].isnull().any()]\nprint(cols_with_all)","ecb58e68":"from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n\n\ndef nan_predictor_train(col):\n    cols_with_all = [col for col in X.columns if not X[col].isnull().any()]\n    cols_used = cols_with_all[:]\n    cols_used.append(col)\n    data = X[cols_used]\n    print(f'before deleting nan values: {data.shape}')\n    data_train = data.dropna(axis=0)\n    print(f'after deleting nan values: {data_train.shape}')\n    target_train = data_train[col]\n    features_train = data_train.drop([col],axis=1)\n    \n    rf_model = RandomForestRegressor(n_estimators=500, random_state=0)\n    rf_model.fit(features_train, target_train)\n    gmb_model = GradientBoostingRegressor(n_estimators=500, random_state=0)\n    gmb_model.fit(features_train, target_train)\n    \n    features_test = data[cols_with_all]\n    pred1 = rf_model.predict(features_test)\n    pred2 = gmb_model.predict(features_test)\n    pred = (pred1 + pred2) \/ 2\n    X[col] = pred\n","d4b27495":"for col in cols_with_missing_value:\n    nan_predictor_train(col)","b04487a6":"cols_with_nan = [col for col in X_test.columns if X_test[col].isnull().any()]\nprint(cols_with_nan)\ncols_with_all = [col for col in X_test.columns if not X_test[col].isnull().any()]\nprint(cols_with_all)","e6e4fe7f":"from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n\n\ndef nan_predictor_test(col):\n    cols_with_all = [col for col in X_test.columns if not X_test[col].isnull().any()]\n    cols_used = cols_with_all[:]\n    cols_used.append(col)\n    data = X_test[cols_used]\n    print(f'before deleting nan values: {data.shape}')\n    data_train = data.dropna(axis=0)\n    print(f'after deleting nan values: {data_train.shape}')\n    target_train = data_train[col]\n    features_train = data_train.drop([col],axis=1)\n    \n    rf_model = RandomForestRegressor(n_estimators=500, random_state=0)\n    rf_model.fit(features_train, target_train)\n    gmb_model = GradientBoostingRegressor(n_estimators=500, random_state=0)\n    gmb_model.fit(features_train, target_train)\n    \n    features_test = data[cols_with_all]\n    pred1 = rf_model.predict(features_test)\n    pred2 = gmb_model.predict(features_test)\n    pred = (pred1 + pred2) \/ 2\n    X_test[col] = pred\n","31d19399":"for col in ['LotFrontage','MasVnrArea','BsmtFinSF1','BsmtFinSF2','BsmtUnfSF','TotalBsmtSF','BsmtFullBath','BsmtHalfBath','GarageYrBlt','GarageCars','GarageArea']:\n    nan_predictor_test(col)","2f4ce80a":"object_cols = [col for col in X_full.columns if X_full[col].dtype == 'object']","6d978c15":"object_col_with_nan = [col for col in object_cols if X_full[col].isnull().any()]\nobject_col_with_nan","ba588f11":"useful_object_col = list(set(object_cols) - set(object_col_with_nan))\nlen(useful_object_col)","db865535":"good_cols = [col for col in useful_object_col if set(X_full[col]).issubset(set(X_test_full[col]))]\ngood_cols","cc24b1d5":"data_object_train = X_full[good_cols]\ndata_object_train.head()","c0256199":"data_object_train = X_full[good_cols]\ndata_object_train.head()\ndata_object_test = X_test_full[good_cols]","124cafb5":"train_target = X.SalePrice\ntrain_features = X.drop(['SalePrice'], axis=1)","4a0c2dd1":"X_train, X_valid, y_train, y_valid = train_test_split(train_features, train_target, train_size=0.8, test_size=0.2,random_state=0)","8f359c87":"import numpy as np\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error\nfrom sklearn.ensemble import GradientBoostingRegressor","f435a091":"def final_pred(n):\n    rf_model = RandomForestRegressor(random_state=1, n_estimators=n)\n    rf_model.fit(X_train,y_train)\n    rf_pred = rf_model.predict(X_valid)\n\n    gbm_model = GradientBoostingRegressor(random_state=1, n_estimators=n)\n    gbm_model.fit(X_train,y_train)\n    gbm_pred = gbm_model.predict(X_valid)\n\n    pred = (rf_pred + gbm_pred)\/2\n    return pred","18747454":"for n in [500]:\n    pred = final_pred(n)\n    print(np.sqrt(mean_squared_error(y_valid,pred)))","49b6ce00":"rf_model = RandomForestRegressor(random_state=1, n_estimators=700)\nrf_model.fit(X_train,y_train)\nrf_pred = rf_model.predict(X_test)\n\ngbm_model = GradientBoostingRegressor(random_state=1, n_estimators=700)\ngbm_model.fit(X_train,y_train)\ngbm_pred = gbm_model.predict(X_test)\n\npred = (rf_pred + gbm_pred)\/2","544bdf16":"output = pd.DataFrame({'Id': X_test.index,\n                       'SalePrice': pred})\noutput.to_csv('submission.csv', index=False)","825c25b0":"### Using Random Forest and GBM for missing values prediction in columns","ec55781d":"### How ?\n\n- Select the columns that has missing vlaues\n- Seperate the rows without missing values as a training data and with missing values as test data\n- Train your model on the training data then use it to infer missing values in test data\n- In the last merge them all and finally use it to predict the final target","22c6da7c":"---\n\n\n\n\n*Have questions or comments? Visit the [Learn Discussion forum](https:\/\/www.kaggle.com\/learn-forum\/161289) to chat with other Learners.*","0277a2d7":"## Titanic: Predicting `NaN` values using Random Forest instead of imputing\n\n- This method of imputing is time taking and requires more effort than using simple imputers or dropping them, but it also works out of the box.","52f9b827":"### Remove the object columns that have missing values","4eb50073":"### using Random Forest and Gradient Boost model via stacking for final predictions"}}