{"cell_type":{"85bf0e1c":"code","fd8575fb":"code","16f6c76c":"code","dc36bdbb":"code","ea1227a4":"code","ce786d28":"code","50472902":"code","8c7f5fb4":"code","d1b5aecf":"code","f08093ec":"code","bfe37946":"code","12387473":"code","f9bc7557":"code","0d77488b":"code","95628ce6":"code","e707495c":"code","5c0834bd":"code","bd3e765d":"code","3a91e7cc":"code","68afea05":"code","0c3842c5":"code","3cbd007b":"code","f095d9d2":"code","34a7c022":"code","184782c9":"markdown","0e291ffd":"markdown","18c7df86":"markdown","dcc92e7b":"markdown","8694feaa":"markdown","8b3864b6":"markdown","b113c093":"markdown","6026b04e":"markdown","24cfedb7":"markdown","9df7e9b6":"markdown"},"source":{"85bf0e1c":"##\n# Import the relevant libraries\n##\n\nimport pandas as pd\nimport numpy as np\nimport json\nimport re\nimport torch.nn as nn \nimport torch\nfrom transformers import BertTokenizer\nfrom nltk.corpus import stopwords\nfrom tqdm import tqdm\nimport random\n\n","fd8575fb":"##### # Set the constants that are used in the code \n##\n\nDATA_PROCESSING_MODE = \"PROCESS_TRAINING_DATA\"\n#DATA_PROCESSING_MODE = \"USE_PREPROCESSED_TRAINING_DATA\"\n\nTRAINING_MODE = \"TRAIN_FROM_SCRATCH\"\n#TRAINING_MODE = \"USE_TRAINED_MODEL\"\n\nTRAIN_PATH = '..\/input\/coleridgeinitiative-show-us-the-data\/train.csv'\nTRAIN_DATA_FOLDER = '..\/input\/coleridgeinitiative-show-us-the-data\/train'\nTRAINING_BATCH_END_INDEX = 5000\nVAL_BATCH_START_INDEX = 5000 \nVAL_BATCH_END_INDEX = 6000  \n\nPREPROCESSED_TRAIN_PATH = \"..\/input\/colldata10-2\/colleridge_processed_train-10.parquet\"\nPREPROCESSED_VAL_PATH =\"..\/input\/colldata10-2\/colleridge_processed_val-10.parquet\"\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nMAX_SEQ = 200\nOVERLAP=20\nEMBEDDING_DIM = 200 \nBATCH_SIZE = 8 \nLEARNING_RATE=1e-3\nEPOCHS = 15 ","16f6c76c":"##\n# Read the data \n##\ntrain = pd.read_csv(TRAIN_PATH)\ntrain","dc36bdbb":"train['cleaned_label'] = train['cleaned_label'].apply(lambda x : x.strip())\n","ea1227a4":"known_labels = np.unique( train['cleaned_label'] ) ","ce786d28":"'''for label in known_labels:\n    ignore=False;\n    for word in label.split():\n        if word in ['study', 'data', 'dataset', 'database', 'survey']:\n            ignore=True\n            break\n    if( ignore==False):\n        print(label)'''\n    ","50472902":"'''with open(TRAIN_DATA_FOLDER+'\/'+ 'fd23e7e0-a5d2-4f98-992d-9209c85153bb' +'.json', 'r') as f:\n    #Load the json \n    paper = json.load(f)\n    print(paper)\n    #print( re.findall('\\([A-Z]{4,}', paper[0].get('text')) ) '''","8c7f5fb4":"####\n#\n# This method is used to clean the input list of words.\n#\n####\nstopwords_set = set(stopwords.words('english')) \ndef clean_text(input_words):\n    #return [ re.sub(' +', ' ', re.sub('[^A-Za-z0-9\\[\\]]+', ' ', str(r).lower()).strip()) for r in input_words if not r.lower() in stopwords_set  ]\n    return [ re.sub(' +', ' ', re.sub('[^A-Za-z0-9\\[\\]]+', ' ', str(r).lower()).strip()) for r in input_words ]\n\n\n\n####\n#\n# This method takes a single chunk of 'input words', cleans and tokenizes them using a tokenizer.  It also compares the input \n# for a set of \"potential output string\". If any of them match, the matches string is also tokenized with a wraping \n# [cls] [sep]. The tokenized input and output array is returned as the output. \n#\n####\ndef tokenize_single_chunk(input_words, potential_output_strings, tokenizer):\n\n    successfully_matched_string = \"[cls] \"\n    \n    matched_list = [] \n    \n    for match_string in potential_output_strings:  \n        if \" \".join(clean_text(match_string.split())) not in \" \".join(clean_text(input_words)):\n            continue    \n        matched_list.append(match_string)\n\n    for match_string1 in matched_list:\n        '''ignore=False        \n        for match_string2 in matched_list:\n            if( (match_string1 != match_string2) and (match_string1 in match_string2 )):\n                ignore=True\n        if( ignore == False): '''  \n        successfully_matched_string = successfully_matched_string  +  match_string1    +  \" [sep] \"\n\n    if (successfully_matched_string == \"[cls] \" ):\n        successfully_matched_string = successfully_matched_string  +  \" [sep] \"\n        \n    #print(successfully_matched_string)\n    \n         \n    encoded_input_word_list = clean_text(input_words)\n    encoded_input_word_list = list(map(tokenizer.convert_tokens_to_ids, tokenizer.tokenize ( \" \".join(encoded_input_word_list)) ))\n    \n\n    if ( len(encoded_input_word_list) <= MAX_SEQ ):\n        encoded_input_word_list = encoded_input_word_list + list(np.zeros( MAX_SEQ - len(encoded_input_word_list)) )\n    else :\n        encoded_input_word_list = encoded_input_word_list[0:MAX_SEQ]\n    \n    \n    encoded_output_word_list = list(map(tokenizer.convert_tokens_to_ids, tokenizer.tokenize ( successfully_matched_string ) ))\n    \n\n    if ( len(encoded_output_word_list) < MAX_SEQ ):\n        encoded_output_word_list = encoded_output_word_list +  list(np.zeros( MAX_SEQ - len(encoded_output_word_list)) )\n    else:\n        encoded_output_word_list = encoded_output_word_list[:MAX_SEQ]\n        \n    \n    return encoded_input_word_list, encoded_output_word_list\n\n\n\n\n####\n#\n# This method takes a dataframe with each row representing an id and 'output labels'. The corresponding text in the json is read  \n# and converted into chunks of 450 (Tokenizer uses word piece that causes the count to increase so setting to 450. (Todo: Relook) ).\n# Process each of these chunks and 'output labels' to create the input and ouputs needed to train the network. \n# This returns a dataframe with encoded inputs and ouput as each row (for each chunk )\n# \n####\ndef process_data (df, tokenizer) :\n    max_word_count = 0 \n    text_array = [] \n    code_array=[]\n     \n    \n    df_final = pd.DataFrame(columns = ['Id', 'text', 'code'])\n\n    #words_in_cap=[]\n    \n    for index, id in tqdm ( enumerate ( df.index) )  :\n        with open(TRAIN_DATA_FOLDER+'\/'+ id +'.json', 'r') as f:\n                #Load the json \n                paper = json.load(f)\n                #print(id)\n                #print(paper)\n                \n                #Convert the relevant sentences into a single paragraph\n                section_sentences =[]\n\n                for section_index in range ( 0, len(paper)):\n                    #words_in_cap = words_in_cap + re.findall('[A-Z]{4,}', paper[section_index].get('text'))\n                    section_sentences = section_sentences +  re.split('[.;\\n\u2022]',paper[section_index].get('text'))   \n                section_sentences = \"[sep]\".join(section_sentences) \n                sentence_words = section_sentences.split() \n        \n               \n            \n                # Chunk the paragraph into size of 500. Todo:Relook at this \n                n = int(0.7*MAX_SEQ)\n                word_chunks = [sentence_words[i:i+n+OVERLAP]  for i in range(0, len(sentence_words), n)] \n                word_chunks[-1] = word_chunks[-1] + list(np.empty( MAX_SEQ - len(word_chunks[-1]), dtype=str) )\n \n                     \n                # Add the words in the labels to the output wordlist\n                labels  = df[index]\n\n                # Encode\/Embed the words for processing\n                df_new = pd.DataFrame(columns = ['Id', 'text', 'code','rseq_no'])\n                chunk_count= len(word_chunks)\n                for i in range ( 0, len(word_chunks)):\n                    encoded_input_word_list, encoded_output_word_list = tokenize_single_chunk(word_chunks[i], labels, tokenizer )\n                    df_temp = pd.DataFrame ({\"Id\":id, \"text\":[encoded_input_word_list], \"code\": [encoded_output_word_list], 'rseq_no':chunk_count- i}, index=[i])\n                    df_new =df_new.append(df_temp, ignore_index=True)\n                    #print(df_new)\n\n        df_final = df_final.append(df_new, ignore_index=True)\n\n    df_final['word_count'] = list( map ( (lambda i : len ( [ x for x in df_final.iloc[i]['code'] if x not in [30522, 30523, 0.0] ] ) ) , range(0, df_final.shape[0])) )\n\n    '''words_in_cap = np.unique( words_in_cap)\n    for cap_word in words_in_cap:\n        print(cap_word)\n        tokenizer.add_tokens(cap_word)'''\n        \n    return df_final","d1b5aecf":"##\n# Group the data to obtain all the output labels for an id \n##\n\ntrain['dataset_title'] = train['dataset_title'].apply ( lambda x: re.sub('[^A-Za-z0-9\\[\\]]+', ' ', str(x).lower()).strip() )\ntrain['dataset_label'] = train['dataset_label'].apply ( lambda x:  re.sub('[^A-Za-z0-9\\[\\]]+', ' ', str(x).lower()).strip() )\n\n'''train_new = train [['Id', 'dataset_title', 'dataset_label', 'cleaned_label']].groupby('Id').apply(lambda r: np.unique( list( r['dataset_label'].values) \n                                                                                +  list ( r['cleaned_label'].values) \n                                                                                + list ( r['dataset_title'].values )) )'''\n\ntrain_new = train [['Id', 'dataset_title', 'dataset_label', 'cleaned_label']].groupby('Id').apply(lambda r: np.unique( list ( r['cleaned_label'].values) \n                                                                                ))\ntrain_new","f08093ec":"temp = list(map( lambda x : list(x), train_new)) \nknown_labels = np.unique( [j for sub in temp for j in sub] )","bfe37946":"##\n# Intialize a BERT Tokenizer and Process the input data and make it ready for training. \n# Create train and validataion ( Todo : do this using Cross Validation )\n##\n\ntokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\ntokenizer.add_tokens('[cls]')\ntokenizer.add_tokens('[sep]')\n\nif ( DATA_PROCESSING_MODE == 'PROCESS_TRAINING_DATA' ):\n    df_train = process_data(train_new[:TRAINING_BATCH_END_INDEX], tokenizer)  \n    df_train.to_parquet(\"colleridge_processed_train.parquet\")\n\n    df_val = process_data(train_new[VAL_BATCH_START_INDEX:VAL_BATCH_END_INDEX], tokenizer)  \n    df_val.to_parquet(\"colleridge_processed_val.parquet\")\n    \nelse :\n    df_train = pd.read_parquet (PREPROCESSED_TRAIN_PATH)\n    df_val = pd.read_parquet(PREPROCESSED_VAL_PATH)\n    df_train = df_train.append(df_val)\n\n    ","12387473":"vocab_len= 30522 + len( tokenizer.get_added_vocab() )\nvocab_len","f9bc7557":"#df_tx = df_train.groupby('Id').apply( lambda row: sum(row['word_count'])) \n#train_new[train_new.index == '01ab57bd-b4ad-4440-a99a-40f1f810f4cd']['01ab57bd-b4ad-4440-a99a-40f1f810f4cd']\n#process_data ( train_new[train_new.index == 'd67cdcda-a6fc-41b5-9ee0-016edabe4aec'] , tokenizer ) ","0d77488b":"df_train = df_train.reset_index()\ndf_train","95628ce6":"df_train[df_train['Id']=='630de001-fa5b-4973-ba3f-52f6f1f4dc2d']","e707495c":"##\n# Define the transformer model \n##\n\nimport torch.nn as nn \n\nclass FFN(nn.Module):\n    def __init__(self, state_size=EMBEDDING_DIM):\n        super(FFN, self).__init__()\n        self.state_size = state_size\n        self.lr1 = nn.Linear(state_size, state_size)\n        self.relu = nn.ReLU()\n        self.lr2 = nn.Linear(state_size, state_size)\n        self.dropout = nn.Dropout(0.2)\n    \n    def forward(self, x):\n        x = self.lr1(x)\n        x = self.relu(x)\n        x = self.lr2(x)\n        return self.dropout(x)\n\ndef future_mask(seq_length):\n    future_mask = np.triu(np.ones((seq_length, seq_length)), k=1).astype('bool')\n    return torch.from_numpy(future_mask)\n\nclass ColleridgeTransformerModel(nn.Module):\n\n    def __init__(self, input_vocab_len , output_vocab_len,embed_dim = EMBEDDING_DIM, max_seq=MAX_SEQ): \n        super(ColleridgeTransformerModel, self).__init__()\n        self.embed_dim = embed_dim\n        self.embedding = nn.Embedding(input_vocab_len, embed_dim)\n        self.pos_embedding = nn.Embedding(max_seq, embed_dim)\n        self.e_embedding = nn.Embedding(output_vocab_len, embed_dim)\n        self.multi_att = nn.MultiheadAttention(embed_dim=embed_dim, num_heads=8, dropout=0.2)\n        self.dropout = nn.Dropout(0.2)\n        self.layer_normal = nn.LayerNorm(embed_dim) \n        self.ffn = FFN(embed_dim)\n        self.pred = nn.Linear(embed_dim, output_vocab_len)\n        self.softmax = nn.LogSoftmax(dim=2)\n    \n    def forward(self, x, generated_words):\n        device = x.device       \n        \n        # Embed the token\n        x = self.embedding(x)\n        \n        # Calculate the position id and embed it\n        pos_id = torch.arange(x.size(1)).unsqueeze(0).to(device)\n        \n        #Add it to the poisition id\n        pos_x = self.pos_embedding(pos_id)\n        x = x + pos_x\n        \n        # This is the target_id\/query ( right shifted input )\n        e = self.e_embedding(generated_words) \n        pos_e_id = torch.arange(e.size(1)).unsqueeze(0).to(device)\n        pos_e = self.pos_embedding(pos_e_id)\n        e = e + pos_e\n         \n        # Send the query key and value to the multi attention network \n        x = x.permute(1, 0, 2) # x: [bs, s_len, embed] => [s_len, bs, embed]\n        att_output, att_weight = self.multi_att(x, x, x)\n        x = self.layer_normal(att_output + x)\n        \n        \n        for i in range ( 0, 1):\n            \n            e = e.permute(1, 0, 2)\n              \n            att_mask = future_mask(e.size(0)).to(device) \n\n            # Send the query key and value to the multi attention network \n            att_output, att_weight = self.multi_att(e, e, e, attn_mask=att_mask)\n\n            e = self.layer_normal(att_output + e)\n\n            # Send the query key and value to the multi attention network \n            att_output, att_weight = self.multi_att(e, x, x)\n\n            # Normalization \n            att_output = self.layer_normal(att_output + e)\n\n            # Reshape the output\n            att_output = att_output.permute(1, 0, 2) # att_output: [s_len, bs, embed] => [bs, s_len, embed]\n\n            # Send the output to FFN.\n            e = self.ffn(att_output)\n\n            # Normalize\n            e = self.layer_normal(e + att_output)\n        \n        # predict the output string \n        e = self.pred(e)\n        # Softmax the output string\n        e = self.softmax(e)\n        e = e.permute(0, 2, 1)\n        return e, att_weight","5c0834bd":"##\n# Create the dataset class that will be used in the training \n##\n\nfrom torch.utils.data import Dataset, DataLoader\n\nclass ColleridgeDataset(Dataset):\n    def __init__(self, df, max_seq=MAX_SEQ):\n        super(ColleridgeDataset, self).__init__()\n        self.max_seq = max_seq\n        self.samples = df\n\n    def __len__(self):\n        #print(len(self.samples.index) )\n        return len(self.samples.index)\n    \n    def __pair_to_tensor__(self, pair):\n        input_tensor = torch.tensor(pair[0] , dtype=torch.int, device=device).view(-1, 1) \n        target_tensor = torch.tensor(pair[1], dtype=torch.int, device=device).view(-1, 1) \n        return (input_tensor, target_tensor)\n    \n    def __getitem__(self, index):\n        x, y = self.__pair_to_tensor__( (self.samples.iloc[index]['text'], self.samples.iloc[index]['code']) )\n        target_id = y[:-1]\n        label = y[1:]\n        #print(target_id.squeeze(-1)[0:10])\n        return  x.squeeze(-1), target_id.squeeze(-1), label\n##\n# This method trains the model with the iterator data for ONE epoch. \n##\nfrom sklearn.metrics import roc_auc_score\ndef train_epoch(model, train_iterator, optim, criterion, device=\"cpu\"):\n    model.train()\n\n    train_loss = []\n    num_corrects = 0\n    num_total = 0\n    labels = []\n    outs = []\n\n    tbar = tqdm(train_iterator)\n    for item in tbar:\n        x = item[0].to(device).long()\n        target_id = item[1].to(device).long()\n        label = item[2].to(device).long() \n\n        # Reset the gradients before invoking the 'forward' again\n        optim.zero_grad()\n        \n        # Invoke the 'forward' of the model\n        output, atten_weight = model(x, target_id)\n        \n        # compute the loss with the actual and computed value\n        loss = criterion(output , label.squeeze(-1) )\n\n        # when we call loss.backward(), the whole graph is \n        # differentiated w.r.t. the loss, and all Variables in the graph will have their \n        # .grad Variable accumulated with the gradient.\n        loss.backward()\n        \n        # move forward the optimizer\n        optim.step()\n        \n        # For display :\n        train_loss.append(loss.item())\n\n        tbar.set_description('loss - {:.4f}'.format(loss))\n\n    loss = np.mean(train_loss)\n\n    return loss","bd3e765d":"## #  Create a dataset that will provide the data for validation. Please note the 'target', initially, contains only the encoding for [cls] token \n#  Each generated ouput token in the previous cycle will be added to the target for the next prediction cycle.\n# \n##\n\nfrom torch.utils.data import Dataset, DataLoader\nclass ColleridgeDatasetEval(Dataset):\n    def __init__(self, df, max_seq=MAX_SEQ):\n        super(ColleridgeDatasetEval, self).__init__()\n        self.max_seq = max_seq\n        self.samples = df\n\n    def __len__(self):\n        return len(self.samples.index)\n    \n    def __getitem__(self, index):\n        x = torch.tensor(self.samples.iloc[index]['text'], dtype=torch.int, device=device).view(-1, 1)\n        y = torch.tensor(np.zeros(MAX_SEQ), dtype=torch.int, device=device).view(-1, 1)\n        y[0] = 30522\n        target_id = y[:-1]\n        \n        #label = torch.tensor(self.samples.iloc[index]['code'], device=device).view(1,-1).tolist()\n        return  x.squeeze(-1), target_id.squeeze(-1), np.array( self.samples.iloc[index]['code'] )  , self.samples.iloc[index]['Id']\n    \n##\n# Validate the model with unseen data and return the fbeta score \n##\n\ndef validate_model (model,dataloader,device ):\n\n    num=0\n\n    df_eval = pd.DataFrame(columns = ['id', 'predicted', 'truth'])\n    model.eval()\n\n    for item in tqdm(dataloader):\n        stop = False \n        i = 0\n        while ( i < MAX_SEQ-2 ):\n            output,_ = model( item[0].to(device).long(),item[1].to(device).long()  )\n            y = torch.argmax( output , dim=1)\n            z = y[0][i]\n            item[1][0][i+1]=z\n            i = i+1\n            if ( z == 0):\n                break\n        y = [30522] + y.tolist()[0]\n\n        pred_string = tokenizer.convert_tokens_to_string (tokenizer.convert_ids_to_tokens([int(z) for z in y if z not in [30522, 0, 0.0, 0 ]]))\n        text = tokenizer.convert_tokens_to_string (tokenizer.convert_ids_to_tokens([z for z in item[0].tolist()[0] if z not in [30522, 0, 0.0, 0 ]]))\n\n        for index, pred_label in enumerate ( pred_string.split('[sep]') )   : \n            if ( text.find(pred_label) != -1 ):\n                df_temp = pd.DataFrame ({\"id\":item[3],  \"predicted\": [pred_label], 'truth': item[2].tolist() }, index=[num])\n                df_eval =df_eval.append(df_temp, ignore_index=True)\n            else:\n                df_temp = pd.DataFrame ({\"id\":item[3],  \"predicted\": '', 'truth': item[2].tolist()  }, index=[num])\n                df_eval =df_eval.append(df_temp, ignore_index=True)\n            \n        num = num+1\n        #print(df_eval.shape)\n \n    \n    #df_eval = df_eval.reset_index()\n    #print(df_eval)\n\n    df_eval ['truth_string'] = df_eval['truth'].apply(lambda r: \"|\".join(tokenizer.convert_tokens_to_string \\\n                        (tokenizer.convert_ids_to_tokens([int(z) for z in r if z not in [30522, 0.0, 0 ]] )).split('[sep]')) )\n    #df_eval ['predicted_string'] = df_eval['predicted'].apply(lambda r: \"|\".join(tokenizer.convert_tokens_to_string (tokenizer.convert_ids_to_tokens( [int(z) for z in r if z not in [30522, 0.0, 0]] ) ).split('[sep]')))\n    df_result=df_eval.groupby('id').apply(lambda row: (  \"|\".join(np.unique(\"|\".join(row['truth_string'].values).split(\"|\"))), \\\n                                                       \"|\".join( np.unique( \"|\".join(row['predicted'].values).split(\"|\")) ) ) )\n        \n        \n    df_final_result = pd.DataFrame(columns = ['id', 'predicted', 'truth'])\n    df_final_result['id'] = df_result.index\n    df_final_result['truth'] = list(map(lambda row: row[0][1:],  df_result.values ))\n    df_final_result['predicted'] =list(map(lambda row: row[1][1:],  df_result.values ))\n    '''df_final_result['truth'] = list(map(lambda row: row[0],  df_result.values ))\n    df_final_result['predicted'] =list(map(lambda row: row[1],  df_result.values ))'''\n\n\n\n    return ( compute_fbeta(df_final_result['truth'].apply(lambda x: x.split('|')), \n                           df_final_result['predicted'].apply(lambda x: x.split('|')),\\\n                           0.5) )\n\n\n\n","3a91e7cc":"\"\"\"Compute the Jaccard-based micro FBeta score.\n\n    References\n    ----------\n    - https:\/\/www.kaggle.com\/c\/coleridgeinitiative-show-us-the-data\/overview\/evaluation\n    - https:\/\/www.kaggle.com\/armandmorin\/matching-baseline-mlm-w-evaluation-metric\n\"\"\"\ndef compute_fbeta(y_true,\n                  y_pred,\n                  beta) -> float:\n    \"\"\"Compute the Jaccard-based micro FBeta score.\n\n    References\n    ----------\n    - https:\/\/www.kaggle.com\/c\/coleridgeinitiative-show-us-the-data\/overview\/evaluation\n    \"\"\"\n\n    def _jaccard_similarity(str1 , str2 ) :\n        #print(str1+\"XXXX\"+str2)\n        a = set(str1.split()) \n        b = set(str2.split())\n        c = a.intersection(b)\n        if ( len(a) + len(b) +len(c) == 0) :\n            return 0.0\n        else:\n            return float(len(c)) \/ (len(a) + len(b) - len(c)+1)\n\n    tp = 0.000001  # true positive\n    fp = 0.000001  # false positive\n    fn = 0.000001  # false negative\n    for ground_truth_list, predicted_string_list in zip(y_true, y_pred):\n        \n        #print(predicted_string_list)\n        #print(ground_truth_list)\n        #print(\"XXX\")\n           \n        predicted_string_list_sorted = sorted(predicted_string_list)\n        for ground_truth in sorted(ground_truth_list):     \n\n            if len(predicted_string_list_sorted) == 0:\n                fn += 1\n            else:\n                similarity_scores = [\n                    _jaccard_similarity(ground_truth, predicted_string)\n                    for predicted_string in predicted_string_list_sorted\n                ]\n                matched_idx = np.argmax(similarity_scores)\n                if similarity_scores[matched_idx] >= 0.5:\n                    predicted_string_list_sorted.pop(matched_idx)\n                    tp += 1\n                else:\n                    #print(\"false negative\")\n                    fn += 1\n        fp += len(predicted_string_list_sorted)\n\n    \n\n    tp *= (1 + beta ** 2)\n    fn *= beta ** 2\n    fbeta_score = tp \/ (tp + fp + fn)\n    return fbeta_score","68afea05":"'''tokenizer.convert_tokens_to_string (tokenizer.convert_ids_to_tokens( \\\n                        df_train_new[ ((df_train_new['word_count']==0) & (df_train_new['rseq_no']< 5)) == True ].iloc[7]['text']))'''","0c3842c5":"##\n# Training and Validating the model\n##\n\nfrom tqdm import tqdm \nfrom sklearn.model_selection import KFold\n\n\nunique_ids = df_train['Id'].unique()\nkfold = KFold(6, shuffle=True, random_state=1)\ncurr_best_cv_score = -1\ncv_score_sum=0\n\ncv_iter=0\n\nif ( TRAINING_MODE ==  \"TRAIN_FROM_SCRATCH\" ):\n        \n    for train, test in kfold.split(range(0,130)):\n\n        #train_ids = [unique_ids[x] for x in train]\n        train_ids = [idx for idx in train_new.index for truth in train_new[idx] if truth  in known_labels[train]  ]\n        df_train_new =  df_train[df_train['Id'].isin(train_ids) ].copy()\n        \n        #val_ids = [unique_ids[x] for x in test]\n        val_ids = [idx for idx in train_new.index for truth in train_new[idx] if truth  in known_labels[test]  ]\n        df_val_new = df_train[df_train['Id'].isin(val_ids)].copy()\n        df_val_new = df_val_new.reset_index()\n        print(df_val_new.shape)\n\n        xfrmer_model = ColleridgeTransformerModel( vocab_len ,vocab_len )\n        xfrmer_model.to(device)\n        \n        # Ensure that the data is balanced with rows which has study and the ones without. \n        pos_samples = df_train_new[ df_train_new['word_count']!=0 ] \n        neg_samples = df_train_new[ ((df_train_new['word_count']==0) & (df_train_new['rseq_no']< 7)) == True ] \n        neg_samples = neg_samples [:pos_samples.shape[0]]\n        print(pos_samples.shape[0], neg_samples.shape[0])\n        df_train_new =  pos_samples.append ( neg_samples ) \n        df_train_new = df_train_new.reset_index()\n        #print(df_train_new.shape)\n        \n        #print(df_train_new.index)\n\n        train_dataset = ColleridgeDataset(df_train_new)\n        train_dataloader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n\n        val_dataset = ColleridgeDatasetEval(df_val_new)\n        val_dataloader = DataLoader(val_dataset, batch_size=1, shuffle=False)\n\n        optimizer = torch.optim.Adam(xfrmer_model.parameters(), lr=LEARNING_RATE)\n        criterion = nn.NLLLoss(reduction=\"sum\")\n        criterion.to(device)\n        \n\n        for epoch in range(EPOCHS):\n            loss = train_epoch(xfrmer_model, train_dataloader, optimizer, criterion, device)\n            print(\"epoch - {} train_loss - {:.2f} \".format(epoch, loss))\n\n        cv_score = validate_model (xfrmer_model,val_dataloader, device)\n        print(cv_score)\n        cv_score_sum = cv_score_sum + cv_score\n        print(cv_score_sum)\n\n        if( cv_score > curr_best_cv_score):\n            curr_best_cv_score = cv_score\n            torch.save(xfrmer_model.state_dict(), \"xform-17.model\")\n            \n        cv_iter = cv_iter+1\n        if(cv_iter>0):\n            break\n\n\n    print (\"average score = \", cv_score_sum\/1)  \n    print( \"best score = \", cv_score)","3cbd007b":"'''test_ids = [idx for idx in train_new.index for truth in train_new[idx] if truth  in known_labels[100:]  ]\ndf_test = df_train[df_train['Id'].isin(test_ids)].copy()\ndf_test = df_test.reset_index()'''","f095d9d2":"'''xfrmer_model = ColleridgeTransformerModel( vocab_len ,vocab_len )\nxfrmer_model.to(device)\nxfrmer_model.load_state_dict(torch.load('..\/input\/model16-3\/xform-16-3.model', map_location=device))\nvalidate_model (xfrmer_model,test_dataloader, device)'''","34a7c022":"'''xfrmer_model = ColleridgeTransformerModel( 30523 ,30523 )\nxfrmer_model.to(device)\nxfrmer_model.load_state_dict(torch.load('..\/input\/model16\/xform-16.model', map_location=device))\nvalidate_model (xfrmer_model,test_dataloader, device)'''","184782c9":"![Predict.jpg](attachment:9a1094e0-9932-4d9b-8f4e-2db330dbf059.jpg)","0e291ffd":"# **Coleridge Initiative - Show US the Data**\n","18c7df86":"For Prediction - the input work tokens along with target ( just encoded [cls] token ) is provided to the trained network. This will generate the next output token. This is then appeneded and provided as target in the next iteration. This process is repeated till no more new tokens are generated.","dcc92e7b":"![Train.jpg](attachment:2f931856-1b98-4fdb-b3b7-40f478cb76ef.jpg)","8694feaa":"## Training","8b3864b6":"Original image taken from : https:\/\/arxiv.org\/abs\/1706.03762","b113c093":"## Prediction","6026b04e":"![DataPreprocessing.jpg](attachment:5c54789f-f52f-48a2-8865-4b94b6c33e3b.jpg)","24cfedb7":"The goal of this competition is to extract out the references to various studies\/dataset that is embedded in the content. \n\nThe approach followed is similar to that of a translation problem using an attention network. We provide the input text and the model has to provide an output text. In this case, the output text is the \"references\" in the text. \n\nThis notebook uses a pretrained BERTTokenizer to generate the encoded tokens for the provided text and then uses a simple transformer model to generate an output that is also a sequence of BERTTokens.\n\nTo acheive this, we are going to do the following\n\n* Preprocessing the data\n    * Read the relevant content for each id.\n    * chunk them into sizes of 500.\n    * clean these chunks and BERTTokenize them. This will be the input to the transformer.\n    * During training, we read the output dataset label\n    * clean these and BERTTokenize them.\n    * wrap these around [CLS] and [SEP] tokens. This will be the ouput for the transformer.\n* Model and training the model\n    * We create a simple pytorch network based on transformers.\n    * we train the network using the inputs and outputs that we generated during pretraining\n    * we use NLLLoss for backpropagation in the network.\n\n\nThis model provides a score of 0.532 during inference with the test data.\nhttps:\/\/www.kaggle.com\/sreejaej\/colleridge-attention-inference\/ \n","9df7e9b6":"## Data Preprocessing"}}