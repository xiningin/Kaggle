{"cell_type":{"9ac02934":"code","a0b48531":"code","cb1abed1":"code","21ba9d8b":"code","8fd401ac":"code","428319d4":"code","32dabec1":"code","21b47375":"code","2049f301":"code","ce28635e":"code","347cbd93":"code","c067ff49":"code","14f9265b":"code","ca92ea50":"code","b20050a5":"code","e0becc8e":"code","8e031778":"code","20a71e12":"code","73d78571":"code","d596a88b":"code","5a5884e5":"code","2ab70c67":"code","31c5cd50":"code","edb00a3a":"code","2d352fba":"code","333b724c":"code","dddbcfa1":"code","789f2304":"code","c5562d07":"code","dd78c76c":"code","12dd9644":"code","6e46106a":"code","cd09e865":"code","88737e0d":"code","01fda426":"code","6d6dedd8":"code","cebb51a7":"code","d1218359":"code","276faa8a":"code","f8592f0c":"code","d467d273":"code","94b679a9":"code","4bb04c85":"code","4db1f549":"code","a2321661":"code","70ed26b7":"code","0328e536":"code","a07ce0a7":"code","22e8a466":"code","12fce22b":"code","b7524b1a":"code","b42c9e6d":"markdown","0fc51121":"markdown","d40e70dd":"markdown","d91b9168":"markdown","e33403dc":"markdown","fbbb09e8":"markdown","9484ac63":"markdown","6d8d45b6":"markdown","c90ef067":"markdown","1f80316d":"markdown","e89f8dd0":"markdown","d355b99c":"markdown"},"source":{"9ac02934":"import pandas as pd","a0b48531":"import matplotlib.pyplot as plt\nimport seaborn as sns\ncolor = sns.color_palette()\n%matplotlib inline\nimport plotly.offline as py\npy.init_notebook_mode(connected=True)\nimport plotly.graph_objs as go\nimport plotly.tools as tls\nimport plotly.express as px\nimport pandas as pd","cb1abed1":"df = pd.read_csv('..\/input\/product-reviews-and-ratings-sentiment-analysis\/Reviews And Ratings.csv')","21ba9d8b":"df.head(10)","8fd401ac":"fig = px.histogram(df, x=\"Rating\")\nfig.update_traces(marker_color=\"turquoise\",marker_line_color='rgb(8,48,107)',\n                  marker_line_width=1.5)\nfig.update_layout(title_text='Stars Rating Score')\nfig.show()","428319d4":"import nltk\nfrom nltk.corpus import stopwords\nfrom spacy.lang.en.stop_words import STOP_WORDS\nfrom wordcloud import WordCloud\n# Create stopword list:\nSTOP_WORDS.add('otter')\nstopwords = set(list(STOP_WORDS) +list(stopwords.words()))\nstopwords.update([\"br\", \"href\", 'https'])\nstopwords.update(stopwords)\ntextt = \" \".join(desc for desc in df.translated)\nwordcloud = WordCloud(stopwords=stopwords).generate(textt)\nplt.imshow(wordcloud, interpolation='bilinear')\nplt.axis(\"off\")\nplt.savefig('wordcloud11.png')\nplt.show()","32dabec1":"df = df[df['Rating'] != 0]\n#Creating Positive & Negative sentiments as +1 and -1 according to rating\ndf['sentiment'] = df['Rating'].apply(lambda rating : +1 if rating >= 4 else -1)","21b47375":"positive = df[df['sentiment'] == 1]\nnegative = df[df['sentiment'] == -1]","2049f301":"nltk.download('stopwords')","ce28635e":"stopwords.update([\"br\", \"href\",\"good\",\"great\", 'https']) \npos = \" \".join(review for review in positive.translated)\nwordcloud2 = WordCloud(stopwords=stopwords).generate(pos)\nplt.imshow(wordcloud2, interpolation='bilinear')\nplt.axis(\"off\")\nplt.show()","347cbd93":"for i in positive[positive['translated'].str.contains(\"otter\")]['translated'].iloc[0:7]:\n    print(i,'\\n')","c067ff49":"neg = \" \".join(review for review in negative.translated)\nwordcloud3 = WordCloud(stopwords=stopwords).generate(neg)\nplt.imshow(wordcloud3, interpolation='bilinear')\nplt.axis(\"off\")\nplt.savefig('wordcloud33.png')\nplt.show()","14f9265b":"#searching negative words that contains the product name i.e. otter\nfor i in negative[negative['translated'].str.contains(\"otter\")]['translated'].iloc[0:7]:\n    print(i,'\\n')","ca92ea50":"df['sentimentt'] = df['sentiment'].replace({-1 : 'negative'})\ndf['sentimentt'] = df['sentimentt'].replace({1 : 'positive'})\nfig = px.histogram(df, x=\"sentimentt\")\nfig.update_traces(marker_color=\"indianred\",marker_line_color='rgb(8,48,107)',\n                  marker_line_width=1.5)\nfig.update_layout(title_text='Product Sentiment')\nfig.show()","b20050a5":"def remove_punctuation(text):\n    final = \"\".join(u for u in text if u not in (\"?\", \".\", \";\", \":\",  \"!\",'\"'))\n    return final\ndf['translated'] = df['translated'].apply(remove_punctuation)\ndf = df.dropna(subset=['translated'])\ndf['translated'] = df['translated'].apply(remove_punctuation)","e0becc8e":"    dfNew = df[['translated','sentiment']]\ndfNew.head()","8e031778":"import numpy as np\nindex = df.index\ndf['random_number'] = np.random.randn(len(index))\ntrain = df[df['random_number'] <= 0.8]\ntest = df[df['random_number'] > 0.8]","20a71e12":"from sklearn.feature_extraction.text import CountVectorizer\nvectorizer = CountVectorizer(token_pattern=r'\\b\\w+\\b')\ntrain_matrix = vectorizer.fit_transform(train['translated'])\ntest_matrix = vectorizer.transform(test['translated'])","73d78571":"# Logistic Regression\nfrom sklearn.linear_model import LogisticRegression\nlr = LogisticRegression()","d596a88b":"X_train = train_matrix\nX_test = test_matrix\ny_train = train['sentiment']\ny_test = test['sentiment']","5a5884e5":"lr.fit(X_train,y_train)","2ab70c67":"predictions = lr.predict(X_test)","31c5cd50":"from sklearn.metrics import confusion_matrix,classification_report\nnew = np.asarray(y_test)\nconfusion_matrix(predictions,y_test)","edb00a3a":"print(classification_report(predictions,y_test))","2d352fba":"count=0\nfor prediction, true_value in zip(predictions,y_test):\n    if prediction == true_value:\n        count+=1\nprint('accuracy is', round((count\/len(predictions))*100,2), '%.')","333b724c":"df = df[df['Rating'] != 0]\ndf['sentiment'] = df['Rating'].apply(lambda rating : +1 if rating >= 4 else -1)","dddbcfa1":"df_1 = df[df['Rating'] == 1]\ndf_2 = df[df['Rating'] == 2]\ndf_3 = df[df['Rating'] == 3]\ndf_4 = df[df['Rating'] == 4]\ndf_5 = df[df['Rating'] == 5]","789f2304":"#wordcloud for rating 1\nstopwords.update([\"br\", \"href\",\"good\",\"great\", 'https']) \npos = \" \".join(review for review in df_1.translated)\nwordcloud2 = WordCloud(stopwords=stopwords).generate(pos)\nplt.imshow(wordcloud2, interpolation='bilinear')\nplt.axis(\"off\")\nplt.show()","c5562d07":"#wordcloud for biwords for rating 1\nword_arr = pos.split(' ')\nfin_arr = []\nfor counter in range(len(word_arr)):\n    if counter+1 != len(word_arr)  and word_arr[counter].lower() not in STOP_WORDS and word_arr[counter+1].lower() not in STOP_WORDS:\n        fin_arr.append((word_arr[counter] + '_' + word_arr[counter+1]))\npos = ' '.join(fin_arr)\nwordcloud2 = WordCloud(stopwords=stopwords).generate(pos)\nplt.imshow(wordcloud2, interpolation='bilinear')\nplt.axis(\"off\")\nplt.show()","dd78c76c":"#freq_plot for rating 1\ncomplete_text = []\nfor sent in df_1['translated']:\n    for word in sent.split(' '):\n        if word.strip(',').lower() not in STOP_WORDS:\n            complete_text.append(word.strip(',').lower())\n\nfrom collections import Counter\nword_count = Counter(complete_text)\n\ncount_plot = word_count.most_common()[0:20]\n\nfrom matplotlib.pyplot import figure\nfigure(figsize=(20, 6), dpi=80)\n\nlabels = []\nvalues = []\nfor tupple in count_plot:\n    labels.append(tupple[0])\n    values.append(tupple[1])\n\nindexes = np.arange(len(labels))\nwidth = 1\n\nplt.bar(indexes, values, width)\nplt.xticks(indexes + width * 0.5, labels)\nplt.show()","12dd9644":"#wordcloud for rating 2\nstopwords.update([\"br\", \"href\",\"good\",\"great\", 'https']) \npos = \" \".join(review for review in df_2.translated)\nwordcloud2 = WordCloud(stopwords=stopwords).generate(pos)\nplt.imshow(wordcloud2, interpolation='bilinear')\nplt.axis(\"off\")\nplt.show()","6e46106a":"#wordcloud for biwords for rating 2\nword_arr = pos.split(' ')\nfin_arr = []\nfor counter in range(len(word_arr)):\n    if counter+1 != len(word_arr)  and word_arr[counter].lower() not in STOP_WORDS and word_arr[counter+1].lower() not in STOP_WORDS:\n        fin_arr.append((word_arr[counter] + '_' + word_arr[counter+1]))\npos = ' '.join(fin_arr)\nwordcloud2 = WordCloud(stopwords=stopwords).generate(pos)\nplt.imshow(wordcloud2, interpolation='bilinear')\nplt.axis(\"off\")\nplt.show()","cd09e865":"#freq_plot for rating 2\ncomplete_text = []\nfor sent in df_2['translated']:\n    for word in sent.split(' '):\n        if word.strip(',').lower() not in STOP_WORDS:\n            complete_text.append(word.strip(',').lower())\n\nfrom collections import Counter\nword_count = Counter(complete_text)\n\ncount_plot = word_count.most_common()[0:20]\n\nfrom matplotlib.pyplot import figure\nfigure(figsize=(20, 6), dpi=80)\n\nlabels = []\nvalues = []\nfor tupple in count_plot:\n    labels.append(tupple[0])\n    values.append(tupple[1])\n\nindexes = np.arange(len(labels))\nwidth = 1\n\nplt.bar(indexes, values, width)\nplt.xticks(indexes + width * 0.5, labels)\nplt.show()","88737e0d":"#wordcloud for rating 3\nstopwords.update([\"br\", \"href\",\"good\",\"great\", 'https']) \npos = \" \".join(review for review in df_3.translated)\nwordcloud2 = WordCloud(stopwords=stopwords).generate(pos)\nplt.imshow(wordcloud2, interpolation='bilinear')\nplt.axis(\"off\")\nplt.show()","01fda426":"#wordcloud for biwords for rating 3\nword_arr = pos.split(' ')\nfin_arr = []\nfor counter in range(len(word_arr)):\n    if counter+1 != len(word_arr)  and word_arr[counter].lower() not in STOP_WORDS and word_arr[counter+1].lower() not in STOP_WORDS:\n        fin_arr.append((word_arr[counter] + '_' + word_arr[counter+1]))\npos = ' '.join(fin_arr)\nwordcloud2 = WordCloud(stopwords=stopwords).generate(pos)\nplt.imshow(wordcloud2, interpolation='bilinear')\nplt.axis(\"off\")\nplt.show()","6d6dedd8":"#freq_plot for rating 3\ncomplete_text = []\nfor sent in df_3['translated']:\n    for word in sent.split(' '):\n        if word.strip(',').lower() not in STOP_WORDS:\n            complete_text.append(word.strip(',').lower())\n\nfrom collections import Counter\nword_count = Counter(complete_text)\n\ncount_plot = word_count.most_common()[0:20]\n\nfrom matplotlib.pyplot import figure\nfigure(figsize=(20, 6), dpi=80)\n\nlabels = []\nvalues = []\nfor tupple in count_plot:\n    labels.append(tupple[0])\n    values.append(tupple[1])\n\nindexes = np.arange(len(labels))\nwidth = 1\n\nplt.bar(indexes, values, width)\nplt.xticks(indexes + width * 0.5, labels)\nplt.show()","cebb51a7":"#wordcloud for rating 4\nstopwords.update([\"br\", \"href\",\"good\",\"great\", 'https']) \npos = \" \".join(review for review in df_4.translated)\nwordcloud2 = WordCloud(stopwords=stopwords).generate(pos)\nplt.imshow(wordcloud2, interpolation='bilinear')\nplt.axis(\"off\")\nplt.show()","d1218359":"#wordcloud for biwords for rating 4\nword_arr = pos.split(' ')\nfin_arr = []\nfor counter in range(len(word_arr)):\n    if counter+1 != len(word_arr)  and word_arr[counter].lower() not in STOP_WORDS and word_arr[counter+1].lower() not in STOP_WORDS:\n        fin_arr.append((word_arr[counter] + '_' + word_arr[counter+1]))\npos = ' '.join(fin_arr)\nwordcloud2 = WordCloud(stopwords=stopwords).generate(pos)\nplt.imshow(wordcloud2, interpolation='bilinear')\nplt.axis(\"off\")\nplt.show()","276faa8a":"#freq_plot for rating 4\ncomplete_text = []\nfor sent in df_4['translated']:\n    for word in sent.split(' '):\n        if word.strip(',').lower() not in STOP_WORDS:\n            complete_text.append(word.strip(',').lower())\n\nfrom collections import Counter\nword_count = Counter(complete_text)\n\ncount_plot = word_count.most_common()[0:20]\n\nfrom matplotlib.pyplot import figure\nfigure(figsize=(20, 6), dpi=80)\n\nlabels = []\nvalues = []\nfor tupple in count_plot:\n    labels.append(tupple[0])\n    values.append(tupple[1])\n\nindexes = np.arange(len(labels))\nwidth = 1\n\nplt.bar(indexes, values, width)\nplt.xticks(indexes + width * 0.5, labels)\nplt.show()","f8592f0c":"#wordcloud for rating 5\nstopwords.update([\"br\", \"href\",\"good\",\"great\", 'https']) \npos = \" \".join(review for review in df_5.translated)\nwordcloud2 = WordCloud(stopwords=stopwords).generate(pos)\nplt.imshow(wordcloud2, interpolation='bilinear')\nplt.axis(\"off\")\nplt.show()","d467d273":"#wordcloud for biwords for rating 5\nword_arr = pos.split(' ')\nfin_arr = []\nfor counter in range(len(word_arr)):\n    if counter+1 != len(word_arr)  and word_arr[counter].lower() not in STOP_WORDS and word_arr[counter+1].lower() not in STOP_WORDS:\n        fin_arr.append((word_arr[counter] + '_' + word_arr[counter+1]))\npos = ' '.join(fin_arr)\nwordcloud2 = WordCloud(stopwords=stopwords).generate(pos)\nplt.imshow(wordcloud2, interpolation='bilinear')\nplt.axis(\"off\")\nplt.show()","94b679a9":"#freq_plot for rating 5\ncomplete_text = []\nfor sent in df_5['translated']:\n    for word in sent.split(' '):\n        if word.strip(',').lower() not in STOP_WORDS:\n            complete_text.append(word.strip(',').lower())\n\nfrom collections import Counter\nword_count = Counter(complete_text)\n\ncount_plot = word_count.most_common()[0:20]\n\nfrom matplotlib.pyplot import figure\nfigure(figsize=(20, 6), dpi=80)\n\nlabels = []\nvalues = []\nfor tupple in count_plot:\n    labels.append(tupple[0])\n    values.append(tupple[1])\n\nindexes = np.arange(len(labels))\nwidth = 1\n\nplt.bar(indexes, values, width)\nplt.xticks(indexes + width * 0.5, labels)\nplt.show()","4bb04c85":"import pandas as pd\nimport spacy","4db1f549":"spacy.load('en_core_web_sm')\nfrom spacy.lang.en import English\nparser = English()\ndef tokenize(text):\n    lda_tokens = []\n    tokens = parser(text)\n    for token in tokens:\n        if token.orth_.isspace():\n            continue\n        elif token.like_url:\n            lda_tokens.append('URL')\n#         elif token.orth_.startswith('@'):\n#             lda_tokens.append('SCREEN_NAME')\n        else:\n            lda_tokens.append(token.lower_)\n    return lda_tokens","a2321661":"import nltk\nnltk.download('wordnet')\nfrom nltk.corpus import wordnet as wn\ndef get_lemma(word):\n    lemma = wn.morphy(word)\n    if lemma is None:\n        return word\n    else:\n        return lemma\n    \nfrom nltk.stem.wordnet import WordNetLemmatizer\ndef get_lemma2(word):\n    return WordNetLemmatizer().lemmatize(word)","70ed26b7":"nltk.download('stopwords')\nen_stop = set(nltk.corpus.stopwords.words('english'))","0328e536":"def prepare_text_for_lda(text):\n    tokens = tokenize(text)\n    tokens = [token for token in tokens if len(token) > 4]\n    tokens = [token for token in tokens if token not in en_stop]\n    tokens = [token for token in tokens if token != 'https']\n    tokens = [get_lemma(token) for token in tokens]\n    tokens = [get_lemma(token) if token not in ('corona', 'covid', 'covid19', 'covid-19', 'coronavirus', 'virus') else 'covid' for token in tokens]\n    return tokens","a07ce0a7":"import random\ntext_data = []\ndef tokens_(line):\n    tokens = prepare_text_for_lda(line)\n    text_data.append(tokens)\n    if random.random() > .99:\n        print(tokens)","22e8a466":"_ = df['translated'].apply(tokens_)","12fce22b":"from gensim import corpora\ndictionary = corpora.Dictionary(text_data)\ncorpus = [dictionary.doc2bow(text) for text in text_data]\nimport pickle\npickle.dump(corpus, open('corpus.pkl', 'wb'))\ndictionary.save('dictionary.gensim')","b7524b1a":"import gensim\nNUM_TOPICS = 5\nldamodel = gensim.models.ldamodel.LdaModel(corpus, num_topics = NUM_TOPICS, id2word=dictionary, passes=15)\nldamodel.save('model5.gensim')\ntopics = ldamodel.print_topics(num_words=10)\nfor topic in topics:\n    print(topic)","b42c9e6d":"## WordCloud for positive samples","0fc51121":"## Training Model on Data","d40e70dd":"## Preparing Data to pass to model","d91b9168":"## Distribution of sentiments amongst samples.","e33403dc":"![](http:\/\/https:\/\/www.google.com\/search?q=sentiment+analysis+wallpaper&sxsrf=ALeKk00slO2IHqLPgcCI9Ke6m8RYNZkyxA:1626723757182&tbm=isch&source=iu&ictx=1&fir=toXV3NWUDYm6RM%252Cpfj19SKi2tmpdM%252C_&vet=1&usg=AI4_-kQ8bpVIS2snbuyFwvdJpvic4AGu9g&sa=X&ved=2ahUKEwjMxsad8u_xAhXQa8AKHbMIBVMQ9QF6BAgOEAE&biw=1536&bih=722#imgrc=toXV3NWUDYm6RM)","fbbb09e8":"## WordCloud for negative samples","9484ac63":"## Creating a dataset with sementiment along with review","6d8d45b6":"## WordCloud for dataset and removing stopwords like 'https'","c90ef067":"## Performing EDA on Data","1f80316d":"## Printing results of training","e89f8dd0":"### Removing any Ratings that is zero","d355b99c":"## Distribution of Scores"}}