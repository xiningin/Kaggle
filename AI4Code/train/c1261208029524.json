{"cell_type":{"3c956c4d":"code","b87ef550":"code","f97ea0f1":"code","e8f6b21a":"code","afafd1ec":"code","a38578f0":"code","c68525fe":"code","c5b6a3c9":"code","d9d97a05":"code","09a64691":"code","28900a59":"code","182f71c2":"code","5a5b665e":"code","ce958dfc":"code","e60eb63d":"code","06ea1de1":"code","c269cc0b":"code","ec380ce0":"code","1ddaaa71":"markdown","1c146a35":"markdown","d99ffc4b":"markdown","21d5b6d0":"markdown","8428282a":"markdown","ff338ea7":"markdown","84f2b326":"markdown","de2793fd":"markdown","ed853b57":"markdown","bd231ffa":"markdown","03b83e75":"markdown","1104b39a":"markdown","60347141":"markdown","22ed981e":"markdown"},"source":{"3c956c4d":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt #data visualization\nimport seaborn as sns #data visualization\nimport numpy as np\n\nimport warnings            \nwarnings.filterwarnings(\"ignore\") \n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","b87ef550":"#Load dataset\ndata=pd.read_csv('..\/input\/churn-modelling\/Churn_Modelling.csv')\n#data includes how many rows and columns\ndata.shape\nprint(\"Our data has {} rows and {} columns\".format(data.shape[0],data.shape[1]))\n#Features name in data\ndata.columns","f97ea0f1":"#diplay first 5 rows\ndata.head()","e8f6b21a":"data.drop(['RowNumber', 'CustomerId', 'Surname','Geography'], axis=1, inplace=True)\n\n#I replaced Gender feature from Male\/Female to 1\/0.\ndata.Gender = [1 if each == 'Male' else 0 for each in data.Gender] ","afafd1ec":"data.describe().T","a38578f0":"#checking for missing values\nprint('Are there missing values? {}'.format(data.isnull().any().any()))\n#missing value control in features\ndata.isnull().sum()","c68525fe":"plt.figure(figsize=[5,5])\nsns.set(style='darkgrid')\nax = sns.countplot(x='Exited', data=data, palette='Set2')\ndata.loc[:,'Exited'].value_counts()","c5b6a3c9":"y = data.Exited.values\nx_data = data.drop(['Exited'], axis=1)","d9d97a05":"#we should normalize our features, features should dominate each other.\nx = (x_data - np.min(x_data)) \/ (np.max(x_data)-np.min(x_data))\nx.head()","09a64691":"from sklearn.model_selection import train_test_split\n\nx_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.10, random_state=42)\n\nprint('x_train shape: ', x_train.shape)\nprint('y_train shape: ', y_train.shape)\nprint('x_test shape: ', x_test.shape)\nprint('y_test shape: ', y_test.shape)","28900a59":"from sklearn.linear_model import LogisticRegression\nlr=LogisticRegression()\nlr.fit(x_train,y_train)\n\ny_pred=lr.predict(x_test)\n\nfrom sklearn.metrics import classification_report,confusion_matrix\nlr_cm = confusion_matrix(y_test, y_pred)\nprint(\"confusion matrix:\\n\",lr_cm)\n\nprint('test accuracy: {}'.format(lr.score(x_test,y_test)))\nprint('Classification report: \\n',classification_report(y_test,y_pred))","182f71c2":"#find k value\nfrom sklearn.neighbors import KNeighborsClassifier\n\nscore_list=[]\nfor each in range(1,15):\n    knn2=KNeighborsClassifier(n_neighbors=each)\n    knn2.fit(x_train,y_train)\n    score_list.append(knn2.score(x_test,y_test))\nplt.plot(range(1,15),score_list)\nplt.xlabel(\"k values\")\nplt.ylabel(\"accuracy\")\nplt.show()","5a5b665e":"#knn algorithm\n\nknn=KNeighborsClassifier(n_neighbors=13) #n_neighbors=k\nknn.fit(x_train,y_train)\nprediction=knn.predict(x_test)\n\nfrom sklearn.metrics import confusion_matrix\nknn_cm = confusion_matrix(y_test, prediction)\nprint(\"confusion matrix:\\n\",knn_cm)\n\nprint('test accuracy: {}'.format(knn.score(x_test,y_test)))\nprint('Classification report: \\n',classification_report(y_test,y_pred))","ce958dfc":"#svm algorithm\nfrom sklearn.svm import SVC\nsvm = SVC(random_state=0)\nsvm.fit(x_train,y_train)\nprediction=svm.predict(x_test)\n\nfrom sklearn.metrics import confusion_matrix\nsvm_cm = confusion_matrix(y_test, prediction)\nprint(\"confusion matrix:\\n\",svm_cm)\n\nprint('test accuracy: {}'.format(svm.score(x_test,y_test)))\nprint('Classification report: \\n',classification_report(y_test,y_pred))","e60eb63d":"#naive bayes algorithm\nfrom sklearn.naive_bayes import GaussianNB\nnb = GaussianNB()\nnb.fit(x_train,y_train)\nprediction=nb.predict(x_test)\n\nfrom sklearn.metrics import confusion_matrix\nnb_cm = confusion_matrix(y_test, prediction)\nprint(\"confusion matrix:\\n\",nb_cm)\n\nprint('test accuracy: {}'.format(nb.score(x_test,y_test)))\nprint('Classification report: \\n',classification_report(y_test,y_pred))","06ea1de1":"#desicion tree algorithm\nfrom sklearn.tree import DecisionTreeClassifier\ncart = DecisionTreeClassifier()\ncart.fit(x_train,y_train)\nprediction=cart.predict(x_test)\n\nfrom sklearn.metrics import confusion_matrix\ncart_cm = confusion_matrix(y_test, prediction)\nprint(\"confusion matrix:\\n\",cart_cm)\n\nprint('test accuracy: {}'.format(cart.score(x_test,y_test)))\nprint('Classification report: \\n',classification_report(y_test,y_pred))","c269cc0b":"#desicion tree algorithm\nfrom sklearn.ensemble import RandomForestClassifier\nrf = RandomForestClassifier(n_estimators=100, random_state=3)\nrf.fit(x_train,y_train)\nprediction=rf.predict(x_test)\n\nfrom sklearn.metrics import confusion_matrix\nrf_cm = confusion_matrix(y_test, prediction)\nprint(\"confusion matrix:\\n\",rf_cm)\n\nprint('test accuracy: {}'.format(rf.score(x_test,y_test)))\nprint('Classification report: \\n',classification_report(y_test,y_pred))","ec380ce0":"fig = plt.figure(figsize=(15,15))\n\nax1 = fig.add_subplot(3, 3, 1) # row, column, position\nax1.set_title('Logistic Regression Classification')\n\nax2 = fig.add_subplot(3, 3, 2) # row, column, position\nax2.set_title('KNN Classification')\n\nax3 = fig.add_subplot(3, 3, 3)\nax3.set_title('SVM Classification')\n\nax4 = fig.add_subplot(3, 3, 4)\nax4.set_title('Naive Bayes Classification')\n\nax5 = fig.add_subplot(3, 3, 5)\nax5.set_title('Decision Tree Classification')\n\nax6 = fig.add_subplot(3, 3, 6)\nax6.set_title('Random Forest Classification')\n\n\nsns.heatmap(data=lr_cm, annot=True, linewidth=0.7, linecolor='cyan', fmt='.0f', ax=ax1, cmap='BuPu')\nsns.heatmap(data=knn_cm, annot=True, linewidth=0.7, linecolor='cyan', fmt='.0f', ax=ax2, cmap='BuPu')   \nsns.heatmap(data=svm_cm, annot=True, linewidth=0.7, linecolor='cyan', fmt='.0f', ax=ax3, cmap='BuPu')\nsns.heatmap(data=nb_cm, annot=True, linewidth=0.7, linecolor='cyan', fmt='.0f', ax=ax4, cmap='BuPu')\nsns.heatmap(data=cart_cm, annot=True, linewidth=0.7, linecolor='cyan', fmt='.0f', ax=ax5, cmap='BuPu')\nsns.heatmap(data=rf_cm, annot=True, linewidth=0.7, linecolor='cyan', fmt='.0f', ax=ax6, cmap='BuPu')\nplt.show()","1ddaaa71":"**<h3>INTRODUCTION<\/h3>**\n* In this kernel, I will compare using sklearn classification algorithms.\n* The data set I use contains information about bank customers.\n\nContent:\n* [Summarize the Dataset](#1)\n* [Train Test separating the Dataset](#2)\n* [Logistic Regression Classification](#3)\n* [K-Nearest Neighbour (KNN) Classification](#4)\n* [Support Vector Machine (SVM) Classification](#5)\n* [Naive Bayes Classification](#6)\n* [Decision Tree Classification](#7)\n* [Random Forest Classification](#8)\n* [Comparison of Algorithms](#8)","1c146a35":"<a id=4><\/a>\n**<h3>K-Nearest Neighbour (KNN) Classification<\/h3>**","d99ffc4b":"<a id=3><\/a>\n**<h3>Logistic Regression Classification<\/h3>**","21d5b6d0":"<a id=6><\/a>\n**<h3>Naive Bayes Classification<\/h3>**","8428282a":"<a id=9><\/a>\n**<h3>Comparison of Algorithms<\/h3>**","ff338ea7":"<a id=7><\/a>\n**<h3>Decision Tree Classification<\/h3>**","84f2b326":"<a id=1><\/a>\n**<h3>Summarize the Dataset<\/h3>**\n* Now it is time to take a look at the data.","de2793fd":"<a id=8><\/a>\n**<h3>Random Forest Classification<\/h3>**","ed853b57":"<a id=2><\/a>\n**<h3>Train Test separating the Dataset<\/h3>**\n* Now separete target feature (y) from other features (x_data).","bd231ffa":"* we can make improvements in our model by playing with hypertuning parameters","03b83e75":"dataset seemed to me unbalanced. Unbalanced data can mislead us in the learning process.","1104b39a":"now let's delete the features i think i don't need","60347141":"<a id=5><\/a>\n**<h3>Support Vector Machine (SVM) Classification<\/h3>**","22ed981e":"Let's check the target variable now"}}