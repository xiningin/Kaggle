{"cell_type":{"48ed42ee":"code","af823f76":"code","64a0ebfd":"code","5e368247":"code","ea57f52a":"code","68ad46e8":"code","902076af":"code","d7695262":"code","a0fc964b":"code","c6bc823b":"code","f6882ffb":"code","7662419a":"code","b6efc297":"code","b2713a58":"code","c8dd1325":"code","d99c9dbc":"code","74e226ca":"code","463e8715":"code","a53555fb":"code","fbf0f011":"code","7f687a25":"code","cea01acc":"code","3145d3cc":"code","2ce689d0":"code","bb362e32":"code","9c63cb0d":"code","d2260d38":"code","e1d1b0d9":"code","019f636e":"code","de950817":"code","767715e6":"code","962891f2":"code","4f42aa68":"code","4e92ff80":"code","42ddef07":"code","7e8fe5b2":"code","63b6ebd6":"code","f5e7e67b":"code","19a81424":"code","77f8538e":"code","95535608":"markdown","b7686bbc":"markdown","86d3e5da":"markdown","20649f0b":"markdown","28e2d03d":"markdown","af75b27e":"markdown"},"source":{"48ed42ee":"import numpy as np \nimport pandas as pd \n\n# text processing libraries\nimport re\nimport string\nimport nltk\nfrom nltk.corpus import stopwords\n\n# XGBoost\nimport xgboost as xgb\nfrom xgboost import XGBClassifier\n\n# sklearn \nfrom sklearn import model_selection\nfrom sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.metrics import f1_score\nfrom sklearn import preprocessing, decomposition, model_selection, metrics, pipeline\nfrom sklearn.model_selection import GridSearchCV,StratifiedKFold,RandomizedSearchCV\n\n# matplotlib and seaborn for plotting\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# File system manangement\nimport os\n\n# Suppress warnings \nimport warnings\nwarnings.filterwarnings('ignore')\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","af823f76":"\nimport cufflinks as cf #importing plotly and cufflinks in offline mode  \nimport plotly.offline  \ncf.go_offline()  \ncf.set_config_file(offline=False, world_readable=True)","64a0ebfd":"import re\nfrom nltk.corpus import stopwords\nfrom nltk.stem.porter import PorterStemmer\nfrom nltk.stem import WordNetLemmatizer","5e368247":"train_df=pd.read_csv('\/kaggle\/input\/nlp-getting-started\/train.csv')\ntest_df=pd.read_csv('\/kaggle\/input\/nlp-getting-started\/test.csv')","ea57f52a":"train_df.head()","68ad46e8":"test_df.head()","902076af":"print(train_df.shape)\nprint(test_df.shape)","d7695262":"from pandas_profiling import ProfileReport\nprofile = ProfileReport(train_df, title='Pandas Profiling Report', html={'style':{'full_width':True}})","a0fc964b":"profile","c6bc823b":"location = train_df['location'].value_counts()[:20]\nlocation.iplot(kind='bar', xTitle='Number of Location', title=\"Location's In The Given Data Set\")","f6882ffb":"target = train_df['target'].value_counts()\ntarget.iplot(kind='bar', title=\"Disaster or Not Distribution\")","7662419a":"# A disaster tweet\ndisaster_tweets = train_df[train_df['target']==1]['text']\ndisaster_tweets.values[1]","b6efc297":"# A Non disaster tweet\ndisaster_tweets = train_df[train_df['target']==0]['text']\ndisaster_tweets.values[1]","b2713a58":"def clean_text(text):\n    '''Make text lowercase, remove text in square brackets,remove links,remove punctuation\n    and remove words containing numbers.'''\n    text = text.lower()\n    text = re.sub('\\[.*?\\]', '', text)\n    text = re.sub('https?:\/\/\\S+|www\\.\\S+', '', text)\n    text = re.sub('<.*?>+', '', text)\n    text = re.sub('[%s]' % re.escape(string.punctuation), '', text)\n    text = re.sub('\\n', '', text)\n    text = re.sub('\\w*\\d\\w*', '', text)\n    return text","c8dd1325":"train_df['text'] = train_df['text'].apply(lambda x: clean_text(x))\ntest_df['text'] = test_df['text'].apply(lambda x : clean_text(x))","d99c9dbc":"train_df['text'].head()","74e226ca":"test_df['text'].head()","463e8715":"tokenizer = nltk.tokenize.RegexpTokenizer(r'\\w+')\ntrain_df['text'] = train_df['text'].apply(lambda x: tokenizer.tokenize(x))\ntest_df['text'] = test_df['text'].apply(lambda x: tokenizer.tokenize(x))","a53555fb":"train_df['text'].head()","fbf0f011":"test_df['text'].head()","7f687a25":"def remove_stopwords(text):\n    \"\"\"\n    Removing stopwords belonging to english language\n    \n    \"\"\"\n    words = [w for w in text if w not in stopwords.words('english')]\n    return words\n\ntrain_df['text'] = train_df['text'].apply(lambda x : remove_stopwords(x))\ntest_df['text'] = test_df['text'].apply(lambda x : remove_stopwords(x))","cea01acc":"train_df['text'].head()","3145d3cc":"test_df['text'].head()","2ce689d0":"def combine_text(list_of_text):\n    '''Takes a list of text and combines them into one large chunk of text.'''\n    combined_text = ' '.join(list_of_text)\n    return combined_text\n\ntrain_df['text'] = train_df['text'].apply(lambda x : combine_text(x))\ntest_df['text'] = test_df['text'].apply(lambda x : combine_text(x))","bb362e32":"train_df.head()","9c63cb0d":"test_df.head()","d2260d38":"def text_preprocessing(text):\n    \"\"\"\n    Cleaning and parsing the text.\n\n    \"\"\"\n    tokenizer = nltk.tokenize.RegexpTokenizer(r'\\w+')\n    \n    nopunc = clean_text(text)\n    tokenized_text = tokenizer.tokenize(nopunc)\n    remove_stopwords = [w for w in tokenized_text if w not in stopwords.words('english')]\n    combined_text = ' '.join(remove_stopwords)\n    return combined_text","e1d1b0d9":"count_vectorizer = CountVectorizer()\ntrain_df_vectors = count_vectorizer.fit_transform(train_df['text'])\ntest_df_vectors = count_vectorizer.transform(test_df[\"text\"])\n\n## Keeping only non-zero elements to preserve space \nprint(train_df_vectors[0].todense())","019f636e":"# TFIDF Features (Term Frequency-Inverse Document Frequency, or TF-IDF for short)\n\ntfidf = TfidfVectorizer(min_df=2, max_df=0.5, ngram_range=(1, 2))\ntrain_df_tfidf = tfidf.fit_transform(train_df['text'])\ntest_df_tfidf = tfidf.transform(test_df[\"text\"])","de950817":"# Fitting a simple Logistic Regression on Counts\nclf = LogisticRegression(C=1.0)\nscores = model_selection.cross_val_score(clf, train_df_vectors, train_df[\"target\"], cv=5, scoring=\"f1\")\nscores","767715e6":"clf.fit(train_df_vectors, train_df[\"target\"])","962891f2":"# Fitting a simple Logistic Regression on TFIDF\n\nclf_tfidf = LogisticRegression(C=1.0)\nscores = model_selection.cross_val_score(clf_tfidf, train_df_tfidf, train_df[\"target\"], cv=5, scoring=\"f1\")\nscores","4f42aa68":"# Fitting a simple Naive Bayes on Counts\n\nclf_NB = MultinomialNB()\nscores = model_selection.cross_val_score(clf_NB, train_df_vectors, train_df[\"target\"], cv=5, scoring=\"f1\")\nscores","4e92ff80":"clf_NB.fit(train_df_vectors, train_df[\"target\"])","42ddef07":"# Fitting a simple Naive Bayes on TFIDF\n\nclf_NB_TFIDF = MultinomialNB()\nscores = model_selection.cross_val_score(clf_NB_TFIDF, train_df_tfidf, train_df[\"target\"], cv=5, scoring=\"f1\")\nscores","7e8fe5b2":"clf_NB_TFIDF.fit(train_df_tfidf, train_df[\"target\"])","63b6ebd6":"import xgboost as xgb\nclf_xgb = xgb.XGBClassifier(max_depth=7, n_estimators=200, colsample_bytree=0.8, \n                        subsample=0.8, nthread=10, learning_rate=0.1)\nscores = model_selection.cross_val_score(clf_xgb, train_df_vectors, train_df[\"target\"], cv=5, scoring=\"f1\")\nscores","f5e7e67b":"import xgboost as xgb\nclf_xgb_TFIDF = xgb.XGBClassifier(max_depth=7, n_estimators=200, colsample_bytree=0.8, \n                        subsample=0.8, nthread=10, learning_rate=0.1)\nscores = model_selection.cross_val_score(clf_xgb_TFIDF, train_df_tfidf, train_df[\"target\"], cv=5, scoring=\"f1\")\nscores","19a81424":"def submission(submission_file_path,model,test_vectors):\n    sample_submission = pd.read_csv(submission_file_path)\n    sample_submission[\"target\"] = model.predict(test_df_vectors)\n    sample_submission.to_csv(\"submission.csv\", index=False)","77f8538e":"submission_file_path = \"..\/input\/nlp-getting-started\/sample_submission.csv\"\ntest_df_vectors=test_df_tfidf\nsubmission(submission_file_path,clf_NB_TFIDF,test_df_vectors)","95535608":"**Train data set and Test data set**\n","b7686bbc":"XGBoost","86d3e5da":"Making the submission","20649f0b":"Bag of words","28e2d03d":"Building Model ","af75b27e":"**Profiling** is a process that helps us in understanding our data and PandasProfiling is python package which does exactly that. It is a simple and fast way to perform exploratory data analysis of a Pandas Dataframe. The pandas **df.describe()** and **df.info()** functions are normally used as a first step in the EDA process. However, it only gives a very basic overview of the data and doesn\u2019t help much in the case of large data sets. The Pandas Profiling function, on the other hand, extends the pandas DataFrame with **df.profile_report()** for quick data analysis. It displays a lot of information with a single line of code and that too in an interactive **HTML** report"}}