{"cell_type":{"733a081e":"code","ef2cd2c3":"code","88df55f6":"code","5441953b":"code","36e6c487":"code","0bfa761c":"code","3ef8bfae":"code","f4b78a47":"code","d30b8525":"markdown","62febda6":"markdown","ed2795f0":"markdown","e2debc9d":"markdown","6282e5ac":"markdown","fee67bbd":"markdown","65483795":"markdown","e219cd8d":"markdown","3f781f81":"markdown","1267d2a4":"markdown","3ec0e3c8":"markdown","d7c147dd":"markdown","21f86e99":"markdown","e4b586be":"markdown","cbace155":"markdown","c8bce82a":"markdown","086e7830":"markdown","6509e4aa":"markdown"},"source":{"733a081e":"import numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline \n\nfrom sklearn.metrics import log_loss\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.datasets import make_hastie_10_2\nfrom sklearn.model_selection import train_test_split\n\ndef sigmoid(x):\n    return 1 \/ (1 + np.exp(-x))","ef2cd2c3":"X_all, y_all = make_hastie_10_2(random_state=0)","88df55f6":"# For convenience we will use sklearn's GBM, the situation will be similar with XGBoost and others\nclf = GradientBoostingClassifier(n_estimators=5000, learning_rate=0.01, max_depth=3, random_state=0)\nclf.fit(X_train, y_train)\n\ny_pred = clf.predict_proba(X_test)[:, 1]\nprint(\"Test logloss: {}\".format(log_loss(y_test, y_pred)))","5441953b":"def compute_loss(y_true, scores_pred):\n    '''\n        Since we use raw scores we will wrap log_loss \n        and apply sigmoid to our predictions before computing log_loss itself\n    '''\n    return log_loss(y_true, sigmoid(scores_pred))\n    \n\n'''\n    Get cummulative sum of *decision function* for trees. i-th element is a sum of trees 0...i-1.\n    We cannot use staged_predict_proba, since we want to maniputate raw scores\n    (not probabilities). And only in the end convert the scores to probabilities using sigmoid\n'''\ncum_preds = np.array([x for x in clf.staged_decision_function(X_test)])[:, :, 0] \n\nprint (\"Logloss using all trees:           {}\".format(compute_loss(y_test, cum_preds[-1, :])))\nprint (\"Logloss using all trees but last:  {}\".format(compute_loss(y_test, cum_preds[-2, :])))\nprint (\"Logloss using all trees but first: {}\".format(compute_loss(y_test, cum_preds[-1, :] - cum_preds[0, :])))","36e6c487":"# Pick an object of class 1 for visualisation\nplt.plot(cum_preds[:, y_test == 1][:, 0])\n\nplt.xlabel('n_trees')\nplt.ylabel('Cumulative decision score');","0bfa761c":"clf = GradientBoostingClassifier(n_estimators=5000, learning_rate=8, max_depth=3, random_state=0)\nclf.fit(X_train, y_train)\n\ny_pred = clf.predict_proba(X_test)[:, 1]\nprint(\"Test logloss: {}\".format(log_loss(y_test, y_pred)))","3ef8bfae":"cum_preds = np.array([x for x in clf.staged_decision_function(X_test)])[:, :, 0] \n\nprint (\"Logloss using all trees:           {}\".format(compute_loss(y_test, cum_preds[-1, :])))\nprint (\"Logloss using all trees but last:  {}\".format(compute_loss(y_test, cum_preds[-2, :])))\nprint (\"Logloss using all trees but first: {}\".format(compute_loss(y_test, cum_preds[-1, :] - cum_preds[0, :])))","f4b78a47":"for f in categorical_feats:\n    # Encode features from top to bottom\n    traintest[f] = traintest[f].factorize()\n\ntraintest.T.drop_duplicates()","d30b8525":"## Feature preprocessing and generation with respect to models\n\n### Overview\n\n* Feature preprocessing is often necessary\n* Feature generation is a powerful technique\n* Preprocessing and generation pipelines depend on a model type\n\n### Numeric features\n\n* **Preprocessing**\n    * Scaling\n        * We use preproccessing to scale all features to one scale, so that their initial impact on the model will be roughly similar.\n        * Important for non-tree-based methods\n        * Types of scaling:\n            * To \\[0,1\\]: sklearn.preprocessing.MinMaxScaler\n            * To mean = 0, std=1: sklearn.preprocessing.StandardScaler\n         \n     * Outliers\n         * To protect linear models from outliers, we can clip features values between two chosen values of lower bound and upper bound. We can choose them as some percentiles of that feature (ex.: 1st and 99th percentiles = Winsorization)\n         \n     * Rank transformations\n         * scipy.stats.rankdata\n         * Setting spaces between proper assorted values to be equal.\n         * It is a better option than MinMaxScaler if we have outliers, because rank transformation will move the outliers closer to other objects\n         * Good for:\n             * Linear models\n             * KNN\n             * Neural networks\n             \n         * To apply to the test data, you need to store the created mapping from feature values to their rank values. Alternatively, you can concatenate train and test data before applying the rank transformation\n         \n     * Other preprocessing for non-tree models (mainly NNs):\n         * Log transform: np.log(1+x)\n         * Raining to the power < 1: np.sqrt(x + 2\/3)\n         * Both these transformations are useful because they drive two big values closer to the feature's average value.\n         * Along with this, the values near zero are becoming a bit more distinguishable\n     \n     * General considerations\n         * Sometimes it is benefitial to train a model on concatenated data frames produced by different preprocessings or to mix models training differently-preprocessed data (linear models, KNNs and NN will benefit hugely from this)\n\n* **Feature generation**\n    * Creativity and data understanding are the keys to productive feature generation\n\n\n### Categorical and ordinal features\n\n* Difference between categorical ordinal feature and numeric feature: we don't know the exact size of the difference between classes\n\n* Label encoding\n    * Types of encoding:\n        * Alphabetical (sorted): sklearn.preprocessing.LabelEncoder\n        * Order of appearance: Pandas.factorize\n    \n* Frequency encoding\n    * Encoding features via mapping values to their frequencies\n    * This will preserve some information about values distribution\n    * It can help:\n        * Non-tree-based models. Example: if frequency of category is correlated with target value, linear model will utilize this dependency\n        * Tree-based models: less number of split because of the same reason\n    * Bad thing: if you have multiple categories with the same feature, they won't be distinguishable\n\n* One-hot encoding\n    * Works good for linear methods, kNN and neural networks\n    * Features are always scaled\n    * Tree-methods may slow down\n    * Options:\n        * pandas.get_dummies\n        * sklearn.preprocessing.OneHotEncoder\n    * To store these new 1-0s arrays efficiently, we must know about **sparse matrices**:\n        * Instead of allocating space in RAM for every element of an array, we can store only non-zero elements and thus save a lot of memory\n        * It is useful if non-zero values are far less than half of all the values\n        * Useful with categorical features or text data\n        \n### Datetime and coordinates\n\n* Date and time can generate new features of some categories:\n    * Time moments in a period\n        * Day number in week, month, season, year, second, minute, hour\n     * Time since\n         * Row-independent moment. For example: since 00:00:00 UTC, 1 January 1970\n         * Row dependent important moment. For example: number of days left until next holidays\/time passed after last holiday\n     * Difference between dates\n         * datetime_feature_1 - datetime_feature_2\n\n* Coordinates\n    * Interesting places from train\/test data or additional data\n    * Centers of clusters\n    * Aggregated statistics\n    \n### Handling missing values\n\n* Types:\n    * Not number\n    * Empty strings\n    * Outliers (-999)\n\n* How to FIND missing values that might be hidden: plot the histogram. Maybe they were replaced by the dataset mean\n\n* Filling approaches:\n    * -999, -1, etc\n        * Gives tree models the possibility to take missing values into a separate category\n        * Performance of linear models and neural networks can suffer\n    \n    * Mean, median:\n        * Good for non-tree models, bad for tree models\n        \n    * Create isnull feature\n        * Solves problem for both tree and non-tree models\n        * Downside: double number of columns\n    \n    * Reconstruction:\n        * When there is a logical order, it is easy to reconstruct (Example: measures taken everyday may be imputed based on nearby values)\n        * In general,avoid filling NaNs before feature generation\n        * XGBoost can handle NaNs\n        ","62febda6":"That is it! Now we see, that it is crucial to have the first tree in the ensemble!","ed2795f0":"# Week 1: Introduction and Recap\n\n## Competition Mechanics\n\n**Example of competition mechanics**:   \n1. Analize data   \n2. Fit model   \n3. Submit\n4. See public score\n5. Repeat 1-4\n   \n**Platforms for competitions**:\n* Kaggle\n* DrivenData\n* CrowdAnalityx\n* CodaLab\n* DataScienceChallenge.net\n* Datascience.net\n* Single-competition sites (like KDD, VizDooM)\n\n### Real World Application vs. Competitions\n\n**Real world ML Pipeline**\n* Understanding of business problem\n* Problem formalization\n* Data collecting\n* Data preprocessing\n* Modelling\n* Way to evaluate model in real life\n* Way to deploy model\n\n** In competition:**\n* No problem formalization necessary\n* No need to choose target metric\n* No deployment issues\n* No inference speed problem\n\n**Phylosophy of competitions**\n* It's not only about algorithms\n    * It's all about data and making things work, not about algorithms themselves\n        * Everyone can and will tune classic approaches\n        * We need some insights to wil\n    * Sometimes there is no ML\n\n**Do not limit yourself**\n* It is ok to use\n    * Heuristics\n    * Manual data analysis\n* Do not be afraid of\n    * Complex solutions\n    * Advanced feature engineering\n    * Doing huge calculation\n","e2debc9d":"* Duplicated and constant features (cont.)\n    * remove duplicated rows\n\n* Check if dataset is shuffled\n    * If it is not, there is a high chance to find data leakage","6282e5ac":"## Validation and overfitting\n\n* Sometimes you see people jumping down on leaderboard after revealing private results. Reasons for that:\n    * Competitors could ignore the validation and select the submission which scored best against the public leaderboard\n    * Sometimes competitions have no consistent public\/private data split or they have too little data in either public or private leaderboard\n\n* Overfitting definitions:\n    * **In general**: the model is overfitted if its quality on the train set is better than on the test set (capturing noise or capturing patterns which do not generalize to test data.\n    * **In competitions**: the models are overfitted only in case when quality on the test set will be worse than we have expected\n\n### Validation strategies\n\n* Holdout: ngroups = 1\n    * Dividing training set into training and validation\n    * sklearn.model_selection.ShuffleSplit\n\n* K-fold: ngroups = k\n    * Good when you have a medium-size sample\n    * Averaging results for several \"holdouts\"\n    * sklearn.model_selection.Kfold\n    \n* Leave-one-out: ngroups = len(train)\n    * Special case of k-fold where k is the size of the train set - 1\n    \n* Stratification vs. shuffling\n    * Stratification is useful for:\n        * Small datasets\n        * Unbalanced datasets\n        * Multiclass classification\n\n### Data splitting strategies\n* Different splitting strategies can differ significantly\n    * In generated features\n    * In a way the model will rely on those features\n    * In some kind of target leak\n    \n* **We need to identify train\/test split made by organizers and reproduce it!**\n\n* Most common types of splits:\n    * Random, by row\n    * Timewise\n        * Signal to use feature generation, specially features based on the target\n        * Example: if we are to predict a number of customers for the shop for each day in the next week, we can come up with something like the number of customers for the same day in the previous week, or the average number of customers for the past month\n    * By id\n        * Sometimes id is hidden and you have to derive it!\n    * Combined\n    \n### Problems occuring during validation\n* Validation stage\n    * Generally, the main problem is a significant difference in scores and optimal parameters for different train validation splits\n    * Causes:\n        * Too little data\n        * Data is too diverse and inconsistent\n    * We should do extensive validation:\n        * Average scores from different k-fold splits\n        * Tune model on one split, evaluate score on the other\n\n* Submission stage\n    * LB score is consistently higher\/lower than validation score\n    * LB score is not correlated with validation score at all\n    * Investigating:\n        * We may already have quite different scores in kfold\n        * Calculate mean and sd of kfold to see if the value in the LB is expected\n    * Other reasons for problems:\n        * Too little data in public leaderboard\n        * Incorrect train\/test split\n        * Train and test data are from different distributions\n            * Try to find the bias that separates those distributions and keep testing having the LB score as metric\n\n* Expect LB shuffle because of \n   * Randomness: competitors have very similar scores (high or low)\n   * Little amount of data\n   * Different public\/private distributions","fee67bbd":"# Week 3: Metrics optimization and Feature Engineering\n\n## Metrics optimization\n\n### Motivation\n* Used to evaluate submissions\n* If your model is scored with some metric, you get best results by optimizing exactly that metric\n\n### Regression metrics review\n* MSE: Mean Square Error\n    * Most common metric for regression\n    * In data science, people use it when they don't have any specific preferences for the solution to their problem, or when they don't know other metric\n    * The constant value for prediction that optimizes MSE is the average of all values is the mean value of the target column\n        \n* RMSE: Root Mean Square Error\n    * The square root is introduced to make scale of the errors to be the same as the scale of the targets\n    * RMSE vs. MSE\n        * Similar in terms of their minimizers\n        * MSE(a) > MSE(b) <=> RMSE(a) > RMSE(b)\n        * MSE is easier to work with\n        * Difference between the two for gradient-based models: traveling along MSE gradient is equivalent to traveling along RMSE gradient but with a different learning rate. The learning rate depends on MSE score itself\n        \n* R-squared\n    * MSE divided by a constant and subtracted from another constant\n    * By optimizing R-square, we optimize MSE\n   \n* MAE: Mean Absolute Error\n    * Error is calculated as ana verage of absolute differences between the target values and the predictions\n    * It penalizes huge errors not as badly as MSE does\n    * It is not that sensitive to outliers as mean square error\n    * Usually used in finance\n    * Optimal constant: median of target\n    \n* MAE vs. MSE:\n    * Do you havev outliers in the data? Use MAE\n    * Are you sure they are outliers? Use MAE\n    * Or they are just unexpected values we should still care about? Use MSE\n     \n* From MSE and MAE to MSPE and MAPE\n    * Mean Square Percentage Error and Mean Absolute Percentage Error\n    * Relative error can be more important for us (example: off by one errors)\n    * Optimized constants:\n        * MSPE: weighted mean of the target\n        * MAPE: weighted median of the target\n        \n* RMSLE: Root Mean Square Logarithmic Error\n    * I is just an RMSE calculated in logarithmic scale\n    * Usually used in the same situation as MSPE and MAPE\n    * Optimal constant: weighted mean in the log space\n\n### Classification metrics review\n\n* Accuracy score\n    * How frequently our class prediction is correct\n    * Best constant: the most frequent class\n    * Issues:\n        * Bad for dealing with unbalanced classes\n        * Doesn't care about how confident the classifier is in the predictions and what soft predictions are\n\n* Logarithmic loss (logloss)\n    * It tries to make the classifier to output two posterior probabilities for their objects to be of a certain kind, of a certain class\n    * Best constant: set ai to frequency of the ith class\n    \n* Area Under Curve (AUC ROC)\n    * Only for binary tasks\n    * Depends only on ordering of the predictions, not on absolute values\n    * Several explanations:\n        * Area under curve\n            * Receiving Operation Curve (ROC)\n            * Curve: x -> FP, y -> TP\n        * Pairs ordering\n            * AUC = # correctly ordered pairs \/ total number of pairs\n            * AUC = 1 - # incorrectly ordered pairs \/ total\n     * Best constant: all constants give the same score\n     * Random predictions lead to AUC 0.5\n\n* Cohen's Kappa\n    * Change the baseline (remember the example that had 90% cats, now the baseline will be 0.9 - new zero)\n    ","65483795":"## LAB: Will performance of GBDT model drop dramatically if we remove the first tree?","e219cd8d":"## Software\/Hardware Requirements\n\n**Hardware**\n* RAM: If you can keep data in memory, everything will be much easies\n* Cores: more cores you have, more (or faster) experiments you can do\n* Storage: SSD is crucial if you work with images or big datasets with a lot of small pieces\n\n**Cloud resources**\n* Amazon AWS (spot option! -> usually cheaper)\n* Microsoft Azure\n* Google Cloud\n\n**Software**\n* Language: Python\n* Libraries:\n    * NumPy\n    * Pandas\n    * Scikit learn\n    * Matplotlib","3f781f81":"**To prove the point**, let's try a larger learning rate of $8$.","1267d2a4":"### Learn GBM\n\nBut we will need 800 trees in GBM to classify it correctly.","3ec0e3c8":"### Make dataset","d7c147dd":"# Week 2: Exploratory Data Analysis\n\n## Exploratory data analysis (EDA)\n\n* EDA: what and why?\n\n    * EDA allows to:\n        * Better understand the data\n        * Build an intuition about the data\n        * Generate hypothesis\n        * Find insights\n    * Top solutions always use advanced and aggressive modeling, but they usually have something more than that\n    * Visualizations:\n        * Visualization -> idea or idea -> visualization\n        * May help identify leaks (and it is allowed to use leaks in competitions)\n    * Do EDA first. Do not immediately dig into modelling\n\n## Building intuition about the data\n\n* Getting domain knowledge\n    * It helps to deeper understand the problem\n* Check if the data is intuitive\n    * If you find some kind of error on the dataset, it may be used to help you achieve better scores (finding patterns in errors)\n* Understand how the data was generated\n    * It is crucial to understand the generation process to set up a proper validation scheme\n\n## Exploring anonymized data\n\n* What is anonymized data\n    * Hashing sensitive data\n    * **Text data**: replacing all word occurences with hash values of those words\n    * **Tabular data**: hiding information on each column with dummies\n\n* What can we do?\n    * Explore individual features\n        * Guess the meaning of the columns\n            * Trying to scale back to normal\n            * Finding numerical difference between unique values of features\n        * Guess the types of the columns\n            * Helpful functions (Pandas):\n                * df.dtypes\n                * df.info()\n                * x.values_counts()\n                * x.is_null()\n            * Each type needs its own preprocessing\n    * Visualizations:\n        * EDA is an art: and visualization are our art tools\n        * Histograms\n            * plt.hist(x)\n            * Split feature edge into bins and show how many poits fall into each bin\n            * May be misleading in some cases, so try to overwrite its number of bins when using it\n            * It aggregates in the data, so we cannot see if all values are unique or there are a lot of repeated values\n            * If you have a hypothesis, try to make several plots to prove it\n            * **Peaks in histograms**: organizers filled the missing values with the mean values for us\n            \n        * Plot index versus value:\n            * plt.plot(x, '.')\n            * If there are horizontal lines on this kind of plot, we understand there are a lot of repeated values in this features\n            * The randomness over the indices means that the data is properly shuffled\n            * We can plot point colors by class and see if features are sorted by class\n            \n        * Feature statistics:\n            * Pandas df.describe()\n            * x.mean()\n            * x.var()\n        \n        * Other tools\n            * x.value_counts()\n            * x.isnull()\n            \n    * Explore feature relations\n        * Find relations between pairs\n            * Scatter plot (color coding points with labels, for classification; heat maps for regression)\n            * Scatter plot may also be used to check if train and test sets are the same\n            * Pandas: pd.scatter_matrix(df)\n            * Correlation plots\n            * New possible features:\n                * How many times one feature is larger than the other?\n                * How many rows are there such the value of the first feature is larger than the value of the second one?\n                * How many distinct combinations the features have in the dataset?\n            \n        * Find feature groups\n            * Corrplot + clustering \n            * Plotting features vs. feature mean (or other statistics) and ordering by feature mean\n            \n## Dataset cleaning and other things to check\n\n* Duplicated and constant features\n    * Sometimes, the organizers could give us a fraction of objects they have\n    * That is why sometimes we can encounter a feature that takes the same value for every object in both train and test\n    * I tis not useful just to occupy some memory, so we are about to remove such constant features\n    * traintest.nunique(axis=1) == 1\n    * traintest.T.drop_duplicates()\n    * Check if categorical features are the same, only with different labels:","21f86e99":"## Recap of main ML algorithms\n\n### Liner models\n* Separating objects with a plane\n* Examples:\n    * Logistic regression\n    * Support Vector Machines\n* Good for sparse high-dimensional data \n* Limitations:\n    * Sets of points that form rings for instance\n* Libraries:\n    * Scikit learn\n    * Vowpal Wabbit: good for handling really large data sets\n    \n### Tree-based methods\n* Decision tree as basic block for building more complicated models\n* Examples:\n    * Decision tree\n    * Random forest\n    * GBDT = Gradient Boosting Decision Trees\n* Good default method for tabular data\n* Libraries:\n    * Scikit learn (recommended for Random Forests)\n    * XGBoost (recommended for Gradient Boosting)\n    * Microsoft LightGBM (recommended for GB)\n\n### k-Nearest Neighbours\n* Features based on nearest neighbours are often very informative\n* Library:\n    * Scikit learn (recommended)\n    \n### Neural Networks\n* Especially good for images, sounds, text and sequences\n* Libraries:\n    * TensorFlow\n    * PyTorch (recommended)\n    * MXNet\n    * Lasagne\n    \n### No free lunch\n* There is no method which outperforms all others for all tasks\n* For every method we can construct a task for which this particular method will not be the best","e4b586be":"See, the decision function improves almost linearly untill about 800 iteration and then stops. And the slope of this line is connected with the learning rate, that we have set in GBM! \n\nIf you remember the main formula of boosting, you can write something like:\n    $$ F(x) = const + \\sum\\limits_{i=1}^{n}\\gamma_i h_i(x) $$\n\nIn our case, $\\gamma_i$ are constant and equal to learning rate $\\eta = 0.01$. And look, it takes about $800$ iterations to get the score $8$, which means at every iteration score goes up for about $0.01$. It means that first 800 terms are approximately equal to $0.01$, and the following are almost $0$. \n\nWe see, that if we drop the last tree, we lower $F(x)$ by $0$ and if we drop the first tree we lower $F(x)$ by $0.01$, which results in a very very little performance drop.  \n\nSo, even in the case of simple dataset which can be solved with single decision stump, in GBM we need to sum a lot of trees (roughly $\\frac{1}{\\eta}$) to approximate this golden single decision stump.","cbace155":"## Data Leakages\n\n### Basic data leaks\n* What is leakage?\n    * An unexpected information in the data that allows us to make unrealistically good predictions\n    \n* Leaks in time series\n    * Split should be done on time\n        * In real life we don't have information from future\n        * In competitions first thing to look: train\/public\/private split, is it on time?\n    * Wven when split by time, features may contain information about future\n        * User history in CTR tasks\n        * Weather\n\n* Unexpected information\n    * Metadata\n    * Information on IDs\n    * Row order\n    \n### Leaderboard probing and examples of rare data leaks\n\n* Types of LB probing:\n    * Simply extracting all ground truth from the public part of the leaderboard\n    * Submitting predictions in such a way that will give out information about private data. \n    \n### Programing assignment: Data Leakages\n\n* Incidence matrix\n    * It comes from Graph Theory (https:\/\/en.wikipedia.org\/wiki\/Incidence_matrix)\n    * The incidence matrix is a matrix of size (maxId + 1, maxId + 1), where each row (column) i corresponds i-th Id. In this matrix we put the value 1 to the position [i, j], if and only if a pair (i, j) or (j, i) is present in a given set of pais (FirstId, SecondId). All the other elements in the incidence matrix are zeros.\n    \n### Final project advice\n\n* A good exercise is to reprocduce previous_value_benchmark\n    * For each shop\/item pair our predictions are just monthly sales from the previous month, i.e. October 2015\n    * Most important step at reproducing this score: correctly aggregating daily data and constructing monthly sales data frame\n    * You need to get lagged values, fill NaNs with zeros and clip the values into [0,20] range\n* If you do it correctly, you'll get precisely 1.16777 on the public leaderboard\n","c8bce82a":"# Intro\n\nThis is a notebook for class notes and some labs from the Coursera Course \"How to Win a Kaggle Competition\"","086e7830":"You can see that there is a difference, but not as huge as one could expect! Moreover, if we get rid of the first tree \u2014 overall model still works!\n\nIf this is supprising for you \u2014 take a look at the plot of cummulative decision function depending on the number of trees.","6509e4aa":"## Feature extraction from text and images\n\n### Bag of words\n\n* If we have text and image feaures as additional data, we usually must grasp different features, which can be edited as complementary to our main dataframe of samples and features\n\n* Sometimes we just want to add new features to existing dataframe. Sometimes, we even might want to use the new features independently, and in end make stake in with the base solution\n\n* Text to vector:\n    * Bag of words:\n        * \"The dog is on the table\"\n        * are=0; cat=0; dog=1, is=1 ...\n        * count the number of occurences\n        * sklearn.feature_extraction.text.CountVectorizer\n        * Term Frequency Transformation: TFiF\n            * term frequency = tf = 1\/x.sum(axis=1) [:,None]\n            * x = x*tf\n            * It is better to register frequencies so we can compare texts of different lenghts\n            * inverse document frequency = idf = np.log(x.shape[0] \/ (x>0).sum(0))\n            * x = x*idf\n            * Common words will be more important\n            * sklearn.feature_extraction.text.TfidfVectorizer\n            \n    * N-grams:\n        * sklearn.feature_extraction.text.CountVectorizer\n        \n    * Text preprocessing\n        * lowercase\n        * lemmatization and stemming:\n            * I had a car = I have car\n            * We have cars = We have car\n            * Example: democracy, democratic, democratization:\n                * stemming: democr\n                * lemmatization: democracy\n        * stopwords\n            * Words which do not contain important information for our model\n                * Articles or prepositions\n                * Very common words\n            * Most languages have predefined stopwords \n            \n    * Pipeline for applying bag of words:\n        * Preprocessing\n        * Ngrams can help to use local context\n        * Postprocessing: TFiDF\n\n### Word2vec, CNN\n\n* Vector representations of words and text\n* Differences between vectors may have meanings\n* Several implementations:\n    * Words: Word2vec, Glove, FastText, etc\n    * Sentences: Doc2vec, etc\n    * There are pretrained models\n\n* Differences between BOW and w2v:\n    * Bag of words\n        * Very large vectors\n        * Meaning of each value in vector is known\n    * Word2vec\n        * Relatively small vectors\n        * Values in vector can be interpreted only in some cases\n        * The words with similar meaning often have similar embeddings\n    * The two approaches may be used both in your solution\n\n* Image to vector:\n    * Descriptors from layers of CNN\n    * Train network from scratch\n    * Finetuning is usually better for small datasets than training a network from scratch\n    * Image augmentation:\n        * Way of increasing model training\n        * Rotating images from training set\n        * Avoids overfitting    "}}