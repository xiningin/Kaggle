{"cell_type":{"fab4e391":"code","2f999d9f":"code","1b931659":"code","c812b967":"code","9583dd0d":"code","d517ebe3":"code","b1c3c7af":"code","b937f071":"code","d0b755ec":"code","8532ebc7":"code","b8b3e37a":"code","02c98add":"code","b1237592":"code","f4c57d36":"code","564a0758":"code","3cb1c4b1":"code","da0a01bd":"code","4ba60eb8":"code","0f5b5241":"code","d3ba0abe":"code","8e6c5fb6":"code","b12397cf":"code","d90b75f0":"code","f8c64106":"code","66349ee3":"code","f972f70a":"markdown","cf28503a":"markdown","73eb5779":"markdown","303d095d":"markdown","702855fe":"markdown","7f60cdda":"markdown","e9dd2683":"markdown","96a12b3c":"markdown","1bd89747":"markdown","fe1d6172":"markdown","4e8a256b":"markdown","b42263ed":"markdown","1c2896bb":"markdown","5b4c06c2":"markdown","3cfab1ce":"markdown","b46249fc":"markdown","7d16d361":"markdown","c1415312":"markdown","82bf8c3e":"markdown","91cc374c":"markdown","4c31e1cf":"markdown","89d92910":"markdown","9735e1fe":"markdown"},"source":{"fab4e391":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","2f999d9f":"#Basic\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom matplotlib import rcParams\nfrom matplotlib.cm import rainbow\n%matplotlib inline\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# Other libraries\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\n\n# Machine Learning\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier","1b931659":"import sklearn","c812b967":"sklearn.__version__","9583dd0d":"dataset = pd.read_csv(\"\/kaggle\/input\/heart-disease-uci\/heart.csv\")","d517ebe3":"dataset.info()","b1c3c7af":"dataset.head()","b937f071":"import seaborn as sns","d0b755ec":"plt.rcParams['figure.figsize'] = (15, 12)\nsns.heatmap(dataset.corr(), cmap='gray', annot=True, cbar=False)\nplt.show()","8532ebc7":"rcParams['figure.figsize'] = 20, 14\nplt.matshow(dataset.corr())\nplt.yticks(np.arange(dataset.shape[1]), dataset.columns)\nplt.xticks(np.arange(dataset.shape[1]), dataset.columns)\nplt.colorbar()","b8b3e37a":"dataset.hist()","02c98add":"rcParams['figure.figsize'] = 8,6\nplt.bar(dataset['target'].unique(), dataset['target'].value_counts(), color = ['red', 'green'])\nplt.xticks([0, 1])\nplt.xlabel('Target Classes')\nplt.ylabel('Count')\nplt.title('Count of each Target Class')","b1237592":"dataset.columns","f4c57d36":"dataset = pd.get_dummies(dataset, columns = ['sex', 'cp', 'fbs', 'restecg', 'exang', 'slope', 'ca', 'thal'])\nstandardScaler = StandardScaler()\ncolumns_to_scale = ['age', 'trestbps', 'chol', 'thalach', 'oldpeak']\ndataset[columns_to_scale] = standardScaler.fit_transform(dataset[columns_to_scale])","564a0758":"dataset.head()","3cb1c4b1":"y = dataset['target']\nX = dataset.drop(['target'], axis = 1)\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 42)","da0a01bd":"knn_scores = []\nfor k in range(1,21):\n    knn_classifier = KNeighborsClassifier(n_neighbors = k)\n    knn_classifier.fit(X_train, y_train)\n    knn_scores.append(knn_classifier.score(X_test, y_test))","4ba60eb8":"plt.figure(figsize=(25,10))\nplt.plot([k for k in range(1, 21)], knn_scores, color = 'red')\nfor i in range(1,21):\n    plt.text(i, knn_scores[i-1], (i, round(knn_scores[i-1],4)))\nplt.xticks([i for i in range(1, 21)])\nplt.xlabel('Number of Neighbors (K)')\nplt.ylabel('Scores')\nplt.title('K Neighbors Classifier scores for different K values')","0f5b5241":"import pickle\n\nknn_classifier = KNeighborsClassifier(n_neighbors = 8)\nknn_classifier.fit(X_train, y_train)\n\n#Save model\nfilename = 'knn.pkl'\npickle.dump(knn_classifier, open(filename, 'wb'))","d3ba0abe":"svc_scores = []\nkernels = ['linear', 'poly', 'rbf', 'sigmoid']\nfor i in range(len(kernels)):\n    svc_classifier = SVC(kernel = kernels[i])\n    svc_classifier.fit(X_train, y_train)\n    svc_scores.append(svc_classifier.score(X_test, y_test))","8e6c5fb6":"colors = rainbow(np.linspace(0, 1, len(kernels)))\nplt.bar(kernels, svc_scores, color = colors)\nfor i in range(len(kernels)):\n    plt.text(i, svc_scores[i], round(svc_scores[i],4))\nplt.xlabel('Kernels')\nplt.ylabel('Scores')\nplt.title('Support Vector Classifier scores for different kernels')","b12397cf":"dt_scores = []\nfor i in range(1, len(X.columns) + 1):\n    dt_classifier = DecisionTreeClassifier(max_features = i, random_state = 0)\n    dt_classifier.fit(X_train, y_train)\n    dt_scores.append(dt_classifier.score(X_test, y_test))","d90b75f0":"plt.figure(figsize=(25,10))\n\nplt.plot([i for i in range(1, len(X.columns) + 1)], dt_scores, color = 'green')\nfor i in range(1, len(X.columns) + 1):\n    plt.text(i, dt_scores[i-1], (i, round(dt_scores[i-1],4)))\nplt.xticks([i for i in range(1, len(X.columns) + 1)])\nplt.xlabel('Max features')\nplt.ylabel('Scores')\nplt.title('Decision Tree Classifier scores for different number of maximum features')","f8c64106":"rf_scores = []\nestimators = [10, 100, 200, 500, 1000]\nfor i in estimators:\n    rf_classifier = RandomForestClassifier(n_estimators = i, random_state = 0)\n    rf_classifier.fit(X_train, y_train)\n    rf_scores.append(rf_classifier.score(X_test, y_test))","66349ee3":"colors = rainbow(np.linspace(0, 1, len(estimators)))\nplt.bar([i for i in range(len(estimators))], rf_scores, color = colors, width = 0.8)\nfor i in range(len(estimators)):\n    plt.text(i, rf_scores[i], round(rf_scores[i],4))\nplt.xticks(ticks = [i for i in range(len(estimators))], labels = [str(estimator) for estimator in estimators])\nplt.xlabel('Number of estimators')\nplt.ylabel('Scores')\nplt.title('Random Forest Classifier scores for different number of estimators')","f972f70a":"# Support Vector Classifier\n\n*This classifier aims at forming a hyperplane that can separate the classes as much as possible by adjusting the distance between the data points and the hyperplane. There are several kernels based on which the hyperplane is decided. I tried four kernels namely, linear, poly, rbf, and sigmoid.*","cf28503a":"*It\u2019s easy to see that there is no single feature that has a very high correlation with our target value. Also, some of the features have a negative correlation with the target value and some have positive.*","73eb5779":"*Taking a look at the bar graph, we can see that the maximum score of 86.88% was achieved for both 100, 200,500 and 1000 trees*","303d095d":"*As can be seen from the plot above, the rbf kernel performed the best for this dataset and achieved a score of 90.16%.*","702855fe":"*From the line graph above, we can clearly see that the maximum score is 88.52% and is achieved for maximum features being selected to be 4.*","7f60cdda":"# Conclusion\n\n*The project involved analysis of the heart disease patient dataset with proper data processing. Then, 4 models were trained and tested with maximum scores as follows:*\n\n1. K Neighbors Classifier: 91.8%\n2. Support Vector Classifier: 90.16%\n3. Decision Tree Classifier: 88.52%\n4. Random Forest Classifier: 86.88%","e9dd2683":"*For x-axis I used the unique() values from the target column and then set their name using xticks. For y-axis, I used value_count() to get the values for each class. I colored the bars as green and red.*","96a12b3c":"*I used the rainbow method to select different colors for each bar and plot a bar graph of the scores achieved by each.*","1bd89747":"*Once we have the scores, we can then plot a line graph and see the effect of the number of features on the model scores.*","fe1d6172":"# Import dataset","4e8a256b":"# Bar Plot for Target Class\n\n*It\u2019s really essential that the dataset we are working on should be approximately balanced. An extremely imbalanced dataset can render the whole model training useless and thus, will be of no use. Let\u2019s understand it with an example.*","b42263ed":"*Next, I plot these scores across a bar graph to see which gave the best results. You may notice that I did not directly set the X values as the array [10, 100, 200, 500, 1000]. It will show a continuous plot from 10 to 1000, which would be impossible to decipher. So, to solve this issue, I first used the X values as [1, 2, 3, 4, 5]. Then, I renamed them using xticks.*","1c2896bb":"# Decision Tree Classifier\n\n*This classifier creates a decision tree based on which, it assigns the class values to each data point. Here, we can vary the maximum number of features to be considered while creating the model. 'i' range features from 1 to 30 (the total features in the dataset after dummy columns were added).*","5b4c06c2":"# Histogram\n\n*The best part about this type of plot is that it just takes a single command to draw the plots and it provides so much information in return. Just use dataset.hist().*","3cfab1ce":"# Data Processing\n\n* To work with categorical variables, we should break each categorical column into dummy columns with 1s and 0s.\n\n* Let\u2019s say we have a column Gender, with values 1 for Male and 0 for Female. It needs to be converted into two columns with the value 1 where the column would be true and 0 where it will be false. Take a look at the Gist below.\n\n* To work with categorical variables, we should break each categorical column into dummy columns with 1s and 0s.\n\n* Let\u2019s say we have a column Gender, with values 1 for Male and 0 for Female. It needs to be converted into two columns with the value 1 where the column would be true and 0 where it will be false. Take a look at the Gist below.","b46249fc":"# Understanding the data\n\n*Correlation Matrix*\n\n*To begin with, let\u2019s see the correlation matrix of features and try to analyse it. The figure size is defined to 12 x 8 by using rcParams. Then, I used pyplot to show the correlation matrix. Using xticks and yticks, I\u2019ve added names to the correlation matrix. colorbar() shows the colorbar for the matrix.*","7d16d361":"# Machine Learning\n\n*In this project, I took 4 algorithms and varied their various parameters and compared the final models. I split the dataset into 80% training data and 20% testing data.*","c1415312":"*Plot a line graph of the number of neighbors and the test score achieved in each case.*","82bf8c3e":"# Random Forest Classifier\n\n*This classifier takes the concept of decision trees to the next level. It creates a forest of trees where each tree is formed by a random selection of features from the total features. Here, we can vary the number of trees that will be used to predict the class. I calculate test scores over 10, 100, 200, 500 and 1000 trees.*","91cc374c":"**K Neighbors Classifier**\n\n*This classifier looks for the classes of K nearest neighbors of a given data point and based on the majority class, it assigns a class to this data point. However, the number of neighbors can be varied. I varied them from 1 to 20 neighbors and calculated the test score in each case.*","4c31e1cf":"As you can see, we achieved the maximum score of 91.8% when the number of neighbors was chosen to be 8.","89d92910":"# Saving the best model","9735e1fe":"*Let\u2019s take a look at the plots. It shows how each feature and label is distributed along different ranges, which further confirms the need for scaling. Next, wherever you see discrete bars, it basically means that each of these is actually a categorical variable. We will need to handle these categorical variables before applying Machine Learning. Our target labels have two classes, 0 for no disease and 1 for disease.*"}}