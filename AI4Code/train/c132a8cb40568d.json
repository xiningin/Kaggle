{"cell_type":{"763495b0":"code","173f012d":"code","b176af9d":"code","7653bb38":"code","5eeb4133":"code","2d2ef3ab":"code","ed71e3e6":"code","71181f7d":"code","61fdc2c0":"code","0dbd84a1":"code","576f7477":"code","886e8091":"code","b5d9f52f":"code","b6e9c53e":"code","c82bab10":"code","3deda022":"code","d5ba618a":"code","6ea628aa":"code","3162e407":"code","111fe7dc":"code","8c24325e":"code","85e27987":"code","b75ceed9":"code","31bbe36f":"code","0792fcfc":"code","9cdceb9a":"code","35578428":"code","b6aee49b":"markdown","ad9afed4":"markdown","7ba0baad":"markdown","712aa738":"markdown","d74781ad":"markdown","c134bbf8":"markdown","1395be5e":"markdown","0057335a":"markdown","4278aaaa":"markdown","06a83d64":"markdown","00e901b4":"markdown","c78f4189":"markdown","fa5ea981":"markdown","26a41948":"markdown","a4144c0d":"markdown","51b2dd33":"markdown","29696b7d":"markdown","1208979b":"markdown"},"source":{"763495b0":"!wget -q -nc https:\/\/www.dropbox.com\/s\/ce8nh5974mno0x4\/enriched_covid_19_week_2.csv","173f012d":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \nimport os\nimport numpy as np\nimport pandas as pd\nimport geopandas as gpd\nfrom shapely.geometry import Point\nimport tensorflow as tf\nfrom tqdm import tqdm\nfrom sklearn.utils import shuffle\nfrom sklearn.metrics import mean_squared_log_error\n\nfrom datetime import datetime\nfrom datetime import timedelta\n\nfrom tensorflow.keras import layers\nfrom tensorflow.keras import Input\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.callbacks import ModelCheckpoint, ReduceLROnPlateau, EarlyStopping\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set()\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","b176af9d":"train_df = gpd.read_file(\"\/kaggle\/input\/covid19-global-forecasting-week-4\/train.csv\")\ntrain_df[\"ConfirmedCases\"] = train_df[\"ConfirmedCases\"].astype(\"float\")\ntrain_df[\"Fatalities\"] = train_df[\"Fatalities\"].astype(\"float\")\n#The country_region got modified in the enriched dataset by @optimo, \n# so we have to apply the same change to this Dataframe to facilitate the merge.\ntrain_df[\"Country_Region\"] = [ row.Country_Region.replace(\"'\",\"\").strip(\" \") if row.Province_State==\"\" else str(row.Country_Region+\"_\"+row.Province_State).replace(\"'\",\"\").strip(\" \") for idx,row in train_df.iterrows()]\n\n#Still using the enriched data from week 2 as there is everything required for the model's training\nextra_data_df = gpd.read_file(\"\/kaggle\/working\/enriched_covid_19_week_2.csv\")\nextra_data_df[\"Country_Region\"] = [country_name.replace(\"'\",\"\") for country_name in extra_data_df[\"Country_Region\"]]\nextra_data_df[\"restrictions\"] = extra_data_df[\"restrictions\"].astype(\"int\")\nextra_data_df[\"quarantine\"] = extra_data_df[\"quarantine\"].astype(\"int\")\nextra_data_df[\"schools\"] = extra_data_df[\"schools\"].astype(\"int\")\nextra_data_df[\"total_pop\"] = extra_data_df[\"total_pop\"].astype(\"float\")\nextra_data_df[\"density\"] = extra_data_df[\"density\"].astype(\"float\")\nextra_data_df[\"hospibed\"] = extra_data_df[\"hospibed\"].astype(\"float\")\nextra_data_df[\"lung\"] = extra_data_df[\"lung\"].astype(\"float\")\nextra_data_df[\"total_pop\"] = extra_data_df[\"total_pop\"]\/max(extra_data_df[\"total_pop\"])\nextra_data_df[\"density\"] = extra_data_df[\"density\"]\/max(extra_data_df[\"density\"])\nextra_data_df[\"hospibed\"] = extra_data_df[\"hospibed\"]\/max(extra_data_df[\"hospibed\"])\nextra_data_df[\"lung\"] = extra_data_df[\"lung\"]\/max(extra_data_df[\"lung\"])\nextra_data_df[\"age_100+\"] = extra_data_df[\"age_100+\"].astype(\"float\")\nextra_data_df[\"age_100+\"] = extra_data_df[\"age_100+\"]\/max(extra_data_df[\"age_100+\"])\n\nextra_data_df = extra_data_df[[\"Country_Region\",\"Date\",\"restrictions\",\"quarantine\",\"schools\",\"hospibed\",\"lung\",\"total_pop\",\"density\",\"age_100+\"]]\nextra_data_df.head()","7653bb38":"train_df = train_df.merge(extra_data_df, how=\"left\", on=['Country_Region','Date']).drop_duplicates()\ntrain_df.head()","5eeb4133":"for country_region in train_df.Country_Region.unique():\n    query_df = train_df.query(\"Country_Region=='\"+country_region+\"' and Date=='2020-03-25'\")\n    train_df.loc[(train_df[\"Country_Region\"]==country_region) & (train_df[\"Date\"]>\"2020-03-25\"),\"total_pop\"] = query_df.total_pop.values[0]\n    train_df.loc[(train_df[\"Country_Region\"]==country_region) & (train_df[\"Date\"]>\"2020-03-25\"),\"hospibed\"] = query_df.hospibed.values[0]\n    train_df.loc[(train_df[\"Country_Region\"]==country_region) & (train_df[\"Date\"]>\"2020-03-25\"),\"density\"] = query_df.density.values[0]\n    train_df.loc[(train_df[\"Country_Region\"]==country_region) & (train_df[\"Date\"]>\"2020-03-25\"),\"lung\"] = query_df.lung.values[0]\n    train_df.loc[(train_df[\"Country_Region\"]==country_region) & (train_df[\"Date\"]>\"2020-03-25\"),\"age_100+\"] = query_df[\"age_100+\"].values[0]\n    train_df.loc[(train_df[\"Country_Region\"]==country_region) & (train_df[\"Date\"]>\"2020-03-25\"),\"restrictions\"] = query_df.restrictions.values[0]\n    train_df.loc[(train_df[\"Country_Region\"]==country_region) & (train_df[\"Date\"]>\"2020-03-25\"),\"quarantine\"] = query_df.quarantine.values[0]\n    train_df.loc[(train_df[\"Country_Region\"]==country_region) & (train_df[\"Date\"]>\"2020-03-25\"),\"schools\"] = query_df.schools.values[0]","2d2ef3ab":"median_pop = np.median(extra_data_df.total_pop)\nmedian_hospibed = np.median(extra_data_df.hospibed)\nmedian_density = np.median(extra_data_df.density)\nmedian_lung = np.median(extra_data_df.lung)\nmedian_centenarian_pop = np.median(extra_data_df[\"age_100+\"])\n#need to replace that with a joint using Pandas\nprint(\"The missing countries\/region are:\")\nfor country_region in train_df.Country_Region.unique():\n    if extra_data_df.query(\"Country_Region=='\"+country_region+\"'\").empty:\n        print(country_region)\n        \n        train_df.loc[train_df[\"Country_Region\"]==country_region,\"total_pop\"] = median_pop\n        train_df.loc[train_df[\"Country_Region\"]==country_region,\"hospibed\"] = median_hospibed\n        train_df.loc[train_df[\"Country_Region\"]==country_region,\"density\"] = median_density\n        train_df.loc[train_df[\"Country_Region\"]==country_region,\"lung\"] = median_lung\n        train_df.loc[train_df[\"Country_Region\"]==country_region,\"age_100+\"] = median_centenarian_pop\n        train_df.loc[train_df[\"Country_Region\"]==country_region,\"restrictions\"] = 0\n        train_df.loc[train_df[\"Country_Region\"]==country_region,\"quarantine\"] = 0\n        train_df.loc[train_df[\"Country_Region\"]==country_region,\"schools\"] = 0","ed71e3e6":"trend_df = pd.DataFrame(columns={\"infection_trend\",\"fatality_trend\",\"quarantine_trend\",\"school_trend\",\"total_population\",\"expected_cases\",\"expected_fatalities\"})","71181f7d":"#Just getting rid of the first days to have a multiple of 7\n#Makes it easier to generate the sequences\ntrain_df = train_df.query(\"Date>'2020-01-22'and Date<'2020-04-01'\")\ndays_in_sequence = 21\n\ntrend_list = []\n\nwith tqdm(total=len(list(train_df.Country_Region.unique()))) as pbar:\n    for country in train_df.Country_Region.unique():\n        for province in train_df.query(f\"Country_Region=='{country}'\").Province_State.unique():\n            province_df = train_df.query(f\"Country_Region=='{country}' and Province_State=='{province}'\")\n            \n            #I added a quick hack to double the number of sequences\n            #Warning: This will later create a minor leakage from the \n            # training set into the validation set.\n            for i in range(0,len(province_df),int(days_in_sequence\/3)):\n                if i+days_in_sequence<=len(province_df):\n                    #prepare all the temporal inputs\n                    infection_trend = [float(x) for x in province_df[i:i+days_in_sequence-1].ConfirmedCases.values]\n                    fatality_trend = [float(x) for x in province_df[i:i+days_in_sequence-1].Fatalities.values]\n                    restriction_trend = [float(x) for x in province_df[i:i+days_in_sequence-1].restrictions.values]\n                    quarantine_trend = [float(x) for x in province_df[i:i+days_in_sequence-1].quarantine.values]\n                    school_trend = [float(x) for x in province_df[i:i+days_in_sequence-1].schools.values]\n\n                    #preparing all the demographic inputs\n                    total_population = float(province_df.iloc[i].total_pop)\n                    density = float(province_df.iloc[i].density)\n                    hospibed = float(province_df.iloc[i].hospibed)\n                    lung = float(province_df.iloc[i].lung)\n                    centenarian_pop = float(province_df.iloc[i][\"age_100+\"])\n\n                    expected_cases = float(province_df.iloc[i+days_in_sequence-1].ConfirmedCases)\n                    expected_fatalities = float(province_df.iloc[i+days_in_sequence-1].Fatalities)\n\n                    trend_list.append({\"infection_trend\":infection_trend,\n                                     \"fatality_trend\":fatality_trend,\n                                     \"restriction_trend\":restriction_trend,\n                                     \"quarantine_trend\":quarantine_trend,\n                                     \"school_trend\":school_trend,\n                                     \"demographic_inputs\":[total_population,density,hospibed,lung,centenarian_pop],\n                                     \"expected_cases\":expected_cases,\n                                     \"expected_fatalities\":expected_fatalities})\n        pbar.update(1)\ntrend_df = pd.DataFrame(trend_list)","61fdc2c0":"trend_df[\"temporal_inputs\"] = [np.asarray([trends[\"infection_trend\"],trends[\"fatality_trend\"],trends[\"restriction_trend\"],trends[\"quarantine_trend\"],trends[\"school_trend\"]]) for idx,trends in trend_df.iterrows()]\n\ntrend_df = shuffle(trend_df)\n\ntrend_df.head()","0dbd84a1":"i=0\ntemp_df = pd.DataFrame()\nfor idx,row in trend_df.iterrows():\n    if sum(row.infection_trend)>0:\n        temp_df = temp_df.append(row)\n    else:\n        if i<25:\n            temp_df = temp_df.append(row)\n            i+=1\ntrend_df = temp_df\n\ntrend_df.head()","576f7477":"sequence_length = 20\ntraining_percentage = 0.9\n\ntraining_item_count = int(len(trend_df)*training_percentage)\nvalidation_item_count = len(trend_df)-int(len(trend_df)*training_percentage)\ntraining_df = trend_df[:training_item_count]\nvalidation_df = trend_df[training_item_count:]\n\nX_temporal_train = np.asarray(np.transpose(np.reshape(np.asarray([np.asarray(x) for x in training_df[\"temporal_inputs\"].values]),(training_item_count,5,sequence_length)),(0,2,1) )).astype(np.float32)\nX_demographic_train = np.asarray([np.asarray(x) for x in training_df[\"demographic_inputs\"]]).astype(np.float32)\nY_cases_train = np.asarray([np.asarray(x) for x in training_df[\"expected_cases\"]]).astype(np.float32)\nY_fatalities_train = np.asarray([np.asarray(x) for x in training_df[\"expected_fatalities\"]]).astype(np.float32)\n\nX_temporal_test = np.asarray(np.transpose(np.reshape(np.asarray([np.asarray(x) for x in validation_df[\"temporal_inputs\"]]),(validation_item_count,5,sequence_length)),(0,2,1)) ).astype(np.float32)\nX_demographic_test = np.asarray([np.asarray(x) for x in validation_df[\"demographic_inputs\"]]).astype(np.float32)\nY_cases_test = np.asarray([np.asarray(x) for x in validation_df[\"expected_cases\"]]).astype(np.float32)\nY_fatalities_test = np.asarray([np.asarray(x) for x in validation_df[\"expected_fatalities\"]]).astype(np.float32)\n\nprint(\"X_temporal_train:    \",X_temporal_train.shape)\nprint(\"X_demographic_train: \",X_demographic_train.shape)\nprint(\"X_temporal_test:     \",X_temporal_test.shape)\nprint(\"X_demographic_test:  \",X_demographic_test.shape)","886e8091":"#temporal input branch\ntemporal_input_layer = Input(shape=(sequence_length,5))\nmain_rnn_layer = layers.GRU(64, return_sequences=True, recurrent_dropout=0.2)(temporal_input_layer)\n\n#demographic input branch\ndemographic_input_layer = Input(shape=(5))\ndemographic_dense = layers.Dense(16)(demographic_input_layer)\ndemographic_dropout = layers.Dropout(0.2)(demographic_dense)\n\n#cases output branch\nrnn_c = layers.GRU(32)(main_rnn_layer)\nmerge_c = layers.Concatenate(axis=-1)([rnn_c,demographic_dropout])\ndense_c = layers.Dense(128)(merge_c)\ndropout_c = layers.Dropout(0.3)(dense_c)\ncases = layers.Dense(1, activation=layers.LeakyReLU(alpha=0.1),name=\"cases\")(dropout_c)\n\n#fatality output branch\nrnn_f = layers.GRU(32)(main_rnn_layer)\nmerge_f = layers.Concatenate(axis=-1)([rnn_f,demographic_dropout])\ndense_f = layers.Dense(128)(merge_f)\ndropout_f = layers.Dropout(0.3)(dense_f)\nfatalities = layers.Dense(1, activation=layers.LeakyReLU(alpha=0.1), name=\"fatalities\")(dropout_f)\n\nmodel = Model([temporal_input_layer,demographic_input_layer], [cases,fatalities])\n\ncallbacks = [ReduceLROnPlateau(monitor='val_loss', patience=10, verbose=1, factor=0.6),\n             EarlyStopping(monitor='val_loss', patience=20),\n             ModelCheckpoint(filepath='best_model.h5', monitor='val_loss', save_best_only=True)]\nmodel.compile(loss=[tf.keras.losses.MeanSquaredLogarithmicError(),tf.keras.losses.MeanSquaredLogarithmicError()], optimizer=\"adam\")\n\nmodel.summary()","b5d9f52f":"history = model.fit([X_temporal_train,X_demographic_train], [Y_cases_train, Y_fatalities_train], \n          epochs = 250, \n          batch_size = 8, \n          validation_data=([X_temporal_test,X_demographic_test],  [Y_cases_test, Y_fatalities_test]), \n          callbacks=callbacks)","b6e9c53e":"plt.plot(history.history['loss'])\nplt.plot(history.history['val_loss'])\nplt.title('Loss over epochs')\nplt.ylabel('Loss')\nplt.xlabel('Epoch')\nplt.legend(['Train', 'Validation'], loc='best')\nplt.show()","c82bab10":"plt.plot(history.history['cases_loss'])\nplt.plot(history.history['val_cases_loss'])\nplt.title('Loss over epochs for the number of cases')\nplt.ylabel('Loss')\nplt.xlabel('Epoch')\nplt.legend(['Train', 'Validation'], loc='best')\nplt.show()","3deda022":"plt.plot(history.history['fatalities_loss'])\nplt.plot(history.history['val_fatalities_loss'])\nplt.title('Loss over epochs for the number of fatalities')\nplt.ylabel('Loss')\nplt.xlabel('Epoch')\nplt.legend(['Train', 'Validation'], loc='best')\nplt.show()","d5ba618a":"model.load_weights(\"best_model.h5\")\npredictions = model.predict([X_temporal_test,X_demographic_test])\n\ndisplay_limit = 30\nfor inputs, pred_cases, exp_cases, pred_fatalities, exp_fatalities in zip(X_temporal_test,predictions[0][:display_limit], Y_cases_test[:display_limit], predictions[1][:display_limit], Y_fatalities_test[:display_limit]):\n    print(\"================================================\")\n    print(inputs)\n    print(\"Expected cases:\", exp_cases, \" Prediction:\", pred_cases[0], \"Expected fatalities:\", exp_fatalities, \" Prediction:\", pred_fatalities[0] )","6ea628aa":"#Will retrieve the number of cases and fatalities for the past 6 days from the given date\ndef build_inputs_for_date(country, province, date, df):\n    start_date = date - timedelta(days=20)\n    end_date = date - timedelta(days=1)\n    \n    str_start_date = start_date.strftime(\"%Y-%m-%d\")\n    str_end_date = end_date.strftime(\"%Y-%m-%d\")\n    df = df.query(\"Country_Region=='\"+country+\"' and Province_State=='\"+province+\"' and Date>='\"+str_start_date+\"' and Date<='\"+str_end_date+\"'\")\n    \n    #preparing the temporal inputs\n    temporal_input_data = np.transpose(np.reshape(np.asarray([df[\"ConfirmedCases\"],\n                                                 df[\"Fatalities\"],\n                                                 df[\"restrictions\"],\n                                                 df[\"quarantine\"],\n                                                 df[\"schools\"]]),\n                                     (5,sequence_length)), (1,0) ).astype(np.float32)\n    \n    #preparing all the demographic inputs\n    total_population = float(province_df.iloc[i].total_pop)\n    density = float(province_df.iloc[i].density)\n    hospibed = float(province_df.iloc[i].hospibed)\n    lung = float(province_df.iloc[i].lung)\n    centenarian_pop = float(province_df.iloc[i][\"age_100+\"])\n    demographic_input_data = [total_population,density,hospibed,lung,centenarian_pop]\n    \n    return [np.array([temporal_input_data]), np.array([demographic_input_data])]\n\n#Take a dataframe in input, will do the predictions and return the dataframe with extra rows\n#containing the predictions\ndef predict_for_region(country, province, df):\n    begin_prediction = \"2020-04-01\"\n    start_date = datetime.strptime(begin_prediction,\"%Y-%m-%d\")\n    end_prediction = \"2020-05-14\"\n    end_date = datetime.strptime(end_prediction,\"%Y-%m-%d\")\n    \n    date_list = [start_date + timedelta(days=x) for x in range((end_date-start_date).days+1)]\n    for date in date_list:\n        input_data = build_inputs_for_date(country, province, date, df)\n        result = model.predict(input_data)\n        \n        #just ensuring that the outputs is\n        #higher than the previous counts\n        result[0] = np.round(result[0])\n        if result[0]<input_data[0][0][-1][0]:\n            result[0]=np.array([[input_data[0][0][-1][0]]])\n        \n        result[1] = np.round(result[1])\n        if result[1]<input_data[0][0][-1][1]:\n            result[1]=np.array([[input_data[0][0][-1][1]]])\n        \n        #We assign the quarantine and school status\n        #depending on previous values\n        #e.g Once a country is locked, it will stay locked until the end\n        df = df.append({\"Country_Region\":country, \n                        \"Province_State\":province, \n                        \"Date\":date.strftime(\"%Y-%m-%d\"), \n                        \"restrictions\": 1 if any(input_data[0][0][2]) else 0,\n                        \"quarantine\": 1 if any(input_data[0][0][3]) else 0,\n                        \"schools\": 1 if any(input_data[0][0][4]) else 0,\n                        \"total_pop\": input_data[1][0],\n                        \"density\": input_data[1][0][1],\n                        \"hospibed\": input_data[1][0][2],\n                        \"lung\": input_data[1][0][3],\n                        \"age_100+\": input_data[1][0][4],\n                        \"ConfirmedCases\":round(result[0][0][0]),\t\n                        \"Fatalities\":round(result[1][0][0])},\n                       ignore_index=True)\n    return df\n\n\n#The functions that are called here need to optimise, sorry about that!\ncopy_df = train_df\nwith tqdm(total=len(list(copy_df.Country_Region.unique()))) as pbar:\n    for country in copy_df.Country_Region.unique():\n        for province in copy_df.query(\"Country_Region=='\"+country+\"'\").Province_State.unique():\n            copy_df = predict_for_region(country, province, copy_df)\n        pbar.update(1)","3162e407":"#to remove annoying warnings from pandas\npd.options.mode.chained_assignment = None\n\ndef get_RMSLE_per_region(region, groundtruth_df, display_only=False):\n    groundtruth_df[\"ConfirmedCases\"] = groundtruth_df[\"ConfirmedCases\"].astype(\"float\")\n    groundtruth_df[\"Fatalities\"] = groundtruth_df[\"Fatalities\"].astype(\"float\")\n    \n    #we only take data until the 30th of March 2020 as the groundtruth was not available for later dates.\n    groundtruth = groundtruth_df.query(\"Country_Region=='\"+region+\"' and Date>='2020-04-01' and Date<='\"+last_date+\"'\")\n    predictions = copy_df.query(\"Country_Region=='\"+region+\"' and Date>='2020-04-01' and Date<='\"+last_date+\"'\")\n    \n    RMSLE_cases = np.sqrt(mean_squared_log_error( groundtruth.ConfirmedCases.values, predictions.ConfirmedCases.values ))\n    RMSLE_fatalities = np.sqrt(mean_squared_log_error( groundtruth.Fatalities.values, predictions.Fatalities.values ))\n    \n    if display_only:\n        print(region)\n        print(\"RMSLE on cases:\",np.mean(RMSLE_cases))\n        print(\"RMSLE on fatalities:\",np.mean(RMSLE_fatalities))\n    else:\n        return RMSLE_cases, RMSLE_fatalities\n    \n    \ndef get_RMSLE_for_all_regions(groundtruth_df):\n    RMSLE_cases_list = []\n    RMSLE_fatalities_list = []\n    for region in groundtruth_df.Country_Region.unique():\n        RMSLE_cases, RMSLE_fatalities = get_RMSLE_per_region(region, groundtruth_df, False)\n        RMSLE_cases_list.append(RMSLE_cases)\n        RMSLE_fatalities_list.append(RMSLE_fatalities)\n    print(\"RMSLE on cases:\",np.mean(RMSLE_cases_list))\n    print(\"RMSLE on fatalities:\",np.mean(RMSLE_fatalities_list))","111fe7dc":"groundtruth_df = gpd.read_file(\"\/kaggle\/input\/covid19-global-forecasting-week-4\/train.csv\")\n\ngroundtruth_df[\"ConfirmedCases\"] = groundtruth_df[\"ConfirmedCases\"].astype(\"float\")\ngroundtruth_df[\"Fatalities\"] = groundtruth_df[\"Fatalities\"].astype(\"float\")\n#The country_region got modifying in the enriched dataset by @optimo, \n# so we have to apply the same change to this Dataframe.\ngroundtruth_df[\"Country_Region\"] = [ row.Country_Region.replace(\"'\",\"\").strip(\" \") if row.Province_State==\"\" else str(row.Country_Region+\"_\"+row.Province_State).replace(\"'\",\"\").strip(\" \") for idx,row in groundtruth_df.iterrows()]\n\nlast_date = groundtruth_df.Date.unique()[-1]\n\ngroundtruth_df.head()","8c24325e":"get_RMSLE_for_all_regions(groundtruth_df)","85e27987":"badly_affected_countries = [\"France\",\"Italy\",\"United Kingdom\",\"Spain\",\"Iran\",\"Germany\"]\nfor country in badly_affected_countries:\n    get_RMSLE_per_region(country, groundtruth_df, display_only=True)","b75ceed9":"healthy_countries = [\"Taiwan*\",\"Singapore\",\"Kenya\",\"Slovenia\",\"Portugal\", \"Israel\"]\nfor country in healthy_countries:\n    get_RMSLE_per_region(country, groundtruth_df, display_only=True)","31bbe36f":"test_df = gpd.read_file(\"\/kaggle\/input\/covid19-global-forecasting-week-4\/test.csv\")\n#The country_region got modifying in the enriched dataset by @optimo, \n# so we have to apply the same change to the test Dataframe.\ntest_df[\"Country_Region\"] = [ row.Country_Region if row.Province_State==\"\" else row.Country_Region+\"_\"+row.Province_State for idx,row in test_df.iterrows() ]\ntest_df.head()","0792fcfc":"submission_df = pd.DataFrame(columns=[\"ForecastId\",\"ConfirmedCases\",\"Fatalities\"])\nwith tqdm(total=len(test_df)) as pbar:\n    for idx, row in test_df.iterrows():\n        #Had to remove single quotes because of countries like Cote D'Ivoire for example\n        country_region = row.Country_Region.replace(\"'\",\"\").strip(\" \")\n        province_state = row.Province_State.replace(\"'\",\"\").strip(\" \")\n        item = copy_df.query(\"Country_Region=='\"+country_region+\"' and Province_State=='\"+province_state+\"' and Date=='\"+row.Date+\"'\")\n        submission_df = submission_df.append({\"ForecastId\":row.ForecastId,\n                                              \"ConfirmedCases\":int(item.ConfirmedCases.values[0]),\n                                              \"Fatalities\":int(item.Fatalities.values[0])},\n                                             ignore_index=True)\n        pbar.update(1)","9cdceb9a":"submission_df.sample(20)","35578428":"submission_df.to_csv(\"submission.csv\",index=False)","b6aee49b":"# Downloads","ad9afed4":"# Apply the model to predict future trends\n\nThe following functions will be used to get the 13 previous days from a given date and demographic information, predict the number of cases and fatalities, before iterating again. Therefore, it will use the prediction for the next day as part of the data for the one afterwards.","7ba0baad":"# Build Model","712aa738":"# Generating the submission file","d74781ad":"# Check the model's performance for the beginning of April","c134bbf8":"Only keeping 25 sequences where the number of cases stays at 0, as there were way too many of these samples in our dataset.","1395be5e":"Now, let's check how we are performing on two groups:\n* Countries known to have an outbreak\n* Countries with relatively \"few\" cases","0057335a":"Preparing the inputs and shuffling the dataframe to make sure we have a bit of everything in our training and validation set.","4278aaaa":"We can first check whether one of the outputs is globally harder to predict than the other.","06a83d64":"# Preparing the training data","00e901b4":"## Train","c78f4189":"Splitting my dataset with 90% for training and 10% for validation","fa5ea981":"## Performance","26a41948":"As I was using an enriched dataset during the Week 2 competition, I have to add the new countries to my dataframe and fill the missing data with median values.","a4144c0d":"# Info\n\nThe goal is to predict the number of confirmed cases and fatalities for different regions using data from March for the public leaderboard, which we will be using for this class (Data starting April 1st is used for testing - you should not be using any April data for your public leaderboard model). See the \u201cData\u201d tab from the competition website for more details. The scores are calculated based on the root mean squared logarithmic error (log error metrics penalize % differences rather than absolute differences, so a predicted 101 instead of 100 is penalized equally to 1,010,000 instead of 1 million - even though one error is 10,000 times larger in absolute terms) \n\nTeams: Students can work in teams of up to 5 (five) people (the largest team size for this competition). You must register your team on Kaggle. Use the teams feature to group individual accounts together. Only teams on Kaggle will be recognized (you can\u2019t simply add yourself after the assignment ends) so be sure to form your team early. Note, there is a team merger deadline where you won\u2019t be able to add yourself or bring teams together (this is to avoid having groups coalesce using ensemble methods throughout the competition), so make sure you form your teams early. Ask Dr. Albert if you would like assistance finding a team or having others join your team.\n\n\nResources:\n* You are required to use a neural network model for the prediction. Understandably this is limiting given the very parametric nature of the disease spread, but it is a deep learning class after all.\n* Aside from requiring a neural network approach, you are free to use any and all external resources. You can share posted links to resources with people outside your group, however, privately sharing code or data outside of your team is not permitted. This is not only a class rule, but also a formal rule for Kaggle competitions that you can see on their \u201cRules\u201d tab.\n* Getting started: The Kaggle notebooks are a great tool for these competitions and are highly recommended. Click on the \u201cNotebooks\u201d tab in the competition website for an updated list. \nHere is an RNN notebook which you should first try to run and get scored before making any changes\nhttps:\/\/www.kaggle.com\/frlemarchand\/covid-19-forecasting-with-an-rnn\n* Alternatively, you can consider replacing any ML models with a simple predictive neural network model, however, depending on the assumptions of the ML model your results may be limited.\n\n<br\/>\n\nPublic notebook \/ final leaderboard submission (not required): For the purposes of our class, we will be using the public leaderboard only. The competition notes that to be eligible for the final private leaderboard, the notebook and any external data used has to be publicly viewable and the prediction can use all available data up to the end of the competition. This is entirely optional. We will not be using the final leaderboard for grading in this competition.\n\n**Grading:** This is a friendly competition to improve your skills and earn extra points. With this in mind, grading will be with the following rubric:\n9 points for the notebook submission\n+ 6 pts - submit a PDF of the notebook used for analysis and submission.\n+ 2 pts - In one of the first cells of the notebook, give a short description and links to resources (tutorials, posts) used for creating your model and be clear what changes you made beyond those resources, if any\n+ 1 pt - assure the team name and individuals are listed in the beginning of the notebook. All team members should be registered on Kaggle with the team.\n+ 0.5 pts - submit on canvas a screenshot of the highest score, ranking, and team name on the public leaderboard. The final score on the public leaderboard will be used at the end of the competition, however this submission assures some record is available.\n\nTeams will be awarded bonus points based on their ranks on the public leaderboard after the competition ends (or the score on April 21st, whichever is sooner). Given the size of the class, everyone will likely receive bonus points. \n\n#### Other Info:\n\n* The [COVID19 Global Forecasting (Week 4)](https:\/\/www.kaggle.com\/c\/covid19-global-forecasting-week-4) competition.\n\n* Using the [RNN Notebook](https:\/\/www.kaggle.com\/frlemarchand\/covid-19-forecasting-with-an-rnn).\n\n\n<br\/>","51b2dd33":"# Generate predictions using the model","29696b7d":"I create a new dataframe where I will only store 13-day trends for each location with the resulting numbers on the 14th day. The time periods extracted do not overlap on purpose.","1208979b":"Still need to complete part of the data for dates past the 25th of March as the enriched dataset didn't go that far."}}