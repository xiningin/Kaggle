{"cell_type":{"992076ac":"code","5ac5ccd3":"code","203d12f4":"code","f8ee9991":"code","b5fec754":"code","508e439d":"code","37af6ac0":"code","b5c5927f":"code","fa786ab0":"code","b8a55c48":"code","d9e7dd77":"code","1e0a7f02":"code","c3282a27":"markdown","adea9b6e":"markdown","8446a44d":"markdown"},"source":{"992076ac":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\n# import numpy as np # linear algebra\n# import pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\n# import os\n# for dirname, _, filenames in os.walk('\/kaggle\/input'):\n#     for filename in filenames:\n#         print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","5ac5ccd3":"import time\nimport numpy as np\nimport xgboost as xgb\nfrom xgboost import plot_importance, plot_tree\nfrom sklearn.datasets import load_iris\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.datasets import load_boston\nimport matplotlib\nimport matplotlib.pyplot as plt\nimport os\n%matplotlib inline","203d12f4":"# load datasets\niris = load_iris()\nX, y = iris.data, iris.target\n# data split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1234565)","f8ee9991":"# setting parameters\nparams = {\n    # General parameters\n    'booster': 'gbtree',\n    'nthread': 4,\n    'silent':0,\n    'num_feature':4,\n    'seed':1000,\n    # Task parameters\n    'objective':'multi:softmax',\n    'num_class':3,\n    # Advanced parameters\n    'gamma':0.1,\n    'max_depth':6,\n    'lambda':2,\n    'subsample':0.7,\n    'colsample_bytree':0.7,\n    'min_child_weight':3,\n    'eta':0.1\n    \n}\nplst = list(params.items())","b5fec754":"# In XGBoost, the form of datasets must be DMatrix\ndtrain = xgb.DMatrix(X_train, y_train)\ndtest = xgb.DMatrix(X_test)","508e439d":"# Number of base learners = number_rounds*number_of_categories\nnumber_rounds = 50\n# Trainning the XGBoost model\nmodel = xgb.train(plst, dtrain, number_rounds)","37af6ac0":"# Predicting\ny_pred = model.predict(dtest)","b5c5927f":"# Calculate the accuracy\naccuracy = accuracy_score(y_test, y_pred)\nprint(\"accuracy: %.2f%%\" % (accuracy*100.0))","fa786ab0":"# show the most important feature\nplot_importance(model)\nplt.show()","b8a55c48":"# plot the tree, num_trees is the index of tree\nplot_tree(model, num_trees=5)","d9e7dd77":"# save the base learners into txt\n# model.dump_model(\"model1.txt\")","1e0a7f02":"# load data\nboston = load_boston()\nX, y = boston.data, boston.target\nfeature_name = boston.feature_names\n\n# split the datasets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)","c3282a27":"# 2. Regression model based on XGBoost","adea9b6e":"# 1. Classification model based on XGBoost","8446a44d":"This notebook is to learn the XGBoost. The learning video I used is https:\/\/www.bilibili.com\/video\/BV1Ca4y1t7DS?p=13.\n\nNow let's begin coding\n\nFirst, I will import the packages."}}