{"cell_type":{"89989b3c":"code","49867220":"code","13180ba6":"code","6ca021f2":"code","978945b5":"code","7f459e93":"code","1fde584d":"code","c434889d":"code","4d7ccacf":"code","55d5e991":"code","86a00aaa":"code","0f2ac451":"code","e940e8ab":"code","27b607aa":"code","ff18bd1f":"code","c33aa50f":"code","b20a8823":"code","bdfa1dcc":"code","72828856":"code","0dd4595e":"code","abb427c2":"code","5adafeda":"code","b1a19123":"code","d8417c31":"markdown","b76d9a2e":"markdown","8370d208":"markdown","9dbed165":"markdown","c4248e8f":"markdown","a4723774":"markdown","efcccfc9":"markdown","bf55ccd7":"markdown","1684d01e":"markdown","0fa917ff":"markdown","270bd085":"markdown"},"source":{"89989b3c":"# Data Processing\nimport numpy as np \nfrom numpy import asarray\nimport pandas as pd \nfrom tqdm import tqdm #TQDM is a progress bar library with good support for nested loops and Jupyter\/IPython notebooks.\nfrom sklearn.model_selection import train_test_split\nimport re\n\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.preprocessing.text import Tokenizer\n!wget --quiet https:\/\/raw.githubusercontent.com\/tensorflow\/models\/master\/official\/nlp\/bert\/tokenization.py\nimport tokenization\n\n# Data Modeling\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.feature_extraction.text import TfidfVectorizer # == CountVectorizer + TfidfTransformer\nfrom sklearn.svm import LinearSVC\n\nfrom keras.models import Sequential\nfrom keras.layers import Dense,Dropout,Embedding,LSTM\nfrom keras.callbacks import EarlyStopping\nfrom keras.losses import categorical_crossentropy\nfrom keras.optimizers import Adam\n\nimport tensorflow as tf\nimport tensorflow_hub as hub\n\n\n# Data Evaluation\nfrom sklearn import metrics\n\n\nimport warnings\nwarnings.filterwarnings(\"ignore\", category=UserWarning, module='bs4')","49867220":"!unzip ..\/input\/sentiment-analysis-on-movie-reviews\/train.tsv.zip \n!unzip ..\/input\/sentiment-analysis-on-movie-reviews\/test.tsv.zip","13180ba6":"train = pd.read_csv('train.tsv', sep='\\t')\ntest = pd.read_csv('test.tsv', sep='\\t')","6ca021f2":"def remove_URL(text):\n    #url = re.compile(r'https?:\/\/\\S+|www\\.\\S+')\n    url = re.compile(r'http\\S+|www.\\S+')  # https \/ http \/ www\n    return url.sub(r'',text)\n\ndef remove_html(text):\n    html=re.compile(r'<.*?>') # '<p>string<p>' -> 'string'\n    #html=re.compile(r'<.*>') # '<p>string<p>' -> ''\n    return html.sub(r'',text)\n\ndef remove_emoji(text):\n    emoji_pattern = re.compile(\"[\"\n                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n                           u\"\\U00002702-\\U000027B0\"\n                           u\"\\U000024C2-\\U0001F251\"\n                           \"]+\", flags=re.UNICODE)\n    return emoji_pattern.sub(r'', text)\n\ndef remove_number(text):\n    url = re.compile(r'[0-9]')  \n    return url.sub(r'',text)\n\ndef remove_non_alphabet(text):\n    url = re.compile(r'[^a-z\\s]')  \n    return url.sub(r' ',text) # space is handled by Tokenizer of Keras, don't worry\n\nimport string\ndef remove_punct(text):\n    table=str.maketrans('','',string.punctuation)\n    return text.translate(table)","978945b5":"train['Phrase'] = train['Phrase'].str.lower()\ntrain['Phrase'] = train['Phrase'].transform(remove_URL)\ntrain['Phrase'] = train['Phrase'].transform(remove_html)\ntrain['Phrase'] = train['Phrase'].transform(remove_emoji)\ntrain['Phrase'] = train['Phrase'].transform(remove_number)\ntrain['Phrase'] = train['Phrase'].transform(remove_non_alphabet)\n\ntest['Phrase'] = test['Phrase'].str.lower()\ntest['Phrase'] = test['Phrase'].transform(remove_URL)\ntest['Phrase'] = test['Phrase'].transform(remove_html)\ntest['Phrase'] = test['Phrase'].transform(remove_emoji)\ntest['Phrase'] = test['Phrase'].transform(remove_number)\ntest['Phrase'] = test['Phrase'].transform(remove_non_alphabet)","7f459e93":"X = train['Phrase']\ny = train['Sentiment']\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y,test_size=0.25, random_state=0)","1fde584d":"# All steps at once\ntext_clf = Pipeline([('tfidf', TfidfVectorizer()),\n                     ('clf', LinearSVC()),])\n\n# Feed the training data through the pipeline\ntext_clf.fit(X_train, y_train)\n\n# Form a prediction set\ny_pred = text_clf.predict(X_test)\n\n# Print the overall accuracy\nprint('LinearSVC Score: ', metrics.accuracy_score(y_test,y_pred))","c434889d":"tokenize = Tokenizer()\ntokenize.fit_on_texts(X_train.values)\n\nvocab_size = len(tokenize.word_index) + 1\n\nX_train = tokenize.texts_to_sequences(X_train)\nX_test = tokenize.texts_to_sequences(X_test)\n#X_test = tokenize.texts_to_sequences(test['Phrase'])\n\nmax_lenght = max([len(s.split()) for s in train['Phrase']])\n\nX_train = pad_sequences(X_train, max_lenght, padding='post')\nX_test = pad_sequences(X_test, max_lenght, padding='post')","4d7ccacf":"model = Sequential()\nmodel.add(Embedding(vocab_size, 300, input_length=max_lenght))\nmodel.add(LSTM(units=128, dropout=0.2, recurrent_dropout=0.2 ))\nmodel.add(Dense(5, activation='softmax'))\nmodel.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\nprint(model.summary())\n\nmodel.fit(X_train, y_train,validation_data=(X_test, y_test), batch_size=128, epochs=4, verbose=1)","55d5e991":"results_Embedding = model.evaluate(X_test, y_test, batch_size=128)\nprint('Embedding Test Accuracy Score: ', results_Embedding[1])","86a00aaa":"# load the whole embedding into memory\nembeddings_index = dict()\nf = open('..\/input\/glove6b100dtxt1\/glove.6B.100d.txt', encoding=\"utf8\")\nfor line in f:\n    values = line.split()\n    word = values[0]\n    coefs = asarray(values[1:], dtype='float32')\n    embeddings_index[word] = coefs\nf.close()","0f2ac451":"# create a weight matrix for words in training docs\nembedding_matrix = np.zeros((vocab_size, 100))\nfor word, i in tokenize.word_index.items():\n    embedding_vector = embeddings_index.get(word)\n    if embedding_vector is not None:\n        embedding_matrix[i] = embedding_vector","e940e8ab":"model = Sequential()\nmodel.add(Embedding(vocab_size, 100, weights=[embedding_matrix], input_length=max_lenght))\nmodel.add(LSTM(units=128, dropout=0.2, recurrent_dropout=0.2 ))\nmodel.add(Dense(5, activation='softmax'))\nmodel.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\nprint(model.summary())\n\nmodel.fit(X_train, y_train,validation_data=(X_test, y_test), batch_size=128, epochs=7, verbose=1)","27b607aa":"# results_GloVe = model.evaluate(X_test, y_test, batch_size=128)\n# print('GloVe Test Accuracy Score: ', results_GloVe[1])","ff18bd1f":"module_url = 'https:\/\/tfhub.dev\/tensorflow\/bert_en_uncased_L-12_H-768_A-12\/2'\nbert_layer = hub.KerasLayer(module_url, trainable=True)","c33aa50f":"vocab_file = bert_layer.resolved_object.vocab_file.asset_path.numpy()\ndo_lower_case = bert_layer.resolved_object.do_lower_case.numpy()\ntokenizer = tokenization.FullTokenizer(vocab_file, do_lower_case)","b20a8823":"def bert_encode(texts, tokenizer, max_len=512):\n    all_tokens = []\n    all_masks = []\n    all_segments = []\n    \n    for text in texts:\n        text = tokenizer.tokenize(text)\n            \n        text = text[:max_len-2]\n        input_sequence = [\"[CLS]\"] + text + [\"[SEP]\"]\n        pad_len = max_len - len(input_sequence)\n        \n        tokens = tokenizer.convert_tokens_to_ids(input_sequence) + [0] * pad_len\n        pad_masks = [1] * len(input_sequence) + [0] * pad_len\n        segment_ids = [0] * max_len\n        \n        all_tokens.append(tokens)\n        all_masks.append(pad_masks)\n        all_segments.append(segment_ids)\n        \n    return np.array(all_tokens), np.array(all_masks), np.array(all_segments)","bdfa1dcc":"def build_model(bert_layer, max_len=512):\n    input_word_ids = tf.keras.Input(shape=(max_len,), dtype=tf.int32, name=\"input_word_ids\")\n    input_mask = tf.keras.Input(shape=(max_len,), dtype=tf.int32, name=\"input_mask\")\n    segment_ids = tf.keras.Input(shape=(max_len,), dtype=tf.int32, name=\"segment_ids\")\n\n    pooled_output, sequence_output = bert_layer([input_word_ids, input_mask, segment_ids])\n    clf_output = sequence_output[:, 0, :]\n    net = tf.keras.layers.Dense(64, activation='relu')(clf_output)\n    net = tf.keras.layers.Dropout(0.2)(net)\n    net = tf.keras.layers.Dense(32, activation='relu')(net)\n    net = tf.keras.layers.Dropout(0.2)(net)\n    out = tf.keras.layers.Dense(5, activation='softmax')(net)\n    \n    model = tf.keras.models.Model(inputs=[input_word_ids, input_mask, segment_ids], outputs=out)\n    model.compile(tf.keras.optimizers.Adam(lr=1e-5), loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n    \n    return model","72828856":"max_len = 52\ntrain_input = bert_encode(train['Phrase'].values, tokenizer, max_len=max_len)\n#train_labels = tf.keras.utils.to_categorical(train['Sentiment'].values, num_classes=5)\ntrain_labels = train['Sentiment'].values","0dd4595e":"model = build_model(bert_layer, max_len=max_len)\nmodel.summary()","abb427c2":"checkpoint = tf.keras.callbacks.ModelCheckpoint('model.h5', monitor='val_accuracy', \n                                                save_best_only=True, verbose=1)\nearlystopping = tf.keras.callbacks.EarlyStopping(monitor='val_accuracy', patience=3, verbose=1)","5adafeda":"train_history = model.fit(\n    train_input, train_labels, \n    validation_split=0.2,\n    epochs=3,\n    callbacks=[checkpoint, earlystopping],\n    batch_size=32)","b1a19123":"# text_clf.fit(X, y)\n\n# # Form a prediction set\n# y_pred = text_clf.predict(test['Phrase'])\n\n# sub = pd.read_csv('..\/input\/sentiment-analysis-on-movie-reviews\/sampleSubmission.csv')\n# sub['Sentiment'] = y_pred\n# sub.to_csv('submission.csv', index=False)\n\n\n# y_pred = model.predict(X_test)\n\n# sub = pd.read_csv('..\/input\/sentiment-analysis-on-movie-reviews\/sampleSubmission.csv')\n# sub['Sentiment'] = np.argmax(y_pred, axis=-1)\n# sub.to_csv('submission.csv', index=False)\n\n\n# y_pred = model.predict_classes(X_test)\n# sub = pd.read_csv('..\/input\/sentiment-analysis-on-movie-reviews\/sampleSubmission.csv')\n# sub['Sentiment'] = y_pred\n# sub.to_csv('submission.csv', index=False)","d8417c31":"### LinearSVC","b76d9a2e":"# CREDITS\n This notebook is inspired by multiple great work:\n - https:\/\/www.kaggle.com\/chiranjeevbit\/movie-review-prediction\n - https:\/\/www.kaggle.com\/stass30\/result-0-66-lstm-vs-machine-learning\n - https:\/\/www.kaggle.com\/carmensandiego\/keras-bert-tfhub-scores-0-695","8370d208":"# DATA MODELING","9dbed165":"# APPROACH:\n1. Exploring: distribution across labels \n2. Cleaning: remove html, emoji, url, number, non-alphabetic. \n3. Modeling: apply LinearSVC, Embedding methods.\n\n### RESULTS:\n\nAfter applying 4 different techniques on validation data set:\n- CountVectorizer, TFIDF and LinearSVC: 64% accuracy.\n- Traditional Embedding: 66.24% after 4 epochs.\n- GloVe Embedding: 67.99% after 9 epochs.\n- BERT Embedding: 66.27% after 3 epochs.","c4248e8f":"# DATA CLEANING","a4723774":"### BERT","efcccfc9":"# INTRODUCTION\nThe Rotten Tomatoes movie review dataset is a corpus of movie reviews used for sentiment analysis, originally collected by Pang and Lee [1]. In their work on sentiment treebanks, Socher et al. [2] used Amazon's Mechanical Turk to create fine-grained labels for all parsed phrases in the corpus. The dataset is comprised of tab-separated files with phrases from the Rotten Tomatoes dataset.\n\nThe sentiment labels are:\n- 0 - negative\n- 1 - somewhat negative\n- 2 - neutral\n- 3 - somewhat positive\n- 4 - positive\n\nDetailed description of dataset content is described in the following link: https:\/\/www.kaggle.com\/c\/sentiment-analysis-on-movie-reviews\/data","bf55ccd7":"### Traditional Embedding method","1684d01e":"# LIBRARY","0fa917ff":"### GloVe Embedding","270bd085":"# SENTIMENT ANALYSIS ON MOVIE REVIEWS\n## Vu Duong\n### Date: June, 2020"}}