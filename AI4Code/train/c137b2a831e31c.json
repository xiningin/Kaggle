{"cell_type":{"54aae7c6":"code","4387738b":"code","2a2b1254":"code","216b151d":"code","14de9916":"code","816d9abb":"code","6a20d83f":"code","ad487c8e":"code","f18ebb7e":"code","9de2d53c":"code","463852e1":"code","994e758c":"markdown","3f7e2297":"markdown","eb2df7d7":"markdown","ce8c47cd":"markdown","72a21fb5":"markdown","7d0dabf8":"markdown","6999f240":"markdown","10c1a33e":"markdown"},"source":{"54aae7c6":"# Common imports\nimport pandas as pd\nimport numpy as np\nimport time\nimport os\nimport tensorflow as tf\nimport tensorflow_hub as hub\nfrom tensorflow import keras","4387738b":"#Verifying pathname of dataset before loading - for Kaggle\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename));\n        print(os.listdir(\"..\/input\"))","2a2b1254":"# Load Datasets\ndef loadDataset(file_name):\n    df = pd.read_csv(file_name,engine = 'python')\n    return df\nstart_time= time.time()\ndf_train = loadDataset(\"\/kaggle\/input\/dataset-of-malicious-and-benign-webpages\/Webpages_Classification_train_data.csv\/Webpages_Classification_train_data.csv\")\ndf_test = loadDataset(\"\/kaggle\/input\/dataset-of-malicious-and-benign-webpages\/Webpages_Classification_test_data.csv\/Webpages_Classification_test_data.csv\")\n#Ensuring correct sequence of columns \ndf_train = df_train[['url','content','label']]\ndf_test = df_test[['url','content','label']]\nprint(\"***Total Time taken --- %s seconds ---***\" % (time.time() - start_time))","216b151d":"start_time= time.time()\ndf_test['content'] = df_test['content'].str.lower()\ndf_test.drop(columns=['url'],inplace=True)\ndf_test.rename(columns={'content':'text'},inplace=True)\ndf_train['content'] = df_train['content'].str.lower()\ndf_train.drop(columns=['url'],inplace=True)\ndf_train.rename(columns={'content':'text'},inplace=True)\nprint(\"***Total Time taken --- %s seconds ---***\" % (time.time() - start_time))","14de9916":"#Converting Label value to 0,1\nstart_time= time.time()\ndf_test['label'].replace(to_replace =\"good\", value =1, inplace=True)\ndf_train['label'].replace(to_replace =\"good\", value =1, inplace=True)\ndf_test['label'].replace(to_replace =\"bad\", value =0, inplace=True)\ndf_train['label'].replace(to_replace =\"bad\", value =0, inplace=True)\nprint(\"***Total Time taken --- %s seconds ---***\" % (time.time() - start_time))","816d9abb":"#Selection lower numbers as of now for fast testing\ntrain= df_train.iloc[:300000,]\nval= df_train.iloc[300001:310000,]\ntest= df_test.iloc[:200000,]\nprint(len(train), 'train examples')\nprint(len(val), 'validation examples')\nprint(len(test), 'test examples')","6a20d83f":"#Converting the dataframes into X, y numpy arrays \nX_train = train['text'].to_numpy()\ny_train = train['label'].astype(int).to_numpy()\nX_val = val['text'].to_numpy()\ny_val = val['label'].astype(int).to_numpy()\nX_test = test['text'].to_numpy()\ny_test = test['label'].astype(int).to_numpy()","ad487c8e":"# Using Transfer Learning from Tensorflow hub- Universal Text Encoder\nstart_time= time.time()\n# Word Embedder with fixed 20 vector output\nencoder = hub.load(\"https:\/\/tfhub.dev\/google\/tf2-preview\/gnews-swivel-20dim\/1\")\n# Use the ecoder from a local file\n#encoder = hub.load(\"datasets\/PretrainedTFModel\/1\")\nprint(\"***Total Time taken --- %s seconds ---***\" % (time.time() - start_time))","f18ebb7e":"# Use scikit-learn to grid search \nimport numpy\nfrom sklearn.model_selection import GridSearchCV\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense\nfrom tensorflow.keras.wrappers.scikit_learn import KerasClassifier\n# Function to create model, required for KerasClassifier\ndef create_model(optimizer='adam'):\n    model = keras.Sequential([\n    hub.KerasLayer(encoder, input_shape=[],dtype=tf.string,trainable=True),\n    keras.layers.Dense(32, activation='relu'),\n    keras.layers.Dense(16, activation='relu'),\n    keras.layers.Dense(1, activation='sigmoid'),\n    ])\n    model.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n    return model\n# create model\nmodel = KerasClassifier(build_fn=create_model, epochs=4, batch_size=2048)\n# define the grid search parameters\noptimizer = ['SGD', 'RMSprop', 'Adagrad', 'Adam']\n#optimizer = ['SGD', 'RMSprop', 'Adagrad', 'Adadelta', 'Adam', 'Adamax', 'Nadam']\nparam_grid = dict(optimizer=optimizer)\ngrid = GridSearchCV(estimator=model, param_grid=param_grid, n_jobs=1,cv=3)\ngrid_result = grid.fit(X_train,y_train)\n# summarize results\nprint(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\nmeans = grid_result.cv_results_['mean_test_score']\nstds = grid_result.cv_results_['std_test_score']\nparams = grid_result.cv_results_['params']\nfor mean, stdev, param in zip(means, stds, params):\n    print(\"%f (%f) with: %r\" % (mean, stdev, param))","9de2d53c":"# Use scikit-learn to grid search the Learning Rate of ADAM Optimizer\nimport numpy\nfrom sklearn.model_selection import GridSearchCV\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense\nfrom tensorflow.keras.wrappers.scikit_learn import KerasClassifier\n# Function to create model, required for KerasClassifier\ndef create_model(learning_rate=0.001):\n    model = keras.Sequential([\n    hub.KerasLayer(encoder, input_shape=[],dtype=tf.string,trainable=True),\n    keras.layers.Dense(32, activation='relu'),\n    keras.layers.Dense(16, activation='relu'),\n    keras.layers.Dense(1, activation='sigmoid'),\n    ])\n    model.compile(loss='binary_crossentropy',optimizer=keras.optimizers.Adam(learning_rate=learning_rate), metrics=['accuracy'])\n    return model\n# create model\nmodel = KerasClassifier(build_fn=create_model, epochs=5, batch_size=2048,verbose=0)\n# define the grid search parameters\nlearning_rate = [1, 0.1, 0.01, 0.001, 0.0001, 0.00001,0.000001,0.0000001]\nparam_grid = dict(learning_rate=learning_rate)\ngrid = GridSearchCV(estimator=model,param_grid=param_grid,n_jobs=1,cv=3)\ngrid_result = grid.fit(X_train,y_train)","463852e1":"# summarize results\nprint(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\nmeans = grid_result.cv_results_['mean_test_score']\nstds = grid_result.cv_results_['std_test_score']\nparams = grid_result.cv_results_['params']\nfor mean, stdev, param in zip(means, stds, params):\n    print(\"%f (%f) with: %r\" % (mean, stdev, param))","994e758c":"## <font color=blue> Basic Initialisation <\/font>","3f7e2297":"### <font color=blue>Loading Dataset <\/font>","eb2df7d7":"# <font style=\"color:red;\">Grid Search for Finding the Best Set of Hyper Parameters<\/font>","ce8c47cd":" ## <font color=blue> Preprocessing the Dataset <\/font>","72a21fb5":"### Earmarking Validation, Train & Test Sets","7d0dabf8":"## <font color=blue>Grid Search for Best Learning Rate<\/font>","6999f240":"## <font color=blue> Preparing the Tensor Flow Deep Learning Model and SciKit GridSearch<\/font>","10c1a33e":"## <font color=blue>Grid Search for Best Optimisation Algorithm<\/font>"}}