{"cell_type":{"e233784f":"code","9621e05d":"code","9566e46f":"code","fd0ee86e":"code","ab5a4233":"code","553c1582":"code","b4d72dfc":"code","353cb96a":"code","fb4e3613":"code","204f714f":"code","3c090ec8":"code","42a048cc":"code","98c9e6b3":"code","68691b3c":"code","4f762c7f":"code","3146d8af":"code","de6f51b9":"code","fc975759":"code","eeba9154":"code","b0f2d5d9":"code","66e90c8a":"code","7993b3de":"code","cc950543":"code","f599a649":"code","b9a75a1a":"code","1fcc6f96":"code","035c11c0":"code","85b41df4":"code","45db4308":"code","6d12579f":"code","36189370":"code","ddb88100":"code","912417f6":"code","9b0b0339":"code","aa8a6f45":"code","310eab58":"code","672254a3":"code","9a9d25f9":"code","23670416":"code","293cfcbb":"code","9c4aa9f7":"code","c8fd750f":"code","496f0a0f":"code","2f1236f3":"code","59e10a7c":"code","dd58be92":"code","c95b2f7b":"code","27cc6e3c":"code","e660f36d":"markdown","7d7abce4":"markdown","f9b4b1dd":"markdown","314cea89":"markdown","e7e0ded4":"markdown","9519270a":"markdown","12636c49":"markdown","e6c064c7":"markdown","e9a65963":"markdown","2487119f":"markdown","3061d1f5":"markdown","6460da0d":"markdown","5df98aec":"markdown","5e34e32a":"markdown","e60abce5":"markdown","1a48b29c":"markdown","d1d5910e":"markdown","339481d9":"markdown","144c4986":"markdown","7b4c9d37":"markdown","ec1d2a5c":"markdown","f4b8c37a":"markdown","b79b5d14":"markdown","17a072c4":"markdown","fe17e808":"markdown","f16aaf97":"markdown","3bfc6286":"markdown","c123e80f":"markdown","1033b4e7":"markdown","b7182368":"markdown","04645d81":"markdown","399883fc":"markdown","dbcf13aa":"markdown","92ee5d09":"markdown","2ba829fa":"markdown","55d9de8d":"markdown","9ece643c":"markdown","16c385ed":"markdown","ac66449c":"markdown","11a83321":"markdown","300a727d":"markdown","61b65f70":"markdown","7518d575":"markdown","80e0db07":"markdown","5ce6dbd9":"markdown","0a2ba48a":"markdown","658a712a":"markdown"},"source":{"e233784f":"%%HTML\n<a id = \"Word_Embeddings\"><\/a>\n<center>\n<iframe width=\"700\" height=\"315\" src=\"https:\/\/www.youtube.com\/embed\/t5wdTK-QtLA\" frameborder=\"0\" allow=\"accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture\" style=\"position: relative;top: 0;left: 0;\" allowfullscreen ng-show=\"showvideo\"><\/iframe>\n<\/center>","9621e05d":"import pandas as pd \nimport numpy as np \nimport matplotlib.pyplot as plt \nimport seaborn as sns # for beautiful plots\n\n# for word cloud\nfrom PIL import Image\nfrom wordcloud import WordCloud \n\n# for preprocessing\n\nfrom wordcloud import STOPWORDS\nimport regex as re\nimport string\nfrom nltk.util import ngrams\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.naive_bayes import GaussianNB\n#!pip install sentencepiece\nimport sentencepiece\n\n# For making models\n\nfrom sklearn.linear_model import RidgeClassifier\nfrom sklearn.svm import LinearSVC\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.model_selection import GridSearchCV\nfrom keras.wrappers.scikit_learn import KerasClassifier\n\n# for our Neural Netword model\n\nfrom keras.preprocessing.sequence import pad_sequences\nfrom tensorflow import keras\nfrom tensorflow.keras.optimizers import SGD\nfrom tensorflow.keras.initializers import Constant\n\n# For checking and evaluating our models\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.model_selection import StratifiedKFold\n","9566e46f":"#!pip install tensorflow-hub\n#!pip install tf-nightly\nimport tensorflow as tf\nimport tensorflow_hub as hub\nprint(\"TF version: \", tf.__version__)\nprint(\"Hub version: \", hub.__version__)","fd0ee86e":"train = pd.read_csv('..\/input\/nlp-getting-started\/train.csv')\ntest = pd.read_csv('..\/input\/nlp-getting-started\/test.csv')\nids = test.id\n\nprint('shape of training set: ', train.shape)\nprint('shape of testing set: ', test.shape)","ab5a4233":"train.head(2)","553c1582":"df_concat = pd.concat([train, test], axis = 0) # concatinating along the 0 axis(rows).\nfor df in [train, test, df_concat]:\n    df.drop(columns = ['id'], inplace = True) # dropping id and inplacing it\n\n\nprint('null values of each field:\\n', df_concat.isnull().sum())\n","b4d72dfc":"for df in [train, test, df_concat]:\n\n    df.keyword.fillna('no_keyword', inplace = True)\n    df.location.fillna('no_location', inplace = True)\n\n","353cb96a":"# top 30 locations in the dataset\n\ntop_30 = df_concat.groupby(['location']).count().text.sort_values(ascending = False)[:30]\n\n# plot the top 30\n\nplt.figure(figsize = (6,10))\nsns.barplot(x = top_30, y = top_30.index);\nplt.xlabel('number of tweets');\n","fb4e3613":"for df in [train, test, df_concat]:\n    \n    df.drop(columns = ['location'], inplace = True)","204f714f":"\ntweet_mask = np.array(Image.open(\"..\/input\/twitterlogo3\/twitter-logo-png-transparent.png\"))\nwc = WordCloud(collocations=False,\n               background_color=\"white\",\n               max_words=200,\n               mask = tweet_mask,\n               contour_color='steelblue',\n               contour_width=10,)\n\n# Generate a wordcloud\nwc.generate(\" \".join(train['keyword']))\n\n\n# show\nplt.figure(figsize=[20,10])\nplt.imshow(wc, interpolation='bilinear')\nplt.axis(\"off\")\nplt.show()\n","3c090ec8":"# top 10 most used keywords in disastrous and non_disastrous tweets\n# We'll use training set for this \n\ncount_dis_keywords = train[train.target == 1].groupby(['keyword']).count().sort_values(by = 'target', ascending = False)[:30]\ncount_non_dis_keywords =  train[train.target == 0].groupby(['keyword']).count().sort_values(by = 'target', ascending = False)[:30]","42a048cc":"sns.set(style=\"white\")\n\n # increasing the fontsize for better viz\n\nfig, ax_ = plt.subplots(1, 2, figsize = (25,15));\n\n# left side, the plot for keywords in disastrous tweets\n\nsns.barplot(x = count_dis_keywords.target, # count of each keyword\n        y = count_dis_keywords.index, # index of this df is our keywords\n        ax = ax_[0],\n        palette = 'Reds_r',\n        label = 'dis')\n\n\n\n# right side, the plot for non_disastrous tweets\n\nsns.barplot(x = count_non_dis_keywords.target, \n            y = count_non_dis_keywords.index, \n            ax = ax_[1],\n            palette = 'Greens_d',\n            label = 'non_dis')\n\n\nfor ax in [ax_[0], ax_[1]]:\n    \n    ax.set_title('Number of tweets per keyword',\n                 fontsize = 20) # setting title\n    \n    ax.set_ylabel('') \n    ax.set_xlabel('')\n\n    ax.set_yticklabels(labels =ax.get_yticklabels() ,\n                       fontsize = 15)","98c9e6b3":"counting_tweets = train.groupby(['keyword', 'target']).count().reset_index()\n\ncounting_tweets = pd.pivot_table(counting_tweets, \n               values = 'text', # count of tweets in here\n               index = 'keyword',# index is keyword\n               columns = 'target') # columns will be 0 and 1\n\n\n\n# let's have the keywords most used in disastrous tweets on top\n# also some keywords don't appear in both categories \n# so we fill the null values with 0\n\ncounting_tweets = counting_tweets.fillna(0).sort_values([0, 1], ascending = [True, False])\ncounting_tweets","68691b3c":"\ncounting_tweets = counting_tweets.apply(lambda x: x*100\/sum(x), axis=1)","4f762c7f":"# top 30 disastrous \n# and their distribution in dis and non_dis tweets \n\ncounting_tweets.head(30).plot(kind=\"bar\", stacked=True,\n                              figsize = (15,5),\n                              color = ['green', 'red']);\nplt.title('Top disastrous keywords');","3146d8af":"# top 30 non_disastrous\ncounting_tweets.tail(30).plot(kind=\"bar\", stacked=True,\n                              figsize = (15,5),\n                              color = ['green', 'red']);\nplt.title('Top non_disastrous keywords');","de6f51b9":"assert train.keyword.all() == test.keyword.all()\n\n# this statement asserts each serie and doesn't give an error when the two parts are equal.\n# here we didn't get any error so we can assume that they are indeed from one source.","fc975759":"# count of words in every text\n\ndf_concat['word_count'] = \\\ndf_concat['text'].apply(lambda x : len(str(x).split()))\n\n# count of stopwords\n# stopwords : words like 'is', 'he', 'a', 'she'\n\ndf_concat['stop_words'] =\\\ndf_concat['text'].apply(lambda x : len([w for w in str(x).lower().split() \\\n                                    if w in STOPWORDS]))\n\n# how many urls were in the text\n\ndf_concat['url_count'] =\\\ndf_concat['text'].apply(lambda x : len([w for w in str(x).lower().split() \\\n                                    if 'http' in w or 'https' in w]))\n\n\n# the mean of the words in each tweet\n\ndf_concat['mean_word_length'] =\\\ndf_concat['text'].apply(lambda x :\\\n                        np.mean([len(w) for w in str(x).split()]))\n\n# how many characters like '!', '#', '.', '\\' were in text\n\ndf_concat['punc_count'] = \\\ndf_concat['text'].apply(lambda x :\\\n                        len([w for w in str(x) \\\n                        if w in string.punctuation]))\n\n# how many hashtags?\n\ndf_concat['hashtag_count'] = \\\ndf_concat['text'].apply(lambda x :\\\n                        len([w for w in str(x) \\\n                        if w == '#']))\n\n# how many mentions with '@'\n\ndf_concat['mention_count'] = \\\ndf_concat['text'].apply(lambda x :\\\n                        len([w for w in str(x) \\\n                        if w == '@']))\n\n# now we separate the concatinated dataset\n# to observe each dataset's behaviour\n\ntrain1 = df_concat.iloc[ : train.shape[0], :]\ntest1 = df_concat.iloc[train.shape[0] :, :]","eeba9154":"\nfields = ['word_count', 'stop_words', \n         'mean_word_length', 'punc_count','hashtag_count', \n         'mention_count', 'url_count',]\n\n\n\nplt.style.use('seaborn-whitegrid')\n\nf, axes = plt.subplots(len(fields), 1,\n                       figsize = (20, 100))\n\n\nfor i, field in enumerate(fields):\n\n    s = sns.distplot(train1.loc[:, field], \n                 ax = axes[i], kde_kws={'bw':5})\n    \n    sns.distplot(test1.loc[:, field], \n                 ax = axes[i], kde_kws={'bw':5})\n    \n    axes[i].set_title(field, fontsize = 20)\n    \n    axes[i].tick_params(labelsize = 20);\n    \n    axes[i].set_xlabel('', fontsize = 20);\n    \n    s.legend(labels = ['train', 'test'], fontsize = 20)","b0f2d5d9":"\nplt.style.use('seaborn-whitegrid')\n\nf, axes = plt.subplots(len(fields), 1,\n                       \n                       figsize = (20, 100))\n\ntrain1_1 = train1[train1.target != 0]\ntrain1_0 = train1[train1.target != 1]\n\n\nfor i, field in enumerate(fields):\n    \n\n\n    sns.distplot(train1_0.loc[:, field], ax = axes[i],\n                label = 'zeros', kde_kws={'bw':5})\n    \n    \n    s = sns.distplot(train1_1.loc[:, field], ax = axes[i],\n                    label = 'ones', kde_kws={'bw':5})\n    \n    \n    axes[i].set_title(field, fontsize = 20)\n    \n    axes[i].tick_params(labelsize = 20);\n    \n    axes[i].set_xlabel('');\n\n    \n    s.legend(labels = ['Zeros', 'Ones'], fontsize = 20)","66e90c8a":"fig, ax_ = plt.subplots(figsize = (20,5))\n\nexplode = (0.09, 0.09)\n\nax_.pie(x = train.groupby(['target']).count()['text'],\n        labels = ['zeros', 'ones'], shadow = True,  \n        pctdistance = '%10f%%', explode= explode,\n        startangle = 0, textprops={'size': 'larger'});\n\nax_. set_title('Distribution of zeros and ones', fontsize = 15);","7993b3de":"def make_ngrams(text, n):\n    \n    c_vectorizer = CountVectorizer(ngram_range = (n,n)).fit(text)\n    \n    vocab = c_vectorizer.transform(text)\n\n    count_of_each_ngram = vocab.sum(axis = 0)\n    \n    word_and_count = [(word, count_of_each_ngram[0, i]) for word, i in c_vectorizer.vocabulary_.items()]\n    \n    # sorting\n    word_and_count = {k:v for k,v in sorted(word_and_count, key = lambda x: x[1], reverse=True) }\n    \n    return word_and_count","cc950543":"make_ngrams(df_concat.text, 1)\n","f599a649":"glove_dict_vector = np.load('..\/input\/pickled-word-embedding\/glove.840B.300d.pkl', allow_pickle = True)\n","b9a75a1a":"\ndef cov_check(texts, pretrained_vocab):\n    \n    oov_dict = {}\n    oov_num = 0\n    cov_num = 0\n\n    \n    for text in texts:\n        for word in text.split():\n\n            vector = pretrained_vocab.get(word)\n\n            if vector is not None:\n                cov_num += 1\n\n            elif word in oov_dict.keys():\n                oov_dict[word] += 1\n                oov_num += 1\n\n            else: \n                oov_dict[word] = 1\n                oov_num += 1\n\n    oov_dict = {k:v for k, v in sorted(oov_dict.items(),\n                                       key = lambda x:x[1],\n                                       reverse = True)}\n    \n    print('Number of covered words: {}, \\nNumber of uncovered wrods: {}'.format(cov_num, oov_num))\n    print('Coverage precentage: {:.2f}'.format(cov_num \/(cov_num + oov_num) *100))\n\n    return oov_dict","1fcc6f96":"oov = cov_check(df_concat.text, glove_dict_vector)\n\nprint('Most used, unrecognized words:\\n', list(oov)[:10])\n","035c11c0":"def clean(tweet):\n    \n    o = ''\n    rep_redicilous_words = {\n            \"China\\x89\u00db\u00aas\":\"china's\", \"let\\x89\u00db\u00aas\": \"let's\",\n            \"\\x89\u00db\u00cfWhen\":'when',\"from\u00e5\u00cawounds\":'from wounds',\n           \"\u00e5\u00ca\":o , \"\u00e5\u00c8\":o, \"Jap\u00cc_n\":\"Japan\", \n           \"\u00cc\u00a9\":o, \"\u00e5\u00a8\":o, \"Suru\u00cc\u00a4\":\"Suruc\",\n          \"\u00e5\u00c7\":o, \"\u00e5\u00a33million\":\" a million\",\n           \"\u00e5\u00c0\":o, \"he's\":\"he is\", \"there's\":\"there is\",\n          \"We're\":\"We are\", \"That's\":\"That is\",\n           \"won't\":\"will not\", \"They're\":\"They are\",\n           \"Can't\":\"Can not\",\n          \"wasn't\":\"was not\", \"don\\x89\u00db\u00aat\":\"do not\",\n           \"aren't\":\"are not\", \"isn't\":\"is not\",\n          \"What's\":\"What is\", \"haven't\":\"have not\",\n           \"hasn't\":\"has not\", \"There's\":\"There is\",\n          \"He's\":\"He is\", \"It's\":\"It is\",\n           \"You're\":\"You are\", \"I'M\":\"I am\", \n           \"shouldn't\":\"should not\",\n          \"wouldn't\":\"would not\", \"i'm\":\"I am\", \"Isn't\":\"is not\",\n          \"Here's\":\"Here is\", \"you've\":\"you have\",\n           \"you\\x89\u00db\u00aave\": \"you have\", \"we're\":\"we are\",\n           \"What's\":\"What is\", \"couln't\":\"could not\",\n           \"we've\": \"we have\", \"it\\x89\u00db\u00aas\": \"it is\",\n           \"doesn\\x89\u00db\u00aat\": \"does not\", \"It\\x89\u00db\u00aas\": \"It is\",\n           \"Here\\x89\u00db\u00aas\":\"Here is\", \"Who's\": \"who is\",\n           \"I\\x89\u00db\u00aave\": \"I have\", \"y'all\":\"you all\",\n           \"can\\x89\u00db\u00aat\": \"cannot\", \"would've\": \"would have\",\n           \"it'll\":\"it will\", \"we'll\": \"we will\",\n           \"wouldn\\x89\u00db\u00aat\": \"would not\", \"We've\": \"We have\",\n           \"he'll\": \"he will\", \"Didn't\": \"Did not\",\n           \"they'll\": \"they will\", \"they'd\": \"they would\",\n           \"DON'T\": \"DO NOT\", \"That\\x89\u00db\u00aas\": \"That is\",\n           \"they've\": \"they have\", \"i'd\": \"I would\",\n           \"should've\": \"should have\", \"You\\x89\u00db\u00aare\": \"You are\",\n           \"where's\": \"where is\", \"Don\\x89\u00db\u00aat\": \"Do not\",\n           \"we'd\": \"we would\",\"i'll\": \"I will\",\n           \"weren't\": \"were not\", \"They're\": \"They are\",\n           \"Can\\x89\u00db\u00aat\": \"Cannot\", \"you\\x89\u00db\u00aall\": \"you will\", \n           \"I\\x89\u00db\u00aad\": \"I would\", \"let's\": \"let us\",\n           \"it's\": \"it is\", \"can't\": \"cannot\", \n           \"don't\": \"do not\", \"i've\": \"I have\", \n           \"that's\": \"that is\", \"i'll\": \"I will\", \n           \"doesn't\": \"does not\", \"i'd\": \"I would\",\n           \"didn't\": \"did not\", \"ain't\": \"am not\", \n           \"you'll\":\"you will\", \"I've\": \"I have\", \n           \"Don't\": \"do not\", \"I'll\": \"I will\",\n           \"I'd\": \"I would\", \"you'd\": \"You would\",\n           \"Ain't\": \"am not\", \"don\u00e5\u00abt\": \"do not\",\n           \"\u00cfWhen\":\"When\",\"\u00cfHatchet\":\"Hatchet\",\n            \"CollisionNo\":\"Collision No\"\n        }\n    \n    \n    \n    # these have to be separatedly cleaned  \n    # because of their format\n    tweet = re.sub(r\"\\x89\u00fb\", \"\", tweet)\n    tweet = re.sub(r\"\\x89\u00db\", \"\", tweet)\n    tweet = re.sub(r\"\\x89\u00e3\u00a2\", \"\", tweet)\n    tweet = re.sub(r\"\\x89\u00db\u00aa\", \"\", tweet)\n    tweet = re.sub(r\"\\x89\u00db\u00f7\", \"\", tweet)\n    tweet = re.sub(r\"\u00aa\", \"\", tweet)\n    tweet = re.sub(r\"\u00f7\", \"\", tweet)\n    tweet = re.sub(r\"\\x9d\", \"\", tweet)\n    tweet = re.sub(r\"\\x89\u00fb\u00f2\", \"\", tweet)\n    \n    \n    \n    # character_entity_refrences\n        \n    rep_character_entity_refrences = {\"&gt;\": \">\", \"&lt;\":\"<\",\n                                      \"&amp;\": \"&\"}\n    \n    # SLANG!\n    \n    slang_and_abbr = {'goooooooaaaaaal':'goal', 'Rockyfire':'rocky fire',\n            'sleepjunkies':  'sleep junkies',\n            'floydmayweather' :'floyd mayweather','Beyhive':'Beyonc\u00e9 fans',\n            \"TrapMusic\": \"Trap Music\", \"djicemoon\": \"dj icemoon\",\n            'HuffPostRelig':\"HuffPost Religion\",\n            \"MH370\":\"Malaysia Airlines Flight 370\" ,\"prebreak\":\"pre-break\",\n            'TyphoonDevastated':'Typhoon Devastated',\n            'ChicagoArea':'Chicago Area',\n            'localarsonist':'local arsonist',\"USAgov\": \"USA government\",\n            \"TAXIWAYS\":\"taxi ways\",\"Collision1141\":\"Collision\",\n            '\u00cfThe':'The',\"savebees\":\"save bees\", \"GreenHarvard\": \"Green Harvard\",\n            'OPPRESSIONS':'Oppression', '\u00cfRichmond':'Richmond',\n            'm\u00cc\u00bcsica':'music','53inch':'53 inch','Politifiact':'PolitiFact',\n            'saddlebrooke':'Saddlebrooke', \n            '11YearOld':'11yearold',\"MNPDNashville\": \"Metropolitan Nashville Police Department\",\n            'TfLBusAlerts': 'TfL Bus Alerts', 'HatCap':'Hat Cap',\n            \"GMMBC\": \"Greater Mt Moriah Baptist Church\",'versethe':'verse the',\n            'KOIN6News':'KOIN 6 News', \"LiveOnK2\":'Live On K2',\n            'NestleIndia': 'Nestle India', 'humanconsumption': 'human consumption',\n            'socialnews':'social news', 'FaroeIslands':'Faroe Islands',\n            'ArianaGrande':'Ariana Grande', 'animalrescue':'animal rescue',\n            '4PlayThursdays':'Foreplay Thursdays', 'RoyalCarribean':'Royal Carribean',\n            'foodscare':'food scare', 'NoSurrender':'No Surrender',\n            'anyname':'any name', 'MicheleBachman':'Michele Bachman',\n            'iTunesMusic':'iTunes Music', 'camilacabello97':'Camila Cabello',\n            'thankU':'thank you', 'GodsLove': \"God's Love\",\n             \"usNWSgov\": \"United States National Weather Service\",        \n            'ibooklove':'I book love','lavenderpoetrycafe':'lavender poetry cafe' ,      \n            'narendramodi':'Narendra Modi', '15PM':'15 PM',\n            '38PM':'38 PM',\"rightwaystan\": \"Richard Tan\",\n            'Soudelor':'Typhoon', 'Time2015':'Time 2015',\n            'IranDeal':'Iran Deal', 'PantherAttack':'Panther Attack',\n            'MikeParrActor':'Mike Parr Actor', 'NASAHurricane':'NASA Hurricane',\n            'ProphetMuhammad':'Prophet Muhammad', 'StrategicPatience':'Strategic Patience', \n            'ViralSpell':'Viral Spell', \"Suruc\": \"Sanliurfa\",\n             \"okwx\": \"Oklahoma City Weather\",\"IDFire\": \"Idaho Fire\",         \n            \"DETECTADO\": \"Detected\", 'KerrickTrial':'Kerrick Trial',\n             'offers2go':'offers to go', 'RockyFire':'Rocky Fire',\n            'NickCannon':'Nick Cannon','aRmageddon':'Armageddon',\n             'Friend59':'Friend 59','\u00cfThats':'That is',\n            'PokemonCards':'Pokemon Cards','Captainn':'Captain'   ,   \n             'thatswhatfriendsarefor':'that is what friends are for',\n            \"RockBottomRadFM\": \"Rock Bottom Radio\",'\u00cfYou':'You',\n            'nikeplus':'nike plus', \"PAKPATTAN\": \"Pakpattan\",\n            \"samanthaturne19\":\"Samantha Turner\",\n            \"DebateQuestionsWeWantToHear\":\"debate questions we want to hear\",\n             \"JonVoyage\": \"Jon Stewart\",\"TeamHendrick\":\"Hendrick Motorsports\",\n            'BakeOffFriends': 'Bake-off Friends', 'KerrickTrial':'Kerrick Trial',\n            \"DETECTADO\": \"Detected\",\"Suruc\": \"Sanliurfa\",'MTVHottest':'MTV Hottest',\n            'AbbsWinston':'Abbs Winston', 'bookboost':'book boost', \n            'realmandyrain':'Mandy Rain', 'OffensiveContent':'Offensive Content',\n            'NickCannon':'Nick Cannon', 'onlinecommunities':'online communities',\n            'DavidVonderhaar':'David Vonderhaar', 'WorstSummerJob':'Worst Summer Job',\n            'jimmyfallon':'jimmy fallon', 'ArtistsUnited':'Artists United', \n            'HarryBeCareful':'Harry Be Careful', 'weathernetwork':'weather network',\n            'NewsInTweets':'News In Tweets', 'RickPerry':'Rick Perry',\n            'NASASolarSystem':'NASA Solar System', 'stormchase':'stormchaser',\n            'TRAUMATISED':'Traumatised', 'OutBid':'Outbid', 'LivingSafely':'Living Safely',\n            'OriginalFunko':'Original Funko', '\u00cfStretcher':'Stretcher',\n            'UndergroundRailraod':'Underground Railraod',\"gunsense\": \"gun sense\",\n            'LondonFire':'London Fire', 'AllLivesMatter':'All Lives Matter', \n            'wordpressdotcom':'wordpress', 'BlackLivesMatter':'Black Lives Matter', \n            'FantasticFour':'Fantastic Four', '16yr': '16 year',\n            \"SummerFate\": \"Summer Fate\", 'yazidis':'yazid',\n            \"realDonaldTrump\" : \"real Donald Trump\", \"StandwithPP\":\"Stand with Planned Parenthood\",     \n            \"ModiMinistry\" : \"Modi Ministry\", \"GraysonDolan\":\"Grayson Dolan\",\n            \"weallheartonedirection\" : \"we all heart one direction\", \n            \"rapidcity\":\"rapid city\", \"afterShock\":\"after Shock\",\n            \"aftershock\":\"after Shock\", \"VideoVeranoMTV\":\"Video Verano MTV\", \"ScottWalker\": \"Scott Walker\",\n            \"scoopit\":\"scoop it\", \"hermancranston\":\"herman cranston\",\n            \"PeterDutton\":\"Peter Dutton\", \"Hiroshima70\": \"Hiroshima\",\n            \"LiteraryCakes\" : \"Literary Cakes\", \"PlannedParenthood\": \"Planned Parenthood\",\n            \"AskConnor\" : \"Ask Connor\", \"OffensiveContent\":\"Offensive Content\",\n            \"CindyNoonan\":\"Cindy Noonan\", \"Rightways\":\"Right ways\", \n            \"TornadoGiveaway\" : \"Giveaway\", \"MTVSummerStar\":\"MTV Summer Star\",\n            \"FreeBitcoin\":\"Free Bitcoin\", \"ColoradoAvalanche\":\"Colorado Avalanche\",\n            \"FoxNew\":\"Fox New\", \"letsFootball\": \"let us Football\",\"cnewslive\":\"C News Live\",\n            \"thisiswhywecanthavenicethings\":\"this is why we can not have nice things\",\n            \"IvanBerroa\":\"Ivan Berroa\", \"c4news\":\"c4 News\", \"YoNews\":\"News\",\n            \"johngreen\":\"John Green\", \"WindStorm\":\"Wind Storm\", \n            \"BioTerrorism\" : \"Bio-terrorism\", \"bioterrorism\":\"bio-terrorism\",\n            \"s2g\":\"swear to God\", \"taylorswift13\":\"Taylor Swift\",\n            \"MissionHills\":\"Mission Hills\", \"cityofcalgary\": \"City of Calgary\",\n            \"CityofCalgary\": \"City of Calgary\", \"9NewsGoldCoast\":\"9 News Gold Coast\",\n            \"jamaicaplain\": \"Jamaica Plain\", \"liveleakfun\": \"live leak fun\",\n            \"michaelgbaron\":\"Michael Baron\", \"HillaryClinton\":\"Hillary Clinton\",\n             \"edsheeran\": \"Ed Sheeran\", \"Throwingknifes\": \"throwing knifes\", \n            \"IndianNews\":\"Indian News\", \"peterjukes\": \"Peter Jukes\", \n            \"LuchaUnderground\": \"Lucha Underground\", \n            \"greatbritishbakeoff\": \"great british bake off\",\n            \"CommoditiesAre\": \"Commodities are\", \"tubestrike\": \"tube strike\",\n            \"Daesh\":\"ISIS\",\"cnnbrk\": \"CNN Breaking News\",      \n             \"RondaRousey\": \"Ronda Rousey\",\"\u00cfWe\" :\"We\",\n            \"GOPDebate\": \"GOP Debate\", \"yycstorm\": \"Calgary Storm\",\n            \"SouthDowns\": \"South Downs\", \"Ph0tos\":\"Photos\", \n            \"exp0sed\":\"exposed\", \"auspol\": \"Australian politics\",\n            \"megynkelly\": \"Megyn Kelly\", \"GRupdates\":\"GR updates\",\n            \"THISIZBWRIGHT\": \"Bonnie Wright\", \"02PM\": \"02 PM\",\n            \"listenlive\":\"listen live\", \"CDCgov\":\"CDC gov\",\n            \"HarvardU\":\"Harvard\", \"JohnEJefferson\":\"John Jefferson\",\n            \"TweetLikeItsSeptember11th2001\": \"Tweet like it is september 11th 2001\",        \n            \"buildings\u00d3we\":\"buildings we\",\"AskCharley\": \"Ask Charley\" ,\n            \"ScreamQueens\": \"Scream Queens\", \"BillNeelyNBC\":\"Bill Neely NBC\",\n            \"mishacollins\": \"Misha Collins\", \"BeClearOnCancer\": \"Be Clear On Cancer\",\n            \"bancodeseries\": \"banco de series\", \"SOUDELOR\": \"Typhoon \",\n            \"KurtSchlichter\":\"Kurt Schlichter\",\"ProtectDenaliWolves\":\"Protect Denali Wolves\",\n                     }\n    \n    \n\n    # adding all toghether and substituding\n\n    for k,v in {**slang_and_abbr, **rep_redicilous_words,\n                **rep_character_entity_refrences,}.items():\n\n        tweet = re.sub(k, v, tweet)\n        \n        \n    # HTTP\n    tweet = re.sub(r\"https?:\\\/\\\/t.co\\\/[A-Za-z0-9]+\", \"\", tweet)\n\n        \n    # PUNCTUATION\n    \n    tweet = tweet.translate(str.maketrans(string.punctuation, ' '*len(string.punctuation)))\n\n        \n    return tweet\n\n","85b41df4":"df_concat['cleaned_text'] = df_concat.text.apply(lambda x : clean(x))\n\noov = cov_check(df_concat['cleaned_text'], glove_dict_vector)\n","45db4308":"train = df_concat[:train.shape[0]]\ntest = df_concat[train.shape[0]:]","6d12579f":"train.groupby('text').nunique().sort_values(by = 'target', ascending = False)[['target']].head(5)","36189370":"df_mislabeled = train.groupby(['text']).nunique().sort_values(by = 'target', ascending = False)\n\ndf_mislabeled = df_mislabeled[df_mislabeled.target > 1]['target'].index.tolist()","ddb88100":"for tweet in df_mislabeled:\n    print(train[train.text == tweet][['text','target']])","912417f6":"train['relabeled'] = train.target\n# you can change it by their text or index in our dataframe\ntrain.loc[train.text == df_mislabeled[0], 'relabeled'] = 0\ntrain.loc[train.text == df_mislabeled[1], 'relabeled'] = 0\ntrain.loc[train.text == df_mislabeled[2], 'relabeled'] = 0\ntrain.loc[train.text == df_mislabeled[3], 'relabeled'] = 0\ntrain.loc[train.text == df_mislabeled[4], 'relabeled'] = 0\ntrain.loc[train.text == df_mislabeled[5], 'relabeled'] = 0\ntrain.loc[train.text == df_mislabeled[6], 'relabeled'] = 0\ntrain.loc[train.text == df_mislabeled[7], 'relabeled'] = 0\ntrain.loc[train.text == df_mislabeled[8], 'relabeled'] = 0\ntrain.loc[train.text == df_mislabeled[9], 'relabeled'] = 1\ntrain.loc[train.text == df_mislabeled[10], 'relabeled'] = 0\ntrain.loc[train.text == df_mislabeled[11], 'relabeled'] = 1\ntrain.loc[train.text == df_mislabeled[12], 'relabeled'] = 0\ntrain.loc[train.text == df_mislabeled[13], 'relabeled'] = 0\ntrain.loc[train.text == df_mislabeled[14], 'relabeled'] = 1\ntrain.loc[train.text == df_mislabeled[15], 'relabeled'] = 1\ntrain.loc[train.text == df_mislabeled[16], 'relabeled'] = 0\ntrain.loc[train.text == df_mislabeled[17], 'relabeled'] = 0","9b0b0339":"# check if they are all correctly labeled:\nfor tweet in df_mislabeled:\n    print(train[train.text == tweet][['text','relabeled']])","aa8a6f45":"counter_vectorizer = CountVectorizer()\n\ncounter_vectorizer.fit(df_concat.cleaned_text)\ntransformed_rows = counter_vectorizer.transform(df_concat.cleaned_text).todense()\n\n\ntransformed_train = transformed_rows[:train.shape[0]]\ntransformed_test = transformed_rows[train.shape[0]:]\n\n# for model evaluation\nx_train, x_test, y_train, y_test = train_test_split(transformed_train, train.relabeled, shuffle = True)","310eab58":"import time\nstart = time.process_time()\n\n# define the model\nmodel = RidgeClassifier()\nmodel.fit(x_train, y_train)\ny_pred = model.predict(x_test)\nprint('accuracy: ', accuracy_score(y_pred, y_test))  \n\n\nprint('time taken:', time.process_time() - start)\n","672254a3":"model.fit(transformed_train, train.relabeled)\ny_pred = model.predict(transformed_test)\n\n#submission_Ridge = pd.DataFrame({'id':ids.values, 'target':y_pred.astype('int')})\n#submission_Ridge.to_csv('Ridge_submission.csv', index=False)","9a9d25f9":"import time\nstart = time.process_time()\n\n# define the model\nnaive_bayes = GaussianNB()\nnaive_bayes.fit(x_train, y_train)\ny_pred = naive_bayes.predict(x_test)\nprint('accuracy: ', accuracy_score(y_pred, y_test))  \n\n\nprint('time taken:', time.process_time() - start)","23670416":"import time\nstart = time.process_time()\n\n# define the model\nlinear_svc = LinearSVC()\nlinear_svc.fit(x_train, y_train)\ny_pred = linear_svc.predict(x_test)\nprint('accuracy: ', accuracy_score(y_pred, y_test))  \n\nprint('time taken:', time.process_time() - start)","293cfcbb":"# specifying the maximum length of each tweet\nmaximum = 0\nfor tweet in df_concat.cleaned_text:\n    x = len(tweet.split())\n    if x > maximum:\n        maximum = x\n        \nprint(maximum)","9c4aa9f7":"# creating a vocab of my data\nvocab_of_text = set(\" \".join(df_concat.text).lower().split())\n\n\n# creating a dictionary of vocab with index starting with 1\n# because index 0 will be for our padding\nvocab_of_text = dict(enumerate(vocab_of_text, 1))\n\n\n# putting the words first and then indices\nindexed_vocab_of_text = {k:v for v,k in dict(vocab_of_text).items()}\n\n\n\ndef text_to_sequence_my_vocab(text, word_index):\n    tokens = text.lower().split()\n    return [word_index.get(token) for token in tokens if word_index.get(token) is not None]\n\n\n# giving ids\n\ndf_concat['sequences'] = df_concat.text\\\n.apply(lambda x : text_to_sequence_my_vocab(x, indexed_vocab_of_text))\n\nmax_len_seq = 34\n\n# padding\n\npadded = pad_sequences(df_concat['sequences'] ,\n              maxlen = max_len_seq, padding = 'post', \n              truncating = 'post')\n\n\npadded.shape # this is (len(train)+len(test) ,max_len_seq)\n\npadded_train = padded[:train.shape[0]]\npadded_test = padded[train.shape[0]:]","c8fd750f":"# creating a matrix for initial weights\ndim = 300\nvocab_matrix = np.zeros((len(indexed_vocab_of_text)+1, dim))\n\n\n\n# searching for vactor of each word in Glove\nfor word, i in indexed_vocab_of_text.items():\n    \n\n    # embedding index is a dictionary of Glove\n    # with the shape of 'word': vecor\n    vector = glove_dict_vector.get(word)\n    \n    if vector is not None:\n        vocab_matrix[i] = vector\n\n# check that the matrix of weights is not empty\n# and there's no bug in your code with\n# np.all(vocab_matrix == 0)","496f0a0f":"model = keras.models.Sequential()\n\nembedding = keras.layers.Embedding(input_dim=len(indexed_vocab_of_text)+1, \n                                   output_dim= dim , mask_zero = True,\n                                   embeddings_initializer=Constant(vocab_matrix),\n                                   trainable = True,\n                                   input_length = max_len_seq)\nmodel.add(embedding)\nmodel.add(keras.layers.AveragePooling1D())\nmodel.add(keras.layers.LSTM(50, dropout=0.2, recurrent_dropout = 0.2))\nmodel.add(keras.layers.Dense(1, activation='sigmoid'))\nmodel.summary()\n\n\n#optimizer = SGD(learning_rate = 0.1, momentum = 0.4 )\n\nmodel.compile(optimizer = 'Adam',\n              loss = keras.losses.BinaryCrossentropy(from_logits=True),\n              metrics = 'accuracy')\n","2f1236f3":"\nx_train, x_test, y_train, y_test = train_test_split(padded_train, train.target, shuffle = True) \n\nmodel.fit(x = x_train, y = y_train, batch_size= 200, epochs = 3,\n          validation_data = (x_test, y_test))","59e10a7c":"# now you can submit your predictions\n\nmodel.fit(x = padded_train, y = train.target,\n          epochs = 3, batch_size = 200, verbose = 2)\n\ny_pred = model.predict(padded_test)","dd58be92":"\nsubmission = pd.DataFrame(columns = ['id', 'target'])\nsubmission['id'] = ids.astype('int')\nsubmission['target'] = np.round(y_pred).flatten().astype('float64')\n\nsubmission.to_csv('submission.csv', index=False)","c95b2f7b":"\nlosses = []\naccuracies = []\nepoch_evaluation = {}\nbatch_s = 200\nepochs = 5\nk = 4\n\nkfold = StratifiedKFold(n_splits = k, shuffle = True)\n\nfor i, (train_idx, val_idx) in enumerate(kfold.split(padded_train, train.target), 1):\n    \n    print('Fold: ', i)\n    model = keras.models.Sequential()\n    embedding = keras.layers.Embedding(input_dim=len(indexed_vocab_of_text)+1, \n                                    output_dim=dim , mask_zero = True,\n                                    embeddings_initializer=Constant(vocab_matrix),\n                                    trainable = False,\n                                    input_length = max_len_seq)\n    model.add(embedding)\n    model.add(keras.layers.AveragePooling1D())\n    model.add(keras.layers.LSTM(50))\n    model.add(keras.layers.Dense(1, activation='sigmoid'))\n\n    model.compile(optimizer = 'adam',\n              loss = keras.losses.BinaryCrossentropy(from_logits=True),\n              metrics = 'accuracy')\n\n    history = model.fit(x = padded_train[train_idx],\n              y = train.target[train_idx],\n              validation_data= (padded_train[val_idx],\n                                 train.target[val_idx]),\n              batch_size= batch_s, epochs = epochs, verbose = 0)\n    \n\n    epoch_evaluation['Fold '+ str(i)] = history.history\n    \n","27cc6e3c":"for i in range(1,k+1):\n    fig, ax_ = plt.subplots(figsize = (8,4))\n\n\n    ax_.plot(range(epochs), epoch_evaluation['Fold ' + str(i)]['accuracy'], label = 'training');\n    ax_.plot(range(epochs), epoch_evaluation['Fold ' + str(i)]['val_accuracy'], label = 'val');\n    leg = ax_.legend();\n\n    plt.xlabel('epochs');\n    plt.ylabel('accuracy');","e660f36d":"One important thing we have to check is whether or not our test and training set are from the same dataset. It's important because we have to know if there is enough similarity between them that we can rely on our training set to predict the testing set. Sometimes when they're not so similar, a more biased prediction can give us better scores. One way to check this is to check the distribution of the data, like here we can check the distribution of keywords and whether or not both training and testing set have the same keywords.","7d7abce4":"If we look at our data a little bit more closely, we can see that some tweets are labeled both as a disastrous and a none disastrous tweet.\nWe use nunique that returns the number of distinct onservations of each text, meaning how many different targets did one text have.","f9b4b1dd":"<a id=\"Text_Cleaning\"><\/a>\n#  **visualizing the target Distribution**\nThe next thing we have to check and be careful about is the distribution of target. \n\nIf we have more ones than zeros, then we should use Stratified Cross Validation for our training process.\nHere we can use pie chart because our distribution isn't that much ","314cea89":"<a id=\"NeuralNetwork_model\"><\/a>\n# **NeuralNetword Model**","e7e0ded4":"## Linear SVM","9519270a":"<a id=\"Adding_Features\"><\/a>\n# **Checking the distribution and generating new features**","12636c49":"Now let's see what these features can tell us!","e6c064c7":"**Keyword:**\n\nThis is one word or phrase from the text field. Let's see what keywords were used most often and which usually came with disastrous tweets.","e9a65963":"<a id=\"Data_analysis\"><\/a>\nAs we can see the traning data is about 2 times bigger than the testing data (which can be very useful because we have more data to train our model) and has 5 columns. The one column that the testing lacks is the target that we have to predict and submit to Kaggle!\n\nOK, let's look at what our data holds:","2487119f":"Now we have about **97 percent coverage** and a cleaned set of text for prediction!\n\n\nIf want to use a neural network for predicting, you'll use the vectors of GloVe for your prediction, otherwise there is no need for you to keep the GloVe file in your notebook and you can easily delete it (and I suggest you do so, because it takes up alot of memory) with this command:\n\n> del glove_dict_vector","3061d1f5":"As we saw here the distribution of training and testing set are very alike and we can assume that indeed these two are from one dataset.\n\nNow we'll see how these features differ in disastrous and non_disastrous tweets.\n","6460da0d":"**Null Values and unuseful data:** \n\nBefore building our model, we have to check whether each column contains enough information that can actually help us with the prediction.\n\n* Firstly we concatinate the training and testing set for an overall feature analysis and cleaning. \n\n* we'll delete the ids. Because their all unique for each tweet and there is no pattern among them.\n* Also we'll see how many null values each column has.","5df98aec":"<a id=\"Text_embedding\"><\/a>\n# Text embeddings and Setting the initial weights for our NN\n\nThis is a step by step guideline of this section:\n1. Creating a vocabulary of our tweets\n\n2. Giving each word an index \n\n3. Creating a matrix for initial weights\n\n4. Filling the matrix with GloVe vectors","5e34e32a":"Right now, our glove_dict_vector is a dictionary with the keys being the words and the values being the embedding vectors. \nThese vectors will help us in setting the initial weights for our neural netword and getting to the answers sooner.\n\nOk, but first lets utilize our text by seeing how many words are recognized by the GloVe dictionary.","e60abce5":"we see that they are somehow equally distributed, but we're safer with a stratified kfold to sure we have the same proportion in every fold.","1a48b29c":"<a id=\"Cross_Validation_for_NN\"><\/a>\n# Cross Validation in Neural Netword","d1d5910e":"# Cleaning the Mislabeled","339481d9":"From what we see here, there are lots of repetetive words that are getting in our way to get to important, words that define the content of tweets. We'd better clean our text and then check on uni\/bio\/trigrams.","144c4986":"![disastrous%20tweets2-2.jpg](attachment:disastrous%20tweets2-2.jpg)","7b4c9d37":"# **ngrams**","ec1d2a5c":"# Prediction with Neural Network Guide","f4b8c37a":"**Now let's see them together...**","b79b5d14":"submit your prediction of this model with:","17a072c4":"<a id = 'gridsearch'><\/a>\n# Grid Search for Neural Network","fe17e808":"**Location:**\n\nCan this field help? lets see what are the most common locations and how many there are.\nThis field contains users inputs and were not automatically generated. take a look at it. it's very dirty and even same places don't have the same name. but first it's good to take a look at the common places people tweet from.","f16aaf97":"## Gaussian Naive Bayes","3bfc6286":"\nWe want to construct a neural network, the first layer of this neural network will be an embedding layer. What is an embedding layer you ask? \nWhen we want to give the text to a neural network we don\u2019t just throw the words into it and leave! Because it cannot understand the meaning of words. Instead we do the following steps:\n1.\tGive each word a digit that represents that word (or an index we can say)\n![1.PNG](attachment:1.PNG)\n\n2.\tWith those digits we turn each tweet into a list of numbers.\n![2.PNG](attachment:2.PNG)\n\n3.\tThose list of numbers have to have the same length so we put enough zeros at the end of each tweet (these zeros are called paddings.)\n![3.PNG](attachment:3.PNG)\n\n4.\tWe give all the digitized, padded tweets to our NN first layer. This layer is called Embedding Layer. What it does is that it turns each digit(word) into an array. We set the dimension of these arrays and the values are set by the Embedding layer. These values are determined based on the meaning and connection of each word to other words.\n![4.PNG](attachment:4.PNG)\n\n6.\tThen we use average pooling to get an average of all these values. By doing this we reduce the dimension and get the gist of all the arrays in one number.\n7.\tThe other layer of the network are constructed like a normal NN and you can decide on how many they should be\n8.\tBe careful though. You have to set the last layer to have a sigmoid activation function because we want to get either 0 or 1.\n","c123e80f":"Resources and very helpful notebooks :\n\n[Gunes Evitan](https:\/\/www.kaggle.com\/gunesevitan\/nlp-with-disaster-tweets-eda-cleaning-and-bert)\n\n[Shahules](https:\/\/www.kaggle.com\/shahules\/basic-eda-cleaning-and-glove)\n\n[help in Stackoverflow](https:\/\/stats.stackexchange.com\/questions\/246512\/convolutional-layers-to-pad-or-not-to-pad)\n","1033b4e7":"<a id=\"simple_models\"><\/a>\n# Prediction with RidgeClassifier, NaiveBayes, LinearSVM","b7182368":"This is a kernel for data analysis, visualization, cleaning and prediction from beginner to expert level. I will use what ever could help me with predicting a better result and construct easy to sophisticated models. I'd love to hear from you and make this notebook a perfect explaination and guide, so feel free to ask me what ever!\nI'll try to explain the code i wrote and packages I use through out this kernel as much as possible.\n\nLet's dive in...","04645d81":"In natural language processing, one of the best method for text classification is using pretrained text. I'll explain embeddings in another kenel but for now the most important thing to know is that here with GloVe we can have a dictionary of words, a dictionary that leads us to know which words are spelled correctly and can be identified by a neural netword.","399883fc":"<font size=\"+3\" color=\"blue\"><b>Table of content<\/b><\/font><br>\n\n*  [Into](#intro)\n\n*  [Importing packages and data](#Importing)\n\n*  [Data analysis and visualization](#Data_analysis)\n\n*  [Adding Features](#Adding_Features)\n\n*  [Text Cleaning](#Text_Cleaning)\n\n*  [LinearSVM, RidgeClassifier, GaussianNB models](#simple_models)\n\n*  [Text embedding](#Text_embedding)\n\n*  [Neural Network model](#NeuralNetwork_model)\n\n*  [Cross Validation and visualization of NN](#Cross_Validation_for_NN)\n\n* [How to do GridSearch for parameter tuning in neural networks](#gridsearch)","dbcf13aa":"<a id=\"Importing\"><\/a>\n# **Importing Packages and our dataset**","92ee5d09":"## RidgeClassifier","2ba829fa":"<a id=\"intro\"><\/a>\n# Introduction","55d9de8d":"The list of common places include strings like 'earth' or 'Everywhere'! It's because this field is the users input and were not automatically generated and is very dirty. we'll get rid of it right away. \n\n> *But if you're still curious that this field may help us in our prediction in someway, I actually did an experiment and used it with get_dummies, but got slightly worse results. This confirms that this field not only doesn't help but also can be harmful!)*","9ece643c":"**ID**: the identification number of each tweet that separates it from other tweets.\n\n**Keyword**: The one specific word that has been found in many tweets\n\n**Location**: where was the person who tweeted?\n\n**text**: The most important field that we rely our prediction on\n\n**target**: Is the tweet disastrous or not?","16c385ed":"As we can see more than 30 precent of the location and about 8 precent of keyword is null. Let's fill the null values with *no_keyword* and *no_location* for better undrestanding and viz.","ac66449c":"This is were we dive into the actuall text and see how clean or dirty it is. Lets first see what words(unigrams) the text contains:","11a83321":"You can:\n* Check what rows of text contain a specific word with:\n> df_concat[df_concat['text'].str.contains('News')]\n\n* See the whole text in columns with:\n> pd.set_option('display.max_colwidth', -1)\n\n* Check if the GloVe dictionary has a specific word:\n> glove_dict_vector.get('Next')","300a727d":"Here we'll use counter vectorizer to specify one column for each word in the tweets and use it as a feature for prediction. If a tweet has a word, the column specife to that word will have the value of 1(or more based on how many times that word has repeated in that tweet.)\n\nAs for the models, I chose these three because they can predict sparse data fast and easily. You can go on and do gridsearch on them and find the best parameters for each.","61b65f70":"\n## **Conclusion from data generation**:\nDisastrous tweets(which are usually formal and from news), have a less varied **length, stop word count, and mean word length** due to the more bell shaped the curve of them are. They are also a little longer than non_disastrous tweets which are casual and written by people.\nother features like hashtag,mention and url count are very much the same and can't help us much in our prediction.\n","7518d575":"## **Edit**:\nI made [another tutorial on kaggle](https:\/\/www.kaggle.com\/mitramir5\/simple-bert-with-video\/edit\/run\/43188732) about word embeddings and how BERT classifies the text with them. From that notebook this video is one thing that can be useful for people who want to know more about embeddings and text classification. ","80e0db07":"We can check the parameters of our neural network and make sure they're the best parameters we could give our model by using GridSearch. This a capability of scikit-learn library and we can bring it in our game by only using a wraper to wrap Keras models.\n\n\nJason Brownlee talks about this extensively on his [amazing blog](https:\/\/machinelearningmastery.com\/). I suggest you check it out. But in summary you can check the following parameters: \n* Batch_size and number of Epochs\n* Optimization Algorithm\n* Learning Rate and Momentum (in case you used SGD)\n* Weight Initialization\n* Neuron Activation Function\n* Dropout rate\n\nand so on...\n\nHere will have a gridsearch on the batch size and epochs.\n\nJust like a normal GridSearch here too, we will define the parameters ranges and give it to the model. The difference here is that we have to wrap our model with  a KerasClassifer (if we had a regression problem we could do it with KerasRegressor).\n","5ce6dbd9":"# Text Cleaning","0a2ba48a":"From the text above we see that there's a lot to be cleaned and most of them are caused by unnecessary punctuations. You can play around and see which words or characters are misspelled and are keeping GloVe from reading our text.","658a712a":"After checking the keywords, we want to check the distribution of the datasets fields and it's features. We'll get to know our dataset by generating new features."}}