{"cell_type":{"be2ce0a5":"code","44da940a":"code","08975560":"code","f6474d10":"code","d3048d94":"code","85150abd":"code","9bec4906":"code","14bfc3fe":"code","5a2bedd4":"code","c47388f2":"code","67dd16f4":"code","460c7246":"markdown"},"source":{"be2ce0a5":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","44da940a":"!pip install lofo-importance\n","08975560":"train_df = pd.read_csv(\"..\/input\/song-popularity-prediction\/train.csv\")\ntrain_df.head()","f6474d10":"features = [col for col in train_df.columns if col not in {'id','song_popularity'}]\nfeatures","d3048d94":"from lofo import Dataset, LOFOImportance, plot_importance\nfrom sklearn.model_selection import GroupKFold\n\nds = Dataset(train_df, target=\"song_popularity\", features = features,\n            feature_groups = None,\n            auto_group_threshold = 0.6)","85150abd":"import xgboost as xgb\n\nparam = {'objective': 'reg:squarederror',\n         'learning_rate': 0.1,\n         'max_depth': 5,\n         \"min_child_weight\": 200,\n         \"tree_method\": 'gpu_hist', \"gpu_id\": 0,\n         'disable_default_eval_metric': 1,\n         \"n_estimators\": 300\n    }\n\nmodel = xgb.XGBRegressor(**param)","9bec4906":"lofo_imp = LOFOImportance(ds, scoring=\"neg_mean_squared_error\", model=model)\n\nimportance_df = lofo_imp.get_importance()\nimportance_df","14bfc3fe":"importance_df[\"feature_full_name\"] = importance_df[\"feature\"].values\nimportance_df[\"feature\"] = importance_df[\"feature_full_name\"].apply(lambda x: x[:100])","5a2bedd4":"importance_df","c47388f2":"plot_importance(importance_df, figsize=(13, 12))","67dd16f4":"plot_importance(importance_df, figsize=(20, 12))\n","460c7246":"# Ubiquant Feature Importance with LOFO\n\nLOFO (Leave One Feature Out) Importance calculates the importances of a set of features based on a metric of choice, for a model of choice, by iteratively removing each feature from the set, and evaluating the performance of the model, with a validation scheme of choice, based on the chosen metric.\n\nLOFO first evaluates the performance of the model with all the input features included, then iteratively removes one feature at a time, retrains the model, and evaluates its performance on a validation set. The mean and standard deviation (across the folds) of the importance of each feature is then reported.\n\nWhile other feature importance methods usually calculate how much a feature is used by the model, LOFO estimates how much a feature can make a difference by itself given that we have the other features. Here are some advantages of LOFO:\n\n* It generalises well to unseen test sets since it uses a validation scheme.\n* It is model agnostic.\n* It gives negative importance to features that hurt performance upon inclusion.\n* It can group the features. Especially useful for high dimensional features like TFIDF or OHE features. It is also good practice to group very correlated features to avoid misleading results.\n* It can automatically group highly correlated features to avoid underestimating their importance.\n\nhttps:\/\/github.com\/aerdem4\/lofo-importance\n"}}