{"cell_type":{"a15e3ce2":"code","6c806053":"code","e4c3f658":"code","13faed0b":"code","450bfa74":"code","895deb81":"code","617f2441":"code","4743331f":"code","a141afe9":"markdown","17bff787":"markdown","1547da80":"markdown","34e6ca25":"markdown","00cf8351":"markdown"},"source":{"a15e3ce2":"import pandas as pd\nimport numpy as np\n\n\n# Set up code checking\nimport os\nif not os.path.exists(\"..\/input\/train.csv\"):\n    os.symlink(\"..\/input\/home-data-for-ml-course\/train.csv\", \"..\/input\/train.csv\")  \n    os.symlink(\"..\/input\/home-data-for-ml-course\/test.csv\", \"..\/input\/test.csv\") \nfrom learntools.core import binder\nbinder.bind(globals())\nfrom learntools.machine_learning.ex7 import *\n\n\n\n# train data\niowa_file_path = '..\/input\/train.csv'\ntrain_data = pd.read_csv(iowa_file_path)\n\n# test data\ntest_data_path = '..\/input\/test.csv'\ntest_data = pd.read_csv(test_data_path)","6c806053":"df = pd.concat([train_data, test_data], ignore_index=True)\ndf","e4c3f658":"def NaN_info(df):\n    global null_view\n    null_view = df[[col for col in df.columns if df[col].isna().sum() > 0]].isna().sum().sort_values(ascending = True)\n    null_view = pd.DataFrame(null_view, columns=['NANs'])\n    null_view[['PERCENT']] = null_view.NANs.apply(lambda x: round((x\/len(df))*100, 2))\n    null_view[['TYPE']] = df.dtypes\n    return null_view\n\nNaN_info(df)","13faed0b":"def NaN_predict(df): \n    import pandas as pd\n    import numpy as np\n\n    # Disabling warnings\n    import sys\n    import warnings\n    if not sys.warnoptions:\n        warnings.simplefilter(\"ignore\")\n\n\n    import lightgbm as lgb\n    from sklearn.model_selection import RandomizedSearchCV\n    from sklearn.model_selection import train_test_split\n    from sklearn.metrics import mean_absolute_error\n    from sklearn.metrics import accuracy_score\n    from sklearn.metrics import f1_score\n    from sklearn.preprocessing import LabelEncoder\n\n    \n    \n    skip_features_with_missing_data_percentage = 50\n    include_features_as_predictors_where_pec_miss_data_less = 10\n    n_iter_for_RandomizedSearchCV = 10\n\n\n    def NaN_info(df):\n        global null_view\n        null_view = df[[col for col in df.columns if df[col].isna().sum() > 0]].isna().sum().sort_values(ascending = True)\n        null_view = pd.DataFrame(null_view, columns=['NANs'])\n        null_view[['PERCENT']] = null_view.NANs.apply(lambda x: round((x\/len(df))*100, 2))\n        null_view[['TYPE']] = df.dtypes\n        return null_view\n\n\n    def encoding(work_predictors, df):\n        for j in work_predictors:\n            el_type = df[j].dtype\n            if el_type == 'object':\n                df[j].replace(np.nan, '0', inplace=True)\n                labelencoder = LabelEncoder()\n                df.loc[:, j] = labelencoder.fit_transform(df.loc[:, j])\n        return df, work_predictors\n\n\n    def imput_missing_value_to_main_df(df, miss_indeces, pred_miss, el):\n        counter = 0\n        for idx in miss_indeces:\n            df.loc[idx, el] = pred_miss[counter]\n            counter += 1\n        return df\n\n\n    # Regressor\n    def hyperparms_tuning_regressor(X_train, X_test, y_train, y_test, n_iter_for_RandomizedSearchCV):\n        global best_params\n        params =   {'num_leaves':  np.arange(10, 300, step=10),\n                    'max_depth': np.arange(4, 20, step=2),\n                    'learning_rate': [0.00001, 0.0001, 0.001, 0.01, 0.1],\n                    'n_estimators': np.arange(300, 800, step=25),\n                    'subsample': [0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0],\n                    'feature_fraction': [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9],\n                    'bagging_fraction': [0.8, 0.9, 1.0],\n                    'lambda_l1': [0.1, 0.2, 0.3, 0.4, 0.5, 0.6],\n                    'lambda_l2': [0.1, 0.2, 0.3, 0.4, 0.5, 0.6],\n                    'min_child_samples': np.arange(5, 105, step=25),\n                    'min_split_gain': [0.0001, 0.001, 0.01, 0.1]\n                   }\n\n        lgbm = lgb.LGBMRegressor()\n        lgbm_randomized_mse = RandomizedSearchCV(estimator=lgbm, \n                                                param_distributions=params, \n                                                n_iter=n_iter_for_RandomizedSearchCV, \n                                                scoring='neg_mean_squared_error', \n                                                cv=3, \n                                                verbose=0,\n                                                n_jobs = -1)\n\n        lgbm_randomized_mse.fit(X_train, y_train)\n        best_params = lgbm_randomized_mse.best_params_\n        pred_test_lgb = lgbm_randomized_mse.predict(X_test)\n        MAE = mean_absolute_error(y_test,pred_test_lgb)\n        y_te = list(round(y_test[:10], 1))\n        y_pred = list(np.round(pred_test_lgb[:10], 1))\n        print(f'first 10 y_test: {y_te}')\n        print(f'first 10 y_pred: {y_pred}')\n        print(f'mean_absolute_error: {MAE}')\n        return best_params\n\n    def predict_regressor(best_params, X, y, miss_df):\n        global pred_miss\n        lgbm = lgb.LGBMRegressor(**best_params)\n        lgbm.fit(X, y)\n        pred_miss = list(lgbm.predict(miss_df))\n        print('-------------------------------')\n        print(f\"predicted missing values: \\n{pred_miss}\")\n        return pred_miss\n\n\n    # Classifier\n    def hyperparms_tuning_classifier(X_train, X_test, y_train, y_test, n_iter_for_RandomizedSearchCV):\n        global best_params\n        params =   {'num_leaves':  np.arange(100, 300, step=10),\n                    'max_depth': np.arange(10, 30, step=2),\n                    'learning_rate': [0.00001, 0.0001, 0.001, 0.01, 0.1],\n                    'n_estimators': np.arange(300, 800, step=50),\n                    'subsample': [0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0],\n                    'feature_fraction': [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9],\n                    'bagging_fraction': [0.8, 0.9, 1.0],\n                    'lambda_l1': [0.1, 0.2, 0.3, 0.4, 0.5, 0.6],\n                    'lambda_l2': [0.1, 0.2, 0.3, 0.4, 0.5, 0.6],\n                    'min_child_samples': np.arange(5, 105, step=25),\n                    'min_split_gain': [0.0001, 0.001, 0.01, 0.1]\n                    }\n\n        lgbm = lgb.LGBMClassifier()\n        lgbm_randomized_mse = RandomizedSearchCV(estimator=lgbm, \n                                                param_distributions=params, \n                                                n_iter=n_iter_for_RandomizedSearchCV, \n                                                scoring='f1_weighted', \n                                                cv=3, \n                                                verbose=0,\n                                                n_jobs = -1)\n\n        lgbm_randomized_mse.fit(X_train, y_train)\n        best_params = lgbm_randomized_mse.best_params_\n        pred_test_lgb = lgbm_randomized_mse.predict(X_test)\n        accuracy = accuracy_score(y_test, pred_test_lgb)\n        print(f'first 10 y_test: {y_test[:10]}')\n        print(f'first 10 y_pred: {pred_test_lgb[:10]}')\n        f1 = f1_score(y_test, pred_test_lgb, average='weighted')\n        print(f'accuracy_score:      {accuracy}')\n        print(f'f1_score (weighted): {f1}')\n        return best_params\n\n    def predict_classifier(best_params, X, y, miss_df):\n        global pred_miss\n        lgbm = lgb.LGBMClassifier(**best_params)\n        lgbm.fit(X, y)\n        pred_miss = list(lgbm.predict(miss_df))\n        print('-------------------------------')\n        print(f\"predicted missing values: \\n{pred_miss}\")\n        return pred_miss\n\n\n\n\n\n\n \n    print(NaN_info(df))\n    print('\\n\\n\\n')\n\n\n    delete_miss_features = list((null_view.loc[null_view['PERCENT'] > skip_features_with_missing_data_percentage]).index)\n    print(f'Exclude from the prediction, because missing data more than \\\n    {skip_features_with_missing_data_percentage}% :\\n{delete_miss_features}')\n    print('')\n    all_miss_features = list(null_view.index)\n\n    for delete_feature in delete_miss_features:\n        all_miss_features.remove(delete_feature)\n\n\n    lot_of_miss_features = list((null_view.loc[null_view['PERCENT'] > include_features_as_predictors_where_pec_miss_data_less]).index)\n    print(f'Unused for prediction, because missing data more than \\\n    {include_features_as_predictors_where_pec_miss_data_less}% :')\n    print(lot_of_miss_features)\n    print('')\n\n    all_features = list(df.columns)\n    now_predictors = list(set(all_features)-set(lot_of_miss_features))\n\n    df_indeces = list(df.index)\n\n\n    for el in all_miss_features:    \n        work_predictors = list(set(now_predictors) - set([el]))\n\n        # missing data (data for prediction)\n        miss_indeces = list((df[pd.isnull(df[el])]).index)\n        miss_df = df.iloc[miss_indeces][:]\n        miss_df = miss_df[work_predictors]\n        encoding(work_predictors, df=miss_df)\n\n        # data without NaN rows (X data for train and evaluation of model)\n        work_indeces = list(set(df_indeces) - set(miss_indeces))\n        work_df = df.iloc[work_indeces][:] \n        encoding(work_predictors, df=work_df)\n\n        X = work_df[work_predictors]\n        y = work_df[el]\n        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=0)\n\n        # Info\n        feature_type = df[el].dtypes\n        print('\\n\\n')\n        perc = null_view['PERCENT'][el]\n        print(f'feature: {el},   type: {feature_type},   missing values: {perc}%')    \n        print(f'X.shape: {X.shape},   y.shape: {y.shape}')\n\n        # Predictions\n        if feature_type == 'object' or feature_type == 'bool':   # or feature_type == 'category' - error\n            print('classifier evaluation:')\n            labelencoder = LabelEncoder()\n            y_train = labelencoder.fit_transform(y_train)\n            y_test = labelencoder.fit_transform(y_test)\n            hyperparms_tuning_classifier(X_train, X_test, y_train, y_test, n_iter_for_RandomizedSearchCV)\n            predict_classifier(best_params, X, y, miss_df)\n            imput_missing_value_to_main_df(df, miss_indeces, pred_miss, el)\n\n        elif feature_type == 'float64' or feature_type == 'int64':\n            print('regressor evaluation:')\n            hyperparms_tuning_regressor(X_train, X_test, y_train, y_test, n_iter_for_RandomizedSearchCV)\n            print(f'mean for {el}: {df[el].mean()}')\n            predict_regressor(best_params, X, y, miss_df)\n            imput_missing_value_to_main_df(df, miss_indeces, pred_miss, el)\n\n        else:\n            print(f\"unprocessed feature: {el} - {feature_type} type\")\n    #     print(f'miss_indexes: \\n{miss_indeces}')\n\n\n    print('\\n\\n\\n')\n    print(f'These features have not been processed, because missing data more than {skip_features_with_missing_data_percentage}%')\n    print(NaN_info(df))\n    print('\\n\\n\\n')\n    \n    return df","450bfa74":"NaN_predict(df)","895deb81":"test_preds = df.loc[1460:,'SalePrice']\ntest_preds = np.array(test_preds)\ntest_preds","617f2441":"output = pd.DataFrame({'Id': test_data.Id,\n                      'SalePrice': test_preds})\noutput.to_csv('submission.csv', index=False)\n ","4743331f":"# Check answer\nstep_1.check()\n# step_1.solution()","a141afe9":"# Algorithm for NaNs Prediction","17bff787":"# Downloading","1547da80":"# NaN prediction in one function","34e6ca25":"# Make Predictions","00cf8351":"# Recomendation)\n\n- cast the data types to the correct ones.   \n- remove irrelevant columns. (But it is not exactly)\n- process those NaNs about which you know something for certain. !!!(try to skip this point)\n- algorithm don't handle types: category, datetime64, timedelta[ns], maby bool\n- algorithm can handle features with 96.30% missing data, but can't handle 99.52% missing data (I set the limiter to 50%, it will skip them. It DEPENDS on the number of raws)\n\n"}}