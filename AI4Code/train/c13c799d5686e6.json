{"cell_type":{"7720c1bb":"code","db034dd7":"code","4c5b4ea5":"code","e4e87eeb":"code","5b85a37f":"code","92565236":"code","1c3bfebd":"code","f129b2da":"code","f686f77a":"code","59c4f5cd":"code","e04e5f80":"code","6c09c8b2":"code","c1ecfbb1":"code","e8cb1046":"code","3948299d":"code","0d0d1fbe":"code","e582e831":"code","8b97e576":"code","b5b0403c":"code","243868c2":"code","023c2eba":"code","1a76d186":"code","aab422fb":"code","b9d8e6fe":"code","0b6ced6f":"code","b971b56d":"code","576039e7":"code","15713d73":"code","870314d6":"code","58d22d77":"code","6fa6bdcb":"code","5e0a8bf5":"markdown","e832b4b5":"markdown","4a221c7b":"markdown","273ab209":"markdown","0aac0873":"markdown","7768bcbf":"markdown","1e29af2e":"markdown","4af28566":"markdown","2b8387bd":"markdown","387bd05a":"markdown","e08b43bb":"markdown","592de2d5":"markdown","ba08d0cc":"markdown","ed0f6b9a":"markdown","dd59e208":"markdown","96988979":"markdown","487e0d45":"markdown","14e8e6eb":"markdown","b0bc56fd":"markdown","2d5764a5":"markdown","b667bb82":"markdown"},"source":{"7720c1bb":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","db034dd7":"########################################\n# Load the packages\n########################################\nimport numpy as np\nimport pandas as pd\nimport re\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom nltk.stem import SnowballStemmer\n\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.layers import Dense, Input, Embedding, Bidirectional, GRU, Conv1D, GlobalMaxPooling1D, Dropout, TimeDistributed\nfrom keras.layers.merge import concatenate\nfrom keras.models import Model\nfrom keras.callbacks import EarlyStopping, ModelCheckpoint\n\n%matplotlib inline","4c5b4ea5":"########################################\n# Define the hyper parameters\n########################################\npath = '..\/input\/jigsaw-toxic-comment-classification-challenge\/'\nTRAIN_DATA_FILE= os.path.join(path, 'train.csv')\nTEST_DATA_FILE= os.path.join(path, 'test.csv')\n\nMAX_SEQUENCE_LENGTH = 100\nMAX_NB_WORDS = 100000\nEMBEDDING_DIM = 50","e4e87eeb":"########################################\n# Load the training \/ testing set with pandas csv format\n########################################\ntrain_df = pd.read_csv(TRAIN_DATA_FILE)\ntest_df = pd.read_csv(TEST_DATA_FILE)","5b85a37f":"print(\"A quick view of training set\")\ntrain_df.head()","92565236":"print(\"A quick view of testing set\")\ntest_df.head()","1c3bfebd":"# What would be toxic?\ntrain_df[train_df.toxic == 1].head(10)","f129b2da":"'''\nWhat's the positive ratio of each class ?\n'''\ndef get_pos_ratio(data):\n    return data.sum() \/ len(data)\n\npos_ratio = []\nfor col in ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']:\n    pos_ratio.append(get_pos_ratio(train_df[col]))","f686f77a":"assert pos_ratio[0] == 0.09584448302009764, \"The answer is not correct.\"\nprint(\"Congrats, you passed the test.\")","59c4f5cd":"x = train_df.iloc[:,2:].sum()\n\nplt.figure(figsize=(8,4))\nax= sns.barplot(x.index, x.values, alpha=0.8)\nplt.title(\"# per class\")\nplt.ylabel('# of Occurrences', fontsize=12)\nplt.xlabel('Type ', fontsize=12)\n\nrects = ax.patches\nlabels = x.values\nfor rect, label in zip(rects, labels):\n    height = rect.get_height()\n    ax.text(rect.get_x() + rect.get_width()\/2, height + 5, label, ha='center', va='bottom')\n\nplt.show()","e04e5f80":"corr=train_df.corr()\nplt.figure(figsize=(10,8))\nsns.heatmap(corr,\n            xticklabels=corr.columns.values,\n            yticklabels=corr.columns.values, annot=True)","6c09c8b2":"########################################\n## Text pre-processing and cleaning\n########################################\nprint('Processing text dataset')\nfrom collections import defaultdict\n\n# regex to remove all Non-Alpha Numeric and space\nspecial_character_removal=re.compile(r'[^a-z\\d ]',re.IGNORECASE)\n\n# regex to replace all numeric\nreplace_numbers=re.compile(r'\\d+',re.IGNORECASE)\n\ndef clean_text(text, stem_words=False):\n    # Clean the text, with the option to remove stopwords and to stem words.\n    text = text.lower()\n    text = re.sub(r\"what's\", \"what is \", text)\n    text = re.sub(r\"\\'s\", \" \", text)\n    text = re.sub(r\"\\'ve\", \" have \", text)\n    text = re.sub(r\"can't\", \"cannot \", text)\n    text = re.sub(r\"n't\", \" not \", text)\n    text = re.sub(r\"i'm\", \"i am \", text)\n    text = re.sub(r\"i\u2019m\", \"i am\", text)\n    text = re.sub(r\"\\'re\", \" are \", text)\n    text = re.sub(r\"\\'d\", \" would \", text)\n    text = re.sub(r\"\\'ll\", \" will \", text)\n    text = re.sub(r\",\", \" \", text)\n    text = re.sub(r\"\\.\", \" \", text)\n    text = re.sub(r\"'\", \" \", text)\n    text = re.sub(r\"\\s{2,}\", \" \", text)\n    text = replace_numbers.sub('', text)\n    text = special_character_removal.sub('',text)\n    \n    return text","c1ecfbb1":"'''\nApply preprocessing and extract the training sentences and testing senteces from pandas dataframe.\nNote that there are some N\/A comment in the train\/test set. Fill them up first.\n'''\ntrain_comments = []\ntest_comments = []","e8cb1046":"list_sentences_train = train_df[\"comment_text\"].fillna(\"no comment\").values\nlist_classes = [\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]\ntrain_labels = train_df[list_classes].values\nlist_sentences_test = test_df[\"comment_text\"].fillna(\"no comment\").values\n\ntrain_comments = [clean_text(text) for text in list_sentences_train]\ntest_comments = [clean_text(text) for text in list_sentences_test]","3948299d":"assert len(train_comments) == 159571 and len(test_comments) == 153164, \"It seems that you lost some data.\"\nassert 'E' not in train_comments[0], \"It seems you did not preprocess the sentecnes. I found a upper case alphabet in your train set.\"","0d0d1fbe":"for i in range(3):\n    print(\"Cleaned\\n\", train_comments[i] + '\\n')\n    print(\"Raw\\n\", train_df.iloc[i]['comment_text'] + '\\n')\n    print(\"------------------\")","e582e831":"# Create a tokenize, which transforms a sentence to a list of ids\ntokenizer = Tokenizer(num_words=MAX_NB_WORDS)\n# Build the relation between words and ids \ntokenizer.fit_on_texts(train_comments + test_comments)","8b97e576":"print(tokenizer.word_index['the']) # map 'the' to 1, map 'to' to 2,......\nprint(tokenizer.word_index['to'])","b5b0403c":"# Transform training\/testing sentences to training\/testing sequences.\ntrain_sequences = tokenizer.texts_to_sequences(train_comments)\ntest_sequences = tokenizer.texts_to_sequences(test_comments)","243868c2":"for i in range(1):\n    print(\"Transformed\\n\", str(train_sequences[i]) + '\\n')\n    print(\"Cleaned\\n\", train_comments[i] + '\\n')\n    print(\"------------------\")","023c2eba":"word_index = tokenizer.word_index\nprint('Found %s unique tokens' % len(word_index))\n\ntrain_data = pad_sequences(train_sequences, maxlen=MAX_SEQUENCE_LENGTH)\nprint('Shape of data tensor:', train_data.shape)\nprint('Shape of label tensor:', train_labels.shape)\n\ntest_data = pad_sequences(test_sequences, maxlen=MAX_SEQUENCE_LENGTH)\nprint('Shape of test_data tensor:', test_data.shape)","1a76d186":"'''\nTry to build a tokenzier, which transform [['Hello', 'World'], ['Greeting', 'my', 'friend'], ['Hello', 'have', 'a', 'nice', 'day']]\nto a list of index sequences. Note that the index should start from 1 because 0 is reserverd for padding token for some framework.\n'''\ntests_input_sentences =  [['Hello', 'World'], ['Greeting', 'my', 'friend'], ['Hello', 'have', 'a', 'nice', 'day']]\ntransform_this_sentences = [['Hello', 'my', 'friend']]\n\ndef index_encoding(sentences, raw_sent):\n    word2idx = {}\n    idx2word = {}\n    ctr = 1\n    for sentence in sentences:\n        for word in sentence:\n            if word not in word2idx.keys():\n                word2idx[word] = ctr\n                idx2word[ctr] = word\n                ctr += 1\n    results = []\n    for sent in raw_sent:\n        results.append([word2idx[word] for word in sent])\n    return results","aab422fb":"transformed = index_encoding(tests_input_sentences, transform_this_sentences)\nassert transformed == [[1, 4, 5]], \"The answer is not correct.\"\nprint(\"Congrats, you passed the test.\")","b9d8e6fe":"########################################\n## Define the text rnn model structure\n########################################\ndef get_text_rnn():\n    recurrent_units = 48\n    dense_units = 32\n    output_units = 6\n    \n    input_layer = Input(shape=(MAX_SEQUENCE_LENGTH,))\n    embedding_layer = Embedding(MAX_NB_WORDS, EMBEDDING_DIM, input_length=MAX_SEQUENCE_LENGTH)(input_layer)\n    \n    x = Bidirectional(GRU(recurrent_units, return_sequences=True))(embedding_layer)\n    x = Bidirectional(GRU(recurrent_units, return_sequences=False))(x)\n    \n    x = Dense(dense_units, activation=\"relu\")(x)\n    output_layer = Dense(output_units, activation=\"sigmoid\")(x)\n    \n    model = Model(inputs=input_layer, outputs=output_layer)\n    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n    return model","0b6ced6f":"########################################\n## Define the text cnn model structure\n########################################\ndef get_text_cnn():\n    filter_nums = 120\n    dense_units = 72\n    output_units = 6\n    \n    input_layer = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')\n    embedding_layer = Embedding(MAX_NB_WORDS, EMBEDDING_DIM, input_length=MAX_SEQUENCE_LENGTH,trainable=False,)(input_layer)\n        \n    conv_0 = Conv1D(filter_nums, 2, kernel_initializer=\"normal\", padding=\"valid\", activation=\"relu\")(embedding_layer)\n    conv_1 = Conv1D(filter_nums, 3, kernel_initializer=\"normal\", padding=\"valid\", activation=\"relu\")(embedding_layer)\n    conv_2 = Conv1D(filter_nums, 4, kernel_initializer=\"normal\", padding=\"valid\", activation=\"relu\")(embedding_layer)\n\n    maxpool_0 = GlobalMaxPooling1D()(conv_0)\n    maxpool_1 = GlobalMaxPooling1D()(conv_1)\n    maxpool_2 = GlobalMaxPooling1D()(conv_2)\n\n    merged_tensor = concatenate([maxpool_0, maxpool_1, maxpool_2])\n    h1 = Dense(units=dense_units, activation=\"relu\")(merged_tensor)\n    output = Dense(units=output_units, activation='sigmoid')(h1)\n\n    model = Model(inputs=input_layer, outputs=output)\n    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n    return model","b971b56d":"########################################\n## Define the text hybrid model structure\n########################################\ndef get_hybrid_textnn():\n    recurrent_units = 48\n    dense_units = 32\n    filter_nums = 64\n    output_units = 6\n\n    input_layer = Input(shape=(MAX_SEQUENCE_LENGTH,))\n    embedding_layer = Embedding(MAX_NB_WORDS, EMBEDDING_DIM, input_length=MAX_SEQUENCE_LENGTH)(input_layer)\n    \n    x = Bidirectional(GRU(recurrent_units, return_sequences=True))(embedding_layer)\n    x = Conv1D(filter_nums, 2, kernel_initializer=\"normal\", padding=\"valid\", activation=\"relu\")(x)    \n    \n    max_pool = GlobalMaxPooling1D()(x)\n    max_pool = Dropout(0.5)(max_pool)\n    \n    output_layer = Dense(output_units, activation=\"sigmoid\")(max_pool)\n\n    model = Model(inputs=input_layer, outputs=output_layer)\n    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n    return model\n","576039e7":"########################################\n## Construct the cross-validation framework\n########################################\ndef _train_model_by_logloss(model, batch_size, train_x, train_y, val_x, val_y, fold_id):\n    # set an early stopping checker.\n    # the training phase would stop when validation log loss decreases continuously for `patience` rounds. \n    early_stopping = EarlyStopping(monitor='val_loss', patience=5)\n    bst_model_path = \"ToxicModel\" + str(fold_id) + '.h5'\n    model_checkpoint = ModelCheckpoint(bst_model_path, save_best_only=True, save_weights_only=True)\n    \n    # training on given fold data\n    hist = model.fit(train_x, train_y,\n        validation_data=(val_x, val_y),\n        epochs=50, batch_size=batch_size, shuffle=True,\n        callbacks=[early_stopping, model_checkpoint])\n    \n    # get the minimal validation log loss on this fold\n    bst_val_score = min(hist.history['val_loss'])\n    model.load_weights(bst_model_path)\n\n    # return the model with best weight, best fold-val score\n    return model, bst_val_score\n\ndef train_folds(X, y, fold_count, batch_size, get_model_func):\n    fold_size = len(X) \/\/ fold_count\n    models = []\n    score = 0\n    \n    # split the whole dataset to `fold_count` fold, and train our model on each fold\n    for fold_id in range(0, fold_count):\n        fold_start = fold_size * fold_id\n        fold_end = fold_start + fold_size\n\n        if fold_id == fold_size - 1:\n            fold_end = len(X)\n\n        # Generate the train\/val data on fold i\n        train_x = np.concatenate([X[:fold_start], X[fold_end:]])\n        train_y = np.concatenate([y[:fold_start], y[fold_end:]])\n\n        val_x = X[fold_start:fold_end]\n        val_y = y[fold_start:fold_end]\n    \n        print(\"Training on fold #\", fold_id)\n        model, bst_val_score = _train_model_by_logloss(get_model_func(), batch_size, train_x, train_y, val_x, val_y, fold_id)\n        score += bst_val_score\n        models.append(model)\n    return models, score \/ fold_count","15713d73":"models, val_loss = train_folds(train_data, train_labels, 2, 256, get_text_cnn)","870314d6":"your_batch_size = 256\n\ndef get_your_model():\n    filter_nums = 120\n    dense_units = 72\n    output_units = 6\n    \n    input_layer = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')\n    embedding_layer = Embedding(MAX_NB_WORDS, EMBEDDING_DIM, input_length=MAX_SEQUENCE_LENGTH,trainable=False,)(input_layer)\n        \n    conv_0 = Conv1D(filter_nums, 2, kernel_initializer=\"normal\", padding=\"valid\", activation=\"relu\")(embedding_layer)\n    conv_1 = Conv1D(filter_nums, 3, kernel_initializer=\"normal\", padding=\"valid\", activation=\"relu\")(embedding_layer)\n    conv_2 = Conv1D(filter_nums, 4, kernel_initializer=\"normal\", padding=\"valid\", activation=\"relu\")(embedding_layer)\n\n    maxpool_0 = GlobalMaxPooling1D()(conv_0)\n    maxpool_1 = GlobalMaxPooling1D()(conv_1)\n    maxpool_2 = GlobalMaxPooling1D()(conv_2)\n\n    merged_tensor = concatenate([maxpool_0, maxpool_1, maxpool_2])\n    h1 = Dense(units=dense_units, activation=\"relu\")(merged_tensor)\n    output = Dense(units=output_units, activation='sigmoid')(h1)\n\n    model = Model(inputs=input_layer, outputs=output)\n    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n    return model","58d22d77":"models, val_loss = train_folds(train_data, train_labels, 2, your_batch_size, get_your_model)","6fa6bdcb":"#test_data = test_df\nCLASSES = [\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]\nsubmit_path_prefix = \"ToxicNN-\" + str(MAX_SEQUENCE_LENGTH) \n\nprint(\"Predicting testing results...\")\ntest_predicts_list = []\nfor fold_id, model in enumerate(models):\n    test_predicts = model.predict(test_data, batch_size=256, verbose=1)\n    test_predicts_list.append(test_predicts)\n\n# merge each folds' predictions by averaging\ntest_predicts = np.zeros(test_predicts_list[0].shape)\nfor fold_predict in test_predicts_list:\n    test_predicts += fold_predict\ntest_predicts \/= len(test_predicts_list)\n\n# create the submission file\ntest_ids = test_df[\"id\"].values\ntest_ids = test_ids.reshape((len(test_ids), 1))\ntest_predicts = pd.DataFrame(data=test_predicts, columns=CLASSES)\ntest_predicts[\"id\"] = test_ids\ntest_predicts = test_predicts[[\"id\"] + CLASSES]\nsubmit_path = submit_path_prefix + \"-Loss{:4f}.csv\".format(val_loss)\ntest_predicts.to_csv(submit_path, index=False)","5e0a8bf5":"> [Class Slides](https:\/\/github.com\/IKMLab\/sentence_classification_tutorial)\n\nIn this competition, you\u2019re challenged to build a multi-headed model that\u2019s capable of detecting different types of of toxicity like threats, obscenity, insults, and identi****ty-based hate better than Perspective\u2019s current models. You\u2019ll be using a dataset of comments from Wikipedia\u2019s talk page edits. Improvements to the current model will hopefully help online discussion become more productive and respectful.","e832b4b5":"## Hybrid Text NN \n\n![RCNN](https:\/\/raw.githubusercontent.com\/IKMLab\/sentence_classification_tutorial\/9cde37484f94b84fe1e47bb199553640b8b99a5e\/resources\/hybrid.png)\n\nThis structure mixed the feature representation ideas of RNN and CNN. We firstly place the recurrent layer after embedding for building sequential information on word level and make it connected with a convolution layer to extract the regional features of hiddens. (LB: 30%, AUC: 0.983)","4a221c7b":"## Preprocessing\u00b6\n\nWe apply 2 propressing method on texts:\n\n* Make all alphabet lower case:\n    * It is very important. We do not want the model cosider 'Hello' and 'hello' as different words.\n* Remove some special tokens and deal with postfix:\n    * For example: what's -> what is, aren't -> are not. Giving the same concept as same token helps for regularization.\n\nAlways remember to do preprocess for NLP task. In many cases,  model trianed with cleaned text significantly outperforms the model trained with raw data. Knowing your data is always the best policy.\n","273ab209":"## Have a look on transformed sequences","0aac0873":"## Check the labels distribution\u00b6\n### Check the balance of labels\n\nWe would like to know the positive ratio of training set. Because we do not want the model become a lazy guy, for a less frequent positive case, we may give it more penalty when model targets it wrong.","7768bcbf":"## Expolary Data Analysis\n","1e29af2e":"## Practice: Try to beat the baseline: val_loss=0.050\u00b6\n","4af28566":"> ## Practice 3","2b8387bd":"## Early Stopping\nTo prevent from overfitting, we have to divide our training data into the training set and the validation set.\n\n![EarlyStopping](https:\/\/raw.githubusercontent.com\/IKMLab\/sentence_classification_tutorial\/9cde37484f94b84fe1e47bb199553640b8b99a5e\/resources\/early-stopping-graphic.jpg)","387bd05a":"## Make the predections","e08b43bb":"## Encoding\nNow, we can split a sentence into serveral word tokens. Since the input of the neural network is a digit not a word,  the tokenizer moreover maintains a encoding table, which map a word to a id.\n\n![Encoding](https:\/\/raw.githubusercontent.com\/IKMLab\/sentence_classification_tutorial\/9cde37484f94b84fe1e47bb199553640b8b99a5e\/resources\/onehot.png)","592de2d5":"## TextCNN\n\n![TextCNN](https:\/\/raw.githubusercontent.com\/IKMLab\/sentence_classification_tutorial\/9cde37484f94b84fe1e47bb199553640b8b99a5e\/resources\/textcnn.png)\n\nConvolution in natural langauge proceessing can be consider as a special type of ngram. We simply select the kernels with window sizes (2, 3, 4) to extract regional features. (LB: 50%, AUC: 0.982)\n","ba08d0cc":"## Prepare the training \/ testing data","ed0f6b9a":"> ## Practice 1","dd59e208":"> ## Practice 2","96988979":"\n# Discussions\n## Better method to compose a sequence of vectors into a single vector ?\n\nEither in CNN or RNN, the outputs are a sequence of vectors, which means a best practice to compose a sequence of vectors into a single one is very important. We have tried to simply select the last one (in RNN) or select the one with max value (in CNN and hybrid NN) to represent a sequence, and there clearly is much room for improvement. For example, how about selecting the top K max vectors? or averaging the whole sequence to get one vector? Furthermore, we can apply weighted averaging to the sequence, which is called Attention in natural language processing and it does help a lot on catching information in long sequences.\n## Jointly training or not ?\n\nThis is a multilabel classification challenge, so why do we jointly train 6 labels together rather than train them one by one? Indeed, this is a good question. In some cases which are labeled sparsely or not clearly related to other classes (like threat in this dataset), training these labels independently might get a better socre because these cases should build their own unique feature representations. You can give it a try, and find the best combination on training labels.\n## The power of unsupervised learing\n\nIn the above tutorial, we just defined a random initialized embedding matrix for text classificaiton. With this method, the embedding matrix will fit well on the training set but it would also be biasd by some special tokens or noise since our dataset is not that large, that cause overfitting.\n\nWe can deal with this by using some pretrained resources, like:\n\n    GloVe embedding\n    Fasttext embedding\n    Word2Vec\n\nYou might get a significant boost by replacing the old matrix with vectors pretrained on a big corpus which catch the similarity between words.\n","487e0d45":"## Summary\n \n* Text-Cleaning: \"Hello, I'm fine.\" -> \"hello i am fine\"\n* Tokenization: \"hello i am fine\" -> [\"hello\", \"i\", \"am\", \"fine\"]\n* Encoding: [\"hello\", \"i\", \"am\", \"fine\"] -> [1, 2, 3, 4]\n* Embedding: [1, 2, 3, 4] -> [[1.4, 3.2],  [0.8, -0.1], [3, 0.9], [-2, -1.3]]","14e8e6eb":"## K-Fold Cross Validation\n![K-fold](https:\/\/raw.githubusercontent.com\/IKMLab\/sentence_classification_tutorial\/9cde37484f94b84fe1e47bb199553640b8b99a5e\/resources\/kfold.png)\n","b0bc56fd":"## Tokenization\nTokenization separates a sentence into words by space, for example:\n* \"Hello world\" -> [\"Hello\", \"world\"]","2d5764a5":"\n## Correlations of labels\n\nBecause this is a mulit-label classification, we will want to know the relation betweens labels, which helps for feature engineering and model design. For example, if we know that a toxic comment is always a insult comment, when we have a high confident toxic comment, we can also consider it as insult comment.","b667bb82":"\n## Embeddings\n\nWe enocde each symbol as a one hot vector which uses an id to represent it to save memory.\n\n```\nCat = [0, 0, 0, 0, 1] = 5\nDog = [0, 0, 1, 0, 0] = 3\n```\n\nHowever, one hot-enoding assumes each symbol is orthogonal to the others. That is, there is no relation between any two symbols, or we can say relation(a, b) = 0 = relation(a, c) for any pairs.\n\n```\nrelation(Cat, Dog) = 0 = relation(Cat, Microphone)\n```\n\nA more reasonable way is to embed each symbol into a vector space, and let the NN figure out the relations by itself.\n\n![word-embedding](https:\/\/raw.githubusercontent.com\/IKMLab\/sentence_classification_tutorial\/9cde37484f94b84fe1e47bb199553640b8b99a5e\/resources\/word-embedding.jpeg)\n\n# Models\n\n## Text RNN\n\n![TextRNN](https:\/\/raw.githubusercontent.com\/IKMLab\/sentence_classification_tutorial\/9cde37484f94b84fe1e47bb199553640b8b99a5e\/resources\/textrnn.png)\n\nHere we present a classical structure, 2 layer bidirectional GRU, for text classification. Instead of adding a fully connected layer after all time steps, here we only select the last hidden unit of sequence. (LB: 50%, AUC: 0.982)"}}