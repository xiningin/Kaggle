{"cell_type":{"bab2f2f2":"code","9b4ded15":"code","926e7a4c":"code","3c6dadd0":"code","e64a4953":"code","fa2928df":"code","f43a65b7":"code","1cad21d7":"code","835a10bd":"code","c5998a96":"code","52b6654e":"code","1076cf9c":"code","76207af8":"code","2eb2d740":"code","65b01687":"code","51eab2f2":"code","394787f4":"code","6adeb57d":"code","64950fb7":"code","8f42ac73":"code","ea83cd32":"code","2b5ba98d":"code","41569823":"code","9612c2a2":"code","33ff40e8":"code","bdc1ab85":"code","968f7d5a":"code","299824d4":"code","a82561b3":"code","af2c4221":"code","1e3d2549":"code","7e17703f":"code","7c9a1813":"code","3c84e1bb":"code","f5ea4d9a":"code","2b568609":"code","76879a73":"code","f882bf6c":"code","e239ff03":"code","4246b36f":"code","f03dc885":"code","ed946fe1":"code","38208bd9":"code","86f2b03a":"code","abcd3f1e":"code","7b311318":"code","244e2c37":"code","6a8796a7":"code","72e63b18":"code","d832148d":"markdown","548be9cf":"markdown","ec6184b7":"markdown","d76ce70d":"markdown","7da7fac0":"markdown","b50ab6dd":"markdown","44dbc419":"markdown","57c61f51":"markdown","158547f0":"markdown","258a237d":"markdown","cb48ab65":"markdown","8453247b":"markdown","daf9a2bc":"markdown","0ec0f63a":"markdown","b34fa90b":"markdown"},"source":{"bab2f2f2":"# import packages\nimport pandas as pd\nimport seaborn as sns\nimport statsmodels.formula.api as smf\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport numpy as np","9b4ded15":"# open train df\ndf = pd.read_csv('\/kaggle\/input\/santander-customer-transaction-prediction\/train.csv')","926e7a4c":"df_test = pd.read_csv('\/kaggle\/input\/santander-customer-transaction-prediction\/test.csv')","3c6dadd0":"# check if there are missing values\n# verificando se existem dados nulos\nimport missingno as msno\nmsno.matrix(df)","e64a4953":"# print df head\ndf.head()","fa2928df":"# print df shape\ndf.shape","f43a65b7":"# print df info\ndf.info()","1cad21d7":"#describe the df variables\ndf.describe().T","835a10bd":"df.target.value_counts()","c5998a96":"sns.countplot(y = df.target)","52b6654e":"## correlation between target variable and the other columns\ndf_corr = df.corr().round(2)['target']","1076cf9c":"df_corr.sort_values(ascending =True)","76207af8":"df.head()","2eb2d740":"# I'll drop the ID_code and target variables of the dataset to do the PCA with the other variables.\n#df_pca = df.drop(['ID_code'], axis = 1)\nX_train = df.drop(['ID_code', 'target'], axis = 1)\nX_test = df.drop(['ID_code', 'target'], axis = 1)","65b01687":"# importando as bibliotecas\nfrom sklearn.preprocessing import StandardScaler \n\n# instanciando a vari\u00e1vel\nsc = StandardScaler() \n\n\n# ajustando com os dados de treino\nX_train = sc.fit_transform(X_train) \nX_test = sc.fit_transform(X_test)","51eab2f2":"# importando as bibliotecas\nfrom sklearn.decomposition import PCA\n\n# instanciando o modelo\npca = PCA(n_components = 2)\n\n# ajustando com os modelos de treino\nX_train_pca = pca.fit_transform(X_train)\n\n# transformando os dados de teste\nX_test_pca = pca.transform(X_test)","394787f4":"X_train_pca","6adeb57d":"# Plotando um gr\u00e1fico de dispers\u00e3o entre as duas vari\u00e1veis criadas pelo PC\n# para identifica\u00e7\u00e0o de como o PCA distribuiu as categorias\ndf2 = df.copy(deep=True)\n# cria\u00e7\u00e3o de duas novas colunas com as 2 dimens\u00f5es do PCA\ndf2['PCA1'] = X_train_pca[:, 0]\ndf2['PCA2'] = X_train_pca[:, 1]\n# cria\u00e7\u00e3o de duas novas colunas com as 2 dimens\u00f5es do PCA\ndf2['PCA1_test'] = X_test_pca[:, 0]\ndf2['PCA2_test'] = X_test_pca[:, 1]","64950fb7":"fig, axs = plt.subplots(1,2)\n# plotando uma dispers\u00e3o das novas colunas diferenciando as esp\u00e9cies\nsns.lmplot('PCA1', 'PCA2', hue='target', data=df2, fit_reg=False);\n# plotando uma dispers\u00e3o das novas colunas diferenciando as esp\u00e9cies\nsns.lmplot('PCA1_test', 'PCA2_test', hue='target', data=df2, fit_reg=False);","8f42ac73":"df.target","ea83cd32":"# importando as bibliotecas\nfrom sklearn.linear_model import LogisticRegression\n\n# instanciando o modelo \nclassifier = LogisticRegression(random_state = 42)\n# ajustando o modelo\nclassifier.fit(X_train_pca, df.target) ","2b5ba98d":"# predi\u00e7\u00e3o de valores com dados de teste com a Regress\u00e3o Log\u00edstica\ny_pred_pca = classifier.predict(X_train_pca) ","41569823":"# plotando a Matriz de Confus\u00e3o entre os valores reais e preditos\n\n# importando a biblioteca\nfrom sklearn.metrics import confusion_matrix \n# plotando a matriz\ncm = confusion_matrix(df.target, y_pred_pca) \ncm","9612c2a2":"sns.heatmap(cm,annot=True,cbar=False, xticklabels='auto' )","33ff40e8":"\n# importar a funcao\nfrom sklearn import metrics\n\n# plotando a curva ROC\ny_pred_proba_pca = classifier.predict_proba(X_train_pca)[::,1]\nfpr, tpr, _ = metrics.roc_curve(df.target,  y_pred_proba_pca)\nauc = metrics.roc_auc_score(df.target, y_pred_proba_pca)\nplt.plot(fpr,tpr,label=\"data 1, auc=\"+str(auc))\nplt.plot([0, 1], [0, 1], color='red', lw=2, linestyle='--')\nplt.legend(loc=4)","bdc1ab85":"# importando as bibliotecas\nfrom sklearn.preprocessing import StandardScaler \n\n# instanciando a vari\u00e1vel\nsc = StandardScaler() \n\n\n# ajustando com os dados de treino\nX= df.drop(['ID_code', 'target'], axis = 1)\nX = sc.fit_transform(X) \ny= df.target","968f7d5a":"# import the function\nfrom sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 42, stratify = y)","299824d4":"print(len(y_train))\nprint(len(X_train))\nprint(len(y_test))\nprint(len(X_test))","a82561b3":"# importar a funcao\nfrom sklearn.linear_model import LogisticRegression\n\n# isntanciar o modelo\nclf = LogisticRegression(dual = False, max_iter = 5000)\n\n# ajustar aos dados de treino\nclf.fit(X_train, y_train)\n\n# predi\u00e7\u00f5es para os dados de teste\ny_pred = clf.predict(X_test)","af2c4221":"# importar a funcao\nfrom sklearn import metrics\n\nconfusion_matrix(y_test, y_pred)","1e3d2549":"# chamando a fun\u00e7\u00e3o da matriz de confus\u00e3o\nmetrics.confusion_matrix","7e17703f":"print(\"Accuracy:\", metrics.accuracy_score(y_test, y_pred))\nprint(\"Precision:\", metrics.precision_score(y_test, y_pred))\nprint(\"Recall:\",metrics.recall_score(y_test, y_pred))","7c9a1813":"# plotando a curva ROC\ny_pred_proba = clf.predict_proba(X_test)[::,1]\nfpr, tpr, _ = metrics.roc_curve(y_test,  y_pred_proba)\nauc = metrics.roc_auc_score(y_test, y_pred_proba)\nplt.plot(fpr,tpr,label=\"data 1, auc=\"+str(auc))\nplt.plot([0, 1], [0, 1], color='red', lw=2, linestyle='--')\nplt.legend(loc=4)","3c84e1bb":"len(df)","f5ea4d9a":"# fazendo um undersampling da classe com output zero (em maior n\u00famero)\ndf_sample=df.sample(n=10000)","2b568609":"# importando as bibliotecas\nfrom sklearn.preprocessing import StandardScaler \n\n# instanciando a vari\u00e1vel\nsc = StandardScaler() \n\n\n# ajustando com os dados de treino\nX_sp= df_sample.drop(['ID_code', 'target'], axis = 1)\nX_sp = sc.fit_transform(X_sp) \ny_sp= df_sample.target\n\nfrom sklearn.model_selection import train_test_split\n\nXsp_train, Xsp_test, ysp_train, ysp_test = train_test_split(X_sp, y_sp, test_size = 0.2, random_state = 42, stratify = y_sp)","76879a73":"print(len(Xsp_train))\nprint(len(Xsp_test))","f882bf6c":"# importando as bibliotecas dos modelos classificadores\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\nfrom sklearn.naive_bayes import GaussianNB\n\n# definindo uma lista com todos os modelos\nclassifiers = [\n    KNeighborsClassifier(),\n    GaussianNB(),\n    LogisticRegression(dual=False,max_iter=5000),\n    SVC(),\n    DecisionTreeClassifier(),\n    RandomForestClassifier(),\n    GradientBoostingClassifier()]\n\n# rotina para instanciar, predizer e medir os resultados de todos os modelos\nfor clf in classifiers:\n    # instanciando o modelo\n    clf.fit(Xsp_train, ysp_train)\n    # armazenando o nome do modelo na vari\u00e1vel name\n    name = clf.__class__.__name__\n    # imprimindo o nome do modelo\n    print(\"=\"*30)\n    print(name)\n    # imprimindo os resultados do modelo\n    print('****Results****')\n    ysp_pred = clf.predict(Xsp_test)\n    print(\"Accuracy:\", metrics.accuracy_score(ysp_test, ysp_pred))\n    print(\"Precision:\", metrics.precision_score(ysp_test, ysp_pred))\n    print(\"Recall:\", metrics.recall_score(ysp_test, ysp_pred))\n    \n     # plotando a curva ROC\n    y_pred_proba = clf.predict_proba(X_test)[::,1]\n    fpr, tpr, _ = metrics.roc_curve(y_test,  y_pred_proba)\n    auc = metrics.roc_auc_score(y_test, y_pred_proba)\n    plt.plot(fpr,tpr,label=name+\", auc=\"+str(auc))\n    plt.plot([0, 1], [0, 1], color='red', lw=2, linestyle='--')\n    plt.legend(loc=4)","e239ff03":"# definindo vari\u00e1veis para cada uma das classes\ndf_0 = df[df.target == 0]\ndf_1 = df[df.target==1]","4246b36f":"print(len(df_0))\nprint(len(df_1))","f03dc885":"# undersampling\ndf_0=df_0.sample(len(df_1))","ed946fe1":"df_concat = pd.concat([df_0,df_1])\ndf_concat.target.value_counts()","38208bd9":"# ajustando com os dados de treino\nX= df_concat.drop(['ID_code', 'target'], axis = 1)\nX = sc.fit_transform(X) \ny = df_concat.target\n\nfrom sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 42, stratify = y)","86f2b03a":"# ignorando os warnings\nimport warnings\nwarnings.simplefilter(action='ignore', category=FutureWarning)\n\n# importnado as bibliotecas com os modelos classificadores\n\n# definindo uma lista com todos os classificadores\nclassifiers = [\n    KNeighborsClassifier(),\n    GaussianNB(),\n    LogisticRegression(),\n    #SVC(),\n    DecisionTreeClassifier(),\n    RandomForestClassifier(),\n    GradientBoostingClassifier()]\n\n# definindo o tamanho da figura para o gr\u00e1fico\nplt.figure(figsize=(12,8))\n\n# rotina para instanciar, predizer e medir os rasultados de todos os modelos\nfor clf in classifiers:\n    # instanciando o modelo\n    clf.fit(X_train, y_train)\n    # armazenando o nome do modelo na vari\u00e1vel name\n    name = clf.__class__.__name__\n    # imprimindo o nome do modelo\n    print(\"=\"*30)\n    print(name)\n    # imprimindo os resultados do modelo\n    print('****Results****')\n    y_pred = clf.predict(X_test)\n    print(\"Accuracy:\", metrics.accuracy_score(y_test, y_pred))\n    print(\"Precision:\", metrics.precision_score(y_test, y_pred))\n    print(\"Recall:\", metrics.recall_score(y_test, y_pred))\n    \n    \n    # plotando a curva ROC\n    y_pred_proba = clf.predict_proba(X_test)[::,1]\n    fpr, tpr, _ = metrics.roc_curve(y_test,  y_pred_proba)\n    auc = metrics.roc_auc_score(y_test, y_pred_proba)\n    plt.plot(fpr,tpr,label=name+\", auc=\"+str(auc))\n    plt.plot([0, 1], [0, 1], color='red', lw=2, linestyle='--')\n    plt.legend(loc=4)","abcd3f1e":"# importar a funcao\nfrom sklearn.naive_bayes import GaussianNB\n\n# isntanciar o modelo\nclassifier = GaussianNB()\n\n# ajustar aos dados de treino\nclassifier.fit(X_train, y_train)\n\n# predi\u00e7\u00f5es para os dados de teste\ny_pred = clf.predict(X_test)","7b311318":"metrics.confusion_matrix\n\nprint(\"Accuracy:\", metrics.accuracy_score(y_test, y_pred))\nprint(\"Precision:\", metrics.precision_score(y_test, y_pred))\nprint(\"Recall:\",metrics.recall_score(y_test, y_pred))","244e2c37":"# plotando a curva ROC\ny_pred_proba = classifier.predict_proba(X_test)[::,1]\nfpr, tpr, _ = metrics.roc_curve(y_test,  y_pred_proba)\nauc = metrics.roc_auc_score(y_test, y_pred_proba)\nplt.plot(fpr,tpr,label=\"data 1, auc=\"+str(auc))\nplt.plot([0, 1], [0, 1], color='red', lw=2, linestyle='--')\nplt.legend(loc=4)","6a8796a7":"# Verificando assimetria:\nfrom scipy import stats\n\n# choose numeric features\nnumeric_feats = df_concat.dtypes[df_concat.dtypes !=\"object\"].index\n\nskewed_feats = df_concat[numeric_feats].apply(lambda x: stats.skew(x.dropna())).sort_values(ascending = False)\n\nprint(\"\\nAssimetria: \\n\")\nskew_df = pd.DataFrame({'Skew' :skewed_feats})\nskew_df.head(20)","72e63b18":"norm = np.linalg.norm(df.var_179)\nnormal_array = df.var_179\/norm\nnormal_array.skew()","d832148d":"https:\/\/medium.com\/@alepukhova526\/principal-component-analysis-for-logistic-regression-with-scikit-learn-869b04b2f923","548be9cf":"# Algumas possibilidades a serem consideradas para desenvolvimento neste Desafio 3:\n\n1. Explorar as vari\u00e1veis explicat\u00f3rias do problema e entender como elas influenciam na vari\u00e1vel target\n\n\n2. Identificar poss\u00edveis solu\u00e7\u00f5es para resolver o desbalancemanto de classes da vari\u00e1vel target\n\n\n3. Entender a explicabilidade dos dados utilizando diferentes t\u00e9cnicas n\u00e3o supervisionadas de Clustering e medir como os modelos respondem \n\n\n4. Buscar algumas formas de selecionar as melhores vari\u00e1veis para classificar as fraudes e medir os resultados em um modelo preditor\n\n\n5. Verificar se a redu\u00e7\u00e3o de dimensionalidade pode ajudar na explica\u00e7\u00e3o das fraudes, com suficiente n\u00edvel de informa\u00e7\u00e3o, e medir os resultados em um modelo preditor\n\n\n6. Submeter o melhor resultado na plataforma Keggle. Para isso, verifique qual a m\u00e9trica adotada pelo examinador do desafio\n\n","ec6184b7":"### Santander DataFrame","d76ce70d":"auc = 0.859","7da7fac0":"Accuracy: 0.9132\n\nPrecision: 0.6783854166666666\n\nRecall: 0.25920398009950246","b50ab6dd":"## Classification","44dbc419":"## Gaussian NB","57c61f51":"The PCA didn't perform very well on data compression.","158547f0":"<img src=\"img\/dh_logo.png\" align=\"right\" width=\"50%\">\n\n# Desafio 3\n\n# Santander Kaggle\n## Customer Transaction Prediction\n\n<img src=\"img\/santander.png\"  >\n\n### Neste desafio ser\u00e1 necess\u00e1rio prever a vari\u00e1vel target do DataSet\n\nPara acessar e participar desta competi\u00e7\u00e3o no Kaggle [clique aqui](https:\/\/www.kaggle.com\/c\/santander-customer-transaction-prediction\/overview)\n\nPara baixar os dados [clique aqui](https:\/\/www.kaggle.com\/c\/santander-customer-transaction-prediction\/data)\n\n## Santander: Overview da proposta do desafio\n\nNo Santander, nossa miss\u00e3o \u00e9 ajudar pessoas e empresas a prosperar. Estamos sempre procurando maneiras de ajudar nossos clientes a entender sua sa\u00fade financeira e identificar quais produtos e servi\u00e7os podem ajud\u00e1-los a atingir suas metas monet\u00e1rias.\n\nNossa equipe de ci\u00eancia de dados est\u00e1 desafiando continuamente nossos algoritmos de aprendizado de m\u00e1quina, trabalhando com a comunidade global de dados cient\u00edficos para garantir que possamos identificar com mais precis\u00e3o novas maneiras de resolver nosso desafio mais comum, problemas de classifica\u00e7\u00e3o bin\u00e1ria como: um cliente est\u00e1 satisfeito? Um cliente comprar\u00e1 este produto? Um cliente pode pagar este empr\u00e9stimo?\n\nNeste desafio, convidamos a Kagglers a nos ajudar a identificar **quais clientes far\u00e3o uma transa\u00e7\u00e3o espec\u00edfica no futuro, independentemente do volume de dinheiro transacionado**. Os dados fornecidos para esta competi\u00e7\u00e3o t\u00eam a mesma estrutura que os dados reais que temos dispon\u00edveis para resolver este problema.","258a237d":"You are provided with an anonymized dataset containing numeric feature variables, the binary target column, and a string ID_code column.\n\nThe task is to predict the value of target column in the test set.","cb48ab65":"### Regress\u00e3o Log\u00edstica","8453247b":"200000 rows and 202 columns.","daf9a2bc":"The variables aren't very correlated with the target value. As there are many variables I'll conduct a PCA to reduce the dataset dimensionality.","0ec0f63a":"### Balancing","b34fa90b":"## PCA"}}