{"cell_type":{"159d1f3b":"code","63f69b35":"code","7d6db565":"code","afd70390":"code","22356a14":"code","c962c50a":"code","8e4fac99":"code","e45ad741":"code","3da712c9":"code","6e52006b":"code","d054f1b2":"code","4c419db5":"code","7223a11d":"code","85c52517":"code","87ee5798":"code","b03defc8":"code","305aaee5":"code","7374ad2f":"code","a11c3bab":"code","ae27722d":"code","f7221a15":"code","9d15ce9d":"code","9b2927dd":"code","e6caf0f9":"code","ebf441c4":"code","b65f1b18":"code","507784df":"code","4cb3a435":"code","e9c8e9e4":"code","36fea5da":"code","45f2a399":"code","46c3032e":"code","5da96a8b":"code","fbfb1367":"code","b5dd6c8c":"code","32a44de6":"code","ff6aeb7e":"code","1b4bc95d":"code","5f506980":"code","50027719":"code","27c8998f":"code","43bc5ad7":"code","1ed9b834":"code","08d02216":"code","009b38e7":"code","50d18839":"code","e4db90e6":"code","233fd0d4":"code","caf922c8":"code","0cad6063":"code","12c0922c":"code","4ba97488":"code","9a8a5edb":"markdown","88dc522f":"markdown","9515dc37":"markdown","c974de9d":"markdown","1d217b70":"markdown","1c3c541c":"markdown","52d8d3c2":"markdown","72d98270":"markdown","102ed16b":"markdown","db8d97ba":"markdown","42b7a986":"markdown","35d4f83b":"markdown","130fcce5":"markdown","810bed9d":"markdown","22496320":"markdown","5ea84c65":"markdown","8305851d":"markdown","99052da4":"markdown","9dd71c9e":"markdown","d32b44ac":"markdown","bcac7aa4":"markdown","c63dbae1":"markdown","cc39eb3b":"markdown","d7e565b7":"markdown","5e6d0a1e":"markdown","48725ff9":"markdown","33ecaa88":"markdown","71972034":"markdown","15c57f7e":"markdown"},"source":{"159d1f3b":"!pip install sweetviz\n!pip install scikit-learn==0.23.1\nimport numpy as np\nimport pandas as pd\nimport sweetviz as sv\nimport os\nimport warnings\nfrom IPython.display import clear_output\nwarnings.filterwarnings('ignore')\nclear_output()","63f69b35":"import torch\nimport torch.nn as nn\nfrom torch.nn import functional as F\nfrom torch.autograd import Variable\nfrom torch.utils.data import Dataset, DataLoader, random_split\nfrom tqdm.notebook import tqdm\nfrom sklearn.preprocessing import LabelEncoder","7d6db565":"from pathlib import Path\nroot_dir = Path(\"\/kaggle\")\ndata_dir = root_dir \/ \"input\/titanic\"\nckpt_dir = root_dir \/ \"working\/checkpoints\"\nlog_dir = root_dir \/ \"working\/logs\"\nsubmission_dir = root_dir \/ \"working\/submissions\"\nlist(root_dir.iterdir())","afd70390":"train_data = pd.read_csv(data_dir \/ \"train.csv\")\ntest_data = pd.read_csv(data_dir \/ \"test.csv\")","22356a14":"train_data.iloc[:3]","c962c50a":"report = sv.compare([train_data, \"Train\"], [test_data, \"Test\"], target_feat=\"Survived\", pairwise_analysis=\"on\")\nreport.show_notebook()","8e4fac99":"data_cleaner = [train_data, test_data]","e45ad741":"test_ids = test_data[\"PassengerId\"]\n\nfor dataset in data_cleaner:\n    # Data Completing\n    dataset[\"Age\"].fillna(dataset[\"Age\"].median(), inplace=True)\n    dataset[\"Embarked\"].fillna(dataset[\"Embarked\"].mode()[0], inplace=True)\n    dataset[\"Fare\"].fillna(dataset[\"Fare\"].median(), inplace = True)\n\n    # Data Cleaning\n    drop_column = ['PassengerId', 'Cabin', 'Ticket']\n    dataset.drop(drop_column, axis=1, inplace=True)\n\n    # Data Creating (Feature Engineering)\n    dataset['FamilySize'] = dataset['SibSp'] + dataset['Parch'] + 1\n    dataset['IsAlone'] = 1  #initialize to 1 = is alone\n    dataset['IsAlone'].loc[dataset['FamilySize'] > 1] = 0  # now update to no if family size is greater than 1\n\n    dataset['Title'] = dataset[\"Name\"].str.split(\", \", expand=True)[1].str.split(\".\", expand=True)[0]\n    title_counts = dataset['Title'].value_counts() < 10\n    dataset[\"Title\"] = dataset[\"Title\"].apply(lambda x: \"Misc\" if title_counts.loc[x] == True else x)\n\n    ## Divide the `Fare` into 4 intervals with similar quantities.\n    dataset['FareBin'] = pd.qcut(dataset['Fare'], 4, labels=False)\n\n    ## Divide the `Age` into 4 discrete intervals according to its value.\n    dataset['AgeBin'] = pd.cut(dataset['Age'], [0, 25, 50, 75, 100], labels=False)\n\n    ## Drop these columns since we have these features in the discrete version.\n    dataset.drop(columns=[\"Name\", \"Age\", \"Fare\"], inplace=True)","3da712c9":"train_data.sample(5)","6e52006b":"for dataset in data_cleaner:\n    label_encoder = LabelEncoder()\n    dataset[\"Sex\"] = label_encoder.fit_transform(dataset[\"Sex\"])\n    dataset[\"Title\"] = label_encoder.fit_transform(dataset[\"Title\"])\n    dataset[\"Embarked\"] = label_encoder.fit_transform(dataset[\"Embarked\"])","d054f1b2":"train_data","4c419db5":"report = sv.compare([train_data, \"Train\"], [test_data, \"Test\"], target_feat=\"Survived\")\nreport.show_notebook()","7223a11d":"y = train_data.pop(\"Survived\")\nX = train_data","85c52517":"!pip install lazypredict==0.2.9\nclear_output()","87ee5798":"from lazypredict.Supervised import LazyClassifier\nfrom sklearn.model_selection import train_test_split\n\ndef lazy_predict(X, y, test_size=0.2):\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=42)\n    clf = LazyClassifier(verbose=0, ignore_warnings=True, custom_metric=None)\n    models, predictions = clf.fit(X_train, X_test, y_train, y_test)\n    return models, predictions\n\nlazy_predict(X, y)[0][:10]","b03defc8":"batch_size = 64\nlr = 0.1\ndropout = 0.3\nhidden_size = 512","305aaee5":"X_train = X.values\ny_train = y.values\nprint(\"train_set shape:\", X_train.shape)\n\nX_test = test_data.values\nprint(\"test_set shape:\", X_test.shape)\n\nfeature_size = X_train.shape[-1]\ntarget_size = 2","7374ad2f":"# Same as torch.utils.data.TensorDataset\nclass MyTensorDataset(Dataset):\n    def __init__(self, X, y=None):\n        if y is None:\n            self.data = X\n        else:\n            self.data = list(zip(X, y))\n    \n    def __len__(self):\n        return len(self.data)\n\n    def __getitem__(self, i):\n        return self.data[i]","a11c3bab":"train_dataset = MyTensorDataset(X_train, y_train)\ntest_dataset = MyTensorDataset(X_test)\n\ntrain_size = int(len(train_dataset) * 0.8)\nval_size = len(train_dataset) - train_size\ntrain_set, val_set = random_split(train_dataset, [train_size, val_size])\n\n\ndef contrived_fn(batch_data):\n    \"\"\"\n    Simulate the behavior of the default collate_fn.\n    The return values must be tensor type.\n    \"\"\"\n    tensor_X = []\n    tensor_y = []\n    for x, y in batch_data:\n        tensor_X.append(x)\n        tensor_y.append(y)\n    \n    return torch.FloatTensor(tensor_X), torch.LongTensor(tensor_y)\n\n# train and val dataloader are moving to ray.tune\ntrain_loader = DataLoader(train_set, batch_size, True, num_workers=4, collate_fn=contrived_fn)\nval_loader = DataLoader(val_set, batch_size, num_workers=4, collate_fn=contrived_fn)\ntest_loader = DataLoader(test_dataset, batch_size, num_workers=4)","ae27722d":"class Net(nn.Module):\n    def __init__(self, feature_size, target_size):\n        super(Net, self).__init__()\n        self.fc1 = nn.Linear(feature_size, hidden_size)\n        self.fc2 = nn.Linear(hidden_size, target_size)\n        self.dropout = nn.Dropout(dropout)\n        \n    def forward(self, x):\n        x = F.relu(self.fc1(x))\n        x = self.dropout(x)\n        x = self.fc2(x)\n        return x","f7221a15":"model = Net(feature_size=feature_size, target_size=target_size)\nmodel","9d15ce9d":"def run_step(model, opt, dev, criterion, batch_X, batch_y, training=True):\n    batch_X = batch_X.to(dev)\n    batch_y = batch_y.to(dev)\n\n    batch_pred = model(batch_X)\n    loss = criterion(batch_pred, batch_y)\n    acc = (batch_pred.max(1)[1] == batch_y).sum().item()\n\n    if training:\n        opt.zero_grad()\n        loss.backward()\n        opt.step()\n\n    return batch_pred, loss, acc\n\n\ndef run_epoch(model, opt, dev, criterion, data_loader, training=True):\n    if training:\n        model.train()\n    else:\n        model.eval()\n    \n    epoch_loss = 0\n    epoch_acc = 0\n\n    for batch_X, batch_y in tqdm(data_loader):\n        _, step_loss, step_acc = run_step(model, opt, dev, criterion, batch_X, batch_y, training)\n        epoch_loss += step_loss\n        epoch_acc += step_acc\n\n    epoch_loss = (epoch_loss \/ len(data_loader)).item()\n    epoch_acc = (epoch_acc \/ len(data_loader.dataset)) * 100\n    return epoch_loss, epoch_acc","9b2927dd":"opt  = torch.optim.SGD(model.parameters(), lr=lr)\ncriterion = nn.CrossEntropyLoss()\ndev  = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")","e6caf0f9":"ckpt_dir.mkdir(exist_ok=True, parents=True)\nlog_dir.mkdir(exist_ok=True, parents=True)","ebf441c4":"epoch = 50\nmodel.to(dev)\n\ntrain_loss = []\ntrain_acc = []\neval_loss = []\neval_acc = []\n\nmin_loss = np.inf\n\nfor i in range(epoch):\n\n    epoch_train_loss, epoch_train_acc = run_epoch(model, opt, dev, criterion, train_loader, training=True)\n    train_loss.append(epoch_train_loss)\n    train_acc.append(epoch_train_acc)\n\n    with torch.no_grad():\n        epoch_eval_loss, epoch_eval_acc = run_epoch(model, opt, dev, criterion, val_loader, training=False)\n        eval_loss.append(epoch_eval_loss)\n        eval_acc.append(epoch_eval_acc)\n\n    if epoch_eval_loss < min_loss:\n        min_loss = epoch_eval_loss\n        torch.save(model.state_dict(), ckpt_dir \/ \"model.pt\")\n\n    print(f\"Epoch {i+1}: \\ntrain=loss: {epoch_train_loss}, acc: {epoch_train_acc}\\nvalidation=loss: {epoch_eval_loss}, acc: {epoch_eval_acc}\")","b65f1b18":"model.load_state_dict(torch.load(ckpt_dir \/ \"model.pt\"))\nmodel.eval()","507784df":"result = []\n\nwith torch.no_grad():\n    for X_test_batch in test_loader:\n        X_test_batch = X_test_batch.to(dev)\n        pred = model(X_test_batch.float())\n        pred = pred.max(1)[1]\n        result.extend(pred.tolist())\n\nsubmission = pd.DataFrame({'PassengerId': test_ids, 'Survived': result})","4cb3a435":"!pip install --upgrade pandas\nclear_output()","e9c8e9e4":"submission_dir.mkdir(exist_ok=True, parents=True)\nsubmission.to_csv(submission_dir \/ \"torch_submission.csv\", index=False)\nsubmission","36fea5da":"!pip install git+https:\/\/github.com\/PyTorchLightning\/pytorch-lightning\nimport pytorch_lightning as pl\nclear_output()","45f2a399":"from torch.utils.data import TensorDataset\n\nclass TitanicDataModule(pl.LightningDataModule):\n    def __init__(self, data_dir, batch_size):\n        super().__init__()\n        self.batch_size = batch_size\n        self.data_dir = data_dir\n\n\n    def prepare_data(self):\n        \"\"\"\n        Download datasets here. Not to assign variables here.\n        ie: `prepare_data` is called from a single GPU. Do not use it to assign state (self.x = y).\n        \"\"\"\n\n\n    def setup(self, stage=None):\n        if stage == \"fit\":\n            full_dataset = pd.read_csv(self.data_dir \/ \"train.csv\")\n            full_dataset = self._data_preprocess(full_dataset)\n\n            y = full_dataset.pop(\"Survived\")\n            X = full_dataset\n            full_dataset = TensorDataset(torch.Tensor(X.values), torch.Tensor(y.values).long())\n\n            train_size = int(len(full_dataset) * 0.8)\n            val_size = len(full_dataset) - train_size\n            self.train_set, self.val_set = random_split(full_dataset, [train_size, val_size])\n\n        if stage == \"test\":\n            test_dataset = pd.read_csv(self.data_dir \/ \"test.csv\")\n            self.test_ids = test_dataset[\"PassengerId\"]\n\n            test_dataset = self._data_preprocess(test_dataset)\n            self.test_set = TensorDataset(torch.Tensor(test_dataset.values))\n            \n\n    def train_dataloader(self):\n        return DataLoader(self.train_set, self.batch_size, shuffle=True, num_workers=8, pin_memory=True)\n\n\n    def val_dataloader(self):\n        return DataLoader(self.val_set, self.batch_size, shuffle=False, num_workers=8, pin_memory=True)\n    \n\n    def test_dataloader(self):\n        return DataLoader(self.test_set, self.batch_size, shuffle=False, num_workers=8, pin_memory=True)\n\n\n    def _data_preprocess(self, pd_dataset):\n        pd_dataset[\"Age\"].fillna(pd_dataset[\"Age\"].median(), inplace=True)\n        pd_dataset[\"Embarked\"].fillna(pd_dataset[\"Embarked\"].mode()[0], inplace=True)\n        pd_dataset[\"Fare\"].fillna(pd_dataset[\"Fare\"].median(), inplace = True)\n        # Data Cleaning\n        drop_column = ['PassengerId', 'Cabin', 'Ticket']\n        pd_dataset.drop(drop_column, axis=1, inplace=True)\n        # Data Creating (Feature Engineering)\n        pd_dataset['FamilySize'] = pd_dataset['SibSp'] + pd_dataset['Parch'] + 1\n        pd_dataset['IsAlone'] = 1  #initialize to 1 = is alone\n        pd_dataset['IsAlone'].loc[pd_dataset['FamilySize'] > 1] = 0  # now update to no if family size is greater than 1\n        pd_dataset['Title'] = pd_dataset[\"Name\"].str.split(\", \", expand=True)[1].str.split(\".\", expand=True)[0]\n        title_counts = pd_dataset['Title'].value_counts() < 10\n        pd_dataset[\"Title\"] = pd_dataset[\"Title\"].apply(lambda x: \"Misc\" if title_counts.loc[x] == True else x)\n        ## Divide the `Fare` into 4 intervals with similar quantities.\n        pd_dataset['FareBin'] = pd.qcut(pd_dataset['Fare'], 4, labels=False)\n        ## Divide the `Age` into 4 discrete intervals according to its value.\n        pd_dataset['AgeBin'] = pd.cut(pd_dataset['Age'], [0, 25, 50, 75, 100], labels=False)\n        ## Drop these columns since we have these features in the discrete version.\n        pd_dataset.drop(columns=[\"Name\", \"Age\", \"Fare\"], inplace=True)\n\n        label_encoder = LabelEncoder()\n        pd_dataset[\"Sex\"] = label_encoder.fit_transform(pd_dataset[\"Sex\"])\n        pd_dataset[\"Title\"] = label_encoder.fit_transform(pd_dataset[\"Title\"])\n        pd_dataset[\"Embarked\"] = label_encoder.fit_transform(pd_dataset[\"Embarked\"])\n\n        return pd_dataset","46c3032e":"dm = TitanicDataModule(data_dir, 64)\n\n# Test whether the data module works by setting it manually.\ndm.setup(\"fit\")\nfirst_batch, *_ = dm.train_dataloader()\nprint(first_batch[0].shape)\n\ndm.setup(\"test\")\nfirst_batch, *_ = dm.test_dataloader()\nprint(first_batch[0].shape)","5da96a8b":"class TitanicModel(pl.LightningModule):\n    def __init__(self, feature_size, hidden_size, target_size, dropout, lr):\n        # super(TitanicModel, self).__init__()\n        super().__init__()\n        self.fc1 = nn.Linear(feature_size, hidden_size)\n        self.fc2 = nn.Linear(hidden_size, target_size)\n        self.dropout = nn.Dropout(dropout)\n        self.lr = lr\n        self.save_hyperparameters()\n\n\n    def forward(self, x):\n        x = F.relu(self.fc1(x))\n        x = self.dropout(x)\n        x = self.fc2(x)\n        return x\n\n\n    def training_step(self, batch, batch_idx):\n        X, y = batch\n        pred = self(X)\n        loss = F.cross_entropy(pred, y)\n        acc = self.accuracy(pred, y)\n        \n        # logs metrics for each training_step,\n        # and the average across the epoch, to the progress bar and logger.\n        # detail: https:\/\/pytorch-lightning.readthedocs.io\/en\/stable\/common\/lightning_module.html#log\n        self.log_dict({\"train_loss\": loss, \"train_acc\": acc}, on_step=False, on_epoch=True, prog_bar=True)\n\n        # must return loss for continued training (ie: grad and step)\n        return {\"loss\": loss, \"pred\": pred}\n\n\n    def training_epoch_end(self, output):\n        \"\"\"\n        If you need to do something with all the outputs of each training_step, override training_epoch_end yourself.\n        \"\"\"\n\n\n    def validation_step(self, batch, batch_idx):\n        X, y = batch\n        pred = self(X)\n        loss = F.cross_entropy(pred, y)\n        acc = self.accuracy(pred, y)\n        \n        self.log_dict({\"val_loss\": loss, \"val_acc\": acc}, on_step=False, on_epoch=True, prog_bar=True)\n\n        # return when you want to do something at validation_epoch_end()\n        # return pred\n    \n\n    def validation_epoch_end(self, output):\n        \"\"\"\n        If you need to do something with all the outputs of each validation_step, override validation_epoch_end.\n        \"\"\"        \n\n\n    def test_step(self, batch, batch_idx):\n        X = batch[0]\n        pred = self(X)\n        return pred\n\n    \n    def test_epoch_end(self, output):\n        pred = torch.cat([batch.max(1)[1] for batch in output])\n        self.test_result = pred.detach()\n\n    \n    def configure_optimizers(self):\n        return torch.optim.SGD(self.parameters(), lr=self.lr)\n\n\n    def accuracy(self, batch_pred, batch_y):\n        correct = (batch_pred.max(1)[1] == batch_y).sum().detach()\n        accuracy = correct \/ len(batch_y)\n        return torch.tensor(accuracy)\n","fbfb1367":"model = TitanicModel(10, 512, 2, 0.3, 0.1)","b5dd6c8c":"# define model checkpoint\ncheckpoint = pl.callbacks.ModelCheckpoint(dirpath=ckpt_dir,  # path for saving checkpoints\n                                          filename=\"{epoch}-{val_loss:.3f}\",\n                                          monitor=\"val_loss\",\n                                          mode=\"min\")","32a44de6":"trainer = pl.Trainer(fast_dev_run=False,\n                     max_epochs=100,\n                     default_root_dir=log_dir,  # path for saving logs\n                     weights_save_path=ckpt_dir,  # path for saving checkpoints\n                     callbacks=[checkpoint])\n\ntrainer.fit(model, dm)","ff6aeb7e":"# Start tensorboard.\n%load_ext tensorboard\n%tensorboard --logdir \/kaggle\/working\/logs\/","1b4bc95d":"model.load_from_checkpoint(checkpoint.best_model_path)","5f506980":"trainer.test(model, datamodule=dm)\nsubmission = pd.DataFrame({'PassengerId': dm.test_ids, 'Survived': model.test_result})\nsubmission.to_csv(submission_dir \/ \"torch_lightning_submission.csv\", index=False)\nsubmission","50027719":"!pip install ray[tune]\nclear_output()","27c8998f":"from ray import tune\nfrom ray.tune import CLIReporter\nfrom ray.tune.schedulers import ASHAScheduler, PopulationBasedTraining\nfrom ray.tune.integration.pytorch_lightning import TuneReportCallback, TuneReportCheckpointCallback","43bc5ad7":"config = {\n    \"batch_size\": tune.choice([64, 128, 256]),\n    \"hidden_size\": tune.grid_search([128, 256, 512]),\n    \"dropout\": tune.uniform(0.1, 0.3),\n    \"lr\": tune.loguniform(0.01, 0.1),\n}","1ed9b834":"from torch.utils.data import TensorDataset\n\nclass TitanicDataModule(pl.LightningDataModule):\n    def __init__(self, data_dir, config):\n        super().__init__()\n        self.data_dir = data_dir\n        self.batch_size = config[\"batch_size\"]\n\n\n    def prepare_data(self):\n        \"\"\"\n        Download datasets here. Not to assign variables here.\n        ie: `prepare_data` is called from a single GPU. Do not use it to assign state (self.x = y).\n        \"\"\"\n\n\n    def setup(self, stage=None):\n        if stage == \"fit\":\n            full_dataset = pd.read_csv(self.data_dir \/ \"train.csv\")\n            full_dataset = self._data_preprocess(full_dataset)\n\n            y = full_dataset.pop(\"Survived\")\n            X = full_dataset\n            full_dataset = TensorDataset(torch.Tensor(X.values), torch.Tensor(y.values).long())\n\n            train_size = int(len(full_dataset) * 0.7)\n            val_size = len(full_dataset) - train_size\n            self.train_set, self.val_set = random_split(full_dataset, [train_size, val_size])\n\n        if stage == \"test\":\n            test_dataset = pd.read_csv(self.data_dir \/ \"test.csv\")\n            self.test_ids = test_dataset[\"PassengerId\"]\n\n            test_dataset = self._data_preprocess(test_dataset)\n            self.test_set = TensorDataset(torch.Tensor(test_dataset.values))\n            \n\n    def train_dataloader(self):\n        return DataLoader(self.train_set, self.batch_size, shuffle=True, num_workers=8, pin_memory=True)\n\n\n    def val_dataloader(self):\n        return DataLoader(self.val_set, self.batch_size, shuffle=False, num_workers=8, pin_memory=True)\n    \n\n    def test_dataloader(self):\n        return DataLoader(self.test_set, self.batch_size, shuffle=False, num_workers=8, pin_memory=True)\n\n\n    def _data_preprocess(self, pd_dataset):\n        pd_dataset[\"Age\"].fillna(pd_dataset[\"Age\"].median(), inplace=True)\n        pd_dataset[\"Embarked\"].fillna(pd_dataset[\"Embarked\"].mode()[0], inplace=True)\n        pd_dataset[\"Fare\"].fillna(pd_dataset[\"Fare\"].median(), inplace = True)\n        # Data Cleaning\n        drop_column = ['PassengerId', 'Cabin', 'Ticket']\n        pd_dataset.drop(drop_column, axis=1, inplace=True)\n        # Data Creating (Feature Engineering)\n        pd_dataset['FamilySize'] = pd_dataset['SibSp'] + pd_dataset['Parch'] + 1\n        pd_dataset['IsAlone'] = 1  #initialize to 1 = is alone\n        pd_dataset['IsAlone'].loc[pd_dataset['FamilySize'] > 1] = 0  # now update to no if family size is greater than 1\n        pd_dataset['Title'] = pd_dataset[\"Name\"].str.split(\", \", expand=True)[1].str.split(\".\", expand=True)[0]\n        title_counts = pd_dataset['Title'].value_counts() < 10\n        pd_dataset[\"Title\"] = pd_dataset[\"Title\"].apply(lambda x: \"Misc\" if title_counts.loc[x] == True else x)\n        ## Divide the `Fare` into 4 intervals with similar quantities.\n        pd_dataset['FareBin'] = pd.qcut(pd_dataset['Fare'], 4, labels=False)\n        ## Divide the `Age` into 4 discrete intervals according to its value.\n        pd_dataset['AgeBin'] = pd.cut(pd_dataset['Age'], [0, 25, 50, 75, 100], labels=False)\n        ## Drop these columns since we have these features in the discrete version.\n        pd_dataset.drop(columns=[\"Name\", \"Age\", \"Fare\"], inplace=True)\n\n        label_encoder = LabelEncoder()\n        pd_dataset[\"Sex\"] = label_encoder.fit_transform(pd_dataset[\"Sex\"])\n        pd_dataset[\"Title\"] = label_encoder.fit_transform(pd_dataset[\"Title\"])\n        pd_dataset[\"Embarked\"] = label_encoder.fit_transform(pd_dataset[\"Embarked\"])\n\n        return pd_dataset\n\n\nclass TitanicModel(pl.LightningModule):\n    def __init__(self, feature_size, target_size, config):\n        # super(TitanicModel, self).__init__()\n        super().__init__()\n        self.fc1 = nn.Linear(feature_size, config[\"hidden_size\"])\n        self.fc2 = nn.Linear(config[\"hidden_size\"], target_size)\n        self.dropout = nn.Dropout(config[\"dropout\"])\n        self.lr = config[\"lr\"]\n        self.save_hyperparameters()\n\n\n    def forward(self, x):\n        x = F.relu(self.fc1(x))\n        x = self.dropout(x)\n        x = self.fc2(x)\n        return x\n\n\n    def training_step(self, batch, batch_idx):\n        X, y = batch\n        pred = self(X)\n        loss = F.cross_entropy(pred, y)\n        acc = self.accuracy(pred, y)\n        \n        # logs metrics for each training_step,\n        # and the average across the epoch, to the progress bar and logger.\n        # detail: https:\/\/pytorch-lightning.readthedocs.io\/en\/stable\/common\/lightning_module.html#log\n        self.log_dict({\"train_loss\": loss, \"train_acc\": acc}, on_step=False, on_epoch=True, prog_bar=True)\n\n        # must return loss for continued training (ie: grad and step)\n        return {\"loss\": loss, \"pred\": pred}\n\n\n    def training_epoch_end(self, output):\n        \"\"\"\n        If you need to do something with all the outputs of each training_step, override training_epoch_end yourself.\n        \"\"\"\n\n\n    def validation_step(self, batch, batch_idx):\n        X, y = batch\n        pred = self(X)\n        loss = F.cross_entropy(pred, y)\n        acc = self.accuracy(pred, y)\n        \n        self.log_dict({\"val_loss\": loss, \"val_acc\": acc}, on_step=False, on_epoch=True, prog_bar=True)\n\n        # return when you want to do something at validation_epoch_end()\n        # return pred\n    \n\n    def validation_epoch_end(self, output):\n        \"\"\"\n        If you need to do something with all the outputs of each validation_step, override validation_epoch_end.\n        \"\"\"        \n\n\n    def test_step(self, batch, batch_idx):\n        X = batch[0]\n        pred = self(X)\n        return pred\n\n    \n    def test_epoch_end(self, output):\n        pred = torch.cat([batch.max(1)[1] for batch in output])\n        self.test_result = pred.detach()\n\n    \n    def configure_optimizers(self):\n        return torch.optim.SGD(self.parameters(), lr=self.lr)\n\n\n    def accuracy(self, batch_pred, batch_y):\n        correct = (batch_pred.max(1)[1] == batch_y).sum().detach()\n        accuracy = correct \/ len(batch_y)\n        return torch.tensor(accuracy)","08d02216":"# Example of ASHA Scheduler\nscheduler_asha = ASHAScheduler(\n    max_t=100,\n    grace_period=1,\n    reduction_factor=2,\n)\n\n# Example of PBT\nscheduler_pbt = PopulationBasedTraining(\n        perturbation_interval=4,\n        hyperparam_mutations={\n            \"batch_size\": tune.grid_search([32, 64, 128]),\n            \"hidden_size\": tune.choice([480, 510, 530]),\n            \"lr\": tune.loguniform(1e-2, 1e-1),\n})","009b38e7":"from ray.tune.integration.pytorch_lightning import TuneReportCallback, TuneReportCheckpointCallback\n\ntune_report_callback = TuneReportCheckpointCallback(\n    metrics={\n    \"val_loss\": \"val_loss\",\n    \"val_acc\": \"val_acc\",\n    },\n    filename=\"ray_ckpt\",\n    on=\"validation_end\",\n)","50d18839":"def run_with_tune(config, data_dir=None, feature_size=10, target_size=2, epochs=50, gpus=0, trained_weights=None):\n    model = TitanicModel(feature_size, target_size, config)\n    dm = TitanicDataModule(data_dir, config)\n\n    trainer = pl.Trainer(\n        max_epochs=50,\n        gpus=0,\n        fast_dev_run=False,\n        progress_bar_refresh_rate=0,\n        weights_summary=None,\n        default_root_dir=log_dir \/ \"ray_logs\",  # path for saving logs\n        callbacks=[\n            tune_report_callback,\n        ],\n    )\n\n    if not trained_weights:\n        trainer.fit(model, dm)\n\n    else:\n        model.load_state_dict(trained_weights)\n        trainer.test(model, datamodule=dm)\n        submission = pd.DataFrame({'PassengerId': dm.test_ids, 'Survived': model.test_result})\n\n        ######### \n        # I check the real accuracy using the cheating answer which has 100% accuracy.\n        # r = pd.read_csv(root_dir \/ \"datasets\" \/ \"cheat.csv\")[\"Survived\"].values\n        # s = submission[\"Survived\"].values\n        # print(\"acc: \", sum(s == r) \/ len(r))\n        #########\n\n        submission.to_csv(submission_dir \/ \"ray_tune_submission.csv\", index=False)\n        print(submission)","e4db90e6":"!pip uninstall dataclasses -y\nclear_output()","233fd0d4":"reporter = CLIReporter(\n    parameter_columns=[\"batch_size\", \"hidden_size\", \"lr\"],\n    metric_columns=[\"val_loss\", \"val_acc\", \"training_iteration\"]\n)\n\nresult = tune.run(\n    tune.with_parameters(\n        run_with_tune,\n        data_dir=data_dir,\n        feature_size=10,\n        target_size=2,\n        epochs=50,\n        gpus=0\n        ),\n    resources_per_trial={\n        \"cpu\": 1,\n        \"gpu\": 0,\n    },\n    local_dir=ckpt_dir \/ \"ray_ckpt\",  # path for saving checkpoints\n    metric=\"val_loss\",\n    mode=\"min\",\n    config=config,\n    num_samples=16,\n    scheduler=scheduler_asha,\n    progress_reporter=reporter,\n    name=\"tune_titanic_asha\",\n)","caf922c8":"# Start tensorboard.\n%load_ext tensorboard\n%tensorboard --logdir \/kaggle\/working\/logs\/","0cad6063":"print(result.best_config)\nprint(result.best_checkpoint)\nresult.best_result_df","12c0922c":"trained_weights = torch.load(Path(result.best_checkpoint) \/ \"ray_ckpt\")[\"state_dict\"]","4ba97488":"run_with_tune(result.best_config, \n              data_dir,\n              trained_weights=trained_weights,\n              )","9a8a5edb":"### 1.1 Loading Data","88dc522f":"## 1  Data Preprocessing","9515dc37":"### 6.1 Train Functions","c974de9d":"### 6.2 Define optimizer, loss, device","1d217b70":"## 2 Train, Validate, Test \u2192 All in LightningModule\n\nhttps:\/\/pytorch-lightning.readthedocs.io\/en\/stable\/common\/lightning_module.html\n\nA LightningModule organizes your PyTorch code into 5 sections.\n\n1. Computations (init).\n2. Train loop (training_step)\n3. Validation loop (validation_step)\n4. Test loop (test_step)\n5. Optimizers (configure_optimizers)\n\nThe LightningModule has many convenience methods, but the core ones you need to know about are:\n\n| Name                 | Description                                          |\n| -------------------- | ---------------------------------------------------- |\n| init                 | Define computations here                             |\n| forward              | Use for inference only (separate from training_step) |\n| training_step        | the full training loop                               |\n| validation_step      | the full validation loop                             |\n| test_step            | the full test loop                                   |\n| configure_optimizers | define optimizers and LR schedulers                  |\n\nThe lightning does under the hood is:\n\n``` python\n# put model in train mode\nmodel.train()\ntorch.set_grad_enabled(True)\n\nlosses = []\nfor batch in model.train_dataloader:\n    # calls hooks like this one\n    model.on_train_batch_start(batch)\n\n    # train step\n    loss = model.training_step(batch)\n\n    # backward\n    loss.backward()\n\n    # apply and clear grads\n    optimizer.step()\n    optimizer.zero_grad()\n\n    losses.append(loss)\n\n    if validate_at_some_point:\n        # disable grads + batchnorm + dropout\n        torch.set_grad_enabled(False)\n        model.eval()\n\n        # ----------------- VAL LOOP ---------------\n        for val_batch in model.val_dataloader:\n            val_out = model.validation_step(val_batch)\n        # ----------------- VAL LOOP ---------------\n\n        # enable grads + batchnorm + dropout\n        torch.set_grad_enabled(True)\n        model.train()\n\ntraining_epoch_end(losses)\n```","1c3c541c":"### 1.4 Data Labeling\n\nIn this step, we have to convert the features: `Sex`, `Title`, and `Embarked` into **discrete form** by using `sklearn.preprocessing.LabelEncoder`.","52d8d3c2":"# Pytorch + Pytorch Lightning + Ray Tune\n\nIn this notebook, we first train, validate, and predict on Titanic problem using the default `Pytorch` framework.\n\nNext, we update and replace the boilerplate codes using the `Pytorch Lightning` framework.\n\nLast, we combine the `Pytorch Lightning` with the `Ray Tune` (a `Hyperparameter Tuning` framework) to find the best hyperparameters.","72d98270":"## 4 Wrap the Trainer and Constants into a Trainable Function\n\nWe have to wrap the `model`, `datamodule`, and `trainer` into a trainable function and put them into the tunning process `tune.run()`.\n\nI also put the **testing** code inside this function. If you pass the `trained_weights` into this function, it will load the `trained_weights` from the checkpoint into the `model`, and perform `trainer.test()` instead of `trainer.fit()`.\n\n\n> \u2757\u2757\u2757\n>\n> I found that when you integrate `lightning` with `ray tune`, you must use `model.load_state_dict()` instead of `model.load_from_checkpoint()` to really get the trained weights. \ud83d\ude25","102ed16b":"## 5 Tuning\n\nWe can decide which parameters and metrics will be displayed on the command line by creating a `CLIReporter` explicitly and pass it into `tune.run(progress_reporter=)`.\n\nThen we run the trainable function in the `tune.run()`. We wrap the trainable function in a `partial` function `tune.with_parameters()` which we can put the constant parameters into each trial.","db8d97ba":"## 7 Predict","42b7a986":"# \ud83d\ude3d Ray Tune\n\n**Concept of Ray Tune**:\n\n* Define **search space** (range of hyperparameters) and **sample methods**\n  * [Search Space](https:\/\/docs.ray.io\/en\/master\/tune\/api_docs\/search_space.html)\n* Define **schedulers** for EarlyStopping or PBT (Population-based training)\n  * You can further optimize your hyperparameters by using **search algorithms**\n* Run well-defined **trainables** (training functions in your epochs) on separate **TRIALS**\n* Communicate the **performance of each trial** back to Tune by calling **REPORT**\n* Evaluate the model with the returned **Analysis object**\n\nReference: [KEY CONCEPTS](https:\/\/docs.ray.io\/en\/master\/tune\/key-concepts.html)\n\n---\n\n**Integration of Lightning and Ray Tune**:\n\n* Define a `config` (**search space**) dict, and plug it into `LightningModules`\n* Define a `Scheduler` to stop unuseful trials early\n* Define a `TuneReportCallback` for reporting the trials results to Tune\n* Define a Trainable function with `constant parameters`, `trainer`, `model`, and `datamodule`\n* Tuning, Analyzing, Predicting \n\nReference: [RAY WITH LIGHTNING](https:\/\/docs.ray.io\/en\/master\/tune\/tutorials\/tune-pytorch-lightning.html)\n\n> \u2757\u2757\u2757\n>\n> I found that when you integrate `lightning` with `ray tune`, you must use `model.load_state_dict()` instead of `model.load_from_checkpoint()` to really get the trained weights. \ud83d\ude25","35d4f83b":"## 7 Predict\n\nWe load the `best_config` from the result obtained from `tune.run()`, and the `trained_weights` (`state_dict`) from the `best_checkpoint` dict.","130fcce5":"## 4 Pytorch Dataset Enclosing\n\nIn this step, we will get the `X` and `y` from the **dataframe's values** and put them into our custom but redundant (for learning purpose) `torch.utils.data.Dataset`.\n\nAfter that, we split the `Dataset` into training and validation datasets, load them using `DataLoader` with the contrived `collate_fn` function (for learning purpose again).","810bed9d":"## 5 Network","22496320":"## 6 Analysis","5ea84c65":"## 3 Let Trainer does everything\n\nhttps:\/\/pytorch-lightning.readthedocs.io\/en\/stable\/common\/trainer.html\n\nNow we have both `datamodule` and `lightningmodule`, we can start training, validating, visualizing, and testing. We only need to create an instance of `Trainer`, pass `lightningmodule`, `datamodule`, some parameters, and useful callbacks, then we're ready to train.\n\nHere are the purposes of parameters which I define below:\n\n- `default_root_dir` will be the path for **tensorboard logs** and **checkpoints**.\n- `weights_save_path` will specifiy the path for **checkpoints**, and the `default_root_dir` remains the path for **tensorboard logs**.\n- `fast_dev_run` will run 1 epoch for debugging; you can also set `n` epochs instead of `bool`.\n\n> * [default_root_dir](https:\/\/pytorch-lightning.readthedocs.io\/en\/stable\/common\/trainer.html#default-root-dir)\n> * [weights_save_path](https:\/\/pytorch-lightning.readthedocs.io\/en\/stable\/common\/trainer.html#weights-save-path)\n\nWe can use many **callbacks** in `Trainer`, such as `ModelCheckpoint`, `EarlyStopping`, etc. Please check the documentation of checkpoint below.\n\n> * [Model Checkpoint](https:\/\/pytorch-lightning.readthedocs.io\/en\/latest\/api\/pytorch_lightning.callbacks.model_checkpoint.html#model-checkpointing)\n> * [Callbacks](https:\/\/pytorch-lightning.readthedocs.io\/en\/stable\/extensions\/callbacks.html)\n\nFor saving checkpoints, we use `self.save_hyperparameters()` in lightningmodule's `__init__()` function to save hyperparameters in `lightningmodule.hparams`. \n\nFor loading checkpoints, `model.load_from_checkpoint(checkpoint.best_model_path)` can help us load the best parameters and hyperparameters into the model.\n\n> * [HYPERPARAMETERS](https:\/\/pytorch-lightning.readthedocs.io\/en\/stable\/common\/hyperparameters.html)\n> * [Saving and loading weights](https:\/\/pytorch-lightning.readthedocs.io\/en\/stable\/common\/weights_loading.html)","8305851d":"## 6 Train","99052da4":"## 1 Data Processing \u2192 LightningDataModules\n\nhttps:\/\/pytorch-lightning.readthedocs.io\/en\/stable\/extensions\/datamodules.html\n\nA datamodule encapsulates the five steps involved in data processing in PyTorch:\n\n1. Download \/ tokenize \/ process.\n2. Clean and (maybe) save to disk.\n3. Load inside Dataset.\n4. Apply transforms (rotate, tokenize, etc\u2026).\n5. Wrap inside a DataLoader.","9dd71c9e":"## 2 Define Schedulers\n\n`Asynchronous Hyperband` scheduler can help us decide at each iteration which trials are likely to perform badly, and stops these trials.\n\n- For more details: [Trial Schedulers](https:\/\/docs.ray.io\/en\/master\/tune\/api_docs\/schedulers.html)\n\nAnother popular method for hyperparameter tuning, called `Population Based Training (PBT)`, instead perturbs hyperparameters during the training run.\n\n- [Guide to Population Based Training (PBT)](https:\/\/docs.ray.io\/en\/master\/tune\/tutorials\/tune-advanced-tutorial.html)","d32b44ac":"# \ud83d\ude3a Basic Pytorch\n\n","bcac7aa4":"## 2  Lazypredict Baseline","c63dbae1":"# \ud83d\ude38 Pytorch Lightning\n\n* [PYTORCH LIGHTNING DOCUMENTATION](https:\/\/pytorch-lightning.readthedocs.io\/en\/stable\/)\n* [LIGHTNING IN 2 STEPS](https:\/\/pytorch-lightning.readthedocs.io\/en\/stable\/starter\/new-project.html)\n* [LIGHTNING STYLE GUIDE](https:\/\/pytorch-lightning.readthedocs.io\/en\/stable\/starter\/style_guide.html)\n\nWe're going to use [Pytorch Lightning](https:\/\/github.com\/PyTorchLightning\/pytorch-lightning) as our new framework to minimize the boilerplate code of our original Pytorch code above.\n","cc39eb3b":"## 3  Hyperparameters","d7e565b7":"## 1 Define Search space and config \n\nThe first thing is to reconstruct the `LightningDataModule` and `LightningModule`; We have to re-define the arguments of two classes and feed the hyperparameters configs into them. \n\nWe can learn how to define search space from [Search Space API](https:\/\/docs.ray.io\/en\/master\/tune\/api_docs\/search_space.html)\n\nFor example:\n\nnum_samples=10 repeats the 3x3 grid search 10 times, for a total of 90 trials, each with randomly sampled values of alpha and beta.\n\n``` python\ntune.run(\n     my_trainable,\n     name=\"my_trainable\",\n     # num_samples will repeat the entire config 10 times.\n     num_samples=10\n     config={\n         # ``sample_from`` creates a generator to call the lambda once per trial.\n         \"alpha\": tune.sample_from(lambda spec: np.random.uniform(100)),\n         # ``sample_from`` also supports \"conditional search spaces\"\n         \"beta\": tune.sample_from(lambda spec: spec.config.alpha * np.random.normal()),\n         \"nn_layers\": [\n             # tune.grid_search will make it so that all values are evaluated.\n             tune.grid_search([16, 64, 256]),\n             tune.grid_search([16, 64, 256]),\n         ],\n     },\n )\n```","5e6d0a1e":"## 3 Define Report Callbacks\n\n`TuneReportCallback` or `TuneReportCheckpointCallback` is inherited from `pytorch_lightning.Callback`, it will report metrics from `LightningModule` back to Tune for finding the best hyperparameters.","48725ff9":"First, we use library `sweetviz` to do a quick data exploration on the titanic dataset.\n\n1. Values in `age` are missing 177 (20%) in `train_data` and 86 (21%) in `test_data`. \n2. Values in `cabin` are missing 687 (77%) and 327 (78%) in both datasets.\n3. The features that have the most impact on `Survived` are `Sex` and `Fare`.\n","33ecaa88":"### 1.2 Data Preview","71972034":"### 6.3 Train with validation","15c57f7e":"### 1.3 Data Completing, Data Cleaning, Data Creating\n\n#### Completing\n\nWe fill the missing values of `age`, `embarked`, `fare` in both `train_data` and `test_data` with **median** and **mode** respectively.\n\n#### Cleaning\n\nThen we drop the `id`, `cabin`, `ticket` columns from both dataset because:\n\n1. `PassengerId` has no meaning.\n2. `Cabin` has too many missing values.\n3. `Ticket` has no meaning from its distribution. (maybe)\n\n#### Creating (Feature Engineering)\n\nWe extract the information from `SibSp` and `Parch`, then create two new features: `FamilySize` and `IsAlone`.\n\nFrom the `Name` feature, we can get the `Title` of each person. After the extraction, we can drop the `Name` column.\n\nFinally, we use `qcut()` and `cut()` from pandas, to convert the `Age` and `Fare` into discrete intervals, then we drop the `Age` and `Fare`.\n\n> - [pandas\u7684cut&qcut\u51fd\u6578](https:\/\/medium.com\/@morris_tai\/pandas%E7%9A%84cut-qcut%E5%87%BD%E6%95%B8-93c244e34cfc)\n> - [pandas\u7684cut\uff0cqcut\u51fd\u6570\u7684\u4f7f\u7528\u548c\u533a\u522b](https:\/\/zhuanlan.zhihu.com\/p\/68194655)\n"}}