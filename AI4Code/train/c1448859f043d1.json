{"cell_type":{"6613b45e":"code","866bce01":"code","4448198d":"code","10fb6602":"code","5e4ae3a6":"code","57efafe2":"code","eb6cc6f0":"code","ffc09017":"code","98ed236b":"code","85218500":"code","e9e25561":"code","eacee4bc":"code","52eea9b0":"code","16adb398":"code","65726543":"code","055fe0c0":"code","3bf60dba":"code","00df28cd":"markdown","5e16d369":"markdown"},"source":{"6613b45e":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","866bce01":"import os\nimport glob\nimport h5py\nimport shutil\nimport imgaug as aug\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport matplotlib.image as mimg\nimport imgaug.augmenters as iaa\nfrom os import listdir, makedirs, getcwd, remove\nfrom os.path import isfile, join, abspath, exists, isdir, expanduser\nfrom PIL import Image\nfrom pathlib import Path\nfrom skimage.io import imread\nfrom skimage.transform import resize\nfrom keras.models import Sequential, Model\nfrom keras.applications.vgg16 import VGG16, preprocess_input\nfrom keras.preprocessing.image import ImageDataGenerator,load_img, img_to_array\nfrom keras.models import Sequential\nfrom keras.layers import Conv2D, MaxPooling2D, Dense, Dropout, Input, Flatten, SeparableConv2D\nfrom keras.layers import GlobalMaxPooling2D\nfrom keras.layers.normalization import BatchNormalization\nfrom keras.layers.merge import Concatenate\nfrom keras.models import Model\nfrom keras.optimizers import Adam, SGD, RMSprop\nfrom keras.callbacks import ModelCheckpoint, Callback, EarlyStopping\nfrom keras.utils import to_categorical\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom mlxtend.plotting import plot_confusion_matrix\nfrom sklearn.metrics import confusion_matrix\nimport cv2\nfrom keras.preprocessing.image import array_to_img, img_to_array, load_img\nfrom keras import backend as K\nfrom sklearn.preprocessing import LabelEncoder\ncolor = sns.color_palette()\n%matplotlib inline\n","4448198d":"df_dir = '..\/input\/chest-xray-pneumonia\/chest_xray\/'","10fb6602":"#Creating Function for creating Dataframes\ndef create_data_frame(z = None):\n    df = pd.DataFrame(columns=['Class','Location'])\n    choices = [z]\n    for choice in choices:\n        for folder in os.listdir(df_dir+choice+'\/'):\n            for Class in os.listdir(df_dir+choice+'\/'+folder+'\/'):\n                df = df.append({'Class': folder,'Location':df_dir+choice+'\/'+folder+'\/'+Class},ignore_index=True)\n                df = df.sample(frac = 1.).reset_index(drop=True)\n    return df","5e4ae3a6":"#create train, test validation data frames \ndf_train = create_data_frame('train')\ndf_test = create_data_frame('test')\ndf_val = create_data_frame('val')\n\nle = LabelEncoder()\ndf_train['Class'] = le.fit_transform(df_train['Class'])\ndf_test['Class'] = le.fit_transform(df_test['Class'])\ndf_val['Class'] = le.fit_transform(df_val['Class'])","57efafe2":"#categories as \n#le.inverse_transform([0, 1])","eb6cc6f0":"# \npneumonia_samples = (df_train[df_train['Class']== 1]['Location'].iloc[:5]).tolist()\nnormal_samples = (df_train[df_train['Class']==0]['Location'].iloc[:5]).tolist()\nsamples = pneumonia_samples + normal_samples\n\nf, ax = plt.subplots(2,5, figsize=(30,10))\nfor i in range(10):\n    img = imread(samples[i])\n    ax[i\/\/5, i%5].imshow(img, cmap='gray')\n    if i<5:\n        ax[i\/\/5, i%5].set_title(\"Pneumonia\")\n    else:\n        ax[i\/\/5, i%5].set_title(\"Normal\")\n    ax[i\/\/5, i%5].axis('off')\n    ax[i\/\/5, i%5].set_aspect('auto')\nplt.show()","ffc09017":"#barchart depicting distributtion of data \nfig = plt.figure(figsize=(16,5))\n\nplt.subplot(1, 3, 1)\nsns.countplot(df_train['Class'])\nplt.title('Train Data')\n\nplt.subplot(1, 3, 2)\nsns.countplot(df_train['Class'])\nplt.title('Test Data')\n\nplt.subplot(1, 3, 3)\nsns.countplot(df_train['Class'])\nplt.title('Validation Data')\n\nplt.show()","98ed236b":"df_train.head()","85218500":"#Process and composing  dataset \ndef process_data(img_path):\n    img = cv2.imread(img_path)\n    img = cv2.resize(img, (64, 64))\n    img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n    img = img\/255.0\n    img = np.reshape(img, (64,64,1))\n    \n    return img\n\ndef compose_dataset(df):\n    data = []\n    labels = []\n\n    for label, img_path in df.values:\n        data.append(process_data(img_path))\n        labels.append(label)\n        \n    return np.array(data), np.array(labels)","e9e25561":"#Getting processed data by passing through the required function \nX_train, y_train = compose_dataset(df_train)\nX_test, y_test = compose_dataset(df_test)\nX_val, y_val = compose_dataset(df_val)\n\nprint('The shape of Training after data proessing is: ', X_train.shape, y_train.shape)\nprint('The shape of Testing after data proessing is: ', X_test.shape, y_test.shape)\nprint('The shape of Validation after data proessing is: ', X_val.shape, y_val.shape)","eacee4bc":"#Image Augmentation\ntrain_datagen = ImageDataGenerator(\n    rotation_range=10,\n    zoom_range = 0.1,\n    width_shift_range=0.1,\n    height_shift_range=0.1,\n    horizontal_flip=True,\n    vertical_flip=True\n)\n#val_datagen = ImageDataGenerator()\n\n\n# fit generator on our train features\ntrain_datagen.fit(X_train)\n#val_datagen.fit(X_test)\n\n#Label Encoding \n##from sklearn.preprocessing import LabelBinarizer\n\n#label_binrizer = LabelBinarizer()\n#y_train = label_binrizer.fit_transform(y_train)\n#y_test = label_binrizer.fit_transform(y_test)\n#print('First image label after encoding: ',y_train)","52eea9b0":"model = Sequential()\n\nmodel.add(Conv2D(filters = 8, kernel_size = (3,3) , activation='relu',padding='same'\n                 ,input_shape=(64,64,1)))\nmodel.add(Conv2D(filters = 8, kernel_size = (3,3) , activation='relu',padding='same'))\nmodel.add(MaxPooling2D(pool_size =(2,2)))\n\nmodel.add(Conv2D(filters = 16, kernel_size = (3,3) , activation='relu',padding='same'))\nmodel.add(Conv2D(filters = 16, kernel_size = (3,3) , activation='relu',padding='same'))\nmodel.add(MaxPooling2D(pool_size =(2,2)))\n\nmodel.add(Conv2D(filters = 64, kernel_size = (3,3) , activation='relu',padding='same'))\nmodel.add(Conv2D(filters = 64, kernel_size = (3,3) , activation='relu',padding='same'))\nmodel.add(MaxPooling2D(pool_size =(2,2)))\n\nmodel.add(Conv2D(filters = 128, kernel_size = (3,3) , activation='relu',padding='same'))\nmodel.add(Conv2D(filters = 128, kernel_size = (3,3) , activation='relu',padding='same'))\nmodel.add(MaxPooling2D(pool_size =(2,2)))\n\nmodel.add(Flatten())\n\nmodel.add(Dense(units = 128,kernel_initializer=\"he_normal\", activation = 'relu'))\nmodel.add(Dropout(0.2))\n\nmodel.add(Dense(units = 512,kernel_initializer=\"he_normal\", activation = 'relu'))\nmodel.add(Dropout(0.2))\n\nmodel.add(Dense(2, activation = 'softmax'))\n\nmodel.summary()","16adb398":"y_train = to_categorical(y_train)\ny_test = to_categorical(y_test)\ny_val = to_categorical(y_val)\n","65726543":"#Compiling and fitting the model.\noptimizer = Adam(lr=0.0001, decay=1e-5)\nmodel.compile(loss='categorical_crossentropy', optimizer=optimizer, \n              metrics=['accuracy'])\n\ncallback = EarlyStopping(monitor='loss', patience=6)\n\n","055fe0c0":"# Train the Model\nhistory = model.fit(train_datagen.flow(X_train,y_train, batch_size=4), \n                    validation_data=(X_test, y_test), epochs = 100,\n                    callbacks=[callback], class_weight={0:6.0, 1:0.5})","3bf60dba":"fig , ax = plt.subplots(1,2)\ntrain_acc = history.history['accuracy']\ntrain_loss = history.history['loss']\nfig.set_size_inches(12,4)\n\nax[0].plot(history.history['accuracy'])\nax[0].plot(history.history['val_accuracy'])\nax[0].set_title('Training Accuracy vs Validation Accuracy')\nax[0].set_ylabel('Accuracy')\nax[0].set_xlabel('Epoch')\nax[0].legend(['Train', 'Validation'], loc='lower right')\n\nax[1].plot(history.history['loss'])\nax[1].plot(history.history['val_loss'])\nax[1].set_title('Training Loss vs Validation Loss')\nax[1].set_ylabel('Loss')\nax[1].set_xlabel('Epoch')\nax[1].legend(['Train', 'Validation'], loc='upper right')\n\nplt.show()","00df28cd":"# Import Libraries","5e16d369":"# Modeling "}}