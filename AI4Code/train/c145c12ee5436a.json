{"cell_type":{"a2816aff":"code","424b3902":"code","796a98ce":"code","30c14281":"code","2262c4f8":"code","6725b222":"code","fa4505e2":"code","d34f7309":"code","edaccd39":"code","f71ccdbe":"code","850f4fa5":"code","32b187f8":"code","bb91d394":"code","7f649cce":"code","185e6ce2":"code","dbbdeef4":"code","8acd2496":"code","d37e754c":"code","78fbed70":"code","d5ae54d0":"code","782b9aef":"code","6d28b33d":"code","f383fcc6":"code","aff9a027":"code","b744df66":"markdown","a77332b3":"markdown","e4c48da4":"markdown","c3f613a8":"markdown","10811561":"markdown","d5d73506":"markdown","ebb422d3":"markdown","2e11fe7a":"markdown","7fe43a5b":"markdown","288b6fed":"markdown","3b7f996c":"markdown","9028011f":"markdown","4e437b5f":"markdown","2a9f7c27":"markdown","aebd6f46":"markdown","9fe1da20":"markdown","1b73b769":"markdown","3b47087a":"markdown","fa03d790":"markdown","6bed633e":"markdown","cf92bb63":"markdown","80286e80":"markdown","761e1ffb":"markdown","07c8af47":"markdown","7f8f09ab":"markdown","b54a0634":"markdown","bd22e338":"markdown","05d93758":"markdown","a7578bd6":"markdown","6e7429ee":"markdown","2162c751":"markdown","431a1cdb":"markdown","3d33efc1":"markdown","ab8eb54a":"markdown","207acae2":"markdown","07b8be84":"markdown","643fe086":"markdown","e5508c64":"markdown","a5a98f58":"markdown","6868afb2":"markdown"},"source":{"a2816aff":"import numpy as np\ndef sigmoid(z):\n return 1 \/ (1 + np.exp(-z))\nprint(\"Sigmoid of 4 is:\",sigmoid (4))","424b3902":"#Non-zero centric","796a98ce":"print(\"Sigmoid of positive number(5) is:\",sigmoid(5))","30c14281":"print(\"Sigmoid of negative number(-5) is:\",sigmoid(-5))","2262c4f8":"print(\"Difference between Derivative of Sigmoid (5) and (-5) is:\", sigmoid(5)*(1- sigmoid(5))-sigmoid(-5)*(1- sigmoid(-5)))","6725b222":"#vanishing gradient","fa4505e2":"print(\"Difference between sigmoid of 14 and 15:\",sigmoid(15)-sigmoid(14))","d34f7309":"def tanh(z):\n return np.tanh(z)","edaccd39":"print(\"tanh of 4 is:\",tanh(4))","f71ccdbe":"#zero-centric ","850f4fa5":"print(\"tanh of positive number(15) is:\",tanh(15))","32b187f8":"print(\"tanh of positive number(-15) is:\",tanh(-15))","bb91d394":"#vanishing gradient\n\nprint(\"Difference between tanh of 14 and 15:\",np.tanh(15)-np.tanh(14))","7f649cce":"def relu(z):\n  return max(0, z)","185e6ce2":"z= 10\nprint(\"ReLU of \"+str(z)+\" is :\",relu(z))","dbbdeef4":"# Dead neuron\nz= -0.4\nprint(\"ReLU of \"+str(z)+\" is :\",relu(z)) \nz= -50\nprint(\"ReLU of \"+str(z)+\" is :\",z * (z > 0))","8acd2496":"def leakyrelu(z):\n  return np.maximum(0.01 * z, z)","d37e754c":"z= 10\nprint(\"ReLU of \"+str(z)+\" is :\",leakyrelu(z)) #positive value","78fbed70":"z= -1\nprint(\"ReLU of \"+str(z)+\" is :\",leakyrelu(z)) #negative number","d5ae54d0":"def parmetricrelu(z,\u03b1):\n  return np.maximum(\u03b1 * z, z)","782b9aef":"\nprint(\"ReLU of \"+str(z)+\" is :\",parmetricrelu(10,0.5)) #positive value\n\nprint(\"ReLU of \"+str(z)+\" is :\",parmetricrelu(-1,0.5)) #negative number\n","6d28b33d":"def erelu(z,alpha):\n    return z if z >= 0 else alpha*(np.exp(z) -1)","f383fcc6":"print(\"Exponential ReLu for 10 is :\",erelu(10,3))\nprint(\"Exponential ReLu for -10 is :\",erelu(-10,3))","aff9a027":"def softmax(x):\n    ex = np.exp(x)\n    sum_ex = np.sum( np.exp(x))\n    return ex\/sum_ex\n\n\nprint (\"softmax of 1,2 and 3 is:\",softmax([1,2,3]))","b744df66":"The only differnce from ReLU is replacing the negative values with exponent times the value instead of 0.\nCondition :\n\n       (e^x-1)   X < 0\n       X  otherwise","a77332b3":"![acti.PNG](attachment:acti.PNG)","e4c48da4":"Referneces:\n\n- https:\/\/towardsdatascience.com\/activation-functions-and-its-types-which-is-better-a9a5310cc8f\n\n- https:\/\/medium.com\/the-theory-of-everything\/understanding-activation-functions-in-neural-networks-9491262884e0\n","c3f613a8":"THANK YOU FOR READING. UPVOTE IF YOU FIND IT USEFUL\n****","10811561":"Implementaion of tanH","d5d73506":"Implementaion of Parametric ReLU\n\n","ebb422d3":"----> The difference in nearby predicted values is very less ","2e11fe7a":"For derivative of sigmoid refer \nhttps:\/\/math.stackexchange.com\/questions\/78575\/derivative-of-sigmoid-function-sigma-x-frac11e-x","7fe43a5b":"It is a form of logistic regression that normalizes an input value into a vector of values that follows a probability distribution whose total sums up to 1\nAdvantange:\n    + Multi-dimensional classification\n    + Generally used as output neuron","288b6fed":"Swish is a new, self-gated activation function discovered by researchers at Google to achieve better performance compared to ReLU \n\nA notable thing is that the \u03b2 scaling factor is introduced in Swish.\n\nIf \u03b2=1, then activation function is called the Sigmoid-weighted Linear Unit (SiL)","3b7f996c":"----> The difference in nearby predicted values is very less ","9028011f":"### TanH","4e437b5f":"Ranges in between -1 and 1\n\nIt is a shifted version of sigmoid from [0,1] to [-1,1]\n\nAdvantages:\n\n    + Zero-centric- it accommodates large positive and negative values because if calculates local (or global) minimum \n    quickly as derivatives of the tanh are larger than the derivatives of the sigmoid. It can minimize the cost function           faster.\n    \n    \nDisadvantages:\n\n    - Vanishing gradient problem\n\n","2a9f7c27":"![leak.PNG](attachment:leak.PNG)","aebd6f46":"#### Basic features of any activation function are:\n\n1)  It must be differential- In the process of backpropgation.\n\n2)  Ideally Non-linear - They allow the model to create complex mappings between the network\u2019s inputs and outputs, which are essential for learning and modeling complex and multidimensional data.\n\n\n\nThere are several activation functions, can be compared by computational efficiency because they are calculated across thousands and millions of data or by taking gradient.\n","9fe1da20":"Ranges in between 0 and 1. \n\nAdvantages:\n\n    + Simple and smooth curve\n    + Clear predictions\n    + Can be used in any layer including the output layer\n    \n    \nDisadvantages:\n\n    - Non-Zero centric- for large negative and positive values the output is positive and in opposite directions but \n    between [0,1] which makes it difficult to calculate gradient for such small values. \n    - Vanishing Gradient- the change in predicted values for large positive numbers is infinitesimal.\n    - The calculation is computationally complex for large networks.\n    \n\n","1b73b769":"### Swish","3b47087a":"Implementaion of ReLU","fa03d790":"Implementaion of Leaky ReLU","6bed633e":"# Activation functions for Neural networks\n\nIn simple words, the activation function is an equation that is used to convert an input of a node in a Neural network\/Layer to an output. It is a logic \u201cgate\u201d in between the input feeding the current neuron and its output going to the next layer(if stacked).\n\nThe main objective of the activation function is to introduce non-linearilty to neuron and determine whether to activate a neuron in a layer or not. Such that, during model building, only the activated neurons are considered.  \n\n\n\n","cf92bb63":"![sig.PNG](attachment:sig.PNG)","80286e80":"### ReLU(Rectified Linear Unit)","761e1ffb":"#### Parametric ReLU","07c8af47":"### Varients of ReLU\n\n    Leaky ReLU \n    Parametric ReLU  \n    Exponential ReLU","7f8f09ab":"----> all the neurons valued from [0,-inf) are deactivated which may harm the model during backpropagation","b54a0634":"## Widely used activation functions are as follows:\n1) Sigmoid \n\n2) Tanh(Hyperbolic tangent)\n\n3) ReLU(Rectified Linear Unit) and its varients \n\n4) Softmax \n\n5) Swish\n\n\n","bd22e338":"----> The output is far from each other hence eas to calulate the gradient","05d93758":"![swish.PNG](attachment:swish.PNG)","a7578bd6":"![leak.PNG](attachment:leak.PNG)","6e7429ee":"Ranges in between 0 to max(x)\n\nCondition :\n\n           0   X < 0\n           X  otherwise\nAdvantages:\n    \n    + Simple and computationally efficent \n    + No vanishing gradient problem- as the input remains the same\n    + Non-linear \n    + Sparsity- increases speed of the model by removing unwanted features(Most of the times)\n    \nDisadvantages:\n    \n    - Dead Neurons- the gradient(slope) in the negative region is 0 deactivates the neurons which cannot be changed during            backpropagation and optimization.\n    - Cannot be used as the activation function for final layer.\n           \n\n\n","2162c751":"#### Leaky ReLU \n\nThe only differnce from ReLU is replacing the negative values with constant(=0.01) times the value instead of 0. \n\nCondition :\n\n           0.01   X < 0\n           X  otherwise\n\n\nAdvantange:\n    \n    + Prevents the dead neurons problem- due to the replacement of negative values the neurons do not deactive and block \n     from active backpropagation.\n    \nDisadvantage:\n      \n    -  Output not constant ## check\n","431a1cdb":"### Softmax ","3d33efc1":"![tan.PNG](attachment:tan.PNG)","ab8eb54a":"Similar to Leaky ReLU but replaces the negative values with variable paramater(\u03b1) times the value instead of 0\n\nCondition :\n\n       \u03b1   X < 0\n       X  otherwise\n\nAdvantange:\n    \n    + Allows the negative slope to be learned\u2014 unlike leaky ReLU, this function provides the slope of the negative part \n    which helps in finding the most appropriate value of \u03b1.","207acae2":"### Sigmoid ","07b8be84":"![relu.PNG](attachment:relu.PNG)","643fe086":"#### Exponential ReLU","e5508c64":"![erelu.PNG](attachment:erelu.PNG)","a5a98f58":"Implementaion of sigmoid","6868afb2":"----> The derivatives of the outputs when compared for gradient decent will be very less"}}