{"cell_type":{"8ea49f58":"code","ac5f848f":"code","c480e038":"code","0b8eea3a":"code","08dc6b99":"code","fd044f82":"code","1f17a568":"code","e06d80c7":"code","24764531":"code","3f1d61c5":"code","c7a2e464":"code","053896ce":"code","4258342d":"code","02674b08":"code","16a83f5e":"code","22c5cf62":"code","ab571116":"code","755ca49d":"code","aa5c5c2b":"markdown","dda2c2be":"markdown","64f1156c":"markdown","23317c0c":"markdown","0b44e235":"markdown","f5e32ddc":"markdown","6c7f98cc":"markdown","44502b59":"markdown","22b3fb0f":"markdown"},"source":{"8ea49f58":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","ac5f848f":"# Load the test data\ntrain_data = pd.read_csv(\"..\/input\/train.csv\")\ntest_data = pd.read_csv(\"..\/input\/test.csv\")","c480e038":"# Extract the target variable from the training data\ntrain_y = train_data.SalePrice\ntrain_data.drop('SalePrice', axis=1, inplace=True)","0b8eea3a":"# The final competition score is based on the MSE between the log of the test predictions and the log of the true SalePrice.\n# With that in mind, always train to fit logy.\ntrain_logy = np.log(train_y)\n# Predictions will need to be of y, however, so for the final test submission, take the exponent of its output.","08dc6b99":"# Separate the Id column from the predictive features\ntrain_X = train_data.drop('Id', axis=1)\ntest_X = test_data.drop('Id', axis=1)","fd044f82":"train_X_numeric = train_X.select_dtypes(include=[np.number]).drop('MSSubClass', axis=1)\ntrain_X_categorical = train_X.drop(train_X_numeric.columns, axis=1)\n\nnum_cols = train_X_numeric.columns\ncat_cols = train_X_categorical.columns","1f17a568":"from sklearn.pipeline import Pipeline\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.preprocessing import RobustScaler, OneHotEncoder\nfrom sklearn.impute import SimpleImputer\n\nnum_pipeline = Pipeline([\n    ('num_imputer', SimpleImputer(strategy='median')),\n    ('num_scaler', RobustScaler())\n])\n\ncat_pipeline = Pipeline([\n    ('cat_nan_filler', SimpleImputer(strategy='constant', fill_value='not_in_data')),\n    ('cat_onehot', OneHotEncoder(handle_unknown='ignore'))\n])\n\n# For XGBoostRegressor\nminimal_preprocessor_pipeline = ColumnTransformer([\n    ('num_pipeline', 'passthrough', num_cols),\n    ('cat_pipeline', cat_pipeline, cat_cols)\n])\n\n# For all other models\npreprocessor_pipeline = ColumnTransformer([\n    ('num_pipeline', num_pipeline, num_cols),\n    ('cat_pipeline', cat_pipeline, cat_cols)\n])","e06d80c7":"# Best models from model examination\nfrom sklearn.ensemble import ExtraTreesRegressor\nfrom sklearn.linear_model import ElasticNet\n\nbest_models = {'ElasticNet': ElasticNet(alpha=0.001, l1_ratio=0.5),\n               'ExtraTrees': ExtraTreesRegressor(n_estimators=100, criterion='mse', min_samples_leaf=2)}","24764531":"# Fit\nmodels = []\nmodel_names = []\n\nfor model_name in best_models:\n    model = Pipeline([\n        ('preprocessor', preprocessor_pipeline),\n        ('actual_model', best_models[model_name])\n    ])\n    model.fit(train_X, train_logy)\n    models.append(model)\n    model_names.append(model_name)","3f1d61c5":"from xgboost import XGBRegressor\n\nxgb_regressor = XGBRegressor(learning_rate=0.1, max_depth=2, n_estimators=650)","c7a2e464":"model = Pipeline([\n    ('preprocessor', minimal_preprocessor_pipeline),\n    ('actual_model', xgb_regressor)\n])\nmodel.fit(train_X, train_logy)\nmodels.append(model)\nmodel_names.append('XGBoost')","053896ce":"train_predictions_allmodels = []\ntest_predictions_allmodels = []\n\nfor model in models:\n    train_predictions = model.predict(train_X)\n    train_predictions_allmodels.append(train_predictions)\n    test_predictions = model.predict(test_X)\n    test_predictions_allmodels.append(test_predictions)","4258342d":"model_names.append('average')\n# This line must be run only AFTER all the single model predictions have been stored\naveraged_train_predictions = np.stack(train_predictions_allmodels).mean(axis=0)\ntrain_predictions_allmodels.append(averaged_train_predictions)\naveraged_test_predictions = np.stack(test_predictions_allmodels).mean(axis=0)\ntest_predictions_allmodels.append(averaged_test_predictions)","02674b08":"from sklearn.linear_model import LinearRegression\nfrom mlxtend.regressor import StackingCVRegressor\n\nregressors_for_stacking = []\nfor model_name in best_models:\n    model = best_models[model_name]\n    regressors_for_stacking.append(model)\nregressors_for_stacking.append(xgb_regressor)\n    \ntrain_X_for_stacking = preprocessor_pipeline.fit_transform(train_X)\n# Not the ideal preprocessor pipeline for XGBoost,\n#  but using different preprocessors with StackingCVRegressor is a problem I have yet to solve\n    \nmodel_names.append('stacked')\nstack_regressor = StackingCVRegressor(regressors=regressors_for_stacking, meta_regressor=LinearRegression())\nstack_regressor.fit(train_X_for_stacking, train_logy)","16a83f5e":"stack_train_predictions = stack_regressor.predict(train_X_for_stacking)\ntrain_predictions_allmodels.append(stack_train_predictions)\nstack_test_predictions = stack_regressor.predict(preprocessor_pipeline.transform(test_X))\ntest_predictions_allmodels.append(stack_test_predictions)","22c5cf62":"import matplotlib.pyplot as plt\n\ndef plot_train_predictions(train_targets, train_predictions, model_name):\n    plt.scatter(train_targets, train_targets, label='SalePrice')\n    plt.scatter(train_targets, train_predictions, label=model_name)\n    plt.xlabel('Actual log(house price)')\n    plt.ylabel('Predicted log(house price)')\n    plt.legend()\n    plt.show()","ab571116":"for idx in range(len(model_names)):\n    plot_train_predictions(train_logy, train_predictions_allmodels[idx], model_names[idx])","755ca49d":"## Save predictions in format used for competition scoring\nfor idx in range(len(model_names)):\n    log_model_predictions = test_predictions_allmodels[idx]\n    model_predictions = np.exp(log_model_predictions)\n    output = pd.DataFrame({'Id': test_data.Id,\n                           'SalePrice': model_predictions})\n    output.to_csv('submission_{model_name}.csv'.format(model_name=model_names[idx]), index=False)","aa5c5c2b":"## Average model predictions","dda2c2be":"Note: Since the ExtraTrees regressor has a minimum of two points per leaf, it can trivially fit the training set very well. In cross-validation, it performs about as well as the linear model on average, and XGBoost beats both models.","64f1156c":"## Save predictions","23317c0c":"## Single model predictions","0b44e235":"After filling in the code above:\n1. Click the **Commit and Run** button. \n2. After your code has finished running, click the small double brackets **<<** in the upper left of your screen.  This brings you into view mode of the same page. You will need to scroll down to get back to these instructions.\n3. Go to the output tab at top of your screen. Select the button to submit your file to the competition.  \n4. If you want to keep working to improve your model, select the edit button. Then you can change your model and repeat the process.","f5e32ddc":"## Load and Preprocess the Data","6c7f98cc":"## Fit best models and make predictions","44502b59":"## Linear stacked model","22b3fb0f":"## XGBoost model\n\nRequires its own distinct data preprocessing"}}