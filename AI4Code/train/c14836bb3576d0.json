{"cell_type":{"aa714bd9":"code","edc7101f":"code","86f134c6":"code","3619faf0":"code","a40a6fe4":"code","55ec147c":"code","8f130cd1":"code","0731de40":"code","70735a6e":"code","f4451e97":"code","e9ed00be":"code","3bd6b7c7":"markdown","3986895a":"markdown","f292331a":"markdown","692aa929":"markdown","191052da":"markdown","e78a7a91":"markdown","d21a44f4":"markdown","6fc18d77":"markdown","b6969737":"markdown"},"source":{"aa714bd9":"import warnings\nwarnings.filterwarnings(\"ignore\", category=DeprecationWarning)\nwarnings.filterwarnings(\"ignore\", category=FutureWarning)","edc7101f":"import numpy as np\nimport gensim\nimport nltk\nimport pandas as pd\nimport pyLDAvis.gensim_models\nimport re\nimport seaborn\n\nfrom gensim.utils import simple_preprocess\nimport gensim.corpora as corpora\nimport matplotlib.pyplot as plt\nfrom wordcloud import WordCloud\nfrom pprint import pprint\n\n%matplotlib inline","86f134c6":"df = pd.read_csv('\/kaggle\/input\/cricket-on-reddit\/reddit_cricket.csv')","3619faf0":"df.head()","a40a6fe4":"df['body'] = df['body'].astype(str)","55ec147c":"# Load the regular expression library\n# Remove punctuation\ndf['body_processed'] = df['body'].map(lambda x: re.sub('[,\\.!?]', '', x))\n# Convert the titles to lowercase\ndf['body_processed'] = df['body_processed'].map(lambda x: x.lower())\n# Print out the first rows of papers\ndf['body_processed'].head()","8f130cd1":"long_string = ','.join(list(df['body_processed'].values))\n# Create a WordCloud object\nwordcloud = WordCloud(background_color=\"white\", max_words=5000, contour_width=3, contour_color='steelblue')\n# Generate a; word cloud\nwordcloud.generate(long_string)\n# Visualize the word cloud\nplt.figure( figsize=(15,10) )\nplt.imshow(wordcloud)\nplt.show()","0731de40":"\nnltk.download('stopwords')\nen_stop = set(nltk.corpus.stopwords.words('english'))\n\nen_stop.update(['nan','https', 'http','wwwredditcom', 'match_thread_', 'comments', 'threads', 'wwwredit'])\n\ndef sent_to_words(sentences):\n    for sentence in sentences:\n        # deacc=True removes punctuations\n        yield(gensim.utils.simple_preprocess(str(sentence), deacc=True))\ndef remove_stopwords(texts):\n    return [[word for word in simple_preprocess(str(doc)) \n             if word not in en_stop] for doc in texts]\n\ndef filter_size(texts):\n    return [[word for word in simple_preprocess(str(doc)) \n             if len(word) > 4 ] for doc in texts]\n\ndata = df['body_processed'].values.tolist()\ndata_words = list(sent_to_words(data))\n# remove stop words\ndata_words = remove_stopwords(data_words)\ndata_words = filter_size(data_words)\nprint(data_words[:1][0][:30])","70735a6e":"# Create Dictionary\nid2word = corpora.Dictionary(data_words)\n# Create Corpus\ntexts = data_words\n# Term Document Frequency\ncorpus = [id2word.doc2bow(text) for text in texts]\n# View\nprint(corpus[:1][0][:30])","f4451e97":"# number of topics\nnum_topics = 10\n# Build LDA model\nlda_model = gensim.models.LdaMulticore(corpus=corpus,\n                                       id2word=id2word,\n                                       num_topics=num_topics)\n# Print the Keyword in the 10 topics\npprint(lda_model.print_topics())\ndoc_lda = lda_model[corpus]","e9ed00be":"lda_display = pyLDAvis.gensim_models.prepare(lda_model, corpus, id2word, sort_topics=False)\npyLDAvis.enable_notebook()\npyLDAvis.display(lda_display)","3bd6b7c7":"Now we'll remove the stopwords, remove words with len < 5 and remove other words that may mess our analysis (that we saw in the word cloud), like 'https' and tokenize the words.","3986895a":"To see which words most appear in the dataset, we'll plot a WordCloud with them.","f292331a":"# Topic analysis using LDA\n\nThis notebook aims to discover which topics were the most discussed in the Cricket Subreddit. For this, we'll use a popular technique called LDA (Latent Dirichlet Allocation)","692aa929":"And that's it! Thank you for checking this notebook. It was largely inspired by these two tutorials:\n1. https:\/\/towardsdatascience.com\/topic-modelling-in-python-with-nltk-and-gensim-4ef03213cd21\n2. https:\/\/towardsdatascience.com\/end-to-end-topic-modeling-in-python-latent-dirichlet-allocation-lda-35ce4ed6b3e0","191052da":"now we'll remove all punctuation and set all letters to lower case","e78a7a91":"Now that we have a trained model let\u2019s visualize the topics for interpretability","d21a44f4":"Now we will build a model with 10 topics where each topic is a combination of keywords, and each keyword contributes a certain weightage to the topic.","6fc18d77":"Next, we convert the tokenized object into a corpus and dictionary.","b6969737":"First we'll take a look on the structure of the dataset"}}