{"cell_type":{"1685b58b":"code","5bee0fb3":"code","96f4763d":"code","dbd8ee83":"code","eda4f69d":"code","b34e81aa":"code","fd388719":"code","b6c0ed12":"code","142b433f":"code","ee95edd3":"code","d7571149":"code","49648d0b":"code","f4fc15ff":"code","ebd12206":"code","7edd97f0":"code","d3408463":"code","a42289d0":"code","1d036e90":"code","511df063":"code","6f0ac3cc":"code","2a5c0ada":"code","1e43e701":"code","c3dc952c":"code","cd558dbf":"code","02b27cbc":"code","1af7d374":"code","51b82be6":"code","c5e46ede":"code","5987a80d":"code","4091aa96":"code","98232745":"code","6c523bf5":"code","7806ff49":"code","b0c9cb52":"code","993d3508":"code","4eaccd8f":"code","e4549260":"code","90b4ca85":"code","cd63a6ad":"code","fbcb931e":"code","13b153ca":"code","f0d959fc":"code","a9ba37e1":"code","6d0e6378":"code","628b8d4a":"code","70b16072":"code","8b9c83e8":"code","bb2e9b2a":"markdown","8b786329":"markdown","ed54cb5b":"markdown","8d28ad3f":"markdown","9de54128":"markdown","b7333b92":"markdown","8b31c1c1":"markdown","de9e604b":"markdown","c151eb1a":"markdown","8dd9961f":"markdown","c5e6f111":"markdown","45a5eb89":"markdown","96f766ce":"markdown","73901ff4":"markdown","18a1b1a2":"markdown","9d6876b5":"markdown","51ff944c":"markdown","336d63db":"markdown","e1a24fb6":"markdown","5f3fd286":"markdown","f6a11180":"markdown","ca8689d3":"markdown","0dd93986":"markdown","8b2e17bc":"markdown","61a08bb6":"markdown","2e3b4f04":"markdown","c846977a":"markdown","bd2398ba":"markdown"},"source":{"1685b58b":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom collections import Counter","5bee0fb3":"!pip install openpyxl","96f4763d":"data = pd.read_excel('..\/input\/soil-detection-for-cotton-crop\/Detection of Soil for cotton crop.xlsx')\ndata.head()","dbd8ee83":"data.drop(['Sample ID'], axis=1, inplace=True)","eda4f69d":"data.info()","b34e81aa":"data.describe()","fd388719":"# columns that are categorical \ncols = [col for col in data.columns if data[col].dtype==object]\n\ndata[cols].describe()","b6c0ed12":"for col in cols:\n    print(col)\n    print()\n    print(data[col].value_counts())\n    print('******************************')\n    print()","142b433f":"data['Cotton Crop'].value_counts()","ee95edd3":"# PLOT DISTRIBUTION OF THE TARGET CLASSES\nplt.figure(figsize=(15,6))\nsns.countplot(data['Cotton Crop'], palette='Set2')\nplt.show()\n\n# SUMMARIZE DISTRIBUTION\ncounter = Counter(data['Cotton Crop'])\nfor k,v in counter.items():\n    per = v \/ len(data['Cotton Crop']) * 100\n    print('Class=%d, n=%d (%.3f%%)' % (k, v, per))","d7571149":"\"\"\"\nUse any of the following method to check for null values\n\"\"\"\n# data.isna().any()\ndata.isnull().any()","49648d0b":"data[data['Particle Width'].isna() == True]","f4fc15ff":"data.corr()","ebd12206":"#print highly correlated variables\ncorr_features =[]\n\nfor i , r in data.corr().iterrows():\n    k=0\n    for j in range(len(r)):\n        if i!= r.index[k]:\n            if r.values[k] >=0.5:\n                corr_features.append([i, r.index[k], r.values[k]])\n            if r.values[k] <= - 0.5:\n                corr_features.append([i, r.index[k], r.values[k]])\n        k += 1\ncorr_features","7edd97f0":"cols","d3408463":"cols.append('Cotton Crop')\ndf = data.drop(cols, axis=1)","a42289d0":"#plot the frequency of the columns\ndf.hist(figsize=(30,20))\nplt.show()","1d036e90":"#let us numerically draw conclusions\n#creating function that can calculate interquartile range of the data\ndef calc_interquartile(data, column):\n    global lower, upper\n    #calculating the first and third quartile\n    first_quartile, third_quartile = np.percentile(data[column], 25), np.percentile(data[column], 75)\n    #calculate the interquartilerange\n    iqr = third_quartile - first_quartile\n    # outlier cutoff (1.5 is a generally taken as a threshold thats why i am also taking it)\n    cutoff = iqr*1.5\n    #calculate the lower and upper limits\n    lower, upper = first_quartile - cutoff , third_quartile + cutoff\n    #remove the outliers from the columns\n    upper_outliers = data[data[column] > upper]\n    lower_outliers = data[data[column] < lower]\n    print('Lower outliers', lower_outliers.shape[0])\n    print('Upper outliers', upper_outliers.shape[0])\n    return print('total outliers', upper_outliers.shape[0] + lower_outliers.shape[0])","511df063":"#applying the above function on columns to find the total outliers in every feature\nplt.figure(figsize=(6,4))\n\nfor i in df.columns:\n    print('Total outliers in ', i)\n    calc_interquartile(df, i)\n    # creating boxplots to see the outliers in the price variable \n    sns.boxplot(y=df[i]).set_title(i)\n    plt.show()\n    print()\n    print()","6f0ac3cc":"#plotting outliers graph for 'K (potassium)' feature \ncalc_interquartile(data, 'K')\nplt.figure(figsize = (10,6))\nsns.histplot(data['K'], kde=False)\nprint(upper, lower)\nplt.axvspan(xmin = lower,xmax= data['K'].min(),alpha=0.2, color='blue', label='Lower Outliers')\nplt.axvspan(xmin = upper,xmax= data['K'].max(),alpha=0.2, color='red', label='Upper Outliers')\nplt.legend()\nplt.show()","2a5c0ada":"df.head()","1e43e701":"df.describe()","c3dc952c":"from sklearn.preprocessing import MinMaxScaler\n  \n# copy the data\nnorm_df = pd.DataFrame(MinMaxScaler().fit_transform(df), columns=df.columns)","cd558dbf":"norm_df.describe()","02b27cbc":"from sklearn.preprocessing import StandardScaler\n\nstd_df = pd.DataFrame(StandardScaler().fit_transform(df), columns=df.columns)","1af7d374":"std_df.describe()","51b82be6":"\"\"\"\nFollowing are the columns that \n\"\"\"\ncols = data.columns[data.dtypes==object]\ncols","c5e46ede":"for col in cols:\n    print(col)\n    print()\n    print(data[col].value_counts())\n    print('******************************')\n    print()","5987a80d":"target = data['Cotton Crop']\ndata.drop(['Particle Attached', 'Cotton Crop'], axis=1, inplace=True)","4091aa96":"data.columns","98232745":"dummy_1 = pd.get_dummies(data['Grain Surface'])\ndummy_1.head()","6c523bf5":"new_df = norm_df.copy()\n# similarly for others\nfor col in cols:\n    try:\n        new_df = pd.concat([new_df, pd.get_dummies(data[col])], axis=1 )\n    except:\n        continue\n    \nnew_df.columns","7806ff49":"new_df.head()","b0c9cb52":"len(new_df)","993d3508":"target.head()","4eaccd8f":"from sklearn.preprocessing import LabelEncoder\n\nenc = LabelEncoder()\ntarget = pd.DataFrame(enc.fit_transform(target))\ntarget.value_counts()","e4549260":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.metrics import fbeta_score, make_scorer\nfrom sklearn.model_selection import train_test_split, KFold, cross_val_score, GridSearchCV,RepeatedStratifiedKFold","90b4ca85":"x_train, x_test, y_train, y_test = train_test_split(new_df, target, test_size = 0.2, random_state=42)\n\nprint(x_train.shape, y_train.shape, x_test.shape, y_test.shape)","cd63a6ad":"from imblearn.over_sampling import SMOTE\n\n# PERFORM OVERSAMPLING USING SMOTE ( SYNTHETIC MINORITY OVERSAMPLING TECHNIQUE )\noversample = SMOTE()\nX_over, y_over = oversample.fit_resample(x_train, y_train)\n\n# PLOT THE NEW DATA\nplt.figure(figsize=(10,6))\nsns.countplot(y_over[0], palette='Wistia')\nplt.show()\n\n# SUMMARIZE DISTRIBUTION\ncounter = Counter(y_over[0])\nfor k,v in counter.items():\n    per = v \/ len(y_over) * 100\n    print('Class=%d, n=%d (%.3f%%)' % (k, v, per))","fbcb931e":"len(X_over)","13b153ca":"X_over.duplicated().any()","f0d959fc":"y_over = np.array(y_over).reshape(-1,)\ny_over.shape","a9ba37e1":"X_over.shape","6d0e6378":"model = LogisticRegression()\nkfold = KFold(n_splits=5)\nresults = cross_val_score(model, x_train, np.array(y_train).reshape(-1,), cv=kfold)\nresults.mean()","628b8d4a":"model = LogisticRegression().fit(x_train, np.array(y_train).reshape(-1,))\nfbeta_score(model.predict(x_test), y_test, beta=2)","70b16072":"model.predict(x_test)","8b9c83e8":"y_test","bb2e9b2a":"### We are getting fbeta score of 1 that is it is classifying with an accuracy of 100%\n\n### We have very less data even after oversampling we were just able to create 54 samples out of 38. Thats why we are getting this magical result.\n\n### In the next notebook we will try too generate more data using Variational Auto Encoders. Stay Tuned!!","8b786329":"### **CORRELATION**","ed54cb5b":"#### Delete the column \"Sample ID\" as it is of no use","8d28ad3f":"#### We can use any of the above two methods and can use norm_df or std_df for training the model.","9de54128":"#### For the categorical columns","b7333b92":"#### Well we can clearly see that the column \"Particle Attached\" has only one value and thus it doesn't give us any information so its better to drop it, \n\n#### while the rest of the colomns are showing that they are very unbalanced.\n\n#### Let's see the label column or the target column which is \"Cotton Crop\"","8b31c1c1":"#### **So only Particle Width has got some null values, let's see how many rows are there like this**","de9e604b":"# **DATA PREPROCESSING**\n\n*********************************************************\n\n> #### Apply normalization or standardization\n> #### Encode the string columns\n\n\n","c151eb1a":"#### We can see from the above that features K (Potassium), Ca (Calcium), Humidity and Mg (Magnesium) has 5 to 4 outliers\n\n### **So should we remove it ?**\n\n#### Answer is same as for correlation. We need to check first whether its really affecting the model or not.","8dd9961f":"#### Now let us check the correlation between the different columns","c5e6f111":"# **RESAMPLING TECHNIQUES**\n\n*******************************************\n#### Well, there are two kinds of resampling techniques one is oversampling and the other is undersampling. As we have very less data, this left us with the choice of oversampling.\n\n### **SMOTE  (Synthetic Minority Oversampling Technique)**","45a5eb89":"## Before Normalization","96f766ce":"#### First thing we need to see some of statistics related to the dataframe\n\n#### it will just show the statistics of the columns which are non-categorical","73901ff4":"### Encoding target variable","18a1b1a2":"#### Positive relation between them shows that they are directly proportional and indirectly proportional for the negative ones. \n\n#### The greatest correlation between any two columns can be 1 and thats why I took the threshold as 0.5 and -0.5. \n\n#### Any number beyond this shows the stronger bond between them.\n\n### **Why are we even bothered about it?**\n\n#### The reason is two highly correlated (negative or positive) columns (features) provides the same kind of information to the model and hence its better to keep only one of them.\n\n### **How should we do this?**\n\n#### In my opinion, first you keep all of them and train the model and then gradually remove them to see whether there is any increase in the model's performance. If yes then remove it, otherwise no need.","9d6876b5":"### **CONCLUSION**\n\n\n#### This is the end of the DATA EXPLORATION & VISUALISATION part and we have concluded following things:-\n\n************************\n> #### Data values are not on the same scale and some columns contains the string values\n> #### Dataset is really small with only 48 rows and highly imbalanced as we have only 6 True values for Cotton Crop and 42 False values.\n> #### There are outliers present in some of the columns.\n> #### There is only one null value thats too in Particle Width column\n> #### Some of the features (columns) are highly correlated.\n\n\n### **SOLUTION**\n\n> #### Apply normalization or standardization to put data on the same scale and encode the string values.\n> #### Apply resampling techniques to balance the dataset and oversampling will help in increasing the dataset.\n> #### Remove the outliers if the model shows low performace\n> #### Replace the null value with the mean value or the previous value of the row above it or remove that row and predict its value applying ML.\n> #### Remove one of the highly correlated features if the model shows low performace.\n","51ff944c":"### **ENCODING STRING VALUES**","336d63db":"#### Now we need to check is whether there are any null values\n\n### **NULL VALUE DETECTION**","e1a24fb6":"### **CONCLUSION**\n\n*******************************\n\n#### We are done with the preprocessing. \n ","5f3fd286":"### **NORMALIZATION**\n\n***********************************\n\n#### Normalization is a technique that rescales the feature to a hard and fast range of [0,1] by subtracting the minimum value of the feature then dividing by the range. \n\n#### Hence, it is also known as Min-Max Scaling.","f6a11180":"### **STANDARDIZATION**\n\n*************************************\n\n#### In standardisation, we subtract the mean from the values and divide it by the standard deviation. It scales the value around the mean of 0 and standard deviation of 1.\n\n#### (x - mean) \/ std\n\n#### It is also known as z-score.","ca8689d3":"# **DATA EXPLORATION & VISUALISATION**","0dd93986":"### **OUTLIER DETECTION**\n\n**************************************************\n\n#### Well, there are different methods of detecting outliers like IQR (Inter quartile range), z-score, isolation forest etc. I am going to use IQR","8b2e17bc":"#### There are 42 false values and 6 true values , its highly unbalanced. Thus we need to apply resampling techniques to overcome this.","61a08bb6":"## After Normalization","2e3b4f04":"# **MODEL TRAINING**","c846977a":"#### The above graphs are showing distribution of the data values in each column. ","bd2398ba":"## Visualising the data with outliers"}}