{"cell_type":{"41293c07":"code","9d9cd636":"code","577017ce":"code","4be6bdcf":"code","cd6d34ea":"code","2ab02803":"code","b4fff37f":"code","098a41da":"code","ad8e80c6":"code","93b226b3":"code","7b7d5382":"code","8983d213":"code","276dba53":"code","0deb3938":"code","387785a8":"code","7b1a4532":"code","dfd1eae6":"code","6eb9430e":"code","eca0d7aa":"code","63d0a06c":"code","cd7a79b7":"code","b51f7941":"code","b3f9c54d":"code","24faa94d":"code","54c87589":"code","d2a30158":"code","c091f3fc":"code","29bba844":"code","8a828e57":"code","aa04dc15":"code","99c77b87":"code","43815f4c":"code","59ed6e9d":"code","1092478d":"code","5a8e2a47":"code","2acd7ba8":"code","cd4d0185":"code","815d58ad":"code","33edc429":"code","01473d59":"code","0efb2914":"code","b5b6332a":"code","6bf2bf39":"code","a9cd580b":"markdown","1b5a40d8":"markdown","30148390":"markdown","24db5645":"markdown","e309db51":"markdown","abb31fc5":"markdown","d50e655f":"markdown","9da94d75":"markdown","034b61bd":"markdown","bb4cda6b":"markdown","fa7145a1":"markdown","cfb22023":"markdown","e1a8b40a":"markdown","c69bc21c":"markdown","2548142c":"markdown","a78dc89a":"markdown","d039d9f0":"markdown","42a6bc89":"markdown","b1391def":"markdown","7c656c40":"markdown","3b5c99c4":"markdown","c2e2ed5d":"markdown","106d503f":"markdown","111ab16c":"markdown","fd39b78d":"markdown","43e0fff3":"markdown","d92ae4c1":"markdown","eb1675e9":"markdown","97bbf0ef":"markdown"},"source":{"41293c07":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom collections import Counter\n\nfrom sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier, ExtraTreesClassifier, VotingClassifier\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.model_selection import GridSearchCV, cross_val_score, StratifiedKFold, learning_curve\n\nsns.set(style='white', context='notebook', palette='deep')\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","9d9cd636":"train = pd.read_csv(\"\/kaggle\/input\/data-ghoul\/train.csv\")\ntest = pd.read_csv(\"\/kaggle\/input\/data-ghoul\/test.csv\")\nIDtest = test[\"id\"]","577017ce":"## Outlier detection \n\ndef detect_outliers(df,n,features):\n    outlier_indices = []\n    \n    for col in features:\n        Q1 = np.percentile(df[col], 25)\n        Q3 = np.percentile(df[col],75)\n        IQR = Q3 - Q1\n        outlier_step = 1.5 * IQR\n        outlier_list_col = df[(df[col] < Q1 - outlier_step) | (df[col] > Q3 + outlier_step )].index\n        outlier_indices.extend(outlier_list_col)\n        \n    # select observations containing more than 2 outliers\n    outlier_indices = Counter(outlier_indices)        \n    multiple_outliers = list( k for k, v in outlier_indices.items() if v > n )\n    \n    return multiple_outliers   \n\n## Detect outliers from Age, SibSp , Parch and Fare (numerical features)\nOutliers_to_drop = detect_outliers(train,2,[\"bone_length\",\"rotting_flesh\",\"hair_length\",\"has_soul\"])","4be6bdcf":"# Show the outliers rows\ntrain.loc[Outliers_to_drop]","cd6d34ea":"## Join train and test data to obtain the same number of features during categorical conversion\ntrain_len = len(train)\ndataset =  pd.concat(objs=[train, test], axis=0).reset_index(drop=True)","2ab02803":"dataset.head()","b4fff37f":"## Fill empty and NaNs values with NaN\ndataset = dataset.fillna(np.nan)\n\n## Check for Null values\ndataset.isnull().sum()","098a41da":"train.isnull().sum()","ad8e80c6":"train.dtypes","93b226b3":"## Replace string to int\ntrain[\"type_int\"] = train[\"type\"].replace({\n    \"Ghoul\":1,\n    \"Goblin\":2,\n    \"Ghost\":3\n})","7b7d5382":"## Correlation matrix\nsns.heatmap(train[[\"type_int\", \"bone_length\", \"rotting_flesh\", \"hair_length\", \"has_soul\"]].corr(), annot=True, fmt=\".2f\", cmap=\"coolwarm\")","8983d213":"## Distribution plot\ndef f_dist_plot(col):\n    g = sns.FacetGrid(train, col=\"type\")\n    g = g.map(sns.distplot, col)\n    \nf_dist_plot(\"bone_length\")","276dba53":"## KDE Plot \ndef f_kde_plot(col):\n    g = sns.kdeplot(train[col][(train[\"type_int\"] == 1) & (train[col].notnull())], color=\"Red\", shade = True)\n    g = sns.kdeplot(train[col][(train[\"type_int\"] == 2) & (train[col].notnull())], ax =g, color=\"Blue\", shade= True)\n    g = sns.kdeplot(train[col][(train[\"type_int\"] == 3) & (train[col].notnull())], ax =g, color=\"Green\", shade= True)\n    g.set_xlabel(col)\n    g.set_ylabel(\"Frequency\")\n    g = g.legend([\"Ghoul\",\"Goblin\",\"Ghost\"])\n\nf_kde_plot(\"bone_length\")","0deb3938":"## Distribution plot\nf_dist_plot(\"rotting_flesh\")","387785a8":"## KDE plot\nf_kde_plot(\"rotting_flesh\")","7b1a4532":"## Distribution plot\nf_dist_plot(\"hair_length\")","dfd1eae6":"## KDE plot\nf_kde_plot(\"hair_length\")","6eb9430e":"## Distribution plot\nf_dist_plot(\"has_soul\")","eca0d7aa":"## KDE plot\nf_kde_plot(\"has_soul\")","63d0a06c":"## Scatter plot has_soul vs hair_length\nsns.scatterplot(x=\"hair_length\", y=\"has_soul\", data=train, hue=\"type\")","cd7a79b7":"## Plot color vs type\nfig, axs = plt.subplots(3, 2, figsize=(10,8))\n\ndef plot_color(x, y, color, color_bar):\n    df_color = train[train[\"color\"] == color].groupby([\"type\"]).size()\n    axs[x,y].bar(df_color.index.values, df_color.values, color=color_bar)\n    axs[x,y].set_title(color)\n\nplot_color(0,0,\"clear\", \"beige\")\nplot_color(0,1,\"green\", \"g\")\nplot_color(1,0,\"black\", \"k\")\nplot_color(1,1,\"white\", \"whitesmoke\")\nplot_color(2,0,\"blue\", \"b\")\nplot_color(2,1,\"blood\", \"r\")\nplt.tight_layout()","b51f7941":"## Convert feature color to binary\ndataset = pd.get_dummies(dataset, columns=[\"color\"])","b3f9c54d":"## Convert type from string to int\ndataset[\"type\"] = dataset[\"type\"].replace({\n    \"Ghoul\":1,\n    \"Goblin\":2,\n    \"Ghost\":3\n})","24faa94d":"dataset.head()","54c87589":"## Separate train dataset and test dataset\ntrain = dataset[:train_len]\ntest = dataset[train_len:]\n\n## Drop type and id label\ntest.drop([\"type\", \"id\"],axis = 1,inplace=True)","d2a30158":"## Separate X and y label\ntrain[\"type\"] = train[\"type\"].astype(int)\n\nY_train = train[\"type\"]\nX_train = train.drop([\"type\", \"id\"], axis = 1)","c091f3fc":"#KFold Stratified cross val\nkfold = StratifiedKFold(n_splits=10)","29bba844":"## Modeling step Test differents algorithms \nrandom_state = 2\nclassifiers = []\nclassifiers.append(SVC(random_state=random_state))\nclassifiers.append(DecisionTreeClassifier(random_state=random_state))\nclassifiers.append(AdaBoostClassifier(DecisionTreeClassifier(random_state=random_state),random_state=random_state,learning_rate=0.1))\nclassifiers.append(RandomForestClassifier(random_state=random_state))\nclassifiers.append(ExtraTreesClassifier(random_state=random_state))\nclassifiers.append(GradientBoostingClassifier(random_state=random_state))\nclassifiers.append(MLPClassifier(random_state=random_state))\nclassifiers.append(KNeighborsClassifier())\nclassifiers.append(LogisticRegression(random_state = random_state))\nclassifiers.append(LinearDiscriminantAnalysis())\n\ncv_results = []\nfor classifier in classifiers :\n    cv_results.append(cross_val_score(classifier, X_train, y = Y_train, scoring = \"accuracy\", cv = kfold, n_jobs=4))\n\ncv_means = []\ncv_std = []\nfor cv_result in cv_results:\n    cv_means.append(cv_result.mean())\n    cv_std.append(cv_result.std())\n\ncv_res = pd.DataFrame({\"CrossValMeans\":cv_means,\"CrossValerrors\": cv_std,\"Algorithm\":[\"SVC\",\"DecisionTree\",\"AdaBoost\",\n\"RandomForest\",\"ExtraTrees\",\"GradientBoosting\",\"MultipleLayerPerceptron\",\"KNeighboors\",\"LogisticRegression\",\"LinearDiscriminantAnalysis\"]})","8a828e57":"## Plot score of cross validation models\ng = sns.barplot(\"CrossValMeans\",\"Algorithm\",data = cv_res, palette=\"Set3\",orient = \"h\",**{'xerr':cv_std})\ng.set_xlabel(\"Mean Accuracy\")\ng = g.set_title(\"Cross validation scores\")","aa04dc15":"cv_res","99c77b87":"## Hyperparameter Cross Validation Models\nclassifiers","43815f4c":"## LinearDiscriminantAnalysis\nLDA = LinearDiscriminantAnalysis()\n\n\n## Search grid for optimal parameters\nex_param_grid = {\"n_components\": [None, 1, 2, 3, 4]}\n\n\ngsLDA = GridSearchCV(LDA,param_grid = ex_param_grid, cv=kfold, scoring=\"accuracy\", n_jobs= 4, verbose = 1)\n\ngsLDA.fit(X_train,Y_train)\n\nLDA_best = gsLDA.best_estimator_\n\n## Best score\ngsLDA.best_score_","59ed6e9d":"## Gradient boosting tunning\n\nGBC = GradientBoostingClassifier()\ngb_param_grid = {'loss' : [\"deviance\"],\n              'n_estimators' : [100,200,300],\n              'learning_rate': [0.1, 0.05, 0.01],\n              'max_depth': [4, 8],\n              'min_samples_leaf': [100,150],\n              'max_features': [0.3, 0.1] \n              }\n\ngsGBC = GridSearchCV(GBC,param_grid = gb_param_grid, cv=kfold, scoring=\"accuracy\", n_jobs= 4, verbose = 1)\n\ngsGBC.fit(X_train,Y_train)\n\nGBC_best = gsGBC.best_estimator_\n\n## Best score\ngsGBC.best_score_","1092478d":"## MultipleLayerPerceptron\nMLP = MLPClassifier()\n\n\n## Search grid for optimal parameters\nex_param_grid = {\n    'hidden_layer_sizes': [(50,50,50), (50,100,50), (100,)],\n    'activation': ['tanh', 'relu'],\n    'solver': ['sgd', 'adam'],\n    'alpha': [0.0001, 0.05],\n    'learning_rate': ['constant','adaptive'],\n}\n\n\ngsMLP = GridSearchCV(MLP, param_grid = ex_param_grid, cv=kfold, scoring=\"accuracy\", n_jobs= 4, verbose = 1)\n\ngsMLP.fit(X_train,Y_train)\n\nMLP_best = gsMLP.best_estimator_\n\n## Best score\ngsMLP.best_score_","5a8e2a47":"## LogisticRegression\nLRC = LogisticRegression()\n\n\n## Search grid for optimal parameters\nex_param_grid = {\n    'C': [0.001, 0.01, 0.1, 1, 10, 100, 1000]\n}\n\n\ngsLRC = GridSearchCV(LRC, param_grid = ex_param_grid, cv=kfold, scoring=\"accuracy\", n_jobs= 4, verbose = 1)\n\ngsLRC.fit(X_train,Y_train)\n\nLRC_best = gsLRC.best_estimator_\n\n## Best score\ngsLRC.best_score_","2acd7ba8":"def plot_learning_curve(estimator, title, X, y, ylim=None, cv=None,\n                        n_jobs=-1, train_sizes=np.linspace(.1, 1.0, 5)):\n    plt.figure()\n    plt.title(title)\n    if ylim is not None:\n        plt.ylim(*ylim)\n    plt.xlabel(\"Training examples\")\n    plt.ylabel(\"Score\")\n    train_sizes, train_scores, test_scores = learning_curve(\n        estimator, X, y, cv=cv, n_jobs=n_jobs, train_sizes=train_sizes)\n    train_scores_mean = np.mean(train_scores, axis=1)\n    train_scores_std = np.std(train_scores, axis=1)\n    test_scores_mean = np.mean(test_scores, axis=1)\n    test_scores_std = np.std(test_scores, axis=1)\n    plt.grid()\n\n    plt.fill_between(train_sizes, train_scores_mean - train_scores_std,\n                     train_scores_mean + train_scores_std, alpha=0.1,\n                     color=\"r\")\n    plt.fill_between(train_sizes, test_scores_mean - test_scores_std,\n                     test_scores_mean + test_scores_std, alpha=0.1, color=\"g\")\n    plt.plot(train_sizes, train_scores_mean, 'o-', color=\"r\",\n             label=\"Training score\")\n    plt.plot(train_sizes, test_scores_mean, 'o-', color=\"g\",\n             label=\"Cross-validation score\")\n\n    plt.legend(loc=\"best\")\n    return plt","cd4d0185":"## Plot learning curves of best estimators\ng = plot_learning_curve(gsLDA.best_estimator_,\"Linear Discriminant Analysis learning curves\",X_train,Y_train,cv=kfold)\ng = plot_learning_curve(gsGBC.best_estimator_,\"Gradient Boosting learning curves\",X_train,Y_train,cv=kfold)\ng = plot_learning_curve(gsMLP.best_estimator_,\"MLP learning curves\",X_train,Y_train,cv=kfold)\ng = plot_learning_curve(gsLRC.best_estimator_,\"Logistic Regression learning curves\",X_train,Y_train,cv=kfold)","815d58ad":"## Plot Feature Importance of Gradiect Boosting\n\nplt.figure(figsize=(10,10))\nnames_classifiers = [(\"GBC\",GBC_best)]\nnclassifier = 0\nname = names_classifiers[nclassifier][0]\nclassifier = names_classifiers[nclassifier][1]\nindices = np.argsort(classifier.feature_importances_)[::-1][:40]\ng = sns.barplot(y=X_train.columns[indices][:40],x = classifier.feature_importances_[indices][:40] , orient='h')\ng.set_xlabel(\"Relative importance\",fontsize=12)\ng.set_ylabel(\"Features\",fontsize=12)\ng.tick_params(labelsize=9)\ng.set_title(name + \" feature importance\")        ","33edc429":"## Corealation between best models\n\ntest_type_LDA = pd.Series(LDA_best.predict(test), name=\"LDA\")\ntest_type_GBC = pd.Series(GBC_best.predict(test), name=\"GBC\")\ntest_type_MLP = pd.Series(MLP_best.predict(test), name=\"MLP\")\ntest_type_LRC = pd.Series(LRC_best.predict(test), name=\"LRC\")\n\nconcatenate_results = pd.concat([test_type_LDA,test_type_GBC,test_type_MLP,test_type_LRC],axis=1)\n\ng= sns.heatmap(concatenate_results.corr(),annot=True)","01473d59":"concatenate_results.corr()","0efb2914":"## Voting Classifier\nvotingC = VotingClassifier(estimators=[('lda', LDA_best), ('gbc', GBC_best),\n('mlp', MLP_best), ('lrc',LRC_best)], voting='soft', n_jobs=4)\n\nvotingC = votingC.fit(X_train, Y_train)","b5b6332a":"## Voting Classifier Model Parameter\nvotingC","6bf2bf39":"## Predict test data\ntest_type = pd.Series(votingC.predict(test), name=\"Type\")\ntest_type = test_type.replace({\n    1:\"Ghoul\",\n    2:\"Goblin\",\n    3:\"Ghost\"\n})\nresults = pd.concat([IDtest,test_type],axis=1)\nresults.to_csv(\"voting_prediction.csv\",index=False)","a9cd580b":"#### 5.1 Preparation","1b5a40d8":"- feature rotting_flesh","30148390":"### Outline\n1. Import Libraries\n2. Load and Check Data\n<br>&nbsp;2.1 Load data\n<br>&nbsp;2.2 Outlier detection\n<br>&nbsp;2.3 Join train and test data\n<br>&nbsp;2.4 Check for null and missing values\n3. Feature Analysis\n<br>&nbsp;3.1 Numerical Analysis\n<br>&nbsp;3.2 Categorical Analysis\n4. Feature Engineering\n<br>&nbsp;4.1 Dummies color\n5. Modelling\n<br>&nbsp;5.1 Preparation\n<br>&nbsp;5.2 Cross Validate Model\n<br>&nbsp;5.3 Hyperparameter tuning for best models\n<br>&nbsp;5.4 Learning curves of best models\n<br>&nbsp;5.5 Tree based feature importance\n<br>&nbsp;5.6 Correlation of best models\n6. Prediction","24db5645":"No outlier found","e309db51":"BEST CV MODELS : LinearDiscriminantAnalysis, GradientBoosting, MultipleLayerPerceptron, LogisticRegression","abb31fc5":"#### 2.2 Outlier detection","d50e655f":"#### 5.5 Tree based feature importance","9da94d75":"#### 2.4 Check for null and missing values","034b61bd":"### 3. Feature Analysis","bb4cda6b":"#### 3.2 Categorical Analysis","fa7145a1":"No missing value found","cfb22023":"### 2. Load and Check Data","e1a8b40a":"- feature has_soul","c69bc21c":"- feature bone_length","2548142c":"# Ghouls, Goblins, and Ghosts Classification with Voting Classifier","a78dc89a":"#### 2.1 Load data","d039d9f0":"- feature hair_length","42a6bc89":"### 1. Import Libraries","b1391def":"#### 5.3 Hyperparameter tuning for best models","7c656c40":"### 5. Modelling","3b5c99c4":"#### 5.2 Cross validate model","c2e2ed5d":"### 6. Prediction","106d503f":"####  2.3 Join train and test data","111ab16c":"#### 5.6 Correlation of best models","fd39b78d":"- Numerical values : bone_length, rotting_flesh, hair_length, has_soul\n- Categorical values : color, type","43e0fff3":"#### 4.1 Get Dummies Color","d92ae4c1":"#### 5.4 Learning curves of best models","eb1675e9":"#### 3.1 Numerical Analysis","97bbf0ef":"### 4. Feature Engineering"}}