{"cell_type":{"0743dd5b":"code","43e56afe":"code","0c6033b0":"code","f348be9e":"code","fd63c566":"code","a78adbbf":"code","7d9b0e67":"code","27fc75c8":"code","fc6a6863":"code","468a39cd":"code","c2be519a":"code","a40716d2":"code","0c501491":"code","65c531eb":"code","46762e50":"code","8c5084bb":"code","be0c6936":"code","0950dc00":"code","5f32ea91":"markdown","337761eb":"markdown","88d5149d":"markdown","4b6d1777":"markdown","3f36c35f":"markdown","56cbb1fb":"markdown","04a54390":"markdown","2150698a":"markdown"},"source":{"0743dd5b":"import warnings\nwarnings.filterwarnings('ignore')\nimport numpy as np\nimport pandas as pd\nimport os\nimport string\nimport re\nimport gc\n\nfrom tqdm import tqdm_notebook as tqdm\ntqdm().pandas()\n\nfrom numpy.random import shuffle\n\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.utils import to_categorical\nfrom keras.utils.vis_utils import plot_model\n\nfrom keras.layers import Input, Dense, LSTM, Embedding, RepeatVector, TimeDistributed\nfrom keras.models import Sequential\nfrom keras.callbacks import ModelCheckpoint\n\nfrom pickle import dump\nfrom unicodedata import normalize","43e56afe":"FILE_PATH = '..\/input\/german-to-english\/deu.txt'\n\n# load doc into memory\ndef load_doc(filename):\n    file = open(filename, mode='rt', encoding='utf-8')\n    text = file.read()\n    file.close()\n    return text\n\n# split a loaded document into sentences\ndef to_pairs(doc):\n    lines = doc.strip().split('\\n')\n    pairs = [line.split('\\t') for line in lines]\n    return pairs\n\n# clean list of lines\ndef clean_pairs(lines):\n    cleaned = list()\n    re_punc = re.compile('[%s]' % re.escape(string.punctuation)) \n    re_print = re.compile('[^%s]' % re.escape(string.printable))\n    for pair in tqdm(lines):\n        clean_pair = list()\n        for line in pair:\n             # normalize unicode characters\n            line = normalize('NFD', line).encode('ascii', 'ignore') \n            line = line.decode('UTF-8')\n              # tokenize on white space\n            line = line.split()\n             # convert to lowercase\n            line = [word.lower() for word in line]\n            # remove punctuation from each token\n            line = [re_punc.sub('', w) for w in line]\n            # remove non-printable chars form each token \n            line = [re_print.sub('', w) for w in line]\n            # remove tokens with numbers in them\n            line = [word for word in line if word.isalpha()] # store as string\n            clean_pair.append(' '.join(line))\n        cleaned.append(clean_pair)\n      \n    return np.array(cleaned)","0c6033b0":"doc = load_doc(FILE_PATH)\npairs = to_pairs(doc)\ntext = clean_pairs(pairs)","f348be9e":"for i in range(10):\n    print('[%s] => [%s]' % (text[i,0], text[i,1]))","fd63c566":"shuffle(text)\ntrain, test = text[:7000], text[7000:8000]","a78adbbf":"def create_tokenizer(lines):\n    tokenizer = Tokenizer()\n    tokenizer.fit_on_texts(lines)\n    return tokenizer\n\ndef max_length(lines):\n    return max(len(line.split()) for line in lines)","7d9b0e67":"eng_tokenizer = create_tokenizer(text[:8000,0])\neng_vocab_size = len(eng_tokenizer.word_index) + 1\neng_max_length = max_length(text[:8000,0])\n\nprint('English Vocab size : ', eng_vocab_size)\nprint('English Max sentence Length : ', eng_max_length)\n\ngerman_tokenizer = create_tokenizer(text[:8000,1])\ngerman_vocab_size = len(german_tokenizer.word_index) + 1\ngerman_max_length = max_length(text[:8000,1])\n\nprint('German Vocab size : ', german_vocab_size)\nprint('German Max sentence Length : ', german_max_length)","27fc75c8":"def encode_sequences(tokenizer, max_length, lines):\n    X = tokenizer.texts_to_sequences(lines)\n    X = pad_sequences(X, maxlen=max_length, padding='post')\n    return X\n\n# one hot encode target sequence\ndef encode_output(sequences, vocab_size):\n    ylist = list()\n    for sequence in tqdm(sequences):\n        encoded = to_categorical(sequence, num_classes=vocab_size)\n        ylist.append(encoded)\n    ylist = np.array(ylist, dtype='float16')\n    ylist = ylist.reshape(sequences.shape[0], sequences.shape[1], vocab_size)\n    \n    return ylist","fc6a6863":"# prepare training data\ntrainX = encode_sequences(german_tokenizer, german_max_length, train[:, 1])\ntrainY = encode_sequences(eng_tokenizer, eng_max_length, train[:, 0])","468a39cd":"del text\ndel pairs\ndel doc\ngc.collect()","c2be519a":"trainY = encode_output(trainY, eng_vocab_size)","a40716d2":"# prepare validation data\ntestX = encode_sequences(german_tokenizer, german_max_length, test[:, 1])\ntestY = encode_sequences(eng_tokenizer, eng_max_length, test[:, 0])\ntestY = encode_output(testY, eng_vocab_size)","0c501491":"# define NMT model\ndef define_model(src_vocab, tar_vocab, src_timesteps, tar_timesteps, n_units):\n    model = Sequential()\n    model.add(Embedding(src_vocab, n_units, input_length=src_timesteps, mask_zero=True))\n    model.add(LSTM(n_units, return_sequences=True))\n    model.add(LSTM(256))\n    model.add(RepeatVector(tar_timesteps))\n    model.add(LSTM(256, return_sequences=True)) \n    model.add(TimeDistributed(Dense(tar_vocab, activation='softmax'))) \n    # compile model\n    model.compile(optimizer='adam', loss='categorical_crossentropy')\n    # summarize defined model\n    model.summary()\n    plot_model(model, to_file='model.png', show_shapes=True)\n    return model","65c531eb":"# define model\nmodel = define_model(german_vocab_size, eng_vocab_size, german_max_length, eng_max_length, 512)\n# fit model\ncheckpoint = ModelCheckpoint('model.h5', monitor='val_loss', verbose=1, save_best_only=True, mode='min')","46762e50":"history = model.fit(trainX, trainY, epochs=30, batch_size=64, validation_data=(testX, testY),\n    callbacks=[checkpoint], verbose=2)","8c5084bb":"from keras.models import load_model\nfrom pickle import load\nfrom nltk.translate.bleu_score import corpus_bleu","be0c6936":"# map an integer to a word\ndef word_for_id(integer, tokenizer):\n    for word, index in tokenizer.word_index.items():\n        if index == integer:\n            return word\n    return None\n\n# generate target given source sequence\ndef predict_sequence(model, tokenizer, source):\n    prediction = model.predict(source, verbose=0)[0]\n    integers = [np.argmax(vector) for vector in prediction]\n    target = list()\n    for i in integers:\n        word = word_for_id(i, tokenizer)\n        if word is None:\n            break\n        target.append(word) \n    return ' '.join(target)\n\n# evaluate the skill of the model\ndef evaluate_model(model, sources, raw_dataset):\n    actual, predicted = list(), list()\n    for i, source in enumerate(sources):\n        # translate encoded source text\n        source = source.reshape((1, source.shape[0]))\n        translation = predict_sequence(model, eng_tokenizer, source)\n        raw_target, raw_src = raw_dataset[i,0], raw_dataset[i,1]\n        if i < 10:\n            print('src=[%s], target=[%s], predicted=[%s]' % (raw_src, raw_target, translation)) \n        actual.append([raw_target.split()])\n        predicted.append(translation.split())\n    # calculate BLEU score\n    print('BLEU-1: %f' % corpus_bleu(actual, predicted, weights=(1.0, 0, 0, 0))) \n    print('BLEU-2: %f' % corpus_bleu(actual, predicted, weights=(0.5, 0.5, 0, 0))) \n    print('BLEU-3: %f' % corpus_bleu(actual, predicted, weights=(0.3, 0.3, 0.3, 0))) \n    print('BLEU-4: %f' % corpus_bleu(actual, predicted, weights=(0.25, 0.25, 0.25, 0.25)))   ","0950dc00":"# load model\nmodel = load_model('model.h5')\n# test on some training sequences\nprint('train')\nevaluate_model(model, trainX, train) \n# test on some test sequences \nprint('\\n\\n\\ntest')\nevaluate_model(model, testX, test)","5f32ea91":"\n<center><h3>Baseline Model<\/h3><\/center>\n\nWe can start-off by describing the baseline model used as the starting point for all experiments. A baseline model configuration was chosen such that the model would perform reasonably well on the translation task.\n<ul><li>Embedding: 512-dimensions.<br>\n    <blockquote>Recommendation: Start with a small embedding, such as 128, perhaps increase the size\n        later for a minor lift in skill.<\/blockquote>\n    <li>RNN Cell: Gated Recurrent Unit or GRU.<br><blockquote>Recommendation: Use LSTM RNN units in your mode.<\/blockquote>\n    <li> Encoder: Bidirectional.<br><blockquote>Recommendation: Use a reversed order input sequence or move to bidirectional for a small lift in model skill.<\/blockquote>\n    <li> Encoder Depth: 2-layers (1 layer in each direction).<br><blockquote>Recommendation: Use a 1-layer bidirectional encoder and extend to 2 bidirectional layers for a small lift in skill.<\/blockquote>\n        <li>Decoder Depth: 2-layers.<br><blockquote>Recommendation: Use a 1-layer decoder as a starting point and use a 4-layer decoder for better results.<\/blockquote>\n    <li> Attention: Bahdanau-style.\n    <li> Optimizer: Adam.\n    <li> Dropout: 20% on input.\n    <\/ul>\nEach experiment started with the baseline model and varied one element in an attempt to isolate the impact of the design decision on the model skill, in this case, BLEU scores.\n\n![image.png](attachment:image.png)","337761eb":"<center><h3>BLEU Score<\/h3><\/center>\n\n<div style=\"font-family:verdana; word-spacing:1.5px;\">\n    The Bilingual Evaluation Understudy Score, or BLEU for short, is a metric for evaluating a generated sentence to a reference sentence. A perfect match results in a score of 1.0, whereas a perfect mismatch results in a score of 0.0.\n    <br><br>\n    The primary programming task for a BLEU implementor is to compare n-grams of the candidate with the n-grams of the reference translation and count the number of matches. These matches are position-independent. The more the matches, the better the candidate translation is.\n<br><br>\nSentence BLEU Score :<br>\nNLTK provides the sentence bleu() function for evaluating a candidate sentence against one or more reference sentences. The reference sentences must be provided as a list of sentences where each reference is a list of tokens. The candidate sentence is provided as a list of tokens.\n <br><br>   \nCorpus BLEU Score :<br>\nNLTK also provides a function called corpus bleu() for calculating the BLEU score for multiple sentences such as a paragraph or a document. The references must be specified as a list of documents where each document is a list of references and each alternative reference is a list of tokens, e.g. a list of lists of lists of tokens. The candidate documents must be specified as a list where each document is a list of tokens, e.g. a list of lists of tokens.\n    <br><br>\nIndividual n-gram Scores :<br>\nAn individual n-gram score is the evaluation of just matching grams of a specific order, such as single words (1-gram) or word pairs (2-gram or bigram). The weights are specified as a tuple where each index refers to the gram order. To calculate the BLEU score only for 1-gram matches, you can specify a weight of 1 for 1-gram and 0 for 2, 3 and 4 (1, 0, 0, 0).\n    <br><br>\n    Cumulative n-gram Scores :<br>\nCumulative scores refer to the calculation of individual n-gram scores at all orders from 1 to n and weighting them by calculating the weighted geometric mean. By default, the sentence bleu() and corpus bleu() scores calculate the cumulative 4-gram BLEU score, also called BLEU-4. The weights for the BLEU-4 are 1\/4 (25%) or 0.25 for each of the 1-gram, 2-gram, 3-gram and 4-gram scores.\n<\/div>","88d5149d":"<center><h3>Tokienizing & Encoding<\/h3><\/center>","4b6d1777":"<center><h3>Loading and Preparing the Text Data","3f36c35f":"<center><h2>Neural Machine Translation<\/h2><\/center>\n<br>\n\n![image.png](attachment:image.png)\n<br>\n\nOne of the earliest goals for computers was the automatic translation of text from one language to another. Automatic or machine translation is perhaps one of the most challenging artificial intelligence tasks given the fluidity of human language. Classically, rule-based systems were used for this task, which were replaced in the 1990s with statistical methods. More recently, deep neural network models achieve state-of-the-art results in a field that is aptly named neural machine translation.\n***\n<div style=\"font-family:verdana; word-spacing:1.5px;\">\n<ol>\n    <li>\n<b>What is Statistical Machine Translation?<\/b><br>\nStatistical machine translation, or SMT for short, is the use of statistical models that learn to translate text from a source language to a target language given a large corpus of examples. This task of using a statistical model can be stated formally as follows:<br>\n<pre>Given a sentence T in the target language, we seek the sentence S from which the translator produced T. We know that our chance of error is minimized by choosing that sentence S that is most probable given T. Thus, we wish to choose S so as to maximize Pr(S|T).<\/pre>\n\n<li><b>What is Neural Machine Translation?<\/b><br>\nNeural machine translation, or NMT for short, is the use of neural network models to learn a statistical model for machine translation. The key benefit to the approach is that a single system can be trained directly on source and target text, no longer requiring the pipeline of specialized systems used in statistical machine learning.\n    <\/ol>\n    \n<\/div>","56cbb1fb":"![image.png](attachment:image.png)\n\n\n<div style=\"font-family:verdana; word-spacing:1.5px;\">\n<ol>\n<li>Encoder-Decoder Model\n    <br>\nMultilayer Perceptron neural network models can be used for machine translation, although the models are limited by a fixed-length input sequence where the output must be the same length.<br><br>\n    <blockquote>\nThese early models have been greatly improved upon recently through the use of recurrent neural networks organized into an encoder-decoder architecture that allow for variable length input and output sequences.\nAn encoder neural network reads and encodes a source sentence into a fixed-length vector. A decoder then outputs a translation from the encoded vector. <\/blockquote>\n    <li>Encoder-Decoders with Attention<br>\nAlthough effective, the Encoder-Decoder architecture has problems with long sequences of text to be translated. The problem stems from the fixed-length internal representation that must be used to decode each word in the output sequence. The solution is the use of an attention mechanism that allows the model to learn where to place attention on the input sequence as each word of the output sequence is decoded.\n        <br><br>\n        <blockquote>\nUsing a fixed-sized representation to capture all the semantic details of a very long sentence [...] is very difficult. [...] A more efficient approach, however, is to read the whole sentence or paragraph [...], then to produce the translated words one at a time, each time focusing on a different part of the input sentence to gather the semantic details required to produce the next output word.\n        <\/blockquote>","04a54390":"<center><h3>Evaluate Model","2150698a":"<center><h3>Importing Libraries"}}