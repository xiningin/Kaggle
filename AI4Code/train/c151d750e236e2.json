{"cell_type":{"caf9ffe9":"code","80a35220":"code","c3723c00":"code","a075c0c7":"code","be4efb47":"code","cfcc4f6b":"code","812bfa86":"code","8303ba68":"code","dd11c555":"code","5344e5dd":"code","adbfd3a5":"code","462d0c22":"code","b90d0249":"code","dfce41b9":"code","83fdc788":"code","972749b7":"code","66b48153":"code","a76d1b7b":"code","d1c3d73d":"code","f99923f0":"code","e35efece":"code","410767d2":"code","40152796":"code","3f461f47":"code","5c209758":"code","ac86c175":"code","acc5d678":"code","e40dcc79":"code","f2f2a338":"code","b0a600b9":"code","fb6cea45":"code","4f0fbb2b":"code","b3941312":"code","9b89dd22":"code","10589a51":"code","60cab1a6":"code","d4286a41":"code","2b8b4c64":"code","15d11c4e":"code","4f90e921":"code","10031558":"code","c887f358":"code","bfa65ac8":"code","6549791e":"code","4853dd0e":"code","ae3330ac":"code","e457b714":"code","c7f5adde":"code","b8f43b04":"code","1c2e22ed":"code","487e738e":"code","08932740":"code","b82af05b":"code","2f618cfb":"code","d091cfe6":"code","ddf94a4e":"code","8c1b194b":"code","4ce9521b":"markdown","503fd188":"markdown","49a9a837":"markdown","e52e3132":"markdown","691f9bb4":"markdown","feacdb42":"markdown","1e34d4bc":"markdown","022ba663":"markdown","a4ef2619":"markdown","f9f047da":"markdown","1550cfc5":"markdown","9500b902":"markdown","182a6b88":"markdown","7b8b53a2":"markdown","486ddddc":"markdown","c7def234":"markdown","1410619c":"markdown","00988267":"markdown","962ab98b":"markdown","6c06c9ff":"markdown","a4325220":"markdown","2d056035":"markdown","d3680add":"markdown","5bc72ca5":"markdown","4a753ab2":"markdown","d5ff5bcd":"markdown","08a235de":"markdown","962ec5d8":"markdown","9d156c0a":"markdown","ff275612":"markdown","d2990233":"markdown","44079450":"markdown","ae419c11":"markdown","be4c5487":"markdown","3b7296f6":"markdown","81aa10bb":"markdown","4edbae9b":"markdown","e33bae57":"markdown","680b4d76":"markdown","c19563bf":"markdown","e11e2337":"markdown","847ee291":"markdown","87906356":"markdown","2baa67ec":"markdown","d04c2b02":"markdown","e8e8ebd1":"markdown","c0583659":"markdown","448e909b":"markdown","057b1162":"markdown","1ff2f6b4":"markdown","4da6ce0d":"markdown","9c74fb33":"markdown","8e1fc9b9":"markdown","cd46d07c":"markdown","a66fff8a":"markdown","92395081":"markdown","cd317e0d":"markdown","d4f53288":"markdown","3d8f9747":"markdown","f04726aa":"markdown","6f286c62":"markdown","ae078249":"markdown","a5d488a0":"markdown","faee6690":"markdown","1f3a711a":"markdown","c367b7f3":"markdown","841f14fd":"markdown","4f7e8ed2":"markdown","004134cd":"markdown","9ddadbf7":"markdown","97d2ac73":"markdown"},"source":{"caf9ffe9":"import os\nimport warnings\nwarnings.simplefilter(action = 'ignore', category=FutureWarning)\nwarnings.filterwarnings('ignore')\ndef ignore_warn(*args, **kwargs):\n    pass\n\nwarnings.warn = ignore_warn #ignore annoying warning (from sklearn and seaborn)\n\nimport numpy as np\nimport pandas as pd\nimport pylab \nimport seaborn as sns\nsns.set(style=\"ticks\", color_codes=True, font_scale=1.5)\nfrom matplotlib import pyplot as plt\nfrom matplotlib.ticker import FormatStrFormatter\nfrom matplotlib.colors import ListedColormap\n%matplotlib inline\nimport mpl_toolkits\nfrom mpl_toolkits.mplot3d import Axes3D\nfrom graphviz import Source\nfrom IPython.display import Image\n\nfrom scipy.stats import skew, norm, probplot, boxcox, f_oneway\nfrom scipy import interp\nfrom sklearn.base import BaseEstimator, TransformerMixin, clone, ClassifierMixin\nfrom sklearn import metrics, tree\nfrom sklearn.preprocessing import LabelEncoder, label_binarize, StandardScaler, PolynomialFeatures, MinMaxScaler\n\nfrom sklearn.decomposition import PCA\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\nfrom sklearn.pipeline import Pipeline, make_pipeline\nfrom sklearn.model_selection import GridSearchCV, cross_val_score, KFold, cross_val_predict, train_test_split\nfrom sklearn.metrics import roc_auc_score, roc_curve, auc, accuracy_score\n\nfrom sklearn.linear_model import LogisticRegression\nfrom keras.models import Sequential\nfrom keras.layers import Dense\nfrom keras.wrappers.scikit_learn import KerasClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nimport xgboost as xgb\nfrom sklearn.gaussian_process import GaussianProcessClassifier\nfrom sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, ExtraTreesClassifier\nfrom sklearn.ensemble.gradient_boosting import GradientBoostingClassifier\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import SVC, LinearSVC\nfrom mlxtend.classifier import StackingClassifier\n\nfrom skater.core.local_interpretation.lime.lime_tabular import LimeTabularExplainer\nfrom skater.core.explanations import Interpretation\nfrom skater.model import InMemoryModel","80a35220":"wines = pd.read_csv('..\/input\/winequalityN.csv')\n\nwines['quality_label'] = wines.quality.apply(lambda q: 'low' if q <= 5 else 'medium' if q <= 7 else 'high')\n#wines.quality_label = pd.Categorical(wines.quality_label, categories=['low', 'medium', 'high'], ordered=True)\n\n# re-shuffle records just to randomize data points\nwines = wines.sample(frac=1, random_state=101).reset_index(drop=True)","c3723c00":"def rstr(df, pred=None): \n    obs = df.shape[0]\n    types = df.dtypes\n    counts = df.apply(lambda x: x.count())\n    uniques = df.apply(lambda x: [x.unique()])\n    nulls = df.apply(lambda x: x.isnull().sum())\n    distincts = df.apply(lambda x: x.unique().shape[0])\n    missing_ration = (df.isnull().sum()\/ obs) * 100\n    skewness = df.skew()\n    kurtosis = df.kurt() \n    print('Data shape:', df.shape)\n    \n    if pred is None:\n        cols = ['types', 'counts', 'distincts', 'nulls', 'missing ration', 'uniques', 'skewness', 'kurtosis']\n        str = pd.concat([types, counts, distincts, nulls, missing_ration, uniques, skewness, kurtosis], axis = 1)\n\n    else:\n        corr = df.corr()[pred]\n        str = pd.concat([types, counts, distincts, nulls, missing_ration, uniques, skewness, kurtosis, corr], axis = 1, sort=False)\n        corr_col = 'corr '  + pred\n        cols = ['types', 'counts', 'distincts', 'nulls', 'missing_ration', 'uniques', 'skewness', 'kurtosis', corr_col ]\n    \n    str.columns = cols\n    dtypes = str.types.value_counts()\n    print('___________________________\\nData types:\\n',str.types.value_counts())\n    print('___________________________')\n    return str","a075c0c7":"details = rstr(wines, 'quality')\ndetails.sort_values(by='missing_ration', ascending=False)","be4efb47":"wines.quality_label.value_counts()","cfcc4f6b":"wines.dropna().quality_label.value_counts()","812bfa86":"wines.dropna(inplace=True)\n\nclass DataFrameImputer(TransformerMixin):\n\n    def __init__(self):\n        \"\"\"\n        Impute missing values:\n        - Columns of dtype object are imputed with the most frequent value in column.\n        - Columns of other types are imputed with mean of column.\n        \"\"\"\n    def fit(self, X, y=None):\n\n        self.fill = pd.Series([X[c].value_counts().index[0]\n            if X[c].dtype == np.dtype('O') else X[c].mean() for c in X],\n            index=X.columns)\n\n        return self\n\n    def transform(self, X, y=None):\n        return X.fillna(self.fill)\n\nwines = DataFrameImputer().fit_transform(wines)","8303ba68":"fig = plt.figure(figsize = (18, 4))\ntitle = fig.suptitle(\"Wine Type Vs Quality\", fontsize=14)\nfig.subplots_adjust(top=0.85, wspace=0.3)\n\nax1 = fig.add_subplot(1,4, 1)\nax1.set_title(\"Red Wine\")\nax1.set_xlabel(\"Quality\")\nax1.set_ylabel(\"Frequency\") \nrw_q = wines.quality[wines.type == 'red'].value_counts()\nrw_q = (list(rw_q.index), list(rw_q.values))\nax1.set_ylim([0, 2500])\nax1.tick_params(axis='both', which='major', labelsize=8.5)\nbar1 = ax1.bar(rw_q[0], rw_q[1], color='red', edgecolor='black', linewidth=1)\n\n\nax2 = fig.add_subplot(1,4, 2)\nax2.set_title(\"White Wine\")\nax2.set_xlabel(\"Quality\")\nax2.set_ylabel(\"Frequency\") \nww_q = wines.quality[wines.type == 'white'].value_counts()\nww_q = (list(ww_q.index), list(ww_q.values))\nax2.set_ylim([0, 2500])\nax2.tick_params(axis='both', which='major', labelsize=8.5)\nbar2 = ax2.bar(ww_q[0], ww_q[1], color='white', edgecolor='black', linewidth=1)\n\nax3 = fig.add_subplot(1,4, 3)\nax3.set_title(\"Red Wine\")\nax3.set_xlabel(\"Quality Class\")\nax3.set_ylabel(\"Frequency\") \nrw_q = wines.quality_label[wines.type == 'red'].value_counts()\nrw_q = (list(rw_q.index), list(rw_q.values))\nax3.set_ylim([0, 3200])\nbar3 = ax3.bar(list(range(len(rw_q[0]))), rw_q[1], color='red', edgecolor='black', linewidth=1, tick_label =rw_q[0])\n\nax4 = fig.add_subplot(1,4, 4)\nax4.set_title(\"White Wine\")\nax4.set_xlabel(\"Quality Class\")\nax4.set_ylabel(\"Frequency\") \nww_q = wines.quality_label[wines.type == 'white'].value_counts()\nww_q = (list(ww_q.index), list(ww_q.values))\nax4.set_ylim([0, 3200])\nbar4 = ax4.bar(list(range(len(ww_q[0]))), ww_q[1], color='white', edgecolor='black', linewidth=1, tick_label =ww_q[0])","dd11c555":"subset_attributes = wines.columns\nrs = round(wines.loc[wines.type == 'red', subset_attributes].describe(),2)\nws = round(wines.loc[wines.type == 'white', subset_attributes].describe(),2)\npd.concat([rs, ws], axis=0, keys=['Red Wine Statistics', 'White Wine Statistics']).T","5344e5dd":"#subset_attributes = ['alcohol', 'volatile acidity', 'pH', 'quality']\nls = round(wines[wines['quality_label'] == 'low'][subset_attributes].describe(),2)\nms = round(wines[wines['quality_label'] == 'medium'][subset_attributes].describe(),2)\nhs = round(wines[wines['quality_label'] == 'high'][subset_attributes].describe(),2)\npd.concat([ls, ms, hs], axis=0, keys=['Low Quality Wine', 'Medium Quality Wine', 'High Quality Wine']).T","adbfd3a5":"def type_inf_stat_test(feature):\n    F, p = f_oneway(wines[wines.type == 'red'][feature], \n                    wines[wines.type == 'white'][feature])\n    if p <= 0.05:\n        msg = 'Reject'\n    else:\n        msg = 'Acept'\n    print('F Statistic: {:.2f} \\tp-value: {:.3f} \\tNull Hypothesis: {}'.format(F, p, msg))\n    \ndef quality_inf_stat_test(feature):\n    F, p = f_oneway(wines[wines.quality_label == 'low'][feature], \n                    wines[wines.quality_label == 'medium'][feature], \n                    wines[wines.quality_label == 'high'][feature])\n    if p <= 0.05:\n        msg = 'Reject'\n    else:\n        msg = 'Acept'\n    print('F Statistic: {:.2f} \\tp-value: {:.3f} \\tNull Hypothesis: {}'.format(F, p, msg))","462d0c22":"print('ANOVA test across wine samples with different types:\\n')\n# - Mean residual sugar and total sulfur dioxide content in white wine seems to be much higher than red wine.\ntype_inf_stat_test('residual sugar')\ntype_inf_stat_test('total sulfur dioxide')\n#- Mean value of sulphates, fixed acidity and volatile acidity seem to be higher in red wine as compared to white wine.\ntype_inf_stat_test('sulphates')\ntype_inf_stat_test('fixed acidity')\ntype_inf_stat_test('volatile acidity')\n#- From all numbers, we can observe that citric acid is more present in white than red wines.\ntype_inf_stat_test('citric acid')\n#- In general, white wines has half concentrations of chlorides then red wines.\ntype_inf_stat_test('chlorides')\n#- Although in Ph the difference seems small it is interesting to note that it is slightly larger in green red wines.\ntype_inf_stat_test('pH')","b90d0249":"print('ANOVA test across wine samples with different levels of quality:\\n')\n#- alcohol makes a lot of difference in quality. Although lower quality wines have the lowest standard deviation.\nquality_inf_stat_test('alcohol')\n#- The chlorides and volatile acidity are less present and presented smaller standard deviation in wines of higher quality.\nquality_inf_stat_test('chlorides')\nquality_inf_stat_test('volatile acidity')\n#- The free sulfur dioxide is higher with higher quality, but their standard deviation decrease with the increase the quality.\nquality_inf_stat_test('free sulfur dioxide')\n#- Higher quality has less fixed acidity, but the standard deviation is slightly higher in mean quality\nquality_inf_stat_test('fixed acidity')","dfce41b9":"# re-shuffle records just to randomize data points\nwines = wines.sample(frac=1, random_state=101).reset_index(drop=True)\n\nclass_tp = LabelEncoder()\ny_tp = class_tp.fit_transform(wines.type.values) # 1 - White; 0 - Red\nwines['color'] = y_tp\n\nclass_ql = {'low':0, 'medium': 1, 'high': 2}\ny_ql = wines.quality_label.map(class_ql)","83fdc788":"corr = wines.corr()\ntop_corr_cols = corr.color.sort_values(ascending=False).keys()\ntop_corr = corr.loc[top_corr_cols, top_corr_cols]\ndropSelf = np.zeros_like(top_corr)\ndropSelf[np.triu_indices_from(dropSelf)] = True\nplt.figure(figsize=(18, 10))\nsns.heatmap(top_corr, cmap=sns.diverging_palette(220, 10, as_cmap=True), annot=True, fmt=\".2f\", mask=dropSelf)\nsns.set(font_scale=1.5)\nplt.show()\ndel corr, dropSelf, top_corr","972749b7":"sns.set(font_scale=1.0)\ng = sns.pairplot(data = wines, hue='type', palette={'red': '#FF9999', 'white': '#FFE888'},plot_kws=dict(edgecolor='black', linewidth=0.5))\nfig = g.fig \nfig.subplots_adjust(top=0.96, wspace=0.2)\nt = fig.suptitle('Wine Attributes Pairwise Plots by Types', fontsize=24)","66b48153":"corr = wines.corr()\ntop_corr_cols = corr.quality.sort_values(ascending=False).keys() \ntop_corr = corr.loc[top_corr_cols, top_corr_cols]\ndropSelf = np.zeros_like(top_corr)\ndropSelf[np.triu_indices_from(dropSelf)] = True\nplt.figure(figsize=(18, 10))\nsns.heatmap(top_corr, cmap=sns.diverging_palette(220, 10, as_cmap=True), annot=True, fmt=\".2f\", mask=dropSelf)\nsns.set(font_scale=1.5)\n\nsns.set(font_scale=1.0)\ncols = wines.columns\ncols = cols.drop('quality')\ng = sns.pairplot(data = wines.loc[:, cols], hue='quality_label')\nfig = g.fig \nfig.subplots_adjust(top=0.93, wspace=0.3)\nt = fig.suptitle('Wine Attributes Pairwise Plots by Quality', fontsize=24)\n\nplt.show()\n\ndel corr, dropSelf, top_corr, g, fig, t","a76d1b7b":"f, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 4))\nf.suptitle('Wine Type - Quality - Alcohol Content', fontsize=14)\n\nsns.boxplot(x='quality', y='alcohol', hue='type', data=wines, palette={\"red\": \"#FF9999\", \"white\": \"white\"}, ax=ax1)\nax1.set_xlabel(\"Wine Quality\",size = 12,alpha=0.8)\nax1.set_ylabel(\"Wine Alcohol %\",size = 12,alpha=0.8)\n\nsns.boxplot(x='quality_label', y='alcohol', hue='type', data=wines, palette={\"red\": \"#FF9999\", \"white\": \"white\"}, ax=ax2)\nax2.set_xlabel(\"Wine Quality Class\",size = 12,alpha=0.8)\nax2.set_ylabel(\"Wine Alcohol %\",size = 12,alpha=0.8)\nplt.show()","d1c3d73d":"f, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 4))\nf.suptitle('Wine Type - Quality - Acidity', fontsize=14)\n\nsns.violinplot(x='quality', y='volatile acidity', hue='type', data=wines, split=True, inner='quart', linewidth=1.3,\n               palette={'red': '#FF9999', 'white': 'white'}, ax=ax1)\nax1.set_xlabel(\"Wine Quality\",size = 12,alpha=0.8)\nax1.set_ylabel(\"Wine Fixed Acidity\",size = 12,alpha=0.8)\n\nsns.violinplot(x='quality_label', y='volatile acidity', hue='type', data=wines, split=True, inner='quart', linewidth=1.3,\n               palette={'red': '#FF9999', 'white': 'white'}, ax=ax2)\nax2.set_xlabel(\"Wine Quality Class\",size = 12,alpha=0.8)\nax2.set_ylabel(\"Wine Fixed Acidity\",size = 12,alpha=0.8)\nplt.show()","f99923f0":"r = sns.jointplot(x='quality', y='sulphates', data=wines[wines.color == 0], # , ax = ax\n                  kind='reg', ylim=(0, 2), color='red', space=0, size=4.5, ratio=4)\nr.ax_joint.set_xticks(list(range(3,9)))\nrfig = r.fig \nrfig.subplots_adjust(top=0.9)\nt = rfig.suptitle('Red Wine Sulphates - Quality', fontsize=12)\n\nw = sns.jointplot(x='quality', y='sulphates', data=wines[wines.color == 1],\n                   kind='reg', ylim=(0, 2), color='#FFE160', space=0, size=4.5, ratio=4)\nw.ax_joint.set_xticks(list(range(3,10)))\nwfig = w.fig \nwfig.subplots_adjust(top=0.9)\nt = wfig.suptitle('White Wine Sulphates - Quality', fontsize=12)","e35efece":"g = sns.FacetGrid(wines, col='type', hue='quality_label', col_order=['red', 'white'], hue_order=['low', 'medium', 'high'],\n                  aspect=1.2, size=3.5, palette=sns.light_palette('navy', 3))\ng.map(plt.scatter, 'volatile acidity', 'alcohol', alpha=0.9, edgecolor='white', linewidth=0.5)\nfig = g.fig\nfig.subplots_adjust(top=0.8, wspace=0.3)\nfig.suptitle('Wine Type - Alcohol - Quality - Acidity', fontsize=14)\nl = g.add_legend(title='Wine Quality Class')\n\ng = sns.FacetGrid(wines, col='type', hue='quality_label', col_order=['red', 'white'], hue_order=['low', 'medium', 'high'],\n                  aspect=1.2, size=3.5, palette=sns.light_palette('green', 3))\ng.map(plt.scatter, \"volatile acidity\", \"total sulfur dioxide\", alpha=0.9, edgecolor='white', linewidth=0.5)\nfig = g.fig \nfig.subplots_adjust(top=0.8, wspace=0.3)\nfig.suptitle('Wine Type - Sulfur Dioxide - Acidity - Quality', fontsize=14)\nl = g.add_legend(title='Wine Quality Class')","410767d2":"numeric_features = list(wines.dtypes[(wines.dtypes != \"str\") & (wines.dtypes !='object')].index)\nnumeric_features.remove('color')\n\nskewed_features = wines[numeric_features].apply(lambda x : skew (x.dropna())).sort_values(ascending=False)\n\n#compute skewness\nskewness = pd.DataFrame({'Skew' :skewed_features})   \n\n# Get only higest skewed features\nskewness = skewness[abs(skewness) > 0.7]\nskewness = skewness.dropna()\nprint (\"There are {} higest skewed numerical features to box cox transform\".format(skewness.shape[0]))\n\nl_opt = {}\n\nfor feat in skewness.index:\n    wines[feat], l_opt[feat] = boxcox((wines[feat]+1))\n\nskewed_features2 = wines[skewness.index].apply(lambda x : skew (x.dropna())).sort_values(ascending=False)\n\n#compute skewness\nskewness2 = pd.DataFrame({'New Skew' :skewed_features2})   \ndisplay(pd.concat([skewness, skewness2], axis=1).sort_values(by=['Skew'], ascending=False))","40152796":"def QQ_plot(data, measure):\n    fig = plt.figure(figsize=(12,4))\n\n    #Get the fitted parameters used by the function\n    (mu, sigma) = norm.fit(data)\n\n    #Kernel Density plot\n    fig1 = fig.add_subplot(121)\n    sns.distplot(data, fit=norm)\n    fig1.set_title(measure + ' Distribution ( mu = {:.2f} and sigma = {:.2f} )'.format(mu, sigma), loc='center')\n    fig1.set_xlabel(measure)\n    fig1.set_ylabel('Frequency')\n\n    #QQ plot\n    fig2 = fig.add_subplot(122)\n    res = probplot(data, plot=fig2)\n    fig2.set_title(measure + ' Probability Plot (skewness: {:.6f} and kurtosis: {:.6f} )'.\\\n                   format(data.skew(), data.kurt()), loc='center')\n\n    plt.tight_layout()\n    plt.show()\n    \nfor feat in skewness.index:\n    QQ_plot(wines[feat], ('Boxcox1p of {}'.format(feat)))","3f461f47":"from patsy import dmatrices\nfrom statsmodels.stats.outliers_influence import variance_inflation_factor\n\ndef VRF(predict, data, y):\n   \n    scale = StandardScaler(with_std=False)\n    df = pd.DataFrame(scale.fit_transform(data), columns= cols)\n    features = \"+\".join(cols)\n    df['quality_label'] = y.values\n\n    # get y and X dataframes based on this regression:\n    y, X = dmatrices(predict + ' ~' + features, data = df, return_type='dataframe')\n\n   # Calculate VIF Factors\n    # For each X, calculate VIF and save in dataframe\n    vif = pd.DataFrame()\n    vif[\"VIF Factor\"] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]\n    vif[\"features\"] = X.columns\n\n    # Inspect VIF Factors\n    display(vif.sort_values('VIF Factor'))\n    return vif\n\ncols = wines.columns.str.replace(' ', '_')\ndf = wines.copy()\ndf.columns = cols\n# Remove the not applied to the qualty classification task\ncols = list(cols.drop(['type', 'quality_label', 'quality']))\n\nvif = VRF('quality_label', df.loc[:, cols], y_ql)","5c209758":"cols = wines.columns.str.replace(' ', '_')\ndf = wines.copy()\ndf.columns = cols\n# Remove the higest correlations and run a multiple regression\ncols = list(cols.drop(['type', 'quality_label', 'quality', 'residual_sugar', 'total_sulfur_dioxide']))\n\nvif = VRF('quality_label', df.loc[:, cols], y_ql)\n\ndel df, vif","ac86c175":"class select_fetaures(object): # BaseEstimator, TransformerMixin, \n    def __init__(self, select_cols):\n        self.select_cols_ = select_cols\n    \n    def fit(self, X, Y ):\n        pass\n\n    def transform(self, X):\n        return X.loc[:, self.select_cols_]    \n\n    def fit_transform(self, X, Y):\n        self.fit(X, Y)\n        df = self.transform(X)\n        return df    \n\n    def __getitem__(self, x):\n        return self.X[x], self.Y[x]","acc5d678":"def pca_analysis(df, y_train, feat):\n    scale = StandardScaler()\n    df = pd.DataFrame(scale.fit_transform(df), index=df.index)\n    pca_all = PCA(random_state=101, whiten=True).fit(df)\n\n    my_color=y_train\n\n    # Store results of PCA in a data frame\n    result=pd.DataFrame(pca_all.transform(df), columns=['PCA%i' % i for i in range(df.shape[1])], index=df.index)\n\n    # Plot initialisation\n    fig = plt.figure(figsize=(10,10))\n    ax = fig.add_subplot(111, projection='3d')\n    ax.scatter(result['PCA0'], result['PCA1'], result['PCA2'], c=my_color, cmap=\"Set2_r\", s=60)\n\n    # make simple, bare axis lines through space:\n    xAxisLine = ((min(result['PCA0']), max(result['PCA0'])), (0, 0), (0,0))\n    ax.plot(xAxisLine[0], xAxisLine[1], xAxisLine[2], 'r')\n    yAxisLine = ((0, 0), (min(result['PCA1']), max(result['PCA1'])), (0,0))\n    ax.plot(yAxisLine[0], yAxisLine[1], yAxisLine[2], 'r')\n    zAxisLine = ((0, 0), (0,0), (min(result['PCA2']), max(result['PCA2'])))\n    ax.plot(zAxisLine[0], zAxisLine[1], zAxisLine[2], 'r')\n\n    # label the axes\n    ax.set_xlabel(\"PC1\")\n    ax.set_ylabel(\"PC2\")\n    ax.set_zlabel(\"PC3\")\n    ax.set_title(\"PCA on the Wines dataset for \" + (feat))\n    plt.show()\n\n    X_train , X_test, y, y_test = train_test_split(df , y_train, test_size=0.3, random_state=0)\n\n    KNC = KNeighborsClassifier(algorithm = 'ball_tree', leaf_size = 12, n_neighbors = 12, p  = 1, weights = 'distance')\n    KNC = KNC.fit(X_train, y)\n    print('KNeighbors Classifier Training Accuracy: {:2.2%}'.format(accuracy_score(y, KNC.predict(X_train))))\n    y_pred = KNC.predict(X_test)\n    print('KNeighbors Classifier Test Accuracy: {:2.2%}'.format(accuracy_score(y_test, y_pred)))\n\n    print('_' * 40)\n    print('\\nAccurance on', feat, ' Prediction By Number of PCA COmponents:\\n')\n    AccPca = pd.DataFrame(columns=['Components', 'Var_ratio', 'Train_Acc', 'Test_Acc'])\n\n    for componets in np.arange(1, df.shape[1]):\n        variance_ratio = sum(pca_all.explained_variance_ratio_[:componets])*100\n        pca = PCA(n_components=componets, random_state=101, whiten=True)\n        X_train_pca = pca.fit_transform(X_train)\n        Components = X_train_pca.shape[1]\n        KNC = KNeighborsClassifier(algorithm = 'ball_tree', leaf_size = 12, n_neighbors = 12, p  = 1, weights = 'distance')\n        KNC = KNC.fit(X_train_pca, y)\n        Training_Accuracy = accuracy_score(y, KNC.predict(X_train_pca))\n        X_test_pca = pca.transform(X_test)\n        y_pred = KNC.predict(X_test_pca)\n        Test_Accuracy = accuracy_score(y_test, y_pred)\n        AccPca = AccPca.append(pd.DataFrame([(Components, variance_ratio, Training_Accuracy, Test_Accuracy)],\n                                            columns=['Components', 'Var_ratio', 'Train_Acc', 'Test_Acc']))#], axis=0)\n\n    AccPca.set_index('Components', inplace=True)\n    display(AccPca.sort_values(by='Test_Acc', ascending=False))\n\ncols = wines.columns\ncols = list(cols.drop(['type', 'quality_label', 'color']))\npca_analysis(wines.loc[:, cols], y_tp, 'Type')\n\ncols = wines.columns\ncols = list(cols.drop(['type', 'quality_label', 'quality']))\npca_analysis(wines.loc[:, cols], y_ql, 'Quality')","e40dcc79":"def LDA_analysis(df, y_train, feat):\n    X_train , X_test, y, y_test = train_test_split(df , y_train, test_size=0.3, random_state=0)\n\n    KNC = KNeighborsClassifier(algorithm = 'ball_tree', leaf_size = 12, n_neighbors = 12, p  = 1, weights = 'distance')\n    KNC = KNC.fit(X_train, y)\n    print('KNC Training Accuracy: {:2.2%}'.format(accuracy_score(y, KNC.predict(X_train))))\n    y_pred = KNC.predict(X_test)\n    print('KNC Test Accuracy: {:2.2%}'.format(accuracy_score(y_test, y_pred)))\n    print('_' * 40)\n    print('\\nApply LDA:\\n')\n    lda = LDA(n_components=2, store_covariance=True)\n    X_train_lda = lda.fit_transform(X_train, y)\n    #X_train_lda = pd.DataFrame(X_train_lda)\n\n    print('Number of features after LDA:',X_train_lda.shape[1])\n    KNC = KNeighborsClassifier(algorithm = 'ball_tree', leaf_size = 12, n_neighbors = 12, p  = 1, weights = 'distance')\n    KNCr = KNC.fit(X_train_lda, y)\n    print('LR Training Accuracy With LDA: {:2.2%}'.format(accuracy_score(y, KNC.predict(X_train_lda))))\n    X_test_lda = lda.transform(X_test)\n    y_pred = KNC.predict(X_test_lda)\n    print('LR Test Accuracy With LDA: {:2.2%}'.format(accuracy_score(y_test, y_pred)))\n\n    if X_train_lda.shape[1]==1:\n        fig = plt.figure(figsize=(20,5))\n        fig.add_subplot(121)\n        plt.scatter(X_train_lda[y==0, 0], np.zeros((len(X_train_lda[y==0, 0]),1)), color='red', alpha=0.1)\n        plt.scatter(X_train_lda[y==1, 0], np.zeros((len(X_train_lda[y==1, 0]),1)), color='blue', alpha=0.1)\n        plt.title('LDA on Training Data Set')\n        plt.xlabel('LDA')\n        fig.add_subplot(122)\n        plt.scatter(X_test_lda[y_test==0, 0], np.zeros((len(X_test_lda[y_test==0, 0]),1)), color='red', alpha=0.1)\n        plt.scatter(X_test_lda[y_test==1, 0], np.zeros((len(X_test_lda[y_test==1, 0]),1)), color='blue', alpha=0.1)\n        plt.title('LDA on Test Data Set')\n        plt.xlabel('LDA')\n    else:\n        fig = plt.figure(figsize=(20,5))\n        fig.add_subplot(121)\n        plt.scatter(X_train_lda[y==0, 0], X_train_lda[y==0, 1], color='black', alpha=0.1)\n        plt.scatter(X_train_lda[y==1, 0], X_train_lda[y==1, 1], color='yellow', alpha=0.1)\n        plt.scatter(X_train_lda[y==2, 0], X_train_lda[y==2, 1], color='red', alpha=0.1)\n        plt.title('LDA on Training Data Set')\n        plt.xlabel('LDA')\n        fig.add_subplot(122)\n        plt.scatter(X_test_lda[y_test==0, 0], X_test_lda[y_test==0, 1], color='black', alpha=0.1)\n        plt.scatter(X_test_lda[y_test==1, 0], X_test_lda[y_test==1, 1], color='yellow', alpha=0.1)\n        plt.scatter(X_test_lda[y_test==2, 0], X_test_lda[y_test==2, 1], color='red', alpha=0.1)\n        plt.title('LDA on Test Data Set')\n        plt.xlabel('LDA')\n\n    plt.show()\n    \ncols = wines.columns\ncols = list(cols.drop(['type', 'quality_label', 'color']))\nLDA_analysis(wines.loc[:, cols], y_tp, 'Type')\n\ncols = wines.columns\ncols = list(cols.drop(['type', 'quality_label', 'quality']))\nLDA_analysis(wines.loc[:, cols], y_ql, 'Quality')","f2f2a338":"def get_results(model, name, data, true_labels, target_names = ['red', 'white'], results=None, reasume=False):\n\n    if hasattr(model, 'layers'):\n        param = wtp_dnn_model.history.params\n        best = np.mean(wtp_dnn_model.history.history['val_acc'])\n        predicted_labels = model.predict_classes(data) \n        im_model = InMemoryModel(model.predict, examples=data, target_names=target_names)\n\n    else:\n        param = gs.best_params_\n        best = gs.best_score_\n        predicted_labels = model.predict(data).ravel()\n        if hasattr(model, 'predict_proba'):\n            im_model = InMemoryModel(model.predict_proba, examples=data, target_names=target_names)\n        elif hasattr(clf, 'decision_function'):\n            im_model = InMemoryModel(model.decision_function, examples=data, target_names=target_names)\n        \n    print('Mean Best Accuracy: {:2.2%}'.format(best))\n    print('-'*60)\n    print('Best Parameters:')\n    print(param)\n    print('-'*60)\n    \n    y_pred = model.predict(data).ravel()\n    \n    display_model_performance_metrics(true_labels, predicted_labels = predicted_labels, target_names = target_names)\n    if len(target_names)==2:\n        ras = roc_auc_score(y_true=true_labels, y_score=y_pred)\n    else:\n        roc_auc_multiclass, ras = roc_auc_score_multiclass(y_true=true_labels, y_score=y_pred, target_names=target_names)\n        print('\\nROC AUC Score by Classes:\\n',roc_auc_multiclass)\n        print('-'*60)\n\n    print('\\n\\n              ROC AUC Score: {:2.2%}'.format(ras))\n    prob, score_roc, roc_auc = plot_model_roc_curve(model, data, true_labels, label_encoder=None, class_names=target_names)\n    \n    interpreter = Interpretation(data, feature_names=cols)\n    plots = interpreter.feature_importance.plot_feature_importance(im_model, progressbar=False, n_jobs=1, ascending=True)\n    \n    r1 = pd.DataFrame([(prob, best, np.round(accuracy_score(true_labels, predicted_labels), 4), \n                         ras, roc_auc)], index = [name],\n                         columns = ['Prob', 'CV Accuracy', 'Accuracy', 'ROC AUC Score', 'ROC Area'])\n    if reasume:\n        results = r1\n    elif (name in results.index):        \n        results.loc[[name], :] = r1\n    else: \n        results = results.append(r1)\n        \n    return results\n\ndef roc_auc_score_multiclass(y_true, y_score, target_names, average = \"macro\"):\n\n  #creating a set of all the unique classes using the actual class list\n  unique_class = set(y_true)\n  roc_auc_dict = {}\n  mean_roc_auc = 0\n  for per_class in unique_class:\n    #creating a list of all the classes except the current class \n    other_class = [x for x in unique_class if x != per_class]\n\n    #marking the current class as 1 and all other classes as 0\n    new_y_true = [0 if x in other_class else 1 for x in y_true]\n    new_y_score = [0 if x in other_class else 1 for x in y_score]\n    num_new_y_true = sum(new_y_true)\n\n    #using the sklearn metrics method to calculate the roc_auc_score\n    roc_auc = roc_auc_score(new_y_true, new_y_score, average = average)\n    roc_auc_dict[target_names[per_class]] = np.round(roc_auc, 4)\n    mean_roc_auc += num_new_y_true * np.round(roc_auc, 4)\n    \n  mean_roc_auc = mean_roc_auc\/len(y_true)  \n  return roc_auc_dict, mean_roc_auc\n\ndef get_metrics(true_labels, predicted_labels):\n    \n    print('Accuracy:  {:2.2%} '.format(metrics.accuracy_score(true_labels, predicted_labels)))\n    print('Precision: {:2.2%} '.format(metrics.precision_score(true_labels, predicted_labels, average='weighted')))\n    print('Recall:    {:2.2%} '.format(metrics.recall_score(true_labels, predicted_labels, average='weighted')))\n    print('F1 Score:  {:2.2%} '.format(metrics.f1_score(true_labels, predicted_labels, average='weighted')))\n                        \n\ndef train_predict_model(classifier,  train_features, train_labels,  test_features, test_labels):\n    # build model    \n    classifier.fit(train_features, train_labels)\n    # predict using model\n    predictions = classifier.predict(test_features) \n    return predictions    \n\n\ndef display_confusion_matrix(true_labels, predicted_labels, target_names):\n    \n    total_classes = len(target_names)\n    level_labels = [total_classes*[0], list(range(total_classes))]\n\n    cm = metrics.confusion_matrix(y_true=true_labels, y_pred=predicted_labels)\n    cm_frame = pd.DataFrame(data=cm, \n                            columns=pd.MultiIndex(levels=[['Predicted:'], target_names], labels=level_labels), \n                            index=pd.MultiIndex(levels=[['Actual:'], target_names], labels=level_labels)) \n    print(cm_frame) \n    \ndef display_classification_report(true_labels, predicted_labels, target_names):\n\n    report = metrics.classification_report(y_true=true_labels, y_pred=predicted_labels, target_names=target_names) \n    print(report)\n    \ndef display_model_performance_metrics(true_labels, predicted_labels, target_names):\n    print('Model Performance metrics:')\n    print('-'*30)\n    get_metrics(true_labels=true_labels, predicted_labels=predicted_labels)\n    print('\\nModel Classification report:')\n    print('-'*30)\n    display_classification_report(true_labels=true_labels, predicted_labels=predicted_labels, target_names=target_names)\n    print('\\nPrediction Confusion Matrix:')\n    print('-'*30)\n    display_confusion_matrix(true_labels=true_labels, predicted_labels=predicted_labels, target_names=target_names)\n\n\ndef plot_model_roc_curve(clf, features, true_labels, label_encoder=None, class_names=None):\n    \n    ## Compute ROC curve and ROC area for each class\n    fpr = dict()\n    tpr = dict()\n    roc_auc = dict()\n    if hasattr(clf, 'classes_'):\n        class_labels = clf.classes_\n    elif label_encoder:\n        class_labels = label_encoder.classes_\n    elif class_names:\n        class_labels = class_names\n    else:\n        raise ValueError('Unable to derive prediction classes, please specify class_names!')\n    n_classes = len(class_labels)\n   \n    if n_classes == 2:\n        if hasattr(clf, 'predict_proba'):\n            prb = clf.predict_proba(features)\n            if prb.shape[1] > 1:\n                y_score = prb[:, prb.shape[1]-1] \n            else:\n                y_score = clf.predict(features).ravel()\n            prob = True\n        elif hasattr(clf, 'decision_function'):\n            y_score = clf.decision_function(features)\n            prob = False\n        else:\n            raise AttributeError(\"Estimator doesn't have a probability or confidence scoring system!\")\n        \n        fpr, tpr, _ = roc_curve(true_labels, y_score)      \n        roc_auc = auc(fpr, tpr)\n\n        plt.plot(fpr, tpr, label='ROC curve (area = {0:3.2%})'.format(roc_auc), linewidth=2.5)\n        \n    elif n_classes > 2:\n        if  hasattr(clf, 'clfs_'):\n            y_labels = label_binarize(true_labels, classes=list(range(len(class_labels))))\n        else:\n            y_labels = label_binarize(true_labels, classes=class_labels)\n        if hasattr(clf, 'predict_proba'):\n            y_score = clf.predict_proba(features)\n            prob = True\n        elif hasattr(clf, 'decision_function'):\n            y_score = clf.decision_function(features)\n            prob = False\n        else:\n            raise AttributeError(\"Estimator doesn't have a probability or confidence scoring system!\")\n            \n        for i in range(n_classes):\n            fpr[i], tpr[i], _ = roc_curve(y_labels[:, i], y_score[:, i])\n            roc_auc[i] = auc(fpr[i], tpr[i])\n\n        ## Compute micro-average ROC curve and ROC area\n        fpr[\"micro\"], tpr[\"micro\"], _ = roc_curve(y_labels.ravel(), y_score.ravel())\n        roc_auc[\"micro\"] = auc(fpr[\"micro\"], tpr[\"micro\"])\n\n        ## Compute macro-average ROC curve and ROC area\n        # First aggregate all false positive rates\n        all_fpr = np.unique(np.concatenate([fpr[i] for i in range(n_classes)]))\n        # Then interpolate all ROC curves at this points\n        mean_tpr = np.zeros_like(all_fpr)\n        for i in range(n_classes):\n            mean_tpr += interp(all_fpr, fpr[i], tpr[i])\n        # Finally average it and compute AUC\n        mean_tpr \/= n_classes\n        fpr[\"macro\"] = all_fpr\n        tpr[\"macro\"] = mean_tpr\n        roc_auc[\"macro\"] = auc(fpr[\"macro\"], tpr[\"macro\"])\n\n        ## Plot ROC curves\n        plt.figure(figsize=(6, 4))\n        plt.plot(fpr[\"micro\"], tpr[\"micro\"], label='micro-average ROC curve (area = {0:2.2%})'\n                       ''.format(roc_auc[\"micro\"]), linewidth=3)\n\n        plt.plot(fpr[\"macro\"], tpr[\"macro\"], label='macro-average ROC curve (area = {0:2.2%})'\n                       ''.format(roc_auc[\"macro\"]), linewidth=3)\n\n        for i, label in enumerate(class_names):\n            plt.plot(fpr[i], tpr[i], label='ROC curve of class {0} (area = {1:2.2%})'\n                                           ''.format(label, roc_auc[i]), linewidth=2, linestyle=':')\n        roc_auc = roc_auc[\"macro\"]   \n    else:\n        raise ValueError('Number of classes should be atleast 2 or more')\n        \n    plt.plot([0, 1], [0, 1], 'k--')\n    plt.xlim([-0.01, 1.0])\n    plt.ylim([0.0, 1.01])\n    plt.xlabel('False Positive Rate')\n    plt.ylabel('True Positive Rate')\n    plt.title('Receiver Operating Characteristic (ROC) Curve')\n    plt.legend(loc=\"lower right\")\n    plt.show()\n    \n    return prob, y_score, roc_auc","b0a600b9":"cols = wines.columns\ncols = list(cols.drop(['type', 'quality_label', 'color']))\nX_train, X_test, y_train, y_test = train_test_split(wines.loc[:, cols], y_tp, test_size=0.20, random_state=101)","fb6cea45":"clf = Pipeline([\n        ('pca', PCA(random_state = 101)),\n        ('clf', LogisticRegression(random_state=101))])  \n\n# a list of dictionaries to specify the parameters that we'd want to tune\nn_components= [10, 12]\nwhiten = [True] #, False]\nC =  [0.003, 0.009, 0.01]#, 0.1, 1.0, 10.0, 100.0, 1000.0]\ntol = [0.001, 0.0001] # [1e-06, 5e-07, 1e-05, 1e-04, 1e-03, 1e-02, 1e-01]\n\nparam_grid =\\\n    [{'clf__C': C\n     ,'clf__solver': ['liblinear', 'saga'] \n     ,'clf__penalty': ['l1', 'l2']\n     ,'clf__tol' : tol \n     ,'clf__class_weight': [None, 'balanced']\n     ,'pca__n_components' : n_components\n     ,'pca__whiten' : whiten\n},\n    {'clf__C': C\n     ,'clf__max_iter': [3, 9, 2, 7, 4]\n     ,'clf__solver': ['newton-cg', 'sag', 'lbfgs']\n     ,'clf__penalty': ['l2']\n     ,'clf__tol' : tol \n     ,'clf__class_weight': [None, 'balanced'] \n     ,'pca__n_components' : n_components\n     ,'pca__whiten' : whiten\n}]\n\ngs = GridSearchCV(estimator=clf, param_grid=param_grid, scoring='accuracy', cv=5, verbose=1, n_jobs=-1)\nLR = Pipeline([\n        #('sel', select_fetaures(select_cols=list(shadow))),\n        ('scl', StandardScaler()),\n        #('lda', LDA(store_covariance=True)),\n        ('gs', gs)\n ]) \n\nLR.fit(X_train,y_train)\n\nresults = get_results(LR, 'LogisticRegression', X_test, y_test, reasume=True)","4f0fbb2b":"# Define the scaler \nwtp_ss = StandardScaler().fit(X_train)\n\n# Scale the train set\nX_train_Ss = wtp_ss.transform(X_train)\n\n# Scale the test set\nX_test_Ss = wtp_ss.transform(X_test)\n\nwtp_dnn_model = Sequential()\nwtp_dnn_model.add(Dense(64, activation='relu', input_shape=(12,)))\nwtp_dnn_model.add(Dense(32, activation='relu'))\nwtp_dnn_model.add(Dense(16, activation='relu'))\nwtp_dnn_model.add(Dense(1, activation='sigmoid'))\n\nwtp_dnn_model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n\nepochs=40\nhistory = wtp_dnn_model.fit(X_train_Ss, y_train, epochs=epochs, batch_size=50, \n                            shuffle=True, validation_split=0.2, verbose=0)\n\nwtp_dnn_ypred = wtp_dnn_model.predict_classes(X_test_Ss)\nwtp_dnn_predictions = class_tp.inverse_transform(wtp_dnn_ypred) \n\nf, (ax1, ax2) = plt.subplots(1, 2, figsize=(20, 7))\nt = f.suptitle('Deep Neural Net Performance', fontsize=12)\nf.subplots_adjust(top=0.85, wspace=0.3)\n\nepochs = list(range(1,epochs+1))\nax1.plot(epochs, history.history['acc'], label='Train Accuracy')\nax1.plot(epochs, history.history['val_acc'], label='Validation Accuracy')\nax1.set_xticks(epochs)\nax1.set_ylabel('Accuracy Value')\nax1.set_xlabel('Epoch')\nax1.set_title('Accuracy')\nl1 = ax1.legend(loc=\"best\")\n\nax2.plot(epochs, history.history['loss'], label='Train Loss')\nax2.plot(epochs, history.history['val_loss'], label='Validation Loss')\nax2.set_xticks(epochs)\nax2.set_ylabel('Loss Value')\nax2.set_xlabel('Epoch')\nax2.set_title('Loss')\nl2 = ax2.legend(loc=\"best\")","b3941312":"results = get_results(wtp_dnn_model, 'DNN Regressor', X_test_Ss, y_test,\n                      target_names = ['red', 'white'], results = results, reasume=False)","9b89dd22":"y_pred_keras = wtp_dnn_model.predict(X_test_Ss).ravel()\nfpr_keras, tpr_keras, thresholds_keras = roc_curve(y_test, y_pred_keras)\nauc_keras = auc(fpr_keras, tpr_keras)\n\ny_pred_clf = LR.predict_proba(X_test)[:, 1]\nfpr_clf, tpr_clf, thresholds_clf = roc_curve(y_test, y_pred_clf)\nauc_clf = auc(fpr_clf, tpr_clf)\n\nfig = plt.figure(figsize=(20, 7))\nax = fig.add_subplot(121)\nax.plot([0, 1], [0, 1], 'k--')\nax.plot(fpr_keras, tpr_keras, label='Keras (area = {:.3f})'.format(auc_keras))\nax.plot(fpr_clf, tpr_clf, label='LR (area = {:.3f})'.format(auc_clf))\nax.set_xlabel('False positive rate')\nax.set_ylabel('True positive rate')\nax.set_title('ROC curve')\nax.legend(loc='best')\n\n# Zoom in view of the upper left corner.\nax2 = fig.add_subplot(122)\nax2.set_xlim(-0.01, 0.2)\nax2.set_ylim(0.7, 1.01)\nax2.plot([0, 1], [0, 1], 'k--')\nax2.plot(fpr_keras, tpr_keras, label='Keras (area = {:.3f})'.format(auc_keras))\nax2.plot(fpr_clf, tpr_clf, label='LR (area = {:.3f})'.format(auc_clf))\nax2.set_xlabel('False positive rate')\nax2.set_ylabel('True positive rate')\nax2.set_title('ROC curve (zoomed in at top left)')\nax2.legend(loc='best')\nplt.show()\n\ndisplay(results.sort_values(by='Accuracy', ascending=False))","10589a51":"wqp_class_labels = np.array(wines['quality_label'])\ntarget_names = ['low', 'medium', 'high']\n\ncols = wines.columns\ncols = list(cols.drop(['type', 'quality_label', 'quality']))\nX_train, X_test, y_train, y_test = train_test_split(wines.loc[:, cols], y_ql.values, test_size=0.20, random_state=101)","60cab1a6":"clf = Pipeline([\n        ('clf', DecisionTreeClassifier(random_state=101))])  \n\n# a list of dictionaries to specify the parameters that we'd want to tune\ncriterion = ['gini', 'entropy']\nsplitter = ['best']\nmax_depth = [8, 9, 10, 11] # [15, 20, 25]\nmin_samples_leaf = [2, 3, 5]\nclass_weight = ['balanced', None] \n\nparam_grid =\\\n    [{ 'clf__class_weight': class_weight\n      ,'clf__criterion': criterion\n      ,'clf__splitter': splitter\n      ,'clf__max_depth': max_depth\n      ,'clf__min_samples_leaf': min_samples_leaf\n}]\n\ngs = GridSearchCV(estimator=clf, param_grid=param_grid, scoring='accuracy', cv=5, verbose=1, n_jobs=-1)\nDT = Pipeline([\n        ('scl', StandardScaler()),\n        ('gs', gs)\n ]) \n\nDT.fit(X_train,y_train)\n\nresults = get_results(DT, 'DT First', X_test, y_test, target_names = target_names, reasume=True)","d4286a41":"dt = gs.best_estimator_.get_params()['clf']\ndt.fit(X_train,y_train)\n\nmax_depth=10\ngraph = Source(tree.export_graphviz(dt, out_file=None, class_names=target_names, filled=True, rounded=True, \n                                    special_characters=False, feature_names=cols, max_depth = max_depth))\npng_data = graph.pipe(format='png')\nwith open('dtree_structure.png','wb') as f:\n    f.write(png_data)\n\nImage(png_data)","2b8b4c64":"cols_clean = cols.copy()\ncols_clean.remove('total sulfur dioxide')\ncols_clean.remove('residual sugar')","15d11c4e":"clf = Pipeline([\n        ('clf', DecisionTreeClassifier(random_state=101))])  \n\n# a list of dictionaries to specify the parameters that we'd want to tune\ncriterion = ['gini', 'entropy']\nsplitter = ['best']\nmax_depth = [8, 9, 10, 11] # [15, 20, 25]\nmin_samples_leaf = [2, 3, 5]\nclass_weight = ['balanced', None] \n\nparam_grid =\\\n    [{ 'clf__class_weight': class_weight\n      ,'clf__criterion': criterion\n      ,'clf__splitter': splitter\n      ,'clf__max_depth': max_depth\n      ,'clf__min_samples_leaf': min_samples_leaf\n}]\n\ngs = GridSearchCV(estimator=clf, param_grid=param_grid, scoring='accuracy', cv=5, verbose=1, n_jobs=-1)\nDT = Pipeline([\n        ('sel', select_fetaures(select_cols=cols_clean)), \n        ('scl', StandardScaler()),\n        ('gs', gs)\n ]) \n\nDT.fit(X_train,y_train)\n\nresults = get_results(DT, 'DT Without TSD & RS', X_test, y_test,\n                      target_names = target_names, results = results, reasume=False)","4f90e921":"display(results.sort_values(by='Accuracy', ascending=False))","10031558":"clf = Pipeline([\n        ('pca', PCA(random_state = 101)),\n        ('clf', DecisionTreeClassifier(random_state=101))])  \n\n# a list of dictionaries to specify the parameters that we'd want to tune\nSEL = cols_clean\nn_components= [len(SEL)-2, len(SEL)-1, len(SEL)] \nwhiten = [True, False]\ncriterion = ['gini', 'entropy']\nsplitter = ['best']\nmax_depth = [8, 9, 10, 11, 12] # [15, 20, 25]\nmin_samples_leaf = [2, 3, 4]\nclass_weight = ['balanced', None] \n\nparam_grid =\\\n    [{ 'clf__class_weight': class_weight\n      ,'clf__criterion': criterion\n      ,'clf__splitter': splitter\n      ,'clf__max_depth': max_depth\n      ,'clf__min_samples_leaf': min_samples_leaf\n      ,'pca__n_components' : n_components\n      ,'pca__whiten' : whiten\n}]\n\ngs = GridSearchCV(estimator=clf, param_grid=param_grid, scoring='accuracy', cv=5, verbose=1, n_jobs=-1)\nDT = Pipeline([\n        ('sel', select_fetaures(select_cols=SEL)), \n        ('scl', StandardScaler()),\n        ('gs', gs)\n ]) \n\nDT.fit(X_train,y_train)\n\nresults = get_results(DT, 'DT PCA', X_test, y_test, \n                      target_names = target_names, results = results, reasume=False)\n\ndisplay(results.sort_values(by='Accuracy', ascending=False))","c887f358":"clf = Pipeline([\n        #('pca', PCA(random_state = 101)),\n        ('clf', RandomForestClassifier(random_state=101))])  \n\n# a list of dictionaries to specify the parameters that we'd want to tune\nSEL = cols\nn_components= [len(SEL)-2, len(SEL)-1, len(SEL)] \nwhiten = [True, False]\ncriterion = ['gini', 'entropy']\nclass_weight = ['balanced', None] \nn_estimators = [155, 175]  \nmax_depth  = [20, None] #, 3, 4, 5, 10] # \nmin_samples_split = [2, 3, 4]\nmin_samples_leaf = [1] #, 2 , 3]\n\nparam_grid =\\\n    [{ #'clf__class_weight': class_weight\n      'clf__criterion': criterion\n      ,'clf__n_estimators': n_estimators      \n      ,'clf__min_samples_split': min_samples_split\n      ,'clf__max_depth': max_depth\n      #,'clf__min_samples_leaf': min_samples_leaf\n      #,'pca__n_components' : n_components\n      #,'pca__whiten' : whiten\n}]\n\ngs = GridSearchCV(estimator=clf, param_grid=param_grid, scoring='accuracy', cv=5, verbose=1, n_jobs=-1)\nRF = Pipeline([\n        #('sel', select_fetaures(select_cols=SEL)), \n        ('scl', StandardScaler()),\n        ('gs', gs)\n ]) \n\nRF.fit(X_train,y_train)\n\nresults = get_results(RF, 'RF', X_test, y_test, \n                      target_names = target_names, results = results, reasume=False)","bfa65ac8":"exp = LimeTabularExplainer(X_train.as_matrix(), feature_names=cols, discretize_continuous=True, class_names=target_names)","6549791e":"print('Corret classified?', ('Yes' if y_test[0]==RF.predict(X_test.iloc[0:1, :])[0] else 'No'))\nexp.explain_instance(X_test.iloc[0, :], RF.predict_proba, top_labels=1).show_in_notebook() ","4853dd0e":"print('Corret classified?', ('Yes' if y_test[157]==RF.predict(X_test.iloc[157:158, :])[0] else 'No'))\nexp.explain_instance(X_test.iloc[157, :], RF.predict_proba, top_labels=1).show_in_notebook() ","ae3330ac":"interpreter = Interpretation(X_test, feature_names=cols)\nim_model = InMemoryModel(RF.predict_proba, examples=X_train, target_names=target_names)\n\naxes_list = interpreter.partial_dependence.plot_partial_dependence(['alcohol'], im_model, \n                                                                   grid_resolution=100, progressbar=False,\n                                                                   with_variance=True,\n                                                                   figsize = (6, 4))\naxs = axes_list[0][3:]\n[ax.set_ylim(0, 1) for ax in axs];\ndel axes_list","e457b714":"plots_list = interpreter.partial_dependence.plot_partial_dependence([('alcohol', 'volatile acidity')], \n                                                                    im_model, n_samples=1000, figsize=(12, 5),\n                                                                    grid_resolution=100, progressbar=False)\naxs = plots_list[0][3:]\n[ax.set_zlim(0, 1) for ax in axs];\ndel im_model, axs, interpreter","c7f5adde":"clf = Pipeline([\n        ('clf', xgb.XGBClassifier(base_score=0.5, colsample_bylevel=1, n_jobs=1,\n                       colsample_bytree=1, gamma=0.0001, max_delta_step=0, random_state=101, \n                       silent=True, subsample=1))])\n\nSEL = cols_clean\nn_est = [112] #112\nmax_depth = [15] \nlearning_rate = [0.1] \nreg_lambda = [0.7] \nreg_alpha= [0.05]\nbooster = ['gbtree'] #  'dart'] #,'gblinear',\nobjective = ['multi:softmax'] #, 'multi:softprob']\n\nparam_grid =\\\n    [{ \n      'clf__n_estimators': n_est\n      ,'clf__booster': booster\n      ,'clf__objective': objective\n      ,'clf__learning_rate': learning_rate\n      ,'clf__reg_lambda': reg_lambda\n      ,'clf__reg_alpha': reg_alpha\n      ,'clf__max_depth': max_depth\n}]\n\ngs = GridSearchCV(estimator=clf, param_grid=param_grid, scoring='accuracy', cv=3,\n                  verbose=1, n_jobs=3)\n\nXGBC = Pipeline([\n        #('sel', select_fetaures(select_cols=SEL)), \n        ('scl', StandardScaler()),\n        ('gs', gs)\n ]) \n\nXGBC.fit(X_train,y_train)\n\nresults = get_results(XGBC, 'XGBC', X_test, y_test, \n                      target_names = target_names, results = results, reasume=False)","b8f43b04":"clf = Pipeline([\n        #('pca', PCA(random_state = 101)),\n        ('clf', KNeighborsClassifier())])\n\n# a list of dictionaries to specify the parameters that we'd want to tune\nSEL = cols_clean\nn_components= [len(SEL)-2, len(SEL)-1, len(SEL)] \nwhiten = [True, False]\n\nparam_grid =\\\n    [{'clf__n_neighbors': [10, 11, 12, 13] \n     ,'clf__weights': ['distance'] \n     ,'clf__algorithm' : ['ball_tree'] #, 'brute', 'auto',  'kd_tree', 'brute']\n     ,'clf__leaf_size': [12, 11, 13]\n     ,'clf__p': [1] \n     #,'pca__n_components' : n_components\n     #,'pca__whiten' : whiten\n     }]\n\ngs = GridSearchCV(estimator=clf, param_grid=param_grid, scoring='accuracy', cv=5, verbose=1, n_jobs=-1)\n\nKNNC = Pipeline([\n        ('sel', select_fetaures(select_cols=SEL)),\n        ('scl', StandardScaler()),\n        ('gs', gs)\n ]) \n\nKNNC.fit(X_train,y_train)\n\nresults = get_results(KNNC, 'KNeighborsClassifier', X_test, y_test, \n                      target_names = target_names, results = results, reasume=False)","1c2e22ed":"clf = Pipeline([\n        #('pca', PCA(random_state = 101)),\n        ('clf', GradientBoostingClassifier(random_state=101))])  \n\n# a list of dictionaries to specify the parameters that we'd want to tune\n#cv=None, dual=False,  scoring=None, refit=True,  multi_class='ovr'\nSEL = cols_clean\nn_components= [len(SEL)-2, len(SEL)-1, len(SEL)] \nwhiten = [True, False]\nlearning_rate =  [1e-02] #, 5e-03, 2e-02]\nn_estimators= [400]\nmax_depth = [10]\nn_comp = [2, 3, 4, 5]\n\nparam_grid =\\\n    [{'clf__learning_rate': learning_rate\n     ,'clf__max_depth': max_depth\n     ,'clf__n_estimators' : n_estimators \n     #,'pca__n_components' : n_components\n     #,'pca__whiten' : whiten\n}]\n\ngs = GridSearchCV(estimator=clf, param_grid=param_grid, scoring='accuracy', cv=5, verbose=1, n_jobs=-1)\n\nGBC = Pipeline([\n        #('sel', select_fetaures(select_cols=SEL)),\n        ('scl', StandardScaler()),\n        ('gs', gs)\n ])  \n\nGBC.fit(X_train,y_train)\n\nresults = get_results(GBC, 'GradientBoostingClassifier', X_test, y_test, \n                      target_names = target_names, results = results, reasume=False)","487e738e":"clf = Pipeline([\n        #('pca', PCA(random_state = 101)),\n        ('clf', AdaBoostClassifier(random_state=101))])\n\n# a list of dictionaries to specify the parameters that we'd want to tune\nSEL = cols_clean\nn_components= [len(SEL)-2, len(SEL)-1, len(SEL)] \nwhiten = [True, False]\nn_comp = [2, 3, 4, 5]\n\nparam_grid =\\\n    [{'clf__learning_rate': [2e-01, 15e-02]\n     ,'clf__n_estimators': [500, 600, 700] \n     ,'clf__algorithm' : ['SAMME.R'] # 'SAMME'\n     #,'pca__n_components' : n_components\n     #,'pca__whiten' : whiten\n     #,'lda__n_components': n_comp\n     }]\n\ngs = GridSearchCV(estimator=clf, param_grid=param_grid, scoring='accuracy', cv=5, verbose=1, n_jobs=-1)\n\nADAB = Pipeline([\n        #('sel', select_fetaures(select_cols=SEL)),\n        ('scl', StandardScaler()),\n        #('lda', LDA(store_covariance=True)),\n        ('gs', gs)\n ])  \n\nADAB.fit(X_train,y_train)\n\nresults = get_results(ADAB, 'AdaBoostClassifier', X_test, y_test, \n                      target_names = target_names, results = results, reasume=False)","08932740":"clf = Pipeline([\n        #('pca', PCA(random_state = 101)),\n        ('clf', LogisticRegression(random_state=101))])  \n\n# a list of dictionaries to specify the parameters that we'd want to tune\nSEL = cols_clean\nn_components= [len(SEL)-2, len(SEL)-1, len(SEL)] \nwhiten = [True, False]\nC =  [1.0] #, 1e-06, 5e-07, 1e-05, 1e-04, 1e-03, 1e-02, 1e-01, 10.0, 100.0, 1000.0]\ntol = [1e-06] #, 5e-07, 1e-05, 1e-04, 1e-03, 1e-02, 1e-01]\n\nparam_grid =\\\n    [{'clf__C': C\n     ,'clf__solver': ['liblinear', 'saga'] \n     ,'clf__penalty': ['l1', 'l2']\n     ,'clf__tol' : tol \n     ,'clf__class_weight': ['balanced']\n     #,'pca__n_components' : n_components\n     #,'pca__whiten' : whiten\n},\n    {'clf__C': C\n     ,'clf__max_iter': [3, 9, 2, 7, 4]\n     ,'clf__solver': ['newton-cg', 'sag', 'lbfgs']\n     ,'clf__penalty': ['l2']\n     ,'clf__tol' : tol \n     ,'clf__class_weight': ['balanced'] \n     #,'pca__n_components' : n_components\n     #,'pca__whiten' : whiten\n}]\n\ngs = GridSearchCV(estimator=clf, param_grid=param_grid, scoring='accuracy', cv=5, verbose=1, n_jobs=-1)\n\nLR = Pipeline([\n        ('sel', select_fetaures(select_cols=SEL)),\n        ('scl', StandardScaler()),\n        ('gs', gs)\n ])  \n\nLR.fit(X_train,y_train)\n\nresults = get_results(LR, 'LogisticRegression', X_test, y_test, \n                      target_names = target_names, results = results, reasume=False)","b82af05b":"clf = Pipeline([\n        ('pca', PCA(random_state = 101)),\n        ('clf', LinearSVC(random_state=101, multi_class='ovr', class_weight='balanced'))])\n\n# a list of dictionaries to specify the parameters that we'd want to tune\nSEL = cols_clean\nn_components= [len(SEL)-2, len(SEL)-1, len(SEL)] \nwhiten = [True, False]\nC =  [0.06, 0.08, 0.07] #, 1.0, 10.0, 100.0, 1000.0]\ntol = [1e-06]\nmax_iter = [10, 15, 9]\n\nparam_grid =\\\n    [{'clf__loss': ['hinge']\n     ,'clf__tol': tol\n     ,'clf__C': C\n     ,'clf__penalty': ['l2']\n     ,'clf__max_iter' : max_iter\n     ,'clf__dual' : [True]\n     #,'pca__n_components' : n_components\n     #,'pca__whiten' : whiten\n     }\n    ,{'clf__loss': ['squared_hinge']\n     ,'clf__tol': tol\n     ,'clf__C': C\n     ,'clf__penalty': ['l2', 'l1']\n     ,'clf__max_iter' : max_iter\n     ,'clf__dual' : [False]\n     #,'pca__n_components' : n_components\n     #,'pca__whiten' : whiten\n     }]\n\ngs = GridSearchCV(estimator=clf, param_grid=param_grid, scoring='accuracy', cv=5, verbose=1, n_jobs=-1)\n\nLSVC = Pipeline([\n        ('sel', select_fetaures(select_cols=SEL)),\n        ('scl', StandardScaler()),\n        #('lda', LDA(n_components = 2, store_covariance=True)),\n        ('gs', gs)\n ])  \n\nLSVC.fit(X_train,y_train)\n\nresults = get_results(LSVC, 'LinearSVC', X_test, y_test, \n                      target_names = target_names, results = results, reasume=False)","2f618cfb":"results.sort_values(by='Accuracy', ascending=False)","d091cfe6":"sclf = StackingClassifier(classifiers=[RF, GBC], # , XGBC\n                          use_probas=False,\n                          average_probas=False,\n                          use_features_in_secondary=False,\n                          meta_classifier= RF)\n\nsclf.fit(X=X_train,y=y_train)\n\nresults = get_results(sclf, 'StackingClassifier', X_test, y_test, \n                      target_names = target_names, results = results, reasume=False)","ddf94a4e":"import numpy as np\nfrom sklearn.model_selection import StratifiedKFold\nfrom keras.models import Sequential\nfrom keras.layers import Dense\nfrom keras.layers import Dropout\nfrom keras.constraints import maxnorm\nfrom keras.optimizers import RMSprop\nfrom keras.optimizers import SGD, RMSprop\n\nseed = 101\nnp.random.seed(seed)\nscale = StandardScaler()\nXtrain, Xtest, y_train, y_test = train_test_split(wines.loc[:, cols_clean], wines.quality.values, test_size=0.20, random_state=101)\nX_train = scale.fit_transform(Xtrain)\nX_test = scale.transform(Xtest)\n\nkfold = StratifiedKFold(n_splits=3, shuffle=True, random_state=seed)\nK = 1\n\nfor train, test in kfold.split(X_train, y_train):\n    model = Sequential()\n    model.add(Dense(128, kernel_initializer='normal',input_dim=10, activation='relu'))\n    model.add(Dropout(0.2))\n    model.add(Dense(256, kernel_initializer='normal', activation='relu', kernel_constraint=maxnorm(3)))\n    model.add(Dropout(0.2))\n    model.add(Dense(256, kernel_initializer='normal', activation='relu', kernel_constraint=maxnorm(3)))\n    model.add(Dense(1))\n    #model.compile(optimizer='rmsprop', loss='mse', metrics=['mae'])\n    #rmsprop = RMSprop(lr=0.0001)\n    #model.compile(optimizer=rmsprop, loss='mse', metrics=['mae'])\n    sgd = SGD(lr=0.01, momentum=0.7, decay=0.01, nesterov=False)\n    model.compile(optimizer=sgd, loss='mse', metrics=['mae'])\n    model.fit(X_train[train], y_train[train], batch_size=16, epochs=30, verbose=1)\n\n    mse_value, mae_value = model.evaluate(X_train[test], y_train[test], verbose=0)\n    print('Results from Kfold', K)\n    print('-'*30)\n    print('MSE {:1.4f}'.format(mse_value))\n    print('MAE {:1.4f}'.format(mae_value))\n    K += 1","8c1b194b":"from sklearn.metrics import r2_score\n\nprint('Final Result:')\nprint('-'*30)\ny_pred = model.predict(X_test)\nprint('r2 Score:',r2_score(y_test, y_pred))","4ce9521b":"This model is terrible, especially in classifying high quality wines, but it is interesting like LR it also does not classify wines of medium and low quality as high, and may be an option to be evaluated in a staking model, but KNNC and GBC did it best by hitting more predictions of high quality wines as high.","503fd188":"After the nulls treatments, we will see below the distributions of the data in the qualities by their types, and confirming the imbalance between classes, especially with few cases in high quality.","49a9a837":"### Understanding Wine and Types\n\nWine is an alcoholic beverage made from grapes which is fermented without the addition of sugars, acids, enzymes, water, or other nutrients\n\nRed wine is made from dark red and black grapes. The color usually ranges from various shades of red, brown and violet. This is produced with whole grapes including the skin which adds to the color and flavor of red wines, giving it a rich flavor.\n\nWhite wine is made from white grapes with no skins or seeds. The color is usually straw-yellow, yellow-green, or yellow-gold. Most white wines have a light and fruity flavor as compared to richer red wines.\n### Understanding Wine Attributes and Properties \n#### Acidity \nAcids are one of the fundamental properties of wine and contribute greatly to the taste of the wine,  Acidity in food and drink tastes tart and zesty. Tasting acidity is also sometimes confused with alcohol. Wines with higher acidity feel lighter-bodied because they come across as \u201cspritzy\u201d. Reducing acids significantly might lead to wines tasting flat. If you prefer a wine that is more rich and round, you enjoy slightly less acidity.\n\n***Acidity Characteristics***\n- Tingling sensation that focuses on the front and sides of your tongue. Feels like pop rocks.\n- If you rub your tongue to the roof of your mouth it feels gravelly.\n- Your mouth feels wet, like you bit into an apple.\n- You feel like you can [gleek](https:\/\/www.youtube.com\/watch?v=BV4ZeNvXfhk).\n\n***Acid Types and Measures***\n - **fixed acidity:**  Fixed acids include tartaric, malic, citric, and succinic acids which are found in grapes (except succinic). This variable is usually expressed in $\\frac{g(tartaricacid)}{dm^3}$ in the dataset.\n - **volatile acidity:** These acids are to be distilled out from the wine before completing the production process. It is primarily constituted of acetic acid though other acids like lactic, formic and butyric acids might also be present. Excess of volatile acids are undesirable and lead to unpleasant flavor. In the US, the legal limits of volatile acidity are 1.2 g\/L for red table wine and 1.1 g\/L for white table wine. The volatile acidity is expressed in $\\frac{g(aceticacid)}{dm^3}$ in the dataset.\n - **citric acid:** This is one of the fixed acids which gives a wine its freshness. Usually most of it is consumed during the fermentation process and sometimes it is added separately to give the wine more freshness. It's usually expressed in $\\frac{g}{dm^3}$ in the dataset.\n - **pH:** Also known as the potential of hydrogen, this is a numeric scale to specify the acidity or basicity the wine. Fixed acidity contributes the most towards the pH of wines. You might know, solutions with a pH less than 7 are acidic, while solutions with a pH greater than 7 are basic. With a pH of 7, pure water is neutral. Most wines have a pH between 2.9 and 3.9 and are therefore acidic.\n[![iamgeph](https:\/\/winefolly-wpengine.netdna-ssl.com\/wp-content\/uploads\/2015\/12\/ph-of-common-drinks.png)](https:\/\/winefolly.com\/review\/understanding-acidity-in-wine\/)\n\n#### Sweetness\nHow sweet or dry (not sweet) is the wine? Our human perception of sweet starts at the tip of our tongue. Often, the very first impression of a wine is its level of sweetness. To taste sweetneww, focus your attention on the taste buds on the tip of your tongue. Are your taste buds tingling?\u2013an indicator of sweetness. Believe it or not, many dry wines can have a hint of sweetness to make them more full-bodied.\n\n***How to Taste Sweetness in Wine***\n- Tingling sensation on the tip of your tongue.\n- Slight oily sensation in the middle of your tongue that lingers.\n- Wine has a higher viscosity; wine tears on side of glass slowly. (also an indicator of high ABV)\n- Dry red wines such as cabernet sauvignon often have up to 0.9 g\/L of residual sugar (common with cheap wines).\n- A bone-dry wine can often be confused with a wine with high tannin.\n    \n***Sweetness Measure:***\n- **residual sugar:** This typically refers to the natural sugar from grapes which remains after the fermentation process stops, or is stopped. It's usually expressed in $\\frac{g}{dm^3}$ in the dataset.\n[![images](https:\/\/winefolly-wpengine.netdna-ssl.com\/wp-content\/uploads\/2015\/05\/sugar-in-wine-in-teaspoons.png)](https:\/\/winefolly.com\/review\/sugar-in-wine-chart\/)\n\n#### Salty\nSalty is not a common wine descriptor. That it\u2019s also not a positive one probably goes without saying. But the fact that wine-producing countries have (widely varying) legal maximums for sodium chloride in wine should tell you something. Salinity is a concern in dry locations when frequent irrigation increases soil salinity, which increases wine salinity. Soil composition often doesn\u2019t translate in the way you\u2019d expect into grape composition; salt is, unfortunately, an exception.\n\n***Salty Measure:***\n- **chlorides:**  Chloride concentration in the wine is influenced by terroir and its highest levels are found in wines coming from countries where irrigation is carried out using salty water or in areas with brackish terrains. This is usually a major contributor to saltiness in wine. It's usually expressed in $\\frac{g(sodiumchloride)}{dm^3}$ in the dataset.\n![imagesalt](https:\/\/www.winesandvines.com\/content\/image\/wv\/wv_2006-06-01_NaClA.jpg)\n\n#### Sulfites \nSulfites in wine are chemical compounds (sulphur dioxide, or SO2) that occur naturally, to varying degree, in all types of wine.  Sulfur Dioxide is naturally found in wines and is a byproduct of fermentation, but most winemakers choose to add a little extra to prevent the growth of undesirable yeasts and microbes, as well as to protect against oxidation.\n\nSulfur dioxide inhibits yeasts, preventing sweet wines from refermenting in the bottle. It\u2019s an antioxidant, keeping the wine fresh and untainted by oxygen. \n\nAncient cultures in Greece, Rome, and Egypt, used sulfites to sterilise their containers of wine. Because sulfites are anti-microbial, it has the ability of killing off unwanted bacterias and wild yeast during wine making.\n\nVery sensitive tasters have been noted to smell sulfur compounds in wine, although sulfur compounds are somewhat unrelated to sulfites. Sulfur compounds in wine called thiols range in flavor from citrus-like smells to cooked egg-like smells.\n\nWhat\u2019s interesting is that the warmer the wine, the more molecular sulfur it releases. This is why some wines have a nasty cooked-egg aroma when you open them. You can fix this issue by decanting your wine and chilling for about 15-30 minutes.\n\n***How Much Sulfur is in Wine?***\n- Wines with lower acidity need more sulfites than higher acidity wines. At pH 3.6 and above wines are much less stable and sulfites are necessary for shelf-life.\n- Wines with more color (i.e. red wines) tend to need less sulfites than clear wines (i.e. white wines). A typical dry white wine may have around 100 mg\/L whereas a typical dry red wine will have around 50\u201375 mg\/L.\n- Wines with higher sugar content tend to need more sulfites to prevent secondary fermentation of the remaining sugar.\n- Wines that are warmer in temperature release free sulfur compounds (the nasty sulfur smell) and can be \u201cfixed\u201d simply through decanting and chilling the wine.\n\n***Sulfites Measure:***\n- **sulphates:** These are mineral salts containing sulfur. Sulphates are to wine as gluten is to food. They are a regular part of the winemaking around the world and are considered essential. They are connected to the fermentation process and affects the wine aroma and flavor. Here, it's expressed in $\\frac{g(potassiumsulphate)}{dm^3}$ in the dataset.\n- **free sulfur dioxide:** This is the part of the sulphur dioxide that when added to a wine is said to be free after the remaining part binds. Winemakers will always try to get the highest proportion of free sulphur to bind. They are also known as sulfites and too much of it is undesirable and gives a pungent odour. This variable is expressed in $\\frac{mg}{dm^3}$ in the dataset.\n- **total sulfur dioxide:** This is the sum total of the bound and the free sulfur dioxide ($SO_2$). Here, it's expressed in $\\frac{mg}{dm^3}$. This is mainly added to kill harmful bacteria and preserve quality and freshness. There are usually legal limits for sulfur levels in wines and excess of it can even kill good yeast and give out undesirable odour.\n[![imagesuf](https:\/\/winefolly-wpengine.netdna-ssl.com\/wp-content\/uploads\/2014\/01\/sulfites-in-wine.jpg)](https:\/\/winefolly.com\/tutorial\/sulfites-in-wine\/)\n\n#### Alcohol\n\nAlcohol is formed as a result of yeast converting sugar during the fermentation process. The percentage of alcohol can vary from wine to wine. \nWe interpret alcohol using many different taste receptors which is why it can taste bitter, sweet, spicy, and oily all at once. Your genetics actually plays a role in how bitter or sweet alcohol tastes. Regardless, we can all sense alcohol towards the backs of our mouths in our throats as a warming sensation. \n\n***Alcohol Characteristics:***\n- Wines with higher alcohol tend to taste bolder and more oily\n- Wines with lower alcohol tend to taste lighter-bodied\n\n***Alcohol Measure:***\n- **alcohol:** It's usually measured in % vol or alcohol by volume (ABV).\n[![imagea](https:\/\/winefolly-wpengine.netdna-ssl.com\/wp-content\/uploads\/beans\/images\/how-much-alcohol-in-wine-beer-liquor-8b38574.gif)](https:\/\/winefolly.com\/tutorial\/alcohol-content-in-wine\/)\n\n#### Body\n\nAre you in the mood for a light, medium or full-bodied wine? Body is the result of many factors \u2013 from wine variety, where it\u2019s from, vintage, alcohol level and how it\u2019s made. Body is a snapshot of the overall impression of a wine. You can improve your skill by paying attention to where and when it\u2019s present.\n\n***Body Measure:***\n- **density:** This can be represented as a comparison of the weight of a specific volume of wine to an equivalent volume of water. It is generally used as a measure of the conversion of sugar to alcohol. Here, it's expressed in $\\frac{g}{cm^3}$.\n[![imageb](https:\/\/winefolly-wpengine.netdna-ssl.com\/wp-content\/uploads\/2015\/12\/red-wine-boldness-chart-by-wine-folly.png)](https:\/\/winefolly.com\/tutorial\/the-spectrum-of-boldness-in-red-wines-chart\/)\n\n#### Classifications Attributes:\n\n- **wine_type:** Since we originally had two datasets for red and white wine, we introduced this attribute in the final merged dataset which indicates the type of wine for each data point. A wine can either be a 'red' or a 'white' wine. One of the predictive models we will build in this chapter would be such that we can predict the type of wine by looking at other wine attributes.\n\n- **quality:** Wine experts graded the wine quality between 0 (very bad) and 10 (very excellent). The eventual quality score is the median of at least three evaluations made by the same wine experts.\n\n- **quality_label:** This is a derived attribute from the `quality` attribute. We bucket or group wine quality scores into three qualitative buckets namely low, medium and high. Wines with a quality score of 3, 4 & 5 are low quality, scores of 6 & 7 are medium quality and scores of 8 & 9 are high quality wines. We will also build another model in this chapter to predict this wine quality label based on other wine attributes. \n[![imagea](https:\/\/winefolly-wpengine.netdna-ssl.com\/wp-content\/uploads\/2016\/11\/tasting.jpg)](https:\/\/winefolly.com\/review\/how-to-taste-wine-develop-palate\/)","e52e3132":"#### Logistic Regression\n\nThis class implements regularized [logistic regression](http:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.linear_model.LogisticRegression.html) using the 'liblinear' library, 'newton-cg', 'sag' and 'lbfgs' solvers. It can handle both dense and sparse input. \n\n**Additional Parameters**\n - class_weight : dict or 'balanced', default: None\n   The \"balanced\" mode uses the values of y to automatically adjust weights inversely proportional to class frequencies in the input data as ``n_samples \/ (n_classes * np.bincount(y))``.\n\n   For how class_weight works: It penalizes mistakes in samples of class[i] with class_weight[i] instead of 1. So higher class-weight means you want to put more emphasis on a class. For example, our class 0 is 1.24 times more frequent than class 1. So you should increase the class_weight of class 1 relative to class 0, say {1: 0.6, 0: 0.4}. If the class_weight doesn't sum to 1, it will basically change the regularization parameter.\n\n   \"balanced\" basically means replicating the smaller class until you have as many samples as in the larger one, but in an implicit way.\n   \n\n - warm_start : bool, default: False. Useless for liblinear solver.\n - ``'clf__multi_class' : ['ovr', 'multinomial']`` for ``'clf__solver': ['newton-cg', 'sag', 'lbfgs']``\n\n**Attributes:**\n - coef_ : array, shape (1, n_features) or (n_classes, n_features)\n - intercept_ : array, shape (1,) or (n_classes,)\n - n_iter_ : array, shape (n_classes,) or (1, )\n\n**See also:**\n - SGDClassifier : incrementally trained logistic regression (when given the parameter ``loss=\"log\"``).\n - sklearn.svm.LinearSVC : learns SVM models using the same algorithm.\n \n See the best results below, wheres get with PCA 21 but take more time then LDA.","691f9bb4":"#### Linear Support Vector Classification\n\n[LSVC](http:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.svm.LinearSVC.html#sklearn.svm.LinearSVC) is similar to SVC with parameter kernel=\u2019linear\u2019, but implemented in terms of liblinear rather than libsvm, so it has more flexibility in the choice of penalties and loss functions and should scale better to large numbers of samples.\n\nThis class supports both dense and sparse input and the multiclass support is handled according to a one-vs-the-rest scheme.\n\nThe combination of penalty='l1' and loss='hinge' is not supported, and penalty='l2' and loss='hinge' needs dual=True.","feacdb42":"#### AdaBoost classifier\n\nIs a meta-estimator that begins by fitting a classifier on the original dataset and then fits additional copies of the classifier on the same dataset but where the weights of incorrectly classified instances are adjusted such that subsequent classifiers focus more on difficult cases.\n\nThis class implements the algorithm known as [AdaBoost-SAMME](http:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.ensemble.AdaBoostClassifier.html#sklearn.ensemble.AdaBoostClassifier).\n\n**Parameters:**\n\n - ***n_estimators***: The maximum number of estimators at which boosting is terminated. In case of perfect fit, the learning procedure is stopped early.\n\n - ***learning_rate***: Learning rate shrinks the contribution of each classifier by learning_rate. There is a trade-off between learning_rate and n_estimators.\n\n - ***algorithm***: {\u2018SAMME\u2019, \u2018SAMME.R\u2019}. If \u2018SAMME.R\u2019 then use the SAMME.R real boosting algorithm. base_estimator must support calculation of class probabilities. If \u2018SAMME\u2019 then use the SAMME discrete boosting algorithm. The SAMME.R algorithm typically converges faster than SAMME, achieving a lower test error with fewer boosting iterations.","1e34d4bc":"While there seems to be some pattern depicting a slight higer sulphate levels for higher quality rated wine samples, the correlation is quite weak. However, we do see this tendecy is caused by higer concentartion on medium quality, and a clearly see that sulphate levels for red wine are much higher as compared to the ones in white wine. \n\nLet's see this obsevation in some 4 variabels graphs analysis:","022ba663":"Each box plot above depicts the distribution of alcohol level for a particular wine quality rating separated by wine types.  We can clearly observe the wine alcohol by volume distribution has an increasing trend based on higher quality rated wine samples, and some  outliers in each quality level often depicted by individual points\n\nLet's see visualizing relationships between wine types, quality and acidity:","a4ef2619":"The results depicted above show us the features that were primarily responsible for the model to predict the wine quality as low. The values for each corresponding feature depicted here are the scaled values obtained after feature scaling. We can see that the most important feature was alcohol and the last is pH, which makes sense considering if you look the feature importances plot.  Let's interpret another prediction, this time for a wine of high quality.","f9f047da":"#### Deep Neural Network\n\nThe most simple neural network is the \"perceptron\", which, in its simplest form, consists of a single neuron. The perceptrons only work with numerical data, so, you should convert any nominal data into a numerical format.\n\nThe the perceptron has a important limitation, it could only represent linear separations between classes. To overcome this we can use the multi-layer perceptron overcomes that can be represent more complex decision boundaries.\n\nMulti-layer perceptrons are also known as \"feed-forward neural networks\". These are more complex networks as they consist of multiple neurons that are organized in layers. The number of layers is usually limited to two or three, but theoretically, there is no limit!\n\nAmong the layers, you can distinguish an input layer, hidden layers and an output layer. Multi-layer perceptrons are often fully connected. This means that there's a connection from each perceptron in a certain layer to each perceptron in the next layer. Even though the connectedness is no requirement, this is typically the case.\n\nOne of the most powerful and easy-to-use Python libraries for developing and evaluating deep learning models is [Keras](https:\/\/keras.io\/models\/sequential\/). It wraps the efficient numerical computation libraries Theano and TensorFlow. The advantage of this is mainly that you can get started with neural networks in an easy and fun way. A quick way to get started is to use the Keras Sequential model: it's a linear stack of layers. You can easily create the model by passing a list of layer instances to the constructor, which you set up by running `model = Sequential()`.\n\nSo, let's go and create our DNN classifier of type wines in  Keras with TensorFlow:","1550cfc5":"## Data Engineering - Cleaning, Transforming, Selection and Reduction\n### Box cox transformation of highly skewed features\nA Box Cox transformation is a way to transform non-normal data distribution into a normal shape. \n![image](https:\/\/i.pinimg.com\/originals\/d1\/9f\/7c\/d19f7c7f5daaed737ab2516decea9874.png)\nWhy does this matter?\n- **Model bias and spurious interactions**: If you are performing a regression or any statistical modeling, this asymmetrical behavior may lead to a bias in the model. If a factor has a significant effect on the average, because the variability is much larger, many factors will seem to have a stronger effect when the mean is larger. This is not due, however, to a true factor effect but rather to an increased amount of variability that affects all factor effect estimates when the mean gets larger. This will probably generate spurious interactions due to a non-constant variation, resulting in a **very complex model** with many **spurious** and **unrealistic** interactions.\n- **Normality is an important assumption for many statistical techniques**: such as individuals control charts, Cp\/Cpk analysis, t-tests and analysis of variance (ANOVA). A substantial departure from normality will bias your capability estimates.\n\nOne solution to this is to transform your data into normality using a [Box-Cox transformation](https:\/\/docs.scipy.org\/doc\/scipy\/reference\/generated\/scipy.stats.boxcox.html) means that you are able to run a broader number of tests.\n\nAt the core of the Box Cox transformation is an exponent, lambda (\u03bb), which varies from -5 to 5. All values of \u03bb are considered and the optimal value for your data is selected; The 'optimal value' is the one which results in the best approximation of a normal distribution curve. The transformation of Y has the form:\n<a href=\"http:\/\/vitarts3.hospedagemdesites.ws\/wp-content\/uploads\/2018\/09\/boxcox.png\"><img src=\"http:\/\/vitarts3.hospedagemdesites.ws\/wp-content\/uploads\/2018\/09\/boxcox.png\" alt=\"\" width=\"222\" height=\"70\" class=\"aligncenter size-full wp-image-13940\" \/><\/a>\n\nThe scipy implementation proceeded with this formula, then you need before take care of negatives values if you have. A common technique for handling negative values is to add a constant value to the data prior to applying the log transform. The transformation is therefore log(Y+a) where a is the constant. Some people like to choose a so that min(Y+a) is a very small positive number (like 0.001). Others choose a so that min(Y+a) = 1. For the latter choice, you can show that a = b \u2013 min(Y), where b is either a small number or is 1.\nThis test only works for positive data. However, Box and Cox did propose a second formula that can be used for negative y-values, not implemented in scipy:\n<a href=\"http:\/\/vitarts3.hospedagemdesites.ws\/wp-content\/uploads\/2018\/09\/boxcoxNeg.png\"><img src=\"http:\/\/vitarts3.hospedagemdesites.ws\/wp-content\/uploads\/2018\/09\/boxcoxNeg.png\" alt=\"\" width=\"272\" height=\"76\" class=\"aligncenter size-full wp-image-13941\" \/><\/a>\nThe formula are deceptively simple. Testing all possible values by hand is unnecessarily labor intensive.\n\n<p align='center'> Common Box-Cox Transformations \n<\/p>\n\n| Lambda value (\u03bb) | Transformed data (Y') |\n|------------------|-----------------------|\n|        -3\t       | Y\\*\\*-3 = 1\/Y\\*\\*3    |\n|        -2        | Y\\*\\*-2 = 1\/Y\\*\\*2    |\n|        -1        | Y\\*\\*-1 = 1\/Y         |\n|       -0.5       | Y\\*\\*-0.5 = 1\/(\u221a(Y))  |\n|         0        | log(Y)(\\*)            |\n|        0.5       | Y0.5 = \u221a(Y)           |\n|         1        | Y\\*\\*1 = Y            |\n|         2        | Y\\*\\*2                |\n|         3        | Y\\*\\*3                |\n\n(\\*)Note: the transformation for zero is log(0), otherwise all data would transform to Y\\*\\*0 = 1.\nThe transformation doesn't always work well, so make sure you check your data after the transformation with a normal probability plot or if the skew are reduced, tending to zero.","9500b902":"With this results, it is interesting try a staked model with RF, GBC and XGB, for meta-classier we can use RF or GBC. Other option can include the KNeighbors and use of LR, LinearSVC or KNeighbors as meta-classier. Let's see one of then to can realize how to make that.","182a6b88":"From the first pair graphs we can see that hHigher quality wine samples have lower levels of volatile acidity and higher levels of alcohol content as compared to wine samples with medium and low ratings. Besides this, we can also see that volatile acidity levels are slightly lower in white wine samples as compared to red wine samples.\n\nIn the second pair graph, the volatile acidity as well as total sulfur dioxide is considerably lower in high quality wine samples. Also, total sulfur dioxide is considerable more in white wine samples as compared to red wine samples. However, volatile acidity levels are slightly lower in white wine samples as compared to red wine samples we also observed in the previous plot.","7b8b53a2":"As you can see, we were able at first to bring most the numerical values closer to normal. Maybe you're not satisfied with the results and want to understand if we really need to continue to transform some discrete data. So, let's take a look at the QQ test of these features.","486ddddc":"As we can see from the graphs above:\n- From the first plot seems to be a strong dependency on low wine quality class prediction with the corresponding decrease in alcohol and the increase in volatile acidity levels. \n- The plot medium wine quality class predictions plot shows that we having a strong dependency with corresponding increase in alcohol and with decrease in volatile acidity levels. \n- In the last plot, note that while some dependency is there for high wine quality class prediction with the increase in alcohol and corresponding decrease in volatile acidity is it quite weak. ","c7def234":"As you can see we improved the results with our stacked model, although small if compared to what we had already obtained in other ensemble models, it was prove the value of staked methods.\n\nIn addition, as you can see we did not lose the ability to evaluate the ROC curve, importance of the characteristics or even to apply the methods of interpretation of the model.","1410619c":"Again it is in alignment with the insights we obtained earlier at ANOVA test and correlations\n- Lower sulphate levels in wines with high quality ratings, but the higest are found in the medium quality\n- Lower levels of volatile acids in wines with high quality ratings\n\nLet's see the relation of wine quality and their acohol:","00988267":"# Wine Type and Quality Classification    \n[![image](http:\/\/www.vinhoverde.pt\/templates\/images\/logoen.PNG)](http:\/\/www.vinhoverde.pt\/en\/)\n\n___Introduction___\n\nWine is an alcoholic beverage made from fermented grapes. Yeast consumes the sugar in the grapes and converts it to ethanol, carbon dioxide, and heat. It is a pleasant tasting alcoholic beverage, loved cellebrated . It will definitely be interesting to analyze the physicochemical attributes of wine and understand their relationships and significance with wine quality and types classifications. To do this, We will proceed according to the standard Machine Learning and data mining workflow models like the CRISP-DM model, mainly for:\n- Predict if each wine sample is a red or white wine.\n- Predict the quality of each wine sample, which can be low, medium, or high.\n\nThe dataset are related to red and white variants of the \"Vinho Verde\" wine. Vinho verde is a unique product from the Minho (northwest) region of Portugal. Medium in alcohol, is it particularly appreciated due to its freshness (specially in the summer). This dataset is public available for research purposes only, for more information, read [Cortez et al., 2009](http:\/\/www3.dsi.uminho.pt\/pcortez\/wine5.pdf). . Due to privacy and logistic issues, only physicochemical (inputs) and sensory (the output) variables are available (e.g. there is no data about grape types, wine brand, wine selling price, etc.). \n\n___Attribute Information:___\n \nInput variables (based on physicochemical tests): \n1 - fixed acidity \n2 - volatile acidity \n3 - citric acid \n4 - residual sugar \n5 - chlorides \n6 - free sulfur dioxide \n7 - total sulfur dioxide \n8 - density \n9 - pH \n10 - sulphates \n11 - alcohol \nOutput variable (based on sensory data): \n12 - quality (score between 0 and 10)\n\n___[UCI](https:\/\/archive.ics.uci.edu\/ml\/datasets\/wine+quality) Notes About the Dataset:___\n- The classes are ordered and not balanced (e.g. there are munch more normal wines than excellent or poor ones). \n- Outlier detection algorithms could be used to detect the few excellent or poor wines. \n- Also, we are not sure if all input variables are relevant. So it could be interesting to test feature selection methods. ","962ab98b":"As you can see, we didn't get a wonderful results like in the wine type classification. Special looking at the class based statistics, we can see the recall for the high quality wine samples is pretty bad since a lot of them have been misclassified into medium and low quality ratings. This is kind of expected since we do not have a lot of training samples for high quality wine if you remember our training sample sizes from earlier. If you run this model by setting class_weight to balanced, you will be able to raise the recall and f1-scroe of the high class, but you will have a significant loss in the other performance metrics.\n\nNow look at the feature importance, Alcohol, volatile acidity and sulphates occupy the top tree ranks to classify quality, unlike the previous ones for type classification, which has density, total sulfur dioxide and chlorides as top tree.\n\nThe main advantage of decision tree based models is model interpretability, since it is quite easy to understand and interpret the decision rules which led to a specific model prediction. Trees can be even visualized to understand and interpret decision rules better. For better understand, I rerun the decision tree with the same parameters, but without scaling the features and include the following code helps us visualize decision trees:","6c06c9ff":"## Bonus Task:\nJust for fun, let's create a DNN on Keras to try predict the quality wine as a regressor. is just a sketch, didn't have good performance, but you can try to optimize it and change its configuration from there.","a4325220":"This model didn't gave a good performance and shows to much loss in the high quality classification. Let's move on.","2d056035":"This model did not present a good result and made one mistake in predict a high quality as low. However observing its ROC curve it is apparently more stable and may be a good candidate for meta classifier model, base of a staking model","d3680add":"### Make Staked Classifier\nCreate an ensemble model by staking models with [StackingClassifier](https:\/\/rasbt.github.io\/mlxtend\/user_guide\/classifier\/StackingClassifier\/#example-2-using-probabilities-as-meta-features) of mlxtend.classifier\n\nStacking is an ensemble learning technique to combine multiple classification models via a meta-classifier. The individual classification models are trained based on the complete training set; then, the meta-classifier is fitted based on the outputs -- meta-features -- of the individual classification models in the ensemble. The meta-classifier can either be trained on the predicted class labels or probabilities from the ensemble.\n\n![image](https:\/\/rasbt.github.io\/mlxtend\/user_guide\/classifier\/StackingClassifier_files\/stackingclassification_overview.png)","5bc72ca5":"As you can see, in addition to having presented significant improvements, they also pass the QQ test and present interesting distributions as we can observe in their respective graphs.\n\nSo, we can continue to apply the BoxCox on this features.\n\n### Identify  and treat multicollinearity:\n**Multicollinearity** is more troublesome to detect because it emerges when three or more variables, which are highly correlated, are included within a model, leading to unreliable and unstable estimates of regression coefficients. To make matters worst multicollinearity can emerge even when isolated pairs of variables are not collinear.\n\nTo identify, we need start with the coefficient of determination, r<sup>2<\/sup>, is the square of the Pearson correlation coefficient r. The coefficient of determination, with respect to correlation, is the proportion of the variance that is shared by both variables. It gives a measure of the amount of variation that can be explained by the model (the correlation is the model). It is sometimes expressed as a percentage (e.g., 36% instead of 0.36) when we discuss the proportion of variance explained by the correlation. However, you should not write r<sup>2<\/sup> = 36%, or any other percentage. You should write it as a proportion (e.g., r<sup>2<\/sup> = 0.36).\n![image](https:\/\/encrypted-tbn0.gstatic.com\/images?q=tbn:ANd9GcQTS_TVxaBpLmAGthSUAS9w7SVKsmLOtocz7ts-MXioJwa-Se0U)\n\nAlready the **Variance Inflation Factor** (**VIF**) is a measure of collinearity among predictor variables within a multiple regression.  It is may be calculated for each predictor by doing a linear regression of that predictor on all the other predictors, and then obtaining the R<sup>2<\/sup> from that regression.  It is calculated by taking the the ratio of the variance of all a given model's betas divide by the variance of a single beta if it were fit alone [1\/(1-R<sup>2<\/sup>)]. Thus, a VIF of 1.8 tells us that the variance (the square of the standard error) of a particular coefficient is 80% larger than it would be if that predictor was completely uncorrelated with all the other predictors. The VIF has a lower bound of 1 but no upper bound. Authorities differ on how high the VIF has to be to constitute a problem (e.g.: 2.50 (R<sup>2<\/sup> equal to 0.6), sometimes 5 (R<sup>2<\/sup> equal to .8), or greater than 10 (R<sup>2<\/sup> equal to 0.9) and so on). \n\nBut there are several situations in which multicollinearity can be safely ignored:\n\n - ***Interaction terms*** and ***higher-order terms*** (e.g., ***squared*** and ***cubed predictors***) ***are correlated*** with main effect terms because they include the main effects terms. **Ops!** Sometimes we use ***polynomials*** to solve problems, **indeed!** But keep calm, in these cases,  **standardizing** the predictors can **removed the multicollinearity**. \n - ***Indicator***, like ***dummy*** or ***one-hot-encode***, that represent a ***categorical variable with three or more categories***. If the proportion of cases in the reference category is small, the indicator will necessarily have high VIF's, even if the categorical is not associated with other variables in the regression model. But, you need check if some dummy is collinear or has multicollinearity with other features outside of their dummies.\n - ***Control feature** if the ***feature of interest*** **do not have high VIF's**. Here's the thing about multicollinearity: it's only a problem for the features that are **collinear**. It increases the standard errors of their coefficients, and it may make those coefficients unstable in several ways. But so long as the collinear feature are only used as control feature, and they are not collinear with your feature of interest, there's no problem. The coefficients of the features of interest are not affected, and the performance of the control feature as controls is not impaired.\n\nSo, generally, we could run the same model twice, once with severe multicollinearity and once with moderate multicollinearity. This provides a great head-to-head comparison and it reveals the classic effects of multicollinearity. However, when standardizing your predictors doesn't work, you can try other solutions such as:\n- removing highly correlated predictors\n- linearly combining predictors, such as adding them together\n- running entirely different analyses, such as partial least squares regression or principal components analysis\n\nWhen considering a solution, keep in mind that all remedies have potential drawbacks. If you can live with less precise coefficient estimates, or a model that has a high R-squared but few significant predictors, doing nothing can be the correct decision because it won't impact the fit.\n\nGiven the potential for correlation among the predictors, we'll have display the variance inflation factors (VIF), which indicate the extent to which multicollinearity is present in a regression analysis. Hence such variables need to be removed from the model. Deleting one variable at a time and then again checking the VIF for the model is the best way to do this.\n\nSo, I start the analysis already having removed the features with he highest collinearities and run VIF.","4a753ab2":"#### Check for correlations based on wines quality labels\nNow, I will make a zoom in these features in order of their correlation with wine quality labels:","d5ff5bcd":"## Preparing environment and uploading data\nYou can download the this python notebook and data from my [github repository](https:\/\/github.com\/mgmarques\/Studies-on-Kaggle). The data can download on Kaggle [here](https:\/\/www.kaggle.com\/rajyellow46\/wine-quality\/downloads\/wine-quality.zip\/1).\n\n### Import Packages","08a235de":"#### Gradient Boosting for Classification\n\n[GB](http:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.ensemble.GradientBoostingClassifier.html) builds an additive model in a forward stage-wise fashion; it allows for the optimization of arbitrary differentiable loss functions. In each stage n_classes_ regression trees are fit on the negative gradient of the binomial or multinomial deviance loss function. Binary classification is a special case where only a single regression tree is induced.\n\n- loss: loss function to be optimized. 'deviance' refers to deviance (= logistic regression) for classification with probabilistic outputs. For loss 'exponential' gradient boosting recovers the AdaBoost algorithm.","962ec5d8":"From the plots above, we can see that with an increase in the quantity of alcohol content, the confidence\\probability of the model predictor increases in predicting the wine to be either medium or high and similarly it decreases for the probability of wine to be of low quality. This shows there is definitely some relationship between the class predictions with the alcohol content and again the influence of alcohol for predictions of class high is pretty low, which is expected considering training samples for high quality wine are less. Let's now plot two-way partial dependence plots for interpreting our random forest predictor's dependence on alcohol and volatile acidity, the top two influential features.","9d156c0a":"From the interpretation in image above, we can see that to classify a high quality wines is not too easy as how are you looking to classify as low or medium, as to our previous case. In this record the features responsible for the model correctly predicting the wine quality as high was residual sugar. 8<sup>th<\/sup> feature in importance followed by volatile acidity, second important feature, with a few support of other features (chlorides, free and total sulfur dioxide, citric and fixed acid). Note that for this prediction, alcohol and density as strong as residual sugar and volatile acidity, but with only the few support of pH, this is fortunately insufficient to misclassified it as a medium.\n\nAlso you can notice the different rules from the a stark difference in the scaled values of features like alcohol, volatile acidity, density, chlorides for the two instances depicted above.\n\nBecause it is very difficult to visualize high dimensional feature spaces, typically one or two influential and important features are used to visualize partial dependence plots. In general, partial dependence helps describe the marginal impact or influence of a feature on the model prediction decision by holding the other features constant. The following code depicts one-way partial dependence plots for our model prediction function based on the most important feature, alcohol:","ff275612":"## Modeling\nWe start to looking at different approaches to implement classifiers models, and use hyper parametrization, cross validation and compare the results between different errors measures.\n\nFirst we will make some support functions help us evaluate ours models through a standard. Next, we proceed with the classifications of wines types and next of wines quality labels. We will finalize our quality classification model with a staking approach.\n\n### Simplify Get Results\nLet's build a function to standardize the capture and exposure of the results of our models, that stander the reports to:\n- The mean best accuracy and best parameters got through the grid  search CV \n- The model performance metrics Accuracy, Precision, Recall and F1 Score\n- The model Classification scores by classes \n- The prediction confusion matrix\n- The ROC AUC Score and the ROC Curve\n- Features importance plot\n\nFor the multiclassification cases, like wine quality, you would need to binarize the output to correct claculate the ROC AUC score and ROC Area. Once this operation is executed, you can plot one ROC curve per class label. Besides this, you can also follow two aggregation metrics for computing the average ROC measures.  Micro-averaging involves plotting an ROC curve over the entire prediction space by considering each predicted element as a binary prediction. Macro-averaging involves giving equal weight to each class label when averaging. ","d2990233":"### Check for any correlations between features\n![image](http:\/\/flowingdata.com\/wp-content\/uploads\/2011\/07\/Cancer-causes-cell-phones-625x203.png)\nTo quantify the linear relationship between the features, I will now create a correlation matrix. \n \nThe correlation matrix is identical to a covariance matrix computed from standardized data. The correlation matrix is a square matrix that contains the Pearson product-moment correlation coefficients (often abbreviated as [Pearson's r](https:\/\/en.wikipedia.org\/wiki\/Pearson_correlation_coefficient)), which measure the linear dependence between pairs of features:\n![image](https:\/\/wikimedia.org\/api\/rest_v1\/media\/math\/render\/svg\/602e9087d7a3c4de443b86c734d7434ae12890bc)\nPearson's correlation coefficient can simply be calculated as the covariance between two features x and y (numerator) divided by the product of their standard deviations (denominator):\n![image](https:\/\/wikimedia.org\/api\/rest_v1\/media\/math\/render\/svg\/f76ccfa7c2ed7f5b085115086107bbe25d329cec)\nThe covariance between standardized features is in fact equal to their linear correlation coefficient.\nUse NumPy's corrcoef and seaborn's heatmap functions to plot the correlation matrix array as a heat map.\n\nTo fit a linear regression model, we are interested in those features that have a high correlation with our target variable. So, let's prepare the dataset and the type and quality label targets variables. ","44079450":"Again it is in alignment with the insights we obtained earlier at ANOVA test and correlations\n- Lower sulphate levels in wines with high quality ratings, but the higest are found in the medium quality\n- Lower levels of volatile acids in wines with high quality ratings\n- The higest correlation, 0,72, is see between total and free sulfur dioxide, which is expected, but the two measures have low correlation and in opposite directions with the quality,. So, we will treat this if necessary when we check the multicollinearity.","ae419c11":"Let's remove the residual suagar and total sulfur dioxide","be4c5487":"We have achieved our goal by getting good models were able to create classification models for both the type and the quality of the wine going through all phases in a standard Machine Learning and data mining workflow models like the CRISP-DM model,\n\nWe obtained good insights in the EDA phase, making use even of tests of validation of our hypotheses, as well as we identify and treat problems of collinearity and multicollinearity.\n\nIn the data engineering and treatment, we identify, transform and validate the transformation of skewed features, handle nulls, provide a function to select features into the pipeline and, have success to use dimensionality reduction to construct visualizations capable of showing us how much we could separate the data and how complex it would be with overlaps,\n\nIn the had success in apply a standard to evaluate ours models running into a grid search cross validation, and apply more modern methods to interpret complex models.\n\nIn the first classification challenge, a simpler model were able to obtain an excellent result and with a low cost of processing, with use only those treatments done in the previous stages of modeling.\n\nIn the classification of quality in low, medium and high, we saw that simple models would not be able to capture the nuances of variation in the data, especially of high quality wines with so few records in comparison to the other classes. In the other hand, we get good results in the three major families of ensemble. The ensemble was expected better generalize , be more robust as we could confirm by their ROC curves, and make superior predictions as compared to each individual base model. \n\nWe conclude that it is possible to predict the quality of a wine and its type from the physicalchemicals attributes. \nAlthough the prediction of the type has only educational foundation, the prediction of quality presents some practical applicabilities, like:\n- A wine store or or large distributor can qualify new wines, even before acquiring them, and thus better evaluate their purchase cost and their opportunity to sell.\n- The result of the model, its interpretation and all the evaluations of EDA provide methods and rules of decision that can help winemakers to look for wines of better qualities.\n\nHowever, our model has a gap of about 13%. Part of this can be explained by the fact that the quality notes are in fact notes singled out by an expert who take into account other attributes, which are not the physicalchemical ones, and therefore are not present in this data set. The statement of the note is a sensory evaluation, based on the tasting process, just to name a few we would have:\n- Others relevant physicals features not present like opacity and viscosity \n- As we have seen, some of the characteristics that we have can influence the smell and taste of the wine, but certainly we do not have all the necessary data, for example the grape variety or type.\n- We don't have the tannin, one of the basic wine characteristics. In wine it is the presence of phenolic compounds that add bitterness to a wine.\n- The wines traditionally produced in the Verde Region of Portugal are young wines that have high acidity and marked freshness. So the guard time is not so relevant for the classification of this wine, but the harvest is, as in other wines too.\n\nFinally, wine is also an emotional drink, so the notes are also affected by this, even though the average of the last three professionals.\n\nSo, cheers!\n![image](https:\/\/i.gifer.com\/ERng.gif)\n\n## Next steps\nFor quality we can:\n- Make some different engineering features and polynomials transformations.\n- Try a regression model to predict the quality as a number.\n- Try a quality classification in the all range of quality.\n- Try a tensorflow, but remember, in general, because you don't have a ton of data, the worse overfitting can and will be. That's why you should use a small network.","3b7296f6":"### Descriptive Statistics\n#### By Type","81aa10bb":"You remember the collinearity observed between total sulfur dioxide, 5<sup>th<\/sup> most important, and free sulfur dioxide, 4<sup>th<\/sup> most important, and between total sulfur dioxide and type, without significant importance?  Respectively 0.72 and 0.70. You remember that our multicollinearity test suggest that we need droop the residual sugar and total sulfur dioxide. Let's do tat to see how this affects our model, so the following code will help us to include a selection of features in our pipeline","4edbae9b":"## Exploratory Data Analysis (EDA)\n### Take a First Look of our Data:\nI created the function below to simplify the analysis of general characteristics of the data. Inspired on the `str` function of R, this function returns the types, counts, distinct, count nulls, missing ratio and uniques values of each field\/feature.\n\nIf the study involve some supervised learning, this function can return the study of the correlation, for this we just need provide the dependent variable to the `pred` parameter.\n\nAlso, if your return is stored in a variable you can evaluate it in more detail, specific of a field, or sort them from different perspectives","e33bae57":"<h1>Table of Contents<span class=\"tocSkip\"><\/span><\/h1>\n<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Preparing-environment-and-uploading-data\" data-toc-modified-id=\"Preparing-environment-and-uploading-data-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;<\/span>Preparing environment and uploading data<\/a><\/span><ul class=\"toc-item\"><li><span><a href=\"#Import-Packages\" data-toc-modified-id=\"Import-Packages-1.1\"><span class=\"toc-item-num\">1.1&nbsp;&nbsp;<\/span>Import Packages<\/a><\/span><\/li><li><span><a href=\"#Load-Dataset\" data-toc-modified-id=\"Load-Dataset-1.2\"><span class=\"toc-item-num\">1.2&nbsp;&nbsp;<\/span>Load Dataset<\/a><\/span><\/li><\/ul><\/li><li><span><a href=\"#Exploratory-Data-Analysis-(EDA)\" data-toc-modified-id=\"Exploratory-Data-Analysis-(EDA)-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;<\/span>Exploratory Data Analysis (EDA)<\/a><\/span><ul class=\"toc-item\"><li><span><a href=\"#Take-a-First-Look-of-our-Data:\" data-toc-modified-id=\"Take-a-First-Look-of-our-Data:-2.1\"><span class=\"toc-item-num\">2.1&nbsp;&nbsp;<\/span>Take a First Look of our Data:<\/a><\/span><\/li><li><span><a href=\"#Nulls-Check-and-Cleaning\" data-toc-modified-id=\"Nulls-Check-and-Cleaning-2.2\"><span class=\"toc-item-num\">2.2&nbsp;&nbsp;<\/span>Nulls Check and Cleaning<\/a><\/span><\/li><li><span><a href=\"#Understanding-Wine-and-Types\" data-toc-modified-id=\"Understanding-Wine-and-Types-2.3\"><span class=\"toc-item-num\">2.3&nbsp;&nbsp;<\/span>Understanding Wine and Types<\/a><\/span><\/li><li><span><a href=\"#Understanding-Wine-Attributes-and-Properties\" data-toc-modified-id=\"Understanding-Wine-Attributes-and-Properties-2.4\"><span class=\"toc-item-num\">2.4&nbsp;&nbsp;<\/span>Understanding Wine Attributes and Properties<\/a><\/span><ul class=\"toc-item\"><li><span><a href=\"#Acidity\" data-toc-modified-id=\"Acidity-2.4.1\"><span class=\"toc-item-num\">2.4.1&nbsp;&nbsp;<\/span>Acidity<\/a><\/span><\/li><li><span><a href=\"#Sweetness\" data-toc-modified-id=\"Sweetness-2.4.2\"><span class=\"toc-item-num\">2.4.2&nbsp;&nbsp;<\/span>Sweetness<\/a><\/span><\/li><li><span><a href=\"#Salty\" data-toc-modified-id=\"Salty-2.4.3\"><span class=\"toc-item-num\">2.4.3&nbsp;&nbsp;<\/span>Salty<\/a><\/span><\/li><li><span><a href=\"#Sulfites\" data-toc-modified-id=\"Sulfites-2.4.4\"><span class=\"toc-item-num\">2.4.4&nbsp;&nbsp;<\/span>Sulfites<\/a><\/span><\/li><li><span><a href=\"#Alcohol\" data-toc-modified-id=\"Alcohol-2.4.5\"><span class=\"toc-item-num\">2.4.5&nbsp;&nbsp;<\/span>Alcohol<\/a><\/span><\/li><li><span><a href=\"#Body\" data-toc-modified-id=\"Body-2.4.6\"><span class=\"toc-item-num\">2.4.6&nbsp;&nbsp;<\/span>Body<\/a><\/span><\/li><li><span><a href=\"#Classifications-Attributes:\" data-toc-modified-id=\"Classifications-Attributes:-2.4.7\"><span class=\"toc-item-num\">2.4.7&nbsp;&nbsp;<\/span>Classifications Attributes:<\/a><\/span><\/li><\/ul><\/li><li><span><a href=\"#Descriptive-Statistics\" data-toc-modified-id=\"Descriptive-Statistics-2.5\"><span class=\"toc-item-num\">2.5&nbsp;&nbsp;<\/span>Descriptive Statistics<\/a><\/span><ul class=\"toc-item\"><li><span><a href=\"#By-Type\" data-toc-modified-id=\"By-Type-2.5.1\"><span class=\"toc-item-num\">2.5.1&nbsp;&nbsp;<\/span>By Type<\/a><\/span><\/li><li><span><a href=\"#By-Quality\" data-toc-modified-id=\"By-Quality-2.5.2\"><span class=\"toc-item-num\">2.5.2&nbsp;&nbsp;<\/span>By Quality<\/a><\/span><\/li><\/ul><\/li><li><span><a href=\"#Inferential-Statistics\" data-toc-modified-id=\"Inferential-Statistics-2.6\"><span class=\"toc-item-num\">2.6&nbsp;&nbsp;<\/span>Inferential Statistics<\/a><\/span><\/li><li><span><a href=\"#Check-for-any-correlations-between-features\" data-toc-modified-id=\"Check-for-any-correlations-between-features-2.7\"><span class=\"toc-item-num\">2.7&nbsp;&nbsp;<\/span>Check for any correlations between features<\/a><\/span><ul class=\"toc-item\"><li><span><a href=\"#Check-for-correlations-based-on-wines-types\" data-toc-modified-id=\"Check-for-correlations-based-on-wines-types-2.7.1\"><span class=\"toc-item-num\">2.7.1&nbsp;&nbsp;<\/span>Check for correlations based on wines types<\/a><\/span><\/li><li><span><a href=\"#Check-for-correlations-based-on-wines-quality-labels\" data-toc-modified-id=\"Check-for-correlations-based-on-wines-quality-labels-2.7.2\"><span class=\"toc-item-num\">2.7.2&nbsp;&nbsp;<\/span>Check for correlations based on wines quality labels<\/a><\/span><\/li><\/ul><\/li><\/ul><\/li><li><span><a href=\"#Data-Engineering---Cleaning,-Transforming,-Selection-and-Reduction\" data-toc-modified-id=\"Data-Engineering---Cleaning,-Transforming,-Selection-and-Reduction-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;<\/span>Data Engineering - Cleaning, Transforming, Selection and Reduction<\/a><\/span><ul class=\"toc-item\"><li><span><a href=\"#Box-cox-transformation-of-highly-skewed-features\" data-toc-modified-id=\"Box-cox-transformation-of-highly-skewed-features-3.1\"><span class=\"toc-item-num\">3.1&nbsp;&nbsp;<\/span>Box cox transformation of highly skewed features<\/a><\/span><\/li><li><span><a href=\"#Identify--and-treat-multicollinearity:\" data-toc-modified-id=\"Identify--and-treat-multicollinearity:-3.2\"><span class=\"toc-item-num\">3.2&nbsp;&nbsp;<\/span>Identify  and treat multicollinearity:<\/a><\/span><\/li><li><span><a href=\"#Feature-Selection-into-the-Pipeline\" data-toc-modified-id=\"Feature-Selection-into-the-Pipeline-3.3\"><span class=\"toc-item-num\">3.3&nbsp;&nbsp;<\/span>Feature Selection into the Pipeline<\/a><\/span><\/li><li><span><a href=\"#Dimensionality-Reduction\" data-toc-modified-id=\"Dimensionality-Reduction-3.4\"><span class=\"toc-item-num\">3.4&nbsp;&nbsp;<\/span>Dimensionality Reduction<\/a><\/span><ul class=\"toc-item\"><li><span><a href=\"#PCA\" data-toc-modified-id=\"PCA-3.4.1\"><span class=\"toc-item-num\">3.4.1&nbsp;&nbsp;<\/span>PCA<\/a><\/span><\/li><li><span><a href=\"#Linear-Discriminant-Analysis-(LDA)\" data-toc-modified-id=\"Linear-Discriminant-Analysis-(LDA)-3.4.2\"><span class=\"toc-item-num\">3.4.2&nbsp;&nbsp;<\/span>Linear Discriminant Analysis (LDA)<\/a><\/span><\/li><\/ul><\/li><\/ul><\/li><li><span><a href=\"#Modeling\" data-toc-modified-id=\"Modeling-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;<\/span>Modeling<\/a><\/span><ul class=\"toc-item\"><li><span><a href=\"#Simplify-Get-Results\" data-toc-modified-id=\"Simplify-Get-Results-4.1\"><span class=\"toc-item-num\">4.1&nbsp;&nbsp;<\/span>Simplify Get Results<\/a><\/span><\/li><li><span><a href=\"#Hyper-Parametrization\" data-toc-modified-id=\"Hyper-Parametrization-4.2\"><span class=\"toc-item-num\">4.2&nbsp;&nbsp;<\/span>Hyper Parametrization<\/a><\/span><\/li><li><span><a href=\"#Wine-Type-Classifier-Models:\" data-toc-modified-id=\"Wine-Type-Classifier-Models:-4.3\"><span class=\"toc-item-num\">4.3&nbsp;&nbsp;<\/span>Wine Type Classifier Models:<\/a><\/span><ul class=\"toc-item\"><li><span><a href=\"#Logistic-Regression\" data-toc-modified-id=\"Logistic-Regression-4.3.1\"><span class=\"toc-item-num\">4.3.1&nbsp;&nbsp;<\/span>Logistic Regression<\/a><\/span><\/li><li><span><a href=\"#Deep-Neural-Network\" data-toc-modified-id=\"Deep-Neural-Network-4.3.2\"><span class=\"toc-item-num\">4.3.2&nbsp;&nbsp;<\/span>Deep Neural Network<\/a><\/span><\/li><\/ul><\/li><li><span><a href=\"#Wine-Quality-Classifier-Models:\" data-toc-modified-id=\"Wine-Quality-Classifier-Models:-4.4\"><span class=\"toc-item-num\">4.4&nbsp;&nbsp;<\/span>Wine Quality Classifier Models:<\/a><\/span><ul class=\"toc-item\"><li><span><a href=\"#Decision-Tree-Classifier\" data-toc-modified-id=\"Decision-Tree-Classifier-4.4.1\"><span class=\"toc-item-num\">4.4.1&nbsp;&nbsp;<\/span>Decision Tree Classifier<\/a><\/span><\/li><li><span><a href=\"#Ensemble-models\" data-toc-modified-id=\"Ensemble-models-4.4.2\"><span class=\"toc-item-num\">4.4.2&nbsp;&nbsp;<\/span>Ensemble models<\/a><\/span><\/li><li><span><a href=\"#Random-Forest-Classifier\" data-toc-modified-id=\"Random-Forest-Classifier-4.4.3\"><span class=\"toc-item-num\">4.4.3&nbsp;&nbsp;<\/span>Random Forest Classifier<\/a><\/span><\/li><li><span><a href=\"#XGBoost-(eXtreme-Gradient-Boosting)\" data-toc-modified-id=\"XGBoost-(eXtreme-Gradient-Boosting)-4.4.4\"><span class=\"toc-item-num\">4.4.4&nbsp;&nbsp;<\/span>XGBoost (eXtreme Gradient Boosting)<\/a><\/span><\/li><li><span><a href=\"#KNeighbors-Classifier\" data-toc-modified-id=\"KNeighbors-Classifier-4.4.5\"><span class=\"toc-item-num\">4.4.5&nbsp;&nbsp;<\/span>KNeighbors Classifier<\/a><\/span><\/li><li><span><a href=\"#Gradient-Boosting-for-Classification\" data-toc-modified-id=\"Gradient-Boosting-for-Classification-4.4.6\"><span class=\"toc-item-num\">4.4.6&nbsp;&nbsp;<\/span>Gradient Boosting for Classification<\/a><\/span><\/li><li><span><a href=\"#AdaBoost-classifier\" data-toc-modified-id=\"AdaBoost-classifier-4.4.7\"><span class=\"toc-item-num\">4.4.7&nbsp;&nbsp;<\/span>AdaBoost classifier<\/a><\/span><\/li><li><span><a href=\"#Logistic-Regression\" data-toc-modified-id=\"Logistic-Regression-4.4.8\"><span class=\"toc-item-num\">4.4.8&nbsp;&nbsp;<\/span>Logistic Regression<\/a><\/span><\/li><li><span><a href=\"#Linear-Support-Vector-Classification\" data-toc-modified-id=\"Linear-Support-Vector-Classification-4.4.9\"><span class=\"toc-item-num\">4.4.9&nbsp;&nbsp;<\/span>Linear Support Vector Classification<\/a><\/span><\/li><\/ul><\/li><li><span><a href=\"#Check-the-best-results-from-the-models-hyper-parametrization\" data-toc-modified-id=\"Check-the-best-results-from-the-models-hyper-parametrization-4.5\"><span class=\"toc-item-num\">4.5&nbsp;&nbsp;<\/span>Check the best results from the models hyper parametrization<\/a><\/span><\/li><li><span><a href=\"#Make-Staked-Classifier\" data-toc-modified-id=\"Make-Staked-Classifier-4.6\"><span class=\"toc-item-num\">4.6&nbsp;&nbsp;<\/span>Make Staked Classifier<\/a><\/span><\/li><\/ul><\/li><li><span><a href=\"#Conclusion\" data-toc-modified-id=\"Conclusion-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;<\/span>Conclusion<\/a><\/span><\/li><li><span><a href=\"#Next-steps\" data-toc-modified-id=\"Next-steps-6\"><span class=\"toc-item-num\">6&nbsp;&nbsp;<\/span>Next steps<\/a><\/span><\/li><li><span><a href=\"#Bonus-Task:\" data-toc-modified-id=\"Bonus-Task:-7\"><span class=\"toc-item-num\">7&nbsp;&nbsp;<\/span>Bonus Task:<\/a><\/span><\/li><\/ul><\/div>","680b4d76":"### Hyper Parametrization\nHyperparameters are also known as meta-parameters and are usually set before we start the model training process. These hyperparameters do not have any dependency on being derived from the underlying data on which the model is trained. Usually these hyperparameters represent some high level concepts or knobs, which can be used to tweak and tune the model during training to improve its performance. \n\nSo, let's start with the wine type classifier models.\n\n### Wine Type Classifier Models:\nWe will start with try classify the wines by red or white. \n\nLet's prepare our data with focus on type of wines and make the split of training and test datasets to use in the following tasks.","c19563bf":"### Dimensionality Reduction\n![image](https:\/\/i.imgur.com\/tLA1EhY.jpg)\n#### PCA\n**Principal component analysis** ([PCA](http:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.decomposition.PCA.html)) is a statistical procedure that uses an orthogonal transformation to convert a set of observations of possibly correlated variables into a set of values of linearly uncorrelated variables called principal components. If there are n observations with p variables, then the number of distinct principal components is `min(n-1,p)`. This transformation is defined in such a way that the first principal component has the largest possible variance, and each succeeding component in turn has the highest variance possible under the constraint that it is orthogonal to the preceding components. The resulting vectors are an uncorrelated orthogonal basis set. PCA is sensitive to the relative scaling of the original variables.\n![image.png](http:\/\/vitarts3.hospedagemdesites.ws\/wp-content\/uploads\/2018\/09\/DimRed.png)\n\nLet's see how PCA can reduce the dimensionality of our dataset with minimum of lose information:","e11e2337":"This model has good performance even without removing the two features, but also does not exceed the RF and commits 2 errors of classification of high as low quality.","847ee291":"### Nulls Check and Cleaning","87906356":"Highlights from descriptive statisct above:\n- Mean residual sugar and total sulfur dioxide content in white wine seems to be much higher than red wine.\n- Mean value of sulphates, fixed acidity and volatile acidity seem to be higher in red wine as compared to white wine.\n- From all numbers, we can observe that citric acid is more present in white than red wines.\n- In general, white wines has half concentrations of chlorides then red wines.\n- Although in Ph the difference seems small it is interesting to note that it is slightly larger in green red wines.\n\n#### By Quality","2baa67ec":"It achieved a good performance but no higher than RF and commented on an error in classifying as low a high quality wine.","d04c2b02":"From the accuracy and loss plot, you can note that we can stop the learning process at 40<sup>th<\/sup> or 23<sup>th<\/sup> epoch, given that at some point the model seems to be tending to overfit if we continue.\n\nLet's get our standard report and check if the DNN models after  40<sup>th<\/sup> can gave us a good generalization.","e8e8ebd1":"#### Linear Discriminant Analysis (LDA)\n\nAs a supervised dimensionality reduction technique for maximizing class separability. LDA can be used as a technique for feature extraction to increase the computational efficiency and reduce the degree of over-fitting due to the curse of dimensionality in nonregularized models. \n\nSo, the goal is to find the feature subspace that optimizes class separability.\n\nHowever, even if one or more of those assumptions are slightly violated, LDA for dimensionality reduction can still work reasonably well.\n\n\n**Some Important Parameters:**\nsolver : string, optional\n    Solver to use, possible values:\n      - svd: Singular value decomposition (default).\n        Does not compute the covariance matrix, therefore this solver is\n        recommended for data with a large number of features.\n      - eigen: Eigenvalue decomposition, can be combined with shrinkage.\n\nshrinkage : string or float, optional\n    Shrinkage parameter, possible values:\n      - None: no shrinkage (default).\n      - auto: automatic shrinkage using the Ledoit-Wolf lemma.\n      - float between 0 and 1: fixed shrinkage parameter.\n\n    Note that shrinkage works only with 'lsqr' and 'eigen' solvers.","c0583659":"As you can notice we will lose only 1 record of high, 12 of low and 21 of medium, out of a total of 34 (0,5%) being safe deletes them.  If you are running this notebook, and prefer try impute nulls instead drop you can you can only comments the first line of the code below","448e909b":"While most of the correlations are weak, we can see that:\n- Total and free sulfur dioxide has the higest correlation with white wines and each other. In fact, the second is a parcel of the fisrt and it is represent a colinearity that can be a problamem for linear classifiers, special if the target will predict the color, in that case we need drop the  free sulfur dioxide.\n- The residual sugar has a half relation to the total sulfur dioxide and 0.40 with free, it is a god indication that wich more residual sugar more sulfur dioxide is added by the winemaker. The 0.5 indicates that white wine tend to have more residual sugar then red wine.\n- Densite has a relatively high positive correlation to residual sugar and relatively high negative correlation to alcohol.\n- The chlorides and volatile acidity has -0.51 and -0,65 correlations between color, indicate a tendency to red wines classification.","057b1162":"We didn't get any improvements with PCA if compared with our DT without TSD and RS, but note that is better than our DT first and didn't make misclassification of high as low and vice-versa.\n\nIf you run the code above without this cut two features, it will have less improves and make some mistakes from high and low, but get this little improvement with cut two PCA components, like cut tow features.\n\nThink of the following hypothesis, you will make use of this model to suggest to a customer a wine within a certain quality level, surely you would like to avoid misclassification between high and low, not seeing too much problem if you erroneously classify one or the other as medium. In this case, precision and recall are more important than accuracy, especially if the difference is small. In our models we have a difference of 1.62%, but the most important is that we have to work to still improve it.","1ff2f6b4":"From pair plot above we can notice several interesting patterns, which are in alignment with the insights we obtained earlier at ANOVA test and correlations\n- Residual sugar, total sulfur dioxide and citric acid in white wine seems to be much higher than red wine.\n- Sulphates, fixed acidity and volatile acidity seem to be higher in red wine as compared to white wine.\n- Density has slightly strong negative correlation with  alcohol, which is confirmed by the linear trend of the points from left to right in a decreasing (downward) direction.\n- Density has slightly strong positive correlation with residual sugar, which is reinforced by two white wine outliers.\n\nLet's see the correlation and pair plots from the perpective of quality:","4da6ce0d":"The plot and the results confirms that DNN is better.","9c74fb33":"### Wine Quality Classifier Models:\nNow, let's move on to modeling a wine quality classifier. For this, let's prepare our dataset to focus on our wine quality label feature.","8e1fc9b9":"### Feature Selection into the Pipeline\nSince we have a few features it may be enough only the removal of collinear and multicollinear. However, with collinearity and multicollinearity levels are low and there are models to handle this. Then, it is interesting that we proceed with the selection of variables within the pipeline, allowing us to decide if there will be selection of variables and by which method. We can still improve the results through hyper parameterization and cross-validation.\n![image](https:\/\/thumbs.gfycat.com\/HonestThinCrocodileskink-small.gif)","cd46d07c":"Highlights from descriptive statisct of quality perspective above:\n- It is interesting how alcohol does not give us much variation to distinguish whether the vine is white or red, but it makes a lot of difference in quality. Note that the higher the quality the higher the average alcohol concentration, increased by about 1% at each level. Although lower quality wines have the lowest standard deviation.\n- The chlorides and volatile acidity are less present and presented smaller standard deviation in wines of higher quality.\n- The free sulfur dioxide is higher with higher quality, but their standard deviation decrease with the increase the quality.\n- Higher quality has less fixed acidity, but the standard deviation is slightly higher in mean quality\n\n### Inferential Statistics\n\nThe general notion of inferential statistics is to draw inferences and propositions of a population using a data sample. The idea is to use statistical methods and models to draw statistical inferences from a given hypotheses. Each hypothesis consists of a null hypothesis and an alternative hypothesis. Based on statistical test results, if the result is statistically significant based on pre-set significance levels (e.g., if obtained p-value is less than 5% significance level), we reject the null hypothesis in favor of the alternative hypothesis. Otherwise, if the results is not statistically significant, we conclude that our null hypothesis was correct.\n\nA great statistical model to prove or disprove the difference in mean among subsets of data is to use the one-way ANOVA test. ANOVA stands for \u201canalysis of variance,\u201d which is a nifty statistical model and can be used to analyze statistically significant differences among means or averages of various groups. This is basically achieved using a statistical test that helps us determine whether or not the means of several groups are equal. \n\nThe alternative hypotheses, H<sub>A<\/sub>, tells us that there exists at least two group means that are statistically significantly different from each other. Usually the F-statistic and the associated p-value from it is used to determine the statistical significance. Typically a p-value less than 0.05 is taken to be a statistically significant result where we reject the null hypothesis in favor of the original. \n\nSo let's evaluate the hypotheses of each of our previous highlights through the statistical inference test","a66fff8a":"#### Decision Tree Classifier\n\n[Decision Trees](http:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.tree.DecisionTreeClassifier.html) (DTs) are a non-parametric supervised learning method used for classification and regression. The goal is to create a model that predicts the value of a target variable by learning simple decision rules inferred from the data features.\n\n___Some advantages of decision trees are:___\n\n- Simple to understand and to interpret. Trees can be visualised.\n- Requires little data preparation. Other techniques often require data normalisation, dummy variables need to be created and blank values to be removed. Note however that this module does not support missing values.\n- The cost of using the tree (i.e., predicting data) is logarithmic in the number of data points used to train the tree.\n- Able to handle both numerical and categorical data. Other techniques are usually specialised in analysing datasets that have only one type of variable. See algorithms for more information.\n- Able to handle multi-output problems.\n- Uses a white box model. If a given situation is observable in a model, the explanation for the condition is easily explained by boolean logic. By contrast, in a black box model (e.g., in an artificial neural network), results may be more difficult to interpret.\n- Possible to validate a model using statistical tests. That makes it possible to account for the reliability of the model.\n- Performs well even if its assumptions are somewhat violated by the true model from which the data were generated.\n\n___The disadvantages of decision trees include:___\n\n- Decision-tree learners can create over-complex trees that do not generalise the data well. This is called overfitting. Mechanisms such as pruning (not currently supported), setting the minimum number of samples required at a leaf node or setting the maximum depth of the tree are necessary to avoid this problem.\n- Decision trees can be unstable because small variations in the data might result in a completely different tree being generated. This problem is mitigated by using decision trees within an ensemble.\n- The problem of learning an optimal decision tree is known to be NP-complete under several aspects of optimality and even for simple concepts. Consequently, practical decision-tree learning algorithms are based on heuristic algorithms such as the greedy algorithm where locally optimal decisions are made at each node. Such algorithms cannot guarantee to return the globally optimal decision tree. This can be mitigated by training multiple trees in an ensemble learner, where the features and samples are randomly sampled with replacement.\n- There are concepts that are hard to learn because decision trees do not express them easily, such as XOR, parity or multiplexer problems.\n- Decision tree learners create biased trees if some classes dominate. It is therefore recommended to balance the dataset prior to fitting with the decision tree.","92395081":"As you can see, we reached a worse CV accuracy, but all metrics showed considerable improvement, proving that the elimination of this two characteristics made our decision tree capable of better generalization. without them we have better model classification report and ROC, in special because we improve the prediction of high class. But note, our model get this with only more one level in the tree, which shows us that we can eliminate it safely.\n\nLet's include a PCA step in our pipeline and see, if can help:","cd317e0d":"The model prediction results on the test dataset depict an overall F1 Score and model accuracy of approximately 82.13%, as seen above. This is quite good considering we got an improvement of 9,47%  from the best decision tree model. Also we can see that no low quality wine sample has been misclassified as high and vice-versa. There is a considerable overlap between medium and high\\low quality wine samples but that is expected given the nature of the data and class distribution.\n\nAs we have few characteristics and it would be necessary to remove only two to solve the problems caused by collinearity and multicollinearity, it is interesting to note that a bagging method is able to handle this, without the necessity of having to delete them, this is because different trees are making use of different sets. If you prefer to check for yourself change the above code to other selections and include or removing the PCA.\n\nLet's look at some model prediction interpretations, for this, we will be leveraging skater and look at model predictions. We will try to interpret why the model predicted a class label and which features were influential in its decision. First we build a LimeTabularExplainer object using the following snippet, which will help us in interpreting and explaining predictions.","d4f53288":"As you can see, we got wonderful with 99.38 in all metrics performance and 99.28% of ROC AUC score. The most important features are total sulfur dioxide, density, chlorides and volatile acidity.\n\nThese results are really sufficient and obtained through a very simple model, but out of curiosity, we will submit the same data to a Deep Neural Network Classifier","3d8f9747":"#### Logistic Regression\nSince it is a binary classification task, we try first with a simple logistic regression.\n\nThis class implements regularized [logistic regression](http:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.linear_model.LogisticRegression.html) using the 'liblinear' library, 'newton-cg', 'sag' and 'lbfgs' solvers. It can handle both dense and sparse input. \n\n**Main Parameters**\n - class_weight : dict or 'balanced', default: None\n   The \"balanced\" mode uses the values of y to automatically adjust weights inversely proportional to class frequencies in the input data as ``n_samples \/ (n_classes * np.bincount(y))``.\n\n   For how class_weight works: It penalizes mistakes in samples of class[i] with class_weight[i] instead of 1. So higher class-weight means you want to put more emphasis on a class. For example, our class 0 is 1.24 times more frequent than class 1. So you should increase the class_weight of class 1 relative to class 0, say {1: 0.6, 0: 0.4}. If the class_weight doesn't sum to 1, it will basically change the regularization parameter.\n\n   \"balanced\" basically means replicating the smaller class until you have as many samples as in the larger one, but in an implicit way.\n   \n\n - warm_start : bool, default: False. Useless for liblinear solver.\n - ``'clf__multi_class' : ['ovr', 'multinomial']`` for ``'clf__solver': ['newton-cg', 'sag', 'lbfgs']``\n\n**Attributes:**\n - coef_ : array, shape (1, n_features) or (n_classes, n_features)\n - intercept_ : array, shape (1,) or (n_classes,)\n - n_iter_ : array, shape (n_classes,) or (1, )\n\n**See also:**\n - SGDClassifier : incrementally trained logistic regression (when given the parameter ``loss=\"log\"``).\n - sklearn.svm.LinearSVC : learns SVM models using the same algorithm.\n \n See our the best results below with first model from a pipeline with grid search CV.","f04726aa":"#### Check for correlations based on wines types\nNow, I will make a zoom in these features in order of their correlation with type of wine:","6f286c62":"## Conclusion\n","ae078249":"### Load Dataset\nLoad dataset, create the quality label based on the quality score according to:\n- Low for less or equal to 5\n- Medium between 6 and 7\n- High for more then 7","a5d488a0":"As you may notice we have very few records with null characteristics, so let's evaluate the option to simply eliminate rows with nulls instead of entering with medians and modes.","faee6690":"It is quite evident that red wine samples have higher acidity as compared to its white wine counterparts. Also we can see an overall decrease in acidity with higher quality wine for red wine samples but not so much for white wine samples. \n\nLet's see this obsevation in some 3 variabels graphs analysis:","1f3a711a":"#### Ensemble models\nMoving forward with our mission of improving our wine quality predictive model, let\u2019s look at some ensemble modeling methods. Ensemble models are typically Machine Learning models that combine or take a weighted (average\\majority) vote of the predictions of each of the individual base model estimators that have been built using supervised methods of their own. The ensemble is expected to generalize better over underlying data, be more robust, and make superior predictions as compared to each individual base model. Ensemble models can be categorized under three major families.\n- ***Bagging methods***: \n    In this approach, a bootstrap samples, i.e. independent samples with replacement, are taken and several base models are built on these sampled datasets. At any instance, an average of all predictions from the individual estimators is taken for the ensemble model to make its final prediction. Random sampling tries to reduce model variance, reduce overfitting, and boost prediction accuracy. \n- ***Boosting methods***: \n    In this method the idea is to combine several weak base learners to form a powerful ensemble. Weak learners are trained sequentially over multiple iterations of the training data with weight modifications inserted at each retrain phase. At each re-training of a weak base learner, higher weights are assigned to those training instances which were misclassified previously. Thus, these methods try to focus on training instances which it wrongly predicted in the previous training sequence. Boosted models are prone to over-fitting so one should be very careful. Examples include Gradient Boosting, AdaBoost, and the very popular XGBoost.\n- ***Stacking methods***:\n    In stacking based methods, we first build multiple base models over the training data. Then the final ensemble model is built by taking the output predictions from these models as its additional inputs for training to make the final prediction.\n \n#### Random Forest Classifier\nLet's now try building a model using a bagging methods by run a random forests\n\nA [random forest](http:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.ensemble.RandomForestClassifier.html#sklearn.ensemble.RandomForestClassifier) is a meta estimator that fits a number of decision tree classifiers on various sub-samples of the dataset and use averaging to improve the predictive accuracy and control over-fitting. The sub-sample size is always the same as the original input sample size but the samples are drawn with replacement if bootstrap=True (default).\n\nIn the random forest model, each base learner is a decision tree model trained on a bootstrap sample of the training data. Besides this, when we want to split a decision node in the tree, the split is chosen from a random subset of all the features instead of taking the best split from all the features. Due to the introduction of this randomness, bias increases and when we average the result from all the trees in the forest, the overall variance decreases, giving us a robust ensemble model which generalizes well. We will be using the RandomForestClassifier from scikit-learn, which averages the probabilistic prediction from all the trees in the forest for the final prediction instead of taking the actual prediction votes and then averaging it","c367b7f3":"It achieved a good performance, not making errors of classification between high and low, but did not surpass the RF.\n\nNote that the third and fourth most important features are different between the two models, but they are the same in the first and second. Here the total sulfur dioxide and residual sugar don't have importance because we drop them.","841f14fd":"### Check the best results from the models hyper parametrization","4f7e8ed2":"Let's now look at two wine sample instances from our test dataset. The first instance is a wine of low quality rating. We show the interpretation for the predicted class with maximum probability\\confidence using the top_labels parameter. You can set it to 3 to view the same for all the three class labels.","004134cd":"As you can see, we got better results then our previous model, a 0.47% better in all metrics performance and a increase of 0.68% on ROC AUC score. The first tree most important features are the same with the change position between first and second, but the forth switched from volatile acidity to residual sugar. which shows that you must be careful when selecting variables by the return of importance, remember that this changes from template to template, but okay, it may be that the changes have occurred due to the multi-collinearity and collinearity that we do not deal with in this case.\n\nLet's compare the two model by a ROC curve plot and see side-by-side some performance metrics:","9ddadbf7":"#### KNeighbors Classifier\n\nNeighbors-based classification is a type of instance-based learning or non-generalizing learning: it does not attempt to construct a general internal model, but simply stores instances of the training data. Classification is computed from a simple majority vote of the nearest neighbors of each point: a query point is assigned the data class which has the most representatives within the nearest neighbors of the point.\n\nscikit-learn implements two different nearest neighbors classifiers: [KNeighborsClassifier](http:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.neighbors.KNeighborsClassifier.html#sklearn.neighbors.KNeighborsClassifier) implements learning based on the  nearest neighbors of each query point, where  is an integer value specified by the user. [RadiusNeighborsClassifier](http:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.neighbors.RadiusNeighborsClassifier.html#sklearn.neighbors.RadiusNeighborsClassifier) implements learning based on the number of neighbors within a fixed radius of each training point, where is a floating-point value specified by the user.\n\nWe will create a KNeighborsClassifier model with following parameters:\n- n_neighbors: Number of neighbors to use by default for kneighbors queries.\n- weights: weight function used in prediction. \n    - 'uniform': uniform weights. All points in each neighborhood are weighted equally.\n    - 'distance': weight points by the inverse of their distance. in this case, closer neighbors of a query point will have a greater influence than neighbors which are further away.\n    - [callable]: a user-defined function which accepts an array of distances, and returns an array of the same shape containing the weights.\n- algorithm: Algorithm used to compute the nearest neighbors:\n    - 'ball_tree' will use BallTree\n    = 'kd_tree' will use KDTree\n    = 'brute' will use a brute-force search.\n    = 'auto' will attempt to decide the most appropriate algorithm based on the values passed to fit method.\n    \n    Note: fitting on sparse input will override the setting of this parameter, using brute force.\n\n- leaf_size: Leaf size passed to BallTree or KDTree. This can affect the speed of the construction and query, as well as the memory required to store the tree. The optimal value depends on the nature of the problem.\n- p: Power parameter for the Minkowski metric. When p = 1, this is equivalent to using manhattan_distance (l1), and euclidean_distance (l2) for p = 2. For arbitrary p, minkowski_distance (l_p) is used.","97d2ac73":"#### XGBoost (eXtreme Gradient Boosting)\n\n[XGBoost](https:\/\/xgboost.readthedocs.io\/en\/latest\/) is an advanced implementation of gradient boosting algorithm. It's a highly sophisticated algorithm, powerful enough to deal with all sorts of irregularities of data.\n\n- Standard GBM implementation has no **regularization** like XGBoost, therefore it also helps to reduce overfitting.\n- XGBoost implements **parallel processing** to making a tree using all cores and is blazingly faster as compared to GBM.\n- XGBoost also supports implementation on Hadoop.\n- **High flexibility**, it allow users to define custom optimization objectives and evaluation criteria.\n- XGBoost has an **in-built routine to handle missing values**.\n- It make splits up to the max_depth specified and then start **pruning the tree backwards** and remove splits beyond which there is no positive gain.\n- Sometimes a split of negative loss say -2 may be followed by a split of positive loss +10. GBM would stop as it encounters -2. But XGBoost will go deeper and it will see a **combined effect** of +8 of the split and keep both.\n- XGBoost **allows user to run a cross-validation** at each iteration of the boosting process and thus it is easy to get the exact optimum number of boosting iterations in a single run. You don't need use grid search.\n- User can **start training** an XGBoost model **from** its last iteration of **previous run**. \n\nThe overall parameters have been divided into 3 categories by XGBoost authors, let's see the most importants:\n1. General Parameters: Guide the overall functioning:\n - **booster**: default is gbtree fom ['gbtree', 'gblinear']\n<p><p>\n2. Booster Parameters: Guide the individual booster (tree\/regression) at each step:\n - **learning_rate** (eta): default is 0.3. Makes the model more robust by shrinking the weights on each step. Typical final values to be used: 0.01-0.2\n - **min_child_weight**: default is 1. Defines the minimum sum of weights of all observations required in a child. Used to control over-fitting. Higher values prevent a over-fitting, but too high values can lead to under-fitting hence, it should be **tuned** using CV.\n - **max_depth**: default is 6. The maximum depth of a tree used to control over-fitting and should be **tuned** using CV. Typical values: 3-10\n - **max_leaf_nodes**: The maximum number of terminal nodes or leaves in a tree. Can be defined in place of max_depth. Since binary trees are created, a depth of 'n' would produce a maximum of 2^n leaves. If this is defined, GBM will ignore max_depth.\n - **gamma**: default is 0. A node is split only when the resulting split gives a positive reduction in the loss function. Gamma specifies the minimum loss reduction required to make a split. Makes the algorithm conservative. **The values can vary depending on the loss function** and should be **tuned**.\n - **max_delta_step**: default is 0. In maximum delta step we allow each tree's weight estimation to be. If the value is set to 0, it means there is no constraint. If it is set to a positive value, it can help making the update step more conservative. Usually this parameter is not needed, but **it might help in logistic regression when class is extremely imbalanced**.\n - **subsample**: default is 1. Denotes the fraction of observations to be randomly samples for each tree. Lower values make the algorithm more conservative and prevents overfitting but too small values might lead to under-fitting. Typical values: 0.5-1\n - **colsample_bytree**: default is 1. Denotes the fraction of columns to be randomly samples for each tree. Typical values: 0.5-1\n - **colsample_bylevel**: default is 1. Denotes the subsample ratio of columns for each split, in each level.\n - **reg_lambda** (lambda): default is 1. L2 regularization term on weights, analogous to Ridge regression, it should be explored to reduce overfitting.\n - **reg_alpha** (alpha): default is 0. L1 regularization term on weight, analogous to Lasso regression, Can be used in case of very high dimensionality so that the algorithm runs faster when implemented.\n - **scale_pos_weight**: default is 1. A value greater than 0 should be used in case of high class imbalance as it helps in faster convergence. **To balance use** `sum(negative cases)\/sum(positive cases)` and Use AUC for evaluation.\n<p><p>\n3. Learning Task Parameters: These parameters are used to define the optimization objective the metric to be calculated at each step: \n - **objective**: default is reg:linear and binary:logistic for XGBClassifier. This defines the loss function to be minimized. Mostly used values are: <p>\n       - binary:logistic \u2013logistic regression for binary classification, returns predicted probability (not class)\n       - multi:softmax \u2013multiclass classification using the softmax objective, returns predicted class (not probabilities). You also need to set an additional num_class (number of classes) parameter defining the number of unique classes\n       - multi:softprob \u2013same as softmax, but returns predicted probability of each data point belonging to each class.\n - **eval_metric**: The default values are rmse for regression and error for classification. The metric to be used for validation data. Typical values are: \n       - rmse \u2013 root mean square error\n       - mae \u2013 mean absolute error\n       - logloss \u2013 negative log-likelihood\n       - error \u2013 Binary classification error rate (0.5 threshold)\n       - merror \u2013 Multiclass classification error rate\n       - mlogloss \u2013 Multiclass logloss\n       - auc: Area under the curve\n - **seed**: The random number seed. Can be used for generating reproducible results and also for parameter tuning.\n<p><p>\n\nBefore proceeding further, since cgb don't accept categorical let's change it to boolean or integer."}}