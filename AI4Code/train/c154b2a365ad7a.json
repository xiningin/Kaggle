{"cell_type":{"b5d3bc34":"code","063e9bba":"code","6042311a":"code","06fee1cd":"code","9fbe959f":"code","af9bd782":"code","fae7b49d":"code","fa7cfd6a":"code","65a9765f":"code","e445eb4d":"code","a9e7fb19":"code","e6c350c5":"code","b9fac06c":"code","652e76ca":"code","df6b7828":"code","d1b42d72":"code","e4461a65":"code","c116e4a9":"code","d1648913":"code","e84dbf67":"code","f7d98696":"code","38434d32":"code","2e25799d":"code","01a5cbdc":"code","9b78e9e2":"code","9c00795d":"code","04f4f299":"code","6116d911":"code","247c0b29":"code","1d008ad7":"code","fce7abe5":"code","ec716aae":"code","d81c3ed3":"code","e1576e76":"code","38d50ddc":"code","771bd54f":"code","761cc9a5":"code","e6ace7de":"code","b282fcca":"code","b41e3f7a":"code","5d35b34b":"markdown","e969af90":"markdown","d8577310":"markdown","148d0f9f":"markdown","c384a80d":"markdown","4cdd1ac4":"markdown","df2cd111":"markdown","d9df10cf":"markdown","95bca1dd":"markdown","09cbd85a":"markdown"},"source":{"b5d3bc34":"import numpy as np \nimport pandas as pd \nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport category_encoders as ce\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","063e9bba":"df = pd.read_csv('..\/input\/titanic\/train.csv')\ndf_ = pd.read_csv('..\/input\/titanic\/test.csv')\ndf_train = df.copy()\ndf_test = df_.copy()\ndf_train","6042311a":"df_train.isnull().sum()","06fee1cd":"df_train.duplicated().sum()","9fbe959f":"def feature_plot(data_train,data_test,feature,target,cat=True,text=True):\n    \n    df = data_train.copy()\n    df_t = data_test.copy()\n    backcolor='#EAEAEA'\n    if cat == True:\n        \n        #train------------------------------------------\n        palette_1 = [\"#F8003D\",\"#08D9D6\",\"#a1d4e2\"]\n        sns.set_palette(palette_1)\n        sm = df.shape[0]\n        fig,ax = plt.subplots(1,4,figsize=(28,6))\n        g = sns.countplot(data=df,x=feature,hue=target,edgecolor=\"black\",linewidth=3,ax=ax[0])\n        g.set_xlabel(feature, weight='bold', size=13,alpha=0.65)\n        g.set_ylabel(feature+\"_counts\", weight='bold', size=13,alpha=0.65)\n        g.set_facecolor(backcolor)\n        g.spines[['top', 'right','bottom']].set_visible(False)\n        g.set_title(feature+\"_counts and percent--traindata\", size=15, weight='bold',fontname=\"Microsoft YaHei\",alpha=0.65)\n        if  text == True:\n            for patch in g.patches:\n                x, height, width = patch.get_x(), patch.get_height(), patch.get_width()\n                g.text(x + width \/ 2, height + 10, f'{height} \/ {height \/ sm * 100:2.2f}%', va='center', ha='center', size=10, alpha=0.5,bbox={'facecolor': 'w', 'boxstyle': 'round4'}\n          )\n            \n        dt = df[feature].value_counts(normalize=True).reset_index()\n        p = sns.barplot(data=dt,x=\"index\",y=feature,edgecolor=\"black\",linewidth=3,ax=ax[1])\n        sns.set_palette(palette_1)\n        p.set_xlabel(feature, weight='bold', size=13,alpha=0.65)\n        p.set_ylabel(feature+\"_counts\", weight='bold', size=13,alpha=0.65)\n        p.set_facecolor(backcolor)\n        p.spines[['top', 'right','bottom']].set_visible(False)\n        p.set_title(feature+\"_percent--traindata\", size=15, weight='bold',fontname=\"Microsoft YaHei\",alpha=0.65)\n        \n        #test------------------------------------------------\n        smt = df_t.shape[0]\n        g = sns.countplot(data=df_t,x=feature,edgecolor=\"black\",linewidth=3,ax=ax[2])\n        g.set_xlabel(feature, weight='bold', size=13,alpha=0.65)\n        g.set_ylabel(feature+\"_counts\", weight='bold', size=13,alpha=0.65)\n        g.set_facecolor(backcolor)\n        g.spines[['top', 'right','bottom']].set_visible(False)\n        g.set_title(feature+\"_counts and percent--testdata\", size=15, weight='bold',fontname=\"Microsoft YaHei\",alpha=0.65)\n        if  text == True:\n            for patch in g.patches:\n                x, height, width = patch.get_x(), patch.get_height(), patch.get_width()\n                g.text(x + width \/ 2, height + 4, f'{height} \/ {height \/ smt * 100:2.2f}%', va='center', ha='center', size=10, alpha=0.5,bbox={'facecolor': 'w', 'boxstyle': 'round4'}\n          )\n            \n        dt = df_t[feature].value_counts(normalize=True).reset_index()\n        p = sns.barplot(data=dt,x=\"index\",y=feature,edgecolor=\"black\",linewidth=3,ax=ax[3])\n        sns.set_palette(palette_1)\n        p.set_xlabel(feature, weight='bold', size=13,alpha=0.65)\n        p.set_ylabel(feature+\"_counts\", weight='bold', size=13,alpha=0.65)\n        p.set_facecolor(backcolor)\n        p.spines[['top', 'right','bottom']].set_visible(False)\n        p.set_title(feature+\"_percent--testdata\", size=15, weight='bold',fontname=\"Microsoft YaHei\",alpha=0.65)\n        plt.show()\n    \n    else:\n        #train-----------------------------------------------\n        fig,ax = plt.subplots(1,4,figsize=(28,6))\n        g = sns.kdeplot(data=df,x=feature,hue=target,shade=True,ax=ax[0])\n        g.set_xlabel(feature, weight='bold', size=13,alpha=0.65)\n        g.set_ylabel(feature+\"_KDE\", weight='bold', size=13,alpha=0.65)\n        g.set_facecolor(backcolor)\n        g.spines[['top', 'right','bottom']].set_visible(False)\n        g.set_title(feature+\"_KDE_FIGURE\", size=15, weight='bold',fontname=\"Microsoft YaHei\",alpha=0.65)\n        \n        p=sns.boxplot(y=df[feature],ax=ax[1])\n        p.set_facecolor(backcolor)\n        p.set_xlabel(feature,weight='bold', size=13,alpha=0.65)\n        p.set_ylabel(feature+\"_VALUES\",weight='bold', size=13,alpha=0.65)\n        p.set_title(feature+\"_BOX\")\n        \n        #testdata-------------------------------------------\n        g = sns.kdeplot(data=df_t,x=feature,shade=True,ax=ax[2])\n        g.set_xlabel(feature, weight='bold', size=13,alpha=0.65)\n        g.set_ylabel(feature+\"_KDE\", weight='bold', size=13,alpha=0.65)\n        g.set_facecolor(backcolor)\n        g.spines[['top', 'right','bottom']].set_visible(False)\n        g.set_title(feature+\"_KDE_FIGURE--testdata\", size=15, weight='bold',fontname=\"Microsoft YaHei\",alpha=0.65)\n        \n        p=sns.boxplot(y=df_t[feature],ax=ax[3])\n        p.set_facecolor(backcolor)\n        p.set_xlabel(feature,weight='bold', size=13,alpha=0.65)\n        p.set_ylabel(feature+\"_VALUES\",weight='bold', size=13,alpha=0.65)\n        p.set_title(feature+\"_BOX--testdata\")\n        plt.show()","af9bd782":"feature_plot(df_train,df_test,\"Pclass\",\"Survived\")","fae7b49d":"feature_plot(df_train,df_test,\"Sex\",\"Survived\")","fa7cfd6a":"feature_plot(df_train,df_test,\"Age\",\"Survived\",cat=False)","65a9765f":"feature_plot(df_train,df_test,\"Embarked\",\"Survived\")","e445eb4d":"df_train[\"SP\"] = df_train[\"SibSp\"] + df_train[\"Parch\"]\ndf_test[\"SP\"] = df_test[\"SibSp\"] + df_test[\"Parch\"]\nfeature_plot(df_train,df_test,\"SP\",\"Survived\",text=False)","a9e7fb19":"feature_plot(df_train,df_test,\"Fare\",\"Survived\",cat=False)","e6c350c5":"df_train[\"Cabin_type\"] = df_train[\"Cabin\"].apply(lambda x:str(x)[0])\ndf_test[\"Cabin_type\"] = df_test[\"Cabin\"].apply(lambda x:str(x)[0])\ndf_train[\"Cabin_type\"] = df_train[\"Cabin_type\"].replace({\"n\":\"Miss\"})\ndf_test[\"Cabin_type\"] = df_test[\"Cabin_type\"].replace({\"n\":\"Miss\"})\nfeature_plot(df_train,df_test,\"Cabin_type\",\"Survived\",text=False)","b9fac06c":"df_train[\"Name_s\"] = df_train[\"Name\"].apply(lambda a: a.split(\".\")[0].split(\",\")[1].strip())\ndf_test[\"Name_s\"] = df_test[\"Name\"].apply(lambda a: a.split(\".\")[0].split(\",\")[1].strip())\nfeature_plot(df_train,df_test,\"Name_s\",\"Survived\",text=False)","652e76ca":"df_train[\"Age\"] = df_train[\"Age\"].fillna(df_train[\"Age\"].mean()) \ndf_test[\"Age\"] = df_test[\"Age\"].fillna(df_train[\"Age\"].mean()) \ndf_train = df_train.drop(columns=[\"Cabin\"])\ndf_test = df_test.drop(columns=[\"Cabin\"])\ndf_test[\"Fare\"] = df_test[\"Fare\"].fillna(df_train[\"Fare\"].mean())\ndf_train[\"Embarked\"] = df_train[\"Embarked\"].fillna(df_train[\"Embarked\"].mode().values[0])\ndf_train = df_train.drop(columns=[\"Name\",\"Ticket\",\"SibSp\",\"Parch\"])\ndf_test = df_test.drop(columns=[\"Name\",\"Ticket\",\"SibSp\",\"Parch\"])","df6b7828":"df_train","d1b42d72":"df_test","e4461a65":"df_train[\"Type\"] = 0 \ndf_test[\"Type\"] =1\nids = df_train[[\"Survived\"]]\ndf_t = df_train.drop(columns=[\"Survived\"])\none_df = pd.concat([df_t,df_test],axis=0)\ncat_df = one_df.copy()\none_df","c116e4a9":"one_df = pd.get_dummies(data=one_df,columns=[\"Pclass\",\"Sex\",\"Embarked\",\"SP\",\"Cabin_type\",\"Name_s\"],drop_first=True)\none_df","d1648913":"from sklearn.preprocessing import StandardScaler\nlis = [\"Age\",\"Fare\"]\nfor i in lis:\n    std = StandardScaler()\n    std.fit(one_df[one_df[\"Type\"] == 0][i].values.reshape(-1,1))\n    one_df[i] = std.transform(one_df[i].values.reshape(-1,1))","e84dbf67":"df_train = one_df[one_df[\"Type\"] == 0]\ndf_train[\"Survived\"] = ids\ndf_test = one_df[one_df[\"Type\"] == 1]","f7d98696":"from sklearn.model_selection import StratifiedKFold\ny = df_train[\"Survived\"]\nx = df_train.drop(columns=[\"PassengerId\",\"Survived\"])\nstf = StratifiedKFold(n_splits=5,shuffle=True,random_state=123)\nfor train_index,test_index in stf.split(x,y):\n    x_train,x_test = x.iloc[train_index],x.iloc[test_index]\n    y_train,y_test = y.iloc[train_index],y.iloc[test_index]","38434d32":"from sklearn.svm import SVC\nfrom sklearn.metrics import recall_score,precision_score,f1_score\nsvc = SVC()\nsvc.fit(x_train,y_train)\nsvc_pre = svc.predict(x_test)\nsvc_recall = recall_score(y_test,svc_pre)\nsvc_precis = precision_score(y_test,svc_pre)\none_svc_f1 = f1_score(y_test,svc_pre)\nprint(\"svm recall:\",svc_recall)\nprint(\"svm precision:\",svc_precis)\nprint(\"svm f1_score:\",one_svc_f1)","2e25799d":"from sklearn.linear_model import LogisticRegression\nlogis = LogisticRegression()\nlogis.fit(x_train,y_train)\nlogis_pre = logis.predict(x_test)\nlogis_recall = recall_score(y_test,logis_pre)\nlogis_precis = precision_score(y_test,logis_pre)\none_logis_f1 = f1_score(y_test,logis_pre)\nprint(\"logis recall:\",logis_recall)\nprint(\"logis precision:\",logis_precis)\nprint(\"logis f1_score:\",one_logis_f1)","01a5cbdc":"from sklearn.ensemble import RandomForestClassifier\nrf = RandomForestClassifier()\nrf.fit(x_train,y_train)\nrf_pre = rf.predict(x_test)\nrf_recall = recall_score(y_test,rf_pre)\nrf_precis = precision_score(y_test,rf_pre)\none_rf_f1 = f1_score(y_test,rf_pre)\nprint(\"rf recall:\",rf_recall)\nprint(\"rf precision:\",rf_precis)\nprint(\"rf f1_score:\",one_rf_f1)","9b78e9e2":"from sklearn.ensemble import GradientBoostingClassifier\ngbdt = GradientBoostingClassifier()\ngbdt.fit(x_train,y_train)\ngbdt_pre = gbdt.predict(x_test)\ngbdt_recall = recall_score(y_test,gbdt_pre)\ngbdt_precis = precision_score(y_test,gbdt_pre)\none_gbdt_f1 = f1_score(y_test,gbdt_pre)\nprint(\"gbdt recall:\",gbdt_recall)\nprint(\"gbdt precision:\",gbdt_precis)\nprint(\"gbdt f1_score:\",one_gbdt_f1)","9c00795d":"lis = [\"Age\",\"Fare\"]\nfor i in lis:\n    std = StandardScaler()\n    std.fit(cat_df[one_df[\"Type\"] == 0][i].values.reshape(-1,1))\n    cat_df[i] = std.transform(cat_df[i].values.reshape(-1,1))\nmean_df = cat_df.copy()","04f4f299":"df_train = cat_df[cat_df[\"Type\"] == 0]\ndf_train[\"Survived\"] = ids\ndf_test = cat_df[cat_df[\"Type\"] == 1]","6116d911":"cat_list = [\"Pclass\",\"SP\"]\nfor col in cat_list:\n    df_train[col] = df_train[col].apply(lambda x:col+\"_\"+str(x))\n    df_test[col] = df_test[col].apply(lambda x:col+\"_\"+str(x))","247c0b29":"surviv = df_train[[\"Survived\"]]\ndf_train = df_train.drop(columns=[\"Survived\"])\ndf = pd.concat([df_train,df_test],axis=0)\nmean_df = df.copy()\ncat_list = [\"Pclass\",\"Sex\",\"Embarked\",\"SP\",\"Cabin_type\",\"Name_s\"]\nfor col in cat_list:\n    ca = ce.CatBoostEncoder()\n    ca.fit(df[df[\"Type\"] ==0][col],surviv)\n    df[col] = ca.transform(df[col])","1d008ad7":"df_train = df[df[\"Type\"] == 0]\ndf_train = df_train.drop(columns=[\"Type\",\"PassengerId\"])\ndf_train[\"Survived\"] = surviv\ndf_test = df[df[\"Type\"] == 1]\ntest_id = df_test[[\"PassengerId\"]]\ndf_test = df_test.drop(columns=[\"Type\",\"PassengerId\"])","fce7abe5":"y = df_train[\"Survived\"]\nx = df_train.drop(columns=[\"Survived\"])\nstf = StratifiedKFold(n_splits=5,shuffle=True,random_state=123)\nfor train_index,test_index in stf.split(x,y):\n    x_train,x_test = x.iloc[train_index],x.iloc[test_index]\n    y_train,y_test = y.iloc[train_index],y.iloc[test_index]","ec716aae":"from sklearn.svm import SVC\nsvc = SVC()\nsvc.fit(x_train,y_train)\nsvc_pre = svc.predict(x_test)\nsvc_recall = recall_score(y_test,svc_pre)\nsvc_precis = precision_score(y_test,svc_pre)\ncat_svc_f1 = f1_score(y_test,svc_pre)\nprint(\"svm recall:\",svc_recall)\nprint(\"svm precision:\",svc_precis)\nprint(\"svm f1_score:\",cat_svc_f1)","d81c3ed3":"logis = LogisticRegression()\nlogis.fit(x_train,y_train)\nlogis_pre = logis.predict(x_test)\nlogis_recall = recall_score(y_test,logis_pre)\nlogis_precis = precision_score(y_test,logis_pre)\ncat_logis_f1 = f1_score(y_test,logis_pre)\nprint(\"logis recall:\",logis_recall)\nprint(\"logis precision:\",logis_precis)\nprint(\"logis f1_score:\",cat_logis_f1)","e1576e76":"rf = RandomForestClassifier()\nrf.fit(x_train,y_train)\nrf_pre = rf.predict(x_test)\nrf_recall = recall_score(y_test,rf_pre)\nrf_precis = precision_score(y_test,rf_pre)\ncat_rf_f1 = f1_score(y_test,rf_pre)\nprint(\"rf recall:\",rf_recall)\nprint(\"rf precision:\",rf_precis)\nprint(\"rf f1_score:\",cat_rf_f1)","38d50ddc":"gbdt = GradientBoostingClassifier()\ngbdt.fit(x_train,y_train)\ngbdt_pre = gbdt.predict(x_test)\ngbdt_recall = recall_score(y_test,gbdt_pre)\ngbdt_precis = precision_score(y_test,gbdt_pre)\ncat_gbdt_f1 = f1_score(y_test,gbdt_pre)\nprint(\"gbdt recall:\",gbdt_recall)\nprint(\"gbdt precision:\",gbdt_precis)\nprint(\"gbdt f1_score:\",cat_gbdt_f1)","771bd54f":"cat_list = pd.DataFrame({\"one_hot_f1\":[one_svc_f1,one_logis_f1,one_rf_f1,one_gbdt_f1],\"cat_f1\":[cat_svc_f1,cat_logis_f1,cat_rf_f1,cat_gbdt_f1]},index=[\"svc\",\"logist\",\"rf\",\"gbdt\"])\ncat_list = cat_list.reset_index()","761cc9a5":"cat_list","e6ace7de":"import plotly.express as px\nimport plotly\nplotly.offline.init_notebook_mode(connected=True)\npx.bar(cat_list,x=\"index\",y=[\"one_hot_f1\",\"cat_f1\"],labels={\"index\":\"algorithm\"},barmode=\"group\")","b282fcca":"survived = gbdt.predict(df_test)\nsub = pd.DataFrame()\nsub[\"PassengerId\"] = test_id\nsub[\"Survived\"] = survived\nsub","b41e3f7a":"sub.to_csv('submission.csv',index=None)","5d35b34b":"- When we use one-hot encoding, the f1_score of svc and logistic regression algorithms slightly exceeds that of random forest and GBDT algorithms.\n- When we use catboost encoding, random forest and GBDT perform better.\n- We use the default parameters of the algorithm, and there is no parameter adjustment. If the parameters are adjusted, I think the effect of GBDT should be better.\n- If you use target encoding, the effect is the same as catboost encoding, and the effect of random forest and gbdt is better than one-hot encoding.","e969af90":"## Catboost coding and model","d8577310":"# EDA\n- View the value of the data\n- Compare the distribution of the data in the training set and the test set.","148d0f9f":"## One-hot encoding and model.","c384a80d":"# data processing\n- Deal with missing values, numerical values, realize different types of feature coding and use the same model to measure.\n- Category encoding mainly uses one-hot encoding, catboost encoding.\n- Feature selection, weed out highly relevant data.","4cdd1ac4":"- Let's submit the result and look at the score, I guess the score should be between 0.77-0.79.","df2cd111":"- For one-hot encoding, I used the default parameters of SVM, logistic regression, random forest, and GBDT. We found that the effect of SVM and logistic regression is better than random forest and GBDT. This may be for the tree model, one-hot encoding should be avoided as much as possible.","d9df10cf":"- Missing value processing","95bca1dd":"- We found that when using catboost encoding, the improvement of random forest algorithm and GBDT algorithm is quite significant.\n- This may prove that when we use random forest, GBDT, XGBoost, LightGBM and other tree model classification, we should probably reduce the use of one-hot encoding.\n- The effects of catboost encoding and target encoding are similar. Although the values are different, the respective values for the same category feature are the same, which is actually similar to label encoding.","09cbd85a":"- The missing value column is age and cabin. By comparing the data distribution of the training set and the test set, the data distribution of the two columns is basically the same, that is, there is no serious data distribution deviation.\n- Next, perform feature engineering. Deal with missing values, categorical data, and numerical data."}}