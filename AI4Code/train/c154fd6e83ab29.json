{"cell_type":{"356b880b":"code","83ab37d7":"code","cb1b5c23":"code","92c05ee5":"code","be284dd2":"code","7776b3f9":"code","798895e4":"code","11fae1d7":"code","6bec7078":"code","a24766fc":"code","76b74990":"code","2f8908f4":"code","aa3c5181":"code","0bdd8916":"code","2e649936":"code","ba0f2b28":"code","24558498":"code","f6d64066":"code","5088534a":"code","bf8a7e3f":"code","9e260178":"code","6f3cbbb9":"code","ba8571f1":"code","275fd2e3":"code","9fc1bed3":"code","0aea640b":"code","a704eca8":"code","d7350dae":"code","2eee3c56":"code","e0449021":"code","fc4b29bb":"code","5e59d391":"code","6ffb464c":"code","2781f08e":"code","cab8e605":"code","e4ef19a6":"code","7d799067":"code","e8b9bdbf":"code","eb9c9775":"code","fee6b3cf":"code","e4c64518":"code","4eb5f657":"code","a95498bc":"code","460e9c8b":"code","84bd934f":"code","5f6e7654":"code","9b0858f7":"code","90c99fbe":"code","a550f959":"code","a43732db":"code","7d60b51e":"code","98b6cb56":"code","092471ff":"code","04f9596a":"code","2b666f68":"code","090cc547":"code","ec118b4d":"code","0ee3b00a":"code","7b7c3811":"code","c528b27e":"code","332fae55":"code","92858270":"code","88788ee0":"code","1d0ad0ae":"code","42a29417":"code","666c3294":"code","7e29b29e":"code","0240ab02":"code","efc8f1bf":"code","a3c0ed88":"code","807bd9cd":"code","7e274b95":"code","83db0e9a":"code","aa50ff5d":"code","0801510f":"code","c31decde":"code","f1254485":"code","16fc6285":"code","6f538445":"code","644453a5":"code","f10d4c8c":"code","8cf8b032":"code","3b059883":"code","02277a79":"code","91f5e1dc":"code","919c3f26":"code","e6b28c39":"code","76fbb281":"code","f63f0adc":"code","6fa03130":"code","297a6c29":"code","93bf2c81":"code","16803b32":"code","1f0f3bd8":"code","09663a2a":"code","bc35587e":"code","962b19ba":"code","019f7e48":"code","75e4118b":"code","5d160b6c":"code","012f648e":"code","3ff40b56":"code","88aab527":"code","ac7a9220":"code","d146daf1":"code","95aa24c0":"markdown","09918462":"markdown","c21681e1":"markdown","4932e94a":"markdown","b9c5944f":"markdown","649ef7e9":"markdown","ec168434":"markdown","bb422d53":"markdown","2fb5a9c5":"markdown","dff5854a":"markdown","9107ccd1":"markdown","795673f2":"markdown","db372854":"markdown","6b204a27":"markdown","5c809b40":"markdown","7e9e0627":"markdown","5d432bfb":"markdown","dd655039":"markdown","9cb5fbfb":"markdown","7952a3bc":"markdown","96c94161":"markdown","57779007":"markdown","c1893f6b":"markdown","9ec2a26c":"markdown","1b0901e1":"markdown","a5ec5a3a":"markdown","782974b3":"markdown","5ef0745e":"markdown","c44cb0bf":"markdown","b21ca9d1":"markdown","5fe45ac6":"markdown","60c23e18":"markdown","f2e7bc45":"markdown","a023bd27":"markdown","1284e462":"markdown","debfec01":"markdown","c3c199ac":"markdown","e91af917":"markdown","867deb5d":"markdown","b6ed71fe":"markdown","42fc67db":"markdown","719d6c69":"markdown","a3750ca7":"markdown","e8094e09":"markdown","3e194954":"markdown","f2b13c20":"markdown","258eda17":"markdown","28d5644a":"markdown","1530220a":"markdown","f698aaf8":"markdown","83e9d4d4":"markdown","cacca49f":"markdown","409089b3":"markdown","82be829d":"markdown","4cbde97c":"markdown","d8bdbd5f":"markdown","07fc78df":"markdown","434a72a9":"markdown","012dbf8f":"markdown","f0f48ade":"markdown","cf76927b":"markdown","9198f806":"markdown","a9ba92a7":"markdown","e2f8a76c":"markdown","6cd9c0f6":"markdown","6a0e07d6":"markdown","c28cec2b":"markdown","cc22e4eb":"markdown","cfd82d27":"markdown","17938570":"markdown","6bb233d5":"markdown","80104730":"markdown","3bab46c2":"markdown","630c97c8":"markdown","646ac68e":"markdown","dda6f101":"markdown","fb546fef":"markdown","28c52408":"markdown","047c4653":"markdown"},"source":{"356b880b":"# HTML stuff\nfrom IPython.display import HTML\nfrom IPython.display import display\n# data structure\nimport pandas as pd\nimport numpy as np\n# plotting\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\n# sklearn\n\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.model_selection import ShuffleSplit, RepeatedKFold, train_test_split, GridSearchCV,  cross_val_score\n# loading data \ntrain_df = pd.read_csv('\/kaggle\/input\/titanic\/train.csv')\ntest_df = pd.read_csv('\/kaggle\/input\/titanic\/test.csv');\ncombined_df= pd.concat([train_df, test_df]) ","83ab37d7":"combined_df.info()","cb1b5c23":"train_df.head()","92c05ee5":"train_df.info()","be284dd2":"test_df.info()","7776b3f9":"train_df.describe()","798895e4":"# find categorical values\nfor col in train_df.columns:\n     if (train_df[col].nunique() < 10):\n        print(f\"{col}: {train_df[col].unique()}\")","11fae1d7":"# colore settings:\nsns.set(rc={'axes.facecolor':\"#EBE0BA\",\n            \"figure.facecolor\":\"#E0D3AF\",\n            \"grid.color\":\"#E0D3AF\",\n            \"axes.edgecolor\":\"#424949\",\n            \"axes.labelcolor\":\"#424949\",\n            \"text.color\":\"#424949\" # color for headlines and sub headlines\n           }) \n\n# font size settings\nsns.set_context(rc={\"axes.labelsize\" : 15})\n\n# Times New Roman: (newspaper look)\nplt.rcParams['font.family'] = 'serif'\nplt.rcParams['font.serif'] = ['Times New Roman'] + plt.rcParams['font.serif']","6bec7078":"# Let us first take a general look at the numeric functions\n\nsurvival_pal = {0: \"#943126\", 1:\"#1D8348\"}\nscreening_df = train_df.copy()\ng = sns.pairplot(screening_df, hue=\"Survived\", palette = survival_pal ,height=2, aspect=1.63)\ng.map_lower(sns.kdeplot, levels=4, color=\"#424949\")\ng.fig.subplots_adjust(top=0.90)\ng.fig.suptitle(\"Pairplot for numeric features\", fontsize=\"28\");","a24766fc":"# What was the overall chance to survive?\nprint(f\"Overall probability to survive: {round(len(screening_df[screening_df['Survived'] == 1]) \/ len(screening_df), 3) * 100} %\")","76b74990":"# What was the price range in the different classes?\n\n# build figure\nfig = plt.figure(figsize=(25,5))\n\n# add grid to figure\ngs = fig.add_gridspec(1,3)\n\n# fill grid with subplots\nax0 = fig.add_subplot(gs[0,0])\nax1 = fig.add_subplot(gs[0,1])\nax2 = fig.add_subplot(gs[0,2])\n\n# adjust subheadline fontsize\nax0.set_title('Pclass = 1', fontsize=20)\nax1.set_title('Pclass = 2', fontsize=20)\nax2.set_title('Pclass = 3', fontsize=20)\n\n# adjust lable fontsize\nax0.tick_params(labelsize=15)\nax1.tick_params(labelsize=15)\nax2.tick_params(labelsize=15)\n\n# plot data into subplots \nsns.kdeplot(data=screening_df[screening_df['Pclass']==1], x=\"Fare\", color=\"#1A5276\", fill = True, ax=ax0, linewidth = 3, ec=\"#424949\").set(xlabel=\"Fare\", ylabel=\"\")\nsns.kdeplot(data=screening_df[screening_df['Pclass']==2], x=\"Fare\", color=\"#1A5276\", fill = True, ax=ax1, linewidth = 3, ec=\"#424949\").set(xlabel=\"Fare\", ylabel=\"\")\nsns.kdeplot(data=screening_df[screening_df['Pclass']==3], x=\"Fare\", color=\"#1A5276\", fill = True, ax=ax2, linewidth = 3, ec=\"#424949\").set(xlabel=\"Fare\", ylabel=\"\")\n\n# add headline\nfig.subplots_adjust(top=0.8)\nfig.suptitle('Fare-Distribution for each class', fontsize=\"28\");","2f8908f4":"# boxplot for 2. look\n\n# build figure\nfig = plt.figure(figsize=(25.5,3))\n\n# add grid to figure\ngs = fig.add_gridspec(1,3)\n\n# fill grid with subplots\nax0 = fig.add_subplot(gs[0,0])\nax1 = fig.add_subplot(gs[0,1])\nax2 = fig.add_subplot(gs[0,2])\n\n# adjust subheadline fontsize\nax0.set_title('Pclass = 1', fontsize=20)\nax1.set_title('Pclass = 2', fontsize=20)\nax2.set_title('Pclass = 3', fontsize=20)\n\n# adjust lable fontsize\nax0.tick_params(labelsize=15)\nax1.tick_params(labelsize=15)\nax2.tick_params(labelsize=15)\n\n# plot data into subplots \nsns.boxplot(x=\"Fare\",data=screening_df[screening_df['Pclass']==1], orient=\"h\", color=\"#97A7B2\", ax=ax0, linewidth = 3, hue=\"Survived\").set(xlabel=\"Fare\")\nsns.boxplot(x=\"Fare\",data=screening_df[screening_df['Pclass']==2], orient=\"h\", color=\"#97A7B2\", ax=ax1, linewidth = 3, hue=\"Survived\").set(xlabel=\"Fare\")\nsns.boxplot(x=\"Fare\",data=screening_df[screening_df['Pclass']==3], orient=\"h\", color=\"#97A7B2\", ax=ax2, linewidth = 3, hue=\"Survived\").set(xlabel=\"Fare\")\n\n# add headline\nfig.subplots_adjust(top=0.7)\nfig.suptitle('Fare-Distribution for each class', fontsize=\"28\");","aa3c5181":"# What is the age distribution in each class and how is it related to the probability of survival?\n\nfig = plt.figure(figsize=(25, 4))\nax = sns.boxplot(x=\"Age\", y=\"Pclass\",hue=\"Survived\",data=screening_df, orient=\"h\", palette={0: \"#E6B0AA\", 1:\"#A9DFBF\"}, linewidth = 3)\nax.tick_params(labelsize=15)\n# add headline\nfig.subplots_adjust(top=0.8)\nfig.suptitle(\"Age\/Pclass and Survived\", fontsize=\"28\");","0bdd8916":"fg = sns.displot(\n    screening_df, x=\"Age\", col=\"Pclass\", row=\"Sex\", kde=True, palette=survival_pal, hue = 'Survived',\n    binwidth=3, height=4, facet_kws=dict(margin_titles=True), aspect=1.63, linewidth = 1)\nfg.set_xticklabels(fontsize=15)\n# change range for x axis\nplt.xlim(0, 85)\n# add headline\nfg.fig.subplots_adjust(top=0.85)\nfg.fig.suptitle('Age distribution per Pclass and Sex', fontsize=\"28\");","2e649936":"# calculate the survival probability per class and sex\nsex_class_prob_dict = {}\nfor n in ['male', 'female']:\n    for m in [1,2,3]:\n        df = screening_df[(screening_df['Pclass'] == m) & (screening_df['Sex'] == n)].copy()\n        sex_class_prob_dict[f\"{n} {str(m)}. class\"] = round(len(df[df['Survived'] == 1]) \/ len(df['Survived']),2)\n\n# let's write a function for the probability visualization. we will need it later on.\n\ndef probability_visualization(prob_dict, title):\n    df = pd.DataFrame.from_dict(prob_dict, orient='index').rename(columns={0: \"survival_probability\"})\n    df['label'] = df.index\n    fg = sns.catplot(data=df, kind=\"bar\", y=\"label\", x=\"survival_probability\", height=5, color=\"#97A7B2\",  aspect=3.9, linewidth = 3, ec=\"#424949\")\n    fg.set_xticklabels(fontsize=15)\n    fg.set_yticklabels(fontsize=15)\n    fg.fig.subplots_adjust(top=0.8)\n    fg.fig.suptitle(title, fontsize=\"28\");","ba0f2b28":"probability_visualization(prob_dict=sex_class_prob_dict, title='Probability of survival for Sex and Pclass')","24558498":"sex_dict = {'male': 0, 'female': 1}\nembarked_dict = {'S': 1, 'C': 2, 'Q': 3}\nscreening_df = screening_df[screening_df['Embarked'].notnull()].copy()\nscreening_df.loc[:, 'Sex'] = screening_df.loc[:,'Sex'].map(lambda x: sex_dict[x])\nscreening_df.loc[:, 'Embarked'] = screening_df.loc[:, 'Embarked'].map(lambda x: embarked_dict[x])","f6d64066":"# claculate correlations:\ncorr = screening_df.corr()\n\nmask = np.triu(np.ones_like(corr, dtype=bool))\n\n# build figure\nf, ax = plt.subplots(figsize=(25, 15))\n# change x- and y-label size\nax.tick_params(axis='both', which='major', labelsize=15)\n# plot mast\nsns.heatmap(corr, mask=mask, cmap=\"coolwarm\", center=0, square=True, linewidths=1, linecolor=\"#424949\", annot=True, \n                cbar_kws={\"shrink\": 0.6}).set_title('Pairwise correlation', fontsize=\"28\");","5088534a":"# let's see if everyone has his own ticket.\ncombined_df['Ticket'].value_counts().nlargest(5)","bf8a7e3f":"combined_df[combined_df['Ticket'] == 'CA. 2343']","9e260178":"# split Ticket into two parts and move all numbers to Ticket_number if there is no space (' ') included. (workaround)\n# we continue using the screening_df since we want to have a look at the chance of survival in each group.\n\ntmp_df = screening_df['Ticket'].str.split(' ', 1, expand=True).copy()\ntmp_df[1][tmp_df[1].isnull()] = tmp_df[0]\ntmp_df[0][tmp_df[1] == tmp_df[0]] = 'no_prefix' \nscreening_df[['Ticket_prefix', 'Ticket_number']]= tmp_df","6f3cbbb9":"# top ten most common prefixes\nscreening_df['Ticket_prefix'].value_counts().nlargest(5)","ba8571f1":"# calculat survival probability for each group of Ticket_prefix\nprefix_prob_dict = {}\nfor n in ['no_prefix', 'PC', 'C.A.', 'STON\/O', 'A\/5', 'W.\/C.', 'CA.', 'SOTON\/O.Q.', 'A\/5.', 'SOTON\/OQ']:\n    df = screening_df[screening_df['Ticket_prefix'] == n].copy()\n    prefix_prob_dict[n] = round(len(df[df['Survived'] == 1]) \/ len(df['Survived']),2)\n\n# plot probabilities\nprobability_visualization(prob_dict=prefix_prob_dict, title='Probability of survival for each ticket prefix')","275fd2e3":"screening_df['Leading_ticket_numbers'] = screening_df['Ticket_number'].map(lambda x : x[0:3])\nscreening_df['Leading_ticket_numbers'].value_counts().nlargest(5) ","9fc1bed3":"# Calculate survival probabilities for each group with more than 15 members:\nnumber_groups = [x[0] for x in screening_df['Leading_ticket_numbers'].value_counts().items() if x[1] > 15]\ngroup_prob_dict = {}\nfor n in number_groups:\n    df = screening_df[screening_df['Leading_ticket_numbers'] == n].copy()\n    group_prob_dict[n] = round(len(df[df['Survived'] == 1]) \/ len(df['Survived']),2)\n\n# plot probabilities\nprobability_visualization(prob_dict=group_prob_dict, title='Probability of survival for each ticket number group')","0aea640b":"# We will use the combined_df for that\ntmp_df = combined_df['Ticket'].str.split(' ', 1, expand=True).copy()\ntmp_df[1][tmp_df[1].isnull()] = tmp_df[0]\ntmp_df[0][tmp_df[1] == tmp_df[0]] = 'no_prefix' \ncombined_df[['Ticket_prefix', 'Ticket_number']]= tmp_df\ncombined_df['Leading_ticket_numbers'] = combined_df['Ticket_number'].map(lambda x : x[0:3])\ncombined_df['First_ticket_numbers'] = combined_df['Ticket_number'].map(lambda x : x[0:1])\n#combined_df.loc[:, 'Pclass'] = combined_df['Pclass'].astype('string') \n\nprint(\"Is there a relation between the first digit of the ticket number and the Pclass?\")\nprint(95*\"_\")\nfor l in combined_df['First_ticket_numbers'].unique():\n    df = combined_df[combined_df['First_ticket_numbers'] == l].copy()\n    print(f\"First_ticket_number {l}: # 1. class ticket {len(df[df['Pclass']==1])} \/ # 2. class tickets {len(df[df['Pclass']==2])} \/ # 3. class tickets {len(df[df['Pclass']==3])}\")","a704eca8":"print(\"How many passengers are in groups of a certain sizes?\")\nprint(55*\"_\")\nfor g in [0, 1, 2, 3, 4, 5, 10, 15, 20]:\n    list_of_groups = [x[1] for x in combined_df['Leading_ticket_numbers'].value_counts().items() if x[1] > g]\n    print(f\"min group size {g}: includes {sum(list_of_groups)} \/ {len(combined_df)} (# groups: {len(list_of_groups)})\")","d7350dae":"# add fare per person and group size: (excluding child discounts)\nscreening_df['Fare_per_person'] = 0.\nscreening_df['Group_size'] = 0.\nfor index, row in screening_df.iterrows():\n    # using the combined_df for group size\n    group_size = combined_df['Ticket'].value_counts()[row['Ticket']]\n    screening_df.at[index, 'Fare_per_person'] = row['Fare'] \/ group_size\n    screening_df.at[index, 'Group_size'] = group_size","2eee3c56":"# Let's take a look at the Fare_per_person distribution.\n\n# build figure\nfig = plt.figure(figsize=(25,5))\n\n# add grid to figure\ngs = fig.add_gridspec(1,3)\n\n# add subplots to grid:\nax0 = fig.add_subplot(gs[0,0])\nax1 = fig.add_subplot(gs[0,1])\nax2 = fig.add_subplot(gs[0,2])\n\n# set title for each subplot:\nax0.set_title('Pclass = 1', fontsize=20)\nax1.set_title('Pclass = 2', fontsize=20)\nax2.set_title('Pclass = 3', fontsize=20)\n\n# change labelsize for each axis:\nax0.tick_params(labelsize=15)\nax1.tick_params(labelsize=15)\nax2.tick_params(labelsize=15)\n\n# set limits for x axis:\nax0.set_xlim(0, 100)\nax1.set_xlim(0, 50)\nax2.set_xlim(0, 30)\n\n# plot:\nsns.kdeplot(data=screening_df[screening_df['Pclass']==1], x=\"Fare\", color=\"#1A5276\", fill = True, ax=ax0, label=\"Fare\").set(xlabel=\"Fare\", ylabel=\"\")\nsns.kdeplot(data=screening_df[screening_df['Pclass']==1], x=\"Fare_per_person\", color=\"#935116\", fill = True, ax=ax0, label=\"Fare_per_person\", linewidth = 1).set(xlabel=\"\", ylabel=\"\")\nsns.kdeplot(data=screening_df[screening_df['Pclass']==2], x=\"Fare\", color=\"#1A5276\", fill = True, ax=ax1, label=\"Fare\").set(xlabel=\"Fare\", ylabel=\"\")\nsns.kdeplot(data=screening_df[screening_df['Pclass']==2], x=\"Fare_per_person\", color=\"#935116\", fill = True, ax=ax1, label=\"Fare_per_person\").set(xlabel=\"\", ylabel=\"\")\nsns.kdeplot(data=screening_df[screening_df['Pclass']==3], x=\"Fare\", color=\"#1A5276\", fill = True, ax=ax2, label=\"Fare\").set(xlabel=\"Fare\", ylabel=\"\")\nsns.kdeplot(data=screening_df[screening_df['Pclass']==3], x=\"Fare_per_person\", color=\"#935116\", fill = True, ax=ax2, label=\"Fare_per_person\").set(xlabel=\"\", ylabel=\"\")\n\n# add legend:\nax0.legend(facecolor=\"#E0D3AF\", edgecolor=\"#424949\")\nax1.legend(facecolor=\"#E0D3AF\", edgecolor=\"#424949\")\nax2.legend(facecolor=\"#E0D3AF\", edgecolor=\"#424949\")\n\n# add headline:\nfig.subplots_adjust(top=0.8)\nfig.suptitle('Fare-Distribution compared to Fare_per_person-Distribution for each class', fontsize=\"28\", color=\"#424949\");","e0449021":"# What about the Group_size distribution for each class?\n\nfg = sns.displot(\n    screening_df, x=\"Group_size\", col=\"Pclass\", palette=survival_pal, hue = 'Survived',\n    binwidth=1, height=4, aspect=1.67)\n\n# set label size for x and y labels\nfg.set_xticklabels(fontsize=15)\nfg.set_yticklabels(fontsize=15)\n\n# add headline\nfg.fig.subplots_adjust(top=0.8)\nfg.fig.suptitle('Group_size vs Survived in each Pclass', fontsize=\"28\", color=\"#424949\");","fc4b29bb":"# Before we plot the correlations again, we need to make sure that all the columns we want to inspect are numeric.\n# Since we want to examine our new feature \"Leading_TicketNumber\", we have to find suitable values for all non-convertible string values.\n\n# (The following 3 values are from the error message when initially trying to convert the entire Fare_per_person column to int.)\n\nprint(45* \"-\")\nprint(f\"# rows with Leading_ticket_numbers '2. : {len(screening_df[screening_df['Leading_ticket_numbers'] =='2. '])}\")\nprint(f\"# rows with Leading_ticket_numbers 'LIN: {len(screening_df[screening_df['Leading_ticket_numbers'] =='LIN'])}\")\nprint(f\"# rows with Leading_ticket_numbers Bas': {len(screening_df[screening_df['Leading_ticket_numbers'] =='Bas'])}\")\nprint(45* \"-\")","5e59d391":"# We are now looking for suitable integer values...\n# What about the class in these groups?\nprint(60* \"-\")\nprint(f\" Unique class values for Leading_ticket_numbers {'2. '}: {screening_df['Pclass'][screening_df['Leading_ticket_numbers'] =='2. '].unique()}\")\nprint(f\" Unique class values for Leading_ticket_numbers {'LIN'}: {screening_df['Pclass'][screening_df['Leading_ticket_numbers'] =='LIN'].unique()}\")\nprint(f\" Unique class values for Leading_ticket_numbers {'Bas'}: {screening_df['Pclass'][screening_df['Leading_ticket_numbers'] =='Bas'].unique()}\")\nprint(60* \"-\")","6ffb464c":"# Let's just use 301, 302 and 201 as Leading_ticket_numbers for these values then. \n# (We remember that the first number usually corresponded to the Pclass)\n\n# Are 301, 302 and 201 already taken?\nprint(45* \"-\")\nprint(f\"# rows with Leading_ticket_numbers 301: {len(screening_df[screening_df['Leading_ticket_numbers'] =='301'])}\")\nprint(f\"# rows with Leading_ticket_numbers 302: {len(screening_df[screening_df['Leading_ticket_numbers'] =='302'])}\")\nprint(f\"# rows with Leading_ticket_numbers 201: {len(screening_df[screening_df['Leading_ticket_numbers'] =='201'])}\")\nprint(45* \"-\")","2781f08e":"# Looks good. Let's adjust the values and cast Leading_ticket_number.\nscreening_df.loc[screening_df['Leading_ticket_numbers'] =='2. ', 'Leading_ticket_numbers']  = 301\nscreening_df.loc[screening_df['Leading_ticket_numbers'] =='LIN', 'Leading_ticket_numbers']  = 301\nscreening_df.loc[screening_df['Leading_ticket_numbers'] =='Bas', 'Leading_ticket_numbers']  = 201\n\n# cast Leading_ticket_number to int:\nscreening_df['Leading_ticket_numbers'] = screening_df['Leading_ticket_numbers'].astype(int)\n# works... ","cab8e605":"# building feature\nscreening_df.loc[screening_df['Age'] <= 9, 'Child'] = 1\nscreening_df.loc[screening_df['Age'] > 9, 'Child'] = 0","e4ef19a6":"# claculate probabilites for survival\nchild_prob_dict = {}\nfor n in [0, 1]:\n    for p in [1, 2, 3]:\n        df = screening_df[(screening_df['Child'] == n) & (screening_df['Pclass'] == p)].copy()\n        if n == 1:\n            dict_key= f\"child_{p}_class\"\n        else:\n            dict_key= f\"adult_{p}_class\"\n        child_prob_dict[dict_key] = round(len(df[df['Survived'] == 1]) \/ len(df['Survived']),2)\n        \n# plot probabilities\nprobability_visualization(prob_dict=child_prob_dict, title='Probability of survival for children compared to adults in each class.')","7d799067":"# calculate correlations\ncorr_2 = screening_df.corr()\n\n# plot correlations\nmask = np.triu(np.ones_like(corr_2, dtype=bool))\nf, ax = plt.subplots(figsize=(25, 15))\nax.tick_params(axis='both', which='major', labelsize=15)\ng = sns.heatmap(corr_2, mask=mask, cmap=\"coolwarm\", center=0, square=True, linewidths=1, linecolor=\"#424949\", annot=True,\n            cbar_kws={\"shrink\": 0.6}).set_title('Pairwise correlation', fontsize=\"28\")","e8b9bdbf":"def copy_df(df):\n    return df.copy()\n\ndef fill_age(df):\n    df.loc[:, \"Age\"] = df.groupby(['Pclass', 'Sex']).transform(lambda x: x.fillna(x.mean())) \n    return df\n\ndef drop_missing_embarked_rows(df): \n    return df[df['Embarked'].notnull()]\n\ndef fill_fare(df):\n    df.loc[:, \"Fare\"] = df.groupby(['Pclass', 'Sex']).transform(lambda x: x.fillna(x.mean())) \n    return df\n\ndef drop_columns(df, columns): \n    df.drop(columns=columns, inplace=True)\n    return df\n\ndef add_leading_ticket_number_feature(df):\n    tmp_df = df['Ticket'].str.split(' ', 1, expand=True).copy()\n    tmp_df[1][tmp_df[1].isnull()] = tmp_df[0]\n    tmp_df['Leading_ticket_numbers'] = tmp_df[1].map(lambda x : x[0:3])\n    df['Leading_ticket_numbers'] = tmp_df['Leading_ticket_numbers'].copy()\n    return df\n\ndef cast_leading_ticket_number_to_int(df):\n    df.loc[df['Leading_ticket_numbers'] =='2. ', 'Leading_ticket_numbers']  = 301\n    df.loc[df['Leading_ticket_numbers'] =='LIN', 'Leading_ticket_numbers']  = 301\n    df.loc[df['Leading_ticket_numbers'] =='Bas', 'Leading_ticket_numbers']  = 201\n    df['Leading_ticket_numbers'] = df['Leading_ticket_numbers'].astype(int)\n    return df\n    \ndef add_group_size_feature(df):\n    df['Group_size'] = 0.\n    for index, row in df.iterrows():\n        df.at[index, 'Group_size'] =  combined_df['Ticket'].value_counts()[row['Ticket']]\n    return df\n        \ndef add_fare_per_person_feature(df):\n    df['Fare_per_person'] = 0.\n    for index, row in df.iterrows():\n        df.at[index, 'Fare_per_person'] = row['Fare'] \/ combined_df['Ticket'].value_counts()[row['Ticket']]\n    return df\n\ndef add_child_feature(df):\n    df.loc[df['Age'] <= 9, 'Child'] = 1\n    df.loc[df['Age'] > 9, 'Child'] = 0\n    df['Child'] = df['Child'].astype(int)\n    return df\n\ndef one_hot_encoding(df, column):\n    # Get one hot encoding of columns B\n    one_hot_df = pd.get_dummies(df[column])\n    # Drop input column as it is now encoded\n    df = df.drop(column, axis=1)\n    # Join the encoded df\n    df = df.join(one_hot_df)\n    return df\n\ndef norm_col(df, column):\n    df[column] = (df[column]-df[column].mean())\/df[column].std() \n    return df\n\ndef pipeline(df):\n    df = (df\n          .pipe(copy_df)\n          .pipe(fill_age)\n          .pipe(fill_fare)\n          .pipe(drop_missing_embarked_rows)\n          .pipe(add_group_size_feature)\n          .pipe(add_fare_per_person_feature)\n          .pipe(add_child_feature)\n          .pipe(add_leading_ticket_number_feature)\n          .pipe(cast_leading_ticket_number_to_int)\n          .pipe(one_hot_encoding, \"Pclass\")\n          .pipe(one_hot_encoding, \"Sex\")\n          .pipe(one_hot_encoding, \"Embarked\")\n          .pipe(norm_col, \"Age\")\n          .pipe(norm_col, \"Fare_per_person\")\n          .pipe(norm_col, \"Leading_ticket_numbers\")\n          .pipe(norm_col, \"Group_size\")\n          .pipe(drop_columns, ['Name', 'PassengerId', 'Cabin', 'Ticket', 'SibSp', 'Fare']))\n    return df","eb9c9775":"# let's have a look...\ntrain_df.head()","fee6b3cf":"pipeline(train_df).head()","e4c64518":"X_y = pipeline(train_df)\ny = X_y['Survived'].copy()\nX = drop_columns(X_y, 'Survived')\n\nX_train, X_test, y_train, y_test = train_test_split(X,y, stratify = y)","4eb5f657":"print(\"probabilitiey for survival in y_train:\")\nprint(y_train.value_counts(normalize=True))\nprint(\"probabilitiey for survival in y_test:\")\nprint(y_test.value_counts(normalize=True))","a95498bc":"# build model\ngbc = GradientBoostingClassifier(n_estimators=2000, learning_rate=0.01,\n     max_depth=3, verbose=0, random_state=1)\n# train model\ngbc.fit(X_train, y_train)\nprint(\"Performance on train data:\", gbc.score(X_train, y_train))\nprint(\"Performance on test data:\", gbc.score(X_test, y_test))","460e9c8b":"def feature_importancy_plot(model, data):\n    feat_imp = pd.Series(model.feature_importances_, data.columns).sort_values(ascending=False)\n    fig = plt.figure(figsize=(25,5))\n    ax = feat_imp.plot(kind='bar', color=\"#97A7B2\", linewidth = 3, ec=\"#424949\")\n    ax.tick_params(labelrotation=45, axis=\"x\")\n    ax.tick_params(labelcolor=\"#424949\", labelsize=15, axis=\"both\")\n    fig.subplots_adjust(top=0.8)\n    fig.suptitle('Feature Importances', fontsize=\"28\", color=\"#424949\");\n    plt.ylabel('Feature Importance Score', color=\"#424949\")\n    plt.xlabel('Features', color=\"#424949\");","84bd934f":"feature_importancy_plot(model=gbc, data=X_train)","5f6e7654":"param_test = {'max_depth':range(3,21,2),\n              'min_samples_split':range(10, 51, 5),\n              'subsample': [0.7, 0.8, 0.9, 1.0]}\n\ngsearch = GridSearchCV(estimator = GradientBoostingClassifier(learning_rate=0.01,\n                                                              n_estimators=500,\n                                                              max_features='sqrt',\n                                                              random_state=1), \nparam_grid = param_test, scoring='roc_auc',n_jobs=4, cv=5)\n\n# This takes around 20 mins on the kaggle kernal.\n#gsearch.fit(X,y)\n#gsearch.best_params_, gsearch.best_score_\n\n# here is the result:\nprint(\"({'max_depth': 13, 'min_samples_split': 10, 'subsample': 0.9}, 0.8905325516361675)\")","9b0858f7":"gbc2 = GradientBoostingClassifier(\n    n_estimators=500,\n    learning_rate=0.01,\n    max_depth=13,\n    verbose=0,\n    min_samples_split=10,\n    subsample=0.9,\n    max_features='sqrt')\ngbc2.fit(X_train, y_train)\nprint(\"Performance on train data:\", gbc2.score(X_train, y_train))\nprint(\"Performance on test data:\", gbc2.score(X_test, y_test))","90c99fbe":"feature_importancy_plot(model=gbc2, data=X_train)","a550f959":"gbc_submission = GradientBoostingClassifier(\n    n_estimators=500,\n    learning_rate=0.01,\n    max_depth=13,\n    verbose=0,\n    min_samples_split=10,\n    subsample=0.9,\n    max_features='sqrt')\ngbc_submission.fit(X, y);","a43732db":"feature_importancy_plot(model=gbc_submission, data=X)","7d60b51e":"# new submission:\nX_submission = pipeline(test_df)\ngbc_prediction = gbc_submission.predict(X_submission)\nsubmission = pd.DataFrame({\n        \"PassengerId\": test_df[\"PassengerId\"],\n        \"Survived\": gbc_prediction\n    })\n#submission.to_csv(\"submission_9.csv\", index=False)\n#print(\"Submission successful\")","98b6cb56":"# comaring 'Age', 'Pclass' 'Sex' and 'Fare_per_person':\nval_train_df = (train_df\n                .pipe(copy_df)\n                .pipe(fill_age)\n                .pipe(fill_fare)\n                .pipe(add_group_size_feature)\n                .pipe(add_fare_per_person_feature)\n                .pipe(drop_columns, ['Name', 'PassengerId', 'Cabin', 'Ticket','Fare']))\nval_test_df = (test_df\n                .pipe(copy_df)\n                .pipe(fill_age)\n                .pipe(fill_fare)\n                .pipe(add_group_size_feature)\n                .pipe(add_fare_per_person_feature)\n                .pipe(drop_columns, ['Name', 'PassengerId', 'Cabin', 'Ticket','Fare']))","092471ff":"print(\"probabilitiey for survival in y_train:\")\nprint(val_test_df['Sex'].value_counts(normalize=True))\nprint(\"probabilitiey for survival in y_test:\")\nprint(val_train_df['Sex'].value_counts(normalize=True))","04f9596a":"# build figure:\nfig = plt.figure(figsize=(25.5,10))\n\n# add grid to figure:\ngs = fig.add_gridspec(2,4) # since we have 8 features\n\n# fill grid with subplots:\nax00 = fig.add_subplot(gs[0,0])\nax01 = fig.add_subplot(gs[0,1])\nax02 = fig.add_subplot(gs[0,2])\nax03 = fig.add_subplot(gs[0,3])\nax10 = fig.add_subplot(gs[1,0])\nax11 = fig.add_subplot(gs[1,1])\nax12 = fig.add_subplot(gs[1,2])\nax13 = fig.add_subplot(gs[1,3])\n\n# build list with axis informations:\nax_list = [{\"axis\": ax00,\"feature\":\"Pclass\",\"is_discret\": True},\n           {\"axis\": ax01,\"feature\":\"Age\",\"is_discret\": False},\n           {\"axis\": ax02,\"feature\":\"Sex\",\"is_discret\": True},\n           {\"axis\": ax03,\"feature\":\"SibSp\",\"is_discret\": True},\n           {\"axis\": ax10,\"feature\":\"Parch\",\"is_discret\": True},\n           {\"axis\": ax11,\"feature\":\"Embarked\",\"is_discret\": True},\n           {\"axis\": ax12,\"feature\":\"Group_size\",\"is_discret\": True},\n           {\"axis\": ax13,\"feature\":\"Fare_per_person\",\"is_discret\": False}]\n\n# plot:\nfor ax in ax_list:\n    ax[\"axis\"].set_title(ax[\"feature\"], fontsize=20)\n    ax[\"axis\"].tick_params(labelsize=15)\n    df_test = pd.DataFrame(val_test_df[ax[\"feature\"]].value_counts(normalize=True))\n    df_test[\"data\"] = \"test\"\n    df_train = pd.DataFrame(val_train_df[ax[\"feature\"]].value_counts(normalize=True))\n    df_train[\"data\"] = \"train\"\n    df = pd.concat([df_test, df_train])\n    df.reset_index(inplace=True)\n    if ax[\"is_discret\"]:\n        sns.barplot(x=\"index\", y=ax[\"feature\"], hue=\"data\", data=df, ax=ax[\"axis\"], palette=\"Set2\").set(xlabel=\"\", ylabel=\"\")\n    else:\n        sns.kdeplot(data=df, x=\"index\", fill=True, ax=ax[\"axis\"], label=\"Fare\", hue=\"data\", palette=\"Set2\").set(xlabel=\"\", ylabel=\"\")\n\n# add headline\nfig.subplots_adjust(top=0.9)\nfig.suptitle(\"Difference between test and training data distribution\", fontsize=\"28\");","2b666f68":"# build figure\nfig = plt.figure(figsize=(25,5))\n\n# add grid to figure\ngs = fig.add_gridspec(1,3)\n\n# add subplots to grid:\nax0 = fig.add_subplot(gs[0,0])\nax1 = fig.add_subplot(gs[0,1])\nax2 = fig.add_subplot(gs[0,2])\n\n# set title for each subplot:\nax0.set_title('Pclass = 1', fontsize=20)\nax1.set_title('Pclass = 2', fontsize=20)\nax2.set_title('Pclass = 3', fontsize=20)\n\n# change labelsize for each axis:\nax0.tick_params(labelsize=15)\nax1.tick_params(labelsize=15)\nax2.tick_params(labelsize=15)\n\n# set limits for x axis:\nax0.set_xlim(0, 100)\nax1.set_xlim(0, 50)\nax2.set_xlim(0, 30)\n\n# plot:\nsns.kdeplot(data=val_train_df[val_train_df['Pclass']==1], x=\"Fare_per_person\", color=\"#1A5276\", fill = True, ax=ax0, label=\"Fare_per_person_train\").set(xlabel=\"\", ylabel=\"\")\nsns.kdeplot(data=val_test_df[val_test_df['Pclass']==1], x=\"Fare_per_person\", color=\"#935116\", fill = True, ax=ax0, label=\"Fare_per_person_test\", linewidth = 1).set(xlabel=\"\", ylabel=\"\")\nsns.kdeplot(data=val_train_df[val_train_df['Pclass']==2], x=\"Fare_per_person\", color=\"#1A5276\", fill = True, ax=ax1, label=\"Fare_per_person_train\").set(xlabel=\"\", ylabel=\"\")\nsns.kdeplot(data=val_test_df[val_test_df['Pclass']==2], x=\"Fare_per_person\", color=\"#935116\", fill = True, ax=ax1, label=\"Fare_per_person_test\").set(xlabel=\"\", ylabel=\"\")\nsns.kdeplot(data=val_train_df[val_train_df['Pclass']==3], x=\"Fare_per_person\", color=\"#1A5276\", fill = True, ax=ax2, label=\"Fare_per_person_train\").set(xlabel=\"\", ylabel=\"\")\nsns.kdeplot(data=val_test_df[val_test_df['Pclass']==3], x=\"Fare_per_person\", color=\"#935116\", fill = True, ax=ax2, label=\"Fare_per_person_test\").set(xlabel=\"\", ylabel=\"\")\n\n# add legend:\nax0.legend(facecolor=\"#E0D3AF\", edgecolor=\"#424949\")\nax1.legend(facecolor=\"#E0D3AF\", edgecolor=\"#424949\")\nax2.legend(facecolor=\"#E0D3AF\", edgecolor=\"#424949\")\n\n# add headline:\nfig.subplots_adjust(top=0.8)\nfig.suptitle('Fare-Distribution compared to Fare_per_person-Distribution for each class', fontsize=\"28\", color=\"#424949\");","090cc547":"# Let's write a function that extracts the salutations from a name\ndef test_salutation_from_name(name):\n    first_comma_pos = name.find(\",\")\n    first_dot_pos = name.find(\".\")\n    return name[first_comma_pos+1:first_dot_pos]\n\n# let's have a look:\ntmp_df = train_df.copy()\ntmp_df[\"Salutation\"] = tmp_df[\"Name\"].map(lambda x: test_salutation_from_name(x))\nfig = plt.figure(figsize=(25,5))\nsns.barplot(x=tmp_df[\"Salutation\"].value_counts().index, y=tmp_df[\"Salutation\"].value_counts(),  data=tmp_df, palette=\"Set2\").set(ylabel=\"Count\")\nfig.axes[0].tick_params(labelsize=15)\nfig.subplots_adjust(top=0.9)\nfig.suptitle('Salutations', fontsize=\"28\");","ec118b4d":"# new pipeline function:\ndef add_salutation_feature(df):\n    def get_salutation_from_name(name):\n        first_comma_pos = name.find(\",\")\n        first_dot_pos = name.find(\".\")\n        salutation = name[first_comma_pos+1:first_dot_pos].strip()\n        if salutation not in [\"Mr\", \"Miss\", \"Mrs\", \"Master\"]:\n            salutation = \"unknown\"\n        return salutation\n    df[\"Salutation\"]=df[\"Name\"].map(lambda x: get_salutation_from_name(x))\n    return df","0ee3b00a":"# let's check..\ntmp_df = (train_df\n          .pipe(copy_df)\n          .pipe(add_salutation_feature))\ntmp_df[[\"Name\", \"Salutation\"]].head()","7b7c3811":"# We will not care about NaN since we will fill those before calling this function.\ndef add_age_bucket_feature(df):\n    def get_age_bucket(number):\n        if int(number) in range(0,10):\n            bucket=\"age_0-9\"\n        elif int(number) in range(10,20):\n            bucket=\"age_10-19\"\n        elif int(number) in range(20,30):\n            bucket=\"age_20-29\"\n        elif int(number) in range(30,40):\n            bucket=\"age_30-39\"\n        elif int(number) in range(40,50):\n            bucket=\"age_40-49\"\n        elif int(number) in range(50,60):\n            bucket=\"age_50-59\"\n        elif int(number) in range(60,70):\n            bucket=\"age_60-69\"\n        elif int(number) in range(70,110):\n            bucket=\"age_70+\"\n        else:\n            bucket=\"unknown\"\n        return bucket\n    \n    df[\"Age_bucket\"]=df[\"Age\"].map(lambda x: get_age_bucket(x))\n    return df","c528b27e":"# let's check..\ntmp_df = (train_df\n          .pipe(copy_df)\n          .pipe(fill_age)\n          .pipe(add_age_bucket_feature))\ntmp_df[[\"Age\", \"Age_bucket\"]].head()","332fae55":"# let's have a look:\nfig = plt.figure(figsize=(25,5))\nsns.barplot(x=tmp_df[\"Age_bucket\"].value_counts().index, y=tmp_df[\"Age_bucket\"].value_counts(),  data=tmp_df, palette=\"Set2\").set(ylabel=\"Count\")\nfig.axes[0].tick_params(labelsize=15)\nfig.subplots_adjust(top=0.9)\nfig.suptitle('Age_bucket', fontsize=\"28\");","92858270":"def add_fpp_bucket_feature(df):\n    def get_fpp_bucket(number):\n        if int(number) in range(0,5):\n            bucket=\"fpp_0-5\"\n        elif int(number) in range(5,10):\n            bucket=\"fpp_5-10\"\n        elif int(number) in range(10,15):\n            bucket=\"fpp_10-15\"\n        elif int(number) in range(15,20):\n            bucket=\"fpp_15-20\"\n        elif int(number) in range(20,30):\n            bucket=\"fpp_20-30\"\n        elif int(number) in range(30,40):\n            bucket=\"fpp_30-40\"\n        elif int(number) in range(40,50):\n            bucket=\"fpp_40_50\"\n        elif int(number) in range(50,60):\n            bucket=\"fpp_50-60\"\n        elif int(number) in range(60,500):\n            bucket=\"fpp_60+\"\n        else:\n            bucket=\"unknown\"\n        return bucket\n    \n    df[\"Fpp_bucket\"]=df[\"Fare_per_person\"].map(lambda x: get_fpp_bucket(x))\n    return df","88788ee0":"# let's check..\ntmp_df = (train_df\n          .pipe(copy_df)\n          .pipe(fill_fare)\n          .pipe(add_fare_per_person_feature)\n          .pipe(add_fpp_bucket_feature))\ntmp_df[[\"Fare_per_person\", \"Fpp_bucket\"]].head()","1d0ad0ae":"# let's have a look:\nfig = plt.figure(figsize=(25,5))\nsns.barplot(x=tmp_df[\"Fpp_bucket\"].value_counts().index, y=tmp_df[\"Fpp_bucket\"].value_counts(),  data=tmp_df, palette=\"Set2\").set(ylabel=\"Count\")\nfig.axes[0].tick_params(labelsize=15)\nfig.subplots_adjust(top=0.9)\nfig.suptitle('Fpp_bucket', fontsize=\"28\");","42a29417":"# let's quickly add a function to rename pclass values\n\ndef rename_pclass_features(df):\n    renaming_dict = {1.0: \"1_class\", 2.0: \"2_class\", 3.0: \"3_class\" }\n    df[\"Pclass\"] = df[\"Pclass\"].map(lambda x: renaming_dict[x])\n    return df","666c3294":"# let's check..\ntmp_df = (train_df\n          .pipe(copy_df)\n          .pipe(rename_pclass_features))\ntmp_df[\"Pclass\"].unique()","7e29b29e":"# def new pipeline function. We wont exclude SibSp this time.\ndef one_hot_pipeline(df):\n    df = (df\n          .pipe(copy_df)\n          .pipe(fill_age)\n          .pipe(fill_fare)\n          .pipe(add_age_bucket_feature)\n          .pipe(drop_missing_embarked_rows)\n          .pipe(add_group_size_feature)\n          .pipe(add_fare_per_person_feature)\n          .pipe(add_fpp_bucket_feature)\n          .pipe(add_child_feature)\n          .pipe(add_leading_ticket_number_feature)\n          .pipe(cast_leading_ticket_number_to_int)\n          .pipe(add_salutation_feature)\n          .pipe(rename_pclass_features)\n          .pipe(one_hot_encoding, \"Pclass\")\n          .pipe(one_hot_encoding, \"Salutation\")\n          .pipe(one_hot_encoding, \"Embarked\")\n          .pipe(one_hot_encoding, \"Fpp_bucket\")\n          .pipe(one_hot_encoding, \"Age_bucket\")\n          .pipe(norm_col, \"Group_size\")\n          .pipe(norm_col, \"Leading_ticket_numbers\")\n          .pipe(norm_col, \"Parch\")\n          .pipe(norm_col, \"SibSp\")\n          .pipe(drop_columns, ['Name', 'PassengerId', 'Cabin', 'Ticket', 'Fare', 'Fare_per_person', 'Age', \"Sex\"]))\n    return df","0240ab02":"# lets see..\ntmp_df = one_hot_pipeline(train_df)\ntmp_df","efc8f1bf":"# pipeline\nX_y = one_hot_pipeline(train_df)\ny = X_y['Survived'].copy()\nX = drop_columns(X_y, 'Survived')","a3c0ed88":"# lets modify the cross validation parameter this time:\ncv_splitter = ShuffleSplit(n_splits=7, test_size=0.4, random_state=0)\n\nparam_test = {'max_depth':range(3,21,2),\n              'min_samples_split':range(10, 51, 5),\n              'subsample': [0.7, 0.8, 0.9, 1.0]}\n\ngsearch = GridSearchCV(estimator=GradientBoostingClassifier(learning_rate=0.01,\n                                                              n_estimators=500,\n                                                              max_features='sqrt',\n                                                              random_state=1),\n                                                              param_grid=param_test, scoring='roc_auc',n_jobs=4, cv=cv_splitter)\n\n# This takes around 20 mins on the kaggle kernal.\n#gsearch.fit(X,y)\n#gsearch.best_params_, gsearch.best_score_\nprint(\"({'max_depth': 9, 'min_samples_split': 40, 'subsample': 0.8}, 0.8802166020703528)\")","807bd9cd":"# train \/ test split\nX_train, X_test, y_train, y_test = train_test_split(X,y, stratify = y)","7e274b95":"gbc3 = GradientBoostingClassifier(\n    n_estimators=500,\n    learning_rate=0.01,\n    max_depth=5,\n    verbose=0,\n    min_samples_split=40,\n    subsample=0.8,\n    max_features='sqrt')\ngbc3.fit(X_train, y_train)\nprint(\"Performance on train data:\", gbc3.score(X_train, y_train))\nprint(\"Performance on test data:\", gbc3.score(X_test, y_test))","83db0e9a":"feature_importancy_plot(model=gbc3, data=X_train)","aa50ff5d":"# train model on the whole dataset\ngbc_submission_model = GradientBoostingClassifier(\n    n_estimators= 500,\n    learning_rate=0.01,\n    max_depth=9,\n    verbose=0,\n    min_samples_split=40,\n    subsample=0.8,\n    max_features='sqrt')\ngbc_submission_model.fit(X, y);\n\n# new submission:\nX_submission = one_hot_pipeline(test_df)\ngbc_prediction = gbc_submission_model.predict(X_submission)\nsubmission = pd.DataFrame({\n        \"PassengerId\": test_df[\"PassengerId\"],\n        \"Survived\": gbc_prediction\n    })\n#submission.to_csv(\"submission_12.csv\", index=False)\n#print(\"Submission successful\")","0801510f":"# let's use our pipeline functions to take a closer look the Leading_ticket_number distribution\nltd_train_df = (train_df\n                .pipe(copy_df)\n                .pipe(add_leading_ticket_number_feature)\n                .pipe(cast_leading_ticket_number_to_int)\n                .pipe(drop_columns, [\"PassengerId\", \"Survived\", \"Pclass\", \"Name\", \"Sex\", \"Age\", \"SibSp\", \"Parch\", \"Ticket\", \"Fare\", \"Cabin\", \"Embarked\"]))\n\n# let's do the same thing for test_df but without dropping Survived \nltd_test_df = (test_df\n               .pipe(copy_df)\n               .pipe(add_leading_ticket_number_feature)\n               .pipe(cast_leading_ticket_number_to_int)\n               .pipe(drop_columns, [\"PassengerId\", \"Pclass\", \"Name\", \"Sex\", \"Age\", \"SibSp\", \"Parch\", \"Ticket\", \"Fare\", \"Cabin\", \"Embarked\"]))\n\n# merge both dfs:\nltd_train_df = pd.DataFrame(ltd_train_df.value_counts(normalize=True))\nltd_train_df.rename(columns={0:\"frequency\"}, inplace=True)\nltd_train_df[\"data\"] = \"train\"\nltd_test_df = pd.DataFrame(ltd_test_df.value_counts(normalize=True))\nltd_test_df.rename(columns={0:\"frequency\"}, inplace=True)\nltd_test_df[\"data\"] = \"test\"\n\nltd_train_df=ltd_train_df.sort_values(\"frequency\").reset_index()\nmost_frequent_numbers_train = list(ltd_train_df[\"Leading_ticket_numbers\"].tail(20).values);\n\nltd_test_df= ltd_test_df.sort_values(\"frequency\").reset_index()\nmost_frequent_numbers_test = list(ltd_test_df[\"Leading_ticket_numbers\"].tail(20).values);\n\nmost_frequent_numbers = set().union(most_frequent_numbers_train, most_frequent_numbers_test)\n\nltd_df = pd.concat([ltd_train_df, ltd_test_df])\n\n# Let's also calculate the survival probabilities for each element in most_frequent_numbers\nsurv_prob_df = (train_df\n                .pipe(copy_df)\n                .pipe(add_leading_ticket_number_feature)\n                .pipe(cast_leading_ticket_number_to_int))\ngroup_prob_dict = {}\nfor n in most_frequent_numbers:\n    df = surv_prob_df[surv_prob_df['Leading_ticket_numbers'] == n].copy()\n    group_prob_dict[n] = round(len(df[df['Survived'] == 1]) \/ len(df['Survived']),2)\n\nltd_df = ltd_df[ltd_df[\"Leading_ticket_numbers\"].isin(most_frequent_numbers)]\nltd_df[\"survival_probability\"] = ltd_df[\"Leading_ticket_numbers\"].map(lambda x: group_prob_dict[x])","c31decde":"# build figure\nfig, ax = plt.subplots(figsize=(25,5))\n\n# change labelsize for each axis:\nax.tick_params(labelsize=15)\n\n# Let's increase the frequency to get a better visual representation in a plot.\n# The exact frequency is less important here. It is the ratio that matters. Therefore this is not a problem.\nltd_df[\"frequency\"] = ltd_df[\"frequency\"]*10\n\n# plot:\nsns.barplot(x=\"Leading_ticket_numbers\", y=\"survival_probability\",  data=ltd_df, ax=ax, linewidth=2.5, facecolor=\"#E0D3AF\", edgecolor=\"#424949\")\nsns.barplot(x=\"Leading_ticket_numbers\", y=\"frequency\",  data=ltd_df, hue=\"data\", palette=\"Set2\", ax=ax, alpha=0.8).set(ylabel=\"Survival Probability\")\n\n# add headline:\nfig.subplots_adjust(top=0.9)\nfig.suptitle('Comparison of the most common ticket numbers and associated survival probabilities', fontsize=\"28\", color=\"#424949\");","f1254485":"# We need to keep in mind that not all Leading_ticket_numbers are included in bouth train and test data.\n# So let's build a funktion to ensure that the output of our pipeline has the same dimensions for train_df and test_df.\n\ndef ltn_one_hot_encoding(df, list_of_ltns):\n    def is_ltn(ltn, obj):\n        if (str(obj) == str(ltn)):\n            return 1\n        else:\n            return 0\n    for ltn in list_of_ltns:\n        df[f\"ltn_{ltn}\"] = df[\"Leading_ticket_numbers\"].map(lambda x: is_ltn(ltn, x))\n    return df","16fc6285":"# lets check\ntmp_df = (train_df\n          .pipe(copy_df)\n          .pipe(add_leading_ticket_number_feature)\n          .pipe(ltn_one_hot_encoding, [211, 310]))\ntmp_df.head()","6f538445":"# Let`s also add discret groups for our Group_size feature as new feature\n\ndef add_discret_group_size_feature(df):\n    def get_size(number):\n        if int(number) == 1:\n            label =\"singel\"\n        elif int(number) in range(2,5):\n            label=\"small\"\n        elif int(number) > 4:\n            label = \"big\"\n        return label\n    \n    df[\"Group_size_disc\"]=df[\"Group_size\"].map(lambda x: get_size(x))\n    return df","644453a5":"# so first of all we need a list of all Leading_ticket_numbers\ntmp_train_df = (train_df\n                .pipe(copy_df)\n                .pipe(add_leading_ticket_number_feature))\ntmp_test_df = (test_df\n               .pipe(copy_df)\n               .pipe(add_leading_ticket_number_feature))\nunique_ltns = set().union(tmp_train_df[\"Leading_ticket_numbers\"].unique(), tmp_test_df[\"Leading_ticket_numbers\"].unique());","f10d4c8c":"# define new pipeline\n\ndef one_hot_pipeline_2(df, ltns_list):\n    df = (df\n          .pipe(copy_df)\n          .pipe(fill_age)\n          .pipe(fill_fare)\n          .pipe(add_age_bucket_feature)\n          .pipe(drop_missing_embarked_rows)\n          .pipe(add_group_size_feature)\n          .pipe(add_discret_group_size_feature)\n          .pipe(add_fare_per_person_feature)\n          .pipe(add_fpp_bucket_feature)\n          .pipe(add_child_feature)\n          .pipe(add_salutation_feature)\n          .pipe(rename_pclass_features)\n          .pipe(add_leading_ticket_number_feature)\n          .pipe(ltn_one_hot_encoding, ltns_list)\n          .pipe(one_hot_encoding, \"Pclass\")\n          .pipe(one_hot_encoding, \"Salutation\")\n          .pipe(one_hot_encoding, \"Embarked\")\n          .pipe(one_hot_encoding, \"Fpp_bucket\")\n          .pipe(one_hot_encoding, \"Age_bucket\")\n          .pipe(one_hot_encoding, \"Group_size_disc\")\n          .pipe(drop_columns, [\"Name\", \"PassengerId\", \"Cabin\", \"Ticket\", \"Fare\",\n                               \"Fare_per_person\", \"Age\", \"Sex\", \"Leading_ticket_numbers\",\n                               \"Parch\", \"SibSp\", \"Group_size\"]))\n    return df","8cf8b032":"# pipeline\nX_y = one_hot_pipeline_2(train_df, unique_ltns)\ny = X_y['Survived'].copy()\nX = drop_columns(X_y, 'Survived')\n\n# train \/ test split\nX_train, X_test, y_train, y_test = train_test_split(X,y, stratify = y)","3b059883":"# Some grid searching again..\n\ncv_splitter = ShuffleSplit(n_splits=7, test_size=0.4, random_state=0)\n\nparam_test = {'max_depth':range(1,15,2),\n              'min_samples_split':range(30, 100, 10),\n              'subsample': [0,1, 0.2, 0.3, 0.5, 0.6, 0.7]}\n\ngsearch = GridSearchCV(estimator=GradientBoostingClassifier(learning_rate=0.01,\n                                                          n_estimators=1000,\n                                                          max_features='sqrt',\n                                                          random_state=1),\n                                                          param_grid=param_test,\n                                                          scoring='roc_auc',\n                                                          n_jobs=4, cv=cv_splitter)\n\n# This takes around 20 mins on the kaggle kernal.\n#gsearch.fit(X,y)\n#gsearch.best_params_, gsearch.best_score_\nprint(\"({'max_depth': 7, 'min_samples_split': 30, 'subsample': 0.2},0.8849810758116153)\")","02277a79":"# model training\ngbc4 = GradientBoostingClassifier(\n    n_estimators=1000,\n    learning_rate=0.01,\n    max_depth=7,\n    verbose=0,\n    min_samples_split=30,\n    subsample=0.2,\n    max_features='sqrt')\ngbc4.fit(X_train, y_train)\nprint(\"Performance on train data:\", gbc4.score(X_train, y_train))\nprint(\"Performance on test data:\", gbc4.score(X_test, y_test))","91f5e1dc":"# train model for final submission\n\ngbc_submission_model = GradientBoostingClassifier(\n    n_estimators=1000,\n    learning_rate=0.01,\n    max_depth=7,\n    verbose=0,\n    min_samples_split=30,\n    subsample=0.2,\n    max_features='sqrt')\ngbc_submission_model.fit(X, y);\n\n# next submission:\nX_submission = one_hot_pipeline_2(test_df, unique_ltns)\ngbc_prediction = gbc_submission_model.predict(X_submission)\nsubmission = pd.DataFrame({\n        \"PassengerId\": test_df[\"PassengerId\"],\n        \"Survived\": gbc_prediction\n    })\n#submission.to_csv(\"submission_15.csv\", index=False)\n#print(\"Submission successful\")","919c3f26":"# Let`s have a look at our feature_importancy_plot. But this we now have so many features let's adjust the plot a bit.\nfeat_imp = pd.Series(gbc_submission_model.feature_importances_, X.columns).sort_values(ascending=False)\nprint(\"Number of features: \", len(feat_imp))\nprint(\"Number of features with weight 0.0: \", len(feat_imp[feat_imp==0.0]))","e6b28c39":"def feature_importances_plot_2(model, data, num_features, headline=\"Feature Importances\"):  # function naming over 9000!!!\n    feat_imp = pd.Series(model.feature_importances_, data.columns).sort_values(ascending=False)\n    fig = plt.figure(figsize=(25,5))\n    ax = feat_imp.head(num_features).plot(kind='bar', color=\"#97A7B2\", linewidth = 3, ec=\"#424949\")\n    ax.tick_params(labelrotation=90, axis=\"x\")\n    ax.tick_params(labelcolor=\"#424949\", labelsize=10, axis=\"both\")\n    fig.subplots_adjust(top=0.85)\n    fig.suptitle(f'{headline} (Top {num_features} Features)', fontsize=\"28\", color=\"#424949\");\n    plt.ylabel('Feature Importance Score', color=\"#424949\")\n    plt.xlabel('Features', color=\"#424949\");","76fbb281":"feature_importances_plot_2(gbc_submission_model, X, 80)","f63f0adc":"def multi_pipeline(df):\n    df = (df\n          .pipe(copy_df)\n          .pipe(fill_age)\n          .pipe(fill_fare)\n          .pipe(add_age_bucket_feature)\n          .pipe(drop_missing_embarked_rows)\n          .pipe(add_group_size_feature)\n          .pipe(add_discret_group_size_feature)\n          .pipe(add_fare_per_person_feature)\n          .pipe(add_fpp_bucket_feature)\n          .pipe(add_child_feature)\n          .pipe(add_salutation_feature)\n          .pipe(rename_pclass_features)\n          .pipe(add_leading_ticket_number_feature)\n          .pipe(ltn_one_hot_encoding, unique_ltns)\n          .pipe(one_hot_encoding, \"Pclass\")\n          .pipe(one_hot_encoding, \"Embarked\")\n          .pipe(one_hot_encoding, \"Fpp_bucket\")\n          .pipe(one_hot_encoding, \"Age_bucket\")\n          .pipe(one_hot_encoding, \"Group_size_disc\")\n          .pipe(drop_columns, [\"Name\", \"PassengerId\", \"Cabin\", \"Ticket\", \"Fare\",\n                               \"Fare_per_person\", \"Age\", \"Sex\", \"Leading_ticket_numbers\",\n                               \"Parch\", \"SibSp\", \"Group_size\"]))\n    return df \nmulti_model_df = multi_pipeline(train_df)\nmulti_model_df[\"Salutation\"].value_counts()","6fa03130":"mr_y_train = multi_model_df[\"Survived\"][multi_model_df[\"Salutation\"]==\"Mr\"].copy()\nnot_mr_y_train = multi_model_df[\"Survived\"][multi_model_df[\"Salutation\"]!=\"Mr\"].copy()\n\nmulti_model_df = drop_columns(multi_model_df, \"Survived\")\n\nmr_train_df = multi_model_df[multi_model_df[\"Salutation\"]==\"Mr\"].copy()\nmr_train_df = drop_columns(mr_train_df, \"Salutation\")\n\nnot_mr_train_df = multi_model_df[multi_model_df[\"Salutation\"]!=\"Mr\"].copy()\nnot_mr_train_df = one_hot_encoding(not_mr_train_df, \"Salutation\")\n\n# train \/ test split\nX_mr_train, X_mr_test, y_mr_train, y_mr_test = train_test_split(mr_train_df, mr_y_train, stratify=mr_y_train)\nX_not_mr_train, X_not_mr_test, y_not_mr_train, y_not_mr_test = train_test_split(not_mr_train_df, not_mr_y_train, stratify=not_mr_y_train)","297a6c29":"# let's use our old parameter setting to start:\n\n# Train Mr. Model:\nmr_model = GradientBoostingClassifier(\n            n_estimators=1000,\n            learning_rate=0.01,\n            max_depth=7,\n            verbose=0,\n            min_samples_split=30,\n            subsample=0.2,\n            max_features='sqrt').fit(X_mr_train, y_mr_train);\n\nnot_mr_model = GradientBoostingClassifier(\n            n_estimators=1000,\n            learning_rate=0.01,\n            max_depth=7,\n            verbose=0,\n            min_samples_split=30,\n            subsample=0.2,\n            max_features='sqrt').fit(X_not_mr_train, y_not_mr_train);\n\nprint(\"Mr. model Performance: \")\nprint(\"Performance on train data:\", mr_model.score(X_mr_train, y_mr_train))\nprint(\"Performance on test data:\", mr_model.score(X_mr_test, y_mr_test))\nprint(50*\"-\")\nprint(\"Not-Mr. model Performance: \")\nprint(\"Performance on train data:\", not_mr_model.score(X_not_mr_train, y_not_mr_train))\nprint(\"Performance on test data:\", not_mr_model.score(X_not_mr_test, y_not_mr_test))","93bf2c81":"# lets have a look at the features of bouth models:\nfeature_importances_plot_2(mr_model, X_mr_train, 80, \"Mr-Model\")","16803b32":"feature_importances_plot_2(not_mr_model, X_not_mr_train, 80, \"Not-Mr-Model\")","1f0f3bd8":"param_test = {'max_depth':range(1,10,2),\n              'min_samples_split':range(5, 80, 5),\n              'subsample': [0.2, 0.3, 0.5, 0.6, 0.7, 0.8, 0.9],\n              'learning_rate':[0.005, 0.01,0.015, 0.02]}","09663a2a":"mr_gsearch = GridSearchCV(estimator=GradientBoostingClassifier(n_estimators=1000),\n                                                          param_grid=param_test,\n                                                          scoring='roc_auc',\n                                                          n_jobs=4, cv=cv_splitter)\n# this takes around 40 mins\n#mr_gsearch.fit(mr_train_df, mr_y_train)\n#mr_gsearch.best_params_, mr_gsearch.best_score_\nprint(\"({'learning_rate': 0.02, 'max_depth': 7,'min_samples_split': 45, 'subsample': 0.7}, 0.6998028136280837)\")","bc35587e":"not_mr_gsearch = GridSearchCV(estimator=GradientBoostingClassifier(n_estimators=1000),\n                                                          param_grid=param_test,\n                                                          scoring='roc_auc',\n                                                          n_jobs=4, cv=cv_splitter)\n\n# this takes around 40 mins\n#not_mr_gsearch.fit(not_mr_train_df, not_mr_y_train)\n#not_mr_gsearch.best_params_, not_mr_gsearch.best_score_\nprint(\"({'learning_rate': 0.005, 'max_depth': 3, 'min_samples_split': 5, 'subsample': 0.7}, 0.8612378030394865)\")","962b19ba":"not_mr_best_params = {'learning_rate': 0.005, 'max_depth': 3,'min_samples_split': 5, 'subsample': 0.7}\nmr_best_params = {'learning_rate': 0.02, 'max_depth': 7, 'min_samples_split': 45, 'subsample': 0.7}\nmr_best_params[\"n_estimators\"] = 1000\nnot_mr_best_params[\"n_estimators\"] = 1000\n\ndef model_evaluation(model, name):\n    cv = RepeatedKFold(n_splits=8, n_repeats=2)\n    if name==\"combined\":\n        scores = cross_val_score(model, X, y, cv=cv, n_jobs=-1)\n    elif name==\"mr\":\n        scores = cross_val_score(model, mr_train_df, mr_y_train, cv=cv, n_jobs=-1)\n    else:\n        scores = cross_val_score(model, not_mr_train_df, not_mr_y_train, cv=cv, n_jobs=-1)\n    return np.abs(scores)\n\nmr_model = GradientBoostingClassifier(**mr_best_params)\nnot_mr_best_params = GradientBoostingClassifier(**not_mr_best_params)\ncombined_model = GradientBoostingClassifier(n_estimators=1000, learning_rate=0.01,max_depth=7,\n                                            verbose=0, min_samples_split=30, subsample=0.2)\n\nmodels = {'mr': mr_model, \n          'not_mr': not_mr_best_params, \n          'combined': combined_model}\n\nscores = []\nnames = []\n\nfor name, model in models.items():\n    score = model_evaluation(model, name)\n    scores.append(score)\n    names.append(name)\nresult_dict = {names[i]: scores[i] for i in range(len(scores))}","019f7e48":"# plot results:\nresult_df = pd.DataFrame().from_dict(result_dict)\nf, ax = plt.subplots(figsize=(25, 5))\nax.tick_params(labelsize=15)\nsns.boxplot(data=result_df, orient=\"h\", palette=\"Set2\")\nsns.swarmplot(data=result_df, orient=\"h\", color=\".25\")\nf.subplots_adjust(top=0.85)\nf.suptitle('Model Performance Comparison', fontsize=\"28\");","75e4118b":"# Train model on entire data set:\nsubmission_mr_model = mr_model.fit(mr_train_df, mr_y_train);\n\nsubmission_not_mr_model = not_mr_best_params.fit(not_mr_train_df, not_mr_y_train);\nprint(\"Mr. model Performance: \")\nprint(\"Performance on train data:\", submission_mr_model.score(mr_train_df, mr_y_train))\nprint(50*\"-\")\nprint(\"Not-Mr. model Performance: \")\nprint(\"Performance on train data:\", submission_not_mr_model.score(not_mr_train_df, not_mr_y_train))","5d160b6c":"feature_importances_plot_2(submission_mr_model, mr_train_df, 80, \"Mr-Submission-Model\")","012f648e":"feature_importances_plot_2(submission_not_mr_model, not_mr_train_df, 80, \"Not-Mr-Submission-Model\")","3ff40b56":"# create submission df's\nmulti_model_submission_df = multi_pipeline(test_df)\n\nmr_multi_model_submission_df = multi_model_submission_df[multi_model_submission_df[\"Salutation\"]==\"Mr\"].copy()\nmr_multi_model_submission_df = drop_columns(mr_multi_model_submission_df, \"Salutation\")\n\nnot_mr_multi_model_submission_df = multi_model_submission_df[multi_model_submission_df[\"Salutation\"]!=\"Mr\"].copy()\nnot_mr_multi_model_submission_df = one_hot_encoding(not_mr_multi_model_submission_df, \"Salutation\");","88aab527":"# predicting\nmr_prediction = submission_mr_model.predict(mr_multi_model_submission_df)\nnot_mr_prediction = submission_not_mr_model.predict(not_mr_multi_model_submission_df)","ac7a9220":"# merge bouth predictions (a bit tricky...)\n\nsub_test_df = (test_df\n              .pipe(copy_df)\n              .pipe(drop_missing_embarked_rows)\n              .pipe(add_salutation_feature))\n\nsubmission_mr = pd.DataFrame({\n        \"PassengerId\": sub_test_df[sub_test_df[\"Salutation\"]==\"Mr\"][\"PassengerId\"],\n        \"Survived\": mr_prediction\n    })\nsubmission_not_mr = pd.DataFrame({\n        \"PassengerId\": sub_test_df[sub_test_df[\"Salutation\"]!=\"Mr\"][\"PassengerId\"],\n        \"Survived\": not_mr_prediction\n    })\nsubmission = pd.concat([submission_mr,submission_not_mr]).sort_index()\nprint(submission)","d146daf1":"submission.to_csv(\"multi_submission.csv\", index=False)\nprint(\"Submission successful\")","95aa24c0":"<a id=\"1.2\"><\/a>\n<div style=\"color:#424949;\n           display:fill;\n           text-align:left;\n           padding: 0.7em;\n           border-radius:10px;\n           border-style: solid;\n           border-color:#424949;\n           background-color:#E0D3AF;\n           font-size:15px;\n           font-family: Didot;\n           letter-spacing:0.5px\">  \n<h2> 1.2 Parameter Explanation (where needed) <\/h2>\n<hr>\n<ul>\n  <li>Pclass : Ticket class (1 = 1st, 2 = 2nd, 3 = 3rd)  <\/li>\n  <li>SibSp : # of siblings \/ spouses aboard the Titanic  <\/li>\n  <li>Parch : # of parents \/ children aboard the Titanic<\/li>\n  <li>Embarked : Port of Embarkation (C = Cherbourg, Q = Queenstown, S = Southampton)<\/li>\n<\/ul> \n<hr>    \n","09918462":"<div style=\"color:#1D8348;\n           display:fill;\n           padding: 5px;\n           border-radius:10px;\n           border-style: solid;\n           border-color: #A9DFBF;\n           background-color:#D5F5E3;\n           font-size: 15px;\n           font-family: Didot;\n           letter-spacing:0.5px\">\n<b>\ud83c\udf89 Score:<\/b> \n<div style=\"font-size: 30px;\n            padding: 0.7em;\"> \n    <b>78.7%<\/b> \n<\/div>\n\nAfter a few submissions, we see a similar performance....  ","c21681e1":"<div style=\"color:#9A7D0A;\n           display:fill;\n           padding: 5px;\n           border-radius:10px;\n           border-style: solid;\n           border-color: #F9E79F;\n           background-color:#FCF3CF;\n           font-size:15px;\n           font-family: Didot;\n           letter-spacing:0.5px\">\n<b>\ud83d\udcdd Note:<\/b> Now, when we include Sex to our inspection we can clearly see the correlation between Survived and the combination of Age and Sex.\nLet's have a look without Age and without total numbers.","4932e94a":"<div style=\"color:#21618C;\n           display:fill;\n           padding: 5px;\n           border-radius:10px;\n           border-style: solid;\n           border-color: #AED6F1;\n           background-color:#D6EAF8;\n           font-size:15px;\n           font-family: Didot;\n           letter-spacing:0.5px\">\n<b>\ud83d\udca1 Idea:<\/b> To tackle this issue we could add a new feature \"Fare_per_person\" later on. But first, we will take a closer look at Ticket-values in general. Are there some patterns we can work with?","b9c5944f":"<a id=\"1.3\"><\/a>\n<div style=\"color:#424949;\n           display:fill;\n           text-align:left;\n           padding: 0.7em;\n           border-radius:10px;\n           border-style: solid;\n           border-color:#424949;\n           background-color:#E0D3AF;\n           font-size:15px;\n           font-family: Didot;\n           letter-spacing:0.5px\">  \n<h2> 1.3 Value Distribution <\/h2>","649ef7e9":"<div style=\"color:#943126;\n           display:fill;\n           padding: 5px;\n           border-radius:10px;\n           border-style: solid;\n           border-color: #F5B7B1;\n           background-color:#FADBD8;\n           font-size:15px;\n           font-family: Didot;\n           letter-spacing:0.5px\">\n<b>\ud83d\uded1 Attention:<\/b> Every time I run the notebook, this output can change as the algorithm chooses a new random_state and thus a new random data split. So don't be surprised if you see something worse than above. I did my submission with 95% on training and 86% on test data. I tried to fix this by setting a fix random_state, but it didn't work. If you have a solution to this problem, let me know in the comment section. Thanks!","ec168434":"<div style=\"color:#9A7D0A;\n           display:fill;\n           padding: 5px;\n           border-radius:10px;\n           border-style: solid;\n           border-color: #F9E79F;\n           background-color:#FCF3CF;\n           font-size:15px;\n           font-family: Didot;\n           letter-spacing:0.5px\">\n<b>\ud83d\udcdd Note:<\/b> The number of groups is probably a bit too large for one hot encoding if we want to include as many passengers as possible. But since there could be an order here for the most part (firt digets = first class and so on in most cases), this should not bother us.","bb422d53":"<a id=\"6.3\"><\/a>\n<div style=\"color:#424949;\n           display:fill;\n           text-align:left;\n           padding: 0.7em;\n           border-radius:10px;\n           border-style: solid;\n           border-color:#424949;\n           background-color:#E0D3AF;\n           font-size:15px;\n           font-family: Didot;\n           letter-spacing:0.5px\">  \n<h2> 6.3 Evaluation <\/h2>\nSo what could be the reason for the difference between our test performance and the real result? Obviously, the model does not generalize as expected. One issue could be the overweighting of individual features like Leading_ticket_number and Mr. To fix this problem, we could train a different model for different salutations or reduce the weighting of these features for the model. But first, let's look at the distribution of Leading_ticket_number in our test and training data. We may gain some insights from this.","2fb5a9c5":"<div style=\"color:#797D7F;\n           display:fill;\n           padding: 5px;\n           border-radius:10px;\n           border-style: solid;\n           border-color: #E5E7E9;\n           background-color:#F2F3F4;\n           font-size:15px;\n           font-family: Didot;\n           letter-spacing:0.5px\">\n<b>\ud83d\udcad Thougts:<\/b> This doesn't look so bad. Let's do some grid search to see if we can find better parameters for this problem.","dff5854a":"<div style=\"color:#797D7F;\n           display:fill;\n           padding: 5px;\n           border-radius:10px;\n           border-style: solid;\n           border-color: #E5E7E9;\n           background-color:#F2F3F4;\n           font-size:15px;\n           font-family: Didot;\n           letter-spacing:0.5px\">\n<b>\ud83d\udcad Thougts:<\/b> This looks promising. The not_mr model seems to have some problems and the combined model looks better overall. So let's create a submission with our new combined model.","9107ccd1":"<div style=\"color:#1D8348;\n           display:fill;\n           padding: 5px;\n           border-radius:10px;\n           border-style: solid;\n           border-color: #A9DFBF;\n           background-color:#D5F5E3;\n           font-size: 15px;\n           font-family: Didot;\n           letter-spacing:0.5px\">\n<b>\ud83c\udf89 Score:<\/b> \n<div style=\"font-size: 30px;\n            padding: 0.7em;\"> \n    <b>78.9%<\/b> \n<\/div>\n\nThats not too bad, but i was hoping for a little bit more to be honest. But we still have a few arrows in our quiver. \ud83c\udff9","795673f2":"<a id=\"6.4\"><\/a>\n<div style=\"color:#424949;\n           display:fill;\n           text-align:left;\n           padding: 0.7em;\n           border-radius:10px;\n           border-style: solid;\n           border-color:#424949;\n           background-color:#E0D3AF;\n           font-size:15px;\n           font-family: Didot;\n           letter-spacing:0.5px\">  \n<h2> 6.4 New Feature \"Group_size_disc\"<\/h2>","db372854":"<div style=\"color:#9A7D0A;\n           display:fill;\n           padding: 5px;\n           border-radius:10px;\n           border-style: solid;\n           border-color: #F9E79F;\n           background-color:#FCF3CF;\n           font-size:15px;\n           font-family: Didot;\n           letter-spacing:0.5px\">\n<b>\ud83d\udcdd Note:<\/b> Let's take a look at the most important features for the algorithm. We will need this later as well, so let's write a function for it now.","6b204a27":"<a id=\"4.2\"><\/a>\n<div style=\"color:#424949;\n           display:fill;\n           text-align:left;\n           padding: 0.7em;\n           border-radius:10px;\n           border-style: solid;\n           border-color:#424949;\n           background-color:#E0D3AF;\n           font-size:15px;\n           font-family: Didot;\n           letter-spacing:0.5px\">  \n<h2> 4.2 First Model Training <\/h2>","5c809b40":"<a id=\"5.2\"><\/a>\n<div style=\"color:#424949;\n           display:fill;\n           text-align:left;\n           padding: 0.7em;\n           border-radius:10px;\n           border-style: solid;\n           border-color:#424949;\n           background-color:#E0D3AF;\n           font-size:15px;\n           font-family: Didot;\n           letter-spacing:0.5px\">  \n<h2> 5.2 New Feature \"Salutation\" <\/h2>\nWe have completely ignored the name of the passengers, but the name usually contains a salutation and this could be relevant. Let's add it as a feature.","7e9e0627":"<div style=\"color:#9A7D0A;\n           display:fill;\n           padding: 5px;\n           border-radius:10px;\n           border-style: solid;\n           border-color: #F9E79F;\n           background-color:#FCF3CF;\n           font-size:15px;\n           font-family: Didot;\n           letter-spacing:0.5px\">\n<b>\ud83d\udcdd Note:<\/b> The first three digits of each Ticket_number build groups of significant sizes.","5d432bfb":"<div style=\"color:#9A7D0A;\n           display:fill;\n           padding: 5px;\n           border-radius:10px;\n           border-style: solid;\n           border-color: #F9E79F;\n           background-color:#FCF3CF;\n           font-size:15px;\n           font-family: Didot;\n           letter-spacing:0.5px\">\n<b>\ud83d\udcdd Note:<\/b> We have missing values in Age and Cabin and one missing value for Fare in the test_df.   \n<\/div>","dd655039":"<a id=\"5.3\"><\/a>\n<div style=\"color:#424949;\n           display:fill;\n           text-align:left;\n           padding: 0.7em;\n           border-radius:10px;\n           border-style: solid;\n           border-color:#424949;\n           background-color:#E0D3AF;\n           font-size:15px;\n           font-family: Didot;\n           letter-spacing:0.5px\">  \n<h2> 5.3 New Feature \"Age_bucket\"<\/h2>\nLet's divide the age into buckets of 10 and use one hot encoding afterwards.","9cb5fbfb":"<a id=\"2.\"><\/a>\n<a id=\"2.1\"><\/a>\n<div style=\"color:#424949;\n           display:fill;\n           text-align:left;\n           padding: 0.7em;\n           border-radius:10px;\n           border-style: solid;\n           border-color:#424949;\n           background-color:#E0D3AF;\n           font-size:15px;\n           font-family: Didot;\n           letter-spacing:0.5px\">  \n<h1> 2. Feature Engineering <\/h1>\n    <hr>\n<h2> 2.1 New Feature \"Leading_ticket_numbers\"<\/h2>\nCabin could hold some information but most of the data is missing so we exclude this column. But is there something hidden in the ticket number?","7952a3bc":"<div style=\"color:#797D7F;\n           display:fill;\n           padding: 5px;\n           border-radius:10px;\n           border-style: solid;\n           border-color: #E5E7E9;\n           background-color:#F2F3F4;\n           font-size:15px;\n           font-family: Didot;\n           letter-spacing:0.5px\">\n<b>\ud83d\udcad Thougts:<\/b>  We can't see a direct connection between Age and Survived. Let's have a closer look...","96c94161":"### Table of Content\n* [1. Data screening And Visualisations](#1.)\n    - [1.1 First Look](#1.1)\n    - [1.2 Parameter Explanation (where needed)](#1.2)\n    - [1.3 Value Distribution](#1.3)\n    - [1.4 Correlations](#1.4)\n* [2. Feature Engineering](#2.)\n    - [2.1 New Feature \"Leading_ticket_numbers\"](#2.1)\n    - [2.2 New Features \"Fare_per_person\" Snd \"Group_size\"](#2.2)\n    - [2.3 New Feature \"Child\"](#2.3)\n    - [2.4 Correlations With Newly Created Features](#2.4)\n* [3. Data Preparation Using Pipe](#3.)\n    - [3.1 Summary Of Observations](#3.1)\n    - [3.2 Pipeline functions](#3.2)\n* [4. Model Training And Validation](#4.)\n    - [4.1 Simple Split](#4.1)\n    - [4.2 First Model Training](#4.2)\n    - [4.3 Grid Search](#4.3)\n    - [4.4 Submission](#4.4)\n* [5. Feature Engenearing And Data Validation 2.0](#5.)\n    - [5.1 Distribution Analysis](#5.1)\n    - [5.2 New Feature \"Salutation\"](#5.2)\n    - [5.3 New Feature \"Age_bucket\"](#5.3)\n    - [5.4 New Feature \"Fpp_bucket\"](#5.4)\n    - [5.5 New Pipeline](#5.5)\n* [6. New Training](#6.)  \n    - [6.1 Grid Search With ShuffleSplit Cross Validation](#6.1)\n    - [6.2 New Submission](#6.2)\n    - [6.3 Evaluation](#6.3)\n    - [6.4 New Feature \"Group_size_disc\"](#6.4)\n    - [6.5 Next Pipeline](#6.5)\n    - [6.6 Next Training](#6.6)\n    - [6.7 Next Submission](#6.7)\n* [7. Model Comparison And Data Splitting](#7.)\n    - [7.1 Training](#7.1)\n    - [7.2 Parameter Tuning](#7.2)\n    - [7.3 Submission](#7.3)\n* [8. Final Thoughts](#8.)","57779007":"<div style=\"color:#9A7D0A;\n           display:fill;\n           padding: 5px;\n           border-radius:10px;\n           border-style: solid;\n           border-color: #F9E79F;\n           background-color:#FCF3CF;\n           font-size:15px;\n           font-family: Didot;\n           letter-spacing:0.5px\">\n<b>\ud83d\udcdd Note:<\/b> The Fare feature contains outliers in each Pclass. This looks a bit strange.","c1893f6b":"<div style=\"color:#797D7F;\n           display:fill;\n           padding: 5px;\n           border-radius:10px;\n           border-style: solid;\n           border-color: #E5E7E9;\n           background-color:#F2F3F4;\n           font-size:15px;\n           font-family: Didot;\n           letter-spacing:0.5px\">\n<b>\ud83d\udcad Thougts:<\/b> This is not looking to bad. Let's build a solid pipeline and see how we perform!","9ec2a26c":"<a id=\"8.\"><\/a>\n<div style=\"color:#424949;\n           display:fill;\n           text-align:left;\n           padding: 0.7em;\n           border-radius:10px;\n           border-style: solid;\n           border-color:#424949;\n           background-color:#E0D3AF;\n           font-size:15px;\n           font-family: Didot;\n           letter-spacing:0.5px\">  \n<h1> 8. Final Thoughts <\/h1>\nAt this point I think it is time to close this workbook, because it is already quite large and not very readable. Let's reflect on what we have learned and what could be done additionally.\n<hr>\n<b> What can we take away from this workbook ?<\/b>\n<ul>\n  <li>It is important to take a closer look at the underlying data. No data field should be excluded without further ado. Here, the name and the ticket can be mentioned as an example. We initially ignored these, only to discover that they do contain important information. <\/li>\n  <li>It is important to constantly monitor the output of your model. For this we have presented the feature importances as a tool.  <\/li>\n  <li>Especially if we want to look at different models and use different combinations of features, it makes sense to define a robust reusable and modular pipeline. For this purpose, we have used the pipe package and adapted our pipeline again and again during the course of the workbook without having to redefine initially defined steps.<\/li>\n  <li>We must always be careful when using numeric values if they are not ordered or the order they contain does not match the natural order. We have seen how we can use one hot encoding to fix this problem for categorical features.<\/li>\n    <li>It is always important to consider the weighting of different features in the test and training data. In our example, both data sets were relatively equally distributed but this is not always the case. <\/li>\n<\/ul> \n    \n<b> What could we do next ?<\/b>  \nFirst of all. Even though we took quite a bit of time to look at the date, we didn't do much on the model side. We just used a standard models and tuned a handful of parameters. This is a good approach to start with, but from here on I would move to Tensorflow 2.0 to have a better control over my model. Beyond that, you could try other models like Random Forests or SVM and see if they perform better. I also didn't spend too much time on grid search or other ways of parameter tuning. If you would invest about 3 or 4 hours, I am sure you will find better parameters for our gradiand boosting model. As I mentioned at the beginning of chapter 6, you could also reduce various weights in the algorithm or train multiple models and stack them together.\nIf you are interested in model stacking and other methods for parameter tuning, take a look at chaptor 5 of my workbook on house price prediction.\n\nHouse Price prediction workbook:\nhttps:\/\/www.kaggle.com\/stefanschulmeister87\/visual-data-inspection-and-model-stacking\/edit\/run\/68644961\n    \n**I hope this workbook was helpful for some readers and I am always happy to get feedback in the comment section.**\n  ","1b0901e1":"<div style=\"color:#9A7D0A;\n           display:fill;\n           padding: 5px;\n           border-radius:10px;\n           border-style: solid;\n           border-color: #F9E79F;\n           background-color:#FCF3CF;\n           font-size:15px;\n           font-family: Didot;\n           letter-spacing:0.5px\">\n<b>\ud83d\udcdd Note:<\/b> We will use one hot encoder later on. Otherwise the natural order of those numbers could irritate the ML algorithm.","a5ec5a3a":"<div style=\"color:#1D8348;\n           display:fill;\n           padding: 5px;\n           border-radius:10px;\n           border-style: solid;\n           border-color: #A9DFBF;\n           background-color:#D5F5E3;\n           font-size: 15px;\n           font-family: Didot;\n           letter-spacing:0.5px\">\n<b>\ud83c\udf89 Score:<\/b> \n<div style=\"font-size: 30px;\n            padding: 0.7em;\"> \n    <b>Still around 80%<\/b> \n<\/div>\n\nSo we have not improved with this ideer. But overall, we have a good result. Let's end this journey at this point.","782974b3":"<div style=\"color:#797D7F;\n           display:fill;\n           padding: 5px;\n           border-radius:10px;\n           border-style: solid;\n           border-color: #E5E7E9;\n           background-color:#F2F3F4;\n           font-size:15px;\n           font-family: Didot;\n           letter-spacing:0.5px\">\n<b>\ud83d\udcad Thougts:<\/b> What if we train multiple models for different salutations?","5ef0745e":"<div style=\"color:#797D7F;\n           display:fill;\n           padding: 5px;\n           border-radius:10px;\n           border-style: solid;\n           border-color: #E5E7E9;\n           background-color:#F2F3F4;\n           font-size:15px;\n           font-family: Didot;\n           letter-spacing:0.5px\">\n<b>\ud83d\udcad Thougts:<\/b> This looks promising, even if the gap between training and test performance is a bit too high. Let's train a model on the entire training data and make a new prediction. Let's see how we perform on the test data.","c44cb0bf":"<div style=\"color:#797D7F;\n           display:fill;\n           padding: 5px;\n           border-radius:10px;\n           border-style: solid;\n           border-color: #E5E7E9;\n           background-color:#F2F3F4;\n           font-size:15px;\n           font-family: Didot;\n           letter-spacing:0.5px\">\n<b>\ud83d\udcad Thougts:<\/b>  What about the probability to survive in those groups?","b21ca9d1":"<a id=\"7.3\"><\/a>\n<div style=\"color:#424949;\n           display:fill;\n           text-align:left;\n           padding: 0.7em;\n           border-radius:10px;\n           border-style: solid;\n           border-color:#424949;\n           background-color:#E0D3AF;\n           font-size:15px;\n           font-family: Didot;\n           letter-spacing:0.5px\">  \n<h2> 7.3 Submission<\/h2>","5fe45ac6":"<a id=\"6.7\"><\/a>\n<div style=\"color:#424949;\n           display:fill;\n           text-align:left;\n           padding: 0.7em;\n           border-radius:10px;\n           border-style: solid;\n           border-color:#424949;\n           background-color:#E0D3AF;\n           font-size:15px;\n           font-family: Didot;\n           letter-spacing:0.5px\">  \n<h2> 6.7 Next Submission<\/h2>","60c23e18":"<div style=\"color:#797D7F;\n           display:fill;\n           padding: 5px;\n           border-radius:10px;\n           border-style: solid;\n           border-color: #E5E7E9;\n           background-color:#F2F3F4;\n           font-size:15px;\n           font-family: Didot;\n           letter-spacing:0.5px\">\n<b>\ud83d\udcad Thougts:<\/b>\nThis does not look so good. (maybe a good point for improvement)","f2e7bc45":"<div style=\"color:#9A7D0A;\n           display:fill;\n           padding: 5px;\n           border-radius:10px;\n           border-style: solid;\n           border-color: #F9E79F;\n           background-color:#FCF3CF;\n           font-size:15px;\n           font-family: Didot;\n           letter-spacing:0.5px\">\n<b>\ud83d\udcdd Note:<\/b> The price range of the thrid class looks a bit suspicious compared to the 2. class. We will have a closer look at that later.  ","a023bd27":"<div style=\"color:#9A7D0A;\n           display:fill;\n           padding: 5px;\n           border-radius:10px;\n           border-style: solid;\n           border-color: #F9E79F;\n           background-color:#FCF3CF;\n           font-size:15px;\n           font-family: Didot;\n           letter-spacing:0.5px\">\n<b>\ud83d\udcdd Note:<\/b> This looks like valuable information. It also seems likely that the first digit of the *Ticket_number* in most cases represents the Pclass. Let's check on that and validate how many passengers are in those big groups.","1284e462":"<div style=\"color:#9A7D0A;\n           display:fill;\n           padding: 5px;\n           border-radius:10px;\n           border-style: solid;\n           border-color: #F9E79F;\n           background-color:#FCF3CF;\n           font-size:15px;\n           font-family: Didot;\n           letter-spacing:0.5px\">\n<b>\ud83d\udcdd Note:<\/b> So what do we have? We have the 20 most frequent Leading_tickets_numbers and their frequencies from test and training dates. It is important to note here that we have normalized the frequencies, since the size of the test data is smaller than the size of the training data. In addition, we have the associated survival probabilities based on the training data.\nI hope the steps in the code are understandable. Now let's try to represent this data in one informative plot.","debfec01":"<div style=\"color:#797D7F;\n           display:fill;\n           padding: 5px;\n           border-radius:10px;\n           border-style: solid;\n           border-color: #E5E7E9;\n           background-color:#F2F3F4;\n           font-size:15px;\n           font-family: Didot;\n           letter-spacing:0.5px\">\n<b>\ud83d\udcad Thougts:<\/b> This looks like a more likely price distribution.","c3c199ac":"<div style=\"color:#797D7F;\n           display:fill;\n           padding: 5px;\n           border-radius:10px;\n           border-style: solid;\n           border-color: #E5E7E9;\n           background-color:#F2F3F4;\n           font-size:15px;\n           font-family: Didot;\n           letter-spacing:0.5px\">\n<b>\ud83d\udcad Thougts:<\/b>  It appears strange that Age seems to play a minor role when it comes to survivability. Maybe we should add a feature here, which would adjust the exact age to the stage of life like \"infant\", \"child\" and \"adult\".","e91af917":"<a id=\"6.\"><\/a>\n<div style=\"color:#424949;\n           display:fill;\n           text-align:left;\n           padding: 0.7em;\n           border-radius:10px;\n           border-style: solid;\n           border-color:#424949;\n           background-color:#E0D3AF;\n           font-size:15px;\n           font-family: Didot;\n           letter-spacing:0.5px\">  \n<h1> 6. New Training <\/h1>","867deb5d":"<a id=\"6.6\"><\/a>\n<div style=\"color:#424949;\n           display:fill;\n           text-align:left;\n           padding: 0.7em;\n           border-radius:10px;\n           border-style: solid;\n           border-color:#424949;\n           background-color:#E0D3AF;\n           font-size:15px;\n           font-family: Didot;\n           letter-spacing:0.5px\">  \n<h2> 6.6 Next Training<\/h2>","b6ed71fe":"<div style=\"color:#9A7D0A;\n           display:fill;\n           padding: 5px;\n           border-radius:10px;\n           border-style: solid;\n           border-color: #F9E79F;\n           background-color:#FCF3CF;\n           font-size:15px;\n           font-family: Didot;\n           letter-spacing:0.5px\">\n<b>\ud83d\udcdd Note:<\/b> Now that's some clear evidence that Sex and Pclass are quite relevant in predicting whether someone survived","42fc67db":"<div style=\"color:#424949;\n           display:fill;\n           border-radius:10px;\n           border-style: solid;\n           border-color:#424949;\n           text-align:center;\n           background-color:#E0D3AF;\n           font-size:15px;\n           font-family: Didot;\n           letter-spacing:0.5px;\n           padding: 0.7em;\n           text-align:left\">  \nThis workbook is not a \"quick best titanic score\" kind of workbook. It is more of a journey from first viewing the data to scoring 80%.  This is my first notebook on Kaggle and I tried my best to make it readable. I tried to highlight important findings and sections of code, and I didn't delete any models that didn't perform as well. I thought it might be interesting to see how I approached this problem and not just how I solved it. I hope you enjoy it. ","719d6c69":"<a id=\"2.2\"><\/a>\n<div style=\"color:#424949;\n           display:fill;\n           text-align:left;\n           padding: 0.7em;\n           border-radius:10px;\n           border-style: solid;\n           border-color:#424949;\n           background-color:#E0D3AF;\n           font-size:15px;\n           font-family: Didot;\n           letter-spacing:0.5px\">  \n<h2> 2.2 New Features \"Fare_per_person\" And \"Group_size\"<\/h2>","a3750ca7":"<div style=\"color:#9A7D0A;\n           display:fill;\n           padding: 5px;\n           border-radius:10px;\n           border-style: solid;\n           border-color: #F9E79F;\n           background-color:#FCF3CF;\n           font-size:15px;\n           font-family: Didot;\n           letter-spacing:0.5px\">\n<b>\ud83d\udcdd Note:<\/b> At this point it is very important to set the stratify properyt to y otherwise we might make a model more biased towards a particular class. Setting the stratify property to y causes the distribution between survived and not survived to be the same in y_test and y_train. So lets first approve that this is true.","e8094e09":"<div style=\"color:#9A7D0A;\n           display:fill;\n           padding: 5px;\n           border-radius:10px;\n           border-style: solid;\n           border-color: #F9E79F;\n           background-color:#FCF3CF;\n           font-size:15px;\n           font-family: Didot;\n           letter-spacing:0.5px\">\n<b>\ud83d\udcdd Note:<\/b> We are dropping Fare_per_person, Sex and Age! Fare_per_person and Age will be replaced by the corresponding Bucket feature and Sex will become obsolete with our new Salutation feature.","3e194954":"<a id=\"4.\"><\/a>\n<a id=\"4.1\"><\/a>\n<div style=\"color:#424949;\n           display:fill;\n           text-align:left;\n           padding: 0.7em;\n           border-radius:10px;\n           border-style: solid;\n           border-color:#424949;\n           background-color:#E0D3AF;\n           font-size:15px;\n           font-family: Didot;\n           letter-spacing:0.5px\">  \n<h1> 4. Model Training And Validation <\/h1>\n<he\n<h2>4.1 Simple Split<\/h2>\nLet's start with a simple split of the data into two parts without cross-validation.","f2b13c20":"<a id=\"7.\"><\/a>\n<div style=\"color:#424949;\n           display:fill;\n           text-align:left;\n           padding: 0.7em;\n           border-radius:10px;\n           border-style: solid;\n           border-color:#424949;\n           background-color:#E0D3AF;\n           font-size:15px;\n           font-family: Didot;\n           letter-spacing:0.5px\">  \n<h1>7. Model Comparison And Data Splitting<\/h1>\nLet's start by customizing the pipeline. We will split the date into two groups, one for all entries with \"Mr.\" as Salutation and one for the rest.","258eda17":"<a id=\"5.4\"><\/a>\n<div style=\"color:#424949;\n           display:fill;\n           text-align:left;\n           padding: 0.7em;\n           border-radius:10px;\n           border-style: solid;\n           border-color:#424949;\n           background-color:#E0D3AF;\n           font-size:15px;\n           font-family: Didot;\n           letter-spacing:0.5px\">  \n<h2> 5.4 New Feature \"Fpp_bucket\"<\/h2>\nLet's do the same thing for Fare_per_person. This could help us overall to prevent overfitting.","28d5644a":"<div style=\"color:#9A7D0A;\n           display:fill;\n           padding: 5px;\n           border-radius:10px;\n           border-style: solid;\n           border-color: #F9E79F;\n           background-color:#FCF3CF;\n           font-size:15px;\n           font-family: Didot;\n           letter-spacing:0.5px\">\n<b>\ud83d\udcdd Note:<\/b> That does not look alarming. So let's keep adding new features and hopefully get better performance.","1530220a":"<a id=\"5.5\"><\/a>\n<div style=\"color:#424949;\n           display:fill;\n           text-align:left;\n           padding: 0.7em;\n           border-radius:10px;\n           border-style: solid;\n           border-color:#424949;\n           background-color:#E0D3AF;\n           font-size:15px;\n           font-family: Didot;\n           letter-spacing:0.5px\">  \n<h2> 5.5 New Pipeline <\/h2>\nLet's create a new pipeline and see if our additional data preparations have an impact on the model performance.","f698aaf8":"<div style=\"color:#9A7D0A;\n           display:fill;\n           padding: 5px;\n           border-radius:10px;\n           border-style: solid;\n           border-color: #F9E79F;\n           background-color:#FCF3CF;\n           font-size:15px;\n           font-family: Didot;\n           letter-spacing:0.5px\">\n<b>\ud83d\udcdd Note:<\/b> We will integrate the 4 most common salutations as a feature. So let's customize the above test function and create a corresponding pipeline function.","83e9d4d4":"<div style=\"color:#9A7D0A;\n           display:fill;\n           padding: 5px;\n           border-radius:10px;\n           border-style: solid;\n           border-color: #F9E79F;\n           background-color:#FCF3CF;\n           font-size:15px;\n           font-family: Didot;\n           letter-spacing:0.5px\">\n<b>\ud83d\udcdd Note:<\/b> Who would have thought it?! \u00af\\_(\u30c4)_\/\u00af","cacca49f":"<div style=\"color:#21618C;\n           display:fill;\n           padding: 5px;\n           border-radius:10px;\n           border-style: solid;\n           border-color: #AED6F1;\n           background-color:#D6EAF8;\n           font-size:15px;\n           font-family: Didot;\n           letter-spacing:0.5px\">\n<b>\ud83d\udca1 Idea:<\/b> We should consider a closer look at very young passengers.","409089b3":"<div style=\"color:#797D7F;\n           display:fill;\n           padding: 5px;\n           border-radius:10px;\n           border-style: solid;\n           border-color: #E5E7E9;\n           background-color:#F2F3F4;\n           font-size:15px;\n           font-family: Didot;\n           letter-spacing:0.5px\">\n<b>\ud83d\udcad Thougts:<\/b>  Before we create different visualizations we should set a uniform style. Maybe something that looks old. Like an old newspaper reporting the Titanic disaster.","82be829d":"<div style=\"color:#9A7D0A;\n           display:fill;\n           padding: 5px;\n           border-radius:10px;\n           border-style: solid;\n           border-color: #F9E79F;\n           background-color:#FCF3CF;\n           font-size:15px;\n           font-family: Didot;\n           letter-spacing:0.5px\">\n<b>\ud83d\udcdd Note:<\/b> This is interesting. Being alone in the first class seems to have a negative impact on your chance of survival. Being in a large group (more than 4 people) in the third class seems to lower your survival probability as well. This could be a useful feature to predict whether someone has survived.","4cbde97c":"<a id=\"2.4\"><\/a>\n<div style=\"color:#424949;\n           display:fill;\n           text-align:left;\n           padding: 0.7em;\n           border-radius:10px;\n           border-style: solid;\n           border-color:#424949;\n           background-color:#E0D3AF;\n           font-size:15px;\n           font-family: Didot;\n           letter-spacing:0.5px\">  \n<h2> 2.4 Correlations With Newly Created Features <\/h2>","d8bdbd5f":"<a id=\"7.2\"><\/a>\n<div style=\"color:#424949;\n           display:fill;\n           text-align:left;\n           padding: 0.7em;\n           border-radius:10px;\n           border-style: solid;\n           border-color:#424949;\n           background-color:#E0D3AF;\n           font-size:15px;\n           font-family: Didot;\n           letter-spacing:0.5px\">  \n<h2> 7.2 Parameter Tuning<\/h2>","07fc78df":"<a id=\"7.1\"><\/a>\n<div style=\"color:#424949;\n           display:fill;\n           text-align:left;\n           padding: 0.7em;\n           border-radius:10px;\n           border-style: solid;\n           border-color:#424949;\n           background-color:#E0D3AF;\n           font-size:15px;\n           font-family: Didot;\n           letter-spacing:0.5px\">  \n<h2> 7.2 Model Training<\/h2>\nWe will train two models and tune their hyper-parameters with Gridsearch. Then we will compare the combined performance with our mone model attempt.","434a72a9":"<div style=\"color:#9A7D0A;\n           display:fill;\n           padding: 5px;\n           border-radius:10px;\n           border-style: solid;\n           border-color: #F9E79F;\n           background-color:#FCF3CF;\n           font-size:15px;\n           font-family: Didot;\n           letter-spacing:0.5px\">\n<b>\ud83d\udcdd Note:<\/b> I think we can say that a delta of the distributions of the main features between test and training data does not seem to be the main reason for our divergent performance. Only Fare_per_person looks a bit different. Let's look a little further here. ","012dbf8f":"<div style=\"color:#9A7D0A;\n           display:fill;\n           padding: 5px;\n           border-radius:10px;\n           border-style: solid;\n           border-color: #F9E79F;\n           background-color:#FCF3CF;\n           font-size:15px;\n           font-family: Didot;\n           letter-spacing:0.5px\">\n<b>\ud83d\udcdd Note:<\/b> Even if it is not clear, there seems to be a relationship between the first digit of the ticket number and the Pclass. ","f0f48ade":"<div style=\"color:#9A7D0A;\n           display:fill;\n           padding: 5px;\n           border-radius:10px;\n           border-style: solid;\n           border-color: #F9E79F;\n           background-color:#FCF3CF;\n           font-size:15px;\n           font-family: Didot;\n           letter-spacing:0.5px\">\n<b>\ud83d\udcdd Note:<\/b> Instead of using combined_df to calculate the group_size we could use row['Parch'] + row['SibSp'] + 1 but we dont know if each group ticket is a single familie or a familie at all. So since we have the data available, we can use it.","cf76927b":"<div style=\"color:#9A7D0A;\n           display:fill;\n           padding: 5px;\n           border-radius:10px;\n           border-style: solid;\n           border-color: #F9E79F;\n           background-color:#FCF3CF;\n           font-size:15px;\n           font-family: Didot;\n           letter-spacing:0.5px\">\n<b>\ud83d\udcdd Note:<\/b> This looks very promising. The accuracy is quite high and, more importantly, the spread between test and training accuracy is quite small. Let's take a look at the importance of the individual featurers.","9198f806":"<div style=\"color:#943126;\n           display:fill;\n           padding: 5px;\n           border-radius:10px;\n           border-style: solid;\n           border-color: #F5B7B1;\n           background-color:#FADBD8;\n           font-size:15px;\n           font-family: Didot;\n           letter-spacing:0.5px\">\n<b> Attention:<\/b> A family or other groups of people can share one ticket and in this case the value for Fare is biased.","a9ba92a7":"<div style=\"color:#797D7F;\n           display:fill;\n           padding: 5px;\n           border-radius:10px;\n           border-style: solid;\n           border-color: #E5E7E9;\n           background-color:#F2F3F4;\n           font-size:15px;\n           font-family: Didot;\n           letter-spacing:0.5px\">\n<b>\ud83d\udcad Thougts:<\/b>  \nThere seems to be some correlations between Ticket_prefix and survivability but since most of the tickets do not include a prefix we will leave this feature aside for now. \n<ul>\n    <li>What about the ticket numbers? <\/li>\n    <li>Can we group some of them and gain some information about the deck or cabin of these tickets?<\/li>\n<\/ul>","e2f8a76c":"<div style=\"color:#9A7D0A;\n           display:fill;\n           padding: 5px;\n           border-radius:10px;\n           border-style: solid;\n           border-color: #F9E79F;\n           background-color:#FCF3CF;\n           font-size:15px;\n           font-family: Didot;\n           letter-spacing:0.5px\">\n<b>\ud83d\udcdd Note:<\/b> Most tickets (663 \/ 891) do not contain a prefix.","6cd9c0f6":"<a id=\"2.3\"><\/a>\n<div style=\"color:#424949;\n           display:fill;\n           text-align:left;\n           padding: 0.7em;\n           border-radius:10px;\n           border-style: solid;\n           border-color:#424949;\n           background-color:#E0D3AF;\n           font-size:15px;\n           font-family: Didot;\n           letter-spacing:0.5px\">  \n<h2> 2.3 New Feature \"Child\"<\/h2>\nWe have previously seen that young children had a greatly increased probability of survival and that age generally did not have a large impact on survival. So let's create a feature that maps whether a passenger is a child or not as a subset of Age. This could make life a little easier for the algorithm.","6a0e07d6":"<a id=\"5.\"><\/a>\n<a id=\"5.1\"><\/a>\n<div style=\"color:#424949;\n           display:fill;\n           text-align:left;\n           padding: 0.7em;\n           border-radius:10px;\n           border-style: solid;\n           border-color:#424949;\n           background-color:#E0D3AF;\n           font-size:15px;\n           font-family: Didot;\n           letter-spacing:0.5px\">  \n<h1> 5. Feature Engenearing And Data Validation 2.0 <\/h1>\n<hr>\nWhat have we seen so far? The gender is very dominant in all models. It also shows that our test performance is strikingly above our actual score. This can have various reasons. I assume it is due to the distribution of the test data. If the algorithm puts too much emphasis on individual characteristics, a different distribution of these characteristics in training- and test-data can have an impact on the ability of the algorithm to generalize. So what can we do about it?\n<ul>\n  <li>First of all, we can compare the distribution of the test data with the distribution of the training data and use cross validation to optimize the training on an optimal subset.<\/li>\n  <li>In addition, we can add more features to fix, at best, the one-sided weighting of the algorithm.<\/li>\n<\/ul> \n<hr>\n<h2> 5.1 Distribution Analysis <\/h2>    \nLets compare the distributions of the most important features in test_df and train_df.","c28cec2b":"<a id=\"1.4\"><\/a>\n<div style=\"color:#424949;\n           display:fill;\n           text-align:left;\n           padding: 0.7em;\n           border-radius:10px;\n           border-style: solid;\n           border-color:#424949;\n           background-color:#E0D3AF;\n           font-size:15px;\n           font-family: Didot;\n           letter-spacing:0.5px\">  \n<h2> 1.4 Correlations <\/h2>\n\nLet's see if we can find some insight by looking at the correlations between most of the features. First of all, we need to calculate the correlations between all features. This is easily done with pandas.DataFrame().corr(), but only for numeric features. (for obvious reasons).\nSo first, let's convert sex and embarked from sting to int. We will ignore *Ticket*, *Name*, and *Cabin* for this first look at the correlations.","cc22e4eb":"<a id=\"4.3\"><\/a>\n<div style=\"color:#424949;\n           display:fill;\n           text-align:left;\n           padding: 0.7em;\n           border-radius:10px;\n           border-style: solid;\n           border-color:#424949;\n           background-color:#E0D3AF;\n           font-size:15px;\n           font-family: Didot;\n           letter-spacing:0.5px\">  \n<h2> 4.3 Grid Search <\/h2>","cfd82d27":"<div style=\"color:#1D8348;\n           display:fill;\n           padding: 5px;\n           border-radius:10px;\n           border-style: solid;\n           border-color: #A9DFBF;\n           background-color:#D5F5E3;\n           font-size: 15px;\n           font-family: Didot;\n           letter-spacing:0.5px\">\n<b>\ud83c\udf89 Score:<\/b> \n<div style=\"font-size: 30px;\n            padding: 0.7em;\"> \n    <b>80%<\/b> \n<\/div>\n\nThere we go...","17938570":"<div style=\"color:#9A7D0A;\n           display:fill;\n           padding: 5px;\n           border-radius:10px;\n           border-style: solid;\n           border-color: #F9E79F;\n           background-color:#FCF3CF;\n           font-size:15px;\n           font-family: Didot;\n           letter-spacing:0.5px\">\n<b>\ud83d\udcdd Note:<\/b> The frequency of these groups is quite similar in the test and training data set. That' s good. However, the problem is that in our current pipelines we normalize the leading ticket number and thus implicitly assume that there is a natural order of magnitude. However, if we look at the survival probabilities of these groups, we see that this is not quite correct. Although the survival probability in groups starting with a 1 is generally higher, there are also relevant exceptions. We will fix this erroneously assumed order by setting up a new pipeline that processes the leading_ticket_number via one hot encoding. This will increase the number of features tremendously but we can adjust the depth of the trees accordingly.","6bb233d5":"<a id=\"6.2\"><\/a>\n<div style=\"color:#424949;\n           display:fill;\n           text-align:left;\n           padding: 0.7em;\n           border-radius:10px;\n           border-style: solid;\n           border-color:#424949;\n           background-color:#E0D3AF;\n           font-size:15px;\n           font-family: Didot;\n           letter-spacing:0.5px\">  \n<h2> 6.2 New Submission <\/h2>","80104730":"![titanic_headline.png](attachment:4a7c8f1b-1007-4afd-9a46-db52782ddade.png)","3bab46c2":"<a id=\"4.4\"><\/a>\n<div style=\"color:#424949;\n           display:fill;\n           text-align:left;\n           padding: 0.7em;\n           border-radius:10px;\n           border-style: solid;\n           border-color:#424949;\n           background-color:#E0D3AF;\n           font-size:15px;\n           font-family: Didot;\n           letter-spacing:0.5px\">  \n<h2> 4.4 Submission <\/h2>","630c97c8":"<a id=\"6.5\"><\/a>\n<div style=\"color:#424949;\n           display:fill;\n           text-align:left;\n           padding: 0.7em;\n           border-radius:10px;\n           border-style: solid;\n           border-color:#424949;\n           background-color:#E0D3AF;\n           font-size:15px;\n           font-family: Didot;\n           letter-spacing:0.5px\">  \n<h2> 6.5 Next Pipeline<\/h2>","646ac68e":"<div style=\"color:#797D7F;\n           display:fill;\n           padding: 5px;\n           border-radius:10px;\n           border-style: solid;\n           border-color: #E5E7E9;\n           background-color:#F2F3F4;\n           font-size:15px;\n           font-family: Didot;\n           letter-spacing:0.5px\">\n<b>\ud83d\udcad Thougts:<\/b> This is what we want for each feature. So let's create a dataframe and plot the result for a more appealing look. Furthermore, a value_count for features like Fare_per_person and Age makes little sense so we will create a distplot here.","dda6f101":"<a id=\"1.\"><\/a>\n<a id=\"1.1\"><\/a>\n<div style=\"color:#424949;\n           display:fill;\n           border-radius:10px;\n           border-style: solid;\n           border-color:#424949;\n           text-align:center;\n           background-color:#E0D3AF;\n           font-size:15px;\n           font-family: Didot;\n           letter-spacing:0.5px;\n           padding: 0.7em;\n           text-align:left\">  \n<h1> 1. Data Screening And Visualisations <\/h1>\n    <hr>\n<h2>1.1 First Look <\/h2>\n<\/div>","fb546fef":"<a id=\"6.1\"><\/a>\n<div style=\"color:#424949;\n           display:fill;\n           text-align:left;\n           padding: 0.7em;\n           border-radius:10px;\n           border-style: solid;\n           border-color:#424949;\n           background-color:#E0D3AF;\n           font-size:15px;\n           font-family: Didot;\n           letter-spacing:0.5px\">  \n<h2> 6.1 Grid Search With ShuffleSplit Cross Validation <\/h2>","28c52408":"<div style=\"color:#9A7D0A;\n           display:fill;\n           padding: 5px;\n           border-radius:10px;\n           border-style: solid;\n           border-color: #F9E79F;\n           background-color:#FCF3CF;\n           font-size:15px;\n           font-family: Didot;\n           letter-spacing:0.5px\">\n<b>\ud83d\udcdd Note:<\/b> We see strong correlations between the features Survived and Pclass, Sex, Fare and Embarked.","047c4653":"<a id=\"3.\"><\/a>\n<a id=\"3.1\"><\/a>\n<a id=\"3.2\"><\/a>\n<div style=\"color:#424949;\n           display:fill;\n           text-align:left;\n           padding: 0.7em;\n           border-radius:10px;\n           border-style: solid;\n           border-color:#424949;\n           background-color:#E0D3AF;\n           font-size:15px;\n           font-family: Didot;\n           letter-spacing:0.5px\">  \n<h1> 3. Data Preparation Using Pipe <\/h1>\n    <hr>\n<h2> 3.1 Summary Of Observations <\/h2>\n    <b> So what have we learned that must be considered in the pipeline?  <\/b> \n<ol>\n  <li>We have some missing Age values and since there is some correlation between Age, Pclass and Sex we should use this information to fill those missing values.<\/li>\n  <li>There are two missing Embarked values in the training data but not in the test data. So we can just drop these two lines.<\/li>\n  <li>There is one missing Fare value in the Test data. We can simply group by Pclass, Sax to fill this value.<\/li>\n  <li>Categorical features like Sex and Embarked are important and using integer encoding alone might not be a good idear since there is no order in theas features.<\/li>\n  <li>Other important features like Pclass, Leading_ticket_numbers and Group_size are ordert. So we probably don't need to use one hot encoding on those features.<\/li>\n  <li>Group_size combinds informations of SibSp and Parch and could replace them.<\/li>\n  <li>We also know that Fare is the price of one ticket and this can include up to 9 passengers. Even if there is no strong evidence that Fare_per_person holds more information we will include it in our model.<\/li>\n  <li>Furthermore we will drop PassengerId, Name, SibSp, Cabin, Ticket and Fare. There is no strong indication that PassengerId and Name includes valuable information. SibSp is included in Group_size and Cabin has too many missing values. Ticket on the other hand includes a lot of information but we will only use the first three numbers without any prefix as categorical feature. Fare will be replaced bei Fare_per_person<\/li>\n  <li>We will also add the Child feature and use one hot incoding for Sex and Embarked.<\/li>\n  <li>Last but not least, we should normalize Age, Group_size, Leading_ticket_number and Fare_per_person<\/li>\n<\/ol> \n    <b>So lets dive right in!<\/b> \n<hr>\n<h2> 3.2 Pipeline functions <\/h2>"}}