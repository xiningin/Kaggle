{"cell_type":{"4348cc63":"code","9d0b4c90":"code","d39e2a0d":"code","55965fe7":"code","db381669":"code","7f271275":"code","0c7e485b":"code","eed6941a":"code","0aabdb20":"code","116c5fa5":"code","e780bc56":"code","3634e640":"code","2047d020":"code","9210a7f9":"code","14246491":"code","07051149":"code","12f56286":"code","2f3e0e19":"markdown","904c625a":"markdown","3df80c99":"markdown","dff219e0":"markdown","e0045467":"markdown","97ce4b3a":"markdown","3695c52e":"markdown","3558d61f":"markdown","b18ead32":"markdown","07b522f9":"markdown","fcf412d4":"markdown","8484822a":"markdown","77fe67ec":"markdown","1b3536d9":"markdown"},"source":{"4348cc63":"# LOAD LIBRARIES\nimport pandas as pd\nimport numpy as np\n\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split\n\n\nfrom keras.utils.np_utils import to_categorical\nfrom keras.models import Sequential\nimport tensorflow as tf\nimport tensorflow.keras.optimizers as O\nimport tensorflow.keras as keras\nimport tensorflow.keras.models as M\nimport tensorflow.keras.layers as L\nimport tensorflow.keras.backend as K\n\nfrom keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPool2D, BatchNormalization\nfrom keras.preprocessing.image import ImageDataGenerator\nfrom keras.callbacks import LearningRateScheduler,EarlyStopping, ModelCheckpoint, TensorBoard, ReduceLROnPlateau\n\nimport random\nfrom kaggle_datasets import KaggleDatasets","9d0b4c90":"TEST_SIZE = 0.2\nRANDOM_SEED = 42\nBATCH_SIZE = 48\nEPOCHS = 45\nIMG_SIZE = 48\nIMG_CHANNELS = 3\nNUM_CLASSES = 26\ninput_shape = (IMG_SIZE, IMG_SIZE, IMG_CHANNELS)\nDEBUG = False\nnets = 15 # Building NETS for ensemble\nif DEBUG == True:\n    EPOCHS = 1\n    nets = 5","d39e2a0d":"AUTO = tf.data.experimental.AUTOTUNE\n# Detect hardware, return appropriate distribution strategy\ntry:\n    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()  # TPU detection. No parameters necessary if TPU_NAME environment variable is set. On Kaggle this is always the case.\n    print('Running on TPU ', tpu.master())\nexcept ValueError:\n    tpu = None\n\nif tpu:\n    tf.config.experimental_connect_to_cluster(tpu)\n    tf.tpu.experimental.initialize_tpu_system(tpu)\n    strategy = tf.distribute.experimental.TPUStrategy(tpu)\nelse:\n    strategy = tf.distribute.get_strategy() # default distribution strategy in Tensorflow. Works on CPU and single GPU.\n\nprint(\"REPLICAS: \", strategy.num_replicas_in_sync)\n\n\n# Data access\nGCS_DS_PATH = KaggleDatasets().get_gcs_path()\nprint(GCS_DS_PATH)","55965fe7":"images = np.load('\/kaggle\/input\/sf-captcha-recognition\/data\/images.npy')\nlabels = np.load('\/kaggle\/input\/sf-captcha-recognition\/data\/labels.npy')\n\nprint(\"Train dataset:\", images.shape, labels.shape, \"Labels are:\", set(labels))\nprint(\"Picture shape:\", images[0].shape)","db381669":"submission = pd.read_csv('..\/input\/sf-captcha-recognition\/sample_submission.csv')","7f271275":"cols = 8\nrows = 2\nfig = plt.figure(figsize=(2 * cols - 1, 2.5 * rows - 1))\nfor i in range(cols):\n    for j in range(rows):\n        random_index = np.random.randint(0, len(labels))\n        ax = fig.add_subplot(rows, cols, i * rows + j + 1)\n        ax.grid('off')\n        ax.axis('off')\n        ax.imshow(images[random_index])\n        ax.set_title(labels[random_index])\nplt.show()","0c7e485b":"def normalize_pictures(pics):\n    return pics.astype('float32')\/255 - 0.5\n\nX = normalize_pictures(images)\nY = labels","eed6941a":"print(\"Shape of X:\",X.shape)\nprint(\"Shape of Y:\",Y.shape)","0aabdb20":"images_sub = np.load('..\/input\/sf-captcha-recognition\/data\/images_sub.npy')\nimages_sub = normalize_pictures(images_sub)","116c5fa5":"# \u0442\u0440\u0435\u0439\u043d \/ \u0442\u0435\u0441\u0442\ntrain_files, test_files, train_labels, test_labels = train_test_split(X, Y, test_size=TEST_SIZE, random_state=RANDOM_SEED, stratify=Y, shuffle=True)\n\ntrain_files.shape, test_files.shape","e780bc56":"def make_callbacks():\n    \n    callback_early_stopping = EarlyStopping(monitor='val_loss',\n                                        patience=10, verbose=1)\n    callback_reduce_lr = ReduceLROnPlateau(monitor='val_loss',\n                                       factor=0.5,\n                                       min_lr=1e-10,\n                                       patience=0,\n                                       verbose=1)\n    callback_learing_rate = LearningRateScheduler(lambda x: 1e-3 * 0.95 ** x),\n    path_checkpoint = 'checkpoint.keras'\n    callback_checkpoint = ModelCheckpoint(filepath=path_checkpoint,\n                                      monitor='val_loss',\n                                      verbose=1,\n                                      save_weights_only=True,\n                                      save_best_only=True)\n\n    return [callback_checkpoint,\n            callback_learing_rate,\n                 callback_reduce_lr]\n\ncallbacks = make_callbacks()\n","3634e640":"# BUILD CONVOLUTIONAL NEURAL NETWORKS\nmodel = [0] * nets\n    \ndef make_model():\n    for i in range(nets):\n        model[i] = Sequential()\n\n        model[i].add(Conv2D(32, kernel_size = 3,padding = 'same', activation='relu', input_shape = input_shape))\n        model[i].add(BatchNormalization())\n\n        model[i].add(Conv2D(32, kernel_size = 3, activation='relu'))\n        model[i].add(BatchNormalization())\n\n        model[i].add(Conv2D(32, kernel_size = 5, strides=2, padding='same', activation='relu'))\n        model[i].add(BatchNormalization())\n        model[i].add(Dropout(0.4))\n\n        model[i].add(Conv2D(64, kernel_size = 3,padding = 'same', activation='relu'))\n\n        model[i].add(L.MaxPooling2D(pool_size = 2, padding = 'same'))\n\n        model[i].add(BatchNormalization())\n        model[i].add(Conv2D(64, kernel_size = 3,padding = 'same', activation='relu'))\n\n        model[i].add(L.MaxPooling2D(pool_size = 2, padding = 'same'))\n\n        model[i].add(BatchNormalization())\n        model[i].add(Conv2D(64, kernel_size = 5, strides=2, padding='same', activation='relu'))\n\n        model[i].add(L.MaxPooling2D(pool_size = 2, padding = 'same'))\n\n        model[i].add(BatchNormalization())\n        model[i].add(Dropout(0.4))\n\n        model[i].add(Conv2D(128, kernel_size = 4,padding = 'same', activation='relu'))\n        model[i].add(L.MaxPooling2D(pool_size = 2, padding = 'same'))\n\n        model[i].add(BatchNormalization())\n        model[i].add(Flatten())\n        model[i].add(Dropout(0.4))\n        model[i].add(Dense(NUM_CLASSES, activation='softmax'))\n\n        # COMPILE WITH ADAM OPTIMIZER AND CROSS ENTROPY COST\n        model[i].compile(optimizer=\"adam\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n    return model ","2047d020":"#base_model = make_model()\n#base_model.summary()\n## first: train only the top layers (which were randomly initialized)\n#base_model.trainable = False","9210a7f9":"#model = make_model()\n#for layer in model.layers:\n#    print(layer,  layer.trainable)","14246491":"opt = O.Adam(learning_rate=0.005)\n\ndef train_model(X, y, make_model_func=make_model, optimizer=opt, ):\n    random_states = [42, 50, 100, 2021, 13,7, 5, 23, 29]\n    \n    \n\n    train_files.shape, test_files.shape\n    model = make_model_func()\n    for i in range(nets):\n        print(f'Model {i} is training')\n        print('One sec, I just shuffle X_train, X_val, y_train, y_val')\n        X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=TEST_SIZE, random_state=random.choice(random_states), stratify=y, shuffle=True)\n        K.clear_session()\n        model[i].compile(\n          loss='sparse_categorical_crossentropy',\n          optimizer=optimizer,\n          metrics=['accuracy']\n      )\n\n        model[i].fit(\n          X_train,\n          y_train,  \n          batch_size=BATCH_SIZE, \n          validation_split=TEST_SIZE,\n          epochs=EPOCHS,\n          callbacks=callbacks,\n          shuffle=False,\n          validation_data=(X_val, y_val),\n      )\n  \n    return model","07051149":"model = train_model(X, Y)","12f56286":"labels_sub = np.zeros((images_sub.shape[0],NUM_CLASSES)) \nfor i in range(nets):\n    labels_sub = labels_sub + model[i].predict(images_sub)\n\nsubmission['Category'] = labels_sub.argmax(axis = 1)\nsubmission.set_index('Id').to_csv('submission.csv')","2f3e0e19":"# CFG","904c625a":"# Libs","3df80c99":"# Ensemble 15 CNN predictions and submit","dff219e0":"# TRAIN 15 CNNs","e0045467":"## Show some images","97ce4b3a":"# Enable TPU","3695c52e":"# Callbacks","3558d61f":"# EDA","b18ead32":"## Let's normalize our images","07b522f9":"# Stratify split","fcf412d4":"# TODO\n* TTA\n* CUTMIX\n* UPMIX","8484822a":"# BUILD 15 Convolutional Neural Networks","77fe67ec":"# Versions:\n### Enable TPU\n### \u2116 4: \n* Train public cnn with stratified data with callbacks [callback_checkpoint,callback_learing_rate,callback_reduce_lr]\n* **Need help** : how to add convert data from npy file to Dataframe, cause as far as i know for keras.ImageDataGenerator we need dataframe\n### \u2116 5:\n* Add debug option\n* Train 15 cnn with random train_test_split and with same callbacks\n* Ensemble in the end | credits to this well-known [master](https:\/\/www.kaggle.com\/dailysergey\/25-million-images-0-99757-mnist)\n\n### Please upvote, if u find this notebook helpful","1b3536d9":"# Credits\n* https:\/\/www.kaggle.com\/romanianvarev\/captcha-recognition\n\n* https:\/\/www.kaggle.com\/dailysergey\/25-million-images-0-99757-mnist"}}