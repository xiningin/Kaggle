{"cell_type":{"8e60ab80":"code","945d46c9":"code","0370f0de":"code","e51892b5":"code","0bd24dc0":"code","a48b2fda":"code","70950515":"code","85bfec53":"code","305e9676":"code","15e81d45":"code","8c93cb41":"code","c4c415a7":"code","e3bc57f5":"code","2fe521fa":"code","2e98d1d7":"code","f379ca4d":"code","d566363c":"code","633fc5a2":"markdown","34535e7e":"markdown","6a619385":"markdown","70b1bd62":"markdown","2f8736a5":"markdown","225d4195":"markdown","b746bede":"markdown","e374318b":"markdown","6d5f15cf":"markdown","2e4f3450":"markdown","a98b39cd":"markdown","9e22b798":"markdown","71f1de74":"markdown","078a4aa5":"markdown"},"source":{"8e60ab80":"# Imports\nfrom functools import partial  # to mock arguments in functions, like f1_score\nfrom os.path import join as pjoin\n\nimport cufflinks as cf\nimport lightgbm as lgb\nimport numpy as np\nimport pandas as pd\nfrom matplotlib import pyplot as plt\nfrom plotly.offline import init_notebook_mode\nfrom sklearn.metrics import f1_score\n\ninit_notebook_mode(connected=False)\ncf.go_offline()\n\npd.options.display.max_rows = 200\npd.options.display.max_columns = 200\npd.options.display.max_colwidth = 200\nplt.style.use('ggplot')","945d46c9":"import plotly.graph_objects as go\n\n\n# f1-score data grid preparation\nsteps = 50\nprecision_grid = np.linspace(0, 1, num=steps)\nrecall_grid =    np.linspace(0, 1, num=steps)\npp, rr = np.meshgrid(precision_grid, recall_grid, sparse=True)\nf1_score_grid = 2*(pp*rr)\/(pp + rr)\nf1_score_grid[np.isnan(f1_score_grid)] = 0\n\n# visualize it with plot.ly\nfig = go.Figure(data=[go.Surface(z=f1_score_grid, x=precision_grid, y=recall_grid)])\n\nfig.update_layout(\n    title='F1-Score surface (feel free to rotate\/scale it as you wish)', \n#     autosize=True,\n    width=640,\n    height=640,\n    margin=dict(l=65, r=50, b=65, t=90),\n    scene_camera_eye=dict(x=2.2, y=0.78, z=0.64),\n    scene=dict(\n        xaxis_title='PRECISION',\n        yaxis_title='RECALL',\n        zaxis_title='F1-SCORE'\n    )\n)\n\nfig.update_traces(\n    contours_z=dict(\n        show=True, \n        usecolormap=True,\n        highlightcolor=\"limegreen\", \n        project_z=True)\n)\n\nfig.show()","0370f0de":"# prepare correct metric\nf1_score_macro = partial(f1_score, average='macro')","e51892b5":"DATA_DIR = '\/kaggle\/input\/nlp-getting-started\/'\n# DATA_DIR = '..\/data'\ntrain = pd.read_csv(pjoin(DATA_DIR, 'train.csv'))\ntest = pd.read_csv(pjoin(DATA_DIR, 'test.csv'))\n\n# glue datasets together, for convenience\ntrain['is_train'] = True\ntest['is_train'] = False\ndf = pd.concat(\n    [train, test], \n    sort=False, ignore_index=True\n).set_index('id').sort_index()\n\nprint(train.shape, test.shape, df.shape)\ndf.head()","0bd24dc0":"import re\n\nfrom sklearn.feature_extraction.text import TfidfVectorizer\n\ntfidf_text = TfidfVectorizer(\n    stop_words='english',\n    max_df=0.33,\n    min_df=5,\n    dtype=np.float32,\n    max_features=500,\n)\n\n# fit transformer\ntfidf_text.fit(df['text'])\n\nNON_WORD_PATTERN = r\"[^A-Za-z0-9\\.\\'!\\?,\\$\\s]\"\n\ndf_tfidfs = pd.DataFrame(\n    np.array(tfidf_text.transform(df['text']).todense()),\n    columns=[\n        f'tfidf__{re.sub(NON_WORD_PATTERN, \"\", k)}'\n        for (k, v) in\n        sorted(tfidf_text.vocabulary_.items(), key=lambda item: item[1])\n    ],\n    index=df.index,\n)\nprint(df_tfidfs.shape)\ndf_tfidfs.head()","a48b2fda":"# add keyword\ndf['keyword_cat_codes'] = df.keyword.fillna('missing').astype('category').cat.codes\n\ndf_features = pd.concat(\n    [\n       df_tfidfs,\n       df[['keyword_cat_codes']]\n    ],\n    axis=1\n)\n\nprint(df_features.shape)","70950515":"import lightgbm as lgb\nfrom sklearn.model_selection import train_test_split, StratifiedKFold","85bfec53":"def f1_score_lgb(preds, dtrain):\n    labels = dtrain.get_label()\n    f_score = f1_score_macro(\n        np.round(preds),\n        labels,\n    )\n    return 'f1_score', f_score, True\n\n\nlgb_params = {\n    'num_leaves': 63,\n    'learning_rate': 0.015,\n    'max_depth': -1,\n    'subsample': 0.9,\n    'colsample_bytree': 0.33,\n}\n\nskf = StratifiedKFold(n_splits=5, random_state=42, shuffle=True)\n\n\ncv_res = lgb.cv(\n    params=lgb_params,\n    train_set=lgb.Dataset(\n        data=df_features[df.is_train],\n        label=df.loc[df.is_train, 'target'],\n        categorical_feature=['keyword_cat_codes'],\n    ),\n    folds=skf,\n#     metrics=['auc'],\n    feval=f1_score_lgb,\n    verbose_eval=50,\n    early_stopping_rounds=200,\n    #     eval_train_metric=True,\n    num_boost_round=1000,\n)","305e9676":"# train simple model according to CV's boosting_rounds params\nmodel = lgb.LGBMClassifier(\n    **lgb_params, \n    n_esimators=int( len(cv_res['f1_score-mean']) * (skf.n_splits + 1)\/skf.n_splits )\n)\n\nmodel.fit(\n    X=df_features[df.is_train],\n    y=df.loc[df.is_train, 'target'],\n    categorical_feature=['keyword_cat_codes'],\n)","15e81d45":"# check feature importance\nlgb.plot_importance(\n    model, \n    importance_type='gain', \n    figsize=(10, 10), \n    max_num_features=50\n)","8c93cb41":"# load leaked data\nleaked_labels = pd.read_csv(\n    pjoin('..\/input\/a-real-disaster-leaked-label', 'submission.csv')\n).set_index('id')\ndf.loc[~df.is_train, 'target'] = leaked_labels\ndf.loc[df.is_train, 'target'].mean(), df.loc[~df.is_train, 'target'].mean()","c4c415a7":"# check total f1-score, on 100% test data\ny_pred = pd.Series(\n    model.predict(df_features[~df.is_train]), \n    index=df[~df.is_train].index\n)\nf1_total_test = np.round(\n    f1_score_macro(df.loc[~df.is_train, 'target'], y_pred), 4\n)\n\nf1_total_test","e3bc57f5":"from tqdm.notebook import tqdm\n\n# generate splits\nsampler = np.random.RandomState(911)\nn_trials = 1000\nrandom_seeds = sorted(set(sampler.randint(0, 10e7, n_trials)))\n\n# make private\/public stratified splits\nf1_scores = []\n\nfor rs in tqdm(random_seeds):\n    public_ind, private_ind = train_test_split(\n        df[~df.is_train].index.values, \n        test_size=0.7, \n        random_state=rs,\n        stratify=df.loc[~df.is_train, 'target'],\n    )\n    f1_public = f1_score_macro(\n        y_true=df.loc[public_ind, 'target'],\n        y_pred=y_pred[public_ind]\n    )\n    f1_private = f1_score_macro(\n        y_true=df.loc[private_ind, 'target'],\n        y_pred=y_pred[private_ind]\n    )\n    \n    f1_scores.append((rs, f1_public, f1_private))\n    \ndf_f1 = pd.DataFrame(f1_scores, columns=['random_seed', 'f1_public', 'f1_private'])\ndf_f1['pub_pr_diff'] = df_f1['f1_public'] - df_f1['f1_private']\n\n# clip some extremes\ndf_f1['pub_pr_diff'] = df_f1['pub_pr_diff'].clip(\n    lower=df_f1['pub_pr_diff'].quantile(0.005),\n    upper=df_f1['pub_pr_diff'].quantile(0.995),\n)\n\n# check sample data\nprint(df_f1.shape)\ndf_f1.head()","2fe521fa":"df_f1.pub_pr_diff.describe()","2e98d1d7":"# let's check BOX plots of PUBLIC \/ PRIVATE F1-Scores\ndf_f1[['f1_public', 'f1_private']].iplot(\n    kind='box', \n    dimensions=(640, 320),\n    title='F1-Score range, public\/private LB'\n)","f379ca4d":"# let's see hist of private f1-scores\nf1_mean_private = np.round(df_f1['f1_private'].mean(), 4)\ndf_f1['f1_private'].iplot(\n    kind='hist', \n    title=f'F1 mean_private: {f1_mean_private}<br>F1 total test:       {f1_total_test}',\n    dimensions=(640, 320)\n)","d566363c":"# let's see hist of differences between public\/private score\ndf_f1['pub_pr_diff'].iplot(\n    kind='hist', \n    title='Public\/Private F1-score diff distribution', \n    dimensions=(640, 320)\n)","633fc5a2":"# F1-Score fundamentals\n---\n\n(Extracted from correspondent [WIKI page](https:\/\/en.wikipedia.org\/wiki\/F1_score))\n>In statistical analysis of binary classification, the F1 score (also F-score or F-measure) is a measure of a test's accuracy. It considers both the **precision p** and the **recall r** of the test to compute the score: p is the number of correct positive results divided by the number of all positive results returned by the classifier, and r is the number of correct positive results divided by the number of all relevant samples (all samples that should have been identified as positive). The F1 score is the **harmonic mean** of the precision and recall, where an F1 score **reaches its best value at 1 (perfect precision and recall)** and worst at 0. ","34535e7e":"# F1-Score Surface visualization\n---","6a619385":"Fortunately, private scores distribution is much more narrow and stable\n<br>However, the main thing to remember - **do not blindly trust public leaderboard results**","70b1bd62":"# Data Preparation\n---","2f8736a5":"Hi guys!\n<br>Please take into account F1-score specifics \n<br>and see the **Private\/Public LB shift estimate** below","225d4195":"## Data Loading\n---","b746bede":"# F1-score calculations\n---\nAs far as we got leaked labels, for **educational purposes only** \n<br>let's see how F1-score varies across different private\/public splits on **test data**","e374318b":"## Prepare Features\n---","6d5f15cf":"---\nThat's all for now\n<br>Stay tuned, this notebook is going to be updated soon\n<br>Hope, you guys, like it and learn something new!\n<br>**As always, upvotes, comments, ideas are always welcome!**\n\n---\nP.s. Check my [EDA + Baseline notebook](https:\/\/www.kaggle.com\/frednavruzov\/starter-graph-based-eda-and-baseline-v1)","2e4f3450":"See, **how huge** discrepancy can be - you may (or may not) be lucky enough \n- to get score boost on private\n- as well as drastically drop\n\n<img src=\"https:\/\/media.giphy.com\/media\/Maz1la1GQaCPkxsODl\/giphy.gif\" align=\"left\"\/>","a98b39cd":"# Make baseline","9e22b798":"For simplicity we'll be using simple LGBM classifier atop of \n<br>[USE embeddings](https:\/\/tfhub.dev\/google\/universal-sentence-encoder-large\/5) (or even on simple TF-IDF scores) - our purpose is \n<br>to estimate discrepancy between F1-score on Public and Private Leaderboards, under the following assumptions:\n- Public\/Private LB split is done by 30% \/ 70%\n- This split was stratified by target (around **0.43 mean target** in both folds, according to [leaked data mean](https:\/\/www.kaggle.com\/szelee\/a-real-disaster-leaked-label))\n- F1-score, used for calculation, is averaged by `macro` strategy:\n```\nfrom sklearn.metrics import f1_score\nf1_score(y_true, y_pred, average='macro')\n```\n\nP.s. this [medium post about Macro-F1's](https:\/\/datascience.stackexchange.com\/questions\/15989\/micro-average-vs-macro-average-performance-in-a-multiclass-classification-settin) is highly recommended to read","71f1de74":"## Imports\n---","078a4aa5":"## Cross-validation"}}