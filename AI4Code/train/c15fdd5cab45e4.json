{"cell_type":{"f744dbc2":"code","07aa3d19":"code","7bdc4908":"code","2d3d0556":"code","9ef109a0":"code","3952f39e":"code","0a47727e":"code","24deae02":"code","5edf2a6a":"code","8333eb0d":"code","60696658":"code","9715e1f9":"code","9bbe3cf3":"code","99daeeec":"code","50bef129":"code","c9f4ab6e":"code","9f7fe968":"code","75bc7b10":"code","f4fcad84":"code","92374825":"code","1cb9942c":"code","01501d79":"code","42a3e769":"code","9ef1e96c":"code","00ae7575":"code","a943023a":"code","a1ebfb70":"code","0665942c":"code","e4315bac":"code","e73b53f1":"code","1f8cd636":"code","af9e68a2":"code","a060188f":"code","72c29f33":"code","6a16cd8c":"code","dd12c522":"code","39f4df9b":"code","63fef5c9":"code","eb0a4cd8":"code","98158616":"code","7a482082":"code","5a2c0b84":"code","75428e9e":"code","f16c848f":"code","168790ab":"code","c5cf1dae":"code","901827e5":"code","30bd3f21":"code","cff978b4":"code","0724e6a7":"code","080f5382":"code","401af931":"code","6759a963":"code","a11958fb":"code","7b86494c":"code","0819c2c1":"code","c7572737":"code","f4058555":"code","61e779e5":"code","d5aca41a":"code","480d37da":"code","aba20972":"code","ba8d4373":"code","f8a45a81":"code","e10aa678":"code","97d94465":"code","6afd8f06":"code","b750a405":"code","c23b19e1":"code","7548a2e9":"code","58c9b7ed":"code","9e4c1318":"code","0abb78ed":"code","e7fe5b18":"code","802d4460":"code","f19828b9":"code","9f26ec89":"code","572b3eac":"code","66a29c47":"code","77260f86":"code","adae1bcc":"code","6d334751":"code","24731120":"code","6bf4a554":"code","d661f1d3":"code","2494aa4a":"code","e16a17af":"code","c8e1c1df":"code","f26af024":"code","b958b097":"code","d6805656":"code","639710fc":"code","e8893364":"code","e27fb61e":"code","8d4b0793":"code","98013d2f":"code","fd1cb53e":"code","a0b8838f":"code","e390b2cb":"code","8d8c2614":"code","a3f96f75":"code","8e1e0b7d":"code","5614d6f4":"code","7eaaa1fe":"code","147380fe":"code","81e0c9a2":"code","b23286ff":"code","85d5e629":"code","e30afd6c":"code","6b7ade56":"code","0aac10b5":"code","b7e49397":"code","6ed71eaa":"code","046349f6":"code","95f0e499":"code","af6157bc":"code","edf9e898":"code","cf3c80da":"code","bf1241e6":"code","5494d15e":"code","7afffcb4":"code","04f4083f":"code","a9739e8e":"code","518cf712":"code","ae25a220":"code","98a3f5e1":"code","ef2aca03":"code","8dd302af":"code","4c6895f4":"code","9b14bbe3":"code","9836e4fa":"code","bf597cb7":"code","7b0ad120":"markdown","8b3c1bf5":"markdown","14f28ae5":"markdown","585c1304":"markdown","9b8a83d3":"markdown","78e7d5e6":"markdown","3a72c4f7":"markdown","7eec7d5d":"markdown","b7588fc3":"markdown","2671b143":"markdown","7e5a950a":"markdown","9fbda752":"markdown","8e705322":"markdown","625d695f":"markdown","71575cee":"markdown","fab11124":"markdown","35125f82":"markdown","5dce4485":"markdown","fe77d50b":"markdown","6b2f3aeb":"markdown","45453a0b":"markdown","744be198":"markdown","c4a31173":"markdown","456d6027":"markdown","422180cc":"markdown","71ec0739":"markdown","1c9715a2":"markdown","afaa4b62":"markdown","6b209c71":"markdown","5f36f526":"markdown","b2282796":"markdown","d69199d4":"markdown","3f10d88e":"markdown","f80dee41":"markdown","cc374ead":"markdown","512feb9d":"markdown","4dce7291":"markdown","c6e20480":"markdown","16fe9438":"markdown","71aac44e":"markdown","df2e1a38":"markdown","e5e0b1fa":"markdown","046c609a":"markdown","0b185637":"markdown","ad53ec8b":"markdown","96f212bf":"markdown","111c43d3":"markdown","84eca88b":"markdown","b5360c0e":"markdown","2356f82b":"markdown","ccac6221":"markdown","2908aeaf":"markdown","47bd4b00":"markdown","af5406ed":"markdown","eb1ff85a":"markdown","46c08154":"markdown","318fccfa":"markdown","2b541657":"markdown","645dc351":"markdown","44593c00":"markdown"},"source":{"f744dbc2":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport warnings\nfrom scipy.stats import norm\nfrom scipy import stats\nimport seaborn as sns\nwarnings.filterwarnings('ignore')\n%matplotlib inline","07aa3d19":"data = pd.read_csv('..\/input\/heart-disease-uci\/heart.csv')","7bdc4908":"data.shape","2d3d0556":"data.head()","9ef109a0":"data.info()","3952f39e":"data.describe()","0a47727e":"data.var()","24deae02":"data.hist(bins=50, figsize=(30,25)) \nplt.style.use('fivethirtyeight')\nplt.show()","5edf2a6a":"sns.countplot(x=\"target\", data=data)\nplt.show()","8333eb0d":"sns.displot(data, x=\"age\", hue=\"age\")\nplt.show()","60696658":"sns.countplot(x=\"sex\", data=data)\nplt.xlabel('Male = 1 , female = 0')\nplt.show()","9715e1f9":"sns.countplot(x=\"cp\", data=data)\nplt.show()","9bbe3cf3":"sns.countplot(x=\"fbs\", data=data)\nplt.show()","99daeeec":"sns.countplot(x=\"ca\", data=data)\nplt.show()","50bef129":"sns.countplot(x=\"slope\", data=data)\nplt.show()","c9f4ab6e":"sns.countplot(x=\"restecg\", data=data)\nplt.show()","9f7fe968":"corr_matrix = data.corr()\nf, ax = plt.subplots(figsize=(25, 15))\ncmap = sns.diverging_palette(230, 20, as_cmap=True)\nsns.heatmap(corr_matrix, cmap=cmap, vmax=.5, annot=True, center=0,\n            square=True, linewidths=.5, cbar_kws={\"shrink\": .5})\nplt.show()","75bc7b10":"High_corr = corr_matrix.nlargest(5, 'target')['target'].index\nHigh_corr","f4fcad84":"corr_matrix[\"target\"].sort_values(ascending=False)","92374825":"new_df = data.copy()","1cb9942c":"# This function will calculate the IQR for us and save the values that is higher or lower as follwow\ndef IQR(column_name):\n    Q1 = new_df[column_name].quantile(0.15)\n    Q3 = new_df[column_name].quantile(0.80)\n    IQR = Q3 - Q1\n    upper_limit = Q3 + 1.5 * IQR\n    lower_limit = Q1 - 1.5 * IQR\n    values_upper = new_df[new_df[column_name] > upper_limit]\n    values_lower = new_df[new_df[column_name] < lower_limit]\n    \n    return values_upper, values_lower, upper_limit, lower_limit","01501d79":"# this Function will check if the returned shape from IQR is higher than zero \n# why zero! cos the output will be for example like this (2,63) that means there are 2 rows contains outliers \n# and if it more than zero it will show us this rows\ndef upper(column_name):\n    if values_upper.shape[0] > 0:\n        print(\"Outliers upper than the higher limit: \")\n        return new_df[new_df[column_name] > upper_limit]\n    else:\n        print(\"There are no values higher than the upper limit!\")","42a3e769":"# same as above but for lower values\ndef lower(column_name):\n    if values_lower.shape[0] > 0:\n        print(\"Outliers lower than the higher limit: \")\n        return new_df[new_df[column_name] < lower_limit]\n    else:\n        print(\"There are no values lower than the lower limit!\")","9ef1e96c":"# this function will delete any outliers upper or lower the limit\ndef outliers_del(column_name):\n    # we will make new_df global to consider the global variable not the local\n    global new_df\n    new_df = new_df[new_df[column_name] < upper_limit]\n    new_df = new_df[new_df[column_name] > lower_limit]\n    print(\"the old data shape is :\", data.shape)\n    print(\"the new data shape is :\", new_df.shape)","00ae7575":"# this function is for ploting the data \ndef plot(column_name):\n    plt.style.use('fivethirtyeight')\n    plt.figure(figsize=(16,5))\n    #plt.subplot(1,2,1)\n    # we will use fit norm to draw the normal distibutions that the data sould be it will be in black \n    #sns.distplot(data[column_name], fit=norm)\n    plt.subplot(1,2,1)\n    sns.boxplot(data[column_name],palette=\"rocket\")\n    plt.show()","a943023a":"def outlier_compare(column_name):\n    plt.style.use('fivethirtyeight')\n    plt.figure(figsize=(25,15))\n    plt.subplot(2,2,1)\n    sns.boxplot(data[column_name], palette=\"rocket\")\n    plt.subplot(2,2,2)\n    sns.boxplot(new_df[column_name], palette=\"rocket\")\n    plt.show()","a1ebfb70":"Upper_Outliers_columns = []\nLower_Outliers_columns = []\nnumeric_dtypes = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\nfor column in new_df:\n    if new_df[column].dtype in numeric_dtypes:\n        values_upper, values_lower, upper_limit, lower_limit = IQR(column)\n        if values_upper.shape[0] > 0:\n            Upper_Outliers_columns.append(column)\n        if values_lower.shape[0] > 0:\n            Lower_Outliers_columns.append(column)","0665942c":"print('Columns upper the limit is: ', Upper_Outliers_columns)\nprint('Columns upper the limit is: ', Lower_Outliers_columns)","e4315bac":"plot('trestbps')","e73b53f1":"values_upper, values_lower, upper_limit, lower_limit = IQR('trestbps')","1f8cd636":"upper('trestbps')","af9e68a2":"lower('trestbps')","a060188f":"outliers_del('trestbps')","72c29f33":"outlier_compare('trestbps')","6a16cd8c":"plot('chol')","dd12c522":"values_upper, values_lower, upper_limit, lower_limit = IQR('chol')","39f4df9b":"upper('chol')","63fef5c9":"lower('chol')","eb0a4cd8":"outliers_del('chol')","98158616":"outlier_compare('chol')","7a482082":"plot('oldpeak')","5a2c0b84":"values_upper, values_lower, upper_limit, lower_limit = IQR('oldpeak')","75428e9e":"upper('oldpeak')","f16c848f":"outliers_del('oldpeak')","168790ab":"outlier_compare('oldpeak')","c5cf1dae":"# this function is for ploting the data \ndef skew_plot(column_name):\n    plt.style.use('fivethirtyeight')\n    plt.figure(figsize=(16,9))\n    plt.subplot(2,2,1)\n    sns.distplot(data[column_name], fit=norm)\n    plt.subplot(2,2,2)\n    res = stats.probplot(data[column_name], plot=plt) \n    plt.show()","901827e5":"# calculate the skewness\ndef skew(column_name):\n    print(\"Skewness: %f\" % new_df[column_name].skew())","30bd3f21":"# checking and make sure that this column doesnot contain zeros or negative values\ndef zeros(column_name):\n    if ((new_df[column_name] == 0).any() or (new_df[column_name] < 0).any()) == False:\n        print(\"Your column is clean!\")\n    else:\n        print(\"Watch out you have zeros or negative values here!\")","cff978b4":"# transform the data with log \ndef log(column_name):\n    new_df[column_name] = np.log(new_df[column_name])","0724e6a7":"# transform the data with square root \ndef sqrt(column_name):\n    new_df[column_name] = np.sqrt(new_df[column_name])","080f5382":"# this one to compair the old data before doing any edit on it like correcting the skeness and after \ndef skew_compare(column_name):\n    plt.figure(figsize=(20,15))\n    plt.subplot(2,2,1)\n    sns.distplot(data[column_name], fit=norm)\n    plt.subplot(2,2,2)\n    res = stats.probplot(data[column_name], plot=plt)\n    plt.subplot(2,2,3)\n    sns.distplot(new_df[column_name], fit=norm)\n    plt.subplot(2,2,4)\n    res = stats.probplot(new_df[column_name], plot=plt)\n    plt.show()","401af931":"from scipy.stats import skew\n\nskewness_list = {}\nfor i in new_df:\n    if new_df[i].dtype != \"object\":\n        skewness_list[i] = skew(new_df[i])\n\nskewness = pd.DataFrame({'Skew' :skewness_list})\nplt.style.use('fivethirtyeight')\nplt.figure(figsize=(15,9))\nplt.xlabel('Features', fontsize=15)\nplt.ylabel('Skewness', fontsize=15)\nplt.xticks(rotation='90')\nplt.bar(range(len(skewness_list)), list(skewness_list.values()), align='center')\nplt.xticks(range(len(skewness_list)), list(skewness_list.keys()))\n\nplt.show()","6759a963":"skewness_list","a11958fb":"skew_plot('chol')","7b86494c":"zeros('chol')","0819c2c1":"log('chol')","c7572737":"skew_compare('chol')","f4058555":"X = new_df.drop(\"target\", axis=1)","61e779e5":"y = new_df['target'].copy()","d5aca41a":"X.shape, y.shape","480d37da":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.20, shuffle=True, random_state=42)","aba20972":"from sklearn.preprocessing import MinMaxScaler","ba8d4373":"scaler = MinMaxScaler()","f8a45a81":"X_train = scaler.fit_transform(X_train)","e10aa678":"X_test = scaler.transform(X_test)","97d94465":"# Calssification models\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.ensemble import RandomForestClassifier\n# Cross validation\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import cross_val_predict\nfrom sklearn.model_selection import ShuffleSplit\n# Evaluation metrices\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import plot_confusion_matrix\nfrom sklearn.metrics import roc_curve\nfrom sklearn.metrics import auc\nfrom sklearn.metrics import jaccard_score\nfrom sklearn.metrics import f1_score\nfrom sklearn.metrics import log_loss","6afd8f06":"cv = ShuffleSplit(n_splits=10, test_size=0.3, random_state=0)","b750a405":"LR = LogisticRegression()\nLR_scores = cross_val_score(LR, X_train, y_train, cv=cv)\nLR_scores","c23b19e1":"LR_scores.mean()","7548a2e9":"LR_cv_pred = cross_val_predict(LR, X_train, y_train, cv=10)\naccuracy_score(y_train, LR_cv_pred)","58c9b7ed":"LR.fit(X_train, y_train)\nLR.score(X_train, y_train)","9e4c1318":"KNN = KNeighborsClassifier()\nKNN_scores = cross_val_score(KNN, X_train, y_train, cv=cv)\nKNN_scores","0abb78ed":"KNN_scores.mean()","e7fe5b18":"KNN_cv_pred = cross_val_predict(KNN, X_train, y_train, cv=10)\naccuracy_score(y_train, KNN_cv_pred)","802d4460":"KNN.fit(X_train, y_train)\nKNN.score(X_train, y_train)","f19828b9":"RF = RandomForestClassifier()\nRF_scores = cross_val_score(RF, X_train, y_train, cv=cv)\nRF_scores","9f26ec89":"RF_scores.mean()","572b3eac":"RF_cv_pred = cross_val_predict(RF, X_train, y_train, cv=10)\naccuracy_score(y_train, RF_cv_pred)","66a29c47":"RF.fit(X_train, y_train)\nRF.score(X_train, y_train)","77260f86":"from sklearn.model_selection import GridSearchCV","adae1bcc":"param_grid = [\n        {'penalty': ['l1', 'l2', 'elasticnet', 'none'], 'C': [.001, .01, .1, 1]},\n        {'solver': ['newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga'], 'max_iter': [100, 1000, 10000], \n         'multi_class': ['auto', 'ovr', 'multinomial']},\n]\n\nLR = LogisticRegression()\ngrid_search = GridSearchCV(LR, param_grid, cv=cv,scoring='accuracy',return_train_score=True)\ngrid_search.fit(X_train, y_train)","6d334751":"grid_search.best_params_","24731120":"grid_search.best_estimator_","6bf4a554":"param_grid = [\n        {'n_neighbors': [2, 3, 4, 5, 6], 'weights': ['uniform','distance']},\n        {'algorithm': ['auto', 'ball_tree', 'kd_tree', 'brute'], 'leaf_size': [30, 40, 50]},\n]\n\nKNN = KNeighborsClassifier()\ngrid_search = GridSearchCV(KNN, param_grid, cv=cv,scoring='accuracy',return_train_score=True)\ngrid_search.fit(X_train, y_train)","d661f1d3":"grid_search.best_params_","2494aa4a":"grid_search.best_estimator_","e16a17af":"param_grid = [\n        {'n_estimators': [100, 150, 200, 250, 300], 'criterion': ['gini','entropy']},\n        {'max_depth': [1, 2, 3, 4, 5,6,7,8,9,10], 'max_features': ['auto', 'sqrt', 'log2']},\n]\n\nRF = RandomForestClassifier()\ngrid_search = GridSearchCV(RF, param_grid, cv=cv,scoring='accuracy',return_train_score=True)\ngrid_search.fit(X_train, y_train)","c8e1c1df":"grid_search.best_params_","f26af024":"grid_search.best_estimator_","b958b097":"LR = LogisticRegression(solver='saga')\nLR_scores = cross_val_score(LR, X_train, y_train, cv=cv)\nLR_scores","d6805656":"LR_score = LR_scores.mean()","639710fc":"LR_cv_pred = cross_val_predict(LR, X_train, y_train, cv=10)\naccuracy_score(y_train, LR_cv_pred)","e8893364":"LR.fit(X_train, y_train)\nLR.score(X_train, y_train)","e27fb61e":"LR_pred = LR.predict(X_train)","8d4b0793":"confusion_matrix(y_train, LR_pred)","98013d2f":"KNN = KNeighborsClassifier(n_neighbors=6, weights='distance')\nKNN_scores = cross_val_score(KNN, X_train, y_train, cv=cv)\nKNN_scores","fd1cb53e":"KNN_scores.mean()","a0b8838f":"KNN_cv_pred = cross_val_predict(KNN, X_train, y_train, cv=10)\naccuracy_score(y_train, KNN_cv_pred)","e390b2cb":"KNN.fit(X_train, y_train)\nKNN.score(X_train, y_train)","8d8c2614":"KNN_pred = KNN.predict(X_train)","a3f96f75":"confusion_matrix(y_train, KNN_pred)","8e1e0b7d":"RF = RandomForestClassifier(max_depth=2)\nRF_scores = cross_val_score(RF, X_train, y_train, cv=cv)\nRF_scores","5614d6f4":"RF_scores.mean()","7eaaa1fe":"RF_cv_pred = cross_val_predict(RF, X_train, y_train, cv=10)\naccuracy_score(y_train, RF_cv_pred)","147380fe":"RF.fit(X_train, y_train)\nRF.score(X_train, y_train)","81e0c9a2":"RF_pred = RF.predict(X_train)","b23286ff":"confusion_matrix(y_train, RF_pred)","85d5e629":"LR_pred = LR.predict(X_test)","e30afd6c":"accuracy_score(y_test, LR_pred)","6b7ade56":"confusion_matrix = confusion_matrix(y_test, LR_pred)","0aac10b5":"plot_confusion_matrix(LR, X_test, y_test, cmap=plt.cm.Blues)  \nplt.show() ","b7e49397":"fpr, tpr, thresholds = roc_curve(y_test, LR_pred)\n\nfig, ax = plt.subplots()\nax.plot(fpr, tpr)\nax.plot([0, 1], [0, 1], transform=ax.transAxes, ls=\"--\", c=\".3\")\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.0])\nplt.rcParams['font.size'] = 12\nplt.title('ROC curve for diabetes classifier')\nplt.xlabel('False Positive Rate (1 - Specificity)')\nplt.ylabel('True Positive Rate (Sensitivity)')\nplt.grid(True)","6ed71eaa":"auc(fpr, tpr)","046349f6":"jaccard_score(y_test, LR_pred)","95f0e499":"f1_score(y_test, LR_pred)","af6157bc":"log_loss(y_test, LR_pred)","edf9e898":"KNN_pred = KNN.predict(X_test)","cf3c80da":"accuracy_score(y_test, KNN_pred)","bf1241e6":"plot_confusion_matrix(KNN, X_test, y_test, cmap=plt.cm.Blues)  \nplt.show() ","5494d15e":"fpr, tpr, thresholds = roc_curve(y_test, KNN_pred)\n\nfig, ax = plt.subplots()\nplt.style.use('fivethirtyeight')\nax.plot(fpr, tpr)\nax.plot([0, 1], [0, 1], transform=ax.transAxes, ls=\"--\", c=\".3\")\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.0])\nplt.rcParams['font.size'] = 12\nplt.title('ROC curve for diabetes classifier')\nplt.xlabel('False Positive Rate (1 - Specificity)')\nplt.ylabel('True Positive Rate (Sensitivity)')\nplt.grid(True)","7afffcb4":"auc(fpr, tpr)","04f4083f":"jaccard_score(y_test, KNN_pred)","a9739e8e":"f1_score(y_test, KNN_pred)","518cf712":"log_loss(y_test, KNN_pred)","ae25a220":"RF_pred = RF.predict(X_test)","98a3f5e1":"accuracy_score(y_test, RF_pred)","ef2aca03":"plot_confusion_matrix(RF, X_test, y_test, cmap=plt.cm.Blues)  \nplt.show() ","8dd302af":"fpr, tpr, thresholds = roc_curve(y_test, RF_pred)\n\nfig, ax = plt.subplots()\nax.plot(fpr, tpr)\nax.plot([0, 1], [0, 1], transform=ax.transAxes, ls=\"--\", c=\".3\")\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.0])\nplt.rcParams['font.size'] = 12\nplt.title('ROC curve for diabetes classifier')\nplt.xlabel('False Positive Rate (1 - Specificity)')\nplt.ylabel('True Positive Rate (Sensitivity)')\nplt.grid(True)","4c6895f4":"auc(fpr, tpr)","9b14bbe3":"jaccard_score(y_test, RF_pred)","9836e4fa":"f1_score(y_test, RF_pred)","bf597cb7":"log_loss(y_test, RF_pred)","7b0ad120":"#### RF","8b3c1bf5":"lets define some new functions here","14f28ae5":"### Evaluation in classification is different than evaluation in regression, in classification we have a lot of metrics to evaluate our model like \n\n    F1_score\n    Confusion Matrix\n    Roc curve\n    Auc \n    Jaccard_score\n    Log_less\n    \n#### F1_score:- The F1 score can be interpreted as a weighted average of the precision and recall, where an F1 score reaches its best value at 1 and worst score at 0.\n\n<img src=\"https:\/\/forums.fast.ai\/uploads\/default\/original\/3X\/c\/c\/cca1b3ad72fc927fbf3d3690f01d2e3b5a31dd2e.png\">\n\n#### Confusion Matrix:- It is a table with 4 different combinations of predicted and actual values, Ture positive \"TP\", True negative \"TN\", False positive \"FP\" and False negative \"FN\", It is extremely useful for measuring Recall, Precision, Specificity, Accuracy, and most importantly AUC-ROC curves.\n\n<img src=\"https:\/\/miro.medium.com\/max\/712\/1*Z54JgbS4DUwWSknhDCvNTQ.png\">\n\n#### Roc curve:- An ROC curve (receiver operating characteristic curve) is a graph showing the performance of a classification model at all classification thresholds. This curve plots two parameters:\n\nTrue Positive Rate (TPR) is a synonym for recall \n \nFalse Positive Rate (FPR) \n\n<img src=\"https:\/\/developers.google.com\/machine-learning\/crash-course\/images\/ROCCurve.svg\" style=\"width:500px;height:600px;\">\n \n#### AUC:- AUC stands for \"Area under the ROC Curve.\" That is, AUC measures the entire two-dimensional area underneath the entire ROC curve\n\n<img src=\"https:\/\/developers.google.com\/machine-learning\/crash-course\/images\/AUC.svg\" style=\"width:500px;height:600px;\">\n\nAUC provides an aggregate measure of performance across all possible classification thresholds.\n\n\n#### Jaccard_score:- Jaccard similarity coefficient, defined as the size of the intersection divided by the size of the union of two label sets, is used to compare set of predicted labels for a sample to the corresponding set of labels in y_true.\n\n\n<img src=\"https:\/\/miro.medium.com\/max\/744\/1*XiLRKr_Bo-VdgqVI-SvSQg.png\" >\n\n\n#### Log_less:- loss function used in (multinomial) logistic regression and extensions of it such as neural networks, defined as the negative log-likelihood of a logistic model that returns y_pred probabilities for its training data y_true. The log loss is only defined for two or more labels.\n\n\n\n\n<math xmlns=\"http:\/\/www.w3.org\/1998\/Math\/MathML\" display=\"block\">\n  <msub>\n    <mi>L<\/mi>\n    <mrow data-mjx-texclass=\"ORD\">\n      <mi>log<\/mi>\n    <\/mrow>\n  <\/msub>\n  <mo stretchy=\"false\">(<\/mo>\n  <mi>y<\/mi>\n  <mo>,<\/mo>\n  <mi>p<\/mi>\n  <mo stretchy=\"false\">)<\/mo>\n  <mo>=<\/mo>\n  <mo>&#x2212;<\/mo>\n  <mo stretchy=\"false\">(<\/mo>\n  <mi>y<\/mi>\n  <mi>log<\/mi>\n  <mo data-mjx-texclass=\"NONE\">&#x2061;<\/mo>\n  <mo stretchy=\"false\">(<\/mo>\n  <mi>p<\/mi>\n  <mo stretchy=\"false\">)<\/mo>\n  <mo>+<\/mo>\n  <mo stretchy=\"false\">(<\/mo>\n  <mn>1<\/mn>\n  <mo>&#x2212;<\/mo>\n  <mi>y<\/mi>\n  <mo stretchy=\"false\">)<\/mo>\n  <mi>log<\/mi>\n  <mo data-mjx-texclass=\"NONE\">&#x2061;<\/mo>\n  <mo stretchy=\"false\">(<\/mo>\n  <mn>1<\/mn>\n  <mo>&#x2212;<\/mo>\n  <mi>p<\/mi>\n  <mo stretchy=\"false\">)<\/mo>\n  <mo stretchy=\"false\">)<\/mo>\n<\/math>\n\n\nnow let's try out some metrics\n\n","585c1304":"### KNeighborsClassifier","9b8a83d3":"### Data spliting ","78e7d5e6":"F1_score","3a72c4f7":"### Loading the data","7eec7d5d":"AUC","b7588fc3":"well the grid search gave me max_depth at 2 for the first time i run the algorithm and gave me 3 for the second time but 2 works fine for me!","2671b143":"#### KNeighborsClassifier","7e5a950a":"### Skewness","9fbda752":"## Performance Measures","8e705322":"i will igonre fbs and thal cos those are cat columns ","625d695f":"Log_Less","71575cee":"### LogisticRegression","fab11124":"## Evaluate Our System on the Test Set ","35125f82":"Log_Less","5dce4485":"ok lets see how the new hyperparameters will performe","fe77d50b":"ROC","6b2f3aeb":"#### KNN","45453a0b":"AUC","744be198":"#### LogisticRegression","c4a31173":"#### RandomForestClassifier","456d6027":"#### LogisticRegression","422180cc":"Log_Less","71ec0739":"### Take a quick look at our data","1c9715a2":"well i tried my best here if you have any good ideas that can improve my methodology and my code feel free to make a comment and let me know!","afaa4b62":"#### KNeighborsClassifier","6b209c71":"Jaccard_score","5f36f526":"# Our methodology\n\n\n## Data visualization \n    Loading the data\n    Take a quick look at our data\n    Understanding our data\n    Finding the correlations \n    \n## Data preperation \n    Outliers detection\n    Skweness correction \n    Data spliting\n    Feature scaling\n    \n## Modeling\n    Building the model\n    Evaluation with cross-validation\n    \n## Fine-tuning \n    Finding the best hyperparameters\n    \n## Performance evaluation\n    Evaluate our model with the new hyperparameters\n    \n## Testing our model\n    Evaluate the model with the test set\n    ","b2282796":"Machine learning algorithms like linear regression, logistic regression, neural network, etc. that use gradient descent as an optimization technique require data to be scaled.\n\nwe have three methods in sklearn \n\nMinMaxScaler(feature_range = (0, 1)) will transform each value in the column proportionally within the range [0,1]. Use this as the first scaler choice to transform a feature, as it will preserve the shape of the dataset (no distortion).\n\nStandardScaler() will transform each value in the column to range about the mean 0 and standard deviation 1, ie, each value will be normalised by subtracting the mean and dividing by standard deviation. Use StandardScaler if you know the data distribution is normal.\n\nIf there are outliers, use RobustScaler(). Alternatively you could remove the outliers and use either of the above 2 scalers (choice depends on whether data is normally distributed)\n\nWe delete most outliers earlier so we can use MinMaxScaler or StandardScaler\n\n","d69199d4":"## Fine-Tune Our Model","3f10d88e":"Jaccard_score","f80dee41":"Jaccard_score","cc374ead":"### Importing needed libraries","512feb9d":"## Data preperation","4dce7291":"### Understanding our data","c6e20480":"ROC","16fe9438":"#### But before getting our hands dirty lets define some functions that we will use a lot like \n    \"IQR\" to calculate the IQR for us \n    \"Upper and Lower\" to fetch upper values and lower values that contain outliers \n    \"outliers_del\" to delete them \n    \"Plot\" function to plot the curves \n    \"outlier_compare\" to compare the data before deleting outliers and correct the skewness and after\n    \nI will write a comment for each function when creating it","71aac44e":"After we trained our model and take an idea about how it performed no time to find the optimal hyperparameters of the model\nOne way to do that would be to fiddle with the hyperparameters manually until you find a great combination of hyperparameter values. This would be very tedious work, and you may not have time to explore many combinations.\nInstead, you should get Scikit-Learn\u2019s GridSearchCV to search for you. All you need to do is tell it which hyperparameters you want it to experiment with, and what values to try out, and it will evaluate all the possible combinations of hyperparameter values, using cross-validation","df2e1a38":"\n#### Attribute Information: \n\nage \n\nsex \n\nchest pain type (4 values) \n\nresting blood pressure \n\nserum cholestoral in mg\/dl \n\nfasting blood sugar > 120 mg\/dl\n\nresting electrocardiographic results (values 0,1,2)\n\nmaximum heart rate achieved \n\nexercise induced angina \n\noldpeak = ST depression induced by exercise relative to rest \n\nthe slope of the peak exercise ST segment \n\nnumber of major vessels (0-3) colored by flourosopy \n\nthal: 3 = normal; 6 = fixed defect; 7 = reversable defect","e5e0b1fa":"### LR","046c609a":"#### RandomForestClassifier","0b185637":"it looks like we have a few of outliers here we will deal with them later!","ad53ec8b":"well we dont have any missing data and all our feature are int!","96f212bf":"### Feature scaling or Data scaling","111c43d3":"confusion_matrix","84eca88b":"F1_score","b5360c0e":"ROC","2356f82b":"confusion_matrix","ccac6221":"Well lets get started!","2908aeaf":"F1_score","47bd4b00":"### Find the correlations","af5406ed":"## Modeing and Evaluation Using Cross-Validation","eb1ff85a":"AUC","46c08154":"confusion_matrix","318fccfa":"#### trestbps","2b541657":"## Data visualization","645dc351":"### RandomForestClassifier","44593c00":"### Outliers Detection\n\nwe have a various methods to detect the outliers i am going to use IQR here this method works fine for me but \n\nyou can try other methods like \n\n            1- Z-score method\n            2. Robust Z-score\n            3. I.Q.R method\n            4. Winterization method(Percentile Capping)\n            5. DBSCAN Clustering\n            6. Isolation Forest\n            7. Visualizing the data\n            \nIQR stands for \"Inter Quartiles Range\"\n\nthis method depends on two values \n    \n    Q1 >> which represents a quarter of the way through the list of all data usually this value is 0.25 but i will use .15 trying not to delete a lot of data \n    \n    Q3 >> which represents three-quarters of the way through the list of all data usually this value is 0.75 but i will use .80 for the same resone\n    \nhow IQR works :\n    well first it sorts the data and finds its median \n    then seperate the numbers before the median and finds its own median \"Q1\"  and also seperates the numbers \n    after the total medain and finds its own median \"Q3\"\n    \n<img src=\"https:\/\/upload.wikimedia.org\/wikipedia\/commons\/thumb\/1\/1a\/Boxplot_vs_PDF.svg\/1200px-Boxplot_vs_PDF.svg.png\">\n\nthen we will take the diffrance between Q3 and Q1"}}