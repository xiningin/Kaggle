{"cell_type":{"58294308":"code","8a93b896":"code","b9d0b7d9":"code","87f61e3a":"code","599ddc6e":"code","86e8677e":"code","3a1eb36c":"code","cc3e8d21":"code","53211001":"code","abacef20":"code","891655f2":"code","ef7dc144":"code","55b30e85":"code","637292e6":"code","6bc019f4":"code","18cd6ce4":"code","26b16461":"code","966ebcc7":"code","af8afdb2":"code","b3f976ae":"code","c1424039":"code","f4430c4b":"code","f3d64d25":"code","d2513a47":"code","2fa8d140":"code","8efe5d38":"markdown","df23d1d6":"markdown","b94d4262":"markdown","058d556b":"markdown","b6e9084c":"markdown","b64f0a68":"markdown","ce31b537":"markdown","fe16769f":"markdown","f69af6c1":"markdown","801fdc9b":"markdown","a5a8d7ee":"markdown"},"source":{"58294308":"#Import the necessary libraries for data prep and viz\nfrom tqdm.notebook import tqdm\nimport numpy as np\nimport pandas as pd\nimport tensorflow as tf\nimport tensorflow.keras\nimport matplotlib.pyplot as plt\nfrom sklearn.metrics import accuracy_score\nfrom nltk.stem import WordNetLemmatizer\nfrom sklearn.model_selection import train_test_split\nimport regex as re\nimport transformers\nfrom keras import backend as K\nimport plotly.express as px\nimport imageio\nfrom wordcloud import WordCloud, STOPWORDS\n\ndata=pd.read_csv('..\/input\/mbti-type\/mbti_1.csv')\ndata.head()","8a93b896":"#Check if TPU is available\nuse_tpu = True\ntry:\n    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()  # TPU detection\n    print('Running on TPU ', tpu.cluster_spec().as_dict()['worker'])\nexcept ValueError:\n    tpu = None\n\nif tpu:\n    tf.config.experimental_connect_to_cluster(tpu)\n    tf.tpu.experimental.initialize_tpu_system(tpu)\n    strategy = tf.distribute.experimental.TPUStrategy(tpu)\nelse:\n    strategy = tf.distribute.MirroredStrategy()\n\nprint(\"REPLICAS: \", strategy.num_replicas_in_sync)","b9d0b7d9":"px.pie(data,names='type',title='Distribution of personality types',hole=0.3)","87f61e3a":"\nfig, ax = plt.subplots(len(data['type'].unique()), sharex=True, figsize=(15,10*len(data['type'].unique())))\n\nk = 0\nfor i in data['type'].unique():\n    df_4 = data[data['type'] == i]\n    wordcloud = WordCloud().generate(df_4['posts'].to_string())\n    ax[k].imshow(wordcloud)\n    ax[k].set_title(i)\n    ax[k].axis(\"off\")\n    k+=1","599ddc6e":"data['type'].value_counts()","86e8677e":"def clean_text(data):\n    data_length=[]\n    lemmatizer=WordNetLemmatizer()\n    cleaned_text=[]\n    for sentence in tqdm(data.posts):\n        sentence=sentence.lower()\n        \n        #removing links from text data\n        sentence=re.sub('https?:\/\/[^\\s<>\"]+|www\\.[^\\s<>\"]+',' ',sentence)\n    \n        #removing other symbols\n        sentence=re.sub('[^0-9a-z]',' ',sentence)\n    \n        \n        data_length.append(len(sentence.split()))\n        cleaned_text.append(sentence)\n    return cleaned_text","3a1eb36c":"data.posts = clean_text(data)\ndata","cc3e8d21":"#Split dataset\nfrom sklearn.model_selection import train_test_split\n\nposts = data['posts'].values\nlabels =  data['type'].values\ntrain_data, test_data = train_test_split(data, random_state=0, test_size=0.2)\n\ntrain_size = len(train_data)\ntest_size = len(test_data)\ntrain_size, test_size","53211001":"#Initialize Bert tokenizer and masks\nfrom transformers import BertTokenizer\nfrom keras.preprocessing.sequence import pad_sequences\n\nbert_model_name = 'bert-base-uncased'\n\ntokenizer = BertTokenizer.from_pretrained(bert_model_name, do_lower_case=True)\nMAX_LEN = 1800\n\ndef tokenize_sentences(sentences, tokenizer, max_seq_len = 1800):\n    tokenized_sentences = []\n\n    for sentence in tqdm(sentences):\n        tokenized_sentence = tokenizer.encode(\n                            sentence,                  # Sentence to encode.\n                            add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n                            max_length = max_seq_len,  # Truncate all sentences.\n                    )\n        \n        tokenized_sentences.append(tokenized_sentence)\n        \n    return tokenized_sentences\n\ndef create_attention_masks(tokenized_and_padded_sentences):\n    attention_masks = []\n\n    for sentence in tokenized_and_padded_sentences:\n        att_mask = [int(token_id > 0) for token_id in sentence]\n        attention_masks.append(att_mask)\n\n    return np.asarray(attention_masks)\n\ntrain_input_ids = tokenize_sentences(train_data['posts'], tokenizer, MAX_LEN)\ntrain_input_ids = pad_sequences(train_input_ids, maxlen=MAX_LEN, dtype=\"long\", value=0, truncating=\"post\", padding=\"post\")\ntrain_attention_masks = create_attention_masks(train_input_ids)\n\ntest_input_ids = tokenize_sentences(test_data['posts'], tokenizer, MAX_LEN)\ntest_input_ids = pad_sequences(test_input_ids, maxlen=MAX_LEN, dtype=\"long\", value=0, truncating=\"post\", padding=\"post\")\ntest_attention_masks = create_attention_masks(test_input_ids)","abacef20":"#train_masks,test_masks, _, _ = train_test_split(attention_masks, labels, random_state=0, test_size=0.2)","891655f2":"#Create train and test datasets\nBATCH_SIZE=32 \nNR_EPOCHS=20\n#def create_dataset(data_tuple, epochs=1, batch_size=32, buffer_size=10000, train=True):\n#    dataset = tf.data.Dataset.from_tensor_slices(data_tuple)\n#    if train:\n#        dataset = dataset.shuffle(buffer_size=buffer_size)\n#    dataset = dataset.repeat(epochs)\n#    dataset = dataset.batch(batch_size)\n#    if train:\n#        dataset = dataset.prefetch(1)\n    \n #   return dataset\n\n#train_dataset = create_dataset((train_inputs, train_masks, train_labels), epochs=NR_EPOCHS, batch_size=BATCH_SIZE)\n#test_dataset = create_dataset((test_inputs, test_masks, test_labels), epochs=NR_EPOCHS, batch_size=BATCH_SIZE, train=False)","ef7dc144":"\n#from transformers import TFBertModel\n\n#from tensorflow.keras.layers import Dense, Flatten\n\n#class BertClassifier(tf.keras.Model):    \n#        def __init__(self, bert: TFBertModel, num_classes: int):\n#            super().__init__()\n#            self.bert = bert\n#            self.classifier = Dense(16, activation='softmax')\n\n#        @tf.function\n#        def call(self, input_ids, attention_mask=None, token_type_ids=None, position_ids=None, head_mask=None):\n#            outputs = self.bert(input_ids,\n#                                   attention_mask=attention_mask,\n#                                   token_type_ids=token_type_ids,\n#                                   position_ids=position_ids,\n#                                   head_mask=head_mask)\n#            cls_output = outputs[1]\n#            cls_output = self.classifier(cls_output)\n\n#            return cls_output\n        \n        \n#with strategy.scope():        \n#    model = BertClassifier(TFBertModel.from_pretrained(bert_model_name), len(label_cols))","55b30e85":"#Define f1 functions for evaluation\ndef f1_m(y_true, y_pred):\n    precision = precision_m(y_true, y_pred)\n    recall = recall_m(y_true, y_pred)\n    return 2*((precision*recall)\/(precision+recall+K.epsilon()))\n\ndef recall_m(y_true, y_pred):\n    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n    recall = true_positives \/ (possible_positives + K.epsilon())\n    return recall\n\ndef precision_m(y_true, y_pred):\n    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n    precision = true_positives \/ (predicted_positives + K.epsilon())\n    return precision","637292e6":"def create_model(): \n    input_word_ids = tf.keras.layers.Input(shape=(MAX_LEN,), dtype=tf.int32,\n                                           name=\"input_word_ids\")\n    bert_layer = transformers.TFBertModel.from_pretrained('bert-large-uncased')\n    bert_outputs = bert_layer(input_word_ids)[0]\n    pred = tf.keras.layers.Dense(16, activation='softmax')(bert_outputs[:,0,:])\n    \n    model = tf.keras.models.Model(inputs=input_word_ids, outputs=pred)\n    loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n    model.compile(loss='categorical_crossentropy', optimizer=tf.keras.optimizers.Adam(\n    learning_rate=0.00002), metrics=['accuracy', f1_m, precision_m, recall_m])\n    return model","6bc019f4":"use_tpu = True\nif use_tpu:\n    # Create distribution strategy\n    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n    tf.config.experimental_connect_to_cluster(tpu)\n    tf.tpu.experimental.initialize_tpu_system(tpu)\n    strategy = tf.distribute.experimental.TPUStrategy(tpu)\n\n    # Create model\n    with strategy.scope():\n        model = create_model()\nelse:\n    model = create_model()\n    \nmodel.summary()","18cd6ce4":"types = np.unique(data.type.values)\n\ndef get_type_index(string):\n    return list(types).index(string)","26b16461":"train_data['type_index'] = data['type'].apply(get_type_index)\ntrain_data","966ebcc7":"one_hot_labels = tf.keras.utils.to_categorical(train_data.type_index.values, num_classes=16)","af8afdb2":"\nmodel.fit(np.array(train_input_ids), one_hot_labels, verbose = 1, epochs = NR_EPOCHS, batch_size = BATCH_SIZE,  callbacks = [tf.keras.callbacks.EarlyStopping(patience = 5)])","b3f976ae":"test_data['type_index'] = data['type'].apply(get_type_index)\ntest_data","c1424039":"test_labels = tf.keras.utils.to_categorical(test_data.type_index.values, num_classes=16)","f4430c4b":"model.evaluate(np.array(test_input_ids), test_labels)","f3d64d25":"cols = data['type'].unique()\ncols = cols.tolist()\n\ncolnames = ['sentence']\ncolnames = colnames+cols\n","d2513a47":"\ndf_predict = pd.DataFrame(columns = colnames)\nsentence = \"Time to debate on it. Strike at the weakest point and make others cry with facts\"\n\ndf_predict.loc[0, 'sentence'] = sentence","2fa8d140":"sentence_inputs = tokenize_sentences(df_predict['sentence'], tokenizer, MAX_LEN)\nsentence_inputs = pad_sequences(sentence_inputs, maxlen=MAX_LEN, dtype=\"long\", value=0, truncating=\"post\", padding=\"post\")\nprediction = model.predict(np.array(sentence_inputs))\ndf_predict.loc[0, cols] = prediction\n\ndf_predict","8efe5d38":"**The accuracy is not great, this is because the data inside the dataset is very imbalanced, which causes the huge disparation between training score and testing score (overfitting).**","df23d1d6":"Where each person will have a personality abbreviated from the combination of all 4 axis i.e. someone who is introverted, relies more on intuitions, feeling, and perceive rather than judge will be be labelled as an INFP.\nThe combinations of all these axis creates a more complex traits, strengths, and weaknesses.\n\n![](https:\/\/yassinetounsi.com\/storage\/2021\/05\/MBTI-Types.jpg)\n\nIt is one of, if not the, the most popular personality test in the world. It is used in businesses, online, for fun, for research and lots more. A simple google search reveals all of the different ways the test has been used over time. It\u2019s safe to say that this test is still very relevant in the world in terms of its use.","b94d4262":"# BERT Model\n* Load the pretrained BERT base-model from Transformers library\n- Take the first hidden-state from BERT output (corresponding to CLS token) and feed it into a Dense layer with 16 neurons and softmax activation","058d556b":"\n\n**This dataset contains quite a lot of URLs and symbols, so let's go ahead and remove those.**","b6e9084c":"# **Data Pipeline**","b64f0a68":"## **INTRODUCTIONS**\nMBTI is a personality  Myers Briggs Type Indicator is a personality type system that divides personalities into 16 distinct  types across 4 axis:\n![](http:\/\/www.allkpop.com\/upload\/2020\/03\/content\/051754\/1583448868-chart.png)","ce31b537":"**Test the model to predict a single sentence. ","fe16769f":"**==================================================================================================================================================**\n**==================================================================================================================================================**","f69af6c1":"**Initialize BERT Tokenizer and attention masks******","801fdc9b":"**Data is extremely imbalanced.** This might cause overfitting to happen since the total data only amounts to 8675.\nBut first, let's see the most significant words in each personality type!","a5a8d7ee":"**Run test and evaluate accuracy**"}}