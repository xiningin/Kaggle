{"cell_type":{"29989571":"code","c54af027":"code","5ffc51e5":"code","d33b372b":"code","8baa50f8":"code","0f388e7e":"code","43c337e7":"code","30e1496e":"code","3d2d275b":"code","73023ff8":"code","47969b92":"code","8d3800d0":"code","1f7c8f06":"code","93c4fbe8":"code","35f61b16":"code","2b48e6c5":"code","ef104e7d":"code","86acfe38":"code","b49476be":"code","b841c14e":"code","3c28ca16":"code","949254d7":"code","0564ba70":"code","7baf315c":"code","540f7cf0":"code","b28636b7":"code","b19fa47b":"markdown","c039351a":"markdown","c1133650":"markdown","e8f2493e":"markdown","5088423e":"markdown","85900464":"markdown","f3d5df2e":"markdown","bc6a7c11":"markdown","13ded61a":"markdown","47b64be3":"markdown","97bc22b6":"markdown","0ba2272c":"markdown","e7c74881":"markdown","9691e8c8":"markdown","a0a815c2":"markdown","72db0725":"markdown","1bebeaf5":"markdown","8867ffab":"markdown","d21344b9":"markdown","d5ec1fa7":"markdown","dcc6891b":"markdown"},"source":{"29989571":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","c54af027":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom sklearn.preprocessing import MinMaxScaler,RobustScaler,StandardScaler\n\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.model_selection import train_test_split,cross_val_score,StratifiedKFold\nfrom sklearn.experimental import enable_iterative_imputer\nfrom sklearn.impute import KNNImputer,IterativeImputer\nfrom sklearn.feature_selection import SelectKBest,chi2,mutual_info_classif\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\nfrom sklearn.svm import SVC\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.decomposition import PCA\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.ensemble import RandomForestClassifier,GradientBoostingClassifier,VotingClassifier\n\nsns.set_style('whitegrid')\nfrom sklearn.metrics import accuracy_score","5ffc51e5":"train=pd.read_csv('\/kaggle\/input\/titanic\/train.csv')\ntest=pd.read_csv('\/kaggle\/input\/titanic\/test.csv')\n\nsubmit=pd.DataFrame(test['PassengerId'])","d33b372b":"train['Sex'].replace({'male':0,'female':1},inplace=True)\ntrain['Embarked'].replace({'S':1,'C':2,'Q':3},inplace=True)\n","8baa50f8":"train['title']=0\nfor i in range(0,len(train)):\n    train.loc[i,'title']=train['Name'].iloc[i].split(',')[1].split('.')[0][1:]\ntrain['title'].replace({'Mr':1,'Miss':2,'Mrs':2,'Master':3,'Dr':4,'Rev':5},inplace=True)\ntrain['title'].replace(['Major','Mlle','Col','Don','the Countess','Sir','Capt','Mme','Lady','Jonkheer','Ms'],6,inplace=True)","0f388e7e":"train['family']=train['SibSp']+train['Parch']+1\ndef family(size):\n    a=''\n    if(size<=1):\n        a=1    #Alone\n    elif(size<=2):\n        a=2    #Couple\n    elif(size<=4):\n        a=3    #small family\n    elif(size<=6):\n        a=4   #medium amilhy\n    else:\n        a=5   #large family\n    return a\ntrain['family']=train['family'].map(family)\n\n\n","43c337e7":"for i in range(len(train)):\n    if not(pd.isnull(train['Cabin'].iloc[i])):\n        train.loc[i,'Cabin']=train['Cabin'].loc[i][0]\ntrain['Cabin'].replace({'C':1,'B':2,'D':3,'E':4,'A':5,'F':6,'G':7,'T':8},inplace=True)\ntrain['Fare']=np.sqrt(train['Fare'])","30e1496e":"train.drop(['Name','SibSp','Parch','Ticket','PassengerId','Cabin'],axis=1,inplace=True)\n\n","3d2d275b":"train.hist(figsize=(15,10))\nplt.show()","73023ff8":"fig,ax=plt.subplots(3,1,figsize=(15,13))\nsns.heatmap(train.corr('spearman'),annot=True,ax=ax[0],label='spearman')    #spearman \nsns.heatmap(train.corr('kendall'),annot=True,ax=ax[1],label='kendall')      #Kendall\nsns.heatmap(train.corr('pearson'),annot=True,ax=ax[2],label='pearson')      #pearson  ","47969b92":"sns.catplot(x='Embarked',data=train,kind='count',hue='Survived',col='Sex')  \n# -----> Male from Southampton has lesser chance to survive\n# -----> Female mostly from 1st and 2nd class a lot more chance to survive\n                         ","8d3800d0":"sns.countplot(x='family',data=train,hue='Survived') # small family has more chance to survive of size 2 and 3","1f7c8f06":"sns.countplot(x='title',data=train,hue='Survived')  # women has a lot more chance to survive according to title","93c4fbe8":"sns.ecdfplot(x='Age',data=train,hue='Survived')\nplt.annotate('The plot has a little up showing young children to survive',xy=(13,0.17),xytext=(60,0.3),arrowprops=({'color':'gray'}))\nplt.show()","35f61b16":"train=pd.get_dummies(train,columns=['Pclass','Embarked','title','family'],drop_first=True)\n\nimpute=KNNImputer(n_neighbors=13)\ntrain=pd.DataFrame(impute.fit_transform(train),columns=train.columns)\n","2b48e6c5":"model=[]\nmodel.append(('Logistic Regression',LogisticRegression(max_iter=1000)))\nmodel.append(('LDA',LinearDiscriminantAnalysis()))\nmodel.append(('SVC',SVC(kernel='rbf')))\nmodel.append(('DTC',DecisionTreeClassifier()))\nmodel.append(('GBC',GradientBoostingClassifier()))\nmodel.append(('RFC',RandomForestClassifier()))\nmodel.append(('Kneig',KNeighborsClassifier()))\n\n\nx=train.drop('Survived',axis=1)   \ny=train['Survived']\nxtrain,xvalid,ytrain,yvalid=train_test_split(x,y,test_size=0.3)","ef104e7d":"\nscores=[]\n\nfor name,models in model:\n    pipeline=Pipeline(steps=[('scale',MinMaxScaler()),('model',models)])\n    cv=StratifiedKFold(n_splits=10,random_state=21,shuffle=True)\n    score=cross_val_score(pipeline,x,y,cv=cv,scoring='accuracy',n_jobs=-1)\n    scores.append((name,np.mean(score)))\n   \n    \nscores","86acfe38":"from sklearn.metrics import classification_report\n\nmodel=LogisticRegression(max_iter=3000)\nmodel.fit(xtrain,ytrain)\nypred=model.predict(xvalid)\nprint(classification_report(yvalid,ypred))","b49476be":"model=RandomForestClassifier()\nmodel.fit(xtrain,ytrain)\nypred=model.predict(xvalid)\nprint(classification_report(yvalid,ypred))","b841c14e":"estimator = []\nestimator.append(('LR', GradientBoostingClassifier()))\nestimator.append(('SVC', RandomForestClassifier()))\nestimator.append(('kd',LogisticRegression(max_iter=3000)))\n\n\n\n  \n# Voting Classifier with hard voting\nvot_hard = VotingClassifier(estimators = estimator, voting ='hard')\nvot_hard.fit(xtrain, ytrain)\nypred=vot_hard.predict(xvalid)\nprint(classification_report(yvalid,ypred))","3c28ca16":"pipeline=Pipeline(steps=[('scale',RobustScaler()),('model',VotingClassifier(estimators = estimator, voting ='hard'))])\ncv=StratifiedKFold(n_splits=10,random_state=21,shuffle=True)\npipeline.fit(x,y)\nypred=pipeline.predict(xvalid)\nprint(classification_report(yvalid,ypred))","949254d7":"\n\"\"\"from sklearn.model_selection import GridSearchCV\n\nmetrics = ['euclidean','manhattan'] \nneighbors = np.arange(1, 16)\nparam_grid  = dict(metric=metrics, n_neighbors=neighbors)\nknn = KNeighborsClassifier()\n\ngrid_search = GridSearchCV(knn, param_grid, cv=10,scoring='accuracy', refit=True)\ngrid_search.fit(x, y)\nprint(grid_search.best_params_)\"\"\"","0564ba70":"\"\"\"from sklearn.model_selection import RandomizedSearchCV\nfrom scipy.stats import randint as sp_randint\n\n\nparam_dist = {\"max_depth\": [3, None],\n              \"max_features\": sp_randint(1, x.shape[1]),\n              \"min_samples_split\": sp_randint(2, 11),\n              \"bootstrap\": [True, False],\n              \"n_estimators\": sp_randint(100, 500)}\n\nrandom_search = RandomizedSearchCV(RandomForestClassifier(), param_distributions=param_dist,\n                                   n_iter=10, cv=5, iid=False, random_state=42)\nrandom_search.fit(x,y)\nprint(random_search.best_params_)\"\"\"","7baf315c":"\"\"\"gb_grid_params = {'learning_rate': [0.1, 0.05, 0.02, 0.01],\n              'max_depth': [4, 6, 8],\n              'min_samples_leaf': [20, 50,100,150],\n              'max_features': [1.0, 0.3, 0.1] \n              }\nprint(gb_grid_params)\n\ngb_gs = GradientBoostingClassifier(n_estimators = 600)\n\nclf =GridSearchCV(gb_gs,\n                               gb_grid_params,\n                               cv=2,\n                               scoring='accuracy', \n                               n_jobs=10);\nclf.fit(x,y)\nprint(clf.best_params_)\"\"\"","540f7cf0":"\ntest['Sex'].replace({'male':0,'female':1},inplace=True)\ntest['Embarked'].replace({'S':1,'C':2,'Q':3},inplace=True)\ntest['title']=0\ntest['Fare']=np.sqrt(test['Fare'])\nfor i in range(0,len(test)):\n    test.loc[i,'title']=test['Name'].iloc[i].split(',')[1].split('.')[0][1:]\ntest['title'].replace({'Mr':1,'Miss':2,'Mrs':2,'Master':3,'Dr':4,'Rev':5},inplace=True)\ntest['title'].replace(['Major','Mlle','Col','Don','the Countess','Sir','Capt','Mme','Lady','Jonkheer','Ms','Dona'],7,inplace=True)\ntest['family']=test['SibSp']+test['Parch']+1\ntest['family']=test['family'].map(family)\n\nfor i in range(len(test)):\n    if not(pd.isnull(test['Cabin'].iloc[i])):\n        test.loc[i,'Cabin']=test['Cabin'].loc[i][0]\ntest.drop(['Name','SibSp','Parch','Ticket','PassengerId','Cabin'],axis=1,inplace=True)\n\ntest=pd.get_dummies(test,columns=['Pclass','Embarked','title','family'],drop_first=True)\n\ntest=pd.DataFrame(impute.fit_transform(test),columns=test.columns)","b28636b7":"submit['Survived']=pipeline.predict(test).astype(int)\nsubmit.to_csv('ver.csv',index=False)\nsubmit","b19fa47b":"# 8. Hyperparameter tuning ","c039351a":"# 4.Model Selection","c1133650":"**3.2. Correlation**","e8f2493e":"# Please upvote if you like it~","5088423e":"**5.1. KNeighborsClassifier**","85900464":"# 3. Data Visualization","f3d5df2e":"**3.1. Histogram**","bc6a7c11":"**Dataset loading**","13ded61a":"**3.3. Embarked**","47b64be3":"\n**2.4. Extracting Cabin name**","97bc22b6":"**Different types of Classification algorithm**","0ba2272c":"**5.2. Random Forest Classifier**","e7c74881":"**Classification report**","9691e8c8":"# 2. Data Manupulation","a0a815c2":"# 1.Loading dataset and modules","72db0725":"**2.2. Deriveing title from name columns**","1bebeaf5":"**2.3. Defining a columns representing family group**","8867ffab":"# 9.Testing","d21344b9":"**2.1. Label Encoder**","d5ec1fa7":"**2.5. Dropping unnecessary columns**","dcc6891b":"# 7.Model classification using voting Classifier"}}