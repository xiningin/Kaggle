{"cell_type":{"9b27ab47":"code","b6624815":"code","bd244f77":"code","36143a5a":"code","cb52db30":"code","d48c54f7":"code","f80f303f":"code","0a2bbf48":"code","3ca69073":"code","be34c408":"code","3a36080e":"code","9f5fa764":"code","77501e3d":"code","a1a66d56":"code","67693b6e":"code","f713ec17":"code","701e527d":"code","896c9276":"code","4840975b":"code","ee7c016b":"code","8c937968":"code","3936b531":"code","f39a879f":"code","e5e6008b":"code","f66bb639":"code","104a9d96":"code","0945cc89":"code","fa892faf":"code","03b65a33":"code","544178bd":"code","bb834f7b":"code","f9c528cb":"code","5b47843b":"code","7cfb3632":"code","8285fad1":"code","9e94b7c9":"code","20b7db20":"code","bd4fada3":"code","b131d78e":"code","93217c7c":"code","e2d99576":"code","063b3797":"code","e4d70671":"code","c92c091a":"code","c6194f71":"code","75b7f161":"code","00a85340":"code","a3252088":"code","4e01b25d":"code","7c0b9968":"code","13e4201d":"code","21dd9b82":"code","79cc4eaa":"code","fa881a2a":"markdown","c37e61e8":"markdown","5c8b8fc8":"markdown","7ecfd5db":"markdown","a9934328":"markdown","21cd2363":"markdown","28288667":"markdown","da1c6a15":"markdown","63fcedd9":"markdown","6bfb161b":"markdown","e63f43c8":"markdown","f59f1d16":"markdown","2eb020ae":"markdown","b334e4cf":"markdown","b35d2f33":"markdown","43c43e03":"markdown","216e5776":"markdown","d2cbd5fc":"markdown","1d5f260c":"markdown","a9f09d7e":"markdown","566df002":"markdown"},"source":{"9b27ab47":"COMPUTE_CV = False\nBS_CLEANING = False","b6624815":"eng_stws = []\nwith open('..\/input\/engst-06061111\/englishST.txt','r') as fff:\n    for line in fff:\n        line = line.strip().lower()\n        if line != '':\n            eng_stws.append(line)\neng_stws = list(set(eng_stws))","bd244f77":"!pip install datasets --no-index --find-links=file:\/\/\/kaggle\/input\/coleridge-packages\/packages\/datasets\n!pip install ..\/input\/coleridge-packages\/seqeval-1.2.2-py3-none-any.whl\n!pip install ..\/input\/coleridge-packages\/tokenizers-0.10.1-cp37-cp37m-manylinux1_x86_64.whl\n!pip install ..\/input\/coleridge-packages\/transformers-4.5.0.dev0-py3-none-any.whl\n\n\nfrom IPython.display import clear_output\nclear_output()","36143a5a":"!pip install ..\/input\/abb-whl\/abbreviations-0.2.5-py3-none-any.whl\n","cb52db30":"# !pip install ..\/input\/simple-transformers-package\/simpletransformers\/ipykernel-5.5.5-py3-none-any.whl\n# !pip install ..\/input\/simple-transformers-package\/simpletransformers\/pydeck-0.6.2-py2.py3-none-any.whl\n# !pip install ..\/input\/simple-transformers-package\/simpletransformers\/streamlit-0.82.0-py2.py3-none-any.whl\n# !pip install ..\/input\/simple-transformers-package\/simpletransformers\/simpletransformers-0.61.5-py3-none-any.whl","d48c54f7":"!pip install --no-dependencies ..\/input\/simple-transformers-package\/simpletransformers\/simpletransformers-0.61.5-py3-none-any.whl","f80f303f":"import json\nwith open('..\/input\/ner-dataset-for-show-us-the-data\/ner_distant.json', 'r') as f:\n    js = json.load(f)\nprint(js.keys())","0a2bbf48":"js['sentences_not_including_dataset_all'][:10]","3ca69073":"js['sentences_including_annotations_all'][:10]","be34c408":"from simpletransformers.ner import NERModel\nimport pandas as pd\nimport logging\nimport torch\n\n# \u30ed\u30b0\u306e\u8a2d\u5b9a\nlogging.basicConfig(level=logging.INFO)\ntransformers_logger = logging.getLogger(\"transformers\")\ntransformers_logger.setLevel(logging.WARNING)","3a36080e":"ner_model = NERModel(\n    \"roberta\", \"..\/input\/ner-0605-st\/content\/outputs\/best_model\", use_cuda=True if torch.cuda.is_available() else False\n)","9f5fa764":"predictions, raw_outputs = ner_model.predict([\"Hermione was the best dataset study national in her class\",\n                                         \"Final dataset study and its survey apperaed\", \"slosh model is great\"])","77501e3d":"predictions","a1a66d56":"# train_params = {\n#     'max_seq_length': 512,\n#     'max_length': 128,\n#     'train_batch_size': 32,\n#     'eval_batch_size': 32,\n#     'num_train_epochs': 2,\n#     'evaluate_during_training': True,\n#     'evaluate_during_training_steps': 10000,\n#     'use_multiprocessing': False,\n#     'fp16': False,\n#     'save_steps': -1,\n#     'save_eval_checkpoints': False,\n#     'save_model_every_epoch': False,\n#     'no_cache': True,\n#     'reprocess_input_data': True,\n#     'overwrite_output_dir': True,\n#     'preprocess_inputs': False,\n#     'num_return_sequences': 1 ,\n#     \"output_dir\": \".\/ner_outputs\/\",\n#     \"cache_dir\": \".\/ner_cache_dir\/\",\n# }\n\n# model = NERModel(\n#     \"roberta\", \"roberta-base\", use_cuda=True if torch.cuda.is_available() else False,\n#     labels=['O','B-DATASET','I-DATASET'], args=train_params\n# )","67693b6e":"# import math, copy\n# train_ner_num = math.floor(len(train_data) * 0.9)\n# train_data_for_ner, test_data_for_ner = train_data[:train_ner_num], train_data[train_ner_num:]\n\n# train_for_smpformer, test_data_for_smpformer = [], []\n\n# for d in train_data_for_ner:\n#     train_for_smpformer += d\n\n# test_data_first_idx = copy.copy(test_data_for_ner[0][0][0])\n\n# for d in test_data_for_ner:\n#     for one_anno in d:\n#         test_data_for_smpformer.append([one_anno[0] - test_data_first_idx,\n#                                        one_anno[1], one_anno[2]])","f713ec17":"# import math\n# train_for_smpformer = pd.DataFrame(train_for_smpformer, columns=[\"sentence_id\", \"words\", \"labels\"])\n# test_data_for_smpformer = pd.DataFrame(test_data_for_smpformer, columns=[\"sentence_id\", \"words\", \"labels\"])\n\n# model.train_model(train_for_smpformer, eval_data=test_data_for_smpformer)\n\n# # Evaluate the model\n# print('eval ner')\n# result, model_outputs, preds_list = model.eval_model(test_data_for_smpformer)","701e527d":"# predictions, raw_outputs = model.predict([\"Hermione was the best dataset study national in her class\",\n#                                          \"study\"])\n","896c9276":"# import os\n# from zipfile import ZipFile\n\n# dirName = \".\/\"\n# zipName = \"packages.zip\"\n\n# # Create a ZipFile Object\n# with ZipFile(zipName, 'w') as zipObj:\n#     # Iterate over all the files in directory\n#     for folderName, subfolders, filenames in os.walk(dirName):\n#         for filename in filenames:\n#             if (filename != zipName):\n#                 # create complete filepath of file in directory\n#                 filePath = os.path.join(folderName, filename)\n#                 # Add file to zip\n#                 zipObj.write(filePath)","4840975b":"import os\nimport re\nimport json\nimport time\nimport datetime\nimport random\nimport glob\nimport importlib\n\nimport numpy as np\nimport pandas as pd\n\nfrom tqdm.autonotebook import tqdm\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nimport torch\nfrom datasets import load_dataset\nfrom transformers import AutoTokenizer, DataCollatorForLanguageModeling, \\\nAutoModelForMaskedLM, Trainer, TrainingArguments, pipeline\n\nfrom typing import List\nimport string\nfrom functools import partial\nSEED=43\nsns.set()\nrandom.seed(SEED)\nnp.random.seed(SEED)\ntorch.manual_seed(SEED)\ntorch.backends.cudnn.deterministic = True\ntorch.backends.cudnn.benchmark = False","ee7c016b":"all_labels = set()","8c937968":"common_data_words = ['study', 'adni', 'disease', 'longitudinal', 'neuroimaging', 'initiative', \n                     'alzheimer', 'international', 'science', 'mathematics', 'trends', 'aging', \n                     'baltimore', 'early', 'childhood', 'survey', 'national', 'continuum', \n                     'codes', 'rural', 'urban', ' adni ', 'data', 'common', 'model', 'trends',\n                     'assesments','assesment','datasets','trend', 'research', 'code',\n                     'databases',\n                     'ibtracs', 'noaa', 'postsecondary', 'beginning', 'management', 'agricultural', \n                     'assessment', 'resource', 'students', 'optimum', 'interpolation', 'surface',\n                     'temperature', 'database', 'program', 'census', 'enumeration', 'poll', 'polls', 'demographics',\n                     'doctorates', 'progress', 'school', 'tide', 'coronavirus', 'analysis', 'dataset', 'analyze',\n                     'cohort', 'cohorts']\ncritical_words = ['survey', 'study', 'data', 'database', 'dataset', 'databases', 'datasets',\n                 'cohort', 'cohorts', 'program', 'programs', 'trand', 'trands', 'assessment', 'assessments',\n                 'analysis', 'analytics', 'census', 'research', 'census', 'model']","3936b531":"with open('..\/input\/filtered-public-data-csv\/filered.csv', 'r') as fpd:\n    for line in fpd:\n        line = line.strip()\n        if line != '' and len(line.split(' ')) > 2:\n            split_w = line.split(' ')\n            if split_w[0] in eng_stws:\n                continue\n            if split_w[-1] in eng_stws:\n                continue\n            if len(set(split_w) & set(common_data_words)) > 0: \n                all_labels.add(str(line).lower())","f39a879f":"adnl_govt_labels = pd.read_csv('..\/input\/bigger-govt-dataset-list\/data_set_800.csv')\n\nfor line in adnl_govt_labels.title:\n    line = line.strip().lower()\n    if line != '' and len(line.split(' ')) > 0:\n        split_w = line.split(' ')\n        if split_w[0] in eng_stws:\n            continue\n        if split_w[-1] in eng_stws:\n            continue\n        all_labels.add(str(line).lower())    \n    \nall_labels = set(all_labels)\nprint(f'No. different labels: {len(all_labels)}')","e5e6008b":"def clean_text(txt):\n    return re.sub('[^A-Za-z0-9]+', ' ', str(txt).lower()).strip()\n\n\ndef totally_clean_text(txt):\n    txt = clean_text(txt)\n    txt = re.sub(' +', ' ', txt)\n    return txt\n\nif not BS_CLEANING:\n    def text_cleaning(text):\n        '''\n        Converts all text to lower case, Removes special charecters, emojis and multiple spaces\n        text - Sentence that needs to be cleaned\n        '''\n        text = re.sub('[^A-Za-z0-9]+', ' ', str(text).lower()).strip()\n        text = re.sub(' +', ' ', text)\n        emoji_pattern = re.compile(\"[\"\n                                   u\"\\U0001F600-\\U0001F64F\"  # emoticons\n                                   u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n                                   u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n                                   u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n                                   \"]+\", flags=re.UNICODE)\n        text = emoji_pattern.sub(r'', text)\n        return text\nelse:\n    def text_cleaning(text):\n        '''\n        Converts all text to lower case, Removes special charecters, emojis and multiple spaces\n        text - Sentence that needs to be cleaned\n        '''\n        text = ''.join([k for k in text if k not in string.punctuation])\n        text = re.sub('[^A-Za-z0-9]+', ' ', str(text).lower()).strip()\n        # text = re.sub(\"\/'+\/g\", ' ', text)\n        return text\n\n\ndef read_json_pub(filename, train_data_path, output='text'):\n    json_path = os.path.join(train_data_path, (filename+'.json'))\n    headings = []\n    contents = []\n    combined = []\n    with open(json_path, 'r') as f:\n        json_decode = json.load(f)\n        for data in json_decode:\n            headings.append(data.get('section_title'))\n            contents.append(data.get('text'))\n            combined.append(data.get('section_title'))\n            combined.append(data.get('text'))\n    \n    all_headings = ' '.join(headings)\n    all_contents = ' '.join(contents)\n    all_data = '. '.join(combined)\n    \n    if output == 'text':\n        return all_contents\n    elif output == 'head':\n        return all_headings\n    else:\n        return all_data","f66bb639":"import pdb\ndef jaccard_similarity(s1, s2):\n    l1 = s1.split(\" \")\n    l2 = s2.split(\" \")    \n    intersection = len(list(set(l1).intersection(l2)))\n    union = (len(l1) + len(l2)) - intersection\n    if union == 0:\n        return 0\n    return float(intersection) \/ union\n\ndef clean_paper_sentence(s):\n    \"\"\"\n    This function is essentially clean_text without lowercasing.\n    \"\"\"\n    s = re.sub('[^A-Za-z0-9]+', ' ', str(s)).strip()\n    s = re.sub(' +', ' ', s)\n    return s\n\ndef shorten_sentences(sentences):\n    \"\"\"\n    Sentences that have more than MAX_LENGTH words will be split\n    into multiple sentences with overlappings.\n    \"\"\"\n    short_sentences = []\n    for sentence in sentences:\n        words = sentence.split()\n        if len(words) > MAX_LENGTH:\n            for p in range(0, len(words), MAX_LENGTH - OVERLAP):\n                short_sentences.append(' '.join(words[p:p+MAX_LENGTH]))\n        else:\n            short_sentences.append(sentence)\n    return short_sentences\n\nconnection_tokens = {'s', 'of', 'and', 'in', 'on', 'for', 'data', 'dataset', 'survey', 'statistics'}\ndef find_mask_candidates(sentence):\n    \"\"\"\n    Extract masking candidates for Masked Dataset Modeling from a given $sentence.\n    A candidate should be a continuous sequence of at least 2 words, \n    each of these words either has the first letter in uppercase or is one of\n    the connection words ($connection_tokens). Furthermore, the connection \n    tokens are not allowed to appear at the beginning and the end of the\n    sequence.\n    \"\"\"\n    def candidate_qualified(words):\n        while len(words) and words[0].lower() in connection_tokens:\n            words = words[1:]\n        while len(words) and words[-1].lower() in connection_tokens:\n            words = words[:-1]\n        \n        return len(words) >= 2\n    \n    candidates = []\n    \n    phrase_start, phrase_end = -1, -1\n    for id in range(1, len(sentence)):\n        word = sentence[id]\n        if word[0].isupper() or word in connection_tokens:\n            if phrase_start == -1:\n                phrase_start = phrase_end = id\n            else:\n                phrase_end = id\n        else:\n            if phrase_start != -1:\n                if candidate_qualified(sentence[phrase_start:phrase_end+1]):\n                    candidates.append((phrase_start, phrase_end))\n                phrase_start = phrase_end = -1\n    \n    if phrase_start != -1:\n        if candidate_qualified(sentence[phrase_start:phrase_end+1]):\n            candidates.append((phrase_start, phrase_end))\n    return candidates","104a9d96":"len(all_labels)","0945cc89":"print([l for l in all_labels if len(l.strip()) < 6])","fa892faf":"def read_append_return(filename, train_files_path, output='text'):\n    \"\"\"\n    Function to read json file and then return the text data from them and append to the dataframe\n    \"\"\"\n    json_path = os.path.join(train_files_path, (filename+'.json'))\n    headings = []\n    contents = []\n    combined = []\n    with open(json_path, 'r') as f:\n        json_decode = json.load(f)\n        for data in json_decode:\n            headings.append(data.get('section_title'))\n            contents.append(data.get('text'))\n            combined.append(data.get('section_title'))\n            combined.append(data.get('text'))\n    \n    all_headings = ' '.join(headings)\n    all_contents = ' '.join(contents)\n    all_data = '. '.join(combined)\n    \n    if output == 'text':\n        return all_contents\n    elif output == 'head':\n        return all_headings\n    else:\n        return all_data\n    \n    \ndef text_cleaning(text):\n    '''\n    Converts all text to lower case, Removes special charecters, emojis and multiple spaces\n    text - Sentence that needs to be cleaned\n    '''\n    text = ''.join([k for k in text if k not in string.punctuation])\n    text = re.sub('[^A-Za-z0-9]+', ' ', str(text).lower()).strip()\n    # text = re.sub(\"\/'+\/g\", ' ', text)\n    return text","03b65a33":"import tempfile\nfrom typing import Dict, Iterable, List, Tuple\n\nimport torch\n\nfrom allennlp.common.util import JsonDict\nfrom allennlp.data import (\n    DataLoader,\n    DatasetReader,\n    Instance,\n    Vocabulary,\n    TextFieldTensors,\n)\nfrom allennlp.data.data_loaders import SimpleDataLoader, MultiProcessDataLoader\nfrom allennlp.data.fields import LabelField, TextField\nfrom allennlp.data.token_indexers import TokenIndexer, SingleIdTokenIndexer\nfrom allennlp.data.tokenizers import Token, Tokenizer, WhitespaceTokenizer\nfrom allennlp.models import Model\nfrom allennlp.modules import TextFieldEmbedder, Seq2VecEncoder\nfrom allennlp.modules.text_field_embedders import BasicTextFieldEmbedder\nfrom allennlp.modules.token_embedders import Embedding\nfrom allennlp.modules.seq2vec_encoders import BagOfEmbeddingsEncoder, LstmSeq2VecEncoder\nfrom allennlp.nn import util\nfrom allennlp.predictors import Predictor\nfrom allennlp.training.metrics import CategoricalAccuracy\nfrom allennlp.training.optimizers import AdamOptimizer\nfrom allennlp.training.trainer import Trainer, GradientDescentTrainer\nfrom allennlp.training.util import evaluate\nimport math\nimport torch\nfrom allennlp.modules.token_embedders.embedding import _read_embeddings_from_text_file\n\n\nclass ClassificationTsvReader(DatasetReader):\n    def __init__(\n        self,\n        sentence_including_annotations: List[str],\n        sentence_not_including_dataset: List[str],\n        tokenizer: Tokenizer = None,\n        token_indexers: Dict[str, TokenIndexer] = None,\n        max_tokens: int = None,\n        **kwargs\n    ):\n        super().__init__(**kwargs)\n        self.tokenizer = tokenizer or WhitespaceTokenizer()\n        self.token_indexers = token_indexers or {\"tokens\": SingleIdTokenIndexer()}\n        self.max_tokens = max_tokens\n        \n        self.train_sent_including_dataset, self.dev_sent_including_dataset, self.train_sent_not_including_dataset, self.dev_sent_not_including_dataset = \\\n         self._data_splitter_with_train_and_dev(sentence_including_annotations, sentence_not_including_dataset)\n        \n        print('length_of_data')\n        print('train including dataset sents', len(self.train_sent_including_dataset))\n        print('dev including dataset sents', len(self.dev_sent_including_dataset))\n        print('train not including dataset sents', len(self.train_sent_not_including_dataset))\n        print('dev not including dataset sents', len(self.dev_sent_not_including_dataset))\n        \n    def _data_splitter_with_train_and_dev(self, sentence_including_annotations, sentence_not_including_dataset):\n        train_frac, dev_frac = 0.85, 0.15\n        sentences_including_annotations_num = len(sentence_including_annotations)\n        train_sentences_including_annotations = sentence_including_annotations[:math.floor(sentences_including_annotations_num * train_frac)]\n        dev_sentences_including_annotations = sentence_including_annotations[math.floor(sentences_including_annotations_num * train_frac):]\n    \n        sentences_not_including_annotations_num = len(sentence_not_including_dataset)\n        train_sentences_not_including_annotations = sentence_not_including_dataset[:math.floor(sentences_not_including_annotations_num * train_frac)]\n        dev_sentences_not_including_annotations = sentence_not_including_dataset[math.floor(sentences_not_including_annotations_num * train_frac):]\n        \n        return train_sentences_including_annotations, \\\n               dev_sentences_including_annotations, \\\n               train_sentences_not_including_annotations, \\\n               dev_sentences_not_including_annotations\n    \n    def text_to_instance(self, text: str, label: str = None) -> Instance:\n        tokens = self.tokenizer.tokenize(text)\n        if self.max_tokens:\n            tokens = tokens[: self.max_tokens]\n        text_field = TextField(tokens, self.token_indexers)\n        fields = {'text': text_field}\n        if label:\n            fields['label'] = LabelField(label)\n        return Instance(fields)\n               \n        \n    def _read(self, train_or_dev: str) -> Iterable[Instance]:\n        if train_or_dev == 'train':\n            train_data = list()\n            \n            for sent in self.train_sent_including_dataset:\n                dataset_including_flag = '1'\n                yield self.text_to_instance(sent, dataset_including_flag)\n            for sent in self.train_sent_not_including_dataset:\n                dataset_including_flag = '0'\n                yield self.text_to_instance(sent, dataset_including_flag)\n            \n            return train_data\n            \n        elif train_or_dev == 'dev':\n            dev_data = list()\n            for sent in self.dev_sent_including_dataset:\n                dataset_including_flag = '1' \n                yield self.text_to_instance(sent, dataset_including_flag)\n            for sent in self.dev_sent_not_including_dataset:\n                dataset_including_flag = '0'\n                yield self.text_to_instance(sent, dataset_including_flag)\n            \n            return dev_data\n        else:\n            raise NotImplementedError\n\nclass SimpleClassifier(Model):\n    def __init__(\n        self, vocab: Vocabulary, embedder: TextFieldEmbedder, encoder: Seq2VecEncoder\n    ):\n        super().__init__(vocab)\n        self.embedder = embedder\n        self.encoder = encoder\n        num_labels = vocab.get_vocab_size(\"labels\")\n        self.classifier = torch.nn.Linear(encoder.get_output_dim(), num_labels)\n        self.accuracy = CategoricalAccuracy()\n\n    def forward(\n        self, text: TextFieldTensors, label: torch.Tensor = None) -> Dict[str, torch.Tensor]:\n\n        # Shape: (batch_size, num_tokens, embedding_dim)\n        embedded_text = self.embedder(text)\n        # Shape: (batch_size, num_tokens)\n        mask = util.get_text_field_mask(text)\n        # Shape: (batch_size, encoding_dim)\n        encoded_text = self.encoder(embedded_text, mask)\n        # Shape: (batch_size, num_labels)\n        logits = self.classifier(encoded_text)\n        # Shape: (batch_size, num_labels)\n        probs = torch.nn.functional.softmax(logits)\n        # Shape: (1,)\n        output = {\"probs\": probs}\n        if label is not None:\n            self.accuracy(logits, label)\n            output['loss'] = torch.nn.functional.cross_entropy(logits, label)\n        return output\n\n    def get_metrics(self, reset: bool = False) -> Dict[str, float]:\n        return {\"accuracy\": self.accuracy.get_metric(reset)}\n\n\ndef build_dataset_reader(sentence_including_annotations: List,\n        sentence_not_including_dataset: List,\n                        max_tokens: int) -> DatasetReader:\n    return ClassificationTsvReader(sentence_including_annotations, sentence_not_including_dataset, \n                                   max_tokens=max_tokens)\n\n\ndef read_data(reader: DatasetReader) -> Tuple[List[Instance], List[Instance]]:\n    print(\"Reading data\")\n    training_data = list(reader.read(\"train\"))\n    validation_data = list(reader.read(\"dev\"))\n    return training_data, validation_data\n\n\ndef build_vocab(instances: Iterable[Instance]) -> Vocabulary:\n    print(\"Building the vocabulary\")\n    return Vocabulary.from_instances(instances)\n\n\ndef build_model(vocab: Vocabulary) -> Model:\n    print(\"Building the model\")\n    vocab_size = vocab.get_vocab_size(\"tokens\")\n    # '..\/input\/glove6b\/glove.6B.300d.txt'\n    \n    embed_matrix = _read_embeddings_from_text_file(\n        file_uri='..\/input\/glove6b\/glove.6B.300d.txt',\n        embedding_dim=300,\n        vocab=vocab\n    )\n    token_embedding = Embedding(num_embeddings=vocab.get_vocab_size('tokens'),\n                                embedding_dim=300,\n                                weight=embed_matrix)\n    \n    embedder = BasicTextFieldEmbedder(\n        {\"tokens\": token_embedding}\n    )\n    encoder = LstmSeq2VecEncoder(input_size=300, hidden_size=300, num_layers=1, bidirectional=True)\n    # BagOfEmbeddingsEncoder(embedding_dim=300)\n    return SimpleClassifier(vocab, embedder, encoder)\n\n\ndef build_data_loaders(dataset_reader) -> Tuple[DataLoader, DataLoader]:\n    if torch.cuda.is_available():\n        train_loader =  MultiProcessDataLoader(dataset_reader, data_path='train', batch_size=64, shuffle=True,\n                                              cuda_device=0)\n        dev_loader =  MultiProcessDataLoader(dataset_reader, data_path='dev', batch_size=64, shuffle=False,\n                                            cuda_device=0)\n    else:\n        train_loader =  MultiProcessDataLoader(dataset_reader, data_path='train', batch_size=64, shuffle=True)\n        dev_loader =  MultiProcessDataLoader(dataset_reader, data_path='dev', batch_size=64, shuffle=False)\n    return train_loader, dev_loader\n\n\ndef build_trainer(\n    model: Model,\n    serialization_dir: str,\n    train_loader: DataLoader,\n    dev_loader: DataLoader,\n) -> Trainer:\n    if torch.cuda.is_available():\n        model.cuda()\n    \n    parameters = [(n, p) for n, p in model.named_parameters() if p.requires_grad]\n    optimizer = AdamOptimizer(parameters, lr=0.01)  # type: ignore\n    trainer = GradientDescentTrainer(\n        model=model,\n        serialization_dir=serialization_dir,\n        data_loader=train_loader,\n        validation_data_loader=dev_loader,\n        num_epochs=5,\n        optimizer=optimizer,\n        cuda_device=0 if torch.cuda.is_available() else -1\n    )\n    return trainer\n\n\ndef run_training_loop(sentence_including_annotations: List[str], \n                      sentence_not_including_dataset: List[str],\n                     max_tokens: int):\n    dataset_reader = build_dataset_reader(sentence_including_annotations, sentence_not_including_dataset,\n                                         max_tokens)\n    train, dev = dataset_reader.read('train'), dataset_reader.read('dev')\n    vocab = build_vocab(train)\n    vocab.extend_from_instances(dev)\n    model = build_model(vocab)\n\n    train_loader, dev_loader = build_data_loaders(dataset_reader)\n    train_loader.index_with(vocab)\n    dev_loader.index_with(vocab)\n\n    # You obviously won't want to create a temporary file for your training\n    # results, but for execution in binder for this guide, we need to do this.\n    with tempfile.TemporaryDirectory() as serialization_dir:\n        trainer = build_trainer(model, serialization_dir, train_loader, dev_loader)\n        trainer.train()\n\n    return model, dataset_reader","544178bd":"sentences_including_annotations_all = js['sentences_including_annotations_all']\nsentences_not_including_dataset_all = js['sentences_not_including_dataset_all']","bb834f7b":"print(len(sentences_including_annotations_all))\nprint(len(sentences_not_including_dataset_all))\nmodel, dataset_reader = run_training_loop(sentences_including_annotations_all, sentences_not_including_dataset_all,\n                                         max_tokens=250)\nvocab = model.vocab","f9c528cb":"from allennlp.predictors import Predictor\nfrom allennlp.training.util import evaluate\n\n\nclass SentenceClassifierPredictor(Predictor):\n    def predict(self, sentence: str) -> JsonDict:\n        return self.predict_json({\"sentence\": sentence})\n    \n    def predict_batch(self, sentences: List[str]):\n        return self.predict_batch_json([{\"sentence\": sentence} for sentence in sentences])\n\n    def _json_to_instance(self, json_dict: JsonDict) -> Instance:\n        sentence = json_dict[\"sentence\"]\n        return self._dataset_reader.text_to_instance(sentence)\n\npredictor = SentenceClassifierPredictor(model, dataset_reader)\n\noutput = predictor.predict_batch([\"A good movie!\", \"Slosh model is needed\"])\nprint(output)","5b47843b":"from nltk import sent_tokenize","7cfb3632":"# Asthetics\nimport warnings\nwarnings.filterwarnings('ignore', category=DeprecationWarning)\nwarnings.filterwarnings('ignore', category=FutureWarning)\n\n# Basic\nimport pandas as pd\npd.set_option('display.max_columns', None)\nimport numpy as np\nimport json\nimport os\nimport random\nfrom tqdm.autonotebook import tqdm\nimport string\nimport re\nfrom functools import partial\n\n# Visualizations\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\nsns.set(style=\"whitegrid\")\nfrom wordcloud import WordCloud, STOPWORDS\n\n","8285fad1":"RANDOM_SEED = 30\ndef seed_everything(seed=RANDOM_SEED):\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    random.seed(seed)","9e94b7c9":"seed_everything()\ntrain_df = pd.read_csv('..\/input\/coleridgeinitiative-show-us-the-data\/train.csv')\nsample_sub = pd.read_csv('..\/input\/coleridgeinitiative-show-us-the-data\/sample_submission.csv')\ntrain_files_path = '..\/input\/coleridgeinitiative-show-us-the-data\/train'\ntest_files_path = '..\/input\/coleridgeinitiative-show-us-the-data\/test'","20b7db20":"def read_append_return(filename, train_files_path=train_files_path, output='text'):\n    json_path = os.path.join(train_files_path, (filename+'.json'))\n    headings = []\n    contents = []\n    combined = []\n    with open(json_path, 'r') as f:\n        json_decode = json.load(f)\n        for data in json_decode:\n            headings.append(data.get('section_title'))\n            contents.append(data.get('text'))\n            combined.append(data.get('section_title'))\n            combined.append(data.get('text'))\n    \n    all_headings = ' '.join(headings)\n    all_contents = ' '.join(contents)\n    all_data = '. '.join(combined)\n    \n    if output == 'text':\n        return all_contents\n    elif output == 'head':\n        return all_headings\n    else:\n        return all_data\ntqdm.pandas()\ntrain_df['text'] = train_df['Id'].progress_apply(read_append_return)","bd4fada3":"tqdm.pandas()\nsample_sub['text'] = sample_sub['Id'].progress_apply(partial(read_append_return, train_files_path=test_files_path))","b131d78e":"sample_sub.head()","93217c7c":"def text_cleaning(text):\n    '''\n    Converts all text to lower case, Removes special charecters, emojis and multiple spaces\n    text - Sentence that needs to be cleaned\n    '''\n    text = ''.join([k for k in text if k not in string.punctuation])\n    text = re.sub('[^A-Za-z0-9]+', ' ', str(text).lower()).strip()\n    text = re.sub(' +', ' ', text)\n    emoji_pattern = re.compile(\"[\"\n                               u\"\\U0001F600-\\U0001F64F\"  # emoticons\n                               u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n                               u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n                               u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n                               \"]+\", flags=re.UNICODE)\n    text = emoji_pattern.sub(r'', text)\n    return text","e2d99576":"tqdm.pandas()\ntrain_df['text'] = train_df['text'].progress_apply(text_cleaning)","063b3797":"def clean_text(txt):\n    return re.sub('[^A-Za-z0-9]+', ' ', str(txt).lower()).strip()","e4d70671":"temp_1 = [x.lower() for x in train_df['dataset_label'].unique()]\n'''\nIdea below of also using the 'dataset_title' is burrowed from\nhttps:\/\/www.kaggle.com\/josephassaker\/coleridge-initiative-eda-na-ve-submission\n'''\ntemp_2 = [x.lower() for x in train_df['dataset_title'].unique()]\ntemp_3 = [x.lower() for x in train_df['cleaned_label'].unique()]\n\nexisting_labels = set(temp_1 + temp_2 + temp_3 ) | all_labels","c92c091a":"len(existing_labels)","c6194f71":"one_word_label = []\nmany_word_label = []\n\nfor extist_label in existing_labels:\n    sp_ext = extist_label.strip().split(' ')\n    if len(sp_ext) == 1:\n        one_word_label.append(' '.join(sp_ext))\n    else:\n        many_word_label.append(' '.join(sp_ext))","75b7f161":"len(one_word_label)","00a85340":"many_word_label[:10]","a3252088":"id_list = []\nlables_list = []\nfrom abbreviations import schwartz_hearst\n\ndef batcher(iterable, n=1):\n    l = len(iterable)\n    for ndx in range(0, l, n):\n        yield iterable[ndx:min(ndx + n, l)]\n\nfor index, row in tqdm(sample_sub.iterrows()):\n    sample_text = row['text']\n\n    abb_pairs = {}\n    '''\n    {'QC': 'quality control', 'MDS': 'multidimensional scaling', 'MLMA': 'mixed-linear-model association',\n    'ii': 'included in COGENT; and', 'CHARGE': 'Cohorts for Heart and Aging Research in Genomic Epidemiology', \n    'FHS': 'Framingham Heart Study, Second Generation Cohort', 'CHS': 'Cardiovascular Health Study',\n    'NCNG': 'Norwegian Cognitive NeuroGenetics', 'lincRNA': 'long intergenic non-coding RNA', \n    'PING': 'Pediatric Imaging, Neurocognition, and Genetics', 'ADNI': \"Alzheimer's Disease Neuroimaging Initiative\", \n    'KDP': 'Kernel density plot'}\n    '''\n    row_id = row['Id']\n    temp_df = train_df[train_df['text'] == text_cleaning(sample_text)]\n    cleaned_labels = temp_df['cleaned_label'].to_list()\n    for known_label in existing_labels:\n        if known_label in sample_text.lower():\n            cleaned_labels.append(clean_text(known_label))\n    cleaned_labels = [clean_text(x).replace(',', ' ') for x in cleaned_labels]\n    cleaned_labels = set(cleaned_labels)\n\n    try:\n        labels_from_ner = list()\n        sents = sent_tokenize(sample_text)\n        sents_num = len(sents)\n        sents = sents[: sents_num \/\/ 2]\n        for batched_sents in batcher(sents, 64):\n            whether_sent_includes_dataset = predictor.predict_batch(batched_sents)\n\n            filtered_texts = list()\n\n            for sent, if_dataset_include_result in zip(batched_sents, whether_sent_includes_dataset):\n                result =     [\n                (vocab.get_token_from_index(label_id, \"labels\"), prob)\n                for label_id, prob in enumerate(if_dataset_include_result[\"probs\"])]\n\n                if result[1][1] > 0.98:\n                    filtered_texts.append(sent)\n                \n                if 'et al' in sent:\n                    filtered_texts.append(sent)\n                \n                try:\n                    if len(set(sent.lower().split(' ')) & set(common_data_words)) > 0 or '(' in sent:\n                        pairs = schwartz_hearst.extract_abbreviation_definition_pairs(doc_text=sent) \n                        if len(pairs) > 0:\n                            filtered_texts.append(sent)\n                            for k, v in pairs.items():\n                                abb_pairs.update({k.lower(): v.lower()})\n                except:\n                    pass\n            \n            filtered_texts = list(set(filtered_texts))\n            if len(filtered_texts) > 0:\n                res = ner_model.predict(filtered_texts)\n                res = res[0]\n\n                tmp_ds_label = []\n                for sent_result in res:\n                    for one_word_pred in sent_result:\n                        for word, pred_label in one_word_pred.items():\n                            if pred_label in ['B-DATASET', 'I-DATASET']:\n                                tmp_ds_label.append(word)\n                            if pred_label == 'O':\n                                if len(tmp_ds_label) > 1: # critical params\n                                    if tmp_ds_label[0] not in eng_stws and tmp_ds_label[-1] not in eng_stws:\n                                        labels_from_ner.append(' '.join(tmp_ds_label))\n                                tmp_ds_label.clear()\n\n        labels_from_ner = [clean_text(label) for label in labels_from_ner if len(label.split(' ')) > 2]\n        filtered_labels = []\n\n        for label in sorted(labels_from_ner, key=len, reverse=True):\n            if len(filtered_labels) == 0 or all(jaccard_similarity(label, got_label) < 0.75 for got_label in filtered_labels):\n                filtered_labels.append(label)\n\n        filtered_labels = [label.replace(',', ' ') for label in filtered_labels if label.split(' ')[0] not in eng_stws\n                          and label.split(' ')[-1] not in eng_stws]\n        \n\n        '''\n        ['floodplain management and hazard mitigation', 'north carolina division of transportation', \n        'nc sea level rise risk management study', 'erosion storm surge and sea level rise',\n        'usgs digital shoreline analysis system', 'area of change from shoreline movement', \n        'periodic reporting of sea level trends', 'estuarine shoreline change and erosion', \n        'storm surge vulnerability assessment', 'albemarle pamlico estuarine system', \n        'oceanic atmospheric administration', 'sea level affecting marshes model',\n        'digital flood insurance rate maps', 'national flood insurance program',\n        'cape hatteras national seashore', 'coastal elevation data quality',\n        'sea level rise inundation maps', 'storm surge vulnerability maps', \n        'tidal or wind tide inundation', 'nc floodplain mapping program', 'coastal vulnerability index', \n        'coastal observation station', 'surge simulation modeling', 'area of shoreline change', \n        'linear regression rate', 'sea level rise mapping', \n        'coastal erosion study', 'envelope of water', 'noaa ocm slr']\n        '''\n    except:\n        filtered_labels = list()\n    \n    set_list = set(cleaned_labels)\n    \n    \n    # critical_words\n    final_filtered_from_ner = [d for d in filtered_labels if d.lower().split(' ')[-1] in critical_words]\n    print('** filtered ner prediction **', final_filtered_from_ner)\n    print('** cleaned str match **',cleaned_labels)\n    \n    filtered_abb = [v.lower() for v in abb_pairs.values()]\n    filtered_abb = [v for v in filtered_abb if v.split(' ')[-1] not in eng_stws and v.split(' ')[0] not in eng_stws]\n    filtered_abb = [clean_text(v) for v in filtered_abb if len(set(v.split(' ')) & set(critical_words)) > 0]\n    print('** filtered_abb **', filtered_abb)\n    \n    entire_labels = []\n    entire_labels += [clean_text(label) for label in list(set_list)]\n    \n    if len(entire_labels) == 0 :\n        entire_labels += [clean_text(label) for label in list(set(filtered_abb))]\n        entire_labels = list(set(entire_labels))\n        \n        if len(entire_labels) == 0:\n\n            for label in sorted(final_filtered_from_ner, key=len, reverse=True):\n                if len(entire_labels) == 0 or all(jaccard_similarity(label, got_label) < 0.50 for got_label in entire_labels):\n                    entire_labels.append(label)\n        entire_labels = entire_labels[:5]\n    \n    entire_labels.sort()\n    print('**final_labels aggregated**', entire_labels)\n    lables_list.append('|'.join(entire_labels))\n    id_list.append(row_id)","4e01b25d":"sample_sub['PredictionString'] = lables_list\nsample_sub[['Id', 'PredictionString']].to_csv('submission.csv', index=False)","7c0b9968":"sample_sub.head()","13e4201d":"assert len(id_list) == len(lables_list)\nassert len(sample_sub) == len(id_list)","21dd9b82":"# submission = pd.DataFrame()\nassert len(sample_sub['Id']) == len(id_list)\nsample_sub[['Id', 'PredictionString']].head()","79cc4eaa":"# submission.to_csv('submission.csv', index=False)","fa881a2a":"#### Additional Govt Datasets","c37e61e8":"### Create a Knowledge Bank","5c8b8fc8":"# About\n\nThis notebook added an splendid `External Dataset` shared by [@mlconsult](https:\/\/www.kaggle.com\/mlconsult) to `MLM` shared by [@tungmphung](https:\/\/www.kaggle.com\/tungmphung). Thanks to everyone willing to share their knowledge, let's make the DS community better ;)\n\n**Please check the original notebooks and discussion here:**\n- [Matching + Baseline + MLM w\/ Evaluation Metric](https:\/\/www.kaggle.com\/chienhsianghung\/matching-baseline-mlm-w-evaluation-metric)\n- [Coleridge Initiative| Naive CV (Local Validation)](https:\/\/www.kaggle.com\/chienhsianghung\/coleridge-initiative-naive-cv-local-validation)\n- [Evaluation metric implementation](https:\/\/www.kaggle.com\/c\/coleridgeinitiative-show-us-the-data\/discussion\/230091)\n- [[Coleridge] Predict with Masked Dataset Modeling](https:\/\/www.kaggle.com\/tungmphung\/coleridge-predict-with-masked-dataset-modeling)\n- [score 57ish with additional govt datasets](https:\/\/www.kaggle.com\/mlconsult\/score-57ish-with-additional-govt-datasets)\n\n**Previous results from:**\n[Matching + Baseline + MLM w\/ Evaluation Metric](https:\/\/www.kaggle.com\/chienhsianghung\/matching-baseline-mlm-w-evaluation-metric)\n\n|   | CV | LB |\n| --- | --- | --- |\n| og | 0.604 | 0.536 |\n| og s1.5 |   | 0.535 |\n| og s2.5 |   | 0.535 |\n| pred_mlm_labels | 0.108 |   |\n| lables_list | 0.548 |   |\n| all | 0.261 |   |\n| og2all | 0.454 |   |\n| 150 400 |   | 0.536 |\n| 200 300 |   | 0.536 |\n| 2021 |   | 0.536 |\n| 42 |   | 0.536 |\n| 42 s1.5 |   | 0.535 |\n| 42 .70 |   | 0.536 |\n| 42 .80 |   | 0.536 |\n| MATCH_ONLY |   | 0.533 |\n| BASELINE_HELPING |   | 0.536 |\n| 1 |   | 0.536 |","7ecfd5db":"# Evaluation Metric","a9934328":"### Transform","21cd2363":"### Predict","28288667":"# Transform data to MLM format","da1c6a15":"# Install packages","63fcedd9":"# Setting","6bfb161b":"# Masked Dataset Modeling","e63f43c8":"### Load model and tokenizer","f59f1d16":"### Paths and Hyperparameters","2eb020ae":"### Matching on test data","b334e4cf":"# Import","b35d2f33":"### Auxiliary functions","43c43e03":"|   | CV | LB |\n| --- | --- | --- |\n| s57 adnl_govt_labels |   | 0.573 |\n| 42 w\/ adnl_govt_labels |   | 0.557 |\n| adnl_govt_labels_26897 | 0.136 |   |\n| adnl_govt_labels |   | 0.556 |\n| adnl_govt_labels KEN |   |   |\n| 42 w\/ adnl_govt_labels KEN |   | 0.557 |\n| **RKM adnl_govt_labels w\/ 42** | **0.514** | **0.574** |\n| RKM adnl_govt_labels BS_CLEANING w\/ 42 |   | 0.568 |\n| RKM adnl_govt_labels_26897 |   | 0.244 |\n| RKM adnl_govt_labels w\/ 42_48 |   | 0.574 |\n| **RKM adnl_govt_labels w\/ 42_36** |   | **0.574** |\n| RKM adnl_govt_labels w\/ 42_60 |   | 0.574 |\n| RKM adnl_govt_labels w\/ 42_24 |   | 0.573 |\n| RKM adnl_govt_labels w\/ 42_60 |   |   |","216e5776":"# Literal Matching","d2cbd5fc":"#### Ken Matching","1d5f260c":"## Aggregate final predictions and write submission file","a9f09d7e":"# Baseline Model","566df002":"# Load data"}}