{"cell_type":{"009996ed":"code","6e1d5636":"code","11959040":"code","572bceef":"code","785cd4c5":"code","1ea99903":"code","8bf48afb":"code","36a7d367":"code","f037307c":"code","bad5e232":"code","aa64a581":"code","392e01d6":"code","2916c9a5":"code","c1414904":"code","30157f37":"markdown"},"source":{"009996ed":"import cv2\nimport numpy as np\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\n## this notebook aims at detecting objects with YOLOv3\n## the whole code was done using cv2 and other libraries. Deep learning parts were handled using cv2","6e1d5636":"#Load Yolo\nnet = cv2.dnn.readNet(\"..\/input\/object-detection-with-yolov3\/weights\/yolov3.weights\", \n                      \"..\/input\/object-detection-with-yolov3\/yolov3.cfg\")","11959040":"classes = []\nwith open(\"..\/input\/object-detection-with-yolov3\/coco.names\",\"r\") as file:\n    lines = file.readlines()  \n# line is now a list with all the individual lines of the file. Each line contain a classname\n\nfor line in lines:\n    classes.append(line.strip())\n\nprint(type(classes), len(classes))\nprint(classes)\n## now the classes list contains the name of all of the classes(80).","572bceef":"## this function shows two images side-by-side\ndef plot_two_images(img1, img2, title1=\"\", title2=\"\"):\n    fig = plt.figure(figsize=[15,15])\n    ax1= fig.add_subplot(121)\n    ax1.imshow(img1, cmap=\"gray\")\n    ax1.set(xticks=[], yticks=[], title=title1)\n    \n    ax2= fig.add_subplot(122)\n    ax2.imshow(img2, cmap=\"gray\")\n    ax2.set(xticks=[], yticks=[], title=title2)","785cd4c5":"#loading image\nimg = cv2.imread(\"..\/input\/object-detection-with-yolov3\/images\/person.jpg\") ## BGR format\nrgb_img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\nplot_two_images(img,rgb_img, 'BGR image', 'RGB_image')\nheight, width, channels = img.shape\nprint(type(img),height, width, channels)\ntemp_image2 = rgb_img.copy() #for later usage","1ea99903":"#Detecting objects\n## the main image needs to be converted into BLOB image for input of YOLO model\nblob = cv2.dnn.blobFromImage(rgb_img, 0.00392, (416, 416), (0,0,0), True, crop = False)\nprint(blob.shape, type(blob)) # (1,3,416,416)\n\nfig, axes = plt.subplots(ncols=3, figsize = (11,11))\nfor b in blob:\n    for n, img_blob in enumerate(b):\n        axes[n].imshow(img_blob)\nplt.show()","8bf48afb":"net.setInput(blob) # blob is passed into the network\nlayer_names = net.getLayerNames()\noutput_layers = [layer_names[i[0]  -1] for i in net.getUnconnectedOutLayers()]\nouts = net.forward(output_layers)\n## now 'outs' is the list containing information about all the detected objects\n\nprint(len(outs), type(outs))\nprint(outs[0].size)","36a7d367":"# following lists will contain the classID, confidences and box-coordinates of detected classes.\nconfidences = []; class_ids = []; boxes = []\n\nheight, width, channels = img.shape\nfor out in outs:\n    for detection in out:\n        scores = detection[5:] # scores contain the confidence for each class on the detected object\n        class_id=np.argmax(scores) # the maximum confidence is the most-probable class\n        confidence = scores[class_id] # taking the ID of the most likely class\n        if confidence >0.5:\n            #object is detected\n            ## finding the co-ordinates to bound-box\n            center_x= int(detection[0]*width)\n            center_y= int(detection[1]*height)\n            w= int(detection[2]*width)\n            h= int(detection[3]*height)\n            ## rectangle coordinates\n            x= int(center_x - w\/2)\n            y= int(center_y - h\/2)\n            \n            cv2.rectangle(rgb_img, (x,y), (x+w, y+h), (0,255,0),2)\n            cv2.circle(rgb_img, (center_x, center_y), 10, (255, 0, 0),5)\n            boxes.append([x,y,w,h])\n            confidences.append(float(confidence))\n            class_ids.append(class_id)\n            \nmyPlt = plt.imshow(rgb_img)\n\nfig = myPlt.get_figure()\nfig.savefig(\"output_detection.png\")\n\nprint('number of boxes ', len(boxes))\nprint(temp_image2.shape)\n\nprint(class_ids)\nfor id in class_ids:\n    print(classes[id]) ## these classes have been detected\n    \n## we can see that some classes have been detected twice. That's why we can see more bounding boxes. \n# Lets remove redundant boxes.","f037307c":"plt.imshow(temp_image2) # we'll make out further analysis on the image that we copied before","bad5e232":"## this block of codes could be used if we wanted a random color-box for each classes.\n#colors = np.random.uniform(0, 255, size = (len(classes), 3))\n#print(colors)","aa64a581":"#help(cv2.dnn.NMSBoxes)","392e01d6":"## removing redundant boxes\n\nindexes = cv2.dnn.NMSBoxes(boxes, confidences, 0.5, 0.4)\n\nprint(indexes) # these indexes are the unique ones\n\nfont = cv2.FONT_HERSHEY_PLAIN\n\nfor i in range (len(boxes)):\n    if i in indexes:\n        x, y, w, h = boxes[i]\n        label = str(classes[class_ids[i]])\n        #color= colors[i]\n        cv2.rectangle(temp_image2, (x, y), (x + w, y + h), (0,255,0), 2)\n        cv2.putText(temp_image2, label, (x, y + 30), font, 3, (255,0,0), 3)\n        print(label) # just for analysis purpose\n    \nplot_two_images(rgb_img, temp_image2, 'Initial detection', 'After removing redundant boxes')","2916c9a5":"cv2.imwrite('output_detection.png', rgb_img)\ncv2.imwrite('result.png', temp_image2)","c1414904":"result_image = cv2.imread('.\/result.png')\nplt.imshow(result_image)","30157f37":"* reference: https:\/\/www.youtube.com\/watch?v=h56M5iUVgGs"}}