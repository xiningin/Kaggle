{"cell_type":{"359d7741":"code","0b7c4fac":"code","506cb42f":"code","0552b64f":"code","54f536fe":"code","cb956d70":"code","15f63a8e":"code","12177f27":"code","21ea56a0":"code","c3439f71":"code","30347230":"code","e6191cfe":"code","6d13afed":"code","230f1642":"code","9ce1230a":"code","5265cf69":"code","8d0cc8b4":"code","88425c40":"code","6d931bfa":"code","b8132086":"code","c1383a32":"code","7df8459a":"code","51c36a01":"code","366840b4":"code","d8765b50":"code","d6f9c1ed":"code","edccc20f":"code","4631a77f":"code","f06aa98b":"code","49a1d5a7":"code","6767c7f6":"code","8c2157fc":"markdown","de3c2880":"markdown","6dc8d988":"markdown","6b714be8":"markdown","dd6cba82":"markdown","2d1beec6":"markdown","5d05444a":"markdown","b5166b50":"markdown","1c7d9749":"markdown","4c9365c3":"markdown","16259ab4":"markdown","f47c4fdd":"markdown"},"source":{"359d7741":"import os\nimport sys\nimport pandas as pd\nimport numpy as np\nfrom tqdm import tqdm\nimport pathlib\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport matplotlib.image as mpimg\n%matplotlib inline\nimport cv2\n\nfrom keras.applications.vgg16 import VGG16\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Dropout, Flatten, BatchNormalization\nfrom keras.layers import Conv2D, MaxPooling2D\nfrom keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\nfrom keras.optimizers import SGD\n\nfrom sklearn.utils import shuffle\nfrom sklearn.metrics import fbeta_score","0b7c4fac":"# set path\npath = '..\/input\/planets-dataset\/'\nos.chdir(path)   \n\nfor dirname, _, filenames in os.walk('planet'):\n    for filename in filenames:\n        os.path.join(dirname, filename)","506cb42f":"!ls","0552b64f":"# load train and test datasets\ntrain_data = pd.read_csv(\"planet\/planet\/train_classes.csv\")\ntest_data = pd.read_csv(\"planet\/planet\/sample_submission.csv\")\n\n# preview train dataset\ntrain_data.head()","54f536fe":"# Build list with unique labels\nlabel_list = []\nfor tag_str in train_data.tags.values:\n    labels = tag_str.split(' ')\n    for label in labels:\n        if label not in label_list:\n            label_list.append(label)\n\n            \n# Display label list and length \nprint(f'There are {len(train_data)} data samples, with {len(label_list)} possible classes.', '\\n' \n      f'The Label list includes {label_list}')","cb956d70":"# Split the tags column to get the unique labels\nflatten = lambda l: [item for sublist in l for item in sublist]\nlabels = list(set(flatten([l.split(' ') for l in train_data['tags'].values])))\n\n\n# Create label map\nlabel_map = {l: i for i, l in enumerate(labels)}\n\nprint(f'label_map = {label_map},\\n length = {len(label_map)}')","15f63a8e":"# Add onehot features for every label\ntrain_tag_data = train_data.copy()\nfor label in label_list:\n    train_tag_data[label] = train_tag_data['tags'].apply(lambda x: 1 if label in x.split(' ') else 0)\n\n# Display head\ntrain_tag_data.head()","12177f27":"# Print all unique tags\nfrom itertools import chain\nlabels_list = list(chain.from_iterable([tags.split(\" \") for tags in train_tag_data['tags'].values]))\nlabels_set = set(labels_list)\nprint(\"There is {} unique labels including {}\".format(len(labels_set), labels_set))\n\n\n# Histogram of label instances\ntag_labels = pd.Series(labels_list).value_counts() # To sort them by count\nfig, ax = plt.subplots(figsize=(16, 8))\nsns.barplot(x=tag_labels, y=tag_labels.index, orient='h')","21ea56a0":"# function for cooocurence matrix plotting\ndef make_cooccurence_matrix(labels):\n    numeric_data = train_tag_data[labels]; \n    c_matrix = numeric_data.T.dot(numeric_data)\n    sns.heatmap(c_matrix)\n    return c_matrix\n    \n# Compute the co-ocurrence matrix\nmake_cooccurence_matrix(label_list)","c3439f71":"# plot weather element cooccurence matrix\nweather_labels = ['clear', 'partly_cloudy', 'haze', 'cloudy']\nmake_cooccurence_matrix(weather_labels)","30347230":"# plot land-use element classes cooccurence matrix\nland_labels = ['primary', 'agriculture', 'water', 'cultivation', 'habitation']\nmake_cooccurence_matrix(land_labels)","e6191cfe":"images = [train_data[train_data['tags'].str.contains(label)].iloc[i]['image_name'] + '.jpg' \n                for i, label in enumerate(labels_set)]\n\nplt.rc('axes', grid=False)\n_, axs = plt.subplots(5, 4, sharex='col', sharey='row', figsize=(15, 20))\naxs = axs.ravel()\n\nfor i, (image_name, label) in enumerate(zip(images, labels_set)):\n    img = mpimg.imread('planet\/planet\/train-jpg' + '\/' + image_name)\n    axs[i].imshow(img)\n    axs[i].set_title('{} - {}'.format(image_name, label))","6d13afed":"# Determining if the length of the train and test dataset csv file equals the actual number of images in the folder\n\n# Assign train and the two test dataset paths\n# train path\ntrain_img_dir = pathlib.Path('planet\/planet\/train-jpg')\ntrain_img_path = sorted(list(train_img_dir.glob('*.jpg')))\n\n# test path\ntest_img_dir = pathlib.Path('planet\/planet\/test-jpg')\ntest_img_path = sorted(list(test_img_dir.glob('*.jpg')))\n\n# additional test path\ntest_add_img_dir = pathlib.Path('test-jpg-additional')\ntest_add_img_path = sorted(list(test_add_img_dir.glob('*\/*.jpg')))\n\n# Length Confirmation\nassert len(train_img_path) == len(train_data)\nprint(len(test_img_path)+len(test_add_img_path))","230f1642":"# define input size\ninput_size = 64","9ce1230a":"# creating x_train and y_train\nx_train = []\ny_train = []\n\nfor f, tags in tqdm(train_data.values, miniters=1000):\n    img = cv2.imread('planet\/planet\/train-jpg\/{}.jpg'.format(f))\n    img = cv2.resize(img, (input_size, input_size))\n    targets = np.zeros(17)\n    for t in tags.split(' '):\n        targets[label_map[t]] = 1\n    x_train.append(img)\n    y_train.append(targets)\n        \nx_train = np.array(x_train, np.float32)\ny_train = np.array(y_train, np.uint8)\n\nprint(x_train.shape)\nprint(y_train.shape)","5265cf69":"# creating x_test\nx_test = []\n\ntest_jpg_dir = 'planet\/planet\/test-jpg'\ntest_image_names = os.listdir(test_jpg_dir)\n\nn_test = len(test_image_names)\ntest_classes = test_data.iloc[:n_test, :]\nadd_classes = test_data.iloc[n_test:, :]\n\ntest_jpg_add_dir = 'test-jpg-additional\/test-jpg-additional'\ntest_add_image_names = os.listdir(test_jpg_add_dir)\n\nfor img_name, _ in tqdm(test_classes.values, miniters=1000):\n    img = cv2.imread(test_jpg_dir + '\/{}.jpg'.format(img_name))\n    x_test.append(cv2.resize(img, (64, 64)))\n    \nfor img_name, _ in tqdm(add_classes.values, miniters=1000):\n    img = cv2.imread(test_jpg_add_dir + '\/{}.jpg'.format(img_name))\n    x_test.append(cv2.resize(img, (64, 64)))\n\nx_test = np.array(x_test, np.float32)\nprint(x_test.shape)","8d0cc8b4":"# split the train data into train and validation data sets\nX_train = x_train[ :33000]\nY_train = y_train[ :33000]\n\nX_valid = x_train[33000: ]\nY_valid = y_train[33000: ]","88425c40":"# specify sizes (batch and model input) and number of input channels\ninput_size = 64\ninput_channels = 3\nbatch_size = 64","6d931bfa":"# Add model parameters including dropout, layers and activation function\nbase_model = VGG16(include_top=False,\n                   weights='imagenet',\n                   input_shape=(input_size, input_size, input_channels))\n\nmodel = Sequential()\nmodel.add(BatchNormalization(input_shape=(input_size, input_size, input_channels)))\n\nmodel.add(base_model)\nmodel.add(Flatten())\nmodel.add(Dropout(0.5))\n\nmodel.add(Dense(17, activation='sigmoid'))","b8132086":"\npath = '\/kaggle\/working'\nos.chdir(path)   ","c1383a32":"\n# define model training optimizer parameters\noptimizer  = SGD(lr=0.01)\nmodel.compile(loss='binary_crossentropy',optimizer=optimizer, metrics=['accuracy'])\ncallbacks = [EarlyStopping(monitor='val_loss', patience=2, verbose=0),\n                ModelCheckpoint(filepath='weights\/best_weights',\n                                 save_best_only=True,\n                                 save_weights_only=True)]\nmodel.summary()","7df8459a":"# train model\nhistory = model.fit(x=X_train, y=Y_train, validation_data=(X_valid, Y_valid),\n                  batch_size=batch_size,verbose=2, epochs=15,callbacks=callbacks,shuffle=True)","51c36a01":"# summarize history for loss\nplt.plot(history.history['loss'])\nplt.plot(history.history['val_loss'])\nplt.title('model loss')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.legend(['train', 'validation'], loc='upper left')\nplt.show()","366840b4":"p_valid = model.predict(X_valid, batch_size = batch_size, verbose=1)\n\nprint(fbeta_score(Y_valid, np.array(p_valid) > 0.18, beta=2, average='samples'))","d8765b50":"y_pred = []\np_test = model.predict(x_test, batch_size=batch_size, verbose=2)\ny_pred.append(p_test)","d6f9c1ed":"labels1 = ['haze', 'primary', 'agriculture', 'clear', 'water', 'habitation', 'road', 'cultivation', 'slash_burn', 'cloudy', 'partly_cloudy', 'conventional_mine', 'bare_ground', 'artisinal_mine', 'blooming', 'selective_logging', 'blow_down']","edccc20f":"result = np.array(y_pred[0])\nfor i in range(1, len(y_pred)):\n    result += np.array(y_pred[i])\nresult = pd.DataFrame(result, columns=labels)","4631a77f":"# Translating the probability predictions to the unique labels\npreds = []\nfor i in tqdm(range(result.shape[0]), miniters=1000):\n    a = result.loc[[i]]\n    a = a.apply(lambda x: x>0.2, axis=1)\n    a = a.transpose()\n    a = a.loc[a[i] == True]\n    ' '.join(list(a.index))\n    preds.append(' '.join(list(a.index)))","f06aa98b":"ls","49a1d5a7":"# Replacing the tags columns with the predicted labels\ntest_data['tags'] = preds\ntest_data.head()","6767c7f6":"# Converting the dataframe to a csv file for submission\ntest_data.to_csv('amazon_sample_submission10.csv', index=False)","8c2157fc":"Image Preprocessing","de3c2880":"**INSPECT COOCCURENCE MATRICES**","6dc8d988":"**MODEL TRAINING**","6b714be8":"**DATA PRE-PROCESSING**","dd6cba82":"**INSPECT IMAGE LABELS**","2d1beec6":"**IMPORT LIBRARIES**","5d05444a":"# PLANET: Understanding the Amazon from Space","b5166b50":"#\u00a0converting\u00a0output\u00a0to\u00a0label\u00a0tags\u00a0for\u00a0submission.csv\u00a0file\n\nmySubmission = sample_sub.copy()\nsorted_labels = list(label_map1.keys())\nfor i in tqdm(range(result.shape[0])):\n    tag = \"\"\n    x = result[i]\n    for lbl in sorted_labels:\n        if x[sorted_labels.index(lbl)] == 1:\n            tag += \" \" + lbl\n            mySubmission[\"tags\"][i] = tag[1:]\n\nmySubmission.to_ccsv(\"submission.csv\", index = False)","1c7d9749":"Data Length Check","4c9365c3":"**PREVIEW IMAGES IN EACH CATEGORY**","16259ab4":"Define Model Architecture","f47c4fdd":"**DOWNLOAD DATASETS**"}}