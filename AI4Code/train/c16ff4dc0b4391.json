{"cell_type":{"c0b320c3":"code","16d1e965":"code","178ad4d3":"code","77bcdc91":"code","45b3ef23":"code","4289db89":"code","bacbcbfd":"code","15365197":"code","0ff9c5e6":"code","664d897f":"code","92ae5a69":"code","ccef5306":"code","d7a68dd3":"code","4f20e3c4":"code","edd56c0b":"code","57bab591":"code","3a6e458c":"code","a04fe4dd":"code","eb116c89":"code","7db0edb6":"code","042f8867":"code","ab2b2a08":"code","0831a59b":"code","49bac9b7":"code","ea0fef19":"code","1c45f20d":"code","7094dc58":"code","fe15cd20":"code","1035d58f":"code","0533ac2f":"code","50791995":"code","f4f9f613":"code","f21a6a68":"markdown","153ccafe":"markdown","6d13abbd":"markdown","b0e62f14":"markdown","a007dbff":"markdown","c0538d32":"markdown","cd8e0d8f":"markdown","d2225c52":"markdown"},"source":{"c0b320c3":"import pandas as pd \nimport numpy as np \nimport scipy as sp \nimport matplotlib.pyplot as plt \nimport seaborn as sns \nimport os, time, json, pickle \nfrom tqdm import tqdm \nfrom janome.tokenizer import Tokenizer\nfrom wordcloud import WordCloud\n\nfrom sklearn.decomposition import PCA \nfrom sklearn.cluster import KMeans \nfrom sklearn.preprocessing import MinMaxScaler, RobustScaler \nfrom sklearn.metrics.pairwise import cosine_similarity\nfrom sklearn.metrics import mean_squared_error\n\nimport tensorflow as tf \nfrom tensorflow.keras.models import Sequential \nfrom tensorflow.keras.layers import Input, Flatten, LSTM, Dense, Bidirectional","16d1e965":"train = pd.read_csv(\"..\/input\/youtube-trending-video-dataset\/JP_youtube_trending_data.csv\")\n\nwith open(\"..\/input\/youtube-trending-video-dataset\/JP_category_id.json\", \"r\") as f:\n    cates = json.load(f)\nf.close()\n    \nid2cate = {}\nfor cate in cates[\"items\"]:\n    ids = int(cate[\"id\"])\n    snippet = cate[\"snippet\"]\n    if ids not in id2cate:\n        id2cate[ids] = snippet[\"title\"]\nid2cate","178ad4d3":"use_col = [\"publishedAt\", \"title\", \"channelTitle\", \"categoryId\", \"tags\", \"view_count\"]\n\ntrain = train[use_col]\ntrain[\"category\"] = train.categoryId.apply(lambda x: id2cate[x] if x in id2cate else \"Unknown\")\ntrain.drop(\"categoryId\", axis=1, inplace=True)\ntrain.head()","77bcdc91":"\n\"\"\"\ndatetime \n\"\"\"\n\ntrain[\"publishedAt\"] = pd.to_datetime(train.publishedAt)\ntrain[\"year\"] = train.publishedAt.dt.year \ntrain[\"month\"] = train.publishedAt.dt.month \ntrain[\"day\"] = train.publishedAt.dt.day \ntrain[\"datetime\"] = train.year.astype(str) + \"-\" + train.month.astype(str) + \"-\" + train.day.astype(str)\ntrain.drop([\"publishedAt\", \"year\", \"month\", \"day\"], axis=1, inplace=True)\ntrain[\"datetime\"] = pd.to_datetime(train.datetime)\n\ntrain[[\"datetime\"]].dtypes","45b3ef23":"\n'''\ntags clensing \n'''\n\n\ntrain[\"split\"] = train.tags.apply(lambda x: x.split(\"|\"))\n\ndef split_tags(x):\n    txt_list = []\n    if len(x) > 1 :\n        for txt in x:\n            txt_list.append(txt)\n        return \" \".join(txt_list)\n    else:\n        return \"\"\n\ntrain[\"tags\"] = train.split.apply(split_tags)\ntrain.drop(\"split\", axis=1, inplace=True)\ntrain.tags","4289db89":"def viz_transition(df):\n    x = df.groupby(\"datetime\").mean().loc[:, [\"view_count\"]]\n    x.plot()\n    \n    \ndef viz_categorical_transition(df):\n    category = df.category.unique()\n    \n    fig, axes = plt.subplots(4, 4, figsize=(12, 12))\n    ax = axes.ravel()\n    \n    for i, cate in enumerate(category):\n        x = df[df.category == cate].groupby(\"datetime\").mean().loc[:, [\"view_count\"]]\n        x.plot(ax=ax[i])\n        ax[i].set_title(cate)\n        ax[i].set_xlabel(\"\")\n        \n    plt.tight_layout()","bacbcbfd":"viz_transition(train)","15365197":"viz_categorical_transition(train)","0ff9c5e6":"\n'''\nAggregate the most played data by date.\n'''\n\nimport warnings \nwarnings.simplefilter(\"ignore\")\n\n\ndef find_everytime(df):\n    grp = df.groupby(\"datetime\")\n    grp = df.loc[grp[\"view_count\"].idxmax(), :]\n    \n    plt.figure(figsize=(20, 8))\n    \n    plt.subplot(121)\n    category = grp.category.value_counts()[:5]\n    plt.title(\"category\")\n    plt.pie(x=category.values, labels=category.index, counterclock=False, startangle=90)\n    plt.legend(category.index)\n    \n    plt.subplot(122)\n    plt.title(\"channel title\")\n    channel = grp.channelTitle.value_counts()[:5]\n    sns.barplot(x=channel.index, y=channel.values)\n    plt.xticks(rotation=90)\n    \n    plt.show()\n    \n    return channel.index \n    \n    \ndef find_holiday(df, is_holiday=True):\n    dfs = df.copy()\n    dfs[\"week\"] = dfs.datetime.dt.dayofweek \n    dfs[\"holiday\"] = dfs.week.apply(lambda x: 1 if x in [5, 6] else 0)\n    dfs.drop([\"week\"], axis=1, inplace=True)\n    \n    if is_holiday:\n        dfs = dfs[dfs.holiday == 1]\n    else:\n        dfs = dfs[dfs.holiday == 0]\n    popular_channel_transition(dfs)\n    \n    \ndef popular_channel_transition(df):\n    popular = find_everytime(df)\n    fig, axes = plt.subplots(3, 2, figsize=(15, 6))\n    ax = axes.ravel()\n    \n    for i, p in enumerate(popular):\n        x = df.loc[df.channelTitle == p, [\"datetime\", \"view_count\"]]\n        x.set_index(\"datetime\").plot(ax=ax[i])\n        ax[i].set_title(p)\n    plt.tight_layout()\n    ","664d897f":"\n'''\nfrom daily records \n'''\n\npopular_channel_transition(train)","92ae5a69":"\n'''\nis holiday ?\n'''\n\nfind_holiday(train)","ccef5306":"\n'''\nis not holiday ?\n'''\n\nfind_holiday(train, False)","d7a68dd3":"\n'''\nprepare tabular \n'''\n\nspec = pd.crosstab(train.channelTitle, train.category)\nspec.head()","4f20e3c4":"\ndef find_specialized(cate_name, n=10) -> pd.DataFrame:\n    x = spec[[cate_name]].sort_values(cate_name, ascending=False)[:n]\n    return x \n\n\ndef freq_word(cate_name):\n    spec_df = find_specialized(cate_name, n=10)\n    \n#     for spec in spce_df.index:\n    tag = train[train.channelTitle.isin(spec_df.index)][\"tags\"]\n    title = train[train.channelTitle.isin(spec_df.index)][\"title\"]\n    x = train[train.channelTitle.isin(spec_df.index)]\n    \n    fig, axes = plt.subplots(1, 3, figsize = (15, 6))\n    ax = axes.ravel()\n    for chann in x.channelTitle.unique():\n        xx = x.loc[x.channelTitle == chann, [\"datetime\", \"view_count\"]]\n        xx.set_index(\"datetime\").plot(ax=ax[0], c=\"g\", alpha=max(1.0\/len(x.channelTitle.unique()), 0.01)) \n        train[train.category == cate_name].groupby(\"datetime\").mean().plot(ax=ax[0], c=\"r\", alpha=0.1)\n        ax[0].legend([])\n        ax[0].set_title(f\"Transition {cate_name} category 10k\")\n        \n    j_t = Tokenizer()    \n    title_list = {} \n    for tt in title:\n        for t in j_t.tokenize(tt, wakati=True):\n            if t not in title_list:\n                title_list[t] = 1\n            else:\n                title_list[t] += 1 \n            \n    tag_list = {}\n    for tt in tag:\n        for t in tt.split():\n            if t not in tag_list:\n                tag_list[t] = 1 \n            else:\n                tag_list[t] += 1 \n                \n    title_cloud = WordCloud(width=1100, height=1500, background_color=\"white\").generate_from_frequencies(title_list)\n    ax[1].imshow(title_cloud)\n    ax[1].set_title(f\"Frquence title {cate_name}\")\n    \n    tag_cloud = WordCloud(width=1100, height=1500, background_color=\"white\").generate_from_frequencies(tag_list)\n    ax[2].imshow(tag_cloud)\n    ax[2].set_title(f\"Frequence tag {cate_name}\")\n                \n    \n        ","edd56c0b":"random_category = np.random.choice(spec.columns, 3)\nfind_specialized(random_category[0])","57bab591":"find_specialized(random_category[1])","3a6e458c":"freq_word(random_category[0])","a04fe4dd":"freq_word(random_category[1])","eb116c89":"freq_word(random_category[2])","7db0edb6":"\n'''\nprepare tabular\n\ncolumns: channelTitle \nindex: channelTitle \n\n'''\n\ns = MinMaxScaler(feature_range=(0.0, 1.0))\ns_spec = s.fit_transform(spec)\n\ndf_sparse = sp.sparse.csr_matrix(s_spec)\ndf_sparse = cosine_similarity(df_sparse)\ndf_sparse = pd.DataFrame(df_sparse, columns=spec.index, index=spec.index)\ndf_sparse.head()\n","042f8867":"\n'''\nPrepare a function to search from the table above\n'''\n\ndef find_similar_channel(channel, n=10):\n    x = df_sparse[[channel]].sort_values(channel, ascending=False)[1:n+1]\n    return x \n\ndef similar_channel_transition(channel):\n    sim_chan = find_similar_channel(channel).index\n    fig, axes = plt.subplots(1, 2, figsize=(15, 6))\n    ax = axes.ravel()\n    for chan in sim_chan:\n        x = train.loc[train.channelTitle == chan, [\"datetime\", \"view_count\"]]\n        x.set_index(\"datetime\").plot(ax=ax[0], alpha=0.2, c=\"g\")\n    train.loc[train.channelTitle == channel, [\"datetime\", \"view_count\"]]\n    train.set_index(\"datetime\").plot(ax=ax[1], alpha=0.2, c=\"r\")\n        \n    ax[0].set_title(f\"{channel} similar for transition 10k\")\n    ax[0].legend([])\n    ax[1].set_title(\"current channel transition\")\n    ax[1].legend([])","ab2b2a08":"random_channel = np.random.choice(train.channelTitle, 3)\nfind_similar_channel(random_channel[0])","0831a59b":"find_similar_channel(random_channel[1])","49bac9b7":"similar_channel_transition(random_channel[0])","ea0fef19":"similar_channel_transition(random_channel[1])","1c45f20d":"\ndf_train = train.pivot_table(index=\"category\", \n                             columns=\"datetime\",\n                             values=\"view_count\", \n                             aggfunc=\"sum\",\n                             fill_value=0)\ndf_train.head()","7094dc58":"print(f\"span datetime {len(df_train.columns)} days\")\nprint(f\"unique categorical length {len(df_train.index)}\")","fe15cd20":"\n'''\nModel structure \n\nmetrics functions \n\n'''\n\n\ndef build_model(input_shape=(425, 1)):\n    model = Sequential()\n    model.add(Input(shape=input_shape))\n    model.add(Bidirectional(LSTM(256, return_sequences=True)))\n    model.add(Bidirectional(LSTM(128, return_sequences=True)))\n    model.add(Bidirectional(LSTM(64, return_sequences=True)))    \n    model.add(Flatten())\n    model.add(Dense(64, activation=\"selu\"))\n    model.add(Dense(1))\n    model.compile(optimizer=\"adam\", loss=\"mae\")\n    return model \n\ndef scaler(tr, va, te):\n    s = RobustScaler()\n    return s.fit_transform(tr), s.transform(va), s.transform(te)\n\ndef mae(pred, corr):\n    return mean_squared_error(pred, corr, squared=False)\n\nmodel = build_model()\nmodel.summary()","1035d58f":"\n'''\nshape: [batch_size, seq_len, hidden]\n\nbatch_size == category \nseq_len == time_step \nhidden == feature_len \n'''\n\nn_span = 427\nx_train, x_val, x_test = df_train.iloc[:, :n_span-2].values, df_train.iloc[:, 1:n_span-1].values, df_train.iloc[:, 2:].values\ny_train, y_val = df_train.iloc[:, -2].values, df_train.iloc[:, -1].values\n\nx_train, x_val, x_test = scaler(x_train, x_val, x_test)\n\nx_train = np.expand_dims(x_train, -1)\nx_val = np.expand_dims(x_val, -1)\nx_test = np.expand_dims(x_test, -1)\n\ny_train = np.expand_dims(y_train, -1)\ny_val = np.expand_dims(y_val, -1)\n\nprint(x_train.shape, x_val.shape, x_test.shape)\nprint(y_train.shape, y_val.shape)\n","0533ac2f":"def train_fn(debug=False):\n    model = build_model()\n    history = model.fit(x_train, \n                       y_train, \n                       validation_data=(x_val, y_val),\n                       epochs= 1 if debug else 300,\n                       batch_size=12)\n    pred_v = model.predict(x_val).flatten()\n    print(f\"CV MAE: {mae(pred_v, y_val.ravel())}\")\n    pred_t = model.predict(x_test).flatten()\n    del model \n    \n    return pred_t, history \n\npred, history = train_fn()","50791995":"category = df_train.index \nsub = pd.DataFrame({\"2021-10-05\": pred}, index=category)\nsub.to_csv(\"submission.csv\", index=False)","f4f9f613":"sub","f21a6a68":"# Visualizetation ","153ccafe":"# Search for similar channels","6d13abbd":"# Prediction categorical tomorrow","b0e62f14":"# Popular Channel","a007dbff":"As you can see, the ranking of popular categories varies greatly depending on whether it is a holiday or a weekday. In addition, Tokai OnAir is stable and high.  \nYou can also try dividing it on a monthly basis in consideration of trends.","c0538d32":"# Specialized channel","cd8e0d8f":"The similarity of the categories is high, but on the contrary, the correlation with the number of views is not so noticeable.","d2225c52":"# Clean Data"}}