{"cell_type":{"2b33289f":"code","44505bae":"code","ac066699":"code","309cde9b":"code","fb224161":"code","a10cf525":"code","070b6f35":"code","575ed006":"code","01031c10":"code","0fe2cc59":"code","3ad6e91a":"code","29d4cb83":"code","3d5be98f":"code","545cd178":"code","706face3":"markdown","e5a7a7f6":"markdown","76f4d3dc":"markdown"},"source":{"2b33289f":"!pip install -U git+https:\/\/github.com\/albu\/albumentations --no-cache-dir","44505bae":"pip install fastai --upgrade","ac066699":"!pip install pretrainedmodels\n!pip install efficientnet_pytorch\n%reload_ext autoreload\n%autoreload 2\n%matplotlib inline\nfrom torchvision.models import *\n\nfrom torch.distributions.beta import Beta\nfrom fastai.vision.all import *\nfrom fastai.callback.mixup import *\nimport pretrainedmodels\nimport seaborn as sns\nfrom fastai import *\nfrom fastai.vision import *\nfrom fastai.vision.models import *\nfrom fastai.vision.learner import model_meta\n#from fastai.vision.all import *\nimport fastai\nimport warnings\nwarnings.filterwarnings(\"ignore\", category=UserWarning, module=\"torch.nn.functional\")\nimport sys\nimport pandas as pd\nimport torch\nimport numpy as np\nfastai.__version__\nimport torchvision.models as models\nimport torch.nn as nn\n# from fastai.widgets import ClassConfusion,DatasetFormatter,ImageCleaner\n# from fastai.vision import transform\nimport os \nfrom efficientnet_pytorch import EfficientNet\nmodel = EfficientNet.from_pretrained('efficientnet-b7')","309cde9b":"from torch.nn import Linear\n\nmodel._fc = Linear(2560, 2)\nnn.init.kaiming_normal_(model._fc.weight)\nmodel = model.to(torch.device(\"cuda\"))","fb224161":"path = \"..\/input\/cat-and-dog\/\"","a10cf525":"import albumentations\nclass AlbumentationsTransform(RandTransform):\n    \"A transform handler for multiple `Albumentation` transforms\"\n    split_idx,order=None,2\n    def __init__(self, train_aug, valid_aug): store_attr()\n    \n    def before_call(self, b, split_idx):\n        self.idx = split_idx\n    \n    def encodes(self, img: PILImage):\n        if self.idx == 0:\n            aug_img = self.train_aug(image=np.array(img))['image']\n        else:\n            aug_img = self.valid_aug(image=np.array(img))['image']\n        return PILImage.create(aug_img)","070b6f35":"def get_train_aug(sz): return albumentations.Compose([\n            albumentations.RandomResizedCrop(sz,sz),\n            albumentations.Transpose(p=0.5),\n            albumentations.HorizontalFlip(p=0.5),\n            albumentations.VerticalFlip(p=0.5),\n            albumentations.ShiftScaleRotate(p=0.5),\n            albumentations.HueSaturationValue(\n                hue_shift_limit=0.2, \n                sat_shift_limit=0.2, \n                val_shift_limit=0.2, \n                p=0.5\n            ),\n            albumentations.RandomBrightnessContrast(\n                brightness_limit=(-0.1,0.1), \n                contrast_limit=(-0.1, 0.1), \n                p=0.5\n            ),\n            albumentations.CoarseDropout(p=0.5),\n            albumentations.Cutout(p=0.5)\n])\n\ndef get_valid_aug(sz): return albumentations.Compose([\n    albumentations.CenterCrop(sz,sz, p=1.),\n    albumentations.Resize(sz,sz)\n], p=1.)","575ed006":"def get_dls(sz,bs):\n    item_tfms = AlbumentationsTransform(get_train_aug(sz), get_valid_aug(sz))\n    batch_tfms = [Normalize.from_stats(*imagenet_stats)]\n    dls = ImageDataLoaders.from_folder(path, #pass in train DataFrame\n                                   valid_pct=0.2, #80-20 train-validation random split\n                                   seed=999, #seed\n                                   #label_col=0, #label is in the first column of the DataFrame\n                                   #fn_col=1, #filename\/path is in the second column of the DataFrame\n                                   bs=bs, #pass in batch size\n                                   item_tfms=item_tfms, #pass in item_tfms\n                                   batch_tfms=batch_tfms) #pass in batch_tfms\n    return dls","01031c10":"dls = get_dls(255,16)","0fe2cc59":"dls.show_batch()","3ad6e91a":"!pip install timm","29d4cb83":"from fastai.vision.all import *\nfrom fastai.callback.mixup import *\nfrom torch.distributions.beta import Beta\nimport timm\nset_seed(314)","3d5be98f":"class CutMix(MixUp):\n    def __init__(self, alpha=1.): self.distrib = Beta(tensor(alpha), tensor(alpha))\n    def before_batch(self):\n        lam = self.distrib.sample().squeeze().to(self.x.device)\n        shuffle = torch.randperm(self.y.size(0)).to(self.x.device)\n        self.yb1 = tuple(L(self.yb).itemgot(shuffle))\n        nx_dims = len(self.x.size())\n        bs, c, h, w = self.x.shape\n        rx, ry = w*self.distrib.sample(), h*self.distrib.sample()\n        rw, rh = w*(1-lam).sqrt(), h*(1-lam).sqrt()\n        x1 = (rx-rw\/2).clamp(min=0).round().to(int)\n        x2 = (rx+rw\/2).clamp(max=w).round().to(int)\n        y1 = (ry-rh\/2).clamp(min=0).round().to(int)\n        y2 = (ry+rh\/2).clamp(max=h).round().to(int)\n        self.learn.xb[0][:,:,y1:y2,x1:x2] = self.learn.xb[0][shuffle,:,y1:y2,x1:x2]\n        self.lam = 1- float(x2-x1)*(y2-y1)\/(h*w)\n        \n        if not self.stack_y:\n            ny_dims = len(self.y.size())\n            self.learn.yb = tuple(L(self.yb1,self.yb).map_zip(torch.lerp,weight=unsqueeze(self.lam, n=ny_dims-1)))","545cd178":"mixup = CutMix()\nwith Learner(dls, model, loss_func=CrossEntropyLossFlat(), cbs=mixup) as learn:\n    learn.epoch,learn.training = 0,True\n    learn.dl = dls.train\n    b = dls.one_batch()\n    learn._split(b)\n    learn('before_batch')\n\n_,axs = plt.subplots(3,3, figsize=(9,9))\ndls.show_batch(b=(mixup.xb[0],mixup.y), ctxs=axs.flatten())","706face3":"## Upvote If you like it :)","e5a7a7f6":"### References: https:\/\/docs.fast.ai\/","76f4d3dc":"## Future Work:\n\n* MixUp\n* Fmix\n* Snapmix"}}