{"cell_type":{"adac31cf":"code","b8410527":"code","305fb2a3":"code","c20f8606":"code","5f178414":"code","09378bd3":"code","a3923f3f":"code","88465c11":"code","9acbd2f9":"code","c703b41a":"code","c9e6fb1a":"code","22adb5e0":"code","502c654b":"code","cab54ed8":"code","e0db12af":"code","7392174b":"code","a022f5e9":"code","dcca10fc":"code","a9923f2c":"code","441527f4":"code","0dfe3504":"code","5c832457":"code","83238cfb":"code","57ac6c13":"code","8179c72a":"code","84f8befe":"code","b2a366b9":"code","28396a88":"code","3350b379":"code","8c8a2d9c":"code","c6fe2be0":"markdown","ad6c9bf0":"markdown","489c275c":"markdown","1647a305":"markdown","4ba68b55":"markdown","66b3617a":"markdown","90c80a3b":"markdown","ba911921":"markdown","32c5a1c0":"markdown","a5d50871":"markdown","bf3d3a02":"markdown","160455f5":"markdown","7f71c893":"markdown","9d63e64a":"markdown"},"source":{"adac31cf":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","b8410527":"# Kaggle environments.\n!git clone https:\/\/github.com\/Kaggle\/kaggle-environments.git\n!cd kaggle-environments && pip install .\n\n# GFootball environment.\n!apt-get update -y\n!apt-get install -y libsdl2-gfx-dev libsdl2-ttf-dev\n\n# Make sure that the Branch in git clone and in wget call matches !!\n!git clone -b v2.5 https:\/\/github.com\/google-research\/football.git\n!mkdir -p football\/third_party\/gfootball_engine\/lib\n\n!wget https:\/\/storage.googleapis.com\/gfootball\/prebuilt_gameplayfootball_v2.5.so -O football\/third_party\/gfootball_engine\/lib\/prebuilt_gameplayfootball.so\n!cd football && GFOOTBALL_USE_PREBUILT_SO=1 pip3 install .","305fb2a3":"# We will define this magic to run and write the cell at the same time\n# This will facilitate the generation of submission file\nfrom IPython.core.magic import register_cell_magic\n@register_cell_magic\ndef write_and_run(line, cell):\n    argz = line.split()\n    file = argz[-1]\n    mode = 'w'\n    if len(argz) == 2 and argz[0] == '-a':\n        mode = 'a'\n    with open(file, mode) as f:\n        f.write(cell)\n    get_ipython().run_cell(cell)","c20f8606":"import gym\nfrom kaggle_environments import make\nfrom typing import Tuple, Any\nimport os\nimport threading\nimport uuid\nfrom queue import Queue\nimport dill\nfrom scipy.signal import lfilter\nfrom threading import Lock\nimport json\nfrom typing import List\nimport multiprocessing as mp\nimport time\nimport seaborn as sns\nfrom tensorflow.keras.layers import Dense,Input,LayerNormalization,Flatten,concatenate\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras import backend as K\nfrom tensorflow.keras.optimizers import Adam","5f178414":"! mkdir -p \/kaggle_simulations\/agent\/","09378bd3":"%%write_and_run -a \/kaggle_simulations\/agent\/main.py\nimport numpy as np\nimport tensorflow as tf","a3923f3f":"right_agent_path = '\/kaggle\/input\/gfootball-template-bot\/submission.py'","88465c11":"# FootEnv: \nclass FootEnv(gym.Env):\n    metadata = {'render.modes': ['human']}\n\n    def __init__(self, right_agent=right_agent_path, env_id=0):\n        super(FootEnv, self).__init__()\n        self.env_id = env_id\n        self.agents = [None, right_agent]# We will step on the None agent\n        self.env = make(\"football\", configuration={\"save_video\": False,\n                                                   \"scenario_name\": \"11_vs_11_kaggle\",\n                                                   \"running_in_notebook\": True})\n        self.trainer = None\n\n\n    def step(self, action):\n        obs, reward, done, info = self.trainer.step([action])\n        obs = obs['players_raw'][0]\n        state,(l_score,r_score,custom_reward) = OBSParser.parse(obs)\n        info['l_score'] = l_score\n        info['r_score'] = r_score\n        return state, custom_reward, done, info\n\n    def reset(self):\n        self.trainer = self.env.train(self.agents)\n        obs = self.trainer.reset()\n        obs = obs['players_raw'][0]\n        state,_ = OBSParser.parse(obs)\n        return state\n\n    def render(self, **kwargs):\n        return self.env.render(**kwargs)\n\n    def close(self):\n        pass","9acbd2f9":"%%write_and_run -a \/kaggle_simulations\/agent\/main.py\n# OBSParser : used to parse observation\nclass OBSParser(object):\n\n    @staticmethod\n    def parse(obs):\n        # parse left players units\n        l_units = [[x[0] for x in obs['left_team']], [x[1] for x in obs['left_team']],\n                   [x[0] for x in obs['left_team_direction']], [x[1] for x in obs['left_team_direction']],\n                   obs['left_team_tired_factor'], obs['left_team_yellow_card'],\n                   obs['left_team_active'], obs['left_team_roles']\n                  ]\n\n        l_units = np.r_[l_units].T\n\n        # parse right players units\n        r_units = [[x[0] for x in obs['right_team']], [x[1] for x in obs['right_team']],\n                   [x[0] for x in obs['right_team_direction']], [x[1] for x in obs['right_team_direction']],\n                   obs['right_team_tired_factor'],\n                   obs['right_team_yellow_card'],\n                   obs['right_team_active'], obs['right_team_roles']\n                  ]\n\n        r_units = np.r_[r_units].T\n        # combine left and right players units\n        units = np.r_[l_units, r_units].astype(np.float32)\n\n        # get other information\n        game_mode = [0 for _ in range(7)]\n        game_mode[obs['game_mode']] = 1\n        scalars = [*obs['ball'],\n                   *obs['ball_direction'],\n                   *obs['ball_rotation'],\n                   obs['ball_owned_team'],\n                   obs['ball_owned_player'],\n                   *obs['score'],\n                   obs['steps_left'],\n                   *game_mode,\n                   *obs['sticky_actions']]\n\n        scalars = np.r_[scalars].astype(np.float32)\n        # get the actual scores and compute a reward\n        l_score,r_score = obs['score']\n        reward = l_score - r_score\n        reward_info = l_score,r_score,reward\n        return (units[np.newaxis, :], scalars[np.newaxis, :]),reward_info","c703b41a":"#Just creat and return an environment. Useful when we run multiples threads to collect experiences.\ndef env_fn(env_id=1,right_agent=right_agent_path):\n    return FootEnv(env_id=env_id,right_agent=right_agent)","c9e6fb1a":"env = env_fn() \nstate = env.reset()\ndone = False\ni = 0\nwhile not done and i <5:\n    i+=1\n    state, reward, done, info = env.step(5)\n    print('reward ', reward, info)\nprint(f\"Units shape {state[0].shape}, Scalars shape {state[1].shape}\")","22adb5e0":"LOSS_CLIPPING = 2  # Only implemented clipping for the surrogate loss, paper said it was best\nENTROPY_LOSS = 5e-3\nGAMMA = 0.99\nN_ACTIONS = 19\nLR = 0.0001\nBATCH_SIZE = 1024\nEPOCHS = 10\nGAMMA = 0.99\nLAMBDA = 0.95","502c654b":"restore_path = '\/kaggle\/input\/data' # if we want to restore the previous checkpoint\ndata_path = ''\nlock = Lock()","cab54ed8":"# This code is shared by both actor and critic\ndef build_shared(units, scalars) -> Dense:\n    scalars =  LayerNormalization()(scalars)\n\n    # units_encoder\n    units_encoder =  LayerNormalization()(units)\n    units_encoder =  Flatten()(units_encoder)\n    # scalars encoder\n    scalars_encoder = LayerNormalization()(scalars)\n    \n    # combine scalars and units\n    encoder = concatenate([units_encoder, scalars_encoder], axis=-1)\n    encoder = Dense(128, activation='tanh', )(encoder)\n    encoder = Dense(128, activation='tanh', )(encoder)\n    return encoder\n\n#build actor\ndef build_actor(verbose=True, lr=1e-4,compile=True):\n    n_actions = 19\n    # create the model architecture\n\n    # inputs\n    units_input = Input(shape=(22, 8), name='units_input')\n    scalars_input = Input(shape=(31,), name='scalars_input')\n\n    # advantage and old_prediction inputs\n    advantage = Input(shape=(1,), name='advantage')\n    old_action = Input(shape=(n_actions,), name='old_action')\n    action_lbl = Input(shape=(n_actions,), name='action_lbl')\n\n    # build_shared\n    encoder = build_shared(units_input, scalars_input)\n\n    # outputs\n    action = Dense(n_actions, activation='softmax')(encoder)\n    inputs = [units_input, scalars_input, advantage, old_action, action_lbl]\n\n    model = Model(inputs, action)\n    model.add_loss(ppo_loss(action_lbl, action, advantage, old_action))\n\n    if compile: model.compile(optimizer=Adam(lr))\n    if verbose: model.summary()\n    return model\n\n#build critic\ndef build_critic(verbose=True, lr=1e-4,compile=True):\n    # inputs\n    units_input = Input(shape=(22, 8), name='units_input')\n    scalars_input = Input(shape=(31,), name='scalars_input')\n\n    # build_shared\n    encoder = build_shared(units_input, scalars_input)\n\n    # outputs\n    value_dense = Dense(1, name='value')(encoder)\n    inputs = [units_input, scalars_input]\n\n    model = Model(inputs, value_dense)\n\n    if compile: model.compile(loss='mse', optimizer=Adam(lr))\n    if verbose: model.summary()\n    return model\n\n\ndef ppo_loss(label_layer, prediction_layer, advantage, old_prediction, clip=True):\n    prob = label_layer * prediction_layer\n    old_prob = label_layer * old_prediction\n    r = prob \/ (old_prob + 1e-10)\n    clipped = r\n    if clip:\n        clipped = K.clip(r, min_value=1 - LOSS_CLIPPING, max_value=1 + LOSS_CLIPPING)\n    return -K.mean(K.minimum(r * advantage,clipped* advantage) + \n                   ENTROPY_LOSS * (prob * K.log(prob + 1e-10)))","e0db12af":"print(\"Actor:\")\nbuild_actor()\nprint(\"Critic:\")\nbuild_critic()","7392174b":"class Memory:\n    def __init__(self):\n        # inputs\n        self.units = []\n        self.scalars = []\n        # action\n        self.actions_matrix = []\n        self.actions_probs = []\n        # rewards\n        self.rewards = []\n        # dones\n        self.terminal = []\n\n    def isEmpty(self):\n        return len(self.rewards) == 0\n\n    def store(self, obs, actions, reward, done):\n        # inputs\n        units, saclars = obs\n        self.units.append(units)\n        self.scalars.append(saclars)\n\n        # actions\n        _, actions_matrix, actions_probs = actions\n        if actions_matrix is not None: self.actions_matrix.append(actions_matrix)\n        if actions_probs is not None: self.actions_probs.append(actions_probs)\n        # reward\n        self.rewards.append(reward)\n        self.terminal.append(done)\n\n    def discount(self, x, gamma=GAMMA):\n        return lfilter([1], [1, -gamma], x[::-1], axis=0)[::-1]\n\n    def discount_rewards(self, GAMMA=0.99):\n        return self.discount(self.rewards, GAMMA)\n\n    def normalize(self, x):\n        mean = np.mean(x)\n        std = np.std(x)\n        return (x - mean) \/ np.maximum(std, 1e-6)\n\n    def compute_advantages(self, pred_value, GAMMA=0.99, LAMBDA=0.95, normalize=True):\n        # Computes GAE (generalized advantage estimations (from the Schulman paper))\n        rewards = np.array(self.rewards, dtype=np.float32)\n        pred_value_t = pred_value\n        pred_value_t1 = np.concatenate([pred_value[1:], [0.]])\n        pred_value_t1[self.terminal] = 0\n        advantage = rewards + GAMMA * pred_value_t1 - pred_value_t\n        advantage = self.normalize(self.discount(advantage, GAMMA * LAMBDA))\n        return np.array(self.discount_rewards(), dtype=np.float32), \\\n               advantage.astype(np.float32)\n\n    def compute_normal_advantages(self, pred_value, GAMMA=0.99):\n        rewards = np.array(self.discount_rewards(GAMMA), dtype=np.float32)\n        advantage = rewards - pred_value\n        return rewards.astype(np.float32), advantage.astype(np.float32)\n\n    def get_all_as_tensors(self):\n        rewards = np.array(self.discount_rewards(), dtype=np.float32)\n        units = tf.concat(self.units, axis=0)\n        scalars = tf.concat(self.scalars, axis=0)\n\n        actions_matrix = tf.convert_to_tensor(self.actions_matrix, dtype=tf.float32)\n        actions_probs = tf.convert_to_tensor(self.actions_probs, dtype=tf.float32)\n        dones = np.array(self.terminal, dtype=np.float32)\n        return (units, scalars), actions_matrix, actions_probs, rewards, dones","a022f5e9":"# The policy class is straightforward, easy to understand\nclass PPOPolicy:\n    def __init__(self, val=False):\n        self.actor = build_actor(lr=LR,verbose=0)\n        self.critic = build_critic(lr=LR*10,verbose=0)\n\n        self.val = val # Validation or trainning\n\n    def get_values(self, X):\n        return self.critic.predict(X).flatten()\n\n    def get_action(self, X):\n        action_prob = self.actor.predict(X)\n        action_prob = action_prob[0]\n        # action_probs = np.nan_to_num(action_probs[0])\n        n_actions = action_prob.size\n        if self.val:\n            action = np.argmax(action_prob, axis=-1)\n        else:\n            action = np.random.choice(n_actions, p=action_prob)\n\n        # matrix\n        action_matrix = np.zeros(n_actions, np.float32)\n        action_matrix[action] = 1\n\n        return action, action_matrix, action_prob\n\n    def train(self, memories):\n        if not memories:\n            return [],[]\n        actor_ds, critic_ds = None, None\n        # prepare dataset\n        # process and combine memories in actor_ds and critic_ds(tf.data.Dataset objects) \n        for i, memory in enumerate(memories):\n            print(f\"Add Memory {i + 1}\/{len(memories)}\")\n            inputs, actions_matrix, actions_probs, rewards, dones = memory.get_all_as_tensors()\n            c_inputs = inputs\n            pred_values = self.get_values(c_inputs)\n\n            # Generalized Advantage Estimation\n            rewards, advantage = memory.compute_advantages(pred_values)\n            rewards = rewards[:, np.newaxis]\n            advantage = advantage[:, np.newaxis]\n\n            labels = actions_matrix\n            a_inputs = *inputs, advantage, actions_probs, labels\n\n            if actor_ds is None:\n                actor_ds = tf.data.Dataset.from_tensor_slices((a_inputs, labels))\n            else:\n                actor_ds = actor_ds.concatenate(tf.data.Dataset.from_tensor_slices((a_inputs, labels)))\n            if critic_ds is None:\n                critic_ds = tf.data.Dataset.from_tensor_slices((c_inputs, rewards))\n            else:\n                critic_ds = critic_ds.concatenate(tf.data.Dataset.from_tensor_slices((c_inputs, rewards)))\n\n        # train\n        print(\"Updating...\")\n        actor_ds = actor_ds.shuffle(100).batch(BATCH_SIZE).prefetch(2)\n        critic_ds = critic_ds.shuffle(100).batch(BATCH_SIZE).prefetch(2)\n\n        s = time.time()\n        a_losses = self.actor.fit(actor_ds, epochs=EPOCHS, verbose=False)\n        a_time = time.time() - s\n        print(f\">>>Actor updated: {a_time}ms\")\n        s = time.time()\n        c_losses = self.critic.fit(critic_ds, epochs=EPOCHS, verbose=False)\n        c_time = time.time() - s\n        print(f\">>>Critic updated: {c_time}ms\")\n        print(f\"Total Duration: {a_time + c_time}\")\n\n        return a_losses.history['loss'], c_losses.history['loss']\n\n    def save(self, path):\n        self.actor.save(path + '.actor.h5')\n        self.critic.save(path + '.critic.h5')\n\n    def load(self, path):\n        if os.path.exists(path + '.actor.h5') or os.path.exists(path + '.critic.h5'):\n            self.actor = tf.keras.models.load_model(path + '.actor.h5')\n            self.critic = tf.keras.models.load_model(path + '.critic.h5')\n","dcca10fc":"# EpisodeCollector\nclass EpisodeCollector(threading.Thread):\n    n_episode = 1\n    reward_sum = 0\n    max_episode = 0\n\n    def __init__(self, env: FootEnv, policy: PPOPolicy, result_queue=None, replays_dir=None):\n        super().__init__()\n        self.result_queue = result_queue\n        self.env = env\n        self.policy = policy\n        self.replays_dir = replays_dir\n        self.n_episode = -1\n\n    def clone(self):\n        obj = EpisodeCollector(self.env, self.policy)\n        obj.result_queue = self.result_queue\n        obj.replays_dir = self.replays_dir\n        obj.n_episode = self.n_episode\n        return obj\n\n    def run(self):\n        self.result_queue.put(self.collect(1))\n\n    def collect(self, n=1):\n        n = max(n, self.n_episode)\n        return [self.collect_() for _ in range(n)]\n\n    def collect_(self):\n        memory = Memory()\n        done = False\n        EpisodeCollector.n_episode += 1\n        obs = self.env.reset()\n        i = 0\n        total_reward = 0\n        while not done:\n            actions = self.policy.get_action(obs)\n            #action,action_matrix,action_prob = actions\n            new_obs, reward, done, info = self.env.step(actions[0])\n            # store data\n            memory.store(obs, actions, reward, done)\n\n            if done or i % 1000 == 0:\n                with lock:\n                    print(\n                        f\"Episode: {EpisodeCollector.n_episode}\/{EpisodeCollector.max_episode} | \"\n                        f\"Step: {i} | \"\n                        f\"Env ID: {self.env.env_id} | \"\n                        f\"Reward: {reward} | \"\n                        f\"Total Rewards: {EpisodeCollector.reward_sum} | \"\n                        f\"{info}\"\n                    )\n\n            obs = new_obs\n            i += 1\n        EpisodeCollector.reward_sum += info['l_score'] # count the total goal scored by the agent\n        if self.replays_dir:\n            with open(os.path.join(self.replays_dir, f'replay-{uuid.uuid4().hex}.dill'), 'wb') as f:\n                dill.dump(memory, f)\n        return memory\n\n# ParallelEpisodeCollector\nclass ParallelEpisodeCollector:\n\n    def __init__(self, env_fn, n_jobs, policy: PPOPolicy, replays_dir=None, ):\n        self.n_jobs = n_jobs\n        self.policy: Policy\n        self.envs = []\n        self.result_queue = Queue()\n        self.replays_dir = replays_dir\n        for i in range(n_jobs):\n            self.envs.append(env_fn(env_id=i))\n        self.collectors = [EpisodeCollector(env,\n                                            policy=policy,\n                                            result_queue=self.result_queue,\n                                            replays_dir=replays_dir) for env in self.envs]\n\n    def collect(self, n_steps=1):\n        if not n_steps: n_steps = 1\n        result_queue = self.result_queue\n        for i, collector in enumerate(self.collectors):\n            collector = collector.clone()\n            self.collectors[i] = collector\n            collector.n_episode = max(1, int(n_steps \/ len(self.collectors)))\n            print(\"Starting collector {}\".format(i))\n            collector.start()\n        tmp = []\n        for _ in self.collectors:\n            res = result_queue.get()\n            tmp.extend(res)\n        [collector.join() for collector in self.collectors]\n        return tmp","a9923f2c":"tf_logs_path = os.path.join(data_path, 'tf_log') # For tensorboard\ninfo_path = os.path.join(data_path, 'info.json')\nwriter = tf.summary.create_file_writer(tf_logs_path)\nos.makedirs(tf_logs_path, exist_ok=True)\n\n# Policy\npolicy_path = '\/kaggle_simulations\/agent\/model'\nval_policy_path = '\/kaggle_simulations\/agent\/model_val'\npolicy = PPOPolicy()\n\n\n# restore previous training state\nbest_reward = float('-inf')\nbest_val_reward = 0.\nn_episodes = 0\nrewards=[]\nif os.path.exists(os.path.join(restore_path, 'info.json')):\n    with open(os.path.join(restore_path, 'info.json'), 'r') as f:\n        info = json.load(f)\n        best_reward = info['best_reward']\n        best_val_reward = info['best_val_reward']\n        n_episodes = info['n_episodes']\n    policy.load(os.path.join(restore_path, 'model'))\n\n# Define the episode collector\nPARALLEL_COLLECTOR = False # ParallelEpisodeCollector No working since last update of kaggle-environment\ncollector = None\nn_collect = 5 #collect 5 episodes each step\nif PARALLEL_COLLECTOR:\n    collector = ParallelEpisodeCollector(env_fn, mp.cpu_count(), policy)\nelse:\n    collector = EpisodeCollector(env_fn(), policy)","441527f4":"def train(steps):\n    global best_reward, best_val_reward,rewards,n_episodes\n\n    EpisodeCollector.max_episode = steps\n    EpisodeCollector.n_episode = n_episodes\n    i = 0\n    while EpisodeCollector.n_episode < EpisodeCollector.max_episode:\n        print(\"Collect episodes...\")\n        memories = collector.collect(n_collect)\n        print(\"Updating the policy...\")\n        losses = policy.train(memories)\n        \n        reward = record(memories, EpisodeCollector.n_episode, losses)\n        # Save the best best policy\n        if reward >= best_reward:\n            best_reward = reward\n            print(\"Saving best policy...\")\n            policy.save(policy_path)\n        print(\n            f\"Episode: {n_episodes} | \"\n            f\"Reward: {int(reward)} | \"\n            f\"Best Reward: {int(best_reward)} | \"\n            f\"Episode Rewards: {[mem.rewards[-1] for mem in memories]} | \"\n        )\n\n        # Validation\n        if i % 10 == 0 and i != 0:\n            print(\"Agent validation...\")\n            policy.val = True\n            memories = collector.collect(2)\n            policy.val = False\n\n            EpisodeCollector.n_episode -= len(memories)\n            rew = sum([mem.rewards[-1] for mem in memories if not mem.isEmpty()]) \/ len(memories)\n            print(f\"Validation reward : {rew}\")\n            with writer.as_default():\n                tf.summary.scalar(\"val_reward\", rew, step=EpisodeCollector.n_episode)\n                writer.flush()\n            if rew >= best_val_reward:\n                best_val_reward = rew\n                print(\"Saving best validation policy...\")\n                policy.save(val_policy_path)\n        i += 1\n\n\ndef record(memories: List[Memory], current_step, losses):\n    global n_episodes, info_path,rewards\n    if not memories:\n        return 0\n    n_episodes += len(memories)\n    reward = sum([memory.rewards[-1] for memory in memories]) \/ len(memories)\n    rewards.append(reward)\n\n    with writer.as_default():\n        if losses[0] is not None: tf.summary.scalar(\"Actor loss\", sum(losses[0]) \/ len(losses[0]), step=current_step)\n        if losses[1] is not None: tf.summary.scalar(\"Critic loss\", sum(losses[1]) \/ len(losses[1]), step=current_step)\n        tf.summary.scalar(\"best_reward\", best_reward, step=current_step)\n        writer.flush()\n\n    with open(info_path, 'w') as f:\n        json.dump({\n            'best_reward': best_reward,\n            'n_episodes': n_episodes,\n            'best_val_reward': best_val_reward,\n        }, f)\n    return reward","0dfe3504":"# Start training\nn_collect = 1\n#train(20) # The training is long. I'll just show a few episodes and continue on my laptop","5c832457":"print('Plot rewards')\n# smooth first\ndef exponential_average(old, new, b1=0.99):\n    return old * b1 + (1 - b1) * new\n\nrewards_ = []\nold = 0\nfor r in rewards:\n    old = exponential_average(old,r)\n    rewards_.append(old)\n\nsns.lineplot(range(len(rewards)),rewards_)","83238cfb":"build_actor(verbose=False).save('\/kaggle_simulations\/agent\/model.actor.h5')","57ac6c13":"! ls \/kaggle_simulations\/agent\/","8179c72a":"%%writefile -a \/kaggle_simulations\/agent\/main.py\n\nimport time\n\nprint('loading agent')\nactor = tf.keras.models.load_model('\/kaggle_simulations\/agent\/model.actor.h5',compile = False)\nactor.predict([np.zeros((1,22,8)),np.zeros((1,31))])\n\ndef agent(obs):\n    s = time.time()\n    obs = obs['players_raw'][0]\n    state,_ = OBSParser.parse(obs)\n    action = actor.predict(state)[0]\n    action = np.argmax(action,axis=-1)\n    \n    print(f'action : {action} | Duration : {time.time()-s}')\n    return [int(action)]","84f8befe":"! cd \/kaggle_simulations\/agent\/ && tar -czvf submission.tar.gz  main.py model.actor.h5\n! mv  \/kaggle_simulations\/agent\/submission.tar.gz \/kaggle\/working\/submission.tar.gz","b2a366b9":"import sys \nsys.path.append(os.path.abspath(\"\/kaggle_simulations\/agent\/\"))","28396a88":"import main","3350b379":"from kaggle_environments import make\nenv = make(\"football\", \n           debug=True,\n           configuration={\"save_video\": True, \n                          \"scenario_name\": \"11_vs_11_kaggle\", \n                          \"running_in_notebook\": True})#do_nothing\noutput = env.run([\"\/kaggle_simulations\/agent\/main.py\", main.agent])[-1]\nprint('Left player: reward = %s, status = %s, info = %s' % (output[0]['reward'], output[0]['status'], output[0]['info']))\nprint('Right player: reward = %s, status = %s, info = %s' % (output[1]['reward'], output[1]['status'], output[1]['info']))\nenv.render(mode=\"human\", width=800, height=600)","8c8a2d9c":"! rm -rf football\n! rm -rf kaggle-environments","c6fe2be0":"!! Let test **FootEnv** and **OBSParser**","ad6c9bf0":"## EpisodeCollector & ParallelEpisodeCollector\n\n* EpisodeCollector : run a single environment \n* ParallelEpisodeCollector : run a multiple environments","489c275c":"# Submission","1647a305":"## Prepare Actor&Critic","4ba68b55":"### init ","66b3617a":"## Start training","90c80a3b":"#### This notebook is a copy of [GFootball PPO-Agent](https:\/\/www.kaggle.com\/tchaye59\/gfootball-ppo-agent). I'll get ride of the Transformer a use simple MLP to encode the Unites","ba911921":"### Test the main_test.py file","32c5a1c0":"## PPO Agent\n\nPPO is known for its ease of use and good results, so I hope it will know how to play football.\n\nWe're going to build 3 modules:\nA more convenient environment wrapper to support parallel episodes collection.\nA transformer with and multi-head self-attention Layers that will help to embed players' units and at last, the PPO agent.\n\n## Agent structure\n\n![diagrams](https:\/\/raw.githubusercontent.com\/tchaye59\/GRFootball\/main\/diagrams.jpg)","a5d50871":"### Define used variables","bf3d3a02":"## FootEnv : custom environment Wrapper\nWe use this wrapper to preprocess the GFootball environment players_raw data.\n\n* **units** : Information on both left and right side players is parsed as units. \n* **scalars** : Contain other information.","160455f5":"## Memory\nTo store states, rewards ... for each episode","7f71c893":"## PPOPolicy","9d63e64a":"Now all that remains is to train the agent, hoping that it will converge. Except that train, this agent from scratch will be a very hard task. In my next notebook, I will show how to train it to imitate a rule base one  which will then serve as a base."}}