{"cell_type":{"5778ddb3":"code","66efdc81":"code","40b60f89":"code","90c26f4a":"code","dc473a00":"code","f4fb852e":"code","49c4815f":"code","ee4133ac":"code","36b12c17":"code","c4d228ad":"code","e33778cc":"code","dc8e5975":"code","c44cb784":"code","21b5439d":"code","591ee729":"code","73cc175f":"code","7f837b18":"code","9c62eff9":"code","5bd8cda2":"code","2c6314c0":"code","d216d588":"code","6fd99bf6":"code","945f69e7":"code","5d83e7e4":"markdown","4658c0bc":"markdown","9cd192eb":"markdown","8a8d8a55":"markdown","7e8a30f4":"markdown","cb9e4209":"markdown","9e80466d":"markdown","38a1074e":"markdown","d3a56333":"markdown","1a3c8e6b":"markdown","1a91063a":"markdown","6e05362c":"markdown","2bd54213":"markdown","158b45b3":"markdown","e7cfb990":"markdown","11165e7f":"markdown","351bc939":"markdown","fa72d337":"markdown"},"source":{"5778ddb3":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns","66efdc81":"import os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","40b60f89":"train = pd.read_csv(\"\/kaggle\/input\/twitter-sentiment-analysis-hatred-speech\/train.csv\")\ntest = pd.read_csv(\"\/kaggle\/input\/twitter-sentiment-analysis-hatred-speech\/test.csv\")","90c26f4a":"train","dc473a00":"test","f4fb852e":"sns.displot(train['label'])","49c4815f":"label_cnt = train['label'].value_counts()\nlabel_cnt","ee4133ac":"label_pct = train['label'].value_counts() \/ len(train)\nlabel_pct","36b12c17":"label = train['label']\n\ntrain.drop(['label'], axis=1, inplace=True)\ntrain","c4d228ad":"combi = train.append(test)\ncombi","e33778cc":"tweets = combi['tweet']\n\ncount_words = tweets.str.findall(r'(\\w+)').str.len()\nprint(count_words.sum())","dc8e5975":"import re\nfrom nltk.corpus import stopwords\n\n\"\"\" Cleaning Tweets \"\"\"\ntweets = tweets.str.lower()\n\n# removing special characters and numbers\ntweets = tweets.apply(lambda x : re.sub(\"[^a-z\\s]\",\"\",x) )\n\n# remove hash tags\ntweets = tweets.str.replace(\"#\", \" \")\n\n#remove words less than 2 character\ntweets = tweets.apply(lambda x: ' '.join([w for w in x.split() if len(w)>2]))\n\n# removing stopwords\nstopwords = set(stopwords.words(\"english\"))\ntweets = tweets.apply(lambda x : \" \".join(word for word in x.split() if word not in stopwords ))\n\n\ncount_words = tweets.str.findall(r'(\\w+)').str.len()\nprint(count_words.sum())","c44cb784":"most_freq_words = pd.Series(' '.join(tweets).lower().split()).value_counts()[:25]\ntweets = tweets.apply(lambda x : \" \".join(word for word in x.split() if word not in most_freq_words ))\nprint(most_freq_words)\n\ncount_words = tweets.str.findall(r'(\\w+)').str.len()\nprint(count_words.sum())","21b5439d":"from collections import Counter\nfrom itertools import chain\n\n# split words into lists\nv = tweets.str.split().tolist() \n# compute global word frequency\nc = Counter(chain.from_iterable(v))\n# filter, join, and re-assign\ntweets = [' '.join([j for j in i if c[j] > 1]) for i in v]\n\ntotal_word = 0\nfor x,word in enumerate(tweets):\n    num_word = len(word.split())\n    #print(num_word)\n    total_word = total_word + num_word\nprint(total_word)","591ee729":"import spacy\nimport spacy.cli\nspacy.cli.download(\"en_vectors_web_lg\")\nnlp = spacy.load('en_vectors_web_lg')","73cc175f":"import spacy\nimport en_vectors_web_lg\n\nnlp = en_vectors_web_lg.load()\ndocument = nlp(tweets[0])\nprint(\"Document : \",document)\nprint(\"Tokens : \")\nfor token in document:\n       print(token.text)","7f837b18":"document = nlp(tweets[0])\nprint(document)\nfor token in document:\n    print(token.text, token.vector.shape)","9c62eff9":"document = nlp.pipe(tweets)\ntweets_vector = np.array([tweet.vector for tweet in document])\nprint(tweets_vector.shape)","5bd8cda2":"X = tweets_vector[: len(train)]\ny = label","2c6314c0":"from sklearn.model_selection import train_test_split\n\nX_train,X_val, y_train, y_val = train_test_split(X,y, stratify=y, test_size=0.3, random_state=1)\nX_train.shape, X_val.shape, y_train.shape, y_val.shape","d216d588":"\nfrom sklearn.linear_model import LogisticRegression\n\nmodel = LogisticRegression(C=10,max_iter=1000).fit(X_train, y_train)\nprint(model.score(X_train, y_train))","6fd99bf6":"y_pred = model.predict(X_val)\nprint(model.score(X_val, y_val))","945f69e7":"from sklearn.metrics import confusion_matrix\n\nprint(confusion_matrix(y_val, y_pred))\n","5d83e7e4":"Import","4658c0bc":"Confusion matrix","9cd192eb":"Split","8a8d8a55":"Select model","7e8a30f4":"Remove rare words","cb9e4209":"Remove frequently used words","9e80466d":"Sentence to vector using pipe","38a1074e":"Combine train and test","d3a56333":"Define X and y","1a3c8e6b":"Clean tweets","1a91063a":"Load","6e05362c":"Analyse label","2bd54213":"Drop label","158b45b3":"The objective of this task is to detect hate speech in tweets. For the sake of simplicity, we say a tweet contains hate speech if it has a racist or sexist sentiment associated with it. So, the task is to classify racist or sexist tweets from other tweets.\n\nFormally, given a training sample of tweets and labels, where label '1' denotes the tweet is racist\/sexist and label '0' denotes the tweet is not racist\/sexist, your objective is to predict the labels on the test dataset.\n\ntry:- https:\/\/medium.com\/analytics-vidhya\/learn-how-to-use-spacy-for-natural-language-processing-661805d3abae","e7cfb990":"Predict on validation set","11165e7f":"Create tokins with spacey","351bc939":"Token to vector","fa72d337":"Read"}}