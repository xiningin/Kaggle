{"cell_type":{"63cfb7a9":"code","7026ccf8":"code","e05b59ca":"code","c751ef43":"code","156f73db":"code","bc20240a":"code","f4775e33":"code","9897c907":"code","7cf04738":"code","57791643":"code","6398ee08":"code","ddc226cd":"code","59b07a65":"code","ca6e1510":"code","c75dde86":"code","ce986eb0":"code","fa2089b0":"code","5db55572":"code","d9577b83":"code","55f5da57":"code","b363fe90":"code","b26b0cb0":"code","0fcb8d8a":"code","806454ae":"code","0f4d9de6":"code","658f1e8a":"code","b56d9b8a":"code","5a9c6446":"code","85ec01b7":"code","64688996":"code","6fbf39b2":"code","649b03f4":"code","33031833":"code","06b02255":"code","98e00aa2":"code","ea5b792e":"code","b5e3b4b2":"code","d7fd9dc3":"code","cce98dea":"code","7981a8fd":"code","c9a74b96":"code","cbf40fbc":"code","1ccac661":"code","9f1013f5":"code","23845569":"code","32be86f0":"code","8a1b637c":"code","2659b3bb":"code","8c29d4f2":"code","0dd8f131":"code","85ff5cfd":"code","4d41b063":"code","220f3322":"code","050c3778":"code","f21e42b1":"code","823fd8d5":"code","365b57a0":"code","ffee8e6c":"code","a1286440":"code","84b960cc":"code","d60ea037":"code","99acfae9":"code","4a8ad711":"code","a8ed1229":"code","52bda60a":"code","68b02be4":"code","75ff2f78":"code","bfbabd97":"code","30ee3638":"code","afd967c4":"markdown","3c66154d":"markdown","d275c628":"markdown","350f3c2a":"markdown","18b705e3":"markdown","a735ac3f":"markdown","00241514":"markdown","33234cdb":"markdown","14d52478":"markdown","46c635a4":"markdown","8a913f2f":"markdown","876e170d":"markdown","9e1448f3":"markdown","68a5aefd":"markdown","f8a11184":"markdown","5297d0c6":"markdown"},"source":{"63cfb7a9":"week = 4","7026ccf8":"import numpy as np \nimport pandas as pd\nimport datetime\nimport os\nimport matplotlib.pyplot as plt\n\npd.set_option('display.max_columns', 500)\n","e05b59ca":"train = pd.read_csv(f\"\/kaggle\/input\/covid19-global-forecasting-week-{week}\/train.csv\")\ntrain[\"geo\"] = np.where(train[\"Province_State\"].isna(), train[\"Country_Region\"], train[\"Country_Region\"] + \"_\" + train[\"Province_State\"])\n\ntrain['Date'] = pd.to_datetime(train['Date'])\ntrain = train.loc[train['Date'] > '2020-02-20', :]\ntrain_last_date = train.Date.unique()[-1]\nprint(f\"Dataset has training data untill : {str(train_last_date)[:10]}\")\nprint(f\"Training dates: {len(train.Date.unique())}\")","c751ef43":"train.info()","156f73db":"train.describe(include='all')","bc20240a":"additional = pd.read_csv(f\"..\/input\/covid19-country-data-wk3-release\/Data Join - RELEASE.csv\")\nadditional.info()","f4775e33":"additional[\"TRUE POPULATION\"] = additional[\"TRUE POPULATION\"].str.strip().str.replace(',','').astype(int)\nadditional['pct_in_largest_city'] = additional['pct_in_largest_city'].str.replace('%','').astype(float)\nadditional[' TFR '] = additional[' TFR '].replace('N.A.',np.nan).astype(float)\nadditional['Personality_uai'] = pd.to_numeric(additional['Personality_uai'], errors='coerce')\nadditional['Personality_pdi'] = pd.to_numeric(additional['Personality_pdi'], errors='coerce')\nadditional['Personality_idv'] = pd.to_numeric(additional['Personality_idv'], errors='coerce')\nadditional['Personality_ltowvs'] = pd.to_numeric(additional['Personality_ltowvs'], errors='coerce')\nadditional['personality_agreeableness'] = pd.to_numeric(additional['personality_agreeableness'], errors='coerce')\nadditional['AIR_AVG'] = pd.to_numeric(additional['AIR_AVG'], errors='coerce')\nadditional[' Avg_age '] = pd.to_numeric(additional[' Avg_age '], errors='coerce')\nadditional['Personality_mas'] = pd.to_numeric(additional['Personality_mas'], errors='coerce')\n","9897c907":"additional.info()","7cf04738":"\nadditional[\"geo\"] = np.where(additional[\"Province_State\"].isna(), additional[\"Country_Region\"], additional[\"Country_Region\"] + \"_\" + additional[\"Province_State\"])\nadditional.describe(include ='all')","57791643":"additional.describe()","6398ee08":"additional.drop(['Province_State','Country_Region'],inplace=True,axis=1)","ddc226cd":"additional.info()","59b07a65":"train_add = train.merge(additional,on='geo',how='left')","ca6e1510":"train_add.describe(include='all')","c75dde86":"from sklearn.pipeline import Pipeline\nfrom sklearn.base import BaseEstimator, TransformerMixin\nfrom functools import partial\nfrom sklearn.feature_selection import mutual_info_regression, SelectKBest\nfrom sklearn.ensemble import RandomForestRegressor\n## Class for feature Selection\nclass selectFeaturesTransformer(BaseEstimator, TransformerMixin):\n    \"\"\"Custom scaling transformer\"\"\"\n    def __init__(self, k=10,method='RF',discreteCol=[], order=[],scores=[]):\n        \"\"\" \n        initialize transformer\n        Inputs : \n            k -- number of features to keep\n            method -- method to use, either 'Mutual Information or RF\n            discreteCol -- if Mutual Information is used, specify indexes of discrete columns\n        \"\"\"\n        self.k = k\n        self.method = method\n        self.order = order\n        self.discreteCol = discreteCol\n        self.scores = scores\n        \n        \n        \n\n    def fit(self, X_train,y_train):\n        \"\"\"\n        Fit the transformer on data\n        Input :\n            X_train -- features array\n            Y_train -- labels array\n        Output :\n            fitted transformer\n        \"\"\"\n        if self.method == \"Mutual Information\" :\n            \n            if len(y_train.shape)>1 and y_train.shape[1]>1 :\n                scores = np.zeros(X_train.shape[1])\n                for i in y_train.columns :\n                    discrete_mutual_info_regression = partial(mutual_info_regression,discrete_features=self.discreteCol)\n                    featS = SelectKBest(k=self.k, score_func=discrete_mutual_info_regression).fit(X_train,y_train[i] )\n                    scores += featS.scores_\n                    print(\"Top 10 selected by Mutual information for \",i)\n                    print(list(X_train.columns[np.flip(featS.scores_.argsort())]))\n                self.order = np.flip(scores.argsort())\n                self.scores = np.flip(np.sort(scores))\n            else :\n                discrete_mutual_info_regression = partial(mutual_info_regression)\n                featS = SelectKBest(k=self.k, score_func=discrete_mutual_info_regression).fit(X_train,y_train )\n                self.order = np.flip(featS.scores_.argsort())\n                self.scores = np.flip(np.sort(featS.scores_))\n            #self.selectedColumns = [columns_eng[i]  for i in self.order[:self.k]]\n            #return X_train[:,order_mi[:self.k]]\n        \n        elif self.method == 'RF' :\n            if len(y_train.shape)>1 and y_train.shape[1]>1 :\n                scores = np.zeros(X_train.shape[1])\n                for i in y_train.columns :\n                    rfModel = RandomForestRegressor(n_estimators=500,random_state =0).fit(X_train.values, y_train[i].values)\n                    scores = scores + rfModel.feature_importances_\n                    print(\"Top 10 selected by Random Forest for \",i)\n                    print(list(X_train.columns[np.flip(rfModel.feature_importances_.argsort())]))\n                self.order = np.flip(scores.argsort())\n                self.scores = np.flip(np.sort(scores))\n            else :        \n                rfModel = RandomForestRegressor(random_state =0).fit(X_train, y_train)\n                order = np.flip(rfModel.feature_importances_.argsort())\n                self.order = np.flip(rfModel.feature_importances_.argsort())\n                self.scores = np.flip(np.sort(rfModel.feature_importances_))\n            #self.selectedColumns = [columns_eng[i]  for i in order_rf[:self.k]]\n            #return X_train[:,order_[:self.k]]\n        return self\n            \n                \n        \n    def transform(self, X_train):\n        \"\"\"\n        apply fitted transformer to select features\n        Input :\n            X_train -- features array\n        Output :\n            array containing only selected features\n        \"\"\"\n        return X_train[self.order]\n","ce986eb0":"add_cols = [c for c in train_add.columns if c not in train.columns]","fa2089b0":"#discreteCol = [i for i in range(len(X_train.columns)) if X_train_norm.columns[i] in ['year','month','day','dayYear']]\nX_train = train_add[add_cols]\nX_train.replace(np.nan,X_train.mean(),inplace=True)\nFs = selectFeaturesTransformer(method=\"Mutual Information\", discreteCol=[],k=len(X_train.columns),order=[])\nFs.fit(X_train,train_add['ConfirmedCases'])\nprint(\"Top 10 selected by Mutual information for ConfirmedCases \")\nprint(list(X_train.columns[Fs.order]))","5db55572":"print(list(Fs.scores))","d9577b83":"Fs_rf = selectFeaturesTransformer(method=\"RF\", discreteCol=[],k=X_train.shape[1],order=[])\nFs_rf.fit(X_train.drop(['geo'],axis=1),train_add['ConfirmedCases'])\nprint(\"Top 10 selected by Random Forest for ConfirmedCases \")\nprint(list(X_train.drop(['geo'],axis=1).columns[Fs_rf.order]))","55f5da57":"print(list(Fs_rf.scores))","b363fe90":"Fs_rf = selectFeaturesTransformer(method=\"RF\", discreteCol=[],k=X_train.shape[1],order=[])\nFs_rf.fit(X_train,train_add['Fatalities'])\nprint(\"Top 10 selected by Random Forest for ConfirmedCases \")\nprint(list(X_train.columns[Fs_rf.order]))","b26b0cb0":"print(list(Fs_rf.scores))","0fcb8d8a":"import seaborn as sns\nplt.figure(figsize=(30,30))\ncorr = X_train.corr()\nfig, ax = plt.subplots(figsize=(12, 9))\nsns.heatmap(corr, vmax=.8,annot=True, square=True)","806454ae":"to_use = ['GDP_region', 'max_high_rises', 'pct_in_largest_city', 'TRUE POPULATION', 'latitude', \n          'personality_perform', 'Personality_idv', 'Personality_ltowvs', 'AIR_AVG', \n          'Personality_mas', 'Personality_pdi', 'continent_Life_expectancy', 'personality_agreeableness', \n          'Personality_uai', 'continent_happiness', 'continent_corruption', 'continent_generosity',\n           'longitude', 'murder', 'humidity', ' TFR ',\n           'temperature', 'Personality_assertive']","0f4d9de6":"test = pd.read_csv(f\"\/kaggle\/input\/covid19-global-forecasting-week-{week}\/test.csv\")\ntest['Date'] = pd.to_datetime(test['Date'])\ntest_first_date = test['Date'].values[0]\ntest_last_date = test['Date'].values[-1]\nprint(f'Test period from {str(test_first_date)[:10]} to {str(test_last_date)[:10]}')","658f1e8a":"period = (np.array(test_last_date, dtype='datetime64[D]').astype(np.int64) - np.array(train_last_date, dtype='datetime64[D]').astype(np.int64))","b56d9b8a":"print(f\"Prediction days: {(np.array(test_last_date, dtype='datetime64[D]').astype(np.int64) - np.array(train_last_date, dtype='datetime64[D]').astype(np.int64))+1}\")\nprint(f\"Public set: {(np.array(train_last_date, dtype='datetime64[D]').astype(np.int64) - np.array(test_first_date, dtype='datetime64[D]').astype(np.int64))+1}\")\nprint(f\"Full prediction set: {(np.array(test_last_date, dtype='datetime64[D]').astype(np.int64) - np.array(test_first_date, dtype='datetime64[D]').astype(np.int64))+1}\")","5a9c6446":"win = 20","85ec01b7":"base_1 = train.pivot(index='Date', columns=\"geo\", values='ConfirmedCases').iloc[-win,:].values\nbase_2 = train.pivot(index='Date', columns=\"geo\", values='Fatalities').iloc[-win,:].values","64688996":"train.pivot(index='geo', columns=\"Date\", values=['ConfirmedCases']).values","6fbf39b2":"train","649b03f4":"train['ConfirmedCases'] = train['ConfirmedCases'] - train.groupby('geo')['ConfirmedCases'].shift(periods=1)\ntrain['Fatalities'] = train['Fatalities'] - train.groupby('geo')['Fatalities'].shift(periods=1)\n\ntrain = train.groupby('geo').tail(train.groupby('geo').size().values[0]-1)\n\ntrain['ConfirmedCases'] = np.where(train['ConfirmedCases'] < 0, 0.0, train['ConfirmedCases'])\ntrain['Fatalities'] = np.where(train['Fatalities'] < 0, 0.0, train['Fatalities'])\ntrain","33031833":"train_add['geo']","06b02255":"add_d","98e00aa2":"X_train['geo'] = train_add['geo']","ea5b792e":"X_train.describe(include='all')","b5e3b4b2":"\nX = X_train.set_index('geo')\nX.loc[~X.index.duplicated(keep='first')]\nadd_d = X.loc[~X.index.duplicated(keep='first')][to_use]","d7fd9dc3":"from sklearn.preprocessing import MinMaxScaler\nscaler = MinMaxScaler()\nadd_d = scaler.fit_transform(add_d)","cce98dea":"train_cases = train.pivot(index='Date', columns=\"geo\", values='ConfirmedCases').iloc[:-3,:].values\nvalid_cases = train.pivot(index='Date', columns=\"geo\", values='ConfirmedCases').iloc[-(win+3):,:].values\n\ntrain_fatal = train.pivot(index='Date', columns=\"geo\", values='Fatalities').iloc[:-3,:].values\nvalid_fatal = train.pivot(index='Date', columns=\"geo\", values='Fatalities').iloc[-(win+3):,:].values","7981a8fd":"train_cases.shape","c9a74b96":"_ = plt.plot(train_cases)","cbf40fbc":"_ = plt.plot(valid_cases)","1ccac661":"_ = plt.plot(train_fatal)","9f1013f5":"_ = plt.plot(valid_fatal)","23845569":"%%bash\n\npip install pytorch_lightning","32be86f0":"import torch.nn as nn\nimport torch\nimport torch.nn.functional as F\nimport pytorch_lightning as ptl\n\nfrom torch import optim\nfrom torch.utils.data import DataLoader\nfrom collections import OrderedDict","8a1b637c":"def rmsle(predict, target): \n    return torch.sqrt(((torch.log(predict + 1) - torch.log(target + 1))**2).mean())","2659b3bb":"torch.from_numpy(np.array([1,2,3])).type(torch.FloatTensor)","8c29d4f2":"class MTSFDataset(torch.utils.data.Dataset):\n\n    def __init__(self, window, horizon, set_type, tra, validation,add_tr,add_val):\n        \n        assert type(set_type) == type('str')\n        \n        self.window = window\n        self.horizon = horizon\n        self.tra = tra\n        self.validation = validation\n        self.set_type = set_type\n        \n        if set_type == 'train':\n            rawdata = tra\n            self.add = torch.from_numpy(add_tr).type(torch.FloatTensor)\n        elif set_type == 'validation':\n            rawdata = validation\n            self.add = torch.from_numpy(add_val).type(torch.FloatTensor)\n\n        self.len, self.var_num = rawdata.shape\n        self.sample_num = max(self.len - self.window - self.horizon + 1, 0)\n        self.samples, self.labels = self.__getsamples(rawdata)\n\n    def __getsamples(self, data):\n        X = torch.zeros((self.sample_num, self.window, self.var_num))\n        Y = torch.zeros((self.sample_num, 1, self.var_num))\n       # print('################',self.sample_num,self.var_num,'#####################')\n        for i in range(self.sample_num):\n            start = i\n            end = i + self.window\n            X[i, :, :] = torch.from_numpy(data[start:end, :])\n            Y[i, :, :] = torch.from_numpy(data[end+self.horizon-1, :])\n        \n        return (X, Y)\n\n    def __len__(self):\n        return self.sample_num\n\n    def __getitem__(self, idx):\n        sample = [self.samples[idx, :, :], self.labels[idx, :, :],self.add]\n        #print(\"######## Sample #####\", sample[0].shape,sample[1].shape,sample[2].shape)\n\n        return sample","0dd8f131":"class ScaledDotProductAttention(nn.Module):\n\n    def __init__(self, temperature, attn_dropout=0.1):\n        super().__init__()\n        self.temperature = temperature\n        self.dropout = nn.Dropout(attn_dropout)\n        self.softmax = nn.Softmax(dim=2)\n\n    def forward(self, q, k, v):\n\n        attn = torch.bmm(q, k.transpose(1, 2))\n        attn = attn \/ self.temperature\n\n        attn = self.softmax(attn)\n        attn = self.dropout(attn)\n        output = torch.bmm(attn, v)\n\n        return output, attn\n\nclass MultiHeadAttention(nn.Module):\n\n    def __init__(self, n_head, d_model, d_k, d_v, dropout=0.1):\n        super().__init__()\n\n        self.n_head = n_head\n        self.d_k = d_k\n        self.d_v = d_v\n\n        self.w_qs = nn.Linear(d_model, n_head * d_k)\n        self.w_ks = nn.Linear(d_model, n_head * d_k)\n        self.w_vs = nn.Linear(d_model, n_head * d_v)\n        nn.init.normal_(self.w_qs.weight, mean=0, std=np.sqrt(2.0 \/ (d_model + d_k)))\n        nn.init.normal_(self.w_ks.weight, mean=0, std=np.sqrt(2.0 \/ (d_model + d_k)))\n        nn.init.normal_(self.w_vs.weight, mean=0, std=np.sqrt(2.0 \/ (d_model + d_v)))\n\n        self.attention = ScaledDotProductAttention(temperature=np.power(d_k, 0.5))\n        self.layer_norm = nn.LayerNorm(d_model)\n\n        self.fc = nn.Linear(n_head * d_v, d_model)\n        nn.init.xavier_normal_(self.fc.weight)\n\n        self.dropout = nn.Dropout(dropout)\n\n\n    def forward(self, q, k, v):\n\n        d_k, d_v, n_head = self.d_k, self.d_v, self.n_head\n\n        sz_b, len_q, _ = q.size()\n        sz_b, len_k, _ = k.size()\n        sz_b, len_v, _ = v.size()\n\n        residual = q\n\n        q = self.w_qs(q).view(sz_b, len_q, n_head, d_k)\n        k = self.w_ks(k).view(sz_b, len_k, n_head, d_k)\n        v = self.w_vs(v).view(sz_b, len_v, n_head, d_v)\n\n        q = q.permute(2, 0, 1, 3).contiguous().view(-1, len_q, d_k)\n        k = k.permute(2, 0, 1, 3).contiguous().view(-1, len_k, d_k)\n        v = v.permute(2, 0, 1, 3).contiguous().view(-1, len_v, d_v)\n\n        output, attn = self.attention(q, k, v)\n\n        output = output.view(n_head, sz_b, len_q, d_v)\n        output = output.permute(1, 2, 0, 3).contiguous().view(sz_b, len_q, -1)\n\n        output = self.dropout(self.fc(output))\n        output = self.layer_norm(output + residual)\n\n        return output, attn\n\nclass PositionwiseFeedForward(nn.Module):\n\n    def __init__(self, d_in, d_hid, dropout=0.1):\n        super().__init__()\n        self.w_1 = nn.Conv1d(d_in, d_hid, 1)\n        self.w_2 = nn.Conv1d(d_hid, d_in, 1)\n        self.layer_norm = nn.LayerNorm(d_in)\n        self.dropout = nn.Dropout(dropout)\n\n    def forward(self, x):\n        residual = x\n        output = x.transpose(1, 2)\n        output = self.w_2(F.relu(self.w_1(output)))\n        output = output.transpose(1, 2)\n        output = self.dropout(output)\n        output = self.layer_norm(output + residual)\n        return output\n\nclass EncoderLayer(nn.Module):\n\n    def __init__(self, d_model, d_inner, n_head, d_k, d_v, dropout=0.1):\n        super(EncoderLayer, self).__init__()\n        self.slf_attn = MultiHeadAttention(\n            n_head, d_model, d_k, d_v, dropout=dropout)\n        self.pos_ffn = PositionwiseFeedForward(d_model, d_inner, dropout=dropout)\n\n    def forward(self, enc_input):\n        enc_output, enc_slf_attn = self.slf_attn(\n            enc_input, enc_input, enc_input)\n\n        enc_output = self.pos_ffn(enc_output)\n\n        return enc_output, enc_slf_attn\n\n\nclass DecoderLayer(nn.Module):\n\n    def __init__(self, d_model, d_inner, n_head, d_k, d_v, dropout=0.1):\n        super(DecoderLayer, self).__init__()\n        self.slf_attn = MultiHeadAttention(n_head, d_model, d_k, d_v, dropout=dropout)\n        self.enc_attn = MultiHeadAttention(n_head, d_model, d_k, d_v, dropout=dropout)\n        self.pos_ffn = PositionwiseFeedForward(d_model, d_inner, dropout=dropout)\n\n    def forward(self, dec_input, enc_output, non_pad_mask=None, slf_attn_mask=None, dec_enc_attn_mask=None):\n        dec_output, dec_slf_attn = self.slf_attn(\n            dec_input, dec_input, dec_input, mask=slf_attn_mask)\n\n        dec_output, dec_enc_attn = self.enc_attn(\n            dec_output, enc_output, enc_output, mask=dec_enc_attn_mask)\n\n        dec_output = self.pos_ffn(dec_output)\n\n        return dec_output, dec_slf_attn, dec_enc_attn\n\n\nclass Single_Global_SelfAttn_Module(nn.Module):\n\n    def __init__(\n            self,\n            window, n_multiv, n_kernels, w_kernel,\n            d_k, d_v, d_model, d_inner,\n            n_layers, n_head, drop_prob=0.1):\n        '''\n        Args:\n        window (int): the length of the input window size\n        n_multiv (int): num of univariate time series\n        n_kernels (int): the num of channels\n        w_kernel (int): the default is 1\n        d_k (int): d_model \/ n_head\n        d_v (int): d_model \/ n_head\n        d_model (int): outputs of dimension\n        d_inner (int): the inner-layer dimension of Position-wise Feed-Forward Networks\n        n_layers (int): num of layers in Encoder\n        n_head (int): num of Multi-head\n        drop_prob (float): the probability of dropout\n        '''\n\n        super(Single_Global_SelfAttn_Module, self).__init__()\n\n        self.window = window\n        self.w_kernel = w_kernel\n        self.n_multiv = n_multiv\n        self.d_model = d_model\n        self.drop_prob = drop_prob\n        self.conv2 = nn.Conv2d(1, n_kernels, (window, w_kernel))\n        self.in_linear = nn.Linear(n_kernels, d_model)\n        self.out_linear = nn.Linear(d_model, n_kernels)\n\n        self.layer_stack = nn.ModuleList([\n            EncoderLayer(d_model, d_inner, n_head, d_k, d_v, dropout=drop_prob)\n            for _ in range(n_layers)])\n\n    def forward(self, x, return_attns=False):\n\n        x = x.view(-1, self.w_kernel, self.window, self.n_multiv)\n        x2 = F.relu(self.conv2(x))\n        x2 = nn.Dropout(p=self.drop_prob)(x2)\n        x = torch.squeeze(x2, 2)\n        x = torch.transpose(x, 1, 2)\n        src_seq = self.in_linear(x)\n\n        enc_slf_attn_list = []\n\n        enc_output = src_seq\n\n        for enc_layer in self.layer_stack:\n            enc_output, enc_slf_attn = enc_layer(enc_output)\n            if return_attns:\n                enc_slf_attn_list += [enc_slf_attn]\n\n        if return_attns:\n            return enc_output, enc_slf_attn_list\n        enc_output = self.out_linear(enc_output)\n        return enc_output,\n\n\nclass Single_Local_SelfAttn_Module(nn.Module):\n\n    def __init__(\n            self,\n            window, local, n_multiv, n_kernels, w_kernel,\n            d_k, d_v, d_model, d_inner,\n            n_layers, n_head, drop_prob=0.1):\n        '''\n        Args:\n        window (int): the length of the input window size\n        n_multiv (int): num of univariate time series\n        n_kernels (int): the num of channels\n        w_kernel (int): the default is 1\n        d_k (int): d_model \/ n_head\n        d_v (int): d_model \/ n_head\n        d_model (int): outputs of dimension\n        d_inner (int): the inner-layer dimension of Position-wise Feed-Forward Networks\n        n_layers (int): num of layers in Encoder\n        n_head (int): num of Multi-head\n        drop_prob (float): the probability of dropout\n        '''\n\n        super(Single_Local_SelfAttn_Module, self).__init__()\n\n        self.window = window\n        self.w_kernel = w_kernel\n        self.n_multiv = n_multiv\n        self.d_model = d_model\n        self.drop_prob = drop_prob\n        self.conv1 = nn.Conv2d(1, n_kernels, (local, w_kernel))\n        self.pooling1 = nn.AdaptiveMaxPool2d((1, n_multiv))\n        self.in_linear = nn.Linear(n_kernels, d_model)\n        self.out_linear = nn.Linear(d_model, n_kernels)\n\n        self.layer_stack = nn.ModuleList([\n            EncoderLayer(d_model, d_inner, n_head, d_k, d_v, dropout=drop_prob)\n            for _ in range(n_layers)])\n\n    def forward(self, x, return_attns=False):\n\n        x = x.view(-1, self.w_kernel, self.window, self.n_multiv)\n        x1 = F.relu(self.conv1(x))\n        x1 = self.pooling1(x1)\n        x1 = nn.Dropout(p=self.drop_prob)(x1)\n        x = torch.squeeze(x1, 2)\n        x = torch.transpose(x, 1, 2)\n        src_seq = self.in_linear(x)\n\n        enc_slf_attn_list = []\n\n        enc_output = src_seq\n\n        for enc_layer in self.layer_stack:\n            enc_output, enc_slf_attn = enc_layer(enc_output)\n            if return_attns:\n                enc_slf_attn_list += [enc_slf_attn]\n\n        if return_attns:\n            return enc_output, enc_slf_attn_list\n        enc_output = self.out_linear(enc_output)\n        return enc_output,\n\nclass AR(nn.Module):\n\n    def __init__(self, window):\n        super(AR, self).__init__()\n        self.linear = nn.Linear(window, 1)\n\n    def forward(self, x):\n        x = torch.transpose(x, 1, 2)\n        x = self.linear(x)\n        x = torch.transpose(x, 1, 2)\n        return x\n\nclass DSANet(ptl.LightningModule):\n\n    def __init__(self, tra, validation, add_t, add_v, n_multiv, batch_size=16, window=64, local=3, n_kernels=32, \n                 drop_prob=0.1, criterion='rmsle_loss', learning_rate=0.005, horizon=14):\n        \n        super(DSANet, self).__init__()\n\n        self.batch_size = batch_size\n\n        self.window = window\n        self.local = local\n        self.n_multiv = n_multiv\n        self.n_kernels = n_kernels\n        self.w_kernel = 1\n        self.n_add = add_t.shape[1]\n\n        self.d_model = 512\n        self.d_inner = 2048\n        self.n_layers = 6\n        self.n_head = 8\n        self.d_k = 64\n        self.d_v = 64\n        self.drop_prob = drop_prob\n\n        self.criterion = criterion\n        self.learning_rate = learning_rate\n        self.horizon = horizon\n        self.tra = tra\n        self.validation = validation\n        self.add_t = add_t\n        self.add_v = add_v\n        \n        self.losses_v = []\n        self.losses_t = []\n\n        self.__build_model()\n\n    def __build_model(self):\n\n        self.sgsf = Single_Global_SelfAttn_Module(\n            window=self.window, n_multiv=self.n_multiv, n_kernels=self.n_kernels,\n            w_kernel=self.w_kernel, d_k=self.d_k, d_v=self.d_v, d_model=self.d_model,\n            d_inner=self.d_inner, n_layers=self.n_layers, n_head=self.n_head, drop_prob=self.drop_prob)\n\n        self.slsf = Single_Local_SelfAttn_Module(\n            window=self.window, local=self.local, n_multiv=self.n_multiv, n_kernels=self.n_kernels,\n            w_kernel=self.w_kernel, d_k=self.d_k, d_v=self.d_v, d_model=self.d_model,\n            d_inner=self.d_inner, n_layers=self.n_layers, n_head=self.n_head, drop_prob=self.drop_prob)\n\n        self.ar = AR(window=self.window)\n        self.W_output1 = nn.Linear(2 * self.n_kernels+self.n_add, 2 * self.n_kernels+self.n_add)\n        self.W_output2 = nn.Linear(2 * self.n_kernels+self.n_add, 1)\n        self.dropout = nn.Dropout(p=self.drop_prob)\n        self.active_func = nn.Tanh()\n\n    def forward(self, x, add):\n \n        sgsf_output, *_ = self.sgsf(x)\n        slsf_output, *_ = self.slsf(x)\n        sf_output = torch.cat((sgsf_output, slsf_output, add), 2)\n        #print('#######',sf_output.shape)\n        \n        #print('#######',sf_output.shape)\n        sf_output = self.dropout(sf_output)\n        sf_output = self.W_output1(sf_output)\n        #print('#######',sf_output.shape)\n        sf_output = self.W_output2(sf_output)\n        #print('#######',sf_output.shape)\n\n        sf_output = torch.transpose(sf_output, 1, 2)\n\n        ar_output = self.ar(x)\n\n        output = sf_output + ar_output\n        output[output < 0] = 0.0\n\n        return output\n\n    def loss(self, labels, predictions):\n        if self.criterion == 'l1_loss':\n            loss = F.l1_loss(predictions, labels)\n        elif self.criterion == 'mse_loss':\n            loss = F.mse_loss(predictions, labels)\n        elif self.criterion == 'rmsle_loss':\n            loss = rmsle(predictions, labels)\n        return loss\n\n    def training_step(self, data_batch, batch_i):\n\n        x, y, add = data_batch\n\n        y_hat = self.forward(x, add)\n\n        loss_val = self.loss(y, y_hat)\n\n        if self.trainer.use_dp:\n            loss_val = loss_val.unsqueeze(0)\n\n        output = OrderedDict({\n            'loss': loss_val\n        })\n        self.losses_t.append(torch.mean(loss_val))\n        return output\n\n    def validation_step(self, data_batch, batch_i):\n\n        x, y, add = data_batch\n\n        y_hat = self.forward(x, add)\n\n        loss_val = self.loss(y, y_hat)\n\n        if self.trainer.use_dp:\n            loss_val = loss_val.unsqueeze(0)\n\n        output = OrderedDict({\n            'val_loss': loss_val,\n            'y': y,\n            'y_hat': y_hat,\n        })\n        self.losses_v.append(torch.mean(loss_val))\n        return output\n\n    def validation_epoch_end(self, outputs):\n\n        loss_sum = 0\n        for x in outputs:\n            loss_sum += x['val_loss'].item()\n        val_loss_mean = loss_sum \/ len(outputs)\n\n        y = torch.cat(([x['y'] for x in outputs]), 0)\n        y_hat = torch.cat(([x['y_hat'] for x in outputs]), 0)\n\n        num_var = y.size(-1)\n        y = y.view(-1, num_var)\n        y_hat = y_hat.view(-1, num_var)\n        sample_num = y.size(0)\n\n        y_diff = y_hat - y\n        y_mean = torch.mean(y)\n        y_translation = y - y_mean\n\n        val_rrse = torch.sqrt(torch.sum(torch.pow(y_diff, 2))) \/ torch.sqrt(torch.sum(torch.pow(y_translation, 2)))\n\n        y_m = torch.mean(y, 0, True)\n        y_hat_m = torch.mean(y_hat, 0, True)\n        y_d = y - y_m\n        y_hat_d = y_hat - y_hat_m\n        corr_top = torch.sum(y_d * y_hat_d, 0)\n        corr_bottom = torch.sqrt((torch.sum(torch.pow(y_d, 2), 0) * torch.sum(torch.pow(y_hat_d, 2), 0)))\n        corr_inter = corr_top \/ corr_bottom\n        val_corr = (1. \/ num_var) * torch.sum(corr_inter)\n\n        val_mae = (1. \/ (sample_num * num_var)) * torch.sum(torch.abs(y_diff))\n\n        tqdm_dic = {\n            'val_loss': val_loss_mean,\n            'RRSE': val_rrse.item(),\n            'CORR': val_corr.item(),\n            'MAE': val_mae.item()\n        }\n        return tqdm_dic\n\n    def configure_optimizers(self):\n\n        optimizer = optim.Adam(self.parameters(), lr=self.learning_rate)\n        scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=10)\n        return [optimizer], [scheduler] \n\n    def __dataloader(self, train):\n\n        set_type = train\n        dataset = MTSFDataset(window=self.window, horizon=self.horizon,\n                              set_type=set_type, \n                              tra=self.tra, validation=self.validation,add_tr=self.add_t, add_val=self.add_v)\n\n        train_sampler = None\n        batch_size = self.batch_size\n\n        should_shuffle = train_sampler is None\n        loader = DataLoader(\n            dataset=dataset,\n            batch_size=batch_size,\n            shuffle=should_shuffle,\n            sampler=train_sampler,\n            num_workers=4\n        )\n\n        return loader\n\n    @ptl.data_loader\n    def train_dataloader(self):\n        return self.__dataloader(train='train')\n\n    @ptl.data_loader\n    def val_dataloader(self):\n        return self.__dataloader(train='validation')","85ff5cfd":"add_d[:,0:1].shape","4d41b063":"#rom p.loggers import TensorBoardLogger\n#logger = ptl.loggers.TensorBoardLogger(\"tb_logs\", name=\"model_bis\")\nmodel_cases = DSANet(train_cases, valid_cases,add_d,add_d, train_cases.shape[1], window=win, learning_rate=0.005, horizon=1, drop_prob=0.2)\n\ntrainer = ptl.Trainer(val_check_interval=1, max_steps=200,gpus=1)#,logger=logger) \ntrainer.fit(model_cases) ","220f3322":"plt.plot(model_cases.losses_t,label = 'T')\nplt.plot(model_cases.losses_v,label='V')\nplt.legend()","050c3778":"plt.plot(model_cases.losses_t,label = 'T')\nplt.plot(model_cases.losses_v,label='V')\nplt.legend()","f21e42b1":"torch.mean(model_cases.losses_v)","823fd8d5":"from glob import glob\n\nsd = torch.load(glob(\"\/kaggle\/working\/lightning_logs\/version_8\/checkpoints\/*.ckpt\")[0])\nmodel_cases.load_state_dict(sd['state_dict'])","365b57a0":"add_cuda.shape","ffee8e6c":"input = train.pivot(index='Date', columns=\"geo\", values='ConfirmedCases').iloc[-win:,:].values\n\nfor i in range(period):\n    ins = torch.tensor(input[-win:, :]).cuda()\n    add_cuda = torch.tensor([add_d]).cuda()\n    pred = model_cases(ins.unsqueeze(dim=0).float(),add_cuda.float())\n    \n    input = np.concatenate([input, np.array(pred.detach().cpu().numpy(), dtype=np.int).reshape(1, train_cases.shape[1])], axis=0)","a1286440":"train_cases.shape","84b960cc":"model_fatal = DSANet(train_fatal, valid_fatal, add_d,add_d, train_fatal.shape[1], window=win, learning_rate=0.0005, horizon=1, drop_prob=0.2)\n\ntrainer = ptl.Trainer(val_check_interval=1, max_steps=200,gpus=1) #\ntrainer.fit(model_fatal) ","d60ea037":"sd = torch.load(glob(\"\/kaggle\/working\/lightning_logs\/version_11\/checkpoints\/*.ckpt\")[0])\nmodel_fatal.load_state_dict(sd['state_dict'])","99acfae9":"input2 = train.pivot(index='Date', columns=\"geo\", values='Fatalities').iloc[-win:,:].values\n\nfor i in range(period):\n    \n    ins = torch.tensor(input2[-win:, :]).cuda()\n    add_cuda = torch.tensor([add_d]).cuda()\n    pred = model_fatal(ins.unsqueeze(dim=0).float(),add_cuda.float())\n    \n    input2 = np.concatenate([input2, np.array(pred.detach().cpu().numpy(), dtype=np.int).reshape(1, train_fatal.shape[1])], axis=0)","4a8ad711":"pred_size = (np.array(test_last_date, dtype='datetime64[D]').astype(np.int64) - np.array(test_first_date, dtype='datetime64[D]').astype(np.int64))+1","a8ed1229":"pd.DataFrame(np.array(input.cumsum(0) + base_1, dtype=np.int)[-pred_size:,:], columns=train.pivot(index='Date', columns=\"geo\", values='ConfirmedCases').columns).loc[:, ['US_New York', 'Ukraine', 'Italy', 'Spain']]","52bda60a":"pd.DataFrame(np.array(input2.cumsum(0) + base_2, dtype=np.int)[-pred_size:,:], columns=train.pivot(index='Date', columns=\"geo\", values='ConfirmedCases').columns).loc[:, ['US_New York', 'Ukraine', 'Italy', 'Spain']]","68b02be4":"input = input.cumsum(0) + base_1\ninput2 = input2.cumsum(0) + base_2","75ff2f78":"import datetime \n\ndef prov(i):\n    try:\n        return i.split(\"_\")[1]\n    except:\n        return None\n\nres = pd.DataFrame(input2[-pred_size:,:], columns=train.pivot(index='Date', columns=\"geo\", values='Fatalities').columns).unstack().reset_index(name='Fatalities') \\\n    .merge(\n    pd.DataFrame(input[-pred_size:,:], columns=train.pivot(index='Date', columns=\"geo\", values='ConfirmedCases').columns).unstack().reset_index(name='ConfirmedCases'),\n          how='left', on=['geo', 'level_1']\n)\n\nres['Date'] = [test.Date[0] + datetime.timedelta(days=i) for i in res['level_1']]\nres['Province_State'] = [prov(i) for i in res['geo']]\nres['Country_Region'] = [i.split(\"_\")[0] for i in res['geo']]\n\nres","bfbabd97":"sub = pd.read_csv(f\"\/kaggle\/input\/covid19-global-forecasting-week-{week}\/submission.csv\")\n\nsub = test.merge(res, how='left', on=['Date', 'Province_State', 'Country_Region']).loc[:, [\"ForecastId\", \"ConfirmedCases\", \"Fatalities\"]]\nsub['Fatalities'] = np.where(sub['Fatalities']*7 > sub[\"ConfirmedCases\"], sub[\"ConfirmedCases\"] \/ 7, sub['Fatalities']) \nsub[\"ConfirmedCases\"] = np.where(((sub['Fatalities'] \/ sub[\"ConfirmedCases\"]) < 0.0005) & (sub[\"ConfirmedCases\"] > 1000), sub['Fatalities']*2000, sub[\"ConfirmedCases\"])\n\nsub['Fatalities'] = np.array(sub['Fatalities'], dtype=np.int)\nsub[\"ConfirmedCases\"] = np.array(sub[\"ConfirmedCases\"], dtype=np.int)\n\nsub","30ee3638":"sub.to_csv(\"submission.csv\", index=False)","afd967c4":"I use only new cases and new death's dynamics to make the prediction.","3c66154d":"We need to install pytorch lightning library:","d275c628":"## New fatal cases prediction","350f3c2a":"## New confirmed cases prediction","18b705e3":"Adjust results and create submission:","a735ac3f":"Load libraries:","00241514":"Data loader:","33234cdb":"Write prediction:","14d52478":"Define week number:","46c635a4":"## Forecast preparation","8a913f2f":"Select only March and April:","876e170d":"Data window for forecast:","9e1448f3":"## DSANet approach\n\nSolution based on these implementations [1](https:\/\/github.com\/bighuang624\/DSANet) & [2](https:\/\/www.kaggle.com\/kirichenko17roman\/old-dsanet-approach) of [Dual Self-Attention Network for Multivariate Time Series Forecasting](https:\/\/dl.acm.org\/doi\/10.1145\/3357384.3358132)\n\nAdditional information about the contries is added before the dense layer.\n\nNN architecture:\n\n![](https:\/\/raw.githubusercontent.com\/bighuang624\/DSANet\/master\/docs\/DSANet-model-structure.png)","68a5aefd":"## Model","f8a11184":"Loss function:","5297d0c6":"Convert predicted new cases to total cases:"}}