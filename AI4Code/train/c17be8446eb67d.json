{"cell_type":{"1728d88b":"code","4c07f226":"code","f7ebec76":"code","6942a11a":"code","fdbbdfc1":"code","ba4b2e4b":"code","2e573012":"code","ca48e4b0":"code","b46783e3":"code","ac6b375e":"code","e534e018":"code","97f4424e":"code","9507f421":"code","c32056a9":"code","c2b277ff":"code","30f08bf2":"code","d2aa8c71":"code","a4133394":"code","1fc5f930":"code","af4a069f":"code","4814c69c":"code","6b1c605c":"code","eb92b1fd":"code","33759b89":"code","2fee678d":"code","aac93c15":"code","033cf94c":"code","c723ef63":"code","ff59b418":"code","174272a5":"code","8aee41b7":"code","951cb460":"code","98348080":"code","75cd8a06":"code","6c68babe":"code","7c4e80e5":"code","22b24193":"code","24750d4a":"code","02048173":"code","03058e5c":"code","2e1620c7":"code","18a899d9":"code","6fa1d443":"code","f580f739":"code","b322d6bd":"code","86b4151a":"code","ea2ff341":"code","418a96a8":"code","f9f6154e":"markdown","5e9e966f":"markdown","0559d829":"markdown","80e45c2b":"markdown","bd30b87b":"markdown","37cf3832":"markdown"},"source":{"1728d88b":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport category_encoders as ce\nimport optuna\nimport warnings\n\nfrom sklearn.linear_model import Ridge\nfrom sklearn.feature_selection import mutual_info_regression\nfrom sklearn.model_selection import cross_val_score, cross_validate, KFold\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_squared_log_error\nfrom sklearn.metrics import make_scorer\nfrom sklearn.preprocessing import MinMaxScaler, StandardScaler, RobustScaler\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.utils import shuffle\n\nfrom catboost import CatBoostRegressor, Pool\nfrom xgboost import XGBRegressor\n\nimport eli5\nfrom eli5.sklearn import PermutationImportance\n\n# Set Matplotlib defaults\nplt.rc(\"figure\", autolayout=True, figsize=(10, 7))\nplt.rc(\n    \"axes\",\n    labelweight=\"bold\",\n    labelsize=\"large\",\n    titleweight=\"bold\",\n    titlesize=14,\n    titlepad=10,\n)\n\n# Mute warnings\nwarnings.filterwarnings('ignore')","4c07f226":"def log_log_RMSLE(y_true, y_pred):\n    \"\"\"\n    The Root Mean Squared Log Error (RMSLE) metric \u0434\u043b\u044f \u0441\u043b\u0443\u0447\u0430\u044f \u043f\u0440\u0435\u0434\u0441\u043a\u0430\u0437\u0430\u043d\u0438\u044f \u043b\u043e\u0433\u0430\u0440\u0438\u0444\u043c\u0430 \u043b\u043e\u0433\u0430\u0440\u0438\u0444\u043c\u043e\u043c\n\n    :param y_true: The ground truth labels given in the dataset\n    :param y_pred: Our predictions\n\n    :return: The RMSLE score between exp(y_true) and exp(y_pred)\n    \"\"\"\n    return np.sqrt(mean_squared_log_error(np.exp(y_true), np.exp(y_pred)))","f7ebec76":"def validate(model, X, y):\n    with warnings.catch_warnings():\n        warnings.simplefilter(\"ignore\")\n        res = cross_val_score(model, X, y, scoring=make_scorer(log_log_RMSLE, greater_is_better=True), cv=5, verbose=1)\n    return res, np.mean(res), np.median(res)","6942a11a":"def validate_default(cat_features, X, y):\n    default_model = CatBoostRegressor(logging_level=\"Silent\", cat_features=cat_features, random_state=42)\n    return validate(default_model, X, y)","fdbbdfc1":"def plot_correlations(df):\n    f = plt.figure(figsize=(13, 9))\n    plt.matshow(df.corr(), fignum=f.number)\n    plt.xticks(range(df.select_dtypes(['number']).shape[1]), df.select_dtypes(['number']).columns, fontsize=14, rotation=45)\n    plt.yticks(range(df.select_dtypes(['number']).shape[1]), df.select_dtypes(['number']).columns, fontsize=14)\n    cb = plt.colorbar()\n    cb.ax.tick_params(labelsize=14)\n    plt.title('Correlation Matrix', fontsize=16);","ba4b2e4b":"def make_mi_scores(X, y):\n    X = X.copy()\n    for colname in X.select_dtypes([\"object\", \"category\"]):\n        X[colname], _ = X[colname].factorize()\n    # All discrete features should now have integer dtypes\n    discrete_features = [pd.api.types.is_integer_dtype(t) for t in X.dtypes]\n    mi_scores = mutual_info_regression(X, y, discrete_features=discrete_features, random_state=0)\n    mi_scores = pd.Series(mi_scores, name=\"MI Scores\", index=X.columns)\n    mi_scores = mi_scores.sort_values(ascending=False)\n    return mi_scores\n\n\ndef plot_mi_scores(scores):\n    scores = scores.sort_values(ascending=True)\n    width = np.arange(len(scores))\n    ticks = list(scores.index)\n    plt.barh(width, scores)\n    plt.yticks(width, ticks)\n    plt.title(\"Mutual Information Scores\")","2e573012":"class CrossFoldEncoder:\n    def __init__(self, encoder, **kwargs):\n        self.encoder_ = encoder\n        self.kwargs_ = kwargs  # keyword arguments for the encoder\n        self.cv_ = KFold(n_splits=5)\n\n    # Fit an encoder on one split and transform the feature on the\n    # other. Iterating over the splits in all folds gives a complete\n    # transformation. We also now have one trained encoder on each\n    # fold.\n    def fit_transform(self, X, y, cols):\n        self.fitted_encoders_ = []\n        self.cols_ = cols\n        X_encoded = []\n        for idx_encode, idx_train in self.cv_.split(X):\n            fitted_encoder = self.encoder_(cols=cols, **self.kwargs_)\n            fitted_encoder.fit(\n                X.iloc[idx_encode, :], y.iloc[idx_encode],\n            )\n            X_encoded.append(fitted_encoder.transform(X.iloc[idx_train, :])[cols])\n            self.fitted_encoders_.append(fitted_encoder)\n        X_encoded = pd.concat(X_encoded)\n        X_encoded.columns = [name for name in X_encoded.columns]\n        return X_encoded\n\n    # To transform the test data, average the encodings learned from\n    # each fold.\n    def transform(self, X):\n        from functools import reduce\n\n        X_encoded_list = []\n        for fitted_encoder in self.fitted_encoders_:\n            X_encoded = fitted_encoder.transform(X)\n            X_encoded_list.append(X_encoded[self.cols_])\n        X_encoded = reduce(\n            lambda x, y: x.add(y, fill_value=0), X_encoded_list\n        ) \/ len(X_encoded_list)\n        X_encoded.columns = [name for name in X_encoded.columns]\n        return X_encoded","ca48e4b0":"train = pd.read_csv(\"..\/input\/ef-msu-autumn-2\/train.csv\")\ntest = pd.read_csv(\"..\/input\/ef-msu-autumn-2\/test.csv\")","b46783e3":"train, test = shuffle(train, random_state=42).reset_index(drop=True), shuffle(test, random_state=42).reset_index(drop=True)\n\ntest_ids = test[\"id\"]\ntrain, test = train.drop([\"id\"], axis=1), test.drop([\"id\"], axis=1)","ac6b375e":"cat_features = [\"cat{}\".format(i) for i in range(1, 117)]\nnum_features = [\"cont{}\".format(i) for i in range(1, 15)]\ntarget = train.pop(\"loss\")","e534e018":"target = np.log(target)","97f4424e":"mi_scores = make_mi_scores(train, target)\n\nmi_scores.max(), mi_scores.median(), mi_scores.min()","9507f421":"len(mi_scores), len(mi_scores[mi_scores > 0.0])","c32056a9":"plot_mi_scores(mi_scores[:30])","c2b277ff":"def drop_uninformative(df, mi_scores, threshold):\n    return df.loc[:, mi_scores > threshold]","30f08bf2":"train_0_0 = drop_uninformative(train, mi_scores, 0.0)\ntrain_0_0_cats = [col for col in train_0_0.columns if train_0_0.dtypes[col] == \"object\"]\ntrain_0_0_nums = [col for col in train_0_0.columns if train_0_0.dtypes[col] == \"float64\"]","d2aa8c71":"plot_correlations(train_0_0[train_0_0_nums])","a4133394":"cleaned_train = train_0_0.drop(columns=\"cont12\")\ncleaned_train_cats = [col for col in cleaned_train.columns if cleaned_train.dtypes[col] == \"object\"]\ncleaned_train_nums = [col for col in cleaned_train.columns if cleaned_train.dtypes[col] == \"float64\"]","1fc5f930":"cleaned_test = test[cleaned_train_cats + cleaned_train_nums]","af4a069f":"validate_default(cleaned_train_cats, cleaned_train, target)","4814c69c":"cardinality = cleaned_train[cleaned_train_cats].nunique().sort_values()\nlow_cardinality = [col for col in cardinality.index if cardinality[col] <= 2]\nmedium_cardinality = [col for col in cardinality.index if 3 <= cardinality[col] <= 22]\nhigh_cardinality = [col for col in cardinality.index if cardinality[col] > 22]","6b1c605c":"oh_encoder = ce.OneHotEncoder(cols=low_cardinality)\ncb_encoder = CrossFoldEncoder(ce.CatBoostEncoder)\ntarget_encoder_m1 = CrossFoldEncoder(ce.MEstimateEncoder, m=1)\ntarget_encoder_m5 = CrossFoldEncoder(ce.MEstimateEncoder, m=5)","eb92b1fd":"def encode3(X_train, X_test, y, low_cardinality, medium_cardinality, high_cardinality):\n    X_tf, X_tf_t = X_train.copy(), X_test.copy()\n    \n    oh_encoder = ce.OneHotEncoder(cols=low_cardinality + medium_cardinality)\n    oh_encoder.fit(X_tf)\n    X_tf_low = oh_encoder.transform(X_tf)\n    X_tf_low = X_tf_low[[col for col in X_tf_low.columns if col not in high_cardinality]]\n    X_tf_t_low = oh_encoder.transform(X_tf_t)\n    X_tf_t_low = X_tf_t_low[[col for col in X_tf_t_low.columns if col not in high_cardinality]]\n    \n    target_encoder_m5 = CrossFoldEncoder(ce.CatBoostEncoder, a=5)\n    X_tf_high, X_tf_t_high = target_encoder_m5.fit_transform(X_tf, y, cols=high_cardinality), target_encoder_m5.transform(X_tf_t)\n    \n    return X_tf_low.join(X_tf_high), X_tf_t_low.join(X_tf_t_high)","33759b89":"encoded_train, encoded_test = encode3(cleaned_train, cleaned_test, target, low_cardinality, medium_cardinality, high_cardinality)","2fee678d":"X_tr, X_val, y_tr, y_val = train_test_split(encoded_train, target, random_state=42)","aac93c15":"min_max_scaler = MinMaxScaler()\nstandard_scaler = StandardScaler()","033cf94c":"xgb1 = XGBRegressor()","c723ef63":"validate(xgb1, encoded_train, target)","ff59b418":"def xgb_objective(trial):\n    xgb_params = dict(\n        max_depth=trial.suggest_int(\"max_depth\", 2, 10),\n        learning_rate=trial.suggest_float(\"learning_rate\", 1e-4, 1e-1, log=True),\n        n_estimators=trial.suggest_int(\"n_estimators\", 500, 5000),\n        min_child_weight=trial.suggest_int(\"min_child_weight\", 1, 10),\n        colsample_bytree=trial.suggest_float(\"colsample_bytree\", 0.2, 1.0),\n        subsample=trial.suggest_float(\"subsample\", 0.2, 1.0),\n        reg_alpha=trial.suggest_float(\"reg_alpha\", 1e-4, 1e2, log=True),\n        reg_lambda=trial.suggest_float(\"reg_lambda\", 1e-4, 1e2, log=True),\n    )\n    xgb_reg = XGBRegressor(**xgb_params)\n    xgb_reg.fit(X_tr, y_tr)\n    return log_log_RMSLE(y_val, xgb_reg.predict(X_val))\n\nstudy = optuna.create_study(direction=\"minimize\")\nstudy.optimize(xgb_objective, n_trials=50)\nxgb_params = study.best_params","174272a5":"xgb_best_params = {'max_depth': 8, 'learning_rate': 0.009523408608230264, 'n_estimators': 2549, 'min_child_weight': 7,\n                   'colsample_bytree': 0.29941687924016985, 'subsample': 0.6340226464244659, 'reg_alpha': 3.2366195499670596,\n                   'reg_lambda': 2.9297852698314104}\nxgb_reg = XGBRegressor(**xgb_best_params)","8aee41b7":"validate(xgb_reg, encoded_train, target)","951cb460":"xgb_reg.fit(encoded_train, target)","98348080":"pred_test = np.exp(xgb_reg.predict(encoded_test))","75cd8a06":"submission = pd.DataFrame()\nsubmission[\"id\"] = test_ids\nsubmission[\"loss\"] = pred_test","6c68babe":"submission.sort_values(by=\"id\").reset_index(drop=True).to_csv(\"xgb_sub.csv\", index=False)","7c4e80e5":"X_tr_cb, X_val_cb, y_tr_cb, y_val_cb = train_test_split(cleaned_train, target, random_state=42)","22b24193":"tr_pool = Pool(X_tr_cb, y_tr_cb, cat_features=cleaned_train_cats)","24750d4a":"def cb_objective(trial):\n    cb_params = dict(\n        depth=trial.suggest_int(\"depth\", 7, 12),\n        n_estimators=trial.suggest_int(\"n_estimators\", 3000, 8000),\n        l2_leaf_reg=trial.suggest_float(\"reg_alpha\", 0, 50),\n        random_strength=trial.suggest_int('random_strength', 0, 100),                       \n        bagging_temperature=trial.suggest_float(\"bagging_temperature\", 0, 10)\n    )\n    cb_reg = CatBoostRegressor(**cb_params, cat_features=cleaned_train_cats, logging_level=\"Silent\", random_state=42)\n    cb_reg.fit(tr_pool)\n    return log_log_RMSLE(y_val_cb, cb_reg.predict(X_val_cb))\n\nstudy = optuna.create_study(direction=\"minimize\")\nstudy.optimize(cb_objective, n_trials=50)\ncb_params = study.best_params","02048173":"cb_best_params = {'depth': 10, 'n_estimators': 4183, 'l2_leaf_reg': 18.8449866493728}\ncb_reg = CatBoostRegressor(**cb_best_params, cat_features=cleaned_train_cats, logging_level=\"Silent\", random_state=42)","03058e5c":"cb_reg.fit(cleaned_train, target)","2e1620c7":"pred_test = np.exp(cb_reg.predict(cleaned_test))","18a899d9":"submission = pd.DataFrame()\nsubmission[\"id\"] = test_ids\nsubmission[\"loss\"] = pred_test","6fa1d443":"submission.sort_values(by=\"id\").reset_index(drop=True).to_csv(\"cb_sub.csv\", index=False)","f580f739":"ridge1 = make_pipeline(standard_scaler, Ridge(random_state=42))","b322d6bd":"validate(ridge1, encoded_train, target)","86b4151a":"def ridge_objective(trial):\n    alpha = trial.suggest_float(\"alpha\", 0, 40)\n    intercept = trial.suggest_categorical(\"fit_intercept\", [True, False])\n    tol = trial.suggest_float(\"tol\", 0.001, 0.01, log=True)\n    solver = trial.suggest_categorical(\"solver\", [\"auto\", \"svd\",\"cholesky\", \"lsqr\", \"saga\", \"sag\"])\n    scaler = trial.suggest_categorical(\"scaler\", [\"standard\", \"min_max\"])\n    \n    if scaler == \"standard\":\n        regressor = make_pipeline(standard_scaler,\n                                  Ridge(alpha=alpha, fit_intercept=intercept, tol=tol, solver=solver, random_state=42))\n    elif scaler == \"min_max\":\n        regressor = make_pipeline(min_max_scaler,\n                                  Ridge(alpha=alpha, fit_intercept=intercept, tol=tol, solver=solver, random_state=42))\n\n    regressor.fit(X_tr, y_tr)\n\n    return log_log_RMSLE(y_val, regressor.predict(X_val))\n\nstudy = optuna.create_study(direction=\"minimize\")\nstudy.optimize(ridge_objective, n_trials=50)\nridge_params = study.best_params","ea2ff341":"ridge_best_params = {'alpha': 10.276104617216598, 'fit_intercept': True, 'tol': 0.009998214597037276,\n                     'solver': 'saga'}\nridge_reg = make_pipeline(min_max_scaler, Ridge(**ridge_best_params, random_state=42))","418a96a8":"validate(ridge_reg, encoded_train, target)","f9f6154e":"## \u041b\u0443\u0447\u0448\u0438\u0439 \u0441\u0430\u0431\u043c\u0438\u0442 \u0431\u044b\u043b \u043f\u043e\u043b\u0443\u0447\u0435\u043d \u043a\u043e\u043c\u0431\u0438\u043d\u0438\u0440\u043e\u0432\u0430\u043d\u0438\u0435\u043c \u043f\u0440\u0435\u0434\u0441\u043a\u0430\u0437\u0430\u043d\u0438\u0439 `xgboost` \u0438 `catboost` \u0441 \u0432\u0435\u0441\u0430\u043c\u0438 \u043f\u043e 0.5","5e9e966f":"### \u041e\u0442\u0431\u0440\u043e\u0441\u0438\u043c \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u0438, \u0432\u0437\u0430\u0438\u043c\u043d\u0430\u044f \u0438\u043d\u0444\u043e\u0440\u043c\u0430\u0446\u0438\u044f \u043a\u043e\u0442\u043e\u0440\u044b\u0445 \u0441 \u0442\u0430\u0440\u0433\u0435\u0442\u043e\u043c - \u043d\u0443\u043b\u0435\u0432\u0430\u044f","0559d829":"# \u0418\u043c\u043f\u043e\u0440\u0442 \u043c\u043e\u0434\u0443\u043b\u0435\u0439","80e45c2b":"# \u041c\u043e\u0434\u0435\u043b\u0438","bd30b87b":"# \u0420\u0430\u0431\u043e\u0442\u0430 \u0441 \u0434\u0430\u043d\u043d\u044b\u043c\u0438","37cf3832":"# Utitlity functions"}}