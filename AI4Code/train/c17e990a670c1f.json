{"cell_type":{"9edcfa25":"code","3850b4f8":"code","3414217e":"code","69f15037":"code","fcbb49f4":"code","c1b09b7d":"code","45ad7555":"code","edc4b1a3":"code","fdbf873d":"code","d669da47":"code","6bc73bf4":"code","09d85b38":"code","e2db818a":"code","05cab4f8":"code","774319d1":"code","7fce9f23":"code","d662eb79":"code","f23a96aa":"code","7dadc246":"code","5491984e":"code","ff7253e1":"code","2a58806d":"code","26fb6ccb":"code","a7ae2117":"code","9bb2ff28":"code","993c7822":"code","a66ecc0a":"code","38c0b478":"code","807dde80":"code","309ec4e5":"code","ad0ddfca":"code","1514933f":"code","a38d077a":"code","ab020901":"code","d38a8233":"code","8741b16f":"code","2d25ce42":"markdown","ed412914":"markdown","7b4223bf":"markdown","446fcfff":"markdown","362e404e":"markdown","52c803ee":"markdown","e4ca733a":"markdown","1eada555":"markdown","8ddd688b":"markdown","f410de6d":"markdown","acefbf50":"markdown","f72ca1f1":"markdown","73652b82":"markdown","87f4ff5f":"markdown","452e9709":"markdown","785c69cf":"markdown","4588f679":"markdown","6614d9d6":"markdown","5a4a82a8":"markdown","108afdce":"markdown","157b1c7c":"markdown","710c76d7":"markdown","b50d8841":"markdown","fa8ccbf8":"markdown"},"source":{"9edcfa25":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","3850b4f8":"data =  pd.read_csv(\"..\/input\/regression-with-neural-networking\/concrete_data.csv\")\ndata.head()","3414217e":"\ndata.columns=['Cement',\n       'Blast_Furnace_Slag',\n       'Fly_Ash',\n       'Water',\n       'Superplasticizer',\n       'Coarse_Aggregate',\n       'Fine_Aggregate', 'Age',\n       'Concrete_compressive_strength']","69f15037":"data.info()","fcbb49f4":"data.isnull().sum()","c1b09b7d":"data['Water_Cement_ratio']=data['Water']\/data['Cement']","45ad7555":"plt.figure(figsize=(12,8))\nsns.heatmap(round(data.describe()[1:].transpose(),2),linewidth=2,annot=True,fmt=\"f\",cmap=\"YlGnBu\")\nplt.xticks(fontsize=20)\nplt.yticks(fontsize=12)\nplt.title(\"Variables summary\")\nplt.show()\n","edc4b1a3":"# Distplot\nfig, ax2 = plt.subplots(3, 3, figsize=(16, 16))\nsns.distplot(data['Cement'],ax=ax2[0][0])\nsns.distplot(data['Blast_Furnace_Slag'],ax=ax2[0][1])\nsns.distplot(data['Fly_Ash'],ax=ax2[0][2])\nsns.distplot(data['Water'],ax=ax2[1][0])\nsns.distplot(data['Superplasticizer'],ax=ax2[1][1])\nsns.distplot(data['Coarse_Aggregate'],ax=ax2[1][2])\nsns.distplot(data['Fine_Aggregate'],ax=ax2[2][0])\nsns.distplot(data['Age'],ax=ax2[2][1])\nsns.distplot(data['Concrete_compressive_strength'],ax=ax2[2][2])","fdbf873d":"\nfrom scipy.stats import skew\nnumerical_features = data.select_dtypes(include=[np.number]).columns\ncategorical_features = data.select_dtypes(include=[np.object]).columns\nskew_values = skew(data[numerical_features], nan_policy = 'omit')\ndummy = pd.concat([pd.DataFrame(list(numerical_features), columns=['Features']), \n           pd.DataFrame(list(skew_values), columns=['Skewness degree'])], axis = 1)\ndummy.sort_values(by = 'Skewness degree' , ascending = False)","d669da47":"data.groupby(\"Age\").mean()","6bc73bf4":"sns.pairplot(data)","09d85b38":"fig, ax2 = plt.subplots(2,4, figsize=(20, 10))\nsns.regplot('Concrete_compressive_strength','Cement',data=data,ax=ax2[0][0])\nsns.regplot('Concrete_compressive_strength','Blast_Furnace_Slag',data=data,ax=ax2[0][1])\nsns.regplot('Concrete_compressive_strength','Fly_Ash',data=data,ax=ax2[0][2])\nsns.regplot('Concrete_compressive_strength','Water',data=data,ax=ax2[0][3])\nsns.regplot('Concrete_compressive_strength','Superplasticizer',data=data,ax=ax2[1][0])\nsns.regplot('Concrete_compressive_strength','Coarse_Aggregate',data=data,ax=ax2[1][1])\nsns.regplot('Concrete_compressive_strength','Fine_Aggregate',data=data,ax=ax2[1][2])\nsns.regplot('Concrete_compressive_strength','Age',data=data,ax=ax2[1][3])","e2db818a":"data=data[['Superplasticizer',\n       'Coarse_Aggregate', 'Fine_Aggregate', 'Age', 'Water_Cement_ratio',\n       'Blast_Furnace_Slag', 'Fly_Ash',\n       'Concrete_compressive_strength']]","05cab4f8":"cor = data.corr()\n\nmask = np.zeros_like(cor)\nmask[np.triu_indices_from(mask)] = True\n\nplt.figure(figsize=(12,10))\n\nwith sns.axes_style(\"white\"):\n    sns.heatmap(cor,annot=True,linewidth=2,\n                mask = mask,cmap=\"YlGnBu\")\nplt.title(\"Correlation between variables\")\nplt.show()","774319d1":"from scipy import stats\noutlier_list=[]\nfor c in data.columns[:-1]:\n    Q1=data[c].quantile(q=0.25)\n    Q3=data[c].quantile(q=0.75)\n    print (\"***************************************************************************\")\n    print('OUTLIER DETECTION FOR',c.upper())\n    print (\"***************************************************************************\")\n    \n    print('1st Quartile (Q1) is: ', Q1)\n    print('3st Quartile (Q3) is: ', Q3)\n    print('Interquartile range (IQR) is ', stats.iqr(data[c]))\n    L_outliers=Q1-1.5*(Q3-Q1)\n    U_outliers=Q3+1.5*(Q3-Q1)\n    print('Lower outliers in',c, L_outliers)\n    print('Upper outliers in ',c, U_outliers)\n    print (\"***************************************************************************\")\n    print('Number of outliers in',c, 'upper : ', data[data[c]>U_outliers][c].count())\n    print('Number of outliers in',c,' lower : ', data[data[c]<L_outliers][c].count())\n    print('% of Outlier in ',c,' upper: ',round(data[data[c]>U_outliers][c].count()*100\/len(data)), '%')\n    print('% of Outlier in ',c,' lower: ',round(data[data[c]<L_outliers][c].count()*100\/len(data)), '%')\n    print (\"***************************************************************************\")\n    print(data[  (data[c] < L_outliers) | (data[c] > U_outliers)  ].index)\n    outlier_list.extend(data[  (data[c] < L_outliers) | (data[c] > U_outliers)  ].index)\n    print('\\n')","7fce9f23":"data.loc[list(set(outlier_list))]","d662eb79":"data_outlier=data.drop(outlier_list,axis=0).reset_index(drop = True)","f23a96aa":"# Input\/independent variables\nX = data.drop('Concrete_compressive_strength', axis = 1)   # here we are droping the output feature as this is the target and 'X' is input features, the changes are not \n                                              # made inplace as we have not used 'inplace = True'\ny = data['Concrete_compressive_strength'] ","7dadc246":"from sklearn.model_selection import  train_test_split, cross_val_score\nimport warnings\nwarnings.filterwarnings(\"ignore\", category=DeprecationWarning)","5491984e":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.25,random_state=2)","ff7253e1":"from sklearn.metrics import mean_squared_error\nfrom sklearn import metrics\nfrom scipy.stats import pearsonr\nwarnings.filterwarnings(\"ignore\")\n#sns.set(style=\"darkgrid\", color_codes=True) \n\ntarget = \"Concrete_compressive_strength\"\ndef model(algorithm,dtrainx,dtrainy,dtestx,dtesty,of_type,plot=False):\n    \n    print (algorithm)\n    print (\"***************************************************************************\")\n    algorithm.fit(dtrainx,dtrainy)\n    \n    #print(algorithm.get_params(deep=True))\n    \n    prediction = algorithm.predict(dtestx)\n    \n    print (\"ROOT MEAN SQUARED ERROR :\", np.sqrt(mean_squared_error(dtesty,prediction)) )\n    print (\"***************************************************************************\")\n    \n    print ('Performance on training data :', algorithm.score(dtrainx,dtrainy)*100)\n    print ('Performance on testing data :', algorithm.score(dtestx,dtesty)*100)\n\n    print (\"***************************************************************************\")\n    if plot==True:\n        sns.jointplot(x=dtesty, y=prediction, stat_func=pearsonr,kind=\"reg\", color=\"k\") \n    \n       \n    prediction = pd.DataFrame(prediction)\n    cross_val = cross_val_score(algorithm,dtrainx,dtrainy,cv=10)#,scoring=\"neg_mean_squared_error\"\n    cross_val = cross_val.ravel()\n    print (\"CROSS VALIDATION SCORE\")\n    print (\"************************\")\n    print (\"cv-mean :\",cross_val.mean()*100)\n    print (\"cv-std  :\",cross_val.std()*100)\n    \n    if plot==True:\n        plt.figure(figsize=(20,22))\n        plt.subplot(211)\n\n        testy = dtesty.reset_index()[\"Concrete_compressive_strength\"]\n\n        ax = testy.plot(label=\"originals\",figsize=(20,9),linewidth=2)\n        ax = prediction[0].plot(label = \"predictions\",figsize=(20,9),linewidth=2)\n        plt.legend(loc=\"best\")\n        plt.title(\"ORIGINALS VS PREDICTIONS\")\n        plt.xlabel(\"index\")\n        plt.ylabel(\"values\")\n        ax.set_facecolor(\"k\")\n\n        plt.subplot(212)\n\n        if of_type == \"coef\":\n            coef = pd.DataFrame(algorithm.coef_.ravel())\n            coef[\"feat\"] = dtrainx.columns\n            ax1 = sns.barplot(coef[\"feat\"],coef[0],palette=\"jet_r\",\n                              linewidth=2,edgecolor=\"k\"*coef[\"feat\"].nunique())\n            ax1.set_facecolor(\"lightgrey\")\n            ax1.axhline(0,color=\"k\",linewidth=2)\n            plt.ylabel(\"coefficients\")\n            plt.xlabel(\"features\")\n            plt.title('FEATURE IMPORTANCES')\n\n        elif of_type == \"feat\":\n            coef = pd.DataFrame(algorithm.feature_importances_)\n            coef[\"feat\"] = dtrainx.columns\n            ax2 = sns.barplot(coef[\"feat\"],coef[0],palette=\"jet_r\",\n                              linewidth=2,edgecolor=\"k\"*coef[\"feat\"].nunique())\n            ax2.set_facecolor(\"lightgrey\")\n            ax2.axhline(0,color=\"k\",linewidth=2)\n            plt.ylabel(\"coefficients\")\n            plt.xlabel(\"features\")\n            plt.title('FEATURE IMPORTANCES')\n","2a58806d":"import xgboost as xgb\nfrom xgboost.sklearn import XGBRegressor\nxgr =XGBRegressor(random_state=2)\n#model(xgr,X_train_or,y_train_or,X_test_or,y_test_or,\"feat\")\nmodel(xgr,X_train,y_train,X_test,y_test,\"feat\")","26fb6ccb":"xgr_1=XGBRegressor(random_state=2,learning_rate = 0.2,\n                max_depth = 2, n_estimators = 800,n_jobs=-1,reg_alpha=0.005,gamma=0.1,subsample=0.7,colsample_bytree=0.9, colsample_bylevel=0.9, colsample_bynode=0.9)\nmodel(xgr_1,X_train,y_train,X_test,y_test,\"feat\",True)","a7ae2117":"from sklearn.model_selection import GridSearchCV\nfrom sklearn.metrics import accuracy_score\nparam_grid={'n_estimators' : [500,800,1000,1200],\n            'max_depth' : [1,2, 3,5,7,9,10,11,15],\n            'learning_rate' :[ 0.0001, 0.001, 0.01, 0.1, 0.15, 0.2, 0.8, 1.0],\n                                                     }\n# Create a base model\nxgbr = XGBRegressor(random_state = 2,reg_alpha=0.005,gamma=0.1,subsample=0.7,colsample_bytree=0.9, colsample_bylevel=0.9, colsample_bynode=0.9)\n\n# Instantiate the grid search model\ngrid_search = GridSearchCV(estimator = xgbr, param_grid = param_grid, \n                          cv = 3, n_jobs = -1, verbose = 2)","9bb2ff28":"grid_search.fit(X_train, y_train)","993c7822":"print(grid_search.best_params_)\nbest_grid = grid_search.best_estimator_\nmodel(best_grid,X_train,y_train,X_test,y_test,\"feat\",True)","a66ecc0a":"from sklearn.ensemble import BaggingRegressor\nregr = BaggingRegressor(base_estimator=xgr_1,\n                         n_estimators=400, random_state=2,n_jobs=-1).fit(X_train, y_train)\npred=regr.predict(X_test)\nprint('Root Mean Squared Error is: ', np.sqrt(mean_squared_error(y_test, pred)))","38c0b478":"regr_1 = BaggingRegressor(base_estimator=best_grid,\n                         n_estimators=400, random_state=2,n_jobs=-1).fit(X_train, y_train)\npred=regr_1.predict(X_test)\nprint('Root Mean Squared Error is: ', np.sqrt(mean_squared_error(y_test, pred)))","807dde80":"from sklearn.ensemble import  GradientBoostingRegressor\ngbr = GradientBoostingRegressor(alpha=0.9, criterion='friedman_mse', init=None,\n             learning_rate=0.1, loss='ls', max_depth=4, max_features=None,\n             max_leaf_nodes=None, min_impurity_decrease=0.0,\n             min_impurity_split=None, min_samples_leaf=1,\n             min_samples_split=2, min_weight_fraction_leaf=0.0,\n             n_estimators=800, n_iter_no_change=None, presort='auto',\n             random_state=2, subsample=1.0, tol=0.0001,\n             validation_fraction=0.1, verbose=0, warm_start=False)\nmodel(gbr,X_train,y_train,X_test,y_test,\"feat\",True)","309ec4e5":"from sklearn.ensemble import  RandomForestRegressor\nrf = RandomForestRegressor(bootstrap=True, criterion='mse', max_depth=80,\n           max_features='auto', max_leaf_nodes=None,\n           min_impurity_decrease=0.0, min_impurity_split=None,\n           min_samples_leaf=1, min_samples_split=2,\n           min_weight_fraction_leaf=0.0, n_estimators=400, n_jobs=-1,\n           oob_score=False, random_state=None, verbose=0, warm_start=False)\nmodel(rf,X_train,y_train,X_test,y_test,\"feat\")","ad0ddfca":"from sklearn.tree import DecisionTreeRegressor\ndtr = DecisionTreeRegressor()\nmodel(dtr,X_train,y_train,X_test,y_test,\"feat\")","1514933f":"from sklearn.ensemble import ExtraTreesRegressor\netr = ExtraTreesRegressor()\nmodel(etr,X_train,y_train,X_test,y_test,\"feat\")","a38d077a":"from sklearn.ensemble import AdaBoostRegressor\nadb = AdaBoostRegressor()\nmodel(adb,X_train,y_train,X_test,y_test,\"feat\")","ab020901":"### Neural Network\nfrom sklearn.preprocessing import StandardScaler\nsc = StandardScaler()\n\n# Building ANN As a Regressor\nimport keras\nfrom keras.models import Sequential\nfrom keras.layers import Dense\nfrom keras.layers.normalization import BatchNormalization\nfrom keras import backend\n\n#Defining Root Mean Square Error As our Metric Function \ndef rmse(y_true, y_pred):\n    return backend.sqrt(backend.mean(backend.square(y_pred - y_true), axis=-1))\n\n# Initialising the ANN\nmodel_nn = Sequential()\n\n# Adding the input layer and the first hidden layer\nmodel_nn.add(Dense(512, activation = 'relu', input_dim = 7))\nmodel_nn.add(BatchNormalization())\n# Adding the second hidden layer\nmodel_nn.add(Dense(units = 256, activation = 'relu'))\nmodel_nn.add(BatchNormalization())\n# Adding the third hidden layer\nmodel_nn.add(Dense(units = 128, activation = 'relu'))\nmodel_nn.add(BatchNormalization())\nmodel_nn.add(Dense(units = 32, activation = 'relu'))\nmodel_nn.add(BatchNormalization())\n# Adding the output layer\nmodel_nn.add(Dense(units = 1))\n\n# Optimize , Compile And Train The Model \nopt =keras.optimizers.Adam(lr=0.0015)\n#print(model_nn.summary())\nmodel_nn.compile(optimizer=opt,loss='mean_squared_error',metrics=[rmse])","d38a8233":"import tensorflow as tf\ncheckpoint_filepath ='best.hdf5'\nmodel_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n    filepath=checkpoint_filepath,\n    save_weights_only=True,\n    monitor='val_rmse',\n    mode='min',\n    save_best_only=True)\n\n# Model weights are saved at the end of every epoch, if it's the best seen\n# so far.\nhistory=model_nn.fit(sc.fit_transform(X_train),y_train,epochs = 100 ,batch_size=32,validation_data=(sc.transform(X_test), y_test), callbacks=[model_checkpoint_callback])\n\n# The model weights (that are considered the best) are loaded into the model.\nmodel_nn.load_weights(checkpoint_filepath)","8741b16f":"# Predicting and Finding R Squared Score\ny_predict = model_nn.predict(sc.transform(X_test))\nprint('Root Mean Squared Error is: ', np.sqrt(mean_squared_error(y_test, y_predict))) \n\nplt.figure(figsize=(20,5))\nplt.plot(list(y_test) ,color = 'red', label = 'Real data',marker='o')\nplt.plot(y_predict, color = 'blue', label = 'Predicted data',marker='o')\nplt.title('Prediction')\nplt.legend()\nplt.show()\n\n# Plotting Loss And Root Mean Square Error For both Training And Test Sets\nplt.plot(history.history['rmse'])\nplt.plot(history.history['val_rmse'])\nplt.title('Root Mean Squared Error')\nplt.ylabel('rmse')\nplt.xlabel('epoch')\nplt.legend(['train', 'test'], loc='upper left')\nplt.show()\n# summarize history for loss\nplt.plot(history.history['loss'])\nplt.plot(history.history['val_loss'])\nplt.title('model loss')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.legend(['train', 'test'], loc='upper left')\nplt.show()","2d25ce42":"**Key Insights**\n* cement is almost normal.\n* slag has two\/three gausssians and rightly skewed.\n* ash has two gaussians and rightly skewed.\n* water has three guassians and slighly left skewed.\n* superplastic has two gaussians and rightly skewed.\n* coarseagg has three guassians and almost normal.\n* fineagg has almost two guassians and looks like normal.\n* age has multiple guassians and rightly skewed.","ed412914":"# Neural Network","7b4223bf":"### Hyperparameter Tuning","446fcfff":"### Visualize pairplot","362e404e":"# Histogram of the complete dataset","52c803ee":"# <h1 align=\"center\"> Compressive Strength of Concrete Analysis<\/h1> \n\n### Prepared by- [Shantanil Bagchi](https:\/\/www.linkedin.com\/in\/shantanilbagchi\/) ([Github Repo](https:\/\/github.com\/ShantanilBagchi\/Hackathons_Notebooks\/tree\/master\/Compressive_Strength_of_Concrete_Study))\n\n## Objective\nCompressive strength or compression strength is the capacity of a material or structure to withstand loads tending to reduce size, as opposed to tensile strength, which withstands loads tending to elongate.\n\ncompressive strength is one of the most important engineering properties of concrete. It is a standard industrial practice that the concrete is classified based on grades. This grade is nothing but the Compressive Strength of the concrete cube or cylinder. Cube or Cylinder samples are usually tested under a compression testing machine to obtain the compressive strength of concrete. The test requisites differ country to country based on the design code.\n\nThe concrete compressive strength is a highly nonlinear function of age and ingredients .These ingredients include cement, blast furnace slag, fly ash, water, superplasticizer, coarse aggregate, and fine aggregate.\n\nThe actual concrete compressive strength (MPa) for a given mixture under a specific age (days) was determined from laboratory. Data is in raw form (not scaled).\n\nThe concrete compressive strength is a highly nonlinear function of age and ingredients. These ingredients include cement, blast furnace slag, fly ash, water, superplasticizer, coarse aggregate, and fine aggregate. Our objective is to build a machine learning model that would help Civil Engineers to estimate the compressive strength of the concrete and they can further take a decision whether the concrete should be used in their current project or not.\n\n## Dataset Summary\n\n| Component           | Variable Type \n| :-------------       |:-------------:|\n| Cement              | Input Variable |\n| Blast Furnace Slag  | Input Variable|\n| Fly Ash             | Input Variable|\n| Water               | Input Variable|\n| Superplasticizer    | Input Variable|\n| Coarse Aggregate    | Input Variable|\n| Fine Aggregate      | Input Variable|\n| Age                 | Input Variable|\n| **Concrete compressive strength** | **Output Variable** |","e4ca733a":"### Decision Tree Regressor","1eada555":"### Extra Trees Regressor","8ddd688b":"# Using Bagging Technique","f410de6d":"# Algo Selection","acefbf50":"# Creating Interaction Terms","f72ca1f1":"### Data Info and Missing Value ","73652b82":"### Further Analysis","87f4ff5f":"# XGBoost Regresssor","452e9709":"## Models\n\n| Model                       | RMSE    | Accuracy     |\n| :--------------------------- |:-------:| :----------: |\n| Gradient Boosting Regressor | 4.23       | 93        |\n| Random Forest Regressor     | 5.06    | 90        |\n| Decision Tree Regressor     | 6.52    | 83.5        |\n| Extra Trees Regressor       | 4.80    | 91        |\n| AdaBoost Regressor          | 8    | 75        |\n| **XGBoost Regressor**       | 4.06    | 93.62        |\n| Deep Neural Network         | 4.63    |            |\n| **Bagging Regressor (estimator= grid_searched XGBoost)**       | 4.17    | 93.24        |\n\n\n***Note-***\n* Outlier detection had been done but resulted in comparatively poor performance.\n* New feature engineering i.e water\/binder ratio introduced but didn't result in improved performance.\n* Columns (Fly Ash, Coarse Agg, Fine Agg) were removed to check performance but didn't do well.\n\nFollwing is the result for reference wrt **XGBoost** (not included in the notebook)\n\n| Detail                                            | RMSE(Whole) | Test Acc |\n| :-------------                                    | :----------:|:--------:|\n|  X_original                                             | 5.04  |  90.78 |\n|  X_without_outliers                                     | 5.06  | 90.37  |\n|  X_with_columns_removed (Fly Ash, Coarse Agg, Fine Agg) | 5.08  |  90.61 |\n|  X_feature_engineered_with_water_cement                 | 4.52  | 91.69  |\n|  X_feature_engineered (without Water, Cement)           | 4.06  |  93.62 |\n\n**Further Analysis can be done to improve performance**","785c69cf":"### **Concrete Compressive Strength comparision independent attributes**\n\n- **Strength vs Cement**: It is linearly related to the cement. Although the relationship is positive, for a given value of cement we have a multiple values of strength. Hence, it is not a very good predictor.\n- **Strength vs Slag and Fly Ash**: There is no particular trend as a lot of values are zero.\n- **Strength vs Age**: For a given value of age, we have different values of strength. Hence, it is not a very good predictor.\n- **Strength vs Superplasticizer**:For a given value of age, we have different values of strength with a lot of vaues being zero. Hence, it is not a good predictor.\n- Other attributes do not give any strong relationship with Strength. \n\nHence, we can see that none of the independent attributes are a good predictors of the strength attribute. So, we will not use Linear model. \n\n**Thus, an interaction term has been created earlier i.e. water\/cement ratio which has inverse relation with Strength. Moreover, Water and Cement Columns are dropped as their relationship has already been captured in the interaction term**","4588f679":"### Data Summary","6614d9d6":"### Dropping Water and Cement Columns (After trial and error) See notes in the top","5a4a82a8":"### AdaBoost Regressor","108afdce":"### Gradient Boosting Regressor","157b1c7c":"# Outlier Detection","710c76d7":"# Skewness Degree","b50d8841":"### Random Forest Regressor","fa8ccbf8":"# Load the Dataset and view few rows"}}