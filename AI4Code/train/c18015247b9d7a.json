{"cell_type":{"52526d60":"code","99f086af":"code","080764b7":"code","f82e8401":"code","675c394e":"code","cfcfc7a5":"code","872f1a15":"code","19e3e89c":"code","c558dedf":"code","62bc53dd":"code","c47d2cbb":"code","4119cd07":"code","aa1fb6e3":"code","8fa7bd7d":"code","3fd04374":"code","7cce30ea":"code","fe7bcc05":"code","d29f57c1":"code","eb401799":"code","caba494d":"code","22701372":"code","3dd0a8db":"code","4e38e7a9":"code","9936f593":"code","feee4102":"code","f12de4a3":"code","8127722b":"code","9f582591":"code","cd1675c0":"code","8c154f6b":"code","1c6024f6":"code","88bce8f0":"code","33f62a5f":"code","48e9fcd6":"code","6d0ebbd7":"code","2f1a407a":"code","8778920d":"code","4f063afb":"code","bd9b6234":"code","6d025ae6":"code","d4a00f43":"code","31d9029c":"code","b4db25d5":"code","173eb6fd":"code","780e097a":"code","6d0a4373":"code","4d596388":"code","7badec09":"code","c02cd81a":"code","eb3c4e76":"code","b845d716":"code","b91fae03":"code","7cbc693f":"code","d6c6dce4":"code","5718845a":"code","0611e214":"code","f7223c73":"code","26d84cc3":"code","dfc1acf0":"code","48cf8d2f":"code","a8f3bfeb":"code","ed4460b1":"code","14846e53":"code","b9e8b942":"code","7390f90d":"code","63180188":"code","300ac788":"code","2b362ef9":"code","c1fb9dc7":"code","67b525a6":"code","38d282bd":"code","2100da86":"code","5579bcbb":"code","bc70ce33":"code","a17cdad0":"code","52b5babc":"code","a6aa69b7":"code","287da839":"code","20e2276c":"code","849cde6e":"code","14263a68":"code","f7fcc052":"code","bdd80848":"code","fc412736":"code","9a3f2378":"code","8844b634":"code","006a56d3":"code","d19315a4":"code","86dcb4ab":"code","aae6d97a":"code","0f3e62f1":"code","85ab1b71":"code","cb5ef081":"code","73931139":"code","6d51b4a1":"code","e6960b31":"code","62223a4c":"code","cbb53535":"code","7beab946":"code","7f4ca4e9":"code","78635984":"code","306fcbc0":"code","b8c817d8":"code","145055a4":"code","6715a88e":"code","99fc3cd2":"code","696adbdb":"code","a1acb015":"code","3660092d":"code","61b8f45c":"code","a25dc3ed":"code","847993a9":"code","dd840f8f":"markdown","6e968898":"markdown","01afb3c3":"markdown","ca4083e4":"markdown","e21ec515":"markdown","7682dffe":"markdown","383daedc":"markdown","51b7e9bb":"markdown","498f3e9f":"markdown","fc01e577":"markdown","94febf9e":"markdown","165473e0":"markdown","be1452b8":"markdown","b9bc2905":"markdown","f4e6fd22":"markdown","d7b0dd82":"markdown","88b4eeda":"markdown","13502975":"markdown","316e5493":"markdown","eb80a7b7":"markdown","c26a7602":"markdown","bc45875c":"markdown","6964031d":"markdown","930ba5ce":"markdown","27be14e2":"markdown","31e707bf":"markdown","a0c2577f":"markdown","e56e3c29":"markdown","81fa6947":"markdown","4a8f9783":"markdown","0656fc47":"markdown","e711c044":"markdown","d74fbb0f":"markdown","aff7597a":"markdown"},"source":{"52526d60":"# Importing required libraries\n \nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# For Feature Engineering\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.compose import ColumnTransformer\n    # for integer encoding using sklearn\nfrom sklearn.preprocessing import LabelEncoder\n\n# Feature Selection\n    # Chi-Square\nfrom sklearn.feature_selection import chi2\nfrom sklearn.feature_selection import SelectKBest, SelectPercentile\n    #mutual information\nfrom sklearn.feature_selection import mutual_info_classif\n    #ANOVA\nfrom sklearn.feature_selection import f_classif\n\n\n# For Model\n    # To split the data set\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import cross_val_score\n\n\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier\n\n# Evaluation\nfrom sklearn.metrics import classification_report,confusion_matrix\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.metrics import r2_score\nfrom sklearn.inspection import permutation_importance\n\n\nfrom sklearn.pipeline import Pipeline\n\n# HyperParameter\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.model_selection import RandomizedSearchCV\n\n# Avoid Warning\nimport warnings\nwarnings.filterwarnings(\"ignore\")","99f086af":"df_test=pd.read_csv('..\/input\/titanic\/test.csv')\ndf_sub= pd.read_csv('..\/input\/titanic\/gender_submission.csv')\ndf=pd.read_csv('..\/input\/titanic\/train.csv')\ndf.head() # will show 1st 5 row of dataframe by default we can put any no.","080764b7":"# Test dataframe\ndf_test.head(3) # here we want to see top rows of df_test 3 ","f82e8401":"# Sub Dataframe\ndf_sub.head(3)","675c394e":"df.shape # will show dimension of dataframe","cfcfc7a5":"type(df.shape)","872f1a15":"# Calculate the total no of data point\n\n# Since df.shape returns tuple we can easily store value in variable \na,b=df.shape\nprint('Total data points = ', a*b)","19e3e89c":"# Show columns name\n\ndf.columns","c558dedf":"df.describe()","62bc53dd":"df.info()","c47d2cbb":"# Checking for NaN Values in data frame\n\ndf.isnull().sum()","4119cd07":"# We can graphically also check null values in df\n# By Default yticklabels=True thus causing conjusted view \n\nsns.heatmap(df.isnull(), yticklabels=False);\n","aa1fb6e3":"# Check missing values in percentage\ndf.isnull().mean()*100","8fa7bd7d":"plt.figure(figsize=(15,18))\n\nplt.subplot(3,2,1)\nsns.countplot(x='Sex', hue='Survived', data=df, palette='RdBu_r')\n#plt.title(\"Countplot of Sex vs Survived, fig:1\")\nplt.subplot(3,2,2)\nsns.countplot(x='Pclass', hue='Survived', data=df)\nplt.subplot(3,2,3)\nsns.countplot(x='Embarked', hue='Survived', data=df)\nplt.subplot(3,2,4)\n#ns.countplot(x='Embarked', data=df)\nsns.boxplot(x='Pclass', y='Age',hue='Survived', data=df)\nplt.subplot(3,2,5)\nsns.scatterplot(x='Fare', y='Age',hue='Survived', data=df)\nplt.subplot(3,2,6)\nsns.countplot(x='SibSp',hue='Survived', data=df)\n\nplt.tight_layout();\n\n","3fd04374":"sns.jointplot(x='Age', y='Fare', data=df, palette='Set1');","7cce30ea":"import cufflinks as cf\ncf.go_offline()\n\n\n#plt.subplot(1,2,1)\n#sns.displot(df['Age'], kde=True)\ndf['Age'].iplot(kind='hist', bins=40, color='blue', title=\"Age Zoomable hist plot\")\n\n\n\n\n#plt.subplot(1,2,2)\ndf['Fare'].iplot(kind='hist', bins=35, title=\"Fare Zoomable hist plot\")\n","fe7bcc05":"plt.figure(figsize=(8,6))\nsns.countplot(x='Parch',hue='Survived', data=df);\n","d29f57c1":"df[df['Pclass']==1].Ticket","eb401799":"df[df['Pclass']==2].Ticket","caba494d":"df[df['Pclass']==3].Ticket.unique()","22701372":"df[(df['Embarked']=='C')& (df['Pclass']==1)].Ticket","3dd0a8db":"df[(df['Embarked']=='Q')& (df['Pclass']==1)].Ticket","4e38e7a9":"df[(df['Embarked']=='S')& (df['Pclass']==1)].Ticket","9936f593":"plt.figure(figsize=(8,8))\nsns.countplot('Embarked', hue='Pclass',data=df, palette=\"Set2\");","feee4102":"df.Embarked.unique()","f12de4a3":"df.dropna(subset=['Embarked'], inplace=True)","8127722b":"# Encode Sex, Embarked\n# Sex is binanry\n# Embarked is non binanry\n\n# we will use Ordinal\/Label\/Integers encoding\n# Benefit of label encoding\n # 1. Staright forward to implement\n    # 2 Does not expand the feature space\n# Limitation\n# not suitable for linear model , doesn't handle new categories in test set automatically\n\nle= LabelEncoder()\n'''le= LabelEncoder()\nle.fit(['male', 'female'])\n#le.fit(df['Embarked'])\nle.transform(df.Sex)'''\n\n# Encode labels in column 'species'.\ndf['Sex']= le.fit_transform(df['Sex'])\ndf['Embarked']= le.fit_transform(df['Embarked'])","9f582591":"df.head(3)","cd1675c0":"# Dropping features with high nullity\n\n# Dropping Cabin column \n# inplace=True > makes the change permananet \n# other wise .drop creates a copy of dataframe and thus the change is temparory\n\ndf.drop(columns={'Cabin','Name','Ticket'}, axis=1, inplace=True)","8c154f6b":"# Imputing Missing data in this case : age \n\n# Here basically we will impute data as per passanger class mean\n\ndef missing_data(cols):\n    Age=cols[0]\n    Pclass= cols[1]\n    #Sex=cols[2]\n    if pd.isnull(Age):\n        if Pclass==1:\n            return 37\n        elif Pclass==2:\n            return 29\n        else:\n            return 24\n    else:\n        return Age","1c6024f6":"# Dry run above function\nd=df\n\nd['Age']=d[['Age','Pclass']].apply(missing_data, axis=1)\nd.head(5)","88bce8f0":"# dryrun\n# 0 represent female\n# 1 represent male \n\nprint('Median age of 1st class female passanger',df[(df['Pclass']==1) & (df['Sex']==0)].Age.median())\nprint('Median age of 2nd class female passanger',df[(df['Pclass']==2) & (df['Sex']==0)].Age.median())\nprint('Median age of 3rd class female passanger',df[(df['Pclass']==3) & (df['Sex']==0)].Age.median())\nprint()\nprint('Median age of 1st class Male passanger',df[(df['Pclass']==1) & (df['Sex']==1)].Age.median())\nprint('Median age of 2nd class Male passanger',df[(df['Pclass']==2) & (df['Sex']==1)].Age.median())\nprint('Median age of 3rd class Male passanger',df[(df['Pclass']==3) & (df['Sex']==1)].Age.median())\n","33f62a5f":"# simple way to replace nan with median\n\n'''med= int(df.Age.median())\ndf['Age'].fillna(med, inplace=True)''';","48e9fcd6":"mid_fare = df.groupby(['Pclass', 'Parch', 'SibSp']).Fare.median()[3][0][0]\nmid_fare","6d0ebbd7":"# Adding a new feature of Family Count\ndf['FamilyCount'] = df.SibSp + df.Parch \n\n# Now visualizing family count\nsns.countplot(df['FamilyCount']);","2f1a407a":"# Feature engineering for name","8778920d":"# Creating a function for feature engineering on test data\n'''\ndef feature_engineering(dataframe):\n    dataframe['Sex']= le.fit_transform(dataframe['Sex'])\n    dataframe['Embarked']= le.fit_transform(dataframe['Embarked'])\n    dataframe.drop(columns={'Cabin','Ticket'}, axis=1, inplace=True)\n    ''';","4f063afb":"sns.pairplot(df, hue='Survived');","bd9b6234":"plt.figure(figsize=(8,6))\nsns.heatmap(df.corr(), annot=True);","6d025ae6":"# only select categorical columns\nfeatures= ['Pclass', 'Sex', 'Age', 'SibSp', 'Parch','Fare', 'Embarked']\n\n# Add 'FamilyCount' in 2nd version of Notebook","d4a00f43":"X_train, X_test, y_train, y_test= train_test_split(df[['Pclass', 'Sex', 'Age', 'SibSp', 'Parch','Fare', 'Embarked']], df['Survived'], test_size=0.3, random_state=7)","31d9029c":"X_train.head(3)","b4db25d5":"from sklearn.feature_selection import mutual_info_classif\n\nY= y_train\nX = X_train\nmi =mutual_info_classif(X,Y)\n\n# Plotting a Bar Graph to compare the models\nplt.bar(X.columns, mi)\nplt.xlabel('Feature Labels')\nplt.ylabel('Feature Importances')\nplt.xticks(rotation=90)\nplt.title('Comparison of different Feature Importances')\nplt.show()","173eb6fd":"from sklearn.ensemble import ExtraTreesClassifier\ny = y_train\nX = X_train\n# Building the model\nextra_tree_forest = ExtraTreesClassifier(n_estimators = 5,criterion ='entropy')\n  \n# Training the model\nextra_tree_forest.fit(X, y)\n  \n# Computing the importance of each feature\nfeature_importance = extra_tree_forest.feature_importances_\n  \n# Normalizing the individual importances\nfeature_importance_normalized = np.std([tree.feature_importances_ for tree in \n                                        extra_tree_forest.estimators_],\n                                        axis = 0)\n\n# Plotting a Bar Graph to compare the models\nplt.bar(X.columns, feature_importance_normalized)\nplt.xlabel('Feature Labels')\nplt.ylabel('Feature Importances')\nplt.xticks(rotation=90)\nplt.title('Comparison of different Feature Importances')\nplt.show()","780e097a":"## Perform chi2 test\n### chi2 returns 2 values\n### Fscore and the pvalue\n\nf_p_score=chi2(X_train, y_train)\n\nf_p_score","6d0a4373":"# 1) let's capture the p_values (in the second array, remember python indexes at 0) in a pandas Series\n# 2) add the variable names in the index\n# 3) order the variables based on their fscore\n\npvalues = pd.Series(f_p_score[1])\npvalues.index = X_train.columns\npvalues.sort_values(ascending=True)","4d596388":"# K= n; n represent top n features \n\nsel_ = SelectKBest(chi2, k=3).fit(X_train, y_train)\n\n# display features\nX_train.columns[sel_.get_support()]","7badec09":"## Hyperparameter optimization using RandomizedSearchCV\nfrom sklearn.model_selection import RandomizedSearchCV, GridSearchCV","c02cd81a":"model_dec = DecisionTreeClassifier(criterion = 'entropy', random_state = 7)","eb3c4e76":"model_dec.fit(X_train,y_train)","b845d716":"pred_dec= model_dec.predict(X_test)","b91fae03":"print(confusion_matrix(y_test, pred_dec))\nprint()\nprint(classification_report(y_test,pred_dec))\n\nprint()\nrmse = (np.sqrt(mean_squared_error(y_test, pred_dec)))\nr2 = r2_score(y_test, pred_dec)\nprint(\"Testing performance\")\nprint(\"RMSE: {:.5f}\".format(rmse))\nprint(\"R2: {:.5f}\".format(r2))\n\n'''\npred= model_dec.predict(test)\nsub.target = preds\nsub.to_csv(\"submission_dec.csv\", index=False)\n''';","7cbc693f":"model_lr = LogisticRegression(solver='liblinear')\n","d6c6dce4":"model_lr.fit(X_train,y_train)\n","5718845a":"pred_lr= model_lr.predict(X_test)","0611e214":"# Evaluating our model\n\nprint(model_lr.score(X_test, y_test))\nprint()\nprint(classification_report(y_test, pred_lr))","f7223c73":"#Using GridSearchCV\ntuned_parameters = [{'C': [10**-4, 10**-2, 10**0, 10**2, 10**4]}]\n\nmodel_lrh = GridSearchCV(LogisticRegression(penalty='l2'), tuned_parameters, scoring = 'roc_auc', cv=5)\nmodel_lrh.fit(X_train, y_train)\n\n\n\nprint(model_lrh.best_estimator_)\nprint(model_lrh.score(X_test, y_test))\nprint()\nprint()\npred_lrh = model_lrh.predict(X_test)\nprint(classification_report(y_test, pred_lrh))","26d84cc3":"score=cross_val_score(model_lrh,X_train,y_train,cv=10)\nscore","dfc1acf0":"score.mean()","48cf8d2f":"model3 = SVC()\nmodel3.fit(X_train,y_train)","a8f3bfeb":"pred_svm= model3.predict(X_test)","ed4460b1":"print(confusion_matrix(y_test, pred_svm))\nprint()\nprint(classification_report(y_test,pred_svm))","14846e53":"param_grid = {'C':[0.1,1,10,100,1000], 'gamma': [1,0.1,0.001,0.0001,0.5]}","b9e8b942":"grid = GridSearchCV(SVC(),param_grid, verbose=1)\n\n# Verbose : text output of description of process","7390f90d":"grid.fit(X_train,y_train)","63180188":"# Show the best parameters \ngrid.best_params_","300ac788":"# Best estimator and best score\ngrid.best_estimator_","2b362ef9":"grid_pred_svm= grid.predict(X_test)","c1fb9dc7":"print(confusion_matrix(y_test, grid_pred_svm))\nprint()\nprint(classification_report(y_test,grid_pred_svm))","67b525a6":"model_rf = RandomForestClassifier()","38d282bd":"model_rf.fit(X_train, y_train)\npred_rf = model_rf.predict(X_test)","2100da86":"print(confusion_matrix(y_test, pred_rf))\nprint()\nprint(classification_report(y_test,pred_rf))","5579bcbb":"# Number of trees in random forest\nn_estimators = [int(x) for x in np.linspace(start = 200, stop = 2000, num = 10)]\n\n# Number of features to consider at every split\nmax_features = ['auto', 'sqrt']\n# Maximum number of levels in tree\nmax_depth = [int(x) for x in np.linspace(10, 110, num = 11)]\nmax_depth.append(None)\n# Minimum number of samples required to split a node\nmin_samples_split = [2, 5, 10]\n# Minimum number of samples required at each leaf node\nmin_samples_leaf = [1, 2, 4]\n# Method of selecting samples for training each tree\nbootstrap = [True, False]\n\n# Create the random grid\nrandom_grid = {'n_estimators': n_estimators,\n               'max_features': max_features,\n               'max_depth': max_depth,\n               'min_samples_split': min_samples_split,\n               'min_samples_leaf': min_samples_leaf,\n               'bootstrap': bootstrap}","bc70ce33":"# Use the random grid to search for best hyperparameters\n# First create the base model to tune\n# Already created model4\n\n# Random search of parameters, using 3 fold cross validation, \n# search across 100 different combinations, and use all available cores\nrf_random = RandomizedSearchCV(estimator = model_rf, param_distributions = random_grid, n_iter = 100, cv = 3, verbose=2, random_state=7, n_jobs = -1)\n\n","a17cdad0":"# Fit the random search model\nrf_random.fit(X_train, y_train)","52b5babc":"pred_rfh= rf_random.predict(X_test)","a6aa69b7":"print(confusion_matrix(y_test, pred_rfh))\nprint()\nprint(classification_report(y_test,pred_rfh))","287da839":"import xgboost","20e2276c":"\n# fit model no training data\nmodel_xgb = xgboost.XGBClassifier()\nmodel_xgb.fit(X_train, y_train)\nprint(model_xgb.score(X_test,y_test))","849cde6e":"pred_xgb= model_xgb.predict(X_test)\n\nprint(confusion_matrix(y_test, pred_rfh))\nprint()\nprint(classification_report(y_test,pred_rfh))","14263a68":"\n## Hyper Parameter Optimization\n\nparams={\n \"learning_rate\"    : [0.05, 0.10, 0.15, 0.20, 0.25, 0.30] ,\n \"max_depth\"        : [ 4, 5, 6, 8, 10, 12, 15],\n \"min_child_weight\" : [ 1, 3, 5, 7 ],\n \"gamma\"            : [ 0.0, 0.1, 0.2 , 0.3, 0.4 ],\n \"colsample_bytree\" : [ 0.3, 0.4, 0.5 , 0.7 ]\n    \n}","f7fcc052":"# Defining a timer\n\ndef timer(start_time=None):\n    if not start_time:\n        start_time = datetime.now()\n        return start_time\n    elif start_time:\n        thour, temp_sec = divmod((datetime.now() - start_time).total_seconds(), 3600)\n        tmin, tsec = divmod(temp_sec, 60)\n        print('\\n Time taken: %i hours %i minutes and %s seconds.' % (thour, tmin, round(tsec, 2)))","bdd80848":"random_search_xgb=RandomizedSearchCV(model_xgb,param_distributions=params,n_iter=5,scoring='roc_auc',n_jobs=-1,cv=5,verbose=3)","fc412736":"from datetime import datetime\n# Here we go\nstart_time = timer(None) # timing starts from this point for \"start_time\" variable\nrandom_search_xgb.fit(X_train,y_train)\ntimer(start_time) # timing ends here for \"start_time\" variable","9a3f2378":"random_search_xgb.best_estimator_","8844b634":"random_search_xgb.best_params_","006a56d3":"pred_xgbH = random_search_xgb.predict(X_test)\n\nprint(confusion_matrix(y_test, pred_xgbH))\nprint()\nprint(classification_report(y_test,pred_xgbH))","d19315a4":"print(random_search_xgb.score(X_test,y_test))","86dcb4ab":"XGB_Mean_Score= xgboost.XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n              colsample_bynode=1, colsample_bytree=0.5, gamma=0.3, gpu_id=-1,\n              importance_type='gain', interaction_constraints='',\n              learning_rate=0.3, max_delta_step=0, max_depth=10,\n              min_child_weight=3, missing=None, monotone_constraints='()',\n              n_estimators=100, n_jobs=4, num_parallel_tree=1, random_state=0,\n              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,\n              tree_method='exact', validate_parameters=1, verbosity=None)","aae6d97a":"score=cross_val_score(model_xgb,X_train,y_train,cv=10)\nscore","0f3e62f1":"score.mean()","85ab1b71":"import catboost","cb5ef081":"model_cat = catboost.CatBoostClassifier","73931139":"'''model_cat.fit(X_train, y_train)\nprint(model_cat.score(X_train,y_train))''';","6d51b4a1":"def featu(df):\n    df.dropna(subset=['Embarked'], inplace=True)\n    # Encode labels in column 'species'.\n    df['Sex']= le.fit_transform(df['Sex'])\n    df['Embarked']= le.fit_transform(df['Embarked'])\n    df.drop(columns={'Cabin','Name','Ticket'}, axis=1, inplace=True) \n    med= int(df.Age.median())\n    df['Age'].fillna(med, inplace=True)\n    \n    return(df)\n    \n","e6960b31":"df_test.head(3)","62223a4c":"temp= df_test","cbb53535":"test=featu(temp)","7beab946":"test.head(3)","7f4ca4e9":"test.info()","78635984":"test.isnull().sum()","306fcbc0":"# filling NaN of Fare column with median value\n#test.dropna(inplace=True)\n\ntest.Fare.fillna(test.Fare.median(), inplace=True)","b8c817d8":"# test['FamilyCount'] = test.SibSp + test.Parch","145055a4":"test.info()","6715a88e":"test.drop('PassengerId', axis=True,inplace=True)","99fc3cd2":"# test_pred = rf_random.predict(test) 1st sub\ntest_pred =random_search_xgb.predict(test)\ntest_pred_lr= model_lrh.predict(test)","696adbdb":"submission = pd.DataFrame(columns=['PassengerId', 'Survived'])\nsub2 = pd.DataFrame(columns=['PassengerId', 'Survived'])","a1acb015":"print('Dimention of 1st df', submission.shape)\nprint('Dimension of 2nd df', sub2.shape)","3660092d":"submission['PassengerId'] = df_sub['PassengerId']\nsubmission['Survived'] = test_pred\n\n#sub 2\nsub2['PassengerId'] = df_sub['PassengerId']\nsub2['Survived'] = test_pred_lr","61b8f45c":"submission.head()","a25dc3ed":"submission.shape","847993a9":"submission.to_csv('submissions_xgb.csv', header=True, index=False)\n\n# Sub 2\nsub2.to_csv('submissions_LRH.csv', header=True, index=False)","dd840f8f":"\n<p style = \"font-size : 20px; color : white ; font-family : 'Comic Sans MS'; text-align : center; background-color : #339966; border-radius: 5px 5px;\"><strong>In this Notebook we will cover EDA, Feature Engineering, Model Creation ( From Logisitc Regression to CatBoosting, Model Evaluation, and Submission<\/strong><\/p>\n\n\ncontent\n1. [Importing libraries + data & EDA](#1)\n1. [Feature Engineering](#3)\n    * [Handeling missing Values](#4)\n    * [Outlier](#5)\n    * [Encoding](#6)\n1. [Feature Selection](#7)\n    * [Pearson Correlation](#8)\n    * [Chi-Square](#9)\n    * [Mutual Information](#10)\n    * [ANOVA](#11)\n1. [Model Creatition](#12)\n    * [Decision Tree Classification](#17)\n    * [Logistic Regressor](#18)\n    * [SVM with GridSearch](#19)\n    * [Random Forest with RandomSearch](#20)\n1. [Submission](#99)","6e968898":"# Encoding ","01afb3c3":"<a id=11>\n\n# ANOVA","ca4083e4":"### Feature Exploration ","e21ec515":"<p style = \"font-size : 15px; color : black ; font-family : 'Comic Sans MS'; text-align : center; background-color : #33ccff; border-radius: 5px 5px;\"><Strong>HyperParameter of XGBoost<\/Strong><\/p>\n\n\n* **We will use logistic loss function to assess the accuracy of predictions, as this is a classification problem**\n* n_estimators no of boosting trees to fit\n* learning_rate : Gradient decent learning rate\n\n### RandomSearch is much faster than GridSearch","7682dffe":"# Submission","383daedc":"<a id=8>\n    \n    \n##  Chi-Square\n\nChi-square is a statistical test, best suited to determine a difference\nbetween expected frequencies and observed frequencies in 1 or\nmore categories of a contingency table.\n\n\nSuited\n* for categorical variables.\n* Target should be categorical.**i.e must be used for Classification problem only**\n* Variable values should be non-negative, and typically Boolean, frequencies, or counts.\n* It compares observed distribution of class among the different labels against the expected one, would there be no labels.\n\n* **Chi2**: ranks features smallest the p-value biggest importance\n* **SelectKBest**: select best k features\n* **SelectPercentile** : select features in top percentile","51b7e9bb":"<a id=3>\n    \n    \n# Feature Engineering","498f3e9f":"<a id=10>\n\n# Mutual Information\n\nMutual information is a measure of the mutual dependence of 2 variables.  \nIn other words, the mutual information quantifies the \"amount of information\" gained about one random variable through observing the other random variable.","fc01e577":"<p><ul style = \"font-size : 20px; color : white ; font-family : 'Comic Sans MS'; text-align : left; background-color : #33ccff; border-radius: 5px 5px;\"><li>Feature Engineering<\/li><ol style = \"font-size : 15px;\"><li>Family members count<\/li><li>We Will see title of passangers<\/li><\/ol><li> We Will apply more advance machine learning models<\/li><li>Will Use HyperParameters to predict, Which is more CPU intense and will return more accurate prediction<\/li><\/ul><\/p>\n","94febf9e":"## Now in Next update I will improve accuracy of my model, by appling some more feature engineering and better model","165473e0":"<a id=7>\n\n# Fature Selection","be1452b8":"Contrarily to MI, where we were interested in the higher MI values, for the chi2, **the smaller the p_value the more significant the feature is to predict the target.**\n\nThus, from the result above, **Sex is the most important feature, as it has the smallest p-value.**\n\nIn this demo, we used chi2 to determine the predictive value of 3 categorical variables only. If the dataset contained several categorical variables, we could then combine this procedure with SelectKBest or SelectPercentile, as we did in the previous notebook, to select the top k features, or the features in the top n percentile, based on the chi2 p-values.\n\nLet's select the top 3 feature for the demo:","b9bc2905":"## Since Random Forest with Hyperparameter performed the best we will use this to predict on df_test","f4e6fd22":"<p style = \"font-size : 20px; color : white ; font-family : 'Comic Sans MS'; text-align : center; background-color : #339966; border-radius: 5px 5px;\">Cabin has majority of data missing where as age column to have many missing data where as embarked has only 2 missing data<\/p>\n\n**cabin : Cabin number**     I think we need to drop Cabin  \n**embarked : Port of Embarkation, there were 3 port of embarkment > C = Cherbourg, Q = Queenstown, S = Southampton**\nEmbarked means > boarding a ship for a journey   \n**For Age we will use some imputation method**","d7b0dd82":"<a id=99>\n\n# Prepare for Submission","88b4eeda":"<a id=19>\n\n## SVM","13502975":"<a id=12>\n\n\n# Model","316e5493":"<a id=17>\n\n## Decision Tree","eb80a7b7":"<p style = \"font-size : 28px; color : white ; font-family : 'Comic Sans MS'; text-align : center; background-color : #fcba03; border-radius: 5px 5px;\"><strong>Do you know why we are splitting the data set before Feature Selection with filter method ?<\/strong><\/p>\n<p style = \"font-size : 18px; color : Blue ; font-family : 'Comic Sans MS'; text-align : center; background-color : #fcfc03; border-radius: 5px 5px;\">> To avoid data spill.<\/p>","c26a7602":"## Hyperparameter tuning with **Random Search**","bc45875c":"\n<p style = \"font-size : 40px; color : white ; font-family : 'Comic Sans MS'; text-align : center; background-color : #ffff00; border-radius: 5px 5px;\"><strong>Thank You<\/strong><\/p>\n","6964031d":"## CATBoost","930ba5ce":"<p style = \"font-size : 20px; color : white ; font-family : 'Comic Sans MS'; text-align : center; background-color : #339966; border-radius: 5px 5px;\"><strong>U cannot control ur luck but u can control ur focus and effort<\/strong><\/p>","27be14e2":"### Applyinh Grid search on SVM\n\n<p style=\"background-color : #e0ccff;\"> The GridSearchCV expects the estimator which in our case is the SVM. We pass the <i>possible parameter values as param_grid<\/i>, and <i>keep the cross-validation set to 5<\/i>. Setting verbose as 5 outputs a log to the console and <i>njobs as -1 makes the model use all cores on the machine<\/i>. Then, I fit this grid and use it to find the best estimator.<\/p>","31e707bf":"## We observe from fig 6 that most of the single passanagers died. they might be men from 3rd class.","a0c2577f":"<a id=18>\n\n## Logistic Regressor\n    \n* model.fit wasn't running so had to use\n    solver='liblinear'\n  \n[Source](https:\/\/stackoverflow.com\/questions\/65682019\/attributeerror-str-object-has-no-attribute-decode-in-fitting-logistic-regre)","e56e3c29":"## XGBoost","81fa6947":"## we need to exclude Passenger Id while training the model but need to publish data with passanger id","4a8f9783":"<a id=20>\n\n## Random Forest","0656fc47":"# Handling Missing data","e711c044":"<a id=1>\n    \n    \n# Importing libraries + Data & EDA","d74fbb0f":"# Extra Tree Classification","aff7597a":"## If this Notebook was helpful, please Upvote.\n## If u disagree please share ur opinion regarding improvement of Notebook etc\n## U can also comment if u have better approach to solve ml problems"}}