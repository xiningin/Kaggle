{"cell_type":{"89fe5d0d":"code","b2e07441":"code","ea2b627e":"code","1b2bb48d":"code","1358e4df":"code","a605e004":"code","eeb6d051":"code","8885d61a":"code","806e554d":"code","5fd8cf25":"code","58fdb8f2":"code","a99d8293":"code","042c0834":"code","ca3700af":"code","e7c21bfd":"code","e7199589":"code","9730b6cc":"code","0ebbd5f7":"code","cd64030b":"code","378b5868":"code","b5a90dfb":"code","aa764097":"code","6857df99":"code","b1488c06":"code","dc5924c7":"code","804d315a":"markdown"},"source":{"89fe5d0d":"import numpy as np\nimport pandas as pd\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.decomposition import PCA\n#from pca import pca","b2e07441":"df=pd.read_csv('..\/input\/us-crime-dataset\/uscrime.txt',sep=\"\t\")","ea2b627e":"df.head()","1b2bb48d":"df.tail()","1358e4df":"df.info()","a605e004":"df.describe()","eeb6d051":"columns_names=df.columns.tolist()\nprint(\"Columns names:\")\nprint(columns_names)","8885d61a":"df.shape","806e554d":"X=df[columns_names[0:15]]\ncorrelation=X.corr()\ncorrelation","5fd8cf25":"plt.figure(figsize=(10,10))\nsns.heatmap(correlation, vmax=1, square=True,annot=True,cmap='cubehelix')\nplt.title('Correlation between different features')\n\n","58fdb8f2":"mask = np.triu(np.ones_like(correlation))\ndataplot = sns.heatmap(correlation,vmax=1,vmin=-1,square=False, cmap=\"vlag\", mask=mask)","a99d8293":"from sklearn.preprocessing import StandardScaler\nX_std = StandardScaler().fit_transform(X)\nmean_vec = np.mean(X_std, axis=0)\ncov_mat = (X_std - mean_vec).T.dot((X_std - mean_vec)) \/ (X_std.shape[0]-1)\nplt.figure(figsize=(8,8))\ndataplot = sns.heatmap(cov_mat,vmax=1,vmin=-1,square=False, cmap=\"vlag\", mask=mask)\nplt.title('Correlation between different features')","042c0834":"pca = PCA().fit(X_std)\npca.explained_variance_","ca3700af":"pca.explained_variance_ratio_","e7c21bfd":"Cumulative_explained_variance=np.cumsum(pca.explained_variance_ratio_)\nCumulative_explained_variance","e7199589":"plt.plot(pca.explained_variance_ratio_*100)\nplt.xlim(0,15,1)\nplt.xlabel('Dimensions')\nplt.ylabel('percentage of explained variance')\n","9730b6cc":"plt.plot(Cumulative_explained_variance*100)\nplt.xlim(0,7,1)\nplt.xlabel('Number of components')\nplt.ylabel('Cumulative explained variance')\n","0ebbd5f7":"X_train=X_std[0:42]\nX_test=X_std[42:]\nY_train=df['Crime'][0:42]\nY_test=df['Crime'][42:]\nX_test.shape","cd64030b":"pca=PCA(n_components=2)\nX_train_pca =pca.fit_transform(X_train)","378b5868":"print(X_train_pca)","b5a90dfb":"X_test_pca = pca.transform(X_test)","aa764097":"from sklearn.linear_model import LinearRegression","6857df99":"model= LinearRegression()","b1488c06":"model.fit(X_train_pca,Y_train)","dc5924c7":"model.score(X_test_pca,Y_test,)","804d315a":"We will use 3 criteria to choose the number of components we will be keeping.\n1\/ Kaiser criterion: We drop all the components with an eigen value under 1.0 so we keep the 4 first components <br>\n2\/ Elbow method: By identifying a point at which the proportion of variance explained by each subsequent principal component drops off we  keep the 2 first components <br>\n3\/ Cumulative explained variance: We notice that the cumulative explained variance of the first 2 components is 58.8% which is an important rate taking into account the fact that we have 15 variables we will therefore keep the first 2 components <br>\n\n=> we will keep 2 since the majority of criteria have chosen 2"}}