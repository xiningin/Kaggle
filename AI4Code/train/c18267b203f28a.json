{"cell_type":{"123e7d34":"code","33d76338":"code","2d8b4ada":"code","40b78e7f":"code","d4a67dd3":"code","b797c002":"code","ceedcfc9":"code","8eb61913":"code","0037c735":"code","eaeb730a":"code","b17589d0":"code","7329ddab":"code","35599e77":"code","588b8142":"code","456b980f":"code","317e54b6":"code","e10e53c7":"code","cbe4e5b9":"code","eb987e70":"code","a0fb1437":"code","cffd9a16":"code","f83d68b4":"code","dd0e9236":"code","0e8ac642":"code","c411855c":"code","1c2d8759":"code","6a41ba5d":"code","78cc3874":"code","a9ea7d00":"code","4776b24d":"code","781cd054":"code","67a03a57":"code","f9b24f5f":"code","1d5033cf":"code","f91ac419":"code","1fa1e4b5":"code","10064639":"markdown","93459ccf":"markdown","e476aa76":"markdown","0645bbdf":"markdown","966636e3":"markdown","41dc78ec":"markdown","2bde7155":"markdown","5ff7635b":"markdown","73543885":"markdown","4baf3bd0":"markdown","48427603":"markdown","9c8eab7c":"markdown","b875257d":"markdown","74afb3ea":"markdown","f3a9166e":"markdown","b792e4a6":"markdown","dcb8e3ee":"markdown","b409151f":"markdown","0a48abb0":"markdown","ee5edce0":"markdown","8d5dd32c":"markdown","c4149943":"markdown","97cd46ba":"markdown","dbab693e":"markdown"},"source":{"123e7d34":"import math, re, os\nimport tensorflow as tf\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom kaggle_datasets import KaggleDatasets\nfrom tensorflow import keras\nimport tensorflow.keras.backend as K\nfrom functools import partial\nfrom sklearn.model_selection import train_test_split\n\nprint(\"Tensorflow version \" + tf.__version__)","33d76338":"try:\n    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n    print('Device:', tpu.master())\n    tf.config.experimental_connect_to_cluster(tpu)\n    tf.tpu.experimental.initialize_tpu_system(tpu)\n    strategy = tf.distribute.experimental.TPUStrategy(tpu)\nexcept:\n    strategy = tf.distribute.get_strategy()\nprint('Number of replicas:', strategy.num_replicas_in_sync)","2d8b4ada":"AUTOTUNE = tf.data.experimental.AUTOTUNE\nGCS_PATH = KaggleDatasets().get_gcs_path(\"cassava-leaf-disease-classification\") # \u6bd4\u8d5b\u5b98\u65b9\u94fe\u63a5 \nGCS_PATH_MY = KaggleDatasets().get_gcs_path(\"cassava-leaf-tfrecord-512\") # \u81ea\u5df1\u505a\u7684tfrecord\nBATCH_SIZE = 16 * strategy.num_replicas_in_sync  # TPU=16\nIMAGE_SIZE = [512, 512]  # tfrecord\u7684\u56fe\u7247\u5927\u5c0f\nRESIZE_IMAGE_SIZE = [512, 512]  #  \u56fe\u50cf\u589e\u5f3a\u538b\u7f29\u540e\u7684\u5927\u5c0f TPU 512,  GPU 300(\u592a\u5927\u7206\u5185\u5b58)\nCLASSES = ['0', '1', '2', '3', '4']\nEPOCHS = 25","40b78e7f":"def decode_image(image):\n    image = tf.image.decode_jpeg(image, channels=3)\n    image = tf.cast(image, tf.float32)\n    image = tf.reshape(image, [*IMAGE_SIZE, 3])\n    return image","d4a67dd3":"def read_tfrecord(example, labeled):\n    tfrecord_format = {\n        \"image\": tf.io.FixedLenFeature([], tf.string),\n        \"target\": tf.io.FixedLenFeature([], tf.int64)\n    } if labeled else {\n        \"image\": tf.io.FixedLenFeature([], tf.string),\n        \"image_name\": tf.io.FixedLenFeature([], tf.string)\n    }\n    example = tf.io.parse_single_example(example, tfrecord_format)\n    image = decode_image(example['image'])\n    if labeled:\n        label = tf.cast(example['target'], tf.int32)\n        return image, label\n    idnum = example['image_name']\n    return image, idnum","b797c002":"def load_dataset(filenames, labeled=True, ordered=False):\n    ignore_order = tf.data.Options()\n    if not ordered:\n        ignore_order.experimental_deterministic = False # disable order, increase speed\n    dataset = tf.data.TFRecordDataset(filenames, num_parallel_reads=AUTOTUNE) # automatically interleaves reads from multiple files\n    dataset = dataset.with_options(ignore_order) # uses data as soon as it streams in, rather than in its original order\n    dataset = dataset.map(partial(read_tfrecord, labeled=labeled), num_parallel_calls=AUTOTUNE)\n    return dataset","ceedcfc9":"TRAINING_FILENAMES, VALID_FILENAMES = train_test_split(\n#     tf.io.gfile.glob(GCS_PATH_NEW + '\/*.tfrec'),\n    tf.io.gfile.glob(GCS_PATH_MY + '\/train_tfrecords\/*.tfrec'),\n    test_size=0.35, random_state=5\n)\n\nTEST_FILENAMES = tf.io.gfile.glob(GCS_PATH + '\/test_tfrecords\/ld_test*.tfrec')","8eb61913":"def data_augment(image, label):\n    # Thanks to the dataset.prefetch(AUTO) statement in the following function this happens essentially for free on TPU. \n    # Data pipeline code is executed on the \"CPU\" part of the TPU while the TPU itself is computing gradients.\n    image = tf.image.random_flip_left_right(image)\n    # \u518d\u52a0\u4e9b\u589e\u5f3a\n    image = tf.reshape(image, [*IMAGE_SIZE, 3])  # \u8fd9\u91cc\u53bb\u6389\u4e86\u6a21\u578b\u91cc\u7684\u524d\u5904\u7406\u5c42, \u76f4\u63a5\u5728\u8fd9\u91ccreshape\n    if not IMAGE_SIZE == RESIZE_IMAGE_SIZE:\n        image = tf.image.resize(image, RESIZE_IMAGE_SIZE)\n    \n    return image, label","0037c735":"def data_val_augment(image, label):\n    # val\u9a8c\u8bc1\u96c6\u56fe\u7247\u9884\u5904\u7406\n    image = tf.reshape(image, [*IMAGE_SIZE, 3])  # \u8fd9\u91cc\u53bb\u6389\u4e86\u6a21\u578b\u91cc\u7684\u524d\u5904\u7406\u5c42, \u76f4\u63a5\u5728\u8fd9\u91ccreshape\n    if not IMAGE_SIZE == RESIZE_IMAGE_SIZE:\n        image = tf.image.resize(image, RESIZE_IMAGE_SIZE)  \n    return image, label","eaeb730a":"def get_training_dataset():\n    dataset = load_dataset(TRAINING_FILENAMES, labeled=True)  \n    dataset = dataset.map(data_augment, num_parallel_calls=AUTOTUNE)  \n    dataset = dataset.repeat()\n    dataset = dataset.shuffle(2048)\n    dataset = dataset.batch(BATCH_SIZE)\n    dataset = dataset.prefetch(AUTOTUNE)\n    return dataset","b17589d0":"def get_validation_dataset(ordered=False):\n    dataset = load_dataset(VALID_FILENAMES, labeled=True, ordered=ordered) \n    dataset = dataset.map(data_val_augment, num_parallel_calls=AUTOTUNE)  \n    dataset = dataset.batch(BATCH_SIZE)\n    if strategy.num_replicas_in_sync > 1: # TPU has more memory # GPU \u5173,TPU\u5f00\n        dataset = dataset.cache()  \n    dataset = dataset.prefetch(AUTOTUNE)\n    return dataset","7329ddab":"def get_test_dataset(ordered=False):\n    dataset = load_dataset(TEST_FILENAMES, labeled=False, ordered=ordered)\n    dataset = dataset.map(data_val_augment, num_parallel_calls=AUTOTUNE)  \n    dataset = dataset.batch(BATCH_SIZE)\n    dataset = dataset.prefetch(AUTOTUNE)\n    return dataset","35599e77":"def count_data_items(filenames):\n    n = [int(re.compile(r\"-([0-9]*)\\.\").search(filename).group(1)) for filename in filenames]\n    return np.sum(n)","588b8142":"NUM_TRAINING_IMAGES = count_data_items(TRAINING_FILENAMES)\nNUM_VALIDATION_IMAGES = count_data_items(VALID_FILENAMES)\nNUM_TEST_IMAGES = count_data_items(TEST_FILENAMES)\n\nprint('Dataset: {} training images, {} validation images, {} (unlabeled) test images'.format(\n    NUM_TRAINING_IMAGES, NUM_VALIDATION_IMAGES, NUM_TEST_IMAGES))","456b980f":"print(\"Training data shapes:\")\nfor image, label in get_training_dataset().take(3):\n    print(image.numpy().shape, label.numpy().shape)\nprint(\"Training data label examples:\", label.numpy())\nprint(\"Validation data shapes:\")\nfor image, label in get_validation_dataset().take(3):\n    print(image.numpy().shape, label.numpy().shape)\nprint(\"Validation data label examples:\", label.numpy())\nprint(\"Test data shapes:\")\nfor image, idnum in get_test_dataset().take(3):\n    print(image.numpy().shape, idnum.numpy().shape)\nprint(\"Test data IDs:\", idnum.numpy().astype('U')) # U=unicode string","317e54b6":"# numpy and matplotlib defaults\nnp.set_printoptions(threshold=15, linewidth=80)\n\ndef batch_to_numpy_images_and_labels(data):\n    images, labels = data\n    numpy_images = images.numpy().astype(np.int)   # \u8fd9\u91cc\u6ce8\u610f\u4e0b\u8303\u56f4 \u5c0f\u6570[0. , 1.], \u6574\u6570[0,255]\n    numpy_labels = labels.numpy()\n    if numpy_labels.dtype == object: # binary string in this case, these are image ID strings\n        numpy_labels = [None for _ in enumerate(numpy_images)]\n    # If no labels, only image IDs, return None for labels (this is the case for test data)\n    return numpy_images, numpy_labels\n\ndef title_from_label_and_target(label, correct_label):\n    if correct_label is None:\n        return CLASSES[label], True\n    correct = (label == correct_label)\n    return \"{} [{}{}{}]\".format(CLASSES[label], 'OK' if correct else 'NO', u\"\\u2192\" if not correct else '',\n                                CLASSES[correct_label] if not correct else ''), correct\n\ndef display_one_plant(image, title, subplot, red=False, titlesize=16):\n    plt.subplot(*subplot)\n    plt.axis('off')\n    plt.imshow(image)\n    if len(title) > 0:\n        plt.title(title, fontsize=int(titlesize) if not red else int(titlesize\/1.2), color='red' if red else 'black', fontdict={'verticalalignment':'center'}, pad=int(titlesize\/1.5))\n    return (subplot[0], subplot[1], subplot[2]+1)\n\ndef display_batch_of_images(databatch, predictions=None):\n    \"\"\"This will work with:\n    display_batch_of_images(images)\n    display_batch_of_images(images, predictions)\n    display_batch_of_images((images, labels))\n    display_batch_of_images((images, labels), predictions)\n    \"\"\"\n    # data\n    images, labels = batch_to_numpy_images_and_labels(databatch)\n    if labels is None:\n        labels = [None for _ in enumerate(images)]\n        \n    # auto-squaring: this will drop data that does not fit into square or square-ish rectangle\n    rows = int(math.sqrt(len(images)))\n    cols = len(images)\/\/rows\n        \n    # size and spacing\n    FIGSIZE = 13.0\n    SPACING = 0.1\n    subplot=(rows,cols,1)\n    if rows < cols:\n        plt.figure(figsize=(FIGSIZE,FIGSIZE\/cols*rows))\n    else:\n        plt.figure(figsize=(FIGSIZE\/rows*cols,FIGSIZE))\n    \n    # display\n    for i, (image, label) in enumerate(zip(images[:rows*cols], labels[:rows*cols])):\n        title = '' if label is None else CLASSES[label]\n        correct = True\n        if predictions is not None:\n            title, correct = title_from_label_and_target(predictions[i], label)\n        dynamic_titlesize = FIGSIZE*SPACING\/max(rows,cols)*40+3 # magic formula tested to work from 1x1 to 10x10 images\n        subplot = display_one_plant(image, title, subplot, not correct, titlesize=dynamic_titlesize)\n    \n    #layout\n    plt.tight_layout()\n    if label is None and predictions is None:\n        plt.subplots_adjust(wspace=0, hspace=0)\n    else:\n        plt.subplots_adjust(wspace=SPACING, hspace=SPACING)\n    plt.show()","e10e53c7":"# load our training dataset for EDA\ntraining_dataset = get_training_dataset()\ntraining_dataset = training_dataset.unbatch().batch(20)\ntrain_batch = iter(training_dataset)","cbe4e5b9":"# run this cell again for another randomized set of training images\ndisplay_batch_of_images(next(train_batch))","eb987e70":"# load our validation dataset for EDA\nvalidation_dataset = get_validation_dataset()\nvalidation_dataset = validation_dataset.unbatch().batch(20)\nvalid_batch = iter(validation_dataset)","a0fb1437":"# run this cell again for another randomized set of training images\ndisplay_batch_of_images(next(valid_batch))","cffd9a16":"# load our test dataset for EDA\ntesting_dataset = get_test_dataset()\ntesting_dataset = testing_dataset.unbatch().batch(20)\ntest_batch = iter(testing_dataset)","f83d68b4":"# we only have one test image\ndisplay_batch_of_images(next(test_batch))","dd0e9236":"lr_scheduler = keras.optimizers.schedules.ExponentialDecay(\n    initial_learning_rate=1e-5, \n    decay_steps=10000, \n    decay_rate=0.9)","0e8ac642":"# model bases\ndef inceptionv3_base():\n    base_model = tf.keras.applications.InceptionV3(weights='imagenet', include_top=False)\n    preprocess_layer = tf.keras.layers.Lambda(tf.keras.applications.inception_v3.preprocess_input, input_shape=[*IMAGE_SIZE, 3])\n    return base_model, preprocess_layer\n\ndef resnet152v2_base():\n    base_model = tf.keras.applications.ResNet152V2(weights='imagenet', include_top=False)\n    preprocess_layer = tf.keras.layers.Lambda(tf.keras.applications.resnet_v2.preprocess_input, input_shape=[*IMAGE_SIZE, 3])\n    return base_model, preprocess_layer\n\ndef resnet101v2_base():\n    base_model = tf.keras.applications.ResNet101V2(weights='imagenet', include_top=False)\n    preprocess_layer = tf.keras.layers.Lambda(tf.keras.applications.resnet_v2.preprocess_input, input_shape=[*IMAGE_SIZE, 3])\n    return base_model, preprocess_layer\n\ndef resnet50v2_base():\n    base_model = tf.keras.applications.ResNet50V2(weights='imagenet', include_top=False)\n    preprocess_layer = tf.keras.layers.Lambda(tf.keras.applications.resnet_v2.preprocess_input, input_shape=[*IMAGE_SIZE, 3])\n    return base_model, preprocess_layer\n\ndef resnet50_base():\n    base_model = tf.keras.applications.ResNet50(weights='imagenet', include_top=False)\n    preprocess_layer = tf.keras.layers.Lambda(tf.keras.applications.resnet.preprocess_input, input_shape=[*IMAGE_SIZE, 3])\n    return base_model, preprocess_layer\n\ndef resnet101_base():\n    base_model = tf.keras.applications.ResNet101(weights='imagenet', include_top=False)\n    preprocess_layer = tf.keras.layers.Lambda(tf.keras.applications.resnet.preprocess_input, input_shape=[*IMAGE_SIZE, 3])\n    return base_model, preprocess_layer\n\ndef inception_resnet_v2_base():\n    base_model = tf.keras.applications.InceptionResNetV2(weights='imagenet', include_top=False)\n    preprocess_layer = tf.keras.layers.Lambda(tf.keras.applications.inception_resnet_v2.preprocess_input, input_shape=[*IMAGE_SIZE, 3])\n    return base_model, preprocess_layer\n\n# densenet121\u6ca1\u6709\u5fc5\u987b\u7684\u9884\u5904\u7406\u5c42, \u4fdd\u6301\u4ee3\u7801\u4e00\u81f4,\u52a0\u4e0a\u4e86\ndef densenet121_base():\n    base_model = tf.keras.applications.DenseNet121(weights='imagenet', include_top=False)\n    preprocess_layer = tf.keras.layers.Lambda(tf.keras.applications.densenet.preprocess_input, input_shape=[*IMAGE_SIZE, 3])\n    return base_model, preprocess_layer\n\ndef xception_base():\n    base_model = tf.keras.applications.Xception(weights='imagenet', include_top=False)\n    preprocess_layer = tf.keras.layers.Lambda(tf.keras.applications.xception.preprocess_input, input_shape=[*IMAGE_SIZE, 3])\n    return base_model, preprocess_layer\n\ndef vgg16_base():\n    base_model = tf.keras.applications.VGG16(weights='imagenet', include_top=False)\n    preprocess_layer = tf.keras.layers.Lambda(tf.keras.applications.vgg16.preprocess_input, input_shape=[*IMAGE_SIZE, 3])\n    return base_model, preprocess_layer\n\ndef nasnet_large_base():\n    base_model = tf.keras.applications.NASNetLarge(weights='imagenet', include_top=False)\n    preprocess_layer = tf.keras.layers.Lambda(tf.keras.applications.nasnet.preprocess_input, input_shape=[*IMAGE_SIZE, 3])\n    return base_model, preprocess_layer\n\ndef efficientnet_b3_base():\n    base_model = tf.keras.applications.EfficientNetB3(weights='imagenet', include_top=False)\n    preprocess_layer = tf.keras.layers.Lambda(tf.keras.applications.efficientnet.preprocess_input, input_shape=[*RESIZE_IMAGE_SIZE, 3])\n    return base_model, preprocess_layer\n\ndef efficientnet_b0_base():\n    base_model = tf.keras.applications.EfficientNetB0(weights='imagenet', include_top=False)\n    preprocess_layer = tf.keras.layers.Lambda(tf.keras.applications.efficientnet.preprocess_input, input_shape=[*RESIZE_IMAGE_SIZE, 3])\n    return base_model, preprocess_layer","c411855c":"MODEL_NAME = 'VGG16' # InceptionV3 ResNet101V2 ResNet152V2 InceptionResNetV2 DenseNet121 Xception VGG16 ResNet50 ResNet101 NASNetLarge (TPU)\n#  EfficientNetB0 EfficientNetB3 (GPUs)\ndef build_model():\n    with strategy.scope():   \n        base_model ,preprocess_layer = vgg16_base()\n        \n        model = tf.keras.Sequential([\n            preprocess_layer,\n            base_model,\n            tf.keras.layers.GlobalAveragePooling2D(),\n            tf.keras.layers.Dense(len(CLASSES), activation='softmax')  \n        ])\n\n        model.compile(\n            optimizer=tf.keras.optimizers.Adam(learning_rate=lr_scheduler, epsilon=0.001),\n            loss='sparse_categorical_crossentropy',  \n            metrics=['sparse_categorical_accuracy'])\n    return model\n","1c2d8759":"# load data\ntrain_dataset = get_training_dataset()\nvalid_dataset = get_validation_dataset()","6a41ba5d":"STEPS_PER_EPOCH = NUM_TRAINING_IMAGES \/\/ BATCH_SIZE\nVALID_STEPS = NUM_VALIDATION_IMAGES \/\/ BATCH_SIZE\n\nmodel = build_model()\n\nsave_model_callback = tf.keras.callbacks.ModelCheckpoint(\n        \"%s-best-{epoch:02d}-{val_sparse_categorical_accuracy:.4f}.h5\"%(MODEL_NAME), \n        monitor='val_sparse_categorical_accuracy', \n        verbose=0, save_best_only=True,\n        save_weights_only=False, mode='max', save_freq='epoch')\n\nhistory = model.fit(train_dataset, \n                    steps_per_epoch=STEPS_PER_EPOCH, \n                    epochs=EPOCHS,\n                    callbacks = [save_model_callback],\n                    validation_data=valid_dataset,\n                    validation_steps=VALID_STEPS)","78cc3874":"# \u53ea\u4fdd\u7559\u6700\u540e\u4e00\u4e2abest\u6743\u91cd, \u5220\u6389best\u6743\u91cd\u5386\u53f2\u6587\u4ef6\nbest_weights = tf.io.gfile.glob( '*best*.h5')\nbest_weights.sort()\nold_best_weights = best_weights[:-1]\nfor old_best_weight in old_best_weights:\n    tf.io.gfile.remove(old_best_weight)\n    \nbest_weight_path = best_weights[-1]\nprint(best_weight_path)","a9ea7d00":"model.summary()","4776b24d":"model.save('%s-512-last.h5'%(MODEL_NAME))","781cd054":"# print out variables available to us\nprint(history.history.keys())","67a03a57":"# create learning curves to evaluate model performance\nhistory_frame = pd.DataFrame(history.history)\nhistory_frame.loc[:, ['loss', 'val_loss']].plot()\nhistory_frame.loc[:, ['sparse_categorical_accuracy', 'val_sparse_categorical_accuracy']].plot();","f9b24f5f":"# this code will convert our test image data to a float32 \ndef to_float32(image, label):\n    return tf.cast(image, tf.float32), label","1d5033cf":"best_model =  tf.keras.models.load_model(best_weight_path)","f91ac419":"test_ds = get_test_dataset(ordered=True) \ntest_ds = test_ds.map(to_float32)\n\nprint('Computing predictions...')\n# test_images_ds = testing_dataset\ntest_images_ds = test_ds.map(lambda image, idnum: image)\nprobabilities = best_model.predict(test_images_ds)\npredictions = np.argmax(probabilities, axis=-1)\nprint(predictions)","1fa1e4b5":"print('Generating submission.csv file...')\ntest_ids_ds = test_ds.map(lambda image, idnum: idnum).unbatch()\ntest_ids = next(iter(test_ids_ds.batch(NUM_TEST_IMAGES))).numpy().astype('U') # all in one batch\nnp.savetxt('submission.csv', np.rec.fromarrays([test_ids, predictions]), fmt=['%s', '%d'], delimiter=',', \n           header='image_id,label', comments='')\n!head submission.csv","10064639":"## Define data loading methods\nThe following functions will be used to load our `training`, `validation`, and `test` datasets, as well as print out the number of images in each dataset.","93459ccf":"## Adding in augmentations ","e476aa76":"## Decode the data\nIn the code chunk below we'll set up a series of functions that allow us to convert our images into tensors so that we can utilize them in our model. We'll also normalize our data. Our images are using a \"Red, Blue, Green (RBG)\" scale that has a range of [0, 255], and by normalizing it we'll set each pixel's value to a number in the range of [0, 1]. ","0645bbdf":"# Creating a submission file\nNow that we've trained a model and made predictions we're ready to submit to the competition! You can run the following code below to get your submission file.","966636e3":"### \u5404\u6a21\u578bBaseline\u6307\u6807\u8bb0\u5f55\n\n\n\n| model |image size |  epoch | train acc | val acc| lb | \u5907\u6ce8 | \n|:----|----|----|----|----|----|----|\n|InceptionV3|512|25\/25|0.9023|0.8504|0.843 | |\n|ResNet50|512|23\/25|0.9301|0.8658| 0.851 | |\n|ResNet101|512|16\/25|0.9119 |0.8667 | |\u4e4b\u540e\u8fc7\u62df\u5408 |\n|ResNet152|512|17\/25|0.9268|0.8640| - |-|\n|ResNet101V2|512|19\/25|0.9322|0.8400| 0.827 |\u4e4b\u540e\u8fc7\u62df\u5408|\n|ResNet152V2|512|23\/25|0.9707|0.8339|  |-|\n|InceptionResnetV2|512|22\/25|0.8749|0.8369|  |-|\n|DenseNet121 |512| 24\/25| 0.8916| 0.8755 | 0.8620 |\u66f2\u7ebf\u770b\u7740\u53ef\u4ee5, 25\u8f6e\u672a\u5b8c\u5168\u6536\u655b|\n|EfficentNetB0|300| 30\/32|0.9030|0.8488|  |\u56fe\u7247\u592a\u5927GPU\u7206\u5185\u5b58|\n|Xception|512|25\/25|0.8082|0.8105|  |25\u8f6e\u672a\u5b8c\u5168\u6536\u655b|\n\n","41dc78ec":"# Intro\n\nI want to see how diffrent model performs at the data, so I set up this simple baseline. \n\nCheck history versions for each model's training log and metrics\n\n\n### Model List \/ \u6dfb\u52a0\u5404\u6a21\u578b\u7684\u8bad\u7ec3\u57fa\u7ebf,\u5305\u542b:\n* InceptionV3 [Version2-TPU](https:\/\/www.kaggle.com\/tianyu5\/tpus-cassava-leaf-disease?scriptVersionId=49382127)\n* resnet50v2\n* resnet101v2 [Version4-TPU](https:\/\/www.kaggle.com\/tianyu5\/tpus-cassava-leaf-disease?scriptVersionId=49386174)\n* resnet152v2\n* InceptionResnetV2 [Version6-TPU](https:\/\/www.kaggle.com\/tianyu5\/tpus-cassava-leaf-disease?scriptVersionId=49390571)\n* DenseNet121 [Version8-TPU](https:\/\/www.kaggle.com\/tianyu5\/tpus-cassava-leaf-disease?scriptVersionId=49397189)\n* Xception [Version9-TPU](https:\/\/www.kaggle.com\/tianyu5\/tpus-cassava-leaf-disease?scriptVersionId=49403249)\n* VGG16 [Version14-TPU]\n* NASNetLarge (\u8c03\u8bd5\u5b8c\u6210,\u7b49\u5f85\u8bad\u7ec3,\u6bd4\u8f83\u6162)\n* ResNet50  [Version12](https:\/\/www.kaggle.com\/tianyu5\/tpus-cassava-leaf-disease?scriptVersionId=49442732)\n* ResNet101  [Version13](https:\/\/www.kaggle.com\/tianyu5\/tpus-cassava-leaf-disease?scriptVersionId=49445531)\n\nGPU for EfficientNet ( Kaggle\u7684TPU\u7248\u672c\u76ee\u524d\u4e3a tf2.2, \u4e0d\u652f\u6301EfficienctNet, \u4f7f\u7528GPU\u8bad\u7ec3)\n* EfficentNetB0 [Version11-GPU](https:\/\/www.kaggle.com\/tianyu5\/tpus-cassava-leaf-disease?scriptVersionId=49419318)\n* EfficentNetB3 TBD (low memory, low memory, low memory !!!)\n\n\n**Change these 2 lines for model switch \/ \u9009\u62e9\u6a21\u578b\u65f6\u4fee\u6539\u6a21\u578b\u540d\u548c\u6a21\u578bbackbone\u65b9\u6cd5\u8fd9\u4e24\u884c\u5373\u53ef:**\n\n```\nMODEL_NAME = 'ResNet101'\n......\nbase_model ,preprocess_layer = resnet101_base()\n\n```\n\n### Here is the Submit Kernel \/ \u63d0\u4ea4Kernel\u5728\u6b64:\n* [TPUs + Cassava Leaf Disease[Infer]](https:\/\/www.kaggle.com\/tianyu5\/tpus-cassava-leaf-disease-infer)\n\n### References\nThanks for these kernels to help me get start:\n\n* [Getting Started: TPUs + Cassava Leaf Disease](https:\/\/www.kaggle.com\/jessemostipak\/getting-started-tpus-cassava-leaf-disease)\n* [Tensorflow Resnet50 (train with new tfrecords)](https:\/\/www.kaggle.com\/wuliaokaola\/tensorflow-resnet50-train-with-new-tfrecords)","2bde7155":"# Making predictions\nNow that we've trained our model we can use it to make predictions! ","5ff7635b":"# Train the model\nAs our model is training you'll see a printout for each epoch, and can also monitor TPU usage by clicking on the TPU metrics in the toolbar at the top right of your notebook.","73543885":"You can also modify the above code to look at your `validation` and `test` data, like this:","4baf3bd0":"# Detect TPU","48427603":"# Evaluating our model\nThe first chunk of code is provided to show you where the variables in the second chunk of code came from. As you can see, there's a lot of room for improvement in this model, but because we're using TPUs and have a relatively short training time, we're able to iterate on our model fairly rapidly.","9c8eab7c":"## Building our model\nIn order to ensure that our model is trained on the TPU, we build it using `with strategy.scope()`.    ","b875257d":"# Brief exploratory data analysis (EDA)\nFirst we'll print out the shapes and labels for a sample of each of our three datasets:","74afb3ea":"# Building the model\n## Learning rate schedule","f3a9166e":"# Set up variables","b792e4a6":"## A note on using train_test_split()\nWhile I used `train_test_split()` to create both a `training` and `validation` dataset, consider exploring **[cross validation instead](https:\/\/www.kaggle.com\/dansbecker\/cross-validation)**.","dcb8e3ee":"# Load the data\n","b409151f":"The following code chunk sets up a series of functions that will print out a grid of images. The grid of images will contain images and their corresponding labels.","0a48abb0":"### \u52a0\u8f7d\u8bad\u7ec3\u5b8c\u7684\u6a21\u578b","ee5edce0":"Be aware that because this is a code competition with a hidden test set, internet and TPUs cannot be enabled on your submission notebook. Therefore TPUs will only be available for training models. For a walk-through on how to train on TPUs and run inference\/submit on GPUs, see our [TPU Docs](https:\/\/www.kaggle.com\/docs\/tpu#tpu6).","8d5dd32c":"# Set up environment","c4149943":"We'll use the following function to load our dataset. One of the advantages of a TPU is that we can run multiple files across the TPU at once, and this accounts for the speed advantages of using a TPU. To capitalize on that, we want to make sure that we're using data as soon as it streams in, rather than creating a data streaming bottleneck.","97cd46ba":"## Model Summary","dbab693e":"# \u4fdd\u5b58\u6a21\u578b,\u7528\u4e8e\u9884\u6d4b\u63d0\u4ea4kernel\n\n* TPU\u7248\u672c\u76f4\u63a5\u5b58\u5168\u6a21\u578b\u8981\u7528GCS,\u7528\u672c\u5730\u78c1\u76d8\u6709\u95ee\u9898,\u6ce8\u610f\u70b9: [Saving to file a model within TPUStrategy\n#36447](https:\/\/github.com\/tensorflow\/tensorflow\/issues\/36447)\n\n* \u53c2\u8003\u522b\u4eba,\u8fd8\u662f\u76f4\u63a5\u5148\u5b58\u6743\u91cd,\u9884\u6d4b\u65f6\u4f7f\u7528\u4ee3\u7801\u5b9a\u4e49\u7ed3\u6784\n\n* TPU\u4e0a\u4fdd\u5b58 https:\/\/www.kaggle.com\/c\/jigsaw-multilingual-toxic-comment-classification\/discussion\/148930#835254\n\n* \u53c8\u53c2\u8003\u522b\u4eba,\u53d1\u73b0\u76f4\u63a5save\u5373\u53ef. \u6bd4\u8f83\u4f18\u96c5,Done. \u4e4b\u524d\u4e0d\u884c\u53ef\u80fd\u8ddf\u4e00\u4e9b\u6a21\u578b\u7684\u5199\u6cd5\u6709\u5173.\n\n"}}