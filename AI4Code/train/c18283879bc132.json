{"cell_type":{"c6e00113":"code","8a4603a3":"code","681558b1":"code","df29a991":"code","b62b5fb9":"code","45d9ea58":"code","f15c727d":"code","15be0e75":"code","21eef0ac":"code","5d2788bf":"code","670ba0c0":"code","06b37660":"code","cbc28c23":"code","5757dc7e":"code","85d79adb":"code","cb65d77c":"code","8e22ac73":"code","6ed5b03e":"code","f053f53d":"code","2baa0bc3":"code","42553e17":"code","f2eacada":"code","020ba35e":"code","2764195a":"code","766d924d":"code","b9c9e7b9":"code","7f6dad1c":"code","023b7399":"code","7cb453b4":"code","ef24f3c3":"code","40531106":"code","08ac28e8":"code","ad54361f":"code","6126c98d":"code","9b64cac7":"code","239f49f3":"code","937c4ad9":"code","b20539be":"code","619a70cd":"code","d7bc77a9":"code","8a10fbd1":"code","cbe6e3cb":"code","9f8d684e":"code","048e0841":"code","3c25b546":"code","72254ad5":"code","060009cc":"code","6318a8a4":"code","54ebd83a":"code","8bfb32d7":"code","cb208d4b":"code","25ca8ae9":"code","6bd01e0c":"code","a67edd8a":"code","0c8f410c":"code","c75fb8c5":"code","9d6e907a":"code","cedbb17b":"code","2466cd90":"code","5bfc0245":"code","fac1b151":"code","a3abe991":"code","01783bc1":"code","d430d383":"code","be0da054":"code","f04f6229":"code","d5c77c2c":"code","3f3909d8":"code","e1a49883":"code","fc248fb9":"code","d271e1f1":"markdown","9b64a686":"markdown","ce01c9be":"markdown","18fb5721":"markdown","2c18dda4":"markdown","3fdfcaf6":"markdown","0424436d":"markdown","b8afc21f":"markdown","379f9f3a":"markdown","01549a29":"markdown","f38971db":"markdown","65e3b2a4":"markdown","c0e547e1":"markdown","a21a5b99":"markdown","613e0de3":"markdown","336eec5c":"markdown","42f8e23d":"markdown","a8b52d94":"markdown","5e8acb77":"markdown","03c7d1b3":"markdown","7951c91a":"markdown","4866e049":"markdown","09ed6977":"markdown","5a9a2373":"markdown","1ca68cc0":"markdown","ccb658d8":"markdown","0412bd28":"markdown","71a5b7bc":"markdown","4f0bb746":"markdown","ac2e0423":"markdown","df9b4de0":"markdown","bb306af3":"markdown","101a8ac7":"markdown","3c2b133c":"markdown","9809fa10":"markdown","52a63745":"markdown","f380d2a4":"markdown","91749ab3":"markdown","6902f4ee":"markdown","45557a98":"markdown","c516a02e":"markdown","4ebc3d00":"markdown","f005d284":"markdown","728c9ebf":"markdown","6d0951fb":"markdown","56596aaf":"markdown","32c454f7":"markdown"},"source":{"c6e00113":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport warnings\nwarnings.filterwarnings(\"ignore\")\nimport seaborn as sns\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.svm import SVC\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.metrics import confusion_matrix,classification_report\nfrom sklearn.metrics import roc_curve\nfrom sklearn.metrics import roc_auc_score","8a4603a3":"# Loading the data\nhr=pd.read_csv(\"..\/input\/ibm-hr-analytics-attrition-dataset\/WA_Fn-UseC_-HR-Employee-Attrition.csv\")\nhr.head()","681558b1":"hr.shape","df29a991":"hr.dtypes","b62b5fb9":"# Removing unnecessary columns in the dataset\nimport warnings\nwarnings.filterwarnings(\"ignore\")\nhr.drop([\"EmployeeNumber\",\"Over18\",\"EmployeeCount\",\"StandardHours\"],axis=1,inplace=True)","45d9ea58":"hr.shape","f15c727d":"numerical_data= hr.select_dtypes(include=[\"int64\"])\nnumerical_data.shape","15be0e75":"categorical_data= hr.select_dtypes(include=[\"O\"])\ncategorical_data.shape","21eef0ac":"hr.isnull().sum()","5d2788bf":"sns.countplot(hr[\"Attrition\"])","670ba0c0":"sns.countplot(hr[\"BusinessTravel\"],palette=\"Set2\")","06b37660":"sns.countplot(hr[\"Department\"])","cbc28c23":"sns.countplot(hr[\"Gender\"])","5757dc7e":"plt.figure(figsize=(20,8))\nsns.countplot(hr[\"JobRole\"])","85d79adb":"sns.countplot(hr[\"MaritalStatus\"])","cb65d77c":"plt.figure(figsize=(8,4))\nplt.title(\"Distribution of age\")\na = sns.distplot(hr[\"Age\"], color = 'orange')","8e22ac73":"plt.figure(figsize=(8,4))\nplt.title(\"Distribution of Daily Rate \")\nb= sns.distplot(hr[\"DailyRate\"], color = 'green')","6ed5b03e":"plt.figure(figsize=(8,4))\nplt.title(\"Distribution of Hourly Rate \")\nc= sns.distplot(hr[\"HourlyRate\"], color = 'lime')","f053f53d":"plt.figure(figsize=(8,4))\nplt.title(\"Distribution of Percentage Salary Hike \")\nd= sns.distplot(hr[\"PercentSalaryHike\"], color = 'green')","2baa0bc3":"plt.figure(figsize=(8,4))\nplt.title(\"Distribution of Years at Company \")\ne= sns.distplot(hr[\"YearsAtCompany\"], color = 'springgreen')","42553e17":"sns.countplot(x=\"BusinessTravel\", hue=\"Attrition\",data=hr,palette=\"Set2\")\nplt.show()","f2eacada":"sns.countplot(x=\"Department\", hue=\"Attrition\",data=hr,palette=\"Set1\")\nplt.show()","020ba35e":"sns.countplot(x=\"Gender\", hue=\"Attrition\",data=hr,palette=\"Set3\")\nplt.show()","2764195a":"sns.countplot(x=\"MaritalStatus\", hue=\"Attrition\",data=hr)\nplt.show()","766d924d":"sns.countplot(x=\"OverTime\", hue=\"Attrition\",data=hr,palette=\"Set1\")\nplt.show()","b9c9e7b9":"sns.barplot(x=\"YearsAtCompany\", y=\"Attrition\",data=hr,palette=\"Set2\")\nplt.show()","7f6dad1c":"sns.countplot(x=\"YearsSinceLastPromotion\", hue=\"Attrition\",data=hr,palette=\"Set1\")\nplt.show()","023b7399":"hr.hist(figsize=(18,18),grid=True,bins='auto');","7cb453b4":"hr.corr()","ef24f3c3":"hr.skew(axis=0)","40531106":"# Treating the skewness in the dataset\nfor index in hr.skew().index:\n    if hr.skew().loc[index]>0.5:\n        hr[index]=np.log1p(hr[index])","08ac28e8":"hr.skew(axis=0)","ad54361f":"# Lets convert the target variable\nfrom sklearn.preprocessing import LabelEncoder\nLE= LabelEncoder()\nhr[\"Attrition\"]=LE.fit_transform(hr[\"Attrition\"])\nhr[\"BusinessTravel\"]=LE.fit_transform(hr[\"BusinessTravel\"])\nhr[\"Department\"]=LE.fit_transform(hr[\"Department\"])\nhr[\"EducationField\"]=LE.fit_transform(hr[\"EducationField\"])\nhr[\"Gender\"]=LE.fit_transform(hr[\"Gender\"])\nhr[\"JobRole\"]=LE.fit_transform(hr[\"JobRole\"])\nhr[\"MaritalStatus\"]=LE.fit_transform(hr[\"MaritalStatus\"])\nhr[\"OverTime\"]=LE.fit_transform(hr[\"OverTime\"])","6126c98d":"from scipy.stats import zscore\nz_score=abs(zscore(hr))\nprint(\"The shape of dataset before removing outliers\",hr.shape)\nhr=hr.loc[(z_score<3).all(axis=1)]\nprint(\"The shape of dataset after removing outliers\",hr.shape)","9b64cac7":"X= hr.drop([\"Attrition\"],axis=1)\ny= hr[\"Attrition\"]","239f49f3":"# Lets bring the dataset features into same scale\nfrom sklearn.preprocessing import StandardScaler\nscaler=StandardScaler()\nX= scaler.fit_transform(X)","937c4ad9":"X_train, X_test, y_train, y_test= train_test_split(X,y, test_size=0.30)","b20539be":"# We will use auc_roc score as the metrics because target variable has imbalance dataset\ndef max_auc_roc_sc(w,X,y):\n    max_auc_roc_sc=0\n    for r_state in range(42,100):\n        X_train,X_test, y_train, y_test= train_test_split(X,y,test_size=0.30, random_state=r_state,stratify=y)\n        w.fit(X_train,y_train)\n        y_pred= w.predict(X_test)\n        auc_roc=roc_auc_score(y_test,y_pred)\n        if auc_roc>max_auc_roc_sc:\n            max_auc_roc_sc=auc_roc\n            a_score=r_state\n    print(\"Maximum AUC_ROC Score corresponding to:\",a_score,\" and it is :\",round((max_auc_roc_sc),3))","619a70cd":"knn= KNeighborsClassifier()\nneighbors={\"n_neighbors\":range(1,30)}\nknn= GridSearchCV(knn, neighbors, cv=5,scoring=\"roc_auc\")\nknn.fit(X,y)\nknn.best_params_","d7bc77a9":"knn=KNeighborsClassifier(n_neighbors=29)\nmax_auc_roc_sc(knn,X,y)","8a10fbd1":"pred_knn= knn.predict(X_test)\nm1= knn.score(X_test, y_test)\nprint(\"The accuracy of the KNN Model is:\",round((m1),3))\nprint(confusion_matrix(y_test,pred_knn))","cbe6e3cb":"print(classification_report(y_test,pred_knn))","9f8d684e":"from sklearn.model_selection import cross_val_score\nmean_knn_auc=cross_val_score(knn, X,y,cv=5,scoring=\"roc_auc\").mean()\nprint(\"Mean AUC_ROC Score after cross validation\", cross_val_score(knn, X,y,cv=5,scoring=\"roc_auc\").mean())\nst_knn_auc= cross_val_score(knn, X,y,cv=5,scoring=\"roc_auc\").std()\nprint(\"standard deviation for KNN from mean AUC_ROC score is\",cross_val_score(knn, X,y,cv=5,scoring=\"roc_auc\").std())","048e0841":"y_pred_prob= knn.predict_proba(X_test)[:,0]\ntpr,fpr, thresholds= roc_curve(y_test, y_pred_prob)\n\n\n# Plot\nplt.plot([0,1],[0,1],\"k--\")\nplt.plot(fpr,tpr,label=\"KNN\")\nplt.xlabel(\"False Positive Rate\")\nplt.ylabel(\"True Positive Rate\")\nplt.title(\"KNN\")\nplt.show()","3c25b546":"a_c1=roc_auc_score(y_test, knn.predict(X_test))\na_c1","72254ad5":"from sklearn.tree import DecisionTreeClassifier\ndtc = DecisionTreeClassifier(criterion = 'entropy',max_depth=50)\n\nmax_auc_roc_sc(dtc,X,y)","060009cc":"pred_dtc= dtc.predict(X_test)\ndtc1= dtc.score(X_test, y_test)\nprint(\"The accuracy of the Decision Tree Model is:\",round((dtc1),3))\nprint(confusion_matrix(y_test,pred_dtc))","6318a8a4":"print(classification_report(y_test,pred_dtc))","54ebd83a":"from sklearn.model_selection import cross_val_score\nmean_dtc_auc=cross_val_score(dtc, X,y,cv=5,scoring=\"roc_auc\").mean()\nprint(\"Mean AUC_ROC Score Score after cross validation\", cross_val_score(dtc, X,y,cv=5,scoring=\"roc_auc\").mean())\ns_dtc_auc= cross_val_score(dtc, X,y,cv=5,scoring=\"roc_auc\").std()\nprint(\"standard deviation for Decision Tree Classifier from mean AUC_ROC score is\",cross_val_score(dtc, X,y,cv=5,scoring=\"roc_auc\").std())","8bfb32d7":"from sklearn.metrics import roc_curve\nfrom sklearn.metrics import roc_auc_score\n\ny_pred_prob= dtc.predict_proba(X_test)[:,0]\ntpr,fpr, thresholds= roc_curve(y_test, y_pred_prob)\n\n\n# Plot\nplt.plot([0,1],[0,1],\"k--\")\nplt.plot(fpr,tpr,label=\"Decision Tree\")\nplt.xlabel(\"False Positive Rate\")\nplt.ylabel(\"True Positive Rate\")\nplt.title(\"Decision Tree\")\nplt.show()","cb208d4b":"a_c2=roc_auc_score(y_test, dtc.predict(X_test))\na_c2","25ca8ae9":"from sklearn.ensemble import GradientBoostingClassifier\ngb = GradientBoostingClassifier()\nparameters= {'learning_rate': [0.01,0.03,0.05], 'subsample': [0.1, 0.5,0.3], 'n_estimators': [10,50,100], 'max_depth': [2,4,8]}\ngb= GridSearchCV(estimator=gb, param_grid= parameters, cv=5, n_jobs=-1)\ngb.fit(X,y)\ngb.best_params_","6bd01e0c":"gb = GradientBoostingClassifier(learning_rate=0.05,max_depth=2,n_estimators=100,subsample=0.1)\nmax_auc_roc_sc(gb,X,y)","a67edd8a":"pred_gb= gb.predict(X_test)\ngb1= gb.score(X_test, y_test)\nprint(\"The accuracy of the Grading Boosting Model is:\",round((gb1),3))","0c8f410c":"print(confusion_matrix(y_test,pred_gb))","c75fb8c5":"print(classification_report(y_test,pred_gb))","9d6e907a":"from sklearn.model_selection import cross_val_score\nmean_gb_auc=cross_val_score(gb, X,y,cv=5,scoring=\"roc_auc\").mean()\nprint(\"Mean AUC_ROC Score after cross validation\", cross_val_score(gb, X,y,cv=5,scoring=\"roc_auc\").mean())\nstd_gb_auc= cross_val_score(gb, X,y,cv=5,scoring=\"roc_auc\").std()\nprint(\"standard deviation for Gradient  Boosting from mean AUC_ROC score is\",cross_val_score(gb, X,y,cv=5,scoring=\"roc_auc\").std())","cedbb17b":"from sklearn.metrics import roc_curve\nfrom sklearn.metrics import roc_auc_score\n\ny_pred_prob= gb.predict_proba(X_test)[:,0]\ntpr,fpr, thresholds= roc_curve(y_test, y_pred_prob)\n\n\n# Plot\nplt.plot([0,1],[0,1],\"k--\")\nplt.plot(fpr,tpr,label=\"Gradient Boosting\")\nplt.xlabel(\"False Positive Rate\")\nplt.ylabel(\"True Positive Rate\")\nplt.title(\"Gradient Boosting\")\nplt.show()","2466cd90":"a_c3=roc_auc_score(y_test, gb.predict(X_test))\na_c3","5bfc0245":"from sklearn.ensemble import RandomForestClassifier\nrfc= RandomForestClassifier()\nparameters= {'n_estimators':[4,6,8],'max_features':['log2','sqrt','auto'],'criterion':['entropy','gini'],'max_depth':[2,5,10],'min_samples_split':[2,3,5],'min_samples_leaf':[3,5,7]}\nrfc= GridSearchCV(rfc,parameters)\nrfc.fit(X,y)\nrfc.best_params_","fac1b151":"rfc=RandomForestClassifier(criterion='gini', max_depth=10,max_features='sqrt',min_samples_leaf=5,min_samples_split=3,n_estimators=8)\nmax_auc_roc_sc(rfc,X,y)","a3abe991":"pred_rfc= rfc.predict(X_test)\nrf= rfc.score(X_test, y_test)\nprint(\"The accuracy of the Random Forest Classifier is:\",round((rf),3))","01783bc1":"print(confusion_matrix(y_test,pred_rfc))","d430d383":"print(classification_report(y_test,pred_rfc))","be0da054":"from sklearn.model_selection import cross_val_score\nmean_rfc_auc=cross_val_score(rfc, X,y,cv=5,scoring=\"roc_auc\").mean()\nprint(\"Mean AUC_ROC Score after cross validation\", cross_val_score(rfc, X,y,cv=5,scoring=\"roc_auc\").mean())\nstd_rfc_auc= cross_val_score(rfc, X,y,cv=5,scoring=\"roc_auc\").std()\nprint(\"standard deviation for Random Forest Classifier from mean AUC_ROC score is\",cross_val_score(rfc, X,y,cv=5,scoring=\"roc_auc\").std())","f04f6229":"y_pred_prob= rfc.predict_proba(X_test)[:,0]\ntpr,fpr, thresholds= roc_curve(y_test, y_pred_prob)\n\n# Plot\nplt.plot([0,1],[0,1],\"k--\")\nplt.plot(fpr,tpr,label=\"Random Forest\")\nplt.xlabel(\"False Positive Rate\")\nplt.ylabel(\"True Positive Rate\")\nplt.title(\"Random Forest Classifier\")\nplt.show()","d5c77c2c":"a_c4=roc_auc_score(y_test, rfc.predict(X_test))\na_c4","3f3909d8":"#Lets initialise the data frame with columns model and f1_score\ndata= [[\"KNN\", m1, mean_knn_auc,st_knn_auc],[\"Decision Tree Classifier\",dtc1,mean_dtc_auc,s_dtc_auc],[\"Gradient Boosting Classifier\", gb1,mean_gb_auc, std_gb_auc],[\"Random Forest Classifier\",rf,mean_rfc_auc,std_rfc_auc]]\ncomparsion_table= pd.DataFrame(data, columns=[\"Model Name\", \"Accuracy\",\"Mean AUC Score\",\" Std from mean AUC Score\"], index=[1,2,3,4])\ncomparsion_table","e1a49883":"np.savetxt('HR.csv',pred_dtc,delimiter=',')","fc248fb9":"#Lets save the above model\nfrom sklearn.externals import joblib \n  \n# Save the model as a pickle in a file \njoblib.dump(dtc, 'hr.pkl')","d271e1f1":"As the Decision Tree Classifier performed well, we are saving the prediction.","9b64a686":"<b>Checking Missing Values<\/b>","ce01c9be":"<b>Observation:<\/b>\nThe average years of employees at company is 5 years.","18fb5721":"<b>Observation:<\/b>\nFrom the above visualization, it is clear that employees who are in Research & Development have high attrition.","2c18dda4":"<b>Observation:<\/b>\nEmployees who do over time have high attrition.","3fdfcaf6":"<b>Gradient Boosting Classifier<\/b>","0424436d":"There are 8 attributes which are of object datatype.","b8afc21f":"<b>Observation:<\/b>\nFrom the above Visualization, it is clear that employees who travel rarely have high attrition.","379f9f3a":"<b> Multi-Variate Analysis<\/b>","01549a29":"As Target variable(Attrition) is binary, its classification problem, we will use KNN, Decision Tree Classifier, Gradient Boosting Classifier and Random Forest Classifier.","f38971db":"<b>Observation:<\/b>\nThe average percentage salary hike of employees are 12.5","65e3b2a4":"<b>Bi-Variate Analysis<\/b>","c0e547e1":"# Evaluation:","a21a5b99":"From the above visualization, we can see that target variable is imbalanced.","613e0de3":"<b>Observations:<\/b>\n<li> From the above models, Decision Tree Classifier performed well with 93.60% accuracy.<\/li>\n<li>As the data was imbalanced, we used AUC ROC for model evaluation and calculated Mean AUC Score and Standard Deviation mean AUC Score<\/li>","336eec5c":"There are 23 attributes which are of int datatype.","42f8e23d":"There are more number of employees who are Male.","a8b52d94":"# Exploratory Data Analysis","5e8acb77":"<b>KNN Classifier<\/b>","03c7d1b3":"<b>Observation:<\/b>\nMost of the Employees Travel Rarely.","7951c91a":"<b>Observation:<\/b>\nThere are 1470 rows and 35 columns in the dataset.","4866e049":"<b>Observation:<\/b>\nThe average DailyRate is ~750.","09ed6977":"There are three departments i.e. Sales, Research & Development, Human Resources. Out of which there are more employees who is in Research & Development.","5a9a2373":"<b>Importing Necessary Libraries<\/b>","1ca68cc0":"<b>Observations:<\/b>\n<li> There are 353 observations which are predicted Positive as TP(True Positive) and it is true.<\/li>\n<li> There are 19 observations which are predicted Negative as TN(True Negative) and it is True.<\/li>\n<li> There are 4 observations which are predicted Negative as FN(False Negative) and it is False. <\/li>\n<li> There are 46 observations which are predicted Positive as FP(False Positive) and it is False.<\/li>","ccb658d8":"<b>Observation:<\/b>\nEmployees who are at the company and have experience <=5 have high attrition.","0412bd28":"<b>Random Forest Classifier<\/b>","71a5b7bc":"<b>Observations:<\/b>\n<li> There are 357 observations which are predicted Positive as TP(True Positive) and it is true.<\/li>\n<li> There are 1 observations which are predicted Negative as TN(True Negative) and it is True.<\/li>\n<li> There are 0 observations which are predicted Negative as FN(False Negative) and it is False. <\/li>\n<li> There are 64 observation which are predicted Positive as FP(False Positive) and it is False.<\/li>","4f0bb746":"# Machine Learning Models","ac2e0423":"<b>Saving the Prediction<\/b>","df9b4de0":"<b>Decision Tree Classifier<\/b>","bb306af3":"<b>Observation:<\/b>\nThere are no missing values in the dataset.","101a8ac7":"<b>Observation:<\/b>\nMost of the employees who are working are Married.","3c2b133c":"<b>Observation:<\/b>\nFrom the above visualization, Males have high attrition.","9809fa10":"<b>Observation:<\/b>\nEmployees who have less experience since last promotion have high attrition.","52a63745":"<b>Observation:<\/b>\nThe average Age of employees is 35.","f380d2a4":"<b>Observations:<\/b>\n<li> There are 343 observations which are predicted Positive as TP(True Positive) and it is true.<\/li>\n<li> There are 52 observations which are predicted Negative as TN(True Negative) and it is True.<\/li>\n<li> There are 14 observations which are predicted Negative as FN(False Negative) and it is False. <\/li>\n<li> There are 13 observation which are predicted Positive as FP(False Positive) and it is False.<\/li>","91749ab3":"<b>Label Encoder<\/b>","6902f4ee":"<b>Observation:<\/b>\nThere are more employee who works as a Sales Executive.","45557a98":"<b>Dividing the input and output variables<\/b>","c516a02e":"<b>Checking Skewness<\/b>","4ebc3d00":"<b>Observation:<\/b>\nEmployees who are single have high attrition.","f005d284":"<b>Observation:<\/b>\nThe Hourly Rate of employees are ~ 70.","728c9ebf":"<b>Univariate Analysis<\/b>","6d0951fb":"<b>Splitting into training and testing<\/b>","56596aaf":"<b>Checking Outliers<\/b>","32c454f7":"<b>Observations:<\/b>\n<li> There are 352 observations which are predicted Positive as TP(True Positive) and it is true.<\/li>\n<li> There are 20 observations which are predicted Negative as TN(True Negative) and it is True.<\/li>\n<li> There are 5 observations which are predicted Negative as FN(False Negative) and it is False. <\/li>\n<li> There are 45 observations which are predicted Positive as FP(False Positive) and it is False.<\/li>"}}