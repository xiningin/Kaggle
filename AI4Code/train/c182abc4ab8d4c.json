{"cell_type":{"72603e24":"code","df6c5ca6":"code","4a434f2e":"code","9fc73f2e":"code","2bd0580a":"code","a1f349d8":"code","30a7a15b":"code","36f14174":"code","f762b048":"code","c312bd6a":"code","ecfd789f":"code","6f88df28":"code","ae43597d":"code","23f4d70d":"code","b5dd046e":"code","068e869e":"code","66f65cd5":"code","43017f44":"code","47b91c08":"code","5567cb98":"code","ff751869":"code","d5365a3a":"code","636740ed":"code","467b9be1":"code","c49a7449":"code","866cedf1":"code","e61a2539":"code","c5220d9d":"code","55eb5b5f":"code","77d7c577":"code","1ec2c67e":"code","9ba403ab":"code","1d54d26c":"markdown","7c333e56":"markdown","0fec1233":"markdown"},"source":{"72603e24":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport os\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport string\nimport gensim\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","df6c5ca6":"from tqdm.notebook import tqdm\nimport torch\nfrom transformers import BertTokenizer\nfrom torch.utils.data import TensorDataset\nimport transformers\nfrom transformers import BertForSequenceClassification\nimport re\nimport nltk\nfrom nltk.corpus import stopwords\nfrom wordcloud import WordCloud\nimport nltk\nimport tensorflow as tf\nfrom tensorflow.keras.layers import Dense,Input\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.callbacks import ModelCheckpoint\nimport tensorflow_hub as hub\nfrom gensim import matutils, models\nfrom wordcloud import WordCloud\nfrom sklearn.model_selection import train_test_split\nfrom torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler","4a434f2e":"train=pd.read_csv('..\/input\/60k-stack-overflow-questions-with-quality-rate\/valid.csv')\ntest=pd.read_csv('..\/input\/60k-stack-overflow-questions-with-quality-rate\/train.csv')","9fc73f2e":"test.isnull().sum()","2bd0580a":"train.head()","a1f349d8":"#convert categorical data to numerical data\ntargets = {'HQ': 0, 'LQ_EDIT': 1, 'LQ_CLOSE': 2}\ntrain['Y'] = train['Y'].map(targets)","30a7a15b":"train.shape","36f14174":"train.columns","f762b048":"sns.countplot(train['Y'])","c312bd6a":"#here we clean text data\ntrain['text']=train['Title']+''+train['Body']","ecfd789f":"train=train[['text','Y']]","6f88df28":"train.head(4)","ae43597d":"# load stop words\nstop_word = stopwords.words('english')","23f4d70d":"def clean_data(data):\n    data = data.lower()\n    data = re.sub(r'[^(a-zA-Z)\\s]','', data)\n     #     remove urls\n    data = re.sub(r'http\\S+', \" \", data)\n #     remove mentions\n    data = re.sub(r'@\\w+',' ',data)\n #     remove hastags\n    data = re.sub(r'#\\w+', ' ',data)\n#     remove digits\n    data = re.sub(r'\\d+', ' ', data)\n#     remove html tags and umber\n    data = re.sub('r<.*?>',' ', data)\n #     remove stop words \n    data = data.split()\n    data = \" \".join([word for word in data if not word in stop_word])\n    return data","b5dd046e":"train['text'] = train['text'].apply(lambda x:clean_data(x))\ntrain.head()","068e869e":"train.drop_duplicates(inplace= True)\nprint(train.head(4))\nprint(train.shape)","66f65cd5":"from sklearn.model_selection import train_test_split\nxtrain, xval, ytrain, yval = train_test_split(train['text'], train['Y'], test_size = 0.2,random_state=0)","43017f44":"#bert tokenizer\ntokenizer = transformers.BertTokenizer.from_pretrained('bert-base-uncased',do_lower_case=True)","47b91c08":"#length of the trains set\nseq_len=[len(i.split()) for i in xtrain]\npd.Series(seq_len).hist(bins=30)","5567cb98":"#length of the test\nseq_len=[len(i.split()) for i in xval]\npd.Series(seq_len).hist(bins=30)","ff751869":"encoded_data_train = tokenizer.batch_encode_plus(\n    xtrain, \n    add_special_tokens=True, \n    return_attention_mask=True, \n    pad_to_max_length=True, \n    max_length=50, \n    return_tensors='pt'\n)\n\nencoded_data_val = tokenizer.batch_encode_plus(\n    xval, \n    add_special_tokens=True, \n    return_attention_mask=True, \n    pad_to_max_length=True, \n    max_length=50, \n    return_tensors='pt'\n)","d5365a3a":"from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler","636740ed":"input_ids_train=encoded_data_train['input_ids']\nattention_mask_train=encoded_data_train['attention_mask']\nlabel_train=torch.tensor(ytrain.values)","467b9be1":"input_ids_val = encoded_data_val['input_ids']\nattention_masks_val = encoded_data_val['attention_mask']\nlabel_val = torch.tensor(yval.values)","c49a7449":"# Pytorch TensorDataset Instance\ndataset_train = TensorDataset(input_ids_train,attention_mask_train, label_train)\ndataset_val = TensorDataset(input_ids_val, attention_masks_val,label_val)","866cedf1":"\nmodel = transformers.BertForSequenceClassification.from_pretrained(\"bert-base-uncased\",\n                                                      num_labels=5,\n                                                      output_attentions=False,\n                                                      output_hidden_states=False)","e61a2539":"from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n\ndataloader_train = DataLoader(dataset_train, \n                              sampler=RandomSampler(dataset_train), \n                              batch_size=128)\n\ndataloader_validation = DataLoader(dataset_val, \n                                   sampler=SequentialSampler(dataset_val), \n                                   batch_size=128)","c5220d9d":"from torch.utils.data import DataLoader,RandomSampler,SequentialSampler\ndataloader_train=DataLoader(dataset_train,sampler=RandomSampler(dataset_train),batch_size=512)\ndataloader_validation=DataLoader(dataset_val,sampler=SequentialSampler(dataset_val),batch_size=128)","55eb5b5f":"from transformers import AdamW, get_linear_schedule_with_warmup\n\noptimizer = AdamW(model.parameters(),\n                  lr=1e-5, \n                  eps=1e-8)\n                  \nepochs = 5\n\nscheduler = get_linear_schedule_with_warmup(optimizer, \n                                            num_warmup_steps=0,\n                                            num_training_steps=len(dataloader_train)*epochs)","77d7c577":"from sklearn.metrics import f1_score\n\ndef f1_score_func(preds, labels):\n    preds_flat = np.argmax(preds, axis=1).flatten()\n    labels_flat = labels.flatten()\n    return f1_score(labels_flat, preds_flat, average='weighted')","1ec2c67e":"import random\n\nseed_val = 17\nrandom.seed(seed_val)\nnp.random.seed(seed_val)\ntorch.manual_seed(seed_val)\ntorch.cuda.manual_seed_all(seed_val)\ndevice = torch.device('cuda')","9ba403ab":"model.to(device)\nfor epoch in tqdm(range(1, epochs+1)):\n    model.train()\n    loss_train_total = 0\n    progress_bar = tqdm(dataloader_train, desc='Epoch {:1d}'.format(epoch), leave=False, disable=False)\n    for batch in progress_bar:\n        model.zero_grad()\n        \n        batch = tuple(b.to(device) for b in batch)\n        \n        inputs = {'input_ids':      batch[0].to(device),\n                  'attention_mask': batch[1].to(device),\n                  'labels':         batch[2].to(device),\n                 }       \n\n        outputs = model(**inputs)\n        \n        loss = outputs[0]\n        loss_train_total += loss.item()\n        loss.backward()\n\n        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n\n        optimizer.step()\n        scheduler.step()\n        \n        progress_bar.set_postfix({'training_loss': '{:.3f}'.format(loss.item()\/len(batch))})\n        \n    tqdm.write(f'\\nEpoch {epoch}')\n    \n    loss_train_avg = loss_train_total\/len(dataloader_train)            \n    tqdm.write(f'Training loss: {loss_train_avg}')","1d54d26c":"# EDA","7c333e56":"# EDA","0fec1233":"# Extracting inputs and attention masks out of encoded data"}}