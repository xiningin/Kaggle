{"cell_type":{"27ee5cbb":"code","178f3cba":"code","d0ca1a89":"code","13fc32a0":"code","9a8f3448":"code","cf12a520":"code","fb226b2d":"code","a07d7d7d":"code","c2a74e3c":"code","c6b855cf":"code","000b980d":"code","e5958698":"code","9f85d978":"code","824875e6":"code","32c8ec56":"code","f6e04338":"code","e73dc93b":"code","229df797":"code","16d427ec":"code","ae8f403c":"code","1bd828a4":"code","81edb1ee":"code","d7783867":"code","b7b09864":"code","e5e969c8":"code","08bc22a8":"code","fdec3602":"code","4610cfa2":"code","bd00d558":"code","cdee5a1c":"code","d4a421e0":"code","b313bdfa":"code","06baed93":"code","78515bdd":"code","a9914722":"code","e183f305":"code","50f4dacb":"code","cfeba3a0":"code","401726b5":"code","c35bc85f":"code","f051d4b5":"code","c78aa4b7":"code","84541093":"code","8be09d1e":"code","c627bc4e":"code","2a8ba752":"code","f1a4faa3":"code","3f474197":"code","056e170b":"code","9ecd96fb":"code","8476e471":"code","4b70f720":"code","9654e4c2":"code","115b578c":"code","434533f4":"code","ea1309a1":"code","3a24fec6":"markdown","377a7898":"markdown","d1abad99":"markdown","163fa419":"markdown","fbb841ee":"markdown","7db2ff7f":"markdown","3b88aff8":"markdown","77420674":"markdown","bf7281ef":"markdown","800be75b":"markdown","9f01ea02":"markdown","141b96fd":"markdown","6f0f68b8":"markdown","55afb7f5":"markdown","7f386618":"markdown","451ee29a":"markdown","ef31f4ab":"markdown","6da3faa7":"markdown","0fab3357":"markdown","6476d1bd":"markdown","63e608d0":"markdown","b12ae236":"markdown","74519dd9":"markdown","da1ee231":"markdown","ce7c2f6e":"markdown","9fb064c4":"markdown","529ce24b":"markdown"},"source":{"27ee5cbb":"# loading necessary packages\n\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport warnings\nwarnings.filterwarnings(\"ignore\")\nfrom sklearn.preprocessing import LabelEncoder\nfrom imblearn.over_sampling import SMOTE, ADASYN\nfrom imblearn.under_sampling import ClusterCentroids, RandomUnderSampler\nfrom sklearn.model_selection import train_test_split, cross_validate\nfrom wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import train_test_split, cross_validate\nfrom sklearn.metrics import (accuracy_score, confusion_matrix, classification_report, recall_score, f1_score, \n                             precision_score, make_scorer)","178f3cba":"dataset = pd.read_csv(\"..\/input\/data.csv\")","d0ca1a89":"dataset.head()","13fc32a0":"# shape of the dataset\n# there are 323138 observations and 8 features \n\ndataset.shape","9a8f3448":"dataset.describe()","cf12a520":"# top 10 most food ordered users \n# by default value_counts() method sort the result in descending order\ndataset.user_id.value_counts().head(10)","fb226b2d":"# number of unique items\nlen(dataset.item_id.unique())","a07d7d7d":"# number of times Each item ordered by user\ndataset.item_id.value_counts().head(15)","c2a74e3c":"items = dataset.item_id.values\n\n# concatenate all the items into a large string \nall_items = \",\".join(items) \n\n# Create and generate a word cloud image (1500 x 800 px):\nwordcloud = WordCloud(width=1500, height=800).generate(all_items)","c6b855cf":"plt.figure(figsize=(20, 8))\n# Display the generated image:\nplt.imshow(wordcloud, interpolation='bilinear')\nplt.axis(\"off\")\nplt.show()","000b980d":"# mapping data for better readability\nday_map = {1: \"Saturday\", 2: \"Sunday\", 3:\"Monday\", 4:\"Tuesday\", 5: \"Wednesday\", 6: \"Thursday\", 7:\"Friday\"}\ndataset.dow = dataset.dow.map(day_map)","e5958698":"# total items ordered by users each day of week\ndataset.dow.value_counts()","9f85d978":"dataset.dow.value_counts().plot.bar()","824875e6":"plt.figure(figsize=(10, 5))\nsns.color_palette(\"Set2\")\nsns.countplot(x = \"dow\", hue=\"item_count\", data = dataset)\nplt.xlabel(\"Day of Week\", fontsize=14)\nplt.xticks(fontsize=12)\nplt.yticks(fontsize=12)\nplt.title(\"Number of Items ordered in a particular day\", fontsize=16)\nplt.show()","32c8ec56":"dataset[dataset.item_count>3].head()","f6e04338":"# only 48 times users ordered more than 3 items at a time\nlen(dataset[dataset.item_count>3])","e73dc93b":"dataset[\"dow\"][dataset.item_count>3].value_counts()","229df797":"dataset.hod.value_counts()","16d427ec":"dataset.hod.value_counts().plot.bar()","ae8f403c":"facet = sns.FacetGrid(dataset, hue =\"item_count\", aspect = 4)\nfacet.map(sns.kdeplot,\"hod\", shade = True)\nfacet.set(xlim = (0, dataset[\"hod\"].max()))\nfacet.add_legend()\n\nplt.show()","1bd828a4":"# there are 15131 unique categories are avialble \nlen(dataset.category_id.unique())","81edb1ee":"# Number of time each category item ordered by users\ndataset.category_id.value_counts().head(10)","d7783867":"categories = dataset.category_id.values\n\n# concatenate all the categories into a large string \nall_categories = \",\".join(map(str, categories))\n\n# Create and generate a word cloud image (1500 x 800 px):\nwordcloud = WordCloud(width=1500, height=800).generate(all_categories)","b7b09864":"plt.figure(figsize=(20, 8))\n# Display the generated image:\nplt.imshow(wordcloud, interpolation='bilinear')\nplt.axis(\"off\")\nplt.show()","e5e969c8":"# there are 47 types of cusines are available\nlen(dataset[\"cusine_id\"].unique())","08bc22a8":"# user's ordered from each cusines\nplt.figure(figsize=(16, 6))\ndataset[\"cusine_id\"].value_counts().plot.bar()","fdec3602":"pd.set_option(\"max_column\", None)\n# which category belongs to which cusine \npd.crosstab(dataset[\"category_id\"], dataset[\"cusine_id\"]).head(10)","4610cfa2":"# most popular cusines\ncusines = dataset.cusine_id.values\n\n# concatenate all the cusines into a large string \nall_cusines = \",\".join(map(str, cusines))\n\n# Create and generate a word cloud image (1500 x 800 px):\nwordcloud = WordCloud(width=1500, height=800).generate(all_cusines)","bd00d558":"plt.figure(figsize=(20, 8))\n# Display the generated image:\nplt.imshow(wordcloud, interpolation='bilinear')\nplt.axis(\"off\")\nplt.show()","cdee5a1c":"# 3357 unique resturants\nlen(dataset[\"restaurant_id\"].unique())","d4a421e0":"# which resturants serves which cusines\npd.crosstab(dataset[\"restaurant_id\"], dataset[\"cusine_id\"]).head(5)","b313bdfa":"# total 871 restaurant serves `2e0c31a5-850` this cusine\nlen(dataset[\"restaurant_id\"][dataset[\"cusine_id\"]==\"2e0c31a5-850\"].unique())","06baed93":"# top 10 restaurants\ndataset[\"restaurant_id\"].value_counts().head(10)","78515bdd":"dataset[\"restaurant_id\"].value_counts().head(10).plot.bar()","a9914722":"len(dataset[dataset.item_count==5]), len(dataset[dataset.item_count==4]), len(dataset[dataset.item_count==3]), \\\nlen(dataset[dataset.item_count==2]), len(dataset[dataset.item_count==1])","e183f305":"# 5 new feature extraction\ndataset[\"single_meal\"] = (dataset.item_count==1).astype(int)\ndataset[\"couple_meal\"] = (dataset.item_count==2).astype(int)\ndataset[\"small_treat\"] = (dataset.item_count==3).astype(int)\ndataset[\"medium_treat\"] = (dataset.item_count==4).astype(int)\ndataset[\"large_treat\"] = (dataset.item_count==5).astype(int)","50f4dacb":"# handling missing values\ndataset.fillna(\"0\", inplace=True)","cfeba3a0":"day_map = {\"Saturday\" : 1, \"Sunday\" : 2,\"Monday\": 3, \"Tuesday\": 4, \"Wednesday\" :5 , \"Thursday\" : 6, \"Friday\": 7}\ndataset.dow = dataset.dow.map(day_map)\n\n# label encoding\nencoder = LabelEncoder()\ndataset[\"item_id\"] = encoder.fit_transform(dataset[\"item_id\"])\ndataset[\"category_id\"] = encoder.fit_transform(dataset[\"category_id\"])\ndataset[\"cusine_id\"] = encoder.fit_transform(dataset[\"cusine_id\"])\ndataset[\"restaurant_id\"] = encoder.fit_transform(dataset[\"restaurant_id\"])","401726b5":"dataset.head()","c35bc85f":"# imbalance problem\nprint(dataset[\"item_count\"].value_counts())\ndataset[\"item_count\"].value_counts().plot.bar()","f051d4b5":"X = dataset.drop([\"user_id\", \"item_count\"], axis=1)\ny = dataset[\"item_count\"]\n\nsmote = SMOTE(random_state=0)\n\nX_sm, y_sm = smote.fit_sample(X, y)","c78aa4b7":"# each class has now equal instances \npd.Series(y_sm).value_counts()","84541093":"ada = ADASYN(random_state=0)\nX_ada, y_ada = ada.fit_sample(X, y)","8be09d1e":"# ADASYN does not create equal synthetic instance for all classes\npd.Series(y_ada).value_counts()","c627bc4e":"cc = ClusterCentroids(random_state=0)\nX_cc, y_cc = cc.fit_sample(X, y)","2a8ba752":"pd.Series(y_cc).value_counts()","f1a4faa3":"rus = RandomUnderSampler()\nX_rus, y_rus = rus.fit_sample(X, y)","3f474197":"pd.Series(y_rus).value_counts()","056e170b":"X_train, X_test, y_train, y_test = train_test_split(X_ada, y_ada, test_size=0.2, random_state=42)","9ecd96fb":"clf_rf = RandomForestClassifier()\npred = clf_rf.fit(X_train, y_train).predict(X_test)\nacc = accuracy_score(pred, y_test)\nacc","8476e471":"cm = confusion_matrix(pred, y_test)\ncm","4b70f720":"sns.heatmap(cm, annot= True, fmt=\".0f\")","9654e4c2":"print(classification_report(pred, y_test))","115b578c":"# 5-fold Cross Validation\n\nclf = RandomForestClassifier()\n\nscoring = {'accuracy': make_scorer(accuracy_score), \n           'precision': make_scorer(precision_score, average='macro'),\n           'recall': make_scorer(recall_score, average='macro'),\n           'f1': make_scorer(f1_score, average='macro'),\n          }\n\nscores = cross_validate(clf, X, y, cv = 5, scoring = scoring, return_train_score=True)","434533f4":"scores","ea1309a1":"print(\"Avg. Accuracy \", scores[\"test_accuracy\"].mean())\nprint(\"Avg. Precision \", scores[\"test_precision\"].mean())\nprint(\"Avg. Recall \", scores[\"test_recall\"].mean())\nprint(\"Avg. F1 \", scores[\"test_f1\"].mean())","3a24fec6":"<h3 id=\"adasyn\"> ADASYN<\/h3>","377a7898":"<h1 id=\"data_exp\">Data Exploration & Analysis \ud83d\udc68\u200d \ud83d\udcbb \ud83d\udd2c <h1> ","d1abad99":"> Most of the Users ordered only `1` items at a time, sometimes they ordered `2` times __but more than `3` items are rare__. ","163fa419":"<h2 id=\"data_balance\"> Data Balancing <\/h2>\n---\nTo deal with imbalance problem there are mainly two approach available:\n* over-sampling and\n* under-sampling\n\n#### Over-sampling \n\nFor over-sampling we use Synthetic Minority Over-sampling Technique `(SMOTE)` and Adaptive Synthetic `(ADASYN)`. \n\n* __SMOTE__: It is an over-sampling method that creates __synthetic__ not duplicate samples of the minority class.\n\n* __ADASYN__: Almost similar to __SMOTE__, __ADYSYN__ also creates synthetic data points with feature space vectors. However, for the new data points to be realistic, ADYSYN adds a small `error` to the data points to allow for some `variance`.\n\n#### Under-sampling\nFor under-sampling we use `ClusterCentroids` and `RandomUnderSampler`. \n\n* __ClusterCentroids__: It makes use of K-means to reduce the number of samples. Therefore, each class will be synthesized with the centroids of the K-means method instead of the original samples.\n\n* __RandomUnderSampler__: It is a fast and easy way to balance the data by randomly selecting a subset of data for the targeted classes. ","fbb841ee":"> __`eb314e4e-8be`,  `9e834de3-b15`, `ec3bee9a-c85`, `665ae873-c39`, `13ad87a0-0dc` are some of the mostly ordered categories.__","7db2ff7f":"> __User's most popular cusine is `2e0c31a5-850`, which is ordered `53305` times by users.__","3b88aff8":"### Statistical description of the dataset\n__`DataFrame.describe()`__ method generates descriptive statistics that summarize the central tendency, dispersion and shape of a dataset\u2019s distribution, excluding `NaN` values. This method tells us a lot of things about a dataset. Keep in mind that the `describe()` method deals only with numeric values. It doesn't work with or show any statistics on `categorical` features.","77420674":"> More then `3` items are ordered only `48` times and __most of the order was take place on `Wednesday`__.","bf7281ef":"<h1 id=\"model_build\">Model Building<\/h1>","800be75b":"<h2 id=\"food_lovers\">Food Lovers \/ Foodies \ud83d\ude1c<\/h2>","9f01ea02":"<h2 id=\"over_sm\">Over-sampling<\/h2>\n<h3 id=\"smote\">SMOTE<\/h3>","141b96fd":"<h2 id=\"fav_cusine\"> User's favorite Cusines \ud83c\udf72 <\/h2>","6f0f68b8":"* `6` times users odered __5__ items together, \n\n* `42` times users ordered __4__ items together,\n\n* `340` times users ordered __3__ items together,\n\n* `4730` times users ordered __2__ items together,\n\n* `318020` times users ordered __1__ items.\n\n#### Let's make some assumption \ud83d\udc40\n* 5 -> large_treat \n* 4 -> medium_Treat\n* 3 -> small_Treat\n* 2 -> couple_meal \n* 1 -> single_meal \ud83d\ude10","55afb7f5":"> __`cbe2efae-0b4` is the most popular resturant.__","7f386618":"<p style='font-size:50px; text-align:center; color:#007959'>Food Recommendation System<p>\n![Food_image](https:\/\/images.unsplash.com\/photo-1498654896293-37aacf113fd9?ixlib=rb-1.2.1&ixid=eyJhcHBfaWQiOjEyMDd9&auto=format&fit=crop&w=1500&q=80)\n*Image taken from [unsplash](https:\/\/unsplash.com\/photos\/4f4YZfDMLeU)\n    \n## Kernel Overview\n* <a style=\"font-size:14px\">Data Exploration & Analysis<\/a>\n    * <a style=\"font-size:14px\">Food Lovers \/ Foodies \ud83d\ude1c<\/a>\n    * <a style=\"font-size:14px\">Most Ordered Items \ud83c\udf5b \ud83c\udf5a \ud83c\udf5c \ud83c\udf54 <\/a>\n    * <a style=\"font-size:14px\">Week's Prime day for Restaurants \ud83d\ude00 <\/a>\n    * <a style=\"font-size:14px\">Pick hour of a Day \u23f1  <\/a>\n    * <a style=\"font-size:14px\">What are the Categories User's Like Most? \ud83d\ude0d <\/a>\n    * <a style=\"font-size:14px\">User's favorite Cusines \ud83c\udf72 <\/a>\n    * <a style=\"font-size:14px\">Favorite Restaurants \ud83c\udf74 <\/a>\n* <a style=\"font-size:14px\">Ferature Engineering<\/a>\n* <a style=\"font-size:14px\">Data Balancing<\/a>\n    *  <a style=\"font-size:14px\">Over-sampling<\/a>\n        * <a style=\"font-size:14px\">SMOTE<\/a>\n        * <a style=\"font-size:14px\">ADASYN<\/a>\n    * <a style=\"font-size:14px\">Under-Sampling<\/a>\n* <a style=\"font-size:14px\">Model Building<\/a>","451ee29a":"> __Undersampling is not suiteable for this dataset__, because undersamping method undersampled the majority class. It lead `huge loss` of information in our case because the `majority` class contains the bigger portion of the dataset. \n__Under-sampling is used when the examples of the majority class are near to others.__\n\n<br\/>\n__I select `ADASYN` approach to further work with.__","ef31f4ab":"<h2 id=\"under_sample\">Under-sampling<\/h2>","6da3faa7":"<h2 id=\"prime_day\"> Week's Prime day for Restaurants \ud83d\ude00 <\/h2>","0fab3357":"> ## That's all for this kernel \ud83d\ude1c Hope you enjoyed it. If so then give a upvote! \ud83d\ude0d","6476d1bd":"> __`047a52bc-dd2`, `42156cb6-a16`, `da553b16-63f`, `2c5a9f37-44c`,`514a2cb3-d2a`, `8810682a-62a` are some of the frequently oderded products by users.__","63e608d0":"> `Thursday` is the Prime day for selling item's.","b12ae236":"> __`0` to `5` o'clock is the pick hour.__","74519dd9":"<h2 id=\"most_like\"> What are the Categories User's Like Most? \ud83d\ude0d <\/h2>","da1ee231":"<h2 id=\"most_order\">Most Ordered Items \ud83c\udf5b \ud83c\udf5a \ud83c\udf5c \ud83c\udf54<\/h2>","ce7c2f6e":"<h2 id=\"fav_restaurant\">User's Favorite Restaurants \ud83c\udf74 <\/h2>","9fb064c4":"<h2 id=\"pick_hour\"> Pick hour of a Day \u23f1 <\/h2>","529ce24b":"<h1 id=\"feature_eng\">Feature Engineering \ud83d\udc68\u200d\ud83d\ude80 <\/h1>"}}