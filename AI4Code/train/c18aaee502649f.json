{"cell_type":{"47416c9a":"code","ae5c91b4":"code","c96f523c":"code","b1679b3b":"code","0977b5af":"code","ef6e0e06":"code","08f67b20":"code","9611e650":"code","4722fe6f":"code","5ca8f045":"code","70c50629":"code","16a3e0fc":"code","a8e20caa":"code","835261fa":"markdown"},"source":{"47416c9a":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","ae5c91b4":"import torch\nimport torchmetrics\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom sklearn.preprocessing import MinMaxScaler, StandardScaler\nfrom sklearn.model_selection import StratifiedKFold, KFold\nfrom sklearn.model_selection import train_test_split\nfrom torchmetrics import MeanAbsoluteError\nfrom datetime import datetime\nimport gc","c96f523c":"X = pd.read_csv('\/kaggle\/input\/ventilator-pressure-prediction\/train.csv')\nX_test = pd.read_csv('\/kaggle\/input\/ventilator-pressure-prediction\/test.csv')\nX.pop('id')\nX_test.pop('id')\ny = X['pressure']\nX.pop('pressure')\n","b1679b3b":"X['u_in_cumsum'] = (X['u_in']).groupby(X['breath_id']).cumsum()\nX_test['u_in_cumsum']  = (X_test['u_in']).groupby(X_test['breath_id']).cumsum()\n# X['u_out_cumsum'] = (X['u_out']).groupby(X['breath_id']).cumsum()\n# X_test['u_out_cumsum']  = (X_test['u_out']).groupby(X_test['breath_id']).cumsum()\n# X['RC']= X['u_in']*(1-X['u_out'])*(1-np.exp(-X['time_step']\/(1e-3*X['R']*X['C'])))\n# X_test['RC']= X_test['u_in']*(1-X_test['u_out'])*(1-np.exp(-X_test['time_step']\/(X_test['R']*X_test['C'])))\n\nX.pop('breath_id')\nX_test.pop('breath_id')","0977b5af":"# pd.set_option('display.max_rows',100)\n# X.iloc[:100]","ef6e0e06":"# X_off=X.iloc[:400,:].copy()\n# y_off=y.iloc[:400].copy()\n# X=X.iloc[400:,:]\n# y=y.iloc[400:]","08f67b20":"X_tmp = X.iloc[:,4].copy()\nX_tmp_test = X_test.iloc[:,4].copy()\n# X_tmp_off = X_off.iloc[:,4].copy()\nst = StandardScaler()\nX = st.fit_transform(X)\nX_test = st.transform(X_test)\n# X_off = st.transform(X_off)\nX[:,4]=np.array(X_tmp)\nX_test[:,4]=np.array(X_tmp_test)\n# X_off[:,4]=np.array(X_tmp_off)","9611e650":"X=np.float32(X).reshape(-1,80,6)\nX_test = np.float32(X_test).reshape(-1,80,6)\ny=np.float32(y).reshape(-1,80,1)\n\n# X_off=np.float32(X_off).reshape(-1,80,6)\n# y_off=np.float32(y_off).reshape(-1,80,1)","4722fe6f":"class LSTMModel(nn.Module):\n    def __init__(self, n_dim, n_hdim=30, n_layers=1, v_dropout=0):\n        super().__init__()\n        self.n_dim = n_dim\n        self.n_hdim = n_hdim\n        self.n_layers = n_layers\n        self.v_dropout = v_dropout\n        self.lstm = nn.LSTM(\n        input_size=128,\n        hidden_size=self.n_hdim,\n        num_layers=self.n_layers,\n        dropout = self.v_dropout,\n        bidirectional=True,\n        batch_first=True)\n        self.inp = nn.Linear(self.n_dim,64)\n        self.inp2 = nn.Linear(64,128)\n        self.fc = nn.Linear(self.n_hdim*2,64)#2380 for 20\n        self.out = nn.Linear(64,1)\n\n\n        torch.nn.init.xavier_normal_(self.out.weight)\n        torch.nn.init.xavier_normal_(self.fc.weight)\n        torch.nn.init.xavier_normal_(self.inp.weight)\n        torch.nn.init.xavier_normal_(self.inp2.weight)\n\n\n    def forward(self, x):\n        x = F.relu(self.inp(x))\n        x = F.relu(self.inp2(x))\n        x, _ = self.lstm(x)\n\n        x = F.relu(self.fc(x))\n        x = F.relu(self.out(x))\n\n        return x","5ca8f045":"class L1Loss_masked(nn.Module):\n    def __init__(self):\n        super().__init__()\n    def forward(self, preds, y, u_out):\n\n\n        mask = 1 - u_out\n        mae = torch.abs(mask * (y - preds))\n        mae = torch.sum(mae) \/ torch.sum(mask)\n\n        return mae","70c50629":"def batch_gd(X, X_test, y, epochs, n_folds):\n\n\n    gc.collect()\n    kfold = KFold(n_splits = n_folds, random_state=42, shuffle=True)\n    y_pred = torch.zeros(4024000,1)\n\n    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n       # device = 'cpu'\n    print('Using {} device'.format(device))   \n    y_pred = y_pred.to(device)\n    xtest = torch.from_numpy(X_test)\n    test_loader = torch.utils.data.DataLoader(xtest, batch_size=32, shuffle=False)\n\n\n    for idx in kfold.split(X=X, y=y):\n        best_loss = 1e4\n        train_idx, val_idx = idx[0], idx[1]\n        xtrain = X[train_idx]\n        ytrain = y[train_idx]\n        xval = X[val_idx]\n        yval = y[val_idx]\n        \n        ytrain = np.array(ytrain)\n        yval = np.array(yval)\n        xtrain = torch.from_numpy(xtrain)\n        xval = torch.from_numpy(xval)\n        ytrain = torch.from_numpy(ytrain)\n        yval = torch.from_numpy(yval)\n\n        \n        train_dataset = torch.utils.data.TensorDataset(xtrain,ytrain)\n        train_loader=torch.utils.data.DataLoader(train_dataset, batch_size=256, shuffle=True, num_workers=4 )\n \n        val_dataset = torch.utils.data.TensorDataset(xval,yval)\n        val_loader=torch.utils.data.DataLoader(val_dataset, batch_size=256, shuffle=False , num_workers=4)\n        \n        model = LSTMModel(6,128,3)\n\n        criterion = L1Loss_masked()\n        optimizer = torch.optim.Adam(model.parameters(), lr=0.0008, eps=1e-08)\n        device = 'cuda' if torch.cuda.is_available() else 'cpu'\n\n        model.to(device)\n        gc.collect()\n        for epoch in range(epochs):\n            model.train()\n            t0 = datetime.now()\n            train_loss = []\n            for inputs, targets in train_loader:\n                inputs, targets = inputs.to(device), targets.to(device)\n\n                optimizer.zero_grad()\n                outputs = model(inputs)\n\n                loss = criterion(outputs, targets, inputs[:,:,4].reshape(-1,80,1))\n                loss.backward()\n                optimizer.step()\n                train_loss.append(loss.item())\n            train_loss = np.mean(train_loss) \n\n            model.eval()\n            with torch.no_grad():\n                val_loss = []\n                for inputs, targets in val_loader:\n                    inputs, targets = inputs.to(device), targets.to(device)\n                    outputs = model(inputs)\n                    loss = criterion(outputs, targets, inputs[:,:,4].reshape(-1,80,1))\n                    val_loss.append(loss.item())\n\n\n                val_loss = np.mean(val_loss)\n\n\n                if val_loss > 10:\n                    model.load_state_dict(torch.load('\/kaggle\/working\/ckpt_pytorch_ep_1'))\n                    model.to(device)\n                elif val_loss < best_loss:\n                    best_loss = val_loss\n                    torch.save(model.state_dict(), '\/kaggle\/working\/ckpt_pytorch')\n                if epoch==0:\n                    torch.save(model.state_dict(), '\/kaggle\/working\/ckpt_pytorch_ep_1')\n        #         auc_roc = metric(outputs,targets.int()).compute()\n                dt = datetime.now() - t0\n                print(f'Epoch {epoch+1}\/{epochs}, Train Loss: {train_loss:.4f}, \\\n          Val Loss: {val_loss:.4f} ,lowest loss {best_loss:.5f} ,    Duration: {dt}')\n\n\n        with torch.no_grad():\n            del model\n            del inputs\n            del targets\n            del xtrain\n            del xval\n            del ytrain\n            del yval\n            del train_dataset\n            del val_dataset\n            del train_loader\n            del val_loader\n            gc.collect()\n            with torch.cuda.device('cuda:0'):\n                torch.cuda.empty_cache()\n            model =LSTMModel(6,128,3)\n            model.load_state_dict(torch.load('\/kaggle\/working\/ckpt_pytorch'))\n            model.to(device)\n            preds = []\n            model.eval()\n            for inputs in test_loader:\n                inputs = inputs.to(device)\n                preds.append(model(inputs).reshape(-1,1)\/kfold.n_splits)\n            y_pred += torch.cat(preds,0)\n\n    return y_pred.cpu().numpy() \n","16a3e0fc":"\ny_pred=batch_gd(X, X_test, y, epochs=120, n_folds=5)","a8e20caa":"\nsub = pd.read_csv('\/kaggle\/input\/ventilator-pressure-prediction\/sample_submission.csv')\nsub.iloc[:,1]=y_pred\nsub=sub.set_index('id')\nsub.to_csv('baseline_pytorch_cv.csv')","835261fa":"#### 1. Baseline LSTM Model without tuning, \n####    Because i misread the evaluation section i have modeled expiratory phase too, this version is modified for inspiratory phase only - modified custom loss function"}}