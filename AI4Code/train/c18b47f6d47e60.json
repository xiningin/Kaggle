{"cell_type":{"997f9bab":"code","1ba0ab62":"code","ee50e544":"code","d7de02b6":"code","1279b1d0":"code","67612736":"code","153b0567":"code","b17d003f":"code","7e237fa4":"code","87cf6005":"code","3aab843e":"code","34bf8f1f":"code","dabc8d65":"code","5b5f9481":"code","5cee00ea":"markdown","86f825eb":"markdown","6780bf4d":"markdown","0c344b9a":"markdown","8ad84838":"markdown","f29e8518":"markdown","303ebf2b":"markdown","5d8a7d03":"markdown","4472a470":"markdown","eed25605":"markdown"},"source":{"997f9bab":"# import libraries\nimport pickle\nimport json\nimport gc\nimport bz2\nimport base64\nfrom glob import glob\n\nimport numpy as np\nimport pandas as pd\nfrom pandas import DataFrame\n\nfrom tqdm.notebook import tqdm\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import log_loss, accuracy_score, recall_score\n\nimport tensorflow as tf\nfrom tensorflow.keras.preprocessing import image\nfrom tensorflow.keras.models import Model, Sequential, load_model\nfrom tensorflow.keras.layers import (GlobalAveragePooling2D, Dense, Conv2D, Activation, Lambda, Add, \n                                     BatchNormalization, Input, Lambda)\nfrom tensorflow.keras.utils import to_categorical, Sequence\nfrom tensorflow.keras import backend as K\nfrom tensorflow.keras.regularizers import l1_l2 ,l1, l2\nfrom tensorflow.keras import mixed_precision","1ba0ab62":"# https:\/\/github.com\/ben-arnao\/Radabelief\nimport tensorflow as tf\nfrom tensorflow_addons.utils.types import FloatTensorLike\n\nfrom typing import Union, Callable, Dict\nfrom typeguard import typechecked\nimport numpy as np\n\n\n@tf.keras.utils.register_keras_serializable(package=\"Addons\")\nclass RadaBelief(tf.keras.optimizers.Optimizer):\n\n    @typechecked\n    def __init__(\n            self,\n            learning_rate: Union[FloatTensorLike, Callable, Dict] = 1e-4,\n            beta_1: FloatTensorLike = 0.9,\n            beta_2: FloatTensorLike = 0.999,\n            epsilon: FloatTensorLike = 1e-12,\n            warmup_steps: int = 10000,\n            name: str = \"Radabelief\",\n            **kwargs\n    ):\n        super().__init__(name, **kwargs)\n\n        self._set_hyper(\"learning_rate\", kwargs.get(\"lr\", learning_rate))\n        self._set_hyper(\"beta_1\", beta_1)\n        self._set_hyper(\"beta_2\", beta_2)\n        self._set_hyper(\"warmup_steps\", warmup_steps)\n        self.epsilon = epsilon or tf.keras.backend.epsilon()\n\n    def _create_slots(self, var_list):\n        for var in var_list:\n            self.add_slot(var, \"m\")\n        for var in var_list:\n            self.add_slot(var, \"v\")\n\n    def set_weights(self, weights):\n        params = self.weights\n        num_vars = int((len(params) - 1) \/ 2)\n        if len(weights) == 3 * num_vars + 1:\n            weights = weights[: len(params)]\n        super().set_weights(weights)\n\n    def _resource_apply_dense(self, grad, var):\n        var_dtype = var.dtype.base_dtype\n\n        # *current* learn rate of optimizer (changed by ReduceLROnPlateau for example)\n        lr_t = self.lr\n\n        # get previous moments\n        m = self.get_slot(var, \"m\")\n        v = self.get_slot(var, \"v\")\n\n        # static hyperparameters of optimizer\n        lr_0 = self._get_hyper(\"learning_rate\", var_dtype)\n        warmup_steps = self._get_hyper(\"warmup_steps\", var_dtype)\n        beta_1 = self._get_hyper(\"beta_1\", var_dtype)\n        beta_2 = self._get_hyper(\"beta_2\", var_dtype)\n\n        # smaller epsilon == more bias == more like SGD\n        # larger epsilon == more adaptive, potential for large LR difference between variables\n        epsilon_t = tf.convert_to_tensor(self.epsilon, var_dtype)\n\n        # current step of optimizer\n        local_step = tf.cast(self.iterations + 1, var_dtype)\n\n        # use linearly scaled lr from 0 to initial LR while steps under 'warmup_steps'\n        lr = tf.where(\n            local_step <= warmup_steps,\n            (local_step \/ warmup_steps) * lr_0,\n            lr_t,\n        )\n\n        # calculate first moment of gradient (momemtum)\n        m_t = m.assign(\n            beta_1 * m + (1.0 - beta_1) * grad,\n            use_locking=self._use_locking\n        )\n\n        # calculate second moment of gradient (RMSprop)\n        # use 'tf.square(grad - m_t)' for Adabelief instead of 'tf.square(grad)'\n        v_t = v.assign(\n            beta_2 * v + (1.0 - beta_2) * tf.square(grad - m_t),\n            use_locking=self._use_locking,\n        )\n\n        # correct bias (mostly affects initial steps)\n        m_corr_t = m_t \/ (1.0 - tf.pow(beta_1, local_step))\n        v_corr_t = v_t \/ (1.0 - tf.pow(beta_2, local_step))\n\n        # calculate step\n        var_t = m_corr_t \/ (tf.sqrt(v_corr_t) + epsilon_t)\n\n        # apply learn rate\n        var_update = var.assign_sub(lr * var_t,\n                                    use_locking=self._use_locking)\n\n        updates = [var_update, m_t, v_t]\n        return tf.group(*updates)\n\n    def get_config(self):\n        config = super().get_config()\n        config.update(\n            {\n                \"learning_rate\": self._serialize_hyperparameter(\"learning_rate\"),\n                \"beta_1\": self._serialize_hyperparameter(\"beta_1\"),\n                \"beta_2\": self._serialize_hyperparameter(\"beta_2\"),\n                \"epsilon\": self.epsilon,\n                \"warmup_steps\": self._serialize_hyperparameter(\"warmup_steps\"),\n            }\n        )\n        return config","ee50e544":"import math\nfrom tensorflow.keras.callbacks import Callback\nfrom tensorflow.keras import backend as K\n\nclass CosineAnnealingScheduler(Callback):\n    \"\"\"Cosine annealing scheduler.\n    \"\"\"\n\n    def __init__(self, T_max, eta_max, eta_min=0, verbose=0):\n        super(CosineAnnealingScheduler, self).__init__()\n        self.T_max = T_max\n        self.eta_max = eta_max\n        self.eta_min = eta_min\n        self.verbose = verbose\n\n    def on_epoch_begin(self, epoch, logs=None):\n        if not hasattr(self.model.optimizer, 'lr'):\n            raise ValueError('Optimizer must have a \"lr\" attribute.')\n        lr = self.eta_min + (self.eta_max - self.eta_min) * (1 + math.cos(math.pi * epoch \/ self.T_max)) \/ 2\n        K.set_value(self.model.optimizer.lr, lr)\n        if self.verbose > 0:\n            print('\\nEpoch %05d: CosineAnnealingScheduler setting learning '\n                  'rate to %s.' % (epoch + 1, lr))\n\n    def on_epoch_end(self, epoch, logs=None):\n        logs = logs or {}\n        logs['lr'] = K.get_value(self.model.optimizer.lr)","d7de02b6":"# besed on https:\/\/github.com\/yu4u\/mixup-generator\nclass GeeseDataGenerator(Sequence):\n        'Generates data for Keras'\n        def __init__(self, X_train, y_train, batch_size=128, shuffle=True, sample_weight=None):\n            'Initialization'\n            self.batch_size = batch_size\n            self.X_train = X_train\n            self.y_train = y_train\n            self.shuffle = shuffle\n            self.on_epoch_end()\n            self.sample_weight = sample_weight\n    \n        def __len__(self):\n            'Denotes the number of batches per epoch'\n            return int(np.ceil(len(self.X_train) \/ self.batch_size))\n    \n        def __getitem__(self, index):\n            'Generate one batch of data'\n            # Generate indexes of the batch\n            indexes = self.indexes[index*self.batch_size:(index+1)*self.batch_size]\n            # Generate data\n            if self.sample_weight is not None:\n                X, y, w = self.__data_generation(indexes)\n                return X, y, w\n            else:\n                X, y = self.__data_generation(indexes)\n                return X, y\n    \n        def on_epoch_end(self):\n            'Updates indexes after each epoch'\n            self.indexes = np.arange(len(self.X_train))\n            if self.shuffle == True:\n                np.random.shuffle(self.indexes)\n        \n        def __data_generation(self, batch_ids):\n            #_, h, w, c = self.X_train.shape\n        \n            X = self.X_train[batch_ids]\n            y = self.y_train[batch_ids]\n            if self.sample_weight is not None:\n                w = self.sample_weight[batch_ids]\n            else:\n                pass\n            \n            series_len = len(X)\n            \n            # vertical flip\n            if np.random.uniform(0,1)>0.5:\n                X = np.flip(X, axis=2)\n                y = y[:,[0,1,3,2]]\n                \n            # horizontal flip\n            if np.random.uniform(0,1)>0.5:\n                X = np.flip(X, axis=1)\n                y = y[:,[1,0,2,3]] \n            \n            if self.sample_weight is not None:\n                return X, y, w\n            else:\n                return X, y","1279b1d0":"def create_dataset_from_json(filepath, json_object=None, standing=0):\n    if json_object is None:\n        json_open = open(path, 'r')\n        json_load = json.load(json_open)\n    else:\n        json_load = json_object\n        \n    try:\n        winner_index = np.argmax(np.argsort(json_load['rewards']) == 3-standing)\n\n        obses = []\n        X = []\n        y = []\n        actions = {'NORTH':0, 'SOUTH':1, 'WEST':2, 'EAST':3}\n\n        for i in range(len(json_load['steps'])-1):\n            if json_load['steps'][i][winner_index]['status'] == 'ACTIVE':\n                y_= json_load['steps'][i+1][winner_index]['action']\n                if y_ is not None:\n                    step = json_load['steps'][i]\n                    step[winner_index]['observation']['geese'] = step[0]['observation']['geese']\n                    step[winner_index]['observation']['food'] = step[0]['observation']['food']\n                    obses.append(step[winner_index]['observation'])\n                    y.append(actions[y_])        \n\n        for j in range(len(obses)):\n            X_ = make_input(obses[:j+1])\n            X_ = centerize(X_)\n            X.append(X_)\n\n        X = np.array(X, dtype=np.float32)#[starting_step:]\n        y = np.array(y, dtype=np.uint8)#[starting_step:]\n\n        return X, y\n    except:\n        return 0, 0","67612736":"# based on https:\/\/www.kaggle.com\/yuricat\/smart-geese-trained-by-reinforcement-learning\ndef centerize(b):\n    dy, dx = np.where(b[0])\n    centerize_y = (np.arange(0,7)-3+dy[0])%7\n    centerize_x = (np.arange(0,11)-5+dx[0])%11\n    \n    b = b[:, centerize_y,:]\n    b = b[:, :,centerize_x]\n    \n    return b\n\ndef make_input(obses):\n    b = np.zeros((17, 7 * 11), dtype=np.float32)\n    obs = obses[-1]\n\n    for p, pos_list in enumerate(obs['geese']):\n        # head position\n        for pos in pos_list[:1]:\n            b[0 + (p - obs['index']) % 4, pos] = 1\n        # tip position\n        for pos in pos_list[-1:]:\n            b[4 + (p - obs['index']) % 4, pos] = 1\n        # whole position\n        for pos in pos_list:\n            b[8 + (p - obs['index']) % 4, pos] = 1\n            \n    # previous head position\n    if len(obses) > 1:\n        obs_prev = obses[-2]\n        for p, pos_list in enumerate(obs_prev['geese']):\n            for pos in pos_list[:1]:\n                b[12 + (p - obs['index']) % 4, pos] = 1\n\n    # food\n    for pos in obs['food']:\n        b[16, pos] = 1\n        \n    b = b.reshape(-1, 7, 11)\n    b = centerize(b) # Where to place the head is arbiterary dicision.\n\n    return b","153b0567":"# based on https:\/\/www.kaggle.com\/yuricat\/smart-geese-trained-by-reinforcement-learning\ndef TorusConv2D(x, ch, kernel, padding=\"same\", strides=1, weight_decay=2e-3):\n    x = Lambda(lambda x: K.tile(x, n=(1,3,3,1)), \n               output_shape=lambda input_shape: (None, 3*input_shape[1], 3*input_shape[2], input_shape[3]))(x)\n    \n    x = Conv2D(ch, kernel, padding=padding, strides=strides,\n                      kernel_regularizer=l2(weight_decay))(x)\n    \n    x = Lambda(lambda x: x[:,int(x.shape[1]\/3):2*int(x.shape[1]\/3), int(x.shape[2]\/3):2*int(x.shape[2]\/3),:], \n               output_shape=lambda input_shape: (None, int(input_shape[1]\/3), int(input_shape[2]\/3), input_shape[3]))(x)\n    return x\n\ndef conv_bn_relu(x0, ch, kernel, padding=\"same\", strides=1, weight_decay=2e-3, add=False):\n    x = TorusConv2D(x0, ch, kernel, padding=padding, strides=strides,\n                      weight_decay=weight_decay)\n    x = BatchNormalization()(x)\n    x = Activation(\"relu\")(x)\n    if add:\n        x = Add()([x0, x])\n    return x\n\ndef GeeseNet(input_shape=(7, 11, 17), layers=12, filters=32, weight_decay=2e-3, compile=True):\n    input = Input(input_shape)\n    x = conv_bn_relu(input, filters, 3)\n    \n    for i in range(layers):\n        x = conv_bn_relu(x, filters, 3, add=True)\n    \n    x = GlobalAveragePooling2D()(x)\n    \n    output = Dense(4, activation='softmax', kernel_regularizer=l1_l2(l1=0.0005, l2=0.0005))(x)   \n    model = Model(input, output)\n    if compile:\n        model.compile(optimizer=RadaBelief(learning_rate=1e-3, epsilon=1e-5), loss='categorical_crossentropy', metrics=['accuracy'])\n    else:\n        pass\n    \n    return model","b17d003f":"# use mixed precision\npolicy = mixed_precision.experimental.Policy('mixed_float16')\nmixed_precision.experimental.set_policy(policy)","7e237fa4":"# Create data set from json\n# Only a few episodes are used to train here. Try it with your own json collection.\npaths = [path for path in glob('..\/input\/nejugeese-public\/0319\/*.json') if 'info' not in path]\n\nprint(len(paths))\n\nX_train = []\ny_train = []\n\nfor path in tqdm(paths[:int(len(paths))]):\n    X, y = create_dataset_from_json(path, standing=0) # use only winners' moves\n    if X is not 0:\n        X_train.append(X)\n        y_train.append(y)\n    \nX_train = np.concatenate(X_train)\ny_train = np.concatenate(y_train)\n\nX_train, unique_index = np.unique(X_train, axis=0, return_index=True)\ny_train = y_train[unique_index]\n\nX_train = np.transpose(X_train, [0, 2, 3, 1])\ny_train = to_categorical(y_train)","87cf6005":"X_train.shape, y_train.shape","3aab843e":"alpha=0.3\nbatch_size = 32\nval_size = 0.1\n\nX_train_, X_val, y_train_, y_val = train_test_split(X_train, y_train, test_size=val_size, random_state=71)\n\ntraining_generator = GeeseDataGenerator(X_train_, (1-alpha)*y_train_+0.25*alpha, batch_size=batch_size)\ncos = CosineAnnealingScheduler(10, 2e-3*batch_size\/32, 0, verbose=1)\n\nmodel = GeeseNet(input_shape=(7, 11, 17), layers=12, filters=32, weight_decay=1e-7, compile=True)\nmodel.fit(training_generator, validation_data=(X_val, y_val), epochs=11, batch_size=batch_size, callbacks=[cos])\nmodel.save('model.h5')","34bf8f1f":"# Write weight to the head of the python script\nweight_base64 = base64.b64encode(bz2.compress(pickle.dumps(model.get_weights())))\nw = \"weight= %s\"%weight_base64\n%store w >submission.py","dabc8d65":"%%writefile -a submission.py\nimport pickle\nimport bz2\nimport base64\nimport numpy as np\n\nimport tensorflow as tf\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.layers import GlobalAveragePooling2D, Dense, Conv2D, Activation, Lambda, Add, BatchNormalization, Input\nfrom tensorflow.keras import backend as K\nfrom tensorflow.keras.regularizers import l1_l2, l2\n\n\n# Neural Network for Hungry Geese\ndef TorusConv2D(x, ch, kernel, padding=\"same\", strides=1, weight_decay=2e-3):\n    x = Lambda(lambda x: K.tile(x, n=(1,3,3,1)), \n               output_shape=lambda input_shape: (None, 3*input_shape[1], 3*input_shape[2], input_shape[3]))(x)\n    \n    x = Conv2D(ch, kernel, padding=padding, strides=strides,\n                      kernel_regularizer=l2(weight_decay))(x)\n    \n    x = Lambda(lambda x: x[:,int(x.shape[1]\/3):2*int(x.shape[1]\/3), int(x.shape[2]\/3):2*int(x.shape[2]\/3),:], \n               output_shape=lambda input_shape: (None, int(input_shape[1]\/3), int(input_shape[2]\/3), input_shape[3]))(x)\n    return x\n\ndef conv_bn_relu(x0, ch, kernel, padding=\"same\", strides=1, weight_decay=2e-3, add=False):\n    x = TorusConv2D(x0, ch, kernel, padding=padding, strides=strides,\n                      weight_decay=weight_decay)\n    x = BatchNormalization()(x)\n    x = Activation(\"relu\")(x)\n    if add:\n        x = Add()([x0, x])\n    return x\n\ndef GeeseNet(input_shape=(7, 11, 17), layers=12, filters=32, weight_decay=2e-3):\n    input = Input(input_shape)\n    x = conv_bn_relu(input, filters, 3)\n    \n    for i in range(layers):\n        x = conv_bn_relu(x, filters, 3, add=True)\n    \n    x = GlobalAveragePooling2D()(x)\n    \n    output = Dense(4, activation='softmax', kernel_regularizer=l1_l2(l1=0.0005, l2=0.0005))(x)   \n    model = Model(input, output)\n    #model.compile(optimizer=RadaBelief(learning_rate=1e-3, epsilon=1e-5), loss='categorical_crossentropy', metrics=['accuracy'])    \n    \n    return model\n\n# Input for Neural Network\ndef centerize(b):\n    dy, dx = np.where(b[0])\n    centerize_y = (np.arange(0,7)-3+dy[0])%7\n    centerize_x = (np.arange(0,11)-5+dx[0])%11\n    \n    b = b[:, centerize_y,:]\n    b = b[:, :,centerize_x]\n    \n    return b\n\ndef make_input(obses):\n    b = np.zeros((17, 7 * 11), dtype=np.float32)\n    obs = obses[-1]\n\n    for p, pos_list in enumerate(obs['geese']):\n        # head position\n        for pos in pos_list[:1]:\n            b[0 + (p - obs['index']) % 4, pos] = 1\n        # tip position\n        for pos in pos_list[-1:]:\n            b[4 + (p - obs['index']) % 4, pos] = 1\n        # whole position\n        for pos in pos_list:\n            b[8 + (p - obs['index']) % 4, pos] = 1\n            \n    # previous head position\n    if len(obses) > 1:\n        obs_prev = obses[-2]\n        for p, pos_list in enumerate(obs_prev['geese']):\n            for pos in pos_list[:1]:\n                b[12 + (p - obs['index']) % 4, pos] = 1\n\n    # food\n    for pos in obs['food']:\n        b[16, pos] = 1\n        \n    b = b.reshape(-1, 7, 11)\n    b = centerize(b) # Where to place the head is arbiterary dicision.\n\n    return b\n\n\n# Load Keras Model\nmodel = GeeseNet(input_shape=(7, 11, 17), layers=12, filters=32, weight_decay=1e-7)\nmodel.set_weights(pickle.loads(bz2.decompress(base64.b64decode(weight))))\n\n# Main Function of Agent\n\nobses = []\n\ndef agent(obs_dict, config_dict):\n    obses.append(obs_dict)\n\n    X_test = make_input(obses)\n    X_test = np.transpose(X_test, (1,2,0))\n    X_test = X_test.reshape(-1,7,11,17) # channel last.\n    \n    # avoid suicide\n    obstacles = X_test[:,:,:,[8,9,10,11,12]].max(axis=3) - X_test[:,:,:,[4,5,6,7]].max(axis=3) # body + opposite_side - my tail\n    obstacles = np.array([obstacles[0,2,5], obstacles[0,4,5], obstacles[0,3,4], obstacles[0,3,6]])\n    \n    y_pred = model.predict(X_test) - obstacles\n\n    \n    \n    actions = ['NORTH', 'SOUTH', 'WEST', 'EAST']\n    return actions[np.argmax(y_pred)]","5b5f9481":"from kaggle_environments import make\nenv = make(\"hungry_geese\", debug=True)\nenemy = '..\/input\/nejugeese\/submission_baseline.py'\n\nenv.reset()\nenv.run(['submission.py','submission.py','submission.py','submission.py'])\nenv.render(mode=\"ipython\", width=600, height=500)","5cee00ea":"#### Create Data from json","86f825eb":"#### Data Pipeline","6780bf4d":"#### Cosine Anealing","0c344b9a":"# Let's create Hungry Geese agent by supervised learning!","8ad84838":"#### import libraries","f29e8518":"The purpose of this Notebook is to provide an easy way to participate in this competition, so that many people can enjoy Hungry Geese. Kagglers are familiar with supervised learning and keras is one of the easiest frameworks to use. So, I would like to introduce a simple approach using keras with supervised learning. Of course, if you are aiming for a higher level, you will need reinforcement learning. I'm learning RL through this competition. Let's enjoy Hungry Geese together!","303ebf2b":"This notebook is based on a lot of public notebooks. In particular, the following ones:  \nMost of technical parts including model architecture, data pipeline etc: https:\/\/www.kaggle.com\/yuricat\/smart-geese-trained-by-reinforcement-learning  \nScraping episode files: https:\/\/www.kaggle.com\/robga\/simulations-episode-scraper-match-downloader  ","5d8a7d03":"#### Radabelief Optimizer","4472a470":"#### Model Architecture","eed25605":" #### Data Generator"}}