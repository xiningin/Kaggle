{"cell_type":{"915bf156":"code","80b0f573":"code","56799827":"code","9479dada":"code","7662b83d":"code","30c69509":"code","c9865cdd":"code","2385432a":"code","fa4632e0":"code","2713ae8e":"code","d06ed51a":"code","a003978d":"code","913e2262":"code","334c363d":"code","b8690208":"code","f1160e38":"code","58b5d808":"code","4b85c273":"code","93c3671e":"code","2c2b7174":"code","a8940a3a":"code","f10c6a00":"code","4627e50b":"code","d73e5c44":"code","4f81420b":"code","1e03eed1":"markdown","a016a05a":"markdown","baa4a1bf":"markdown","641e96e8":"markdown"},"source":{"915bf156":"# Main libraries\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n#Processing libraries\nfrom sklearn.preprocessing import StandardScaler\n\n# Model libraries\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import SVC, LinearSVC, NuSVC\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier,ExtraTreesClassifier\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis, QuadraticDiscriminantAnalysis\nfrom sklearn.naive_bayes import GaussianNB\n\n# Testing libraries\nfrom sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold, GridSearchCV, learning_curve\nfrom sklearn.metrics import accuracy_score, log_loss, confusion_matrix, classification_report, roc_curve, auc\n\n\n# Other useful things\n# View all columns of dataframes\npd.options.display.max_columns = None\n\n# View all cell outputs\nfrom IPython.core.interactiveshell import InteractiveShell\nInteractiveShell.ast_node_interactivity = \"all\"\n\n# Ignore warnings\nimport warnings\nwarnings.filterwarnings('ignore')\nprint('-'*25)\n","80b0f573":"# Load the data\nd = pd.read_csv('..\/input\/train.csv')\nloan_default = d['loan_default']","56799827":"# Looking at the data\nd.head(8)\nd.info()\nd.describe()\nd.describe(include=['O'])","9479dada":"# Look at data distribution, count values, visualisations (to decide what variables to use for the model)\n# Look at the Starter: Loan Default Prediction for data analysis.","7662b83d":"# Here I'm not going into details about visualisations\n# Look at data distribution, count values, visualisations (to decide what variables to use for the model)\n# Look at the Starter: Loan Default Prediction for data analysis\n# I've decided based on things above how I would deal with data.\n\n# Must do - checklist:\n#    - create new variables\n#    - pca dimentionality reduction if possible\n#    - drop irrelevant variables\n#    - fill nulls (mean, median, 0, depending on the data)\n#    - deal with categorical - posibly create dummy variables\n#    - normilise\/standadise (int and floats)\n#    - balance the data set (by the dependant variable)\n#    - check for multicolinearity (posibly drop variables)\n","30c69509":"# Create new variables (combine old ones)\nd['PRI.percent_default'] = d['PRI.OVERDUE.ACCTS'].divide(d['PRI.ACTIVE.ACCTS'], fill_value=0)\nd['PRI.percent_default'][d['PRI.percent_default'] == np.inf] = 0\n\nd['PRI.disbursed_to_outstanding'] = d['PRI.DISBURSED.AMOUNT'].divide(d['PRI.CURRENT.BALANCE'], fill_value=0)\nd['PRI.disbursed_to_outstanding'][d['PRI.disbursed_to_outstanding'] == np.inf] = 0\n\nd['SEC.percent_default'] = d['SEC.OVERDUE.ACCTS'].divide(d['PRI.ACTIVE.ACCTS'], fill_value=0)\nd['SEC.percent_default'][d['SEC.percent_default'] == np.inf] = 0\n\nd['SEC.disbursed_to_outstanding'] = d['SEC.DISBURSED.AMOUNT'].divide(d['SEC.CURRENT.BALANCE'], fill_value=0)\nd['SEC.disbursed_to_outstanding'][d['SEC.disbursed_to_outstanding'] == np.inf] = 0\n\nd['SEC_PRI.sanctioned'] = d['SEC.SANCTIONED.AMOUNT'].divide(d['PRI.SANCTIONED.AMOUNT'], fill_value=0)\nd['SEC_PRI.sanctioned'][d['SEC_PRI.sanctioned'] == np.inf] = 0\n\nd = d.round(3)","c9865cdd":"# Do pca (dimentionality reduction): 'branch_id', 'supplier_id', 'manufacturer_id'\n# will be done in the later versions.","2385432a":"# Drop some irrelevant variables\nd = d.drop(['UniqueID', 'branch_id', 'supplier_id', 'manufacturer_id',\n            'Date.of.Birth', 'DisbursalDate', 'State_ID',\n            'Current_pincode_ID', 'Employee_code_ID', \n            'MobileNo_Avl_Flag',  'PERFORM_CNS.SCORE.DESCRIPTION', \n            'PRI.OVERDUE.ACCTS', 'PRI.ACTIVE.ACCTS',  'PRI.DISBURSED.AMOUNT', \n            'PRI.CURRENT.BALANCE', 'SEC.OVERDUE.ACCTS', 'PRI.ACTIVE.ACCTS', \n            'SEC.DISBURSED.AMOUNT', 'SEC.CURRENT.BALANCE', 'SEC.SANCTIONED.AMOUNT', \n            'PRI.SANCTIONED.AMOUNT', 'PRI.NO.OF.ACCTS', 'SEC.NO.OF.ACCTS',\n            'AVERAGE.ACCT.AGE', 'CREDIT.HISTORY.LENGTH', 'loan_default'], axis=1)","fa4632e0":"# Dealing with null values, for numerical:\nfor col in d.columns:\n    if d[col].dtype != object:\n        d[col]=d[col].fillna(d[col].mean())","2713ae8e":"# Create dummies - for better model outcomes\nd = pd.get_dummies(d)\n\n# Data Normalisation\nnames = d.columns\nscaler = StandardScaler()\nscaled_d = scaler.fit_transform(d)\nd = pd.DataFrame(scaled_d, columns=names)","d06ed51a":"d['loan_default'] = pd.Series(loan_default)\n\n# Class balance\nld = d[d.loan_default == 1]\nno_ld = d[d.loan_default == 0]\n\nbalanced_d = pd.concat([ld.sample(int(len(no_ld)*2\/3), replace = True), no_ld] )\nx = balanced_d.iloc[:,:-1]\ny = balanced_d.iloc[:,-1:]","a003978d":"# Check for multicollinearity\n#d.corr() #less representative way\nplt.figure(figsize=(30,15))\nsns.heatmap(d[d.columns].corr(),cmap=\"BrBG\",annot=True)\nplt.show();","913e2262":"# Drop values to avoid multicolinearity\nx = x.drop(['asset_cost', 'Aadhar_flag'], axis=1)","334c363d":"# Random state\nrs = 2\n\n# Split the data to check which algorithms learn better (later on we can check )\nx_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.3, random_state=rs)\n\n# look at the shape of the data (many problems can arise from wrong shape)\nx_train.shape\ny_test.shape\n","b8690208":"# List of classifiers:\nclassifiers = [\nLogisticRegression(random_state = rs),\nDecisionTreeClassifier(random_state=rs),\nRandomForestClassifier(n_estimators = 10, random_state=rs),\nExtraTreesClassifier(random_state=rs),\nAdaBoostClassifier((DecisionTreeClassifier(random_state=rs)), random_state=rs, learning_rate=0.1),\nGradientBoostingClassifier(random_state=rs),\nGaussianNB(),\nMLPClassifier(random_state=rs),\nLinearDiscriminantAnalysis(),\nQuadraticDiscriminantAnalysis()\n]\n\n# List of results that will occure:\nclf_name = [] # names of the clasifiers\nmodel_results = pd.DataFrame.copy(y_test) #resulting of prediction from the models\n\nkfold = StratifiedKFold(n_splits=10) #cross-validation\ncv_results = [] # scores from cross validation\ncv_acc = [] # mean accuracy from cross validation, need to maximize\ncv_std = [] # standard deviation from cross validation, need to minimise\n\ncnfm = [] #confusion matrix\nclr = [] #clasification report\nroc_auc = [] #roc curve:\nroc_tpr = []\nroc_fpr = []\n","f1160e38":"# Training the algorithms and results\nfor clf in classifiers:\n    name = clf.__class__.__name__\n    clf_name.append(name)\n    \n    #fitting and predictions\n    model = clf.fit(x_train, y_train)\n    y_pred = model.predict(x_test)\n    model_results[name] = y_pred\n    \n    #accuracy and log loss\n    cv_results.append(cross_val_score(clf, x_train, y_train, scoring = \"accuracy\",cv = kfold))\n    acc = round(accuracy_score(y_test, y_pred), 4) #need to maximise\n    train_pred = clf.predict_proba(x_test)\n    ll = round(log_loss(y_test, train_pred), 4) #need to minimise\n    print(f'Accuracy: {acc} \\t Log Loss: {ll} \\t ---> {name} ')\n    \n    #confusion matrix, clasification report, roc curve\n    cnfm.append(confusion_matrix(y_test, y_pred))\n    clr.append(classification_report(y_test, y_pred))\n    fpr, tpr, thresholds = roc_curve(y_pred, y_test)\n    roc_auc.append(auc(fpr, tpr))\n    roc_tpr.append(tpr)\n    roc_fpr.append(fpr)\n    \n\nfor i in cv_results:\n    cv_acc.append(i.mean())\n    cv_std.append(i.std())\n","58b5d808":"# Cross validation accuracy results graph\ncv_res = pd.DataFrame({\"CrossValMeans\":cv_acc, \"CrossValerrors\": cv_std,\"Algorithm\":clf_name})\n\nplt.figure(figsize=(15,8));\nsns.barplot(\"CrossValMeans\",\"Algorithm\",data = cv_res, palette=\"Set2\",orient = \"h\",**{'xerr':cv_std});\nplt.xlabel(\"Mean Accuracy\");\nplt.title(\"Cross validation scores\");\nplt.grid();\nplt.show;","4b85c273":"# Confusion matrixes (not-normalized confusion matrix)\nplt.figure(figsize=(10,10))\nfor i in range(len(classifiers)):\n    plt.subplot(5,2,i+1) #adjust this acourding to the number of algorithms\n    sns.heatmap(cnfm[i], annot=True, fmt=\"d\",cmap=\"Blues\");\n    plt.xlabel('Predicted');\n    plt.ylabel('Actual');\n    plt.title(clf_name[i]);\nplt.show;\n","93c3671e":"#Clasification reports\nfor i in range(len(classifiers)):\n    print (f\"{clf_name[i]} Clasification Report:\" );\n    print (clr[i]);","2c2b7174":"# ROC Curve\nplt.figure(figsize=(15,8))\nfor i in range(len(classifiers)):\n    cm = ['red', 'blue', 'orange', 'green', 'pink', 'yellow', 'lightgreen', 'black', 'purple', 'lightblue'] #add more colours for more algorithms\n    plt.plot(roc_fpr[i], roc_tpr[i], c=cm[i], lw=1, label=f'{clf_name[i]}: area = {roc_auc[i]}0.2f)')\n    plt.plot([0, 1], [0, 1], color='navy', lw=1, linestyle='--')\n    plt.xlabel('False Positive Rate');\n    plt.ylabel('True Positive Rate');\n    plt.title('ROC curve: Receiver Operating Characteristic');\n    plt.legend(loc=\"lower right\");\nplt.show;","a8940a3a":"# Best model tuning (tuning Extra trees)\n# GridSearch\n#ET_param_gs = {\"max_depth\": [None],\n#              \"max_features\": [1, 3, 10],\n#              \"min_samples_split\": [2, 3, 10],\n#              \"min_samples_leaf\": [1, 3, 10],\n#              \"bootstrap\": [False],\n#              \"n_estimators\" :[100,300],\n#              \"criterion\": [\"gini\"]}\n\n# 4n_jobs for faster processing\n#ET_gs = GridSearchCV(ExtraTreesClassifier(), param_grid = ET_param_gs, cv=kfold, n_jobs=4, scoring=\"accuracy\", verbose = 1)\n\n#models = [ET_gs]\n\n#for model in models:\n#    model.fit(x_train, y_train)\n#    best_model = model.best_estimator_\n#    score_bm = model.best_score_\n\n#score_bm","f10c6a00":"best_model = ExtraTreesClassifier(random_state=rs)","4627e50b":"# Load the test data\nd2 = pd.read_csv('..\/input\/test_bqCt9Pv.csv')\nUniqueID = d2['UniqueID'] #save this variable for correct format of the submision\n\n\n# Create new variables (combine old ones)\nd2['PRI.percent_default'] = d2['PRI.OVERDUE.ACCTS'].divide(d2['PRI.ACTIVE.ACCTS'], fill_value=0)\nd2['PRI.percent_default'][d2['PRI.percent_default'] == np.inf] = 0\n\nd2['PRI.disbursed_to_outstanding'] = d2['PRI.DISBURSED.AMOUNT'].divide(d2['PRI.CURRENT.BALANCE'], fill_value=0)\nd2['PRI.disbursed_to_outstanding'][d2['PRI.disbursed_to_outstanding'] == np.inf] = 0\n\nd2['SEC.percent_default'] = d2['SEC.OVERDUE.ACCTS'].divide(d2['PRI.ACTIVE.ACCTS'], fill_value=0)\nd2['SEC.percent_default'][d2['SEC.percent_default'] == np.inf] = 0\n\nd2['SEC.disbursed_to_outstanding'] = d2['SEC.DISBURSED.AMOUNT'].divide(d2['SEC.CURRENT.BALANCE'], fill_value=0)\nd2['SEC.disbursed_to_outstanding'][d2['SEC.disbursed_to_outstanding'] == np.inf] = 0\n\nd2['SEC_PRI.sanctioned'] = d2['SEC.SANCTIONED.AMOUNT'].divide(d2['PRI.SANCTIONED.AMOUNT'], fill_value=0)\nd2['SEC_PRI.sanctioned'][d2['SEC_PRI.sanctioned'] == np.inf] = 0\n\nd2 = d2.round(3)\n\n\n# Drop the irrelevant columns\nd2 = d2.drop(['UniqueID', 'branch_id', 'supplier_id', 'manufacturer_id','Aadhar_flag',\n            'Date.of.Birth', 'DisbursalDate', 'State_ID', 'asset_cost',\n            'Current_pincode_ID', 'Employee_code_ID', \n            'MobileNo_Avl_Flag',  'PERFORM_CNS.SCORE.DESCRIPTION', \n            'PRI.OVERDUE.ACCTS', 'PRI.ACTIVE.ACCTS',  'PRI.DISBURSED.AMOUNT', \n            'PRI.CURRENT.BALANCE', 'SEC.OVERDUE.ACCTS', 'PRI.ACTIVE.ACCTS', \n            'SEC.DISBURSED.AMOUNT', 'SEC.CURRENT.BALANCE', 'SEC.SANCTIONED.AMOUNT', \n            'PRI.SANCTIONED.AMOUNT', 'PRI.NO.OF.ACCTS', 'SEC.NO.OF.ACCTS',\n            'AVERAGE.ACCT.AGE', 'CREDIT.HISTORY.LENGTH'], axis=1)\n\n\n# Dealing with null values, for numerical:\nfor col in d2.columns:\n    if d2[col].dtype != object:\n        d2[col]=d2[col].fillna(d2[col].mean())\n        \n        \n\n# Create dummies - for better model outcomes\nd2 = pd.get_dummies(d2)\n\n# Data Normalisation\nnames = d2.columns\nscaler = StandardScaler()\nscaled_d = scaler.fit_transform(d2)\nd2 = pd.DataFrame(scaled_d, columns=names)\n","d73e5c44":"#train the model on the whole dataset and produce results:\nmodel = best_model.fit(x, y)\nloan_default_pred = pd.Series(model.predict(d2), name='loan_default')\n","4f81420b":"# Results (#loan_default = d2['loan_default'])\nd3 = pd.concat([UniqueID, loan_default_pred], axis=1)\nd3.to_csv('submision.csv', index=False)\n","1e03eed1":"> **Preprocessiong - Working with data **","a016a05a":"> ** Machine Learnign Models**","baa4a1bf":"> ** Submision **","641e96e8":"> ** Processing - Data used for the models **"}}