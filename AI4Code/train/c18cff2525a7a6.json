{"cell_type":{"aa4deccf":"code","3665a01b":"code","8c80c00e":"code","ca35907d":"code","8d465276":"code","c810f0c3":"code","025a7585":"code","0c7b59a2":"code","eecbbf81":"code","25230958":"code","965e0a9d":"code","e4402771":"code","3a4600b2":"code","546e1a2b":"code","4a971cb1":"code","6429ebca":"code","b972442b":"code","e537a4ac":"code","3c6073c9":"code","0dce4a76":"code","9673ab53":"code","cd7f85ad":"code","aac27fba":"code","35d5c528":"code","b7c35706":"code","490bff4a":"code","ec33f5f1":"code","2e76fa24":"code","311999e9":"code","0ecbff54":"code","4e929ece":"code","66425267":"code","753b2534":"code","8af668cb":"code","a8fb05d6":"code","fe90e35a":"code","4920d47b":"code","59b9ac17":"code","21adad09":"code","948f092a":"code","d49f7785":"code","ac233315":"code","3bd2ac4e":"code","48433533":"code","16ca165b":"code","cbef5014":"code","df754fce":"code","69e19128":"code","81003602":"code","500f4deb":"code","09bef4cb":"code","714c45ca":"code","1504f5ef":"code","ddb2683a":"code","d19c5c25":"code","dc5c26bd":"code","e087520c":"code","81019834":"code","3dcc9d2a":"code","a109f3bc":"code","bb6c15e1":"code","4f418b7a":"code","332d2675":"code","f2efb945":"code","6f656feb":"code","3a83099e":"code","68f20d6f":"code","d9d39d5c":"code","edfea57c":"code","ed6c1723":"code","a295c60c":"code","3ca5bf5e":"code","5e3e5484":"code","91755623":"code","d59c1758":"code","39c738c3":"code","e9e5f6ba":"code","06347eca":"code","75e55713":"code","43f87853":"code","93eb5522":"code","f4e22742":"code","fcb55300":"code","763f6cbf":"code","f2241b3c":"code","c78b15bb":"code","3677aa6b":"code","2d271fde":"code","9304f895":"code","d315a4ec":"code","05ce73b7":"code","c8b6a801":"code","7ee22ebb":"code","9ad72dc3":"code","60ede2a2":"code","a0273bbb":"code","cce4c0da":"code","3f2b583c":"markdown","3859799d":"markdown","7b9ce6d9":"markdown","956293ed":"markdown","7716ab24":"markdown","ea3d77b9":"markdown","c268b3db":"markdown","27f63df8":"markdown","9b94efb6":"markdown","6e3812ad":"markdown","f2e6ab34":"markdown","5d2302c6":"markdown","aed873af":"markdown","aaf2852f":"markdown","f6345063":"markdown","d30d3e21":"markdown","dcf07319":"markdown","0843f8cc":"markdown","34402074":"markdown","7cf19b66":"markdown","b0798608":"markdown","a195070a":"markdown","6507f612":"markdown","f58cea39":"markdown","60de8889":"markdown","4fcf4ad3":"markdown","8067f3dc":"markdown","883d95b7":"markdown","19128705":"markdown","f446489e":"markdown","4c2ae7c8":"markdown","bf005f44":"markdown","6e3793bd":"markdown","7ce0e4b8":"markdown","b5eeef1b":"markdown","2bc9e417":"markdown","a21e10d1":"markdown"},"source":{"aa4deccf":"# Data Processing\nimport numpy as np\nimport pandas as pd\nimport datetime as dt\n\n# Data Visualizing\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\nimport matplotlib.gridspec as gridspec\nfrom matplotlib.ticker import MaxNLocator\nfrom IPython.display import display, HTML\nimport plotly.express as px\nimport plotly.graph_objs as go\nfrom IPython.display import display, HTML\nfrom IPython.display import Image\n\n# Data Clustering\nfrom mlxtend.frequent_patterns import apriori # Data pattern exploration\nfrom mlxtend.frequent_patterns import association_rules # Association rules conversion\n\n# Data Modeling\nfrom sklearn.ensemble import RandomForestRegressor\n\n# Math\nfrom scipy import stats  # Computing the t and p values using scipy \nfrom statsmodels.stats import weightstats \n\n# Warning Removal\nimport warnings\ndef ignore_warn(*args, **kwargs):\n    pass\nwarnings.warn = ignore_warn #ignore annoying warning (from sklearn and seaborn)","3665a01b":"# https:\/\/stackoverflow.com\/questions\/22216076\/unicodedecodeerror-utf8-codec-cant-decode-byte-0xa5-in-position-0-invalid-s\/50538501#50538501\ndf = pd.read_csv('..\/input\/ecommerce-data\/data.csv', encoding= 'unicode_escape')","8c80c00e":"df","ca35907d":"df.describe()","8d465276":"df.info()","c810f0c3":"df.columns","025a7585":"print(df.duplicated().sum())\ndf.drop_duplicates(inplace = True)","0c7b59a2":"# https:\/\/stackoverflow.com\/questions\/574730\/python-how-to-ignore-an-exception-and-proceed\/575711#575711\n# https:\/\/stackoverflow.com\/questions\/59127458\/pandas-fillna-using-groupby-and-mode\ndef cleaning_description(df):\n    try: \n        return df.mode()[0] # df.mode().iloc[0]\n    except Exception:\n        return 'unknown'\n    \ndf[['StockCode', 'Description']] = df[['StockCode', 'Description']].fillna(df[['StockCode', 'Description']].groupby('StockCode').transform(cleaning_description))\n\n# Cleaning Description field for proper aggregation\ndf['Description'] = df['Description'].str.strip().copy()","eecbbf81":"def clean_InvoiceNo(InvoiceNo):    \n    if InvoiceNo[0] == 'C':\n        return InvoiceNo.replace(InvoiceNo[0], '')\n    else:\n        return InvoiceNo\ndf['InvoiceNo'] = df['InvoiceNo'].apply(clean_InvoiceNo)","25230958":"# Plot Quantity\nplt.figure(constrained_layout=True, figsize=(12, 5))\nsns.boxplot(df['Quantity'])\n\n# remove outliers for Quantity\ndf = df[(df['Quantity'] < 15000) & (df['Quantity'] > -15000)]","965e0a9d":"# Change datatype of InvoiceDate as datetime type\ndf['InvoiceDate'] = pd.to_datetime(df['InvoiceDate'])\n# df['date'] = pd.to_datetime(df['InvoiceDate'], utc=False)\n# df['date'].dtypes\n\n# Create new features\ndf['date'] = df['InvoiceDate'].dt.date   # df['date'].dt.normalize()  # Show only date\ndf['day'] = df['InvoiceDate'].dt.day\ndf['month'] = df['InvoiceDate'].dt.month\ndf['year'] = df['InvoiceDate'].dt.year\ndf['hour'] = df['InvoiceDate'].dt.hour\ndf['dayofweek'] = df['InvoiceDate'].dt.dayofweek\ndf['dayofweek'] = df['dayofweek'].map( {0: '1_Mon', 1: '2_Tue', 2: '3_Wed', 3: '4_Thur', 4: '5_Fri', 5: '6_Sat', 6: '7_Sun'})","e4402771":"# Clean UnitPrice\n''' \nSteps to clean Unit Price\n    df['UnitPrice'].describe()\n    df[df['UnitPrice'] < 0]\n    sns.boxplot(df['UnitPrice'])\n    sns.distplot(df['UnitPrice'])\n    df[df['StockCode'] == 'M']\n    df[df['UnitPrice'] > 15000]\n'''\ndf = df[df['UnitPrice'] >= 0]","3a4600b2":"# Fill CustomerID with unknown\ndf['CustomerID'].dropna(inplace=True)","546e1a2b":"# Create a new feature Revenue\ndf['Revenue'] = df['UnitPrice'] * df['Quantity']","4a971cb1":"CustomerID_Rev = df.groupby('CustomerID')[['Revenue',\n                          'Quantity',\n                          'UnitPrice']].agg(['sum',\n                                             'mean',\n                                             'median']).sort_values(by=[('Revenue', 'sum')], ascending=False)\ndisplay(CustomerID_Rev.reset_index())\n\ndisplay(pd.DataFrame(CustomerID_Rev.iloc[1:][('Revenue','sum')].describe()))\n\n# Remove the unknown CustomerID\nsns.distplot(CustomerID_Rev.iloc[1:][('Revenue','sum')], kde=False)","6429ebca":"Item_retured = df[df['Quantity'] < 0].groupby('CustomerID')[['Revenue',\n                                              'Quantity']].agg(['sum']).sort_values(by=[('Quantity', 'sum')], ascending=True).head(10)\n\nsns.barplot(x=Item_retured.index, y=abs(Item_retured[('Quantity','sum')]))\nplt.ylabel('A number of Quantity returned')\nplt.xticks(rotation=90)\nplt.show()\n\nItem_retured","b972442b":"most_prefered_items = df.groupby(['StockCode', 'UnitPrice'])[['Quantity']].sum().sort_values(by=['Quantity'],ascending=False).head(10)\n\nmost_prefered_items","e537a4ac":"most_prefered_items1 = df.groupby(['StockCode'])[['Quantity']].sum().sort_values(by=['Quantity'],ascending=False).head(10)\n\nmost_prefered_items2 = df.groupby(['StockCode', 'UnitPrice'])[['Quantity']].sum().sort_values(by=['Quantity'],ascending=False).head(10)\n\nsns.barplot(x=most_prefered_items1.index, y=most_prefered_items1['Quantity'])\nplt.ylabel('A number of Quantity returned')\nplt.xticks(rotation=90)\nplt.show()\n\ndisplay(most_prefered_items1)\ndisplay(most_prefered_items2)","3c6073c9":"least_prefered_items = df.groupby(['StockCode'])[['Quantity']].sum().sort_values(by=['Quantity'],ascending=False)\nleast_prefered_items = least_prefered_items[least_prefered_items['Quantity']==0]\nprint('A list of least preferred items: ', len(least_prefered_items))\nleast_prefered_items","0dce4a76":"InvoiceNumber_Country = pd.DataFrame(df.groupby(['Country'])['InvoiceNo'].count())\n\nfig = go.Figure(data=go.Choropleth(\n                locations=InvoiceNumber_Country.index, # Spatial coordinates\n                z = InvoiceNumber_Country['InvoiceNo'].astype(float), # Data to be color-coded\n                locationmode = 'country names', # set of locations match entries in `locations`\n                colorscale = 'Reds',\n                colorbar_title = \"Order number\",\n            ))\n\nfig.update_layout(\n    title_text = 'Order number per country',\n    geo = dict(showframe = True, projection={'type':'mercator'})\n)\nfig.layout.template = None\nfig.show()","9673ab53":"# Source: https:\/\/stackoverflow.com\/questions\/36220829\/fine-control-over-the-font-size-in-seaborn-plots-for-academic-papers\/36222162#36222162\ncountry_revenue = df.groupby('Country')[['Revenue']].agg(['sum',\n                                        'mean',\n                                        'median']).sort_values(by=[('Revenue', 'sum')], ascending=False)\ndisplay(country_revenue)\n\nfig = plt.figure(constrained_layout=True, figsize=(20, 6))\na = sns.barplot(y=country_revenue.index, x=country_revenue[('Revenue', 'sum')])\nplt.xlabel('Total Revenue from all country', fontsize=18)\nplt.ylabel('Country', fontsize=18)\n\n\nfig = plt.figure(constrained_layout=True, figsize=(20, 6))\ncountry_revenue = country_revenue.drop('United Kingdom')\nsns.barplot(y=country_revenue.index, x=country_revenue[('Revenue', 'sum')])\nplt.xlabel('Total Revenue from all country but UK', fontsize=18)\nplt.ylabel('Country', fontsize=18)\nplt.show()\n\n","cd7f85ad":"country_quantity = df.groupby('Country')[['Quantity']].agg(['sum',\n                                        'mean',\n                                        'median']).sort_values(by=[('Quantity', 'sum')], ascending=False)\n\ndisplay(country_quantity)\n\nfig = plt.figure(constrained_layout=True, figsize=(20, 6))\na = sns.barplot(y=country_quantity.index, x=country_quantity[('Quantity', 'sum')])\nplt.xlabel('Total Quantity from all country', fontsize=18)\nplt.ylabel('Country', fontsize=18)\n\n\nfig = plt.figure(constrained_layout=True, figsize=(20, 6))\ncountry_quantity = country_quantity.drop('United Kingdom')\nsns.barplot(y=country_quantity.index, x=country_quantity[('Quantity', 'sum')])\nplt.xlabel('Total Quantity from all country but UK', fontsize=18)\nplt.ylabel('Country', fontsize=18)\nplt.show()","aac27fba":"unitprice_average = df.groupby('Country')[['UnitPrice']].agg(['sum',\n                                        'mean']).sort_values(by=[('UnitPrice', 'mean')], ascending=False)\ndisplay(unitprice_average)\n\nfig = plt.figure(constrained_layout=True, figsize=(20, 6))\na = sns.barplot(y=unitprice_average.index, x=unitprice_average[('UnitPrice', 'mean')])\nplt.xlabel('Total Quantity from all country', fontsize=18)\nplt.ylabel('Country', fontsize=18)\n\n\nfig = plt.figure(constrained_layout=True, figsize=(20, 6))\nunitprice_average = unitprice_average.drop('United Kingdom')\nsns.barplot(y=country_quantity.index, x=unitprice_average[('UnitPrice', 'mean')])\nplt.xlabel('Total Quantity from all country but UK', fontsize=18)\nplt.ylabel('Country', fontsize=18)\nplt.show()\n","35d5c528":"month_sales = df.groupby(['month'])['Revenue'].agg(['sum','mean'])\n\nfig, axes = plt.subplots(1, 2, figsize=(18, 5))\naxes = axes.flatten()\n\nsns.barplot(x=month_sales.index, y=month_sales['sum'], ax=axes[0]).set_title(\"Total Revenue over a year\")\nplt.ylabel('a')\nplt.xticks(rotation=90)\n\nsns.barplot(x=month_sales.index, y=month_sales['mean'], ax=axes[1]).set_title(\"Average Revenue over a year\")\nplt.xticks(rotation=90)\nplt.show()\n\nmonth_sales","b7c35706":"hour_sales = df.groupby(['hour'])['Revenue'].agg(['sum','mean'])\n\nfig, axes = plt.subplots(1, 2, figsize=(18, 5))\naxes = axes.flatten()\n\nsns.barplot(x=hour_sales.index, y=hour_sales['sum'], ax=axes[0]).set_title(\"Total Revenue in a day\")\nplt.ylabel('a')\nplt.xticks(rotation=90)\n\nsns.barplot(x=hour_sales.index, y=hour_sales['mean'], ax=axes[1]).set_title(\"Average Revenue per Invoice in a day\")\nplt.xticks(rotation=90)\nplt.show()\n\nhour_sales","490bff4a":"dayofweek_sales = df.groupby(['dayofweek'])['Revenue'].agg(['sum','mean',])\n\nfig, axes = plt.subplots(1, 2, figsize=(18, 5))\naxes = axes.flatten()\n\nsns.barplot(x=dayofweek_sales.index, y=dayofweek_sales['sum'], ax=axes[0]).set_title(\"Total Revenue over a week\")\nplt.ylabel('a')\nplt.xticks(rotation=90)\n\nsns.barplot(x=dayofweek_sales.index, y=dayofweek_sales['mean'], ax=axes[1]).set_title(\"Average Revenue over a week\")\nplt.xticks(rotation=90)\nplt.show()\n\ndayofweek_sales","ec33f5f1":"# Get our date range for our data\nprint('Date Range: %s to %s' % (df['InvoiceDate'].min(), df['InvoiceDate'].max()))\n\n# We're taking all of the transactions that occurred before December 01, 2011 \ndf = df[df['InvoiceDate'] < '2011-12-01']","2e76fa24":"# Get total amount spent per invoice and associate it with CustomerID and Country\ninvoice_customer_df = df.groupby(by=['InvoiceNo', 'InvoiceDate']).agg({'Revenue': sum,'CustomerID': max,'Country': max,}).reset_index()\ninvoice_customer_df","311999e9":"# Source: https:\/\/pandas.pydata.org\/pandas-docs\/stable\/user_guide\/timeseries.html#dateoffset-objects\n# We set our index to our invoice date\n# And use Grouper(freq='M') groups data by the index 'InvoiceDate' by Month\n# We then group this data by CustomerID and count the number of unique repeat customers for that month (data is the month end date)\n# The filter fucntion allows us to subselect data by the rule in our lambda function i.e. those greater than 1 (repeat customers)\n\nmonthly_repeat_customers_df = invoice_customer_df.set_index('InvoiceDate').groupby([\n              pd.Grouper(freq='M'), 'CustomerID']).filter(lambda x: len(x) > 1).resample('M').nunique()['CustomerID']\n\nmonthly_repeat_customers_df","0ecbff54":"# Number of Unique customers per month\nmonthly_unique_customers_df = df.set_index('InvoiceDate')['CustomerID'].resample('M').nunique()\nmonthly_unique_customers_df","4e929ece":"# Ratio of Repeat to Unique customers\nmonthly_repeat_percentage = monthly_repeat_customers_df\/monthly_unique_customers_df*100.0\nmonthly_repeat_percentage","66425267":"fig = plt.figure(constrained_layout=True, figsize=(20, 6))\ngrid = gridspec.GridSpec(nrows=1, ncols=1,  figure=fig)\n\nax = fig.add_subplot(grid[0, 0])\n\npd.DataFrame(monthly_repeat_customers_df.values).plot(ax=ax, figsize=(12,8))\n\npd.DataFrame(monthly_unique_customers_df.values).plot(ax=ax,grid=True)\n\nax.set_xlabel('Date')\nax.set_ylabel('Number of Customers')\nax.set_title('Number of Unique vs. Repeat Customers Over Time')\nplt.xticks(range(len(monthly_repeat_customers_df.index)), [x.strftime('%m.%Y') for x in monthly_repeat_customers_df.index], rotation=45)\nax.legend(['Repeat Customers', 'All Customers'])","753b2534":"# Let's investigate the relationship between revenue and repeat customers\nmonthly_revenue_df = df.set_index('InvoiceDate')['Revenue'].resample('M').sum()\n\nmonthly_rev_repeat_customers_df = invoice_customer_df.set_index('InvoiceDate').groupby([\n    pd.Grouper(freq='M'), 'CustomerID']).filter(lambda x: len(x) > 1).resample('M').sum()['Revenue']\n\n# Let's get a percentage of the revenue from repeat customers to the overall monthly revenue\nmonthly_rev_perc_repeat_customers_df = monthly_rev_repeat_customers_df\/monthly_revenue_df * 100.0\nmonthly_rev_perc_repeat_customers_df","8af668cb":"fig = plt.figure(constrained_layout=True, figsize=(20, 6))\ngrid = gridspec.GridSpec(nrows=1, ncols=1,  figure=fig)\n\nax = fig.add_subplot(grid[0, 0])\npd.DataFrame(monthly_rev_repeat_customers_df.values).plot(ax=ax, figsize=(12,8))\n\npd.DataFrame(monthly_revenue_df.values).plot(ax=ax,grid=True)\n\nax.set_xlabel('Date')\nax.set_ylabel('Number of Customers')\nax.set_title('Number of Unique vs. Repeat Customers Over Time')\nplt.xticks(range(len(monthly_repeat_customers_df.index)), [x.strftime('%m.%Y') for x in monthly_repeat_customers_df.index], rotation=45)\nax.legend(['Repeat Customers', 'All Customers'])","a8fb05d6":"# Now let's get quantity of each item sold per month\ndate_item_df = df.set_index('InvoiceDate').groupby([pd.Grouper(freq='M'), 'StockCode'])['Quantity'].sum()\ndate_item_df.head(15)","fe90e35a":"# Rank items by the last month's sales\nlast_month_sorted_df = date_item_df.loc['2011-11-30']\nlast_month_sorted_df = last_month_sorted_df.reset_index()\nlast_month_sorted_df.sort_values(by='Quantity', ascending=False).head(10)","4920d47b":"# Let's look at the top 5 items sale over a year\ndate_item_df = df.loc[df['StockCode'].isin(['23084', '84826', '22197', '22086', '85099B'])].set_index('InvoiceDate').groupby([\n    pd.Grouper(freq='M'), 'StockCode','Description'])['Quantity'].sum().reset_index()\n\ndate_item_df","59b9ac17":"date_item_df = date_item_df.reset_index()\n\nsns.set(style='whitegrid')\nplt.figure(constrained_layout=True, figsize=(12, 5))\nsns.lineplot(x=date_item_df['InvoiceDate'], y=date_item_df['Quantity'], hue=date_item_df['StockCode'])","21adad09":"df.groupby(['StockCode', 'Description'])['InvoiceNo'].count().sort_values(ascending = False).head(10)","948f092a":"Num_Canceled_Orders = df[df['Quantity']<0]['InvoiceNo'].nunique()\nTotal_Orders = df['InvoiceNo'].nunique()\nprint('Cancellation Rate: {:.2f}%'.format(Num_Canceled_Orders\/Total_Orders*100 ))","d49f7785":"Monthly_Reorder_Items_Revenue = df.set_index('InvoiceDate').groupby([ pd.Grouper(freq='M'), 'StockCode']).filter(lambda x: len(x) > 1).resample('M').sum()['Revenue']\nMonthly_One_Items_Revenue = df.set_index('InvoiceDate').groupby([ pd.Grouper(freq='M'), 'StockCode']).filter(lambda x: len(x) == 1).resample('M').sum()['Revenue']\n#Monthly_Revenue = df.groupby(['year','month']).sum()['Revenue']  # Generate the same Result\nMonthly_Revenue = df.set_index('InvoiceDate').groupby([pd.Grouper(freq='M')]).sum()['Revenue']","ac233315":"fig = plt.figure(constrained_layout=True, figsize=(20, 6))\n\nax = fig.add_subplot()\npd.DataFrame(Monthly_Reorder_Items_Revenue.values).plot(ax=ax, figsize=(12,8))\npd.DataFrame(Monthly_Revenue.values).plot(ax=ax,grid=True)\npd.DataFrame(Monthly_One_Items_Revenue.values).plot(ax=ax,grid=True)\n\nax.set_xlabel('Date')\nax.set_ylabel('Number of Customers')\nax.set_title('Number of Unique vs. Repeat vs Total Items Over Time')\nplt.xticks(range(len(monthly_repeat_customers_df.index)), [x.strftime('%m.%Y') for x in monthly_repeat_customers_df.index], rotation=45)\nax.legend(['Repeat Items', 'All Items', 'One Item'])","3bd2ac4e":"Sample_df = df[:50]\nSample_df = Sample_df[['InvoiceNo', 'Description']]","48433533":"Sample_df.set_index('InvoiceNo', inplace=True)","16ca165b":"# Note that the quantity bought is not considered, only if the item was present or not in the basket\nbasket = pd.get_dummies(Sample_df)\nbasket_sets = pd.pivot_table(basket, index='InvoiceNo', aggfunc='sum')\nbasket_sets","cbef5014":"# Apriori aplication: frequent_itemsets\n# Note that min_support parameter was set to a very low value, this is the Spurious limitation, more on conclusion section\nfrequent_itemsets = apriori(basket_sets, min_support=0.22, use_colnames=True)\nfrequent_itemsets['length'] = frequent_itemsets['itemsets'].apply(lambda x: len(x))\nfrequent_itemsets","df754fce":"# Advanced and strategical data frequent set selection\nfrequent_itemsets[ (frequent_itemsets['length'] > 1) &\n                   (frequent_itemsets['support'] >= 0.02)]","69e19128":"# Generating the association_rules: rules\n# Selecting the important parameters for analysis\nrules = association_rules(frequent_itemsets, metric=\"lift\", min_threshold=1)\nrules","81003602":"rules = association_rules(frequent_itemsets, metric=\"lift\", min_threshold=1)\nrules[['antecedents', 'consequents', 'support', 'confidence', 'lift']].sort_values('support', ascending=False).head()","500f4deb":"# Visualizing the rules distribution color mapped by Lift\nplt.figure(figsize=(14, 8))\nplt.scatter(rules['support'], rules['confidence'], c=rules['lift'], alpha=0.9, cmap='YlOrRd');\nplt.title('Rules distribution color mapped by lift');\nplt.xlabel('Support')\nplt.ylabel('Confidence')\nplt.colorbar();","09bef4cb":"# df.InvoiceDate = pd.to_datetime(df.InvoiceDate, format=\"%m\/%d\/%Y %H:%M\")\ndf['InvoiceDate'] = pd.to_datetime(df['InvoiceDate'])\n\ndf['Revenue'] = df['Quantity']*df['UnitPrice']","714c45ca":"invoice_ct = df.groupby(by='CustomerID', as_index=False)['InvoiceNo'].count()\ninvoice_ct.columns = ['CustomerID', 'NumberOrders']\ninvoice_ct","1504f5ef":"unitprice = df.groupby(by='CustomerID', as_index=False)['UnitPrice'].mean()\nunitprice.columns = ['CustomerID', 'Unitprice']\nunitprice","ddb2683a":"revenue = df.groupby(by='CustomerID', as_index=False)['Revenue'].sum()\nrevenue.columns = ['CustomerID', 'Revenue']\nrevenue","d19c5c25":"total_items = df.groupby(by='CustomerID', as_index=False)['Quantity'].sum()\ntotal_items.columns = ['CustomerID', 'NumberItems']\ntotal_items","dc5c26bd":"earliest_order = df.groupby(by='CustomerID', as_index=False)['InvoiceDate'].min()\nearliest_order","e087520c":"earliest_order.columns = ['CustomerID', 'EarliestInvoice']","81019834":"earliest_order['now'] = pd.to_datetime((df['InvoiceDate']).max())","3dcc9d2a":"earliest_order","a109f3bc":"# == earliest_order['days_as_customer'] = 1 + (earliest_order.now-earliest_order.EarliestInvoice).dt.days\n# Source: https:\/\/kite.com\/python\/docs\/pandas.core.indexes.accessors.TimedeltaProperties\nearliest_order['days_as_customer'] = 1 + (earliest_order['now']-earliest_order['EarliestInvoice']).dt.days","bb6c15e1":"earliest_order.drop('now', axis=1, inplace=True)\nearliest_order","4f418b7a":"# when was their last order and how long ago was that from the last date in file (presumably\n# when the data were pulled)\nlast_order = df.groupby(by='CustomerID', as_index=False)['InvoiceDate'].max()\nlast_order.columns = ['CustomerID', 'last_purchase']\nlast_order['now'] = pd.to_datetime((df['InvoiceDate']).max())\nlast_order['days_since_last_purchase'] = 1 + (last_order.now-last_order.last_purchase).astype('timedelta64[D]')\nlast_order.drop('now', axis=1, inplace=True)\nlast_order","332d2675":"#combine all the dataframes into one\nimport functools\ndfs = [invoice_ct,unitprice,revenue,earliest_order,last_order,total_items]\nCustomerTable = functools.reduce(lambda left,right: pd.merge(left,right,on='CustomerID', how='outer'), dfs)\nCustomerTable['OrderFrequency'] = CustomerTable['NumberOrders']\/CustomerTable['days_as_customer']\nCustomerTable","f2efb945":"CustomerTable.corr()['Revenue'].sort_values(ascending = False)","6f656feb":"x = CustomerTable[['NumberOrders','Unitprice', 'days_as_customer', 'days_since_last_purchase', 'NumberItems', 'OrderFrequency']]\ny = CustomerTable['Revenue']","3a83099e":"reg = RandomForestRegressor()\nreg.fit(x.values, y)\n\n#list(zip(x, reg.feature_importances_))\ncoef = pd.Series(reg.feature_importances_, index = x.columns)\n\nimp_coef = coef.sort_values()\nimp_coef.plot(kind = \"barh\")\nplt.title(\"Feature importance using Linear Model\")","68f20d6f":"recency = df.groupby(by='CustomerID', as_index=False)['InvoiceDate'].max()\nrecency.columns = ['CustomerID', 'last_purchase']\nrecency['now'] = pd.to_datetime((df['InvoiceDate']).max())\nrecency['Recency'] = 1 + (recency.now-recency['last_purchase']).astype('timedelta64[D]')\nrecency.drop(['now','last_purchase'], axis=1, inplace=True)\nrecency.head()","d9d39d5c":"#check frequency of customer means how many transaction has been done..\n\nfrequency = df.copy()\nfrequency.drop_duplicates(subset=['CustomerID','InvoiceNo'], keep=\"first\", inplace=True) \nfrequency = frequency.groupby('CustomerID',as_index=False)['InvoiceNo'].count()\nfrequency.columns = ['CustomerID','Frequency']\nfrequency.head()","edfea57c":"monetary=df.groupby('CustomerID',as_index=False)['Revenue'].sum()\nmonetary.columns = ['CustomerID','Monetary']\nmonetary.head()","ed6c1723":"dfs = [recency, frequency, monetary]\nrfm = functools.reduce(lambda left,right: pd.merge(left,right,on='CustomerID', how='outer'), dfs)","a295c60c":"rfm","3ca5bf5e":"#bring all the quartile value in a single dataframe\nrfm_segmentation = rfm.copy()","5e3e5484":"rfm_segmentation","91755623":"from sklearn.cluster import KMeans\nSSE_to_nearest_centroid = []\n\nfor k in range(1,15):\n    kmeans = KMeans(n_clusters=k)\n    kmeans.fit(rfm_segmentation)\n    SSE_to_nearest_centroid.append(kmeans.inertia_)\n\nplt.figure(figsize=(20,8))\nplt.plot(range(1,15),SSE_to_nearest_centroid,\"-o\")\nplt.title(\"SSE \/ K Chart\", fontsize=18)\nplt.xlabel(\"Amount of Clusters\",fontsize=14)\nplt.ylabel(\"Inertia (Mean Distance)\",fontsize=14)\nplt.xticks(range(1,20))\nplt.grid(True)\nplt.show()","d59c1758":"#fitting data in Kmeans theorem.\nkmeans = KMeans(n_clusters=3, random_state=0).fit(rfm_segmentation)\n\n# this creates a new column called cluster which has cluster number for each row respectively.\nrfm_segmentation['cluster'] = kmeans.labels_\nrfm_segmentation.head()","39c738c3":"plt.figure(figsize=(8,5))\nsns.boxplot(rfm_segmentation['cluster'],rfm_segmentation.Recency)\n\nplt.figure(figsize=(8,5))\nsns.boxplot(rfm_segmentation['cluster'],rfm_segmentation.Frequency)\n\nplt.figure(figsize=(8,5))\nsns.boxplot(rfm_segmentation['cluster'],rfm_segmentation.Frequency)","e9e5f6ba":"quantile = rfm.quantile(q=[0.25,0.5,0.75])\nquantile","06347eca":"# lower the recency, good for store..\ndef RScore(x):\n    if x <= quantile['Recency'][0.25]:\n        return 1\n    elif x <= quantile['Recency'][0.50]:\n        return 2\n    elif x <= quantile['Recency'][0.75]: \n        return 3\n    else:\n        return 4\n\n# higher value of frequency and monetary lead to a good consumer.\ndef FScore(x):\n    if x <= quantile['Frequency'][0.25]:\n        return 4\n    elif x <= quantile['Frequency'][0.50]:\n        return 3\n    elif x <= quantile['Frequency'][0.75]: \n        return 2\n    else:\n        return 1\n\ndef MScore(x):\n    if x <= quantile['Monetary'][0.25]:\n        return 4\n    elif x <= quantile['Monetary'][0.50]:\n        return 3\n    elif x <= quantile['Monetary'][0.75]: \n        return 2\n    else:\n        return 1","75e55713":"rfm_segmentation","43f87853":"rfm_segmentation['R_quartile'] = rfm_segmentation['Recency'].apply(RScore)\nrfm_segmentation['F_quartile'] = rfm_segmentation['Frequency'].apply(FScore)\nrfm_segmentation['M_quartile'] = rfm_segmentation['Monetary'].apply(MScore)","93eb5522":"rfm_segmentation","f4e22742":"# Approach 1: group customer's attributes, leading to detail customer's profile\n# for example 121 and 112 are different.\nrfm_segmentation['RFMScore'] = rfm_segmentation['R_quartile'].astype(str) \\\n                               + rfm_segmentation['F_quartile'].astype(str) \\\n                               + rfm_segmentation['M_quartile'].astype(str)","fcb55300":"# Approach 2: group customer's attributes, leading to more general customers' profile\n# for example 121 and 112 are the same.\nrfm_segmentation['TotalScore'] = rfm_segmentation['R_quartile'] \\\n                               + rfm_segmentation['F_quartile'] \\\n                               + rfm_segmentation['M_quartile']","763f6cbf":"print(\"Best Customers: \",len(rfm_segmentation[rfm_segmentation['RFMScore']=='111']))\nprint('Loyal Customers: ',len(rfm_segmentation[rfm_segmentation['F_quartile']==1]))\nprint(\"Big Spenders: \",len(rfm_segmentation[rfm_segmentation['M_quartile']==1]))\nprint('Almost Lost: ', len(rfm_segmentation[rfm_segmentation['RFMScore']=='134']))\nprint('Lost Customers: ',len(rfm_segmentation[rfm_segmentation['RFMScore']=='344']))\nprint('Lost Cheap Customers: ',len(rfm_segmentation[rfm_segmentation['RFMScore']=='444']))\n\nImage(url= \"https:\/\/i.imgur.com\/YmItbbm.png?\")","f2241b3c":"rfm_segmentation.sort_values(by=['RFMScore', 'Monetary'], ascending=[True, False])","c78b15bb":"rfm_segmentation.groupby('RFMScore')['Monetary'].mean()","3677aa6b":"Score_Recency = rfm_segmentation.groupby('TotalScore')['Recency'].mean().reset_index()\nScore_Monetatry = rfm_segmentation.groupby('TotalScore')['Monetary'].mean().reset_index()\nScore_Frequency = rfm_segmentation.groupby('TotalScore')['Frequency'].mean().reset_index()","2d271fde":"sns.barplot(x=Score_Recency['TotalScore'],y=Score_Recency['Recency'])\n\nplt.figure(constrained_layout=True, figsize=(12, 4))\n\nplt.subplot(1,2,1)\nsns.barplot(x=Score_Frequency['TotalScore'],y=Score_Frequency['Frequency'])\n\nplt.subplot(1,2,2)\nsns.barplot(x=Score_Monetatry['TotalScore'],y=Score_Monetatry['Monetary'])\nplt.subplots_adjust(wspace = 0.2)","9304f895":"def get_month(x): \n    return dt.datetime(x.year, x.month, 1)","d315a4ec":"df['InvoiceMonth'] = df['InvoiceDate'].apply(get_month)","05ce73b7":"# https:\/\/stackoverflow.com\/questions\/27517425\/apply-vs-transform-on-a-group-object\n# explain the difference between   apply - transform. In this case, use transform for CohortMonth.\n# CohortMonth: the first time a customer came to our retail store.\ndf['CohortMonth'] = df.groupby('CustomerID')['InvoiceMonth'].transform('min')","c8b6a801":"def get_date_int(df, column):\n    year = df[column].dt.year\n    month = df[column].dt.month\n    day = df[column].dt.day\n    return year, month, day","7ee22ebb":"invoice_year, invoice_month, _ = get_date_int(df, 'InvoiceMonth')\ncohort_year, cohort_month, _ = get_date_int(df, 'CohortMonth')\n\nyears_diff = invoice_year - cohort_year\nmonths_diff = invoice_month - cohort_month\n\ndf['CohortIndex'] = years_diff * 12 + months_diff + 1\n\ndf.head()","9ad72dc3":"## grouping customer berdasarkan masing masing cohort\ncohort_data = df.groupby(['CohortMonth', 'CohortIndex'])['CustomerID'].nunique().reset_index()\n# To solve the problem when ploting heatmap diagram below.\ncohort_data['CohortMonth'] = cohort_data['CohortMonth'].dt.date\ncohort_counts = cohort_data.pivot(index='CohortMonth', columns='CohortIndex', values='CustomerID')","60ede2a2":"cohort_counts","a0273bbb":"cohort_sizes = cohort_counts.iloc[:,0]\nretention = cohort_counts.divide(cohort_sizes, axis=0)\nretention.round(2) * 100","cce4c0da":"plt.figure(figsize=(15, 8))\nplt.title('Retention rates')\nsns.heatmap(data = retention,\n            annot = True,\n            fmt = '.0%',\n            vmin = 0.0, vmax = 0.5,\n            cmap = 'BuGn')\nplt.show()","3f2b583c":"### Question 9: Are there any relationship between Repeat Customers and All Customers over a year?\n- Investigating the number of Repeat Customers and All Customers\n- Looking at the revenue generated from the Repeat Customers and All Customers","3859799d":"# LIBRARY","7b9ce6d9":"#### Observation\n- Recency graph: cluster 0 have high recency rate which is bad, yet cluster 1 and cluster 2 having low so they are in race of platinum and gold customer.\n- Frequency graph: cluster 1 and cluster 2 having low so they are in race of platinum and gold customer regarding frequency metrics.\n- Monetary graph: cluster 1 have highest Montary (money spend) platinum where as cluster 2 have medium level(Gold) and cluster 0 is silver customer.","956293ed":"# DATA EXPLORATION","7716ab24":"### Question 12: What is the Mall's Cancellation Rate?","ea3d77b9":"# COHORT ANALYSIS\n- It is a subset of behavioral analytics that takes the data from a given eCommerce platform, web application, or online game and rather than looking at all users as one unit, it breaks them into related groups for analysis.\n- A cohort is a group of users who share a common characteristic. For example, all users with the same Acquisition Date belong to the same cohort. \n- The retention rate show the percentage of customers return in the following months after the their first purchase.\n- Customer acquisition cost is so expensive that we have to do remarketing our clients to retain them. If The retention rate is low, it means we have to spend more budget amount to acquire more customers to visit.","c268b3db":"### Question 13: The revenue comes from repeat items or 1 items per month?","27f63df8":"### Question 6: Which month we sell out most and least?\n- As we see, from January to  August, the revenue makes a gradual increase from \\\\$560K to \\\\$700K.\n- Towards the end of the year, sales make a huge jump to over a million and peak in November with \\\\$1461K\n- However, looking at average revenue diagram indicates nothing change drastically.","9b94efb6":"# E-COMMERCE DATA\n\n### Author: Vu Duong\n\n#### Date: June, 2020\n\n# CREDITS:\n\nThis work is inspired by multiple greate sources done before:\n- https:\/\/www.kaggle.com\/admond1994\/e-commerce-data-eda\/notebook\n- https:\/\/www.kaggle.com\/fabiendaniel\/customer-segmentation\n- https:\/\/www.udemy.com\/course\/data-science-deep-learning-for-business-20-case-studies\/\n- https:\/\/www.kaggle.com\/ostrowski\/market-basket-analysis-exploring-e-commerce-data\n- https:\/\/www.geeksforgeeks.org\/implementing-apriori-algorithm-in-python\/\n- https:\/\/www.kaggle.com\/carrie1\/customer-insights\n- https:\/\/www.kaggle.com\/yugagrawal95\/rfm-analysis\n- https:\/\/www.kaggle.com\/fszlnwr\/customer-segmentation-rfm-cohort-analysis","6e3812ad":"# RFM - Recency Frequency Monetary ","f2e6ab34":"### Quantity\n- Most invoices cluster around 0, in a range of [-10000, 10000].\n- There are some outliers way above and below the range above. We can safely remove for the sake of revenue analysis.","5d2302c6":"### CustomerID\n- There are multiple unknown customers, yet we know from which country the invoice comes from. Thus, we should fill missing value of customerID with 'unknown' rather than filtering out those rows containing unknown customerID.","aed873af":"### Question 7: What time do people tend to buy our products?\n- At 6 o'clock, people may want to return undesired stuff\n- Starting from 7 am, people tend to make purchase on the online retail. As we can see the revenue hit the top at 12pm. Afterwards, sales gradually decrease till 18pm. After that only a few of customers left make purchases.\n- Taking a look at the 2nd image, average revenue for an invoice at 7 am is substantially higher than the rest hours in a day. It suggests people make a huge quantity of items per transaction at the beginning of a day. ","aaf2852f":"### Invoice Number\n- Invoice Number should be in a form of 6-digit integral number. If the code starts with a letter C, it shows the invoice is cancelled.\n- However, some codes that don't start with the letter C are categorized with cancellation by a negative number from Quantity feature.\n- Thus, cleaning the starting letter C is right.","f6345063":"### Question 3: Who, customerID, is likely to return the product?\n- As we can see, customerIDs of 15838 and 15749 return so many items, 9361 and 9014 respectively, which bring a huge deduction in our Revenue overal.","d30d3e21":"### RandomForest Regression\n#### Observation:\n- Apply 6 new features such as NumberOrders, Unitprice, days_as_customer, days_since_purchase, NumberItems, OrderFrequency to regression model to predict which features have most influence on Revenue the company receive, then accordingly running marketing campaigns to yeild highest profit. \n- The diagram below indicates NumberOrders and UnitPrice are 2 most important factors of forming revenue.\n- As we already discuss above, having higher price may have traded off against NumberOrders, thus the next step for the company is to run the A\/B Test to know if we should increase UnitPrice followed by a deduction of NumberOrder and vice versa.\n- days-as-customers and days-since-purcharse may not contribute much to see if a customer is loyal and bring most revenue to us. ","dcf07319":"### UnitPrice\n- Generally, unit price is at least 0, thus any unit price is below the baseline is considered as outliers.","0843f8cc":"### Question 8: Which day of a week people tend to visit and purchase stuff?\n- It seems sales goes up and down during week days.\n- At the weekend, there is no transaction on Saturday, and sales on Sunday is just as a half or a third as compared to weekdays.\n- 2 Images below look the same in distribution","34402074":"### Question 4: Which item is bought most and least?\n- StockCode of 22197 and 84007 are leading at the price of 0.72, 0.29 & 0.21 in this question. This somehow can explain if the unit price is low, people are able to afford more.\n- There are about 14 least preferred items among all.\n- Some items with negative quantity figure are not considered in this analysis, because those items were bought before the period of this dataset, probably in 2009 and they were returned in 2010 or 2011. Thus, we have not enough evidence to analyze. ","7cf19b66":"### InvoiceDate\n- Applying feature extraction on InvoiceDate to get new features such as date, day, month, year, hour, day of week for further analysis.","b0798608":"### Observation:\n- Based on Recency, categories 10,11,12 have highest value which is good for model. because it could have combination of values such as 444, 434, 334 etc.\n- Based on Frequency, categories 3,4,5 have highest value which is good for model. because it could have combination of values such as 111, 121, 122 etc.\n- Based on Monetary, categories 3,4,5 have highest value which is good for model. because it could have combination of values such as 111, 121, 122 etc.","a195070a":"#### Observation:\n- NumberOrders and UnitPrice are 2 most important factors of forming revenue.\n- days-as-customers and days-since-purcharse may not contribute much to see if a customer is loyal and bring most revenue to us.","6507f612":"#### Observation:\n- The graph show customer numbers as percentage","f58cea39":"# DATA ANALYSIS\nFor the curiosity of data, we conduct a depth analysis based on our following questions:\n1. Who, customerID, bring most revenue?\n2. Who, customerID, buy most in term of quantity?\n3. Who, customerID, is likely to return the product?\n4. Which item is bought most and least?\n5. Which country bring most revenue in total and average?\n6. Which month we sell out most and least?\n7. What time people tend to buy our product?\n8. Which day of a week people tend to visit and purchase stuff?\n9. Are there any relationship between Repeat Customers and All Customers over a year?\n10. What is the most trending of some items?","60de8889":"### Revenue\n- Revenue is the product of UnitPrice and Quantity from each transaction.","4fcf4ad3":"# MARKET BASKET ANALYSIS\n- The solution focus on improving marketing performance upon data driven.\n- Applying Association Rule with Apriori Algorithm to extract frequent itemsets in data mining.\n- For further explanation, find this source: https:\/\/stackabuse.com\/association-rule-mining-via-apriori-algorithm-in-python\/\n- Final thought: Apriori is very useful for finding simple associations between our data items. They are easy to implement and have high explain-ability.","8067f3dc":"# INTRODUCTION\n\nThis is a transnational data set which contains all the transactions occurring between 01\/12\/2010 and 09\/12\/2011 for a UK-based and registered non-store online retail.The company mainly sells unique all-occasion gifts. Many customers of the company are wholesalers\n\nDetailed description of dataset content is described in the following link:\nhttps:\/\/www.kaggle.com\/carrie1\/ecommerce-data","883d95b7":"### Question 5: Which country bring most revenue in total and average?\n- This is the e-commerce UK-based online retail, so United Kingdom brings most revenue and quantity.\n- However, Netherlands only comes as the second place, but spend quite much, around \\\\$120 with 84 in quantity for each transaction.\n- Average UnitPrices per Invoice from Singapore and HongKong, around \\\\$109 and \\\\$42, outstand from that of rest, only between \\\\$2 and \\\\$8.5 ","19128705":"### More granularity level of analysis","f446489e":"### Question 1 and 2: Who, customerID, bring most revenue? And, who, customerID, buy most in term of quantity?\n- Regardless unknown customerID, we notice CustomerID of 14646 bring \\\\$279489 in total, and an amount of each transaction is around \\\\$134. A total number of items are bought from this customer is 196719, however the unit price for each item is approximately \\\\$2.6.\n- Most people bring revenue between 0 and \\\\$1608","4c2ae7c8":"# MODELING\n- RandomForest Regression","bf005f44":"### Check for any Duplicated Rows","6e3793bd":"### Observation:\n- CohortMonth of 2021-12-01 indicates 949 distinct customers when they first came (CohortIndex 1), \n- The following month (CohortIndex 2) has 363 repeat customers, so on. ","7ce0e4b8":"### Description\n- For any StockCode, there is one specific description along.","b5eeef1b":"# DATA MANIPULATION","2bc9e417":"### Question 10: What are the item trends?\n\nLet's count the number of items sold for each product for each period.","a21e10d1":"### Question 11: Top 10 Reorder Items"}}