{"cell_type":{"34459fb6":"code","7fff46c8":"code","abeb9d94":"code","302d6353":"code","b050613c":"code","33f55c5b":"code","0c044ac2":"code","7b2910b7":"code","8d5b9bbf":"code","53a671dc":"code","1a72df98":"code","9e1c1b0e":"code","c9abda7a":"markdown","01c624dc":"markdown"},"source":{"34459fb6":"import pandas as pd \nimport numpy as np\npd.set_option('display.max_columns', 50)\nfrom sklearn.metrics import mean_squared_error\nimport math","7fff46c8":"train = pd.read_csv('..\/input\/30-days-of-ml\/train.csv')\ntest = pd.read_csv('..\/input\/30-days-of-ml\/test.csv')\nsubmission = pd.read_csv('..\/input\/30-days-of-ml\/sample_submission.csv')","abeb9d94":"# take a look at the data\ntrain.head()","302d6353":"# define the features for convenience.\nCAT_FEATS = ['cat0', 'cat1', 'cat2', 'cat3', 'cat4', 'cat5', 'cat6', 'cat7', 'cat8', 'cat9']\nNUM_FEATS = ['cont0', 'cont1', 'cont2', 'cont3', 'cont4', 'cont5', 'cont6', 'cont7', 'cont8', 'cont9', 'cont10', 'cont11', 'cont12', 'cont13']\nALL_FEATS = CAT_FEATS + NUM_FEATS\n\nTARGET = 'target'","b050613c":"# the concatenation is for preprocess.\n# because we want to apply LabelEncoder to all the data including test\ndata = pd.concat([train, test])","33f55c5b":"from sklearn.preprocessing import LabelEncoder\n\n# encode all the categorical features.\n# in this notebook, we don't use one hot encoding as LGBM can deal with categorical features\nfor cat in CAT_FEATS:\n    le = LabelEncoder()\n    data[cat] = le.fit_transform(data[cat])","0c044ac2":"# take a look at the data after preprocess\ndata.head()","7b2910b7":"# split the encoded data into train and test again\ntrain_size = train.shape[0]\ntrain = data[:train_size]\ntest = data[train_size:]","8d5b9bbf":"# parameter for LGBM. The parameter is not optimized. Feel free changing these values\nparams = {\n     'max_bin': 500,\n    'feature_fraction': 0.78,\n    'bagging_fraction': 0.78,\n    'objective': 'regression',\n    'max_depth': -1,\n    'learning_rate': 0.01,\n    \"bagging_seed\": 42,\n    'random_state': 42,\n    \"metric\": 'mse',\n    \"verbosity\": -1,\n}","53a671dc":"from sklearn.model_selection import KFold\nimport lightgbm as lgb\n\n# parameters for CV\nN_FOLDS = 5\nseed = 42\n\n# to preserve the prediction for CV\nval_pred = train[TARGET].copy()\nval_pred.iloc[:] = 0\n\n# to store the prediction for the test\npred = submission['target'].copy()\npred.iloc[:] = 0\n\n# CV starts here.\nfor fold, (train_idx, valid_idx) in enumerate(KFold(n_splits=N_FOLDS, random_state=seed, shuffle=True).split(train)):\n    \n    # split the train data into train and validation\n    X_train = train.loc[train_idx, ALL_FEATS]\n    X_valid = train.loc[valid_idx, ALL_FEATS]\n    y_train = train.loc[train_idx, TARGET]\n    y_valid = train.loc[valid_idx, TARGET]\n    \n    tr_data = lgb.Dataset(X_train[ALL_FEATS].values.astype(np.float32), label=y_train)\n    va_data = lgb.Dataset(X_valid[ALL_FEATS].values.astype(np.float32), label=y_valid)\n\n    # set the random seed of LGBM param\n    params['random_state'] = seed\n    params['bagging_seed'] = seed\n\n    model = lgb.train(\n        params, \n        tr_data,\n        num_boost_round=10000,\n        valid_sets=[tr_data, va_data],\n        early_stopping_rounds=30, # passing \"valid_sets\", we can use early stopping. if the validation score doesn't improve for 30 rounds, it automatically stops the learning\n        feature_name=ALL_FEATS,\n        categorical_feature=CAT_FEATS, # we can tell which features are categorical.\n        verbose_eval=10\n    )\n\n    # we can save the model so if we want to use the trained model later, we can use this.\n    # we won't use this model in this notebook though.\n    model.save_model(f'lgbm_{fold}_{seed}.md')\n\n    # make prediction for calculate CV later\n    val_pred.loc[valid_idx] = model.predict(X_valid)\n    # make prediction for test set. just add up all the folds\n    pred += model.predict(test[ALL_FEATS])\n\n# as we added up all the predictions of folds, we have to divide the pred with number of fold.\npred \/= N_FOLDS","1a72df98":"# show the CV value.\nrmse = math.sqrt(mean_squared_error(train.target, val_pred))\nprint(f'CV: {rmse}')","9e1c1b0e":"# make submission\nsubmission['target'] = pred\nsubmission.to_csv('submission.csv', index=False)","c9abda7a":"## LightGBM Starter\n\nIn this notebook, we will make prediction with one of the most popular ML framework, [LightGBM](https:\/\/lightgbm.readthedocs.io\/en\/latest\/index.html).\n\nApplying LGBM is very easy and is very powerful. \n\nI hope you will find it useful. ","01c624dc":"## Preprocess\nTo make the data understandable for LGBM, we have to encode the categorical features into something.\n\nIn this notebook, we use LabelEncoder to make these categorical features into numbers. "}}