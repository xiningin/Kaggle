{"cell_type":{"4687776a":"code","207a6a44":"code","ab2c0d6b":"code","cce3666b":"code","48f0a0dc":"code","77e86f39":"code","75a9af2c":"code","504a1165":"code","588f5076":"code","920e5ce4":"code","db1943d2":"code","5f7d83fc":"code","17db09cd":"code","9814ae90":"code","2f166a75":"code","d4197191":"code","bd31b3e5":"code","80d2d5bd":"code","d0444a58":"code","7217c0c3":"code","506e0f71":"code","a5723f78":"code","cd8b4a52":"code","40997692":"code","62ebc53e":"markdown"},"source":{"4687776a":"import pandas as pd\nimport numpy as np\nimport os\nimport gc \ndevice = 'cuda'","207a6a44":"!pip uninstall fsspec -qq -y\n!pip uninstall transformers -y\n!pip install --no-index --find-links ..\/input\/hf-datasets\/wheels datasets -qq\n\nimport sys\nsys.path.append(\"..\/input\/transformers-master\/src\/\")","ab2c0d6b":"sub = pd.read_csv('..\/input\/chaii-hindi-and-tamil-question-answering\/sample_submission.csv')\nsub.head(1)","cce3666b":"train = pd.read_csv('..\/input\/chaii-hindi-and-tamil-question-answering\/train.csv')\ntrain.head()","48f0a0dc":"train.language.value_counts()","77e86f39":"test = pd.read_csv('..\/input\/chaii-hindi-and-tamil-question-answering\/test.csv')\ntest.head()","75a9af2c":"len(test)","504a1165":"import transformers","588f5076":"model_checkpoint = '..\/input\/chaii-model-0816-2\/chaii-bert-trained_knr_alex'\nbatch_size = 200","920e5ce4":"from transformers import AutoTokenizer\n","db1943d2":"max_length = 384 # The maximum length of a feature (question and context)\ndoc_stride = 128 # The authorized overlap between two part of the context when splitting it is needed.","5f7d83fc":"from datasets import Dataset","17db09cd":"def convert_answers(r):\n    start = r[0]\n    text = r[1]\n    return {\n        'answer_start': [start],\n        'text': [text]\n    }","9814ae90":"%env WANDB_DISABLED=True","2f166a75":"from transformers import AutoModelForQuestionAnswering, TrainingArguments, Trainer, AutoTokenizer, default_data_collator\nargs = TrainingArguments(\n    f\"chaii-qa\",\n    do_eval=False,\n    do_predict=True,\n    dataloader_num_workers=2,\n    per_device_eval_batch_size=batch_size,\n)","d4197191":"from transformers import default_data_collator\nfrom scipy.special import softmax\ndata_collator = default_data_collator\nmax_answer_length = 30\nfrom tqdm.auto import tqdm\n\nclass Prep:\n    def __init__(self, tokenizer):\n        self.tokenizer = tokenizer\n        \n    def prepare_validation_features(self, examples):\n        \n        # Some of the questions have lots of whitespace on the left, which is not useful and will make the\n        # truncation of the context fail (the tokenized question will take a lots of space). So we remove that\n        # left whitespace\n        examples[\"question\"] = [q.lstrip() for q in examples[\"question\"]]\n        pad_on_right = self.tokenizer.padding_side == \"right\"\n\n        # Tokenize our examples with truncation and maybe padding, but keep the overflows using a stride. This results\n        # in one example possible giving several features when a context is long, each of those features having a\n        # context that overlaps a bit the context of the previous feature.\n        tokenized_examples = self.tokenizer(\n            examples[\"question\" if pad_on_right else \"context\"],\n            examples[\"context\" if pad_on_right else \"question\"],\n            truncation=\"only_second\" if pad_on_right else \"only_first\",\n            max_length=max_length,\n            stride=doc_stride,\n            return_overflowing_tokens=True,\n            return_offsets_mapping=True,\n            padding=\"max_length\",\n        )\n\n        # Since one example might give us several features if it has a long context, we need a map from a feature to\n        # its corresponding example. This key gives us just that.\n        sample_mapping = tokenized_examples.pop(\"overflow_to_sample_mapping\")\n\n        # We keep the example_id that gave us this feature and we will store the offset mappings.\n        tokenized_examples[\"example_id\"] = []\n\n        for i in range(len(tokenized_examples[\"input_ids\"])):\n            # Grab the sequence corresponding to that example (to know what is the context and what is the question).\n            sequence_ids = tokenized_examples.sequence_ids(i)\n            context_index = 1 if pad_on_right else 0\n\n            # One example can give several spans, this is the index of the example containing this span of text.\n            sample_index = sample_mapping[i]\n            tokenized_examples[\"example_id\"].append(examples[\"id\"][sample_index])\n\n            # Set to None the offset_mapping that are not part of the context so it's easy to determine if a token\n            # position is part of the context or not.\n            tokenized_examples[\"offset_mapping\"][i] = [\n                (o if sequence_ids[k] == context_index else None)\n                for k, o in enumerate(tokenized_examples[\"offset_mapping\"][i])\n            ]\n\n        return tokenized_examples\n\n    def postprocess_qa_predictions(self, examples, features, raw_predictions, n_best_size = 20, max_answer_length = 30):\n        all_start_logits, all_end_logits = raw_predictions\n        #all_start_logits = expit(all_start_logits)\n        #all_end_logits = expit(all_end_logits)\n        # Build a map example to its corresponding features.\n        example_id_to_index = {k: i for i, k in enumerate(examples[\"id\"])}\n        features_per_example = collections.defaultdict(list)\n        for i, feature in enumerate(features):\n            features_per_example[example_id_to_index[feature[\"example_id\"]]].append(i)\n\n        # The dictionaries we have to fill.\n        predictions = collections.OrderedDict()\n\n        # Logging.\n        print(f\"Post-processing {len(examples)} example predictions split into {len(features)} features.\")\n\n        #ta_stemmer = TamilStemmer()\n\n        # Let's loop over all the examples!\n        for example_index, example in enumerate(tqdm(examples)):\n            # Those are the indices of the features associated to the current example.\n            feature_indices = features_per_example[example_index]\n\n            min_null_score = None # Only used if squad_v2 is True.\n            valid_answers = []\n\n            context = example[\"context\"]\n            start_flgs = np.ones(len(context)) * (-100)\n            end_flgs = np.ones(len(context)) * (-100)\n            # Looping through all the features associated to the current example.\n            for feature_index in feature_indices:\n                # We grab the predictions of the model for this feature.\n                start_logits = all_start_logits[feature_index]\n                end_logits = all_end_logits[feature_index]\n                # This is what will allow us to map some the positions in our logits to span of texts in the original\n                # context.\n                offset_mapping = features[feature_index][\"offset_mapping\"]\n\n                # Update minimum null prediction.\n                cls_index = features[feature_index][\"input_ids\"].index(self.tokenizer.cls_token_id)\n                feature_null_score = start_logits[cls_index] + end_logits[cls_index]\n                if min_null_score is None or min_null_score < feature_null_score:\n                    min_null_score = feature_null_score\n\n                for i, offset in enumerate(offset_mapping):\n                    if offset is not None:\n                        start_flgs[offset[0]:offset[1]] = start_flgs[offset[0]:offset[1]].clip(start_logits[i], None)\n                        end_flgs[offset[0]:offset[1]] = end_flgs[offset[0]:offset[1]].clip(end_logits[i], None)\n\n\n            best_answer = {\"start_logits\": softmax(start_flgs), \"end_logits\": softmax(end_flgs)}\n\n            # Let's pick our final answer: the best one or the null answer (only for squad_v2)\n            for k, v in example.items():\n                if k not in best_answer:\n                    best_answer[k] = v\n\n            predictions[example[\"id\"]] = best_answer#[\"text\"]#text#best_answer[\"text\"]\n\n        return predictions\n\n","bd31b3e5":"import torch\nfrom transformers import AutoModelForQuestionAnswering, TrainingArguments, Trainer\nimport collections\n\ntest_dataset = Dataset.from_pandas(test)","80d2d5bd":"#model_checkpoint = '..\/input\/exp25\/chaii\/mrm8488_bert-multi-cased-finedtuned-xquad-tydiqa-goldp_exp25\/'\n\npaths = [\n    '..\/input\/chaii-model-0816\/chaii-bert-trained_knr_gtrain\/chaii-bert-trained_knr_gtrain',\n    '..\/input\/exp32\/deepset_xlm-roberta-large-squad2_exp32',\n    '..\/input\/exp32-ru\/AlexKay_xlm-roberta-large-qa-multilingual-finedtuned-ru_exp32',\n    '..\/input\/exp33\/deepset_xlm-roberta-large-squad2_exp33',\n    '..\/input\/exp33-ru\/AlexKay_xlm-roberta-large-qa-multilingual-finedtuned-ru_exp33',\n    \n    '..\/input\/exp32-exp33-seed1\/chaii\/deepset_xlm-roberta-large-squad2_exp32',\n    '..\/input\/exp32-exp33-seed1\/chaii\/AlexKay_xlm-roberta-large-qa-multilingual-finedtuned-ru_exp32',\n        ]\ntokenizer = AutoTokenizer.from_pretrained(paths[0])\nmodel = AutoModelForQuestionAnswering.from_pretrained(paths[0])\n\n\nprep = Prep(tokenizer)\ntest_features = test_dataset.map(\n    prep.prepare_validation_features,\n    batched=True,\n    remove_columns=test_dataset.column_names\n)\ntest_feats_small = test_features.map(lambda example: example, remove_columns=['example_id', 'offset_mapping'])\n\npred = [0, 0]\n\nfor path in paths:\n    print(path)\n    checkpoint = torch.load(os.path.join(path, 'pytorch_model.bin'))\n    model.load_state_dict(checkpoint)\n    model.eval()\n    model = model.to(device)\n    trainer = Trainer(\n        model,\n        args,\n        data_collator=data_collator,\n        tokenizer=tokenizer,\n    )\n\n    test_predictions = trainer.predict(test_feats_small)\n    \n    pred[0] += test_predictions.predictions[0] \/ len(paths)\n    pred[1] += test_predictions.predictions[1] \/ len(paths)\n    \ntest_features.set_format(type=test_features.format[\"type\"], columns=list(test_features.features.keys()))\nfinal_test_predictions_tkm_xlm = prep.postprocess_qa_predictions(test_dataset, test_features, pred)\n\ndel test_features, model, trainer\ngc.collect()","d0444a58":"#model_checkpoint = '..\/input\/exp25\/chaii\/mrm8488_bert-multi-cased-finedtuned-xquad-tydiqa-goldp_exp25\/'\n\npaths = [\n    '..\/input\/exp32-exp33-rem-info\/chaii\/google_rembert_exp32',\n    '..\/input\/exp32-exp33-rem-info\/chaii\/google_rembert_exp33',\n    '..\/input\/exp32-exp33-seed1\/chaii\/google_rembert_exp32',\n        ]\ntokenizer = AutoTokenizer.from_pretrained(paths[0])\nprep = Prep(tokenizer)\ntest_features = test_dataset.map(\n    prep.prepare_validation_features,\n    batched=True,\n    remove_columns=test_dataset.column_names\n)\ntest_feats_small = test_features.map(lambda example: example, remove_columns=['example_id', 'offset_mapping'])\n\npred = [0, 0]\n\nfor path in paths:\n    print(path)\n    \n    model = AutoModelForQuestionAnswering.from_pretrained(path)\n\n    model.to(device)\n    model.eval()\n    trainer = Trainer(\n        model,\n        args,\n        data_collator=data_collator,\n        tokenizer=tokenizer,\n    )\n\n    test_predictions = trainer.predict(test_feats_small)\n    \n    pred[0] += test_predictions.predictions[0] \/ len(paths)\n    pred[1] += test_predictions.predictions[1] \/ len(paths)\n    \ntest_features.set_format(type=test_features.format[\"type\"], columns=list(test_features.features.keys()))\nfinal_test_predictions_tkm_rem = prep.postprocess_qa_predictions(test_dataset, test_features, pred)\n\ndel test_features, model, trainer\ngc.collect()","7217c0c3":"#model_checkpoint = '..\/input\/exp25\/chaii\/mrm8488_bert-multi-cased-finedtuned-xquad-tydiqa-goldp_exp25\/'\n\npaths = [\n    '..\/input\/exp32-exp33-muril\/chaii\/google_muril-large-cased_exp32',\n    '..\/input\/exp32-exp33-muril\/chaii\/google_muril-large-cased_exp33',\n        ]\ntokenizer = AutoTokenizer.from_pretrained(paths[0])\nprep = Prep(tokenizer)\ntest_features = test_dataset.map(\n    prep.prepare_validation_features,\n    batched=True,\n    remove_columns=test_dataset.column_names\n)\ntest_feats_small = test_features.map(lambda example: example, remove_columns=['example_id', 'offset_mapping'])\n\npred = [0, 0]\n\nfor path in paths:\n    print(path)\n    \n    model = AutoModelForQuestionAnswering.from_pretrained(path)\n\n    model.to(device)\n    model.eval()\n    trainer = Trainer(\n        model,\n        args,\n        data_collator=data_collator,\n        tokenizer=tokenizer,\n    )\n\n    test_predictions = trainer.predict(test_feats_small)\n    w = 2 if '_exp32' in path else 1\n    pred[0] += test_predictions.predictions[0] * w \/ 3\n    pred[1] += test_predictions.predictions[1] * w \/ 3\n    print('w =', w)\n    \ntest_features.set_format(type=test_features.format[\"type\"], columns=list(test_features.features.keys()))\nfinal_test_predictions_tkm_mul = prep.postprocess_qa_predictions(test_dataset, test_features, pred)\n\ndel test_features, model, trainer\ngc.collect()","506e0f71":"#model_checkpoint = '..\/input\/exp25\/chaii\/mrm8488_bert-multi-cased-finedtuned-xquad-tydiqa-goldp_exp25\/'\n\npaths = [\n    '..\/input\/exp32-exp33-rem-info\/chaii\/microsoft_infoxlm-large_exp32',\n    '..\/input\/exp32-exp33-rem-info\/chaii\/microsoft_infoxlm-large_exp33',\n    '..\/input\/exp32-exp33-seed1\/chaii\/microsoft_infoxlm-large_exp32',\n        ]\ntokenizer = AutoTokenizer.from_pretrained(paths[0])\nprep = Prep(tokenizer)\ntest_features = test_dataset.map(\n    prep.prepare_validation_features,\n    batched=True,\n    remove_columns=test_dataset.column_names\n)\ntest_feats_small = test_features.map(lambda example: example, remove_columns=['example_id', 'offset_mapping'])\n\npred = [0, 0]\n\nfor path in paths:\n    print(path)\n    \n    model = AutoModelForQuestionAnswering.from_pretrained(path)\n\n    model.to(device)\n    model.eval()\n    trainer = Trainer(\n        model,\n        args,\n        data_collator=data_collator,\n        tokenizer=tokenizer,\n    )\n\n    test_predictions = trainer.predict(test_feats_small)\n    \n    pred[0] += test_predictions.predictions[0] \/ len(paths)\n    pred[1] += test_predictions.predictions[1] \/ len(paths)\n    \ntest_features.set_format(type=test_features.format[\"type\"], columns=list(test_features.features.keys()))\nfinal_test_predictions_tkm_info = prep.postprocess_qa_predictions(test_dataset, test_features, pred)\n\ndel test_features, model, trainer\ngc.collect()","a5723f78":"starts = []\nends = []\nfor id_ in tqdm(test['id'].values):\n    start_logit = (final_test_predictions_tkm_xlm[id_]['start_logits'] * 2.2\n                  + final_test_predictions_tkm_rem[id_]['start_logits'] * 1.7\n                  + final_test_predictions_tkm_info[id_]['start_logits'] * 0.3\n                  + final_test_predictions_tkm_mul[id_]['start_logits'] * 1.2\n                  ) \/ 4.2\n    start_logit += np.arange(start_logit.shape[0])[::-1] * 1.e-10\n    end_logit = (final_test_predictions_tkm_xlm[id_]['end_logits'] * 2.2\n                  + final_test_predictions_tkm_rem[id_]['end_logits'] * 1.7\n                  + final_test_predictions_tkm_info[id_]['end_logits'] * 0.3\n                 + final_test_predictions_tkm_mul[id_]['end_logits'] * 1.2\n                  ) \/ 4.2\n    end_logit += np.arange(end_logit.shape[0]) * 1.e-10\n    \n    idx, score = max((\n                 ((i, j), start_logit[i] + end_logit[j]) \n                 for i in np.argsort(start_logit)[-35:] \n                 for j in np.argsort(end_logit)[-35:] if i <= j and j - i <= 100\n                ), key=lambda x: x[1])\n    starts.append(idx[0])\n    ends.append(idx[1] + 1)\n    \ntest['start'] = starts\ntest['end'] = ends\ntest['text'] = test.apply(lambda x: x.context[x.start:x.end], axis=1)","cd8b4a52":"import re\nbad_starts = [\".\", \",\", \"(\", \")\", \"-\", \"\u2013\",  \",\", \";\"]\nbad_endings = [\"...\", \"-\", \"(\", \")\", \"\u2013\", \",\", \";\"]\n\naaa = [\"\u0b95\u0bbf.\u0baa\u0bbf\",  \n        \"\u0b95\u0bbf.\u0bae\u0bc1\",\n        \"\u0b95\u0bbf.\u0bae\u0bc0\",\n         \"\u0908\",\n        \"\u0908.\u092a\u0942\",\n\"\u0935\u0940.\u090f\u0928\",\n\"\u0915\u093f.\u092e\u0940\",\n      ]\ndeg = '|'.join(aaa)\n\ndef postproc(data):\n    text = data['text']\n    text = text.replace('\\n', '').strip()\n    for s in '()-':\n        text = text.strip(s).strip()\n    text = re.sub('[\\(\\)]', ' ', text)\n    if re.match('^[\u0966\u0967\u0968\u0969\u096a\u096b\u096c\u096d\u096e\u096f]+$', text) is not None:\n        text_ = str(text)\n        for i, s in enumerate('\u0966\u0967\u0968\u0969\u096a\u096b\u096c\u096d\u096e\u096f'):\n            text_ = text_.replace(s, f'{i}')\n        if text_ in data['context']:\n            text = text_\n\n    data['text'] = text\n    return data\n\ndef postproc2(data):\n    pred = data['text']\n    context  = data['context']\n    if pred == \"\":\n        return data\n    while any([pred.startswith(y) for y in bad_starts]):\n        pred = pred[1:]\n    while any([pred.endswith(y) for y in bad_endings]):\n        if pred.endswith(\"...\"):\n            pred = pred[:-3]\n        else:\n            pred = pred[:-1]\n    \n        \n    if re.search(f'({deg})$', pred) is not None and pred + \".\" in context:\n        pred = pred+\".\"\n\n    if pred[-7:] == '\u0b95\u0bbf.\u0bae\u0bc0.2':\n        pred = pred[:-1]\n    #if pred[-5:] == '\u0b95\u0bbf\u0bae\u0bc0\u00b2':\n    #    pred = pred[:-1]\n        \n    pred = re.sub('^0-', '', pred)\n\n    data['text'] = pred\n    return data\n\ntest['PredictionString'] = test.apply(lambda x: postproc2(postproc(x)).text, axis=1)","40997692":"sub = test[['id', 'PredictionString']]\nsub.to_csv('submission.csv', index=False)\nsub","62ebc53e":"# Baseline\n\nBased on: https:\/\/github.com\/huggingface\/notebooks\/blob\/master\/examples\/question_answering.ipynb"}}