{"cell_type":{"382a39d0":"code","6f16a4db":"code","ec93c194":"code","9c5eb4f6":"code","b72c735e":"code","3b594564":"code","6771a0c3":"code","b5a7aaa7":"code","07a7a6cf":"code","1abd53c7":"code","08b6652f":"code","e2e18f53":"code","37173ef6":"code","1664e26e":"code","215fc35d":"code","0d64af70":"code","5d1ae323":"code","59f3e8a6":"code","fd1f7417":"code","9400525b":"code","16b7b8b3":"code","38b85b84":"code","dd641026":"code","2f1d92e2":"code","f511153d":"code","9b8ddea3":"code","a25cf626":"code","8d469fac":"code","fbe82947":"code","eab80a1d":"code","02dd8c3c":"code","dc1014a7":"code","4a903bb4":"code","f66e0f07":"code","997f7af3":"code","517b117f":"code","e0344335":"code","6d5ed44b":"code","d2f1fdd6":"code","d42e0229":"code","6c51ed73":"code","233a3f32":"code","6cd6d446":"markdown","91a31121":"markdown","ce211e9a":"markdown","0543338f":"markdown","e9bbe48a":"markdown","cd922ee0":"markdown","6cd15c5e":"markdown","2301515d":"markdown","8a373a58":"markdown","9e45250b":"markdown","1c361045":"markdown","483b60b0":"markdown","ef11bec8":"markdown","30c4205f":"markdown"},"source":{"382a39d0":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\nfrom pprint import pprint as pp\n\nfrom bs4 import BeautifulSoup, Tag\nimport re\nfrom enum import Enum\n\n# for garbage collection\nimport gc\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","6f16a4db":"!tar -xvzf \/kaggle\/input\/stanford-plato-corpus\/plato_mirror_spr2020.tgz","ec93c194":"!ls stanford.library.sydney.edu.au","9c5eb4f6":"!ls -lah stanford.library.sydney.edu.au\/entries | head -10","b72c735e":"!ls -lah stanford.library.sydney.edu.au\/entries\/abduction","3b594564":"MAIN_FOLDER = \"stanford.library.sydney.edu.au\/entries\/\"","6771a0c3":"article_folders = [  os.path.join(MAIN_FOLDER, fol) for fol in os.listdir(MAIN_FOLDER) if os.path.isdir(os.path.join(MAIN_FOLDER, fol)) ]\n\narticle_file_paths = [os.path.join(fol, file) for fol in article_folders for file in os.listdir(fol) ]\n\nprint(\"we have\", len([x for x in article_file_paths if \"html\" in x]), \"article HTMLs to scrape\")\n\nprint(\"\\n\\nhere are the first few filenames:\\n\")\npp(article_file_paths[:5])","b5a7aaa7":"print(os.path.split(\"stanford.library.sydney.edu.au\/entries\/meaning\/index.html\"))\nprint(os.path.split(os.path.split(\"stanford.library.sydney.edu.au\/entries\/meaning\/index.html\")[0]))","07a7a6cf":"df = pd.DataFrame(article_file_paths)\n\ndf.columns = [\"filenames\"]\n\ndef get_filetype(x):\n    return os.path.splitext(x)[-1]\n\n\ndef get_topic(x):\n    path, fil = os.path.split(x)\n    _, fol = os.path.split(path)\n    return fol\n\n\n\ndf['filetype'] = df['filenames'].apply(get_filetype)\ndf['topic'] = df['filenames'].apply(get_topic)\n\ndf.head()","1abd53c7":"class MESSAGE(Enum):\n    SUCCESS = {'num': 0, 'msg': \"all good\"}\n    ARTICLE_ERROR = {'num': 1, 'msg': \"expected exactly 1 id='article'\"}\n    MAIN_TEXT_ERROR = {'num': 2, 'msg': \"expected there to be an id='main-text'\"}\n    FAILURE = {'num': 3, 'msg': \"something bad\"}\n    def __bool__(self):\n        \"\"\"\n        >>> MESSAGE.SUCCESS or MESSAGE.FAILURE\n        <<< MESSAGE.FAILURE\n        \"\"\"\n        return self.value['num'] != 0\n    def choose_over(self, older_error):\n        \"\"\"\n        prefer latest errors over older errors\n        prefer errors over successes\n        prefer specific errors over general errors\n        \"\"\"\n        if self is not self.SUCCESS:  \n            # prefer latest errors over older errors\n            if self is self.FAILURE:\n                return older_error or self\n            return self\n        else:\n            return older_error","08b6652f":"print(MESSAGE.ARTICLE_ERROR.choose_over(MESSAGE.SUCCESS))  # prefer errors over successes\nprint(MESSAGE.SUCCESS.choose_over(MESSAGE.ARTICLE_ERROR))  # prefer errors over successes\nprint(MESSAGE.MAIN_TEXT_ERROR.choose_over(MESSAGE.ARTICLE_ERROR))  # prefer latest errors over older errors\nprint(MESSAGE.ARTICLE_ERROR.choose_over(MESSAGE.MAIN_TEXT_ERROR))  # prefer latest errors over older errors\nprint(MESSAGE.FAILURE.choose_over(MESSAGE.ARTICLE_ERROR))  # prefer specific errors over general errors\nprint(MESSAGE.ARTICLE_ERROR.choose_over(MESSAGE.FAILURE))  # prefer specific errors over general errors\nprint(MESSAGE.SUCCESS or MESSAGE.FAILURE)\nprint(MESSAGE.FAILURE or MESSAGE.SUCCESS)\nprint(MESSAGE.FAILURE or MESSAGE.ARTICLE_ERROR)\nprint(MESSAGE.ARTICLE_ERROR or MESSAGE.FAILURE)","e2e18f53":"DEBUG = False\n\n\ndef get_soup(filename):\n    with open(filename, 'r') as f:\n        print(filename) if DEBUG else None\n        return BeautifulSoup(f, 'html.parser')\n\n\ndef is_heading(sib):\n    return (\n        isinstance(sib, Tag) \n        and \n        sib.name in ['h1','h2','h3','h4','h5','h6']\n    )\n    \n    \n\ndef _get_metadata(soup):\n    metadata_list = soup.find_all('meta')\n    doc_title = soup.find('meta', attrs={'name':'DC.title'})\n    doc_creator = soup.find('meta', attrs={'name':'DC.creator'})\n    doc_terms_modified = soup.find('meta', attrs={'name':'DCTERMS.modified'})\n    citation_pub_date = soup.find('meta', attrs={'property':'citation_publication_date'})\n    citation_author = soup.find('meta', attrs={'property':'citation_author'})\n    return {\n        \"doc_title_html\": doc_title,\n        \"doc_creator_html\": doc_creator,\n        \"doc_terms_modified_html\": doc_terms_modified,\n        \"citation_pub_date_html\": citation_pub_date,\n        \"citation_author_html\": citation_author,\n    }\n\n\ndef _get_article_content(article=None):\n    if article == None:\n        return None\n\n    article_content_list = article.find_all(\"div\", id=\"article-content\")\n    # ASSUME: there is only one article && HTML structure has 'article-content'\n    assert len(article_content_list) == 1\n    article_content = article_content_list[0]\n    return article_content\n\n\ndef _get_article(soup):\n    article_list = soup.find_all(\"div\", id=\"article\")\n    # ASSUME: there is only one article\n    if len(article_list) != 1:\n        print(f\"WEIRD! {MESSAGE.ARTICLE_ERROR.value}\") if DEBUG else None\n        return (None, MESSAGE.ARTICLE_ERROR)\n    article = article_list[0]\n    return (article, MESSAGE.SUCCESS)\n\n\ndef _get_user_section(article_content=None):\n    if article_content == None:\n        return None\n    article_user_editable_section_list = article_content.find_all(\"div\", id=\"aueditable\")\n    # ASSUME: there is only one article && HTML structure has 'article-content' && \"aueditable\" in structure too\n    assert len(article_user_editable_section_list) == 1\n    user_section = article_user_editable_section_list[0]\n    return user_section\n\n\ndef _get_structured_content_data(root_tag=None):\n    err_msg = MESSAGE.SUCCESS\n    if root_tag == None:\n        return (\n            {\n                \"pubinfo_html\"         : None, \n                \"preamble_html\"        : None, \n                \"toc_html\"             : None, \n                \"main_text_html\"       : None, \n                \"biblio_html\"          : None, \n                \"academic_tools_html\"  : None, \n                \"resources_html\"       : None, \n                \"related_entries_html\" : None,\n            },\n            MESSAGE.FAILURE\n        )\n    \n    pubinfo_list         = root_tag.find_all(\"div\", id=\"pubinfo\") or None\n    preamble_list        = root_tag.find_all(\"div\", id=\"preamble\") or None\n    toc_list             = root_tag.find_all(\"div\", id=\"toc\") or None\n    main_text_list       = root_tag.find_all(\"div\", id=\"main-text\") or None\n    biblio_list          = root_tag.find_all(\"div\", id=\"bibliography\") or None\n    academic_tools_list  = root_tag.find_all(\"div\", id=\"academic-tools\") or None\n    resources_list       = root_tag.find_all(\"div\", id=\"other-internet-resources\") or None\n    related_entries_list = root_tag.find_all(\"div\", id=\"related-entries\") or None\n\n    div_lists = [\n        pubinfo_list,\n        preamble_list,\n        toc_list,\n        main_text_list,\n        biblio_list,\n        academic_tools_list,\n        resources_list,\n        related_entries_list,\n    ]\n    # ASSUME: there is a main_text\n    if main_text_list == None:\n        print(f\"VERY WEIRD! {MESSAGE.MAIN_TEXT_ERROR.value}\") if DEBUG else None\n        err_msg = MESSAGE.MAIN_TEXT_ERROR\n\n    return (\n        {\n            \"pubinfo_html\"         : pubinfo_list[0] if pubinfo_list else None,\n            \"preamble_html\"        : preamble_list[0] if pubinfo_list else None,\n            \"toc_html\"             : toc_list[0] if pubinfo_list else None,\n            \"main_text_html\"       : main_text_list[0] if pubinfo_list else None,\n            \"biblio_html\"          : biblio_list[0] if pubinfo_list else None,\n            \"academic_tools_html\"  : academic_tools_list[0] if pubinfo_list else None,\n            \"resources_html\"       : resources_list[0] if pubinfo_list else None,\n            \"related_entries_html\" : related_entries_list[0] if pubinfo_list else None,\n        },\n        err_msg,\n    )\n\n\ndef _get_sections_between_headings(main_text=None):\n    if main_text == None:\n        return None\n    \n    headings = main_text.find_all(re.compile('^h[1-6]$'))\n\n    sections = []\n\n    tag = headings[0]\n    heading_text = headings[0].text\n    section_num = 1\n    sections.append(\n        {\n            \"id\": section_num,\n            \"heading_text\": heading_text,\n            \"soup_data\": str(tag),\n        }\n    )\n\n    for sib in tag.next_siblings:\n        if( is_heading(sib) ):\n            section_num += 1\n            heading_text = sib.text\n            sections.append(\n                {\n                    \"id\": section_num,\n                    \"heading_text\": heading_text,\n                    \"soup_data\": str(sib),\n                }\n            )\n        else:\n            sections.append(\n                {\n                    \"id\": section_num,\n                    \"heading_text\": heading_text,\n                    \"soup_data\": str(sib),\n                }\n            )\n\n    return sections\n\n\ndef _get_all_the_useful_data(soup):\n    # this is good data\n    metadata_dict = _get_metadata(soup)\n\n    # drill into article data\n    article, err_msg = _get_article(soup)\n    article_content = _get_article_content(article)\n    user_section = _get_user_section(article_content)\n\n    root_tag = user_section\n\n    struct_data_dict, err_msg2 = _get_structured_content_data(root_tag)\n    \n    final_err_msg = err_msg2.choose_over(err_msg)\n    \n    main_text = struct_data_dict['main_text_html']\n\n    # the main thing we care about\n    sections = _get_sections_between_headings(main_text)\n\n    return (\n        { \n            \"sections\": sections, \n            \"article_html\": article,\n            \"article_content_html\": article_content,\n            \"user_section_html\": user_section,\n            **struct_data_dict, \n            **metadata_dict\n        },\n        final_err_msg\n    )\n    \n\ndef get_full_html_and_plain_text_and_sections(filename):\n    soup = get_soup(filename)\n    html = soup.html\n    plain_text = soup.text\n    useful_data, err_msg = _get_all_the_useful_data(soup)\n    data = {\n        \"full_html\": html,\n        \"plain_text\": plain_text,\n        \"err_msg\": err_msg,\n        **useful_data\n    }\n    return pd.Series(data)","37173ef6":"# SAMPLE_FILE = \"stanford.library.sydney.edu.au\/entries\/meaning\/index.html\"\n# SAMPLE_SOUP = get_soup(SAMPLE_FILE)\n\n# SAMPLE_METADATA_list = SAMPLE_SOUP.find_all('meta')\n# print(\"\\n\", SAMPLE_METADATA_list, \"\\n\")\n# SAMPLE_DOC_TITLE = SAMPLE_SOUP.find('meta', attrs={'name':'DC.title'})\n# SAMPLE_DOC_CREATOR = SAMPLE_SOUP.find('meta', attrs={'name':'DC.creator'})\n# SAMPLE_DOC_TERMS_MODIFIED = SAMPLE_SOUP.find('meta', attrs={'name':'DCTERMS.modified'})\n# SAMPLE_DOC_PUB_DATE = SAMPLE_SOUP.find('meta', attrs={'property':'citation_publication_date'})\n# SAMPLE_DOC_AUTHOR = SAMPLE_SOUP.find('meta', attrs={'property':'citation_author'})\n\n# SAMPLE_ARTICLES = SAMPLE_SOUP.find_all(\"div\", id=\"article\")\n# print(\"there are\", len(SAMPLE_ARTICLES), \"articles within `SAMPLE_SOUP`\")\n# # ASSUME: there is only one article\n# assert len(SAMPLE_ARTICLES) == 1\n# SAMPLE_ARTICLE = SAMPLE_ARTICLES[0]\n\n\n# SAMPLE_ARTICLE_CONTENTS = SAMPLE_ARTICLE.find_all(\"div\", id=\"article-content\")\n# print(\"there are\", len(SAMPLE_ARTICLE_CONTENTS), \"top-level article containers within `SAMPLE_ARTICLE`\")\n# # ASSUME: there is only one article && HTML structure has 'article-content'\n# assert len(SAMPLE_ARTICLE_CONTENTS) == 1\n# SAMPLE_ARTICLE_CONTENT = SAMPLE_ARTICLE_CONTENTS[0]\n\n\n# SAMPLE_ARTICLE_USER_EDITABLE_SECTIONS = SAMPLE_ARTICLE_CONTENT.find_all(\"div\", id=\"aueditable\")\n# print(\"there are\", len(SAMPLE_ARTICLE_USER_EDITABLE_SECTIONS), \"user editable sections within `SAMPLE_ARTICLE_CONTENT`\")\n# # ASSUME: there is only one article && HTML structure has 'article-content' && \"aueditable\" in structure too\n# assert len(SAMPLE_ARTICLE_USER_EDITABLE_SECTIONS) == 1\n# SAMPLE_USER_SECTION = SAMPLE_ARTICLE_USER_EDITABLE_SECTIONS[0]\n\n\n# SAMPLE_PUBINFO_list                   = SAMPLE_USER_SECTION.find_all(\"div\", id=\"pubinfo\")\n# SAMPLE_PREAMBLE_list                  = SAMPLE_USER_SECTION.find_all(\"div\", id=\"preamble\")\n# SAMPLE_TOC_list                       = SAMPLE_USER_SECTION.find_all(\"div\", id=\"toc\")\n# SAMPLE_MAIN_TEXT_list                 = SAMPLE_USER_SECTION.find_all(\"div\", id=\"main-text\")\n# SAMPLE_BIBLIO_list                    = SAMPLE_USER_SECTION.find_all(\"div\", id=\"bibliography\")\n# SAMPLE_ACADEMIC_TOOLS_list            = SAMPLE_USER_SECTION.find_all(\"div\", id=\"academic-tools\")\n# SAMPLE_ONLINE_INTERNET_RESOURCES_list = SAMPLE_USER_SECTION.find_all(\"div\", id=\"other-internet-resources\")\n# SAMPLE_RELATED_ENTRIES_list           = SAMPLE_USER_SECTION.find_all(\"div\", id=\"related-entries\")\n\n# div_lists = [\n#     SAMPLE_PUBINFO_list,\n#     SAMPLE_PREAMBLE_list,\n#     SAMPLE_TOC_list,\n#     SAMPLE_MAIN_TEXT_list,\n#     SAMPLE_BIBLIO_list,\n#     SAMPLE_ACADEMIC_TOOLS_list,\n#     SAMPLE_ONLINE_INTERNET_RESOURCES_list,\n#     SAMPLE_RELATED_ENTRIES_list\n# ]\n# # ASSUME: all of this HTML structure\n# assert all([len(x) == 1 for x in div_lists])\n\n# SAMPLE_PUBINFO                   = SAMPLE_PUBINFO_list[0]\n# SAMPLE_PREAMBLE                  = SAMPLE_PREAMBLE_list[0]\n# SAMPLE_TOC                       = SAMPLE_TOC_list[0]\n# SAMPLE_MAIN_TEXT                 = SAMPLE_MAIN_TEXT_list[0]\n# SAMPLE_BIBLIO                    = SAMPLE_BIBLIO_list[0]\n# SAMPLE_ACADEMIC_TOOLS            = SAMPLE_ACADEMIC_TOOLS_list[0]\n# SAMPLE_ONLINE_INTERNET_RESOURCES = SAMPLE_ONLINE_INTERNET_RESOURCES_list[0]\n# SAMPLE_RELATED_ENTRIES           = SAMPLE_RELATED_ENTRIES_list[0]\n\n# SAMPLE_SECTIONS = _get_sections_between_headings(SAMPLE_MAIN_TEXT)\n\n# print(\"there are\", len(SAMPLE_SECTIONS), \"sections within `SAMPLE_MAIN_TEXT`\")","1664e26e":"html_df = df[df['filetype'] == '.html']\nseries = html_df['filenames']\nfun = get_full_html_and_plain_text_and_sections\nmask = ['full_html', 'plain_text', 'err_msg', 'sections', 'article_html',\n       'article_content_html', 'user_section_html', 'pubinfo_html',\n       'preamble_html', 'toc_html', 'main_text_html', 'biblio_html',\n       'academic_tools_html', 'resources_html', 'related_entries_html',\n       'doc_title_html', 'doc_creator_html', 'doc_terms_modified_html',\n       'citation_pub_date_html', 'citation_author_html']\ndf[mask] = series.apply(fun)\n# df[mask] = series[:20].apply(fun)","215fc35d":"x = get_full_html_and_plain_text_and_sections('stanford.library.sydney.edu.au\/entries\/contractarianism\/index.html')","0d64af70":"x['sections']\n","5d1ae323":"get_full_html_and_plain_text_and_sections(\"stanford.library.sydney.edu.au\/entries\/albo-joseph\/vita.html\")","59f3e8a6":"def _get_related_entries_list(soup):\n    return [a['href'] for a in soup.find_all('a', href=True)] if isinstance(soup, Tag) else None\n\ndf['related_entries_list'] = df['related_entries_html'].apply(_get_related_entries_list)","fd1f7417":"def _get_preamble_text(soup):\n    return soup.text if isinstance(soup, Tag) else None\n\ndf['preamble_text'] = df['preamble_html'].apply(_get_preamble_text)","9400525b":"def _get_author(soup):\n    return soup['content'] if isinstance(soup, Tag) else None\n\ndf['author'] = df['citation_author_html'].apply(_get_author)","16b7b8b3":"def _get_creator(soup):\n    return soup['content'] if isinstance(soup, Tag) else None\n\ndf['creator'] = df['doc_creator_html'].apply(_get_creator)","38b85b84":"def _get_title(soup):\n    return soup['content'] if isinstance(soup, Tag) else None\n\ndf['title'] = df['doc_title_html'].apply(_get_title)","dd641026":"def soup_data_agg(series):\n    #str_series = [str(x) for x in series]\n    agg_str = \"\".join(series)\n    doc = BeautifulSoup(agg_str)\n    paragraph_list = doc.find_all('p')\n    iteration = zip(paragraph_list, range(1,len(paragraph_list)+1))\n    p_struct_list = [{\"id\": i, \"text\": p.text} for p, i in iteration]\n    if p_struct_list != []:\n        return p_struct_list\n    elif paragraph_list != []:\n        text = \"\".join([x.text for x in paragraph_list])\n        return [{\"id\": 1, \"text\": text}]\n    else:\n        return None\n    \n\ndef heading_text_agg(series):\n    s = set(series)\n    res = s.pop()\n    assert s == set(), f\"should only be 1 heading_text.. but s = {s}\"\n    return res\n\n\nagg_funs = {\n    \"heading_text\": heading_text_agg,\n    \"soup_data\": soup_data_agg,\n}\n\ndef paragraph_agg(series):\n    return \"\".join(series)\n\n\ndef organize_paragraphs(paragraph_dict_list):\n    if type(paragraph_dict_list) == list:\n        sdf = pd.DataFrame(paragraph_dict_list)\n        agg = sdf.groupby('id').agg(agg_funs)#.rename(columns={'text': 'paragraphs'})\n        return \"text\"\n    else:\n        return None    \n\n\ndef organize_section_data(section_dict_list):\n    if type(section_dict_list) == list:\n        sdf = pd.DataFrame(section_dict_list)\n        agged = sdf.groupby('id').agg(agg_funs).rename(columns={'soup_data': 'paragraphs'})\n        return list(agged.reset_index().T.to_dict().values())\n    else:\n        return None\n\ndf['sections_agged'] = df['sections'].apply(organize_section_data)\n\n#list_dict_data = df['sections'].values[1]\n#sections_df_1 = pd.DataFrame(list_dict_data)\n#sections_df_1_agged = sections_df_1.groupby('id').agg(agg_funs).rename(columns={'soup_data': 'paragraphs'})\n#ps_df_0 = pd.DataFrame(sections_df_1_agged['paragraphs'].values[0])\n#ps_df.groupby('id').agg(paragraph_agg).rename(columns={'text': 'paragraph'})","2f1d92e2":"df.columns","f511153d":"df['sections_agged'][2]","9b8ddea3":"df.head()","a25cf626":"simple_df = df[ ['filenames', 'filetype', 'topic', 'title', \n                 'author', 'creator', 'preamble_text', \n                 'sections_agged', 'related_entries_list', \n                 'plain_text'] \n              ]\n\nsimple_df = simple_df.rename(columns={\"sections_agged\": \"sections\"})","8d469fac":"simple_df.head()","fbe82947":"CSV_DATA = df.to_csv(index=False)\nCSV_FILENAME = \"data.csv\"\n\nwith open(CSV_FILENAME, 'w') as f:\n    f.write(CSV_DATA)","eab80a1d":"del df\ndel CSV_DATA\ndel f","02dd8c3c":"gc.collect()","dc1014a7":"SIMPLE_CSV_DATA = simple_df.to_csv(index=False)\nSIMPLE_CSV_FILENAME = \"simple_data.csv\"\n\nwith open(SIMPLE_CSV_FILENAME, 'w') as f:\n    f.write(SIMPLE_CSV_DATA)","4a903bb4":"del simple_df\ndel SIMPLE_CSV_DATA\ndel f","f66e0f07":"gc.collect()","997f7af3":"!ls","517b117f":"df = pd.read_csv(CSV_FILENAME)\ndf.head()","e0344335":"# %timeit -n1 pd.read_csv(CSV_FILENAME).head()\nprint(\"60 ms \u00b1 4.29 ms per loop (mean \u00b1 std. dev. of 7 runs, 1 loop each)\")","6d5ed44b":"df['filetype'].astype('category').dtype","d2f1fdd6":"df['topic'].astype('category').dtype.categories.tolist()","d42e0229":"!rm -rf stanford.library.sydney.edu.au","6c51ed73":"!ls","233a3f32":"df.explode(\"related_entries_list\").head()\n# df.explore(\"sections_agged\").head()","6cd6d446":"# Delete variables to conserve memory","91a31121":"# again delete variables to conserve memory","ce211e9a":"# Make sure it saved\/try opening","0543338f":"# Unzip the archive","e9bbe48a":"# Clean up the outputs","cd922ee0":"# Here are all the *filetypes*","6cd15c5e":"# make a JSON per `<article_name>.html` file \nFormatted similar to [SQuAD](https:\/\/rajpurkar.github.io\/SQuAD-explorer\/)\n\n\n**SQuAD JSON (just some important bits... there are other key-value pairs)**\n\n```json\n{\n\"Title\": \"Document Title\",\n\"paragraphs\": [\n  { \n    \"id\": 1,\n    \"context\": \"....\",\n    \"qas\": [{\n      \"q\":\"???\", \n      \"a\":\"!!!\"\n    }]\n  }\n]}\n\n```\n\n**our JSON**\n\n```json\n{\n    \"filename\": \"stanford.library.sydney.edu.au\/entries\/meaning\/index.html\",\n    \"fileype\": \"html\",  \/\/ also could be JPG, GIF, PNG, JS, etc...\n    \"topic\": \"meaning\",\n    \/\/ \"full_html\": \"<html>.....<\/html>\",\n    \"plain_text\": \"....\",\n    \"sections\": [\n        {\n            \"id\": 1,\n            \"heading_text\": \"The Meaning of Meaning\", \/\/ extract from the <h> tag \n            \"paragraphs\": [ \n                {\n                    \"id\": 1,\n                    \"text\": \"The meaning of meaning is meaningful, for without meaning it has no meaning...\" \/\/ extract from the <p> tag\n                },\n                ...\n            ]\n        },\n        ...\n    ],\n}\n```","2301515d":"# TODO: consider the `df.explode` function","8a373a58":"# extract some *simple* columns","9e45250b":"# Save the Simple DataFrame too","1c361045":"# go through the `entries`\n\nThere are folders for each _article_ in the `entries` folder.\n\nEach folder contains an `index.html`\n\n\nWe looked at the [table of contents page](https:\/\/plato.stanford.edu\/contents.html)\n\nWe noticed that the \"topic\" about [Aesthesics](https:\/\/plato.stanford.edu\/entries\/aesthetic-concept\/) has subtopics, which all remain within their own folders \n\nFor example, [aesthetics-19th-romantic](https:\/\/plato.stanford.edu\/entries\/aesthetics-19th-romantic\/) is inside its own folder","483b60b0":"### notice the file below for an example of where the weird `MESSAGE.ARTICLE_ERROR` happens","ef11bec8":"# Save the DataFrame","30c4205f":"# Here are all the *topics*\n(click `show output`)"}}