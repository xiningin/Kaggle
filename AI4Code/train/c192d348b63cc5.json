{"cell_type":{"43b305df":"code","ed1fe321":"code","38b6e7d0":"code","af676c7d":"code","8bab9a64":"code","f54a1815":"code","ca0a2572":"code","8297238e":"code","5a54c36d":"code","c15fdd9d":"code","ad617078":"markdown","4a89a80b":"markdown","c847c76d":"markdown","96afc2a6":"markdown","629c446d":"markdown","13281206":"markdown","214179e9":"markdown"},"source":{"43b305df":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport os\n\nimport cv2\nimport skimage\nimport IPython.display\nimport matplotlib.pyplot as plt\nfrom PIL import Image\nimport numpy as np\nfrom glob import glob\n\nfrom collections import OrderedDict\nimport torch\nimport gc\n\n%matplotlib inline\n%config InlineBackend.figure_format = 'retina'","ed1fe321":"!pip install ..\/input\/openaiclipweights\/python-ftfy-master\/python-ftfy-master\n!pip install ..\/input\/openaiclipweights\/clip\/CLIP\n!cp ..\/input\/openaiclipweights\/CLIP-main\/CLIP-main\/clip\/bpe_simple_vocab_16e6.txt \/opt\/conda\/lib\/python3.7\/site-packages\/clip\/.\n!gzip -k \/opt\/conda\/lib\/python3.7\/site-packages\/clip\/bpe_simple_vocab_16e6.txt\n!ls \/opt\/conda\/lib\/python3.7\/site-packages\/clip\/.","38b6e7d0":"import torch\nimport clip\nfrom tqdm.notebook import tqdm\nfrom torch.utils.data import DataLoader, Dataset\n\nprint(\"Torch version:\", torch.__version__)","af676c7d":"!wget https:\/\/farm8.staticflickr.com\/6036\/6426668771_b5b915e46c_o.jpg\n!wget https:\/\/c6.staticflickr.com\/8\/7457\/10806045045_02d3dbdcee_o.jpg\n!wget https:\/\/c1.staticflickr.com\/4\/3267\/2888764405_0a0a608604_o.jpg\n!wget https:\/\/farm8.staticflickr.com\/4028\/4294212194_a49663b2b9_o.jpg\n!wget https:\/\/c5.staticflickr.com\/9\/8173\/8019508216_6540c8686a_o.jpg\n!wget https:\/\/farm3.staticflickr.com\/1146\/1357102390_943c5cb999_o.jpg","8bab9a64":"files = glob('*.jpg')\nprint(files)","f54a1815":"clip.available_models()","ca0a2572":"!ls ..\/input\/openaiclipweights\/clip\/CLIP\/models\/","8297238e":"model, preprocess = clip.load(\"..\/input\/openaiclipweights\/clip\/CLIP\/models\/ViT-B-32.pt\")\nmodel.cuda().eval()","5a54c36d":"QUERIES = [\n    \"a dog\",\n    \"a cat\",\n    \"a elephant\",\n    \"a zebra\",\n    \"a sleeping dog\",\n    \"a sleeping cat\",\n    \"a giraffe\",\n    \"a poodle\",\n    \"animal inside a car\",\n    \"animal outside a car\",\n    \"a sofa\",\n    \"some animals\",\n    \"santa claus\",\n    \"ipod\",\n    \"two mugs\",\n    \"three mugs\",\n    \"blue sky\",\n] ","c15fdd9d":"with torch.no_grad():\n    for file in files:\n        print(file)\n        # Load image from file\n        img = Image.open(file).convert(\"RGB\")\n\n        # Just show image in the notebook\n        plt.imshow(cv2.resize(np.array(img), (256, 256)))\n        plt.show()\n        \n        # Preprocess image using clip\n        img = preprocess(img).unsqueeze(0).cuda()\n        \n        # Get Image embeddings\n        image_embeddings = model.encode_image(img)\n        image_embeddings \/= image_embeddings.norm(dim=-1, keepdim=True)\n        \n        \n        score = []\n        for query in QUERIES:\n            texts = clip.tokenize(query).cuda()\n            \n            # Get Text Embeddings\n            text_embeddings = model.encode_text(texts)\n            text_embeddings \/= text_embeddings.norm(dim=-1, keepdim=True)\n            \n            # Calc dot product between image and text embeddings\n            sc = float((image_embeddings @ text_embeddings.T).cpu().numpy())\n            score.append(sc)\n        \n        print( pd.DataFrame({'query': QUERIES, 'score': score}).sort_values('score', ascending=False) )\n        print('')\n        print('-------------------------')\n        print('')\n","ad617078":"# Score images vs queries using clip model","4a89a80b":"# List pretrained weights available","c847c76d":"# List pretrained CLIP models available","96afc2a6":"# Load CLIP Vision Transformer based model","629c446d":"# For each image we will query for the following senteces and see what CLIP predicts. \n# You can add custom sentences here.","13281206":"# Install CLIP library","214179e9":"# Download some images from open images collection."}}