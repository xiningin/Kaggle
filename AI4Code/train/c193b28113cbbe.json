{"cell_type":{"0db4ec8a":"code","c20ff407":"code","3b8f27be":"code","be0fbcdf":"code","9706e435":"code","27761aba":"code","22ed7c63":"code","613ce35a":"code","2dda5f07":"code","5e9acaa7":"code","27b8944c":"code","07e4df62":"code","d94fead9":"code","5f329d67":"code","ce94a505":"code","88cc1f40":"code","048fdc1f":"code","386151c8":"code","8994f615":"code","239dae79":"code","a54144e1":"markdown","00f27328":"markdown","76e6f642":"markdown","3075af8e":"markdown","f4e9549e":"markdown","fc7a8ce4":"markdown","2064f177":"markdown","e1c6cb10":"markdown","4370c960":"markdown","f5fa2642":"markdown","1e1f50f4":"markdown","65d18e9d":"markdown","f9a47b5e":"markdown","ff717c41":"markdown","562df3d1":"markdown"},"source":{"0db4ec8a":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","c20ff407":"import seaborn as sns\nimport matplotlib.pyplot as plt\nimport lightgbm as lgb\n\nfrom sklearn.ensemble import StackingClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.decomposition import PCA\nfrom sklearn.model_selection import RandomizedSearchCV\n\nimport warnings\nwarnings.simplefilter('ignore')","3b8f27be":"water_potability = pd.read_csv('\/kaggle\/input\/water-potability\/water_potability.csv')\nwater_potability.head()","be0fbcdf":"water_potability.describe()","9706e435":"# let's seek information\nwater_potability.info()","27761aba":"# look null percentage\nwater_potability.isnull().sum() \/ len(water_potability) * 100","22ed7c63":"# to decide the value, we have to know how this data look like\n\nvar = 'ph'\nsns.displot(x=var, data=water_potability, kde=True, bins=30)","613ce35a":"columns = water_potability.columns\n\nfig, axes = plt.subplots(3, 3, figsize=(10, 10))\n\n\nfrom itertools import product\n\nproduct_indexes = product([0, 1, 2], [0, 1, 2])\n\nfor idx, col in zip(product_indexes, columns):\n    sns.histplot(x=col, data=water_potability, kde=True, bins=30, ax=axes[idx[0], idx[1]])","2dda5f07":"# fillna\nwater_potability.ph = water_potability.ph.fillna(np.mean(water_potability.ph))\nwater_potability.Sulfate = water_potability.Sulfate.fillna(np.mean(water_potability.Sulfate))\nwater_potability.Trihalomethanes = water_potability.Trihalomethanes.fillna(np.mean(water_potability.Trihalomethanes))\n\nwater_potability.isnull().sum()","5e9acaa7":"# Potability hist\nsns.countplot(x='Potability', data=water_potability)","27b8944c":"fig = plt.figure(figsize=(10, 10))\ncormat = water_potability.corr()\n\nsns.heatmap(cormat, annot=True)","07e4df62":"scaler = StandardScaler()\n\nX = water_potability.drop('Potability', axis=1).values\ny = water_potability['Potability'].values","d94fead9":"X_scaled = scaler.fit_transform(X)","5f329d67":"X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, stratify=y)","ce94a505":"scores = cross_val_score(estimator=LogisticRegression(), \n                        X=X_train, y=y_train, \n                        scoring='accuracy',\n                        cv=10, n_jobs=1)\n\nprint(f'First Accuracy Try is {np.mean(scores):.3f} +\/- {np.std(scores)}')","88cc1f40":"# we have to use only train data so we use kfold\nkfold = StratifiedKFold(n_splits=10).split(X_train, y_train)\n\n\nscores = []\n\nfor (train, test) in kfold:\n    # firstly we set tempolary.\n    params = {\n        'task': 'train',\n        'boosting_type': 'gbdt',\n        'objective': 'multiclass',\n        'num_class': 2\n    }\n\n    train_data = lgb.Dataset(X_train[train], label=y_train[train])\n    eval_data = lgb.Dataset(X_train[test], label=y_train[test], reference= train_data)\n    \n    gbm = lgb.train(\n        params,\n        train_data,\n        valid_sets=eval_data,\n        num_boost_round=50\n    )\n    \n\n    preds = gbm.predict(X_train[test])\n    y_pred = []\n    for x in preds:\n      y_pred.append(np.argmax(x))\n    \n    score = accuracy_score(y_train[test], y_pred)\n    scores.append(score)","048fdc1f":"np.mean(scores)","386151c8":"estimators = Pipeline([\n    ('pca', PCA()),\n    ('lr', LogisticRegression(max_iter=1000))\n])\n\nparam_dist = [\n    {'lr__penalty': ['l1', 'l2']},\n    {'lr__C': [0.01, 0.1, 1.0, 10]},\n#     {'lr__solver': ['newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga']},\n    {'pca__n_components': [2, 3, 4, 5, 6]}\n]\n\nrs = RandomizedSearchCV(estimator=estimators, \n                       param_distributions=param_dist,\n                       scoring='accuracy', n_iter=100, \n                       cv=10, refit=True)\n\nrs = rs.fit(X_train, y_train)\n\nprint(f\"best param's are {rs.best_params_}\")\n\nrs.best_score_","8994f615":"\nestimators = [\n   ('rf', RandomForestClassifier(n_estimators=100)),\n   ('pipeline', Pipeline([\n                        ('pca', PCA(n_components=2)),\n                        ('lr', LogisticRegression(max_iter=500))\n                    ])\n   )\n]\n\n\n\nstack = StackingClassifier(estimators=estimators, \n                           final_estimator=LogisticRegression(max_iter=1000))\n\nscores = cross_val_score(estimator=stack,\n                        X=X_train, y=y_train,\n                        cv=10, scoring='accuracy')\n\nnp.mean(scores)","239dae79":"# let's try use test data\n\nscores = cross_val_score(estimator=stack,\n                        X=X_test, y=y_test,\n                        cv=10, scoring='accuracy')\n\nnp.mean(scores)","a54144e1":"#### well we should keep trying to predict more collectly.","00f27328":"### I'm getting to think about those features can't help the model's accuary growing..\n\n### So, we move to use ensemble","76e6f642":"#### https:\/\/www.kaggle.com\/adityakadiwal\/water-potability\n\n1. Predict if water is safe for Human consumption\n2. EDA for water potability","3075af8e":"#### OK! We can probably use all columns!\n#### So, we try to predict potability once","f4e9549e":"### We use RandomizeSeach and ensemble to find the best hyperparameters!","fc7a8ce4":"#### still not enough accuracy","2064f177":"#### Next, we seek correlation because we have to care about multico","e1c6cb10":"#### So, only LR is not enough to predict\n#### we try lightGBM or some ensemble or PCA action","4370c960":"#### all right, every parameters distributes normal distribution\n#### so, I think I can fill the null with each mean value ","f5fa2642":"### That's not overfitting\n### So, I suspend 1st task.","1e1f50f4":"### Yes! a little higher..","65d18e9d":"all right! we've done!","f9a47b5e":"### First, We try 1st task\n\n#### so, we have to make model to predict Potability whether 1 or 0","ff717c41":"there are many nulls in some columns\n\nso, we have to fill those null with some representive value(mean? median? predictions??)\n\n\nbecause the each rate of null is higher than 1%...","562df3d1":"PH is great normal distributions!!\n\nkeep seeking and show all in bulk"}}