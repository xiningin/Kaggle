{"cell_type":{"547e36d3":"code","65157077":"code","767f9ac7":"code","398c2295":"code","e6891537":"code","d20db027":"code","cbe49b21":"code","105161b2":"code","cfc52e28":"code","158e7810":"code","a138507c":"code","5d6b54df":"code","3d608c54":"code","accb7668":"code","8ee7e767":"code","8b577ff2":"code","635aed19":"code","a3f10bf7":"code","6e82a82d":"code","9f17c3c7":"markdown","e3cf4c65":"markdown","e8d86e85":"markdown","c22a5d3b":"markdown","0474990b":"markdown"},"source":{"547e36d3":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport os\nfrom xgboost import XGBClassifier\nfrom sklearn.metrics import mean_absolute_error\n\n# apply ignore\nimport warnings\nwarnings.filterwarnings('ignore')","65157077":"#load train data\ntrain_data = pd.read_csv('..\/input\/learn-together\/train.csv')\ntrain_data.head()","767f9ac7":"from sklearn.model_selection import train_test_split\n\n# Select columns \nselected_features = [cname for cname in train_data.columns if cname not in ['Id','Cover_Type']]\n\nX = train_data[selected_features]\ny = train_data.Cover_Type\n\n# Break off validation set from training data\nX_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.15, random_state=0)","398c2295":"# Define the model \n# multiclass classification objective ('multi:softmax', 'multi:softprob')\nxgb_model = XGBClassifier(max_depth=50, learning_rate=0.04, \n                          objective='multi:softmax', num_class=7)\n# Train and evaluate.\n# multiclass classification eval_metric ('merror', 'mlogloss')\nevalmetric = 'merror'\nxgb_model.fit(X_train, y_train, eval_metric=[evalmetric], eval_set=[(X_train, y_train),(X_valid, y_valid)], verbose=False)","e6891537":"# Import the library\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\ndef visualize_acuracy(xgb, metric):\n    # Plot and display the performance evaluation\n    xgb_eval = xgb.evals_result()\n    eval_steps = range(len(xgb_eval['validation_0'][metric]))\n    fig, ax = plt.subplots(1, 1, sharex=True, figsize=(8, 6))\n    ax.plot(eval_steps, [1-x for x in xgb_eval['validation_0'][metric]], label='Train')\n    ax.plot(eval_steps, [1-x for x in xgb_eval['validation_1'][metric]], label='Test')\n    ax.legend()\n    ax.set_title('Accuracy')\n    ax.set_xlabel('Number of iterations')","d20db027":"visualize_acuracy(xgb_model, evalmetric)    ","cbe49b21":"from sklearn.metrics import accuracy_score\ndef score_accuracy(xgb, X, y):\n    # run trained model.\n    y_pred = xgb.predict(X)\n    # Check the accuracy of the trained model.\n    accuracy = accuracy_score(y, y_pred)\n    print(\"Accuracy: %.1f%%\" % (accuracy * 100.0))","105161b2":"score_accuracy(xgb_model, X_valid, y_valid)","cfc52e28":"from sklearn.model_selection import cross_val_score\ndef score_mean(xgb, X, y):\n    accuracies = cross_val_score(estimator = xgb, X = X, y = y, cv = 6)\n    print(\"Mean_XGB_Acc : \", accuracies.mean())","158e7810":"score_mean(xgb_model, X_valid, y_valid)","a138507c":"from sklearn.metrics import classification_report\ndef score_report(xgb, X, y):\n    # run trained model.\n    y_pred = xgb.predict(X)\n    print(classification_report(y, y_pred))","5d6b54df":"score_report(xgb_model, X_valid, y_valid)","3d608c54":"import seaborn as sns\nfrom sklearn.metrics import confusion_matrix\ndef plot_confusion_matrix(model, X, y, normalized=True, cmap='bone'):\n    # run trained model.\n    y_pred = model.predict(X)\n    classes = np.sort(y.unique()) # depends (y should have all labels)\n    # run trained model.\n    cm = confusion_matrix(y, y_pred)\n    # run trained model.\n    plt.figure(figsize=[7, 6])\n    norm_cm = cm\n    if normalized:\n        norm_cm = cm.astype('float') \/ cm.sum(axis=1)[:, np.newaxis]\n        sns.heatmap(norm_cm, annot=cm, fmt='g', xticklabels=classes, yticklabels=classes, cmap=cmap)\n        plt.savefig('confusion-matrix.png')","accb7668":"plot_confusion_matrix(xgb_model, X_valid, y_valid)","8ee7e767":"# Search for the best parameters.\nfrom sklearn.model_selection import GridSearchCV\n# set up parameter grid.\nparameters = { 'n_estimators': np.arange(50, 150, 10)}\nclf = GridSearchCV(xgb_model, parameters, scoring='accuracy', cv=5, n_jobs=-1, refit=True)\nclf.fit(X, y)","8b577ff2":"# Display the accuracy of best parameter combination on the test set.\nprint(\"Best score: %.1f%%\" % (clf.best_score_*100))\nprint(\"Best parameter set: %s\" % (clf.best_params_))","635aed19":"xgb_best = clf.best_estimator_","a3f10bf7":"# read test data file using pandas\ntest_data = pd.read_csv('..\/input\/learn-together\/test.csv')\n\n# make predictions \ntest_preds = xgb_best.predict(test_data[selected_features])\n\n# save to submit\noutput = pd.DataFrame({'Id': test_data.Id,\n                       'Cover_Type': test_preds})\noutput.to_csv('submission.csv', index=False)","6e82a82d":"output.head()","9f17c3c7":"The cross_val_score will return an array of values which are the accuracy returned by the model on 6 samples\/folds (cv argument).","e3cf4c65":"**Use grid search and cross-validation to tune the model**","e8d86e85":"### Build model\n\nSelect mlogloss as eval_metric, as we target multiclass clasification","c22a5d3b":"**Make Predictions**<br>\nRead the file of \"test\" data. And apply model to make predictions","0474990b":"> #### Visualize accuracy"}}