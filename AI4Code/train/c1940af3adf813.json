{"cell_type":{"dedbc687":"code","ffce2817":"code","2a154699":"code","c4da4715":"code","9d6aed95":"code","859a8d38":"code","3038b5bd":"code","1a9f3bba":"code","e896e1c7":"code","df29a31c":"code","85855ebc":"code","986d8729":"code","9ee71afd":"code","af7147a4":"code","385a351b":"code","0cb55cff":"code","89e2502d":"code","5d8297c7":"code","6c1dcbd8":"code","5f7132c3":"code","82f16970":"code","9886a644":"code","ea227558":"code","9bfdc2c3":"code","977eaa39":"code","0b07eb76":"code","92d58c02":"code","6f86ca79":"code","b1c824a1":"code","7b3fe07f":"markdown","a96068e0":"markdown","8c01d64d":"markdown","c5d886ef":"markdown","8064cd7d":"markdown","d29195b8":"markdown","b60836c6":"markdown","28837799":"markdown","6f427df3":"markdown","906efc57":"markdown"},"source":{"dedbc687":"# Pandas : librairie de manipulation de donn\u00e9es\n# NumPy : librairie de calcul scientifique\n# MatPlotLib : librairie de visualisation et graphiques\nimport pandas as pd\nimport numpy as np\nfrom matplotlib import pyplot as plt\nimport seaborn as sns\n\nfrom sklearn import metrics\nfrom sklearn import preprocessing\nfrom sklearn import model_selection\nfrom sklearn.metrics import classification_report, confusion_matrix, roc_curve, roc_auc_score,auc, accuracy_score\n\nfrom sklearn.preprocessing import StandardScaler, MinMaxScaler\n\nfrom sklearn.model_selection import train_test_split\n\nfrom IPython.core.display import HTML # permet d'afficher du code html dans jupyter","ffce2817":"from sklearn.model_selection import learning_curve\ndef plot_learning_curve(est, X_train, y_train) :\n    train_sizes, train_scores, test_scores = learning_curve(estimator=est, X=X_train, y=y_train, train_sizes=np.linspace(0.1, 1.0, 10),\n                                                        cv=5,\n                                                        n_jobs=-1)\n    train_mean = np.mean(train_scores, axis=1)\n    train_std = np.std(train_scores, axis=1)\n    test_mean = np.mean(test_scores, axis=1)\n    test_std = np.std(test_scores, axis=1)\n    plt.figure(figsize=(8,10))\n    plt.plot(train_sizes, train_mean, color='blue', marker='o', markersize=5, label='training accuracy')\n    plt.fill_between(train_sizes, train_mean + train_std, train_mean - train_std, alpha=0.15, color='blue')\n    plt.plot(train_sizes, test_mean,color='green', linestyle='--',marker='s', markersize=5,label='validation accuracy')\n    plt.fill_between(train_sizes,test_mean + test_std,test_mean - test_std,alpha=0.15, color='green')\n    plt.grid(b='on')\n    plt.xlabel('Number of training samples')\n    plt.ylabel('Accuracy')\n    plt.legend(loc='lower right')\n    plt.ylim([0.6, 1.0])\n    plt.show()","2a154699":"def plot_roc_curve(est,X_test,y_test) :\n    probas = est.predict_proba(X_test)\n    false_positive_rate, true_positive_rate, thresholds = roc_curve(y_test,probas[:, 1])\n    roc_auc = auc(false_positive_rate, true_positive_rate)\n    plt.figure(figsize=(8,8))\n    plt.title('Receiver Operating Characteristic')\n    plt.plot(false_positive_rate, true_positive_rate, 'b', label='AUC = %0.2f'% roc_auc)\n    plt.legend(loc='lower right')\n    plt.plot([0,1],[0,1],'r--')        # plus mauvaise courbe\n    plt.plot([0,0,1],[0,1,1],'g:')     # meilleure courbe\n    plt.xlim([-0.05,1.2])\n    plt.ylim([-0.05,1.2])\n    plt.ylabel('Taux de vrais positifs')\n    plt.xlabel('Taux de faux positifs')\n    plt.show","c4da4715":"df = pd.read_csv('..\/input\/fashionmnist\/fashion-mnist_train.csv')","9d6aed95":"df.head()","859a8d38":"labels = [\"T-shirt\/top\",\"Trouser\",\"Pullover\",\"Dress\",\"Coat\",\"Sandal\",\"Shirt\",\n          \"Sneaker\",\"Bag\",\"Ankle boot\"]","3038b5bd":"print(labels[df.label[0]])","1a9f3bba":"df.head(50)","e896e1c7":"y = df['label']","df29a31c":"X = df.drop(['label'], axis=1)","85855ebc":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)","986d8729":"X1 = np.array(X)","9ee71afd":"print(X1[0])","af7147a4":"image = X1[0].reshape(28,28)\nprint(image)","385a351b":"plt.imshow(image)","0cb55cff":"plt.imshow(image, cmap=\"gray_r\")\nplt.axis('off')\nplt.title(y[0])","89e2502d":"n_samples = len(df.index)\nimages = X1.reshape(n_samples,28,28)","5d8297c7":"plt.figure(figsize=(10,20))\nfor i in range(0,49) :\n    plt.subplot(10,5,i+1)\n    plt.axis('off')\n    plt.imshow(images[i], cmap=\"gray_r\")\n    plt.title(y[i])","6c1dcbd8":"from sklearn import ensemble\nrf = ensemble.RandomForestClassifier()\nrf.fit(X_train, y_train)\ny_rf = rf.predict(X_test)","5f7132c3":"plot_learning_curve(rf, X, y)","82f16970":"print(classification_report(y_test, y_rf))","9886a644":"cm = confusion_matrix(y_test, y_rf)\nprint(cm)","ea227558":"import xgboost as XGB\nxgb  = XGB.XGBClassifier()\nxgb.fit(X_train, y_train)\ny_xgb = xgb.predict(X_test)\ncm = confusion_matrix(y_test, y_xgb)\nprint(cm)\nprint(classification_report(y_test, y_xgb))","9bfdc2c3":"#Appliquer les r\u00e9seaux de neurones\n# MLPClassifier de sklearn - r\u00e9seau de neurones \u00e0 deux couches cach\u00e9es de 200 et 60 neurones\n\n## S\u00e9paration de la cible\ny = df['label']\nX = df.drop(['label'] , axis=1)\n\n## Normalisation des valeurs entre 0 et 1\nX = X\/255\n\n## S\u00e9paration des ensembles de train et de test\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)\n\n##MLPClassifier\nfrom sklearn.neural_network import MLPClassifier\nmlp = MLPClassifier(hidden_layer_sizes=(200,60))\nmlp.fit(X_train,y_train)\ny_mlp = mlp.predict(X_test)","977eaa39":"## Score de pertinence et matrice de confusion\nmlp_score = accuracy_score(y_test, y_mlp)\nprint(mlp_score)\npd.crosstab(y_test, y_mlp, rownames=['Reel'], colnames=['Prediction'], margins=True)","0b07eb76":"#Avec Keras il y a des \u00e9tapes suppl\u00e9mentaires qui ne sont pas faites automatiquement.  \n#On commence par coder la cible, un index est attribu\u00e9 \u00e0 chacune des 25 classes (0 \u00e0 24) - qui vaut alors 0 ou 1 (quand la classe correspond \u00e0 ce qui se trouve \u00e0 l'index).\n#Comme d'habitude, nous s\u00e9parons l'ensemble d'apprentissage (80%) et l'ensemble de test (20%), mais pour Keras il faut transformer ces dataframes en tableaux.Nous d\u00e9finissons ici un mod\u00e8le \u00e0 deux couches de 200 et 60 neurones. La derni\u00e8re couche comporte num_classes = 25 neurones qui correspond au nombre de classes. \n#Son activation est un softmax : la somme des valeurs de ses neurones est 1.Le mod\u00e8le est ensuite compil\u00e9 avec categorical_crossentropy comme mesure de distance.","92d58c02":"# R\u00e9seaux denses - Keras\/tensorflow\n\n## Codage de la cible en 0 ou 1\nfrom keras.utils.np_utils import to_categorical\nprint(y[0])\ny_cat = to_categorical(y)\nprint(y_cat[0])\nnum_classes = y_cat.shape[1]\nprint(num_classes)\n\n## S\u00e9paration de train et test\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y_cat, test_size=0.2, random_state=1)\n\n## Transformation des dataframes en tableaux\nX_train = np.array(X_train)\nX_test = np.array(X_test)\ny_train = np.array(y_train)\ny_test = np.array(y_test)\n\n## Mod\u00e8le en couches : Sequential\nfrom keras.models import Sequential\nfrom keras.layers import Dense\n\nmodel = Sequential()\nmodel.add(Dense(200, activation='relu'))\nmodel.add(Dense(60, activation='relu'))\nmodel.add(Dense(num_classes, activation='softmax'))\n\n## Compilation et entrainement du mod\u00e8le\nmodel.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\ntrain = model.fit(X_train , y_train , validation_data=(X_test,y_test), epochs=30, verbose=1)","6f86ca79":"## Evaluation de la pertinence du mod\u00e8le\nmodel.evaluate(X_test,y_test)\nprint(train.history['accuracy'])\nprint(train.history['val_accuracy'])\n\n## Affichaque des scores sur un graphique\ndef plot_scores(train) :\n    accuracy = train.history['accuracy']\n    val_accuracy = train.history['val_accuracy']\n    epochs = range(len(accuracy))\n    plt.plot(epochs, accuracy, 'b', label='Score apprentissage')\n    plt.plot(epochs, val_accuracy, 'r', label='Score validation')\n    plt.title('Scores')\n    plt.legend()\n    plt.show()\n\n\nplot_scores(train)","b1c824a1":"#On remarque ici un surapprentissage assez important.","7b3fe07f":"<img src=\"https:\/\/github.com\/zalandoresearch\/fashion-mnist\/blob\/master\/doc\/img\/fashion-mnist-sprite.png?raw=true\">","a96068e0":"**Appliquer des m\u00e9thodes de machine learning \u00e0 la reconnaissance des objets (for\u00eats al\u00e9atoires, xgboost, ...)**  ","8c01d64d":"# Exercice : Zalando Fashion MNIST","c5d886ef":"## Librairies et fonctions utiles","8064cd7d":"La premi\u00e8re image du dataset est un pull :","d29195b8":"## Zalando Fashion MNIST","b60836c6":"Fonction pour tracer les courbes d'apprentissage sur l'ensemble d'apprentissage et l'ensemble de validation :","28837799":"Fonction pour tracer la courbe ROC :","6f427df3":"Le dataset a \u00e9t\u00e9 constitu\u00e9 par Zalando :  \nhttps:\/\/github.com\/zalandoresearch\/fashion-mnist  \n  \nOn a un ensemble d'apprentissage de 60 000 images 28x28 pixels en niveaux de gris, et 10 classes de v\u00eatements : jupes, pantalons, baskets, ...","906efc57":"**Afficher les 50 premiers \u00e9l\u00e9ments du dataset avec leur label**  \n"}}