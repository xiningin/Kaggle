{"cell_type":{"ee8431d5":"code","4a9670da":"code","82f93793":"code","c63f7aa3":"code","37889d8c":"code","dc6331a6":"code","07971663":"code","7f5beb30":"code","6041287d":"code","56fd53ca":"code","74466169":"code","f76ca36f":"code","4f3188a6":"code","aa08e9b3":"code","34e5db35":"code","e669e992":"markdown","d60064ff":"markdown","53af8645":"markdown","68b74ec8":"markdown","d21f4166":"markdown","aa4e391f":"markdown","a31478c9":"markdown","ac1f7d58":"markdown","8f2ed044":"markdown","f2f77e8a":"markdown","54144486":"markdown","574ee08e":"markdown","867a8ada":"markdown","f3764e1f":"markdown","16b97d3a":"markdown","59fe9c95":"markdown","85ab425b":"markdown","cfaa0828":"markdown","b2c2ef13":"markdown","ee14c58c":"markdown","5177ce5b":"markdown"},"source":{"ee8431d5":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","4a9670da":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\n#Suppressing warnings\nimport warnings\nwarnings.filterwarnings('ignore')","82f93793":"df = pd.read_csv('..\/input\/occupancy-detection-data-set-uci\/datatraining.txt')","c63f7aa3":"df.head()","37889d8c":"feature_names = [\n    'Temperature', 'Humidity', 'Light', 'CO2',\n       'HumidityRatio',\n    ]\n\ntarget_name = 'Occupancy'\n\nX = df[feature_names]\ny = df[target_name]","dc6331a6":"from yellowbrick.features import Rank1D\n\n# Instantiate the 1D visualizer with the Sharpiro ranking algorithm\nvisualizer = Rank1D(features=feature_names, algorithm='shapiro')\n\nvisualizer.fit(X, y)                # Fit the data to the visualizer\nvisualizer.transform(X)             # Transform the data\nvisualizer.poof()                   # Draw\/show\/poof the data","07971663":"from yellowbrick.features import Rank2D \n\n# Instantiate the visualizer with the Pearson ranking algorithm\nvisualizer = Rank2D(features=feature_names, algorithm='pearson')\n\nvisualizer.fit(X, y)                # Fit the data to the visualizer\nvisualizer.transform(X)             # Transform the data\nvisualizer.poof()                   # Draw\/show\/poof the data","7f5beb30":"from yellowbrick.features import Rank2D \n\n# Instantiate the visualizer with the Pearson ranking algorithm\nvisualizer = Rank2D(features=feature_names, algorithm='covariance')\n\nvisualizer.fit(X, y)                # Fit the data to the visualizer\nvisualizer.transform(X)             # Transform the data\nvisualizer.poof()                   # Draw\/show\/poof the data","6041287d":"from yellowbrick.features import ParallelCoordinates\n\n# Instantiate the visualizer\nvisualizer = visualizer = ParallelCoordinates(\n    classes=classes, features=features,\n    sample=0.1, size = (800,200)\n)\n\nvisualizer.fit(X, y)      # Fit the data to the visualizer\nvisualizer.transform(X)   # Transform the data\nvisualizer.poof()         # Draw\/show\/poof the data","56fd53ca":"from yellowbrick.features import ParallelCoordinates\nfeatures = feature_names\nclasses = ['unoccupied', 'occupied']\n# Instantiate the visualizer\nvisualizer = visualizer = ParallelCoordinates(\n    classes=classes, features=features,\n    normalize='standard', sample=0.1, size = (800,300)\n)\nvisualizer.fit(X, y)     \nvisualizer.transform(X)   \nvisualizer.poof()","74466169":"# Classifier Evaluation Imports\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.linear_model import LogisticRegression \nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import train_test_split\nfrom yellowbrick.classifier import ClassificationReport,ConfusionMatrix","f76ca36f":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)","4f3188a6":"# Instantiate the classification model and visualizer \nbayes = GaussianNB()\nvisualizer = ClassificationReport(bayes, classes=classes)\nvisualizer.fit(X_train, y_train)  \nvisualizer.score(X_test, y_test)  \ng = visualizer.poof()","aa08e9b3":"bayes = LogisticRegression()\nvisualizer = ClassificationReport(bayes, classes=classes)\nvisualizer.fit(X_train, y_train)  \nvisualizer.score(X_test, y_test)  \ng = visualizer.poof()","34e5db35":"logReg = LogisticRegression()\nvisualizer = ConfusionMatrix(logReg)\nvisualizer.fit(X_train, y_train)  \nvisualizer.score(X_test, y_test)\ng = visualizer.poof()","e669e992":"Let\u2019s visualize classification reports for two models to decide which is better.\n* **Classification report using Gaussian NB**","d60064ff":"### Loading the dataset","53af8645":"# Analyzing Machine Learning Models with Yellowbrick","68b74ec8":"## Hyperparameter Tuning\n","d21f4166":"# Model Evaluation Visualizers\n\n\n\n![](https:\/\/miro.medium.com\/max\/589\/1*8FNVeKiyKljEDVInvVS_Vw.png)\n\nEssentials of Algorithm Selection\u00b2\n\nModel evaluation signifies how well the values predicted by the model match the actual labeled ones.  Yellowbrick has visualizers for classification,[regression](https:\/\/heartbeat.fritz.ai\/5-regression-loss-functions-all-machine-learners-should-know-4fb140e9d4b0), and clustering algorithms.  Let\u2019s see a select few.\n\n## [Evaluating Classifiers](https:\/\/www.scikit-yb.org\/en\/latest\/api\/classifier\/index.html#classification-visualizers)\n\nClassification models try to assign the dependent variables one or more categories. The  `[sklearn.metrics](https:\/\/scikit-learn.org\/stable\/modules\/classes.html#module-sklearn.metrics)`  module implements a function to measure classification performance.\n\n\n\n![](https:\/\/miro.medium.com\/max\/684\/1*bTFtDdlXUc5JMGkxX3tUtw.png)\n\n[Metrics for Classification](https:\/\/www.slideshare.net\/RebeccaBilbro\/learning-machine-learning-with-yellowbrick)\n\nYellowbrick implements the following classifier evaluations:\n![](https:\/\/miro.medium.com\/max\/385\/1*vBFjanPvJQNLu6nBDTtmfg.png)\n\nLet\u2019s implement a few of them on our data:","aa4e391f":"### [Parallel Coordinates](https:\/\/www.scikit-yb.org\/en\/latest\/api\/features\/pcoords.html#parallel-coordinates)\n\nThis technique is useful when we need to detect clusters of instances that have similar classes, and to note features that have high variance or different distributions.  Points that tend to cluster will appear closer together:\n","a31478c9":"### Importing the necessary libraries","ac1f7d58":"## Evaluating Regressors\nRegression models try to predict a target in a continuous space. The sklearn.metrics module implements function to measure classification performance.\n\n![](https:\/\/miro.medium.com\/max\/672\/1*Xo9ch5Js0Sg1nIcDYiOzig.png)\n\nMetrics for Regression\n\nYellowbrick implements the following regressor evaluation methods:\n![](https:\/\/miro.medium.com\/max\/490\/1*XH_iaHg8QZIU2ivOry9rtA.png)","8f2ed044":"[Parallel coordinates](https:\/\/www.juiceanalytics.com\/writing\/writing\/parallel-coordinates)  is a visualization technique used to plot individual data elements across many dimensions. Each of the dimensions corresponds to a vertical axis, and each data element is displayed as a series of connected points along the dimensions\/axes.\n\nThe groups of similar instances are called \u2018braids\u2019, and when there are distinct braids of different classes, it suggests there\u2019s enough separability that a  [classification algorithm](https:\/\/heartbeat.fritz.ai\/classification-model-evaluation-90d743883106)  might be able to discern between each class.","f2f77e8a":"Parallel Coordinates Normalised","54144486":"* **Confusion Matrix**\nThe ConfusionMatrix visualizer displays the accuracy score of the model, i.e. it shows how each of the predicted classes compares to their actual classes. Let\u2019s check out the confusion matrix for the Logistic Regression Model:","574ee08e":"Visual classification reports are used to compare classification models to select models that are \u201credder\u201d, e.g. have stronger classification metrics or that are more balanced.","867a8ada":"* **Classification report using Logistic Regression**","f3764e1f":"Split the dataset into training and testing sets:","16b97d3a":"## Model Selection Process\n\n![](https:\/\/miro.medium.com\/max\/816\/1*a9Po3znWlW2M_iFBKMB3EA.png)\n\n[](http:\/\/cseweb.ucsd.edu\/~arunkk\/vision\/SIGMODRecord15.pdf)\n\n\n[The Model Selection triple](https:\/\/www.scikit-yb.org\/en\/latest\/about.html)\n\nIt\u2019s been seen that more often than not, machine learning relies primarily on the models being used for inference. Practitioners have their favorites when it comes to  [model selection](https:\/\/heartbeat.fritz.ai\/model-evaluation-selection-i-30d803a44ee). This preference is built over time through experience and knowledge, but what actually happens under the hood is often not given enough importance.\n\nWhat is important to note is that the model selection process isn\u2019t all about picking the \u201cright\u201d or \u201cwrong\u201d algorithm\u2014it\u2019s actually much deeper and an  iterative process  that involves the following steps:\n\n1.  Selecting and\/or engineering the smallest and most predictive feature set.\n2.  Choosing a set of algorithms from a model family.\n3.  [Tuning the algorithm hyperparameters](https:\/\/heartbeat.fritz.ai\/tuning-machine-learning-hyperparameters-40265a35c9b8) to optimize performance.\n\nAll of the above points together constitute the  **Model Selection Triple**,which was first discussed in a 2015  [SIGMOD](http:\/\/cseweb.ucsd.edu\/~arunkk\/vision\/SIGMODRecord15.pdf)\u00b9 paper by Kumar et al.\n\n> \u201cModel selection is iterative and exploratory because the space of [model selection triples] is usually infinite, and it is generally impossible for analysts to know a priori which [combination] will yield satisfactory accuracy and\/or insights.\u201d\n\nThe  **Yellowbrick library**  is a diagnostic visualization platform for machine learning that allows data scientists to steer the model selection process and assist in diagnosing problems throughout the machine learning workflow.  In short, it tries to find a model described by a triple composed of features, an algorithm, and hyperparameters that best fit the data.\n\n----------\n\n## Yellowbrick\n\n[Yellowbrick](https:\/\/www.scikit-yb.org\/en\/latest\/about.html)  is an open source, Python project that extends the  [**scikit-learn API**](http:\/\/scikit-learn.org\/stable\/modules\/classes.html)  with visual analysis and diagnostic tools. The Yellowbrick API also wraps  [matplotlib](https:\/\/heartbeat.fritz.ai\/introduction-to-matplotlib-data-visualization-in-python-d9143287ae39)  to create interactive data explorations.\n\nIt extends the scikit-learn API with a new core object: **the Visualizer**.Visualizers allow visual models to be fit and transformed as part of the scikit-learn pipeline process, providing visuals throughout the transformation of high-dimensional data.\n\n### Advantages\n\nYellowbrick isn\u2019t a replacement for other data visualization libraries but helps to achieve the following:\n\n-   Model Visualization\n-   Data visualization for machine learning\n-   Visual Diagnostics\n-   Visual Steering\n\n## Installation\n\nYellowbrick can either be installed through  `pip`  or through  `conda`distribution. For detailed instructions, you may want to refer the  [documentation](https:\/\/www.scikit-yb.org\/en\/latest\/quickstart.html#installation).\n\n## Usage\n\nThe Yellowbrick API should appear easy if you are familiar with the scikit-learn interface.\n\n\n![](https:\/\/miro.medium.com\/max\/1073\/1*3zNcu8BnQDQ8KSTd8_6miw.png)\n\nThe  primary interface is a `Visualizer` \u2013 an object that learns from data to produce a visualization.  In order to use visualizers, import the visualizer, instantiate it, call the visualizer\u2019s `fit()` method, and then, in order to render the visualization, call the visualizer\u2019s `poof()` method, which does the magic!\n\n### Dataset\nThe datasets bellongs to the  [UCI Machine Learning Repository](https:\/\/archive.ics.uci.edu\/ml\/datasets\/Occupancy+Detection+). It\u2019s an experimental dataset used for binary classification wherein the idea is to predict room occupancy given the variables such as Temperature, Humidity, Light and CO2. You can download the dataset here.\n\n\n","59fe9c95":"## Feature Analysis with Yellowbrick\n\n\n![](https:\/\/miro.medium.com\/max\/579\/1*c8HvVmh0IJi-2uYagygdXA.png)\n\nEssentials of Feature Analysis\u00b2\n\n[Feature engineering](https:\/\/heartbeat.fritz.ai\/introduction-to-automated-feature-engineering-using-deep-feature-synthesis-dfs-3feb69a7c00b)  requires an understanding of the relationships between features\u2014hence the  [feature analysis visualizer](http:\/\/following%20feature%20analysis%20visualizers%20implemented\/)s in Yellowbrick help to visualize the data in space so that important features can be detected.\n\nThe visualizers focus on aggregation, optimization, and other techniques to give overviews of the data. The following feature analysis visualizers have been implemented in Yellowbrick currently:\n\n\n![](https:\/\/miro.medium.com\/max\/641\/1*_bebdhecZRjZp-eFAXLTlQ.png)\n\n__Feature analysis visualizers in Yellowbrick__\n\nLet\u2019s go through some of them to see how they\u2019re implemented.\n\n### Rank Features\n\nRank Features rank single and pairs of features to detect **covariance**. Ranking can be 1D or 2D depending on the number of features utilized for ranking.\n\n[**Rank 1D**](https:\/\/www.scikit-yb.org\/en\/latest\/api\/features\/rankd.html#rank-1d)\n\nRank 1D utilizes a ranking algorithm that takes into account only a single feature at a time. By default, the  [**Shapiro-Wilk algorithm**](https:\/\/en.wikipedia.org\/wiki\/Shapiro%E2%80%93Wilk_test)  is used to assess the normality of the distribution of instances with respect to the feature:","85ab425b":"\nVisualization thus has a critical role to play throughout the analytical process and is a, frankly, a must-have for any effective analysis, for model selection, and for evaluation. This notebook aims to discuss a diagnostic platform called Yellowbrick that allows data scientists to visualize the entire model selection process to steer us towards better, more explainable models\u2014and avoid pitfalls and traps along the way.\n\n\n## Table of Contents\n\n* Model Selection Process\n* Yellowbrick\n* Usage and Dataset\n* Feature Analysis with Yellowbrick\n* Model Evaluation Visualizers\n* Hyperparameter Tuning\n* Repository\n* Conclusion\n* References","cfaa0828":"![](https:\/\/miro.medium.com\/max\/852\/1*Jp0fjDOLeytGIE5QFS9bfw.png)\n\nEssentials of Hyperparameter Tuning\u00b2\n\nTuning a model is as important as model selection. One of the ways you can use Yellowbrick for hyperparameter tuning apart from the alpha selection includes:\n\n## [Silhouette Visualizer](https:\/\/www.scikit-yb.org\/en\/latest\/api\/cluster\/silhouette.html#silhouette-visualizer)\n\nThe Silhouette Visualizer displays the silhouette coefficient for each sample on a per-cluster basis, visualizing which clusters are dense and which are not.\n\n\n\n![](https:\/\/miro.medium.com\/max\/518\/1*iqR23_ftzPVVl0JZSdb0oA.png)\n\nApart from this, there are a bunch of other visualizer APIs that can be used for tuning, like the Elbow method, which is also widely used. But for the sake of demonstration, we\u2019ll stick with just this one method.\n\n\n# Repository\n\nThe code and the datasets used in this article are available on  [**_my GitHub Repository_**](https:\/\/nbviewer.jupyter.org\/github\/parulnith\/Analysing-Machine-Learning-Models-with-Yellowbrick\/blob\/master\/Analysing%20Machine%20Learning%20Models%20with%20Yellowbrick.ipynb).\n\n# Conclusion\n\nThe Yellowbrick library allows data scientists to steer the model selection process. The fact that it extends the scikit-learn API lowers the learning curve considerably. This can help in understanding a large variety of algorithms and methods and in monitoring model performance in real-world applications.\n\n----------\n\n# References\n\n1.  [Model Selection Management Systems: The Next Frontier of Advanced Analytics](http:\/\/cseweb.ucsd.edu\/~arunkk\/vision\/SIGMODRecord15.pdf)\n2.  [Visual diagnostics for more effective machine learning](https:\/\/www.slideshare.net\/BenjaminBengfort\/visual-diagnostics-for-more-effective-machine-learning-127690054)\n3.  [Learning machine learning with Yellowbrick](https:\/\/www.slideshare.net\/RebeccaBilbro\/learning-machine-learning-with-yellowbrick).\n4.  [Yellowbrick documentation](https:\/\/www.scikit-yb.org\/en\/latest\/)\n\n","b2c2ef13":"[**Rank 2D**](https:\/\/www.scikit-yb.org\/en\/latest\/api\/features\/rankd.html#rank-2d)\n\nRank 2D, on the other hand, performs pairwise feature analysis as a heatmap. The default ranking algorithm is covariance, but we can also use the Pearson score:","ee14c58c":"### Specifying the feature and target column","5177ce5b":"#### Classification Report\nThe classification report visualizer displays the precision, recall, and F1 scores for the model:\n\nprecision = true positives \/ (true positives + false positives)\nrecall = true positives \/ (false negatives + true positives)\nF1 score = 2 * ((precision * recall) \/ (precision + recall))"}}