{"cell_type":{"7f72ffd1":"code","4c1daf62":"code","24bc769c":"code","1e516d20":"code","79a67c53":"code","01057a07":"code","de17c07f":"code","79541191":"code","0225b960":"code","8c5bb176":"code","a0d7cf54":"code","228f656b":"code","549ec050":"code","294c1f8c":"code","2ddf5f03":"code","37b78123":"code","93cb6a1f":"code","ac05aed1":"code","d5115ab2":"code","999fda8a":"code","e97fc48c":"code","b0bc9e41":"code","25d2adcd":"code","304d8e02":"code","891897e3":"code","54e23707":"code","c749f125":"code","4f1edc36":"code","2c2bc25d":"code","9148e837":"code","fe1a61f7":"code","4868699b":"code","b011d17f":"code","edcc2615":"code","54bbcfe9":"code","8eaf2a99":"code","b1070323":"code","e1c5e28e":"code","5c127add":"code","aeede202":"code","305cb121":"code","91ba42b8":"code","cb0bc37a":"code","7c894e39":"code","d4439a30":"code","4c29b241":"markdown","2c90d803":"markdown","309be144":"markdown","2e8e24bb":"markdown","df879beb":"markdown","a5803d35":"markdown","ccd4a14d":"markdown","cd351493":"markdown","5c713a9c":"markdown","dc5e7872":"markdown","edc5418c":"markdown","8e3c54ce":"markdown","c0789fbf":"markdown","45501a3a":"markdown","b7c23d08":"markdown","14c91512":"markdown","c918e387":"markdown","0d63bf56":"markdown","a4cfa1a2":"markdown","98889a6e":"markdown","f56a1420":"markdown","62f829a1":"markdown","11423624":"markdown","003c9de0":"markdown","c6ee8724":"markdown","8244ea23":"markdown","230463eb":"markdown","5f407f0d":"markdown","b0c64e44":"markdown","21469185":"markdown","b4e1fdfe":"markdown","1c28c253":"markdown","69505b2c":"markdown","646ce4d8":"markdown","f6069ae0":"markdown","7c9fc138":"markdown","163ae8ec":"markdown","dc9ac229":"markdown","702523fd":"markdown","c3e9ecec":"markdown","1d9165f0":"markdown","aa233444":"markdown","2d31a28f":"markdown"},"source":{"7f72ffd1":"import pandas as pd\nimport numpy as np\nimport os\nimport matplotlib.pyplot as plt\nimport warnings \nimport seaborn as sns\nwarnings.filterwarnings('ignore')\n%matplotlib inline","4c1daf62":"data = pd.read_csv('..\/input\/epileptic-seizure-recognition\/Epileptic Seizure Recognition.csv', error_bad_lines=False, index_col=False, engine='python')\ndata=data.drop('Unnamed', axis=1) # This col is uninformative","24bc769c":"print(data.shape)","1e516d20":"print(data.columns.tolist())","79a67c53":"print(data.dtypes)\nprint(data.dtypes.value_counts())","01057a07":"data['y']=np.where(data['y'] >1, 0,  1)","de17c07f":"data.y.value_counts()","79541191":"print(data.isna().sum().sort_values(ascending=False))","0225b960":"feature_cols=data.drop('y', axis=1).columns","8c5bb176":"log_columns = data[feature_cols].skew().sort_values(ascending=False)\nto_log = log_columns.loc[log_columns > 0.75]","a0d7cf54":"print('The number of columns which are sufficiently skewed to require transformation is: ', len(to_log))","228f656b":"data_orig = data.copy() # quickly make a copy before we change the data too much\nfrom sklearn.preprocessing import MinMaxScaler","549ec050":"scaler = MinMaxScaler()","294c1f8c":"for col in data[feature_cols].columns:\n    data[col] = scaler.fit_transform(data[[col]]).squeeze()\n    \ndata.describe() # Ensure all cols are between 0-1, looks good.","2ddf5f03":"from sklearn.model_selection import StratifiedShuffleSplit","37b78123":"X=data.drop('y', axis=1)\ny=data.y","93cb6a1f":"strat_shuff_split = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=42)","ac05aed1":"train_idx, test_idx = next(strat_shuff_split.split(X, y))","d5115ab2":"X_train = data.loc[train_idx, X.columns.values]\ny_train = data.loc[train_idx, 'y']","999fda8a":"X_test = data.loc[test_idx, X.columns.values]\ny_test = data.loc[test_idx, 'y']","e97fc48c":"print(y_train.value_counts(normalize=True).sort_index())","b0bc9e41":"print(y_test.value_counts(normalize=True).sort_index())","25d2adcd":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis,  QuadraticDiscriminantAnalysis\nfrom sklearn.svm import SVC\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.metrics import confusion_matrix, accuracy_score, log_loss, precision_score, recall_score, f1_score, roc_auc_score","304d8e02":"classifiers = [\n    LogisticRegression(),\n    SVC(kernel=\"rbf\", C=0.025, probability=True),\n    DecisionTreeClassifier(),\n    RandomForestClassifier(),\n    GradientBoostingClassifier(),\n    MLPClassifier()]","891897e3":"for clf in classifiers:\n    clf.fit(X_train, y_train)\n    name = clf.__class__.__name__\n    \n    print(\"=\"*30)\n    print(name)\n    \n    print('****Results****')\n    train_predictions = clf.predict(X_test)\n    \n    # calculate score\n    acc = accuracy_score(y_test, train_predictions)\n    precision = precision_score(y_test, train_predictions, average = 'macro') \n    recall = recall_score(y_test, train_predictions, average = 'macro') \n    f_score = f1_score(y_test, train_predictions, average = 'macro')\n    \n    print(\"Precision: {:.4%}\".format(precision))\n    print(\"Recall: {:.4%}\".format(recall))\n    print(\"F-score: {:.4%}\".format(recall))\n    print(\"Accuracy: {:.4%}\".format(acc))\n    \nprint(\"=\"*30)","54e23707":"from sklearn.decomposition import PCA","c749f125":"pca_list = list()\nfeature_weight_list = list()","4f1edc36":"for n in range(2, 30):\n    \n    # Create and fit the model\n    PCAmod = PCA(n_components=n)\n    PCAmod.fit(data)\n    \n    # Store the model and variance\n    pca_list.append(pd.Series({'n':n, 'model':PCAmod,\n                               'var': PCAmod.explained_variance_ratio_.sum()}))\n    \n    # Calculate and store feature importances\n    abs_feature_values = np.abs(PCAmod.components_).sum(axis=0)\n    feature_weight_list.append(pd.DataFrame({'n':n, \n                                             'features': data.columns,\n                                             'values':abs_feature_values\/abs_feature_values.sum()}))\n    \npca_df = pd.concat(pca_list, axis=1).T.set_index('n')\nprint (pca_df)","2c2bc25d":"pca_df['factors']=pca_df.index\npca_df['var'] = pd.to_numeric(pca_df['var'], errors='coerce')","9148e837":"sns.lineplot(data=pca_df, x='factors', y='var')","fe1a61f7":"from sklearn.pipeline import Pipeline\nstrat_shuff_split = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=42)","4868699b":"def get_avg_score(n):\n    pipe = [\n        ('scaler', MinMaxScaler()),\n        ('pca', PCA(n_components=n)),\n        ('estimator', RandomForestClassifier())\n    ]\n    pipe = Pipeline(pipe)\n    f1_scores = []\n    for train_index, test_index in strat_shuff_split.split(X, y):\n        X_train, X_test = X.loc[train_index], X.loc[test_index]\n        y_train, y_test = y.loc[train_index], y.loc[test_index]\n        pipe.fit(X_train, y_train)\n        f1_scores.append(f1_score(y_test, pipe.predict(X_test)))\n    return np.mean(f1_scores)","b011d17f":"ns = [5, 10, 20, 30, 40, 50, 100]\nscore_list = [get_avg_score(n) for n in ns]\nprint(score_list)","edcc2615":"ax = plt.axes()\nax.plot(ns, score_list)\nax.set(xlabel='Number of Dimensions',\n       ylabel='F1 Score')\nax.grid(True)","54bbcfe9":"from sklearn.cluster import KMeans","8eaf2a99":"kmeans_list = list()","b1070323":"for clusters in range(1,15):\n    km = KMeans(n_clusters=clusters, random_state=42)\n    km = km.fit(data[feature_cols])\n    \n    kmeans_list.append(pd.Series({'clusters': clusters, \n                              'inertia': km.inertia_,\n                              'model': km}))\n# Prepare to plot    \nelbow_graph_data = (pd.concat(kmeans_list, axis=1)\n             .T\n             [['clusters','inertia']])\nelbow_graph_data[['clusters','inertia']]=elbow_graph_data[['clusters','inertia']].apply(pd.to_numeric, errors='coerce')","e1c5e28e":"sns.lineplot(data=elbow_graph_data, x='clusters', y='inertia')","5c127add":"km = KMeans(n_clusters=5, random_state=42)\nkm = km.fit(data[feature_cols])","aeede202":"data['km_cluster'] = km.predict(data[feature_cols])","305cb121":"from sklearn.cluster import AgglomerativeClustering\nAClust = AgglomerativeClustering(n_clusters=5, linkage='ward')\nAClust = AClust.fit(data[feature_cols])\ndata['AClust'] = AClust.fit_predict(data[feature_cols])","91ba42b8":"data_KM=data.drop(['AClust'], axis=1)\ndata_AG=data.drop(['km_cluster'], axis=1)\ndata_no_clusters=data.drop(['AClust', 'km_cluster'], axis=1)","cb0bc37a":"def test_clusters(estimator, X, y):\n    f1_scores = []\n    for train_index, test_index in strat_shuff_split.split(X, y):\n        X_train, X_test = X.loc[train_index], X.loc[test_index]\n        y_train, y_test = y.loc[train_index], y.loc[test_index]\n        estimator.fit(X_train, y_train)\n        f1_scores.append(f1_score(y_test, estimator.predict(X_test)))\n    return np.mean(f1_scores)","7c894e39":"f1_with_kmeans = test_clusters(RandomForestClassifier(), data_KM, y)\nf1_with_hierarchial = test_clusters(RandomForestClassifier(), data_AG, y)\nf1_with_no_clustering = test_clusters(RandomForestClassifier(), data_no_clusters, y)","d4439a30":"print('F1 score with K-means clustering:', f1_with_kmeans)\nprint('F1 score with hierarchial clustering:', f1_with_hierarchial)\nprint('F1 score with no clustering:', f1_with_no_clustering)","4c29b241":"# Import and Load ","2c90d803":"Great, we're working with complete data. Lets consider the distribution\/skew of features.","309be144":"# Baseline classification score<br>\nIn order to test the influence of PCA and clustering, we should first (a) find which clf algo we will use and (b) check what score we can get with a basic baseline model.","2e8e24bb":"# Main Objective<br>\nThe current primary objective is to compare how dimensionality reduction (PCA) and clustering affect classification performance.<br>\nI will split these into 2 discrete objectives. I will then test a few classification algos to determine the best approach to binary clf.","df879beb":"Do some cols have NA's?","a5803d35":"So we can avoid transforming at feature cols.","ccd4a14d":"What type are the columns?","cd351493":" Data Description","5c713a9c":"Here we see a very small improvement with clustering but the effect is minor as the performance is already maxed out.","dc5e7872":"# Suggestions for next steps.<br>\nThe baseline model was already very strong here. A couple of extra steps that could be worthy:<br>\n* Tuning the RF model to get optimal performance from the hyperparameters<br>\n* Combining PCA and clustering in order to greatly reduce n features (improving compute speed and scalability)<br>\n* Building ensemble models may potentially help. Though we are near perfect performance at this point.","edc5418c":"# Influence of Clustering on RF Performance","8e3c54ce":"From the above, it looks like Random Forest gives us our best F1 score at around 96. Lets see if we can improve with PCA\/Clustering.","c0789fbf":"# Epileptic Seizure Recognition","45501a3a":"Lets plot to see if there's any visual elbow","b7c23d08":"Lets first do some basic functions to understand the data","14c91512":"All subjects falling in classes 2, 3, 4, and 5 are subjects who did not have epileptic seizure. Only subjects in class 1 have epileptic seizure. Our motivation for creating this version of the data was to simplify access to the data via the creation of a .csv version of it. Although there are 5 classes most authors have done binary classification, namely class 1 (Epileptic seizure) against the rest<br>\nFor this reason we will use a binary classification.","c918e387":"What columns do we have?","0d63bf56":"The original dataset from the reference consists of 5 different folders, each with 100 files, with each file representing a single subject\/person. Each file is a recording of brain activity for 23.6 seconds. The corresponding time-series is sampled into 4097 data points. Each data point is the value of the EEG recording at a different point in time. So we have total 500 individuals with each has 4097 data points for 23.5 seconds. We divided and shuffled every 4097 data points into 23 chunks, each chunk contains 178 data points for 1 second, and each data point is the value of the EEG recording at a different point in time. So now we have 23 x 500 = 11500 pieces of information(row), each information contains 178 data points for 1 second(column), the last column represents the label y {1,2,3,4,5}. The response variable is y in column 179, the Explanatory variables X1, X2, ..., X178","a4cfa1a2":"The following data has been downloaded from the UCI ML Repo. The following blurb has been taken directly from the data webpage (https:\/\/archive.ics.uci.edu\/ml\/datasets\/Epileptic+Seizure+Recognition)","98889a6e":" Preparing data for ML","f56a1420":"Fit a range of PCA models","62f829a1":"Test","11423624":"# Finding the ideal PCA clusters","003c9de0":"# Exploratory Data Analysis<br>\nSince all of the feature columns are not very well labelled (e.g. X1, X2, .. X177), and there are a huge number of them, I won't bother with any vis as it's not very informative.","c6ee8724":"# Scaling<br>\nBefore any kind of dimension reduction we must ensure to scale the data","8244ea23":"## Clustering<br>\nNext I will consider whether adding a cluster variable functions to improve the performance above the basic baseline RF model.<br>\nFirst, lets work out how many clusters are required, by looping through clusters and using the elbow method. I will begin withna basic K-means","230463eb":"## Principle Component Analysis<br>\nFirst, we'll consider the number of components we should use to get a reasonable explanation of the data.","5f407f0d":"The data is not balanced. We'll need to pay attention to the scoring metrics and stratifying splitting. I won't bother up\/down sampling for this current exercise.","b0c64e44":"Lets split the y variable into 1 (having seizure) and 0 (no seizure)","21469185":"Perfect! Lets get classifying...<br>\nWe'll consider the F1 score as our primary indicator because it deals well with unbalanced data sets by comparing both precision and recall.","b4e1fdfe":"Ensure the train\/test split is equal","1c28c253":"Create the data sets","69505b2c":"From the above plot we can see that we achieve a very high score with as few as 40 components. If rapid computation is requied, we may consider using PCA to greatly reduce the features. ","646ce4d8":"# Classification\n\nLets see what performance we can get with a baseline model. This should help us pick our classifier algorithm.","f6069ae0":"Whats the shape of the data?","7c9fc138":"Now lets run the model with KM and AC clusters.","163ae8ec":"For good measure, lets investigate hierarchial clustering too.","dc9ac229":"No clear elbow is available. Lets run through a range of PCA values and see where the clf model gets us.","702523fd":"Is the data balanced?","c3e9ecec":" Visualising the Influence of n Components on F1 score","1d9165f0":"Get the index values from the generator","aa233444":"Theres really no clear elbow here. However, we do have domain knowledge- these features relate to 5 y classifications therefore there should technically be 5 clusters. Lets use this.","2d31a28f":"The categories of y pertain to:<br>\n* 5 - eyes open, means when they were recording the EEG signal of the brain the patient had their eyes open<br>\n* 4 - eyes closed, means when they were recording the EEG signal the patient had their eyes closed<br>\n* 3 - Yes they identify where the region of the tumor was in the brain and recording the EEG activity from the healthy brain area<br>\n* 2 - They recorder the EEG from the area where the tumor was located<br>\n* 1 - Recording of seizure activity"}}