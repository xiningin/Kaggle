{"cell_type":{"9e333635":"code","04c1bed1":"code","fc439255":"code","cee57d6c":"code","d0690b8e":"code","3b5b9d69":"code","d1120db8":"code","34150755":"code","0e9a2ad6":"code","10a62b95":"code","8339cb9d":"code","b38ae93b":"code","44af9816":"code","7ce3137b":"code","7e7b162f":"code","9646f00a":"code","da9fce14":"code","4b82cee1":"code","1fe28e05":"code","3e829cd9":"code","65687436":"code","1c3d4838":"code","95c7cb73":"code","dac3e587":"code","7263c5cd":"code","ab43f10a":"code","9ddea7b9":"code","a1665e3d":"code","b3e091f4":"code","6ae282cd":"code","ff65b052":"code","3b8f69b3":"code","c5b26946":"code","e89901ee":"code","63df72b5":"code","c2ade686":"code","e57023f2":"code","83528849":"code","41b6ba96":"code","4e7a2afe":"code","99c29a1c":"code","bcd4651b":"code","a1bb8f0b":"code","4cd5059a":"code","7e1d10c7":"code","db123b38":"code","0b8be456":"code","4823daa6":"code","46c6afc2":"code","51bc6cfd":"code","5f0cefcf":"code","bc0d08b8":"code","27a4bcd8":"code","309db5b7":"code","adbe7af9":"code","80859656":"code","9628d4ce":"code","bdab8983":"code","1afceec3":"code","3255c69e":"code","0849dc69":"code","8c0bab2b":"code","e2ed67a8":"code","3503733f":"code","6a1dcb0a":"code","1efb6522":"code","4fb4716f":"code","6ef3fbec":"code","e3b2aa53":"code","114eabc3":"code","f960124e":"code","321fa691":"code","778ef504":"code","4af21b2c":"code","870321fc":"code","9db9dee0":"code","7f683b76":"markdown","3d4c4be1":"markdown","e7269602":"markdown","96207024":"markdown","b82f5af1":"markdown","1eb9b24e":"markdown","e47038e5":"markdown","d8200489":"markdown","4199a578":"markdown","f25cbf41":"markdown","18b169ab":"markdown","3f3da067":"markdown","c9da730d":"markdown","3bd3f2b7":"markdown","307d1dc0":"markdown","9dc496da":"markdown","4263b1b3":"markdown","4d812a55":"markdown","3a1293dc":"markdown"},"source":{"9e333635":"folds = 3\n\ndebug = False\nnum_rounds=1000\n\nuse_ucf  = False # use ucf data in train\nuse_sort = False # sort by month\ndel_2016 = False # delete site0 2016 from train\ncopy_site2 = False\nucf_year = [2017, 2018] # ucf data year used in train ","04c1bed1":"import gc\nimport os\nfrom pathlib import Path\nimport random\nimport sys\n\nfrom tqdm import tqdm_notebook as tqdm\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom IPython.core.display import display, HTML\n\n# --- plotly ---\nfrom plotly import tools, subplots\nimport plotly.offline as py\npy.init_notebook_mode(connected=True)\nimport plotly.graph_objs as go\nimport plotly.express as px\nimport plotly.figure_factory as ff\n\n# --- models ---\nfrom sklearn import preprocessing\nfrom sklearn.model_selection import KFold\nimport lightgbm as lgb\nimport xgboost as xgb\nimport catboost as cb\n\nfrom sklearn.metrics import mean_squared_error","fc439255":"# Original code from https:\/\/www.kaggle.com\/gemartin\/load-data-reduce-memory-usage by @gemartin\n# Modified to support timestamp type, categorical type\n# Modified to add option to use float16 or not. feather format does not support float16.\nfrom pandas.api.types import is_datetime64_any_dtype as is_datetime\nfrom pandas.api.types import is_categorical_dtype\n\ndef reduce_mem_usage(df, use_float16=False):\n    \"\"\" iterate through all the columns of a dataframe and modify the data type\n        to reduce memory usage.        \n    \"\"\"\n    start_mem = df.memory_usage().sum() \/ 1024**2\n    print('Memory usage of dataframe is {:.2f} MB'.format(start_mem))\n    \n    for col in df.columns:\n        if is_datetime(df[col]) or is_categorical_dtype(df[col]):\n            # skip datetime type or categorical type\n            continue\n        col_type = df[col].dtype\n        \n        if col_type != object:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)  \n            else:\n                if use_float16 and c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                    df[col] = df[col].astype(np.float16)\n                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)\n        else:\n            df[col] = df[col].astype('category')\n\n    end_mem = df.memory_usage().sum() \/ 1024**2\n    print('Memory usage after optimization is: {:.2f} MB'.format(end_mem))\n    print('Decreased by {:.1f}%'.format(100 * (start_mem - end_mem) \/ start_mem))\n    \n    return df","cee57d6c":"!ls ..\/usr\/lib\/ucl_data_leakage_episode_2","d0690b8e":"%%time\nroot = Path('..\/input\/ashrae-feather-format-for-fast-loading')\n\ntrain_df = pd.read_feather(root\/'train.feather')\nweather_train_df = pd.read_feather(root\/'weather_train.feather')\nweather_test_df = pd.read_feather(root\/'weather_test.feather')\nbuilding_meta_df = pd.read_feather(root\/'building_metadata.feather')","3b5b9d69":"# load site 0 data\nucf_root = Path('..\/input\/ashrae-ucf-spider-and-eda-full-test-labels')\nleak0_df = pd.read_pickle(ucf_root\/'site0.pkl') \nleak0_df['meter_reading'] = leak0_df.meter_reading_scraped\nleak0_df.drop(['meter_reading_original','meter_reading_scraped'], axis=1, inplace=True)\nleak0_df.fillna(0, inplace=True)\nleak0_df.loc[leak0_df.meter_reading < 0, 'meter_reading'] = 0\nleak0_df = leak0_df[leak0_df.timestamp.dt.year > 2016]\nprint(len(leak0_df))","d1120db8":"leak0_df.head()","34150755":"# load site 1 data\nucl_root = Path('..\/usr\/lib\/ucl_data_leakage_episode_2')\nleak1_df = pd.read_pickle(ucl_root\/'site1.pkl') \nleak1_df['meter_reading'] = leak1_df.meter_reading_scraped\nleak1_df.drop(['meter_reading_scraped'], axis=1, inplace=True)\nleak1_df.fillna(0, inplace=True)\nleak1_df.loc[leak1_df.meter_reading < 0, 'meter_reading'] = 0\nleak1_df = leak1_df[leak1_df.timestamp.dt.year > 2016]\nprint(len(leak1_df))","0e9a2ad6":"leak1_df.head()","10a62b95":"# load site 2 data\nleak2_df = pd.read_csv('\/kaggle\/input\/asu-buildings-energy-consumption\/asu_2016-2018.csv')\nleak2_df['timestamp'] = pd.to_datetime(leak2_df['timestamp'])\nleak2_df.fillna(0, inplace=True)\nleak2_df.loc[leak2_df.meter_reading < 0, 'meter_reading'] = 0\n\nif copy_site2: # some data looks better than train in 2016\n    train_df = train_df.merge(leak2_df, left_on = ['building_id', 'meter', 'timestamp'], right_on = ['building_id', 'meter', 'timestamp'], how = \"left\")\n    train_df.loc[train_df.meter_reading_y.isnull() == False, 'meter_reading_x'] = train_df[train_df.meter_reading_y.isnull() == False].meter_reading_y\n    train_df.drop('meter_reading_y', axis=1, inplace=True)\n    train_df.rename(columns={'meter_reading_x': 'meter_reading'}, inplace=True)\n\nleak2_df = leak2_df[leak2_df.timestamp.dt.year > 2016]\nleak2_df = leak2_df[leak2_df.building_id!=245] # building 245 is missing now.\n\nprint(len(leak2_df))","8339cb9d":"train_df.head()","b38ae93b":"leak2_df.head()","44af9816":"print ('leak rate = {}'.format((len(leak0_df) + len(leak1_df) + len(leak2_df))  \/ len(train_df)))","7ce3137b":"if use_ucf:\n    leak0_df = leak0_df[leak0_df.timestamp.dt.year.isin(ucf_year)]\n    leak1_df = leak1_df[leak1_df.timestamp.dt.year.isin(ucf_year)]\n    leak2_df = leak2_df[leak2_df.timestamp.dt.year.isin(ucf_year)]\n    \n    train_df = pd.concat([train_df, leak0_df, leak1_df, leak2_df])\n    train_df.reset_index(inplace=True)\n    \n    weather_train_df = pd.concat([weather_train_df, weather_test_df])\n    weather_train_df.reset_index(inplace=True)","7e7b162f":"del weather_test_df, leak0_df, leak1_df, leak2_df #leak_meta_df\ngc.collect()","9646f00a":"train_df['date'] = train_df['timestamp'].dt.date\ntrain_df['meter_reading_log1p'] = np.log1p(train_df['meter_reading'])","da9fce14":"def plot_date_usage(train_df, meter=0, building_id=0):\n    train_temp_df = train_df[train_df['meter'] == meter]\n    train_temp_df = train_temp_df[train_temp_df['building_id'] == building_id]    \n    train_temp_df_meter = train_temp_df.groupby('date')['meter_reading_log1p'].sum()\n    train_temp_df_meter = train_temp_df_meter.to_frame().reset_index()\n    fig = px.line(train_temp_df_meter, x='date', y='meter_reading_log1p')\n    fig.show()","4b82cee1":"plot_date_usage(train_df, meter=0, building_id=0)","1fe28e05":"plot_date_usage(train_df, meter=0, building_id=105)","3e829cd9":"plot_date_usage(train_df, meter=0, building_id=156)","65687436":"building_meta_df[building_meta_df.site_id == 0]","1c3d4838":"train_df = train_df.query('not (building_id <= 104 & meter == 0 & timestamp <= \"2016-05-20\")')","95c7cb73":"zone_dict={0:4,1:0,2:7,3:4,4:7,5:0,6:4,7:4,8:4,9:5,10:7,11:4,12:0,13:5,14:4,15:4} \n\ndef set_localtime(df):\n    for sid, zone in zone_dict.items():\n        sids = df.site_id == sid\n        df.loc[sids, 'timestamp'] = df[sids].timestamp - pd.offsets.Hour(zone)","dac3e587":"def preprocess(df):\n    df[\"hour\"] = df[\"timestamp\"].dt.hour\n#     df[\"day\"] = df[\"timestamp\"].dt.day\n    df[\"weekend\"] = df[\"timestamp\"].dt.weekday\n    df[\"month\"] = df[\"timestamp\"].dt.month\n    df[\"dayofweek\"] = df[\"timestamp\"].dt.dayofweek\n\n#     hour_rad = df[\"hour\"].values \/ 24. * 2 * np.pi\n#     df[\"hour_sin\"] = np.sin(hour_rad)\n#     df[\"hour_cos\"] = np.cos(hour_rad)","7263c5cd":"preprocess(train_df)","ab43f10a":"# sort train. i dont know it is best\nif use_ucf and use_sort:\n    train_df = train_df.sort_values('month')\n    train_df = train_df.reset_index()","9ddea7b9":"df_group = train_df.groupby('building_id')['meter_reading_log1p']\n#building_mean = df_group.mean().astype(np.float16)\nbuilding_median = df_group.median().astype(np.float16)\n#building_min = df_group.min().astype(np.float16)\n#building_max = df_group.max().astype(np.float16)\n#building_std = df_group.std().astype(np.float16)\n\n#train_df['building_mean'] = train_df['building_id'].map(building_mean)\ntrain_df['building_median'] = train_df['building_id'].map(building_median)\n#train_df['building_min'] = train_df['building_id'].map(building_min)\n#train_df['building_max'] = train_df['building_id'].map(building_max)\n#train_df['building_std'] = train_df['building_id'].map(building_std)\ndel df_group","a1665e3d":"#building_mean.head()","b3e091f4":"weather_train_df.head()","6ae282cd":"# weather_train_df.describe()","ff65b052":"weather_train_df.isna().sum()","3b8f69b3":"weather_train_df.shape","c5b26946":"weather_train_df.groupby('site_id').apply(lambda group: group.isna().sum())","e89901ee":"weather_train_df = weather_train_df.groupby('site_id').apply(lambda group: group.interpolate(limit_direction='both'))","63df72b5":"weather_train_df.groupby('site_id').apply(lambda group: group.isna().sum())","c2ade686":"def add_lag_feature(weather_df, window=3):\n    group_df = weather_df.groupby('site_id')\n    cols = ['air_temperature', 'cloud_coverage', 'dew_temperature', 'precip_depth_1_hr', 'sea_level_pressure', 'wind_direction', 'wind_speed']\n    rolled = group_df[cols].rolling(window=window, min_periods=0)\n    lag_mean = rolled.mean().reset_index().astype(np.float16)\n    lag_max = rolled.max().reset_index().astype(np.float16)\n    lag_min = rolled.min().reset_index().astype(np.float16)\n    lag_std = rolled.std().reset_index().astype(np.float16)\n    for col in cols:\n        weather_df[f'{col}_mean_lag{window}'] = lag_mean[col]\n        weather_df[f'{col}_max_lag{window}'] = lag_max[col]\n        weather_df[f'{col}_min_lag{window}'] = lag_min[col]\n        weather_df[f'{col}_std_lag{window}'] = lag_std[col]","e57023f2":"# skip lag feature to save memory\n#add_lag_feature(weather_train_df, window=3)\n#add_lag_feature(weather_train_df, window=72)","83528849":"set_localtime(weather_train_df)","41b6ba96":"weather_train_df.head()","4e7a2afe":"weather_train_df.columns","99c29a1c":"# categorize primary_use column to reduce memory on merge...\n\nprimary_use_list = building_meta_df['primary_use'].unique()\nprimary_use_dict = {key: value for value, key in enumerate(primary_use_list)} \nprint('primary_use_dict: ', primary_use_dict)\nbuilding_meta_df['primary_use'] = building_meta_df['primary_use'].map(primary_use_dict)\n\ngc.collect()","bcd4651b":"train_df = reduce_mem_usage(train_df, use_float16=True)\nbuilding_meta_df = reduce_mem_usage(building_meta_df, use_float16=True)\nweather_train_df = reduce_mem_usage(weather_train_df, use_float16=True)","a1bb8f0b":"building_meta_df.head()","4cd5059a":"category_cols = ['building_id', 'site_id', 'primary_use']  # , 'meter'\nfeature_cols = ['square_feet', 'year_built'] + [\n    'hour', 'weekend', # 'month' , 'dayofweek'\n    'building_median'] + [\n    'air_temperature', 'cloud_coverage',\n    'dew_temperature', 'precip_depth_1_hr', 'sea_level_pressure',\n    'wind_direction', 'wind_speed',]\n#     'air_temperature_mean_lag72',\n#     'air_temperature_max_lag72', 'air_temperature_min_lag72',\n#     'air_temperature_std_lag72', 'cloud_coverage_mean_lag72',\n#     'dew_temperature_mean_lag72', 'precip_depth_1_hr_mean_lag72',\n#     'sea_level_pressure_mean_lag72', 'wind_direction_mean_lag72',\n#     'wind_speed_mean_lag72', 'air_temperature_mean_lag3',\n#     'air_temperature_max_lag3',\n#     'air_temperature_min_lag3', 'cloud_coverage_mean_lag3',\n#     'dew_temperature_mean_lag3',\n#     'precip_depth_1_hr_mean_lag3', 'sea_level_pressure_mean_lag3',\n#     'wind_direction_mean_lag3', 'wind_speed_mean_lag3']","7e1d10c7":"def create_X_y(train_df, target_meter):\n    target_train_df = train_df[train_df['meter'] == target_meter]\n    target_train_df = target_train_df.merge(building_meta_df, on='building_id', how='left')\n    target_train_df = target_train_df.merge(weather_train_df, on=['site_id', 'timestamp'], how='left')\n    X_train = target_train_df[feature_cols + category_cols]\n    y_train = target_train_df['meter_reading_log1p'].values\n\n    del target_train_df\n    return X_train, y_train","db123b38":"def fit_lgbm(train, val, devices=(-1,), seed=None, cat_features=None, num_rounds=1500, lr=0.1, bf=0.1):\n    \"\"\"Train Light GBM model\"\"\"\n    X_train, y_train = train\n    X_valid, y_valid = val\n    metric = 'l2'\n    params = {'num_leaves': 31,\n              'objective': 'regression',\n#               'max_depth': -1,\n              'learning_rate': lr,\n              \"boosting\": \"gbdt\",\n              \"bagging_freq\": 5,\n              \"bagging_fraction\": bf,\n              \"feature_fraction\": 0.9,\n              \"metric\": metric,\n#               \"verbosity\": -1,\n#               'reg_alpha': 0.1,\n#               'reg_lambda': 0.3\n              }\n    device = devices[0]\n    if device == -1:\n        # use cpu\n        pass\n    else:\n        # use gpu\n        print(f'using gpu device_id {device}...')\n        params.update({'device': 'gpu', 'gpu_device_id': device})\n\n    params['seed'] = seed\n\n    early_stop = 20\n    verbose_eval = 20\n\n    d_train = lgb.Dataset(X_train, label=y_train, categorical_feature=cat_features)\n    d_valid = lgb.Dataset(X_valid, label=y_valid, categorical_feature=cat_features)\n    watchlist = [d_train, d_valid]\n\n    print('training LGB:')\n    model = lgb.train(params,\n                      train_set=d_train,\n                      num_boost_round=num_rounds,\n                      valid_sets=watchlist,\n                      verbose_eval=verbose_eval,\n                      early_stopping_rounds=early_stop)\n\n    # predictions\n    y_pred_valid = model.predict(X_valid, num_iteration=model.best_iteration)\n    \n    print('best_score', model.best_score)\n    log = {'train\/mae': model.best_score['training']['l2'],\n           'valid\/mae': model.best_score['valid_1']['l2']}\n    return model, y_pred_valid, log","0b8be456":"#folds = 5\nseed = 666\nshuffle = False\nkf = KFold(n_splits=folds, shuffle=shuffle, random_state=seed)\noof_total = 0","4823daa6":"target_meter = 0\nX_train, y_train = create_X_y(train_df, target_meter=target_meter)\ny_valid_pred_total = np.zeros(X_train.shape[0])\ngc.collect()\nprint('target_meter', target_meter, X_train.shape)\n\ncat_features = [X_train.columns.get_loc(cat_col) for cat_col in category_cols]\nprint('cat_features', cat_features)\n\nmodels0 = []\nfor train_idx, valid_idx in kf.split(X_train, y_train):\n    train_data = X_train.iloc[train_idx,:], y_train[train_idx]\n    valid_data = X_train.iloc[valid_idx,:], y_train[valid_idx]\n\n    print('train', len(train_idx), 'valid', len(valid_idx))\n#     model, y_pred_valid, log = fit_cb(train_data, valid_data, cat_features=cat_features, devices=[0,])\n    model, y_pred_valid, log = fit_lgbm(train_data, valid_data, cat_features=category_cols,\n                                        num_rounds=num_rounds, lr=0.05, bf=0.7)\n    y_valid_pred_total[valid_idx] = y_pred_valid\n    models0.append(model)\n    gc.collect()\n    if debug:\n        break\n\nsns.distplot(y_train)\nsns.distplot(y_valid_pred_total)\n\noof0 = mean_squared_error(y_train, y_valid_pred_total)\noof_total += oof0 * len(y_train)\n\ndel X_train, y_train\ngc.collect()","46c6afc2":"def plot_feature_importance(model):\n    importance_df = pd.DataFrame(model.feature_importance(),\n                                 index=feature_cols + category_cols,\n                                 columns=['importance']).sort_values('importance')\n    fig, ax = plt.subplots(figsize=(8, 8))\n    importance_df.plot.barh(ax=ax)\n    fig.show()","51bc6cfd":"target_meter = 1\nX_train, y_train = create_X_y(train_df, target_meter=target_meter)\ny_valid_pred_total = np.zeros(X_train.shape[0])\ngc.collect()\nprint('target_meter', target_meter, X_train.shape)\n\ncat_features = [X_train.columns.get_loc(cat_col) for cat_col in category_cols]\nprint('cat_features', cat_features)\n\nmodels1 = []\nfor train_idx, valid_idx in kf.split(X_train, y_train):\n    train_data = X_train.iloc[train_idx,:], y_train[train_idx]\n    valid_data = X_train.iloc[valid_idx,:], y_train[valid_idx]\n\n    print('train', len(train_idx), 'valid', len(valid_idx))\n#     model, y_pred_valid, log = fit_cb(train_data, valid_data, cat_features=cat_features, devices=[0,])\n    model, y_pred_valid, log = fit_lgbm(train_data, valid_data, cat_features=category_cols, num_rounds=num_rounds,\n                                       lr=0.05, bf=0.5)\n    y_valid_pred_total[valid_idx] = y_pred_valid\n    models1.append(model)\n    gc.collect()\n    if debug:\n        break\n\nsns.distplot(y_train)\nsns.distplot(y_valid_pred_total)\n\noof1 = mean_squared_error(y_train, y_valid_pred_total)\noof_total += oof1 * len(y_train)\n\ndel X_train, y_train\ngc.collect()","5f0cefcf":"target_meter = 2\nX_train, y_train = create_X_y(train_df, target_meter=target_meter)\ny_valid_pred_total = np.zeros(X_train.shape[0])\n\ngc.collect()\nprint('target_meter', target_meter, X_train.shape)\n\ncat_features = [X_train.columns.get_loc(cat_col) for cat_col in category_cols]\nprint('cat_features', cat_features)\n\nmodels2 = []\nfor train_idx, valid_idx in kf.split(X_train, y_train):\n    train_data = X_train.iloc[train_idx,:], y_train[train_idx]\n    valid_data = X_train.iloc[valid_idx,:], y_train[valid_idx]\n\n    print('train', len(train_idx), 'valid', len(valid_idx))\n#     model, y_pred_valid, log = fit_cb(train_data, valid_data, cat_features=cat_features, devices=[0,])\n    model, y_pred_valid, log = fit_lgbm(train_data, valid_data, cat_features=category_cols,\n                                        num_rounds=num_rounds, lr=0.05, bf=0.8)\n    y_valid_pred_total[valid_idx] = y_pred_valid\n    models2.append(model)\n    gc.collect()\n    if debug:\n        break\n\nsns.distplot(y_train)\nsns.distplot(y_valid_pred_total)\n\noof2 = mean_squared_error(y_train, y_valid_pred_total)\noof_total += oof2 * len(y_train)\n\ndel X_train, y_train\ngc.collect()","bc0d08b8":"target_meter = 3\nX_train, y_train = create_X_y(train_df, target_meter=target_meter)\ny_valid_pred_total = np.zeros(X_train.shape[0])\n\ngc.collect()\nprint('target_meter', target_meter, X_train.shape)\n\ncat_features = [X_train.columns.get_loc(cat_col) for cat_col in category_cols]\nprint('cat_features', cat_features)\n\nmodels3 = []\nfor train_idx, valid_idx in kf.split(X_train, y_train):\n    train_data = X_train.iloc[train_idx,:], y_train[train_idx]\n    valid_data = X_train.iloc[valid_idx,:], y_train[valid_idx]\n\n    print('train', len(train_idx), 'valid', len(valid_idx))\n#     model, y_pred_valid, log = fit_cb(train_data, valid_data, cat_features=cat_features, devices=[0,])\n    model, y_pred_valid, log = fit_lgbm(train_data, valid_data, cat_features=category_cols, num_rounds=num_rounds,\n                                       lr=0.03, bf=0.9)\n    y_valid_pred_total[valid_idx] = y_pred_valid\n    models3.append(model)\n    gc.collect()\n    if debug:\n        break\n\nsns.distplot(y_train)\nsns.distplot(y_valid_pred_total)\n\noof3 = mean_squared_error(y_train, y_valid_pred_total)\noof_total += oof3 * len(y_train)\n\ndel X_train, y_train\ngc.collect()","27a4bcd8":"oof_total = oof_total \/ len(train_df)","309db5b7":"del train_df, weather_train_df, building_meta_df \ngc.collect()","adbe7af9":"print('loading...')\ntest_df = pd.read_feather(root\/'test.feather')\nweather_test_df = pd.read_feather(root\/'weather_test.feather')\nbuilding_meta_df = pd.read_feather(root\/'building_metadata.feather')\n\nset_localtime(weather_test_df)\n\nprint('preprocessing building...')\ntest_df['date'] = test_df['timestamp'].dt.date\npreprocess(test_df)\n#test_df['building_mean'] = test_df['building_id'].map(building_mean)\ntest_df['building_median'] = test_df['building_id'].map(building_median)\n# test_df['building_min'] = test_df['building_id'].map(building_min)\n# test_df['building_max'] = test_df['building_id'].map(building_max)\n# test_df['building_std'] = test_df['building_id'].map(building_std)\n\nprint('preprocessing weather...')\nweather_test_df = weather_test_df.groupby('site_id').apply(lambda group: group.interpolate(limit_direction='both'))\nweather_test_df.groupby('site_id').apply(lambda group: group.isna().sum())\n\n# add_lag_feature(weather_test_df, window=3)\n# add_lag_feature(weather_test_df, window=72)\n\nprint('reduce mem usage...')\ntest_df = reduce_mem_usage(test_df, use_float16=True)\nweather_test_df = reduce_mem_usage(weather_test_df, use_float16=True)\n\ngc.collect()","80859656":"sample_submission = pd.read_feather(os.path.join(root, 'sample_submission.feather'))\nsample_submission = reduce_mem_usage(sample_submission)","9628d4ce":"def create_X(test_df, target_meter):\n    target_test_df = test_df[test_df['meter'] == target_meter]\n    target_test_df = target_test_df.merge(building_meta_df, on='building_id', how='left')\n    target_test_df = target_test_df.merge(weather_test_df, on=['site_id', 'timestamp'], how='left')\n    X_test = target_test_df[feature_cols + category_cols]\n    return X_test","bdab8983":"def pred(X_test, models, batch_size=1000000):\n    iterations = (X_test.shape[0] + batch_size -1) \/\/ batch_size\n    print('iterations', iterations)\n\n    y_test_pred_total = np.zeros(X_test.shape[0])\n    for i, model in enumerate(models):\n        print(f'predicting {i}-th model')\n        for k in tqdm(range(iterations)):\n            y_pred_test = model.predict(X_test[k*batch_size:(k+1)*batch_size], num_iteration=model.best_iteration)\n            y_test_pred_total[k*batch_size:(k+1)*batch_size] += y_pred_test\n\n    y_test_pred_total \/= len(models)\n    return y_test_pred_total\n","1afceec3":"%%time\nX_test = create_X(test_df, target_meter=0)\ngc.collect()\n\ny_test0 = pred(X_test, models0)\n\nsns.distplot(y_test0)\n\ndel X_test\ngc.collect()","3255c69e":"%%time\nX_test = create_X(test_df, target_meter=1)\ngc.collect()\n\ny_test1 = pred(X_test, models1)\nsns.distplot(y_test1)\n\ndel X_test\ngc.collect()","0849dc69":"%%time\nX_test = create_X(test_df, target_meter=2)\ngc.collect()\n\ny_test2 = pred(X_test, models2)\nsns.distplot(y_test2)\n\ndel X_test\ngc.collect()","8c0bab2b":"X_test = create_X(test_df, target_meter=3)\ngc.collect()\n\ny_test3 = pred(X_test, models3)\nsns.distplot(y_test3)\n\ndel X_test\ngc.collect()","e2ed67a8":"sample_submission.loc[test_df['meter'] == 0, 'meter_reading'] = np.expm1(y_test0)\nsample_submission.loc[test_df['meter'] == 1, 'meter_reading'] = np.expm1(y_test1)\nsample_submission.loc[test_df['meter'] == 2, 'meter_reading'] = np.expm1(y_test2)\nsample_submission.loc[test_df['meter'] == 3, 'meter_reading'] = np.expm1(y_test3)","3503733f":"if not debug:\n    sample_submission.to_csv('submission.csv', index=False, float_format='%.4f')","6a1dcb0a":"# sie 0\nleak_score0 = 0\n\nleak_df = pd.read_pickle(ucf_root\/'site0.pkl') \nleak_df['meter_reading'] = leak_df.meter_reading_scraped\nleak_df.drop(['meter_reading_original','meter_reading_scraped'], axis=1, inplace=True)\nleak_df.fillna(0, inplace=True)\nleak_df = leak_df[leak_df.timestamp.dt.year > 2016]\nleak_df.loc[leak_df.meter_reading < 0, 'meter_reading'] = 0 # remove large negative values\n\nsample_submission.loc[sample_submission.meter_reading < 0, 'meter_reading'] = 0\n\nfor bid in leak_df.building_id.unique():\n    temp_df = leak_df[(leak_df.building_id == bid)]\n    for m in temp_df.meter.unique():\n        v0 = sample_submission.loc[(test_df.building_id == bid)&(test_df.meter==m), 'meter_reading'].values\n        v1 = temp_df[temp_df.meter==m].meter_reading.values\n        \n        leak_score0 += mean_squared_error(np.log1p(v0), np.log1p(v1)) * len(v0)\n        \n        sample_submission.loc[(test_df.building_id == bid)&(test_df.meter==m), 'meter_reading'] = temp_df[temp_df.meter==m].meter_reading.values\n        \nleak_score0 \/= len(leak_df)","1efb6522":"# site 1\nleak_score1 = 0\n\nleak_df = pd.read_pickle(ucl_root\/'site1.pkl') \nleak_df['meter_reading'] = leak_df.meter_reading_scraped\nleak_df.drop(['meter_reading_scraped'], axis=1, inplace=True)\nleak_df.fillna(0, inplace=True)\nleak_df = leak_df[leak_df.timestamp.dt.year > 2016]\nleak_df.loc[leak_df.meter_reading < 0, 'meter_reading'] = 0 # remove large negative values\n\n#sample_submission.loc[sample_submission.meter_reading < 0, 'meter_reading'] = 0\n\nfor bid in leak_df.building_id.unique():\n    temp_df = leak_df[(leak_df.building_id == bid)]\n    for m in temp_df.meter.unique():\n        v0 = sample_submission.loc[(test_df.building_id == bid)&(test_df.meter==m), 'meter_reading'].values\n        v1 = temp_df[temp_df.meter==m].meter_reading.values\n        \n        leak_score1 += mean_squared_error(np.log1p(v0), np.log1p(v1)) * len(v0)\n        \n        sample_submission.loc[(test_df.building_id == bid)&(test_df.meter==m), 'meter_reading'] = temp_df[temp_df.meter==m].meter_reading.values\n\nleak_score1 \/= len(leak_df)        ","4fb4716f":"# site 2\nleak_score2 = 0\n\n# load site 2 data\nleak_df = pd.read_csv('\/kaggle\/input\/asu-buildings-energy-consumption\/asu_2016-2018.csv')\nleak_df['timestamp'] = pd.to_datetime(leak_df['timestamp'])\nleak_df.fillna(0, inplace=True)\nleak_df.loc[leak_df.meter_reading < 0, 'meter_reading'] = 0\nleak_df = leak_df[leak_df.timestamp.dt.year > 2016]\n\nleak_df = leak_df[leak_df.building_id!=245] # building 245 is missing now.\n\n#sample_submission.loc[sample_submission.meter_reading < 0, 'meter_reading'] = 0\n\nfor bid in leak_df.building_id.unique():\n    temp_df = leak_df[(leak_df.building_id == bid)]\n    for m in temp_df.meter.unique():\n        v0 = sample_submission.loc[(test_df.building_id == bid)&(test_df.meter==m), 'meter_reading'].values\n        v1 = temp_df[temp_df.meter==m].meter_reading.values\n        \n        leak_score2 += mean_squared_error(np.log1p(v0), np.log1p(v1)) * len(v0)\n        \n        sample_submission.loc[(test_df.building_id == bid)&(test_df.meter==m), 'meter_reading'] = temp_df[temp_df.meter==m].meter_reading.values\n\nleak_score2 \/= len(leak_df)        ","6ef3fbec":"if not debug:\n    sample_submission.to_csv('submission_ucf_replaced.csv', index=False, float_format='%.4f')","e3b2aa53":"sample_submission.head()","114eabc3":"np.log1p(sample_submission['meter_reading']).hist(bins=100)","f960124e":"print ('oof score meter0 =', np.sqrt(oof0))\nprint ('oof score meter1 =', np.sqrt(oof1))\nprint ('oof score meter2 =', np.sqrt(oof2))\nprint ('oof score meter3 =', np.sqrt(oof3))\nprint ('oof score total  =', np.sqrt(oof_total))","321fa691":"print ('UCF score = ', np.sqrt(leak_score0))\nprint ('UCL score = ', np.sqrt(leak_score1))\nprint ('ASU score = ', np.sqrt(leak_score2))","778ef504":"plot_feature_importance(models0[0])","4af21b2c":"plot_feature_importance(models1[0])","870321fc":"plot_feature_importance(models2[0])","9db9dee0":"plot_feature_importance(models3[0])","7f683b76":"Seems number of nan has reduced by `interpolate` but some property has never appear in specific `site_id`, and nan remains for these features.","3d4c4be1":"# Leak Data loading and concat","e7269602":"# OOF Scores","96207024":"# ASHRAE - Great Energy Predictor III\n\n\nOur aim in this competition is to predict energy consumption of buildings.\n\nThere are 4 types of energy to predict:\n\n - 0: electricity\n - 1: chilledwater\n - 2: steam\n - 3: hotwater\n\nElectricity and water consumption may have different behavior!\nSo I tried to separately train & predict the model.\n\nI moved previous [ASHRAE: Simple LGBM submission](https:\/\/www.kaggle.com\/corochann\/ashrae-simple-lgbm-submission) kernel.\n\n**[Update] I published \"[Optuna tutorial for hyperparameter optimization](https:\/\/www.kaggle.com\/corochann\/optuna-tutorial-for-hyperparameter-optimization)\" notebook.\nPlease also check it :)**","b82f5af1":"# Highway Route 2 is now open.\n\nHighway route0 and rroute 1 have been built [here](https:\/\/www.kaggle.com\/yamsam\/new-ucf-starter-kernel) and [here](https:\/\/www.kaggle.com\/yamsam\/ashrae-highway-kernel-route1).\n\nAs you already know there are huge data leak in this competition. Until now site-0, site-1 and site-2 building meter reading data were discovered by great kagglers.\nIn this kernel I will share all my findings about those leak data to try to keep this competition to be fair for all paticipants.\n\nThis Kernel are based on following great kernels:\n\n* [ASHRAE: Training LGBM by meter type](https:\/\/www.kaggle.com\/corochann\/ashrae-training-lgbm-by-meter-type)\n* [ASHRAE - UCF Spider and EDA (Full Test Labels)](https:\/\/www.kaggle.com\/gunesevitan\/ashrae-ucf-spider-and-eda-full-test-labels)\n* [UCL: Data Leakage (Episode 2)](https:\/\/www.kaggle.com\/mpware\/ucl-data-leakage-episode-2)\n* [ASU train and scraped test data](https:\/\/www.kaggle.com\/pdnartreb\/scrap-asu-data)\n\nThank you corochann and Gunes Evitan and MPWARE and Bertrand P. You are Great Kagglers!!\n\nAnd also Thanks UCF,UCL and ASU to make great meter reading data public, I have no offence indeed \ud83d\ude05","1eb9b24e":"# Add time feature","e47038e5":"# Data preprocessing\n\nNow, Let's try building GBDT (Gradient Boost Decision Tree) model to predict `meter_reading_log1p`. I will try using LightGBM in this notebook.","d8200489":"# References\n\nThese kernels inspired me to write this kernel, thank you for sharing!\n\n - https:\/\/www.kaggle.com\/rishabhiitbhu\/ashrae-simple-eda\n - https:\/\/www.kaggle.com\/isaienkov\/simple-lightgbm\n - https:\/\/www.kaggle.com\/ryches\/simple-lgbm-solution","4199a578":"# Train model by each meter type","f25cbf41":"# Replace to Leak data","18b169ab":"# Prediction on test data","3f3da067":"## Removing weired data on site_id 0\n\nAs you can see above, this data looks weired until May 20. It is reported in [this discussion](https:\/\/www.kaggle.com\/c\/ashrae-energy-prediction\/discussion\/113054#656588) by @barnwellguy that **All electricity meter is 0 until May 20 for site_id == 0**. I will remove these data from training data.\n\nIt corresponds to `building_id <= 104`.","c9da730d":"Some features introduced in https:\/\/www.kaggle.com\/ryches\/simple-lgbm-solution by @ryches\n\nFeatures that are likely predictive:\n\n#### Weather\n\n- time of day\n- holiday\n- weekend\n- cloud_coverage + lags\n- dew_temperature + lags\n- precip_depth + lags\n- sea_level_pressure + lags\n- wind_direction + lags\n- wind_speed + lags\n\n#### Train\n\n- max, mean, min, std of the specific building historically\n\n\n\nHowever we should be careful of putting time feature, since we have only 1 year data in training,\nincluding `date` makes overfiting to training data.\n\nHow about `month`? It may be better to check performance by cross validation.\nI go not using this data in this kernel for robust modeling.","3bd3f2b7":"# Fast data loading\n\nThis kernel uses the preprocessed data from my previous kernel, [\nASHRAE: feather format for fast loading](https:\/\/www.kaggle.com\/corochann\/ashrae-feather-format-for-fast-loading), to accelerate data loading!","307d1dc0":"# Fill Nan value in weather dataframe by interpolation\n\n\nweather data has a lot of NaNs!!\n\n![](http:\/\/)I tried to fill these values by **interpolating** data.","9dc496da":"# Train model\n\nTo win in kaggle competition, how to evaluate your model is important.\nWhat kind of cross validation strategy is suitable for this competition? This is time series data, so it is better to consider time-splitting.\n\nHowever this notebook is for simple tutorial, so I will proceed with KFold splitting without shuffling, so that at least near-term data is not included in validation.","4263b1b3":"# Leak Scores","4d812a55":"# FIX Time Zone","3a1293dc":"## lags\n\nAdding some lag feature"}}