{"cell_type":{"a0709222":"code","bc89bc02":"code","646a2511":"code","42f3313a":"code","4d218899":"code","f234e0f9":"code","15c1f1c2":"code","c884f299":"code","b4a61819":"code","bc3f3bce":"code","4666b407":"code","5540167b":"code","ef7483aa":"code","61e2f234":"code","48bccb10":"code","a09e8c55":"code","891dc62a":"code","60e137bd":"code","8f6ee595":"code","ef387e8a":"code","4c3882a3":"code","bd309852":"code","645ab4d4":"code","8228cfe3":"code","c3de2098":"code","06073788":"code","1361fa02":"code","07949424":"code","ddf93aab":"code","bb09d09f":"code","05164c3f":"code","5d42a98d":"code","acc1a762":"code","80897a42":"code","9d9b7c37":"code","e48b66b0":"code","e6d6d2d9":"code","5cd9f2d2":"code","81fb4617":"code","0a5c7b67":"code","97f4d2f8":"code","27bbb83c":"code","f54a5d04":"code","6227ffc5":"code","ed3632ef":"code","878a02e3":"code","29d2d8b4":"code","80a19c6c":"code","8626d6a4":"code","4058d979":"code","45441a3a":"code","50fd162f":"code","8cffc6ae":"code","15ce7a07":"code","9ea19a66":"code","3718606e":"code","119ac70d":"code","2a13d856":"code","00b0e811":"code","5c94dfac":"code","2e15e2da":"code","aef902b4":"code","0167ebbc":"code","f721d054":"code","24cae9f4":"code","e896740b":"code","b1f702b5":"code","d07043b4":"code","955ca1ed":"code","1be2fa4a":"code","f8182d08":"code","f30db709":"code","a99759d4":"code","8994d86f":"code","a77a9dba":"code","162bdc09":"code","23566d5d":"code","07747e3a":"code","9f58c68c":"code","6161a7ea":"code","414eb196":"code","a4506f4e":"code","ea9de7e1":"code","f554f73b":"code","e5018a88":"code","2e09bf20":"code","524f49c2":"code","235dfb51":"code","9cacb507":"markdown","f249e85f":"markdown","4031eeae":"markdown","0bab78cd":"markdown","8a09f997":"markdown","349b0c98":"markdown","5c6af1f3":"markdown","6300e90f":"markdown","e9aefa82":"markdown","9e0ab6e0":"markdown","b4cae402":"markdown","44b4cf5a":"markdown","68c60848":"markdown","cce3dcb7":"markdown","f97e797a":"markdown","e9b1bdb1":"markdown","7240fe60":"markdown","045f73ad":"markdown","ed008a26":"markdown","8c42dfa7":"markdown","d79ce397":"markdown","1a94cc98":"markdown","9653d926":"markdown","fcac11ce":"markdown","eac3be4c":"markdown","b10b6a3c":"markdown","30eba97f":"markdown","b9844595":"markdown","dbefca46":"markdown","d0c767fa":"markdown","4208444b":"markdown","a001fe65":"markdown","a0d2b17b":"markdown","55e9f7ff":"markdown","7e79d878":"markdown","de861d59":"markdown","0f5f7807":"markdown","c2b689bb":"markdown","0f904cc8":"markdown","10264c85":"markdown","60d220e7":"markdown","2adcd7f3":"markdown","6560400d":"markdown","77e32049":"markdown","2d4c7ee8":"markdown","3f7f3127":"markdown","c7da6e25":"markdown","0f61c982":"markdown","654b2da6":"markdown","a72fe38c":"markdown","a0f0392e":"markdown","1a4a48ba":"markdown","3d420a63":"markdown","c2f2652f":"markdown","1cfe1263":"markdown","226000a4":"markdown","bf86e146":"markdown"},"source":{"a0709222":"# Import packages\nimport numpy as np \nimport pandas as pd \nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\nplt.style.use('seaborn')\npd.set_option('max_columns', None)\nimport ast\nfrom collections import Counter\nfrom wordcloud import WordCloud\nfrom sklearn.model_selection import train_test_split, KFold\nfrom sklearn.model_selection import GridSearchCV\nimport xgboost as xgb\nimport lightgbm as lgb\nfrom catboost import CatBoostRegressor\nimport shap\nimport time\nfrom datetime import datetime\nfrom sklearn.metrics import mean_squared_error\nimport warnings\nwarnings.filterwarnings('ignore')","bc89bc02":"train = pd.read_csv('..\/input\/tmdb-box-office-prediction\/train.csv')\ntest = pd.read_csv('..\/input\/tmdb-box-office-prediction\/test.csv')","646a2511":"train.head()","42f3313a":"train.info()","4d218899":"test.info()","f234e0f9":"# Check missing values\ntrain.isnull().sum().sort_values(ascending=False)","15c1f1c2":"# Check missing values\ntest.isnull().sum().sort_values(ascending=False)","c884f299":"# Revising some wrong information for training and test sets\n# The information is from:\n# https:\/\/www.kaggle.com\/kamalchhirang\/eda-feature-engineering-lgb-xgb-cat#Feature-Engineering-&-Prediction\n\ntrain.loc[train['id'] == 16,'revenue'] = 192864          # Skinning\ntrain.loc[train['id'] == 90,'budget'] = 30000000         # Sommersby          \ntrain.loc[train['id'] == 118,'budget'] = 60000000        # Wild Hogs\ntrain.loc[train['id'] == 149,'budget'] = 18000000        # Beethoven\ntrain.loc[train['id'] == 313,'revenue'] = 12000000       # The Cookout \ntrain.loc[train['id'] == 451,'revenue'] = 12000000       # Chasing Liberty\ntrain.loc[train['id'] == 464,'budget'] = 20000000        # Parenthood\ntrain.loc[train['id'] == 470,'budget'] = 13000000        # The Karate Kid, Part II\ntrain.loc[train['id'] == 513,'budget'] = 930000          # From Prada to Nada\ntrain.loc[train['id'] == 797,'budget'] = 8000000         # Welcome to Dongmakgol\ntrain.loc[train['id'] == 819,'budget'] = 90000000        # Alvin and the Chipmunks: The Road Chip\ntrain.loc[train['id'] == 850,'budget'] = 90000000        # Modern Times\ntrain.loc[train['id'] == 1007,'budget'] = 2              # Zyzzyx Road \ntrain.loc[train['id'] == 1112,'budget'] = 7500000        # An Officer and a Gentleman\ntrain.loc[train['id'] == 1131,'budget'] = 4300000        # Smokey and the Bandit   \ntrain.loc[train['id'] == 1359,'budget'] = 10000000       # Stir Crazy \ntrain.loc[train['id'] == 1542,'budget'] = 1              # All at Once\ntrain.loc[train['id'] == 1570,'budget'] = 15800000       # Crocodile Dundee II\ntrain.loc[train['id'] == 1571,'budget'] = 4000000        # Lady and the Tramp\ntrain.loc[train['id'] == 1714,'budget'] = 46000000       # The Recruit\ntrain.loc[train['id'] == 1721,'budget'] = 17500000       # Cocoon\ntrain.loc[train['id'] == 1865,'revenue'] = 25000000      # Scooby-Doo 2: Monsters Unleashed\ntrain.loc[train['id'] == 1885,'budget'] = 12             # In the Cut\ntrain.loc[train['id'] == 2091,'budget'] = 10             # Deadfall\ntrain.loc[train['id'] == 2268,'budget'] = 17500000       # Madea Goes to Jail budget\ntrain.loc[train['id'] == 2491,'budget'] = 6              # Never Talk to Strangers\ntrain.loc[train['id'] == 2602,'budget'] = 31000000       # Mr. Holland's Opus\ntrain.loc[train['id'] == 2612,'budget'] = 15000000       # Field of Dreams\ntrain.loc[train['id'] == 2696,'budget'] = 10000000       # Nurse 3-D\ntrain.loc[train['id'] == 2801,'budget'] = 10000000       # Fracture\ntrain.loc[train['id'] == 335,'budget'] = 2 \ntrain.loc[train['id'] == 348,'budget'] = 12\ntrain.loc[train['id'] == 470,'budget'] = 13000000 \ntrain.loc[train['id'] == 513,'budget'] = 1100000\ntrain.loc[train['id'] == 640,'budget'] = 6 \ntrain.loc[train['id'] == 696,'budget'] = 1\ntrain.loc[train['id'] == 797,'budget'] = 8000000 \ntrain.loc[train['id'] == 850,'budget'] = 1500000\ntrain.loc[train['id'] == 1199,'budget'] = 5 \ntrain.loc[train['id'] == 1282,'budget'] = 9               # Death at a Funeral\ntrain.loc[train['id'] == 1347,'budget'] = 1\ntrain.loc[train['id'] == 1755,'budget'] = 2\ntrain.loc[train['id'] == 1801,'budget'] = 5\ntrain.loc[train['id'] == 1918,'budget'] = 592 \ntrain.loc[train['id'] == 2033,'budget'] = 4\ntrain.loc[train['id'] == 2118,'budget'] = 344 \ntrain.loc[train['id'] == 2252,'budget'] = 130\ntrain.loc[train['id'] == 2256,'budget'] = 1 \ntrain.loc[train['id'] == 2696,'budget'] = 10000000\n\ntest.loc[test['id'] == 6733,'budget'] = 5000000\ntest.loc[test['id'] == 3889,'budget'] = 15000000\ntest.loc[test['id'] == 6683,'budget'] = 50000000\ntest.loc[test['id'] == 5704,'budget'] = 4300000\ntest.loc[test['id'] == 6109,'budget'] = 281756\ntest.loc[test['id'] == 7242,'budget'] = 10000000\ntest.loc[test['id'] == 7021,'budget'] = 17540562       #  Two Is a Family\ntest.loc[test['id'] == 5591,'budget'] = 4000000        # The Orphanage\ntest.loc[test['id'] == 4282,'budget'] = 20000000       # Big Top Pee-wee\ntest.loc[test['id'] == 3033,'budget'] = 250 \ntest.loc[test['id'] == 3051,'budget'] = 50\ntest.loc[test['id'] == 3084,'budget'] = 337\ntest.loc[test['id'] == 3224,'budget'] = 4  \ntest.loc[test['id'] == 3594,'budget'] = 25  \ntest.loc[test['id'] == 3619,'budget'] = 500  \ntest.loc[test['id'] == 3831,'budget'] = 3  \ntest.loc[test['id'] == 3935,'budget'] = 500  \ntest.loc[test['id'] == 4049,'budget'] = 995946 \ntest.loc[test['id'] == 4424,'budget'] = 3  \ntest.loc[test['id'] == 4460,'budget'] = 8  \ntest.loc[test['id'] == 4555,'budget'] = 1200000 \ntest.loc[test['id'] == 4624,'budget'] = 30 \ntest.loc[test['id'] == 4645,'budget'] = 500 \ntest.loc[test['id'] == 4709,'budget'] = 450 \ntest.loc[test['id'] == 4839,'budget'] = 7\ntest.loc[test['id'] == 3125,'budget'] = 25 \ntest.loc[test['id'] == 3142,'budget'] = 1\ntest.loc[test['id'] == 3201,'budget'] = 450\ntest.loc[test['id'] == 3222,'budget'] = 6\ntest.loc[test['id'] == 3545,'budget'] = 38\ntest.loc[test['id'] == 3670,'budget'] = 18\ntest.loc[test['id'] == 3792,'budget'] = 19\ntest.loc[test['id'] == 3881,'budget'] = 7\ntest.loc[test['id'] == 3969,'budget'] = 400\ntest.loc[test['id'] == 4196,'budget'] = 6\ntest.loc[test['id'] == 4221,'budget'] = 11\ntest.loc[test['id'] == 4222,'budget'] = 500\ntest.loc[test['id'] == 4285,'budget'] = 11\ntest.loc[test['id'] == 4319,'budget'] = 1\ntest.loc[test['id'] == 4639,'budget'] = 10\ntest.loc[test['id'] == 4719,'budget'] = 45\ntest.loc[test['id'] == 4822,'budget'] = 22\ntest.loc[test['id'] == 4829,'budget'] = 20\ntest.loc[test['id'] == 4969,'budget'] = 20\ntest.loc[test['id'] == 5021,'budget'] = 40 \ntest.loc[test['id'] == 5035,'budget'] = 1 \ntest.loc[test['id'] == 5063,'budget'] = 14 \ntest.loc[test['id'] == 5119,'budget'] = 2 \ntest.loc[test['id'] == 5214,'budget'] = 30 \ntest.loc[test['id'] == 5221,'budget'] = 50 \ntest.loc[test['id'] == 4903,'budget'] = 15\ntest.loc[test['id'] == 4983,'budget'] = 3\ntest.loc[test['id'] == 5102,'budget'] = 28\ntest.loc[test['id'] == 5217,'budget'] = 75\ntest.loc[test['id'] == 5224,'budget'] = 3 \ntest.loc[test['id'] == 5469,'budget'] = 20 \ntest.loc[test['id'] == 5840,'budget'] = 1 \ntest.loc[test['id'] == 5960,'budget'] = 30\ntest.loc[test['id'] == 6506,'budget'] = 11 \ntest.loc[test['id'] == 6553,'budget'] = 280\ntest.loc[test['id'] == 6561,'budget'] = 7\ntest.loc[test['id'] == 6582,'budget'] = 218\ntest.loc[test['id'] == 6638,'budget'] = 5\ntest.loc[test['id'] == 6749,'budget'] = 8 \ntest.loc[test['id'] == 6759,'budget'] = 50 \ntest.loc[test['id'] == 6856,'budget'] = 10\ntest.loc[test['id'] == 6858,'budget'] =  100\ntest.loc[test['id'] == 6876,'budget'] =  250\ntest.loc[test['id'] == 6972,'budget'] = 1\ntest.loc[test['id'] == 7079,'budget'] = 8000000\ntest.loc[test['id'] == 7150,'budget'] = 118\ntest.loc[test['id'] == 6506,'budget'] = 118\ntest.loc[test['id'] == 7225,'budget'] = 6\ntest.loc[test['id'] == 7231,'budget'] = 85\ntest.loc[test['id'] == 5222,'budget'] = 5\ntest.loc[test['id'] == 5322,'budget'] = 90\ntest.loc[test['id'] == 5350,'budget'] = 70\ntest.loc[test['id'] == 5378,'budget'] = 10\ntest.loc[test['id'] == 5545,'budget'] = 80\ntest.loc[test['id'] == 5810,'budget'] = 8\ntest.loc[test['id'] == 5926,'budget'] = 300\ntest.loc[test['id'] == 5927,'budget'] = 4\ntest.loc[test['id'] == 5986,'budget'] = 1\ntest.loc[test['id'] == 6053,'budget'] = 20\ntest.loc[test['id'] == 6104,'budget'] = 1\ntest.loc[test['id'] == 6130,'budget'] = 30\ntest.loc[test['id'] == 6301,'budget'] = 150\ntest.loc[test['id'] == 6276,'budget'] = 100\ntest.loc[test['id'] == 6473,'budget'] = 100\ntest.loc[test['id'] == 6842,'budget'] = 30\n\ntest.loc[test['release_date'].isnull() == True, 'release_date'] = '01\/01\/98'","b4a61819":"# Convert JSON format columns to dictionary format\ntext_cols = ['belongs_to_collection', 'genres', 'production_companies',\n                'production_countries', 'spoken_languages', 'Keywords', 'cast', 'crew']\n\ndef text_to_dict(df):\n    for col in text_cols:\n        df[col] = df[col].apply(lambda x: {} if pd.isna(x) else ast.literal_eval(x))\n    return df\n\ntrain = text_to_dict(train)\ntest = text_to_dict(test)","bc3f3bce":"# Show top five columns\nfor i, e in enumerate(train['belongs_to_collection'][:5]):\n    print(i, e)","4666b407":"# Return unique value and count\ntrain['belongs_to_collection'].apply(lambda x: len(x) if x != {} else 0).value_counts()","5540167b":"# Bar plot of most frequent values\ncollections = train['belongs_to_collection'].apply(lambda x: x[0]['name'] if x != {} else 0).value_counts()[1:20]\nfig = plt.figure(figsize=(8, 5))\nsns.barplot(collections, collections.index)\nplt.xlabel('Count')\nplt.title('Top 20 Collecction Count')\nplt.show()","ef7483aa":"# Show the distribution of revenue for movies with or without a collection\ntrain['has_collection'] = train['belongs_to_collection'].apply(lambda x: len(x) if x != {} else 0)\nplt.figure(figsize=(8, 5))\nsns.catplot(x='has_collection', y='revenue', data=train)\nplt.xlabel('Does the movie belong to a collection?')\nplt.ylabel('Revenue')\nplt.show()","61e2f234":"# Show top five columns\nfor i, e in enumerate(train['genres'][:5]):\n    print(i, e)","48bccb10":"# Return unique value and count, and the frequency bar plot\ngenres_num = train['genres'].apply(lambda x: len(x) if x != {} else 0).value_counts()\nprint(genres_num)\nfig = plt.figure(figsize=(8, 5))\nsns.barplot(genres_num, genres_num.index, orient=\"h\", order=genres_num.sort_values(ascending = False).index)\nplt.xlabel('Count')\nplt.title('Number of Genres in Movies')\nplt.show()","a09e8c55":"# Extract genres of each film\ngenres_per = train['genres'].apply(lambda x: [i['name'] for i in x] if x != {} else [])\ngenres_per","891dc62a":"# To show which genre is the most common\ngenres_count = Counter([i for j in genres_per for i in j]).most_common()\nfig = plt.figure(figsize=(8, 5))\nsns.barplot([val[1] for val in genres_count],[val[0] for val in genres_count])\nplt.xlabel('Count')\nplt.title('Top 20 Genre Count')\nplt.show()","60e137bd":"# Show top five columns\nfor i, e in enumerate(train['production_companies'][:5]):\n    print(i, e)","8f6ee595":"# Unique value and count, visualization\ncompanies_num = train['production_companies'].apply(lambda x: len(x) if x != {} else 0).value_counts()\nprint(companies_num)\nfig = plt.figure(figsize=(8, 5))\nsns.barplot(companies_num, companies_num.index, orient=\"h\", order=companies_num.sort_values(ascending = False).index)\nplt.xlabel('Count')\nplt.title('Number of Production Companies of Movies')\nplt.show()","ef387e8a":"# Show top 20 production company (ranked by number of movies)\ncompanies_per = train['production_companies'].apply(lambda x: [i['name'] for i in x] if x != {} else [])\ncompanies_count = Counter([i for j in companies_per for i in j]).most_common(20)\nfig = plt.figure(figsize=(8, 5))\nsns.barplot([val[1] for val in companies_count],[val[0] for val in companies_count])\nplt.xlabel('Count')\nplt.title('Top 20 Production Company Count')\nplt.show()","4c3882a3":"# SHow top five columns\nfor i, e in enumerate(train['production_countries'][:5]):\n    print(i, e)","bd309852":"# count\ncountries_num = train['production_countries'].apply(lambda x: len(x) if x != {} else 0).value_counts()\ncountries_num","645ab4d4":"# Show which country produced most movies\ncountries_per = train['production_countries'].apply(lambda x: [i['name'] for i in x] if x != {} else [])\ncountries_count = Counter([i for j in countries_per for i in j]).most_common(20)\nfig = plt.figure(figsize=(8, 5))\nsns.barplot([val[1] for val in countries_count],[val[0] for val in countries_count])\nplt.xlabel('Count')\nplt.title('Top 20 Production Country Count')\nplt.show()","8228cfe3":"# Top 5 columns\nfor i, e in enumerate(train['spoken_languages'][:5]):\n    print(i, e)","c3de2098":"# Unique values and frequency\nlanguages_num = train['spoken_languages'].apply(lambda x: len(x) if x != {} else 0).value_counts()\nprint(languages_num)\nfig = plt.figure(figsize=(8, 5))\nsns.barplot(languages_num, languages_num.index, orient=\"h\", order=languages_num.sort_values(ascending = False).index)\nplt.xlabel('Count')\nplt.title('Number of Spoken Languages of Movies')\nplt.show()","06073788":"# Show languages which are used frequently\nlanguages_per = train['spoken_languages'].apply(lambda x: [i['name'] for i in x] if x != {} else [])\nlanguages_count = Counter([i for j in languages_per for i in j]).most_common(20)\nfig = plt.figure(figsize=(8, 5))\nsns.barplot([val[1] for val in languages_count],[val[0] for val in languages_count])\nplt.xlabel('Count')\nplt.title('Top 20 Spoken Language Count')\nplt.show()","1361fa02":"train['original_language'].value_counts()[:10]","07949424":"# Show some columns\nfor i, e in enumerate(train['Keywords'][:5]):\n    print(i, e)","ddf93aab":"# count and visualize\nkeywords_per = train['Keywords'].apply(lambda x: [i['name'] for i in x] if x != {} else [])\nkeywords_count = Counter([i for j in keywords_per for i in j]).most_common(20)\nfig = plt.figure(figsize=(8, 5))\nsns.barplot([val[1] for val in keywords_count],[val[0] for val in keywords_count])\nplt.xlabel('Count')\nplt.title('Top 20 Keywords Count')\nplt.show()","bb09d09f":"# Create a word cloud for keywords\nplt.figure(figsize = (10, 6))\ntext = ' '.join(['_'.join(i.split(' ')) for j in keywords_per for i in j])\nwordcloud = WordCloud(max_font_size=None, collocations=False, background_color=\"white\", width=1000, height=600).generate(text)\nplt.imshow(wordcloud)\nplt.title('Top keywords')\nplt.axis(\"off\")\nplt.show()","05164c3f":"train['cast'][1][1]","5d42a98d":"# Most popular actors\ncast_per = train['cast'].apply(lambda x: [i['name'] for i in x] if x != {} else [])\ncast_count = Counter([i for j in cast_per for i in j]).most_common(20)\nfig = plt.figure(figsize=(8, 5))\nsns.barplot([val[1] for val in cast_count],[val[0] for val in cast_count])\nplt.xlabel('Count')\nplt.title('Top 20 Actor Count')\nplt.show()","acc1a762":"train['crew'][0][0]","80897a42":"# Show crew who participated in lots of movies\ncrew_per = train['cast'].apply(lambda x: [i['name'] for i in x] if x != {} else [])\ncrew_count = Counter([i for j in crew_per for i in j]).most_common(20)\nfig = plt.figure(figsize=(8, 5))\nsns.barplot([val[1] for val in crew_count],[val[0] for val in crew_count])\nplt.xlabel('Count')\nplt.title('Top 20 Crew Count')\nplt.show()","9d9b7c37":"# Show unique value and count\ntrain['homepage'].isna().value_counts()","e48b66b0":"# Ranked by frequency\ntrain['homepage'].value_counts().sort_values(ascending=False)[:5]","e6d6d2d9":"# Show the distribution of revenue\ntrain['has_homepage'] = (1 - train.homepage.isna())\nplt.figure(figsize=(8, 5))\nsns.catplot(x='has_homepage', y='revenue', data=train)\nplt.xlabel('Does the movie have a homepage?')\nplt.ylabel('Revenue')\nplt.show()","5cd9f2d2":"train['status'].value_counts()","81fb4617":"# Show frequency\nlang_counts = train['original_language'].value_counts()\nplt.figure(figsize=(8, 5))\nsns.barplot(lang_counts[:10],lang_counts[:10].index)\nplt.title('Top 20 Original Language Count')\nplt.ylabel('Original Language')\nplt.xlabel('Revenue')\nplt.show()","0a5c7b67":"# Show top 15 languages\ntrain['original_language'].value_counts().head(15)","97f4d2f8":"# The percentage of English movies\n(train['original_language'] == 'en').mean()","27bbb83c":"# Show the difference of revenue between movies in different languages\ntop20_lang = train.loc[train['original_language'].isin(lang_counts[:20].index),:]\nplt.figure(figsize=(8, 5))\nsns.catplot(x='original_language', y='revenue', data=top20_lang)\nplt.title('Revenue of Top 20 Languages')\nplt.xlabel('Original Language')\nplt.ylabel('Revenue')\nplt.show()","f54a5d04":"# A word cloud for title\ntext = ' '.join(train['title'].apply(lambda x:x if x is not np.nan else ''))\nplt.figure(figsize = (10, 6))\nwordcloud = WordCloud(max_font_size=None, collocations=False, background_color=\"white\", width=1000, height=600).generate(text)\nplt.imshow(wordcloud)\nplt.title('Top Words in Titles')\nplt.axis(\"off\")\nplt.show()","6227ffc5":"train['is_title_different'] = 1 - (train['original_title'] == train['title'])\nsns.catplot(x=\"is_title_different\", y=\"revenue\", data=train)\nplt.xlabel('Does the movie have multiple titles?')\nplt.ylabel('Revenue')\nplt.show()","ed3632ef":"# A word cloud for overview\ntext = ' '.join(train['overview'].apply(lambda x:x if x is not np.nan else ''))\nplt.figure(figsize = (10, 6))\nwordcloud = WordCloud(max_font_size=None, collocations=False, background_color=\"white\", width=1000, height=600).generate(text)\nplt.imshow(wordcloud)\nplt.title('Top Words in Overview')\nplt.axis(\"off\")\nplt.show()","878a02e3":"# A word cloud for tagline\ntext = ' '.join(train['tagline'].apply(lambda x:x if x is not np.nan else ''))\nplt.figure(figsize = (10, 6))\nwordcloud = WordCloud(max_font_size=None, collocations=False, background_color=\"white\", width=1000, height=600).generate(text)\nplt.imshow(wordcloud)\nplt.title('Top Words in Tagline')\nplt.axis(\"off\")\nplt.show()","29d2d8b4":"# Does the tagline influence the revenue?\ntrain['has_tagline'] = 1 - train['tagline'].isna()\nsns.catplot(x=\"has_tagline\", y=\"revenue\", data=train)\nplt.xlabel('Does the movie have a tagline?')\nplt.ylabel('Revenue')\nplt.show()","80a19c6c":"# Fixes dates which are in 20xx (they are later than 2020, so all are wrong values)\ndef fix_date(x):\n    year = x.split('\/')[2]\n    if int(year) <= 19:\n        return x[:-2] + '20' + year\n    else:\n        return x[:-2] + '19' + year\n\ntrain['release_date'] = train['release_date'].apply(lambda x: fix_date(x))\ntest['release_date'] = test['release_date'].apply(lambda x: fix_date(x))\n\n# Extract date features\ndef date_features(df):\n    df['release_date'] = pd.to_datetime(df['release_date'])\n    df['release_year'] = df['release_date'].dt.year\n    df['release_month'] = df['release_date'].dt.month\n    df['release_day'] = df['release_date'].dt.day\n    #df['release_quarter'] = df['release_date'].dt.quarter\n    df['release_dayofweek'] = df['release_date'].dt.dayofweek\n    df.drop(columns=['release_date'], inplace=True)\n    return df\n\ntrain = date_features(train)\ntest = date_features(test)","8626d6a4":"# Number of movies by year\nplt.figure(figsize=(15,6))\nsns.countplot(train['release_year'].sort_values())\nplt.title(\"Movie Release Count by Year\")\nplt.xlabel(\"Release Year\")\nplt.xticks(fontsize=8, rotation=90)\nplt.show()","4058d979":"# Number of movies by month\nplt.figure(figsize=(15,6))\nsns.countplot(train['release_month'].sort_values())\nplt.title(\"Movie Release Count by Month\")\nplt.xlabel(\"Release Month\")\nplt.show()","45441a3a":"# Number of movies by day of month\nplt.figure(figsize=(15,6))\nsns.countplot(train['release_day'].sort_values())\nplt.title(\"Movie Release Count by Day of Month\")\nplt.xlabel(\"Release Day of Month\")\nplt.show()","50fd162f":"# Number of movies by day of week\nplt.figure(figsize=(15,6))\nsns.countplot(train['release_dayofweek'].sort_values())\nplt.title(\"Movie Release Count by Day of Week\")\nplt.xlabel(\"Release Day of Week\")\nplt.gca().set_xticklabels([\"Monday\",\"Tuesday\",\"Wednesday\",\"Thursday\",\"Friday\",\"Saturday\",\"Sunday\"]) # add labels\nplt.show()","8cffc6ae":"# Distribution of target variable\ntrain['log_revenue'] = np.log1p(train['revenue'])\nfig, ax = plt.subplots(1, 2, figsize=(12, 4))\nplt.subplot(1, 2, 1)\nsns.distplot(train['revenue'], kde=False)\nplt.title('Distribution of Revenue')\nplt.xlabel('Revenue')\nplt.subplot(1, 2, 2)\nsns.distplot(train['log_revenue'], kde=False)\nplt.title('Distribution of Log of Revenue')\nplt.xlabel('Log of Revenue')\nplt.show()","15ce7a07":"# Mean revenue by year\nMeanRevenueByYear = train.groupby('release_year')['revenue'].agg('mean')\nMeanRevenueByYear.plot(figsize=(15,6))\nplt.xlabel('Year')\nplt.ylabel('Revenue')\nplt.title('Mean Revenue By Year')\nplt.show()","9ea19a66":"# Mean revenue by month\nMeanRevenueByMonth = train.groupby('release_month')['revenue'].agg('mean')\nMeanRevenueByMonth.plot(figsize=(15,6), kind='bar')\nplt.xlabel('Month')\nplt.ylabel('Revenue')\nplt.title('Mean Revenue By Month')\nplt.xticks(rotation=360)\nplt.show()","3718606e":"# Mean revenue by day of month\nMeanRevenueByDayOfMonth = train.groupby('release_day')['revenue'].agg('mean')\nMeanRevenueByDayOfMonth.plot(figsize=(15,6), kind='bar')\nplt.xlabel('Day Of Month')\nplt.ylabel('Revenue')\nplt.title('Mean Revenue By Day Of Month')\nplt.xticks(rotation=360)\nplt.show()","119ac70d":"# Mean revenue by day of week\nMeanRevenueByDayOfWeek = train.groupby('release_dayofweek')['revenue'].agg('mean')\nMeanRevenueByDayOfWeek.plot(figsize=(15,6), kind='bar')\nplt.xlabel('Day Of Week')\nplt.ylabel('Revenue')\nplt.title('Mean Revenue By Day Of Week')\nplt.gca().set_xticklabels([\"Monday\",\"Tuesday\",\"Wednesday\",\"Thursday\",\"Friday\",\"Saturday\",\"Sunday\"])\nplt.xticks(rotation=360)\nplt.show()","2a13d856":"# Distribution of budget\ntrain['log_budget'] = np.log1p(train['budget'])\nfig, ax = plt.subplots(1, 2, figsize=(12, 4))\nplt.subplot(1, 2, 1)\nsns.distplot(train['budget'], kde=False)\nplt.title('Distribution of Budget')\nplt.xlabel('Budget')\nplt.subplot(1, 2, 2)\nsns.distplot(train['log_budget'], bins=30, kde=False)\nplt.title('Distribution of Log of Budget')\nplt.xlabel('Log of Budget')\nplt.show()","00b0e811":"# Mean budget by year\nMeanBudgetByYear = train.groupby('release_year')['budget'].agg('mean')\nMeanBudgetByYear.plot(figsize=(15,6))\nplt.xlabel('Year')\nplt.ylabel('Budget')\nplt.title('Mean Budget By Year')\nplt.show()","5c94dfac":"# Relationship between budget and revenue\nfig, ax = plt.subplots(1, 2, figsize=(12, 4))\nplt.subplot(1, 2, 1)\nplt.scatter(x='budget', y='revenue', data=train)\nplt.xlabel('Budget')\nplt.ylabel('Revenue')\nplt.subplot(1, 2, 2)\nplt.scatter(x='log_budget', y='log_revenue', data=train)\nplt.xlabel('Log of Budget')\nplt.ylabel('Log of Revenue')\nplt.show()","2e15e2da":"# Distribution of popularity\ntrain['log_popularity'] = np.log1p(train['popularity'])\nfig, ax = plt.subplots(1, 2, figsize=(12, 4))\nplt.subplot(1, 2, 1)\nsns.distplot(train['popularity'], kde=False)\nplt.title('Distribution of Popularity')\nplt.xlabel('Popularity')\nplt.subplot(1, 2, 2)\nsns.distplot(train['log_popularity'], bins=30, kde=False)\nplt.title('Distribution of Log of Popularity')\nplt.xlabel('Log of Popularity')\nplt.show()","aef902b4":"# Relationship between popularity and revenue\nfig, ax = plt.subplots(1, 2, figsize=(12, 4))\nplt.subplot(1, 2, 1)\nplt.scatter(x='popularity', y='revenue', data=train)\nplt.xlabel('Popularity')\nplt.ylabel('Revenue')\nplt.subplot(1, 2, 2)\nplt.scatter(x='popularity', y='log_revenue', data=train)\nplt.xlabel('Popularity')\nplt.ylabel('Log of Revenue')\nplt.show()","0167ebbc":"# Distribution of runtime\nsns.distplot(train['runtime'], kde=False)\nplt.title('Distribution of Runtime')\nplt.xlabel('Runtime')\nplt.show()","f721d054":"# Mean runtime by year\nMeanRuntimeByYear = train.groupby('release_year')['runtime'].agg('mean')\nMeanRuntimeByYear.plot(figsize=(15,6))\nplt.xlabel('Year')\nplt.ylabel('Runtime')\nplt.title('Mean Runtime By Year')\nplt.show()","24cae9f4":"# Relationship between runtime and revenue\nfig, ax = plt.subplots(1, 2, figsize=(12, 4))\nplt.subplot(1, 2, 1)\nplt.scatter(x='runtime', y='revenue', data=train)\nplt.xlabel('Runtime')\nplt.ylabel('Revenue')\nplt.subplot(1, 2, 2)\nplt.scatter(x='runtime', y='log_revenue', data=train)\nplt.xlabel('Runtime')\nplt.ylabel('Log of Revenue')\nplt.show()","e896740b":"num_vars = ['revenue','budget','popularity','runtime','release_year','release_month','release_day','release_dayofweek']\nmask = np.zeros_like(train[num_vars].corr(), dtype=np.bool)\nmask[np.triu_indices_from(mask)] = True\nf,ax = plt.subplots(figsize=(10, 10))\ncmap = sns.diverging_palette(220, 10, as_cmap=True)\nsns.heatmap(train[num_vars].corr(), annot=True, mask=mask, cmap=cmap, vmax=.3, center=0,\n            square=True, linewidths=.5, cbar_kws={\"shrink\": .5})\nplt.show()","b1f702b5":"def prepare_data(df):\n    \n    # belongs to collection\n    \n    df['has_collection'] = df['belongs_to_collection'].apply(lambda x: len(x) if x != {} else 0)\n    \n    # homepage\n    \n    df['has_homepage'] = 1 - df['homepage'].isna()\n    \n    # status\n    \n    df['is_released'] = (df['status'] == 'Released')*1\n       \n    # original title\n    \n    df['original_title_letter_count'] = df['original_title'].str.len() \n    df['original_title_word_count'] = df['original_title'].str.split().str.len()\n    \n    # title\n    \n    df['title_letter_count'] = df['title'].str.len() \n    df['title_word_count'] = df['title'].str.split().str.len()\n    \n    # overview\n    \n    df['overview_letter_count'] = df['overview'].str.len() \n    df['overview_word_count'] = df['overview'].str.split().str.len()\n    \n    # tagline\n    \n    df['has_tagline'] = 1 - df['tagline'].isna()\n    df['tagline_letter_count'] = df['tagline'].str.len()\n    df['tagline_word_count'] = df['tagline'].str.split().str.len()\n    \n    # gender of cast\n    \n    df['genders_0_cast'] = df['cast'].apply(lambda x: sum([1 for i in x if i['gender'] == 0]))\n    df['genders_1_cast'] = df['cast'].apply(lambda x: sum([1 for i in x if i['gender'] == 1]))\n    df['genders_2_cast'] = df['cast'].apply(lambda x: sum([1 for i in x if i['gender'] == 2]))\n    \n    # gender of crew\n    \n    df['genders_0_crew'] = df['crew'].apply(lambda x: sum([1 for i in x if i['gender'] == 0]))\n    df['genders_1_crew'] = df['crew'].apply(lambda x: sum([1 for i in x if i['gender'] == 1]))\n    df['genders_2_crew'] = df['crew'].apply(lambda x: sum([1 for i in x if i['gender'] == 2]))\n    \n    # log\n    \n    df['log_budget'] = np.log1p(df['budget'])\n    df['log_popularity'] = np.log1p(df['popularity'])\n    \n    # create new features about budget\n        \n    df['ratio_budget_runtime'] = (df['log_budget'] \/ df['runtime']).replace([np.inf,-np.inf,np.nan],0)\n    df['ratio_budget_popularity'] = df['log_budget'] \/ df['log_popularity']\n    df['ratio_budget_year'] = df['log_budget'] \/ df['release_year'] #\n    df['budget_to_mean_year'] = df['log_budget'] \/ df.groupby('release_year')['log_budget'].transform('mean')\n    \n    # create new features about popularity\n    \n    df['ratio_popularity_year'] = df['log_popularity'] \/ df['release_year']\n    df['popularity_to_mean_year'] = df['log_popularity'] \/ df.groupby('release_year')['log_popularity'].transform('mean')\n    \n    # create new features about runtime\n    \n    df['ratio_runtime_year'] = df['runtime'] \/ df['release_year']\n    df['runtime_to_mean_year'] = df['runtime'] \/ df.groupby('release_year')['runtime'].transform('mean')\n                 \n    # fill in null values\n    \n    df.fillna(value=0.0, inplace = True)\n    \n    return df","d07043b4":"train = prepare_data(train)\ntest = prepare_data(test)","955ca1ed":"# Process dist columns, create dummy variables for values with high frequency\n\ndef dist_processing(train, test, col, key): # cast character, crew job\/department?\n        \n    value_list = train[col].apply(lambda x: [i[key] for i in x] if x != {} else [])\n    value_count = Counter([i for j in value_list for i in j]).most_common()\n    top_list = [m[0] for m in value_count if m[1] > 10] # regard freq>10 as high frequency\n        \n    train['num_' + col] = train[col].apply(lambda x: len(x) if x != {} else 0)\n    train['all_' + col] = train[col].apply(lambda x: ' '.join(sorted([i[key] for i in x])) if x != {} else '')\n    test['num_' + col] = test[col].apply(lambda x: len(x) if x != {} else 0)\n    test['all_' + col] = test[col].apply(lambda x: ' '.join(sorted([i[key] for i in x])) if x != {} else '')\n    \n    for p in top_list:\n        train[col + '_' + p] = train['all_' + col].apply(lambda x: 1 if p in x else 0)\n        test[col + '_' + p] = test['all_' + col].apply(lambda x: 1 if p in x else 0)\n    \n    train = train.drop([col, 'all_' + col], axis=1)\n    test = test.drop([col, 'all_' + col], axis=1)\n    \n    return train, test\n    \ntext_cols = ['genres', 'production_companies', 'production_countries', 'Keywords', 'cast', 'crew']\n    \nfor col in text_cols:\n    train, test = dist_processing(train, test, col, 'name')\n    \ntrain, test = dist_processing(train, test, 'spoken_languages', 'iso_639_1')\n    \n# original language\n    \nvalue_count = Counter(train['original_language']).most_common()\ntop_list = [m[0] for m in value_count if m[1] > 10]\n\nfor p in top_list: \n    train['original_language_' + p] = train['original_language'].apply(lambda x: 1 if p in x else 0)\n    test['original_language_' + p] = test['original_language'].apply(lambda x: 1 if p in x else 0)","1be2fa4a":"# Delete useless columns\ntrain = train.drop(['id', 'belongs_to_collection', 'homepage', 'status', 'original_language', 'original_title', 'title', 'overview',\n                    'tagline', 'imdb_id', 'poster_path', 'budget', 'popularity', 'revenue', 'is_title_different'], axis=1)\ntest = test.drop(['id', 'belongs_to_collection', 'homepage', 'status', 'original_language', 'original_title', 'title', 'overview',\n                  'tagline', 'imdb_id', 'poster_path', 'budget', 'popularity'], axis=1)","f8182d08":"# Remove non-ASCII characters in feature names (for lightGBM)\nnewnames = []\nfor col in train.columns.values:\n    encoded_string = col.encode(\"ascii\", \"ignore\")\n    decode_string = encoded_string.decode()\n    newnames.append(decode_string)\ntrain.columns = newnames\n\nnewnames = []\nfor col in test.columns.values:\n    encoded_string = col.encode(\"ascii\", \"ignore\")\n    decode_string = encoded_string.decode()\n    newnames.append(decode_string)\ntest.columns = newnames","f30db709":"# Check the number of columns in two data sets\nprint(train.shape)\nprint(test.shape)","a99759d4":"X = train.drop(['log_revenue'], axis=1)\ny = train['log_revenue']\n\n# Make the order of features consistent (for XGBoost)\nX_col = list(X.columns)\ntest = test.loc[:,X_col]\n\nX_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.1, random_state=22)","8994d86f":"# Use 10-fold cross-validation\nn_fold = 10\nfolds = list(KFold(n_splits=n_fold, shuffle=True, random_state=22).split(X))","a77a9dba":"# Train a LGB model\nparams = {'objective': 'regression',\n          'num_leaves': 30,\n          'min_data_in_leaf': 20,\n          'max_depth': 5, #4\n          'learning_rate': 0.005,\n          'boosting': \"gbdt\",\n          'feature_fraction': 0.9, #0.7\n          'bagging_freq': 1,\n          'bagging_fraction': 0.9,\n          'bagging_seed': 22,\n          'metric': 'rmse',\n          'lambda_l1': 0.2,\n          'verbosity': -1}\nmodel = lgb.LGBMRegressor(**params, n_estimators = 100000, nthread = 4, n_jobs = -1)\nmodel.fit(X_train, y_train, \n          eval_set=[(X_train, y_train), (X_valid, y_valid)], \n          eval_metric='rmse',\n          verbose=False, \n          early_stopping_rounds=200)","162bdc09":"# Explain the model\nexplainer = shap.TreeExplainer(model, X_train)\nshap_values = explainer.shap_values(X_train)","23566d5d":"# Create a summary plot\nshap.summary_plot(shap_values, X_train)","07747e3a":"# Create dependence plot for important features\ntop_cols = X_train.columns[np.argsort(shap_values.std(0))[::-1]][:10]\nfor col in top_cols:\n    shap.dependence_plot(col, shap_values, X_train)","9f58c68c":"def new_interactions(df):\n    \n    # Create new interacrions for important features\n    \n    df['ratio_budget_year2'] = df['log_budget'].fillna(0) \/ (df['release_year']*df['release_year'])\n    df['ratio_year_budget'] = df['release_year'] \/ df['log_budget']\n    df['popularity_to_budget_to_mean_year'] = df['popularity_to_mean_year'] \/ df['budget_to_mean_year']\n    df['genders_2_crew_to_budget_to_mean_year'] = df['genders_2_crew'] \/ df['budget_to_mean_year']\n    df['num_crew_to_ratio_popularity_year'] = df['num_crew'] \/ df['ratio_popularity_year']\n    df['popularity_runtime_to_budget'] = df['log_popularity'] \/ df['ratio_budget_runtime']\n    \n    df['mean_budget_to_year'] = df['budget_to_mean_year'] \/ df['release_year']\n    df['budget_to_runtime_to_year'] = df['ratio_budget_runtime'] \/ df['release_year']\n    df['ratio_year_popularity'] = df['release_year'] \/ df['log_popularity']\n    \n    df.fillna(value=0.0, inplace = True)\n    \n    return df\n\nX = new_interactions(X)\ntest = new_interactions(test)\nX_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.1, random_state=22)","6161a7ea":"# Grid Search for LGB\n\"\"\"\"\"\"\"\"\"\"\"\nlg = lgb.LGBMRegressor()\nparam_dist = {\n    'learning_rate': [0.01,0.005],\n    'boosting_type': ['gbdt'],\n    'max_depth': [5,7,9],\n    'num_leaves': [25,30,35,40],\n    'min_data_in_leaf': [10,20,25],\n    'feature_fraction': [0.7,0.8,0.9],\n    'bagging_freq': [1],\n    'bagging_fraction': [0.7,0.8,0.9],\n    'lambda_l1': [0.2],\n    'objective': ['regression'],\n    'random_state': [22]\n}\nstart = time.time()\nlg_search = GridSearchCV(lg, param_grid=param_dist, cv = 3, scoring='neg_mean_squared_error', verbose=5, n_jobs=-1)\nlg_search.fit(X_train,y_train)\nend = time.time()\nprint('Time elapsed: {0:.2f} m'.format((end-start)\/60))\nlg_search.best_params_\n\"\"\"\"\"\"\"\"\"\"\"","414eb196":"# Grid Search for XGB\n\"\"\"\"\"\"\"\"\"\"\"\nxg = xgb.XGBRegressor()\nparam_dist = {\n    'objective': ['reg:squarederror'],\n    'eta': [0.01],\n    'gamma': [0,1,1.45], \n    'max_depth': [5,6,7],\n    'min_child_weight': [1,3],\n    'subsample': [0.6,0.7,0.8],\n    'colsample_bytree': [0.6,0.7,0.8], \n    'colsample_bylevel': [0.5,1], \n    'seed': [22]\n}\nstart = time.time()\nxg_search = GridSearchCV(xg, param_grid=param_dist, cv = 3, scoring='neg_mean_squared_error', verbose=10, n_jobs=-1)\nxg_search.fit(X_train, y_train)\nend = time.time()\nprint('Time elapsed: {0:.2f} m'.format((end-start)\/60))\nxg_search.best_params_\n\"\"\"\"\"\"\"\"\"\"\"","a4506f4e":"# Grid Search for CAT\n\"\"\"\"\"\"\"\"\"\"\"\nct = CatBoostRegressor()\nparams = {\n    'learning_rate': [0.002, 0.004, 0.01],\n    'depth': [5,6,7],\n    'l2_leaf_reg': [1,3,4,9],#default=3\n    'colsample_bylevel': [0.7,0.8],\n    'bagging_temperature': [0.2],\n    'random_seed': [22]\n}\nstart = time.time()\nct_search = GridSearchCV(ct, params, scoring='neg_mean_squared_error', cv = 3)\nct_search.fit(X_train, y_train)\nend = time.time()\nprint('Time elapsed: {0:.2f} m'.format((end-start)\/60))\nct_search.best_params_\n\"\"\"\"\"\"\"\"\"\"\"","ea9de7e1":"def models(X_train, X_valid, y_train, y_valid, test, model_type, params, plot_feature_importance=False):\n\n    pred = np.zeros(test.shape[0])\n    val_pred = np.zeros(X_valid.shape[0])\n    scores = []\n    feature_importance = pd.DataFrame()\n\n    for fold_n, (train_index, valid_index) in enumerate(folds):\n        \n        print('Fold', fold_n, 'started at', time.ctime())\n        X_train = X.loc[train_index,:]\n        y_train = y[train_index]\n        X_valid = X.loc[valid_index,:]\n        y_valid = y[valid_index] \n    \n        # lightGBM model\n        \n        if model_type == 'lgb':\n            \n            model = lgb.LGBMRegressor(**params, n_estimators = 100000, nthread = 4, n_jobs = -1)\n            model.fit(X_train, y_train, \n                      eval_set=[(X_train, y_train), (X_valid, y_valid)], \n                      eval_metric='rmse',\n                      verbose=False, \n                      early_stopping_rounds=200)\n            \n            y_pred_valid = model.predict(X_valid, num_iteration = model.best_iteration_)\n            y_pred = model.predict(test, num_iteration = model.best_iteration_)\n            \n              \n        # XGBoost model \n        \n        if model_type == 'xgb':\n            \n            model = xgb.XGBRegressor(**params, n_estimators = 10000)\n            model.fit(X_train, y_train,\n                    eval_set=[(X_train, y_train), (X_valid, y_valid)],\n                    eval_metric='rmse',\n                    verbose=False,\n                    early_stopping_rounds=200)\n\n            y_pred_valid = model.predict(X_valid, ntree_limit = model.best_ntree_limit)\n            y_pred = model.predict(test, ntree_limit = model.best_ntree_limit)\n            \n        # CatBoost model  \n        \n        if model_type == 'cat':\n            \n            model = CatBoostRegressor(**params, iterations=20000, eval_metric = 'RMSE')\n            model.fit(X_train, y_train, \n                      eval_set=(X_valid, y_valid), \n                      use_best_model=True, \n                      verbose=False)\n\n            y_pred_valid = model.predict(X_valid)\n            y_pred = model.predict(test)\n        \n        # Record scores\n        \n        scores.append(mean_squared_error(y_valid, y_pred_valid) ** 0.5) # RMSE for valid\n        pred += y_pred\n        \n        # Feature importance\n        \n        fold_importance = pd.DataFrame()\n        fold_importance['feature'] = X.columns\n        fold_importance['importance'] = model.feature_importances_\n        feature_importance = pd.concat([feature_importance, fold_importance], axis=0)\n            \n    pred \/= n_fold\n    print('Mean RMSE: {0:.5f}, std: {1:.5f}.'.format(np.mean(scores), np.std(scores)))\n    \n    # Plot the importance\/weight of features\n           \n    feature_importance['importance'] \/= n_fold\n    if plot_feature_importance:\n        cols = feature_importance[['feature', 'importance']].groupby('feature').mean().sort_values(\n            by='importance', ascending=False)[:50].index\n        best_features = feature_importance.loc[feature_importance.feature.isin(cols)]\n\n        plt.figure(figsize=(16, 12))\n        sns.barplot(x='importance', y='feature', data=best_features.sort_values(by='importance', ascending=False))\n        plt.title('Important Features (avg over folds)')\n    \n    return pred","f554f73b":"# Best parameters by grid search\nlgb_params = {'objective': 'regression',\n              'num_leaves': 40,\n              'min_data_in_leaf': 10,\n              'max_depth': 7,\n              'learning_rate': 0.01,\n              'boosting': \"gbdt\",\n              'feature_fraction': 0.9, \n              'bagging_freq': 1, \n              'bagging_fraction': 0.7,\n              'bagging_seed': 22,\n              'metric': 'rmse',\n              'lambda_l1': 0.2,\n              'verbosity': -1}\nstart = time.time()\nlgb_pred = models(X_train, X_valid, y_train, y_valid, test, 'lgb', lgb_params, True)\nend = time.time()\nprint('Time elapsed: {0:.2f} m'.format((end-start)\/60))","e5018a88":"# Best parameters by grid search\nxgb_params = {'objective': 'reg:squarederror',\n              'eta': 0.01,\n              'max_depth': 5,\n              'min_child_weight': 3,\n              'subsample': 0.8,\n              'colsample_bytree': 0.7, #0.8\n              'colsample_bylevel': 0.5, \n              #'gamma': 0, default\n              'eval_metric': 'rmse',\n              'seed': 22}\nstart = time.time()\nxgb_pred = models(X_train, X_valid, y_train, y_valid, test, 'xgb', xgb_params, True)\nend = time.time()\nprint('Time elapsed: {0:.2f} m'.format((end-start)\/60))","2e09bf20":"# Best parameters by grid search\ncat_params = {'learning_rate': 0.01,\n              'depth': 7,\n              'colsample_bylevel': 0.7,\n              'bagging_temperature': 0.2,\n              'l2_leaf_reg': 1, #add new\n              'random_seed': 22,\n              'allow_writing_files': False,\n              'early_stopping_rounds': 200\n             }\nstart = time.time()\ncat_pred = models(X_train, X_valid, y_train, y_valid, test, 'cat', cat_params, True)\nend = time.time()\nprint('Time elapsed: {0:.2f} m'.format((end-start)\/60))","524f49c2":"# Blending\ntest_pred = lgb_pred*0.4 + xgb_pred*0.2 + cat_pred*0.4","235dfb51":"# Submit\nsub = pd.read_csv('..\/input\/tmdb-box-office-prediction\/sample_submission.csv')\ndf_sub = pd.DataFrame()\ndf_sub['id'] = sub['id']\ndf_sub['revenue'] = np.expm1(test_pred)\ndf_sub.to_csv(\"submission.csv\", index=False)","9cacb507":"### Belongs to Collection","f249e85f":"### Training and Prediction","4031eeae":"### Title","0bab78cd":"**Note: The next two parts are feature analysis and parameter search before formal modeling.**","8a09f997":"### Find Optimal Parameter\nI referenced a few notebooks and used grid search to find the optimal parameter set.      \n***Important: Don't need to run in final modeling and predictions.***","349b0c98":"#### Traning with CatBoost","5c6af1f3":"Most of movies are from US and UK.","6300e90f":"Now let's continue to explore other categorical variables and their relationships with revenue.","e9aefa82":"There are some JSON format columns. Let's convert them to the dictionary format and analyse them at first.","9e0ab6e0":"## Feature Engineering","b4cae402":"Before studying the numeric variables, let's fix and convert the release date column for time analysis.","44b4cf5a":"### Status","68c60848":"### Original Language","cce3dcb7":"### Spoken Language","f97e797a":"Let's also look at the original language column.","e9b1bdb1":"Obvious linear relationship between them.","7240fe60":"## Data Loading and EDA","045f73ad":"Let's see some basic information first.","ed008a26":"There is no obvious rule for the influence of language on revenue.","8c42dfa7":"### Budget","d79ce397":"Many important features still have obvious linear or nonlinear relationships.      \nCreate more interactions.","1a94cc98":"We can build dummy variables for it later.","9653d926":"## Modeling and Prediction","fcac11ce":"Using log of revenue is better.","eac3be4c":"### Genres","b10b6a3c":"There are strong relevances between budget, popularity, runtime and realse year, so create some interactions later.","30eba97f":"### Relevance of Important Independent Features\nBefore creating certain interactions in feature engineering, run this part to check the correlation.    \n***Important: Don't need to run in final modeling and predictions.***","b9844595":"Like the revenue, the mean budget also increases year by year.","dbefca46":"This function returns prediction results and feature importance plot(optional).","d0c767fa":"### Blending and Submitting","4208444b":"### Keywords","a001fe65":"With the development and maturity of the film industry, the runtime tends to a certain range.    \nThis change has been especially obvious since the 1980s.","a0d2b17b":"### Production Companies","55e9f7ff":"### Release Date","7e79d878":"Different movies have different homepage, so we don't need to create dummy variables for each webpage.        \nA binary variable is enough.","de861d59":"#### Budget vs Revenue","0f5f7807":"### Overview","c2b689bb":"How to decide the percentage?      \nModels with smaller RMSE have a larger percentage.","0f904cc8":"### Tagline","10264c85":"### Production Countries","60d220e7":"#### Training with XGBoost","2adcd7f3":"Show relationships between numeric variables.","6560400d":"#### Revenue By Year","77e32049":"Only about 20% of the rows have information about collections, and the rest are empty.\nBoth poster_path and backdrop_path are image information, so only collection name can be used for modeling.","2d4c7ee8":"### Revenue","3f7f3127":"# TMDB Box Office Prediction","c7da6e25":"### Crew","0f61c982":"#### Training with LightGBM","654b2da6":"Only four movies havn't been released, so I think this column is not very useful for modeling.","a72fe38c":"### Runtime","a0f0392e":"### Cast","1a4a48ba":"### Build Models and Functions","3d420a63":"### Homepage","c2f2652f":"The popularity doesn't have obvious relationship with the revenue.","1cfe1263":"The runtime doesn't have obvious relationship with the revenue.","226000a4":"Some movies only have one type of genre, while others have more than one.","bf86e146":"### Popularity"}}