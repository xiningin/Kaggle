{"cell_type":{"c1c24735":"code","98737c87":"code","f8c10ecf":"code","a06dd8fb":"code","5452dc33":"code","5f74b673":"code","b00a4471":"code","030b18e2":"code","0d9d2618":"code","50a11da9":"code","e925eb15":"code","f3c22de6":"code","b3822eef":"code","bc9ed42f":"code","2e2e9ef6":"code","14f5a13b":"code","de769bac":"code","1bda35b0":"code","46b7ceae":"code","70b6d72b":"code","9c8bb59f":"code","a60d73b1":"code","ed76aab3":"code","b1e4d37b":"code","e4452dc4":"code","52fbf3d7":"code","d2c4aa42":"code","4e0b047b":"code","d488500d":"code","ec9cf0eb":"code","6a6e2296":"code","4ae6dd4b":"code","fd771f48":"code","ff02a473":"code","582dba58":"code","71bef770":"code","8475fe26":"code","a07234c5":"code","8c3e77d7":"code","3d1b2d47":"code","74d26930":"code","8dcd3b5b":"markdown","e83e65d3":"markdown","4d2b1272":"markdown","77584a3e":"markdown","628d5882":"markdown","b6a236fb":"markdown","11646748":"markdown","1c1cd94a":"markdown","f56b8a1d":"markdown"},"source":{"c1c24735":"#Pretty Display of Variables\nfrom IPython.core.interactiveshell import InteractiveShell\nInteractiveShell.ast_node_interactivity = \"all\"","98737c87":"# import required packages\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n%matplotlib inline\n\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\nfrom sklearn.model_selection import train_test_split,GridSearchCV\nfrom sklearn.metrics import accuracy_score, f1_score, roc_auc_score,confusion_matrix,roc_curve,auc\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.preprocessing import StandardScaler,MinMaxScaler,PolynomialFeatures,LabelEncoder,OneHotEncoder\nfrom sklearn.decomposition import PCA\nfrom sklearn.neural_network import MLPClassifier\n\n\nimport lightgbm as lgb\nimport xgboost as xgb","f8c10ecf":"\n# load data\n#data = pd.read_csv(\"dell_data.csv\")\ndata = pd.read_csv(\"..\/input\/inventory\/inventory.csv\")\n\n# View first 5 rows\ndata.head()\n\n# View last 5 rows\ndata.tail()\n\n# Shape of the data\ndata.shape","a06dd8fb":"# dtypes of the data\ndata.dtypes","5452dc33":"# Finding null values\ndata.isnull().sum()","5f74b673":"# columns of the data\ndata.columns","b00a4471":"# Converting date columns to pandas datetime\ndata['DMND_WEEK_STRT_DATE'] = pd.to_datetime(data['DMND_WEEK_STRT_DATE'])\ndata['VRSN_WEEK_STRT_DATE'] = pd.to_datetime(data['VRSN_WEEK_STRT_DATE'])","030b18e2":"# Assaign difference of the DMND_WEEK_STRT_DATE & VRSN_WEEK_STRT_DATE to DMND_VRSN_Diff\ndata['DMND_VRSN_Diff'] = (data['DMND_WEEK_STRT_DATE'] - data['VRSN_WEEK_STRT_DATE']).dt.days.astype(int)","0d9d2618":"# Converting DW_PKG_UPD_DTS to min(int)\ndata[\"UPD_TIME\"] = data.DW_PKG_UPD_DTS.str.slice(0, 2).astype(int)\n\n# view first rows\ndata[\"UPD_TIME\"].head()","50a11da9":"# Dropping processed columns\ndata = data.drop(columns=[\"DMND_WEEK_STRT_DATE\",\"VRSN_WEEK_STRT_DATE\", \"DW_PKG_UPD_DTS\"], axis=1)\n\n# Shape of the data\ndata.shape","e925eb15":"#The number of unique values??\nfor col in data.columns[0:]:\n    print(col, \"------\", data[col].nunique())","f3c22de6":"# Descriptive Summary of the data\n\ndef description(data):\n    summary = pd.DataFrame(data.dtypes,columns=['dtypes'])\n    summary = summary.reset_index()\n    summary['Name'] = summary['index']\n    summary = summary[['Name','dtypes']]\n    summary['Missing'] = data.isnull().sum().values    \n    summary['Uniques'] = data.nunique().values\n    summary['First Value'] = data.iloc[0].values\n    summary['Second Value'] = data.iloc[1].values\n    summary['Third Value'] = data.iloc[2].values\n    return summary\n\n\ndescription(data)","b3822eef":"# object columns\nstr_cols= data.loc[:, data.dtypes=='object'].columns.tolist()\n\nstr_cols","bc9ed42f":"print(f'Shape before dummy transformation: {data.shape}')\n\ndummies = pd.get_dummies(data, columns=str_cols,\\\n                          prefix=str_cols, drop_first=True)\nprint(f'Shape after dummy transformation: {dummies.shape}')","2e2e9ef6":"\nfrom sklearn.model_selection import train_test_split\n\n# feature columns and target\nX = data.drop(\"MRP_FCST_QTY\", axis=1)\ny = data.MRP_FCST_QTY\n\n# Break off test and train set from data\nX_train, X_test, y_train, y_test = train_test_split(X, y,\n                                                      test_size=0.3,\n                                                      random_state=42)","14f5a13b":"from sklearn.linear_model import LinearRegression\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.metrics import mean_absolute_error\n\n# RMSLE\nfrom sklearn.metrics import mean_squared_log_error\n\n\n# RandomForestRegressor\ndef score_dataset(X_train, X_test, y_train, y_test):\n    model = RandomForestRegressor(n_estimators=1000, random_state=42)\n    model.fit(X_train, y_train)\n    preds = model.predict(X_test)\n    return np.sqrt(mean_squared_log_error(y_test, preds))\n\n\n#RMSLE = np.sqrt(mean_squared_log_error( y_test, predictions ))","de769bac":"# Drop categorical columns in training and testing data\ndrop_X_train = X_train.select_dtypes(exclude=['object'])\ndrop_X_test = X_test.select_dtypes(exclude=['object'])","1bda35b0":"# print(\"LR RMSLE from Approach 1 (Dropping categorical variables):\")\n# print(score_dataset_lr(drop_X_train, drop_X_test, y_train, y_test))\n\nprint(\"RMSLE from Approach 1 (Dropping categorical variables):\")\nprint(score_dataset(drop_X_train, drop_X_test, y_train, y_test))","46b7ceae":"# All categorical columns\nobject_cols = [col for col in X_train.columns if X_train[col].dtype == \"object\"]\n\n# Columns that can be safely label encoded\ngood_label_cols = [col for col in object_cols if \n                   set(X_train[col]) == set(X_test[col])]\n        \n# Problematic columns that will be dropped from the dataset\nbad_label_cols = list(set(object_cols)-set(good_label_cols))\n        \nprint('Categorical columns that will be label encoded:', good_label_cols)\nprint('\\nCategorical columns that will be dropped from the dataset:', bad_label_cols)","70b6d72b":"from sklearn.preprocessing import LabelEncoder\n\n# Drop categorical columns that will not be encoded\nlabel_X_train = X_train.drop(bad_label_cols, axis=1)\nlabel_X_test = X_test.drop(bad_label_cols, axis=1)\n\n# Apply label encoder\nlabel_encoder = LabelEncoder()\nfor col in set(good_label_cols):\n    label_X_train[col] = label_encoder.fit_transform(X_train[col])\n    label_X_test[col] = label_encoder.transform(X_test[col])","9c8bb59f":"# Train & Score Model for Label Encoding\nprint(\"RMSLE from Approach 2 (Label Encoding):\") \nprint(score_dataset(label_X_train, label_X_test, y_train, y_test))","a60d73b1":"# Investigating cardinality\n\n# Get number of unique entries in each column with categorical data\nobject_nunique = list(map(lambda col: X_train[col].nunique(), object_cols))\nd = dict(zip(object_cols, object_nunique))\n\n# Print number of unique entries by column, in ascending order\nsorted(d.items(), key=lambda x: x[1])","ed76aab3":"# Columns that will be one-hot encoded\nlow_cardinality_cols = [col for col in object_cols if X_train[col].nunique() < 30]\n\n# Columns that will be dropped from the dataset\nhigh_cardinality_cols = list(set(object_cols)-set(low_cardinality_cols))\n\nprint('Categorical columns that will be one-hot encoded:', low_cardinality_cols)\nprint('\\nCategorical columns that will be dropped from the dataset:', high_cardinality_cols)","b1e4d37b":"from sklearn.preprocessing import OneHotEncoder\n\n\n# Apply one-hot encoder to each column with categorical data\nOH_encoder = OneHotEncoder(handle_unknown='ignore', sparse=False)\nOH_cols_train = pd.DataFrame(OH_encoder.fit_transform(X_train[low_cardinality_cols]))\nOH_cols_test = pd.DataFrame(OH_encoder.transform(X_test[low_cardinality_cols]))\n\n# One-hot encoding removed index; put it back\nOH_cols_train.index = X_train.index\nOH_cols_test.index = X_test.index\n\n# Remove categorical columns (will replace with one-hot encoding)\nnum_X_train = X_train.drop(object_cols, axis=1)\nnum_X_test = X_test.drop(object_cols, axis=1)\n\n# Add one-hot encoded columns to numerical features\nOH_X_train = pd.concat([num_X_train, OH_cols_train], axis=1)\nOH_X_test = pd.concat([num_X_test, OH_cols_test], axis=1)","e4452dc4":"print(\"RMSLE from Approach 3 (One-Hot Encoding):\") \nprint(score_dataset(OH_X_train, OH_X_test, y_train, y_test))","52fbf3d7":"y_test.head()\ny_test.shape","d2c4aa42":"\n# create dataset for lightgbm\nlgb_train = lgb.Dataset(OH_X_train, y_train)\nlgb_eval = lgb.Dataset(OH_X_test, y_test, reference=lgb_train)\n\n\n# specify your configurations as a dict\n\nparams = {\n    'boosting_type': 'gbdt',\n    'objective': 'regression',\n    'metric': {'l2', 'l1'},\n    'num_leaves': 31,\n    'learning_rate': 0.005,\n    'feature_fraction': 0.9,\n    'bagging_fraction': 0.8,\n    'bagging_freq': 5,\n    'verbose': 0\n}\n\n\nprint('Starting training...')\n# train\ngbm = lgb.train(params,\n                lgb_train,\n                num_boost_round=20,\n                valid_sets=lgb_eval,\n                early_stopping_rounds=5)\n\nprint('Saving model...')\n# save model to file\ngbm.save_model('model.txt')\n\nprint('Starting predicting...')\n# predict\ny_pred = gbm.predict(OH_X_test, num_iteration=gbm.best_iteration)\n# eval\nprint('The rmlse of prediction is:', np.sqrt(mean_squared_log_error(y_test, y_pred)))","4e0b047b":"cat_dat = data.select_dtypes(include=['object']).copy()\ncat_dat.head()","d488500d":"#The number of unique values??\nfor col in cat_dat.columns[0:]:\n    print(col, \" ------ \", data[col].nunique())","ec9cf0eb":"str_cols= cat_dat.loc[:, cat_dat.dtypes=='object'].columns.tolist()\n\nstr_cols","6a6e2296":"import category_encoders as ce\n\nencoder = ce.BinaryEncoder(cols=str_cols)\ndf_binary = encoder.fit_transform(cat_dat)\n\ndf_binary.head()\ndf_binary.shape","4ae6dd4b":"data_c = data.drop(str_cols, axis=1)\n\ndata_c.shape","fd771f48":"data_f = pd.concat([data_c, df_binary], axis=1)\n\ndata_f.shape","ff02a473":"data_f.dtypes","582dba58":"\nfrom sklearn.model_selection import train_test_split\n\n# feature columns and target\nX = data_f.drop(\"MRP_FCST_QTY\", axis=1)\ny = data_f.MRP_FCST_QTY\n\n# Break off test and train set from data\nX_train, X_test, y_train, y_test = train_test_split(X, y,\n                                                      test_size=0.3,\n                                                      random_state=42)","71bef770":"\n# RandomForestRegressor\ndef score_dataset_be(X_train, X_test, y_train, y_test):\n    model = RandomForestRegressor(n_estimators=1000, random_state=42)\n    model.fit(X_train, y_train)\n    preds = model.predict(X_test)\n    return np.sqrt(mean_squared_log_error(y_test, preds))\n\n\n#RMSLE = np.sqrt(mean_squared_log_error( y_test, predictions ))","8475fe26":"\n\nprint(\"RMSLE from Binary Encoding:\")\nprint(score_dataset_be(X_train, X_test, y_train, y_test))","a07234c5":"\n# create dataset for lightgbm\nlgb_train = lgb.Dataset(X_train, y_train)\nlgb_eval = lgb.Dataset(X_test, y_test, reference=lgb_train)\n\n\n# specify your configurations as a dict\n\nparams = {\n    'boosting_type': 'gbdt',\n    'objective': 'regression',\n    'metric': {'l2', 'l1'},\n    'num_leaves': 31,\n    'learning_rate': 0.005,\n    'feature_fraction': 0.9,\n    'bagging_fraction': 0.8,\n    'bagging_freq': 5,\n    'verbose': 0\n}\n\n\nprint('Starting training...')\n# train\ngbm = lgb.train(params,\n                lgb_train,\n                num_boost_round=200,\n                valid_sets=lgb_eval,\n                early_stopping_rounds=50)\n\nprint('Saving model...')\n# save model to file\ngbm.save_model('model_be.txt')\n\nprint('Starting predicting...')\n# predict\ny_pred = gbm.predict(X_test, num_iteration=gbm.best_iteration)\n# eval\nprint('The rmlse of prediction is:', np.sqrt(mean_squared_log_error(y_test, y_pred)))","8c3e77d7":"#BackwardDifferenceEncoder\n\n#encoder = ce.BackwardDifferenceEncoder(cols=str_cols)\n#df_bd = encoder.fit_transform(cat_dat)\n\n#df_bd.head()","3d1b2d47":"#Polynomial Coding\n#Helmert Coding\n'''\nimport category_encoders as ce\n\nencoder = ce.BackwardDifferenceEncoder(cols=[...])\nencoder = ce.BaseNEncoder(cols=[...])\nencoder = ce.BinaryEncoder(cols=[...])\nencoder = ce.CatBoostEncoder(cols=[...])\nencoder = ce.HashingEncoder(cols=[...])\nencoder = ce.HelmertEncoder(cols=[...])\nencoder = ce.JamesSteinEncoder(cols=[...])\nencoder = ce.LeaveOneOutEncoder(cols=[...])\nencoder = ce.MEstimateEncoder(cols=[...])\nencoder = ce.OneHotEncoder(cols=[...])\nencoder = ce.OrdinalEncoder(cols=[...])\nencoder = ce.SumEncoder(cols=[...])\nencoder = ce.PolynomialEncoder(cols=[...])\nencoder = ce.TargetEncoder(cols=[...])\nencoder = ce.WOEEncoder(cols=[...])\n\nencoder.fit(X, y)\nX_cleaned = encoder.transform(X_dirty)\n'''","74d26930":"#http:\/\/www.willmcginnis.com\/2015\/11\/29\/beyond-one-hot-an-exploration-of-categorical-variables\/","8dcd3b5b":"**Things to Try**","e83e65d3":"RMSLE penalizes an under-predicted estimate greater than an over-predicted estimate.\n\nRMSLE measures the ratio between actual and predicted.\n\nlog(pi+1)\u2212log(ai+1)\ncan be written as log((pi+1)\/(ai+1))","4d2b1272":"## Approach 2\n\n### Label Encoding\nLabel encoding assigns each unique value to a different integer","77584a3e":"## Approach 3\n\n### One-Hot Encoding\nOne-hot encoding creates new columns indicating the presence (or absence) of each possible value in the original data. ","628d5882":"'''\n# LinearRegression\n\ndef score_dataset_lr(X_train, X_test, y_train, y_test):\n    model = LinearRegression()\n    model.fit(X_train, y_train)\n    preds = model.predict(X_test)\n    return np.sqrt(mean_squared_log_error(y_test, preds))\n'''","b6a236fb":"### Binary Encoding\n\nIn this technique, first the categories are encoded as ordinal, then those integers are converted into binary code, then the digits from that binary string are split into separate columns. This encodes the data in fewer dimensions than one-hot.","11646748":"## Approach 1\n\n### Drop Categorical Variables","1c1cd94a":"Note that in the formulation X is the predicted value and Y is the actual value.\n\n<p align=\"center\"> \n<img src=\"https:\/\/miro.medium.com\/max\/1492\/0*AUzyQ1rc6mpQVYfn\" width=\"400\">\n<\/p>","f56b8a1d":"### Simple LightGBM"}}