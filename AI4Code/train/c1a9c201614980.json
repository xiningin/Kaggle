{"cell_type":{"63417fa6":"code","3794db92":"code","26a88780":"code","63b149f0":"code","eaa9dccd":"code","e141a642":"code","5118f291":"code","7cf7b2d1":"code","91221f71":"markdown","7db90aac":"markdown","7f1b2c0f":"markdown","b3e894b9":"markdown","d1627bb9":"markdown","6b64dd6a":"markdown","d319617f":"markdown","325e519e":"markdown","60d09b58":"markdown"},"source":{"63417fa6":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","3794db92":"import pandas as pd\n# Read datasets\/papers.csv into papers\npapers = pd.read_csv(\"..\/input\/nips-papers\/papers.csv\")\n\n# Print out the first rows of papers\n# -- YOUR CODE HERE --\npapers.head()","26a88780":"# Remove the columns\n\npapers.drop(['id', 'event_type', 'pdf_name'], axis=1, inplace=True)\n# Print out the first rows of papers\n\npapers.head(10)","63b149f0":"#Group the papers by year\ngroups = papers.groupby('year')\n\n# Determine the size of each group\ncounts = groups.size()\n\n# Visualise the counts as a bar plot\nimport matplotlib.pyplot\n%matplotlib inline\n\ncounts.plot(kind='bar', color='indigo')","eaa9dccd":"# Load the regular expression library\nimport re\n\n# Print the titles of the first rows \nprint(papers['title'].head())\n\n# The following line \npapers['title_processed'] = papers['title'].map(lambda x: re.sub('[,\\.!?]', '', x))\n\n# Convert the titles to lowercase\npapers['title_processed'] = papers['title_processed'].map(lambda x: x.lower())\n\n# Print the processed titles of the first rows \npapers['title_processed'].head()","e141a642":"# Import the wordcloud library\n# -- YOUR CODE HERE --\nimport wordcloud\n# Join the different processed titles together.\nlong_string = ' '.join(papers['title_processed'])\n\n# Create a WordCloud object\nwordcloud = wordcloud.WordCloud()\n\n# Generate a word cloud\n# -- YOUR CODE HERE --\nwordcloud.generate(long_string)\n# Visualize the word cloud\nwordcloud.to_image()","5118f291":"# Load the library with the CountVectorizer method\nfrom sklearn.feature_extraction.text import CountVectorizer\nimport numpy as np\n\n# Helper function\ndef plot_10_most_common_words(count_data, count_vectorizer):\n    import matplotlib.pyplot as plt\n    words = count_vectorizer.get_feature_names()\n    total_counts = np.zeros(len(words))\n    for t in count_data:\n        total_counts+=t.toarray()[0]\n    \n    count_dict = (zip(words, total_counts))\n    count_dict = sorted(count_dict, key=lambda x:x[1], reverse=True)[0:10]\n    words = [w[0] for w in count_dict]\n    counts = [w[1] for w in count_dict]\n    x_pos = np.arange(len(words)) \n\n    plt.bar(x_pos, counts,align='center')\n    plt.xticks(x_pos, words, rotation=90) \n    plt.xlabel('words')\n    plt.ylabel('counts')\n    plt.title('10 most common words')\n    plt.show()\n\n# Initialise the count vectorizer with the English stop words\ncount_vectorizer = CountVectorizer(stop_words ='english')\n\n# Fit and transform the processed titles\ncount_data = count_vectorizer.fit_transform(papers['title_processed'])\n\n# Visualise the 10 most common words\n\nplot_10_most_common_words(count_data, count_vectorizer)","7cf7b2d1":"import warnings\nwarnings.simplefilter(\"ignore\", DeprecationWarning)\n\n# Load the LDA model from sk-learn\nfrom sklearn.decomposition import LatentDirichletAllocation as LDA\n \n# Helper function\ndef print_topics(model, count_vectorizer, n_top_words):\n    words = count_vectorizer.get_feature_names()\n    for topic_idx, topic in enumerate(model.components_):\n        print(\"\\nTopic #%d:\" % topic_idx)\n        print(\" \".join([words[i]\n                        for i in topic.argsort()[:-n_top_words - 1:-1]]))\n        \n# Tweak the two parameters below (use int values below 15)\nnumber_topics = 10\nnumber_words = 10\n\n# Create and fit the LDA model\nlda = LDA(n_components=number_topics)\nlda.fit(count_data)\n\n# Print the topics found by the LDA model\nprint(\"Topics found via LDA:\")\nprint_topics(lda, count_vectorizer, number_words)","91221f71":"# Preparing the data for analysis","7db90aac":"# A word cloud to visualize the preprocessed text data","7f1b2c0f":"Thank you!\n> Upvote if you like! Comment for further ideas!","b3e894b9":"# Preprocessing the text data","d1627bb9":"Machine learning has become increasingly popular over the past years. The number of NIPS conference papers has risen exponentially, and people are continuously looking for ways on how they can incorporate machine learning into their products and services.\n\nAlthough this analysis focused on analyzing machine learning trends in research, a lot of these techniques are rapidly being adopted in industry. Following the latest machine learning trends is a critical skill for a data scientist, and it is recommended to continuously keep learning by going through blogs, tutorials, and courses.","6b64dd6a":"# The future of machine learning","d319617f":"# Analysing trends with LDA\nFinally, the research titles will be analyzed using LDA. Note that in order to process a new set of documents (e.g. news articles), a similar set of steps will be required to preprocess the data. The flow that was constructed here can thus easily be exported for a new text dataset.\n\nThe only parameter we will tweak is the number of topics in the LDA algorithm. Typically, one would calculate the 'perplexity' metric to determine which number of topics is best and iterate over different amounts of topics until the lowest 'perplexity' is found. For now, let's play around with a different number of topics. From there, we can distinguish what each topic is about ('neural networks', 'reinforcement learning', 'kernel methods', 'gaussian processes', etc.).","325e519e":"# Plotting how machine learning has evolved over time","60d09b58":"# Prepare the text for LDA analysis\nThe main text analysis method that we will use is latent Dirichlet allocation (LDA). LDA is able to perform topic detection on large document sets, determining what the main 'topics' are in a large unlabeled set of texts. A 'topic' is a collection of words that tend to co-occur often. The hypothesis is that LDA might be able to clarify what the different topics in the research titles are. These topics can then be used as a starting point for further analysis.\n\nLDA does not work directly on text data. First, it is necessary to convert the documents into a simple vector representation. This representation will then be used by LDA to determine the topics. Each entry of a 'document vector' will correspond with the number of times a word occurred in the document. In conclusion, we will convert a list of titles into a list of vectors, all with length equal to the vocabulary. For example, 'Analyzing machine learning trends with neural networks.' would be transformed into [1, 0, 1, ..., 1, 0]."}}