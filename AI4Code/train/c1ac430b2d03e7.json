{"cell_type":{"abb9bfbf":"code","20311954":"code","297f336a":"code","5f00231b":"code","6e50c831":"code","deb5e5e1":"code","c139f676":"code","43403d72":"code","352a1434":"code","f2c0780b":"code","1d845432":"code","a0df2829":"code","cb2ebc83":"code","cf6acb37":"code","4626ec40":"code","333a9eb0":"code","73539954":"code","dfe8a52d":"code","a3bbbd6a":"code","2b794aa4":"code","993dabbe":"code","93a03b1f":"code","03d0e184":"code","f67b3cdb":"code","be19aba1":"code","0be69336":"code","9e96e549":"code","7682b28d":"markdown","7524aad1":"markdown","01d3695c":"markdown","004412b4":"markdown","ee88a6d5":"markdown","6f797e93":"markdown","8da7bae4":"markdown","8ef02ca5":"markdown","83ca2d73":"markdown","b289acf6":"markdown"},"source":{"abb9bfbf":"# import dependencies\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport matplotlib\nimport numpy as np\nimport seaborn as sns\n\nsns.set(style=\"darkgrid\")","20311954":"# load data \ndf = pd.read_csv(\"..\/input\/HR_comma_sep.csv\");\n\n# get the column names to list\ncol_names = df.columns.tolist()\n\nprint(\"Column names:\")\nprint(col_names)\n\nprint(\"\\nSample data:\")\ndf.head()","297f336a":"# we have 14999 rows and 10 columns\ndf.shape","5f00231b":"# change the names of sales to department\ndf = df.rename(columns = {'sales':'department'})\n\ndf.head()","6e50c831":"# check is the data contains 'null values'\ndf.isnull().any()","deb5e5e1":"# check what athe departments are \ndf['department'].unique()","c139f676":"#numpy.where(condition[, x, y])\n#Return elements, either from x or y, depending on condition.\n\n# turn support category in technical category\ndf['department'] = np.where(df['department'] == 'support', 'technical', df['department'])\n\n# turn IT in technical category\ndf['department'] = np.where(df['department'] == 'IT' , 'technical', df['department'])\n\ndf['department'].unique()","43403d72":"df['left'].value_counts()","352a1434":"3571\/11428","f2c0780b":"# check the numbers across people that left and people that didnt left\n\n# pandas groupby function allows you to group by certain features\ndf.groupby('left').mean()","1d845432":"df.groupby('department').mean()","a0df2829":"df.groupby('salary').mean()","cb2ebc83":"# Compute a simple cross-tabulation of two (or more) factors\n\npd.crosstab(df.department, df.left).plot(kind='bar')\nplt.title('Turnover Frequency per Department')\nplt.xlabel('Department')\nplt.ylabel('0; stayed | 1; left')","cf6acb37":"table = pd.crosstab(df.salary, df.left)\ntable.div(table.sum(1).astype(float), axis=0).plot(kind='bar', stacked=True)\nplt.title('Turnover Frequency and Salary')\nplt.xlabel('Salary')\nplt.ylabel('0; stayed | 1; left')","4626ec40":"# convert to dummies\ncat_vars=['department','salary']\n\nfor var in cat_vars:\n    cat_list='var'+'_'+ var\n    cat_list = pd.get_dummies(df[var], prefix=var) # convert to dummy variables\n    df1 = df.join(cat_list)\n    df = df1","333a9eb0":"# remove the old categorical variables\ndf.drop(df.columns[[8,9]], axis=1, inplace=True)\ndf.columns.values","73539954":"# the outcome variable is left (y) all the other variables are predictors\n\ndf_vars = df.columns.values.tolist()\ny=['left']\nX=[i for i in df_vars if i not in y]","dfe8a52d":"from sklearn.feature_selection import RFE\nfrom sklearn.linear_model import LogisticRegression\n\nmodel = LogisticRegression()\n\nrfe = RFE(model, 10)\nrfe = rfe.fit(df[X], df[y])\nprint(rfe.support_)\nprint('the selected features are ranked with 1')\nprint(rfe.ranking_)","a3bbbd6a":"# so these are the columns that we should select\ncols = ['satisfaction_level', 'last_evaluation', 'time_spend_company', 'Work_accident', 'promotion_last_5years', \n        'department_hr', 'department_management', 'salary_high', 'salary_low'] \n# the predictors\nX = df[cols]\n\n# the outcome \nY = df['left']","2b794aa4":"# create a train and a test set\nfrom sklearn.cross_validation import train_test_split\n\n# all lowercase for random forest\nx_train, x_test, y_train, y_test = train_test_split(X, Y, test_size=0.3, random_state=0)","993dabbe":"from sklearn.ensemble import RandomForestClassifier\nrf = RandomForestClassifier()\nrf.fit(x_train, y_train)\n\nfrom sklearn.metrics import accuracy_score\nprint('Random Forest Accuracy: {:.3f}'.format(accuracy_score(y_test, rf.predict(x_test))))","93a03b1f":"import xgboost as xgb\n\n# we first have to convert the dataset into an optimised data structure that xgb supports\ndata_dmatrix = xgb.DMatrix(data=X,label=Y)","03d0e184":"from sklearn.model_selection import train_test_split\n\n# split data into test and train\nX_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=123)","f67b3cdb":"# instantiate an XGBoost regressor object by calling the XGBregressor() class from the xgboost library\n# pass the necessary hyperparameters as arguments\n\nxg_reg = xgb.XGBRegressor(objective ='reg:linear', colsample_bytree = 0.3, learning_rate = 0.1,\n                max_depth = 5, alpha = 10, n_estimators = 10)\n\nxg_reg.fit(X_train,Y_train)","be19aba1":"from sklearn.metrics import accuracy_score\nprint('XGBoost Accuracy: {:.3f}'.format(accuracy_score(Y_test, xg_reg.predict(X_test))))","0be69336":"# Random Forest model precision and recall\nfrom sklearn.metrics import classification_report\n\n# use sklearn to give us the report\nprint(classification_report(y_test, rf.predict(x_test)))","9e96e549":"# confusion matrix for Random Forrest\ny_pred = rf.predict(x_test)\n\nfrom sklearn.metrics import confusion_matrix\nimport seaborn as sns\nfrom sklearn import metrics\n\nforest_cm = metrics.confusion_matrix(y_pred, y_test, [1,0])\nsns.heatmap(forest_cm, annot=True, fmt='.2f',xticklabels = [\"Left\", \"Stayed\"] , yticklabels = [\"Left\", \"Stayed\"] )\n\nplt.ylabel('True class')\nplt.xlabel('Predicted class')\nplt.title('Random Forest')\nplt.savefig('random_forest')\n","7682b28d":"# Random Forest","7524aad1":"## 2 Data Exploration\n\nLets find out how many people left the company.\n\n","01d3695c":"Observations;\n\n- The average satisfaction level of employees who stayed with the company is higher than that of the employees who left.\n- The average monthly work hours of employees who left the company is more than that of the employees who stayed.\n- The employees who had workplace accidents are less likely to leave than that of the employee who did not have workplace accidents.\n- The employees who were promoted in the last five years are less likely to leave than those who did not get a promotion in the last five years.\n\nNow we also want to get a sort of average for categorical variables; **department, salary and number_of_projects**","004412b4":"In order to use all the data for modelling, we need to convert the categorical variables to dummy variables.\n\n**Dummy variables**\n\nDummy variables are used when you want to work with categorical variables that have no quantifiable relationship with each other. \nWe assign 0 to each category that is not it and 1 to each category that it is. We sort of convert it to binary.\n\nThis is the process:\n\n1- convert categorical variables to dummy variables\n\n2- delete the old categorical variables\n\n","ee88a6d5":"## 1 Feature Engineering\n\n\n","6f797e93":"## 3 Visualisation \n\nGet better insight into the data, a clearer picture. Recognise the significant features.\n","8da7bae4":"## 4 Feature Selection\n\n- We only want to pick the features that are truly relevant for predicting y ( whether someone left or not )\n- How do we select the right features \/ predictors?\n\nWe can use sk.learn's ```sklearn.feature_selection.RFE ```\n\nGiven an external estimator that assigns weights to features (e.g., the coefficients of a linear model), the goal of recursive feature elimination (RFE) is to **select features by recursively considering smaller and smaller sets of features**. First, the estimator is trained on the initial set of features and the importance of each feature is obtained either through a ```coef_``` attribute or through a ```feature_importances_``` attribute. Then, the least important features are pruned from current set of features. That procedure is recursively repeated on the pruned set until the desired number of features to select is eventually reached.\n\n\nTo do:\ncheck other methods for feature selection","8ef02ca5":"For this dataset, **recall**  measures: when an employee left, how often is that predicted correctly?\nOut of all the turnover cases, random forest correctly got 987 out of 1038. This means we have a turnover \u201crecall\u201d of about 95% (987\/1038)\n\n**Precision** measures in this case: when the model predicts an employee will leave, how often do they actually leave? \nThe Random Forest has about 95% precision ( 87 out of 1045) ","83ca2d73":"# XGBoost\n\n#### Hyperparameters\n\n- learning_rate: step size shrinkage used to prevent overfitting. Range is [0,1]\n- max_depth: determines how deeply each tree is allowed to grow during any boosting round.\n- subsample: percentage of samples used per tree. Low value can lead to underfitting.\n- colsample_bytree: percentage of features used per tree. High value can lead to overfitting.\n- n_estimators: number of trees you want to build.\n- objective: determines the loss function to be used like reg:linear for regression problems, reg:logistic for classification problems with only decision, binary:logistic for classification problems with probability.\n\nXGBoost also supports regularization parameters to penalize models as they become more complex and reduce them to simple (parsimonious) models.\n\n- gamma: controls whether a given node will split based on the expected reduction in loss after the split. A higher value leads to fewer splits. Supported only for tree-based learners.\n- alpha: L1 regularization on leaf weights. A large value leads to more regularization.\n- lambda: L2 regularization on leaf weights and is smoother than L1 regularization.","b289acf6":"## 5 Precision and Recall\n\n- Knowing the accuracy of a model is not enough, we need to know the precision and recall.\n- In many tasks such as imbalanced classification problems the accuracy is certainly not the whole story.\n\nNow, we visualise predictions and evaluate the accuracy of a classification.\n\nThen we can compare the true accuracy of the two models - Random Forest & XGBoost\n\n- **Precision**: the fraction of relevant instances among the retrieved instances. ( i.e. the ability of a classification model to identify only the relevant data points)\n\n> *precision = number of true positives \/ n true positives + n false positives*\n\n\n- **Recall**: the fraction of relevant instances that have been retrieved over the total amount of relevant instances. ( i.e. the ability of a model to find all the relevant cases within a dataset )\n\n> *recall = number of true positives \/ n  true positives + n false negatives*\n\n![](https:\/\/upload.wikimedia.org\/wikipedia\/commons\/2\/26\/Precisionrecall.svg)\n\n\ne.g. When a search engine returns 30 pages only 20 of which were relevant while failing to return 40 additional relevant pages, its precision is 20\/30 = 2\/3 while its recall is 20\/60 = 1\/3. So, in this case, precision is \"how useful the search results are\", and recall is \"how complete the results are\".\n\n\n- In cases where we want to find an optimal blend of precision and recall we can combine the two of them in what is called an **F1 score**. The F1 score is the harmonic mean of precision and recall taking both metrics into account in the following equation:\n\n> *F1 = 2 x ( precision x recall ) \/ precision + recall *\n\nIts better to use the F1 score instead of a normal average because this way we punish the extreme values. "}}