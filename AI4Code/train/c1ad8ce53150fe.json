{"cell_type":{"c1eb399c":"code","b954b04a":"code","5bc38a4e":"code","007effb7":"code","c0896def":"code","2bb4746e":"code","cd5ebd27":"code","7344640a":"code","11109ece":"code","af1aaa59":"code","98eb2c2d":"code","73f62ffd":"code","002e4c23":"code","a869ace9":"code","8f857100":"code","f59d06cf":"code","e91fc024":"code","b9c31fec":"code","85d0f487":"code","64015bcd":"code","f4861cdb":"code","258051df":"markdown","e4e968ed":"markdown","5186083a":"markdown","dfdc0121":"markdown","ebed8adb":"markdown"},"source":{"c1eb399c":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt # data visualization\nimport seaborn as sns #data visualization\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","b954b04a":"# Loading in the training data\ntrain_data = pd.read_csv(\"..\/input\/clothing\/train.csv\")\ntrain_data.info()","5bc38a4e":"# Loading in the testing data\ntest_data = pd.read_csv(\"..\/input\/clothing\/eval.csv\")\ntest_data.info()","007effb7":"# Check for missing training data\nfor column in train_data.columns:\n    if (train_data[column].isnull().sum() != 0):\n        print('Missing {} Data: {} \/ {} = {}%'.format(column, train_data[column].isnull().sum(), len(train_data.index), round(train_data[column].isnull().sum() \/ len(train_data.index) * 100)))\nprint(\"Done checking for missing values\")","c0896def":"# Check for missing training data\nfor column in test_data.columns:\n    if (test_data[column].isnull().sum() != 0):\n        print('Missing {} Data: {} \/ {} = {}%'.format(column, test_data[column].isnull().sum(), len(test_data.index), round(test_data[column].isnull().sum() \/ len(test_data.index) * 100)))\nprint(\"Done checking for missing values\")","2bb4746e":"def create_img(img_id):\n    img = train_data.iloc[img_id]\n    img_arr = []\n    for i in range(28):\n        pixel_row = []\n        for j in range(28):\n            pix = i * 28 + j + 1\n            pixel_row.append(img[\"pixel\" + str(pix)])\n        img_arr.append(pixel_row)\n    return(img_arr)","cd5ebd27":"def create_test_img(img_id):\n    img = test_data.iloc[img_id]\n    img_arr = []\n    for i in range(28):\n        pixel_row = []\n        for j in range(28):\n            pix = i * 28 + j + 1\n            pixel_row.append(img[\"pixel\" + str(pix)])\n        img_arr.append(pixel_row)\n    return(img_arr)","7344640a":"test = create_img(0)\nplt.imshow(test, cmap='gray')","11109ece":"data = []\nn = 60000\nfor i in range(n):\n    data.append(create_img(i))\ntraining_data = np.array(data)\nprint(training_data.shape)","af1aaa59":"data = []\nfor i in range(10000):\n    data.append(create_test_img(i))\ntesting_data = np.array(data)\nprint(testing_data.shape)","98eb2c2d":"categories = np.array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])\nohe_labels = np.zeros((len(train_data['label']), 10))\nprint(ohe_labels.shape)\nfor i in range(len(ohe_labels)):\n    j = np.where(categories == train_data['label'][i])\n    ohe_labels[i, j] = 1","73f62ffd":"import tensorflow as tf\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Conv2D, Flatten, MaxPool2D, Dropout, BatchNormalization\nfrom keras.callbacks import ModelCheckpoint\nfrom sklearn.model_selection import train_test_split\nfrom tensorflow.keras.wrappers.scikit_learn import KerasRegressor\nfrom sklearn.model_selection import RandomizedSearchCV\nfrom sklearn.model_selection import RepeatedKFold\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import validation_curve\n\ntf.random.set_seed(13)\n\nX = training_data\ny = ohe_labels\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.1)","002e4c23":"print(X_train.shape)\nprint(X_test.shape)\nprint(y_train.shape)\nprint(y_test.shape)\n\nX_train = X_train.reshape(X_train.shape[0], 28, 28, 1)\ny_train = y_train.reshape(y_train.shape[0], 10)\nX_test = X_test.reshape(X_test.shape[0], 28, 28, 1)\ny_test = y_test.reshape(y_test.shape[0], 10)\n\nprint(X_train.shape)\nprint(X_test.shape)\nprint(y_train.shape)\nprint(y_test.shape)","a869ace9":"model = Sequential()\nmodel.add(Conv2D(10, kernel_size=3, activation='relu', input_shape=(28, 28, 1), padding='same'))\nmodel.add(BatchNormalization())\nmodel.add(Conv2D(10, kernel_size=3, activation='relu'))\nmodel.add(BatchNormalization())\nmodel.add(MaxPool2D(2))\nmodel.add(Conv2D(20, kernel_size=3, activation='relu'))\nmodel.add(BatchNormalization())\nmodel.add(Conv2D(20, kernel_size=3, activation='relu'))\nmodel.add(BatchNormalization())\nmodel.add(MaxPool2D(2))\nmodel.add(Flatten())\nmodel.add(Dense(256, activation=\"relu\"))\nmodel.add(BatchNormalization())\nmodel.add(Dense(10, activation='softmax'))\nmodel.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\nmodel.summary()","8f857100":"checkpoint = ModelCheckpoint(\"weights.hdf5\", monitor = \"val_accuracy\", save_best_only = True)\n\nmodel.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n\n# Fit the model on a training set\ntraining = model.fit(X_train, y_train, validation_split=0.2, epochs=25, batch_size=32, callbacks = [checkpoint]);","f59d06cf":"history = training.history\nplt.plot(history['loss']);\nplt.plot(history['val_loss']);\n\nplt.plot(history['accuracy']);\nplt.plot(history['val_accuracy']);","e91fc024":"best_model = Sequential()\nbest_model.add(Conv2D(10, kernel_size=3, activation='relu', input_shape=(28, 28, 1), padding='same'))\nbest_model.add(BatchNormalization())\nbest_model.add(Conv2D(10, kernel_size=3, activation='relu'))\nbest_model.add(BatchNormalization())\nbest_model.add(MaxPool2D(2))\nbest_model.add(Conv2D(20, kernel_size=3, activation='relu'))\nbest_model.add(BatchNormalization())\nbest_model.add(Conv2D(20, kernel_size=3, activation='relu'))\nbest_model.add(BatchNormalization())\nbest_model.add(MaxPool2D(2))\nbest_model.add(Flatten())\nbest_model.add(Dense(256, activation=\"relu\"))\nbest_model.add(BatchNormalization())\nbest_model.add(Dense(10, activation='softmax'))\nbest_model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\nbest_model.summary()","b9c31fec":"best_model.load_weights(\"weights.hdf5\")","85d0f487":"model.evaluate(X_test, y_test, batch_size=10)\nbest_model.evaluate(X_test, y_test, batch_size=10)","64015bcd":"final_data = testing_data.reshape(testing_data.shape[0], 28, 28, 1)\n\npredicts = best_model.predict(final_data)\nprint(predicts.shape)\nsubmission_values = np.argmax(predicts, axis=1)\nprint(len(submission_values))","f4861cdb":"output = pd.DataFrame({'id': test_data['id'], 'label': submission_values})\n\noutput.to_csv(\"submisssion.csv\", index=False)\nprint(\"Submission saved\")\n\nprint(output)","258051df":"# Loading Data","e4e968ed":"# Exploratory Data Analysis","5186083a":"# Submission Generation","dfdc0121":"The obvious choice for this data set would be a convolutional dataset, which means we need to turn each row of the dataset into a 2d array, like create_img does.","ebed8adb":"# Making Models"}}