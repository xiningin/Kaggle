{"cell_type":{"b29d4643":"code","335cbff5":"code","8340ff93":"code","ce440658":"code","ce3f84fc":"code","5cf55c87":"code","a02c64c5":"code","ad1759c3":"code","30eb936a":"code","e5e3c7c3":"code","ab71b533":"code","975c1754":"code","7bb0976a":"code","1f856438":"code","6628ebba":"code","64ee1b46":"code","92ad5f27":"code","e1ffdd9c":"code","9e48c982":"code","8eb5ce00":"code","48c2821c":"code","983edf8a":"code","beaf2f44":"code","856df13a":"code","c2dca7f6":"code","515fcaa3":"markdown","c3e4acdc":"markdown","885f419a":"markdown","31541c44":"markdown","312cf180":"markdown"},"source":{"b29d4643":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","335cbff5":"import numpy as np\nimport pandas as pd\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split,cross_val_score\nfrom sklearn.feature_selection import SelectKBest,chi2,f_regression\nfrom sklearn.linear_model import LogisticRegression,LogisticRegressionCV\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.metrics import r2_score,mean_absolute_error,accuracy_score,confusion_matrix,classification_report\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n%matplotlib inline\n%config Completer.use_jedi = False\n\nscale = StandardScaler()\ndf = pd.read_csv('\/kaggle\/input\/bank-marketing-campaigns-dataset\/bank-additional-full.csv',sep=\";\")\ndf.head()","8340ff93":"df.info()","ce440658":"df.select_dtypes('object')","ce3f84fc":"df['y'].value_counts()","5cf55c87":"df = df.replace('unknown',np.NaN)\ndf['y'] = df['y'].apply(lambda x: 1 if x=='yes' else (0 if x=='no' else None))\ndf.head()","a02c64c5":"df.isna().sum()","ad1759c3":"def one_hot_encoder(df,columns,prefixes):\n    df = df.copy()\n    for column,prefix in zip(columns,prefixes):\n        dummies = pd.get_dummies(df[column],prefix=prefix)\n        df = pd.concat([df,dummies],axis=1)\n        df = df.drop(column,axis=1)\n    return df\n\ndef ordinal_encoder(df,columns,orderings):\n    df = df.copy()\n    for column,ordering in zip(columns,orderings):\n        df[column] = df[column].apply(lambda x:ordering.index(x)) \n    return df\n    \n#binary encoder\ndef binary_encoder(df, columns, positive_values):\n    df = df.copy()\n    for column, positive_value in zip(columns, positive_values):\n        df[column] = df[column].apply(lambda x: 1 if x == positive_value else x)\n        df[column] = df[column].apply(lambda x: 0 if str(x) != 'nan' else x)\n    return df","30eb936a":"nominal_features = ['job','marital','education','day_of_week','month','poutcome']\nprefixes = ['j','m','e','d','mo','p']\n\ndf = one_hot_encoder(df,nominal_features,prefixes)","e5e3c7c3":"binary_features = ['default','housing','loan','contact']\npositive_values = ['yes','yes','yes','cellular']\n# df = binary_encoder(df,binary_features,positive_values)\nbinVal = {'yes':1,'no':0}\ncontVal = {'cellular':1,'telephone':0}\ndf['housing'].replace(binVal,inplace=True)\ndf['default'].replace(binVal,inplace=True)\ndf['loan'].replace(binVal,inplace=True)\ndf['contact'].replace(contVal,inplace=True)\n# df['housing'].value_counts()","ab71b533":"df.head()","975c1754":"df.isna().sum()\nprint(df['housing'].unique())","7bb0976a":"for column in ['default','housing','loan']:\n    df[column] = df[column].fillna(df[column].mean())","1f856438":"print(\"missing values are: {}\".format(df.isna().sum().sum()))","6628ebba":"y = df['y']\nX = df.drop(columns='y').copy()\n\ntrainX,testX,trainY,testY = train_test_split(X,y,random_state=36,stratify=y,test_size=0.25)\n#Scaling X\ntrainX = scale.fit_transform(trainX)\ntestX = scale.fit_transform(testX)","64ee1b46":"sns.countplot(df['y'])","92ad5f27":"best_feature = SelectKBest(k=\"all\").fit(trainX,trainY)\nscores = pd.DataFrame(best_feature.scores_)\ncolumns = pd.DataFrame(X.columns)\nbestFeatures = pd.concat([columns,scores],axis=1)\nbestFeatures.columns = ['Feature','Score']\nbestFeatures = bestFeatures.sort_values(by=\"Score\",ascending=False)\nbestFeatures","e1ffdd9c":"model_acc_scores = {}\ndef predictionResult(testY,pred,model):\n    conf_mat = confusion_matrix(testY,pred)\n    correct = conf_mat[0,0]+conf_mat[1,1]\n    wrong = conf_mat.sum() - correct\n    mae = mean_absolute_error(testY,pred)\n    acc_score = accuracy_score(testY,pred)\n    model_acc_scores[model] = {'correct':correct,'wrong':wrong,'mae':mae,'accuracy_score':acc_score}\n    print(\"{} {} {}\".format(\"-\"*20,model,\"-\"*20))\n    print(\"Model predicted {} correct and {} wrong\".format(correct,wrong))\n    print(\"Mean Absolute Error is: {}\".format(round(mae*100,2)))\n    print(\"Accuracy Score is: {}\".format(round(acc_score*100,2)))","9e48c982":"model = LogisticRegression(max_iter=200).fit(trainX,trainY)\ncv_score = cross_val_score(model,trainX,trainY,cv=10)\npred = model.predict(testX)\nprint(\"Cross val score is: {}%\".format(round(cv_score.mean()*100,2)))\npredictionResult(testY,pred,\"LogisticRegression\")","8eb5ce00":"model = LogisticRegressionCV(cv=10,max_iter=320).fit(trainX,trainY)\npred = model.predict(testX)\npredictionResult(testY,pred,\"LogisticRegressionCV\")\nreport = classification_report(testY,pred)\nprint(report)","48c2821c":"model = RandomForestClassifier().fit(trainX,trainY)\ncv_score = cross_val_score(model,trainX,trainY,cv=10)\npred = model.predict(testX)\nprint(\"Cross val score is: {}%\".format(round(cv_score.mean()*100,2)))\npredictionResult(testY,pred,\"RandomForestClassifier\")\nreport = classification_report(testY,pred)\nprint(report)","983edf8a":"model = DecisionTreeClassifier().fit(trainX,trainY)\ncv_score = cross_val_score(model,trainX,trainY,cv=10)\npred = model.predict(testX)\nprint(\"Cross val score is: {}%\".format(round(cv_score.mean()*100,2)))\npredictionResult(testY,pred,\"DecisionTreeClassifier\")","beaf2f44":"model = KNeighborsClassifier(n_neighbors=23).fit(trainX,trainY)\ncv_score = cross_val_score(model,trainX,trainY,cv=10)\npred = model.predict(testX)\nprint(\"Cross val score is: {}%\".format(round(cv_score.mean()*100,2)))\npredictionResult(testY,pred,\"KNeighborsClassifier\")","856df13a":"model = GaussianNB().fit(trainX,trainY)\ncv_score = cross_val_score(model,trainX,trainY,cv=10)\npred = model.predict(testX)\nprint(\"Cross val score is: {}%\".format(round(cv_score.mean()*100,2)))\npredictionResult(testY,pred,\"GaussianNB\")","c2dca7f6":"res = pd.DataFrame(model_acc_scores)\nres.head()","515fcaa3":"# Feature Encoding","c3e4acdc":"# Applying ML Algorithms","885f419a":"# Features Selection","31541c44":"# Filling Missing Values","312cf180":"# Splitting \/ Scaling Data"}}