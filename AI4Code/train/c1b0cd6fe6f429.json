{"cell_type":{"20bf2dcb":"code","a31e8364":"code","6932c064":"code","96076a61":"code","799e2e46":"code","5e3a0eca":"code","8bc1208e":"code","1d261919":"code","a37d26df":"markdown","98cb0ebb":"markdown"},"source":{"20bf2dcb":"#@title Download from Github\n!git clone https:\/\/github.com\/openai\/glide-text2im.git\n\n%cd .\/glide-text2im\/\n!pip install -e .\n!pip install ftfy","a31e8364":"#@title Imports\nfrom PIL import Image\nfrom IPython.display import display\nimport torch as th\nimport torch.nn as nn\n\nfrom glide_text2im.clip.model_creation import create_clip_model\nfrom glide_text2im.download import load_checkpoint\nfrom glide_text2im.model_creation import (\n    create_model_and_diffusion,\n    model_and_diffusion_defaults,\n    model_and_diffusion_defaults_upsampler,\n)\nfrom glide_text2im.tokenizer.simple_tokenizer import SimpleTokenizer","6932c064":"# This notebook supports both CPU and GPU.\n# On CPU, generating one sample may take on the order of 20 minutes.\n# On a GPU, it should be under a minute.\n\nhas_cuda = th.cuda.is_available()\ndevice = th.device('cpu' if not has_cuda else 'cuda')\n\nimport multiprocessing\nimport torch\nimport os\nfrom psutil import virtual_memory\ndevice = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n!nvidia-smi","96076a61":"#@title Create base model.\noptions = model_and_diffusion_defaults()\noptions['use_fp16'] = has_cuda\noptions['timestep_respacing'] = '100' # use 100 diffusion steps for fast sampling\nmodel, diffusion = create_model_and_diffusion(**options)\nmodel.eval()\nif has_cuda:\n    model.convert_to_fp16()\nmodel.to(device)\nmodel.load_state_dict(load_checkpoint('base', device))\nprint('total base parameters', sum(x.numel() for x in model.parameters()))","799e2e46":"#@title Create upsampler model.\noptions_up = model_and_diffusion_defaults_upsampler()\noptions_up['use_fp16'] = has_cuda\noptions_up['timestep_respacing'] = 'fast27' # use 27 diffusion steps for very fast sampling\nmodel_up, diffusion_up = create_model_and_diffusion(**options_up)\nmodel_up.eval()\nif has_cuda:\n    model_up.convert_to_fp16()\nmodel_up.to(device)\nmodel_up.load_state_dict(load_checkpoint('upsample', device))\nprint('total upsampler parameters', sum(x.numel() for x in model_up.parameters()))","5e3a0eca":"#@title Create CLIP model.\nclip_model = create_clip_model(device=device)\nclip_model.image_encoder.load_state_dict(load_checkpoint('clip\/image-enc', device))\nclip_model.text_encoder.load_state_dict(load_checkpoint('clip\/text-enc', device))","8bc1208e":"#@title Definitions\ndef show_images(batch: th.Tensor):\n    \"\"\" Display a batch of images inline. \"\"\"\n    scaled = ((batch + 1)*127.5).round().clamp(0,255).to(th.uint8).cpu()\n    reshaped = scaled.permute(2, 0, 3, 1).reshape([batch.shape[2], -1, 3])\n    display(Image.fromarray(reshaped.numpy()))","1d261919":"# Parameters\nprompt = \"super mario\" #@param {type: \"string\"}\nbatch_size = 2 #@param {type: \"number\"}\nguidance_scale = 3.0 #@param {type: \"number\"}\n\n#@markdown Tune this parameter to control the sharpness of 256x256 images.\n#@markdown <br>A value of 1.0 is sharper, but sometimes results in grainy artifacts.\nupsample_temp = 0.997 #@param {type: \"number\"}\n\n# Images emerge here\n##############################\n# Sample from the base model #\n##############################\nprint(prompt)\n# Create the text tokens to feed to the model.\ntokens = model.tokenizer.encode(prompt)\ntokens, mask = model.tokenizer.padded_tokens_and_mask(\n    tokens, options['text_ctx']\n)\n\n# Pack the tokens together into model kwargs.\nmodel_kwargs = dict(\n    tokens=th.tensor([tokens] * batch_size, device=device),\n    mask=th.tensor([mask] * batch_size, dtype=th.bool, device=device),\n)\n\n# Setup guidance function for CLIP model.\ncond_fn = clip_model.cond_fn([prompt] * batch_size, guidance_scale)\n\n# Sample from the base model.\nmodel.del_cache()\nsamples = diffusion.p_sample_loop(\n    model,\n    (batch_size, 3, options[\"image_size\"], options[\"image_size\"]),\n    device=device,\n    clip_denoised=True,\n    progress=True,\n    model_kwargs=model_kwargs,\n    cond_fn=cond_fn,\n)\nmodel.del_cache()\n\n# Show the output\nshow_images(samples)\n\n##############################\n# Upsample the 64x64 samples #\n##############################\n\ntokens = model_up.tokenizer.encode(prompt)\ntokens, mask = model_up.tokenizer.padded_tokens_and_mask(\n    tokens, options_up['text_ctx']\n)\n\n# Create the model conditioning dict.\nmodel_kwargs = dict(\n    # Low-res image to upsample.\n    low_res=((samples+1)*127.5).round()\/127.5 - 1,\n\n    # Text tokens\n    tokens=th.tensor(\n        [tokens] * batch_size, device=device\n    ),\n    mask=th.tensor(\n        [mask] * batch_size,\n        dtype=th.bool,\n        device=device,\n    ),\n)\n\n# Sample from the base model.\nmodel_up.del_cache()\nup_shape = (batch_size, 3, options_up[\"image_size\"], options_up[\"image_size\"])\nup_samples = diffusion_up.ddim_sample_loop(\n    model_up,\n    up_shape,\n    noise=th.randn(up_shape, device=device) * upsample_temp,\n    device=device,\n    clip_denoised=True,\n    progress=True,\n    model_kwargs=model_kwargs,\n    cond_fn=None,\n)[:batch_size]\nmodel_up.del_cache()\n\n# Show the output\nshow_images(up_samples)","a37d26df":"# Press Run All.\nBefore using this Kaggle notebook, you have to go to the right settings pane and enable GPU and Internet.\n\nThis is a Kaggle version of @ai_curio's GLIDE collab\n\nOriginal: https:\/\/colab.research.google.com\/drive\/1DAlt3Eowa2vlps75P3HLQOBfzyjXwE_y","98cb0ebb":"# Generate"}}