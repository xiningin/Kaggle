{"cell_type":{"c70e2af7":"code","82e9927b":"code","362b656b":"code","866bdb4a":"code","bdc4b447":"code","ac0aa804":"code","e43fb75c":"code","199c690b":"code","adf610dc":"code","6e50f6aa":"code","4eb5535c":"code","5304ed0f":"code","42744bc8":"code","b5c4d5fe":"code","92da0bff":"code","138e9d81":"code","c10236b5":"code","18693ef0":"code","09b4632b":"code","57141807":"code","32b614cf":"code","0a44520e":"code","8d0e315a":"code","4cc4e204":"code","4a40b302":"code","204a2b76":"code","2aa82e25":"code","fbc292cd":"code","73da3ff8":"code","21d42a6d":"markdown","333b41b6":"markdown","0bbd973a":"markdown","945b7631":"markdown","73850103":"markdown","cb1c8ea6":"markdown","4342a7a3":"markdown","8fd57906":"markdown","dd8ce5e5":"markdown","852fee2e":"markdown","91051fac":"markdown","865b62dd":"markdown","2b053f03":"markdown","15acc0c1":"markdown","c6ed32e9":"markdown","9ad75a36":"markdown","2b41c05a":"markdown"},"source":{"c70e2af7":"# Importing necessary libraries:\n\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nfrom matplotlib import pyplot as plt\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score, roc_auc_score, confusion_matrix, classification_report, plot_roc_curve\nfrom sklearn.model_selection import cross_validate\nfrom sklearn.preprocessing import LabelEncoder, RobustScaler\n\n\n# Setting necessary display options\n\n\npd.set_option('display.max_columns', None)\npd.set_option('display.max_rows', None)\npd.set_option('display.float_format', lambda x: '%.3f' % x)\npd.set_option('display.width', 500)","82e9927b":"# Importing necessary functions:\n\n\ndef outlier_thresholds(dataframe, col_name, q1=0.05, q3=0.95):\n    quartile1 = dataframe[col_name].quantile(q1)\n    quartile3 = dataframe[col_name].quantile(q3)\n    interquantile_range = quartile3 - quartile1\n    up_limit = quartile3 + 1.5 * interquantile_range\n    low_limit = quartile1 - 1.5 * interquantile_range\n    return low_limit, up_limit\n\ndef grab_col_names(dataframe, cat_th=10, car_th=20):\n    cat_cols = [col for col in dataframe.columns if dataframe[col].dtypes == \"O\"]\n    num_but_cat = [col for col in dataframe.columns if dataframe[col].nunique() < cat_th and\n                   dataframe[col].dtypes != \"O\"]\n    cat_but_car = [col for col in dataframe.columns if dataframe[col].nunique() > car_th and\n                   dataframe[col].dtypes == \"O\"]\n    cat_cols = cat_cols + num_but_cat\n    cat_cols = [col for col in cat_cols if col not in cat_but_car]\n    num_cols = [col for col in dataframe.columns if dataframe[col].dtypes != \"O\"]\n    num_cols = [col for col in num_cols if col not in num_but_cat]\n\n    print(f\"Observations: {dataframe.shape[0]}\")\n    print(f\"Variables: {dataframe.shape[1]}\")\n    print(f'cat_cols: {len(cat_cols)}')\n    print(f'num_cols: {len(num_cols)}')\n    print(f'cat_but_car: {len(cat_but_car)}')\n    print(f'num_but_cat: {len(num_but_cat)}')\n    return cat_cols, num_cols, cat_but_car\n\ndef check_outlier(dataframe, col_name):\n    low_limit, up_limit = outlier_thresholds(dataframe, col_name)\n    if dataframe[(dataframe[col_name] > up_limit) | (dataframe[col_name] < low_limit)].any(axis=None):\n        return True\n    else:\n        return False\n\ndef grab_outliers(dataframe, col_name, index=False):\n    low, up = outlier_thresholds(dataframe, col_name)\n\n    if dataframe[((dataframe[col_name] < low) | (dataframe[col_name] > up))].shape[0] > 10:\n        print(\"#####################################################\")\n        print(str(col_name) + \" variable have too much outliers: \" + str(dataframe[((dataframe[col_name] < low) | (dataframe[col_name] > up))].shape[0]))\n        print(\"#####################################################\")\n        print(dataframe[((dataframe[col_name] < low) | (dataframe[col_name] > up))].head(15))\n        print(\"#####################################################\")\n        print(\"Lower threshold: \" + str(low) + \"   Lowest outlier: \" + str(dataframe[col_name].min()) +\n              \"   Upper threshold: \" + str(up) + \"   Highest outlier: \" + str(dataframe[col_name].max()))\n        print(\"#####################################################\")\n    elif (dataframe[((dataframe[col_name] < low) | (dataframe[col_name] > up))].shape[0] < 10) & \\\n            (dataframe[((dataframe[col_name] < low) | (dataframe[col_name] > up))].shape[0] > 0):\n        print(\"#####################################################\")\n        print(str(col_name) + \" variable have less than 10 outlier values: \" + str(dataframe[((dataframe[col_name] < low) | (dataframe[col_name] > up))].shape[0]))\n        print(\"#####################################################\")\n        print(dataframe[((dataframe[col_name] < low) | (dataframe[col_name] > up))])\n        print(\"#####################################################\")\n        print(\"Lower threshold: \" + str(low) + \"   Lowest outlier: \" + str(dataframe[col_name].min()) +\n              \"   Upper threshold: \" + str(up) + \"   Highest outlier: \" + str(dataframe[col_name].max()))\n        print(\"#####################################################\")\n    else:\n        print(\"#####################################################\")\n        print(str(col_name) + \" variable does not have outlier values\")\n        print(\"#####################################################\")\n\n    if index:\n        print(str(col_name) + \" variable's outlier indexes\")\n        print(\"#####################################################\")\n        outlier_index = dataframe[((dataframe[col_name] < low) | (dataframe[col_name] > up))].index\n        return outlier_index\n\ndef replace_with_thresholds(dataframe, variable):\n    low_limit, up_limit = outlier_thresholds(dataframe, variable)\n    dataframe.loc[(dataframe[variable] < low_limit), variable] = low_limit\n    dataframe.loc[(dataframe[variable] > up_limit), variable] = up_limit\n\ndef missing_values_table(dataframe):\n    na_columns = [col for col in dataframe.columns if dataframe[col].isnull().sum() > 0]\n\n    n_miss = dataframe[na_columns].isnull().sum().sort_values(ascending=False)\n    ratio = (dataframe[na_columns].isnull().sum() \/ dataframe.shape[0] * 100).sort_values(ascending=False)\n    missing_df = pd.concat([n_miss, np.round(ratio, 2)], axis=1, keys=['n_miss', 'ratio'])\n    if len(missing_df) > 0:\n        print(missing_df, end=\"\\n\")\n    else:\n        print(\"No NaN values\")\n\ndef rare_analyser(dataframe, target, cat_cols):\n    for col in cat_cols:\n        print(col, \":\", len(dataframe[col].value_counts()))\n        print(pd.DataFrame({\"COUNT\": dataframe[col].value_counts(),\n                            \"RATIO\": dataframe[col].value_counts() \/ len(dataframe),\n                            \"TARGET_MEAN\": dataframe.groupby(col)[target].mean()}), end=\"\\n\\n\\n\")\n\ndef one_hot_encoder(dataframe, categorical_cols, drop_first=True):\n    dataframe = pd.get_dummies(dataframe, columns=categorical_cols, drop_first=drop_first)\n    return dataframe\n\ndef label_encoder(dataframe, binary_col):\n    labelencoder = LabelEncoder()\n    dataframe[binary_col] = labelencoder.fit_transform(dataframe[binary_col])\n    return dataframe\n\ndef rare_encoder(dataframe, rare_perc):\n    temp_df = dataframe.copy()\n\n    rare_columns = [col for col in temp_df.columns if temp_df[col].dtypes == 'O'\n                    and (temp_df[col].value_counts() \/ len(temp_df) < rare_perc).any(axis=None)]\n\n    for var in rare_columns:\n        tmp = temp_df[var].value_counts() \/ len(temp_df)\n        rare_labels = tmp[tmp < rare_perc].index\n        temp_df[var] = np.where(temp_df[var].isin(rare_labels), 'Rare', temp_df[var])\n\n    return temp_df\n","362b656b":"# Importing dataset:\n\ndf_ = pd.read_csv(\"..\/input\/titanicdataset-traincsv\/train.csv\")\ndf = df_.copy()\ndf.head()","866bdb4a":"# Setting column names as .upper():\ndf.columns = [col.upper() for col in df.columns]","bdc4b447":"# Missing values control:\n\nmissing_values_table(df)","ac0aa804":"# FEATURE ENGINEERING\n\n# Creating a new CABIN bool variable by setting 1 for values in CABIN and 0 for NaN values:\ndf[\"NEW_CABIN_BOOL\"] = df[\"CABIN\"].notnull().astype('int')\n\n# We don't need any more the CABIN variable:\ndel df[\"CABIN\"]\n\n# Cagegorizing people by their title in their name:\ndf['NEW_TITLE'] = df.NAME.str.extract('([A-Za-z]+)\\.', expand=False)\n\n# Filling AGE category's NaN values by NEW_TITLE group medians\ndf[\"AGE\"] = df[\"AGE\"].fillna(df.groupby(\"NEW_TITLE\")[\"AGE\"].transform(\"median\"))\n\n# Merging SIBSP and PARCH variables + the value itself to get the size of family:\ndf[\"NEW_FAMILY_SIZE\"] = df[\"SIBSP\"] + df[\"PARCH\"] + 1\n\n# AGE_PCLASS - A variable as indicator of wealth of Passenger:\ndf[\"NEW_AGE_PCLASS\"] = df[\"AGE\"] * df[\"PCLASS\"]\n\n# Finding if passenger embarked alone or not in the ship:\ndf.loc[((df['SIBSP'] + df['PARCH']) > 0), \"NEW_IS_ALONE\"] = \"NO\"\ndf.loc[((df['SIBSP'] + df['PARCH']) == 0), \"NEW_IS_ALONE\"] = \"YES\"\n\n# Categorical AGE variable:\ndf.loc[(df['AGE'] < 18), 'NEW_AGE_CAT'] = 'young'\ndf.loc[(df['AGE'] >= 18) & (df['AGE'] < 56), 'NEW_AGE_CAT'] = 'mature'\ndf.loc[(df['AGE'] >= 56), 'NEW_AGE_CAT'] = 'senior'\n\n# New SEX categoric variable based in SEX and AGE variables:\ndf.loc[(df['SEX'] == 'male') & (df['AGE'] <= 21), 'NEW_SEX_CAT'] = 'youngmale'\ndf.loc[(df['SEX'] == 'male') & ((df['AGE'] > 21) & (df['AGE']) <= 50), 'NEW_SEX_CAT'] = 'maturemale'\ndf.loc[(df['SEX'] == 'male') & (df['AGE'] > 50), 'NEW_SEX_CAT'] = 'seniormale'\ndf.loc[(df['SEX'] == 'female') & (df['AGE'] <= 21), 'NEW_SEX_CAT'] = 'youngfemale'\ndf.loc[(df['SEX'] == 'female') & ((df['AGE'] > 21) & (df['AGE']) <= 50), 'NEW_SEX_CAT'] = 'maturefemale'\ndf.loc[(df['SEX'] == 'female') & (df['AGE'] > 50), 'NEW_SEX_CAT'] = 'seniorfemale'\n\n# NEW_AGE_PCLASS \/ FARE\ndf[\"NEW_AGE_PCLASS_FARE\"] = df[\"NEW_AGE_PCLASS\"] \/ df[\"FARE\"]\ndf['NEW_AGE_PCLASS_FARE'].replace([np.inf], '0', inplace=True)\ndf[\"NEW_AGE_PCLASS_FARE\"] = df[\"NEW_AGE_PCLASS_FARE\"].astype(float)\n\n# Implementing mode to the NaN values of categoric variables, i.e.: EMBARKED.\ndf = df.apply(lambda x: x.fillna(x.mode()[0]) if (x.dtype == \"O\" and len(x.unique()) <= 10) else x, axis=0)\n\n# Grab column names by their types:\n\ncat_cols, num_cols, cat_but_car = grab_col_names(df)\n\n\n# Removing PASSENGERID variable from num_cols:\n\nnum_cols = [col for col in num_cols if \"PASSENGERID\" not in col]","e43fb75c":"# Checking for outliers:\n\nfor col in num_cols:\n    print(col, check_outlier(df, col))","199c690b":"# Detailed control of outliers:\n\nfor col in num_cols:\n    grab_outliers(df, col)","adf610dc":"# Replacing outliers with thresholds:\n\nfor col in num_cols:\n    replace_with_thresholds(df, col)","6e50f6aa":"# Removing unnecessary columns from dataset:\n\nremove_cols = [\"TICKET\", \"NAME\"]\ndf.drop(remove_cols, inplace=True, axis=1)","4eb5535c":"# Label Encoding for columns that aren't int or float and have nunique = 2:\n\nbinary_cols = [col for col in df.columns if df[col].dtype not in [int, float]\n               and df[col].nunique() == 2]\ndf.head()\nfor col in binary_cols:\n    df = label_encoder(df, col)","5304ed0f":"# Rare analyser for dataset:\n\nrare_analyser(df, \"SURVIVED\", cat_cols)","42744bc8":"df = rare_encoder(df, 0.01)","b5c4d5fe":"# One-Hot Encoding for variables with levels not-comparable with each other (nominal levels):\n\nohe_cols = [col for col in df.columns if 10 >= df[col].nunique() > 2]\ndf = one_hot_encoder(df, ohe_cols)\n\n\n# We created new columns with One-Hot Encoding, so we need to grab column names again:\n\ncat_cols, num_cols, cat_but_car = grab_col_names(df)\n\nnum_cols = [col for col in num_cols if \"PASSENGERID\" not in col]","92da0bff":"useless_cols = [col for col in df.columns if df[col].nunique() == 2 and\n                (df[col].value_counts() \/ len(df) < 0.01).any(axis=None)]\n\nuseless_cols","138e9d81":"df.drop(useless_cols, axis=1, inplace=True)","c10236b5":"df.head(10)","18693ef0":"# RobustScaler\n\nrs = RobustScaler()\ndf[num_cols] = rs.fit_transform(df[num_cols])\n\ndf.head(10)","09b4632b":"# Setting dependent and independent columns:\n\ny = df[\"SURVIVED\"]\nX = df.drop([\"PASSENGERID\", \"SURVIVED\"], axis=1)\n\n\n# Model\nlog_model = LogisticRegression().fit(X, y)","57141807":"# b - bias value:\n\nlog_model.intercept_[0]","32b614cf":"# w - weight values:\n\nlog_model.coef_[0]","0a44520e":"# Prediction of the dependent column:\n\ny_pred = log_model.predict(X)","8d0e315a":"def plot_confusion_matrix(y, y_pred):\n    acc = round(accuracy_score(y, y_pred), 2)\n    cm = confusion_matrix(y, y_pred)\n    sns.heatmap(cm, annot=True, fmt=\".0f\", cmap=\"GnBu\")\n    plt.xlabel('y_pred')\n    plt.ylabel('y')\n    plt.title('Accuracy Score: {0}'.format(acc), size=10)\n    plt.show()\n\n    \nplot_confusion_matrix(y, y_pred)","4cc4e204":"print(classification_report(y, y_pred))","4a40b302":"y_prob = log_model.predict_proba(X)[:, 1]\nroc_auc_score(y, y_prob)","204a2b76":"# ROC curve graph:\n\nplot_roc_curve(log_model, X, y, color=\"k\")\nplt.title('ROC Curve')\nplt.plot([0, 1], [0, 1], color=\"b\", marker=\"x\")\nplt.show()","2aa82e25":"# Model Validation: 10-Fold Cross Validation\n\ny = df[\"SURVIVED\"]\nX = df.drop([\"PASSENGERID\", \"SURVIVED\"], axis=1)\nlog_model = LogisticRegression().fit(X, y)\n\n\ncv_results = cross_validate(log_model,\n                            X, y,\n                            cv=10,\n                            scoring=[\"accuracy\", \"precision\", \"recall\", \"f1\", \"roc_auc\"])\n\ndef cv_mean(cv_nums):\n    print(f\"Test Accuracy: {cv_nums['test_accuracy'].mean():.4f}\")\n    print(f\"Test Precision: {cv_nums['test_precision'].mean():.4f}\")\n    print(f\"Test Recall: {cv_nums['test_recall'].mean():.4f}\")\n    print(f\"Test F1: {cv_nums['test_f1'].mean():.4f}\")\n    print(f\"Test ROC AUC: {cv_results['test_roc_auc'].mean():.4f}\")\n\ncv_mean(cv_results)","fbc292cd":"no_effect_list = (\"PASSENGERID\", \"SURVIVED\")\n\ndef variable_importance(dataframe):\n    importance = log_model.coef_[0]\n    imp = pd.DataFrame(importance, columns=[\"Score\"])\n    dff_col_names = [col for col in dataframe.columns if col not in no_effect_list]\n    dff_col_names = pd.DataFrame(dff_col_names, columns=[\"col_names\"])\n    new_df = pd.concat([dff_col_names, imp], axis=1)\n    for i in new_df.index:\n        print(f\"{new_df.values[i][0]} variable's score is: {new_df.values[i][1]:.4f}\")\n\nvariable_importance(df)","73da3ff8":"def plot_importance():\n    feature_imp = pd.DataFrame({\"Value\": log_model.coef_[0], \"Feature\": X.columns})\n    plt.figure(figsize=(10,10))\n    sns.barplot(x=\"Value\", y=\"Feature\", data=feature_imp, color=\"k\")\n    plt.title(\"Variable importance plot map\")\n    plt.grid(axis=\"x\")\n    plt.tight_layout()\n    plt.show()\n\nplot_importance()","21d42a6d":"**Comment:**\n\nThe model is ready, fitted to the independent columns and predicted the dependent value.\n\nNow we need to see the confusion matrix and classification report to comment the accuracy metrics of our model.","333b41b6":"**Comment:**\n\nThe variables above are the ones which represent less then %1 of dataset values. So in case to not complicate our model, we will be removing this variables. This is not mandatory, we would've keep them within our model.","0bbd973a":"**Comment:**\n\nLet's find useless columns after One-Hot Encoding done to dataset (rare-analyser for other variables):","945b7631":"**Comment:**\n\nNow our dataset is ready to me modeled.\n\n# Logistic Regression","73850103":"**Comment:**\n\nThe dataset preparation and feature engineering is done. Now we need to encode variables for the machine learning phase.","cb1c8ea6":"**Comment:**\n\nNow we have new results. These results are not better than the previous ones but are more reliable and transparent. So these results should be considered as right.","4342a7a3":"**Comment:**\n\nNow let's read the classification report as it is the descriptive analyser of confusion matrix.\n\nTrue Positive (TP) - 262\n\nTrue Negative (TN) - 487\n\nFalse Positive (FP) - 62\n\nFalse Negative (FN) - 80\n\n**Precision** - Our model predicted %81 of survivors (1) as survived (1),\n\nso %19 are died (0) but our model predicted them as survived (1).\n\n**Recall** - Our model predicted %77 of survivors (1) with the right decision (1),\n\nso %23 are survived (1) but our model predicted them as died (0).\n\n**F1 Score** - (Harmonic mean of Precision and Recall) is %79.\n\n**Accuracy** - Total right predicted value rate is %84.","8fd57906":"**Comment:**\n\nCABIN variable has too much missing values which might indicate a hidden clue within story. So deleting this might've stop as entirely by evaluating our model; and removing the variable completely would be \"chosing the easy way\" without extracting information from it.\n\nAGE variable also has too much missing values (around %20 of dataset).\n\nEMBARKED has only 2 missing values.\n\n**Now let's generate new variables from the dataset:**\n","dd8ce5e5":"**Comment:**\n\nWe have done Logistic Regression with all the dataset. To get rid of problematic predictor variables in the prediction model as well as influential observations in the validation data that adversely affect the fit of the model, we need to do the model validation.\n\nModel validation can be done with Holdout method or K-fold cross model validation method.\n\nIn this case I'll be using 10-fold cross model validation:","852fee2e":"**Comment:**\n\nAs we can see, some columns are categorized as 0 or 1 and others are numerical, which should be scaled to be generated in the Logistic Regression model.\n\nSo below is the RobustScaler method which is using median and the IQR levels of dataset to scale numerical columns. RobustScaler is used here instead of StandardScaler, because it is not effected by \"possible\" outlier values.","91051fac":"**Comment:**\n\nAs we can see, only FARE variable has only 3 outliers, which if replaced with threshold won't effect the result of the dataset in general.","865b62dd":"**Thank you for your reading!**\n\n**This work has been done with the support of** [VBO](https:\/\/www.veribilimiokulu.com\/), [Miuul](https:\/\/miuul.com\/),  [Vahit Keskin](https:\/\/www.linkedin.com\/in\/vahitkeskin\/), [O\u011fuz Erdo\u011fan](https:\/\/www.linkedin.com\/in\/oguzerdo\/), [Hande K\u00fc\u00e7\u00fckbulut](https:\/\/www.linkedin.com\/in\/hande-kucukbulut\/), [Mehmet Tuzcu](https:\/\/www.linkedin.com\/in\/mehmettuzcu\/) & [Burak Do\u011frul](https:\/\/www.linkedin.com\/in\/burakdogrul\/).","2b053f03":"**Comment:**\n\nThe variables above has many levels. String variables like NEW_TITLE has too much levels which some of them represent less then %1 of dataset. These level can be merged and categorized as RARE with the function below:\n\n*P.S.: Int variables like NEW_FAMILY_SIZE or PARCH will be rare_analyzed with a different function below.*","15acc0c1":"Below you can find also the weight of each variable to the model and the plot of weights:","c6ed32e9":"**Comment:**\n\nLogistic Regression predicts only with one default classification threshold of 0.50. So we need to build Receiver Operation Characteristic Curve (ROC) and calculate the Area Under Curve (AUC) of the model. These should be done because we would be able to do better interpretation of our model accuracy. AUC tells how much the model is capable of distinguishing between classes (different thresholds) and find the largest area under ROC curve with the best possible classification threshold.","9ad75a36":"**Comment:**\n\nNow let's see first the head of our dataset and take necessary steps to continue modelling.","2b41c05a":"# Logistic Regression with Titanic dataset\n\nThe sinking of the Titanic is one of the most famous disasters had happened in turism sector.\n\nOn 15th April of 1912, during Titanic's voyage, the very much considered as \u201cunsinkable\u201d, Titanic sank after collided with an iceberg. Titanic was well equipped, but unfortunately there were not enough lifeboats for everyone. This collision resulted in the death of 1502 out of 2224 passengers and crew.\n\nThere was luck in surviving of travelers onboard, but data is showing us some critical differences between people which survived and died. The model below will be evaluating a Logistic Regression which will help us find the weights of variables who led to survive and other variables which led to the death.\n\n**Variables:**\n\nSurvived - Survived (1) or Died (0)\n\nPclass - Passenger's class\n\nName - Passenger's name\n\nSex - Passenger's sex \n\nAge - Passenger's age\n\nSibSp - Number of siblings\/spouses abroad \n\nParch - Number of parents\/children abroad\n\nTicket - Ticket number\n\nFare - Fare\n\nCabin - Cabin\n\nEmbarked - Port of embarkation"}}