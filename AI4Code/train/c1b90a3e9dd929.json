{"cell_type":{"0f45b879":"code","5400752b":"code","3a195848":"code","3d338d40":"code","5b5e6335":"code","67d349e4":"code","ab435d78":"code","b5526206":"code","5187b760":"code","7f75608f":"code","3480290b":"code","54f4d397":"code","d9885b63":"code","819c777d":"code","404dc36f":"code","0de264b2":"code","c4b02810":"code","6d8c8e8a":"code","d0f6cdfa":"code","b152c5ad":"code","ca4df263":"code","3a4d8b46":"code","fe81f803":"code","ba2e5387":"code","04b35893":"code","2db4dfc2":"code","4bb7c826":"code","cd726bb7":"code","d9cb0b93":"code","15d5e4d9":"code","6926c5c9":"code","0d9ceac9":"code","cc592552":"code","b3eace90":"code","e133a4bb":"code","da48ff1c":"code","d1799b73":"code","221c8d25":"code","2efeb0bd":"code","5d3d5d25":"markdown","4d49a571":"markdown","b7133419":"markdown","7284545e":"markdown","43f96b19":"markdown","239ae2b5":"markdown","50031c92":"markdown","d2d517bf":"markdown","8f872874":"markdown","d31f12d6":"markdown","2feca5d6":"markdown","2079c01c":"markdown","026b4083":"markdown","9ff41e1f":"markdown","16f3f211":"markdown","029e0c84":"markdown","81b3e58f":"markdown","97b811df":"markdown"},"source":{"0f45b879":"import pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nimport statsmodels.formula.api as sm\nimport scipy.stats as stats\nimport pandas_profiling   #need to install using anaconda prompt (pip install pandas_profiling)\n\n%matplotlib inline\nplt.rcParams['figure.figsize'] = 10, 7.5\nplt.rcParams['axes.grid'] = True\nplt.gray()\n\nfrom matplotlib.backends.backend_pdf import PdfPages\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn import metrics\nfrom sklearn.linear_model import LogisticRegression\nfrom statsmodels.stats.outliers_influence import variance_inflation_factor\nfrom patsy import dmatrices","5400752b":"bankloans=pd.read_csv('..\/input\/bankloans.csv')","3a195848":"len(bankloans)","3d338d40":"## Generic functions for data explorations\ndef var_summary(x):\n    return pd.Series([x.count(), x.isnull().sum(), x.sum(), x.mean(), x.median(),  x.std(), x.var(), x.min(), x.dropna().quantile(0.01), x.dropna().quantile(0.05),x.dropna().quantile(0.10),x.dropna().quantile(0.25),x.dropna().quantile(0.50),x.dropna().quantile(0.75), x.dropna().quantile(0.90),x.dropna().quantile(0.95), x.dropna().quantile(0.99),x.max()], \n                  index=['N', 'NMISS', 'SUM', 'MEAN','MEDIAN', 'STD', 'VAR', 'MIN', 'P1' , 'P5' ,'P10' ,'P25' ,'P50' ,'P75' ,'P90' ,'P95' ,'P99' ,'MAX'])\n\n\ndef cat_summary(x):\n    return pd.Series([x.count(), x.isnull().sum(), x.value_counts()], \n                  index=['N', 'NMISS', 'ColumnsNames'])\n\ndef create_dummies( df, colname ):\n    col_dummies = pd.get_dummies(df[colname], prefix=colname)\n    col_dummies.drop(col_dummies.columns[0], axis=1, inplace=True)\n    df = pd.concat([df, col_dummies], axis=1)\n    df.drop( colname, axis = 1, inplace = True )\n    return df\n\n#Handling outliers\ndef outlier_capping(x):\n    x = x.clip_upper(x.quantile(0.99))\n    x = x.clip_lower(x.quantile(0.01))\n    return x\n\ndef Missing_imputation(x):\n    x = x.fillna(x.mean())\n    return x\n","5b5e6335":"bankloans.apply(lambda x: var_summary(x)).T\n","67d349e4":"bankloans_existing = bankloans[bankloans.default.isnull()==0]\nbankloans_new = bankloans[bankloans.default.isnull()==1]","ab435d78":"bankloans_existing=bankloans_existing.apply(lambda x: outlier_capping(x))\nbankloans_existing=bankloans_existing.apply(lambda x: Missing_imputation(x))","b5526206":"numeric_var_names=[key for key in dict(bankloans.dtypes) if dict(bankloans.dtypes)[key] in ['float64', 'int64', 'float32', 'int32']]\ncat_var_names=[key for key in dict(bankloans.dtypes) if dict(bankloans.dtypes)[key] in ['object']]","5187b760":"sns.heatmap(bankloans_existing.corr())","7f75608f":"bp = PdfPages('BoxPlots with default Split.pdf')\n\nfor num_variable in numeric_var_names:\n    fig,axes = plt.subplots(figsize=(10,4))\n    sns.boxplot(x='default', y=num_variable, data = bankloans_existing)\n    bp.savefig(fig)\nbp.close()","3480290b":"tstats_df = pd.DataFrame()\nfor num_variable in bankloans_existing.columns.difference(['default']):\n    tstats=stats.ttest_ind(bankloans_existing[bankloans_existing.default==1][num_variable],bankloans_existing[bankloans_existing.default==0][num_variable])\n    temp = pd.DataFrame([num_variable, tstats[0], tstats[1]]).T\n    temp.columns = ['Variable Name', 'T-Statistic', 'P-Value']\n    tstats_df = pd.concat([tstats_df, temp], axis=0, ignore_index=True)\nprint(tstats_df)","54f4d397":"for num_variable in numeric_var_names:\n    fig,axes = plt.subplots(figsize=(10,4))\n    #sns.distplot(hrdf[num_variable], kde=False, color='g', hist=True)\n    sns.distplot(bankloans_existing[bankloans_existing['default']==0][num_variable], label='Not Default', color='b', hist=True, norm_hist=False)\n    sns.distplot(bankloans_existing[bankloans_existing['default']==1][num_variable], label='Default', color='r', hist=True, norm_hist=False)\n    plt.xlabel(str(\"X variable \") + str(num_variable) )\n    plt.ylabel('Density Function')\n    plt.title(str('Default Split Density Plot of ')+str(num_variable))\n    plt.legend()","d9885b63":"bp = PdfPages('Transformation Plots.pdf')\n\nfor num_variable in bankloans_existing.columns.difference(['default']):\n    binned = pd.cut(bankloans_existing[num_variable], bins=10, labels=list(range(1,11)))\n    binned = binned.dropna()\n    ser = bankloans_existing.groupby(binned)['default'].sum() \/ (bankloans_existing.groupby(binned)['default'].count()-bankloans_existing.groupby(binned)['default'].sum())\n    ser = np.log(ser)\n    fig,axes = plt.subplots(figsize=(10,4))\n    sns.barplot(x=ser.index,y=ser)\n    plt.ylabel('Log Odds Ratio')\n    plt.title(str('Logit Plot for identifying if the bucketing is required or not for variable ') + str(num_variable))\n    bp.savefig(fig)\n\nbp.close()","819c777d":"print('These variables need bucketing - creddebt, othdebt, debtinc, employ, income ')\nbankloans_existing.columns","404dc36f":"bankloans_existing[['creddebt', 'othdebt', 'debtinc', 'employ','income' ]].describe(percentiles=[0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9]).T","0de264b2":"features = \"+\".join(bankloans_existing.columns.difference(['default']))\na,b = dmatrices(formula_like='default ~ '+ features, data = bankloans_existing, return_type='dataframe')\n\nvif = pd.DataFrame()\nvif[\"VIF Factor\"] = [variance_inflation_factor(b.values, i) for i in range(b.shape[1])]\nvif[\"features\"] = b.columns\n\nprint(vif)","c4b02810":"train_features = bankloans_existing.columns.difference(['default'])\ntrain_X, test_X = train_test_split(bankloans_existing, test_size=0.3, random_state=42)\ntrain_X.columns","6d8c8e8a":"logreg = sm.logit(formula='default ~ ' + \"+\".join(train_features), data=train_X)\nresult = logreg.fit()\nsumm = result.summary()\nsumm","d0f6cdfa":"AUC = metrics.roc_auc_score(train_X['default'], result.predict(train_X))\n\nprint('AUC is -> ' + str(AUC))","b152c5ad":"train_gini = 2*metrics.roc_auc_score(train_X['default'], result.predict(train_X)) - 1\nprint(\"The Gini Index for the model built on the Train Data is : \", train_gini)\n\ntest_gini = 2*metrics.roc_auc_score(test_X['default'], result.predict(test_X)) - 1\nprint(\"The Gini Index for the model built on the Test Data is : \", test_gini)","ca4df263":"train_predicted_prob = pd.DataFrame(result.predict(train_X))\ntrain_predicted_prob.columns = ['prob']\ntrain_actual = train_X['default']\n# making a DataFrame with actual and prob columns\ntrain_predict = pd.concat([train_actual, train_predicted_prob], axis=1)\ntrain_predict.columns = ['actual','prob']\n\ntest_predicted_prob = pd.DataFrame(result.predict(test_X))\ntest_predicted_prob.columns = ['prob']\ntest_actual = test_X['default']\n# making a DataFrame with actual and prob columns\ntest_predict = pd.concat([test_actual, test_predicted_prob], axis=1)\ntest_predict.columns = ['actual','prob']\n\n## Intuition behind ROC curve - predicted probability as a tool for separating the '1's and '0's\ndef cut_off_calculation(result,train_X,train_predict):\n    \n    roc_like_df = pd.DataFrame()\n    train_temp = train_predict.copy()\n\n    for cut_off in np.linspace(0,1,50):\n        train_temp['cut_off'] = cut_off\n        train_temp['predicted'] = train_temp['prob'].apply(lambda x: 0.0 if x < cut_off else 1.0)\n        train_temp['tp'] = train_temp.apply(lambda x: 1.0 if x['actual']==1.0 and x['predicted']==1 else 0.0, axis=1)\n        train_temp['fp'] = train_temp.apply(lambda x: 1.0 if x['actual']==0.0 and x['predicted']==1 else 0.0, axis=1)\n        train_temp['tn'] = train_temp.apply(lambda x: 1.0 if x['actual']==0.0 and x['predicted']==0 else 0.0, axis=1)\n        train_temp['fn'] = train_temp.apply(lambda x: 1.0 if x['actual']==1.0 and x['predicted']==0 else 0.0, axis=1)\n        sensitivity = train_temp['tp'].sum() \/ (train_temp['tp'].sum() + train_temp['fn'].sum())\n        specificity = train_temp['tn'].sum() \/ (train_temp['tn'].sum() + train_temp['fp'].sum())\n        roc_like_table = pd.DataFrame([cut_off, sensitivity, specificity]).T\n        roc_like_table.columns = ['cutoff', 'sensitivity', 'specificity']\n        roc_like_df = pd.concat([roc_like_df, roc_like_table], axis=0)\n    return roc_like_df\n\nroc_like_df = cut_off_calculation(result,train_X,train_predict)","3a4d8b46":"## Finding ideal cut-off for checking if this remains same in OOS validation\nroc_like_df['total'] = roc_like_df['sensitivity'] + roc_like_df['specificity']\nroc_like_df[roc_like_df['total']==roc_like_df['total'].max()]","fe81f803":"train_predict['predicted'] = train_predict['prob'].apply(lambda x: 1 if x > 0.24 else 0)\nsns.heatmap(pd.crosstab(train_predict['actual'], train_predict['predicted']), annot=True, fmt='.0f')\nplt.title('Train Data Confusion Matrix')\nplt.show()\n\ntest_predict['predicted'] = test_predict['prob'].apply(lambda x: 1 if x > 0.24 else 0)\nsns.heatmap(pd.crosstab(test_predict['actual'], test_predict['predicted']), annot=True, fmt='.0f')\nplt.title('Train Data Confusion Matrix')\nplt.show()\n\n# (117+236)\/(117+236+120+17)","ba2e5387":"print(\"The overall accuracy score for the Train Data is : \", metrics.accuracy_score(train_predict.actual, train_predict.predicted))\nprint(\"The overall accuracy score for the Test Data  is : \", metrics.accuracy_score(test_predict.actual, test_predict.predicted))","04b35893":"train_predict['Deciles']=pd.qcut(train_predict['prob'],10, labels=False)\n#test['Deciles']=pd.qcut(test['prob'],10, labels=False)\ntrain_predict.head()","2db4dfc2":"df = train_predict[['Deciles','actual']].groupby(train_predict.Deciles).sum().sort_index(ascending=False)","4bb7c826":"df","cd726bb7":"train_features = bankloans_existing.columns.difference(['default'])\ntrain_sk_X,test_sk_X, train_sk_Y ,test_sk_Y = train_test_split(bankloans_existing[train_features],bankloans_existing['default'], test_size=0.3, random_state=42)\ntrain_sk_X.columns","d9cb0b93":"logisticRegr = LogisticRegression()\nlogisticRegr.fit(train_sk_X, train_sk_Y)\n","15d5e4d9":"#Predicting the test cases\ntrain_pred = pd.DataFrame({'actual':train_sk_Y,'predicted':logisticRegr.predict(train_sk_X)})\ntrain_pred = train_pred.reset_index()\ntrain_pred.drop(labels='index',axis=1,inplace=True)","6926c5c9":"train_gini = 2*metrics.roc_auc_score(train_sk_Y, logisticRegr.predict(train_sk_X)) - 1\nprint(\"The Gini Index for the model built on the Train Data is : \", train_gini)\n\ntest_gini = 2*metrics.roc_auc_score(test_sk_Y, result.predict(test_sk_X)) - 1\nprint(\"The Gini Index for the model built on the Test Data is : \", test_gini)","0d9ceac9":"predict_proba_df = pd.DataFrame(logisticRegr.predict_proba(train_sk_X))\nhr_test_pred = pd.concat([train_pred,predict_proba_df],axis=1)\nhr_test_pred.columns=['actual','predicted','Left_0','Left_1']","cc592552":"auc_score = metrics.roc_auc_score( hr_test_pred.actual, hr_test_pred.Left_1  )\nround( float( auc_score ), 2 )","b3eace90":"# Finding the optimal cutoff probability\nfpr, tpr, thresholds = metrics.roc_curve( hr_test_pred.actual,hr_test_pred.Left_1,drop_intermediate=False )\nplt.figure(figsize=(6, 4))\nplt.plot( fpr, tpr, label='ROC curve (area = %0.2f)' % auc_score )\nplt.plot([0, 1], [0, 1], 'k--')\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.xlabel('False Positive Rate or [1 - True Negative Rate]')\nplt.ylabel('True Positive Rate')\nplt.title('Receiver operating characteristic example')\nplt.legend(loc=\"lower right\")\nplt.show()","e133a4bb":"cutoff_prob = thresholds[(np.abs(tpr - 0.72)).argmin()]","da48ff1c":"cutoff_prob","d1799b73":"hr_test_pred['new_labels'] = hr_test_pred['Left_1'].map( lambda x: 1 if x >= 0.36 else 0 )","221c8d25":"print(\"The overall accuracy score for the Train Data is : \", round(metrics.accuracy_score(train_sk_Y, logisticRegr.predict(train_sk_X)),2))\nprint(\"The overall accuracy score for the Test Data is : \", round(metrics.accuracy_score(test_sk_Y, logisticRegr.predict(test_sk_X)),2))","2efeb0bd":"# Creating a confusion matrix\n\nfrom sklearn import metrics\n\ncm_train = metrics.confusion_matrix(hr_test_pred['actual'],\n                            hr_test_pred['new_labels'], [1,0] )\nsns.heatmap(cm_train,annot=True, fmt='.0f')\nplt.title('Train Data Confusion Matrix')\nplt.show()","5d3d5d25":"### Data Exploratory Analysis\n- Bivariate Analysis - Numeric(TTest) and Categorical(Chisquare)<br>\n\nBelow ttest is performed to check the relationship between the independent samples in respect to the dependent variables. This helps us to identify if the variable is biased towards any given segment of the dependent variables. \n\nOur H0 Hypothesis is that the independent samples of num_variable is not biased \/ related to the dependent variable i.e. default. However if Pvalue is higher then the variables are independent and are not significant for the model. We may exclue some of them based on pvalue and some other statistics that we will calculate below\n","4d49a571":"# Decile Analysis","b7133419":"Credit scoring is perhaps one of the most \"classic\" applications for predictive\nmodeling, to predict whether or not credit extended to an applicant will likely\nresult in profit or losses for the lending institution. There are many variations\nand complexities regarding how exactly credit is extended to individuals,\nbusinesses, and other organizations for various purposes (purchasing\nequipment, real estate, consumer items, and so on), and using various\nmethods of credit (credit card, loan, delayed payment plan). But in all cases, a\nlender provides money to an individual or institution, and expects to be paid\nback in time with interest commensurate with the risk of default.\nCredit scoring is the set of decision models and their underlying techniques\nthat aid lenders in the granting of consumer credit. These techniques\ndetermine who will get credit, how much credit they should get, and what\noperational strategies will enhance the profitability of the borrowers to the\nlenders. Further, they help to assess the risk in lending. Credit scoring is a\ndependable assessment of a person\u2019s credit worthiness since it is based on\nactual data.<>\n\nA lender commonly makes two types of decisions: first, whether to grant credit\nto a new applicant, and second, how to deal with existing applicants, including\nwhether to increase their credit limits. In both cases, whatever the techniques\nused, it is critical that there is a large sample of previous customers with their\napplication details, behavioral patterns, and subsequent credit history\navailable. Most of the techniques use this sample to identify the connection\nbetween the characteristics of the consumers (annual income, age, number of\nyears in employment with their current employer, etc.) and their subsequent\nhistory.\n\nTypical application areas in the consumer market include: credit cards, auto\nloans, home mortgages, home equity loans, mail catalog orders, and a wide\nvariety of personal loan products. ","7284545e":"Above density graphs depicts that the otherdebts,credebts,ed doesnt have any considerable impact on the customer default situation in respect to the other variables","43f96b19":"## Hence with the decile analysis the model looks fine statistically as with the reducing probabilities we are also seeing the reducing actual default.\n\n\n\n\n## Implement using logistic regression using sklearn","239ae2b5":"### Visualization of variable importance","50031c92":"Seeing the above data there are <b>few outliers<\/b> in the dataset in moving from 99 to 100 percentile.<\/br>\nIn practise its better to cap the outliers\n\nHowever there are <b>missing values for the dependent variables<\/b>, However the missing values means that these are the <b>new customers<\/b> for whom we need to evaluate if we need to provide loans or not. Hence we will be building the model with <b>existing customers<\/b> only.\n","d2d517bf":"<b> Data Pre-Processing - <\/b>\n- Missing Values Treatment - Numerical (Mean\/Median imputation) and Categorical (Separate Missing Category or Merging)\n- Univariate Analysis - Outlier and Frequency Analysis","8f872874":"Lets evalulate the above plots with the different variables and see if they make sense with fetching some information.\n\nAge - It depicts that people with lower age group tends to default more than with higher age group<br>\ncreddebt  - People with higher creddebt tends to default more often which is expected in normal circumstances.<br>\naddress - This depicts that people who are living at the same location for less duration tends to default more.<br>\ndebtinc - People with higher debtinc ratio tends to default more often which is expected in normal circumstances.<br>\nemply - This depicts that people who are employed more recently tends to default more in comparison to the senior people.<br>\n\nWell these interpretations confirms that some of the basic assumptions are being followed with the given dataset.","d31f12d6":"<b>Dependent variable<\/b> - default <br>\n<b>Independent Variable<\/b> - age\ted\temploy\taddress\tincome\tdebtinc\tcreddebt\tothdebt","2feca5d6":"### Variable Transformation: Bucketing","2079c01c":"This shows that age,ed,income and otherdebt are comparatively insignifincat but can not be ignored on the basis on Ttest. We need to evaluate them with Somers'D to be sure to remove them from our model","026b4083":"The above correlation depicts that there is very strong correlation between default ~ employ,address, income\nAlso debtinc shows very strong relationship with income, and is expected as the same is derived variable from income and debt.\n\nLets try to understand the impact of each independent variable on the dependent variable with whiskers plot","9ff41e1f":"### It seems that age,ed  are not significant for medelling as their VIF is also low and Pvalue was high","16f3f211":"###### Definition of Target and Outcome Window:\nOne of the leading banks would like to predict bad customer while customer applying for loan. This model also called as PD Models (Probability of Default)\n\n\n###### Data Pre-Processing - \n    - Missing Values Treatment - Numerical (Mean\/Median imputation) and Categorical (Separate Missing Category or Merging)\n    - Univariate Analysis - Outlier and Frequency Analysis\n###### Data Exploratory Analysis\n    - Bivariate Analysis - Numeric(TTest) and Categorical(Chisquare)\n    - Bivariate Analysis - Visualization\n    - Variable Transformation - P-Value based selection\n    - Variable Transformation - Bucketing \/ Binning for numerical variables and Dummy for Categorical Variables\n    - Variable Reduction - IV \/ Somers'D\n    - Variable Reduction - Multicollinearity\n###### Model Build and Model Diagnostics\n    - Train and Test split\n    - Significance of each Variable\n    - Gini and ROC \/ Concordance analysis - Rank Ordering\n    - Classification Table Analysis - Accuracy\n    - H-L Test for Accuracy by segments (Not done in this notebook)\n###### Model Validation\n    - OOS validation - p-value and sign testing for the model coefficients\n    - Diagnostics check to remain similar to Training Model build\n    - BootStrapping, if necessary\n###### Model Interpretation for its properties\n    - Inferencing for finding the most important contributors\n    - Prediction of risk and proactive prevention by targeting segments of the population","029e0c84":"## Implementing the logistic regression via Statistical model","81b3e58f":"<b>DATA AVAILABLE<\/b>: Bankloans.csv\nThe data contains the credit details about credit borrowers:\nData Description:<br>\n\nage - Age of Customer<br>\ned - Eductation level of customer <br>\nemploy: Tenure with current employer (in years) <br>\naddress: Number of years in same address <br>\nincome: Customer Income <br>\ndebtinc: Debt to income ratio <br>\ncreddebt: Credit to Debt ratio <br>\nothdebt: Other debts <br>\ndefault: Customer defaulted in the past (1= defaulted, 0=Never defaulted) <br>","97b811df":"## Credit Risk Analysis - \n\nOne of the leading banks would like to predict bad customer while customer applying for loan. This model also called as PD Models (Probability of Default)"}}