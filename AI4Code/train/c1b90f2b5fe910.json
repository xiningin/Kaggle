{"cell_type":{"a485a370":"code","a75f88f7":"code","e2d6328e":"code","29c5b42c":"code","5b1869a7":"code","c1b985f8":"code","2089e6ee":"code","ff7f4f4a":"code","885e7d25":"code","7dc824f7":"code","ae308bf6":"code","56203095":"code","c184575f":"code","e0591b73":"code","7e6d5a9c":"code","24b124b6":"code","cf754b59":"code","30c2a194":"code","cf5bd986":"code","031cebf8":"code","a34d813c":"code","4ec624ef":"code","4c69446c":"code","a1af415b":"code","a59a28a2":"markdown","1feb385c":"markdown","3b7ea77b":"markdown","512fddca":"markdown","569e6320":"markdown","5a779f28":"markdown","ff934b01":"markdown","9044e55e":"markdown","806d1e0c":"markdown","4ec1730b":"markdown","3ace93d4":"markdown"},"source":{"a485a370":"import os\nimport re\nimport numpy as np \nimport pandas as pd \n\nfrom nltk.corpus import stopwords\nfrom nltk.tokenize import word_tokenize\nfrom nltk.stem import WordNetLemmatizer\n\nfrom sklearn.model_selection import train_test_split, GridSearchCV\nfrom sklearn.preprocessing import StandardScaler, LabelEncoder\nfrom sklearn.base import TransformerMixin, BaseEstimator\nfrom sklearn.pipeline import Pipeline, FeatureUnion\nfrom sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\nfrom sklearn.metrics import accuracy_score, f1_score, log_loss\nfrom sklearn.feature_extraction.text import (CountVectorizer, \n                                             TfidfVectorizer, \n                                             TfidfTransformer)\n\nimport tensorflow as tf\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom tensorflow.keras import layers","a75f88f7":"test_df = pd.read_csv('\/kaggle\/input\/nlp-getting-started\/test.csv')\n\ntrain_df = pd.read_csv('\/kaggle\/input\/nlp-getting-started\/train.csv')\ntrain_df.head()","e2d6328e":"# Check the shape of the training and test data sets\nprint(\"Shape of training data:\", train_df.shape)\nprint(\"Shape of test data:\", test_df.shape)","29c5b42c":"# Check the number of null values in the training data\ntrain_df.isnull().sum()","5b1869a7":"# Check the number of null values in the test data\ntest_df.isnull().sum()","c1b985f8":"train_df.drop('location', axis = 1, inplace = True)\ntest_df.drop('location', axis = 1, inplace = True)","2089e6ee":"# Display few tweets from training data set\nprint(train_df['text'][:10].values)","ff7f4f4a":"# substitute url with a placeholder in the text messages\nurl_regex = 'http[s]?:\/\/(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+'\n\ndef url_holder(text):\n    \"\"\"\n    This function will receive text message as input and will\n    substitute url with a placeholder\n    \n    Args: \n        text: text message \n    \"\"\"\n    # find all the urls in the text message\n    detected_url = re.findall(url_regex, text)\n    \n    # iterate over the urls and substitute with the placeholder\n    for url in detected_url:\n        text = text.replace(url, 'url')\n    \n    return text","885e7d25":"def add_features(df):\n    \"\"\"\n    This function will create additional features to improve the performace\n    of the model. Features such as length of the message, number of words, \n    number of non stopwords and average word length in each message will be\n    created by this method.\n    \n    Args: \n        df: original dataframe\n        \n    Returns:\n        df: dataframe with new added features\n    \"\"\"\n    # create a set of stopwords\n    StopWords = set(stopwords.words('english'))\n    \n    # substitute url with the placeholder in the text message\n    train_df['text'] = train_df['text'].apply(url_holder)\n    \n    # lowering and removing punctuation\n    df['processed_text'] = df['text'].apply(lambda x: re.sub(r'[^\\w\\s]', '', x.lower()))\n    \n    # apply lemmatization\n    df['processed_text'] = df['processed_text'].apply(\n        lambda x: ' '.join([WordNetLemmatizer().lemmatize(token) for token in x.split()]))\n    \n    # get length of the message\n    df['length'] = df['processed_text'].apply(lambda x: len(x))\n    \n    # get number of words in each message\n    df['num_words'] = df['processed_text'].apply(lambda x: len(x.split()))\n    \n    # get the number of non stopwords in each message\n    df['non_stopwords'] = df['processed_text'].apply(\n        lambda x: len([t for t in x.split() if t not in StopWords]))\n    \n    # get the average word length\n    df['avg_word_len'] = df['processed_text'].apply(\n        lambda x: np.mean([len(t) for t in x.split() if t not in StopWords]) \\\n        if len([len(t) for t in x.split() if t not in StopWords]) > 0 else 0)\n    \n    # update stop words (didn't want to remove negation)\n    StopWords = StopWords.difference(\n        [\"aren't\", 'nor', 'not', 'no', \"isn't\", \"couldn't\", \"hasn't\", \"hadn't\", \"haven't\",\n         \"didn't\", \"doesn't\", \"wouldn't\", \"can't\"])\n    \n    # remove stop words from processed text message\n    df['processed_text'] = df['processed_text'].apply(\n        lambda x: ' '.join([token for token in x.split() if token not in StopWords]))\n        \n    # filter the words with length > 2\n    df['processed_text'] = df['processed_text'].apply(\n        lambda x: ' '.join([token for token in x.split() if len(token) > 2]))\n    \n    return df","7dc824f7":"train_df = add_features(train_df)\ntrain_df.head()","ae308bf6":"# display few processed text messages\nprint(train_df['processed_text'][:10].values)","56203095":"# fill null values with the most frequent in keyword column\ntrain_df['keyword'] = train_df['keyword'].fillna('0')\ntest_df['keyword'] = test_df['keyword'].fillna('0')","c184575f":"x_train, x_val, y_train, y_val = train_test_split(train_df.drop('target', axis = 1).iloc[:, 1:],\n                                                  train_df['target'].values,\n                                                  test_size = 0.2, \n                                                  stratify = train_df['target'].values, \n                                                  random_state = 42)\n\n# print the shape of the training and validation sets\nprint(f'x_train shape: {x_train.shape}\\ny_train shape: {y_train.shape}')\nprint(f'x_val shape: {x_val.shape}\\ny_val shape: {y_val.shape}')","e0591b73":"class TextColumnSelector(BaseEstimator, TransformerMixin):\n    \"\"\"\n    Transformer to select a single column from the data frame to perform additional transformations.\n    This class will select columns containing text data.\n    \"\"\"\n    def __init__(self, key):\n        self.key = key\n        \n    def fit(self, X, y = None):\n        return self\n    \n    def transform(self, X):\n        return X[self.key]\n    \n    \n\nclass NumColumnSelector(BaseEstimator, TransformerMixin):\n    \"\"\"\n    Transformer to select a single column from the data frame to perform additional transformations.\n    This class will select the columns containing numeric data.\n    \"\"\"\n    def __init__(self, key):\n        self.key = key\n    \n    def fit(self, X, y = None):\n        return self\n    \n    def transform(self, X):\n        return X[[self.key]]","7e6d5a9c":"# create separate pipelines to process individual features\n\n# pipeline to process num_words column\nnum_words = Pipeline([\n    ('selector', NumColumnSelector(key = 'num_words')),\n    ('scaler', StandardScaler())\n])\n\n# pipeline to process non_stopwords column\nnum_non_stopwords = Pipeline([\n    ('selector', NumColumnSelector(key = 'non_stopwords')),\n    ('scaler', StandardScaler())\n])\n\n# pipeline to process avg_word_len column\navg_word_length = Pipeline([\n    ('selector', NumColumnSelector(key = 'avg_word_len')),\n    ('scaler', StandardScaler())\n])\n\n# pipeline to process processed_text column\nmessage_processing = Pipeline([\n    ('selecor', TextColumnSelector(key = 'processed_text')),\n    ('tfidf', TfidfVectorizer())\n])\n\n\n# pipeline to process length column\nlength = Pipeline([\n    ('selector', NumColumnSelector(key = 'length')),\n    ('scaler', StandardScaler())\n])\n\n\n# pipeline to process keyword column\ncounter = Pipeline([\n    ('selector', TextColumnSelector(key = 'keyword')),\n    ('counter', CountVectorizer())\n])","24b124b6":"# process all the pipelines in parallel using feature union\nfeature_union = FeatureUnion([\n    ('num_words', num_words),\n    ('num_non_stopwords', num_non_stopwords),\n    ('avg_word_length', avg_word_length),\n    ('message_processing', message_processing),\n    ('length', length),\n    ('counter', counter)\n])\n\n\n# create final pipeline to train the classifier\nfinal_pipeline = Pipeline([\n    ('feature_union', feature_union),\n    ('clf', RandomForestClassifier())\n])\n\n# fit the pipeline on trainig data\nfinal_pipeline.fit(x_train, y_train)","cf754b59":"# calculate accuracy on validation data\ny_pred = final_pipeline.predict(x_val)\nprint(f'Accuracy on validation data: {accuracy_score(y_val, y_pred)}')","30c2a194":"# get the parameters of final pipeline\nfinal_pipeline.get_params().keys()","cf5bd986":"# prepare dictionary of parameters\nparameters = {'feature_union__message_processing__tfidf__max_df': [0.5, 0.75, 1.0],\n              'feature_union__message_processing__tfidf__ngram_range': [(1, 1), (1, 2)],\n              'feature_union__message_processing__tfidf__use_idf': [True, False],\n              'clf__n_estimators': [200, 400],\n              'clf__max_features': ['auto', 'sqrt', 'log2'],\n             }\n\n\n# create GridSearchCV object\ngrid_cv = GridSearchCV(final_pipeline, parameters, cv = 5, n_jobs = -1)\n\n# Fit and tune the model\ngrid_cv.fit(x_train, y_train)","031cebf8":"# display the best parameters\ngrid_cv.best_params_","a34d813c":"# refitting on entire training data using best settings\ngrid_cv.refit\n\n# calculate accuracy on validation data\ny_pred = grid_cv.predict(x_val)\nprint(f'Accuracy on validation data: {accuracy_score(y_val, y_pred)}')","4ec624ef":"test_df = add_features(test_df)\ntest_df.head()","4c69446c":"# extract test features\ntest_features = test_df.iloc[:, 1:]\ntest_preds = grid_cv.predict(test_features)\n\n# create submission file\nsubmission = {'id': test_df.id, 'target': test_preds}\nsubmission = pd.DataFrame(submission)\nsubmission.head()","a1af415b":"# save as csv file\nsubmission.to_csv('pipeline.csv', index = None)","a59a28a2":"### Evaluate performance on validation data","1feb385c":"### Add some new features to the data","3b7ea77b":"### Drop location column as it contains a lot of null values","512fddca":"### Hyperparameter tuning","569e6320":"### Feature Union to combine data processing","5a779f28":"## Load training and test data sets","ff934b01":"### Split data into train and validation sets","9044e55e":"### Scikit Learn Pipeline","806d1e0c":"## Import Section","4ec1730b":"### Prepare test data for making predictions","3ace93d4":"### Clean text by removing\n- Hash tag\n- Punctuation marks\n- Lower case all the text "}}