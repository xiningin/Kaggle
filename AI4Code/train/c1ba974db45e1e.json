{"cell_type":{"a8ed3618":"code","190f9499":"code","0760e661":"code","a02cccc0":"code","bb49fdaf":"code","43429a99":"code","c2ef26a1":"code","c7bf4bc5":"code","29ea7052":"code","dacdc94a":"code","d016236d":"code","07532061":"code","a7b73cf9":"code","8793be67":"code","e8c3005d":"code","22e478e1":"code","5a7b96e8":"code","44ba6a5f":"code","6f6748e0":"code","f631e030":"code","9a3ce89f":"code","a9ba2249":"code","23393ed0":"markdown","7e389fc2":"markdown","d36b754d":"markdown","514286e4":"markdown","9d187965":"markdown","aec36a04":"markdown","1e296f38":"markdown","9b18d928":"markdown","898b8476":"markdown"},"source":{"a8ed3618":"# Import packages\nimport re, math, pathlib, numbers, functools, IPython, graphviz\nimport numpy as np \nimport pandas as pd\nimport altair as alt\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport category_encoders as ce\nfrom pathlib import Path\nfrom sklearn import metrics, ensemble, model_selection, tree\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.preprocessing import OrdinalEncoder, OneHotEncoder, LabelEncoder, StandardScaler, FunctionTransformer\nfrom sklearn.impute import SimpleImputer, MissingIndicator\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.tree import export_graphviz\n\n# Settings\npd.options.display.max_columns=100\npd.options.display.max_rows=100\n%config InlineBackend.figure_format = 'retina'","190f9499":"# Read data\npath = Path('\/kaggle\/input\/house-prices-advanced-regression-techniques\/')\ntrain = pd.read_csv(path\/'train.csv')\ntest = pd.read_csv(path\/'test.csv')\nsample = pd.read_csv(path\/'sample_submission.csv')\ntrain.SalePrice = np.log(train.SalePrice)","0760e661":"def newFeatures(df):\n    \n    df['_OQuGLA'] = df['OverallQual'] * df['GrLivArea']\n    \n    df['___OQuGLAdTBSF'] = df['OverallQual'] * df['GrLivArea'] \/ (df['TotalBsmtSF'] + 1)\n    df['___OQdGLAuTBSF'] = df['OverallQual'] \/ (df['GrLivArea'] + 1) * df['TotalBsmtSF']\n    df['___dOQuGLAuTBSF'] = df['GrLivArea'] * df['TotalBsmtSF'] \/ (df['OverallQual'] + 1)\n    \n    df['___mdOQpGLApTBSF'] = df['GrLivArea'] + df['TotalBsmtSF'] - (df['OverallQual'] + 1)\n    \n    ","a02cccc0":"newFeatures(train)\nnewFeatures(test)","bb49fdaf":"def check_features(df):\n      return pd.DataFrame({'unique_values': df.nunique(),'type': df.dtypes,'pct_missing': df.isna().sum()\/len(df) * 100}).sort_values(by = 'pct_missing', ascending=False) \n\ndef rmse(targets, preds): return metrics.mean_squared_error(targets, preds, squared=False)\n\ndef rf_feat_importance(m, df): return pd.DataFrame({'feats':df.columns, 'imp':m.feature_importances_}).sort_values('imp', ascending=False)\n\ndef missing_cats(df, cats): return df[cats].fillna('#na#')\n\ndef get_fi(model, features):\n    return pd.DataFrame(np.stack([features, model.feature_importances_], axis=1), columns=['feats', 'imp']).astype({'imp':np.float32}).sort_values(by='imp', ascending=False).reset_index(drop=True) \n\ndef plot_fi(model, features):\n    fi = pd.DataFrame({'feature':features, 'importance':(model.feature_importances_).tolist()}).round(4).sort_values(by='importance', ascending=False)\n    bars = alt.Chart(fi).mark_bar().encode(\n      x='importance',\n      y=alt.Y('feature', sort='-x'),\n      tooltip=['feature','importance']\n    )\n    text = bars.mark_text(\n      align='left',\n      baseline='middle',\n      dx=3  # Nudges text to right so it doesn't appear on top of the bar\n    ).encode(\n      text='importance:Q'\n    )\n    return (bars + text).properties(width=400).configure_axis(labelFontSize=13, titleFontSize=16)\n\ndef submit(model, test, features, fname=None):\n    x_test = pd.DataFrame(ctf.transform(test[features]), columns=features)\n    df_preds = pd.DataFrame({'Id':test.Id , 'SalePrice': np.exp(model.predict(x_test))})\n    df_preds.to_csv(fname ,index=False)\n","43429a99":"# Check number of train & test examples\ntrain.shape, test.shape","c2ef26a1":"# Check for columns with nulls in train set.\n(train\n .isnull()\n .sum()\n .to_frame(name='num_missing')\n .query('num_missing>0')\n .sort_values(by='num_missing', ascending=False)\n .plot.barh(figsize=(6,4))\n);","c7bf4bc5":"# Check for columns with nulls in test set.\n(test\n .isnull()\n .sum()\n .to_frame(name='num_missing')\n .query('num_missing>0')\n .sort_values(by='num_missing', ascending=False)\n .plot\n .barh(figsize=(6,8))\n);","29ea7052":"# Define feature types\ntarget = ['SalePrice']\ncat_feats = train.drop(columns=['Id', 'SalePrice']).select_dtypes(include='object').columns.tolist()\ncont_feats = train.drop(columns=['Id', 'SalePrice']).select_dtypes(include=np.number).columns.tolist()\nall_feats = cat_feats + cont_feats\nlen(train.columns), len(cat_feats), len(cont_feats)","dacdc94a":"# Pipeline for categorical feature transformation.\ncat_tfms = Pipeline(steps=[\n    ('cat_ordenc', ce.OrdinalEncoder(return_df=True, handle_unknown='value', handle_missing='value'))\n])\n\n# Pipeline for numeric feature transformation.\ncont_tfms = Pipeline(steps=[\n    ('cont_imputer', SimpleImputer(missing_values=np.nan, strategy='median'))\n])\n\n# Transform cat & cont features separately and concatenate the features\nctf = ColumnTransformer(transformers=[\n    ('cat_tfms', cat_tfms, cat_feats),\n    ('cont_tfms', cont_tfms, cont_feats)\n], remainder='passthrough')","d016236d":"# Initialize a dict to caputure categorical features and their encodings.\nordenc_map = dict()\nctf = ColumnTransformer(transformers=[])","07532061":"def data_preproc(cat_feats, cont_feats):\n    cat_feats = train.drop(columns=['Id', 'SalePrice']).loc[:, feats].select_dtypes(include='object').columns.tolist()\n    cont_feats = train.drop(columns=['Id', 'SalePrice']).loc[:, feats].select_dtypes(include=np.number).columns.tolist()\n    all_feats = cat_feats + cont_feats\n\n    # Pipeline for categorical feature transformation.\n    cat_tfms = Pipeline(steps=[\n        ('cat_ordenc', ce.OrdinalEncoder(return_df=True, handle_unknown='value', handle_missing='value'))\n    ])\n\n    # Pipeline for numeric feature transformation.\n    cont_tfms = Pipeline(steps=[\n        ('cont_imputer', SimpleImputer(missing_values=np.nan, strategy='median'))\n    ])\n\n    # Transform cat & cont features separately and concatenate the features\n    ctf = ColumnTransformer(transformers=[\n        ('cat_tfms', cat_tfms, cat_feats),\n        ('cont_tfms', cont_tfms, cont_feats)\n    ], remainder='passthrough')\n    \n    return ctf\n","a7b73cf9":"def model_fit_eval(model, train, test, feats, splits=4):\n    # Define features\n    target = ['SalePrice']\n    cat_feats = train.drop(columns=['Id', 'SalePrice']).loc[:, feats].select_dtypes(include='object').columns.tolist()\n    cont_feats = train.drop(columns=['Id', 'SalePrice']).loc[:, feats].select_dtypes(include=np.number).columns.tolist()\n    all_feats = cat_feats + cont_feats\n    errors = []\n    \n    # Pipeline for categorical feature transformation.\n    cat_tfms = Pipeline(steps=[\n        ('cat_ordenc', ce.OrdinalEncoder(return_df=True, handle_unknown='value', handle_missing='value'))\n    ])\n\n    # Pipeline for numeric feature transformation.\n    cont_tfms = Pipeline(steps=[\n        ('cont_imputer', SimpleImputer(missing_values=np.nan, strategy='median'))\n    ])\n\n    # Transform cat & cont features separately and concatenate the features\n    ctf = ColumnTransformer(transformers=[\n        ('cat_tfms', cat_tfms, cat_feats),\n        ('cont_tfms', cont_tfms, cont_feats)\n    ], remainder='passthrough')\n    \n    \n    kf = model_selection.KFold(n_splits=splits, shuffle=True, random_state=42)\n    for fold, (train_idx, val_idx) in enumerate(kf.split(X = train)):\n        # split complete data into train and validation sets\n        df_train = train.iloc[train_idx]\n        df_val = train.iloc[val_idx]\n        \n        # extract the X, y from training data.\n        x_train = df_train[all_feats]\n        y_train = df_train.SalePrice.to_numpy()\n        \n        # extract the X, y from validation data.\n        x_val = df_val[all_feats]\n        y_val = df_val.SalePrice.to_numpy()\n        \n        # Preprocess the train & validation data before modeling\n        x_train_tf = pd.DataFrame(ctf.fit_transform(x_train), columns=all_feats)\n        x_val_tf = pd.DataFrame(ctf.transform(x_val), columns = all_feats)\n        \n        # create a map for categorical features and their encodings.\n        for feat in cat_feats:\n            ordenc_map[feat] = dict(zip(x_train[feat], x_train_tf[feat]))\n        \n        # Model fit & predict\n        model.fit(x_train_tf, y_train)\n        preds_train = model.predict(x_train_tf)\n        preds_val = model.predict(x_val_tf)    \n        \n        # Calculate train & validation rmse.\n        rmse_train = rmse(y_train, preds_train)\n        rmse_val = rmse(y_val, preds_val)\n        \n        errors.append([fold, rmse_train, rmse_val])\n    \n    # Display results.\n    result = pd.DataFrame(np.stack(errors), columns=['fold', 'rmse_train', 'rmse_val'])\n    mean_error = result[['rmse_train', 'rmse_val']].mean().to_frame(name='rmse')\n    display(mean_error)\n    display(result)\n    \n    return ordenc_map, ctf","8793be67":"rf1 = RandomForestRegressor(\n    n_estimators=100, \n    max_depth=None, \n    min_samples_leaf=1, \n    min_samples_split=2,\n    max_features=.8, \n    max_samples=None, \n    n_jobs=-1)\n\ncat_map, ctf = model_fit_eval(rf1, train, test, all_feats, splits=4)","e8c3005d":"submit(rf1, test, all_feats, fname='sub1.csv')","22e478e1":"plot_fi(rf1, all_feats)","5a7b96e8":"cols_1 = ['GrLivArea', 'OverallQual', 'TotalBsmtSF', 'GarageCars']\nfig, ax = plt.subplots(2,2, figsize=(12,8))\nfor i, col in enumerate(cols_1):\n    plt.subplot(2,2,i+1)\n    plt.scatter(train[col], train['SalePrice']);\n","44ba6a5f":"fi = get_fi(rf1, all_feats)\nto_keep = fi[:20].feats.tolist()","6f6748e0":"rf2 = RandomForestRegressor(\n    n_estimators=100, \n    max_depth=None, \n    min_samples_leaf=1, \n    min_samples_split=2,\n    max_features=.8, \n    max_samples=None, \n    n_jobs=-1)\n\nmodel_fit_eval(rf2, train, test, to_keep, splits=4)","f631e030":"plot_fi(rf2, to_keep)","9a3ce89f":"# import json\n# !mkdir ~\/.kaggle\n# !touch ~\/.kaggle\/kaggle.json\n# api_token = {\"username\":\"adityabhat\",\"key\":\"077eab99785d783113fedbf2698bcc53\"}\n# with open('\/root\/.kaggle\/kaggle.json', 'w') as file:\n#     json.dump(api_token, file)\n# !chmod 600 ~\/.kaggle\/kaggle.json","a9ba2249":"# !kaggle competitions submit -c house-prices-advanced-regression-techniques -f sub1.csv -m \"Message\"","23393ed0":"It is fork of https:\/\/www.kaggle.com\/adityabhat\/house-price-what-factors-make-people-pay-more\n\n_OQuGLA feature ('OverallQual' * 'GrLivArea') : 0.144 -> 0.136\n\nother features : 0.136 -> 0.135","7e389fc2":"## Random Forest Model","d36b754d":"## \ud83c\udfc1 Preliminaries","514286e4":"## Data Preprocessing","9d187965":"## Scratchpad","aec36a04":"## Understanding the top 20 features","1e296f38":"# [\ud83c\udfe0 House Prices: Which factors influence the house prices the most? \ud83d\udd0e](https:\/\/www.kaggle.com\/c\/house-prices-advanced-regression-techniques\/data)","9b18d928":"## Data Exploration","898b8476":"## Utility Code"}}