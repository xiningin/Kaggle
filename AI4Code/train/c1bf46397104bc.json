{"cell_type":{"fe5e5d8b":"code","220d7c42":"code","dd06aecf":"code","54cbc5e1":"code","4b358f7c":"code","f564d2fb":"code","a989a7f7":"code","2a9da4df":"code","3a75cd89":"code","c2bcf7dc":"code","284fca5f":"code","a243e88b":"code","e7c871c6":"code","c7d2c72c":"code","e496c55d":"code","6a193752":"code","79c7bbf3":"code","465808e1":"code","a059ae7d":"code","c670956e":"code","ac5fbd17":"code","1d7ac13a":"code","8d7d510b":"code","79518afa":"code","e5e688a1":"code","f977c95a":"code","cea99660":"code","301ceae2":"code","df1d0d29":"code","64127f92":"code","a2726ada":"code","033483e2":"code","f3dd35ac":"code","a4f101ca":"code","9b02d6d4":"code","152a7549":"code","2db65bf0":"code","ed2c4726":"code","d14b2895":"code","f20a02e8":"code","cd24f346":"code","2e827d64":"code","53707771":"code","f3b7bcde":"code","d9e75f1d":"code","2033a843":"code","d38df872":"code","cf6122e5":"code","5810d3f5":"code","0b70f7eb":"code","eb2e6d84":"code","5ab00609":"code","8531ad17":"code","0d80e7aa":"markdown","d25217f5":"markdown","3578bc80":"markdown","6a1e2fd0":"markdown","f1b1bc29":"markdown","fad1c46a":"markdown","e7e3a4af":"markdown","94bd9103":"markdown","b048823e":"markdown","e3414732":"markdown","f08d55ce":"markdown","12940a40":"markdown","23d37ebd":"markdown","eef71506":"markdown","d4dfa507":"markdown","d5d3d1d7":"markdown","b416b5ae":"markdown","f4b766e9":"markdown","d6cab3d2":"markdown","67c03a31":"markdown","7b10cc56":"markdown","d3ae21e7":"markdown","9af7f4d1":"markdown","bc5f1e6e":"markdown","12742466":"markdown","bd45f95c":"markdown","578a59cb":"markdown","c1cc22cb":"markdown","adf9160a":"markdown","062d37ef":"markdown","f87f9601":"markdown","a1c48b88":"markdown","fb02dc32":"markdown"},"source":{"fe5e5d8b":"import os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","220d7c42":"import pandas as pd\nimport numpy as np\nimport plotly.graph_objects as go\nimport plotly.express as px\nfrom plotly.offline import iplot, init_notebook_mode\nfrom tqdm.notebook import tqdm\nfrom pandas_profiling import ProfileReport\nfrom itertools import islice\nfrom datetime import datetime\nimport pprint\nimport ast\nfrom catboost.text_processing import Tokenizer\nimport pprint\nimport ast\nfrom catboost.text_processing import Tokenizer\nfrom collections import Counter\nimport cufflinks\n\n\ncufflinks.go_offline(connected=True)\ninit_notebook_mode(connected=True)\n\npp = pprint.PrettyPrinter(indent=4)\n\n\ndef take(n, iterable):\n    \"\"\"Return first n items of the iterable as a list\"\"\"\n    return list(islice(iterable, n))\n\n\nfood_data_train = pd.read_csv(\"\/kaggle\/input\/sf-dst-restaurant-rating\/main_task.csv\")\nfood_data_test = pd.read_csv(\"\/kaggle\/input\/sf-dst-restaurant-rating\/kaggle_task.csv\")\n\nworld_cityes = pd.read_csv(\"\/kaggle\/input\/world-cities\/worldcities.csv\")\ncountry_population = pd.read_csv(\"\/kaggle\/input\/pop-by-country\/pop_by_country.csv\")\n\n\nfood_data_train[\"sample\"] = 1  # it's traning dataset\nfood_data_test[\"sample\"] = 0  # it's test dataset\nfood_data_test[\"Rating\"] = 0  # equal 0 for prediction\n\nfood_data = food_data_test.append(food_data_train, sort=False).reset_index(\n    drop=True\n)\n\n\npd.set_option(\"display.max_rows\", 50)\npd.set_option(\"display.max_columns\", 50)\ndisplay(food_data.head(5))\ndisplay(country_population.head(5))","dd06aecf":"# Cardinality \u0445\u0430\u0440\u0430\u043a\u0442\u0435\u0440\u0438\u0437\u0443\u0435\u0442 \u0443\u043d\u0438\u043a\u0430\u043b\u044c\u043d\u043e\u0441\u0442\u044c \u0434\u0430\u043d\u043d\u044b\u0445.\n# \u0412\u044b\u0441\u043e\u043a\u0430\u044f \u043a\u0430\u0440\u0434\u0438\u043d\u0430\u043b\u044c\u043d\u043e\u0441\u0442\u044c - \u0443\u043d\u0438\u043a\u0430\u043b\u044c\u043d\u044b\u0435 \u0434\u0430\u043d\u043d\u044b\u0435,\n# \u043d\u0438\u0437\u043a\u0430\u044f \u043a\u0430\u0440\u0434\u0438\u043d\u0430\u043b\u044c\u043d\u043e\u0441\u0442\u044c - \u043f\u043e\u0432\u0442\u043e\u0440\u044f\u044e\u0449\u0438\u0435\u0441\u044f \u0434\u0430\u043d\u043d\u044b\u0435.\n# profile = ProfileReport(food_data, config_file=\".\/input\/config_dark.yaml\")\n# profile.to_file(\"project3.html\")","54cbc5e1":"index = food_data.index\n\nnumber_of_rows = len(index)\nuniq_vs_nan = pd.DataFrame(index=food_data.columns, columns=[\"uniq\", \"nulled\"])\n\nfor col in tqdm(food_data.columns):\n    # this is more easy way to countiing nulled values\n    uniq_vs_nan.loc[col, \"nulled\"] = food_data[col].isnull().sum(axis=0)\n    # try:\n    #    nan_count = pd.notnull(food_data[col]).value_counts()[False]\n    # except KeyError:  # \u043d\u0435\u0442 \u0437\u043d\u0430\u0447\u0435\u043d\u0438\u0439 nan\n    #    nan_count = 0\n    uniq_vs_nan.loc[col, \"uniq\"] = food_data[col].nunique()\n\n\nfig = go.Figure(\n    data=[\n        go.Bar(\n            name=\"Uniq values (abs)\",\n            x=uniq_vs_nan.index,\n            y=uniq_vs_nan[\"uniq\"],\n        ),\n        go.Bar(\n            name=\"NaN values (abs)\",\n            x=uniq_vs_nan.index,\n            y=uniq_vs_nan[\"nulled\"],\n        ),\n    ]\n)\n# Change the bar mode\nfig.update_layout(barmode=\"group\")\nfig.update_layout(\n    {\"plot_bgcolor\": \"rgba(0, 0, 0, 0)\", \"paper_bgcolor\": \"rgba(0, 0, 0, 0)\",},\n    title={\n        \"text\": \"uniq to NaN\",\n        \"y\": 0.85,\n        \"x\": 0.4,\n        \"xanchor\": \"center\",\n        \"yanchor\": \"top\",\n    },\n    xaxis_title=\"Values count\",\n    yaxis_title=\"Column name\",\n    font={\"family\": \"Courier New, monospace\", \"size\": 14, \"color\": \"#7f7f7f\"},\n)\nfig.show()","4b358f7c":"# if the zero values are rather low, they may not be visible in the chart\n# check all columns that have zero values.\ndisplay(uniq_vs_nan[uniq_vs_nan.nulled > 0])","f564d2fb":"# just other zero values checking method \n# duble check matter :)))\nnull_value_stats = food_data.isnull().sum(axis=0)\nnull_value_stats[null_value_stats != 0]\ndisplay(null_value_stats)","a989a7f7":"fig = go.Figure(\n    data=[\n        go.Bar(\n            name=\"Rating\",\n            x=food_data[\"Rating\"].value_counts().index,\n            y=food_data[\"Rating\"].value_counts(),\n        )\n    ]\n)\n# Change the bar mode\nfig.update_layout(\n    {\"plot_bgcolor\": \"rgba(0, 0, 0, 0)\", \"paper_bgcolor\": \"rgba(0, 0, 0, 0)\",},\n    title={\n        \"text\": \"Target var distribution\",\n        \"y\": 0.85,\n        \"x\": 0.4,\n        \"xanchor\": \"center\",\n        \"yanchor\": \"top\",\n    },\n    xaxis_title=\"Rating value\",\n    yaxis_title=\"Rating value count\",\n    font={\"family\": \"Courier New, monospace\", \"size\": 14, \"color\": \"#7f7f7f\"},\n)\nfig.show()","2a9da4df":"# we have nan in 3 columns: \"Cuisine Style\", \"Price Range\", \"Number of Reviews\"\n# don't sav info about 'Reviews' because it's only 2 nan values\n# save information about nan's in dataset\ndef save_nan(param):\n    return food_data[param].apply(lambda x: 1 if pd.isna(x) else 0)\n\n\nfood_data[\"NO Cuisine Style\"] = save_nan(\"Cuisine Style\")\nfood_data[\"NO Price Range\"] = save_nan(\"Price Range\")\nfood_data[\"NO Number of Reviews\"] = save_nan(\"Number of Reviews\")\nfood_data['Number of Reviews'].fillna(0, inplace=True)\n\n# check\ndisplay(food_data[food_data[\"NO Cuisine Style\"] == 1].head(5))","3a75cd89":"food_data.info()","c2bcf7dc":"food_data[\"Price Range\"].value_counts()","284fca5f":"# We have 17361 NaN's in \"Price Range\"\n# Most popular value  is \"$$ - $$$\"\n# when fill all nan  with most popular\n\nfood_data[\"Price Range\"].fillna(\n    food_data[\"Price Range\"].mode()[0], inplace=True\n)\n# check values\nprint(food_data[\"Price Range\"].value_counts())","a243e88b":"# Convert \"Price Range\" to numeerical\nprice_range_dict = {\"$\": 1.0, \"$$ - $$$\": 2.0, \"$$$$\": 3.0}\nfood_data[\"Price Range\"] = food_data[\"Price Range\"].apply(\n    lambda x: price_range_dict[x]\n)\n# check\nprint(food_data[\"Price Range\"].value_counts())\nprint(food_data.info())","e7c871c6":"food_data[\"City\"].value_counts()","c7d2c72c":"# use only capitals in dataset\nworld_cityes = world_cityes.dropna(subset=[\"capital\"])\n\n# change Oporto to Porto - it's same\nfood_data.loc[food_data[\"City\"] == \"Oporto\", \"City\"] = \"Porto\"\n\n\ndef get_dict_by_city(param):\n    return pd.Series(\n        world_cityes[param].values, index=world_cityes[\"city_ascii\"]\n    ).to_dict()\n\n\npopulation_dict = get_dict_by_city(\"population\")\ncapital_dict = get_dict_by_city(\"capital\")\ncountry_code_dict = get_dict_by_city(\"iso3\")\n\nfood_data[\"population in thousands\"] = food_data[\"City\"].apply(\n    lambda x: population_dict[x] \/ 1000\n)\nfood_data[\"capital\"] = food_data[\"City\"].apply(lambda x: capital_dict[x])\nfood_data[\"country_code\"] = food_data[\"City\"].apply(\n    lambda x: country_code_dict[x]\n)\n\ndisplay(food_data.info())","e496c55d":"# country_pop_dict = pd.Series(\n#     country_population[\"2018\"].values, index=country_population[\"Country Code\"]\n# ).to_dict()\n# # country_pop_dict = country_population[['Country Code','2018']].to_dict()\n\n\n# food_data[\"Population in country\"] = food_data[\"country_code\"].apply(lambda x: country_pop_dict[x])\n","6a193752":"fig = go.Figure(\n    data=[\n        go.Bar(\n            name=\"Rest peer City\",\n            x=food_data[\"City\"].value_counts().index,\n            y=food_data[\"City\"].value_counts(),\n        ),\n        go.Bar(\n            name=\"Population\",\n            y=food_data[\"population in thousands\"].value_counts().index,\n            x=food_data[\"City\"].value_counts().index\n        ),\n    ]\n)\n# Change the bar mode\nfig.update_layout(barmode=\"group\")\nfig.update_layout(\n    {\"plot_bgcolor\": \"rgba(0, 0, 0, 0)\", \"paper_bgcolor\": \"rgba(0, 0, 0, 0)\",},\n    title={\n        \"text\": \"Rest peer City\",\n        \"y\": 0.85,\n        \"x\": 0.4,\n        \"xanchor\": \"center\",\n        \"yanchor\": \"top\",\n    },\n    xaxis_title=\"City name\",\n    yaxis_title=\"Rest count\",\n    font={\"family\": \"Courier New, monospace\", \"size\": 14, \"color\": \"#7f7f7f\"},\n)\nfig.show()","79c7bbf3":"# add new vars: 1. rest count per City, 2. rest count per 1 000 population\nrest_counts_dict = food_data[\"City\"].value_counts().to_dict()\nfood_data[\"Rest counts per City\"] = food_data[\"City\"].apply(\n    lambda x: rest_counts_dict[x]\n)\nfood_data[\"Rest counts per 1000\"] = (\n    food_data[\"Rest counts per City\"] \/ food_data[\"population in thousands\"]\n)\ndisplay(food_data.info())","465808e1":"fig = go.Figure(\n    data=[\n        go.Bar(\n            name=\"Rest peer Country\",\n            x=food_data[\"country_code\"].value_counts().index,\n            y=food_data[\"country_code\"].value_counts(),\n        )\n    ]\n)\n# Change the bar mode\nfig.update_layout(\n    {\"plot_bgcolor\": \"rgba(0, 0, 0, 0)\", \"paper_bgcolor\": \"rgba(0, 0, 0, 0)\",},\n    title={\n        \"text\": \"Rest peer Country\",\n        \"y\": 0.85,\n        \"x\": 0.4,\n        \"xanchor\": \"center\",\n        \"yanchor\": \"top\",\n    },\n    xaxis_title=\"Country code\",\n    yaxis_title=\"Rest count\",\n    font={\"family\": \"Courier New, monospace\", \"size\": 14, \"color\": \"#7f7f7f\"},\n)\nfig.show()","a059ae7d":"# add new var - rest count per Country\nrest_counts_dict = food_data[\"country_code\"].value_counts().to_dict()\nfood_data[\"Rest counts per Country\"] = food_data[\"country_code\"].apply(\n    lambda x: rest_counts_dict[x]\n)","c670956e":"display(food_data[\"Rest counts per Country\"].value_counts())","ac5fbd17":"# replace NaN to ['Other'] to prevent assertion of ast.literal_eval\nfood_data[\"Cuisine Style\"].fillna(\"['Other']\", inplace=True)\nfood_data[\"Cuisine Style\"] = food_data[\"Cuisine Style\"].apply(\n    lambda x: ast.literal_eval(x)\n)\n\nflat_list = [\n    item for sublist in food_data[\"Cuisine Style\"] for item in sublist\n]\ncuisine_style_counter = Counter(flat_list)\n\nprint(len(cuisine_style_counter))\npp.pprint((take(10, cuisine_style_counter.most_common())))","1d7ac13a":"# Cuisines to dummyes\nfor cuisine_style in cuisine_style_counter:\n    food_data[cuisine_style] = food_data[\"Cuisine Style\"].apply(\n        lambda x: 1 if cuisine_style in x else 0\n    )","8d7d510b":"food_data[\"Cuisine Style Count\"] = food_data[\"Cuisine Style\"].apply(\n    lambda x: len(x)\n)\n# get mean cousine styles per restaurant\npp.pprint(food_data[\"Cuisine Style Count\"].mean())\ndisplay(food_data.head(5))","79518afa":"fig = px.box(food_data, x=\"Cuisine Style Count\", orientation='h')\nfig.update_layout(\n    {\"plot_bgcolor\": \"rgba(0, 0, 0, 0)\", \"paper_bgcolor\": \"rgba(0, 0, 0, 0)\",},\n    title={\n        \"text\": \"BoxPlot \u043f\u043e \u043a\u043e\u043b\u0438\u0447\u0435\u0441\u0442\u0432\u0443 \u043a\u0443\u0445\u043e\u043d\u044c\",\n        \"y\": 0.95,\n        \"x\": 0.4,\n        \"xanchor\": \"center\",\n        \"yanchor\": \"top\",\n    },\n    xaxis_title=\"\u041a\u043e\u043b\u0438\u0447\u0435\u0441\u0442\u0432\u043e \u043a\u0443\u0445\u043e\u043d\u044c\",    \n    font={\"family\": \"Courier New, monospace\", \"size\": 14, \"color\": \"#7f7f7f\"},\n)\nfig.show()","e5e688a1":"# food_data[\"Reviews\"] = food_data[\"Reviews\"].str.lower()\nfood_data[\"Number of Reviews\"].mean()\n\n\nfood_data[\"Reviews\"] = food_data[\"Reviews\"].str.replace(\"nan\", \"' '\")\n# remove 'nan' for prevent catboost fall with assertion\nfood_data[\"Reviews\"].fillna(\"[[], []]\", inplace=True)\n\n# for debug only(for catch assertion) instead use lambda x: tokenizer.tokenize(x)\n# def tokenize_review_debug(tok):\n#    try:\n#      ret = tokenizer.tokenize(tok)\n#    except:\n#      print(tok)\n#    return ret\n\n\n# for debug only(for catch assertion) instead use lambda x: ast.literal_eval(x)\n# def ast_review_debug(tok):\n#   try:\n#     ret = ast.literal_eval(tok)\n#   except:\n#         print(tok)\n#     return ret\n\n\nfood_data[\"Reviews\"] = food_data[\"Reviews\"].apply(\n    lambda x: ast.literal_eval(x)\n)\n\n# how many reviews have the date and text?\n# first of all get max and min list of dates len\nreviews_dates_list_len = food_data[\"Reviews\"].apply(lambda x: len(x[1]))\n\nprint(f\"Maximum Reviews with text`s: {reviews_dates_list_len.max()}\")\nprint(f\"Minimum Reviews with text`s: {reviews_dates_list_len.min()}\")\n\n# ok min is 0 it's mean that we have Reviews with no date\n# check how many it is?\nreviews_dates_list_0 = food_data[\"Reviews\"].apply(\n    lambda x: 1 if len(x[1]) == 0 else 0\n)\nprint(f\"Number of rows with no date in Review: {reviews_dates_list_0.sum()}\")\n\n# now check how many records have not Review at all\nreviews_list_0 = food_data[\"Reviews\"].apply(\n    lambda x: 1 if len(x[0]) == 0 else 0\n)\nprint(f\"Number of rows with no text in Review:{reviews_dates_list_0.sum()}\")\n\n# hmm these numbers are equal, bith is 8114\n# now check what we have not rows with dates in Review but without Reviews text\n# and save this in dataset\n\nfood_data[\"No Dates or No texts in Reviews\"] = food_data[\"Reviews\"].apply(\n    lambda x: 1\n    if (len(x[0]) == 0 and len(x[1]) != 0)\n    or (len(x[1]) == 0 and len(x[0]) != 0)\n    else 0\n)\n\nprint(\n    f\"Number of rows with text but no date and vice versa: \\\n    {food_data['No Dates or No texts in Reviews'].sum()}\"\n)","f977c95a":"# convert text to date\nfood_data[\"Reviews Dates\"] = food_data[\"Reviews\"].apply(\n    lambda x: [datetime.strptime(d, \"%m\/%d\/%Y\") for d in x[1]]\n)\n\n# Check if dates in list in right order and reordering if order is wrong\nfood_data[\"Reviews Dates\"] = food_data[\"Reviews Dates\"].apply(\n    lambda x: [x[i] for i in [1, 0]] if ((len(x) > 1) and (x[1] > x[0])) else x\n)\n\n# Add diffs between first and last review to dataset\nfood_data[\"Reviews Dates Diff\"] = food_data[\"Reviews Dates\"].apply(\n    lambda x: (x[0] - x[1]).days if len(x) > 1 else 0\n)\n\n# Add number of reviews per restaurant to dataset\nfood_data[\"Number of Reviews per rest\"] = food_data[\"Reviews Dates\"].apply(\n    lambda x: len(x)\n)\n\n\nprint(\n    f\"Maximal number of days between Reviews: \\\n    {food_data['Reviews Dates Diff'].max()}\"\n)\n\nprint(\n    f\"Minimal number of days between Reviews: \\\n    {food_data['Reviews Dates Diff'].min()}\"\n)\n\n\nprint(\n    f\"Mean number of days between Reviews: \\\n    {food_data['Reviews Dates Diff'].mean()}\"\n)\n\ndates_list = food_data[\"Reviews Dates\"].to_list()\ndates_flat = [item for sublist in dates_list for item in sublist]\nlatest_review = sorted(dates_flat, reverse=True)[0]\nearlest_review = sorted(dates_flat)[0]\nprint(f\"Earlest review date: {earlest_review}\")\nprint(f\"Latest review date: {latest_review}\")\n\nprint(\n    f'Reviews per restaraunt value counts:\\n {food_data[\"Number of Reviews\"].value_counts().to_frame()}'\n)\n\n# Add information how far current review is from latest review\nfood_data[\"How far from latest review\"] = food_data[\"Reviews Dates\"].apply(\n    lambda x: (latest_review - x[0]).days\n    if len(x) > 0\n    else (latest_review - earlest_review).days\n)","cea99660":"# add new var - number of Reviews per City\nreview_counts_dict = (\n    food_data.groupby([\"City\"])[\"Number of Reviews\"].sum().to_dict()\n)\n\nfood_data[\"Number of Reviews per City\"] = food_data[\"City\"].apply(\n    lambda x: review_counts_dict[x]\n)\n\n# add new var - number of Reviews per Country\nreview_counts_dict = (\n    food_data.groupby([\"country_code\"])[\"Number of Reviews\"].sum().to_dict()\n)\n\nfood_data[\"Number of Reviews per Country\"] = food_data[\"country_code\"].apply(\n    lambda x: review_counts_dict[x]\n)","301ceae2":"food_data.info()\ndisplay(food_data.head(5))","df1d0d29":"# lets go to work with Review texts\n\n# add  \"Number\" to token_types for number processing\ntokenizer = Tokenizer(\n    lowercasing=True, separator_type=\"BySense\", token_types=[\"Word\"]\n)\n\n\nstop_words = set(\n    (\"be\", \"is\", \"are\", \"the\", \"an\", \"of\", \"and\", \"in\", \"food\", \"a\")\n)\n\n\ndef filter_stop_words(tokens):\n    return list(filter(lambda x: x not in stop_words, tokens))\n\n\nreviews_tokens = food_data[\"Reviews\"].apply(\n    lambda x: [\n        item\n        for sublist in [tokenizer.tokenize(i) for i in x[0]]\n        for item in sublist\n    ]\n)\nreviews_tokens = [filter_stop_words(tokens) for tokens in reviews_tokens]\n\n# convert list of lists to list\nflat_list = [item for sublist in reviews_tokens for item in sublist]\ntokens_counter = Counter(flat_list)\n\nall_tokens_count = 0\n\nreview_meaning_words = [\n    \"awesome\",\n    \"excellent\",\n    \"great\",\n    \"good\",\n    \"ok\",\n    \"bad\",\n    \"awful\",\n    \"terrible\",\n    \"horrible\",\n]\n\n\nfor w in review_meaning_words:\n    try:\n        all_tokens_count += tokens_counter[w]\n        print(f'Token {w} occured {tokens_counter[w]} times')\n    except KeyError:\n        print(f\"word {w} not found!\")\n\nprint(f\"Total tokens count: {all_tokens_count}\")","64127f92":"# import nltk\n# import os\n\n# nltk_data_path = os.path.join(os.path.dirname(nltk.__file__), 'nltk_data')\n# nltk.data.path.append(nltk_data_path)\n# nltk.download('wordnet', nltk_data_path)\n\n# lemmatizer = nltk.stem.WordNetLemmatizer()\n\n# def lemmatize_tokens_nltk(tokens):\n#     return list(map(lambda t: lemmatizer.lemmatize(t), tokens))\n\n\n# text_small_lemmatized_nltk = [lemmatize_tokens_nltk(tokens) for tokens in reviews_tokens]\n# # convert list of lists to list\n# flat_list = [item for sublist in text_small_lemmatized_nltk for item in sublist]\n# text_small_lemmatized_nltk_counter = Counter(flat_list)\n\n# total_lemmas_count = 0\n# for w in [\n#     \"awesome\",\n#     \"excellent\",\n#     \"great\",\n#     \"good\",\n#     \"ok\",\n#     \"bad\",\n#     \"awful\",\n#     \"terrible\",\n#     \"horrible\",\n# ]:\n#     try:\n#         # print(text_small_lemmatized_nltk_counter[w])\n#         total_lemmas_count += text_small_lemmatized_nltk_counter[w]\n#     except:\n#         print(f\"word {w} not found!\")\n\n# print(f'Total lemmas count: {total_lemmas_count}')\n# print(f'Lemmas and tokens count diff: {total_lemmas_count-all_tokens_count}')","a2726ada":"def review_word_counter(rev, word):\n    ret = 0\n    for i in rev[0]:\n        ret += i.count(word)\n    return ret\n\n\nfor w in review_meaning_words:\n    food_data[w] = food_data[\"Reviews\"].apply(review_word_counter, args=(w,))","033483e2":"display(food_data.head(5))","f3dd35ac":"# remove first 'd' character from ID_TA parameter and convert it to digit\nfood_data[\"ID_TA\"] = food_data[\"ID_TA\"].apply(lambda x: int(x[1:]))","a4f101ca":"# read group ID from URL\n\nfood_data[\"Group ID from URL_TA\"] = food_data[\"URL_TA\"].str.extract(\n    pat=\"(-g\\d+)\"\n)\nfood_data[\"Group ID from URL_TA\"] = food_data[\"Group ID from URL_TA\"].apply(\n    lambda x: x[1:]\n)\ndisplay(food_data[\"Group ID from URL_TA\"].value_counts())","9b02d6d4":"url_ta_group_counter = Counter(food_data[\"Group ID from URL_TA\"].to_list())\n\nprint(len(url_ta_group_counter))\npp.pprint((take(10, url_ta_group_counter.most_common())))\n# looks like it's some geografic region, may be it's city suburb","152a7549":"# rest counts per \"Group ID from URL_TA\"\n\n# rest_counts_dict = food_data[\"Group ID from URL_TA\"].value_counts().to_dict()\n# food_data[\"Rest counts peer Group ID from URL_TA\"] = food_data[\n#     \"Group ID from URL_TA\"\n# ].apply(lambda x: rest_counts_dict[x])","2db65bf0":"# add new var - number of Reviews per Group ID from URL_TA\n# review_counts_dict = (\n#     food_data.groupby([\"Group ID from URL_TA\"])[\"Number of Reviews\"].sum().to_dict()\n# )\n\n# food_data[\"Number of Reviews per Group ID from URL_TA\"] = food_data[\"Group ID from URL_TA\"].apply(\n#     lambda x: review_counts_dict[x]\n# )\n","ed2c4726":"# URL_TA grpups to dummyes\nfor url_ta_group in url_ta_group_counter:\n    food_data[url_ta_group] = food_data[\"Group ID from URL_TA\"].apply(\n        lambda x: 1 if url_ta_group in x else 0\n    )","d14b2895":"fig = px.box(food_data, x=\"Ranking\", orientation='h')\nfig.update_layout(\n    {\"plot_bgcolor\": \"rgba(0, 0, 0, 0)\", \"paper_bgcolor\": \"rgba(0, 0, 0, 0)\",},\n    title={\n        \"text\": \"Ranking BoxPlot\",\n        \"y\": 0.95,\n        \"x\": 0.4,\n        \"xanchor\": \"center\",\n        \"yanchor\": \"top\",\n    },\n    xaxis_title=\"Rank\",    \n    font={\"family\": \"Courier New, monospace\", \"size\": 14, \"color\": \"#7f7f7f\"},\n)\nfig.show()","f20a02e8":"fig = px.histogram(\n    food_data[\n        food_data[\"City\"].isin(\n            [\"London\", \"Paris\", \"Madrid\", \"Barcelona\", \"Berlin\"]\n        )\n    ],\n    x=\"Ranking\",\n    nbins=200,\n    color=\"City\",\n)\n\nfig.update_layout(\n    {\"plot_bgcolor\": \"rgba(0, 0, 0, 0)\", \"paper_bgcolor\": \"rgba(0, 0, 0, 0)\",},\n    title={\n        \"text\": \"Ranking Distribution\",\n        \"y\": 0.92,\n        \"x\": 0.4,\n        \"xanchor\": \"center\",\n        \"yanchor\": \"top\",\n    },\n    xaxis_title=\"Rank\",\n    yaxis_title=\"Rest count\",\n    font={\"family\": \"Courier New, monospace\", \"size\": 14, \"color\": \"#7f7f7f\"},\n)\n\nfig.show()","cd24f346":"fig = px.histogram(\n    food_data[\n        food_data[\"City\"].isin(\n            [\"Heelsinki\", \"Bratislava\", \"Luxembourg\", \"Ljubljana\", \"Oslo\"]\n        )\n    ],\n    x=\"Ranking\",\n    nbins=200,\n    color=\"City\",\n)\n\nfig.update_layout(\n    {\"plot_bgcolor\": \"rgba(0, 0, 0, 0)\", \"paper_bgcolor\": \"rgba(0, 0, 0, 0)\",},\n    title={\n        \"text\": \"Ranking Distribution\",\n        \"y\": 0.92,\n        \"x\": 0.4,\n        \"xanchor\": \"center\",\n        \"yanchor\": \"top\",\n    },\n    xaxis_title=\"Rank\",\n    yaxis_title=\"Rest count\",\n    font={\"family\": \"Courier New, monospace\", \"size\": 14, \"color\": \"#7f7f7f\"},\n)\n\nfig.show()","2e827d64":"# add new var - Ranking per Country\nreview_counts_dict = (\n    food_data.groupby([\"country_code\"])[\"Ranking\"].sum().to_dict()\n)\n\nfood_data[\"Ranking per Country\"] = food_data[\"country_code\"].apply(\n    lambda x: review_counts_dict[x]\n)\n\n\nfood_data[\"Ranking per Country\"] = (\n    food_data[\"Ranking per Country\"] \/ food_data[\"Rest counts per Country\"]\n)","53707771":"# Create four new params\n\nfood_data[\"Relative Ranking per Rest counts (City)\"] = (\n    food_data[\"Ranking\"] \/ food_data[\"Rest counts per City\"]\n)\n\nfood_data[\"Relative Ranking per Number of Reviews (City)\"] = (\n    food_data[\"Ranking\"] \/ food_data[\"Number of Reviews per City\"]\n)\n\nfood_data[\"Relative Ranking per Rest counts (Country)\"] = (\n    food_data[\"Ranking per Country\"] \/ food_data[\"Rest counts per Country\"]\n)\n\nfood_data[\"Relative Ranking per Number of Reviews (Country)\"] = (\n    food_data[\"Ranking per Country\"] \/ food_data[\"Number of Reviews per Country\"]\n)\n\n","f3b7bcde":"food_data = pd.get_dummies(\n    food_data, columns=[\"country_code\", \"capital\"]\n)","d9e75f1d":"food_data = food_data.select_dtypes(exclude=['object'])\nfood_data.info()","2033a843":"from sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn import metrics\n\n\nfood_data_train = food_data[food_data[\"sample\"] == 1].drop([\"sample\"], axis=1)\nfood_data_test = food_data[food_data[\"sample\"] == 0].drop([\"sample\"], axis=1)\n\ny = food_data_train[\"Rating\"].values\nX = food_data_train.drop([\"Rating\"], axis=1)\n","d38df872":"RANDOM_SEED = 42\n\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.2, random_state=RANDOM_SEED\n)\nmodel = RandomForestRegressor(\n    n_estimators=100, verbose=1, n_jobs=-1, random_state=RANDOM_SEED\n)\n\nmodel.fit(X_train, y_train)\ny_pred = model.predict(X_test)\ny_pred = np.round(y_pred * 2) \/ 2\n\nprint(\"MAE:\", metrics.mean_absolute_error(y_test, y_pred))\n\nfeat_importances = pd.Series(model.feature_importances_, index=X.columns)\ndisplay(feat_importances.nlargest(15))","cf6122e5":"test_data = food_data_test.drop([\"Rating\"], axis=1)","5810d3f5":"sample_submission = pd.read_csv('\/kaggle\/input\/sf-dst-restaurant-rating\/sample_submission.csv')","0b70f7eb":"test_data","eb2e6d84":"predict_submission = model.predict(test_data)","5ab00609":"predict_submission = np.round(predict_submission*2)\/2\npredict_submission.shape","8531ad17":"sample_submission['Rating'] = predict_submission\nsample_submission.to_csv('submission.csv', index=False)\nsample_submission.head(10)","0d80e7aa":"## Working with Reviews texts\n\nWe will try to work with reviews texts using catboost text tools. For more details see [this](https:\/\/github.com\/catboost\/catboost\/blob\/564d8c016a04d088f8af75d9bc5d986731188a11\/catboost\/tutorials\/events\/2020_06_04_catboost_tutorial\/text_features.ipynb)","d25217f5":"### pandas_profile\n\nRead about this [here](https:\/\/www.kaggle.com\/nulldata\/intro-to-pandas-profiling-simple-fast-eda)","3578bc80":"### Write info about words in dataset","6a1e2fd0":"#### As we can see, the distributions are similar.","f1b1bc29":"### Build target variable distribution graph","fad1c46a":"### Tokenize words","e7e3a4af":"We not need lemmalization. Dofference between tokens and lemmas is too small","94bd9103":"### Build rest per Country graph","b048823e":"## Clear  from non digital data","e3414732":"# 5. ID_TA","f08d55ce":"### Add new columnt to dataset with Cuisine Style count","12940a40":"### Gather information about Countries population from **country_population** dataset and save it do **food_data** dataset","23d37ebd":"## Split model to test and train ","eef71506":"### Lemalize words","d4dfa507":"### Saving information about **nan**","d5d3d1d7":"# 3. Cuisine Style\n\n**Cuisine Style** column looks like a python list. For prevent assertion in _ast.literal_eval()_ we should replace nan's with _\"['Other']\"_ value","b416b5ae":"## Create dummy vars","f4b766e9":"### Working with review dates","d6cab3d2":"### Build BoxPlot for Cuisine Style count","67c03a31":"# 1. Price Range <a name=\"price_range\"><\/a>","7b10cc56":"# 7. Ranking","d3ae21e7":"# 2. City <a name=\"city\"><\/a>","9af7f4d1":"# 4. Reviews\n\nReviews column looks like a python list of lists,but two times we have values like: **\"[['Will certainly be back', nan], ['11\/26\/2017', '07\/27\/2017']]\"** with **nan**. For prevent assertion in _ast.literal_eval()_ we should replace nan's\n","bc5f1e6e":"### Now check Ranking distribution in the most common Cities","12742466":"### Now we check the Ranking distribution in the least common cities.","bd45f95c":"# 8. Train model","578a59cb":"### Build restaraunt per City graph","c1cc22cb":"### Train the model","adf9160a":"### Lets build BoxPlot for Ranking","062d37ef":"#### We have 10 000 rating values equal 0 from test dataset","f87f9601":"# 6. URL_TA","a1c48b88":"### Gather information about cities from **world_cityes** dataset and save it do **food_data** dataset","fb02dc32":"# Number of Reviews"}}