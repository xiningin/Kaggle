{"cell_type":{"4efb2ff2":"code","4d7714a4":"code","9f2a94b9":"code","141652bc":"code","81043c5f":"code","83726f8f":"code","6267d28b":"code","3499ceb5":"code","58bf387f":"code","78b790ba":"code","214acb3b":"code","f91ca0c9":"code","79511b6b":"code","836329d0":"code","895996c2":"code","120d9f5c":"code","fee981dd":"code","dd85451d":"code","b864d8c5":"code","9c65e999":"code","9e19ec04":"code","cbfe184d":"code","ceb5ce20":"code","2b8b0f31":"code","1086c9fd":"code","16a06c47":"code","85259dd7":"code","48bf91fd":"code","f7e4a133":"code","4ecd759d":"code","e1b36980":"code","5435b82a":"code","a90652f1":"code","9ed6dbe8":"code","fbce1dd6":"code","47016eae":"code","24e1c65c":"code","ceb06ce0":"code","5279767e":"code","877f120b":"code","17669ddc":"code","a7e28ddf":"code","8840ac13":"code","5fb27ddf":"code","7b341fc1":"markdown","f69c7b44":"markdown","fcc2f238":"markdown","1d7ce4c4":"markdown","d6951fe0":"markdown","4681fae9":"markdown","5a370b9b":"markdown","abbf2d33":"markdown","42cdd2a5":"markdown","6ff96850":"markdown","5bdc3662":"markdown","7314443d":"markdown","7e243df5":"markdown","62fb8654":"markdown","f3d26efc":"markdown","afad1afa":"markdown","c684a658":"markdown","457e9397":"markdown"},"source":{"4efb2ff2":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","4d7714a4":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline","9f2a94b9":"df=pd.read_csv(\"..\/input\/creditcardfraud\/creditcard.csv\")\ndf.head(5)","141652bc":"df.tail(5)","81043c5f":"df.shape","83726f8f":"#df['Class'].value_counts().plot(kind='bar')\nsns.countplot('Class',data=df)\nplt.title(\"class distribution\")\nplt.show()","6267d28b":"df.describe()","3499ceb5":"df.isnull().sum()","58bf387f":"print(round(df.Class.value_counts()[0]\/len(df)*100,2),'%')\nprint(round(df.Class.value_counts()[1]\/len(df)*100,2),'%')","78b790ba":"df.columns","214acb3b":"fig,axs=plt.subplots(1,2,figsize=(10,4))\n\namnt=df.Amount.values\ntime=df.Time.values\n\nsns.distplot(amnt,ax=axs[0],color='r')\naxs[0].set_title(\"Distribution of transaction amount\")\naxs[0].set_xlim([min(amnt),max(amnt)])\n\nsns.distplot(time,ax=axs[1],color='b')\naxs[1].set_title(\"Distribution of Trnasaction over time\")\naxs[1].set_xlim([min(time),max(time)])\nplt.show()","f91ca0c9":"#Plotting a heatmap to visualize the correlation between the variables\nsns.heatmap(df.corr())","79511b6b":"df[['Amount','Time']].describe()","836329d0":"# Descriptive statistics of  of frauds transactions\nsummary = (df[df['Class'] == 1].describe().transpose().reset_index())\n\nsummary = summary.rename(columns = {\"index\" : \"feature\"})\nsummary = np.around(summary,3)\nsummary","895996c2":"# Descriptive statistics of geniune transactions\n\nsummary = (df[df['Class'] == 0].describe().transpose().reset_index())\n\nsummary = summary.rename(columns = {\"index\" : \"feature\"})\nsummary = np.around(summary,3)\nsummary","120d9f5c":"df[(df['Class']==1)]['Amount'].value_counts().head(10)","fee981dd":"#distribution of classes withr espect to time\nfig, (ax1, ax2) = plt.subplots(2, 1, sharex=True, figsize=(20,10))\n\nfraud_transactions = df.Time[df.Class == 1]\nnormal_transactions = df.Time[df.Class == 0]\n\nax1.hist(fraud_transactions, bins = 50, color='deeppink', edgecolor=\"black\")\nax1.set_xlim([min(fraud_transactions), max(fraud_transactions)])\nax1.set_title('Fraudulent Transactions', fontsize=15)\nax1.set_ylabel(\"Number of Transactions\",  fontsize=13)\n\nax2.hist(normal_transactions, bins = 50, color='deepskyblue', edgecolor=\"black\")\nax2.set_xlim([min(normal_transactions), max(normal_transactions)])\nax2.set_title('Normal Transactions',  fontsize=15)\n\nax2.set_xlabel('Time (in Seconds)',  fontsize=13)\nax2.set_ylabel('Number of Transactions',  fontsize=13)\n\nplt.show()","dd85451d":"# converting seconds to time delta to extract hours and mins\n\ntimedelta = pd.to_timedelta(df['Time'], unit='s')\n\ndf['mins'] = (timedelta.dt.components.minutes).astype(int)\ndf['hours'] = (timedelta.dt.components.hours).astype(int)","b864d8c5":"# Countplots for hours vs count of transactions\n\nfig, axs = plt.subplots(3, figsize=(12,20))\n\nfig.subplots_adjust(hspace=.5)\n\nsns.countplot(df['hours'], ax = axs[0], palette=\"Pastel1\")\naxs[0].set_title(\"Distribution of Total Transactions\",fontsize=20)\naxs[0].set_facecolor(\"black\")\n\nsns.countplot(df[(df['Class'] == 1)]['hours'], ax=axs[1], palette='Pastel2')\naxs[1].set_title(\"Distribution of Fraudulent Transactions\", fontsize=20)\naxs[1].set_facecolor('black')\n\nsns.countplot(df[(df['Class'] == 0)]['hours'], ax=axs[2], palette='Set3')\naxs[2].set_title(\"Distribution of Normal Transactions\", fontsize=20)\naxs[2].set_facecolor(\"black\")\n\nplt.show()","9c65e999":"# Scatter plot of Class vs Amount and Time for Normal Transactions \n\nplt.figure(figsize=(20,8))\n\nfig = plt.scatter(x=df[df['Class'] == 0]['Time'], y=df[df['Class'] == 0]['Amount'], color=\"dodgerblue\", s=50, edgecolor='black')\nplt.title(\"Time vs Transaction Amount in Normal Transactions\", fontsize=20)\nplt.xlabel(\"Time (in seconds)\", fontsize=13)\nplt.ylabel(\"Amount of Transaction\", fontsize=13)\n\nplt.show()","9e19ec04":"plt.figure(figsize=(20,8))\n\nfig = plt.scatter(x=df[df['Class'] == 1]['Time'], y=df[df['Class'] == 1]['Amount'], color=\"c\", s=80)\nplt.title(\"Time vs Transaction Amount in Fraud Cases\", fontsize=20)\nplt.xlabel(\"Time (in seconds)\", fontsize=13)\nplt.ylabel(\"Amount of Transaction\", fontsize=13)\n\nplt.show()","cbfe184d":"import warnings \nwarnings.filterwarnings('ignore')\nimport matplotlib.gridspec as gridspec\n#Looking the V's features\ncolumns = df.iloc[:,1:29].columns\n\nfrauds = df.Class == 1\nnormals = df.Class == 0\n\ngrid = gridspec.GridSpec(14, 2)\nplt.figure(figsize=(20,20*4))\n\nfor n, col in enumerate(df[columns]):\n    ax = plt.subplot(grid[n])\n    sns.distplot(df[col][frauds], color='deeppink', kde_kws={\"color\": \"k\", \"lw\": 1.5},  hist_kws=dict(alpha=1)) \n    sns.distplot(df[col][normals],color='darkturquoise', kde_kws={\"color\": \"k\", \"lw\": 1.5},  hist_kws=dict(alpha=1))\n    ax.set_ylabel('Density', fontsize=13)\n    ax.set_title(str(col), fontsize=20)\n    ax.set_xlabel('')\nplt.show()","ceb5ce20":"# Finding the 3rd and 1st Quantile for Amount Column\n\nQ3 = np.percentile(df['Amount'], 75)\nQ1 = np.percentile(df['Amount'], 25)\n\n# setting the cutoff\ncutoff = 5.0\n\n# computing the interquartile range\nIQR = (Q3 - Q1)\n\n# computing lower bound and upper bound\nlower_bound = Q1 - (IQR * cutoff)\nupper_bound = Q3 + (IQR * cutoff)\n\n# creating a filter to remove values less than lower bound and greater than\n# upper bound\nfilter_data = (df['Amount'] < lower_bound) | (df['Amount'] > upper_bound)\n\n# filtering data\noutliers = df[filter_data]['Amount']\nfraud_outliers = df[(df['Class'] == 1) & filter_data]['Amount']\nnormal_outliers = df[(df['Class'] == 0) & filter_data]['Amount']\n\nprint(f\"Total Number of Outliers : {outliers.count()}\")\nprint(f\"Number of Outliers in Fraudulent Class : {fraud_outliers.count()}\")\nprint(f\"No of Outliers in Normal Class : {normal_outliers.count()}\")\nprint(f\"Percentage of Fraud amount outliers : {round((fraud_outliers.count()\/outliers.count())*100,2)}%\")","2b8b0f31":"# dropping the outliers\n\ndata = df.drop(outliers.index)\ndata.reset_index(inplace=True, drop=True)","1086c9fd":"data.head()","16a06c47":"data.shape","85259dd7":"data['Amount'] = np.log(data['Amount'] + 0.001)","48bf91fd":"# Box Plot for transformed Amount feature with class\n\nplt.figure(figsize=(12,8))\nsns.boxplot(x =\"Class\", y=\"Amount\", data=data, palette='Set1');\nplt.xlabel(\"Amount\", fontsize=13)\nplt.ylabel(\"Class\", fontsize=13)\nplt.title(\"Box Plot for Transformed Amount Feature\", fontsize=20);","f7e4a133":"from imblearn.over_sampling import SMOTE\nfrom sklearn.preprocessing import RobustScaler\nfrom sklearn.manifold import TSNE\nfrom sklearn.model_selection import train_test_split, RandomizedSearchCV, StratifiedKFold\nfrom sklearn.linear_model import LogisticRegression \nfrom sklearn.metrics import classification_report,accuracy_score, roc_curve, precision_score\nfrom sklearn.metrics import confusion_matrix, recall_score, roc_auc_score, f1_score, precision_recall_curve\nfrom sklearn.model_selection import cross_val_score\nfrom imblearn.pipeline import make_pipeline\nfrom sklearn.ensemble import ExtraTreesClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.ensemble import IsolationForest","4ecd759d":"# scaling the time column\n\nrobust_scaler = RobustScaler()\ndata['Time'] = robust_scaler.fit_transform(data['Time'].values.reshape(-1,1))","e1b36980":"# Divide into X and Y after removing useless columns\n\nX = data.drop(['Class','hours','mins'], 1)\nY = data.Class","5435b82a":"# Apply SMOTE\nfrom collections import Counter\nprint(f'Original dataset shape : {Counter(Y)}')\n\nsmote = SMOTE(random_state=42)\nX_res, y_res = smote.fit_resample(X, Y)\n\nprint(f'Resampled dataset shape {Counter(y_res)}')","a90652f1":"# creating a random sample of 5000 points \n\nX_vis = X_res.sample(5000, random_state=42)\ny_vis = y_res.sample(5000, random_state=42)\n\nprint(X_vis.shape)\nprint(y_vis.shape)","9ed6dbe8":"# training the t-SNE model to reduce dimensionality\n# to 3\n\ntsne3d = TSNE(\n    n_components=3,\n    random_state=42,\n    verbose=2,\n).fit_transform(X_vis)","fbce1dd6":"import plotly.graph_objs as go\nimport plotly\nimport seaborn as sns\nimport plotly.express as px\nimport plotly.figure_factory as ff\nimport plotly.offline as py\npy.init_notebook_mode(connected=True)\n# plotting a 3D scatter plot \n\ntrace1 = go.Scatter3d(\n    x=tsne3d[:,0],\n    y=tsne3d[:,1],\n    z=tsne3d[:,2],\n    mode='markers',\n    marker=dict(\n        color = y_vis,\n        colorscale = ['deeppink', 'deepskyblue'],\n        colorbar = dict(title = 'Fraud'),\n        line=dict(color='rgb(255, 255, 255)'),\n        opacity=0.5\n    )\n)\n\ndata=[trace1]\n\nlayout = dict(title = 'TSNE (T-Distributed Stochastic Neighbourhood Embedding)',\n              showlegend= False, height=800, width=800,)\n\nfig=dict(data=data, layout=layout)\npy.iplot(fig, filename='3DBubble')","47016eae":"# creating instance of statrifiedkfold split for 5 splits \nstrat = StratifiedKFold(n_splits=5, random_state=None, shuffle=False)\n\n# splitting the data\nfor train_index, test_index in strat.split(X, Y):\n    print(\"Train:\", train_index, \"Test:\", test_index)\n    original_Xtrain, original_Xtest = X.iloc[train_index], X.iloc[test_index]\n    original_ytrain, original_ytest = Y.iloc[train_index], Y.iloc[test_index]","24e1c65c":"# Turning the splits into an array\n\noriginal_Xtrain = original_Xtrain.values\noriginal_Xtest = original_Xtest.values\noriginal_ytrain = original_ytrain.values\noriginal_ytest = original_ytest.values","ceb06ce0":"# Creating a utility function to plot correlation matrix and roc_auc_curve\n\ndef show_metrics(model, y_test, y_pred):\n    fig = plt.figure(figsize=(20, 8))\n\n    # Confusion matrix\n    ax = fig.add_subplot(121)\n    sns.heatmap(confusion_matrix(y_test, y_pred), annot=True, annot_kws={\"size\": 16}, fmt='g', \n                cmap='Set3', linewidths=1, linecolor='white')\n\n    \n    # labels, title and ticks\n    ax.set_xlabel('Predicted labels', fontsize=15);\n    ax.set_ylabel('True labels', fontsize=15); \n    ax.set_title(\"Confusion Matix\", fontsize=20) \n    ax.xaxis.set_ticklabels(['No Fraud', 'Fraud'], fontsize=12); \n    ax.yaxis.set_ticklabels(['Fraud', 'No Fraud'], fontsize=12);\n\n    # ROC Curve\n    fig.add_subplot(122)\n    \n    \n    auc_roc = roc_auc_score(y_test, model.predict(original_Xtest))\n    fpr, tpr, thresholds = roc_curve(y_test, model.predict_proba(original_Xtest)[:,1])\n\n    plt.plot(fpr, tpr, color='darkturquoise', lw=2, marker='o', label='Trained Model (area = {0:0.3f})'.format(auc_roc))\n    plt.plot([0, 1], [0, 1], color='deeppink', lw=2, linestyle='--', label= 'No Skill (area = 0.500)')\n    plt.xlim([0.0, 1.0])\n    plt.ylim([0.0, 1.05])\n    plt.xlabel('False Positive Rate', fontsize=13)\n    plt.ylabel('True Positive Rate', fontsize=13)\n    plt.title('Receiver operating characteristic', fontsize=20)\n    plt.legend(loc=\"lower right\")\n    plt.show()","5279767e":"# List to append the score and then find the average\naccuracy_lst = []\nprecision_lst = []\nrecall_lst = []\nf1_lst = []\nauc_lst = []\n\n# specifying the parameter grid for logistic regression\nlog_reg_params = {'C': [0.001, 0.01, 0.1, 1, 10, 100, 1000]}\n\n# Applying RandomsearchCV to find best model\nrand_log_reg = RandomizedSearchCV(LogisticRegression(), log_reg_params, n_iter=4)\n\n\n# iterating over all the splits\nfor train, test in strat.split(original_Xtrain, original_ytrain):\n    \n    # create pipeline with smote and the model \n    # sampling_strategy = minority because we want to only resample the minority class\n    pipeline = make_pipeline(SMOTE(sampling_strategy='minority'), rand_log_reg)\n    \n    # fit the pipeline\n    model = pipeline.fit(original_Xtrain[train], original_ytrain[train])\n    best_est = rand_log_reg.best_estimator_\n    prediction = best_est.predict(original_Xtrain[test])\n    \n    # finding mean for all the necessary measures to evaluate performance\n    accuracy_lst.append(pipeline.score(original_Xtrain[test], original_ytrain[test]))\n    precision_lst.append(precision_score(original_ytrain[test], prediction))\n    recall_lst.append(recall_score(original_ytrain[test], prediction))\n    f1_lst.append(f1_score(original_ytrain[test], prediction))\n    auc_lst.append(roc_auc_score(original_ytrain[test], prediction))\n","877f120b":"print(\"Accuracy: {0:0.2f}%\".format(np.mean(accuracy_lst)*100))\nprint(\"Precision: {0:0.2f}\".format(np.mean(precision_lst)))\nprint(\"Recall: {0:0.2f}\".format(np.mean(recall_lst)))\nprint(\"f1 Score: {0:0.2f}\".format(np.mean(f1_lst)))","17669ddc":"# predict on test set\n\ny_pred = best_est.predict(original_Xtest)","a7e28ddf":"# plot confusion matrix and ROC curve\n\nshow_metrics(best_est, original_ytest, y_pred)","8840ac13":"# Random forest Classifier\nrf_cfl = RandomForestClassifier(n_estimators = 200, \n                                 max_features = 3, \n                                 min_samples_leaf = 1, \n                                 min_samples_split = 2, \n                                 n_jobs = -1,\n                                random_state = 42)\n\nrf_cfl.fit(original_Xtrain, original_ytrain)\ny_pred = rf_cfl.predict(original_Xtest)\nshow_metrics(rf_cfl, original_ytest, y_pred)","5fb27ddf":"print('Accuracy :{0:0.5f}'.format(accuracy_score(y_pred , original_ytest))) \nprint('AUC : {0:0.5f}'.format(roc_auc_score(original_ytest , y_pred)))\nprint('Precision : {0:0.5f}'.format(precision_score(original_ytest , y_pred)))\nprint('Recall : {0:0.5f}'.format(recall_score(original_ytest , y_pred)))\nprint('F1 : {0:0.5f}'.format(f1_score(original_ytest , y_pred)))","7b341fc1":"Random Forest Classifier\n\u25ba Now, let's try something which can take account of complex realationships. There are many such models but Random forest is bit better as it is a ensemble model and focuses on reducing variance i.e overfitting without much effecting the bias which is all we want. Also, this algorithm works in time complexity, O(d.n.log(n)) where d is the number of features.\n\n\u25ba I have shown the best parameters after GridsearchCV and not the whole process itself as it is very time consuming and takes forever so you can try it yourself.","f69c7b44":"Logistic Regression\n\u25ba Let's start off with a simple model like Linear Regression. Note that I will be doing cross validation using Randomized search as the data is very huge and we will do this cross validation after splitting to avoid Data Leakage as discussed above.","fcc2f238":"Precision and accuracy are good but the recall is pretty low due the which the model is not performing well on fraudulent data.","1d7ce4c4":"FRAUDULENT\n\u25ba Mean transaction is around 122 and standard deviation is around 256.\n\n\u25ba Maximum Transaction was 2125 and minimum was 0.\n\nNORMAL\n\u25ba Mean transaction is around 88 and standard deviation is around 250.\n\n\u25ba Maximum Transaction was 25691 and minimum was 0.","d6951fe0":"Let's do some feature engineering on time and transform it to minutes and hours to uncover some of the hidden patterns.","4681fae9":"Handling Class Imbalance\n\u25ba Imbalanced data is a problem in supervised learning problems which can result is high bias towards majority class. As we have already seen that this data is severly imbalanced so to balance it we can use various techniques such as:\n\nOversampling\nUndersampling\nSMOTE\n\u25ba Out of all these three SMOTE is the most effective so we will go with it, In this technique, instead of simply duplicating data from the minority class, we synthesize new data from the minority class. This is a type of data augmentation for tabular data can be very effective. This approach to synthesizing new data is called the Synthetic Minority Oversampling TEchnique, or SMOTE for short.","5a370b9b":"Modelling\n\u25ba In this section, we will finally apply models and classify whether a certain transaction done a particular time is fraud or geniune. Thus, this is a binary classification problem.\n\n\u25ba Important thing to note here is that we did SMOTE but we won't use that data, Why?\n\nIf we used that data to predict the classes then it will result in a problem know as 'Data Leakage' which is another term for using test data for prediction or cross validation. So, this sounds like a good point to use Pipelines. Pipelines make our life easier by specifying what order should the operations be done on the data.\n\n\u25ba One thing we should keep in mind that we might get very high accuracy but we should focus on optimising out f1_score and recall as we want to perform better on fraud cases as they are the most important.","abbf2d33":"As data is completed balanced now. we will try to visulaise the features using dimensionality reduction techniques","42cdd2a5":"it is able to seperate almost 80% of the sample data ","6ff96850":"This dataset has 492 frauds out of 284,315 transactions. Thus, the dataset is highly unbalanced, the positive class (frauds) account for 0.173% of all transactions.","5bdc3662":"FRAUDULENT\n\u25ba There are much more outliers as compared to normal transactions.\n\n\u25ba The plot seems to not have any inherent pattern.\n\n\nNORMAL\n\u25ba There are a less number of outliers as compared to fraudulent transactions.\n\n\u25ba There are a lot of transactions with amount less than 5000.","7314443d":"Due to confidentiality issue, original features V1, V2,... V28 have been transformed using PCA, however, my guess is that these features might be credit card number, expiry date, CVV, cardholder name, transaction location, transaction date-time, etc.\n\n\u25ba Only features which have not been transformed with PCA are 'Time',  'Amount' and 'Class'.\n\n'Time' contains the seconds elapsed between each transaction and the first transaction in the dataset.\n\nThe feature 'Amount' is the transaction Amount, this feature can be used for example-dependant cost-senstive learning.\n\nThe Feature 'Class' is the response or target variable and it takes value 1 in case of fraud and 0 otherwise.","7e243df5":"There are 113 fraud transactions for just one dollar and 27 fraud transaction for 99.99 dollars. Also, there are 27 fraud transaction for zero amount.\n\n\u25ba The reason for zero transaction can be the Zero Authorization which is an account verification method for credit cards that is used to verify a cardholders information without charging the consumer.","62fb8654":"V17, V14, V12 and V10 are negatively correlated. Notice how the lower these values are, the more likely the end result will be a fraud transaction.\n\n\u25ba V2, V4, V11, and V19 are positively correlated. Notice how the higher these values are, the more likely the end result will be a fraud transaction.","f3d26efc":"Outlier Removal\n\u25ba As we already saw that amount column has a extreme outliers so it necessary to remove them as they can effect the model's performance. We will used Interquartile range to detect outliers which removes anything below the lower limit (25 percentile) and anything above upper limit (75 Percentile).\n\n\u25ba Note that, the data we have for fraudulent cases is very low so we wanna keep our cutoff a bit high so as avoid removing much of the fraud cases. Here, as the data is skewed (kind of exponential) so having high cutoff will help us. Let's take the cutoff value as 5.0 instead of 1.5 which is usually used.","afad1afa":"As the amount column is highly skewed so it will be better to apply log transoformation as it can result in nearly normal distribution which is suited for most of the algorithms.","c684a658":"\u25ba For some of the features we can observe a good selectivity in terms of distribution for the two values of Class: V4, V11 have clearly separated distributions for Class values 0 and 1, V12, V14, V18 are partially separated, V1, V2, V3, V10 have a quite distinct profile, whilst V20-V28 have similar profiles for the two values of Class and thus not very useful in differentiation of both the classes.\n\n\u25ba In general, with just few exceptions (Time and Amount), the features distribution for legitimate transactions (values of Class = 0) is centered around 0, sometime with a long queue at one of the extremities. In the same time, the fraudulent transactions (values of Class = 1) have a skewed (asymmetric) distribution.","457e9397":"Mean transaction is somewhere is 88 and standard deviation is around 250.\n\n\u25ba The median is 22 which is very less as compared to mean which signifies that there are outliers or our data is highly positive skewed which is effecting the amount and thus the mean. High Skewness can be handled by using log transformation or boxcox transformation.\n\n\u25ba The maximum transaction that was done is of 25,691 and minimum is 0."}}