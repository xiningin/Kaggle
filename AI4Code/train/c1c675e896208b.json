{"cell_type":{"728dadf8":"code","c7cf2b75":"code","b608439b":"code","2c5d64ce":"code","65649322":"code","236d419f":"code","fc17b073":"code","6fcb11b9":"code","63d2dbc0":"code","2fa1f0da":"code","f567c608":"code","8865d5a2":"code","92294e99":"code","ff80576e":"code","3ca0118b":"code","313eecff":"code","8b020a1b":"code","8de42db3":"code","e70cea0e":"code","1f178f3d":"code","daf87b0a":"code","cfbcb14c":"code","fce7ad11":"code","412049e8":"code","a53b7231":"markdown","527af18d":"markdown","19538ab8":"markdown","59a89e85":"markdown","450fc4ca":"markdown","2380496a":"markdown","fc4ef41c":"markdown","a8356668":"markdown","70fbc228":"markdown","445459fb":"markdown","38fe823d":"markdown"},"source":{"728dadf8":"import numpy as np\nimport pandas as pd\nimport random\nimport itertools\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.datasets import load_digits, load_boston\nfrom sklearn.neural_network import MLPRegressor, MLPClassifier\nfrom sklearn.model_selection import train_test_split ,GridSearchCV\nfrom sklearn.metrics import confusion_matrix,r2_score\nimport warnings \nwarnings.filterwarnings('ignore')\n%matplotlib inline\n","c7cf2b75":"#Digits Dataset\ndigits = load_digits()\nx_digits = digits.data\ny_digits = digits.target\nprint(\"x_digits Datasets size\",x_digits.shape,\"\\n y_digits Datasets size\",y_digits.shape)","b608439b":"# Display Images\nfig, axes = plt.subplots(2,5,figsize=(10,10),subplot_kw = {'xticks':[],'yticks':[]})\nfor i,ax in enumerate(axes.flat):\n    ax.imshow(digits.images[i], cmap='gray', interpolation='nearest') \n    ax.text(0.5,-0.2,str(digits.target[i]),transform = ax.transAxes)\n","2c5d64ce":"x_train,x_test,y_train,y_test = train_test_split(x_digits,y_digits,test_size=0.2,random_state=42,stratify= y_digits)\nprint(\"Train size: \",x_train.shape,y_train.shape,\"Test Size: \",x_test.shape,y_test.shape )","65649322":"mlp_clf = MLPClassifier(random_state=42)\nmlp_clf.fit(x_train,y_train)","236d419f":"y_preds = mlp_clf.predict(x_test)\nprint(y_test[:20])\nprint(y_preds[:20])","fc17b073":"print(\"Train Accuracy: \", mlp_clf.score(x_train,y_train))\nprint(\"Test Accuracy: \", mlp_clf.score(x_test,y_test))\nprint(\"Loss : \", mlp_clf.loss_)","6fcb11b9":"con_max= confusion_matrix(y_test,y_preds)\nplt.figure(figsize = (9,9))\nsns.heatmap(con_max,annot= True,square= True,cbar= False,cmap='YlOrBr')\nplt.xlabel(\"Predicted Value\")\nplt.ylabel('Actual_value')\nplt.show()\n","63d2dbc0":"print(\"Number of Iterations: \",mlp_clf.n_iter_)\nprint(\"Output Layer Activation Function :\", mlp_clf.out_activation_)","2fa1f0da":"%%time\nparams = {'activation': ['relu', 'tanh', 'logistic', 'identity','softmax'],\n          'hidden_layer_sizes': [(100,), (50,100,), (50,75,100,)],\n          'solver': ['adam', 'sgd', 'lbfgs'],\n          'learning_rate' : ['constant', 'adaptive', 'invscaling']\n         }\n\nmlp_clf_grid = GridSearchCV(MLPClassifier(random_state=42), param_grid=params, n_jobs=-1, cv=5, verbose=5)\nmlp_clf_grid.fit(x_train,y_train)","f567c608":"print('Train Accuracy : ',mlp_clf_grid.best_estimator_.score(x_train,y_train))\nprint('Test Accuracy : ',mlp_clf_grid.best_estimator_.score(x_test, y_test))\nprint('Grid Search Best Accuracy  :',mlp_clf_grid.best_score_)\nprint('Best Parameters : ',mlp_clf_grid.best_params_)\nprint('Best Estimators: ',mlp_clf_grid.best_estimator_)","8865d5a2":"y_preds = mlp_clf_grid.best_estimator_.predict(x_test)\ncon_max= confusion_matrix(y_test,y_preds)\nplt.figure(figsize = (9,9))\nsns.heatmap(con_max,annot= True,square= True,cbar= False,cmap='Pastel1')\nplt.xlabel(\"Predicted Value\")\nplt.ylabel('Actual_value')\nplt.show()","92294e99":"clf_model = MLPClassifier(activation = 'logistic', hidden_layer_sizes= (100,), learning_rate = 'constant', solver = 'adam')\nclf_model.fit(x_train,y_train)\ny_preds = clf_model.predict(x_test)\nprint(\"Loss: \",clf_model.loss_)\nprint(\" Score is \",clf_model.score(x_test,y_test))","ff80576e":"from sklearn.datasets import load_boston\nboston = load_boston()\nx_boston = boston.data\ny_boston = boston.target\nprint(\"Dataset Sizes \",x_boston.shape,y_boston.shape)","3ca0118b":"# Spliting dataset into train and test dataset\nx_train,x_test,y_train,y_test = train_test_split(x_boston,y_boston,test_size = 0.25, random_state = 42)","313eecff":"# import the regressor\nfrom sklearn.neural_network import MLPRegressor\nreg = MLPRegressor(random_state  = 42)\nreg.fit(x_train,y_train)","8b020a1b":"y_preds = reg.predict(x_test)\n\nprint(y_preds[:5])\nprint(y_test[:5])\n\nprint(\"Train Score\",reg.score(x_train,y_train))\nprint(\"Test Score\" , reg.score(x_test,y_test))\n","8de42db3":"print(\"Loss:\",reg.loss_)","e70cea0e":"print(\"Number of Coefficents :\", len(reg.coefs_))\n[weights.shape for weights in reg.coefs_]\n","1f178f3d":"print(\"Number of intecepts :\",len(reg.intercepts_))\n[intercepts.shape for intercepts in reg.intercepts_]","daf87b0a":"print(\"Number of iterations estimators run: \", reg.n_iter_)\nprint(\"name of output layer activation function: \", reg.out_activation_)","cfbcb14c":"%%time\nreg= MLPRegressor(random_state = 42)\nparams= {'activation': ['relu','identity','tanh','logistic'],\n        'hidden_layer_sizes': [50,100,150] + list(itertools.permutations([50,100,150],2)) + list(itertools.permutations([50,100,150],3)),\n         'solver' : ['lbfgs','adam'],\n         'learning_rate': ['constant','adaptive','invscaling']\n        }\n\nreg_grid = GridSearchCV(reg,param_grid = params,n_jobs= -1,verbose = 10,cv=5)\nreg_grid.fit(x_train,y_train)\n\n","fce7ad11":"print(\"Train score: \", reg_grid.score(x_train,y_train))\nprint(\"Test score: \", reg_grid.score(x_test,y_test))\nprint(\"Best R2 Score by grid search: \",reg_grid.best_score_)\nprint(\"Best Parameters: \", reg_grid.best_params_)\nprint(\"Best Estimators: \",reg_grid.best_estimator_)","412049e8":"reg_model = MLPRegressor(activation = 'relu', hidden_layer_sizes = (150, 50, 100), learning_rate= 'constant', solver= 'adam',random_state = 42)\nreg_model.fit(x_train,y_train)\ny_preds = reg_model.predict(x_test)\nprint(\"Loss: \",reg_model.loss_)\nprint(\"R2 Score is \",r2_score(y_test,y_preds))","a53b7231":"### Introduction \n\nMulti-Layer Perceptron (MLP) is a  most common neural network. MLP is a function that maps input to output. MLP has a single input layer, a single output layer. In between, there can be one or more hidden layers. The input layer has the same set of neurons as that of features. Hidden layers can have more than one neuron as well. Each neuron is a linear function to which activation function is applied to solve complex problems. The output from each layer is given as input to all neurons of the next layers.","527af18d":"## GridSearchCV","19538ab8":"### Datasets\n\nFor classification we will use Digits dataset which has 8*8 size images for 0-9 digits. ","59a89e85":"## Regression","450fc4ca":"## Classification","2380496a":"### Finetuning Model By Doing Grid Search \n\n\n","fc4ef41c":"#### Dataset\nFor regression we will use boston housing dataset.","a8356668":"Updating.........","70fbc228":"### MLPRegressor\n`MLPRegressor`is an estimator available as a part of the `neural_network` module of sklearn for performing regression tasks using a multi-layer perceptron.","445459fb":"`Scikit-learn` has **MLPRegressor** for regression problems and **MLPClassifier** for classification problems.","38fe823d":"### Finetuning Model By  Grid Search On Various Hyperparameters.\n\nBelow is a list of common hyperparameters that needs tuning for getting the best fit for our data. We'll try various hyperparameters settings to various splits of train\/test data to find out best fit which will have almost the same accuracy for both train & test dataset or have quite less difference between accuracy.\n\n- **hidden_layer_sizes** - It accepts tuple of integer specifying sizes of hidden layers in multi layer perceptrons. According to size of tuple, that many perceptrons will be created per hidden layer. `default=(100,)`\n\n- **activation** - It specifies activation function for hidden layers. It accepts one of below strings as input. default=relu\n - 'identity' - No Activation. f(x) = x\n - 'logistic' - Logistic Sigmoid Function. f(x) = 1 \/ (1 + exp(-x))\n - 'tanh' - Hyperbolic tangent function. f(x) = tanh(x)\n - 'relu' - Rectified Linear Unit function. f(x) = max(0, x)\n\n- **solve**r - It accepts one of below strings specifying which optimization solver to use for updating weights of neural network hidden layer perceptrons. default='adam'\n'lbfgs'\n'sgd'\n'adam'\n\n- **learning_rate_init** - It specifies initial learning rate to be used. Based on value of this parameter weights of perceptrons are updated.default=0.001\n\n- **learning_rate** - It specifies learning rate schedule to be used for training. It accepts one of below strings as value and only applicable when solver='sgd'.\n - 'constant' - Keeps learning rate constant through a learning process which was set in learning_rate_init.\n - 'invscaling' - It gradually decreases learning rate. `effective_learning_rate = learning_rate_init \/ pow(t, power_t) ` \n - 'adaptive' - It keeps learning rate constant as long as loss is decreasing or score is improving. If consecutive epochs fails in decreasing loss according to tol parameter and early_stopping is on, then it divides current learning rate by 5.\n- **batch_size** - It accepts integer value specifying size of batch to use for dataset. default='auto'. The default auto batch size will set batch size to min(200, n_samples).\n- **tol** - It accepts float values specifying threshold for optimization. When training loss or score is not improved by at least tol for n_iter_no_change iterations, then optimization ends if learning_rate is constant else it decreases learning rate if learning_rate is adaptive. default=0.0001\n- **alpha** - It specifies L2 penalty coefficient to be applied to perceptrons. default=0.0001\n- **momentum** - It specifies momentum to be used for gradient descent and accepts float value between 0-1. It's applicable when solver is sgd.\n- **early_stopping** - It accepts boolean value specifying whether to stop training if training score\/loss is not improving. default=False\n- **validation_fraction** It accepts float value between 0-1 specifying amount of training data to keep aside if early_stopping is set.default=0.1"}}