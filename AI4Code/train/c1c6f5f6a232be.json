{"cell_type":{"abfa18c3":"code","032b81ca":"code","51dc0553":"code","54e61756":"code","165f38ba":"code","bcb34db6":"code","5c065e0d":"code","6ea5385d":"code","cb94f6fb":"code","361bcbf3":"code","c56b4960":"code","992f7947":"code","89f30e31":"code","e9bdace1":"code","ef79fe5a":"code","1138f58a":"code","89ec94e2":"code","768eac33":"code","2b28916e":"code","252c47a4":"code","ebf47605":"code","e2989d97":"code","1ffc0443":"code","c20ce864":"code","9ed43bc6":"code","7d8ef420":"markdown","006d87e5":"markdown","91d696e9":"markdown","70cac95e":"markdown","6764aa84":"markdown","89da8600":"markdown","54ab181f":"markdown","bd41da95":"markdown","ab99e4eb":"markdown","d0893c75":"markdown","775a375a":"markdown","8437d240":"markdown","f0e6699f":"markdown","f5d307d1":"markdown","b564f01c":"markdown","ba7e1936":"markdown","56f21765":"markdown","863e8828":"markdown","0503d094":"markdown","b4d76af5":"markdown","9df84189":"markdown","88a03a19":"markdown","77daa943":"markdown","15fa4d14":"markdown","3e5652f9":"markdown"},"source":{"abfa18c3":"# import libraries\nimport numpy as np\nimport pandas as pd\nimport nltk\nfrom nltk.corpus import stopwords\nnltk.download('stopwords')\nimport string\nfrom nltk.tokenize import word_tokenize\nfrom sklearn.metrics import confusion_matrix\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.ensemble import AdaBoostClassifier,RandomForestClassifier\nimport xgboost\nfrom sklearn import svm,tree\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.metrics import confusion_matrix,classification_report,accuracy_score\n\n\nimport tensorflow as tf\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing import sequence\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom tensorflow.keras.layers import Embedding,Input,LSTM,Dense,Bidirectional,Dropout, Activation\nfrom keras.models import Model\nfrom tensorflow.keras.models import Sequential\ntf.__version__\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n","032b81ca":"# Load the data\ndf = pd.read_csv('..\/input\/spam-filter\/emails.csv')\ndf.head()","51dc0553":"# checking the number of duplicate columns\nprint('Number of duplicate rows in the data are : ',df[df.duplicated(subset=None, keep='first') == True].shape[0], '\\nSo we drop them')\n\n# dropping the duplicate columns\ndf.drop_duplicates(inplace = True)","54e61756":"# Describing the values in the Spam column\ndf.groupby('spam').describe()","165f38ba":"# creating a column with the length of each message\ndf['mail_len'] = df.text.apply(len)","bcb34db6":"plt.figure(figsize=(6,6))\n\ndf.spam[df.spam==1].plot(bins=4, kind='hist', color='blue', \n                                       label='Spam Mails', alpha=0.6)\n\ndf.spam[df.spam==0].plot(bins=4, kind='hist', color='red', \n                                       label='Ham Mails', alpha=0.6)\nplt.legend()\nplt.xlabel(\"Ham\/Spam\")","5c065e0d":"plt.style.use('seaborn-darkgrid')\nplt.figure(figsize=(10,5))\nsns.distplot(df['mail_len'],kde=True,color='red',hist=True)\nplt.xlabel(\"Message Length\",size=15)\nplt.ylabel(\"Frequency\",size=15)\nplt.title(\"Length Histogram\",size=15)","6ea5385d":"plt.figure(figsize=(12, 8))\ndf[df.spam==1].mail_len.plot( kind='hist', color='blue',label='Spam Mails', alpha=0.6)\ndf[df.spam== 0].mail_len.plot(kind='hist', color='red',label='Ham Mails', alpha=0.6)\nplt.legend()\nplt.xlabel(\"Mail Length\")","cb94f6fb":"#1.Punctuations are [!\"#$%&'()*+,-.\/:;<=>?@[\\]^_`{|}~]\n#2.Stop words in natural language processing, are useless words (data).\n\ndef process_text(text):\n    \n    #1 Remove Punctuationa\n    nopunc = [char for char in text if char not in string.punctuation]\n    nopunc = ''.join(nopunc)\n    \n    #2 Remove Stop Words\n    clean_words = [word for word in nopunc.split() if word.lower() not in stopwords.words('english')]\n    \n    #3 Return a list of clean words\n    return clean_words","361bcbf3":"#Show the processed data\ndf.text = df.text.apply(process_text)\ndf.text.head()\n","c56b4960":"vocab_size = 10000\nmax_len = 250\n\n# Tokenize the mails\ntok = Tokenizer(num_words=vocab_size)\ntok.fit_on_texts(df.text)\n\n# Use text_to_sequence to convert it into vectors\nsequences = tok.texts_to_sequences(df.text)\n\n# pad seqence to create a matrix of equal length mails\nsequences_matrix = sequence.pad_sequences(sequences,maxlen=max_len)","992f7947":"sequences_matrix[0]","89f30e31":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(sequences_matrix, df.spam, test_size = 0.2, random_state = 1)","e9bdace1":"models=[RandomForestClassifier(),\n        GaussianNB(),\n        AdaBoostClassifier(),\n        xgboost.XGBClassifier(),\n        svm.SVC(),\n        tree.DecisionTreeClassifier(),\n        KNeighborsClassifier()]\n\nmodel_names=['Random Forest Classifier',\n             'Gaussian Naive Bayes Classifier',\n             'Adaboost Classifier',\n             'XGBoost Classifier',\n             'Support Vector Classifier',\n             'Decision Tree Classifier',\n             'K Nearest Neighbour Classifier']\naccuracy=[]\nd={}\nfor model in range (len(models)):\n    clf=models[model]\n    clf.fit(X_train,y_train)\n    y_pred=clf.predict(X_test)\n    accuracy.append(accuracy_score(y_test,y_pred))\nd={'Modelling Algo':model_names,'Accuracy':accuracy} ","ef79fe5a":"accuracy_frame=pd.DataFrame.from_dict(d, orient='index').transpose()\naccuracy_frame","1138f58a":"from sklearn.model_selection import RandomizedSearchCV\n# Number of trees in random forest\nn_estimators = [int(x) for x in np.linspace(start = 200, stop = 2000, num = 10)]\n# Number of features to consider at every split\nmax_features = ['auto', 'sqrt']\n# Maximum number of levels in tree\nmax_depth = [int(x) for x in np.linspace(10, 110, num = 11)]\nmax_depth.append(None)\n# Minimum number of samples required to split a node\nmin_samples_split = [2, 5, 10]\n# Minimum number of samples required at each leaf node\nmin_samples_leaf = [1, 2, 4]\n# Method of selecting samples for training each tree\nbootstrap = [True, False]\n# Create the random grid","89ec94e2":"random_grid = {'n_estimators': n_estimators,\n               'max_features': max_features,\n               'max_depth': max_depth,\n               'min_samples_split': min_samples_split,\n               'min_samples_leaf': min_samples_leaf,\n               'bootstrap': bootstrap}","768eac33":"print(random_grid)","2b28916e":"# Use the random grid to search for best hyperparameters\n# First create the base model to tune\nrf = xgboost.XGBClassifier()\n# Random search of parameters, using 2 fold cross validation, \n# search across 5 different combinations, and use all available cores\nrf_random = RandomizedSearchCV(estimator = rf, param_distributions = random_grid, n_iter = 5, cv = 2, verbose=2, random_state=42, n_jobs = -1)\n# Fit the random search model\nrf_random.fit(X_train, y_train)","252c47a4":"rf_random.best_params_","ebf47605":"rf_random.best_estimator_","e2989d97":"rfc = rf_random.best_estimator_\nrfc.fit(X_train, y_train)\ny_pred1 = rfc.predict(X_test) \nprint(confusion_matrix(y_test,y_pred1))\nprint(accuracy_score(y_test,y_pred1))\nprint(classification_report(y_test,y_pred1))","1ffc0443":"model = Sequential()\nmodel.add(Embedding(vocab_size, 200, input_length=max_len))\nmodel.add(LSTM(32))\nmodel.add(Dense(1,activation='sigmoid'))\nmodel.summary()\nmodel.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])","c20ce864":"model.fit(X_train,y_train,validation_data=(X_test,y_test),epochs=5,batch_size=64)","9ed43bc6":"scores = model.evaluate(X_test, y_test, verbose=0)\ny_pred = model.predict_classes(X_test)\n\nprint('Test loss:', scores[0])\nprint('Test accuracy:', scores[1])\nprint('confusion matrix:\\n', confusion_matrix(y_pred,y_test))","7d8ef420":"https:\/\/en.wikipedia.org\/wiki\/Long_short-term_memory\n## What is LSTM and why it is used..\n\nLong short-term memory (LSTM) is an artificial recurrent neural network (RNN) architecture used in the field of deep learning.\n\nIn theory, classic (or \"vanilla\") RNNs can keep track of arbitrary long-term dependencies in the input sequences. The problem with vanilla RNNs is computational (or practical) in nature: when training a vanilla RNN using back-propagation, the gradients which are back-propagated can \"vanish\" (that is, they can tend to zero) or \"explode\" (that is, they can tend to infinity), because of the computations involved in the process, which use finite-precision numbers. RNNs using LSTM units partially solve the vanishing gradient problem, because LSTM units allow gradients to also flow unchanged.\nWe also have Bi-directional LSTM which overcomes the drwbacks of LSTM model","006d87e5":"Lets see the histogram of length of mails for both the labels in the same plot one over the other","91d696e9":"We can see that the ","70cac95e":"Evaluating the score with best parameters","6764aa84":"lets put all the models with their accuracies and compare to see whcih one has the highest score.","89da8600":"## Model Evaluation","54ab181f":"## So we can see that a simple LSTM model gives an accuracy of 0.98 whereas best ML model had just 0.89.","bd41da95":"Okay...So finally the data is ready for training.","ab99e4eb":"Lets see how the mails look like now.....","d0893c75":"## Exploratory Data Analysis","775a375a":"### Accuracy using XGBoost with best parameters does improve improved the accuracy score but its still not satisfactory\n","8437d240":"## So finally we have our machine ready....You feed the message and it will tell you whether its a SPAM or HAM\n\n<img src=\"https:\/\/digitalmarketingbypsk.files.wordpress.com\/2017\/05\/21.gif\" style=\"width:30%; float:center;\">\n","f0e6699f":"### Train Test Split","f5d307d1":"## Thank You........","b564f01c":"## Vectorization of the text data \n\nWe can not feed text data directly to the models. So we will vectorize each mail into a matrix by tokenizing it, then converting  into numerial vectors and finally padding it to create a matrix of numbers for each mail input.","ba7e1936":"We see that the initial rows have got bigger mails than the later ones","56f21765":"### Now lets use a simple single layered LSTM model","863e8828":"[https:\/\/machinelearningmastery.com\/use-word-embedding-layers-deep-learning-keras\/](http:\/\/)\n## Embedding layer - \nAlso we are using an embedding layer before giving the data to the LSTM layer\n\nThe Embedding layer is defined as the first hidden layer of a network. It must specify 3 arguments: \n1. input_dim: This is the size of the vocabulary in the text data. For example, if your data is integer encoded to values between 0-10, then the size of the vocabulary would be 11 words.\n2. output_dim: This is the size of the vector space in which words will be embedded. It defines the size of the output vectors from this layer for each word.\n3. input_length: This is the length of input sequences, as you would define for any input layer of a Keras model. For example, if all of your input documents are comprised of 1000 words, this would be 1000.\n\n","0503d094":"## Preprocessing of the text data\nIn preprocessing we will remove the punctuations and stopwords and lower case all the mails data","b4d76af5":"Plottin the distplot to see the distribution of mail length...bigger the mail lenght, higher the plot goes.","9df84189":"We cam see that the Ham mails are more almost 4 times in number than the spam mails","88a03a19":"Plotting the histogram of data for the count of Ham and Spam mails","77daa943":"<h1 style=\"font-size: 30px; margin-left:50px\">SPAM detector<\/h1>\n\n<img src=\"https:\/\/gifimage.net\/wp-content\/uploads\/2018\/05\/spam-gif-6.gif\" style=\"width:20%; float:center;\">\n","15fa4d14":"We will now train the data on the below ML Models.\nLets make a list of the classification models, fit them on training data and check for their respective accuracies.","3e5652f9":"XGBoost Classifier has the highest score, we will do hyperparameter tuning and see how much the accuracy improves"}}