{"cell_type":{"f32d0e22":"code","bc96d97a":"code","d53635e6":"code","4a01cd02":"code","5cd2afa7":"code","9ca4edc2":"code","32149ae1":"code","ea369d11":"code","4221fa8a":"code","444419b1":"code","263fdc6c":"code","1e8d4047":"code","1ceed5e9":"code","78f6b7c3":"code","042bb624":"code","bc4ce1f3":"code","d8045b60":"code","4923bbd3":"code","2f36c123":"code","2638f0af":"code","e549286e":"code","1189da00":"code","9f1d5240":"code","e7be38af":"code","803402d4":"code","d27731ab":"code","6e48bc1a":"code","bfc4b4b0":"code","4540baa7":"code","b86a729a":"code","12a979a5":"code","54150b33":"code","30de86f6":"code","47d6a64f":"code","c8c2e067":"code","70877e1f":"code","2f78c0aa":"code","8cfde335":"code","7b76dec9":"code","d3e5487d":"code","8f02fc87":"code","5064e8cd":"markdown","ec7abc41":"markdown","4b929836":"markdown","f680ccb3":"markdown","5e8b58b2":"markdown","c95c3723":"markdown","e99b047c":"markdown","923a23d0":"markdown","3136c216":"markdown","be8d1abb":"markdown","12b1f689":"markdown","9d6562cb":"markdown","4adf1f91":"markdown","f7551dd9":"markdown","974d1057":"markdown","a7ab9ca2":"markdown","84aa251c":"markdown","2bafd327":"markdown","42af6a32":"markdown","f0791b11":"markdown","f0c258b8":"markdown","b1ac0c84":"markdown","d6db36db":"markdown","e910cd1a":"markdown","29706228":"markdown","1f2346b7":"markdown","4f2fa3ab":"markdown","e1981036":"markdown","d169a3dd":"markdown","c8d9aec9":"markdown","4b59109c":"markdown","a061e061":"markdown","bf5922c6":"markdown","3c06c545":"markdown","3f225eda":"markdown","797d2551":"markdown","c34605b4":"markdown","c7444631":"markdown","5c2425fc":"markdown","9cd92fbf":"markdown","8a842565":"markdown","9424bbcd":"markdown","c0b7fe57":"markdown","87408482":"markdown","5367de82":"markdown","bb813dfd":"markdown","e7dada52":"markdown","6de537d5":"markdown","37abb1a6":"markdown","34dd11f6":"markdown","528de136":"markdown","6749a4b1":"markdown","74b8cbb9":"markdown"},"source":{"f32d0e22":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","bc96d97a":"import numpy as np\nimport pandas as pd\nimport os\nimport cv2\nfrom matplotlib import pyplot as plt\nimport albumentations\nimport seaborn as sns\nfrom matplotlib import rcParams","d53635e6":" temp = dir(albumentations)","4a01cd02":"df = pd.read_csv(\"\/kaggle\/input\/cassava-leaf-disease-classification\/train.csv\")\ndf.head()","5cd2afa7":"df[\"disease\"] = df.label.map({0:\"Cassava Bacterial Blight (CBB)\",\n1:\"Cassava Brown Streak Disease (CBSD)\",\n2:\"Cassava Green Mottle (CGM)\",\n3:\"Cassava Mosaic Disease (CMD)\",\n4:\"Healthy\"})","9ca4edc2":"df.head()","32149ae1":"rcParams[\"figure.figsize\"] = 30,10\nsns.countplot(data = df,x=\"disease\",hue=\"label\")","ea369d11":"image = cv2.imread('..\/input\/cassava-leaf-disease-classification\/train_images\/1000723321.jpg', 1)  # BGR\nprint(image.shape)\nimage = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)","4221fa8a":"plt.title(\"Original Image without any Augmentations\")\nplt.imshow(image)","444419b1":"randomResizeCrop = albumentations.RandomResizedCrop(height=512,width=512)\nval = randomResizeCrop(image=image)\nimg1 =val[\"image\"]","263fdc6c":"plt.title(\"After Random Resized Crop\")\nplt.imshow(img1)","1e8d4047":"Blur = albumentations.Blur(blur_limit=10,p=0.3)\nval = Blur(image=image)\nimg2 = val[\"image\"]\nplt.title(\"After Blur\")\nplt.imshow(img2)","1ceed5e9":"centerCrop = albumentations.CenterCrop(height=512,width=512,p=0.9,always_apply=False)\nval = centerCrop(image=image)\nimg3 = val[\"image\"]\nplt.title(\"After CenterCrop\")\nplt.imshow(img3)","78f6b7c3":"channelShuffle = albumentations.ChannelShuffle(p=0.8,always_apply=False)\nval4 = channelShuffle(image=image)\nimg4 = val4[\"image\"]\nplt.title(\"After Channel Shuffle\")\nplt.imshow(img4)","042bb624":"colorJitter = albumentations.ColorJitter(brightness=0.4,contrast=0.5,saturation=0.5,hue=0.3,p=0.7,always_apply=False)\nval5 = colorJitter(image=image)\nimg5 = val5[\"image\"]\nplt.title(\"After Color Jitter\")\nplt.imshow(img5)","bc4ce1f3":"flip = albumentations.Flip(always_apply=False,p=0.9)\nval6 = flip(image=image)\nimg6 = val6[\"image\"]\nplt.title(\"After Flipping\")\nplt.imshow(img6)","d8045b60":"gaussianBlur = albumentations.GaussianBlur(blur_limit=(5,9),sigma_limit=3,always_apply=False,p=0.7)\nval7 = gaussianBlur(image=image)\nimg7 = val7[\"image\"]\nplt.title(\"After Gaussian Blur\")\nplt.imshow(img7)","4923bbd3":"medianBlur = albumentations.MedianBlur(blur_limit=15,\n                                       always_apply=False,p=0.9)\nval8 = medianBlur(image=image)\nimg8 = val8[\"image\"]\nplt.title(\"After Median Blur\")\nplt.imshow(img8)","2f36c123":"randomFog = albumentations.RandomFog(fog_coef_lower=0.7,fog_coef_upper=1,alpha_coef=0.8,always_apply=False,p=0.7)\nval9 = randomFog(image=image)\nimg9 = val9[\"image\"]\nplt.title(\"After Random Fog\")\nplt.imshow(img9)","2638f0af":"# randomSizedBBox = albumentations.RandomSizedBBoxSafeCrop(height=512,width=512,erosion_rate=0.3,interpolation=1,p=0.7)\n# val10 = randomSizedBBox(image=image)\n# img10 = val10[\"image\"]\n# plt.title(\"After Random Sized BBoxSafeCrop\")\n# plt.imshow(img10)","e549286e":"solarize = albumentations.Solarize(threshold=140,always_apply=False,p=0.7)\nval11 = solarize(image=image)\nimg11 = val11[\"image\"]\nplt.title(\"After Solarize\")\nplt.imshow(img11)","1189da00":"shiftScaleRotate = albumentations.ShiftScaleRotate(shift_limit=0.0625,scale_limit=0.1,rotate_limit=90,interpolation=1,border_mode=4,value=None,mask_value=None,\n                                                       shift_limit_x=None,shift_limit_y=None,always_apply=False,p=0.5)\nval12 = shiftScaleRotate(image=image)\nimg12 = val12[\"image\"]\nplt.title(\"After Shift Scale Rotate\")\nplt.imshow(img12)","9f1d5240":"hueSaturationValue = albumentations.HueSaturationValue(hue_shift_limit=40,sat_shift_limit=50,val_shift_limit=70,always_apply=False,p=0.5,)\nval13 = hueSaturationValue(image=image)\nimg13 = val13[\"image\"]\nplt.title(\"After Hue Saturation Value\")\nplt.imshow(img13)","e7be38af":"randomBrightnessContrast = albumentations.RandomBrightnessContrast(brightness_limit=0.5,contrast_limit=0.7,brightness_by_max=True,always_apply=False,p=0.5)\nval14 = randomBrightnessContrast(image=image)\nimg14 = val14[\"image\"]\nplt.title(\"After Random Brightness Contrast\")\nplt.imshow(img14)","803402d4":"normalize = albumentations.Normalize(mean=(0.485, 0.456, 0.406),std=(0.229, 0.224, 0.225),max_pixel_value=255.0,always_apply=False,p=1.0,)\nval15 = normalize(image=image)\nimg15 = val15[\"image\"]\nplt.title(\"After Normalize\")\nplt.imshow(img15)","d27731ab":"coarseDropout = albumentations.CoarseDropout(max_holes=16,max_height=6,max_width=20,min_holes=None,min_height=None,min_width=None,fill_value=0,mask_fill_value=None,always_apply=False,p=0.5,)\nval16 = coarseDropout(image=image)\nimg16 = val16[\"image\"]\nplt.title(\"After Coarse DropOut\")\nplt.imshow(img16)","6e48bc1a":"cutOut = albumentations.Cutout(num_holes=16,max_h_size=10,max_w_size=4,fill_value=0,always_apply=False,p=0.5,)\nval17 = cutOut(image=image)\nimg17 = val17[\"image\"]\nplt.title(\"After Cut Out\")\nplt.imshow(img17)","bfc4b4b0":"downscale = albumentations.Downscale(scale_min=0.15,scale_max=0.25,interpolation=0,always_apply=False,p=0.5,)\nval18 = downscale(image=image)\nimg18 = val18[\"image\"]\nplt.title(\"After Down Scale\")\nplt.imshow(img18)","4540baa7":"clahe = albumentations.CLAHE(clip_limit=4.0,tile_grid_size=(8, 8),always_apply=False,p=0.5,)\nval19 = clahe(image=image)\nimg19 = val19[\"image\"]\nplt.title(\"After Clahe\")\nplt.imshow(img19)","b86a729a":"iaaaffine = albumentations.IAAAffine(scale=3.0,translate_percent=None,translate_px=None,rotate=0.45,shear=0.0,order=1,cval=0,mode='reflect',always_apply=False,p=0.5,)\nval20 = iaaaffine(image=image)\nimg20 = val20[\"image\"]\nplt.title(\"After AIIIffine\")\nplt.imshow(img20)","12a979a5":"gaussNoise = albumentations.GaussNoise(var_limit=(50.0, 110.0),mean=4,always_apply=False,p=0.5,)\nval21 = gaussNoise(image = image)\nimg21 = val21[\"image\"]\nplt.title(\"After  Gauss Noise\")\nplt.imshow(img21)","54150b33":"refImag = [\"\/kaggle\/input\/cassava-leaf-disease-classification\/train_images\/3459977804.jpg\",\n            \"\/kaggle\/input\/cassava-leaf-disease-classification\/train_images\/1258625916.jpg\",\n            \"\/kaggle\/input\/cassava-leaf-disease-classification\/train_images\/2174460518.jpg\",\n            \"\/kaggle\/input\/cassava-leaf-disease-classification\/train_images\/4054194563.jpg\"]","30de86f6":"def red_img(path):\n    image = cv2.imread(path, 1)  # BGR\n    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n    return image","47d6a64f":"histogramMatching = albumentations.HistogramMatching(reference_images= refImag,read_fn = red_img,blend_ratio=(0.5, 1.0),always_apply=False,p=0.5,)\nval22 = histogramMatching(image=image)\nimg22 = val22[\"image\"]\nplt.title(\"After Histogram Matching\")\nplt.imshow(img22)","c8c2e067":"temp","70877e1f":"channelDropout = albumentations.ChannelDropout(channel_drop_range=(5,9),fill_value=2,always_apply=False,p=0.5,)\nval23 = channelDropout(image=image)\nimg23 = val23[\"image\"]\nplt.title(\"After Channel Dropout\")\nplt.imshow(img23)","2f78c0aa":"elasticTransform = albumentations.ElasticTransform(alpha=4,sigma=100,alpha_affine=50,interpolation=1,  border_mode=4,   value=None,\n                mask_value=None,always_apply=False,approximate=False,p=0.5,)\nval24 = elasticTransform(image=image)\nimg24 = val24[\"image\"]\nplt.title(\"After elasticTransform\")\nplt.imshow(img24)","8cfde335":"equalize = albumentations.Equalize(mode='cv',by_channels=True,mask=None,mask_params=(),always_apply=False,p=0.9,)\nval25 = equalize(image=image)\nimg25 = val25[\"image\"]\nplt.title(\"After equalize\")\nplt.imshow(img25)","7b76dec9":"fda = albumentations.FDA(reference_images=refImag,read_fn=red_img)\nval26 = fda(image=image)\nimg26 = val26[\"image\"]\nplt.title(\"After FDA\")\nplt.imshow(img26)","d3e5487d":"fancyPCA = albumentations.FancyPCA(alpha=2.3, always_apply=False, p=0.5)\nval27 = fancyPCA(image=image)\nimg27 = val27[\"image\"]\nplt.title(\"After Fancy PCA\")\nplt.imshow(img27)","8f02fc87":"randomFlare = albumentations.RandomSunFlare(flare_roi=(0, 0, 1, 0.5),angle_lower=0,angle_upper=1,num_flare_circles_lower=6,num_flare_circles_upper=10,\n                                            src_radius=600,src_color=(255, 255, 255),always_apply=False,p=0.5,)\nval28 = randomFlare(image=image)\nimg28 = val28[\"image\"]\nplt.title(\"After RandomSunFlare\")\nplt.imshow(img28)","5064e8cd":"## Normalize - Divide pixel values by 255 = 2**8 - 1, subtract mean per channel and divide by std per channel.","ec7abc41":"* blur_limit\n        maximum aperture linear size for blurring the input image.Must be odd and in range [3, inf). Default: (3, 7).","4b929836":"* Threshold - \n        range for solarizing threshold\n        ((int, int) or int, or (float, float) or float)","f680ccb3":"* clip_limit (float or (float, float)): \n        upper threshold value for contrast limiting.\n        If clip_limit is a single float value, the range will be (1, clip_limit). Default: (1, 4).\n* tile_grid_size ((int, int)): \n        size of grid for histogram equalization. Default: (8, 8).\n* p (float): \n        probability of applying the transform. Default: 0.5.","5e8b58b2":"## CutOut - CoarseDropout of the square regions in the image.","c95c3723":"Initial Image","e99b047c":"* reference_images (List[str] or List(np.ndarray)): \n        List of file paths for reference images\n        or list of reference images.\n* blend_ratio (float, float): \n        Tuple of min and max blend ratio. Matched image will be blended with original\n        with random blend factor for increased diversity of generated images.\n* read_fn (Callable): \n        Used-defined function to read image. Function should get image path and return numpy\n        array of image pixels.\n* p (float): \n        probability of applying the transform. Default: 1.0.","923a23d0":"* alpha (float):  \n        how much to perturb\/scale the eigen vecs and vals.\n* scale is samples from gaussian distribution (mu=0, sigma=alpha)\n","3136c216":"* flare_roi (float, float, float, float): \n        region of the image where flare will\n* appear (x_min, y_min, x_max, y_max).\n        All values should be in range [0, 1].\n* angle_lower (float): \n        should be in range [0, `angle_upper`].\n* angle_upper (float):\n        should be in range [`angle_lower`, 1].\n* num_flare_circles_lower (int): \n        lower limit for the number of flare circles.Should be in range [0, `num_flare_circles_upper`].\n* num_flare_circles_upper (int): \n        upper limit for the number of flare circles.Should be in range [`num_flare_circles_lower`, inf].\n* src_radius (int):\n        src_color ((int, int, int)): color of the flare","be8d1abb":"## Flip - Flip the input either horizontally, vertically or both horizontally and vertically.","12b1f689":"## ElasticTransform","9d6562cb":"## RandomSunFlare","4adf1f91":"## RandomFog - Simulates fog for the image","f7551dd9":"## HueSaturationValue - Randomly change hue, saturation and value of the input image.","974d1057":"## IAAAffine - Place a regular grid of points on the input and randomly move the neighbourhood of these point around via affine transformations.","a7ab9ca2":"## CenterCrop - Crop the central part of the input.","84aa251c":"## Solarize - Invert all pixel values above a threshold.","2bafd327":"### some random images","42af6a32":"## FDA -  Fourier Domain Adaptation from https:\/\/github.com\/YanchaoYang\/FDA Simple \"style transfer\".","f0791b11":"* scale_min (float):\n        lower bound on the image scale. Should be < 1.\n* scale_max (float):  \n        lower bound on the image scale. Should be .\n* interpolation: \n        cv2 interpolation method. cv2.INTER_NEAREST by default","f0c258b8":"## CLAHE - Apply Contrast Limited Adaptive Histogram Equalization to the input image.","b1ac0c84":"## Blur - Blur the input image using a random-sized kernel.","d6db36db":"## ShiftScaleRotate - Randomly apply affine transforms: translate, scale and rotate the input.","e910cd1a":"## GaussianBlur - Blur the input image using a Gaussian filter with a random kernel size.","29706228":"## DownScale","1f2346b7":"# List of Augmentations available","4f2fa3ab":"## ColorJitter - Randomly changes the brightness, contrast, and saturation of an image.","e1981036":"* reference_images (List[str] or List(np.ndarray)): \n        List of file paths for reference images or list of reference images.\n* beta_limit (float or tuple of float): \n        coefficient beta from paper. Recommended less 0.3.\n* read_fn (Callable): \n        Used-defined function to read image. Function should get image path and return numpyarray of image pixels.","d169a3dd":"* num_holes (int): \n        number of regions to zero out\n* max_h_size (int):\n        maximum height of the hole\n* max_w_size (int):\n        maximum width of the hole\n* fill_value (int, float, lisf of int, list of float):\n        value for dropped pixels.","c8d9aec9":"* channel_drop_range (int, int): \n        range from which we choose the number of channels to drop.\n* fill_value (int, float):\n        pixel value for the dropped channel.\n* p (float):\n        probability of applying the transform. Default: 0.5.","4b59109c":"* mean (float, list of float): \n        mean values\n* std  (float, list of float): \n        std values\n* max_pixel_value (float):\n        maximum possible pixel value","a061e061":"* hue_shift_limit ((int, int) or int): range for changing hue. If hue_shift_limit is a single int, the range\n        will be (-hue_shift_limit, hue_shift_limit). Default: (-20, 20).\n*  sat_shift_limit ((int, int) or int): range for changing saturation. If sat_shift_limit is a single int,\n        the range will be (-sat_shift_limit, sat_shift_limit). Default: (-30, 30).\n*  val_shift_limit ((int, int) or int): range for changing value. If val_shift_limit is a single int, the range\n        will be (-val_shift_limit, val_shift_limit). Default: (-20, 20).\n*  p (float): probability of applying the transform. Default: 0.5.","bf5922c6":"## RandomResizedCrop - Torchvision's variant of crop a random part of the input and rescale it to some size.","3c06c545":"* brightness_limit ((float, float) or float): \n        factor range for changing brightness.\n        If limit is a single float, the range will be (-limit, limit). Default: (-0.2, 0.2).\n* contrast_limit ((float, float) or float):\n        factor range for changing contrast.\n        If limit is a single float, the range will be (-limit, limit). Default: (-0.2, 0.2).\n* brightness_by_max (Boolean): \n        If True adjust contrast by image dtype maximum,\n        else adjust contrast by image mean.\n* p (float):\n        probability of applying the transform. Default: 0.5.","3f225eda":"* alpha (float):\n* sigma (float): \n        Gaussian filter parameter.\n* alpha_affine (float): \n            The range will be (-alpha_affine, alpha_affine)\n* interpolation (OpenCV flag): \n        flag that is used to specify the interpolation algorithm. Should be one of:\n        cv2.INTER_NEAREST, cv2.INTER_LINEAR, cv2.INTER_CUBIC, cv2.INTER_AREA, cv2.INTER_LANCZOS4. Default: cv2.INTER_LINEAR.\n* border_mode (OpenCV flag): \n        flag that is used to specify the pixel extrapolation method. Should be one of:\n        cv2.BORDER_CONSTANT, cv2.BORDER_REPLICATE, cv2.BORDER_REFLECT, cv2.BORDER_WRAP, cv2.BORDER_REFLECT_101.\n        Default: cv2.BORDER_REFLECT_101\n* value (int, float, list of ints, list of float): \n        padding value if border_mode is cv2.BORDER_CONSTANT.\n* mask_value (int, float,\n        list of ints,\n        list of float): padding value if border_mode is cv2.BORDER_CONSTANT applied for masks.\n* approximate (boolean): Whether to smooth displacement map with fixed kernel size.\n        Enabling this option gives ~2X speedup on large images.","797d2551":"## RandomBrightnessContrast - Randomly change brightness and contrast of the input image.","c34605b4":"**Images are of size of**\n* Height - 600\n* Width - 400\n* Channel - 3","c7444631":"*  shift_limit ((float, float) or float): \n        shift factor range for both height and width. If shift_limit\n        is a single float value, the range will be (-shift_limit, shift_limit). Absolute values for lower and\n        upper bounds should lie in range [0, 1]. Default: (-0.0625, 0.0625).\n*  scale_limit ((float, float) or float): \n        scaling factor range. If scale_limit is a single float value, the\n        range will be (-scale_limit, scale_limit). Default: (-0.1, 0.1).\n*  rotate_limit ((int, int) or int): \n        rotation range. If rotate_limit is a single int value, the\n        range will be (-rotate_limit, rotate_limit). Default: (-45, 45).\n*  interpolation (OpenCV flag): \n        flag that is used to specify the interpolation algorithm. Should be one of:\n        cv2.INTER_NEAREST, cv2.INTER_LINEAR, cv2.INTER_CUBIC, cv2.INTER_AREA, cv2.INTER_LANCZOS4.\n        Default: cv2.INTER_LINEAR.\n*  border_mode (OpenCV flag): \n        flag that is used to specify the pixel extrapolation method. Should be one of:\n        cv2.BORDER_CONSTANT, cv2.BORDER_REPLICATE, cv2.BORDER_REFLECT, cv2.BORDER_WRAP, cv2.BORDER_REFLECT_101.\n        Default: cv2.BORDER_REFLECT_101\n* value (int, float, list of int, list of float): \n        padding value if border_mode is cv2.BORDER_CONSTANT.\n*  mask_value (int, float,\n                list of int,\n                list of float): padding value if border_mode is cv2.BORDER_CONSTANT applied for masks.\n*  shift_limit_x ((float, float) or float): shift factor range for width. If it is set then this value\n        instead of shift_limit will be used for shifting width.  If shift_limit_x is a single float value,\n        the range will be (-shift_limit_x, shift_limit_x). Absolute values for lower and upper bounds should lie in\n        the range [0, 1]. Default: None.\n*  shift_limit_y ((float, float) or float): shift factor range for height. If it is set then this value\n        instead of shift_limit will be used for shifting height.  If shift_limit_y is a single float value,\n        the range will be (-shift_limit_y, shift_limit_y). Absolute values for lower and upper bounds should lie\n        in the range [0, 1]. Default: None.\n*  p (float): probability of applying the transform. Default: 0.5.","5c2425fc":"## Equalize -  Equalize the image histogram.","9cd92fbf":"* var_limit ((float, float) or float): \n        variance range for noise. If var_limit is a single float, the range\n        will be (0, var_limit). Default: (10.0, 50.0).\n* mean (float): \n        mean of the noise. Default: 0\n* p (float): \n        probability of applying the transform. Default: 0.5.","8a842565":"## Coarse DropOut -  CoarseDropout of the rectangular regions in the image.","9424bbcd":"## ChannelDropout","c0b7fe57":"* Blur Limit \n        maximum Gaussian kernel size for blurring the input image.Must be zero or odd and in range [0, inf) If set to 0 it will be computed from sigma as `round(sigma * (3 if img.dtype == np.uint8 else 4) * 2 + 1) + 1`.If set single value `blur_limit` will be in range (0, blur_limit) Default: (3, 7).\n* Sigma Limit\n        Gaussian kernel standard deviation. Must be greater in range [0, inf).If set single value `sigma_limit` will be in range (0, sigma_limit).If set to 0 sigma will be computed as `sigma = 0.3*((ksize-1)*0.5 - 1) + 0.8`. Default: 0.","87408482":"* fog_coef_lower \n        fog_coef_lower (float): lower limit for fog intensity coefficient. Should be in [0, 1] range.\n* fog_coef_upper\n         fog_coef_upper (float): upper limit for fog intensity coefficient. Should be in [0, 1] range.\n* alpha_coef\n         alpha_coef (float): transparency of the fog circles. Should be in [0, 1] range.\n","5367de82":"* height (int): \n        height after crop and resize.\n* width (int): \n        width after crop and resize.\n* erosion_rate (float): \n        erosion rate applied on input image height before crop.\n* interpolation (OpenCV flag): \n        flag that is used to specify the interpolation algorithm. Should be one of:\n        cv2.INTER_NEAREST, cv2.INTER_LINEAR, cv2.INTER_CUBIC, cv2.INTER_AREA, cv2.INTER_LANCZOS4.\n        Default: cv2.INTER_LINEAR.\n* p (float): \n        probability of applying the transform. Default: 1.","bb813dfd":"* mode (str): {'cv', 'pil'}. \n        Use OpenCV or Pillow equalization method.\n* by_channels (bool): \n        If True, use equalization by channels separately,\n        else convert image to YCbCr representation and use equalization by `Y` channel.\n* mask (np.ndarray, callable): \n        If given, only the pixels selected by\n        the mask are included in the analysis. Maybe 1 channel or 3 channel array or callable.\n        Function signature must include `image` argument.\n* mask_params (list of str): \n        Params for mask function.","e7dada52":"## FancyPCA -  Augment RGB image using FancyPCA from Krizhevsky's paper\n\"ImageNet Classification with Deep Convolutional Neural Networks\"","6de537d5":"## HistogramMatching -  Apply histogram matching. It manipulates the pixels of an input image so that its histogram matches the histogram of the reference image. If the images have multiple channels, the matching is done independently for each channel, as long as the number of channels is equal in the input image and the reference.\n","37abb1a6":"## RandomSizedBBoxSafeCrop -  Crop a random part of the input and rescale it to some size without loss of bboxes.","34dd11f6":"## ChannelShuffle - Randomly rearrange channels of the input RGB image.","528de136":"* max_holes (int): \n        Maximum number of regions to zero out.\n* max_height (int):\n        Maximum height of the hole.\n* max_width (int): \n        Maximum width of the hole.\n* min_holes (int):\n        Minimum number of regions to zero out. If `None`,\n        `min_holes` is be set to `max_holes`. Default: `None`.\n* min_height (int): \n        Minimum height of the hole. Default: None. If `None`,\n        `min_height` is set to `max_height`. Default: `None`.\n* min_width (int): \n        Minimum width of the hole. If `None`, `min_height` is\n        set to `max_width`. Default: `None`.\n* fill_value (int, float, lisf of int, list of float): \n        value for dropped pixels.\n* mask_fill_value (int, float, lisf of int, list of float): \n        fill value for dropped pixels\n        in mask. If None - mask is not affected.","6749a4b1":"## GaussNoise -  Apply gaussian noise to the input image.","74b8cbb9":"## MedianBlur -  Blur the input image using a median filter with a random aperture linear size."}}