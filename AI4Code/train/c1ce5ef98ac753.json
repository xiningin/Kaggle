{"cell_type":{"5158e338":"code","f326ff34":"code","ef1a2170":"code","30fbc3c6":"code","be33cba3":"code","a3c7753b":"code","c7e5ea67":"code","ae423069":"code","43f70b50":"code","052b528f":"code","979e7022":"code","325de7ed":"code","d7bc3eec":"code","7c744014":"code","1e5aae69":"code","c6d06a58":"code","9471cca1":"code","588f6cec":"code","120e019b":"code","589bb060":"code","b6583f31":"code","5908493c":"code","e09f0d87":"code","717c05a2":"code","d8297422":"code","558dee63":"code","722ec58b":"code","68fb6be8":"code","15e3aa35":"code","b2ccaba1":"code","e85613b9":"code","13948f45":"code","804e8381":"code","78d2d15c":"code","327f5024":"code","9e44b4b8":"code","1d370592":"code","47d5bbe5":"code","fb1fabec":"code","f9ac0b24":"code","06555152":"code","fd11db91":"code","6005f21c":"code","0aa1ca27":"code","ad9d883e":"code","6f73b3fe":"code","fb3914da":"code","7e8f24e0":"code","b4a5f70c":"code","06c58e7d":"code","d4d0a585":"markdown","b65fb79c":"markdown","8984c4e6":"markdown","3a75855e":"markdown","7ea0ebaf":"markdown","b8676d11":"markdown","a01d3f6f":"markdown","466e79a5":"markdown","928f2b6c":"markdown","21666bcb":"markdown","f20fe069":"markdown","bcb9751d":"markdown","8fc5e139":"markdown","0928df28":"markdown","2da5ad10":"markdown","bedc0fb7":"markdown","cb41a4a3":"markdown","c4fddda4":"markdown","c3e13782":"markdown","4376474f":"markdown","8bbccd6d":"markdown","0c1cff9c":"markdown","5a22da35":"markdown","157ca9d6":"markdown","47afd29b":"markdown","47dc9a93":"markdown","912cd1fb":"markdown","c5de6c0a":"markdown","192972ca":"markdown","4ec99201":"markdown","c31ec5d7":"markdown","2d0786e3":"markdown","04f45561":"markdown","0ac86c3e":"markdown"},"source":{"5158e338":"import numpy as np\nimport pandas as pd\nfrom tqdm.notebook import tqdm\nimport os\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport math\nfrom scipy import stats\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.ensemble import GradientBoostingRegressor\nfrom sklearn.model_selection import train_test_split, KFold, cross_val_score\nfrom sklearn.linear_model import LinearRegression, Ridge, SGDRegressor, ElasticNetCV, Lasso\nfrom sklearn.metrics import mean_squared_error, SCORERS\nfrom sklearn.kernel_ridge import KernelRidge\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import PolynomialFeatures, MinMaxScaler\nfrom sklearn.model_selection import GridSearchCV\nfrom xgboost.sklearn import XGBRegressor\nimport warnings\nfrom datetime import datetime","f326ff34":"warnings.filterwarnings('ignore')\nGD = False","ef1a2170":"ROOT = '\/kaggle\/input\/house-prices-advanced-regression-techniques\/'\nTEST = os.path.join(ROOT, 'test.csv')\nTRAIN = os.path.join(ROOT, 'train.csv')\ndf_test = pd.read_csv(TEST)\ndf_train = pd.read_csv(TRAIN)","30fbc3c6":"rows, cols = df_train.shape\nprint(f'Training Dataset\\n-------\\ncolumns: {cols}\\nrows: {rows}')\ncat_cols = df_train.loc[:, df_train.columns != 'SalePrice'].select_dtypes(include=['object']).columns\nnum_cols = df_train.loc[:, df_train.columns != 'SalePrice'].select_dtypes(exclude=['object']).columns\nprint(f'categorical columns: {len(cat_cols)}\\nnumeric columns: {len(num_cols)}\\n\\n=================\\n')\n\nrows, cols = df_test.shape\nprint(f'Test Dataset\\n-------\\ncolumns: {cols}\\nrows: {rows}')\ncat_cols = df_test.loc[:, df_test.columns != 'SalePrice'].select_dtypes(include=['object']).columns\nnum_cols = df_test.loc[:, df_test.columns != 'SalePrice'].select_dtypes(exclude=['object']).columns\nprint(f'categorical columns: {len(cat_cols)}\\nnumeric columns: {len(num_cols)}')","be33cba3":"nulls = {}\n\nfor col in df_train.columns:\n    nulls[col] = (1-(len(df_train[df_train[col].isna()][col]) \/ df_train.shape[0]))\n\nlabels = []\nvals = []\n\nfor k, v in nulls.items():\n    if v < 1.0:\n        labels.append(k)\n        vals.append(v)\n\n_, ax = plt.subplots(figsize=(12,5))\n\nsns.barplot(y=vals, x=labels, color='lightskyblue')\nax.set_xticklabels(labels=labels, rotation=45)\nplt.title('% non-null values by columns')\nax.set_xlabel('columns')\nax.set_ylabel('%')\nplt.show()","a3c7753b":"to_drop = []\n\nfor k, v in nulls.items():\n    if v < 0.6:\n        to_drop.append(k)\n\n# Let's use a copy of our dataframe so that we won't have to reload our entire dataset in case we need\n# to do so (especially a good idea when we are working with very large dataset)\ndf_train_c = df_train.drop(to_drop, axis=1)\n\nrows, cols = df_train_c.shape\nprint(f'columns: {cols}\\nrows: {rows}')\ncat_cols = df_train_c.loc[:, df_train_c.columns != 'SalePrice'].select_dtypes(include=['object']).columns\nnum_cols = df_train_c.loc[:, df_train_c.columns != 'SalePrice'].select_dtypes(exclude=['object']).columns\nprint(f'categorical columns: {len(cat_cols)}\\nnumeric columns: {len(num_cols)}')","c7e5ea67":"si = SimpleImputer(strategy='most_frequent')\n\nfor k,v in nulls.items():\n    if (v < 1) and (k not in to_drop):\n        df_train_c[k] = si.fit_transform(df_train_c[k].values.reshape(-1,1))","ae423069":"df_train_c = df_train_c[df_train_c.GrLivArea < 4000]","43f70b50":"X = df_train_c.loc[:, df_train_c.columns != 'SalePrice']\ny = df_train_c.loc[:, df_train_c.columns == 'SalePrice']\n\ndf_train_ID = df_train.Id\ndf_test_ID = df_test.Id\n\nX = X.loc[:, X.columns != 'Id']","052b528f":"fig, ax = plt.subplots(1,2, figsize=(15,5))\n\nsns.distplot(y, fit=stats.norm, ax=ax[0], kde=False)\nstats.probplot(y.SalePrice,  plot=ax[1])\nplt.show()\nprint(f'Fisher-Pearson coeficient of skewness: {stats.skew(y.SalePrice.values):.2f}')","979e7022":"numerical_columns = X.loc[:, ~X.columns.isin(['MSSubClass', 'OverallQual', 'OverallCond', 'GarageYrBlt'])].select_dtypes(include=['int', 'float']).columns\nsk = X[numerical_columns].apply(lambda x: stats.skew(x.dropna())).to_frame('Fisher-Pearson Coef')\nskw_cols = list(sk[abs(sk['Fisher-Pearson Coef']) > 0.5].index)\nsk[abs(sk['Fisher-Pearson Coef']) > 0.5]","325de7ed":"lmbda = 0.0\nX[skw_cols] = X[numerical_columns].loc[:, X[numerical_columns].columns.isin(skw_cols)].apply(lambda x: stats.boxcox(1+x, lmbda=lmbda))","d7bc3eec":"y = y.apply(lambda x: stats.boxcox(1+x, lmbda=lmbda))","7c744014":"sk['Fisher-Pearson Coef (After)'] = X[numerical_columns].apply(lambda x: stats.skew(x))\nsk[sk.index.isin(skw_cols)]","1e5aae69":"fig, ax = plt.subplots(1,2, figsize=(15,5))\n\nsns.distplot(y, fit=stats.norm, ax=ax[0], kde=False)\nstats.probplot(y.SalePrice,  plot=ax[1])\nplt.show()\nprint(f'Fisher-Pearson coeficient of skewness: {stats.skew(y.SalePrice.values):,.2f}')","c6d06a58":"X_disc = X.loc[:,~(X.columns.isin(['MoSold', 'YrSold', 'YearBuilt', 'YearRemodAdd', 'Id'])) &\n                  (X.columns.isin(numerical_columns))]\n\nX_disc['y'] = y\n\n_, ax = plt.subplots(figsize=(25,15))\n\nsns.heatmap(X_disc.corr(), annot=True, cbar=False, cmap='YlGnBu')\nplt.show()","9471cca1":"mask = (abs(X_disc.corr()['y'] >= 0.3))\ncorr_variables = X_disc.corr()['y'][mask]\ncorr_variables = list(corr_variables[corr_variables.index != 'y'].index)\n\ncorr_variables","588f6cec":"_, ax = plt.subplots(figsize=(15,8))\n\nsns.heatmap(X_disc.loc[:, corr_variables].corr(), annot=True, cbar=True, cmap='YlGnBu')\nplt.show()","120e019b":"mask = ((abs(X_disc.loc[:, corr_variables].corr()) > 0.8) & \n        (X_disc.loc[:, corr_variables].corr() != 1.0))\ncols = list(X_disc.loc[:, corr_variables].corr()[mask].dropna(how='all', axis=1).columns)\n\nto_remove = []\n\nfor i in range(0,len(cols),2):\n    to_remove.append(cols[i])\n    \ncontinous_features = list(set(corr_variables) - set(to_remove))\ncontinous_features","589bb060":"X['IsRemod'] = np.where(np.expm1(X[['YearBuilt']]).astype('int').YearBuilt == X.YearRemodAdd, 0, 1)\nX['YrSinceBuilt'] = X.YrSold - X.YearBuilt\nX['YrSinceRemod'] = X.YrSold - X.YearRemodAdd\nX['YrSinceGarageYrBlt'] = X.YrSold - X.GarageYrBlt\nX['HasMasVnr'] = np.where(X.MasVnrType == 'None',0,1)","b6583f31":"# tmp = X[X['YrSinceRemod'] == 0]\n# X =  X[X['YrSinceRemod'] != 0]\n# tmp['YrSinceRemod'] = tmp.YrSinceRemod.replace(0,np.nan)\n# X = X.append(tmp)","5908493c":"X_discrete = X.loc[:, X.columns.isin(['MoSold', 'YrSold', 'YrSinceBuilt', 'YrSinceRemod','YrSinceGarageYrBlt'])]\nX_discrete['y'] = y\n\n_, ax = plt.subplots(figsize=(15,8))\n\nsns.heatmap(X_discrete.corr('spearman'), annot=True, cmap='YlGnBu')\nplt.show()","e09f0d87":"mask = (((abs(X_discrete.corr('spearman')['y']) >= 0.3) &\n       (X_discrete.corr('spearman')['y'] != 1.0)))\nX_discrete_cols = list(X_discrete.corr('spearman')['y'][mask].index)\ndiscrete_features = list(set(X_discrete_cols) - set(['YrSinceGarageYrBlt']))","717c05a2":"X_num = X.loc[:, X.columns.isin(continous_features + discrete_features)]\nX_num['y'] = y","d8297422":"sns.pairplot(x_vars=continous_features[:int(len(continous_features)\/2)],\n             y_vars=['y'],\n            data=X_num,\n            height=3.5)\n\nsns.pairplot(x_vars=continous_features[int(len(continous_features)\/2):],\n             y_vars=['y'],\n            data=X_num,\n            height=3.5)\n\nsns.pairplot(x_vars=discrete_features,\n             y_vars=['y'],\n            data=X_num,\n            height=3.5)\n\nplt.show()","558dee63":"le = LabelEncoder()\n\nX['CentralAir_enc']  = X[['CentralAir']].apply(lambda x: le.fit_transform(x.values))","722ec58b":"r, p = stats.pointbiserialr(X.IsRemod.values, y.values.ravel())\nprint(f'IsRemod - r: {r} | p: {p}')\nr, p = stats.pointbiserialr(X.CentralAir_enc.values, y.values.ravel())\nprint(f'CentralAir_enc - r: {r} | p: {p}')\nr, p = stats.pointbiserialr(X.HasMasVnr.values, y.values.ravel())\nprint(f'MasVnr_enc - r: {r} | p: {p}')","68fb6be8":"dico = ['CentralAir_enc', 'HasMasVnr']","15e3aa35":"categoricals = list(X.loc[:,X.columns != 'CentralAir'].select_dtypes(include='object').columns)\ncategoricals = categoricals + ['MSSubClass', 'OverallQual', 'OverallCond']","b2ccaba1":"X_categoricals = X[categoricals].apply(lambda x: le.fit_transform(x))\nX_categoricals['y']  = y","e85613b9":"corr = []\n\n\nfor col in tqdm(X_categoricals.columns):\n    cat = X_categoricals[col].unique()\n   \n    y_avg = []\n    n_cat = []\n    for c in cat:\n        y_avg.append(X_categoricals[X_categoricals[col] == c].y.mean())\n        n_cat.append(len(X_categoricals[X_categoricals[col] == c]))\n    \n    y_total_avg = np.sum(np.multiply(y_avg,n_cat) \/ np.sum(n_cat))\n\n    numerator = np.sum((np.multiply(n_cat,np.power(np.subtract(y_avg, y_total_avg),2))))\n    denominator = np.sum(np.power(np.subtract(X_categoricals.y, y_total_avg),2))\n\n    if denominator == 0:\n        eta = 0.0\n        corr.append((col, eta))\n    else:\n        eta = np.sqrt(numerator\/denominator)\n        corr.append((col, eta))\n        \nprint(corr)","13948f45":"categoricals_columns = []\n\nfor el in corr:\n    if el[1] >= 0.3:\n        categoricals_columns.append(el[0])\n\ncategoricals_columns.pop(len(categoricals_columns)-1)\ncategoricals_columns","804e8381":"X_cat = X_categoricals[categoricals_columns]\nX_cat['y'] = y","78d2d15c":"sns.pairplot(x_vars=categoricals_columns[:int(len(categoricals_columns)\/2)],\n             y_vars=['y'],\n            data=X_cat,\n            height=3.5)\nsns.pairplot(x_vars=categoricals_columns[int(len(categoricals_columns)\/2):],\n             y_vars=['y'],\n            data=X_cat,\n            height=3.5)\nplt.show()","327f5024":"features = categoricals_columns + continous_features + discrete_features + dico\nX = X[features]\ny = y\nX[categoricals_columns] = X[categoricals_columns].apply(lambda x: le.fit_transform(x))\nX_train, X_test, y_train, y_test = train_test_split(X.loc[:,X.columns != 'y'], y, test_size=0.3)\nX_test, X_val, y_test, y_val = train_test_split(X_test, y_test, test_size=0.3)","9e44b4b8":"n_fold = 5\n\ndef rmseModel(m):\n    kf = KFold(n_splits=n_fold, random_state=0, shuffle=True).get_n_splits()\n    rmse = np.sqrt(-cross_val_score(m, X, y, scoring='neg_mean_squared_error', cv=kf))\n    return rmse","1d370592":"XGBBaselie = XGBRegressor(objective='reg:squarederror')\nXGBBaselie.fit(X_test, y_test)\npred = XGBBaselie.predict(X_val)\n\nrmseBaseline = np.sqrt(mean_squared_error(pred, y_val.values))\nprint(f'Baseline RMSE: {rmseBaseline}')","47d5bbe5":"ols_reg = LinearRegression()\nols_rge_scores = rmseModel(ols_reg)\nprint(f'OLS Reg RMSE, mean: {np.mean(ols_rge_scores)}, stdv: {np.std(ols_rge_scores)}')","fb1fabec":"if GD:\n    print('Running Grid Search for model tunning')\n    params = {'alpha': [0.1,0.3,0.5,0.7,0.9],\n             'solver': ['auto', 'svd', 'cholesky', 'lsqr']}\n    ridge_reg = Ridge()\n    gs = GridSearchCV(ridge_reg, params, cv=5)\n    gsf = gs.fit(X_train, y_train).best_params_\nelse:\n    gsf = {'alpha': 0.9, 'solver': 'auto'}\n\nridge_reg = Ridge(**gsf)\nridge_reg_scores = rmseModel(ridge_reg)\nprint(f'Ridge Reg RMSE, mean: {np.mean(ridge_reg_scores)}, stdv: {np.std(ridge_reg_scores)}')","f9ac0b24":"if GD:\n    print('Running Grid Search for model tunning')\n    params = {'l1_ratio': [.1, .5, .7, .9, .92, .95, .99, 1],\n             'n_alphas': [10,15,50, 100],\n              'normalize': [True, False],\n              'max_iter': [5,10,50,100],\n              'tol': [0.001, 0.0001, 0.00001]\n                }\n    el_reg = ElasticNetCV()\n    gs = GridSearchCV(el_reg, params, cv=5, n_jobs=-1, verbose=1)\n    gsf = gs.fit(X_train, y_train).best_params_\nelse:\n    gsf = {'l1_ratio': 0.9,\n         'max_iter': 50,\n         'n_alphas': 50,\n         'normalize': True,\n         'tol': 0.0001}\n\n    \nel_reg = ElasticNetCV(**gsf)\nel_reg_scores = rmseModel(el_reg)\nprint(f'Elastic Net Reg RMSE, mean: {np.mean(el_reg_scores)}, stdv: {np.std(el_reg_scores)}')","06555152":"if GD:\n    print('Running Grid Search for model tunning')\n    params = {'min_samples_split': [80],\n             'min_samples_leaf': [25],\n             'max_depth':[9],\n            'max_features': [4],\n             'subsample': [0.8],\n             'n_estimators': [2500],\n             'learning_rate': [0.005],\n             'subsample':[0.87]}\n    GB = GradientBoostingRegressor()\n    gs = GridSearchCV(GB, param_grid=params, cv=5, n_jobs=-1, verbose=1)\n    gsf = gs.fit(X_train, y_train).best_params_\nelse:\n    gsf = {'learning_rate': 0.005,\n         'max_depth': 9,\n         'max_features': 4,\n         'min_samples_leaf': 25,\n         'min_samples_split': 80,\n         'n_estimators': 2500,\n         'subsample': 0.87}\n\nGB_reg = GradientBoostingRegressor(**gsf)\nGB_reg_scores = rmseModel(GB_reg)\nprint(f'GB Reg, mean: {np.mean(GB_reg_scores)}, stdv: {np.std(GB_reg_scores)}')","fd11db91":"if GD:\n    print('Running Grid Search for model tunning')\n    params = {'max_depth ': [1],\n              'min_child_weight': [2],\n              'gamma ': [0.0],\n              'subsample':[0.7],\n              'reg_alpha':[1e-5, 1e-4, 1e-6],\n              'colsample_bytree': [0.87],\n              'scale_pos_weight':[1],\n                }\n\n    xgb_reg = XGBRegressor()\n    gs = GridSearchCV(xgb_reg, params, cv=5, n_jobs=-1, verbose=1)\n    gsf = gs.fit(X_train, y_train).best_params_\nelse:\n    gsf = {'colsample_bytree': 0.87,\n         'gamma ': 0.0,\n         'max_depth ': 1,\n         'min_child_weight': 2,\n         'reg_alpha': 1e-06,\n         'scale_pos_weight': 1,\n         'subsample': 0.7}\n    \nxgb_reg = XGBRegressor(**gsf, objective='reg:squarederror', nthread=4, learning_rate=0.005, n_estimators=10000)\nxgb_reg_scores = rmseModel(xgb_reg)\nprint(f'XGB Reg, mean: {np.mean(xgb_reg_scores)}, stdv: {np.std(xgb_reg_scores)}')","6005f21c":"olsM = ols_reg.fit(X_train, y_train)\nelM = el_reg.fit(X_train, y_train)\nRidgeM = ridge_reg.fit(X_train, y_train)\nGBregM = GB_reg.fit(X_train, y_train)\nXGBoostM = xgb_reg.fit(X_train, y_train)","0aa1ca27":"ensembleOutput = np.hstack((olsM.predict(X_test), RidgeM.predict(X_test), elM.predict(X_test).reshape(-1,1), GBregM.predict(X_test).reshape(-1,1)))\nstackedReg = LinearRegression()\nsackedM = stackedReg.fit(ensembleOutput, y_test)","ad9d883e":"valEnsembleOutput = np.hstack((olsM.predict(X_val), RidgeM.predict(X_val), elM.predict(X_val).reshape(-1,1),GBregM.predict(X_val).reshape(-1,1)))\nstackedPred = sackedM.predict(valEnsembleOutput)","6f73b3fe":"pred = (np.expm1(stackedPred).reshape(1,-1)[0]*0.55 +\\\nnp.expm1(XGBoostM.predict(X_val))*0.45)\n\nrmse_test = np.sqrt(mean_squared_error(np.log(pred), y_val.values))\nprint(f'rmse for test data: {rmse_test}')","fb3914da":"df_test['IsRemod'] = np.where(df_test.YearBuilt == df_test.YearRemodAdd, 0, 1)\ndf_test['YrSinceBuilt'] = df_test.YrSold - df_test.YearBuilt\ndf_test['YrSinceRemod'] = df_test.YrSold - df_test.YearRemodAdd\ndf_test['YrSinceGarageYrBlt'] = df_test.YrSold - df_test.GarageYrBlt\ndf_test['HasMasVnr'] = np.where(df_test.MasVnrType == 'None',0,1)\ndf_test['CentralAir_enc']  = df_test[['CentralAir']].apply(lambda x: le.fit_transform(x.values))\n\ndfPred = df_test[features]","7e8f24e0":"nulls = {}\n\nfor col in dfPred.columns:\n    nulls[col] = (1-(len(dfPred[dfPred[col].isna()][col]) \/ dfPred.shape[0]))\n    \nfor k, v in nulls.items():\n    if v < 1.0:\n        dfPred[k] = si.fit_transform(dfPred[k].values.reshape(-1,1))\n        \ndfPred[list(set(skw_cols).intersection(set(dfPred.columns)))] = dfPred[list(set(skw_cols).intersection(set(dfPred.columns)))].\\\n                                                                                apply(lambda x: stats.boxcox(1+x, lmbda=lmbda))\n\ndfPred[categoricals_columns] = dfPred[categoricals_columns].apply(lambda x: le.fit_transform(x))","b4a5f70c":"outputPred = np.hstack((olsM.predict(dfPred), RidgeM.predict(dfPred), elM.predict(dfPred).reshape(-1,1), GBregM.predict(dfPred).reshape(-1,1)))\nstackedPred = sackedM.predict(outputPred)\nfinalPred = (np.expm1(stackedPred).reshape(1,-1)[0]*0.55 +\\\nnp.expm1(XGBoostM.predict(dfPred))*0.45)","06c58e7d":"dff = pd.DataFrame({\n    'Id': df_test.Id,\n    'SalePrice':finalPred\n})\n\ndff.to_csv(f\"submission_{datetime.today().strftime('%Y%m%d')}.csv\", index=False)","d4d0a585":"#### 3.2.3 ElasticNet","b65fb79c":"### 3.3 Building and Testing our Ensemble Model\n#### 3.3.1 How is our model going to work?\nOur approach to building our stack ensemble will be to use the output from 4 of our model (OLS, Ridge, Elastic Net, and GB) as our input for an ensemble regression model. We'll then combine the prediction from this model with the one of the XGB (allocating appropriate weights) to get our final prediction.","8984c4e6":"Based on the standard we set to consider a relationship with our target variable, we can see that `CentralAir_enc` and `MasVnr_enc` both fit our requirement.\n#### 2.2.4 Categorical Variables\nNow let's move on to the categorical variables. We'll use the correlation ratio to measure the relationship between our categorical value and our target value. Correlation ratio range from 0-1 where 1 indicates the variance in our target value comes from differences between categories and where 0 indicates the differences in our target value comes from differences within our categories.\n\nWhat we are interested is the variance between category (i.e. a value close to 1) as it indicates that belonging to a specific category influences the `SalePrice`","3a75855e":"#### 3.2.1 OLS Regression","7ea0ebaf":"We now have applied our transformation to our training data. Let's check the skewness of our data again to confirm we are now working with clean data.","b8676d11":"#### 2.2.2 Continuous Variables\nNow that we have our continuous numerical data, let's look at our discrete variables. One thing to note is that `YearRemodAdd` would be equal to `YearBuilt` if no remodeling has been done. Therefore, we'll engineer it into a `1`\/`0` dichotomous variable where `1` indicates a remodeled house and `2` indicates a non remodeled house. \n\nWe'll also add 3 new field in replacement to `YearBuilt`,`'YearRemodAdd`,`GarageYrBlt` capturing the rencency (as opposed to the year). It is fair to assume that what the relationship really capture is the timeframe between the the remodeling\/construction and the sell date.","a01d3f6f":"### 3.2 Model Definition\nWe now have our baseline score and the approach we want to use to tackle this problem. Let's now train our models, implement our stacked ensemble and evaluate its performance against our baseline.","466e79a5":"## 4.3 Submitting our File","928f2b6c":"#### 3.2.5 Extrem Gradient Boost Regression\nWe followed the approach laid out by [Aarshay Jain](https:\/\/www.analyticsvidhya.com\/blog\/2016\/03\/complete-guide-parameter-tuning-xgboost-with-codes-python\/) on [analyticsvidhya.com](http:\/\/analyticsvidhya.com) to tune our XGB model.","21666bcb":"It is encouraging to see that all of tuned models perform better than our baseline - with our XGB model performing the best, though not as stable based on the standard deviation. We'll now build and test our stacked ensemble model and test how it performs.","f20fe069":"# 4. Submitting our Predictions\n## 4.1 Transforming our data","bcb9751d":"Not all of our columns have been successfuly transformed, though the majority of them now have a distribution with a coeficient of skewness close to 0.","8fc5e139":"Most of our columns have non-null data (18 out of 81). Among those 18 only 4 have an amount of non-null data that is very small (<20%). Base on this information, it is fair to drop those columns from our dataset.","0928df28":"## 2.1 Checking Data Skewness\nAs we are going to perform a regression to predict our house sales price, we should make sure our numerical features are normaly distributed. If not, we should apply a transform before moving forward.\n\nLet's first take a look at our target variable y.","2da5ad10":"The transformation for our target variable had a great effect. It is now almost normally distributed with a very low coeficient of skewness.","bedc0fb7":"We are now able to see the correlation of our discrete variables to our dependent variable `y`. We'll keep only variables with a moderate to strong correlation (i.e > |0.3|).","cb41a4a3":"Our code gives us 3 output:\n* our data distribution with a normal fitted. This gives a way to see the actual distribution of our data\n* a QQ plot. This is use as a visual check to see if our data is normaly distributed. It sorts our values in ascending order (y-axis) and plot these againsta theorical quantiles (x-axis) from a normal distribution. A normaly distributed set will form a straigh line\n* Fisher-Pearson coefficient of skewness. A coeficient of 0 indicates no skenews while a positive coeficient indicates a right skewed distribution\n\nWe can visualy see that our target varibale is not normally distributed. This is confirmed when we compute the . We should apply a transformation. \n\nLet's now check if any of our numerical varibles are normaly distributed. We'll consider that any feature with an absolute skewness greater than 0.5 is skewed and will need to be transformed.\nWe know that the following features are categorical variable that have been encoded so we should ignore them:\n* `MSSubClass`\n* `OverallQual` \n* `OverallCond`","c4fddda4":"## 4.4 Results\nWith this solution we rank 2,334 with an RMSE of 0.13507. This solution places us in the 54th percentile, which is slightly better than the average submission. Feel free to add any comments below on suggested ways to improve the existing model.","c3e13782":"## 4.2 Making our Prediction","4376474f":"# 1. Context\nBased on the information available, we want to predict the price of a house based on a set of specific features. The answer to this question implies a regression model.\n\nSince we are looking to use a linear regression model, we'll want to keep features that have a significant correlation with our target value (`SalesPrice`). Keeping this in mind, we'll need to evaluate the correlation of 3 types of variables:\n1. Discrete and continuous variable (we'll be using pearson-r)\n2. Binary variable (we'll be using a point-biseral correlation)\n3. Categorical variable with more than 2 options (we'll use the [correlation ratio](https:\/\/towardsdatascience.com\/the-search-for-categorical-correlation-a1cf7f1888c9))\n\nWe'll also want to exclude features correlated with each other (colinearity). Another important element we'll want to look at is the type of distribution our features have. Since we'll use a regression model we need to have our data normaly distributed - this may require to apply some transformation.","8bbccd6d":"We now have 77 columns. The 4 columns we dropped were all categorical.\nNow that we have a somewhat cleaner set, we can start working on understanding the correlation of variables with `SalePrice`. First we'll seperate our dependent (y) and independent variables (X). We'll also remove the `Id` from our dataframe as we do not need this information right now.","0c1cff9c":"### 2.2 Selecting our features\n#### 2.2.1 Discrete Variables\nLet's start with our numerical values. This is the simpliest. `Pandas` offers a simple way to calculate pearson-r correlation, so this will be straightforward. Now we need to be award of a few element:\n* `MSSubClass`, `OverallQual`, `OverallCond`, are encoded categorical variable. We may need to re-encode with a code starting at 0\n* `MoSold` and `YrSold`, `YearBuilt`, `YearRemodAdd`, `GarageYrBlt` are discret variable and a spearman correlation may be more appropriate\n\nLet's exclude those features and run our pearson correlation only against discrete variables.","5a22da35":"#### 2.2.3 Dichotomous Variables\nTo analyse the relationship between our dichotomous variable and our discrete dependent variable `y` we'll use the point-biseral correlation. It is important to note that we'll consider only natural dichotomous variables. We have the following variables:\n* `CentralAir`\n* `IsRemod`","157ca9d6":"#### 3.2.4 Gradient Boost Regression\nWe followed the approach laid out by [Aarshay Jain](https:\/\/www.analyticsvidhya.com\/blog\/2016\/02\/complete-guide-parameter-tuning-gradient-boosting-gbm-python\/) on [analyticsvidhya.com](http:\/\/analyticsvidhya.com) to tune our GB model.","47afd29b":"We can see 3 features that have a medium to high correlation with our dependent variable. We'll check for any colinearality between these 3 variables. The relationship tend to indicate that as the recency grows (was built or renovated farther in the past) we get a smaller price. Now, we can see a pretty strong colinerality between these 3 variables (especially between garage and construction). Hence we'll go ahead and only keep 2 of these 3 variables:\n* `YrSinceBuilt`\n* `YrSinceRemod`","47dc9a93":"#### 3.2.2 Ridge Regression","912cd1fb":"Let's now fill in the missing values for our 14 columns with N\/A values. We'll use the `SimpleImputer()` method using the most frequent value present in the column to replace null values.","c5de6c0a":"Now that we have selected our discrete features let's controle for colinearity in features. Features with strong correlation between each other will explain the same information in our model, hence we can get rid of one of the 2 variables.","192972ca":"# 2. Data Exploration\n## 2.1 High Level Data Structure \nWe'll start by looking at our data from a hugh level. We'll try to understand:\n* shape of data\n* data type\n* missing values\n\nWe'll also seperate our dataset into target (y) and features (X). Finaly, if we find any missing values we'll work on either dropping the column or replacing null values.","4ec99201":"24 of our numerical columns have a skewed distribution. We'll need to transform these in order to perform our regression. We'll use a Box Cox transformation. It is important to keep the lambda value constant in our transformation when using box cox.\n\nA lambda value of 0 for the Box Cox transformation simply applies a log transformation.","c31ec5d7":"## 3. Training & Testing our Model\nLet's first recap. what we have done so far:\n1. *descriptive analytics*: we looked at the shape of our data, the missing values as well as the data types we are working with. We also did some very light data engineering and data transformation\n2. *Selected our features*: after looking at the data we selected our features based on their correlation with our target variable. We split the feature selection based on the type of variables we were working with.\n\nIt is now time to train and test our model. We'll first define a baseline so that we can see how well our model is performing. For our model, I have chosen to use a stacked ensemble model using 4 submodels:\n* OLS Regression\n* Ridge Regression\n* ElasticNet Regression\n* GB Regression\n* XGB Regression\nThe choice of uing a stacked model was driven by notebooks from other users and an interest in learning this technique.\n\n### 3.1 Bookkeeping\nLet's first do some bookkeeping work. First we'll combine all of our features, then split our model into a train, a test and a validation set and finaly get our baseline score. We'll use an untune single XGB Regressor model as a baseline.","2d0786e3":"Now that we have our sets, let's define our cross validation function that will be used to measure the performance of our model when performing some tunning.","04f45561":"Now that we have our numerical features let's plot these variable against our target variable `SalePrice -` to have a visuale representation of the relationship.","0ac86c3e":"Our training dataset has 1,460 rows and 81 columns (80 features total). This is a pretty small dataset with a somewhat large amount of features. Out of our 80 columns we have:\n* 37 numeric columns\n* 43 categorical\/string columns\n\nThis tells us we'll have to do some encoding on our categorical values to be able to take advantage of the full dataset"}}