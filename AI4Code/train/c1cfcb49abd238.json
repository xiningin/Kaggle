{"cell_type":{"20a770c1":"code","f888ecd6":"code","ce06db39":"code","b87268c2":"code","a10b720b":"code","e13a6c9e":"code","30d7bd47":"code","e22d70c0":"code","9dc73ef3":"code","ce1ff921":"code","02a87769":"code","b6371b62":"code","90a863ed":"code","4cd12ab6":"code","180a104f":"code","513636db":"code","0c394070":"code","ca8d44c8":"code","105a1e6e":"code","31e2a365":"code","dba1de8d":"code","a881b011":"code","3865f6a0":"code","9c032c61":"code","a931559c":"code","1c949a94":"code","f731f92f":"code","312ddef1":"code","ddf7801b":"code","04ddacd8":"code","095e4af4":"code","60e54e7c":"code","1b7464f2":"code","5c082ef6":"code","9ea17c16":"code","d4b6b955":"code","f78b3bb1":"code","361fb656":"code","b0300886":"code","0b541063":"code","e4a5ffa9":"code","d19f4103":"code","092900c2":"code","2766a4bb":"code","d9567abe":"code","6ae96ac4":"code","1ed110d8":"code","f89f97b6":"code","e35e3161":"code","0195e399":"markdown","c2d6712e":"markdown","7e8ed1dc":"markdown","56c9637e":"markdown","7a00da02":"markdown","dee48cf4":"markdown","448830ee":"markdown","7fcd674a":"markdown","34f347c5":"markdown","fba66d2d":"markdown","78f70e02":"markdown","bce70ec0":"markdown","68172ac9":"markdown","71d3fb91":"markdown","a01a5278":"markdown","547307e6":"markdown","56b83e44":"markdown","0e2fdd9e":"markdown","07111411":"markdown","e559119f":"markdown","8cd65e93":"markdown","d6424325":"markdown","ef3a606e":"markdown","64e5e7f4":"markdown","2dfecefb":"markdown","88be03f0":"markdown","981079a8":"markdown","3f0ad5a0":"markdown","c904dad4":"markdown","b78707a8":"markdown","3c4b194a":"markdown","8a64f8a2":"markdown","93a565dc":"markdown","32282fae":"markdown","859e1144":"markdown","a27e645a":"markdown","42a0717f":"markdown","d7b19574":"markdown","a9c7a44f":"markdown","d60b4abf":"markdown"},"source":{"20a770c1":"import numpy as np \nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom statsmodels.stats.outliers_influence import variance_inflation_factor\nfrom patsy import dmatrices\nfrom imblearn.over_sampling import SMOTE\nfrom sklearn.model_selection import StratifiedShuffleSplit\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score, classification_report\nimport warnings\n","f888ecd6":"df=pd.read_csv(\"..\/input\/hr-analytics\/HR_comma_sep.csv\")","ce06db39":"df.head(5)","b87268c2":"df.describe()","a10b720b":"df.shape","e13a6c9e":"df.info()","30d7bd47":"df.isnull().sum()","e22d70c0":"## Let's separate numerical and categorical vaiables into 2 dfs\n\ndef sep_data(data):\n    \n    numerics = ['int32','float32','int64','float64']\n    num_data = df.select_dtypes(include=numerics)\n    cat_data = df.select_dtypes(exclude=numerics)\n    \n    return num_data, cat_data\n\nnum_data,cat_data = sep_data(df)","9dc73ef3":"df.Department.value_counts()","ce1ff921":"df.salary.value_counts()","02a87769":"## 1. Let's run VIF to check highly correlated and hence redundant variables.\n\nfeatures = num_data.drop(columns='left')\nfeature_list = \"+\".join(features.columns)\ny, X = dmatrices('left~'+feature_list,num_data,return_type='dataframe')","b6371b62":"vif = pd.DataFrame()\nvif['VIF Factor'] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]\nvif['Features'] = X.columns\nvif","90a863ed":"num_data.groupby('left').mean()","4cd12ab6":"fig,ax = plt.subplots(2,3, figsize=(17,12))               \nsns.distplot(x= df['satisfaction_level'], ax = ax[0,0]) \nsns.distplot(df['last_evaluation'], ax = ax[0,1]) \nsns.countplot(df['number_project'], ax = ax[0,2]) \nsns.distplot(df['average_montly_hours'], ax = ax[1,0]) \nsns.distplot(df['time_spend_company'], ax = ax[1,1]) \nsns.countplot(df['promotion_last_5years'], ax = ax[1,2])\nplt.show()","180a104f":"fig = plt.figure(figsize=(15,7))\nsns.barplot(x='left', y ='satisfaction_level' ,data=df)\nplt.show()","513636db":"#Target Variable distribution\nfig = plt.figure(figsize=(15,7))\nsns.countplot(x='left',data=df)\nplt.show()","0c394070":"fig,ax = plt.subplots(2,1, figsize=(17,12))\nsns.countplot(x='salary',data=df , ax=ax[0])\nsns.countplot(x='salary', hue = 'left' , data=df , ax=ax[1])\nplt.show()","ca8d44c8":"fig,ax = plt.subplots(2,1, figsize=(17,12))\nsns.countplot(x='Department',data=df , ax=ax[0])\nsns.countplot(df['Department'],hue=df['left'],data=df , ax=ax[1])\nplt.show()\n","105a1e6e":"fig = plt.figure(figsize=(15,7))\nsns.barplot(y='left',x='Department', hue= 'salary',data=df)\nplt.show()","31e2a365":"fig,ax = plt.subplots(2,1, figsize=(17,12))\nsns.countplot(df['Department'],hue=df['left'],data=df , ax=ax[0])\nsns.boxplot(y='average_montly_hours', x='Department', hue= 'left',data=df , ax=ax[1])\nplt.show()","dba1de8d":"fig = plt.figure(figsize=(15,7))\nsns.boxplot(y='average_montly_hours',x='time_spend_company', hue= 'left',data=df)\nplt.show()","a881b011":"fig,ax = plt.subplots(2,1, figsize=(17,12))\nsns.boxplot(y='satisfaction_level',x='time_spend_company', hue= 'left',data=df , ax=ax[0])\nsns.boxplot(y='last_evaluation',x='time_spend_company', hue= 'left',data=df , ax=ax[1])\nplt.show()","3865f6a0":"fig = plt.figure(figsize=(15,7))\nsns.countplot(x='salary', hue = 'number_project',data=df)\nplt.show()","9c032c61":"fig = plt.figure(figsize=(15,7))\nsns.boxplot(y='last_evaluation',x='number_project', hue= 'salary',data=df)\nplt.show()","a931559c":"fig = plt.figure(figsize=(15,7))\nsns.boxplot(y='satisfaction_level',x='number_project', hue= 'salary',data=df)\nplt.show()","1c949a94":"# Find the effect of satisfaction level and the average monthly hours with department and salary level\n# on departure of  employees.\nplt.figure(figsize=(9,9))\nsns.relplot(x=\"satisfaction_level\",\n                y=\"average_montly_hours\",\n                col=\"Department\",\n                hue=\"salary\",\n                kind=\"scatter\",\n                height=10,\n                aspect=0.3,\n                data=df[df['left']==1])","f731f92f":"plt.figure(figsize=(9,9))\nsns.relplot(x=\"last_evaluation\",\n                y=\"average_montly_hours\",\n                col=\"Department\",\n                hue=\"salary\",\n                kind=\"scatter\",\n                height=10,\n                aspect=0.3,\n                data=df[df['left']==1])","312ddef1":"df['Work_accident'].value_counts()","ddf7801b":"fig = plt.figure(figsize=(15,7))\nsns.countplot(x='Work_accident', hue = 'left',data=df)\nplt.show()","04ddacd8":"fig = plt.figure(figsize=(15,7))\ncor_mat=num_data.corr()\nsns.heatmap(cor_mat ,annot = True, cmap='Blues')\nplt.show()","095e4af4":"fig = plt.figure(figsize=(15,7))\nsns.scatterplot(x=\"average_montly_hours\", y= \"satisfaction_level\",hue='left' ,data=df)\nplt.show()","60e54e7c":"df.salary=df.salary.astype('category')\ndf.salary=df.salary.cat.reorder_categories(['low', 'medium', 'high'])\ndf.salary = df.salary.cat.codes","1b7464f2":"departments = pd.get_dummies(df.Department)\ndepartments.head(5)","5c082ef6":"departments = departments.drop(\"accounting\", axis=1)\ndf = df.drop(\"Department\", axis=1)\ndf = df.join(departments)\ndf.head(5)","9ea17c16":"df.satisfaction_level=df.satisfaction_level*10\ndf.last_evaluation=df.last_evaluation*10","d4b6b955":"n_employees = len(df)\nprint(df.left.value_counts())\nprint(df.left.value_counts()\/n_employees*100)","f78b3bb1":"def count_target_plot(data,target):\n    plt.figure(figsize=(8,8))\n    ax=sns.countplot(data=data,x=data[target],order=data[target].value_counts().index)\n    plt.xlabel('Target Variable- Left')\n    plt.ylabel('Distribution of target variable')\n    plt.title('Distribution of Left')\n    total = len(data)\n    for p in ax.patches:\n            ax.annotate('{:.1f}%'.format(100*p.get_height()\/total), (p.get_x()+0.1, p.get_height()+5))","361fb656":"count_target_plot(df,'left')","b0300886":"target=df.left\nfeatures=df.drop('left',axis=1)","0b541063":"splitter = StratifiedShuffleSplit(n_splits=10, test_size=0.2, random_state=42)\nfor train_index, test_index in splitter.split(features, target):\n        features_train, features_test = features.iloc[train_index], features.iloc[test_index]\n        target_train, target_test = target.iloc[train_index], target.iloc[test_index]","e4a5ffa9":"smote = SMOTE(sampling_strategy='minority', random_state=47)\nos_values, os_labels = smote.fit_sample(features_train, target_train)\nfeatures_train = pd.DataFrame(os_values)\ntarget_train = pd.DataFrame(os_labels)\nprint(\"Dimensions of Oversampled dataset is :\", os_values.shape)","d19f4103":"count_target_plot(target_train,'left')","092900c2":"logr = LogisticRegression(random_state=42)\nlogr.fit(features_train, target_train)\ny_pred_logr = logr.predict(features_test)\nprint(\"Score on Train data : \" , logr.score(features_train,target_train)*100)\nprint(\"Score on Test data : \" , accuracy_score(target_test, y_pred_logr)*100)","2766a4bb":"DT = DecisionTreeClassifier(random_state=42)\nDT.fit(features_train, target_train)\ny_pred_DT = DT.predict(features_test)\nprint(\"Score on Train data : \" , DT.score(features_train,target_train)*100)\nprint(\"Score on Test data : \" , accuracy_score(target_test, y_pred_DT)*100)","d9567abe":"DT1 = DecisionTreeClassifier(max_depth=12,min_samples_leaf=1, random_state=42)\nDT1.fit(features_train, target_train)\ny_pred_DT1 = DT1.predict(features_test)\nprint(\"Score on Train data : \" , DT1.score(features_train,target_train)*100)\nprint(\"Score on Test data : \" , accuracy_score(target_test, y_pred_DT1)*100)","6ae96ac4":"RF = RandomForestClassifier(random_state=42)\nRF.fit(features_train, target_train)\ny_pred_RF = RF.predict(features_test)\nprint(\"Score on Train data : \" , RF.score(features_train,target_train)*100)\nprint(\"Score on Test data : \" , accuracy_score(target_test, y_pred_RF)*100)","1ed110d8":"RF1 = RandomForestClassifier(min_samples_leaf=1, max_features= 6, max_depth=16,random_state=42)\nRF1.fit(features_train, target_train)\ny_pred_RF1 = RF.predict(features_test)\nprint(\"Score on Train data : \" , RF1.score(features_train,target_train)*100)\nprint(\"Score on Test data : \" , accuracy_score(target_test, y_pred_RF1)*100)","f89f97b6":"knn = KNeighborsClassifier(n_neighbors=3, metric='euclidean')\nknn.fit(features_train, target_train)\ny_pred_knn = knn.predict(features_test)\nprint(\"Score on Train data : \" , knn.score(features_train,target_train)*100)\nprint(\"Score on Test data : \" , accuracy_score(target_test, y_pred_knn)*100)","e35e3161":"print(\"Logistic Regression : \")\nprint(classification_report(target_test,y_pred_logr))\nprint(\"=======================================================\")\nprint(\"Decison Tree : \")\nprint(classification_report(target_test,y_pred_DT))\nprint(classification_report(target_test,y_pred_DT1))\nprint(\"=======================================================\")\nprint(\"Random Forest : \")\nprint(classification_report(target_test,y_pred_RF))\nprint(classification_report(target_test,y_pred_RF1))\nprint(\"=======================================================\")\nprint(\"KNN : \")\nprint(classification_report(target_test,y_pred_knn))","0195e399":"When we compare the proportions between the hired and those who leave, we notice that for the hired the satisfaction ranges that prevail are those of satisfaction and regular satisfaction, the two corresponding to 87% of the hired. For those who leave the dissatisfied range alone corresponds to 49% of this set. It is quite significant to think that half of who leaves were not satisfied.\n\nAmong the hired we observe that the satisfied ones form the largest group, followed by those of regular satisfaction, dissatisfied individuals are the minority.","c2d6712e":"This chart brings new and relevant information, only who left worked on 7 projects, one more evidence of who left was overwhelmed. Those who left the company involved in 2 projects, regardless of the salary range, predominantly received low scores in their evaluations. People with more than 4 projects tended to receive better grades in their assessments.","7e8ed1dc":"### Random Forest","56c9637e":"### We know that the employee who left work for longer hours and lower salaries, did they manage to stay in the company for many years?","7a00da02":"Here,In the set of data we have people who have worked in the company between 2 and 10 years. For those who worked for a total of 2 years, we noticed the smallest difference between the hired and who leaves, but as already noticed, even in this group, who leaves tended to work for more hours. The data for those who worked for 3 years is quite different from the others, they exhibit a new grouping of people who leaves and worked less hours than the average monthly. During the univariate graphs section, when we printed the distribution of the total number of people per years worked, we observed a peak in the 3rd year and from there a fall in the total of people per year after the 4th year.","dee48cf4":"### Percentage of Employee Churn\/Left","448830ee":"The Tagret Variable is balanced now, we can move forward to predictive modeling ","7fcd674a":"### Work Accident","34f347c5":"## Explorartory Ananlysis","fba66d2d":"As the values in the column satisfaction_level and last_evaluation are not in the order of the other entries we can multiply the values in the column by a constant to make it in the order of the column values. Since the columns are a kind of rating multiplying it by 10 won't change the meaning, previously the ratings were out of 1,now it would be out of 10.","78f70e02":"### Which Department employess left the company most","bce70ec0":"11,428 employees stayed, which accounts for about 76% of the total employee count. Similarly, 3,571 employees left, which accounts for about 24% of them","68172ac9":"Both people with high, medium or low salary worked on 2 projects or more. Around 140 people worked on 7 projects and received a low salary. So answering the question, we can see no evidence that the larger the number of projects, the better the salary range.","71d3fb91":"The above table shows that there is no variable with a 'high\" Variance Inflation Factor.\nSo, this method suggests we should not drop any variable","a01a5278":"### Employees in each Department","547307e6":"### Decision tree ","56b83e44":"### Splitting the dataset\nWe will split both target and features into train and test sets with 80%\/20% ratio, respectively","0e2fdd9e":"## Data Preprocessing\nConvert the salary column to categorical","07111411":"### Did those who left receive a low salary?","e559119f":"### KNN","8cd65e93":"### Did Employees who were involved in more projects received more?`","d6424325":"<h3>Problem Statement : <\/h3>Employees are considered as vital asset for an Organization and if its valuable employees are leaving, it can be a great loss for a company. Our example concerns a big company that wants to understand why some of their best and most experienced employees are leaving prematurely. The company also wishes to predict which valuable employees will leave next\n\n<h3>Submission by :<\/h3> Name : Kunal Aaryen Sinha <br> ","ef3a606e":"### Department was salaries in relation with employees left","64e5e7f4":"The Sales department is the one with the largest number of employees, followed by the Technical and Support department, totaling 9,089 employees. Management is the smallest of them with 630 employees.","2dfecefb":"Both people with high, medium or low salary worked on 2 projects or more. 144 people worked on 7 projects and received a low salary. So answering the question, I saw no evidence that the larger the number of projects, the better the salary range.\n\nEven though they have suffered an accident at work, the employees remain in the company. This variable does not seem to be related to the employee's exit.\n\nNow that we know the importance of the variables satisfaction_level and average_montly_hours we will calculate the value of the pearson correlation for the data set variables hr:","88be03f0":"### Is Work Load a reason for employee Atrrition.","981079a8":"### Logistic Regression","3f0ad5a0":"### Does the salary received depends upon  on projects done","c904dad4":"We see dissatisfaction with employees who leave the company in the 3rd or 4th year. People who left the company in the 5th grade, mostly showed low or high satisfaction. Most of then who worked at the company for 6 years demonstrated a high satisfaction level of over 0.75. \n\nAgain the 3rd year appears as a divisor. The people who left the company in the 3rd year received a low score in the last evaluation, a fact that does not repeat from the 4th to the 6th year, where the majority of those employees received grades higher than 0.8.","b78707a8":"We have people who left with the most diverse levels of satisfaction, the proportion of people by satisfaction range and numbers of projects can help us to understand these values. In this plot we have the proportion of the satisfaction of who left by numbers of projects. We see the polarization again, people with 2 projects showed a low to regular satisfaction. The same goes for the group with 6 or 7 projects. Employees who worked on 4 or 5 projects had a predominantly high satisfaction.\n","3c4b194a":"### Is the employee's dissatisfaction a factor for departure? ","8a64f8a2":"## Model Building","93a565dc":"### Correlation Matrix","32282fae":"Satisfaction - It is possible to observe that there are employees with a low level of satisfaction (<0.3). From the total, 1,941 people had a satisfaction level of less than 0.3. At the other end of the distribution, we have 6,502 employees with a satisfaction level equal to or above 0.7.\n\nLast Evaluation - The company is made up of a majority of employees with medium to high valuation. For this analysis we will consider that a grade equal to or greater than 7 is considered high. Thus, we have 8,015 employees well evaluated by the company, this corresponds to 53% of the people.\n\nNumber of Project - Most are the emloyees are he ones who did 3-4 projects and the employees with 7 Projects are the least.There is no record of anyone working on a single project. The employees worked on at least 2 and at most 7 projects.\n\nAverage Monthly Hours -  Most of the employees worked for 150 to 250 hrs and highest density\/frequency can be seen in this range\n\nTime Spend - This is a positively skewed distribution, with a reduction in the number of employees per year as it approaches the right tail. Let's check this trend and confirm that the median value is less than the mean value through the statistical summary of the variable time_spend_company:\n\nPromotions - Very few people promoted. Only 2% of people have already been promoted. Knowing the career plan policy would be important to understand this shortage of promotions.","859e1144":"As we saw above the accuracy is 100% on training and test set, model is overfitting,\nSo we will set the maximum depth , limiting the sample size, and maximum features.\n","a27e645a":"As we saw above the accuracy is 100% on training and test set, model is overfitting. So we will purne the tree, by setting the maximum depth and limiting the sample size","42a0717f":"We see here a summary of the observations made in this project, we conclude that the people who left the company were overburdened and unsatisfied. For those people we have:\n\n- The unsatisfied who worked less than the general average and those who worked more than the average and more than the remain;\n\n- Those with a good level of satisfaction, but who also had a monthly average of hours worked over 201 hours of the general average.","d7b19574":"###  Seperating target and features\nlets seperate the dependent variable(target) and the independent variables(predictors) seperately","a9c7a44f":"Dataset contains 14999 rows and 10 columns, each row has the details of an employee.  \n2 variables are categorical, remaining columns are of int and float\n\n### Checking for any missing values","d60b4abf":"The darker, the more correlated are the variables, the whites have negative correlation values and the positive blue ones.\n\nFor the left variable the correlations were found:\n\n- Satisfaction_level = This was expected, with a value of -0.39 that is variable with the strongest correlation with left\n- Work_accident = Poor correlation, value -0.15\n- Time_spend_company = Poor correlation, value 0.14\n- Average_montly_hours = Very poor correlation, value 0.071\n- last_evaluation = Very weak correlation, value 0.0066\n- number_project = Very poor correlation, value 0.024\n- Promotion_last_5years = Very weak correlation, value -0.062"}}