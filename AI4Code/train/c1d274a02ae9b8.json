{"cell_type":{"a0e271c3":"code","901a3991":"code","f250a9fc":"code","3a92ebd8":"code","5b5a38ce":"code","1932531d":"code","efac631a":"code","21a6c951":"code","8ad3a266":"code","7014676d":"code","be0067d5":"code","12022303":"code","a9ecb213":"code","6326dc69":"code","9cd53136":"code","302b3e85":"code","81ae5e6d":"code","27529486":"code","315e750c":"code","8cd778d3":"code","5629eb4a":"code","088eb905":"code","cfe595a9":"code","81a43176":"code","0ab29b17":"code","2712ef4c":"code","64c66b97":"code","66a6375a":"code","fd01a6b4":"markdown","ecf2a40c":"markdown","4a288c5a":"markdown","f98f71e6":"markdown","c091d7d1":"markdown","734e8f41":"markdown","69ab225d":"markdown","f7df8e28":"markdown","24cfe023":"markdown","4807e2eb":"markdown","8ac2fe3b":"markdown","6d6b6661":"markdown","628c6c8b":"markdown","ba1ac5f1":"markdown","f8fd8d3e":"markdown","55bf5dfd":"markdown","067d3d20":"markdown","1c3b93b0":"markdown","8edfa79b":"markdown"},"source":{"a0e271c3":"BUILD95 = True\nBUILD96 = True\n\nimport numpy as np, pandas as pd, os, gc\nfrom sklearn.model_selection import GroupKFold\nfrom sklearn.metrics import roc_auc_score\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# COLUMNS WITH STRINGS\nstr_type = ['ProductCD', 'card4', 'card6', 'P_emaildomain', 'R_emaildomain','M1', 'M2', 'M3', 'M4','M5',\n            'M6', 'M7', 'M8', 'M9', 'id_12', 'id_15', 'id_16', 'id_23', 'id_27', 'id_28', 'id_29', 'id_30', \n            'id_31', 'id_33', 'id_34', 'id_35', 'id_36', 'id_37', 'id_38', 'DeviceType', 'DeviceInfo']\nstr_type += ['id-12', 'id-15', 'id-16', 'id-23', 'id-27', 'id-28', 'id-29', 'id-30', \n            'id-31', 'id-33', 'id-34', 'id-35', 'id-36', 'id-37', 'id-38']\n\n# FIRST 53 COLUMNS\ncols = ['TransactionID', 'TransactionDT', 'TransactionAmt',\n       'ProductCD', 'card1', 'card2', 'card3', 'card4', 'card5', 'card6',\n       'addr1', 'addr2', 'dist1', 'dist2', 'P_emaildomain', 'R_emaildomain',\n       'C1', 'C2', 'C3', 'C4', 'C5', 'C6', 'C7', 'C8', 'C9', 'C10', 'C11',\n       'C12', 'C13', 'C14', 'D1', 'D2', 'D3', 'D4', 'D5', 'D6', 'D7', 'D8',\n       'D9', 'D10', 'D11', 'D12', 'D13', 'D14', 'D15', 'M1', 'M2', 'M3', 'M4',\n       'M5', 'M6', 'M7', 'M8', 'M9']\n\n# V COLUMNS TO LOAD DECIDED BY CORRELATION EDA\n# https:\/\/www.kaggle.com\/cdeotte\/eda-for-columns-v-and-id\nv =  [1, 3, 4, 6, 8, 11]\nv += [13, 14, 17, 20, 23, 26, 27, 30]\nv += [36, 37, 40, 41, 44, 47, 48]\nv += [54, 56, 59, 62, 65, 67, 68, 70]\nv += [76, 78, 80, 82, 86, 88, 89, 91]\n\n#v += [96, 98, 99, 104] #relates to groups, no NAN \nv += [107, 108, 111, 115, 117, 120, 121, 123] # maybe group, no NAN\nv += [124, 127, 129, 130, 136] # relates to groups, no NAN\n\n# LOTS OF NAN BELOW\nv += [138, 139, 142, 147, 156, 162] #b1\nv += [165, 160, 166] #b1\nv += [178, 176, 173, 182] #b2\nv += [187, 203, 205, 207, 215] #b2\nv += [169, 171, 175, 180, 185, 188, 198, 210, 209] #b2\nv += [218, 223, 224, 226, 228, 229, 235] #b3\nv += [240, 258, 257, 253, 252, 260, 261] #b3\nv += [264, 266, 267, 274, 277] #b3\nv += [220, 221, 234, 238, 250, 271] #b3\n\nv += [294, 284, 285, 286, 291, 297] # relates to grous, no NAN\nv += [303, 305, 307, 309, 310, 320] # relates to groups, no NAN\nv += [281, 283, 289, 296, 301, 314] # relates to groups, no NAN\n#v += [332, 325, 335, 338] # b4 lots NAN\n\ncols += ['V'+str(x) for x in v]\ndtypes = {}\nfor c in cols+['id_0'+str(x) for x in range(1,10)]+['id_'+str(x) for x in range(10,34)]+\\\n    ['id-0'+str(x) for x in range(1,10)]+['id-'+str(x) for x in range(10,34)]:\n        dtypes[c] = 'float32'\nfor c in str_type: dtypes[c] = 'category'","901a3991":"%%time\n# LOAD TRAIN\nX_train = pd.read_csv('..\/input\/ieee-fraud-detection\/train_transaction.csv',index_col='TransactionID', dtype=dtypes, usecols=cols+['isFraud'])\ntrain_id = pd.read_csv('..\/input\/ieee-fraud-detection\/train_identity.csv',index_col='TransactionID', dtype=dtypes)\nX_train = X_train.merge(train_id, how='left', left_index=True, right_index=True)\n# LOAD TEST\nX_test = pd.read_csv('..\/input\/ieee-fraud-detection\/test_transaction.csv',index_col='TransactionID', dtype=dtypes, usecols=cols)\ntest_id = pd.read_csv('..\/input\/ieee-fraud-detection\/test_identity.csv',index_col='TransactionID', dtype=dtypes)\nfix = {o:n for o, n in zip(test_id.columns, train_id.columns)}\ntest_id.rename(columns=fix, inplace=True)\nX_test = X_test.merge(test_id, how='left', left_index=True, right_index=True)\n# TARGET\ny_train = X_train['isFraud'].copy()\ndel train_id, test_id, X_train['isFraud']; x = gc.collect()\n# PRINT STATUS\nprint('Train shape',X_train.shape,'test shape',X_test.shape)","f250a9fc":"# PLOT ORIGINAL D\nplt.figure(figsize=(15,5))\nplt.scatter(X_train.TransactionDT,X_train.D15)\nplt.title('Original D15')\nplt.xlabel('Time')\nplt.ylabel('D15')\nplt.show()","3a92ebd8":"# NORMALIZE D COLUMNS\nfor i in range(1,16):\n    if i in [1,2,3,5,9]: continue\n    X_train['D'+str(i)] =  X_train['D'+str(i)] - X_train.TransactionDT\/np.float32(24*60*60)\n    X_test['D'+str(i)] = X_test['D'+str(i)] - X_test.TransactionDT\/np.float32(24*60*60) ","5b5a38ce":"# PLOT TRANSFORMED D\nplt.figure(figsize=(15,5))\nplt.scatter(X_train.TransactionDT,X_train.D15)\nplt.title('Transformed D15')\nplt.xlabel('Time')\nplt.ylabel('D15n')\nplt.show()","1932531d":"%%time\n# LABEL ENCODE AND MEMORY REDUCE\nfor i,f in enumerate(X_train.columns):\n    # FACTORIZE CATEGORICAL VARIABLES\n    if (np.str(X_train[f].dtype)=='category')|(X_train[f].dtype=='object'): \n        df_comb = pd.concat([X_train[f],X_test[f]],axis=0)\n        df_comb,_ = df_comb.factorize(sort=True)\n        if df_comb.max()>32000: print(f,'needs int32')\n        X_train[f] = df_comb[:len(X_train)].astype('int16')\n        X_test[f] = df_comb[len(X_train):].astype('int16')\n    # SHIFT ALL NUMERICS POSITIVE. SET NAN to -1\n    elif f not in ['TransactionAmt','TransactionDT']:\n        mn = np.min((X_train[f].min(),X_test[f].min()))\n        X_train[f] -= np.float32(mn)\n        X_test[f] -= np.float32(mn)\n        X_train[f].fillna(-1,inplace=True)\n        X_test[f].fillna(-1,inplace=True)","efac631a":"# FREQUENCY ENCODE TOGETHER\ndef encode_FE(df1, df2, cols):\n    for col in cols:\n        df = pd.concat([df1[col],df2[col]])\n        vc = df.value_counts(dropna=True, normalize=True).to_dict()\n        vc[-1] = -1\n        nm = col+'_FE'\n        df1[nm] = df1[col].map(vc)\n        df1[nm] = df1[nm].astype('float32')\n        df2[nm] = df2[col].map(vc)\n        df2[nm] = df2[nm].astype('float32')\n        print(nm,', ',end='')\n        \n# LABEL ENCODE\ndef encode_LE(col,train=X_train,test=X_test,verbose=True):\n    df_comb = pd.concat([train[col],test[col]],axis=0)\n    df_comb,_ = df_comb.factorize(sort=True)\n    nm = col\n    if df_comb.max()>32000: \n        train[nm] = df_comb[:len(train)].astype('int32')\n        test[nm] = df_comb[len(train):].astype('int32')\n    else:\n        train[nm] = df_comb[:len(train)].astype('int16')\n        test[nm] = df_comb[len(train):].astype('int16')\n    del df_comb; x=gc.collect()\n    if verbose: print(nm,', ',end='')\n        \n# GROUP AGGREGATION MEAN AND STD\n# https:\/\/www.kaggle.com\/kyakovlev\/ieee-fe-with-some-eda\ndef encode_AG(main_columns, uids, aggregations=['mean'], train_df=X_train, test_df=X_test, \n              fillna=True, usena=False):\n    # AGGREGATION OF MAIN WITH UID FOR GIVEN STATISTICS\n    for main_column in main_columns:  \n        for col in uids:\n            for agg_type in aggregations:\n                new_col_name = main_column+'_'+col+'_'+agg_type\n                temp_df = pd.concat([train_df[[col, main_column]], test_df[[col,main_column]]])\n                if usena: temp_df.loc[temp_df[main_column]==-1,main_column] = np.nan\n                temp_df = temp_df.groupby([col])[main_column].agg([agg_type]).reset_index().rename(\n                                                        columns={agg_type: new_col_name})\n\n                temp_df.index = list(temp_df[col])\n                temp_df = temp_df[new_col_name].to_dict()   \n\n                train_df[new_col_name] = train_df[col].map(temp_df).astype('float32')\n                test_df[new_col_name]  = test_df[col].map(temp_df).astype('float32')\n                \n                if fillna:\n                    train_df[new_col_name].fillna(-1,inplace=True)\n                    test_df[new_col_name].fillna(-1,inplace=True)\n                \n                print(\"'\"+new_col_name+\"'\",', ',end='')\n                \n# COMBINE FEATURES\ndef encode_CB(col1,col2,df1=X_train,df2=X_test):\n    nm = col1+'_'+col2\n    df1[nm] = df1[col1].astype(str)+'_'+df1[col2].astype(str)\n    df2[nm] = df2[col1].astype(str)+'_'+df2[col2].astype(str) \n    encode_LE(nm,verbose=False)\n    print(nm,', ',end='')\n    \n# GROUP AGGREGATION NUNIQUE\ndef encode_AG2(main_columns, uids, train_df=X_train, test_df=X_test):\n    for main_column in main_columns:  \n        for col in uids:\n            comb = pd.concat([train_df[[col]+[main_column]],test_df[[col]+[main_column]]],axis=0)\n            mp = comb.groupby(col)[main_column].agg(['nunique'])['nunique'].to_dict()\n            train_df[col+'_'+main_column+'_ct'] = train_df[col].map(mp).astype('float32')\n            test_df[col+'_'+main_column+'_ct'] = test_df[col].map(mp).astype('float32')\n            print(col+'_'+main_column+'_ct, ',end='')","21a6c951":"%%time\n# TRANSACTION AMT CENTS\nX_train['cents'] = (X_train['TransactionAmt'] - np.floor(X_train['TransactionAmt'])).astype('float32')\nX_test['cents'] = (X_test['TransactionAmt'] - np.floor(X_test['TransactionAmt'])).astype('float32')\nprint('cents, ', end='')\n# FREQUENCY ENCODE: ADDR1, CARD1, CARD2, CARD3, P_EMAILDOMAIN\nencode_FE(X_train,X_test,['addr1','card1','card2','card3','P_emaildomain'])\n# COMBINE COLUMNS CARD1+ADDR1, CARD1+ADDR1+P_EMAILDOMAIN\nencode_CB('card1','addr1')\nencode_CB('card1_addr1','P_emaildomain')\n# FREQUENCY ENOCDE\nencode_FE(X_train,X_test,['card1_addr1','card1_addr1_P_emaildomain'])\n# GROUP AGGREGATE\nencode_AG(['TransactionAmt','D9','D11'],['card1','card1_addr1','card1_addr1_P_emaildomain'],['mean','std'],usena=True)","8ad3a266":"cols = list( X_train.columns )\ncols.remove('TransactionDT')\nfor c in ['D6','D7','D8','D9','D12','D13','D14']:\n    cols.remove(c)\n    \n# FAILED TIME CONSISTENCY TEST\nfor c in ['C3','M5','id_08','id_33']:\n    cols.remove(c)\nfor c in ['card4','id_07','id_14','id_21','id_30','id_32','id_34']:\n    cols.remove(c)\nfor c in ['id_'+str(x) for x in range(22,28)]:\n    cols.remove(c)","7014676d":"print('NOW USING THE FOLLOWING',len(cols),'FEATURES.')\nnp.array(cols)","be0067d5":"# CHRIS - TRAIN 75% PREDICT 25%\nidxT = X_train.index[:3*len(X_train)\/\/4]\nidxV = X_train.index[3*len(X_train)\/\/4:]\n\n# KONSTANTIN - TRAIN 4 SKIP 1 PREDICT 1 MONTH\n#idxT = X_train.index[:417559]\n#idxV = X_train.index[-89326:]","12022303":"import xgboost as xgb\nprint(\"XGBoost version:\", xgb.__version__)\n\nif BUILD95:\n    clf = xgb.XGBClassifier( \n        n_estimators=2000,\n        max_depth=12, \n        learning_rate=0.02, \n        subsample=0.8,\n        colsample_bytree=0.4, \n        missing=-1, \n        eval_metric='auc',\n        # USE CPU\n        #nthread=4,\n        #tree_method='hist' \n        # USE GPU\n        tree_method='gpu_hist' \n    )\n    h = clf.fit(X_train.loc[idxT,cols], y_train[idxT], \n        eval_set=[(X_train.loc[idxV,cols],y_train[idxV])],\n        verbose=50, early_stopping_rounds=100)","a9ecb213":"if BUILD95:\n\n    feature_imp = pd.DataFrame(sorted(zip(clf.feature_importances_,cols)), columns=['Value','Feature'])\n    plt.figure(figsize=(20, 10))\n    sns.barplot(x=\"Value\", y=\"Feature\", data=feature_imp.sort_values(by=\"Value\", ascending=False).iloc[:50])\n    plt.title('XGB95 Most Important Features')\n    plt.tight_layout()\n    plt.show()\n    del clf, h; x=gc.collect()","6326dc69":"import datetime\nSTART_DATE = datetime.datetime.strptime('2017-11-30', '%Y-%m-%d')\nX_train['DT_M'] = X_train['TransactionDT'].apply(lambda x: (START_DATE + datetime.timedelta(seconds = x)))\nX_train['DT_M'] = (X_train['DT_M'].dt.year-2017)*12 + X_train['DT_M'].dt.month \n\nX_test['DT_M'] = X_test['TransactionDT'].apply(lambda x: (START_DATE + datetime.timedelta(seconds = x)))\nX_test['DT_M'] = (X_test['DT_M'].dt.year-2017)*12 + X_test['DT_M'].dt.month ","9cd53136":"if BUILD95:\n    oof = np.zeros(len(X_train))\n    preds = np.zeros(len(X_test))\n\n    skf = GroupKFold(n_splits=6)\n    for i, (idxT, idxV) in enumerate( skf.split(X_train, y_train, groups=X_train['DT_M']) ):\n        month = X_train.iloc[idxV]['DT_M'].iloc[0]\n        print('Fold',i,'withholding month',month)\n        print(' rows of train =',len(idxT),'rows of holdout =',len(idxV))\n        clf = xgb.XGBClassifier(\n            n_estimators=5000,\n            max_depth=12,\n            learning_rate=0.02,\n            subsample=0.8,\n            colsample_bytree=0.4,\n            missing=-1,\n            eval_metric='auc',\n            # USE CPU\n            #nthread=4,\n            #tree_method='hist'\n            # USE GPU\n            tree_method='gpu_hist' \n        )        \n        h = clf.fit(X_train[cols].iloc[idxT], y_train.iloc[idxT], \n                eval_set=[(X_train[cols].iloc[idxV],y_train.iloc[idxV])],\n                verbose=100, early_stopping_rounds=200)\n    \n        oof[idxV] += clf.predict_proba(X_train[cols].iloc[idxV])[:,1]\n        preds += clf.predict_proba(X_test[cols])[:,1]\/skf.n_splits\n        del h, clf\n        x=gc.collect()\n    print('#'*20)\n    print ('XGB95 OOF CV=',roc_auc_score(y_train,oof))","302b3e85":"if BUILD95:\n    plt.hist(oof,bins=100)\n    plt.ylim((0,5000))\n    plt.title('XGB OOF')\n    plt.show()\n\n    X_train['oof'] = oof\n    X_train.reset_index(inplace=True)\n    X_train[['TransactionID','oof']].to_csv('oof_xgb_95.csv')\n    X_train.set_index('TransactionID',drop=True,inplace=True)\n    \nelse: X_train['oof'] = 0","81ae5e6d":"if BUILD95:\n    sample_submission = pd.read_csv('..\/input\/ieee-fraud-detection\/sample_submission.csv')\n    sample_submission.isFraud = preds\n    sample_submission.to_csv('sub_xgb_95.csv',index=False)\n\n    plt.hist(sample_submission.isFraud,bins=100)\n    plt.ylim((0,5000))\n    plt.title('XGB95 Submission')\n    plt.show()","27529486":"X_train['day'] = X_train.TransactionDT \/ (24*60*60)\nX_train['uid'] = X_train.card1_addr1.astype(str)+'_'+np.floor(X_train.day-X_train.D1).astype(str)\n\nX_test['day'] = X_test.TransactionDT \/ (24*60*60)\nX_test['uid'] = X_test.card1_addr1.astype(str)+'_'+np.floor(X_test.day-X_test.D1).astype(str)","315e750c":"%%time\n# FREQUENCY ENCODE UID\nencode_FE(X_train,X_test,['uid'])\n# AGGREGATE \nencode_AG(['TransactionAmt','D4','D9','D10','D15'],['uid'],['mean','std'],fillna=True,usena=True)\n# AGGREGATE\nencode_AG(['C'+str(x) for x in range(1,15) if x!=3],['uid'],['mean'],X_train,X_test,fillna=True,usena=True)\n# AGGREGATE\nencode_AG(['M'+str(x) for x in range(1,10)],['uid'],['mean'],fillna=True,usena=True)\n# AGGREGATE\nencode_AG2(['P_emaildomain','dist1','DT_M','id_02','cents'], ['uid'], train_df=X_train, test_df=X_test)\n# AGGREGATE\nencode_AG(['C14'],['uid'],['std'],X_train,X_test,fillna=True,usena=True)\n# AGGREGATE \nencode_AG2(['C13','V314'], ['uid'], train_df=X_train, test_df=X_test)\n# AGGREATE \nencode_AG2(['V127','V136','V309','V307','V320'], ['uid'], train_df=X_train, test_df=X_test)\n# NEW FEATURE\nX_train['outsider15'] = (np.abs(X_train.D1-X_train.D15)>3).astype('int8')\nX_test['outsider15'] = (np.abs(X_test.D1-X_test.D15)>3).astype('int8')\nprint('outsider15')","8cd778d3":"cols = list( X_train.columns )\ncols.remove('TransactionDT')\nfor c in ['D6','D7','D8','D9','D12','D13','D14']:\n    cols.remove(c)\nfor c in ['oof','DT_M','day','uid']:\n    cols.remove(c)\n    \n# FAILED TIME CONSISTENCY TEST\nfor c in ['C3','M5','id_08','id_33']:\n    cols.remove(c)\nfor c in ['card4','id_07','id_14','id_21','id_30','id_32','id_34']:\n    cols.remove(c)\nfor c in ['id_'+str(x) for x in range(22,28)]:\n    cols.remove(c)","5629eb4a":"print('NOW USING THE FOLLOWING',len(cols),'FEATURES.')\nnp.array(cols)","088eb905":"# CHRIS - TRAIN 75% PREDICT 25%\nidxT = X_train.index[:3*len(X_train)\/\/4]\nidxV = X_train.index[3*len(X_train)\/\/4:]\n\n# KONSTANTIN - TRAIN 4 SKIP 1 PREDICT 1 MONTH\n#idxT = X_train.index[:417559]\n#idxV = X_train.index[-89326:]","cfe595a9":"if BUILD96:\n    clf = xgb.XGBClassifier( \n        n_estimators=2000,\n        max_depth=12, \n        learning_rate=0.02, \n        subsample=0.8,\n        colsample_bytree=0.4, \n        missing=-1, \n        eval_metric='auc',\n        #nthread=4,\n        #tree_method='hist' \n        tree_method='gpu_hist' \n    )\n    h = clf.fit(X_train.loc[idxT,cols], y_train[idxT], \n        eval_set=[(X_train.loc[idxV,cols],y_train[idxV])],\n        verbose=50, early_stopping_rounds=100)","81a43176":"if BUILD96:\n\n    feature_imp = pd.DataFrame(sorted(zip(clf.feature_importances_,cols)), columns=['Value','Feature'])\n\n    plt.figure(figsize=(20, 10))\n    sns.barplot(x=\"Value\", y=\"Feature\", data=feature_imp.sort_values(by=\"Value\", ascending=False).iloc[:50])\n    plt.title('XGB96 Most Important')\n    plt.tight_layout()\n    plt.show()\n        \n    del clf, h; x=gc.collect()","0ab29b17":"if BUILD96:\n    oof = np.zeros(len(X_train))\n    preds = np.zeros(len(X_test))\n\n    skf = GroupKFold(n_splits=6)\n    for i, (idxT, idxV) in enumerate( skf.split(X_train, y_train, groups=X_train['DT_M']) ):\n        month = X_train.iloc[idxV]['DT_M'].iloc[0]\n        print('Fold',i,'withholding month',month)\n        print(' rows of train =',len(idxT),'rows of holdout =',len(idxV))\n        clf = xgb.XGBClassifier(\n            n_estimators=5000,\n            max_depth=12,\n            learning_rate=0.02,\n            subsample=0.8,\n            colsample_bytree=0.4,\n            missing=-1,\n            eval_metric='auc',\n            # USE CPU\n            #nthread=4,\n            #tree_method='hist'\n            # USE GPU\n            tree_method='gpu_hist' \n        )        \n        h = clf.fit(X_train[cols].iloc[idxT], y_train.iloc[idxT], \n                eval_set=[(X_train[cols].iloc[idxV],y_train.iloc[idxV])],\n                verbose=100, early_stopping_rounds=200)\n    \n        oof[idxV] += clf.predict_proba(X_train[cols].iloc[idxV])[:,1]\n        preds += clf.predict_proba(X_test[cols])[:,1]\/skf.n_splits\n        del h, clf\n        x=gc.collect()\n    print('#'*20)\n    print ('XGB96 OOF CV=',roc_auc_score(y_train,oof))","2712ef4c":"if BUILD96:\n    plt.hist(oof,bins=100)\n    plt.ylim((0,5000))\n    plt.title('XGB OOF')\n    plt.show()\n\n    X_train['oof'] = oof\n    X_train.reset_index(inplace=True)\n    X_train[['TransactionID','oof']].to_csv('oof_xgb_96.csv')\n    X_train.set_index('TransactionID',drop=True,inplace=True)","64c66b97":"if BUILD96:\n    sample_submission = pd.read_csv('..\/input\/ieee-fraud-detection\/sample_submission.csv')\n    sample_submission.isFraud = preds\n    sample_submission.to_csv('sub_xgb_96.csv',index=False)\n\n    plt.hist(sample_submission.isFraud,bins=100)\n    plt.ylim((0,5000))\n    plt.title('XGB96 Submission')\n    plt.show()","66a6375a":"X_test['isFraud'] = sample_submission.isFraud.values\nX_train['isFraud'] = y_train.values\ncomb = pd.concat([X_train[['isFraud']],X_test[['isFraud']]],axis=0)\n\nuids = pd.read_csv('\/kaggle\/input\/ieee-submissions-and-uids\/uids_v4_no_multiuid_cleaning..csv',usecols=['TransactionID','uid']).rename({'uid':'uid2'},axis=1)\ncomb = comb.merge(uids,on='TransactionID',how='left')\nmp = comb.groupby('uid2').isFraud.agg(['mean'])\ncomb.loc[comb.uid2>0,'isFraud'] = comb.loc[comb.uid2>0].uid2.map(mp['mean'])\n\nuids = pd.read_csv('\/kaggle\/input\/ieee-submissions-and-uids\/uids_v1_no_multiuid_cleaning.csv',usecols=['TransactionID','uid']).rename({'uid':'uid3'},axis=1)\ncomb = comb.merge(uids,on='TransactionID',how='left')\nmp = comb.groupby('uid3').isFraud.agg(['mean'])\ncomb.loc[comb.uid3>0,'isFraud'] = comb.loc[comb.uid3>0].uid3.map(mp['mean'])\n\nsample_submission.isFraud = comb.iloc[len(X_train):].isFraud.values\nsample_submission.to_csv('sub_xgb_96_PP.csv',index=False)","fd01a6b4":"# Local Validation\nFor this competition, we used time based local validation. I evaluated features by training on the first 75% of the data and predicting the last 25% of the data. Konstantin had a slightly different local validaiton. He trained on the first 4 months, skipped a month, and predicted the last month. Our current XGB model with 216 features achieves AUC = 0.9363 on Chris' local validation and AUC = 0.9241 on Konstantin's local validation. Note that this is the same AUC that Konstantin's LGBM achieves without magic that he posted [here][1] and [here][2].\n\n[1]: https:\/\/www.kaggle.com\/kyakovlev\/ieee-fe-for-local-test\n[2]: https:\/\/www.kaggle.com\/kyakovlev\/ieee-lgbm-with-groupkfold-cv","ecf2a40c":"# How the Magic Works\nThe magic is two things. First we need a UID variable to identify clients (credit cards). Second, we need to create aggregated group features. Then we remove UID. Suppose we had 10 transactions `A, B, C, D, E, F, G, H, I, J` as below.  \n  \n![table.jpg](attachment:table.jpg)\n  \nIf we only use FeatureX, we can classify 70% of the transactions correctly. Below, yellow circles are `isFraud=1` and blue circles are `isFraud=0` transactions. After the tree model below splits data into left child and right child, we predict `isFraud=1` for left child and `isFraud=0` for right child. Thus 7 out of 10 predictions are correct.\n  \n![tran.jpg](attachment:tran.jpg)  \n  \nNow suppose that we have a UID which defines groups and we make an aggregated feature by taking the average of FeatureX within each group. We can now classify 100% of the transactions correctly. Note that we never use the feature UID in our decision tree.  \n  \n![cred.jpg](attachment:cred.jpg)","4a288c5a":"# Encoding Functions\nBelow are 5 encoding functions. (1) `encode_FE` does frequency encoding where it combines train and test first and then encodes. (2) `encode_LE` is a label encoded for categorical features (3) `encode_AG` makes aggregated features such as aggregated mean and std (4) `encode_CB` combines two columns (5) `encode_AG2` makes aggregated features where it counts how many unique values of one feature is within a group. For more explanation about feature engineering, see the discussion [here][1]\n\n[1]: https:\/\/www.kaggle.com\/c\/ieee-fraud-detection\/discussion\/108575#latest-641841","f98f71e6":"# The Magic Feature - UID\nWe will now create and use the MAGIC FEATURES. First we create a UID which will help our model find clients (credit cards). This UID isn't perfect. Many UID values contain 2 or more clients inside. However our model will detect this and by adding more splits with its trees, it will split these UIDs and find the single clients (credit cards).","c091d7d1":"![image](http:\/\/playagricola.com\/Kaggle\/xgbpp.png)","734e8f41":"# Predict test.csv","69ab225d":"# XGB Fraud with Magic scores LB 0.96\nThis model is part of the 1st place solution to Kaggle's \"IEEE-CIS Fraud Detection\" competition. When this model is ensembled together with [Konstantin's][1] CatBoost and LGBM models, the result achieves public LB 0.9677 and private LB 0.9459 taking first place [here][2]\n  \nIn this kernel, we build two XGB models. The first model does not use the magic features and achieves LB 0.95. The second model uses the magic features and achieves LB 0.96. In the appendix, we demonstrate how to increase LB further with post processing.\n\nReading one million rows of data from disk and engineering features takes 5 minutes using Pandas and CPU. Alternatively if we use RAPIDS cuDF and GPU, it takes only 20 seconds! CPU times are displayed beneath code blocks below and GPU 15x speed up is demonstrated [here][3]. \n\n[1]: https:\/\/www.kaggle.com\/kyakovlev\n[2]: https:\/\/www.kaggle.com\/c\/ieee-fraud-detection\/leaderboard\n[3]: https:\/\/www.kaggle.com\/cdeotte\/rapids-feature-engineering-fraud-0-96\/","f7df8e28":"# Kaggle Submission File XGB_95","24cfe023":"# Kaggle Submission File XGB_96","4807e2eb":"# Post Process File XGB_96_PP\nOur final submission is an ensemble of XGB, CatBoost, and LGBM. Then we post process the ensemble. We will not load the CatBoost and LGBM here, but we will show you the post process. Konstantin wrote a script [here][1] that finds precise UIDs (more precise than `card1_addr1_D1n`). We believe each to be an individual client (credit card). Analysis shows us that all transactions from a single client (one of Konstantin's UIDs) are either all `isFraud=0` or all `isFraud=1`. In other words, all their predictions are the same. Therefore our post process is to replace all predictions from one client with their average prediction including the `isFraud` values from the train dataset. We have two slightly different versions so we apply them sequentially.\n\nApplying post process on our XGB model increases its Public LB to 0.9618 from LB 0.9602. And increases its Private LB to 0.9341 from LB 0.9324. This is an improvement of LB 0.0016 !!\n\n[1]: https:\/\/www.kaggle.com\/kyakovlev\/ieee-uid-detection-v6","8ac2fe3b":"# Load Data\nWe will load all the data except 219 V columns that were determined redundant by correlation analysis [here][1]\n\n[1]: https:\/\/www.kaggle.com\/cdeotte\/eda-for-columns-v-and-id","6d6b6661":"![image](http:\/\/www.playagricola.com\/Kaggle\/9510.png)","628c6c8b":"# Predict test.csv\nWe will predict `test.csv` using GroupKFold with months as groups. The training data are the months December 2017, January 2018, February 2018, March 2018, April 2018, and May 2018. We refer to these months as 12, 13, 14, 15, 16, 17. Fold one in GroupKFold will train on months 13 thru 17 and predict month 12. Note that the only purpose of month 12 is to tell XGB when to `early_stop` we don't actual care about the backwards time predictions. The model trained on months 13 thru 17 will also predict `test.csv` which is forward in time.\n  \nNote that we use local validation to determine features but GroupKFold to predict `test.csv`. Many other prediction schemes were tried but GroupKFold performed best.","ba1ac5f1":"![image](http:\/\/playagricola.com\/Kaggle\/9600.png)","f8fd8d3e":"# Feature Engineering\nWe will now engineer features. All of these features where chosen because each increases local validation. The procedure for engineering features is as follows. First you think of an idea and create a new feature. Then you add it to your model and evaluate whether local validation AUC increases or decreases. If AUC increases keep the feature, otherwise discard the feature.","55bf5dfd":"# Feature Selection - Time Consistency\nWe added 28 new feature above. We have already removed 219 V Columns from correlation analysis done [here][1]. So we currently have 242 features now. We will now check each of our 242 for \"time consistency\". We will build 242 models. Each model will be trained on the first month of the training data and will only use one feature. We will then predict the last month of the training data. We want both training AUC and validation AUC to be above `AUC = 0.5`. It turns out that 19 features fail this test so we will remove them. Additionally we will remove 7 D columns that are mostly NAN. More techniques for feature selection are listed [here][2]\n\n[1]: https:\/\/www.kaggle.com\/cdeotte\/eda-for-columns-v-and-id\n[2]: https:\/\/www.kaggle.com\/c\/ieee-fraud-detection\/discussion\/111308","067d3d20":"# Normalize D Columns\nThe D Columns are \"time deltas\" from some point in the past. We will transform the D Columns into their point in the past. This will stop the D columns from increasing with time. The formula is `D15n = Transaction_Day - D15` and `Transaction_Day = TransactionDT\/(24*60*60)`. Afterward we multiple this number by negative one.","1c3b93b0":"# Local Validation\nWe will now perform local validation with the new magic features included. Chris' local validation now achieves AUC = 0.9472 and Konstantin's local validation achieves AUC = 0.9343. Note that without the magic features we achieved AUC = 0.9363 and AUC = 0.9241. We gained AUC 0.01 in both validations therefore our LB should increase from 0.95 to 0.96. Konstantin's LGBM with magic scores Konstantin local validation AUC = 0.9377 [here][1]\n\n[1]: https:\/\/www.kaggle.com\/kyakovlev\/ieee-basic-fe-part-1","8edfa79b":"# Group Aggregation Features\nFor our model to use the new UID, we need to make lots of aggregated group features. We will add 47 new features! The pictures in the introduction to this notebook explain why this works. Note that after aggregation, we remove UID from our model. We don't use UID directly."}}