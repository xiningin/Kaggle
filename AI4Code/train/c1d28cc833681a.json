{"cell_type":{"87b9e071":"code","9838287c":"code","4faa69e0":"code","a45e46c6":"code","866e4446":"code","90d07573":"code","0ce5a1c0":"code","93d0ab76":"code","9fd99a36":"code","3999c36f":"code","1240abdd":"code","67d074b9":"code","0e958b23":"code","1c7e8353":"code","fe42fd81":"code","ed706f72":"code","06651d8c":"code","849b8e2a":"code","c4942eed":"code","53eae591":"code","bffbca59":"code","b10a2639":"code","afc50442":"code","5248dc27":"code","503f61ff":"code","024e7fe6":"code","536c0d21":"code","72de058a":"code","1954897f":"code","93fc64e5":"code","7f4bbf6c":"code","63d198f1":"code","d30f95be":"code","baa2a717":"code","c7e3401e":"code","b6d5226e":"code","799f5e06":"code","ad8eca14":"code","9dac2a35":"code","44db82e6":"code","cc737965":"code","7cf489b7":"code","4c528823":"code","d265c8b0":"code","2d9220b2":"code","81a36bac":"markdown","b37ddad1":"markdown","4ab5dfaf":"markdown","bf1bf756":"markdown","0a1b243d":"markdown","b8b12a4b":"markdown","6e5716ed":"markdown","236f481c":"markdown","ebc074fa":"markdown","19cc51fd":"markdown","e216a3f7":"markdown","cd3faeb3":"markdown","d44129ee":"markdown","7a9432e2":"markdown","027f31d8":"markdown","f5292b13":"markdown","ee5d6fd4":"markdown","9a115ad3":"markdown","a92290c0":"markdown","9fb48138":"markdown","3129f149":"markdown","cfddc537":"markdown","c2381d01":"markdown","f3ef90eb":"markdown","d30d95fe":"markdown","00ae1ee6":"markdown","56dd08ec":"markdown","73f452ff":"markdown","72ca8acd":"markdown","66fcc10d":"markdown","1c9a5e7e":"markdown"},"source":{"87b9e071":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport cv2\nfrom tqdm import tqdm\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","9838287c":"DATADIR = \"..\/input\/dataset\"\nCATEGORIES = [\"fork\",\"glass\"]","4faa69e0":"for category in CATEGORIES:  # catallar ve bardaklar icin kategoriler\n    path = os.path.join(DATADIR,category)  # dizin yolunu olusturuyoruz\n    for img in os.listdir(path):  # her catal ve bardak resmi icin ilerlet for dongusu ile \n        img_array = cv2.imread(os.path.join(path,img))  # arraye ceviriyoruz\n        plt.imshow(img_array, cmap='gray')  # color map, gray belirledik\n        plt.show()  # gostersin\n        break     \n    break   ","a45e46c6":"print(img_array)","866e4446":"print(img_array.shape)","90d07573":"IMG_HEIGHT = 30\nIMG_WIDTH = 30","0ce5a1c0":"new_array = cv2.resize(img_array, (IMG_WIDTH,IMG_HEIGHT))","93d0ab76":"new_array.shape","9fd99a36":"plt.imshow(new_array, cmap='gray') \nplt.show()","3999c36f":"training_data = []\n\ndef create_training_data():\n    for category in CATEGORIES:  # catallar ve bardaklar\n        path = os.path.join(DATADIR,category)  # resimlerin yolunu olustur\n        class_num = CATEGORIES.index(category)\n        for img in os.listdir(path):  # her catal ve bardak icin devam et ilerlet,\n            try:\n                img_array = cv2.imread(os.path.join(path,img))  # array'e donustur.\n                new_array = cv2.resize(img_array, (IMG_WIDTH,IMG_HEIGHT))\n                training_data.append([new_array, class_num])\n            except Exception as e:\n                pass","1240abdd":"create_training_data()","67d074b9":"print(len(training_data))","0e958b23":"import random\nrandom.shuffle(training_data)","1c7e8353":"for sample in training_data[:10]:\n    print(sample[1])","fe42fd81":"X = []\ny = []\n\nfor features,label in training_data:\n    X.append(features)\n    y.append(label)\n\nprint(X[0].reshape(-1, IMG_HEIGHT, IMG_WIDTH, 3))\n\nX = np.array(X).reshape(-1, IMG_HEIGHT, IMG_WIDTH, 3)","ed706f72":"plt.imshow(X[1])","06651d8c":"import pickle\n\npickle_out = open(\"X.pickle\",\"wb\")\npickle.dump(X, pickle_out)\npickle_out.close()\n\npickle_out = open(\"y.pickle\",\"wb\")\npickle.dump(y, pickle_out)\npickle_out.close()","849b8e2a":"pickle_in = open(\"X.pickle\",\"rb\")\nX = pickle.load(pickle_in)\n\npickle_in = open(\"y.pickle\",\"rb\")\nY = pickle.load(pickle_in)","c4942eed":"X.shape","53eae591":"Y[0:10]","bffbca59":"import tensorflow as tf\nfrom tensorflow.keras.datasets import cifar10\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, Dropout, Activation, Flatten\nfrom tensorflow.keras.layers import Conv2D, MaxPooling2D\nimport pickle","b10a2639":"pickle_in = open(\"X.pickle\",\"rb\")\nX = pickle.load(pickle_in)\n\npickle_in = open(\"y.pickle\",\"rb\")\ny = pickle.load(pickle_in)\n","afc50442":"X = X\/255","5248dc27":"model = Sequential()\nmodel.add(Conv2D(256, (3, 3), input_shape=X.shape[1:]))\nmodel.add(Activation('relu'))\nmodel.add(MaxPooling2D(pool_size=(2, 2)))\nmodel.add(Conv2D(256, (3, 3)))\nmodel.add(Activation('relu'))\nmodel.add(MaxPooling2D(pool_size=(2, 2)))\nmodel.add(Flatten())  # 3 boyutu tek boyuta indirgiyor flatten ile, d\u00fczl\u00fcyoruz.\nmodel.add(Dense(64))\nmodel.add(Dense(1))\nmodel.add(Activation('sigmoid'))","503f61ff":"model.compile(loss='binary_crossentropy',\n              optimizer='adam',\n              metrics=['accuracy'])","024e7fe6":"model.summary()","536c0d21":"history = model.fit(X, y, batch_size=2, epochs=6, validation_split=0.2)","72de058a":"results = model.evaluate(X, y)\nresults","1954897f":"history_dict = history.history\nhistory_dict.keys()\nacc = history.history['acc']\nval_acc = history.history['val_acc']\nloss = history.history['loss']\nval_loss = history.history['val_loss']","93fc64e5":"epochs = range(1, len(acc) + 1)\n\n# \"bo\" is for \"blue dot\"\nplt.plot(epochs, loss, 'bo', label='Training loss')\n# b is for \"solid blue line\"\nplt.plot(epochs, val_loss, 'b', label='Validation loss')\nplt.title('Training and validation loss')\nplt.xlabel('Epochs')\nplt.ylabel('Loss')\nplt.legend()\n\nplt.show()","7f4bbf6c":"plt.clf()  \nacc_values = history_dict['acc']\nval_acc_values = history_dict['val_acc']\n\nplt.plot(epochs, acc, 'bo', label='Training acc')\nplt.plot(epochs, val_acc, 'b', label='Validation acc')\nplt.title('Training and validation accuracy')\nplt.xlabel('Epochs')\nplt.ylabel('Loss')\nplt.legend()\n\nplt.show()","63d198f1":"model = Sequential()\nmodel.add(Conv2D(256, (3, 3), input_shape=X.shape[1:]))\nmodel.add(Activation('relu'))\nmodel.add(MaxPooling2D(pool_size=(2, 2)))\nmodel.add(Conv2D(256, (3, 3)))\nmodel.add(Activation('relu'))\nmodel.add(MaxPooling2D(pool_size=(2, 2)))\nmodel.add(Flatten())  # this converts our 3D feature maps to 1D feature vectors\nmodel.add(Dense(64))\nmodel.add(Dense(1))\nmodel.add(Activation('sigmoid'))\nmodel.add(Dropout(0.0001))","d30f95be":"model.compile(loss='binary_crossentropy',\n              optimizer='adam',\n              metrics=['accuracy'])","baa2a717":"model.fit(X, y, batch_size=2, epochs=10, validation_split=0.2)","c7e3401e":"results = model.evaluate(X, y)\nresults","b6d5226e":"flipped_img = np.fliplr(new_array)\nplt.imshow(flipped_img)\nplt.show()","799f5e06":"for j in range(IMG_WIDTH):\n    for i in range(IMG_HEIGHT):\n        if (i < IMG_HEIGHT-1000):\n          new_array[j][i] = img_array[j][i+1000]\n\nplt.imshow(new_array)\nplt.show()","ad8eca14":"noise = np.random.randint(5, size = (164, 278, 4), dtype = 'uint8')\n\nfor i in range(IMG_WIDTH):\n    for j in range(IMG_HEIGHT):\n        for k in range(2):\n            if (new_array[i][j][k] != 255):\n                new_array[i][j][k] += noise[i][j][k]\nplt.imshow(new_array)\nplt.show()","9dac2a35":"from keras import regularizers\nfrom keras import layers\nfrom keras import models\n\nmodel = Sequential()\nmodel.add(Dense(64, activation='relu', input_shape=(X.shape[1],)))\nmodel.add(Dense(64, activation='relu'))\nmodel.add(Dense(1))\n\n#model.add(layers.Dense(16, \n#           kernel_regularizer = regularizers.12(0.001), \n#           activation=relu,input_shape =1000,))\n\nmodel.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['accuracy'])","44db82e6":"model = Sequential()\nmodel.add(Conv2D(256, (4, 4), input_shape=X.shape[1:]))\nmodel.add(Activation('relu'))\nmodel.add(MaxPooling2D(pool_size=(3, 3)))\nmodel.add(Conv2D(256, (4,4)))\nmodel.add(Activation('relu'))\nmodel.add(MaxPooling2D(pool_size=(3, 3)))\nmodel.add(Flatten())  #this converts our 3D feature maps to 1D feature vectors\nmodel.add(Dense(64))\nmodel.add(Dense(1))\nmodel.add(Activation('sigmoid'))","cc737965":"model.compile(loss='binary_crossentropy',\n              optimizer='RMSprop', #optimizers.RMSprop(lr=1e-4),\n              metrics=['accuracy'])","7cf489b7":"history = model.fit(X, \n                    y,\n                    batch_size=2,\n                    epochs=5,\n                    validation_split=0.2)","4c528823":"history_dict = history.history\nhistory_dict.keys()\nacc = history.history['acc']\nval_acc = history.history['val_acc']\nloss = history.history['loss']\nval_loss = history.history['val_loss']","d265c8b0":"epochs = range(1, len(acc) + 1)\n\nplt.plot(epochs, loss, 'bo', label='Training loss')\nplt.plot(epochs, val_loss, 'b', label='Validation loss')\nplt.title('Training and validation loss')\nplt.xlabel('Epochs')\nplt.ylabel('Loss')\nplt.legend()\n\nplt.show()","2d9220b2":"plt.clf()\nacc_values = history_dict['acc']\nval_acc_values = history_dict['val_acc']\n\nplt.plot(epochs, acc, 'bo', label='Training acc')\nplt.plot(epochs, val_acc, 'b', label='Validation acc')\nplt.title('Training and validation accuracy')\nplt.xlabel('Epochs')\nplt.ylabel('Loss')\nplt.legend()\n\nplt.show()","81a36bac":"X de resimlerimiz vard\u0131, burada ilkel bir normalizasyon i\u015flemi yap\u0131yoruz.","b37ddad1":"Yeni resim dizimizin boyutu.","4ab5dfaf":"Tensorflow altyap\u0131s\u0131n\u0131 kulalnmak i\u00e7in gerekli olan k\u00fct\u00fcphaneleri import ettim.","bf1bf756":"Bu derin \u00f6\u011frenme algoritmas\u0131n\u0131 implemente ederken konvol\u00fcsyonel algoritman\u0131n daha \u00e7ok i\u015fe yarayaca\u011f\u0131n\u0131 d\u00fc\u015f\u00fcnd\u00fcm. \u00c7ok basit olmamas\u0131 i\u00e7in konvol\u00fcsyonel tasarlamak istedim modeli.","0a1b243d":"E\u011fitim datas\u0131n\u0131 olu\u015fturuyoruz. ","b8b12a4b":"Gerekli kutuphaneleri import ederiz ve dizini belirleriz.","6e5716ed":"Dropout kulland\u0131\u011f\u0131mda veriyi \u00f6\u011frenmesi zorla\u015ft\u0131. Normalde 6 epochda iyi performans ald\u0131\u011f\u0131m algoritma 10 epochs da \u00f6\u011frenebilir hale geldi. ","236f481c":"Burada yine ba\u015fka bir \u00e7e\u015fit a\u011f\u0131rl\u0131k reg\u00fclarizesi olan noise (g\u00fcr\u00fclt\u00fc) ekliyoruz. ","ebc074fa":"3, 5 sat\u0131r \u00f6nce sarmalad\u0131\u011f\u0131m pickle dosyas\u0131n\u0131 \u015fimdi a\u00e7\u0131yoruz.","19cc51fd":"Kar\u0131\u015ft\u0131\u011f\u0131n\u0131 g\u00f6rebiliriz.","e216a3f7":"Burada kategorisine g\u00f6re dosyalar\u0131 \u00e7ekerek dizilerde tutuyoruz, ve birini \u00f6rnek olarak g\u00f6steriyoruz.","cd3faeb3":"Ayn\u0131 i\u015flemleri dropout ekleyerek yapmay\u0131 denedim fakat en k\u00fc\u00e7\u00fck oranlarda drop out bile veri setim \u00e7ok dar oldu\u011fu i\u00e7in iyi y\u00f6nde etki etmedi. \u00c7ok b\u00fcy\u00fck loss de\u011ferleri d\u00f6nd\u00fc. Ben de drop out oran\u0131n\u0131 olduk\u00e7a minimal tuttum.","d44129ee":"\u015eimdi ayn\u0131 modeli pencere boyutlar\u0131n\u0131 de\u011fi\u015ftirerek tekrarlad\u0131m.","7a9432e2":"Olu\u015fturdu\u011fum bo\u015f X ve Y dizilerine resimlerin i\u00e7eri\u011fini ve etiketlerini atar\u0131z. \u00c7atal resmine kar\u015f\u0131l\u0131k gelen etiket de \u00e7atal olmal\u0131 gibi.","027f31d8":"Data augmentation olarak g\u00f6rsellere takla att\u0131ran flip arac\u0131l\u0131\u011f\u0131yla bir deneme.","f5292b13":"Olu\u015fturdu\u011fumuz resim dizilerini istedi\u011fimiz zaman kaydedip tekrardan a\u00e7\u0131p kullanabilmek i\u00e7in pickle yap\u0131s\u0131n\u0131 kullan\u0131r\u0131z. B\u00f6ylelikle istedi\u011fimiz zaman pickle i\u00e7ini a\u00e7abiliriz. wb, rb gibi yaz\u0131labilir, okunabilir modlar\u0131 ekleriz.","ee5d6fd4":"Veri seti az oldu\u011fu i\u00e7in epochs ve loss de\u011ferleri \u00e7ok net bir grafik ortaya koymuyor ama genel olarak y\u00f6nelimi belli. Yani makas a\u00e7\u0131lm\u0131yor, birbirlerine paralel gidiyorlar.","9a115ad3":"Optimizer olarak rmsprop \u00e7ok k\u00f6t\u00fc sonu\u00e7 g\u00f6sterdi\u011fi i\u00e7in adam kulland\u0131m.","a92290c0":"E\u011fer a\u011f\u0131rl\u0131k reg\u00fclarizasyonu kaggle'da \u00e7al\u0131\u015fsayd\u0131, keras ve bile\u015fenlerini import ederek bir katman olu\u015fturup yukar\u0131daki yoruma al\u0131nm\u0131\u015f kodlar da \u00e7al\u0131\u015ft\u0131r\u0131lacakt\u0131.","9fb48138":" Resim dizimizin boyutu, 2400x3200 boyutlar\u0131nda ve RGB kanallar\u0131na sahip. ","3129f149":"Veri seti k\u00fc\u00e7\u00fck oldu\u011fu i\u00e7in kerneli her restart edi\u015fimde kodlamada ve parametrelerde bir de\u011fi\u015fiklik olmamas\u0131na ra\u011fmen  loss ve acc de\u011ferleri de\u011fi\u015febiliyor. Bu a\u015f\u0131r\u0131 bir farkl\u0131l\u0131k olmasa da dropout kulland\u0131\u011f\u0131mda ezberden biraz daha kurtulmu\u015f g\u00f6r\u00fcn\u00fcyor.","cfddc537":"Modelde nelerin kullan\u0131ld\u0131\u011f\u0131n\u0131 g\u00f6steren bir \u00f6zet.","c2381d01":"Ger\u00e7ekten \u00f6\u011frennebilmesi i\u00e7in resimleri kendi aralar\u0131nda kar\u0131\u015ft\u0131r\u0131yoruz.","f3ef90eb":"Olu\u015fturdu\u011fumuz resim dizisinin boyutlar\u0131n\u0131 matris formunda yazd\u0131r\u0131yoruz.","d30d95fe":"Plot grafi\u011fi \u00e7\u0131karmak i\u00e7in bunu history de\u011fi\u015fkeninde tan\u0131mlar\u0131z.","00ae1ee6":"Burada new_Array olu\u015fturarak yeni boyutlara d\u00f6n\u00fc\u015ft\u00fcr\u00fcp, resimleri atar\u0131z.","56dd08ec":"Burada ba\u015fka bir data augment modeli olan shift uyguluyoruz, sanki ayna varm\u0131s gibi yans\u0131mas\u0131n\u0131 tersten g\u00f6stermeye yar\u0131yor. \u00c7atal\u0131n ucu sa\u011fa bakarken, art\u0131k sola bak\u0131yor gibi d\u00fc\u015f\u00fcn\u00fclebilir.","73f452ff":"\u00c7ok k\u00fc\u00e7\u00fck bir veri setim oldu\u011fu i\u00e7in batch size ve epoch ufak tutarak bir e\u011fitim denemesi yapt\u0131m.","72ca8acd":"Ba\u015fta 2400 ve 3200 olan boyutlar\u0131 \u00f6nce 240 320 ye sonra 100 100 e d\u00fc\u015f\u00fcrsem de 30 30 boyutlar\u0131yla en ideal e\u011fitim oluyor.","66fcc10d":"Kendi \u00e7ekti\u011fimiz resim dosyalar\u0131n\u0131n dizin adresini ve kategorilerini belirleriz.","1c9a5e7e":"A\u011f\u0131rl\u0131k  Reg\u00fclarizsayonu: Keras\u2019ta  weight_regularization  parametresi  katmanlara  \u201ccost\u201d olarak eklenerek overfitting\u2019i d\u00fc\u015f\u00fcr\u00fcr.\n\nA\u011f\u0131rl\u0131k reg\u00fclarizasyonu jupyterde kendi desktop dizinimde sorun \u00e7\u0131karmamas\u0131na ra\u011fmen kaggle ortam\u0131na ta\u015f\u0131d\u0131\u011f\u0131mda image dataset'ten \u00f6t\u00fcr\u00fc oldu\u011funu tahmin etti\u011fim bir sorun \u00e7\u0131kard\u0131. Ben yine yorum olarak kodunu payla\u015ft\u0131m."}}