{"cell_type":{"2161dc7a":"code","c7683501":"code","1cf9bea2":"code","eab3525e":"code","257d9d0d":"code","62891a41":"code","d5ce7e89":"code","0d139caa":"code","41640af3":"code","3466629b":"code","82149068":"code","b8b6b9a2":"code","e95d089d":"code","4b9bfd81":"code","c0033ca3":"code","448c0b2d":"code","4bca8481":"code","38f30f7c":"code","04940d59":"code","a7353ced":"code","982b455e":"code","86b3419c":"code","8a52891f":"code","903b15bc":"code","f7fdb5e4":"code","5c5805e3":"code","0518fe74":"code","42c3904b":"code","4fa2f909":"code","0a5ecef2":"code","4d36ff31":"code","5bb898ee":"code","3e2cbb4b":"code","51fa5423":"code","c4389b0f":"code","549ad8ff":"code","6de4c4ea":"code","5c81fbc0":"code","6f9f2959":"markdown","a8a6661f":"markdown","3af1edc1":"markdown","f5f68953":"markdown","26d80dbf":"markdown","6886b2c0":"markdown","10804309":"markdown","8600e771":"markdown","a3196888":"markdown","75fe7e06":"markdown","f82c320a":"markdown","3a43cc46":"markdown","c9ad1e6c":"markdown","52c0d89c":"markdown","57a07f98":"markdown","ac51853a":"markdown","a2c912bd":"markdown","50dcd2fa":"markdown","d9893033":"markdown","dd1001dc":"markdown","0a45fd16":"markdown","81650096":"markdown","370edac3":"markdown","e274f475":"markdown","e9634001":"markdown","bf533dd1":"markdown","1633b4ee":"markdown","ef49d9fa":"markdown","1634416a":"markdown","1e0bf34a":"markdown","88765a79":"markdown","8034ef04":"markdown","effaf6b4":"markdown","3ab5a2ee":"markdown"},"source":{"2161dc7a":"# here we will import the libraries used for machine learning\nimport sys \nimport numpy as np # linear algebra\nfrom scipy.stats import randint\nimport pandas as pd # data processing, CSV file I\/O, data manipulation \nimport matplotlib.pyplot as plt # this is used for the plot the graph \nimport seaborn as sns # used for plot interactive graph. \n\nfrom sklearn.linear_model import LogisticRegression # to apply the Logistic regression\nfrom sklearn.model_selection import train_test_split # to split the data into two parts\nfrom sklearn.model_selection import KFold # use for cross validation\nfrom sklearn.model_selection import GridSearchCV# for tuning parameter\nfrom sklearn.preprocessing import StandardScaler # for normalization\nfrom sklearn.preprocessing import Imputer  # dealing with NaN\nfrom sklearn.pipeline import Pipeline # pipeline making\nfrom sklearn.ensemble import RandomForestClassifier # for random forest classifier\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.neighbors import KNeighborsClassifier #KNN\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import ExtraTreesClassifier\nfrom sklearn.feature_selection import SelectFromModel\nfrom sklearn.ensemble import RandomForestClassifier # for random forest classifier\nfrom sklearn.model_selection import RandomizedSearchCV\nfrom sklearn import metrics # for the check the error and accuracy of the model\nfrom sklearn import svm, datasets # for Support Vector Machine\nfrom sklearn.svm import SVC\n\n\n## for Deep-learing:\nimport keras\nfrom keras.layers import Dense\nfrom keras.models import Sequential\nfrom keras.utils import to_categorical\nfrom keras.optimizers import SGD \nfrom keras.callbacks import EarlyStopping\nfrom keras.wrappers.scikit_learn import KerasClassifier\nfrom keras.utils import np_utils\nfrom sklearn.preprocessing import LabelEncoder\n\nimport warnings\nwarnings.filterwarnings(\"ignore\", category=FutureWarning)","c7683501":"data0 = pd.read_csv(\"..\/input\/data.csv\")","1cf9bea2":"data0.head()","eab3525e":"data0.diagnosis.unique()","257d9d0d":"data0[data0 == '?'] = np.NaN\n# Drop missing values and print shape of new DataFrame\ndata0.info()","62891a41":"# drop columns:  \"Unnamed: 32\" and \"ID\"\n# To keep the same name of file, write: inplace=True\n# Separating target from features (predictor variables)\n\ny = data0.diagnosis     # target= M or B \n\nlist = ['Unnamed: 32','id','diagnosis']\nfeatures = data0.drop(list,axis = 1,inplace = False)\n\nlist = ['Unnamed: 32','id']\ndata0.drop(list, axis = 1, inplace = True)","d5ce7e89":"print(data0.isnull().sum())","0d139caa":"# The frequency of cancer stages\nB, M = data0['diagnosis'].value_counts()\nprint('Number of Malignant : ', M)\nprint('Number of Benign: ', B)\n\nplt.figure(figsize=(10,6))\nsns.set_context('notebook', font_scale=1.5)\nsns.countplot('diagnosis',data=data0, palette=\"Set1\")\nplt.annotate('Malignant = 212', xy=(-0.2, 250), xytext=(-0.2, 250), size=18, color='red')\nplt.annotate('Benign = 357', xy=(0.8, 250), xytext=(0.8, 250), size=18, color='w');","41640af3":"features.describe()","3466629b":"#data0.columns or\ndata0.keys()","82149068":"# Standardization of features\nstdX = (features - features.mean()) \/ (features.std())              \ndata_st = pd.concat([y,stdX.iloc[:,:]],axis=1)\ndata_st = pd.melt(data_st,id_vars=\"diagnosis\",\n                    var_name=\"features\",\n                    value_name='value')","b8b6b9a2":"plt.figure(figsize=(12,30))\nsns.set_context('notebook', font_scale=1.5)\nsns.boxplot(x=\"value\", y=\"features\", hue=\"diagnosis\", data=data_st, palette='Set1')\nplt.legend(loc='best');","e95d089d":"plt.figure(figsize=(12,30))\nsns.set_context('notebook', font_scale=1.5)\nsns.violinplot(x=\"value\", y=\"features\", hue=\"diagnosis\", data=data_st,split=True, \n               inner=\"quart\", palette='Set1')\nplt.legend(loc='best');","4b9bfd81":"corr = data0.corr() # .corr is used to find corelation\nf,ax = plt.subplots(figsize=(20, 20))\nsns.heatmap(corr, cbar = True,  square = True, annot = True, fmt= '.1f', \n            xticklabels= True, yticklabels= True\n            ,cmap=\"coolwarm\", linewidths=.5, ax=ax);","c0033ca3":"def pearson_r(x, y):\n    # Compute correlation matrix: corr_mat\n    corr_mat = np.corrcoef(x, y)\n\n    # Return entry [0,1]\n    return corr_mat[0,1]\n\n# Compute Pearson correlation coefficient for 'radius_mean', 'symmetry_mean'\nr1 = pearson_r(data0['radius_mean'], data0['perimeter_mean'])\nr2= pearson_r(data0['radius_mean'], data0['symmetry_mean'])\n\nname_c = []\nfor (i,j) in zip(range(1,31),range(1,31)):\n        r = pearson_r(data0.iloc[:,1], data0.iloc[:,j])\n        if abs(r) >= 0.80 and data0.columns[j]  not in name_c:\n                    name_c.append(data0.columns[j]) \nprint()\nprint('* Lenght of columns assuming r >=0.80:', len(name_c)) \nprint('name_c =',name_c)","448c0b2d":"name_c = []\nfor (i,j) in zip(range(1,31),range(1,31)):\n        r = pearson_r(data0.iloc[:,1], data0.iloc[:,j])\n        if abs(r) <= 0.40 and data0.columns[j]  not in name_c:\n                    name_c.append(data0.columns[j])\n                            \nprint('* Lenght of columns assuming r <=0.40:', len(name_c)) \nprint('name_c =',name_c) ","4bca8481":"sns.lmplot(x='radius_mean', y= 'symmetry_mean', data = data0, hue ='diagnosis', \n           palette='Set1')\nplt.title('Linear Regression: distinguishing between M and B', size=16)\n\n\nsns.lmplot(x='radius_mean', y= 'perimeter_mean', data = data0, hue ='diagnosis', \n           palette='Set1')\nplt.title('Linear Regression: Cannot distinguish between M and B', size=16);\n\nprint('Uncorrelated data are poentially more useful: discrimentory!')","38f30f7c":"plt.figure(figsize=(15,15))\nsns.set_context('notebook', font_scale=1.5)\nplt.subplot(2, 2, 1)\nsns.boxplot(y=\"radius_mean\", x=\"diagnosis\", data=data0, palette=\"Set1\") \nsns.swarmplot(x=\"diagnosis\", y=\"radius_mean\",data=data0, palette=\"Set3\", dodge=True)\nplt.subplot(2, 2, 2)  \nsns.boxplot(y=\"fractal_dimension_mean\", x=\"diagnosis\", data=data0, palette=\"Set1\")\nsns.swarmplot(x=\"diagnosis\", y=\"fractal_dimension_mean\",data=data0, palette=\"Set3\",\n              dodge=True)\nplt.subplots_adjust(wspace=0.4); ","04940d59":"# CDF function\ndef ecdf(data0):\n    n=len(data0)\n    x=np.sort(data0)\n    y=np.arange(1, n+1)\/n\n    return x, y \n\ndata2 = data0['radius_mean']\nMalignant = data2[data0['diagnosis']=='M']\nBenign = data2[data0['diagnosis']=='B']\n\nx1, y1 = ecdf(Malignant)\nx2, y2 = ecdf(Benign)\n\ndata3 = data0['fractal_dimension_mean']\nMalignant_f = data3[data0['diagnosis']=='M']\nBenign_f = data3[data0['diagnosis']=='B']\n\nx3, y3 = ecdf(Malignant_f)\nx4, y4 = ecdf(Benign_f)\n\nplt.figure(figsize=(15,15))\n#plt.close('all')\nplt.subplot(2, 2,  1)\nplt.subplots_adjust(wspace=0.4, hspace=2)\nplt.plot(x1, y1, marker='.',linestyle='none', color='red', label='M')\nplt.plot(x2, y2, marker='.',linestyle='none', color ='blue', label='B')\nplt.margins(0.02)\nplt.xlabel('radius_mean', size=20)\nplt.ylabel('ECDF', size=20)\nplt.title('Empirical Cumulative Distribution Function', size=20)\nplt.legend(prop={'size':20})\n#plt.show()\nplt.subplot(2, 2,  2)\nplt.subplots_adjust(wspace=0.4, hspace=2)\nplt.plot(x3, y3, marker='.',linestyle='none', color='red', label='M')\nplt.plot(x4, y4, marker='.',linestyle='none', color ='blue', label='B')\nplt.margins(0.02)\nplt.xlabel('fractal_dimension_mean', size=20)\nplt.ylabel('ECDF', size=20)\nplt.title('Empirical Cumulative Distribution Function', size=20)\nplt.legend(prop={'size':20});","a7353ced":"def permutation_sample(data1, data2):\n    \"\"\"Generate a permutation sample from two data sets.\"\"\"\n\n    # Concatenate the data sets: data\n    data = np.concatenate((data1, data2))\n\n    # Permute the concatenated array: permuted_data\n    permuted_data = np.random.permutation(data)\n\n    # Split the permuted array into two: perm_sample_1, perm_sample_2\n    perm_sample_1 = permuted_data[:len(data1)]\n    perm_sample_2 = permuted_data[len(data1):]\n\n    return perm_sample_1, perm_sample_2\n\n\n\ndef draw_perm_reps(data_1, data_2, func, size=1):\n    \"\"\"Generate multiple permutation replicates.\"\"\"\n\n    # Initialize array of replicates: perm_replicates\n    perm_replicates = np.empty(size)\n\n    for i in range(size):\n        # Generate permutation sample\n        perm_sample_1, perm_sample_2 = permutation_sample(data_1, data_2)\n\n        # Compute the test statistic\n        perm_replicates[i] = func(perm_sample_1, perm_sample_2)\n\n    return perm_replicates\n\n\n\ndef diff_of_means(data_1, data_2):\n    \"\"\"Difference in means of two arrays.\"\"\"\n\n    # The difference of means of data_1, data_2: diff\n    diff = np.mean(data_1)-np.mean(data_2)\n\n    return diff","982b455e":"diff_of_means(Malignant, Benign)","86b3419c":"# Computing difference of mean overall acore\nempirical_diff_means = diff_of_means(Malignant, Benign)\n\n# Drawing 10,000 permutation replicates: perm_replicates\nperm_replicates = draw_perm_reps(Malignant, Benign,diff_of_means, size=10000)\n\n# Computing p-value: p\np = np.sum(perm_replicates >= empirical_diff_means)\/ len(perm_replicates) \n\nprint('p-value =', p)","8a52891f":"# Let's map diagnosis column[object] to integer value:0, 1\n# later on below I show how to use LabelEncoder(): it is better way to categorize\ndata=data0.copy()\ndata['diagnosis']=data0['diagnosis'].map({'M':1,'B':0})","903b15bc":"# Split the data into train (0.7) and test (0.3)\n\n## all data without dropping those with correlations\nX = data.drop('diagnosis', axis=1)\ny = data['diagnosis']\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=21, \n                                                    stratify=y)\n\nprint(type(X))\nprint(type(y))","f7fdb5e4":"# Creating a k-NN classifier with 3 neighbors: knn\nknn = KNeighborsClassifier(n_neighbors=3)\n\n# Fit the classifier to the training data\nknn.fit(X_train, y_train)\n\n# Print the accuracy\nprint('Accuracy KNN(1): ', knn.score(X_test, y_test))","5c5805e3":"neighbors = np.arange(1, 22)\ntrain_accuracy = np.empty(len(neighbors))\ntest_accuracy = np.empty(len(neighbors))\n\n# Loop over different values of k\nfor i, k in enumerate(neighbors):\n    # Setup a k-NN Classifier with k neighbors: knn\n    knn = KNeighborsClassifier(n_neighbors=k)\n    knn.fit(X_train, y_train)\n    #Compute accuracy on the training and testing sets\n    train_accuracy[i] = knn.score(X_train, y_train)\n    test_accuracy[i] = knn.score(X_test, y_test)\n\nplt.figure(figsize=(12,7))\nsns.set_context('notebook', font_scale=1.5)\nplt.title('Learning curves for k-NN: Varying Number of Neighbors', size=20)\nplt.plot(neighbors, test_accuracy, marker ='o', label = 'Testing Accuracy')\nplt.plot(neighbors, train_accuracy, marker ='o', label = 'Training Accuracy')\nplt.legend(prop={'size':15})\nplt.xlabel('Number of Neighbors (k)', size=15)\nplt.ylabel('Accuracy', size=15)\nplt.annotate('Over-fitting', xy=(0.5, 0.94), xytext=(0.3, 0.935), size=15, color='red')\nplt.annotate('Under-fitting', xy=(0.5, 0.94), xytext=(18, 0.93), size=15, color='red')\nplt.xticks(np.arange(min(neighbors), max(neighbors)+1, 1.0));","0518fe74":"## data are distributed in a wide range (below), need to be normalizded.\nplt.figure(figsize=(15,3))\nax= data.drop('diagnosis', axis=1).boxplot(data.columns.name, rot=90)\nplt.xticks( size=20)\nax.set_ylim([0,50]);","42c3904b":"steps = [('scaler', StandardScaler()), \n         ('knn', KNeighborsClassifier())]\n\npipeline = Pipeline(steps)\nparameters = {'knn__n_neighbors' : np.arange(1, 50)}\n\n\nk_nn = GridSearchCV(pipeline, param_grid=parameters)\nk_nn.fit(X_train, y_train)\ny_pred = k_nn.predict(X_test)\n\nprint(k_nn.best_params_)\nprint()\nprint(classification_report(y_test, y_pred))\nprint(\"Best score is: {}\".format(k_nn.best_score_))\n\nConfMatrix = confusion_matrix(y_test,k_nn.predict(X_test))\nsns.heatmap(ConfMatrix,annot=True, cmap=\"Blues\", fmt=\"d\", \n            xticklabels = ['B', 'M'], yticklabels = ['B', 'M'])\nplt.ylabel('True label')\nplt.xlabel('Predicted label')\nplt.title(\"Confusion Matrix\");","4fa2f909":"cv_knn = cross_val_score(k_nn, X, y, cv=5, scoring='accuracy')\nprint('Average 5-Fold CV Score: ', cv_knn.mean(), ', Standard deviation: ', cv_knn.std())","0a5ecef2":"# To Setup the pipeline\nsteps = [('scaler', StandardScaler()),\n         ('SVM', SVC())]\n\npipeline = Pipeline(steps)\n\n# Specify the hyperparameter space: C is regularization strength while gamma controls the kernel coefficient. \nparameters = {'SVM__C':[1, 10, 100],\n              'SVM__gamma':[0.1, 0.01]}\n\n# Create train & test sets\n\n# Instantiate the GridSearchCV object: cv\ncv =GridSearchCV(pipeline,parameters, cv=3)\n\n# Fit to the training set\ncv.fit(X_train, y_train)\n\n# Predict the labels of the test set: y_pred\ny_pred = cv.predict(X_test)\n\n# Compute and print metrics\nprint(\"Tuned Model Parameters: {}\".format(cv.best_params_))\nprint(\"Accuracy: {}\".format(cv.score(X_test, y_test)))\nprint(classification_report(y_test, y_pred))\nprint(\"Best score is: {}\".format(cv.best_score_))\n\nConfMatrix = confusion_matrix(y_test,cv.predict(X_test))\nsns.heatmap(ConfMatrix,annot=True, cmap=\"Blues\", fmt=\"d\", \n            xticklabels = ['B', 'M'], yticklabels = ['B', 'M'])\nplt.ylabel('True label')\nplt.xlabel('Predicted label')\nplt.title(\"Confusion Matrix\");","4d36ff31":"X = data.drop('diagnosis', axis=1).values[:,:2]\ny = data['diagnosis'].values\n\nh = .02  # step size in the mesh\n\n# we create an instance of SVM and fit out data. We do not scale our\n# data since we want to plot the support vectors\nC = 1.0  # SVM regularization parameter\nsvc = svm.SVC(kernel='linear', C=C).fit(X, y)\nrbf_svc = svm.SVC(kernel='rbf', gamma=0.7, C=C).fit(X, y)\npoly_svc = svm.SVC(kernel='poly', degree=3, C=C).fit(X, y)\nlin_svc = svm.LinearSVC(C=C).fit(X, y)\n\n# create a mesh to plot in\nx_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\ny_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\nxx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n                     np.arange(y_min, y_max, h))\n\n# title for the plots\ntitles = ['SVC with linear kernel',\n          'LinearSVC (linear kernel)',\n          'SVC with RBF kernel',\n          'SVC with polynomial (degree 3) kernel']\nplt.figure(figsize=(10,12))\nfor j, clf in enumerate((svc, lin_svc, rbf_svc, poly_svc)):\n# Plot the decision boundary by assigning a color to each point in the mesh \n    plt.subplot(2, 2, j + 1)\n    plt.subplots_adjust(wspace=0.4, hspace=0.4)\n\n    Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])\n\n# Put the result into a color plot\n    Z = Z.reshape(xx.shape)\n    plt.contourf(xx, yy, Z, cmap=plt.cm.get_cmap('RdBu_r'), alpha=0.6)\n\n# Ploting  the training points\n    plt.scatter(X[:, 0], X[:, 1], c=y, cmap=plt.cm.get_cmap('RdBu_r'))\n    plt.xlabel('radius_mean',size=20)\n    plt.ylabel('texture_mean',size=20)\n    plt.xlim(xx.min(), xx.max())\n    plt.ylim(yy.min(), yy.max())\n    plt.xticks(())\n    plt.yticks(())\n    plt.title(titles[j],size=20);","5bb898ee":"# Setup the hyperparameter grid, (not scaled data)\nparam_grid = {'C': np.logspace(-5, 8, 15)}\n\n# Instantiate a logistic regression classifier: logreg\nlogreg = LogisticRegression()\n\n# Instantiate the GridSearchCV object: logreg_cv\nlogreg_cv = GridSearchCV(logreg,param_grid , cv=5)\n\n# Fit it to the data\nlogreg_cv.fit(X_train, y_train)\n\n# Print the tuned parameters and score\nprint(\"Tuned Logistic Regression Parameters: {}\".format(logreg_cv.best_params_)) \nprint()\nprint(classification_report(y_test, y_pred))\nprint(\"Best score is {}\".format(logreg_cv.best_score_))\n\nConfMatrix = confusion_matrix(y_test,logreg_cv.predict(X_test))\nsns.heatmap(ConfMatrix,annot=True, cmap=\"Blues\", fmt=\"d\", \n            xticklabels = ['B', 'M'], yticklabels = ['B', 'M'])\nplt.ylabel('True label')\nplt.xlabel('Predicted label')\nplt.title(\"Confusion Matrix\");","3e2cbb4b":"# Setup the parameters and distributions to sample from: param_dist\nparam_dist = {\"max_depth\": [3, None],\n              \"min_samples_leaf\": randint(1, 9),\n              \"criterion\": [\"gini\", \"entropy\"]}\n\n# Instantiate a Decision Tree classifier: tree\n#tree = DecisionTreeClassifier() # ExtraTrees is better here. \ntree= ExtraTreesClassifier()\n\n# Instantiate the RandomizedSearchCV object: tree_cv\ntree_cv = RandomizedSearchCV(tree, param_dist, cv=5)\n\n# Fit it to the data\ntree_cv.fit(X_train, y_train)\ny_pred = tree_cv.predict(X_test)\n\n# Print the tuned parameters and score\nprint(\"Tuned Extra Tree Parameters: {}\".format(tree_cv.best_params_))\nprint()\nprint(classification_report(y_test, y_pred))\nprint(\"Best score is {}\".format(tree_cv.best_score_))\n# metrics.accuracy_score(y_pred,y_test) # the same as above\n\nConfMatrix = confusion_matrix(y_test,tree_cv.predict(X_test))\nsns.heatmap(ConfMatrix,annot=True, cmap=\"Blues\", fmt=\"d\", \n            xticklabels = ['B', 'M'], yticklabels = ['B', 'M'])\nplt.ylabel('True label')\nplt.xlabel('Predicted label')\nplt.title(\"Confusion Matrix\");","51fa5423":"Ran = RandomForestClassifier(n_estimators=50)\nRan.fit(X_train, y_train)\ny_pred = Ran.predict(X_test)\nprint('Accuracy:', metrics.accuracy_score(y_pred,y_test))\n\n## 5-fold cross-validation \ncv_scores =cross_val_score(Ran, X, y, cv=5)\n\n# Print the 5-fold cross-validation scores\nprint()\nprint(classification_report(y_test, y_pred))\nprint()\nprint(\"Average 5-Fold CV Score: {}\".format(np.mean(cv_scores)), \n      \", Standard deviation: {}\".format(np.std(cv_scores)))\n\nConfMatrix = confusion_matrix(y_test,Ran.predict(X_test))\nsns.heatmap(ConfMatrix,annot=True, cmap=\"Blues\", fmt=\"d\", \n            xticklabels = ['B', 'M'], yticklabels = ['B', 'M'])\nplt.ylabel('True label')\nplt.xlabel('Predicted label')\nplt.title(\"Confusion Matrix\");","c4389b0f":"tree_2= ExtraTreesClassifier()\ntree_2.fit(X_train, y_train)\nprint('Extra-Tree score:',tree_2.score(X_test, y_test))\nprint('Shape of original data:', X_train.shape)\nprint()\ntree_2.feature_importances_\nmodel_reduced = SelectFromModel(tree_2, prefit=True)\nX_reduced = model_reduced.transform(X_train)\nprint('Shape of data with most important features:', X_reduced.shape)\nprint()\nprint(classification_report(y_test, y_pred))\nprint()\n\nConfMatrix = confusion_matrix(y_test,tree_2.predict(X_test))\nsns.heatmap(ConfMatrix,annot=True, cmap=\"Blues\", fmt=\"d\", \n            xticklabels = ['B', 'M'], yticklabels = ['B', 'M'])\nplt.ylabel('True label')\nplt.xlabel('Predicted label')\nplt.title(\"Confusion Matrix\");","549ad8ff":"cv_tree2 = cross_val_score(logreg_cv, X, y, cv=5, scoring='accuracy')\nprint(\"Average 5-Fold CV Score: {}\".format(np.mean(cv_tree)), \n      \"Standard deviation: {}\".format(np.std(cv_tree)));","6de4c4ea":"### The data is not normalized. \n\n## method 1 \npredictors= data.drop('diagnosis', axis=1).values  # .values to conver it to array\ntarget = to_categorical(data.diagnosis.values)\nn_cols = predictors.shape[1]\n\n#np.random.seed(1337) # for reproducibility\nseed = 1337\nnp.random.seed(seed)\n\nmodel = Sequential()\n\n# Add layers and nodes\nmodel.add(Dense(50, activation='relu', input_shape = (n_cols,)))\nmodel.add(Dense(50, activation='relu'))\nmodel.add(Dense(2, activation='softmax'))\n# Compile the model\nmodel.compile(optimizer = 'adam', loss = 'categorical_crossentropy', metrics=['accuracy']) \n    \n\n## Fit with 0.3 splitting with early_stopping_monitor with 30 epochs\nearly_stopping_monitor =EarlyStopping(patience=2) \n# Fit the model\n\nhistory=model.fit(predictors, target, validation_split=0.3, epochs=100, batch_size=5,\n                  callbacks = [early_stopping_monitor])\n\n# 1 epoch = one forward pass and one backward pass of all the training examples\n# batch size = number of samples that going to be propagated through the network.\n# The higher the batch size, the more memory space. ","5c81fbc0":"#RandomForest\nimpor_Forest=Ran.feature_importances_\nindices_1 = np.argsort(impor_Forest)[::-1]\n\n#ExtraTree\nimpor_Extra_tree=tree_2.feature_importances_\nindices_2= np.argsort(impor_Extra_tree)[::-1]\n\nfeatimp_1 = pd.Series(impor_Forest, index=data.columns[1:]).sort_values(ascending=False)\nfeatimp_2 = pd.Series(impor_Extra_tree, index=data.columns[1:]).sort_values(ascending=False)\n\nTable_impor= pd.DataFrame({'ExtraTree': featimp_2,'Random-Forest': featimp_1})\nTable_impor=Table_impor.sort_values('ExtraTree', ascending=False)\nprint(Table_impor)\nprint()\nprint('The six most important features:')\nprint(featimp_1[0:6])\n\nsns.set_context('notebook', font_scale=1.5)\nTable_impor.plot(kind='barh', figsize=(12,10))\nplt.title('Feature importance', size=20);","6f9f2959":"<a id='eda'><\/a>\n## Exploratory Data Analysis (EDA)","a8a6661f":"<a id='imp'><\/a>\n## Importing packages and loading data","3af1edc1":"Finding un-correlated variables via Pearson correlation coefficient between two arrays","f5f68953":"<a id='corr'><\/a>\n### Correlation\n\nLooking at correlations matrix, defined via Pearson function.","26d80dbf":"<a id='knn'><\/a>\n### k-nearest neighbors (K-NN) \n\nA k-Nearest Neighbors (k-NN) classifier with 3 neighbors (k) was created, and fitted to the training data.","6886b2c0":"#### K-NN Learning Pipeline (including scaling)\nA second k-NN model was created using a standardized data.","10804309":"The bloxblot and swarm plots below show that malignant and benign tumors have almost the same measures of fractal_dimension_mean, while radius_mean provides more information for classification.","8600e771":"From this plot it is possible to identify that some features are very similar, as for example, perimeter_mean and area_mean, and perimeter_se and area_se. Furthermore, we can also observe that some features have different measures when the tumor is malignant or benign. Examples of this are area_mean, radius_mean, and concavity_mean. On the other hand, in features such as fractal_dimension_mean and texture_se, the distribution of malignant and benign tumoers seems to be similar.","a3196888":"<a id='fs'><\/a>\n## Feature Selection\nLet's check the relative importance of features.","75fe7e06":"#### Normalization issue\n\nFrom the plot below, and as it was shown in the EDA, it is possible to see that features are in different scales.","f82c320a":"<a id='sp'><\/a>\n### Spliting the data into train and test sets\n\nBefore modelling, the original data was split into train (70%) and test (30%).","3a43cc46":"> <a id='ml'><\/a>\n## Machine Learning: Classification models","c9ad1e6c":"<a id='lr'><\/a>\n### Logistic Regression ","52c0d89c":"<a id='stat'><\/a>\n## Statistical inference\n\nTo further investigate the properties of features, we constructed the empirical cumulative distribution of features (ECDF), fractal_dimension_mean and radius_mean. ","57a07f98":"<a id='sv'><\/a>\n### Support Vector Classification","ac51853a":"The target variable (response), as shown in the bar chart above, has unbalanced data. In other words, classes are not represented equally. One way to deal with this issue is resampling. However, this tutorial will not address that problem.","a2c912bd":"# Table of Content\n\n* [Objectives](#obj)\n* [Importing packages and loading data](#imp)\n* [Data cleaning and data wrangling](#clean)\n* [Exploratory Data Analysis (EDA)](#eda)\n    * [Boxplot of features by diagnosis](#p1)\n    * [Violin plot of features by diagnosis](#p2)\n    * [Correlation](#corr)\n* [Statistical inference](#stat)\n    * [Hypothesis testing](#t)\n* [Machine Learning: Classification models](#ml)\n    * [Spliting the data: train and test](#sp)\n    * [k-nearest neighbors (k-NN)](#knn)\n    * [Support Vector Classification](#sv)\n    * [Logistic Regression](#log)\n    * [ExtraTree-decision](#tree)\n    * [Random-Forest Classifier](#rf)\n    * [Nueral-Networks: KERAS-Tensorflow](#nn)\n    * [Keras with normalization](kn)\n* [Feature Selection](#fs)\n* [Summary of models performance](#sum)","50dcd2fa":"#### Mapping the target: categorizing ","d9893033":"### Extra Tree Classifier (reduced features)","dd1001dc":"<a id='p2'><\/a>\n### Violin plot of features by diagnosis","0a45fd16":"# Breast Cancer Diagnostic - Classification\n\n### Tutorial for beginners in Supervised Machine Learning and classification data analysis.","81650096":"**What do correlations mean?**\n\nLets separately fit correlated and uncorrelated data via linear regression: ","370edac3":"<a id='t'><\/a>\n### Hypothesis testing\n\nSome necessary functions were created in order to do hypothesis analysis.","e274f475":"<a id='p1'><\/a>\n### Boxplot of features by diagnosis","e9634001":"<a id='rf'><\/a>\n### Random Forest Classifier","bf533dd1":"#### Learning curves: over\/underfitting\n\nThe learning curves for k-NN model were constructed, varying the number of neighbors. The results can be seen bellow.","1633b4ee":"<a id='clean'><\/a>\n## Data cleaning and data wrangling","ef49d9fa":"<a id='obj'><\/a>\n## Objectives:<br>\n**1)** Determine which features of data (measurements) are most important for diagnosing breast cancer.<br><br>\n**2)** Test performance of different classification models: \n* k-nearest neighbors (k-NN)\n* Suport Vector Classifier\n* Logistic Regression\n* ExtraTree-decision\n* Random-Forest\n* Keras (Deep-Learning)","1634416a":"<a id='tree'><\/a>\n### Extra Tree Classifier","1e0bf34a":"To visualize which SVC kernel is better: ","88765a79":"Ten features (predictor variables) were computed for each cell nucleus: radius, texture, perimeter, area, smoothness, compactness, concavity, concave points, symmetry, and fractal dimension. In addition, mean, standard error (se), and largest (\u201cworst\u201d) mean of the were also registered for each feature.\n\nData are distributed in a wide range, therefore, features were standardized before their visualization, so they have a mean of \u20180\u2019 and a standard deviation of \u20181\u2019.","8034ef04":"We failed to reject the null hypothesis. The p-value tells us that there is 0.0% chance that we would get the difference of means observed, if Malignant and Benign radius_mean were exactly the same.","effaf6b4":"<a id='nn'><\/a>\n### Nueral-Networks: KERAS-Tensorflow ","3ab5a2ee":"* <font color='blue'>It is seen from above over-fitting (low k) and under-fitting (high k).  <\/font>\n\n* We can find the optimized values of k via GridSearchCV in scikit-learn:"}}