{"cell_type":{"509421b3":"code","955f51a7":"code","5bdde767":"code","2b84f43e":"code","9315510c":"code","b6f8e22f":"code","10193fc1":"code","1a9fc5b0":"markdown"},"source":{"509421b3":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport math\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import minmax_scale, normalize\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import f1_score\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","955f51a7":"# Loading our dataset\ndf = pd.read_csv('\/kaggle\/input\/pima-indians-diabetes-database\/diabetes.csv')\n\ndisplay(df)\nprint(df.info())","5bdde767":"# Our target\/dependent variable is 'Outcome' here, and all the other variables are independant variables\n# We'll clean some of our independent variables, replacing the 0's in the features with their mean.\ncols_with_zero = ['Glucose','BloodPressure','SkinThickness','Insulin','BMI']\nfor col in cols_with_zero:\n    df[col] = df[col].replace(0, np.NaN)\n    mean = int(df[col].mean(skipna= True))\n    df[col] = df[col].replace(np.NaN, mean)\n\ndf","2b84f43e":"# Let's split our data into two sets, one with only dependent variable and the other with the independent variables\n\nx = df.iloc[:,0:8]\ny = df.iloc[:,8]\n\n# splitting our data into Training & Testing data sets in the ratio of 8:2 for both dependent and independent variables\nx_train,x_test, y_train,y_test = train_test_split(x,y, test_size= 0.2, random_state= 123)","9315510c":"# Let's standardize our data to transform the data into equal scale range\nsc = StandardScaler()\nx_train = sc.fit_transform(x_train)\nx_test = sc.fit_transform(x_test)","b6f8e22f":"# Identifying the value of the k-NearestNeighbors we need.\nmath.sqrt(len(y_test))","10193fc1":"# Let's train our model on the Training datasets using k-NearestNeighbors (k-NN) algorithm\nclassifier = KNeighborsClassifier(n_neighbors= 11, p= 2, metric= 'euclidean')\nclassifier.fit(x_train,y_train)\n\n# Using our trained model to predict the values for our Testing dataset\ny_pred = classifier.predict(x_test)\n\n# Calculating the metrics for our classification model and the accuracy for the predicted values\ncm1= confusion_matrix(y_test, y_pred)\nprint(cm1)\nprint(f1_score(y_test,y_pred))\nprint(accuracy_score(y_test,y_pred))","1a9fc5b0":"#### We have got a f1_score of 73%, and an accuracy of 81%, implementing that it's not a very good model but a sustainable one which is workable."}}