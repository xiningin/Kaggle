{"cell_type":{"dac391f1":"code","464285be":"code","3477be3c":"code","9f041f49":"code","2df54912":"code","af15d293":"code","21653d1f":"code","f10ff4e7":"code","c4b9eadc":"code","07409c05":"code","d3e06d68":"code","6a812286":"code","c52d1363":"code","9d32ef8c":"code","98bc99ef":"code","8956c401":"code","87dba365":"code","3dbb32b5":"code","a08feaa5":"code","85c26490":"code","f9186b9a":"code","2c77422b":"code","8a8f292d":"code","bb2aed4d":"code","b6973816":"code","47e4bf40":"code","e3f25705":"code","306d2539":"code","bffea270":"code","871e930a":"code","3a4cebb5":"code","79f15169":"code","317f97cf":"code","7de954ca":"code","ca858b55":"code","f29c4642":"markdown","ff9d88f4":"markdown","ad3267a3":"markdown","eb5df6ce":"markdown","6976fde5":"markdown","831f4c8c":"markdown","e32b7e5e":"markdown","024a76bf":"markdown","affb3734":"markdown","ea82d277":"markdown","e51fb33f":"markdown","28b4c2a9":"markdown","6a62b243":"markdown","b50b15fa":"markdown","068cec94":"markdown","d45831f5":"markdown"},"source":{"dac391f1":"import os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","464285be":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import LabelEncoder, OneHotEncoder\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.feature_extraction import FeatureHasher\nfrom sklearn.model_selection import train_test_split\npd.set_option('max_columns', None)\nimport catboost\nfrom catboost import CatBoostClassifier\nfrom catboost import Pool","3477be3c":"df_train = pd.read_csv('..\/input\/cat-in-the-dat\/train.csv')\ndf_test = pd.read_csv('..\/input\/cat-in-the-dat\/test.csv')","9f041f49":"df_train.head()","2df54912":"X = df_train.drop(columns=['id', 'target'])\ny = df_train['target']\ntest = df_test.drop(columns=['id'])\nlabels = X.columns\nIDs = df_test['id']","af15d293":"print(\"Training set shape: {} \\nTest set shape: {}\".format(X.shape, test.shape))","21653d1f":"bin_cols = [col for col in X.columns.values if col.startswith('bin')]\nnum_cols = [col for col in X.columns.values if col.startswith('nom')]\nord_cols = [col for col in X.columns.values if col.startswith('ord')]\ntim_cols = [col for col in X.columns.values if col.startswith('day') or col.startswith('month')]","f10ff4e7":"bin_cols","c4b9eadc":"X.nunique()","07409c05":"# Count of the dtypes for each column in our training set\nX.dtypes.value_counts()","d3e06d68":"X.dtypes","6a812286":"# Finding and plotting the count of the target variable\ncounts = y.value_counts()\nplt.bar(counts.index, counts)\nplt.gca().set_xticks([0,1])\nplt.title('Distribution of Target Variable')\nplt.show()\ncounts\n# The dataset is imbalanced","c52d1363":"def logistic(X, y):\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n    lr = LogisticRegression(solver='liblinear')\n    lr.fit(X_train, y_train)\n    pred = lr.predict(X_test)\n    print('Accuracy : ' , accuracy_score(y_test, pred))","9d32ef8c":"# Using label encoding to convert categorical variables in the training set to numerical variables\nencoder = LabelEncoder()\ntrain = pd.DataFrame()\nfor col in X.columns:\n    if (X[col].dtype == \"object\"):\n        train[col] = encoder.fit_transform(X[col])\n    else:\n        train[col] = X[col]\ntrain.head()","98bc99ef":"# All dtypes are now int64\ntrain.dtypes.value_counts()","8956c401":"logistic(train, y)","87dba365":"# Using one hot encoding to convert categorical variables in the training set to numerical variables\none = OneHotEncoder(handle_unknown=\"ignore\")\none.fit(X)\ndf_train = one.transform(X)\ndf_test = one.transform(test)","3dbb32b5":"# df_train is a sparse matrix, which is the default type returned with OneHotEncoding\ntype(df_train)","a08feaa5":"# Using one hot encoding added a lot of features to our training set, as would be expected\nprint('train data set has {} rows and {} columns'.format(df_train.shape[0],df_train.shape[1]))","85c26490":"logistic(df_train, y)\n# Accuracy of logistic regression improves with one hot encoding compared to label encoding","f9186b9a":"X_train_hash = X.copy()\nfor c in X.columns:\n    X_train_hash[c]=X[c].astype('str')","2c77422b":"hashing=FeatureHasher(input_type='string')\ntrain = hashing.transform(X_train_hash.to_numpy())","8a8f292d":"print(\"Train shape: {}\".format(train.shape))","bb2aed4d":"logistic(train, y)\n# Accuracy is better than with label encoding, but not quite as good as one hot encoding","b6973816":"cat_features = list([])\n#cat_features.append(X.columns.get_loc(c)) for c in labels if c.dtypes == object\nfor column in labels:\n    if X[column].dtype == 'object':\n        cat_features.append(labels.get_loc(column))","47e4bf40":"cat_features","e3f25705":"def cross_entropy(known, predicted):\n    ce_array = np.average(-known *np.log(predicted) - (1-known) * np.log(1-predicted))\n    return np.average(ce_array)","306d2539":"X_train, X_validation, y_train, y_validation = train_test_split(X, y, test_size=0.2, random_state=1)\ntrain_pool = Pool(\n    data = X_train,\n    label = y_train,\n    cat_features = cat_features\n    )\n\nvalidation_pool = Pool(\n    data = X_validation,\n    label = y_validation,\n    cat_features = cat_features\n    )\n\nall_pool = Pool(\n    data = X,\n    label = y,\n    cat_features = cat_features\n    )\n\ntest_pool = Pool(\n    data = test,\n    cat_features = cat_features)","bffea270":"cat_model = CatBoostClassifier(iterations=500, learning_rate = .01, l2_leaf_reg=1, loss_function='CrossEntropy')\ncat_model.fit(train_pool, eval_set = validation_pool, verbose=100)","871e930a":"pred = cat_model.predict(test_pool)","3a4cebb5":"submission = pd.DataFrame(IDs, columns = ['id'])\nsubmission['target'] = pred\nsubmission","79f15169":"submission.to_csv('submission.csv', index = False)","317f97cf":"reg = LogisticRegression(solver = 'liblinear')\nreg.fit(df_train, y)\n# Fitting the final classifier using the one hot encoding dataframe\npred = reg.predict(df_test)","7de954ca":"submission = pd.DataFrame(IDs, columns=['id'])\nsubmission['target'] = pred\nsubmission","ca858b55":"submission.to_csv('submission.csv', index=False)","f29c4642":"## From here we can see that we have 5 binary columns, 10 nominal columns, 6 ordinal columns, and 2 potentially time series columns","ff9d88f4":"## Using a for loop to create a list with the index of each column with categorical data","ad3267a3":"## Method 2: One Hot Encoding","eb5df6ce":"## Method 3: Feature hashing ","6976fde5":"## Plot a chart of the distribution of the Target 'y' variable. From this, we can see that the dataset is imbalanced. There is more than twice as many 0's as 1's","831f4c8c":"# Split up our columns between nominal, ordinal, binary, and time series","e32b7e5e":"# Import Libraries","024a76bf":"## Predictions","affb3734":"# Get train and test dataset","ea82d277":"## Creating a Logistic Regression algorithm with cross validation which we will use to test the effectiveness of different types of feature encoding","e51fb33f":"## From this we can see that 17 of the columns have an 'object' data type and 6 columns have an 'int64' data type","28b4c2a9":"## Method 1: Label Encoding","6a62b243":"# Set up our X and y for our training set","b50b15fa":"## Checking to see how many unique values are in each column. From this we can see that most of the columns have a smaller number of different categories, but some have a large number. For example, nom_9 has 11,981 different categories","068cec94":"## Creating a cross entropy loss function","d45831f5":"# From here we can see that we have 300,000 rows of training data and 200,00 rows of test data"}}