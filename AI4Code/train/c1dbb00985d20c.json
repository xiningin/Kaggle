{"cell_type":{"ace83a4a":"code","fb688cf8":"code","cdf4cbcb":"code","f8af8ca6":"code","098b51a4":"code","2d3ef33e":"code","4af48bfb":"code","e3aa7fe2":"code","e5357085":"code","b7060d22":"code","30bdfa6d":"code","2fb481b3":"code","8f2c7d5e":"code","632ae395":"code","c7cfb52c":"code","7bb6d6d9":"code","215b75cb":"code","f0ef18aa":"code","bc8d29c5":"code","5555bb61":"code","4043f86b":"code","79f8ba84":"code","84884d76":"code","181b910d":"code","7b8757d9":"code","4e2111bc":"code","6b88a15d":"code","6dcab23a":"code","e349d007":"code","594bfe51":"code","578f2f93":"code","1e1bacde":"code","0267eff4":"code","27d5d14b":"code","ae303825":"code","038e759e":"code","d838cd5c":"code","647646dd":"code","54a87724":"code","04b5a6bf":"code","95656f71":"code","efcddf7e":"code","209139d7":"code","533239dc":"code","41f5a78a":"code","fece8055":"code","2bd371bf":"code","5eb7e2d5":"code","214dd01a":"code","33e3e292":"code","b023c597":"code","d1c6a015":"code","55bfeb3a":"code","50d148fb":"code","7bc48866":"code","e9a8fed0":"markdown","59fe0f6e":"markdown","4db8abf9":"markdown","c86243f2":"markdown","3e3dd44b":"markdown","d0e09616":"markdown","88d78707":"markdown","8b72ef4b":"markdown","2ac555d2":"markdown"},"source":{"ace83a4a":"import pandas as pd\nimport numpy as np\nimport datetime\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom IPython.display import Image\nimport math\nfrom scipy.stats import kendalltau\n\nimport timeit\n\nimport warnings\nwarnings.filterwarnings('ignore')","fb688cf8":"#kills = pd.read_csv('C:\\\\Users\\\\Phil\\\\Documents\\\\LoL Model\\\\kills.csv')\n#matchinfo = pd.read_csv('C:\\\\Users\\\\Phil\\\\Documents\\\\LoL Model\\\\matchinfo.csv')\n#monsters = pd.read_csv('C:\\\\Users\\\\Phil\\\\Documents\\\\LoL Model\\\\monsters.csv')\n#structures = pd.read_csv('C:\\\\Users\\\\Phil\\\\Documents\\\\LoL Model\\\\structures.csv')\n\nkills = pd.read_csv('..\/input\/kills.csv')\nmatchinfo = pd.read_csv('..\/input\/matchinfo.csv')\nmonsters = pd.read_csv('..\/input\/monsters.csv')\nstructures = pd.read_csv('..\/input\/structures.csv')\n","cdf4cbcb":"matchinfo.head()","f8af8ca6":"# Add ID column based on last 16 digits in match address for simpler matching\n\nmatchinfo['id'] = matchinfo['Address'].astype(str).str[-16:]\nkills['id'] = kills['Address'].astype(str).str[-16:]\nmonsters['id'] = monsters['Address'].astype(str).str[-16:]\nstructures['id'] = structures['Address'].astype(str).str[-16:]\nmatchinfo.head()","098b51a4":"# Dragon became multiple types in patch v6.9 (http:\/\/leagueoflegends.wikia.com\/wiki\/V6.9) \n# so we remove and games before this change occured and only use games with the new dragon system\nmonsters['Type'].unique()","2d3ef33e":"old_dragon_id = monsters[ monsters['Type']==\"DRAGON\"]['id'].unique()\nold_dragon_id","4af48bfb":"monsters = monsters[ ~monsters['id'].isin(old_dragon_id)]\nmonsters[monsters['Type']==\"DRAGON\"]","e3aa7fe2":"# Again remove old games, we have some missing values (probably for other events) so remove this\n# Create a column for the minute in which the kill took place\n# Reassign the team column to a simpler Red\/Blue accordingly for matching with other tables\n\nkills = kills[ ~kills['id'].isin(old_dragon_id)]\nkills = kills[ kills['Time']>0]\n\nkills['Minute'] = kills['Time'].astype(int)\n\nkills['Team'] = np.where( kills['Team']==\"rKills\",\"Red\",\"Blue\")\nkills.head()","e5357085":"# For the Kills table, we need decided to group by the minute in which the kills took place and averaged \n# the time of the kills which we use later for the order of events\n\nf = {'Time':['mean','count']}\n\nkillsGrouped = kills.groupby( ['id','Team','Minute'] ).agg(f).reset_index()\nkillsGrouped.columns = ['id','Team','Minute','Time Avg','Count']\nkillsGrouped = killsGrouped.sort_values(by=['id','Minute'])\nkillsGrouped.head(13)","b7060d22":"# Repeat similar steps for the structures table\n\nstructures = structures[ ~structures['id'].isin(old_dragon_id)]\nstructures = structures[ structures['Time']>0]\n\nstructures['Minute'] = structures['Time'].astype(int)\nstructures['Team'] = np.where(structures['Team']==\"bTowers\",\"Blue\",\n                        np.where(structures['Team']==\"binhibs\",\"Blue\",\"Red\"))\nstructures2 = structures.sort_values(by=['id','Minute'])\nstructures2.head(13)","30bdfa6d":"# Merge the two together\nkills_structures = killsGrouped.merge(structures2[['id','Minute','Team','Time','Lane','Type']],\n                                      on=['id','Minute','Team'],how='outer')\nkills_structures.head(20)","2fb481b3":"# Again repeat same steps, we also map the types of dragon to a simpler 'Dragon' label\n\nmonsters = monsters[ ~monsters['id'].isin(old_dragon_id)]\nmonsters['Type2'] = np.where( monsters['Type']==\"FIRE_DRAGON\", \"DRAGON\",\n                    np.where( monsters['Type']==\"EARTH_DRAGON\",\"DRAGON\",\n                    np.where( monsters['Type']==\"WATER_DRAGON\",\"DRAGON\",       \n                    np.where( monsters['Type']==\"AIR_DRAGON\",\"DRAGON\",   \n                             monsters['Type']))))\n\nmonsters = monsters[ monsters['Time']>0]\n\nmonsters['Minute'] = monsters['Time'].astype(int)\n\nmonsters['Team'] = np.where( monsters['Team']==\"bDragons\",\"Blue\",\n                   np.where( monsters['Team']==\"bHeralds\",\"Blue\",\n                   np.where( monsters['Team']==\"bBarons\", \"Blue\", \n                           \"Red\")))\n\n\n\nmonsters.head()","8f2c7d5e":"# Merge the monsters to our previously merged table\n# This provides us with a table that has each event seperated by columns depending on what type of event it was\nkills_structures_monsters = kills_structures.merge(monsters[['id','Minute','Team','Time','Type2']], on=['id','Minute'],how='outer')\nkills_structures_monsters = kills_structures_monsters.sort_values(by=['id','Minute'])\nkills_structures_monsters.head(5)","632ae395":"# Although this is a good start, information is repeated on the rows if multiple \n# events occured in the same minute.\n#\n# Therefore, I decided to let each event have its own row by stacking the tables\n# on top of one another. We then add a more detailed time column and sort by this \n# so we know exactly which event came first (allowing for some errors with kill time\n# being averaged).\n\n\nstackedData = killsGrouped.append(structures2)\nstackedData = stackedData.append(monsters[['id','Address','Team','Minute','Time','Type2']])\n\nstackedData['Time2'] = stackedData['Time'].fillna(stackedData['Time Avg'])\n\nstackedData = stackedData.sort_values(by=['id','Time2'])\n\nstackedData['EventNum'] = stackedData.groupby('id').cumcount()+1\n\nstackedData = stackedData[['id','EventNum','Team','Minute','Time2','Count','Type','Lane','Type2']]\n\nstackedData.columns = ['id','EventNum','Team','Minute','Time','KillCount','StructType','StructLane','Monster']\n\nstackedData.head(5)","c7cfb52c":"# We then add an 'Event' column to merge the columns into one, where kills are now\n# simple labelled as 'KILLS'\n\nstackedData['Event'] = np.where(stackedData['KillCount']>0,\"KILLS\",None)\nstackedData['Event'] = stackedData['Event'].fillna(stackedData['StructType'])\nstackedData['Event'] = stackedData['Event'].fillna(stackedData['Monster'])\n\n                        \n\nstackedData.head(10)","7bb6d6d9":"stackedData['Event'].unique()","215b75cb":"\nNumEventAnalysis = stackedData[['id','EventNum']].groupby('id').max().reset_index()\n\nNumEventAnalysis2 = NumEventAnalysis.groupby('EventNum').count().reset_index()\n\nNumEventAnalysis2.head()","f0ef18aa":"plt.bar(NumEventAnalysis2['EventNum'],NumEventAnalysis2['id'] ,alpha=0.3)\nplt.plot(NumEventAnalysis2['EventNum'],NumEventAnalysis2['id'])\nplt.title('Distribution of Number of Events in Each Match (EXACT)')\nplt.xlim(0,100)\nplt.xlabel(\"Number of Events\")\nplt.ylabel(\"Number of Matches\")\nplt.show()","bc8d29c5":"sns.distplot(NumEventAnalysis['EventNum'],bins=65)\nplt.title('Distribution of Number of Events in Each Match (NORMAL DIST)')\nplt.xlim(0,100)\nplt.xlabel(\"Number of Events\")\nplt.ylabel(\"Number of Matches\")\nplt.show()","5555bb61":"print(\"The max number of events for any team in a single game is:\",NumEventAnalysis['EventNum'].max())\nprint(\"The min number of events for any team in a single game is:\",NumEventAnalysis['EventNum'].min())","4043f86b":"# We then create a table with just the unique match ids that we will use to merge our tables to shortly\nmatchevents = pd.DataFrame(stackedData['id'].unique())\nmatchevents.columns = ['id']\n\nmatchevents.head()\n","79f8ba84":"# WARNING: Takes a while to run\n\n# This cell has a lot of steps but the idea is to:\n#    1) Seperate the the events into each team (Red\/Blue)\n#    2) For each, go through each match and transpose the list of events into a single row\n#    3) Stack a table that has the events for both team of the matches\n\nbluerows = pd.DataFrame()\nstackedData_blue = stackedData\nstackedData_blue['EventBlue'] = np.where( stackedData_blue['Team']!=\"Red\",stackedData_blue['Event'],np.nan)\n\n\nredrows = pd.DataFrame()\nstackedData_red = stackedData\nstackedData_red['EventRed'] = np.where( stackedData_red['Team']==\"Red\",stackedData_red['Event'],np.nan)\n\n\nfor i in range(0,len(matchevents)):\n    \n    #Red Team Output\n    stackedData_match_red = stackedData_red[stackedData_red['id'] == matchevents.iloc[i,0] ]\n    \n    redextract = stackedData_match_red.iloc[:,[1,11]]\n    redextract.iloc[:,0] = redextract.iloc[:,0]-1\n    redextract = redextract.set_index('EventNum')\n    \n    redrow = pd.DataFrame(redextract.transpose())\n    redrow['id'] = (stackedData_match_red['id'].unique())\n    \n    redrows = redrows.append((redrow))\n    redrows = redrows.reset_index(drop=True)\n    \n    \n    \n    #Blue Team Output\n    stackedData_match_blue = stackedData_blue[stackedData_blue['id'] == matchevents.iloc[i,0] ]\n    \n    blueextract = stackedData_match_blue.iloc[:,[1,10]]\n    blueextract.iloc[:,0] = blueextract.iloc[:,0]-1\n    blueextract = blueextract.set_index('EventNum')\n    \n    bluerow = pd.DataFrame(blueextract.transpose())\n    bluerow['id'] = (stackedData_match_blue['id'].unique())\n    \n    bluerows = bluerows.append((bluerow))\n    bluerows = bluerows.reset_index(drop=True)\n    \n  \n    ","84884d76":"redrows = redrows.sort_values('id')\nredrows.head(5)","181b910d":"bluerows = bluerows.sort_values('id')\nbluerows.head(5)","7b8757d9":"# We can now merge these two tables for each team's events in the match to\n# our table with just the match ids. We also add a column for the result of \n# the red team for the match and change column names according to which team \n# made the event.\n\n\n\nmatchevents2 = matchevents.merge(redrows,how='left',on='id')\nmatchevents3 = matchevents2.merge(bluerows,how='left',on='id')\n    \n\n    \nmatchevents4 = matchevents3.merge(matchinfo[['id','rResult','gamelength']], on='id',how='left')\n\n\nmatchevents4.columns = ['id',\n'RedEvent1','RedEvent2','RedEvent3',\n'RedEvent4','RedEvent5','RedEvent6','RedEvent7',\n'RedEvent8','RedEvent9','RedEvent10','RedEvent11',\n'RedEvent12','RedEvent13','RedEvent14','RedEvent15',\n'RedEvent16','RedEvent17','RedEvent18','RedEvent19',\n'RedEvent20','RedEvent21','RedEvent22','RedEvent23',\n'RedEvent24','RedEvent25','RedEvent26','RedEvent27',\n'RedEvent28','RedEvent29','RedEvent30','RedEvent31',\n'RedEvent32','RedEvent33','RedEvent34','RedEvent35',\n'RedEvent36','RedEvent37','RedEvent38','RedEvent39',\n'RedEvent40','RedEvent41','RedEvent42','RedEvent43',\n'RedEvent44','RedEvent45','RedEvent46','RedEvent47',\n'RedEvent48','RedEvent49','RedEvent50','RedEvent51',\n'RedEvent52','RedEvent53','RedEvent54','RedEvent55',\n'RedEvent56','RedEvent57','RedEvent58','RedEvent59',\n'RedEvent60','RedEvent61','RedEvent62','RedEvent63',\n'RedEvent64','RedEvent65','RedEvent66','RedEvent67',\n'RedEvent68','RedEvent69','RedEvent70','RedEvent71',\n'RedEvent72','RedEvent73','RedEvent74','RedEvent75',\n'RedEvent76','RedEvent77','RedEvent78','RedEvent79',\n                        \n                        \n'BlueEvent1','BlueEvent2','BlueEvent3','BlueEvent4',\n'BlueEvent5','BlueEvent6','BlueEvent7','BlueEvent8',\n'BlueEvent9','BlueEvent10','BlueEvent11','BlueEvent12',\n'BlueEvent13','BlueEvent14','BlueEvent15','BlueEvent16',\n'BlueEvent17','BlueEvent18','BlueEvent19','BlueEvent20',\n'BlueEvent21','BlueEvent22','BlueEvent23','BlueEvent24',\n'BlueEvent25','BlueEvent26','BlueEvent27','BlueEvent28',\n'BlueEvent29','BlueEvent30','BlueEvent31','BlueEvent32',\n'BlueEvent33','BlueEvent34','BlueEvent35','BlueEvent36',\n'BlueEvent37','BlueEvent38','BlueEvent39','BlueEvent40',\n'BlueEvent41','BlueEvent42','BlueEvent43','BlueEvent44',\n'BlueEvent45','BlueEvent46','BlueEvent47','BlueEvent48',\n'BlueEvent49','BlueEvent50','BlueEvent51','BlueEvent52',\n'BlueEvent53','BlueEvent54','BlueEvent55','BlueEvent56',\n'BlueEvent57','BlueEvent58','BlueEvent59','BlueEvent60',\n'BlueEvent61','BlueEvent62','BlueEvent63',\n'BlueEvent64','BlueEvent65','BlueEvent66','BlueEvent67',\n'BlueEvent68','BlueEvent69','BlueEvent70','BlueEvent71',\n'BlueEvent72','BlueEvent73','BlueEvent74','BlueEvent75',\n'BlueEvent76','BlueEvent77','BlueEvent78','BlueEvent79',\n                        \n                        'rResult','gamelength']\n\n\n\nmatchevents4.head(20)","4e2111bc":"# We now decided, for the purpose of calculating probabilities, to consider one team's perseperctive.\n# Therefore, we make all events either positive or negative for red team but keep their label otherwise.\n\nmatchevents5=matchevents4\nfor j in range(1,len(list(redrows))):\n    matchevents5['RedEvent'+str(j)] = '+'+ matchevents5['RedEvent'+str(j)].astype(str)\n    matchevents5['BlueEvent'+str(j)] = '-'+ matchevents5['BlueEvent'+str(j)].astype(str)\n    \n    matchevents5 = matchevents5.replace('+nan',np.nan)\n    matchevents5['RedEvent'+str(j)] =  matchevents5['RedEvent'+str(j)].fillna(\n                                        (matchevents5['BlueEvent'+str(j)]).astype(str))\n    \nmatchevents5.head()","6b88a15d":"# We take on the red event columns now  and re-add the end result of the game for red team (1=win, 0=loss)\n\nRedMatchEvents = matchevents5.iloc[:,0:80]\nRedMatchEvents['RedResult'] = matchevents5['rResult']\nRedMatchEvents['MatchLength'] = matchevents5['gamelength']\nRedMatchEvents.iloc[0:10]","6dcab23a":"RedMatchEvents[['RedEvent1','id']].groupby('RedEvent1').count()","e349d007":"RedMatchEvents[['RedEvent1','MatchLength']].groupby('RedEvent1').mean()","594bfe51":"sns.boxplot(RedMatchEvents['RedEvent1'],RedMatchEvents['MatchLength'],RedMatchEvents['RedResult'],\n            boxprops=dict(alpha=.7) )\nplt.xticks(rotation=45)\nplt.title('Distribution of Match Length by First Event and Match Result (Win = 1, Loss = 0)')\nplt.ylim(0,100)\nplt.xlabel('Event 1')\nplt.plot([1.5, 1.5], [0, 100],'k', linewidth=2,alpha=0.8 )\nplt.plot([3.5, 3.5], [0, 100],'k', linewidth=2,alpha=0.8 )\nplt.plot([5.5, 5.5], [0, 100],'k', linewidth=2,alpha=0.8 )\nplt.show()","578f2f93":"# We can now use this to calculate some conditional probabilities as shown\n\nTestData = RedMatchEvents\n\nPwinGivenFirstBloodWon = ( (len(TestData[(TestData['RedEvent1']==\"+KILLS\")&(TestData['RedResult']==1)])\/len(TestData))\/\n        (len( TestData[TestData['RedEvent1']==\"+KILLS\"])\/len(TestData)) )\n    \nPwinGivenFirstBloodLost = ( (len(TestData[(TestData['RedEvent1']==\"-KILLS\")&(TestData['RedResult']==1)])\/len(TestData))\/\n        (len( TestData[TestData['RedEvent1']==\"-KILLS\"])\/len(TestData)) )\n\n\nPwinGivenFirstTowerWon = ( (len(TestData[(TestData['RedEvent1']==\"+OUTER_TURRET\")&(TestData['RedResult']==1)])\/len(TestData))\/\n        (len( TestData[TestData['RedEvent1']==\"+OUTER_TURRET\"])\/len(TestData)) )\n    \nPwinGivenFirstTowerLost = ( (len(TestData[(TestData['RedEvent1']==\"-OUTER_TURRET\")&(TestData['RedResult']==1)])\/len(TestData))\/\n        (len( TestData[TestData['RedEvent1']==\"-OUTER_TURRET\"])\/len(TestData)) )\n\n\nPwinGivenFirstDragonWon = ( (len(TestData[(TestData['RedEvent1']==\"+DRAGON\")&(TestData['RedResult']==1)])\/len(TestData))\/\n        (len( TestData[TestData['RedEvent1']==\"+DRAGON\"])\/len(TestData)) )\n    \nPwinGivenFirstDragonLost = ( (len(TestData[(TestData['RedEvent1']==\"-DRAGON\")&(TestData['RedResult']==1)])\/len(TestData))\/\n        (len( TestData[TestData['RedEvent1']==\"-DRAGON\"])\/len(TestData)) )\n\n\nPwinGivenFirstRiftHeraldWon = ( (len(TestData[(TestData['RedEvent1']==\"+RIFT_HERALD\")&(TestData['RedResult']==1)])\/len(TestData))\/\n        (len( TestData[TestData['RedEvent1']==\"+RIFT_HERALD\"])\/len(TestData)) )\n    \nPwinGivenFirstRiftHeraldLost = ( (len(TestData[(TestData['RedEvent1']==\"-RIFT_HERALD\")&(TestData['RedResult']==1)])\/len(TestData))\/\n        (len( TestData[TestData['RedEvent1']==\"-RIFT_HERALD\"])\/len(TestData)) )\n\n\n\n\n\nprint(\"-------FIRST BLOOD--------------------------------\")\nprint(\"P(Won | First Blood Taken):\",PwinGivenFirstBloodWon)\nprint(\"P(Won | First Blood Lost):\",PwinGivenFirstBloodLost)\n\nprint(\"\")\nprint(\"-------FIRST TURRET-------------------------------\")\nprint(\"P(Won | First Tower Won):\",PwinGivenFirstTowerWon)\nprint(\"P(Won | First Tower Lost):\",PwinGivenFirstTowerLost)\n\nprint(\"\")\nprint(\"-------FIRST DRAGON-------------------------------\")\nprint(\"P(Won | First Dragon Won):\",PwinGivenFirstDragonWon)\nprint(\"P(Won | First Dragon Lost):\",PwinGivenFirstDragonLost)\n\nprint(\"\")\nprint(\"-------FIRST RIFT HERALD (NOTE: ONLY 17 GAMES)----\")\nprint(\"P(Won | First Rift Herald Won):\",PwinGivenFirstRiftHeraldWon)\nprint(\"P(Won | First Rift Herald Lost):\",PwinGivenFirstRiftHeraldLost)\n","1e1bacde":"aggs = {'id':'count','MatchLength':'mean'}\n\nRedMatchTWOEvents = (RedMatchEvents[['RedEvent1','RedEvent2','RedResult','id','MatchLength']].groupby(\n        ['RedEvent1','RedEvent2','RedResult']).agg(aggs).reset_index())\n\nRedMatchTWOEvents = RedMatchTWOEvents.sort_values(['RedEvent1','RedEvent2','RedResult'])\n\nRedMatchTWOEventsWINS = RedMatchTWOEvents[RedMatchTWOEvents['RedResult']==1]\nRedMatchTWOEventsLOSS = RedMatchTWOEvents[RedMatchTWOEvents['RedResult']==0]\n\n","0267eff4":"# First merge the RedWin and RedLoss data tables\n# Then remove events which only resulted in a win then calculate the total number of games that has these two events\n# Use this total to calculate the prob of win and loss respectively \n\nRedMatchTWOEventsMERGED = RedMatchTWOEventsWINS.merge(RedMatchTWOEventsLOSS, how='left',on=['RedEvent1','RedEvent2'])\n\n\nRedMatchTWOEventsMERGED = RedMatchTWOEventsMERGED[RedMatchTWOEventsMERGED['id_y']>0]\nRedMatchTWOEventsMERGED['Total'] = RedMatchTWOEventsMERGED['id_x']+RedMatchTWOEventsMERGED['id_y']\n\nRedMatchTWOEventsMERGED['ProbWIN'] = RedMatchTWOEventsMERGED['id_x']\/RedMatchTWOEventsMERGED['Total'].sum()\nRedMatchTWOEventsMERGED['ProbLOSS'] = RedMatchTWOEventsMERGED['id_y']\/RedMatchTWOEventsMERGED['Total'].sum()\n\nRedMatchTWOEventsMERGED['ProbE1ANDE2'] = RedMatchTWOEventsMERGED['Total']\/(RedMatchTWOEventsMERGED['Total'].sum())\n\nRedMatchTWOEventsMERGED['ProbWINgivenE1ANDE2'] = RedMatchTWOEventsMERGED['ProbWIN']\/RedMatchTWOEventsMERGED['ProbE1ANDE2']\nRedMatchTWOEventsMERGED['ProbLOSSgivenE1ANDE2'] = RedMatchTWOEventsMERGED['ProbLOSS']\/RedMatchTWOEventsMERGED['ProbE1ANDE2']\n\n# Create column to single binary digit for whether the first event is positive or negative\n\nRedMatchTWOEventsMERGED['RedEvent1Gain'] = np.where(\n                                (RedMatchTWOEventsMERGED['RedEvent1']==\"+KILLS\") |\n                                (RedMatchTWOEventsMERGED['RedEvent1']==\"+OUTER_TURRET\") |\n                                (RedMatchTWOEventsMERGED['RedEvent1']==\"+DRAGON\") |\n                                (RedMatchTWOEventsMERGED['RedEvent1']==\"+RIFT_HERALD\") ,1,0\n                                                   \n                                                   \n                                                   )\n# Repeat for second event\n\nRedMatchTWOEventsMERGED['RedEvent2Gain'] = np.where(\n                                (RedMatchTWOEventsMERGED['RedEvent2']==\"+KILLS\") |\n                                (RedMatchTWOEventsMERGED['RedEvent2']==\"+OUTER_TURRET\") |\n                                (RedMatchTWOEventsMERGED['RedEvent2']==\"+DRAGON\") |\n                                (RedMatchTWOEventsMERGED['RedEvent2']==\"+RIFT_HERALD\") ,1,0\n                                                   \n                                                   \n                                                   )\n# Create another column for combination of first and second event outcomes classification\nRedMatchTWOEventsMERGED['Event1AND2Outcome'] = np.where(\n    (RedMatchTWOEventsMERGED['RedEvent1Gain']==1)&(RedMatchTWOEventsMERGED['RedEvent2Gain']==1),\"Both Positive\",\n                \n    np.where(\n        (((RedMatchTWOEventsMERGED['RedEvent1Gain']==1)&(RedMatchTWOEventsMERGED['RedEvent2Gain']==0))|\n        ((RedMatchTWOEventsMERGED['RedEvent1Gain']==0)&(RedMatchTWOEventsMERGED['RedEvent2Gain']==1))),\"One Positive\",\n    \n    np.where(\n        (RedMatchTWOEventsMERGED['RedEvent1Gain']==0)&(RedMatchTWOEventsMERGED['RedEvent2Gain']==0),\"Neither Positive\",\n             \"MISSING\",)))\n\n# Sort by highest probability of win to lowest\nRedMatchTWOEventsMERGED = RedMatchTWOEventsMERGED.sort_values('ProbWINgivenE1ANDE2',ascending=False)\n\n# Remove event combination with less than x number of games to remove possible outliers\nRedMatchTWOEventsMERGED = RedMatchTWOEventsMERGED[RedMatchTWOEventsMERGED['Total']>=0]\n\n\nRedMatchTWOEventsMERGED.head(5)","27d5d14b":"\nsns.pairplot(data = RedMatchTWOEventsMERGED, x_vars='ProbWINgivenE1ANDE2',y_vars='MatchLength_x',\n           hue= 'Event1AND2Outcome', size=8)\nplt.title('Probability of Winning Given the First Two Events against Average Game Duration, \\n Coloured by Event 1 and 2 Outcomes')\nplt.xlabel('Probability of Win GIVEN First Two Events')\nplt.ylabel('Average Game Length')\nplt.xlim([0,1])\nplt.xticks(np.arange(0,1.1,0.1))\n#plt.ylim([20,50])\n\nplt.show()","ae303825":"RedMatchEvents.head()","038e759e":"# WARNING: Takes a while to run\n# Replace all N\/As with the match outcome so that our final state is either a Win or Loss\nfor i in range(1,80):\n    RedMatchEvents['RedEvent'+str(i)] = RedMatchEvents['RedEvent'+str(i)].replace('-nan',RedMatchEvents['RedResult'].astype(str))\n    RedMatchEvents['RedEvent'+str(i)] = RedMatchEvents['RedEvent'+str(i)].replace('+nan',RedMatchEvents['RedResult'].astype(str))\n    #Print i for progress tracking\n    #print(i)\nRedMatchEvents.head()","d838cd5c":"RedMatchEvents[['RedEvent60','id']].groupby('RedEvent60').count()","647646dd":"RedMatchEvents2 = RedMatchEvents","54a87724":"# WARNING: Takes a little while to run\n\nEventList = [\n    #Positive Events\n       '+KILLS', '+OUTER_TURRET', '+DRAGON', '+RIFT_HERALD', '+BARON_NASHOR',\n       '+INNER_TURRET', '+BASE_TURRET', '+INHIBITOR', '+NEXUS_TURRET',\n       '+ELDER_DRAGON',\n    #Negative Events\n       '-KILLS', '-OUTER_TURRET', '-DRAGON', '-RIFT_HERALD', '-BARON_NASHOR',\n       '-INNER_TURRET', '-BASE_TURRET', '-INHIBITOR', '-NEXUS_TURRET',\n       '-ELDER_DRAGON',\n    #Game Win or Loss Events        \n       '1','0']\n\nRedMatchMDP = pd.DataFrame()\n\nfor i in range(1,79):\n                              \n    Event = i\n    for j1 in range(0,len(EventList)):\n        Event1 = EventList[j1]\n        for j2 in range(0,len(EventList)):\n            \n            Event2 = EventList[j2]\n            \n            \n            if  len(RedMatchEvents2[(RedMatchEvents2['RedEvent'+str(Event)]==Event1)])==0:\n                continue\n            #elif len(RedMatchEvents2[(RedMatchEvents2['RedEvent'+str(Event)]==Event1)&\n            #                   (RedMatchEvents2['RedEvent'+str(Event+1)]==Event2) ])==0:\n                continue\n                \n            else:\n                TransProb = (\n                    len(RedMatchEvents2[(RedMatchEvents2['RedEvent'+str(Event)]==Event1)&\n                               (RedMatchEvents2['RedEvent'+str(Event+1)]==Event2) ])\/\n\n                    len(RedMatchEvents2[(RedMatchEvents2['RedEvent'+str(Event)]==Event1)])\n                    )\n\n\n            RedMatchMDP2 = pd.DataFrame({'StartState':Event,'EndState':Event+1,'Event1':Event1,'Event2':Event2,'Probability':TransProb},\n                                  index=[0])\n            RedMatchMDP = RedMatchMDP.append(RedMatchMDP2)\n   \n    #Print i for tracking progress\n    #print(i)\n    \n","04b5a6bf":"RedMatchMDP = RedMatchMDP[['StartState','EndState','Event1','Event2','Probability']]\nRedMatchMDP[(RedMatchMDP['StartState']==61)&(RedMatchMDP['Event1']==\"+INHIBITOR\")]","95656f71":"EndCondition = RedMatchMDP[\n    ((RedMatchMDP['Event1']!=\"1\")&(RedMatchMDP['Event2']==\"1\") )|\n    ((RedMatchMDP['Event1']!=\"0\")&(RedMatchMDP['Event2']==\"0\"))]\n\nEndCondition = EndCondition.sort_values('Probability',ascending=False)\n\nEndConditionGrouped = EndCondition[['StartState','Probability']].groupby('StartState').mean().reset_index()\nEndConditionGrouped['CumProb'] = EndConditionGrouped['Probability'].cumsum()\n\nEndConditionGrouped2 = EndCondition[['StartState','Probability']].groupby('StartState').sum().reset_index()\nEndConditionGrouped2['CumProb'] = EndConditionGrouped2['Probability'].cumsum()\n\nfig, axes = plt.subplots(nrows=2, ncols=2)\n\naxes[0,0].bar(EndConditionGrouped['StartState'],EndConditionGrouped['Probability'] ,alpha=0.3)\naxes[0,0].plot(EndConditionGrouped['StartState'],EndConditionGrouped['Probability'])\naxes[0,0].set_title('Mean Probability Dist')\naxes[0,0].set_xlabel(\"State\")\naxes[0,0].set_ylabel(\"Probability of Ending\")\naxes[0,0].set_xticks([],[])\naxes[0,0].set_xlabel(\"\")\naxes[0,0].set_xlim([0,80])\naxes[0,0].grid(False)\n\naxes[0,1].bar(EndConditionGrouped['StartState'],EndConditionGrouped['CumProb'] ,alpha=0.3)\naxes[0,1].plot(EndConditionGrouped['StartState'],EndConditionGrouped['CumProb'])\naxes[0,1].set_title('Mean Cumulative Probability Dist')\naxes[0,1].set_xlabel(\"State\")\naxes[0,1].set_ylabel(\"Cumlative Probability of Ending\")\naxes[0,1].set_xticks([])\naxes[0,1].set_xlabel(\"\")\naxes[0,1].set_xlim([0,80])\naxes[0,1].grid(False)\n\naxes[1,0].bar(EndConditionGrouped2['StartState'],EndConditionGrouped2['Probability'] ,alpha=0.3)\naxes[1,0].plot(EndConditionGrouped2['StartState'],EndConditionGrouped2['Probability'])\naxes[1,0].set_title('Sum Probability Dist')\naxes[1,0].set_xlabel(\"State\")\naxes[1,0].set_ylabel(\"Probability of Ending\")\naxes[1,0].set_xlim([0,80])\naxes[1,0].grid(False)\n\naxes[1,1].bar(EndConditionGrouped2['StartState'],EndConditionGrouped2['CumProb'] ,alpha=0.3)\naxes[1,1].plot(EndConditionGrouped2['StartState'],EndConditionGrouped2['CumProb'])\naxes[1,1].set_title('Sum Cumulative Probability Dist')\naxes[1,1].set_xlabel(\"State\")\naxes[1,1].set_ylabel(\"Cumlative Probability of Ending\")\naxes[1,1].set_xlim([0,80])\naxes[1,1].grid(False)\n\nfig.suptitle(\"Probability of Game Ending in Each State Averaged and Summed over Varying Start Events\")\n\nfig.set_figheight(15)\nfig.set_figwidth(15)\nplt.show()","efcddf7e":"RedMatchMDP['Reward'] = 0\n\nRedMatchMDP.head()","209139d7":"len(RedMatchMDP)","533239dc":"RedMatchMDP[(RedMatchMDP['StartState']==15)&(RedMatchMDP['Event1']==\"+ELDER_DRAGON\")]","41f5a78a":"alpha = 0.1\ngamma = 0.9\nnum_episodes = 100\nepsilon = 0.1\n\nreward = RedMatchMDP['Reward']\n\nStartState = 1\nStartEvent = '+KILLS'\nStartAction = '+OUTER_TURRET'","fece8055":"def MCModelv1(data, alpha, gamma, epsilon, reward, StartState, StartEvent, StartAction, num_episodes):\n    \n    # Initiatise variables appropiately\n    \n    data['V'] = 0\n \n    \n    outcomes = pd.DataFrame()\n    episode_return = pd.DataFrame()\n    actions_output = pd.DataFrame()\n    \n    for e in range(0,num_episodes):\n        \n        action = []\n\n        current_state = StartState\n        current_action = StartEvent\n        next_action = StartAction \n   \n        actions = pd.DataFrame()\n \n        for a in range(0,100):\n            \n            action_table = pd.DataFrame()\n\n            \n            if (current_action==\"1\") | (current_action==\"0\") | (current_state==79):\n                continue\n            else:\n                \n                data_e = data[(data['StartState']==current_state)&(data['Event1']==current_action)]\n\n                data_e = data_e.sort_values('Probability')\n                data_e['CumProb'] = data_e['Probability'].cumsum()\n                data_e['CumProb'] = np.round(data_e['CumProb'],4)\n\n                \n                rng = np.round(np.random.random()*data_e['CumProb'].max(),4)\n                action_table = data_e[ data_e['CumProb'] >= rng]\n                action_table = action_table[ action_table['CumProb'] == action_table['CumProb'].min()]\n                action_table = action_table.reset_index()\n                \n                action = action_table['Event2'][0]\n                \n                if action == \"1\":\n                    step_reward = 10*(gamma**a)\n                elif action == \"0\":\n                    step_reward = -10*(gamma**a)\n                else:\n                    step_reward = -0.005*(gamma**a)\n                \n                action_table['StepReward'] = step_reward\n                \n\n                action_table['Episode'] = e\n                action_table['Action'] = a\n                \n                current_action = action\n                current_state = current_state+1\n                \n                \n                actions = actions.append(action_table)\n\n        actions_output = actions_output.append(actions)\n                \n        episode_return = actions['StepReward'].sum()\n\n                \n        actions['Return']= episode_return\n                \n        data = data.merge(actions[['StartState','EndState','Event1','Event2','Return']], how='left',on =['StartState','EndState','Event1','Event2'])\n        data['Return'] = data['Return'].fillna(0)    \n             \n        data['V'] = data['V'] + alpha*(data['Return']-data['V'])\n        data = data.drop('Return', 1)\n        \n        \n                \n        if current_action==\"1\":\n            outcome = \"WIN\"\n        elif current_action==\"0\":\n            outcome = \"LOSS\"\n        else:\n            outcome = \"INCOMPLETE\"\n        outcome = pd.DataFrame({'Epsiode':[e],'Outcome':[outcome]})\n        outcomes = outcomes.append(outcome)\n\n        \n        \n\n        \n   \n        \n    \n        \n    optimal_policy_table = data[ ( data['StartState']==StartState) & (data['Event1']==StartEvent)&(data['Event2']==StartAction)]\n     \n    for i in range(2,79):\n        optimal_V = data[data['StartState']==i]['V'].max()\n        optimal_policy = data[ ( data['V']==optimal_V) & (data['StartState']==i)]      \n        optimal_policy_table = optimal_policy_table.append(optimal_policy)\n                \n    return(outcomes,actions_output,data,optimal_policy_table)\n    ","2bd371bf":"start_time = timeit.default_timer()\n\n\nMdl = MCModelv1(data=RedMatchMDP, alpha = alpha, gamma=gamma, epsilon = epsilon, reward = reward,\n                StartState=StartState, StartEvent=StartEvent,StartAction=StartAction,\n                num_episodes = num_episodes)\n\nelapsed = timeit.default_timer() - start_time\n\nprint(\"Time taken to run model:\",np.round(elapsed\/60,2),\"mins\")","5eb7e2d5":"Mdl[3].head()","214dd01a":"def MCModelv2(data, alpha, gamma, epsilon, reward, StartState, StartEvent, StartAction, num_episodes):\n    \n    # Initiatise variables appropiately\n    \n    data['V'] = 0\n \n    \n    outcomes = pd.DataFrame()\n    episode_return = pd.DataFrame()\n    actions_output = pd.DataFrame()\n    \n    for e in range(0,num_episodes):\n        action = []\n\n        current_state = StartState\n        current_action = StartEvent\n         \n        \n      \n            \n            \n        actions = pd.DataFrame()\n \n        for a in range(0,100):\n            \n            action_table = pd.DataFrame()\n\n            \n            if (current_action==\"1\") | (current_action==\"0\") | (current_state==79):\n                continue\n            else:\n                \n                data_e = data[(data['StartState']==current_state)&(data['Event1']==current_action)]\n\n                data_e = data_e[data_e['Probability']>0]\n\n                \n                if (StartAction is None)&(a==0):\n                    random_first_action = data_e.sample()\n                    action_table = random_first_action\n                    action_table = action_table.reset_index()\n                    action = action_table['Event2'][0]\n                elif (a==0):\n                    action_table = data_e[ data_e['Event2'] ==StartAction]\n                    action = StartAction\n                else:\n                    data_e = data_e.sort_values('Probability')\n                    data_e['CumProb'] = data_e['Probability'].cumsum()\n                    data_e['CumProb'] = np.round(data_e['CumProb'],4)\n                    rng = np.round(np.random.random()*data_e['CumProb'].max(),4)\n                    action_table = data_e[ data_e['CumProb'] >= rng]\n                    action_table = action_table[ action_table['CumProb'] == action_table['CumProb'].min()]\n                    action_table = action_table.reset_index()\n\n                    action = action_table['Event2'][0]\n                if action == \"1\":\n                    step_reward = 10*(gamma**a)\n                elif action == \"0\":\n                    step_reward = -10*(gamma**a)\n                else:\n                    step_reward = -0.005*(gamma**a)\n\n                action_table['StepReward'] = step_reward\n\n\n                action_table['Episode'] = e\n                action_table['Action'] = a\n\n                current_action = action\n                current_state = current_state+1\n\n\n                actions = actions.append(action_table)\n\n        actions_output = actions_output.append(actions)\n                \n        episode_return = actions['StepReward'].sum()\n\n                \n        actions['Return']= episode_return\n                \n        data = data.merge(actions[['StartState','EndState','Event1','Event2','Return']], how='left',on =['StartState','EndState','Event1','Event2'])\n        data['Return'] = data['Return'].fillna(0)    \n             \n        data['V'] = data['V'] + alpha*(data['Return']-data['V'])\n        data = data.drop('Return', 1)\n        \n        \n                \n        if current_action==\"1\":\n            outcome = \"WIN\"\n        elif current_action==\"0\":\n            outcome = \"LOSS\"\n        else:\n            outcome = \"INCOMPLETE\"\n        outcome = pd.DataFrame({'Epsiode':[e],'Outcome':[outcome]})\n        outcomes = outcomes.append(outcome)\n\n        \n        \n\n        \n   \n        \n    \n        if StartAction is None:\n            optimal_policy_table = pd.DataFrame()\n            for i in range(1,79):\n                optimal_V = data[data['StartState']==i]['V'].max()\n                optimal_policy = data[ ( data['V']==optimal_V) & (data['StartState']==i)]      \n                optimal_policy_table = optimal_policy_table.append(optimal_policy)        \n        else:\n            optimal_policy_table = data[ ( data['StartState']==StartState) & (data['Event1']==StartEvent)&(data['Event2']==StartAction)]\n            for i in range(2,79):\n                optimal_V = data[data['StartState']==i]['V'].max()\n                optimal_policy = data[ ( data['V']==optimal_V) & (data['StartState']==i)]      \n                optimal_policy_table = optimal_policy_table.append(optimal_policy)\n\n    return(outcomes,actions_output,data,optimal_policy_table)\n    ","33e3e292":"alpha = 0.1\ngamma = 0.9\nnum_episodes = 100\nepsilon = 0.1\n\nreward = RedMatchMDP['Reward']\n\nStartState = 1\nStartEvent = '+KILLS'\nStartAction = None\n\n\nstart_time = timeit.default_timer()\n\n\nMdl2 = MCModelv2(data=RedMatchMDP, alpha = alpha, gamma=gamma, epsilon = epsilon, reward = reward,\n                StartState=StartState, StartEvent=StartEvent,StartAction=None,\n                num_episodes = num_episodes)\n\nelapsed = timeit.default_timer() - start_time\n\nprint(\"Time taken to run model:\",np.round(elapsed\/60,2),\"mins\")","b023c597":"Mdl2[3].head(30)","d1c6a015":"def MCModelv3(data, alpha, gamma, epsilon, reward, StartState, StartEvent, StartAction, num_episodes):\n    \n    # Initiatise variables appropiately\n    \n    data['V'] = 0\n    data_output = data\n    \n    outcomes = pd.DataFrame()\n    episode_return = pd.DataFrame()\n    actions_output = pd.DataFrame()\n    \n    for e in range(0,num_episodes):\n        action = []\n\n        current_state = StartState\n        current_action = StartEvent\n        \n        \n        data_e1 = data\n    \n    \n        actions = pd.DataFrame()\n\n        for a in range(0,100):\n            \n            action_table = pd.DataFrame()\n       \n           \n            if (current_action==\"1\") | (current_action==\"0\") | (current_state==79):\n                continue\n            else:\n                if a==0:\n                    data_e1=data_e1\n                    \n                elif (len(individual_actions_count[individual_actions_count['Event2']==\"+RIFT_HERALD\"])==1):\n                    data_e1_e1 = data_e1[(data_e1['Event2']!='+RIFT_HERALD')|(data_e1['Event2']!='-RIFT_HERALD')]\n                    \n                elif (len(individual_actions_count[individual_actions_count['Event2']==\"-RIFT_HERALD\"])==1):\n                    data_e1 = data_e1[(data_e1['Event2']!='+RIFT_HERALD')|(data_e1['Event2']!='-RIFT_HERALD')]\n                \n                elif (len(individual_actions_count[individual_actions_count['Event2']==\"+OUTER_TURRET\"])==3):\n                    data_e1 = data_e1[data_e1['Event2']!='+OUTER_TURRET']\n                elif (len(individual_actions_count[individual_actions_count['Event2']==\"-OUTER_TURRET\"])==3):\n                    data_e1 = data_e1[data_e1['Event2']!='-OUTER_TURRET']\n                    \n                elif (len(individual_actions_count[individual_actions_count['Event2']==\"+INNER_TURRET\"])==3):\n                    data_e1 = data_e1[data_e1['Event2']!='+INNER_TURRET']\n                elif (len(individual_actions_count[individual_actions_count['Event2']==\"-INNER_TURRET\"])==3):\n                    data_e1 = data_e1[data_e1['Event2']!='-INNER_TURRET']\n                    \n                elif (len(individual_actions_count[individual_actions_count['Event2']==\"+BASE_TURRET\"])==3):\n                    data_e1 = data_e1[data_e1['Event2']!='+BASE_TURRET']\n                elif (len(individual_actions_count[individual_actions_count['Event2']==\"-BASE_TURRET\"])==3):\n                    data_e1 = data_e1[data_e1['Event2']!='-BASE_TURRET']\n                    \n                elif (len(individual_actions_count[individual_actions_count['Event2']==\"+INHIBITOR\"])==3):\n                    data_e1 = data_e1[data_e1['Event2']!='+INHIBITOR']\n                elif (len(individual_actions_count[individual_actions_count['Event2']==\"-INHIBITOR\"])==3):\n                    data_e1 = data_e1[data_e1['Event2']!='-INHIBITOR']\n                    \n                elif (len(individual_actions_count[individual_actions_count['Event2']==\"+NEXUS_TURRET\"])==2):\n                    data_e1 = data_e1[data_e1['Event2']!='+NEXUS_TURRET']\n                elif (len(individual_actions_count[individual_actions_count['Event2']==\"-NEXUS_TURRET\"])==2):\n                    data_e1 = data_e1[data_e1['Event2']!='-NEXUS_TURRET']\n                \n                       \n                else:\n                    data_e1 = data_e1\n\n                \n                data_e = data_e1[(data_e1['StartState']==current_state)&(data_e1['Event1']==current_action)]\n                \n                data_e = data_e[data_e['Probability']>0]\n                \n                if (StartAction is None)&(a==0):\n                    random_first_action = data_e.sample()\n                    action_table = random_first_action\n                    action_table = action_table.reset_index()\n                    action = action_table['Event2'][0]\n                elif (a==0):\n                    action_table = data_e[ data_e['Event2'] ==StartAction]\n                    action = StartAction\n                else:\n                    data_e = data_e.sort_values('Probability')\n                    data_e['CumProb'] = data_e['Probability'].cumsum()\n                    data_e['CumProb'] = np.round(data_e['CumProb'],4)\n                    \n\n                    rng = np.round(np.random.random()*data_e['CumProb'].max(),4)\n                    action_table = data_e[ data_e['CumProb'] >= rng]\n                    action_table = action_table[ action_table['CumProb'] == action_table['CumProb'].min()]\n                    action_table = action_table.reset_index()\n\n                    action = action_table['Event2'][0]\n                if action == \"1\":\n                    step_reward = 10*(gamma**a)\n                elif action == \"0\":\n                    step_reward = -10*(gamma**a)\n                else:\n                    step_reward = -0.005*(gamma**a)\n\n                action_table['StepReward'] = step_reward\n\n\n                action_table['Episode'] = e\n                action_table['Action'] = a\n\n                current_action = action\n                current_state = current_state+1\n\n                \n                actions = actions.append(action_table)\n                \n                individual_actions_count = actions\n            \n\n        actions_output = actions_output.append(actions)\n                \n        episode_return = actions['StepReward'].sum()\n\n                \n        actions['Return']= episode_return\n                \n        data_output = data_output.merge(actions[['StartState','EndState','Event1','Event2','Return']], how='left',on =['StartState','EndState','Event1','Event2'])\n        data_output['Return'] = data_output['Return'].fillna(0)    \n             \n        data_output['V'] = data_output['V'] + alpha*(data_output['Return']-data_output['V'])\n        data_output = data_output.drop('Return', 1)\n        \n        \n                \n        if current_action==\"1\":\n            outcome = \"WIN\"\n        elif current_action==\"0\":\n            outcome = \"LOSS\"\n        else:\n            outcome = \"INCOMPLETE\"\n        outcome = pd.DataFrame({'Epsiode':[e],'Outcome':[outcome]})\n        outcomes = outcomes.append(outcome)\n\n        \n        \n\n        optimal_policy_table = pd.DataFrame()\n   \n        \n        if (StartAction is None):\n            \n            optimal_policy_table =    data_output[ (data_output['StartState']==StartState)&(data_output['Event1']==StartEvent) &\n                (data_output['V']==(data_output[(data_output['StartState']==StartState)&(data_output['Event1']==StartEvent)]['V'].max()))  ]\n            for i in range(2,79):\n                optimal_V = data_output[(data_output['StartState']==i)]['V'].max()\n                optimal_policy = data_output[ ( data_output['V']==optimal_V) & (data_output['StartState']==i)]      \n                optimal_policy_table = optimal_policy_table.append(optimal_policy)        \n        else:\n            optimal_policy_table = data_output[ ( data_output['StartState']==StartState) & (data_output['Event1']==StartEvent)&(data_output['Event2']==StartAction)]\n            for i in range(2,79):\n                optimal_V = data_output[data_output['StartState']==i]['V'].max()\n                optimal_policy = data_output[ ( data_output['V']==optimal_V) & (data_output['StartState']==i)]      \n                optimal_policy_table = optimal_policy_table.append(optimal_policy)\n                \n        if (StartAction is None):\n            currentpath_action = StartEvent\n            optimal_path = pd.DataFrame()\n\n            for i in range(1,79):\n                StartPathState = i\n                nextpath_action = data_output [ (data_output['V'] == data_output[ (data_output['StartState']==StartPathState) & (data_output['Event1']==currentpath_action) ]['V'].max()) & \n                                               (data_output['StartState']==StartPathState) & (data_output['Event1']==currentpath_action)  ]\n                if (nextpath_action['V'].max()==0):\n                    break\n                else:\n                    nextpath_action = nextpath_action.reset_index(drop=True)\n                    currentpath_action = nextpath_action['Event2'][0]\n                    optimal_path = optimal_path.append(nextpath_action)\n                    \n        else:\n            currentpath_action = StartEvent\n            optimal_path = data_output[(data_output['StartState']==StartPathState) & (data_output['Event1']==currentpath_action) & (data_output['Event2']==StartAction) ]\n            for i in range(2,79):\n                StartPathState = i\n                nextpath_action = data_output [ (data_output['V'] == data_output[ (data_output['StartState']==StartPathState) & (data_output['Event1']==currentpath_action) ]['V'].max()) & \n                                               (data_output['StartState']==StartPathState) & (data_output['Event1']==currentpath_action)  ]\n                if (nextpath_action['V'].max()==0):\n                    break\n                else:\n\n                    nextpath_action = nextpath_action.reset_index(drop=True)\n                    currentpath_action = nextpath_action['Event2'][0]\n                    optimal_path = optimal_path.append(nextpath_action)\n\n\n                \n                \n                \n    \n\n        \n\n\n\n    return(outcomes,actions_output,data_output,optimal_policy_table,optimal_path)\n    ","55bfeb3a":"alpha = 0.1\ngamma = 0.9\nnum_episodes = 100\nepsilon = 0.1\n\nreward = RedMatchMDP['Reward']\n\nStartState = 1\nStartEvent = '+KILLS'\nStartAction = None\n\n\nstart_time = timeit.default_timer()\n\n\nMdl3 = MCModelv3(data=RedMatchMDP, alpha = alpha, gamma=gamma, epsilon = epsilon, reward = reward,\n                StartState=StartState, StartEvent=StartEvent,StartAction=StartAction,\n                num_episodes = num_episodes)\n\nelapsed = timeit.default_timer() - start_time\n\nprint(\"Time taken to run model:\",np.round(elapsed\/60,2),\"mins\")\nprint(\"Avg Time taken per episode:\", np.round(elapsed\/num_episodes,2),\"secs\")","50d148fb":"Mdl3[3].head()","7bc48866":"Mdl3[4]","e9a8fed0":"### Part 1 Conclusion\n\nSo we have performed some interesting exploratory analysis, modelled the environment as an MDP and even created a RL model, however, there are still many challenges to overcome.\n \n  \n \n#### The biggest issue right now is our output doesn't account for cumulative success or failures. #\n\nIn other words, our model thinks we are just as likely to take good objectives in later stages irrespective of whether we have lost each one before and would likely be behind.\n\nWe can see this emphasised by our output getting us to a win from 20+ minutes by simply taking objective after objective even though we had lost the majority of objectives before it. There is no accountability for being in a losing position in our model.\n\nTo fix this, we must re-asses our MDP and consider features that would account for the long term success or failure of taking or losing objectives. \n\n#### This is a good start, in our next part we will redesign our MDP to account for this and fix some other issues. ","59fe0f6e":"# AI in Video Games: Improving Decision Making in League of Legends using Real Match Statistics and Personal Preferences\n\n## Part 1: Initial Exploratory Analysis and First Markov Decision Process Model","4db8abf9":"#### Import Packages and Data","c86243f2":"## Reinforcement Learning AI Model\n\n\nNow that we have our data modelled as an MDP, we can apply Reinforcement Learning. In short, this applied a model that simulates thousands of games and learns how good or bad each decision is towards reaching a win given the team\u2019s current position. \n\nWhat makes this AI is its ability to learn from its own trial and error experience. It starts with zero knowledge about the game but, as it is rewarded for reaching a win and punished for reaching a loss, it begins to recognise and remember which decisions are better than others. Our first models start with no knowledge but I later demonstrate the impact initial information about decisions can be fed into the model to represent a person\u2019s preferences.  \n\nSo how is the model learning? In short, we use Monte Carlo learning whereby each episode is a simulation of a game based on our MDP probabilities and depending on the outcome for the team, our return will vary (+1 terminal reward for win and -1 terminal reward for loss). The value of each action taken in this episode is then updated accordingly based on whether the outcome was a win or loss. \n\nIn Monte Carlo learning, we have a parameter 'gamma' that discounts the rewards and will give a higher value to immediate steps than later one. In our model, this can be understood by the fact that as we reach later stages of the games, the decisions we make will have a much larger impact on the final outcome than those made in the first few minutes. For example, losing a team fight in minute 50 is much more likely to lead to a loss than losing a team fight in the first 5 minutes.\n","3e3dd44b":"### RL Model V2\n\nThis is a good start but our first model requires the first action to be provided. Instead, we now repeat the process but enable it to calculate the optimal first action when none is given.","d0e09616":"### Pre-Processing and Exploratory Analysis","88d78707":"## Markov Decision Process (MDP)\n\nWe could calculate the conditional probabilities for more events but, as had already become a challenge, the calculation process would be increasingly complicated. Therefore, instead of this, we can model our data as an MDP where we create pairwise probabilities between the events. Each event stage is a state and we calculate the probability of going to the next state given we are in the current one at that event stage.\n\n\n","8b72ef4b":"### RL Model V3\n\nIt quickly became apparent that our model was not following the structure of the game correctly because we had set no limitations on the number of turrets available to destroy on either team. \n\nTherefore, it was taking more objectives than possible and so we now introduce a rule that removes the turrets, rift heralds and inhibitors after so many are taken by one team.","2ac555d2":"### Motivations and Objectives\nLeague of Legends is a team oriented video game where on two team teams (with 5 players in each) compete for objectives and kills. Gaining an advantage enables the players to become stronger (obtain better items and level up faster) than their opponents and, as their advantage increases, the likelihood of winning the game also increases. We therefore have a sequence of events dependent on previous events that lead to one team destroying the other\u2019s base and winning the game. \n\nSequences like this being modelled statistically is nothing new; for years now researchers have considered how this is applied in sports, such as basketball (https:\/\/arxiv.org\/pdf\/1507.01816.pdf), where a sequence of passing, dribbling and foul plays lead to a team obtaining or losing points. The aim of research such as this one mentioned is to provide more detailed insight beyond a simple box score (number of points or kill gained by player in basketball or video games respectively) and consider how teams perform when modelled as a sequence of events connected in time. \n\nModelling the events in this way is even more important in games such as League of Legends as taking objectives and kills lead towards both an item and level advantage. For example, a player obtaining the first kill of the game nets them gold that can be used to purchase more powerful items. With this item they are then strong enough to obtain more kills and so on until they can lead their team to a win. Facilitating a lead like this is often referred to as \u2018snowballing\u2019 as the players cumulatively gain advantages but often games are not this one sided and objects and team plays are more important. \n\n#### The aim of this is project is simple; can we calculate the next best event given what has occurred previously in the game so that the likelihood of eventually leading to a win increases based on real match statistics?\n\nHowever, there are many factors that lead to a player\u2019s decision making in a game that cannot be easily measured. No how matter how much data collected, the amount of information a player can capture is beyond any that a computer can detect (at least for now!). For example, players may be over or underperforming in this game or may simply have a preference for the way they play (often defined by the types of characters they play). Some players will naturally be more aggressive and look for kills while others will play passively and push for objectives instead.\nTherefore, we further develop our model to allow the player to adjust the recommended play on their preferences.\n"}}