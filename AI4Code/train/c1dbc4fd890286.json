{"cell_type":{"acfb8026":"code","408ad64b":"code","7275711a":"code","32b40d3d":"code","38570c57":"code","38953c59":"code","c97750fb":"code","5ce42f0a":"code","f731aaca":"code","013e933a":"code","ff008981":"code","7132a2fa":"code","64fa2993":"code","6474fb54":"code","cbe78c5a":"code","54f3d1ae":"code","88c6a5b6":"code","1431f295":"code","a4c42777":"code","45583d3d":"code","06068ba3":"code","866a97a9":"code","0c973231":"code","4cc56bdb":"code","d9570e31":"code","5a0eb7ee":"code","2911f44c":"code","7989b797":"code","996e4206":"code","42a972a0":"code","e883579d":"code","801af3f2":"code","8f9763a0":"code","569cf664":"code","9c7aaf6a":"code","5513a812":"code","e715e005":"code","ef7f0639":"code","905bd60c":"code","763a820b":"code","a684541b":"code","cf92ef61":"code","f1bc6108":"code","8b3c2fad":"code","2816eb1d":"code","733a204f":"code","15b411f0":"code","f56398fa":"code","6305f948":"code","62c97ab2":"markdown","8ba08c95":"markdown","c63c54b9":"markdown","1c8e8946":"markdown","7c91ae63":"markdown","4e5abfb9":"markdown","028fce77":"markdown"},"source":{"acfb8026":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\nfrom matplotlib import pyplot as plt\nimport seaborn as sns \n%matplotlib inline \nimport os\nimport matplotlib as mpl\nfrom pandas import Series\nimport re\n#Pas de message d'alertes\nimport warnings\nwarnings.filterwarnings('ignore')\n# Any results you write to the current directory are saved as output.","408ad64b":"df = pd.read_csv(\"..\/input\/train.csv\")  \nxtest=pd.read_csv('..\/input\/xtest.csv')","7275711a":"#Lire 5 ligne de mani\u00e8re al\u00e9a \ndf.sample(5)","32b40d3d":"#Regardons la taille  de notre dataset\ndf.dtypes","38570c57":"#convertir le type de author en object \ndf['author']=df['author'].astype('object')","38953c59":"df.columns","c97750fb":"#Nous allons supprimer la variables entities \ndf.drop(columns=['entities', 'Unnamed: 0'], inplace=True)","5ce42f0a":"#Regardons les vleurs manquantes \ndf.isnull().sum()","f731aaca":"#Statistiques descriptives des variables quantitatives \ndf_qual=df.select_dtypes(exclude=['object'])\ndf_qual.describe().plot(kind = \"area\",fontsize=22, figsize = (18,8), table = True,colormap=\"rainbow\")\nplt.xlabel('',)\nplt.ylabel('Value')\nplt.title(\"Statistiques g\u00e9n\u00e9rales des variables \")","013e933a":"df1=df\ndf1['author']=df1['author'].astype('object')\ndf1['author'][df1['author'] == '0' ] = 'Trump'\ndf1['author'][df1['author'] == '1' ] = 'Clinton'\nf,ax=plt.subplots(1,2,figsize=(16,7))\ndf['author'].value_counts().plot.pie(explode=[0.1,0.1],autopct='%1.1f%%',ax=ax[0],shadow=True)\nax[0].set_title('df1 author Count')\nax[0].set_ylabel('Count')\nsns.countplot('author',data=df,ax=ax[1])\nax[1].set_title('df1 author Count')\nplt.show()","ff008981":"#Description des variables quantitatives \ndf.select_dtypes(exclude='object').describe()","7132a2fa":"#description des variables quantitatives \ndf.select_dtypes(include='object').describe()","64fa2993":"df2=df[['retweet_count', 'favorite_count', 'author']]\ndf2['author'] = df2['author'].astype('object')\ndf2['author'][df2['author'] == 1 ] = 'Trump'\ndf2['author'][df2['author'] == 0 ] = 'Clinton'\nfig=plt.gcf()\nfig.set_size_inches(10,7)\nfig=sns.stripplot(x='author',\n                  y='favorite_count',data=df2,jitter=True,\n                  edgecolor='gray',size=8,palette='winter',orient='v')","6474fb54":"df2=df[['retweet_count', 'favorite_count', 'author']]\ndf2['author'] = df2['author'].astype('object')\ndf2['author'][df2['author'] == 1 ] = 'Trump'\ndf2['author'][df2['author'] == 0 ] = 'Clinton'\nfig=plt.gcf()\nfig.set_size_inches(10,7)\nfig=sns.stripplot(x='author',\n                  y='retweet_count',data=df2,jitter=True,\n                  edgecolor='gray',size=8,palette='winter',orient='v')","cbe78c5a":"#Allos nous allons d\u00e9finir un new data appel\u00e9 data qui contient uniquement lentgh, text et author \ndata=df[['author', 'text', 'favorite_count']] \ndata['author'][data['author'] == 1 ] = 'Trump'\ndata['author'][data['author'] == 0 ] = 'Clinton'","54f3d1ae":"data.sample(5)","88c6a5b6":"#Regardons est ce que la taille des twett peut nous indiquer sa  provenence \nsns.factorplot('author','favorite_count',data=df)\nplt.ioff()\nplt.show()","1431f295":"#Regardone la distribution des length en fonction des candidants \nmpl.rcParams['patch.force_edgecolor'] = True\nplt.style.use('seaborn-bright')\ndf.hist(column='favorite_count', by='author', bins=50,figsize=(11,5))","a4c42777":"import nltk\nfrom wordcloud import WordCloud,STOPWORDS\nimport re\nfrom nltk.corpus import stopwords\n#nltk.download('stopwords')","45583d3d":"def cleanedWords(raw_sentence):\n    letters_only = re.sub(\"[^a-zA-Z]\", \" \", raw_sentence)\n    words = letters_only.lower().split()                            \n    stops = set(stopwords.words(\"english\"))                  \n    meaningful_words = [w for w in words\n                       if w not in stops]\n    return meaningful_words","06068ba3":"#sms=tweet\n#cr\u00e9ation de la base tweet\ntweet=data[['text','author']]\n#fonction de code en binaire 0\/1\ndef transformSpamColumn(x):\n    if x=='Clinton':\n        return 0\n    return 1\n#appication de la fonction sur tweet['author']\ntweet['author']=tweet['author'].apply(transformSpamColumn)","866a97a9":"def getWordCloud(data, author):\n    data=data[data['author'] == author]\n    words = ' '.join(df['text'])\n    cleaned_word = \" \".join(cleanedWords(words))\n    wordcloud = WordCloud(stopwords=STOPWORDS,\n                      background_color='black',\n                      width=3000,\n                      height=2500\n                     )\n    wordcloud.generate(cleaned_word)\n    plt.figure(1,figsize=(12, 12))\n    plt.imshow(wordcloud)\n    plt.axis('off')\n    plt.show()","0c973231":"#les mots les plus fr\u00e9quent de Trump\ngetWordCloud(tweet,1)","4cc56bdb":"#Les mots plus fr\u00e9quents de Clinton\ngetWordCloud(tweet,0)","d9570e31":"from textblob import TextBlob\nfrom sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.svm import SVC, LinearSVC\nfrom sklearn.metrics import classification_report, f1_score, accuracy_score, confusion_matrix\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.model_selection import learning_curve, GridSearchCV\nfrom sklearn.model_selection import StratifiedKFold, cross_val_score, train_test_split \n#from sklearn.cross_validation import StratifiedKFold\nfrom sklearn.tree import DecisionTreeClassifier \n\nimport calendar\nimport datetime\nimport re","5a0eb7ee":"messages = df[['author','text']]\ntest = xtest[['text']] \nmessages.columns = ['label', 'message']\ntest.columns = ['message']","2911f44c":"print(messages[:5])","7989b797":"def split_into_tokens(message):\n    message = message  # convert bytes into proper unicode\n    return TextBlob(message).words","996e4206":"messages.message.head()","42a972a0":"messages.message.apply(split_into_tokens).head()","e883579d":"# nltk.download('wordnet')\ndef split_into_lemmas(message):\n    message = message.lower()\n    words = TextBlob(message).words\n    # for each word, take its \"base form\" = lemma \n    return [word.lemma for word in words]\n\nmessages.message.apply(split_into_lemmas).head()","801af3f2":"bow_transformer = CountVectorizer(analyzer=split_into_lemmas).fit(messages['message'])\nprint(len(bow_transformer.vocabulary_))\nprint(bow_transformer.get_feature_names()[:5])","8f9763a0":"messages_bow = bow_transformer.transform(messages['message'])\nprint('sparse matrix shape:', messages_bow.shape)\nprint('number of non-zeros:', messages_bow.nnz)\nprint('sparsity: %.2f%%' % (100.0 * messages_bow.nnz \/ (messages_bow.shape[0] * messages_bow.shape[1])))","569cf664":"tfidf_transformer = TfidfTransformer().fit(messages_bow)","9c7aaf6a":"print (tfidf_transformer.idf_[bow_transformer.vocabulary_['the']])\nprint (tfidf_transformer.idf_[bow_transformer.vocabulary_['hannity']])","5513a812":"messages_tfidf = tfidf_transformer.transform(messages_bow)\nprint(messages_tfidf.shape)","e715e005":" messages['label'] = messages['label']\n messages['label'] =  messages['label'].astype('int64')","ef7f0639":"spam_detector = SVC(kernel='sigmoid', gamma=1.0).fit(messages_tfidf,  messages['label'])","905bd60c":"all_predictions = spam_detector.predict(messages_tfidf)","763a820b":"tr_acc = accuracy_score(messages['message'], all_predictions)\n#print(\"Accuracy on training set:  %.2f%%\" % (100 * tr_acc))","a684541b":"pipeline = Pipeline([\n    ('bow', CountVectorizer(analyzer=split_into_lemmas)),  # strings to token integer counts\n    ('tfidf', TfidfTransformer()),  # integer counts to weighted TF-IDF scores\n    ('classifier', SVC(kernel='sigmoid', gamma=1.0)),  # train on TF-IDF vectors w\/ Naive Bayes classifier\n])        ","cf92ef61":"scores = cross_val_score(pipeline,  # steps to convert raw messages into models\n                         messages['message'], # training data\n                         messages['label'],  # training labels\n                         cv=10,  # split data randomly into 10 parts: 9 for training, 1 for scoring\n                         scoring='accuracy',  # which scoring metric?\n                         n_jobs=-1,  # -1 = use all cores = faster\n                         )\nprint(scores)","f1bc6108":"print(classification_report(messages['label'], all_predictions))","8b3c2fad":"print('Mean score:', scores.mean(), '\\n')\nprint('Stdev:', scores.std())","2816eb1d":"skf = StratifiedKFold(n_splits=5)\nparams = {\n    'tfidf__use_idf': (True, False),\n    'bow__analyzer': (split_into_lemmas, split_into_tokens),\n}\n\ngrid = GridSearchCV(\n    pipeline,  # pipeline from above\n    params,  # parameters to tune via cross validation\n    refit=True,  # fit using all available data at the end, on the best found param combination\n    n_jobs=-1,  # number of cores to use for parallelization; -1 for \"all cores\"\n    scoring='accuracy',  # what score are we optimizing?\n    cv=StratifiedKFold( n_splits=5),  # what type of cross validation to use\n)\n\n%time nb_detector = grid.fit(messages['message'], messages['label'])\nprint(nb_detector.cv_results_)","733a204f":"from sklearn.model_selection import GridSearchCV\nnb_detector.cv_results_","15b411f0":"predictions = nb_detector.predict(test['message'])","f56398fa":"print(predictions.shape[0])","6305f948":"predictions = pd.Series(predictions)\npredictions.index = range(predictions.shape[0])\npredictions.to_csv(\"predictions.csv\", index_label =\"index\", header = [\"prediction\"])","62c97ab2":" Dans notre base donn\u00e9es df, il n'y a aucune valeurs manquantes ","8ba08c95":"En moyenne  Trump \u00e0 plus de favorite_count que Clinton ","c63c54b9":"Dans la  base de donn\u00e9e, nous avons 50% de tweet fait par Clinton et 50% fait par Trump","1c8e8946":"Soumission ","7c91ae63":"# Textmining Tweets Trump vs Clinton","4e5abfb9":"1. D'apr\u00e8s ce graphique les twet de Clinton sont plus longue en moyenne que ceux de Trump ","028fce77":"# Partie Mod\u00e9lisation "}}