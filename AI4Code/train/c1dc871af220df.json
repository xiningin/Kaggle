{"cell_type":{"d0b75669":"code","17513200":"code","389fe0a2":"code","d80c0d9b":"code","55323e4b":"code","48a619bf":"code","7fca6f11":"code","892347c6":"code","60537abb":"code","3c0c398d":"code","8e8457af":"code","a3619fa0":"code","ade2e2b8":"code","153abd0f":"code","afab4afa":"code","ea7be93f":"code","b977b397":"code","df602577":"code","72b2abe2":"code","583ed6a5":"code","58e34b79":"code","f9cfbc94":"code","25e1e9d2":"code","88fae9e2":"code","b3475cb2":"code","d343287c":"code","32592789":"code","fcb18747":"code","d6f37358":"code","7ced3f0f":"code","9836951f":"code","09449d6b":"code","3e837767":"code","4343b848":"code","836aed6e":"code","abdae51c":"code","5f3a4a9f":"code","b41686dc":"code","56bfb156":"code","4c3da5b2":"code","cce988b6":"code","e62577fe":"code","54826fce":"code","f1bc096e":"markdown","4eece710":"markdown","e7f5a373":"markdown","16537f04":"markdown","4661403f":"markdown","76f3bc57":"markdown","1dce2230":"markdown","bc5ab075":"markdown","6af5a453":"markdown","c6a4cbf5":"markdown","006e7903":"markdown","1cdf6183":"markdown","ecff1002":"markdown","a324df6f":"markdown","c3518794":"markdown","6e2cad73":"markdown","d789e52d":"markdown","bc27b31f":"markdown","b8bd683d":"markdown","d3ed6113":"markdown","5a346ade":"markdown","886ae478":"markdown","9cdf3f82":"markdown","f7183164":"markdown","fa76e478":"markdown","b6d9145d":"markdown","bbedc537":"markdown","71914795":"markdown","ac385605":"markdown","5fd2a007":"markdown","9e6ec8f3":"markdown","307a171e":"markdown","b25c1f42":"markdown","69a6b1f1":"markdown","9723615f":"markdown","3ba9efec":"markdown","e6c7cba2":"markdown"},"source":{"d0b75669":"!pip install pdpipe","17513200":"# Loading neccesary packages\n\nimport numpy as np\nimport pandas as pd\nimport pdpipe as pdp\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom datetime import datetime\n\n#\n\nfrom scipy import stats\nfrom scipy.stats import skew, boxcox_normmax, norm\nfrom scipy.special import boxcox1p\n\n#\n\nfrom typing import Dict, List, Tuple, Sequence","389fe0a2":"def get_data_file_path(in_kaggle: bool) -> Tuple[str, str]:\n    train_set_path = ''\n    test_set_path = ''\n    \n    if in_kaggle:\n        # running in Kaggle, inside \n        # 'House Prices: Advanced Regression Techniques' competition kernel container\n        train_set_path = '..\/input\/house-prices-advanced-regression-techniques\/train.csv'\n        test_set_path = '..\/input\/house-prices-advanced-regression-techniques\/test.csv'\n    else:\n        # running locally\n        train_set_path = 'data\/train.csv'\n        test_set_path = 'data\/test.csv'\n    \n    return train_set_path,test_set_path","d80c0d9b":"\n# Loading datasets\nin_kaggle = True\ntrain_set_path, test_set_path = get_data_file_path(in_kaggle)\n\ntrain = pd.read_csv(train_set_path)\ntest = pd.read_csv(test_set_path)","55323e4b":"# check train dimension\ndisplay(train.shape)","48a619bf":"# check test dimension\ndisplay(test.shape)","7fca6f11":"# dropping unneccessary columns, merging training and test sets\n\ntrain.drop('Id', axis=1, inplace=True)\ntest.drop('Id', axis=1, inplace=True)","892347c6":"# Dropping outliers after detecting them by eye\n\ntrain = train.drop(train[(train['OverallQual'] < 5)\n                                  & (train['SalePrice'] > 200000)].index)\ntrain = train.drop(train[(train['GrLivArea'] > 4000)\n                                  & (train['SalePrice'] < 200000)].index)\ntrain = train.drop(train[(train['GarageArea'] > 1200)\n                                  & (train['SalePrice'] < 200000)].index)\ntrain = train.drop(train[(train['TotalBsmtSF'] > 3000)\n                                  & (train['SalePrice'] > 320000)].index)\ntrain = train.drop(train[(train['1stFlrSF'] < 3000)\n                                  & (train['SalePrice'] > 600000)].index)\ntrain = train.drop(train[(train['1stFlrSF'] > 3000)\n                                  & (train['SalePrice'] < 200000)].index)\n","60537abb":"# Backing up target variables and dropping them from train data\n\ny = train['SalePrice'].reset_index(drop=True)\ntrain_features = train.drop(['SalePrice'], axis=1)\ntest_features = test\n\n# Merging features\n\nfeatures = pd.concat([train_features, test_features]).reset_index(drop=True)\nprint(features.shape)","3c0c398d":"# List of NaN including columns where NaN's mean none.\nnone_cols = [\n    'Alley', 'PoolQC', 'MiscFeature', 'Fence', 'FireplaceQu', 'GarageType',\n    'GarageFinish', 'GarageQual', 'GarageCond', 'BsmtQual', 'BsmtCond',\n    'BsmtExposure', 'BsmtFinType1', 'BsmtFinType2', 'MasVnrType'\n]\n\n# List of NaN including columns where NaN's mean 0.\n\nzero_cols = [\n    'BsmtFinSF1', 'BsmtFinSF2', 'BsmtUnfSF', 'TotalBsmtSF', 'BsmtFullBath',\n    'BsmtHalfBath', 'GarageYrBlt', 'GarageArea', 'GarageCars', 'MasVnrArea'\n]\n\n# List of NaN including columns where NaN's actually missing gonna replaced with mode.\n\nfreq_cols = [\n    'Electrical', 'Exterior1st', 'Exterior2nd', 'Functional', 'KitchenQual',\n    'SaleType', 'Utilities'\n]\n\n# Filling the list of columns above:\n\nfor col in zero_cols:\n    features[col].replace(np.nan, 0, inplace=True)\n\nfor col in none_cols:\n    features[col].replace(np.nan, 'None', inplace=True)\n\nfor col in freq_cols:\n    features[col].replace(np.nan, features[col].mode()[0], inplace=True)\n    \n# Filling MSZoning according to MSSubClass\nfeatures['MSZoning'] = features.groupby('MSSubClass')['MSZoning'].apply(\n    lambda x: x.fillna(x.mode()[0]))\n\n# Filling LotFrontage according to Neighborhood\nfeatures['LotFrontage'] = features.groupby(\n    ['Neighborhood'])['LotFrontage'].apply(lambda x: x.fillna(x.median()))","8e8457af":"# Features which numerical on data but should be treated as category.\nfeatures['MSSubClass'] = features['MSSubClass'].astype(str)\nfeatures['YrSold'] = features['YrSold'].astype(str)\nfeatures['MoSold'] = features['MoSold'].astype(str)","a3619fa0":"# Transforming rare values(less than 10) into one group - dimensionality reduction\n\nothers = [\n    'Condition1', 'Condition2', 'RoofMatl', 'Exterior1st', 'Exterior2nd',\n    'Heating', 'Electrical', 'Functional', 'SaleType'\n]\n\nfor col in others:\n    mask = features[col].isin(\n        features[col].value_counts()[features[col].value_counts() < 10].index)\n    features[col][mask] = 'Other'","ade2e2b8":"# Converting some of the categorical values to numeric ones.\n\nneigh_map = {\n    'MeadowV': 1,\n    'IDOTRR': 1,\n    'BrDale': 1,\n    'BrkSide': 2,\n    'OldTown': 2,\n    'Edwards': 2,\n    'Sawyer': 3,\n    'Blueste': 3,\n    'SWISU': 3,\n    'NPkVill': 3,\n    'NAmes': 3,\n    'Mitchel': 4,\n    'SawyerW': 5,\n    'NWAmes': 5,\n    'Gilbert': 5,\n    'Blmngtn': 5,\n    'CollgCr': 5,\n    'ClearCr': 6,\n    'Crawfor': 6,\n    'Veenker': 7,\n    'Somerst': 7,\n    'Timber': 8,\n    'StoneBr': 9,\n    'NridgHt': 10,\n    'NoRidge': 10\n}\n\nfeatures['Neighborhood'] = features['Neighborhood'].map(neigh_map).astype('int')\next_map = {'Po': 1, 'Fa': 2, 'TA': 3, 'Gd': 4, 'Ex': 5}\nfeatures['ExterQual'] = features['ExterQual'].map(ext_map).astype('int')\nfeatures['ExterCond'] = features['ExterCond'].map(ext_map).astype('int')\nbsm_map = {'None': 0, 'Po': 1, 'Fa': 2, 'TA': 3, 'Gd': 4, 'Ex': 5}\nfeatures['BsmtQual'] = features['BsmtQual'].map(bsm_map).astype('int')\nfeatures['BsmtCond'] = features['BsmtCond'].map(bsm_map).astype('int')\nbsmf_map = {\n    'None': 0,\n    'Unf': 1,\n    'LwQ': 2,\n    'Rec': 3,\n    'BLQ': 4,\n    'ALQ': 5,\n    'GLQ': 6\n}\n\nfeatures['BsmtFinType1'] = features['BsmtFinType1'].map(bsmf_map).astype('int')\nfeatures['BsmtFinType2'] = features['BsmtFinType2'].map(bsmf_map).astype('int')\nheat_map = {'Po': 1, 'Fa': 2, 'TA': 3, 'Gd': 4, 'Ex': 5}\nfeatures['HeatingQC'] = features['HeatingQC'].map(heat_map).astype('int')\nfeatures['KitchenQual'] = features['KitchenQual'].map(heat_map).astype('int')\nfeatures['FireplaceQu'] = features['FireplaceQu'].map(bsm_map).astype('int')\nfeatures['GarageCond'] = features['GarageCond'].map(bsm_map).astype('int')\nfeatures['GarageQual'] = features['GarageQual'].map(bsm_map).astype('int')","153abd0f":"# Creating new features  based on previous observations\n\nfeatures['TotalSF'] = (features['BsmtFinSF1'] + features['BsmtFinSF2'] +\n                       features['1stFlrSF'] + features['2ndFlrSF'])\nfeatures['TotalBathrooms'] = (features['FullBath'] +\n                              (0.5 * features['HalfBath']) +\n                              features['BsmtFullBath'] +\n                              (0.5 * features['BsmtHalfBath']))\n\nfeatures['TotalPorchSF'] = (features['OpenPorchSF'] + features['3SsnPorch'] +\n                            features['EnclosedPorch'] +\n                            features['ScreenPorch'] + features['WoodDeckSF'])\n\nfeatures['YearBlRm'] = (features['YearBuilt'] + features['YearRemodAdd'])\n\n# Merging quality and conditions\n\nfeatures['TotalExtQual'] = (features['ExterQual'] + features['ExterCond'])\nfeatures['TotalBsmQual'] = (features['BsmtQual'] + features['BsmtCond'] +\n                            features['BsmtFinType1'] +\n                            features['BsmtFinType2'])\nfeatures['TotalGrgQual'] = (features['GarageQual'] + features['GarageCond'])\nfeatures['TotalQual'] = features['OverallQual'] + features[\n    'TotalExtQual'] + features['TotalBsmQual'] + features[\n        'TotalGrgQual'] + features['KitchenQual'] + features['HeatingQC']\n\n# Creating new features by using new quality indicators\n\nfeatures['QualGr'] = features['TotalQual'] * features['GrLivArea']\nfeatures['QualBsm'] = features['TotalBsmQual'] * (features['BsmtFinSF1'] +\n                                                  features['BsmtFinSF2'])\nfeatures['QualPorch'] = features['TotalExtQual'] * features['TotalPorchSF']\nfeatures['QualExt'] = features['TotalExtQual'] * features['MasVnrArea']\nfeatures['QualGrg'] = features['TotalGrgQual'] * features['GarageArea']\nfeatures['QlLivArea'] = (features['GrLivArea'] -\n                         features['LowQualFinSF']) * (features['TotalQual'])\nfeatures['QualSFNg'] = features['QualGr'] * features['Neighborhood']","afab4afa":"# Creating some simple features\n\nfeatures['HasPool'] = features['PoolArea'].apply(lambda x: 1 if x > 0 else 0)\n\nfeatures['Has2ndFloor'] = features['2ndFlrSF'].apply(lambda x: 1 if x > 0 else 0)\n\nfeatures['HasGarage'] = features['QualGrg'].apply(lambda x: 1 if x > 0 else 0)\n\nfeatures['HasBsmt'] = features['QualBsm'].apply(lambda x: 1 if x > 0 else 0)\n\nfeatures['HasFireplace'] = features['Fireplaces'].apply(lambda x: 1 if x > 0 else 0)\n\nfeatures['HasPorch'] = features['QualPorch'].apply(lambda x: 1 if x > 0 else 0)","ea7be93f":"possible_skewed = [\n    'LotFrontage', 'LotArea', 'MasVnrArea', 'BsmtFinSF1', 'BsmtFinSF2',\n    'BsmtUnfSF', 'TotalBsmtSF', '1stFlrSF', '2ndFlrSF', 'GrLivArea',\n    'GarageArea', 'WoodDeckSF', 'OpenPorchSF', 'EnclosedPorch', '3SsnPorch',\n    'ScreenPorch', 'PoolArea', 'LowQualFinSF', 'MiscVal'\n]\n\n# Finding skewness of the numerical features\n\nskew_features = np.abs(features[possible_skewed].apply(lambda x: skew(x)).sort_values(\n    ascending=False))\n\n# Filtering skewed features\n\nhigh_skew = skew_features[skew_features > 0.3]\n\n# Taking indexes of high skew\n\nskew_index = high_skew.index\n\n# Applying boxcox transformation to fix skewness\n\nfor i in skew_index:\n    features[i] = boxcox1p(features[i], boxcox_normmax(features[i] + 1))","b977b397":"# Features to drop\n\nto_drop = [\n    'Utilities',\n    'PoolQC',\n    'YrSold',\n    'MoSold',\n    'ExterQual',\n    'BsmtQual',\n    'GarageQual',\n    'KitchenQual',\n    'HeatingQC',\n]\n\n# Dropping ML-irrelevant features\n\nfeatures.drop(columns=to_drop, inplace=True)","df602577":"# Getting dummy variables for ategorical data\nfeatures = pd.get_dummies(data=features)","72b2abe2":"print(f'Number of missing values: {features.isna().sum().sum()}')","583ed6a5":"features.shape","58e34b79":"# Separating train and test set\n\ntrain = features.iloc[:len(y), :]\ntest = features.iloc[len(train):, :]","f9cfbc94":"# Setting model data\n\nX = train\nX_test = test\ny = np.log1p(y)","25e1e9d2":"# Loading neccesary packages for modelling and feature selection\nfrom sklearn.model_selection import cross_val_score, KFold, cross_validate\nfrom sklearn.feature_selection import RFE, f_regression\nfrom sklearn.linear_model import (LinearRegression, Ridge, Lasso)\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.ensemble import RandomForestRegressor\n\n# Setting kfold for future use\nkf = KFold(10, random_state=42, shuffle=True)\n\n# Train our baseline RF Regression model for feature importance scoring\/feature selection\nn_trees = 100\nrf = RandomForestRegressor(n_jobs=-1, n_estimators=n_trees, verbose=1)\nrf.fit(X, y)\n","88fae9e2":"def rfe_select_featurs(X, y, estimator, num_features) -> List[str]:\n    rfe_selector = RFE(estimator=estimator, \n                       n_features_to_select=num_features, \n                       step=10, verbose=5)\n    rfe_selector.fit(X, y)\n    rfe_support = rfe_selector.get_support()\n    rfe_feature = X.loc[:,rfe_support].columns.tolist()\n    print(str(len(rfe_feature)), 'selected features')\n    \n    return rfe_feature","b3475cb2":"# total list of features\ncolnames = X.columns\n# Define dictionary to store our rankings\nranks = {}\n# Create our function which stores the feature rankings to the ranks dictionary\ndef ranking(ranks, names, order=1):\n    minmax = MinMaxScaler()\n    ranks = minmax.fit_transform(order*np.array([ranks]).T).T[0]\n    ranks = map(lambda x: round(x,2), ranks)\n    return dict(zip(names, ranks))","d343287c":"# Do FRE feature importance scoring - \n# stop the search when only the last feature is left\nrfe = RFE(rf, n_features_to_select=1, verbose =3 )\nrfe.fit(X, y)\nranks[\"RFE_RF\"] = ranking(list(map(float, rfe.ranking_)), colnames, order=-1)","32592789":"# Extract feature importance coefficients as calculated by the trained model\nranks[\"RF\"] = ranking(rf.feature_importances_, colnames);","fcb18747":"# all ranks\n# Put the mean scores into a Pandas dataframe\nrfe_rf_df = pd.DataFrame(list(ranks['RFE_RF'].items()), columns= ['Feature','rfe_importance'])\nrf_df = pd.DataFrame(list(ranks['RF'].items()), columns= ['Feature','alg_importance'])\n\nall_ranks = pd.merge(rfe_rf_df, rf_df, on=['Feature'])\n\ndisplay(all_ranks.head(10))","d6f37358":"from sklearn.feature_selection import SelectFromModel\n\nembeded_rf_selector = SelectFromModel(rf, max_features=200)\nembeded_rf_selector.fit(X, y)\n\nembeded_rf_support = embeded_rf_selector.get_support()\nembeded_rf_feature = X.loc[:,embeded_rf_support].columns.tolist()\nprint(str(len(embeded_rf_feature)), 'selected features')\n","7ced3f0f":"print(embeded_rf_feature)","9836951f":"from sklearn.inspection import permutation_importance\n\n\n# Here's how you use permutation importance\ndef get_permutation_importance(X, y, model) -> pd.DataFrame:\n    result = permutation_importance(model, X, y, n_repeats=1,\n                                random_state=0)\n    \n    # permutational importance results\n    result_df = pd.DataFrame(colnames,  columns=['Feature'])\n    result_df['permutation_importance'] = result.get('importances')\n    \n    return result_df","09449d6b":"\npermutate_df = get_permutation_importance(X, y, rf)\npermutate_df.sort_values('permutation_importance', \n                   ascending=False)[\n                                    ['Feature','permutation_importance'\n                                    ]\n                                  ][:30].style.background_gradient(cmap='Blues')","3e837767":"from sklearn.base import clone \n\ndef drop_col_feat_imp(model, X_train, y_train, random_state = 42):\n    \n    # clone the model to have the exact same specification as the one initially trained\n    model_clone = clone(model)\n    # set random_state for comparability\n    model_clone.random_state = random_state\n    # training and scoring the benchmark model\n    model_clone.fit(X_train, y_train)\n    benchmark_score = model_clone.score(X_train, y_train)\n    # list for storing feature importances\n    importances = []\n    \n    # iterating over all columns and storing feature importance (difference between benchmark and new model)\n    for col in X_train.columns:\n        model_clone = clone(model)\n        model_clone.random_state = random_state\n        model_clone.fit(X_train.drop(col, axis = 1), y_train)\n        drop_col_score = model_clone.score(X_train.drop(col, axis = 1), y_train)\n        importances.append( round( (benchmark_score - drop_col_score)\/benchmark_score, 4) )\n    \n    importances_df = pd.DataFrame(X_train.columns, columns=['Feature'])\n    importances_df['drop_col_importance'] = importances\n    return importances_df\n\ndrop_col_impt_df = drop_col_feat_imp(rf, X, y)\n\n","4343b848":"drop_col_impt_df.sort_values('drop_col_importance', \n                   ascending=False)[\n                                    ['Feature','drop_col_importance'\n                                    ]\n                                  ][:30].style.background_gradient(cmap='Blues')","836aed6e":"# merge drop_col_impt_df\nall_ranks = pd.merge(all_ranks, drop_col_impt_df, on=['Feature'])\n\n# merge permutate_df\nall_ranks = pd.merge(all_ranks, permutate_df, on=['Feature'])\n\n# calculate average feature importance\naverage_fi_pipeline = pdp.PdPipeline([\n    pdp.ApplyToRows(\n        lambda row: (row['drop_col_importance'] + row['permutation_importance'] + row['rfe_importance'] + row['alg_importance'])\/4, \n        colname='mean_feature_importance') # 'mean_feature_importance\n])\n\nall_ranks = average_fi_pipeline.apply(all_ranks)\n\ndisplay(all_ranks.reset_index().drop(['index'], axis=1).style.background_gradient(cmap='summer_r'))","abdae51c":"def get_top_features_by_rank(metric_col_name: str, feature_number: int):\n    features_df = all_ranks.copy()\n    \n    # features_df = features_df.sort_values(by=['feature_number'])\n    \n    # TODO: [:feature_number]\n    \n    # top n rows ordered by multiple columns\n    features_df = features_df.nlargest(feature_number, [metric_col_name])\n    \n    result_list = list(features_df['Feature'])\n    return result_list\n\ndef model_check(X, y, estimator, model_name, model_description, cv):\n    model_table = pd.DataFrame()\n\n    cv_results = cross_validate(estimator,\n                                X,\n                                y,\n                                cv=cv,\n                                scoring='neg_root_mean_squared_error',\n                                return_train_score=True,\n                                n_jobs=-1)\n\n    train_rmse = -cv_results['train_score'].mean()\n    test_rmse = -cv_results['test_score'].mean()\n    test_std = cv_results['test_score'].std()\n    fit_time = cv_results['fit_time'].mean()\n\n    attributes = {\n        'model_name': model_name,\n        'train_score': train_rmse,\n        'test_score': test_rmse,\n        'test_std': test_std,\n        'fit_time': fit_time,\n        'description': model_description,\n    }\n    \n    model_table = pd.DataFrame(data=[attributes])\n    return model_table","5f3a4a9f":"# check the baseline RF\nbaseline = model_check(X, y, rf, 'Baseline RF', \"Baseline RF (100 trees, all features)\", kf)\nresult_df = baseline","b41686dc":"n_estimators = [200, 300, 400, 500, 600, 700, 800, 900, 1000]\nfor n in n_estimators:\n    rf2 = RandomForestRegressor(n_jobs=-1, n_estimators=n, verbose=1)\n    description = \"RF with n_trees = {}\".format(n)\n    model_check_df = model_check(X, y, rf2, 'RF - All Features', description, kf)\n    \n    # concatenate\n    frames = [result_df, model_check_df]\n    result_df = pd.concat(frames)","56bfb156":"# subset of features selected by RFE feature importance\ntop_rfe_features = 50\nrfe_features = get_top_features_by_rank('rfe_importance', top_rfe_features)\nX_important_features = X[rfe_features]\nn_estimators = [100, 200, 300, 400, 500, 600, 700, 800, 900, 1000]\nfor n in n_estimators:\n    rf2 = RandomForestRegressor(n_jobs=-1, n_estimators=n, verbose=1)\n    description = \"RF with top {} RFE features, n_trees = {}\".format(top_rfe_features, n)\n    model_check_df = model_check(X_important_features, y, rf2, 'RFE Features', description, kf)\n    \n    # concatenate\n    frames = [result_df, model_check_df]\n    result_df = pd.concat(frames)","4c3da5b2":"# subset of features selected by RF embedded Feature Selection\nX_embedded_features = X[embeded_rf_feature]\nn_estimators = [100, 200, 300, 400, 500, 600, 700, 800, 900, 1000]\nfor n in n_estimators:\n    rf2 = RandomForestRegressor(n_jobs=-1, n_estimators=n, verbose=1)\n    description = \"RF witn n_trees = {}\".format(n)\n    model_check_df = model_check(X_embedded_features, y, rf2, 'RF - Embedded Features', description, kf)\n    \n    # concatenate\n    frames = [result_df, model_check_df]\n    result_df = pd.concat(frames)","cce988b6":"# train RF with the top importance feautres selected via the permutation method\ntop_features = 50\nimportant_features = get_top_features_by_rank('permutation_importance', top_features)\nX_important_features = X[important_features]\n\nn_estimators = [100, 200, 300, 400, 500, 600, 700, 800, 900, 1000]\nfor n in n_estimators:\n    rf2 = RandomForestRegressor(n_jobs=-1, n_estimators=n, verbose=1)\n    description = \"RF with top {} permutatively important features, n_trees = {}\".format(top_rfe_features, n)\n    model_check_df = model_check(X_important_features, y, rf2, 'RF - Permutatively Important Features', description, kf)\n    \n    # concatenate\n    frames = [result_df, model_check_df]\n    result_df = pd.concat(frames)\n","e62577fe":"# train RF with the top importance feautres selected via the drop-column method\ntop_features = 50\nimportant_features = get_top_features_by_rank('drop_col_importance', top_features)\nX_important_features = X[important_features]\n\nn_estimators = [100, 200, 300, 400, 500, 600, 700, 800, 900, 1000]\nfor n in n_estimators:\n    rf2 = RandomForestRegressor(n_jobs=-1, n_estimators=n, verbose=1)\n    description = \"RF with top {} drop-col-important features, n_trees = {}\".format(top_rfe_features, n)\n    model_check_df = model_check(X_important_features, y, rf2, 'RF - Drop-Column Important Features', description, kf)\n    \n    # concatenate\n    frames = [result_df, model_check_df]\n    result_df = pd.concat(frames)","54826fce":"display(result_df.reset_index().drop(['index'], axis=1).style.background_gradient(cmap='summer_r'))","f1bc096e":"After that, we are going to train a set of RF models, using the entire feature set we have after the  feature engineering above","4eece710":"# New Feature Engineering","e7f5a373":"First of all, we are going to train a baseline RF model. It is going to be rather simple and not likely to perform in a good fashion. However, it will provide as the reasonable indication on how the more advanced models improve their performace vs. the baseline.","16537f04":"As a final step in our experiments, we are going to train a set of RF models with the top 50 features selected by **drop-column method**","4661403f":"# References\n\nYou can refer to the blog posts below if you like to undertake the deeper dive into industrial feature selection\/feature importance calculation techniques\n\n- Feature Selection with pandas - https:\/\/towardsdatascience.com\/feature-selection-with-pandas-e3690ad8504b \n- 5 feature selection methods every data scientist need to know - https:\/\/towardsdatascience.com\/the-5-feature-selection-algorithms-every-data-scientist-need-to-know-3a6b566efd2 \n- Feature Importance May Be Lying to You - https:\/\/towardsdatascience.com\/feature-importance-may-be-lying-to-you-3247cafa7ee7 \n- Beware Default Random Forest Importance - https:\/\/explained.ai\/rf-importance\/ \n\n","76f3bc57":"# Forecasting Model Experiments\n\nBased on the feature selection experiments above, we are going to train a bunch of different Random Forests (RF) models (using different feature sets and number of trees to train) to see which one performs better.\n\nIn the course of the activities down the road, we will have to repeat two core activities below\n\n- subset top N features from the entire set of available features, based on a certain feature importance metrics\n- measure an individual model performace as well as output it in a format ready to aggregate in a single dataframe (to compare each model with its counterparts  as well as identify the one with the best performance)\n\nSo we are going to automate it with two auxiliary functions below\n- *get_top_features_by_rank*\n- *model_check*","1dce2230":"# Feature Importance Scores From Model and via RFE","bc5ab075":"##  Converting Some Categorical Variables to Numeric Ones","6af5a453":"#  Separating Train and Test Sets Again","c6a4cbf5":"# Dropping Unnecessary Features","006e7903":"# Merging Train and Test Sets","1cdf6183":"**Note**: the model training piece above generates the weird job termination issues, if running the notebook here at Kaggle; therefore you could not see the final dataframe with each model scoring rendered properly in the views above. You can generate it, however, if you download the code and the competion data to run it locally.","ecff1002":"As our next step, we are going to train a set of RF models that utilize only the tiny subset of features selected from the model via embedded feature selection algorithm.","a324df6f":"We will find that\n\n- almost every model we trained (except for the set of models to use the tiny subset of features selected  via embedded feature  selection method)\n- feature importance scores calculated by RF algorithm directly are really misleading, and they do not reflect the actual feature importance for the model trained\n- the reason why embedded feature selection led to a poor result (see the chart above) can be explained by the fact RF needs reasonable variance in the feature space (to train diverse set of poor predictors - individual decision tree model estimators - to represent the  regression problem  complexity in a proper way)\n- among the bunch of models we trained, the best performance on the CV testing sets (withe testing score of 0.127509) was demonstrated by the model that used top 50 RFE features and n_trees = 800\n- in this case, the appropriate feature selection not only improved the interpretability of our model but also added the edge in its forecasting performance (vs. the set of RF models that used the entire set of features in training)","c3518794":"# Imputing Missing Values","6e2cad73":"# Load Initial Data","d789e52d":"# Creating the Feature Importance Dataframe\n\nLet's build the initial version of the dataframe to collect feature importance scores calculated by different methods.\n\nInitially, it will contain the RFE scores as well as the scores calculated by the model directly.\n\nIn future, we will extend it with the additional metrics (permutational feature importance, and drop-column feature importance).","bc27b31f":"# Numeric Features to be Treated as Categories\n\nBelow we cast some numeric features to categories based on the logical assessment of the feature essence","b8bd683d":"# Advanced House Pricing Prediction: Feature Importance Case Study","d3ed6113":"# Modelling and Feature Selection Pre-Requisite","5a346ade":"# Embedded Feature Selection: Selecting Features From a Model","886ae478":"# Drop-Column Importance\n\nDrop-column importance can be done simply by dropping each column and re-training, which you already know how to do if you\u2019ve trained a model.","9cdf3f82":"# What\u2019s In Your Toolbox?\n\nBefore we jump on the practical implementation of instrumental Feature Importance calculation and Feature selection techniques, I would like to briefly outline the essence of each of the methods we are going to use down the road.\n\nIn real machine-learning problems, tackling the curse of dimensionality as well as increasing the model interpretability are translated to solid feature selection techniques.\n\nApart from the filtering-based methods (correlation- or chi-square-based) or PCA (the latter is mostly applicable to linear regression problems), there is a set of analytical (computational) feature selection methods. \n\nIn the subsections below, we will review the available analytical feature selection options, along with the technical implementation details for them in Python.\n\n## Wrapper-based methods\nWrapper-based methods treat the selection of a set of features as a search problem. \n\n**RFE** and its implementation in sckit-learn can be referred to as one of the good options to go on with it, for example. \nThe utility function to benefit from RFE feature importance score calculations is provided below\n\nhttps:\/\/gist.github.com\/gvyshnya\/7349198e74b4c5fc6caad18ac150ff07\n\nOther options in the wrapper-based feature selection domain are Backward Elimination, Forward Selection, and Bidirectional Elimination.\n\n## Embedded methods\nEmbedded methods are applicable to ML algorithms that have built-in feature selection methods. For instance, **Lasso, Random Forest, or lightgbm** have them.\n\nFrom the technical standpoint, feature selection with the embedded methods relies on scikit-learn\u2019s SelectFromModel. You can see it in action with the demo code snippet below\n\nhttps:\/\/gist.github.com\/gvyshnya\/de775c04f7f752eb66c1d40ed40bcb05\n\n## Permutation method\n\nPut simply, this method changes the data in a column and then tests how much that affects the model accuracy. If changing the data in a column drops the accuracy, then that column is assumed to be important.\n\nYou can benefit from the out-of-the box scikit-learn utility to facilitate permutable feature importance calculation. I wrapped it up in a utility function below\nhttps:\/\/gist.github.com\/gvyshnya\/abe6c06767922f8762bd288c2d897ce5\n\n## Drop-Column Method\n\nThis method is focused on measuring performance of a ML model on a full-feature dataset of predictors vs. the set of smaller datasets (each one of them to drop exactly one feature from the original fully featured dataset). In such a way, the difference between the performance of the model on the original dataset and every dataset with a dropped feature will be a metric of the dropped feature importance for the model performance.\n\nI could not find the stable Pythonic implementation of such a feature selection \/ feature importance measurement therefore I ended up with the custom implementation below\n\nhttps:\/\/gist.github.com\/gvyshnya\/513080f611491b8baa08cc1bf6987144\n\n## Several Practical Considerations\nWhen deciding which feature selection method is the best one for your specific problem, you should keep in mind the points below\n- there is no a single silver bullet-proof method of feature selection that worsks well for each and every project - typically, you will always have to undertake several feature selection experiments (using different methods) to figure out which one leads to the ML model with the highest performance metric score\n- all methods except filter-based ones have its computational time tall, and it may take too much time to go through them appropriately for large datasets\n- embedded methods sometimes introduce the confusion (or even a misinterpretation on the feature importance) as the feature importance scores are often calculated separately from the model training\n\nWith that said, let's get back to coding.\n","f7183164":"Now we are going to display and compare the results of scoring each of the trained model in the tabular view below","fa76e478":"# Merging All Feature Importance Metrics Into a Single Results Dataframe\n\nNow we are going to add *permutation_importance* and *drop_col_importance* columns to *all_ranks* dataframe as well as calculate the average (mean) feature importance score across all of 4 methods.","b6d9145d":"# Label Encoding The Categorical Variables","bbedc537":"In the next series of experiemnts, we are going to train a number of RF models that use only top 50 features as ranked by **RFE** feature importance\/feature selection algorithm.","71914795":"# Dropping Outliers From Training Set","ac385605":"# Permutable Feature Importance","5fd2a007":"## Binning the Rare Category Values","9e6ec8f3":"## Creating New Features","307a171e":"# Setting Model Data and Log Transforming the Target","b25c1f42":"# Transforming The Skewed Features","69a6b1f1":"# Initial Data Transformation: Dropping Id col\n\nStarting this section and down the activities in **'Setting Model Data and Log Transforming the Target'** section below, we are going to reuse the feature enginering steps defined, justified and explained in https:\/\/www.kaggle.com\/datafan07\/beginner-eda-with-feature-eng-and-blending-models","9723615f":"Now we are ready to train one more set of RF models that use top 50 features selected by **permutation method**","3ba9efec":"# Final Check of The Data Before Feature Selection and ML Experiments","e6c7cba2":"# Feature Importance in the Age of Interpretable AI\/ML\n\nIn the recent years, we see increasing demand in interpretable AI\/ML. Human decision-makers would like to trust their AI-based decision support on the ground of rationale rather than via religious belief in what AI system calculates\/suggests\/forecasts.\n\nIt was quite easy in good old days when the stage was preoccupied by the easily interpretable ML algorithms (like Linear regression, polynomial regression, logistic regression, CART-based decision trees etc.).\n\nUnfortunately such algorithms lacked accuracy in many real-world business forecasting and decision support scenarios. It resulted in the advent of highly accurate and complicated algorithms (starting from Random Forests through Gradient Boosting Machine-like algorithms up to the wide spectrum of Neural Networks of the second generation). \n\nAs everything in this world, however, the accuracy came at a price. There was no more easy way to interpret the decision-making flow of such AI\/ML algorithms in a human-friendly and rational way.\n\nOne of the early attempts to address the challenge was adding the supplementary capability to calculate the feature importance scores by some of the modern algorithms (this is featured, for instance, by Random Forest, GBM, or lightgbm).\n\nThe poor side of the feature importance scores is that they are sometimes confusing \/ misleading due to the fact they are calculated separately from the ML model training itself.\n\nSuch a collision gave a birth to several analytical algorithms to calculate the feature importance \/ do the feature selection for ML models. As opposed to the classical statistical (filtering) approaches (where feature importance is determined on a basis of a certain statistical metric, whether it is a Pierson correlation or Chi Square), such techniques embrace a series of model training experiments under certain feature space tweaks. In such a way, they relatively score the importance of each feature for a specific model to train.\n\nIn this notebook, we are going to review and implement such feature selection \/ feature importance detection methods. They all will be useful in the strive to build the industrial culture of interpretable AI\/ML. \n\nWith this direction, we are on a par with the industry giant like Google (who recently launched the services of Explainable AI \u2013 see https:\/\/cloud.google.com\/explainable-ai).\n\n**Note**: in addition to building comprehensive interpretable ML models, the relevant feature selection will also help you to handle other ML challenges as follows\n- Dropping the garbage (non-informative) features from your modelling pipeline\n- Tackling the curse of dimensionality as well as minimizing the impact of the model overfitting\n- Improve accuracy\/performance of our model forecasting\n\nNow, we are ready to get our hands dirty in the code, to orchestrate a solid ML solution that benefits from industrial feature selection techniques.\n"}}