{"cell_type":{"4445cd33":"code","edf4c7e3":"code","86de1c7a":"code","39829674":"code","820e4837":"code","386753f3":"code","79b4c8e3":"code","0fd86606":"code","e90219de":"code","34079d8c":"code","2d3fb972":"code","633ae49c":"code","7ad83ab3":"code","43681ad0":"code","692d9750":"code","495d089e":"code","dc4cee6f":"code","61143701":"code","072f931b":"code","9ab35ead":"code","2ef49337":"code","a96fc533":"code","67ab899e":"markdown","3367d742":"markdown","a72dde7e":"markdown","8741d5c8":"markdown","58a5aea7":"markdown","f8018d18":"markdown","48f5efff":"markdown","362b9ce6":"markdown","107b25d9":"markdown"},"source":{"4445cd33":"#install the fastai library version 0.7 as used in the fastai course\n!pip install fastai==0.7.0 --no-deps","edf4c7e3":"#import libraries, this code follows the imports from the fastai course\nimport os\n%matplotlib inline\nfrom fastai.imports import *\nfrom fastai.structured import *\n\nfrom pandas_summary import DataFrameSummary\nfrom sklearn.ensemble import  RandomForestClassifier, GradientBoostingClassifier\nfrom IPython.display import display\n\nfrom sklearn import metrics\nnp.random.seed(20190810)","86de1c7a":"#load data, we only need the transactions dataset for now\n#train_id = pd.read_csv('\/kaggle\/input\/train_identity.csv')\ntrain_trans = pd.read_csv('\/kaggle\/input\/train_transaction.csv')\n#test_id = pd.read_csv('\/kaggle\/input\/test_identity.csv')\ntest_trans = pd.read_csv('\/kaggle\/input\/test_transaction.csv')\n#sample_submission = pd.read_csv('\/kaggle\/input\/sample_submission.csv')","39829674":"#the proc_df installed in kaggle was not working properly so copied the definition from my pc\ndef proc_df(df, y_fld=None, skip_flds=None, ignore_flds=None, do_scale=False, na_dict=None,\n            preproc_fn=None, max_n_cat=None, subset=None, mapper=None):\n\n    if not ignore_flds: ignore_flds=[]\n    if not skip_flds: skip_flds=[]\n    if subset: df = get_sample(df,subset)\n    else: df = df.copy()\n    ignored_flds = df.loc[:, ignore_flds]\n    df.drop(ignore_flds, axis=1, inplace=True)\n    if preproc_fn: preproc_fn(df)\n    if y_fld is None: y = None\n    else:\n        if not is_numeric_dtype(df[y_fld]): df[y_fld] = pd.Categorical(df[y_fld]).codes\n        y = df[y_fld].values\n        skip_flds += [y_fld]\n    df.drop(skip_flds, axis=1, inplace=True)\n\n    if na_dict is None: na_dict = {}\n    else: na_dict = na_dict.copy()\n    na_dict_initial = na_dict.copy()\n    for n,c in df.items(): na_dict = fix_missing(df, c, n, na_dict)\n    if len(na_dict_initial.keys()) > 0:\n        df.drop([a + '_na' for a in list(set(na_dict.keys()) - set(na_dict_initial.keys()))], axis=1, inplace=True)\n    if do_scale: mapper = scale_vars(df, mapper)\n    for n,c in df.items(): numericalize(df, c, n, max_n_cat)\n    df = pd.get_dummies(df, dummy_na=True)\n    df = pd.concat([ignored_flds, df], axis=1)\n    res = [df, y, na_dict]\n    if do_scale: res = res + [mapper]\n    return res","820e4837":"#testing for some sort of time order in the data set\nexperiment_set = train_trans.tail(100000)\nlast_20k = experiment_set.tail(20000)\nfirst_80k = experiment_set.head(80000)\nrandom_20k = first_80k.sample(20000)\nexperiment_train = first_80k[~first_80k.TransactionID.isin(random_20k.TransactionID)]\n","386753f3":"def print_score(m,X_train,y_train,X_valid,y_valid):\n    res = {'auc_train' : metrics.roc_auc_score(y_train,m.predict_proba(X_train)[:,1]),\n           'auc_valid' : metrics.roc_auc_score(y_valid,m.predict_proba(X_valid)[:,1])}\n    if hasattr(m, 'oob_score_'): res[\"oob\"] = m.oob_score_\n    print(res)","79b4c8e3":"train_cats(experiment_train)\napply_cats(df=random_20k, trn=experiment_train)\n\nX, y , nas = proc_df(experiment_train, 'isFraud')\nrandom_valid, random_valid_y, _ = proc_df(random_20k,'isFraud', na_dict=nas)\n\n\nm = RandomForestClassifier(n_jobs=-1, n_estimators=10)\n\nm.fit(X, y)\n\nprint_score(m,X,y,random_valid,random_valid_y)","0fd86606":"train_cats(experiment_train)\napply_cats(df=last_20k, trn=experiment_train)\n\nX, y , nas = proc_df(experiment_train, 'isFraud')\n\nlast_valid, last_valid_y, _= proc_df(last_20k,'isFraud', na_dict=nas)\n\n\nm = RandomForestClassifier(n_jobs=-1, n_estimators=10)\n\nm.fit(X, y)\n\nprint_score(m,X,y, last_valid, last_valid_y)","e90219de":"#create a data frame from the TransactionDT columns of the train and test set and add an index to make plotting easy.\ndf1 = pd.DataFrame(train_trans.TransactionDT)\ndf1['datasource'] = \"train\"\ndf2 = pd.DataFrame(test_trans.TransactionDT)\ndf2['datasource'] = \"test\"\ndf = pd.concat([df1,df2])\ndf = df.reset_index()","34079d8c":"#create a sampled scatterplot showing the distribution\nsns.scatterplot(x='TransactionDT', y='index',hue='datasource', data=df.sample(100000),edgecolors='none',marker='.')\nplt.axvline(df1.TransactionDT.max())\nplt.axvline(df2.TransactionDT.min())\nplt.title(\"Ranges of the TransactionDT values in the two data sets with lines showing max and min of the two datasets\")","2d3fb972":"#need to delete objects from memory in order to free up memory for model training.\ndel[df1,df2,df,last_valid,last_valid_y,random_valid,random_valid_y,experiment_set,last_20k,first_80k,random_20k,experiment_train,m]","633ae49c":"tt_train = train_trans.head(500000)\ntt_valid = train_trans.tail(90540)\n\ntrain_cats(tt_train)\napply_cats(df=tt_valid, trn=tt_train)\n\nX, y , nas = proc_df(tt_train, 'isFraud')\n\nvalid, valid_y, _= proc_df(tt_valid,'isFraud', na_dict=nas)\n\napply_cats(df=test_trans, trn=tt_train)\nX_test, _ , _= proc_df(test_trans,na_dict=nas)","7ad83ab3":"del([test_trans,train_trans,tt_train,tt_valid])","43681ad0":"mrf1 = RandomForestClassifier(n_jobs=-1, n_estimators=10)\nmrf1.fit(X, y)\n\n#output predictions and make submission files\nmrf1_sub = pd.DataFrame()\nmrf1_sub['TransactionID']=X_test['TransactionID']\nmrf1_sub['isFraud']=mrf1.predict_proba(X_test)[:,1]\nmrf1_sub.to_csv('mrf1_submission.csv', index=False)\nprint_score(mrf1,X,y, valid, valid_y)","692d9750":"del([mrf1,mrf1_sub])","495d089e":"mrf2 = RandomForestClassifier(n_jobs=-1, n_estimators=10,min_samples_leaf=1000)\nmrf2.fit(X, y)\n\nmrf2_sub = pd.DataFrame()\nmrf2_sub['TransactionID']=X_test['TransactionID']\nmrf2_sub['isFraud']=mrf2.predict_proba(X_test)[:,1]\nmrf2_sub.to_csv('mrf2_submission.csv', index=False)\nprint_score(mrf2,X,y, valid, valid_y)","dc4cee6f":"del([mrf2,mrf2_sub])","61143701":"mgb1 = GradientBoostingClassifier(n_estimators=10)\nmgb1.fit(X, y)\n\nmgb1_sub = pd.DataFrame()\nmgb1_sub['TransactionID']=X_test['TransactionID']\nmgb1_sub['isFraud']=mgb1.predict_proba(X_test)[:,1]\nmgb1_sub.to_csv('mgb1_submission.csv', index=False)\nprint_score(mgb1,X,y, valid, valid_y)","072f931b":"del([mgb1,mgb1_sub])","9ab35ead":"mgb2 = GradientBoostingClassifier(n_estimators=10,min_samples_leaf=1000)\nmgb2.fit(X, y)\n\nmgb2_sub = pd.DataFrame()\nmgb2_sub['TransactionID']=X_test['TransactionID']\nmgb2_sub['isFraud']=mgb2.predict_proba(X_test)[:,1]\nmgb2_sub.to_csv('mgb2_submission.csv', index=False)\nprint_score(mgb2,X,y, valid, valid_y)","2ef49337":"del([mgb2,mgb2_sub])","a96fc533":"outcome = pd.DataFrame({'valid':[0.8273,0.8599,0.8044,0.8105],\n          'public':[0.8506,0.8826, 0.8386,0.8424]})\noutcome.plot.scatter(x='valid',y='public')\nplt.title('Performance of different models on public leaderboard and validation set')","67ab899e":"valid score: 0.8044\n\ntest lb score: 0.8368","3367d742":"We see that there is a strong correlation between the auc on the public lb and the valid dataset.\nThis seems to be a reliable validation set for comparing different models.","a72dde7e":"The goal of machine learning excercises is to train an effective generaliser - that is to find some function that is effectively able to map some new samples of data to an unknown target label or value.\n\nIn order to assess the effectiveness of our models we need to use some form of test set. Kaggle provides a test set divided into two parts: public and private. \n\nHowever, we need to avoid over fitting - building models which are too sensitive to patterns in a subset of our data and so do not generalise as well to new data.\n\nIf we simply used the kaggle public test set for our test set we may end up overfitting to that data set and so see a big drop in our model accuracy on the private leaderboard.\nUsing the Kaggle public leaderboard to evaluate our models and feature engineering is also slow and inefficient.\n\nThe solution to these problems is constructing a validation data set from the training data and using this for our models.\nThis validation set needs to be sufficiently similar to the training data set.\n\nThis kernel seeks to investigate different validation data set construction approaches and is based on some of the lessons from Jeremy Howard's Fastai \"Introduction to Machine Learning for Coders\"\n\nI'm used to working in R so a lot of these python libraries are new to me. Any suggestions for improvement are greatly appreciated.","8741d5c8":"valid score: 0.8105\n\ntest lb score: 8424","58a5aea7":"valid score: 0.8273\n\ntest lb score: 0.8506","f8018d18":"We can see that the auc on the random validation set is significantly higher than the auc on the last rows of the dataset - this indicates data leakage.\nIf our test set is on data later than the training set then we need to replicate this in our validation set.\nA simple plot will allow us to see whether this is the case.","48f5efff":"One potential issue in our validation data set is data leakage due to a time component in the data set.\nOur data set has a time based column TransactionDT ","362b9ce6":"We can see that the test set does not overlap the training set and is completely after it in time. \nThis visualisation and the clear differences in the results of our models mean that we should use the last rows of the train set as the validation dataset. \n\nNext step is to check that the prediction AUC is similar for an actual local validation data set and the public leaderboard.\nTo do this we will take the last 90540 rows of the training set as a validation set.\nWe will then train a number of different models on this training set and compare the performance of these models on this validations set and the public leaderboard score. If the validation set is good we would expect a consistent positive correlation.\n\nNB. Our goal in building a validation set is to create a data set that will allow us to reliably identify improvements in our models. We do not need to know what the AUC on the test set will be as much as we need to know if a change we make to our model will result in an improvement to the AUC on the test set.","107b25d9":"valid score: 0.8599\n\ntest lb score: 0.8826"}}