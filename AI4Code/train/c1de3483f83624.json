{"cell_type":{"f71f4184":"code","2cf93be9":"code","88bf1919":"code","8047d861":"code","27d88ef6":"code","c0857510":"code","4a72a513":"code","57d7e267":"code","1a0c5b94":"code","db8bdee7":"code","a5d24ab6":"code","a5fc9d5d":"code","6f38d3ae":"markdown"},"source":{"f71f4184":"import keras\nimport tensorflow as tf\nimport keras.layers as layers\nfrom keras.preprocessing.image import load_img\nfrom keras.preprocessing.image import array_to_img\nimport numpy as np\nfrom math import floor, ceil\nfrom tensorflow.python.ops import math_ops\nfrom tensorflow import math, random, shape\nimport os\nfrom keras.losses import MeanSquaredError, BinaryCrossentropy\nfrom keras.optimizers import Nadam, SGD, Adam, Adamax\nfrom keras.activations import sigmoid\nfrom tensorflow import convert_to_tensor as tens\nfrom keras import backend as K\nfrom cv2 import getGaborKernel as Gabor\nfrom functools import reduce\nfrom matplotlib import pyplot as plt\nfrom math import sqrt\nimport itertools\nimport re\nfrom random import shuffle, seed\nfrom tensorflow.keras.utils import Sequence\nfrom keras.constraints import NonNeg\nfrom keras.regularizers import l1,l2,l1_l2\nfrom keras.initializers import RandomNormal\nimport pandas as pd\nimport tables","2cf93be9":"import os\nimport urllib.request\nimport gzip, shutil\nfrom tensorflow.keras.utils import get_file\n\n\ncache_dir=os.path.expanduser(\"~\/data\")\ncache_subdir=\"hdspikes\"\nprint(\"Using cache dir: %s\"%cache_dir)\n\n# The remote directory with the data files\nbase_url = \"https:\/\/compneuro.net\/datasets\"\n\n# Retrieve MD5 hashes from remote\nresponse = urllib.request.urlopen(\"%s\/md5sums.txt\"%base_url)\ndata = response.read() \nlines = data.decode('utf-8').split(\"\\n\")\nfile_hashes = { line.split()[1]:line.split()[0] for line in lines if len(line.split())==2 }\n\ndef get_and_gunzip(origin, filename, md5hash=None):\n    gz_file_path = get_file(filename, origin, md5_hash=md5hash, cache_dir=cache_dir, cache_subdir=cache_subdir)\n    hdf5_file_path=gz_file_path[:-3]\n    if not os.path.isfile(hdf5_file_path) or os.path.getctime(gz_file_path) > os.path.getctime(hdf5_file_path):\n        print(\"Decompressing %s\"%gz_file_path)\n        with gzip.open(gz_file_path, 'r') as f_in, open(hdf5_file_path, 'wb') as f_out:\n            shutil.copyfileobj(f_in, f_out)\n    return hdf5_file_path\n\n\n\n# Download the Spiking Heidelberg Digits (SHD) dataset\nfiles = [ \"shd_train.h5.gz\", \n          \"shd_test.h5.gz\",\n        ]\n\n\nfor fn in files:\n    origin = \"%s\/%s\"%(base_url,fn)\n    hdf5_file_path = get_and_gunzip(origin, fn, md5hash=file_hashes[fn])\n    print(hdf5_file_path)","88bf1919":"origin = \"%s\/%s\"%(base_url,\"shd_train.h5.gz\")\nhdf5_file_path = get_and_gunzip(origin, \"shd_train.h5.gz\", md5hash=file_hashes[fn])\ntrain_filename = hdf5_file_path\ntrain_ds = {\"name\": train_filename, \"fileh\": tables.open_file(train_filename, mode='r')}\ntrain_ds[\"units\"] = train_ds['fileh'].root.spikes.units\ntrain_ds[\"times\"] = train_ds['fileh'].root.spikes.times\ntrain_ds[\"labels\"] = train_ds['fileh'].root.labels\n\n\norigin = \"%s\/%s\"%(base_url,\"shd_test.h5.gz\")\nhdf5_file_path = get_and_gunzip(origin, \"shd_test.h5.gz\", md5hash=file_hashes[fn])\ntest_filename = hdf5_file_path\ntest_ds = {\"name\": train_filename, \"fileh\": tables.open_file(test_filename, mode='r')}\ntest_ds[\"units\"] = test_ds['fileh'].root.spikes.units\ntest_ds[\"times\"] = test_ds['fileh'].root.spikes.times\ntest_ds[\"labels\"] = test_ds['fileh'].root.labels","8047d861":"# At this point we can visualize some of the data\n\nfileh = tables.open_file(hdf5_file_path, mode='r')\nunits = fileh.root.spikes.units\ntimes = fileh.root.spikes.times\nlabels = fileh.root.labels\n\n# This is how we access spikes and labels\nindex = 0\nprint(\"Times (ms):\", times[index])\nprint(\"Unit IDs:\", units[index])\nprint(\"Label:\", labels[index])\n\n\n# A quick raster plot for one of the samples\nimport matplotlib.pyplot as plt\n\nfig = plt.figure(figsize=(16,4))\nidx = np.random.randint(len(times),size=3)\nfor i,k in enumerate(idx):\n    ax = plt.subplot(1,3,i+1)\n    ax.scatter(times[k],700-units[k], color=\"k\", alpha=0.33, s=2)\n    ax.set_title(\"Label %i\"%labels[k])\n    ax.axis(\"off\")\nplt.show()","27d88ef6":"while True:\n    i+=1\n    if max(train_ds[\"times\"][i]) > 1.3:\n        plt.scatter(train_ds[\"times\"][i], 700-train_ds[\"units\"][i], color=\"k\", alpha=0.33, s=2)\n        break","c0857510":"plt.figure(figsize=(20,10))\n\nplt.subplot(3,1,1)\nplt.scatter(train_ds[\"times\"][i], 700-train_ds[\"units\"][i], color=\"k\", alpha=0.33, s=2)\nplt.xlim((0,1.5))\n\nplt.subplot(3,1,2)\nplt.scatter((1000*train_ds[\"times\"][i]).astype('int'), 700-train_ds[\"units\"][i], color=\"k\", alpha=0.33, s=2)\nplt.xlim((0,1500))\n\nplt.subplot(3,1,3)\nmatrix = np.zeros((700,1500))\nmatrix[700-train_ds[\"units\"][i]-1, (1000*train_ds[\"times\"][i]).astype('int')] = 1\nplt.imshow(matrix, cmap='binary', vmin=0, vmax=1, aspect='auto', origin='lower')\nplt.xlim((0,1500))\n\nprint(np.sqrt(np.mean(np.sum(((1000*train_ds[\"times\"][i]).astype('int') - 1000*train_ds[\"times\"][i])**2))))","4a72a513":"os.makedirs('..\/working\/shd')","57d7e267":"from PIL import Image  ","1a0c5b94":"def shd_to_images(processor, ds, out_dirc):\n    for i in range(ds[\"times\"].shape[0]):\n        direc = out_dirc + \"\/\" + str(ds[\"labels\"][i])\n        if not os.path.exists(direc): os.makedirs(direc)\n        new_filename = direc + \"\/\" + str(i) + \".jpeg\"\n        img = Image.fromarray(processor(ds[\"times\"][i], ds[\"units\"][i]))\n        if img.mode != 'RGB':\n            img = img.convert('RGB')\n        img.save(new_filename)\n        print('Converted:', new_filename)","db8bdee7":"def convert_spikes_to_matrix(time, unit, nTime=1400, nUnits=700):\n    matrix = np.zeros((nUnits,nTime))\n    matrix[nUnits-1-unit, (1000*time).astype('int')] = 255\n    return matrix#np.stack([matrix, np.zeros((nUnits,nTime)),np.zeros((nUnits,nTime))], axis=-1)","a5d24ab6":"def run_convert(ds, path):\n    os.makedirs(path)\n    shd_to_images(convert_spikes_to_matrix, ds, path)","a5fc9d5d":"run_convert(train_ds,  '..\/working\/shd\/train')\nrun_convert(test_ds,  '..\/working\/shd\/test')\nshutil.make_archive(\"..\/working\/shd\", 'zip', \"..\/working\/shd\")\nshutil.rmtree(\"..\/working\/shd\")","6f38d3ae":"# Transform data to dataset"}}