{"cell_type":{"1b8de867":"code","5a9a3208":"code","ec302d42":"code","5cf71e23":"code","12d843eb":"code","169b5e4e":"code","036202db":"code","0bb6714c":"code","d93410b3":"code","e4c25a71":"code","9687b8f5":"code","133198cc":"code","a2a7dcda":"code","28f2aec3":"code","aeb465d2":"code","44ecaa7f":"code","4151486f":"code","f2093107":"code","3c2d6ff3":"code","bab549da":"code","2e9e50cf":"code","b32003ea":"code","f8284d50":"code","c1676012":"code","a56d62e7":"code","8aeb40a1":"code","e3f7a171":"code","ff732e4a":"code","56d51449":"code","7c532ed9":"code","21334148":"code","8f8c5c6e":"code","90ab4417":"code","cbbd863a":"code","131a8749":"code","56cfe7fb":"code","c727ab0b":"code","70832015":"code","23ecb284":"markdown","648224c6":"markdown","feab7eec":"markdown","2596a4e1":"markdown","f1ec4105":"markdown","2e7d75b4":"markdown","375bac7c":"markdown","63cc5367":"markdown","54edf7ee":"markdown","cbde5cd0":"markdown","11ac536d":"markdown","9ab2cd0b":"markdown","2c50b11a":"markdown","33ed17d8":"markdown","ae096267":"markdown","442475c4":"markdown","7acd41a7":"markdown","879a35db":"markdown","d695e030":"markdown","f48d8b4a":"markdown","9ad3bc10":"markdown","252d57e5":"markdown","138c979c":"markdown","f6dcebf7":"markdown"},"source":{"1b8de867":"import os\nimport re\nimport json\nimport string\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nimport plotly.graph_objects as go\nimport plotly.express as px\n\nfrom wordcloud import WordCloud\nfrom collections import Counter\nfrom functools import partial\n\nfrom sklearn.feature_extraction.text import CountVectorizer\n\nfrom nltk.corpus import stopwords\nstoplist = stopwords.words('english')\n\npd.set_option('display.max_rows', None)\npd.set_option('display.max_columns', None)","5a9a3208":"TRAIN_DIR = \"..\/input\/coleridgeinitiative-show-us-the-data\/train\"\nTEST_DIR = \"..\/input\/coleridgeinitiative-show-us-the-data\/test\"","ec302d42":"train = pd.read_csv(\"..\/input\/coleridgeinitiative-show-us-the-data\/train.csv\")\nsample_sub = pd.read_csv(\"..\/input\/coleridgeinitiative-show-us-the-data\/sample_submission.csv\")\ntrain.head()","5cf71e23":"train.info()","12d843eb":"print(f\"Number of Unique Publication Titles is {train['pub_title'].nunique()}\")","169b5e4e":"print(f\"Number of Unique Dataset Titles is {train['dataset_title'].nunique()}\")","036202db":"print(f\"Number of Unique Dataset Labels is {train['dataset_label'].nunique()}\")","0bb6714c":"print(f\"Number of Unique Cleaned Dataset Labels is {train['cleaned_label'].nunique()}\")","d93410b3":"train['publication_title_words'] = train['pub_title'].apply(lambda x: len(x.split()))\ntrain['dataset_title_words'] = train['dataset_title'].apply(lambda x: len(x.split()))\ntrain['dataset_label_words'] = train['dataset_label'].apply(lambda x: len(x.split()))\ntrain['cleaned_label_words'] = train['cleaned_label'].apply(lambda x: len(x.split()))","e4c25a71":"fig = px.histogram(train, x=\"publication_title_words\",\n                   marginal=\"box\") # or violin, rug)\nfig.update_layout(go.Layout(template= \"plotly_dark\",title = 'Length of Publication Titles' , xaxis = dict(title = 'Length'), yaxis = dict(title = 'Count')))\nfig.show()","9687b8f5":"fig = px.histogram(train, x=\"dataset_title_words\",\n                   marginal=\"box\") # or violin, rug)\nfig.update_layout(go.Layout(template= \"plotly_dark\",title = 'Length of Dataset Titles' , xaxis = dict(title = 'Length'), yaxis = dict(title = 'Count')))\nfig.show()","133198cc":"fig = px.histogram(train, x=\"dataset_label_words\",\n                   marginal=\"box\") # or violin, rug)\nfig.update_layout(go.Layout(template= \"plotly_dark\",title = 'Length of Dataset Labels' , xaxis = dict(title = 'Length'), yaxis = dict(title = 'Count')))\nfig.show()","a2a7dcda":"publication_title_list = []\nfor publication_title in train['pub_title'].tolist():\n    words = publication_title.split()\n    publication_title_list.extend(words)","28f2aec3":"publication_title_word_freq = Counter(publication_title_list)","aeb465d2":"sorted_word_freq = sorted(publication_title_word_freq.items(), key=lambda pair: pair[1], reverse=True)\nsorted_word_freq[:5]","44ecaa7f":"dataset_title_list = []\nfor dataset_title in train['dataset_title'].tolist():\n    words = dataset_title.split()\n    dataset_title_list.extend(words)","4151486f":"dataset_title_word_freq = Counter(dataset_title_list)","f2093107":"sorted_word_freq = sorted(dataset_title_word_freq.items(), key=lambda pair: pair[1], reverse=True)\nsorted_word_freq[:5]","3c2d6ff3":"dataset_label_list = []\nfor dataset_label in train['dataset_label'].tolist():\n    words = dataset_label.split()\n    dataset_label_list.extend(words)","bab549da":"dataset_label_word_freq = Counter(dataset_label_list)","2e9e50cf":"sorted_word_freq = sorted(dataset_label_word_freq.items(), key=lambda pair: pair[1], reverse=True)\nsorted_word_freq[:5]","b32003ea":"# Define a function to plot word cloud\ndef plot_cloud(wordcloud):\n    # Set figure size\n    plt.figure(figsize=(40, 30))\n    # Display image\n    plt.imshow(wordcloud) \n    # No axis details\n    plt.axis(\"off\");","f8284d50":"wordcloud = WordCloud(width = 1000, height = 500, random_state=1, colormap='twilight', \n                      font_path='..\/input\/all-elon-musks-tweets\/acetone_font.otf', collocations=False)","c1676012":"plot_cloud(wordcloud.generate_from_frequencies(publication_title_word_freq))","a56d62e7":"plot_cloud(wordcloud.generate_from_frequencies(dataset_title_word_freq))","8aeb40a1":"plot_cloud(wordcloud.generate_from_frequencies(dataset_label_word_freq))","e3f7a171":"c_vec = CountVectorizer(stop_words=stoplist, ngram_range=(2,3))\n# matrix of ngrams\nngrams = c_vec.fit_transform(train['pub_title'])\n# count frequency of ngrams\ncount_values = ngrams.toarray().sum(axis=0)\n# list of ngrams\nvocab = c_vec.vocabulary_\ndf_ngram = pd.DataFrame(sorted([(count_values[i],k) for k,i in vocab.items()], reverse=True)\n            ).rename(columns={0: 'frequency', 1:'bigram\/trigram'})\n\n\ntop10_ngrams_freq = df_ngram.head(10)['frequency'].tolist()\ntop10_ngrams = df_ngram.head(10)['bigram\/trigram'].tolist()\n\nfig = go.Figure(data=[go.Table(header=dict(values=['Bigram\/Trigram', 'Count'], fill_color='yellow', line_color='darkslategray'),\n                 cells=dict(values=[top10_ngrams, top10_ngrams_freq], fill_color='lavender', line_color='darkslategray'))\n                     ])\nfig.show()","ff732e4a":"colors = ['rgb(160, 50, 168)']*len(top10_ngrams_freq)\n\ntrace = go.Bar(\n                x = top10_ngrams_freq[::-1],\n                y = top10_ngrams[::-1],\n                marker = dict(color = colors,\n                              line=dict(color='rgb(0,0,0)',width=1.5)),\n                text=top10_ngrams_freq[::-1], textposition='outside', orientation='h')\nlayout = go.Layout(template= \"plotly_dark\",title = 'TOP 10 BIGRAMS \/ TRIGRAMS IN PUBLICATION TITLE' , xaxis = dict(title = 'Count', automargin=True), yaxis = dict(title = 'Bigram\/Trigram'))\nfig = go.Figure(data = [trace], layout = layout)\nfig.show()","56d51449":"c_vec = CountVectorizer(stop_words=stoplist, ngram_range=(2,3))\n# matrix of ngrams\nngrams = c_vec.fit_transform(train['dataset_title'])\n# count frequency of ngrams\ncount_values = ngrams.toarray().sum(axis=0)\n# list of ngrams\nvocab = c_vec.vocabulary_\ndf_ngram = pd.DataFrame(sorted([(count_values[i],k) for k,i in vocab.items()], reverse=True)\n            ).rename(columns={0: 'frequency', 1:'bigram\/trigram'})\n\n\ntop10_ngrams_freq = df_ngram.head(10)['frequency'].tolist()\ntop10_ngrams = df_ngram.head(10)['bigram\/trigram'].tolist()\n\nfig = go.Figure(data=[go.Table(header=dict(values=['Bigram\/Trigram', 'Count'], fill_color='yellow', line_color='darkslategray'),\n                 cells=dict(values=[top10_ngrams, top10_ngrams_freq], fill_color='lavender', line_color='darkslategray'))\n                     ])\nfig.show()","7c532ed9":"colors = ['rgb(160, 50, 168)']*len(top10_ngrams_freq)\n\ntrace = go.Bar(\n                x = top10_ngrams_freq[::-1],\n                y = top10_ngrams[::-1],\n                marker = dict(color = colors,\n                              line=dict(color='rgb(0,0,0)',width=1.5)),\n                text=top10_ngrams_freq[::-1], textposition='outside', orientation='h')\nlayout = go.Layout(template= \"plotly_dark\",title = 'TOP 10 BIGRAMS \/ TRIGRAMS IN DATASET TITLE' , xaxis = dict(title = 'Count', automargin=True), yaxis = dict(title = 'Bigram\/Trigram'))\nfig = go.Figure(data = [trace], layout = layout)\nfig.show()","21334148":"c_vec = CountVectorizer(stop_words=stoplist, ngram_range=(2,3))\n# matrix of ngrams\nngrams = c_vec.fit_transform(train['dataset_label'])\n# count frequency of ngrams\ncount_values = ngrams.toarray().sum(axis=0)\n# list of ngrams\nvocab = c_vec.vocabulary_\ndf_ngram = pd.DataFrame(sorted([(count_values[i],k) for k,i in vocab.items()], reverse=True)\n            ).rename(columns={0: 'frequency', 1:'bigram\/trigram'})\n\n\ntop10_ngrams_freq = df_ngram.head(10)['frequency'].tolist()\ntop10_ngrams = df_ngram.head(10)['bigram\/trigram'].tolist()\n\nfig = go.Figure(data=[go.Table(header=dict(values=['Bigram\/Trigram', 'Count'], fill_color='yellow', line_color='darkslategray'),\n                 cells=dict(values=[top10_ngrams, top10_ngrams_freq], fill_color='lavender', line_color='darkslategray'))\n                     ])\nfig.show()","8f8c5c6e":"colors = ['rgb(160, 50, 168)']*len(top10_ngrams_freq)\n\ntrace = go.Bar(\n                x = top10_ngrams_freq[::-1],\n                y = top10_ngrams[::-1],\n                marker = dict(color = colors,\n                              line=dict(color='rgb(0,0,0)',width=1.5)),\n                text=top10_ngrams_freq[::-1], textposition='outside', orientation='h')\nlayout = go.Layout(template= \"plotly_dark\",title = 'TOP 10 BIGRAMS \/ TRIGRAMS IN DATASET LABEL' , xaxis = dict(title = 'Count', automargin=True), yaxis = dict(title = 'Bigram\/Trigram'))\nfig = go.Figure(data = [trace], layout = layout)\nfig.show()","90ab4417":"def clean_text(txt):\n    return re.sub('[^A-Za-z0-9]+', ' ', str(txt).lower())","cbbd863a":"def text_cleaning(text):\n    '''\n    Converts all text to lower case, Removes special charecters, emojis and multiple spaces\n    text - Sentence that needs to be cleaned\n    '''\n    text = ''.join([k for k in text if k not in string.punctuation])\n    text = re.sub('[^A-Za-z0-9]+', ' ', str(text).lower()).strip()\n    \n    return text","131a8749":"def read_append_return(filename, train_files_path=TRAIN_DIR, output='text'):\n    \"\"\"\n    Function to read json file and then return the text data from them and append to the dataframe\n    \"\"\"\n    json_path = os.path.join(train_files_path, (filename+'.json'))\n    headings = []\n    contents = []\n    combined = []\n    with open(json_path, 'r') as f:\n        json_decode = json.load(f)\n        for data in json_decode:\n            headings.append(data.get('section_title'))\n            contents.append(data.get('text'))\n            combined.append(data.get('section_title'))\n            combined.append(data.get('text'))\n    \n    all_headings = ' '.join(headings)\n    all_contents = ' '.join(contents)\n    all_data = '. '.join(combined)\n    \n    if output == 'text':\n        return all_contents\n    elif output == 'head':\n        return all_headings\n    else:\n        return all_data","56cfe7fb":"train['text'] = train['Id'].apply(read_append_return)\nsample_sub['text'] = sample_sub['Id'].apply(partial(read_append_return, train_files_path=TEST_DIR))","c727ab0b":"temp_1 = [x.lower() for x in train['dataset_label'].unique()]\ntemp_2 = [x.lower() for x in train['dataset_title'].unique()]\ntemp_3 = [x.lower() for x in train['cleaned_label'].unique()]\n\nexisting_labels = set(temp_1 + temp_2 + temp_3)\nid_list = []\nlables_list = []\nfor index, row in sample_sub.iterrows():\n    sample_text = row['text']\n    row_id = row['Id']\n    temp_df = train[train['text'] == text_cleaning(sample_text)]\n    cleaned_labels = temp_df['cleaned_label'].to_list()\n    for known_label in existing_labels:\n        if known_label in sample_text.lower():\n            cleaned_labels.append(clean_text(known_label))\n    cleaned_labels = [clean_text(x) for x in cleaned_labels]\n    cleaned_labels = set(cleaned_labels)\n    lables_list.append('|'.join(cleaned_labels))\n    id_list.append(row_id)","70832015":"submission = pd.DataFrame()\nsubmission['Id'] = id_list\nsubmission['PredictionString'] = lables_list\nsubmission.to_csv('submission.csv', index=False)\nsubmission.head(5)","23ecb284":"<span style=\"color: #0087e4; font-family: Segoe UI; font-size: 2.3em; font-weight: 300;\">Baseline Submission<\/span>","648224c6":"<span style=\"color: #000508; font-family: Segoe UI; font-size: 2.0em; font-weight: 300;\">Dataset Label<\/span>","feab7eec":"<span style=\"color: #000508; font-family: Segoe UI; font-size: 2.0em; font-weight: 300;\">Dataset Title<\/span>","2596a4e1":"![](https:\/\/cusp.nyu.edu\/wp-content\/uploads\/2018\/09\/CI_horizontal.png)","f1ec4105":"<span style=\"color: #0087e4; font-family: Segoe UI; font-size: 2.3em; font-weight: 300;\">Let's Check the Distribution of words<\/span>","2e7d75b4":"<span style=\"color: #0087e4; font-family: Segoe UI; font-size: 2.3em; font-weight: 300;\">Basic Exploration<\/span>","375bac7c":"<span style=\"color: #0087e4; font-family: Segoe UI; font-size: 2.3em; font-weight: 300;\">Let's Visualize using WordCloud<\/span>","63cc5367":"<span style=\"color: #0087e4; font-family: Segoe UI; font-size: 2.3em; font-weight: 300;\">Import Packages<\/span>","54edf7ee":"Code taken from [https:\/\/www.kaggle.com\/prashansdixit\/coleridge-initiative-eda-baseline-model](https:\/\/www.kaggle.com\/prashansdixit\/coleridge-initiative-eda-baseline-model)","cbde5cd0":"<span style=\"color: #000508; font-family: Segoe UI; font-size: 2.0em; font-weight: 300;\">Dataset Label<\/span>","11ac536d":"<p style='text-align: center;'><span style=\"color: #000508; font-family: Segoe UI; font-size: 2.6em; font-weight: 300;\">Coleridge Initiative - Show US the Data<\/span><\/p>\n<p style='text-align: center;'><span style=\"color: #000508; font-family: Segoe UI; font-size: 2.6em; font-weight: 300;\">Let's See the Data\ud83d\udd25<\/span><\/p>","9ab2cd0b":"<span style=\"color: #000508; font-family: Segoe UI; font-size: 2.0em; font-weight: 300;\">Publication Title<\/span>","2c50b11a":"<span style=\"color: #000508; font-family: Segoe UI; font-size: 2.0em; font-weight: 300;\">Publication Title<\/span>","33ed17d8":"![Upvote!](https:\/\/img.shields.io\/badge\/Upvote-If%20you%20like%20my%20work-07b3c8?style=for-the-badge&logo=kaggle)","ae096267":"<span style=\"color: #000508; font-family: Segoe UI; font-size: 2.0em; font-weight: 300;\">Helper Functions<\/span>","442475c4":"<span style=\"color: #000508; font-family: Segoe UI; font-size: 2.0em; font-weight: 300;\">Helper Function<\/span>","7acd41a7":"<span style=\"color: #0087e4; font-family: Segoe UI; font-size: 2.3em; font-weight: 300;\">Load the Dataframe<\/span>","879a35db":"<span style=\"color: #0087e4; font-family: Segoe UI; font-size: 2.3em; font-weight: 300;\">Now let's see the most frequent words<\/span>","d695e030":"<span style=\"color: #000508; font-family: Segoe UI; font-size: 2.0em; font-weight: 300;\">Dataset Title<\/span>","f48d8b4a":"<span style=\"color: #000508; font-family: Segoe UI; font-size: 2.0em; font-weight: 300;\">Dataset Label<\/span>","9ad3bc10":"<span style=\"color: #000508; font-family: Segoe UI; font-size: 2.0em; font-weight: 300;\">Publication Title<\/span>","252d57e5":"<span style=\"color: #000508; font-family: Segoe UI; font-size: 2.0em; font-weight: 300;\">Dataset Title<\/span>","138c979c":"<span style=\"color: #0087e4; font-family: Segoe UI; font-size: 2.3em; font-weight: 300;\">Most Common Bigrams\/Trigrams<\/span>","f6dcebf7":"<span style=\"color: #000508; font-family: Segoe UI; font-size: 2.0em; font-weight: 100;\">It is quite indicative that we have a problem of stopwords here<\/span>"}}