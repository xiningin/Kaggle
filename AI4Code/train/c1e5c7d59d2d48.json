{"cell_type":{"07ffdc45":"code","19006c68":"code","cff2268d":"code","f775dff2":"code","8fc15617":"code","ed8f4ae0":"code","494aeb9f":"code","505e8be8":"code","76353d84":"code","6d212df0":"code","7d1887a9":"markdown","4e9cccf8":"markdown","24c995ac":"markdown","0af5079a":"markdown","8bbab717":"markdown"},"source":{"07ffdc45":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","19006c68":"#Importing the cross-sectional CSV data from the OASIS (Alzheimers) Study: \ncrossSectionalMRIData = \"\/kaggle\/input\/oasis_cross-sectional.csv\"\ncrossSectionalMRI = pd.read_csv(\"..\/input\/mri-and-alzheimers\/oasis_cross-sectional.csv\")\ncrossSectionalMRI.head()","cff2268d":"#Importing the Linear Regression Package from Scikit Learn.  \nfrom sklearn.linear_model import LinearRegression \n\nAgeAndCDRLinearReg = LinearRegression()\n\n#Because there are a number of NaN values in the CDR column, all of the subjects with NaN CDR values will be dropped from the following correlation. \ncrossSectionalMRI.dropna(subset = [\"CDR\",\"SES\",\"Educ\"], axis = 0, inplace = True)\ncrossSectionalMRI.head()\n\nAge = crossSectionalMRI[['Age']]\nCDRScores = crossSectionalMRI[['CDR']]\n\nAgeAndCDRLinearRegModel = AgeAndCDRLinearReg.fit(Age,CDRScores)\n\nAgeAndCDRLinearRegModel.score(Age,CDRScores)","f775dff2":"import seaborn as sns \nfrom matplotlib import pyplot as plt\nsns.regplot(x='Age',y='CDR',data=crossSectionalMRI)\nplt.ylim(0)","8fc15617":"#Let's see what the residuals plot from this model reveals to determine whether a non-linear model would be more revealing:\n\nsns.residplot(crossSectionalMRI['Age'],crossSectionalMRI['CDR'])\n\n#Judging by the above and below plots, it seems like another form of Regression more appropriate for discrete target variables (like Logistic)\n#would be more appropriate. Let's try that approach to unravel the relationship between age, SES, Education, and CDR scores.\n#Afterwards, let's see if a K-nearest neighbor's model can be built to\n#predict whether a subject would depict behaviors and cognition typical of a 0, 0.5, or 1 CDR-scored individual based on \n#multiple feature variables: ","ed8f4ae0":"#Let's import some libraries useful for Logistic Regression with Train and Test splitting:\nimport pandas as pd\nimport pylab as pl\nimport numpy as np\nimport scipy.optimize as opt\nfrom sklearn import preprocessing\n%matplotlib inline \nimport matplotlib.pyplot as plt\n\n#Let's check the datatype of the target variable column (CDR): \nbefore = crossSectionalMRI.dtypes\nbefore\n#This column contains data of the type: float64. Let's change that for scikit learn compatability. \n\n#Let's convert the target data type to integer (as required by scikit learn): \ncrossSectionalMRI['CDR'] = crossSectionalMRI['CDR'].astype(int)\n\n#Let's check to ensure the datatype was correctly changed: \nafter = crossSectionalMRI.dtypes\nafter\n\n#Let's convert the Pandas dataframe above into two numpy arrays for more ease of use with scikit learn functions (train\/test splitting, etc.): \ncrossSectionalMRIFeatures = np.asarray(crossSectionalMRI[['Age','SES','Educ']])\ncrossSectionalMRITarget = np.asarray(crossSectionalMRI['CDR'])\n\n#Next, let's split the whole dataset into training and testing sets for higher validity: \nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(crossSectionalMRIFeatures, crossSectionalMRITarget, test_size=0.2, random_state=4)\n\n#Let's build the Multiple Logistic Regression Model using the training sets \n#and compute some relevant metrics and, perhaps, make a few predictive statements: \n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import confusion_matrix\ncrossSectionalMRILogistic = LogisticRegression(C=0.01, solver='newton-cg', multi_class='multinomial').fit(X_train,y_train)\ncrossSectionalMRILogistic\n\n#Finally, let's make a few predictions using this model and the test set, as well as the probability of each of the class targets (0, 0.5, and 1 CDR Scores): \nLogisticAlzhemiersCDRScorePreds = crossSectionalMRILogistic.predict(X_test) \nLogisticAlzhemiersCDRScorePreds","494aeb9f":"LogisticAlzheimersCDRScoreProbas = crossSectionalMRILogistic.predict_proba(X_test)\nLogisticAlzheimersCDRScoreProbas","505e8be8":"#First, let's look at the Jaccard Similarity score: \nfrom sklearn.metrics import jaccard_score\ny_test\nLogisticAlzhemiersCDRScorePreds\njaccard_score(y_test, LogisticAlzhemiersCDRScorePreds)","76353d84":"#Next, let's take a look at the specifics of the issue with the Logistic Model here with a Confusion Matrix: \nfrom sklearn.metrics import classification_report, confusion_matrix\n\n#The following chunk of code (the rest of this code cell)\n#was borrowed from the Coursera IBM Professional Certificate Course on Machine Learning (Course 8):\n\nimport itertools\ndef plot_confusion_matrix(cm, classes,\n                          normalize=False,\n                          title='Confusion matrix',\n                          cmap=plt.cm.Blues):\n    \"\"\"\n    This function prints and plots the confusion matrix.\n    Normalization can be applied by setting `normalize=True`.\n    \"\"\"\n    if normalize:\n        cm = cm.astype('float') \/ cm.sum(axis=1)[:, np.newaxis]\n        print(\"Normalized confusion matrix\")\n    else:\n        print('Confusion matrix, without normalization')\n\n    print(cm)\n\n    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n    plt.title(title)\n    plt.colorbar()\n    tick_marks = np.arange(len(classes))\n    plt.xticks(tick_marks, classes, rotation=45)\n    plt.yticks(tick_marks, classes)\n\n    fmt = '.2f' if normalize else 'd'\n    thresh = cm.max() \/ 2.\n    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n        plt.text(j, i, format(cm[i, j], fmt),\n                 horizontalalignment=\"center\",\n                 color=\"white\" if cm[i, j] > thresh else \"black\")\n\n    plt.tight_layout()\n    plt.ylabel('True label')\n    plt.xlabel('Predicted label')\nprint(confusion_matrix(y_test, LogisticAlzhemiersCDRScorePreds, labels=[1,0]))\n\nAlzheimersLogisticConfusionMatrix = confusion_matrix(y_test, LogisticAlzhemiersCDRScorePreds, labels=[1,0.5,0])\nnp.set_printoptions(precision=2)\n\n\n# Plot non-normalized confusion matrix\nplt.figure()\nplot_confusion_matrix(AlzheimersLogisticConfusionMatrix, classes=['CDR=1','CDR=0.5','CDR=0'], normalize= False,  title='Alzheimers Logistic CDR Score Confusion Matrix')","6d212df0":"#First, let's make the necessary imports to implement the K Nearest Neighbors Classification Algorithm with scikit learn and determine \n#the value of K resulting in the highest accuracy classification score: \nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn import metrics \n\n\nAlzNeighborsAccuracyArray = np.zeros(19)\nkTestIterationList = range(1,20,1)\nfor k in kTestIterationList:\n\n    AlzCDRNeighbors = KNeighborsClassifier(n_neighbors=k).fit(X_train,y_train)\n    AlzPredictions=AlzCDRNeighbors.predict(X_test)\n    AlzNeighborsAccuracyArray[k-1]=metrics.accuracy_score(y_test,AlzPredictions)","7d1887a9":"Question 1: To what extent is subject age correlated with Alzheimer's disease severity? (measured through the OASIS Study's CDR scale).\nIn other words, how age related is Alzheimer's disease?","4e9cccf8":"Now, let's build a K-Nearest Neighbors Model to predict whether a new subject will have a CDR of 0, 0.5, or 1 based on their Age, \nSES, and Education Level. ","24c995ac":"Note: The correlation between Age and CDR scores is actually extremely low within this sample of Alzheimer's patients. Pearson's Coefficient in this case is only ~0.09.\n\nLet's see what this seemingly small relationship looks like on a plot: \n\n\n\n\n\n","0af5079a":"Interesting! Unlike the Jaccard Index Score, the results from the 3x3 Confusion Matrix above suggest that the Multiple Logistic Regression Model built correctly classified a total of 40\/44 of the train <--> test set comparisons as having a CDR of 0.","8bbab717":"Given that the above probability results indicate an over 60% chance of every prediction landing in the 0 CDR Score category, let's compute a few metrics to determine whether this was a good model for this dataset."}}