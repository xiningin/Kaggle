{"cell_type":{"1b47b5eb":"code","3c357511":"code","0d4c79d4":"code","f51d0ab8":"code","16078833":"code","78f46200":"code","1e4cba5a":"code","c491bedc":"code","4ba5559c":"code","d127f11d":"code","c4f53915":"code","ce91b05d":"code","71100f6a":"code","ccbb57b7":"code","5b0652af":"code","73f13c47":"code","2c5945ae":"code","ce96a91e":"code","0454b14a":"code","cd6f9cf2":"code","426fb7d7":"code","6ed38724":"code","7aca4c0a":"code","d0aec03d":"code","327b6ca2":"code","e8960ec2":"code","aa3facf3":"code","4c716bf2":"code","a0bfe757":"code","45dbbab2":"code","567392bc":"code","94a262aa":"code","fea184cd":"code","9a327440":"code","c7b03c03":"code","6d82baa5":"code","c7950476":"code","73e7c8d2":"code","0d02bff8":"code","4fe6c1f6":"code","b71bf616":"code","57e41bee":"code","574bf2ff":"code","e1788a84":"code","42075c45":"code","a4cc4d54":"code","32b487dd":"code","a7103799":"code","5c3bc2db":"code","fd303ef3":"code","cd56f164":"code","f981f36d":"code","fbf25c04":"code","37bd8918":"code","3e44b2d3":"code","1b293ad4":"code","24bc817f":"code","da805488":"code","d0a044ae":"code","b12de9f6":"markdown","6628ab0a":"markdown","bd11cd00":"markdown","1cda146b":"markdown","c3e08f83":"markdown","4161e9a3":"markdown","b3ea31cd":"markdown","98b9245a":"markdown","9171bcf7":"markdown","aa0878f2":"markdown","6b710a1c":"markdown","c14b9fc6":"markdown","9f83cc70":"markdown","728c3840":"markdown","e796182e":"markdown","8e73c573":"markdown","ba91707d":"markdown","52f6724b":"markdown"},"source":{"1b47b5eb":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","3c357511":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.preprocessing import LabelEncoder,StandardScaler\nimport pandas as pd \nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split,cross_val_score,GridSearchCV,KFold\nfrom sklearn.preprocessing import MinMaxScaler, StandardScaler\nfrom sklearn.decomposition import PCA\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier,AdaBoostClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.linear_model import SGDClassifier\nfrom sklearn.ensemble import VotingClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import roc_curve,confusion_matrix\nfrom sklearn.metrics import roc_auc_score,recall_score\nfrom sklearn.metrics  import accuracy_score,classification_report,roc_auc_score,plot_roc_curve,plot_precision_recall_curve\nimport scipy.stats as stats\nfrom sklearn.decomposition import PCA\nimport warnings\nwarnings.filterwarnings('ignore')","0d4c79d4":"data=pd.read_csv('\/kaggle\/input\/telecom-users-dataset\/telecom_users.csv')","f51d0ab8":"data","16078833":"data.columns","78f46200":"data.drop(columns=['Unnamed: 0','customerID'],inplace=True)","1e4cba5a":"data.info()","c491bedc":"data['TotalCharges']=pd.to_numeric(data['TotalCharges'],errors='coerce')","4ba5559c":"data.isnull().sum()","d127f11d":"data['TotalCharges'].fillna(data['TotalCharges'].median(),inplace=True)# missing value treated","c4f53915":"data.describe()","ce91b05d":"data.describe(include=[np.object])","71100f6a":"cat_var=data.select_dtypes(include=object).columns.to_list()\nnum_var=data.select_dtypes(include=np.number).columns.to_list()","ccbb57b7":"data1=data.copy()","5b0652af":"data1.hist(figsize=(10,10),color='red')\nplt.show()","73f13c47":"for i in num_var:\n    sns.boxplot(data=data,x=i)\n    plt.show()","2c5945ae":"sns.pairplot(data)","ce96a91e":"i=1\nplt.figure(figsize=(15,50))\nfor col in cat_var:\n    plt.subplot(9,2,i)\n    sns.countplot(data=data1, x=col,hue='Churn')\n    i+=1","0454b14a":"for i in num_var:\n    sns.boxplot(data=data,y=i,x='Churn')\n    plt.show()","cd6f9cf2":"lb=LabelEncoder()","426fb7d7":"for i in cat_var:\n    data1[i]=lb.fit_transform(data[i])","6ed38724":"data1","7aca4c0a":"plt.figure(figsize=(15,15))\nsns.heatmap(data1.corr(),linewidths=1,cmap='Accent_r',annot=True)","d0aec03d":"plt.figure(figsize=(10,10))\ncorr=data1.corr()['Churn'].sort_values(ascending=False)\ncorr.plot(kind='bar')","327b6ca2":"data2=data.copy()","e8960ec2":"data2","aa3facf3":"data2=pd.get_dummies(data2,drop_first=True)","4c716bf2":"sc=StandardScaler()","a0bfe757":"scaled=sc.fit_transform(data2[['tenure','MonthlyCharges','TotalCharges']])\nscaled=pd.DataFrame(scaled,columns=['tenure','MonthlyCharges','TotalCharges'])","45dbbab2":"scaled_data=pd.concat([scaled,data2.drop(columns=['tenure','MonthlyCharges','TotalCharges'])],axis=1)","567392bc":"scaled_data","94a262aa":"X_scaled=scaled_data.drop(columns='Churn_Yes')\nY=scaled_data[['Churn_Yes']]","fea184cd":"pca=PCA()","9a327440":"model=pca.fit_transform(X_scaled)\nmodel","c7b03c03":"np.cumsum(pca.explained_variance_ratio_)","6d82baa5":"feature=range(pca.n_components_)","c7950476":"plt.figure(figsize=(20,5))\nplt.bar(feature,pca.explained_variance_ratio_)\nplt.step(feature,np.cumsum(pca.explained_variance_ratio_),color='red')\nplt.axhline(y=.98,color='green')\nplt.grid(axis='x')\nplt.text(0.5,.85,'98% cutt-off thresold',color='green',fontsize=15)\nplt.show()","73e7c8d2":"pca1=PCA(n_components=19,whiten=True)","0d02bff8":"model1=pca1.fit_transform(X_scaled)","4fe6c1f6":"np.cumsum(pca1.explained_variance_ratio_)","b71bf616":"x_train,x_test,y_train,y_test=train_test_split(model1,Y,test_size=.3,random_state=0)","57e41bee":"lg=LogisticRegression()\ndt=DecisionTreeClassifier()\nrf=RandomForestClassifier()\nnb=GaussianNB()\nknn=KNeighborsClassifier()\ngbc=GradientBoostingClassifier()\nadb=AdaBoostClassifier()\nsgd=SGDClassifier()\nsvc=SVC()\nmlpc=MLPClassifier()","574bf2ff":"algo=[lg,dt,rf,nb,knn,adb,sgd,svc,mlpc]","e1788a84":"model=[]\nacc=[]\nrecall=[]\nfor i in algo:\n    i.fit(x_train,y_train)\n    y_pred=i.predict(x_test)\n    model.append(i)\n    acc.append(i.score(x_test,y_test))\n    recall.append(recall_score(y_pred,y_test))","42075c45":"table=pd.DataFrame([model,acc,recall]).T\ntable.columns=['model','accuracy','recall']","a4cc4d54":"table","32b487dd":"svc1=SVC(probability=True)\nsvc1.fit(x_train,y_train)","a7103799":"svc1.score(x_test,y_test)","5c3bc2db":"y_pred=svc1.predict(x_test)","fd303ef3":"print(classification_report(y_pred,y_test))","cd56f164":"plot_roc_curve(svc1,x_test,y_test)","f981f36d":"roc_auc_score(y_test,y_pred)","fbf25c04":"plot_precision_recall_curve(svc1,x_test,y_test)","37bd8918":"y_pred_prob=svc1.predict_proba(x_test)[:,1]","3e44b2d3":"FPR,TPR,THRESOLD=roc_curve(y_test,y_pred_prob)","1b293ad4":"for i in THRESOLD:\n    y_test_pred=np.where(y_pred_prob>i,1,0)\n    acc=accuracy_score(y_test,y_test_pred)\n    print(acc,i)","24bc817f":"for i in THRESOLD:\n    y_test_pred=np.where(y_pred_prob>i,1,0)\n    recall=recall_score(y_test,y_test_pred)\n    print(recall,i)","da805488":"y_test_pred=np.where(y_pred_prob>=0.17,1,0)","d0a044ae":"print(classification_report(y_test,y_test_pred))","b12de9f6":"#### upto 19 PCA component 98% variance of the data has been covered,so we will take n_component=19","6628ab0a":"#### Before going into the Modelling ,it should be noted that our data is highly sensitive to False Negative.\n#### Let suppose a customer has high probability to leave the sevice(churn) ,but if our model predict that he is not going to leave (not churn) which is False Negative, then company will not be  primarily focusing to that customer's services and then customer will definatly leave the sevices(churn) which results in loss to the company.\n#### And if customer is not willing to leave the services and is satisfied, but our model tells that he is going to leave the services then company will provide him extra benefits ,offers and then customer will become extra happy. Thats why we will not focus on getting best accuracy but try to get best Recall Score.","bd11cd00":"#### We can see that logistic regression and SVC has the same accuracy and recall score. we choose SVC  for final model model building","1cda146b":"#### By observing above plot carefully ,we reach upto certain conclusion that there are some categories of variable where the customer has more probability to leave the services.\n#### So ,to retain them in  telecom services company will have to primarily focus on those factors.\n#### Here is the list of those key factor where there is more probability of customer getting churn,\n#### Organisation has to improve those field of services\n\n#### 1- Parner-No\n#### 2- Dependent-NO\n#### 3- Phone Services-Yes\n#### 4- Multiple lines-Yes\n#### 5- Internet Service- Fibre optics\n#### 6- Online security -NO\n#### 7- Online Backup-NO\n#### 8- Device Protection-No\n#### 9- Tech Support -No\n#### 10- Streaming TV- NO\n#### 11- Streaming Movies no\n#### 12- Contract-month to month\n#### 13- Paperless billing -YES\n#### 14- Payment Method - Electronic check\n\n#### So all above are the key area where services will have to be the best , otherwise customer will more tendency to churn.\n#### Telecom company has to focus on more to retain old customer rather than to find new customers only.","c3e08f83":"### UNIVARIATE ANALYSIS","4161e9a3":"#### There are 2 variable which is useless for our model..[Unnamed: 0', 'customerID'],so we drop it.","b3ea31cd":"#### Any business wants to maximize the number of customers. To achieve this goal, it is important not only to try to attract new ones, but also to retain existing ones. Retaining a client will cost the company less than attracting a new one. In addition, a new client may be weakly interested in business services and it will be difficult to work with him, while old clients already have the necessary data on interaction with the service.\n\n#### Accordingly, predicting the churn, we can react in time and try to keep the client who wants to leave. Based on the data about the services that the client uses, we can make him a special offer, trying to change his decision to leave the operator. This will make the task of retention easier to implement than the task of attracting new users, about which we do not know anything yet.\n\n#### You are provided with a dataset from a telecommunications company. The data contains information about almost six thousand users, their demographic characteristics, the services they use, the duration of using the operator's services, the method of payment, and the amount of payment.\n\n#### The task is to analyze the data and predict the churn of users (to identify people who will and will not renew their contract). The work should include the following mandatory items:\n\n#### Description of the data (with the calculation of basic statistics);\n#### Research of dependencies and formulation of hypotheses;\n#### Building models for predicting the outflow (with justification for the choice of a particular model) based on tested hypotheses and identified relationships;\n#### Comparison of the quality of the obtained models.","98b9245a":"### Model Selection","9171bcf7":"## EDA","aa0878f2":"#### The customers whose tenure is less, monthly charge is high , total charge is less are more probable to leave the services.","6b710a1c":"### FEATURE ENGINEERING","c14b9fc6":"#### we see that variable TotalCharges is object type but it should be float type,so lets convert it.","9f83cc70":"About this file\n\ncustomerID - customer id\n\ngender - client gender (male \/ female)\n\nSeniorCitizen - is the client retired (1, 0)\n\nPartner - is the client married (Yes, No)\n\ntenure - how many months a person has been a client of the company\n\nPhoneService - is the telephone service connected (Yes, No)\n\nMultipleLines - are multiple phone lines connected (Yes, No, No phone service)\n\nInternetService - client's Internet service provider (DSL, Fiber optic, No)\n\nOnlineSecurity - is the online security service connected (Yes, No, No internet service)\n\nOnlineBackup - is the online backup service activated (Yes, No, No internet service)\n\nDeviceProtection - does the client have equipment insurance (Yes, No, No internet service)\n\nTechSupport - is the technical support service connected (Yes, No, No internet service)\n\nStreamingTV - is the streaming TV service connected (Yes, No, No internet service)\n\nStreamingMovies - is the streaming cinema service activated (Yes, No, No internet service)\n\nContract - type of customer contract (Month-to-month, One year, Two year)\n\nPaperlessBilling - whether the client uses paperless billing (Yes, No)\n\nPaymentMethod - payment method (Electronic check, Mailed check, Bank transfer (automatic), Credit card (automatic))\n\nMonthlyCharges - current monthly payment\n\nTotalCharges - the total amount that the client paid for the services for the entire time\n\nChurn - whether there was a churn (Yes or No)","728c3840":"#### Here we can see that we did'nt get any outlier","e796182e":"### BIVARIATE ANALYSIS","8e73c573":"#### We know that in any binary classification model, the cuttoff point for probability is by default =0.5.\n#### So we choose the cuttoff point to reduce the accuracy to 75%. so that recall can increase.","ba91707d":"#### so our recall score has been increased","52f6724b":"#### From above we can see that at cuttoff point 0.17 we get accuracy upto 70%"}}