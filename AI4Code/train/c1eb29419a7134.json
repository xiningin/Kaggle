{"cell_type":{"906df9d9":"code","bb9593fe":"code","d35508ba":"code","f13bb5dd":"code","d86eba49":"code","056e9295":"code","8229c8e6":"code","50464952":"code","186a1d4a":"code","543f7fab":"code","d98c89d5":"code","9789ee21":"code","324cea03":"code","924e5b0f":"code","9aa30740":"code","f5dc5139":"code","f752eff8":"code","06a1c65a":"code","755d2394":"code","de6689e5":"code","df68ba2a":"code","31771d36":"code","d5cfe374":"code","9d90e7ce":"code","e3f086f0":"code","47a174cc":"code","d78efbca":"code","8b289185":"code","ba29dad3":"code","b8de6bd5":"code","164399d3":"code","2fe1ae7f":"markdown","b8e02ae7":"markdown","f5ed55ee":"markdown","21e9555b":"markdown","2807b439":"markdown","8335c9f5":"markdown","2c937731":"markdown","4b8a6948":"markdown","0ae2ba29":"markdown"},"source":{"906df9d9":"import numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn import model_selection\nimport lightgbm as lgb\nfrom sklearn.metrics import roc_auc_score,accuracy_score\nimport optuna\nfrom sklearn.metrics import log_loss\nimport matplotlib.pyplot as plt \nimport seaborn as sns\n%matplotlib inline\nplt.style.use('dark_background')\nsns.set_color_codes(palette='deep')\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","bb9593fe":"df_train = pd.read_csv('..\/input\/tabular-playground-series-may-2021\/train.csv')\ndf_test = pd.read_csv('..\/input\/tabular-playground-series-may-2021\/test.csv')","d35508ba":"df_train.head()","f13bb5dd":"df_test.head()","d86eba49":"df_train.describe()","056e9295":"df_train['target'].value_counts()","8229c8e6":"plt.rcParams[\"figure.figsize\"] = (5,3)\nsns.countplot(df_train['target'])","50464952":"plt.rcParams[\"figure.figsize\"] = (5,5)\nsns.heatmap(df_train.corr())","186a1d4a":"plt.rcParams[\"figure.figsize\"] = (6,4)\nplt.subplot(1, 3, 1)\nsns.distplot(df_train['feature_0'],kde=True,color = 'white',hist=True)\nplt.subplot(1,3,2)\nsns.distplot(df_train['feature_25'],kde=True,color = 'white',hist=True)\nplt.subplot(1,3,3)\nsns.distplot(df_train['feature_49'],kde=True,color = 'white',hist=True)\nplt.suptitle('Distribution of three features out of 50')","543f7fab":"# Replacing the class 1 as 0, class 2 as 1 and so on. I have used the replace function on can use the mapping function as well\ndf_train['target'] = df_train['target'].replace({'Class_1':0,'Class_2':1,'Class_3':2,'Class_4':3})","d98c89d5":"# Checking for missing values\ndf_train.isnull().sum()","9789ee21":"# Splitting the training data into train and test halves. \ntrain,test = train_test_split(df_train,test_size = 0.1,shuffle=True)","324cea03":"train.head()","924e5b0f":"# Creating a target column\ntarget = train.pop('target')\n# removing the id column from both train and test as it is of not much use\ntrain.pop('id')\ntest.pop('id')","9aa30740":"# creating the object function of the optuna\ndef objective(trial,data=train,target = target):\n    # defining the parameters for lgbm\n    param = {\n        \"objective\": \"multiclass\",\n        \"num_class\":4,\n        \"metric\": \"multi_logloss\",\n        \"verbosity\": -1,\n        \"boosting_type\": \"gbdt\",\n        \"lambda_l1\": trial.suggest_float(\"lambda_l1\", 1e-8, 10.0, log=True),\n        \"lambda_l2\": trial.suggest_float(\"lambda_l2\", 1e-8, 10.0, log=True),\n        \"num_leaves\": trial.suggest_int(\"num_leaves\", 2, 256),\n        \"feature_fraction\": trial.suggest_float(\"feature_fraction\", 0.4, 1.0),\n        \"bagging_fraction\": trial.suggest_float(\"bagging_fraction\", 0.4, 1.0),\n        \"bagging_freq\": trial.suggest_int(\"bagging_freq\", 1, 7),\n        \"min_child_samples\": trial.suggest_int(\"min_child_samples\", 5, 100),\n    }\n    \n    # Initializing a array for storing all the accuracy scores\n    acc = []\n    # making the cross validation split\n    kf = model_selection.StratifiedKFold(n_splits=5)\n    # Starting the cross validation loop\n    for train_index, test_index in kf.split(train,target):\n        # Storing the data in my favoured way\n        X_train, X_test = train.iloc[train_index], train.iloc[test_index]\n        y_train, y_test = target.iloc[train_index],target.iloc[test_index]\n        # Creating the dataset for passing it to the lbg model. \n        dtrain = lgb.Dataset(X_train,y_train)\n        # training the model\n        gbm = lgb.train(param, dtrain)\n        # The predict function here returns the probabilities of each class unlike that of the sklearn version\n        preds = gbm.predict(X_test)\n        # Converting the probabilities to predictions\n        pred_labels = [np.argmax(line) for line in preds]\n        # calculating the accuracy score\n        accuracy = accuracy_score(y_test,pred_labels)\n        acc.append(accuracy)\n    print(np.average(acc))\n    return np.average(acc)","f5dc5139":"# creating the study for optuna\nstudy = optuna.create_study(study_name = 'lgbm_parameter_opt', direction=\"maximize\")\nstudy.optimize(objective,n_trials=10) ","f752eff8":"# Selecting the best parameters according to the above optimisation\nbest_params = study.best_trial.params","06a1c65a":"# Adding a few more parameters that are not trainable to the best_parameters dictionary\nbest_params['objective'] = 'multiclass'\nbest_params['num_class'] = 4\nbest_params['metric'] = 'multi_logloss'","755d2394":"df_test.head()","de6689e5":"df_test.pop('id')","df68ba2a":"df_test.head()","31771d36":"test=[]\nlogloss_arr = []\n# Making the final training\nkf = model_selection.StratifiedKFold(n_splits=5)\nfor train_index, test_index in kf.split(train,target):\n    X_train, X_test = train.iloc[train_index], train.iloc[test_index]\n    y_train, y_test = target.iloc[train_index],target.iloc[test_index]\n    dtrain = lgb.Dataset(X_train,y_train)\n    fgbm = lgb.train(best_params, dtrain)\n    preds = fgbm.predict(X_test)\n    logloss = log_loss(y_test,preds)\n    logloss_arr.append(logloss)\n    \n    \ntest.append(fgbm.predict(df_test))\n\n# One thing I missed that I realized while doing the final compilation is that I have not calculated any metrics on the initial test halve which should have been done here. \n","d5cfe374":"logloss_arr","9d90e7ce":"submission = pd.read_csv('..\/input\/tabular-playground-series-may-2021\/sample_submission.csv')","e3f086f0":"submission.head()","47a174cc":"test","d78efbca":"test_predictions = np.mean(test, axis = 0)\nassert len(test_predictions) == len(df_test)","8b289185":"predictions_df = pd.DataFrame(test_predictions, columns = [\"Class_1\", \"Class_2\", \"Class_3\", \"Class_4\"])","ba29dad3":"predictions_df.to_csv(\"submission.csv\", index = False)","b8de6bd5":"predictions_df['id'] = submission['id']\n","164399d3":"predictions_df.to_csv(\"submission.csv\", index = False)","2fe1ae7f":"### Submission","b8e02ae7":"**From the following feature distribution we can say that these three features are a little left skewed. On going through the distribution of all the features,we will see that most of them are left skewed.**","f5ed55ee":"### Hyperparameter Tuning","21e9555b":"### Importing the Libraries and Loading the data","2807b439":"### Feature Engineering","8335c9f5":"**From the aobve plot we can say that there is a chance that the data in imbalanced, however, I will try to deal with it in a future version**","2c937731":"## Tabular Playground Series May 2021 \n### Complete Step by Step guide to approaching the Problem \n\n**Description of what you are going to find in the notebook**\n\nIn the problem I have used a lightgbm model with a 5 fold stratefied cross validation. For finding the best hyperparameters I have used the optuna library with 10 trials.For the metric I have used logloss and in the next version I will be using AUCROC curve. A very basic data visualization and feature engineering has been done as well.  This is just a starter notebook and a lot of improvements are going to be made in this like dimensionality reduction, better hyperparameter optimisation and training with more folds for better validation. \n\n\n\n![tttt.png](attachment:25a04a88-d279-4dae-9b92-c51de6c20bb1.png)\n","4b8a6948":"### Compiling the Final model","0ae2ba29":"### Basic Data Visualization"}}