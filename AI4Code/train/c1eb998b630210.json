{"cell_type":{"15bccf91":"code","28787ff4":"code","1b0ce59c":"code","07fec4c9":"code","b9566348":"code","9d39ec89":"code","1f7e9b36":"code","641a48fa":"code","125e9120":"code","f89214a0":"code","9eab1d42":"code","e2634ed0":"code","895c85c7":"code","8401aa90":"code","40ce8e84":"code","565c0b65":"code","9336d490":"code","bf3f1f17":"code","f62ad876":"code","f0a26af1":"code","2d8c06f0":"code","90171e3a":"code","94b873d1":"markdown","8c885e68":"markdown","dcdf22cc":"markdown","98140776":"markdown","983667ad":"markdown","b0f935b6":"markdown","ed7ec0d8":"markdown","87bf9ae0":"markdown","79776275":"markdown"},"source":{"15bccf91":"from tqdm.notebook import tqdm\nfrom functools import partial\nimport pandas as pd\nimport numpy as np\nimport gc\nimport re\nimport json\nimport nltk\n\nfrom nltk.stem import WordNetLemmatizer\nfrom nltk.tokenize import sent_tokenize, word_tokenize\nfrom sklearn.model_selection import train_test_split\n\npd.set_option('display.max_rows', 1000)\npd.set_option('display.max_columns', 500)\npd.set_option('display.width', 1000)\npd.set_option('display.max_colwidth', None)","28787ff4":"def get_raw_txt(_id:str, path=\"train\"):\n    \"\"\"\n    Gets row publication text by its _id\n    :param _id: publication string id \n    :param path: root directory to the publication json files - train\/test\n    :return: raw string of text\n    \"\"\"\n    _d = json.loads(open(f\"{PATH}\/{path}\/{_id}.json\").read())\n    return \" \".join([i[\"text\"] for i in _d])\n\ndef clean_text(txt:str):\n    \"\"\"\n    Code snippet from the main competition page\n    \"\"\"\n    return re.sub('[^A-Za-z0-9]+', ' ', str(txt).lower()).strip()\n\ndef get_lbl_positions(label:str, text:str):\n    \"\"\"\n    Method provides the position of the 1st and the last word of the label in the given text\n    :return: None if no match or _f,_l - for the first and the last word positions\n    \"\"\"\n    _f = re.search(label+\" \",text)\n    if _f:\n        _f = len(word_tokenize(text[:_f.start()]))\n        _l = _f + len(word_tokenize(label))\n        return int(_f), int(_l)\n        \ndef encode_bio_by_position(text:list, positions:list, padding:int):\n    \"\"\"\n    Returns bio encoded string by its features\n    :param positions: list of tuples\n    \"\"\"\n    bio = np.zeros((padding,),dtype=np.int8)\n    for _f, _l in positions:\n        bio[_f] = 2 #B\n        bio[_f+1:_l] = 1 #I\n    return bio","1b0ce59c":"PATH = \"..\/input\/coleridgeinitiative-show-us-the-data\/\"\ntqdm_params = dict(bar_format='{desc}{bar} [ {n} \/ {total} (remaining: {remaining}) ]', colour=\"darkgreen\")\nwordmodel = pd.read_csv(\"..\/input\/wordmodel\/wordmodel.csv\").values.squeeze()\nwordmodel = sorted(wordmodel, key=len)\n\ntqdm.pandas(desc=\"Text download from JSON files status :\",**tqdm_params)\ndf_train = pd.DataFrame(np.unique(pd.read_csv(f\"{PATH}train.csv\")[[\"Id\"]]), columns = [\"Id\"])\ndf_train['text'] = df_train['Id'].progress_apply(partial(get_raw_txt))\n\ntqdm.pandas(desc=\"Text cleaning with clean_text function :\",**tqdm_params)\ndf_train['text'] = df_train['text'].progress_apply(clean_text)\n\ndf_train['PredictionString'] = ''\ndesc=\"Grouping & relabeling by the wordmodel :\"\nfor i in tqdm(df_train.index, desc=desc ,**tqdm_params):\n    for ws in wordmodel:\n        if ws in df_train.loc[i][\"text\"]:\n            df_train.loc[i]['PredictionString'] += ws.strip() + '|'\n    df_train.loc[i]['PredictionString'] = df_train.loc[i]['PredictionString'][:-1]\n\ndf_train = df_train.drop(\"text\",axis=1)","07fec4c9":"df_train.sample(10)","b9566348":"x,y=[],[]\nmaxlen = 100\ndesc=\"X & Y extraction + BIO labeling :\"\ncounter, too_long = 0 , []\nfor _id in tqdm(df_train.index, desc=desc, **tqdm_params):\n    # Getting list of cleaned sentances from text by Id    \n    _txt = [clean_text(_i) for _i in sent_tokenize(get_raw_txt(df_train.loc[_id,\"Id\"]))]\n    # Getting list of labels\n    _labels = df_train.loc[_id,\"PredictionString\"].split('|')\n    for _sent in _txt:\n        _positions = []\n        for _label in _labels:\n            _pos = get_lbl_positions(_label,_sent)\n            # Non-pruned labels check\n            if _pos:\n                if _pos[1] <= maxlen:\n                    _positions.append(_pos)\n        if _positions:\n            x.append(word_tokenize(_sent)[:maxlen])\n            y.append(encode_bio_by_position(_sent,_positions,maxlen))","9d39ec89":"print(\" \".join(x[0]))\nprint(\" \".join([str(_i) for _i in y[0]]))","1f7e9b36":"glove = open('..\/input\/glove6b100dtxt\/glove.6B.100d.txt', encoding=\"utf8\")\nglove_dict = {}\n\ndesc=\"Filling up the GloVe dict :\"\nfor _line in tqdm(glove, desc=desc, **tqdm_params):\n    records = _line.split()\n    glove_dict[records[0]] = [float(i) for i in records[1:]]","641a48fa":"maxlen = max([len(i) for i in x])\nwnl = WordNetLemmatizer()\nzero_vector = [0]*100\n\ndesc=\"Embedding with the GloVe vectors :\"\nfor _i in tqdm(range(len(x)), desc=desc, **tqdm_params):\n    for _j in range(len(x[_i])):\n        try:\n            x[_i][_j] = glove_dict[wnl.lemmatize(x[_i][_j])]\n        except KeyError:\n            x[_i][_j] = zero_vector\n\ndesc=\"Padding with the zero vaectors :\"\nfor _i in tqdm(range(len(x)), desc=desc, **tqdm_params):\n    _pad = maxlen - len(x[_i])\n    for _j in range(_pad):\n        x[_i].append(zero_vector)","125e9120":"x_tr,x_val,y_tr,y_val = train_test_split(np.array(x),np.array(y),test_size=0.3, shuffle=True, random_state=0)","f89214a0":"del x,y,wordmodel, ws, train_test_split, too_long, df_train, zero_vector\ngc.collect()","9eab1d42":"%who","e2634ed0":"del WordNetLemmatizer, clean_text, counter, desc, encode_bio_by_position, get_lbl_positions, get_raw_txt, glove, glove_dict\n#del x,y,wordmodel, ws, train_test_split, too_long, df_train, zero_vector\ngc.collect()","895c85c7":"!pip3 install ..\/input\/keras-crf-whl-files\/seqeval-1.2.2-py3-none-any.whl\n!pip3 install ..\/input\/keras-crf-whl-files\/keras_crf-0.2.0-py3-none-any.whl","8401aa90":"from keras_crf import CRFModel\nfrom keras import Input, Model\nfrom keras.optimizers import Adam\nfrom keras.layers import Bidirectional,LSTM, Dense,TimeDistributed\n\nfrom keras.metrics import Recall","40ce8e84":"inputs = Input(shape=(maxlen,100))\noutputs = Bidirectional(LSTM(units=maxlen, return_sequences=True,recurrent_dropout=0.1))(inputs)\noutputs = TimeDistributed(Dense(maxlen, activation=\"relu\"))(outputs) \nmodel = Model(inputs, outputs)\nmodel = CRFModel(model,3)\n\nmodel.compile(Adam(lr=0.0005,beta_1=0.9,beta_2=0.999,epsilon=1e-07,),metrics=['acc'])\nmodel.fit(x_tr, y_tr, epochs=100, batch_size=2000, validation_data=(x_val,y_val))","565c0b65":"model.save(\"model\")\n!zip -r model.zip model","9336d490":"del x_tr,x_val,y_tr,y_val\ngc.collect()","bf3f1f17":"from tqdm.notebook import tqdm\nimport json\nfrom nltk.stem import WordNetLemmatizer\nfrom nltk.tokenize import sent_tokenize, word_tokenize\nfrom sklearn.model_selection import train_test_split","f62ad876":"glove = open('..\/input\/glove6b100dtxt\/glove.6B.100d.txt', encoding=\"utf8\")\nglove_dict = {}\n\ndesc=\"Filling up the GloVe dict :\"\nfor _line in tqdm(glove, desc=desc, **tqdm_params):\n    records = _line.split()\n    glove_dict[records[0]] = [float(i) for i in records[1:]]","f0a26af1":"PATH = \"..\/input\/coleridgeinitiative-show-us-the-data\/\"\ntqdm_params = dict(bar_format='{desc}{bar} [ {n} \/ {total} (remaining: {remaining}) ]', colour=\"darkgreen\")\nwordmodel = pd.read_csv(\"..\/input\/wordmodel\/wordmodel.csv\").values.squeeze()\nwordmodel = sorted(wordmodel, key=len)\n\ntqdm.pandas(desc=\"Text download from JSON files status :\",**tqdm_params)\ndeploy = pd.read_csv(f\"{PATH}sample_submission.csv\")\n\ndeploy.index = deploy['Id']","2d8c06f0":"maxlen = 100\nzero_vector = [0]*100\nwnl = WordNetLemmatizer()\ndebinarize  = lambda arr: {\"100\":\"0\",\"010\":\"1\",\"001\":\"2\"}[\"\".join([str(i) for i in arr])]\n\nfor _id in deploy.index:\n    print(_id)\n    result = []\n    _txt = [clean_text(_i) for _i in sent_tokenize(get_raw_txt(deploy.loc[_id,\"Id\"]))]\n    for _sent in _txt:\n        _sent = word_tokenize(_sent[:maxlen])\n        _row_sent = _sent.copy()\n        \n        #Glove Embadding\n        for _i in range(len(_sent)):\n            try:\n                _sent[_i] = glove_dict[wnl.lemmatize(_sent[_i])]\n            except KeyError:\n                _sent[_i] = zero_vector\n\n        #Padding with the zero vaectors\n        _pad = maxlen - len(_sent)\n        for _j in range(_pad):\n            _sent.append(zero_vector)\n        \n        prediction = model.predict(np.array([_sent])).astype(int)\n        prediction = \"\".join([debinarize(i) for i in prediction[0]])\n        prediction = re.finditer(r\"(21*)\",prediction)\n        if prediction:\n            for i in prediction:\n                # I filter one-words for now, need to be emprooved\n                if i.span()[1]- i.span()[0]>1:\n                    result.append(\" \".join(_row_sent[i.span()[0]:i.span()[1]]))\n    print(\"|\".join(set(result)))\n    print(\"---\")\n    deploy.loc[_id,\"PredictionString\"]=\"|\".join(set(result))\n","90171e3a":"deploy.to_csv(\"deploy.csv\", index=False)","94b873d1":"## 1. Data Preprocessing","8c885e68":"### GloVe embedding\nAgain,i decided to apply Glove embadding manually, without putting it into the Keras layer, to save the model train performance \nI took the Stanford's GloVe 100d word embeddings","dcdf22cc":"### BIO Labeling\nI did the BIO labeling with the self-written code, to have more control of it. It looks like not be the optimal solution, may work a bit long. There are some bottlenecks for the labeling:\n* Some sentances have a really huge length after the tokenization. To handle such of them, we simply prune all the values on the positions after the maxlen.\n* When prunning the values, we need to check if our lable exist in non-pruned values or not.\n* We need to collect the BIO-labeled sequences into one sequence, when multipal matches are in text","98140776":"# Coleridge Initiative solution\n\n### Named Entity Recognition task (BIO labeling + Glove Embadding + biLSTM + CDF layer)\n----\n\n\n#### \ud83d\udc4b Hi, I am new to NLP and espessially to Named Entity Recognition.\n \n#### \ud83c\udf93 Actually, I took this task just to get more envolved into this sphere and to sort out my knowledge.\n \n#### \ud83d\udcb8 I hope that it will be useful for you.\n \n#### \ud83e\udde0 I will be glad to hear your advices and best practices examples\n \n#### \ud83d\udc4d And ofcourse I'll be happy to geet your upvotes, enjoy","983667ad":"Prediction procedure should be optimised - just the first brief version","b0f935b6":"### Train set transformation\nOn the previouse steps i have generated the wordset from the matches with the text in the train_df. Now we need to relable it with the aim of:\n* It is not fully labeled (not all matches are taken into account)\n* I will group it in a form where Id is a primar key, so our dataset length equels to the number of publications\n* Our train dataset will be putted into the same form as the test one wil be (Id & PredictionString)","ed7ec0d8":"<div class=\"alert alert-block alert-info\">\n\n<b>Note:<\/b> This is a work in progress notebook!   \n\n<\/div>\n\n*Also, check my \"Fast solution. Coleridge Initiative: Baseline\" which i used to generate the first version of the wordmodel dataset*\n\n*(https:\/\/www.kaggle.com\/nikitakudriashov\/fast-solution-coleridge-initiative-baseline)*","87bf9ae0":"## 2. Model Fitting","79776275":"## 3. Prediction on the test set"}}