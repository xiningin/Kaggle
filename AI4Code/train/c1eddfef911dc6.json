{"cell_type":{"f2bfd3a3":"code","f5a786dd":"code","1c4a62ec":"code","ba69b7e9":"code","b6824476":"code","752bff30":"code","5b077465":"code","aa58e8bf":"code","e7710078":"code","c25ec030":"code","90bd892c":"code","46fbf082":"code","4dcf58c5":"code","c88670ff":"code","4c0eed9b":"code","61b3de85":"code","2d507f93":"code","4dcdcf4f":"markdown","699878dc":"markdown","cc27ff22":"markdown","1f1a266e":"markdown","abadb5db":"markdown","47648ee8":"markdown","5fa1c181":"markdown","edc0bdf1":"markdown","c147fb58":"markdown"},"source":{"f2bfd3a3":"!git clone https:\/\/github.com\/openai\/glide-text2im.git\n%cd glide-text2im\n!pip install -q -e .\n!pip install gdown","f5a786dd":"import torch\n!pip install bitsandbytes-cuda{''.join(c for c in torch.version.cuda if c.isdigit())}","1c4a62ec":"!nvidia-smi","ba69b7e9":"from PIL import Image\nfrom IPython.display import display\nimport torch as th\n\nfrom glide_text2im.download import load_checkpoint\nfrom glide_text2im.model_creation import (\n    create_gaussian_diffusion,\n    create_model_and_diffusion,\n    model_and_diffusion_defaults,\n    model_and_diffusion_defaults_upsampler\n)","b6824476":"# This notebook supports both CPU and GPU.\n# On CPU, generating one sample may take on the order of 20 minutes.\n# On a GPU, it should be under a minute.\n\nhas_cuda = th.cuda.is_available()\nfp16 = False  #@param {type: \"boolean\"}\ndevice = th.device('cpu' if not has_cuda else 'cuda')","752bff30":"# Create base model.\noptions = model_and_diffusion_defaults()\noptions[\"use_fp16\"] = has_cuda and fp16\noptions[\"timestep_respacing\"] = \"1000\"  # use 100 diffusion steps for fast sampling\nmodel, diffusion = create_model_and_diffusion(**options)\nmodel.eval()\nif has_cuda and fp16:\n    model.convert_to_fp16()\nmodel.to(device)\n#@markdown path to checkpoint to load, leave empty for default checkpoint\ncheckpoint_path = \"\" #@param {type: \"string\"}\nif not checkpoint_path:\n    checkpoint_path = load_checkpoint(\"base\", device)\nmodel.load_state_dict(checkpoint_path)\nprint(\"total base parameters\", sum(x.numel() for x in model.parameters()))","5b077465":"# Create upsampler model.\noptions_up = model_and_diffusion_defaults_upsampler()\noptions_up['use_fp16'] = has_cuda and fp16\noptions_up['timestep_respacing'] = 'fast27' # use 27 diffusion steps for very fast sampling\nmodel_up, diffusion_up = create_model_and_diffusion(**options_up)\nmodel_up.eval()\nif has_cuda and fp16:\n    model_up.convert_to_fp16()\nmodel_up.to(device)\nmodel_up.load_state_dict(load_checkpoint('upsample', device))\nprint('total upsampler parameters', sum(x.numel() for x in model_up.parameters()))","aa58e8bf":"def show_images(batch: th.Tensor):\n    \"\"\" Display a batch of images inline. \"\"\"\n    scaled = ((batch + 1)*127.5).round().clamp(0,255).to(th.uint8).cpu()\n    reshaped = scaled.permute(2, 0, 3, 1).reshape([batch.shape[2], -1, 3])\n    display(Image.fromarray(reshaped.numpy()))","e7710078":"#@title resolution settings\n\n#@markdown the resolution goes up to 192x128 on a 12GB K80. it takes 2 minutes to generate at 128x64 and 10 for 256x128.\n\n#@markdown the final resolution of the image after upsampling will be 4x higher \n\nside_x = 64  #@param {type: \"integer\"}\nside_y =   64#@param {type: \"integer\"}\nupsample_x = 4  #@param {type: \"integer\"}","c25ec030":"#@title image to tune with\nimport torch\nimport requests\nimport numpy as np\nfrom PIL import Image\n\n\npreprocess = lambda xs: torch.from_numpy(np.stack([np.asarray(x.resize((side_x, side_y)).convert(\"RGB\")) for x in xs], axis=0)).permute(0, 3, 1, 2) \/ 127. - 1.\ndownload = lambda x: preprocess([Image.open(requests.get(x, stream=True).raw)])\nimg_path = \"https:\/\/cdn.discordapp.com\/attachments\/730484623028519072\/926386737167622175\/unknown.png\" #@param [\"https:\/\/cdn.discordapp.com\/attachments\/869675061211181107\/924292876601540638\/Title.jpg\", \"https:\/\/cdn.discordapp.com\/attachments\/730484623028519072\/926386737167622175\/unknown.png\"] {type: \"string\", allow-input: true} \nt = download(img_path)\nt.shape\nshow_images(t)","90bd892c":"#@title (optional) multiple additional images\nimport gdown\nimport glob\n\n\n#@markdown leave empty if you upload your own zip file\ngdown_link = \"1cNrxEGPyzFazxyhgRAgKj7wU-IYp20W-\" #@param [\"\", \"1cNrxEGPyzFazxyhgRAgKj7wU-IYp20W-\"] {type: \"string\", allow-input: true}\n#@markdown path to the zip file. change this to the name of your own, leave empty if your images are already extracted.\nzip_path = \"ffhq.zip\"  #@param [\"\", \"ffhq.zip\", \"dataset.zip\"] {type: \"string\", allow-input: true}\n#@markdown a specification of your input files. leave empty if there are no files\nfiles_glob = \"ffhq\/*.jpg\"  #@param [\"\", \"**\/*.jpg\", \"**\/*.png\", \"ffhq\/*.jpg\"] {type: \"string\", allow-input: true}\n#@markdown how many images to use and process?\ntrain_imgs =   100000#@param {type: \"integer\"}\n\n\nif gdown_link is not None:\n  zip_path = \"dataset.zip\"\n  gdown.download(f\"https:\/\/drive.google.com\/uc?id={gdown_link}\", zip_path, quiet=False)\nif zip_path:\n  print(\"unzipping...\")\n  !unzip -o {zip_path} > \/dev\/null 2> \/dev\/null\nfiles = glob.glob(files_glob)\nif files:\n  files = list(map(Image.open, files[:train_imgs]))\n  t = preprocess(files)\n  row_size, show_total = 10, 100  # TODO\n  show_total = min(show_total, len(files))\n  for i in range(0, show_total, row_size):\n    show_images(t[i:i+row_size])","46fbf082":"#@title sampling parameters\nprompt = \"a picture of a face\" #@param {type: \"string\", allow-input: true} [\"a picture of a face\", \"an landscape painting of switzerland\", \"an oil painting of a cat\"]\n\nbatch_size = 4  #@param {type: \"integer\"}\nsample_steps = 27  #@param {allow-input: true}\nguidance_scale = 0.0  #@param {type: \"number\"}\nupsample_temp = 0.997  #@param {type: \"number\"}","4dcf58c5":"##############################\n#@title Sample from the base model #\n##############################\nfrom IPython.display import display, clear_output\nfrom ipywidgets import Output\nimport torch\n\n\nout = Output()\ndisplay(out)\n\n\neval_diffusion = create_gaussian_diffusion(\n    steps=options[\"diffusion_steps\"],\n    noise_schedule=options[\"noise_schedule\"],\n    timestep_respacing=str(sample_steps),\n)\n\n\n# Create a classifier-free guidance sampling function\ndef model_fn(x_t, ts, **kwargs):\n    half = x_t[: len(x_t) \/\/ 2]\n    combined = th.cat([half, half], dim=0)\n    # print(ts)\n    model_out = model(combined,\n                      ts,\n                      **kwargs)\n    eps, rest = model_out[:, :3], model_out[:, 3:]\n    cond_eps, uncond_eps = th.split(eps, len(eps) \/\/ 2, dim=0)\n    beta = eval_diffusion.betas[int(ts.flatten()[0].item() \/ options[\"diffusion_steps\"] * len(eval_diffusion.betas))]\n    half_eps = uncond_eps + guidance_scale * (cond_eps - uncond_eps)\n    eps = th.cat([half_eps, half_eps], dim=0)\n    with out:\n        clear_output()\n        show_images((x_t - eps * (beta ** 0.5))[:batch_size])\n    return th.cat([eps, rest], dim=1)\n\n# Sample from the base model.\ndef sample(prompt=prompt):\n    model.del_cache()\n\n    # Create the text tokens to feed to the model.\n    tokens = model.tokenizer.encode(prompt)\n    tokens, mask = model.tokenizer.padded_tokens_and_mask(\n        tokens, options['text_ctx']\n    )\n\n    # Create the classifier-free guidance tokens (empty)\n    full_batch_size = batch_size * 2\n    uncond_tokens, uncond_mask = model.tokenizer.padded_tokens_and_mask(\n        [], options['text_ctx']\n    )\n\n    # Pack the tokens together into model kwargs.\n    model_kwargs = dict(\n        tokens=th.tensor(\n            [tokens] * batch_size + [uncond_tokens] * batch_size, device=device\n        ),\n        mask=th.tensor(\n            [mask] * batch_size + [uncond_mask] * batch_size,\n            dtype=th.bool,\n            device=device,\n        ),\n    )\n    with torch.inference_mode():\n        samples = eval_diffusion.p_sample_loop(\n            model_fn,\n            (full_batch_size, 3, side_y, side_x),  # only thing that's changed\n            device=device,\n            clip_denoised=True,\n            progress=True,\n            model_kwargs=model_kwargs,\n            cond_fn=None,\n        )[:batch_size]\n    model.del_cache()\n    return samples\n\n\nsample()\n0","c88670ff":"#@title training\nfrom tqdm.auto import trange, tqdm\n\nimport os\nimport gc\nimport random\nimport bitsandbytes as bnb\nfrom itertools import cycle\nfrom more_itertools import chunked\nfrom matplotlib import pyplot as plt\nfrom IPython.display import clear_output\n\n\nmodel.zero_grad()\ntry:\n    del optim\n    del xs\n    del out\n    del loss\nexcept NameError:\n    pass\ngc.collect()\ntorch.cuda.empty_cache()\nseed = 0\nlearning_rate = 2e-5  #@param {type: \"number\"}\nmodel.requires_grad_(True)\n\ntrain_fp16 = False  #@param {type: \"boolean\"}\nadam8bit = False  #@param {type: \"boolean\"}\nif not adam8bit:\n    optim_class = th.optim.Adam\nelse:\n    optim_class = bnb.optim.Adam8bit\noptim = optim_class([x for x in model.parameters() if x.requires_grad], lr=learning_rate)\n\nbatch_size = 4  #@param {type: \"integer\"}\ngrad_acc = 1  #@param {type: \"integer\"}\nepochs =   5  #@param {type: \"integer\"}\nsample_every =   100  #@param {type: \"integer\"}\nrespace =   True  #@param {type: \"boolean\"}\n\nsave_path = \".\/glide-finetuned\"  #@param {type: \"string\"}\nos.makedirs(save_path, exist_ok=True)\nsave_every = 500  #@param {type: \"integer\"}\n\n# prompts disabled\n# # Create the text tokens to feed to the model.\n# tokens = model.tokenizer.encode(prompt)\n# tokens, mask = model.tokenizer.padded_tokens_and_mask(\n#     tokens, options['text_ctx']\n# )\n# Create the classifier-free guidance tokens (empty)\nfull_batch_size = batch_size  # * 2\nuncond_tokens, uncond_mask = model.tokenizer.padded_tokens_and_mask(\n    [], options['text_ctx']\n)\n\n# Pack the tokens together into model kwargs.\nmodel_kwargs = dict(\n    tokens=th.tensor(\n        # [tokens] * batch_size + \n        [uncond_tokens] * batch_size, device=device\n    ),\n    mask=th.tensor(\n        # [mask] * batch_size + \n        [uncond_mask] * batch_size,\n        dtype=th.bool,\n        device=device,\n    ),\n)\n\n\ndef _extract_into_tensor(arr, timesteps, broadcast_shape):\n    \"\"\"\n    Extract values from a 1-D numpy array for a batch of indices.\n    :param arr: the 1-D numpy array.\n    :param timesteps: a tensor of indices into the array to extract.\n    :param broadcast_shape: a larger shape of K dimensions with the batch\n                            dimension equal to the length of timesteps.\n    :return: a tensor of shape [batch_size, 1, ...] where the shape has K dims.\n    \"\"\"\n    # print(arr.shape, timesteps.shape)\n    res = th.from_numpy(arr).to(device=timesteps.device)[timesteps].float()\n    # res = th.gather(res, 0, timesteps)\n    # print(res.shape, timesteps.shape)# [timesteps].float()\n    # print(\"before while\")\n    # return\n    while len(res.shape) < len(broadcast_shape):\n        res = res[..., None]\n    return res.expand(broadcast_shape)\n\n\nout = Output()\ndisplay(out)\nlosses = []\nbar = trange(epochs * t.shape[0])\nl = 0\nif not seed:\n    seed = random.getrandbits(32)\n    print(\"seed:\", seed)\nrandom.seed(seed)\ntorch.manual_seed(seed)\ntry:\n    model.train()\n    xs = list(t.detach().clone())\n    random.shuffle(xs)\n    for i, x in zip(bar, chunked(cycle(torch.stack(xs)), full_batch_size)):\n        with torch.cuda.amp.autocast(enabled=train_fp16):\n            x = torch.stack(x, dim=0).to(device)\n            ts = torch.randint(0, len(diffusion.betas)-1,\n                               (full_batch_size,)).to(x.device)\n            orig_noise = th.randn_like(x, device=x.device)\n            if respace:\n                ts = ts \/\/ (options[\"diffusion_steps\"] \/\/ len(diffusion.betas))\n            x_t = diffusion.q_sample(x, ts, noise=orig_noise)\n            output = model(x_t, ts, **model_kwargs)\n            eps = output[..., :3, :, :]\n            loss = th.nn.functional.mse_loss(eps, orig_noise.detach())\n        l += loss.item()\n        loss.backward()\n        if i % grad_acc == grad_acc - 1:\n            optim.step()\n            optim.zero_grad()\n            l \/= grad_acc\n            bar.set_description(f\"loss: {l}\")\n            losses.append(l)\n            l = 0\n            with out:\n                clear_output()\n                plt.plot(losses)\n                plt.show()\n        if i % sample_every == 0:\n            samples = sample()\n            out = Output()\n            display(out)\n        if i % save_every == save_every - 1:\n            torch.save(model.state_dict(), os.path.join(save_path, f\"{i}.pth\"))\nexcept KeyboardInterrupt:\n    pass","4c0eed9b":"show_images(samples)","61b3de85":"##############################\n#@title Upsample the low-res images #\n##############################\n\ntokens = model_up.tokenizer.encode(prompt)\ntokens, mask = model_up.tokenizer.padded_tokens_and_mask(\n    tokens, options_up['text_ctx']\n)\n\n# Create the model conditioning dict.\nmodel_kwargs = dict(\n    # Low-res image to upsample.\n    low_res=((samples+1)*127.5).round()\/127.5 - 1,\n\n    # Text tokens\n    tokens=th.tensor(\n        [tokens] * batch_size, device=device\n    ),\n    mask=th.tensor(\n        [mask] * batch_size,\n        dtype=th.bool,\n        device=device,\n    ),\n)\n\n# Sample from the base model.\nmodel_up.del_cache()\nup_shape = (batch_size, 3, side_y * upsample_x, side_x * upsample_x)\nup_samples = diffusion_up.ddim_sample_loop(\n    model_up,\n    up_shape,\n    noise=th.randn(up_shape, device=device) * upsample_temp,\n    device=device,\n    clip_denoised=True,\n    progress=True,\n    model_kwargs=model_kwargs,\n    cond_fn=None,\n)[:batch_size]\nmodel_up.del_cache()","2d507f93":"show_images(up_samples)","4dcdcf4f":"### Fine-tuning","699878dc":"### Show the output","cc27ff22":"### Show the output of sampling at your resolution","1f1a266e":"### Install dependencies","abadb5db":"based on woctezuma's Colab\n\nedits by @nev#4905\n\n2021\/12\/21: added non-square resolution, colab forms\n\n2021\/12\/31: added fine-tuning\n\n2022\/01\/01 - 2021\/01\/10: multiple-image tuning, minor fixes\n\n2021\/01\/19: afiaka47's changes\n\nReferences:\n- https:\/\/github.com\/openai\/glide-text2im\n- https:\/\/github.com\/woctezuma\/glide-text2im-colab","47648ee8":"the end\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n---","5fa1c181":"### Resolution settings","edc0bdf1":"### Upsample to 4x resolution","c147fb58":"### Set-up functions, models and options"}}