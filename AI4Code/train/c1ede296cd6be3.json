{"cell_type":{"1ab19a09":"code","7694c923":"code","08eda4b7":"code","e454efba":"code","b02acab2":"code","b9b5eadf":"code","f6ab7888":"code","2de7d574":"code","ae97f05f":"code","72f49601":"code","5af6d6aa":"code","af30abb1":"code","f34ae121":"code","f3a4919e":"code","20c325cf":"code","aebc522f":"code","bae3176b":"code","16ed4d5a":"code","c33ddc9a":"code","c4a437b4":"code","29daadbb":"code","76af8513":"code","14e7fbc2":"code","c11e95c6":"code","5260724d":"code","b05377a9":"code","896362c2":"code","6ad45a32":"markdown","4747704f":"markdown","eb885ed5":"markdown","f5d21fed":"markdown","a8ebe205":"markdown","c19a8402":"markdown","3a8bbb1d":"markdown","82913fe5":"markdown","1830a226":"markdown","24b861ef":"markdown","9465600d":"markdown","cfb8b6c2":"markdown","0df6caf6":"markdown","4bdc9a67":"markdown","4268b3da":"markdown","fb9e2ea0":"markdown","4d68366d":"markdown","a385c754":"markdown","41468401":"markdown","7cf29216":"markdown","f403d848":"markdown","999c7a51":"markdown","eb07a8ac":"markdown","cbf3a1bb":"markdown","237d714e":"markdown"},"source":{"1ab19a09":"!pip install scipy\n!pip install statistics\n!pip install weightedstats","7694c923":"from sklearn.model_selection import train_test_split\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\nimport numpy as np\nimport pandas as pd\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier, VotingClassifier\nfrom collections import Counter\nfrom sklearn.utils import shuffle\nfrom sklearn.metrics import auc, precision_recall_curve\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.metrics import precision_recall_curve\nfrom sklearn.svm import SVC\nfrom statsmodels.stats.stattools import medcouple\nfrom imblearn.under_sampling import RandomUnderSampler\nfrom sklearn.utils.class_weight import compute_class_weight\nfrom imblearn.over_sampling import SMOTE\nfrom imblearn.pipeline import Pipeline\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.linear_model import HuberRegressor, LinearRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom numpy import mean\nfrom sklearn.datasets import make_classification\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import RepeatedStratifiedKFold\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.model_selection import GridSearchCV\n\nimport math\nfrom statistics import median\nfrom scipy.stats import skew\nimport weightedstats as ws\n","08eda4b7":"dataset = pd.read_csv('\/kaggle\/input\/creditcardfraud\/creditcard.csv')\nlabel = dataset['Class']\norigin_data = dataset # use detection outlier","e454efba":"dataset.describe()","b02acab2":"dataset['Time'].describe()","b9b5eadf":"dataset['Amount'].describe()","f6ab7888":"dataset.fillna(np.nan)\ndataset.isnull().sum().max()","2de7d574":"graph_time = sns.distplot(dataset['Time'], color=\"m\", label=\"Skewness : %.2f\"%(dataset['Time'].skew()))\ngraph_time = graph_time.legend(loc=\"best\")","ae97f05f":"g = sns.distplot(dataset['Amount'], color=\"m\", label=\"Skewness : %.2f\"%(dataset['Amount'].skew()))\ng = g.legend(loc=\"best\")","72f49601":"def log_transform(feature):\n    dataset[feature] = dataset[feature].map(lambda x:np.log(x) if x>0 else 0 )","5af6d6aa":"log_transform('Time')\ngraph_time = sns.distplot(dataset['Time'], color=\"m\", label=\"Skewness : %.2f\"%(dataset['Time'].skew()))\ngraph_time = graph_time.legend(loc=\"best\")","af30abb1":"log_transform('Amount')\ng = sns.distplot(dataset['Amount'], color=\"m\", label=\"Skewness : %.2f\"%(dataset['Amount'].skew()))\ng = g.legend(loc=\"best\")","f34ae121":"rforest_checker = RandomForestClassifier(random_state = 0)\nrforest_checker.fit(dataset, label)\nimportances_df = pd.DataFrame(rforest_checker.feature_importances_, columns=['Feature_Importance'],\n                              index=dataset.columns)\nimportances_df.sort_values(by=['Feature_Importance'], ascending=False, inplace=True)\nprint(importances_df)","f3a4919e":"\"\"\"\n    The purpose of this class is to find MC measure. \n    You might reference my Github: https:\/\/github.com\/tks1998\/statistical-function-and-algorithm-ML-\/blob\/master\/medcople.py\n    Complexity : O(nlogn)\n    The code is the implementation of the formula on wiki. But it is still having some issues, since there are errors arised from finite-precision floating point arithmetic (when compare two float number). \n    I tested 1000 tests and compare function Medcouple with statsmodels, total errors for 1000 tests ~2.2, That means that each test I deviate by about 0.022.\n    Function medcouple from Statsmodels is implemented with complexity O(n^2). If applying the problems (n ~ 285000), I do not have no enough RAM to run the test. ***So that I must implement the function???***.   \n\"\"\"\nclass Med_couple:\n    \n    def __init__(self,data):\n        self.data = np.sort(data,axis = None)[::-1] # sorted decreasing  \n        self.med = np.median(self.data)\n        self.scale = 2*np.amax(np.absolute(self.data))\n        self.Zplus = [(x-self.med)\/self.scale for x in self.data if x>=self.med]\n        self.Zminus = [(x-self.med)\/self.scale for x in self.data if x<=self.med]\n        self.p = len(self.Zplus)\n        self.q = len(self.Zminus)\n    \n    def H(self,i,j):\n        a = self.Zplus[i]\n        b = self.Zminus[j]\n\n        if a==b:\n            return np.sign(self.p - 1 - i - j)\n        else:\n            return (a+b)\/(a-b)\n\n    def greater_h(self,u):\n\n        P = [0]*self.p\n\n        j = 0\n\n        for i in range(self.p-1,-1,-1):\n            while j < self.q and self.H(i,j)>u:\n                j+=1\n            P[i]=j-1\n        return P\n\n    def less_h(self,u):\n\n        Q = [0]*self.p\n\n        j = self.q - 1\n\n        for i in range(self.p):\n            while j>=0 and self.H(i,j) < u:\n                j=j-1\n            Q[i]=j+1\n        \n        return Q\n    #Kth pair algorithm (Johnson & Mizoguchi)\n    def kth_pair_algorithm(self):\n        L = [0]*self.p\n        R = [self.q-1]*self.p\n\n        Ltotal = 0\n\n        Rtotal = self.p*self.q\n\n        medcouple_index = math.floor(Rtotal \/ 2)\n\n        while Rtotal - Ltotal > self.p:\n\n            middle_idx = [i for i in range(self.p) if L[i]<=R[i]]\n            row_medians = [self.H(i,math.floor((L[i]+R[i])\/2)) for i in middle_idx]\n\n            weight = [R[i]-L[i] + 1 for i in middle_idx]\n\n            WM = ws.weighted_median(row_medians,weights = weight)\n            \n            P = self.greater_h(WM)\n\n            Q = self.less_h(WM)\n\n            Ptotal = np.sum(P)+len(P) \n            Qtotal = np.sum(Q)\n\n            if medcouple_index <= Ptotal-1:\n                R = P\n                Rtotal = Ptotal\n            else:\n                if medcouple_index > Qtotal - 1:\n                    L = Q\n                    Ltotal = Qtotal\n                else:\n                    return WM\n        remaining = np.array([])\n       \n        for i in range(self.p):\n            for j in range(L[i],R[i]+1):\n                remaining = np.append(remaining,self.H(i,j))\n\n        find_index = medcouple_index-Ltotal\n\n        k_minimum_element = remaining[np.argpartition(remaining,find_index)] # K-element algothrm  \n    \n        return k_minimum_element[find_index]\n       ","20c325cf":"\"\"\"\n    Reference from gits.github of joseph-allen : https:\/\/gist.github.com\/joseph-allen\/14d72af86689c99e1e225e5771ce1600 \n\"\"\"\ndef detection_outlier(n,df):\n    \n    outlier_indices = []\n    \n    for col in df.columns:\n        Q1 = np.percentile(df[col],25)\n        Q3 = np.percentile(df[col],75)\n        IQR = Q3 - Q1\n        outlier_step = 1.5 * IQR \n        medcouple = Med_couple(np.array(df[col])).kth_pair_algorithm()\n        if (medcouple >=0):\n            outlier_list_col = df[(df[col] < Q1 - outlier_step*math.exp(-3.5*medcouple)) | (df[col] > Q3 + outlier_step*math.exp(4*medcouple) )].index\n        else:\n            outlier_list_col = df[(df[col] < Q1 - outlier_step*math.exp(-4*medcouple)) | (df[col] > Q3 + outlier_step*math.exp(3.5*medcouple) )].index\n        outlier_indices.extend(outlier_list_col)     \n    outlier_indices = Counter(outlier_indices)        \n    multiple_outliers = list( k for k, v in outlier_indices.items() if v > n )\n    return multiple_outliers   ","aebc522f":"index_outlier = detection_outlier(20,origin_data)\nprint(index_outlier)","bae3176b":"\"\"\"\n    Removing outlier from dataset using medcouple method\n\"\"\"\nx_train = dataset\nx_train = x_train.drop(index_outlier, axis = 0).reset_index(drop=True)\ny_train = x_train['Class']\nx_train = x_train.drop(columns = ['Class'])","16ed4d5a":"\"\"\"\n    reference from kaggle: https:\/\/www.kaggle.com\/lane203j\/methods-and-common-mistakes-for-evaluating-models\n\"\"\"\nclass CreditCard:\n    def __init__(self,clf,data,label,k_fold = 10):\n        self.clf = clf\n        self.data = data\n        self.label = label\n        self.k_fold = k_fold\n    \n    def ROC_score(self,x, y):\n        precisions, recalls,_ = precision_recall_curve(y, self.clf.predict_proba(x)[:,1], pos_label=1)\n        return auc(recalls, precisions)\n    \n    def calculate_score_model(self,num_random_state = None, is_shuffle = False):\n        \n        skf = StratifiedKFold(n_splits=self.k_fold, random_state=num_random_state, shuffle=is_shuffle)\n        \n        sum = 0\n        \n        for train_idx, test_idx in skf.split(self.data,self.label):\n            \n            x_train = self.data.loc[train_idx]\n            y_train = self.label.loc[train_idx]\n            \n            x_test = self.data.loc[test_idx]\n            y_test = self.label.loc[test_idx]\n            self.clf.fit(x_train, y_train)\n            sum+=self.ROC_score(x_test,y_test)\n            \n        return sum\/self.k_fold    ","c33ddc9a":"class_weights = compute_class_weight('balanced', np.unique(y_train), y_train)","c4a437b4":"class_weights","29daadbb":"clf1 = LogisticRegression(max_iter = 1000000,class_weight = {0:0.50070004, 1: 357.62311558})\nlogstic = CreditCard(clf1,x_train,y_train,10).calculate_score_model()\nlogstic","76af8513":"steps = [('over', SMOTE()), ('model', LogisticRegression())]\n\npipeline = Pipeline(steps=steps)\n# evaluate pipeline\ncv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n\nscores = cross_val_score(pipeline, x_train, y_train, scoring='roc_auc', cv=cv, n_jobs=-1)\nprint('Mean ROC AUC : %.3f' % mean(scores))","14e7fbc2":"model = LogisticRegression()\nunder = RandomUnderSampler(sampling_strategy=0.5)\nsteps = [('under', under), ('model', model)]\npipeline = Pipeline(steps=steps)\n# evaluate pipeline\ncv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\nscores = cross_val_score(pipeline, x_train, y_train, scoring='roc_auc', cv=cv, n_jobs=-1)\nprint('Mean ROC AUC: %.3f' % mean(scores))","c11e95c6":"model = LogisticRegression()\nover = SMOTE(sampling_strategy=0.1)\nunder = RandomUnderSampler(sampling_strategy=0.5)\nsteps = [('over', over), ('under', under), ('model', model)]\npipeline = Pipeline(steps=steps)\n# evaluate pipeline\ncv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\nscores = cross_val_score(pipeline, dataset, label, scoring='roc_auc', cv=cv, n_jobs=-1)\nprint('Mean ROC AUC: %.3f' % mean(scores))","5260724d":"clf1 = LogisticRegression()\nclf2 = RandomForestClassifier()\nclf3 = GaussianNB()\nvoting = VotingClassifier(estimators=[('lr', clf1), ('rf', clf2), ('gnb', clf3)], voting='soft')\n\nover = SMOTE(sampling_strategy=0.1)\nunder = RandomUnderSampler(sampling_strategy=0.5)\nsteps = [('over', over), ('under', under), ('model', voting)]\npipeline = Pipeline(steps=steps)\n# evaluate pipeline\ncv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n\nscores = cross_val_score(pipeline, dataset, label, scoring='roc_auc', cv=cv, n_jobs=-1)\nprint('Mean ROC AUC: %.3f' % mean(scores))\n","b05377a9":"\"\"\"\n    loaded data\n\"\"\"\ny_train  = dataset['Class']\nx_train  = dataset.drop(columns = ['Class'])","896362c2":"steps = [('over', SMOTE()), ('model', HuberRegressor())]\npipeline = Pipeline(steps=steps)\n# evaluate pipeline\ncv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\nscores = cross_val_score(pipeline, x_train, y_train, scoring='roc_auc', cv=cv, n_jobs=-1)\nprint('Mean ROC AUC: %.3f' % mean(scores))","6ad45a32":"## 3.2  Log transformation  <a class=\"anchor\" id=\"logtransform\" ><\/a>","4747704f":"# *II Loading and checking data* <a class=\"anchor\" id=\"load-data\" ><\/a>","eb885ed5":"\n## **TUKEY\u2019S METHOD && ADJUSTED BOXPLOT**\n\n## 4.1. Why do we use Tukey's method && adjusted boxplot ? <a class=\"anchor\" id=\"tukey_adjusted\" ><\/a> \n   \n   * The dataset is very large(~285000 rows),\n   * Data are heavily skewed in a some feature. \n   * Tukey\u2019s method is applicable to skewed or non mound-shaped data since it makes no distributional assumptions and it does not depend on the mean or standard deviation.\n   * The adjusted boxplot applies when the data is heavily skewed. From the above visualisation: \n\n## 4.2. TUKEY\u2019S METHOD <a class=\"anchor\" id=\"tukey\" ><\/a>\n   ![](https:\/\/upload.wikimedia.org\/wikipedia\/commons\/thumb\/a\/a9\/Empirical_Rule.PNG\/350px-Empirical_Rule.PNG)   \n    \n   * The plot shows that about 68%,95%, and 99.7% of the data from a normal distribution are within 1, 2, and 3 standard deviations of the mean, respectively[3].\n   * Tukey method depend on a mean or standard deviation of the data. \n   * The plot from [wikimedia commons][4] shows for us Tukey's method work into the data.\n![](https:\/\/upload.wikimedia.org\/wikipedia\/commons\/thumb\/1\/1a\/Boxplot_vs_PDF.svg\/550px-Boxplot_vs_PDF.svg.png)    \n   * The lower quartile (Q1) is the 25th percentile, and the upper quartile (Q3) is the 75th percentile of the data.\n   * The IQR (Inter Quartile Range) is the distance between the lower (Q1) and upper (Q3) quartiles.\n   * Inner fences are located at a distance 1.5 IQR below Q1 and above Q3 [Q1-1.5 IQR,Q3+1.5IQR].\n   * Further information can be found in [book][3].\n    \n## 4.3. Adjusted boxplot.<a class=\"anchor\" id=\"Adjustedboxplot\" ><\/a>\n\n   * The distribution migth skewed left or skewed right. \n   * If our data has skewed right, then the fence may move to the right and vise versa. \n   * When Mc = 0 -> e^0 = 1 then formula looks similarly to to Tukey's method. Apparently, adjust boxplot is a general case of the Tukey's method.\n   * In the method, It uses the measure MC to calculate score skewed. And calculate e^MC is the interval move of the fence.\n   * The [plot][5] below shows us left-skewed of the data, and right-skewed of the data  \n    \n   ![](https:\/\/a8h2w5y7.rocketcdn.me\/wp-content\/uploads\/2014\/02\/pearson-mode-skewness.jpg)    \n   \n   * We apply the adjusted box\n\n   * Then We folowing the compute:\n    \n            If MC>0, the data has a right skewed distribution\n            If MC<0, the data has a left skewed distribution\n            [L, R] = [Q1-1.5 * exp (-3.5MC) * IQR, Q3+1.5 * exp (4MC) * IQR] if MC \u2265 0\n                   = [Q1-1.5 * exp (-4MC) * IQR, Q3+1.5 * exp (3.5MC) * IQR] if MC \u2264 0\n                   \n    * All the data in [L,R] is inlier and remaning is outlier. \n    * In the problems, If Any rows of the data have 20\/30 value fall in outlier, When I consider for each column row is the outlier, and remove it. \n   \n## 4.4. How to calculate medcouple measure ? <a class=\"anchor\" id=\"MC measure\" ><\/a>\n   * We easily find formula for medcouple in the [wikipedia][1]. But I didn't find any packages which implement it efftively. If you know any packages implement it more effective, please share it with me. \n   * I implement medcouple method with a complexity O(nlogn)([wikipedia][1]), but the performance is still bad because my programming skill is not good. Please comment below or report in my github repository if you have any issues.[2]\n    \n    \n[1]: https:\/\/en.wikipedia.org\/wiki\/Medcouple\n[2]: https:\/\/github.com\/tks1998\/statistical-function-and-algorithm-ML-\/blob\/master\/medcople.py\n[3]: http:\/\/d-scholarship-dev.library.pitt.edu\/7948\n[4]: https:\/\/commons.wikimedia.org\/wiki\/File:Boxplot_vs_PDF.svg\n[5]: https:\/\/www.statisticshowto.com\/probability-and-statistics\/skewed-distribution\/        ","f5d21fed":"# III Visualization and Feature extraction <a class=\"anchor\" id=\"visualization\" ><\/a> ","a8ebe205":"The data have a non-missing value.","c19a8402":"## 3.1 Analysis of the features 'Time' and 'Amount'  <a class=\"anchor\" id=\"plot-time-amount\" ><\/a> \n\nThe feature Time and Amount are both has very high mean, as in the two charts below ","3a8bbb1d":"# V. Algorithm <a class=\"anchor\" id=\"Algorithm\" ><\/a>","82913fe5":"## 5.4 Undersampling <a class=\"anchor\" id=\"undersampling\" ><\/a>\n\n* Randomly delete examples in the majority class.Random undersampling involves randomly selecting examples from the majority class and deleting them from the training dataset.\n* Hyperpamater reference from [random oversampling and undersampling for imbalanced classification][1]:\n* sampling_strategy = 0.5: \n   For example, we set sampling_strategy to 0.5 in an imbalanced data dataset with 1,000 examples in the majority class and 100 examples in the minority class, then there would be 200 examples for the majority class in the transformed dataset (or 100\/200 = 0.5) .\n\n**REFERENCE:**\n* [Random Oversampling and Undersampling for Imbalanced Classification][1]\n\n\n[1]: https:\/\/machinelearningmastery.com\/random-oversampling-and-undersampling-for-imbalanced-classification\/","1830a226":"## 5.7 Hubber regressor  <a class=\"anchor\" id=\"hubber\" ><\/a>\n\n* As another approach for handling outlier of the data, we can use another loss function. This function has reduced errors (from linear regressio) to  to help smoothing the graph thereby reducing loss. In other words, it doesn\u2019t detect and clean the outliers. Instead, it reduces the impact that outliers will have in the model.\n\n    **Example: ** \n*     When I use loss function MSE(mean square errors) with an exponent of 2.  if an outlier has an error of 10, the squared error for that instance will be 100.\n*     If I use Minkowski, the error has an exponent of 1.5, then total error ~ 31.62.\n*     The model trained with sum squared error is plotted[1] in the below figure.\n![](https:\/\/www.kdnuggets.com\/wp-content\/uploads\/outlier-detection-6.png)    \n    \n    The model trained with minkowski is  plotted[1] in the next figure.\n\n![](https:\/\/www.kdnuggets.com\/wp-content\/uploads\/outlier-detection-7.png)\n\n**SUMMARY**: Changing loss function can improved the quality of our model notably.\n\n* The Huber loss is a loss function used in robust regression, that is less sensitive to outliers in data than the squared error loss. \n* The [image][2] bellow shows a formula of Hubber loss\n\n![](https:\/\/wikimedia.org\/api\/rest_v1\/media\/math\/render\/svg\/e384efc4ae2632cb0bd714462b7c38c272098cf5)\n![](https:\/\/wikimedia.org\/api\/rest_v1\/media\/math\/render\/svg\/21983befe82b2509d1bb8dfa1064a35b6031d508)\n\n* When there is a big difference between the output and the actual output, it will deduct a quantity, which reduces the loss function to make the model better.\n* Now I mix oversampling, undersampling, Hubber regressor and k-ford cross validation I get 1.00 roc_auc score.\n\n**REFERENCE:** \n* [There method dealing outlier][1]\n* [Hubber loss][2]\n\n[1]: https:\/\/www.kdnuggets.com\/2017\/01\/3-methods-deal-outliers.html\n[2]: https:\/\/en.wikipedia.org\/wiki\/Huber_loss","24b861ef":"## 3.3 Randomforest calculation on impact of feature to outcomes <a class=\"anchor\" id=\"randomforest\" ><\/a> \n* An alternative way to analyze how much impact a feature could make on outcomes is to use Random Forest Classifier.","9465600d":"* Using outliers detect function to deal with outlier from origin_data","cfb8b6c2":"# IV  Outliers detection <a class=\"anchor\" id=\"outliers detection\" ><\/a>\n","0df6caf6":"## 5.1. k-fold cross validation <a class=\"anchor\" id=\"k-fold\" ><\/a>\n \n * This technique split training data to k part. For each running of the algorithm, It chooses one part to be the test set and k-1 part remaining to train data. \n \n * Following the [plot][1] below, We can understand how the algorithm processes the data.\n\n![](https:\/\/miro.medium.com\/max\/4984\/1*me-aJdjnt3ivwAurYkB7PA.png)\n\n* The data are split with k=10. And the total error is equal to mean k times testing on the valid test. \n* In my nootebook, I use RepeatedStratifiedKFold to run the k-fold cross validation.\n    \n * Hyperparameter of the model:\n\n    * n_splits = 10 : the paramater is k in k-fold cross validation. \n    * n_repeats = 3 : Repeat 3 times of data splitting, It means we run with 30 different models.\n    * scoring = roc_auc : This is Area Under the ROC Curve metrics.\n    * n_job = -1: use maximum CPU.\n    \n. \n\n[1]: https:\/\/medium.com\/@sebastiannorena\/some-model-tuning-methods-bfef3e6544f0","4bdc9a67":"# *Set up the notebook*","4268b3da":"## 5.3 Oversampling <a class=\"anchor\" id=\"oversampling\" ><\/a>\n\n* Randomly duplicate examples in the minority class. Random oversampling involves randomly selecting examples from the minority class, with replacement, and adding them to the training dataset[1](https:\/\/machinelearningmastery.com\/random-oversampling-and-undersampling-for-imbalanced-classification\/)\n* Hyperpamater reference from [random oversampling and undersampling for imbalanced classification][1]:\n* sampling_strategy = 0.5: \n    This would ensure that the minority class was oversampled to have half the number of examples as the majority class, for binary classification problems. This means that if the majority class had 1,000 examples and the minority class had 100, the transformed dataset would have 500 examples of the minority class[1].\n    \n\n  **REFERENCE:**\n * [Random Oversampling and Undersampling for Imbalanced Classification][1]\n\n\n[1]: https:\/\/machinelearningmastery.com\/random-oversampling-and-undersampling-for-imbalanced-classification\/","fb9e2ea0":"## 5.2 Use class weight in classifier <a class=\"anchor\" id=\"classweight\" ><\/a>\n\n* In the first algorithm, I think we should use class weighting for this problems. Apply a cost penalty on the minority class misclassification. For calculate the class weight, I use compute_class_weight from sklearn ","4d68366d":"# Table of Contents\n\n### [INTRODUCTION](#introduction)","a385c754":"#  [I. What metric should be used for the problem?](#choose-metric)\n   \n   * 1.1. [Why is it important to pick a good metric?](#important-choose)\n   * 1.2. [Receiver Operating Characteristic (ROC) metric](#roc)\n\n#  [II. Loading and checking data](#load-data)\n\n#  [III. Visualization and Feature extraction ](#visualization)\n \n   * 3.1. [Analysis of the features 'Time' and 'Amount'](#plot-time-amount)\n   * 3.2. [Log transformatio](#logtransform)\n   * 3.3. [Randomforest calculation on impact of feature to outcomes](#randomforest)\n\n#  [IV.  Outliers detection](#outliers detection)\n   \n   * 4.1. [Why do we use Tukey's method && adjusted boxplot ?](#tukey_adjusted) \n   * 4.2. [TUKEY\u2019S METHOD](#tukey)\n   * 4.3. [Adjusted boxplot](#Adjustedboxplot)\n   * 4.4. [How to calculate medcouple measure ?](#MC measure)\n   \n#  [V. Algorithm](#Algorithm)\n   \n   * 5.1. [k-fold cross validation](#k-fold)\n   * 5.2. [Use class weight in classifier](#classweight)\n   * 5.4. [Undersampling](#undersampling)\n   * 5.5. [Mixing oversampling and undersampling](#mix)\n   * 5.6. [Ensemble modeling + oversampling + undersamping](#Ensemble)\n   * 5.7. [Hubber regressor](#hubber)\n\n","41468401":"I have read a lot of comments said: the time feature does not affect the outcomes. Random forests, however, apparently show that there is some impacts of the features on the outcomes. In addition, we can see that the feature values does not vary widely (most values are in the range of 0.01 ~ 0.008). So I decided not to drop any features.","7cf29216":"## 5.6 Ensemble modeling + oversampling + undersamping <a class=\"anchor\" id=\"Ensemble\" ><\/a>\n* Ensemble Logistic regression, Randomforest and GaussianNB.\n* I set voting = 'soft', because The outcomes we need are the probabilities of data fall in the class, then we compute the roc_auc from that probability.","f403d848":"# I. What metric should be used for the problem? <a class=\"anchor\" id=\"choose-metric\" ><\/a>\n*  This problem has a highly practical application. So the processing needs to be continuously optimized (in current and future). The main purpose of this notebook is to solve data **imbalance**. The data is heavily imbalanced (~ 0.17% label 1, 99.83% label 0).\n\n# 1.1. Why is it important to pick a good metric?<a class=\"anchor\" id=\"important-choose\" ><\/a>\n\n   * Let's assume that I have 9 class 0 and 1 class 1. In this case, my model predict 0 for all and I have  TP = 9, FP = 1, TN = 0, FN = 0.\n   * It's easy to calculate: Precision = 0.9, recall = 1.0.\n   * In this case the precision and recall are both very high, but we have a poor classifier.[1]   \n\n**In conclusion, metric is very important since it heavily affects the model.**\n# 1.2.  Receiver Operating Characteristic (ROC) metric <a class=\"anchor\" id=\"roc\" ><\/a>\n\n* [The plot from sklearn ROC Curve.][3] \n\n![Image ROC curve](https:\/\/scikit-learn.org\/stable\/_images\/sphx_glr_plot_roc_001.png)\n\n* In the plot above, the X-axis features the False Positive Rate(FPR) and Y-axis  features the True Positive Rate(TPR). As we can see, the TPR is higher than the FPR, so we can then confirm that this model is good. The top left conrner of the plot is also a good point, because The model has TPR = 1 and FPR = 0. It means that a model with a large area under the curve (ROC) is good. Hence, it is called \"Area Under the ROC Curve\".\n\nSummary: \nFor this problem, I decide to use \"Area Under the ROC Curve (ROC_AUC)\" as the metric.\n\n\n**REFERENCE: **\n* [DEALING WITH IMBALANCED DATA: UNDERSAMPLING, OVERSAMPLING AND PROPER CROSS-VALIDATION][2] \n* [What metrics should be used for evaluating a model on an imbalanced data set?][1]\n\n[1]: https:\/\/towardsdatascience.com\/what-metrics-should-we-use-on-imbalanced-data-set-precision-recall-roc-e2e79252aeba\n\n[2]: https:\/\/www.marcoaltini.com\/blog\/dealing-with-imbalanced-data-undersampling-oversampling-and-proper-cross-validation\n[3]: https:\/\/scikit-learn.org\/stable\/auto_examples\/model_selection\/plot_roc.html\n","999c7a51":"First of all, all the columns are numeric. The data overview shows that Time and Amount have a very high mean compared to other features. This greatly affects to the outcome of the model. So the two features should be transformed before training. \n","eb07a8ac":"## 5.5 Mixing oversampling and undersampling <a class=\"anchor\" id=\"mix\" ><\/a>","cbf3a1bb":"# Introduction <a class=\"anchor\" id=\"introduction\" ><\/a>\nHello everyone!\n\nThis is my first kernel. I have just started to join Kaggle and do not have much experience in the field. I hope everyone can contribute ideas to make the kernel more efficient. If you like my notebook, please click upvote for me. Thank you!","237d714e":"* After visualization, the Features = ['Time','Amount'] has large values while the distribution of Amount is highly skewed. Because all values in Features are numerical and positive, a logarithm transformation is possible. It can help to fit a skewed distribution and reduce the impact on other features. The shape of the transformed graph was not different from the original data as the logarithm function is an increasing function."}}