{"cell_type":{"29b32957":"code","435e6575":"code","f94b9ffe":"code","aa0a486f":"code","abb46d5b":"code","59d01d84":"code","9cce0cab":"code","13164832":"code","588e6471":"code","ea23f43c":"code","ffdf0049":"code","e77a1076":"code","d2cd88f4":"markdown"},"source":{"29b32957":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","435e6575":"data = pd.read_csv('..\/input\/train.csv')","f94b9ffe":"data.head()","aa0a486f":"import matplotlib.pyplot as plt\nimport seaborn as sns\nk = 10\ncols = data.corr().nlargest(k,'SalePrice')['SalePrice'].index\nsns.set(font_scale=1.25)\nfig, ax = plt.subplots(figsize=(20,15))\nsns.heatmap(data[cols].corr(),annot=True, fmt='.2f', annot_kws={'size': 10}, yticklabels=cols.values, xticklabels=cols.values,ax=ax)","abb46d5b":"sns.set()\ncols = ['SalePrice','OverallQual','GrLivArea', 'GarageCars','TotalBsmtSF', 'FullBath', 'TotRmsAbvGrd', 'YearBuilt']\nsns.pairplot(data[cols], size = 2.5)\nplt.show()","59d01d84":"\ncols = ['OverallQual','GrLivArea', 'GarageCars','TotalBsmtSF', 'FullBath', 'TotRmsAbvGrd', 'YearBuilt']\nx = data[cols].values\ny = data['SalePrice'].values\nfrom sklearn.model_selection import train_test_split\nx_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.33, random_state=42)","9cce0cab":"from sklearn import preprocessing\n\nx_train = preprocessing.StandardScaler().fit_transform(x_train)\n#y_train = preprocessing.StandardScaler().fit_transform(y_train.reshape(-1,1))\n\nx_test = preprocessing.StandardScaler().fit_transform(x_test)\n#y_test = preprocessing.StandardScaler().fit_transform(y_test.reshape(-1,1))","13164832":"from keras.models import Sequential\nfrom keras.layers import Dense,Dropout\n\ndef build_model():\n    model = Sequential()\n    model.add(Dense(128,activation='relu',input_shape=(x_train.shape[1],)))\n    model.add(Dense(64,activation='relu'))\n    model.add(Dense(32,activation='relu'))\n    model.add(Dense(16,activation='relu'))\n    model.add(Dense(8,activation='relu'))\n    model.add(Dense(4,activation='relu'))\n    model.add(Dense(1))\n    model.compile(optimizer='rmsprop',loss='mse',metrics=['mae'])\n    return model","588e6471":"model = build_model()\nhistory = model.fit(x_train,y_train,epochs=100,batch_size=32,validation_data=(x_test,y_test))\nimport matplotlib.pyplot as plt\ndef show(history,loss,val_loss,label):\n    epochs = range(1,len(history.history[loss])+1)\n    plt.plot(epochs,history.history[loss],label=label)\n    plt.plot(epochs,history.history[val_loss],label='Validation '+label)\n    plt.title(label)\n    plt.legend()\n","ea23f43c":"plt.figure(figsize=(25,8))\nplt.subplot(121)\nshow(history,'mean_absolute_error','val_mean_absolute_error','mean_absolute_error')\nplt.subplot(122)\nshow(history,'loss','val_loss','loss')\n\nplt.show()","ffdf0049":"test_origin = pd.read_csv('..\/input\/test.csv')\ntest = test_origin[cols]\n#Fill the null data\ntest['GarageCars'].fillna(1.766118, inplace=True)\ntest['TotalBsmtSF'].fillna(1046.117970, inplace=True)\ntest = preprocessing.StandardScaler().fit_transform(test)\npred = model.predict(test)\n\norigin_pred = pd.DataFrame(pred,columns=['SalePrice'])\nresult = pd.concat([test_origin['Id'], origin_pred], axis=1)\nresult","e77a1076":"result.to_csv('.\/Predictions.csv', index=False)","d2cd88f4":"# This result is bad and gives us a mean absolute error just above 20000 dollars.\n## In my Opinion,Because Deep learning need big datasets."}}