{"cell_type":{"46e8d94e":"code","06608bd1":"code","979a2af2":"code","ead47346":"code","c0a8ed2b":"code","444892a4":"code","7aeee3c7":"code","69bad635":"code","e2afe746":"code","be509e12":"code","8cccdc90":"code","555fba74":"code","9eb50018":"code","e26637e0":"code","e6445297":"code","787dda5e":"code","20c23fdb":"code","289dee81":"code","9f55850d":"code","caa6e045":"code","adf953b2":"code","68a3a9a4":"code","6270e65c":"code","1b42235b":"code","ddd46682":"code","cedd9983":"code","c7810933":"code","fa20b5e4":"code","43da40e1":"code","bcd001f8":"code","f78b2fa5":"code","3f5e19a1":"code","a1d40e39":"code","4ef4ec14":"code","462a1158":"code","2b8c1934":"code","8932aac0":"markdown","61ce395a":"markdown","b2daf968":"markdown","646adfed":"markdown","203d9370":"markdown","cfd2a8bf":"markdown","9d5c9be6":"markdown","4647993d":"markdown","2eeae670":"markdown","51b47338":"markdown","deea8d7c":"markdown","75bd960b":"markdown","ff916f9e":"markdown","63aeedce":"markdown","c955b726":"markdown","b8eaf7b3":"markdown","70a1ab06":"markdown","fa676e9e":"markdown","0cd8e928":"markdown","18ca1d70":"markdown","339b1db7":"markdown","067a2d07":"markdown","a282aee9":"markdown","0dc07900":"markdown","0d788864":"markdown","a554ee61":"markdown","e155dfbd":"markdown","a177a7a4":"markdown","597ffd1e":"markdown","32115d33":"markdown","f861af0d":"markdown","554f07fc":"markdown","aeb7a81c":"markdown","e80259d9":"markdown","9e7f5c90":"markdown"},"source":{"46e8d94e":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.metrics import mean_absolute_error as mae\nfrom sklearn.preprocessing import RobustScaler, normalize, StandardScaler\nfrom sklearn.model_selection import train_test_split, GroupKFold, KFold\n\n\npath = '..\/input\/ventilator-pressure-prediction'\ntrain = pd.read_csv(f\"{path}\/train.csv\")\ntrain[\"time_step_id\"] = list(range(1,81,1)) * int(len(train)\/80)\nrange_bins = [0, 5, 10, 15, 20, 25, 30, 35, 40, 45, 50, 55, 60, 65,70,75,80,85,90,95,100]\nbins_name = ['~5', '~10', '~15','~20','~25', '~30', '~35', '~40','~45', '~50', '~55', '~60','~65','~70','~75','~80','~85','~90','~95','~100']\ntrain[\"u_in_range\"] = pd.cut(train[\"u_in\"],bins=range_bins,labels=bins_name)\ntmp_df = pd.DataFrame()\nfor k,grp in train.groupby(\"time_step_id\"):\n    tmp_df = tmp_df.append(grp[\"u_in_range\"].value_counts())\ntmp_df = tmp_df.reset_index(drop=True)\ntmp_df.columns = tmp_df.columns.astype(str)\ntmp_df = tmp_df.reindex(columns=bins_name)\nax = sns.heatmap(tmp_df)\nax.set_xlabel(\"u_in_range\", fontsize = 20)\nax.set_ylabel(\"time_step_id\", fontsize = 20)\nplt.show()","06608bd1":"train = pd.read_csv(f\"{path}\/train.csv\")\ncolumns = train.columns\n#scaler = RobustScaler()\nscaler = StandardScaler()\ntrain = scaler.fit_transform(train)\ntrain = pd.DataFrame(train,columns= columns)\ntrain[\"time_step_id\"] = list(range(1,81,1)) * int(len(train)\/80)\nrange_bins = [-1.8, -1.6, -1.4, -1.2, -1.0, -0.8, -0.6, -0.4,-0.2,0,0.2,0.4,0.6,0.8,1.0,1.2,1.4,1.6,1.8]\nbins_name = ['~-1.6', '~-1.4', '~-1.2','~-1.0','~-0.8','~-0.6','~-0.4','~-0.2','~0','~0.2','~0.4','~0.6','~0.8','~1.0','~1.2','~1.4','~1.6','~1.8']\ntrain[\"pressure_range\"] = pd.cut(train[\"pressure\"],bins=range_bins,labels=bins_name)\ntmp_df = pd.DataFrame()\nfor k,grp in train.groupby(\"time_step_id\"):\n    tmp_df = tmp_df.append(grp[\"pressure_range\"].value_counts())\ntmp_df = tmp_df.reset_index(drop=True)\ntmp_df.columns = tmp_df.columns.astype(str)\ntmp_df = tmp_df.reindex(columns=bins_name)\nax = sns.heatmap(tmp_df)\nax.set_xlabel(\"pressure_range\", fontsize = 20)\nax.set_ylabel(\"time_step_id\", fontsize = 20)\nplt.show()","979a2af2":"train = pd.read_csv(f\"{path}\/train.csv\")\ncolumns = train.columns\nscaler = RobustScaler()\n#scaler = StandardScaler()\ntrain = scaler.fit_transform(train)\ntrain = pd.DataFrame(train,columns= columns)\ntrain[\"time_step_id\"] = list(range(1,81,1)) * int(len(train)\/80)\n#range_bins = [-0.9, -0.8, -0.7, -0.6, -0.5, -0.4, -0.3, -0.2,-0.1,0,0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9,1.0]\nrange_bins = [-9, -8, -7, -6, -5, -4, -3, -2,-1,0,1,2,3,4,5,6,7,8,9,10]\nbins_name = ['~-8', '~-7','~-6','~-5','~-4','~-3','~-2','~-1','~0','~1','~2','~3','~4','~5','~6','~7','~8','~9','10']\n#train[\"pressure_range\"] = pd.cut(train[\"pressure\"],bins=range_bins,labels=bins_name)\ntrain[\"u_in_range\"] = pd.cut(train[\"u_in\"],bins=range_bins,labels=bins_name)\ntmp_df = pd.DataFrame()\nfor k,grp in train.groupby(\"time_step_id\"):\n    #tmp_df = tmp_df.append(grp[\"pressure_range\"].value_counts())\n    tmp_df = tmp_df.append(grp[\"u_in_range\"].value_counts())\ntmp_df = tmp_df.reset_index(drop=True)\ntmp_df.columns = tmp_df.columns.astype(str)\ntmp_df = tmp_df.reindex(columns=bins_name)\nax = sns.heatmap(tmp_df)\nax.set_xlabel(\"u_in_range\", fontsize = 20)\nax.set_ylabel(\"time_step_id\", fontsize = 20)\nplt.show()","ead47346":"#!pip install joypy\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport plotly.express as px\nimport plotly.graph_objects as go\n#from joypy import joyplot for matplotlib\nimport tensorflow as tf\nfrom tqdm import tqdm\nimport optuna\n\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras.callbacks import EarlyStopping\nfrom tensorflow.keras.callbacks import LearningRateScheduler\nfrom tensorflow.keras.optimizers.schedules import ExponentialDecay\n\nfrom sklearn.metrics import mean_absolute_error as mae\nfrom sklearn.preprocessing import RobustScaler, normalize, StandardScaler\nfrom sklearn.model_selection import train_test_split, GroupKFold, KFold\n\ntqdm.pandas()\n\n%matplotlib inline","c0a8ed2b":"if tf.device('\/CPU:0'):\n    print(tf.device('\/CPU:0'))","444892a4":"# When Debug mode is true, this notebook use small train data.\ndebug_mode = True\ndebug_data_count = int(1000)\n\ntqdm.pandas()\n\n# folloiwng settig takes time due to plotly is so slow.\nvisualize = False  # [True | False]\nvisualize2 = False\n\n# Use trained model. no run training. Just use trained model only.\n\nuse_trained_model = True","7aeee3c7":"path = '..\/input\/ventilator-pressure-prediction'\ntrain = pd.read_csv(f\"{path}\/train.csv\")\ntest = pd.read_csv(f\"{path}\/test.csv\")\nsubmission = pd.read_csv(f'{path}\/sample_submission.csv')","69bad635":"if visualize:\n    time_step_diff_limit = 0.04\n    non_liner_timestep_breath_ids = list()\n    for k, grp in tqdm(train.groupby(\"breath_id\")):\n        diff_se = grp[\"time_step\"].diff()\n        diff_chk = diff_se[diff_se > time_step_diff_limit]\n        if len(diff_chk) != 0:\n            non_liner_timestep_breath_ids.append(k)\n#\n#print(non_liner_timestep_breath_ids)\n## results are following:\n## [803, 2327, 3178, 4199, 5830, 10277, 11502, 13238, 15803, 16315, 16634, 18117, 18600, 24127, 25397, 28189, 28942, 30181, 32296, 36128, 36175, 37711, 38237, 38415, 39045, 39722, 42317, 42988, 43344, 44245, 45197, 46324, 49849, 53877, 54129, 55244, 55851, 61454, 64662, 67422, 67748, 72104, 74766, 76037, 78768, 79105, 80375, 87127, 87776, 89084, 91883, 93186, 98677, 102063, 104001, 106034, 107067, 109693, 111439, 112027, 115588, 119689, 120878, 121135, 125136]","e2afe746":"if visualize:\n    non_liner_timestep_df = train[train[\"breath_id\"].isin(non_liner_timestep_breath_ids)]\n    fig = go.Figure()\n    for k,grp in non_liner_timestep_df.groupby(\"breath_id\"):\n        grp = grp.reset_index(drop=True)\n        fig.add_trace(go.Scatter(x=grp.index, y=grp[\"time_step\"], mode='lines', name=k))\n    fig.show()","be509e12":"if visualize:\n    liner_timestep_df = train[~train[\"breath_id\"].isin(non_liner_timestep_breath_ids)]\n    fig = go.Figure()\n    for k,grp in tqdm(liner_timestep_df[:80*10000].groupby(\"breath_id\")):\n        grp = grp.reset_index(drop=True)\n        fig.add_trace(go.Scatter(x=grp.index, y=grp[\"time_step\"], mode='lines', name=k))\n    fig.show()","8cccdc90":"if visualize:\n    minus_pressure_breath_ids = list()\n    for k, grp in tqdm(train.groupby(\"breath_id\")):\n        m = grp[\"pressure\"].min()\n        if m < 0:\n            minus_pressure_breath_ids.append(k)\n# print(minus_pressure_breath_ids)\n# [542, 851, 3928, 7949, 11216, 13594, 16599, 19236, 20075, 22164, 23710, 27195, 27731, 30127, 36474, 40431, 40753, 43103, 43630, 44309, 45099, 45681, 45877, 46018, 46020, 46486, 47325, 49376, 49941, 50459, 52137, 53057, 54206, 56152, 56760, 57119, 58835, 59101, 60949, 65596, 67080, 67788, 70753, 71461, 72011, 74977, 77803, 83713, 85391, 86508, 90584, 91132, 91464, 92955, 94037, 97520, 98041, 98080, 101951, 106703, 108406, 109424, 109761, 110499, 111419, 112036, 113323, 113639, 118131, 119582, 120445, 121570, 124575]","555fba74":"if visualize:\n    minus_pressure_df = train[train[\"breath_id\"].isin(minus_pressure_breath_ids)]\n    minus_pressure_df_plotly = pd.melt(minus_pressure_df,id_vars=[\"time_step\",\"breath_id\"], value_vars=[\"pressure\"])\n    fig = px.line(minus_pressure_df_plotly, x=\"time_step\" , y=\"value\",color = \"variable\",line_group =\"breath_id\")\n    for line in fig.data:\n        line['line']['color']='rgba(0, 0, 255, 0.1)'\n    fig.show()","9eb50018":"if visualize:\n    u_out_open_step_counts = list()\n    for k, grp in tqdm(train.groupby(\"breath_id\")):\n        count = grp.groupby(\"u_out\")[\"id\"].count()[1]\n        u_out_open_step_counts.append(count)\n    \n    u_out_open_step_counts_test = list()\n    for k, grp in tqdm(test.groupby(\"breath_id\")):\n        count = grp.groupby(\"u_out\")[\"id\"].count()[1]\n        u_out_open_step_counts_test.append(count)","e26637e0":"if visualize: \n    fig = px.histogram(x=u_out_open_step_counts,nbins=8)\n    fig.update_layout(title=\"u_out = 1 count histgram in train\")\n    fig.show()","e6445297":"if visualize:\n    fig = px.histogram(x=u_out_open_step_counts_test,nbins=8)\n    fig.update_layout(title=\"u_out = 1 count histgram in test\")\n    fig.show()","787dda5e":"if visualize: \n    u_out_open_step_counts_over52 = list()\n    for k, grp in tqdm(train.groupby(\"breath_id\")):\n        count = grp.groupby(\"u_out\")[\"id\"].count()[1]\n        if count > 51:\n            u_out_open_step_counts_over52.append(count)\n    len(u_out_open_step_counts_over52)","20c23fdb":"if visualize:\n    train = train[:80*1000]\n    train = train[train[\"u_out\"] == 1]\n    pressure_df_uout = pd.melt(train,id_vars=[\"time_step\",\"breath_id\"], value_vars=[\"pressure\"])\n    fig = px.line(pressure_df_uout, x=\"time_step\" , y=\"value\",color = \"variable\",line_group =\"breath_id\")\n    for line in fig.data:\n        line['line']['color']='rgba(0, 0, 255, 0.05)'\n    fig.show()\n    ","289dee81":"if visualize2:\n    u_out_close_step_counts = list()\n    for k, grp in tqdm(train.groupby(\"breath_id\")):\n        count = grp.groupby(\"u_out\")[\"id\"].count()[0]\n        u_out_close_step_counts.append(count)\n    \n    u_out_close_step_counts_test = list()\n    for k, grp in tqdm(test.groupby(\"breath_id\")):\n        count = grp.groupby(\"u_out\")[\"id\"].count()[0]\n        u_out_close_step_counts_test.append(count)","9f55850d":"if visualize2: \n    fig = px.histogram(x=u_out_close_step_counts,nbins=8)\n    fig.update_layout(title=\"u_out = 0 count histgram in train\")\n    fig.show()","caa6e045":"if visualize2:\n    fig = px.histogram(x=u_out_close_step_counts_test,nbins=8)\n    fig.update_layout(title=\"u_out = 0 count histgram in test\")\n    fig.show()","adf953b2":"def data_clean(df):\n    ## pickup ignore breath id\n    ignore_breath_ids = set()\n    \n    \n    time_step_diff_limit = 0.04\n    for k, grp in tqdm(df.groupby(\"breath_id\")):\n        \n        ## ignore non liner time_step data\n        diff_se = grp[\"time_step\"].diff()\n        diff_chk = diff_se[diff_se > time_step_diff_limit]\n        if len(diff_chk) != 0:\n            ignore_breath_ids.add(k)\n            \n        ## ignor negative pressure data\n        m = grp[\"pressure\"].min()\n        if m < 0:\n            ignore_breath_ids.add(k)\n            \n        ## ignore (u_out = 0 step) < 29\n        #count0 = grp.groupby(\"u_out\")[\"id\"].count()[0]\n        #if count0 < 29:\n        #    ignore_breath_ids.add(k)\n    \n    df = df[~df[\"breath_id\"].isin(np.array(list(ignore_breath_ids)))]\n    return df\n\ndef change_type(df):\n    df['R'] = df['R'].astype(str)\n    df['C'] = df['C'].astype(str)\n    return df\n\ndef add_features(df):\n    df['u_in_cumsum'] = df.groupby('breath_id')['u_in'].cumsum()\n    df['u_in_lag1'] = df.groupby('breath_id')['u_in'].shift(1)\n    df['u_in_lag2'] = df.groupby('breath_id')['u_in'].shift(2)\n    df = df.fillna(0)\n    return df\n\n\ndef tf_tpu_or_gpu_or_cpu():\n    tpu = None\n    try:\n        tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n        print('Running on TPU ', tpu.master())\n    except ValueError:\n        tpu = None\n\n    if tpu:\n        tf.config.experimental_connect_to_cluster(tpu)\n        tf.tpu.experimental.initialize_tpu_system(tpu)\n        strategy = tf.distribute.experimental.TPUStrategy(tpu)\n        return \"tpu\"\n\n    elif tf.test.is_gpu_available():\n        strategy = tf.distribute.get_strategy()\n        print('Running on GPU')\n        return \"gpu\"\n\n    else:\n        strategy = tf.distribute.get_strategy()\n        print('Running on CPU')\n        return \"cpu\"\n","68a3a9a4":"def exhaust_mode_df(df):\n    grp_len = int(51)\n    new_df = pd.DataFrame()\n    for k, grp in df.groupby(\"breath_id\"):\n        tmp_df = grp[grp[\"u_out\"] == 1]\n        rowno = tmp_df.shape[0]\n        for l in range(grp_len - rowno):\n            tmp_df = tmp_df.append(grp.tail(1),ignore_index=True)\n        new_df = new_df.append(tmp_df,ignore_index=True)\n    return new_df","6270e65c":"def uout0_mode_df(df):\n    grp_len = int(32)\n    new_df = pd.DataFrame()\n    for k, grp in df.groupby(\"breath_id\"):\n        tmp_df = grp[grp[\"u_out\"] == 0]\n        rowno = tmp_df.shape[0]\n        if \"pressure\" in tmp_df.columns:\n            for l in range(grp_len - rowno):\n                tmp_df = tmp_df.append({\"\"},ignore_index=True)\n        else:\n            for l in range(grp_len - rowno):\n                tmp_df = tmp_df.append(grp.tail(1),ignore_index=True)\n        new_df = new_df.append(tmp_df,ignore_index=True)\n    return new_df","1b42235b":"path = '..\/input\/ventilator-pressure-prediction'\ntrain = pd.read_csv(f\"{path}\/train.csv\")\ntest = pd.read_csv(f\"{path}\/test.csv\")\nsubmission = pd.read_csv(f'{path}\/sample_submission.csv')","ddd46682":"if debug_mode:\n    if use_trained_model:\n        train = train[80*debug_data_count:80*debug_data_count*2]\n        test = test[80*debug_data_count:80*debug_data_count*2]\n    else:\n        train = train[:80*debug_data_count]\n        test = test[:80*debug_data_count]","cedd9983":"# exhaust_mode_df() do not work collectry\nu_out1_only = False\n\n\n## train\u306e\u307f\u3001data_clean\u3092\u5b9f\u65bd\ntrain = data_clean(train)  ## time_step\u304c\u30ea\u30cb\u30a2\u51fa\u306a\u3044\u3082\u306e\u306f\u524a\u9664\nif u_out1_only:\n    train = exhaust_mode_df(train)\ntrain = add_features(train)\ntrain = change_type(train)\n\nif u_out1_only:\n    test = exhaust_mode_df(test)\ntest = add_features(test)\ntest = change_type(test)","c7810933":"if u_out1_only:\n    targets = train[['pressure']].to_numpy().reshape(-1, 51)\n    train = train.drop(['pressure', 'id', 'breath_id','u_out'], axis=1)\n    test = test.drop(['id', 'breath_id','u_out'], axis=1)\nelse:\n    targets = train[['pressure']].to_numpy().reshape(-1, 80)\n    train = train.drop(['pressure', 'id', 'breath_id'], axis=1)\n    test = test.drop(['id', 'breath_id'], axis=1)\n","fa20b5e4":"#scaler = RobustScaler()\nscaler = StandardScaler()\ntrain = scaler.fit_transform(train)\ntest = scaler.transform(test)","43da40e1":"if u_out1_only:\n    train = train.reshape(-1, 51, train.shape[-1])\n    test = test.reshape(-1, 51, test.shape[-1])\nelse:\n    train = train.reshape(-1, 80, train.shape[-1])\n    test = test.reshape(-1, 80, test.shape[-1])","bcd001f8":"def run_tf_blstm(epoch=int(50),batch_size=int(1024),train=None,test=None,targets=None):\n    #kf = KFold(n_splits=5, shuffle=True, random_state=2000)\n    kf = KFold(n_splits=5, shuffle=True)\n    test_preds = []\n    test_history = []\n    for fold, (train_idx, test_idx) in enumerate(kf.split(train, targets)):\n        print('-'*15, '>', f'Fold {fold+1}', '<', '-'*15)\n        X_train, X_valid = train[train_idx], train[test_idx]\n        y_train, y_valid = targets[train_idx], targets[test_idx]\n        model = keras.models.Sequential([\n            #keras.layers.Embedding(input_dim=train.shape[-2:], output_dim=300, mask_zero=True),\n            keras.layers.Input(shape=train.shape[-2:]),\n            keras.layers.Bidirectional(keras.layers.LSTM(300, return_sequences=True)),          \n            keras.layers.Bidirectional(keras.layers.LSTM(250, return_sequences=True)),\n            keras.layers.Bidirectional(keras.layers.LSTM(150, return_sequences=True)),\n            keras.layers.Bidirectional(keras.layers.LSTM(100, return_sequences=True)),\n            keras.layers.Dense(50, activation='selu'),\n            keras.layers.Dense(1),\n        ])\n        model.compile(optimizer=\"adam\", loss=\"mae\")    \n        \n        scheduler = ExponentialDecay(1e-3, 400*((len(train)*0.8)\/batch_size), 1e-5)\n        lr = LearningRateScheduler(scheduler, verbose=1)\n\n        #es = EarlyStopping(monitor=\"val_loss\", patience=15, verbose=1, mode=\"min\", restore_best_weights=True)\n\n        history = model.fit(X_train, y_train, validation_data=(X_valid, y_valid), epochs=epoch, batch_size=batch_size, callbacks=[lr])\n        test_history.append(history.history)\n        model.save(f'model_save_fold{fold+1}')\n        #test_preds.append(model.predict(test).squeeze().reshape(-1, 1).squeeze())\n        test_preds.append(model.predict(test).squeeze())\n    \n    return test_preds, test_history\n\ndef run_tf_lstm(epoch=int(50),batch_size=int(1024),train=None,test=None,targets=None):\n    #kf = KFold(n_splits=5, shuffle=True, random_state=2000)\n    kf = KFold(n_splits=5, shuffle=True)\n    test_preds = []\n    test_history = []\n    for fold, (train_idx, test_idx) in enumerate(kf.split(train, targets)):\n        print('-'*15, '>', f'Fold {fold+1}', '<', '-'*15)\n        X_train, X_valid = train[train_idx], train[test_idx]\n        y_train, y_valid = targets[train_idx], targets[test_idx]\n        model = keras.models.Sequential([\n            #keras.layers.Embedding(input_dim=train.shape[-2:], output_dim=300, mask_zero=True),\n            keras.layers.Input(shape=train.shape[-2:]),\n            keras.layers.LSTM(300, return_sequences=True),\n            keras.layers.LSTM(250, return_sequences=True),\n            keras.layers.LSTM(150, return_sequences=True),\n            keras.layers.LSTM(100, return_sequences=True),\n            keras.layers.Dense(50, activation='selu'),\n            keras.layers.Dense(1),\n        ])\n        model.compile(optimizer=\"adam\", loss=\"mae\")    \n        \n        scheduler = ExponentialDecay(1e-3, 400*((len(train)*0.8)\/batch_size), 1e-5)\n        lr = LearningRateScheduler(scheduler, verbose=1)\n\n        #es = EarlyStopping(monitor=\"val_loss\", patience=15, verbose=1, mode=\"min\", restore_best_weights=True)\n\n        history = model.fit(X_train, y_train, validation_data=(X_valid, y_valid), epochs=epoch, batch_size=batch_size, callbacks=[lr])\n        test_history.append(history.history)\n        model.save(f'model_save_fold{fold+1}')\n        result = model.predict(test)\n        #print(result.shape) #(50300, 80, 1)\n        #print(result.squeeze().shape)  #(50300, 80)\n        #print(result.squeeze().reshape(-1,1).shape) #(4024000, 1)\n        #test_preds.append(result.squeeze().reshape(-1, 1).squeeze())\n        test_preds.append(result.squeeze())\n    \n    return test_preds, test_history","f78b2fa5":"EPOCH = 200\nBATCH_SIZE = 1024\ntest_preds = list()\n\ndevice = tf_tpu_or_gpu_or_cpu()\n\nif use_trained_model:\n    pass\nelse:\n    if device == \"cpu\" :\n        test_preds,history = run_tf_blstm(epoch=EPOCH,batch_size=BATCH_SIZE,train=train,test=test,targets=targets)\n\n    elif device == \"gpu\":\n        test_preds,history = run_tf_blstm(epoch=EPOCH,batch_size=BATCH_SIZE,train=train,test=test,targets=targets)\n    elif device == \"tpu\":\n        try:\n            with tpu_strategy.scope():\n                test_preds,history = run_tf_blstm(epoch=EPOCH,batch_size=BATCH_SIZE,train=train,test=test,targets=targets)\n        except :\n            print('Error')","3f5e19a1":"if use_trained_model:\n    pass\nelse:\n    preds = np.sum(test_preds, axis=0).tolist()","a1d40e39":"def add_pressure_data(df,preds):\n    index = 0\n    new_df = pd.DataFrame()\n    for k,grp in df.groupby(\"breath_id\"):\n        u_out_len = len(grp[grp[\"u_out\"] == 1])\n        if u_out_len < 52:\n            grp.loc[grp[\"u_out\"] == 1,\"pressure\"] = preds[index][:u_out_len]\n        else:\n            out_preds = preds[index]\n            for l in range(u_out_len - 51):\n                out_preds = out_preds.append(out_preds[-1])\n            grp.loc[grp[\"u_out\"] == 1,\"pressure\"] = out_preds\n        new_df = new_df.append(grp,ignore_index=True)\n        index += 1\n    return new_df","4ef4ec14":"if debug_mode:\n    pass\nelse:\n    submission[\"pressure\"] = preds\n    submission.to_csv('submission.csv', index=False)","462a1158":"if use_trained_model:\n    model = tf.keras.models.load_model('..\/input\/update-simple-lstm-simple-data-model\/update-simple-lstm-simple-data\/model_save_fold5')\n    result = model.predict(train)\n    result = result.squeeze()","2b8c1934":"## res and tgt is numpy array\nfor res, tgt in zip(result,targets):\n    abs_diff =  np.sum(np.abs(res - tgt))\n    if abs_diff > 500:\n        df = pd.DataFrame(res)\n        df = pd.concat([df, pd.DataFrame(tgt)], axis=1)\n        df.columns = [\"res\",\"tgt\"]\n        df[\"id\"] = df.index\n        df = pd.melt(df,id_vars=[\"id\"])\n        plt.figure()\n        fig = sns.lineplot(data=df, x='id', y=\"value\", hue=\"variable\")\n        fig.set_title(f\"abs_diff : {abs_diff}\")\n    elif abs_diff < 30:\n        df = pd.DataFrame(res)\n        df = pd.concat([df, pd.DataFrame(tgt)], axis=1)\n        df.columns = [\"res\",\"tgt\"]\n        df[\"id\"] = df.index\n        df = pd.melt(df,id_vars=[\"id\"])\n        plt.figure()\n        fig = sns.lineplot(data=df, x='id', y=\"value\", hue=\"variable\")\n        fig.set_title(f\"abs_diff : {abs_diff}\")","8932aac0":"# Settings","61ce395a":"# \u5165\u529b\u30c7\u30fc\u30bf\u306e\u30c7\u30fc\u30bf\u305d\u306e\u3082\u306e\u306e\u5909\u5f62\n\n### RobustScaler\u306b\u3064\u3044\u3066\n\u5165\u529b\u30c7\u30fc\u30bf\u306e\u5206\u5e03\u306b\u5fdc\u3058\u3067\u5909\u5f62\u3002\u5916\u308c\u5024\u306b\u5f37\u304f\u3057\u305f\u308a\u3001\u6b63\u898f\u5206\u5e03\u306b\u8fd1\u304f\u3057\u305f\u308a\u3067\u304d\u308b\u3002<br>\nhttps:\/\/helve-blog.com\/posts\/python\/scikit-learn-feature-scaling\/\n<br><br>\n### numpy bload cast error \u306b\u3064\u3044\u3066\n\u539f\u56e0\u306f\u3001train\u3068test\u306e\u30c7\u30fc\u30bf\u306e\u5e45\u304c\u9055\u3063\u305f\u304b\u3089\u3002<br>\nRobustScaler\u306efit\u3067\u5024\u3092\u5909\u63db\u3059\u308b\u305d\u306e\u5185\u5bb9\u3092\u6c7a\u5b9a\u3057\u3001transform\u3067\u5024\u306e\u5909\u66f4\u3092\u5b9f\u65bd\u3059\u308b\u3002<br>\n\u305d\u306e\u305f\u3081\u3001fit\u3057\u305f\u3068\u304d\u306ecolumn\u306e\u5e45\u3068\u3001transform\u3059\u308b\u3068\u304d\u306ecolumn\u306e\u5e45\u304c\u63c3\u308f\u306a\u3044\u3068\u3060\u3081\u3002<br>\nhttps:\/\/www.headboost.jp\/numpy-array-broadcasting\/\n<br><br>","b2daf968":"# \u30c7\u30fc\u30bf\u8aad\u307f\u8fbc\u307f\u3068config\u8a2d\u5b9a","646adfed":"# Train \u3068 Test\n\n## \u6226\u7565\n* full randam\u3067\u30ed\u30d0\u30b9\u30c8\u6027\u3092\u72d9\u3046\u3002\u5916\u308c\u5024\u306f\u5916\u3057\u305f\u30c7\u30fc\u30bf\u306a\u306e\u3067\u3001randam\u3067\u632f\u3089\u308c\u3066\u3082\u5b89\u5b9a\u3059\u308b\u306f\u305a\u3002\n* model\u3092save\u3002\u5f8c\u3067\u7b54\u3048\u3068\u3069\u308c\u3060\u3051\u305a\u308c\u3066\u3044\u308b\u304b\u898b\u308b\u3002\n* <s>use only u_out = 1 data.<\/s>\n\nA.I<br>\ncheck ","203d9370":"# apply utilitys for data\n\u30c7\u30fc\u30bf\u306e\u8ffd\u52a0\u3068\u524a\u9664\u304c\u30e1\u30a4\u30f3","cfd2a8bf":"# confirm linearity of time_step","9d5c9be6":"# run tf function","4647993d":"# visualize by histgram counts of u_out = 1 (\u6392\u6c17\u6bb5\u968e)\u306estep\u6570\u306e\u5206\u5e03\u3092\u53ef\u8996\u5316\n48 ~ 51\u306b\u307e\u3068\u307e\u3063\u3066\u3044\u308b\u3002\u88fe\u306e\u306f\u300155\u307e\u3067\u3002","2eeae670":"# Load Pre Train model","51b47338":"# count steps of u_out = 0 step","deea8d7c":"# re shape prediction results","75bd960b":"# \u6392\u6c17\u6642\u306e\u307f\u306e\u30c7\u30fc\u30bf\u4f5c\u6210\n\nu_out\u306estep\u6570\u6700\u5927(\u4eca\u56de\u306f51)\u3067cut\u3002\u77ed\u3044\u5834\u5408\u306f\u3001\u4e00\u756a\u6700\u5f8c\u306e\u30c7\u30fc\u30bf\u3092\u30b3\u30d4\u30fc\u3057\u306651\u306b\u5408\u308f\u305b\u308b\u3002","ff916f9e":"# Read data","63aeedce":"# data count of counts over 52 of u_out = 1 (\u6392\u6c17\u6bb5\u968e)\u306estep\u6570\u304c52\u4ee5\u4e0a\u306e\u30c7\u30fc\u30bf\u6570","c955b726":"# re read test.csv and add pred results to \"pressure\"\n\nIn \"u_out1_only = False\" case, do not need to run following function.<br>\nJust only for \"u_out1_only = True\" case.","b8eaf7b3":"# In non debug mode, create submission data","70a1ab06":"# visualize non linearity time_step (\u76f4\u7dda\u3067\u306a\u3044time_step\u306e\u53ef\u8996\u5316)","fa676e9e":"# \u7528\u8a9e\u96c6\n\n### twinx @ matplotlib\n\nmatplotlib\u3067\u5de6\u53f3\u306ex\u8ef8\u3092\u4f7f\u3046\u5ba3\u8a00\u3002\n\nhttps:\/\/qiita.com\/supersaiakujin\/items\/e2ee4019adefce08e381\n<br><br>\n\n### RobustScaler @scikit-learn\n\nscikit-learn \u3092\u4f7f\u3063\u305f\u7279\u5fb4\u91cf\u306e\u30b9\u30b1\u30fc\u30ea\u30f3\u30b0\u65b9\u6cd5\u306e\u4e00\u3064\u3002<br>\n\u7b2c1\u5206\u4f4d\u70b9 Q_1Q \u3068\u7b2c3\u5206\u4f4d\u70b9 Q_3Q \u306e\u7bc4\u56f2\u304c1\u306b\u306a\u308b\u3088\u3046\u306b\u5909\u63db\u3002<br>\n\u5916\u308c\u5024\u306b\u5f15\u3063\u5f35\u3089\u308c\u308b\u306e\u3092\u9632\u3050\u3002<br>\n\nhttps:\/\/pystyle.info\/ml-feature-scaling\/\n\n\u3053\u306e\u30ce\u30fc\u30c8\u30d6\u30c3\u30af\u3067\u306f\u4f7f\u308f\u306a\u3044\u3002<br>\n\n<br><br>\n\n### fit\u3068transform\u306b\u3064\u3044\u3066\nfit\u3067\u6700\u5927\u30fb\u6700\u5c0f\u3092\u8a08\u7b97<br>\nmmscaler.fit(x)           # x\u306e\u6700\u5927\u30fb\u6700\u5c0f\u3092\u8a08\u7b97<br>\ntransform\u3067\u5024\u3092fit\u306e\u9593\u306b\u5165\u308b\u3088\u3046\u306b\u5909\u63db<br>\ny = mmscaler.transform(x) # x\u3092\u5909\u63db<br>\n\ntrain\u3067fit\u3067\u6700\u5927\u30fb\u6700\u5c0f\u3092\u8a08\u7b97<br>\ntransform\u3092train\u3068test\u3078\u5b9f\u65bd\u3002<br>\n\n\u3010\u6ce8\u610f\u3011<br>\nRobustScaler\u306efit\u3067\u5024\u3092\u5909\u63db\u3059\u308b\u305d\u306e\u5185\u5bb9\u3092\u6c7a\u5b9a\u3057\u3001transform\u3067\u5024\u306e\u5909\u66f4\u3092\u5b9f\u65bd\u3059\u308b\u3002<br>\n\u305d\u306e\u305f\u3081\u3001fit\u3057\u305f\u3068\u304d\u306ecolumn\u306e\u5e45\u3068\u3001transform\u3059\u308b\u3068\u304d\u306ecolumn\u306e\u5e45\u304c\u63c3\u308f\u306a\u3044\u3068\u3060\u3081\u3002<br>\n\n<br><br>\n### add_prefix \/ add_suffix @ dataframe\n\n\u5217\u540d\u306b\u30d7\u30ec\u30d5\u30a3\u30c3\u30af\u30b9\uff08\u63a5\u982d\u8f9e\uff09\u3001\u30b5\u30d5\u30a3\u30c3\u30af\u30b9\uff08\u63a5\u5c3e\u8f9e\uff09\u3092\u8ffd\u52a0\u3059\u308b\u305f\u3081\u306e\u30e1\u30bd\u30c3\u30c9\u3002\n```\nprint(df.add_prefix('X_'))\n#        X_A  X_B  X_C\n```\nhttps:\/\/note.nkmk.me\/python-pandas-dataframe-rename\/\n<br><br>\n\n### \u884c\u5217\u306e@ @ numpy \/ tf\n\ndot\u306e\u610f\u5473\u3002numpy\u3068tf\u3067\u4f7f\u3048\u308b\u3002\n\n```\nA = np.random.rand(3,3)\nB = tf.random.uniform((3,3))\n\nAAA = A @ A @ A # AAA = A.dot(A).dot(A)\nBBB = B @ B @ B # BBB = tf.matmul(ft.matmul(B,B),B)\n```\n<br><br>\n\n### .agg(list) @ groupby()\ngroupby\u3057\u305f\u9805\u76ee\u4ee5\u5916\u306e\u30c7\u30fc\u30bf\u3092\u307e\u308b\u307e\u308blist\u306b\u3059\u308b\u3002\n\n```\ndf2 = df.groupby('breath_id').agg(list).reset_index()\n```\n\n|breath_id|id|R|C|time_step|u_in|u_out|pressure|\n|---|---|---|---|---|---|---|---|\n|0|1|[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14...|[20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 2...|[50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 5...|[0.0, 0.0336523056030273, 0.067514419555664, 0...|[0.0833340056346443, 18.38304147263472, 22.509...|[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...|[5.837491705069121, 5.907793850520346, 7.87625...|\n\n\n<br><br>\n### tf.test.is_gpu_available()\n\nGPU\u304c\u4f7f\u3048\u308b\u304b\u3069\u3046\u304b\u3002\u4f7f\u3048\u308c\u3070True\u304c\u5e30\u3063\u3066\u304f\u308b\u3002\n\n<br><br>\n\n### model.save @ tf\nmode.save()\u3067\u30c7\u30a3\u30ec\u30af\u30c8\u30ea\u3092\u6307\u5b9a\u3002.hd5\u3092\u3064\u3051\u308c\u3070hd5\u3067\u4fdd\u5b58\u3002<br>\n\n\u5229\u7528\u306f\u6b21\u306e\u901a\u308a\u3002.hd5\u3067\u3042\u308c\u3070\u62e1\u5f35\u5b50\u3082\u5fd8\u308c\u305a\u306b\u3002<br>\n```\nnew_model = tf.keras.models.load_model('saved_model\/my_model')\nloss, acc = new_model.evaluate(test_images,  test_labels, verbose=2)\n```\nhttps:\/\/www.tensorflow.org\/guide\/keras\/save_and_serialize?hl=ja#savedmodel_%E5%BD%A2%E5%BC%8F\n\n<br><br>\n### model.save_weights @ tf\n\n\u5b66\u7fd2\u4e2d\u3084\u7d42\u4e86\u4e2d\u306bsave\u304c\u53ef\u80fd\u3002weight\u3060\u3051\u4fdd\u5b58\u3059\u308b\u306e\u3067\u3001\u540c\u3058model\u69cb\u6210\u3092\u4f5c\u308c\u3070\u4ed6\u306etf\u30a4\u30f3\u30b9\u30bf\u30f3\u30b9\u306b\u30b3\u30d4\u30fc\u304c\u53ef\u80fd\u3002<br>\n\u3061\u3087\u3063\u3068\u3081\u3093\u3069\u304f\u3055\u305d\u3046\u3060\u3051\u3069\u3001\u3053\u3063\u3061\u306e\u307b\u3046\u304c\u826f\u3055\u305d\u3046\u3002\n\nhttps:\/\/www.tensorflow.org\/tutorials\/keras\/save_and_load?hl=ja#%E8%A8%93%E7%B7%B4%E4%B8%AD%E3%81%AB%E3%83%81%E3%82%A7%E3%83%83%E3%82%AF%E3%83%9D%E3%82%A4%E3%83%B3%E3%83%88%E3%82%92%E4%BF%9D%E5%AD%98%E3%81%99%E3%82%8B\n\n```\n# \u30d5\u30a1\u30a4\u30eb\u540d\u306b(`str.format`\u3092\u4f7f\u3063\u3066)\u30a8\u30dd\u30c3\u30af\u6570\u3092\u57cb\u3081\u8fbc\u3080\ncheckpoint_path = \"training_2\/cp-{epoch:04d}.ckpt\"\ncheckpoint_dir = os.path.dirname(checkpoint_path)\n\n# 5\u30a8\u30dd\u30c3\u30af\u3054\u3068\u306b\u30e2\u30c7\u30eb\u306e\u91cd\u307f\u3092\u4fdd\u5b58\u3059\u308b\u30b3\u30fc\u30eb\u30d0\u30c3\u30af\u3092\u4f5c\u6210\ncp_callback = tf.keras.callbacks.ModelCheckpoint(\n    filepath=checkpoint_path, \n    verbose=1, \n    save_weights_only=True,\n    period=5)\n\n# \u65b0\u3057\u3044\u30e2\u30c7\u30eb\u306e\u30a4\u30f3\u30b9\u30bf\u30f3\u30b9\u3092\u4f5c\u6210\nmodel = create_model()\n\n# `checkpoint_path` \u30d5\u30a9\u30fc\u30de\u30c3\u30c8\u3067\u91cd\u307f\u3092\u4fdd\u5b58\nmodel.save_weights(checkpoint_path.format(epoch=0))\n\n# \u65b0\u3057\u3044\u30b3\u30fc\u30eb\u30d0\u30c3\u30af\u3092\u4f7f\u3044\u3001\u30e2\u30c7\u30eb\u3092\u8a13\u7df4\nmodel.fit(train_images, \n          train_labels,\n          epochs=50, \n          callbacks=[cp_callback],\n          validation_data=(test_images,test_labels),\n          verbose=0)\n```\n\n\u4ee5\u4e0b\u3067\u30e2\u30c7\u30eb\u3092\u30c1\u30a7\u30c3\u30af\nls {checkpoint_dir}\n\n```\ncheckpoint            cp-0025.ckpt.index\ncp-0000.ckpt.data-00000-of-00001  cp-0030.ckpt.data-00000-of-00001\ncp-0000.ckpt.index        cp-0030.ckpt.index\ncp-0005.ckpt.data-00000-of-00001  cp-0035.ckpt.data-00000-of-00001\ncp-0005.ckpt.index        cp-0035.ckpt.index\n```\n\u6700\u65b0\u7248\u306e\u5229\u7528\u306f\n```\nlatest = tf.train.latest_checkpoint(checkpoint_dir)\n\n# \u65b0\u3057\u3044\u30e2\u30c7\u30eb\u306e\u30a4\u30f3\u30b9\u30bf\u30f3\u30b9\u3092\u4f5c\u6210\nmodel = create_model()\n\n# \u5148\u307b\u3069\u4fdd\u5b58\u3057\u305f\u91cd\u307f\u3092\u8aad\u307f\u8fbc\u307f\nmodel.load_weights(latest)\n\n# \u30e2\u30c7\u30eb\u3092\u518d\u8a55\u4fa1\nloss, acc = model.evaluate(test_images,  test_labels, verbose=2)\nprint(\"Restored model, accuracy: {:5.2f}%\".format(100*acc))\n```\n<br><br>\n\n### Dense layer\u3000\u5168\u7d50\u5408\u5c64\n\n\u6b21\u306e\u30b3\u30fc\u30c9\u306e\u7d50\u679c\u304c\u4e0b\u306e\u7d75\u3002\n```\nmodel = models.Sequential()\nmodel.add(Dense(10, activation='relu', input_shape=(4,)))   \nmodel.add(Dense(3, activation='softmax'))\n```\n\n<img src=\"https:\/\/sinyblog.com\/wp\/wp-content\/uploads\/2019\/04\/keras-003.jpg\" width=\"500\">\n\n<br><br>\n\n### selu @ \u6d3b\u6027\u5316\u95a2\u6570 tf dense layer\n\n\u52fe\u914d\u55aa\u5931\u306b\u5f37\u3044\u6d3b\u6027\u5316\u95a2\u6570\u3002Relu\u306e\u6642\u4ee3\u306f\u7d42\u308f\u3063\u305f\u3001SeLu\u3089\u3057\u3044\u3002\n\nhttps:\/\/data-analysis-stats.jp\/%E6%B7%B1%E5%B1%9E%E5%AD%A6%E7%BF%92\/%E6%B4%BB%E6%80%A7%E5%8C%96%E9%96%A2%E6%95%B0%E3%81%AE%E3%81%BE%E3%81%A8%E3%82%81\/\n\n<img src=\"https:\/\/data-analysis-stats.jp\/wp-content\/uploads\/2021\/03\/SELU-activation-01.png\" width=\"500\">\n\n<br><br>\n\n### keras.layers.Bidirectional\n\n\u5f93\u6765\u306eRNN\u306e\u30cb\u30e5\u30fc\u30ed\u30f3\u3092\u30d5\u30a9\u30ef\u30fc\u30c9(\u672a\u6765)\u306a\u3082\u306e\u3068\u30d0\u30c3\u30af\u30ef\u30fc\u30c9(\u904e\u53bb)\u306a\u3082\u306e\u3068\u306e2\u3064\u306b\u5206\u3051\u308b\u3002<br>\n\u4eca\u56de\u306f\u5fc5\u8981\u304b\u306a\uff1f\u80ba\u30e2\u30c7\u30eb\u304b\u3089\u306e\u30d5\u30a3\u30fc\u30c9\u30d0\u30c3\u30af\u304c\u5165\u529b\u5074\u306b\u3042\u308b\u306a\u3089\u3001Bidirectional\u304c\u5fc5\u8981\u305d\u3046\u3060\u3051\u3069\u3002<br>\n\nhttps:\/\/www.i2tutorials.com\/deep-dive-into-bidirectional-lstm\/\n\n<img src=\"https:\/\/d1zx6djv3kb1v7.cloudfront.net\/wp-content\/media\/2019\/05\/Deep-Dive-into-Bidirectional-LSTM-i2tutorials.jpg\" width=\"500\">","0cd8e928":"# Visualize original data u_in histgram for each time step id","18ca1d70":"# Utilitys\n\ndata_clean\u306e\u4e2d\u3067\u3001\u76f4\u7dda\u6027\u306e\u7121\u3044\u30c7\u30fc\u30bf\u3001\u8ca0\u306epressure\u3092\u6301\u3064\u30c7\u30fc\u30bf\u3001u_out = 1\u306estep\u657052\u4ee5\u4e0a\u3042\u308b\u30c7\u30fc\u30bf\u3092\u524a\u9664\u3002<br>","339b1db7":"\n\n# \u30e9\u30a4\u30d6\u30e9\u30ea\u8aad\u307f\u8fbc\u307f","067a2d07":"# Compare model result and train data, And visualize","a282aee9":"# visualize minus pressure data (\u8ca0\u306epressure\u3092\u6301\u3064\u30c7\u30fc\u30bf\u3092\u53ef\u8996\u5316)","0dc07900":"# visualize pressuer of u_out = 1","0d788864":"# Visualize u_in hist with RobustScaler() for each time step id","a554ee61":"# Codes that surprised me recently\n\n## use for in groupby()\n\nIt can modify data after groupby.\ngroupby\u3057\u305f\u3042\u3068\u306edf\u3092\u4f7f\u3044\u305f\u3044\u4e8b\u304c\u591a\u3005\u3042\u3063\u305f\u306e\u3067\u3059\u304c\u3001\u307e\u3055\u304bfor\u3067\u5b9f\u73fe\u3067\u304d\u308b\u3068\u306f\u601d\u3044\u307e\u305b\u3093\u3067\u3057\u305f\u3002\n\n```\nfor key, grp in train.groupby('breath_id'):\n    fig.add_trace(go.Violin(x=grp[\"u_in\"]))\n```\n<br><br>\n\n## use tqdm in groupby()\n\nCan use tqdm in groupby process.\n\n```\nfrom tqdm import tqdm\ntqdm.pandas()\nfor key, grp in tqdm(train.groupby('breath_id')):\n    fig.add_trace(go.Violin(x=grp[\"u_in\"]))\n```\n\n\n## \u4efb\u610f\u306e\u7bc4\u56f2\u3067heatmap\n\n\u51fa\u73fe\u983b\u5ea6\u3092\u4efb\u610f\u306e\u7bc4\u56f2\u3067\u78ba\u8a8d\u3057\u305f\u3044\u3053\u3068\u304c\u3042\u3063\u305f\u306e\u3067\u3059\u304c\u3001bins\u3092\u4f5c\u3063\u3066\u3001\u305d\u308c\u3092cut()\u3067\u5229\u7528\u3059\u308b\u3053\u3068\u3067\u5b9f\u73fe\u53ef\u80fd\u3067\u3057\u305f\u3002\n\n```\nrange_bins = pd.interval_range(start=-10, freq=10, end=100)\npd.cut(grp,bins=range_bins).value_counts()\n```\n<br><br>","e155dfbd":"# \u306f\u3058\u3081\u306b\n\nscore\u3092\u6c42\u3081\u308b\u3088\u308a\u306f\u3001\u30b9\u30b3\u30a2\u3092\u4e0a\u3052\u308b\u70ba\u306e\u60c5\u5831\u3092\u3069\u3046\u5f97\u308b\u304b\u3092\u66f8\u304d\u6e9c\u3081\u307e\u3059\u3002<br>\n\u5b66\u7fd2\u6e08\u307fmodel\u306esave\u3001train\u7d50\u679c\u3068target\u3068\u306e\u5dee\u306e\u53ef\u8996\u5316\u3001\u3069\u306efeature\u304c\u91cd\u8981\u304b\u306e\u53ef\u8996\u5316\u3092\u76ee\u6307\u3057\u307e\u3059\u3002<br>\n\u5f8c\u3067\u898b\u8fd4\u305b\u308b\u3088\u3046\u306b\u3001\u81ea\u5206\u7528\u306b\u307e\u3068\u3081\u3066\u3044\u307e\u3059\u3002<br>\n\n# NOTATION\n\n### <font color=\"red\">I try to find hints of feature engineering by comparing model result and train data.<br>You can find visualization result of model output and train data. <\/font>\n\n#### <font color=\"red\">This notebook use the trained model of following link due to run visualize with model result quckly and easy to change model of yours.<\/font>\n[copied model to this notebook](https:\/\/www.kaggle.com\/tfukuda675\/update-simple-lstm-simple-data-model)\n\n#### <font color=\"red\">When you use trained model in this notebook, please set \"use_trained_model = True\" in cell of <\/font>\n[Settings of this notebook](https:\/\/www.kaggle.com\/tfukuda675\/update-simple-lstm-simple-data#Settings) <br>\nnote : above model is copied of this notebook result.\n\n<font color=\"red\">If you want to look over data visualize result, please refer save version 37<\/font><br>\n<font color=\"red\"><s>I create u_out = 1 data for my understanding until version 37.<\/s><\/font><br>\n<font color=\"red\">From version 38, <s>I treat u_out = 0\" data!!<\/s><\/font><br>\n<font color=\"red\">From version 38, at first, create model with simple data with above stragety.<\/font><br>\n<font color=\"red\">Next, compare model result of train data and pressure of train data due to consider feature engineering of data.<\/font><br>\n\n## New Notebook\n\nThis notebook takes long runtime to compair model result vs train data, because this notebook run all steps, create data, create model, compair and visualize difference of model result and train data.<br>\nSo, I am editing new notebooks as following, due to run all steps quickly and easy to swap of your data or models.<br>\n\n* https:\/\/www.kaggle.com\/tfukuda675\/simple-lstm-create-train-data\n* https:\/\/www.kaggle.com\/tfukuda675\/simple-lstm-run-train\n* https:\/\/www.kaggle.com\/tfukuda675\/simple-lstm-compair-model-result\n\n## train data strategy (\u6226\u7565)\ntimestep\u306b\u76f4\u7dda\u6027\u304c\u7121\u3044\u30c7\u30fc\u30bf\u304c\u3042\u308a\u307e\u3059\u3002<br>\npressure(\u30bf\u30fc\u30b2\u30c3\u30c8\u3067\u3059\u304c)\u306b\u8ca0\u306e\u3082\u306e\u304c\u3042\u308a\u307e\u3059\u3002<br>\n\u8a55\u4fa1\u306b\u95a2\u4fc2\u3057\u306a\u3044u_out = 0\u306e\u30c7\u30fc\u30bf\u3082\u3042\u308a\u307e\u3059\u3002<br>\n\u305d\u3046\u3044\u3063\u305f\u30ad\u30ec\u30a4\u3067\u306f\u306a\u3044\u30c7\u30fc\u30bf\u3092\u524a\u9664\u3057\u3066\u5b66\u7fd2\u3057\u307e\u3059\u3002<br>\ntest\u30c7\u30fc\u30bf\u306b\u306f\u30ad\u30ec\u30a4\u3067\u306a\u3044\u30c7\u30fc\u30bf\u3082\u5165\u3063\u3066\u3044\u307e\u3059\u304c\u3001\u7121\u8996\u3059\u308b\u6226\u7565\u3067\u3059\u3002<br>\n\nStrategy of this notebook is \"simplify data as much as possible\"\n* Ignore non liner time_step data\n* Ignore negative pressure data\n* <s>use u_out = 0 data only for train\/test<\/s><br>\nPlease let me now, when you have any recommendations.<br>\n\nAt first, create model with above stragety.<br>\nNext, compare model result with train data and train pressure data.<br>\nThen consider additional feature of data with above result.<br>\n\n## refer to for visualize\n\u904e\u53bb\u306b\u4f5c\u3063\u305f\u3082\u306e\u3067\u3059\u3002plotly\/seaborn\/matplotlib\u304c\u671f\u5f85\u3059\u308bdf\u306e\u5f62\u72b6\u306b\u8a00\u53ca\u3057\u3066\u3044\u307e\u3059\u3002<br>\nhttps:\/\/www.kaggle.com\/tfukuda675\/data-visualization-plotly-seaborn-matplot?scriptVersionId=76838377&cellId=1\n\n","a177a7a4":"# Debug \u30e2\u30fc\u30c9\u6642\u306e\u51e6\u7406\n\u30c7\u30fc\u30bf\u6570\u3092\u524a\u6e1b<br>\nIn use_trained_model case, use new data.","597ffd1e":"# find minus pressure data (\u30bf\u30fc\u30b2\u30c3\u30c8\u5024\u3067\u3059\u304c)\u306b\u30de\u30a4\u30ca\u30b9\u5024\u3092\u6301\u3064\u30c7\u30fc\u30bf\u3092\u63a2\u3059","32115d33":"# count steps of u_out = 1   (\u6392\u6c17\u6bb5\u968e)\u306estep\u6570\u3092train\/test\u30c7\u30fc\u30bf\u3067\u78ba\u8a8d","f861af0d":"# \u30c7\u30fc\u30bf\u5f62\u72b6\u3092TF\u5411\u3051\u306b\u5909\u66f4\nbreath_id\u3054\u3068\u306e\u30c7\u30fc\u30bf\u306b\u306a\u308b\u3088\u3046\u306b\u6574\u5f62\u3002<br>","554f07fc":"# visualize linearity time_step (\u76f4\u7dda\u306atime_step\u306e\u53ef\u8996\u5316)\n\n\u304a\u3063\u3068\u3001\u50be\u304d\u304c\u8272\u3005\u3068\u3042\u308b\uff01\n\u8a08\u6e2c\u6a5f\u5668\u5074\u306e\u8a08\u6e2c\u30a4\u30f3\u30bf\u30fc\u30d0\u30eb\u304c\u305a\u308c\u308b\u3068\u306f\u601d\u3048\u306a\u3044\u306e\u3067\u3001\u4e00\u56de\u306e\u30b5\u30a4\u30af\u30eb\u309280\u5206\u5272\u3057\u305f\u306e\u3060\u3068\u601d\u3046\u3002","aeb7a81c":"# ","e80259d9":"# choose useful data for train and test and create target data\ntarget\u30c7\u30fc\u30bf\u306e\u4f5c\u6210\u3068\u3001\u4e0d\u8981\u306acolumn\u306e\u524a\u9664\u3002","9e7f5c90":"# Visualize u_in hist with StandardScaler() for each time step id"}}