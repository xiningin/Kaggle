{"cell_type":{"e7136167":"code","8cdad018":"code","0f71568d":"code","e7fbd4dc":"code","ca91d315":"code","bc30dfd8":"code","5ec2fde6":"code","ede2c5aa":"code","eff4d05a":"code","c914da93":"code","de9783eb":"code","5968808d":"code","ffa1311e":"code","1174320e":"code","2e36765b":"code","c20b7420":"code","832db860":"code","96766858":"code","1b837753":"code","ecf45cf5":"code","893b039e":"markdown","a73a5f04":"markdown","64932372":"markdown","ba701d02":"markdown","398ada06":"markdown","f8c4f61e":"markdown","d30d8da5":"markdown"},"source":{"e7136167":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport pandas as pd\nimport numpy as np\nimport re\nimport string\nimport nltk\nfrom nltk.corpus import stopwords\nstop = set(stopwords.words('english'))\nfrom wordcloud import WordCloud, STOPWORDS\nimport matplotlib as plty\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\nimport sklearn\n\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom subprocess import check_output\n\n%matplotlib inline\nimport plotly.offline as py\npy.init_notebook_mode(connected=True)\nimport plotly.graph_objs as go\nimport plotly.tools as tls\n\nimport plotly.graph_objs as go\nfrom sklearn import preprocessing\nEncode = preprocessing.LabelEncoder()\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.feature_extraction.text import TfidfVectorizer\ntfid = TfidfVectorizer()\nvect = CountVectorizer()\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.manifold import TSNE\nNB = MultinomialNB()\n\nimport nltk\nfrom nltk.corpus import stopwords\nstopwords = stopwords.words(\"english\")\nfrom sklearn import metrics\nfrom sklearn.svm import SVC\nfrom sklearn.ensemble import RandomForestClassifier\n\nprint(check_output([\"ls\", \"..\/input\"]).decode(\"utf8\"))","8cdad018":"chatbot = pd.read_csv(\"..\/input\/Sheet_1.csv\",usecols=['response_id','class','response_text'],encoding='latin-1')\nresume = pd.read_csv(\"..\/input\/Sheet_2.csv\",encoding='latin-1')","0f71568d":"chatbot.head()","e7fbd4dc":"chatbot['class'].value_counts()","ca91d315":"def cloud(text):\n    wordcloud = WordCloud(background_color=\"white\",stopwords=stop).generate(\" \".join([i for i in text.str.upper()]))\n    plt.imshow(wordcloud)\n    plt.axis(\"off\")\n    plt.title(\"Chat Bot Response\")\ncloud(chatbot['response_text'])","bc30dfd8":"chatbot['Label'] = Encode.fit_transform(chatbot['class'])","5ec2fde6":"chatbot['Label'].value_counts()\n#not_flagged    55\n#flagged        25","ede2c5aa":"x = chatbot.response_text\ny = chatbot.Label\nx_train,x_test,y_train,y_test = train_test_split(x,y,random_state=1)\nx_train_dtm = vect.fit_transform(x_train)\nx_test_dtm = vect.transform(x_test)\nNB.fit(x_train_dtm,y_train)\ny_predict = NB.predict(x_test_dtm)\nmetrics.accuracy_score(y_test,y_predict)\n\n\n\n\n","eff4d05a":"rf = RandomForestClassifier(max_depth=10,max_features=10)\nrf.fit(x_train_dtm,y_train)\nrf_predict = rf.predict(x_test_dtm)\nmetrics.accuracy_score(y_test,rf_predict)","c914da93":"Chatbot_Text = chatbot[\"response_text\"]\nlen(Chatbot_Text)","de9783eb":"Tf_idf = CountVectorizer(max_features=256).fit_transform(Chatbot_Text.values)","5968808d":"Tf_idf","ffa1311e":"tsne = TSNE(\n    n_components=3,\n    init='random', # pca\n    random_state=101,\n    method='barnes_hut',\n    n_iter=250,\n    verbose=2,\n    angle=0.5\n).fit_transform(Tf_idf.toarray())\n\n\n","1174320e":"resume.head()","2e36765b":"resume['class'].value_counts()","c20b7420":"def cloud(text):\n    wordcloud = WordCloud(background_color=\"white\",stopwords=stop).generate(\" \".join([i for i in text.str.upper()]))\n    plt.imshow(wordcloud)\n    plt.axis(\"off\")\n    plt.title(\"Resume Bot Response\")\ncloud(resume['resume_text'])\n\n\n\n\n","832db860":"resume['Label'] = Encode.fit_transform(resume['class'])","96766858":"xr = resume.resume_text\nyr= resume.Label\nxr_train,xr_test,yr_train,yr_test = train_test_split(xr,yr,random_state=1)\nxr_train_dtm = vect.fit_transform(xr_train)\nxr_test_dtm = vect.transform(xr_test)\nNB.fit(xr_train_dtm,yr_train)\nyr_predict = NB.predict(xr_test_dtm)\nmetrics.accuracy_score(yr_test,yr_predict)\n\n\n\n\n","1b837753":"rf = RandomForestClassifier(max_depth=10,max_features=10)\nrf.fit(xr_train_dtm,yr_train)\nrf_predict = rf.predict(xr_test_dtm)\nmetrics.accuracy_score(yr_test,rf_predict)","ecf45cf5":"resume_Text = resume[\"resume_text\"]\nlen(resume_Text)","893b039e":"**Random Forest**","a73a5f04":"**Resume**","64932372":"**Model Building **","ba701d02":"As we can that,Random forest shows the higher successful rate of 80% as compared to Navie bayes, which is 70%.\nThough the minimum successful rate for such small data should be 95% then and then only it can be accepted to work better in real senario.\nI think fine tuning of certain parameters will help to imporve the success rate.\nLudwig library by Uber can be the best option considered for improving the success rate.","398ada06":"**Naive Bayes**","f8c4f61e":"For resume data,it can be seen that the Navie Bayes shows higher success rate as compare to Random forest.","d30d8da5":"****Sheet_1.csv contains 80 user responses, in the response_text column, to a therapy chatbot. Bot said: 'Describe a time when you have acted as a resource for someone else'.  User responded. If a response is 'not flagged', the user can continue talking to the bot. If it is 'flagged', the user is referred to help. ****"}}