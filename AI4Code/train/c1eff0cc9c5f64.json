{"cell_type":{"3e0ee3e2":"code","443dee76":"code","2e6ac63f":"code","afdcc60a":"code","1073c8be":"code","d560cbc7":"code","9eb783ec":"code","d0d7f998":"code","06518f42":"code","cb190199":"code","4c6149b7":"markdown","2325053d":"markdown","ca3a75b0":"markdown","4a233f8b":"markdown","ccf34569":"markdown","978e1b82":"markdown","8ebb621b":"markdown","906df2ff":"markdown","5c50ea14":"markdown","27cffeee":"markdown"},"source":{"3e0ee3e2":"from sklearn import datasets     #for dataset\nimport numpy as np               #for maths\nimport matplotlib.pyplot as plt  #for plotting","443dee76":"iris = datasets.load_iris()      #load the dataset\ndata = iris.data                 #get features  \ntarget = iris.target             #get labels\n\nshape = data.shape               #shape of data\n\n#convert into numpy array\ndata = np.array(data).reshape(shape[0],shape[1])\ntarget = np.array(target).reshape(shape[0],1)\n\n#print shape\nprint(\"Data Shape   = {}\".format(data.shape))\nprint(\"Target Shape = {}\".format(target.shape))\nprint('Classes : {}'.format(np.unique(target)))\nprint('Sample data : {} , Target = {}'.format(data[70],target[70]))","2e6ac63f":"#HYPERPARAMETERS\n\n#num of target labels\nnum_classes = len(np.unique(target))\n\n#define layer_neurons\ninput_units  = 4   #neurons in input layer\nhidden_units = 8   #neurons in hidden layer\noutput_units = 3   #neurons in output layer\n\n#define hyper-parameters\nlearning_rate = 0.03\n\n#regularization parameter\nbeta = 0.00001\n\n#num of iterations\niters = 4001","afdcc60a":"#PARAMETERS\n\n#initialize parameters i.e weights\ndef initialize_parameters():\n    #initial values should have zero mean and 0.1 standard deviation\n    mean = 0        #mean of parameters \n    std = 0.03      #standard deviation\n    \n    layer1_weights = np.random.normal(mean,std,(input_units,hidden_units))          \n    layer1_biases = np.ones((hidden_units,1))                                       \n    layer2_weights = np.random.normal(mean,std,(hidden_units,output_units))\n    layer2_biases = np.ones((output_units,1))\n    \n    parameters = dict()\n    parameters['layer1_weights'] = layer1_weights\n    parameters['layer1_biases'] = layer1_biases\n    parameters['layer2_weights'] = layer2_weights\n    parameters['layer2_biases'] = layer2_biases\n    \n    return parameters","1073c8be":"#activation function\ndef sigmoid(X):\n    return 1\/(1+np.exp((-1)*X))\n\n#softmax function for output\ndef softmax(X):\n    exp_X = np.exp(X)\n    exp_X_sum = np.sum(exp_X,axis=1).reshape(-1,1)\n    exp_X = (exp_X\/exp_X_sum)\n    return exp_X","d560cbc7":"#forward propagation\ndef forward_propagation(train_dataset,parameters):\n    cache = dict()            #to store the intermediate values for backward propagation\n    m = len(train_dataset)    #number of training examples\n    \n    #get the parameters\n    layer1_weights = parameters['layer1_weights']\n    layer1_biases = parameters['layer1_biases']\n    layer2_weights = parameters['layer2_weights']\n    layer2_biases = parameters['layer2_biases']\n    \n    #forward prop\n    logits = np.matmul(train_dataset,layer1_weights) + layer1_biases.T\n    activation1 = np.array(sigmoid(logits)).reshape(m,hidden_units)\n    activation2 = np.array(np.matmul(activation1,layer2_weights) + layer2_biases.T).reshape(m,output_units)\n    output = np.array(softmax(activation2)).reshape(m,num_classes)\n    \n    #fill in the cache\n    cache['output'] = output\n    cache['activation1'] = activation1\n    \n    return cache,output\n\n#backward propagation\ndef backward_propagation(train_dataset,train_labels,parameters,cache):\n    derivatives = dict()         #to store the derivatives\n    \n    #get stuff from cache\n    output = cache['output']\n    activation1 = cache['activation1']\n    \n    #get parameters\n    layer1_weights = parameters['layer1_weights']\n    layer2_weights = parameters['layer2_weights']\n    \n    #calculate errors\n    error_output = output - train_labels\n    error_activation1 = np.matmul(error_output,layer2_weights.T)\n    error_activation1 = np.multiply(error_activation1,activation1)\n    error_activation1 = np.multiply(error_activation1,1-activation1)\n    \n    \n    #calculate partial derivatives\n    partial_derivatives2 = np.matmul(activation1.T,error_output)\/len(train_dataset)\n    partial_derivatives1 = np.matmul(train_dataset.T,error_activation1)\/len(train_dataset)\n    \n    #store the derivatives\n    derivatives['partial_derivatives1'] = partial_derivatives1\n    derivatives['partial_derivatives2'] = partial_derivatives2\n    \n    return derivatives\n\n\n#update the parameters\ndef update_parameters(derivatives,parameters):\n    #get the parameters\n    layer1_weights = parameters['layer1_weights']\n    layer2_weights = parameters['layer2_weights']\n    \n    #get the derivatives\n    partial_derivatives1 = derivatives['partial_derivatives1']\n    partial_derivatives2 = derivatives['partial_derivatives2']\n    \n    #update the derivatives\n    layer1_weights -= (learning_rate*(partial_derivatives1 + beta*layer1_weights))\n    layer2_weights -= (learning_rate*(partial_derivatives2 + beta*layer2_weights))\n    \n    #update the dict\n    parameters['layer1_weights'] = layer1_weights\n    parameters['layer2_weights'] = layer2_weights\n    \n    return parameters\n    \n#calculate the loss and accuracy\ndef cal_loss_accuray(train_labels,predictions,parameters):\n    #get the parameters\n    layer1_weights = parameters['layer1_weights']\n    layer2_weights = parameters['layer2_weights']\n    \n    #cal loss and accuracy\n    loss = -1*np.sum(np.multiply(np.log(predictions),train_labels) + np.multiply(np.log(1-predictions),(1-train_labels)))\/len(train_labels) + np.sum(layer1_weights**2)*beta\/len(train_labels) + np.sum(layer2_weights**2)*beta\/len(train_labels)\n    accuracy = np.sum(np.argmax(train_labels,axis=1)==np.argmax(predictions,axis=1))\n    accuracy \/= len(train_dataset)\n    \n    return loss,accuracy","9eb783ec":"#Implementation of 3 layer Neural Network\n\n#training function\ndef train(train_dataset,train_labels,iters=2):\n    #To store loss after every iteration.\n    J = []\n  \n    #WEIGHTS\n    global layer1_weights\n    global layer1_biases\n    global layer2_weights\n    global layer2_biases\n  \n    #initialize the parameters\n    parameters = initialize_parameters()\n    \n    layer1_weights = parameters['layer1_weights']\n    layer1_biases = parameters['layer1_biases']\n    layer2_weights = parameters['layer2_weights']\n    layer2_biases = parameters['layer2_biases']\n    \n    #to store final predictons after training\n    final_output = []\n    \n    for j in range(iters):\n        #forward propagation\n        cache,output = forward_propagation(train_dataset,parameters)\n        \n        #backward propagation\n        derivatives = backward_propagation(train_dataset,train_labels,parameters,cache)\n        \n        #calculate the loss and accuracy\n        loss,accuracy = cal_loss_accuray(train_labels,output,parameters)\n        \n        #update the parameters\n        parameters = update_parameters(derivatives,parameters)\n        \n        #append loss\n        J.append(loss)\n        \n        #update final output\n        final_output = output\n        \n        #print accuracy and loss\n        if(j%500==0):\n            print(\"Step %d\"%j)\n            print(\"Loss %f\"%loss)\n            print(\"Accuracy %f%%\"%(accuracy*100))\n    \n    return J,final_output","d0d7f998":"#shuffle the dataset\nz = list(zip(data,target))\nnp.random.shuffle(z)\ndata,target = zip(*z)\n\n#make train_dataset and train_labels\ntrain_dataset = np.array(data).reshape(-1,4)\ntrain_labels = np.zeros([train_dataset.shape[0],num_classes])\n\n#one-hot encoding\nfor i,label in enumerate(target):\n    train_labels[i,label] = 1\n\n#normalizations\nfor i in range(input_units):\n    mean = train_dataset[:,i].mean()\n    std = train_dataset[:,i].std()\n    train_dataset[:,i] = (train_dataset[:,i]-mean)\/std","06518f42":"#train data\nJ,final_output = train(train_dataset,train_labels,iters=4001)","cb190199":"#plot loss graph\nplt.plot(list(range(1,len(J))),J[1:])\nplt.xlabel('Iterations')\nplt.ylabel('Loss')\nplt.title('Iterations VS Loss')\nplt.show()","4c6149b7":"### Train Function\n\n1. Initialize Parameters\n2. Forward Propagation\n3. Backward Propagation\n4. Calculate Loss and Accuracy\n5. Update the parameters\n\nRepeat the steps 2-5 for the given number of iterations","2325053d":"One hidden layer Neural Network.\n ![](https:\/\/github.com\/navjindervirdee\/neural-networks\/blob\/master\/Neural%20Network\/network.JPG?raw=true)\nInput Units  = 4 <br>\nHidden Units = 8 <br>\nOutput Units = 3 <br>","ca3a75b0":"### Activation Function\n\n**Sigmoid**\n\n![](https:\/\/upload.wikimedia.org\/wikipedia\/commons\/8\/88\/Logistic-curve.svg)\n","4a233f8b":"# Implementing Neural Network from Scratch.\nNeural Networks are really powerful algorithms used for classification. <br>\nDataset = Iris_Dataset <br>\nLink = http:\/\/scikit-learn.org\/stable\/auto_examples\/datasets\/plot_iris_dataset.html <br>\n\n### Import required libraries","ccf34569":"### Get Dataset","978e1b82":"#### Reached an Accuracy of 97%","8ebb621b":"### Plot the loss vs iteration graph","906df2ff":"### Define Utility Functions\n#### 1. Forward Propagation\n---- Logits = matmul(X,Wxh) + Bh       <br>\n---- A = sigmoid(logits)       <br>\n---- logits = matmul(A,Why) + By       <br>\n---- output = softmax(logits)  <br>\n\nStore output and A in cache to use it in backward propagation <br>\n\n#### 2. Backward Propagation\n---- Error_output = output - train_labels <br>\n---- Error_activation = (matmul(error_output,Why.T))(A)(1-A) <br>\n---- dWhy = (matmul(A.T,error_output))\/m <br>\n---- dWxh = (matmul(train_dataset.T,error_activation))\/m <br>\n\nm = len(train_dataset) <br>\nStore derivatives in derivatives dict\n\n#### 3. Update Parameters\n---- Wxh = Wxh - learning_rate(dWxh + beta*Wxh) <br>\n---- Why = Why - learning_rate(dWhy + beta*Why) <br>\n\n#### 4. Calculate Loss and Accuracy\n---- Loss = (-1(Y log(prediction)) + (1-Y) (log(1-predictions)))  + beta * (sum(Wxh^2) + sum(Why^2)))\/m  <br>\n---- Accuracy = sum(Y==predictions)\/m ","5c50ea14":"### Dimesions of Parameters\nShape of layer1_weights (Wxh) = (4,8)  <br>\nShape of layer1_biasess (Bh) = (8,1)  <br>\nShape of layer2_weights (Why) = (8,3)  <br>\nShape of layer2_biasess (By) = (3,1)  <br>","27cffeee":"### Define Parameters and Hyperparameters\n\n\n\n\n"}}