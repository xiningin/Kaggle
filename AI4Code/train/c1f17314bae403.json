{"cell_type":{"d5b5f29b":"code","5da15f0e":"code","b8ab0ea7":"code","f0db4168":"code","53030ea1":"code","b80a5b71":"code","409c6c97":"code","5da530af":"code","3843fe3a":"code","f4260c79":"code","f22206d0":"code","c4f6950e":"markdown","74af69b7":"markdown","7d04768e":"markdown","f714cba0":"markdown","f09732ab":"markdown","748bd91e":"markdown","4eaf4fca":"markdown","30d4f1f8":"markdown","ce14884d":"markdown"},"source":{"d5b5f29b":"VERBOSE = True","5da15f0e":"import numpy as np \nimport pandas as pd \nimport gc\n\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.preprocessing import RobustScaler\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.metrics import plot_confusion_matrix\n\nfrom sklearn.linear_model import SGDClassifier\nfrom sklearn.calibration import CalibratedClassifierCV\n","b8ab0ea7":"## from: https:\/\/www.kaggle.com\/bextuychiev\/how-to-work-w-million-row-datasets-like-a-pro\ndef reduce_memory_usage(df, verbose=True):\n    numerics = [\"int8\", \"int16\", \"int32\", \"int64\", \"float16\", \"float32\", \"float64\"]\n    start_mem = df.memory_usage().sum() \/ 1024 ** 2\n    for col in df.columns:\n        col_type = df[col].dtypes\n        if col_type in numerics:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == \"int\":\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)\n            else:\n                if (\n                    c_min > np.finfo(np.float16).min\n                    and c_max < np.finfo(np.float16).max\n                ):\n                    df[col] = df[col].astype(np.float16)\n                elif (\n                    c_min > np.finfo(np.float32).min\n                    and c_max < np.finfo(np.float32).max\n                ):\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)\n    end_mem = df.memory_usage().sum() \/ 1024 ** 2\n    if verbose:\n        print(\n            \"Mem. usage decreased to {:.2f} Mb ({:.1f}% reduction)\".format(\n                end_mem, 100 * (start_mem - end_mem) \/ start_mem\n            )\n        )\n    return df\n","f0db4168":"train = pd.read_csv(\"..\/input\/tabular-playground-series-oct-2021\/train.csv\")\ntest = pd.read_csv(\"..\/input\/tabular-playground-series-oct-2021\/test.csv\")\nsubmission = pd.read_csv(\"..\/input\/tabular-playground-series-oct-2021\/sample_submission.csv\")\n\ntrain = reduce_memory_usage(train, VERBOSE)\ntest = reduce_memory_usage(test, VERBOSE)\n\nTARGET = 'target'","53030ea1":"continous_cols = ['f'+str(i) for i in range(242)]\ncontinous_cols.remove('f22')\ncontinous_cols.remove('f43')\ncategorical_cols = ['f'+str(i) for i in range(242,285)]+['f22','f43']","b80a5b71":"train[\"mean\"] = train[continous_cols].mean(axis=1)\ntrain[\"std\"] = train[continous_cols].std(axis=1)\ntrain[\"min\"] = train[continous_cols].min(axis=1)\ntrain[\"max\"] = train[continous_cols].max(axis=1)\n\ntest[\"mean\"] = test[continous_cols].mean(axis=1)\ntest[\"std\"] = test[continous_cols].std(axis=1)\ntest[\"min\"] = test[continous_cols].min(axis=1)\ntest[\"max\"] = test[continous_cols].max(axis=1)\n\ncontinous_cols.append('mean')\ncontinous_cols.append('std')\ncontinous_cols.append('min')\ncontinous_cols.append('max')\n","409c6c97":"feature_cols = continous_cols + categorical_cols","5da530af":"scaler = RobustScaler()\ntrain[continous_cols] = scaler.fit_transform(train[continous_cols])\ntest[continous_cols] = scaler.transform(test[continous_cols])","3843fe3a":"ResultOfPreviousRun = True\n\nparams = {\n    \"loss\" : [\"hinge\", \"log\", \"squared_hinge\", \"modified_huber\", \"perceptron\"], #[\"hinge\"]\n    \"alpha\" : [0.0001, 0.001, 0.01, 0.1],\n    \"penalty\" : [\"l2\", \"l1\", \"elasticnet\"],    \n    \"random_state\":[2021],\n    \"class_weight\":[\"balanced\"]\n}\n\nif ResultOfPreviousRun and VERBOSE:\n    print(\"best score: 0.7562200000000001\")\n    print(\"best estimator: SGDClassifier(alpha=0.1, class_weight='balanced', penalty='l1',random_state=2021)\") \nelif not PrintResultOfPreviousRun:\n    tmptrain = train.copy()\n\n    tmptrain = tmptrain.sample(frac=0.10, replace=True, random_state=999)\n\n    model = SGDClassifier(max_iter=1000)\n    clf = GridSearchCV(model, cv=5, param_grid=params, verbose=10)\n\n    clf.fit(tmptrain[feature_cols],tmptrain[TARGET])\n\n    if VERBOSE:\n        print(\"best score: \",clf.best_score_)\n        print(\"best estimator: \",clf.best_estimator_)\n","f4260c79":"params = {\n    \"loss\" : \"hinge\",\n    \"alpha\" : 0.1, \n    \"random_state\":2021   \n}\n\npreds = []\nscores = []\n              \nkf = StratifiedKFold(n_splits=50, shuffle=True, random_state=13)\n\nfor fold, (idx_train, idx_valid) in enumerate(kf.split(train[feature_cols],train[TARGET])):\n    \n    X_train, y_train = train[feature_cols].iloc[idx_train], train[TARGET].iloc[idx_train]\n    X_valid, y_valid = train[feature_cols].iloc[idx_valid], train[TARGET].iloc[idx_valid]\n    \n    model = SGDClassifier(**params)    \n    clf = model.fit(X_train, y_train)\n    calibrator = CalibratedClassifierCV(clf, cv='prefit')\n    model = calibrator.fit(X_train, y_train)     \n    \n    pred_valid = model.predict_proba(X_valid)[:,1]\n    score = roc_auc_score(y_valid, pred_valid)\n    scores.append(score)\n    \n    if VERBOSE:\n        print(f\"Fold: {fold + 1} score auc: {score}\")\n    \n    y_hat = model.predict_proba(test[feature_cols])[:,1]\n    preds.append(y_hat)\n\nif VERBOSE:\n    print(f\"Overall Validation Score : {np.mean(scores)}\")\n              \ndel model\ngc.collect()","f22206d0":"submission[TARGET] = np.mean(np.column_stack(preds), axis=1)\nsubmission.to_csv('submission.csv', index=False)","c4f6950e":"## Create submission","74af69b7":"## Get features","7d04768e":"## Load datasets and memory reduction","f714cba0":"## Imports","f09732ab":"## Modeling with SGDClassifier\n\nSGDClassifier(loss = 'hinge') does not have probability by default.\n\nYou have to pass SGDclassifier(loss = 'hinge') to CalibratedClassifierCV() which will calculate the probability values of SGDclassifier(loss = 'hinge').        \n  ","748bd91e":"## Scale data\n\nStochastic Gradient Descent is sensitive to feature scaling, so it is highly recommended to scale your data. ","4eaf4fca":"## Conclusion\n\nI had better results with LGBM classifier but not bad for the first time.\n\nThis is the first time I use Stochastic Gradient Descent. Your comments and suggestions are welcome.","30d4f1f8":"## Automatic hyper-parameter search\n\nTo find the best parameters I'm using GridSearchCV, with 10% of data. ","ce14884d":"I never used the Stochastic Gradient Descent classifier before. \nIt's being capable of handling verly large datasets efficiently. \n\nSo let's pick this one and train it."}}