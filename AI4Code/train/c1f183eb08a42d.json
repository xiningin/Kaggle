{"cell_type":{"248b0c5c":"code","d1254180":"code","facc88ce":"code","b3843254":"code","271efaba":"code","7be6fdfd":"code","444dfa2c":"code","fb811219":"code","21986d8c":"code","9e77144f":"code","72b3dd5f":"code","26870d36":"code","afc09a74":"code","d8c00610":"markdown","87b58d75":"markdown","0cc95a49":"markdown","565a04e9":"markdown","676c233a":"markdown","db10c948":"markdown","c50905db":"markdown","930cb2f5":"markdown","763f8ff9":"markdown","c2263b88":"markdown","b7fbe02a":"markdown","3216e32d":"markdown"},"source":{"248b0c5c":"import numpy as np\nimport pandas as pd\nimport tensorflow as tf\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","d1254180":"DATAFILE=\"\/kaggle\/input\/mushroom-classification\/mushrooms.csv\"\nDATASET_SIZE=8124\nnp.set_printoptions(precision=3, suppress=True)\nN_CLASSES=1 # Edible\/Poisonous\nN_INPUTS=22 # Columnas del CSV usadas para clasificar\n\nfull_dataset = tf.data.experimental.make_csv_dataset(DATAFILE, batch_size=1, label_name=\"class\", shuffle=True)\n# Ya que hay una sola clase \"edible\/poisonous\", codificamos el label de los rows como 0\/1. Luego codificaremos los features.\nfull_dataset = full_dataset.map(lambda features, label: (features, 0 if label==\"e\" else 1))\n\nfor features, label in full_dataset.take(1):\n    print(\"Label:\", label.numpy())\n    tf.print(features)","facc88ce":"train_data = full_dataset.take(round(DATASET_SIZE\/3*2))\ntest_data = full_dataset.skip(round(DATASET_SIZE\/3*2)).take(round(DATASET_SIZE\/3)-1)\n\n\nprint(tf.data.experimental.cardinality(train_data).numpy())\nprint(tf.data.experimental.cardinality(test_data).numpy())","b3843254":"from tensorflow import feature_column\n\nfeature_columns=[]\n\nVOCABULARY={\n    'cap-shape': ['b', 'c', 'x', 'f', 'k', 's'],\n    'cap-surface': ['f', 'g', 'y', 's'],\n    'cap-color': ['n', 'b', 'c', 'g', 'r', 'p', 'u', 'e', 'w', 'y'],\n    'bruises': ['t', 'f'],\n    'odor': ['a', 'l', 'c', 'y', 'f', 'm', 'n', 'p', 's'],\n    'gill-attachment': ['a', 'd', 'f', 'n'],\n    'gill-spacing': ['c', 'w', 'd'],\n    'gill-size': ['b', 'n'],\n    'gill-color': ['k', 'n', 'b', 'h', 'g', 'r', 'o', 'p', 'u', 'e', 'w', 'y'],\n    'stalk-shape': ['e', 't'],\n    'stalk-root': ['b', 'c', 'u', 'e', 'z', 'r', '?'],\n    'stalk-surface-above-ring': ['f', 'y', 'k', 's'],\n    'stalk-surface-below-ring': ['f', 'y', 'k', 's'],\n    'stalk-color-above-ring': ['n', 'b', 'c', 'g', 'o', 'p', 'e', 'w', 'y'],\n    'stalk-color-below-ring': ['n', 'b', 'c', 'g', 'o', 'p', 'e', 'w', 'y'],\n    'veil-type': ['p', 'u'],\n    'veil-color': ['n', 'o', 'w', 'y'],\n    'ring-number': ['n', 'o', 't'],\n    'ring-type': ['c', 'e', 'f', 'l', 'n', 'p', 's', 'z'],\n    'spore-print-color': ['k', 'n', 'b', 'h', 'r', 'o', 'u', 'w', 'y'],\n    'population': ['a', 'c', 'n', 's', 'v', 'y'],\n    'habitat': ['g', 'l', 'm', 'p', 'u', 'w', 'd'],\n}\n\nfor header in VOCABULARY:\n    feature_columns.append(feature_column.indicator_column(feature_column.categorical_column_with_vocabulary_list(header, VOCABULARY[header])))\n\nfeature_layer = tf.keras.layers.DenseFeatures(feature_columns)\n","271efaba":"for features, label in train_data.take(1):\n    print(\"Label:\", label.numpy())\n    print(\"Parametros:\")\n    tf.print(features)\n    print(\"Parametros codificados:\", feature_layer(features))","7be6fdfd":"from tensorflow.keras import layers\nfrom keras.optimizers import SGD\n\nmodel = tf.keras.Sequential([\n  #tf.keras.Input(shape=(126)),\n  feature_layer,\n  layers.Dense(20, activation='relu', kernel_initializer='he_normal', name=\"layer1\"),\n  layers.Dense(20, activation='relu', kernel_initializer='he_normal', name=\"layer1\"),\n  layers.Dense(1, name=\"output\")\n])\n\nmodel.compile(optimizer=SGD(learning_rate=0.01, momentum=0.9), loss='mse', metrics=['accuracy'])\n#print(model.summary())\n\n#train_data = train_data.batch(100)\nmodel.fit(train_data.batch(100), epochs=10, batch_size=100, verbose=1)","444dfa2c":"for features, label in test_data.take(20):\n    print(\"Label\/Prediction:\", label.numpy(), model.predict_classes(features)[0][0])","fb811219":"from math import sqrt\n\n# Evaluamos usando el conjunto de datos de test\nloss, accuracy = model.evaluate(test_data.batch(1))\nprint('MSE: %.3f' % (loss))\nprint('Accuracy: %.3f' % (accuracy))\n\n","21986d8c":"from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n\n#predictions = test_data.map(lambda features, labels: (labels, tf.map_fn(lambda f:model.predict_classes(f), features)))\npredictionPairs = []\n\nfor features, label in test_data.take(10):\n    predictionPairs.append( (label.numpy(), model.predict_classes(features)[0][0]) )\n\ny_true, y_pred = zip(*predictionPairs)\n    \nprint({\n    \"accuracy\": accuracy_score(y_true, y_pred),\n    \"precision\": precision_score(y_true, y_pred),\n    \"recall\": recall_score(y_true, y_pred),\n    \"f1\": f1_score(y_true, y_pred)\n})","9e77144f":"dataPd = pd.read_csv(DATAFILE)\ndataPd.head()","72b3dd5f":"from sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import OneHotEncoder\n\nY = dataPd['class']\nX = dataPd.drop(['class'], axis=1)\n\n# Necesitamos codificar los features como n\u00fameros para alimentar el Random Forest\nX = OneHotEncoder().fit_transform(X).toarray()\n\ntrain_X, test_X, train_y, test_y = train_test_split(X, Y, test_size=0.33, random_state=0)","26870d36":"from sklearn.ensemble import RandomForestClassifier\n\nRF_model = RandomForestClassifier(bootstrap=True, n_estimators=50)\nRF_model.fit(train_X, train_y)","afc09a74":"from sklearn.metrics import accuracy_score, recall_score, f1_score\n\nreal = list(test_y.tolist())\nRF_predictions = list(RF_model.predict(test_X))\n\nprint({\n    \"accuracy\": accuracy_score(real, RF_predictions),\n    \"precision\": precision_score(real, RF_predictions, pos_label=\"e\"),\n    \"recall\": recall_score(real, RF_predictions, pos_label=\"e\"),\n    \"f1\": f1_score(real, RF_predictions, pos_label=\"e\")\n})","d8c00610":"Construimos el pipeline del modelo:\nHabiendo codificado los features mediante categorical_column_with_vocabulary_list, terminamos con 126 neuronas de entrada.\nFunci\u00f3n de activaci\u00f3n: relu\nFunci\u00f3n de inicializaci\u00f3n de pesos: Funci\u00f3n Normal Est\u00e1ndar truncada\nUsamos dos layer ocultas con 20 neuronas cada una.\n\nPara la Backpropagation, usamos un optimizador de Gradient Descent con una funci\u00f3n de error de Cuadrados M\u00ednimos. Realizamos 10 ciclos de entrenamiento.","87b58d75":"# <center>(7570) SIST.DE PROG. NO CONVENCIONAL DE ROBOTS\n### <center>Trabajo Pr\u00e1ctico 02: Redes Neuronales y Random Forest<\/center>\n### <center>Marco Luis Fleres, Padr\u00f3n 93174<\/center>","0cc95a49":"Probamos el Predictor con algunos datos:","565a04e9":"Construimos el modelo del perceptr\u00f3n multicapa. Empezamos por codificar los valores de las columnas como n\u00fameros. Ya que construiremos un modelo de keras, usamos feature_columns para codificar las columnas:","676c233a":"> Obtenemos el Error Cuadr\u00e1tico Medio, la precisi\u00f3n, recall y f1 del modelo con los datos de prueba, tanto con el puntaje provisto por Keras como el evaluado con sklearn.","db10c948":"# Random Forest","c50905db":"Par\u00e1metros del Random Forest:\nCantidad de \u00e1rboles (n_estimators): 50\n","930cb2f5":"Observamos el preprocesado:","763f8ff9":"Separamos los datos en los conjuntos de entrenamiento y validaci\u00f3n.","c2263b88":"# Conclusiones\n\nTanto la red neuronal como el RandomForest tuvieron una precisi\u00f3n muy alta. Es posible que se haya dado overfitting, aunque los RandomForest son menos suceptibles a esto. Har\u00edan falta mas datos para comprobarlo. La Red Neuronal si bien converge r\u00e1pidamente, no es tan r\u00e1pida de entrenar como el RandomForest, y requiere m\u00e1s hiperpar\u00e1metros, por lo que a igualdad de resultados, preferiremos el RandomForest.","b7fbe02a":"Cargamos el dataset:","3216e32d":"---"}}