{"cell_type":{"fa761a85":"code","7fb0a3cc":"code","92a59f1e":"code","e577dc4b":"code","d5bb1a6a":"code","d5da584c":"code","55937644":"code","7de56766":"code","5f7a929b":"code","19ede29c":"code","f35400d0":"code","ded28fe3":"code","da12a490":"code","597c9895":"code","8348471c":"code","d87e65a9":"code","9a6f6bcc":"code","db441724":"code","9f3d33a1":"code","f9c4ca06":"code","1db41233":"code","a7cdec39":"code","b1e4b33b":"code","56be7e95":"code","2d5a5aae":"code","1cf702d5":"code","c87e4a06":"code","fb1c4419":"code","506311ff":"code","73919a3f":"code","12b62274":"code","69464ee0":"code","1412e538":"code","02f8447b":"code","aed10629":"code","c5ef4d80":"code","8fb8433b":"code","6f5bbb30":"code","eb4683d0":"code","b5330fab":"code","6a8e1e3f":"code","2968230a":"markdown","c143f5cd":"markdown","d7fca475":"markdown","b1ed7933":"markdown","52767b55":"markdown","b7f5d560":"markdown","8dc28558":"markdown","9ae551d3":"markdown","c925069f":"markdown","d7ff634e":"markdown","861d4e38":"markdown","f1395ed5":"markdown","e7e70550":"markdown","470bc341":"markdown","c6f15285":"markdown","186cb15d":"markdown","6324dd3d":"markdown","3279a0a3":"markdown","4073bcc8":"markdown","50b0e127":"markdown","c9decef6":"markdown","4853f67d":"markdown","c2afabe0":"markdown","d73a7c7d":"markdown","b7da5cfb":"markdown","d496e7c3":"markdown","2c3b30d5":"markdown","3db5d650":"markdown","7ae02f67":"markdown","e5fe0682":"markdown","b9cceb7a":"markdown","bbb968f1":"markdown","389a9b72":"markdown","c9bee95c":"markdown","1ec2c5c9":"markdown","b9393539":"markdown"},"source":{"fa761a85":"import numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline","7fb0a3cc":"df=pd.read_csv('..\/input\/company-bankruptcy-prediction\/data.csv')\nprint(df.shape)\ndf.head(5)","92a59f1e":"df.info()","e577dc4b":"df.isna().sum()","d5bb1a6a":"plt.figure(figsize= (10,10))\nplt.pie(df['Bankrupt?'].value_counts().tolist(), labels = ['0','1'],autopct = '%.2f',colors=['skyblue','black'] ,explode = (0,0.1))\nplt.title('Comparison of the number of companies that survive and go bankrupt')\nplt.legend(['Survive','Bankrupt'])\nplt.axis('equal')\nplt.show()","d5da584c":"corr = df.corr(method = 'spearman')\n\nmask = np.zeros_like(corr, dtype=np.bool)\nmask[np.triu_indices_from(mask)] = True\n\nplt.figure(figsize=(100,100))\nwith sns.axes_style(\"white\"):\n  ax = sns.heatmap(corr, mask=mask,vmin=0., vmax=1,annot = True, cmap='Blues')\nplt.show()","55937644":"corr_df = df[['Bankrupt?',' Total debt\/Total net worth',' Debt ratio %',' Borrowing dependency',' Liability to Equity',' Net Value Growth Rate',' Quick Ratio',' Net Value Per Share (B)', ' Net Value Per Share (A)',\n       ' Net Value Per Share (C)',' ROA(C) before interest and depreciation before interest',' ROA(A) before interest and % after tax',' ROA(B) before interest and depreciation after tax',' Pre-tax net Interest Rate',\n       ' After-tax net Interest Rate',' Non-industry income and expenditure\/revenue',' Net worth\/Assets',' Equity to Liability',' Continuous interest rate (after tax)',' Per Share Net profit before tax (Yuan \u00a5)',\n       ' Net profit before tax\/Paid-in capital',' Persistent EPS in the Last Four Seasons']]","7de56766":"corr2 = corr_df.corr(method = 'spearman')\n\nmask = np.zeros_like(corr2, dtype=np.bool)\nmask[np.triu_indices_from(mask)] = True\n\nplt.figure(figsize=(100,100))\nwith sns.axes_style(\"white\"):\n  ax = sns.heatmap(corr2, mask=mask,vmin=0., vmax=1,annot = True, cmap='Blues')\nplt.show()","5f7a929b":"plt.figure(figsize=(40,40))\nplt.subplot(6,4,1)\nsns.boxplot(x=\"Bankrupt?\", y=\" Total debt\/Total net worth\", data=df, order=[0, 1])\nplt.subplot(6,4,2)\nsns.boxplot(x=\"Bankrupt?\", y=\" Debt ratio %\", data=df, order=[0, 1])\nplt.subplot(6,4,3)\nsns.boxplot(x=\"Bankrupt?\", y=\" Borrowing dependency\", data=df, order=[0, 1])\nplt.subplot(6,4,4)\nsns.boxplot(x=\"Bankrupt?\", y=\" Liability to Equity\", data=df, order=[0, 1])\nplt.subplot(6,4,5)\nsns.boxplot(x=\"Bankrupt?\", y=\" Net Value Growth Rate\", data=df, order=[0, 1])\nplt.subplot(6,4,6)\nsns.boxplot(x=\"Bankrupt?\", y=\" Quick Ratio\", data=df, order=[0, 1])\nplt.subplot(6,4,7)\nsns.boxplot(x=\"Bankrupt?\", y=\" Net Value Per Share (B)\", data=df, order=[0, 1])\nplt.subplot(6,4,8)\nsns.boxplot(x=\"Bankrupt?\", y=\" Net Value Per Share (A)\", data=df, order=[0, 1])\nplt.subplot(6,4,9)\nsns.boxplot(x=\"Bankrupt?\", y=\" Net Value Per Share (C)\", data=df, order=[0, 1])\nplt.subplot(6,4,10)\nsns.boxplot(x=\"Bankrupt?\", y=\" ROA(C) before interest and depreciation before interest\", data=df, order=[0, 1])\nplt.subplot(6,4,11)\nsns.boxplot(x=\"Bankrupt?\", y=\" ROA(A) before interest and % after tax\", data=df, order=[0, 1])\nplt.subplot(6,4,12)\nsns.boxplot(x=\"Bankrupt?\", y=\" ROA(B) before interest and depreciation after tax\", data=df, order=[0, 1])\nplt.subplot(6,4,13)\nsns.boxplot(x=\"Bankrupt?\", y=\" Pre-tax net Interest Rate\", data=df, order=[0, 1])\nplt.subplot(6,4,14)\nsns.boxplot(x=\"Bankrupt?\", y=\" After-tax net Interest Rate\", data=df, order=[0, 1])\nplt.subplot(6,4,15)\nsns.boxplot(x=\"Bankrupt?\", y=\" Net worth\/Assets\", data=df, order=[0, 1])\nplt.subplot(6,4,16)\nsns.boxplot(x=\"Bankrupt?\", y=\" Non-industry income and expenditure\/revenue\", data=df, order=[0, 1])\nplt.subplot(6,4,17)\nsns.boxplot(x=\"Bankrupt?\", y=\" Equity to Liability\", data=df, order=[0, 1])\nplt.subplot(6,4,18)\nsns.boxplot(x=\"Bankrupt?\", y=\" Continuous interest rate (after tax)\", data=df, order=[0, 1])\nplt.subplot(6,4,19)\nsns.boxplot(x=\"Bankrupt?\", y=\" Per Share Net profit before tax (Yuan \u00a5)\", data=df, order=[0, 1])\nplt.subplot(6,4,20)\nsns.boxplot(x=\"Bankrupt?\", y=\" Net profit before tax\/Paid-in capital\", data=df, order=[0, 1])\nplt.subplot(6,4,21)\nsns.boxplot(x=\"Bankrupt?\", y=\" Persistent EPS in the Last Four Seasons\", data=df, order=[0, 1])","19ede29c":"# Plotting Boxplots of the numerical features\n\nplt.figure(figsize = (20,20))\nax =sns.boxplot(data = df, orient=\"h\")\nax.set_title('Bank Data Boxplots', fontsize = 18)\nax.set(xscale=\"log\")\nplt.show()","f35400d0":"from sklearn.feature_selection import SelectKBest, mutual_info_classif\nX = df.drop(['Bankrupt?'], axis = 1)\ny = df['Bankrupt?']\nselector = SelectKBest(mutual_info_classif,k=9)\nselector.fit(X, y)\nX.columns[selector.get_support()]","ded28fe3":"df_model = df[['Bankrupt?',\n       ' ROA(A) before interest and % after tax',\n       ' Continuous interest rate (after tax)',\n       ' Persistent EPS in the Last Four Seasons',\n       ' Per Share Net profit before tax (Yuan \u00a5)', \n       ' Debt ratio %',\n       ' Borrowing dependency', \n       ' Net profit before tax\/Paid-in capital',\n       ' Net Income to Total Assets', \n       \" Net Income to Stockholder's Equity\"]]","da12a490":"df_model.columns = df_model.columns.str.strip()\ndf_model.columns = df_model.columns.str.replace(\" \" ,\"_\")\nprint(\"Nama Kolom setelah dirubah\",\"\\n\",df_model.columns[:10])","597c9895":"X = df_model.drop(['Bankrupt?'], axis = 1)\ny = df_model['Bankrupt?']","8348471c":"from sklearn.model_selection import train_test_split, StratifiedKFold, cross_val_score","d87e65a9":"X_train, X_test, y_train, y_test = train_test_split(X,y,\n                                                   stratify = y,\n                                                   test_size = 0.3,\n                                                   random_state = 2021)","9a6f6bcc":"!pip install lazypredict","db441724":"!pip install pandas -U","9f3d33a1":"import warnings\nwarnings.filterwarnings(\"ignore\")\nfrom sklearn.utils.testing import ignore_warnings","f9c4ca06":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport missingno as msno\nimport sklearn.metrics as met\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import classification_report, precision_score, roc_auc_score, plot_roc_curve, f1_score,confusion_matrix , accuracy_score, recall_score\nimport lazypredict\nfrom lazypredict.Supervised import LazyClassifier","1db41233":"LC = LazyClassifier(verbose=0,ignore_warnings=True, custom_metric=None)\nmodel,predictions = LC.fit(X_train, X_test, y_train, y_test)","a7cdec39":"predictions.sort_values(by='Accuracy')","b1e4b33b":"from sklearn.ensemble import ExtraTreesClassifier\n\nclf = ExtraTreesClassifier()\nclf.fit(X_train,y_train)\ny_pred=clf.predict(X_test)\n\nprint('the result is :')\nprint('Accuracy : '+str(met.accuracy_score(y_test,y_pred)))\nprint('f1 score: '+str(met.f1_score(y_test,y_pred,average='weighted')))","56be7e95":"from sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import StandardScaler\n\nfrom sklearn.svm import SVC\nsvc = make_pipeline(StandardScaler(), SVC(gamma='auto'))\nsvc.fit(X_train,y_train)\ny_pred=svc.predict(X_test)\n\nprint('the result is :')\nprint('Accuracy : '+str(met.accuracy_score(y_test,y_pred)))\nprint('f1 score: '+str(met.f1_score(y_test,y_pred,average='weighted')))","2d5a5aae":"from sklearn.ensemble import AdaBoostClassifier\nfrom sklearn.datasets import make_classification\nabc = AdaBoostClassifier(n_estimators=100, random_state=0)\nabc.fit(X_train,y_train)\ny_pred=abc.predict(X_test)\n\nprint('the result is :')\nprint('Accuracy : '+str(met.accuracy_score(y_test,y_pred)))\nprint('f1 score: '+str(met.f1_score(y_test,y_pred,average='weighted')))","1cf702d5":"from sklearn.linear_model import LogisticRegression\nlr = LogisticRegression()\nlr.fit(X_train,y_train)\ny_pred=lr.predict(X_test)\n\nprint('the result is :')\nprint('Accuracy : '+str(met.accuracy_score(y_test,y_pred)))\nprint('f1 score: '+str(met.f1_score(y_test,y_pred,average='weighted')))","c87e4a06":"from sklearn.ensemble import RandomForestClassifier\nadb = RandomForestClassifier()\nadb.fit(X_train,y_train)\ny_pred=adb.predict(X_test)\n\nprint('the result is :')\nprint('Accuracy : '+str(met.accuracy_score(y_test,y_pred)))\nprint('f1 score: '+str(met.f1_score(y_test,y_pred,average='weighted')))","fb1c4419":"from sklearn.neighbors import KNeighborsClassifier\nknn = KNeighborsClassifier()\nknn.fit(X_train,y_train)\ny_pred=knn.predict(X_test)\n\nprint('the result is :')\nprint('Accuracy : '+str(met.accuracy_score(y_test,y_pred)))\nprint('F1 score: '+str(met.f1_score(y_test,y_pred,average='weighted')))","506311ff":"skfold = StratifiedKFold(n_splits=5, random_state=2021, shuffle=False)\n\nfor train_index, test_index in skfold.split(X_train,y_train):\n    \n    print(\"Train:\", train_index, \"Test:\", test_index)\n    X_train_sm, X_val_sm = X_train.iloc[train_index], X_train.iloc[test_index]\n    y_train_sm, y_val_sm = y_train.iloc[train_index], y_train.iloc[test_index]\n\n\n\nX_train_sm = X_train_sm.values\nX_val_sm = X_val_sm.values\ny_train_sm = y_train_sm.values\ny_val_sm = y_val_sm.values\n\n\ntrain_unique_label, train_counts_label = np.unique(y_train_sm, return_counts=True)\ntest_unique_label, test_counts_label = np.unique(y_val_sm, return_counts=True)\nprint('-' * 84)\n\nprint('Label Distributions: \\n')\nprint(train_counts_label\/ len(y_train_sm))\nprint(test_counts_label\/ len(y_val_sm))","73919a3f":"from sklearn.model_selection import GridSearchCV\nfrom imblearn.pipeline import make_pipeline as imbalanced_make_pipeline\nfrom imblearn.over_sampling import SMOTE\n\n\naccuracy_lst_xtc = []\nprecision_lst_xtc = []\nrecall_lst_xtc = []\nf1_lst_xtc = []\nauc_lst_xtc = []\n\nxtc = ExtraTreesClassifier()\n\n\n## Search grid for optimal parameters\nex_param_grid = {\"max_depth\": [None],\n              \"max_features\": [1, 3, 10],\n              \"min_samples_split\": [2, 3, 10],\n              \"min_samples_leaf\": [1, 3, 10],\n              \"bootstrap\": [False],\n              \"n_estimators\" :[100,300],\n              \"criterion\": [\"gini\"]}\n\ngsxtc = GridSearchCV(xtc,param_grid = ex_param_grid, cv=skfold, scoring=\"accuracy\", n_jobs= 4, verbose = 1)\n\n\nfor train, val in skfold.split(X_train_sm, y_train_sm):\n    pipeline_xtc = imbalanced_make_pipeline(SMOTE(sampling_strategy='minority'), gsxtc)\n    model_xtc = pipeline_xtc.fit(X_train_sm[train], y_train_sm[train])\n    best_est_xtc = gsxtc.best_estimator_\n    prediction_xtc = best_est_xtc.predict(X_train_sm[val])\n    \n    accuracy_lst_xtc.append(pipeline_xtc.score(X_train_sm[val], y_train_sm[val]))\n    precision_lst_xtc.append(precision_score(y_train_sm[val], prediction_xtc))\n    recall_lst_xtc.append(recall_score(y_train_sm[val], prediction_xtc))\n    f1_lst_xtc.append(f1_score(y_train_sm[val], prediction_xtc))\n    auc_lst_xtc.append(roc_auc_score(y_train_sm[val], prediction_xtc))\n\nprint('---' * 45)\nprint('')\nprint('Extra Trees Classifier results:')\nprint('')\nprint(\"accuracy: {}\".format(np.mean(accuracy_lst_xtc)))\nprint(\"precision: {}\".format(np.mean(precision_lst_xtc)))\nprint(\"recall: {}\".format(np.mean(recall_lst_xtc)))\nprint(\"f1: {}\".format(np.mean(f1_lst_xtc)))\n#gsxtc.fit(X_train,y_train)\n#xtc_best = gsxtc.best_estimator_\nprint('Best Estimator = {}',best_est_xtc)\nprint('')\nprint('---' * 45)","12b62274":"# Printing the classification report\n\nlabel = ['0', '1']\npred_xtc_sm = best_est_xtc.predict(X_val_sm)\nprint(classification_report(y_val_sm, pred_xtc_sm, target_names=label))","69464ee0":"CM = pd.DataFrame(confusion_matrix(y_val_sm, pred_xtc_sm), columns = ['Survive','Bankrupt'], index = ['Survive','Bankrupt'])\nCM","1412e538":"from xgboost import XGBClassifier\n\naccuracy_lst_xgb = []\nprecision_lst_xgb = []\nrecall_lst_xgb = []\nf1_lst_xgb = []\nauc_lst_xgb = []\n\nxgb=XGBClassifier()\n\nxgb_params={\"n_estimators\":[67,70,100,120],\n        'reg_lambda':[2,1],\n        'gamma':[0,0.3,0.2,0.1],\n        'eta':[0.06,0.05,0.04],\n        \"max_depth\":[3,5],\n        'objective':['binary:logistic']}\n\ngsxgb = GridSearchCV(xgb,param_grid = xgb_params, cv=skfold, scoring=\"accuracy\", n_jobs= 4, verbose = 1)\n\n\nfor train, val in skfold.split(X_train_sm, y_train_sm):\n    pipeline_xgb = imbalanced_make_pipeline(SMOTE(sampling_strategy='minority'), gsxgb) # SMOTE happens during Cross Validation not before..\n    model_xgb = pipeline_xgb.fit(X_train_sm[train], y_train_sm[train])\n    best_est_xgb = gsxgb.best_estimator_\n    prediction_xgb = best_est_xgb.predict(X_train_sm[val])\n    \n    accuracy_lst_xgb.append(pipeline_xgb.score(X_train_sm[val], y_train_sm[val]))\n    precision_lst_xgb.append(precision_score(y_train_sm[val], prediction_xgb))\n    recall_lst_xgb.append(recall_score(y_train_sm[val], prediction_xgb))\n    f1_lst_xgb.append(f1_score(y_train_sm[val], prediction_xgb))\n    auc_lst_xgb.append(roc_auc_score(y_train_sm[val], prediction_xgb))\n\nprint('---' * 45)\nprint('')\nprint('XGBOOST results:')\nprint('')\nprint(\"accuracy: {}\".format(np.mean(accuracy_lst_xgb)))\nprint(\"precision: {}\".format(np.mean(precision_lst_xgb)))\nprint(\"recall: {}\".format(np.mean(recall_lst_xgb)))\nprint(\"f1: {}\".format(np.mean(f1_lst_xgb)))\n#gsxgb.fit(X_train,y_train)\n#gsxgb_best = gsxgb.best_estimator_\nprint('Best Estimator = {}',best_est_xgb)\nprint('')\nprint('---' * 45)","02f8447b":"# Printing the classification report\n\nlabel = ['0', '1']\npred_xgb_sm = best_est_xgb.predict(X_val_sm)\nprint(classification_report(y_val_sm, pred_xgb_sm, target_names=label))","aed10629":"CM = pd.DataFrame(confusion_matrix(y_val_sm, pred_xgb_sm), columns = ['Survive','Bankrupt'], index = ['Survive','Bankrupt'])\nCM","c5ef4d80":"accuracy_lst_reg = []\nprecision_lst_reg = []\nrecall_lst_reg = []\nf1_lst_reg = []\nauc_lst_reg = []\n\nlog_reg_sm = LogisticRegression()\n#log_reg_params = {}\nlog_reg_params = {\"penalty\": ['l2'],\n                  'C': [0.001, 0.01, 0.1, 1, 10, 100, 1000],\n                  'class_weight': ['balanced',None],\n                  'solver':['newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga']}\n\nrand_log_reg = GridSearchCV(log_reg_sm,param_grid = log_reg_params, cv=skfold, scoring=\"accuracy\", n_jobs= 4, verbose = 1)\n\n\nfor train, val in skfold.split(X_train_sm, y_train_sm):\n    pipeline_reg = imbalanced_make_pipeline(SMOTE(sampling_strategy='minority'), rand_log_reg) # SMOTE happens during Cross Validation not before..\n    model_reg = pipeline_reg.fit(X_train_sm[train], y_train_sm[train])\n    best_est_reg = rand_log_reg.best_estimator_\n    prediction_reg = best_est_reg.predict(X_train_sm[val])\n    \n    accuracy_lst_reg.append(pipeline_reg.score(X_train_sm[val], y_train_sm[val]))\n    precision_lst_reg.append(precision_score(y_train_sm[val], prediction_reg))\n    recall_lst_reg.append(recall_score(y_train_sm[val], prediction_reg))\n    f1_lst_reg.append(f1_score(y_train_sm[val], prediction_reg))\n    auc_lst_reg.append(roc_auc_score(y_train_sm[val], prediction_reg))\n\n\nprint('---' * 45)\nprint('')\nprint('Logistic Regression (SMOTE) results:')\nprint('')\nprint(\"accuracy: {}\".format(np.mean(accuracy_lst_reg)))\nprint(\"precision: {}\".format(np.mean(precision_lst_reg)))\nprint(\"recall: {}\".format(np.mean(recall_lst_reg)))\nprint(\"f1: {}\".format(np.mean(f1_lst_reg)))\nprint('Best Estimator = {}',best_est_reg)\nprint('')\nprint('---' * 45)\n","8fb8433b":"# Printing the classification report\n\nlabel = ['Fin.Stable', 'Fin.Unstable']\npred_reg_sm = best_est_reg.predict(X_val_sm)\nprint(classification_report(y_val_sm, pred_reg_sm, target_names=label))","6f5bbb30":"CM = pd.DataFrame(confusion_matrix(y_val_sm, pred_reg_sm), columns = ['Survive','Bankrupt'], index = ['Survive','Bankrupt'])\nCM","eb4683d0":"accuracy_lst_rfc = []\nprecision_lst_rfc = []\nrecall_lst_rfc = []\nf1_lst_rfc = []\nauc_lst_rfc = []\n\nrfc_sm = RandomForestClassifier()\n#rfc_params = {}\nrfc_params = {'max_features' : ['auto', 'sqrt', 'log2'],\n              'random_state' : [42],\n              'class_weight' : ['balanced','balanced_subsample'],\n              'criterion' : ['gini', 'entropy'],\n              'bootstrap' : [True,False]}\n    \n    \nrand_rfc = GridSearchCV(rfc_sm,param_grid = rfc_params, cv=skfold, scoring=\"accuracy\", n_jobs= 4, verbose = 1)\n\nfor train, val in skfold.split(X_train_sm, y_train_sm):\n    pipeline_rfc = imbalanced_make_pipeline(SMOTE(sampling_strategy='minority'), rand_rfc) # SMOTE happens during Cross Validation not before..\n    model_rfc = pipeline_rfc.fit(X_train_sm, y_train_sm)\n    best_est_rfc = rand_rfc.best_estimator_\n    prediction_rfc = best_est_rfc.predict(X_train_sm[val])\n    \n    accuracy_lst_rfc.append(pipeline_rfc.score(X_train_sm[val], y_train_sm[val]))\n    precision_lst_rfc.append(precision_score(y_train_sm[val], prediction_rfc))\n    recall_lst_rfc.append(recall_score(y_train_sm[val], prediction_rfc))\n    f1_lst_rfc.append(f1_score(y_train_sm[val], prediction_rfc))\n    auc_lst_rfc.append(roc_auc_score(y_train_sm[val], prediction_rfc))\n    \nprint('---' * 45)\nprint('')\nprint(\"accuracy: {}\".format(np.mean(accuracy_lst_rfc)))\nprint(\"precision: {}\".format(np.mean(precision_lst_rfc)))\nprint(\"recall: {}\".format(np.mean(recall_lst_rfc)))\nprint(\"f1: {}\".format(np.mean(f1_lst_rfc)))\nprint('Best Estimator = {}',best_est_rfc)\nprint('---' * 45)","b5330fab":"# Printing the classification report\n\nlabel = ['Fin.Stable', 'Fin.Unstable']\npred_reg_rfc = best_est_rfc.predict(X_val_sm)\nprint(classification_report(y_val_sm, pred_reg_rfc, target_names=label))","6a8e1e3f":"CM = pd.DataFrame(confusion_matrix(y_val_sm, pred_reg_rfc), columns = ['Survive','Bankrupt'], index = ['Survive','Bankrupt'])\nCM","2968230a":"#### <center> XGBoost","c143f5cd":"#### <center> Extra Trees Classifier","d7fca475":"as we know, we have many columns in this dataset, so we will find which column has strong correlation using the spearman method","b1ed7933":"i use mutual info classif to do feature selection because to estimate mutual information for a discrete target variable. Now lets make a new dataframe to do modelling","52767b55":"# I. Exploratory Data Analysis","b7f5d560":"#### Logistic Regression","8dc28558":"Now we want to detecting outliers of the dataset","9ae551d3":"#### <center> Random Forest Classifier","c925069f":"#### KNN Classifier","d7ff634e":"### <center> Moment of Truth\nis the result from lazy predictor accurate? lets see","861d4e38":"its accurate, we got the same value\n\nBut we cant 100% believe the lazy predictor, we must re-check again, and using cross validation to make sure the model work better","f1395ed5":"Based on the results of the analysis that has been carried out, there are points that can be noted, including:\n\n1. Data sets are not normally distributed\n2. There are many outliers which are very likely to influence the results of the modeling\n3. imbalanced dataset, the ratio of 1 and 0 is not comparable, this can be overcome by random over sampling and smote methods","e7e70550":"there is no missing value, which is good, now we are looking at the comparison between companies that go bankrupt and those that survive","470bc341":"As a first step to choosing the best method for modeling, we use the lazy predicton as the initial assumption for selecting the method, then we crosscheck again.","c6f15285":"first of all we need to install this fucking library, im confused how to install it","186cb15d":"There is a very extreme comparison of data between companies that go bankrupt and those that survive\"","6324dd3d":"#### Random Forest Classifier","3279a0a3":"# <center> Intro <center>\n1. Problem = how to predict the company will survive or not with company current condition, and we position ourselves as company consultant, and we must give our client insight to keep their company running well\n2. Data =\n>* what is being predicted? = Company bankruptcy paramater\n>* what is needed in prediction? = selecting the feature we have that impact the bankrupt for company, so we can reduce the risk of the company going bankrupt, and can increase the company's chances of continuing to work\n3. Machine Learning Objective = Maximize chance to survive\n4. Action = dont let the predict fail, we say the company will survive but eventually go bankrupt, and the company will go bankrupt while surviving\n5. Value = keep the company from going bankrupt","4073bcc8":"## Feature Selection","50b0e127":"as we can see, the data types of our dataset is numerical, there are integer and float type\n\nnext step is to check is there any missing value on this dataset?","c9decef6":"### <center> Confussion Matrix <center>","4853f67d":"but we need to tidy up the column names first to avoid calling the names wrong in the future","c2afabe0":"\nBased on the case that our dataset is unbalanced, we must balance the data during modeling","d73a7c7d":"# II. Modelling","b7da5cfb":"#### AdaBoost Classifier","d496e7c3":"* * 0 = Survive *\n* * 1 = Bankrupt *\n\n         - TP: There are companies that are predicted to go bankrupt and in fact go bankrupt\n         - TN: There are companies that are predicted to be Survive and in fact they are Survive\n         - FP: There are companies that are predicted to go bankrupt even though they are surviving\n         - FN: There are companies that are predicted to survive even though they are bankrupt\n\nAction:\n* FP: the image of the consulting firm is inaccurate, but not detrimental to the client company\n* FN: the image of the consulting firm is inaccurate, is detrimental to clients and can cause problems\n\n-> What will be pressed is FN, using recall","2c3b30d5":"### <center> Method Selection <center>","3db5d650":"'weighted': Calculate metrics for each label, and find their average weighted by support (the number of true instances for each label). This alters \u2018macro\u2019 to account for label imbalance; it can result in an F-score that is not between precision and recall.","7ae02f67":"#### <center> Logistic Regression","e5fe0682":"there is still a column that is worth 1, but not as much as the initial heatmap, its okay, we can use the feature selection later","b9cceb7a":"\nIn the heatmap above we focus on the Bankrupt feature, and those that have a correlation value (> 0.2 and >-0.2) with the bankrupt feature are:\n1. Total debt\/Total net worth                            = 0.22\n2. Debt ratio %                                          = 0.22\n3. Borrowing dependency                                  = 0.22\n4. Liability to Equity                                   = 0.2\n5. Net Value Growth Rate                                 = -0.2\n6. Quick Ratio                                           = -0.2\n7. Net Value Per Share (C)                                = -0.2\n8. Net Value Per Share (B)                                = -0.21\n9. Net Value Per Share (A)                                = -0.21\n10. ROA(C) before interest and depreciation before interes = -0.22\n11. ROA(A) before interest and % after tax                 = -0.22\n12. ROA(B) before interest and depreciation after tax      = -0.22\n13. Pre-tax net interest rate                              = -0.22\n14. After-tax net Interest Rate                            = -0.22\n15. Non-industry income and expenditure\/revenue            = -0.22\n16. Net worth\/Assets                                      = -0.22\n17. Equity to Liability                                   = -0.22\n18. Continuous interest rate (after tax)                   = -0.23\n19. Per Share Net profit before tax (Yuan \u00a5)              = -0.23\n20. Net profit before tax\/Paid-in capital                 = -0.23\n21. Persistent EPS in the Last Four Seasons               = -0.24\n\nbut, there is too many column that have strong correlation (1 value) that might be make a bias on correlation, so we try to make a new heatmap based on the column that have strong correlation (positive and negative) with bankruptcy column","bbb968f1":"based on confussion matrix we choose XGBoost","389a9b72":"Now we compare","c9bee95c":"#### SVC","1ec2c5c9":"As seen in the boxplot above, most of the minimum value of companies that survive is higher than the minimum value of a company that is bankrupt, except for one feature debt ratio.","b9393539":"#### Extra Trees Classifier"}}