{"cell_type":{"9924fee3":"code","ab43ab88":"code","bd47b953":"code","aaece341":"code","d0e48225":"code","860cce10":"code","7310da79":"code","0fd40d8a":"code","7fc1cf93":"code","16dda1ae":"code","d5dcd233":"code","56094cc7":"code","19e0001a":"code","4370983b":"code","53d6054f":"code","0830ba75":"code","b78b5bbd":"code","ef0af1d9":"code","92a7197b":"markdown","69fe38b7":"markdown","92b55062":"markdown","9290a71e":"markdown","f1023975":"markdown","1c749917":"markdown","00299abf":"markdown","ca357a4f":"markdown","0626c827":"markdown"},"source":{"9924fee3":"# Visualization\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Text processing\nimport string\nfrom nltk.corpus import stopwords\n\nimport numpy as np\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","ab43ab88":"sample_submission = pd.read_csv('..\/input\/commonlitreadabilityprize\/sample_submission.csv')\nsample_submission","bd47b953":"df_train = pd.read_csv('..\/input\/commonlitreadabilityprize\/train.csv')\nprint(f'Train shape: {df_train.shape}\\n')\nprint('Train info:')\ndf_train.info()\nprint()\ndf_train.head()","aaece341":"df_test = pd.read_csv('..\/input\/commonlitreadabilityprize\/test.csv')\nprint(f'Test shape: {df_test.shape}\\n')\nprint('Test info:')\ndf_test.info()\nprint()\ndf_test","d0e48225":"df_train.describe(include='all')","860cce10":"df_test.describe(include='all')","7310da79":"for feature in df_train.columns:\n    print(f\"Cardinality of {feature.upper()} in train dataset: {df_train[feature].nunique()}\")\nprint()\nfor feature in df_test.columns:\n    print(f\"Cardinality of {feature.upper()} in test dataset: {df_test[feature].nunique()}\")","0fd40d8a":"df_train['url_legal'] = df_train['url_legal'].fillna('Missing')\ndf_train['license'] = df_train['license'].fillna('Missing')\n\ndf_test['url_legal'] = df_test['url_legal'].fillna('Missing')\ndf_test['license'] = df_test['license'].fillna('Missing')","7fc1cf93":"df_train['target'].sort_values(ascending=True)","16dda1ae":"pd.set_option('display.max_colwidth', 300) # That allow us to check first 300 symbols of text in cell (replace 300 with None to see whole text)","d5dcd233":"df_train.sort_values('target')['excerpt'].head()","56094cc7":"df_train.sort_values('target')['excerpt'].tail()","19e0001a":"pd.set_option('display.max_colwidth', 50)","4370983b":"df_train_new = df_train.copy()\ndf_test_new = df_test.copy()","53d6054f":"df_train_new = df_train_new.sort_values(by='target', ascending=False)\ndf_train_new","0830ba75":"df_train_new['word_count'] = df_train_new['excerpt'].apply(lambda x: len(str(x).split()))\ndf_test_new['word_count'] = df_test_new['excerpt'].apply(lambda x: len(str(x).split()))\n\ndf_train_new['unique_word_count'] = df_train_new['excerpt'].apply(lambda x: len(set(str(x).split())))   \ndf_test_new['unique_word_count'] = df_test_new['excerpt'].apply(lambda x: len(set(str(x).split())))\n\ndf_train_new['stop_words_count'] = df_train_new['excerpt'].apply(lambda x: len([w for w in str(x).lower().split() if w in stopwords.words('english')]))\ndf_test_new['stop_words_count'] = df_test_new['excerpt'].apply(lambda x: len([w for w in str(x).lower().split() if w in stopwords.words('english')]))\n\ndf_train_new['mean_word_length'] = df_train_new['excerpt'].apply(lambda x: np.mean(len(str(x).split())))\ndf_test_new['mean_word_length'] = df_test_new['excerpt'].apply(lambda x:  np.mean(len(str(x).split())))\n\ndf_train_new['char_count'] = df_train_new['excerpt'].apply(lambda x: len(str(x)))\ndf_test_new['char_count'] = df_test_new['excerpt'].apply(lambda x: len(str(x)))\n\ndf_train_new['punctuation_count'] = df_train_new['excerpt'].apply(lambda x: len([c for c in str(x) if c in string.punctuation]))\ndf_test_new['punctuation_count'] = df_test_new['excerpt'].apply(lambda x: len([c for c in str(x) if c in string.punctuation]))\n\n\n# Individual punctuation marks can also be important\ndf_train_new['question_mark_count'] = df_train_new['excerpt'].apply(lambda x: len([c for c in str(x) if c == '?']))\ndf_test_new['question_mark_count'] = df_test_new['excerpt'].apply(lambda x: len([c for c in str(x) if c == '?']))\n\ndf_train_new['exclamation_mark_count'] = df_train_new['excerpt'].apply(lambda x: len([c for c in str(x) if c == '!']))\ndf_test_new['exclamation_mark_count'] = df_test_new['excerpt'].apply(lambda x: len([c for c in str(x) if c == '!']))\n\ndf_train_new['comma_mark_count'] = df_train_new['excerpt'].apply(lambda x: len([c for c in str(x) if c == ',']))\ndf_test_new['comma_mark_count'] = df_test_new['excerpt'].apply(lambda x: len([c for c in str(x) if c == ',']))\n\ndf_train_new['point_count'] = df_train_new['excerpt'].apply(lambda x: len([c for c in str(x) if c == '.']))\ndf_test_new['pointCount'] = df_test_new['excerpt'].apply(lambda x: len([c for c in str(x) if c == '.']))\n\ndf_train_new['ellipsis_count'] = df_train_new['excerpt'].apply(lambda x: len([c for c in str(x) if c == '...']))\ndf_test_new['ellipsis_count'] = df_test_new['excerpt'].apply(lambda x: len([c for c in str(x) if c == '...']))\n\n\n# I guess that in texts for elementary school number of pronouns is more because sentences are easier\nmy_stopwords = ['i', 'me', 'my', 'mine', 'you', 'your', 'yours', 'he', 'him', 'his', 'she', 'her', 'it', 'its', 'we', 'our', 'they', 'their']\n\ndf_train_new['pronoun_count'] = df_train_new['excerpt'].apply(lambda x: len([w for w in str(x).lower().split() if w in my_stopwords]))\ndf_test_new['pronoun_count'] = df_test_new['excerpt'].apply(lambda x: len([w for w in str(x).lower().split() if w in my_stopwords]))","b78b5bbd":"bins = 100\nplt.figure(figsize=(8,6))\nplt.hist(df_train_new['target'], bins, alpha=0.5, label='target')\nplt.title('Target distribution')\nplt.legend(loc='upper right')\nplt.show()","ef0af1d9":"meta_features = ['standard_error', 'word_count', 'unique_word_count', 'stop_words_count', 'mean_word_length', 'char_count', 'punctuation_count', 'question_mark_count',\n                'exclamation_mark_count', 'comma_mark_count', 'point_count', 'ellipsis_count', 'pronoun_count']\n\nfig, axs = plt.subplots(ncols=2, nrows=len(meta_features), figsize=(20, 50), dpi=100)\n\nfor i, feature in enumerate(meta_features):\n    sns.histplot(df_train_new[feature], label=f'{feature} distribution in Training dataset', ax=axs[i][0], kde=True)\n    sns.regplot(data=df_train_new, x='target', y=feature, ax=axs[i][1])\n    \n    for j in range(2):\n        axs[i][j].set_xlabel('')\n        axs[i][j].tick_params(axis='x', labelsize=12)\n        axs[i][j].tick_params(axis='y', labelsize=12)\n        axs[i][j].legend()\n        \n    axs[i][0].set_title(f'{feature} distribution in Training dataset', fontsize=13)\n    axs[i][1].set_title('Target distribution', fontsize=13)\n    \nplt.show()","92a7197b":"With creating new features we can improve our understanding of undependent features","69fe38b7":"Let's check cardinality of features","92b55062":"As seen word_count, unique_word_count, unique_word_count, stop_words_count, mean_word_length, char_count and comma_mark_count decreases while target feature increases. But **word_count, unique_word_count and char_count decreases** more than others. It is logical because vocabulary in primary school is small, words are simple, not very long and they often repeated.\nSuch features as **point_count and pronoun_count** increases while target feature increases. That's because in primary school sentences are short -> more sentences -> more points. As I suggested pronoun_count is more in primary school than in high school, because when text consists of small sentences noun quantity is small and author have to replace nouns with pronouns.\n**standard_error** is higher for primary and high school.\nAll other features (that I haven't highlighted in bold) are not representative.","9290a71e":"Value of target feature corresponds to complexity of text. Larger value means that text is easier, i.e. text with target value -3.676268 is the most complex. Text appropriate 1.711390 value is the easiest.","f1023975":"You can see that suggestion was prooved","1c749917":"Let's impute all missing values","00299abf":"# Feature engeneering","ca357a4f":"Let's check this","0626c827":"Target feature has normal distribution"}}