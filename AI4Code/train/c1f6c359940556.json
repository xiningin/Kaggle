{"cell_type":{"d3e38901":"code","d3572d0a":"code","9460360a":"code","624adae3":"code","c62e469c":"code","90905b37":"code","0ed304cb":"code","00b40053":"code","67eab18a":"code","b74c5ec3":"code","99f38910":"code","5c9eada8":"code","69b002d8":"code","d68be349":"markdown","346e734b":"markdown","056145a7":"markdown","ab39b334":"markdown","3ffde658":"markdown","2f0b7134":"markdown","df25ffc9":"markdown","d3eadd3c":"markdown","f417da01":"markdown","78f9cee7":"markdown","45de96ad":"markdown","a64cc617":"markdown","ccdb8e88":"markdown"},"source":{"d3e38901":"import pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom sklearn.datasets import load_wine\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.model_selection import StratifiedKFold, GridSearchCV, train_test_split\nfrom sklearn.svm import SVC\nfrom sklearn.metrics import confusion_matrix, accuracy_score\nfrom sklearn.feature_selection import f_classif","d3572d0a":"df = pd.DataFrame(load_wine().data, columns=load_wine().feature_names) #transforming the dataset to dataframe type\ndf['wine'] = pd.Series(load_wine().target) #adding target column\ndf.head()","9460360a":"df.describe(include='all')","624adae3":"df.info()","c62e469c":"sns.barplot(x=df.wine.value_counts(normalize=True).index, y=df.wine.value_counts(normalize=True).values, palette='winter')","90905b37":"for feature in df.columns:\n    print(feature)\n    print('Skewness: ', df[feature].skew())\n    print('Kurtosis: ', df[feature].kurt())\n    print('---------------')","0ed304cb":"plt.figure(figsize=(12,12))\nplt.title('Paerson correlation coefficient between dataset variables \\n')\nsns.heatmap(df.drop('wine',axis=1).corr(), annot=True, cmap='viridis_r')","00b40053":"scores = f_classif(df.drop('wine', axis=1), df.wine)   #performing ANOVA for the features against the target\n\nfor i in range(len(df.columns)-1):                     #printing f-value and p-value for every feature\n        print(df.drop('wine', axis=1).columns[i] + ': F-value --> ', scores[0][i], ' p-value --> ', scores[1][i])\n        print('--------------------------------------------------------------------------------')","67eab18a":"df_scores = pd.DataFrame({'features': df.drop('wine',axis=1).columns, 'ANOVA_f_values': scores[0]}).sort_values(by=['ANOVA_f_values'], ascending=False)\nplt.figure(figsize=(10,6))\nbar = sns.barplot(x='features', y='ANOVA_f_values', data=df_scores)\nbar.set_xticklabels(bar.get_xticklabels(),rotation=90);\nbar.set_title('ANOVA f_values for each label against target')","b74c5ec3":"sns.pairplot(data=df[['flavanoids','proline','od280\/od315_of_diluted_wines','alcohol','color_intensity','wine']], hue='wine')","99f38910":"scaler = MinMaxScaler() #initializing a scaler which will transform all data in numbers with range (0,1)\nX = df.drop('wine', axis=1)\ny = df.wine.values #separating labels from target\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=1) #splitting the data into training set and test set\nX_train = scaler.fit_transform(X_train) #scaling separately train and test sets\nX_test = scaler.fit_transform(X_test)","5c9eada8":"clf = SVC()\ncv = StratifiedKFold(n_splits=10, random_state=56, shuffle=True) #initializing StratifiedKFold for cross validation purpose\n\n#dictionary of possible hyperparameters values of SVC model\nparam = {'C': [0.1,1, 10, 100],                        # C is a regularization parameter, squared l2 parameter\n         'gamma': [0.01,0.1,1,10],                     # gamma is the kernel coefficient for \u2018rbf\u2019, \u2018poly\u2019, and \u2018sigmoid\u2019\n         'kernel': ['poly','linear','rbf','sigmoid'],  # kernel type to be used in the algorithm\n         #'degree': [2,3,4]                            # degree is the degree of poly function\n        }\n\n\ngrid = GridSearchCV(clf, param_grid = param, cv=cv, scoring='accuracy', verbose=1) #initializing a cross validation for tuning\ngrid.fit(X_train, y_train) #fitting on the training set\n\nprint('Best estimator is:\\n\\n',grid.best_estimator_)\nprint('\\n')\nprint('Average accuracy of the model on the training set is: ', grid.best_score_)","69b002d8":"pred = grid.best_estimator_.predict(X_test) #predicting target for the test set\nprint(confusion_matrix(y_test, pred))\nprint('\\nAccuracy Score: ', accuracy_score(y_test, pred))","d68be349":"Checking skewness and kurtosis, we can see that Magnesium has higher absolute values of both skewness and kurtosis.\n\nSkewness and Kurtosis are useful to understand how far is a distribution from the normal distribution, in some cases some features or a dependent variable in regression problems, need to be transformed, for example using log-transformation, because many algorithms and statistical model have as assumption that the distribution of the variable is normal.\n\nIf skewness is in the interval (-1, 1) the distribution has an acceptable level of skeweness.\nThen, if kurtosis is in the interval (-2, 2) the distribution has an acceptable level of kurtosis.\n\nIn this case I won't transform or delete some features, but in other cases that can be a good idea.","346e734b":"Checking for multicollinearity between continuous variables, we can see that:\n- od280\/0d315_of_diluted_wines and total_phenols has an high value of positive correlation\n- od280\/0d315_of_diluted_wines and flavonoids has an high value of positive correlation\n\nIn some cases, we can decide to drop a feature that is highly correlated to another one.\n\nMoreover, we used the Pearson's correlation coefficient because all of our independent variables were numerical and continuous or at least discrete.\n\nBut remember that Pearson's coefficient isn't the only way to compute correlation or relationships between variables. In this notebook we will try to understand how to \"compute\" the relationship between contonuous variables and our target variables, that is categorical.","056145a7":"I used GridSearchCV in order to find the best hyperparameter based on accuracy score:\n- C = 1\n- gamma = 0.1\n- kernel = 'rbf'\n\nI decided to evaluate the model using accuracy because it's a multiclass problem and classes aren't so unbalanced to use other metrics. So we got about 99,2% of accuracy on the training set, let's check the performance of this model on the test set.","ab39b334":"This dataset is for learning or testing purpose, so we can't see any null value among the columns.","3ffde658":"On the test set we obtained 97,7% of accuracy score.\n\nHope you found this notebook useful :D","2f0b7134":"Computing ANOVA f-values, we can see that flavanoids is the feature that better separate classes because this test checks, for each label, grouping by target classes, if the mean of the subsamples are significantly different.\n\nIn this case we could try to build model dropping some features having a low realationship with target, but for now we will use all features.","df25ffc9":"What ANOVA show us: this test show us if subsamples made by each different class of our target (0,1 and 2 of wine column) have different means at a significant level of confidence, let's say 95%. In this case we have a significant evidence that, for each feature, the subsamples have a different mean if p-value is less than 5%.\n\nInterpreting the f value, the biggest value (flavanoids) tells us that this feature separate better the target classes.","d3eadd3c":"I'm a beginner at data science, so I made this notebook to help other beginners to find some usefull intuitions about correlation, relationship within different kind of variables and hyperparameter optimization with a simple model like Support Vector Machine.\n\nFor this purpose we will use a simple dataset, the wine dataset available directly in sklearn.\n\nRemember to upvote this notebook if you find it useful :D","f417da01":"We can see that classes are quite balanced.","78f9cee7":"# Support Vector Classifier - Wine Dataset","45de96ad":"We will use a support vector machine: this algorithm is based on geometrical distances in the space, so it is better to have points in the same scale.\n\nRemember, whene dividing in training and test set, to scale the data after the split because if you have new data and you want to do a prediction, you will scale only the new data.","a64cc617":"From these table and the first 5 rows of the dataset we can say:\n- all features are numerical\n- magnesium and proline are always integers, so can be considered discrete variables\n- all other features are continuous variables\n- features have different scales so probably we need to scale them\n- target has 3 classes\n- we have 178 observations","ccdb8e88":"These are the variables that better separate the classes, and we can see from both univariate and bivariate plots.\n\nThis plot is really useful and allows to visualize, also without computing any statistical test, which features better separate classes, obviously in classification problems.\n\nBut if we have many variables, it's really hard to plot a complete visualization."}}