{"cell_type":{"3a3bd1cc":"code","5cc81370":"code","98a112b1":"code","4723594b":"code","7d7d6707":"code","0f3cc2eb":"code","abad7c58":"code","52b50ff1":"code","4bc59c53":"code","a7cf706b":"code","d858e377":"code","512f7f9d":"code","d7f35a92":"code","738e2ae3":"code","76b08273":"code","7cd638dc":"code","a9e4cd8d":"code","77d67ce1":"code","f5b7c556":"code","62252c15":"code","de55a5b1":"code","570312b9":"code","13c55891":"code","599685b3":"code","c14d8dce":"code","697e2831":"code","9ae87939":"code","9d27b141":"code","30e93931":"code","169aecaf":"code","3575cf05":"code","c1e6643b":"code","2e6bec68":"code","2a3efa16":"code","d1a9b9ae":"code","b55a8e61":"code","e9ed622b":"code","e5794b29":"code","da9c37b1":"code","97c55570":"code","4a500446":"markdown","f9127c78":"markdown","5e08d720":"markdown","03ecebee":"markdown","2a1399d3":"markdown","3d72cd2e":"markdown","9c3cae86":"markdown","9c8f9bf9":"markdown","ad946327":"markdown","16bebc7d":"markdown","76b323b9":"markdown","6cb66850":"markdown","6b737d9b":"markdown","6acac7c4":"markdown","c8a471aa":"markdown","e70d49ac":"markdown","54bec3b0":"markdown","1367d2fc":"markdown","ed518a18":"markdown"},"source":{"3a3bd1cc":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","5cc81370":"!cp '\/kaggle\/input\/nlp-getting-started\/test.csv' '\/kaggle\/working\/test.csv'\n!cp '\/kaggle\/input\/nlp-getting-started\/train.csv' '\/kaggle\/working\/train.csv'","98a112b1":"!pip install fastai --upgrade","4723594b":"%reload_ext autoreload\n%autoreload 2\n\nfrom fastai.text.all import *\npath = Path('\/kaggle\/working\/')\npath.ls()","7d7d6707":"train_df = pd.read_csv(path\/'train.csv')\ntest_df = pd.read_csv(path\/'test.csv')\nprint(train_df.head())\n#simple_train_df = train_df[['id', 'text', 'target']]\n#simple_test_df = train_df[['id', 'text']]","0f3cc2eb":"dls = TextDataLoaders.from_csv(path=path, \n                               csv_fname='train.csv', \n                               text_col='text', \n                               label_col='target', \n                               valid_pct=0.2)\nfirst(dls[0])","abad7c58":"learn = text_classifier_learner(dls, AWD_LSTM, drop_mult=0.5, metrics=F1Score())\nlearn.model_dir = '\/kaggle\/working\/'\nlearn.fine_tune(2, 1e-2)","52b50ff1":"learn.save('simple_NLP')","4bc59c53":"learn.load('simple_NLP')","a7cf706b":"print(learn.predict('It was crazy'))\nprint(learn.predict('It was a disaster'))","d858e377":"print('Total number of needed predictions:',len(test_df))\npredictions = []\nids = []\n\nfor i in range(len(test_df)):\n    print(i)\n    pred, _, _ = learn.predict(test_df.loc[i, 'text'])\n    ids.append(test_df.loc[i, 'id'])\n    predictions.append(pred)\n\n#print(predictions, ids)","512f7f9d":"submission = pd.DataFrame({'id': ids, 'target':predictions})\nprint(submission)\nsubmission.to_csv(\"simple_submission.csv\", index=False)","d7f35a92":"df = pd.concat([train_df, test_df])","738e2ae3":"df.head()","76b08273":"dls_lm = TextDataLoaders.from_df(df, text_col='text', is_lm=True, valid_pct=0.0)\ndls_lm.show_batch(max_n=3)","7cd638dc":"lm_learn = language_model_learner(dls_lm, AWD_LSTM, metrics=[accuracy, Perplexity()], path=path, wd=0.1)\nlm_learn.model_dir = '\/kaggle\/working\/'","a9e4cd8d":"#lm_learn.lr_find()","77d67ce1":"lm_learn.fit_one_cycle(1, 1e-2)\nlm_learn.save('1epoch')","f5b7c556":"lm_learn = lm_learn.load('1epoch')\nlm_learn.unfreeze()\nlm_learn.fit_one_cycle(10, 1e-3)\nlm_learn.save_encoder('finetuned')","62252c15":"dls_clas = TextDataLoaders.from_csv(path=path, \n                               csv_fname='train.csv', \n                               text_col='text', \n                               label_col='target', \n                               valid_pct=0.2,\n                               text_vocab=dls_lm.vocab)","de55a5b1":"adv_learn = text_classifier_learner(dls_clas, AWD_LSTM, drop_mult=0.5, metrics=F1Score())\nadv_learn.model_dir = '\/kaggle\/working\/'\nadv_learn = adv_learn.load_encoder('finetuned')\nadv_learn.fit_one_cycle(1, 2e-2)","570312b9":"adv_learn.freeze_to(-2)\nadv_learn.fit_one_cycle(1, slice(1e-2\/(2.6**4),1e-2))","13c55891":"adv_learn.freeze_to(-3)\nadv_learn.fit_one_cycle(1, slice(5e-3\/(2.6**4),5e-3))","599685b3":"adv_learn.unfreeze()\nadv_learn.fit_one_cycle(2, slice(1e-3\/(2.6**4),1e-3))","c14d8dce":"test_df.loc[0, 'text']","697e2831":"adv_learn.predict(test_df.loc[0, 'text'])","9ae87939":"predictions = []\nids = []\n\nfor i in range(len(test_df)):\n    print(i)\n    pred, _, _ = adv_learn.predict(test_df.loc[i, 'text'])\n    ids.append(test_df.loc[i, 'id'])\n    predictions.append(pred)","9d27b141":"submission = pd.DataFrame({'id': ids, 'target':predictions})\nprint(submission)\nsubmission.to_csv(\"complex_submission.csv\", index=False)","30e93931":"dls_lm_back = TextDataLoaders.from_df(df, text_col='text', is_lm=True, valid_pct=0, backward = True)\ndls_lm_back.show_batch(max_n=3)","169aecaf":"lm_learn_back = language_model_learner(dls_lm_back, AWD_LSTM, metrics=[accuracy, Perplexity()], path=path, wd=0.1)\nlm_learn_back.model_dir = '\/kaggle\/working\/'","3575cf05":"lm_learn_back.fit_one_cycle(1, 1e-2)\nlm_learn_back.save('1epoch_back')","c1e6643b":"lm_learn_back = lm_learn_back.load('1epoch_back')\nlm_learn_back.unfreeze()\nlm_learn_back.fit_one_cycle(10, 1e-3)\nlm_learn_back.save_encoder('finetuned_back')","2e6bec68":"dls_clas_back = TextDataLoaders.from_csv(path=path, \n                               csv_fname='train.csv', \n                               text_col='text', \n                               label_col='target', \n                               valid_pct=0.2,\n                               text_vocab=dls_lm_back.vocab,\n                               backwards = True)","2a3efa16":"back_learn = text_classifier_learner(dls_clas_back, AWD_LSTM, drop_mult=0.5, metrics=F1Score())\nback_learn.model_dir = '\/kaggle\/working\/'\nback_learn = back_learn.load_encoder('finetuned_back')\nback_learn.fit_one_cycle(1, 2e-2)","d1a9b9ae":"back_learn.freeze_to(-2)\nback_learn.fit_one_cycle(1, slice(1e-2\/(2.6**4),1e-2))","b55a8e61":"back_learn.freeze_to(-3)\nback_learn.fit_one_cycle(1, slice(5e-3\/(2.6**4),5e-3))","e9ed622b":"back_learn.unfreeze()\nback_learn.fit_one_cycle(2, slice(1e-3\/(2.6**4),1e-3))","e5794b29":"back_learn.predict(test_df.loc[0, 'text'])","da9c37b1":"predictions = []\nids = []\n\nfor i in range(len(test_df)):\n    print(i)\n    _, _, forward_predict = adv_learn.predict(test_df.loc[i, 'text'])\n    _, _, backwards_predict = back_learn.predict(test_df.loc[i, 'text'])\n    pred = bool(forward_predict[1]+backwards_predict[1] > 1)\n    ids.append(test_df.loc[i, 'id'])\n    predictions.append(int(pred))","97c55570":"submission = pd.DataFrame({'id': ids, 'target':predictions})\nprint(submission)\nsubmission.to_csv(\"ensemble_submission.csv\", index=False)","4a500446":"# Simplest possible model","f9127c78":"We want first to create a very simple model that allows us to predict the classification. For that we create a DataLoader","5e08d720":"dls = TextDataLoaders.from_csv(path=path, \n                               csv_fname='train.csv', \n                               text_col='text', \n                               label_col='target', \n                               valid_pct=0.2)\n\nconfig = awd_lstm_clas_config.copy()\nconfig['bidir'] = True\nlearn = text_classifier_learner(dls, AWD_LSTM, config=config, drop_mult=0.5, wd=1e-2, pretrained=False, metrics = F1Score())\nlearn.model_dir = '\/kaggle\/working\/'","03ecebee":"We can predict the result of simple sentences","2a1399d3":"Then we create the learner, and train it","3d72cd2e":"And submit it","9c3cae86":"# Backwards model","9c8f9bf9":"We create two lists with the predictions","ad946327":"We can create an ensemble running the model backwards","16bebc7d":"If we read the data, we will get the corresponding DataFrames","76b323b9":"# Complex model","6cb66850":"Now we create the language model","6b737d9b":"We copy the files to working, so that the fastai library has not trouble with them","6acac7c4":"# NLP learning using fast.ai library","c8a471aa":"First we load some libraries","e70d49ac":"Now we want to create a more complex model. Let us start by creating a language model.","54bec3b0":"Once we have the language model, we can train the text classifier","1367d2fc":"Now create the learner","ed518a18":"Finally, produce the output"}}