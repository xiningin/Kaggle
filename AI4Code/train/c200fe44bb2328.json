{"cell_type":{"5c7b9afc":"code","f1dd835b":"code","b9d15f18":"code","dc4b0633":"code","2a50bef9":"code","7d60e37d":"code","032e36c3":"code","25849cc0":"code","bafe2300":"code","ca6ed748":"code","aaaa123e":"code","84a764d8":"code","e7a6bf96":"code","048bd9d3":"code","b126151a":"code","55f01776":"code","668bcfff":"code","75e130cd":"code","f10e370f":"code","4f8afa84":"code","f7ad15eb":"code","2281cbc6":"code","d6cc7d3e":"code","3857559e":"code","1562651f":"code","784c13f3":"code","6ecc1b66":"code","f338ae7a":"markdown","41145ad0":"markdown","56fc8455":"markdown","53b173e0":"markdown","4c41847a":"markdown","5dc70fc7":"markdown","55d5570a":"markdown","ce0066f2":"markdown","55c0adbc":"markdown"},"source":{"5c7b9afc":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","f1dd835b":"import pandas as pd\nimport numpy as np\n\nfrom sklearn import tree\nfrom sklearn.datasets import load_wine\nfrom sklearn.model_selection import train_test_split\n\nfrom sklearn.datasets import load_boston\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.tree import DecisionTreeRegressor\n\n\nimport graphviz\nimport matplotlib.pyplot as plt","b9d15f18":"#1) We used wine data as example to simulate the operation on Decision tree classification model\nwine=load_wine()\npd.concat([pd.DataFrame(wine.data),pd.DataFrame(wine.target)],axis=1)","dc4b0633":"xtrain,xtest,ytrain,ytest=train_test_split(wine.data,wine.target,test_size=0.3)","2a50bef9":"xtrain.shape","7d60e37d":"# 3 categories\nwine.target","032e36c3":"# Turning and find the best max_depth\ntest=[]\nfor i in range(10):\n    clf=tree.DecisionTreeClassifier(criterion='entropy'\n                                    ,random_state=30\n                                    ,splitter='random'\n                                    ,max_depth=i+1\n                                   )\n    clf=clf.fit(xtrain,ytrain)\n    score=clf.score(xtest,ytest)\n    test.append(score)\nplt.plot(range(1,11),test,color='red',label='max_depth')\nplt.legend()\nplt.show()","25849cc0":"# training\nclf=tree.DecisionTreeClassifier(criterion='entropy' # two criterion  'entropy','gini'\n# The calculation formula of 'entropy' is more complicated than 'gini', so it will run slowly,\n# and have higher accuracy. However 'entropy' is easy to overfitor for high-dimensional data, \n# and too much noise data. \n                                \n                                ,random_state=30\n# random_state ,splitter can increase accuracy of the learning model\n                                ,splitter='random'\n                                ,max_depth=6\n# max_depth \uff1aEffective when high dimensionality and low sample\n                                #,min_samples_leaf=10\n# min_samples_leaf \uff1aThe number of samples of child nodes must exceed this number\n                                #,min_samples_split=25\n# min_samples_split : A node must meet the training sample that will be allowed to branch                                \n# min_impurity_decrease : if the impurity < setting number, it will not allowed to branch                                \n                                #,class_weight=None\n# class_weight : operate imblance data \n                               )\n\nclf=clf.fit(xtrain,ytrain)\nscore=clf.score(xtest,ytest)\nscore  # return the accuracy ","bafe2300":"wine.feature_names","ca6ed748":"# draw graph\ndot_data=tree.export_graphviz(clf\n                             ,feature_names=wine.feature_names\n                             ,class_names=['gin','sherry','Berumotto']# 3 categories of wine.target\n                             ,filled=True  # filled color\n                             ,rounded=True)  # box rounded or right angled\ngraph=graphviz.Source(dot_data)\ngraph","aaaa123e":"# feature importances_\n[*zip(wine.feature_names,clf.feature_importances_)]","84a764d8":"# test score\nscore_test=clf.score(xtest,ytest)\nscore_test","e7a6bf96":"# predict\nclf.apply(xtest)\nclf.predict(xtest)","048bd9d3":"\nfrom matplotlib.colors import ListedColormap\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.datasets import make_moons,make_circles,make_classification\n","b126151a":"# create 3 type of data (moon data, circle data, two subtypes data)\n#make_classfication into random two subtypes data\nx,y=make_classification(n_samples=100 # 100 sample\n                      ,n_features=2 # 2features,2 dimensions\n                      ,n_redundant=0 \n                      ,n_informative=2 # contain 2 information data\n                      ,random_state=1 \n                      ,n_clusters_per_class=1 # label=1\n                      )\nrng=np.random.RandomState(2)\nx+=2*rng.uniform(size=x.shape)\nlinearly_separable=(x,y)\ndatasets=[make_moons(noise=0.3,random_state=0),\n         make_circles(noise=0.2,factor=0.5,random_state=1),\n         linearly_separable]","55f01776":"# draw figure\nfigure=plt.figure(figsize=(6,9))\ni=1\n\nfor ds_index,ds in enumerate(datasets):\n    x,y=ds\n    x=StandardScaler().fit_transform(x)\n    x_train,x_test,y_train,y_test=train_test_split(x,y,test_size=4,random_state=42)\n    x1_min,x1_max=x[:,0].min()-.5,x[:,0].max()+.5\n    x2_min,x2_max=x[:,1].min()-.5,x[:,1].max()+.5\n    array1,array2=np.meshgrid(np.arange(x1_min,x1_max,0.2),np.arange(x2_min,x2_max,0.2))\n    cm=plt.cm.RdBu\n    cm_bright=ListedColormap('#FF0000','#0000FF')\n    ax=plt.subplot(len(datasets),2,i)\n    if ds_index==0:\n        ax.set_title('Input data')\n#train set   \n    ax.scatter(x_train[:,0],x_train[:,1],c=y_train,cmap=cm_bright,edgecolors='k')\n#test set\n    ax.scatter(x_test[:,0],x_test[:,1],c=y_test,cmap=cm_bright,alpha=0.6,edgecolors='k')\n    ax.set_xlim(array1.min(),array1.max())\n    ax.set_ylim(array2.min(),array2.max())\n    ax.set_xticks(())\n    ax.set_yticks(())\n        \n    i+=1\n    \n    ax=plt.subplot(len(datasets),2,i)\n    clf=tree.DecisionTreeClassifier(max_depth=5)\n    clf.fit(x_train,y_train)\n    score=clf.score(x_test,y_test)\n    \n    z=clf.predict_proba(np.c_[array1.ravel(),array2.ravel()])[:,1]\n#ravel() change multi-dimensional arrays with 1-dimensional arrays\n# np.c_   combine two arrays\n\n    z=z.reshape(array1.shape)\n    ax.contourf(array1,array2,z,cmap=cm,alph=.8)\n\n    ax.scatter(x_train[:,0],x_train[:,1],c=y_train,cmap=cm_bright,edgecolors='k')\n    ax.scatter(x_test[:,0],x_test[:,1],c=y_test,cmap=cm_bright,edgecolors='k',alpha=0.6)\n    \n    ax.set_xlim(array1.min(),array1.max())\n    ax.set_ylim(array2.min(),array2.max())\n    ax.set_xticks(())\n    ax.set_yticks(())\n\n    if ds_index==0:\n        ax.set_title('Decision Tree')\n        ax.text(array1.max()-.3,array2.min()+.3,('{:.1f}%'.format(score*100)),size=15,horizontalalignment='right')\n    i+=1\nplt.tight_layout()\nplt.show()\n","668bcfff":"# Decision tree is not good at circles data,even turnning a lot, it can't be improve much.\n# so, we should better to consider use other way like KNN.","75e130cd":"boston=load_boston()\nregressor=DecisionTreeRegressor(random_state=0)\ncross_val_score(regressor,boston.data,boston.target,cv=10,scoring='neg_mean_squared_error')\n# if setting without scoring, the result will show R^2 , \n# the more value closer to 1 of R^2,the better performance.","f10e370f":"# create random 0-5 value on horizontal axis and then create value on vertical axis\nrng=np.random.RandomState(1)\nx=np.sort(5*rng.rand(80,1),axis=0)\ny=np.sin(x).ravel()\ny[::5]+=3*(0.5 - rng.rand(16))\n\n# np.random.rand() random create value 0-1\n# rand(80) is 1 Dimensionality,which can't be excute on DecisionTreeRegressor,\n# so we need to create 2 Dimensionality,which is rand(80,1)\n","4f8afa84":"# ravel --Dimensionality reduction 2 to 1\nnp.random.random((2,1))","f7ad15eb":"np.random.random((2,1)).ravel()","2281cbc6":"# draw picture\nplt.figure()\nplt.scatter(x,y,s=20,edgecolor='black',c='darkorange',label='data')","d6cc7d3e":"regr_1=DecisionTreeRegressor(max_depth=2)\nregr_2=DecisionTreeRegressor(max_depth=5)\nregr_1.fit(x,y)\nregr_2.fit(x,y)","3857559e":"x_test=np.arange(0.0,5.0,0.01)[:,np.newaxis]# np.newaxis increase Dimensionality from 1 to 2\n# np.arange(start,end,range)","1562651f":"y_1=regr_1.predict(x_test)\ny_2=regr_2.predict(x_test)","784c13f3":"plt.figure()\nplt.scatter(x,y,s=20,edgecolor='black',c='darkorange',label='data')\nplt.plot(x_test,y_1,color='cornflowerblue',label='max_depth=2',linewidth=2)\nplt.plot(x_test,y_2,color='yellowgreen',label='max_depth=5',linewidth=2)\nplt.xlabel('data')\nplt.ylabel('target')\nplt.title('Decision Tree Regression')\nplt.legend()\nplt.show()","6ecc1b66":"# The regression tree model learns a sin curve. \n# If the maximum depth of the tree is setted too high, such as 5 max_depth, \n# the decision tree will learn too many details that cause overfitting.\n# So we need to cut leaf to let the performance on training closer to test.","f338ae7a":"# DecisionTreeRegressor","41145ad0":"# import dataset","56fc8455":"# train the learning model","53b173e0":"### \u25a0Principal\n#### The decision tree uses impurity to find the best node and the best branching method. The lower the impurity, the better the fit of the training.","4c41847a":"# Express the effect of decision tree by different type of data ","5dc70fc7":"### Decision tree is a kind of non-parametric supervised learning. It can summarize decision rules from a series of features and labels and present these rules in a tree structure to solve regression and classification problems.","55d5570a":"# test model","ce0066f2":"# Decision tree classification","55c0adbc":"### \u25a0Model\n\n\u201c\u201d\u201c\nclass sklearn.tree.DecisionTreeClassifier\n(criterion='gini',splitter='best',max_depth=None,\nmin_sample_split=2,min_sample_leaf=1,min_weight_fraction=0.0,\nmax_feature=None,random_state=None,max_leaf_node=None,\nmin_impurity_decrease=0.0,min_impurity_split=None,\ncalss_wight=None,presort=False)\n\u201d\u201c\u201d"}}