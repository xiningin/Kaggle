{"cell_type":{"628c56a2":"code","3b7e5460":"code","9a35080a":"code","b350d65c":"code","8cc2cedd":"code","50eda054":"code","38da49a6":"code","e6dcac42":"code","3d2ba75a":"code","bcb0820a":"markdown","c13735d0":"markdown","d39b4713":"markdown","a5abafef":"markdown"},"source":{"628c56a2":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# System packages \nimport os\nimport sys\n","3b7e5460":"# Set your own project id here\nPROJECT_ID = 'project-x-262017' # This needs to be replaced with your own project name \n\nfrom google.cloud import bigquery # Import bigquery API client library \nbigquery_client = bigquery.Client(project=PROJECT_ID)\n\nfrom google.cloud import storage  # Import storage client library \nstorage_client = storage.Client(project=PROJECT_ID)","9a35080a":"bucket_name = 'tpu-aakash' # select the GCP bucket \nbucket = storage_client.get_bucket(bucket_name)","b350d65c":"%%time\nblob_name = \"DrugVisData_small.csv\" # Select filename & print its meta information\nblob = bucket.get_blob(blob_name)\n\nprint(\"Name: {}\".format(blob.id))\nprint(\"Size: {} bytes\".format(blob.size))\nprint(\"Content type: {}\".format(blob.content_type))\nprint(\"Public URL: {}\".format(blob.public_url))","8cc2cedd":"output_file_name = \"DrugVisData_small.csv\" # select local filename to save file\nblob.download_to_filename(output_file_name)\n\nprint(\"Downloaded blob {} to {}.\".format(blob.name, output_file_name))","50eda054":"# Read the csv file & print out its header\ndrugData =pd.read_csv(output_file_name)\nprint(drugData.shape)\ndrugData.head()","38da49a6":"# This is an additional code block if you want to identify the list of files <- Uncomment to run\n# blobs = bucket.list_blobs()\n\n# print(\"Blobs in {}:\".format(bucket.name))\n#for item in blobs:\n#    print(\"\\t\" + item.name)","e6dcac42":"#This is the public url which is available once you add access to *allUsers* \nurl = \"https:\/\/storage.googleapis.com\/tpu-aakash\/DrugVisData_small.csv\"","3d2ba75a":"%%time\n# csv files can be directly read using pandas *read_csv* \ndrugData = pd.read_csv(url)\nprint(drugData.shape)\ndrugData.head()","bcb0820a":"# 2. Data is publicly available\n\n\nOnce the file is uploaded to a GCP bucket, you can generate a publicly visible download link. Click on the three dots to the right most of the row\n\n1. Select *Edit permissions* \n2. Select *+ Add Item*\n3. Now select *Entity* = *User*\n4. Under *Name* add *allUsers*\n5. Keep *Access* = *Reader*\n6. Save the settings \n\n![Edit permissions](https:\/\/storage.googleapis.com\/tpu-aakash\/Access%20GCP%20bucket\/editPermission.jpg)\n\nA publicly available url will be visible. Copy it below","c13735d0":"This notebook will walk you through accessing your files stored in a GCP bucket\n\n*Note: The best way to access your embeddings or pre-processed data is by uploading it to a kaggle dataset.*\n\nBut if you have an automated data pipelines, it will be easier to pre-process the data on an offline VM or local machine & store it in a GCP bucket. From there you can access your data via the following two methods. \n\n1. Access data via the client storage API \n2. Data is publicly available  - anyone with the link can access the data\n\n+++++++++++++  ***Note***  +++++++++++++\n\nAs per the tasks submission guidelines: To be valid, a submission must be contained in a single notebook made public on or before the submission deadline. Participants are free to use additional datasets in addition to the official Kaggle dataset, but those datasets must also be publicly available on either Kaggle, Allen.ai, or Semantic Scholar in order for the submission to be valid. Participants must also accept the competition rules.\nSource: [Tasks description](https:\/\/www.kaggle.com\/allen-institute-for-ai\/CORD-19-research-challenge\/tasks?taskId=568)\n\n","d39b4713":"# Code for accessing file in GCP bucket\n\nPlease follow the above steps to enable GCP access on your notebook & enabling the relevant APIs. \nNote that Internet access should be enabled from your settings panel. *(default setting)*\n\n![Internet access](https:\/\/storage.googleapis.com\/tpu-aakash\/Access%20GCP%20bucket\/default_internet.jpg)","a5abafef":"# 1. Access data via client storage API\n\nA google account can be added when a new notebook is being created. Select *\"SHOW ADVANCED SETTINGS\"*\n\nSelect *\"Link a Google Account to access Google services\"*\n\n![Selecting gcp services](https:\/\/storage.googleapis.com\/tpu-aakash\/Access%20GCP%20bucket\/Access_GCP_account.jpg)\n\n\nYour google account will be visible. Select *Attach to Notebook*\n\n![Attach to notebook](https:\/\/storage.googleapis.com\/tpu-aakash\/Access%20GCP%20bucket\/Select%20Account.jpg)\n\nOnce the account is attached, you will see the list of GCP APIs that are linked. \n\nBig Query is automatically selected. Its good if you are using data stored as a BigTable. Note that Storage is not automatically authorized. \n\n![Select API](https:\/\/storage.googleapis.com\/tpu-aakash\/Access%20GCP%20bucket\/Select_api.jpg)\n\nClick on *\"Add authorization\"*\n\nYou will have to select your google account & allow kaggle to access the API service. Once the API is authorized you will instantiate the new notebook, with GCP access. The next code block will walk you through accessing a csv file in your GCP bucket\n\n![Authorized API](https:\/\/storage.googleapis.com\/tpu-aakash\/Access%20GCP%20bucket\/GCP_bucket_authorization.jpg)"}}