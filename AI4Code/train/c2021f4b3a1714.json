{"cell_type":{"c0c2f2c1":"code","944a9f65":"code","88b787e4":"code","c12303b8":"code","43260e1f":"code","d47e5c38":"code","81edbc91":"code","154378fa":"code","bec62208":"code","5d341d83":"code","8f9ee595":"code","db17f56e":"code","233bcbc2":"code","35819187":"code","57af382e":"code","bc858c11":"code","e8ea461c":"code","cb1d81b8":"code","9fd7c187":"code","449b12e3":"code","190dddd9":"code","3f608ca4":"code","fc4930e0":"code","60fc8a23":"code","388370b3":"code","f5343a9f":"code","30252528":"code","5f47f7ad":"code","58941b8f":"code","148d6b5f":"code","df8688f9":"code","bbc8086e":"code","141e682b":"code","1116e7af":"code","738c0ad8":"code","50ec8177":"code","6925f30c":"code","5c947071":"code","ea65760c":"code","ac34cc64":"code","9686c8b7":"code","c01c8ba1":"code","6dfdb37e":"code","8866c869":"code","026fa04f":"code","a1049503":"code","fa1b2fec":"code","51115a11":"code","df387272":"code","b250fd48":"code","1a3168f4":"code","761f3e65":"code","8f5a8f3d":"code","01bcff09":"code","defa3a50":"code","0aa3afe3":"markdown","4b4b3d9e":"markdown","ae29022f":"markdown","f15f919b":"markdown","fe96291a":"markdown","136d3d47":"markdown","6b8e7288":"markdown","21dba846":"markdown","5390606d":"markdown","94b290b7":"markdown","47d5e919":"markdown","cc9c3bbc":"markdown","428b5af2":"markdown","27c482ea":"markdown","431bfd56":"markdown","78fc7069":"markdown","6f530f43":"markdown","21b48453":"markdown","ddcb8231":"markdown","77d958b2":"markdown","f484ab57":"markdown","8e75c256":"markdown","af140ca4":"markdown","ac0eed40":"markdown","d6464dad":"markdown","74a18ff6":"markdown","a1429561":"markdown","7e361a4f":"markdown","04a4bf4a":"markdown","98cecd35":"markdown","20b28e2c":"markdown","03311d8d":"markdown","5b07a9ad":"markdown"},"source":{"c0c2f2c1":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nfrom matplotlib import pyplot as plt\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","944a9f65":"df=pd.read_csv('\/kaggle\/input\/satellites-and-debris-in-earths-orbit\/space_decay.csv')\ndf","88b787e4":"df = df[df['RCS_SIZE'].notna()]\ndf","c12303b8":"df.info()","43260e1f":"df.describe()","d47e5c38":"df.columns","81edbc91":"df1=df[[\"LAUNCH_DATE\",\"OBJECT_TYPE\",\"RCS_SIZE\",\"COUNTRY_CODE\",\"APOAPSIS\",\"PERIAPSIS\",\"MEAN_ANOMALY\",\"ECCENTRICITY\",\"MEAN_MOTION\",\"INCLINATION\",\"RA_OF_ASC_NODE\",\"ARG_OF_PERICENTER\",\"PERIOD\",\"SEMIMAJOR_AXIS\"]]\ndf1","154378fa":"df1['CENT_FOCUS_DIST'] = df['SEMIMAJOR_AXIS']*df['ECCENTRICITY']\ndf1","bec62208":"import math","5d341d83":"M=df1[\"MEAN_ANOMALY\"]\ne=df1[\"ECCENTRICITY\"]\nc=df1[\"CENT_FOCUS_DIST\"]\na=df1[\"SEMIMAJOR_AXIS\"]","8f9ee595":"df1[\"CURRENT_YEAR\"]=2021\ndf1","db17f56e":"df1.LAUNCH_DATE=df1.LAUNCH_DATE.dropna()","233bcbc2":"df1[\"OBJECT_AGE\"]=df1.CURRENT_YEAR-df1.LAUNCH_DATE","35819187":"df1","57af382e":"import matplotlib\nfrom matplotlib import pyplot\n%matplotlib inline\npyplot.figure(figsize=(14,6))\ndf2=df1.RCS_SIZE.value_counts()\npyplot.hist(df2)\npyplot.show()","bc858c11":"from scipy.stats import shapiro\nDataToTest=df2\nstat, p = shapiro(DataToTest)\nprint('stat=%.2f, p=%.20f' %(stat,p))\n\nif p>0.05:\n    print('Normal distribution')\nelse:\n    print('Not a Normal distribution')","e8ea461c":"%matplotlib inline\npyplot.figure(figsize=(14,6))\ndf3=df1.COUNTRY_CODE.value_counts()\npyplot.hist(df2)\npyplot.show()","cb1d81b8":"from scipy.stats import shapiro\nDataToTest=df3\nstat, p = shapiro(DataToTest)\nprint('stat=%.2f, p=%.20f' %(stat,p))\n\nif p>0.05:\n    print('Normal distribution')\nelse:\n    print('Not a Normal distribution')","9fd7c187":"from numpy.random import randn\nDataToTest2=randn(100)\nDataToTest2","449b12e3":"from scipy.stats import shapiro\nstat, p = shapiro(DataToTest2)\nprint('stat=%.2f, p=%.20f' %(stat,p))\n\nif p>0.05:\n    print('Normal distribution')\nelse:\n    print('Not a Normal distribution')","190dddd9":"from scipy.stats import normaltest\nDataToTest=df2\nstat, p = shapiro(DataToTest)\nprint('stat=%.2f, p=%.20f' %(stat,p))\n\nif p>0.05:\n    print('Normal distribution')\nelse:\n    print('Not a Normal distribution')","3f608ca4":"df3=df1['OBJECT_TYPE'].value_counts()\nfrom scipy.stats import normaltest\nDataToTest=df3\nstat, p = shapiro(DataToTest)\nprint('stat=%.2f, p=%.20f' %(stat,p))\n\nif p>0.05:\n    print('Normal distribution')\nelse:\n    print('Not a Normal distribution')","fc4930e0":"from matplotlib import pyplot\ntest1=df1[1:20]['RCS_SIZE']\ntest2=df1[1:20]['OBJECT_AGE']\ntest3=df1['COUNTRY_CODE']\npyplot.plot(test1 , test2)\npyplot.show()","60fc8a23":"from scipy.stats import spearmanr\nstat, p = spearmanr(test1, test2)\nprint('stat=%.2f, p=%.5f' %(stat,p))\n\nif p>0.05:\n    print('Independent samples')\nelse:\n    print('dependent samples')","388370b3":"df1.corr(method='pearson')","f5343a9f":"contingency_data=pd.crosstab(df1['RCS_SIZE'],df1['OBJECT_AGE'],margins=False)\ncontingency_data","30252528":"from scipy.stats import chi2_contingency\nstat, p, dof, expected = chi2_contingency(contingency_data)\nprint('stat=%.3f, p=%.3f' %(stat,p))\n\nif p>0.05:\n    print('Independent categories')\nelse:\n    print('dependent categories')","5f47f7ad":"contingency_data2=pd.crosstab(df1['COUNTRY_CODE'],df1['OBJECT_AGE'],margins=False)\ncontingency_data2","58941b8f":"from scipy.stats import chi2_contingency\nstat, p, dof, expected = chi2_contingency(contingency_data)\nprint('stat=%.3f, p=%.3f' %(stat,p))\n\nif p>0.05:\n    print('Independent categories')\nelse:\n    print('dependent categories')","148d6b5f":"DEBRIS_DATA = df1[df1['OBJECT_TYPE'] == 'DEBRIS']\nDEBRIS_DATA","df8688f9":"DEBRIS_DATA.RCS_SIZE=DEBRIS_DATA.RCS_SIZE.map({'SMALL':1, 'MEDIUM':2, 'LARGE':3})\nDEBRIS_DATA","bbc8086e":"DEBRIS_DATA.columns","141e682b":"X = DEBRIS_DATA[[\"OBJECT_AGE\",\"CENT_FOCUS_DIST\",\"APOAPSIS\",\"PERIAPSIS\",\"MEAN_ANOMALY\",\"MEAN_MOTION\",\"INCLINATION\",\"RA_OF_ASC_NODE\",\"ARG_OF_PERICENTER\",\"PERIOD\"]]\ny = DEBRIS_DATA[\"RCS_SIZE\"]","1116e7af":"from sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import mean_squared_error, r2_score\nfrom sklearn.metrics import classification_report, confusion_matrix\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.30, random_state = 49)","738c0ad8":"from statistics import mode\nfrom sklearn.linear_model import LogisticRegression\nmodel = LogisticRegression(max_iter = 500000)\nmodel.fit(X_train, y_train)\ny_pred = model.predict(X_test)\naccuracy = model.score(X_test, y_test)\nprint(accuracy)","50ec8177":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=5)\nfrom sklearn.gaussian_process import GaussianProcessClassifier\nmodel = GaussianProcessClassifier()\nmodel.fit(X_train, y_train)\ny_pred = model.predict(X_test)\naccuracy = model.score(X_test, y_test)\nprint(accuracy)","6925f30c":"print(classification_report(y_test,y_pred))","5c947071":"from sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.svm import SVC\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.25, random_state = 0)\nclassifier = SVC(kernel = 'rbf', random_state = 0)\nclassifier.fit(X_train, y_train)\ny_pred = classifier.predict(X_test)\naccuracies = cross_val_score(estimator = classifier, X = X_train, y = y_train, cv = 10)\naccuracies.mean()","ea65760c":"print(classification_report(y_test,y_pred))","ac34cc64":"from sklearn.naive_bayes import GaussianNB\nfrom sklearn.naive_bayes import BernoulliNB\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import accuracy_score","9686c8b7":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.20, random_state = 0)\nsc_X = StandardScaler()\nX_train = sc_X.fit_transform(X_train)\nX_test = sc_X.transform(X_test)\nclassifier=GaussianNB()\nclassifier.fit(X_train,y_train)\ny_pred=classifier.predict(X_test)\nacc=accuracy_score(y_test, y_pred)\nprint(acc)","c01c8ba1":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.20, random_state = 0)\nsc_X = StandardScaler()\nX_train = sc_X.fit_transform(X_train)\nX_test = sc_X.transform(X_test)\nclassifier=BernoulliNB()\nclassifier.fit(X_train,y_train)\ny_pred=classifier.predict(X_test)\nacc=accuracy_score(y_test, y_pred)\nprint(acc)","6dfdb37e":"from sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.neighbors import KNeighborsRegressor\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.20, random_state = 0)\nsc_X = StandardScaler()\nX_train = sc_X.fit_transform(X_train)\nX_test = sc_X.transform(X_test)\nclassifier=KNeighborsClassifier(n_neighbors=5,metric='minkowski',p=2)\nclassifier.fit(X_train,y_train)\ny_pred=classifier.predict(X_test)\nacc=accuracy_score(y_test, y_pred)\nprint(acc)","8866c869":"print(classification_report(y_test,y_pred))","026fa04f":"from sklearn.linear_model import Perceptron\nfrom sklearn.neighbors import KNeighborsClassifier\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.20, random_state = 0)\nsc_X = StandardScaler()\nX_train = sc_X.fit_transform(X_train)\nX_test = sc_X.transform(X_test)\nclassifier=Perceptron()\nclassifier.fit(X_train,y_train)\ny_pred=classifier.predict(X_test)\nacc=accuracy_score(y_test, y_pred)\nprint(acc)","a1049503":"print(classification_report(y_test,y_pred))","fa1b2fec":"from sklearn.ensemble import RandomForestClassifier\nX_train, X_test, y_train, y_test = train_test_split(X, y)\nmodel = RandomForestClassifier(n_estimators=100, max_depth=10, random_state=1)\nmodel.fit(X_train, y_train)\nmodel.score(X_test, y_test)","51115a11":"from sklearn.tree import DecisionTreeClassifier\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.20, random_state = 0)\nsc_X = StandardScaler()\nX_train = sc_X.fit_transform(X_train)\nX_test = sc_X.transform(X_test)\nclassifier=DecisionTreeClassifier(criterion=\"entropy\",random_state=0)\nclassifier.fit(X_train,y_train)\ny_pred=classifier.predict(X_test)\nacc=accuracy_score(y_test, y_pred)\nprint(acc)","df387272":"from sklearn.ensemble import ExtraTreesClassifier\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.20, random_state = 0)\nsc_X = StandardScaler()\nX_train = sc_X.fit_transform(X_train)\nX_test = sc_X.transform(X_test)\nclassifier=ExtraTreesClassifier(criterion=\"entropy\",random_state=0)\nclassifier.fit(X_train,y_train)\ny_pred=classifier.predict(X_test)\nacc=accuracy_score(y_test, y_pred)\nprint(acc)","b250fd48":"from sklearn.ensemble import AdaBoostClassifier\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.20, random_state = 0)\nsc_X = StandardScaler()\nX_train = sc_X.fit_transform(X_train)\nX_test = sc_X.transform(X_test)\nclassifier=AdaBoostClassifier(random_state=0)\nclassifier.fit(X_train,y_train)\ny_pred=classifier.predict(X_test)\nacc=accuracy_score(y_test, y_pred)\nprint(acc)","1a3168f4":"from sklearn.linear_model import PassiveAggressiveClassifier\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.20, random_state = 0)\nsc_X = StandardScaler()\nX_train = sc_X.fit_transform(X_train)\nX_test = sc_X.transform(X_test)\nclassifier=PassiveAggressiveClassifier(random_state=0)\nclassifier.fit(X_train,y_train)\ny_pred=classifier.predict(X_test)\nacc=accuracy_score(y_test, y_pred)\nprint(acc)","761f3e65":"from sklearn.ensemble import BaggingClassifier\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.20, random_state = 0)\nsc_X = StandardScaler()\nX_train = sc_X.fit_transform(X_train)\nX_test = sc_X.transform(X_test)\nclassifier=BaggingClassifier(random_state=0)\nclassifier.fit(X_train,y_train)\ny_pred=classifier.predict(X_test)\nacc=accuracy_score(y_test, y_pred)\nprint(acc)","8f5a8f3d":"from sklearn.ensemble import GradientBoostingClassifier\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.20, random_state = 0)\nsc_X = StandardScaler()\nX_train = sc_X.fit_transform(X_train)\nX_test = sc_X.transform(X_test)\ngbk = GradientBoostingClassifier()\ngbk.fit(X_train, y_train)\npred = gbk.predict(X_test)\nacc=accuracy_score(y_test, y_pred)\nprint(acc)","01bcff09":"from sklearn.linear_model import RidgeClassifierCV\nrcc = RidgeClassifierCV()\nrcc.fit(X, y)\nr2_score(rcc.predict(X), y)","defa3a50":"from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.20, random_state = 0)\nsc_X = StandardScaler()\nX_train = sc_X.fit_transform(X_train)\nX_test = sc_X.transform(X_test)\nModel=LinearDiscriminantAnalysis()\nModel.fit(X_train,y_train)\ny_pred=Model.predict(X_test)\nprint('accuracy is ',accuracy_score(y_pred,y_test))","0aa3afe3":"# Thank you!","4b4b3d9e":"# Ridge Classifier CV","ae29022f":"**TAKING SAMPLES OF THE FEATURES RCS_SIZE AND OBJECT AGE**","f15f919b":"# Passive Aggressive Classifier","fe96291a":"# Naive Bayes Algorithm","136d3d47":"# DECISION TREE CLASSIFIER","6b8e7288":"# KNN","21dba846":"# HYPOTHESIS TESTING\n\n***(ASSUMPTION THROUGHOUT: IS NORMALLY DISTRIBUTED)***","5390606d":"# Support Vector Machine","94b290b7":"**Columns which might be useful to analyze and create new features: LAUNCH DATE , OBJECT_TYPE , RCS_SIZE , COUNTRY_CODE , APOAPSIS , PERIAPSIS , MEAN_ANOMALY , ECCENTRICITY**","47d5e919":"# Gaussian Process Classifier","cc9c3bbc":"# DEFINING FEATURES AND TARGET VARIABLE","428b5af2":"**CHECKING IF THE FEATURE RCS_SIZE IS NORMALLY DISTRIBUTED**","27c482ea":"# STARTING OFF WITH APPLYING ML ALGORITHMS ON DEBRIS DATA.","431bfd56":"***HENCE, We got to know that the size of the object and it's age are two dependable categories***","78fc7069":"**NOW, LET'S CHECK IF THE FEATURE COUNTRY_CODE IS NORMALLY DISTRIBUTED**","6f530f43":"# Gradient Boosting","21b48453":"# CORRELATION TEST : PEARSON AND SPEARMAN'S RANK CORRELATION","ddcb8231":"**Gaussian NB**","77d958b2":"# K^2 NORMALITY TEST ON THE FEATURES RCS_SIZE and OBJECT_TYPE","f484ab57":"**Bernoulli NB**","8e75c256":"# RANDOM FOREST CLASSIFIER","af140ca4":"***Hence, the object age and the country to which it belongs are also two dependent categories.***","ac0eed40":"# AdaBoost Classifier","d6464dad":"# LDA","74a18ff6":"# CHI2 CATEGORICAL TEST","a1429561":"# Bagging Classifier","7e361a4f":"# EXTRA TREE CLASSIFIER","04a4bf4a":"**ADDING ANOTHER FEATURE: OBJECT_AGE**","98cecd35":"# Perceptron","20b28e2c":"**ADDING A NEW FEATURE : CENTER FOCUS DISTANCE OF PARABOLA which is the multiplication of Eccentricity and the semimajor axis of the orbital object.**","03311d8d":"**SPEARMAN'S RANK CORRELATION**","5b07a9ad":"# LOGISTIC REGRESSION"}}