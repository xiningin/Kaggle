{"cell_type":{"77a63bc3":"code","8893df78":"code","87db3716":"code","ca1a219b":"code","44d8ea64":"code","a2f1f105":"code","ee6b1499":"code","e8e40c2f":"code","75217030":"code","6b3277ef":"code","5497768d":"code","513bba58":"code","43543c22":"code","e3d50800":"code","876ca616":"code","72c2cdc6":"code","7d256ffe":"code","fc05f7a8":"code","3a3ed25d":"code","168b182b":"code","4cb5b05f":"code","83715c30":"code","8dd55797":"code","58b31435":"code","89c83286":"code","1abddf6b":"code","0ae6a1c3":"code","be4862fb":"code","02aef512":"code","ed60801f":"code","4b764c70":"code","7d7dc944":"code","95715e33":"code","c288b61d":"code","4c4edf4c":"code","b56fca5b":"code","15d242b6":"code","828c5ed4":"code","84e3f4c7":"code","0bd76cad":"code","47226e6e":"code","13a0cb0e":"code","61c958de":"code","df6a7c13":"code","a0ddfde9":"code","ae2a6aad":"code","0e22af34":"code","7ee2ba69":"markdown","074ff69c":"markdown","c35acfb7":"markdown","4f18e802":"markdown","1a261712":"markdown","ecf0782d":"markdown","9ad8bab8":"markdown","69f4d9ed":"markdown","eed4a99e":"markdown","1167adc3":"markdown","e368b866":"markdown","ce3e55bb":"markdown","bdcd2d19":"markdown","f9e06821":"markdown","9af6b9f3":"markdown","c895732e":"markdown","66850632":"markdown","3197d0bc":"markdown","d2877cf4":"markdown","ee0a055f":"markdown","b13ab42d":"markdown","b0bcd383":"markdown","e2a16663":"markdown","0e9b7aeb":"markdown","68c87057":"markdown","870ca1f0":"markdown","f840c380":"markdown","04afd430":"markdown","9efb73b4":"markdown","c0b69c54":"markdown","39f3bf86":"markdown","7d678554":"markdown","91671c21":"markdown","0fe71749":"markdown","0ce9865a":"markdown","fe27744b":"markdown","ea224976":"markdown","21c5f4a2":"markdown","9eb26924":"markdown","754b5226":"markdown","6d96bcf4":"markdown","ff4bb02c":"markdown","0ac3b775":"markdown","5173d52c":"markdown","b2bc90c4":"markdown","142d1459":"markdown","04d8a3eb":"markdown","ca39369e":"markdown","780d3dcb":"markdown","51f62382":"markdown","0c9c4d1e":"markdown","1ab50fcb":"markdown","7296db1c":"markdown","7afaa2c7":"markdown","01877d2f":"markdown","5b1a6add":"markdown","aee38bcc":"markdown","3dfea391":"markdown","d91e3a86":"markdown","32ddfa79":"markdown","46868379":"markdown","9cc1bbd8":"markdown","492acff4":"markdown","c35d6ce8":"markdown","f1d2e910":"markdown","6cfa8e8a":"markdown","eba0ad1c":"markdown","03f0e003":"markdown","debd1d90":"markdown","31b117ec":"markdown","de30aa0f":"markdown","23372610":"markdown","85e15c5a":"markdown","aa3b306c":"markdown","6d81b375":"markdown","c9212ff3":"markdown","300ce074":"markdown","c1dda6aa":"markdown","dbe16e0a":"markdown","a44a32a4":"markdown"},"source":{"77a63bc3":"import numpy as np\nimport pandas as pd\nfrom matplotlib import pyplot as plt\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import mean_squared_error, r2_score\ntrain = pd.read_csv(\"..\/input\/random-linear-regression\/train.csv\") \ntest = pd.read_csv(\"..\/input\/random-linear-regression\/test.csv\") \ntrain = train.dropna()\ntest = test.dropna()\ntrain.head()","8893df78":"X_train = np.array(train.iloc[:, :-1].values)\ny_train = np.array(train.iloc[:, 1].values)\nX_test = np.array(test.iloc[:, :-1].values)\ny_test = np.array(test.iloc[:, 1].values)\nmodel = LinearRegression(fit_intercept=True, normalize=True,copy_X=True,n_jobs=-1)\n\n\nmodel.fit(X_train, y_train)\n\ny_pred = model.predict(X_test)\naccuracy = model.score(X_test, y_test)\n\nplt.plot(X_train, model.predict(X_train), color='green')\nplt.show()\nprint(accuracy)","87db3716":"import sklearn\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import r2_score\nfrom statistics import mode\n\n\ntrain = pd.read_csv(\"..\/input\/titanic\/train.csv\")\ntest  = pd.read_csv('..\/input\/titanic\/test.csv')\ntrain.head()","ca1a219b":"ports = pd.get_dummies(train.Embarked , prefix='Embarked')\ntrain = train.join(ports)\ntrain.drop(['Embarked'], axis=1, inplace=True)\ntrain.Sex = train.Sex.map({'male':0, 'female':1})\ny = train.Survived.copy()\nX = train.drop(['Survived'], axis=1) \nX.drop(['Cabin'], axis=1, inplace=True) \nX.drop(['Ticket'], axis=1, inplace=True) \nX.drop(['Name'], axis=1, inplace=True) \nX.drop(['PassengerId'], axis=1, inplace=True)\nX.Age.fillna(X.Age.median(), inplace=True) \n","44d8ea64":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=5)\nfrom sklearn.linear_model import LogisticRegression\n#linear_model.LogisticRegression(penalty='l2\u2019,dual=False,tol=0.0001,C=1.0,fit_intercept=True,intercept_scaling=1,\n#  class_weight=None,random_state=None,solver='warn\u2019,max_iter=100,\n#   multi_class='warn\u2019, verbose=0,warm_start=False, n_jobs=None)\n\n\nmodel = LogisticRegression(max_iter = 500000)\nmodel.fit(X_train, y_train)\ny_pred = model.predict(X_test)\naccuracy = model.score(X_test, y_test)\nprint(accuracy)\n","a2f1f105":"from sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.svm import SVC\ndata_svm = pd.read_csv(\"..\/input\/svm-classification\/UniversalBank.csv\")\ndata_svm.head()","ee6b1499":"X = data_svm.iloc[:,1:13].values\ny = data_svm.iloc[:, -1].values\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.25, random_state = 0)\n\n'''\nsklearn.svm.SVC(C=1.0, kernel=\u2019rbf\u2019, degree=3, gamma=\u2019auto_deprecated\u2019, coef0=0.0, shrinking=True,\n                probability=False, tol=0.001, cache_size=200, class_weight=None,verbose=False,\n                max_iter=-1, decision_function_shape='ovr\u2019, random_state=None)\n'''\n\nclassifier = SVC(kernel = 'rbf', random_state = 0)\nclassifier.fit(X_train, y_train)\ny_pred = classifier.predict(X_test)\naccuracies = cross_val_score(estimator = classifier, X = X_train, y = y_train, cv = 10)\naccuracies.mean()","e8e40c2f":"from sklearn.naive_bayes import GaussianNB\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import accuracy_score\ndata = pd.read_csv('..\/input\/classification-suv-dataset\/Social_Network_Ads.csv')\ndata_nb = data\ndata_nb.head()","75217030":"X = data_nb.iloc[:, [2,3]].values\ny = data_nb.iloc[:, 4].values\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.20, random_state = 0)\nsc_X = StandardScaler()\nX_train = sc_X.fit_transform(X_train)\nX_test = sc_X.transform(X_test)\n\n'''\n#sklearn.naive_bayes.GaussianNB(priors=None, var_smoothing=1e-09)\n'''\n\nclassifier=GaussianNB()\nclassifier.fit(X_train,y_train)\ny_pred=classifier.predict(X_test)\nacc=accuracy_score(y_test, y_pred)\nprint(acc)","6b3277ef":"from sklearn.neighbors import KNeighborsClassifier\nknn = pd.read_csv(\"..\/input\/iris\/Iris.csv\")\nknn.head()","5497768d":"X = knn.iloc[:, [1,2,3,4]].values\ny = knn.iloc[:, 5].values\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.20, random_state = 0)\nsc_X = StandardScaler()\nX_train = sc_X.fit_transform(X_train)\nX_test = sc_X.transform(X_test)\n\n\n'''\nsklearn.neighbors.KNeighborsClassifier(n_neighbors=5, weights='uniform\u2019, algorithm=\u2019auto\u2019, leaf_size=30,\n                                      p=2, metric='minkowski\u2019, metric_params=None,n_jobs=None)\n'''\n\nclassifier=KNeighborsClassifier(n_neighbors=5,metric='minkowski',p=2)\nclassifier.fit(X_train,y_train)\ny_pred=classifier.predict(X_test)\nacc=accuracy_score(y_test, y_pred)\nprint( acc)","513bba58":"from sklearn.linear_model import Perceptron\nfrom sklearn.neighbors import KNeighborsClassifier\np = pd.read_csv(\"..\/input\/iris\/Iris.csv\")\np.head()","43543c22":"X = p.iloc[:, [1,2,3,4]].values\ny = p.iloc[:, 5].values\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.20, random_state = 0)\nsc_X = StandardScaler()\nX_train = sc_X.fit_transform(X_train)\nX_test = sc_X.transform(X_test)\n'''\nclassifier=Perceptron(penalty=None, alpha=0.0001, \ufb01t_intercept=True, max_iter=None, tol=None, shuf\ufb02e=True,\nverbose=0, eta0=1.0, n_jobs=None, random_state=0, early_stopping=False, validation_fraction=0.1,\nn_iter_no_change=5, class_weight=None, warm_start=False, n_iter=None) \n'''\n\nclassifier=Perceptron()\nclassifier.fit(X_train,y_train)\ny_pred=classifier.predict(X_test)\nacc=accuracy_score(y_test, y_pred)\nprint(acc)","e3d50800":"from sklearn.tree import DecisionTreeClassifier\ndt = data\ndt.head()","876ca616":"X = dt.iloc[:, [2,3]].values\ny = dt.iloc[:, 4].values\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.20, random_state = 0)\nsc_X = StandardScaler()\nX_train = sc_X.fit_transform(X_train)\nX_test = sc_X.transform(X_test)\n\n\n'''\nsklearn.tree.DecisionTreeClassifier(criterion='gini\u2019, splitter=\u2019best\u2019, max_depth=None,min_samples_split=2,\n                                    min_samples_leaf=1,min_weight_fraction_leaf=0.0,max_features=None,\n                                    random_state=None, max_leaf_nodes=None,min_impurity_decrease=0.0,\n                                    min_impurity_split=None, class_weight=None,presort=False)\n'''\n\n\nclassifier=DecisionTreeClassifier(criterion=\"entropy\",random_state=0)\nclassifier.fit(X_train,y_train)\ny_pred=classifier.predict(X_test)\nacc=accuracy_score(y_test, y_pred)\nprint(acc)","72c2cdc6":"from sklearn.ensemble import ExtraTreesClassifier\net = data\net.head()","7d256ffe":"X = et.iloc[:, [2,3]].values\ny = et.iloc[:, 4].values\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.20, random_state = 0)\nsc_X = StandardScaler()\nX_train = sc_X.fit_transform(X_train)\nX_test = sc_X.transform(X_test)\n\n\n''' ExtraTreesRegressor(n_estimators=10, max_features=32, random_state=0)'''\n\nclassifier=ExtraTreesClassifier(criterion=\"entropy\",random_state=0)\nclassifier.fit(X_train,y_train)\ny_pred=classifier.predict(X_test)\nacc=accuracy_score(y_test, y_pred)\nprint(acc)","fc05f7a8":"from sklearn.ensemble import RandomForestClassifier\nrf = pd.read_csv(\"..\/input\/mushroom-classification\/mushrooms.csv\")\nrf.head()","3a3ed25d":"X = rf.drop('class', axis=1)\ny = rf['class']\nX = pd.get_dummies(X)\ny = pd.get_dummies(y)\nX_train, X_test, y_train, y_test = train_test_split(X, y)\n\n\n'''\nensemble.RandomForestClassifier(n_estimators='warn\u2019, criterion=\u2019gini\u2019, max_depth=None,\n                                min_samples_split=2, min_samples_leaf=1,min_weight_fraction_leaf=0.0,\n                                max_features='auto\u2019,max_leaf_nodes=None,min_impurity_decrease=0.0,\n                                min_impurity_split=None, bootstrap=True,oob_score=False, n_jobs=None,\n                                random_state=None, verbose=0,warm_start=False, class_weight=None)\n'''\n\n\nmodel = RandomForestClassifier(n_estimators=100, max_depth=10, random_state=1)\nmodel.fit(X_train, y_train)\nmodel.score(X_test, y_test)","168b182b":"from sklearn.ensemble import GradientBoostingClassifier\ngb = data\ngb.head()","4cb5b05f":"X = gb.iloc[:, [2,3]].values\ny = gb.iloc[:, 4].values\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.20, random_state = 0)\nsc_X = StandardScaler()\nX_train = sc_X.fit_transform(X_train)\nX_test = sc_X.transform(X_test)\n\n\n'''\nensemble.GradientBoostingClassifier(loss='deviance\u2019, learning_rate=0.1,n_estimators=100, subsample=1.0,\n                                    criterion='friedman_mse\u2019,min_samples_split=2,min_samples_leaf=1,\n                                    min_weight_fraction_leaf=0.0,max_depth=3,min_impurity_decrease=0.0,\n                                    min_impurity_split=None,init=None, random_state=None,max_features=None,\n                                    verbose=0, max_leaf_nodes=None,warm_start=False, presort='auto\u2019, \n                                    validation_fraction=0.1,n_iter_no_change=None, tol=0.0001)\n'''\n\n\ngbk = GradientBoostingClassifier()\ngbk.fit(X_train, y_train)\npred = gbk.predict(X_test)\nacc=accuracy_score(y_test, y_pred)\nprint(acc)","83715c30":"from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\nlda = data\nlda.head()","8dd55797":"X = gb.iloc[:, [2,3]].values\ny = gb.iloc[:, 4].values\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.20, random_state = 0)\nsc_X = StandardScaler()\nX_train = sc_X.fit_transform(X_train)\nX_test = sc_X.transform(X_test)\nModel=LinearDiscriminantAnalysis()\nModel.fit(X_train,y_train)\ny_pred=Model.predict(X_test)\nprint('accuracy is ',accuracy_score(y_pred,y_test))","58b31435":"from sklearn.cluster import KMeans\nkm = pd.read_csv(\"..\/input\/datosimp\/km.csv\")\nkm.head()","89c83286":"K_clusters = range(1,8)\nkmeans = [KMeans(n_clusters=i) for i in K_clusters]\nY_axis = km[['latitude']]\nX_axis = km[['longitude']]\nscore = [kmeans[i].fit(Y_axis).score(Y_axis) for i in range(len(kmeans))]\nplt.plot(K_clusters, score)\nplt.xlabel('Number of Clusters')\nplt.ylabel('Score')\nplt.show()","1abddf6b":"kmeans = KMeans(n_clusters = 3, init ='k-means++')\nkmeans.fit(km[km.columns[1:3]])\nkm['cluster_label'] = kmeans.fit_predict(km[km.columns[1:3]])\ncenters = kmeans.cluster_centers_\nlabels = kmeans.predict(km[km.columns[1:3]])\nkm.cluster_label.unique()","0ae6a1c3":"km.plot.scatter(x = 'latitude', y = 'longitude', c=labels, s=50, cmap='viridis')\nplt.scatter(centers[:, 0], centers[:, 1], c='black', s=100, alpha=0.5)\n","be4862fb":"from sklearn.model_selection import train_test_split\nfrom tensorflow.keras.utils import to_categorical\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPool2D\nimport tensorflow as tf\ntrain_data = pd.read_csv(\"..\/input\/digit-recognizer\/train.csv\")\ntest_data = pd.read_csv(\"..\/input\/digit-recognizer\/test.csv\")\ntrain_data.head()","02aef512":"X = np.array(train_data.drop(\"label\", axis=1)).astype('float32')\ny = np.array(train_data['label']).astype('float32')\nfor i in range(9):\n    plt.subplot(3,3,i+1)\n    plt.xticks([])\n    plt.yticks([])\n    plt.grid(False)\n    plt.imshow(X[i].reshape(28, 28), cmap=plt.cm.binary)\n    plt.xlabel(y[i])\nplt.show()\n\nX = X \/ 255.0\nX = X.reshape(-1, 28, 28, 1)\ny = to_categorical(y)\nX_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2)\nX_test = np.array(test_data).astype('float32')\nX_test = X_test \/ 255.0\nX_test = X_test.reshape(-1, 28, 28, 1)\nplt.figure(figsize=(10,10))\n","ed60801f":"model = Sequential()\nmodel.add(Conv2D(filters = 32, kernel_size = (5,5),padding = 'Same', \n                 activation ='relu', input_shape = (28,28,1)))\nmodel.add(Conv2D(filters = 32, kernel_size = (5,5),padding = 'Same', \n                 activation ='relu'))\nmodel.add(MaxPool2D(pool_size=(2,2)))\nmodel.add(Dropout(0.25))\nmodel.add(Conv2D(filters = 64, kernel_size = (3,3),padding = 'Same', \n                 activation ='relu'))\nmodel.add(Conv2D(filters = 64, kernel_size = (3,3),padding = 'Same', \n                 activation ='relu'))\nmodel.add(MaxPool2D(pool_size=(2,2), strides=(2,2)))\nmodel.add(Dropout(0.25))\nmodel.add(Flatten())\nmodel.add(Dense(256, activation = \"relu\"))\nmodel.add(Dropout(0.5))\nmodel.add(Dense(10, activation = \"softmax\"))\nmodel.summary()\nfrom tensorflow.keras.utils import plot_model\nplot_model(model, to_file='model1.png')\n","4b764c70":"#increse to epochs to 30 for better accuracy\nmodel.compile(optimizer='adam', loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\nhistory = model.fit(X_train, y_train, epochs=10, batch_size=85, validation_data=(X_val, y_val))","7d7dc944":"accuracy = history.history['accuracy']\nval_accuracy = history.history['val_accuracy']\nepochs = range(len(accuracy))\nplt.plot(epochs, accuracy, 'bo', label='Training accuracy')\nplt.plot(epochs, val_accuracy, 'b', label='Validation accuracy')\nplt.show()\n\nprint(model.evaluate(X_val, y_val))\n","95715e33":"prediction = model.predict_classes(X_test)\nsubmit = pd.DataFrame(prediction,columns=[\"Label\"])\nsubmit[\"ImageId\"] = pd.Series(range(1,(len(prediction)+1)))\nsubmission = submit[[\"ImageId\",\"Label\"]]\nsubmission.to_csv(\"submission.csv\",index=False)\n","c288b61d":"import math\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.preprocessing import MinMaxScaler\nfrom keras.models import Sequential\nfrom keras.layers import Dense, LSTM\nlstm = pd.read_csv(\"..\/input\/nyse\/prices.csv\")\nlstm = lstm[lstm['symbol']==\"NFLX\"]\nlstm['date'] = pd.to_datetime(lstm['date'])\nlstm.set_index('date',inplace=True)\nlstm = lstm.reset_index()\nlstm.head()","4c4edf4c":"data = lstm.filter(['close'])\ndataset = data.values \ntraining_data_len = math.ceil(len(dataset)*.75)  \nscaler = MinMaxScaler(feature_range=(0,1))\nscaled_data = scaler.fit_transform(dataset)\ntrain_data = scaled_data[0:training_data_len, :]\nx_train = []\ny_train = []\nfor i in range(60,len(train_data)):\n    x_train.append(train_data[i-60:i, 0])\n    y_train.append(train_data[i,0])\nx_train,y_train = np.array(x_train), np.array(y_train)\nx_train = np.reshape(x_train,(x_train.shape[0],x_train.shape[1],1))\n","b56fca5b":"model =Sequential()\nmodel.add(LSTM(64,return_sequences=True, input_shape=(x_train.shape[1],1)))\nmodel.add(LSTM(64, return_sequences= False))\nmodel.add(Dense(32))\nmodel.add(Dense(1))\nmodel.summary()\nfrom tensorflow.keras.utils import plot_model \nplot_model(model, to_file='model1.png')\n","15d242b6":"model.compile(optimizer='adam', loss='mean_squared_error')\nmodel.fit(x_train,y_train, batch_size=85, epochs=20)\n","828c5ed4":"test_data= scaled_data[training_data_len-60:, :]\nx_test = []\ny_test = dataset[training_data_len:,:]\nfor i in range(60,len(test_data)):\n    x_test.append(test_data[i-60:i,0])\nx_test = np.array(x_test)\nx_test = np.reshape(x_test, (x_test.shape[0], x_test.shape[1],1))\npredictions = model.predict(x_test)\npredictions = scaler.inverse_transform(predictions)\nrmse = np.sqrt(np.mean(predictions - y_test)**2)\nrmse","84e3f4c7":"from sklearn.datasets import make_blobs\nfrom sklearn import datasets\nclass PCA:\n  def __init__(self, n_components):\n    self.n_components = n_components\n    self.components = None\n    self.mean = None\n\n  def fit(self, X):\n    self.mean = np.mean(X, axis=0)\n    X = X - self.mean\n    cov = np.cov(X.T)\n\n    evalue, evector = np.linalg.eig(cov)\n\n    eigenvectors = evector.T\n    idxs = np.argsort(evalue)[::-1]\n    \n    evalue = evalue[idxs]\n    evector = evector[idxs]\n    self.components = evector[0:self.n_components]\n\n  def transform(self, X):\n    #project data\n    X = X - self.mean\n    return(np.dot(X, self.components.T))\n\ndata = datasets.load_iris()\nX = data.data\ny = data.target\n\npca = PCA(2)\npca.fit(X)\nX_projected = pca.transform(X)\n\n\n\nx1 = X_projected[:,0]\nx2 = X_projected[:,1]\n\nplt.scatter(x1,x2,c=y,edgecolor='none',alpha=0.8,cmap=plt.cm.get_cmap('viridis',3))\nplt.xlabel('Principal Component 1')\nplt.ylabel('Principal Component 2')\nplt.colorbar()\nplt.show()","0bd76cad":"df = pd.read_csv('..\/input\/supermarket\/GroceryStoreDataSet.csv',names=['products'],header=None)\ndata = list(df[\"products\"].apply(lambda x:x.split(',')))\ndata","47226e6e":"from mlxtend.frequent_patterns import apriori\nfrom mlxtend.preprocessing import TransactionEncoder\nte = TransactionEncoder()\nte_data = te.fit(data).transform(data)\ndf = pd.DataFrame(te_data,columns=te.columns_)\ndf1 = apriori(df,min_support=0.01,use_colnames=True)\ndf1.head()","13a0cb0e":"import plotly.offline as py\nimport plotly.express as px\nfrom fbprophet import Prophet\nfrom fbprophet.plot import plot_plotly, add_changepoints_to_plot\n\npred = pd.read_csv(\"..\/input\/coronavirus-2019ncov\/covid-19-all.csv\")\npred = pred.fillna(0)\npredgrp = pred.groupby(\"Date\")[[\"Confirmed\",\"Recovered\",\"Deaths\"]].sum().reset_index()\npred_cnfrm = predgrp.loc[:,[\"Date\",\"Confirmed\"]]\npr_data = pred_cnfrm\npr_data.columns = ['ds','y']\npr_data.head()","61c958de":"m=Prophet()\nm.fit(pr_data)\nfuture=m.make_future_dataframe(periods=15)\nforecast=m.predict(future)\nforecast\n","df6a7c13":"fig = plot_plotly(m, forecast)\npy.iplot(fig) \n\nfig = m.plot(forecast,xlabel='Date',ylabel='Confirmed Count')","a0ddfde9":"import datetime\nfrom statsmodels.tsa.arima_model import ARIMA\nar = pd.read_csv(\"..\/input\/competitive-data-science-predict-future-sales\/sales_train.csv\")\nar.date=ar.date.apply(lambda x:datetime.datetime.strptime(x, '%d.%m.%Y'))\nar=ar.groupby([\"date_block_num\"])[\"item_cnt_day\"].sum()\nar.index=pd.date_range(start = '2013-01-01',end='2015-10-01', freq = 'MS')\nar=ar.reset_index()\nar=ar.loc[:,[\"index\",\"item_cnt_day\"]]\nar.columns = ['confirmed_date','count']\nar.head()","ae2a6aad":"model = ARIMA(ar['count'].values, order=(1, 2, 1))\nfit_model = model.fit(trend='c', full_output=True, disp=True)\nfit_model.summary()\n\n","0e22af34":"fit_model.plot_predict()\nplt.title('Forecast vs Actual')\npd.DataFrame(fit_model.resid).plot()\nforcast = fit_model.forecast(steps=6)\npred_y = forcast[0].tolist()\npred = pd.DataFrame(pred_y)\n","7ee2ba69":"**Libraries and Data**","074ff69c":"# KNN \n**KNN does not learn any model. and stores the entire training data set which it uses as its representation.The output can be calculated as the class with the highest frequency from the K-most similar instances. Each instance in essence votes for their class and the class with the most votes is taken as the prediction \n**\n\n![Example-on-KNN-classifier.png](attachment:Example-on-KNN-classifier.png)\n\n\n**Example: Should the bank give a loan to an individual? Would an individual default on his or her loan? Is that person closer in characteristics to people who defaulted or did not default on their loans? **\n","c35acfb7":"# We can apply machine learning model by following six steps:-\n1. Problem Definition \n2. Analyse Data \n3. Prepare Data \n4. Evaluate Algorithm \n5. Improve Results \n6. Present Results \n","4f18e802":"**Libraries and Data**","1a261712":"# important note\nThis notebook has become very large, and I could not add more information to it, so I will divide it into a number of notebooks.\nSo you can understand the content\nAnd be an excellent reference for you","ecf0782d":"**Libraries and Data**","9ad8bab8":"**Model and Accuracy**","69f4d9ed":"**Libraries and Data**","eed4a99e":"**Model and Accuracy**","1167adc3":"**Library and Data**","e368b866":"**Libraries and Data**","ce3e55bb":"**Compiling Model**","bdcd2d19":"# Support Vector Machine\n![support-vector-machine-classification-step-by-step-2-638.jpg](attachment:support-vector-machine-classification-step-by-step-2-638.jpg)\n\n\n**Support Vector Machines are perhaps one of the most popular and talked about machine learning algorithms.It is primarily a classier method that performs classification tasks by constructing hyperplanes in a multidimensional space that separates cases of different class labels. SVM supports both regression and classification tasks and can handle multiple continuous and categorical variables \n**\n\n**Example: One class is linearly separable from the others like if we only had two features like Height and Hair length of an individual, we\u2019d first plot these two variables in two dimensional space where each point has two co-ordinates **\n","f9e06821":"# Prophet","9af6b9f3":"#  Supervised Machine Learning \n\n**It is a type of learning in which both input and desired output data are provided. Input and output data are labeled for classification to provide a learning basis for future data processing.This algorithm consist of a target \/ outcome variable (or dependent variable) which is to be predicted from a given set of predictors (independent variables). Using these set of variables, we generate a function that map inputs to desired outputs. The training process continues until the model achieves a desired level of accuracy on the training data.   \n**","c895732e":"# Principle Component Analysis","66850632":"**Prediction**","3197d0bc":"![download.png](attachment:download.png)","d2877cf4":"**Libraries and Data**","ee0a055f":"**Preprocessing and Data Split**","b13ab42d":"**Model and Accuracy**","b0bcd383":"**There are three types of machine learning** \n1. Supervised Machine Learning \n2. Unsupervised Machine Learning \n3. Reinforcement Machine Learning ","e2a16663":"**Library and Data**","0e9b7aeb":"# Application of Supervised Machine Learning \n1. Bioinformatics \n2. Quantitative structure \n3. Database marketing \n4. Handwriting recognition \n5. Information retrieval \n6. Learning to rank \n7. Information extraction \n8. Object recognition in computer vision \n9. Optical character recognition \n10. Spam detection \n11. Pattern recognition \n\n","68c87057":"**Model with plots and accuracy**","870ca1f0":"1. Prophet only takes data as a dataframe with a ds (datestamp) and y (value we want to forecast) column. So first, let\u2019s convert the dataframe to the appropriate format.\n1. Create an instance of the Prophet class and then fit our dataframe to it.\n2. Create a dataframe with the dates for which we want a prediction to be made with make_future_dataframe(). Then specify the number of days to forecast using the periods parameter.\n3. Call predict to make a prediction and store it in the forecast dataframe. What\u2019s neat here is that you can inspect the dataframe and see the predictions as well as the lower and upper boundaries of the uncertainty interval.\n","f840c380":"**Model and Accuracy**","04afd430":"**Fitting Model**","9efb73b4":"# If you like this notebook, do hit upvote\n# Thanks\n\n","c0b69c54":"# Unsupervised Machine Learning\n\n**Unsupervised learning is the training of an algorithm using information that is neither classified nor labeled and allowing the algorithm to act on that information without guidance.The main idea behind unsupervised learning is to expose the machines to large volumes of varied data and allow it to learn and infer from the data. However, the machines must first be programmed to learn from data. **\n\n** Unsupervised learning problems can be further grouped into clustering and association problems.  \n**\n1. Clustering: A clustering problem is where you want to discover the inherent groupings in the data, such as grouping customers by purchasing behaviour. \n2. Association: An association rule learning problem is where you want to discover rules that describe large portions of your data, such as people that buy X also tend to buy Y. \n\n\n","39f3bf86":"**Model and Accuracy**","7d678554":"# **Evaluate Algorithms** \n**The evaluation of algorithm consist three following steps:- **\n1. Test Harness  \n2. Explore and select algorithms \n3. Interpret and report results \n\n","91671c21":"**Prediction and Accuracy**","0fe71749":"# Naive Bayes Algorithm \n![1_39U1Ln3tSdFqsfQy6ndxOA.png](attachment:1_39U1Ln3tSdFqsfQy6ndxOA.png)\n\n\n**A naive Bayes classifier is not a single algorithm, but a family of machine learning algorithms which use probability theory to classify data with an assumption of independence between predictors It is easy to build and particularly useful for very large data sets. Along with simplicity, Naive Bayes is known to outperform even highly sophisticated classification methods    \n**\n\n**Example: Emails are given and we have to find the spam emails from that.A spam filter looks at email messages for certain key words and puts them in a spam folder if they match.**\n","0ce9865a":"**Library and  Data**","fe27744b":"# Machine Learning","ea224976":"Hello guys\n# In thisnotebook we will learn machine learning  very quickly and in a very easy way","21c5f4a2":"**Library and Data **","9eb26924":"# Logistic Regression \n**It\u2019s a classification algorithm, that is used where the response variable is categorical. The idea of Logistic Regression is to find a relationship between features and probability of particular outcome.**   \n* odds= p(x)\/(1-p(x)) = probability of event occurrence \/ probability of not event occurrence \n\n**Example- When we have to predict if a student passes or fails in an exam when the number of hours spent studying is given as a feature, the response variable has two values, pass and fail. \n**\n![multinomial-logistic-regression-with-apache-spark-4-638.jpg](attachment:multinomial-logistic-regression-with-apache-spark-4-638.jpg)","754b5226":"# CNN","6d96bcf4":"# Perceptron ","ff4bb02c":"**Model and Accuracy**","0ac3b775":"# Apriori","5173d52c":"# Arima","b2bc90c4":"# Factors help to choose algorithm \n1. Type of algorithm \n2. Parametrization \n3. Memory size \n4. Overfitting tendency \n5. Time of learning \n6. Time of predicting","142d1459":"**Libraries and data**","04d8a3eb":"![https___specials-images_forbesimg_com_dam_imageserve_966248982_960x0.jpg](attachment:https___specials-images_forbesimg_com_dam_imageserve_966248982_960x0.jpg)","ca39369e":"**Model and Forecast**","780d3dcb":"# LSTM ","51f62382":"**Model and Accuracy**","0c9c4d1e":"**Machine Learning is the science of getting computers to learn and act like humans do, and improve their learning over time in autonomous fashion, by feeding them data and information in the form of observations and real-world interactions.\nThere are many algorithm for getting machines to learn, from using basic decision trees to clustering to layers of artificial neural networks depending on what task you\u2019re trying to accomplish and the type and amount of data that you have available.  \n**","1ab50fcb":"**Model**","7296db1c":"**LSTM  blocks are part of a recurrent neural network structure. Recurrent neural networks are made to utilize certain types of artificial memory processes that can help these artificial intelligence programs to more effectively imitate human thought.It is  capable of learning order dependence \nLSTM can be used for machine translation, speech recognition, and more.**","7afaa2c7":"**Library and Data**","01877d2f":"**Library and Data**","5b1a6add":"# Linear Regression \n**It is a basic and commonly used type of predictive analysis. These regression estimates are used to explain the relationship between one dependent variable and one or more independent variables. \nY = a + bX where **\n* Y \u2013 Dependent Variable \n* a \u2013 intercept - Bise\n* X \u2013 Independent variable \n* b \u2013 Slope -Weights\n![SharedScreenshot%D8%B3%D8%A1%D8%A6%D8%A1.jpg](attachment:SharedScreenshot%D8%B3%D8%A1%D8%A6%D8%A1.jpg)\n**Example: University GPA' = (0.675)(High School GPA) + 1.097**","aee38bcc":"**Model**","3dfea391":"**Preprocessing**","d91e3a86":"# **LDA**","32ddfa79":"**Model**","46868379":"**Model and Accuracy**","9cc1bbd8":"# Application of Reinforcement Machine Learning \n1. Resources management in computer clusters \n2. Traffic Light Control \n3. Robotics \n4. Web System Configuration \n5. Personalized Recommendations \n6. Deep Learning \n","492acff4":"# K-Means Algorithm \nK-means clustering is a type of unsupervised learning, which is used when you have unlabeled data and the goal of this algorithm is to find groups in the data \n\n**Steps to use this algorithm:-**\n* 1-Clusters the data into k groups where k is predefined. \n* 2-Select k points at random as cluster centers. \n* 3-Assign objects to their closest cluster center according to the Euclidean distance function. \n* 4-Calculate the centroid or mean of all objects in each cluster. \n\n**Examples: Behavioral segmentation like segment by purchase history or by activities on application, website, or platform Separate valid activity groups from bots  **\n","c35d6ce8":"# Table of Content\n1. Machine Learning and Types\n2. Application of Machine Learning\n3. Steps of Machine Learning\n4. Factors help to choose algorithm\n5. Algorithm\n         Linear Regression\n         Logistic Regression\n         Support Vector Machine\n         Naive Bayes Algorithm\n         KNN\n         Perceptron\n         Random Forest\n         Decision Tree\n         Extra Tree\n         Gradient Boosting\n                 Light GBM\n                 XGBoost\n                 Catboost\n                 Stochastic Gradient Descent\n         Lasso\n         Kernel Ridge Regression\n         Bayesian Ridge\n         Elastic Net Regression\n         LDA\n         K-Means Algorithm\n         CNN\n         LSTM\n         PCA\n         Apriori\n         Prophet\n         ARIMA\n6. Evaluate Algorithms\n                \n   \n","f1d2e910":"# Random Forest \n**Random forest is collection of tress(forest) and it builds multiple decision trees and merges them together to get a more accurate and stable prediction.It can be used for both classification and regression problems.**\n\n![download%20%281%29.png](attachment:download%20%281%29.png)\n\n**Example: Suppose we have a bowl of 100 unique numbers from 0 to 99. We want to select a random sample of numbers from the bowl. If we put the number back in the bowl, it may be selected more than once. \n**","6cfa8e8a":"**It's an important method for dimension reduction.It extracts low dimensional set of features from a high dimensional data set with a motive to capture as much information as possible and to visualise high-dimensional data, it also reduces noise and finally makes other algorithms to work better because we are injecting fewer inputs.**\n* Example: When we have to bring out strong patterns in a data set or to make data easy to explore and visualize","eba0ad1c":"# Reinforcement Machine Learning \n**Reinforcement Learning is a type of Machine Learning which allows machines to automatically determine the ideal behaviour within a specific context, in order to maximize its performance. Simple reward feedback is required for the agent to learn its behaviour; this is known as the reinforcement signal.It differs from standard supervised learning, in that correct input\/output pairs need not be presented, and sub-optimal actions need not be explicitly corrected. Instead the focus is on performance, which involves finding a balance between exploration of uncharted territory and exploitation of current knowledge  \n**\n","03f0e003":"# Many of the code and explanations were quoted by Mr.Vansh Jatana.\nFrom notebook (applied machine learning )\nand this is his notebook https:\/\/www.kaggle.com\/vanshjatana\/applied-machine-learning\/notebook","debd1d90":"**Library and Data**","31b117ec":"# Extra Tree","de30aa0f":"\nProphet is an extremely easy tool for analysts to produce reliable forecasts","23372610":"**Compiling model**","85e15c5a":"**Plotting Clusters**","aa3b306c":"**A classifier with a linear decision boundary, generated by fitting class conditional densities to the data and using Bayes\u2019 rule.The model fits a Gaussian density to each class, assuming that all classes share the same covariance matrix.Itis  used in statistics, pattern recognition, and machine learning to find a linear combination of features that characterizes or separates two or more classes of objects or events. The resulting combination may be used as a linear classifier, or, more commonly, for dimensionality reduction before later classification.**","6d81b375":"![download%20%281%29.jpg](attachment:download%20%281%29.jpg)\n\n** It is single layer neural network and used for classification **","c9212ff3":"# Application of Unsupervised Machine Learning \n1. Human Behaviour Analysis \n2. Social Network Analysis to define groups of friends. \n3. Market Segmentation of companies by location, industry, vertical. \n4. Organizing computing clusters based on similar event patterns and processes. \n","300ce074":"**Checking for number of clusters**","c1dda6aa":"# Decision Tree\n**Decision tree algorithm is classification algorithm under supervised machine learning and it is simple to understand and use in data.The idea of Decision tree is to split the big data(root) into smaller(leaves)**\n\n![decision_tree.png](attachment:decision_tree.png)","dbe16e0a":"**It is a categorisation algorithm attempts to operate on database records, particularly transactional records, or records including certain numbers of fields or items.It is mainly used for sorting large amounts of data. Sorting data often occurs because of association rules. **\n* Example: To analyse data for frequent if\/then patterns and using the criteria support and confidence to identify the most important relationships. ","a44a32a4":"# Gradient Boosting\n\n![1_8T4HEjzHto_V8PrEFLkd9A.png](attachment:1_8T4HEjzHto_V8PrEFLkd9A.png)\n\n**Gradient boosting is an alogithm under supervised machine learning, boosting means converting weak into strong. In this new tree is boosted over the previous tree**"}}