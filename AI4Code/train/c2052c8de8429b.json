{"cell_type":{"a5d95aa7":"code","81a2de93":"code","e2004745":"code","8594da48":"code","b4aadaf6":"code","f649dbce":"code","16247089":"code","7da72f30":"code","73ec0bbc":"code","74498c43":"code","ae03c0ad":"code","24f3301e":"code","a931a491":"code","a4fb32c1":"code","22ff4b59":"code","c893d881":"code","8f1a2249":"markdown","a4c0ae26":"markdown","0e1fbcad":"markdown","fa2de9ae":"markdown","4161e3dc":"markdown","35c13fe9":"markdown","e6c9eea5":"markdown","3b01460c":"markdown","3da02209":"markdown","1684bb20":"markdown","31e1ce57":"markdown","a9b55814":"markdown","86156520":"markdown","0e9956c4":"markdown","0d2e1491":"markdown","848fed38":"markdown","f7d003d8":"markdown","6a44e3e4":"markdown","69bd121f":"markdown","3a7d79ec":"markdown","939295b1":"markdown"},"source":{"a5d95aa7":"! conda install -c conda-forge gdcm -y","81a2de93":"# import numpy as np # linear algebra\n# import pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n# import os\n# import pydicom\n# import glob\n# from tqdm.notebook import tqdm\n# from pydicom.pixel_data_handlers.util import apply_voi_lut\n# import matplotlib.pyplot as plt\n# from skimage import exposure\n# import cv2\n# import warnings\n# from fastai.vision.all import *\n# from fastai.medical.imaging import *\n# warnings.filterwarnings('ignore')\n\nimport os\nimport pydicom\nimport glob\nimport cv2\nimport random\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt \n\n","e2004745":"dataset_path = '..\/input\/siim-covid19-detection\/'\n\nfor path in glob.glob(dataset_path  + '*'):\n    print(path)","8594da48":"train_images = '..\/input\/siim-covid19-detection\/train\/'\n\nn_folders = len(glob.glob(train_images  + '*'))\nn_subfolders = len(glob.glob(train_images  + '*\/*'))\nn_images = len(glob.glob(train_images  + '*\/*\/*.dcm'))\n\nprint(f'There are {n_subfolders} subfolders in {n_folders} folders.')\nprint(f'There are altogether {n_images} images.')","b4aadaf6":"folders = glob.glob(train_images  + '*')\n\nsubfolder_dict = {}\n\nfor folder in folders:\n    n = len(glob.glob(folder + '\/*'))\n    if n in subfolder_dict.keys():\n        subfolder_dict[n] += 1\n    else:\n        subfolder_dict[n] = 1\n        \nfor k, v in subfolder_dict.items():\n    print(f'There is {v} subfolders with {k} images.')\n\n","f649dbce":"subfolders = glob.glob(train_images  + '*\/*')\n\nimage_dict = {}\n\nfor subfolder in subfolders:\n    n = len(glob.glob(subfolder + '\/*'))\n    if (n!=1):\n        print (subfolder.split('\/')[-1])\n        for image in glob.glob(subfolder + '\/*'):\n            print ('-->' + image.split('\/')[-1])\n\n    if n in image_dict.keys():\n        image_dict[n] += 1\n    else:\n        image_dict[n] = 1\n        \nfor k, v in image_dict.items():\n    print(f'There is {v} subfolders with {k} images.')\n","16247089":"train_study_df = pd.read_csv(dataset_path +'\/train_study_level.csv')\n\ntrain_study_df","7da72f30":"study_classes = train_study_df.drop(columns = ['id']).columns.tolist()\nplt.figure(figsize = (10,5))\nplt.bar([1,2,3,4], train_study_df[study_classes].values.sum(axis=0))\nplt.xticks([1,2,3,4],study_classes)\nplt.ylabel('Frequency')\nplt.show()","73ec0bbc":"train_study_df[study_classes].sum(axis = 1).value_counts()","74498c43":"train_image_df = pd.read_csv(dataset_path +'\/train_image_level.csv')\n\ntrain_image_df","ae03c0ad":"train_image_df['split_label'] = train_image_df.label.apply(lambda x: [x.split()[offs:offs+6] for offs in range(0, len(x.split()), 6)])\n\ntrain_image_df['split_label']","24f3301e":"train_image_df['label_len'] = train_image_df['split_label'].apply(lambda x: 0 if (x[0][0] == 'none') else len(x))\ntrain_image_df['label_len'].value_counts()","a931a491":"classes_freq = []\nfor i in range(len(train_image_df)):\n    for j in train_image_df.iloc[i].split_label: classes_freq.append(j[0])\nplt.hist(classes_freq)\nplt.ylabel('Frequency')","a4fb32c1":"def dicom2array(path, voi_lut=True, fix_monochrome=True):\n    dicom = pydicom.read_file(path)\n    # VOI LUT (if available by DICOM device) is used to\n    # transform raw DICOM data to \"human-friendly\" view\n    if voi_lut:\n        data = pydicom.pixel_data_handlers.util.apply_voi_lut(dicom.pixel_array, dicom)\n    else:\n        data = dicom.pixel_array\n    # depending on this value, X-ray may look inverted - fix that:\n    if fix_monochrome and dicom.PhotometricInterpretation == \"MONOCHROME1\":\n        data = np.amax(data) - data\n    data = data - np.min(data)\n    data = data \/ np.max(data)\n    data = (data * 255).astype(np.uint8)\n    return data\n        \n    \ndef plot_img(img, size=(7, 7), is_rgb=True, title=\"\", cmap='gray'):\n    plt.figure(figsize=size)\n    plt.imshow(img, cmap=cmap)\n    plt.suptitle(title)\n    plt.show()\n    \ndef plot_imgs(imgs, cols=4, size=7, is_rgb=True, title=\"\", cmap='gray', img_size=(500,500)):\n    rows = len(imgs)\/\/cols + 1\n    fig = plt.figure(figsize=(cols*size, rows*size))\n    for i, img in enumerate(imgs):\n        if img_size is not None:\n            img = cv2.resize(img, img_size)\n        fig.add_subplot(rows, cols, i+1)\n        plt.imshow(img, cmap=cmap)\n    plt.suptitle(title)\n    plt.show()\n","22ff4b59":"imgs = []\nimage_ids = train_image_df['id'].values\n\n# map label_id to specify color\nthickness = 10\nscale = 5\n\n\nfor i in range(8):\n    image_id = random.choice(image_ids)\n    image_path = glob.glob(f\"..\/input\/siim-covid19-detection\/train\/*\/*\/{image_id.split('_')[0]}.dcm\")[0]\n    print(image_path)\n    img = dicom2array(path=image_path)\n    img = cv2.resize(img, None, fx=1\/scale, fy=1\/scale)\n    img = np.stack([img, img, img], axis=-1)\n    print(train_image_df.loc[train_image_df['id'] == image_id])\n#     for i in train_image_df.loc[train_image_df['id'] == image_id].split_label.values[0]:\n#         if i[0] == 'opacity':\n#             img = cv2.rectangle(img,\n#                                 (int(float(i[2])\/scale), int(float(i[3])\/scale)),\n#                                 (int(float(i[4])\/scale), int(float(i[5])\/scale)),\n#                                 [255,0,0], thickness)\n    \n    img = cv2.resize(img, (500,500))\n    imgs.append(img)\n    \nplot_imgs(imgs, cmap=None)","c893d881":"bbox_areas = []\nfor i in range(len(train_image_df)):\n    for j in train_image_df.iloc[i].split_label:\n        bbox_areas.append((float(j[4])-float(j[2]))*(float(j[5])*float(j[3])))\nplt.hist(bbox_areas)\nplt.ylabel('Frequency')","8f1a2249":"This seems okay. Because, naturally number of `Typycal Appearance` will be highest, because that is the general case. Then there should be number of `Negative for Pneumonia` , which would be less than the normal case, as those X-rays are for pataints, not random people.There will be a moderate number of cases that the doctors will fail to identify. Those are `Indeteminate Appearence`. And the lowest number willbe of wired `Atypical Cases`.\n\n<span style='color:blue;font-size:16px;font-weight:500;'>Lets check if any row has multiples labels<\/span>","a4c0ae26":"Firstly, Bboxex are goven here. That means it has something to do with localizing some portion of image. <b>This is an `Objet Detection Task`<\/b><br>\nDo you recognize the row number?? <br>\nYap! That is the total image number.\n\n<b>Thus `Image Level` means nothing but prediction for each image.<\/b>\n\nWe have our bounding box labels provided in the `label` column. The format is as follows:\n\n`[class ID] [confidence score] [bounding box]`\n\n* class ID - either `opacity` or `none`\n* confidence score - confidence from your neural network model. If none, the confidence is `1`.\n* bounding box - typical `xmin ymin xmax ymax` format. If class ID is none, the bounding box is `1 0 0 1 1`.\n\nThe bounding boxes are also provided in easily readable dictionary format in column `boxes`, and the study that each image is a part of is provided in`StudyInstanceUID`.\n\n<span style='color:blue;font-size:16px;font-weight:500;'>Now we will create a new column, splitting the label to more understandable list format.<\/span>","0e1fbcad":"<span style='color:red;font-size:18px;font-weight:500;'>train_image_level.csv :<\/span>","fa2de9ae":"So, It looks like , there is 2040 images with no detection. 3113 images with 2 detections (understadibly in two lobes of lung).\n\n<span style='color:blue;font-size:16px;font-weight:500;'>Let's quick look at the distribution of opacity vs none:<\/span>","4161e3dc":"That's it for now!\n\n**Please upvote if you found this helpful!**","35c13fe9":"<span style='color:blue;font-size:15px;font-weight:500;'>Some kind Kagglers have converted the dataset to JPG or PNG format. Though I will show in this notebook how to convert from `.dcm` to .`.jpg'\/'.png`, but the converted datasets will also be linked. <\/span> ","e6c9eea5":"`So , some folders have more than one subfolders and some subfolders have more than one image.`","3b01460c":"<span style='color:crimson;font-size:18px;font-weight:500;'> Major thing to remember is: <\/span>\n`There are 6331 subfolders in 6054 folders.\nThere are altogether 6334 images. Lets keep those numbers around.`","3da02209":"\n<span style='color:blue;font-size:18px;font-weight:500;'>DICOM : <\/span> `It is the standard format for the communication and management of medical imaging information and related data.`\n\n","1684bb20":"<span style='color:blue;font-size:18px;font-weight:500;'> We can see that we have:<\/span>\n\n* `train_study_level.csv` - the train study-level metadata, with one row for each study, including correct labels.\n* `train_image_level.csv` - the train image-level metadata, with one row for each image, including both correct labels and any bounding boxes in a dictionary format. Some images in both test and train have multiple bounding boxes.\n* `sample_submission.csv` - a sample submission file containing all image- and study-level IDs.\n* `train` folder - comprises 6,334 chest scans in DICOM format, stored in paths with the form `study`\/`series`\/`image`\n* `test` folder - The hidden test dataset is of roughly the same scale as the training dataset.\n","31e1ce57":"<span style='color:green;font-size:20px;font-weight:500;'>This Notebook is forked from anoder EDA notebook. But in this one , I am going to explain all the fact about this competition that confused me from beginning. <\/span>","a9b55814":"We can see that each row has sum 1 after summing them along the row. So, `each of the 6054 studies have only one label each.`","86156520":"<span style='color:crimson;font-size:18px;font-weight:500;'> Train folder analysis: <\/span>\n\n\n`We see that there are some folders in train folder. Each of them have atleast one subfolder in them, and each of the subfolders have at least one dcm file, ","0e9956c4":"<span style='color:green;font-size:18px;font-weight:500;'>What is Study Level? What is Image Level? <\/span> <br><br> `Wait for a sec...We will get the answer  just after analyzing the CSV files.`\n\n","0d2e1491":"<span style='color:crimson;font-size:20px;font-weight:500;'><b> First thing before Getting Started : <\/b> The data might look huge, but there is only a small number of images. We will determine the exact number of images in later part of the notebook. The huge size is mainly due to `DICOM` format. If we extract only the images in PNG format, even in high quality the dataset comes down to 3-4 GB only.  <\/span> ","848fed38":"So we have `6054` rows in this. All the rows are classified in following classes:\n* `Negative for Pneumonia`\n* `Typical Appearance`\n* `Intermediate Appearance`\n* `Atypical Appearance`\n\n <b>So the `Study Level`  looks something like a classification task<\/b>. We will check later, if this is single label classification or multilabel. \n\nDo you remeber those numbers ?? <br> \nYes, The `train_study_level.csv` targets these folder number.<b> Each Folder refers to a study.<\/b>\n\n<span style='color:blue;font-size:16px;font-weight:500;'>Now, Checking frequency of classes...<\/span>\n","f7d003d8":"<span style='color:red;font-size:18px;font-weight:500;'>train_study_level.csv :<\/span>","6a44e3e4":"Okay, If the label is None, there is inly one detection. But if not `None` , there might be a number of detections.\n\n<span style='color:blue;font-size:16px;font-weight:500;'>Now I will create one more column to estimate how many bounding boxes are there for each image.<\/span>\n","69bd121f":"<h1 style='color:green;'>A look at the provided data <\/h1>","3a7d79ec":"<span style='color:crimson;font-size:20px;font-weight:500;'><b> If you like the EDA approach or the notebook, a lot more thing will be added. Thank you. <\/b>  <\/span> ","939295b1":"<h1 style='color:red;font-weight:500;'>SIIM-FISABIO-RSNA COVID-19 Detection: An Extended EDA <\/h1>\n\nIn this competition, we are provided with <span style='color:blue;font-weight:500;'>DICOM images <\/span> of chest X-ray radiographs, and we are asked to identify and localize COVID-19 abnormalities. This is important because typical diagnosis of COVID-19 requires molecular testing (polymerase chain reaction) requires several hours, while chest radiographs can be obtained in minutes, but it is hard to distinguish between COVID-19 pneumonia and other other viral and bacterial pneumonias. Therefore, in this competition, be hope to develop AI that that eventually help radiologists diagnose the millions of COVID-19 patients more confidently and quickly.\n\nI'll provide a quick and simple EDA to help you get started with this very interesting competition!\n"}}