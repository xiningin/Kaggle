{"cell_type":{"cfe7521b":"code","e456bdc0":"code","25bc60b1":"code","dc686290":"code","0eeb7066":"code","d88851a2":"code","b4c5ca4c":"code","f772d1b5":"code","d47d0425":"code","d4cee053":"code","7be76d0c":"code","9008cc1a":"code","7f9d93f0":"code","2ce1b3a7":"code","0399857d":"code","b9f76708":"code","a3ec176c":"code","d4c0a790":"code","0b8ea91e":"code","e151a932":"code","85e557bb":"code","9f197650":"code","126ea749":"code","7b9ecbc0":"code","5def0ae5":"code","2813c9ae":"code","dddf0a53":"code","317393a0":"code","b7f1499e":"code","fe4bb5fd":"code","62878290":"code","0707390c":"code","ad0e65af":"code","f50bac73":"code","ba5cafcd":"code","2a00b594":"code","eceae3ef":"code","9a500b3d":"code","7237ff1e":"code","be825d58":"code","fabff406":"code","4bf488ff":"code","f32a8acf":"code","baebdd11":"code","cc51920e":"markdown","a0a2674c":"markdown","ab4e0d8f":"markdown","f58c05f5":"markdown","e6896ae9":"markdown","29271864":"markdown","b730f548":"markdown","f1d70209":"markdown","0de2c092":"markdown"},"source":{"cfe7521b":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nimport seaborn as sns","e456bdc0":"import tensorflow as tf\ntf.__version__","25bc60b1":"data = pd.read_csv('..\/input\/tabular-playground-series-jan-2022\/train.csv')","dc686290":"data.info()","0eeb7066":"data.head()","d88851a2":"data.drop('row_id', axis = 1, inplace=True)","b4c5ca4c":"data.isna().sum()","f772d1b5":"country = data['country'].unique()\nprint('Unique value of country column: ',country)","d47d0425":"# figure(figsize=(8, 6), dpi=80)\ndata.value_counts(data['country']).plot.bar()\nplt.title('Data Distribution')\nplt.xlabel('Country')\nplt.ylabel('counts')\n\nplt.show()","d4cee053":"store = data['store'].unique()\nprint('Unique value of store column: ', store)\n\n\ndata.value_counts(data['store']).plot.bar()\nplt.title('Data Distribution')\nplt.xlabel('Store')\nplt.ylabel('counts')\n\nplt.show()","7be76d0c":"product = data['product'].unique()\nprint('Unique value of product column: ', product)\n\ndata.value_counts(data['product']).plot.bar()\nplt.title('Data Distribution')\nplt.xlabel('Product')\nplt.ylabel('counts')\n\nplt.show()","9008cc1a":"data['date'] = pd.to_datetime(data['date'])\n\ndata.info()","7f9d93f0":"# Create new columns\ndata['day'] = data['date'].dt.day\ndata['month'] = data['date'].dt.month\ndata['year'] = data['date'].dt.year\n\ndata.info()","2ce1b3a7":"data.head()","0399857d":"data.drop(['date'], axis = 1, inplace = True)\ndata.head()","b9f76708":"from sklearn.preprocessing import LabelEncoder\n\nle_cols = ['country', 'store', 'product']\n\ndef convert2num(X_new):\n\n    label_encoder = LabelEncoder()\n\n    for col in le_cols:\n        X_new[col] = label_encoder.fit_transform(X_new[col])\n        \n    return X_new","a3ec176c":"data = convert2num(data)\n\ndata[le_cols].head()","d4c0a790":"data.head()","0b8ea91e":"data['year'] = data['year']\/ 2022","e151a932":"data.drop(['day'], axis = 1, inplace = True)\nX = data.drop('num_sold', axis=1).to_numpy()\ny = data['num_sold'].to_numpy()\n\nX.shape, y.shape","85e557bb":"X[:5]","9f197650":"from sklearn.model_selection import train_test_split\n\ntf.random.set_seed(42)\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\nX_train.shape, X_test.shape, y_train.shape, y_test.shape","126ea749":"# let's build a model to find patterns in it\n\n# Set random seed\ntf.random.set_seed(42)\n\n# 1. Create a model\nmodel_1 = tf.keras.Sequential([\n           tf.keras.layers.Dense(500, activation='relu'),\n           tf.keras.layers.Dense(250, activation='relu'),\n           tf.keras.layers.Dense(100, activation='relu'),\n           tf.keras.layers.Dense(10, activation='relu'), \n           tf.keras.layers.Dense(1)\n])\n\n# 2. Comile the model\nmodel_1.compile(loss=tf.keras.losses.mae,\n                 optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n                 metrics=['MAE'])\n\n# 3. Fit the model\nhistory = model_1.fit(X_train, \n                      y_train, \n                      epochs=50,\n                      verbose = 1,\n                      validation_data=(X_test, y_test))","7b9ecbc0":"pd.DataFrame(history.history).plot()\nplt.ylabel(\"loss\")\nplt.xlabel(\"epochs\")","5def0ae5":"y_p = model_1.predict(X_test)","2813c9ae":"y_p[:5], y_test[:5]","dddf0a53":"from sklearn.metrics import r2_score\nr2_score(y_test, y_p)","317393a0":"model_1.summary()","b7f1499e":"# Let's check out a way of viewing our deep learning models\nfrom tensorflow.keras.utils import plot_model\n\n# See the inputs and outputs of each layer\nplot_model(model_1, show_shapes=True)","fe4bb5fd":"df_test =  pd.read_csv('..\/input\/tabular-playground-series-jan-2022\/test.csv')","62878290":"test_passengerIds = df_test['row_id'].values\ndf_test.drop('row_id', axis = 1, inplace=True)\ndf_test.head()","0707390c":"df_test['date'] = pd.to_datetime(df_test['date'])\n\ndf_test.info()","ad0e65af":"# Create new columns\ndf_test['day'] = df_test['date'].dt.day\ndf_test['month'] = df_test['date'].dt.month\ndf_test['year'] = df_test['date'].dt.year\n\ndf_test.info()","f50bac73":"df_test.drop(['date'], axis = 1, inplace = True)\ndf_test.head()","ba5cafcd":"df_test = convert2num(df_test)\n\ndf_test[le_cols].head()","2a00b594":"df_test['year'] = df_test['year']\/ 2022\ndf_test.head()","eceae3ef":"df_test.drop(['day'], axis = 1, inplace = True)\ndf_test = df_test.to_numpy()\ndf_test[:5]","9a500b3d":"y_pred = model_1.predict(df_test)[:, 0]","7237ff1e":"y_pred[:5]","be825d58":"y_pred = np.array(y_pred)","fabff406":"y_pred.shape","4bf488ff":"test_passengerIds.shape","f32a8acf":"output = pd.DataFrame({'row_id':test_passengerIds, 'num_sold': y_pred})\noutput.to_csv('submission.csv', index=False)","baebdd11":"output","cc51920e":"# Splitting traning set","a0a2674c":"# Check if there is null values","ab4e0d8f":"# Import Packages\nLets load all the needed packages for this notebook:","f58c05f5":"# Split Data","e6896ae9":"# Label Encoding","29271864":"# Test Data","b730f548":"# The Dataset\nFor this notebook we will use Tabular Playground Series - Mar 2021.\n\nLet's define the path to the dataset:","f1d70209":"# Building and Training our model","0de2c092":"# Quick Look at the Data\nLet\u2019s take a look at the top five rows:"}}