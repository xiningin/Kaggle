{"cell_type":{"e6dcdb83":"code","964411e4":"code","7cfd362f":"code","f87f2f7e":"code","549bad1f":"code","ad019728":"code","eb4d8068":"code","3cca95be":"code","764f4816":"code","0d1bc233":"code","bc43548c":"code","34d6d091":"code","6203a3f6":"code","7b80b853":"code","cf18a954":"code","02691fee":"code","bad85c94":"code","39e28b85":"code","6804b92a":"code","f5072c83":"code","1a8fa252":"code","ca827c3d":"code","10c008e9":"code","062ade10":"code","a0248049":"code","2c3cced2":"code","4ed79677":"code","0eb8e484":"code","38a2f286":"code","4365a8aa":"code","6697a53b":"code","6d449434":"code","1ef2dc10":"code","e4e006e5":"code","5082f047":"code","1223e563":"code","777122f5":"code","3a27beab":"code","f55a4e99":"code","f5409d4b":"code","57fb9b0f":"code","176a2813":"code","467c8b37":"code","b1c2c773":"code","46986d56":"code","7feac253":"code","4e2f3bbf":"code","8cd21647":"code","d552481c":"code","3725872c":"code","6020b60d":"code","de0326c4":"code","15335905":"code","0899e92e":"code","13c394cc":"code","6abe1c63":"code","56191d12":"code","554177e2":"code","a65abc80":"code","85e81fac":"code","9b5fe4c3":"code","b56accaa":"code","9f79b22e":"code","a7acb0d4":"code","ec9333a6":"code","38e76cc6":"code","ea3cadd8":"code","ea58006f":"code","7f05c7ca":"code","9164bb0b":"code","a920024d":"code","0c1f7f59":"code","466dbd17":"code","edcf03ea":"code","f189932d":"code","d97a1316":"code","d60f5c19":"code","376e3515":"code","ef33902f":"code","65d5a740":"code","14e1ee84":"code","ba4bac06":"code","9657b5e3":"code","b63be57e":"code","f4911e5b":"code","f32b36ab":"code","d059ff37":"code","fefa360d":"markdown","36ed6907":"markdown","c0709dfb":"markdown","b68f17bf":"markdown","749367af":"markdown","4a9cf7fe":"markdown","f088e7c0":"markdown","f864edfb":"markdown","56809971":"markdown","006b7282":"markdown","7b3fe251":"markdown","54bc3d29":"markdown","919337f9":"markdown","d2449128":"markdown","1c6566ec":"markdown","015a970c":"markdown","dc854640":"markdown","a64623af":"markdown","a528e93f":"markdown","04945c31":"markdown","0c7b6c40":"markdown","f2f4dc93":"markdown","f8ac9fd7":"markdown","2865762c":"markdown","2f96f51b":"markdown","ef96228b":"markdown","4b78bc22":"markdown","3fa0a19f":"markdown","e895234a":"markdown","dbfd2f9c":"markdown","8a5233f7":"markdown","9414dda4":"markdown","375dec52":"markdown","26f47539":"markdown","1018f0a9":"markdown","a54449e6":"markdown","99f3d8e9":"markdown","ceb120ea":"markdown","83b1b505":"markdown","30f80352":"markdown","60287990":"markdown","56e801da":"markdown","1fdd97bd":"markdown","e5130ec3":"markdown","cffca772":"markdown","7a93c7e9":"markdown","fde3c1ac":"markdown","adb5e5fc":"markdown","9b5e5b2e":"markdown","c3029d64":"markdown","f837ac3d":"markdown","61a783dc":"markdown","76447bc0":"markdown","ec303a2b":"markdown","9150aa46":"markdown","9c7dc74a":"markdown","328379d6":"markdown","8e88acb6":"markdown","6e005329":"markdown","0ee95eb2":"markdown","abea4db6":"markdown","a927b4db":"markdown","28c3f022":"markdown","f90dc00d":"markdown","53ba1912":"markdown","06236db6":"markdown","929ddb85":"markdown","865d4c02":"markdown","f8759c19":"markdown","00615958":"markdown","c444340e":"markdown","98d36f8d":"markdown","e1aa6fdf":"markdown","0fc236b2":"markdown","d84ba8d2":"markdown","21b4f67e":"markdown","114a40c3":"markdown","eeafa94e":"markdown","fc68c901":"markdown","3780bd84":"markdown","93cf744b":"markdown","565d084a":"markdown","bcbb1f8c":"markdown","ac8afe32":"markdown","94031119":"markdown"},"source":{"e6dcdb83":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nplt.style.use('seaborn-colorblind')\nimport seaborn as sns\nimport warnings\nwarnings.filterwarnings('ignore')\n\nfrom sklearn.preprocessing import LabelEncoder","964411e4":"train = pd.read_csv(\"..\/input\/titanic\/train.csv\")\ntrain.head(2)\nprint(train.isna().sum())","7cfd362f":"fig, ax = plt.subplots(1,2, figsize=(18,8))\n\ntrain['Survived'].value_counts().plot.pie(autopct='%1.1f%%', ax=ax[0])\nsns.countplot('Survived', data=train, ax=ax[1])\n\nax[0].set_title(\"Survived\")\nax[0].set_ylabel(\"\")\n\nax[1].set_title(\"Survived\")\nplt.show()\nplt.cla()\nplt.close()","f87f2f7e":"pd.crosstab(train['Pclass'], train['Survived'], margins=True).style.background_gradient(cmap='summer_r')","549bad1f":"print(train.groupby(['Pclass'])['Survived'].mean())","ad019728":"fig, ax = plt.subplots(1,2, figsize=(18,8))\n\ntrain['Pclass'].value_counts().plot(kind='bar', ax=ax[1])\nax[1].set_title(\"Pclass count\")\n\nlabels = train['Pclass'].value_counts().index\nax[1].set_xticklabels(labels, rotation=0)\n\nsns.countplot('Pclass', hue='Survived', data=train, ax=ax[0])\nax[0].set_title(\"Pclass Survivability\")\nplt.show()\nplt.cla()\nplt.close()","eb4d8068":"pd.crosstab(train['Sex'],train['Survived'], margins=True).style.background_gradient(cmap='summer_r')","3cca95be":"print(train.groupby(['Sex'])['Survived'].mean())","764f4816":"fig, ax = plt.subplots(1,2, figsize=(18,8))\n\ntrain[['Sex','Survived']].groupby('Sex').mean().plot.bar(ax=ax[1])\nsns.countplot('Sex', hue='Survived', data=train, ax=ax[0])\n\nlabels = train[['Sex','Survived']].groupby('Sex').mean().index\nax[1].set_xticklabels(labels, rotation=0)\n\nplt.show()\nplt.cla()\nplt.close()","0d1bc233":"pd.crosstab([train['Sex'],train['Survived']],train['Pclass'], margins=True).style.background_gradient(cmap='summer_r')","bc43548c":"sns.factorplot('Pclass','Survived', col='Sex', data=train)\nplt.show()\nplt.cla()\nplt.close()","34d6d091":"train['Age'].plot.hist()\nprint(\"The total of missing value:\", train['Age'].isna().sum())\nplt.show()\nplt.cla()\nplt.close()","6203a3f6":"def get_title(data):\n    for t in data['Name']:\n        data['Title'] = data['Name'].str.extract(\"([A-Za-z]+)\\.\")\nget_title(train)","7b80b853":"pd.crosstab(train['Sex'], train['Title'])","cf18a954":"age_nan = train['Age'].isna()\n\nnan_title = train[age_nan]['Title'].unique()\ntitle_age = train[train['Title'].isin(nan_title)].groupby('Title')['Age'].mean().reset_index()\ntitle_age","02691fee":"train.loc[age_nan & (train['Title']=='Dr'), 'Age'] = 42\ntrain.loc[age_nan & (train['Title']=='Master'), 'Age'] = 5\ntrain.loc[age_nan & (train['Title']=='Miss'), 'Age'] = 22\ntrain.loc[age_nan & (train['Title']=='Mr'), 'Age'] = 33\ntrain.loc[age_nan & (train['Title']=='Mrs'), 'Age'] = 36","bad85c94":"train['Age'].plot.hist()\nplt.xlabel(\"Age\")\nprint(\"The total of missing value:\", train['Age'].isna().sum())\nplt.show()\nplt.cla()\nplt.close()","39e28b85":"fig, ax = plt.subplots(1,2, figsize=(18,8))\n\nsns.violinplot('Pclass', 'Age', hue='Survived', data=train, split=True, ax=ax[0])\nsns.violinplot('Sex','Age', hue=\"Survived\", data=train, split=True, ax=ax[1])\n\nax[0].set_title(\"Pclass and Age vs Survived\")\nax[1].set_title(\"Sex and Age vs Survived\")\n\nplt.show()\n\nprint(\"The oldest age that survived:\", train[train['Survived']==1]['Age'].max())\nprint(\"Average age that survived:\", round(train[train['Survived']==1]['Age'].mean(),2))\nprint(\"The youngest age that survived:\", train[train['Survived']==1]['Age'].min())\nplt.show()\nplt.cla()\nplt.close()","6804b92a":"def convert_title(data):\n    group = data.groupby(['Title','Sex'])['Age'].count().reset_index()\n    count = group['Age'] < 10\n    female = group['Sex'] == 'female'\n\n    replace_f = group[count & female]['Title'].unique()[:-1]\n    replace_m = group[count & ~female]['Title'].unique()\n    with_f = group[~count & female]['Title'].unique()\n    with_m = group[~count & ~female]['Title'].unique()[1]\n\n    data.loc[data['Title']==\"Ms\", 'Title'] = with_f[0]\n    data.loc[data['Title'].isin(replace_f), 'Title'] = with_f[1]\n    data.loc[data['Title'].isin(replace_m), 'Title'] = with_m\n\nconvert_title(train)","f5072c83":"sns.factorplot('Pclass', 'Survived', col='Title', data=train)\nplt.show()\nplt.cla()\nplt.close()","1a8fa252":"pd.crosstab(train['SibSp'], train['Survived'], margins=True).style.background_gradient(cmap='summer_r')","ca827c3d":"sibsp_survived = train.groupby('SibSp')['Survived'].mean()\n\nlabels = sibsp_survived.index\nfig, ax = plt.subplots(1,2, figsize=(18,8))\n\nsibsp_survived.plot.bar(ax=ax[1])\nsns.countplot('SibSp', hue='Survived', data=train, ax=ax[0])\n\nax[1].set_xticklabels(labels, rotation=0)\nplt.show()\nplt.cla()\nplt.close()","10c008e9":"pd.crosstab([train['SibSp'],train['Survived']], train['Pclass'], margins=True).style.background_gradient(cmap='summer_r')","062ade10":"sns.factorplot('SibSp', 'Survived', col='Pclass', data=train)\nplt.show()\nplt.cla()\nplt.close()","a0248049":"pd.crosstab(train['Parch'],train['Survived'],margins=True).style.background_gradient(cmap='summer_r')","2c3cced2":"parch_survived = train.groupby('Parch')['Survived'].mean()\n\nfig, ax = plt.subplots(1,2, figsize=(18,8))\n\nparch_survived.plot.bar(ax=ax[1])\nsns.countplot('Parch', hue='Survived', data=train, ax=ax[0])\n\nax[1].set_xticklabels(parch_survived.index, rotation=0)\nax[0].legend(title=\"Survived\", loc='upper right')\nplt.show()\nplt.cla()\nplt.close()","4ed79677":"pd.crosstab([train['Parch'], train['Survived']],train['Pclass'], margins=True).style.background_gradient(cmap='summer_r')","0eb8e484":"sns.factorplot('Parch', 'Survived', col='Pclass', data=train)\nplt.show()\nplt.cla()\nplt.close()","38a2f286":"train['Ticket'].describe()","4365a8aa":"train.loc[train['Ticket'].duplicated(keep=False), 'shared_ticket'] = 1\ntrain['shared_ticket'].fillna(0, inplace=True)\ntrain['shared_ticket'] = train['shared_ticket'].astype(int)","6697a53b":"pd.crosstab([train['shared_ticket'],train['Survived']],train['Pclass'], margins=True).style.background_gradient(cmap='summer_r')","6d449434":"fig, ax = plt.subplots(1,2, figsize=(18,8))\n\nshared_survived = train.groupby('shared_ticket')['Survived'].mean()\n\nshared_survived.plot.bar(ax=ax[1])\nsns.countplot('shared_ticket', hue='Survived', data=train, ax=ax[0])\n\nax[1].set_xticklabels(shared_survived.index, rotation=0)\nplt.show()\nplt.cla()\nplt.close()","1ef2dc10":"sns.factorplot('shared_ticket', 'Survived', col='Pclass', data=train, ax=ax[1])\nplt.show()\nplt.cla()\nplt.close()","e4e006e5":"train.groupby('Pclass').Fare.describe()","5082f047":"fig, ax = plt.subplots(3, figsize=(18,8))\nsns.distplot(train[train['Pclass']==1].Fare, ax=ax[0])\nsns.distplot(train[train['Pclass']==2].Fare, ax=ax[1])\nsns.distplot(train[train['Pclass']==2].Fare, ax=ax[2])\nax[0].set_title('Pclass 1 Fare Distribution')\nax[1].set_title('Pclass 2 Fare Distribution')\nax[2].set_title('Pclass 3 Fare Distribution')\nplt.tight_layout()\nplt.show()\nplt.cla()\nplt.close()","1223e563":"describe = train['Embarked'].describe()\nisna = train['Embarked'].isna().sum()\nprint(describe)\nprint(\"NaN\", isna)","777122f5":"train['Embarked'].fillna(\"S\", inplace=True)","3a27beab":"pd.crosstab(train['Embarked'], train['Survived'], margins=True).style.background_gradient(cmap='summer_r')","f55a4e99":"pd.crosstab([train.Embarked, train.Survived], train.Pclass, margins=True).style.background_gradient(cmap=\"summer_r\")","f5409d4b":"embarked_survived = train.groupby('Embarked').Survived.mean()\n\nfig,ax = plt.subplots(2,2, figsize=(18,8))\nembarked_survived.plot.bar(ax=ax[0,0])\nsns.countplot('Embarked', hue='Survived', data=train, ax=ax[0,1])\nsns.countplot('Embarked', hue='Sex', data=train, ax=ax[1,0])\nsns.countplot('Embarked', hue='Pclass', data=train, ax=ax[1,1])\n\nax[0,0].set_xticklabels(embarked_survived.index, rotation=0)\n\nplt.show()\nplt.cla()\nplt.close()","57fb9b0f":"sns.factorplot('Pclass', 'Survived', hue='Sex', col='Embarked', data=train)\n\nplt.show()\nplt.cla()\nplt.close()","176a2813":"sns.heatmap(train.corr(), annot=True, linewidth=0.2)\nfig = plt.gcf()\nfig.set_size_inches(10,8)\nplt.show()\nplt.cla()\nplt.close()","467c8b37":"train['age_band']=0\ntrain.loc[train['Age']<=16,'age_band']=0\ntrain.loc[(train['Age']>16)&(train['Age']<=32),'age_band']=1\ntrain.loc[(train['Age']>32)&(train['Age']<=48),'age_band']=2\ntrain.loc[(train['Age']>48)&(train['Age']<=64),'age_band']=3\ntrain.loc[train['Age']>64,'age_band']=4\n\ntrain['age_band'].value_counts().to_frame().style.background_gradient(cmap='summer')","b1c2c773":"sns.factorplot('age_band', 'Survived', col='Pclass', data=train)\nplt.show()\nplt.cla()\nplt.close()","46986d56":"train['fare_range'] = pd.qcut(train['Fare'], 4)\ntrain.groupby('fare_range').Survived.mean().to_frame().style.background_gradient(cmap=\"summer_r\")","7feac253":"train.loc[train['Fare'] <= 7.91, 'fare_cat'] = 0\ntrain.loc[(train['Fare'] > 7.91) & (train['Fare'] <= 14.454), 'fare_cat'] = 1\ntrain.loc[(train['Fare'] > 14.454) & (train['Fare'] <= 31.0), 'fare_cat'] = 2\ntrain.loc[(train['Fare'] > 31.0) & (train['Fare'] <= 512.3292), 'fare_cat'] = 3","4e2f3bbf":"sns.factorplot('fare_cat', 'Survived', hue='Sex', data=train)\nplt.show()\nplt.cla()\nplt.close()","8cd21647":"def merge_ticket_count(data):\n    if 'Ticket' in data.columns:\n        ticket = data.groupby('Ticket')['PassengerId'].count().reset_index()\n        ticket = ticket.rename(columns={'PassengerId':'count'})\n    \n    if 'count' not in data.columns:\n        data = data.merge(ticket, how='left', on='Ticket')\n    if data['count'].isna().sum() > 0:\n        data['count'].fillna(0, inplace=True)\n    \n    data.loc[data['shared_ticket'] == 0, 'count'] = 0\n    data['count'] = data['count'].astype(int)\n    return data['count']\n\ntrain['shared_count'] = merge_ticket_count(train)","d552481c":"train['group_size'] = 1 + train['SibSp'] + train['Parch']\ntrain.loc[(train['shared_count'] != 0) & (train['shared_count'] > train['group_size']), 'group_size'] = train['shared_count']\n\ntrain['alone'] = 0\ntrain.loc[train['group_size']==1, 'alone'] = 1 ","3725872c":"sns.factorplot('group_size','Survived',data=train)\nplt.title(\"group_size vs Survived\")\nplt.show()\nplt.cla()\nplt.close()","6020b60d":"sns.factorplot('alone','Survived', hue='Sex', col='Pclass', data=train)\nplt.title(\"alone vs Survived\")\nplt.show()\nplt.cla()\nplt.close()","de0326c4":"LE = LabelEncoder()\n\ntrain['sex_cat'] = LE.fit_transform(train['Sex'])\nprint(\"Sex string: \",LE.classes_)\nprint(\"Sex num :\", sorted(train['sex_cat'].unique()))\n\ntrain['embarked_cat'] = LE.fit_transform(train['Embarked'])\nprint(\"Embarked string: \",LE.classes_)\nprint(\"Embarked num :\", sorted(train['embarked_cat'].unique()))\n\ntrain['title_cat'] = LE.fit_transform(train['Title'])\nprint(\"Title string: \",LE.classes_)\nprint(\"Title num :\", sorted(train['title_cat'].unique()))","15335905":"train.drop(columns=['PassengerId','Name','Sex','Age','Ticket','Fare','fare_range','Cabin','Embarked','Title'], inplace=True)","0899e92e":"sns.heatmap(train.corr(), annot=True)\nfig=plt.gcf()\nfig.set_size_inches(18,15)\nplt.xticks(fontsize=14)\nplt.yticks(fontsize=14)\nplt.show()","13c394cc":"from sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import confusion_matrix\n\nfeatures = train.columns[1:]\nX = train[features]\ny = train['Survived']\n\nX_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.3, random_state=0, stratify=y)","6abe1c63":"logreg = LogisticRegression().fit(X_train, y_train)\nlogreg_pred = logreg.predict(X_test)\nlogreg_score = accuracy_score(logreg_pred, y_test)\n\nprint(\"The accuracy score of Logistic Regression: \", logreg_score)","56191d12":"lsvm = SVC(kernel='linear').fit(X_train, y_train)\nlsvm_pred = lsvm.predict(X_test)\nlsvm_score = accuracy_score(lsvm_pred, y_test)\nprint(\"The accuracy score of Linear Support Vector Machines: \", lsvm_score)","554177e2":"rsvm = SVC(kernel='rbf').fit(X_train, y_train)\nrsvm_pred = rsvm.predict(X_test)\nrsvm_score = accuracy_score(rsvm_pred, y_test)\nprint(\"The accuracy score of Linear Support Vector Machines: \", rsvm_score)","a65abc80":"rf = RandomForestClassifier().fit(X_train, y_train)\nrf_pred = rf.predict(X_test)\nrf_score = accuracy_score(rf_pred, y_test)\nprint(\"The accuracy of Random Forest Classifier: \", rf_score)","85e81fac":"knn = KNeighborsClassifier().fit(X_train, y_train)\nknn_pred = knn.predict(X_test)\nknn_score = accuracy_score(knn_pred, y_test)\nprint(\"The accuracy of K Nearest Neighbor: \", knn_score)","9b5fe4c3":"import numpy as np\n\nx = list(range(0,16))\na_index = x[1:]\na = pd.Series()\nerror = pd.Series()\n\nfor i in a_index:\n    knn = KNeighborsClassifier(n_neighbors=i).fit(X_train, y_train)\n    pred = knn.predict(X_test)\n    score = accuracy_score(pred, y_test)\n    a = a.append(pd.Series(score))\n\nfor k in a_index:\n    model = KNeighborsClassifier(n_neighbors=k).fit(X_train, y_train)\n    predict = model.predict(X_test)\n    error = error.append(pd.Series(np.mean(predict != y_test)))\n\nplt.subplot(2,1,1)\nplt.plot(a_index, a)\nplt.title(\"Accuracy Score and Error Rate\")\nplt.xticks(x)\nplt.ylabel(\"Accuracy Score\")\n\nplt.subplot(2,1,2)\nplt.plot(a_index, error)\nplt.xticks(x)\nplt.ylabel(\"Error Rate\")\nplt.xlabel(\"N Neighbors\")\n\nplt.show()\nplt.cla()\nplt.close()","b56accaa":"knn = KNeighborsClassifier(n_neighbors=11).fit(X_train, y_train)\nknn_pred = knn.predict(X_test)\nknn_score = accuracy_score(knn_pred, y_test)\nprint(\"The accuracy of K Nearest Neighbor: \", knn_score)","9f79b22e":"nb = GaussianNB().fit(X_train, y_train)\nnb_pred = nb.predict(X_test)\nnb_score = accuracy_score(nb_pred, y_test)\nprint(\"The accuracy of Naive Bayes: \", nb_score)","a7acb0d4":"dt = DecisionTreeClassifier().fit(X_train, y_train)\ndt_predict = dt.predict(X_test)\ndt_score = accuracy_score(dt_predict, y_test)\nprint(\"The accuracy of Decision Tree Classifier: \", dt_score)","ec9333a6":"from sklearn.model_selection import KFold\nfrom sklearn.model_selection import cross_val_score\n\nkfold = KFold(n_splits=10, random_state=22)\ncv_mean = []\nstd = []\naccuracy = []\n\nclassifiers = [\"Logistic Regression\", \"Linear SVM\", \"Radial SVM\",\"Random Forest\",\n               \"K-Nearest Neighbours\", \"Naive Bayes\", \"Decision Tree\"]\nmodels = [LogisticRegression(), SVC(kernel='linear', C=1, gamma=0.1), SVC(kernel='rbf', C=1, gamma=0.1),\n          RandomForestClassifier(n_estimators=700, random_state=0),KNeighborsClassifier(n_neighbors=11), GaussianNB(), DecisionTreeClassifier()]\n\nfor i in models:\n    model = i\n    cv_score = cross_val_score(model,X,y, cv=kfold, scoring=\"accuracy\")\n    accuracy.append(cv_score)\n    cv_mean.append(cv_score.mean())\n    std.append(cv_score.std())\n    \nmodels_df = pd.DataFrame({\"CV Mean\":cv_mean, \"Error\":std}, index=classifiers)\nmodels_df.sort_values(by='CV Mean', ascending=False)\n    ","38e76cc6":"box = pd.DataFrame(accuracy, index=classifiers)\n\nbox.T.boxplot(figsize=(18,8))\nplt.show()\nplt.cla()\nplt.close()","ea3cadd8":"models_df['CV Mean'].sort_values().plot.barh(width=0.8, figsize=(8,6))\nplt.title('Average CV Accuracy Score')\nplt.show()\nplt.cla()\nplt.close()","ea58006f":"from sklearn.model_selection import cross_val_predict\n\nfig, ax = plt.subplots(2,4, figsize=(18,6))\n# Logistic Regression\ny_predict = cross_val_predict(LogisticRegression(), X,y, cv=10)\ncnf = confusion_matrix(y_predict, y)\nsns.heatmap(cnf, annot=True, ax=ax[0,0], fmt='2.0f')\nax[0,0].set_title(\"Logistic Regression\")\n# Linear SVM\ny_predict = cross_val_predict(SVC(kernel='linear', C=1, gamma=0.1), X,y, cv=10)\ncnf = confusion_matrix(y_predict, y)\nsns.heatmap(cnf, annot=True, ax=ax[0,1], fmt='2.0f')\nax[0,1].set_title(\"Linear SVM\")\n# Radial SVM\ny_predict = cross_val_predict(SVC(kernel='rbf', C=1, gamma=0.1), X,y, cv=10)\ncnf = confusion_matrix(y_predict, y)\nsns.heatmap(cnf, annot=True, ax=ax[0,2], fmt='2.0f')\nax[0,2].set_title(\"Radial SVM\")\n# Random Forest\ny_predict = cross_val_predict(RandomForestClassifier(n_estimators=700, random_state=0), X,y, cv=10)\ncnf = confusion_matrix(y_predict, y)\nsns.heatmap(cnf, annot=True, ax=ax[0,3], fmt='2.0f')\nax[0,3].set_title(\"Random Forest\")\n# K-Nearest Neighbours\ny_predict = cross_val_predict(KNeighborsClassifier(n_neighbors=11), X,y, cv=10)\ncnf = confusion_matrix(y_predict, y)\nsns.heatmap(cnf, annot=True, ax=ax[1,0], fmt='2.0f')\nax[1,0].set_title(\"K-Nearest Neighbours\")\n# Naive Bayes\ny_predict = cross_val_predict(GaussianNB(), X,y, cv=10)\ncnf = confusion_matrix(y_predict, y)\nsns.heatmap(cnf, annot=True, ax=ax[1,1], fmt='2.0f')\nax[1,1].set_title(\"Naive Bayes\")\n# Decision Tree\ny_predict = cross_val_predict(DecisionTreeClassifier(), X,y, cv=10)\ncnf = confusion_matrix(y_predict, y)\nsns.heatmap(cnf, annot=True, ax=ax[1,2], fmt='2.0f')\nax[1,2].set_title(\"Decision Tree\")\n\nax[-1,-1].axis('off')\n\nfig.tight_layout()\n\nplt.show()\nplt.cla()\nplt.close()","7f05c7ca":"from sklearn.model_selection import GridSearchCV","9164bb0b":"C=[0.05,0.1,0.2,0.3,0.25,0.4,0.5,0.6,0.7,0.8,0.9,1]\ngamma=[0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9,1.0]\nkernel=['rbf','linear']\nhyper={'kernel':kernel,'C':C,'gamma':gamma}\ngd=GridSearchCV(estimator=SVC(),param_grid=hyper,verbose=True)\ngd.fit(X,y)\nprint(gd.best_score_)\nprint(gd.best_estimator_)","a920024d":"n_estimators=list(range(100,1000,100))\nhyper={'n_estimators':n_estimators}\ngd=GridSearchCV(estimator=RandomForestClassifier(random_state=0),param_grid=hyper,verbose=True)\ngd.fit(X,y)\nprint(gd.best_score_)\nprint(gd.best_estimator_)","0c1f7f59":"from sklearn.ensemble import VotingClassifier, BaggingClassifier","466dbd17":"vclass_models = VotingClassifier(estimators=[(\"logreg\",LogisticRegression()),\n                                              (\"linear_svm\",SVC(kernel='linear', C=1, gamma=0.1, probability=True)),\n                                              (\"radial_svm\", SVC(kernel='rbf', C=1, gamma=0.1, probability=True)),\n                                              (\"rf\", RandomForestClassifier(n_estimators=700, random_state=0)),\n                                              (\"knn\", KNeighborsClassifier(n_neighbors=11)),\n                                              (\"gnb\", GaussianNB()),\n                                              (\"dt\", DecisionTreeClassifier())], voting='soft').fit(X_train, y_train)\nvclass_score = vclass_models.score(X_test, y_test)\nvclass_cv = cross_val_score(vclass_models,X,y, cv=10, scoring='accuracy')\n\nprint(\"The accuracy score of Ensembled Model: \",vclass_score)\nprint(\"The cross validated of Ensembled Model: \",vclass_cv.mean())","edcf03ea":"bg_knn = BaggingClassifier(base_estimator=(KNeighborsClassifier(n_neighbors=5)), random_state=0, n_estimators=700)\nbg_knn = bg_knn.fit(X_train, y_train)\npredict = bg_knn.predict(X_test)\nbg_knn_score = accuracy_score(predict, y_test)\ncv_bg_knn = cross_val_score(bg_knn,X,y, scoring='accuracy')\n\nprint(\"The accuracy score of Bagged KNN\",bg_knn_score)\nprint(\"The cross validated score of Bagged KNN\",cv_bg_knn.mean())","f189932d":"bg_dt = BaggingClassifier(base_estimator=(DecisionTreeClassifier()), random_state=0, n_estimators=700)\nbg_dt = bg_dt.fit(X_train, y_train)\npredict = bg_dt.predict(X_test)\nbg_dt_score = accuracy_score(predict, y_test)\ncv_bg_dt = cross_val_score(bg_dt,X,y, scoring='accuracy')\n\nprint(\"The accuracy score of Bagged Decision Tree\",bg_dt_score)\nprint(\"The cross validated score of Bagged Decision Tree\",cv_bg_dt.mean())","d97a1316":"bg_gnb = BaggingClassifier(base_estimator=(GaussianNB()), random_state=0, n_estimators=700)\nbg_gnb = bg_gnb.fit(X_train, y_train)\npredict = bg_gnb.predict(X_test)\nbg_gnb_score = accuracy_score(predict, y_test)\ncv_bg_gnb = cross_val_score(bg_gnb,X,y, scoring='accuracy')\n\nprint(\"The accuracy score of Bagged Gaussian Naive Bayes\",bg_gnb_score)\nprint(\"The cross validated score of Bagged Gaussian Naive Bayes\",cv_bg_gnb.mean())","d60f5c19":"bg_rsvm = BaggingClassifier(base_estimator=(SVC(kernel='rbf', C=1, gamma=0.1)), random_state=0, n_estimators=100)\nbg_rsvm = bg_rsvm.fit(X_train, y_train)\npredict = bg_rsvm.predict(X_test)\nbg_rsvm_score = accuracy_score(predict, y_test)\ncv_bg_rsvm = cross_val_score(bg_rsvm,X,y, scoring='accuracy')\n\nprint(\"The accuracy score of Bagged Radial SVM\",bg_rsvm_score)\nprint(\"The cross validated score of Bagged Radial SVM\",cv_bg_rsvm.mean())","376e3515":"n_estimators = list(range(100,1000, 100))\n\nhyper={'n_estimators':n_estimators}\n\ngd = GridSearchCV(estimator=BaggingClassifier(base_estimator=SVC(kernel='rbf', C=1, gamma=0.1)), param_grid=hyper, verbose=True).fit(X,y)\nprint(gd.best_score_)\nprint(gd.best_estimator_)","ef33902f":"from sklearn.ensemble import AdaBoostClassifier\n\nada = AdaBoostClassifier(n_estimators=900, random_state=0, learning_rate=0.1)\nada_cv = cross_val_score(ada,X,y, cv=10, scoring='accuracy')\nprint(\"The cross validated score of AdaBoost\",ada_cv.mean())","65d5a740":"from sklearn.ensemble import GradientBoostingClassifier\n\ngb = GradientBoostingClassifier(n_estimators=200, random_state=0, learning_rate=0.05)\ngb_cv = cross_val_score(gb,X,y, cv=10, scoring='accuracy')\nprint(\"The cross validated score of GradientBoosting\",gb_cv.mean())","14e1ee84":"from xgboost import XGBClassifier\n\nxgb = XGBClassifier(importance_type='gain',n_estimators=100,learning_rate=0.05, random_state=0, tree_method='exact',validate_parameters=1, verbosity=None)\nxg_cv = cross_val_score(xgb,X,y, cv=10, scoring='accuracy')\nprint(\"The cross validated score of XGBoost\",xg_cv.mean())","ba4bac06":"n_estimators = list(range(100,1000, 100))\nlearning_rate = [0.05,0.1,0.2,0.25,0.3,0.4,0.5,0.6,0.7,0.8,0.9,1]\n\nhyper={'n_estimators':n_estimators, 'learning_rate':learning_rate}\n\ngd = GridSearchCV(estimator=AdaBoostClassifier(), param_grid=hyper, verbose=True).fit(X,y)\n\nprint(gd.best_score_)\nprint(gd.best_estimator_)","9657b5e3":"ada = AdaBoostClassifier(n_estimators=900, random_state=0, learning_rate=0.1)\nada_cv_pred = cross_val_predict(ada,X,y, cv=10)\ncnf = confusion_matrix(ada_cv_pred, y)\n\nplt.figure(figsize=(18,8))\nsns.heatmap(cnf, annot=True, fmt='2.0f', cmap='winter')\nplt.title(\"Best Model: AdaBoost\")\n\nplt.show()\nplt.cla()\nplt.close()","b63be57e":"fig, ax = plt.subplots(2,2 , figsize=(18,8))\n\n# Random Forest\nmodel = RandomForestClassifier(n_estimators=700, random_state=0).fit(X,y)\nrf_series = pd.Series(model.feature_importances_, X.columns)\nrf_series.sort_values().plot.barh(ax=ax[0,0], color='#94c1c0')\nax[0,0].set_title(\"Feature Importance in Random Forest\")\n# AdaBoost\nmodel = AdaBoostClassifier(n_estimators=900, random_state=0, learning_rate=0.1).fit(X,y)\nada_series = pd.Series(model.feature_importances_, X.columns)\nada_series.sort_values().plot.barh(ax=ax[0,1], color='#e54c3a')\nax[0,1].set_title(\"Feature Importance in AdaBoost\")\n# GradientBoosting\nmodel = GradientBoostingClassifier(n_estimators=200, random_state=0, learning_rate=0.05).fit(X,y)\ngb_series = pd.Series(model.feature_importances_, X.columns)\ngb_series.sort_values().plot.barh(ax=ax[1,0], color='#93a85c')\nax[1,0].set_title(\"Feature Importance in GradientBoosting\")\n# XGBoost\nmodel = XGBClassifier(importance_type='gain',n_estimators=100,learning_rate=0.05, random_state=0, tree_method='exact',validate_parameters=1, verbosity=None).fit(X,y)\nxgb_series = pd.Series(model.feature_importances_, X.columns)\nxgb_series.sort_values().plot.barh(ax=ax[1,1], color='#fec821')\nax[1,1].set_title(\"Feature Importance in XGBoost\")\n\nplt.show()\nplt.cla()\nplt.close()","f4911e5b":"def get_title(data):\n    for t in data['Name']:\n        data['Title'] = data['Name'].str.extract(\"([A-Za-z]+)\\.\")\n        \ndef fillna_age(data):\n    age_nan = data['Age'].isna()\n\n    data.loc[age_nan & (data['Title']=='Dr'), 'Age'] = 42\n    data.loc[age_nan & (data['Title']=='Master'), 'Age'] = 5\n    data.loc[age_nan & (data['Title']=='Ms'), 'Age'] = 22\n    data.loc[age_nan & (data['Title']=='Miss'), 'Age'] = 22\n    data.loc[age_nan & (data['Title']=='Mr'), 'Age'] = 33\n    data.loc[age_nan & (data['Title']=='Mrs'), 'Age'] = 36\n    \ndef get_age_band(data):\n    data['age_band']=0\n    data.loc[data['Age']<=16,'age_band']=0\n    data.loc[(data['Age']>16)&(data['Age']<=32),'age_band']=1\n    data.loc[(data['Age']>32)&(data['Age']<=48),'age_band']=2\n    data.loc[(data['Age']>48)&(data['Age']<=64),'age_band']=3\n    data.loc[data['Age']>64,'age_band']=4\n    \n\ndef get_shared_ticket(data):\n    data.loc[data['Ticket'].duplicated(keep=False), 'shared_ticket'] = 1\n    data['shared_ticket'].fillna(0, inplace=True)\n    data['shared_ticket'] = data['shared_ticket'].astype(int)\n\n    \ndef get_group_alone(data):\n    data['group_size'] = 1 +  data['SibSp'] +  data['Parch']\n    data.loc[( data['shared_count'] != 0) & ( data['shared_count'] > data['group_size']), 'group_size'] =  data['shared_count']\n    \n    data['alone'] = 0\n    data.loc[ data['group_size']==1, 'alone'] = 1 \n    \n\ndef fillna_fare(data):\n    fare_mean = data.groupby('Pclass').Fare.mean().to_list()\n    data.Fare.fillna(fare_mean[-1], inplace=True)\n    \ndef get_fare_cat(data):\n    data.loc[data['Fare'] <= 7.91, 'fare_cat'] = 0\n    data.loc[(data['Fare'] > 7.91) & (data['Fare'] <= 14.454), 'fare_cat'] = 1\n    data.loc[(data['Fare'] > 14.454) & (data['Fare'] <= 31.0), 'fare_cat'] = 2\n    data.loc[(data['Fare'] > 31.0) & (data['Fare'] <= 512.3292), 'fare_cat'] = 3\n    \ndef string_to_num(data):\n    LE = LabelEncoder()\n    data['sex_cat'] = LE.fit_transform(data['Sex'])\n    data['embarked_cat'] = LE.fit_transform(data['Embarked'])\n    data['title_cat'] = LE.fit_transform(data['Title'])\n\n    \ndef data_prep(data):\n    get_title(data)\n    fillna_age(data)\n    get_age_band(data)\n    convert_title(data)\n\n    get_shared_ticket(data)\n    data['shared_count'] = merge_ticket_count(data)\n    get_group_alone(data)\n    \n    fillna_fare(data)\n    get_fare_cat(data)\n    \n    string_to_num(data)","f32b36ab":"test = pd.read_csv('..\/input\/titanic\/test.csv')\ndata_prep(test)\ntf = X_train.columns\ntitanic = test[tf]\n\nada = ada.fit(X_train, y_train)\ntitanic_pred = ada.predict(titanic)","d059ff37":"submission = pd.DataFrame()\nsubmission['PassengerId'] = test['PassengerId']\nsubmission['Survived'] = titanic_pred\n\nsubmission.to_csv(\"ada_prediction.csv\", index=False)","fefa360d":"## Embarked (Categorical Feature)","36ed6907":"## Observations in a Nutshell for all features:\nSex: The chance of survival for women is high as compared to men.\n\nPclass: 1st class passenger gives better chances of survival. The survival rate for Pclass3 is very low. For women, the chance of survival from Pclass1 and Pclass2 is high, at Pclass1 the survivability almost 1.\n\nAge: Children do have a high chance of survival. Passengers between age group 15 to 35 died a lot.\n\nEmbarked: This is a very interesting feature. The chances of survival at C looks to be better than even though the majority of Pclass1 passengers came from port S. Passengers at Q were all from Pclass3.\n\nParch+SibSp: Having 1-2 siblings,spouse on board or 1-3 Parents shows a greater chance of survivability rather than being alone or having a large family travelling with you.","c0709dfb":"## Support Vector Machines - Linear","b68f17bf":"## Pclass (Ordinal feature)","749367af":"The best score for **Radial SVM** is 83.4% with C=1 and gamma=0.1. For **RandomForest**, accuracy score is about 82.4% with n_estimators=700.","4a9cf7fe":"## Bagging\nBagging is a general ensemble method. It works by applying similar classifiers on small partitions of the dataset and then taking the average of all the predictions. Due to the averaging,there is reduction in variance. Unlike Voting Classifier, Bagging makes use of similar classifiers.\n\nBagging works best with models with high variance. An example for this can be Decision Tree or Random Forests. We can use KNN with small value of n_neighbours.","f088e7c0":"## EDA to Prediction\n\nThe objective of this notebook is to get used with the workflow of data analysis referred to [DieTanic](https:\/\/www.kaggle.com\/ash316\/eda-to-prediction-dietanic) workflow","f864edfb":"We will use the mean age per title to fillna in Age ","56809971":"# Gaussian Naive Bayes","006b7282":"* Pclass 3 has the highest amount of passengers with the lowest survivability rate, only 24% of it survived\n* Pclass 2 has the lowest amount of passenger with almost 50% survivability rate\n* Pclass 1 passengers mostly survived, at 62% rate\n\nPclass 1 where given higher priority while rescue despite Pclass 3 has the highest passengers, it seems a higher class means a higher survivability.","7b3fe251":"As stated before the more expensive your ticket then the higher survivability rate\n\nNow we need to categorized it by its range","54bc3d29":"## Dropping Unneeded Feature\n\n* PassengerId -> can't be categorized\n* Name -> can't be categorized\n* Sex -> categorized with sex_cat\n* Age -> categorized with age_band\n* Ticket -> categorized with shared_ticket\n* Fare, fare_range -> categorized with fare_cat\n* Cabin -> Lots of NaN and hard to clean because we found lots of passengger with multiple cabin\n* Embarked -> categorized with embarked_cat\n* Title -> categorized with title_cat","919337f9":"* Train.csv will contain the details of a subset of the passengers on board (891 to be exact) and importantly, will reveal whether they survived or not, also known as the \u201cground truth\u201d.\n\n* The `test.csv` dataset contains similar information but does not disclose the \u201cground truth\u201d for each passenger. It\u2019s your job to predict these outcomes.\n\n* Using the patterns you find in the train.csv data, predict whether the other 418 passengers on board (found in test.csv) survived.","d2449128":"### Bagged Decision Tree","1c6566ec":"We found that N 11 gives the best accuracy score and the lowest error rate, so we will use it forward.","015a970c":"* The passengers with 3 parent or children on board have 60% survivability rate\n* Having 1-2 parent or children on board gives more than 50% chance to survive\n* The passenger who on board alone have 34% chance to survive\n* Having 5 children gives 20% survivability rate\n* While passenger with 4 or 6 children has zero chance to survive","dc854640":"We can see there's some duplicated ticket, we also see earlier from SibSp and Parch that in a group boost survivability rate.","a64623af":"## Confusion Matrix\nIt gives the number of correct and incorrect classifications made by the classifier.","a528e93f":"## Logistic Regression","04945c31":"## Fare (Continous Feature)\n\nThis feature worth to look at because based on Pclass feature, Pclass 1 has more chance to survive, the higher the class the higher chance to survive","0c7b6c40":"* The order from th highest amount of passenger with shared ticket is Pclass 1, 2,then 3\n* Irrespectable to their Pclass, passenger with shared ticket have higher chance to survive ","f2f4dc93":"## Ticket (Cardinal Features)\n\nThe nunique ticket is lower than the total passenger, it means there are duplicated tickets. SibSp and Parch features tell us, passenger with no relatives has lower chance to survive. I think it would be interesting to looked more, does have a shared ticket means the passenger have relative(s) on board? does it affect the survivability?. ","f8ac9fd7":"We fill the Embarked missing value with S as it is the most frequent embarked port and the missing value is small","2865762c":"We have gained some insights from the EDA part. But with that, we cannot accurately predict or tell whether a passenger will survive or die. So now we will predict it using Classification Algorithms. Following are the algorithms I will use to make the model:\n\n1)Logistic Regression\n\n2)Support Vector Machines(Linear and radial)\n\n3)Random Forest\n\n4)K-Nearest Neighbours\n\n5)Naive Bayes\n\n6)Decision Tree\n\n7)Logistic Regression","2f96f51b":"## Correlation Between Feature","ef96228b":"* 35% of Passenger with no shared ticket survived\n* 49% of Passenger with shared ticket survived","4b78bc22":"The classification accuracy can be sometimes misleading due to imbalance. We can get a summarized result with the help of confusion matrix, which shows where did the model go wrong, or which class did the model predict wrong.","3fa0a19f":"## SibSp (Ordinal Feature)","e895234a":"It's visible trend that for older passengger has lower survivability rate irrespective with their Pclass.","dbfd2f9c":"We will hyper parameter tuning Bagged radial SVM as it gives the best cross validated score","8a5233f7":"## Observations:\n1. Some of the common important features are sex_cat, Pclass, title_cat, and group_size.\n\n2. The Sex and Pclass features importance are inline with the observation earlier. Pclass looks to be not in top 5 feature only in AdaBoost.\n\n3. Similarly the Pclass and fare_cat refer to the status of the passengers and group_size with Alone,Parch and SibSp.\n\nWe can see the feature title_cat, which is at the top in many classifiers have positive correlation with Sex, so they both can refer to the gender.","9414dda4":"The accuracy of a model is not the only factor that determines the robustness of the classifier. Let's say that a classifier is trained over a training data and tested over the test data and it scores an accuracy of 90%.\n\nNow this seems to be very good accuracy for a classifier, but can we confirm that it will be 90% for all the new test sets that come over??. The answer is No, because we can't determine which all instances will the classifier will use to train itself. As the training and testing data changes, the accuracy will also change. It may increase or decrease. This is known as model variance.\n\nTo overcome this and get a generalized model,we use **Cross Validation**.","375dec52":"### Bagged Gaussian Naive Bayes","26f47539":"Female get more priority while rescue irrespective to their Pclass.\n\nIt shows sex affects survivability rate.","1018f0a9":"* It seems irrespective to Pclass, female and child has high survivability rate","a54449e6":"* The highest amount of passenger that survived is having 1 SibSp then 2 SibSp\n* Having 1 SibSp on board gives you more than 50% survivability\n* More SibSp the bigger chance to not make it\n","99f3d8e9":"## Sex (Categorical Feature)","ceb120ea":"## group_size and alone (categorical)\n\nWe can create a new feature called \"group_size\" and \"alone\" then analyse it. It gives us a combined data of SibSP, Parch, and shared_ticket then we can check if survival rate have anything to do with group size of the passengers. Alone will denote whether a passenger is alone or not\n\nWe try to find passenger group size by:\n* find shared ticket count\n* add the passenger with sibsp and parch count\n* if sibsp and parch lower than shared ticket count then use shared ticket count as the group size","83b1b505":"### Bagged KNN","30f80352":"### XGBoost","60287990":"## Interpreting Confusion Matrix\nThe left diagonal shows the number of correct predictions made for each class while the right diagonal shows the number of wrong prredictions made. Lets consider the first plot for **Logistic Regression**:\n\n1)The no. of correct predictions are 470 (for dead) + 237 (for survived) with the mean CV accuracy being (470+237)\/891 = 79.5% which we did get earlier.\n\n2)Errors--> Wrongly Classified 78 dead people as survived and 105 survived as dead. Thus it has made more mistakes by predicting dead as survived.\n\nBy looking at all the matrices, we can say that rbf-SVM has a higher chance in correctly predicting dead passengers but NaiveBayes has a higher chance in correctly predicting passengers who survived.","56e801da":"Now the above correlation plot, we can see some positively related features. Some of them being **SibSp-group_size**, **Parch-group_size**, **shared_count-group_size**, **shared_ticket-group_size** and some negative ones like **alone-group_size**.","1fdd97bd":"Clearly, as the fare cat increases, the survival rates increases.","e5130ec3":"## Submission","cffca772":"## fare_range (categorical)\n\nwe categorized fare by its range, we would use pandas.qcut to automatically make range","7a93c7e9":"## Cross Validation\nMany times, the data is imbalanced, i.e there may be a high number of class1 instances but less number of other class instances. Thus we should train and test our algorithm on each and every instance of the dataset. Then we can take an average of all the noted accuracies over the dataset.\n\n1)The K-Fold Cross Validation works by first dividing the dataset into k-subsets.\n\n2)Let's say we divide the dataset into (k=5) parts. We reserve 1 part for testing and train the algorithm over the 4 parts.\n\n3)We continue the process by changing the testing part in each iteration and training the algorithm over the other parts. The accuracies and errors are then averaged to get a average accuracy of the algorithm.\n\nThis is called **K-Fold Cross Validation**.\n\n4)An algorithm may underfit over a dataset for some training data and sometimes also overfit the data for other training set. Thus with cross-validation, we can achieve a generalised model.","fde3c1ac":"The Women and Child first policy thus holds true irrespective of the class.","adb5e5fc":"* group_size=1 means that the passeneger is alone.\n* Clearly, if you are alone or group_size=0,then chances for survival is very low.\n* For family size > 4,the chances decrease too.","9b5e5b2e":"## Decision Tree","c3029d64":"The feature shows something interesting, The majority of the passenger is a male but only around 18% of it survived, while 74% of the female survived from the accident.\n\nIt seems female has the higher priority while rescue.","f837ac3d":"### Random Forest","61a783dc":"## Predictive Modelling","76447bc0":"## Age (Continous Feature)","ec303a2b":"## Ensembling\u00b6\nEnsembling is a good way to increase the accuracy or performance of a model. In simple words, it is the combination of various simple models to create a single powerful model.\n\nLets say we want to buy a phone and ask many people about it based on various parameters. So then we can make a strong judgement about a single product after analysing all different parameters. This is Ensembling, which improves the stability of the model. Ensembling can be done in ways like:\n\n1. Voting Classifier\n\n2. Bagging\n\n3. Boosting","9150aa46":"* Irrespectable with the Pclass having parent or children amount to 3 gives more survivability rate\n* Parch more than 3 only found at Pclass 3","9c7dc74a":"## Parch (Ordinal Feature)\n\nParch is a feature that shows the count of parent and children on board with the passenger","328379d6":"out of 891 people on train dataset 38.4% or more than 300 people survived from it.\n\nThen we will look with another feature, like **Pclass**, **Sex**, **Age**, etc","8e88acb6":"## Feature Importance","6e005329":"## Boosting\n\nBoosting is an ensembling technique which uses sequential learning of classifiers. It is a step by step enhancement of a weak model.\n\nBoosting works as follows:\n1. A model is first trained on the complete dataset, now the model will get some instances right while some wrong.\n1. The next iteration, the learner will focus more on the wrongly predicted instances or give more weight to it. Thus it will try to predict the wrong instance correctly.\n1. Now this iterative process continous, and new classifers are added to the model until the limit is reached on the accuracy.\n\n\n","0ee95eb2":"* Have 1 SibSp rather than 0 relative on board gives more chance to survive on each Pclass\n* for Pclass 1 and 2 passenger with 1-3 siblings on board has higher survivability rate than passenger in Pclass 3\n* Only Pclass 3 passenger that has more than 4 SibSp on board, a higher SibSp makes a lower survivability\n\nSibSp shows have effect on survivability","abea4db6":"It is visible that being alone is harmful irrespective of Sex or Pclass, except for Pclass3 where the chances of females who are alone is high than those with family.","a927b4db":"## Exploratory Data Analysis (EDA)","28c3f022":"### Stochastic Gradient Boosting\nHere too the weak learner is a Decision Tree.","f90dc00d":"## Hyper-Parameters Tuning\nThe machine learning models are like a Black-Box. There are some default parameter values for this Black-Box, which we can tune or change to get a better model. Like the C and gamma in the SVM model and similarly different parameters for different classifiers, are called the hyper-parameters, which we can tune to change the learning rate of the algorithm and get a better model. This is known as Hyper-Parameter Tuning.\n\nWe will tune the hyper-parameters for the 2 best classifiers i.e the SVM and RandomForests.","53ba1912":"## Hyper-Parameter Tuning for AdaBoost\u00b6","06236db6":"## K Nearest Neighbor","929ddb85":"### AdaBoost\n\nThe weak learner or estimator in this case is a Decsion Tree. But we can change the dafault base_estimator to any algorithm of our choice.","865d4c02":"### How many has survived?","f8759c19":"### Bagged Radial SVM","00615958":"### SVM","c444340e":"* The majority of the passenger embarked from port S\n* The survivability of passenger from port S is the lowest","98d36f8d":"**Age**, **Cabin**, and **Embarked** have null values","e1aa6fdf":"Passengger in Pclass 1 mostly embarked from S\nThe majority of passengger embarked from Q is in Pclass 3","0fc236b2":"## Confusion Matrix for the Best Model","d84ba8d2":"### Hyper-Parameter Tuning for Bagged Radial SVM","21b4f67e":"KNN accuracy depend on the number of n. We can test it out","114a40c3":"Regardless of the sex and Pclass, passengger embarked from C has the highest survivability rate.","eeafa94e":"## age_band (categorical)\n\nThere's a problem using continous value in machine learning so we need to group age based on its range","fc68c901":"We got the highest accuracy of ensembled model with AdaBoost. We will try to increase it with Hyper-Parameter Tuning","3780bd84":"## Convert String value to Numeric\n\nSince we cannot pass strings to a machine learning model, we need to convert Embarked, Sex, and Title into numeric values. We will use sklearn preprocessing LabelEncoder to do this.","93cf744b":"Unneeded:\n* PassengerId -> cannot be categorized\n* Name -> cannot be categorized\n\nUsed:\n* Survived -> categorical\n* Pclass -> categorical\n* Sex -> categorical\n* SibSp -> discrete\n* Parch -> discrete\n* Embarked -> categorical\n* shared_ticket -> categorical\n\nNeed to modified:\n* age -> convert continous value to categorical\n* fare -> convert cardinal to categorical\n+ SibSp + Parch + Ticket -> to show group size onboard\n* Embarked, Sex, Initial -> convert string to numerical","565d084a":"## Feature Engineer\n\nSome features may have problem while doing machine learning so we need to add or remove it","bcbb1f8c":"## Random Forest","ac8afe32":"The highest correlation is Parch-shared_ticket followed with SibSp-shared_ticket at the amount of 0.47 and 0.44. We still can use all the feature because it's not redundant","94031119":"## Support Vector Machines - Radial"}}