{"cell_type":{"2946dc22":"code","af248621":"code","1566b60b":"code","a933910d":"code","95b4fac2":"code","76834297":"code","c010bf47":"code","d6e9d506":"code","eae529a2":"code","78199149":"code","dcc78577":"code","591ea978":"code","d1dd07d8":"code","3387a3d8":"code","6e80b6bb":"code","f945f98d":"code","a34a472b":"code","e309f7f6":"code","6ce881b8":"code","d5ea0461":"code","75b4395e":"code","4fa83c21":"code","1e29017f":"code","0e82b4b4":"code","fa7ded1c":"code","3a0c012d":"code","f85a2395":"code","8f999afc":"code","61dcdef7":"code","b01f5ba6":"code","0c19c2fd":"markdown","c554dcfc":"markdown","cf20f173":"markdown","bde9234a":"markdown","a83f481e":"markdown","ba1dcd34":"markdown","ed2471c4":"markdown","63c84c92":"markdown","abc16456":"markdown","0c0a9383":"markdown","157b875c":"markdown","a9bd422b":"markdown","308b3914":"markdown","c1330222":"markdown","d0f5d0db":"markdown","bb7cdd2e":"markdown","2d38ab09":"markdown","b066e9c4":"markdown","f012e70c":"markdown","a45b396b":"markdown","dc7eb281":"markdown","8afd3c1d":"markdown","4e4811b3":"markdown","fe34983a":"markdown","e084691e":"markdown","8d244ebf":"markdown","094c1a2e":"markdown","a51f387b":"markdown","1cc5b9db":"markdown","fb922ec1":"markdown","d4b8c628":"markdown","bd59c2e8":"markdown","0fd1c69a":"markdown","6e703315":"markdown"},"source":{"2946dc22":"!pip install -q gym_super_mario_bros==7.3.0 nes_py","af248621":"!xvfb-run -s \"-screen 0 1400x900x24\" jupyter notebook","1566b60b":"import matplotlib.pyplot as plt\n%matplotlib inline\nfrom IPython import display","a933910d":"def show_state(env, step=0, info=\"\"):\n    plt.figure(3)\n    plt.clf()\n    plt.imshow(env.render(mode='rgb_array'))\n    plt.title(\"Step: %d %s\" % (step, info))\n    plt.axis('off')\n\n    display.clear_output(wait=True)\n    display.display(plt.gcf())","95b4fac2":"import gym_super_mario_bros\nfrom nes_py.wrappers import JoypadSpace\n\nfrom gym_super_mario_bros.actions import SIMPLE_MOVEMENT","76834297":"SIMPLE_MOVEMENT","c010bf47":"env = gym_super_mario_bros.make('SuperMarioBros-v0')","d6e9d506":"env = JoypadSpace(env, SIMPLE_MOVEMENT)","eae529a2":"env.action_space","78199149":"env.observation_space.shape","dcc78577":"done = True\nfor step in range(100):\n    if done:\n        env.reset()\n    state, reward, done, info = env.step(env.action_space.sample())\n    #print(state, reward, done, info)\n    show_state(env,step,info)\n#env.close()","591ea978":"#!pip install torch==1.10.1+cu113 torchvision==0.11.2+cu113 torchaudio===0.10.1+cu113\n!pip install -q torch torchvision torchaudio","d1dd07d8":"!pip install -q stable-baselines3[extra]","3387a3d8":"from gym.wrappers import GrayScaleObservation\n","6e80b6bb":"from stable_baselines3.common.vec_env import VecFrameStack, DummyVecEnv\n#from matplotlib import pyplot as plt","f945f98d":"# env = gym_super_mario_bros.make('SuperMarioBros-v0')\n# env = JoypadSpace(env, SIMPLE_MOVEMENT)\nenv = GrayScaleObservation(env, keep_dim=True)\nenv = DummyVecEnv([lambda: env])\nenv = VecFrameStack(env, 4, channels_order='last')","a34a472b":"state = env.reset()","e309f7f6":"state, reward, done, info = env.step([5])","6ce881b8":"plt.figure(figsize=(20,16))\nfor idx in range(state.shape[3]):\n    plt.subplot(1,4,idx+1)\n    plt.imshow(state[0][:,:,idx])\nplt.show()","d5ea0461":"import os\nfrom stable_baselines3 import PPO\nfrom stable_baselines3.common.callbacks import BaseCallback","75b4395e":"class TrainAndLoggingCallback(BaseCallback):\n    \n    def __init__(self, check_freq, save_path, verbose=1):\n        super(TrainAndLoggingCallback, self).__init__(verbose)\n        self.check_freq = check_freq\n        self.save_path = save_path\n    \n    def _init_callback(self):\n        if self.save_path is not None:\n            os.makedirs(self.save_path, exist_ok=True)\n    \n    def _on_step(self):\n        if self.n_calls % self.check_freq == 0:\n            model_path = os.path.join(self.save_path, 'best_model_{}'.format(self.n_calls))\n            self.model.save(model_path)\n        return True","4fa83c21":"CHECKPOINT_DIR = '.\/train\/'\nLOG_DIR = '.\/logs\/'","1e29017f":"check_freq = 100\n#check_freq = 100000","0e82b4b4":"callback = TrainAndLoggingCallback(check_freq=check_freq, save_path=CHECKPOINT_DIR)","fa7ded1c":"model = PPO('CnnPolicy', env, verbose=1, tensorboard_log=LOG_DIR, learning_rate=0.000001, n_steps=512)","3a0c012d":"model.learn(total_timesteps=check_freq, callback=callback)","f85a2395":"model.save('astatemodel')","8f999afc":"model = PPO.load('.\/train\/best_model_100')\n#model = PPO.load('.\/train\/best_model_100000')","61dcdef7":"state = env.reset()","b01f5ba6":"state = env.reset()\n\n#for step in range(10000):\nfor step in range(1000):\n    if done:\n        env.reset()\n    action, _ = model.predict(state)\n    state, reward, done, info = env.step(action)\n    #print(state, reward, done, info)\n    show_state(env,step,info)\n#env.close()","0c19c2fd":"# PREPROCESS ENVIRONMENT","c554dcfc":"### 1. Defining callback","cf20f173":"### 3. Let environment know the input space.","bde9234a":"PPO stands for proximal policy optimisation. Read more it here: https:\/\/arxiv.org\/abs\/1707.06347","a83f481e":"Adapted for kaggle notebook from\nhttps:\/\/www.youtube.com\/watch?v=2eeYqJ0uBKE","ba1dcd34":"### 2. We need to initialise the environment.","ed2471c4":"### NOTE: \n1. Youll notice that on lower values of check_freq mario doesn't even know that he needs to get big inorder to jump over the first pipe.\n2. Try increasing the check_freq value after initial run, more the value better trained our mario.","63c84c92":"## For rendering and display remotely on kaggle notebook\n\nSince you are using a remote server extra code needs to be added to render mario in your notebook itself.\nhttps:\/\/stackoverflow.com\/a\/45179251","abc16456":"### Displaying sample input","0c0a9383":"### 1. lets import all the functions needed.","157b875c":"There are seven unique SIMPLE_MOVEMENT values.","a9bd422b":"Stable Baselines is a set of improved implementations of Reinforcement Learning (RL) algorithms based on OpenAI Baselines. Which is what we would be using here.","308b3914":"## IMPORTANT\n\n### If you increase the `check_freq` value you will get better result. but it takes more time. So its a question of how much time you have.\nFor quick demostration sake I am keeping it low.","c1330222":"Starting from scratch with `env.reset()`.","d0f5d0db":"Thank you for reading!\n\n![](https:\/\/encrypted-tbn0.gstatic.com\/images?q=tbn:ANd9GcQxZPsJQ5H4SXIUa5uekV-HQZyjpRMmlmmArQ&usqp=CAU)","bb7cdd2e":"# INTRODUCTION TO OPEN AI GYM\n\nOpenAi is an \"Open source interface to reinforcement learning tasks.\nThe gym library provides an easy-to-use suite of reinforcement learning tasks.\" (https:\/\/gym.openai.com\/)\n\nBasically gives you multiple environments out of the box for you to train machine learning.\n\n![](https:\/\/miro.medium.com\/max\/1318\/1*ZHISh_zLYIlJPTq_6lX5LQ.png)\n\nOpenAI Gym is a toolkit for developing and comparing reinforcement learning algorithms. It supports teaching agents everything from walking to playing games like pong or pinball. Gym is an open source interface to reinforcement learning tasks. Gym provides an environment and its is upto the developer to implement any reinforcement learning algorithms. Developers can write agent using existing numerical computation library, such as TensorFlow or Theano. (from : https:\/\/medium.com\/@ashish_fagna\/understanding-openai-gym-25c79c06eccb)","2d38ab09":"This is the sample game state world we are going to work with.","b066e9c4":"Lets make our pipeline more efficient with some quality of life changes. Lets start by installing torch, torchvision, torchaudio. (It should be already installed in kaggle but just to be sure).\n\n![](https:\/\/encrypted-tbn0.gstatic.com\/images?q=tbn:ANd9GcQyrn8kARfzWOySc97OsBa4v_dncfeNxB3GCcZA9Ufssdy7PP1--9FM8W8tMVdyLMt3k3Y&usqp=CAU)","f012e70c":"### 2. Initialising the PPO model.","a45b396b":"Lets initialise the env with added parameters.","dc7eb281":"### Mario is up and running. Yay!\n\n`Note: if you uncomment the env.close() line you'll have to initialise environment again`\n\n\n## Note on what we observe\n\nObservations\nIf we ever want to do better than take random actions at each step, it\u2019d probably be good to actually know what our actions are doing to the environment.\n\nThe environment\u2019s step function returns exactly what we need. In fact, step returns four values. These are:\n\nobservation (object): an environment-specific object representing your observation of the environment. For example, pixel data from a camera, joint angles and joint velocities of a robot, or the board state in a board game.\nreward (float): amount of reward achieved by the previous action. The scale varies between environments, but the goal is always to increase your total reward.\ndone (boolean): whether it\u2019s time to reset the environment again. Most (but not all) tasks are divided up into well-defined episodes, and done being True indicates the episode has terminated. (For example, perhaps the pole tipped too far, or you lost your last life.)\ninfo (dict): diagnostic information useful for debugging. It can sometimes be useful for learning (for example, it might contain the raw probabilities behind the environment\u2019s last state change). However, official evaluations of your agent are not allowed to use this for learning.\nThis is just an implementation of the classic \u201cagent-environment loop\u201d. Each timestep, the agent chooses an action, and the environment returns an observation and a reward.\n\n\nThe process gets started by calling reset(), which returns an initial observation. So a more proper way of writing the previous code would be to respect the done flag. (https:\/\/gym.openai.com\/docs\/#environments)","8afd3c1d":"### Note on Callbacks\nA callback is a set of functions that will be called at given stages of the training procedure. You can use callbacks to access internal state of the RL model during training. It allows one to do monitoring, auto saving, model manipulation, progress bars, ","4e4811b3":"# OUR AIM - TRAINING SUPER MARIO !\n\nTo use Reinforcent Learning to make mario automatically play the game. L-Lets go! \ud83c\udf44\n\n![](https:\/\/www.notebookcheck.net\/fileadmin\/Notebooks\/News\/_nc3\/2DD297B4_9BA0_4828_A4A2_149A8A73AF52.png)","fe34983a":"Lets create a function to render the game state in notebook.","e084691e":"## Using Stable Baselines RL algorithms","8d244ebf":"# INITIAL RUN","094c1a2e":"## Installing gym and nes py","a51f387b":"# TRAINING THE RL MODEL\n\nIts training time. \n\n![](https:\/\/giffiles.alphacoders.com\/260\/2600.gif)","1cc5b9db":"### 4. Creating a simple loop for display.","fb922ec1":"### 3. Learning","d4b8c628":"### A note on VECTORIZED ENVIRONMENT in Stable Baselines\n\nVectorized Environments are a method for stacking multiple independent environments into a single environment. Instead of training an RL agent on 1 environment per step, it allows us to train it on n environments per step. Because of this, actions passed to the environment are now a vector (of dimension n). It is the same for observations, rewards and end of episode signals (dones). In the case of non-array observation spaces such as Dict or Tuple, where different sub-spaces may have different shapes, the sub-observations are vectors (of dimension n). (https:\/\/stable-baselines.readthedocs.io\/en\/master\/guide\/vec_envs.html)","bd59c2e8":"## TEST IT OUT\n\nHere is how you test the algorithm.","0fd1c69a":"We use `GrayScaleObservation` to reduce the overhead of using color pictures while training.","6e703315":"Lets examine the number of \"observations\" that can result in the enviroment."}}