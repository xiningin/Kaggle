{"cell_type":{"6c59a27a":"code","091ae0c0":"code","c7f53b50":"code","4dd86539":"code","71820a6b":"code","d6fc8181":"code","df1fdda5":"code","d52b3c60":"code","597723de":"code","c3be1b51":"code","59aea97c":"markdown","286f3dff":"markdown","4d818b87":"markdown","7a570695":"markdown","42b959b6":"markdown","90bd5e4e":"markdown","fbca3748":"markdown","e8c4962a":"markdown","90ce881d":"markdown","4924d25d":"markdown","63546ac8":"markdown"},"source":{"6c59a27a":"# Familiar imports\nimport numpy as np\nimport pandas as pd\n\n# For ordinal encoding categorical variables, splitting data\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_squared_error\nimport matplotlib.pyplot as plt\nfrom tqdm import tqdm\nimport xgboost as xgb\nfrom hyperopt import fmin, tpe, hp, STATUS_OK, Trials\nimport warnings\nwarnings.filterwarnings('ignore')","091ae0c0":"# Load the training data\ntrain = pd.read_csv(\"..\/input\/30-days-of-ml\/train.csv\", index_col=0)\ntest = pd.read_csv(\"..\/input\/30-days-of-ml\/test.csv\", index_col=0)","c7f53b50":"# Separate target from features\ny = train['target']\nfeatures = train.drop(['target'], axis=1)","4dd86539":"#List of categorical and numerical columns.\nobject_cols = [col for col in features.columns if 'cat' in col]\nnum_cols = [col for col in features.columns if 'cont' in col]\n\n#Create training and testing sets.\nX = features.copy()\nX_test = test.copy()\n\n#Frequency Encode\nfor col in X.columns:\n    if col.startswith('cat'):\n        group = X.groupby(col).size() \/ len(X)\n        X[col] = X[col].map(group)\n        X_test[col] = X_test[col].map(group)\n        \nX['sum_cat'] =  X[object_cols].sum(axis=1)\nX_test['sum_cat'] = X_test[object_cols].sum(axis=1)\n\nX['sum_num'] = X[num_cols].sum(axis=1)\nX_test['sum_num'] = X_test[num_cols].sum(axis=1)\n\nX['sum_tot'] = X.sum(axis=1)\nX_test['sum_tot'] = X_test.sum(axis=1)","71820a6b":"#Split the data into training and validation sets.\nX_train, X_valid, y_train, y_valid = train_test_split(X, y, random_state=3, train_size=.8)","d6fc8181":"#Import 'scope' from hyperopt in order to obtain int values for certain hyperparameters.\nfrom hyperopt.pyll.base import scope\n\n#Define the space over which hyperopt will search for optimal hyperparameters.\nspace = {'max_depth': scope.int(hp.quniform(\"max_depth\", 1, 5, 1)),\n        'gamma': hp.uniform ('gamma', 0,1),\n        'reg_alpha' : hp.uniform('reg_alpha', 0,50),\n        'reg_lambda' : hp.uniform('reg_lambda', 10,100),\n        'colsample_bytree' : hp.uniform('colsample_bytree', 0,1),\n        'min_child_weight' : hp.uniform('min_child_weight', 0, 5),\n        'n_estimators': 10000,\n        'learning_rate': hp.uniform('learning_rate', 0, .15),\n        'tree_method':'gpu_hist', \n        'gpu_id': 0,\n        'random_state': 5,\n        'max_bin' : scope.int(hp.quniform('max_bin', 200, 550, 1))}","df1fdda5":"#Define the hyperopt objective.\ndef hyperparameter_tuning(space):\n    model = xgb.XGBRegressor(**space)\n    \n    #Define evaluation datasets.\n    evaluation = [(X_train, y_train), (X_valid, y_valid)]\n    \n    #Fit the model. Define evaluation sets, early_stopping_rounds, and eval_metric.\n    model.fit(X_train, y_train,\n            eval_set=evaluation, eval_metric=\"rmse\",\n            early_stopping_rounds=100,verbose=False)\n\n    #Obtain prediction and rmse score.\n    pred = model.predict(X_valid)\n    rmse = mean_squared_error(y_valid, pred, squared=False)\n    print (\"SCORE:\", rmse)\n    \n    #Specify what the loss is for each model.\n    return {'loss':rmse, 'status': STATUS_OK, 'model': model}","d52b3c60":"#Run 20 trials.\ntrials = Trials()\nbest = fmin(fn=hyperparameter_tuning,\n            space=space,\n            algo=tpe.suggest,\n            max_evals=30,\n            trials=trials)\n\nprint(best)","597723de":"#Create instace of best model.\nbest_model = trials.results[np.argmin([r['loss'] for r in \n    trials.results])]['model']\n\n#Examine model hyperparameters\nprint(best_model)","c3be1b51":"xgb_preds_best = best_model.predict(X_valid)\nxgb_score_best = mean_squared_error(y_valid, xgb_preds_best, squared=False)\nprint('RMSE_Best_Model:', xgb_score_best)\n\nxgb_standard = xgb.XGBRegressor().fit(X_train, y_train)\nstandard_score = mean_squared_error(y_valid, xgb_standard.predict(X_valid), squared=False)\nprint('RMSE_Standard_Model:', standard_score)","59aea97c":"## Examine RMSE using Optimal Hyperparameters vs Standard Hyperparameters","286f3dff":"In this next section I decided to frequency encode the categorical variables. In doing so, I grouped the variables in each categorical column and divided the length of each group by the length of the dataframe. Note, I used the mapping of the training dataset to transform the values of the testing dataset. I tried numerous other encoding methods, but frequency encoding seemed to perform the best with XGBoost using standard hyperparameters. I then added additional features to the dataset; for each observation, I added the sum of all values in the categorical columns, the sum of all values in the numerical columns, and the sum of all values in both categorical and numerical columns. This was performed for both the training and testing dataset.","4d818b87":"## Prepare the Data","7a570695":"## Search for Optimal Hyperparameters - Hyperopt","42b959b6":"## Import Libraries","90bd5e4e":"# Hyperopt and XGBRegressor - 30 Days of Machine Learning Project","fbca3748":"As you can see, our model for which we used hyperopt to identify optimal hyperparameters performed far better than the standard XGB regression model. When running hyperopt with XGBRegressor over GPU, it took only around 6 minutes to peform 30 different trails. Given how quickly one can test different combinations of hyperparameters with hyperopt, it makes sense to keep this library in your arsenal when building classification\/regression models.\n\nHope you found this kernel helpful!","e8c4962a":"## Conclusion","90ce881d":"Hi guys, for the 30 Days of Machine Learning project I experimented with various hyperparameter optimization techniques and stumbled upon Hyperopt, which performs Bayesian optimization. I found it extremely useful and wanted to share my notebook in case anyone else was interested. With the final scores, I ranked in the top 16%. Hope you find this useful! Also, if anyone notices anything done incorrectly, please do let me know!","4924d25d":"In this section I will use Hyperopt to search for the optimal hyperparameters, given the data, for the XGBRegressor estimator. It's important to note here that I ran hyperopt using a GPU on Paperspace, and it took a few hours, as I ran 400 trials. As this is an example, I will only run 30 trials; this will likely provide suboptimal hyperparameter values. In my experience, running longer trails often results in better hyperparameter values being found with regard to your objective.\n\nHere is a link to the hyperopt documentation: http:\/\/hyperopt.github.io\/hyperopt\/","63546ac8":"## Load the Data"}}