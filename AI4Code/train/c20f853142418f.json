{"cell_type":{"ceebf863":"code","07831c1b":"code","4ed6679e":"code","4effbd59":"code","e1f4086b":"code","efc6b969":"code","661985b2":"code","c27950af":"code","3591e157":"code","5684a6ad":"code","2357c751":"code","c7eaf23a":"code","7abaff1b":"code","84ed5de2":"code","053ef39a":"code","bb5f10b8":"code","83950444":"code","d39e6640":"code","10189e20":"code","e9950629":"code","090f7b10":"code","a5a4935c":"code","74a3e265":"code","c9b95552":"code","493c8ce8":"code","e667a6ab":"code","35209df7":"code","58276e72":"code","cc6899ce":"code","49351d2d":"code","e4eb53f7":"code","17164be1":"code","87ff1641":"code","65e216a3":"code","eb162008":"code","5277a586":"code","ac702371":"code","e55a6428":"code","1fea6a0e":"code","d70044d1":"code","93eaa6b9":"code","e493eabf":"code","313172a0":"code","98717cab":"code","e8f2335a":"code","bce72c7a":"code","616631b9":"code","e4ea6be7":"code","f04073ee":"code","b35e6b01":"code","55f592c0":"code","79555370":"code","8362ec53":"code","f90bbc76":"code","67680425":"code","5e618ac7":"markdown","b3308446":"markdown","5c0b57d3":"markdown","cd949143":"markdown","76a81beb":"markdown","3e94bf47":"markdown","8d5a23b7":"markdown","fe8e49f3":"markdown","8cb44c12":"markdown","09515d6e":"markdown","6fc63a9f":"markdown","e4b89f12":"markdown","152d08fc":"markdown","99dcbb87":"markdown","4e178700":"markdown","4f9bbfa3":"markdown","e79d790c":"markdown","54891f31":"markdown","f2f4b97e":"markdown","8613c093":"markdown","feb3122f":"markdown","c6e7f582":"markdown","caca9a3f":"markdown","2d3e59e3":"markdown","1991614a":"markdown","23feb7ab":"markdown","a2ad6ded":"markdown","7377659d":"markdown","cce64121":"markdown","e423ff06":"markdown","56c050f1":"markdown","c05d257b":"markdown","c7ccf612":"markdown","4ace08d8":"markdown","47c962cd":"markdown","bd2ee154":"markdown","8038176b":"markdown","0ead799a":"markdown","d30d72b2":"markdown","fec5ca1c":"markdown","274d01e1":"markdown","ccda1f61":"markdown","69db3b1a":"markdown","2e8e2d07":"markdown","34ebb6ce":"markdown","bae98f6d":"markdown","e9ec9f17":"markdown"},"source":{"ceebf863":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport re\n# For plot marker colours\nimport colorlover as cl\nfrom sklearn import preprocessing\nfrom sklearn.feature_selection import SelectKBest, f_classif, mutual_info_classif, chi2\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.preprocessing import PowerTransformer\nfrom sklearn.model_selection import cross_val_score\nfrom scipy import stats\nfrom sklearn import metrics","07831c1b":"sc = pd.read_csv(\"\/kaggle\/input\/skillcraft\/SkillCraft.csv\")\nsc.head()","4ed6679e":"print(f\"Dimension of the data set is{sc.shape}\\n\")\nprint(f\"Data Types are:\")\nprint(sc.dtypes)","4effbd59":"sc.drop(columns=[\"Age\", \"HoursPerWeek\", \"TotalHours\"], inplace=True)","e1f4086b":"print(\"Number of missing value for each feature:\")\nprint(sc.isnull().sum())","efc6b969":"sc.describe()","661985b2":"%matplotlib inline\n\n# Name the leagues\nleague_lbls = [\"Bronze\",\"Silver\",\"Gold\",\"Platinum\",\"Diamond\",\"Master\",\"Grandmaster\"]\nleague_indexs = sc[\"LeagueIndex\"].unique()\nleague_indexs.sort()\nleague_lbls_dict = dict()\nfor i, ind in enumerate(league_indexs):\n    league_lbls_dict[ind] = league_lbls[i]\nleague_labeled = sc[\"LeagueIndex\"].replace(league_lbls_dict)\n\n\ndef clrgb_to_hex(rgb):\n    rgb = re.search(\"\\(([^\\)]+)\\)\", rgb).group(1).split(\",\")\n    hex_clr = \"#\"\n    for n in rgb:\n        val = hex(int(n))[2:]\n        if len(val)<2:\n            val = \"0\"+val\n        hex_clr+=val\n    return hex_clr\n\n\n# Define league colours for consistency\nleague_colours_raw = cl.scales['8']['qual']['Paired']\nleague_colours = []\nfor i, clr in enumerate(league_colours_raw):\n    league_colours.append(clrgb_to_hex(league_colours_raw[i]))\n\nleague_colours_dict = dict()\nfor i, lbl in enumerate(league_lbls):\n    league_colours_dict[lbl] = league_colours[i]\n    \n    \ndef box_hist_plot(x, title, w, h):\n    fig, (ax_box, ax_hist)= plt.subplots(2, sharex=True,gridspec_kw={\"height_ratios\": (.15,.85)})\n    fig.set_size_inches(w, h)\n    \n    ax_box.set_xlim(0,x.max())\n    ax_hist.set_xlim(0,x.max())\n    \n    sns.boxplot(x, ax=ax_box)\n    sns.distplot(x, ax=ax_hist)\n    ax_box.set(yticks=[])\n    \n    sns.despine(ax=ax_hist)\n    sns.despine(ax=ax_box, left=True)\n    ax_box.set_title(title)\n    ax_hist.set_title(None)\n    plt.show()\n    \ndef violin_plot(y, title, w, h):\n    plt.figure(figsize=(w, h))\n    ax1 = sns.violinplot(x=league_labeled, y=y, palette=league_colours_dict, order=league_lbls)\n    ax1.set_ylim(0,)\n    ax1.set(xlabel='League')\n    plt.title(title)\n    plt.show()\n    \ndef auto_plot(feature, fig_num):\n    box_hist_plot(sc[feature], f\"Figure {fig_num}: {feature} Distribution\", 11, 8)\n    violin_plot(sc[feature], f\"Figure {fig_num+1}: {feature} by League\", 11, 8)","c27950af":"def league_dist():\n    global fig_count\n    \n    #labels\n    lab = league_lbls\n    #values: counts for each category\n    val = sc[\"LeagueIndex\"].value_counts().sort_index().values.tolist()\n    pct = [x\/sum(val)for x in val]\n    \n    fig1, ax1 = plt.subplots()\n    #ax1.pie(val, labels=lab, autopct='%1.2f%%', pctdistance=0.8,shadow=True, startangle=90)\n    \n    wedges, texts = ax1.pie(pct, wedgeprops=dict(width=0.5), startangle=90, colors=league_colours)\n    ax1.axis('equal')  # Equal aspect ratio ensures that pie is drawn as a circle.\n    \n    bbox_props = dict(boxstyle=\"square,pad=0.3\", fc=\"w\", ec=\"k\", lw=0.72)\n    kw = dict(xycoords='data', textcoords='data', arrowprops=dict(arrowstyle=\"-\"),\n          bbox=bbox_props, zorder=0, va=\"center\")\n    \n    for i, p in enumerate(wedges):\n        ang = (p.theta2 - p.theta1)\/2. + p.theta1\n        y = np.sin(np.deg2rad(ang))\n        x = np.cos(np.deg2rad(ang))\n        horizontalalignment = {-1: \"right\", 1: \"left\"}[int(np.sign(x))]\n        connectionstyle = \"angle,angleA=0,angleB={}\".format(ang)\n        kw[\"arrowprops\"].update({\"connectionstyle\": connectionstyle})\n        ax1.annotate(f\"{val[i]} {league_lbls[i]} ({pct[i]*100:.2f}%)\", xy=(x, y), xytext=(1.35*np.sign(x), 1.4*y), horizontalalignment=horizontalalignment, **kw)\n    \n    ax1.set_title(\"Figure 1: Distribution of Leagues\", y=1.2)\n    \nleague_dist()","3591e157":"auto_plot(\"APM\",2)","5684a6ad":"auto_plot(\"SelectByHotkeys\",4)","2357c751":"auto_plot(\"AssignToHotkeys\",6)","c7eaf23a":"auto_plot(\"UniqueHotkeys\",8)","7abaff1b":"auto_plot(\"MinimapAttacks\",10)\nauto_plot(\"MinimapRightClicks\",12)","84ed5de2":"auto_plot(\"NumberOfPACs\",14)\nauto_plot(\"GapBetweenPACs\",16)\nauto_plot(\"ActionLatency\",18)\nauto_plot(\"ActionsInPAC\",20)","053ef39a":"auto_plot(\"TotalMapExplored\",22)","bb5f10b8":"auto_plot(\"WorkersMade\",24)","83950444":"auto_plot(\"UniqueUnitsMade\",26)\nauto_plot(\"ComplexUnitsMade\",28)\nauto_plot(\"ComplexAbilitiesUsed\",30)","d39e6640":"leagues = sc.LeagueIndex\nsc_data = sc.drop(columns='LeagueIndex')\nleagues.value_counts()","10189e20":"# Reencode\nleagues.replace({2:1, 3:2, 4:3, 5:4, 6:5, 7:5, 8:5}, inplace=True)\n# New Value Counts\nleagues.value_counts()","e9950629":"def normalize_data(data):\n    scaler = preprocessing.MinMaxScaler()\n    data_norm = scaler.fit_transform(data)\n    \n    # When the data has been transformed, a np.array is returned,\n    # So we have to convert it back to a dataframe, and insert column names\n    return pd.DataFrame(data_norm, columns=data.columns)\n    \nsc_norm = normalize_data(sc_data)\nsc_norm.describe()","090f7b10":"def plot_scores():\n    def get_k_best(data, target, method, k):\n        skb = SelectKBest(method, k = k)\n        skb.fit(data.values, target.values)\n        fs_indices = np.argsort(skb.scores_)[::-1]\n\n        return pd.DataFrame({\"features\": data.columns[fs_indices].values, \n                      \"scores\": skb.scores_[fs_indices]})\n    \n    fig, axs = plt.subplots(ncols=3, figsize=(20,6))\n    plt.subplots_adjust(left=None, bottom=None, right=None, top=None, wspace=None, hspace=0.6)\n    \n    titles = []\n    data = []\n    titles.append(\"ANOVA F-value\")\n    data.append(get_k_best(sc_norm, leagues, f_classif, len(sc_norm.columns)))\n    titles.append(\"Mutual Information\")\n    data.append(get_k_best(sc_norm, leagues, mutual_info_classif, len(sc_norm.columns)))\n    titles.append(\"Chi-squared\")\n    data.append(get_k_best(sc_norm, leagues, chi2, len(sc_norm.columns)))\n    \n    for i in range(3):\n        p = sns.barplot(x='features', y='scores', data=data[i], ax=axs[i])\n        p.set_xticklabels(p.get_xticklabels(), rotation=90)\n        p.set_title(titles[i])\n        \n\nplot_scores()","a5a4935c":"sc_train, sc_test, leagues_train, leagues_test = train_test_split(sc_norm, leagues, \n                                                                  test_size = 0.3, random_state=1,\n                                                                  stratify = leagues)\n\nprint(f\"Training dataset shape: {sc_train.shape}\")\nprint(f\"Test dataset shape: {sc_test.shape}\")\nprint(f\"Training target shape: {leagues_train.shape}\")\nprint(f\"Test target shape: {leagues_test.shape}\")","74a3e265":"from sklearn.model_selection import RepeatedStratifiedKFold, GridSearchCV\n\ncv = RepeatedStratifiedKFold(n_splits=5, n_repeats=3, random_state=1)","c9b95552":"def run_KNN_pipe(n_neighbours, p):\n    pipe_KNN = Pipeline([('selector', SelectKBest()), \n                         ('knn', KNeighborsClassifier())])\n\n    params_pipe_KNN = {'selector__score_func': [f_classif, mutual_info_classif, chi2],\n                       'selector__k': [3, 4, 5, 6, 7, 10, sc_norm.shape[1]],\n                       'knn__n_neighbors': n_neighbours,\n                       'knn__p': p}\n\n    gs_pipe_KNN = GridSearchCV(estimator=pipe_KNN, \n                               param_grid=params_pipe_KNN, \n                               cv=cv,\n                               n_jobs = -1,\n                               scoring='accuracy',\n                               verbose=0)\n\n    gs_pipe_KNN.fit(sc_train, leagues_train);\n    \n    return gs_pipe_KNN\n\n\ngs_pipe_KNN = run_KNN_pipe([150, 160 ,170 ,180 ,190, 200], [1,2,5])","493c8ce8":"gs_pipe_KNN.best_params_","e667a6ab":"gs_pipe_KNN.best_score_","35209df7":"# custom function to format the search results as a Pandas data frame\ndef get_search_results(gs):\n\n    def model_result(scores, params):\n        scores = {'mean_score': np.mean(scores),\n             'std_score': np.std(scores),\n             'min_score': np.min(scores),\n             'max_score': np.max(scores)}\n        return pd.Series({**params,**scores})\n\n    models = []\n    scores = []\n\n    for i in range(gs.n_splits_):\n        key = f\"split{i}_test_score\"\n        r = gs.cv_results_[key]        \n        scores.append(r.reshape(-1,1))\n\n    all_scores = np.hstack(scores)\n    for p, s in zip(gs.cv_results_['params'], all_scores):\n        models.append((model_result(s, p)))\n\n    pipe_results = pd.concat(models, axis=1).T.sort_values(['mean_score'], ascending=False)\n\n    columns_first = ['mean_score', 'std_score', 'max_score', 'min_score']\n    columns = columns_first + [c for c in pipe_results.columns if c not in columns_first]\n\n    return pipe_results[columns]\n\nresults_KNN = get_search_results(gs_pipe_KNN)\nresults_KNN.head(5)","58276e72":"def plot_KNN_results(res):\n    def get_selector_data(d, p, sel):\n        return d[(d.knn__p==p) & (d.selector__score_func==sel)].iloc[:,[0,4,6]]\n    \n    rows = len(res[\"knn__p\"].unique())\n    \n    fig, axs = plt.subplots(ncols=3, nrows=rows, figsize=(20,rows*6), sharey='all')\n    plt.subplots_adjust(left=None, bottom=None, right=None, top=None, wspace=None, hspace=None)\n    \n    titles = []\n    data = []\n    temp = res.copy().infer_objects()\n    for i, p in enumerate(res[\"knn__p\"].unique()):\n        titles.append(f\"ANOVA F-value, p={p}\")\n        data.append(get_selector_data(temp, p, f_classif))\n        titles.append(f\"Mutual Information, p={p}\")\n        data.append(get_selector_data(temp, p, mutual_info_classif))\n        titles.append(f\"Chi-squared, p={p}\")\n        data.append(get_selector_data(temp, p, chi2))\n    \n    row = 0\n    col = 0\n    ax = None\n    for i in range(rows*3):\n        if col%3==0 and col!=0:\n            col=0\n            row+=1\n        if rows == 1:\n            ax = axs[col]\n        else:\n            ax=axs[row,col]\n        p = sns.lineplot(x='knn__n_neighbors', \n                         y='mean_score', \n                         hue='selector__k', \n                         data=data[i], \n                         ax=ax, \n                         palette=sns.color_palette(\"Set1\", 7))\n        p.set_title(titles[i])\n        p.legend(loc='lower right')\n        col+=1\n        \nplot_KNN_results(results_KNN)","cc6899ce":"gs_pipe_KNN2 = run_KNN_pipe([210, 220, 230, 240, 250, 260, 270, 280, 290, 300, 310, 320, 330, 340, 350], [1])","49351d2d":"gs_pipe_KNN2.best_params_","e4eb53f7":"gs_pipe_KNN2.best_score_","17164be1":"results_KNN2 = get_search_results(gs_pipe_KNN2)\nresults_KNN2.head(5)","87ff1641":"plot_KNN_results(results_KNN2)","65e216a3":"def run_dt_pipe(max_depth, min_split):\n    pipe_DT = Pipeline([('selector', SelectKBest()), \n                         ('dt', DecisionTreeClassifier(criterion='gini'))])\n\n    params_pipe_DT = {'selector__score_func': [f_classif, mutual_info_classif, chi2],\n                       'selector__k': [3, 4, 5, 6, 7, 10, sc_norm.shape[1]],\n                       'dt__max_depth': max_depth,\n                       'dt__min_samples_split': min_split}\n \n    gs_pipe_DT = GridSearchCV(estimator=pipe_DT, \n                               param_grid=params_pipe_DT, \n                               cv=cv,\n                               n_jobs = -1,\n                               scoring='accuracy',\n                               verbose=0)\n\n    gs_pipe_DT.fit(sc_train, leagues_train);\n    \n    return gs_pipe_DT\n\n\ngs_pipe_DT = run_dt_pipe([5, 7, 9], [2, 3, 5, 7, 9, 11])","eb162008":"gs_pipe_DT.best_params_","5277a586":"gs_pipe_DT.best_score_","ac702371":"results_DT = get_search_results(gs_pipe_DT)\nresults_DT.head(5)","e55a6428":"def plot_DT_results(res):\n    def get_selector_data(d, p, sel):\n        return d[(d.dt__max_depth==p) & (d.selector__score_func==sel)].iloc[:,[0,5,6]]\n    \n    rows = len(res[\"dt__max_depth\"].unique())\n    \n    fig, axs = plt.subplots(ncols=3, nrows=rows, figsize=(20,rows*6), sharey='all')\n    plt.subplots_adjust(left=None, bottom=None, right=None, top=None, wspace=None, hspace=None)\n    \n    titles = []\n    data = []\n    temp = res.copy().infer_objects()\n    for i, p in enumerate(res[\"dt__max_depth\"].unique()):\n        titles.append(f\"ANOVA F-value, max_depth={p}\")\n        data.append(get_selector_data(temp, p, f_classif))\n        titles.append(f\"Mutual Information, max_depth={p}\")\n        data.append(get_selector_data(temp, p, mutual_info_classif))\n        titles.append(f\"Chi-squared, max_depth={p}\")\n        data.append(get_selector_data(temp, p, chi2))\n    \n    row = 0\n    col = 0\n    ax = None\n    for i in range(rows*3):\n        if col%3==0 and col!=0:\n            col=0\n            row+=1\n        if rows == 1:\n            ax = axs[col]\n        else:\n            ax=axs[row,col]\n        p = sns.lineplot(x='dt__min_samples_split', \n                         y='mean_score', \n                         hue='selector__k', \n                         data=data[i], \n                         ax=ax, \n                         palette=sns.color_palette(\"Set1\", 7))\n        p.set_title(titles[i])\n        p.legend(loc='lower right')\n        col+=1\n\n        \nplot_DT_results(results_DT)","1fea6a0e":"def compare_depths():\n    def get_selector_data(d, p, sel):\n        return d[(d.dt__min_samples_split==p) & (d.selector__score_func==sel)].iloc[:,[0,4,6]]\n    \n    pipe_DT = Pipeline([('selector', SelectKBest()), \n                         ('dt', DecisionTreeClassifier(criterion='gini'))])\n\n    params_pipe_DT = {'selector__score_func': [f_classif],\n                       'selector__k': [3, 4, 5],\n                       'dt__max_depth': [2,3,4,5,6,7,8,9,10,11,12,13,14,15,16],\n                       'dt__min_samples_split': [2]}\n \n    gs_pipe_DT = GridSearchCV(estimator=pipe_DT, \n                               param_grid=params_pipe_DT, \n                               cv=cv,\n                               n_jobs = -1,\n                               scoring='accuracy',\n                               verbose=0)\n\n    res = get_search_results(gs_pipe_DT.fit(sc_train, leagues_train));\n    res = res.infer_objects()\n    p = sns.lineplot(x='dt__max_depth', \n                     y='mean_score', \n                     hue='selector__k', \n                     data=get_selector_data(res,2,f_classif), \n                     palette=sns.color_palette(\"Set1\", 3))\n    #p.set_title(titles[i])\n    p.legend(loc='lower right')\n    \ncompare_depths()","d70044d1":"gs_pipe_DT2 = run_dt_pipe([5], [11, 21, 31, 41, 51, 61, 71, 81, 91, 101, 111, 121, 131, 141, 151, 161, 171, 181, 191, 201])","93eaa6b9":"gs_pipe_DT2.best_params_","e493eabf":"gs_pipe_DT2.best_score_","313172a0":"results_DT2 = get_search_results(gs_pipe_DT2)\nresults_DT2.head(5)","98717cab":"plot_DT_results(results_DT2)","e8f2335a":"np.random.seed(1)\n\ndef run_NB_pipe(var_smoothing):\n    pipe_NB = Pipeline([('selector', SelectKBest()), \n                         ('nb', GaussianNB())])\n\n    params_pipe_NB = {'selector__score_func': [f_classif, mutual_info_classif],\n                       'selector__k': [3, 4, 5, 6, 7, 10, sc_norm.shape[1]],\n                       'nb__var_smoothing': var_smoothing}\n\n    gs_pipe_NB = GridSearchCV(estimator=pipe_NB, \n                               param_grid=params_pipe_NB, \n                               cv=cv,\n                               n_jobs = -1,\n                               scoring='accuracy',\n                               verbose=0)\n    \n    sc_train_transformed = PowerTransformer().fit_transform(sc_train)\n    gs_pipe_NB.fit(sc_train_transformed, leagues_train);\n    \n    return gs_pipe_NB\n\n\ngs_pipe_NB = run_NB_pipe(np.logspace(2,-2, num=100))","bce72c7a":"gs_pipe_NB.best_params_","616631b9":"gs_pipe_NB.best_score_","e4ea6be7":"results_NB = get_search_results(gs_pipe_NB)\nresults_NB.head(5)","f04073ee":"def plot_NB_results(res):\n    def get_selector_data(d, sel):\n        return d[(d.selector__score_func==sel)].iloc[:,[0,4,5]]\n    \n    fig, axs = plt.subplots(ncols=2, figsize=(20,6), sharey='all')\n    plt.subplots_adjust(left=None, bottom=None, right=None, top=None, wspace=None, hspace=None)\n    \n    \n    titles = []\n    data = []\n    temp = res.copy().infer_objects()\n    \n    titles.append(f\"ANOVA F-value\")\n    data.append(get_selector_data(temp, f_classif))\n    titles.append(f\"Mutual Information\")\n    data.append(get_selector_data(temp, mutual_info_classif))\n    \n    for i in range(2):\n        p = sns.lineplot(x='nb__var_smoothing', \n                         y='mean_score', \n                         hue='selector__k', \n                         data=data[i], \n                         ax=axs[i], \n                         palette=sns.color_palette(\"Set1\", 7))\n        p.set_xscale(\"log\")\n        p.set_title(titles[i])\n        p.legend(loc='lower right')\n\n\nplot_NB_results(results_NB)","b35e6b01":"cv2 = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=2)\n\ncv_results_KNN = cross_val_score(estimator=gs_pipe_KNN2.best_estimator_,\n                                 X=sc_test,\n                                 y=leagues_test, \n                                 cv=cv2, \n                                 n_jobs=-1,\n                                 scoring='accuracy')\ncv_results_KNN.mean()","55f592c0":"cv_results_DT = cross_val_score(estimator=gs_pipe_DT2.best_estimator_,\n                                X=sc_test,\n                                y=leagues_test, \n                                cv=cv2, \n                                n_jobs=-1,\n                                scoring='accuracy')\ncv_results_DT.mean()","79555370":"sc_test_transformed = PowerTransformer().fit_transform(sc_test)\n\ncv_results_NB = cross_val_score(estimator=gs_pipe_NB.best_estimator_,\n                                X=sc_test_transformed,\n                                y=leagues_test, \n                                cv=cv2, \n                                n_jobs=-1,\n                                scoring='accuracy')\ncv_results_NB.mean()","8362ec53":"print(stats.ttest_rel(cv_results_KNN, cv_results_DT))\nprint(stats.ttest_rel(cv_results_KNN, cv_results_NB))\nprint(stats.ttest_rel(cv_results_DT, cv_results_NB))","f90bbc76":"pred_KNN = gs_pipe_KNN.predict(sc_test)\n\npred_DT = gs_pipe_DT2.predict(sc_test)\n\nsc_test_transformed = PowerTransformer().fit_transform(sc_test)\npred_NB = gs_pipe_NB.predict(sc_test_transformed)\n\nprint(\"\\nK-Nearest Neighbour Report\") \nprint(metrics.classification_report(leagues_test, pred_KNN))\nprint(\"\\nDecision Tree Report\") \nprint(metrics.classification_report(leagues_test, pred_DT))\nprint(\"\\nNaive Bayes Report\") \nprint(metrics.classification_report(leagues_test, pred_NB))","67680425":"print(\"\\nConfusion matrix for K-Nearest Neighbour\") \nprint(metrics.confusion_matrix(leagues_test, pred_KNN))\nprint(\"\\nConfusion matrix for Decision Tree\") \nprint(metrics.confusion_matrix(leagues_test, pred_DT))\nprint(\"\\nConfusion matrix for Naive Bayes\") \nprint(metrics.confusion_matrix(leagues_test, pred_NB))","5e618ac7":"## Fine Tuning DT\n\nThere is a very clear downward trend for the mean score as the max depth increases. Thus the next step would be to rerun the model with depths that are less than 5. After that, we will also increase the values that are being checked for min_samples_split to check for better values.","b3308446":"This feature does not seem to provide much information regarding the target feature. This is also quite unreliable as the amount of the map that is explored by a player can vary wildly depending on the design of the map, was well as the type of strategy the player employs. \n\n\n### WorkersMade","5c0b57d3":"Similar to before, the top 3 features were selected by the Chi-squared test and is used in conjunction with max_depth=5 and min_samples_split=181 to obtain a mean accuracy score of 0.4187. \n\nThis is a minor improvement when compared with the best mean score of 0.4174 from the last run. The graphs above show that it is unlikely we will find a better value by increasing the value of min_samples_split as there is a steep decline at around 161.","cd949143":"Because the data was collected through online gaming communities and social media, it is not surprising that the proportion of data available for Bronze and Silver league games are very small compared to the ladder distribution at the time, whereas Master league games exceed its ladder proportion by a large margin. Players in higher leagues tend to be more involved in the community, keeping up with the latest strategies and discussions. On the other hand, lower league players are likely new or casual players, logging in just to play the game for fun without necessarily engaging with the community outside of the game client. It also appears that this dataset does not contain professional players.\n\n\n## Data Visualization\n\n\nAll of the features we have are numerical, the auto_plot function is applied to all of them. This function produces a plot of the feature on its own, as well as a plot split by the target feature.\n\n\n### APM","76a81beb":"From the above summary, the values for the data seem reasonable. ","3e94bf47":"From the reports and confusion matrix, NB and KNN are very closely matched for the highest averages for precision, recall and accuracy. However, upon closer inspection NB and KNN has a very wide range in all its scores such as going from a recall of 0.61 in class 1 to 0.14 in class 2, whereas DT is the most balanced. The confusion matrix does align with these findings.\n\nGiven this information, I would say that DT could possibly perform the best in practice.","8d5a23b7":"# Simple Preprocessing <a name=\"data_prepro\"><\/a>\n\n## Data Import","fe8e49f3":"Thus we will condense the number of classifications in the target feature from 8 to 5 to balance out the populations.","8cb44c12":"## Model Evaluation Strategy\n\nOur model will be trained and tuned on 2336 rows of data, and tested on 102 rows of data as seen in the previous section.\n\nFor each model, we will use 5-fold repeated stratified cross-validation evaluation method (with 3 repeats) for hyperparameter tuning.","09515d6e":"## Decision Trees (DT)\n\nHere we will build a decision tree that uses the Gini index to split the data. We will use GridSearchCV to determine the optimal values for the maximum depth and minimum sample split.\n\nDT hyperparameters include:\n\n* The maximum depth of the DT, where we will initially consider the values 5, 7 and 9.\n\n* The minimum number of samples required to be at a leaf node, where we will initially consider the values 2, 3, 5, 7, 9, 11.","6fc63a9f":"As there doesn't seem to be any pattern pointing to values of var_smoothing improving the mean score, there won't be any fine tuning done for this model.","e4b89f12":"### KNN Fine Tuning\n\nWe can see in the graphs that many of the lines are trending upward as we increase our n value, so we will rerun the pipeline with higher values to compare. Because p=1 is the best performing value for p in general, we will stick with that for this new run.","152d08fc":"Similar to WorkersMade, it will be very hard to differentiate players by using these features. \n\nDepending on the strategy, a player can just win by making as little as 1 unit type, often having a very lean, specialized army can make greater impact than a large one with a lot of different units. Complex units as well as their abilities are accessible to any player with enough time and resources, but could still be used to differentiate weak players from average or better players.","99dcbb87":"## Naive Bayes (NB)\n\nHere we will build a Gaussian Naive Bayes model and optimize the value of var_smoothing. We will perform a logspace search for the optimal var_smoothing, starting with the 100 to $10^{-2}$ with a total of 100 values. There is no prior information available to provide this model.\n\nEach descriptive feature must follow a Gaussian distribution in a NB model, a power transformation is first performed on the input data.\n\nBecause the Chi-sqaured test does not work with negative values, it will be excluded from the pipeline of this section.","4e178700":"# Introduction\n\n## Objective\n\n\nStarcraft II is an incomplete information multiplayer real-time strategy game that centers around gathering resources to build units and defeat an opponent. Players of this game generally play in an online ladder that places players into different leagues based on their performance against other players using a ranking system similar to ELO in chess. Unlike in chess, players must race against their opponent to execute their strategy, while at the same time gathering information on their opponent and defending their attacks.\n\nWhen joining the current ladder system, players are required to play 10 placement matches in order to find their leagues. However, this tends to under estimate their skills and takes many games for players to arrive at the correct placement. During this adjustment period, it could give other players who are correctly placed bad experiences by being completely outmatched.\n\nThe objective of this project is to predict the league placement of a Starcraft II player by only using the information contained inside a replay. The real-time nature of the game allows us to differentiate players through the speed and efficiency of their in game actions. This is in an attepmt to  reduce the time taken to arrive at the correct placement.\n\nThis report is organised as follows:\n\n* [Section 2 (Overview)](#overview) outlines our methodology. \n\n* [Section 3 (Simple Preprocessing)](#data_prepro) makes sure the data is clean and what we expect.\n\n* [Section 4 (Data Exploration)](#explore) explores and visualizes the data. \n\n* [Section 5 (Data Preparation)](#data_prep) summarizes the data preparation process and our model evaluation strategy.\n\n* [Section 6 (Hyperparameter Tuning)](#param_tuning) describes the hyperparameter tuning process for each classification algorithm. \n\n* [Section 7 (Performance Comparison)](#comparison) presents model performance comparison results.\n\n* [Section 8 (Limitations)](#limitations) discusses a limitations of our approach and possible solutions. \n\n* [Section 9 (Summary)](#summary) provides a brief summary of our work in this project.\n\n\nCompiled from Jupyter Notebook, this report contains both narratives and the Python codes used for data processing, model buiding and evaluation.\n\n\n# Overview <a name=\"overview\"><\/a>\n\n## Target Feature\n\nThe target feature is the league index which is numbers 1-8 representing Bronze, Silver, Gold, Platinum, Diamond, Master, Grandmaster, Professional leagues. However, it was found that we have very small proportions of Bronze, Grandmaster and Professional league players in the dataset. In order to prevent this imbalance from impacting the predictive performance of our models, these leagues will be rolled into the leagues closest to them, resulting in the following index:\n\n1. Bronze-Silver\n\n2. Gold\n\n3. Platinum\n\n4. Diamond\n\n5. Master-Grandmaster-Professional\n\n\nThe objective being to accurately place a player into one of these leagues just by using data that can be automatically mined from a replay.\n\n## Packages Used","4f9bbfa3":"The top 7 features were selected by the Chi-squared test and is used with a maximum depth of 5 and minimum split value of 2. The output shows that this optimal DT model has a mean accuracy score of 0.4174. ","e79d790c":"# Data Preparation <a name=\"data_prep\"><\/a>\n\n## Reencoding the Target Feature\n\nAs discussed in the overview, the target feature in the dataset has some fairly imbalanced populations. To remedy this, first we will separate our target variable from our dataset and merge them with they larger neighbours. Below shows the current population of each league in our dataset. Below shows the current population of each league in our dataset.","54891f31":"## Data Cleaning\n\nFor this step, the feature types are checked against the documentation.","f2f4b97e":"Because the goal of this project is to make predictions based on the data mined from submitted replays, the Age, HoursPerWeek and TotalHours columns are not important and will not be a feature used in predictions. Thus I shall remove these columns:","8613c093":"## Normalizing Features\n\nBecause the data contains values from very different ranges, it is important to make sure our features are in the same scale.","feb3122f":"In figure 2, the mean of Action Per Minutes (APM) is about 100, which is what I would expect from an average player. The positive skew generally comes from 2 sources:\n\n1. Good players being able to perform much more effective actions than the average player.\n\n2. A player repeatedly performing the same actions in quick succession that do not contribute to the outcome of the game.\n\n\nPoint number 2 can be a source of noise which lowers the reliability of this feature. However the proportion of players that would play like this is likely to be small as it takes a lot of effort while potentially negatively impacting the players ability to play the game.\n\n\nIn figure 3, there is an upward trend in APM as the league increases. However, there is fair bit of overlap between the leagues.\n\n\n### SelectByHotkeys","c6e7f582":"With a maximum of 10 assignable hotkeys, figure 8 shows a fairly reasonable distribution of unique hotkey use with a mean of 4. Using more than 4 or 5 unique hotkeys takes practice and muscle memory as the movements require the player to move their hand across the keyboard using the number keys 1 to 0 without looking down in combination with other modifier keys to maximize efficiency.\n\n\nThere is actually an interesting step up of 1 unique hotkey used per timestamp per every 2 leagues in figure 9. But this also makes sense remembering the discussion in the section on SelectByHotkeys.\n\n\n### MinimapAttacks, MinimapRightClicks","caca9a3f":"## Methodology\n\nWe buill build the following classifiers to predict the target feature:\n\n* K-Nearest Neighbours (KNN)\n\n* Decision Tree (DT)\n\n* Naive Bayes (NB)\n\nFirst, the dataset from phase 1 will be transformed to suit our analysis. This includes reencoding the target feature to balance out its populations, as well as dropping unnecessary features and scaling the descriptive features. \n\nThe data will be randomly split with stratificationinto training and test sets with a 70:30 ratio. This results in a training set with 2336 rows and a test set with 1002 rows. Stratification is necessary to ensure that each validation set has the same proportion of target classes as in the original dataset because the population is imbalanced.\n\nAs part of the model fitting pipeline, we will be using ANOVA F-value, Mutual Information and Chi-squared methods to select the top 3, 4, 5, 6, 7, 10 or 15 (full set) of features.\n\nWe will conduct a 5-fold repeated stratified cross-validation (with 3 repeats) to fine-tune hyperparameters of each classifier using Accuracy as the performance metric. This will be done in a single pipeline along with feature selection for each model with parallel processing using \"-2\" cores. \n\nOnce the best model of the three has been identified, using hyperparameter search on the training data, we conduct a 10-fold repeated (3 repeats) cross-validation on the test data and perform a paired t-test to see if any performance difference is statistically significant. In addition, we compare the classifiers with respect to their precision scores, recall scores and confusion matrices on the test data.","2d3e59e3":"I shall also perform a surface level check for NaN values.","1991614a":"# Limitations and Proposed Solutions<a name=\"limitations\"><\/a>\n\nOur dataset currently only has 3395 rows of data, which was when it was collected, a very small proportion of the existing player base. This also resulted in the imbalance in the spread of leagues in the data that caused us to merge several leagues together. Additionally, these are gathered from voluntarily submitted replays, which could make the model biased towards being accurate in predicting placements for active members of the community, who likely have more strategic knowledge, rather than the general population. These could possibly be solved by increasing the number of submitted replays from the community. Professional replays can be obtained when tournaments release their replay packs. We can also try to work with the developer Blizzard Entertainment to allow all players to opt in to automatic replay submissions for analysis after a ladder match.\n\nThe features found in the data alone are likely not sufficient to very accurately predict a player's league placement. They are very focused on the physical interactions the player has with the game, on top of that, there is a fair bit of overlap between the leagues. A core aspect of the game is resource management for which there is a very clear difference between skilled and unskilled players as well. Additional features such as average income, average savings, spending habits (such as use of production queueing) and production building activity counters could help paint a more well rounded picture of the players in game.\n\nWhen creating the models, we chose to use pipelines to try multiple feature selectors and many hyperparameter combinations, and then refining the model after. While it seems to have been effective to a degree, we could potentially improve our predictive power by using in depth analysis to manually select features and hyperparameter values.","23feb7ab":"Looking at these results, we conclude that at a 95% significance level, NB is statistically the best model in terms of accuracy when compared on the test data.\n\nWe will now compare the models using other measures such as:\n\n* Precision\n\n* Recall\n    \n* F1 Score (the harmonic average of precision and recall)\n\n* Confusion Matrix","a2ad6ded":"We will plot these expected results to get an understanding of how changing the hyperparameters affect the prediction accuracy.","7377659d":"## Target Feature Distribution\n\n\nAround the time when this data was collected(2011), the targeted ratio of ladder players were as follows:\n\n* Grandmaster Top 200 players in a region\n\n* Master 2%\n\n* Diamond 18%\n\n* Platinum 20%\n\n* Gold 20%\n\n* Silver 20%\n\n* Bronze 20%\n\n\nGenerally professional players are also Grandmasters in their respective regions, however there is a wide enough skill gap between Grandmasters who are and aren't professionals that the distinction matters.","cce64121":"# Performance Comparison <a name=\"comparison\"><\/a>\n\nNow that we have created our optimized models for each classifier, we will use them on the test data with 10 fold repeated cross-validation. Due to the randomness of the cross-validation process, any differences in performance of the optimized models will be put through pairwise t-tests to determine if they are statistically significant.\n\nFirst, we will calculate the cross-validation scores for each of our models.","e423ff06":"Because all the data in the table currently has numerical values, I do a quick summary to check for abnormalities.","56c050f1":"We can see from the above plot that the top 5 spots are generally taken up by Action Latency, APM, NumberOfPACs, SelectByHotkeys and GapBetweenPACs. The fetures that are ranked lower tend to move around depending on the selection method, which will make a difference when not all features are used to train the model.","c05d257b":"This time around, the top 10 features were selected by the Mutual Info (and ANOVA F-value) method and is used by our KNN model to find its 240 nearest neighbours at a p of 1 with a mean accuracy score of 0.4443. \n\nThis is a minor improvement when compared with the best mean score of 0.4408 from the last run. The graphs above show that many of the lines have either levelled out or begun to decline.","c7ccf612":"# Summary<a name=\"summary\"><\/a>\n\nThe F-value selector selected the 7 best features which were used to create a Decision Tree (DT) model that obtained the highest cross-validated Accuracy score on the training data. Although on average the DT model model on average does not outperform the Naive Bayes and k-Nearest Neighbours (KNN) models, it is the most balanced model with the smallest range for all of its classification measures.\n\nWith a mean training score of 0.4187 and 0.4034 on the test data, the model's performance is not good. However, it does provide better chance of success than purely guessing placement in 5 categories.","4ace08d8":"The top 7 features were selected by the ANOVA F-value test and is used by our KNN model to find its 170 nearest neighbours at a p of 1. The output shows that this optimal KNN model has a mean accuracy score of 0.4408. \n\nTo better gauge how much and why this optimal set of hyperparameters performs better compared its peers, the results should be formatted into a more readable state. Below is a function provided by the teaching staff of the RMIT course for Machine Learning:","47c962cd":"The top 10 features were selected by the Mutual Info selector and using var_smoothing=0.16297508346206435, our model obtains a mean score of 0.4603.","bd2ee154":"Figure 11 shows that minimap attacks are very few and far between in a game, and in a lot of cases players don't even use it. There are many ways to perform attacks and the usage of the minimap can be a stylistic choice for a lot of players. However, we do still see a trend upward in use as the league improves, as players multitask more and improve the efficiency of their actions by using all the tools available to them.\n\nThe logic behind this MinimapRightClicks is similar to that of MinimapAttacks, however this is used more in general likely because the action is easier to perform.\n\n\n### NumberOfPACs, GapBetweenPACs, ActionLatency, ActionsInPAC","8038176b":"# Hyperparameter Tuning <a name=\"param_tuning\"><\/a>\n\n## K-Nearest Neighbours (KNN)\n\nHere we will stack feature selection and grid search for hyperparameter tuning (via cross-validation) in a \"pipeline\". The same Pipieline methodology will be used for NB and DT. Our pipeline will include multiple selectors, which were discussed in the Feature Selection and Ranking section.\n\nKNN hyperparameters include:\n\n* The number of neighbours, where we will initially consider n of 150, 160 ,170 ,180 ,190, and 200.\n\n* The distance metric p, where we will initially consider p values of 1 (Manhattan), 2 (Euclidean), and 5 (Minkowski)","0ead799a":"These 3 features essentially show us the performance values for each stage in a PAC. \n\n\nSimilar to other activities which require fast reaction time, the more highly ranked players perceive events and take action more quickly. NumberOfPACs, GapBetweenPACs and ActionLatency improve significantly as the league improves. These feature essentially quantifies the players ability to multitask. ActionsInPAC don't change very much between leagues. When NumberOfPACs are taken into account, ActionsInPAC don't matter as much as long as players have more PACs and those actions are efficiently used.\n\n\n### TotalMapExplored","d30d72b2":"## Feature Selection and Ranking\n\nHere we will use ANOVA F-value, Mutual Information and Chi-squared scoring on the full dataset to rank and select features. ","fec5ca1c":"Second, we conduct a paired t-test for the accuracy score between the following model combinations:\n\n* KNN vs. DT\n\n* KNN vs. NB\n\n* DT vs. NB","274d01e1":"Figures 6 and 7 show that the assignments of hotkeys are generally more balanced between the leagues, however this also tells us that proportionally many people are aware of hotkeys, and go through the setup process in game, but then do not use actually use them as much for selection.\n\n\n### UniqueHotkeys","ccda1f61":"In figure 4, a large proportion of players don't use selection by hotkeys very much, as it is likely more intuitive to select units on the screen using a mouse.\n\n\nAs games get more competitive, efficiency in actions become more important. Selecting by hotkeys while requiring continous maintenance throughout the game, is much faster and allows the player to not only select units anywhere on the map, but also allows instantaneous movement of the player camera to show their selected units. Unsurprisingly, hotkey usage goes up as the league improves, with a relatively large spike in professional games in figure 5. \n\n\n### AssignToHotkeys","69db3b1a":"We can see that the value 5 for maximum depth is clearly the optimal one.\n\nNext, we will try many different values for minimum sample splitting threshold. We are able to do so many as locking down the depth frees up the computing time.","2e8e2d07":"## Train-Test Splitting\n\nBecause this dataset only has 3395 rows, there is no need for sampling. However, it is still necessary to split the dataset into train and test partitions. The partitions will be created using stratification at a 70:30 train to test ratio.","34ebb6ce":"As we did before, we will plot these expected results to get an understanding of how changing the hyperparameters affect the prediction accuracy.","bae98f6d":"# Data Exploration <a name=\"explore\"><\/a>\n\nFor convenience and consistency, I defined a few variables the ensure the correct league labels and colours. The league labels will be used instead of LeagueIndex for the visualizations. Additionally, functions for creating for univariate box plot paired with histogram as well as multivariate violin plots was created.","e9ec9f17":"This feature also does not seem to provide much information regarding the target feature. It is heavily dependent on type of strategies the both players employs as well as game length. There is a balance that must be struck between using resources to create workers or improving the players army, often once a certain number of workers are created, no more will be made for the rest of the game. It can be argued that this can be used to differentiate very weak players from average or better players, but I doubt it will be very helpful in the context of this notebook.\n\n\n### UniqueUnitsMade, ComplexUnitsMade, ComplexAbilitiesUsed"}}