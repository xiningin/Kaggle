{"cell_type":{"4ad82595":"code","43c58766":"code","b02765e7":"code","25907908":"code","c79ef859":"code","392e6d71":"code","758ea51c":"code","126aa1d6":"code","5e912f29":"code","3e7d8e89":"code","7a49e35d":"code","f62d4468":"code","fda05942":"code","1094129d":"code","a81d5133":"code","0a47fc9e":"code","a9c5bdd3":"markdown"},"source":{"4ad82595":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\n%matplotlib inline\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","43c58766":"df = pd.read_csv('..\/input\/Iris.csv')\ndf['Species'].loc[101:102]","b02765e7":"#See info about dataset\ndf.info()","25907908":"# See how many classes are available totally in dataset\ndf['Species'].unique()","c79ef859":"from sklearn.preprocessing import LabelEncoder\n\n#Encode text classes to numeric classes\nencoder = LabelEncoder().fit(df['Species'])","392e6d71":"# Get hot encoded numeric classes from text classes\nhot_encoded_labels = pd.DataFrame(encoder.transform(df['Species']))","758ea51c":"# Assign required features to features dataframe\nfeatures = df.drop(['Species','Id'], axis=1)","126aa1d6":"features.head()","5e912f29":"# Draw scatter_matrix to to see relevance of features\npd.plotting.scatter_matrix(features, figsize=(10,10))\nx = 1","3e7d8e89":"# Applying unsupervised learning techniques on features\nfrom sklearn.cluster import KMeans\nfrom sklearn.mixture import GaussianMixture","7a49e35d":"# Create two models KMeans and GaussianMixture\nclf = KMeans(n_clusters=3).fit(features)\ngclf = GaussianMixture(n_components=3).fit(features)","f62d4468":"# Predict features i.e.,predict inputs based on formed clusters\npreds = clf.predict(features)\ngpreds = gclf.predict(features)","fda05942":"def draw_clusters(features, preds):\n    fig, axis = plt.subplots(2,2,figsize=(10,8))\n    fea_matrix=  [('SepalLengthCm','SepalWidthCm'), ('PetalLengthCm', 'PetalWidthCm'),\\\n                 ('SepalLengthCm','PetalWidthCm'), ('PetalLengthCm','SepalWidthCm')]\n\n    for i,row in enumerate(axis):\n        for j,col in enumerate(row):\n            x,y = fea_matrix[i+j]\n            col.set_xlabel(x)\n            col.set_ylabel(y)\n            col.scatter(features[x],features[y], alpha=0.5,  c=preds,cmap='viridis',s=20 )\n    plt.show()","1094129d":"#Clusters formed based on KMeans model\ndraw_clusters(features, preds)","a81d5133":"#Clusters formed based on GaussianMixture Model\ndraw_clusters(features, gpreds)","0a47fc9e":"#Clusters of actual output labels.\ndraw_clusters(features, list(hot_encoded_labels[0]))","a9c5bdd3":">  Hence both KMeans and GaussianMixture Model are performing best on this dataset, as we can see the graphs."}}