{"cell_type":{"253aedac":"code","f06abbfe":"code","226dd05f":"code","a0999641":"code","2e6e438a":"code","879890d2":"code","51a194b4":"code","44608cd9":"code","774c9be2":"code","b9f8a505":"code","fd93e9cf":"code","93972213":"markdown","990af91f":"markdown","ea7c8d30":"markdown","5bc05d94":"markdown","1b50d712":"markdown","dbb6dd33":"markdown"},"source":{"253aedac":"import pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport numpy as np\nfrom sklearn.tree import DecisionTreeRegressor #for decision tree\nfrom sklearn.naive_bayes import GaussianNB     #for naive bayes\nimport warnings\nwarnings.filterwarnings('ignore')\nimport os\nprint(os.listdir(\"..\/input\"))\n","f06abbfe":"df = pd.read_csv('..\/input\/melb_data.csv')\nprint(df.iloc[3])\nprint(df.isnull().sum())","226dd05f":"features = ['Rooms','Distance','Bedroom2','Bathroom','Car','Landsize','Price']\ndf = df[features]        #we are throw away the nan values \ndf = df.dropna(axis=0)   #cause alghoritms not working when you have nan value \n                         #it's actually drop the hole line ","a0999641":"traning_df = df[:10000]     #we will use this data for traning our model \ntesting_df = df[10000:]     #and this is for testing\n\n\ny = traning_df.Price       #Price of houses \nfeatures_x = ['Rooms','Distance','Bedroom2','Bathroom','Car','Landsize']\nx = traning_df[features_x] #and features of houses","2e6e438a":"model_melb = GaussianNB()\nmodel_melb.fit(x,y)       #we are giving features and results for training","879890d2":"testing_df_x = testing_df[features_x] #we are taking only house features cause we want to find prices \n                                      #using by our model\npredict = model_melb.predict(testing_df_x)#giving features and here our predict for house price as a list\npredict = pd.DataFrame(predict)           #now we make a dataframe","51a194b4":"plt.figure(figsize=(10,10))        \nsns.distplot(predict,color='red')\nsns.distplot(testing_df.Price)\nplt.show()","44608cd9":"model_melb = DecisionTreeRegressor(random_state=1)\nmodel_melb.fit(x,y)\n\n\ntesting_df_x = testing_df[features_x] #we are taking only house features cause we want to find prices \n                                      #using by our model\npredict = model_melb.predict(testing_df_x)#giving features and here our predict for house price as a list\npredict = pd.DataFrame(predict)\n\nplt.figure(figsize=(10,10))        \nsns.distplot(predict,color='red')\nsns.distplot(testing_df.Price)\nplt.show()\n\n","774c9be2":"df = pd.read_csv('..\/input\/melb_data.csv')\n\nid=0\nfor suburb in df.Suburb.unique():         #for using suburb we have to give every unique string to id\n    df['Suburb'][df['Suburb']==suburb]=id\n    id+=1\n    #now we can use suburb for our model\n\n\n#################################################################################################\nfeatures = ['Suburb','Rooms','Distance','Bedroom2','Bathroom','Car','Landsize','Lattitude','Longtitude','Price']\ndf = df[features]\ndf = df.dropna(axis=0)\n\n###########################################################################################\ntraning_df = df[:10000]\ntesting_df = df[10000:]\n\ny = traning_df.Price  \n\nfeatures_x = ['Suburb','Rooms','Distance','Bedroom2','Bathroom','Car','Landsize','Lattitude','Longtitude']\nx = traning_df[features_x]\n\n\n#################################################################################################\nmodel_melb = DecisionTreeRegressor(random_state=1)\nmodel_melb.fit(x,y)\n\n##################################################################################################\ntesting_df_x = testing_df[features_x]\npredict = model_melb.predict(testing_df_x)\npredict = pd.DataFrame(predict)\n\n#################################################################################################\nplt.figure(figsize=(10,10))\nsns.distplot(predict,color='red')\nsns.distplot(testing_df.Price)\nplt.show()","b9f8a505":"df = pd.read_csv('..\/input\/melb_data.csv')\n\nid=0\nfor suburb in df.Suburb.unique():     \n    df['Suburb'][df['Suburb']==suburb]=id\n    id+=1\n    \n\n\n#################################################################################################\nfeatures = ['Suburb','Rooms','Distance','Bedroom2','Bathroom','Car','Landsize','Lattitude','Longtitude','Price']\ndf = df[features]\ndf = df.dropna(axis=0)\n\n###########################################################################################\ntraning_df = df[:10000]\ntesting_df = df[10000:]\n\n\ndef find_outlier(x):        #here we finding outliers\n    q1 = np.percentile(x, 5)   #those numbers after x they are control the limit \n    q3 = np.percentile(x, 95)\n    iqr = q3-q1\n    floor = q1 - 1.5*iqr\n    ceiling= q3 + 1.5*iqr\n    outlier_indices = list(x.index[(x<floor)|(x>ceiling)])\n    outlier_values = list(x[(x<floor)|(x>ceiling)])\n    return outlier_indices, outlier_values   #it's returning two array index and values\n\nfor columns in features:\n    outlier_traning_df = np.sort(find_outlier(traning_df[columns]))\n    #print(outlier_tranin_df)  if you want you can print outliers and you can take a look\n    traning_df = traning_df.drop(outlier_traning_df[0])\n    \n\ny = traning_df.Price\n\nfeatures_x = ['Suburb','Rooms','Distance','Bedroom2','Bathroom','Car','Landsize','Lattitude','Longtitude']\nx = traning_df[features_x]\n\n\n#################################################################################################\nmodel_melb = DecisionTreeRegressor(random_state=1)\nmodel_melb.fit(x,y)\n\n##################################################################################################\ntesting_df_x = testing_df[features_x]\npredict = model_melb.predict(testing_df_x)\npredict = pd.DataFrame(predict)\n\n#################################################################################################\nplt.figure(figsize=(10,10))\nsns.distplot(predict,color='red')\nsns.distplot(testing_df.Price)\nplt.show()","fd93e9cf":"df = pd.read_csv('..\/input\/melb_data.csv')\n\n#################################################################################################\nfeatures = ['Rooms','Distance','Bedroom2','Bathroom','Car','Landsize','Price']\ndf = df[features]\ndf = df.dropna(axis=0)\n\n###########################################################################################\ntraning_df = df[:10000]\ntesting_df = df[10000:]\n\n\ndef find_outlier(x):        #here we finding outliers\n    q1 = np.percentile(x, 3)   #those numbers after x they are control the limit \n    q3 = np.percentile(x, 97)\n    iqr = q3-q1\n    floor = q1 - 1.5*iqr\n    ceiling= q3 + 1.5*iqr\n    outlier_indices = list(x.index[(x<floor)|(x>ceiling)])\n    outlier_values = list(x[(x<floor)|(x>ceiling)])\n    return outlier_indices, outlier_values   #it's returning two array index and values\n\n\noutlier_traning_df = np.sort(find_outlier(traning_df['Price']))#HERE WE SENDING ONLY ONE FEATURE\ntraning_df = traning_df.drop(outlier_traning_df[0])\n    \n\ny = traning_df.Price\n\nfeatures_x = ['Rooms','Distance','Bedroom2','Bathroom','Car','Landsize']\nx = traning_df[features_x]\n\n\n#################################################################################################\nmodel_melb = DecisionTreeRegressor(random_state=1)\nmodel_melb.fit(x,y)\n\n##################################################################################################\ntesting_df_x = testing_df[features_x]\npredict = model_melb.predict(testing_df_x)\npredict = pd.DataFrame(predict)\n\n#################################################################################################\nplt.figure(figsize=(10,10))\nsns.distplot(predict,color='red')\nsns.distplot(testing_df.Price)\nplt.show()","93972213":"**NAIVE BAYES:**\n*Reds are our model predict blues are real price of house.It's looking good so smilar with real prices. We will try to make it better.*\n\n**Let's try decision tree model**","990af91f":"**Decision Tree:**\n*looking better than naive bayes actually it's looking great. I wasn't expect that but let's play more maybe we can take better results*\n\n**After this I will use decision tree model for better results**\n\n**Now lets use more features it can give better results**","ea7c8d30":"**RESULTS:**\n\n*It's mean giving every feature to model not work we should we give the right features to models*\n\n*It's also same for outlier fucntion we should give right features*","5bc05d94":"**WE ARE DIVIDING OUR DATA FOR TRAINING AND TESTING**\n\n**Traning:**\nFirst we will train our model, for this we will give features and results \nSo our model can say the results looking by the features\nWe will take 10.000 lines for training\n\n**Testing:**\nIn here we will give only features and we will take results\nThan we will compare results to real price of data ","1b50d712":"**OUTLIERS:**\n*Outliers are like the corners of our data*\n*If we make it more between limits we might cute more corners and sometimes it's not good cause we can lose lots of line *\n\n*It's give better results than one step before.*\n\n*Now average place better cause of corners(I mean the highest point of figure)*\n\n**ALSO HERE WE SEND EVERY FEATURES TO OUTLIER DEF **","dbb6dd33":"**With more features:**\n*It's not better than before why I don't know I thought it will give better results when we use more values for traning. \nLets keep going*\n\n**NOW WE WILL DETECT OUTLIERS**"}}