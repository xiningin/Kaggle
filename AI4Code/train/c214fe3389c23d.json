{"cell_type":{"c72f0e1d":"code","630a98a7":"code","2ff79845":"code","80fca63c":"code","a41955b0":"markdown","294201c2":"markdown","5a19174d":"markdown","9c09a21c":"markdown","57af5d5b":"markdown"},"source":{"c72f0e1d":"import pandas as pd\nfrom pathlib import Path\n\ndata_dir = Path('..\/input\/tabular-playground-series-dec-2021\/')\n\ndf_train = pd.read_csv(\n    data_dir \/ \"train.csv\",\n    index_col='Id',\n    nrows=25000 # comment this row to use the full dataset\n)\n\nFEATURES = df_train.columns[:-1]\nTARGET = df_train.columns[-1]\n\ndf_train.head()","630a98a7":"from xgboost import XGBClassifier\n\nX = df_train.loc[:, FEATURES]\ny = df_train.loc[:, TARGET]\n\nmodel = XGBClassifier(\n    max_depth=3,\n    subsample=0.5,\n    colsample_bytree=0.5,\n    n_jobs=-1,\n    # Uncomment if you want to use GPU. Recommended for whole training set.\n    #tree_method='gpu_hist',\n    random_state=0,\n)","2ff79845":"from sklearn.model_selection import cross_validate\nimport warnings \nwarnings.filterwarnings('ignore')\n\ndef score(X, y, model, cv):\n    scoring = [\"accuracy\"]\n    scores = cross_validate(\n        model, X, y, scoring=scoring, cv=cv, return_train_score=True\n    )\n    scores = pd.DataFrame(scores).T\n    return scores.assign(\n        mean = lambda x: x.mean(axis=1),\n        std = lambda x: x.std(axis=1),\n    )\n\nscores = score(X, y, model, cv=2)\n\ndisplay(scores)","80fca63c":"# Fit on full training set\nmodel.fit(X, y)\n\nX_test = pd.read_csv(data_dir \/ \"test.csv\", index_col='Id')\n\n# Make predictions\ny_pred = pd.Series(\n    model.predict(X_test),\n    index=X_test.index,\n    name=TARGET,\n)\n\n# Create submission file\ny_pred.to_csv(\"submission_getting_started.csv\")","a41955b0":"The target attribute 'Cover_Type' contains 7 types of Forest Cover (1, 2, 3, 4, 5, 6, 7).\n\n# Model\n\nLet's try out a simple XGBoost model. This algorithm can handle missing values. We use XGBClassifier (instead of XGBRegressor, for instance), since this is a classification problem.","294201c2":"# References\n1. Thank you to Ryan Holbrook, Alexis Cook and inversion for demonstrating how to get started using their [notebook](https:\/\/www.kaggle.com\/ryanholbrook\/getting-started-september-2021-tabular-playground\/notebook).\n","5a19174d":"# Problem Statement\n\nIn this competition, we use cartographic variables to classify forest categories. \n\nThe word 'cartographic' is an adjective which means 'relating to the science or practice of drawing maps'. Example usage: \"he started his own cartographic printing company\".\n\nWe are going to cover the following steps:\n1. Import Libraries\n2. Model\n3. Evaluation\n4. Submission\n5. References\n\nLet's get started.\n\nThe full training dataset has 4,000,000 (4M) rows. We'll use just a sample so we can explore the data more quickly.\n\n# Import Libraries","9c09a21c":"# Submission","57af5d5b":"# Evaluation\n\nThe evaluation metric is multi-class classification accuracy."}}