{"cell_type":{"608e0fe1":"code","00bb5542":"code","825f3b26":"code","c8691a47":"code","43e3259a":"code","e65bb126":"code","49c3d242":"code","c70c1af7":"code","7cf8ccd8":"code","7c92ca9b":"code","c3711518":"code","a08dac08":"code","60ccd8a5":"code","08da5587":"code","4857f3d7":"code","edefe40f":"code","a1d43d1c":"code","d370fcb5":"code","832c2922":"markdown","dc02ab14":"markdown","d5fd7c9a":"markdown","1eba9b69":"markdown","683c3fce":"markdown","3bd7410b":"markdown","f76a3172":"markdown"},"source":{"608e0fe1":"import numpy as np\nimport pandas as pd\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline \nplt.style.use('seaborn-whitegrid')\n\nimport xgboost as xgb\n\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import confusion_matrix\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","00bb5542":"df = pd.read_csv('\/kaggle\/input\/capital-cities-feats\/00 df.csv')","825f3b26":"char= ['&','<','>','-','=','+',' ']\nfor col in df.select_dtypes(include ='object') :\n    for i in char:\n        df[col] = df[col].str.replace(i,'')\ndf.head()","c8691a47":"for col in df:\n    print (df[col].unique())","43e3259a":"train = df[df['flag']=='train']\ntest = df[df['flag']=='test']","e65bb126":"cat_feats = ['age_bin','capital_gl_bin','education_bin','hours_per_week_bin','msr_bin','occupation_bin','race_sex_bin']\n\ny_train = train['y']\nx_train = train[['age_bin','capital_gl_bin','education_bin','hours_per_week_bin','msr_bin','occupation_bin','race_sex_bin']]\nx_train = pd.get_dummies(x_train,columns=cat_feats,drop_first=True)\n\ny_test = test['y']\nx_test = test[['age_bin','capital_gl_bin','education_bin','hours_per_week_bin','msr_bin','occupation_bin','race_sex_bin']]\nx_test = pd.get_dummies(x_test,columns=cat_feats,drop_first=True)","49c3d242":"results = []\nmax_depth = [x for x in range(1,10)]\nfor depth in max_depth:\n    xgb_model = xgb.XGBClassifier(max_depth= depth)\n    xgb_model.fit(x_train , y_train)\n    y_pred_xgb = xgb_model.predict(x_test)\n    accuracy = np.mean(y_test==y_pred_xgb)\n    results.append(accuracy)\n\nplt.figure(figsize=(8,4))\npd.Series(results, max_depth).plot(kind=\"bar\",color=\"darkred\",ylim=(0.7,0.9))","c70c1af7":"index_acc = pd.DataFrame({'accuracy': results, 'max_depth':max_depth})\nprint (index_acc.loc[index_acc.accuracy.idxmax(), 'max_depth'])","7cf8ccd8":"results = []\nn_estimators = [x for x in range(100,200,10)]\nfor estimators in n_estimators:\n    xgb_model = xgb.XGBClassifier(max_depth=5,n_estimators= estimators , objective=\"binary:logistic\")\n    xgb_model.fit(x_train, y_train)\n    y_pred_xgb = xgb_model.predict(x_test)\n    accuracy = np.mean(y_test==y_pred_xgb)\n    results.append(accuracy)\n\nplt.figure(figsize=(8,4))\npd.Series(results, n_estimators).plot(color=\"darkred\",marker=\"o\")","7c92ca9b":"results = []\nlearning_rate = np.arange(0.1, 1, 0.1)\nfor rate in learning_rate:\n    xgb_model = xgb.XGBClassifier(max_depth=5,n_estimators= 140 , learning_rate = rate, objective=\"binary:logistic\")\n    xgb_model.fit(x_train, y_train)\n    y_pred_xgb = xgb_model.predict(x_test)\n    accuracy = np.mean(y_test==y_pred_xgb)\n    results.append(accuracy)\n\nplt.figure(figsize=(8,4))\npd.Series(results, learning_rate).plot(color=\"darkred\",marker=\"o\")","c3711518":"results = []\ntree_method = ['auto', 'exact', 'approx', 'hist'] # 'gpu_exact', 'gpu_hist'\nfor tree in tree_method:\n    xgb_model = xgb.XGBClassifier(max_depth=5,n_estimators= 140 , learning_rate = 0.1, tree_method= tree, objective=\"binary:logistic\")\n    xgb_model.fit(x_train, y_train)\n    y_pred_xgb = xgb_model.predict(x_test)\n    accuracy = np.mean(y_test==y_pred_xgb)\n    results.append(accuracy)\n\nplt.figure(figsize=(8,4))\npd.Series(results, tree_method).plot(color=\"darkred\",marker=\"o\")","a08dac08":"xgb_model = xgb.XGBClassifier(max_depth=5,n_estimators= 140 , learning_rate = 0.1, tree_method= 'auto', objective=\"binary:logistic\")\nxgb_model.fit(x_train, y_train)\ny_pred_xgb = xgb_model.predict(x_test)","60ccd8a5":"# To get list and number of xgb trees\ndump_list=xgb_model.get_booster().get_dump()\nlen(dump_list)","08da5587":"#xgb.plot_tree(xgb_model,num_trees=0)\n#plt.show()","4857f3d7":"predictions = [round(value) for value in y_pred_xgb] # If objective=\"logistic\", predictions is unnecessary\naccuracy_xgb = accuracy_score(y_test, predictions)\nprint(accuracy_xgb)","edefe40f":"print('Classification report: \\n',classification_report(y_test,y_pred_xgb))","a1d43d1c":"xgb_cm = confusion_matrix(y_test,y_pred_xgb)\nxgb_cm","d370fcb5":"# visualize with seaborn library\nsns.heatmap(xgb_cm,annot=True,fmt=\"d\") \nplt.show()","832c2922":"class xgboost.XGBClassifier(max_depth=3, learning_rate=0.1, n_estimators=100, verbosity=1, objective='binary:logistic', booster='gbtree', tree_method='auto', n_jobs=1, gpu_id=-1, gamma=0, min_child_weight=1, max_delta_step=0, subsample=1, colsample_bytree=1, colsample_bylevel=1, colsample_bynode=1, reg_alpha=0, reg_lambda=1, scale_pos_weight=1, base_score=0.5, random_state=0, missing=None, **kwargs)","dc02ab14":"Parameters\n\nmax_depth (int) \u2013 Maximum tree depth for base learners.\n\nlearning_rate (float) \u2013 Boosting learning rate (xgb\u2019s \u201ceta\u201d)\n\nn_estimators (int) \u2013 Number of trees to fit.\n\nverbosity (int) \u2013 The degree of verbosity. Valid values are 0 (silent) - 3 (debug).\n\nobjective (string or callable) \u2013 Specify the learning task and the corresponding learning objective or a custom objective function to be used (see note below).\n\nbooster (string) \u2013 Specify which booster to use: gbtree, gblinear or dart.\n\ntree_method (string) \u2013 Specify which tree method to use. Default to auto. If this parameter is set to default, XGBoost will choose the most conservative option available. It\u2019s recommended to study this option from parameters document.\n\nn_jobs (int) \u2013 Number of parallel threads used to run xgboost.\n\ngamma (float) \u2013 Minimum loss reduction required to make a further partition on a leaf node of the tree.\n\nmin_child_weight (int) \u2013 Minimum sum of instance weight(hessian) needed in a child.\n\nmax_delta_step (int) \u2013 Maximum delta step we allow each tree\u2019s weight estimation to be.\n\nsubsample (float) \u2013 Subsample ratio of the training instance.\n\ncolsample_bytree (float) \u2013 Subsample ratio of columns when constructing each tree.\n\ncolsample_bylevel (float) \u2013 Subsample ratio of columns for each level.\n\ncolsample_bynode (float) \u2013 Subsample ratio of columns for each split.\n\nreg_alpha (float (xgb's alpha)) \u2013 L1 regularization term on weights\n\nreg_lambda (float (xgb's lambda)) \u2013 L2 regularization term on weights\n\nscale_pos_weight (float) \u2013 Balancing of positive and negative weights.\n\nbase_score \u2013 The initial prediction score of all instances, global bias.\n\nrandom_state (int) \u2013 Random number seed.\n\nmissing (float, optional) \u2013 Value in the data which needs to be present as a missing value. If None, defaults to np.nan.\n\nnum_parallel_tree (int) \u2013 Used for boosting random forest.\n\nimportance_type (string, default \"gain\") \u2013 The feature importance type for the feature_importances_ property: either \u201cgain\u201d, \u201cweight\u201d, \u201ccover\u201d, \u201ctotal_gain\u201d or \u201ctotal_cover\u201d.\n\n**kwargs (dict, optional) \u2013\n\nKeyword arguments for XGBoost Booster object. Full documentation of parameters can be found here: https:\/\/github.com\/dmlc\/xgboost\/blob\/master\/doc\/parameter.rst. Attempting to set a parameter via the constructor args and **kwargs dict simultaneously will result in a TypeError.","d5fd7c9a":"# Get the Data","1eba9b69":"# Parameters of XGBoost","683c3fce":"# XGBoost ","3bd7410b":"# Result","f76a3172":"for objective parameter in XGBClassifier :\n    objective: determines the loss function to be used like reg:linear for regression problems, reg:logistic for classification problems with only decision, binary:logistic for classification problems with probability."}}