{"cell_type":{"b8bb6789":"code","ccd13881":"code","f4fad4b3":"code","7dd82b60":"code","c5bcc31c":"code","3416c793":"code","0316a5e2":"code","4d1381d1":"code","779ea651":"code","bddf389c":"code","dc519e7b":"code","a2a657e1":"code","e2df154a":"code","5db731fa":"code","42165910":"markdown","5bead614":"markdown","e75a52d4":"markdown","d5a087ea":"markdown","de50a814":"markdown","9b294b5c":"markdown","98b93227":"markdown","bb9cc34c":"markdown","12aa74e6":"markdown","9ac24fbb":"markdown","d7a34268":"markdown","f9a237e7":"markdown","eecc4d72":"markdown","8e12f4d7":"markdown","800822c7":"markdown","34466380":"markdown","ef28ba89":"markdown","26ac056e":"markdown"},"source":{"b8bb6789":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","ccd13881":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom keras import models\nfrom keras import layers\nfrom keras import optimizers\nfrom keras.utils import to_categorical","f4fad4b3":"data = pd.read_csv(\"\/kaggle\/input\/facial-expression-recognitionferchallenge\/fer2013\/fer2013\/fer2013.csv\")\ndata.head()","7dd82b60":"print(\"Datan\u0131n sat\u0131r ve s\u00fct\u00fcn say\u0131lar\u0131 = \", data.shape)\nprint(\"S\u00fct\u00fcnlar\u0131n ismi = \", data.columns)","c5bcc31c":"data[\"Usage\"].value_counts()","3416c793":"training = data.loc[data[\"Usage\"] == \"Training\"]\npublic_test = data.loc[data[\"Usage\"] == \"PublicTest\"]\nprivate_test = data.loc[data[\"Usage\"] == \"PrivateTest\"]\n\nprint(\"E\u011fitim seti = \", training.shape)\nprint(\"Genel test seti = \", public_test.shape)\nprint(\"\u00d6zel test seti = \", private_test.shape)","0316a5e2":"print(\"========================= Emotion Adetleri ===========================\")\nprint(\"train adet = \\n{}, \\npublic adet = \\n{}, \\nprivate adet = \\n{}\".format(training[\"emotion\"].value_counts(),\n      public_test[\"emotion\"].value_counts(), private_test[\"emotion\"].value_counts()))\n","4d1381d1":"train_labels = training[\"emotion\"]\ntrain_labels = to_categorical(train_labels)\n\ntrain_pixels = training[\"pixels\"].str.split(\" \").tolist()\ntrain_pixels = np.uint8(train_pixels)\ntrain_pixels = train_pixels.reshape((28709, 48, 48, 1))\ntrain_pixels = train_pixels.astype(\"float32\") \/ 255\n\n\nprivate_labels = private_test[\"emotion\"]\nprivate_labels = to_categorical(private_labels)\n\nprivate_pixels = private_test[\"pixels\"].str.split(\" \").tolist()\nprivate_pixels = np.uint8(private_pixels)\nprivate_pixels = private_pixels.reshape((3589, 48, 48, 1))\nprivate_pixels = private_pixels.astype(\"float32\") \/ 255\n\n\npublic_labels = public_test[\"emotion\"]\npublic_labels = to_categorical(public_labels)\n\npublic_pixels = public_test[\"pixels\"].str.split(\" \").tolist()\npublic_pixels = np.uint8(public_pixels)\npublic_pixels = public_pixels.reshape((3589, 48, 48, 1))\npublic_pixels = public_pixels.astype(\"float32\") \/ 255","779ea651":"import seaborn as sns\nplt.figure(0, figsize=(12,6))\nfor i in range(1, 13):\n    plt.subplot(3,4,i)\n    plt.imshow(train_pixels[i, :, :, 0], cmap=\"gray\")\n\nplt.tight_layout()\nplt.show()\n","bddf389c":"model_1 = models.Sequential()\n\n# Conv (evri\u015fim katman\u0131)\nmodel_1.add(layers.Conv2D(64, (5, 5), activation='relu', input_shape=(48,48,1)))\n#Ortaklama katman\u0131\nmodel_1.add(layers.MaxPooling2D(pool_size=(5,5), strides=(2, 2)))\n\nmodel_1.add(layers.Conv2D(64, (3, 3), activation='relu'))\nmodel_1.add(layers.Conv2D(64, (3, 3), activation='relu'))\nmodel_1.add(layers.AveragePooling2D(pool_size=(3,3), strides=(2, 2)))\n\nmodel_1.add(layers.Conv2D(128, (3, 3), activation='relu'))\nmodel_1.add(layers.Conv2D(128, (3, 3), activation='relu'))\nmodel_1.add(layers.AveragePooling2D(pool_size=(3,3), strides=(2, 2)))\n\nmodel_1.add(layers.Flatten())\n\n# Tam ba\u011flant\u0131 katman\u0131\nmodel_1.add(layers.Dense(1024, activation='relu'))\nmodel_1.add(layers.Dropout(0.2))\nmodel_1.add(layers.Dense(1024, activation='relu'))\nmodel_1.add(layers.Dropout(0.2))\n\nmodel_1.add(layers.Dense(7, activation='softmax'))\n","dc519e7b":"model_1.summary()","a2a657e1":"model_1.compile(optimizer = \"Adam\", loss = \"categorical_crossentropy\", metrics = [\"accuracy\"])\n\nhist = model_1.fit(train_pixels, train_labels, batch_size = 256, epochs = 30,\n                validation_data = (private_pixels, private_labels))\n","e2df154a":"acc = hist.history[\"accuracy\"]\nval_acc = hist.history[\"val_accuracy\"]\nloss = hist.history[\"loss\"]\nval_loss = hist.history[\"val_loss\"]\n\nepochs = range(1, len(acc) + 1)\n\nplt.plot(epochs, acc, \"bo\", label = \"E\u011fitim Ba\u015far\u0131s\u0131\")\nplt.plot(epochs, val_acc, \"b\", label = \"Do\u011frulama Ba\u015far\u0131s\u0131\")\nplt.title(\"E\u011fitim ve Do\u011frulama Ba\u015far\u0131s\u0131\")\nplt.legend()\n\nplt.figure()\n\nplt.plot(epochs, loss, \"bo\", label = \"E\u011fitim Kayb\u0131\")\nplt.plot(epochs, val_loss, \"b\", label = \"Do\u011frulama Kayb\u0131\")\nplt.legend()\n\n\nplt.show()","5db731fa":"print(\"Te\u015fekk\u00fcrler!\")","42165910":"Now let's look at some photos","5bead614":"We will do the same for each set. These:\n\n    1) We will categorize it by taking the \"emotion\" column which is our answers. In other words, we will output a transaction with 0 in all the remaining lines except the class it is in.\n    \n    2) We will take the pixels column in our set and turn it into a tensor and standardize it.\n     ","e75a52d4":"Yine her bir setimizde \"emotion\", \"pixels\" ve \"usage\" de\u011fi\u015fkenlerimiz var \u015fimdi e\u011fitim ve test setlerimizi d\u00fczenleme i\u015flemlerine ge\u00e7elim.","d5a087ea":"**FIRST LOOK**","de50a814":"Graf --> [https:\/\/www.kaggle.com\/omarensaj\/fer-emotion-detection-psd07](http:\/\/)","9b294b5c":"\nModel 16-17. After the epoch (1 full round on the data), the training success increases and the test success decreases, that is, it overfit.\n\nIn this study, we were able to predict 7 different emotions by 58-60%. Data Diversification can be applied to photos in different filter sizes, more layers or training set to improve the model.\n\nSee you...\n\nThanks.","98b93227":"First, we will load our data and examine the various information in it.\n\nLater, we will make our train and test data to be able to model with convolutional neural network and build a model and expect to get a good result.","bb9cc34c":"**MODEL**","12aa74e6":"**DATA**","9ac24fbb":"\nOver 35,000 photographs collected in the data in 2013, some of the existing photographs were taken from various media publications and some of them consist of stock photographs. Our goal here is to recognize 7 different emotions (angry, happy, scared, natural, etc.) through photographs with a convolutional neural network.","d7a34268":"The number of transactions in the model output is almost 1.5 million. Let's train the model now","f9a237e7":"The emotion variable represents emotions and is our target variable.\nPixels variable expresses the value per pixel in the photos.\nUsage, on the other hand, shows which set the row it belongs to (such as training and testing).\n\nLet's look at our training and test sets and separate these sets.","eecc4d72":"Her bir veri setimizde hangi duygudan ne kadar oldu\u011funu g\u00f6rmek istersek;","8e12f4d7":"So far, we have processed the data and made it ready to build a model, now we can start building the model.","800822c7":"[](http:\/\/)**Determine a Road Map**","34466380":"Hello,\n\nIn this work, I will try to recognize emotions in photographs through a convolutional neural network using a keras library.","ef28ba89":"**PREPROCESSING**","26ac056e":"**UPLOAD DATA**"}}