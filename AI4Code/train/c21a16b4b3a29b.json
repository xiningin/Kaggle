{"cell_type":{"dd244d0f":"code","c99bc410":"code","844dd066":"code","fbd61468":"code","4dcc51f9":"code","98db227b":"code","d640da85":"code","d4323310":"code","93cdf9ee":"code","e86402e3":"code","6b6c0866":"code","b270890d":"code","7b0753d2":"code","caabfee5":"code","b86d15f3":"code","7c0d666d":"code","dd215360":"code","3767f1bb":"code","50512e00":"code","dc4bfecd":"code","7b1a48ea":"code","576ac0b2":"code","ef1ba8bf":"code","ecb83cdb":"code","e18fe0b8":"code","3e49110f":"code","4f2cc301":"code","e65b7089":"code","a7291201":"code","caffdcab":"code","d815263c":"code","3c855bd3":"code","dda828e8":"code","cf02b5b7":"code","ec30a5f5":"code","c5844d26":"code","9a799254":"code","c2371e1c":"code","83676086":"code","37c97a21":"code","c3c7bf3a":"code","6f30a9fd":"code","dad3db78":"code","2fb362e6":"code","6f675a8c":"code","b8216338":"code","ff422b18":"code","1bacfb53":"markdown","95a7f039":"markdown","0f3a3f00":"markdown","fed5014c":"markdown","7dc4079a":"markdown","cf6702f2":"markdown","59ac91c6":"markdown","7a19d4b9":"markdown","d2ae1959":"markdown","151988e4":"markdown","2c9cc38a":"markdown","72fb6c54":"markdown","70bbf8a1":"markdown"},"source":{"dd244d0f":"import tensorflow as tf\nimport matplotlib.pyplot as plt\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nimport keras\nimport csv\nimport pandas as pd\nimport random\nimport numpy as np\nfrom keras import regularizers\nimport re\nfrom tqdm import tqdm_notebook\nfrom nltk.corpus import stopwords\nfrom nltk import WordNetLemmatizer\nimport nltk\nnltk.download('wordnet')\nimport itertools\nfrom string import ascii_lowercase\nfrom functools import reduce\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split","c99bc410":"train_path='..\/input\/jigsaw-toxic-comment-classification-challenge\/train.csv'\ntrain=pd.read_csv(train_path)","844dd066":"train.head()","fbd61468":"train.isnull().sum()","4dcc51f9":"labels = ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']\ny = train[labels].values  \nprint(y[:5])","98db227b":"lens=train['comment_text'].str.len()","d640da85":"resize = np.arange(0, 1500,200)\nlens.hist(color='pink', figsize=(10, 6), bins=resize,width=200 )\nplt.title('Length Distribution')\nplt.xlabel('Length of the Comments')\nplt.ylabel('Number of Comments')","d4323310":"class_cnt = {}\nfor label in labels:\n    class_cnt[label] = len(train[train[label] == 1])\n    \nclass_cnt = {k: v for k, v in sorted(class_cnt.items(), key = lambda item: item[1], reverse = True)}\nvalues=[*class_cnt.values()]\nplt.pie(values, labels=labels, radius=2.5,autopct='%1.1f%%')\nplt.show()","93cdf9ee":"RE_PATTERNS = {\n    ' american ':\n        [\n            'amerikan'\n        ],\n\n    ' adolf ':\n        [\n            'adolf'\n        ],\n\n\n    ' hitler ':\n        [\n            'hitler'\n        ],\n\n    ' fuck':\n        [\n            '(f)(u|[^a-z0-9 ])(c|[^a-z0-9 ])(k|[^a-z0-9 ])([^ ])*',\n            '(f)([^a-z]*)(u)([^a-z]*)(c)([^a-z]*)(k)',\n            ' f[!@#\\$%\\^\\&\\*]*u[!@#\\$%\\^&\\*]*k', 'f u u c',\n            '(f)(c|[^a-z ])(u|[^a-z ])(k)', r'f\\*',\n            'feck ', ' fux ', 'f\\*\\*', \n            'f\\-ing', 'f\\.u\\.', 'f###', ' fu ', 'f@ck', 'f u c k', 'f uck', 'f ck','fuk', 'wtf','fucck','f cking'\n        ],\n\n    ' ass ':\n        [\n            '[^a-z]ass ', '[^a-z]azz ', 'arrse', ' arse ', '@\\$\\$'\n                                                           '[^a-z]anus', ' a\\*s\\*s', '[^a-z]ass[^a-z ]',\n            'a[@#\\$%\\^&\\*][@#\\$%\\^&\\*]', '[^a-z]anal ', 'a s s'\n        ],\n\n    ' asshole ':\n        [\n            ' a[s|z]*wipe', 'a[s|z]*[w]*h[o|0]+[l]*e', '@\\$\\$hole', 'ass hole'\n        ],\n\n    ' bitch ':\n        [\n            'b[w]*i[t]*ch', 'b!tch',\n            'bi\\+ch', 'b!\\+ch', '(b)([^a-z]*)(i)([^a-z]*)(t)([^a-z]*)(c)([^a-z]*)(h)',\n            'biatch', 'bi\\*\\*h', 'bytch', 'b i t c h','beetch'\n        ],\n\n    ' bastard ':\n        [\n            'ba[s|z]+t[e|a]+rd'\n        ],\n\n    ' transgender':\n        [\n            'transgender','trans gender'\n        ],\n\n    ' gay ':\n        [\n            'gay'\n        ],\n\n    ' cock ':\n        [\n            '[^a-z]cock', 'c0ck', '[^a-z]cok ', 'c0k', '[^a-z]cok[^aeiou]', ' cawk',\n            '(c)([^a-z ])(o)([^a-z ]*)(c)([^a-z ]*)(k)', 'c o c k'\n        ],\n\n    ' dick ':\n        [\n            ' dick[^aeiou]', 'deek', 'd i c k','diick '\n        ],\n\n    ' suck ':\n        [\n            'sucker', '(s)([^a-z ]*)(u)([^a-z ]*)(c)([^a-z ]*)(k)', 'sucks', '5uck', 's u c k'\n        ],\n\n    ' cunt ':\n        [\n            'cunt', 'c u n t'\n        ],\n\n    ' bullshit ':\n        [\n            'bullsh\\*t', 'bull\\$hit','bs'\n        ],\n\n    ' homosexual':\n        [\n            'homo sexual','homosex'\n        ],\n\n    ' jerk ':\n        [\n            'jerk'\n        ],\n\n    ' idiot ':\n        [\n            'i[d]+io[t]+', '(i)([^a-z ]*)(d)([^a-z ]*)(i)([^a-z ]*)(o)([^a-z ]*)(t)', 'idiots', 'i d i o t'\n        ],\n\n    ' dumb ':\n        [\n            '(d)([^a-z ]*)(u)([^a-z ]*)(m)([^a-z ]*)(b)'\n        ],\n\n    ' shit ':\n        [\n            'shitty', '(s)([^a-z ]*)(h)([^a-z ]*)(i)([^a-z ]*)(t)', 'shite', '\\$hit', 's h i t'\n        ],\n\n    ' shithole ':\n        [\n            'shythole','shit hole'\n        ],\n\n    ' retard ':\n        [\n            'returd', 'retad', 'retard', 'wiktard', 'wikitud'\n        ],\n\n    ' rape ':\n        [\n            ' raped'\n        ],\n\n    ' dumbass':\n        [\n            'dumb ass', 'dubass'\n        ],\n\n    ' asshead':\n        [\n            'butthead', 'ass head'\n        ],\n\n    ' sex ':\n        [\n            's3x', 'sexuality',\n        ],\n\n\n    ' nigger ':\n        [\n            'nigger', 'ni[g]+a', ' nigr ', 'negrito', 'niguh', 'n3gr', 'n i g g e r'\n        ],\n\n    ' shut the fuck up':\n        [\n            'stfu'\n        ],\n\n    ' pussy ':\n        [\n            'pussy[^c]', 'pusy', 'pussi[^l]', 'pusses'\n        ],\n\n    ' faggot ':\n        [\n            'faggot', ' fa[g]+[s]*[^a-z ]', 'fagot', 'f a g g o t', 'faggit',\n            '(f)([^a-z ]*)(a)([^a-z ]*)([g]+)([^a-z ]*)(o)([^a-z ]*)(t)', 'fau[g]+ot', 'fae[g]+ot',\n        ],\n\n    ' motherfucker':\n        [\n            ' motha ', ' motha f', ' mother f', 'motherucker', 'mother fucker'\n        ],\n\n    ' whore ':\n        [\n            'wh\\*\\*\\*', 'w h o r e'\n        ],\n}","e86402e3":"##Text Normalization\n\ndef clean_text(text,remove_repeat_text=True, remove_patterns_text=True, is_lower=True):\n\n  if is_lower:\n    text=text.lower()\n    \n  if remove_patterns_text:\n    for target, patterns in RE_PATTERNS.items():\n      for pat in patterns:\n        text=str(text).replace(pat, target)\n\n  if remove_repeat_text:\n    text = re.sub(r'(.)\\1{2,}', r'\\1', text) \n\n  text = str(text).replace(\"\\n\", \" \")\n  text = re.sub(r'[^\\w\\s]',' ',text)\n  text = re.sub('[0-9]',\"\",text)\n  text = re.sub(\" +\", \" \", text)\n  text = re.sub(\"([^\\x00-\\x7F])+\",\" \",text)\n  return text ","6b6c0866":"train_texts = [] \nfor line in tqdm_notebook(train['comment_text'], total=159571): \n    train_texts.append(clean_text(line))","b270890d":"##Lemmatization\n\nlemmatizer= WordNetLemmatizer()\ndef lemma(text, lemmatization=True):\n  output=''\n  if lemmatization:\n    text=text.split(' ')\n    for word in text:\n      word1 = lemmatizer.lemmatize(word, pos = \"n\") #noun \n      word2 = lemmatizer.lemmatize(word1, pos = \"v\") #verb\n      word3 = lemmatizer.lemmatize(word2, pos = \"a\") #adjective\n      word4 = lemmatizer.lemmatize(word3, pos = \"r\") #adverb\n      output=output + \" \" + word4\n  else:\n    output=text\n  \n  return str(output.strip())","7b0753d2":"lema_train_text=[]\nfor line in train_texts:\n    lema_train_text.append(lemma(line))","caabfee5":"##Stopwords Removal\n\nstopword_list=[]\ndef iter_all_strings():\n    for size in itertools.count(1):\n        for s in itertools.product(ascii_lowercase, repeat=size):\n            yield \"\".join(s)\n\ndual_alpha_list=[]\nfor s in iter_all_strings():\n    dual_alpha_list.append(s)\n    if s == 'zz':\n        break\n\ndual_alpha_list.remove('i')\ndual_alpha_list.remove('a')\ndual_alpha_list.remove('am')\ndual_alpha_list.remove('an')\ndual_alpha_list.remove('as')\ndual_alpha_list.remove('at')\ndual_alpha_list.remove('be')\ndual_alpha_list.remove('by')\ndual_alpha_list.remove('do')\ndual_alpha_list.remove('go')\ndual_alpha_list.remove('he')\ndual_alpha_list.remove('hi')\ndual_alpha_list.remove('if')\ndual_alpha_list.remove('is')\ndual_alpha_list.remove('in')\ndual_alpha_list.remove('me')\ndual_alpha_list.remove('my')\ndual_alpha_list.remove('no')\ndual_alpha_list.remove('of')\ndual_alpha_list.remove('on')\ndual_alpha_list.remove('or')\ndual_alpha_list.remove('ok')\ndual_alpha_list.remove('so')\ndual_alpha_list.remove('to')\ndual_alpha_list.remove('up')\ndual_alpha_list.remove('us')\ndual_alpha_list.remove('we')\n\n\nfor letter in dual_alpha_list:\n    stopword_list.append(letter)","b86d15f3":"print(stopword_list)\nprint(len(stopword_list))\nprint(len(lema_train_text))","7c0d666d":"def search_stopwords(data, search_stop=True):\n  output=\"\"\n  if search_stop:\n    data=data.split(\" \")\n    for word in data:\n      if not word in stopword_list:\n        output=output+\" \"+word \n  else:\n    output=data\n  return str(output.strip())\n\npotential_stopwords = []\nfor line in tqdm_notebook(lema_train_text, total=159571): \n    potential_stopwords.append(search_stopwords(line))\nprint(len(potential_stopwords))\n\ndef string_combine_a(stopword):\n  final_a=\"\"\n  for item in range(39893):\n    final_a=final_a+\" \"+stopword[item]\n  return final_a\n\ndef string_combine_b(stopword):\n  final_b=\"\"\n  for item in range(39893,79785):\n    final_b=final_b+\" \"+stopword[item]\n  return final_b\n\ndef string_combine_c(stopword):\n  final_c=\"\"\n  for item in range(79785,119678):\n    final_c=final_c+\" \"+stopword[item]\n  return final_c\n\ndef string_combine_d(stopword):\n  final_d=\"\"\n  for item in range(119678,159571):\n    final_d=final_d+\" \"+stopword[item]\n  return final_d\n\ntotal_string_potential_a=string_combine_a(potential_stopwords)\ntotal_string_potential_b=string_combine_b(potential_stopwords)\ntotal_string_potential_c=string_combine_c(potential_stopwords)\ntotal_string_potential_d=string_combine_d(potential_stopwords)\n\ndef word_count(str):\n    counts = dict()\n    words = str.split()\n\n    for word in words:\n        if word in counts:\n            counts[word] += 1\n        else:\n            counts[word] = 1\n    return counts\n\ntotal_string_potential_a_dict=word_count(total_string_potential_a)\ntotal_string_potential_b_dict=word_count(total_string_potential_b)\ntotal_string_potential_c_dict=word_count(total_string_potential_c)\ntotal_string_potential_d_dict=word_count(total_string_potential_d)\n\ntotal_string_potential_a_df = pd.DataFrame(list(total_string_potential_a_dict.items()),columns = ['Word','Count'])\ntotal_string_potential_b_df = pd.DataFrame(list(total_string_potential_b_dict.items()),columns = ['Word','Count'])\ntotal_string_potential_c_df = pd.DataFrame(list(total_string_potential_c_dict.items()),columns = ['Word','Count'])\ntotal_string_potential_d_df = pd.DataFrame(list(total_string_potential_d_dict.items()),columns = ['Word','Count'])\n\ntop50_potential_stopwords_a=total_string_potential_a_df.sort_values(by=['Count'],ascending=False).head(50)\ntop50_potential_stopwords_b=total_string_potential_b_df.sort_values(by=['Count'],ascending=False).head(50)\ntop50_potential_stopwords_c=total_string_potential_c_df.sort_values(by=['Count'],ascending=False).head(50)\ntop50_potential_stopwords_d=total_string_potential_d_df.sort_values(by=['Count'],ascending=False).head(50)\n\n","dd215360":"#Looking for common terms in all top 50 dataframes.\ncommon_potential_stopwords=list(reduce(set.intersection,map(set,[top50_potential_stopwords_a.Word,top50_potential_stopwords_b.Word,top50_potential_stopwords_c.Word,top50_potential_stopwords_d.Word])))\nprint(common_potential_stopwords)","3767f1bb":"#Retaining certain words and removing others from the above list.\npotential_stopwords=['editor', 'reference', 'thank', 'work','find', 'good', 'know', 'like', 'look', 'thing', 'want', 'time', 'list', 'section','wikipedia', 'doe', 'add','new', 'try', 'think', 'write','use', 'user', 'way', 'page']\n\n#Adding above retrived words into the stopwords list.\nfor word in potential_stopwords:\n    stopword_list.append(word)","50512e00":"def remove_stopwords(text, remove_stop=True):\n  output = \"\"\n  if remove_stop:\n    text=text.split(\" \")\n    for word in text:\n      if word not in stopword_list:\n        output=output + \" \" + word\n  else :\n    output=text\n\n  return str(output.strip())\n\n#Removing Stopwords from Train Data\nprocessed_train_data = [] \nfor line in tqdm_notebook(lema_train_text, total=159571): \n    processed_train_data.append(remove_stopwords(line))","dc4bfecd":"processed_train_data[:5]","7b1a48ea":"max_features=148844\nmaxpadlen = 300          \nval_split = 0.2      \nembedding_dim_fasttext = 300","576ac0b2":"#Tokenization\ntokenizer = Tokenizer(num_words=max_features)\ntokenizer.fit_on_texts(list(processed_train_data))\nlist_tokenized_train = tokenizer.texts_to_sequences(processed_train_data)","ef1ba8bf":"#Indexing\nword_index=tokenizer.word_index\nprint(len(word_index))","ecb83cdb":"#padding\ntraining_padded=pad_sequences(list_tokenized_train, maxlen=maxpadlen, padding = 'post')","e18fe0b8":"print('Tokenized sentences: \\n', training_padded[10])\nprint('One hot label: \\n', y[10])","3e49110f":"#Splitting data into Training and Validation Set\n\nx_train, x_val, y_train, y_val = train_test_split(training_padded, y, test_size=0.2, random_state=2)","4f2cc301":"print('Number of entries in each category: ')\nprint('Training: ', y_train.sum(axis=0))\nprint('Validation: ', y_val.sum(axis=0))","e65b7089":"embeddings_index_fasttext = {}\nf = open('..\/input\/popular-embedding\/wiki-news-300d-1M\/wiki-news-300d-1M.vec', encoding='utf8')\nfor line in f:\n    line.encode('utf-8').strip()\n    values = line.split()\n    word = values[0]\n    embeddings_index_fasttext[word] = np.asarray(values[1:], dtype='float32')\nf.close()\nembedding_matrix_fasttext = np.random.random((len(word_index) + 1, embedding_dim_fasttext))\nfor word, i in word_index.items():\n    embedding_vector = embeddings_index_fasttext.get(word)\n    if embedding_vector is not None:\n        embedding_matrix_fasttext[i] = embedding_vector","a7291201":"print(type(x_train))\nprint(type(x_val))\nprint(type(y_train))\nprint(type(y_val))","caffdcab":"model_1 = tf.keras.Sequential([\n\ttf.keras.layers.Embedding(len(word_index) + 1,\n                           embedding_dim_fasttext,\n                           weights = [embedding_matrix_fasttext],\n                           input_length = maxpadlen,\n                           trainable=False,\n                           name = 'embeddings'),\n  tf.keras.layers.Input(shape=(maxpadlen, ),dtype='int32'),\n  tf.keras.layers.LSTM(40,return_sequences=True, name='lstm_layer'),\n  tf.keras.layers.GlobalMaxPooling1D(),\n  tf.keras.layers.Dropout(.1),\n  tf.keras.layers.Dense(30, activation='relu', kernel_initializer='he_uniform'),\n\ttf.keras.layers.Dropout(.1),\n\ttf.keras.layers.Dense(6, activation='sigmoid', kernel_initializer='he_uniform')\n])\nmodel_1.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\nmodel_1.summary()","d815263c":"history_1 = model_1.fit(x_train,y_train, epochs=2, batch_size=32,  validation_data=(x_val, y_val))","3c855bd3":"def plot_graphs(history_1, string):\n  plt.plot(history_1.history[string])\n  plt.plot(history_1.history['val_'+string])\n  plt.xlabel(\"Epochs\")\n  plt.ylabel(string)\n  plt.legend([string, 'val_'+string])\n  plt.show()\n\nplot_graphs(history_1, 'accuracy')\nplot_graphs(history_1, 'loss')","dda828e8":"model_2 = tf.keras.Sequential([\n\ttf.keras.layers.Embedding(len(word_index) + 1,\n                           embedding_dim_fasttext,\n                           weights = [embedding_matrix_fasttext],\n                           input_length = maxpadlen,\n                           trainable=False,\n                           name = 'embeddings'),\n  tf.keras.layers.Input(shape=(maxpadlen, ),dtype='int32'),\n  tf.keras.layers.LSTM(50,return_sequences=True, name='lstm_layer'),\n  tf.keras.layers.Conv1D(filters=64, kernel_size=3, padding='same', activation='relu', kernel_initializer='he_uniform'),\n  tf.keras.layers.MaxPooling1D(3),\n  tf.keras.layers.GlobalMaxPool1D(),\n  tf.keras.layers.BatchNormalization(),\n  tf.keras.layers.Dense(40, activation=\"relu\", kernel_initializer='he_uniform'),\n  tf.keras.layers.Dropout(0.2),\n  tf.keras.layers.Dense(30, activation=\"relu\", kernel_initializer='he_uniform'),\n  tf.keras.layers.Dropout(0.2),\n  tf.keras.layers.Dense(6, activation=\"sigmoid\", kernel_initializer='glorot_uniform')\n])\nmodel_2.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\nmodel_2.summary()","cf02b5b7":"history_2 = model_2.fit(x_train,y_train, epochs=2, batch_size=32,  validation_data=(x_val, y_val))","ec30a5f5":"def plot_graphs(history_2, string):\n  plt.plot(history_2.history[string])\n  plt.plot(history_2.history['val_'+string])\n  plt.xlabel(\"Epochs\")\n  plt.ylabel(string)\n  plt.legend([string, 'val_'+string])\n  plt.show()\n\nplot_graphs(history_2, 'accuracy')\nplot_graphs(history_2, 'loss')","c5844d26":"sub = pd.read_csv(\"..\/input\/jigsaw-toxic-severity-rating\/comments_to_score.csv\")","9a799254":"c=0\nclean=[]\nfor i in sub['text']:\n  i=clean_text(i)\n  i=lemma(i)\n  i=remove_stopwords(i)\n  clean.append(i)","c2371e1c":"sub['Cleaned_Comments']=clean","83676086":"sub.head()","37c97a21":"X_ = tokenizer.texts_to_sequences(sub['Cleaned_Comments'])\ntest_padded = pad_sequences(X_, maxlen=maxpadlen, truncating='post')\npred = model_1.predict(test_padded)","c3c7bf3a":"sub['score']=0.0","6f30a9fd":"for i in range(len(pred)):\n    sub['score'][i]=pred[i][0]+pred[i][1]+pred[i][2]+pred[i][3]+pred[i][4]+pred[i][5]","dad3db78":"sub.head()","2fb362e6":"for i in range(len(pred)):\n  if sub['score'][i]>1:\n    sub['score'][i]=1","6f675a8c":"sub.drop(['text'],axis=1,inplace=True)\nsub.drop(['Cleaned_Comments'],axis=1,inplace=True)","b8216338":"sub.head()","ff422b18":"sub[['comment_id', 'score']].to_csv(\"submission.csv\", index=False)","1bacfb53":"# **Data Visualization**","95a7f039":"# **LSTM**","0f3a3f00":"Number of toxic comments in the dataset are significantly higher. Identity Hate comments, on the other hand, are the lowest.","fed5014c":"# **Data Pre-Processing**\n\n","7dc4079a":"## **Importing Data**\nThe data that we have used for the model is Kaggle\u2019s Toxic Comment Classification Dataset on Wikipedia\u2019s Talk page edits.","cf6702f2":"# **Submission**","59ac91c6":"# **Plotting Graph for LSTM-CNN**","7a19d4b9":"**You can check out our github for in depth understanding of our project. Link in our profile.**","d2ae1959":"### **We'd really appriciate an upvote. It is our first competition and we'd really motivates us to go on and do more projects. and we are open to all the suggestions and feedback from you all since we believe in learning from out mistakes. Thank You!**\n\n## **Introduction**\nIdentifying harassment in social media discussions is vital, as is determining the number of unpleasant sentiments conveyed inside the comment. Organizations can save time and manual labor in regulating these platforms by automating this comment classification approach.The primary purpose is to determine the level of negativity and toxicity in internet comments. This could help to identify people who abuse individuals. As a result, it will eventually assist in the execution of policies and the penalty of individuals who do not obey, which can subsequently be used to reduce the level of toxicity in discussions. Using LSTM, CNN and FastText we aspire to build a multi-label classification model that separates comments into six groups according to their toxicity level: toxic, severe toxic, obscene, threat, insult, and identity-hate.\n\n**We have used FastText word embedding to make the model more efficient even on the unseen data.**","151988e4":"Maximum comment length are ranging from 0 to 200","2c9cc38a":"# **Plotting Graph for LSTM**","72fb6c54":"# **LSTM-CNN Model**","70bbf8a1":"# **Importing Libraries**"}}