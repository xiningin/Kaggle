{"cell_type":{"96ba085a":"code","6dd89106":"code","1280ead3":"code","f901e93f":"code","48643f39":"code","a8c465e2":"code","64452e46":"code","fd2426ff":"code","c8fa4545":"code","8306a9d9":"code","f22aae29":"code","0f793566":"code","1f2cbc07":"code","ab06c257":"code","986ce1e3":"code","da4aa70d":"code","ae88694e":"code","e5a73765":"code","596adc6a":"code","26c7a3d7":"code","663f862a":"code","93e7a3bb":"code","bb17a9cc":"markdown","74683ce1":"markdown","5cd70e19":"markdown","3442d915":"markdown","70eda3f0":"markdown","7fa94c5b":"markdown","bfda29b3":"markdown","e516b8e5":"markdown","6fcb83cd":"markdown","c59a5059":"markdown","22fd04a9":"markdown","6103ddf4":"markdown","c2a34823":"markdown","bc34b507":"markdown"},"source":{"96ba085a":"import numpy as np \nimport pandas as pd \nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","6dd89106":"train = pd.read_csv('\/kaggle\/input\/tabular-playground-series-jan-2021\/train.csv')\ntest = pd.read_csv('\/kaggle\/input\/tabular-playground-series-jan-2021\/test.csv')\nsample_submission = pd.read_csv('\/kaggle\/input\/tabular-playground-series-jan-2021\/sample_submission.csv')","1280ead3":"train.head(5)","f901e93f":"train.describe()","48643f39":"train.info()","a8c465e2":"filename = 'submission_as_provided.csv'\nsample_submission.to_csv(filename,index=False)\nprint('Saved file: ' + filename)","64452e46":"train.columns","fd2426ff":"from sklearn.tree import DecisionTreeRegressor\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.linear_model import Ridge\nfrom sklearn.linear_model import Lasso\nfrom xgboost import XGBRegressor\n\ny = train['target']\nX = train[['cont1', 'cont2', 'cont3', 'cont4', 'cont5', 'cont6', 'cont7',\n       'cont8', 'cont9', 'cont10', 'cont11', 'cont12', 'cont13', 'cont14']]\n\nfrom sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42, shuffle=True)\n\nmodels = [DecisionTreeRegressor(), LinearRegression(), Ridge(),  Lasso(), XGBRegressor()]\n\nfor model in models:\n    \n    model.fit(X_train, y_train)\n    y_pred = model.predict(X_test)\n\n    from sklearn import metrics\n    print('Model:', model)\n    print('Mean Absolute Error:', metrics.mean_absolute_error(y_test, y_pred))\n    print('Mean Squared Error:', metrics.mean_squared_error(y_test, y_pred))\n    print('Root Mean Squared Error:', np.sqrt(metrics.mean_squared_error(y_test, y_pred)))\n    print('r2_score:', metrics.r2_score (y_test, y_pred))\n    print('-------------------------------------')\n","c8fa4545":"X_for_submission = test[['cont1', 'cont2', 'cont3', 'cont4', 'cont5', 'cont6', 'cont7',\n       'cont8', 'cont9', 'cont10', 'cont11', 'cont12', 'cont13', 'cont14']]\n\nmodel = LinearRegression()\nmodel.fit(X, y)\n\npred = model.predict(X_for_submission)\n\nsample_submission['target'] = pred\n\nfilename = 'submission_lazy_model.csv'\nsample_submission.to_csv(filename,index=False)\nprint('Saved file: ' + filename)","8306a9d9":"X_for_submission = test[['cont1', 'cont2', 'cont3', 'cont4', 'cont5', 'cont6', 'cont7',\n       'cont8', 'cont9', 'cont10', 'cont11', 'cont12', 'cont13', 'cont14']]\n\nmodel = XGBRegressor()\nmodel.fit(X, y)\n\npred = model.predict(X_for_submission)\n\nsample_submission['target'] = pred\n\nfilename = 'submission_lazy_model_XGBRegressor.csv'\nsample_submission.to_csv(filename,index=False)\nprint('Saved file: ' + filename)","f22aae29":"fig, ax = plt.subplots(1, 1, figsize=(15, 9))\nsns.heatmap(train.corr(),  annot=True, fmt='.2f')\nplt.show()","0f793566":"from sklearn.tree import DecisionTreeRegressor\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.linear_model import Ridge\nfrom sklearn.linear_model import Lasso\nfrom xgboost import XGBRegressor\n\ny = train['target']\nX = train[['cont1', 'cont2', 'cont3', 'cont4', 'cont5', 'cont7',\n       'cont8', 'cont9', 'cont10', 'cont11', 'cont12', 'cont13', 'cont14']]\n\nfrom sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42, shuffle=True)\n\nmodels = [DecisionTreeRegressor(), LinearRegression(), Ridge(),  Lasso(), XGBRegressor()]\n\nfor model in models:\n    \n    model.fit(X_train, y_train)\n    y_pred = model.predict(X_test)\n\n    from sklearn import metrics\n    print('Model:', model)\n    print('Mean Absolute Error:', metrics.mean_absolute_error(y_test, y_pred))\n    print('Mean Squared Error:', metrics.mean_squared_error(y_test, y_pred))\n    print('Root Mean Squared Error:', np.sqrt(metrics.mean_squared_error(y_test, y_pred)))\n    print('r2_score:', metrics.r2_score (y_test, y_pred))\n    print('-------------------------------------')\n","1f2cbc07":"X_for_submission = test[['cont1', 'cont2', 'cont3', 'cont4', 'cont5', 'cont7',\n       'cont8', 'cont9', 'cont10', 'cont11', 'cont12', 'cont13', 'cont14']]\n\nmodel = XGBRegressor()\nmodel.fit(X, y)\n\npred = model.predict(X_for_submission)\n\nsample_submission['target'] = pred\n\nfilename = 'submission_cont6_removed_XGBRegressor.csv'\nsample_submission.to_csv(filename,index=False)\nprint('Saved file: ' + filename)","ab06c257":"fig, ax = plt.subplots(1, 1, figsize=(15, 9))\nsns.distplot(train['target'])\nplt.title('Distribution of target value', fontsize = 20, c='black')\nplt.show()","986ce1e3":"X = train[['cont1', 'cont2', 'cont3', 'cont4', 'cont5', 'cont6', 'cont7',\n       'cont8', 'cont9', 'cont10', 'cont11', 'cont12', 'cont13', 'cont14']]\n\nfrom sklearn.cluster import KMeans\n\nsse = []\nfor k in range(1, 10):\n    kmeans = KMeans(n_clusters=k, random_state=42, init='random', n_init=10, max_iter=10)\n    kmeans.fit(X)\n    sse.append(kmeans.inertia_)\n\nf, ax = plt.subplots(1,1,figsize=(15,9))\nplt.plot(range(1, 10), sse)\nplt.xticks(range(1, 10))\n#ax.annotate('Optimal number of clusters', xy=(2.05,1370000))\nplt.xlabel(\"Number of Clusters\")\nplt.ylabel(\"SSE\")\nplt.title('SSE for different number of clusters', fontsize = 20, c='black')\nplt.show()","da4aa70d":"kmeans = KMeans(n_clusters=2, random_state=0, init='random', n_init=10, max_iter=10)\nkmeans.fit(X)\ntrain['Cluster']=kmeans.predict(X)\ntrain","ae88694e":"fig, ax = plt.subplots(1, 1, figsize=(15, 9))\nsns.distplot(train[train['Cluster']==0]['target'])\nsns.distplot(train[train['Cluster']==1]['target'])\nplt.title('Distribution of target value', fontsize = 20, c='black')\nplt.show()","e5a73765":"continuous_features = ['cont1', 'cont2', 'cont3', 'cont4', 'cont5', 'cont6', 'cont7',\n                       'cont8', 'cont9', 'cont10', 'cont11', 'cont12', 'cont13', 'cont14']\n\nfor continuous_feature in continuous_features:\n    fig, ax = plt.subplots(1, 1, figsize=(15, 9))\n    sns.distplot(train[continuous_feature])\n    plt.title('Distribution of ' + continuous_feature , fontsize = 20, c='black')\n    plt.show()","596adc6a":"!pip install sweetviz","26c7a3d7":"import sweetviz as sv","663f862a":"my_report = sv.analyze(train)","93e7a3bb":"my_report.show_notebook()","bb17a9cc":"# Correlation matrix\n\n- cont9 and cont1 are highly positively correlated.\n- cont6 and cont9 to cont13 are highly positively correlated.","74683ce1":"# Sweetviz","5cd70e19":"Submission file as provided gave a score of 7.44666","3442d915":"# Submission file as provided","70eda3f0":"# Lazy model\n\nBuilding a model on the data, as it is..","7fa94c5b":"Hmm.. I wanted to get clusters which would give some kind of a unimodal distribution.. but I got this instead..","bfda29b3":"# Removing the cont6 feature","e516b8e5":"Above file got a score of 0.72782","6fcb83cd":"Checking with the linear regression model.","c59a5059":"Above file got a score of 0.70495","22fd04a9":"# First look at the data\n\nThe dataset is just numbers.. numbers only. This is the first time I will be working with data where there is no context at all..\n\nThe dataset has no missing values. ","6103ddf4":"# Clustering experimentation","c2a34823":"# Exploring the continuous variables 1 - 14","bc34b507":"# Exploring the target value\n\nThe target value is has a bimodal distribution.\n\n*In statistics, a Multimodal distribution is a probability distribution with two different modes, which may also be referred to as a bimodal distribution. These appear as distinct peaks (local maxima) in the probability density function, as shown below. Categorical, continuous, and discrete data can all form bimodal distributions*\n\n*More generally, a multimodal distribution is a probability distribution with two or more modes*"}}