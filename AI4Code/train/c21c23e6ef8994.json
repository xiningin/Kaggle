{"cell_type":{"450128f0":"code","d38e8c47":"code","f7d91a0f":"code","2f013ca0":"code","0549d297":"code","bacc665f":"code","0b982c00":"code","4caccfb5":"code","98ac4805":"code","f400d1c5":"code","eb6c61a4":"code","7e942e08":"code","85b57e86":"code","1ba8ef71":"code","8587d0ae":"code","c4cf8268":"code","e907f35d":"code","98106970":"markdown","72a3a6f7":"markdown","e8888797":"markdown","b7aec5cc":"markdown","661ed014":"markdown","b4f52769":"markdown","5f60cf85":"markdown","e6588c9c":"markdown","3be3232f":"markdown"},"source":{"450128f0":"import re\nimport os\nimport nltk\nimport spacy\nimport random\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nfrom PIL import Image\nfrom wordcloud import WordCloud\n\nfrom nltk import word_tokenize\nfrom nltk.util import ngrams","d38e8c47":"# count the number of episodes in each season\ns1_ep_num = len([name for name in os.listdir('\/kaggle\/input\/scripts\/season1\/')])\ns2_ep_num = len([name for name in os.listdir('\/kaggle\/input\/scripts\/season2\/')])\n\nprint(\"Season 1 consists of {} episodes.\".format(s1_ep_num))\nprint(\"Season 2 consists of {} episodes.\".format(s2_ep_num))","f7d91a0f":"texts = \"\"\nfor ep_name in range(1, s1_ep_num+1):\n    with open(os.path.join(\"\/kaggle\/input\/scripts\/season1\/\", \n                           str(ep_name)+\".txt\")) as f:\n        texts += f.read()","2f013ca0":"len(texts)","0549d297":"text = re.sub('[^A-Za-z]+', ' ', texts)","bacc665f":"# adding screenplay notes to stopwords\nnlp = spacy.load(\"en\")\nnlp.Defaults.stop_words |= {\"d\",\"ll\",\"m\",\"re\",\"s\",\"ve\", \"t\", \"oh\", \"uh\", \"na\", \"okay\",\n                           \"didn\",\"don\",\"gon\",\"j\",\"hm\",\"um\",\"dr\",\"room\",\"int\", \"ext\", \n                           \"cut\", \"day\", \"night\", \"theme\", \"tune\",\"music\", \"ends\",\"view\",\n                            \"closeup\", 'freshly', 'squeezed', 'fade'}\nstopwords = nlp.Defaults.stop_words","0b982c00":"def plot_words(words,title,color=\"#114d1e\"):\n    counts = {}\n    for i in range(len(words)):\n        counts[words[i][0]] = words[i][1]\n    plt.figure(figsize=(8,6))\n    plt.title(title, fontsize=14)\n    plt.barh(range(len(counts)), list(counts.values()), color=color, align=\"center\")\n    plt.yticks(range(len(counts)), list(counts.keys()), fontsize=12)\n    plt.gca().invert_yaxis()\n    plt.show()\n    \ndef plot_ngrams(ngrams,title,color=\"#7a2822\"):\n    counts = {}\n    for i in range(len(ngrams)):\n        counts[\" \".join(ngrams[i][0])] = ngrams[i][1]\n    plt.figure(figsize=(8,6))\n    plt.title(title, fontsize=14)\n    plt.barh(range(len(counts)), list(counts.values()), color=color,align=\"center\")\n    plt.yticks(range(len(counts)), list(counts.keys()), fontsize=12)\n    plt.gca().invert_yaxis()\n    plt.show()","4caccfb5":"all_words = nltk.tokenize.word_tokenize(text.lower())\nall_words_no_stop = nltk.FreqDist(w.lower() for w in all_words if w not in stopwords)\nplot_words(all_words_no_stop.most_common(10), \"Top 10 frequent words. Season 1\")","98ac4805":"bigram = nltk.FreqDist(nltk.bigrams(w.lower() for w in all_words if w not in stopwords))\nplot_ngrams(bigram.most_common(10), \"Top 10 frequent bigrams. Season 1\")","f400d1c5":"trigrams = nltk.FreqDist(nltk.trigrams(w.lower() for w in all_words if w not in stopwords))\nplot_ngrams(trigrams.most_common(10), \"Top 10 frequent trigrams. Season 1\", \"#2b2e2b\")","eb6c61a4":"characters = [\n'Dale Cooper',\n'Sheriff Harry Truman',\n'Shelly Johnson',\n'Bobby Briggs',\n'Benjamin Horne',\n'Donna Hayward',\n'Audrey Horne',\n'Will Hayward',\n'Norma Jennings',\n'James Hurley',\n'Ed Hurley',\n'Pete Martell',\n'Leland Palmer',\n'Josie Packard',\n'Catherine Martell',\n'Lucy Moran',\n'Laura Palmer',\n'Lawrence Jacoby',\n'Leo Johnson',\n'Eileen Hayward',\n'Andy Brennan',\n'Mike Nelson',\n'Tommy Hawk Hill'\n'Sarah Palmer',\n'Jacques Renault',\n'Windom Earle',\n'Ronette Pulaski',\n'Phillip Jeffries',\n'Albert Rosenfield',\n'Teresa Banks',\n'Annie Blackburn',\n'Chester Desmond',\n'Gordon Cole',\n'Carl Rodd',\n'Sam Stanley',\n'Harold Smith'\n]\n\n# unique names only\nnames = set(\" \".join(set(characters)).lower().split())\n\nnlp.Defaults.stop_words |= names","7e942e08":"no_names = nltk.FreqDist(w.lower() for w in all_words if w not in stopwords)\nplot_words(no_names.most_common(10), \"Top 10 frequent words except for names\")","85b57e86":"no_names_bigram = nltk.FreqDist(nltk.bigrams(w.lower() for w in all_words if w not in stopwords))\nplot_ngrams(no_names_bigram.most_common(10), \"Top 10 frequent bigrams except for names\")","1ba8ef71":"no_names_trigram = nltk.FreqDist(nltk.trigrams(w.lower() for w in all_words if w not in stopwords))\nplot_ngrams(no_names_trigram.most_common(10), \"Top 10 frequent trigrams except for names\", \"#2b2e2b\") ","8587d0ae":"# the mask image taken from https:\/\/www.reddit.com\/r\/twinpeaks\/comments\/2mtbtf\/that_other_post_made_me_remember_heres_a_picture\/\ncooper_mask = np.array(Image.open('\/kaggle\/input\/masks\/cooper_mask.png'))\n\ndef color_func(word, font_size, position, orientation, random_state=None,\n                    **kwargs):\n    return \"hsl(0, 100%, 27%)\"\n\nwc = WordCloud(background_color=\"white\", mask=cooper_mask, max_words=1000,\n               stopwords=stopwords, contour_width=4, contour_color='steelblue')\n\nwc.generate(\" \".join(all_words_no_stop.keys()))\n\nplt.figure(figsize=(18, 10))\nplt.imshow(wc.recolor(color_func=color_func, random_state=3),interpolation=\"bilinear\")\nplt.axis(\"off\")","c4cf8268":"\"Well, exactly {} times\".format(all_words_no_stop['coffee'])","e907f35d":"\"It was mentioned {} times throughout 8 episodes\".format(all_words_no_stop['pie'])","98106970":"What will change if we remove the names?","72a3a6f7":"## And what about a famous cheery pie?","e8888797":"Unsuprisingly, these are the names of the main characters.\n\nNow let's get the most frequent bigrams and bigrams, i.e. the sequences of two and three neighbouring words respectively.","b7aec5cc":"![cherrypie.jpg](attachment:cherrypie.jpg)\n\n*Credits:* http:\/\/clipart-library.com\/clipart\/di487n96T.htm\n\nIn this notebook we will produce a basic analysis for Twin Peaks transcripts and hopefully get a few insights. So, pour yourself a damn fine cup of coffee and bear with me!","661ed014":"## Data loading and preprocessing","b4f52769":"## Most frequent words of season 1","5f60cf85":"## What's next?\nIt would be great to do some sentiment analysis on the scripts.","e6588c9c":"## Word Cloud for Season 1","3be3232f":"## How many times coffee was mentioned in season 1?"}}