{"cell_type":{"4fa22ed4":"code","28491143":"code","b7fea203":"code","b723d8c4":"code","5ef8f69a":"code","9c6d6caa":"code","f97e07eb":"code","532d47f1":"code","2d366714":"code","a4cfb8fe":"code","e5e3a87e":"code","f77a4fec":"code","27a11958":"code","d47be8c5":"code","31a895a6":"code","4bed6969":"code","9e69eab2":"code","c092edf9":"code","9f188fac":"code","900fcb34":"code","aa121ebe":"code","747f4f34":"code","2128f434":"code","68196cba":"code","6da0c8d3":"code","1e1eb584":"code","7b9ffde1":"code","bd9c417a":"code","c2396d08":"code","44279389":"code","3cd38644":"code","028641cc":"code","a7fee434":"code","385d52d7":"code","f240b5cf":"code","174c348f":"code","8df91c54":"markdown","b1029328":"markdown","a070249e":"markdown","b17d78a7":"markdown","fbea283b":"markdown","579c238e":"markdown","8ce1c896":"markdown","d82b654c":"markdown","f5e909ec":"markdown","20569974":"markdown","defea810":"markdown","a7f2018d":"markdown","cb34f343":"markdown","a7d6383d":"markdown","5b37161d":"markdown","a6eb022a":"markdown","4b25435b":"markdown"},"source":{"4fa22ed4":"import sys\nimport numpy as np\nimport pandas as pd \nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport re\nfrom tqdm import tqdm\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.utils import shuffle\nimport gensim\n\nfrom wordcloud import WordCloud, STOPWORDS\nimport nltk\nnltk.download('stopwords')\nfrom nltk.probability import FreqDist\nfrom nltk.corpus import stopwords\nfrom nltk.stem.porter import PorterStemmer\n\nimport tensorflow as tf\nfrom keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.layers import Flatten, Dropout, Dense, LSTM, Embedding\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.preprocessing.text import one_hot\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom keras.callbacks import EarlyStopping\n\nfrom sklearn.metrics import confusion_matrix, accuracy_score","28491143":"dff = pd.read_csv('..\/input\/jigsaw-toxic-comment-classification-challenge\/train.csv')\ndff.head()","b7fea203":"dff.shape","b723d8c4":"dff.isna().sum()","5ef8f69a":"dff.describe()","9c6d6caa":"dff.severe_toxic.value_counts()","f97e07eb":"dff['toxicity'] = (dff[['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']].sum(axis=1) > 0 ).astype(int)\ndff = dff[['comment_text', 'toxicity']].rename(columns={'comment_text': 'text'})\ndff.sample(5)","532d47f1":"dff.describe()","2d366714":"dff.head()","a4cfb8fe":"dff.toxicity.value_counts()","e5e3a87e":"min_len = (dff['toxicity'] == 1).sum()\ndf_undersample = dff[dff['toxicity'] == 0].sample(n=min_len, random_state=201)\ndff = pd.concat([df_undersample, dff[dff['toxicity'] == 1]])\ndff = shuffle(dff)","f77a4fec":"dff.text = dff.text.map(lambda x:x.replace('\\n', ' '))\ndff.text[:2]","27a11958":"toxic = dff[dff['toxicity'] == 1]\nnot_toxic = dff[dff['toxicity'] == 0]","d47be8c5":"wordcloud = WordCloud(width=1400, height=700, background_color='white').generate(' '.join(toxic.text.tolist()))\nfig = plt.figure(figsize=(30,10), facecolor='white')\nplt.imshow(wordcloud)\nplt.axis('off')\nplt.title('The most 100 frequent words in the toxic comments', fontsize=50)\nplt.tight_layout(pad=0)\nplt.show()","31a895a6":"wordcloud = WordCloud(width=1400, height=700, background_color='white').generate(' '.join(not_toxic.text.tolist()))\nfig = plt.figure(figsize=(30,10), facecolor='white')\nplt.imshow(wordcloud)\nplt.axis('off')\nplt.title('The most 100 frequent words in the normal comments', fontsize=50)\nplt.tight_layout(pad=0)\nplt.show()","4bed6969":"y = dff.toxicity\nx = dff.drop('toxicity', axis = 1)","9e69eab2":"texts = x.copy()\ntexts.reset_index(inplace = True, drop = True)\ntexts.head()","c092edf9":"print(sys.getrecursionlimit())","9f188fac":"sys.setrecursionlimit(6000)","900fcb34":"ps = PorterStemmer()\ncorpus = []\n\nfor i in tqdm(range(0, len(texts))) :\n    cleaned = re.sub('[^a-zA-Z]', ' ', texts['text'][i])\n    cleaned = cleaned.lower().split()\n    \n    cleaned = [ps.stem(word) for word in cleaned if not word in stopwords.words('english')]\n    cleaned = ' '.join(cleaned)\n    corpus.append(cleaned)","aa121ebe":"DIM = 100\n\nX = [d.split() for d in corpus]\nw2v_model = gensim.models.Word2Vec(sentences = X, vector_size = DIM, window = 10, min_count = 1)","747f4f34":"len(w2v_model.wv.key_to_index.keys()) ","2128f434":"w2v_model.wv.most_similar('toxic')","68196cba":"tokenizer = Tokenizer()\ntokenizer.fit_on_texts(X) ","6da0c8d3":"X = tokenizer.texts_to_sequences(X)\nX[:3]","1e1eb584":"X = pad_sequences(X, padding = 'pre', maxlen = 20)\nX[:3]","7b9ffde1":"vocab_size = len(tokenizer.word_index) + 1 \nvocab = tokenizer.word_index","bd9c417a":"def get_weights_matrix(model) :\n    weights_matrix = np.zeros((vocab_size, DIM))\n    \n    for word, i in vocab.items() :\n        weights_matrix[i] = model.wv[word]\n        \n    return weights_matrix\n\n\nembedding_vectors = get_weights_matrix(w2v_model) ","c2396d08":"model = Sequential()\n\nmodel.add(Embedding(vocab_size, output_dim = DIM, weights = [embedding_vectors], input_length = 20)) \nmodel.add(Dropout(0.2))\n\nmodel.add(LSTM(64))\nmodel.add(Dropout(0.2))\n\nmodel.add(Dense(64, activation='relu'))\nmodel.add(Dropout(0.2))\n\nmodel.add(Dense(64, activation='relu'))\nmodel.add(Dropout(0.2))\n\nmodel.add(Dense(1, activation = 'linear'))","44279389":"model.compile(loss = 'mean_squared_error', optimizer = 'adam', metrics = 'accuracy')\nmodel.summary()","3cd38644":"x_train, x_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 0)\n\nes = EarlyStopping(patience=3, \n                   monitor='loss', \n                   restore_best_weights=True, \n                   mode='min', \n                   verbose=1)\n\n# train the model \nhist = model.fit(x_train, y_train, validation_data = (x_test, y_test), epochs = 15,\n                 callbacks=es, batch_size = 32, shuffle=True)","028641cc":"plt.style.use('fivethirtyeight')\n\n# visualize the models accuracy\nplt.plot(hist.history['accuracy'])\nplt.plot(hist.history['val_accuracy'])\nplt.ylabel('accuracy')\nplt.xlabel('epoch')\nplt.legend(['train', 'val'], loc = 'upper left')\nplt.show()  ","a7fee434":"sub = pd.read_csv(\"..\/input\/jigsaw-toxic-severity-rating\/comments_to_score.csv\")","385d52d7":"new_text = tokenizer.texts_to_sequences(sub.text)\nnew_text = pad_sequences(new_text, maxlen = 20)","f240b5cf":"sub['score'] = model.predict(new_text) * 1000 \nsub.head()","174c348f":"sub[['comment_id', 'score']].to_csv(\"submission.csv\", index=False)","8df91c54":"Let's convert all the sentences to have the same length which is 20 in our case :","b1029328":"Our model will not be able to deal with text, it should have numbers as input, that's why we do first word embedding.\n\n### What is Word Embedding ?\n\nWord embedding is one of the most popular representation of document vocabulary. It is capable of capturing context of a word in a document, semantic and syntactic similarity, relation with other words, etc.\n\n\nWord Embeddings are vector representations of a particular word. ","a070249e":"Now we tokenize the sentences and convert X into sequences of numbers :","b17d78a7":"<p style=\"background-color:orange; font-family:newtimeroman; font-size:200%; text-align:center; border-radius: 10px 100px;\"><b>Importing Libraries & Data<\/b><\/p> ","fbea283b":"When dealing with text, we should first do some cleaning and stemming :\n\n### What Is Stemming ?\n\nThe process of removing a part of a word, or reducing a word to its stem or root.\n\n### Example :\n\nLet\u2019s assume we have a set of words \u2014 **send, sent and sending**. All three words are different tenses of the same root word **send**. So after we stem the words, we\u2019ll have just the one word \u2014 send. ","579c238e":"## Please If You Like This Notebook, Please don't Forget To Upvote It ;","8ce1c896":"<p style=\"background-color:orange; font-family:newtimeroman; font-size:200%; text-align:center; border-radius: 10px 100px;\"><b>Quick EDA<\/b><\/p> ","d82b654c":"Our data is not balanced.","f5e909ec":"<p style=\"background-color:orange; font-family:newtimeroman; font-size:200%; text-align:center; border-radius: 10px 100px;\"><b>Modeling & Training<\/b><\/p> ","20569974":"We get our X and y variables, then create a copy to work on it :","defea810":"We will feed these vectors as initial weights to our model then recreate these vectors to get better accuracy :","a7f2018d":"We can find similar words to a specific one, let's try with the word 'toxic' :","cb34f343":"Let's see how many words in our vocabulary :","a7d6383d":"In order to not get a RecursionError, we reset our recursionlimit to 6000.","5b37161d":"<p style=\"background-color:orange; font-family:newtimeroman; font-size:200%; text-align:center; border-radius: 10px 100px;\"><b>Text Pre-Processing<\/b><\/p> ","a6eb022a":"<p style=\"background-color:orange; font-family:newtimeroman; font-size:200%; text-align:center; border-radius: 10px 100px;\"><b>Submission<\/b><\/p> ","4b25435b":"<h2 style=\"text-align:center;color:blue;\">Objectif :<\/h2>\n\n<h3 style=\"text-align:center;\">        In this competition we will be ranking comments in order of severity of toxicity. We are given a list of comments, and each comment should be scored according to their relative toxicity. Comments with a higher degree of toxicity should receive a higher numerical value compared to comments with a lower degree of toxicity.<\/h3>\n    \n  <img src=\"https:\/\/encrypted-tbn0.gstatic.com\/images?q=tbn:ANd9GcSblnNX1zqaG70dan0DywBXM1VP75dbjCYbkA&usqp=CAU\" width=\"400\"><\/img>"}}