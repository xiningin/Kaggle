{"cell_type":{"9cddace6":"code","8fd5e85c":"code","ac1c6db4":"code","01f19f24":"code","4f424d53":"code","2d6754af":"code","342a8cb4":"code","5697b86d":"code","85005d69":"code","c123cd65":"code","469ed2c2":"code","eb5a1aa4":"code","8ded9f97":"code","b2361099":"code","1396d74b":"code","5b6a3cd8":"code","dd117dd6":"code","611e6c7e":"code","5c13066c":"code","81cde346":"code","f09716c6":"code","bb29bbe6":"code","d58a4e58":"code","ef6162eb":"code","6c5714c1":"code","78aacfd1":"code","0dc8a09f":"code","781cfef9":"code","9968a226":"code","e90f215c":"markdown","599ddd76":"markdown","2d7c0328":"markdown","0a669e51":"markdown","5e9a623f":"markdown","78445dd2":"markdown","6b744475":"markdown","9d22b364":"markdown","0e30ebfa":"markdown","fe5816f6":"markdown","66680591":"markdown","18d69a89":"markdown","305874c6":"markdown","329bbe5a":"markdown","1e379ba0":"markdown","573b7b45":"markdown","fa30d33e":"markdown","5aa38fd8":"markdown","36fbd2bb":"markdown","30b60551":"markdown","9dc40d26":"markdown","50a5c80b":"markdown","b1ddd6df":"markdown","828c33b2":"markdown","acfa32f3":"markdown"},"source":{"9cddace6":"# Import everything\nfrom mlxtend.plotting import plot_decision_regions\nimport os\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set()\nimport shap\nimport warnings\nwarnings.filterwarnings('ignore')\n%matplotlib inline\n","8fd5e85c":"#from google.colab import files\n#uploads = files.upload()\n","ac1c6db4":"#Loading the dataset\ndf = pd.read_csv('..\/input\/2class\/ChoiceAddress-2Class.csv')\n\n#Print the all of the dataframe.\ndf","01f19f24":"print(df.columns)       # \u5217\u51fa\u6bcf\u4e00\u500b column \u7684\u6a19\u982d\n","4f424d53":"df.describe()","2d6754af":"df.describe().T","342a8cb4":"p=sns.pairplot(df, hue = 'Class')","5697b86d":"# Show the joint distribution using kernel density estimation\n\n# x=\"University\", y=\"Performance\", hue=\"Class\"\ng = sns.jointplot(\n    data=df,\n    x=\"University\", y=\"Performance\", hue=\"Class\",\n    kind=\"kde\",\n)\n# x=\"Convenience\", y=\"Performance\", hue=\"Class\"\ng = sns.jointplot(\n    data=df,\n    x=\"Convenience\", y=\"Performance\", hue=\"Class\",\n    kind=\"kde\",\n)\n# x=\"Fastfood\", y=\"Performance\", hue=\"Class\"\ng = sns.jointplot(\n    data=df,\n    x=\"Fastfood\", y=\"Performance\", hue=\"Class\",\n    kind=\"kde\",\n)","85005d69":"g = sns.lmplot(\n    data=df,\n    x=\"University\", y=\"Performance\", hue=\"Class\",\n    height=5\n)\n# Use more informative axis labels than are provided by default\ng.set_axis_labels(\"University\", \"Performance\")\n\n# x=\"Convenience\", y=\"Performance\", hue=\"Class\"\ng = sns.lmplot(\n    data=df,\n    x=\"Convenience\", y=\"Performance\", hue=\"Class\",\n    height=5\n)\n# Use more informative axis labels than are provided by default\ng.set_axis_labels(\"Convenience\", \"Performance\")\n# x=\"ConvenienceStore\", y=\"Performance\", hue=\"Class\"\ng = sns.lmplot(\n    data=df,\n    x=\"Fastfood\", y=\"Performance\", hue=\"Class\",\n    height=5\n)\n# Use more informative axis labels than are provided by default\ng.set_axis_labels(\"Fastfood\", \"Performance\")","c123cd65":"df.corr()","469ed2c2":"# on this line I just set the size of figure to 12 by 10.\n# seaborn has very simple solution for heatmap\nplt.figure(figsize=(12,10))  \np=sns.heatmap(df.corr(), annot=True,cmap ='RdYlGn',vmax=0.6, center=0,)  ","eb5a1aa4":"p=sns.pairplot(df.corr())","8ded9f97":"from sklearn.preprocessing import StandardScaler\n\nscaler = StandardScaler()\nscaler.fit(df.drop('Class', axis=1))\nsc_transform = scaler.transform(df.drop('Class', axis=1))\n#scaler.fit(df)\n#sc_transform = scaler.transform(df)\nsc_df = pd.DataFrame(sc_transform)\n\n# Now you can safely use sc_df as your input features.\nsc_df","b2361099":"sc_transform","1396d74b":"from sklearn.model_selection import train_test_split\n\nX = sc_df\n\ny0 = sc_df[0]\ny = df['Class']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)\n\n\nprint(X)\nprint(\"-------------------------------------------\")\nprint(y0)\nprint(\"-------------------------------------------\")\nprint(y)","5b6a3cd8":"# Initialize an array that stores the error rates.\nfrom sklearn.neighbors import KNeighborsClassifier\n\n#error_rates = []\n\ntest_scores = []\ntrain_scores = []\n\nfor a in range(1, 40):\n    k = a\n    knn = KNeighborsClassifier(n_neighbors=k)\n    knn.fit(X_train, y_train)\n    preds = knn.predict(X_test)\n    #error_rates.append(np.mean(y_test - preds))\n\n    train_scores.append(knn.score(X_train,y_train))\n    test_scores.append(knn.score(X_test,y_test))\n\n#plt.figure(figsize=(12, 7))\n#plt.plot(range(1,40),error_rates,color='blue', linestyle='dashed', marker='o', markerfacecolor='red', markersize=10)\n#plt.title('Error Rate vs. K Value')\n#plt.xlabel('K')\n#plt.ylabel('Error Rate')","dd117dd6":"## score that comes from testing on the same datapoints that were used for training\nmax_train_score = max(train_scores)\ntrain_scores_ind = [i for i, v in enumerate(train_scores) if v == max_train_score]\nprint('Max train score {} % and k = {}'.format(max_train_score*100,list(map(lambda x: x+1, train_scores_ind))))","611e6c7e":"## score that comes from testing on the datapoints that were split in the beginning to be used for testing solely\nmax_test_score = max(test_scores)\ntest_scores_ind = [i for i, v in enumerate(test_scores) if v == max_test_score]\nprint('Max test score {} % and k = {}'.format(max_test_score*100,list(map(lambda x: x+1, test_scores_ind))))\nts=list(map(lambda x: x+1, test_scores_ind))","5c13066c":"plt.figure(figsize=(12,7))\np = sns.lineplot(range(1,40),train_scores,marker='*',label='Train Score')\np = sns.lineplot(range(1,40),test_scores,marker='o',label='Test Score')\nplt.title('Score vs. K Value')\nplt.xlabel('K Value')\nplt.ylabel('Score')","81cde346":"k = ts[0]\nprint(\"Use K =\",k)\nknn = KNeighborsClassifier(n_neighbors=k)\nknn.fit(X_train, y_train)\npreds = knn.predict(X_test)","f09716c6":"from sklearn import metrics\ncnf_matrix = metrics.confusion_matrix(y_test, preds)\np = sns.heatmap(pd.DataFrame(cnf_matrix), annot=True, cmap=\"YlGnBu\" ,fmt='g')\nplt.title('Confusion matrix', y=1.1)\nplt.ylabel('Actual label')\nplt.xlabel('Predicted label')","bb29bbe6":"from sklearn.metrics import classification_report\nprint(classification_report(y_test, preds))","d58a4e58":"#import GridSearchCV\nfrom sklearn.model_selection import GridSearchCV\n#In case of classifier like knn the parameter to be tuned is n_neighbors\nparam_grid = {'n_neighbors':np.arange(1,110)}\nknn = KNeighborsClassifier()\nknn_cv= GridSearchCV(knn,param_grid,cv=20)\nknn_cv.fit(X,y)\n\nprint(\"Best Score:\" + str(knn_cv.best_score_))\nprint(\"Best Parameters: \" + str(knn_cv.best_params_))","ef6162eb":"from sklearn import svm\nclf = svm.SVC()\n# X,y = sc_df , df[\"Class\"]\nclf.fit(X,y)\n\nfrom joblib import dump, load\ndump(clf, 'ChoiceAddress-2.joblib') ","6c5714c1":"# explain AI with Shap\nimport sklearn\n\n# knn Classifier model\n# model = KNeighborsClassifier(n_neighbors=k)\n# knn.fit(X_train, y_train)\n# preds = knn.predict(X_test)\n\n\nknn = KNeighborsClassifier()\nknn.fit(X, y)\n\nf = lambda x: knn.predict_proba(x)[:,1]\nmed = X_train.median().values.reshape((1,X_train.shape[1]))\n\n# compute SHAP values\nexplainer = shap.Explainer(f, med)\nshap_values = explainer(X_train.iloc[0:110,:])\n","78aacfd1":"shap.plots.beeswarm(shap_values)\n","0dc8a09f":"shap.plots.heatmap(shap_values)\n","781cfef9":"shap.plots.heatmap(shap_values, max_display=16)\n","9968a226":"shap.plots.heatmap(shap_values, max_display=16, feature_values=shap_values.abs.max(0))\n","e90f215c":"# \u5c07\u8cc7\u6599\u4ee5hue = \"Class\"\u70ba\u4e3b\uff0c\u756b\u51faPairPlot\u5716\u5f62","599ddd76":"# \u5206\u985e\u9810\u6e2c\u7d50\u679c\u5831\u544a","2d7c0328":"# \u5c07\u8cc7\u6599\u6a19\u6e96\u5316","0a669e51":"# \u627e\u51fa\u6e2c\u8a66\u96c6\u6700\u9ad8\u5206\u6578\u7684K\u503c","5e9a623f":"# \u6700\u4f73\u53c3\u6578 K\u503c\u7684\u63a2\u8a0e","78445dd2":"# \u5217\u51fa\u8cc7\u6599\u96c6\u7684\u5404\u9805\u7d71\u8a08\u8cc7\u6599","6b744475":"# \u8a66\u756b\u51fa\u4ee5x=\"University\", y=\"Performance\", hue=\"Class\"\u7684LmPlot\u5716\u5f62","9d22b364":"# \u5217\u51fa\u6bcf\u4e00\u500b column \u7684\u6a19\u982d","0e30ebfa":"# \u756b\u51fa K = 1 ~ 40 \u7684 Score vs. K Value \u7684\u66f2\u7dda","fe5816f6":"# Model Performance Analysis\n\n1. Confusion Matrix\n\n![1](https:\/\/www.ycc.idv.tw\/media\/mechine_learning_measure\/mechine_learning_measure.001.jpeg)\n\n\n\n\nFor Reference: https:\/\/www.ycc.idv.tw\/confusion-matrix.html","66680591":"# \u8a66\u756b\u51fa\u4ee5x=\"University\", y=\"Performance\", hue=\"Class\"\u7684joint\u5716\u5f62","18d69a89":"# \u5c0d\u8cc7\u6599\u9032\u884c\u76f8\u95dc\u4fc2\u6578\uff08corr\uff09\u5206\u6790","305874c6":"# \u5c07\u8cc7\u6599\u96c6\u7684\u76f8\u95dc\u4fc2\u6578corr\uff0c\u756b\u51fa PairPlot \u5716\u3002","329bbe5a":"# \u5c07\u8cc7\u6599\u6a94ChoiceAddress-2Class.csv\u76f4\u63a5\u653e\u5230kaggle\uff0c\u4e26\u4e14\u5c07\u8cc7\u6599\u6a94\u8b80\u9032\u4f86\uff0c\u4e26\u5217\u51fa\u4f86\u3002","1e379ba0":"# \u5c07\u76f8\u95dc\u4fc2\u6578corr\uff0c\u4f7f\u7528HeatMap \u756b\u51fa\u4f86\u3002","573b7b45":"# \u5c07\u6700\u9ad8\u5206\u6578\u7684K\u503c\u4ee3\u5165KNN\u6f14\u7b97\u6cd5\u904b\u7b97\u201c\u9810\u6e2c\u503c\u201d","fa30d33e":"# \u5c07K\u503c\u4ee51~40\u4ee3\u5165\u904b\u7b97\uff0c\u5c0b\u627e\u8a13\u7df4\u96c6\u8207\u6e2c\u8a66\u96c6\u6700\u9ad8\u5206\u7684K\u503c\uff0cError Rates\u5728\u6b64\u7121\u6cd5\u4f7f\u7528\u3002","5aa38fd8":"# Explain AI with Shapley values \n\n# \u7528 Shapley values \u89e3\u91cbAI\u6a21\u578b","36fbd2bb":"# \u5c07\u8cc7\u6599\u5207\u5206\u70ba\u8a13\u7df4\u96c6\u8207\u6e2c\u8a66\u96c6\uff0c\u6e2c\u8a66\u96c6\u7684\u6bd4\u4f8b\u70ba30%","30b60551":"\u4ee5\u6708\u5e73\u5747\u71df\u696d\u984d\uff08Performance\uff09\u4f86\u5224\u65b7\u5e97\u7684\u7b49\u7d1a\uff0c\n\n\n1.   Performance >= 500000 , Calss = 1\n2.   500000 > Performance >= 430000 , Calss = 2\n2.   Performance < 430000 , Calss = 3\n\n\n\n","9dc40d26":"# \u756b\u51fa\u6df7\u6dc6\u77e9\u9663\u5716","50a5c80b":"# **\u9078\u5740KNN-2\u985e\u5225**\n\n\u4ee5\u6708\u5e73\u5747\u71df\u696d\u984d\uff08Performance\uff09\u4f86\u5224\u65b7\u5e97\u7684\u7b49\u7d1a\n1.   Performance >= 430000 , Calss = A\n2.   Performance < 430000 , Calss = B\n\n\n\u53c3\u8003\u8cc7\u6599\uff1a\n1. https:\/\/www.kdnuggets.com\/2020\/04\/introduction-k-nearest-neighbour-algorithm-using-examples.html\n1. https:\/\/www.kaggle.com\/kwoshunhung\/step-by-step-diabetes-classification-knn-detailed\n1. https:\/\/seaborn.pydata.org\/examples\/index.html","b1ddd6df":"# \u628a\u6a21\u578b\u5b58\u8d77\u4f86","828c33b2":"# \u627e\u51fa\u8a13\u7df4\u96c6\u6700\u9ad8\u5206\u6578\u7684K\u503c","acfa32f3":"\u5f9eGoogle Colab\u532f\u5165files\u6a21\u7d44\n\u4e26\u4e0a\u50b3\u8cc7\u6599\u6a94\u5230Google Colab\n\u5728Kaggle\u57f7\u884c\u6703\u6709\u932f\u8aa4--->ModuleNotFoundError: No module named 'google.colab'\n"}}