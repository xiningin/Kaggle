{"cell_type":{"bd59fe9c":"code","c6528dba":"code","86d2a4d7":"code","a94fdd36":"code","589ab6e3":"code","85875e7b":"code","6443ab80":"code","27a385b2":"code","4c7b6216":"code","de4cf8a2":"code","2feff817":"code","79fab805":"code","71489e13":"code","f6026694":"code","446cd893":"code","f0b0da0f":"code","a7994ab3":"code","65ae00f2":"code","c7e49b52":"code","7554e499":"code","fa143240":"code","571dff31":"code","6701582f":"code","dd0a7266":"code","daddb910":"code","868bec76":"code","93f59f3e":"code","99a9ec09":"code","b2893338":"code","0f3de99a":"code","678f2fb0":"code","22f4518e":"code","4af83e09":"code","c6e43dce":"code","f78161d2":"code","64684db0":"code","ae05979e":"code","d612f81b":"code","e40528d4":"code","a8aca59f":"code","2c6186a6":"code","4da4cb70":"code","bf9525d7":"code","eb23bdbe":"code","73b96802":"code","1444ae15":"code","62cd931e":"code","5fb1b4e0":"code","72b2acd4":"code","40c2ca7c":"code","1c69b575":"code","67c44d19":"code","13369701":"code","606ef77d":"code","dbf43920":"code","038af8a8":"code","70b1d343":"code","8d6b77d4":"code","7fea00b0":"code","32115a31":"code","cbf80983":"code","257ec8c7":"code","14d8677a":"code","e3ce014d":"code","56fad278":"code","425c2b74":"code","85e65828":"code","ae9bdc95":"code","c030c3c0":"code","f260513f":"code","2ac2cf6c":"code","d7fee0d9":"code","9421bc8f":"code","6c2d9265":"code","10c22f3c":"code","d403d0e0":"code","1227492e":"code","c87ce8c3":"code","14200ed0":"code","db220bec":"code","0849edd9":"code","a82c45ce":"code","c7e18f9f":"code","217d05db":"code","70646fa9":"code","8343b6ec":"code","289c1973":"code","3c5ed439":"markdown","723fe439":"markdown","0c57186a":"markdown","23a25cb9":"markdown","3f4864d2":"markdown","b8b19112":"markdown","69523039":"markdown","b5219eb9":"markdown","65391916":"markdown","21d1863b":"markdown","bbe63b47":"markdown","27d697f3":"markdown","09354587":"markdown","9b2b0da3":"markdown","72e2c5b0":"markdown","1ba28d11":"markdown","1047d4d4":"markdown","232f4143":"markdown","876a712d":"markdown","e02f62fd":"markdown","9b696bab":"markdown","e8610cc3":"markdown","0fc7a706":"markdown"},"source":{"bd59fe9c":"import numpy as np\nimport pandas\nimport seaborn\nimport tensorflow as tf\nimport matplotlib.pyplot as plt\nfrom tabulate import tabulate\nfrom tensorflow import keras","c6528dba":"import warnings\nwarnings.filterwarnings('ignore')","86d2a4d7":"df = pandas.read_csv('..\/input\/creditcardfraud\/creditcard.csv')","a94fdd36":"df.head()","589ab6e3":"print('Number of missing values in dataset : ' + str(df.isna().sum().sum()))\nprint('Number of Fraudulent samples in  df : ' + str(df[df['Class'] == 1].shape[0]))\nprint('Number of Normal transactions in df : ' + str(df[df['Class'] == 0].shape[0]))","85875e7b":"plt.figure(figsize = (15,15))\nmat = df.corr()\nseaborn.heatmap(mat, vmin = -1.0, square = True)","6443ab80":"X = np.array(df.drop(['Class'], axis = 1))\ny = np.array(df['Class'])","27a385b2":"from sklearn.preprocessing import MinMaxScaler","4c7b6216":"print(X.shape)\nprint(y.shape)","de4cf8a2":"X = MinMaxScaler(feature_range = (-1,1)).fit_transform(X)","2feff817":"info=[[df.columns[j],X[:,j].shape[0],X[:,j].min(),X[:,j].max(), X[:,j].mean(), X[:,j].std()] for j in range(X.shape[1])]\nprint(tabulate(info, headers = ['Column', 'Count', 'Minimum', 'Maximum', 'Mean', 'Standard dev'], tablefmt =  'orgtbl'))","79fab805":"from sklearn.model_selection import train_test_split","71489e13":"X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size = 0.2, random_state = 1)","f6026694":"print(X_train.shape)\nprint(y_train.shape)","446cd893":"print(X_valid.shape)\nprint(y_valid.shape)","f0b0da0f":"print(f'Fraudulent samples in train set : {y_train[y_train == 1].shape[0]}')\nprint(f'Fraudulent samples in valid set : {y_valid[y_valid == 1].shape[0]}')","a7994ab3":"info = [\n    'normal',  # 0\n    'fraud' ,  # 1\n]","65ae00f2":"from imblearn.under_sampling import RandomUnderSampler","c7e49b52":"X_under, y_under = RandomUnderSampler(sampling_strategy = 1.00, random_state = 1).fit_resample(X_train, y_train)","7554e499":"print(X_under.shape)\nprint(y_under.shape)","fa143240":"print(X_valid.shape)\nprint(y_valid.shape)","571dff31":"value = []\nfor x in y_under :\n    value.append(info[x])","6701582f":"seaborn.histplot(pandas.DataFrame({'id' : value}), x = 'id')","dd0a7266":"from sklearn.ensemble import RandomForestClassifier","daddb910":"model = RandomForestClassifier(criterion = 'gini',\n                               max_features = None,\n                               min_samples_split = 2,\n                               random_state = 1, verbose = 1)","868bec76":"from sklearn.model_selection import RandomizedSearchCV, KFold","93f59f3e":"search = RandomizedSearchCV(estimator = model, verbose = 1,\n                            param_distributions = {'n_estimators' : [100,110, 135]},\n                            cv = KFold(n_splits = 2, shuffle=True, random_state=1))","99a9ec09":"search.fit(X_under, y_under)","b2893338":"print(search.best_params_)","0f3de99a":"model.n_estimators = search.best_params_['n_estimators']","678f2fb0":"model.fit(X_under, y_under)","22f4518e":"y_pred = model.predict(X_valid)","4af83e09":"from sklearn.metrics import classification_report\nfrom sklearn.metrics import confusion_matrix","c6e43dce":"print(classification_report(y_valid, y_pred, target_names = info))","f78161d2":"cm = confusion_matrix(y_valid, y_pred)","64684db0":"plt.figure(figsize = (8,8))\nax = seaborn.heatmap(cm, cmap=plt.cm.Greens, annot=True, square=True,\n                     xticklabels = info,\n                     yticklabels = info)\n\nax.set_ylabel('GroundTruth',fontsize=20)\nax.set_xlabel('Predictions',fontsize=20)","ae05979e":"from imblearn.over_sampling import SMOTE","d612f81b":"X_over, y_over = SMOTE(sampling_strategy = 1.00, random_state = 1).fit_resample(X_train, y_train)","e40528d4":"'''\nHad todecrease the size,\nfor faster computation.I\nwill use this much data.\n'''\nX_over, _, y_over, _ = train_test_split(X_over, y_over, test_size = 0.8, random_state = 1)","a8aca59f":"print(X_over.shape)\nprint(y_over.shape)","2c6186a6":"print(X_valid.shape)\nprint(y_valid.shape)","4da4cb70":"value = []\nfor x in y_over :\n    value.append(info[x])","bf9525d7":"seaborn.histplot(pandas.DataFrame({'id' : value}), x = 'id')","eb23bdbe":"model = RandomForestClassifier(criterion = 'gini',\n                               max_features = None,\n                               min_samples_split = 2,\n                               random_state = 1, verbose = 1)","73b96802":"search = RandomizedSearchCV(estimator = model, verbose = 5,\n                            param_distributions = {'n_estimators' : [135,150, 175]},\n                            cv = KFold(n_splits = 2, shuffle=True, random_state=1))","1444ae15":"search.fit(X_over, y_over)","62cd931e":"print(search.best_params_)","5fb1b4e0":"model.n_estimators = search.best_params_['n_estimators']","72b2acd4":"model.fit(X_over, y_over)","40c2ca7c":"y_pred = model.predict(X_valid)","1c69b575":"print(classification_report(y_valid, y_pred, target_names = info))","67c44d19":"cm = confusion_matrix(y_valid, y_pred)","13369701":"plt.figure(figsize = (8,8))\nax = seaborn.heatmap(cm, cmap=plt.cm.Blues, annot=True, square=True,\n                     xticklabels = info,\n                     yticklabels = info)\n\nax.set_ylabel('GroundTruth',fontsize=20)\nax.set_xlabel('Predictions',fontsize=20)","606ef77d":"print(X_train.shape)\nprint(y_train.shape)","dbf43920":"seaborn.set_theme(style = \"darkgrid\")","038af8a8":"plt.figure(figsize = (20,70))\ni = 0\nwhile i < 30 :\n    \n    plt.subplot(10, 3, i + 1)\n    \n    FRAUD = np.mean(X_train[y_train == 1][:,i])\n    LEGIT = np.mean(X_train[y_train == 0][:,i])\n    temp  = pandas.DataFrame({'class' : ['Fraud', 'Legit'], 'means' : [FRAUD, LEGIT]})\n    \n    seaborn.pointplot(data  = temp,x = 'class',y = 'means')\n    plt.title(f'mean dist. {df.columns[i]}', fontsize = 20)\n    i += 1\nplt.show()","70b1d343":"import cv2\nfrom tqdm import tqdm","8d6b77d4":"img = cv2.imread('..\/input\/cganimage\/1*CLgoDPChiyvl7dToEwkWGw.png')\nimg = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\nplt.figure(figsize = (20,20))\nplt.imshow(img)\nplt.axis('off')","7fea00b0":"from keras.initializers import RandomNormal\nfrom keras.layers import Activation\nfrom keras.layers import Dropout\nfrom keras.models import Model\nfrom keras.layers import Input\nfrom keras.layers import Dense\nfrom keras.layers import LeakyReLU\nfrom keras.layers import Concatenate\nfrom keras.layers import BatchNormalization","32115a31":"init = RandomNormal(mean = 0.0, stddev = 0.02)","cbf80983":"BATCH  = 64\nBUFFER = 400\nW_CLIP = 0.01\nLABELS_DIM = 1\nALPHA = 0.00005\nLATENT_SPACE = (100,)\nLABELS_SPACE = (1,)\nLATENT_DIM = 100\nN_CRITIC = 5\nN_GEN = 1","257ec8c7":"'''\nCombination of Dense, Batch\nNorm and leakyRELU with 0.2\n'''\n\ndef dens_batch_relu (hiddenx, x, batchnorm) :\n    \n    y = Dense(hiddenx, use_bias = False, kernel_initializer =  init)(x)\n    y = BatchNormalization()(y)\n    y = LeakyReLU(alpha=0.2)(y)\n    \n    return y # the output tensor","14d8677a":"'''\nGenerator model\n'''\ndef generator_model (hidden1) :\n    \n    inp = Input(LATENT_SPACE)\n    lab = Input(LABELS_SPACE)\n    con = Concatenate()([inp, lab])\n    h00 = dens_batch_relu(hiddenx = hidden1, x = con, batchnorm = True)\n    out = Dense(30, activation = 'tanh',kernel_initializer = init)(h00)\n    \n    return Model(inputs = [inp,lab], outputs = out, name = 'Generator')\n\n'''\nWC_critic model\n'''\ndef wc_critic_model (hidden1) :\n    \n    inp = Input(shape =(30,))\n    lab = Input(LABELS_SPACE)\n    con = Concatenate()([inp, lab])\n    h00 = dens_batch_relu(hiddenx = hidden1, x = con, batchnorm = True)\n    out = Dense(1,activation = 'linear',kernel_initializer = init)(h00)\n    \n    return Model(inputs = [inp,lab], outputs = out, name = 'WC_critic')","e3ce014d":"generator = generator_model(64)\nwc_critic = wc_critic_model(32)","56fad278":"from keras.utils import plot_model","425c2b74":"plot_model(generator, '.\/generator.png', show_shapes = True)","85e65828":"plot_model(wc_critic, '.\/WC_critic.png', show_shapes = True)","ae9bdc95":"import tensorflow as tf","c030c3c0":"def wc_critic_loss (critic_fake, critic_real) :\n    return  tf.reduce_mean(critic_fake) - tf.reduce_mean(critic_real)\n\ndef generator_loss (critic_fake) :\n    return -tf.reduce_mean(critic_fake)","f260513f":"wc_critic_optimizer = tf.keras.optimizers.RMSprop(learning_rate = ALPHA)\ngenerator_optimizer = tf.keras.optimizers.RMSprop(learning_rate = ALPHA)","2ac2cf6c":"train_dataset = tf.data.Dataset.from_tensor_slices((X_train , y_train))\ntrain_dataset = train_dataset.shuffle(BUFFER).batch(batch_size = BATCH)","d7fee0d9":"print(generator.summary())\nprint(wc_critic.summary())","9421bc8f":"@tf.function\ndef train_on_batch (real_image, label) :\n    \n    '''\n    Critic\n    '''\n    i = 0\n    while i < N_CRITIC :\n        with tf.GradientTape(persistent=True, watch_accessed_variables = True) as tape :\n            noise = tf.random.normal(shape = (BATCH, LATENT_DIM))\n            fake_image = generator([noise ,label], training=True)\n            opfake = wc_critic([fake_image,label], training=True)\n            opreal = wc_critic([real_image,label], training=True)\n            \n            loss = wc_critic_loss(critic_fake = opfake, critic_real = opreal)\n        \n        grads = tape.gradient(loss,wc_critic.trainable_variables)\n        wc_critic_optimizer.apply_gradients(zip(grads, wc_critic.trainable_variables))\n        \n        # ====================\n        #     Weight clip\n        # ====================\n        for x in range(len(wc_critic.trainable_variables)) :\n            wc_critic.trainable_variables[x].assign(tf.clip_by_value(wc_critic.trainable_variables[x], -W_CLIP, W_CLIP))\n        i += 1\n    \n    '''\n    Generator\n    '''\n    j = 0\n    while j < N_GEN :\n        with tf.GradientTape(persistent=True, watch_accessed_variables = True) as tape :\n            noise = tf.random.normal(shape = (BATCH, LATENT_DIM))\n            fake_image = generator([noise ,label], training=True)\n            opfake = wc_critic([fake_image,label], training=True)\n            \n            loss = generator_loss(critic_fake = opfake)\n        \n        grads = tape.gradient(loss,generator.trainable_variables)\n        generator_optimizer.apply_gradients(zip(grads, generator.trainable_variables))\n        j += 1","6c2d9265":"logs = {\n    'WC_critic_loss' : [],\n    'generator_loss' : [],\n}","10c22f3c":"def fit (EPOCHS = 100) :\n    \n    print('WC_Critic')\n    \n    for epoch in range(EPOCHS) :\n        \n        # =======================\n        #      Model Training\n        # =======================\n        print(f'{epoch} out of {EPOCHS}')\n        for n, (real_image, label) in train_dataset.enumerate() :\n            if n ==  3560 :\n                print('#....End')\n                break\n            if n%100 == 0 :\n                print('#',end='')\n            train_on_batch(real_image,label)\n        \n        # =======================\n        #      Log the losses\n        # =======================\n        for real_image, label in train_dataset.take(1) :\n            \n            noise = tf.random.normal(shape = (BATCH , LATENT_DIM))\n            fake_image = generator([noise ,label], training=False)\n            opfake = wc_critic([fake_image,label], training=False)\n            opreal = wc_critic([real_image,label], training=False)\n            \n            loss1 = wc_critic_loss(critic_fake = opfake, critic_real = opreal)\n            loss2 = generator_loss(critic_fake = opfake)\n            logs['WC_critic_loss'].append(loss1.numpy())\n            logs['generator_loss'].append(loss2.numpy())\n            \n            \n            if epoch%50 == 0 :\n                \n                # ===========================\n                #    Plot the distribution\n                # ===========================\n                plt.figure(figsize = (20,70))\n                \n                i = 0\n                while i < 30 :\n                    \n                    plt.subplot(10, 3, i+1)\n                    seaborn.kdeplot(real_image[:,i], color = 'r')\n                    seaborn.kdeplot(fake_image[:,i], color = 'b')\n                    plt.legend(['Real image', 'generated image'])\n                    plt.title(f'{df.columns[i]}', fontsize = 20 )\n                    \n                    i += 1\n                plt.show()\n                \n                # ===========================\n                #     Plot the loss func\n                # ===========================\n                plt.figure(figsize = (20,5))\n                plt.plot(logs['WC_critic_loss'])\n                plt.plot(logs['generator_loss'])\n                plt.legend(['WC_critic', 'Generator'])\n                plt.title('Loss in terms Epochs', fontsize = 20 )\n                plt.xlabel('Epochs')\n                plt.ylabel('Losses')\n                plt.show()\n                \n                # ===========================\n                #       Save the models\n                # ===========================\n                print('Saving the models....')\n                generator.save(f'.\/generator_{epoch}_epochs.h5')\n                wc_critic.save(f'.\/wc_critic_{epoch}_epochs.h5')","d403d0e0":"fit(EPOCHS = 1001)","1227492e":"def dens_batch_norm (x, hidden_nodes) :\n    \n    y = SpectralNormalization(Dense(hidden_nodes,\n                                    use_bias = False,\n                                    kernel_initializer = init,\n                                    kernel_regularizer = keras.regularizers.l2()))(x)\n    \n    y = BatchNormalization()(y)\n    y = Dropout(.2)(y)\n    y = LeakyReLU()(y)\n    \n    return y # output\n\n'''\nCritic using Spectral\nNorm to enforce 1-lip\n-chitz condition.\n'''\ndef SN_critic_model (hidden0, hidden1) :\n    \n    inp = Input(shape = (30,))\n    h00 = dens_batch_norm(x = inp, hidden_nodes = hidden0)\n    h01 = dens_batch_norm(x = h00, hidden_nodes = hidden1)\n    out = Dense(1,'linear',kernel_initializer = init)(h01)\n    \n    return Model(inputs = inp, outputs = out, name = 'SN')","c87ce8c3":"SN_critic = SN_critic_model(64, 5)\ngenerator = generator_model(64)","14200ed0":"plot_model(SN_critic, '.\/SN_critic.png', show_shapes = True)","db220bec":"print(generator.summary())\nprint(SN_critic.summary())","0849edd9":"def SN_critic_loss (critic_gen_output, critic_tar_output) :\n    return -(tf.reduce_mean(critic_tar_output) - tf.reduce_mean(critic_gen_output))","a82c45ce":"SN_critic_optimizer = keras.optimizers.RMSprop(learning_rate = ALPHA)\ngenerator_optimizer = keras.optimizers.RMSprop(learning_rate = ALPHA)","c7e18f9f":"@tf.function\ndef train_on_batch (tar_images) :\n    \n    '''\n    Train Critic for\n    C_ITER times.\n    '''\n    \n    for _ in range(C_ITER) :\n        \n        with tf.GradientTape(persistent = True) as tape :\n            \n            '''\n            Output\n            '''\n            gen_images = generator(tf.random.normal((BATCH,LATENT_SPACE)),training=True)\n            critic_gen_output = SN_critic(gen_images, training = True)\n            critic_tar_output = SN_critic(tar_images, training = True)\n            \n            '''\n            Losses\n            '''\n            loss = SN_critic_loss(critic_gen_output,critic_tar_output)\n        \n        SN_grad = tape.gradient(loss  , SN_critic.trainable_variables)\n        SN_critic_optimizer.apply_gradients(zip(SN_grad, SN_critic.trainable_variables))\n    \n    '''\n    Train  Generator\n    for G_ITER times.\n    '''\n    for _ in range(G_ITER) :\n        \n        with tf.GradientTape(persistent = True) as tape :\n            \n            '''\n            Output\n            '''\n            gen_images = generator(tf.random.normal((BATCH,LATENT_SPACE)),training=True)\n            critic_gen_output = SN_critic(gen_images, training = True)\n            \n            '''\n            Losses\n            '''\n            loss = generator_loss(critic_gen_output=critic_gen_output)\n        \n        GE_grad = tape.gradient(loss  , generator.trainable_variables)\n        generator_optimizer.apply_gradients(zip(GE_grad, generator.trainable_variables))","217d05db":"logs = {\n    'SN_critic_loss' : [],\n    'generator_loss' : [],\n}\n\ndef fit (EPOCHS = 2500) :\n    \n    print('CriticwithSpecNorm')\n    print(f'Epochs : {EPOCHS}')\n    \n    for epoch  in range(EPOCHS) :\n        \n        print(f'{epoch} out of {EPOCHS}')\n        \n        for n, tar_images in train_dataset.enumerate() :\n            \n            if n ==  3560 :\n                print('#....End')\n            if n%100 == 0 :\n                print('#',end='')\n            train_on_batch(tar_images)\n        \n        '''\n        Compute and Log\n        the losses.\n        '''\n        for tar_images in train_dataset.take(1) :\n            \n            '''\n            Output\n            '''\n            gen_images = generator(tf.random.normal((BATCH, LATENT_SPACE)), training  =  True)\n            critic_gen_output = SN_critic(gen_images, training = True)\n            critic_tar_output = SN_critic(tar_images, training = True)\n            \n            '''\n            Losses\n            '''\n            logs['SN_critic_loss'].append(SN_critic_loss(critic_gen_output,critic_tar_output))\n            logs['generator_loss'].append(generator_loss(critic_gen_output))","70646fa9":"fit(EPOCHS = 200)","8343b6ec":"plt.figure(figsize = (20,5))\nplt.plot(logs['SN_critic_loss'])\nplt.plot(logs['generator_loss'])\nplt.legend(['SN_critic', 'gen'])\nplt.xlabel('Epochs')\nplt.ylabel('Losses')\nplt.title('Losses wrt. Epochs (Spectral Norm)', fontsize = 15)","289c1973":"generator.save('.\/generator_with_spec_normal.h5')","3c5ed439":"# Aim\n<div style = \"text-align: justify\">Aim is mitigate data imbalance using <b>Undersampling, Oversampling, and WCGAN with modified Loss function and Spectral Normalization.<\/b> It is observed that our model performs better than the other two conventional techniques.<\/div>\n\n# Dataset\n<div style = \"text-align: justify\">The dataset was taken from Pozzolo et al. (2015). The dataset can be found <a href = \"https:\/\/www.kaggle.com\/mlg-ulb\/creditcardfraud\"><b>here.<\/b><\/a> The dataset consists of credit card transactions made by European cardholders in September, 2013. It has <b>492 fraudulent samples<\/b> out of 284807 total transactions, which makes upto <b>0.172%<\/b> of the entire dataset. The imbalance is clearly visible.<\/div>\n\n# Structure\n<div style = \"text-align: justify\">The dataset has 31 columns, which includes <b>Time<\/b> elapsed between each transaction and the first transaction in the dataset, <b>Amount<\/b> of transaction and <b>28 hidden features<\/b> (due to confidentiality issues). Class <b>1<\/b> represent fraud samples and class <b>0<\/b> represents normal samples.<\/div>","723fe439":"# train-test divide","0c57186a":"### fit the RandomForest model","23a25cb9":"**Hyperparameter set**","3f4864d2":"### Loss function","b8b19112":"### fit()","69523039":"<div style = \"text-align: justify\">No strong correlation was observed. So, all the features will be used for training the model. Now we should normalize the dataset to have <b>zero mean and unit deviation<\/b>. Then scale the dataframe between -1 and +1.<\/div>","b5219eb9":"# Plot the loss function","65391916":"### fit for the RandomForest model","21d1863b":"### fit() function","bbe63b47":"### tensorflow datasets","27d697f3":"# Distribution of means\n<div style = \"text-align: justify\">To see how different the features of the <b>fraud samples are from the normal samples<\/b>, we will plot the means of both the classes for each feature.<\/div>","09354587":"<div style = \"text-align: justify\">It may look that the model is doing not very bad, <b>but it is !!<\/b> The model fauled for fraud samples, which were the real target to be classified. <b>Notice that out of 9 of the fraud transactions were labeled as normal.<\/b> This can't be good for any financial purposes.<\/div>","9b2b0da3":"# Normalize the features","72e2c5b0":"# Oversampling\n<div style = \"text-align: justify\">Oversampling involves creating samples for the minority class. There are sveral ways to do this. <b>RandomOversampling<\/b> copies a minority sample and adds it back to the original set. The model however, does not learn anything new from the sampled examples. <b>SMOTE (or Sythetic Minority Oversampling Technique)<\/b> selects a minority sample and one of its k nearest-neighbours. Then <b>a line is drawn between the two ponts and a new sample is generated anywhere on this line in the feature space.<\/b><\/div>","1ba28d11":"<div style = \"text-align: justify\">Again as stated earlier, the performance of model is not good. <b>It fails to flag a significant number of fraud samples.<\/b> Also, many normal transactions get stuck as frauds.<\/div>","1047d4d4":"### Evaluate performance on testing data","232f4143":"### Conditional part\n<div style = \"text-align: justify\">For targeted generation of feature space based on the clss label, label will be added to both the Discriminator and the Generator. <b>The Discriminator judges if the feature vector is the correct representation of the label, while the Generator tries to create feature vector depending on the label.<\/b><\/div>\n\n<\/br>\n\n![](https:\/\/miro.medium.com\/max\/1050\/1*l2tSqFN0Afwizm4LgalCGg.png)","876a712d":"### evaluate on testing data","e02f62fd":"# Undersampling\n<div style = \"text-align: justify\">Undersampling involves downsampling the majority samples, to reduce imbalance in the dataset. The problem with this approach is the <b>information loss<\/b> it causes. The classifier model won't get to see all possible samples and this will affect its performance. <b>We will use RandomUnderSampler() method from imblearn package.<\/b><\/div>","9b696bab":"# WGAN with Spectral Normalization\n<div style = \"text-align: justify\">The condition for <b>critic being 1-Lipchitz<\/b> can also be applied by using Spectral normalization, instead of weight clipping. The formula for norm is,<\/div>\n\n![Spec norm](https:\/\/miro.medium.com\/max\/3600\/1*5AfPcYEv29KcJ9cMnoGU7w.jpeg)\n\n<div style = \"text-align: justify\">The spectral norm of an array A is the maximum sigular value of matrix A. The singular value of A can be calculated using <b>Singular Value Decomposition (or SVD)<\/b>.<\/div>\n\n<\/br>\n\n![img](https:\/\/miro.medium.com\/max\/3600\/0*4_rhGcIcvHV1fFy1.jpeg)\n\n<\/br>\n<div style = \"text-align: justify\">Here, U and V are the eigen vectors for AA' and A'A, repectively, and S contains the square root of these eigen values. Both AA' and A'A have the same positive eigen values. Ultimately, the weights of each layer are divided by their norms to <b>make each layer obey the 1-Lipchitz constraint.<\/b><\/div>","e8610cc3":"# WGAN with Weight Clipping\n\n### Problem\n<div style = \"text-align: justify\">The vanilla GAN models use <b>J-S Divergence<\/b> as the discriminator's cost, and had unstable training due to <b>exploding and vanishing gradients.<\/b> Due to this GANs had the possibility of suffering from mode collapse.<\/div>\n\n### Solution\n<div style = \"text-align: justify\">Solution to the issue was <b>Wasserstein distance.<\/b> It can be seen as the minimum cost path to convert generated distribution to real distribution. The cost function has smoother gradients. The equation,<\/div>\n<\/br>\n\n![func](https:\/\/miro.medium.com\/max\/3600\/1*6y-tz57odJpHh4pwRfXACw.png)\n\n<\/br>\n<div style = \"text-align: justify\">Here, f is the 1-Lipchitz function meaning the norm of its gradients is always less than or equal to 1. <b>The 1-Lipchitz constrain is implemented by clipping the weights of the discriminator.<\/b><\/div>","0fc7a706":"### Optimizers"}}