{"cell_type":{"1cb4a008":"code","49017b8a":"code","6db340d7":"code","b7038fe5":"code","dd3c4fb3":"code","1af7630c":"code","58edc414":"code","70f4f42d":"code","2944eac8":"code","a404badc":"code","f363225f":"code","3ede8c2d":"code","7262de31":"code","fba7bb07":"code","baba4e9c":"code","f838dd59":"code","12f12e7a":"code","40aa98bb":"code","5e192f9f":"code","9036f651":"code","bfbc3e2f":"code","5315ece5":"code","6c65efd1":"code","d1a71987":"code","c6d599c9":"code","a1a6a59a":"code","6e684cd2":"code","67f1035c":"code","9b98c73f":"code","40de344a":"code","88c850e2":"code","b791854f":"code","dc71a773":"code","967fabf5":"code","a695ffdd":"code","c2ab5276":"code","596fd1f2":"code","3a69ad82":"code","cc258028":"code","09a6c9d6":"code","f54e42e7":"code","f636bfcf":"code","4a836391":"code","296f48d1":"code","08125c95":"code","7b58dba7":"markdown","54cd61c7":"markdown","316869d6":"markdown","e5c4408f":"markdown","d1ae2391":"markdown","b7cc01fe":"markdown","e790f443":"markdown","80491636":"markdown","f05fc9e3":"markdown","75905db5":"markdown","f1b9a42a":"markdown","c0932f2b":"markdown","5ea24aff":"markdown","c3793950":"markdown","0222c1c1":"markdown","ec91085a":"markdown","d88e4fcf":"markdown","f73b9c1b":"markdown","948ead57":"markdown","474b3f8f":"markdown","e83fe642":"markdown","1f4c7f5a":"markdown","f272e957":"markdown","b4c08bfd":"markdown","4908c148":"markdown","11b7e302":"markdown","e447a337":"markdown","c5c1e616":"markdown","3f7d78fe":"markdown","1e967e38":"markdown","299e4f15":"markdown","5dd0a420":"markdown","ad7a27b6":"markdown","28b72837":"markdown","ab0badd4":"markdown"},"source":{"1cb4a008":"#Importing the Libraries\nimport numpy as np\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt \nimport seaborn as sns \n\nimport warnings \nwarnings.filterwarnings(\"ignore\")\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n","49017b8a":"dataset_train = pd.read_csv(\"\/kaggle\/input\/Stock_Price_Train.csv\")","6db340d7":"dataset_train.head()","b7038fe5":"dataset_train.info() #1258 verimiz var","dd3c4fb3":"train = dataset_train.loc[:,[\"Open\"]].values #T\u00fcm sat\u0131rlarda Open column se\u00e7tik\n#Values ile Numpy library ge\u00e7i\u015fi ve reshape ile de 1D array'i 2D array haline getirdik.\ntrain","1af7630c":"# Feature Scaling (Normalization) i\u015flemini yapal\u0131m \nfrom sklearn.preprocessing import MinMaxScaler\nscaler = MinMaxScaler(feature_range = (0,1)) # De\u011ferleri 0 ile 1 aras\u0131nda scale ettik. \ntrain_scaled = scaler.fit_transform(train) #Fit ile datam\u0131z\u0131 0 ile 1 aras\u0131na fitledik. Transform ile ise uygun \u015fekilde d\u00f6n\u00fc\u015ft\u00fcrd\u00fck.\ntrain_scaled","58edc414":"plt.plot(train_scaled)\nplt.xlabel(\"Sample\")\nplt.ylabel(\"Values\")\nplt.show()","70f4f42d":"X_train = []\ny_train = []\ntimesteps = 50\nfor i in range(timesteps, 1258):\n    X_train.append(train_scaled[i-timesteps:i, 0])\n    #Burda kayd\u0131rarak yap\u0131yoruz yani 50 100 diye timestepler olmayacak \n    #Onun yerine 1 50 aras\u0131 2 51 aras\u0131 gibi olacak ve bir sonrakini tahmini edecek. \n    y_train.append(train_scaled[i, 0])\nX_train, y_train = np.array(X_train), np.array(y_train)","2944eac8":"#Reshape i\u015flemini yapalm \nX_train = np.reshape(X_train, (X_train.shape[0], X_train.shape[1], 1))\nX_train","a404badc":"y_train","f363225f":"# #K\u00fct\u00fcphanelerin import edilmesi \n# from keras.models import Sequential #T\u00fcm layerlar\u0131 i\u00e7inde bulunduran mod\u00fcl\n# from keras.layers import Dense #Layer yap\u0131lar\u0131\n# from keras.layers import SimpleRNN #RNN i\u00e7in \u00f6zelle\u015fmi\u015f K\u00fct\u00fcphanemiz\n# from keras.layers import Dropout #Overfitting engellemek i\u00e7in kulland\u0131\u011f\u0131m\u0131z yap\u0131\n#                                 #Bir regularization methodudur.\n# #Modelin olu\u015fturulmas\u0131\n# regressor = Sequential()\n# \n# #Modele RNN Layer ve Dropout Eklenmesi \n# regressor.add(SimpleRNN(units = 264, activation =\"relu\",\n#                         return_sequences = True, \n#                         input_shape = (X_train.shape[1],1)))\n# regressor.add(Dropout(0.2))\n# \n# #2. RNN Layer Eklenmesi - Input shape sadece ilk layerda belirtilir.\n# regressor.add(SimpleRNN(units = 264, activation =\"relu\",\n#                         return_sequences = True)) \n# regressor.add(Dropout(0.2))\n# \n# #2. RNN Layer Eklenmesi - Input shape sadece ilk layerda belirtilir.\n# regressor.add(SimpleRNN(units = 128, activation =\"relu\",\n#                         return_sequences = True)) \n# regressor.add(Dropout(0.2))\n# \n# \n# #3. RNN Layer Eklenmesi\n# regressor.add(SimpleRNN(units = 128, activation =\"relu\",\n#                         return_sequences = True)) \n# regressor.add(Dropout(0.2))\n# \n# #5. RNN Layer Eklenmesi\n# regressor.add(SimpleRNN(units = 64))\n# regressor.add(Dropout(0.15))\n# \n# #Output Layer Eklenmesi \n# regressor.add(Dense(units = 1)) # 1 Node'a sahip Layer ekledik = Output\n# \n# #Modelin Compile (Derleme) Edilmesi\n# regressor.compile(optimizer = 'adam', loss = 'mean_squared_error')\n# \n# #Modelin Fit edilmesi ve e\u011fitilmesi \n# regressor.fit(X_train,y_train, epochs = 200, batch_size = 64) #Burada verileri 32\u015ferli olarak alacak ve her batch i\u00e7in 100 iterasyon yapacakt\u0131r. \n","3ede8c2d":"#K\u00fct\u00fcphanelerin import edilmesi \nfrom keras.models import Sequential #T\u00fcm layerlar\u0131 i\u00e7inde bulunduran mod\u00fcl\nfrom keras.layers import Dense #Layer yap\u0131lar\u0131\nfrom keras.layers import SimpleRNN #RNN i\u00e7in \u00f6zelle\u015fmi\u015f K\u00fct\u00fcphanemiz\nfrom keras.layers import Dropout #Overfitting engellemek i\u00e7in kulland\u0131\u011f\u0131m\u0131z yap\u0131\n                                #Bir regularization methodudur.\n#Modelin olu\u015fturulmas\u0131\nregressor = Sequential()\n\nregressor.add(SimpleRNN(units = 50,activation='relu', return_sequences = True, input_shape = (X_train.shape[1], 1)))\n\nregressor.add(SimpleRNN(units = 50,activation='relu', return_sequences = True))\n\nregressor.add(SimpleRNN(units = 50,activation='relu', return_sequences = True))\n\nregressor.add(SimpleRNN(units = 50,activation='tanh', return_sequences = True))\n\nregressor.add(SimpleRNN(units = 50,activation='relu', return_sequences = True))\n\nregressor.add(SimpleRNN(units = 50))\n\nregressor.add(Dense(units = 1))\n\nregressor.compile(optimizer = 'adam', loss = 'mean_squared_error')\n\nregressor.fit(X_train, y_train, epochs = 150, batch_size = 100)","7262de31":"#Test datam\u0131z\u0131n \u00e7a\u011fr\u0131lmas\u0131\ndataset_test = pd.read_csv('\/kaggle\/input\/Stock_Price_Test.csv')\ndataset_test.head()","fba7bb07":"dataset_test.info()","baba4e9c":"real_stock_price = dataset_test.loc[:, [\"Open\"]].values\nreal_stock_price","f838dd59":"# Getting the predicted stock price of 2017\ndataset_total = pd.concat((dataset_train['Open'], dataset_test['Open']), axis = 0)\ninputs = dataset_total[len(dataset_total) - len(dataset_test) - timesteps:].values.reshape(-1,1)\ninputs = scaler.transform(inputs)  # min max scaler\ninputs","12f12e7a":"X_test = []\nfor i in range(timesteps, 70):\n    X_test.append(inputs[i-timesteps:i, 0])\nX_test = np.array(X_test)\nX_test = np.reshape(X_test, (X_test.shape[0], X_test.shape[1], 1))\npredicted_stock_price = regressor.predict(X_test)\npredicted_stock_price = scaler.inverse_transform(predicted_stock_price)\n\n# Visualising the results\nplt.plot(real_stock_price, color = 'red', label = 'Real Google Stock Price')\nplt.plot(predicted_stock_price, color = 'blue', label = 'Predicted Google Stock Price')\nplt.title('Google Stock Price Prediction')\nplt.xlabel('Time')\nplt.ylabel('Google Stock Price')\nplt.legend()\nplt.show()\n# epoch = 250 daha g\u00fczel sonu\u00e7 veriyor.","40aa98bb":"import numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd \nimport math \nfrom keras.models import Sequential\nfrom keras.layers import Dense\nfrom keras.layers import LSTM\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.metrics import mean_squared_error\n\nimport warnings \nwarnings.filterwarnings(\"ignore\")\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","5e192f9f":"data = pd.read_csv(\"\/kaggle\/input\/international-airline-passengers.csv\",skipfooter = 5) \ndata.head(10)","9036f651":"data.info()","bfbc3e2f":"#G\u00f6rselle\u015ftirme\ndataset = data.iloc[:,1].values #Numpy array'a cevirdik\nplt.plot(dataset)\nplt.xlabel(\"Time\")\nplt.ylabel(\"Number of Passengers\")\nplt.title(\"International Airline Passengers\")\nplt.show()","5315ece5":"#Reshaping\ndataset = dataset.reshape(-1,1) #Bu i\u015flemi yapmazsak (142, ) formunda olur ancak keras hata verebildi\u011fi i\u00e7in (142,1) format\u0131na \u00e7evirdik\ndataset = dataset.astype(\"float32\") #int format\u0131n\u0131 float haline getirdik.\ndataset.shape","6c65efd1":"#Scaling\nscaler = MinMaxScaler(feature_range = (0,1))\ndataset = scaler.fit_transform(dataset) # Datam\u0131z\u0131 0 ile 1 aras\u0131na normalize ettik\n#T\u00fcm Neural Networklerde yap\u0131lmal\u0131d\u0131r. - H\u0131z artar, - Sonu\u00e7 iyile\u015fir","d1a71987":"#Train Test Split\ntrain_size = int(len(dataset)*0.5) #Verilerimizin yar\u0131s\u0131n\u0131 train i\u00e7in ay\u0131rd\u0131k\ntest_size = len(dataset)- train_size #Kalan yar\u0131s\u0131n\u0131 da test i\u00e7in ay\u0131rd\u0131k\ntrain = dataset[0:train_size,:] # \u0130lk sat\u0131r ile datan\u0131n yar\u0131s\u0131 aras\u0131 kadar train verisi\ntest = dataset[train_size:len(dataset),:] #Di\u011fer yar\u0131s\u0131n\u0131 da Test datas\u0131na atad\u0131k \nprint(\"Train Size : {}, Test Size: {}\".format(len(train),len(test)))","c6d599c9":"time_stemp = 10\ndataX = []\ndataY = []\nfor i in range(len(train)-time_stemp-1):\n    a = train[i:(i+time_stemp), 0]\n    dataX.append(a)\n    dataY.append(train[i + time_stemp, 0])\ntrainX = np.array(dataX)\ntrainY = np.array(dataY)  ","a1a6a59a":"dataX = []\ndataY = []\nfor i in range(len(test)-time_stemp-1):\n    a = test[i:(i+time_stemp), 0]\n    dataX.append(a)\n    dataY.append(test[i + time_stemp, 0])\ntestX = np.array(dataX)\ntestY = np.array(dataY)  ","6e684cd2":"trainX = np.reshape(trainX, (trainX.shape[0], 1, trainX.shape[1]))\ntestX = np.reshape(testX, (testX.shape[0], 1, testX.shape[1]))","67f1035c":"# model\nmodel = Sequential()\nmodel.add(LSTM(10, input_shape=(1, time_stemp))) # 10 lstm neuron(block)\nmodel.add(Dense(1))\nmodel.compile(loss='mean_squared_error', optimizer='adam')\nmodel.fit(trainX, trainY, epochs=50, batch_size=1)","9b98c73f":"trainPredict = model.predict(trainX)\ntestPredict = model.predict(testX)\n# invert predictions\ntrainPredict = scaler.inverse_transform(trainPredict)\ntrainY = scaler.inverse_transform([trainY])\ntestPredict = scaler.inverse_transform(testPredict)\ntestY = scaler.inverse_transform([testY])\n# calculate root mean squared error\ntrainScore = math.sqrt(mean_squared_error(trainY[0], trainPredict[:,0]))\nprint('Train Score: %.2f RMSE' % (trainScore))\ntestScore = math.sqrt(mean_squared_error(testY[0], testPredict[:,0]))\nprint('Test Score: %.2f RMSE' % (testScore))","40de344a":"# shifting train\ntrainPredictPlot = np.empty_like(dataset)\ntrainPredictPlot[:, :] = np.nan\ntrainPredictPlot[time_stemp:len(trainPredict)+time_stemp, :] = trainPredict\n# shifting test predictions for plotting\ntestPredictPlot = np.empty_like(dataset)\ntestPredictPlot[:, :] = np.nan\ntestPredictPlot[len(trainPredict)+(time_stemp*2)+1:len(dataset)-1, :] = testPredict\n# plot baseline and predictions\nplt.plot(scaler.inverse_transform(dataset))\nplt.plot(trainPredictPlot)\nplt.plot(testPredictPlot)\nplt.show()","88c850e2":"import numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd \nimport math \nfrom keras.models import Sequential\nfrom keras.layers import Dense\nfrom keras.layers import LSTM\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.metrics import mean_squared_error\n\nimport warnings \nwarnings.filterwarnings(\"ignore\")\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","b791854f":"stocklstm_train = pd.read_csv(\"\/kaggle\/input\/Stock_Price_Train.csv\")\nstocklstm_test = pd.read_csv(\"\/kaggle\/input\/Stock_Price_Test.csv\")","dc71a773":"stocklstm_train.head(5)","967fabf5":"stock_train = stocklstm_train.loc[:,[\"Open\"]].values #Numpy array'e \u00e7evirdik.\nstock_train","a695ffdd":"stock_test = stocklstm_test.loc[:,[\"Open\"]].values #Numpy array'e \u00e7evirdik.\nstock_test","c2ab5276":"#Reshape \nstock_train = stock_train.reshape(-1,1)\nstock_train = stock_train.astype(\"float32\")\nstock_train.shape","596fd1f2":"stock_test = stock_test.reshape(-1,1)\nstock_test = stock_test.astype(\"float32\")\nstock_test.shape","3a69ad82":"#Scaling\nscaler2 = MinMaxScaler(feature_range = (0,1))\nstock_train = scaler2.fit_transform(stock_train) # Datam\u0131z\u0131 0 ile 1 aras\u0131na normalize ettik\nstock_test = scaler2.fit_transform(stock_test)\n#T\u00fcm Neural Networklerde yap\u0131lmal\u0131d\u0131r. - H\u0131z artar, - Sonu\u00e7 iyile\u015fir\ndataset_top = np.concatenate((stock_train, stock_test), axis = 0)","cc258028":"plt.plot(stock_train)\nplt.xlabel(\"Sample\")\nplt.ylabel(\"Values\")\nplt.show()","09a6c9d6":"time_stemp = 10\ndataX2 = []\ndataY2 = []\nfor i in range(len(stock_train)-time_stemp-1):\n    b = stock_train[i:(i+time_stemp), 0]\n    dataX2.append(b)\n    dataY2.append(stock_train[i + time_stemp, 0])\ntrainX2 = np.array(dataX2)\ntrainY2 = np.array(dataY2)  ","f54e42e7":"dataX2 = []\ndataY2 = []\nfor i in range(len(stock_test)-time_stemp-1):\n    b = stock_test[i:(i+time_stemp), 0]\n    dataX2.append(b)\n    dataY2.append(stock_test[i + time_stemp, 0])\ntestX2 = np.array(dataX2)\ntestY2 = np.array(dataY2)  ","f636bfcf":"trainX2 = np.reshape(trainX2, (trainX2.shape[0], 1, trainX2.shape[1]))\ntestX2 = np.reshape(testX2, (testX2.shape[0], 1, testX2.shape[1]))","4a836391":"model = Sequential()\nmodel.add(LSTM(10, input_shape=(1, time_stemp))) # 10 lstm neuron(block)\nmodel.add(Dense(8))\nmodel.add(Dense(4))\nmodel.add(Dense(1))\nmodel.compile(loss='mean_squared_error', optimizer='adam')\nmodel.fit(trainX2, trainY2, epochs=50, batch_size=1)","296f48d1":"trainPredict2 = model.predict(trainX2)\ntestPredict2 = model.predict(testX2)\n# invert predictions\ntrainPredict2 = scaler2.inverse_transform(trainPredict2)\ntrainY2 = scaler2.inverse_transform([trainY2])\ntestPredict2 = scaler2.inverse_transform(testPredict2)\ntestY2 = scaler2.inverse_transform([testY2])\n# calculate root mean squared error\ntrainScore2 = math.sqrt(mean_squared_error(trainY2[0], trainPredict2[:,0]))\nprint('Train Score: %.2f RMSE' % (trainScore2))\ntestScore2 = math.sqrt(mean_squared_error(testY2[0], testPredict2[:,0]))\nprint('Test Score: %.2f RMSE' % (testScore2))","08125c95":"# shifting train\ntrainPredictPlot2 = np.empty_like(dataset_top)\ntrainPredictPlot2[:, :] = np.nan\ntrainPredictPlot2[time_stemp:len(trainPredict2)+time_stemp, :] = trainPredict2\n# shifting test predictions for plotting\ntestPredictPlot2 = np.empty_like(dataset_top)\ntestPredictPlot2[:, :] = np.nan\ntestPredictPlot2[len(trainPredict2)+(time_stemp*2)+1:len(dataset_top)-1, :] = testPredict2\n# plot baseline and predictions\nplt.plot(scaler2.inverse_transform(dataset_top))\nplt.plot(trainPredictPlot2)\nplt.plot(testPredictPlot2)\nplt.show()","7b58dba7":"Classification column olarak open'i kullanal\u0131m","54cd61c7":"### 3-) Many to Many\n<a href=\"https:\/\/i.hizliresim.com\/dIwDQL.jpg\"><img src=\"https:\/\/i.hizliresim.com\/dIwDQL.jpg\" alt=\"9\" border=\"0\"><\/a>\n","316869d6":"### LSTM Modelin Olu\u015fturulmas\u0131","e5c4408f":"- Kaynak olarak DATAI Team \u00e7al\u0131\u015fmalar\u0131 kullan\u0131lm\u0131\u015ft\u0131r.","d1ae2391":"- \u015eimdi modelimizi test datas\u0131 \u00fczerindeki ba\u015far\u0131s\u0131n\u0131 g\u00f6relim.","b7cc01fe":"- LSTM, RNN'in \u00f6zelle\u015fmi\u015f yap\u0131s\u0131d\u0131r.\n- RNN'den fark\u0131 daha uzun bir memory yap\u0131s\u0131 oldu\u011fundan daha fazla eski bilgiyi tutar. \n- * LSTM architecture:\n    * x: scaling of information\n    * +: Adding information\n    * sigmoid layer. Sigmoid memory den bir \u015feyi hat\u0131rlamak i\u00e7in yada unutmak i\u00e7in kullan\u0131l\u0131r. 1 yada 0'd\u0131r.\n    * tanh: activation function tanh. Tanh vanishing gradient(yava\u015f \u00f6\u011frenme - \u00e7ok k\u00fc\u00e7\u00fck gradient) problemini \u00e7\u00f6zer. \u00c7\u00fcnk\u00fc parametreleri update ederken t\u00fcrev al\u0131yorduk. Tanh'\u0131n t\u00fcrevi hemen s\u0131f\u0131r'a ula\u015fmaz.\n    * h(t-1): output of LSTM unit\n    * c(t-1): memory from previous LSTM unit\n    * X(t): input\n    * c(t): new updated memory\n    * h(t): output\n    * From c(t-1) to c(t) is memory pipeline. or only memory.\n    * Oklar vekt\u00f6r.\n    * h(t-1) ile X(t) birle\u015fmiyor parallel iki yol olarak d\u00fc\u015f\u00fcnebilirsiniz.\n*<a href=\"https:\/\/i.hizliresim.com\/uAAkoH.jpg\"><img src=\"https:\/\/i.hizliresim.com\/uAAkoH.jpg\" alt=\"9\" border=\"0\"><\/a> \n\n\n* 1) Forget gate: input olarak X(t) ve h(t-1) al\u0131r. Gelen bilginin unutulup unutulmayaca\u011f\u0131na karar verir.\n* 2) Input gate: Hangi bilginin memory de depolan\u0131p depolanmayaca\u011f\u0131na karar verir.\n* 3) Output gate: Hangi bilginin output olup olmayaca\u011f\u0131na karar verir.\n* \u00d6rne\u011fin: \n    * ... \"Boys are watching TV\"\n    * \"On the other hand girls are playing baseball.\"\n    * Forget \"boys\". new input is \"girls\" and output is \"girls\"","e790f443":"## Keras ile LSTM Modeli Implementasyonu","80491636":"- G\u00f6rd\u00fc\u011f\u00fcm\u00fcz gibi modelimiz test datam\u0131z\u0131n \u00fczerinde ve cok benzer bir e\u011fri izliyor yani bu modeli ba\u015far\u0131yla tamamlad\u0131k. ","f05fc9e3":"## RNN Model \u00c7e\u015fitleri ","75905db5":"- Havaalan\u0131ndaki yolcu say\u0131s\u0131\n- NLP'de zamanla akan konu\u015fmalar \n- Apple Siri ve Google Voice Search \n- Duygu S\u0131n\u0131fland\u0131rmas\u0131 \n    - \u00c7ok g\u00fczel ders diye bir \u00f6rne\u011fimiz olsun. Ders i\u00e7eri\u011fini ANN'de pozitif veya negatif s\u0131n\u0131fland\u0131rmas\u0131n\u0131 istesek bir\u015fey diyemez ancak RNN'de \u00f6ncesini akl\u0131nda tuttu\u011fundan g\u00fczel kelimesi bize pozitif s\u0131n\u0131fland\u0131rma olana\u011f\u0131 sa\u011flar.\n- RNN'de memory mant\u0131\u011f\u0131 vard\u0131r ve bir k\u0131s\u0131m eski verileri akl\u0131nda tutarak yeni verilerde bunlar\u0131 kullan\u0131r. \n","f1b9a42a":"### Kavramsal Yap\u0131lar\n- Gradient = Cost'a uygun \u015fekilde weight de\u011fi\u015fimi, matematiksel t\u00fcrev i\u015flemi ile weight de\u011ferini belirleme i\u015flemi \n- Exploding Gradient = Gradient de\u011ferinin \u00e7ok b\u00fcy\u00fck olmas\u0131 nedeniyle gereksiz veri \u00f6\u011frenimi, weight'e fazla \u00f6nem y\u00fckleme\n- Vanishing Gradient = Gradient de\u011ferinin \u00e7ok k\u00fc\u00e7\u00fck olmas\u0131 ve \u00e7ok yava\u015f \u00f6\u011frenme i\u015flemi \n- Bu iki durumda hatalara ve aksamalara sebep olaca\u011f\u0131ndan optimize edilmelidir. ","c0932f2b":"## Tahminler ve G\u00f6rselle\u015ftirme","5ea24aff":"**Sezonsal bir de\u011fi\u015fim oldu\u011funu ini\u015f \u00e7\u0131k\u0131\u015flardan g\u00f6rebiliyoruz.**","c3793950":"- RNN ve LSTM haf\u0131zaya sahiptir ve kendinden \u00f6nceki verileri belli oranda tuttu\u011fu i\u00e7in zamana ba\u011fl\u0131 veri ge\u00e7i\u015flerinde kullan\u0131lmak i\u00e7in \u00f6zelle\u015fmi\u015ftir (Sequence Models) \n- RNN ile LSTM fark\u0131 ise RNN k\u00fc\u00e7\u00fck bir haf\u0131zaya sahipken yani az say\u0131da eski veri tutabilirken, LSTM hem az hem de \u00e7ok say\u0131da veriyi haf\u0131zada tutabilme yetisine sahiptir. \n- ANN'in bu 2 t\u00fcr ile fark\u0131 ise herhangi haf\u0131za yap\u0131s\u0131na sahip olmamas\u0131d\u0131r. Bu nedenle ANN modelleri \u00f6nceki veriye dayal\u0131 bir tahminde bulunamamaktad\u0131r. \u00d6rne\u011fin \"Semih\" \u015feklinde bir string yap\u0131m\u0131z olsun ve 4.harfi i olan bir string'in 5.harfi ne olmal\u0131d\u0131r sorusuna ANN modelleri \"Sem\" harflerini tutmad\u0131\u011f\u0131ndan cevap veremezken, \n- RNN ve LSTM 5. harfin \"h\" olaca\u011f\u0131n\u0131 tahmin edebilmektedir. ","0222c1c1":"# LSTM - Long Short Term Memory ","ec91085a":"Bu t\u00fcrde input bir resim olabilir ve bu resimden hareketle bir\u00e7ok outputun birle\u015fmesinden 1 c\u00fcmle elde edebiliriz. \u00d6rne\u011fin Bu s\u00f6rf\u00e7\u00fc resminden hareketle \"Adam s\u00f6rf yap\u0131yor\" gibi bir output alabiliriz. ","d88e4fcf":"Open column ile \u00e7al\u0131\u015faca\u011f\u0131m\u0131zdan onu alal\u0131m","f73b9c1b":"Bu t\u00fcrde bir\u00e7ok input verilerek tek bir output \u00e7\u0131kt\u0131s\u0131 al\u0131nabilir. \u00d6rne\u011fin Input ifademiz bir c\u00fcmle olabilir ve output olarak da bir bir duygu, olumlu veya olumsuz ifadesini elde edebiliriz. ","948ead57":"## RNN Modelinin Olu\u015fturulmas\u0131","474b3f8f":"# RNN(Recurrent Neural Network)\n# LSTM(Long Short Term Memory)\n- (Recurrent Neural Network - Long Short Term Memory)\n- CNN ve ANN'e g\u00f6re daha zor bir modeldir. - NLP'de yayg\u0131n kullan\u0131l\u0131r.\n","e83fe642":"### Preprocessing the Data\n- Reshape\n- Change Type\n- Scaling \n- Train Test Split\n- Create Dataset","1f4c7f5a":"### 1-) One to Many \n<a href=\"https:\/\/i.hizliresim.com\/6mGL4W.jpg\"><img src=\"https:\/\/i.hizliresim.com\/6mGL4W.jpg\" alt=\"9\" border=\"0\"><\/a>","f272e957":"### LSTM Modeli Olu\u015fturulmas\u0131","b4c08bfd":"### Sequence Models (Zamana Ba\u011fl\u0131 model)","4908c148":"### 2-) Many to One\n<a href=\"https:\/\/i.hizliresim.com\/f3SCgY.jpg\"><img src=\"https:\/\/i.hizliresim.com\/f3SCgY.jpg\" alt=\"9\" border=\"0\"><\/a>","11b7e302":"Bu t\u00fcrde daha \u00e7ok translate gibi i\u015flemlerde kullan\u0131l\u0131r. \u00d6rne\u011fin T\u00fcrk\u00e7e bir c\u00fcmlenin ingilizce bir c\u00fcmleye \u00e7evrilmesi bu t\u00fcre \u00f6rnektir. ","e447a337":"Train ve Test olarak ay\u0131rm\u0131yorum \u00e7\u00fcnk\u00fc elimizde ayr\u0131lm\u0131\u015f bir test datas\u0131 bulunuyor ancak istenilirse train test split ile validation data da olu\u015fturmak m\u00fcmk\u00fcn. \n","c5c1e616":"- Yeniden sadece Open Column kullanaca\u011f\u0131z.","3f7d78fe":"### RNN - LSTM - ANN Kar\u015f\u0131la\u015ft\u0131rmas\u0131  ","1e967e38":"## Stock Price Dataset ile LSTM Modeli Olu\u015ftural\u0131m","299e4f15":"- Time Stemp = 10 iken de\u011ferlerimiz \n\n- Train Score: 1.26 RMSE\n- Test Score: 9.49 RMSE\n- -----------------------------------------\n- Time Stemp = 5 iken de\u011ferlerimiz \n- Train Score: 1.08 RMSE\n- Test Score: 7.84 RMSE\n- -----------------------------------------\n- Time Stemp = 15 iken de\u011ferlerimiz\n- Train Score: 1.24 RMSE\n- Test Score: 12.52 RMSE","5dd0a420":"### Tahmin yapal\u0131m ve G\u00f6rselle\u015ftirelim","ad7a27b6":"## Keras ile RNN Modeli Implementasyonu ","28b72837":"**Tahmin ve e\u011fitim verilerini g\u00f6rselle\u015ftirelim**","ab0badd4":"Tahmin de\u011ferlerimizi alal\u0131m "}}