{"cell_type":{"e6b76068":"code","335e8ad1":"code","ed83fa13":"code","ca2b0ae6":"code","5885246b":"code","fd42b4c1":"code","9d43695a":"code","6328af52":"code","82d38f95":"code","bbfe6845":"code","54c84db2":"code","6f1e84f9":"code","5509cdbb":"code","142151c2":"code","dd679694":"code","76bfa155":"code","adb5bcb6":"code","5a23c887":"markdown","60e37fef":"markdown","0132533a":"markdown","39f9ff92":"markdown","75fcc48b":"markdown","e3a42622":"markdown","cfa64c3a":"markdown"},"source":{"e6b76068":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n## To display max column and row\npd.set_option('display.max_columns', None)\npd.set_option('display.max_rows', None)\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","335e8ad1":"train = pd.read_csv(\"..\/input\/tabular-playground-series-nov-2021\/train.csv\")\ntest = pd.read_csv(\"..\/input\/tabular-playground-series-nov-2021\/test.csv\")\nsample_sub = pd.read_csv(\"..\/input\/tabular-playground-series-nov-2021\/sample_submission.csv\")","ed83fa13":"from sklearn.model_selection import train_test_split\ntrain,validation = train_test_split(train, test_size = .25, random_state = 111)","ca2b0ae6":"numr_cols = [col for col in train.columns if train[col].dtype in [\"float16\",\"float32\",\"float64\"]]\ncatg_cols = [col for col in train.columns if train[col].dtype not in [\"float16\",\"float32\",\"float64\"]]\ncatg_cols","5885246b":"train.describe().T","fd42b4c1":"y = train.target\n\n#drop original dependent var and id \nX = train.drop(['id','target'], axis=1)\n\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42, test_size=.33)","9d43695a":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import classification_report, confusion_matrix\n#model = LogisticRegression(solver='liblinear', random_state=0).fit(X, y)\nlogit = LogisticRegression(solver='liblinear', C=1.0, random_state=0).fit(X_train, y_train)\ny_pred = logit.predict(X_test)\nfrom sklearn.metrics import confusion_matrix\ncm = confusion_matrix(y_test, y_pred)\nprint (\"Confusion Matrix : \\n\", cm)\nfrom sklearn.metrics import accuracy_score\nprint (\"Accuracy : \", accuracy_score(y_test, y_pred))","6328af52":"submit= pd.DataFrame()\nsubmit['id'] = test.id\n#select features \ntest_features = test.select_dtypes(include=[np.number]).drop(['id'], axis=1).interpolate()\npreds = logit.predict(test_features)\n#unlog\/exp the prediction  \nprint('Original preds :\\n', preds[:5])\nsubmit['target'] = preds\n#final submission  \nsubmit.to_csv('logit_subm.csv', index=False)","82d38f95":"print(logit.score(X, y) )\nprint(classification_report(y, logit.predict(X)))\nprint(\"R-Square : \" ,logit.score(X_test,y_test))\n#rmse \npreds = logit.predict(X_test)\nfrom sklearn.metrics import mean_squared_error\nprint ('RMSE: ', mean_squared_error(y_test, preds))\n\nimport matplotlib.pyplot as plt\ncm = confusion_matrix(y, logit.predict(X))\n\nfig, ax = plt.subplots(figsize=(8, 8))\nax.imshow(cm)\nax.grid(False)\nax.xaxis.set(ticks=(0, 1), ticklabels=('Predicted 0s', 'Predicted 1s'))\nax.yaxis.set(ticks=(0, 1), ticklabels=('Actual 0s', 'Actual 1s'))\nax.set_ylim(1.5, -0.5)\nfor i in range(2):\n    for j in range(2):\n        ax.text(j, i, cm[i, j], ha='center', va='center', color='red')\nplt.show()","bbfe6845":"df_train = pd.read_csv('\/kaggle\/input\/tabular-playground-series-nov-2021\/train.csv')\ndf_test = pd.read_csv('\/kaggle\/input\/tabular-playground-series-nov-2021\/test.csv')\nfrom sklearn.model_selection import train_test_split\nrandom_state_val =42\ntest_size_val =0.2","54c84db2":"import seaborn as sns\nimport sys\nimport csv\nimport datetime\nimport operator\nimport joblib\nimport warnings\nwarnings.simplefilter('ignore')\n\n\nimport matplotlib.pyplot as plt\nimport xgboost as xgb\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import scale\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import precision_recall_fscore_support\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import cohen_kappa_score\nfrom collections import OrderedDict\nfrom sklearn.preprocessing import LabelEncoder, OneHotEncoder\nfrom scipy.stats import norm, skew, probplot\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.metrics import roc_curve, auc\nfrom sklearn.preprocessing import QuantileTransformer\nfrom sklearn.model_selection import RandomizedSearchCV, GridSearchCV\nfrom category_encoders.target_encoder import TargetEncoder\nfrom sklearn.model_selection import StratifiedKFold","6f1e84f9":"y_nm = 'target'\n\ndf_train_x = train.drop(y_nm, axis = 1)\ndf_train_y = pd.DataFrame(train[y_nm])\n\ndf_val_x = validation.drop(y_nm, axis = 1)\ndf_val_y = pd.DataFrame(validation[y_nm])\n\ndf_test_x = df_test\nnum_cols = [col for col in df_train_x.columns if df_train_x[col].dtype in [\"float16\",\"float32\",\"float64\"]]\ncat_cols = [col for col in df_train_x.columns if df_train_x[col].dtype not in [\"float16\",\"float32\",\"float64\"]]\ny = train[\"target\"].copy()\n\nfor cols in cat_cols:\n    enc = TargetEncoder(cols=[cols])\n    df_train_x = enc.fit_transform(df_train_x, y)\n    df_val_x = enc.transform(df_val_x)\n    df_test_x = enc.transform(df_test_x)\nscaler = QuantileTransformer()\nscaler.fit(df_train_x)\ndf_train_x = pd.DataFrame(scaler.transform(df_train_x))\ndf_val_x = pd.DataFrame(scaler.transform(df_val_x))\ndf_test_x = pd.DataFrame(scaler.transform(df_test_x))\nXGBClassifier = xgb.XGBClassifier(max_depth = 10,\n                                 learning_rate = 0.001,\n                                 n_estimators = 5000,\n                                 objective = 'binary:logistic',\n                                 tree_method = 'gpu_hist',\n                                 booster = 'gbtree',\n                                 gamma = 0.64,\n                                 max_delta_step = 3,\n                                 min_child_weight = 7,\n                                 subsample = 0.7,\n                                 colsample_bytree = 0.8,\n                                 n_jobs = -1\n                                 )\nstart = datetime.datetime.now()\nxgb = XGBClassifier.fit(df_train_x.values,\n                       df_train_y.values.ravel(),\n                       eval_set = [(df_train_x.values, df_train_y), (df_val_x.values, df_val_y)], \n                       eval_metric = 'auc',\n                       early_stopping_rounds = 15,\n                       verbose = True)\nend = datetime.datetime.now()\nend-start\nfi_vals = xgb.get_booster().get_score(importance_type = 'weight')\nfi_dict = {df_train_x.columns[i]:float(fi_vals.get('f'+str(i),0.)) for i in range(len(df_train_x.columns))}\nfeature_importance_ = sorted(fi_dict.items(), key=operator.itemgetter(1), reverse=True)\nfeature_importance_result = OrderedDict(feature_importance_)\n\nimportance = pd.DataFrame(feature_importance_)\nimportance.columns = ['feature','weight']\nimportance.head(10)","5509cdbb":"importance_ten = importance[:10]\nimportance_ten.set_index('feature').sort_values(by='weight').plot(kind='barh', figsize=(5, 5))","142151c2":"fpr, tpr, _ = roc_curve(df_val_y, xgb.predict_proba(df_val_x.values)[:, 1])\nroc_auc = auc(fpr, tpr)\n\nresult_lst =[]\nmax_roc_auc =0.\nopt_threshold =0.\nval_y_prob = xgb.predict_proba(df_val_x.values)[:, 1]\n\nfor n in range(0,50):\n    threshold = round(((n+1)*0.01),2)\n    pred_yn = val_y_prob.copy()\n    pred_yn = np.where(pred_yn > threshold, 1., 0.)\n    \n    result_dict = {}\n    precision, recall, f1_score, support = precision_recall_fscore_support(df_val_y.values.ravel(), pred_yn, average='binary')\n    accuracy = accuracy_score(df_val_y.values.ravel(), pred_yn)\n    kappa = cohen_kappa_score(df_val_y.values.ravel(), pred_yn)\n    \n    result_dict ={'Threshold': threshold, 'Accuracy': round(accuracy,4), 'Precision': round(precision,4), 'Recall': round(recall,4), 'F1_Score': round(f1_score,4),'roc_auc': round(roc_auc,4), 'Kappa': round(kappa,4)}\n    result_lst.append(result_dict)\n    \n    if max_roc_auc <= roc_auc:\n        max_roc_auc = roc_auc\n        opt_threshold = threshold\n        \n    confMat = confusion_matrix(df_val_y.values.ravel(), pred_yn, labels=[1,0])\n    \nmatric_df = pd.DataFrame(result_lst, columns=['Threshold','Accuracy', 'Precision', 'Recall', 'F1_Score','roc_auc' ,'Kappa'])\nmatric_df.to_csv('test_score.csv',sep=',', header=True, index=False, encoding='UTF-8')\n\nprint('Max roc_auc =%f, optimized_threshold=%f'%(max_roc_auc, opt_threshold))\nprint('Complete')","dd679694":"predict_xgb = xgb.predict_proba(df_train_x.values)[:,1]\npred_train = np.where(predict_xgb > opt_threshold, 1., 0.)\n\ntp, fn, fp, tn = confusion_matrix(df_train_y.values.ravel(), pred_train, labels=[1,0]).ravel()\n\nconf_matrix = pd.DataFrame(\n    confusion_matrix(df_train_y.values.ravel(), pred_train),\n    columns=['Predicted Value 0', 'Predicted Value 1'],\n    index=['True Value 0', 'True Value 1']\n)\n\nprint(\"1. Counfusion Matrix\")\nprint(conf_matrix.T)\nprint(\"\")\n\nprint(\"2. Classification Report\")\nprint(classification_report(df_train_y.values.ravel(), pred_train))","76bfa155":"from sklearn.metrics import roc_curve, auc\nfpr, tpr, _ = roc_curve(df_train_y.values.ravel(), predict_xgb)\n\nimport matplotlib.pyplot as plt\nroc_auc = auc(fpr, tpr)\n\n# Plot of a ROC curve for a specific class\nplt.figure()\nplt.plot(fpr, tpr, color='darkorange', label='ROC curve (area = %0.3f)' % roc_auc)\nplt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('ROC Curve')\nplt.legend(loc=\"lower right\")\nplt.show()","adb5bcb6":"pred_test = xgb.predict_proba(df_test_x.values)[:,1]\n\ntest_result= pd.DataFrame(pred_test)\ntest_result.columns = ['target']\npredict = test_result['target']\nId_No = df_test['id']\nsubmission = pd.DataFrame({'id': Id_No, 'target': predict})\nsubmission['target'] = submission['target'].astype('float32')\nsubmission.to_csv('submission.csv', index=False)\nsubmission.head()","5a23c887":"### Logistics regression","60e37fef":"### SVM","0132533a":"### KNN ","39f9ff92":"### XGB regression\n#### Need to tweak. #notmine.","75fcc48b":"> ","e3a42622":"#Import svm model\nfrom sklearn import svm\n#Create a svm Classifier\nsvm = svm.SVC(kernel='linear').fit(X_train, y_train)\n#Predict the response for test dataset\ny_pred = svm.predict(X_test)\n\ncm = confusion_matrix(y_test, y_pred)\nprint (\"Confusion Matrix : \\n\", cm)\nfrom sklearn.metrics import accuracy_score\nprint (\"Accuracy : \", accuracy_score(y_test, y_pred))","cfa64c3a":"from sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import confusion_matrix\nimport datetime\nfor i in range(3,5):\n    start_time= datetime.datetime.now()\n    knn = KNeighborsClassifier(n_neighbors=i).fit(X_train, y_train)\n    # Predict on dataset which model has not seen before\n    cm = confusion_matrix(y_test, y_pred)\n    print (\"Confusion Matrix : \\n\", cm)\n    print (\"knn =\", i, \" model score :\", knn.score(X, y) ,\" Accuracy : \", accuracy_score(y_test, y_pred), \" time taken:\" ,datetime.datetime.now()-start_time)"}}