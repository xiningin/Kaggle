{"cell_type":{"03eae631":"code","ba4bf2b3":"code","3e18489a":"code","7c8a195b":"code","add6bd53":"code","ff296858":"code","15382b8d":"code","ebdae35b":"code","3bf50a5e":"code","7d5f60c3":"code","0dacd435":"code","e6e01e5e":"code","cb06912e":"code","d3e52c41":"code","e5ea575b":"code","6c30b3df":"code","75b5a07b":"code","6c6181cd":"code","f412b6e2":"code","6020cf34":"code","55848ea2":"code","9d00b96e":"code","61c69cb7":"code","93fb3aea":"code","ef0ed940":"code","b556772d":"code","b5893ec8":"code","3ab4e50b":"markdown","18952410":"markdown","8d209364":"markdown","65392beb":"markdown","436c0a1e":"markdown","84b9b10c":"markdown","bcd80ae8":"markdown","dbad8f24":"markdown","cf556e4f":"markdown","3cb89d09":"markdown","40995090":"markdown","467ca06a":"markdown","5226f9b9":"markdown","98ba7553":"markdown","ef82eb5f":"markdown","93cf00df":"markdown","645f7c14":"markdown","a09fe3bd":"markdown","a9c3abbb":"markdown","347abced":"markdown","bf4d91e8":"markdown","bda97e06":"markdown","5063d72d":"markdown","edc08ef3":"markdown","00b06041":"markdown","df4fcb70":"markdown"},"source":{"03eae631":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport csv\nimport re\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport tensorflow as tf\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.models import Sequential\nfrom keras.layers import Embedding,LSTM,Dense,SpatialDropout1D\nfrom sklearn.model_selection import train_test_split\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","ba4bf2b3":"tweets = [] \nlabels = []\nstopwords = [ \"a\", \"about\", \"above\", \"after\", \"again\", \"against\", \"all\", \"am\", \"an\", \"and\", \"any\", \"are\", \"as\", \"at\", \"be\", \"because\", \"been\", \"before\", \"being\", \"below\", \"between\", \"both\", \"but\", \"by\", \"could\", \"did\", \"do\", \"does\", \"doing\", \"down\", \"during\", \"each\", \"few\", \"for\", \"from\", \"further\", \"had\", \"has\", \"have\", \"having\", \"he\", \"he'd\", \"he'll\", \"he's\", \"her\", \"here\", \"here's\", \"hers\", \"herself\", \"him\", \"himself\", \"his\", \"how\", \"how's\", \"i\", \"i'd\", \"i'll\", \"i'm\", \"i've\", \"if\", \"in\", \"into\", \"is\", \"it\", \"it's\", \"its\", \"itself\", \"let's\", \"me\", \"more\", \"most\", \"my\", \"myself\", \"nor\", \"of\", \"on\", \"once\", \"only\", \"or\", \"other\", \"ought\", \"our\", \"ours\", \"ourselves\", \"out\", \"over\", \"own\", \"same\", \"she\", \"she'd\", \"she'll\", \"she's\", \"should\", \"so\", \"some\", \"such\", \"than\", \"that\", \"that's\", \"the\", \"their\", \"theirs\", \"them\", \"themselves\", \"then\", \"there\", \"there's\", \"these\", \"they\", \"they'd\", \"they'll\", \"they're\", \"they've\", \"this\", \"those\", \"through\", \"to\", \"too\", \"under\", \"until\", \"up\", \"very\", \"was\", \"we\", \"we'd\", \"we'll\", \"we're\", \"we've\", \"were\", \"what\", \"what's\", \"when\", \"when's\", \"where\", \"where's\", \"which\", \"while\", \"who\", \"who's\", \"whom\", \"why\", \"why's\", \"with\", \"would\", \"you\", \"you'd\", \"you'll\", \"you're\", \"you've\", \"your\", \"yours\", \"yourself\", \"yourselves\" ]\n\ntrain_with_stopwords = False\n\nwith open(\"..\/input\/nlp-getting-started\/train.csv\") as csvfile: #iterate in csv files and extract tweets and relevant labels\n    reader = csv.reader(csvfile, delimiter=',')\n    next(reader) #skip table header\n    for row in reader:\n        labels.append(int(row[4]))\n        url = re.compile(r'https?:\/\/\\S+|www\\.\\S+') #remove url if any in tweet message.\n        tweet = url.sub(r'',row[3])\n        if train_with_stopwords == True:\n            for word in stopwords:\n                token = \" \" + word + \" \"\n                tweet = tweet.replace(token, \" \")\n        tweets.append(tweet)\nprint(f'Total training set = {len(tweets)} tweets')\nprint(f'Total training set = {len(labels)} tweets')","3e18489a":"#for total number of training size = 7613 tweets, first let's train on 6,000 samples and validate on 1,613 samples. After fine tuning model until it reach desired performance, later we'll train on total training set.\ntrain_tweets , val_tweets, train_labels , val_labels = train_test_split(tweets,labels, train_size = 6000 , stratify = labels, random_state=42)\nprint(f'Training set size = {len(train_tweets)}')\nprint(f'Validation set size = {len(val_tweets)}')","7c8a195b":"embedding_dim = 100 #to match with glove model we import\nmax_length = 50\ntrunc_type='post'\npadding_type='post'\noov_tok = \"<OOV>\"","add6bd53":"tokenizer = Tokenizer()\ntokenizer.fit_on_texts(tweets)\nword_index = tokenizer.word_index\nvocab_size = len(word_index) #set vocab size to maximum\n\ntrain_sequences = tokenizer.texts_to_sequences(train_tweets)\ntrain_padded = pad_sequences(train_sequences, padding = padding_type, truncating = trunc_type, maxlen = max_length)\n\nval_sequences = tokenizer.texts_to_sequences(val_tweets)\nval_padded = pad_sequences(val_sequences, padding = padding_type, truncating = trunc_type, maxlen = max_length)","ff296858":"sns.set(style = 'darkgrid',\n       context = 'notebook',\n       palette = 'muted',\n       )\nsns.despine(left=True)","15382b8d":"df = pd.read_csv('..\/input\/nlp-getting-started\/train.csv')\ndf['length'] = df['text'].str.len() # compute tweet message count\ndf['word_counts'] = df['text'].str.split().apply(len)\ndf.head()","ebdae35b":"print(f\"Training dataset shape\\n {df.shape[0]} rows, {df.shape[1]} columns\")\nprint('\\nTotal missing values')\nprint(df.isnull().sum())\ndf = df.drop(columns = ['keyword','location'])","3bf50a5e":"word_count = tokenizer.word_counts\nword_count_sorted = {k:v for k,v in sorted(word_count.items(),reverse=True,key=lambda item: item[1])}\nmost_common_word = []\nmost_common_word_count = []\ni = 0\nfor k,v in word_count_sorted.items():\n    if i == 20:\n        break\n    most_common_word.append(k)\n    most_common_word_count.append(v)\n    i += 1","7d5f60c3":"f, axes = plt.subplots(2, 2, figsize=(15, 10))\nsns.despine(left=True)\n\n#Class distribution graph\nax1 = sns.countplot(x='target',data=df, ax = axes[0,0])\nax1.set_title('Class distribution',size = 15, y = 1.02)\nax1.set_xlabel('Target')\nax1.set_ylabel('Number of observation')\n\n#Distribution of tweet message length of Real and Fake Disaster\nax2 = sns.kdeplot(df[df['target'] == 1].length ,shade = True, label = 'Real Disaster!' ,color ='b', ax=axes[0,1])\nax3 = sns.kdeplot(df[df['target'] == 0].length , shade = True, label = 'Fake!', color = 'r', ax=axes[0,1])\nax2.set_xlabel('Tweet message length (number of characters)')\nax2.set_title('Distribution of tweet message length of Real and Fake Disaster',size = 15, y = 1.02)\n\n#Distribution of Number of words per tweets of Real and Fake Disaster\nax3 = sns.kdeplot(df[df['target'] == 1].word_counts ,shade = True, label = 'Real Disaster!' ,color ='b', ax=axes[1,0])\nax4 = sns.kdeplot(df[df['target'] == 0].word_counts , shade = True, label = 'Fake!', color = 'r', ax=axes[1,0])\nax3.set_xlabel('Number of words per tweets')\nax3.set_title('Distribution of Number of words per tweets of Real and Fake Disaster',size = 15, y = 1.02)\n\n#Word counts\nax5 = sns.barplot(x=most_common_word,y=most_common_word_count,\n            palette = sns.cubehelix_palette(n_colors=20,rot=-.3,reverse=True), ax=axes[1,1])\nax5.set_xticklabels(ax5.get_xticklabels(),\n                  rotation=90);\nax5.set_ylabel('Number of observation')\nax5.set_title('20 Most frequent words',size = 15, y = 1.02)\n\nplt.tight_layout(pad=2.0);","0dacd435":"#Special thanks to Laurence Moroney for his hosted site for easier download\n\n#This block of code simply map our words in tokenizer to 100-dimensional vectors with pretrained weight\nembeddings_index = {};\nwith open('..\/input\/standford-glove\/glove.6B.100d.txt') as f:\n    for line in f:\n        values = line.split();\n        word = values[0];\n        coefs = np.asarray(values[1:], dtype='float32');\n        embeddings_index[word] = coefs;\n\nembeddings_matrix = np.zeros((vocab_size+1, embedding_dim));\nfor word, i in word_index.items():\n    embedding_vector = embeddings_index.get(word);\n    if embedding_vector is not None:\n        embeddings_matrix[i] = embedding_vector;","e6e01e5e":"import matplotlib.image  as mpimg\nimport matplotlib.pyplot as plt\n\n#-----------------------------------------------------------\n# Retrieve a list of list results on training and test data\n# sets for each training epoch\n#-----------------------------------------------------------\ndef plot_lr_lc(history):\n    acc=history.history['accuracy']\n    val_acc=history.history['val_accuracy']\n    loss=history.history['loss']\n    val_loss=history.history['val_loss']\n    \n    epochs=range(len(acc)) # Get number of epochs\n    \n    #------------------------------------------------\n    # Plot learning rate vs loss to find optimal learning rate\n    #------------------------------------------------\n    plt.semilogx(history.history[\"lr\"], history.history[\"loss\"])\n    plt.axis([1e-5, 10, 0, 1])\n    plt.xlabel('Learning Rate')\n    plt.ylabel('Loss')\n    plt.title(str(history))    \n    plt.figure()\n    \n    #------------------------------------------------\n    # Plot training and validation accuracy per epoch\n    #------------------------------------------------\n    plt.plot(epochs, acc, 'r')\n    plt.plot(epochs, val_acc, 'b')\n    plt.title('Training and validation accuracy')\n    plt.xlabel(\"Epochs\")\n    plt.ylabel(\"Accuracy\")\n    plt.legend([\"Accuracy\", \"Validation Accuracy\"])\n    plt.axis([0,80,0,1])\n    plt.figure()\n    \n    #------------------------------------------------\n    # Plot training and validation loss per epoch\n    #------------------------------------------------\n    plt.plot(epochs, loss, 'r')\n    plt.plot(epochs, val_loss, 'b')\n    plt.title('Training and validation loss')\n    plt.xlabel(\"Epochs\")\n    plt.ylabel(\"Loss\")\n    plt.legend([\"Loss\", \"Validation Loss\"])\n    plt.axis([0,80,0,5])\n    plt.figure()\n\n\n# Expected Output\n# A chart where the validation loss does not increase sharply!","cb06912e":"tf.keras.backend.clear_session()\ntf.random.set_seed(51)\nnp.random.seed(51)\n\n#hyperparameter to tune\nl2_weight = 0.01\ndropout_rate = 0.3\ninitial_lr = 0.001\n\nlr_schedule = tf.keras.callbacks.LearningRateScheduler(\n    lambda epoch: initial_lr * 10**(epoch\/20))\n\nmodel = tf.keras.Sequential([\n    tf.keras.layers.Embedding(vocab_size+1,embedding_dim,input_length = max_length, weights = [embeddings_matrix], trainable=False),\n    tf.keras.layers.Conv1D(16, 5, activation='relu'),\n    tf.keras.layers.Dropout(dropout_rate),\n    tf.keras.layers.MaxPooling1D(pool_size=4),\n    tf.keras.layers.Bidirectional(LSTM(32,dropout = dropout_rate)),\n    tf.keras.layers.Dense(8, activation='relu',kernel_regularizer = tf.keras.regularizers.l2(l2_weight)),\n    tf.keras.layers.Dense(1, activation='sigmoid')\n])\n\n\nmodel.compile(loss='binary_crossentropy', \n              optimizer = tf.keras.optimizers.SGD(lr=initial_lr ,momentum=0.9) ,\n              metrics=['accuracy']\n             )\nmodel.summary()","d3e52c41":"num_epochs = 50\nhistory = model.fit(train_padded, np.array(train_labels), epochs=num_epochs, \n                    validation_data=(val_padded, np.array(val_labels)), \n                    verbose=1,\n                   callbacks = [lr_schedule]\n                   )","e5ea575b":"plot_lr_lc(history)","6c30b3df":"tf.keras.backend.clear_session()\ntf.random.set_seed(51)\nnp.random.seed(51)\n#hyperparameter to tune\nl2_weight = 0.01\ndropout_rate = 0.3\n\nmodel = tf.keras.Sequential([\n    tf.keras.layers.Embedding(vocab_size+1,embedding_dim,input_length = max_length, weights = [embeddings_matrix], trainable=False),\n    tf.keras.layers.Conv1D(16, 5, activation='relu'),\n    tf.keras.layers.Dropout(dropout_rate),\n    tf.keras.layers.MaxPooling1D(pool_size=4),\n    tf.keras.layers.Bidirectional(LSTM(32,dropout = dropout_rate)),\n    tf.keras.layers.Dense(8, activation='relu',kernel_regularizer = tf.keras.regularizers.l2(l2_weight)),\n    tf.keras.layers.Dense(1, activation='sigmoid')\n])\n\n\nmodel.compile(loss='binary_crossentropy', optimizer = tf.keras.optimizers.SGD(lr=3e-2 ,momentum=0.9) ,metrics=['accuracy'])\nmodel.summary()","75b5a07b":"num_epochs = 100\n\n\n#Early stop the training if there's no improvement in model performance for 20 epochs\nearly = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=20)\n\n\n#Reduce model learning rate if validation loss reach plateau\nreduce_lr = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.2,\n                              patience=20, min_lr=0.001)\n\n#Checkpoint callback for model with lowest validation accuracy\ncheckpoint_filepath = 'checkpoint'\nmodel_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n    filepath=checkpoint_filepath,\n    save_weights_only=True,\n    monitor='val_acc',\n    save_best_only=True)\n\n\nhistory = model.fit(train_padded, np.array(train_labels), epochs=num_epochs, \n                    validation_data=(val_padded, np.array(val_labels)), \n                    verbose=1,\n                    callbacks= [early,model_checkpoint_callback]\n                   )","6c6181cd":"#-----------------------------------------------------------\n# Retrieve a list of list results on training and test data\n# sets for each training epoch\n#-----------------------------------------------------------\n\nacc=history.history['accuracy']\nval_acc=history.history['val_accuracy']\nloss=history.history['loss']\nval_loss=history.history['val_loss']\n# lr = history.history['lr']\n\nepochs=range(len(acc)) # Get number of epochs\n\n#------------------------------------------------\n# Plot lr change vs epoch\n#------------------------------------------------\n# plt.plot(epochs,lr)\n# plt.title('lr change vs epoch')\n# plt.xlabel(\"Epochs\")\n# plt.ylabel(\"learning rate\")\n# plt.figure()\n\n#------------------------------------------------\n# Plot training and validation accuracy per epoch\n#------------------------------------------------\nplt.plot(epochs, acc, 'r')\nplt.plot(epochs, val_acc, 'b')\nplt.title('Training and validation accuracy')\nplt.xlabel(\"Epochs\")\nplt.ylabel(\"Accuracy\")\nplt.legend([\"Accuracy\", \"Validation Accuracy\"])\nplt.axis([0,len(history.history['loss']),.5,1])\nplt.figure()\n\n#------------------------------------------------\n# Plot training and validation loss per epoch\n#------------------------------------------------\nplt.plot(epochs, loss, 'r')\nplt.plot(epochs, val_loss, 'b')\nplt.title('Training and validation loss')\nplt.xlabel(\"Epochs\")\nplt.ylabel(\"Loss\")\nplt.legend([\"Loss\", \"Validation Loss\"])\nplt.axis([0,len(history.history['loss']),0,1])\nplt.figure()","f412b6e2":"model.load_weights(r'.\/' + checkpoint_filepath)","6020cf34":"#load test set with same preprocessing steps as training set\ntweets_test = [] \nids = []\nwith open(\"..\/input\/nlp-getting-started\/test.csv\") as csvfile: #iterate in csv files and extract tweets and relevant labels\n    reader = csv.reader(csvfile, delimiter=',')\n    next(reader) #skip table header\n    for row in reader:\n        ids.append(int(row[0]))\n        url = re.compile(r'https?:\/\/\\S+|www\\.\\S+') #remove url if any in tweet message.\n        tweet = url.sub(r'',row[3])\n        tweets_test.append(tweet)\nprint(f'Total training set = {len(tweets_test)} tweets')\nprint(f'Total training set = {len(ids)} tweets')","55848ea2":"test_sequences = tokenizer.texts_to_sequences(tweets_test)\ntest_padded = pad_sequences(test_sequences, padding = padding_type, truncating = trunc_type, maxlen = max_length)","9d00b96e":"y_predict = model.predict(test_padded)\ny_predict = np.round(y_predict,0).astype(int).flatten()","61c69cb7":"submission = pd.DataFrame({'id' : ids, 'target' : y_predict})\nprint(submission.head())\nsubmission.to_csv('submission.csv',index=False)","93fb3aea":"train_tweets_full = np.vstack((train_padded,val_padded))\ntrain_labels_full = np.concatenate((np.array(train_labels),np.array(val_labels)))","ef0ed940":"num_epochs = 30\n\nhistory = model.fit(train_tweets_full, train_labels_full, epochs=num_epochs, \n                    verbose=1,\n                   )","b556772d":"y_predict_full = model.predict(test_padded)\ny_predict_full = np.round(y_predict_full,0).astype(int).flatten()","b5893ec8":"submission_full = pd.DataFrame({'id' : ids, 'target' : y_predict_full})\nprint(submission_full.head())\nsubmission_full.to_csv('submission_full.csv',index=False)","3ab4e50b":"## Prediction on test set","18952410":"It's been observed that. With learning rate ~ 3x10^-2, the model tend to have lowest loss and more stable than larger learning rate. \n\nHowever our model able to reach highest accuracy around 80%","8d209364":"Let's make prediction on test set and see how our model perform","65392beb":"## Baseline model","436c0a1e":"# Conclusion","84b9b10c":"## Data Preprocessing","bcd80ae8":"## Exploratory Data Analysis","dbad8f24":"For training and evaluating, I've splited full training set into\n1. Training set : 6,000 tweets and labels for model training purpose\n1. Validation set : the rest (7,613-6,000 = 1,613 tweets) for evaluating and fine tuning the model\n\nAfter we tune the model to have desired performance, I'll train the model again on full training set","cf556e4f":"Now we've come to the main preprocessing steps before feeding into the model.\n\n1. We use [keras.preprocessing.text.Tokenizer](https:\/\/www.tensorflow.org\/api_docs\/python\/tf\/keras\/preprocessing\/text\/Tokenizer) class to tokenize words in tweet message into number (which can be understood by computer).\n1. By calling [.fit_on_texts](https:\/\/www.tensorflow.org\/api_docs\/python\/tf\/keras\/preprocessing\/text\/Tokenizer#fit_on_texts) method on full training set (tweets) tokenizer object will fit itself on training set and generate dictionary of key value pair which keys is words and value is index corresponding to each word (when we don't specify num_words argument while construcing tokenizer object it will have total unique word from tweets in its dictionary).\n1. By calling [.texts_to_sequences](https:\/\/www.tensorflow.org\/api_docs\/python\/tf\/keras\/preprocessing\/text\/Tokenizer#texts_to_sequences) method on tweets list (both training and validation set) will convert sentence into sequences of number according to tokenizer dictionary.\n1. Finally, we call [pad_sequences](https:\/\/www.tensorflow.org\/api_docs\/python\/tf\/keras\/preprocessing\/sequence\/pad_sequences) method on train and validation sequences to pad it to have same length as our max_length parameter specified.","3cb89d09":"So far we've developed a model to classify whether a tweeted tweet will be real disaster or not. With existing progress we are able to reach 83% accuracy on test set which is quite good. However I can identify some room for improvement as follow...\n\n* try removing some uninterpretable phrases\/words likes \\x89\u00db\u00d3 or \\x89\u00db_\n* try different network architecture\n* try different pretrained weight from GloVe variances","40995090":"Now, it's time to train our model on full training set and see if it could learn more!","467ca06a":"**Self learning notebook for NLP**","5226f9b9":"We focus on analysis on full training set. It's my intention to not include test set in analysis to minimize chance of information leakage.","98ba7553":"* In training set, we've comparative amount of Real and Fake news. There should not be any problem with class imbalance\n* In aspect of tweet message length, there is no significant different between real and fake disaster. Both of them median around 135 charaters\n* For number of words per tweet, for average both real and fake disaster tweet has around 10-20 words per tweets. However, there is no significant different for distribution of each group ","ef82eb5f":"With model trained on part of training set our model have reached about 80.2% accuracy on test set which is pretty good enough","93cf00df":"![twitter banner](https:\/\/www.e-learningmatters.com\/wp-content\/uploads\/2016\/10\/Twitter-banner.jpg)","645f7c14":"## Train model on full training set","a09fe3bd":"After training for 6 epochs, model training loss continue to decrease. On the other hand, validation loss start to break away. This is a sign of model are going to overfit on training set. So I decide to stop the training and load weights of the model at the time which model has lowest validation loss.","a9c3abbb":"Let's train our model with optimal learning rate (3x10^-2) and see if we can improve the performance\n\nAt this time, I also set 2 callback event for training\n1. EarlyStopping callback : to save training time if there is no performance improvement\n1. ModelCheckpoint callback : to load model weight which result in lowest validation accuracy","347abced":"I decided to use pretrianed word vectors from [GloVe : Global Vectors for Word Representation from Stanford university](https:\/\/nlp.stanford.edu\/projects\/glove\/) \n\n> GloVe is an unsupervised learning algorithm for obtaining vector representations for words. Training is performed on aggregated global word-word co-occurrence statistics from a corpus, and the resulting representations showcase interesting linear substructures of the word vector space.\n\n*Please note that, we use glove6B with 100-dimensional vector for our training.*","bf4d91e8":"## Train final model","bda97e06":"Our model have improved in another step. At time of submission model has accuracy about 83% on test set.","5063d72d":"Tweet data is stored in ***train.csv*** file, we'll read it into 2 lists of ...\n1. tweets : contains each tweet message\n1. labels : contains label of \"0\" (not disaster) or \"1\" (disaster) corresponding to each tweet message in tweets list\n\nI've included preprocessing step to filter out url from tweet message if they have any.\n\nI've also try to filter out stopwords (which is listed below) from tweet and train the model. The outcome is not promising, Therefore, I decide to keep stopwords in tweet message as it be.","edc08ef3":"This notebook will focus on analyzing tweet message, Therefore I'll drop \"keyword\" and \"location\" attribute out of our consideration","00b06041":"# Introduction\n\nTwitter has become an important communication channel in times of emergency.\nThe ubiquitousness of smartphones enables people to announce an emergency they\u2019re observing in real-time. Because of this, more agencies are interested in programatically monitoring Twitter (i.e. disaster relief organizations and news agencies).\n\nBut, it\u2019s not always clear whether a person\u2019s words are actually announcing a disaster. Take this example:\n\nThe author explicitly uses the word \u201cABLAZE\u201d but means it metaphorically. This is clear to a human right away, especially with the visual aid. But it\u2019s less clear to a machine.\n\n## Covered Topics\n1. Data preprocessing\n    * for Exploratory Data Analysis : I'll import dataset into pandas DataFrame for better understanding\n    * for modelling : this will be done in tensorflow\n1. Exploratory Data Analysis\n1. Model building using Tensorflow","df4fcb70":"# Modeling"}}