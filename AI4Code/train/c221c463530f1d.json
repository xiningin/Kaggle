{"cell_type":{"c6cc3ffa":"code","f7ef8863":"code","ac7e6a88":"code","bc032a78":"code","4dd4006d":"code","9e3a8ce3":"code","92bfb2f3":"code","366d3501":"code","971e2b19":"code","08cc4ad6":"code","76633a06":"code","eca088a7":"code","51b2fbf9":"code","defe1799":"code","537e1bef":"code","cec669a5":"code","2c001117":"code","45c7d31b":"code","95fe8131":"code","c5e0dd54":"code","fc7b446d":"code","0b380d04":"code","416cab1e":"code","e93a4bfd":"code","daf4b9d8":"code","7ff40508":"code","7955b9cd":"code","c6ca72af":"code","e02f63b9":"code","9e65ef73":"markdown","b1968dd9":"markdown","1aae8e28":"markdown","3dca16c9":"markdown","7a28a519":"markdown","f836a5b4":"markdown","308ce69d":"markdown","0dd0c788":"markdown","36f48399":"markdown","e9a45423":"markdown","ac1b0d0d":"markdown","9346d096":"markdown","68f1b7d3":"markdown","256c4a21":"markdown"},"source":{"c6cc3ffa":"%%capture\n!pip install pyspark\nimport numpy as np \nimport pandas as pd\nimport os\nimport seaborn as sns\nimport plotly.express as px\n# these 2 lines fix a sporatic loading error in plotly\nfrom plotly.offline import plot, iplot, init_notebook_mode\ninit_notebook_mode(connected=True)\nsns.set_style('darkgrid')\n# pyspark\nfrom pyspark import SparkConf, SparkContext\nfrom pyspark.sql import SparkSession, SQLContext\n\nfrom pyspark.sql.types import *\nimport pyspark.sql.functions as F\nfrom pyspark.sql.functions import udf, col\n\n# from pyspark.ml.regression import LinearRegression\n# from pyspark.mllib.evaluation import RegressionMetrics\n\n# from pyspark.ml.tuning import ParamGridBuilder, CrossValidator, CrossValidatorModel\n# from pyspark.ml.feature import VectorAssembler, StandardScaler\n# from pyspark.ml.evaluation import RegressionEvaluator\n\nfrom sklearn.pipeline import Pipeline\n# from sklearn.manifold import TSNE\n# from sklearn.decomposition import PCA\n\nCSV_FILE= '\/kaggle\/input\/spotify-huge-database-daily-charts-over-3-years\/Final database.csv'\n# CSV_FILE= '\/kaggle\/input\/spotify-huge-database-daily-charts-over-3-years\/Database to calculate popularity.csv'\n# df = pd.read_csv(CSV_FILE)\n# df.head()","f7ef8863":"spark = SparkSession.builder.master(\"local[2]\").appName(\"Spotify-Huge-Dataset\").getOrCreate() #.enableHiveSupport()\nspark","ac7e6a88":"sc = spark.sparkContext\nsqlContext = SQLContext(sc)","bc032a78":"df = spark.read.option(\"header\", True).csv(CSV_FILE)\ndf = df.withColumn(\"Release_date\", F.to_date(\"Release_date\", \"yyyy-MM-dd\"))\nnumerical_features = ['danceability', 'energy', 'instrumentalness', 'valence', 'liveliness', 'speechiness', 'acoustics',\n                      'speechiness', 'acoustics', 'instrumentalness', 'liveliness', 'valence', 'tempo', 'duration_ms', \n                      'time_signature', 'Days_since_release', 'n_words']\nfor c in numerical_features:\n    df = df.withColumn(c, df[c].cast(\"float\"))\ncols_to_drop = ['syuzhet_norm', 'bing_norm', 'afinn_norm', 'nrc_norm', 'syuzhet', 'bing'] \nfor c in cols_to_drop:\n    df.drop(c).collect()\ndf.printSchema()\n# df.show(n=1, truncate=False, vertical=True)","4dd4006d":"# each count is a song that was in the top 200 most played on a day on spotify during the last 3 years\nresult_df = df.groupBy(\"Artist\") \\\n              .count() \\\n              .orderBy(\"count\", ascending=False) \\\n              .limit(10) \\\n              .toPandas()\npx.bar(result_df, y='Artist', x='count', title='Most Prolific Artists')","9e3a8ce3":"# same as above but with seaborn (sometimes plotly doesn't show up in the published notebok)\nsns.barplot(data=result_df, y='Artist', x='count').set_title('Most Prolific Artists');","92bfb2f3":"df.registerTempTable(\"df_table\")","366d3501":"# Most popular artist (by sum of popularity of songs) in the USA\nres = spark.sql('SELECT Artist, ROUND(SUM(Popularity), 2) AS Populartiy \\\n                 FROM df_table \\\n                 WHERE USA == 1 \\\n                 GROUP BY Artist \\\n                 ORDER BY AVG(Popularity) DESC \\\n                 LIMIT 10'\n               )\nres.show(10, truncate=False)","971e2b19":"df.sample(.1).select('Artist').distinct().count() # number of unique artists in 10% random sample","08cc4ad6":"# select only the songs released in 1939\ndf.filter(F.year(df['Release_date']) == 1939) \\\n  .select('Title', 'Artist','Release_date', 'Genre') \\\n  .distinct() \\\n  .show(5, truncate=False)","76633a06":"res = spark.sql('SELECT \\\n                     ROUND(Year(Release_date), -1) AS Decade, \\\n                     Round(Popularity, 2) AS Popularity, Title, Artist \\\n                 FROM df_table \\\n                 INNER JOIN (SELECT Max(Popularity) as mp \\\n                                FROM df_table \\\n                             WHERE ROUND(Year(Release_date), -1) IS NOT NULL \\\n                                 AND USA == 1 \\\n                             GROUP BY ROUND(Year(Release_date), -1) \\\n                             ) AS temp \\\n                 ON temp.mp = df_table.Popularity \\\n                 ORDER BY Decade ASC, Popularity ASC \\\n                ')\nres.toPandas().drop_duplicates(subset='Decade', keep=\"last\")","eca088a7":"# highly optimized version of the above query via scalar-aggregate-reduction\nspark.sql('SELECT \\\n              ROUND(Year(Release_date), -1) as Decade, \\\n              ROUND(Max(Popularity), 2) as Popularity, \\\n              SUBSTRING(MAX(CONCAT(LPAD(Popularity, 11, 0), Title)), 12) AS Title, \\\n              SUBSTRING(MAX(CONCAT(LPAD(Popularity, 11, 0), Artist)), 12) AS Artist \\\n          FROM df_table \\\n              WHERE ROUND(Year(Release_date), -1) IS NOT NULL \\\n                  AND USA == 1 \\\n          GROUP BY Decade \\\n          ORDER BY Decade ASC \\\n          ').show()","51b2fbf9":" # Most popular genres, period.\nspark.sql('SELECT \\\n              Genre, COUNT(*) AS Tally \\\n          FROM df_table \\\n          GROUP BY Genre \\\n          ORDER BY Tally DESC \\\n          ').show(5)","defe1799":"res = spark.sql('SELECT  \\\n                    ROUND(Year(Release_date), -1) AS Decade, \\\n                    Genre, COUNT(Genre) AS counts \\\n                FROM df_table \\\n                WHERE ROUND(Year(Release_date), -1) IS NOT NULL \\\n                GROUP BY Decade, Genre \\\n                ORDER BY COUNT(Genre) DESC \\\n                ') \\\n            .dropDuplicates(subset=['Decade']) \\\n            .orderBy('Decade') \\\n            .show()\n# res.toPandas().drop_duplicates(subset='Decade', keep=\"first\")","537e1bef":"sound_features = ['danceability', 'energy', 'instrumentalness', 'valence', 'liveliness', 'speechiness', 'acoustics']\ncol_names = ['Decade']\ncol_names.extend(sound_features)\ndf_music_features = df.sample(.2, seed=42) \\\n                      .groupBy(F.round(F.year(df.Release_date), -1)) \\\n                      .agg({feature: 'mean' for feature in sound_features}) \\\n                      .toDF(*col_names) \\\n                      .orderBy('Decade') \\\n                      .toPandas() \\\n                      .dropna(axis=0)\nfig = px.line(df_music_features, x='Decade', y=sound_features, title='Song Characteristics Over the Decades')\nfig.show()","cec669a5":"# same as above but with seaborn. (sometimes plotly doesn't show up in the published notebok)\nsns.lineplot(data=pd.melt(df_music_features, ['Decade']), x='Decade', y='value', hue='variable').set_title('Song Characteristics Over the Decades');","2c001117":"from kaggle_secrets import UserSecretsClient\nuser_secrets = UserSecretsClient()\nSPOTIFY_CLIENT_ID = user_secrets.get_secret(\"SPOTIFY_CLIENT_ID\")\nSPOTIFY_CLIENT_SECRET = user_secrets.get_secret(\"SPOTIFY_CLIENT_SECRET\")","45c7d31b":"%%capture\n!pip install spotipy","95fe8131":"import spotipy\nfrom spotipy.oauth2 import SpotifyClientCredentials\nfrom collections import defaultdict\n\nsp = spotipy.Spotify(auth_manager=SpotifyClientCredentials(client_id=SPOTIFY_CLIENT_ID,\n                                                           client_secret=SPOTIFY_CLIENT_SECRET\n                                                          )\n                    )","c5e0dd54":"# to search for a specific song title and filter the returned JSON\nsp.search(q='track: smells like teen spirit')['tracks']['items'][0]['album']","fc7b446d":"def find_song(name, year):\n    song_data = defaultdict()\n    results = sp.search(q=f'track: {name} year: {year}', limit=1)\n    if results['tracks']['items'] == []:\n        return None\n\n    results = results['tracks']['items'][0]\n    track_id = results['id']\n    audio_features = sp.audio_features(track_id)[0]\n\n    song_data['name'] = [name]\n    song_data['year'] = [year]\n    song_data['explicit'] = [int(results['explicit'])]\n    song_data['duration_ms'] = [results['duration_ms']]\n    song_data['popularity'] = [results['popularity']]\n\n    for key, value in audio_features.items():\n        song_data[key] = value\n\n    return pd.DataFrame(song_data)","0b380d04":"df_kpop_songs = spark.sql('SELECT Title, Artist, {} \\\n                          FROM df_table \\\n                          WHERE `k-pop` = 1 \\\n                          ' \\\n                         .format(', '.join(numerical_features)) \\\n                        ) \\\n                    .sample(.1) \\\n                    .dropna() \\\n                    .toPandas() # don't do this, it's better to sample before querying\n\ndf_rap_songs = spark.sql('SELECT Title, Artist, {} \\\n                          FROM df_table \\\n                          WHERE rap = 1 \\\n                          ' \\\n                         .format(', '.join(numerical_features)) \\\n                        ) \\\n                    .sample(.1) \\\n                    .dropna() \\\n                    .toPandas() # don't do this, it's better to sample before querying\n\ndf_rap_songs.head()","416cab1e":"df_kpop_songs.head()","e93a4bfd":"# it might be better to used a normalized cosine similarity instead of scaling first and then doing it.\nfrom scipy import spatial\nfrom sklearn.preprocessing import Normalizer\nfrom sklearn.preprocessing import MinMaxScaler\nscaler = MinMaxScaler()\ntransformer = Normalizer()\n\nscaled_kpop_df = scaler.fit_transform(df_kpop_songs.iloc[:, 2:]) # drop the title and artist with the iloc\nscaled_rap_df = scaler.fit_transform(df_rap_songs.iloc[:, 2:])\n\n\n# cos similarity of a rap and a k-pop song\nsong1 = np.array(scaled_rap_df[1])\nsong2 = np.array(scaled_kpop_df[2])\nresult = 1 - spatial.distance.cosine(song1, song2)\nresult","daf4b9d8":"# cos similarity of two rap songs\nsong1 = np.array(scaled_rap_df[1])\nsong2 = np.array(scaled_rap_df[10])\nresult = 1 - spatial.distance.cosine(song1, song2)\nresult","7ff40508":"# take only a certain percent of the database for training\nsongs_pd_df = df.sample(.1) \\\n             .select([*numerical_features, 'Title']) \\\n             .dropna() \\\n             .toPandas()\nsongs_labels = songs_pd_df.pop('Title')\nsong_ids = np.array(songs_pd_df.index)\nsongs_arr = np.asarray(songs_pd_df.values).astype('float32')  # needs to be float32 for tensorflow to auto-convert to tensors \nsongs_pd_df.head()","7955b9cd":"songs_arr.shape","c6ca72af":"song_ids.shape","e02f63b9":"# from tensorflow import keras\n# from tensorflow.keras import layers\n# # from keras.layers import Dense , Flatten ,Embedding,Input\n\n# EMBEDDING_SIZE = 10\n# NUM_SONGS, ROW_COUNT = songs_arr.shape[0] + 1, songs_arr.shape[0] + 1\n\n# model = keras.Sequential([\n#         layers.Embedding(input_dim=NUM_SONGS, \n#                          output_dim=EMBEDDING_SIZE, \n#                          input_length=16), # , input_length=ROW_COUNT\n#         layers.GlobalAveragePooling1D(),\n#         layers.Dense(24, activation='relu'),\n#         layers.Dense(6, activation='softmax')\n#                         ])\n\n# model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy']) #'sparse_categorical_crossentropy'\n# num_epochs = 10\n# history = model.fit(songs_arr, song_ids,  epochs=num_epochs,\n#                     validation_split=.2,  verbose=2)\n\n# def plot_graphs(history, string):\n#   plt.plot(history.history[string])\n#   plt.plot(history.history['val_'+string])\n#   plt.xlabel(\"Epochs\")\n#   plt.ylabel(string)\n#   plt.legend([string, 'val_'+string])\n#   plt.show()\n  \n# plot_graphs(history, \"accuracy\")\n# plot_graphs(history, \"loss\")","9e65ef73":"### Most Popular Song per Decade\nFirst with a nested query and using pandas to drop duplicates. Then optimized with [scalar-aggregate reduction](https:\/\/www.stevenmoseley.com\/blog\/tech\/high-performance-sql-correlated-scalar-aggregate-reduction-queries)","b1968dd9":"There are a few possible approaches for comparing song similarities. One is to just use the continuous, numerical variables (things like danceability, energy, etc.) and do PCA or k-means or some other way to reduce dimensionality.  If you're just considering the song features (continuous variables) you could just create a feature vector and look at the cosine similartity to find the most similar sounding song, taking into account the numerical features and the one-hot-encoded countries.\n\nSome options: \n- [Non-linear PCA (NLPCA) With CATPCA](https:\/\/pubmed.ncbi.nlm.nih.gov\/22176263\/)\n- [Factor Analysis of Mixed Data (FAMD)](https:\/\/github.com\/MaxHalford\/Prince#factor-analysis-of-mixed-data-famd)\n\nAlternatively, we can create an embedding, where we map all the songs into an n-dimensional feature space and then look for the most similar vectors in this space (probably with k-NNN. Then we can get the k-most similar songs). ","1aae8e28":"Note I'm fixing the numerical features after loading the df. This is much slower than defining the schema before loading into a spark dataframe. I'll come back and define the schema explicitely later when I have some free time.","3dca16c9":"## Compare song similaries in preparation for making recomendations","7a28a519":"## First let's do recomendations via the cosine similiarty of song feature vectors","f836a5b4":"## Let's see how music changed over the decades","308ce69d":"From SparkByExample:\n> A spark session unifies all the different contexts, and you can access all the different contexts by invoking them on the spark session object. A Spark \u201cdriver\u201d is an application that creates a SparkContext for executing one or more jobs in the Spark cluster. It allows your Spark\/PySpark application to access Spark Cluster with the help of Resource Manager.\n> \n> When you create a SparkSession object, SparkContext is also created and can be retrieved using spark.sparkContext. SparkContext will be created only once for an application; even if you try to create another SparkContext, it still returns existing SparkContext.","0dd0c788":"# EDA\nSome good SQL queries, Plotly figures, and examples of using pyspark to filter results from a large dataset.","36f48399":"This notebook serves as an example of using PySpark to explore big data, as well as explore the Spotify API functionality, and build a deep-embedding recommendation system. I have some good examples of SQL queries for EDA","e9a45423":"Most popular artist, all countries. Each tally represents a song on a given day (during the last 3 days) that was one of the most 200 most played songs on that day. An artist can have multiple songs per day, and the same song can be counted on again on subsequent days.","ac1b0d0d":"# Let's check out the spotify API\n\n[currently based off this](https:\/\/www.kaggle.com\/vatsalmavani\/music-recommendation-system-using-spotify-dataset). We can extract more song information than is provided by the dataset by interacting with the Spotify API. Using this, we can get features like song length using `spotipy.audio_features()`","9346d096":"Still working on this. ","68f1b7d3":"### Most popular Genre per decade","256c4a21":"## And now with embeddings\n\n(Still working on this, but keeping some of the boiler plate in this version in case someone wants it)"}}