{"cell_type":{"05eaa797":"code","354700d2":"code","101b7ee8":"code","7cea8df6":"code","f9ef5d0f":"code","0881f86e":"code","fa3d1986":"code","91546d53":"code","a1f497c0":"code","479f8317":"code","253edf16":"code","8589abaf":"code","4d844722":"code","f44290c7":"markdown","930051f2":"markdown","a19a2ad0":"markdown","eec95cfd":"markdown","6ae64262":"markdown","1742946d":"markdown","cef4f50d":"markdown","503890fd":"markdown","fadda9b8":"markdown","395b525d":"markdown","a53d172f":"markdown","ba2d3ee1":"markdown","30172cbc":"markdown","61a78666":"markdown","428169c0":"markdown","cb5ff310":"markdown","62bc1002":"markdown","79914be2":"markdown","20e6da86":"markdown","ce59bd65":"markdown","9abcd11c":"markdown","264368df":"markdown","313aecf8":"markdown","997ed290":"markdown"},"source":{"05eaa797":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","354700d2":"# Import pandas and sklearn's train_test_split libraries\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\n\n# Load the datasets\ntarget = 'vacc_h1n1_f'\ndf = pd.merge(pd.read_csv('..\/input\/prediction-of-h1n1-vaccination\/train.csv'), \n                 pd.read_csv('..\/input\/prediction-of-h1n1-vaccination\/train_labels.csv')[target], left_index=True, right_index=True)\ntest = pd.read_csv('..\/input\/prediction-of-h1n1-vaccination\/test.csv')","101b7ee8":"# Separate data for train \ntrain, val = train_test_split(df, train_size=0.80, test_size=0.20, stratify=df[target], random_state=2)\n\ntrain.shape, val.shape, test.shape","7cea8df6":"# Quick overview of the training dataset\nfrom pandas_profiling import ProfileReport as pr\nprofile = pr(train, minimal=True).to_notebook_iframe()","f9ef5d0f":"# Check duplicates\ntrain.T.duplicated()","0881f86e":"# Check cardinality\ntrain.describe(exclude='number')","fa3d1986":"# Feature engineering\n\ndef engineer(df):\n    \n    # Create \"behaviorals\" feature\n    behaviorals = [col for col in df.columns if 'behavioral' in col] \n    df['behaviorals'] = df[behaviorals].sum(axis=1)\n    \n    # Transform employment_status feature values(\"Not in Labor Force\" -> \"Unemployed\")\n    fixed_data = []\n    for i in df[\"employment_status\"]:\n      if i == \"Not in Labor Force\":\n        fixed_data.append(\"Unemployed\")\n      else:\n        fixed_data.append(i)\n    df[\"employment_status\"] = fixed_data\n    \n    # Remove any feature with cardinality of over 30\n    selected_cols = df.select_dtypes(include=['number', 'object'])\n    colnames = selected_cols.columns.tolist()\n    labels = selected_cols.nunique()\n    \n    selected_features = labels[labels <= 30].index.tolist()\n    df = df[selected_features]\n        \n    return df\n\n\ntrain = engineer(train)\nval = engineer(val)\ntest = engineer(test)","91546d53":"# Separate the target feature from the training data\nfeatures = train.drop(columns=[target]).columns\n\n# Diving training, validation, and testing data into X and y\nX_train = train[features]\ny_train = train[target]\nX_val = val[features]\ny_val = val[target]\nX_test = test[features]","a1f497c0":"# Import libraries for OrdinalEncoder, SimpleImputer, RandomForestClassifier, and make_pipeline\nfrom category_encoders import OrdinalEncoder\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.pipeline import make_pipeline","479f8317":"%%time\n\n# ordinal encoding\npipe_ord = make_pipeline(\n    OrdinalEncoder(), \n    SimpleImputer(), \n    RandomForestClassifier(n_estimators=100, random_state=10, max_depth=14, oob_score=True, n_jobs=-1, criterion=\"gini\", min_samples_split=5, max_features=6)\n)\n\npipe_ord.fit(X_train, y_train)\nprint('\uac80\uc99d \uc815\ud655\ub3c4', pipe_ord.score(X_val, y_val))","253edf16":"# Out-of-Bag samples accuracy\npipe_ord.named_steps['randomforestclassifier'].oob_score_","8589abaf":"# Predict the target using the testing data\ny_pred_test = pipe_ord.predict(X_test)\ny_pred_test = pd.Series(y_pred_test)\ny_pred_test.value_counts()","4d844722":"# Create the DataFrame including predictions with \"id\" feature from the original data as index \nid = pd.Series(range(len(y_pred_test)))\ny_pred_test = pd.Series(y_pred_test)\nsubmission = pd.concat([id, y_pred_test], axis=1)\nsubmission.rename(columns={0:\"id\", 1:target}, inplace=True)\n\n# Display the submission data information\nprint(submission.shape)\nprint(submission.value_counts(target))\n\n# Export the dataset to a *csv* file.\nsubmission.to_csv(\".\/submission.csv\",index=False)","f44290c7":"Afterwards, we will separate the target feature from the training data. Then we will divide the training, validation, and testing data into independent(X) and dependent(y) variables.","930051f2":"## Decisions:\n1. The data with high cardinality could be removed\n2. The missing values need to be imputed","a19a2ad0":"# 2. EDA(Exploratory Data Analysis)","eec95cfd":"So what would be our final prediction using the test data? Let us find out!","6ae64262":"# 1. Data Preparation\n\nFirst of all let us prepare the data, because our data is divided into a label file and a data file, these two needs to be merged. Our target to predict is \"vacc_h1n1_f\", which indicates whether a person is vaccinated(1) or not(0). The test data except the target variable is also provided.\n\nTo import data *pandas* library as awell as *sklearn*'s *train_test_split* are necessary","1742946d":"As we are going to model a Random Forest Classifier, *Ordinal Encoder* is a good choice for converting categorical features to numerical. For missing values, we will use a *Simple Imputer*. Then we will train the encoded and imputed data with a *Random Forest Classifier* model. All of these can be conveniently done with *Pipelines* of *sklearn* library. ","cef4f50d":"The score was actually lower than expected. It only scored about 0.33. It actually scored far lower than my second submission, which included the Random Classifier model trained on all of the features with those of high cardinality. This indicated me that maybe removing high-cardinality features was afar from the ideal solution. Perhaps, I can think about different approaches to improve the model.","503890fd":"From the Profile Report generated beforehand, I found that the *employment_status* feature is also divided into three categories. Among these, I thought that \"Not in Labor Force\" could be converted to \"Unemployed\", since people in these categories are not economically active. (I know that not everyone would agree with such manipulation of the feature, but I am only doing this to see if it helps to improve the performance of our results.)\n\nAlso, a feature called *behaviorals* is created, in order to sum up all of the behavioral characteristics of the individuals. Any feature with over cardinality of 30, would be removed as well.\n\n","fadda9b8":"# 3. Modelling","395b525d":"# 4. Submission Results","a53d172f":"## Feature Engineering","ba2d3ee1":"For the next step, we will check whether there are duplicates in our training data.","30172cbc":"![Screen%20Shot%202020-10-20%20at%204.53.12%20PM.png](attachment:Screen%20Shot%202020-10-20%20at%204.53.12%20PM.png)","61a78666":"Let us have a quick overview of the training dataset by looking at its summary, utilizing *pandas_profiling* to generate a Profile Report.","428169c0":"There are about 12 categorical features identified within the dataset. *state*, which has the highest cardinality, is found to be 51. It is about twice as much as the cardinality of the *employment_occupation* feature, which is the second highest. Maybe we can get rid of *state* feature, as it may negatively impact the performance of our model.","cb5ff310":"## Observations:\n\n* *state* variable has the highest cardinality\n* Many of the features have at least have 20% of missing values out of their entirety\n* There are some features that have more than half of their data as missing values","62bc1002":"Let us create the model, using the training datasets including features that we created. Then let us see the score of how well our model performed by, evaluating with valdiation dataset. We would like the model to use as much CPU as possible so will set the *n_jobs* variable to -1, and as the Random Forest Classifier makes use of out-of-bag samples to test its accuracy, set *oob_score* to \"True\". And after some iterative adjustments of RandomForestClassifier, by referring to [3.2.4.3.1. sklearn.ensemble.RandomForestClassifier](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.ensemble.RandomForestClassifier.html#sklearn.ensemble.RandomForestClassifier.decision_path), the following optimized model is created.\n","79914be2":"It seems that there is no duplicates. Good!\n\nWe found from the previous observation of our Profile Report that there are features with high cardinality. Let us see this in detail.","20e6da86":"The submission file is generated with as correct dimensions as it is required. The output file is generated as \"submission.csv\" in the Kaggle directory. Let us submit it and find out the results!","ce59bd65":"What about out-of-bag samples accuracy? Let us find out!","9abcd11c":"I would be happy to find out how others approached the problem. Please let me know down the comment below! \n\nThanks for reading!","264368df":"Finally we will generate our submission file. We will need to form the dataset that has both the \"id\" information of the individual as well as our final prediciton, as required by the challenge. We will display our submission data to check if it was created alright. Finally, we will export the submission data to a *csv* file for actual submission.","313aecf8":"Next, we are going to split data for the purposes of training and validation using *train_test_split* ","997ed290":"There is only about 0.01 difference between the two scores. It seems that the model performed well with a very small overfit. "}}