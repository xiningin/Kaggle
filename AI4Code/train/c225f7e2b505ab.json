{"cell_type":{"775ec4dd":"code","f839cfc7":"code","4c50147f":"code","7811bc64":"code","18cfc3c5":"code","019b46b7":"code","6f4004fa":"code","a32e3bfd":"code","c3029ef9":"code","427f4689":"code","e8a2cfc4":"code","7ed850a3":"code","e3231231":"code","3d36fee2":"code","c854cf34":"code","d9c0a01b":"code","3ce32aa1":"code","8779bf49":"markdown","4dd1e55f":"markdown","536b2a8c":"markdown","5c0abc54":"markdown","d2ae4318":"markdown","1222cd46":"markdown","d6c0f8cd":"markdown","395aa457":"markdown","cca9a422":"markdown","413e0bf5":"markdown"},"source":{"775ec4dd":"import re\nimport os\nimport numpy as np\nimport pandas as pd\nimport tensorflow as tf\nfrom tensorflow.keras import layers\nfrom kaggle_datasets import KaggleDatasets\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split\nimport time\nfrom IPython import display\nimport PIL\nimport math\nfrom tqdm.auto import tqdm\n\n# Initialize TPU\ntry:\n    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n    print('Running on TPU@{}'.format(tpu.master()))\nexcept ValueError:\n    tpu = None\n    \nif tpu:\n    tf.config.experimental_connect_to_cluster(tpu)\n    tf.tpu.experimental.initialize_tpu_system(tpu)\n    strategy = tf.distribute.experimental.TPUStrategy(tpu)\n    \nelse:\n    strategy = tf.distribute.get_strategy()\n    \nREPLICAS = strategy.num_replicas_in_sync \nprint('# REPLICAS: {}'.format(REPLICAS))","f839cfc7":"AUTOTUNE = tf.data.experimental.AUTOTUNE                                    # For accessing data parallelly.\nGCS_PATH = KaggleDatasets().get_gcs_path()                                  # Path of the public dataset on GCP,\nGLOBAL_BATCH_SIZE = 512 * strategy.num_replicas_in_sync              \nBS = 512                                                                    # Batch size for each replica.\nIMAGE_SIZE = [128,128]                                                      ","4c50147f":"TRAINING_FILENAMES = tf.io.gfile.glob(str(GCS_PATH + '\/*.tfrec'))           # access the tfrecord file, in this case all 130k images are stored \nIMG_COUNT = len(TRAINING_FILENAMES)                                         # in a single .tfrec file!\nprint(\"Image count for training: \" + str(IMG_COUNT))","7811bc64":"# Code block for displaying dataset\n# Code taken from Dimitri's Notebook!\n\nnp.set_printoptions(threshold=15, linewidth=80)\nCLASSES = [0,1]\n\ndef batch_to_numpy_images_and_labels(data):\n    images = data\n    numpy_images = images.numpy()\n\n    return numpy_images\n\ndef title_from_label_and_target(label, correct_label):\n    if correct_label is None:\n        return CLASSES[label], True\n    correct = (label == correct_label)\n    return \"{} [{}{}{}]\".format(CLASSES[label], 'OK' if correct else 'NO', u\"\\u2192\" if not correct else '',\n                                CLASSES[correct_label] if not correct else ''), correct\n\ndef display_one_flower(image, subplot, red=False, titlesize=16):\n    plt.subplot(*subplot)\n    plt.axis('off')\n    plt.imshow(image)\n    return (subplot[0], subplot[1], subplot[2]+1)\n    \ndef display_batch_of_images(databatch, predictions=None):\n    \"\"\"This will work with:\n    display_batch_of_images(images)\n    display_batch_of_images(images, predictions)\n    display_batch_of_images((images, labels))\n    display_batch_of_images((images, labels), predictions)\n    \"\"\"\n\n    images = batch_to_numpy_images_and_labels(databatch)\n    rows = int(math.sqrt(len(images)))\n    cols = len(images)\/\/rows\n        \n    # size and spacing\n    FIGSIZE = 13.0\n    SPACING = 0.1\n    subplot=(rows,cols,1)\n    if rows < cols:\n        plt.figure(figsize=(FIGSIZE,FIGSIZE\/cols*rows))\n    else:\n        plt.figure(figsize=(FIGSIZE\/rows*cols,FIGSIZE))\n    \n    # display\n    for i, (image) in enumerate((images[:rows*cols])):\n\n        correct = True\n        dynamic_titlesize = FIGSIZE*SPACING\/max(rows,cols)*40+3                  \n        subplot = display_one_flower(image, subplot, not correct, titlesize=dynamic_titlesize)\n    \n    #layout\n    plt.tight_layout()\n\n    plt.subplots_adjust(wspace=SPACING, hspace=SPACING)\n    plt.show()","18cfc3c5":"\ndef decode_image(image_data):\n    image = tf.image.decode_jpeg(image_data, channels=3)                           # Decode a JPEG-encoded image to a uint8 tensor.\n    channels = tf.unstack(image, axis=-1)                                          # In this case, images were serialized in BGR format, \n    image    = tf.stack([channels[2], channels[1], channels[0]], axis=-1)          # hence we need to convert them back to RGB\n    image = (tf.cast(image, tf.float32) \/ 255.0)                                   # convert image to floats in [0, 1] range\n    image = tf.reshape(image, [*IMAGE_SIZE, 3])                                    # explicit size needed for TPU \n    return image\n\n\ndef read_labeled_tfrecord(example):\n    LABELED_TFREC_FORMAT = {                                                       # Tfrecord format :-> dataset specific.\n        \"image\": tf.io.FixedLenFeature([], tf.string),                             # tf.string means bytestring\n        \"name\": tf.io.FixedLenFeature([], tf.string),                   \n    }\n    \n    example = tf.io.parse_single_example(example, LABELED_TFREC_FORMAT)            # decode the image [Serialized -> jpeg]\n    image = decode_image(example['image'])\n    return image \n\n\ndef load_dataset(filenames, labeled=True, ordered=False):\n    '''\n    Read from TFRecords. For optimal performance, reading from multiple files at once and\n    disregarding data order. Order does not matter since we will be shuffling the data anyway.\n    '''\n    ignore_order = tf.data.Options()\n    if not ordered:\n        ignore_order.experimental_deterministic = False                            # disable order, increase speed\n\n    dataset = tf.data.TFRecordDataset(filenames, num_parallel_reads=AUTOTUNE)      # automatically interleaves reads from multiple files\n    dataset = dataset.with_options(ignore_order)                                   # uses data as soon as it streams in, rather than in its original order\n    dataset = dataset.map(read_labeled_tfrecord)\n                                                                \n    return dataset\n\ndef get_training_dataset():\n    dataset = load_dataset(TRAINING_FILENAMES, labeled=True)\n    dataset = dataset.shuffle(2048)\n    dataset = dataset.batch(BS,drop_remainder=True)\n    dataset = dataset.prefetch(AUTOTUNE)                                           # prefetch next batch while training (autotune prefetch buffer size)\n    return dataset","019b46b7":"training_dataset = get_training_dataset()\nx = training_dataset.unbatch().batch(20)\ntrain_batch = iter(x)\n\ndisplay_batch_of_images(next(train_batch))","6f4004fa":"EPOCHS = 300                                    \nnoise_dim = 100                                                         # [1,100] dimensional vector to sample noise from when generating new images.\nnum_examples_to_generate = 16\n\nseed = tf.random.normal([num_examples_to_generate, noise_dim])          ","a32e3bfd":"def make_generator_model():\n    '''\n    Pretty basic generator :D\n    '''\n    model = tf.keras.Sequential()\n    model.add(layers.Dense(8*8*1024, use_bias=False, input_shape=(noise_dim,)))\n    model.add(layers.BatchNormalization())\n    model.add(layers.LeakyReLU())\n\n    model.add(layers.Reshape((8, 8, 1024)))\n    assert model.output_shape == (None, 8, 8, 1024) \n\n    model.add(layers.Conv2DTranspose(512, (5, 5), strides=(1, 1), padding='same', use_bias=False))\n    assert model.output_shape == (None, 8, 8, 512)\n    model.add(layers.BatchNormalization())\n    model.add(layers.LeakyReLU())\n\n    model.add(layers.Conv2DTranspose(256, (5, 5), strides=(2, 2), padding='same', use_bias=False))\n    assert model.output_shape == (None, 16, 16, 256)\n    model.add(layers.BatchNormalization())\n    model.add(layers.LeakyReLU())\n    \n    model.add(layers.Conv2DTranspose(128, (5, 5), strides=(2, 2), padding='same', use_bias=False))\n    assert model.output_shape == (None, 32, 32, 128)\n    model.add(layers.BatchNormalization())\n    model.add(layers.LeakyReLU())\n\n    model.add(layers.Conv2DTranspose(64, (5, 5), strides=(2, 2), padding='same', use_bias=False))\n    assert model.output_shape == (None, 64, 64, 64)\n    model.add(layers.BatchNormalization())\n    model.add(layers.LeakyReLU())\n    \n    model.add(layers.Conv2DTranspose(3, (5, 5), strides=(2, 2), padding='same', use_bias=False, activation='tanh'))\n    assert model.output_shape == (None, 128, 128, 3)\n\n    return model","c3029ef9":"def make_discriminator_model():\n\n    model = tf.keras.Sequential()\n    \n    model.add(layers.Conv2D(64, (5, 5), strides=(2, 2), padding='same',\n                                     input_shape=[128, 128, 3]))\n    model.add(layers.LeakyReLU())\n    model.add(layers.Dropout(0.3))\n\n    model.add(layers.Conv2D(128, (5, 5), strides=(2, 2), padding='same'))\n    model.add(layers.LeakyReLU())\n    model.add(layers.Dropout(0.3))\n    \n    model.add(layers.Conv2D(256, (5, 5), strides=(2, 2), padding='same'))\n    model.add(layers.LeakyReLU())\n    model.add(layers.Dropout(0.3))\n    \n    model.add(layers.Conv2D(512, (5, 5), strides=(2, 2), padding='same'))\n    model.add(layers.LeakyReLU())\n    model.add(layers.Dropout(0.3))\n\n    model.add(layers.Flatten())\n    model.add(layers.Dense(1))\n\n    return model","427f4689":"# Let's code this!\n\nwith strategy.scope():                                                                            # Note, I initialize the loss functions under the strategy's scope\n    loss_object = tf.keras.losses.BinaryCrossentropy(\n    from_logits=True, \n    reduction= tf.keras.losses.Reduction.NONE)                                                    # Set reduction to `none` so we can do the reduction afterwards and \n                                                                                                  # divide by global batch size\/\n    \n    \n    def generator_loss(fake_output):\n        per_example_loss =  loss_object(tf.ones_like(fake_output), fake_output)                   # Getting the unscaled loss.\n        return tf.nn.compute_average_loss(per_example_loss, global_batch_size=GLOBAL_BATCH_SIZE)  # 'tf.nn.compute_average_loss()' takes the unscaled loss and \n                                                                                                  # GLOBAL_BATCH_SIZE and scale it according to -> \n                                                                                                  # scale_loss = tf.reduce_sum(loss) * (1. \/ GLOBAL_BATCH_SIZE)\n    \n    def discriminator_loss(real_output, fake_output):                                             # Same goes for the discriminator_loss\n        real_loss = loss_object(tf.ones_like(real_output), real_output)\n        fake_loss = loss_object(tf.zeros_like(fake_output), fake_output)\n        total_loss  = real_loss + fake_loss\n        return tf.nn.compute_average_loss(total_loss, global_batch_size=GLOBAL_BATCH_SIZE)","e8a2cfc4":"checkpoint_dir = '.\/training_checkpoints'\ncheckpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")","7ed850a3":"with strategy.scope():\n    generator = make_generator_model()                                              # Instantiate the Generator and discriminator under the scope!\n    discriminator = make_discriminator_model()\n    \n    generator_optimizer = tf.keras.optimizers.Adam(1e-4)                            # Create the optimizers, here using 2 differnt objects but with same LR\n    discriminator_optimizer = tf.keras.optimizers.Adam(1e-4)     \n    \n    checkpoint = tf.train.Checkpoint(generator_optimizer=generator_optimizer,       # Create the checkpoint object to store intermediate states of the optimizers and models\n                                 discriminator_optimizer=discriminator_optimizer,\n                                 generator=generator,\n                                 discriminator=discriminator)\n\ndef train_step(images):\n    noise = tf.random.normal([BS, noise_dim])                                       # Generate noise-vector!\n    with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:              \n        generated_images = generator(noise, training=True)                          # sample images using the latent-noise vector\n        \n        real_output = discriminator(images, training=True)                          # Get discriminator's output on real images.\n        fake_output = discriminator(generated_images, training=True)                # Get discriminator's output on fake images.\n        gen_loss = generator_loss(fake_output)                                      # Compute generator's loss.\n        disc_loss = discriminator_loss(real_output, fake_output)                    # Computer discriminator's loss.\n\n        gradients_of_generator = gen_tape.gradient(gen_loss, generator.trainable_variables)                   # Compute gradients of differentiable elements of gen.\n        gradients_of_discriminator = disc_tape.gradient(disc_loss, discriminator.trainable_variables)         # Compute gradients of differentiable elements of dis.\n\n        generator_optimizer.apply_gradients(zip(gradients_of_generator, generator.trainable_variables))               #Apply the gradients!\n        discriminator_optimizer.apply_gradients(zip(gradients_of_discriminator, discriminator.trainable_variables))   #Apply the gradients!\n    \n        return gen_loss, disc_loss","e3231231":"@tf.function\ndef distributed_train_step(dataset_inputs):\n    '''\n    Computes per replica losses of a single step and return the scaled losses!\n    '''\n    # `run` replicates the provided computation and runs it with the distributed input.\n    \n    per_replica_gloss, per_replica_dloss = strategy.run(train_step, args=(dataset_inputs,))\n    return strategy.reduce(tf.distribute.ReduceOp.SUM, per_replica_gloss,axis=None), strategy.reduce(tf.distribute.ReduceOp.SUM, per_replica_dloss,axis=None)\n\nfor epoch in range(EPOCHS):\n    total_loss = 0.0\n    gloss = 0.0\n    dloss = 0.0\n    num_batches = 0\n    for x in (training_dataset):                                                  # Iterate the dataset.\n        G, D = distributed_train_step(x)                                          # Get the scaled losses for the generator and discriminator.\n        gloss += G                                                                \n        dloss += D\n        num_batches +=1\n        total_loss = gloss+dloss\n    train_loss = total_loss\/ num_batches\n    gloss = gloss\/num_batches\n    dloss = dloss\/num_batches\n    if epoch %2 ==0:\n        template = (\"Epoch {}\\n G-Loss {}\\n D-Loss {}\\n Train-loss {} \")\n        print(template.format(epoch+1, gloss,dloss,train_loss))\n        \n    ","3d36fee2":"noise = tf.random.normal([1, noise_dim])\ngenerated_image = generator(noise, training=False)\n\nplt.imshow(generated_image[0, :, :,:], cmap='hot')\n\ndecision = discriminator(generated_image)\nprint (decision)","c854cf34":"def generate_and_save_images(model):\n  # Notice `training` is set to False.\n  # This is so all layers run in inference mode (batchnorm).\n    \n    for idx in range(5):\n        test_input = tf.random.normal([16,noise_dim])\n        predictions = model(test_input, training=False)\n\n        fig = plt.figure(figsize=(4,4))\n\n        for i in range(predictions.shape[0]):\n            plt.subplot(4, 4, i+1)\n            plt.imshow(predictions[i, :, :, :])\n            plt.axis('off')\n\n        plt.savefig('image_at_try_{:04d}.png'.format(idx))\n        plt.show()","d9c0a01b":"generate_and_save_images(generator)","3ce32aa1":"generator.save('gen.h5')              # Save the Generator and Discriminator explicitly for future use :)\ndiscriminator.save('dis.h5')","8779bf49":"## <span style=\"font-family:cursive;\"> **Utility Functions for dataset creation! \ud83d\udc31\u200d\ud83d\udc64\ud83d\udc31\u200d\ud83d\udc53** <\/span>","4dd1e55f":"## <span style=\"font-family:cursive;\"> **Visualize the data \ud83e\udd1e!** <\/span>","536b2a8c":"## <span style=\"font-family:cursive;\"> **Let's Visualize the Generated Images \ud83d\udd75\ufe0f\u200d\u2642\ufe0f\ud83d\udd75\ufe0f\u200d\u2640\ufe0f** <\/span>","5c0abc54":"## <span style=\"font-family:cursive;\"> **Future Work \ud83d\udc69\u200d\ud83d\udcbb\ud83d\udc68\u200d\ud83d\udcbb:** <\/span>\n1. <span style=\"font-family:cursive;\"> **As you can see the quality of images is not excellent. Hence in my upcoming notebooks I'll try out different variations of the GAN architecture** <\/span>\n2. <span style=\"font-family:cursive;\"> **WGAN, WGAN-GP, Self-Attention GAN, BIG-GAN etc. are currently on my list!** <\/span>\n\n\n## <span style=\"font-family:cursive;\"> **If you liked my work, do Upvote! \ud83d\udc4d\ud83d\udc4d** <\/span>\n\n<span style=\"font-family:cursive;\"> **Cheers !**\ud83c\udf7b\ud83c\udf7b <\/span>","d2ae4318":"# <span style=\"font-family:cursive;\"> **Imports** <\/span>","1222cd46":"## <span style=\"font-family:verdana;\"> **Little Background!**<\/span>\n1. <span style=\"font-family:verdana;\"> **Normally, on a single machine with 1 GPU\/CPU, loss is divided by the number of examples in the batch of input**.<\/span>\n2. <span style=\"font-family:verdana;\"> **In distributed case, The model on each replica does a forward pass with its respective input and calculates the loss. Now, instead of dividing the loss by the number of examples in its respective input (BATCH_SIZE_PER_REPLICA = 512), the loss should be divided by the GLOBAL_BATCH_SIZE (512*8).**<\/span>","d6c0f8cd":"## <span style=\"font-family:cursive;\"> **It's Training Time \ud83c\udfcb\ufe0f\u200d\u2642\ufe0f\ud83c\udfcb\ufe0f\u200d\u2642\ufe0f\ud83d\udcaa** <\/span>","395aa457":"### <span style=\"font-family:verdana;\"> Notebook Specifics :<\/span>\n* <span style=\"font-family:verdana;\"> **Dataset**: BitMoji 130k   <\/span>\n\n* <span style=\"font-family:verdana;\"> **V1: Model**  : DCGAN   <\/span>","cca9a422":"## <span style=\"font-family:cursive;\"> **Create the models!\ud83e\uddb8\u200d\u2642\ufe0f** <\/span>","413e0bf5":"1. <span style=\"font-family:verdana;\"> **Training a DCGAN on a GPU is fairly straightforward**.<\/span>\n2. <span style=\"font-family:verdana;\"> **Using Custom training loops (needed for a GAN) in a distributed environment is bit more sophisticated and requires to handle certain cases which are done under-the-hood in normal circumstances**.<\/span>\n\n<span style=\"font-family:verdana;\"> **Therefore, I present you with an example of how to train a DCGAN on TPU in *tensorflow*** <\/span>\n"}}