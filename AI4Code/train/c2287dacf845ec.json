{"cell_type":{"feb65598":"code","0756e345":"code","b7140b5d":"code","9112ab57":"code","76f6b7d4":"code","ccd3c144":"code","ec5d960d":"code","f8de6c0b":"code","368c66c2":"code","71b4fea9":"code","70a29bd2":"code","d8e03cf6":"code","2845f945":"code","ceb1e197":"code","5de9996b":"code","811340ed":"code","940ab84d":"code","d46c4cdc":"code","2287f5ea":"code","a9be45bc":"code","b72140f0":"code","55d3a20e":"code","7242a319":"code","469bed6f":"code","216fcdfc":"code","b2d2af87":"code","099e6aae":"code","15d3d68d":"code","0ee95231":"code","be1ba4a7":"code","e6e4f262":"code","dd015582":"code","1cb457fe":"code","5ebcf230":"code","448a276f":"code","8d10100e":"code","80f4a619":"code","f6c8787f":"code","d50a41d3":"code","959ef8aa":"code","a793ec4f":"code","52f536ae":"code","641b404c":"code","f23937fb":"code","e13d6682":"code","35d55ea0":"code","c351dedb":"code","eefb21da":"code","5c30c557":"code","3309e99c":"code","e8dc1291":"code","b5fe0546":"code","159c1606":"code","0a74dfa0":"code","f0c38324":"code","74da3179":"code","b0ab5e7f":"code","3f983c6d":"code","8a7be1ba":"code","42ecd470":"code","ceb63abf":"code","ab3ae30d":"code","3a93d9c4":"code","4af25ffa":"code","1d8e8a20":"code","ded4e40c":"code","bfbb77fa":"code","0aa22e6b":"code","a2112f34":"code","deed9544":"code","fa40546e":"code","55d53b00":"code","49cc2272":"code","dba399b6":"code","0c0584e4":"code","2f50fff0":"code","a7516c1f":"code","84218876":"code","d78145d5":"code","c86b8435":"code","7dd3843a":"code","0332c13a":"code","6b0b5b2d":"code","0403ce8c":"code","8ae28861":"code","8a2eb33e":"code","8cce8aaa":"code","80893472":"code","18d68c74":"code","c0c41d6b":"code","c7f99d82":"code","4839a041":"code","c1b686ba":"code","3aa6a88e":"code","8446c770":"code","a7630e83":"code","731206df":"code","d568c67f":"code","7caeee29":"code","ad7b6d79":"code","916b6747":"code","39565c06":"code","0a6efe96":"code","fc230461":"code","c072eef2":"code","bdd4e5d4":"code","0ce9e2de":"code","4914c81f":"code","3b6407fb":"code","bde2fcc0":"code","e6cab967":"code","756a1fda":"code","97c1cff9":"code","e453a6c5":"code","d70d9eca":"code","e69efd2c":"code","e956a2a5":"code","15f3a6fb":"code","c1a24abb":"code","ad712912":"code","7b1f7e6a":"markdown","252309f0":"markdown","3375b5ec":"markdown","1175d6df":"markdown","49b45968":"markdown","e27a31bc":"markdown","ab155cb0":"markdown","5ac75158":"markdown","4d035450":"markdown","0fc91a01":"markdown","82f7bd11":"markdown","1c7dfadf":"markdown","2324eebb":"markdown","f21a66c0":"markdown","0a18d93b":"markdown","d7d867a5":"markdown","07946a48":"markdown","b7dbc518":"markdown","ff760f94":"markdown","a7ac888e":"markdown","72b6d7de":"markdown","c5a8368b":"markdown","e027edda":"markdown","770fa5e2":"markdown","116ecd70":"markdown","fd1e61fb":"markdown","44c32baf":"markdown","deec5ee9":"markdown","5acdba37":"markdown","d0dc6423":"markdown","01dc2b9e":"markdown","3ff08152":"markdown","cb33aa90":"markdown","98eb6346":"markdown","d7df8ca5":"markdown","8b714c72":"markdown","9f99fbe2":"markdown","a92cd4a8":"markdown","ff90d610":"markdown","be7abf7d":"markdown","e24010e9":"markdown","20252a4c":"markdown","77628435":"markdown","5c2ec3bb":"markdown","c4bcf0e6":"markdown","c346ad18":"markdown","9fa892e4":"markdown","1fdd64d1":"markdown","d8d87541":"markdown","8ed9a42c":"markdown","a3555e0f":"markdown","34a927c4":"markdown","5a123e43":"markdown"},"source":{"feb65598":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nfrom pandas_profiling import ProfileReport\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom copy import deepcopy\npd.set_option('display.max_columns', None)\nfrom sklearn.impute import KNNImputer\nfrom sklearn.model_selection import KFold, cross_val_score, train_test_split\nimport xgboost as xgb\nimport sklearn.utils as sku\nimport sklearn.metrics as skm\nimport sklearn.model_selection as skms\nimport sklearn.base as skb\nimport sklearn.ensemble as ske\nimport sklearn.preprocessing as skp\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.manifold import MDS\nimport numpy as np\nimport pandas as pd\nfrom scipy import sparse\nfrom sklearn.preprocessing import LabelEncoder\nfrom tqdm import tqdm\nfrom sklearn.metrics import accuracy_score\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","0756e345":"pip install miceforest","b7140b5d":"PATH = \"..\/input\/churn-risk-rate-hackerearth-ml\/\"\ntrain = pd.read_csv(f\"{PATH}train.csv\")\ntest = pd.read_csv(f\"{PATH}test.csv\")\nsubmission= pd.read_csv(f\"{PATH}sample_submission.csv\")","9112ab57":"train.head()","76f6b7d4":"test.head()","ccd3c144":"submission.head()","ec5d960d":"train.drop(['customer_id'], inplace=True, axis=1)\ntest.drop(['customer_id'], inplace=True, axis=1)","f8de6c0b":"# select numerical and categorical features\ndef divideFeatures(df):\n    numerical_features = df.select_dtypes(include=[np.number])\n    categorical_features = df.select_dtypes(include=[np.object])\n    return numerical_features, categorical_features\n\ncont_features, cat_features = divideFeatures(train)\ncat_features.head()","368c66c2":"test.shape","71b4fea9":"train = train.drop([\"security_no\",\"Name\"], axis=1)\ntest = test.drop([\"security_no\",\"Name\"], axis=1)","70a29bd2":"#adding test and train set together\n\ndf =  pd.concat(objs=[train, test], axis=0).reset_index(drop=True)","d8e03cf6":"train['churn_risk_score'].value_counts()","2845f945":"sns.countplot('churn_risk_score', data=train)","ceb1e197":"train.dtypes","5de9996b":"#Converting the joining_date type datetime format\ntrain['joining_date'] = train['joining_date'].astype('datetime64[ns]')\ntest['joining_date'] = test['joining_date'].astype('datetime64[ns]')\ntest['last_visit_time'] = test['last_visit_time'].astype('datetime64[ns]')\ntrain['last_visit_time'] = train['last_visit_time'].astype('datetime64[ns]')","811340ed":"feature_cols = train.columns\n\n## Getting all the data that are not of \"object\" type. \nnumerical_columns = train[feature_cols].select_dtypes(include=['int64','float64']).columns\ncategorical_columns = train[feature_cols].select_dtypes(exclude=['int64','float64','datetime64[ns]']).columns\n\nprint(len(numerical_columns), len(categorical_columns))\n\n","940ab84d":"from plotly.subplots import make_subplots\nimport plotly.graph_objects as go\n\ntrain_0_df = train.loc[train['churn_risk_score'] == -1]\ntrain_1_df = train.loc[train['churn_risk_score'] == 1]\ntrain_2_df = train.loc[train['churn_risk_score'] == 2]\ntrain_3_df = train.loc[train['churn_risk_score'] == 3]\ntrain_4_df = train.loc[train['churn_risk_score'] == 4]\ntrain_5_df = train.loc[train['churn_risk_score'] == 5]\n\nnum_rows, num_cols = 7,2\nfig = make_subplots(rows=num_rows, cols=num_cols)\n\nfor index, column in enumerate(df[categorical_columns].columns):\n    i,j = ((index \/\/ num_cols)+1, (index % num_cols)+1)\n    data = train_0_df.groupby(column)[column].count().sort_values(ascending=False)\n    data = data if len(data) < 10 else data[:10]\n    fig.add_trace(go.Bar(\n        x = data.index,\n        y = data.values,\n        name='Label: -1',\n    ), row=i, col=j)\n\n    data = train_1_df.groupby(column)[column].count().sort_values(ascending=False)\n    data = data if len(data) < 10 else data[:10]\n    fig.add_trace(go.Bar(\n        x = data.index,\n        y = data.values,\n        name='Label: 1'\n    ), row=i, col=j)\n    data = train_2_df.groupby(column)[column].count().sort_values(ascending=False)\n    data = data if len(data) < 10 else data[:10]\n    fig.add_trace(go.Bar(\n        x = data.index,\n        y = data.values,\n        name='Label: 2'\n    ), row=i, col=j)\n    data = train_3_df.groupby(column)[column].count().sort_values(ascending=False)\n    data = data if len(data) < 10 else data[:10]\n    fig.add_trace(go.Bar(\n        x = data.index,\n        y = data.values,\n        name='Label: 3'\n    ), row=i, col=j)\n    data = train_4_df.groupby(column)[column].count().sort_values(ascending=False)\n    data = data if len(data) < 10 else data[:10]\n    fig.add_trace(go.Bar(\n        x = data.index,\n        y = data.values,\n        name='Label: 4'\n    ), row=i, col=j)\n    data = train_5_df.groupby(column)[column].count().sort_values(ascending=False)\n    data = data if len(data) < 10 else data[:10]\n    fig.add_trace(go.Bar(\n        x = data.index,\n        y = data.values,\n        name='Label: 5'\n    ), row=i, col=j)\n    \n    fig.update_xaxes(title=column, row=i, col=j)\n    fig.update_layout(barmode='stack')\n    \nfig.update_layout(\n    autosize=False,\n    width= 1300,\n    height=1700,\n    showlegend=False,\n)\nfig.show()","d46c4cdc":"#argument errors='coerce' converts invalid values  into NaN and the data type is float.\n#We will deal with the feature engineering of this column in numeric section\ntrain['avg_frequency_login_days'] = pd.to_numeric(train['avg_frequency_login_days'], errors='coerce')\ntest['avg_frequency_login_days'] = pd.to_numeric(test['avg_frequency_login_days'], errors='coerce')","2287f5ea":"train['gender'].value_counts(dropna=False)","a9be45bc":"test['gender'].value_counts(dropna=False)","b72140f0":"#Replacing Male,Female,Unkown  value with 2,1 and 0 \n\ntrain['gender'].replace('F', 2,inplace=True)\ntrain['gender'].replace('M', 1,inplace=True)\ntrain['gender'].replace('Unknown', 0,inplace=True)\n\ntest['gender'].replace('F', 2,inplace=True)\ntest['gender'].replace('M', 1,inplace=True)\ntest['gender'].replace('Unknown', 0,inplace=True)","55d3a20e":"train['region_category'].value_counts(dropna=False)","7242a319":"test['region_category'].value_counts(dropna=False)","469bed6f":"#Replacing Town,City and Village value with 1,2 and 0\n\ntrain['region_category'].replace('Town', 2,inplace=True)\ntrain['region_category'].replace('City', 1,inplace=True)\ntrain['region_category'].replace('Village', 0,inplace=True)\ntest['region_category'].replace('Town', 2,inplace=True)\ntest['region_category'].replace('City', 1,inplace=True)\ntest['region_category'].replace('Village', 0,inplace=True)","216fcdfc":"#Changing the type of the column\n\ntrain['gender'] = pd.to_numeric(train['gender'])\ntest['gender'] = pd.to_numeric(test['gender'])","b2d2af87":"train['membership_category'].value_counts(dropna=False)","099e6aae":"test['membership_category'].value_counts(dropna=False)","15d3d68d":"# The column has normal values.So,We can directly replace with numeric values\ntrain['membership_category'].replace('No Membership', 5,inplace=True)\ntrain['membership_category'].replace('Basic Membership', 4,inplace=True)\ntrain['membership_category'].replace('Gold Membership', 3,inplace=True)\ntrain['membership_category'].replace('Silver Membership', 2,inplace=True)\ntrain['membership_category'].replace('Premium Membership', 1,inplace=True)\ntrain['membership_category'].replace('Platinum Membership', 0,inplace=True)\n\ntest['membership_category'].replace('No Membership', 5,inplace=True)\ntest['membership_category'].replace('Basic Membership', 4,inplace=True)\ntest['membership_category'].replace('Gold Membership', 3,inplace=True)\ntest['membership_category'].replace('Silver Membership', 2,inplace=True)\ntest['membership_category'].replace('Premium Membership', 1,inplace=True)\ntest['membership_category'].replace('Platinum Membership', 0,inplace=True)","0ee95231":"#Changing the type of the column\n\ntrain['membership_category'] = pd.to_numeric(train['membership_category'])\ntest['membership_category'] = pd.to_numeric(test['membership_category'])","be1ba4a7":"train['joined_through_referral'].value_counts(dropna=False)","e6e4f262":"test['joined_through_referral'].value_counts(dropna=False)","dd015582":"# ? might mean that the users didn't joined through refferal.So, we will replace the Yes,No with 2,1 respectively.\ntrain['joined_through_referral'].replace('Yes', 2,inplace=True)\ntrain['joined_through_referral'].replace('No', 1,inplace=True)\n\ntest['joined_through_referral'].replace('Yes', 2,inplace=True)\ntest['joined_through_referral'].replace('No', 1,inplace=True)\n","1cb457fe":"#this function replaces the non num '?'values NaN\ntrain['joined_through_referral'] = pd.to_numeric(train['joined_through_referral'], errors='coerce')\ntest['joined_through_referral'] = pd.to_numeric(test['joined_through_referral'], errors='coerce')","5ebcf230":"train['preferred_offer_types'].value_counts(dropna=False)","448a276f":"test['preferred_offer_types'].value_counts(dropna=False)","8d10100e":"# The column has normal values.So,We can directly replace with numeric values\ntrain['preferred_offer_types'].replace('Without Offers', 2,inplace=True)\ntrain['preferred_offer_types'].replace('Credit\/Debit Card Offers', 1,inplace=True)\ntrain['preferred_offer_types'].replace('Gift Vouchers\/Coupons', 0,inplace=True)\n\ntest['preferred_offer_types'].replace('Without Offers', 2,inplace=True)\ntest['preferred_offer_types'].replace('Credit\/Debit Card Offers', 1,inplace=True)\ntest['preferred_offer_types'].replace('Gift Vouchers\/Coupons', 0,inplace=True)","80f4a619":"train['medium_of_operation'].value_counts(dropna=False)","f6c8787f":"test['medium_of_operation'].value_counts(dropna=False)","d50a41d3":"# ? might mean that the we don't know the medium of operation of the user.\n#So, we will replace the Desktop,Smartphone and Both with 2,1,0 respectively and ? with Nan.\ntrain['medium_of_operation'].replace('Desktop', 2,inplace=True)\ntrain['medium_of_operation'].replace('Smartphone', 1,inplace=True)\ntrain['medium_of_operation'].replace('Both', 0,inplace=True)\n\ntest['medium_of_operation'].replace('Desktop', 2,inplace=True)\ntest['medium_of_operation'].replace('Smartphone', 1,inplace=True)\ntest['medium_of_operation'].replace('Both', 0,inplace=True)","959ef8aa":"#this function replaces the non num '?'values NaN\ntrain['medium_of_operation'] = pd.to_numeric(train['medium_of_operation'], errors='coerce')\ntest['medium_of_operation'] = pd.to_numeric(test['medium_of_operation'], errors='coerce')","a793ec4f":"train['internet_option'].value_counts(dropna=False)","52f536ae":"test['internet_option'].value_counts(dropna=False)","641b404c":"#So, we will replace the Fiber_Optic,Wi-Fi and Mobile_Data with 2,1,0 respectively.\ntrain['internet_option'].replace('Fiber_Optic', 2,inplace=True)\ntrain['internet_option'].replace('Wi-Fi', 1,inplace=True)\ntrain['internet_option'].replace('Mobile_Data', 0,inplace=True)\n\ntest['internet_option'].replace('Fiber_Optic', 2,inplace=True)\ntest['internet_option'].replace('Wi-Fi', 1,inplace=True)\ntest['internet_option'].replace('Mobile_Data', 0,inplace=True)","f23937fb":"train['used_special_discount'].value_counts(dropna=False)","e13d6682":"test['used_special_discount'].value_counts(dropna=False)","35d55ea0":"#Replacing the values\ntrain['used_special_discount'].replace('Yes', 1,inplace=True)\ntrain['used_special_discount'].replace('No', 0,inplace=True)\n\ntest['used_special_discount'].replace('Yes', 1,inplace=True)\ntest['used_special_discount'].replace('No', 0,inplace=True)","c351dedb":"train['offer_application_preference'].value_counts(dropna=False)","eefb21da":"test['offer_application_preference'].value_counts(dropna=False)","5c30c557":"#Replacing the values\ntrain['offer_application_preference'].replace('Yes', 1,inplace=True)\ntrain['offer_application_preference'].replace('No', 0,inplace=True)\n\ntest['offer_application_preference'].replace('Yes', 1,inplace=True)\ntest['offer_application_preference'].replace('No', 0,inplace=True)","3309e99c":"train['past_complaint'].value_counts(dropna=False)","e8dc1291":"test['past_complaint'].value_counts(dropna=False)","b5fe0546":"#Replacing the values\ntrain['past_complaint'].replace('Yes', 1,inplace=True)\ntrain['past_complaint'].replace('No', 0,inplace=True)\n\ntest['past_complaint'].replace('Yes', 1,inplace=True)\ntest['past_complaint'].replace('No', 0,inplace=True)","159c1606":"train['complaint_status'].value_counts(dropna=False)","0a74dfa0":"test['complaint_status'].value_counts(dropna=False)","f0c38324":"#Replacing the values\ntrain['complaint_status'].replace('Not Applicable', 4,inplace=True)\ntrain['complaint_status'].replace('Solved in Follow-up', 3,inplace=True)\ntrain['complaint_status'].replace('No Information Available', 2,inplace=True)\ntrain['complaint_status'].replace('Unsolved', 1,inplace=True)\ntrain['complaint_status'].replace('Solved', 0,inplace=True)\n\ntest['complaint_status'].replace('Not Applicable', 4,inplace=True)\ntest['complaint_status'].replace('Solved in Follow-up', 3,inplace=True)\ntest['complaint_status'].replace('No Information Available', 2,inplace=True)\ntest['complaint_status'].replace('Unsolved', 1,inplace=True)\ntest['complaint_status'].replace('Solved', 0,inplace=True)","74da3179":"train['feedback'].value_counts(dropna=False)","b0ab5e7f":"test['feedback'].value_counts(dropna=False)","3f983c6d":"#Replacing the values\ntrain['feedback'].replace('No reason specified', 8,inplace=True)\ntrain['feedback'].replace('Poor Customer Service', 7,inplace=True)\ntrain['feedback'].replace('Too many ads', 6,inplace=True)\ntrain['feedback'].replace('Poor Product Quality', 5,inplace=True)\ntrain['feedback'].replace('Poor Website', 4,inplace=True)\ntrain['feedback'].replace('Reasonable Price', 3,inplace=True)\ntrain['feedback'].replace('Products always in Stock', 2,inplace=True)\ntrain['feedback'].replace('Quality Customer Care', 1,inplace=True)\ntrain['feedback'].replace('User Friendly Website', 0,inplace=True)\n\ntest['feedback'].replace('No reason specified', 8,inplace=True)\ntest['feedback'].replace('Poor Customer Service', 7,inplace=True)\ntest['feedback'].replace('Too many ads', 6,inplace=True)\ntest['feedback'].replace('Poor Product Quality', 5,inplace=True)\ntest['feedback'].replace('Poor Website', 4,inplace=True)\ntest['feedback'].replace('Reasonable Price', 3,inplace=True)\ntest['feedback'].replace('Products always in Stock', 2,inplace=True)\ntest['feedback'].replace('Quality Customer Care', 1,inplace=True)\ntest['feedback'].replace('User Friendly Website', 0,inplace=True)","8a7be1ba":"train['referral_id'].value_counts(dropna=False)","42ecd470":"train = train.drop('referral_id', axis=1)\ntest = test.drop('referral_id', axis=1)","ceb63abf":"train.dtypes","ab3ae30d":"train['avg_frequency_login_days'].value_counts(dropna=False)\n","3a93d9c4":"test['avg_frequency_login_days'].value_counts(dropna=False)","4af25ffa":"test['avg_frequency_login_days'].describe()","1d8e8a20":"train['avg_frequency_login_days'].describe()","ded4e40c":"#As we can see that there are missing values and outliers \nsns.boxplot(x=train['avg_frequency_login_days'])","bfbb77fa":"\ntrain.avg_frequency_login_days.hist()","0aa22e6b":"#It looks like there are outliers from -40 to -8 and greater than 40.We will remove them now.\n#index = train[(train['avg_frequency_login_days'] >= 40)|(train['avg_frequency_login_days'] <= -8)].index\n#train.drop(index, inplace=True)\n#train['avg_frequency_login_days'].describe()\n","a2112f34":"train['age'].describe()","deed9544":"sns.boxplot(x=train['age'])","fa40546e":"train['days_since_last_login'].value_counts(dropna=False)","55d53b00":"#We can see that there are -999 values, generally -999 values are used to replace NaN values.\n#So, we will now check the how the dataset is when the value is -999\n\nfiltered_data = train[train[\"days_since_last_login\"]==-999]\nfiltered_data.head(3)","49cc2272":"#train['days_since_last_login'].replace(-999, np.NaN,inplace=True)\n#test['days_since_last_login'].replace(-999, np.NaN,inplace=True)","dba399b6":"sns.boxplot(x=train['days_since_last_login'])","0c0584e4":"test['avg_time_spent'].describe()","2f50fff0":"train['avg_time_spent'].describe()","a7516c1f":"#As we have seen there are no NaN values in avg_time_spent.\n#We will visualize the outlier first.\nsns.boxplot(x=train['avg_time_spent'])\n","84218876":"train.avg_time_spent.hist()","d78145d5":"#index = train[(train['avg_time_spent'] >= 990)|(train['avg_time_spent'] <= -500)].index\n#train.drop(index, inplace=True)\n#train['avg_time_spent'].describe()","c86b8435":"train['avg_transaction_value'].value_counts(dropna=False)","7dd3843a":"#There are no nun values.We will now try to visualize the outliers\nsns.boxplot(x=train['avg_transaction_value'])","0332c13a":"#filtered_data = train[train['avg_transaction_value']>=80000]\n#filtered_data.head()","6b0b5b2d":"#Amount of values which are outliers\nlen(filtered_data)","0403ce8c":"filtered_data['churn_risk_score'].value_counts()","8ae28861":"train['avg_transaction_value'].describe()","8a2eb33e":"test['avg_transaction_value'].describe()","8cce8aaa":"#train['avg_transaction_value'].value_counts(bins=3, sort=False)","80893472":"#We will now bin the avg_transaction_value into 0-avg low spend,1-medium  spend,2-high spend \n\n#train['avg_transaction_value_cat'] = pd.qcut(train['avg_transaction_value'], q=3, precision = 2)\n#test['avg_transaction_value_cat'] = pd.qcut(test['avg_transaction_value'], q=3, precision = 2)","18d68c74":"#train['avg_transaction_value_cat'].value_counts()","c0c41d6b":"#test['avg_transaction_value_cat'].value_counts()","c7f99d82":"#Replacing the values\n#test['avg_transaction_value_cat'] = test['avg_transaction_value_cat'].cat.codes\n#train['avg_transaction_value_cat'] = train['avg_transaction_value_cat'].cat.codes","4839a041":"#We will now drop the avg_transaction_value column\n#train = train.drop([\"avg_transaction_value\"], axis=1)\n#test = test.drop([\"avg_transaction_value\"], axis=1)","c1b686ba":"train['points_in_wallet'].value_counts(dropna=False)","3aa6a88e":"test['points_in_wallet'].value_counts(dropna=False)","8446c770":"train['points_in_wallet'].describe()","a7630e83":"test['points_in_wallet'].describe()","731206df":"sns.boxplot(x=train['points_in_wallet'])","d568c67f":"#train['points_in_wallet'].replace(np.NaN, 1,inplace=True)\n#train['points_in_wallet'] = np.where(train['points_in_wallet'].between(-760,0), 0, train['points_in_wallet'])\n#train['points_in_wallet'] = np.where(train['points_in_wallet'].between(0,2069), 2, train['points_in_wallet'])\n\n#test['points_in_wallet'].replace(np.NaN, 1,inplace=True)\n#test['points_in_wallet'] = np.where(test['points_in_wallet'].between(-549,0), 0, test['points_in_wallet'])\n#test['points_in_wallet'] = np.where(test['points_in_wallet'].between(0,1816), 2, test['points_in_wallet'])\n","7caeee29":"print(numerical_columns)","ad7b6d79":"train['days_since_joined'] = train['joining_date'].apply(lambda x:(pd.Timestamp('today') - x).days)\ntest['days_since_joined'] = test['joining_date'].apply(lambda x:(pd.Timestamp('today') - x).days)\ntrain.head()","916b6747":"train.drop(['joining_date'], inplace=True, axis=1)\ntest.drop(['joining_date'], inplace=True, axis=1)\ntrain.head()","39565c06":"train.drop(['last_visit_time'], inplace=True, axis=1)\ntest.drop(['last_visit_time'], inplace=True, axis=1)\ntrain.head()","0a6efe96":"# fill missing values with mean column values\n#train['avg_frequency_login_days'].fillna(train['avg_frequency_login_days'].median(), inplace=True)\n#test['avg_frequency_login_days'].fillna(test['avg_frequency_login_days'].median(), inplace=True)","fc230461":"#new_completed_data.isnull().sum()","c072eef2":"import miceforest as mf\n\n# Create kernels. \nkernel = mf.MultipleImputedKernel(\n  data=train,\n  save_all_iterations=True,\n  random_state=1991\n)\n\n# Run the MICE algorithm for 3 iterations on each of the datasets\nkernel.mice(3,verbose=True)\n","bdd4e5d4":"# Make a multiple imputed dataset with our new data\nnew_data_train = kernel.impute_new_data(train)\n\n# Return a completed dataset\ndf_train = new_data_train.complete_data(0)\n","0ce9e2de":"import miceforest as mf\n\n# Create kernels. \nkernel = mf.MultipleImputedKernel(\n  data=test,\n  save_all_iterations=True,\n  random_state=1991\n)\n\n# Run the MICE algorithm for 3 iterations on each of the datasets\nkernel.mice(3,verbose=True)","4914c81f":"# Make a multiple imputed dataset with our new data\nnew_data_test = kernel.impute_new_data(test)\n\n# Return a completed dataset\ndf_test = new_data_test.complete_data(0)","3b6407fb":"# setting target wrong value -1 to 1 assuming sign issue, \n# and setting 5 to 0 for training after prediction revert it back to 5\ndf_train['churn_risk_score'] = df_train['churn_risk_score'].apply(lambda x:1 if x == -1 else 0 if x == 5 else x)\ndf_train['churn_risk_score'].unique()","bde2fcc0":"#separateing the dataset\nseed = 12\nnp.random.seed(seed)\n\ndf_train = df_train.sample(frac=1, random_state=seed).reset_index(drop=True)\ny = df_train.pop('churn_risk_score')\nX = df_train\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.25, random_state=1)","e6cab967":"# split into train dev and test\nX_train, X_test, y_train, y_test = skms.train_test_split(X, y, train_size=0.8)\n","756a1fda":"# reset index for X_train and X_test\nX_train.reset_index(drop=True, inplace=True)\nX_test.reset_index(drop=True, inplace=True)\n","97c1cff9":"class_weights = sku.class_weight.compute_class_weight('balanced', np.unique(y_train), y_train)\nclass_weights = dict(enumerate(class_weights))\nclass_weights","e453a6c5":"sample_weights = sku.class_weight.compute_sample_weight('balanced', y_train)\nsample_weights","d70d9eca":"def printScore(y_train, y_train_pred):\n    print(skm.f1_score(y_train, y_train_pred, average=\"macro\"))","e69efd2c":"import catboost as cb\n\ncat_model = cb.CatBoostClassifier(verbose=0, iterations=100, \n#                                   eval_metric='F1', \n                                  class_weights=class_weights, \n#                                   use_best_model=True\n                                 )\ncat_model.fit(X_train, y_train, eval_set=(X_test, y_test))\nprint(cat_model.best_score_)\n\ny_train_pred = cat_model.predict(X_train)\ny_test_pred = cat_model.predict(X_test)\nprint(skm.accuracy_score(y_train, y_train_pred))\nprint(skm.accuracy_score(y_test, y_test_pred))\nprintScore(y_train, y_train_pred)\nprintScore(y_test, y_test_pred)","e956a2a5":"#applying xgboost\nmodel_xgb = xgb.XGBClassifier(objective ='multi:softprob',verbose=0, scoring='f1', \n                             learning_rate=0.001, subsample=0.5, n_jobs=-1, \n                             n_estimators=100, max_depth = 10)\n\n\nmodel_xgb.fit(X_train, y_train, early_stopping_rounds=7, eval_set=[(X_test, y_test)], verbose=1)\n","15f3a6fb":"from sklearn.metrics import f1_score\nimport sklearn.metrics as metrics\ny_pred = model_xgb.predict(X_test)\nscore = 100 * metrics.f1_score(y_test, y_pred, average=\"macro\")","c1a24abb":"print(score)","ad712912":"import lightgbm as lgb\nmodel_lgb = lgb.LGBMClassifier(objective='multi', \n                               class_weight=class_weights,\n                               random_state=1, n_jobs=-1, \n                               learning_rate=0.15, \n                               n_estimators=100)\nmodel_lgb.fit(X_train, y_train)\ny_pred = model_lgb.predict(X_test)\nscore = 100 * metrics.f1_score(y_test, y_pred, average=\"macro\")\nprint(score)","7b1f7e6a":"1.2.4. points_in_wallet","252309f0":"test.isnull().sum()","3375b5ec":"train.shape","1175d6df":" # 1.1.1 avg_frequency_login_days(Represents the no. of times a customer has logged in to the website)","49b45968":"1.2.1 . avg_frequency_login_days","e27a31bc":"As, we have completed the feature engineering part.We will no work towards repalcing NaN values or droping them.","ab155cb0":"1.2.2. Age","5ac75158":"<h1> 1.1.8 used_special_discount<\/h1>","4d035450":"# \"joining_date\"","0fc91a01":"test.nunique()","82f7bd11":"<h1> 1.1.3 region Category<\/h1>","1c7dfadf":"Out of 37k values , 17846 vales are xxxxxx which is around 40 percent value and it means they are missing.\nSo, as of now we will drop the column.","2324eebb":"1.2.4. avg_time_spent","f21a66c0":"So, It looks like -999 is an 'Error',i.e., the website didn't populate the column when the data was populated.\nWe will now replace -999 with NaN value,So, that we can visualize how the data is spread out and find the outliers.","0a18d93b":"We can see that there are negative values in the column.It happns when :\nmost companies deduct rewards for returns and refunds. If you already have a low rewards balance, these refunds can cause a negative rewards balance(points in wallet).\n\nWe will now divide negative values in 0 category,1 to NaN and 2 to +ve values.\n","d7d867a5":"So, It looks like there are 3522 Nan values in 'avg_frequency_login_days' column.","07946a48":"We will work with NA imputation later.","b7dbc518":"There is a huge difference between some of the classes in the target(churn_risk_score) column.(-1,1 and 2 are quite less as compared 3,4,5).\nWe will handle it when creating the model","ff760f94":"It looks like avg_freq_login_days(Represents the no. of times a customer has logged in to the website) column is a numeric type column \nbut it is showing up in the cat columns because we weren't able to convert it as it had a value = ERROR (count-3500 values).\n\nERROR value means the website was unable to register the avg_freq_login_days due to internal problem may be software glitch etc.\nIt will be NaNs inplace of the ERROR values.\nSo, we will replace it with NaN value ","a7ac888e":"1.2.5. avg_transaction_value","72b6d7de":"test.info()","c5a8368b":"#I removed the extreme ones as they were quite less in number\n\nindex = train[(train['points_in_wallet'] >= 2000)|(train['points_in_wallet'] <= -600)].index\ntrain.drop(index, inplace=True)\n","e027edda":"So, we can see that Name and security_no are tottaly unique in test and train data.\nWe can remove them as of now.","770fa5e2":"<h1> 1.1.13 referral_id<\/h1>","116ecd70":"<h1> 1.1.7 internet_option<\/h1>","fd1e61fb":"We can see that there are -values in the columns but how can average time spent can be negative?\nI will find out later but after as of now. I will remove the outliers.\ndf=train.copy()\n\n","44c32baf":" # 1.Data Exploration","deec5ee9":"<h1> 1.1.12 feedback<\/h1>","5acdba37":"# Catboost","d0dc6423":"<h1> 1.1.5 joined_through_referral<\/h1>","01dc2b9e":"So, It looks like that the charn rate is only 2,1 and -1 and the outliers are customers who pay the highest money to the website.\nWe won't be removing the value rather we will replace the column with bins.","3ff08152":"train.nunique()","cb33aa90":"**NULL VALUES**\n\nIt looks like there are null values in some of the columns in test and train set.\nWe will now try to the find the amount of null values in each dataset.","98eb6346":"<h1> 1.1.10 past_complaint<\/h1>","d7df8ca5":"<h1> 1.2. Cont Columns <\/h1>\n\n","8b714c72":"There are NaN values in the points in wallet columns.There might be two reason:\n    \n    1. The user never bought anything from the website but as we have seen before average_transaction_value doesn't have any NaN value.So, our first assumption is wrong.\n    2. The users have a option to select whether they want to use a wallet or not.If they opt out of using wallet there points_in_wallet column is marked as NaN.\n    ","9f99fbe2":"profile = ProfileReport(df)\nprofile","a92cd4a8":"1.2.3. Days_since_last_login","ff90d610":"# 3.Replacing NaN values ","be7abf7d":"<h1> 1.1.6 medium_of_operation<\/h1>\n\n","e24010e9":"# 1.1. Categorical variable","20252a4c":"<h1> 1.1.6 preferred_offer_types<\/h1>\n","77628435":"<h1> 1.1.11 complaint_status<\/h1>","5c2ec3bb":"It looks like there are no  outliers.So, we won't be removing any values.","c4bcf0e6":"According the compeition, the churn score is between 1 to 5  but here we can see that -1.\nAfter a little searching over the internet, I found that the negative churn is actually good.\n\n**What is negative churn?**\n\nIt is achieved when the total additional revenue generated from existing customers is greater than the revenue lost from cancellations and downgrades. When your recurring revenue grows without the addition of new customers, you\u2019re achieving positive net revenue retention.\n\nSimply, net negative churn is when current customers are spending so much additional money (services, upgrades, and add-ons) that your churn is offset by it. \n\nfor more information : [https:\/\/www.profitwell.com\/recur\/all\/negative-churn](http:\/\/)","c346ad18":"<h1> 1.1.2 Gender<\/h1>","9fa892e4":"<h1> 1.1.4 membership_category<\/h1>","1fdd64d1":" **Train data**:\n\n36992 rows, 25 columns\n\nTypes of columns:\n    1. numerical : 6\n    2. categorical : 19 \n    \nMissing values in 3 columns, they are:\n    1. region_category\n    2. preferred_offer_types\n    3. points_in_wallet\n    \n **Test data**\n\n19919 rows, 24 columns\n\nTypes of columns:\n    1. numerical : 5\n    2. categorical : 19 \n \nMissing values in 3 columns, they are:\n    1. region_category\n    2. preferred_offer_types\n    3. points_in_wallet\n ","d8d87541":"So, It looks like there are no outliers in the column.","8ed9a42c":"train.info()","a3555e0f":"train.isnull().sum()","34a927c4":"** Target Variable**","5a123e43":"<h1> 1.1.9 offer_application_preference<\/h1>"}}