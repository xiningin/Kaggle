{"cell_type":{"9058a317":"code","265a3174":"code","4ac14b36":"code","f8f4a506":"code","bd536d37":"code","a4ebd9c2":"code","3e1c6ffa":"code","bd477612":"code","29c47f67":"code","7afca321":"code","03174914":"code","fa7b1bf9":"code","d1af3424":"code","cd03ccfd":"code","b3642401":"code","3c399d72":"code","a8188ecd":"code","c8715824":"code","34ac64df":"code","b864f7d4":"code","55491f2f":"code","5a9d140f":"code","a120b282":"code","3f3f5029":"code","2c08afd6":"code","26140b8b":"code","d9abc702":"code","d64571ac":"code","d13d732e":"code","5c6a7021":"code","6b90818f":"code","83339150":"code","815eacd0":"code","c7c14dda":"markdown","8f87ed09":"markdown","1e26844a":"markdown","24930f18":"markdown","bb267070":"markdown","a9477c66":"markdown","af8cca45":"markdown","70a3c44b":"markdown","de0edd17":"markdown","69a2b376":"markdown","eedc77d8":"markdown","7dbb97ca":"markdown"},"source":{"9058a317":"import torch\nimport torch.nn as nn\n\nfrom torch.utils.data import Dataset, DataLoader\nimport torchvision.datasets as datasets\nimport torch.optim as optim\nimport torch.nn.functional as F","265a3174":"VGG16 = [64, 64, 'M', 128, 128, 'M', 256, 256,256, 'M', 512,512,512, 'M', 512,512,512,'M' ]","4ac14b36":"num_classes = 5\nin_channels = 3","f8f4a506":"#testing\na = []\nb = [2]\na += b\n# print(a += b)\na.append(b)\nprint(a)\n# c = a+=b\nprint(*a)","bd536d37":"class genVGG(nn.Module):\n    def __init__(self, in_channels=in_channels, num_classes=num_classes, arch=VGG16):\n        super(genVGG, self).__init__()\n        self.in_channels = in_channels\n        self.conv_layer = self.create_conv_layers(arch)\n        \n        self.fc = nn.Sequential(\n            nn.Linear(512*7*7, 4096),\n            nn.ReLU(),\n            nn.Dropout(p=0.5),\n            nn.Linear(4096, 4096),\n            nn.ReLU(),\n            nn.Dropout(p=0.5),\n            nn.Linear(4096, num_classes),\n        )\n        \n    def create_conv_layers(self, arch):\n        layers = []\n        \n        in_channels = self.in_channels\n        \n        for x in arch:\n            if isinstance(x, int):\n                out_channels = x\n                layers += [\n                            nn.Conv2d(in_channels=in_channels, out_channels=out_channels, kernel_size=(3,3), padding=(1,1), stride=(1,1)), \n                            nn.BatchNorm2d(x),\n                            nn.ReLU()\n                          ]\n                \n                in_channels = x\n            \n            if isinstance(x, str):\n                layers += [nn.MaxPool2d(kernel_size=(2,2),stride=(2,2))]\n        \n#         print(\"Model Summary: \")\n#         for layer in layers:\n#             print(layer)\n#         print(f\"conv Model loaded. number of layers : {len(layers)}\")\n\n            \n        return nn.Sequential(*layers)\n                \n    def forward(self, x):\n        x = self.conv_layer(x)\n        x = x.reshape(x.size(0), -1)\n        x = self.fc(x)\n        return x\n        ","a4ebd9c2":"model_1 = genVGG()","3e1c6ffa":"x = torch.randn(1,3,224,224)\ny = model_1(x)\ny.shape","bd477612":"class selfVGG(nn.Module):\n    def __init__(self,num_classes=5):\n        super(VGG, self).__init__()\n        self.conv1 = nn.Conv2d(in_channels=3, out_channels=64, kernel_size=(3,3), padding=(0,0), stride=(1,1))\n        self.conv2 = nn.Conv2d(in_channels=64, out_channels=128, kernel_size=(3,3), padding=(0,0), stride=(1,1))\n        self.conv3 = nn.Conv2d(in_channels=128, out_channels=256, kernel_size=(3,3),padding = (0,0), stride=(1,1))\n        self.conv4 = nn.Conv2d(in_channels=256, out_channels=512, kernel_size=(3,3), padding=(0,0), stride=(1,1))\n        self.linear1 = nn.Linear(73728, num_classes)\n        self.maxpool = nn.MaxPool2d(kernel_size=(2,2), stride=(2,2))\n        \n    def forward(self, x):\n        out = F.relu(self.conv1(x))\n        out = self.maxpool(out)\n        out = F.relu(self.conv2(out))\n        out = self.maxpool(out)\n        out = F.relu(self.conv3(out))\n        out = self.maxpool(out)\n        out = F.relu(self.conv4(out))\n        out = self.maxpool(out)\n        out = out.reshape(out.size(0), -1)\n        out = self.linear1(out)\n        return out\n        ","29c47f67":"import os\n\nclass_names = []\nlabels = []\nall_inputs = []\n\nacceptable_formats = ['jpg', 'JPG', 'jpeg', 'JPEG']\nroot_dir = '..\/input\/yoga-poses-dataset\/DATASET\/TRAIN'\nfor idx, name in enumerate(os.listdir(root_dir)):\n    class_names.append(name)\n    \n    for root, subdir,files in os.walk(os.path.join(root_dir, name)):\n        for file in files:\n            if file.split('.')[1] in acceptable_formats:\n                all_inputs.append(os.path.join(root_dir, name, file))\n                labels.append(idx)\n            else: \n#                 print(f\"file {file} not included.\")\n                continue\n\nall_inputs[0], labels[0], len(all_inputs), len(labels)","7afca321":"from PIL import Image\nImage.LOAD_TRUNCATED_IMAGES = True\n\nimport torchvision.transforms as transform","03174914":"my_transform = transform.Compose([\n    transform.Resize((224,224)),\n    transform.ToTensor()\n])","fa7b1bf9":"class_names = []","d1af3424":"class yogaDataset(Dataset):\n    def __init__(self, root_dir, transform=my_transform):\n\n        self.labels = []\n        self.all_inputs = []\n        self.transform = transform\n        \n        self.total_inputs = 0\n        for idx, name in enumerate(os.listdir(root_dir)):\n            class_names.append(name)\n\n            for root, subdir,files in os.walk(os.path.join(root_dir, name)):\n                print(f\"Total files in {name}: {len(files)}\")\n                self.total_inputs += len(files)\n                for file in files:\n                    if file.split('.')[1] in acceptable_formats:\n                        current_file = os.path.join(root_dir, name, file)\n                        \n                        #checking for dimensions\n                        img = Image.open(current_file)\n                        \n#                         print('current file : ', current_file, \" : \")\n                        \n                        try:\n                            if (transform):\n                                img = self.transform(img)\n                            \n                                if(img.size(0) == 3):\n                                    self.all_inputs.append(current_file)\n                                    self.labels.append(idx)\n#                                     print(\"added\")\n                                else:\n#                                     print(f\"{img.size()} not included. Incorrect dimension!\")\n                                    continue\n                                    \n                        except Exception as e:\n#                             print(e, f\"not included! transform error!\")\n                            continue\n                            \n                        \n                    else:\n                        continue\n#                         print(f\"not included. Incorrect format!\")\n        print(\"Total files processed: \", len(self.all_inputs), \"\/\", self.total_inputs)\n                    \n                          \n    def __len__(self):\n        return len(self.all_inputs)\n    \n    def __getitem__(self, index): \n        \n#         print(f\"accessed: , {all_inputs[index]}\")\n        \n        img = Image.open(all_inputs[index])\n        \n        if(self.transform is not None):\n            img = self.transform(img)\n        \n        target = torch.tensor(self.labels[index])\n        return img, target","cd03ccfd":"# train_dataset = yogaDataset('..\/input\/yoga-poses-dataset\/DATASET\/TRAIN', transform=my_transform)\ntest_dataset = yogaDataset('..\/input\/yoga-poses-dataset\/DATASET\/TEST', transform=my_transform)","b3642401":"x,y = test_dataset[300]\n\n# print('total dataset size: ', train_dataset.__len__())\nprint('test dataset length: ', test_dataset.__len__())\nimport matplotlib.pyplot as plt\n\nplt.imshow(x.permute(1,2,0))\nprint(class_names[int(y)])\n\n","3c399d72":"# for i in range(len(train_dataset)):\n#     x, y = train_dataset[i]\n#     if x.shape[0] != 3:\n#         print(x.shape)","a8188ecd":"transformations = transform.Compose([\n    transform.Resize(255),\n    transform.CenterCrop(224),\n    transform.ToTensor(),\n    transform.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n])","c8715824":"train_set = datasets.ImageFolder(\"..\/input\/100-bird-species\/train\", transform = transformations)\ntest_set = datasets.ImageFolder(\"..\/input\/100-bird-species\/test\", transform = transformations)","34ac64df":"x,y = train_set[30000]\n\nprint('test dataset size: ', test_set.__len__())\nprint('train dataset length: ', train_set.__len__())\nimport matplotlib.pyplot as plt\n\nplt.imshow(x.permute(1,2,0))\nprint(int(y))\n","b864f7d4":"import numpy","55491f2f":"train_subset = torch.utils.data.Subset(train_set, numpy.random.choice(len(train_set), 5000, replace=False))","5a9d140f":"train_dataloader = DataLoader(dataset=train_subset, batch_size=64, shuffle=True )\ntest_dataloader = DataLoader(dataset=test_set, batch_size=64, shuffle=True)","a120b282":"device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')","3f3f5029":"model = genVGG().to(device=device)","2c08afd6":"import torchvision.models as models\n\nmodel = models.vgg16(pretrained=True)\n\nfor param in model.parameters():\n    param.requires_grad = False\n    ","26140b8b":"num_classes =270","d9abc702":"# model.avgpool = torch.nn.Identity()\nmodel.classifier[6] = nn.Linear(in_features=4096, out_features=270, bias=True)","d64571ac":"model = model.to(device=device)\nmodel","d13d732e":"# testing\nx = torch.randn(10, 3, 224, 224).to(device=device)\ny = model(x)\ny.shape","5c6a7021":"optimizer = optim.Adam(model.parameters(), lr=0.01)\nloss_criterion = nn.CrossEntropyLoss()","6b90818f":"for epoch in range(5):\n    try:\n        for data, target in train_dataloader:      \n                data = data.to(device=device)\n                target = target.to(device=device)\n\n                score = model(data)\n                optimizer.zero_grad()\n\n                loss = loss_criterion(score, target)\n                \n                loss.backward()\n\n                optimizer.step()\n    except Exception as e:\n        print(e)\n    print(f\"For epoch {epoch}, the loss is {loss}\")\n\n            \n            ","83339150":"def check_accuracy(model, loader):\n    model.eval()\n    correct_samples = 0\n    total_samples = 0\n    \n    for x, y in loader:\n        x = x.to(device=device)\n        y = y.to(device=device)\n        \n        score = model(x)\n        _, predictions = score.max(1)\n        correct_samples += (y==predictions).sum()\n        total_samples += predictions.size(0)\n    \n    model.train()\n    print(f'total correct {correct_samples} out of {total_samples} with accuracy of {(correct_samples\/total_samples)*100}%')","815eacd0":"check_accuracy(model,train_dataloader)\ncheck_accuracy(model, test_dataloader)","c7c14dda":"Now according to the paper, the VGG kernel size is 3x3 with stride of 1 and padding of 0, which is same convolution. if you look at the table implementing the architecture of VGG 16, you will notice the specification. Ill make an array of that, the numbers in the array means channels while the `M` stands for maxpooling layer.\n\n![](https:\/\/raw.githubusercontent.com\/yashk2810\/yashk2810.github.io\/master\/images\/vgg-16-architecture.png)\n\nBut I guess this picture makes more sense, in terms of implementation details.\n\n![](https:\/\/miro.medium.com\/max\/3818\/1*MEuoX8lso9vNn8savmTwHQ.png)","8f87ed09":"I am going to use a totally different dataset. Just to see whether its really the dataset that is causing the problems.","1e26844a":"### We are going to implement the VGG model from scratch in pytorch\n\nMade with reference to this wonderful video tutorial by [Aladdin Persson](https:\/\/www.youtube.com\/watch?v=ACmuBbuXn20)\n\nPaper available at [here](https:\/\/arxiv.org\/pdf\/1409.1556.pdf)\n\nThis model is nicely summarized in this diagram:\n\n![](https:\/\/www.researchgate.net\/profile\/Jose_Cano31\/publication\/327070011\/figure\/fig1\/AS:660549306159105@1534498635256\/VGG-16-neural-network-architecture.png) \n\n","24930f18":"Lets test the model. We will use `torch.randn` to create a tensor.","bb267070":"Turns out you cannot train VGG implementation from scratch like that.","a9477c66":"lets try and download a vgg model","af8cca45":"The amount of data is huge here so lets create a subset.","70a3c44b":"given below is a vgg implementaion I tried.","de0edd17":"Lets start by importing the necessary modules for building our code.","69a2b376":"If you look at the dataset provided you'll notice that there are five different poses of yoga.","eedc77d8":"This dataset has a lot of errors. and random pictures scraped form web scraped together.","7dbb97ca":"### Problems with the dataset\n\nThis dataset is actually a headache because of all the png file which have dimension 4 compared to jpeg files of dimension 3, mixed with some single channel images. So we need to cater for that while building dataset.\n\nI am going to skip all images that have incorrect dimension."}}