{"cell_type":{"044faf82":"code","10aab9f8":"code","1b2a75b2":"code","ab30094c":"code","7d58013e":"code","79471311":"code","41c38160":"code","2793c362":"code","5b360cf3":"code","90a365b1":"code","cff89e30":"code","c5eaebca":"code","fc2f2f42":"code","5895d27c":"code","1307b104":"code","94449fe2":"code","b3f68909":"code","3891edb8":"code","661bec21":"code","681ce510":"code","18547fb8":"code","a29c9631":"code","11e21fde":"code","61c729c2":"code","3a71e49e":"code","c79da673":"code","d289b284":"code","739b042e":"code","2a106ae0":"code","3445c4b0":"code","0dbbefcf":"code","7cedfb5a":"code","ceac871c":"code","d456b52a":"code","43eea206":"code","bd06334b":"code","97220a8c":"code","1fc0f983":"code","de423d9e":"code","9a385690":"code","ad3ebc5c":"code","6e97b172":"code","121d01f7":"code","d2579996":"code","9c0a6a15":"code","0108e4a9":"code","f8264c19":"markdown","85a56bf1":"markdown","398f14cc":"markdown","10c298e1":"markdown","0dba2fd3":"markdown","541b2cd1":"markdown","14d37d8d":"markdown","022a1093":"markdown","c4287c59":"markdown","e87fded3":"markdown","0c59c180":"markdown","20933ce2":"markdown","51d1b3f1":"markdown","fe89f3b2":"markdown","8eccc0ce":"markdown","66cf2c21":"markdown"},"source":{"044faf82":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nfrom mpl_toolkits.mplot3d.axes3d import Axes3D\nfrom matplotlib import cm\n\nfrom math import log\nfrom sympy import symbols, diff\n\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import mean_squared_error\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n        \n%matplotlib inline","10aab9f8":"def f(x):\n    return x**2 + x + 1","1b2a75b2":"# Making custom data\nx_1 = np.linspace(start = -3, stop = 3, num=100)","ab30094c":"# Plotting the data\nplt.xlim(-3, 3)\nplt.xlabel('X', fontsize=16)\nplt.ylim(0, 8)\nplt.ylabel('f(x)', fontsize=16)\nplt.plot(x_1, f(x_1))\n\nplt.show()","7d58013e":"def df(x):\n    return 2*x + 1","79471311":"# Plot function and derivative side by side \n\nplt.figure(figsize=(15,5))\n\n# Chart 1: Cost Function\nplt.subplot(1, 2, 1)\n\nplt.xlim(-3, 3)\nplt.ylim(0, 8)\n\nplt.title(\"Cost Function\", fontsize=17)\nplt.xlabel('X', fontsize=16)\nplt.ylabel('f(x)', fontsize=16)\n\nplt.plot(x_1, f(x_1), color='blue', linewidth=3)\n\n# Chart 2: Derivative\nplt.subplot(1, 2, 2)\n\nplt.xlim(-2, 3)\nplt.ylim(-3, 6)\n\nplt.title(\"Slope of the Cost Function\", fontsize=17)\nplt.xlabel('X', fontsize=16)\nplt.ylabel('df(x)', fontsize=16)\n\nplt.plot(x_1, df(x_1), color='skyblue', linewidth=5)\n\nplt.show()","41c38160":"# Gradient Descent\n\nnew_x = 3\nprevious_x = 0\nstep_multiplier = 0.1\nprecision = 0.00001\n\nx_list = [new_x]\nslope_list = [df(new_x)]\n\nfor n in range(500):\n    previous_x = new_x\n    gradient = df(new_x)\n    new_x = previous_x - gradient * step_multiplier\n    step_size = abs(new_x - previous_x)\n    \n    x_list.append(new_x)\n    slope_list.append(df(new_x))\n    \n    if step_size < precision:\n        print(\"Loop ran this many times before reaching precision point: \", n)\n        break\n\nprint(\"Local minimum occurs at: \", new_x)\nprint(\"Slope or df(x) value at this point is: \", df(new_x))\nprint(\"f(x) value at this point is: \", f(new_x))","2793c362":"# Superimposing the gradient descent calculations on the existing graphs\n\nplt.figure(figsize=(15,5))\n\n# Chart 1: Cost Function\n\nplt.subplot(1,3,1)\n\nplt.xlim(-3, 3)\nplt.ylim(0, 8)\n\nplt.title('Cost Function', fontsize=17)\nplt.xlabel('X', fontsize=16)\nplt.ylabel('f(x)', fontsize=16)\n\nplt.plot(x_1, f(x_1), color='blue', linewidth=3, alpha=0.8)\n\nvalues = np.array(x_list)\nplt.scatter(x_list, f(values), color='red', s=100, alpha=0.6)\n\n# Chart 2: Derivative\n\nplt.subplot(1,3,2)\n\nplt.xlim(-2, 3)\nplt.ylim(-3, 6)\n\nplt.title('Slope of the Cost Function', fontsize=17)\nplt.xlabel('X', fontsize=16)\nplt.ylabel('df(x)', fontsize=16)\nplt.grid()\n\nplt.plot(x_1, df(x_1), color='skyblue', linewidth=3, alpha=0.8)\n\nplt.scatter(x_list, slope_list, color='red', s=100, alpha=0.6)\n\n# Chart 3: Derivative (Close-up)\n\nplt.subplot(1,3,3)\n\nplt.xlim(-0.55, -0.2)\nplt.ylim(-0.3, 0.8)\n\nplt.title('Gradient Descent (Close-up)', fontsize=17)\nplt.xlabel('X', fontsize=16)\nplt.ylabel('df(x)', fontsize=16)\nplt.grid()\n\nplt.plot(x_1, df(x_1), color='skyblue', linewidth=6, alpha=0.8)\n\nplt.scatter(x_list, slope_list, color='red', s=300, alpha=0.6)\n\nplt.show()","5b360cf3":"x_2 = np.linspace(start=-2, stop=2, num=1000)\n\ndef g(x):\n    return x**4 - 4*x**2 + 5\n\ndef dg(x):\n    return 4*x**3 - 8*x","90a365b1":"# Plot function and derivative side by side\n\nplt.figure(figsize=(15,5))\n\n# Chart 1: Cost Function \nplt.subplot(1,2,1)\n\nplt.xlim(-2, 2)\nplt.ylim(0.5, 5.5)\n\nplt.title('Cost Function', fontsize=17)\nplt.xlabel('X', fontsize=16)\nplt.ylabel('g(x)', fontsize=16)\n\nplt.plot(x_2, g(x_2), color='blue', linewidth=3)\n\n# Chart 2: Derivative\nplt.subplot(1,2,2)\n\nplt.xlim(-2, 2)\nplt.ylim(-6, 8)\n\nplt.title('Slope of the Cost Function', fontsize=17)\nplt.xlabel('X', fontsize=16)\nplt.ylabel('dg(x)', fontsize=16)\n\nplt.plot(x_2, dg(x_2), color='skyblue', linewidth=5)\n\nplt.show()","cff89e30":"def gradient_descent(derivative_func, initial_guess, multiplier=0.02, precision=0.001, max_iter=300):\n    new_x = initial_guess\n    x_list = [new_x]\n    slope_list = [derivative_func(new_x)]\n    \n    for n in range(max_iter):\n        previous_x = new_x\n        gradient = derivative_func(new_x)\n        new_x = previous_x - gradient * multiplier\n        \n        step_size = abs(new_x - previous_x)\n        x_list.append(new_x)\n        slope_list.append(derivative_func(new_x))\n        \n        if step_size < precision:\n            break\n    return new_x, x_list, slope_list","c5eaebca":"local_min, list_x, deriv_list = gradient_descent(dg, 0.5, 0.02, 0.001)\nprint(\"Local minimum occurs at: \", local_min)\nprint(\"Number of steps: \", len(list_x))","fc2f2f42":"local_min, list_x, deriv_list = gradient_descent(derivative_func=dg, initial_guess=-0.5,\n                                                 multiplier=0.01, precision=0.0001)\nprint(\"Local minimum occurs at: \", local_min)\nprint(\"Number of steps: \", len(list_x))","5895d27c":"local_min, list_x, deriv_list = gradient_descent(derivative_func=dg, initial_guess=-0.1)\nprint(\"Local minimum occurs at: \", local_min)\nprint(\"Number of steps: \", len(list_x))","1307b104":"# Calling gradient descent function\nlocal_min, list_x, deriv_list = gradient_descent(derivative_func=dg, initial_guess=0.5)\n\n# Plot function, derivative and scatter plot side by side \n\nplt.figure(figsize=(15,5))\n\n# Chart 1: Cost Function\nplt.subplot(1,2,1)\n\nplt.xlim(-2, 2)\nplt.ylim(0.5, 5.5)\n\nplt.title('Cost Function', fontsize=17)\nplt.xlabel('X', fontsize=16)\nplt.ylabel('g(x)', fontsize=16)\n\nplt.plot(x_2, g(x_2), color='blue', linewidth=3, alpha=0.8)\nplt.scatter(list_x, g(np.array(list_x)), color='red', s=100, alpha=0.6)\n\n# Chart 2: Cost\nplt.subplot(1,2,2)\n\nplt.xlim(-2, 2)\nplt.ylim(-6, 8)\n\nplt.title('Slope of the Cost Function', fontsize=17)\nplt.xlabel('X', fontsize=16)\nplt.ylabel('dg(x)', fontsize=16)\nplt.grid()\n\nplt.plot(x_2, dg(x_2), color='skyblue', linewidth=5, alpha=0.6)\nplt.scatter(list_x, deriv_list, color='red', s=100, alpha=0.5)\n\nplt.show()","94449fe2":"# Make Data\nx_3 = np.linspace(start=-2.5, stop=2.5, num=1000)\n\ndef h(x):\n    return x**5 - 2*x**4 + 2\ndef dh(x):\n    return 5*x**4 - 8*x**3","b3f68909":"# Calling gradient descent function\nlocal_min, list_x, deriv_list = gradient_descent(derivative_func=dh, initial_guess=-0.2,\n                                                max_iter=70)\n\n# Plot function and derivative and scatter plot side by side\n\nplt.figure(figsize=(15,5))\n\n# 1 Chart : Cost Function\nplt.subplot(1, 2, 1)\n\nplt.xlim(-1.2, 2.5)\nplt.ylim(-1, 4)\n\nplt.title('Cost Function', fontsize=17)\nplt.xlabel('X', fontsize=16)\nplt.ylabel('h(x)', fontsize=16)\n\nplt.plot(x_3, h(x_3), color='blue', linewidth=3, alpha=0.8)\nplt.scatter(list_x, h(np.array(list_x)), color='red', s=100, alpha=0.6)\n\n# 2 Chart : Derivative\nplt.subplot(1, 2, 2)\n\nplt.xlim(-1, 2)\nplt.ylim(-4, 5)\n\nplt.title('Slope of the Cost Function', fontsize=17)\nplt.xlabel('X', fontsize=16)\nplt.ylabel('dh(x)', fontsize=16)\nplt.grid()\n\nplt.plot(x_3, dh(x_3), color='skyblue', linewidth=5, alpha=0.6)\nplt.scatter(list_x, deriv_list, color='red', s=100, alpha=0.5)\n\nplt.show()\n\nprint('Local Min occurs at: ', local_min)\nprint('Cost at the minimum is: ', h(local_min))\nprint('Number of steps: ', len(list_x))","3891edb8":"import sys \n# sys version\nsys.float_info.max","661bec21":"# Creating a tuple - tuple packing\nbreakfast = 'bacon', 'eggs', 'avocado'\nunlucky_numbers = 13, 9, 26, 17\n\n# How to access value in a tuple\nprint('I love', breakfast[0])\nprint('My hotel has no ' + str(unlucky_numbers[1]) + 'th floor')\n\nnot_my_address = 1, 'Infinite Loop', 'Cupertino', 95014\n\ntuple_with_single_value = 42,\ntype(tuple)\n\nmain, side, greens = breakfast\nprint('Main course is:', main)\n\ndata_tuple = gradient_descent(derivative_func=dh, initial_guess=0.2)\nprint('Local min is at: ', data_tuple[0])\nprint('Cost at local minimum is: ', h(data_tuple[0]))\nprint('Number of steps: ', len(data_tuple[1]))","681ce510":"# Calling gradient descent function\nlocal_min, list_x, deriv_list = gradient_descent(derivative_func=dg, initial_guess=1.9,\n                                                multiplier=0.02, max_iter=500)\n\n# Plot function and derivative and scatter plot side by side\n\nplt.figure(figsize=(15,5))\n\n# 1 Chart : Cost Function\nplt.subplot(1, 2, 1)\n\nplt.xlim(-2, 2)\nplt.ylim(0.5, 5.5)\n\nplt.title('Cost Function', fontsize=17)\nplt.xlabel('X', fontsize=16)\nplt.ylabel('g(x)', fontsize=16)\n\nplt.plot(x_2, g(x_2), color='blue', linewidth=3, alpha=0.8)\nplt.scatter(list_x, g(np.array(list_x)), color='red', s=100, alpha=0.6)\n\n# 2 Chart : Derivative\nplt.subplot(1, 2, 2)\n\nplt.xlim(-2, 2)\nplt.ylim(-6, 8)\n\nplt.title('Slope of the Cost Function', fontsize=17)\nplt.xlabel('X', fontsize=16)\nplt.ylabel('dg(x)', fontsize=16)\nplt.grid()\n\nplt.plot(x_2, dg(x_2), color='skyblue', linewidth=5, alpha=0.6)\nplt.scatter(list_x, deriv_list, color='red', s=100, alpha=0.5)\n\nplt.show()","18547fb8":"# Running gradient descent 3 times\nlow_gamma = gradient_descent(derivative_func=dg, initial_guess=3,\n                                                multiplier=0.0005, precision=0.0001, max_iter=n)\n\nmid_gamma = gradient_descent(derivative_func=dg, initial_guess=3,\n                                                multiplier=0.001, precision=0.0001, max_iter=n)\n\nhigh_gamma = gradient_descent(derivative_func=dg, initial_guess=3,\n                                                multiplier=0.002, precision=0.0001, max_iter=n)\n\n# Experiment\ninsane_gamma = gradient_descent(derivative_func=dg, initial_guess=1.9,\n                                                multiplier=0.25, precision=0.0001, max_iter=n)\n\n# Plotting reduction in cost for each iteration\n\nplt.figure(figsize=(25,10))\n\nplt.xlim(0, n)\nplt.ylim(0, 50)\n\nplt.title('Effect of Learning Rate', fontsize=17)\nplt.xlabel('Number of iterations', fontsize=16)\nplt.ylabel('Cost', fontsize=16)\n\n# Y-axis data:\nlow_values = np.array(low_gamma[1])\n\n# X-axis data:\niteration_list = list(range(0, n+1))\n\n# Plotting low learning rate\nplt.plot(iteration_list, g(low_values), color='lightgreen', linewidth=5)\nplt.scatter(iteration_list, g(low_values), color='lightgreen', s=80)\n\n# Plotting mid learning rate\nplt.plot(iteration_list, g(np.array(mid_gamma[1])), color='steelblue', linewidth=5)\nplt.scatter(iteration_list, g(np.array(mid_gamma[1])), color='steelblue', s=80)\n\n# Plotting high learning rate\nplt.plot(iteration_list, g(np.array(high_gamma[1])), color='hotpink', linewidth=5)\nplt.scatter(iteration_list, g(np.array(high_gamma[1])), color='hotpink', s=80)\n\n# Plotting insane learning rate\nplt.plot(iteration_list, g(np.array(insane_gamma[1])), color='red', linewidth=5)\nplt.scatter(iteration_list, g(np.array(insane_gamma[1])), color='red', s=80)\n\nplt.show()","a29c9631":"def f(x,y):\n    r = 3**(-x**2 - y**2)\n    return 1\/(r+1)","11e21fde":"# Make data\nx_4 = np.linspace(start=-2, stop=2, num=200)\ny_4 = np.linspace(start=-2, stop=2, num=200)\n\nprint('Shape of X array:', x_4.shape)\n\nx_4, y_4 = np.meshgrid(x_4, y_4)\nprint('Array after meshgrid: ', x_4.shape)","61c729c2":"# Generating 3D plot\nfig = plt.figure(figsize=(16,14))\nax = fig.gca(projection='3d')\n\nax.set_xlabel('X', fontsize=20)\nax.set_ylabel('Y', fontsize=20)\nax.set_zlabel('f(x,y) - Cost', fontsize=20)\n\nax.plot_surface(x_4, y_4, f(x_4, y_4), cmap=cm.coolwarm, alpha=0.4)\n\nplt.show()","3a71e49e":"a,b = symbols('x,y')\nprint('Our Cost Function f(x, y) is: ',f(a, b))\nprint('Partial derivative wrt x is: ', diff(f(a,b), a))\nprint('Value of f(x,y) at x = 1.8 and y = 1.0 is: ',\n      f(a,b).evalf(subs={a:1.8, b:1.0})) # Python Dictionary\nprint('Value of partial derivative wrt to x at x = 1.8 and y = 1.0 is: ',\n      diff(f(a,b), a).evalf(subs={a:1.8, b:1.0}))","c79da673":"# Setup\nmultiplier = 0.1\nmax_iter = 500\nparams = np.array([1.8, 1.0]) # initial guess\n\nfor n in range(max_iter):\n    gradient_x = diff(f(a,b), a).evalf(subs={a:params[0], b:params[1]})\n    gradient_y = diff(f(a,b), b).evalf(subs={a:params[0], b:params[1]})\n    gradients = np.array([gradient_x, gradient_y])\n    params = params - multiplier*gradients\n    \n# Results\nprint('Values in gradient array: ', gradients)\nprint('Minimum occurs at x value of: ', params[0])\nprint('Minimum occurs at y value of: ', params[1])\nprint('The cost is: ', f(params[0], params[1]))","d289b284":"# Partial derivative functions example 4\ndef fpx(x, y):\n    r = 2*3**(-x**2 - y**2)*x*log(3)\/(3**(-x**2 - y**2) + 1)**2\n    return r\n\ndef fpy(x, y):\n    r = 2*3**(-x**2 - y**2)*y*log(3)\/(3**(-x**2 - y**2) + 1)**2\n    return r","739b042e":"# Setup\nmultiplier = 0.1\nmax_iter = 500\nparams = np.array([1.8, 1.0]) # initial guess\n\nfor n in range(max_iter):\n    gradient_x = fpx(params[0], params[1])\n    gradient_y = fpy(params[0], params[1])\n    gradients = np.array([gradient_x, gradient_y])\n    params = params - multiplier*gradients\n    \n# Results\nprint('Values in gradient array: ', gradients)\nprint('Minimum occurs at x value of: ', params[0])\nprint('Minimum occurs at y value of: ', params[1])\nprint('The cost is: ', f(params[0], params[1]))","2a106ae0":"# Setup\nmultiplier = 0.1\nmax_iter = 500\nparams = np.array([1.8, 1.0]) # initial guess\nvalues_array = params.reshape(1,2)\nprint(values_array.shape)\n\nfor n in range(max_iter):\n    gradient_x = fpx(params[0], params[1])\n    gradient_y = fpy(params[0], params[1])\n    gradients = np.array([gradient_x, gradient_y])\n    params = params - multiplier*gradients\n    # values_array = np.append(values_array, params.reshape(1,2), axis=0) or\n    values_array = np.concatenate((values_array, params.reshape(1,2)), axis=0)\n    \n# Results\nprint('Values in gradient array: ', gradients)\nprint('Minimum occurs at x value of: ', params[0])\nprint('Minimum occurs at y value of: ', params[1])\nprint('The cost is: ', f(params[0], params[1]))","3445c4b0":"# Generating 3D plot\nfig = plt.figure(figsize=(16,12))\nax = fig.gca(projection='3d')\n\nax.set_xlabel('X', fontsize=20)\nax.set_ylabel('Y', fontsize=20)\nax.set_zlabel('f(x,y) - Cost', fontsize=20)\n\nax.plot_surface(x_4, y_4, f(x_4, y_4), cmap=cm.coolwarm, alpha=0.4)\nax.scatter(values_array[:, 0], values_array[:, 1], \n           f(values_array[:, 0],values_array[:, 1]), s=50, color='red')\n\nplt.show()","0dbbefcf":"# Advanced Numpy Array Practice\n\nkirk = np.array([['Captain', 'Guitar']])\nprint(kirk.shape)\n\nhs_band = np.array([['Black Thought', 'MC'],['QuestLove', 'Drums']])\nprint(hs_band.shape)\n\nprint('hs_band[0] :', hs_band[0])\nprint('hs_band[0][1] :', hs_band[0][1])\nprint('hs_band[1][0] :', hs_band[1][0])\n\nthe_roots = np.append(arr=hs_band, values=kirk, axis=0)\nprint(the_roots)\n\nprint('Printing nicknames....', the_roots[:, 0])\n\nthe_roots = np.append(arr=the_roots, values=[['Malik B', 'MC']], axis=0)\nprint('Printing band roles....', the_roots[:, 1])","7cedfb5a":"# Make Sample Data\nx_5 = np.array([[0.1, 1.2, 2.4, 3.2, 4.1, 5.7, 6.5]]).transpose()\ny_5 = np.array([1.7, 2.4, 3.5, 3.0, 6.1, 9.4, 8.2]).reshape(7,1)\n\nprint('Shape of x_5 array :', x_5.shape)\nprint('Shape of y_5 array :', y_5.shape)","ceac871c":"# Quick Linear Regression\n\nregr = LinearRegression()\nregr.fit(x_5, y_5)\n\nprint('Theta 0 :', regr.intercept_[0])\nprint('Theta 1 :', regr.coef_[0][0])","d456b52a":"plt.scatter(x_5, y_5, s=50)\nplt.plot(x_5, regr.predict(x_5), color='orange', linewidth=3)\nplt.xlabel('X Values')\nplt.ylabel('Y Values')","43eea206":"# y_hat = theta0 + theta1*x1\ny_hat = 0.8475351486029545 + 1.2227264637835913*x_5\nprint('Estimated values of y_hat are :\\n', y_hat)","bd06334b":"print('In comparison the actual y values are :\\n', y_5)","97220a8c":"def mse(y, y_hat):\n    # mse_calc = 1\/7 * (sum((y-y_hat)**2))\n    # mse_calc = (1\/y.size) * (sum((y-y_hat)**2)) \n    mse_calc = np.average((y - y_hat)**2, axis=0)\n    return mse_calc","1fc0f983":"print('Manually calculated MSE function :', mse(y_5, y_hat))\nprint('MSE Regression using manual calc is :', mean_squared_error(y_5, y_hat))\nprint('MSE Regression is :', mean_squared_error(y_5, regr.predict(x_5)))","de423d9e":"nr_thetas = 200\nth_0 = np.linspace(start=-1, stop=3, num=nr_thetas)\nth_1 = np.linspace(start=-1, stop=3, num=nr_thetas)\nplot_t0, plot_t1 = np.meshgrid(th_0, th_1)","9a385690":"plot_cost = np.zeros((nr_thetas, nr_thetas))\n\nfor i in range(nr_thetas):\n    for j in range(nr_thetas):\n        #print(plot_t0[j][i])\n        y_hat = plot_t0[i][j] + plot_t1[i][j]*x_5\n        plot_cost[i][j] = mse(y_5, y_hat)\n        \nprint('Shape of plot_t0 :', plot_t0.shape)\nprint('Shape of plot_t1 :', plot_t1.shape)\nprint('Shape of plot_cost :', plot_cost.shape)","ad3ebc5c":"# Nested Loop Practice \nfor i in range(3):\n    for j in range(3):\n        print(f'value of i is {i} and j is {j}')","6e97b172":"# Plotting MSE\n\nfig = plt.figure(figsize=[16,12])\nax = fig.gca(projection='3d')\n\nax.set_xlabel('Theta 0', fontsize=20)\nax.set_ylabel('Theta 1', fontsize=20)\nax.set_zlabel('Cost - MSE', fontsize=20)\n\nax.plot_surface(plot_t0, plot_t1, plot_cost, cmap=cm.rainbow, alpha=0.8)\nplt.show()","121d01f7":"print('Min Value of plot_cost', plot_cost.min())\nij_min = np.unravel_index(indices=plot_cost.argmin(), shape=plot_cost.shape)\nprint('Minimum occurs at (i,j) :', ij_min)\nprint('Min MSE for theta 0 at plot_t0[111][91] :', plot_t0[111][91])\nprint('Min MSE for theta 1 at plot_t1[111][91] :', plot_t1[111][91])","d2579996":"# x_values, y_values, array of theta parameters (theta0 at index 0 and theta1 at index 1)\ndef grad(x, y, thetas):\n    n = y.size\n    theta0_slope = (-2\/n) * sum(y - thetas[0] - thetas[1]*x)\n    theta1_slope = (-2\/n) * sum((y - thetas[0] - thetas[1]*x)*x)\n    \n    return np.concatenate((theta0_slope, theta1_slope), axis=0)","9c0a6a15":"multiplier = 0.01\nthetas = np.array([2.9, 2.9])\n\n# Collect data points for scatter plot\nplot_vals = thetas.reshape(1,2)\nmse_vals = mse(y_5, thetas[0] + thetas[1]*x_5)\n\nfor i in range(1000):\n    thetas = thetas - multiplier*grad(x_5, y_5, thetas)\n    \n    # Append the new vales into our numpy arrays\n    plot_vals = np.concatenate((plot_vals, thetas.reshape(1,2)), axis=0)\n    mse_vals = np.append(mse_vals, values=mse(y_5, thetas[0] + thetas[1]*x_5))\n    \nprint('The min occurs at theta0 :', thetas[0])\nprint('The min occurs at theta1 :', thetas[1])\nprint('MSE is :', mse(y_5, thetas[0] + thetas[1]*x_5))","0108e4a9":"# Plotting MSE\n\nfig = plt.figure(figsize=[16,12])\nax = fig.gca(projection='3d')\n\nax.set_xlabel('Theta 0', fontsize=20)\nax.set_ylabel('Theta 1', fontsize=20)\nax.set_zlabel('Cost - MSE', fontsize=20)\n\nax.scatter(plot_vals[:, 0], plot_vals[:, 1], mse_vals, s=80, color='black')\n\nax.plot_surface(plot_t0, plot_t1, plot_cost, cmap=cm.rainbow, alpha=0.5)\nplt.show()","f8264c19":"# Example 3 - Diversions Overflow & Python Tuples\n\n## $$ h(x) = x^5 - 2x^4 + 2 $$","85a56bf1":"## MSE and Gradient Descent","398f14cc":"## Partial Derivatives of the MSE w.r.t. $ \\theta_0 $  and $\\theta_1$\n\n## $$ \\frac{\\partial MSE}{\\partial \\theta_0} = -\\frac{2}{n} \\sum_{i=1}^{n}\\big(y^{(i)} - \\theta_0 - \\theta_1 x^{(i)} \\big)$$\n\n## $$ \\frac{\\partial MSE}{\\partial \\theta_1} = -\\frac{2}{n} \\sum_{i=1}^{n}\\big(y^{(i)} - \\theta_0 - \\theta_1 x^{(i)} \\big) \\big(x^{(i)} \\big)$$","10c298e1":"# Example 2 - Multiple Minima vs Initial Guess & Advanced Functions\n\n## $$g(x) = x^4 - 4x^2 + 5$$","0dba2fd3":"# Notebook Imports and Packages","541b2cd1":"# Python Tuples","14d37d8d":"## Partial Derivative and Symbolic Computation\n\n## $$\\frac{\\partial f}{\\partial x} = \\frac{2x \\ln(3) \\cdot 3^{-x^2 - y^2}}{\\left (3^{-x^2 - y^2} + 1 \\right) ^2}$$\n\n## $$\\frac{\\partial f}{\\partial y} = \\frac{2y \\ln(3) \\cdot 3^{-x^2 - y^2}}{\\left (3^{-x^2 - y^2} + 1 \\right) ^2}$$","022a1093":"### Calc MSE using nested for loops","c4287c59":"# Batch Gradient Descent with SymPy","e87fded3":"# Example 4 - Data Vizualisation with 3D Charts\n\n## Minimise $$f(x,y) = \\frac{1}{3^{-x^2 - y^2} + 1} $$\n\nMinimise $$f(x,y) = \\frac{1}{r+1} $$ where $r$ is $3^{-x^2 - y^2} + 1$","0c59c180":"# The Learning Rate","20933ce2":"## Gradient Descent as a Python Function","51d1b3f1":"## Slope and Derivatives","fe89f3b2":"## 3D Plot for Mean Squared Error\n\n### Make Data for Theta","8eccc0ce":"# Example 1 - A simple cost function\n\n## $$f(x) = x^2 + x + 1$$","66cf2c21":"# Example 5 - Working with Data & Real Cost Function\n\n## Mean Squared Error : A cost function for regression problems\n\n### $$ RSS = \\sum_{i=1}^{n} \\big(y^{(i)} - h_\\theta x^{(i)} \\big)^2 $$\n\n### $$ MSE = \\frac{1}{n} \\sum_{i=1}^{n} \\big(y^{(i)} - h_\\theta x^{(i)} \\big)^2 $$\n\n### $$ MSE = \\frac{1}{n} \\sum_{i=1}^{n} \\big(y - \\hat{y} \\big)^2 $$"}}