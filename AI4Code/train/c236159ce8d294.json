{"cell_type":{"4b44e83e":"code","b2a0524a":"code","c7294477":"code","149b24a2":"code","1057d21e":"code","9d650c96":"code","90d9e1c1":"code","ba90c82f":"code","79c4751b":"code","943078f2":"code","4b4a9838":"code","a11c8531":"code","e59262dd":"code","3357cd23":"code","1397e8cf":"code","db2066bb":"code","dbfcf2ee":"code","2422074c":"code","a957d17f":"code","06fe553d":"code","8a664098":"code","b125684c":"code","07e3925c":"code","1cf226a1":"code","c18fd7b1":"code","01ded2f0":"code","7d238926":"code","198d3b79":"markdown","8c4e8911":"markdown","5c62d34c":"markdown","bc0d3610":"markdown","d5c9c868":"markdown","f5c2f39f":"markdown","03da89f8":"markdown","93a7a118":"markdown","afec490b":"markdown","efb59b05":"markdown","f5892a0d":"markdown","e7bf6f21":"markdown","9028d469":"markdown","5784ce91":"markdown","11f0f6ba":"markdown","13532479":"markdown","90638c7b":"markdown","7eca1ed7":"markdown","c1f03b39":"markdown","ed591b5d":"markdown","07ddffcc":"markdown"},"source":{"4b44e83e":"import numpy as np\nimport os\nimport matplotlib.pyplot as plt\nimport matplotlib as mpl\n%matplotlib inline","b2a0524a":"def LoadData(file_path):\n    \n    X_ = np.genfromtxt(file_path, dtype='uint8', delimiter=',', skip_header=1)\n    \n    y_ = X_[:, 0]\n    X_ = X_[:, 1:]\n    print(X_.shape, y_.shape)\n    return X_, y_","c7294477":"X_train, y_train = LoadData('..\/input\/fashionmnist\/fashion-mnist_train.csv')","149b24a2":"def SampleM(m=30):\n    \"\"\"\n    Samples m images for each class and\n    reshapes the sample in (-1, 28, 28).\n    returns: np.array\n    \"\"\"\n    rows_idx = []\n    m = 30\n    idx = np.arange(len(y_train))\n\n    for i in range(10):\n        idx_single_label = idx[y_train == i]\n        rows_idx.extend(np.random.choice(idx_single_label, m))\n    \n    sample = X_train[rows_idx]\n    return sample.reshape(-1, 28, 28)","1057d21e":"def CombineIntoMatrix(sample):\n    \"\"\"\n    Takes np.array (assumes shape(-1, 28, 28)\n    or any other square size can be passed as well)\n    Combine elements into an image with 10 rows and\n    infered number of columns.\n    \"\"\"\n    line_img = np.concatenate(sample, axis=1)\n    col = line_img.shape[1]\n    step = int(col \/ 10)\n\n    idx = np.arange(0, col + 1, step)\n\n    cutted_line = [line_img[:, idx[i]:idx[i+1]] for i in range(10)]\n    return np.concatenate(cutted_line, axis=0)","9d650c96":"def PlotImage(image, figsize=(25, 25), save_fig=False):\n    \n    if figsize:\n        plt.figure(figsize=figsize)\n    plt.imshow(image, cmap=mpl.cm.gray)\n    plt.axis('off')\n    if save_fig:\n        plt.savefig('fashion_sample')","90d9e1c1":"sample = SampleM()\nimage = CombineIntoMatrix(sample)\nPlotImage(image)","ba90c82f":"from sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import cross_val_score","79c4751b":"rand_forest = RandomForestClassifier(random_state=42)\nscores = cross_val_score(rand_forest, X_train, y_train, cv=3, scoring='accuracy')","943078f2":"print('Random Forest score:', np.mean(scores))","4b4a9838":"from sklearn.model_selection import train_test_split\n\nX_train_, X_val, y_train_, y_val = train_test_split(X_train, y_train, random_state=42)\nrand_forest.fit(X_train_, y_train_)\ny_val_predicted = rand_forest.predict(X_val)","a11c8531":"from sklearn.metrics import confusion_matrix","e59262dd":"conf_matrix = confusion_matrix(y_val_predicted, y_val)\nconf_matrix","3357cd23":"np.fill_diagonal(conf_matrix, 0)\n\nplt.figure(figsize=(8,8))\nplt.matshow(conf_matrix)\nplt.colorbar()\nplt.show()","1397e8cf":"i, j = [6, 6, 6], [0, 2, 4]\nX_misclf = [X_val[(y_val==i_) & (y_val_predicted==j_)] for i_, j_ in zip(i, j)]","db2066bb":"m = 100\n\nplt.figure(figsize=(15,15))\ntitles = ['Shirt - T-shirt', 'Shirt - Pullover', 'Shirt - Coat']\n\nfor num, array in enumerate(X_misclf):\n    idx = np.random.permutation(len(array))[:m]\n    image = CombineIntoMatrix(array[idx].reshape(-1, 28, 28))\n    plt.subplot(2, 2, num+1)\n    plt.title(titles[num])\n    PlotImage(image, None)\n\nplt.show()","dbfcf2ee":"from sklearn.neighbors import NearestNeighbors\n\nneare = NearestNeighbors(n_neighbors=2)\nneare.fit(X_train)","2422074c":"y_dist, y_pred_idx = neare.kneighbors(X_train)\n\nthreshold = np.percentile(y_dist[:, 1], 99.5)\n\nX_outliers = X_train[y_dist[:, 1] > threshold]","a957d17f":"image = CombineIntoMatrix(X_outliers.reshape(-1, 28, 28))\nPlotImage(image)","06fe553d":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.preprocessing import StandardScaler\n\nstd_scaler = StandardScaler()\nX_train_scaled = std_scaler.fit_transform(X_train)","8a664098":"from sklearn.ensemble import BaggingClassifier\n\nbag_clf = BaggingClassifier(n_estimators=15, bootstrap=False)\nbag_clf.fit(X_train, y_train)","b125684c":"from sklearn.ensemble import RandomForestClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\n\nfrom sklearn.ensemble import StackingClassifier\n\nestimators = [\n    ('knn', KNeighborsClassifier()),\n    ('svc', SVC(C=100, gamma=0.001)),\n    ('rand_forest', RandomForestClassifier(n_estimators=1000)),\n    ('log_reg', LogisticRegression(C=0.1, solver='saga'))\n]\n\nstacking_clf = StackingClassifier(estimators=estimators, \n                                  final_estimator=LogisticRegression(solver='liblinear'))\nstacking_clf.fit(X_train_scaled, y_train)","07e3925c":"def LoadModels(file_names):\n    models = []\n    for name in file_names:\n        file_path = os.path.join('models', name)\n        models.append(load(file_path))\n    return models","1cf226a1":"X_test, y_test = LoadData('..\/input\/fashionmnist\/fashion-mnist_test.csv')","c18fd7b1":"X_test_scaled = std_scaler.transform(X_test)","01ded2f0":"stacking_clf.score(X_test_scaled, y_test)","7d238926":"bag_clf.score(X_test, y_test)","198d3b79":"### Data Analysis ","8c4e8911":"Plotting some misclassified instances","5c62d34c":"### Testing models ","bc0d3610":"### Building models","d5c9c868":"# Fashion MNIST","f5c2f39f":"Analysing Errors","03da89f8":"Scaling","93a7a118":"Stacking classifier wins.","afec490b":"Test scores for each classifier","efb59b05":"The analysis show that our Random Forest does not make silly mistakes. Many of the instances are controversial and does not provide enogh information to tell what class to assign them to. Look at the bottom of the last square, you can see some shirts with square patterns. I woold say they are shirts before 2021 in which I met fashion for the square patterns, but now: 'Maybe they are such coats?'. \n\nSearch for women shirts some of them I would consider as T-shirts.","f5892a0d":"Do not see instances to consider errors. Another insight is that ankle boots are the most unique in the set, so we should not have problems with classifying them.","e7bf6f21":"Bagging Classifier","9028d469":"Having a lot of shirts classified as t-shirt. Not surpirsingly, they even spell almost identically. There are some shirts being pullovers and coats in eyes of our Random Forest as well.\n\n","5784ce91":"Stacking classifier","11f0f6ba":"Another interesting idea that I have seen in one notebook to use PCA to simplify dataset in case if you still want to use ensemble learning.\n\nhttps:\/\/thiagolcmelo.github.io\/2020-08-19-solving-fashion-mnist-using-ensemble-learning\/","13532479":"Trying out RandomForestClassifier ","90638c7b":"Loading training data","7eca1ed7":"Here, I intend to check if there are images to consider as errors. For example, an image of anything else but fashion or something indistinguible.\n\nMethod: Taking instances that are farthest away from theier nearest neighbours.","c1f03b39":"I have gone through optimal parameters search, but you will not see it.","ed591b5d":"Plotting images of each class from training set","07ddffcc":"#### Analysing Outliers"}}