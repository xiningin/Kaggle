{"cell_type":{"59b9fab1":"code","551fcdb9":"code","2967af73":"code","833dc003":"code","6b9f8718":"code","d73e7cac":"code","43da89cf":"code","06f89d12":"code","b4a09887":"code","51c524fe":"code","72f671fd":"code","787ee13a":"code","df0e1ec9":"code","0bc275b7":"code","27870ab3":"code","a7b2bbe7":"code","c0f2a477":"code","a78b92c8":"code","cd91d5eb":"markdown","13e516e3":"markdown","3f50f995":"markdown","c5914867":"markdown","d63a3132":"markdown","4f8e183c":"markdown","27e42374":"markdown","325aa0cb":"markdown","63746461":"markdown","1d50255f":"markdown","cd77218e":"markdown","0af785a9":"markdown","f9d2ab09":"markdown","e164e651":"markdown","23554900":"markdown"},"source":{"59b9fab1":"# Import numpy, pandas, and matplotlib using the standard aliases.\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\n# Import the following tools from sklearn: \n#     Pipeline, SimpleImputer, ColumnTransformer, OneHotEncoder, StandardScaler\n#     LogisticRegression, DecisionTreeClassifier, RandomForestClassifier, GridSearchCV\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.preprocessing import StandardScaler\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import GridSearchCV\n\n# Import joblib\nimport joblib","551fcdb9":"# Load the training data into a DataFrame named 'train'. \ntrain = pd.read_csv(\"\/kaggle\/input\/titanic\/train.csv\")\ntrain = train.sample(frac=1, random_state=1)\n# Print the shape of the resulting DataFrame.\nprint(train.shape)\n# You do not need the test data in this notebook. ","2967af73":"# Display the head of the train DataFrame. \ntrain.head()","833dc003":"# Calculate and print the number of number of missing values in each column.\ntrain.isnull().sum().sort_values(ascending=False)","6b9f8718":"(train.Survived.value_counts()\/len(train)).to_frame()","d73e7cac":"y_train = train.Survived.values\ntrain.drop(columns=['PassengerId','Survived'], inplace=True)","43da89cf":"train['FamSize'] = train['SibSp'] + train['Parch']\n\n# We will use the function below to determine the deck letter for each passenger:\ndef set_deck(cabin):\n    if str(cabin) == 'nan':\n        return 'Missing'\n    return cabin[0]\n\n# Use the map() method of the train DataFrame to apply the function above \n# to the 'Cabin' column. Store the results in a new column named 'Deck'. \ntrain['Deck'] = train['Cabin'].map(set_deck)\ntrain.head()","06f89d12":"# Create a list of numberical feature names. Use the following features: 'Age', 'FamSize', 'Fare'\nnum_features = ['Age','FamSize','Fare']\n\n# Create a list of categorical feature names. Use the following features: 'Sex', 'Pclass', 'Deck', 'Embarked'\ncat_features = ['Sex','Pclass','Deck','Embarked']\n\n# Combine the two previous lists into one list named 'features'\nfeatures = num_features + cat_features\n\n# Create a Pipeline object for processing the numerical features. \n# This pipeline should consist of a SimpleImputer and a StandardScaler\nnum_transformer = Pipeline(steps=[\n    ('imputer', SimpleImputer(strategy='mean')),\n    ('scaler', StandardScaler())])\n\n# Create a Pipeline object for processing the categorical features. \n# This pipeline should consist of a SimpleImputer and a OneHotEncoder\ncat_transformer = Pipeline(steps=[\n    ('imputer', SimpleImputer(strategy='constant', fill_value='Missing')),\n    ('onehot', OneHotEncoder(handle_unknown='ignore'))])\n\n# Create a ColumnTransformer object that combines the two pipelines created above. \n# Name this ColumnTransformer 'preprocessor'\npreprocessor = ColumnTransformer(\n    transformers=[\n        ('num', num_transformer, num_features),\n        ('cat', cat_transformer, cat_features)])","b4a09887":"# Fit the preprocessor to the training data, selecting only the columns in the 'features' list. \n# Store the array created in the previous step into a variable named 'X_train'.\npreprocessor.fit(train[features])\nX_train = preprocessor.transform(train[features])\n\n# Create a variable named 'y_train' that contains the training labels.\n\n# Print the shapes of X_train and y_train.\nprint('X_train shape:', X_train.shape)\nprint('y_train shape:', y_train.shape)","51c524fe":"# Select a range of parameter values of C. \n# You might need to experiment with this to find a good value for C\n# Update the code below to perform 10-Fold cross-validation. \n# Set the scoring parameter below to 'accuracy'\n\nlr_clf = LogisticRegression(max_iter=1000, solver='saga', penalty='elasticnet')\n\nlr_parameters = {\n    'l1_ratio':[0, 1],\n    'C': [0.001, 0.01, 0.1, 1, 10]\n}\n\nlr_grid = GridSearchCV(lr_clf, lr_parameters, cv=10, refit='True', n_jobs=-1, verbose=10, scoring='accuracy')\nlr_grid.fit(X_train, y_train)\n\nlr_model = lr_grid.best_estimator_\n\nprint('Best Parameters:', lr_grid.best_params_)\nprint('Best CV Score:  ', lr_grid.best_score_)\nprint('Training Acc:   ', lr_model.score(X_train, y_train))","72f671fd":"# Run this cell without any changes to view the CV results.\n\nlr_summary = pd.DataFrame(lr_grid.cv_results_['params'])\nlr_summary['cv_score'] = lr_grid.cv_results_['mean_test_score']\n\nfor r in lr_parameters['l1_ratio']:\n    temp = lr_summary.query(f'l1_ratio == {r}')\n    plt.plot(temp.C, temp.cv_score, label=r)\nplt.xscale('log')\nplt.ylim([0.75, 0.82])\nplt.xlabel('Regularization Parameter (C)')\nplt.ylabel('CV Score')\nplt.legend(title='L1 Ratio', loc='lower right')\nplt.grid()\nplt.show()\n\nprint(lr_summary.to_string(index=False))","787ee13a":"# Select values to consider for the max_depth and min_samples_leaf hyperparameters.\n# You might need to experiment to find a good range of parameter values. \n# Update the code below to perform 10-Fold cross-validation. \n# Set the scoring parameter below to 'accuracy'\n\ndt_clf = DecisionTreeClassifier(random_state=1)\n\ndt_parameters = {\n    'max_depth': [2, 4, 6, 7, 10, 12, 14, 16],\n    'min_samples_leaf': [2, 4, 8, 16]\n}\n\ndt_grid = GridSearchCV(dt_clf, dt_parameters, cv=10, refit='True', n_jobs=-1, verbose=0, scoring='accuracy')\ndt_grid.fit(X_train, y_train)\n\ndt_model = dt_grid.best_estimator_\n\nprint('Best Parameters:', dt_grid.best_params_)\nprint('Best CV Score:  ', dt_grid.best_score_)\nprint('Training Acc:   ', dt_model.score(X_train, y_train))","df0e1ec9":"# Run this cell without any changes to view the CV results.\n\ndt_summary = pd.DataFrame(dt_grid.cv_results_['params'])\ndt_summary['cv_score'] = dt_grid.cv_results_['mean_test_score']\n\nfor ms in dt_parameters['min_samples_leaf']:\n    temp = dt_summary.query(f'min_samples_leaf == {ms}')\n    plt.plot(temp.max_depth, temp.cv_score, label=ms)\nplt.xlabel('Maximum Depth')\nplt.ylabel('CV Score')\nplt.legend(title='Min Samples')\nplt.grid()\nplt.show()\n\nprint(dt_summary.to_string(index=False))","0bc275b7":"# Select a number of trees to use in your random forest.\n# Select values to consider for the max_depth and min_samples_leaf hyperparameters.\n# You might need to experiment to find a good range of parameter values. \n# Update the code below to perform 10-Fold cross-validation. \n# Set the scoring parameter below to 'accuracy'\n\nrf_clf = RandomForestClassifier(random_state=1, n_estimators=100)\n\nrf_parameters = {\n    'max_depth': [1, 2, 4, 6, 9, 10, 14, 18, 20, 24, 28, 32],\n    'min_samples_leaf': [1, 2, 6, 8]\n}\n\nrf_grid = GridSearchCV(rf_clf, rf_parameters, cv=10, refit='True', n_jobs=-1, verbose=0, scoring='accuracy')\nrf_grid.fit(X_train, y_train)\n\nrf_model = rf_grid.best_estimator_\n\nprint('Best Parameters:', rf_grid.best_params_)\nprint('Best CV Score:  ', rf_grid.best_score_)\nprint('Training Acc:   ', rf_model.score(X_train, y_train))","27870ab3":"# Run this cell without any changes to view the CV results.\n\nrf_summary = pd.DataFrame(rf_grid.cv_results_['params'])\nrf_summary['cv_score'] = rf_grid.cv_results_['mean_test_score']\n\nfor ms in rf_parameters['min_samples_leaf']:\n    temp = rf_summary.query(f'min_samples_leaf == {ms}')\n    plt.plot(temp.max_depth, temp.cv_score, label=ms)\nplt.xlabel('Maximum Depth')\nplt.ylabel('CV Score')\nplt.legend(title='Min Samples')\nplt.grid()\nplt.show()\n\nprint(rf_summary.to_string(index=False))","a7b2bbe7":"print(rf_grid.best_params_)","c0f2a477":"finalmodel = RandomForestClassifier(random_state=1, n_estimators=100, max_depth=9, min_samples_leaf=1)\nfinalmodel.fit(X_train, y_train)\n\nprint('Final Model Score:', finalmodel.score(X_train, y_train))","a78b92c8":"# Save your pipeline to a file. \n# Determine the best model found above and save that to a file. \n# Download both files to your local device and then upload them as a Kaggle dataset.\njoblib.dump(preprocessor, 'titanic_preprocessor_01.joblib')\njoblib.dump(finalmodel, 'titanic_model_01.joblib')\nprint('Model written to file.')","cd91d5eb":"# Titanic Dataset","13e516e3":"Display a dataframe showing the proportion of observations with each possible of the target variable (Survived).","3f50f995":"# Create y_train","c5914867":"# Check Label Distribution","d63a3132":"## Random Forests","4f8e183c":"# Load Training Data","27e42374":"# Import Statements","325aa0cb":"We will start with some feature engineering.\n \nAdd a new column named 'FamSize' to the DataFrame. This should be the sum of the 'SibSp' and 'Parch' columns. \n ","63746461":"## Logistic Regression","1d50255f":"# Model Selection","cd77218e":"# Preprocessing","0af785a9":"# Save Pipeline and Model","f9d2ab09":"# Check for Missing Values","e164e651":"# Train Final Model","23554900":"## Decicion Trees"}}