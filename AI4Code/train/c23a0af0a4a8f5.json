{"cell_type":{"043ad346":"code","0874cea4":"code","bf908ed4":"code","dc8897a2":"code","c0844b1f":"code","62b12b10":"code","175449f3":"code","45098fd5":"code","f4342cb2":"code","5ec2ef8c":"code","730e412f":"code","9fe00244":"code","fc241003":"code","1847d255":"code","dcebeed3":"code","f052223d":"code","1d9b571a":"code","513ac7af":"code","02c8ebc7":"code","7596caca":"code","b316cf7e":"code","37e0bc60":"code","2ee9c5a5":"code","3f170da3":"code","05c866c6":"code","1a2bf7f9":"code","e6bb6eea":"code","19ec4183":"code","d7c9ee47":"code","6fc2bb00":"code","63473ad2":"code","35fc660f":"code","5f5fc63e":"code","2c94545d":"code","061fa531":"code","8a585c9b":"code","e6683dea":"code","afb24d0c":"code","b57ee08c":"code","33b77895":"code","d1adb695":"code","e24b6ee9":"code","5e851000":"code","47abe46c":"code","aa46c8fd":"markdown","6f420331":"markdown","2cea78bc":"markdown","c13ea223":"markdown","8bb7c8dd":"markdown","e6bc99be":"markdown","cf1c8ed8":"markdown","5f673755":"markdown","522c7f8f":"markdown","69dc06ee":"markdown","ccfba0ca":"markdown","6b86cbf1":"markdown","d06128ff":"markdown","004f56e7":"markdown","35cc241b":"markdown","fc508498":"markdown","5331e74a":"markdown","1d82434d":"markdown","e3ee9339":"markdown","1b40959b":"markdown","6f2ef623":"markdown","5ac08664":"markdown","aede3c92":"markdown","d314dec1":"markdown","e2a7657c":"markdown","b17002cd":"markdown","34067c23":"markdown"},"source":{"043ad346":"import os\nimport sys\nimport gc\nimport glob\nimport time\nfrom os import listdir\nimport tqdm\nfrom typing import Dict\n\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import roc_auc_score\n\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nplt.style.use(\"fivethirtyeight\")\n\nimport plotly.express as px\nimport plotly.graph_objs as go\nfrom plotly.offline import iplot\nimport plotly.figure_factory as ff\n\n#supress warnings\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nfrom colorama import Fore, Back, Style\ny_ = Fore.YELLOW\nr_ = Fore.RED\ng_ = Fore.GREEN\nb_ = Fore.BLUE\nm_ = Fore.MAGENTA\nc_ = Fore.CYAN\nsr_ = Style.RESET_ALL","0874cea4":"folder_path = '..\/input\/riiid-test-answer-prediction\/'\ntrain_csv = folder_path + 'train.csv'\ntest_csv =  folder_path + 'example_test.csv'\nlec_csv  =  folder_path + 'lectures.csv'\nque_csv =   folder_path + 'questions.csv'\nsample_csv =    folder_path + 'example_sample_submission.csv'\n\ndtype = {'row_id':'int64',\n         'timestemp':'int64',\n        'user_id':'int32',\n        'content_id':'int16',\n        'content_type_id':'int8',\n        'task_container_id':'int16',\n        'user_answer':'int8',\n        'answered_correctly':'int8',\n        'prior_question_elapsed_time':'float32',\n        'prior_question_had_explanation':'boolean'}\n\ntrain_data = pd.read_csv(train_csv,dtype=dtype,nrows=10**6)\ntest_data = pd.read_csv(test_csv)\nlec_data = pd.read_csv(lec_csv)\nque_data = pd.read_csv(que_csv)\nsample = pd.read_csv(sample_csv)","bf908ed4":"print(f\"{y_}Number of rows in train data: {r_}{train_data.shape[0]}\\n{y_}Number of columns in train data: {r_}{train_data.shape[1]}\")\nprint(f\"{g_}Number of rows in test data: {r_}{test_data.shape[0]}\\n{g_}Number of columns in test data: {r_}{test_data.shape[1]}\")\nprint(f\"{c_}Number of rows in lecture data: {r_}{lec_data.shape[0]}\\n{c_}Number of columns in lecture data: {r_}{lec_data.shape[1]}\")\nprint(f\"{m_}Number of rows in question data: {r_}{que_data.shape[0]}\\n{m_}Number of columns in question data: {r_}{que_data.shape[1]}\")\nprint(f\"{b_}Number of rows in submission data: {r_}{sample.shape[0]}\\n{b_}Number of columns in submission data:{r_}{sample.shape[1]}\")","dc8897a2":"train_data.head().style.applymap(lambda x:\"background-color:lightgreen\")","c0844b1f":"train_data.info()","62b12b10":"#looking for null values\ntrain_data.isna().sum()","175449f3":"lec_data.head()","45098fd5":"que_data.head()","f4342cb2":"def countplot(column):\n    plt.figure(dpi=100)\n    sns.countplot(train_data[column])\n    plt.show()","5ec2ef8c":"countplot('user_answer')","730e412f":"countplot('answered_correctly')","9fe00244":"countplot('content_type_id')","fc241003":"countplot(\"prior_question_had_explanation\")","1847d255":"plt.figure(dpi=100)\nsns.distplot(train_data[~train_data[\"prior_question_elapsed_time\"].isna()][\"prior_question_elapsed_time\"],color='yellow')\nplt.show()","dcebeed3":"def distribution1(column,color,n=40):\n    df = train_data[column].value_counts().reset_index()\n    df.columns = [column,'count']\n    df[column] = df[column].astype(str) + '-'\n    df = df.sort_values(['count'],ascending=False)\n\n    plt.figure(figsize=(15,10))\n    plt.subplot(121)\n    sns.distplot(df['count'],color=color)\n\n    plt.subplot(122)\n    sns.barplot(x='count',y=column,data=df[:n],orient='h')\n    plt.show()","f052223d":"distribution1(\"user_id\",\"purple\") ","1d9b571a":"distribution1(\"content_id\",\"red\")","513ac7af":"distribution1(\"task_container_id\",\"green\",n=50)","02c8ebc7":"answered_correctly = train_data.groupby(['user_id'])['answered_correctly'].agg(['sum','count']).reset_index()\nanswered_correctly = answered_correctly[answered_correctly['count']>=10]\nanswered_correctly['user_id'] = answered_correctly['user_id'].astype(str) + \"_\"\nanswered_correctly['percentage'] = (answered_correctly['sum'] \/ answered_correctly['count']) * 100\nanswered_correctly = answered_correctly.sort_values(['percentage'],ascending=False)\n\nplt.figure(figsize=(7,10))\nsns.barplot(x='percentage',y='user_id',data=answered_correctly[:50],orient='h');","7596caca":"df_correct_user_answers = train_data[train_data['answered_correctly']==1]['user_answer']\ndf_incorrect_user_answers = train_data[train_data['answered_correctly']==0]['user_answer']\n\nplt.figure(figsize=(15,7))\nplt.subplot(121)\nsns.countplot(df_correct_user_answers)\nplt.title(\"Correctly answered user answers\")\nplt.subplot(122)\nsns.countplot(df_incorrect_user_answers)\nplt.title(\"Incorrectl answered user answers\");","b316cf7e":"sorted_user_id_timestamp = train_data.sort_values(['user_id','timestamp'])\ntrain_data[\"time_required_to_answer\"] = sorted_user_id_timestamp.groupby('user_id')['prior_question_elapsed_time'].shift(periods=-1)\nresponce_time_correct = train_data[train_data['answered_correctly']==1].groupby('user_answer')['time_required_to_answer'].mean()\nresponce_time_incorrect = train_data[train_data['answered_correctly']==0].groupby('user_answer')['time_required_to_answer'].mean()\n\nplt.figure(figsize=(15,7))\nplt.subplot(121)\nsns.barplot(responce_time_incorrect.index,responce_time_correct.values)\nplt.title(\"Responce time for correctly answered answers\")\nplt.subplot(122)\nsns.barplot(responce_time_correct.index,responce_time_correct.values)\nplt.title(\"Responce time for incorrectly answered answers\");","37e0bc60":"train_data[\"timespend\"]=train_data.groupby('user_id')[\"timestamp\"].transform(lambda x: x.max() - x.min())","2ee9c5a5":"plt.figure(dpi=100)\nplt.hist(train_data.timespend,color='red')\nplt.xlabel(\"timespend\");","3f170da3":"train_data = train_data.sort_values(\"timestamp\").reset_index(drop=True)\ntrain_data['interaction_count'] = 1\ntrain_data['interaction_count'] = train_data.groupby(\"user_id\")['interaction_count'].transform('cumsum')\ntrain_data['correct_answers_till_now'] = train_data.groupby('user_id')['answered_correctly'].transform('cumsum')\ntrain_data['accuracy_per_timestamp'] = train_data['correct_answers_till_now']*100 \/ train_data['interaction_count']\n\nf = plt.figure(figsize=(7,7))\nsns.set_style(style=\"whitegrid\")\nsns.despine(f, left=True, bottom=True)\nsns.scatterplot(x='timestamp',y='accuracy_per_timestamp',data=train_data,hue='content_type_id');\nplt.xlabel(\"accuracy of user\")\nplt.ylabel(\"timestamp\");","05c866c6":"top_user = train_data[train_data.user_id == 7171715]\ntop_user = pd.merge(top_user,que_data,left_on='content_id',right_on='question_id',how='left')\ntop_user = pd.merge(top_user,lec_data,left_on='content_id',right_on='lecture_id',how='left')\ntop_user.head()","1a2bf7f9":"print(\"number of question and lecture attented by user: \",top_user.content_id.nunique())\nprint(\"number of questions attented by user; \",top_user.question_id.nunique())\nprint(\"number of lectures attented by user: \",top_user[top_user.content_type_id==1].content_id.nunique())\nprint(\"number of bundles attented by user: \",top_user.bundle_id.nunique())","e6bb6eea":"sns.set_style(style=\"darkgrid\")\nplt.figure(figsize=(15,7))\nplt.subplot(121)\nsns.lineplot(x='timestamp',y='accuracy_per_timestamp',data=top_user[:100],color='green')\nplt.subplot(122)\nsns.lineplot(x='timestamp',y='accuracy_per_timestamp',data=top_user[100:],color='green');","19ec4183":"dtype = {\n    'row_id': 'int64',\n    'timestamp': 'int64',\n    'user_id': 'int32',\n    'content_id': 'int16',\n    'answered_correctly': 'int8',\n    'prior_question_elapsed_time': 'float16',\n    'prior_question_had_explanation': 'boolean'\n}\n\ntrain_data = pd.read_csv(\n    '\/kaggle\/input\/riiid-test-answer-prediction\/train.csv',\n    usecols = dtype.keys(),\n    dtype=dtype, \n    index_col = 0,\n    nrows = 10**7\n)","d7c9ee47":"# feature_data = train_data.iloc[:int(0.9 * len(train_data))]\n# train_data = train_data.iloc[int(0.9 * len(train_data)):]\n\ntrain_data = train_data.sort_values(\"timestamp\").reset_index(drop=True)\ntrain_data['time_required_to_answer'] = train_data.groupby('user_id')['prior_question_elapsed_time'].shift(-1)\ntrain_data['question_has_explanation'] = train_data.groupby('user_id')['prior_question_had_explanation'].shift(-1)\n\ntag = que_data[\"tags\"].str.split(\" \", n = 10, expand = True) \ntag.columns = ['tags1','tags2','tags3','tags4','tags5','tags6']\n\nque_data =  pd.concat([que_data,tag],axis=1).drop(['tags'],axis=1)\nque_data['tags1'] = pd.to_numeric(que_data['tags1'], errors='coerce',downcast='integer').fillna(-1)\nque_data['tags2'] = pd.to_numeric(que_data['tags2'], errors='coerce',downcast='integer').fillna(-1)\nque_data['tags3'] = pd.to_numeric(que_data['tags3'], errors='coerce',downcast='integer').fillna(-1)\n\ntrain_data = pd.merge(train_data,que_data,left_on='content_id',right_on='question_id',how='left')\ntrain_data['timespend'] = train_data.groupby(\"user_id\")[\"timestamp\"].transform(lambda x: (x.max() - x.min())\/1000)\ntrain_answered_question = train_data[train_data['answered_correctly']!=-1]\n\ngrouped_by_user_id = train_answered_question.groupby(\"user_id\")\ndf1 = grouped_by_user_id.agg({'answered_correctly':['mean','count','std','median']}).copy()\ndf1.columns =  ['mean_user_accuracy', 'questions_answered', 'std_user_accuracy', 'median_user_accuracy']\n\ndel grouped_by_user_id\ngc.collect()","6fc2bb00":"grouped_by_content_id = train_answered_question.groupby(\"content_id\")\ndf2 = grouped_by_content_id.agg({'answered_correctly':['mean','count','std','median']}).copy()\ndf2.columns =  ['mean_accuracy', 'questions_asked', 'std_accuracy', 'median_accuracy']\n\n# df3 = grouped_by_content_id.agg({'timespend':['mean','std','median']}).copy()\n# df3.columns =  ['mean_time', 'std_time', 'median_time']\n\ndel grouped_by_content_id\ndel train_answered_question\n# del feature_data\ngc.collect()","63473ad2":"features = [\n    #numerical columns\n    'mean_user_accuracy', \n    'questions_answered',\n    'std_user_accuracy', \n    'median_user_accuracy',\n    'mean_accuracy', \n    'questions_asked',\n    'std_accuracy', \n    'median_accuracy',\n    'prior_question_elapsed_time', \n    'time_required_to_answer',\n    #categorical columns\n    'prior_question_had_explanation',\n    'question_has_explanation',\n    'timespend',\n    'bundle_id',\n    'tags1',\n    'tags2',\n    'tags3',\n#     'mean_time',\n#     'std_time',\n#     'median_time',\n]\ntarget_column = 'answered_correctly'","35fc660f":"train_data = train_data[train_data[target_column] != -1]\ntrain_data = train_data.merge(df1, how='left', on='user_id')\ntrain_data = train_data.merge(df2, how='left', on='content_id')\n# train_data = train_data.merge(df3, how='left', on='content_id')\n\ntrain_data['prior_question_had_explanation'] = train_data['prior_question_had_explanation'].fillna(value = False).astype(bool)\ntrain_data['question_has_explanation'] = train_data['question_has_explanation'].fillna(value = False).astype(bool)\n\ntrain_data = train_data.fillna(value = -1)\n\ntarget = train_data[target_column].values\ntrain_data = train_data[features]\ntrain_data = train_data.replace([np.inf, -np.inf], np.nan)\ntrain_data = train_data.fillna(-1)","5f5fc63e":"train_data.head()","2c94545d":"scaler = StandardScaler()\ntrain_data = scaler.fit_transform(train_data)","061fa531":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom torch.utils.data import TensorDataset, DataLoader\nfrom sklearn.model_selection import KFold, StratifiedKFold\nfrom sklearn.metrics import roc_auc_score","8a585c9b":"class Model(nn.Module):\n    def __init__(self,input_size,output_size):\n        super(Model,self).__init__()\n        self.batch_norm1 = nn.BatchNorm1d(input_size)\n        self.dropout1 = nn.Dropout(0.3)\n        self.linear1 = nn.utils.weight_norm(nn.Linear(input_size,128))\n        \n        self.batch_norm2 = nn.BatchNorm1d(128)\n        self.dropout2 = nn.Dropout(0.2)\n        self.linear2 = nn.utils.weight_norm(nn.Linear(128,32))\n        \n        self.batch_norm3 = nn.BatchNorm1d(32)\n        self.dropout3 = nn.Dropout(0.2)\n        self.linear3 = nn.utils.weight_norm(nn.Linear(32,output_size))\n        \n    def forward(self,xb):\n        x = self.batch_norm1(xb)\n        x = self.dropout1(x)\n        x = F.leaky_relu(self.linear1(x))\n        \n        x = self.batch_norm2(x)\n        x = self.dropout2(x)\n        x = F.leaky_relu(self.linear2(x))\n        \n        x = self.batch_norm3(x)\n        x = self.dropout3(x)\n        return self.linear3(x)\n\n\n# class Model(nn.Module):\n#     def __init__(self,input_dim,output_dim):\n#         super(Model,self).__init__()\n#         self.layer1 = nn.Linear(input_dim,100)\n#         self.layer2 = nn.Linear(100,100)\n#         self.layer3 = nn.Linear(100,output_dim)\n            \n#     def forward(self,xb):\n#         x1 =  F.leaky_relu(self.layer1(xb))\n#         x1 =  F.leaky_relu(self.layer2(x1))\n#         return self.layer3(x1)\n","e6683dea":"config = {\n    \"epochs\":15,\n    \"train_batch_size\":50_000,\n    \"valid_batch_size\":50_000,\n    \"test_batch_size\":50_000,\n    \"nfolds\":3,\n    \"learning_rate\":0.001,\n}","afb24d0c":"def run(plot_losses=True):\n  \n    def train_loop(train_loader,model,loss_fn,device,optimizer,lr_scheduler=None):\n        model.train()\n        total_loss = 0\n        for i, (inputs, targets) in enumerate(train_loader):\n            inputs = inputs.to(device)\n            targets = targets.to(device)\n            \n            optimizer.zero_grad()\n            outputs = model(inputs)\n\n            loss = loss_fn(outputs,targets)\n            loss.backward()\n                \n            total_loss += loss.item()\n\n            optimizer.step()\n            if lr_scheduler != None:\n                lr_scheduler.step(loss.item())\n                    \n        total_loss \/= len(train_loader)\n        return total_loss\n    \n    def valid_loop(valid_loader,model,loss_fn,device):\n        model.eval()\n        total_loss = 0\n        predictions = list()\n        \n        for i, (inputs, targets) in enumerate(valid_loader):\n            inputs = inputs.to(device)\n            targets = targets.to(device)\n            outputs = model(inputs)                 \n\n            loss = loss_fn(outputs,targets)\n            predictions.extend(outputs.sigmoid().detach().cpu().numpy())\n            \n            total_loss += loss.item()\n        total_loss \/= len(valid_loader)\n            \n        return total_loss,np.array(predictions)    \n    \n\n    kfold = StratifiedKFold(n_splits=config['nfolds'])\n    \n    #for storing losses of every fold\n    fold_train_losses = list()\n    fold_valid_losses = list()\n    \n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    print(f\"{device} is used\")\n    \n    def loss_fn(outputs,targets):\n        targets = targets.view(-1,1)\n        return nn.BCEWithLogitsLoss()(outputs,targets)\n    \n    #kfold\n    for k , (train_idx,valid_idx) in enumerate(kfold.split(train_data,target)):\n      \n        x_train,x_valid,y_train,y_valid = train_data[train_idx,:],train_data[valid_idx,:],target[train_idx],target[valid_idx]\n        \n        input_dim = x_train.shape[1]\n        output_dim = 1\n\n        model = Model(input_dim,output_dim)\n        model.to(device)\n        \n        train_tensor = torch.tensor(x_train,dtype=torch.float)\n        y_train_tensor = torch.tensor(y_train,dtype=torch.float)\n\n        train_ds = TensorDataset(train_tensor,y_train_tensor)\n        train_dl = DataLoader(train_ds,\n                             batch_size = config[\"train_batch_size\"],\n                             shuffle=True,\n                              num_workers = 4,\n                              pin_memory=True\n                             )\n\n        valid_tensor = torch.tensor(x_valid,dtype=torch.float)\n        y_valid_tensor = torch.tensor(y_valid,dtype=torch.float)\n\n        valid_ds = TensorDataset(valid_tensor,y_valid_tensor)\n        valid_dl = DataLoader(valid_ds,\n                             batch_size =config[\"valid_batch_size\"],\n                             shuffle=False,\n                              num_workers = 4,\n                              pin_memory=True,\n                             )\n        \n        optimizer = optim.Adam(model.parameters(),lr=config['learning_rate'])\n        lr_scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=5, eps=1e-4, verbose=True)\n\n        print(f\"Fold {k}\")\n        best_loss = 999\n        \n        train_losses = list()\n        valid_losses = list()\n        start = time.time()\n        for i in range(config[\"epochs\"]):\n            train_loss = train_loop(train_dl,model,loss_fn,device,optimizer,lr_scheduler=lr_scheduler)\n            valid_loss,predictions = valid_loop(valid_dl,model,loss_fn,device)\n            \n            train_losses.append(train_loss)\n            valid_losses.append(valid_loss)\n            end = time.time()\n            epoch_time = end - start\n            start = end\n            \n            score = roc_auc_score(y_valid,predictions)\n                          \n            print(f\"epoch:{i} Training loss:{train_loss} | Validation loss:{valid_loss} | Score: {score:.4f} | epoch time {epoch_time:.2f} \")\n            \n            if valid_loss <= best_loss:\n                print(f\"{g_}Validation loss Decreased from {best_loss} to {valid_loss}{sr_}\")\n                best_loss = valid_loss\n                torch.save(model.state_dict(),f'model{k}.bin')\n                \n        fold_train_losses.append(train_losses)\n        fold_valid_losses.append(valid_losses)\n        \n        \n    if plot_losses == True:\n        plt.figure(figsize=(20,14))\n        for i, (t,v) in enumerate(zip(fold_train_losses,fold_valid_losses)):\n            plt.subplot(2,5,i+1)\n            plt.title(f\"Fold {i}\")\n            plt.plot(t,label=\"train_loss\")\n            plt.plot(v,label=\"valid_loss\")\n            plt.legend()\n        plt.show()   ","b57ee08c":"run()","33b77895":"def inference(test):\n    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n    all_prediction = np.zeros((test.shape[0],1))\n    \n    for i in range(config[\"nfolds\"]):\n        \n        input_dim = test.shape[1]\n        output_dim = 1\n        model = Model(input_dim,output_dim)\n        model.load_state_dict(torch.load(f\"model{i}.bin\"))\n        \n        predictions = list()\n        model.to(device)\n        test_tensor = torch.tensor(test,dtype=torch.float)\n        test_dl = DataLoader(test_tensor,\n                        batch_size=config[\"test_batch_size\"],\n                        shuffle=False)\n    \n        with torch.no_grad():\n            for i, inputs in enumerate(test_dl):\n                inputs = inputs.to(device, dtype=torch.float)\n                outputs= model(inputs) \n                predictions.extend(outputs.sigmoid().cpu().detach().numpy())\n\n        all_prediction += np.array(predictions)\/config['nfolds']\n        \n    return all_prediction","d1adb695":"import riiideducation\nenv = riiideducation.make_env()","e24b6ee9":"iter_test = env.iter_test()","5e851000":"for (test_data,sample_prediction_df) in iter_test:\n    test_data = pd.merge(test_data,que_data,left_on='content_id',right_on='question_id',how='left')\n    test_data['timespend'] = test_data.groupby(\"user_id\")['timestamp'].transform(lambda x: x.max() - x.min())\n    test_data['time_required_to_answer'] = test_data.groupby('user_id')['prior_question_elapsed_time'].shift(-1)\n    test_data['question_has_explanation'] = test_data.groupby('user_id')['prior_question_had_explanation'].shift(-1)\n    test_data = test_data.merge(df1,how='left',on='user_id')\n    test_data = test_data.merge(df2,how='left',on='content_id')\n#     test_data = test_data.merge(df3,how='left',on='content_id')\n\n    test_data['prior_question_had_explanation'] = test_data['prior_question_had_explanation'].fillna(value = False).astype(bool)\n    test_data['question_has_explanation'] = test_data['question_has_explanation'].fillna(value = False).astype(bool)\n\n    test_data.fillna(value = -1, inplace = True)\n    test_transform = scaler.transform(test_data[features])\n    test_data['answered_correctly'] = inference(test_transform)\n    env.predict(test_data.loc[test_data['content_type_id']==0,['row_id','answered_correctly']])","47abe46c":"sub = pd.read_csv(\".\/submission.csv\")\nsub.shape","aa46c8fd":"### 3.4 Distribution of elapsed time\u231a","6f420331":"### Let's examine user with top interaction to understand all columns of data\n\nuser with most interaction has user_id 7171715","2cea78bc":"How good a user performs will also depend on how much time does he\/she spends on the app or content\n\nHere timestamp starts with 0 but some users might have started later so let's see how much time user has spent on the app","c13ea223":"## 3 EDA \ud83d\udcca\n\n### 3.1 countplot of user answers","8bb7c8dd":"### 3.9 Mean Responce time for each answers","e6bc99be":"## Pytorch Baseline Model \ud83d\udd25","cf1c8ed8":"## Inference","5f673755":"### Getting data\ud83d\udcbd","522c7f8f":"### 3.8 Bar Graph of correctly,incorrectly answered user answers","69dc06ee":"## 1. What is the competition about?\ud83d\udca1\n\n* Riiid labs made an AI based tutor for south korean students.<br\/>\n  so they tracked the interaction of the student with the app.<br\/>\n  and here we have to predict how the student will perform in <br\/>\n  future interaction.\n  \n**Note: Let me know if any information or code is incorrect.<br\/>\n  and if you find the notebook usefull please UPVOTE**.","ccfba0ca":"### 3.8 Top users who answered correctly\n\nwe will look into users who answered highest percentage of their answer<br\/>\ngiven that they have altleast 10 interaction.","6b86cbf1":"### 3.10 Distribution of time spend by user on app.\u231a","d06128ff":"### 3.4 Count plot of prior question has explanation","004f56e7":"### 3.2 count plot of answered correctly","35cc241b":"### Now let us try to find change in accuracy with timestamp.","fc508498":"\n### Importing Libraries \ud83d\udcd8","5331e74a":"## 2.Metrics: area under ROC curve\ud83d\udccf.\n\n![metrics](https:\/\/www.medcalc.org\/manual\/_help\/images\/roc_intro3.png)\n\nIn order to understand ROC curve we need to understand True Positive Rate(sensitivity) and False Posivtive Rate.\n\n\n**True Positive Rate**:<br\/>\n* For a binary classification true positive rate is ratio of predicted samples which are predicted true.<br\/.\n  to all the samples which are actually true.\n \n* TPR (sensitivity) = T.P \/ T.P + F.N\n\n**False Positive Rate**:<br\/>\n* False Positive rate is ratio of samples which are faslely predicted as Positive to all negative samples.\n\n* FPR =  F.P \/ F.P + T.N \n\n\n#### What is ROC (Receiver Operator Characteristic) ?\n\n* ROC curve is the graph of TPR vs FPR over different threshold.<br\/>\n  It means ROC curve requires to have model predict probability for different classes<br\/>\n\n* AUC is the area under that ROC curve.\n\n* more information on roc auc on [this](https:\/\/www.dataschool.io\/roc-curves-and-auc-explained\/) video\n","1d82434d":"## Work in Progress","e3ee9339":"## Riiid: Answer Correctness Prediction\n![image](https:\/\/www.riiid.co\/assets\/opengraph.png)\n\nRiiid is a company whose goal is to imporve quality of education using AI.<br\/>\nRiiid wants to make persnolised education better for every student using AI.\n","1b40959b":"## 4. Data Preprocessing","6f2ef623":"* Looking at the data this are the possible columns we can use to make prediction.<br\/>\n  we have to make prediction for all question mean content_type_id == 0.<br\/>\n* From answer correctly we can get mean ,accuracy, median of the answers.<br\/>\n  prior_question_elapsed time may be help to understand how hard previous question was or which question takes more time.<br\/>.\n  \n* prior_question_had_explanation can be used to see if the user saw the explanatio of previous question.<br\/>\n  should we shift these two columns ? but it would be difficult beacuse prior questions are grouped by bundle_id.\n* bundle_id can be usefull to determine which question were of same budle means they might belong to same type.<br\/>\n  we will use part and tags later.\n  \n* Our most important columns for training are one which uses answer correctly.\n  ","5ac08664":"### 3.11 Timestamp vs Accuracy of the user","aede3c92":"### 3.3 count plot of content_type_id","d314dec1":"### 3.12 Change in accuracy with time of top user\n\nIn starting 100 question acc is high and then it decreases so inorder to see a good graph let's make two graphs<br\/>\nfor first 100 and then after 100","e2a7657c":"### 3.6 Distribution of content_id and top 40 content_id","b17002cd":"### 3.5 Distribution of interaction and top 40 of user interaction","34067c23":"### 3.7 Distribution of task_container_id and top 50 task_container_id"}}