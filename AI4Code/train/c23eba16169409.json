{"cell_type":{"1c42ef22":"code","b719e275":"code","0bfcb81f":"code","337e8207":"code","14be89c7":"code","50d82727":"code","4e343d1e":"code","e050cec6":"code","4eb49d2e":"code","24e1aaca":"code","a645c6eb":"code","93d24b55":"code","bbff5cf7":"code","eaaa3f2a":"code","d8dda801":"code","93b37984":"code","aba2b63c":"code","bbc79fbd":"code","ddff626c":"code","c15a425e":"code","6b44d149":"code","98b38aad":"code","6d3420fd":"code","58e73bdb":"code","a38785df":"code","3749fa0b":"code","a8bd8859":"code","9cea3831":"code","b1fd3a76":"code","8b660c4e":"code","488f3c72":"code","d1cbb4c5":"code","4a25a617":"markdown","cf9aceea":"markdown","a6f56a02":"markdown","28d36c41":"markdown","e78d4540":"markdown","d1a02e4a":"markdown","4e73728b":"markdown","4f0b2cf4":"markdown","f6a1d322":"markdown","a3350cae":"markdown","938edfa0":"markdown"},"source":{"1c42ef22":"import numpy as np \nimport pandas as pd\npd.set_option('max_columns', None)\npd.set_option('max_rows', None)\nimport seaborn as sns","b719e275":"train= pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/train.csv')\ntest= pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/test.csv')","0bfcb81f":"print('train Shape:'+ str(train.shape))\nprint('test Shape:'+ str(test.shape))","337e8207":"train.isnull().sum()","14be89c7":"#drop the colomns have more null values\ntrain0=train.drop(['Alley','PoolQC','Fence','MiscFeature','FireplaceQu'], axis='columns')\ntrain0.shape","50d82727":"test.isna().sum()","4e343d1e":"#drop the colomns have more null values\ntest0=test.drop(['Alley','PoolQC','Fence','MiscFeature','FireplaceQu'], axis='columns')\ntest0.shape","e050cec6":"sns.heatmap(train0.isnull(),yticklabels=False,cbar=False)","4eb49d2e":"sns.heatmap(test0.isnull(),yticklabels=False,cbar=False)","24e1aaca":"target = train0['SalePrice']\ntest_ids = test0['Id']\n\ntrain1 = train0.drop(['Id', 'SalePrice'], axis=1)\ntest1 = test0.drop('Id', axis=1)\n\ndata1 = pd.concat([train1, test1], axis=0).reset_index(drop=True)\ndata1.head()","a645c6eb":"data1.shape","93d24b55":"# Select categorical columns \ncategorical_cols = [cname for cname in data1.columns if data1[cname].dtype == \"object\"]\n\n# Select numerical columns\nnumerical_cols = [cname for cname in data1.columns if data1[cname].dtype in ['int64', 'float64']]\n\n# Keep selected columns only\nmy_cols = categorical_cols + numerical_cols\ndata2 = data1[my_cols].copy()","bbff5cf7":"from sklearn.impute import SimpleImputer\n\n# Imputation\ncat_imputer = SimpleImputer(strategy='most_frequent')\ndata3 = pd.DataFrame(cat_imputer.fit_transform(data2[categorical_cols]))\n\n# Imputation removed column names; put them back\ndata3.columns = data2[categorical_cols].columns\ndata3.head()","eaaa3f2a":"from sklearn.impute import SimpleImputer\n\n# Imputation\nnum_imputer = SimpleImputer(strategy='mean')\ndata4 = pd.DataFrame(num_imputer.fit_transform(data2[numerical_cols]))\n\n# Imputation removed column names; put them back\ndata4.columns = data2[numerical_cols].columns\ndata4.head()","d8dda801":"print(data3.shape)\nprint(data4.shape)","93b37984":"data5=pd.concat([data3,data4], axis='columns')\ndata5.shape","aba2b63c":"sns.heatmap(data5.isnull(),yticklabels=False,cbar=False)","bbc79fbd":"data5.shape","ddff626c":"data5.head()","c15a425e":"gd_columns = pd.get_dummies(data5[categorical_cols],drop_first=True)\n\n# Remove categorical columns (will replace with one-hot encoding)\ndrop_cat_columns = data5.drop(categorical_cols, axis=1)\n\n# Add one-hot encoded columns to numerical features\ndata6 = pd.concat([drop_cat_columns, gd_columns], axis=1)\n","6b44d149":"data6.head()","98b38aad":"data6.shape","6d3420fd":"from sklearn.preprocessing import StandardScaler\nscaler = StandardScaler()\ndata7 = pd.DataFrame(scaler.fit_transform(data6), index=data6.index, columns=data6.columns)","58e73bdb":"data7.head()","a38785df":"f_train = data7.loc[:train.index.max(), :].copy()\nf_test = data7.loc[train.index.max() + 1:, :].reset_index(drop=True).copy()","3749fa0b":"print(f_train.shape)\nprint(f_test.shape)","a8bd8859":"f_train.head()","9cea3831":"from sklearn.model_selection import train_test_split\nx_train, x_valid, y_train, y_valid= train_test_split(f_train, target, test_size=0.2, random_state=21)","b1fd3a76":"from sklearn.ensemble import GradientBoostingRegressor\nmodel = GradientBoostingRegressor(n_estimators=600)\nmodel.fit(x_train, y_train)","8b660c4e":"from sklearn.metrics import mean_absolute_error\n\npredictions = model.predict(x_valid)\nprint(\"Mean Absolute Error: \" + str(mean_absolute_error(predictions, y_valid)))","488f3c72":"predict_test= model.predict(f_test)","d1cbb4c5":"submission = pd.concat([test_ids, pd.Series(predict_test, name='SalePrice')], axis=1)\nsubmission.to_csv('.\/submission.csv', index=False, header=True)","4a25a617":"### Encode categorical columns","cf9aceea":"### submission","a6f56a02":"### Group numerical and categrical coumns","28d36c41":"# Model","e78d4540":"# Data Preprocessing\n### Handling Mising Values","d1a02e4a":"### Scaling","4e73728b":"### Prediction of test data","4f0b2cf4":"### filling numerical values by most frequent","f6a1d322":"### Combine Test and Train data to Hnadle missing values","a3350cae":"### filling catagorical values by most frequent","938edfa0":"### Split back the data into trian and test"}}