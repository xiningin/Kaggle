{"cell_type":{"0fe6e7af":"code","46311d1b":"code","638ea558":"code","01000088":"code","a5c956a3":"code","9d501f18":"code","a11d7994":"code","56f627c2":"code","ae195ad0":"code","b5e8f01e":"code","b283e902":"code","ae4746d8":"code","559db037":"code","4671f3ad":"code","499e36e7":"code","3f97d360":"code","1e46d34c":"code","dbbd3404":"code","cf2bf5e9":"code","7a000d9e":"code","dd388ef8":"code","5085b979":"code","7b5285cb":"code","1521f44c":"code","41e7a607":"code","d28d6d7b":"code","7d6bd067":"code","1a6444b9":"code","f06f6089":"code","3b9f0d5f":"code","e599e146":"code","79bb867e":"markdown","7a69e289":"markdown","43a40fea":"markdown"},"source":{"0fe6e7af":"import numpy as np\nimport librosa \n","46311d1b":"audio , sample_rate = librosa.load('..\/input\/urbansound8k\/fold10\/100648-1-2-0.wav',res_type='kaiser_fast')\n","638ea558":"audio.shape","01000088":"sample_rate","a5c956a3":"mfcc = librosa.feature.mfcc(y=audio,sr=sample_rate,n_mfcc=40)","9d501f18":"mfcc","a11d7994":"max_pad_len = 174\n\ndef extract_features(filename):\n    try:\n        audio,sample_rate = librosa.load(filename,res_type='kaiser_fast')\n        mfccs = librosa.feature.mfcc(y=audio,sr=sample_rate,n_mfcc=40)\n        pad_width = max_pad_len - mfccs.shape[1]\n        mfccs = np.pad(mfccs,pad_width=((0,0),(0,pad_width)),mode='constant')\n    except Exception as e:\n        print('Error with : ',filename)\n        return None\n    return mfccs","56f627c2":"from tqdm import tqdm\nimport pandas as pd","ae195ad0":"audio_dataset_path = '..\/input\/urbansound8k'\nmetadata = pd.read_csv('..\/input\/urbansound8k\/UrbanSound8K.csv')","b5e8f01e":"import numpy as np\nfrom tqdm import tqdm\nextracted_features = []\nfor index_num,row in tqdm(metadata.iterrows()):\n    file_name = os.path.join(os.path.abspath(audio_dataset_path),'fold'+str(row[\"fold\"])+'\/',str(row['slice_file_name']))\n    final_class_labels = row['class']\n    data= extract_features(file_name)\n    extracted_features.append([data,final_class_labels])","b283e902":"extracted_features_df = pd.DataFrame(extracted_features,columns=['feature','class'])\nextracted_features_df.head()","ae4746d8":"x = np.array(extracted_features_df['feature'].tolist())\ny = np.array(extracted_features_df['class'].tolist())","559db037":"x.shape","4671f3ad":"y.shape","499e36e7":"from sklearn.preprocessing import LabelEncoder\nfrom tensorflow.keras.utils import to_categorical","3f97d360":"le = LabelEncoder()\nyy = to_categorical(le.fit_transform(y))","1e46d34c":"# split the dataset \nfrom sklearn.model_selection import train_test_split \n\nx_train, x_test, y_train, y_test = train_test_split(x, yy, test_size=0.2, random_state = 42)","dbbd3404":"x_train.shape","cf2bf5e9":"\nnum_rows = 40\nnum_columns = 174\nnum_channels = 1\nx_train =  x_train.reshape(x_train.shape[0], num_rows, num_columns, num_channels)\nx_test = x_test.reshape(x_test.shape[0], num_rows, num_columns, num_channels)","7a000d9e":"x_train.shape","dd388ef8":"y_train","5085b979":"from tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Conv2D, MaxPooling2D,Dropout , GlobalAveragePooling2D,Dense\nfrom sklearn import metrics","7b5285cb":"num_labels = yy.shape[1]\nfilter_size = 2","1521f44c":"\n# Construct model \nmodel = Sequential()\nmodel.add(Conv2D(filters=16, kernel_size=2, input_shape=(num_rows, num_columns, num_channels), activation='relu'))\nmodel.add(MaxPooling2D(pool_size=2))\nmodel.add(Dropout(0.2))\n\nmodel.add(Conv2D(filters=32, kernel_size=2, activation='relu'))\nmodel.add(MaxPooling2D(pool_size=2))\nmodel.add(Dropout(0.2))\n\nmodel.add(Conv2D(filters=64, kernel_size=2, activation='relu'))\nmodel.add(MaxPooling2D(pool_size=2))\nmodel.add(Dropout(0.2))\n\nmodel.add(Conv2D(filters=128, kernel_size=2, activation='relu'))\nmodel.add(MaxPooling2D(pool_size=2))\nmodel.add(Dropout(0.2))\nmodel.add(GlobalAveragePooling2D())\n\nmodel.add(Dense(num_labels, activation='softmax'))","41e7a607":"# Compile the model\nmodel.compile(loss='categorical_crossentropy', metrics=['accuracy'], optimizer='adam')","d28d6d7b":"# Display model architecture summary \nmodel.summary()\n\n# Calculate pre-training accuracy \nscore = model.evaluate(x_test, y_test, verbose=1)\naccuracy = 100*score[1]\n\nprint(\"Pre-training accuracy: %.4f%%\" % accuracy)","7d6bd067":"from tensorflow.keras.callbacks import ModelCheckpoint \nfrom datetime import datetime \n\n#num_epochs = 12\n#num_batch_size = 128\n\nnum_epochs = 72\nnum_batch_size = 256\n\ncheckpointer = ModelCheckpoint(filepath='saved_models\/weights.best.basic_cnn.hdf5', \n                               verbose=1, save_best_only=True)\nstart = datetime.now()\n\nmodel.fit(x_train, y_train, batch_size=num_batch_size, epochs=num_epochs, validation_data=(x_test, y_test), callbacks=[checkpointer], verbose=1)\n\n\nduration = datetime.now() - start\nprint(\"Training completed in time: \", duration)","1a6444b9":"# Evaluating the model on the training and testing set\nscore = model.evaluate(x_train, y_train, verbose=0)\nprint(\"Training Accuracy: \", score[1])\n\nscore = model.evaluate(x_test, y_test, verbose=0)\nprint(\"Testing Accuracy: \", score[1])","f06f6089":"\ndef print_prediction(file_name):\n    prediction_feature = extract_features(file_name) \n    prediction_feature = prediction_feature.reshape(1, num_rows, num_columns, num_channels)\n\n    predicted_vector = model.predict_classes(prediction_feature)\n    predicted_class = le.inverse_transform(predicted_vector) \n    print(\"The predicted class is:\", predicted_class[0], '\\n') \n\n    predicted_proba_vector = model.predict_proba(prediction_feature) \n    predicted_proba = predicted_proba_vector[0]\n    for i in range(len(predicted_proba)): \n        category = le.inverse_transform(np.array([i]))\n        print(category[0], \"\\t\\t : \", format(predicted_proba[i], '.32f') )","3b9f0d5f":"print_prediction('..\/input\/gunshot\/GunShotSnglShotIn PE1097906.wav')","e599e146":"import IPython.display as ipd\nipd.Audio('..\/input\/gunshot\/GunShotSnglShotIn PE1097906.wav')","79bb867e":"USING CNN FOR SOUND CLASSIFICATION\n\n","7a69e289":"Feature Extraction refinement\nIn the prevous feature extraction stage, the MFCC vectors would vary in size for the different audio files (depending on the samples duration).\n\nHowever, CNNs require a fixed size for all inputs. To overcome this we will zero pad the output vectors to make them all the same size.","43a40fea":"# CNN MODEL"}}