{"cell_type":{"5a67f7d4":"code","7fdf37f4":"code","34e107a7":"code","bf364a1c":"code","baafa17f":"code","40c3f68f":"code","eca88ebb":"code","314be51f":"code","cd629eca":"code","c2c425f2":"code","e106a561":"code","6e709f64":"code","ca26cfea":"code","f25c6d46":"code","1fb4f476":"code","2b4b4d1b":"code","098ad96f":"code","d8f15ab7":"code","3741c9e4":"code","9e667778":"code","3c85d932":"code","7bf75d80":"code","772b47a2":"code","60f9677c":"code","b004354d":"code","5f54aa53":"code","426d200c":"code","57cf9834":"code","c411ea3c":"code","4af8be81":"code","828e73b8":"code","bdfeae67":"code","aec2146d":"code","76ce7bd8":"code","d3788d1d":"code","fb98d51e":"code","16b59e21":"code","f956c9d8":"code","026722a2":"code","f36fd13a":"code","0f51a2b3":"code","4d5822ff":"code","cdd7c1ad":"code","ecc458c9":"code","f962a3fb":"code","bebedd00":"code","cba1a822":"code","e8e4314c":"code","67344289":"code","3dcee589":"code","aec8e675":"code","a982fd67":"code","3e4c9e89":"code","a44ac71d":"code","f365e798":"markdown","7b246c21":"markdown","d1736478":"markdown","c5564df5":"markdown","8ea289c9":"markdown","fa7d99d4":"markdown","ff1d5f8c":"markdown","1aac6a00":"markdown","5448e594":"markdown","df7475f7":"markdown","ef87f00f":"markdown","0ab86e0e":"markdown","bcf1cd2e":"markdown","993e24f9":"markdown","abb24b1d":"markdown","e7ac6822":"markdown","7c8c9769":"markdown","415d52df":"markdown","e3634882":"markdown","3aa3721b":"markdown","cd5e8a5e":"markdown","eb79dc9b":"markdown","4558727d":"markdown","7a0f965e":"markdown","47912fe5":"markdown","446255a0":"markdown","01fd389d":"markdown","86ea956b":"markdown","eb691456":"markdown","b81a3997":"markdown","97a9db12":"markdown","4107f6a4":"markdown","a91db8a3":"markdown","33d0458a":"markdown","45cf8246":"markdown","00104614":"markdown","fe6f441d":"markdown","63d7eb7b":"markdown","f97177c9":"markdown","48492b62":"markdown"},"source":{"5a67f7d4":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.covariance import EllipticEnvelope\nfrom sklearn.ensemble import IsolationForest\nfrom sklearn.decomposition import PCA\nfrom sklearn.cross_decomposition import PLSRegression\nfrom sklearn.preprocessing import PowerTransformer, Normalizer\nfrom sklearn.feature_selection import mutual_info_regression\nfrom sklearn.inspection import permutation_importance\nfrom sklearn.linear_model import Ridge, Lasso, ElasticNet, LinearRegression\nfrom sklearn.model_selection import train_test_split, cross_val_score, LeaveOneOut\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.compose import TransformedTargetRegressor\nfrom sklearn.metrics import r2_score, mean_squared_error, make_scorer\nfrom sklearn.model_selection import RandomizedSearchCV\nfrom scipy.stats import skew, kurtosis\nfrom tqdm import tqdm\n\nplt.rc('figure',figsize=(18,11))","7fdf37f4":"df_original = pd.read_csv('\/kaggle\/input\/body-fat-prediction-dataset\/bodyfat.csv')\ndf_original","34e107a7":"np.sum(np.sum(df_original.isna()))","bf364a1c":"df_original.describe().loc[['min', 'max']]","baafa17f":"plt.hist(df_original['BodyFat'], bins=100)\nplt.ylabel('Count')\nplt.xlabel('BodyFat')\nplt.title('Distribution of BodyFat variable');","40c3f68f":"df_original.loc[df_original['BodyFat'] < 5, 'BodyFat']","eca88ebb":"df_1 = df_original.drop(df_original[df_original['BodyFat'] < 1].index)\ndf_1.describe().loc[['min', 'max']]","314be51f":"plt.hist(df_1['Height'], bins=100)\nplt.ylabel('Count')\nplt.xlabel('Height')\nplt.title('Distribution of Height variable');","cd629eca":"df_1.loc[df_1['Height'] < 35]","c2c425f2":"df_1 = df_1.drop(df_1.loc[df_1['Height'] < 35].index)","e106a561":"axes = pd.plotting.scatter_matrix(df_1, figsize=(35, 35), s=75, diagonal='kde')\nfor ax in axes.flatten():\n    ax.set_ylabel(ax.get_ylabel(), fontsize=25, rotation=45)\n    ax.set_xlabel(ax.get_xlabel(), fontsize=25)","6e709f64":"plt.scatter(df_1['BodyFat'], df_1['Density'])\nplt.xlabel('BodyFat')\nplt.ylabel('Density');","ca26cfea":"corr = df_1.corr()\nsns.heatmap(corr, annot=True, mask=np.triu(np.ones(shape=corr.shape), k=0));","f25c6d46":"outlier_detector_cov = EllipticEnvelope(contamination=0.01)\noutliers_mask = outlier_detector_cov.fit_predict(df_1[['BodyFat', 'Density']]) == -1\n\nplt.scatter(df_1['BodyFat'][outliers_mask], df_1['Density'][outliers_mask], color='red', label='Outliers')\nplt.scatter(df_1['BodyFat'][~outliers_mask], df_1['Density'][~outliers_mask], label='Not outliers')\nplt.xlabel('BodyFat')\nplt.ylabel('Density')\nplt.legend();","1fb4f476":"# Reset the indexes in df, so they match the numpy mask.\ndf_1.reset_index(inplace=True, drop=True)\ndf_2 = df_1.drop(index=np.where(outliers_mask == True)[0])\nplt.scatter(df_2['BodyFat'], df_2['Density'])\nplt.xlabel('BodyFat')\nplt.ylabel('Density');","2b4b4d1b":"# I have used the Density column for what I needed it, so now I will drop it.\ndf_2.drop(columns='Density', inplace=True)","098ad96f":"df_scaled_centered = PowerTransformer(method='box-cox').fit_transform(df_2.drop(columns='BodyFat'))\n\npca = PCA()\npca_components = pca.fit_transform(df_scaled_centered)\n\nplt.plot(np.arange(pca.n_components_) + 1, pca.explained_variance_ratio_)\nplt.xlabel('Components')\nplt.ylabel('Explained variance')\nplt.title('PCA');","d8f15ab7":"fig, axes = plt.subplots(ncols=2)\nfor i, ax in enumerate(axes.flatten()):\n    ax.scatter(pca_components[:, i], pca_components[:, i + 1])\n    ax.set_xlabel(f'Component {i + 1}')\n    ax.set_ylabel(f'Component {i + 2}')\nfig.suptitle('PCA');","3741c9e4":"fig, axes = plt.subplots(ncols=2)\nfor i, ax in enumerate(axes.flatten()):\n    sns.regplot(x=pca_components[:, i], y=df_2['BodyFat'], ax=ax)\n    ax.set_xlabel(f'Component {i + 1}')\n    ax.set_ylabel('BodyFat')\nfig.suptitle('PCA', y=0.95, fontsize=20);","9e667778":"pls = PLSRegression()\npls_components = pls.fit_transform(X=df_scaled_centered, y=df_2['BodyFat'])\n\nfig, axes = plt.subplots(ncols=2)\nfor i, ax in enumerate(axes.flatten()):\n    sns.regplot(x=pls_components[0][:, i], y=df_2['BodyFat'], ax=ax)\n    ax.set_xlabel(f'Component {i + 1}')\n    ax.set_ylabel('BodyFat')\nfig.suptitle('PLS', y=0.95, fontsize=20);","3c85d932":"def scatter_dependent_independent(X1, X2, y1, y2):\n    n_rows = X1.shape[1] + 1\n    fig, axes = plt.subplots(ncols=2, nrows=n_rows, figsize=(20, 55))\n    for X, y, ax_col in zip((X1, X2), (y1, y2), (0, 1)):\n        sns.kdeplot(x=y, ax=axes[0, ax_col])\n        if ax_col == 0:\n            data_name = 'Orignal data.'\n        else:\n            data_name = 'Transformed data.'\n        axes[0, ax_col].set_title(f'{data_name} Target: Skew = {skew(y):.4f}, Kurt = {kurtosis(y):.4f}')\n        for i, var in enumerate(X, start=1):\n            sns.regplot(x=X[var], y=y, ax=axes[i, ax_col])\n            axes[i, ax_col].set_xlabel(f'{var}')\n            axes[i, ax_col].set_ylabel('BodyFat')","7bf75d80":"df_x_transformed = pd.DataFrame(data=Normalizer().fit_transform(\n    PowerTransformer(method='box-cox').fit_transform(df_2.drop(columns='BodyFat'))), \n                                index=df_2.drop(columns='BodyFat').index, \n                                columns=df_2.drop(columns='BodyFat').columns)\ndf_y_transformed = PowerTransformer(method='box-cox').fit_transform(\n        df_2['BodyFat'].to_numpy().reshape(-1, 1)\n    ).reshape(1, -1)[0]\nscatter_dependent_independent(X1=df_2.drop(columns='BodyFat'), X2=df_x_transformed,\n                              y1=df_2['BodyFat'], y2=df_y_transformed)","772b47a2":"df_y = df_2['BodyFat']\ndf_features_original = df_2.drop(columns='BodyFat')\n# In creating the features I will ommit only Age.\ndf_features_created = pd.concat(\n    [\n        df_features_original.drop(columns=['Abdomen', 'Age']).div(df_features_original['Abdomen'], axis=0),\n        df_features_original[['Age', 'Abdomen']]\n    ], axis=1)","60f9677c":"df_features_original","b004354d":"df_features_created","5f54aa53":"mi_original = mutual_info_regression(df_features_original, df_y)\norder = mi_original.argsort()\nplt.bar(df_features_original.columns[order], mi_original[order])\nplt.title('Mutual information of original features with target variable');","426d200c":"mi_created = mutual_info_regression(df_features_created, df_y)\norder = mi_created.argsort()\nplt.bar(df_features_created.columns[order], mi_created[order])\nplt.title('Mutual information of created features with target variable');","57cf9834":"fig, axes = plt.subplots(nrows=2, figsize=(20, 30))\nfor ax, df, name in zip(axes.flatten(), (df_features_created, df_features_original),\n                        ('Created', 'Original')):\n    corr = pd.concat([df, df_y], axis=1).corr()\n    sns.heatmap(corr, annot=True, mask=np.triu(np.ones(shape=corr.shape), k=0), ax=ax)\n    ax.set_title(f'{name} features', fontsize=20)","c411ea3c":"axes = pd.plotting.scatter_matrix(pd.concat([df_features_created, df_y], axis=1),\n                                  figsize=(35, 35), s=75, diagonal='kde')\nfor ax in axes.flatten():\n    ax.set_ylabel(ax.get_ylabel(), fontsize=25, rotation=45)\n    ax.set_xlabel(ax.get_xlabel(), fontsize=25)\nplt.suptitle('Scatter plots matrix with the new features', fontsize=40, y=0.9);","4af8be81":"models = (LinearRegression(), PLSRegression(n_components=6), Ridge(), Lasso(alpha=0.1), \n          ElasticNet(alpha=0.1), RandomForestRegressor())\nmodels_names = ('PCR', 'PLSR', 'Ridge', 'Lasso', 'ElasticNet', 'RandomForest')\n    \ndef get_pipeline(*additional_pipe_steps, model):\n    pipe_steps = [PowerTransformer(method='box-cox'), Normalizer(norm='l2')]\n    pipe_steps.extend(additional_pipe_steps)\n    return make_pipeline(*pipe_steps,\n                         TransformedTargetRegressor(regressor=model, \n                                                    transformer=PowerTransformer(method='box-cox')))\n\n    \ndef get_model_score(pipe, data_x, data_y, cv=5):\n    cv_score = cross_val_score(pipe, data_x, data_y, cv=cv)\n    return np.mean(cv_score), np.std(cv_score)\n\n\ndef plot_feature_analysis(data_1, data_2, data_1_name, data_2_name, data_y):\n    models_scores = np.empty(shape=(len(models), 2))\n    for datax, data_id in zip((data_1, data_2), (0, 1)):\n        for i, (model, model_name) in enumerate(zip(models, models_names)):\n            add_step = []\n            if model_name == 'PCR':\n                add_step = [PCA(n_components=6)]\n            models_scores[i, data_id], _ = get_model_score(get_pipeline(*add_step,\n                                                                                              model=model),\n                                                                                 data_x=datax, data_y=data_y)\n    bar_width = 0.4\n    xs_loc = np.arange(len(models))\n    fig, ax = plt.subplots(figsize=(20, 10))\n    rects1 = ax.bar(xs_loc - bar_width\/2, np.around(models_scores[:, 0], decimals=2), bar_width,\n                    label=data_1_name)\n    rects2 = ax.bar(xs_loc + bar_width\/2, np.around(models_scores[:, 1], decimals=2), bar_width,\n                    label=data_2_name)\n    ax.set_xticks(xs_loc)\n    ax.set_xticklabels(models_names)\n    ax.legend()\n    ax.set_ylabel('R_2 score')\n    ax.bar_label(rects1, padding=3)\n    ax.bar_label(rects2, padding=3)\n    fig.tight_layout()\n\n    \ndef plot_feature_importance(*additional_pipe_steps, model, model_name, data_x, data_y, ax, cv):\n    pipe = get_pipeline(*additional_pipe_steps, model=model)\n    model_score, score_std = get_model_score(pipe=pipe, data_x=data_x,\n                                               data_y=data_y, cv=cv)\n    X_train, X_test, Y_train, Y_test = train_test_split(data_x, data_y, random_state=42)\n    pipe.fit(X_train, Y_train)\n    result = permutation_importance(pipe, X_test, Y_test, n_repeats=10, random_state=42)\n    ax = sns.barplot(x=data_x.columns, y=result.importances_mean, yerr=result.importances_std, ax=ax,\n                     capsize=.5)\n    ax.set_ylabel('Feature importance')\n    ax.set_title(f'{model_name}, with R_2 = {model_score:.2f}(+\/-{score_std:.2f})', fontsize=20)\n    \n    \ndef plot_importance_analysis(data_1, data_2, data_1_name, data_2_name, data_y):\n    fig, axes = plt.subplots(nrows=len(models), ncols=2, figsize=(20, 50))\n    for data, data_name, ax_col in zip((data_1, data_2),\n                                       (data_1_name, data_2_name), (axes[:, 0], axes[:, 1])):\n        for model, model_name, ax in zip(models, models_names, ax_col):\n            if model_name == 'PCR':\n                plot_feature_importance(PCA(n_components=6), model=LinearRegression(), \n                                        model_name=f'{model_name} on {data_name}', data_x=data,\n                                        data_y=data_y, ax=ax, cv=5)\n            else:\n                plot_feature_importance(model=model, model_name=f'{model_name} on {data_name}',\n                                        data_x=data, data_y=data_y, ax=ax, cv=5)\n\n    ","828e73b8":"plot_feature_analysis(data_1=df_features_original, data_2=df_features_created,\n                      data_1_name='original features', data_2_name='created features',\n                      data_y=df_y)","bdfeae67":"plot_importance_analysis(data_1=df_features_original, data_2=df_features_created,\n                         data_1_name='original features', data_2_name='created features',\n                         data_y=df_y)","aec2146d":"plot_feature_analysis(data_1=df_features_created, data_2=df_features_created.drop(columns='Abdomen'),\n                      data_1_name='all features', data_2_name='no Abdomen',\n                      data_y=df_y)","76ce7bd8":"plot_importance_analysis(data_1=df_features_created, data_2=df_features_created.drop(columns='Abdomen'),\n                         data_1_name='all features', data_2_name='no Abdomen',\n                         data_y=df_y)","d3788d1d":"abdomen_product = pd.concat(\n        [\n            df_features_original.drop(columns=['Abdomen', 'Age']).multiply(\n                df_features_original['Abdomen'], axis=0),\n            df_features_original[['Age']]\n        ], axis=1)\nplot_feature_analysis(data_1=df_features_created, data_2=abdomen_product,\n                      data_1_name='abdomen division', data_2_name='abdomen multiplication',\n                      data_y=df_y)","fb98d51e":"weight_division = pd.concat(\n        [\n            df_features_original.drop(columns=['Weight', 'Age']).div(\n                df_features_original['Weight'], axis=0),\n            df_features_original[['Age']]\n        ], axis=1)\nplot_feature_analysis(data_1=df_features_created, data_2=weight_division,\n                      data_1_name='abdomen division', data_2_name='weight division',\n                      data_y=df_y)","16b59e21":"bmi_like_division = pd.concat(\n        [\n            df_features_original.drop(columns=['Weight', 'Age', 'Height']).div(\n                df_features_original['Weight'].div(df_features_original['Height'], axis=0),\n                axis=0),\n            df_features_original[['Age']]\n        ], axis=1)\n\nplot_feature_analysis(data_1=df_features_created, data_2=bmi_like_division,\n                      data_1_name='abdomen division', data_2_name='bmi like division',\n                      data_y=df_y)","f956c9d8":"abdomen_hip_division = pd.concat(\n        [\n            df_features_original.drop(columns=['Abdomen', 'Hip', 'Age']).div(\n                (df_features_original['Abdomen'] + df_features_original['Hip'].values),\n                axis=0),\n            df_features_original[['Age']]\n        ], axis=1)\n\nplot_feature_analysis(data_1=df_features_created, data_2=abdomen_hip_division,\n                      data_1_name='abdomen division', data_2_name='abdomen hip division',\n                      data_y=df_y)","026722a2":"abdomen_hip_sum = pd.concat(\n        [\n            df_features_original.drop(columns=['Abdomen', 'Hip', 'Age']),\n            df_features_original['Age'],\n            df_features_original['Abdomen'] + df_features_original['Hip'].values\n        ], axis=1)\n\nplot_feature_analysis(data_1=df_features_created, data_2=abdomen_hip_sum,\n                      data_1_name='abdomen division', data_2_name='abdomen hip sum',\n                      data_y=df_y)","f36fd13a":"# Some prep.\ndata_engineered = df_features_created.drop(columns='Abdomen')\n\ndef evaluate_model(pipe, X, y):\n    y_pred, y_true = np.empty(len(y)), np.empty(len(y))\n    loo = LeaveOneOut()\n    for i, (train_idx, test_idx) in tqdm(enumerate(loo.split(X)), total=len(y)):\n        X_train, X_test = X.iloc[train_idx], X.iloc[test_idx]\n        y_train, y_test = y.iloc[train_idx], y.iloc[test_idx]\n        y_pred[i] = pipe.fit(X_train, y_train).predict(X_test)[0]\n        y_true[i] = y_test\n    return r2_score(y_true, y_pred), np.sqrt(mean_squared_error(y_true, y_pred))","0f51a2b3":"pipe = get_pipeline(PCA(n_components=6), model=LinearRegression())\nr2_all, rmse_all = evaluate_model(pipe, data_engineered, df_y)\nr2_reduced, rmse_reduced = evaluate_model(pipe, data_engineered.drop(columns=['Biceps', 'Knee', 'Chest']),\n                                          df_y)\nr2_no_age, rmse_no_age = evaluate_model(pipe, data_engineered.drop(columns=['Age']),\n                                          df_y)\nprint(f'All features: R^2 = {r2_all:.3f}, RMSE = {rmse_all:.3f}\\n'\n      f'Reduced by importance: R^2 = {r2_reduced:.3f}, RMSE = {rmse_reduced:.3f}\\n'\n      f'No Age: R^2 = {r2_no_age:.3f}, RMSE = {rmse_no_age:.3f}')","4d5822ff":"pipe = get_pipeline(model=PLSRegression())\nr2_all, rmse_all = evaluate_model(pipe, data_engineered, df_y)\nr2_reduced, rmse_reduced = evaluate_model(pipe, data_engineered.drop(columns=['Biceps', 'Forearm']),\n                                          df_y)\nr2_no_age, rmse_no_age = evaluate_model(pipe, data_engineered.drop(columns=['Age']),\n                                          df_y)\nprint(f'All features: R^2 = {r2_all:.3f}, RMSE = {rmse_all:.3f}\\n'\n      f'Reduced by importance: R^2 = {r2_reduced:.3f}, RMSE = {rmse_reduced:.3f}\\n'\n      f'No Age: R^2 = {r2_no_age:.3f}, RMSE = {rmse_no_age:.3f}')","cdd7c1ad":"pipe = get_pipeline(model=Ridge())\nr2_all, rmse_all = evaluate_model(pipe, data_engineered, df_y)\nr2_reduced, rmse_reduced = evaluate_model(pipe, data_engineered.drop(columns=['Biceps', 'Chest']),\n                                          df_y)\nr2_no_age, rmse_no_age = evaluate_model(pipe, data_engineered.drop(columns=['Age']),\n                                          df_y)\nprint(f'All features: R^2 = {r2_all:.3f}, RMSE = {rmse_all:.3f}\\n'\n      f'Reduced by importance: R^2 = {r2_reduced:.3f}, RMSE = {rmse_reduced:.3f}\\n'\n      f'No Age: R^2 = {r2_no_age:.3f}, RMSE = {rmse_no_age:.3f}')","ecc458c9":"def plot_r2_rmse(r2s, rmses, parameter_name, parameter_values, model_name):\n    fig, (ax1, ax2) = plt.subplots(nrows=1, ncols=2)\n    ax1.plot(parameter_values, r2s)\n    ax1.set_xlabel(parameter_name)\n    ax1.set_ylabel('R^2')\n    ax2.plot(parameter_values, rmses)\n    ax2.set_xlabel(parameter_name)\n    ax2.set_ylabel('RMSE')\n    fig.suptitle(model_name)\n    \n    # Print results.\n    r2_best_idx = r2s.argmax()\n    r2_best_value = r2s[r2_best_idx]\n    rmse_best_value = rmses[r2_best_idx]\n    print(f'Model acheives the optimal result for {parameter_name} = {parameter_values[r2_best_idx]} with'\n          f'\\n'\n          f'RMSE = {rmse_best_value}\\n'\n          f'R-Squared = {r2_best_value}')","f962a3fb":"max_components = 12\ncomponents_values = np.arange(1, max_components + 1)\nr2s, rmses = np.empty(max_components), np.empty(max_components)\nfor n_components in components_values:\n    r2s[n_components - 1], rmses[n_components - 1] = evaluate_model(get_pipeline(PCA(n_components=\n                                                                                     n_components),\n                                                                                 model=LinearRegression()),\n                                                                   X=data_engineered, y=df_y)\nplot_r2_rmse(r2s=r2s, rmses=rmses, parameter_values=components_values,\n             parameter_name='Number of components', model_name='PCR')\n","bebedd00":"max_components = 12\ncomponents_values = np.arange(1, max_components + 1)\nr2s, rmses = np.empty(max_components), np.empty(max_components)\nfor n_components in components_values:\n    r2s[n_components - 1], rmses[n_components - 1] = evaluate_model(\n        get_pipeline(model=PLSRegression(n_components=n_components)), X=data_engineered, y=df_y\n    )\nplot_r2_rmse(r2s=r2s, rmses=rmses, parameter_values=components_values,\n             parameter_name='Number of components', model_name='PLSR')\n","cba1a822":"max_alpha = 5\nalpha_values = np.linspace(2, max_alpha, 20)\nr2s, rmses = np.empty(len(alpha_values)), np.empty(len(alpha_values))\nfor i, alpha in enumerate(alpha_values):\n    r2s[i], rmses[i] = evaluate_model(\n        get_pipeline(model=Ridge(alpha=alpha)), X=data_engineered, y=df_y\n    )\nplot_r2_rmse(r2s=r2s, rmses=rmses, parameter_values=alpha_values,\n             parameter_name='Alpha', model_name='Ridge')\n","e8e4314c":"min_alpha = 0.002\nmax_alpha = 0.05\nalpha_values = np.linspace(min_alpha, max_alpha, 20)\nr2s, rmses = np.empty(len(alpha_values)), np.empty(len(alpha_values))\nfor i, alpha in enumerate(alpha_values):\n    r2s[i], rmses[i] = evaluate_model(\n        get_pipeline(model=Lasso(alpha=alpha)), X=data_engineered, y=df_y\n    )\nplot_r2_rmse(r2s=r2s, rmses=rmses, parameter_values=alpha_values,\n             parameter_name='Alpha', model_name='Lasso')","67344289":"min_alpha = 0.007\nmax_alpha = 0.015\nalpha_values = np.linspace(min_alpha, max_alpha, 20)\nr2s, rmses = np.empty(len(alpha_values)), np.empty(len(alpha_values))\nfor i, alpha in enumerate(alpha_values):\n    r2s[i], rmses[i] = evaluate_model(\n        get_pipeline(model=ElasticNet(alpha=alpha, l1_ratio=0.35)), X=data_engineered, y=df_y\n    )\nplot_r2_rmse(r2s=r2s, rmses=rmses, parameter_values=alpha_values,\n             parameter_name='Alpha', model_name='ElasticNet')","3dcee589":"alpha = 0.009947368421052632\nmin_ratio = 0.01\nmax_ratio = 0.99\nratio_values = np.linspace(min_ratio, max_ratio, 20)\nr2s, rmses = np.empty(len(ratio_values)), np.empty(len(ratio_values))\nfor i, ratio in enumerate(ratio_values):\n    r2s[i], rmses[i] = evaluate_model(\n        get_pipeline(model=ElasticNet(alpha=alpha, l1_ratio=ratio)), X=data_engineered, y=df_y\n    )\nplot_r2_rmse(r2s=r2s, rmses=rmses, parameter_values=ratio_values,\n             parameter_name='L1 Ratio', model_name='ElasticNet')","aec8e675":"n_estimators = np.arange(50, 200, 15)\nmin_samples_split = np.arange(3, 10)\nmin_samples_leaf = np.arange(2, 10)\ngrid = {'transformedtargetregressor__regressor__n_estimators': n_estimators,\n        'transformedtargetregressor__regressor__min_samples_split': min_samples_split,\n        'transformedtargetregressor__regressor__min_samples_leaf': min_samples_leaf}\n\npipe_steps = [PowerTransformer(method='box-cox'), Normalizer(norm='l2')]\npipe =  make_pipeline(*pipe_steps, TransformedTargetRegressor(regressor=RandomForestRegressor(),\n                                                              transformer=PowerTransformer(method='box-cox'))\n                     )\n                      \nforest_search = RandomizedSearchCV(estimator=pipe,\n                                   param_distributions=grid,\n                                   n_iter=100,\n                                   cv=10,\n                                   verbose=1)\n\nforest_search.fit(data_engineered, df_y)\nbest_params = forest_search.best_params_\nbest_forest = forest_search.best_estimator_\nbest_score = forest_search.best_score_\nprint(f'\\n\\nThe best R^2 score of {100 * best_score:.2f}% is achieved by forest with parameters:\\n'\n      f'{best_params}')","a982fd67":"def predict_loo(model, X, y):\n    y_pred, y_true = np.empty(len(y)), np.empty(len(y))\n    loo = LeaveOneOut()\n    for i, (train_idx, test_idx) in tqdm(enumerate(loo.split(X)), total=len(y)):\n        X_train, X_test = X.iloc[train_idx], X.iloc[test_idx]\n        y_train, y_test = y.iloc[train_idx], y.iloc[test_idx]\n        y_pred[i] = model.fit(X_train, y_train).predict(X_test)[0]\n        y_true[i] = y_test\n    return y_pred, y_true\n\n\nfinal_model = get_pipeline(model=Lasso(alpha=0.012105263157894737))\ny_pred, y_true = predict_loo(final_model, data_engineered, df_y)","3e4c9e89":"plt.scatter(y_pred, y_true)\nplt.plot([0, 50], [0, 50], ls='--')\nplt.xlabel('Predicted')\nplt.ylabel('True')\nplt.title('Comparison of the real vs predicted values of body fat level');","a44ac71d":"my_measurements = np.array([156.528, 68.11, 38.5, 102.5, 90, 55.2, 38.3, 26.3, 32.5, 29.4, 17.2, 23]).\\\n    reshape(1, -1)\nabdomen = 82\nmy_measurements[0, :-1] \/= abdomen\nprediction = final_model.predict(my_measurements)[0]\nprint(f'The prediction for my body fat percent is {prediction:.1f}%.')","f365e798":"# Feature engineering\n\nIt might be tempting to use BMI as a feature, since it is often used to categorize overweight and underweight people, however in reality it will not help in predicting someones body fat level. The issue with BMI is that it does not differentiate between lean and fat tissue. As a consequence of that virtually all of bodybuilders would be categorized as overweight using this metric, even those that in fact have a body fat level of 5%.\n\nIn an effort to predict the percentage of body fat just from basic body measurements informative features might be the ratios between the circumference in the abdominal area to the circumferences in other parts of the body. This dataset consists of observations of men and men store fat mostly in the abdominal area. That's why someone with a great abdominal circumference and a small biceps circumference is sure to have a greater level of body fat.\n\nAn increase in circumference of any body part might be caused by an increase of either fat or lean tissue. A comparison to the measurement of the abdominal area should help recognize which one is it.\n\nIn order to obtain such features I will divide all of the existing predictors, with the exception of age, by the measurements of the abdomen.","7b246c21":"#### Observations\n\nAll of the transformed features are now more correlated and have a higher level of mutual information with the outcome variable, except for Weight. This might be counter-intuitive, but I would argue that it can make sense. A change in weight might be associated with both gaining and losing body fat. Just like the BMI, weight is not the best indicator of body fat percentage on its own. It might be beneficial to engineer some kind of relationship between weight and the body measurements.\n\nThe reason for higher correlations is partly the higher correlation of predictors with Abdomen, but this does not have to be the whole story.\n\nLet's see how the the features compare when creating an actual model.","d1736478":"### ElasticNet - changing alpha","c5564df5":"Let's also take a look at the scatter plots matrix of the new features.","8ea289c9":"The features I have created produce better results than the original ones. The difference is even smaller in the models trained on engineered features, with the exclusion of Abdomen, but overall I would say that they yield better results, both in $R^2$ means and the standard deviations.\n\nWhile I'm here and have a function ready I would like to try creating some more features and see how they perform.","fa7d99d4":"Because of the multicollinearity in the data most of the variability is contained in just a few PCA components. ","ff1d5f8c":"The minimal value of body fat is 0.0, which is definitely not possible.\n\nThe minimal height is 29.5\u2032\u2032 or 76 centimeters, which considering that the minimal age in the dataset is 22 years, is at least an outlier.\n\nOther than that I have no grounds to say any other measurements are invalid based on their ranges. I will investigate body fat and height columns further.","1aac6a00":"<a id='feature_importance'\/>","5448e594":"# Predicting body fat -  Introduction\n\n![image.png](attachment:image.png)\n\nThe level of body fat of an individual can be desirable to know for both medical and athletic purposes. Taking a measurement of body fat level is however quite a challenging task for multiple reasons. People have different body structures and they deposit fat differently in their body parts. Those factors can have such a strong effect, that two people with the same percentage of body fat might give the impression like one of them has a significantly lower one.\n\nThere are many methods of measuring body fat level, but almost all of them come with a significant margin of error and many are both cost and labor intensive and require specialized personnel. An analysis of those methods is provided by Karolina Dopiera\u0142a in her [PhD Thesis](https:\/\/www.awf.poznan.pl\/files\/wydzial_wfsir\/doktoraty\/komisje\/Konspekt_pracy_doktorskiej_K.Dopierala.pdf) (pages 9-10; it's in polish though).\n\nFor most people who wish to know their body fat level the options come down to either a visual comparison with an image or simple body measurements either with a skinfold caliper (which is visible on the image above) or of body parts circumferences. Both of those methods come with much uncertainty, which is amplified by the reasons mentioned before.\nA predictive model capable of determining ones body fat percentage would be of great use.\n\n","df7475f7":"### PCR","ef87f00f":"# EDA and Outlier Detection","0ab86e0e":"#### Observations\n\nThere does not seem to be much of a problem with heteroskedasticity. Even though the error is not constant on some of the regression plots for the data in its original form it changes mostly in regions with lower densities of data points and the overall shapes of the plots do not raise any concerns.\n\nThe original skewness of BodyFat is about 0.2, which is not a lot, but it might be beneficial to apply box-cox transformation anyway.\n\nThere are a lot of linear relationships between the variables and a lot of them are highly correlated. That will need to be taken into consideration going forward.\n\nSince the relationships between variables are linear I will use linear models in this notebook. Those are sensitive to outliers, but considering that the sample size is small I don't think it's best to hastily remove observations. In this dataset there may be reasons for why valid data points seem like outliers. For example it might be possible that some of them are representing a trend that is just being sampled and would make its own cluster on the plots. To lessen the influence of outliers I will later use spatial sign transformation.","bcf1cd2e":"### Lasso","993e24f9":"Great!\nNow let's see if all the values are physically possible and if there are any clear outliers.","abb24b1d":"Let's check if there are any missing values in the dataset.","e7ac6822":"And now for the Height column.","7c8c9769":"### ElasticNet - changing penalty ratio","415d52df":"I never had my body fat measured precisely and it's hard for me to know for sure what it is in reality. There might also be some error coming from the measurements I have taken, but based on my prior estimates I would say the prediction is really close.","e3634882":"### RandomForest","3aa3721b":"It does not look like reducing the number of features improves performance of the models, so I will stop here. Even features with low mutual information and importance provide proportional, but still, informative data.","cd5e8a5e":"### PLSR","eb79dc9b":"Since the features were created using the column Abdomen let's also check how do models trained on the engineered features compare to those with the exclusion of Abdomen.","4558727d":"A person weighting 93kg with a height of 73cm is maybe physically possible, but it is a huge outlier in the dataset, so I will delete this observation as well.","7a0f965e":"Now let's look at some scatter plots and relationships between variables.","47912fe5":"### Ridge","446255a0":"# Predicting my body fat level!\n\nSince I have created this model I would like to use it to predict my own body fat.","01fd389d":"<a id='mutual_info'><\/a>","86ea956b":"#### Observations\nThe best result is achieved by Lasso Regression (alpha ~= 0.012) with $R^2 \\approx 0.66$ and $RMSE \\approx 4.8$.\n\nLet's train this model once more and see how the predicted values compare to the real ones.\n","eb691456":"### Ridge","b81a3997":"#### Observations and conclusions\nThe predicted values generally follow a linear trend and no regions are getting under or over predicted. The greatest error is for 3 highest values of body fat. The reason for that is those levels of obesity are underrepresented in the dataset, but thanks to the spatial sping transformation that was applied it does not look like they influence the overall trend of the predictions.","97a9db12":"## Hyperparameter tuning","4107f6a4":"### PCR","a91db8a3":"The levels of body fat achieved by professional bodybuilders during season can reach 3-4%. Those levels are extremely hard to achieve and impossible to maintain. The two observations with bodyfat% of 0.0 and 0.7 are simply not possible and I will delete those.","33d0458a":"I guess I was right the first time around. In that case I will take the data engineered by dividing columns by Abdomen and create models with that.","45cf8246":"#### Observations\nThere is a near perfect linear relationship between BodyFat and Density, which indicates that just the density would be sufficient to predict the percentage of body fat in ones body. This is an expected observation, since body density is the only variable in the [Siri Equation](https:\/\/www.topendsports.com\/testing\/siri-equation.htm) mentioned in the description of the dataset by [fedesoriano](https:\/\/www.kaggle.com\/fedesoriano), which was used to populate the BodyFat column. However measurement of body density is the hardest one to acquire among all of the variables and I don't think it is meaningful to the problem of predicting, not calculating, body fat percentage. Body fat percentage and body density are synonymous, so I will drop body density and use only the circumference measurements to predict body fat percentage. Not using the body density variable will also make predictions of the model more accessible for many people.\n\nSince body fat in this dataset was calculated using body density all points on the scatter plot should follow a straight line with extremely slight variation. Three or four points on that plot are outliers.","00104614":"# Model building and evaluation\n\nSince the sample size is quite small I think it's best to use LeaveOneOut cross validation.\n\nI will start with comparing models trained on different numbers of features. To decide which features to drop I will look up the outputs from both [mutual information](#mutual_info) and [feature importance](#feature_importance) analysis.","fe6f441d":"### PLSR","63d7eb7b":"Both PCA and PLS create components with strong relationships with the target variable, which is good.\n\nI would also like to see how will the relationships between the independent variables and the dependent variable look like after applying the transformation I am planning to use. \n\nNote: I will not transform the variables, as that should be performed in accordance to the train-test split. I will create temporary dataframes just to see the results.","f97177c9":"There seem to be a few observations that have extremely small values. Let's see what they are.","48492b62":"Let's see how the created features compare to the original ones."}}