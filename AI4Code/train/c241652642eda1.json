{"cell_type":{"108dc684":"code","c6c8e97b":"code","5c72a059":"code","22ef0c7f":"code","3ea13f05":"code","51370ca2":"code","9df73fbc":"code","b5bf27cf":"code","d0de065d":"code","b6757e2e":"code","381f34c9":"code","3df082ca":"code","80c5c4dd":"code","72e2762a":"code","a9dcb961":"code","1c73e56e":"code","d778eafa":"code","bde1d512":"code","d3ec91ac":"code","763b7dec":"code","4e51d03f":"code","93ca07ef":"code","e7810587":"code","e4b339cf":"code","d6a344ca":"code","8b4ee88f":"code","7aeb2b05":"code","5ba0a097":"code","7d1a5451":"code","9e8a0cea":"code","e5348bba":"code","be9acb5f":"code","cde7ce73":"markdown","4faec027":"markdown","52fcc52b":"markdown","f5690fed":"markdown","1ec5f979":"markdown","a596fd11":"markdown","96d66e99":"markdown","2c94aa5e":"markdown","01663980":"markdown"},"source":{"108dc684":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","c6c8e97b":"import seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\nplt.style.use(\"fivethirtyeight\")\nHF_data = pd.read_csv(\"..\/input\/heart-failure-clinical-data\/heart_failure_clinical_records_dataset.csv\")","5c72a059":"HF_data.head()","22ef0c7f":"HF_data.info()\n# no null values in the dataset","3ea13f05":"HF_data[\"age\"] = HF_data['age'].astype(\"int\")","51370ca2":"HF_data.nunique()","9df73fbc":"print(sns.kdeplot(data = HF_data[\"anaemia\"],shade = True))\n# a decent amount of people does not have a lower quantity of blood cells.\nsns.barplot(x = HF_data[\"anaemia\"],y = HF_data[\"DEATH_EVENT\"],saturation  = 0.1)\n# as expected, there are more deaths from stroke who have anaemia.\nplt.title(\"Anaemia VS Death_event\")","b5bf27cf":"print(sns.boxplot(x = HF_data[\"age\"]))\n# majority of ages in the data are inbetween 50-70\nplt.title(\"Range of age in the dataset\")","d0de065d":"sns.barplot(x = 'DEATH_EVENT' , y = 'age' , data = HF_data)\n# seems like the average age does not play any vital role in Heart Failure\nplt.title(\"Age VS Death_event\")","b6757e2e":"sns.kdeplot(x = HF_data[\"creatinine_phosphokinase\"],shade = True)\n# most of the creatinine_phosphokinase is below 2000","381f34c9":"sns.stripplot(x = HF_data[\"DEATH_EVENT\"],y = HF_data[\"creatinine_phosphokinase\"])\n# higher creatinine_phosphokinase means a high level of stress. but it does not leave any major impact on heart failure\nplt.title(\"Creatinine Phosphokinase VS Death_event\")","3df082ca":"HF_data[HF_data[\"diabetes\"] & HF_data[\"DEATH_EVENT\"] == 1][\"DEATH_EVENT\"].value_counts()\n# out of 299 entries, 40 died who had diabetes.","80c5c4dd":"sns.swarmplot(x = HF_data[\"DEATH_EVENT\"], y = HF_data[\"ejection_fraction\"],hue = HF_data[\"DEATH_EVENT\"],s = 4)\n# when the ejection fraction falls below 30 death event increases\nplt.title(\"Ejection fraction VS Death_event\")","72e2762a":"mask1 = HF_data[HF_data[\"high_blood_pressure\"] & HF_data[\"DEATH_EVENT\"] == 1] \nmask1[\"DEATH_EVENT\"].sum()","a9dcb961":"plt.figure(figsize = (16,6))\nplt.subplot(1,2,1)\nsns.kdeplot(x = HF_data[\"platelets\"])\nplt.subplot(1,2,2)\nsns.scatterplot(x = HF_data[\"DEATH_EVENT\"],y = HF_data[\"platelets\"] ,hue = HF_data[\"DEATH_EVENT\"])\n# majority of entries have normal number of platelets (between 150,000 - 400,000) and \n# entries with platelets < 200,000 have greater risk of heart failure.\nplt.title(\"Platelets VS Death_event\")","1c73e56e":"plt.figure(figsize = (16,6))\nplt.subplot(1,2,1)\nsns.kdeplot(x = HF_data[\"serum_creatinine\"])\n# average values of serum creatinine is <= 1 for normal behaviour of body\nplt.subplot(122)\nsns.scatterplot(x = HF_data[\"DEATH_EVENT\"],y = HF_data[\"serum_creatinine\"],hue = HF_data[\"DEATH_EVENT\"])\n# increased values of serum creatinine leads to heart failure\nplt.title(\"Serum Creatinine VS Death_event\")","d778eafa":"plt.figure(figsize = (16,6))\nplt.subplot(1,2,1)\nsns.kdeplot(x = HF_data[\"serum_sodium\"])\nplt.subplot(122)\nsns.barplot(x = HF_data[\"DEATH_EVENT\"],y = HF_data[\"serum_sodium\"],)\n# Less quantity of serum sodium leads to heart failure but it not affect our model much,\n#since most of the death are from normal serum sodium quntity\nplt.title(\"Serum Sodium VS Death_event\")","bde1d512":"print(HF_data[\"sex\"].value_counts())\n# 1 - Male , 0 - Female\n# changing sex dtype to string\ncopy = HF_data.copy()\nMale = copy['sex'] == 1\ncopy.loc[Male,'sex'] = \"Male\"\nFemale = copy['sex'] == 0\ncopy.loc[Female,'sex'] = \"Female\"\n\n\n# changing smoking dtype to string\nsmoke_1 = copy['smoking'] == 1\ncopy.loc[smoke_1,'smoking'] = \"Yes\"\nsmoke_0 = copy['smoking'] == 0\ncopy.loc[smoke_0,'smoking'] = \"No\"\n\n# changing DEATH_EVENT dtype to string\ndeath_1 = copy['DEATH_EVENT'] == 1\ncopy.loc[smoke_1,'DEATH_EVENT'] = \"Yes\"\ndeath_0 = copy['DEATH_EVENT'] == 0\ncopy.loc[smoke_0,'DEATH_EVENT'] = \"No\"\n\ncopy.head()","d3ec91ac":"sex_count = copy[['sex','DEATH_EVENT']].value_counts()\nsex_count.head()\nsex_count = sex_count.reset_index()\nsex_count.columns  = ['sex','Death_event','count']\nsex_count","763b7dec":"smoker_count = copy[['smoking','DEATH_EVENT']].value_counts()\nsmoker_count = smoker_count.reset_index()\nsmoker_count.columns = ['smoking','Death_event','count']\nsmoker_count","4e51d03f":"smoke_sex_count = copy[['smoking','sex','DEATH_EVENT']].value_counts()\nsmoke_sex_count = smoke_sex_count.reset_index()\nsmoke_sex_count.columns = ['smoking','sex','Death_event','count']\nsmoke_sex_count\n# we see in males, the smokers and non-smokers does not have a big difference in death_count but\n# we see a big difference in females. It might be that majority of females does not smoke which in that case this result \n# will not be of much use. ","93ca07ef":"plt.figure(figsize = (16,10))\nplt.subplot(221)\nsns.barplot(x = 'Death_event',y = 'count',data = sex_count,hue = 'sex')\nplt.subplot(222)\nsns.barplot(x = 'Death_event',y = 'count',data = smoker_count,hue = 'smoking')\nplt.subplot(2,2,3)\nsns.barplot(x = 'Death_event',y = 'count',data = smoke_sex_count)","e7810587":"# Splitting the dataset using Train test split\nfeatures = HF_data[['anaemia','ejection_fraction','platelets','serum_creatinine']]\nresult = HF_data['DEATH_EVENT']\nprint(features.head(3))\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(features,result,train_size = 0.7,random_state = 1)","e4b339cf":"X_train = X_train.reset_index() \nX_train.drop('index',axis = 1,inplace = True)\nX_train.head()","d6a344ca":"X_test = X_test.reset_index()\nX_test.drop('index',axis = 1,inplace = True)\nX_test.head()","8b4ee88f":"from sklearn.preprocessing import StandardScaler\n\n\nsc = StandardScaler()\nX_train_fs = X_train.copy()\nX_test_fs = X_test.copy()\nX_train_fs.iloc[:,1:] = sc.fit_transform(X_train.iloc[:,1:])\nX_test_fs.iloc[:,1:] = sc.fit_transform(X_test.iloc[:,1:])","7aeb2b05":"from sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import f1_score\n\nClassifier = RandomForestClassifier(random_state = 0,max_leaf_nodes=100,max_depth=5,n_estimators = 100)\nClassifier.fit(X_train,y_train)\ny_pred = Classifier.predict(X_test)\ncm = confusion_matrix(y_test,y_pred)\nscore = accuracy_score(y_test,y_pred)\nf1_score = f1_score(y_test,y_pred)\nprint(cm)\nprint(\"Accuracy Score:{}\".format(score))\nprint(\"f1 Score:{}\".format(f1_score))\nplt.figure(figsize = (10,5))\nplt.subplot(121)\nsns.countplot(x = y_test)\nplt.title(\"Countplot of y_test\")\nplt.subplot(122)\nsns.countplot(x = y_pred)\nplt.title(\"Countplot of y_pred\")\nplt.xlabel(\"DEATH_EVENT\")","5ba0a097":"from sklearn.tree import DecisionTreeClassifier\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import f1_score\n\nClassifier_1 = DecisionTreeClassifier(random_state = 1,max_leaf_nodes = 100,max_depth=5)\nClassifier_1.fit(X_train,y_train)\ny_pred_1 = Classifier_1.predict(X_test)\ncm = confusion_matrix(y_test,y_pred_1)\nscore = accuracy_score(y_test,y_pred_1)\nf1_score = f1_score(y_test,y_pred_1)\nprint(cm)\nprint(\"Accuracy Score:{}\".format(score))\nprint(\"f1 Score:{}\".format(f1_score))\n\nplt.figure(figsize = (10,5))\nplt.subplot(121)\nsns.countplot(x = y_test)\nplt.title(\"Countplot of y_test\")\nplt.subplot(122)\nsns.countplot(x = y_pred_1)\nplt.title(\"Countplot of y_pred_1\")\nplt.xlabel(\"DEATH_EVENT\")\n\n\n# Decision Tree Classifier performs poorly in comparision to Random Forest Classifier","7d1a5451":"from sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import f1_score\n\nClassifier_2 = KNeighborsClassifier(weights = 'distance')\nClassifier_2.fit(X_train,y_train)\ny_pred_2 = Classifier_2.predict(X_test)\ncm = confusion_matrix(y_test,y_pred_2)\nscore = accuracy_score(y_test,y_pred_2)\nf1_score = f1_score(y_test,y_pred_2)\nprint(cm)\nprint(\"Accuracy Score: {}\".format(score))\nprint(\"f1 Score:{}\".format(f1_score))\n\nplt.figure(figsize = (10,5))\nplt.subplot(121)\nsns.countplot(x = y_test)\nplt.title(\"Countplot of y_test\")\nplt.subplot(122)\nsns.countplot(x = y_pred_2)\nplt.title(\"Countplot of y_pred_2\")\nplt.xlabel(\"DEATH_EVENT\")\n\n# since the dataset is not big and most of the DEATH_EVENT are '0', setting n_neighbors >10 will result in predicting\n# DEATH_EVENT to '0' most of the time\n\n# the K-NN plot matchs the test set almost perfectly even thought the accuracy score is low. I don't know what to say in this case","9e8a0cea":"from sklearn.svm import SVC\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import f1_score\n\nClassifier_3 = SVC(kernel='sigmoid')\nClassifier_3.fit(X_train_fs,y_train)\ny_pred_3 = Classifier_3.predict(X_test_fs)\ncm = confusion_matrix(y_test,y_pred_3)\nscore = accuracy_score(y_test,y_pred_3)\nf1_score = f1_score(y_test,y_pred_3)\nprint(cm)\nprint(\"Accuracy Score: {}\".format(score))\nprint(\"f1 Score:{}\".format(f1_score))\n\nplt.figure(figsize = (10,5))\nplt.subplot(121)\nsns.countplot(x = y_test)\nplt.title(\"Countplot of y_test\")\nplt.subplot(122)\nsns.countplot(x = y_pred_3)\nplt.title(\"Countplot of y_pred_3\")\nplt.xlabel(\"DEATH_EVENT\")","e5348bba":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import f1_score\n\nClassifier_4 = LogisticRegression(random_state = 1)\nClassifier_4.fit(X_train_fs,y_train)\ny_pred_4 = Classifier_4.predict(X_test_fs)\ncm = confusion_matrix(y_test,y_pred_4)\nscore = accuracy_score(y_test,y_pred_4)\nf1_score = f1_score(y_test,y_pred_4)\nprint(cm)\nprint(\"Accuracy Score:{}\".format(score))\nprint(\"f1 Score:{}\".format(f1_score))\n\nplt.figure(figsize = (10,5))\nplt.subplot(121)\nsns.countplot(x = y_test)\nplt.title(\"Countplot of y_test\")\nplt.subplot(122)\nsns.countplot(x = y_pred_4)\nplt.title(\"Countplot of y_pred_4\")\nplt.xlabel(\"DEATH_EVENT\")","be9acb5f":"from sklearn.naive_bayes import GaussianNB\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import f1_score\n\nClassifier_5 = GaussianNB()\nClassifier_5.fit(X_train_fs,y_train)\ny_pred_5 = Classifier_5.predict(X_test_fs)\ncm = confusion_matrix(y_test,y_pred_5)\nscore = accuracy_score(y_test,y_pred_5)\nf1_score = f1_score(y_test,y_pred_5)\nprint(cm)\nprint(\"Accuracy Score:{}\".format(score))\nprint(\"f1 Score:{}\".format(f1_score))\n\nplt.figure(figsize = (10,5))\nplt.subplot(121)\nsns.countplot(x = y_test)\nplt.title(\"Countplot of y_test\")\nplt.subplot(122)\nsns.countplot(x = y_pred_5)\nplt.title(\"Countplot of y_pred_5\")\nplt.xlabel(\"DEATH_EVENT\")","cde7ce73":"# **Logistic Regression**","4faec027":"# **Decision Tree Classifier**","52fcc52b":"# **Support Vector Machine**","f5690fed":"# **Random Forest Classifier**","1ec5f979":"# The time column cannot be a feature. It contains the time when the patient died which is of no use when predicting heart failures.","a596fd11":"# **Machine learning part(Feature scaling,Training and Fitting the model)**","96d66e99":"# **K - Nearest Neighbor**","2c94aa5e":"# **Naive Bayes**","01663980":"# Seeing the impact of each column on DEATH_EVENT column"}}