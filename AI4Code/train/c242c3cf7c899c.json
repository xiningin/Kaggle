{"cell_type":{"5701f109":"code","af25d7d5":"code","4e3b1e0a":"code","9c5bddd6":"code","48c26f5f":"code","5a880a09":"code","6b2a98f3":"code","fdeb87a5":"code","b1c1cc67":"code","ec9fd4fe":"code","7a362f25":"code","6a1adef5":"code","501e40fb":"code","2d6a6245":"code","2e4e1502":"code","2e3a005e":"code","1b734540":"code","5387f5f2":"markdown","fba20820":"markdown","6d2af092":"markdown","411ea199":"markdown","9689424d":"markdown"},"source":{"5701f109":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","af25d7d5":"# Load data\n\nX = pd.read_csv(\"\/kaggle\/input\/data-science-london-scikit-learn\/train.csv\", header = None)\ny = pd.read_csv(\"\/kaggle\/input\/data-science-london-scikit-learn\/trainLabels.csv\", header = None)\ntest = pd.read_csv(\"\/kaggle\/input\/data-science-london-scikit-learn\/test.csv\", header = None)","4e3b1e0a":"# Exploring the data\n\nX.shape,y.shape,test.shape","9c5bddd6":"X.info() # No null values","48c26f5f":"y.info() # No null values in y either","5a880a09":"X.describe()","6b2a98f3":"y.describe() # y only consist of 0 and 1","fdeb87a5":"X.head()","b1c1cc67":"y.head()","ec9fd4fe":"# Selecting certain datatypes in datasets \n\nX_object_included = X.select_dtypes(include=\"object\")\nX_object_excluded = X.select_dtypes(exclude=\"object\")\ny_object_included = y.select_dtypes(include=\"object\")\ny_object_excluded = y.select_dtypes(exclude=\"object\")","7a362f25":"# no column with objects so no categorical values\n\nlen(X_object_included.axes[1])","6a1adef5":"# all columns are numerical\n\nlen(X_object_excluded.axes[1])","501e40fb":"len(y_object_included.axes[1])","2d6a6245":"len(y_object_excluded.axes[1])","2e4e1502":"from sklearn.model_selection import train_test_split\n\n# Train-Test split\n\nX_train, X_test, y_train, y_test = train_test_split(X,y, test_size = 0.30, random_state = 123)\nX_train.shape,X_test.shape,y_train.shape,y_test.shape","2e3a005e":"from sklearn.metrics import accuracy_score \nfrom sklearn.model_selection import cross_val_score\n\n# NAIBE BAYES\nfrom sklearn.naive_bayes import GaussianNB\n\nmodel_1 = GaussianNB()\nmodel_1.fit(X_train,y_train.values.ravel()) #values.ravel\npredicted_1 = model_1.predict(X_test)\nprint('Naive Bayes',accuracy_score(y_test, predicted_1)) #accuracy score\n\n#KNN\nfrom sklearn.neighbors import KNeighborsClassifier\n\nmodel_2 = KNeighborsClassifier()\nmodel_2.fit(X_train,y_train.values.ravel()) \npredicted_2 = model_2.predict(X_test)\nprint('KNN',accuracy_score(y_test, predicted_2))\n\n#RANDOM FOREST\nfrom sklearn.ensemble import RandomForestClassifier\n\nmodel_3 = RandomForestClassifier(n_estimators = 100,random_state = 99) #n_estimators, random_state\nmodel_3.fit(X_train,y_train.values.ravel())\npredicted_3 = model_3.predict(X_test)\nprint('Random Forest',accuracy_score(y_test,predicted_3))\n\n#LOGISTIC REGRESSION\nfrom sklearn.linear_model import LogisticRegression\n\nmodel_4 = LogisticRegression(solver = 'saga') #solver\nmodel_4.fit(X_train,y_train.values.ravel())\npredicted_4 = model_4.predict(X_test)\nprint('Logistic Regression',accuracy_score(y_test, predicted_4))\n\n#SVM\nfrom sklearn.svm import SVC\n\nmodel_5 = SVC(gamma = 'auto') #gamma\nmodel_5.fit(X_train,y_train.values.ravel())\npredicted_5 = model_5.predict(X_test)\nprint('SVM',accuracy_score(y_test, predicted_5))\n\n#DECISON TREE\nfrom sklearn.tree import DecisionTreeClassifier\n\nmodel_6 = DecisionTreeClassifier()\nmodel_6.fit(X_train,y_train.values.ravel())\npredicted_6 = model_6.predict(X_test)\nprint('Decision Tree',accuracy_score(y_test, predicted_6))\n\n#XGBOOST\nfrom xgboost import XGBClassifier\n\nmodel_7 = XGBClassifier(use_label_encoder=False) #use_label_encoder=False\nmodel_7.fit(X_train,y_train.values.ravel())\npredicted_7 = model_7.predict(X_test)\nprint('XGBoost',accuracy_score(y_test, predicted_7))","1b734540":"#SVM has highest accuracy so far KNN is second XGBo","5387f5f2":"**Train various models**","fba20820":"This notebook is written to explore scikit on my own. \nI have used the Kaggle users notebook and referenced them below\n1. https:\/\/www.kaggle.com\/aman9d\/data-science-london-scikit\n2. https:\/\/www.kaggle.com\/chahat1\/data-science-london-classification\/notebook\n3. https:\/\/www.kaggle.com\/vigneshprakash\/data-science-london-scikit-learn-modeling\n4. https:\/\/www.kaggle.com\/abdelwahed43\/data-science-london-classification\n\nI have used Kaggle Courses and Scikit Docs too. I will keep coding and add my other notebooks here:\n","6d2af092":"**Train-Test Split**","411ea199":"**Looking the datatypes, how many cols are categorical or numerical**","9689424d":"**Loading and exploring data via describe, info, head ...**"}}