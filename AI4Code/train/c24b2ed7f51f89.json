{"cell_type":{"c844ca61":"code","2e1af2e5":"code","1b268e34":"code","c8db5dec":"code","b27f6ae8":"code","f4dd1143":"code","58c8394d":"code","3b559926":"code","fbc31354":"code","0b71d930":"code","4e852d29":"code","9e5ecc70":"code","6cd1cf51":"code","3383c6bf":"code","3b5bdf4c":"code","3ea9ccd3":"code","b943c517":"code","2532847f":"code","94b124bb":"code","99a795fa":"code","6c713cc4":"code","89d85440":"code","9b479e7e":"code","e1e302e3":"code","08ab02bb":"code","2706533a":"code","4ae6fafd":"code","ae0531b3":"code","cb0eadae":"code","4b933ea2":"code","e8e743a9":"code","016ca219":"code","3d6c809f":"code","538d4f39":"code","b9fa8dd0":"code","04755c39":"code","78badbb4":"code","38345a3c":"code","38b5d9d1":"code","31806595":"code","cc3f5ed1":"code","89b6ae46":"code","e8af38c9":"code","c4307db3":"code","16a5152e":"code","86e75322":"code","f725ae14":"code","94714fd1":"code","ffe11681":"markdown","f863b7f5":"markdown","a5e3c4b3":"markdown","2d107d27":"markdown","e59f07cd":"markdown","4d462a89":"markdown","522aecd4":"markdown","37ab786f":"markdown","58799316":"markdown","e222899f":"markdown","8fee4298":"markdown","8439db53":"markdown","37363acd":"markdown","6dda403d":"markdown","eaa6eee0":"markdown","32a3355c":"markdown","181b5b0a":"markdown","0592e3fc":"markdown","9d13f4df":"markdown","fb3bf63f":"markdown","222dbf19":"markdown","0c33d7ea":"markdown","629f1ff8":"markdown","df6244eb":"markdown","18c95292":"markdown","1d332314":"markdown","8901c7a8":"markdown","773f5c28":"markdown","3f50a84d":"markdown","c92884ed":"markdown","29b19903":"markdown","662b39c8":"markdown","08e62f3d":"markdown","2ef0f5e9":"markdown","1fa9baea":"markdown","4aee4050":"markdown"},"source":{"c844ca61":"import os\nimport re\nfrom tqdm import tqdm\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)   \nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nfrom nltk.corpus import stopwords   \nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder","2e1af2e5":"# generic function to plot the train Vs validation loss\/accuracy:\ndef plot_history(history):\n    loss_list = [s for s in history.history.keys() if 'loss' in s and 'val' not in s]\n    val_loss_list = [s for s in history.history.keys() if 'loss' in s and 'val' in s]\n    acc_list = [s for s in history.history.keys() if 'acc' in s and 'val' not in s]\n    val_acc_list = [s for s in history.history.keys() if 'acc' in s and 'val' in s]\n    if len(loss_list) == 0:\n        print('Loss is missing in history')\n        return \n    ## As loss always exists\n    epochs = range(1,len(history.history[loss_list[0]]) + 1)\n    plt.figure(figsize=(25,15))\n    ## Accuracy\n    plt.subplot(2,2,1)\n    for l in acc_list:\n        plt.plot(epochs, history.history[l], 'b', label='Training accuracy (' + str(format(history.history[l][-1],'.4f'))+')')\n    for l in val_acc_list:    \n        plt.plot(epochs, history.history[l], 'g', label='Validation accuracy (' + str(format(history.history[l][-1],'.4f'))+')')\n\n    plt.title('Training Accuracy Vs Validation Accuracy\\n')\n    plt.xlabel('Epochs')\n    plt.ylabel('Accuracy')\n    plt.legend()\n    ## Loss\n    plt.subplot(2,2,2)\n    for l in loss_list:\n        plt.plot(epochs, history.history[l], 'b', label='Training loss (' + str(str(format(history.history[l][-1],'.4f'))+')'))\n    for l in val_loss_list:\n        plt.plot(epochs, history.history[l], 'g', label='Validation loss (' + str(str(format(history.history[l][-1],'.4f'))+')'))\n    \n    plt.title('Training Loss Vs Validation Loss\\n')\n    plt.xlabel('Epochs')\n    plt.ylabel('Loss')\n    plt.legend()\n    plt.show()","1b268e34":"# Load the input features\n\ndef load_train():\n    pd.set_option('display.max_colwidth',None)\n    train =pd.read_csv('..\/input\/clean-quora-train-data\/clean_lem_stemmed_train_data.csv') \n    train=train.dropna()\n    return train","c8db5dec":"train_df = load_train()\ntrain_df.head()","b27f6ae8":"import tensorflow as tf\ntf.__version__","f4dd1143":"import tensorflow as tf\nfrom tensorflow import keras\nfrom keras.models import Sequential, Model\nfrom tensorflow.keras import regularizers\nfrom keras import layers\nfrom tensorflow.keras.layers import Embedding, Bidirectional, GlobalMaxPool1D\nfrom gensim.models import KeyedVectors\nfrom tensorflow.keras import regularizers\nfrom keras import layers\nfrom tensorflow.keras.layers import Embedding\nfrom keras.layers import Bidirectional, GlobalMaxPool1D\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom tensorflow.keras.layers import LSTM, Dense,Flatten,Conv2D,Conv1D,GlobalMaxPooling1D\nfrom tensorflow.keras.layers import Input, Embedding, Dense, Concatenate, TimeDistributed, Bidirectional\nfrom keras.optimizers import Adam\nfrom tensorflow.keras.callbacks import EarlyStopping\nfrom keras.utils import to_categorical","58c8394d":"from tensorflow import keras\nfrom keras.preprocessing.text import Tokenizer\n\nX = train_df['question_text'] # input\ny = train_df['target'].values # target \/label\n\nsentences_train,sentences_val,y_train,y_val = train_test_split(X,y,test_size=0.2,random_state=11)\n\ntokenizer = Tokenizer(num_words=30000)\ntokenizer.fit_on_texts(sentences_train)\nX_train = tokenizer.texts_to_sequences(sentences_train)\nX_val = tokenizer.texts_to_sequences(sentences_val)\n\n# Adding 1 because of  reserved 0 index\nvocab_size = len(tokenizer.word_index) + 1 # (in case of pre-trained embeddings it's +2)                         \nmaxlen = 131 # sentence length\n\nX_train = pad_sequences(X_train, padding='post', maxlen=maxlen)\nX_val = pad_sequences(X_val, padding='post', maxlen=maxlen)\n\n\nprint(\"Padded and Tokenized Training Sequence\".format(),X_train.shape)\nprint(\"Target Training Values Shape\".format(),y_train.shape)\nprint(\"_____________________________________________\")\nprint(\"Padded and Tokenized Validation Sequence\".format(),X_val.shape)\nprint(\"Target Validatation Values Shape\".format(),y_val.shape)","3b559926":"num_tokens=len(tokenizer.word_index)+2\nprint(\"Number of Features\/Tokens:\",num_tokens)","fbc31354":"# delete unused varibles\ndel train_df\nimport gc\ngc.collect()","0b71d930":"# seq2seq encoder-decoder\n\nmaxlen = 131\nmax_features = 50000\nembed_size = 131\n\nencoder_inp   = Input(shape=(maxlen,))\nencoder_embed = Embedding(max_features,embed_size,input_length=maxlen,trainable=True)(encoder_inp)\nencoder_lstm_cell = LSTM(60,return_state='True')\nencoder_output,encoder_state_h,encoder_state_c = encoder_lstm_cell(encoder_embed)\n#Creating LSTM decoder model and feeding the output states (h,c) of lstm of encoders\ndecoder_inp   = Input(shape=(maxlen,))\ndecoder_embed = Embedding(max_features,embed_size,input_length=maxlen,trainable=True)(decoder_inp)\ndecoder_lstm_cell = LSTM(60,return_sequences='True',return_state=True)\ndecoder_output,decoder_state_h,decoder_state_c = decoder_lstm_cell(decoder_embed,initial_state=[encoder_state_h,encoder_state_c])\ndecoder_dense_cell1 = Dense(16,activation='relu')\ndecoder_d_output    = decoder_dense_cell1(decoder_output)\ndecoder_dense_cell2 = Dense(1,activation='sigmoid')\ndecoder_output = decoder_dense_cell2(decoder_d_output)\nmodel = Model([encoder_inp,decoder_inp],decoder_output) \nmodel.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])\nmodel.summary()","4e852d29":"%%time\nfrom tqdm import tqdm\nfrom tqdm.keras import TqdmCallback\n\nhistory = model.fit([X_train,X_train],y_train,batch_size=1024,epochs=2, callbacks=[TqdmCallback(verbose=0)])","9e5ecc70":"plot_history(history)","6cd1cf51":"from keras.utils import plot_model\n\nplot_model(model,to_file=\"seq2seq_encoder_decoder_model.png\",\n    show_shapes=True,\n    show_layer_names=True,\n    rankdir=\"TB\",\n    expand_nested=False,\n    dpi=96)","3383c6bf":"gc.collect()","3b5bdf4c":"%%time\n\n# Using the fasttext word embeddding from crawl\n\nfasttext_file= \"..\/input\/pretrained\/crawl-300d-2M.vec\"\nfasttext_model = KeyedVectors.load_word2vec_format(fasttext_file, binary=False)\n\nword_index = tokenizer.word_index\nnum_tokens = len(tokenizer.word_index)+2\nprint(\"Found %s word vectors.\" % len(fasttext_model.vocab))","3ea9ccd3":"## prepare a corresponding embedding matrix that used in a Keras Embedding layer. \n## It's a simple NumPy matrix where entry at index i is the pre-trained vector for the word of index i in our vectorizer's vocabulary.\n\nembed_size = 300\nembedding_matrix = np.zeros((num_tokens, embed_size))\n\nhits = 0\nmisses = 0\n\n# embedding matrix\nfor word, i in word_index.items():\n    try:\n        embedding_vector = fasttext_model.get_vector(word)\n        if embedding_vector is not None:\n            embedding_matrix[i] = embedding_vector\n            hits += 1\n        else:\n            misses += 1\n    except:\n        misses += 1\n    \n        \nprint(\"Converted %d words (%d misses)\" % (hits, misses))","b943c517":"embedding_matrix.shape","2532847f":"num_tokens = len(tokenizer.word_index)+2\nnum_tokens","94b124bb":"embedding_dim=300\nnum_tokens = len(tokenizer.word_index)+2\n\nencoder_inp=Input(shape=(maxlen,))\nencoder_embed = Embedding(num_tokens,embedding_dim,weights=[embedding_matrix],trainable=False)(encoder_inp)\nencoder_lstm_cell=Bidirectional(LSTM(60,return_state='True'))\nencoder_outputs,encoder_state_flstm_h,encoder_state_flstm_c,encoder_state_blstm_h,encoder_state_blstm_c=encoder_lstm_cell(encoder_embed)\nencoded_states=[encoder_state_flstm_h,encoder_state_flstm_c,encoder_state_blstm_h,encoder_state_blstm_c]\n\n# Creating LSTM decoder model and feeding the output states (h,c) of lstm of encoders\ndecoder_inp=Input(shape=(maxlen,))\ndecoder_embed=Embedding(num_tokens,embedding_dim,weights=[embedding_matrix])(decoder_inp)\ndecoder_lstm_cell=Bidirectional(LSTM(60,return_sequences='True',return_state=True),merge_mode=\"concat\")\ndecoder_outputs,decoder_state_lstm_h,decoder_state_lstm_c,_,_=decoder_lstm_cell(decoder_embed,initial_state=[encoder_state_flstm_h,encoder_state_flstm_c,encoder_state_blstm_h,encoder_state_blstm_c])\n\n# decoderoutputs,_,_=decoder_lstm_cell(decoder_embed,initial_state=encoded_states)\n\n\ndecoder_dense_cell = Dense(100,activation='relu')\ndecoder_d_output = decoder_dense_cell(decoder_outputs)\ndecoder_dense_cell2 =Dense(1,activation='sigmoid')\ndecoder_output =decoder_dense_cell2(decoder_d_output)\n    \nmodel = Model([encoder_inp,decoder_inp],decoder_output)\n\nmodel.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])\nmodel.summary()","99a795fa":"from keras.utils import plot_model\n\nplot_model(model,\n           to_file=\"seq2seq_encoder_decoder_model_fast_bilstm.png\",\n           show_shapes=True,\n           show_layer_names=True,\n           rankdir=\"TB\",\n           expand_nested=False,\n           dpi=96)","6c713cc4":"from tqdm import tqdm\nfrom tqdm.keras import TqdmCallback\n\n#history = model.fit([X_train,X_train],y_train,batch_size=1024,epochs=2, verbose=0, callbacks=[TqdmCallback(verbose=0)])","89d85440":"# ELMO works well with TF 1.15 (rather any TF version <2.0.0) . For using ELMO from Tensorflow Hub, we have to follow the steps:\n# Restart the Kernel\n# Run the following\n\n#!pip install --upgrade pip\n#!pip install -U tensorflow==1.15\n#import tensorflow as tf\n#tf.__version__","9b479e7e":"#elmo_embed = hub.Module(\"https:\/\/tfhub.dev\/google\/elmo\/2\",trainable=True)\n\n#Creating the elmo embeddings by squeezing the inputs\n#def create_embedding(z):return elmo_embed(tf.squeeze(tf.cast(z,tf.string)),signature='default',as_dict=True)[\"default\"]","e1e302e3":"!pip install tensorflow-addons==0.11.2 -q\nimport tensorflow_addons as tfa","08ab02bb":"from tensorflow import keras\nfrom keras.preprocessing.text import Tokenizer","2706533a":"import os\nimport unicodedata\nimport re\nimport io\nimport time\n\ndef download_nmt():\n    path_to_zip = tf.keras.utils.get_file(\n    'spa-eng.zip', origin='http:\/\/storage.googleapis.com\/download.tensorflow.org\/data\/spa-eng.zip',\n    extract=True)\n\n    path_to_file = os.path.dirname(path_to_zip)+\"\/spa-eng\/spa.txt\"\n    return path_to_file\n\npath_to_file = download_nmt()\n\nprint(path_to_file)","4ae6fafd":"class NMTDataset:\n    def __init__(self, problem_type='en-spa'):\n        self.problem_type = 'en-spa'\n        self.inp_lang_tokenizer = None\n        self.targ_lang_tokenizer = None\n    \n\n    def unicode_to_ascii(self, s):\n        return ''.join(c for c in unicodedata.normalize('NFD', s) if unicodedata.category(c) != 'Mn')\n\n    ## Step 1 and Step 2 \n    def preprocess_sentence(self, w):\n        w = self.unicode_to_ascii(w.lower().strip())\n\n        # creating a space between a word and the punctuation following it\n        # eg: \"he is a boy.\" => \"he is a boy .\"\n        # Reference:- https:\/\/stackoverflow.com\/questions\/3645931\/python-padding-punctuation-with-white-spaces-keeping-punctuation\n        w = re.sub(r\"([?.!,\u00bf])\", r\" \\1 \", w)\n        w = re.sub(r'[\" \"]+', \" \", w)\n\n        # replacing everything with space except (a-z, A-Z, \".\", \"?\", \"!\", \",\")\n        w = re.sub(r\"[^a-zA-Z?.!,\u00bf]+\", \" \", w)\n\n        w = w.strip()\n\n        # adding a start and an end token to the sentence\n        # so that the model know when to start and stop predicting.\n        w = '<start> ' + w + ' <end>'\n        return w\n    \n    def create_dataset(self, path, num_examples):\n        # path : path to spa-eng.txt file\n        # num_examples : Limit the total number of training example for faster training (set num_examples = len(lines) to use full data)\n        lines = io.open(path, encoding='UTF-8').read().strip().split('\\n')\n        word_pairs = [[self.preprocess_sentence(w) for w in l.split('\\t')]  for l in lines[:num_examples]]\n\n        return zip(*word_pairs)\n\n    # Step 3 and Step 4\n    def tokenize(self, lang):\n        # lang = list of sentences in a language\n        \n        # print(len(lang), \"example sentence: {}\".format(lang[0]))\n        lang_tokenizer = tf.keras.preprocessing.text.Tokenizer(filters='', oov_token='<OOV>')\n        lang_tokenizer.fit_on_texts(lang)\n\n        ## tf.keras.preprocessing.text.Tokenizer.texts_to_sequences converts string (w1, w2, w3, ......, wn) \n        ## to a list of correspoding integer ids of words (id_w1, id_w2, id_w3, ...., id_wn)\n        tensor = lang_tokenizer.texts_to_sequences(lang) \n\n        ## tf.keras.preprocessing.sequence.pad_sequences takes argument a list of integer id sequences \n        ## and pads the sequences to match the longest sequences in the given input\n        tensor = tf.keras.preprocessing.sequence.pad_sequences(tensor, padding='post')\n\n        return tensor, lang_tokenizer\n\n    def load_dataset(self, path, num_examples=None):\n        # creating cleaned input, output pairs\n        targ_lang, inp_lang = self.create_dataset(path, num_examples)\n\n        input_tensor, inp_lang_tokenizer = self.tokenize(inp_lang)\n        target_tensor, targ_lang_tokenizer = self.tokenize(targ_lang)\n\n        return input_tensor, target_tensor, inp_lang_tokenizer, targ_lang_tokenizer\n\n    def call(self, num_examples, BUFFER_SIZE, BATCH_SIZE):\n        file_path = download_nmt()\n        input_tensor, target_tensor, self.inp_lang_tokenizer, self.targ_lang_tokenizer = self.load_dataset(file_path, num_examples)\n        \n        input_tensor_train, input_tensor_val, target_tensor_train, target_tensor_val = train_test_split(input_tensor, target_tensor, test_size=0.2)\n\n        train_dataset = tf.data.Dataset.from_tensor_slices((input_tensor_train, target_tensor_train))\n        train_dataset = train_dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)\n\n        val_dataset = tf.data.Dataset.from_tensor_slices((input_tensor_val, target_tensor_val))\n        val_dataset = val_dataset.batch(BATCH_SIZE, drop_remainder=True)\n\n        return train_dataset, val_dataset, self.inp_lang_tokenizer, self.targ_lang_tokenizer","ae0531b3":"BUFFER_SIZE = 32000\nBATCH_SIZE = 32\n# Let's limit the #training examples for faster training\nnum_examples = 18000\n\ndataset_creator = NMTDataset('en-spa')\ntrain_dataset, val_dataset, inp_lang, targ_lang = dataset_creator.call(num_examples, BUFFER_SIZE, BATCH_SIZE)","cb0eadae":"example_input_batch, example_target_batch = next(iter(train_dataset))\nexample_input_batch.shape, example_target_batch.shape","4b933ea2":"vocab_inp_size = len(inp_lang.word_index)+1\nvocab_tar_size = len(targ_lang.word_index)+1\nmax_length_input = example_input_batch.shape[1]  # max_length_english\nmax_length_output = example_target_batch.shape[1]# max_length_spanish\n\nembedding_dim = 256\nunits = 1024\nsteps_per_epoch = num_examples\/\/BATCH_SIZE\n\n\nprint(\"max_length_english, max_length_spanish; vocab_size_english, vocab_size_spanish\")\nmax_length_input, max_length_output,vocab_inp_size, vocab_tar_size","e8e743a9":"class Encoder(tf.keras.Model):\n    def __init__(self, vocab_size, embedding_dim, enc_units, batch_size):\n        super(Encoder, self).__init__()\n        self.batch_size = batch_size\n        self.enc_units = enc_units\n        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n          ##-------- LSTM layer in Encoder ------- ##\n        self.lstm_layer = tf.keras.layers.LSTM(self.enc_units,return_sequences=True,\n                                               return_state=True,\n                                               recurrent_initializer='glorot_uniform')\n    def call(self, x, hidden):\n        x = self.embedding(x)\n        output, h, c = self.lstm_layer(x, initial_state = hidden)\n        return output, h, c\n    \n    def initialize_hidden_state(self):\n        return [tf.zeros((self.batch_size, self.enc_units)), tf.zeros((self.batch_size, self.enc_units))] # all zeros","016ca219":"## Test Encoder Stack\n\nencoder = Encoder(vocab_inp_size, embedding_dim, units, BATCH_SIZE)\n\n\n# sample input\nsample_hidden = encoder.initialize_hidden_state()\nsample_output, sample_h, sample_c = encoder(example_input_batch, sample_hidden)\nprint ('Encoder output shape: (batch size, sequence length, units) {}'.format(sample_output.shape))\nprint ('Encoder h vecotr shape: (batch size, units) {}'.format(sample_h.shape))\nprint ('Encoder c vector shape: (batch size, units) {}'.format(sample_c.shape))","3d6c809f":"sample_hidden","538d4f39":"class Decoder(tf.keras.Model):\n    def __init__(self, vocab_size, embedding_dim, dec_units, batch_size, attention_type='luong'):\n        super(Decoder, self).__init__()\n        self.batch_size = batch_size\n        self.dec_units = dec_units\n        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n        self.attention_type = attention_type\n        #Final Dense layer on which softmax will be applied\n        self.fc = tf.keras.layers.Dense(vocab_size)\n        # Define the fundamental cell for decoder recurrent structure\n        self.decoder_rnn_cell = tf.keras.layers.LSTMCell(self.dec_units)\n        \n        # Sampler\n        self.sampler = tfa.seq2seq.sampler.TrainingSampler()\n        \n        # Create attention mechanism with memory = None\n        self.attention_mechanism = self.build_attention_mechanism(self.dec_units,None,\n                                                                  self.batch_size*[max_length_input], self.attention_type)\n        # Wrap attention mechanism with the fundamental rnn cell of decoder\n        self.rnn_cell = self.build_rnn_cell(batch_size)\n        # Define the decoder with respect to fundamental rnn cell\n        self.decoder = tfa.seq2seq.BasicDecoder(self.rnn_cell, sampler=self.sampler, output_layer=self.fc)\n    \n    def build_rnn_cell(self, batch_size):\n        rnn_cell = tfa.seq2seq.AttentionWrapper(self.decoder_rnn_cell,self.attention_mechanism,\n                                                attention_layer_size=self.dec_units)\n        return rnn_cell\n\n    def build_attention_mechanism(self, dec_units, memory, memory_sequence_length, attention_type='luong'):\n    # type: Which sort of attention (Bahdanau, Luong)\n    # dec_units: final dimension of attention outputs \n    # memory: encoder hidden states of shape (batch_size, max_length_input, enc_units)\n    # memory_sequence_length: 1d array of shape (batch_size) with every element set to max_length_input (for masking purpose)\n        if(attention_type=='bahdanau'):\n            return tfa.seq2seq.BahdanauAttention(units=dec_units, memory=memory, memory_sequence_length=memory_sequence_length)\n        else:\n            return tfa.seq2seq.LuongAttention(units=dec_units, memory=memory, memory_sequence_length=memory_sequence_length)\n\n    def build_initial_state(self, batch_size, encoder_state, Dtype):\n        decoder_initial_state = self.rnn_cell.get_initial_state(batch_size, dtype=Dtype)\n        decoder_initial_state = decoder_initial_state.clone(cell_state=encoder_state)\n        return decoder_initial_state\n    def call(self, inputs, initial_state):\n        x = self.embedding(inputs)\n        outputs, _, _ = self.decoder(x, initial_state=initial_state, sequence_length=self.batch_size*[maxlen_output-1])\n        return outputs","b9fa8dd0":"# Test decoder stack \n\ndecoder = Decoder(vocab_tar_size, embedding_dim, units, BATCH_SIZE, 'bahdanau')\nsample_x = tf.random.uniform((BATCH_SIZE, max_length_output))\ndecoder.attention_mechanism.setup_memory(sample_output)\ninitial_state = decoder.build_initial_state(BATCH_SIZE, [sample_h, sample_c], tf.float32)\n\n\nsample_decoder_outputs = decoder(sample_x, initial_state)\n\nprint(\"Decoder Outputs Shape: \", sample_decoder_outputs.rnn_output.shape)","04755c39":"optimizer = tf.keras.optimizers.Adam()\n\n\ndef loss_function(real, pred):\n  # real shape = (BATCH_SIZE, max_length_output)\n  # pred shape = (BATCH_SIZE, max_length_output, tar_vocab_size )\n    cross_entropy = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction='none')\n    loss = cross_entropy(y_true=real, y_pred=pred)\n    mask = tf.logical_not(tf.math.equal(real,0))   #output 0 for y=0 else output 1\n    mask = tf.cast(mask, dtype=loss.dtype)  \n    loss = mask* loss\n    loss = tf.reduce_mean(loss)\n    return loss","78badbb4":"#Checkpoints \n\ncheckpoint_dir = '.\/training_checkpoints'\ncheckpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\ncheckpoint = tf.train.Checkpoint(optimizer=optimizer,\n                                 encoder=encoder,\n                                 decoder=decoder)","38345a3c":"@tf.function\ndef train_step(inp, targ, enc_hidden):\n  loss = 0\n\n  with tf.GradientTape() as tape:\n    enc_output, enc_h, enc_c = encoder(inp, enc_hidden)\n\n\n    dec_input = targ[ : , :-1 ] # Ignore <end> token\n    real = targ[ : , 1: ]         # ignore <start> token\n\n    # Set the AttentionMechanism object with encoder_outputs\n    decoder.attention_mechanism.setup_memory(enc_output)\n\n    # Create AttentionWrapperState as initial_state for decoder\n    decoder_initial_state = decoder.build_initial_state(BATCH_SIZE, [enc_h, enc_c], tf.float32)\n    pred = decoder(dec_input, decoder_initial_state)\n    logits = pred.rnn_output\n    loss = loss_function(real, logits)\n    variables = encoder.trainable_variables + decoder.trainable_variables\n    gradients = tape.gradient(loss, variables)\n    optimizer.apply_gradients(zip(gradients, variables))\n    return loss","38b5d9d1":"gc.collect()","31806595":"%%time\n\nEPOCHS = 10\n\nfor epoch in range(EPOCHS):\n    start = time.time()\n    enc_hidden = encoder.initialize_hidden_state()\n    total_loss = 0\n    # print(enc_hidden[0].shape, enc_hidden[1].shape)\n    \n    for (batch, (inp, targ)) in enumerate(train_dataset.take(steps_per_epoch)):\n        batch_loss = train_step(inp, targ, enc_hidden)\n        total_loss += batch_loss\n        if batch % 100 == 0:\n            print('Epoch {} Batch {} Loss {:.4f}'.format(epoch + 1,batch,batch_loss.numpy()))\n        # saving (checkpoint) the model every 2 epochs\nif (epoch + 1) % 2 == 0:\n    checkpoint.save(file_prefix = checkpoint_prefix)\n    print('Epoch {} Loss {:.4f}'.format(epoch + 1,total_loss \/ steps_per_epoch))\n    print('Time taken for 1 epoch {} sec\\n'.format(time.time() - start))","cc3f5ed1":"def evaluate_sentence(sentence):\n    sentence = dataset_creator.preprocess_sentence(sentence)\n    inputs = [inp_lang.word_index[i] for i in sentence.split(' ')]\n    inputs = tf.keras.preprocessing.sequence.pad_sequences([inputs],maxlen=max_length_input,\n                                                           padding='post')\n    inputs = tf.convert_to_tensor(inputs)\n    inference_batch_size = inputs.shape[0]\n    result = ''\n    \n    enc_start_state = [tf.zeros((inference_batch_size, units)), tf.zeros((inference_batch_size,units))]\n    enc_out, enc_h, enc_c = encoder(inputs, enc_start_state)\n    dec_h = enc_h\n    dec_c = enc_c\n    \n    start_tokens = tf.fill([inference_batch_size], targ_lang.word_index['<start>'])\n    end_token = targ_lang.word_index['<end>']\n\n    greedy_sampler = tfa.seq2seq.GreedyEmbeddingSampler()\n    \n    # Instantiate BasicDecoder object\n    decoder_instance = tfa.seq2seq.BasicDecoder(cell=decoder.rnn_cell, \n                                                sampler=greedy_sampler, output_layer=decoder.fc)\n    # Setup Memory in decoder stack\n    decoder.attention_mechanism.setup_memory(enc_out)\n    # set decoder_initial_state\n    decoder_initial_state = decoder.build_initial_state(inference_batch_size, [enc_h, enc_c], tf.float32)\n\n\n  ### Since the BasicDecoder wraps around Decoder's rnn cell only, you have to ensure that the inputs to BasicDecoder \n  ### decoding step is output of embedding layer. tfa.seq2seq.GreedyEmbeddingSampler() takes care of this. \n  ### You only need to get the weights of embedding layer, which can be done by decoder.embedding.variables[0] and pass this callabble to BasicDecoder's call() function\n\n    decoder_embedding_matrix = decoder.embedding.variables[0]\n    outputs, _, _ = decoder_instance(decoder_embedding_matrix, start_tokens = start_tokens, \n                                     end_token= end_token, initial_state=decoder_initial_state)\n    return outputs.sample_id.numpy()\n\ndef translate(sentence):\n    result = evaluate_sentence(sentence)\n    print(result)\n    result = targ_lang.sequences_to_texts(result)\n    print('Input: %s' % (sentence))\n    print('Predicted translation: {}'.format(result))","89b6ae46":"# restoring the latest checkpoint in checkpoint_dir\ncheckpoint.restore(tf.train.latest_checkpoint(checkpoint_dir))","e8af38c9":"translate(u'hace mucho frio aqui.')","c4307db3":"translate(u'\u00bftodavia estan en casa?')","16a5152e":"#!pip install pyyaml~=3.12\n!pip install nbdime==2.0.0","86e75322":"!python3.7 -m pip install --upgrade pip\n!pip install --upgrade requests -q","f725ae14":"!pip install transformers -q","94714fd1":"from transformers import pipeline\nprint(pipeline('sentiment-analysis')('we do not love you'))","ffe11681":"# \ud83c\udf89End-to-End-Natural Language Processing-4 \n\n### Recap the salient concepts demonstrated in the previous kernels:\n- Cleaning the text data free from the stop words, punctuations, numbers, special characters, emojis, email IDs, websites, etc\n- Lemmating the text data to the root word.e.g., went -> go\n- Stemming the words to avoid duplication of the words.e.g., raining -> rain\n- Exploratory analysis of text data\n- Word clouds and Ngram analysis\n- Non-semantic Vectorization of words (One Hot, TF-IDF)\n- Dimension reduction techniques (PCA,SVD,TSNE) and visualisation of Ndim vectors in 2D space\n- Statistical and ensemble algorithms\n- Semantic vectorization of words in Ndim space - Static word embeddings\n- Word2Vec algorithm - CBOW and Skipgram - Word Embeddings in vector space\n- Pre-trained word embeddings: GloVe, GoogleNews, fasttext \n- Neural Networks - Convolutional, Recurrent, and BiRNN with pre-trained embeddings","f863b7f5":"### Self-Attention (Scaled Dot-Product Attention)\n\nTransformer is a model introduced in the paper Attention is All You Need in 2017. It is based solely on attention mechanisms.\n\nCurrently, Transformers (with variations) are de-facto standard models not only in sequence to sequence tasks but also for language modeling and in pretraining settings.\n\n#### Why self-attention?\n\nTransformer's encoder tokens interact with each other all at once and exchanges information to understand each other better in the context of the whole sentence. This happens in several layers (N=6). Basically, self-attention layer computes similarity scores among the tokens, and normalised to give the better representation of input\/output tokens.\n\nIn each decoder layer, tokens of the prefix also interact with each other via a self-attention mechanism, but additionally, they look at the encoder states.\n\nThe difference between \"attention\" and \"self-attention\" is that self-attention operates between representations of the same nature: e.g., all encoder states in some **K**th layer.\n\n#### Calculating self-attention weights?\n\n**The first step** in calculating self-attention matrix is to create three vectors from each of the encoder\u2019s input vectors (e.g.,X). So for each word, we create a Query vector(Q), a Key vector(K), and a Value vector(V). \n\n![fig](https:\/\/jalammar.github.io\/images\/t\/self-attention-matrix-calculation.png)\n\nIn the above figure, Wq, Wk,Wv are weight vectors are of dimensionality [maximum sequence length x 64] to make the computations faster. These vectors are randomly initialised and learn the accurate weights while training.\n\n#### Query, Key, and Value in Self-Attention\n\nEach input token in self-attention receives three representations corresponding to the roles it can play:\n\nquery (Q) - asks for information (how similar is the token vis-a-vis other tokens?)\n\nkey (K)   - says it has some information\n\nvalue (V) - gives the information\n\nThe query is used when a token looks at others - it's seeking the information to understand itself better. \nThe key is responding to a query's request: it is used to compute attention weights. \nThe value is used to compute attention output: it gives information to the tokens which \"say\" they need it (i.e. assigned large weights to the similar tokens).\n\n1. compute the dot product between query matrix and key matrix, QK^T\n2. divide QK^T by square-root of Key vector\n3. apply the softmax to normalise the scores to obtain the score matrix\n4. compute the attention weights matrix by multiplying the score matrix by the value matrix V.\n\n![self attention score](https:\/\/www.tensorflow.org\/images\/tutorials\/transformer\/scaled_attention.png)\n\n\n![fig](https:\/\/images.prismic.io\/peltarionv2\/a7445eec-3dd8-4e62-948c-e3c3f9b3a987_self-attention_head.svg?auto=compress%2Cformat&rect=0%2C0%2C148%2C150&w=1980&h=2007)\n\n### The following clip illustrates the self-attention:\n\n![](https:\/\/lena-voita.github.io\/resources\/lectures\/seq2seq\/transformer\/encoder_self_attention.mp4)","a5e3c4b3":"## Attention Architectures","2d107d27":"## Installing Transformers library","e59f07cd":"## Sequence-to-Sequence (seq2seq) Model\n\nSequence-to-sequence (seq2seq) models (Sutskever et al., 2014, Cho et al., 2014) have enjoyed great success in a variety of tasks such as machine translation, speech recognition, and text summarization. \n\n> ### **Neural Machine Translation (NMT) which was the very first testbed for seq2seq models is wild success!**\n\nEncoder-decoder is the standard modeling paradigm for sequence-to-sequence tasks. It can capture long-range dependencies in languages, e.g., gender agreements; syntax structures; etc., and produce much more fluent translations as demonstrated by Google Neural Machine Translation systems.\n\n\nA natural choice for sequential data is the recurrent neural network (RNN), used by most NMT models. Usually an RNN is used for both the encoder and decoder. The RNN models, however, differ in terms of: (a) directionality \u2013 unidirectional or bidirectional; (b) depth \u2013 single- or multi-layer; and (c) type \u2013 often either a vanilla RNN, a Long Short-term Memory (LSTM), or a gated recurrent unit (GRU). \n\n\n[\ud83d\udcd6 READ the tutorial on NMT](https:\/\/github.com\/tensorflow\/nmt)\n\n### Let's understand how Encoder - Decoder works?\n\n> ### Encoder-decoder Framework \n\n![Neural Machine Translation - Source: Github ](https:\/\/raw.githubusercontent.com\/tensorflow\/nmt\/master\/nmt\/g3doc\/img\/encdec.jpg)\n\nThis framework consists of two components:\n\n1. encoder - reads source sequence(inputs) to build a \"thought\" vector (context vector)\n\n2. decoder - uses context vector from the encoder to generate the target sequence\n\nsequence-to-sequence models need to estimate the conditional probability p(y|x) of a sequence y given a source x.That's why sequence-to-sequence tasks can be modeled as Conditional Language Models (CLM) - they operate similarly to LMs, but additionally receive source information x.\n\nSeq2seq models are trained to predict probability distributions of the next token given previous context (source and previous target tokens). Intuitively, at each step we maximize the probability a model assigns to the correct token.\n\n![Lena Voita's Blog](https:\/\/lena-voita.github.io\/resources\/lectures\/seq2seq\/general\/enc_dec_prob_idea.mp4)\n\n![Encoder Decoder- Lena Voita's Blog](https:\/\/lena-voita.github.io\/resources\/lectures\/seq2seq\/general\/enc_dec_linear_out-min.png)\n\n### What happens under the hood\n\nRNN takes two inputs at each time step: an input (in the case of the encoder, one word from the input sentence), and a hidden state. The word, however, needs to be represented by a vector. To transform a word into a vector, we turn to the class of methods called \u201cword embedding\u201d algorithms.\n\nThe next RNN step takes the second input vector and hidden state #1 to create the output of that time step. In the following visualization, each pulse for the encoder or decoder is that RNN processing its inputs and generating an output for that time step. Since the encoder and decoder are both RNNs, each time step one of the RNNs does some processing, it updates its hidden state based on its inputs and previous inputs it has seen. The decoder also maintains a hidden states that it passes from one time step to the next.\n\n![Jay Alammar's Blog](http:\/\/jalammar.github.io\/images\/seq2seq_6.mp4)\n\n[\ud83d\udcd6 READ Sequence to Sequence Learning with Neural Networks-Ilya Sutskever,Oriol Vinyals,Quoc V. Le](https:\/\/arxiv.org\/pdf\/1409.3215.pdf)\n\n> ### Source: [Tutorial on NMT on github](https:\/\/raw.githubusercontent.com\/tensorflow\/nmt), [Jay Alammar's Blog](http:\/\/jalammar.github.io) & [Lena Voita's Blog](https:\/\/lena-voita.github.io\/resources)","4d462a89":"### Masked Self-Attention of Decoder\n\nIn the Transdormer decoder, self-attention is a bit different from the one in the encoder. \n\nThe encoder receives all tokens at once, while the decoder generates one token at a time: during generation, it doesn't know which tokens will be generated in the future.\n\nTo forbid the decoder to look ahead, the model uses masked self-attention: future tokens are masked out. \n\n![](https:\/\/lena-voita.github.io\/resources\/lectures\/seq2seq\/transformer\/masked_self_attn.mp4)","522aecd4":"## Using tf-addons BasicDecoder for decoding\u00b6","37ab786f":"<img src =\"https:\/\/cdn.analyticsvidhya.com\/wp-content\/uploads\/2019\/03\/output_YyJc8E.gif\">","58799316":"# Transformer Architecture Explained!!\n\n[\ud83d\udcd6 READ **Attention Is All You Need**- Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, Illia Polosukhine](https:\/\/arxiv.org\/abs\/1706.03762)","e222899f":"> ### Calculating Attention Vector\n\n![Decoder](https:\/\/jalammar.github.io\/images\/attention_process.mp4)\n\nIn order to focus on the parts of the input that are relevant to the decoding time step, the decoder does the following:\n\n1. Look at the set of encoder hidden states it received \u2013 each encoder hidden states is most associated with a certain word in the input sentence\n2. Give each hidden states a (alignment) score (either dot product or concatanation of encoder hidden state and previous decoder hidden state)\n![](https:\/\/raw.githubusercontent.com\/tensorflow\/nmt\/master\/nmt\/g3doc\/img\/attention_equation_1.jpg)\n3. Multiply each hidden states by its softmaxed score, thus amplifying hidden states with high scores, and drowning out hidden states with low scores.\n4. Weighted sum of the vector to get Context vector for decoder for this time step.\n\n\nSource: https:\/\/jalammar.github.io","8fee4298":"## One train_step Operation","8439db53":"## Encoder Class","37363acd":"## Decoder Class","6dda403d":"### Seq2Seq (BiLSTM) Model with fasttext Embeddings","eaa6eee0":"##  Layer Normalization\n\n![](https:\/\/lena-voita.github.io\/resources\/lectures\/seq2seq\/transformer\/layer_norm-min.png)\n\nIt independently normalizes vector representation of each example in batch - this is done to control \"flow\" to the next layer. Layer normalization improves convergence stability and sometimes even quality.\n\n**LayerNorm** has trainable parameters, \nscale\nand \nbias,\nwhich are used after normalization to rescale layer's outputs (or the next layer's inputs). Note that \n\u03bc\nk\n and \n\u03c3\nk\nare evaluated for each example, but \nscale\nand \nbias,\nare the same - these are layer parameters.","32a3355c":"# Attention in Sequence-to-Sequence Model\n\n> ### The Problem of Fixed Encoder Representation\n\nThe encoder of seq2seq model compressed the whole 'source sentence' into a single vector (last hidden state) as the context vector for the decoder. When the encoder is forced to put all information into a single vector, it is likely to forget something. Hence, the context vector turned out to be a bottleneck for these types of standard seq2seq models.\n\nThe encoder (RNN\/LSTM) of the seq2seq model is generally unable to accurately process long input sequences\/ large dependencies.\n\nOn the other hand, the Attention Mechanism directly addresses this issue as it retains and utilises 'all the hidden states of the input(source) sequence' during the decoding process.\n\nIt creates a unique mapping between the decoder output to all the encoder hidden states at each time step. For each output that the decoder makes, it has access to the entire input sequence and can selectively pick out specific elements from that sequence to produce the output.\n\n[\ud83d\udcd6 READ Floydhub Blog ](https:\/\/blog.floydhub.com\/attention-mechanism\/#luong-att)\n\nLet's explore:\n- Bahdanau Attention\n- Luong Attention","181b5b0a":"# Attention Mechanisms","0592e3fc":"![Attention Mechanisms](https:\/\/blog.floydhub.com\/content\/images\/2019\/09\/Slide41-1.JPG)\nSource: Floydhub Blog","9d13f4df":"### Multi-head Attention\n\nIf the Value vector of other tokens dominates the actual token, where the actual token is ambigous, then the dominance is useful. Otherwise, it may cause misunderstanding the right representation of the token. So, instead of computing single attention, we compute multiple attention weights over respective multiple heads and concatanate the final vectors.\n\nThus, multi-head attention will be more accurate.\n\n![fig](https:\/\/jalammar.github.io\/images\/t\/transformer_attention_heads_weight_matrix_o.png)\n\n![](http:\/\/www.tensorflow.org\/images\/tutorials\/transformer\/multi_head_attention.png)\n\n### Figure: Multi-head Attention\n\n![](https:\/\/images.prismic.io\/peltarionv2\/ee0b5ada-211f-4d33-ba6b-91a2ace50714_self-attention_multihead.svg?auto=compress%2Cformat&rect=0%2C0%2C133%2C150&w=1980&h=2233)\n\nSource: [Lena Voita's Blog](https:\/\/lena-voita.github.io\/nlp_course\/seq2seq_and_attention.html#transformer_intro); [Jay Allamar's Blog](https:\/\/jalammar.github.io\/illustrated-transformer); www.peltarion.com ;www.tensorflow.org","fb3bf63f":"## Loss function\n\nAs we know, we compare two probability distributions with Cross-entropy \/KL Divergence. Cross entropy is loss function and choose Adam as optimizer.\n\nThe model produces the outputs one at a time, we can assume that the model is selecting the word with the highest probability from that probability distribution and throwing away the rest. That\u2019s one way to do it (called greedy decoding).","222dbf19":"## Define Optimizer and Loss function","0c33d7ea":"## Calculating Attention Weights in Luong Attention Mechanism\n\n![](https:\/\/blog.floydhub.com\/content\/images\/2019\/09\/Slide51.JPG)\n\nSource: Floydhub's Blog (www.blog.floydhub.com)","629f1ff8":"**Please visit the kernel [Text Classification with BERT](https:\/\/www.kaggle.com\/rizdelhi\/text-classification-with-bert)to understand about the Transformer Code and BERT.**","df6244eb":"#### Learn the position with Positional Encoding\n\nTo account for the order of the words\/tokens in the input sequence, the transformer adds a positional encoding to each input embedding token.These vectors follow a specific pattern that the model learns, which helps it determine the position of each word, or the distance between different words in the sequence. \n\n![](https:\/\/miro.medium.com\/max\/1572\/1*xCeAOFp17t-NcWWpF2k9Gw.png)\n\n#### Residuals\n\nEach layer has a feed-forward network block: two linear layers with ReLU non-linearity between them:\nand is followed by a layer-normalization step.ach layer has a feed-forward network block: two linear layers with ReLU non-linearity between them:\n\n![](https:\/\/lena-voita.github.io\/resources\/lectures\/seq2seq\/transformer\/ffn-min.png)\n\n\n<math xmlns=\"http:\/\/www.w3.org\/1998\/Math\/MathML\" display=\"block\">\n  <mi>Feed<\/mi>\n  <mi>Forward<\/mi>\n  <mi>Network<\/mi>\n  <mo stretchy=\"false\">(<\/mo>\n  <mi>x<\/mi>\n  <mo stretchy=\"false\">)<\/mo>\n  <mo>=<\/mo>\n  <mo data-mjx-texclass=\"OP\" movablelimits=\"true\">max<\/mo>\n  <mo stretchy=\"false\">(<\/mo>\n  <mn>0<\/mn>\n  <mo>,<\/mo>\n  <mi>x<\/mi>\n  <msub>\n    <mi>W<\/mi>\n    <mn>1<\/mn>\n  <\/msub>\n  <mo>+<\/mo>\n  <msub>\n    <mi>b<\/mi>\n    <mn>1<\/mn>\n  <\/msub>\n  <mo stretchy=\"false\">)<\/mo>\n  <msub>\n    <mi>W<\/mi>\n    <mn>2<\/mn>\n  <\/msub>\n  <mo>+<\/mo>\n  <msub>\n    <mi>b<\/mi>\n    <mn>2<\/mn>\n  <\/msub>\n  <mo>.<\/mo>\n<\/math>","18c95292":"### Seq2Seq (LSTM) Model without Pre-trained Embeddings","1d332314":"# Code: Sequence-to-Sequence NMT with Attention Mechanisms\n\n> source: https:\/\/github.com\/tensorflow\/addons\/blob\/master\/docs\/tutorials\/networks_seq2seq_nmt.ipynb\n\n#### Dataset provided by http:\/\/www.manythings.org\/anki\/\n\u200b\nThis dataset contains language translation pairs in the format:\n\u200b\nMay I borrow this book?    \u00bfPuedo tomar prestado este libro?\n\u200b\n#### After downloading the dataset, here are the steps to prepare the data\n1. Add a start and end token to each sentence\n2. Clean the sentences by removing special characters\n3. Create a Vocabulary with word index (mapping from word \u2192 id) and reverse word index (mapping from id \u2192 word)\n4. Pad each sentence to a maximum length. (Why? you need to fix the maximum length for the inputs to recurrent encoders)","8901c7a8":"## Calculating the Attention (weights) in Bahdanau Attention Mechanism\n\n![Bahdanau](https:\/\/blog.floydhub.com\/content\/images\/2019\/09\/Slide50.JPG)","773f5c28":"### Steps to calculate Bahdanau Attention Weights:\n\u200b\n1. Producing the Encoder Hidden States - Encoder produces hidden states of each element in the input(source) sequence\n\u200b\n2. Calculating Alignment Scores between the previous decoder hidden state and each of the encoder\u2019s hidden states are calculated (Note: The last encoder hidden state can be used as the first hidden state in the decoder)\n\u200b\n3. Softmaxing the Alignment Scores - the alignment scores for each encoder hidden state are combined and represented in a single vector and subsequently softmaxed\n\u200b\n4. Calculating the Context(Attention) Vector - the encoder hidden states and their respective alignment scores are multiplied to form the context vector\n\u200b\n5. Decoding the Output - the context vector is concatenated with the previous decoder output and fed into the Decoder RNN for that time step along with the previous decoder hidden state to produce a new output\n\n\nThe process (steps 2-5) repeats itself for each time step of the decoder until an token is produced or output is past the specified maximum length\n\n\n> ### The alignment scores for Bahdanau Attention are calculated using the hidden state produced by the decoder in the previous time step and the encoder outputs.","3f50a84d":"### Steps to calculate Luong Attention Weights:\n\n1. Producing the Encoder Hidden States - Encoder produces hidden states of each element in the input sequence\n2. Decoder RNN - the previous decoder hidden state and decoder output is passed through the Decoder RNN to generate a new hidden state for that time step\n3. Calculating Alignment Scores - using the new decoder hidden state and the encoder hidden states, alignment scores are calculated\n4. Softmaxing the Alignment Scores - the alignment scores for each encoder hidden state are combined and represented in a single vector and subsequently softmaxed\n5. Calculating the Context Vector - the encoder hidden states and their respective alignment scores are multiplied to form the context vector\n6. Producing the Final Output - the context vector is concatenated with the decoder hidden state generated in step 2 as passed through a fully connected layer to produce a new output\n\nThe process (steps 2-6) repeats itself for each time step of the decoder until an token is produced or output is past the specified maximum length","c92884ed":"### Train data","29b19903":"## Jotting all the components - Transformer Architecture\n\n![fig](https:\/\/www.tensorflow.org\/images\/tutorials\/transformer\/transformer.png)\n\nThe final linear layer is a fully connected neural network that projects the vector produced by the stack of decoders, into a much, much larger vector called a logits vector. The softmax layer then turns those scores into probabilities (all positive, all add up to 1.0). The cell with the highest probability is chosen, and the word associated with it is produced as the output for this time step.\n\n![predict](https:\/\/jalammar.github.io\/images\/t\/transformer_decoder_output_softmax.png)","662b39c8":"## Dynamic Embeddings - ELMo (Embeddings from Language Models)\n\n<img src=\"https:\/\/images.squarespace-cdn.com\/content\/v1\/5208f2f8e4b0f3bf53b73293\/1407293570153-DZJEI1WX3J5I2YK7CYAX\/ke17ZwdGBToddI8pDm48kHxZW8G9yQIB8WDax7xpbxxZw-zPPgdn4jUwVcJE1ZvWhcwhEtWJXoshNdA9f1qD7WhFDJnCzrBmtNAj8M71q_A25spwTDnl_gCvSDHYIQArOx3xS_zC7rpYg0txtq7IHQ\/Elmo_Blink.gif?format=2500w\">\n\n\nDeep contextual embeddings and sentence\/word vectors falls under dynamic embeddings. These embeddings are current SOTA implying that there is a need for robust Neural Network models.\n\n[\ud83d\udcd6 READ **Deep contextualized word representations** - Matthew E. Peters, Mark Neumann, Mohit Iyyer, Matt Gardner, Christopher Clark, Kenton Lee, Luke Zettlemoyer](https:\/\/arxiv.org\/abs\/1802.05365)\n\nBoth these papers are essentially important for their contributions to contextual deep embeddings.\n\nI  highly recommend to read these papers!!! These are really cool explanation of how ELMo was designed.\n\nELMo deep contextualized word embeddings are helpful in achieving state-of-the-art (SOTA) results in several NLP tasks. \n\n![Lena Voita's Blog](https:\/\/lena-voita.github.io\/resources\/lectures\/transfer\/elmo\/training-min.png)\n\n### Under the hood:\n\nThe architecture above uses a character-level convolutional neural network (CNN) to represent words of a text string into raw word vectors\nThese raw word vectors act as inputs to the first layer of biLM\nThe forward pass contains information about a certain word and the context (other words) before that word\nThe backward pass contains information about the word and the context after it\nThis pair of information, from the forward and backward pass, forms the intermediate word vectors\nThese intermediate word vectors are fed into the next layer of biLM\nThe final representation (ELMo) is the weighted sum of the raw word vectors and the 2 intermediate word vectors\n\nAs the input to the biLM is computed from characters rather than words, it captures the inner structure of the word. For example, the biLM will be able to figure out that terms like beauty and beautiful are related at some level without even looking at the context they often appear in. Sounds incredible!\n\n\n[\ud83d\udcd6 READ Analytical Vidya](https:\/\/www.analyticsvidhya.com\/blog\/2019\/03\/learn-to-use-elmo-to-extract-features-from-text\/)","08e62f3d":"## In this kernel, I am exploring as well as building the following models:\n\n1. Seq2Seq Model (Encoder-Decoder without Attention) - NMT\n2. Dynamic embeddings (Pre-trained word embeddings on Bidirectional LSTMs): ELMo\n3. Seq2Seq Models (Encoder-Decoder with Bahdanu, Luong Attention,etc)\n4. Neural Machine Translation Code for Seq2Seq model with attention on English-Spanish data set\n5. Transformer Architecture Explained\n\n**As always, I hope you find this 'notebook' useful.**\n\n[UPVOTES](https:\/\/www.kaggle.com\/rizdelhi\/quora-insincere-questions-part-4-bert) would be highly appreciated and great motivation.","2ef0f5e9":"#### Define a NMTDataset class with necessary functions to follow Step 1 to Step 4.\n\nThe call() will return:\n\n> train_dataset and val_dataset : tf.data.Dataset objects\n\n> inp_lang_tokenizer and targ_lang_tokenizer : tf.keras.preprocessing.text.Tokenizer objects","1fa9baea":"### Parameters","4aee4050":"## Visualising NMT - seq2seq2 model with attention mechanism\n![](https:\/\/jalammar.github.io\/images\/attention_tensor_dance.mp4)\n\nSource: Jay Alammar's Blog"}}