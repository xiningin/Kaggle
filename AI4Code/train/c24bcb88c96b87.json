{"cell_type":{"46ffaf32":"code","85065970":"code","98d0a710":"code","f1c1c1d9":"code","a05fccfd":"code","ca95f50b":"code","0c4b8179":"code","ab529cfd":"code","3d379084":"code","f6598494":"code","88eeac4f":"code","8694fefe":"code","1f10a757":"code","bd505dbb":"code","a36394a6":"code","30098aad":"code","c1c606a7":"code","2106348f":"code","2e2a95a3":"markdown","81c28f22":"markdown","1d8ef02d":"markdown","61d0f520":"markdown","9f4ef070":"markdown","31ec3eb9":"markdown","4096bee3":"markdown"},"source":{"46ffaf32":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport seaborn as sns\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.\n__print__ = print\ndef print(string):\n    os.system(f'echo \\\"{string}\\\"')\n    __print__(string)","85065970":"train = pd.read_csv('\/kaggle\/input\/jigsaw-toxic-comment-classification-challenge\/train.csv')\ntest = pd.read_csv('\/kaggle\/input\/jigsaw-toxic-comment-classification-challenge\/test.csv')\nprint('Imported train and test')","98d0a710":"train.head()","f1c1c1d9":"test.head()","a05fccfd":"train.isnull().any()","ca95f50b":"test.isnull().any()","0c4b8179":"x_train = train['comment_text']\ny_train = train[['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']]\nx_test = test['comment_text']","ab529cfd":"from keras.preprocessing.text import Tokenizer\n\ntokenizer = Tokenizer()\ntokenizer.fit_on_texts(x_train)\nprint('Fit tokenizer on texts')","3d379084":"x_tokenized_train = tokenizer.texts_to_sequences(x_train)\nx_tokenized_test = tokenizer.texts_to_sequences(x_test)\nprint('Converted x_train and x_test to tokenized form')","f6598494":"lengths = [len(comment) for comment in x_tokenized_train]\nprint(f'The longest comment is {max(lengths)} words long.')\nsns.distplot(lengths, kde=False)","88eeac4f":"from keras.preprocessing.sequence import pad_sequences\n\nmax_length = 200\nX_train = pad_sequences(x_tokenized_train, maxlen=max_length)\nX_test = pad_sequences(x_tokenized_test, maxlen=max_length)","8694fefe":"len(tokenizer.word_index)","1f10a757":"from keras.models import Sequential\nfrom keras.layers import Embedding, LSTM, GlobalAveragePooling1D, Dropout, Dense, LeakyReLU, Activation, GlobalMaxPool1D\nfrom keras import metrics\n\nnum_features, embed_size = len(tokenizer.word_index), 128\nmetric = ['accuracy']\n\nmodels = []\n\nmodel1 = Sequential()\nmodel1.add(Embedding(num_features + 1, embed_size, input_length=max_length))\nmodel1.add(LSTM(64, return_sequences=True))\nmodel1.add(GlobalAveragePooling1D())\nmodel1.add(Dropout(0.1))\nmodel1.add(Dense(32))\nmodel1.add(Activation('relu'))\nmodel1.add(Dropout(0.1))\nmodel1.add(Dense(16))\nmodel1.add(Activation('relu'))\nmodel1.add(Dropout(0.1))\nmodel1.add(Dense(6, activation='sigmoid'))\nmodel1.compile(loss='binary_crossentropy', optimizer='adam', metrics=metric)\n\nmodels.append(model1)\n\nmodel2 = Sequential()\nmodel2.add(Embedding(num_features + 1, embed_size, input_length=max_length))\nmodel2.add(LSTM(64, return_sequences=True))\nmodel2.add(GlobalMaxPool1D())\nmodel2.add(Dropout(0.1))\nmodel2.add(Dense(32))\nmodel2.add(Activation('relu'))\nmodel2.add(Dropout(0.1))\nmodel2.add(Dense(16))\nmodel2.add(Activation('relu'))\nmodel2.add(Dropout(0.1))\nmodel2.add(Dense(6, activation='sigmoid'))\nmodel2.compile(loss='binary_crossentropy', optimizer='adam', metrics=metric)\n\nmodels.append(model2)\n\nmodel3 = Sequential()\nmodel3.add(Embedding(num_features + 1, embed_size, input_length=max_length))\nmodel3.add(LSTM(64, return_sequences=True))\nmodel3.add(GlobalMaxPool1D())\nmodel3.add(Dropout(0.05))\nmodel3.add(Dense(6, activation='sigmoid'))\nmodel3.compile(loss='binary_crossentropy', optimizer='adam', metrics=metric)\n\nmodels.append(model3)\n\nmodel4 = Sequential()\nmodel4.add(Embedding(num_features + 1, embed_size, input_length=max_length))\nmodel4.add(LSTM(64, return_sequences=True))\nmodel4.add(GlobalMaxPool1D())\nmodel4.add(Dropout(0.1))\nmodel4.add(Dense(32, activation='relu'))\nmodel4.add(Dropout(0.05))\nmodel4.add(Dense(6, activation='sigmoid'))\nmodel4.compile(loss='binary_crossentropy', optimizer='adam', metrics=metric)\n\nmodels.append(model4)\n\ndel model1, model2, model3, model4\n\n# # 0.958\n# models[0].add(Embedding(num_features + 1, embed_size, input_length=max_length))\n# models[0].add(LSTM(64, return_sequences=True))\n# models[0].add(GlobalAveragePooling1D())\n# models[0].add(Dropout(0.1))\n# models[0].add(Dense(48))\n# models[0].add(LeakyReLU())\n# models[0].add(Dropout(0.1))\n# models[0].add(Dense(6, activation='sigmoid'))\n# models[0].compile(loss='binary_crossentropy', optimizer='adam', metrics=metric)\n\n# # 0.961\n# models[1].add(Embedding(num_features + 1, embed_size, input_length=max_length))\n# models[1].add(LSTM(64, return_sequences=True))\n# models[1].add(GlobalAveragePooling1D())\n# models[1].add(Dropout(0.1))\n# models[1].add(Dense(48))\n# models[1].add(Activation('relu'))\n# models[1].add(Dropout(0.1))\n# models[1].add(Dense(6, activation='sigmoid'))\n# models[1].compile(loss='binary_crossentropy', optimizer='adam', metrics=metric)\n\n# # 0.962\n# models[2].add(Embedding(num_features + 1, embed_size, input_length=max_length))\n# models[2].add(LSTM(64, return_sequences=True))\n# models[2].add(GlobalAveragePooling1D())\n# models[2].add(Dropout(0.1))\n# models[2].add(Dense(32))\n# models[2].add(Activation('relu'))\n# models[2].add(Dropout(0.1))\n# models[2].add(Dense(16))\n# models[2].add(Activation('relu'))\n# models[2].add(Dropout(0.1))\n# models[2].add(Dense(6, activation='sigmoid'))\n# models[2].compile(loss='binary_crossentropy', optimizer='adam', metrics=metric)\n\nprint('Created models')\nmodels","bd505dbb":"import time\n\nbatch_size = 16\nvalidation_split = 0.1\nepochs = 3\n\nhistories = []\n\nfor i, model in enumerate(models):\n    print(f'Beginning to fit model {i}')\n    start_time = time.time()\n    history = model.fit(X_train, y_train,\n                        validation_split=validation_split,\n                        batch_size=batch_size,\n                        epochs=epochs)\n    histories.append(history)\n    end_time = time.time()\n    print(f'Fit model {i} in {end_time - start_time} seconds.')","a36394a6":"from IPython.display import display\nfor i, history in enumerate(histories):\n    print(f'Model {i}')\n    h = pd.DataFrame(history.history)\n    h.index.name = 'epoch'\n    display(h)","30098aad":"y_preds = []\n\nfor i, model in enumerate(models):\n    print(f'Started predicting for model {i}')\n    y_pred = model.predict(X_test, batch_size=4096)\n    y_preds.append(y_pred)\n    print(f'Predicted stuff for model {i}')","c1c606a7":"y = []\nfor i, model in enumerate(models):\n    y_i = pd.DataFrame(data=y_preds[i],\n                        columns=['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate'])\n    y_i = pd.concat([test['id'], y_i], axis=1)\n    y.append(y_i)\n# y = pd.DataFrame(data=y_pred, columns=['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate'])\n# y = pd.concat([test['id'], y], axis=1)\ny","2106348f":"for i, y_i in enumerate(y):\n    filename = f'submision_{i}.csv'\n    y_i.to_csv(filename, index=False)\n    print(f'Created file {filename}')\n# y.to_csv('submission.csv', index=False)","2e2a95a3":"# Checking for nulls","81c28f22":"# Training the model","1d8ef02d":"# Get x and y","61d0f520":"# Pad our sequences","9f4ef070":"# Get the submission","31ec3eb9":"# Tokenize our words","4096bee3":"# Building the model"}}