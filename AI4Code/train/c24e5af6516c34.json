{"cell_type":{"7be0702a":"code","6a78755c":"code","1136df92":"code","a1bccf30":"code","ad230c27":"code","79ab4242":"code","b07c826b":"code","31d123f3":"code","def91fd8":"code","0facf356":"code","6299646f":"code","d7c58694":"code","80b46951":"code","aec0d2fd":"code","1d36695c":"code","3eb82ab8":"code","44488475":"code","9f9cf4e8":"code","a7fcc4a4":"code","742a8f36":"code","e94a4d4a":"code","0be6a5d6":"code","eb191382":"code","ee9af542":"code","50ba101f":"code","cd10f7b7":"code","04c25117":"code","16079f7d":"code","ea7fdf9d":"code","2964cb98":"code","791ab328":"code","81d572c2":"code","1426d2ab":"code","ae21eb34":"code","5e96006c":"code","a3b68b0c":"code","1e668383":"code","97234fd2":"code","2c929a60":"code","9883f03e":"code","7a30164e":"code","3924e937":"code","ee04d231":"code","6c5957bd":"code","25c4f246":"code","b24c6891":"code","1b214d14":"code","5ad100d6":"code","7affd5eb":"code","27ad45be":"code","3f49af47":"code","8a8a007f":"code","40a10824":"code","8f4513d1":"code","e5785be9":"code","978c5014":"code","83c9a83c":"code","9ce50e0e":"code","a959a0f2":"code","50332a9d":"markdown","7ee5c2e9":"markdown","3da48337":"markdown","a03e9120":"markdown","e19db70e":"markdown","3b845aa7":"markdown","2f42c6c8":"markdown","1e58b3c9":"markdown","674d61c4":"markdown","99d9da4a":"markdown","37943e82":"markdown","34c88ee5":"markdown","7ed1c5f6":"markdown","085f208d":"markdown","869c801d":"markdown","a6a97eca":"markdown","ffc217a3":"markdown","159433f1":"markdown","44edfd41":"markdown","c40643e1":"markdown","74e560d9":"markdown","8c823cb9":"markdown","3570ed03":"markdown","c1598188":"markdown","52d60fff":"markdown","405599b4":"markdown","d0c23abb":"markdown","4608f658":"markdown","8d91e247":"markdown","9b7a8ba0":"markdown","4c0cf6ac":"markdown","1b21e50f":"markdown","8f933c9a":"markdown","41df609b":"markdown","d8247791":"markdown"},"source":{"7be0702a":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","6a78755c":"import warnings\nimport gc\nimport pandas as pd\nimport seaborn as sns\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nfrom sklearn.model_selection import KFold, cross_val_score\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.metrics import mean_squared_error\n\nfrom catboost import Pool, CatBoostRegressor\nimport catboost\nwarnings.filterwarnings('ignore')\nget_ipython().run_line_magic('matplotlib', 'inline')","1136df92":"df_train = pd.read_csv('\/kaggle\/input\/house-prices-advanced-regression-techniques\/train.csv')","a1bccf30":"corr_total = df_train.corr()","ad230c27":"len(df_train.columns)","79ab4242":"len(corr_total)","b07c826b":"f, ax = plt.subplots(figsize=(12, 9))\nsns.heatmap(corr_total, vmax=1, square=True)","31d123f3":"corr_saleprice = df_train.corrwith(df_train.SalePrice)","def91fd8":"corr_saleprice = corr_saleprice.to_frame()\ncorr_saleprice.columns = ['corr_with_saleprice']\ncorr_saleprice.sort_values('corr_with_saleprice', ascending=False, inplace=True)","0facf356":"corr_saleprice[1:11]","6299646f":"#a. what kind of values are there?\ndf_train['OverallQual'].unique()","d7c58694":"#b. distribution plot\nsns.distplot(df_train['OverallQual'])","80b46951":"#a. what kind of values are there?\ndf_train['GrLivArea'].unique()","aec0d2fd":"#b. distribution plot\nsns.distplot(df_train['GrLivArea'])","1d36695c":"df_train['GrLivArea'] = np.log(df_train['GrLivArea'])","3eb82ab8":"sns.distplot(df_train['GrLivArea'])","44488475":"# Kernel doesn't support sns.displot!\n\n#c. distribution plot with SalePrice\n#sns.displot(data=df_train, x='GrLivArea', y='SalePrice')","9f9cf4e8":"df_train[df_train['GrLivArea'] > 8.25][df_train['SalePrice'] < 200000]","a7fcc4a4":"df_train = df_train.drop(df_train[df_train['GrLivArea'] > 8.25][df_train['SalePrice'] < 200000].index)","742a8f36":"# To check whether it's gone\ndf_train[df_train['GrLivArea'] > 8.25][df_train['SalePrice'] < 200000]","e94a4d4a":"#sns.displot(data=df_train, x='GrLivArea', y='SalePrice')","0be6a5d6":"#a. what kind of values are there?\ndf_train['GarageArea'].unique()","eb191382":"#b. distribution plot\nsns.distplot(df_train['GarageArea'])","ee9af542":"#c. distribution plot with SalePrice\n#sns.displot(data=df_train, x='GarageArea', y='SalePrice')","50ba101f":"df_train[df_train['GarageArea'] > 1200]['SalePrice']","cd10f7b7":"df_train = df_train.drop(df_train[df_train['GarageArea'] > 1200][df_train['SalePrice'] < 260000].index)","04c25117":"#c. distribution plot with SalePrice\n#sns.displot(data=df_train, x='GarageArea', y='SalePrice')","16079f7d":"#a. what kind of values are there?\ndf_train['TotalBsmtSF'].unique()","ea7fdf9d":"#b. distribution plot\nsns.distplot(df_train['TotalBsmtSF'])","2964cb98":"#c. distribution plot with SalePrice\n#sns.displot(data=df_train, x='TotalBsmtSF', y='SalePrice')","791ab328":"#a. what kind of values are there?\ndf_train['FullBath'].unique()","81d572c2":"#b. distribution plot\nsns.distplot(df_train['FullBath'])","1426d2ab":"#c. distribution plot with SalePrice\n#sns.displot(data=df_train, x='FullBath', y='SalePrice')","ae21eb34":"#a. what kind of values are there?\ndf_train['TotRmsAbvGrd'].unique()","5e96006c":"#b. distribution plot\nsns.distplot(df_train['TotRmsAbvGrd'])","a3b68b0c":"df_train['TotRmsAbvGrd'] = np.log(df_train['TotRmsAbvGrd'])","1e668383":"sns.distplot(df_train['TotRmsAbvGrd'])","97234fd2":"#c. distribution plot with SalePrice\n#sns.displot(data=df_train, x='TotRmsAbvGrd', y='SalePrice')","2c929a60":"#a. what kind of values are there?\ndf_train['YearBuilt'].unique()","9883f03e":"#b. distribution plot\nsns.distplot(df_train['YearBuilt'])","7a30164e":"#c. distribution plot with SalePrice\n#sns.displot(data=df_train, x='YearBuilt', y='SalePrice')","3924e937":"#a. what kind of values are there?\ndf_train['YearRemodAdd'].unique()","ee04d231":"#b. distribution plot\nsns.distplot(df_train['YearRemodAdd'])","6c5957bd":"#c. distribution plot with SalePrice\n#sns.displot(data=df_train, x='YearRemodAdd', y='SalePrice')","25c4f246":"feature_list = ['OverallQual', 'GrLivArea', 'GarageArea', 'TotalBsmtSF', 'FullBath', 'TotRmsAbvGrd', 'YearBuilt', 'YearRemodAdd']","b24c6891":"for feature in feature_list:\n    print('Current feature : {}'.format(feature))\n    print(df_train[feature].isnull().sum())","1b214d14":"y_train = df_train['SalePrice'].values\nX_train = df_train[feature_list].values","5ad100d6":"from sklearn.preprocessing import RobustScaler\nX_train = RobustScaler().fit_transform(X_train)","7affd5eb":"N_FOLDS = 5 \nkf = KFold(n_splits = N_FOLDS, shuffle=True, random_state=42)\nkf.get_n_splits(X_train)","27ad45be":"df_test = pd.read_csv('\/kaggle\/input\/house-prices-advanced-regression-techniques\/test.csv')","3f49af47":"X_test = df_test[feature_list].values","8a8a007f":"X_test = RobustScaler().fit_transform(X_test)","40a10824":"from catboost import Pool, CatBoostRegressor\nimport catboost\nfrom sklearn.metrics import mean_squared_error\n\npred_final = np.zeros(X_test.shape[0])","8f4513d1":"for i, (train_index, valid_index) in enumerate(kf.split(X_train)):\n    print('{}th cross validation'.format(i+1))\n    train_x_seq = X_train[train_index]\n    train_y_seq = y_train[train_index]\n    valid_x_seq = X_train[valid_index]\n    valid_y_seq = y_train[valid_index]\n    \"\"\n    train_pool = Pool(data=train_x_seq, label=train_y_seq)\n    valid_pool = Pool(data=valid_x_seq)\n    test_pool = Pool(data=X_test)\n\n    model = CatBoostRegressor(iterations=10,\n                              loss_function='RMSE',\n                              verbose=False)\n    model.fit(train_pool)\n    \n    pred = model.predict(valid_pool)\n    \n    rmse_score = np.sqrt(mean_squared_error(valid_y_seq, pred))\n    print('rmse_score : {}'.format(rmse_score))\n    \n    pred_final += model.predict(test_pool) \/ N_FOLDS","e5785be9":"pred_final.shape, X_test.shape","978c5014":"parameters = {'depth'         : [2, 4, 6, 8, 10],\n              'learning_rate' : [0.01, 0.05, 0.1],\n              'iterations'    : [10, 30, 50, 100, 200, 500],\n              'loss_function' : ['RMSE']\n             } ","83c9a83c":"from sklearn.model_selection import GridSearchCV\n\nmodel = CatBoostRegressor()\ngrid = GridSearchCV(estimator=model, param_grid=parameters, cv=5, n_jobs=-1)\ngrid.fit(X_train, y_train)\n\nprint(\" Results from Grid Search \" )\nprint(\"\\n The best estimator across ALL searched params:\\n\", grid.best_estimator_)\nprint(\"\\n The best score across ALL searched params:\\n\", grid.best_score_)\nprint(\"\\n The best parameters across ALL searched params:\\n\", grid.best_params_)","9ce50e0e":"pred_final = np.zeros(X_test.shape[0])\n\nfor i, (train_index, valid_index) in enumerate(kf.split(X_train)):\n    print('{}th cross validation'.format(i+1))\n    train_x_seq = X_train[train_index]\n    train_y_seq = y_train[train_index]\n    valid_x_seq = X_train[valid_index]\n    valid_y_seq = y_train[valid_index]\n    \"\"\n    train_pool = Pool(data=train_x_seq, label=train_y_seq)\n    valid_pool = Pool(data=valid_x_seq)\n    test_pool = Pool(data=X_test)\n\n    model = CatBoostRegressor(iterations=200,\n                              depth=6,\n                              learning_rate=0.05,\n                              loss_function='RMSE',\n                              verbose=False)\n    model.fit(train_pool)\n    \n    pred = model.predict(valid_pool)\n    \n    rmse_score = np.sqrt(mean_squared_error(valid_y_seq, pred))\n    print('rmse_score : {}'.format(rmse_score))\n    \n    pred_final += model.predict(test_pool) \/ N_FOLDS\n\nsub = pd.DataFrame()\nsub['Id'] = df_test['Id'].values\nsub['SalePrice'] = pred_final\nsub.to_csv('submission.csv', index=False)","a959a0f2":"sub","50332a9d":"Reference : https:\/\/www.kaggle.com\/pmarcelino\/comprehensive-data-exploration-with-python","7ee5c2e9":"# 1. EDA - What are the most important features?","3da48337":"I think this feature is positive-skewed. We can massage this feature by taking log feature.\n","a03e9120":"### 3) GarageArea","e19db70e":"TotRmsAbvGrd: Total rooms above grade (does not include bathrooms)\n","3b845aa7":"### 5) FullBath","2f42c6c8":"### 8) YearRemodAdd","1e58b3c9":"## 1. Null Value Handling\n","674d61c4":"OverallQual: Overall material and finish quality","99d9da4a":"When we look at correlation distributions with SalePrice, OverallQual, GrLivArea and etc seems positively correlated with SalePrice. Let's check it more.","37943e82":"### 4) TotalBsmtSF","34c88ee5":"Let's cut out values under 260000.\n","7ed1c5f6":"# 2. Preprocessing","085f208d":"## 2. Checking each feature and outliers","869c801d":"### 1) OverallQual","a6a97eca":"## 2. Standardization","ffc217a3":"It seems there're outliers around GrLivArea == 8.5. Let's remove those outliers.\n","159433f1":"Now we have X_train, y_train, X_test\n","44edfd41":"GrLivArea: Above grade (ground) living area square feet\n","c40643e1":"### 2) GrLivArea","74e560d9":"Let's check top 10 highly correlated with SalePrice.\n","8c823cb9":"## 3. Parameter Tuning","3570ed03":"## 1. Train\/Valid Dataset Split","c1598188":"We're finally decided to use features below:  \nOverallQual, GrLivArea, GarageArea, TotalBsmtSF, FullBath, TotRmsAbvGrd, YearBuilt, YearRemodAdd\n\nWe'll first make model with numerical feature first and add categorical features and look if there's an improvement on scores.\n","52d60fff":"I think we can also handle outliers at area around GarageArea 1200~1400!\n","405599b4":"FullBath: Full bathrooms above grade\n","d0c23abb":"GarageCars and GarageArea is almost same feature. I would like to use GarageArea beccause it has much more values that GarageCars so it would help making regression model better.\n","4608f658":"YearRemodAdd: Remodel date\n","8d91e247":"We can figure out that there're 38 numerical features and 43 cateogorical features.  \nSince this task is regression, we have to focus on numerical features.","9b7a8ba0":"## 1. Correlation maps with SalePrice!","4c0cf6ac":"### 6) TotRmsAbvGrd","1b21e50f":"TotalBsmtSF: Total square feet of basement area\n","8f933c9a":"# 3. Modeling","41df609b":"## 2. Baseline Training","d8247791":"### 7) YearBuilt"}}