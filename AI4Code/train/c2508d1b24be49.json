{"cell_type":{"838d70e1":"code","e8b6767b":"code","fb5e99e4":"code","fc9c9478":"code","41e06732":"code","56f457ee":"code","f996a44a":"code","2301574b":"code","98f0279a":"code","3970119f":"code","9cddb965":"code","6253d293":"code","b6d37dad":"code","3514a0d5":"code","d9fbf344":"code","35a79860":"code","9b57daa3":"code","145099a2":"code","f296912d":"code","9ffe2bec":"code","dbc48cb0":"code","d9e406e9":"code","95dede25":"code","27ce065e":"code","fc040bad":"code","739b6824":"code","513e0aa6":"code","5ac46481":"code","33f51d55":"code","85c199ab":"code","18615a33":"code","18ed220d":"code","72c351b8":"code","c90462bb":"code","96609d8f":"code","40bead5c":"code","613daa99":"code","1b16a444":"code","6662282c":"code","f1c7dc31":"code","68b5bb15":"code","708f1764":"code","79ab5bc9":"code","60f5d579":"code","a33f9f6d":"code","3de7a87d":"code","da97f350":"code","bc4ad813":"code","ebb76efb":"code","7138837c":"code","d4c87e63":"code","0be678e2":"code","5fb462d4":"code","9a4bc21d":"code","81cc7f77":"code","1a02da8a":"code","e188cb5d":"code","9c9d106c":"markdown","16290297":"markdown","cc2168f6":"markdown","a5dd6898":"markdown","61fdfda1":"markdown","dd6375a3":"markdown","4a09bce2":"markdown","340add41":"markdown","5cb6388c":"markdown","51f2c199":"markdown","31f9a51d":"markdown","562f743b":"markdown","cf9a8cf2":"markdown","4a038718":"markdown","bfae39fa":"markdown","9cd23d56":"markdown","e034e731":"markdown","104a1bb1":"markdown","22fa8233":"markdown","3021a7f1":"markdown","c8b02e14":"markdown","3a0b0335":"markdown","3f3031f6":"markdown","af77e9fb":"markdown","1dc0a109":"markdown","71a89a37":"markdown","fc4d00ba":"markdown"},"source":{"838d70e1":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport scipy as sp\nimport warnings\nimport os \nwarnings.filterwarnings(\"ignore\")\nimport datetime\n","e8b6767b":"data=pd.read_csv('\/kaggle\/input\/breast-cancer-wisconsin-data\/data.csv')\n","fb5e99e4":"data.head()      #displaying the head of dataset they gives the 1st to 5 rows of the data","fc9c9478":"data.describe()      #description of dataset ","41e06732":"data.info()","56f457ee":"data.shape       #569 rows and 33 columns","f996a44a":"data.columns     #displaying the columns of dataset","2301574b":"data.value_counts","98f0279a":"data.dtypes","3970119f":"data.isnull().sum()","9cddb965":"data.drop('Unnamed: 32', axis = 1, inplace = True)\n","6253d293":"data","b6d37dad":"data.corr()","3514a0d5":"plt.figure(figsize=(18,9))\nsns.heatmap(data.corr(),annot = True, cmap =\"Accent_r\")\n\n\n\n","d9fbf344":"sns.barplot(x=\"id\", y=\"diagnosis\",data=data[160:190])\nplt.title(\"Id vs Diagnosis\",fontsize=15)\nplt.xlabel(\"Id\")\nplt.ylabel(\"Diagonis\")\nplt.show()\nplt.style.use(\"ggplot\")\n","35a79860":"sns.barplot(x=\"radius_mean\", y=\"texture_mean\", data=data[170:180])\nplt.title(\"Radius Mean vs Texture Mean\",fontsize=15)\nplt.xlabel(\"Radius Mean\")\nplt.ylabel(\"Texture Mean\")\nplt.show()\nplt.style.use(\"ggplot\")\n","9b57daa3":" \nmean_col = ['diagnosis','radius_mean', 'texture_mean', 'perimeter_mean',\n       'area_mean', 'smoothness_mean', 'compactness_mean', 'concavity_mean',\n       'concave points_mean', 'symmetry_mean', 'fractal_dimension_mean']\n\nsns.pairplot(data[mean_col],hue = 'diagnosis', palette='Accent')\n","145099a2":"sns.violinplot(x=\"smoothness_mean\",y=\"perimeter_mean\",data=data)","f296912d":"plt.figure(figsize=(14,7))\nsns.lineplot(x = \"concavity_mean\",y = \"concave points_mean\",data = data[0:400], color='green')\nplt.title(\"Concavity Mean vs Concave Mean\")\nplt.xlabel(\"Concavity Mean\")\nplt.ylabel(\"Concave Points\")\nplt.show()\n\n","9ffe2bec":"worst_col = ['diagnosis','radius_worst', 'texture_worst',\n       'perimeter_worst', 'area_worst', 'smoothness_worst',\n       'compactness_worst', 'concavity_worst', 'concave points_worst',\n       'symmetry_worst', 'fractal_dimension_worst']\n\nsns.pairplot(data[worst_col],hue = 'diagnosis', palette=\"CMRmap\")","dbc48cb0":"# Getting Features\n\nx = data.drop(columns = 'diagnosis')\n\n# Getting Predicting Value\ny = data['diagnosis']\n","d9e406e9":"\n#train_test_splitting of the dataset\nfrom sklearn.model_selection import train_test_split \nx_train,x_test,y_train,y_test = train_test_split(x,y,test_size=0.2,random_state=0)\n","95dede25":"print(len(x_train))\n","27ce065e":"print(len(x_test))","fc040bad":"print(len(y_train))","739b6824":"print(len(y_test))","513e0aa6":"from sklearn.linear_model import LogisticRegression\nreg = LogisticRegression()\nreg.fit(x_train,y_train)                         \n","5ac46481":"y_pred=reg.predict(x_test)\nfrom sklearn.metrics import accuracy_score,classification_report,confusion_matrix,r2_score\nprint(classification_report(y_test,y_pred))\nprint(confusion_matrix(y_test,y_pred))\nprint(\"Training Score: \",reg.score(x_train,y_train)*100)\n\n\n","33f51d55":"data = pd.DataFrame({'Actual': y_test, 'Predicted': y_pred})\ndata\n\n\n\n\n","85c199ab":"print(accuracy_score(y_test,y_pred)*100)","18615a33":"from sklearn.model_selection import GridSearchCV\nparam = {\n         'penalty':['l1','l2'],\n         'C':[0.001, 0.01, 0.1, 1, 10, 20,100, 1000]\n}\nlr= LogisticRegression(penalty='l1')\ncv=GridSearchCV(reg,param,cv=5,n_jobs=-1)\ncv.fit(x_train,y_train)\ncv.predict(x_test)\n","18ed220d":"print(\"Best CV score\", cv.best_score_*100)","72c351b8":"from sklearn.tree import DecisionTreeClassifier\ndtree = DecisionTreeClassifier(max_depth=6, random_state=123)\n\ndtree.fit(x_train,y_train)\n\n#y_pred = dtree.predict(x_test)\n","c90462bb":"y_pred=dtree.predict(x_test)\nfrom sklearn.metrics import classification_report,confusion_matrix,accuracy_score,mean_squared_error\nprint(classification_report(y_test,y_pred))\nprint(confusion_matrix(y_test,y_pred))\nprint(\"Training Score: \",dtree.score(x_train,y_train)*100)\n\n","96609d8f":"print(accuracy_score(y_test,y_pred)*100)","40bead5c":"from sklearn.ensemble import RandomForestClassifier\nrfc=RandomForestClassifier()\nrfc.fit(x_train,y_train)\n\n","613daa99":"y_pred=rfc.predict(x_test)\nfrom sklearn.metrics import classification_report,confusion_matrix,accuracy_score,mean_squared_error\nprint(classification_report(y_test,y_pred))\nprint(confusion_matrix(y_test,y_pred))\nprint(\"Training Score: \",rfc.score(x_train,y_train)*100)\n","1b16a444":"print(accuracy_score(y_test,y_pred)*100)","6662282c":"from sklearn.neighbors import KNeighborsClassifier\nknn=KNeighborsClassifier(n_neighbors=7)\n\nknn.fit(x_train,y_train)\n","f1c7dc31":"y_pred=knn.predict(x_test)\nfrom sklearn.metrics import classification_report,confusion_matrix,accuracy_score,mean_squared_error,r2_score\nprint(classification_report(y_test,y_pred))\nprint(confusion_matrix(y_test,y_pred))\nprint(\"Training Score: \",knn.score(x_train,y_train)*100)\nprint(knn.score(x_test,y_test))\n","68b5bb15":"print(accuracy_score(y_test,y_pred)*100)\n","708f1764":"from sklearn.svm import SVC\n\nsvc = SVC()\nsvc.fit(x_train, y_train)\n","79ab5bc9":"y_pred=svc.predict(x_test)\nfrom sklearn.metrics import classification_report,confusion_matrix,accuracy_score,mean_squared_error,r2_score\nprint(classification_report(y_test,y_pred))\nprint(confusion_matrix(y_test,y_pred))\nprint(\"Training Score: \",svc.score(x_train,y_train)*100)\nprint(svc.score(x_test,y_test))\n","60f5d579":"print(\"Training Score: \",svc.score(x_train,y_train)*100)","a33f9f6d":"from sklearn.ensemble import AdaBoostClassifier\nadb = AdaBoostClassifier(base_estimator = None)\nadb.fit(x_train,y_train)\n\n\n\n\n\n","3de7a87d":"y_pred=adb.predict(x_test)\nfrom sklearn.metrics import classification_report,confusion_matrix,accuracy_score,mean_squared_error,r2_score\nprint(classification_report(y_test,y_pred))\nprint(confusion_matrix(y_test,y_pred))\nprint(\"Training Score: \",adb.score(x_train,y_train)*100)","da97f350":"print(accuracy_score(y_test,y_pred)*100)","bc4ad813":"from sklearn.ensemble import GradientBoostingClassifier\ngbc=GradientBoostingClassifier()\ngbc.fit(x_train,y_train)\n","ebb76efb":"y_pred=gbc.predict(x_test)\nfrom sklearn.metrics import classification_report,confusion_matrix,accuracy_score,mean_squared_error,r2_score\nprint(classification_report(y_test,y_pred))\nprint(confusion_matrix(y_test,y_pred))\nprint(\"Training Score: \",gbc.score(x_train,y_train)*100)\nprint(gbc.score(x_test,y_test))\n","7138837c":"print(accuracy_score(y_test,y_pred)*100)","d4c87e63":"from xgboost import XGBClassifier\n\nxgb =XGBClassifier(objective ='reg:linear', colsample_bytree = 0.3, learning_rate = 0.1,\n                max_depth = 5, alpha = 10, n_estimators = 10)\n\nxgb.fit(x_train, y_train)\n","0be678e2":"y_pred=xgb.predict(x_test)\nfrom sklearn.metrics import classification_report,confusion_matrix,accuracy_score,mean_squared_error,r2_score\nprint(classification_report(y_test,y_pred))\nprint(confusion_matrix(y_test,y_pred))\nprint(\"Training Score: \",xgb.score(x_train,y_train)*100)\nprint(xgb.score(x_test,y_test))\n","5fb462d4":"print(\"Training Score: \",xgb.score(x_train,y_train)*100)","9a4bc21d":"data = pd.DataFrame({'Actual': y_test, 'Predicted': y_pred})\ndata","81cc7f77":"from sklearn.naive_bayes import GaussianNB\ngnb = GaussianNB()\ngnb.fit(x_train,y_train)","1a02da8a":"y_pred=gnb.predict(x_test)\nfrom sklearn.metrics import classification_report,confusion_matrix,accuracy_score,mean_squared_error,r2_score\nprint(classification_report(y_test,y_pred))\nprint(confusion_matrix(y_test,y_pred))\nprint(accuracy_score(y_test,y_pred))\nprint(\"Training Score: \",gnb.score(x_train,y_train)*100)\nprint(gnb.score(x_test,y_test))\n","e188cb5d":"data = pd.DataFrame({'Actual': y_test, 'Predicted': y_pred})\ndata","9c9d106c":"# 5. SVC","16290297":"# MODELS","cc2168f6":"# TRAINING AND TESTING DATA","a5dd6898":"**So we get a accuracy score of 96.49 % using Random Forest Classifier**","61fdfda1":"**So we get a accuracy score of 58.7 % using logistic regression**","dd6375a3":"#  7. Gradient Boosting Classifier","4a09bce2":"**So we get a accuracy score of 63.29 % using Naive Bayes**","340add41":"**So we get a accuracy score of 98.24 % using AdaBoostClassifier**","5cb6388c":"# VISUALIZING THE DATA","51f2c199":"**So we get a accuracy score of 95.61 % using GradientBoostingClassifier**","31f9a51d":"**So we get a accuracy score of 97.80 % using  XGBClassifier**","562f743b":"# 4. KNeighborsClassifier\n\n","cf9a8cf2":"# 6. AdaBoostClassifier","4a038718":"# 3. Random Forest Classifier","bfae39fa":"# 9. Naive Bayes","9cd23d56":"**So we get a accuracy score of 63.7 % using SVC**","e034e731":"# IMPORTING THE LIBRARIES","104a1bb1":"# 8. XGBClassifier","22fa8233":"**Ada Boost Classifier got the highest accuracy**","3021a7f1":"# 2. DECISION TREE CLASSIFIER","c8b02e14":"**So now we conclude the accuracy of different models:**\n\n**1. AdaBoost Classifier = 98.24 %**\n\n**2. XGB Classifier= 97.84 %**\n\n**3. Random Forest Classifier =96.57 %**\n\n**4. Gradient Boosting Classifier= 95.66%**\n\n**5. Decision Tree Classifier= 94.78 %**\n\n**6. K Neighbours Classifier= 70.18 %**\n\n**7. SVC = 63.80 %**\n\n**8. Naiye Bayes= 63.30 %**\n\n**9. Logistic Regression = 58.82%**\n","3a0b0335":"# If you liked this notebook, please UPVOTE it.","3f3031f6":"**So we get a accuracy score of 94.73 % using Decision Tree Classifier**","af77e9fb":"# LOADING THE DATASET","1dc0a109":"# 1. Logistic Regression","71a89a37":"**So we have to drop the Unnamed: 32 coulumn which contains NaN values**","fc4d00ba":"**So we get a accuracy score of 70.17 % using KNeighborsClassifier**"}}