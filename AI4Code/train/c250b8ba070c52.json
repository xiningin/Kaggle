{"cell_type":{"448cb3e9":"code","c61b04ae":"code","15e631b4":"code","73061b6f":"code","b17482c2":"code","1a3eb523":"code","b620932c":"code","97c435ec":"code","b13a904d":"code","891fb5d0":"code","e6534ae2":"code","8417a71b":"code","46f55f8a":"code","01bbbb23":"code","14bf29cb":"code","27e057f6":"code","9395ea29":"code","bb88532d":"code","583167d1":"code","615e2930":"code","2d8e42ee":"code","d0a07a59":"code","bdf8fff7":"code","8b793b1c":"code","94d87b46":"code","10983841":"code","419c4c7b":"code","1d106b93":"code","9f92e0b6":"code","e24724ab":"code","648be01d":"code","20e58f57":"code","6e70c72f":"code","61d96c27":"code","d3745dd8":"code","ad31e923":"code","585ea218":"code","f990ec42":"code","7a978817":"code","2a34fae8":"code","2f978d48":"code","77a825d4":"code","5b5b124c":"code","4d4b0117":"code","f5f3548a":"code","58750ce6":"code","ff439778":"code","cb75f4b1":"code","ad97e556":"code","64e0abad":"code","3812f86c":"code","477efd64":"code","3561bf65":"code","9f0def97":"code","832c44d7":"code","d42633fe":"code","f22092dc":"markdown","97e3bdcd":"markdown","871d5183":"markdown","e953b4d8":"markdown","b1e78fe0":"markdown","b187a72c":"markdown","a444865e":"markdown","2196acd5":"markdown","181cc67c":"markdown","347a1135":"markdown","494fc57b":"markdown","450e20ed":"markdown","591e3c6f":"markdown","67d9990d":"markdown","13a7c846":"markdown","88b414ce":"markdown","9c9fd87c":"markdown","757b4aef":"markdown","2ed97ea2":"markdown","bb82cc51":"markdown","3433b517":"markdown","0f49b96d":"markdown","9a72f448":"markdown","1517b4a5":"markdown","ef235692":"markdown","4e32cadc":"markdown","4dba3107":"markdown","cfa604cb":"markdown","719daad6":"markdown","3b6fb5c4":"markdown","2a19d933":"markdown","a786bf9d":"markdown","aff04b7f":"markdown","b9c967a6":"markdown","1d4c7313":"markdown","fb7cc078":"markdown","3b33ccea":"markdown","9dbb4611":"markdown","a0ddaa94":"markdown"},"source":{"448cb3e9":"import pandas as pd\nimport nltk, gensim, re, math\nimport en_core_web_sm\nimport matplotlib.ticker\nimport pyLDAvis.gensim\nimport gensim.corpora as corpora\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.colors as mcolors\nfrom gensim.utils import lemmatize, simple_preprocess\nfrom gensim.models import CoherenceModel\nfrom collections import Counter\nfrom scipy.spatial.distance import cdist\nfrom sklearn.cluster import KMeans\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.ensemble import RandomForestClassifier\n\ndf = pd.read_csv('\/kaggle\/input\/superheroes-nlp-dataset\/superheroes_nlp_dataset.csv')","c61b04ae":"cluster_df = df.copy()\nprint(cluster_df.shape)\n","15e631b4":"cluster_df.describe(include = 'object')","73061b6f":"cluster_df['first_appearance'].head(n=20)","b17482c2":"cluster_df.describe(include = 'float')","1a3eb523":"cluster_df.describe(include = 'int')","b620932c":"cluster_df['height_clean'] = cluster_df.height.str.extract(r'(\\d+)\\s*cm', expand=True)\ncluster_df['weight_clean'] = cluster_df.weight.str.extract(r'(\\d+)\\s*kg', expand=True)\ncluster_df['height_clean'] = pd.to_numeric(cluster_df['height_clean'], errors='coerce')\ncluster_df['weight_clean'] = pd.to_numeric(cluster_df['weight_clean'], errors='coerce')\n\ncluster_df['height_clean'] = cluster_df['height_clean'].fillna(cluster_df['height_clean'].median())\ncluster_df['weight_clean'] = cluster_df['weight_clean'].fillna(cluster_df['weight_clean'].median())\ncluster_df['height_clean'] = cluster_df['height_clean'].astype('int')\ncluster_df['weight_clean'] = cluster_df['weight_clean'].astype('int')","97c435ec":"sns.pairplot(cluster_df.select_dtypes(include=['int']), plot_kws={'alpha': 0.1})","b13a904d":"sns.pairplot(cluster_df[(cluster_df['strength_score'] < 25) | (cluster_df['combat_score'] < 25)].select_dtypes(include=['int']), plot_kws={'alpha':0.1})","891fb5d0":"cluster_df[(cluster_df['height_clean'] > 500) & (cluster_df['weight_clean'] < 500)]","e6534ae2":"print(cluster_df.shape)\ncluster_df = cluster_df[cluster_df['name'] != \"Batman (1966)\"]\nprint(cluster_df.shape)","8417a71b":"cluster_df[cluster_df['weight_clean'] > 750]","46f55f8a":"cluster_df[(cluster_df['strength_score'] == 0) & (cluster_df['intelligence_score'] == 0)]","01bbbb23":"print(cluster_df.shape)\ncluster_df = cluster_df[(cluster_df['strength_score'] != 0) & (cluster_df['intelligence_score'] != 0)]\nprint(cluster_df.shape)","14bf29cb":"sns.pairplot(cluster_df.select_dtypes(include=['int']), plot_kws={'alpha': 0.1})","27e057f6":"cluster_df = pd.get_dummies(cluster_df, columns=['creator','alignment','gender'])","9395ea29":"cluster_df['type_race'].value_counts()","bb88532d":"plt.figure(figsize=(8,14))\nsns.countplot(y=cluster_df['type_race'], order = cluster_df['type_race'].value_counts().index)","583167d1":"cluster_df['type_race'] = cluster_df['type_race'].fillna('Unknown')","615e2930":"bad_cols = ['name','real_name','full_name','overall_score','history_text',\n            'powers_text','superpowers','alter_egos','aliases','place_of_birth',\n            'first_appearance','occupation','base','teams','relatives',\n            'height','weight','eye_color','hair_color', 'skin_color',\n            'img']\ncluster_df = cluster_df[[c for c in cluster_df.columns if c not in bad_cols]]","2d8e42ee":"power_cols = ['has_electrokinesis','has_energy_constructs','has_matter_manipulation', 'has_telepathy_resistance',\n            'has_mind_control','has_enhanced_hearing','has_dimensional_travel', 'has_element_control','has_size_changing',\n            'has_fire_resistance','has_fire_control','has_dexterity','has_reality_warping','has_illusions','has_energy_beams',\n            'has_peak_human_condition','has_shapeshifting','has_jump','has_self-sustenance','has_energy_absorption',\n            'has_cold_resistance','has_magic','has_telekinesis','has_toxin_and_disease_resistance','has_telepathy',\n            'has_regeneration','has_immortality','has_teleportation','has_force_fields','has_energy_manipulation',\n            'has_endurance','has_longevity','has_weapon-based_powers','has_energy_blasts', 'has_enhanced_senses','has_invulnerability',\n            'has_stealth','has_marksmanship','has_flight', 'has_accelerated_healing', 'has_weapons_master', 'has_intelligence', 'has_reflexes',\n            'has_super_speed','has_durability','has_stamina','has_agility','has_super_strength', 'has_heat_resistance',\n            'has_mind_control_resistance']\n\ncluster_df[power_cols] = cluster_df[power_cols].fillna(0)","d0a07a59":"print(cluster_df.isnull().any().sum())","bdf8fff7":"corrmat = cluster_df.corr()\nsns.heatmap(corrmat, vmax=0.9, square=True, center = 0, cmap = 'viridis')","8b793b1c":"power_corrmat = cluster_df[power_cols].corr()\nsns.heatmap(power_corrmat, vmax=0.9, square=True, center = 0, cmap = 'viridis')","94d87b46":"corrmat.abs().unstack().sort_values().drop_duplicates().sort_values(kind='quicksort', ascending=False).head(n=20)\n","10983841":"cluster_df.drop(columns=['alignment_Bad', 'gender_Female'])","419c4c7b":"score_cols = ['intelligence_score', 'strength_score', 'speed_score', 'durability_score', 'power_score', 'combat_score']\nsns.pairplot(cluster_df[score_cols], plot_kws={'alpha': 0.1})","1d106b93":"cluster_df['overall'] = cluster_df[score_cols].sum(axis=1) \/ 6\nsns.distplot(cluster_df['overall'], rug=True)\n","9f92e0b6":"score_cols.append('overall')\nsns.pairplot(cluster_df[score_cols], plot_kws={'alpha': 0.1})","e24724ab":"heroes = cluster_df.loc[cluster_df['alignment_Good'] == 1]\nvillians = cluster_df.loc[cluster_df['alignment_Bad'] == 1]\n\nsns.distplot(heroes['overall'], color = '#3498db') #blue\nsns.distplot(villians['overall'], color = '#e74c3c') #red","648be01d":"cluster_df = pd.get_dummies(cluster_df, columns=['type_race'])","20e58f57":"distortions = []\nK = range(1,20)\nfor k in K:\n    kmeanModel = KMeans(n_clusters=k).fit(cluster_df)\n    kmeanModel.fit(cluster_df)\n    distortions.append(sum(np.min(cdist(cluster_df, kmeanModel.cluster_centers_, 'euclidean'), axis=1)) \/ cluster_df.shape[0])\n\n# Plot the elbow\nplt.plot(K, distortions, 'bx-')\nplt.xlabel('k')\nplt.ylabel('Distortion')\nplt.title('The Elbow Method showing the optimal k')\nplt.show()","6e70c72f":"kmeans = KMeans(n_clusters=8)\nkmeans.fit(cluster_df)\npred = kmeans.predict(cluster_df)\ncluster_df['clust_pred'] = pred\nsns.scatterplot(cluster_df['overall'], cluster_df['strength_score'], hue = cluster_df['clust_pred'])","61d96c27":"X = cluster_df.copy()\nX.drop('clust_pred', axis=1, inplace=True)\ny = pred\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\nrf = RandomForestClassifier(max_depth=4, n_estimators = 2000)\nrf.fit(X_train, y_train)\n\ny_rf_pred = rf.predict(X_test)","d3745dd8":"importances = pd.Series(data=rf.feature_importances_,\n                        index= X_train.columns)\n\n# Sort importances\nimportances_sorted = importances.sort_values()\nimportances_sorted = importances_sorted[importances_sorted > 0.02]\n# Draw a horizontal barplot of importances_sorted\nimportances_sorted.plot(kind='barh', color='lightgreen')\nplt.title('Features Importances')\nplt.show()","ad31e923":"print(accuracy_score(y_test, y_rf_pred))","585ea218":"all_hist_text = \"\"\nall_pow_text = \"\"\n\nfor index, row in df.iterrows():\n    if not pd.isna(row['history_text']):\n        all_hist_text = all_hist_text + \" \" + row['history_text']\n    if not pd.isna(row['powers_text']):\n        all_pow_text = all_pow_text + \" \" + row['powers_text']\n     ","f990ec42":"from nltk.tokenize import RegexpTokenizer\nfrom nltk.corpus import stopwords\n\nstopwords = stopwords.words('english')\n\nhist_stops = ['time','x','man','would','later','however','also','men','could']\n\npow_stops = ['able','also','even','ability','one','use','without','could', 'power']\n\nhist_stopwords = stopwords + hist_stops\npow_stopwords = stopwords + pow_stops\n\ntokenizer = RegexpTokenizer(r'\\w+')\nhist_tokens = tokenizer.tokenize(all_hist_text.lower())\npow_tokens = tokenizer.tokenize(all_pow_text.lower())\n\nhist_tokens = [token for token in hist_tokens if token not in hist_stopwords]    \npow_tokens = [token for token in pow_tokens if token not in pow_stopwords] \n\nhist_freq_dist = nltk.FreqDist(hist_tokens)\nhist_freq_dist.plot(25)\n\npow_freq_dist = nltk.FreqDist(pow_tokens)\npow_freq_dist.plot(25)","7a978817":"def preprocess(text_list):\n    text_list = [text for text in text_list if type(text)==str]\n    for text in text_list:\n        #Common acronym that I don't want to lose\n        #lowercases, tokenizes, and deaccents, outputs tokens\n        text = re.sub(\"X-Men\", \"Xmen\", text)\n        text = simple_preprocess(str(text), deacc=True)   \n        yield text\n","2a34fae8":"def bis_n_tris(words, threshold = 75):\n    bigram = gensim.models.Phrases(words, min_count=2, threshold=threshold) # higher threshold fewer phrases.\n    trigram = gensim.models.Phrases(bigram[words], threshold=threshold)  \n    bigram_mod = gensim.models.phrases.Phraser(bigram)\n    trigram_mod = gensim.models.phrases.Phraser(trigram)\n    return bigram_mod, trigram_mod","2f978d48":"def process_words(texts, stopwords, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV'], threshold=100):\n    \"\"\"Remove Stopwords, Form Bigrams, Trigrams and Lemmatization\"\"\"\n    \n    bigram_mod, trigram_mod = bis_n_tris(texts, threshold = threshold)\n    \n    texts = [bigram_mod[doc] for doc in texts]\n    texts = [trigram_mod[bigram_mod[doc]] for doc in texts]\n    texts_out = []\n    nlp = en_core_web_sm.load()\n    for sent in texts:\n        doc = nlp(\" \".join(sent)) \n        texts_out.append([token.lemma_ for token in doc if token.pos_ in allowed_postags])\n    # remove stopwords once more after lemmatization\n    texts_out = [[word for word in simple_preprocess(str(doc)) if word not in stopwords] for doc in texts_out]    \n    return texts_out","77a825d4":"def create_model_and_corpus(words, num_topics):\n    id2word = corpora.Dictionary(words)\n    corpus = [id2word.doc2bow(text) for text in words]\n    lda_model = gensim.models.ldamodel.LdaModel(corpus=corpus,\n                                           id2word=id2word,\n                                           num_topics=num_topics, \n                                           random_state=100,\n                                           update_every=1,\n                                           chunksize=10,\n                                           passes=10,\n                                           alpha='symmetric',\n                                           iterations=100,\n                                           per_word_topics=True)\n    return lda_model, corpus","5b5b124c":"\ndef format_topics_sentences(corpus, texts, ldamodel=None):\n    # Init output\n    sent_topics_df = pd.DataFrame()\n\n    # Get main topic in each document\n    for i, row_list in enumerate(ldamodel[corpus]):\n        row = row_list[0] if ldamodel.per_word_topics else row_list            \n        # print(row)\n        row = sorted(row, key=lambda x: (x[1]), reverse=True)\n        # Get the Dominant topic, Perc Contribution and Keywords for each document\n        for j, (topic_num, prop_topic) in enumerate(row):\n            if j == 0:  # => dominant topic\n                wp = ldamodel.show_topic(topic_num)\n                topic_keywords = \", \".join([word for word, prop in wp])\n                sent_topics_df = sent_topics_df.append(pd.Series([int(topic_num), round(prop_topic,4), topic_keywords]), ignore_index=True)\n            else:\n                break\n    sent_topics_df.columns = ['Dominant_Topic', 'Perc_Contribution', 'Topic_Keywords']\n\n    # Add original text to the end of the output\n    contents = pd.Series(texts)\n    sent_topics_df = pd.concat([sent_topics_df, contents], axis=1)\n    dominant_topic_df = sent_topics_df.reset_index()\n    dominant_topic_df.columns = ['Document_No', 'Dominant_Topic', 'Topic_Perc_Contrib', 'Keywords', 'Text']\n    return sent_topics_df, dominant_topic_df","4d4b0117":"def doc_word_count_plot(df_dominant_topic):\n    doc_lens = [len(d) for d in df_dominant_topic.Text]\n    \n    # Plot\n    plt.figure(figsize=(16,7), dpi=160)\n    plt.hist(doc_lens, bins = 1000, color='navy')\n    plt.text(750, 100, \"Mean   : \" + str(round(np.mean(doc_lens))))\n    plt.text(750,  90, \"Median : \" + str(round(np.median(doc_lens))))\n    plt.text(750,  80, \"Stdev   : \" + str(round(np.std(doc_lens))))\n    plt.text(750,  70, \"1%ile    : \" + str(round(np.quantile(doc_lens, q=0.01))))\n    plt.text(750,  60, \"99%ile  : \" + str(round(np.quantile(doc_lens, q=0.99))))\n    \n    plt.gca().set(xlim=(0, 1000), ylabel='Number of Documents', xlabel='Document Word Count')\n    plt.tick_params(size=16)\n    plt.xticks(np.linspace(0,1000,9))\n    plt.title('Distribution of Document Word Counts', fontdict=dict(size=22))\n    plt.show()","f5f3548a":"def plot_words_by_dominant_topic(df_dominant_topic, num_topics):\n    \n    cols = [color for name, color in mcolors.TABLEAU_COLORS.items()] # more colors: 'mcolors.XKCD_COLORS'\n    p_width = 2\n    p_height = math.ceil(num_topics\/p_width)\n    \n    fig, axes = plt.subplots(p_width,p_height,figsize=(16,14), dpi=160, sharex=True, sharey=True)\n    \n    for i, ax in enumerate(axes.flatten()):    \n        df_dominant_topic_sub = df_dominant_topic.loc[df_dominant_topic.Dominant_Topic == i, :]\n        doc_lens = [len(d) for d in df_dominant_topic_sub.Text]\n        ax.hist(doc_lens, bins = 1000, color=cols[i])\n        ax.tick_params(axis='y', labelcolor=cols[i], color=cols[i])\n        sns.kdeplot(doc_lens, color=\"black\", shade=False, ax=ax.twinx())\n        ax.set(xlim=(0, 1000), xlabel='Document Word Count')\n        ax.set_ylabel('Number of Documents', color=cols[i])\n        ax.set_title('Topic: '+str(i), fontdict=dict(size=16, color=cols[i]))\n    \n    fig.tight_layout()\n    fig.subplots_adjust(top=0.90)\n    plt.xticks(np.linspace(0,1000,9))\n    fig.suptitle('Distribution of Document Word Counts by Dominant Topic', fontsize=22)\n    plt.show()","58750ce6":"def get_weights_and_counts(lda_model, words_cleaned):\n    topics = lda_model.show_topics(formatted=False)\n    data_flat = [w for w_list in words_cleaned for w in w_list]\n    counter = Counter(data_flat)\n    out = []\n    for i, topic in topics:\n        for word, weight in topic:\n            out.append([word, i , weight, counter[word]])\n    \n    df = pd.DataFrame(out, columns=['word', 'topic_id', 'importance', 'word_count'])  \n    return df  ","ff439778":"def word_count_weight_plot(df, num_topics):\n    p_width = 2\n    p_height = math.ceil(num_topics\/p_width)\n    fig, axes = plt.subplots(p_width, p_height, figsize=(16,10), sharey=True, dpi=160)\n    cols = [color for name, color in mcolors.TABLEAU_COLORS.items()]\n    wc_max = max(df['word_count'])*1.2\n    importance_max = max(df['importance'])*1.2\n    for i, ax in enumerate(axes.flatten()):\n        ax.bar(x='word', height=\"word_count\", data=df.loc[df.topic_id==i, :], color=cols[i], width=0.5, alpha=0.3, label='Word Count')\n        ax_twin = ax.twinx()\n        ax_twin.bar(x='word', height=\"importance\", data=df.loc[df.topic_id==i, :], color=cols[i], width=0.2, label='Weights')\n        ax.set_ylabel('Word Count', color=cols[i])\n        ax_twin.set_ylim(0, importance_max); ax.set_ylim(0, wc_max)\n        ax.set_title('Topic: ' + str(i), color=cols[i], fontsize=16)\n        ax.tick_params(axis='y', left=False)\n        ax.set_xticklabels(df.loc[df.topic_id==i, 'word'], rotation=30, horizontalalignment= 'right')\n        ax.legend(loc='upper left'); ax_twin.legend(loc='upper right')\n        l = ax.get_ylim()\n        l2 = ax_twin.get_ylim()\n        f = lambda x : l2[0]+(x-l[0])\/(l[1]-l[0])*(l2[1]-l2[0])\n        ticks = f(ax.get_yticks())\n        ax_twin.yaxis.set_major_locator(matplotlib.ticker.FixedLocator(ticks))  \n    fig.tight_layout(w_pad=2)    \n    fig.suptitle('Word Count and Importance of Topic Keywords', fontsize=22, y=1.05)    \n    plt.show()","cb75f4b1":"\ndef mct(texts, stopwords, num_topics):\n    \n    print(\"Warning: This will crash if you've picked too many topics for your dataset\")\n    texts_clean = list(preprocess(texts))\n    tokens = process_words(texts_clean, stopwords = stopwords)\n    lda_model, corpus = create_model_and_corpus(tokens, num_topics = num_topics)\n    return lda_model, corpus, tokens","ad97e556":"def other_visualizations(lda_model, corpus, tokens):\n    \n    sent_df, dom_topic_df = format_topics_sentences(corpus, texts = tokens, ldamodel=lda_model)\n    doc_word_count_plot(dom_topic_df)\n    plot_words_by_dominant_topic(dom_topic_df)\n    wc_df = get_weights_and_counts(lda_model, tokens)\n    word_count_weight_plot(wc_df)","64e0abad":"def lda_pipeline(texts, stopwords, num_topics):\n    print(\"Warning: This will crash if you've picked too many topics for your dataset\")\n    texts_clean = list(preprocess(texts))\n    tokens = process_words(texts_clean, stopwords = stopwords)\n    lda_model, corpus = create_model_and_corpus(tokens, num_topics = num_topics)\n    sent_df, dom_topic_df = format_topics_sentences(corpus, texts = tokens, ldamodel=lda_model)\n    doc_word_count_plot(dom_topic_df)\n    plot_words_by_dominant_topic(dom_topic_df, num_topics = num_topics)\n    wc_df = get_weights_and_counts(lda_model, tokens)\n    word_count_weight_plot(wc_df, num_topics= num_topics)\n    #Removing for DEMO\n    #pyLDAvis.enable_notebook()\n    #pyLDAvis.gensim.prepare(lda_model, corpus, dictionary=lda_model.id2word)\n   # abs_vis = pyLDAvis.gensim.prepare(lda_model, corpus, dictionary=lda_model.id2word)\n   # pyLDAvis.show(abs_vis) \n    # pyLDAvis.display(abs_vis)","3812f86c":"hist_data = df['history_text'].values.tolist()\npow_data = df['powers_text'].values.tolist()\n\n\nhist_lda, hist_corpus, hist_tokens = mct(hist_data, stopwords = hist_stopwords, num_topics = 8)\npyLDAvis.enable_notebook()\npyLDAvis.gensim.prepare(hist_lda, hist_corpus, dictionary=hist_lda.id2word)","477efd64":"pow_lda, pow_corpus, pow_tokens = mct(pow_data, stopwords = pow_stopwords, num_topics = 4)\npyLDAvis.enable_notebook()\npyLDAvis.gensim.prepare(pow_lda, pow_corpus, dictionary=pow_lda.id2word)","3561bf65":"lda_pipeline(hist_data, hist_stopwords, num_topics = 8)","9f0def97":"sent_df, dom_topic_df = format_topics_sentences(hist_corpus, texts = hist_tokens, ldamodel=hist_lda)\ndoc_word_count_plot(dom_topic_df)","832c44d7":"plot_words_by_dominant_topic(dom_topic_df, num_topics = 8)","d42633fe":"wc_df = get_weights_and_counts(hist_lda, hist_tokens)\nword_count_weight_plot(wc_df, num_topics= 8)","f22092dc":"I want to create a better metric of a supes overall power- taking the scores given in the data as well as the powers that supe has into effect. The easiest thing to do would be count everything equally and take a sum, but based on the distributions we saw in the pairplots, I don't think we can do that. To revisit, let's take a look. ","97e3bdcd":"Eight topics may be too many- there really appear to be only four distinct topics per the PCA. That said, there are some interesting distinctions between topics that are close, e.g. Topic 7 is Tony Stark related, but Topic 6, which is right next to it, is a menagerie of wizards, anime characters, and supernatural characters.","871d5183":"I'm immediately making a copy of the dataframe- we'll come back to the original to do some NLP after our exploratory data analysis. There are a good chunk of columns here- 81, in total. Not all of them are going to be useful but some of them will have to be split into additional columns.","e953b4d8":"I'm not a comic book expert but I'm fairly certain that Bruce Wayne is not supposed to be 30 ft tall. The others I'm not sure about. Part of the magic of superheroes is that they're outliers and I don't know these eight characters well enough to know if this is poor data or if the height and weight values are accurate. Because I don't know (and don't want to read up on these characters to find out) I'm going to just keep them in. \n","b1e78fe0":"Before even digging into distributions and all that, I'm doing some feature engineering to turn height and weight into useable variables instead of a weird text variable. ","b187a72c":"We can get more advanced though. The next portion of the notebook gets into topic modeling. I've structured the code here somewhat differently. This was originally written to be a part of a more versitile script, so I've left this code in function form. ","a444865e":"Show graphics better","2196acd5":"Taking a look at these by type of variable. First 'object'. ","181cc67c":"Looking at some of the weight outliers, we can see that they're not really outliers, just a cluster of Hulks and other supes who sound like they would be really heavy. I'm willing to let these stay. ","347a1135":"Some of the relationships between powers are really interesting. It makes total sense that a supe with Fire Resistance would also have Heat Resistance. Picking and choosing from these powers could be tricky but there are two easy wins here: gender and alignment. It's true that there are some genderless and neutral supes, but we shouldn't lose much taking out these two columns since the neutral parties should still be accounted for. ","494fc57b":"I'm going to try the naive approach first and see how it works. The answer? Surprisingly well. Though none of the distributions of the individual scores are anywhere near normal, this is pretty close. Evidently whoever created these scores along with the writers actually do a pretty good job of keeping the supes \"balanced.\" There's clear bimodality here near the top of the range. This isn't entirely unexpected- there are going to be some overpowered supes. I would wager that  a good number of these these ultra-powerful beings are going to be villians that require the cooperation of teams like the Avengers to take them down.","450e20ed":"Now lets take a look at distributions. Starting with the power \"scores\" that each super received. It appears as though the scores were given quite generously- there is a clear skew towards the top end of the scale, especially for intelligence, power, and combat scores. Interestingly, for strength and combat, there's also a spike near the bottom of the range. This is probably reserved for characters who get by on deviousness and intelligence. ","591e3c6f":"To pull that thread, let's plot it. Indeed, a higher portion of those uber-powerful beings are villians. ","67d9990d":"In some pre-work, I was able to get some additional stopwords that I want to take out of each of these texts, respectively. I tokenize the text, remove the stopwords, and plot the most common terms. Nothing super surprising here, just some basic NLP.","13a7c846":"I want to turn some of the other columns in the dataframe into a more useable form- 'creator', 'alignment', and 'gender' are all given dummy variables here. I'll check the correlations later to see if I need to drop any of the dummies.","88b414ce":"Out of curiousity, I'm going to pull that thread:","9c9fd87c":"Moving on to the ints, there are only six. They are all the scores given to the individual supes on six different metrics. Perhaps add one \"overall score\"?","757b4aef":"I'm going to remove all of these supes. I could try to impute a value for the scores for each of them, but I don't think calling all 106 of them \"average\" supes is a solid approach. ","2ed97ea2":"I want to check out 'type_race.' I think this could be a really helpful column, but I also think that it could require some feature engineering to add real value. ","bb82cc51":"There are some columns I don't want for this portion of my analysis. It's possible that 'teams' could add some value here, but it seems like cheating to add 'teams' to the analysis since I'm trying to find natural clusters of supes. I'm also throwing out 'overall_score' because I want to build my own metric for an overall score of a supes power. Note that 'height' and 'weight' are thrown out here, because I renamed the clean height and weight columns. ","3433b517":"One quick check to make sure we've handled all the NAs.","0f49b96d":"With the NAs handled and our data clean, we can do some feature engineering to get a new overall power metric.","9a72f448":"# Exploratory Data Analysis, Data Cleaning","1517b4a5":"Next I'm handling the columns of powers. There are 50 of these bad boys, with a lot of NAs. For our purposes, I'm going to say that a NA means that a given supe does not have that power. ","ef235692":"Here's the K-Means model using 8 clusters, the best choice based on the elbow plot above. There are better visualizations that could be used here, either a 3D plot or using some PCA to reduce the dimensionalty. ","4e32cadc":"Putting it all together here. I create lists of the history and power data for each supe and then run my fancy functions on them to show the LDA visualizations. ","4dba3107":"This notebook needs a lot of libraries. Importing most of them here and bringing in the dataset. Much of the LDA pipeline used in this script was originally written by Selva Prabhakaran at Machine Learning Plus (From the notebook [here](https:\/\/www.machinelearningplus.com\/nlp\/topic-modeling-visualization-how-to-present-results-lda-models\/)). I've adapted parts of it, but mostly it is his work. \n\nThis notebook is still under construction. I've made it public to facilitate collaboration but will be updating regularly. ","cfa604cb":"# NLP ","719daad6":"There's more engineering to be done though. I'm going to take a look at 'type_race'. ","3b6fb5c4":"There's one column I want to dig unto immediately- 'first_appearance'. Have the powers and abilities given to superheroes changed over time? Unfortunately, this column doesn't have a lot to offer. Some detailed background research could get years for each of these observations, but without that work we'd have to throw out a lot of rows to use this data. ","2a19d933":"There also appear to be quite a few supes who have values of zero for multiple \"scores.\" Lets take a look at them","a786bf9d":"Some of this will be useful later on for some NLP effort, but for now a lot will be discarded. Maybe come back to \"history_text\" to see if there's something in common for certain creators. Name related columns, place of birth, teams, and relatives will all get discarded. Gender, height, and weight I definitely want, but will need to recode.\n\nMoving on to the float values. There are 50 of them, all based on powers that the supes in question either have or don't. ","aff04b7f":"This is really interesting- there are four distinct topics in powers. One is related to the supernatural, one is combat prowess, one is superhuman strength\/speed\/durability, and one is energy manipulation. ","b9c967a6":"With the NAs handled we can look at the correlations between existing columns. If any columns have to strong of a correlation, we should do something about it or the values will essentially be double-counted in analysis. ","1d4c7313":"Awesome. Lets get into some \n# Clustering","fb7cc078":"That's interesting- few supes with low combat scores had a high strength score, but the inverse was not true. Plenty of supes with a low strength score had a high combat score. As we predicted, the intelligence scores skew quite high here. There's a wider range of speed, durability, and power than I expected but we do see that the distributions skew lower than the original inquiry. \n\nThere are some pesky outliers in both height and weight. Let's check those out","3b33ccea":"# Feature Engineering","9dbb4611":"Taking a look at the pair plot, we see numbers that begin to make a little more sense.","a0ddaa94":"I'm going to start with some basic analysis of the entire corpus of history and power texts. I'm back to working with 'df', the dataframe I brought in at the start of the notebook. First, I bring all the text into one string for history and one string for powers. "}}