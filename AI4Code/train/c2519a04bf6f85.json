{"cell_type":{"d1768c11":"code","05c4d8de":"code","806f927b":"code","17c5f62c":"code","76c201e5":"code","a68338f1":"code","c4964e2f":"code","5d291d2e":"code","fb8f6af6":"code","7355cee3":"code","1555a1b0":"code","8e9517d3":"code","f7b0ad28":"code","84df97ed":"code","8f08c923":"code","66b937ab":"code","ffda8ab9":"code","464ab253":"code","1e89292b":"code","bcaa9826":"markdown","60b90a25":"markdown","5ab3ff74":"markdown","82441ac0":"markdown","6bd79856":"markdown","0f0c707f":"markdown","a274341b":"markdown","94a218d1":"markdown","3d7e917d":"markdown","90cb09f1":"markdown","409ffc27":"markdown","f591b7fb":"markdown"},"source":{"d1768c11":"import numpy as np\nimport cv2\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport os\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator, load_img\nfrom tensorflow.keras.preprocessing.image import img_to_array, array_to_img \nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.layers import Input, SeparableConv2D, MaxPooling2D, Lambda\nfrom tensorflow.keras.layers import Flatten, Dense, Dropout, BatchNormalization\nfrom tensorflow.keras.layers import Conv2DTranspose, Conv2D, add, concatenate\nfrom tensorflow.keras.layers import LeakyReLU, Activation, Reshape\nfrom tensorflow.keras.utils import plot_model\nfrom tensorflow.keras.optimizers import Adam \nfrom tensorflow.keras.callbacks import ModelCheckpoint,ReduceLROnPlateau\nplt.style.use('ggplot')","05c4d8de":"attributes = pd.read_csv('\/kaggle\/input\/celeba-dataset\/list_attr_celeba.csv')\nbboxes = pd.read_csv('\/kaggle\/input\/celeba-dataset\/list_bbox_celeba.csv')\npartition = pd.read_csv('\/kaggle\/input\/celeba-dataset\/list_eval_partition.csv')\nlandmarks = pd.read_csv('\/kaggle\/input\/celeba-dataset\/list_landmarks_align_celeba.csv')\nbase_directory = '\/kaggle\/input\/celeba-dataset\/img_align_celeba\/img_align_celeba'","806f927b":"images_in_row = 10\nimages_in_column = 7\n\npaths_to_images = [os.path.join(base_directory, filename) \n                   for filename in os.listdir(base_directory)[:images_in_row * images_in_column]]\nimage = cv2.imread(paths_to_images[0])\n\nheight = image.shape[0]\nwidth = image.shape[1]\nchannels = image.shape[2]\n\n\nh_stack = np.zeros(shape = (images_in_row, height, width, channels))\nimages = np.zeros(shape = (images_in_column, height, width * images_in_row, channels))\n\n\nfor iteration in range(images_in_column):\n\n    start = images_in_row * iteration\n    end = images_in_row * (iteration + 1)\n    \n    h_stack = np.zeros(shape = (images_in_row, height, width, channels))\n    \n    for index, filename in enumerate(paths_to_images[start:end]):\n        \n        current_image = cv2.imread(filename)\n        current_image = cv2.cvtColor(current_image, cv2.COLOR_BGR2RGB) \/ 255\n        h_stack[index] = current_image\n        \n    h_stack = np.hstack(h_stack)\n    images[iteration] = h_stack\n\nimages = np.concatenate(images)\nplt.figure(figsize = (20, 20))\nplt.axis('off')\nplt.imshow(images)\n","17c5f62c":"def show_partition(examples):\n    types_of_partition = len(np.unique(partition.partition))\n    images = np.zeros(shape = (examples, height, width * types_of_partition, channels))\n    for iteration in range(examples):\n        h_stack = np.zeros(shape = (types_of_partition, height, width, channels))\n        for index, cur_partition in enumerate(np.unique(partition.partition)):\n            rand = np.random.randint(100)\n            current_path = partition.query('partition == {}'.format(cur_partition)).iloc[rand].image_id\n            current_path = os.path.join(base_directory, current_path)\n            current_image = cv2.imread(current_path) \n            current_image = cv2.cvtColor(current_image, cv2.COLOR_BGR2RGB)\n            h_stack[index] = current_image \/ 255\n        h_stack = np.hstack(h_stack)\n        images[iteration] = h_stack\n    images = np.concatenate(images)\n    \n    plt.figure(figsize = (20, 10))\n    suptitle = 'partition: 0' + ' ' * 10\n    suptitle += 'partition: 1' + ' ' * 10\n    suptitle += 'partition: 2' + ' ' * 10\n    plt.suptitle(suptitle)\n    plt.axis('off')\n    plt.imshow(images)\n        \n        \n        \nshow_partition(examples = 3)","76c201e5":"example_image = cv2.imread(sorted(paths_to_images)[0])\nexample_image = cv2.cvtColor(example_image, cv2.COLOR_BGR2RGB)\n\noriginal_image = example_image.copy()\n\ncurrent_landmarks = landmarks.query('image_id == \"{}\"'.format(paths_to_images[1].split('\/')[-1]))\neye_x, eye_y, eye_w, eye_h = np.array(landmarks.iloc[:, 1:5])[0]\nnose_x,\tnose_y,\tleftmouth_x, leftmouth_y, rightmouth_x, rightmouth_y = np.array(landmarks.iloc[:, 5:])[0]\n\nleft_eye = (eye_x, eye_y)\nright_eye = (eye_w, eye_h)\nnose = (nose_x + 10,\tnose_y)\nleft_mounth = (leftmouth_x, leftmouth_y)\nright_mounth = (rightmouth_x, rightmouth_y)\n\ncurrent_bbox = bboxes.query('image_id == \"{}\"'.format(paths_to_images[1].split('\/')[-1]))\nx, y, w, h = np.array(current_bbox.iloc[:, 1:])[0]\n\nexample_image = cv2.rectangle(example_image, (x - w, y ), (w , h ), (0, 255, 255), 1)\n\nexample_image = cv2.line(example_image, left_eye, right_eye, (0, 255, 255),1)\nexample_image = cv2.line(example_image, left_eye, nose, (0, 255, 255), 1)\nexample_image = cv2.line(example_image, right_eye, nose, (0, 255, 255), 1)\nexample_image = cv2.line(example_image, nose, left_mounth,(0, 255, 255), 1)\nexample_image = cv2.line(example_image, nose, right_mounth, (0, 255, 255), 1)\n\nplt.figure(figsize = (10, 20))\nplt.subplot(1,2,1)\nplt.axis('off')\nplt.title('original image')\nplt.imshow(original_image)\nplt.subplot(1,2,2)\nplt.axis('off')\nplt.title('with bbox and landmarks')\nplt.imshow(example_image)","a68338f1":"train_images = partition.query('partition == 0')\nvalid_images = partition.query('partition == 1')\ntest_images = partition.query('partition == 2')","c4964e2f":"def relu(a):\n    if type(a) != str:\n        a = (a > 0) * a\n    return a\n\nattributes.iloc[:, 1:] = attributes.iloc[:, 1:].apply(lambda a: relu(a))","5d291d2e":"train_data = train_images.merge(attributes, how = 'inner').drop(['partition'], axis = 1)\nvalid_data = valid_images.merge(attributes, how = 'inner').drop(['partition'], axis = 1)\ntest_data = test_images.merge(attributes, how = 'inner').drop(['partition'], axis = 1)","fb8f6af6":"train_data","7355cee3":"def create_batch(data, iteration, batch_size):\n    '''\n    this function create batch of images as labels \n    for each iteration during training\n    \n    Arguments:\n        data(dataframe): DataFrame with labels and paths to images\n        iteration(int): current iteration\n        batch_size(int): number of images in one batch\n    Returns:\n        batch of data\n    '''\n    start = iteration * batch_size\n    end = batch_size * (iteration + 1)\n    current_data = data.iloc[start:end]\n    \n    num_features = 23\n    \n    images = np.zeros(shape = (batch_size, image_size, image_size, channels))\n#     here 23 features\n    is_hairs = np.zeros(shape = (batch_size, 2))\n    hairs_type = np.zeros(shape = (batch_size, 2))\n    hair_colors = np.zeros(shape = (batch_size, 4))\n    hairs_on_face = np.zeros(shape = (batch_size, 4))\n    gender = np.zeros(shape = (batch_size, 2))\n    mimics = np.zeros(shape = (batch_size, 2))\n    face_form = np.zeros(shape = (batch_size, 2))\n    young = np.zeros(shape = (batch_size, 2))\n    skin = np.zeros(shape = (batch_size, 2))\n    attractive = np.zeros(shape = (batch_size, 2))\n    nose = np.zeros(shape = (batch_size, 2))\n    eyes = np.zeros(shape = (batch_size, 2))\n    cheekbones = np.zeros(shape = (batch_size, 2))\n    eyebrows = np.zeros(shape = (batch_size, 2))\n    chin = np.zeros(shape = (batch_size, 2))\n    lips = np.zeros(shape = (batch_size, 2))\n    makeup = np.zeros(shape = (batch_size, 2))\n    hat = np.zeros(shape = (batch_size, 2))\n    necktie = np.zeros(shape = (batch_size, 2))\n    earrings = np.zeros(shape = (batch_size, 2))\n    necklace = np.zeros(shape = (batch_size, 2))\n    necktie = np.zeros(shape = (batch_size, 2))\n    effects = np.zeros(shape = (batch_size, 2))\n    \n   \n    for index in range(batch_size):\n        \n#         load and preprocess image\n        current_image_path = os.path.join(base_directory, current_data['image_id'].iloc[index])\n        current_image = load_img(current_image_path).resize((image_size, image_size))\n        \n#         z_codes == features\n        five_o_Clock_Shadow = current_data['5_o_Clock_Shadow'].iloc[index]\n        Arched_Eyebrows     = current_data['Arched_Eyebrows'].iloc[index]\n        Attractive          = current_data['Attractive'].iloc[index]\n        Bags_Under_Eyes     = current_data['Bags_Under_Eyes'].iloc[index]\n        Bald                = current_data['Bald'].iloc[index]\n        Bangs               = current_data['Bangs'].iloc[index]\n        Big_Lips            = current_data['Big_Lips'].iloc[index]\n        Big_Nose            = current_data['Big_Nose'].iloc[index]\n        Black_Hair          = current_data['Black_Hair'].iloc[index]\n        Blond_Hair          = current_data['Blond_Hair'].iloc[index]\n        Blurry              = current_data['Blurry'].iloc[index]\n        Brown_Hair          = current_data['Brown_Hair'].iloc[index]\n        Bushy_Eyebrows      = current_data['Bushy_Eyebrows'].iloc[index]\n        Chubby              = current_data['Chubby'].iloc[index]\n        Double_Chin         = current_data['Double_Chin'].iloc[index]\n        Eyeglasses          = current_data['Eyeglasses'].iloc[index]\n        Goatee              = current_data['Goatee'].iloc[index]\n        Gray_Hair           = current_data['Gray_Hair'].iloc[index]\n        Heavy_Makeup        = current_data['Heavy_Makeup'].iloc[index]\n        High_Cheekbones     = current_data['High_Cheekbones'].iloc[index]\n        Male                = current_data['Male'].iloc[index]\n        Mouth_Slightly_Open = current_data['Mouth_Slightly_Open'].iloc[index]\n        Mustache            = current_data['Mustache'].iloc[index]\n        Narrow_Eyes         = current_data['Narrow_Eyes'].iloc[index]\n        No_Beard            = current_data['No_Beard'].iloc[index]\n        Oval_Face           = current_data['Oval_Face'].iloc[index]\n        Pale_Skin           = current_data['Pale_Skin'].iloc[index]\n        Pointy_Nose         = current_data['Pointy_Nose'].iloc[index]\n        Receding_Hairline   = current_data['Receding_Hairline'].iloc[index]\n        Rosy_Cheeks         = current_data['Rosy_Cheeks'].iloc[index]\n        Sideburns           = current_data['Sideburns'].iloc[index]\n        Smiling             = current_data['Smiling'].iloc[index]\n        Straight_Hair       = current_data['Straight_Hair'].iloc[index]\n        Wavy_Hair           = current_data['Wavy_Hair'].iloc[index]\n        Wearing_Earrings    = current_data['Wearing_Earrings'].iloc[index]\n        Wearing_Hat         = current_data['Wearing_Hat'].iloc[index]\n        Wearing_Lipstick    = current_data['Wearing_Lipstick'].iloc[index]\n        Wearing_Necklace    = current_data['Wearing_Necklace'].iloc[index]\n        Wearing_Necktie     = current_data['Wearing_Necktie'].iloc[index]\n        Young               = current_data['Young'].iloc[index]\n        \n#         one image from batch\n        images[index] = current_image\n    \n#         z-codes\n        is_hairs[index] = [Bald, Bangs]\n        hairs_type[index] = [Wavy_Hair, Straight_Hair] \n        hair_colors[index] = [Black_Hair,Blond_Hair,Brown_Hair,Gray_Hair]\n        hairs_on_face[index] = [Goatee, Mustache, No_Beard, Sideburns, ]\n        gender[index] = [Male, 1 - Male]\n        mimics[index] = [Smiling, Mouth_Slightly_Open,]\n        face_form[index] = [Oval_Face, Chubby]\n        young[index] = [Young, 1 - Young]\n        skin[index] = [Pale_Skin, 1- Pale_Skin]\n        attractive[index] = [Attractive, 1 - Attractive]\n        nose[index] = [Big_Nose, Pointy_Nose]\n        eyes[index] = [Narrow_Eyes, Bags_Under_Eyes,]\n        cheekbones[index] = [High_Cheekbones, Rosy_Cheeks]\n        eyebrows[index] = [Arched_Eyebrows, Bushy_Eyebrows]\n        chin[index] = [Double_Chin, 1 - Double_Chin]\n        lips[index] = [Big_Lips, Wearing_Lipstick]\n        makeup[index] = [Heavy_Makeup, 1 - Heavy_Makeup]\n        hat[index] = [Wearing_Hat, 1- Wearing_Hat]\n        necktie[index] = [Wearing_Necktie, 1- Wearing_Necktie]\n        earrings[index] = [Wearing_Earrings, 1 - Wearing_Earrings]\n        necklace[index] = [Wearing_Necklace, 1 - Wearing_Necklace]\n        necktie[index] = [Wearing_Necktie, 1 - Wearing_Necktie]\n        effects[index] = [Blurry, five_o_Clock_Shadow]\n        \n        \n    z_codes = dict()    \n#     z_codes\n    z_codes['is_hairs'] = is_hairs\n    z_codes['hairs_type'] = hairs_type\n    z_codes['hair_colors'] = hair_colors \n    z_codes['hairs_on_face'] = hairs_on_face\n    z_codes['gender'] = gender\n    z_codes['mimics'] = mimics \n    z_codes['face_form'] = face_form \n    z_codes['young'] = young \n    z_codes['skin'] = skin\n    z_codes['attractive'] = attractive\n    z_codes['nose'] = nose \n    z_codes['eyes'] = eyes \n    z_codes['cheekbones'] = cheekbones \n    z_codes['eyebrows'] = eyebrows \n    z_codes['chin'] = chin \n    z_codes['lips'] = lips \n    z_codes['makeup'] = makeup\n    z_codes['hat'] = hat\n    z_codes['necktie'] = necktie \n    z_codes['earrings'] = earrings\n    z_codes['necklace'] = necklace\n    z_codes['necktie'] = necktie \n    z_codes['effects'] = effects \n    \n    \n    images \/= 255\n    \n    return images, z_codes\n\nimage_size = 128   \n\nbatch_size = 20\ntrain_images, train_z_codes = create_batch(data = train_data, iteration = 0, batch_size = batch_size)","1555a1b0":"def build_encoder(input_shape, name):\n    '''\n    This function builds a lightweight encoder\n    \n    Arguments:\n        input_shape(tuple): input images shape\n        name (str): the name of the feature the generator \n        is responsible for generating\n        \n        name must be in z_codes dictionary\n    '''\n    \n    output_shape = train_z_codes[name].shape[1]\n    \n    last_activation = 'sigmoid'\n    optimizer = Adam(lr = 0.001)\n    loss = 'binary_crossentropy'\n    metrics = ['accuracy']\n    \n    if output_shape > 2:\n        last_activation = 'softmax'\n        loss = 'categorical_crossentropy'\n    \n    inputs = Input(shape = input_shape)\n    \n    x = inputs   \n    \n    x = SeparableConv2D(filters = 128, \n                        kernel_size = 3, \n                        activation = 'relu', \n                        name = 'block_1')(x)\n    x = MaxPooling2D(2)(x)\n    x = BatchNormalization()(x)\n    \n    x = SeparableConv2D(filters = 64, \n                        kernel_size = 3, \n                        activation = 'relu', \n                        name = 'block_2')(x)\n    x = MaxPooling2D(2)(x)\n    x = BatchNormalization()(x)\n    \n    x = SeparableConv2D(filters = 32, \n                        kernel_size = 3, \n                        activation = 'relu', \n                        name = 'block_3')(x)\n    x = MaxPooling2D(2)(x)\n    x = BatchNormalization()(x)\n    \n    x = SeparableConv2D(filters = 16, \n                        kernel_size = 3, \n                        activation = 'relu', \n                        name = 'block_4')(x)\n    x = MaxPooling2D(2)(x)\n    x = BatchNormalization()(x)\n    \n    x = Flatten(name = 'Flatten')(x)\n    x = Dropout(0.3, name = 'Dropout')(x)\n    \n    outputs = Dense(output_shape, \n                    activation = last_activation)(x)\n    \n    encoder = Model(inputs = inputs, \n                    outputs = outputs, \n                    name = f'{name}_encoder')\n    \n    encoder.compile(optimizer = optimizer, \n                    loss = loss, \n                    metrics = metrics)\n    \n    return encoder\n                                       \n    \ninput_shape = (image_size, image_size, channels)\nname = 'hair_colors'\nencoder = build_encoder(input_shape = input_shape, name = name) ","8e9517d3":"plot_model(encoder, show_shapes = True, to_file = 'encoder_model.png')","f7b0ad28":"def build_generator(z_codes_shape, image_size, name):\n    '''\n    description:\n        this function creates a generator\n        each generator responds\n        for the feature it should generate\n    \n    Arguments:\n        inputs(tensor): batch of images and corresponds z_codes\n        image_size(tuple) tuple with input image parameters\n        name(str): the name of the feature the generator \n        is responsible for generating\n        \n        name must be in z_codes dictionary\n        \n    Returns:\n        Generator model\n    '''\n    \n    \n#     network parameters\n    layer_filters = [512, 128, 64, 3]\n    kernel_size = 5\n    strides = 2    \n    \n    channels = 3\n    latent_shape = 100\n    image_resize = image_size \/\/ len(layer_filters)\n    noise_input_shape = (image_size, image_size, channels)\n\n    \n    noise_shape = (image_size, image_size, channels)\n    \n    noise_input = Input(shape = noise_shape, name = 'noise_input')\n    noise_branch = Reshape(target_shape = (128 * 128 * 3, ),\n                           name = 'noise_flatten')(noise_input)\n    noise_branch = Dense(units = latent_shape, \n                         activation = 'relu',\n                         name = 'latent')(noise_branch)\n    \n    z_codes_input = Input(shape = z_codes_shape, name = 'z_codes_input')\n    \n    two_inputs = [noise_input, z_codes_input]\n    inputs = concatenate([noise_branch, z_codes_input], \n                         name = 'concatenate_layer')\n    x = Dense(image_resize * image_resize * layer_filters[0])(inputs)\n    x = Reshape(target_shape = (image_resize, image_resize, layer_filters[0]))(x)\n    \n    for index, filters in enumerate(layer_filters):\n        if filters == layer_filters[-2]:\n            strides = 1\n            \n        x = BatchNormalization(name = f'BN_block_{index}')(x)\n        x = LeakyReLU(alpha = 0.2,\n                     name = f'Leaky_block_{index}')(x)\n        \n        x = Conv2DTranspose(filters = filters, \n                            kernel_size = kernel_size,\n                            strides = strides,\n                            padding = 'same',\n                            name = f'block_{index}')(x)\n    outputs = Activation(activation = 'tanh', \n                         name = 'generator_output')(x)\n    \n    generator = Model(inputs = two_inputs, outputs = outputs, name = f'{name}_generator')\n    \n    loss = 'binary_crossentropy'\n    optimizer = Adam(lr = 0.01)\n    metrics = ['accuracy']\n    generator.compile(loss = loss, optimizer = optimizer, metrics = metrics)\n    \n    return generator\n        \n    \nname = 'gender'\nnoise = np.random.random(size = (batch_size, 128, 128, 3))\nlabels = train_z_codes[name]\ninputs = [noise, labels]\ngenerator_1 = build_generator(labels.shape[1], \n                              image_size = train_images.shape[1], \n                              name = name)","84df97ed":"plot_model(generator_1, show_shapes = True, to_file = 'generator_model.png')","8f08c923":"def build_discriminator(input_shape):\n    \n    inputs = Input(shape = input_shape)\n    x = inputs\n    blocks_count = 4\n    layers_filters = [128, 64, 32,]\n    \n    for index, filters in enumerate(layers_filters):\n        x = SeparableConv2D(filters = filters , \n                            kernel_size = 3,\n                            activation = 'relu', \n                            name = f'block_{index}')(x)\n        \n        x = MaxPooling2D(pool_size = 2)(x)\n        x = BatchNormalization()(x)\n    \n    x = Flatten(name = 'flatten')(x)\n    x = Dropout(0.3)(x)\n    \n    outputs = Dense(1, activation = 'sigmoid')(x)\n    \n    discriminator_model = Model(inputs = inputs, \n                          outputs = outputs,\n                          name = 'discriminator'\n                         )\n    \n    optimizer = Adam(lr = 0.01)\n    loss = 'binary_crossentropy'\n    metrics = ['accuracy']\n    discriminator_model.compile(optimizer = optimizer,\n                                loss = loss,\n                                metrics = metrics,\n                               )\n    return discriminator_model\n\ndiscriminator = build_discriminator(input_shape = input_shape)","66b937ab":"plot_model(discriminator, show_shapes = True, to_file = 'discriminator_model.png')","ffda8ab9":"!nvidia-smi","464ab253":"    def train_stack(epoch,\n                    previous_generator_output,\n                    train_labels,\n                    train_images,\n                    encoder,\n                    generator,\n                    discriminator):\n        images_directory = '\/kaggle\/working\/generated_images'\n        if os.path.isdir(images_directory) == False:\n            os.mkdir(images_directory)\n    \n        name_index = generator.name.index('_generator')\n        name = generator.name[:name_index]\n\n        current_train_labels = train_labels[name]\n        # train encoder         \n        train_loss, train_accuracy = encoder.train_on_batch(x = train_images,\n                                                            y = current_train_labels)\n\n        print(f'{name}_Encoder --> epoch: {epoch} --> loss: {train_loss}, accuracy: {train_accuracy}')\n\n        encoder_output = encoder.predict(train_images)\n\n        # train generator         \n        train_loss, train_accuracy = generator.train_on_batch(x = [previous_generator_output, \n                                                                   encoder_output], \n                                                              y = train_images)\n        print(f'{name}_generator --> epoch: {epoch} --> loss: {train_loss},accuracy: {train_accuracy}')\n\n        generator_output = generator.predict([previous_generator_output, current_train_labels])\n        predict = cv2.cvtColor(generator_output[0], cv2.COLOR_RGB2BGR) * 255\n\n        if epoch % 5  == 0:\n            generated_path = os.path.join(images_directory, f'epoch_{epoch}_.jpg')\n            cv2.imwrite(generated_path, predict)\n        # train discriminator\n        # discriminator decide original this image or fake\n        # for this we it need to have train and fake images\n        # for training. let's help to discriminator\n\n        # generated images and labels\n        fake_images = generator_output\n        fake_labels = np.zeros(shape = batch_size)\n        \n        # original images and labels from train dataset\n        original_images = train_images\n        original_labels = np.ones(shape = batch_size)\n\n        rand_indexes = np.random.shuffle(np.arange(batch_size * 2))\n\n        #  mix train_data\n        real_fake_images = np.concatenate([fake_images, original_images])[rand_indexes]\n        real_fake_labels = np.concatenate([fake_labels, original_labels])[rand_indexes]\n\n        real_fake_images = real_fake_images.reshape((real_fake_images.shape[1:]))\n        real_fake_labels = real_fake_labels.reshape((real_fake_labels.shape[1:]))\n        # train discriminator\n        train_loss, train_accuracy = discriminator.train_on_batch(x = real_fake_images, \n                                                                 y = real_fake_labels)\n\n        print(f'{name}_discriminator --> epoch: {epoch} --> loss: {train_loss}, accuracy: {train_accuracy}')\n\n        discriminator_output = discriminator.predict(generator_output)\n\n        return generator_output","1e89292b":"def train_and_build_StackedGAN(train_data,\n                               batch_size, \n                               epochs):\n    \n#     this small batch just for parameters setting\n    train_images, train_labels = create_batch(data = train_data, \n                                              iteration = 0, \n                                              batch_size = batch_size)   \n    #  parameters\n    image_size = train_images.shape[1]\n    channels = train_images.shape[3]\n    noise = np.random.normal(size = train_images.shape)\n    input_shape = (image_size, image_size, channels)\n    \n#     code below create discriminators, generators and encoders\n#     which number equal number of z_codes in discrionary\n    encoders = [build_encoder(input_shape = input_shape, name = name) for name in names]\n    \n    generators = [build_generator(name = name, \n                                  z_codes_shape = train_labels[name].shape[1], \n                                  image_size = image_size) for name in names]\n    \n    discriminators = [build_discriminator(input_shape = input_shape) for name in names]\n        \n#         training process\n    generator_output = noise\n    \n    for epoch in range(epochs):\n        print('-'* 10 + f'epoch: {epoch}' + 10*'-')\n        \n        # data\n        train_images, train_labels = create_batch(data = train_data, \n                                                  iteration = epoch, \n                                                  batch_size = batch_size)\n\n        # model architecture and training\n        for index in range(len(generators)):\n        \n            generator_output = train_stack(epoch = epoch,\n                                           train_labels = train_labels,\n                                           train_images = train_images,\n                                           previous_generator_output = generator_output,\n                                           encoder = encoders[index], \n                                           generator = generators[index], \n                                           discriminator = discriminators[index])\n      \n        \nepochs = 100\nbatch_size = 128\nnames = list(train_z_codes)\ntrain_and_build_StackedGAN(train_data = train_data,\n                           batch_size = batch_size, \n                           epochs = epochs)","bcaa9826":"# Discriminator","60b90a25":"GAN: adversarial = discriminator(generator)","5ab3ff74":"okay partition is just recomended parameter of dataset distribution ","82441ac0":"# this model architecture","6bd79856":"# Model Architecture","0f0c707f":"GAN = generator + discriminator","a274341b":"# StackedGAN","94a218d1":"# encoder","3d7e917d":"# let's figure out what all this data mean","90cb09f1":"# Generator","409ffc27":"![image.png](attachment:6fdf8fde-f2e2-499d-8eea-c31e71104d2e.png)","f591b7fb":"![image.png](attachment:b10cb6bf-4539-4081-9d0d-bbb5d117631b.png)"}}