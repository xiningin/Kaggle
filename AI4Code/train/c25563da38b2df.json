{"cell_type":{"8f2b4a24":"code","9ba28a28":"code","462e6cc9":"code","2a42f0d0":"code","09ada13e":"code","95ded526":"code","ced0cca5":"code","9ceeacbd":"code","a293898e":"code","b021fd80":"code","85e8a3a9":"code","82933031":"code","dd94c2c4":"code","b7ac83c0":"code","abd0d1ec":"code","2b839ddb":"code","a27ecaac":"code","9285372b":"code","15740115":"code","42b715c2":"code","85716089":"code","94a2e375":"code","4a0084b8":"code","2c399a22":"code","b438b9d7":"code","e8d7c995":"code","0cf81d18":"code","d4f20f63":"code","2c14ad7c":"code","b77f0934":"code","3e39128a":"markdown","e3bc80f3":"markdown","909dc3c9":"markdown","772fc259":"markdown","bb8e1c12":"markdown","70c7c6f6":"markdown","4e8254eb":"markdown","cd7b7ae3":"markdown","dbfdf3ee":"markdown","6fd1a69e":"markdown","a728f067":"markdown","8e79fc78":"markdown","9a24016e":"markdown","88917e6b":"markdown","ee43d42e":"markdown","ecf0ce59":"markdown","07a1f289":"markdown","82cc1676":"markdown","feece0eb":"markdown","4e77630a":"markdown","01faecc5":"markdown","055f65ac":"markdown","90c8bdb3":"markdown","766197ff":"markdown","6e3dbcf8":"markdown","c6c08885":"markdown"},"source":{"8f2b4a24":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","9ba28a28":"! pip install wikipedia","462e6cc9":"# Importing required libraries\nimport wikipedia\nimport numpy as np\nimport pandas as pd\nfrom PIL import Image\nimport re\n# For handling string\nimport string\nimport requests\nimport io\nimport seaborn as sns\nfrom io import BytesIO\nimport random\nimport matplotlib.pyplot as plt \nfrom wordcloud import WordCloud, STOPWORDS, ImageColorGenerator","2a42f0d0":"# Function for grey colour of cloud\ndef grey_color_func(word, font_size, position, orientation, random_state=None, **kwargs):\n    return \"hsl(0, 0%%, %d%%)\" % random.randint(60, 100)\n\n# Function that makes the cloud\ndef make_cloud(x, url):\n    response = requests.get(url) # Requesting the url for image\n    mask = np.array(Image.open(BytesIO(response.content))) # Converting image to numpy array to make mask\n    cloud = WordCloud(background_color='black',\n                      width=5000, height=5000, \n                      max_words=2000, max_font_size=200, \n                      min_font_size=1, mask=mask, stopwords=STOPWORDS)\n    cloud.generate(x) # Generating WordCloud\n    \n    fig, ax = plt.subplots(figsize=(15, 15))\n    ax.imshow(cloud.recolor(color_func=grey_color_func, random_state=3), interpolation='bilinear') # Adding grey colour\n    ax.set_axis_off()\n    \n    plt.show(cloud)","09ada13e":"# Looking up wikipedia pages for the TV show\nwikipedia.search('The Office (US)')","95ded526":"# Collecting the content to create a word cloud\nthe_office = wikipedia.page('The Office (American TV Series)')\ndf_content = the_office.content\nthe_office.content","ced0cca5":"# Creating a word cloud\nmake_cloud(df_content, 'https:\/\/www.givememyremote.com\/remote\/wp-content\/uploads\/2011\/08\/the-office-featured.jpg')","9ceeacbd":"# Loading the data set\ndata_path = '\/kaggle\/input\/the-office-us-complete-dialoguetranscript'\noffice = pd.read_csv('\/kaggle\/input\/the-office-us-complete-dialoguetranscript\/The-Office-Lines-V2.csv')\nPLOT_BGCOLOR='#DADEE3'\nPAPER_BGCOLOR='rgb(255,255,255)'\n\npd.set_option('display.max_rows', 200)\npd.set_option('display.max_columns', 200)","a293898e":"# Visualizing the top few rows of the data set\noffice.head(10)","b021fd80":"# Let us take a look at the shape of the data set\noffice.shape","85e8a3a9":"# Let us check whether there are any missing values in the data set\noffice.isnull().sum()","82933031":"# Let us take a look at the different characters in the show\noffice['speaker'].unique()","dd94c2c4":"# Let us take a look at few of the dialogues spoken by each character\nline = office.groupby(['season']) \nline.first()","b7ac83c0":"# Let us take a look at the most frequently used word in the series\nword_cloud = WordCloud(width = 1000,\n                       height = 800,\n                       colormap = 'Blues', \n                       margin = 0,\n                       max_words = 200,  \n                       min_word_length = 4,\n                       max_font_size = 120, min_font_size = 15,  \n                       background_color = \"white\").generate(\" \".join(office['line']))\n\nplt.figure(figsize = (10, 15))\nplt.imshow(word_cloud, interpolation = \"gaussian\")\nplt.axis(\"off\")\nplt.show()","abd0d1ec":"# Since there are many contractions in the dialogues like 'you're', 'aren't'and so on\n# we need to expand the contractions to better understand and analyse the data\n# Dictionary of English Contractions\ncontractions_dict = { \"ain't\": \"are not\",\"'s\":\" is\",\"aren't\": \"are not\",\n                     \"can't\": \"cannot\",\"can't've\": \"cannot have\",\n                     \"'cause\": \"because\",\"could've\": \"could have\",\"couldn't\": \"could not\",\n                     \"couldn't've\": \"could not have\", \"didn't\": \"did not\",\"doesn't\": \"does not\",\n                     \"don't\": \"do not\",\"hadn't\": \"had not\",\"hadn't've\": \"had not have\",\n                     \"hasn't\": \"has not\",\"haven't\": \"have not\",\"he'd\": \"he would\",\n                     \"he'd've\": \"he would have\",\"he'll\": \"he will\", \"he'll've\": \"he will have\",\n                     \"how'd\": \"how did\",\"how'd'y\": \"how do you\",\"how'll\": \"how will\",\n                     \"I'd\": \"I would\", \"I'd've\": \"I would have\",\"I'll\": \"I will\",\n                     \"I'll've\": \"I will have\",\"I'm\": \"I am\",\"I've\": \"I have\", \"isn't\": \"is not\",\n                     \"it'd\": \"it would\",\"it'd've\": \"it would have\",\"it'll\": \"it will\",\n                     \"it'll've\": \"it will have\", \"let's\": \"let us\",\"ma'am\": \"madam\",\n                     \"mayn't\": \"may not\",\"might've\": \"might have\",\"mightn't\": \"might not\", \n                     \"mightn't've\": \"might not have\",\"must've\": \"must have\",\"mustn't\": \"must not\",\n                     \"mustn't've\": \"must not have\", \"needn't\": \"need not\",\n                     \"needn't've\": \"need not have\",\"o'clock\": \"of the clock\",\"oughtn't\": \"ought not\",\n                     \"oughtn't've\": \"ought not have\",\"shan't\": \"shall not\",\"sha'n't\": \"shall not\",\n                     \"shan't've\": \"shall not have\",\"she'd\": \"she would\",\"she'd've\": \"she would have\",\n                     \"she'll\": \"she will\", \"she'll've\": \"she will have\",\"should've\": \"should have\",\n                     \"shouldn't\": \"should not\", \"shouldn't've\": \"should not have\",\"so've\": \"so have\",\n                     \"that'd\": \"that would\",\"that'd've\": \"that would have\", \"there'd\": \"there would\",\n                     \"there'd've\": \"there would have\", \"they'd\": \"they would\",\n                     \"they'd've\": \"they would have\",\"they'll\": \"they will\",\n                     \"they'll've\": \"they will have\", \"they're\": \"they are\",\"they've\": \"they have\",\n                     \"to've\": \"to have\",\"wasn't\": \"was not\",\"we'd\": \"we would\",\n                     \"we'd've\": \"we would have\",\"we'll\": \"we will\",\"we'll've\": \"we will have\",\n                     \"we're\": \"we are\",\"we've\": \"we have\", \"weren't\": \"were not\",\"what'll\": \"what will\",\n                     \"what'll've\": \"what will have\",\"what're\": \"what are\", \"what've\": \"what have\",\n                     \"when've\": \"when have\",\"where'd\": \"where did\", \"where've\": \"where have\",\n                     \"who'll\": \"who will\",\"who'll've\": \"who will have\",\"who've\": \"who have\",\n                     \"why've\": \"why have\",\"will've\": \"will have\",\"won't\": \"will not\",\n                     \"won't've\": \"will not have\", \"would've\": \"would have\",\"wouldn't\": \"would not\",\n                     \"wouldn't've\": \"would not have\",\"y'all\": \"you all\", \"y'all'd\": \"you all would\",\n                     \"y'all'd've\": \"you all would have\",\"y'all're\": \"you all are\",\n                     \"y'all've\": \"you all have\", \"you'd\": \"you would\",\"you'd've\": \"you would have\",\n                     \"you'll\": \"you will\",\"you'll've\": \"you will have\", \"you're\": \"you are\",\n                     \"you've\": \"you have\"}\n\n# Regular expression for finding contractions\ncontractions_re = re.compile('(%s)' % '|'.join(contractions_dict.keys()))\n\n# Function for expanding contractions\ndef expand_contractions(text,contractions_dict=contractions_dict):\n  def replace(match):\n    return contractions_dict[match.group(0)]\n  return contractions_re.sub(replace, text)\n\n# Expanding Contractions in the reviews\noffice['line'] = office['line'].apply(lambda x:expand_contractions(x))","2b839ddb":"# Let us take a look at the dialogues now, to verify whether the contractions have been expanded or not\noffice['line'].sample(10)","a27ecaac":"# Let us take a look at number of words in each dialogue\noffice.line.str.split().\\\n    map(lambda x: len(x)).\\\n    hist()","9285372b":"# Defining a function to visualise n-grams\ndef get_top_ngram(corpus, n=None):\n    vec = CountVectorizer(ngram_range=(n, n)).fit(corpus)\n    bag_of_words = vec.transform(corpus)\n    sum_words = bag_of_words.sum(axis=0) \n    words_freq = [(word, sum_words[0, idx]) \n                  for word, idx in vec.vocabulary_.items()]\n    words_freq =sorted(words_freq, key = lambda x: x[1], reverse=True)\n    return words_freq[:10]","15740115":"# Visualising the most frequent bigrams occurring in the conversation\nfrom sklearn.feature_extraction.text import CountVectorizer\ntop_bigrams = get_top_ngram(office['line'],2)[:10]\nx,y = map(list,zip(*top_bigrams))\nsns.barplot(x = y,y = x)","42b715c2":"# Visualising the most frequent trigrams occurring in the conversation\nfrom sklearn.feature_extraction.text import CountVectorizer\ntop_trigrams = get_top_ngram(office['line'],3)[:10]\nx,y = map(list,zip(*top_trigrams))\nsns.barplot(x = y,y = x)","85716089":"# Let us take a look at the dialogues said by Michael\noffice_filtered_m = office[office['speaker'] == 'Michael'] \n  \n# Print the new dataframe \nprint(office_filtered_m.head(15))\n\n# Print the shape of the dataframe \nprint(office_filtered_m.shape) ","94a2e375":"# Visualizing the most used words in the series by Michael\nword_cloud = WordCloud(width = 1000,\n                       height = 800,\n                       colormap = 'GnBu', \n                       margin = 0,\n                       max_words = 200,  \n                       min_word_length = 4,\n                       max_font_size = 120, min_font_size = 15,  \n                       background_color = \"white\").generate(\" \".join(office_filtered_m['line']))\n\nplt.figure(figsize = (10, 15))\nplt.imshow(word_cloud, interpolation = \"gaussian\")\nplt.axis(\"off\")\nplt.show()","4a0084b8":"# Let us take a look at the dialogues said by Dwight\noffice_filtered_d = office[office['speaker'] == 'Dwight'] \n  \n# Visualizing the most used words in the series by Dwight\nword_cloud = WordCloud(width = 1000,\n                       height = 800,\n                       colormap = 'twilight_shifted', \n                       margin = 0,\n                       max_words = 200,  \n                       min_word_length = 4,\n                       max_font_size = 120, min_font_size = 15,  \n                       background_color = \"white\").generate(\" \".join(office_filtered_d['line']))\n\nplt.figure(figsize = (10, 15))\nplt.imshow(word_cloud, interpolation = \"gaussian\")\nplt.axis(\"off\")\nplt.show()","2c399a22":"# Let us take a look at the dialogues said by Jim\noffice_jim = office[office['speaker'] == 'Jim'] \n  \n# Visualizing the most used words in the series by Dwight\nword_cloud = WordCloud(width = 1000,\n                       height = 800,\n                       colormap = 'twilight_shifted_r', \n                       margin = 0,\n                       max_words = 200,  \n                       min_word_length = 4,\n                       max_font_size = 120, min_font_size = 15,  \n                       background_color = \"white\").generate(\" \".join(office_jim['line']))\n\nplt.figure(figsize = (10, 15))\nplt.imshow(word_cloud, interpolation = \"gaussian\")\nplt.axis(\"off\")\nplt.show()","b438b9d7":"# Let us take a look at the dialogues said by Pam\noffice_pam = office[office['speaker'] == 'Pam'] \n  \n# Visualizing the most used words in the series by Dwight\nword_cloud = WordCloud(width = 1000,\n                       height = 800,\n                       colormap = 'inferno', \n                       margin = 0,\n                       max_words = 200,  \n                       min_word_length = 4,\n                       max_font_size = 120, min_font_size = 15,  \n                       background_color = \"white\").generate(\" \".join(office_pam['line']))\n\nplt.figure(figsize = (10, 15))\nplt.imshow(word_cloud, interpolation = \"gaussian\")\nplt.axis(\"off\")\nplt.show()","e8d7c995":"# TextBlob library provides a consistent API for NLP tasks such as POS Tagging, noun-phrase extraction and sentiment analysis\nfrom textblob import TextBlob\n\n# Defining a function to check the sentiment polarity (whether it is positive or negative or neutral)\ndef polarity(text):\n    return TextBlob(text).sentiment.polarity\n\noffice['polarity_score'] = office['line'].\\\n   apply(lambda x : polarity(x))\noffice['polarity_score'].hist()","0cf81d18":"# Defining a function to classify the sentiment based on the polarity \ndef sentiment(x):\n    if x<0:\n        return 'neg'\n    elif x==0:\n        return 'neu'\n    else:\n        return 'pos'\n    \noffice['polarity'] = office['polarity_score'].\\\n   map(lambda x: sentiment(x))\n\nplt.bar(office.polarity.value_counts().index,\n        office.polarity.value_counts())","d4f20f63":"# Printing dialogues having a positive sentiment\noffice[office['polarity'] == 'pos']['line'].head()","2c14ad7c":"# Printing dialogues having a negative sentiment\noffice[office['polarity'] == 'neg']['line'].head()","b77f0934":"# Printing dialogues having a neutral sentiment\noffice[office['polarity'] == 'neu']['line'].head()","3e39128a":"## Most used words by Pam","e3bc80f3":"## Most frequently used words in the series","909dc3c9":"![image.png](attachment:image.png)","772fc259":"Here we can see that the number of words in most of the dialogues range from 0 to 50. While  ost of the dialogues have the range of 0 to 25","bb8e1c12":"## Visualizing the most frequently used bigrams","70c7c6f6":"# Sentiment Analysis using TextBlob","4e8254eb":"## Most used words by Dwight","cd7b7ae3":"## Visualizing the most frequently used trigrams","dbfdf3ee":"## **About The Show**","6fd1a69e":"We can see here that Pam calls Michael and Dwight more than Jim, even though Jim and Pam were dating and later got married in the show. One reason for this can be that, Pam worked as a receptionist at Dunder Mifflin, and mostly handled calls for Michael who was the Regional Manager, while Dwight was the 'Assistant to the Regional Manager'. Also, we can see that she has used words like 'Love', 'Baby' to address Jim.","a728f067":"## Lead Characters in the Series ","8e79fc78":"## Exploratory Data Analysis ","9a24016e":"We can see that the data set has 6 attributes, namely: the season of the show, episode number for each dialogue, scene number for every episode, the speaker or the character in the show and the line or dialogue the person speaks","88917e6b":"The Office is an American mockumentary sitcom television series that depicts the everyday lives of office employees in the Scranton, Pennsylvania, branch of the fictional Dunder Mifflin Paper Company. It aired on NBC from March 24, 2005, to May 16, 2013, lasting a total of nine seasons. It is an adaptation of the 2001-2003 BBC series of the same name, being adapted for American television by Greg Daniels, a veteran writer for Saturday Night Live, King of the Hill, and The Simpsons. It was co-produced by Daniels's Deedle-Dee Productions, and Reveille Productions (later Shine America), in association with Universal Television. The original executive producers were Daniels, Howard Klein, Ben Silverman, Ricky Gervais, and Stephen Merchant, with numerous others being promoted in later seasons.","ee43d42e":"## Removing contractions from the dialogues","ecf0ce59":"Here we can see the lines spoken by Michael, and he has spoken a total of 10712 lines in the entire series","07a1f289":"## Most used words by Jim","82cc1676":"## Distribution of positive, negative and neutral sentiments","feece0eb":"## Most used words by Michael ","4e77630a":"### Sentiment score distribution of the dialogues","01faecc5":"## Dialogues having neutral sentiment","055f65ac":"We can see that there are no missing values in the dataset","90c8bdb3":"## Dialogues having negative sentiment","766197ff":"## Dialogues having positive sentiment ","6e3dbcf8":"\nLike its British counterpart, the series was filmed in a single-camera setup without a studio audience or a laugh track in order to simulate the look of an actual documentary. The series debuted on NBC as a mid-season replacement and aired 201 episodes over the course of its run. The Office originally featured Steve Carell, Rainn Wilson, John Krasinski, Jenna Fischer, and B. J. Novak as the main cast; however, the series experienced numerous changes to its ensemble cast during its run. Notable stars outside the original main cast include Ed Helms, Mindy Kaling, Craig Robinson, James Spader, Ellie Kemper, and Catherine Tate.","c6c08885":"The lead characters in the series are:<br>\n1. Michael\n2. Dwight\n3. Jim \n4. Pam\n5. Ryan\n6. Andy\n7. Stanley\n8. Kevin\n9. Angela \n10. Oscar\n11. Phyllis\n12. Jan, and \n13. Kelly"}}