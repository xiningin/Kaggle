{"cell_type":{"ccdbe135":"code","aab29104":"code","e730aa8b":"code","ed47f708":"code","fda72a77":"code","cc8d0672":"code","af1f312c":"code","02d0d60d":"code","5a124ff1":"code","ce77afed":"code","e0ef77ea":"code","3b6a79f8":"code","1c3b07e9":"code","30f54e2d":"code","865a568e":"code","0be6070c":"markdown","9691578c":"markdown","0fc7de21":"markdown"},"source":{"ccdbe135":"# LOAD DATASET\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n","aab29104":"import os\nimport json\nfrom pprint import pprint\nfrom copy import deepcopy\n\nimport numpy as np\nimport pandas as pd\nfrom tqdm.notebook import tqdm\n\ndef format_body(body_text):\n    texts = [(di['section'], di['text']) for di in body_text]\n    texts_di = {di['section']: \"\" for di in body_text}\n    \n    for section, text in texts:\n        texts_di[section] += text\n\n    body = \"\"\n\n    for section, text in texts_di.items():\n        body += section\n        body += \"\\n\\n\"\n        body += text\n        body += \"\\n\\n\"\n    \n    return body\n\n\ndef load_files(dirname):\n    filenames = os.listdir(dirname)\n    raw_files = []\n\n    for filename in tqdm(filenames):\n        filename = dirname + filename\n        file = json.load(open(filename, 'rb'))\n        raw_files.append(file)\n    \n    return raw_files\n\ndef generate_clean_df(all_files):\n    cleaned_files = []\n    \n    for file in tqdm(all_files):\n        features = [\n            file['paper_id'],\n            file['metadata']['title'],\n           # format_authors(file['metadata']['authors']),\n           # format_authors(file['metadata']['authors'], \n            #               with_affiliation=True),\n            format_body(file['abstract']),\n            format_body(file['body_text']),\n            #format_bib(file['bib_entries']),\n           # file['metadata']['authors'],\n            #file['bib_entries']\n        ]\n\n        cleaned_files.append(features)\n\n    col_names = ['paper_id', 'title', \n                 'abstract', 'text', \n                 ]\n\n    clean_df = pd.DataFrame(cleaned_files, columns=col_names)\n    clean_df.head()\n    \n    return clean_df\n\n","e730aa8b":"\n# LOAD DIRECTORY, PARSE AND PRINT TARGET JSON SUB-TEXT\n\n\nbiorxiv_dir = '\/kaggle\/input\/CORD-19-research-challenge\/biorxiv_medrxiv\/biorxiv_medrxiv\/pdf_json\/'\nfilenames = os.listdir(biorxiv_dir)\nprint(\"Number of articles retrieved from biorxiv:\", len(filenames))\n\nall_files = []\n\nfor filename in filenames:\n    filename = biorxiv_dir + filename\n    file = json.load(open(filename, 'rb'))\n    all_files.append(file)\n    \n#print('Test', all_files)\n    \nfile = all_files[0]\nprint(\"Dictionary keys:\", file.keys())\n\npprint(file['abstract'])","ed47f708":"\n# FORMAT AND PRINT TARGET KEY\/VALUES\n\n\nprint(\"body_text type:\", type(file['body_text']))\nprint(\"body_text length:\", len(file['body_text']))\nprint(\"body_text keys:\", file['body_text'][0].keys())\n\nprint(\"body_text content:\")\npprint(file['body_text'][:2], depth=3)\n\ntexts = [(di['section'], di['text']) for di in file['body_text']]\ntexts_di = {di['section']: \"\" for di in file['body_text']}\nfor section, text in texts:\n    texts_di[section] += text\n\npprint(list(texts_di.keys()))\n\nbody = \"\"\n\nfor section, text in texts_di.items():\n    body += section\n    body += \"\\n\\n\"\n    body += text\n    body += \"\\n\\n\"\n\n#print(body)","fda72a77":"# PRINT CUSTOMIZED JSON TARGET KEY TEXT FROM SINGLE FILE\nprint(format_body(file['body_text']))","cc8d0672":"\n# BUILD CUSTOM COLUMN LIST FROM ALL FILES \n\ncleaned_files = []\n\nfor file in tqdm(all_files):\n    features = [\n        file['paper_id'],\n        file['metadata']['title'],\n        #format_authors(file['metadata']['authors']),\n        #format_authors(file['metadata']['authors'], \n        #              with_affiliation=True),\n        format_body(file['abstract']),\n        format_body(file['body_text']),\n       # format_bib(file['bib_entries']),\n       # file['metadata']['authors'],\n       # file['bib_entries']\n    ]\n    \n    cleaned_files.append(features)","af1f312c":"# LOAD CUSTOM LIST INTO CSV PD DATAFRAME\n\ncol_names = [\n    'paper_id', \n    'title', \n    #'authors',\n    #'affiliations', \n    'abstract', \n    'text', \n    #'bibliography',\n    #'raw_authors',\n    #'raw_bibliography'\n]\n\nclean_df = pd.DataFrame(cleaned_files, columns=col_names)\nclean_df.head()","02d0d60d":"# LOAD DIRECTORY , CLEAN, AND CREATE CSV DATAFRAME\n\npmc_dir = '\/kaggle\/input\/CORD-19-research-challenge\/custom_license\/custom_license\/pdf_json\/'\npmc_files = load_files(pmc_dir)\npmc_df = generate_clean_df(pmc_files)\npmc_df.to_csv('clean_pmc.csv', index=False)\npmc_df.head()","5a124ff1":"# FIND MOST FREQUENT WORD WITHIN TARGET COLUMN\n\nimport nltk\nfrom nltk import FreqDist\nimport pandas as pd\npd.set_option(\"display.max_colwidth\", 200)\nimport numpy as np\nimport re\nimport spacy\nimport gensim\nfrom gensim import corpora\n\n# libraries for visualization\nimport pyLDAvis\nimport pyLDAvis.gensim\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\n\n# function to plot most frequent terms\ndef freq_words(x, terms = 30):\n  all_words = ' '.join([text for text in x])\n  all_words = all_words.split()\n\n  fdist = FreqDist(all_words)\n  words_df = pd.DataFrame({'word':list(fdist.keys()), 'count':list(fdist.values())})\n\n  # selecting top 20 most frequent words\n  d = words_df.nlargest(columns=\"count\", n = terms) \n  plt.figure(figsize=(20,5))\n  ax = sns.barplot(data=d, x= \"word\", y = \"count\")\n  ax.set(ylabel = 'Count')\n  plt.show()\n    \n\n# TARGET COLUMN\nfreq_words(pmc_df['abstract'])\n\npmc_df['abstract'] = pmc_df['abstract'].str.replace(\"[^a-zA-Z#]\", \" \")\n","ce77afed":"# REMOVE STOP-WORDS \n\nfrom nltk.corpus import stopwords\nstop_words = stopwords.words('english')\n\n# function to remove stopwords\ndef remove_stopwords(rev):\n    rev_new = \" \".join([i for i in rev if i not in stop_words])\n    return rev_new\n\n# remove short words (length < 3)\npmc_df['abstract'] = pmc_df['abstract'].apply(lambda x: ' '.join([w for w in x.split() if len(w)>2]))\n\n# remove stopwords from the text\nclean_text = [remove_stopwords(r.split()) for r in pmc_df['abstract']]\n\n# make entire text lowercase\nclean_text = [r.lower() for r in clean_text]\n\nfreq_words(clean_text, 35)","e0ef77ea":"# Lemmatize text\n\nnlp = spacy.load('en', disable=['parser', 'ner'])\n\ndef lemmatization(texts, tags=['NOUN', 'ADJ']): # filter noun and adjective\n       output = []\n       for sent in texts:\n             doc = nlp(\" \".join(sent)) \n             output.append([token.lemma_ for token in doc if token.pos_ in tags])\n       return output\n\n\ntokenized_text = pd.Series(clean_text).apply(lambda x: x.split())\nprint(tokenized_text[1])\n\nlemmat_text = lemmatization(tokenized_text)\nprint(lemmat_text[1]) # print lemmatized review","3b6a79f8":"# MOST RELAVENT FREQUENT WORDS\n\nfreq_list = []\nfor i in range(len(lemmat_text)):\n    freq_list.append(' '.join(lemmat_text[i]))\n\npmc_df['abstract'] = freq_list\n\nfreq_words(pmc_df['abstract'], 35)","1c3b07e9":"# Building LDA Model to find Topics \n\ndictionary = corpora.Dictionary(lemmat_text)\n\ndoc_term_matrix = [dictionary.doc2bow(rev) for rev in lemmat_text]\n\n# Creating the object for LDA model using gensim library\nLDA = gensim.models.ldamodel.LdaModel\n\n# Build LDA model\nlda_model = LDA(corpus=doc_term_matrix, id2word=dictionary, num_topics=7, random_state=100,\n                chunksize=1000, passes=50)","30f54e2d":"# PRINT FINAL TOPICS\n\nlda_model.print_topics()","865a568e":"# Visualize the topics\n\npyLDAvis.enable_notebook()\nvis = pyLDAvis.gensim.prepare(lda_model, doc_term_matrix, dictionary)\nvis","0be6070c":"# TOPIC MODELING","9691578c":"# Convert Large DataSet of JSON files to custom CSV DataFrame file","0fc7de21":"*Helper Functions*"}}