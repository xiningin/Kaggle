{"cell_type":{"7f9a1286":"code","6e0eb37c":"code","59e558fa":"code","06ec981d":"code","fc291d6c":"code","957e66bb":"code","a140fc8b":"code","3321195d":"code","3b7c6dd3":"code","64545dbc":"code","3c5dd62f":"code","be0a1b89":"code","de2a02dd":"code","be07121c":"code","37944dd6":"code","6654349b":"code","5d008ef6":"code","c3fcb769":"code","c278a19e":"code","7d49ef58":"code","ab45d72e":"code","9c355ba6":"markdown","bb291f83":"markdown","088619d3":"markdown","452f65be":"markdown","fb96ee31":"markdown","76b0b343":"markdown","14b0cbd4":"markdown","c6f8d567":"markdown","7c10588d":"markdown","23bd373c":"markdown"},"source":{"7f9a1286":"#K\u00fct\u00fcphaneleri y\u00fckl\u00fcyoruz: Numpy, Pandas, Matplotlib, Seaborn, Sklearn\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.metrics import accuracy_score as ass\nfrom sklearn.metrics import r2_score\nfrom sklearn.model_selection import cross_validate\nfrom sklearn.metrics import make_scorer\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.linear_model import SGDClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.naive_bayes import BernoulliNB\nfrom sklearn.naive_bayes import MultinomialNB\n\n#Linear Discriminant Analysis k\u00fct\u00fcphaneleri\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\nfrom sklearn.covariance import LedoitWolf\nfrom sklearn.covariance import MinCovDet\nfrom sklearn.covariance import OAS\nfrom sklearn.covariance import GraphicalLasso","6e0eb37c":"data=pd.read_csv('\/kaggle\/input\/iris\/Iris.csv')\ndata.head(), print(data.columns),print(type(data)), print(\"data shape is :\" ,data.shape)","59e558fa":"data = data.iloc[:,1:]\ndata.head(), print(data.columns), print(\"new data shape is :\" ,data.shape)","06ec981d":"# #Her s\u0131n\u0131f\u0131n ortalamalar\u0131n\u0131 histogram olarak g\u00f6steriyoruz.\ndata_mean = data.groupby(data.Species).mean()\ndata_mean.plot.bar()\nplt.title(\"Iris Data Setini \u0130nceliyoruz\")\nplt.xlabel(\"S\u0131n\u0131flar\")\nplt.ylabel(\"De\u011ferler\")\nplt.show()\n\n\n#Da\u011f\u0131l\u0131m\u0131na bakmak istiyorum: Gaussian Distribution\ndata.plot.kde()","fc291d6c":"korelasyon=data.corr()\nfigure, axis=plt.subplots(figsize=(10,10))\nsns.heatmap(korelasyon, annot=True)","957e66bb":"#2 boyutta da\u011f\u0131l\u0131m\u0131n\u0131 g\u00f6rmek istiyorum.\n\nsns.relplot(data=data, x=\"SepalWidthCm\", y=\"PetalLengthCm\" ,hue=\"Species\" )\nsns.relplot(data=data, x=\"SepalLengthCm\", y=\"PetalLengthCm\" ,hue=\"Species\" )\nsns.relplot(data=data, x=\"SepalWidthCm\", y=\"PetalWidthCm\" ,hue=\"Species\" )\nsns.relplot(data=data, x=\"SepalLengthCm\", y=\"PetalWidthCm\" ,hue=\"Species\" )","a140fc8b":"#S\u0131n\u0131flara g\u00f6re data da\u011f\u0131l\u0131m\u0131n\u0131 g\u00f6rmek istiyorum. Seaborn pairplpt kullanaca\u011f\u0131m.\n#Toplu g\u00f6relim\n\nsns.pairplot(data, hue=\"Species\")\n","3321195d":"sns.boxplot(x=\"Species\", y=\"PetalLengthCm\",  data=data)\n","3b7c6dd3":"x = data.iloc[:,:4]\ny = data.iloc[:,4:]","64545dbc":"k_nn=KNeighborsClassifier(n_neighbors=3, metric=\"chebyshev\")\nlogi = LogisticRegression(random_state=5)\nDT = DecisionTreeClassifier(max_features=\"sqrt\")\nSDF = SGDClassifier(penalty=\"l2\", random_state=10)\nS_VC= SVC(degree=3,C=8, kernel=\"rbf\")\nRF= RandomForestClassifier(n_estimators=78, criterion= \"gini\") # criterion = \"gini\" or \"entropy\"\nBayes=  GaussianNB()\nMBayes = MultinomialNB()\nBBayes = BernoulliNB()\nLDA = LinearDiscriminantAnalysis(solver=\"eigen\")    #solver= \u2018svd\u2019, \u2018lsqr\u2019, \u2018eigen\u2019\n","3c5dd62f":"cv_sonuc= cross_validate(SDF, x, y, cv=5 , scoring='accuracy')\nres=cv_sonuc['test_score'].mean()\nprint(\"Accuracy of SDG: \", res*100, \"%\")\n\nResult = []\nResult.append( \"SDG :\")\nResult.append( res)\n","be0a1b89":"cv_sonuc= cross_validate(k_nn, x, y, cv=5 , scoring='accuracy')\nres=cv_sonuc['test_score'].mean()\nprint(\"Accuracy of KNN: \", res*100, \"%\")\nResult.append( \"KNN :\")\nResult.append( res)","de2a02dd":"cv_sonuc= cross_validate(logi, x, y, cv=5 , scoring='accuracy')\nres=cv_sonuc['test_score'].mean()\nprint(\"Accuracy of Logistic Regression: \", res*100, \"%\")\nResult.append( \"LR :\")\nResult.append( res)","be07121c":"cv_sonuc= cross_validate(DT, x, y, cv=5 , scoring='accuracy')\nres=cv_sonuc['test_score'].mean()\n\nprint(\"Accuracy of Decision Tree: \", res*100, \"%\")\nResult.append( \"DT :\")\nResult.append( res)","37944dd6":"cv_sonuc= cross_validate(S_VC, x, y, cv=5 , scoring='accuracy')\nres=cv_sonuc['test_score'].mean()\nprint(\"Accuracy of Support Vector Classifier: \", res*100, \"%\")\nResult.append( \"SVC :\")\nResult.append( res)","6654349b":"cv_sonuc= cross_validate(RF, x, y, cv=5 , scoring='accuracy')\nres=cv_sonuc['test_score'].mean()\nprint(\"Accuracy of Random Forest: \", res*100, \"%\")\nResult.append( \"RF :\")\nResult.append( res)","5d008ef6":"cv_sonuc= cross_validate(Bayes, x, y, cv=5 , scoring='accuracy')\nres=cv_sonuc['test_score'].mean()\nprint(\"Accuracy of Naive Bayes: \", res*100, \"%\")\nResult.append( \"NB :\")\nResult.append( res)","c3fcb769":"cv_sonuc= cross_validate(MBayes, x, y, cv=5 , scoring='accuracy')\nres=cv_sonuc['test_score'].mean()\nprint(\"Accuracy of Multinomial Naive Bayes: \", res*100, \"%\")\nResult.append( \"MNB :\")\nResult.append( res)","c278a19e":"cv_sonuc= cross_validate(BBayes, x, y, cv=5 , scoring='accuracy')\nres=cv_sonuc['test_score'].mean()\nprint(\"Accuracy of Bernoulli Naive Bayes: \", res*100, \"%\")\nResult.append( \"BNB :\")\nResult.append( res)","7d49ef58":"cv_sonuc= cross_validate(LDA, x, y, cv=5 , scoring='accuracy')\nres=cv_sonuc['test_score'].mean()\nprint(\"Accuracy of Linear Discriminant Analysis: \", res*100, \"%\")\nResult.append( \"LDA :\")\nResult.append( res)","ab45d72e":"print(Result)","9c355ba6":"Outlier'lar\u0131 bulmak ise bir ba\u015fka \u00f6nemli ad\u0131m olmaktad\u0131r. Outlier bulmak i\u00e7in bir \u00e7ok y\u00f6ntem bulunmaktad\u0131r. Benim \u00f6nerece\u011fim y\u00f6ntem boxplot ile bunu kontrol etmek olacakt\u0131r. Box plot, Interquartile range'i (IQR) temel alarak veri setinin da\u011f\u0131l\u0131m\u0131 \u00fczerinde median merkezi ile ilk \u00e7eyrek ve 3. \u00e7eyre\u011fe kutu i\u00e7ine almaktad\u0131r. Outlier ise Q1-1.5*IQR (minimum) ve Q3+1.5*IQR (maximum) s\u0131n\u0131rlar\u0131 d\u0131\u015f\u0131nda kalan de\u011ferler olmaktad\u0131r. \u0130statistiksel yakla\u015f\u0131m kullan\u0131labilecek g\u00fczel bir silaht\u0131r.\n\n![boxplot](https:\/\/miro.medium.com\/max\/18000\/1*2c21SkzJMf3frPXPAR_gZA.png)","bb291f83":"Korelasyon ger\u00e7ekten data setlerinde \u00f6nemli bir konudur. Feature engineering ismi de verilen bu kapsam b\u00fcnyesinde output (y) de\u011ferini en iyi anlamland\u0131racak \u00f6zniteliklerin se\u00e7ilmesi gerekmektedir. Se\u00e7ilen \u00f6z niteliklerin hedef de\u011feri en iyi a\u00e7\u0131kl\u0131yor olmas\u0131 gerekirken, \u00f6z niteliklerin kendi aralar\u0131nda y\u00fcksek korele olmamas\u0131 gerekmektedir. Korele olan \u00f6zniteliklerin model i\u00e7erisinde bulunmas\u0131 durumunda modelin ba\u015far\u0131m\u0131 d\u00fc\u015fece\u011fi gibi, modelin ifade edilebilinirli\u011fi (a\u00e7\u0131klanabilirli\u011fi) de d\u00fc\u015fmektedir.  \n\nA\u015fa\u011f\u0131da heatmap \u015feklinde \u00f6zniteliklerin korelasyon matrixini seaborn k\u00fct\u00fcphanesi vas\u0131tas\u0131yla g\u00f6rselle\u015ftiriyoruz. Bak\u0131ld\u0131\u011f\u0131 zaman petalLength \u00f6z niteli\u011finin petalwidth ve sepal length ile korele oldu\u011fudur.Benzer durum di\u011fer \u00f6znitelikler i\u00e7inde ge\u00e7erlidir fakat g\u00f6r\u00fcld\u00fc\u011f\u00fc \u00fczere bir \u00f6z nitelik t\u00fcm \u00f6znitelikler ile korele olmad\u0131\u011f\u0131 s\u00fcrece halen bir \u015fans\u0131 bulunmaktad\u0131r. Bu performans\u0131 data science gereklili\u011fi olan deneme yan\u0131lma ve istatistiksel metodlar ( p value check gibi) ile denemek gerekmektedir.","088619d3":"Modelleri test - train olarak ay\u0131rarak ba\u015far\u0131m\u0131na karar vermek g\u00fczel bir y\u00f6ntem fakat unutmay\u0131n ki t\u00fcm data setini temsil etmeden ba\u015far\u0131m\u0131 kar\u015f\u0131la\u015ft\u0131rmak eksik olacakt\u0131r. Bundan doaly\u0131 \u00e7apraz ge\u00e7erleme kullanarak sonucu kar\u015f\u0131la\u015ft\u0131rmak her zaman en do\u011frusu olacakt\u0131r. K-Fold Cross Validation denilen yap\u0131 i\u00e7in 5 Fold'luk yap\u0131 kullan\u0131ca\u011f\u0131z. Ba\u015far\u0131m olarak da \"accuracy\" se\u00e7ilmi\u015ftir.","452f65be":"# IRIS Veri seti \u00fczerine ke\u015fifsel veri analizi ve s\u0131n\u0131fland\u0131rma makine \u00f6\u011frenimi y\u00f6ntemlerinin kar\u015f\u0131la\u015ft\u0131r\u0131lmas\u0131\n\n![IRIS](https:\/\/www.almanac.com\/sites\/default\/files\/styles\/primary_image_in_article\/public\/image_nodes\/iris-flowers.jpg?itok=RvCxClOE)\n\n\nIRIS data seti 3 farkl\u0131 \u00e7i\u00e7ek s\u0131n\u0131f\u0131na ait \u00f6l\u00e7\u00fcmler i\u00e7eren olduk\u00e7a eski ama temel bir veri setidir. \u0130ngiliz istatik\u00e7i ve biolog Ronald Fisher'in 1936 y\u0131l\u0131nda yay\u0131nlad\u0131\u011f\u0131 \"The use of multiple measurements in taxonomic problems as an example of linear discriminant analysis[1] \" isimli makalesinde ilk olarak g\u00f6r\u00fclmektedir. Iris setosa, Iris versicolor ve Iris virginica olmak \u00fczere 3 \u00e7i\u00e7ek s\u0131n\u0131f\u0131 bulunmaktad\u0131r. Her bir \u00e7i\u00e7ek s\u0131n\u0131f\u0131 i\u00e7in, \u00e7i\u00e7e\u011fin uzun yapra\u011f\u0131 Petal ve geni\u015f yapra\u011f\u0131 Sepal olarak adland\u0131r\u0131lmaktad\u0131r. Fisher bu iki yaprak i\u00e7in uzunluk ve geni\u015flik de\u011ferlerini her bir \u00e7i\u00e7ek t\u00fcr\u00fc i\u00e7in 50 farkl\u0131 \u00f6rnekte toplam\u0131\u015ft\u0131r. Toplam 150 veri bar\u0131nd\u0131ran data seti temel olarak (150,4) boyutludur.\n\n![Petal & Sepal](https:\/\/encrypted-tbn0.gstatic.com\/images?q=tbn:ANd9GcRaU43Ra3xTYTpU2XLBw5yl20Qty3OCJzjDBw&usqp=CAU)\n\nBu \u00e7al\u0131\u015fman\u0131n amac\u0131 makine \u00f6\u011frenimi ve ke\u015fifsel veri analizi (Exploratory Data Analysis)'ne giri\u015f yapan ara\u015ft\u0131rmac\u0131lar ve ilgililer i\u00e7in haz\u0131rlanm\u0131\u015ft\u0131r. Dil olarak \u00f6zellikle T\u00fcrk\u00e7e se\u00e7ilmi\u015ftir. Ama\u00e7 T\u00fcrk ara\u015ft\u0131rmac\u0131lara, ilgililere ve literat\u00fcr\u00fcne katk\u0131 sa\u011flamakt\u0131r. \u0130lk k\u0131s\u0131mda veri setini anlamak i\u00e7in \u00e7e\u015fitli istatistiksel de\u011ferler kar\u015f\u0131la\u015ft\u0131r\u0131lacakt\u0131r. \u0130kinci k\u0131sm\u0131nda ise s\u0131n\u0131fland\u0131rma algoritmalar\u0131 uygulanarak ba\u015far\u0131mlar\u0131 kar\u015f\u0131la\u015ft\u0131r\u0131lacakt\u0131r. Umar\u0131m faydal\u0131 bir i\u00e7erik olur. Takip etmeyi unutmay\u0131n\u0131z :)\n\n[1]  R. A. Fisher (1936). \"The use of multiple measurements in taxonomic problems\". Annals of Eugenics. 7 (2): 179\u2013188. doi:10.1111\/j.1469-1809.1936.tb02137.x. hdl:2440\/15227.","fb96ee31":"Sonu\u00e7lar\u0131 inceledi\u011fimizde 0.987'lik ba\u015far\u0131m ile Support vector machines algoritmas\u0131n\u0131 ( degree=3,C=8, kernel=\"rbf\" hiper parametreleri ) \u00f6nde gitmektedir.\n\n* SDG: 0.80\n* KNN: 0.97\n* Logistic Regression: 0.97\n* Decision Tree: 0.93\n* Support Vector Classifier: 0.98\n* Random Forest:: 0.96\n* Naive Bayes - Gauissian: 0.95\n* Naive Bayes - Multinomial: 0.95\n* Naive Bayes - Bernoulli: 0.33\n* Linear Discriminant Analysis: 0.90","76b0b343":"# Makine \u00f6\u011frenmesi modelleri ile Iris veri setinin incelenmesi ve modellerin kurulmas\u0131\n\n\u00d6ncelikli olarak x ve y olarak \u00f6znitelikler ile \u00e7\u0131kt\u0131 de\u011ferlerini ayr\u0131 dataframe'lere ay\u0131r\u0131yoruz.\n","14b0cbd4":"T\u00fcm \u00f6z niteliklerin s\u0131n\u0131flara g\u00f6re 2D da\u011f\u0131l\u0131m\u0131na bakt\u0131\u011f\u0131m\u0131zda ise Iris setosa'n\u0131n di\u011fer iki s\u0131n\u0131fa g\u00f6re kolayca ayr\u0131\u015ft\u0131r\u0131labilece\u011fi g\u00f6r\u00fclmektedir. Versicolor ve virginica ise ay\u0131rmas\u0131 biraz daha zor g\u00f6r\u00fcnmektedir. Lakin unutmayal\u0131m 2D grafik da\u011f\u0131l\u0131m\u0131na nazaran 4 boyutlu bir problemle u\u011fra\u015f\u0131yoruz, 2 boyutta ayr\u0131\u015ft\u0131r\u0131lamayan veri 4 boyutta kolayl\u0131kla ayr\u0131\u015ft\u0131r\u0131labilir (do\u011fru \u00f6znitelikler se\u00e7ildiyse).","c6f8d567":"Veri seti orjinal olarak csv (comma seperated values) \u015feklide bulunmaktad\u0131r. Pandas k\u00fct\u00fcphanesi ile y\u00fckl\u00fcyoruz. Pandas data frame olarak ald\u0131\u011f\u0131m\u0131z veriseti 150,6 boyutlu g\u00f6r\u00fcnmektedir. Sut\u00fcnlar\u0131 inceledi\u011fimizde bir tanesi data frame'in do\u011fas\u0131ndan gelen s\u0131ralama oldu\u011funu, bir tanesini ise \" Id \" isimli bir sut\u00fcn oldu\u011fu g\u00f6r\u00fclmektedir.","7c10588d":"Kar\u015f\u0131la\u015ft\u0131raca\u011f\u0131m\u0131z modeller:\n\n* K Nearest Neighbours\n* Logistic Regression\n* Decision Tree\n* Stochastic Gradient Descent \n* Support Vector Classifier \n* Random Forest\n* Gaussien Naive Bayes\n* Multinomial Naive Bayes\n* Bernoulli Naive Bayes\n* Linear Discriminant Anaylsis\n\nHer bir modelin detayl\u0131 incelemesini yapmasak da, modeller i\u00e7erisindeki cross-validation (\u00e7apraz ge\u00e7erleme) ile belirlenmesi gereken parametreler oldu\u011funu unutmamak laz\u0131m. Bu parametrelere hiper-parametre (hyperparameter) demekteyiz. Bu parametreleri el ile deneyerek veya grid-search dedi\u011fimiz yap\u0131lar ile yapmam\u0131z gerekmektedir.","23bd373c":"Id Sut\u00fcnunu .iloc komutu ile d\u00fc\u015f\u00fcr\u00fcyoruz."}}