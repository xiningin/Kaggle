{"cell_type":{"4be102cb":"code","a6fda7b9":"code","2b21c433":"code","5a44b0c3":"code","7958b187":"code","a902b679":"code","cd7be3e0":"code","d3cfda7c":"code","67a61553":"code","43588483":"code","bce5d3bd":"code","0c63cf58":"code","e115f769":"code","cbc2cd9b":"code","72ca1e39":"code","db4a4772":"code","802b44b9":"code","a21253f2":"code","db358b72":"code","1c3d5c8b":"code","71c19ea5":"code","f7d89661":"markdown","cf705e6d":"markdown","feff0a74":"markdown","f6e0c8c2":"markdown","f4e75093":"markdown","0fad3f83":"markdown","6dc61bfa":"markdown","d5465a13":"markdown","e317db3b":"markdown","c2167832":"markdown","b1595938":"markdown","ee59bc37":"markdown","94a78def":"markdown","bcd3683a":"markdown","ea87edef":"markdown","4dcfcad3":"markdown","50e69bd9":"markdown","b6ad94fe":"markdown","02cf4fe8":"markdown","57c7a72c":"markdown","20c296c6":"markdown","3083f89d":"markdown","6a161b6d":"markdown"},"source":{"4be102cb":"import matplotlib.pyplot as plt\nimport seaborn as sns\nimport pandas as pd\nimport numpy as np\nimport gc","a6fda7b9":"url = 'https:\/\/raw.githubusercontent.com\/malikrb\/HumanResourcesDemonstration\/master\/data\/hr_data.csv'\ndf = pd.read_csv(url)","2b21c433":"with pd.option_context('display.max_columns', 35):\n    display(df.head(6))","5a44b0c3":"# Bin the Data into ['Close', 'Middle', 'Far']\ntemp = df.copy()\ntemp['DistanceFromHome'] = pd.cut(temp['DistanceFromHome'], 3, labels=['Closest', 'Middle', 'Farthest'])\n\ntemp = pd.concat([temp, pd.get_dummies(temp['Attrition'], prefix='Attrition')], axis=1)\n\n# Visualize Attrition\ndisplay(temp.groupby(['DistanceFromHome'])[['Attrition_No', 'Attrition_Yes']].sum())","7958b187":"display(temp.groupby(['Department'])[['Attrition_No', 'Attrition_Yes']].sum())\nprint('--------------------------------------------')\ndisplay(temp.groupby(['JobRole'])[['Attrition_No', 'Attrition_Yes']].sum())\nprint('-------------------------------------------------------------------')\ndisplay(temp.groupby(['Department', 'JobRole'])[['Attrition_No', 'Attrition_Yes']].sum())\n\ndel temp\ngc.collect();","a902b679":"from sklearn.preprocessing import LabelEncoder\n\ndf['Attrition'] = LabelEncoder().fit_transform(df['Attrition'])\ndf = df.drop(['EmployeeNumber', 'EmployeeCount', 'Over18', 'StandardHours'], axis=1)\n\nwith pd.option_context('display.max_columns', 35):\n    display(df.head())","cd7be3e0":"plt.style.use('seaborn')\n\ndf.hist(bins=30, figsize=(22,22), edgecolor='w')\n\nplt.show()","d3cfda7c":"f, ax = plt.subplots(1, 1, figsize=(18,8))\nsns.countplot(x='Age', hue='Attrition', data=df,\n              edgecolor='w', linewidth=1.15)\nplt.title('Attrition by Age', size=26, y=1.05)\nplt.xlabel('Age', size=20)\nplt.ylabel('Count', size=20)\n\nplt.legend(['Stayed', 'Left'])\n\nplt.show()","67a61553":"f, axes = plt.subplots(4, 1, figsize=(18,18), tight_layout=True)\n\nx_vars = ['JobRole', 'MaritalStatus', 'JobLevel', 'StockOptionLevel']\nfor ax, x in zip(axes, x_vars):\n    sns.countplot(x=x, hue='Attrition', data=df, ax=ax)\n    ax.set_xlabel(x, size=16)\n    ax.set_ylabel(\"\")\n    ax.legend(['Stayed', 'Left'])\n\nf.text(x=-0.0275, y=0.5, s='Count', rotation=90, size=22)\n\nplt.show()\n\ndel x_vars\ngc.collect();","43588483":"f, (ax1, ax2) = plt.subplots(1, 2, figsize=(16,6), tight_layout=True)\n\n# Split the DataFrame by Attrition\nattrition_0 = df.loc[df['Attrition'] == 0]\nattrition_1 = df.loc[df['Attrition'] == 1]\n\nsns.kdeplot(attrition_0['DistanceFromHome'], label='Stayed',\n            shade=True, ax=ax1, alpha=0.6)\nsns.kdeplot(attrition_1['DistanceFromHome'], label='Left',\n            shade=True, ax=ax1, alpha=0.6)\nax1.set_xlabel('Distance From Home', size=18)\n\nsns.kdeplot(attrition_0['YearsWithCurrManager'], label='Stayed',\n            shade=True, ax=ax2, alpha=0.6)\nsns.kdeplot(attrition_1['YearsWithCurrManager'], label='Left',\n            shade=True, ax=ax2, alpha=0.6)\nax2.set_xlabel('Years With Current Manager', size=18)\n\nplt.show()\n\ndel attrition_0, attrition_1\ngc.collect();","bce5d3bd":"# Encode all 'object' columns\ncolumns = df.select_dtypes(include='object').columns\nfor col in columns:\n    df[col] = LabelEncoder().fit_transform(df[col])\n    \ndf = pd.get_dummies(df)\n    \ndel columns\ngc.collect();","0c63cf58":"df","e115f769":"# Splitting the DataFrame into train test\nfrom sklearn.model_selection import train_test_split\n\nX = df.drop('Attrition', axis=1)\ny = df['Attrition']\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)","cbc2cd9b":"from sklearn.model_selection import GridSearchCV\nfrom sklearn.metrics import accuracy_score\nfrom multiprocessing import cpu_count\n\ndef algorithm_pipeline(X_train_data, X_test_data, y_train_data, y_test_data, \n                       model, param_grid, cv=10, scoring_fit='accuracy',\n                       do_probabilities=False):\n    gs = GridSearchCV(\n        estimator=model,\n        param_grid=param_grid, \n        cv=cv,  \n        scoring=scoring_fit,\n        verbose=2,\n        n_jobs=cpu_count()\/\/2,\n    )\n    fitted_model = gs.fit(X_train_data, y_train_data)\n    \n    if do_probabilities:\n      pred = fitted_model.predict_proba(X_test_data)\n    else:\n      pred = fitted_model.predict(X_test_data)\n    \n    score = accuracy_score(pred, y_test)\n    \n    return fitted_model, pred, score","72ca1e39":"from xgboost import XGBClassifier\n\nparam_grid = {\n    'colsample_bytree': [0.7],\n    'learning_rate': [0.01],\n    'max_depth': [5],\n    'n_estimators': [500],\n    'reg_alpha': [1.1],\n    'reg_lambda': [1.2],\n    'subsample': [0.8],\n#     'colsample_bytree': [0.7, 0.8],\n#     'learning_rate': [0.01, 0.05],\n#     'n_estimators': [500, 1000],\n#     'max_depth': [5, 10],\n#     'reg_alpha': [1.1, 1.2, 1.3],\n#     'reg_lambda': [1.1, 1.2, 1.3],\n#     'subsample': [0.7, 0.8, 0.9]\n}\n\nmodel = XGBClassifier()\n\nxgb_model, xgb_pred, xgb_score = algorithm_pipeline(X_train, X_test, y_train, y_test, model, \n                                        param_grid, cv=5, scoring_fit='accuracy')\n\nprint(xgb_model.best_score_)\nprint(xgb_model.best_params_)\nprint(f'xgb_score: {xgb_score}')","db4a4772":"from tensorflow.keras import Sequential\nfrom tensorflow.keras.layers import Dense\n\ndef build_model(activation='relu', optimizer='adam', dropout_rate=0.2):\n    model = Sequential()\n    model.add(Dense(500, activation=activation, input_shape=(30, )))\n    model.add(Dense(500, activation=activation))\n    model.add(Dense(500, activation=activation))\n    model.add(Dense(1, activation='sigmoid'))\n    \n    model.compile(\n        loss='binary_crossentropy',\n        optimizer=optimizer,\n        metrics=['accuracy']\n    )\n    \n    return model","802b44b9":"from tensorflow.keras.wrappers.scikit_learn import KerasClassifier\n\nparam_grid = {\n    'batch_size':   [50],\n    'epochs':       [125],\n    # 'dropout_rate': [0.2, 0.3],\n    # 'activation':   ['relu', 'elu'],\n    # 'batch_size':   [50, 100, 150],\n    # 'optimizer':    ['Adam', 'Nadam'],\n    # 'epochs':       [25, 75, 125],\n}\n\nmodel = KerasClassifier(build_fn=build_model, verbose=0)\n\nnn_model, nn_pred, nn_score = algorithm_pipeline(X_train, X_test, y_train, y_test, model, \n                                        param_grid, cv=5, scoring_fit='accuracy')\n\nprint(nn_model.best_score_)\nprint(nn_model.best_params_)\nprint(f'nn_score: {nn_score}')","a21253f2":"features = xgb_model.best_estimator_.feature_importances_\ncolumns = X.columns\nsorted_features = sorted(zip(columns, features), key=lambda x: x[1], reverse=True)\n\n## Uncomment lines below to see feature importances\n## with respective column\n# for col, feature in sorted_features:\n#     print(f'{col}: {feature}')","db358b72":"plt.subplots(1, 1, figsize=(14,6), tight_layout=True)\n\nx = [x[0] for x in sorted_features]\nheight = [x[1] for x in sorted_features]\n\nplt.bar(x=x, height=height)\nplt.title('Feature Importances', size=22, y=1.05)\nplt.xticks(rotation=90, ha='right')\nplt.show()","1c3d5c8b":"from sklearn.metrics import roc_curve, auc\n\nxgb_prob = xgb_model.predict_proba(X_test)[:,1]\nfpr, tpr, thresh = roc_curve(y_test, xgb_prob)\nroc_auc = auc(fpr, tpr)\n\nplt.subplots(1, 1, figsize=(6,6), tight_layout=True)\n\nplt.title(f'Recieving Operator Characteristics', size=16, y=1.05)\nplt.plot(fpr, tpr, label=f'XBGClassifier: {roc_auc:.2f}')\nplt.plot([0,1], [0,1], 'k--', alpha=0.6)\n\nplt.legend(frameon=1, facecolor='w')\n\nplt.show()","71c19ea5":"from sklearn.metrics import confusion_matrix\n\nmxN = confusion_matrix(y_test, xgb_pred)\nplt.subplots(1, 1, figsize=(6,6), tight_layout=True)\n\nplt.title('Confusion Matrix', size=18, y=1.025)\nsns.heatmap(mxN, annot=True, fmt='d',\n            cmap='Blues', cbar=False,\n            xticklabels=['Stayed', 'Left'],\n            yticklabels=['Stayed', 'Left']\n           )\n\nplt.xlabel('Actual', size=16)\nplt.ylabel('Predicted', size=16)\nplt.yticks(rotation=0)\n\nplt.show()","f7d89661":"#### Attrition by Age","cf705e6d":"#### Kernel Density Estimation","feff0a74":"## Modeling","f6e0c8c2":"## Data Analysis","f4e75093":"The countplot seems to suggest that the ages with the highest  \n*raw attrition count* are those within the range of *26-35*.\n\nThe highest *rate of attrition* however seems to be those within  \nthe age range of *18-21*.","0fad3f83":"This frame shows that the *raw attrition count* was higher for  \nthose who live the closest to the company, but suprisingly the  \n*Middle* group had the highest attrition rate at *44:215*.\n\nHypothesis 1 seems to be debunk by this observation but we won't  \ncompletely accept this until further investigation is conducted.","6dc61bfa":"#### Neural Network","d5465a13":"From the graphs above, we can see that the distributions of  \nnumerous features are positively skewed. In this context, the graphs  \nsuggest that employees tend to spent quite a large amount of years  \nwith the company, however, the majority of categories seem to have  \na diminishing pattern as they approach their respective maximum.","e317db3b":"## **Quick Navigation**\n- [Introduction](#Introduction)\n- [Hypotheses](#Hypotheses)\n- [Data Analysis](#Data-Analysis)\n    - [Feature Distribution](#Feature-Distribution)\n    - [Attrition by Age](#Attrition-by-Age)\n    - [Kernel Density Estimation](#Kernel-Density-Estimation)\n- [Preprocessing](#Preprocessing)\n- [Modeling](#Modeling)\n    - [Linear Model (Gradient Boosted Classifier)](#Linear-Model)\n    - [Neural Network](#Neural-Network)\n    - [Results](#Results)\n- [Actionble Insight](#Actionable-Insight)","c2167832":"## Hypotheses \n1. Employee attrition increases as distance from home increases.  \n2. Employee attrition is strongly correlated with Department\/Job Title.","b1595938":"The confusion matrix seems to suggest that our XGB model has taken an aggressive stance on  \npredicting that employees have left, even if they haven't.","ee59bc37":"## **Actionable Insight**","94a78def":"## Preprocessing","bcd3683a":"With Reference to the groupby frames earlier, this plot further supports the sentiment  \nthat the majority of the attrition comes from those who live in the shortest of distances  \nfrom the company.\n\nIt's safe to say that hypothesis 1 is no longer worth pursuing with futher resources.","ea87edef":"#### Linear Model","4dcfcad3":"The data shown suggests that a company should focus on at the following 3 key  \nfactors when planning to maximize employee retention while minimizing adverse  \naffection on other factors of influence.\n\n**Over time**  \nThose with higher overtime hours logged tended to be be much more likely to quit  \nor be fired than those with less overtime hours. [Occupational burnout](https:\/\/en.wikipedia.org\/wiki\/Occupational_burnout) is a legitimate  \nconcern that should be addressed when considering employee retention.\n\nPossible  \nsteps to consider include *improved distribution of responsibility* and encouragement  \nto not work to such an extent that an employee may evetually suffer from  [occupational burnout.](https:\/\/en.wikipedia.org\/wiki\/Occupational_burnout)  \n\n**Job Level**  \nIn an almost linear fashion, those with a lower job level saw an increased rate  \nof attrition.\n\nThe employer should aim to focus on promoting job advancement opportunities when  \npossible to get employees out of that stage OR they should improve the quality of  \nlife for the employees who are in that stage.\n\n**Stock Options Level**  \nThose with a 0 Level stock option had not only the highest raw attrition count  \nat nearly 150,  but also held the highest attrition ratio with respect to the  \nother stock options levels. \n\nAn employee's stock options satisfaction should be considered as another  \npossible metric to further investigate as a factor retention influence.","50e69bd9":"#### Feature Distribution","b6ad94fe":"## Results\n\nThe xgb_boost model performed slightly better than the neural network  \nmodel, so we will use this one to estimate the feature importances.","02cf4fe8":"# **Human Resources Data Science Project**\n*(This project is based off of the [IBM HR Analytics Competition](https:\/\/www.kaggle.com\/pavansubhasht\/ibm-hr-analytics-attrition-dataset))*  \n  \nAuthor: Malik R. Booker  \nCreated: 19-MAY-2020  \nPublished: 22-MAY-2020  \nEdited: 24-MAY-2020","57c7a72c":"Not the greatest ROC curve, but that can be more finely tuned as   \nmore data comes in.","20c296c6":"## Introduction\n\nThe Human Resources Department is responsible for a large portion of a  \ncompany's internal operations.  \nThese operations include:\n- Hiring and Recruiting\n- Training and Development\n- Employee Compensation\n- Employee Benefits\n- Employee Relations\n- Legal Responsibilities\n\nHiring employees is an expensive task. The final cost of hiring a new  \nemployee is the sum of training, briefing, recruiting, and of course  \nlost profit from not having an open spot on a team filled.  \n\nRetention, on the other hand, is much less expensive with respect to  \nthe hiring process. We can circumvent the affect that the hiring process  \nwould cost a company by simply making it more desirable to stay with the  \ncompany longer.","3083f89d":"Once again, we have visualized the relationship of job title\/role and attrition  \nand Sales Representative again appears to have the highest attrition ratio.\n\nThe numeric Values like: Job Involvement and Job Level actually seem to have a linear  \nrelationship with the attrition rate. Of course, further investigation will be done in  \nthe modeling step to see if this hypothesis is supported.\n\nAs far as we are concerned from a obervational standpoint, we will take the claim from  \nhypothesis 2 and formulate a controlled study to further investigate the correlation  \nbetween job title\/role and attrition.","6a161b6d":"The highest and lowest raw attrition counts were for the positions  \nLaboratory Technition and (Human Resources) Manager at *62* and *0,*  \nrespectively.  \n\nThe highest and lowest rate of attrition were for the positions  \nSales Representative and (Human Resources) Manager at *50:33* and  \n*11:0*, respectively.\n\nSales Representatives hold a significantly large *50:33* No\/Yes  \nattrition ratio while the next closest ratio belongs to Sales  \nExecutives at *269:57*\n\nThe data above somewhat supports the claim that Hypothesis 2  \nstates that attrition is correlated, however, more analysis is  \nneeded to determine if the two are strongly correlated."}}