{"cell_type":{"e721537c":"code","a3f3c01f":"code","bdb99f62":"code","e23d243e":"code","8ab7feab":"code","804cf943":"code","d0a17bae":"code","4caadbab":"code","8da4e91f":"code","6a07aebd":"code","e68c2884":"code","27f2faa9":"code","b22c66c7":"code","1860ee23":"code","35b5a638":"code","b9719605":"code","e24ec85b":"code","99a933bf":"code","a1f7a03b":"code","a8a8afb3":"code","3321f01e":"code","d0a8c9a2":"code","42411b21":"code","92168385":"code","5bf61638":"code","99cbec3a":"code","7911c00e":"code","d967c86e":"code","d5ef9e82":"code","9c988805":"code","7adbc4e4":"code","3f656bea":"code","1bd0ba8a":"code","d174d150":"code","51f31747":"code","810e85a4":"code","bd1daa07":"code","520b78c9":"code","9bc5bcac":"code","877267c2":"code","081da0e5":"code","20ff2742":"code","b36c1eab":"code","af47bced":"code","6ce3ed8d":"markdown","29fdde3b":"markdown","95d88020":"markdown","78b5802a":"markdown"},"source":{"e721537c":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","a3f3c01f":"!pip install autoviz --quiet\n!pip install xlrd --quiet\n!pip install pycaret --quiet","bdb99f62":"from autoviz.AutoViz_Class import AutoViz_Class\nAV = AutoViz_Class()\npd.options.display.float_format = '{:.2f}'.format","e23d243e":"# Importando os dados\ntrain = pd.read_csv('\/kaggle\/input\/big-mart-sales-prediction-datasets\/train.csv')\ntest = pd.read_csv('\/kaggle\/input\/big-mart-sales-prediction-datasets\/test.csv')\ntrain_original = train.copy(deep=True)\ntest_original = test.copy(deep=True)\n\ntrain.shape, test.shape","8ab7feab":"# Verificando os tipos\ntrain.info()","804cf943":"# Verificando valores \u00fanicos em cada coluna\ntrain.nunique()","d0a17bae":"# Verificando os valores nulos\ntrain.isna().sum()","4caadbab":"# Analisando a vari\u00e1vel target\ntrain['Item_Outlet_Sales'].describe()","8da4e91f":"# Item_Fat_Content tem valores em excesso\ntrain['Item_Fat_Content'].unique()","6a07aebd":"# Criando um dicion\u00e1rio para mapear todos os registros em 'low' ou 'regular'\nitem_fat = {'Low Fat':'low', 'Regular':'regular', 'LF':'low', 'reg':'regular','low fat':'low'}\n\ntrain['Item_Fat_Content'] = train['Item_Fat_Content'].map(item_fat)\ntest['Item_Fat_Content'] = test['Item_Fat_Content'].map(item_fat)\n\n# Verificando\ntrain['Item_Fat_Content'].unique()","e68c2884":"# Verificando Outlet_Size\ntrain['Outlet_Size'].unique()","27f2faa9":"# Verificando Outlet_Location_Type\ntrain['Outlet_Location_Type'].unique()","b22c66c7":"# Verificando Outlet_Type\ntrain['Outlet_Type'].unique()","1860ee23":"# Verificando Item_Type\ntrain['Item_Type'].unique()","35b5a638":"# Verificando Outlet_Identifier\ntrain['Outlet_Identifier'].unique()","b9719605":"# Quantos valores \u00fanicos de Item_Identifier\ntrain['Item_Identifier'].nunique()","e24ec85b":"# As colunas 'Item_Weight' e 'Outlet_Size' possuem valores nulos\n\n# Vamos usar a m\u00e9dia para imputar valores em Item_Weight\ntrain['Item_Weight'].fillna(train['Item_Weight'].mean(), inplace=True)\ntest['Item_Weight'].fillna(test['Item_Weight'].mean(), inplace=True)\n\n# Vamos usar a moda para imputar valores em Outlet_Size\ntrain['Outlet_Size'].fillna(train['Outlet_Size'].mode()[0], inplace=True)\ntest['Outlet_Size'].fillna(test['Outlet_Size'].mode()[0], inplace=True)","99a933bf":"# Precisamos codificar as vari\u00e1veis categ\u00f3ricas\ntrain.dtypes","a1f7a03b":"# Obtendo as colunas para treinamento\nfeatures = [c for c in train.columns if c not in ['Item_Identifier']]","a8a8afb3":"filename = \"\"\nsep = \",\"\ndft = AV.AutoViz(\n    filename,\n    sep=\",\",\n    depVar=\"\",\n    dfte=train[features],\n    header=0,\n    verbose=0,\n    lowess=False,\n    chart_format=\"svg\",\n    max_rows_analyzed=10_000\n)","3321f01e":"# Vamos codificar as colunas categ\u00f3ricas usando one hot encoding\ncat_cols = ['Item_Fat_Content', 'Item_Type', 'Outlet_Identifier', 'Outlet_Size', 'Outlet_Location_Type', 'Outlet_Type']\n\ntrain = pd.get_dummies(train, columns=cat_cols)\ntest = pd.get_dummies(test, columns=cat_cols)\n\ntrain.shape","d0a8c9a2":"from sklearn.model_selection import train_test_split\ntrain, valid = train_test_split(train, random_state=42)\n\ntrain.shape, valid.shape","42411b21":"features = [c for c in train.columns if c not in ['Item_Identifier', 'Item_Outlet_Sales']]\ntarget = 'Item_Outlet_Sales'","92168385":"from sklearn.ensemble import GradientBoostingRegressor, RandomForestRegressor\nfrom sklearn.linear_model import Lasso, BayesianRidge, OrthogonalMatchingPursuit\nfrom sklearn.metrics import mean_squared_error","5bf61638":"gbr = GradientBoostingRegressor(alpha=0.9, ccp_alpha=0.0, criterion='friedman_mse',\n                           init=None, learning_rate=0.1, loss='ls', max_depth=5,\n                           max_features='sqrt', max_leaf_nodes=None,\n                           min_impurity_decrease=0.3, min_impurity_split=None,\n                           min_samples_leaf=3, min_samples_split=5,\n                           min_weight_fraction_leaf=0.0, n_estimators=50,\n                           n_iter_no_change=None, presort='deprecated',\n                           random_state=41, subsample=0.6, tol=0.0001,\n                           validation_fraction=0.1, verbose=0, warm_start=False)\n\ngbr.fit(train[features], train[target])\n\n\npred_gbr = gbr.predict(valid[features])\nrmse_gbr = mean_squared_error(valid[target], pred_gbr, squared=False)\nrmse_gbr","99cbec3a":"omp = OrthogonalMatchingPursuit(fit_intercept=False, n_nonzero_coefs=13,\n                           normalize=False, precompute='auto', tol=None)\n\nomp.fit(train[features], train[target])\n\n\npred_omp = omp.predict(valid[features])\nrmse_omp = mean_squared_error(valid[target], pred_omp, squared=False)\nrmse_omp","7911c00e":"las = Lasso(alpha=4.07, copy_X=True, fit_intercept=True, max_iter=1000,\n       normalize=False, positive=False, precompute=False, random_state=41,\n       selection='cyclic', tol=0.0001, warm_start=False)\n\nlas.fit(train[features], train[target])\n\n\npred_las = las.predict(valid[features])\nrmse_las = mean_squared_error(valid[target], pred_las, squared=False)\nrmse_las","d967c86e":"\nbr = BayesianRidge(alpha_1=0.001, alpha_2=0.2, alpha_init=None, compute_score=True,\n               copy_X=True, fit_intercept=False, lambda_1=0.1, lambda_2=0.15,\n               lambda_init=None, n_iter=300, normalize=False, tol=0.001,\n               verbose=False)\n\nbr.fit(train[features], train[target])\n\npred_br = br.predict(valid[features])\nrmse_br = mean_squared_error(valid[target], pred_br, squared=False)\nrmse_br","d5ef9e82":"rd = RandomForestRegressor(bootstrap=True, ccp_alpha=0.0, criterion='mse',\n                       max_depth=6, max_features=1.0, max_leaf_nodes=None,\n                       max_samples=None, min_impurity_decrease=0.5,\n                       min_impurity_split=None, min_samples_leaf=4,\n                       min_samples_split=9, min_weight_fraction_leaf=0.0,\n                       n_estimators=300, n_jobs=-1, oob_score=False,\n                       random_state=41, verbose=0, warm_start=False)\n\nrd.fit(train[features], train[target])\n\npred_rd = rd.predict(valid[features])\nrmse_rd = mean_squared_error(valid[target], pred_rd, squared=False)\nrmse_rd","9c988805":"# Usaremos o VotingRegressor\nfrom sklearn.ensemble import VotingRegressor","7adbc4e4":"estimators = [('gbr', gbr),('rd',rd)]\nensemble = VotingRegressor(estimators=estimators, n_jobs=-1)\nensemble.fit(train[features], train['Item_Outlet_Sales'])\npreds_ens = ensemble.predict(valid[features])\nrmse_ens = mean_squared_error(valid['Item_Outlet_Sales'], preds_ens, squared=False)\nrmse_ens","3f656bea":"models = dict([('gbr', gbr),('omp',omp),('las', las),('br',br),('rd',rd),('ensemble',ensemble)])","1bd0ba8a":"for name, model in models.items():\n    predict = model.predict(test[features])\n    test_original['Item_Outlet_Sales'] = [item if item >= 0 else 0 for item in predict]\n    test_original[['Item_Identifier','Outlet_Identifier','Item_Outlet_Sales']].to_csv(f'{name}_solution.csv',index=False)","d174d150":"resultados ={\n'ensemble_solution.csv': 1168.07780220611,\n'omp_solution.csv':\t1186.9823672419,\n'gbr_solution.csv':\t1169.41769791302,\n'rd_solution.csv':\t1158.26292596073,\n'las_solution.csv':\t1188.42007566568,\n'br_solution.csv':\t1187.39657613478\n}\nresultados","51f31747":"from pycaret.regression import *","810e85a4":"exp_setup = setup(data = train_original, target = 'Item_Outlet_Sales', session_id=41, n_jobs=-1, silent=False,\n                verbose=False, ignore_features=['Item_Identifier'])","bd1daa07":"top4 = compare_models(n_select=4, fold=10, sort='MSE' ,exclude=['xgboost', 'lar'])","520b78c9":"blender_top4 = blend_models(top4, optimize='MSE', fold=10, verbose=False)\nblender_top4_final = finalize_model(blender_top4)","9bc5bcac":"tuned_top4 = [tune_model(i) for i in top4]","877267c2":"blender_tuned_top4 = blend_models(tuned_top4, optimize='MSE', fold=10, verbose=False)\nblender_tuned_top4_final = finalize_model(blender_tuned_top4)","081da0e5":"blender_tuned_top2 = blend_models(tuned_top4[:2], optimize='MSE', fold=10, verbose=False)\nblender_tuned_top2_final = finalize_model(blender_tuned_top2)","20ff2742":"automl_models = {\n    'top4': blender_top4_final,\n    'tuned_top4':blender_tuned_top4_final,\n    'tuned_top2':blender_tuned_top2_final\n}","b36c1eab":"for name, model in automl_models.items():\n    predict = predict_model(model, test_original)\n    predict[['Item_Identifier','Outlet_Identifier','Item_Outlet_Sales']].to_csv(f'{name}_solution.csv',index=False)","af47bced":"{\n    'top4': 1156.9559264255895,\n    'tuned_top4':1156.9559264255895,\n    'tuned_top2':1156.9559264255895.\n}","6ce3ed8d":"## Modelo inicial","29fdde3b":"# IESB - CIA035 - Aula 07 - Ensemble de Modelos\n\n## Dados\nhttps:\/\/datahack.analyticsvidhya.com\/contest\/practice-problem-big-mart-sales-iii\/","95d88020":"# Criando nosso pr\u00f3prio Ensemble de M\u00e9todos","78b5802a":"## Tratamento de Dados"}}