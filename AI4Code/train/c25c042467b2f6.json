{"cell_type":{"be895e12":"code","f3d06c37":"code","c2f7f5af":"code","e5458cb5":"code","2a9c7c35":"code","b07e2dd1":"code","e6ec51b2":"code","4e09b61e":"code","9917e847":"code","a3133fe3":"code","23edabe4":"code","fec6e6a7":"code","6bae74e5":"code","7fb4fa87":"code","2c2dc489":"code","3130d14c":"code","cfefd58c":"code","351834b0":"code","c2dea605":"code","b9077692":"code","d1701d95":"code","0aba83dc":"code","9ba6788c":"code","c86e976a":"code","3a9b9507":"code","49edc684":"code","15e02ab8":"code","4683c9bf":"code","2f349a91":"code","233d8a91":"code","54e7c7a9":"code","4193a5a8":"code","e96f094e":"code","d94e432a":"code","9ecf43eb":"markdown","3be3ec0b":"markdown","3f98b847":"markdown","4f74f3da":"markdown","b555f45b":"markdown","f66cf142":"markdown","6084d677":"markdown","bdd360e0":"markdown","be70daa1":"markdown","8563125b":"markdown","fd61962c":"markdown","a710913e":"markdown","a6d9966d":"markdown","7193b98a":"markdown","ecbc36d9":"markdown","6dd36e1d":"markdown","c321a435":"markdown","571e2365":"markdown","bc69264d":"markdown","b44296a0":"markdown","343031bc":"markdown","3cfd11e6":"markdown","bf95ccd1":"markdown","86d93155":"markdown","056ffd78":"markdown","6e6d417e":"markdown"},"source":{"be895e12":"# Importing libraries\nimport warnings\nwarnings.filterwarnings('ignore')\nimport sqlite3\nimport pandas as pd\nimport numpy as np\nfrom time import time\nfrom nltk.corpus import stopwords\nimport nltk\nimport re\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.manifold import TSNE\nfrom sklearn.feature_extraction.text import TfidfTransformer\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nimport seaborn as sb\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport gensim","f3d06c37":"# Connection to the dataset\ncon = sqlite3.connect('..\/input\/database.sqlite')\n\n# It is given that the table name is 'Reviews'\n# Creating pandas dataframe and storing into variable 'dataset' by help of sql query\ndataset = pd.read_sql_query(\"\"\"\nSELECT *\nFROM Reviews\n\"\"\", con)\n\n# Getting the shape of actual data: row, column\ndisplay(dataset.shape)","c2f7f5af":"# Displaying first 5 data points\ndisplay(dataset.head())","e5458cb5":"# Considering only those reviews which score is either 1,2 or 4,5\n# Since, 3 is kind of neutral review, so, we are eliminating it\nfiltered_data = pd.read_sql_query(\"\"\"\nSELECT *\nFROM Reviews\nWHERE Score != 3\n\"\"\", con)","2a9c7c35":"# Getting shape of new dataset\ndisplay(filtered_data.shape)","b07e2dd1":"# Changing the scores into 'positive' or 'negative'\n# Score greater that 3 is considered as 'positive' and less than 3 is 'negative'\ndef partition(x):\n    if x>3:\n        return 'positive'\n    return 'negative'\n\nactual_score = filtered_data['Score']\npositiveNegative = actual_score.map(partition)\nfiltered_data['Score'] = positiveNegative","e6ec51b2":"# Sorting data points according to the 'ProductId'\nsorted_data = filtered_data.sort_values('ProductId', axis=0, ascending=True, inplace=False, kind='quicksort', na_position='last')\n\n# Eliminating the duplicate data points based on: 'UserId', 'ProfileName', 'Time', 'Summary'\nfinal = sorted_data.drop_duplicates(subset={'UserId', 'ProfileName', 'Time', 'Summary'}, keep='first', inplace=False)\n\n# Eliminating the row where 'HelpfulnessDenominator' is greater than 'HelpfulnessNumerator' as these are the wrong entry\nfinal = final[final['HelpfulnessDenominator'] >= final['HelpfulnessNumerator']]\n\n# Getting shape of final data frame\ndisplay(final.shape)","4e09b61e":"%%time\n\n# Creating the set of stopwords\nstop = set(stopwords.words('english'))\n\n# For stemming purpose\nsnow = nltk.stem.SnowballStemmer('english')\n\n# Defining function to clean html tags\ndef cleanhtml(sentence):\n    cleaner = re.compile('<.*>')\n    cleantext = re.sub(cleaner, ' ', sentence)\n    return cleantext\n\n# Defining function to remove special symbols\ndef cleanpunc(sentence):\n    cleaned = re.sub(r'[?|.|!|*|@|#|\\'|\"|,|)|(|\\|\/]', r'', sentence)\n    return cleaned\n\n\n# Important steps to clean the text data. Please trace it out carefully\ni = 0\nstr1 = ''\nall_positive_words = []\nall_negative_words = []\nfinal_string = []\ns=''\nfor sent in final['Text'].values:\n    filtered_sentence = []\n    sent = cleanhtml(sent)\n    for w in sent.split():\n        for cleaned_words in cleanpunc(w).split():\n            if ((cleaned_words.isalpha()) & (len(cleaned_words)>2)):\n                if (cleaned_words.lower() not in stop):\n                    s = (snow.stem(cleaned_words.lower())).encode('utf-8')\n                    filtered_sentence.append(s)\n                    if (final['Score'].values)[i] == 'positive':\n                        all_positive_words.append(s)\n                    if (final['Score'].values)[i] == 'negative':\n                        all_negative_words.append(s)\n                else:\n                    continue\n            else:\n                continue\n    str1 = b\" \".join(filtered_sentence)\n    final_string.append(str1)\n    i += 1\n    \n# Adding new column into dataframe to store cleaned text\nfinal['CleanedText'] = final_string\nfinal['CleanedText'] = final['CleanedText'].str.decode('utf-8')\n\n# Creating new dataset with cleaned text for future use\nconn = sqlite3.connect('final.sqlite')\nc = conn.cursor()\nconn.text_factory = str\nfinal.to_sql('Reviews', conn, schema=None, if_exists='replace', index=True, index_label=None, chunksize=None, dtype=None)\n\n# Getting shape of new datset\nprint(final.shape)","9917e847":"# Creating connection to read from database\nconn = sqlite3.connect('.\/final.sqlite')\n\n# Creating data frame for visualization using sql query\nfinal = pd.read_sql_query(\"\"\"\nSELECT *\nFROM Reviews\n\"\"\", conn)","a3133fe3":"# Displaying first 3 data points of newly created datset\ndisplay(final.head(3))","23edabe4":"# Getting the number of data points in each class: positive or negative\ndisplay(final['Score'].value_counts())","fec6e6a7":"# Taking equal sample of negative and positive reviews to keep it balanced.\n# If it is not balanced then there is chance that one class lebel can dominant other class label which might be sever probelm sometimes.\npositive_points = final[final['Score'] == 'positive'].sample(n=3000)\nnegative_points = final[final['Score'] == 'negative'].sample(n=3000)\n\n# Concatenating both of above\ntotal_points = pd.concat([positive_points, negative_points])","6bae74e5":"%%time\n# Initializing vectorizer for bigram\ncount_vect = CountVectorizer(ngram_range=(1,1))\n\n# Initializing standard scaler\nstd_scaler = StandardScaler(with_mean=False)\n\n# Creating count vectors and converting into dense representation\nsample_points = total_points['CleanedText']\nsample_points = count_vect.fit_transform(sample_points)\nsample_points = std_scaler.fit_transform(sample_points)\nsample_points = sample_points.todense()\n\n# Storing class label in variable\nlabels = total_points['Score']\n\n# Getting shape\nprint(sample_points.shape, labels.shape)","7fb4fa87":"%%time\nfrom sklearn.manifold import TSNE\n\ntsne_data = sample_points\ntsne_labels = labels\n\n# Initializing with most explained variance\nmodel = TSNE(n_components=2, random_state=15)\n\n# Fitting model\ntsne_data = model.fit_transform(tsne_data)\n\n# Adding labels to the data point\ntsne_data = np.vstack((tsne_data.T, tsne_labels)).T\n\n# Creating data frame\ntsne_df = pd.DataFrame(data=tsne_data, columns=('Dim_1', 'Dim_2', 'label'))\n\n# Plotting graph for class labels\nsb.FacetGrid(tsne_df, hue='label', size=5).map(plt.scatter, 'Dim_1', 'Dim_2').add_legend()\nplt.title(\"TSNE with default parameters\")\nplt.xlabel(\"Dim_1\")\nplt.ylabel(\"Dim_2\")\nplt.show()\n","2c2dc489":"%%time\nfrom sklearn.manifold import TSNE\n\ntsne_data = sample_points\ntsne_labels = labels\n\n# Initializing with most explained variance\nmodel = TSNE(n_components=2, random_state=15, perplexity=20, n_iter=2000)\n\n# Fitting model\ntsne_data = model.fit_transform(tsne_data)\n\n# Adding labels to the data point\ntsne_data = np.vstack((tsne_data.T, tsne_labels)).T\n\n# Creating data frame\ntsne_df = pd.DataFrame(data=tsne_data, columns=('Dim_1', 'Dim_2', 'label'))\n\n# Plotting graph for class labels\nsb.FacetGrid(tsne_df, hue='label', size=5).map(plt.scatter, 'Dim_1', 'Dim_2').add_legend()\nplt.title(\"TSNE with perplexity: 20, n_iter:2000\")\nplt.xlabel(\"Dim_1\")\nplt.ylabel(\"Dim_2\")\nplt.show()\n","3130d14c":"%%time\n\n# Initializing tf-idf vectorizer for bigram\ntfidf_vect = TfidfVectorizer(ngram_range=(1,2))\n\ntfidf_data = total_points['CleanedText']\ntfidf_data = tfidf_vect.fit_transform(tfidf_data)\ntfidf_data = tfidf_data.todense()\n\ntfidf_labels = labels\n","cfefd58c":"print(tfidf_data.shape, tfidf_labels.shape)","351834b0":"%%time\n\nmodel = TSNE(n_components=2, random_state=15)\n\n# Fitting model\ntsne_data = model.fit_transform(tfidf_data)\n\n\n# Attaching feature and label\ntsne_data = np.vstack((tsne_data.T, tfidf_labels)).T\n\n# Creating data frame\ntsne_df = pd.DataFrame(data=tsne_data, columns=('Dim_1', 'Dim_2', 'label'))\n\n# Plotting graph for class labels\nsb.FacetGrid(tsne_df, hue='label', size=5).map(plt.scatter, 'Dim_1', 'Dim_2').add_legend()\nplt.title(\"TSNE with default parameters\")\nplt.xlabel(\"Dim_1\")\nplt.ylabel(\"Dim_2\")\nplt.show()","c2dea605":"%%time\nfrom sklearn.manifold import TSNE\n\ntsne_data = sample_points\ntsne_labels = labels\n\n# Initializing with most explained variance\nmodel = TSNE(n_components=2, random_state=15, perplexity=20, n_iter=2000)\n\n# Fitting model\ntsne_data = model.fit_transform(tsne_data)\n\n# Adding labels to the data point\ntsne_data = np.vstack((tsne_data.T, tsne_labels)).T\n\n# Creating data frame\ntsne_df = pd.DataFrame(data=tsne_data, columns=('Dim_1', 'Dim_2', 'label'))\n\n# Plotting graph for class labels\nsb.FacetGrid(tsne_df, hue='label', size=5).map(plt.scatter, 'Dim_1', 'Dim_2').add_legend()\nplt.title(\"TSNE with perplexity: 20, n_iter:2000\")\nplt.xlabel(\"Dim_1\")\nplt.ylabel(\"Dim_2\")\nplt.show()","b9077692":"# Getting text from Review\nw2v_points = total_points['Text']\nw2v_labels = labels.copy()","d1701d95":"import re\ndef cleanhtml(sentence):\n    cleantext = re.sub('<.*>', '', sentence)\n    return cleantext\n\ndef cleanpunc(sentence):\n    cleaned = re.sub(r'[?|!|\\'|#|@|.|,|)|(|\\|\/]', r'', sentence)\n    return cleaned","0aba83dc":"# Creating list of sentences\nsent_list = []\nfor sent in w2v_points:\n    sentence = []\n    sent = cleanhtml(sent)\n    for w in sent.split():\n        for cleaned_words in cleanpunc(w).split():\n            if(cleaned_words.isalpha()):\n                sentence.append(cleaned_words.lower())\n            else:\n                continue\n    sent_list.append(sentence)","9ba6788c":"print(sent_list[1])","c86e976a":"# Initializing model for words occur atleast 5 times\nw2v_model = gensim.models.Word2Vec(sent_list, min_count=5, size=50, workers=4)\n\n# Applying model for word2vec\nw2v_words = w2v_model[w2v_model.wv.vocab]","3a9b9507":"print(\"Number of words occur min 5 times: \", len(w2v_words))","49edc684":"print(w2v_words.shape)","15e02ab8":"# Getting 10 similar words\ndisplay(w2v_model.wv.most_similar(\"sweet\"))","4683c9bf":"# Producing average word to vec vectors\nimport numpy as np\nsent_vectors = []\nfor sent in sent_list:\n    sent_vec = np.zeros(200)\n    cnt_words = 0\n    for word in sent:\n        try:\n            vec = w2v_model.wv[word]\n            sent_vec += vec\n            cnt_words += 1\n        except:\n            pass\n    sent_vec \/= cnt_words\n    sent_vectors.append(sent_vec)\nsent_vectors = np.nan_to_num(sent_vectors)","2f349a91":"print(sent_vectors.shape)","233d8a91":"%%time\n\nmodel = TSNE(n_components=2, random_state=15)\n\n# Fitting model\nw2v_points = model.fit_transform(sent_vectors)\n\n\n# Attaching feature and label\ntsne_data = np.vstack((w2v_points.T, w2v_labels)).T\n\n# Creating data frame\ntsne_df = pd.DataFrame(data=tsne_data, columns=('Dim_1', 'Dim_2', 'label'))\n\n# Plotting graph for class labels\nsb.FacetGrid(tsne_df, hue='label', size=5).map(plt.scatter, 'Dim_1', 'Dim_2').add_legend()\nplt.title(\"TSNE with default parameters\")\nplt.xlabel(\"Dim_1\")\nplt.ylabel(\"Dim_2\")\nplt.show()","54e7c7a9":"%%time\nfrom sklearn.manifold import TSNE\n\ntsne_data = sample_points\ntsne_labels = labels\n\n# Initializing with most explained variance\nmodel = TSNE(n_components=2, random_state=15, perplexity=20, n_iter=2000)\n\n# Fitting model\ntsne_data = model.fit_transform(tsne_data)\n\n# Adding labels to the data point\ntsne_data = np.vstack((tsne_data.T, tsne_labels)).T\n\n# Creating data frame\ntsne_df = pd.DataFrame(data=tsne_data, columns=('Dim_1', 'Dim_2', 'label'))\n\n# Plotting graph for class labels\nsb.FacetGrid(tsne_df, hue='label', size=5).map(plt.scatter, 'Dim_1', 'Dim_2').add_legend()\nplt.title(\"TSNE with perplexity: 20, n_iter:2000\")\nplt.xlabel(\"Dim_1\")\nplt.ylabel(\"Dim_2\")\nplt.show()\n","4193a5a8":"%%time\n\ntfidf_feat = tfidf_vect.get_feature_names()\ntfidf_w2v_vectors = []\nrow = 0\nfor sent in sent_list:\n    sent_vec = np.zeros(200)\n    weight_sum = 0\n    for word in sent:\n        if word in w2v_words:\n            vec = w2v_model.wv[word]\n            tf_idf = final_tf_idf[row, tfidf_feat.index(word)]\n            sent_vec += (vec*tf_idf)\n            weight_sum += tf_idf\n    if weight_sum != 0:\n        sent_vec \/= weight_sum\n    tfidf_w2v_vectors.append(sent_vec)\n    row += 1\n","e96f094e":"%%time\n\n# Defining model for two features with most explained variance\nmodel = TSNE(n_components=2, random_state=15)\n\n# Fitting model\ntfidf_w2v_points = model.fit_transform(tfidf_w2v_vectors)\n\n# Attaching feature and label\ntsne_data = np.vstack((tfidf_w2v_points.T, labels)).T\n\n# Creating data frame\ntsne_df = pd.DataFrame(data=tsne_data, columns=('Dim_1', 'Dim_2', 'label'))\n\n# Plotting graph for class labels\nsb.FacetGrid(tsne_df, hue='label', size=5).map(plt.scatter, 'Dim_1', 'Dim_2').add_legend()\nplt.title(\"TSNE with default parameters\")\nplt.xlabel(\"Dim_1\")\nplt.ylabel(\"Dim_2\")\nplt.show()","d94e432a":"%%time\nfrom sklearn.manifold import TSNE\n\ntsne_data = sample_points\ntsne_labels = labels\n\n# Initializing with most explained variance\nmodel = TSNE(n_components=2, random_state=15, perplexity=20, n_iter=2000)\n\n# Fitting model\ntsne_data = model.fit_transform(tsne_data)\n\n# Adding labels to the data point\ntsne_data = np.vstack((tsne_data.T, tsne_labels)).T\n\n# Creating data frame\ntsne_df = pd.DataFrame(data=tsne_data, columns=('Dim_1', 'Dim_2', 'label'))\n\n# Plotting graph for class labels\nsb.FacetGrid(tsne_df, hue='label', size=5).map(plt.scatter, 'Dim_1', 'Dim_2').add_legend()\nplt.title(\"TSNE with perplexity: 20, n_iter:2000\")\nplt.xlabel(\"Dim_1\")\nplt.ylabel(\"Dim_2\")\nplt.show()\n","9ecf43eb":"## Default parameters","3be3ec0b":"## Loading dataset","3f98b847":"# TF-IDF","4f74f3da":"## With different parameters","b555f45b":"## TSNE for TF-IDF","f66cf142":"# BoW ","6084d677":"## Filtering","bdd360e0":"# TFIDF-W2V","be70daa1":"# Word2Vec","8563125b":"## TSNE for Word2Vec","fd61962c":"# Observation:\n- Even though t-SNE is very powerful technique to visualize high dimension data but still it's unable to separate the class labels in our dataset.\n- It accepts dense matrix to execute on it but computationally it is very expensive for much higher dimensional data as in our case.\n- There were completely overlapping of class labels on each other that means t-SNE failed to separate the class labels.\n- Changing parameters and getting stable t-SNE model is very important but even after trying out many combinations of parameters. It is found out that it doesn't help in our case of Amazon Fine Food Reviews dataset and some of the parameter adjustment are shown here also.\n","a710913e":"# Avg W2V","a6d9966d":"## TSNE for TFIDF-W2V","7193b98a":"## With default parameters","ecbc36d9":"# Data Preprocessing","6dd36e1d":"- We can see that the new dataset is containing new column with cleaned text which will be useful for next procedure.","c321a435":"- Next step is to clean the text of the reviews.","571e2365":"## Different parameters","bc69264d":"- It can be observed that the data set is imbalanced.\n- There are much more data points belonging to positive class than negative.","b44296a0":"## Different parameters","343031bc":"- Almost all the negative and positive labels are overlapping to each other which means, even after applying tSNE, class labels are not separated. ","3cfd11e6":"## TSNE for Avg W2V","bf95ccd1":"## Different parameters","86d93155":"## Default parameters","056ffd78":"## Default parameters","6e6d417e":"# tSNE\n\n## Introduction\n- tSNE: t-distributed Stochastic Neighborhood Embedding\n- Why tSNE?\n    - tSNE is a very useful technique to visualize higher dimension data. It is state of art to visualize n-D data into 2-D surface. There are other techniques as well but tSNE is somewhat better. Basically it groups points based on visual similarity.\n- t-distribution?\n    - Used to solve the crowding problem.\n- Neighborhood?\n    - Points that are geometrically closed.\n- Embedding?\n    - Finding low dimensional space for points given in higher dimensional space.\n- How does it do?\n    - It preserves local structure of data.\n- Important parameters\n    - perplexity: Number of points considered as neighbor geometrically.\n    - n_iter: Number of iterations to be repeated.\n    \n## Important\n   - It is very important to understand the limitations and advantage of tsne otherwise there is high chance to misinterpret it.\n   - Please visit \"https:\/\/distill.pub\/2016\/misread-tsne\/\" and play with it.\n\n\n## Objective\n- Get Amazon Fine Foor Review dataset and prepare 4 categories of datasets: \n      (i) BoW, \n      (ii) TF-IDF, \n      (iii) Word2Vec, \n      (iv) TFIDF-W2V\n        \n- Apply following of all of the above:\n    1. Perform TSNE representation for positive and negative reviews.\n    2. Analyze, on which of the four datasets, TSNE is giving better result.\n        \n        \n- There are many important things we should take care while working with tSNE.\n- We will explore and see interesting behavior of tSNE with real world data in this document.\n- It is prepared by taking the help of other's sources too, so, many-many thanks to them."}}