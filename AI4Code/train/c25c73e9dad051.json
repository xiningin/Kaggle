{"cell_type":{"9a4d7822":"code","b44a9ea4":"code","bd365e22":"code","4a47e3f7":"code","e66220a1":"code","af712709":"code","6c18d399":"code","eb765f1e":"code","a85ca636":"code","ef0b1218":"code","247c37c1":"code","b9f09407":"code","62eae3ba":"code","fa52bdba":"code","eb38d7f5":"code","a676dfef":"code","64c28038":"code","73bb0e4c":"code","83099a89":"code","ed7b36f3":"code","39f2d6fd":"code","648eff7a":"code","be28d4e6":"code","5751bdb6":"code","ad90007b":"code","0a7bf3e5":"code","1e6b08db":"code","3a14ab0f":"code","0fd6df07":"code","c704a152":"code","e3858f5f":"code","f1526665":"code","300600a3":"code","6ec4679b":"markdown","730cc35e":"markdown","419ef126":"markdown","5b2cdb11":"markdown","0cae3a66":"markdown","6c57deda":"markdown","f2d07e0f":"markdown","acc71b66":"markdown","d57d91a6":"markdown","8b3b26ff":"markdown","1085706b":"markdown","f33717af":"markdown","d70cab29":"markdown","a93fc4d4":"markdown","11103330":"markdown"},"source":{"9a4d7822":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","b44a9ea4":"DATA_PATH = \"\/kaggle\/input\/jigsaw-multilingual-toxic-comment-classification\/\"\nos.listdir(DATA_PATH)\n","bd365e22":"TEST_PATH = DATA_PATH + \"test.csv\"\nVAL_PATH = DATA_PATH + \"validation.csv\"\nTRAIN_PATH = DATA_PATH + \"jigsaw-toxic-comment-train.csv\"\n\nval_data = pd.read_csv(VAL_PATH)\ntest_data = pd.read_csv(TEST_PATH)\ntrain_data = pd.read_csv(TRAIN_PATH)","4a47e3f7":"train_data.head()","e66220a1":"val_data.head()","af712709":"test_data.head()","6c18d399":"from wordcloud import WordCloud, STOPWORDS\nimport matplotlib.pyplot as plt \n\ndef clean_comment(x):\n    if type(x) == str:\n        return x.replace(\"\\n\", \" \")\n    else:\n        return \"\"","eb765f1e":"text = train_data.apply(lambda row: clean_comment(row[\"comment_text\"]), axis=1).to_string()\nwordcloud = WordCloud(max_font_size=None, background_color='black', collocations=False,\n                      width=1200, height=1200, stopwords=set(STOPWORDS)).generate(text)\nplt.figure(figsize = (7, 7), facecolor = None) \nplt.imshow(wordcloud) \nplt.axis(\"off\") \nplt.tight_layout(pad = 0) \nplt.show() ","a85ca636":"text = val_data.apply(lambda row: clean_comment(row[\"comment_text\"]), axis=1).to_string()\nwordcloud = WordCloud(max_font_size=None, background_color='black', collocations=False,\n                      width=1200, height=1200, stopwords=set(STOPWORDS)).generate(text)\nplt.figure(figsize = (7, 7), facecolor = None) \nplt.imshow(wordcloud) \nplt.axis(\"off\") \nplt.tight_layout(pad = 0) \nplt.show() ","ef0b1218":"text = test_data.apply(lambda row: clean_comment(row[\"content\"]), axis=1).to_string()\nwordcloud = WordCloud(max_font_size=None, background_color='black', collocations=False,\n                      width=1200, height=1200, stopwords=set(STOPWORDS)).generate(text)\nplt.figure(figsize = (7, 7), facecolor = None) \nplt.imshow(wordcloud) \nplt.axis(\"off\") \nplt.tight_layout(pad = 0) \nplt.show() ","247c37c1":"!apt-get install -y python-numpy libicu-dev\n\n!pip install pyicu==2.3.1\n\n!pip install morfessor\n\n!pip install pycld2\n","b9f09407":"from polyglot.detect import Detector\nfrom polyglot.detect.langids import isoLangs\n\ndef detect_lang(comment):\n    return Detector(\"\".join(x for x in comment if x.isprintable()), quiet=True).languages[0].name\n    \ndef get_lang_name(code):\n    try:\n        return isoLangs[code][\"name\"]\n    except:\n        return \"Unknown\" ","62eae3ba":"train_data[\"lang_code\"] = train_data.apply(lambda row: detect_lang(row[\"comment_text\"]), axis=1)\ntrain_data[\"lang_name\"] = train_data[\"lang_code\"].apply(lambda row: get_lang_name(row))","fa52bdba":"print(\"\\n------ Training data -------\\n\")\nprint(\"-> Unknown language code: \", train_data[\"lang_code\"].loc[\n    train_data[\"lang_name\"] == \"Unknown\"].unique())\nprint(\"-> Unknown language examples count: \",train_data[\"lang_code\"].loc[\n    train_data[\"lang_name\"] == \"Unknown\"].count() )\nprint(\"-> Unknown language value counts: \\n\", train_data[\"lang_code\"].loc[\n    train_data[\"lang_name\"] == \"Unknown\"].value_counts())","eb38d7f5":"print(\"\\n------ Validation data -------\\n\")\n  \nval_data[\"lang_name\"] = val_data[\"lang\"].apply(lambda row: get_lang_name(row))\nprint(\"-> Unknown language code: \", val_data[\"lang\"].loc[\n    val_data[\"lang_name\"] == \"Unknown\"].unique())\nprint(\"-> Unknown language examples count: \",val_data[\"lang\"].loc[\n    val_data[\"lang_name\"] == \"Unknown\"].count() )\nprint(\"-> Unknown language value counts: \\n\", val_data[\"lang\"].loc[\n    val_data[\"lang_name\"] == \"Unknown\"].value_counts())","a676dfef":"print(\"\\n------ Testing data -------\\n\")\n  \ntest_data[\"lang_name\"] = test_data[\"lang\"].apply(lambda row: get_lang_name(row))\nprint(\"-> Unknown language code: \", test_data[\"lang\"].loc[\n    test_data[\"lang_name\"] == \"Unknown\"].unique())\nprint(\"-> Unknown language examples count: \",test_data[\"lang\"].loc[\n    test_data[\"lang_name\"] == \"Unknown\"].count() )\nprint(\"-> Unknown language value counts: \\n\", test_data[\"lang\"].loc[\n    test_data[\"lang_name\"] == \"Unknown\"].value_counts())","64c28038":"train_eng_count = train_data[\"lang_name\"].loc[train_data[\"lang_name\"] == \"English\"].count()\ntrain_non_eng_count = train_data[\"lang_name\"].loc[~train_data[\"lang_name\"].isin([\"English\", \"Unknown\"])].count()\n\nval_eng_count = val_data[\"lang_name\"].loc[val_data[\"lang_name\"] == \"English\"].count()\nval_non_eng_count = val_data[\"lang_name\"].loc[~val_data[\"lang_name\"].isin([\"English\", \"Unknown\"])].count()\n\ntest_eng_count = test_data[\"lang_name\"].loc[test_data[\"lang_name\"] == \"English\"].count()\ntest_non_eng_count = test_data[\"lang_name\"].loc[~test_data[\"lang_name\"].isin([\"English\", \"Unknown\"])].count()\n","73bb0e4c":"x = ['English', 'Non-English']\nlang_count = [[train_eng_count, train_non_eng_count], [val_eng_count, val_non_eng_count], \n          [test_eng_count, test_non_eng_count]]\nlang_count_titles =  [ \"Training data\", \"Validation data\", \"Testing data\"]\nx_pos = np.arange(len(x))\n\nfig, ax = plt.subplots(1, 3, figsize=(15,5))\n\nfor i, col in enumerate(ax):\n    tmp_data = lang_count[i]\n    col.bar(x_pos, tmp_data)\n#     plt.bar(x_pos, tmp_data)\n    for j, v in enumerate(tmp_data):\n        col.text(x_pos[j] - 0.1, v + .01, \"{:.2f}%\".format(v*100\/sum(tmp_data)))\n    col.set_title(lang_count_titles[i])\n    col.set_xlabel(\"Language\")\n    col.set_ylabel(\"Count\")\n    col.set_xticks(x_pos)\n    col.set_xticklabels(x)\n    col.plot()\n\n\nfig.suptitle(\"Language Count Across Datasets\", fontsize = 33)\nfig.tight_layout()\nfig.subplots_adjust(top=0.8)\n\nfig.show()","83099a89":"train_non_eng_count = train_data[\"lang_name\"][~train_data[\"lang_name\"].isin(\n    ['English', 'Unknown'])].value_counts().rename_axis('lang').reset_index(name='lang_count')\n\nval_non_eng_count = val_data[\"lang_name\"][~val_data[\"lang_name\"].isin(\n    ['English', 'Unknown'])].value_counts().rename_axis('lang').reset_index(name='lang_count')\n\ntest_non_eng_count = test_data[\"lang_name\"][~test_data[\"lang_name\"].isin(\n    ['English', 'Unknown'])].value_counts().rename_axis('lang').reset_index(name='lang_count')","ed7b36f3":"df_set = [train_non_eng_count, val_non_eng_count, test_non_eng_count]\nnon_eng_count_titles =  [ \"Training data\", \"Validation data\", \"Testing data\"]\n\nfig, ax = plt.subplots(1, 3, figsize=(18,5))\n\nfor i, col in enumerate(ax):\n    tmp_data = df_set[i]\n    \n    if i == 0:\n        mean_count = tmp_data.lang_count.mean()\n        idx = tmp_data[tmp_data['lang_count'] > int(mean_count)].lang\n        vals = tmp_data[tmp_data['lang_count'] > int(mean_count)].lang_count\n    else:\n        idx = tmp_data.lang\n        vals = tmp_data.lang_count\n    col.pie(vals, labels=idx, autopct='%1.1f%%')\n    col.axis('equal')\n    col.set_title(non_eng_count_titles[i])\n    col.plot()\n\nfig.suptitle(\"Non English Count Across Datasets\", fontsize = 33)\nfig.tight_layout()\nfig.subplots_adjust(top=0.8)\nplt.show()","39f2d6fd":"country_map = {\n    'Unknown': 'Unknown',\n    'English': 'United Kingdom',\n    'Azerbaijani': 'Azerbaijan',\n    'Basque': 'Spain',\n    'Interlingua': 'Italy',\n    'German': 'Germany',\n    'Volap\u00fck': 'Germany',\n    'Norwegian Nynorsk': 'Norway',\n    'Scots': 'Scotland',\n    'Dutch': 'Netherlands',\n    'Polish': 'Poland',\n    'Greek, Modern': 'Greece',\n    'Manx': 'Ireland',\n    'Portuguese': 'Portugal',\n    'Luganda': 'Uganda',\n    'Hungarian': 'Hungary',\n    'Kinyarwanda': 'Rwanda',\n    'Danish': 'Denmark',\n    'Latin': 'Italy',\n    'Western Frisian': 'Netherlands',\n    'Galician': 'Spain',\n    'Italian': 'Italy',\n    'Fijian': 'Fiji',\n    'Sanskrit (Sa\u1e41sk\u1e5bta)': 'India',\n    'Occitan': 'Spain',\n    'Xhosa': 'South Africa',\n    'Quechua': 'Peru',\n    'Welsh': 'Wales',\n    'Maltese': 'Malta',\n    'Chichewa; Chewa; Nyanja': 'Zimbabwe',\n    'Czech': 'Czech Republic',\n    'Oromo': 'Ethiopia',\n    'Tagalog': 'Philippines',\n    'Romansh': 'Switzerland',\n    'Turkish': 'Turkey',\n    'Irish': 'Ireland',\n    'Venda': '\tSouth Africa',\n    'Indonesian': 'Indonesia',\n    'Icelandic': 'Iceland',\n    'Uzbek': 'Uzbekistan',\n    'Interlingue': 'Italy',\n    'Malagasy': 'Madagascar',\n    'Swedish': 'Sweden',\n    'Breton': 'France',\n    'Somali': 'Somalia',\n    'Luxembourgish, Letzeburgesch': 'Luxembourg',\n    'Swahili': 'Kenya',\n    'Faroese': 'Denmark',\n    'Wolof': 'Senegal',\n    'Spanish; Castilian': 'Spain',\n    'Tsonga': 'Mozambique',\n    'Shona': 'Zambia',\n    'French': 'France',\n    'Tonga (Tonga Islands)': 'Tonga Islands',\n    'Southern Sotho': 'South Africa',\n    'Norwegian': 'Norway',\n    'Lithuanian': 'Lithuania',\n    'Malay': 'Malaysia',\n    'Tamil': 'India',\n    'Corsican': 'France',\n    'Japanese': 'Japan',\n    'Turkmen': 'Turkmenistan',\n    'Scottish Gaelic; Gaelic': 'Scotland',\n    'Nauru': 'Nauru',\n    'Samoan': 'Samoa',\n    'Estonian': 'Estonia',\n    'Waray-Waray': 'Philippines',\n    'Latvian': 'Latvia',\n    'Albanian': 'Albania',\n    'Slovak': 'Slovakia',\n    'Haitian; Haitian Creole': 'Haiti',\n    'Esperanto': 'Brazil',\n    'M\u0101ori': 'New Zealand',\n    'Bulgarian': 'Bulgaria',\n    'Sundanese': 'Indonesia',\n    'Finnish': 'Finland',\n    'Tatar': 'Russia',\n    'Afar': 'Ethiopia',\n    'Romanian, Moldavian, Moldovan': 'Romania',\n    'Chinese': 'China',\n    'Tswana': 'South Africa',\n    'Zhuang, Chuang': '',\n    'Serbian': 'Serbia',\n    'Cebuano': 'China',\n    'Lingala': 'Democratic Republic of the Congo',\n    'Catalan; Valencian': 'Spain',\n    'Ukrainian': 'Ukraine',\n    'Persian': 'Iran',\n    'Marathi (Mar\u0101\u1e6dh\u012b)': 'India',\n    'Guaran\u00ed': 'Paraguay',\n    'Korean': 'South Korea',\n    'Arabic': 'UAE',\n    'Bosnian': 'Bosnia',\n    'Vietnamese': 'Vietnam',\n    'Urdu': 'India',\n    'Thai': 'Thailand',\n    'Croatian': 'Croatia',\n    'Bengali': 'India',\n    'Kurdish': 'Iraq',\n    'Malayalam': 'India',\n    'Hindi': 'India',\n    'Macedonian': 'Macedonia',\n    'Aymara': 'Bolivia',\n    'Afrikaans': 'Australia',\n    'Georgian': 'Georgia',\n    'Oriya': 'India',\n    'Kannada': 'India',\n    'Russian': 'Russia',\n    'Tibetan Standard, Tibetan, Central': 'Tibet',\n    'Gujarati': 'India',\n    'Mongolian': 'Mangolia',\n    'Khmer': 'Vietnam',\n    'Kirundi': 'Tanzania',\n    'Nepali': 'Nepal',\n    'Sinhala, Sinhalese': '',\n    'Burmese': 'Burma',\n    'Kalaallisut, Greenlandic': '',\n    'Panjabi, Punjabi': 'India',\n    'Swati': 'South Africa',\n    'Yoruba': 'Nigeria',\n    'Kazakh': 'Kazakhstan',\n    'Hausa': 'Nigeria',\n    'Slovene': 'Slovenia',\n    'Tigrinya': 'Ethiopia',\n    'Pashto, Pushto': 'Pakistan',\n    'Akan': 'Ghana',\n    'Telugu': 'India',\n    'Bislama': 'Republic of Vanuatu',\n    'Igbo': 'Nigeria',\n    'Belarusian': 'Belarus'\n}","648eff7a":"train_data[\"country\"] = train_data.apply(lambda row: country_map[row[\"lang_name\"]], axis=1)\nval_data[\"country\"] = val_data.apply(lambda row: country_map[row[\"lang_name\"]], axis=1)\ntest_data[\"country\"] = test_data.apply(lambda row: country_map[row[\"lang_name\"]], axis=1)","be28d4e6":"import plotly.express as px","5751bdb6":"country_data_train = train_data.country.value_counts().rename_axis('country').reset_index(name='count')\nfig = px.choropleth(country_data_train.query(\"country != 'United Kingdom' and country != 'Unknown'\"), locations=\"country\", hover_name=\"country\",\n                     projection=\"natural earth\", locationmode=\"country names\", title=\"Training data - geographical distribution\", color=\"count\",\n                     template=\"plotly\", color_continuous_scale=\"agsunset\")\nfig.data[0].marker.line.color = 'rgb(0, 0, 0)'\nfig.data[0].marker.line.width = 0.2\nfig.show()","ad90007b":"country_data_val = val_data.country.value_counts().rename_axis('country').reset_index(name='count')\nfig = px.choropleth(country_data_val.query(\"country != 'United Kingdom' and country != 'Unknown'\"), locations=\"country\", hover_name=\"country\",\n                     projection=\"natural earth\", locationmode=\"country names\", title=\"Validation data - geographical distribution\", color=\"count\",\n                     template=\"plotly\", color_continuous_scale=\"agsunset\")\nfig.data[0].marker.line.color = 'rgb(0, 0, 0)'\nfig.data[0].marker.line.width = 0.2\nfig.show()","0a7bf3e5":"country_data_test = test_data.country.value_counts().rename_axis('country').reset_index(name='count')\nfig = px.choropleth(country_data_test.query(\"country != 'United Kingdom' and country != 'Unknown'\"), locations=\"country\", hover_name=\"country\",\n                     projection=\"natural earth\", locationmode=\"country names\", title=\"Test data - geographical distribution\", color=\"count\",\n                     template=\"plotly\", color_continuous_scale=\"agsunset\")\nfig.data[0].marker.line.color = 'rgb(0, 0, 0)'\nfig.data[0].marker.line.width = 0.2\nfig.show()","1e6b08db":"import seaborn as sns","3a14ab0f":"train_data['label'] = 0\ntrain_data.loc[(train_data.toxic == 1) | (train_data.obscene == 1) | \n           (train_data.insult == 1) | (train_data.identity_hate == 1) | \n           (train_data.severe_toxic == 1) | (train_data.threat == 1), 'label'] = 1","0fd6df07":"fig, ax = plt.subplots(nrows=1, ncols=1, figsize=(7,7), )\nax = sns.countplot(x=\"label\", data=train_data)\nfor i, p in enumerate(ax.patches):\n    height = p.get_height()\n    ax.text(p.get_x() + p.get_width()\/2., height + 0.7,\n        \"{:.2f}%\".format(train_data['label'].value_counts(normalize=True)[i]*100),\n            ha=\"center\", rotation=10)\nax.set_xticklabels(['Non-Toxic', 'Toxic'])\nax.set_xlabel(\"Comments\")\nax.set_ylabel(\"Count\")\nax.set_title('Toxic and Non-Toxic comments in training data', fontsize=14)\nfig.show()","c704a152":"fig, ax = plt.subplots(nrows=1, ncols=1, figsize=(7,7), )\nax = sns.countplot(x=\"toxic\", data=val_data)\nfor i, p in enumerate(ax.patches):\n    height = p.get_height()\n    ax.text(p.get_x() + p.get_width()\/2., height + 0.7,\n        \"{:.2f}%\".format(val_data['toxic'].value_counts(normalize=True)[i]*100),\n            ha=\"center\", rotation=10)\nax.set_xticklabels(['Non-Toxic', 'Toxic'])\nax.set_xlabel(\"Comments\")\nax.set_ylabel(\"Count\")\nax.set_title('Toxic and Non-Toxic comments in validation data', fontsize=14)\nfig.show()","e3858f5f":"comment_cat_df = pd.melt(train_data[['toxic','severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']])","f1526665":"\nfig, ax = plt.subplots(nrows=1, ncols=1, figsize=(7,7), )\nax = sns.countplot(data=comment_cat_df[comment_cat_df.value == 1], \n                   x='variable', \n                  order=comment_cat_df[comment_cat_df.value == 1].variable.value_counts().index)\nfor i, p in enumerate(ax.patches):\n    height = p.get_height()\n    ax.text(p.get_x() + p.get_width()\/2., height + 0.7,\n        \"{:.2f}%\".format(comment_cat_df[comment_cat_df.value==1].variable.value_counts(normalize=True)[i]*100),\n            ha=\"center\", rotation=10)\n    ax.set_xticklabels(ax.get_xticklabels(), rotation=10, ha=\"right\")\n    ax.set_xlabel(\"Toxic comments\")\n    ax.set_ylabel(\"Count\")\n\n\nax.set_title('Categories within toxic comments', fontsize=14)\nfig.show()","300600a3":"train_data.to_csv(\"notebook_one_train_data.csv\", index = False)\nval_data.to_csv(\"notebook_one_val_data.csv\", index = False)\ntest_data.to_csv(\"notebook_one_test_data.csv\", index = False)","6ec4679b":"## Findings\n\n1. Training data is heavily skewed. 90% of data is non-toxic. \n2. Validation data follows the same pattern. ","730cc35e":"## Findings\n\n**Training data**\n1. The top-5 non-english languages are German, Scot, Danish, Arabic and Spanish\n2. They constitutes 34% of total contribution. \n3. Considered the languages with counts more than mean count, which is 17. \n\n**Validation Data**\n\n1. Didn't adhere mean count criteria since there are only a few languages present in the data. \n2. There are only three languages present in validation dataset.\n    - Turkish\n    - Spanish\n    - Italian\n \n**Testing Data**\n\n1. Didn't adhere mean count criteria since there are only a few languages present in the data. \n2. There are only six languages present in validation dataset.\n    - Turkish\n    - Spanish\n    - Italian\n    - French\n    - Portugese\n    - Russian","419ef126":"# Understanding categories with in toxic comments","5b2cdb11":"# Geographical distribution of comment data\n\nEnglish is a universal language. How other languages are distributed in the data geographically? ","0cae3a66":"# About non-english comments","6c57deda":"## Findings \n\n1. Test and validation data doesn't seem to be really similar to training data. \n2. Training data shows a lot english words, but the validation and test data shows many non-english data. \n3. This is going to an issue. ","f2d07e0f":"## Findings \n\n1. The data is a highly skewed. 99% of the comments are in English.\n2. But the validation and test data contains only non-english comments. \n3. So viable solution is to translate these comments to english and build a model on it.\n4. On validation and testing, first translate the comment to English and pass it to the model for prediction. ","acc71b66":"## Findings\n\n1. Among the toxic comments, 43% is coming under `toxic` category. \n2. `obscene` and `toxic` categories overlaps the most.  \n3. `threat` category messages are the least in the training set. ","d57d91a6":"# Wordcloud - For training, val and test data","8b3b26ff":"# Save the updated datasets","1085706b":"# English Language v\/s Non-English languages","f33717af":"## Findings \n\n\n1. The english language was not considered since it is a universal language.\n2. That explains the absence or feeble presence of training data at USA, Canada etc. \n3. Also, the languages are allotted to majority speaking country. Eg. French is allotted only to France\n\n**Training data**\n\n1. According to the plot, comment data is coming from various countries all over the planet.\n2. In Asia, India has a significant representation in the data.\n3. Considering the population of China, surprisingly there is no instance of Mandarin. (But it could be in the list of unknown languages.)\n\n**Validation data**\n\n1. The validation data confines to only three countries\n    - Turkey\n    - Italy\n    - Spain\n    \n**Test data**\n\n1. The test data confines to only six countries\n    - Turkey\n    - Italy\n    - Spain\n    - Russia\n    - Portugal\n    - France","d70cab29":"# Understanding categories of training & validation data","a93fc4d4":"## Findings \n\n1. There are 717 examples in the data whose language is unknown.\n2. Among them, 616 are completely unknown (Language code: 'un')\n3. Rest of them are not categorized by the polyglot package. ('jw', 'hmn', 'haw', 'nso', 'zu', 'zzp', 'tlh', 'kha', 'iw', 'mfe', 'crs', 'syr', 'xx')\n3. Interestingly, testing and validation data has no such instances of an unknown language. \n4. Thus the unknown language comments can be removed from training data. ","11103330":"# Major non-english languages"}}