{"cell_type":{"a96f5961":"code","9fca89c5":"code","7a64cefc":"code","b3ca7d0e":"code","68af0d41":"code","15034499":"code","0111bfe5":"code","e5c066e0":"code","ea07106d":"code","62847a63":"code","60eb3209":"code","8f407bee":"code","e78d3674":"code","8f18de9a":"code","254a86b8":"code","f287e53e":"code","c6012cb9":"code","565b6b0d":"code","5bff7976":"code","7ce83d4e":"code","8c8bb94c":"code","0b83059d":"code","e84598aa":"code","6686fa1f":"code","ab6c8e00":"code","61b07c08":"code","35d09a7d":"code","6b732fe4":"code","5900b413":"code","1064b9dc":"code","6a17b9ad":"code","8b6baa4c":"code","253e770d":"code","250fffe4":"code","f05bea86":"code","16b96ceb":"code","98732122":"code","0a4a50f1":"code","8bd0b659":"code","dbecea71":"markdown","424a9077":"markdown","77f70975":"markdown","a3a5fb04":"markdown","715a72f2":"markdown","93810c39":"markdown","bcff9344":"markdown","477f8719":"markdown","ab56e199":"markdown","fb8ef1e0":"markdown","0118de30":"markdown","0aa9028e":"markdown","c1061a22":"markdown","7592dfe1":"markdown","0c57f2dd":"markdown","5a153acc":"markdown","e095a687":"markdown","33f9ad7d":"markdown","e3f7ef99":"markdown","dcce23dd":"markdown","11febaf5":"markdown","c7d6e561":"markdown"},"source":{"a96f5961":"import pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib\n\nimport matplotlib.pyplot as plt\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\nfrom sklearn.model_selection import train_test_split\nfrom scipy.stats import norm\n","9fca89c5":"train = pd.read_csv(\"..\/input\/train.csv\")\ntest = pd.read_csv(\"..\/input\/test.csv\")\nSalePrice = train['SalePrice']  # Separate the column \"SalePrice\"\ntrain_len = len(train) # the length og training data\nprint(\"The dimensions of training data is: {}\".format(train.shape))\nprint(\"The dimensions of testing data is: {}\".format(test.shape))\n","7a64cefc":"train.head()\n","b3ca7d0e":"test.head()\n","68af0d41":"all_data = pd.concat(objs=[train, test], axis=0, sort=False).reset_index(drop=True)  # combine the data\nall_data = all_data.fillna(np.nan) # fill the all different kinds of missing data with NaN\n","15034499":"all_data.head()\n","0111bfe5":"all_data.tail()\n","e5c066e0":"all_data.info()\n","ea07106d":"f, ax = plt.subplots(figsize=(12, 9))\ng = sns.heatmap(train.corr(),cmap=\"coolwarm\")\n","62847a63":"g = sns.heatmap(train[[\"SalePrice\", \"OverallQual\", \"GrLivArea\", \n                       \"TotalBsmtSF\", \"1stFlrSF\", \"GarageCars\", \n                       \"GarageArea\", \"YearBuilt\", \"FullBath\", \n                       \"TotRmsAbvGrd\",\"LotFrontage\", \"YearRemodAdd\", \n                       \"MasVnrArea\", \"BsmtFinSF1\",\"Fireplaces\",\"GarageYrBlt\"]].corr(),\n                cmap=\"coolwarm\")\n","60eb3209":"g = sns.pairplot(train[[\"SalePrice\", \"OverallQual\", \"GrLivArea\", \"TotalBsmtSF\", \n                        \"GarageCars\", \"YearBuilt\", \"FullBath\", ]], height = 2.5)\n","8f407bee":"train[[\"SalePrice\", \"OverallQual\", \"GrLivArea\", \"TotalBsmtSF\", \n       \"GarageCars\", \"YearBuilt\", \"FullBath\", ]].isnull().sum().sort_values(ascending=False)\n","e78d3674":"selected_feature = [\"OverallQual\", \"GrLivArea\", \"TotalBsmtSF\", \"GarageCars\", \"YearBuilt\", \"FullBath\"]\n","8f18de9a":"total = train.isnull().sum().sort_values(ascending=False)\npercent = (train.isnull().sum()\/train.isnull().count()).sort_values(ascending=False)\nmissing_count = pd.concat([total, percent], axis=1, keys=['Total', 'Percent'])\nmissing_count.head(20)\n","254a86b8":"all_data = all_data.drop((missing_count[missing_count['Total'] > 1]).index,1)\nall_data['Electrical'] = all_data['Electrical'].fillna(all_data['Electrical'].mode()[0]) # since there is only one missing, we fill it with omst common data\n","f287e53e":"new_data = all_data.select_dtypes(include='object')  # Categorical features\nfor f in selected_feature:\n    new_data[f] = all_data[f]  # Numerical features\nnew_data.info()\n","c6012cb9":"new_data.head()\n","565b6b0d":"new_data.tail()","5bff7976":"new_data.isnull().sum().sort_values(ascending=False)\n","7ce83d4e":"new_data['MSZoning'] = new_data['MSZoning'].fillna(new_data['MSZoning'].mode()[0])\nnew_data['Utilities'] = new_data['Utilities'].fillna(new_data['Utilities'].mode()[0])\nnew_data['Functional'] = new_data['Functional'].fillna(new_data['Functional'].mode()[0])\nnew_data['TotalBsmtSF'] = new_data['TotalBsmtSF'].fillna(0)\nnew_data['Exterior1st'] = new_data['Exterior1st'].fillna(new_data['Exterior1st'].mode()[0])\nnew_data['Exterior2nd'] = new_data['Exterior2nd'].fillna(new_data['Exterior1st'].mode()[0])\nnew_data['GarageCars'] = new_data['GarageCars'].fillna(0.0)\nnew_data['SaleType'] = new_data['SaleType'].fillna(new_data['SaleType'].mode()[0])\nnew_data['KitchenQual'] = new_data['KitchenQual'].fillna(new_data['KitchenQual'].mode()[0])\n","8c8bb94c":"new_data.isnull().sum().sort_values(ascending=False)\n","0b83059d":"for col in new_data.dtypes[new_data.dtypes == 'object'].index:\n    new_data[col] = new_data[col].astype('category')  # converting to a category dtype\n    new_data[col] = new_data[col].cat.codes\nprint(new_data.shape)\n","e84598aa":"new_data.head()\n","6686fa1f":"new_data=(new_data-new_data.mean())\/new_data.std()\n","ab6c8e00":"new_data.head()","61b07c08":"SalePrice.describe()\n","35d09a7d":"g = sns.distplot(SalePrice, fit=norm)\n","6b732fe4":"print('Skewness : {}'.format(SalePrice.skew()))\nprint('Kurtosis : {}'.format(SalePrice.kurt()))\n","5900b413":"SalePrice = np.log(SalePrice)\n","1064b9dc":"g = sns.distplot(SalePrice, fit=norm);\n","6a17b9ad":"train_info = new_data[:train_len]\ntrain_label = SalePrice\ntrain = pd.concat([train_info, train_label], axis=1, sort=False)\n\ntest_info = new_data[train_len:]\n","8b6baa4c":"from sklearn.linear_model import ElasticNet, Lasso\nfrom sklearn.ensemble import GradientBoostingRegressor\nfrom sklearn.kernel_ridge import KernelRidge\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import RobustScaler\nfrom sklearn.model_selection import KFold, cross_val_score\nfrom xgboost import XGBRegressor\n","253e770d":"n_folds = 5\n\ndef rmsle_cv(model):\n    kf = KFold(n_folds, shuffle=True, random_state=42).get_n_splits(train.values)\n    rmse= np.sqrt(-cross_val_score(model, train_info.values, train_label.values, scoring=\"neg_mean_squared_error\", cv = kf))\n    return(rmse)\n","250fffe4":"lasso = make_pipeline(RobustScaler(), Lasso(alpha =0.0005, random_state=42))\n\nENet = make_pipeline(RobustScaler(), ElasticNet(alpha=0.0005, l1_ratio=.8, random_state=3))\n\nKR = KernelRidge(alpha=6, kernel='polynomial', degree=2, coef0=2.5)\n\nGBoost = GradientBoostingRegressor(n_estimators=2000, learning_rate=0.05, max_depth=4, \n                                   max_features='sqrt',min_samples_leaf=15, min_samples_split=10, \n                                   loss='huber',random_state =5)\n\nXGBoost = XGBRegressor(n_estimators=2000, learning_rate=0.05, random_state =7)\n","f05bea86":"score = rmsle_cv(lasso)\nprint(\"\\nLasso score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))\n\nscore = rmsle_cv(ENet)\nprint(\"ElasticNet score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))\n\nscore = rmsle_cv(KR)\nprint(\"Kernel Ridge score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))\n\nscore = rmsle_cv(GBoost)\nprint(\"Gradient Boosting score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))\n\nscore = rmsle_cv(XGBoost)\nprint(\"Xgboost score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))\n","16b96ceb":"GBoost.fit(train_info, train_label)\nres = GBoost.predict(test_info)\n","98732122":"res = np.expm1(res)\nprint(res)","0a4a50f1":"prediction = pd.DataFrame(res, columns=['SalePrice'])\nresult = pd.concat([test['Id'], prediction], axis=1)\nresult.columns","8bd0b659":"result.to_csv('.\/submission.csv', index=False)","dbecea71":"I choose the GBoost as final model","424a9077":"## SalePrice Analysis","77f70975":"## Choose features (Numerical)\nIn this part, we deal eoth the numerical data. We first see how these data are related to the sal price then we only keep the most related few as training data","a3a5fb04":"We simply abandone the features that have more than 15% missing data\n\nThe features begin with \"Garage\" are highly related to the feature \"GarageCars\" which is already considered in numerical feature. So we simple abandone these features. So as features begin with \"Bsmt\" and \"MasVnr\"\n\nThus the only feature we care and with missing data is \"Electrical\"","715a72f2":"Define models","93810c39":"Now the data is more \"Normal\" than the original data","bcff9344":"## Choose features (Categorical)","477f8719":"A more normal distributed data will be beneficial to our training","ab56e199":"## Generate new data set (only have chosen features)","fb8ef1e0":"See the score for different models","0118de30":"No missing data","0aa9028e":"## Deal with chosen features (Numerical)","c1061a22":"## Data for Training\nHere we prepare the data for training and predicting","7592dfe1":"Since we take the natural log of the \"Saleprice\", we need to undo that.","0c57f2dd":"We can see from the pllt that the following data are closely related to sale price compared to others.  \n\nThey are:    \n\"SalePrice\", \"OverallQual\", \"GrLivArea\", \"TotalBsmtSF\", \"1stFlrSF\", \"GarageCars\", \"GarageArea\", \"YearBuilt\", \"FullBath\", \"TotRmsAbvGrd\",\"LotFrontage\", \"YearRemodAdd\", \"MasVnrArea\", \"BsmtFinSF1\",\"Fireplaces\",\"GarageYrBlt\"\n\nSo we zoom in the plot to conduct a second choose.","5a153acc":"There is no missing data in these features, so we don't need to do anything!","e095a687":"# Submission","33f9ad7d":"The final choice are made as follows: \"OverallQual\", \"GrLivArea\", \"TotalBsmSF\", \"GarageCars\", \"FullBath\", \"YearBuilt\"  \nOther abandoned data are either noe so related to sale price or highly related to one of the chosen data","e3f7ef99":"## Preapre the data\n","dcce23dd":"There are still some missing data in the test set. So we need to fill them.","11febaf5":"# ML on House Pricing\n## Yutao Chen\n## 04\/15\/2019","c7d6e561":"Normalize the data"}}