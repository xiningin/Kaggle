{"cell_type":{"7f8f147c":"code","092fc464":"code","b4c18f0c":"code","bf886d8b":"code","969aa337":"code","981f46da":"code","002b9d0a":"code","d11cde34":"code","0167a9f5":"code","490d9cb7":"code","4243acf5":"code","569993b2":"code","b08cc08e":"code","f01d0afb":"code","4f3012af":"code","fb301c0f":"code","181ac39d":"code","8c2e3381":"code","522a5070":"code","b8362a98":"code","3bf52c68":"code","619ce4d6":"code","68a6ac5e":"code","ae3b6e9d":"code","03593721":"code","010fec9f":"code","54f9513a":"code","010d5c28":"code","a26f7ec6":"code","dedd9351":"code","6c475b7c":"code","a854d8d6":"code","4e795cff":"code","c3b87298":"code","ffb199b8":"markdown","573bfaff":"markdown","c6d697ca":"markdown","9faa7fdb":"markdown","9611cd5d":"markdown","96c5c8f2":"markdown","e501f511":"markdown","c4b6d891":"markdown","884ed0b6":"markdown","59a07646":"markdown","a2053c5a":"markdown","d3052940":"markdown","f189b859":"markdown","a8842e35":"markdown","8e8e14aa":"markdown","85e757b1":"markdown","e0eb7972":"markdown","35956ab5":"markdown","9f6f08c7":"markdown","8d2b97d5":"markdown","9840cb68":"markdown","ad12b1ac":"markdown","96381afb":"markdown"},"source":{"7f8f147c":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","092fc464":"import pandas as pd\nimport numpy as np\n\nimport matplotlib.pyplot as plt\nfrom matplotlib import cm\nplt.style.use('ggplot')\n%matplotlib inline\nimport seaborn as sns\nsns.set(color_codes=True)\n\nfrom sklearn.preprocessing import LabelEncoder, OneHotEncoder\nfrom sklearn.model_selection  import train_test_split, KFold, StratifiedKFold  \nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.metrics import accuracy_score, make_scorer, roc_auc_score\n\nfrom sklearn.ensemble import RandomForestClassifier\nimport xgboost as xgb\nfrom xgboost.sklearn import XGBClassifier\nfrom xgboost import plot_importance","b4c18f0c":"train_data = pd.read_csv(\"..\/input\/titanic\/train.csv\")\ntest_data = pd.read_csv(\"..\/input\/titanic\/test.csv\")\n\nwhole_data = train_data.append(test_data)\nwhole_data.reset_index(inplace=True)\nwhole_data.drop('index',inplace=True,axis=1)\nprint(\"Dimension of whole dataset: {}\".format(whole_data.shape)+\"\\n\")\nprint(\"Dimension of train dataset: {}\".format(train_data.shape)+\"\\n\")\nprint(\"Dimension of test dataset: {}\".format(test_data.shape)+\"\\n\")\n\n","bf886d8b":"train_data = train_data.drop('PassengerId', axis = 1)\n","969aa337":"train_data.head()","981f46da":"train_data.info()","002b9d0a":"train_data.apply(lambda x: sum(x.isnull()))","d11cde34":"test_data.apply(lambda x: sum(x.isnull()))","0167a9f5":"whole_data.loc[:,['Age', 'SibSp', 'Parch', 'Fare']].describe()","490d9cb7":"cat_var=['Survived','Pclass', 'Sex', 'Embarked']\n\nfor i in cat_var:\n     print (train_data[i].value_counts()) \n","4243acf5":"plt.figure(figsize=(5,5))\nlabels = ('Not survived', 'Survived')\nx_pos = np.arange(len(labels))\nplt.bar(x_pos, train_data['Survived'].value_counts(), align='center', alpha=0.5, width=0.25)\nplt.xticks(x_pos, labels)\nplt.ylabel('Frequency')\nplt.title('Survival Count')","569993b2":"plt.figure(figsize=(5,5))\nlabels = ('Not survived', 'Survived')\nx_pos = np.arange(len(labels))\ndf_temp = pd.crosstab(train_data[\"Survived\"], train_data['Pclass']).plot(kind = 'bar', stacked='True')\nplt.xticks(x_pos, labels)\nplt.ylabel('Frequency')\nplt.title('Survival Count')\nplt.title('Survived'+\" vs \"+'Pclass')\n","b08cc08e":"ax = sns.FacetGrid(train_data,  margin_titles=True, size = 4)\nax = ax.map(sns.barplot, 'Pclass', 'Survived') \nax.add_legend()","f01d0afb":"plt.figure(figsize=(5,5))\nlabels = ('Not survived', 'Survived')\nx_pos = np.arange(len(labels))\ndf_temp = pd.crosstab(train_data[\"Survived\"], train_data['Sex']).plot(kind = 'bar', stacked='True')\nplt.xticks(x_pos, labels)\nplt.ylabel('Frequency')\nplt.title('Survival Count')\nplt.title('Survived'+\" vs \"+'Sex')","4f3012af":"ax = sns.FacetGrid(train_data,  margin_titles=True, size = 4)\nax = ax.map(sns.barplot, 'Sex', 'Survived') \nax.add_legend()","fb301c0f":"ax = sns.FacetGrid(train_data,  margin_titles=True, size = 4)\nax = ax.map(sns.barplot, 'Embarked', 'Survived') \nax.add_legend()","181ac39d":"plt.figure(figsize=(12,10))\nplt.hist([train_data[train_data['Survived']==1]['Age'].dropna(), train_data[train_data['Survived']==0]['Age'].dropna()], stacked=True, color = ['g','b'],\nbins = 20,label = ['Survived','Not survived'])\nplt.xlabel('Age')\nplt.ylabel('Count of passengers')\nplt.legend()","8c2e3381":"whole_data.boxplot(column ='Age', by = ['Sex'])","522a5070":"whole_data.boxplot(column ='Age', by = ['Sex','Pclass'])","b8362a98":"corr = whole_data.corr()\nax = plt.subplots(figsize =(7, 6))\nsns.heatmap(corr,annot = True)","3bf52c68":"whole_data.drop(['PassengerId', 'Survived'], axis =1).apply(lambda x: sum(x.isnull()))","619ce4d6":"whole_data['Fare'].fillna(whole_data.Fare.median(), inplace = True)\nwhole_data['Embarked'].fillna(whole_data.Embarked.mode()[0], inplace = True)\nwhole_data['Cabin'].fillna(\"N\", inplace = True) \nwhole_data['Cabin']=whole_data['Cabin'].dropna().map(lambda x: x[0])","68a6ac5e":"pd.crosstab(whole_data['Pclass'], whole_data['Cabin'])","ae3b6e9d":"for i, row in whole_data.iterrows():\n    whole_data.loc[(whole_data['Pclass']==1) & (whole_data['Cabin']==\"N\"), 'Cabin'] = 'C'\n    whole_data.loc[(whole_data['Pclass']==2) & (whole_data['Cabin']==\"N\"), 'Cabin'] = 'F'\n    whole_data.loc[(whole_data['Pclass']==3) & (whole_data['Cabin']==\"N\"), 'Cabin'] = 'F'","03593721":"whole_data['Age'] = whole_data.groupby(['Sex','Pclass'])['Age'].transform(lambda x: x.fillna(x.median()))","010fec9f":"whole_data= whole_data.drop(['Name', 'Parch', 'SibSp', 'Ticket'], axis=1)","54f9513a":"var_mod=whole_data.dtypes[whole_data.dtypes == \"object\"].index.tolist()\nvar_mod.append('Pclass')\nle = LabelEncoder()\nfor i in var_mod:\n    whole_data[i] = le.fit_transform(whole_data[i])","010d5c28":"predictor = whole_data.drop(['PassengerId', 'Survived'], axis = 1).columns.tolist()\ndataset = whole_data[:891].drop('PassengerId', axis=1)\noutcome = 'Survived'\n\nX_train, X_test, y_train, y_test = train_test_split(dataset[predictor], dataset[outcome], test_size = 0.30, random_state = 0)\n    \ndef _modelfit(xgb_model, dataset, outcome, **kwargs):  \n    dtrain = xgb.DMatrix(X_train.values, label=y_train.values)\n    cvresult = xgb.cv(xgb_model.get_xgb_params(), dtrain, num_boost_round=xgb_model.get_params()['n_estimators'], nfold=5, metrics='error', early_stopping_rounds=50, seed = 0)  \n    n_boosting=cvresult.index[cvresult['test-error-mean'] == cvresult['test-error-mean'].min()].tolist()[0]+1\n    print(cvresult)\n    xgb_model.set_params(n_estimators=n_boosting)\n    xgb_model.fit(X_train, y_train, eval_metric='error') #auc\n    dtrain_predictions = xgb_model.predict(X_test)\n    dtrain_predprob = xgb_model.predict_proba(X_test)[:,1]\n    print (\"Accuracy score (test): {}\".format(accuracy_score(y_test, dtrain_predictions)))\n    \n    \n    plot_importance(xgb_model, color = 'green')\n    plt.show()\n    \nclf = XGBClassifier(max_depth=6, n_estimators=2000, objective='binary:logistic', \n                     subsample=1.0, colsample_bytree=1.0, random_state = 0)\n\n_modelfit(clf, dataset, outcome, predictor = predictor)","a26f7ec6":"def _grid_depth_weight(dataset, predictor, outcome):\n       \n    param_test = {\n     'max_depth':[i for i in range(3,11,1)],\n     'min_child_weight':[i for i in range(1,11,1)]    \n    }\n    \n    clf = XGBClassifier(n_estimators = 23, objective='binary:logistic', subsample=1.0, colsample_bytree=1.0, random_state=0)\n     \n    _grid = GridSearchCV(clf, param_grid=param_test, scoring='accuracy', \n                         n_jobs=4, cv=StratifiedKFold(n_splits=5)).fit(X_train,y_train)\n    \n    clf=_grid.best_estimator_.fit(X_train,y_train)\n    \n    return _grid.best_params_, _grid.best_score_\n\n_grid_depth_weight(dataset, predictor, outcome)","dedd9351":"def _grid_gamma(dataset, predictor, outcome):\n    \n    param_test = {\n      'gamma':[i\/10.0 for i in range(0,20)]   \n    }\n    \n    clf = XGBClassifier(n_estimators =23, objective='binary:logistic', max_depth=6, min_child_weight=5, \n                        subsample=1.0, colsample_bytree=1.0, random_state=0)\n    \n    _grid = GridSearchCV(clf, param_grid = param_test, scoring='accuracy',n_jobs=4, \n                         cv=StratifiedKFold(n_splits=5)).fit(X_train,y_train)\n    \n    clf=_grid.best_estimator_.fit(X_train, y_train)\n    \n    return _grid.best_params_, _grid.best_score_\n\n_grid_gamma(dataset, predictor, outcome)","6c475b7c":"def _grid_sample(dataset, predictor, outcome):\n    \n    param_test = {\n    'subsample':[i\/10.0 for i in range(1,11)],\n    'colsample_bytree':[i\/10.0 for i in range(1,11)]  \n    }\n    \n    clf = XGBClassifier(n_estimators=23, objective='binary:logistic', max_depth=6, min_child_weight=5, \n                                                     gamma = 0.0, seed =0)\n    \n    _grid = GridSearchCV(clf, param_grid = param_test, scoring='accuracy',n_jobs=4, \n                         cv=StratifiedKFold(n_splits=5)).fit(X_train,y_train)\n    \n    clf=_grid.best_estimator_.fit(X_train,y_train)\n    \n    return _grid.best_params_, _grid.best_score_\n\n_grid_sample(dataset, predictor, outcome)","a854d8d6":"def _grid_reg(dataset, predictor, outcome):\n    \n    param_test = {\n    'reg_alpha':[0, 0.001, 0.005, 0.01, 0.05, 1, 100]  \n    }\n    \n    clf = XGBClassifier(n_estimators=23, objective='binary:logistic', max_depth=6, min_child_weight=5, \n                                                     gamma = 0.0, subsample = 0.9, colsample_bytree = 0.7, seed = 0)   \n                         \n    _grid = GridSearchCV(clf, param_grid = param_test, scoring='accuracy',n_jobs=4, \n                         cv=StratifiedKFold(n_splits=5)).fit(X_train,y_train)\n    \n    clf=_grid.best_estimator_.fit(X_train,y_train)\n    \n    return _grid.best_params_, _grid.best_score_\n\n_grid_reg(dataset, predictor, outcome)","4e795cff":"clf = XGBClassifier(\n learning_rate =0.05,\n n_estimators=2000,\n max_depth=8,\n min_child_weight=3,\n gamma=0.3,\n subsample=1,\n colsample_bytree=0.9,\n reg_alpha = 0.005,\n objective='binary:logistic',\n random_state=0)\n\n_modelfit(clf, dataset, outcome)","c3b87298":"Survived = clf.predict(whole_data[891:].drop(['Survived', 'PassengerId'], axis=1)).astype(int)\nPassengerId = whole_data[891:].PassengerId\nxgb = pd.DataFrame({'PassengerId': PassengerId ,'Survived': Survived})\nprint(\"Dimension of final set: {}\".format(xgb.shape)+\"\\n\")\nprint(xgb.head())\nxgb.to_csv('final_2.csv', index = False)","ffb199b8":"# Visualizing","573bfaff":"Imputing missing values","c6d697ca":"The female passengers in the lower class of tickets are considered as the youngest group of Titanic passenfers","9faa7fdb":"We don't need PassengerID, So droping this column","9611cd5d":"Most people survived are females.","96c5c8f2":"Exploring the distribution of Pclass by survival variable.","e501f511":"# Applying XGBoost","c4b6d891":"There are some missing values.","884ed0b6":"'Survived','Pclass', 'Sex', 'Embarked' have categorical data.","59a07646":"Plotting the correlation matrix which investigates the dependence between multiple variables. As shown by the correlation plot, SibSp and Parch are quite correlated.","a2053c5a":"# Titanic Dataset Analysis with XGBoost","d3052940":"# **Exploring Data**","f189b859":"# Importing necessaary Packages","a8842e35":"The passnegers holding higher-class tickets have the highest chance of survival. They are, then, followed by the middle-class tickets. ","8e8e14aa":"Passengers' age distribution per gender in the full dataset.","85e757b1":"Summary of the neumerical data. 'Age', 'SibSp', 'Parch', 'Fare' have neumerical data.","e0eb7972":"The number of missing values per each variable in Titanic dataset, except the target one, is given below.","35956ab5":"# Loading Datasets[](http:\/\/)","9f6f08c7":"The missing value for this feature can be estimated wisely given the mode value for all categories of cabin number per each Pclass.","8d2b97d5":"# Exporting Output file","9840cb68":" The same analysis is done for gender and embarkation port as shown below.","ad12b1ac":"Transforming categorical values","96381afb":"As we have seen females have higher chance than males. Now we will see the distribution of ages."}}