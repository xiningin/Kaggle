{"cell_type":{"19df7ef9":"code","a222ac3e":"code","3d3fd10f":"code","3c8d92eb":"code","d67f25c2":"code","802ff488":"code","bca532d8":"code","6ae7c57e":"code","7f7c32cf":"code","a6a6f98f":"code","b28f00f2":"code","f4d362a9":"code","a65cfc5d":"code","2aa1b864":"code","82569019":"code","80101a92":"code","12884814":"code","30666aa5":"code","00ca98d0":"code","a04e910a":"code","ebccf981":"markdown","1666078a":"markdown","fffe6691":"markdown","a1bbeac1":"markdown","5ea83c3b":"markdown","128bff3f":"markdown","35955dbe":"markdown","2b06fc34":"markdown","66754f8e":"markdown","8569c357":"markdown"},"source":{"19df7ef9":"!pip install -q efficientnet","a222ac3e":"!pip install transformers","3d3fd10f":"import numpy as np \nimport pandas as pd \nimport os\nfrom matplotlib import pyplot as plt\nimport seaborn as sns\n\nimport tensorflow as tf\nimport tensorflow.keras.layers as l\nfrom keras.regularizers import l2\nfrom keras.optimizers import Adam\nimport efficientnet.tfkeras as efn\nfrom sklearn.model_selection import train_test_split\nfrom kaggle_datasets import KaggleDatasets\n\n\nfrom transformers import BertForSequenceClassification, AdamW, BertConfig\nfrom transformers import BertTokenizer\nfrom transformers import get_linear_schedule_with_warmup","3c8d92eb":"tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\ntf.config.experimental_connect_to_cluster(tpu)\ntf.tpu.experimental.initialize_tpu_system(tpu)\n\n# instantiate a distribution strategy\ntpu_strategy = tf.distribute.experimental.TPUStrategy(tpu)\n\nprint(tpu.master())\nprint(tpu_strategy.num_replicas_in_sync)","d67f25c2":"# For tensorflow dataset\nAUTO = tf.data.experimental.AUTOTUNE\nignore_order = tf.data.Options()\nignore_order.experimental_deterministic = False\n\n# Pass\ngcs_path = KaggleDatasets().get_gcs_path('alaska2-image-steganalysis')","802ff488":"sample = pd.read_csv(\"\/kaggle\/input\/alaska2-image-steganalysis\/sample_submission.csv\")\nBATCH_SIZE = 8 * tpu_strategy.num_replicas_in_sync # batch size in tpu\nEPOCHS = 1\n\n#Variables\n\ndir_name = ['Test', 'JUNIWARD', 'JMiPOD', 'Cover', 'UERD']\ndf = pd.DataFrame({})\nlists = []\ncate = []\n\n#get filenames\nfor dir_ in dir_name:\n    # file name\n    list_ = os.listdir(\"\/kaggle\/input\/alaska2-image-steganalysis\/\"+dir_+\"\/\")\n    lists = lists+list_\n    # category name\n    cate_ = np.tile(dir_,len(list_))\n    cate = np.concatenate([cate,cate_])\n    \n#to dataframe\ndf[\"cate\"] = cate\ndf[\"name\"] = lists","bca532d8":"#add path to df\ndf[\"path\"] = [str(os.path.join(gcs_path,cate,name)) for cate, name in zip(df[\"cate\"], df[\"name\"])]\n\n#Labeling func\ndef cate_label(x):\n    if x[\"cate\"] == \"Cover\":\n        res = 0\n    else:\n        res = 1\n    return res\n\n#Training & test sets\nTest_df = df.query(\"cate=='Test'\").sort_values(by=\"name\")\nTrain_df = df.query(\"cate!='Test'\")\n\n#apply Labeling func\nTrain_df[\"labled\"] = df.apply(cate_label, axis=1)","6ae7c57e":"print(\"Training set: \\n\",Train_df[\"cate\"].value_counts())\n\nprint('\\n', Train_df[\"path\"].head())\nprint('\\n',Train_df[\"labled\"].head())\nprint('\\n',Test_df[\"path\"].head())","7f7c32cf":"#Validating set\n\n\nX = Train_df[\"path\"]\ny = Train_df[\"labled\"]\nz = Test_df[\"path\"]\n\n#Train & test split\nX_train, X_val, y_train, y_val = train_test_split(X,y, test_size=0.2, random_state=10)\n\n#convert to numpy array\nX_train, X_val, y_train, y_val = np.array(X_train), np.array(X_val), np.array(y_train), np.array(y_val)\n\n#test set\nX_test = np.array(Test_df[\"path\"])","a6a6f98f":"print(X_test[7])\nprint(X_train[7])\nprint(X_val[7])\nprint(y_train[7])\nprint(y_val[7])","b28f00f2":"def decode_image(filename, label=None, image_size=(512,512)):\n    bits = tf.io.read_file(filename)\n    image = tf.image.decode_jpeg(bits, channels=3)\n    image = tf.cast(image, tf.float32)\/255.0\n    image = tf.image.resize(image, image_size)\n    \n    if label is None:\n        return image\n    else:\n        return image, label","f4d362a9":"train_dataset = (\n    tf.data.Dataset\n    .from_tensor_slices((X_train, y_train))\n    .map(decode_image, num_parallel_calls=AUTO)\n    .repeat()\n    .shuffle(1024)\n    .batch(BATCH_SIZE)\n    .prefetch(AUTO)\n)\n\nvalid_dataset = (\n    tf.data.Dataset\n    .from_tensor_slices((X_val, y_val))\n    .map(decode_image, num_parallel_calls=AUTO)\n    .batch(BATCH_SIZE)\n    .prefetch(AUTO)\n)\n\ntest_dataset = (\n    tf.data.Dataset\n    .from_tensor_slices(X_test)\n    .map(decode_image, num_parallel_calls=AUTO)\n    .batch(BATCH_SIZE)\n)","a65cfc5d":"with tpu_strategy.scope():\n    model = tf.keras.Sequential([\n        efn.EfficientNetB3(\n            input_shape=(512, 512, 3),\n            weights='imagenet',\n            include_top=False\n        ),\n        #l.Dense(32, activation=\"relu\",kernel_regularizer=l2(0.001), bias_regularizer=l2(0.001)),\n        #l.Dropout(0.4),\n       # l.BatchNormalization(),\n        l.GlobalAveragePooling2D(),\n        #l.BatchNormalization(),\n        #l.Activation('relu'),\n        l.Dropout(0.1),\n       # l.Dense(1),\n        l.Dense(1, activation='sigmoid')\n    ])\n    opt = Adam(lr=0.002, beta_1=0.9, beta_2=0.999, decay=0.01, amsgrad=False)    \n    model.compile(\n        optimizer=opt,\n        loss = 'binary_crossentropy',\n        metrics=['accuracy']\n    )","2aa1b864":"model.summary()","82569019":"STEPS_PER_EPOCH = X_train.shape[0] \/\/ BATCH_SIZE\n#callbacks = [tf.keras.callbacks.EarlyStopping(patience=3, restore_best_weights=True)] \n\nhistory = model.fit(train_dataset, steps_per_epoch=STEPS_PER_EPOCH, epochs=EPOCHS, validation_data=valid_dataset)","80101a92":"model.save(\"Mymodel.h5\") ","12884814":"#plt.clf()\n#history_dict = history.history\n#loss_values = history_dict['loss']\n#val_loss_values = history_dict['val_loss']\n#epochs = range(1, (len(history_dict['loss']) + 1))\n#plt.plot(epochs, loss_values, 'bo', label='Training loss')\n#plt.plot(epochs, val_loss_values, 'b', label='Validation loss')\n#plt.title('Training and validation loss')\n#plt.xlabel('Epochs')\n#plt.ylabel('Loss')\n#plt.legend()\n#plt.show()","30666aa5":"#plt.clf()\n#acc_values = history_dict['accuracy']\n#val_acc_values = history_dict['val_accuracy']\n#epochs = range(1, (len(history_dict['accuracy']) + 1))\n#plt.plot(epochs, acc_values, 'bo', label='Training acc')\n#plt.plot(epochs, val_acc_values, 'b', label='Validation acc')\n#plt.title('Training and validation accuracy')\n#plt.xlabel('Epochs')\n#plt.ylabel('Accuracy')\n#plt.legend()\n#plt.show()","00ca98d0":"pred = model.predict(test_dataset, verbose=1)","a04e910a":"my_sample = sample.copy()\nmy_sample[\"Label\"] = pred\nmy_sample.to_csv(\"my_sample.csv\", index=False)\nmy_sample.head()","ebccf981":"Dataset objects","1666078a":"Train the model","fffe6691":"Building the model","a1bbeac1":"Building the inputs","5ea83c3b":"Data & path preprocessing","128bff3f":"Paths & Hyper-Parameters & filenames","35955dbe":"**Data access**","2b06fc34":"Load Dependencies","66754f8e":"**TPU Setting:**","8569c357":"Prediction"}}