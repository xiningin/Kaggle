{"cell_type":{"9232286a":"code","feff060f":"code","ff11b950":"code","00186983":"code","8564433c":"code","8edee92e":"code","d136ad25":"code","6f9c76bc":"code","6a652b8d":"code","90474807":"code","043b9612":"code","90d7c91c":"code","81a766d3":"code","d63d4522":"code","cf885f7b":"code","138e2892":"code","316e0694":"code","9a6c470d":"code","e4bea430":"code","3b8f8607":"code","41d38f86":"code","069738da":"code","f5baf35a":"code","823d18b4":"code","d1b0b37e":"code","0cf34f12":"code","39a8995c":"code","71a24817":"code","38eac2d3":"code","36ad250b":"code","046936b6":"code","a9fd041d":"code","8f7abb99":"code","4e65995e":"code","abc715a9":"code","a7891a21":"code","51c0b3a7":"code","b024fcbe":"code","f06e2449":"code","be7e248a":"code","0b49cf87":"code","ed8f2fe9":"code","626b77b8":"code","2e77cd4f":"markdown","ffb0104b":"markdown","b37db972":"markdown","b20d27a2":"markdown","93b6928e":"markdown","9db77a60":"markdown","4280b689":"markdown","39b70120":"markdown","6ca408a1":"markdown","8c769dc8":"markdown","433595b7":"markdown","8be7ac65":"markdown","eec4572c":"markdown","5ddef9fd":"markdown","bf4cf343":"markdown","dd790f19":"markdown","de0faf4b":"markdown","cd0cae75":"markdown","5c02be55":"markdown","a95dcc4d":"markdown","12ed76dc":"markdown","0de165e2":"markdown"},"source":{"9232286a":"import numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n%matplotlib inline","feff060f":"data = pd.read_csv('..\/input\/creditcardfraud\/creditcard.csv')\ndata.columns","ff11b950":"data.head()","00186983":"# Check the types of each feature\ndata.dtypes","8564433c":"# Check the shape of the data\ndata.shape","8edee92e":"# Check null values\ndata.isnull().sum()","d136ad25":"# Check the data summary\ndata.describe()","6f9c76bc":"print(\"Fraud Statistics\")\n\navg_amt = data[data['Class']== 1]['Amount'].mean()\nstd_dev_amt = data[data['Class']== 1]['Amount'].std()\nmin_amt = data[data['Class']== 1]['Amount'].min()\nmax_amt = data[data['Class']== 1]['Amount'].max()\n\nprint(f\"The average amount is {avg_amt}\")\nprint(f\"The std deviation for amount is {std_dev_amt}\")\nprint(f\"The min amount is {min_amt}\")\nprint(f\"The max amount is {max_amt}\")","6a652b8d":"# Check classes counts\ndata['Class'].value_counts()","90474807":"# Percentage\nprint(f'Non-Fraud is: {100*284315\/(284315+492)}%')","043b9612":"sns.distplot(data['Time'])","90d7c91c":"sns.distplot(data[data['Class'] == 1]['Amount'])","81a766d3":"plt.figure(figsize = (27,15))\ncorrelation_matrix = data.corr()\nsns.heatmap(correlation_matrix, annot=True,fmt=\".2f\")","d63d4522":"dataC = data.copy()","cf885f7b":"from sklearn.preprocessing import StandardScaler\n\nsc = StandardScaler()\n\ndataC[['Time', 'Amount']] = sc.fit_transform(dataC[['Time', 'Amount']])","138e2892":"dataC.describe()","316e0694":"frauds = dataC[dataC['Class']==1].copy()\nnon_frauds = dataC[dataC['Class']==0].copy()","9a6c470d":"x_train_fraud = frauds.sample(frac=0.8, random_state=27)\nx_test_fraud = frauds.loc[~frauds.index.isin(x_train_fraud.index)]\n\nx_train_non_fraud = non_frauds.sample(frac=0.8, random_state=27)\nx_test_non_fraud = non_frauds.loc[~non_frauds.index.isin(x_train_non_fraud.index)]","e4bea430":"print(f'Train \\'fraud\\' shape is: {x_train_fraud.shape}')\nprint(f'Test \\'fraud\\' shape is: {x_test_fraud.shape}')\nprint(f'Train \\'non-fraud\\' shape is: {x_train_non_fraud.shape}')\nprint(f'Test \\'non-fraud\\' shape is: {x_test_non_fraud.shape}')","3b8f8607":"train = pd.concat([x_train_non_fraud, x_train_fraud])\ntest = pd.concat([x_test_non_fraud, x_test_fraud])","41d38f86":"# Shuffle dataframes in-place and reset the index\ntrain = train.sample(frac=1, random_state=27).reset_index(drop=True)\ntest = test.sample(frac=1, random_state=27).reset_index(drop=True)","069738da":"print(f'Original size is: {data.shape}')\nprint(f'Train size is: {train.shape} with perc = {train.shape[0]\/data.shape[0]}')\nprint(f'Test size is: {test.shape} with perc = {test.shape[0]\/data.shape[0]}')","f5baf35a":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import roc_auc_score, precision_recall_fscore_support","823d18b4":"def evaluate_model(labels, preds):\n    accuracy = (preds == labels).sum() \/ preds.shape[0]\n    print(f'Accuracy: {accuracy}')\n\n    auc = roc_auc_score(labels, preds)\n    print(f'AUC     : {auc}')\n\n    precision, recall, f1_score, _ = precision_recall_fscore_support(labels, preds, average = 'binary')\n    print(f'Precision: {precision}')\n    print(f'Recall: {recall}')\n    print(f'F1_score: {f1_score}')\n\n    confusion_matrix = pd.crosstab(index=labels, columns=np.round(preds), \n                                   rownames=['True'], colnames=['predictions']).astype(int)\n    plt.figure(figsize = (5,5))\n    sns.heatmap(confusion_matrix, annot=True, fmt='.2f', cmap=\"YlGnBu\").set_title('Confusion Matrix') \n    \n    return {'Accuracy':accuracy, 'Auc':auc, 'Precicion':precision, 'Recall':recall, 'F1_Score':f1_score}","d1b0b37e":"xtrain = train.drop(columns=['Class'], axis=1)\nytrain = train.Class\n\nxtest = test.drop(columns=['Class'], axis=1)\nytest = test.Class","0cf34f12":"logReg_base = LogisticRegression()\nlogReg_base.fit(xtrain, ytrain)\n\npredictions_base = logReg_base.predict(xtest)\nresult_base = evaluate_model(ytest, predictions_base)","39a8995c":"sns.countplot(x=data['Class'])\nplt.title('Original Distribution of the dataset')","71a24817":"from imblearn.under_sampling import RandomUnderSampler\n\n'''\nxtrain: training features\nytrain: training labels\n\nxtest: testinf features\nytest: testing labels\n'''\n\nrus = RandomUnderSampler(random_state=27)\n\n# Undersample the training data\nX_rus, Y_rus = rus.fit_resample(xtrain, ytrain) \nrus_train = pd.DataFrame(X_rus)\nrus_train['Class'] = Y_rus\n\n# Undersample the testing data\nx_rus, y_rus = rus.fit_resample(xtest, ytest) \nrus_test = pd.DataFrame(x_rus)\nrus_test['Class'] = y_rus\n\n# Shuffle the data\nrus_train = rus_train.sample(frac=1, random_state=345).reset_index(drop=True)\nrus_test = rus_test.sample(frac=1, random_state=345).reset_index(drop=True)\n\nrus_train.head()","38eac2d3":"print(f'Original size: {train.shape[0]}')\nprint(f'New size: {rus_train.shape[0]}')\nprint(f'Removed: {rus_train.shape[0]-data.shape[0]}')","36ad250b":"sns.countplot(x=rus_train['Class'])","046936b6":"plt.figure(figsize = (27,17))\nrus_correlation_matrix = rus_train.corr()\nsns.heatmap(rus_correlation_matrix, annot=True, fmt=\".2f\")","a9fd041d":"xtrain_rus = rus_train.drop(columns=['V16', 'V18', 'Class'], axis=1)\nytrain_rus = rus_train['Class']","8f7abb99":"logReg_rus = LogisticRegression()\nlogReg_rus.fit(xtrain_rus, ytrain_rus)","4e65995e":"# For the testing data\nxtest_rus = rus_test.drop(columns=['V16', 'V18', 'Class'], axis=1)\nytest_rus = rus_test['Class']\n\npredictions_rus = logReg_rus.predict(xtest_rus)\nresult_rus = evaluate_model(ytest_rus, predictions_rus)","abc715a9":"from imblearn.over_sampling import SMOTE\n\n'''\nxtrain: training features\nytrain: training labels\n\nxtest: testinf features\nytest: testing labels\n'''\n\nsmote = SMOTE(random_state=27)\n\n# Undersample the training data\nX_smote, Y_smote = smote.fit_resample(xtrain, ytrain)\nsmote_train = pd.DataFrame(X_smote)\nsmote_train['Class'] = Y_smote\n\n# Undersample the testing data\nx_smote, y_smote = smote.fit_resample(xtest, ytest) \nsmote_test = pd.DataFrame(x_smote)\nsmote_test['Class'] = y_smote\n\n# Shuffle the data\nsmote_train = smote_train.sample(frac=1, random_state=345).reset_index(drop=True)\nsmote_test = smote_test.sample(frac=1, random_state=345).reset_index(drop=True)\n\nsmote_train.head()","a7891a21":"print(f'Original size: {train.shape[0]}')\nprint(f'New size: {smote_train.shape[0]}')\nprint(f'Added: {smote_train.shape[0]-train.shape[0]}')","51c0b3a7":"sns.countplot(x=smote_train['Class'])","b024fcbe":"plt.figure(figsize = (27,17))\nsmote_correlation_matrix = smote_train.corr()\nsns.heatmap(smote_correlation_matrix, annot=True, fmt=\".2f\")","f06e2449":"xtrain_smote = smote_train.drop(columns=['V16', 'V18', 'Class'], axis=1)\nytrain_smote = smote_train['Class']","be7e248a":"logReg_smote = LogisticRegression()\nlogReg_smote.fit(xtrain_smote, ytrain_smote)","0b49cf87":"# For the testing data\nxtest_smote = smote_test.drop(columns=['V16', 'V18', 'Class'], axis=1)\nytest_smote = smote_test['Class']\n\npredictions_smote = logReg_smote.predict(xtest_smote)\nresult_smote = evaluate_model(ytest_smote, predictions_smote)","ed8f2fe9":"# Apply logReg_rus to xtest_smote\npredictions_rus = logReg_rus.predict(xtest_smote)\nresult_ruc_smote = evaluate_model(ytest_smote, predictions_rus)","626b77b8":"all_results = pd.DataFrame({'BaseLine':result_base, 'RUC':result_rus,\n                            'SMOTE':result_smote, 'RUC-SMOTE':result_ruc_smote})\n\nall_results","2e77cd4f":"* V18, V17 ---> highly correlated  0.94 ---> \u2234 remove one of them.\n* V18, V16 ---> highly correlated  0.91 ---> \u2234 remove one of them.\n* V17, V16 ---> highly correlated  0.95 ---> \u2234 remove one of them.\n* V16, V12 ---> highly correlated  0.91 ---> \u2234 remove one of them.\n\n\u2234 Remove V16, V18","ffb0104b":"## Data Splitting","b37db972":"## Oversampling\n* Try using **SMOTE** to increase the number of positive examples.","b20d27a2":"* We see that the data is highly imbalanced. Most of the transactions are non-fraud. This may result in a baised model.\n* As 99.8% of the data is non-fraud, this may result in bias when splitting the data, i.e. the testing data will not have any fraud samples. ","93b6928e":"* As we saw before, 99.8% of the data is non-fraud, which may result in a pure non-fraud testing data, so will follow the followung steps:\n    - Separate the fraud observation from the original datset.\n    - Split the fraud observation into training and testing datasets.\n    - Split the non-fraud observations into training and testing datasets.\n    - Combine fraud and non0fraud for each subsets.\n    - Shuffel them.","9db77a60":"* In the RUC-SMOTE case, we see that all the metrics have decreased than it was in the case of pure RUC, which means that this model cannot generalize well to the new data because the model was train on a very small data due to undersampling in a highly imbalanced dataset.\n<br><br>\n* The SMOTE case, we see that the model can generalize well to the new data as it was trained on a larger data due to oversampling. And by performing some optimazation or trying a nother model we can get a better result.","4280b689":"## Feature Scaling","39b70120":"### Build a function to print evaluation metrics","6ca408a1":"**NOTE**: Sampling will be performed on training and testing data separately to avoid data leakage.","8c769dc8":"* Let's test the 'logReg_rus' model with the same oversampled testing data to judge the two models in a better way.","433595b7":"## Undersampling","8be7ac65":"# Model Training and Evaluation","eec4572c":"# Feature Engineering","5ddef9fd":"## Data Visualization","bf4cf343":"**There are two main ways to handle imbalanced datasets:**\n\n* Undersample to reduce the negative samples\n    - Random undersampling.\n    - Generate centroids using clustering methods.\n<br><br>    \n* Oversample to add more positive samples:\n    - Random oversampling.\n    - Synthetic minority oversampling technique (SMOTE).","dd790f19":"## Baseline Model","de0faf4b":"### Features\nThe dataset contains over 30 numerical features, most of which have undergone principal component analysis (PCA) transformations because of personal privacy issues with the data. The only features that have not been transformed with PCA are 'Time' and 'Amount'. \n<br><br>\nFeatures: **V1, V2, ... V28:** Principal components obtained with PCA.\n#### Non-PCA features:\n* **Time:** Seconds elapsed between each transaction and the first transaction in the dataset,  Tx\u2212t0 \n* **Amount:** Transaction amount; this feature can be used for example-dependent cost-sensitive learning\n* **Class:** Target variable where Fraud = 1 and Not Fraud = 0","cd0cae75":"# Data preprocessing and visualization","5c02be55":"* All the features are independant.","a95dcc4d":"* After resampling, check if there is correlation between variables.","12ed76dc":"* These statistics include both fraud and non-fraud transactions, so we will look at the fraud statistics only with respect to 'Amount' of transaction.","0de165e2":"* As in the Undersampling case, we eill drop V16 and V18."}}