{"cell_type":{"fe08882c":"code","f69005b4":"code","47f11e0e":"code","181124ab":"code","6af9124f":"code","b7beb746":"code","71e13926":"code","6ba1d654":"code","34863bb0":"code","ed8b1756":"code","dc79274a":"code","15b5d5b4":"code","458b6021":"code","04e19791":"code","a07eca0c":"code","36d0a386":"code","5b6bc7be":"code","014dbb59":"code","02874ad6":"code","b25a7137":"code","5116482e":"code","c82ee0a6":"code","27140734":"code","2472c225":"code","09f1ea0e":"code","d12e85c4":"code","e60fad7a":"code","42f669f9":"code","9ab9b4ea":"code","3439c680":"code","3d9fb346":"code","abfbea10":"code","89b3e5fb":"code","b61d1e44":"markdown","68cd2162":"markdown","a7ed97ea":"markdown","9bff4603":"markdown","387e76ff":"markdown","05c8bc3d":"markdown","08c20e20":"markdown","02e1223e":"markdown","5e0eaa61":"markdown","5a3b8e61":"markdown","63df2e34":"markdown"},"source":{"fe08882c":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nimport random\n\nfrom matplotlib.lines import Line2D\n\nfrom lightgbm import LGBMRegressor\nfrom catboost import CatBoostRegressor\nfrom xgboost import XGBRegressor\n\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import StratifiedKFold, StratifiedShuffleSplit\nfrom sklearn.preprocessing import MinMaxScaler, StandardScaler\nfrom sklearn.ensemble import IsolationForest\n\nimport optuna\n\n# Pandas setting to display more dataset rows and columns\npd.set_option('display.max_rows', 100)\npd.set_option('display.max_columns', 500)\npd.set_option('display.max_colwidth', None)\npd.set_option('display.float_format', lambda x: '%.5f' % x)\n\n# import warnings\n# warnings.simplefilter(action='ignore', category=UserWarning)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('..\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n        \n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","f69005b4":"# Read the data\ntrain = pd.read_csv('..\/input\/30-days-of-ml\/train.csv', index_col='id')\ntest = pd.read_csv('..\/input\/30-days-of-ml\/test.csv', index_col='id')\nsample = pd.read_csv('..\/input\/30-days-of-ml\/sample_submission.csv', index_col='id')\n","47f11e0e":"train.head()","181124ab":"test.head()","6af9124f":"# Colors to be used for plots\n\ncolors = [\"lightcoral\", \"sandybrown\", \"darkorange\", \"mediumseagreen\",\n          \"lightseagreen\", \"cornflowerblue\", \"mediumpurple\", \"palevioletred\",\n          \"lightskyblue\", \"sandybrown\", \"yellowgreen\", \"indianred\",\n          \"lightsteelblue\", \"mediumorchid\", \"deepskyblue\"]","b7beb746":"fig, ax = plt.subplots(figsize=(5, 5))\npie = ax.pie([len(train), len(test)],\n             labels=[\"Train dataset\", \"Test dataset\"],\n             colors=[\"salmon\", \"teal\"],\n             textprops={\"fontsize\": 15},\n             autopct='%1.1f%%')\nax.axis(\"equal\")\nax.set_title(\"Dataset length comparison\", fontsize=18)\nfig.set_facecolor('white')\nplt.show();","71e13926":"train.describe().T","6ba1d654":"test.describe().T","34863bb0":"print(\"(train, test) na --> \",(train.isna().sum().sum(), test.isna().sum().sum()))","ed8b1756":"# Check if multiple values in target\n(train[\"target\"].value_counts()==2).value_counts()","dc79274a":"# fig, ax = plt.subplots(figsize=(16, 8))\n\n# bars = ax.bar(train[\"target\"].value_counts().sort_index().index,\n#               train[\"target\"].value_counts().sort_index().values,\n#               color=colors,\n#               edgecolor=\"black\")\n# ax.set_title(\"Target distribution\", fontsize=20, pad=15)\n# ax.set_ylabel(\"Amount of values\", fontsize=14, labelpad=15)\n# ax.set_xlabel(\"Target value\", fontsize=14, labelpad=10)\n# ax.bar_label(bars, [f\"{x:2.2f}%\" for x in train[\"target\"].value_counts().sort_index().values\/(len(train)\/100)],\n#                  padding=5, fontsize=10, rotation=90)\n# ax.margins(0.025, 0.12)\n# ax.grid(axis=\"y\")\n\n# plt.show();","15b5d5b4":"hist1 = plt.hist(train[\"target\"].values,\n                                   range=(train[\"target\"].min(),\n                                          train[\"target\"].max()),\n                                   bins=30,\n                                   color=\"deepskyblue\",\n                                   edgecolor=\"black\",\n                                   alpha=0.7,\n                                   label=\"Target\")\nplt.show();","458b6021":"test.columns","04e19791":"num_attribs = ['cont1', 'cont2', 'cont3', 'cont4', 'cont5', 'cont6',\n       'cont7', 'cont8', 'cont9', 'cont10', 'cont11', 'cont12', 'cont13']\ncat_attribs = ['cat0', 'cat1', 'cat2', 'cat3', 'cat4', 'cat5', 'cat6', 'cat7', 'cat8',\n       'cat9']","a07eca0c":"df = pd.concat([train.drop(\"target\", axis=1), \n                test], axis=0)\ncolumns = num_attribs\n\ncols = 3\nrows = len(columns) \/\/ cols + 1\n\nfig, axs = plt.subplots(ncols=cols, nrows=rows, figsize=(16,rows*3), sharex=False)\n\nplt.subplots_adjust(hspace = 0.3)\ni=0\n\nfor r in np.arange(0, rows, 1):\n    for c in np.arange(0, cols, 1):\n        if i >= len(columns):\n            axs[r, c].set_visible(False)\n        else:\n            hist1 = axs[r, c].hist(train[columns[i]].values,\n                                   range=(df[columns[i]].min(),\n                                          df[columns[i]].max()),\n                                   bins=40,\n                                   color=\"deepskyblue\",\n                                   edgecolor=\"black\",\n                                   alpha=0.7,\n                                   label=\"Train Dataset\")\n            hist2 = axs[r, c].hist(test[columns[i]].values,\n                                   range=(df[columns[i]].min(),\n                                          df[columns[i]].max()),\n                                   bins=40,\n                                   color=\"palevioletred\",\n                                   edgecolor=\"black\",\n                                   alpha=0.7,\n                                   label=\"Test Dataset\")\n            axs[r, c].set_title(columns[i], fontsize=14, pad=5)\n            axs[r, c].tick_params(axis=\"y\", labelsize=13)\n            axs[r, c].tick_params(axis=\"x\", labelsize=13)\n            axs[r, c].grid(axis=\"y\")\n            axs[r, c].legend(fontsize=13)\n                                  \n        i+=1\n#plt.suptitle(\"Feature values distribution in both datasets\", y=0.99)\nplt.show();","36d0a386":"train.nunique().sort_values().head(14)","5b6bc7be":"# Plot dataframe\ndf = train.corr().round(5)\n\n# Mask to hide upper-right part of plot as it is a duplicate\nmask = np.zeros_like(df)\nmask[np.triu_indices_from(mask)] = True\n\n# Making a plot\nplt.figure(figsize=(16,16))\nax = sns.heatmap(df, annot=False, mask=mask, cmap=\"RdBu\", annot_kws={\"weight\": \"bold\", \"fontsize\":13})\nax.set_title(\"Feature correlation heatmap\", fontsize=17)\nplt.setp(ax.get_xticklabels(), rotation=90, ha=\"right\",\n         rotation_mode=\"anchor\", weight=\"normal\")\nplt.setp(ax.get_yticklabels(), weight=\"normal\",\n         rotation_mode=\"anchor\", rotation=0, ha=\"right\")\nplt.show();","014dbb59":"\ncolumns = train.drop(\"target\", axis=1).columns.values\n\ncols = 4\nrows = len(columns) \/\/ cols + 1\n\nfig, axs = plt.subplots(ncols=cols, nrows=rows, figsize=(16,rows*3), sharex=False)\n\nplt.subplots_adjust(hspace = 0.3)\ni=0\n\nfor r in np.arange(0, rows, 1):\n    for c in np.arange(0, cols, 1):\n        if i >= len(columns):\n            axs[r, c].set_visible(False)\n        else:\n            scatter = axs[r, c].scatter(train[columns[i]].values,\n                                        train[\"target\"],\n                                        alpha=0.1,\n                                        color=random.choice(colors))\n            axs[r, c].set_title(columns[i], fontsize=14, pad=5)\n            axs[r, c].tick_params(axis=\"y\", labelsize=11)\n            axs[r, c].tick_params(axis=\"x\", labelsize=11)\n                                  \n        i+=1\n#plt.suptitle(\"Features vs loss\", y=0.99)\nplt.show();","02874ad6":"from sklearn.model_selection import train_test_split","b25a7137":"y = train[\"target\"]\nX = train.drop(\"target\", axis = 1)\nX_test = test.copy()","5116482e":"X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.25, random_state=42)","c82ee0a6":"# managing Categoriacal\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.preprocessing import StandardScaler\n","27140734":"full_pipeline = ColumnTransformer([\n    (\"num\", StandardScaler(), num_attribs),\n    (\"cat\", OneHotEncoder(), cat_attribs)\n])","2472c225":"X_train_prepared = full_pipeline.fit_transform(X_train)\nX_valid_prepared = full_pipeline.transform(X_valid)\nX_test_prepared = full_pipeline.transform(X_test)\n","09f1ea0e":"X_train_prepared_df = pd.DataFrame(X_train_prepared, \n                                    index = X_train.index\n                                    )\nX_valid_prepared_df = pd.DataFrame(X_valid_prepared, \n                                    index = X_valid.index\n                                    )\nX_test_prepared_df = pd.DataFrame(X_test_prepared, \n                                    index = X_test.index\n                                    )","d12e85c4":"X_train_prepared_df.head()","e60fad7a":"from xgboost import XGBRegressor\nfrom sklearn.model_selection import StratifiedKFold","42f669f9":"xgbr_model = XGBRegressor()\n","9ab9b4ea":"xgbr_model.fit(X=X_train_prepared_df, y=y_train)","3439c680":"mean_squared_error(y_valid, xgbr_model.predict(X_valid_prepared_df))","3d9fb346":"preds =   xgbr_model.predict(X_test_prepared_df)","abfbea10":"predictions = pd.DataFrame()\npredictions[\"id\"] = test.index\npredictions[\"target\"] = preds\n\npredictions.to_csv('submission.csv', index=False, header=predictions.columns)\npredictions.head()","89b3e5fb":"sample.head()","b61d1e44":"# Data preparation","68cd2162":"# Predictions","a7ed97ea":"# EDA\nEDA base on Notebook https:\/\/www.kaggle.com\/maximkazantsev\/tps-08-21-xgboost\nThanks to @maximkazantsev","9bff4603":"# Distribuition","387e76ff":"### Checking if any there are some na values in train and test","05c8bc3d":"# Evalutating the model","08c20e20":"## EDA on continuous values","02e1223e":"# Checking for correlations","5e0eaa61":"# Fitting the model","5a3b8e61":"## Checking for Categorical features","63df2e34":"# Train-test split"}}