{"cell_type":{"13fccca6":"code","956f997f":"code","4ff49a5b":"code","8e57d89c":"code","bac9bdfc":"code","f7639bfc":"code","07682284":"code","3d0dcbe4":"code","07951963":"code","b78987c7":"code","6603f31a":"code","390987b4":"code","93555399":"code","87020058":"code","2ec66921":"code","712c64dd":"code","0ec7915e":"code","24c2e8b8":"code","0426db5e":"code","fcc2abcc":"code","a867ba05":"code","1fd07690":"code","fa02cb11":"code","f25ec881":"code","aae663ea":"code","f9bb058a":"code","f6511564":"code","497ca6cd":"code","5cb9d7c0":"code","a4c40f27":"code","482942e4":"code","9632b82e":"code","511f1f50":"code","ed81f989":"code","bd27901a":"code","fbc41999":"markdown","b8ab899d":"markdown","abedf82f":"markdown","f0622c0c":"markdown","d32db840":"markdown","770fcd5c":"markdown","1511885f":"markdown","4fabd636":"markdown","a627a6a4":"markdown","b69b540d":"markdown"},"source":{"13fccca6":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport seaborn as sns\nimport matplotlib.pyplot as plt \n%matplotlib inline\n\n#machine learning models to implement \nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.linear_model import Perceptron\nfrom sklearn.linear_model import SGDClassifier\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","956f997f":"traindf =  pd.read_csv('..\/input\/titanic\/train.csv')\ntraindf","4ff49a5b":"testdf = pd.read_csv('..\/input\/titanic\/test.csv')\ntestdf","8e57d89c":"combine = [traindf,testdf]","bac9bdfc":"traindf.describe","f7639bfc":"sns.countplot('Survived', data = traindf)","07682284":"plt.figure(figsize=(10,8))\nsns.heatmap(traindf.corr(), cmap='coolwarm',annot=True)","3d0dcbe4":"sns.countplot('Sex', hue = 'Survived', data = traindf)","07951963":"sns.countplot('Pclass', hue='Survived', data = traindf)","b78987c7":"sns.countplot('Survived', hue='Embarked', data =traindf)","6603f31a":"# grid = sns.FacetGrid(train_df, col='Embarked')\ngrid = sns.FacetGrid(traindf, row='Embarked', size=2.2, aspect=1.6)\ngrid.map(sns.pointplot, 'Pclass', 'Survived', 'Sex', palette='deep')\ngrid.add_legend()","390987b4":"#dropping features \nprint('Before', traindf.shape, testdf.shape, combine[0].shape, combine[1].shape)\n\ntraindf.drop(['Ticket','Cabin'], axis=1, inplace=True)\ntestdf.drop(['Ticket','Cabin'], axis=1 , inplace=True)\ncombine =[traindf,testdf]\n\nprint('After', traindf.shape, testdf.shape, combine[0].shape, combine[1].shape )","93555399":"#CREATING A NEW FEATURE FROM EXISITING \nfor dataset in combine : \n    dataset['Title'] = dataset.Name.str.extract('([A-Za-z]+)\\.', expand=False)\npd.crosstab(traindf['Title'],traindf['Sex'])","87020058":"for dataset in combine:\n    dataset['Title'] = dataset['Title'].replace(['Lady', 'Countess','Capt', 'Col',\\\n \t'Don', 'Dr', 'Major', 'Rev', 'Sir', 'Jonkheer', 'Dona'], 'Rare')\n    dataset['Title'] = dataset['Title'].replace('Mlle', 'Miss')\n    dataset['Title'] = dataset['Title'].replace('Mme', 'Mrs')\n    dataset['Title'] = dataset['Title'].replace('Ms', 'Miss')\n    \ntraindf[['Title','Survived']].groupby(['Title'], as_index=False).mean()","2ec66921":"title_mapping = {\"Mr\": 1, \"Miss\": 2, \"Mrs\": 3, \"Master\": 4, \"Rare\": 5}\nfor dataset in combine:\n    dataset['Title'] = dataset['Title'].map(title_mapping)\n    dataset['Title'] = dataset['Title'].fillna(0)\n    \ntraindf.head()","712c64dd":"traindf = traindf.drop(['Name', 'PassengerId'], axis=1)\ntestdf = testdf.drop(['Name'], axis=1)\ncombine = [traindf, testdf]\ntraindf.shape, testdf.shape","0ec7915e":"#encoding categorical data \nfrom sklearn.preprocessing import LabelEncoder\nle = LabelEncoder()\nembark_mapping =  {\"Q\":1, \"S\":2, \"C\":3}\nfor dataset in combine:\n    dataset['Sex'] = le.fit_transform(dataset['Sex'])\n    dataset['Embarked'] = dataset['Embarked'].map(embark_mapping)\n    dataset['Embarked'] = dataset['Embarked'].fillna(0)\n    \ntraindf.head()","24c2e8b8":"from sklearn.impute import SimpleImputer\nimputer = SimpleImputer(missing_values=np.nan, strategy='median')\ntraindf[['Age']] = imputer.fit_transform(traindf[['Age']])\ntestdf[['Age']] = imputer.fit_transform(testdf[['Age']])","0426db5e":"for dataset in combine:\n    dataset.columns[dataset.isnull().any()]\n    nanfare =  pd.isnull(dataset[\"Fare\"])\ndataset[nanfare]","fcc2abcc":"for dataset in combine:\n    dataset[['Fare']] = imputer.fit_transform(dataset[['Fare']])","a867ba05":"dataset.columns[dataset.isnull().any()]","1fd07690":"traindf.head()","fa02cb11":"testdf.head()","f25ec881":"X_train = traindf.drop(\"Survived\", axis=1)\ny_train = traindf[\"Survived\"]\nX_test = testdf.drop(\"PassengerId\", axis=1).copy()\nX_train.shape, y_train.shape, X_test.shape","aae663ea":"# Feature Scaling\nfrom sklearn.preprocessing import StandardScaler\nsc = StandardScaler()\nX_train = sc.fit_transform(X_train)\nX_test = sc.transform(X_test)","f9bb058a":"#LOGISTIC REGRESSION \nlr = LogisticRegression()\nlr.fit(X_train, y_train)\ny_pred =  lr.predict(X_test)\nacc_lr = round(lr.score(X_train,y_train)*100, 2)\nacc_lr","f6511564":"coeff_df = pd.DataFrame(traindf.columns.delete(0))\ncoeff_df.columns = ['Feature']\ncoeff_df[\"Correlation\"] = pd.Series(lr.coef_[0])\ncoeff_df.sort_values(by='Correlation', ascending=False)","497ca6cd":"#GAUSSIAN NAIVE BAYES \ngauss = GaussianNB()\ngauss.fit(X_train, y_train)\ny_pred =  gauss.predict(X_test)\nacc_gauss = round(gauss.score(X_train,y_train)*100, 2)\nacc_gauss","5cb9d7c0":"#PERCEPTRON\nper = Perceptron()\nper.fit(X_train,y_train)\ny_pred = per.predict(X_test)\nacc_per = round(per.score(X_train,y_train)*100, 2)\nacc_per","a4c40f27":"#STOCHASTIC GRADIENT DESCENT\nsgd = SGDClassifier()\nsgd.fit(X_train,y_train)\ny_pred = sgd.predict(X_test)\nacc_sgd = round(sgd.score(X_train,y_train)*100, 2)\nacc_sgd","482942e4":"#K-Neighbors\nknn = KNeighborsClassifier(n_neighbors = 3)\nknn.fit(X_train, y_train)\ny_pred = knn.predict(X_test)\nacc_knn = round(knn.score(X_train, y_train) * 100, 2)\nacc_knn","9632b82e":"#SUPPORT VECTOR MACHINES \nsvc = SVC()\nsvc.fit(X_train, y_train)\ny_pred = svc.predict(X_test)\nacc_svc = round(svc.score(X_train, y_train) * 100, 2)\nacc_svc","511f1f50":"#RANDOM FOREST\nran =  RandomForestClassifier(n_estimators = 100)\nran.fit(X_train,y_train)\ny_pred = ran.predict(X_test)\nacc_ran = round(ran.score(X_train,y_train)*100, 2)\nacc_ran","ed81f989":"models =  pd.DataFrame({\n    'Model': ['Logistic Regression', 'KNN', 'GaussianNB',\n              'Perceptron', 'Random Forest', 'K-Neighbors', 'SGD', 'Support Vector Machines'],\n    'Score': [acc_lr, acc_knn, acc_gauss,\n             acc_per, acc_ran, acc_knn, acc_sgd, acc_svc]})\nmodels.sort_values(by='Score', ascending=False)","bd27901a":"submission_df = pd.DataFrame\nsubmission_df = pd.DataFrame({\n    \"PassengerId\" : testdf[\"PassengerId\"],\n    \"Survived\" : y_pred\n})\n\nsubmission_df.PassengerId = submission_df.PassengerId.astype(int)\nsubmission_df.Survived = submission_df.Survived.astype(int)\nsubmission_df.to_csv('submission.csv', header=True, index=False)\nsubmission_df.head(10)","fbc41999":"*Convert categorical variables into ordinal*","b8ab899d":" **Feature engineering**","abedf82f":"*OBSERVATIONS::*\n* Sex is highest positivie coefficient, implying as the Sex value increases (male: 0 to female: 1), the probability of Survived=1 increases the most.\n* Inversely as Pclass increases, probability of Survived=1 decreases the most.\n* This way Age*Class is a good artificial feature to model as it has second highest negative correlation with Survived.\n* So is Title as second highest positive correlation.","f0622c0c":"**Data visualization**","d32db840":"**MODEL EVALUATION**","770fcd5c":"**Data preprocessing**","1511885f":"*Replacing titles with common name*","4fabd636":"**Handling Missing values**","a627a6a4":"**Importing libraries**","b69b540d":"**MODEL TRAINING**"}}