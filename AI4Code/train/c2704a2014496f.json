{"cell_type":{"c0ff1544":"code","1deff17e":"code","8ff4b437":"code","30cf2e49":"code","0a7d45d3":"code","c4e73308":"code","4159f6fb":"code","cd5e6aec":"code","5887b521":"markdown","ef92d48c":"markdown","ded9e91e":"markdown","87a39a42":"markdown","bf2cbf61":"markdown","115de83a":"markdown","bd22abb9":"markdown","fa88edd4":"markdown","5db00a01":"markdown","5db74a4c":"markdown","f6c195c2":"markdown","5e73a623":"markdown","2ca3f341":"markdown"},"source":{"c0ff1544":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","1deff17e":"df = pd.read_csv('..\/input\/elon-musks-tweets\/data_elonmusk.csv', encoding='latin1')\ndf.head()","8ff4b437":"df1 = df.drop(['row ID', 'Retweet from', 'User'], axis = 1)\n","30cf2e49":"from nltk.corpus import stopwords\nen_stop_words = set(stopwords.words('english'))","0a7d45d3":"from sklearn.feature_extraction.text import CountVectorizer\nfrom gensim.corpora import Dictionary\nfrom gensim.models.ldamodel import LdaModel\nfrom gensim.models import CoherenceModel\nfrom nltk.corpus import stopwords\nfrom nltk.tokenize import RegexpTokenizer\nfrom datetime import datetime\nimport nltk\nnltk.download('stopwords')\nimport pandas as pd\nimport re\nimport math\n\ndef clean_tweets(df=df1, \n                 tweet_col='Tweet', \n                 date_col='Time',\n                 start_datetime=datetime(2017,1,20, 0, 0, 0)\n                ):\n    \n    df_copy = df.copy()\n    \n    # drop rows with empty values\n    df_copy.dropna(inplace=True)\n    \n    # format the date\n    df_copy[date_col] = df_copy[date_col].apply(lambda row: datetime.strptime(row, '%Y-%m-%d %H:%M:%S'))\n    \n    # filter rows older than a given date\n    df_copy = df_copy[df_copy[date_col] >=start_datetime]\n    \n    # lower the tweets\n    df_copy['preprocessed_' + tweet_col] = df_copy[tweet_col].str.lower()\n    \n    # filter out stop words and URLs\n    stop_words = set(stopwords.words('english'))\n    extended_stop_words = en_stop_words | \\\n                        {\n                            '&amp;', 'rt',                           \n                            'th','co', 're', 've', 'kim', 'daca'\n                        }\n    url_re = '(https?:\\\/\\\/(?:www\\.|(?!www))[a-zA-Z0-9][a-zA-Z0-9-]+[a-zA-Z0-9]\\.[^\\s]{2,}|www\\.[a-zA-Z0-9][a-zA-Z0-9-]+[a-zA-Z0-9]\\.[^\\s]{2,}|https?:\\\/\\\/(?:www\\.|(?!www))[a-zA-Z0-9]+\\.[^\\s]{2,}|www\\.[a-zA-Z0-9]+\\.[^\\s]{2,})'        \n    df_copy['preprocessed_' + tweet_col] = df_copy['preprocessed_' + tweet_col].apply(lambda row: ' '.join([word for word in row.split() if (not word in stop_words) and (not re.match(url_re, word))]))\n    \n    # tokenize the tweets\n    tokenizer = RegexpTokenizer('[a-zA-Z]\\w+\\'?\\w*')\n    df_copy['tokenized_' + tweet_col] = df_copy['preprocessed_' + tweet_col].apply(lambda row: tokenizer.tokenize(row))\n    \n    return df_copy\n  \n#df_tweets = pd.read_csv('trump_tweets.csv')\ndf_tweets_clean = clean_tweets(df1)\ndf_tweets_clean.head()","c4e73308":"def get_most_freq_words(str, n=None):\n    vect = CountVectorizer().fit(str)\n    bag_of_words = vect.transform(str)\n    sum_words = bag_of_words.sum(axis=0) \n    freq = [(word, sum_words[0, idx]) for word, idx in vect.vocabulary_.items()]\n    freq =sorted(freq, key = lambda x: x[1], reverse=True)\n    return freq[:n]\n  \nget_most_freq_words([ word for tweet in df_tweets_clean.tokenized_Tweet for word in tweet],10)","4159f6fb":"# build a dictionary where for each tweet, each word has its own id.\ntweets_dictionary = Dictionary(df_tweets_clean.tokenized_Tweet)\n\n# build the corpus i.e. vectors with the number of occurence of each word per tweet\ntweets_corpus = [tweets_dictionary.doc2bow(tweet) for tweet in df_tweets_clean.tokenized_Tweet]\n\n# compute coherence\ntweets_coherence = []\nfor nb_topics in range(1,36):\n    lda = LdaModel(tweets_corpus, num_topics = nb_topics, id2word = tweets_dictionary, passes=10)\n    cohm = CoherenceModel(model=lda, corpus=tweets_corpus, dictionary=tweets_dictionary, coherence='u_mass')\n    coh = cohm.get_coherence()\n    tweets_coherence.append(coh)\n\n# visualize coherence\nimport matplotlib.pyplot as plt\nplt.figure(figsize=(10,5))\nplt.plot(range(1,36),tweets_coherence)\nplt.xlabel(\"Number of Topics\")\nplt.ylabel(\"Coherence Score\");","cd5e6aec":"import matplotlib.gridspec as gridspec\nk = 5\ntweets_lda = LdaModel(tweets_corpus, num_topics = k, id2word = tweets_dictionary, passes=10)\n\ndef plot_top_words(lda=tweets_lda, nb_topics=k, nb_words=10):\n    top_words = [[word for word,_ in lda.show_topic(topic_id, topn=50)] for topic_id in range(lda.num_topics)]\n    top_betas = [[beta for _,beta in lda.show_topic(topic_id, topn=50)] for topic_id in range(lda.num_topics)]\n\n    gs  = gridspec.GridSpec(round(math.sqrt(k))+1,round(math.sqrt(k))+1)\n    gs.update(wspace=0.5, hspace=0.5)\n    plt.figure(figsize=(20,15))\n    for i in range(nb_topics):\n        ax = plt.subplot(gs[i])\n        plt.barh(range(nb_words), top_betas[i][:nb_words], align='center',color='blue', ecolor='black')\n        ax.invert_yaxis()\n        ax.set_yticks(range(nb_words))\n        ax.set_yticklabels(top_words[i][:nb_words])\n        plt.title(\"Topic \"+str(i))\n        \n  \nplot_top_words()\n","5887b521":"Next, let\u2019s perform a simple preprocessing on the content to make it more amenable for analysis, and reliable results","ef92d48c":"Next, let\u2019s work to transform the textual data in a format that will serve as an input for training LDA model. We start by converting the documents into a simple vector representation (Bag of Words BOW).","ded9e91e":"Since the goal of this analysis is to perform topic modeling we do not need all of the columns.","87a39a42":"# Introduction","bf2cbf61":"The above LDA model is built with 5 different topics where each topic is a combination of keywords and each keyword contributes a certain weightage to the topic.","115de83a":"# Topic Coherence","bd22abb9":"**If you find this notebook useful, please upvoat.**","fa88edd4":"Topic Coherence measures score a single topic by measuring the degree of semantic similarity between high scoring words in the topic. These measurements help distinguish between topics that are semantically interpretable topics and topics that are artifacts of statistical inference.\n\nBelow, I visualize the average coherence score per topic for a range of models trained with a different number of topic","5db00a01":"Sources:\n* Topic model \u2014 Wikipedia. https:\/\/en.wikipedia.org\/wiki\/Topic_model\n* https:\/\/towardsdatascience.com\/end-to-end-topic-modeling-in-python-latent-dirichlet-allocation-lda-35ce4ed6b3e0\n* https:\/\/towardsdatascience.com\/topic-modeling-with-latent-dirichlet-allocation-by-example-3b22cd10c835","5db74a4c":"# Visualize Topics","f6c195c2":"Topic Models, in a nutshell, are a type of statistical language models used for uncovering hidden structure in a collection of texts.\n\nThere are several algorithms which can be use to perform the topic modeling. The most common of it are, Latent Semantic Analysis (LSA\/LSI), Probabilistic Latent Semantic Analysis (pLSA), and Latent Dirichlet Allocation (LDA)\n\nIn this notebook, I\u2019ll take a closer look at LDA.\n\n**If you find this notebook useful, please upvoat.**","5e73a623":"# Bag of Words","2ca3f341":"# Data pre-processing"}}