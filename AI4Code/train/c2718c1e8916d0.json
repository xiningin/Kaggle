{"cell_type":{"0fc5acf0":"code","60d76c01":"code","59561b8c":"code","406bfc9d":"code","7a10c430":"code","a0d5068d":"code","b42f044d":"code","00658292":"code","1a662856":"code","a225260d":"code","a2fb54a4":"code","24b430d4":"code","18334ddf":"code","9c244984":"code","f12c75dc":"code","89e28aa3":"code","54dfe603":"code","718dad98":"code","63b498c2":"code","0e37a8cc":"code","0fd9b70c":"code","a9dd53f0":"code","306e0e64":"code","90e14721":"markdown","3d4159ef":"markdown","3c6edb36":"markdown","cf44277b":"markdown","0686ac21":"markdown","6d415edb":"markdown","f9109c06":"markdown","3e3b6173":"markdown","b6c5b405":"markdown","126aedc7":"markdown","19246870":"markdown","f2dc91bf":"markdown","6c7c6503":"markdown","709dd59d":"markdown","2fa3146a":"markdown","73b4e029":"markdown"},"source":{"0fc5acf0":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport plotly_express as px\nimport plotly.graph_objs as go\nimport plotly.plotly as py\n# from plotly.offline import init_notebook_mode, iplot\n# init_notebook_mode(connected=True)\n\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","60d76c01":"df=pd.read_csv('..\/input\/data.csv')","59561b8c":"df.columns","406bfc9d":"skill_cols = ['Crossing',\n       'Finishing', 'HeadingAccuracy', 'ShortPassing', 'Volleys', 'Dribbling',\n       'Curve', 'FKAccuracy', 'LongPassing', 'BallControl', 'Acceleration',\n       'SprintSpeed', 'Agility', 'Reactions', 'Balance', 'ShotPower',\n       'Jumping', 'Stamina', 'Strength', 'LongShots', 'Aggression',\n       'Interceptions', 'Positioning', 'Vision', 'Penalties', 'Composure',\n       'Marking', 'StandingTackle', 'SlidingTackle', 'GKDiving', 'GKHandling',\n       'GKKicking', 'GKPositioning', 'GKReflexes']\ndfskills = df[skill_cols]\nlen(dfskills.columns)","7a10c430":"import pandas_profiling\n# pandas_profiling.ProfileReport(dfskills)","a0d5068d":"dfskills.dropna(inplace=True)","b42f044d":"corr = dfskills.corr()\ntrace = go.Heatmap(z=corr,x=corr.index,y=corr.columns)\ndata = [trace]\nlayout = dict(title=\"Correlation Plot of Player Skills\")\nfig = dict(data=data, layout=layout)\n#iplot(fig)","00658292":"from sklearn.decomposition import PCA\npca = PCA().fit(dfskills)","1a662856":"pcaratio = pca.explained_variance_ratio_\ntrace = go.Scatter(x=np.arange(len(pcaratio)),y=np.cumsum(pcaratio))\ndata = [trace]\nlayout = dict(title=\"Player Skills Dataset - PCA Explained Variance || 89% achieved at 5 components\")\nfig = dict(data=data, layout=layout)\n#iplot(fig)","a225260d":"pca = PCA(n_components=5)\nskillsPCA = pca.fit_transform(dfskills)","a2fb54a4":"from sklearn.cluster import KMeans\nkmeans = KMeans(n_clusters=6)\nskillsPCA_labels = kmeans.fit_predict(skillsPCA)","24b430d4":"dfskillsPCA = pd.DataFrame(skillsPCA)\ndfskillsPCA['cluster'] = skillsPCA_labels","18334ddf":"from sklearn.manifold import TSNE\nX = dfskillsPCA.iloc[:,:-1]\nXtsne = TSNE(n_components=2).fit_transform(X)\ndftsne = pd.DataFrame(Xtsne)\ndftsne['cluster'] = skillsPCA_labels\ndftsne.columns = ['x1','x2','cluster']","9c244984":"pca2 = PCA(n_components=2)\nskillsPCA2 = pca2.fit_transform(dfskills)\ndfskillsPCA2 = pd.DataFrame(skillsPCA2)\ndfskillsPCA2['cluster'] = skillsPCA_labels\ndfskillsPCA2.columns = ['x1','x2','cluster']\n","f12c75dc":"fig, ax = plt.subplots(1, 2, figsize=(12,6))\nsns.scatterplot(data=dftsne,x='x1',y='x2',hue='cluster',legend=\"full\",alpha=0.5,ax=ax[0])\nax[0].set_title('Visualized on TSNE 2D')\nsns.scatterplot(data=dfskillsPCA2,x='x1',y='x2',hue='cluster',legend=\"full\",alpha=0.5,ax=ax[1])\nax[1].set_title('Visualized on PCA 2D')\nfig.suptitle('Comparing clustering result when visualized using TSNE2D vs. PCA2D')\ndisplay(fig)","89e28aa3":"kmeans = KMeans(n_clusters=6)\nclustering_ori = kmeans.fit_predict(dfskills)","54dfe603":"dftsne2D = dftsne\ndftsne2D['cluster'] = clustering_ori","718dad98":"X = dfskills\nXtsne = TSNE(n_components=2).fit_transform(X)\ndftsneFull = pd.DataFrame(Xtsne)","63b498c2":"dftsneFull['cluster'] = clustering_ori\ndftsneFull.columns = ['x1','x2','cluster']","0e37a8cc":"fig, ax = plt.subplots(1, 2, figsize=(12,6))\nsns.scatterplot(data=dftsne2D,x='x1',y='x2',hue='cluster',legend=\"full\",alpha=0.5,ax=ax[0])\nax[0].set_title('Visualized on TSNE 5D>2D')\nsns.scatterplot(data=dftsneFull,x='x1',y='x2',hue='cluster',legend=\"full\",alpha=0.7,ax=ax[1])\nax[1].set_title('Visualized on TSNE 34D>2D')\nfig.suptitle('Comparing clustering result when visualized using TSNE 5D>2D vs. TSNE 34D>2D')\ndisplay(fig)","0fd9b70c":"dfskills['cluster'] = clustering_ori","a9dd53f0":"# Some functions to plot just the variables that has significant deviation from global mean\ndef outside_limit(df, label_col, label, sensitivity):\n  feature_list = dfskills.columns[:-1]\n  \n  plot_list = []\n  mean_overall_list = []\n  mean_cluster_list = []\n  \n  for i,varname in enumerate(feature_list):\n    \n    #     get overall mean for a variable, set lower and upper limit\n    mean_overall = df[varname].mean()\n    lower_limit = mean_overall - (mean_overall*sensitivity)\n    upper_limit = mean_overall + (mean_overall*sensitivity)\n\n    #     get cluster mean for a variable\n    cluster_filter = df[label_col]==label\n    pd_cluster = df[cluster_filter]\n    mean_cluster = pd_cluster[varname].mean()\n    \n    #     create filter to display graph with 0.5 deviation from the mean\n    if mean_cluster <= lower_limit or mean_cluster >= upper_limit:\n      plot_list.append(varname)\n      mean_overall_std = mean_overall\/mean_overall\n      mean_cluster_std = mean_cluster\/mean_overall\n      mean_overall_list.append(mean_overall_std)\n      mean_cluster_list.append(mean_cluster_std)\n   \n  mean_df = pd.DataFrame({'feature_list':plot_list,\n                         'mean_overall_list':mean_overall_list,\n                         'mean_cluster_list':mean_cluster_list})\n  mean_df = mean_df.sort_values(by=['mean_cluster_list'], ascending=False)\n  \n  return mean_df\n\ndef plot_barchart_all_unique_features(df, label_col, label, ax, sensitivity):\n  \n  mean_df = outside_limit(df, label_col, label, sensitivity)\n  mean_df_to_plot = mean_df.drop(['mean_overall_list'], axis=1)\n  \n  if len(mean_df.index) != 0:\n    sns.barplot(y='feature_list', x='mean_cluster_list', data=mean_df_to_plot, palette=sns.cubehelix_palette(20, start=.5, rot=-.75, reverse=True), \\\n                alpha=0.75, dodge=True, ax=ax)\n\n    for i,p in enumerate(ax.patches):\n      ax.annotate(\"{:.02f}\".format((p.get_width())), \n                  (1, p.get_y() + p.get_height() \/ 2.), xycoords=('axes fraction', 'data'),\n                  ha='right', va='top', fontsize=10, color='black', rotation=0, \n                  xytext=(0, 0),\n                  textcoords='offset pixels')\n  \n  ax.set_title('Unique Characteristics of Cluster ' + str(label))\n  ax.set_xlabel('Standardized Mean')\n  ax.axvline(x=1, color='k')\n\ndef plot_features_all_cluster(df, label_col, n_clusters, sensitivity):\n  n_plot = n_clusters\n  fig, ax = plt.subplots(n_plot, 1, figsize=(12, n_plot*6), sharex='col')\n  ax= ax.ravel()\n  \n  label = np.arange(n_clusters)\n  for i in label:\n    plot_barchart_all_unique_features(df, label_col, label=i, ax=ax[i], sensitivity=sensitivity)\n    ax[i].xaxis.set_tick_params(labelbottom=True)\n    \n  plt.tight_layout()\n  display(fig)","306e0e64":"plot_features_all_cluster(df=dfskills, label_col='cluster', n_clusters=6, sensitivity=0.2)","90e14721":"## We also manually create correlation heatmap manually using Plotly\nPlotly enables you to hover and examine each variable","3d4159ef":"# Clustering 2A: Visualize clustering-2 using TSNE (5D > 2D)**","3c6edb36":"# Clustering 1B: Visualize clustering-1 using PCA 2D for comparison\nWhat if we use PCA instead of TSNE? PCA has a much faster compute, but will we lose a lot in terms of visualization and insight? \nIn our case, it's not too much different. But in other cases, you could find that there are a lot of manifolds of the data that could be seen better with TSNE, which got \"squeezed\" when just using PCA","cf44277b":"### Now we plot unique characteristics of each cluster","0686ac21":"# Comparing Clustering 1A and 1B","6d415edb":"## Data Preparation & Clean Up","f9109c06":"# Clustering 1: Create clusters based on PCA (5D) transformed\nAfter we have the 5-D, we create clusters using just those 5 components. Here we pick an arbitrary number of 6 clusters for illustration, but you should review the elbow curve to pick more optimal clustering.","3e3b6173":"# Profiling each of the cluster\n\nFor each cluster, we want to show the average stats of the cluster, and highlight the distinguishing characteristics. We don't want to show the entire set of attributes for each cluster, but we just want to highlight metrics that are \"unique\" (i.e. significantly different from global mean)","b6c5b405":"#### We select only skills attributes for clustering\nIn total there are 34 columns. There are 5 skills that are specific to Goal Keepers, but the value is still nonzero for non-GK players. In this iteration, we will keep the Goal Keepers skills to observe how it will behave in the clustering","126aedc7":"## Context\n\nWe have a rich dataset containing large number of player attributes. To help managers better understand player characteristics, we can generate archetype of player profiles. We can achieve this by applying unsupervised learning techniques. In this kernel, I would like to go through different ways to do the clustering and also communicate the results\n\nA couple different techniques & libraries being used in this kernel:\n* **Comparing PCA & TSNE** for dimensionality reduction. In particular, we use PCA for clustering, and TSNE primarily for visualization. We also compare 2D viz from PCA and TSNE\n* **Pandas Profiling** library for EDA\n* **Combination of Plotly, Plotly Express, and Seaborn** Plotly \/ Plotly Express gives interactivity, as we can hover on the chart. However, Plotly \/ Express don't work well for scatter plot because it stores each data point, thus the size of the notebook gets too large, so we revert to regular Seaborn\n* **Cluster Explainer** Lastly we use selective-variable charts to profile unique characteristic of clusters","19246870":"# Clustering 2: Create clusters based on full original dimensions (34 vars)","f2dc91bf":"# Clustering 1A: Visualize clustering-1 using TSNE (5D > 2D)\nThe TSNE here is created from the 5D PCA components. In contrast, later on we will create the TSNE that is based on the original 34 variables. We will see later that the visualized 2D scatter doesn't look to different. However, the computational time required for TSNE from original vector space is much longer","6c7c6503":"# Clustering 2B: Visualize clustering-2 using TSNE (34D > 2D)**","709dd59d":"# PCA to reduce dimensionality\nIn this step we are using PCA to reduce the dimensionality of original attribute space. We use PCA Explained Variance plot below to help determine the right level of components. We chose 5 components PCA as it can achieve 89% explained variance with just few components. As you can see later in the notebook, the result of kmeans clusterint from using 5 components looks very similar with clustering based on the full variables, except for a small number of observations that are sprinkled away from the big clusters","2fa3146a":"# Comparing Clustering 2A and Clustering 2B","73b4e029":"# Pandas Profiling for EDA and data cleaning\nPandas Profiling is a very convenient library to do quick overview EDA. It is useful for us to understand the landscape of each variables in the dataset. In addition, Pandas Profiling provides diagnostics and recommendation on missing values and correlated variables.\n\nIn the Pandas Profiling output below, we will observe that some variables are correlated and suggested to be removed. Nonetheless, we will keep it in this iteration. From the Profiling, there are only 0.2% of players having missing value, so we will just drop them out of the dataset.\n\nGo check out more information about Pandas Profiling in here: https:\/\/github.com\/pandas-profiling\/pandas-profiling\n\n*Note: I had to comment out Pandas Profiling when committing the Kernel, because it seems that the html file gets too large for the commit 1MB limit. I got this message when running it: \"An error occurred while commiting kernel: The kernel source must be less than 1 megabyte in size\"*"}}