{"cell_type":{"b7fb6bd4":"code","d265a5b8":"code","e4765012":"code","f15065fd":"code","ea94be09":"code","ace0bf08":"code","93508746":"code","cb0571e7":"code","7af14d4b":"code","31a9d0cc":"code","8c3fa805":"code","761e64f3":"code","7c64ddef":"code","5b8dec16":"code","43fa63ef":"code","beecde2e":"code","069c6b6c":"code","68fcc586":"code","7614edcf":"code","67b85c93":"code","39007350":"code","de33024c":"code","ae0abf12":"code","590575d5":"code","8e2b8cb4":"code","13303137":"code","0c252551":"code","28960538":"code","295354d4":"code","42b74695":"markdown","e94f01e0":"markdown","8ff2c076":"markdown","fecc6025":"markdown","050670ef":"markdown","f9fe95c4":"markdown","574ebcfa":"markdown","5114edac":"markdown","a1143746":"markdown","6349c91a":"markdown","1aebed36":"markdown","be9e97c8":"markdown","6d26c16a":"markdown","a3dcae8c":"markdown","78e4458f":"markdown","9bb587f5":"markdown","bc0c5067":"markdown"},"source":{"b7fb6bd4":"%%writefile reactionary.py\n\nimport random\nfrom kaggle_environments.envs.rps.utils import get_score\n\nlast_react_action = None\n\n\ndef reactionary(observation, configuration):\n    global last_react_action\n    if observation.step == 0:\n        last_react_action = random.randrange(0, configuration.signs)\n    elif get_score(last_react_action, observation.lastOpponentAction) <= 1:\n        last_react_action = (observation.lastOpponentAction + 1) % configuration.signs\n\n    return last_react_action","d265a5b8":"%%writefile counter_reactionary.py\n\nimport random\nfrom kaggle_environments.envs.rps.utils import get_score\n\nlast_counter_action = None\n\n\ndef counter_reactionary(observation, configuration):\n    global last_counter_action\n    if observation.step == 0:\n        last_counter_action = random.randrange(0, configuration.signs)\n    elif get_score(last_counter_action, observation.lastOpponentAction) == 1:\n        last_counter_action = (last_counter_action + 2) % configuration.signs\n    else:\n        last_counter_action = (observation.lastOpponentAction + 1) % configuration.signs\n\n    return last_counter_action","e4765012":"%%writefile statistical.py\n\nimport random\nfrom kaggle_environments.envs.rps.utils import get_score\n\naction_histogram = {}\n\ndef statistical(observation, configuration):\n    global action_histogram\n    if observation.step == 0:\n        action_histogram = {}\n        return\n    action = observation.lastOpponentAction\n    if action not in action_histogram:\n        action_histogram[action] = 0\n    action_histogram[action] += 1\n    mode_action = None\n    mode_action_count = None\n    for k, v in action_histogram.items():\n        if mode_action_count is None or v > mode_action_count:\n            mode_action = k\n            mode_action_count = v\n            continue\n\n    return (mode_action + 1) % configuration.signs","f15065fd":"%%writefile nash_equilibrium.py\n\nimport random\n\ndef nash_equilibrium(observation, configuration):\n    return random.randint(0, 2)","ea94be09":"%%writefile markov_agent.py\n\nimport numpy as np\nimport collections\n\ndef markov_agent(observation, configuration):\n    k = 2\n    global table, action_seq\n    if observation.step % 250 == 0: # refresh table every 250 steps\n        action_seq, table = [], collections.defaultdict(lambda: [1, 1, 1])    \n    if len(action_seq) <= 2 * k + 1:\n        action = int(np.random.randint(3))\n        if observation.step > 0:\n            action_seq.extend([observation.lastOpponentAction, action])\n        else:\n            action_seq.append(action)\n        return action\n    # update table\n    key = ''.join([str(a) for a in action_seq[:-1]])\n    table[key][observation.lastOpponentAction] += 1\n    # update action seq\n    action_seq[:-2] = action_seq[2:]\n    action_seq[-2] = observation.lastOpponentAction\n    # predict opponent next move\n    key = ''.join([str(a) for a in action_seq[:-1]])\n    if observation.step < 500:\n        next_opponent_action_pred = np.argmax(table[key])\n    else:\n        scores = np.array(table[key])\n        next_opponent_action_pred = np.random.choice(3, p=scores\/scores.sum()) # add stochasticity for second part of the game\n    # make an action\n    action = (next_opponent_action_pred + 1) % 3\n    # if high probability to lose -> let's surprise our opponent with sudden change of our strategy\n    if observation.step > 900:\n        action = next_opponent_action_pred\n    action_seq[-1] = action\n    return int(action)","ace0bf08":"%%writefile memory_patterns.py\n\nimport random\n\ndef find_pattern(memory_patterns, memory, memory_length):\n    \"\"\" find appropriate pattern in memory \"\"\"\n    for pattern in memory_patterns:\n        actions_matched = 0\n        for i in range(memory_length):\n            if pattern[\"actions\"][i] == memory[i]:\n                actions_matched += 1\n            else:\n                break\n        # if memory fits this pattern\n        if actions_matched == memory_length:\n            return pattern\n    # appropriate pattern not found\n    return None\n\ndef get_step_result_for_my_agent(my_agent_action, opp_action):\n    \"\"\" \n        get result of the step for my_agent\n        1, 0 and -1 representing win, tie and lost results of the game respectively\n    \"\"\"\n    if my_agent_action == opp_action:\n        return 0\n    elif (my_agent_action == (opp_action + 1)) or (my_agent_action == 0 and opp_action == 2):\n        return 1\n    else:\n        return -1\n\n\n# maximum steps in the pattern\nsteps_max = 3\n# minimum steps in the pattern\nsteps_min = 3\n# maximum amount of steps until reassessment of effectiveness of current memory patterns\nmax_steps_until_memory_reassessment = random.randint(80, 120)\n\n# current memory of the agent\ncurrent_memory = []\n# list of 1, 0 and -1 representing win, tie and lost results of the game respectively\n# length is max_steps_until_memory_reassessment\nresults = []\n# current best sum of results\nbest_sum_of_results = 0\n# how many times each action was performed by opponent\nopponent_actions_count = [0, 0, 0]\n# memory length of patterns in first group\n# steps_max is multiplied by 2 to consider both my_agent's and opponent's actions\ngroup_memory_length = steps_max * 2\n# list of groups of memory patterns\ngroups_of_memory_patterns = []\nfor i in range(steps_max, steps_min - 1, -1):\n    groups_of_memory_patterns.append({\n        # how many steps in a row are in the pattern\n        \"memory_length\": group_memory_length,\n        # list of memory patterns\n        \"memory_patterns\": []\n    })\n    group_memory_length -= 2\n    \n\ndef my_agent(obs, conf):\n    \"\"\" your ad here \"\"\"\n    global results\n    global best_sum_of_results\n    # action of my_agent\n    my_action = None\n    \n    # if it's not first step, add opponent's last action to agent's current memory\n    # and reassess effectiveness of current memory patterns\n    if obs[\"step\"] > 0:\n        # count opponent's actions\n        opponent_actions_count[obs[\"lastOpponentAction\"]] += 1\n        # add opponent's last step to current_memory\n        current_memory.append(obs[\"lastOpponentAction\"])\n        # previous step won or lost\n        results.append(get_step_result_for_my_agent(current_memory[-2], current_memory[-1]))\n        \n        # if there is enough steps added to results for memery reassessment\n        if len(results) == max_steps_until_memory_reassessment:\n            results_sum = sum(results)\n            # if effectiveness of current memory patterns has decreased significantly\n            if results_sum < (best_sum_of_results * 0.5):\n                # flush all current memory patterns\n                best_sum_of_results = 0\n                results = []\n                for group in groups_of_memory_patterns:\n                    group[\"memory_patterns\"] = []\n            else:\n                # if effectiveness of current memory patterns has increased\n                if results_sum > best_sum_of_results:\n                    best_sum_of_results = results_sum\n                del results[:1]\n    \n    # search for my_action in memory patterns\n    for group in groups_of_memory_patterns:\n        # if length of current memory is bigger than necessary for a new memory pattern\n        if len(current_memory) > group[\"memory_length\"]:\n            # get momory of the previous step\n            previous_step_memory = current_memory[:group[\"memory_length\"]]\n            previous_pattern = find_pattern(group[\"memory_patterns\"], previous_step_memory, group[\"memory_length\"])\n            if previous_pattern == None:\n                previous_pattern = {\n                    \"actions\": previous_step_memory.copy(),\n                    \"opp_next_actions\": [\n                        {\"action\": 0, \"amount\": 0, \"response\": 1},\n                        {\"action\": 1, \"amount\": 0, \"response\": 2},\n                        {\"action\": 2, \"amount\": 0, \"response\": 0}\n                    ]\n                }\n                group[\"memory_patterns\"].append(previous_pattern)\n            # if such pattern already exists\n            for action in previous_pattern[\"opp_next_actions\"]:\n                if action[\"action\"] == obs[\"lastOpponentAction\"]:\n                    action[\"amount\"] += 1\n            # delete first two elements in current memory (actions of the oldest step in current memory)\n            del current_memory[:2]\n            \n            # if action was not yet found\n            if my_action == None:\n                pattern = find_pattern(group[\"memory_patterns\"], current_memory, group[\"memory_length\"])\n                # if appropriate pattern is found\n                if pattern != None:\n                    my_action_amount = 0\n                    for action in pattern[\"opp_next_actions\"]:\n                        # if this opponent's action occurred more times than currently chosen action\n                        # or, if it occured the same amount of times and this one is choosen randomly among them\n                        if (action[\"amount\"] > my_action_amount or\n                                (action[\"amount\"] == my_action_amount and random.random() > 0.5)):\n                            my_action_amount = action[\"amount\"]\n                            my_action = action[\"response\"]\n    \n    # if no action was found\n    if my_action == None:\n        # choose action randomly\n        my_action = random.randint(0, 2)\n    \n    current_memory.append(my_action)\n    return my_action","93508746":"%%writefile multi_armed_bandit.py\n\n\nimport pandas as pd\nimport numpy as np\nimport json\n\n\n# base class for all agents, random agent\nclass agent():\n    def initial_step(self):\n        return np.random.randint(3)\n    \n    def history_step(self, history):\n        return np.random.randint(3)\n    \n    def step(self, history):\n        if len(history) == 0:\n            return int(self.initial_step())\n        else:\n            return int(self.history_step(history))\n    \n# agent that returns (previousCompetitorStep + shift) % 3\nclass mirror_shift(agent):\n    def __init__(self, shift=0):\n        self.shift = shift\n    \n    def history_step(self, history):\n        return (history[-1]['competitorStep'] + self.shift) % 3\n    \n    \n# agent that returns (previousPlayerStep + shift) % 3\nclass self_shift(agent):\n    def __init__(self, shift=0):\n        self.shift = shift\n    \n    def history_step(self, history):\n        return (history[-1]['step'] + self.shift) % 3    \n\n\n# agent that beats the most popular step of competitor\nclass popular_beater(agent):\n    def history_step(self, history):\n        counts = np.bincount([x['competitorStep'] for x in history])\n        return (int(np.argmax(counts)) + 1) % 3\n\n    \n# agent that beats the agent that beats the most popular step of competitor\nclass anti_popular_beater(agent):\n    def history_step(self, history):\n        counts = np.bincount([x['step'] for x in history])\n        return (int(np.argmax(counts)) + 2) % 3\n    \n    \n# simple transition matrix: previous step -> next step\nclass transition_matrix(agent):\n    def __init__(self, deterministic = False, counter_strategy = False, init_value = 0.1, decay = 1):\n        self.deterministic = deterministic\n        self.counter_strategy = counter_strategy\n        if counter_strategy:\n            self.step_type = 'step' \n        else:\n            self.step_type = 'competitorStep'\n        self.init_value = init_value\n        self.decay = decay\n        \n    def history_step(self, history):\n        matrix = np.zeros((3,3)) + self.init_value\n        for i in range(len(history) - 1):\n            matrix = (matrix - self.init_value) \/ self.decay + self.init_value\n            matrix[int(history[i][self.step_type]), int(history[i+1][self.step_type])] += 1\n\n        if  self.deterministic:\n            step = np.argmax(matrix[int(history[-1][self.step_type])])\n        else:\n            step = np.random.choice([0,1,2], p = matrix[int(history[-1][self.step_type])]\/matrix[int(history[-1][self.step_type])].sum())\n        \n        if self.counter_strategy:\n            # we predict our step using transition matrix (as competitor can do) and beat probable competitor step\n            return (step + 2) % 3 \n        else:\n            # we just predict competitors step and beat it\n            return (step + 1) % 3\n    \n\n# similar to the transition matrix but rely on both previous steps\nclass transition_tensor(agent):\n    \n    def __init__(self, deterministic = False, counter_strategy = False, init_value = 0.1, decay = 1):\n        self.deterministic = deterministic\n        self.counter_strategy = counter_strategy\n        if counter_strategy:\n            self.step_type1 = 'step' \n            self.step_type2 = 'competitorStep'\n        else:\n            self.step_type2 = 'step' \n            self.step_type1 = 'competitorStep'\n        self.init_value = init_value\n        self.decay = decay\n        \n    def history_step(self, history):\n        matrix = np.zeros((3,3, 3)) + 0.1\n        for i in range(len(history) - 1):\n            matrix = (matrix - self.init_value) \/ self.decay + self.init_value\n            matrix[int(history[i][self.step_type1]), int(history[i][self.step_type2]), int(history[i+1][self.step_type1])] += 1\n\n        if  self.deterministic:\n            step = np.argmax(matrix[int(history[-1][self.step_type1]), int(history[-1][self.step_type2])])\n        else:\n            step = np.random.choice([0,1,2], p = matrix[int(history[-1][self.step_type1]), int(history[-1][self.step_type2])]\/matrix[int(history[-1][self.step_type1]), int(history[-1][self.step_type2])].sum())\n        \n        if self.counter_strategy:\n            # we predict our step using transition matrix (as competitor can do) and beat probable competitor step\n            return (step + 2) % 3 \n        else:\n            # we just predict competitors step and beat it\n            return (step + 1) % 3\n\n        \n# looks for the same pattern in history and returns the best answer to the most possible counter strategy\nclass pattern_matching(agent):\n    def __init__(self, steps = 3, deterministic = False, counter_strategy = False, init_value = 0.1, decay = 1):\n        self.deterministic = deterministic\n        self.counter_strategy = counter_strategy\n        if counter_strategy:\n            self.step_type = 'step' \n        else:\n            self.step_type = 'competitorStep'\n        self.init_value = init_value\n        self.decay = decay\n        self.steps = steps\n        \n    def history_step(self, history):\n        if len(history) < self.steps + 1:\n            return self.initial_step()\n        \n        next_step_count = np.zeros(3) + self.init_value\n        pattern = [history[i][self.step_type] for i in range(- self.steps, 0)]\n        \n        for i in range(len(history) - self.steps):\n            next_step_count = (next_step_count - self.init_value)\/self.decay + self.init_value\n            current_pattern = [history[j][self.step_type] for j in range(i, i + self.steps)]\n            if np.sum([pattern[j] == current_pattern[j] for j in range(self.steps)]) == self.steps:\n                next_step_count[history[i + self.steps][self.step_type]] += 1\n        \n        if next_step_count.max() == self.init_value:\n            return self.initial_step()\n        \n        if  self.deterministic:\n            step = np.argmax(next_step_count)\n        else:\n            step = np.random.choice([0,1,2], p = next_step_count\/next_step_count.sum())\n        \n        if self.counter_strategy:\n            # we predict our step using transition matrix (as competitor can do) and beat probable competitor step\n            return (step + 2) % 3 \n        else:\n            # we just predict competitors step and beat it\n            return (step + 1) % 3\n        \n# if we add all agents the algorithm will spend more that 1 second on turn and will be invalidated\n# right now the agens are non optimal and the same computeations are repeated a lot of times\n# the approach can be optimised to run much faster\nagents = {\n    'mirror_0': mirror_shift(0),\n    'mirror_1': mirror_shift(1),  \n    'mirror_2': mirror_shift(2),\n    'self_0': self_shift(0),\n    'self_1': self_shift(1),  \n    'self_2': self_shift(2),\n    'popular_beater': popular_beater(),\n    'anti_popular_beater': anti_popular_beater(),\n    'random_transitison_matrix': transition_matrix(False, False),\n    'determenistic_transitison_matrix': transition_matrix(True, False),\n    'random_self_trans_matrix': transition_matrix(False, True),\n    'determenistic_self_trans_matrix': transition_matrix(True, True),\n    'random_transitison_tensor': transition_tensor(False, False),\n    'determenistic_transitison_tensor': transition_tensor(True, False),\n    'random_self_trans_tensor': transition_tensor(False, True),\n    'determenistic_self_trans_tensor': transition_tensor(True, True),\n    \n    'random_transitison_matrix_decay': transition_matrix(False, False, decay = 1.05),\n    'random_self_trans_matrix_decay': transition_matrix(False, True, decay = 1.05),\n    'random_transitison_tensor_decay': transition_tensor(False, False, decay = 1.05),\n    'random_self_trans_tensor_decay': transition_tensor(False, True, decay = 1.05),\n    \n    'determenistic_transitison_matrix_decay': transition_matrix(True, False, decay = 1.05),\n    'determenistic_self_trans_matrix_decay': transition_matrix(True, True, decay = 1.05),\n    'determenistic_transitison_tensor_decay': transition_tensor(True, False, decay = 1.05),\n    'determenistic_self_trans_tensor_decay': transition_tensor(True, True, decay = 1.05),\n    \n#     'random_transitison_matrix_decay2': transition_matrix(False, False, decay = 1.001),\n#     'random_self_trans_matrix_decay2': transition_matrix(False, True, decay = 1.001),\n#     'random_transitison_tensor_decay2': transition_tensor(False, False, decay = 1.001),\n#     'random_self_trans_tensor_decay2': transition_tensor(False, True, decay = 1.001),\n    \n#     'determenistic_transitison_matrix_decay2': transition_matrix(True, False, decay = 1.001),\n#     'determenistic_self_trans_matrix_decay2': transition_matrix(True, True, decay = 1.001),\n#     'determenistic_transitison_tensor_decay2': transition_tensor(True, False, decay = 1.001),\n#     'determenistic_self_trans_tensor_decay2': transition_tensor(True, True, decay = 1.001),\n    \n#     'random_pattern_matching_decay_1': pattern_matching(1, False, False, decay = 1.001),\n#     'random_self_pattern_matching_decay_1': pattern_matching(1, False, True, decay = 1.001),\n#     'determenistic_pattern_matching_decay_1': pattern_matching(1, True, False, decay = 1.001),\n#     'determenistic_self_pattern_matching_decay_1': pattern_matching(1, True, True, decay = 1.001),\n    \n#     'random_pattern_matching_decay_2': pattern_matching(2, False, False, decay = 1.001),\n#     'random_self_pattern_matching_decay_2': pattern_matching(2, False, True, decay = 1.001),\n#     'determenistic_pattern_matching_decay_2': pattern_matching(2, True, False, decay = 1.001),\n#     'determenistic_self_pattern_matching_decay_2': pattern_matching(2, True, True, decay = 1.001),\n    \n#     'random_pattern_matching_decay_3': pattern_matching(3, False, False, decay = 1.001),\n#     'random_self_pattern_matching_decay_3': pattern_matching(3, False, True, decay = 1.001),\n    'determenistic_pattern_matching_decay_3': pattern_matching(3, True, False, decay = 1.001),\n    'determenistic_self_pattern_matching_decay_3': pattern_matching(3, True, True, decay = 1.001),\n    \n#     'random_pattern_matching_decay_4': pattern_matching(4, False, False, decay = 1.001),\n#     'random_self_pattern_matching_decay_4': pattern_matching(4, False, True, decay = 1.001),\n#     'determenistic_pattern_matching_decay_4': pattern_matching(4, True, False, decay = 1.001),\n#     'determenistic_self_pattern_matching_decay_4': pattern_matching(4, True, True, decay = 1.001),\n    \n#     'random_pattern_matching_decay_5': pattern_matching(5, False, False, decay = 1.001),\n#     'random_self_pattern_matching_decay_5': pattern_matching(5, False, True, decay = 1.001),\n#     'determenistic_pattern_matching_decay_5': pattern_matching(5, True, False, decay = 1.001),\n#     'determenistic_self_pattern_matching_decay_5': pattern_matching(5, True, True, decay = 1.001),\n    \n#     'random_pattern_matching_decay_6': pattern_matching(6, False, False, decay = 1.001),\n#     'random_self_pattern_matching_decay_6': pattern_matching(6, False, True, decay = 1.001),\n#     'determenistic_pattern_matching_decay_6': pattern_matching(6, True, False, decay = 1.001),\n#     'determenistic_self_pattern_matching_decay_6': pattern_matching(6, True, True, decay = 1.001),\n}\n\nhistory = []\nbandit_state = {k:[1,1] for k in agents.keys()}\n    \ndef multi_armed_bandit_agent (observation, configuration):\n    \n    # bandits' params\n    step_size = 3 # how much we increase a and b \n    decay_rate = 1.05 # how much do we decay old historical data\n    \n    global history, bandit_state\n    \n    def log_step(step = None, history = None, agent = None, competitorStep = None, file = 'history.csv'):\n        if step is None:\n            step = np.random.randint(3)\n        if history is None:\n            history = []\n        history.append({'step': step, 'competitorStep': competitorStep, 'agent': agent})\n        if file is not None:\n            pd.DataFrame(history).to_csv(file, index = False)\n        return step\n    \n    def update_competitor_step(history, competitorStep):\n        history[-1]['competitorStep'] = int(competitorStep)\n        return history\n    \n    # load history\n    if observation.step == 0:\n        pass\n    else:\n        history = update_competitor_step(history, observation.lastOpponentAction)\n        \n        # updating bandit_state using the result of the previous step\n        # we can update all states even those that were not used\n        for name, agent in agents.items():\n            agent_step = agent.step(history[:-1])\n            bandit_state[name][1] = (bandit_state[name][1] - 1) \/ decay_rate + 1\n            bandit_state[name][0] = (bandit_state[name][0] - 1) \/ decay_rate + 1\n            \n            if (history[-1]['competitorStep'] - agent_step) % 3 == 1:\n                bandit_state[name][1] += step_size\n            elif (history[-1]['competitorStep'] - agent_step) % 3 == 2:\n                bandit_state[name][0] += step_size\n            else:\n                bandit_state[name][0] += step_size\/2\n                bandit_state[name][1] += step_size\/2\n            \n    # we can use it for analysis later\n    with open('bandit.json', 'w') as outfile:\n        json.dump(bandit_state, outfile)\n    \n    \n    # generate random number from Beta distribution for each agent and select the most lucky one\n    best_proba = -1\n    best_agent = None\n    for k in bandit_state.keys():\n        proba = np.random.beta(bandit_state[k][0],bandit_state[k][1])\n        if proba > best_proba:\n            best_proba = proba\n            best_agent = k\n        \n    step = agents[best_agent].step(history)\n    \n    return log_step(step, history, best_agent)","cb0571e7":"%%writefile opponent_transition_matrix.py\n\nimport numpy as np\nimport pandas as pd\nimport random\n\nT = np.zeros((3, 3))\nP = np.zeros((3, 3))\n\n# a1 is the action of the opponent 1 step ago\n# a2 is the action of the opponent 2 steps ago\na1, a2 = None, None\n\ndef transition_agent(observation, configuration):\n    global T, P, a1, a2\n    if observation.step > 1:\n        a1 = observation.lastOpponentAction\n        T[a2, a1] += 1\n        P = np.divide(T, np.maximum(1, T.sum(axis=1)).reshape(-1, 1))\n        a2 = a1\n        if np.sum(P[a1, :]) == 1:\n            return int((np.random.choice(\n                [0, 1, 2],\n                p=P[a1, :]\n            ) + 1) % 3)\n        else:\n            return int(np.random.randint(3))\n    else:\n        if observation.step == 1:\n            a2 = observation.lastOpponentAction\n        return int(np.random.randint(3))","7af14d4b":"%%writefile decision_tree_classifier.py\n\nimport numpy as np\nimport collections\nfrom sklearn.tree import DecisionTreeClassifier\n\ndef construct_local_features(rollouts):\n    features = np.array([[step % k for step in rollouts['steps']] for k in (2, 3, 5)])\n    features = np.append(features, rollouts['steps'])\n    features = np.append(features, rollouts['actions'])\n    features = np.append(features, rollouts['opp-actions'])\n    return features\n\ndef construct_global_features(rollouts):\n    features = []\n    for key in ['actions', 'opp-actions']:\n        for i in range(3):\n            actions_count = np.mean([r == i for r in rollouts[key]])\n            features.append(actions_count)\n    \n    return np.array(features)\n\ndef construct_features(short_stat_rollouts, long_stat_rollouts):\n    lf = construct_local_features(short_stat_rollouts)\n    gf = construct_global_features(long_stat_rollouts)\n    features = np.concatenate([lf, gf])\n    return features\n\ndef predict_opponent_move(train_data, test_sample):\n    classifier = DecisionTreeClassifier(random_state=42)\n    classifier.fit(train_data['x'], train_data['y'])\n    return classifier.predict(test_sample)\n\ndef update_rollouts_hist(rollouts_hist, last_move, opp_last_action):\n    rollouts_hist['steps'].append(last_move['step'])\n    rollouts_hist['actions'].append(last_move['action'])\n    rollouts_hist['opp-actions'].append(opp_last_action)\n    return rollouts_hist\n\ndef warmup_strategy(observation, configuration):\n    global rollouts_hist, last_move\n    action = int(np.random.randint(3))\n    if observation.step == 0:\n        last_move = {'step': 0, 'action': action}\n        rollouts_hist = {'steps': [], 'actions': [], 'opp-actions': []}\n    else:\n        rollouts_hist = update_rollouts_hist(rollouts_hist, last_move, observation.lastOpponentAction)\n        last_move = {'step': observation.step, 'action': action}\n    return int(action)\n\ndef init_training_data(rollouts_hist, k):\n    for i in range(len(rollouts_hist['steps']) - k + 1):\n        short_stat_rollouts = {key: rollouts_hist[key][i:i+k] for key in rollouts_hist}\n        long_stat_rollouts = {key: rollouts_hist[key][:i+k] for key in rollouts_hist}\n        features = construct_features(short_stat_rollouts, long_stat_rollouts)        \n        data['x'].append(features)\n    test_sample = data['x'][-1].reshape(1, -1)\n    data['x'] = data['x'][:-1]\n    data['y'] = rollouts_hist['opp-actions'][k:]\n    return data, test_sample\n\ndef agent(observation, configuration):\n    # hyperparameters\n    k = 5\n    min_samples = 25\n    global rollouts_hist, last_move, data, test_sample\n    if observation.step == 0:\n        data = {'x': [], 'y': []}\n    # if not enough data -> randomize\n    if observation.step <= min_samples + k:\n        return warmup_strategy(observation, configuration)\n    # update statistics\n    rollouts_hist = update_rollouts_hist(rollouts_hist, last_move, observation.lastOpponentAction)\n    # update training data\n    if len(data['x']) == 0:\n        data, test_sample = init_training_data(rollouts_hist, k)\n    else:        \n        short_stat_rollouts = {key: rollouts_hist[key][-k:] for key in rollouts_hist}\n        features = construct_features(short_stat_rollouts, rollouts_hist)\n        data['x'].append(test_sample[0])\n        data['y'] = rollouts_hist['opp-actions'][k:]\n        test_sample = features.reshape(1, -1)\n        \n    # predict opponents move and choose an action\n    next_opp_action_pred = predict_opponent_move(data, test_sample)\n    action = int((next_opp_action_pred + 1) % 3)\n    last_move = {'step': observation.step, 'action': action}\n    return action","31a9d0cc":"%%writefile statistical_prediction.py\n\nimport random\nimport pydash\nfrom collections import Counter\n\n# Create a small amount of starting history\nhistory = {\n    \"guess\":      [0,1,2],\n    \"prediction\": [0,1,2],\n    \"expected\":   [0,1,2],\n    \"action\":     [0,1,2],\n    \"opponent\":   [0,1],\n}\ndef statistical_prediction_agent(observation, configuration):    \n    global history\n    actions         = list(range(configuration.signs))  # [0,1,2]\n    last_action     = history['action'][-1]\n    opponent_action = observation.lastOpponentAction if observation.step > 0 else 2\n    \n    history['opponent'].append(opponent_action)\n\n    # Make weighted random guess based on the complete move history, weighted towards relative moves based on our last action \n    move_frequency       = Counter(history['opponent'])\n    response_frequency   = Counter(zip(history['action'], history['opponent'])) \n    move_weights         = [ move_frequency.get(n,1) + response_frequency.get((last_action,n),1) for n in range(configuration.signs) ] \n    guess                = random.choices( population=actions, weights=move_weights, k=1 )[0]\n    \n    # Compare our guess to how our opponent actually played\n    guess_frequency      = Counter(zip(history['guess'], history['opponent']))\n    guess_weights        = [ guess_frequency.get((guess,n),1) for n in range(configuration.signs) ]\n    prediction           = random.choices( population=actions, weights=guess_weights, k=1 )[0]\n\n    # Repeat, but based on how many times our prediction was correct\n    prediction_frequency = Counter(zip(history['prediction'], history['opponent']))\n    prediction_weights   = [ prediction_frequency.get((prediction,n),1) for n in range(configuration.signs) ]\n    expected             = random.choices( population=actions, weights=prediction_weights, k=1 )[0]\n\n    # Play the +1 counter move\n    action = (expected + 1) % configuration.signs\n    \n    # Persist state\n    history['guess'].append(guess)\n    history['prediction'].append(prediction)\n    history['expected'].append(expected)\n    history['action'].append(action)\n\n    # Print debug information\n    print('opponent_action                = ', opponent_action)\n    print('move_weights,       guess      = ', move_weights, guess)\n    print('guess_weights,      prediction = ', guess_weights, prediction)\n    print('prediction_weights, expected   = ', prediction_weights, expected)\n    print('action                         = ', action)\n    print()\n    \n    return action","8c3fa805":"list_names = [\n    \"reactionary\", \n    \"counter_reactionary\", \n    \"statistical\", \n    \"nash_equilibrium\",\n    \"markov_agent\", \n    \"memory_patterns\", \n    \"multi_armed_bandit\",\n    \"opponent_transition_matrix\",\n    \"decision_tree_classifier\",\n    \"statistical_prediction\",\n]\n\nlist_agents = [agent_name + \".py\" for agent_name in list_names]\n\n# initate output format\noutput = {}\nfor agent in list_agents:\n    output[agent] = {\n        'win'  : 0,\n        'loss' : 0,\n        'tie'  : 0\n    }","761e64f3":"from ray.util.multiprocessing import Pool as rayPool\nimport pandas as pd\nimport numpy as np\nfrom kaggle_environments import make, evaluate\nimport tqdm","7c64ddef":"# set number of times to play. If there are 5 agents and x = 4, there are going to be total of 5*(5-1)*4 = 80 plays.\nx = 3\nlist_of_games = []\nfor times in range(x):\n    for i in list_agents:\n        for j in list_agents:\n            if i != j:\n                list_of_games.append([i,j])\nlist_of_games[:10]","5b8dec16":"# function to return score\ndef get_result(agents):\n    output = evaluate(\n                \"rps\", \n                [agents[0], agents[1]], \n                configuration={\"episodeSteps\": 1000})\n    return agents[0], agents[1], output[0]","43fa63ef":"results = []\n\n# multiprocessing\npool = rayPool()\nfor content in tqdm.tqdm(pool.imap_unordered(get_result, list_of_games), total = len(list_of_games)):\n    results.append(content)\npool.close()\n    \n# generate final output\nfor result in results:\n    if abs(result[2][0]) < 20:\n        output[result[0]]['tie']  += 1\n        output[result[1]]['tie']  += 1\n    elif result[2][0] >= 20:\n        output[result[0]]['win']  += 1\n        output[result[1]]['loss'] += 1\n    elif result[2][0] <= -20:\n        output[result[0]]['loss'] += 1\n        output[result[1]]['win']  += 1\n","beecde2e":"pd.DataFrame.from_dict(output).T[['win','loss', 'tie']].sort_values('win', ascending=False)","069c6b6c":"def show_me(agent1, agent2):\n    w = 0\n    l = 0\n    t = 0\n    for i in results:\n        if i[0] == agent1 and i[1] == agent2:\n            if i[2][0] >= 20:\n                w += 1\n            elif i[2][0] <= -20:\n                l += 1\n            else:\n                t += 1\n        elif i[0] == agent2 and i[1] == agent1:\n            if i[2][0] >= 20:\n                l += 1\n            elif i[2][0] <= -20:\n                w += 1\n            else:\n                t += 1           \n    print('win: ' + str(w) + ' loss: ' + str(l) + ' tie: ' + str(t) + '  -  ' + agent1 + ' vs ' + agent2)\n    \nfor i in list_agents:\n    for j in list_agents:\n        if i != j:\n            show_me(i, j)\n    print('')","68fcc586":"individual_output = {}\n\nfor i in range(len(list_agents)):\n    for j in range(i+1, len(list_agents)):\n        filtered_results = [x for x in results if (list_agents[i] in x) and (list_agents[j] in x)]\n        individual_output[list_agents[i] + ' vs ' + list_agents[j]] = {\n        'agent1': list_agents[i],\n        'agent2': list_agents[j],\n        'win'   : 0,\n        'loss'  : 0,\n        'tie'   : 0            \n        }\n        individual_output[list_agents[i] + ' vs ' + list_agents[j]]['win']  += sum((1 for x in filtered_results if x[0] == list_agents[i] and x[2][0]>=20))\n        individual_output[list_agents[i] + ' vs ' + list_agents[j]]['loss'] += sum((1 for x in filtered_results if x[0] == list_agents[i] and x[2][0]<=-20))\n        individual_output[list_agents[i] + ' vs ' + list_agents[j]]['tie']  += sum((1 for x in filtered_results if x[0] == list_agents[i] and abs(x[2][0])<20))\n        individual_output[list_agents[i] + ' vs ' + list_agents[j]]['win']  += sum((1 for x in filtered_results if x[0] == list_agents[j] and x[2][0]<=-20))\n        individual_output[list_agents[i] + ' vs ' + list_agents[j]]['loss'] += sum((1 for x in filtered_results if x[0] == list_agents[j] and x[2][0]>=20))\n        individual_output[list_agents[i] + ' vs ' + list_agents[j]]['tie']  += sum((1 for x in filtered_results if x[0] == list_agents[j] and abs(x[2][0])<20))\npd.DataFrame.from_dict(individual_output).T[['agent1', 'agent2','win','loss', 'tie']].reset_index(drop=True)","7614edcf":"from multiprocessing import Pool as mpPool","67b85c93":"# initate output format\noutput = {}\nfor agent in list_agents:\n    output[agent] = {\n        'win'  : 0,\n        'loss' : 0,\n        'tie'  : 0\n    }","39007350":"results = []\n\n# multiprocessing\npool = mpPool()\nfor content in tqdm.tqdm(pool.imap_unordered(get_result, list_of_games), total = len(list_of_games)):\n    results.append(content)\npool.close()\n    \n# generate final output\nfor result in results:\n    if abs(result[2][0]) < 20:\n        output[result[0]]['tie']  += 1\n        output[result[1]]['tie']  += 1\n    elif result[2][0] >= 20:\n        output[result[0]]['win']  += 1\n        output[result[1]]['loss'] += 1\n    elif result[2][0] <= -20:\n        output[result[0]]['loss'] += 1\n        output[result[1]]['win']  += 1\n","de33024c":"pd.DataFrame.from_dict(output).T[['win','loss', 'tie']].sort_values('win', ascending=False)","ae0abf12":"def show_me(agent1, agent2):\n    w = 0\n    l = 0\n    t = 0\n    for i in results:\n        if i[0] == agent1 and i[1] == agent2:\n            if i[2][0] >= 20:\n                w += 1\n            elif i[2][0] <= -20:\n                l += 1\n            else:\n                t += 1\n        elif i[0] == agent2 and i[1] == agent1:\n            if i[2][0] >= 20:\n                l += 1\n            elif i[2][0] <= -20:\n                w += 1\n            else:\n                t += 1           \n    print('win: ' + str(w) + ' loss: ' + str(l) + ' tie: ' + str(t) + '  -  ' + agent1 + ' vs ' + agent2)\n    \nfor i in list_agents:\n    for j in list_agents:\n        if i != j:\n            show_me(i, j)\n    print('')","590575d5":"individual_output = {}\n\nfor i in range(len(list_agents)):\n    for j in range(i+1, len(list_agents)):\n        filtered_results = [x for x in results if (list_agents[i] in x) and (list_agents[j] in x)]\n        individual_output[list_agents[i] + ' vs ' + list_agents[j]] = {\n        'agent1': list_agents[i],\n        'agent2': list_agents[j],\n        'win'   : 0,\n        'loss'  : 0,\n        'tie'   : 0            \n        }\n        individual_output[list_agents[i] + ' vs ' + list_agents[j]]['win']  += sum((1 for x in filtered_results if x[0] == list_agents[i] and x[2][0]>=20))\n        individual_output[list_agents[i] + ' vs ' + list_agents[j]]['loss'] += sum((1 for x in filtered_results if x[0] == list_agents[i] and x[2][0]<=-20))\n        individual_output[list_agents[i] + ' vs ' + list_agents[j]]['tie']  += sum((1 for x in filtered_results if x[0] == list_agents[i] and abs(x[2][0])<20))\n        individual_output[list_agents[i] + ' vs ' + list_agents[j]]['win']  += sum((1 for x in filtered_results if x[0] == list_agents[j] and x[2][0]<=-20))\n        individual_output[list_agents[i] + ' vs ' + list_agents[j]]['loss'] += sum((1 for x in filtered_results if x[0] == list_agents[j] and x[2][0]>=20))\n        individual_output[list_agents[i] + ' vs ' + list_agents[j]]['tie']  += sum((1 for x in filtered_results if x[0] == list_agents[j] and abs(x[2][0])<20))\npd.DataFrame.from_dict(individual_output).T[['agent1', 'agent2','win','loss', 'tie']].reset_index(drop=True)","8e2b8cb4":"# initate output format\noutput = {}\nfor agent in list_agents:\n    output[agent] = {\n        'win'  : 0,\n        'loss' : 0,\n        'tie'  : 0\n    }","13303137":"results = []\n\nfor content in tqdm.tqdm(list_of_games, total = len(list_of_games)):\n    results.append(get_result(content))\npool.close()\n    \n# generate final output\nfor result in results:\n    if abs(result[2][0]) < 20:\n        output[result[0]]['tie']  += 1\n        output[result[1]]['tie']  += 1\n    elif result[2][0] >= 20:\n        output[result[0]]['win']  += 1\n        output[result[1]]['loss'] += 1\n    elif result[2][0] <= -20:\n        output[result[0]]['loss'] += 1\n        output[result[1]]['win']  += 1\n","0c252551":"pd.DataFrame.from_dict(output).T[['win','loss', 'tie']].sort_values('win', ascending=False)","28960538":"def show_me(agent1, agent2):\n    w = 0\n    l = 0\n    t = 0\n    for i in results:\n        if i[0] == agent1 and i[1] == agent2:\n            if i[2][0] >= 20:\n                w += 1\n            elif i[2][0] <= -20:\n                l += 1\n            else:\n                t += 1\n        elif i[0] == agent2 and i[1] == agent1:\n            if i[2][0] >= 20:\n                l += 1\n            elif i[2][0] <= -20:\n                w += 1\n            else:\n                t += 1           \n    print('win: ' + str(w) + ' loss: ' + str(l) + ' tie: ' + str(t) + '  -  ' + agent1 + ' vs ' + agent2)\n    \nfor i in list_agents:\n    for j in list_agents:\n        if i != j:\n            show_me(i, j)\n    print('')","295354d4":"individual_output = {}\n\nfor i in range(len(list_agents)):\n    for j in range(i+1, len(list_agents)):\n        filtered_results = [x for x in results if (list_agents[i] in x) and (list_agents[j] in x)]\n        individual_output[list_agents[i] + ' vs ' + list_agents[j]] = {\n        'agent1': list_agents[i],\n        'agent2': list_agents[j],\n        'win'   : 0,\n        'loss'  : 0,\n        'tie'   : 0            \n        }\n        individual_output[list_agents[i] + ' vs ' + list_agents[j]]['win']  += sum((1 for x in filtered_results if x[0] == list_agents[i] and x[2][0]>=20))\n        individual_output[list_agents[i] + ' vs ' + list_agents[j]]['loss'] += sum((1 for x in filtered_results if x[0] == list_agents[i] and x[2][0]<=-20))\n        individual_output[list_agents[i] + ' vs ' + list_agents[j]]['tie']  += sum((1 for x in filtered_results if x[0] == list_agents[i] and abs(x[2][0])<20))\n        individual_output[list_agents[i] + ' vs ' + list_agents[j]]['win']  += sum((1 for x in filtered_results if x[0] == list_agents[j] and x[2][0]<=-20))\n        individual_output[list_agents[i] + ' vs ' + list_agents[j]]['loss'] += sum((1 for x in filtered_results if x[0] == list_agents[j] and x[2][0]>=20))\n        individual_output[list_agents[i] + ' vs ' + list_agents[j]]['tie']  += sum((1 for x in filtered_results if x[0] == list_agents[j] and abs(x[2][0])<20))\npd.DataFrame.from_dict(individual_output).T[['agent1', 'agent2','win','loss', 'tie']].reset_index(drop=True)","42b74695":"#### Check overall win-loss records","e94f01e0":"#### Check overall win-loss records","8ff2c076":"#### Check individual win-loss records","fecc6025":"#### Check individual win-loss records","050670ef":"This notebook is going to evaluate local agents by comparing each agent with others for x times.\n\nThis notebook also includes 1) multiprocess using ray library; 2) multiprocess using multiprocessing library; 3) without multiprocess.","f9fe95c4":"## Without multiprocessing","574ebcfa":"#### Check results between two agents","5114edac":"#### Check individual win-loss records","a1143746":"### Updates\n\n**12\/13\/2020**: Added individual level's win-loss records.\n\n**01\/10\/2021**: Added function to check results between two agents.","6349c91a":"#### Check results between two agents","1aebed36":"#### Check results between two agents","be9e97c8":"#### Check overall win-loss records","6d26c16a":"## Writing some popular agents","a3dcae8c":"## Multiprocessing Using multiprocessing","78e4458f":"## Multiprocessing Using ray","9bb587f5":"## About this Notebook","bc0c5067":"Agents are from https:\/\/www.kaggle.com\/ihelon\/rock-paper-scissors-agents-comparison"}}