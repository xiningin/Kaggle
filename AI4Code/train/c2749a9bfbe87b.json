{"cell_type":{"ffe1d499":"code","4be20efb":"code","072b166a":"code","d2935720":"code","365374ea":"code","84244b29":"code","c30028b1":"code","dad1ad2d":"code","63291cae":"code","901a68a7":"code","46664426":"code","bb90edab":"code","371b4918":"code","6c0a316d":"code","63cc40b1":"code","20a5d4a6":"code","6c86ce8e":"code","26883a52":"code","1ead28e3":"code","bab17480":"markdown","04bb5d28":"markdown","bfefebb1":"markdown","87a2c072":"markdown","3c9930cb":"markdown"},"source":{"ffe1d499":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n        \nfrom datetime import datetime\nfrom scipy.special import logsumexp\n\nfrom catboost import Pool, cv, CatBoostClassifier, CatBoostRegressor\nfrom sklearn.metrics import mean_squared_error, classification_report","4be20efb":"train = pd.read_csv(\"\/kaggle\/input\/caltech-cs155-2020\/train.csv\")\ntest = pd.read_csv(\"\/kaggle\/input\/caltech-cs155-2020\/test.csv\")\ndf = pd.concat([train,test],sort=False)\nprint(df.shape)\nprint(df.columns)\ndf.tail()","072b166a":"train.head()","d2935720":"test.head()","365374ea":"## y is binary.\ndisplay(train[\"y\"].describe())","84244b29":"bid_cols = ['bid1','bid2', 'bid3', 'bid4', 'bid5']\nbid_vol_cols = ['bid1vol', 'bid2vol', 'bid3vol', 'bid4vol', 'bid5vol']\nask_cols = ['ask1', 'ask2', 'ask3', 'ask4', 'ask5',]\nask_vol_cols = ['ask1vol','ask2vol', 'ask3vol', 'ask4vol', 'ask5vol']\n\ngroup_cols = {\"bid_cols\":bid_cols,\"bid_vol_cols\":bid_vol_cols,\"ask_cols\":ask_cols,\"ask_vol_cols\":ask_vol_cols}","c30028b1":"for group in group_cols.keys():\n    print(group)\n    df[f\"{group}_max\"] = df[group_cols[group]].max(axis=1)\n    df[f\"{group}_min\"] = df[group_cols[group]].min(axis=1)\n    df[f\"{group}_spread\"] = df[f\"{group}_max\"].div(df[f\"{group}_min\"])\n    df[f\"{group}_logsumexp\"] = df[group_cols[group]].apply(logsumexp)\n    \n    df[f\"{group}_max\"] = df[group_cols[group]].max(axis=1)\n    \ndf[\"last_price_div__mid\"] = df[\"last_price\"].div(df[\"mid\"])","dad1ad2d":"df[\"date\"] = pd.to_datetime(\"1.1.2019\")\ndf[\"date\"] = df[\"date\"] + pd.to_timedelta(df[\"id\"]\/2,unit=\"s\") # 500 ms per row\n\ndf[\"date\"].describe()","63291cae":"train = df.loc[~df.y.isna()]\nprint(f\"train shape {train.shape[0]}\")\ntest = df.loc[df.y.isna()]\nprint(f\"test shape {test.shape[0]}\")","901a68a7":"train.drop([\"id\"],axis=1).to_csv(\"train_hft.csv.gz\",index=False,compression=\"gzip\")\ntest.to_csv(\"test_hft_nodates.csv.gz\",index=False,compression=\"gzip\")","46664426":"# we don't know if the test set has a temporal split, so we'll just try a random split for now\nX = train.drop([\"id\",\"date\",\"y\"],axis=1)\ny = train[\"y\"]","bb90edab":"train_pool = Pool(data=X,label = y)","371b4918":"# ### hyperparameter tuning example grid for catboost : \n# grid = {'learning_rate': [0.05, 0.1],\n#         'depth': [6, 11],\n# #         'l2_leaf_reg': [1, 3,9],\n# #        \"iterations\": [1000],\n#        \"custom_metric\":['Logloss', 'AUC']}\n\n# model = CatBoostClassifier()\n\n# ## can also do randomized search - more efficient typically, especially for large search space - `randomized_search`\n# grid_search_result = model.grid_search(grid, \n#                                        train_pool,\n#                                        plot=True,\n#                                        refit = True, #  refit best model on all data\n#                                       partition_random_seed=42)\n\n# print(model.get_best_score())","6c0a316d":"model = CatBoostClassifier()\n    \nmodel.fit(train_pool, plot=True,silent=True)\nprint(model.get_best_score())","63cc40b1":"feature_importances = model.get_feature_importance(train_pool)\nfeature_names = X.columns\nfor score, name in sorted(zip(feature_importances, feature_names), reverse=True):\n    if score > 0.2:\n        print('{0}: {1:.2f}'.format(name, score))","20a5d4a6":"import shap\nshap.initjs()\n\nexplainer = shap.TreeExplainer(model)\nshap_values = explainer.shap_values(train_pool)\n\n# visualize the training set predictions\n# SHAP plots for all the data is very slow, so we'll only do it for a sample. Taking the head instead of a random sample is dangerous! \nshap.force_plot(explainer.expected_value,shap_values[0,:300], X.iloc[0,:300])","6c86ce8e":"# summarize the effects of all the features\nshap.summary_plot(shap_values, X)","26883a52":"## todo : PDP features +- from shap","1ead28e3":"test[\"Predicted\"] = model.predict(test.drop([\"id\",\"date\",\"y\"],axis=1),prediction_type='Probability')[:,1]\ntest[[\"id\",\"Predicted\"]].to_csv(\"submission.csv\",index=False)","bab17480":"* Additional features could include: rank, which bid number is the max\/min, etc' \n* features between the aggregated features (e.g. max bid div max ask..)","04bb5d28":"## Features importances\n","bfefebb1":"## export predictions","87a2c072":"# Split back into train and test, and build model","3c9930cb":"* The temporal order is scrambled in the test data making TS useless there.\n* Still, just for learning \/ realism, we can still do it in the training data! \n* Let's add pseudo dates, and aggregate features on column subsets. Finally i'll run a model to predict the target!"}}