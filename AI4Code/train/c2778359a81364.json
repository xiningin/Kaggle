{"cell_type":{"677471bd":"code","43366060":"code","1152f031":"code","f7fee6f0":"code","0ede3ef1":"code","e821fe3d":"code","a96e6e22":"code","6aea6d68":"code","088ccb52":"code","ca078064":"code","d80e9223":"code","101d287f":"code","bac90555":"code","0c8cc1bc":"code","165f76b0":"code","7489c998":"code","9ec686cf":"code","17716d09":"markdown","3a7d4213":"markdown","a35ea4d5":"markdown","fef5539b":"markdown","32f9bf03":"markdown","26da3a49":"markdown","35671801":"markdown","335011cb":"markdown","92ced428":"markdown","c1679063":"markdown","ac96b884":"markdown","9c375e26":"markdown","ec740307":"markdown"},"source":{"677471bd":"!pip install tf-agents","43366060":"import tensorflow as tf\nimport numpy as np\nimport tf_agents\n\nfrom tf_agents.environments import py_environment\nfrom tf_agents.specs import array_spec\nfrom tf_agents.trajectories import time_step as ts\nfrom tf_agents.trajectories import policy_step\n\nimport matplotlib.pyplot as plt\nimport pandas as pd\n\nimport random\nimport collections\nimport time\nimport pickle\nimport sys\nimport os\n\n%config Completer.use_jedi = False","1152f031":"class ticTacToe(py_environment.PyEnvironment):\n    def __init__(self):\n        self._action_spec = array_spec.BoundedArraySpec((3,), np.int32, 0, 3, 'action')  #[x,y,symbol]\n        self._observation_spec = array_spec.BoundedArraySpec(shape=(3,3), dtype=np.int32, minimum=0, name='observation') \n        self.board = np.zeros(shape=(3,3), dtype=np.int32)\n        \n        \n        \n    def action_spec(self):\n        return self._action_spec\n\n    def observation_spec(self):\n        return self._observation_spec\n    \n    \n    def _reset(self):\n        self.board = np.zeros(shape=(3,3), dtype=np.int32)\n        return ts.restart(np.array(self.board))\n        \n    def _step(self, action):\n        '''\n        Rewards:\n        -1 for invalid moves\n        1 if you make the winning move\n        0 otherwise\n        '''\n        \n        \n        index = tuple(action[:-1])\n        \n        if not self.inrange(index[0],index[1]) or self.board[tuple(index)] != 0:\n            print(\"INVALID \",action)\n            return ts.termination(np.array(self.board), -1)\n        \n        else:\n            self.board[tuple(index)] = action[-1]\n            if(self.check_winner(index, action[-1])):\n                return ts.termination(np.array(self.board), 1)\n            \n            elif(self.check_draw()):\n                return ts.termination(np.array(self.board), 0)\n            \n            else:\n                return ts.transition(np.array(self.board), 0)\n    \n    def inrange(self, i,j):\n        if 0<=i<=2 and 0<=j<=2:\n            return True\n        return False\n\n    def check_winner(self, index, player):\n        '''\n        Utility function to check if the current move is a winning move\n        index: (x,y) coordinates of the current move\n        player: [1|2] symbol of the player who made the current move\n        '''\n        x,y = index[0],index[1]\n        \n        for i in [-1,0,1]:    #for all neighbours of current move\n            for j in [-1,0,1]:\n                if i==j==0:    #except itself\n                    continue\n        \n                if self.inrange(x+i,y+j):\n                    if self.board[(x+i,y+j)] == player:    #has the same symbol\n                        \n                        if self.inrange(x+i*2,y+j*2):    #check along the same direction(when you are starting from corners)\n                            if self.board[(x+i*2,y+j*2)] == player:\n                                return True\n                            \n                        \n                        if self.inrange(x-i,y-j):    #check the other direction(when you are starting from middle)\n                            if self.board[(x-i,y-j)] == player:\n                                return True\n        \n        return False\n    \n    \n    def check_draw(self): \n        '''\n        Utility function to check if the game ended in a draw\n        Note: check for draw only after checking for a winner\n        '''\n        for i in [0,1,2]:\n            for j in [0,1,2]:\n                if self.board[(i,j)]==0:\n                    return False\n        \n        return True\n    \n    \n    def display(self):\n        print(self.board)","f7fee6f0":"class GymTTT():\n    \"\"\"\n    It is a wrapper around the tictactoe env.\n    It takes an agent as argument which will be used to choose the action\n    \n    Arguments:\n        agent: instance of an agent\n        verbose: [True|False] should messages be printed to console output?\n    \n    Usage:\n        gym = GymTTT(Agent(),True)\n    \"\"\"\n    \n    \n    \n    \n    def __init__(self, agent, verbose = False):\n        self.agent = agent\n        self.env = ticTacToe()\n        self.verbose = verbose\n    \n    def reset(self):\n        if self.agent.symbol == 1:    #if gym's agent has to start, then make the first move\n            timeStep = self.env.reset()\n            policyStep = self.agent.action(timeStep)\n            timeStep = self.env.step(policyStep.action)\n            timeStep = timeStep._replace(reward = timeStep.reward * -1)    #invert reward\n            \n            \n            self.print_message(\"\\n********GYM STARTS********\")\n            self.print_message(\"GYM Agent Move: \"+str(policyStep.action[:2]))\n            self.fancy_display(timeStep.observation)\n            return timeStep\n        \n        else:\n            self.print_message(\"\\n********PLAYER STARTS********\")    \n            return self.env.reset()\n    \n    \n    def step(self, action):\n            \n        #player's move\n        timeStep = self.env.step(action)\n        self.print_message(\"Player Move: \"+str(action[:2]))\n        self.fancy_display(timeStep.observation)\n\n        #if player's move was last\n        if timeStep.is_last():\n            if timeStep.reward==0:\n                self.print_message(\"***Game Over: Draw***\")\n            elif timeStep.reward==-1:\n                self.print_message(\"***Invalid move: Gym Agent Wins\")\n            else:\n                self.print_message(\"*** Yay!: Test Agent Wins ***\")\n            \n            return timeStep\n        \n        else:\n            #agent's move\n            policyStep = self.agent.action(timeStep)\n            timeStep = self.env.step(policyStep.action)\n            timeStep = timeStep._replace(reward = timeStep.reward * -1)    #invert agent's reward for player\n            \n            self.print_message(\"GYM Agent Move: \"+str(policyStep.action[:2]))\n            self.fancy_display(timeStep.observation)\n            \n            if timeStep.is_last():\n                if timeStep.reward==0:\n                    self.print_message(\"***Game Over: Draw***\")\n                else:\n                    self.print_message(\"***Yay!: Gym Agent Wins ***\")\n            \n            return timeStep\n            \n    \n    def print_message(self,message):\n        if self.verbose:\n            print(message)\n        \n        \n    def fancy_display(self,board,action=[-1,-1]):\n        #array representation was good enough\n        if self.verbose:\n            print(board)","0ede3ef1":"class RandomTTTAgent():\n    \"\"\"\n         Arguments:\n            symbol: [1|2] - player symbol\n        Usage:\n            Agent =  RandomTTTAgent(1) # Plays first\n            \n    \"\"\"\n    def __init__(self,symbol):\n        self.symbol = symbol\n        self.trainable = False\n        \n    def action(self, timestep):\n        board = timestep.observation\n        empty_slots = []\n        for i in range(0,3):\n            for j in range(0,3):\n                if board[(i,j)] == 0:\n                    empty_slots.append([i,j])\n        \n        choice = random.choice(empty_slots)\n        return tf_agents.trajectories.PolicyStep(action=choice+[self.symbol], state = board, info=self.symbol)\n    ","e821fe3d":"class MinMaxAgent():    #builds a minmax tree\n    \n    \"\"\"\n     Arguments:\n            symbol: [1|2] - player symbol\n            verbose: (optional) [True|False] - want messages printed to console output?\n            \n        Usage:\n            Agent =  MinMaxAgent(2, True) -- plays first and prints debug messages\n            \n    \"\"\"\n    \n    def __init__(self, symbol, verbose=False):\n        self.symbol = symbol\n        self.verbose = verbose\n        self.trainable = False\n        \n    def action(self, timeStep):\n        board = np.array(timeStep.observation)\n        \n        act = self.getBestAction(np.array(board))\n        \n        return tf_agents.trajectories.PolicyStep(action = list(act)+[self.symbol], state = board, info = self.symbol)\n    \n    \n    def getBestAction(self,board):\n        empty_slots = []\n        for i in range(0,3):\n            for j in range(0,3):\n                if board[(i,j)] == 0:\n                    empty_slots.append([i,j])\n    \n        best_score = -100\n        act = ()\n        temp_board = np.array(board)\n        if self.verbose:\n            print(\"MinMax Says:\")\n        for [i,j] in empty_slots:\n            temp_board[(i,j)] = self.symbol\n            score = self.minmax(np.array(temp_board), 0, False, (i,j,self.symbol))\n            if self.verbose:\n                print(i,j,score)\n            if score>best_score:\n                best_score = score\n                act = (i,j)\n            temp_board[(i,j)] = 0\n            \n        return act\n    \n    \n    def minmax(self, board, depth, maximise, last_move):\n        '''\n            MinMax tree: Alternate between your and opponent's move. \n            In your move you'll pick the maximising choice; In the opponents move pick the minimising choice(which is his maximum)\n            Here: \n                A winning move gets 10 points\n                A losing move gets -10 points\n                A drawn board gets 0\n                All intermediate moves gets (finalScore - numberOfStepsLeft)\n            Imagine a tree with leaf nodes having 0\/10\/-10 and with each level above gets max or min of all its branches depending on the level\n        '''\n\n        if self.check_winner(board,last_move):\n            if last_move[-1] == self.symbol:\n                return 10 \n            else:\n                return -10 \n             \n        if self.check_draw(board): #check draw only after checking for winners\n            return 0\n           \n        \n        empty_slots = []\n        for i in range(0,3):\n            for j in range(0,3):\n                if board[(i,j)] == 0:\n                    empty_slots.append([i,j])\n                    \n        temp_board = np.array(board)\n        \n        if maximise:\n            best_val = -100\n            for (i,j) in empty_slots:\n                player = 3 - last_move[-1]    #switch b\/w 1 and 2\n                \n                temp_board[(i,j)] = player    #pick one of the empty spots\n                \n                best_val = max(self.minmax(np.array(temp_board), depth+1, not maximise, (i,j,player)),best_val)\n\n                temp_board[(i,j)] = 0    #clear the picked empty spot\n                \n        else:\n            best_val = 100\n            for (i,j) in empty_slots:\n                player = 3 - last_move[-1]\n                temp_board[(i,j)] = player\n                \n                best_val = min(self.minmax(np.array(temp_board), depth+1, not maximise, (i,j,player)), best_val)\n                \n                temp_board[(i,j)] = 0\n        \n        return best_val\n        \n            \n    \n    def check_draw(self,board):\n        board = np.array(board)\n        for i in [0,1,2]:\n            for j in [0,1,2]:\n                if board[(i,j)]==0:\n                    return False\n        \n        return True\n    \n    \n    def check_winner(self, board, last_move):\n        board = np.array(board)\n        x,y,player = last_move[0],last_move[1],last_move[2]\n        \n        for i in [-1,0,1]:    #for all neighbours of current move\n            for j in [-1,0,1]:\n                if i==j==0:    #except itself\n                    continue\n        \n                if self.inrange(x+i,y+j):\n                    if board[(x+i,y+j)] == player:    #has the same symbol\n                        \n                        if self.inrange(x+i*2,y+j*2):    #check along the same direction(when you are starting from corners)\n                            if board[(x+i*2,y+j*2)] == player:\n                                return True\n                            \n                        \n                        if self.inrange(x-i,y-j):    #check the other direction(when you are starting from middle)\n                            if board[(x-i,y-j)] == player:\n                                return True\n        \n        return False\n    \n    def inrange(self, i,j):\n        if 0<=i<=2 and 0<=j<=2:\n            return True\n        return False\n    ","a96e6e22":"class DPMinMaxAgent():    #minmax takes for ever, so memoize it\n    '''\n        Here we use a dictionary to store the rewards, where board is the key and its minmax score is its value.\n        Arguments:\n            symbol: [1|2] - player symbol\n            verbose: [True|False] - want messages printed to console output?\n            saveTree: [True|False] - save the constructed tree dict into a file?\n            loadTree: [True|False] - load a previously constructed tree dict from memory?\n            saveTreeFreq: if saveTree is True, save the tree into memory once every 'x' updates. This is coded to decrease over time.\n            \n        Usage:\n            Agent = DPMinMaxAgent(2) -- plays second, doesn't save or load from memory\n            Agent = DPMinMaxAgent(1, verbose=True, saveTree=False, loadTree=True) -- to load from memory but not save it.\n            \n    '''\n    def __init__(self, symbol, verbose=False, saveTree=False, loadTree=False, saveTreeFreq=100):\n        self.symbol = symbol\n        self.verbose = verbose\n        self.trainable = False\n        \n        self.saveTreeFreq = saveTreeFreq\n        self.saveTreeFreqStart = saveTreeFreq    # high val would mean a lot of them won't be saved; low value would update too many times, so decrease freq periodically\n        self.saveTree = saveTree\n        self.pickle_loaded = False\n        if loadTree:\n            try:\n                with open(\"\/kaggle\/working\/minmaxtree.pickle\",\"rb\") as f:\n                    self.tree = pickle.load(f)\n                    self.pickle_loaded = True\n            except Exception as e:\n                self.pickle_loaded = False\n                print(e)\n                loadTree = False\n            \n        if not self.pickle_loaded:\n            self.tree = {}\n     \n        \n    def action(self, timeStep):\n        board = np.array(timeStep.observation)\n        \n        act = self.getBestAction(np.array(board))\n        \n        return tf_agents.trajectories.PolicyStep(action = list(act)+[self.symbol], state = board, info = self.symbol)\n    \n    \n    def getBestAction(self,board):\n        empty_slots = []\n        for i in range(0,3):\n            for j in range(0,3):\n                if board[(i,j)] == 0:\n                    empty_slots.append([i,j])\n    \n        best_score = -100\n        act = ()\n        temp_board = np.array(board)\n        \n        if self.verbose:\n            print(\"MinMax Says:\")\n            \n        for [i,j] in empty_slots:\n            temp_board[(i,j)] = self.symbol\n            score = self.minmax(temp_board, 0, False, (i,j,self.symbol))\n            if self.verbose:\n                print(i,j,score)\n            if score>best_score:\n                best_score = score\n                act = (i,j)\n            temp_board[(i,j)] = 0\n            \n        return act\n    \n    \n    def minmax(self, board, depth, maximise, last_move):\n        last_move = list(last_move)\n        \n        if self.tree.get(board.tobytes()):    #check dict and return value if it already exists\n            return self.tree.get(board.tobytes())\n        \n        \n        if self.check_winner(board,last_move):\n            if last_move[-1] == self.symbol:\n                return 20\n            else:\n                return -20\n             \n        if self.check_draw(board):\n            return 0\n        \n            \n        \n        empty_slots = []\n        for i in range(0,3):\n            for j in range(0,3):\n                if board[(i,j)] == 0:\n                    empty_slots.append([i,j])\n                    \n        temp_board = np.array(board)\n        \n        if maximise:\n            best_val = -100\n            for (i,j) in empty_slots:\n                player = 3 - last_move[-1]\n                \n                temp_board[(i,j)] = player                \n                temp_board_value = self.oneLess(self.minmax(np.array(temp_board), depth+1, not maximise, (i,j,player)))\n                if not self.tree.get(temp_board.tobytes()):    #store value if not exists\n                    self.tree[temp_board.tobytes()] = temp_board_value\n                    self.saveTreeFreq -= 1\n                \n                best_val = max(temp_board_value,best_val)\n                temp_board[(i,j)] = 0\n                \n        else:\n            best_val = 100\n            for (i,j) in empty_slots:\n                player = 3 - last_move[-1]\n                \n                temp_board[(i,j)] = player\n                temp_board_value = self.oneLess(self.minmax(np.array(temp_board), depth+1, not maximise, (i,j,player)))\n                if not self.tree.get(temp_board.tobytes()):\n                    self.tree[temp_board.tobytes()] = temp_board_value\n                    self.saveTreeFreq -= 1\n                \n                best_val = min(temp_board_value, best_val)\n                temp_board[(i,j)] = 0\n        \n        \n        if self.saveTree and self.saveTreeFreq==0:\n            with open(\"\/kaggle\/working\/minmaxtree.pickle\",\"wb\") as f:\n                pickle.dump(self.tree, f)\n                self.saveTreeFreq = self.saveTreeFreqStart\n                self.saveTreeFreqStart -= 1    # n*(n-1)\/2 updates\n        \n        return best_val\n        \n        \n    def oneLess(self,x):\n        if x>0:\n            return x-1\n        elif x< 0:\n            return x+1\n        else:\n            return 0\n            \n    \n    def check_draw(self,board):\n        board = np.array(board)\n        for i in [0,1,2]:\n            for j in [0,1,2]:\n                if board[(i,j)]==0:\n                    return False\n        \n        return True\n    \n    \n    def check_winner(self, board, last_move):\n        board = np.array(board)\n        x,y,player = last_move[0],last_move[1],last_move[2]\n        \n        for i in [-1,0,1]:    #for all neighbours of current move\n            for j in [-1,0,1]:\n                if i==j==0:    #except itself\n                    continue\n        \n                if self.inrange(x+i,y+j):\n                    if board[(x+i,y+j)] == player:    #has the same symbol\n                        \n                        if self.inrange(x+i*2,y+j*2):    #check along the same direction(when you are starting from corners)\n                            if board[(x+i*2,y+j*2)] == player:\n                                return True\n                            \n                        \n                        if self.inrange(x-i,y-j):    #check the other direction(when you are starting from middle)\n                            if board[(x-i,y-j)] == player:\n                                return True\n        \n        return False\n    \n    def inrange(self, i,j):\n        if 0<=i<=2 and 0<=j<=2:\n            return True\n        return False\n    \n    ","6aea6d68":"t = time.time()\n\nTestAgent = DPMinMaxAgent(1)    #Agent 1\ngymAgent = RandomTTTAgent(2)    #Agent 2\n\ngym = GymTTT(gymAgent,True)\n\ntimeStep = gym.reset()  \n\nwhile not timeStep.is_last():\n    timeStep = gym.step(TestAgent.action(timeStep).action)\n\nprint(time.time() - t)","088ccb52":"class HumanTTTAgent():    #for humans vs machine\n    def __init__(self, symbol, verbose=False):\n        self.symbol= symbol\n        self.verbose = verbose\n        self.trainable = False\n        \n    \n    def action(self, timeStep):\n        board = timeStep.observation\n        \n        empty_slots = []\n        for i in range(0,3):\n            for j in range(0,3):\n                if board[(i,j)] == 0:\n                    empty_slots.append([i,j])\n        \n        print(board)\n        print(\"input space seperated indices; choose from the empty slots\")\n        print(\"EmptySlots: \",empty_slots)\n        i,j = self.get_inputs()\n        tries = 2\n        while [i,j] not in empty_slots and tries:\n            print(\"invalid choice, input space seperated indices, tries left: \",tries)\n            i,j = self.get_inputs()\n            tries -= 1\n        \n        if not tries:\n            print(\"Illiterate\")\n            \n        act = [i,j,self.symbol]\n        return tf_agents.trajectories.PolicyStep(action = act, state = board, info = self.symbol)\n    \n    def get_inputs(self):\n        try:\n            i,j = [int(x) for x in input().split()]\n            return (i,j)\n        except:\n            return (9,9)\n        \n    def updateActionValue(self, qtuple):\n        pass","ca078064":"#Use this as a template to play around with the agents\n'''\nt = time.time()\n\nTestAgent =     #Agent 1\ngymAgent =     #Agent 2\n\ngym = GymTTT(gymAgent,True)\n\ntimeStep = gym.reset()  \n\nwhile not timeStep.is_last():\n    timeStep = gym.step(TestAgent.action(timeStep).action)\n\nprint(time.time() - t)\n'''","d80e9223":"class QLearningAgent():\n    def __init__(self, symbol, trainable = True, alpha = 0.1, gamma =1, load_av = False, save_av = False, saveAVFreq = 100):\n        '''\n        Action values: Dictionary with board as index and a dictionary of {(state,symbol):{action:reward}} as values\n        \n        Arguments:\n            symbol: [1|2] - player symbol\n            trainable: [True|False] - should we use the current game for training?\n            alpha: learning rate\n            gamma: discount rate\n            load_av: [True|False] - load a previously trained dict from memory?\n            save_av: [True|False] - save action values into memory?\n            saveAVFreq: save action values after these many updates\n            \n        Usage:\n            Agent = QLearningAgent(2) -- plays second, doesn't save or load from memory\n            Agent = QLearningAgent(1, save_av=False, load_av=True) -- load trained data from memory but don't save any updates.\n            Agent = QLearningAgent(1, alpha=0.3, gamma=0.9) \n        \n        '''\n        self.symbol = symbol\n        self.trainable = trainable\n        self.alpha = alpha\n        self.gamma = gamma\n        \n        self.saveAVFreq = saveAVFreq\n        self.save_av = save_av\n        self.pickle_loaded = False\n        if load_av:\n            try:\n                with open(\"\/kaggle\/working\/av.pickle\",\"rb\") as f:\n                    self.av = pickle.load(f)\n                    self.pickle_loaded = True\n            except Exception as e:\n                self.pickle_loaded = False\n                print(e)\n                load_av = False\n            \n        if not self.pickle_loaded:\n            self.av = collections.defaultdict(self.module_default_dict)\n     \n    \n    def module_default_dict(self):    #pickle cant save lambdas\n        return collections.defaultdict(int)\n        \n    def updateActionValue(self, qtuple):\n        '''\n            qtuple holds: \n                current state: board - 3x3 array\n                action taken: indices - [i,j]\n                reward obtained\n                next state: board - 3x3 array\n        '''\n        \n        current = qtuple['cur'].tobytes() #serialize it\n        action = qtuple['act'] if len(qtuple['act']) == 2 else qtuple['act'][:2]\n        reward = qtuple['rew']\n        nextState = qtuple['nex'].tobytes()\n        \n        qmax = max(self.av[(nextState,self.symbol)].values()) if self.av[(nextState,self.symbol)].values() else 0    #if next_state is not present in av, then return 0\n        self.av[(current,self.symbol)][tuple(action)] += self.alpha * (reward + self.gamma * qmax - self.av[(current,self.symbol)][tuple(action)])\n        \n        self.saveAVFreq -= 1\n        if self.save_av and self.saveAVFreq == 0:\n            with open(\"\/kaggle\/working\/av.pickle\",\"wb\") as f:\n                pickle.dump(self.av, f)\n            self.saveAVFreq = 100\n        \n    \n    def action(self, timeStep):\n        board = timeStep.observation\n        qsa = dict(self.av[(board.tobytes(),self.symbol)])\n        \n        was_random_choice = False\n        empty_slots = []    #find the list of actions possible\n        for i in range(0,3):\n            for j in range(0,3):\n                if board[(i,j)] == 0:\n                    empty_slots.append([i,j])\n        \n        while qsa and max(qsa.values())>=0:  #choose from the existing options only if it has a non -ve value\n            act = max(qsa, key=qsa.get) \n            if board[tuple(act)] ==0:\n                break\n            qsa.pop(act,None)\n        \n        else:\n            was_random_choice = True\n            act = random.choice(empty_slots)    #else chose randomly\n                \n            \n        return tf_agents.trajectories.PolicyStep(action=list(act) + [self.symbol], state = board, info=was_random_choice)\n        \n        \n    def displayAV(self):\n        print(\"***AV***\")\n        for k,v in dict(self.av).items():\n            board = np.ndarray((3,3), np.int32, k[0])\n            print(board,k[1])\n            print(v)","101d287f":"#Sanity test against a random agent\nt = time.time()\n\nTestAgent = QLearningAgent(1)    #Agent 1\ngymAgent = RandomTTTAgent(2)    #Agent 2\n\ngym = GymTTT(gymAgent,True)\n\ntimeStep = gym.reset()  \n\nwhile not timeStep.is_last():\n    timeStep = gym.step(TestAgent.action(timeStep).action)\n\nprint(time.time() - t)","bac90555":"#stats\nresults = []\nrandom_actions = []","0c8cc1bc":"# initialize the agent to be train\nTestAgent = QLearningAgent(1)","165f76b0":"#initialize gym\ngymAgent = RandomTTTAgent(2)\ngym = GymTTT(gymAgent,False)\n","7489c998":"start = time.time()\ngames = 1000    #number of episodes\nfor i in range(games):\n    timeStep = gym.reset()\n    random_action_count = 0\n    while not timeStep.is_last():\n        policyStep = TestAgent.action(timeStep)\n        nexTimeStep = gym.step(policyStep.action)\n        \n        if (policyStep.info):\n            random_action_count+=1\n            \n        qtuple = {}\n        qtuple['cur'] = timeStep.observation\n        qtuple['act'] = policyStep.action[:2]\n        qtuple['rew'] = nexTimeStep.reward\n        qtuple['nex'] = nexTimeStep.observation\n        \n        if TestAgent.trainable:\n            TestAgent.updateActionValue(qtuple)\n\n        timeStep = nexTimeStep\n    \n    results.append(int(timeStep.reward))    #collect stats\n    random_actions.append(random_action_count)\n    \n    \nprint(\"Time taken\",time.time() - start)\nprint(\"Win Percentage: \",results[-games:].count(1)\/games)    #stats for last x games\nprint(\"Draw Percentage: \",results[-games:].count(0)\/games)\nprint(\"Loss Percentage: \",results[-games:].count(-1)\/games)","9ec686cf":"freq = 50    #bucket-size for plotting higher sizes give smoother curves\n\na,b = 0, len(results)    #range to plot; (change a to len(results)-x to plot only the last x)\n\nmetrics = collections.defaultdict(list)\n\nfor i in range(a, b, freq):\n    if i==0:\n        continue\n    \n    metrics['games'].append(i)\n    metrics['wins'].append(results[i-freq:i].count(1))\n    metrics['draws'].append(results[i-freq:i].count(0))\n    metrics['loses'].append(results[i-freq:i].count(-1))\n    metrics['win_pct'].append(results[i-freq:i].count(1)\/float(freq))\n\nmetrics['randomness'] = [sum(random_actions[i-freq:i]) for i in range(a, b,freq) if i!=0] #how many of the actions were just random(by a q agent)\n        \n\n\nplt.plot(metrics['games'], metrics['wins'], label='wins')\n# plt.plot(metrics['games'], metrics['win_pct'], label='wins_pct')\nplt.plot(metrics['games'], metrics['draws'], label='draws')\nplt.plot(metrics['games'], metrics['loses'], label='loses')\n# plt.plot(metrics['games'], metrics['randomness'], label='randomness')\nplt.legend(loc = 0)\nplt.show()","17716d09":"# Intro to game bots on Kaggle\n\n","3a7d4213":"An untrained Q-learning agent would pick random actions. With each game, it learns to differentiate good moves from bad ones. So let us train our agent.<br>\nAs we train, it is imperitive to collect each game's result. Its the best metric to track the training.<br>\nAs I had mentioned earlier, the q-learning agent makes a random choice when it hasn't seen the current state before, hence measuring the number of random actions vs choosen actions could also serve as a metric to track the training levels.<br>\nWe can choose any of the agents(including itself) as the opponent.<br>\n\n\nNote: The agent must be trained as both First Player and Second Player","a35ea4d5":"You may have noticed, minimax algorithm builds a tree in each step even though they are just subtrees of the first computed tree. This excess computation can be avoided by storing the obtained rewards which significantly reduces the compute time of the algorithm.","fef5539b":"This notebook is a simple intro for those who are looking to get started with writing bots for two player game setups on kaggle like this one: [ConnectX](https:\/\/www.kaggle.com\/c\/connectx)\nCompetitions like these seemed intimidating, so I started with writing bots for a simpler game. Hope this helps those who are stuck on the first step.\n\nIn this notebook I have touched upon two methods. \nFirst the minimax algo, which is a traditional backtracking algorithm. Then the Q-learning algo which is one of the most popular Reinforcement learning methods.\n\n#### NoteBook Flow:\n* First we look at Environment and GYM classes\n* Then we look at four agents: random, minimax, q-learning, human\n* Then we mix and match various agents and evaluate their win rates.","32f9bf03":"## MiniMax\nHere we implement an agent that chooses the best action by playing out all possible scenarios.<br>\nFor a theoretical understanding, check out [Minimax Algorithm](https:\/\/www.geeksforgeeks.org\/minimax-algorithm-in-game-theory-set-1-introduction\/)<br>\nIn Brief,<br>\nIt builds a tree of all possible actions from the current state and picks the one that fetches the highest reward. To calculate the reward, it simulates the game and plays along all possible sequences. In the simulation, it tries to maximise the reward obatained in its turn and minimise the reward obtained in the opponent's turn.","26da3a49":"# Agents\n\nAn agent needs to evaluate the current observation and choose a legal action\nFor uniformity, in this notebook, all agents have a function called action which accepts a timestep as argument and returns a PolicyStep.<br>\nFor more info on the datatypes check out:<br>\n* [tf_agents.trajectories.TimeStep](https:\/\/www.tensorflow.org\/agents\/api_docs\/python\/tf_agents\/trajectories\/TimeStep)<br>\n* [tf_agents.trajectories.PolicyStep](https:\/\/www.tensorflow.org\/agents\/api_docs\/python\/tf_agents\/trajectories\/PolicyStep)<br>\n\nAn agent needs the player symbol as argument. For simplicity we use 1 and 2 to represent the First and the Second player<br>\n\n\nTo start with a simple agent, we can have it choose randomly from a list of empty slots\n\n\n","35671801":"# Results\nPlotting the metrics","335011cb":"In order to evaluate an agent, its best to play against it yourself. So use a Human agent if you want to  play against one of the other agents.","92ced428":"Let us test out the minimax agent against the random agent<br>","c1679063":"## Concepts explored:\n* MiniMax Algorithm\n* Q-learning","ac96b884":"# GYM\n\nGym is a wrapper around the environment. This is necessary in a 2 player environment to simulate the second player. It takes an agent as argument which will be used to choose the opponent's action.<br>\nAny of the agents can be plugged into the gym to train against.<br>\n\nGym encapsulates the env object and handles all interactions between the player agent and the env appropriately.<br> \n\nUsage is described in the class<br>\n\n\nPS: in competitions, this is unnecessary","9c375e26":"## Q-Learning\n\nHere we implement an agent which uses Q-learning to choose actions. Its a classic reinforcement technique that determines the action based on its past experiences.<br>\nFor a structured theoretical understanding please refer to [Suton and Barto](https:\/\/web.stanford.edu\/class\/psych209\/Readings\/SuttonBartoIPRLBook2ndEd.pdf).<br>\nIn brief, it relies on its experience to determine how good is an action at a certain stage of the game, which is quantified by reward. For each step of the game, we store the reward obtained and this value is corrected each time we visit this state using the q-learning formula:<br>\n![Q-Learning-Formula](https:\/\/cdn.analyticsvidhya.com\/wp-content\/uploads\/2019\/04\/1_lTVHyzT3d26Bd_znaKaylQ.png)<br>\nThe next time we are in the same state, we pick the action which has the highest reward associated with it.\nHere we use a dictionary to store and update these action values. Since this involves training, I have already attached a trained dictionary which can optionally be loaded by setting the load_av argument to true.<br>","ec740307":"# Environment\nEnvironment is the setup in which your agent will perform actions. Here the environment will be a tictactoe board which records the actions that the agent took. We must also make sure that the action taken was legal.<br>\ntf-agents library defines a template that aids us design an environment. We inherit a class call PyEnvironment and override two functions: **_reset** and **_step**<br>\nFor more details: [tf-environments](https:\/\/www.tensorflow.org\/agents\/tutorials\/2_environments_tutorial)<br>\n* Action spec: The list of possible actions an agent can take (here: {x,y} co-ordinate and the player's symbol- '1' for the first player and '2' for the second one)\n* Observation spec: How the environment is defined. (here a 3x3 board, initialized to 0 to indicate vacant spots)<br> \n\n\n**Overriden functions:**\n* _reset: defines how the board should look when a game begins\n* _step: decides how the action affects the board\n\nPS: In competitions, this is unnecessary as kaggle provides the environment."}}