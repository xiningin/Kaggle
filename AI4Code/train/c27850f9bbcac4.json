{"cell_type":{"6c38d8d4":"code","f34383a6":"markdown","1c7d3327":"markdown","d1866a2a":"markdown","92e0a22c":"markdown","1efd2c07":"markdown","6574991c":"markdown","49831d80":"markdown","d4d1bfee":"markdown","e7529972":"markdown","5a932c2f":"markdown","72b3c352":"markdown","f87c1725":"markdown","aa24cfb2":"markdown","db582ff6":"markdown","2cec003c":"markdown"},"source":{"6c38d8d4":"###construct a non linear decision boundary####\nimport numpy as np \nimport sklearn \nimport matplotlib.pyplot as plt\nfrom sklearn.datasets.samples_generator import make_circles\nX,y = make_circles(90, factor=0.2, noise=0.1) \nplt.scatter(X[:,0],X[:,1], c=y, s=50, cmap='seismic')\nplt.show()","f34383a6":"We can rewrite our equation as $g(x)=w_0^Tx+b_0$ and, the distance of x from plane will be normal projection of x on plane and let r be the normal distance between point and plane , by the diagram we can write x as \n$x=x_p+r \\times \\frac{w_0} {||w_0||}$.which means that value of x will be value of x at any point plus distance of vector from that plane divided by norm the vector. Now r be will be postive and negative depending upon location of our point on either side of plane\nnow $g(x)=w_0^T x+b_0=r\\times||w_0||$ or $r=\\frac{g(x)}{||w_0||}$","1c7d3327":"# Kernel Trick","d1866a2a":"### Changing the one reference point to origin","92e0a22c":"# Mathematics of Support Vector Machines","1efd2c07":"Now you try to maximize the width of the raod so that weeds have good seperation from roses.But you have to also make sure none of weed or rose comes on road, this is brief idea of SVM , to find the decision boundary that seperates two classes with maximum margin, if make roses and weed infinetely deep inside earth, then this road becomes a plane instead of line , these decision boundaries are called hyperplanes. Now the Red rose and green weed that are two boundary conditions deciding the width of our road are called Support Vectors","6574991c":"## Topics covered in this notebook\n* The SVC Algorithm\n* The Lagrange Multiplier\n* Kernel Trick","49831d80":"## Types of kernel\n\n$K(xi,xj) = (xi.xj +c)^d$- This is example of polynomial kernel where dot product of original vector space and multiplied by transformed kernel is raised to some power n>1 if n=1 this becomes Linear Kernel.\n\nThe second most important Kernel is Guassian RBF kernel:\n\n$K(xi,xj) = \\exp ^ -\\gamma(||xi \u2013 xj||)^2$ which takes exponential of euclidean distance between two vectors","d4d1bfee":"When we have a problem where two classes are not linearly seperable, then we use kernel trick to transform lower dimensional data into higher dimensions and vice versa. We can reformulate our Objective function as $Q(\\alpha)=\\sum_{n=1}^{n}\\alpha_i-\\frac{1}{2}\\sum_{n=1}^{n}\\sum_{n=1}^{n}\\alpha_i\\alpha_jd_id_jx_i^Tx_j$. If we see the maximization of Q function only depends on the dot product of two vectors. \n\nThe idea is mapping the non-linear separable data-set into a higher dimensional space where we can find a hyperplane that can separate the samples. We may never need to know the two vectors exactly and only rely on the dot product of original vector into transformed vetor $K(x,x_t)=\\phi^T(x)\\phi(x_t)$","e7529972":"Let's say that our roses are represented by d = 1 and weed as d=-1 and on the road we do not have any of weed and roses so d=0\n\nLets donate them with square and circles respectively also let us take distance from our dotted decision boundary and call that rho $\\rho $ Notice the decision boundary is not two lines but dotted middle lines.\n\n![svm2.png](attachment:svm2.png)\n\nLet x be our inputs and corresponsing to x we get values of decision 1,0,-1. Let w be the weight vector for input values x that is adjustable and b be the bias term , which is noise in the data\n\n* The decision surface euqation will look like this \n\n$w^Tx + b = 0$\n\n* If we are one side of road\n$w^T+b >=0 $ for d=1 or $w^T+b <0$ for d=-1\n\nOur goal is maximise rho , let us assume we found a weightage w and value of x where we have the width of road maximised, \n\nLet us denote this by $w_0$ for weigth and $x_0$ for x thus our equation becomes $w_0^Tx+b_0=0$","5a932c2f":"# Lagrangian Multipliers\n\nThe general idea is to transform a constrained optimization objective into an unconstrained one, by moving the constraints into the objective function. The support vector machine problem falls under quadratic optimization problem.\n \n## The conditions for the problem\n* We have to minimize weight vector w so that we have largest seperation between support vectors.$\\psi(w)=\\frac {1}{2} \\times w^Tw$ . This is transformed formula of above 2\/||w0||.\n* the original equation is also a constraint $(w^Tx+b)>=1$\n\n## Now we construct the Lagrange function\n$J(w,b,\\alpha)=\\frac{1}{2}w^Tw-\\sum_{n=1}^{n}\\alpha\\times[(w^Tx+b)-1]$\n\nThe equation consists of two parts the first is the convex function of weight vector w. The part consists of Lagrange multiplier alpha. Alpha is a non negative number,  The saddle point has to minimized with respect to w and b and maximized w.r.t alpha.\n\nNow there are two conditions for minimzing:\n1 $\\frac{\\partial J(w,b,\\alpha)}{\\partial w}=0$\n2 $\\frac{\\partial J(w,b,\\alpha)}{\\partial b}=0$\n3 $\\frac{\\partial J(w,b,\\alpha)}{\\partial \\alpha}=0$\n\nFinally we can solve for equation 1 as $w=\\sum_{n=1}^{n}\\alpha_i d_i x_i$ and equation 2 becomes $\\sum_{n=1}^{n}\\alpha_i d_i=0$\n\nWe also get from 3rd equation $\\alpha\\times[(w^Tx+b)-1]$ =0 , we have two options either alpha the lagrange multiplier is zero or the second equation the $[(w^Tx+b)-1]$ =0 is zero. Now we have have one It is also important to note that for all the constraints that we are not satisfied as equalities, the corresponding mutliplier $\\alpha$ must be zero. Only those multipliers thats satisfy the above condition.","72b3c352":"Now x is at origin hence hence $r=\\frac{g(x)}{||w_0||}$ will now become $r=\\frac{b_0}{||w_0||}$ this is the optimal plane which seperates a point from origion and another point x . We earlier said that $w^T+b >=0 $ for d=1 or $w^T+b <0$ for d=-1 now we ignore the points on the plane and focus on exactly two points where we will have the value of value of our function as 1 or -1\n\nThus our support vectors which touches the two bounaries of road becomes \n$w_0^T.x_p+b=1$ for one side and $w_0^T.x_n+b=-1$ for the other side. Now if we substract these two vectors we get $w_0^T.x_p-w_0^T.x_n=2$ . \n\nRemember we denoted $\\rho$ as distance of one support vector to plane hence $\\rho\\times 2$= r \nthe width of road which is $x_p-x_n$ and dot product of weight vector, hence $w_0.r=2$ hence $r=\\frac{2}{||w_0||}$\n\nMaximizing the margin of separation between binary classes is equivalent to minimizing the\nnorm of the weight vector w.","f87c1725":"## Lets look at mathematical expression defining the conditions\n\nThis notebook is inspired by MIT course on SVM, the youtube link is here:\nhttps:\/\/www.youtube.com\/watch?v=_PwhiWxHK8o","aa24cfb2":"# The Algorithm\n\nI will use very basic level examples starting with 2D space to explain this:\n\nLets say that you are walking on the road that is very very long and deosnt have curves , on one side of the road you have bad weed growing and the other side there are roses. The weed is bad species and destroys the roses, so the road is the barrier that stops weed from going on roses side.\n![svm.png](attachment:svm.png)\n","db582ff6":"The shortest distance between a point a plane is given by the orthogonal projection of a line into another line, i.e., projecting a line from the origin into the plane into the normal vector of the plane. The formula for this orthogonal projection uses the dot product.\n\n![image.png](attachment:image.png)![svm3.png](attachment:svm3.png)\n\n","2cec003c":"![svm4.PNG](attachment:svm4.PNG)"}}