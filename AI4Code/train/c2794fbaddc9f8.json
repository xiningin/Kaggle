{"cell_type":{"600d5658":"code","6bf1f962":"code","d2f8e4da":"code","f899050b":"code","96b30925":"code","8803c62f":"code","b51d1e70":"code","758cb8b6":"code","829e4bb4":"code","c2330dd4":"code","5a1f98b8":"code","0a95797b":"code","ddb70162":"code","35d57f42":"code","be3d94c1":"code","25f87e49":"code","05ce78e0":"code","a2d95380":"code","04c7a446":"code","277a352d":"code","d8db305d":"code","770718cc":"code","e0cdb90c":"code","6a11f981":"code","be2bbc04":"code","94ba1432":"code","c2563642":"code","421c2dba":"code","dcaf78d8":"code","4d5dac88":"code","a8f877a6":"code","05b363ab":"code","92b1b683":"code","2952fcaa":"code","1021e997":"code","78636e9a":"code","377fb97c":"code","9aa2c53f":"code","e52f459b":"code","50f7ee05":"code","399bcfb1":"code","792b65af":"code","28cd9ff5":"code","0ecb432c":"code","3dc38fe8":"code","8494f332":"code","e20ebdd4":"code","2c834497":"code","3b57f03c":"code","78d2a499":"code","b30ba8f7":"code","5eee66e7":"code","7621d015":"code","28cbe86b":"code","40b3347d":"code","7a75392c":"code","2271ca6c":"code","86ab6f33":"code","ff2a56d4":"code","632c71fa":"code","ac4806f7":"code","e9053286":"code","8ef36fc5":"code","9014d1bf":"code","7bd34fb3":"code","512b1ade":"code","d4c10c07":"code","58c6fba0":"code","08a88f8f":"markdown","042d3001":"markdown","79ad4d57":"markdown","ea39112d":"markdown","7df9f150":"markdown","64e70f47":"markdown","b98d298a":"markdown","c2b74e74":"markdown","4d3a2439":"markdown","9fc411da":"markdown","e46fe839":"markdown","9c049dd4":"markdown","357f0269":"markdown","60657465":"markdown","9b45859f":"markdown"},"source":{"600d5658":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","6bf1f962":"import matplotlib .pyplot as plt\nimport seaborn as sns","d2f8e4da":"train_data = pd.read_excel(\"..\/input\/flight-fare-prediction-mh\/Data_Train.xlsx\")\npd.set_option('display.max_columns', None)\ntrain_data.head()\n","f899050b":"train_data.info()\n","96b30925":"train_data[\"Duration\"].value_counts()\n","8803c62f":"train_data.dropna(inplace = True)\n","b51d1e70":"train_data.isnull().sum()\n","758cb8b6":"train_data[\"Journey_day\"] = pd.to_datetime(train_data.Date_of_Journey, format=\"%d\/%m\/%Y\").dt.day\n","829e4bb4":"train_data[\"Journey_month\"] = pd.to_datetime(train_data[\"Date_of_Journey\"], format = \"%d\/%m\/%Y\").dt.month\n","c2330dd4":"train_data.head()\n","5a1f98b8":"# Since we have converted Date_of_Journey column into integers, Now we can drop as it is of no use.\n\ntrain_data.drop([\"Date_of_Journey\"], axis = 1, inplace = True)","0a95797b":"# Departure time is when a plane leaves the gate. \n# Similar to Date_of_Journey we can extract values from Dep_Time\n\n# Extracting Hours\ntrain_data[\"Dep_hour\"] = pd.to_datetime(train_data[\"Dep_Time\"]).dt.hour\n\n# Extracting Minutes\ntrain_data[\"Dep_min\"] = pd.to_datetime(train_data[\"Dep_Time\"]).dt.minute\n\n# Now we can drop Dep_Time as it is of no use\ntrain_data.drop([\"Dep_Time\"], axis = 1, inplace = True)","ddb70162":"train_data.head()","35d57f42":"# Arrival time is when the plane pulls up to the gate.\n# Similar to Date_of_Journey we can extract values from Arrival_Time\n\n# Extracting Hours\ntrain_data[\"Arrival_hour\"] = pd.to_datetime(train_data.Arrival_Time).dt.hour\n\n# Extracting Minutes\ntrain_data[\"Arrival_min\"] = pd.to_datetime(train_data.Arrival_Time).dt.minute\n\n# Now we can drop Arrival_Time as it is of no use\ntrain_data.drop([\"Arrival_Time\"], axis = 1, inplace = True)","be3d94c1":"train_data.head()","25f87e49":"# Time taken by plane to reach destination is called Duration\n# It is the differnce betwwen Departure Time and Arrival time\n\n\n# Assigning and converting Duration column into list\nduration = list(train_data[\"Duration\"])\n\nfor i in range(len(duration)):\n    if len(duration[i].split()) != 2:    # Check if duration contains only hour or mins\n        if \"h\" in duration[i]:\n            duration[i] = duration[i].strip() + \" 0m\"   # Adds 0 minute\n        else:\n            duration[i] = \"0h \" + duration[i]           # Adds 0 hour\n\nduration_hours = []\nduration_mins = []\nfor i in range(len(duration)):\n    duration_hours.append(int(duration[i].split(sep = \"h\")[0]))    # Extract hours from duration\n    duration_mins.append(int(duration[i].split(sep = \"m\")[0].split()[-1]))   # Extracts only minutes from duration","05ce78e0":"# Adding duration_hours and duration_mins list to train_data dataframe\n\ntrain_data[\"Duration_hours\"] = duration_hours\ntrain_data[\"Duration_mins\"] = duration_mins","a2d95380":"train_data.drop([\"Duration\"], axis = 1, inplace = True)","04c7a446":"train_data.head()","277a352d":"train_data[\"Airline\"].value_counts()\n","d8db305d":"# From graph we can see that Jet Airways Business have the highest Price.\n# Apart from the first Airline almost all are having similar median\n\n# Airline vs Price\nsns.catplot(y = \"Price\", x = \"Airline\", data = train_data.sort_values(\"Price\", ascending = False), kind=\"boxen\", height = 6, aspect = 3)\nplt.show()","770718cc":"# As Airline is Nominal Categorical data we will perform OneHotEncoding\n\nAirline = train_data[[\"Airline\"]]\n\nAirline = pd.get_dummies(Airline, drop_first= True)\n\nAirline.head()","e0cdb90c":"train_data[\"Source\"].value_counts()\n","6a11f981":"# Source vs Price\n\nsns.catplot(y = \"Price\", x = \"Source\", data = train_data.sort_values(\"Price\", ascending = False), kind=\"boxen\", height = 4, aspect = 3)\nplt.show()\n","be2bbc04":"# As Source is Nominal Categorical data we will perform OneHotEncoding\n\nSource = train_data[[\"Source\"]]\n\nSource = pd.get_dummies(Source, drop_first= True)\n\nSource.head()","94ba1432":"train_data[\"Destination\"].value_counts()","c2563642":"# As Destination is Nominal Categorical data we will perform OneHotEncoding\n\nDestination = train_data[[\"Destination\"]]\n\nDestination = pd.get_dummies(Destination, drop_first = True)\n\nDestination.head()","421c2dba":"train_data[\"Route\"]","dcaf78d8":"# Additional_Info contains almost 80% no_info\n# Route and Total_Stops are related to each other\n\ntrain_data.drop([\"Route\", \"Additional_Info\"], axis = 1, inplace = True)","4d5dac88":"train_data[\"Total_Stops\"].value_counts()","a8f877a6":"# As this is case of Ordinal Categorical type we perform LabelEncoder\n# Here Values are assigned with corresponding keys\n\ntrain_data.replace({\"non-stop\": 0, \"1 stop\": 1, \"2 stops\": 2, \"3 stops\": 3, \"4 stops\": 4}, inplace = True)\n","05b363ab":"train_data.head()\n","92b1b683":"# Concatenate dataframe --> train_data + Airline + Source + Destination\n\ndata_train = pd.concat([train_data, Airline, Source, Destination], axis = 1)","2952fcaa":"data_train.head()\n","1021e997":"data_train.drop([\"Airline\", \"Source\", \"Destination\"], axis = 1, inplace = True)\n","78636e9a":"data_train.head()\n","377fb97c":"data_train.shape\n","9aa2c53f":"test_data = pd.read_excel(\"..\/input\/flight-fare-prediction-mh\/Test_set.xlsx\")","e52f459b":"test_data.head()\n","50f7ee05":"# Preprocessing same as training data that we have done\n\nprint(\"Test data Info\")\nprint(\"-\"*75)\nprint(test_data.info())\n\nprint()\nprint()\n\nprint(\"Null values :\")\nprint(\"-\"*75)\ntest_data.dropna(inplace = True)\nprint(test_data.isnull().sum())\n\n# EDA\n\n# Date_of_Journey\ntest_data[\"Journey_day\"] = pd.to_datetime(test_data.Date_of_Journey, format=\"%d\/%m\/%Y\").dt.day\ntest_data[\"Journey_month\"] = pd.to_datetime(test_data[\"Date_of_Journey\"], format = \"%d\/%m\/%Y\").dt.month\ntest_data.drop([\"Date_of_Journey\"], axis = 1, inplace = True)\n\n# Dep_Time\ntest_data[\"Dep_hour\"] = pd.to_datetime(test_data[\"Dep_Time\"]).dt.hour\ntest_data[\"Dep_min\"] = pd.to_datetime(test_data[\"Dep_Time\"]).dt.minute\ntest_data.drop([\"Dep_Time\"], axis = 1, inplace = True)\n\n# Arrival_Time\ntest_data[\"Arrival_hour\"] = pd.to_datetime(test_data.Arrival_Time).dt.hour\ntest_data[\"Arrival_min\"] = pd.to_datetime(test_data.Arrival_Time).dt.minute\ntest_data.drop([\"Arrival_Time\"], axis = 1, inplace = True)\n\n# Duration\nduration = list(test_data[\"Duration\"])\n\nfor i in range(len(duration)):\n    if len(duration[i].split()) != 2:    # Check if duration contains only hour or mins\n        if \"h\" in duration[i]:\n            duration[i] = duration[i].strip() + \" 0m\"   # Adds 0 minute\n        else:\n            duration[i] = \"0h \" + duration[i]           # Adds 0 hour\n\nduration_hours = []\nduration_mins = []\nfor i in range(len(duration)):\n    duration_hours.append(int(duration[i].split(sep = \"h\")[0]))    # Extract hours from duration\n    duration_mins.append(int(duration[i].split(sep = \"m\")[0].split()[-1]))   # Extracts only minutes from duration\n\n# Adding Duration column to test set\ntest_data[\"Duration_hours\"] = duration_hours\ntest_data[\"Duration_mins\"] = duration_mins\ntest_data.drop([\"Duration\"], axis = 1, inplace = True)\n\n\n# Categorical data\n\nprint(\"Airline\")\nprint(\"-\"*75)\nprint(test_data[\"Airline\"].value_counts())\nAirline = pd.get_dummies(test_data[\"Airline\"], drop_first= True)\n\nprint()\n\nprint(\"Source\")\nprint(\"-\"*75)\nprint(test_data[\"Source\"].value_counts())\nSource = pd.get_dummies(test_data[\"Source\"], drop_first= True)\n\nprint()\n\nprint(\"Destination\")\nprint(\"-\"*75)\nprint(test_data[\"Destination\"].value_counts())\nDestination = pd.get_dummies(test_data[\"Destination\"], drop_first = True)\n\n# Additional_Info contains almost 80% no_info\n# Route and Total_Stops are related to each other\ntest_data.drop([\"Route\", \"Additional_Info\"], axis = 1, inplace = True)\n\n# Replacing Total_Stops\ntest_data.replace({\"non-stop\": 0, \"1 stop\": 1, \"2 stops\": 2, \"3 stops\": 3, \"4 stops\": 4}, inplace = True)\n\n# Concatenate dataframe --> test_data + Airline + Source + Destination\ndata_test = pd.concat([test_data, Airline, Source, Destination], axis = 1)\n\ndata_test.drop([\"Airline\", \"Source\", \"Destination\"], axis = 1, inplace = True)\n\nprint()\nprint()\n\nprint(\"Shape of test data : \", data_test.shape)","399bcfb1":"X = data_train.loc[:, ['Total_Stops', 'Journey_day', 'Journey_month', 'Dep_hour',\n       'Dep_min', 'Arrival_hour', 'Arrival_min', 'Duration_hours',\n       'Duration_mins', 'Airline_Air India', 'Airline_GoAir', 'Airline_IndiGo',\n       'Airline_Jet Airways', 'Airline_Jet Airways Business',\n       'Airline_Multiple carriers',\n       'Airline_Multiple carriers Premium economy', 'Airline_SpiceJet',\n       'Airline_Trujet', 'Airline_Vistara', 'Airline_Vistara Premium economy',\n       'Source_Chennai', 'Source_Delhi', 'Source_Kolkata', 'Source_Mumbai',\n       'Destination_Cochin', 'Destination_Delhi', 'Destination_Hyderabad',\n       'Destination_Kolkata', 'Destination_New Delhi']]\nX.head()","792b65af":"y = data_train.iloc[:, 1]\ny.head()\n","28cd9ff5":"# Finds correlation between Independent and dependent attributes\n\nplt.figure(figsize = (18,18))\nsns.heatmap(train_data.corr(), annot = True, cmap = \"RdYlGn\")\n\nplt.show()","0ecb432c":"import sklearn ","3dc38fe8":"# Important feature using ExtraTreesRegressor\n\nfrom sklearn.ensemble import ExtraTreesRegressor\nselection = ExtraTreesRegressor()\nselection.fit(X, y)","8494f332":"print(selection.feature_importances_)\n","e20ebdd4":"#plot graph of feature importances for better visualization\n\nplt.figure(figsize = (12,8))\nfeat_importances = pd.Series(selection.feature_importances_, index=X.columns)\nfeat_importances.nlargest(20).plot(kind='barh')\nplt.show()\n","2c834497":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 42)","3b57f03c":"from sklearn.ensemble import RandomForestRegressor\nreg_rf = RandomForestRegressor()\nreg_rf.fit(X_train, y_train)","78d2a499":"y_pred = reg_rf.predict(X_test)\n","b30ba8f7":"reg_rf.score(X_train, y_train)\n","5eee66e7":"reg_rf.score(X_test, y_test)\n","7621d015":"sns.distplot(y_test-y_pred)\nplt.show()","28cbe86b":"plt.scatter(y_test, y_pred, alpha = 0.5)\nplt.xlabel(\"y_test\")\nplt.ylabel(\"y_pred\")\nplt.show()\n","40b3347d":"from sklearn import metrics\n","7a75392c":"print('MAE:', metrics.mean_absolute_error(y_test, y_pred))\nprint('MSE:', metrics.mean_squared_error(y_test, y_pred))\nprint('RMSE:', np.sqrt(metrics.mean_squared_error(y_test, y_pred)))","2271ca6c":"# RMSE\/(max(DV)-min(DV))\n\n2090.5509\/(max(y)-min(y))","86ab6f33":"metrics.r2_score(y_test, y_pred)","ff2a56d4":"from sklearn.model_selection import RandomizedSearchCV\n","632c71fa":"#Randomized Search CV\n\n# Number of trees in random forest\nn_estimators = [int(x) for x in np.linspace(start = 100, stop = 1200, num = 12)]\n# Number of features to consider at every split\nmax_features = ['auto', 'sqrt']\n# Maximum number of levels in tree\nmax_depth = [int(x) for x in np.linspace(5, 30, num = 6)]\n# Minimum number of samples required to split a node\nmin_samples_split = [2, 5, 10, 15, 100]\n# Minimum number of samples required at each leaf node\nmin_samples_leaf = [1, 2, 5, 10]","ac4806f7":"# Create the random grid\n\nrandom_grid = {'n_estimators': n_estimators,\n               'max_features': max_features,\n               'max_depth': max_depth,\n               'min_samples_split': min_samples_split,\n               'min_samples_leaf': min_samples_leaf}","e9053286":"# Random search of parameters, using 5 fold cross validation, \n# search across 100 different combinations\nrf_random = RandomizedSearchCV(estimator = reg_rf, param_distributions = random_grid,scoring='neg_mean_squared_error', n_iter = 10, cv = 5, verbose=2, random_state=42, n_jobs = 1)\n","8ef36fc5":"rf_random.fit(X_train,y_train)\n","9014d1bf":"rf_random.best_params_\n","7bd34fb3":"prediction = rf_random.predict(X_test)\n","512b1ade":"plt.figure(figsize = (8,8))\nsns.distplot(y_test-prediction)\nplt.show()\n","d4c10c07":"plt.figure(figsize = (8,8))\nplt.scatter(y_test, prediction, alpha = 0.5)\nplt.xlabel(\"y_test\")\nplt.ylabel(\"y_pred\")\nplt.show()","58c6fba0":"print('MAE:', metrics.mean_absolute_error(y_test, prediction))\nprint('MSE:', metrics.mean_squared_error(y_test, prediction))\nprint('RMSE:', np.sqrt(metrics.mean_squared_error(y_test, prediction)))","08a88f8f":"#  flight price prediction","042d3001":"1. * > Split dataset into train and test set in order to prediction w.r.t X_test\n2. * > If needed do scaling of data\n3. * > Scaling is not done in Random forest\n4. * > Import model\n5. * > Fit the data\n6. * > Predict w.r.t X_test\n7. * > In regression check RSME Score\n8. * > Plot graph","79ad4d57":"# test data","ea39112d":"thanku for reading this notebook hope you learn alot","7df9f150":"From description we can see that Date_of_Journey is a object data type,\\ Therefore, we have to convert this datatype into timestamp so as to use this column properly for prediction\n\nFor this we require pandas to_datetime to convert object data type to datetime dtype.\n\n**.dt.day method will extract only day of that date**\\ **.dt.month method will extract only month of that date*","64e70f47":"# importing the data set","b98d298a":"# EDA","c2b74e74":"* > Choose following method for hyperparameter tuning\n* > RandomizedSearchCV --> Fast\n* > GridSearchCV\n* > Assign hyperparameters in form of dictionery\n* > Fit the model\n* > Check best paramters and best score","4d3a2439":"# handling categorical data","9fc411da":"# hyperparameter tuning","e46fe839":"ne can find many ways to handle categorical data. Some of them categorical data are,\n\n**Nominal data** --> data are not in any order --> **OneHotEncoder** is used in this case\n**Ordinal data** --> data are in order --> **LabelEncoder** is used in this case","9c049dd4":"![title](https:\/\/www.usnews.com\/dims4\/USNEWS\/ed7b154\/2147483647\/crop\/2000x1334%2B0%2B1\/resize\/970x647\/quality\/85\/?url=http%3A%2F%2Fmedia.beam.usnews.com%2Fb7%2Ff4%2F847d373a4218bcbb9383ff501eaa%2F191004-flightapp-stock.jpg)","357f0269":"if you like the notebook please comment and please upvote this","60657465":"# save the model and reuse it again","9b45859f":"# fitting model using random forest"}}