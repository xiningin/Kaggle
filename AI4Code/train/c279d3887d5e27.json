{"cell_type":{"ea596ccc":"code","d393a15f":"code","22b5a16c":"code","e705c941":"code","f514f974":"code","7998a592":"code","787678ee":"code","104ce926":"code","eabec543":"code","940eef4d":"code","5806463b":"code","a9d28e1b":"code","8ec35149":"code","4182242e":"code","90de4b94":"code","216d7787":"code","e1c1a182":"code","1e4bbb85":"code","d40bf4d8":"code","35ba4f83":"code","878836d2":"code","cbb2815e":"code","dea93299":"code","b33192f1":"code","3b965c8f":"markdown","75d1b7d4":"markdown","d91c7066":"markdown","48435697":"markdown","12c64c9a":"markdown","ad356482":"markdown","48cf5dfa":"markdown","dd521fec":"markdown","9d09f9d9":"markdown","c1abf7ed":"markdown","48775764":"markdown","44aaea15":"markdown","8ea50542":"markdown","834c2154":"markdown","4ea17205":"markdown","044189ed":"markdown","d3451f82":"markdown","afa588ad":"markdown","f48cb477":"markdown","a3137649":"markdown","22406e16":"markdown"},"source":{"ea596ccc":"import os\nimport time\nimport numpy as np\nimport pandas as pd\nfrom matplotlib import pyplot as plt\nimport seaborn as sns\n\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import accuracy_score\n\n\nfrom skopt import BayesSearchCV","d393a15f":"for dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","22b5a16c":"train_data = pd.read_csv(\"\/kaggle\/input\/titanic\/train.csv\")\ntest_data = pd.read_csv(\"\/kaggle\/input\/titanic\/test.csv\")","e705c941":"train_data.head()","f514f974":"train_data.describe()","7998a592":"train_data.pivot_table(train_data, index=[\"Survived\"])","787678ee":"fig, axes = plt.subplots(1,2)\nfig.set_figwidth(25)\nfig.set_figheight(8)\n\ncheck_cols = [\"Age\", \"Fare\"]\nsns.set_style(\"dark\")\nfor i in range(len(check_cols)):\n    \n    sns.kdeplot(data=train_data.loc[train_data[\"Survived\"] == 1, check_cols[i]],\n                  ax=axes[i],\n                  label=\"Survived\",\n                  color='blue',\n                  shade=True)\n\n    sns.kdeplot(data=train_data.loc[train_data[\"Survived\"] == 0, check_cols[i]],\n                  ax=axes[i],\n                  label=\"Did not survive\",\n                  color='red',\n                  shade=True)\n\n    # plot vertical lines\n    axes[i].axvline(train_data.loc[train_data[\"Survived\"] == 1, check_cols[i]].median(),\n                   color='blue')\n\n    axes[i].axvline(train_data.loc[train_data[\"Survived\"] == 0, check_cols[i]].median(),\n                   color='red')\n    \n    # plot annotations of values corresponding to the vertical lines\n    axes[i].annotate(\"Median {} Survived: {}\".format(check_cols[i],\n                                                        train_data.loc[train_data[\"Survived\"] == 1, check_cols[i]].median()),\n                                                        xy=(0.45, 0.95),\n                                                        xycoords='axes fraction',\n                                                        fontsize=15)\n    \n    axes[i].annotate(\"Median {} Not Survived: {}\".format(check_cols[i],\n                                                         train_data.loc[train_data[\"Survived\"] == 0, check_cols[i]].median()),\n                                                         xy=(0.45, 0.90),\n                                                         xycoords='axes fraction',\n                                                         fontsize=15)\n    \n    axes[i].title.set_text(\"{} and Survival\".format(check_cols[i]))\n    axes[i].legend()\nplt.show()\nplt.close()","104ce926":"fig, axes = plt.subplots(1,2)\nfig.set_figwidth(15)\nfig.set_figheight(5)\nsns.countplot(x=\"Pclass\", hue=\"Survived\", data=train_data, palette=[\"#F34D4D\", \"#2B72D9\"], ax=axes[0])\naxes[0].set_xticks([1,2,3])\nsns.countplot(x=\"Sex\", hue=\"Survived\", data=train_data, palette=[\"#F34D4D\", \"#2B72D9\"], ax=axes[1])\naxes[0].set_title(\"Pclass and Survival\")\naxes[1].set_title(\"Sex and Survival\")\nplt.show()\nplt.close()","eabec543":"print(train_data.isnull().sum())","940eef4d":"sns.set(rc={\"figure.figsize\":(15,5)}) # set size of figure plotted\nsns.set_style(\"dark\")\nsns.histplot(train_data[\"Age\"], kde=True, bins=20, color=\"teal\")\nplt.axvline(train_data[\"Age\"].median(), c=\"red\", label=\"Median Age: {:.1f}\".format(train_data[\"Age\"].median()))\nplt.axvline(train_data[\"Age\"].mean(), c=\"blue\", label=\"Mean Age: {:.1f}\".format(train_data[\"Age\"].mean()))\nplt.legend()\nplt.suptitle(\"Age of Passengers: Right Skewed\", fontsize=20)\nplt.xlabel(\"Age\")\nplt.ylabel(\"Count\")\nplt.show()","5806463b":"train_data = train_data[train_data['Embarked'].notna()]\ntrain_data['Age'].fillna(train_data['Age'].median(), inplace=True)","a9d28e1b":"train_data.drop([\"PassengerId\", \"Name\", \"Ticket\", \"Cabin\"], axis=1, inplace=True)\ntrain_data.head(n=10)","8ec35149":"vals_sex = train_data[\"Sex\"].unique() # variations and NaN values have been checked and removed\nvals_embarked = train_data[\"Embarked\"].unique()\n\nfor v in vals_sex:\n    train_data[v] = (train_data[\"Sex\"] == v).astype(int)\n\nfor v in vals_embarked:\n    train_data[v] = (train_data[\"Embarked\"] == v).astype(int)\n\ntrain_data.drop(\"Sex\", axis=1, inplace=True)\ntrain_data.drop(\"Embarked\", axis=1, inplace=True)\n\ntrain_data.head(n=10)","4182242e":"clf = RandomForestClassifier(random_state=123)\n\nparams = {\n    \"n_estimators\": (10, 200),\n    \"max_depth\": (4, 15),\n    \"min_samples_split\": (2, 10),\n    \"max_features\": (0.3, 1.0)\n}\n\nopt = BayesSearchCV(clf,\n                    params,\n                    cv=10,\n                    random_state=123,\n                    verbose=0)","90de4b94":"X_train, X_test, y_train, y_test = train_test_split(train_data.iloc[:, 1:],\n                                                    train_data.iloc[:,0],\n                                                    train_size=0.8,\n                                                    random_state=123)\nprint(X_train.shape)\nprint(y_train.shape)","216d7787":"time_start = time.time()\n\n_ = opt.fit(X_train, y_train)\n\ntime_end = time.time()\n\nprint(\"Optimisation Time: {}\".format(time_end - time_start))","e1c1a182":"print(opt.score(X_test, y_test))","1e4bbb85":"mat = confusion_matrix(opt.predict(X_test), y_test)\n\nplt.figure(figsize = (6,4))\nsns.heatmap(mat, annot=True)","d40bf4d8":"for k in opt.best_params_:\n    print(k, \":\", opt.best_params_[k])","35ba4f83":"plt.figure(figsize=(8,5))\nplt.barh(train_data.columns[1:], opt.best_estimator_.feature_importances_)\nplt.suptitle(\"Feature Importance\", size=20)\nplt.title(\"Shows the importance of features with optimised hyper-parameters\")\nplt.show()","878836d2":"check_cols = ['Pclass', 'Sex', 'Embarked']\n\ntest_data = test_data[test_data['Embarked'].notna()]\ntest_data['Age'].fillna(test_data['Age'].mean(), inplace=True)\ntest_data['Fare'].fillna(test_data['Fare'].median(), inplace=True)\n\ntest_id = test_data[\"PassengerId\"].copy()\n\ntest_data.drop([\"PassengerId\", \"Name\", \"Ticket\", \"Cabin\"], axis=1, inplace=True)\n\nvals_sex = test_data[\"Sex\"].unique() # variations and NaN values have been checked and removed\nvals_embarked = test_data[\"Embarked\"].unique()\n\nfor v in vals_sex:\n    test_data[v] = (test_data[\"Sex\"] == v).astype(int)\n\nfor v in vals_embarked:\n    test_data[v] = (test_data[\"Embarked\"] == v).astype(int)\n\ntest_data.drop(\"Sex\", axis=1, inplace=True)\ntest_data.drop(\"Embarked\", axis=1, inplace=True)\n\ntest_data.head(n=10)","cbb2815e":"clf = RandomForestClassifier()\nclf.set_params(**opt.best_params_)","dea93299":"clf.fit(train_data.iloc[:, 1:], train_data.iloc[:, 0]) # X_train, y_train\npred = clf.predict(test_data)","b33192f1":"submission = pd.DataFrame(test_id)\nsubmission[\"Survived\"] = pred\n\nsubmission.to_csv(\"submission.csv\", index=False)","3b965c8f":"Creating a train\/test split to gain an understanding of how well the classifier performs. The entire training dataset will be used for the final submission to Kaggle","75d1b7d4":"<a class=\"anchor\" id=\"model_train\"><\/a>\n# <p style=\"background-color:#0d101c;font-family:arial;color:#ffffff;font-size:150%;text-align:center;border-radius:20px;\">Model Training<\/p>\n\nFor this problem a random forest is going to be used. Hyper-parameter tuning will be carried out","d91c7066":"<b>How does Age and Fare correlate with survivability<\/b>\n\nBelow, we can see that there was not a large disparity of the survival rate of different ages. The median age of passengers that survived and those that did not were the same. There was a difference spotted with the fare price, where those who paid more tended to be more likely to survive.","48435697":"Print confusion-matrix to better understand the prediction results. <br>\n\nThe left to right diagonal shows the correct predictions. The rest show incorrect predictions (what it should be versus what was predicted). All the numbers add up to the total number of observations in the test dataset.","12c64c9a":"<a class=\"anchor\" id=\"data_transformation\"><\/a>\n# <p style=\"background-color:#0d101c;font-family:arial;color:#ffffff;font-size:150%;text-align:center;border-radius:20px;\">Data Transformation<\/p>\n# Data Transformation\n\n## Category Encoding\n\nUsing string data with a Random Forest classifier produces the ValueError: 'could not convert string to float'. This can be resolved by encoding the different categories. One way of doing this is assigning each category value an integer value, and incrementing by 1 until all categorical values are accounted for. An example with our data: <br>\n\n*'male'* -> 1 <br>\n*'female'* -> 2 <br>\n\nThis solution, however, only works with ordinal data. As these values are not ordinal\n\n### Solution\nA different approach is needed, where a new column for each value is created, and a 1 or 0 is assigned to its relevant attribute. Example:\n\n| Survived | Pclass| male | female |\n| --- | --- | --- | --- |\n| 0 | 3 | 1 | 0 |\n| 1 | 1 | 0 | 1 |\n| 1 | 3 | 0 | 1 |\n\nThis will also need to be done with the *'Embarked'* attribute. *'Pclass'* is left alone, because this is nominal data. ","ad356482":"<b>How does Pclass and Sex correlate with survivability<\/b>\n\nWe can see that those of a lower socio-economic status (Pclass) were more likely to have died than those of a higher class. This could potentially caused by people of higher class being let on life-boats, and\/or people of higher socio-economic class were able to spend more. It was shown earlier that those who paid a higher fare were also more likely to have survived than those who paid a lower fare.\n\n<b>It is clear that *'Age'*, *'Fare'*, *'Pclass'* and *'Sex'* are important attributes to consider when deciding on whether a passenger was likely to have survived or not <\/b>","48cf5dfa":"Optimising the hyper-parameters of the RandomForestClassifier.\n\nA warning is produced when running this. It is due to the optimiser converging on a set of points. <br>\n\n*\"**UserWarning**: The objective has been evaluated at this point before.\n  warnings.warn(\"The objective has been evaluated \"\"*","dd521fec":"<a class=\"anchor\" id=\"data_cleaning\"><\/a>\n# <p style=\"background-color:#0d101c;font-family:arial;color:#ffffff;font-size:150%;text-align:center;border-radius:20px;\">Data Cleaning<\/p>\n\nIn this section, the data will be tidied up by removing data not necessary, and attempting to deal with missing data values. After viewing the training data, it is clear that there are missing values, and attributes with awkward values that may not be useful when creating the classification model.\n\n## Dealing With Missing Values\n\nThere are several approaches one can take when dealing with missing values. A popular approach is simply deleting the observation containing missing values in its attributes. From previous observations, it appears that the attributes that are useful that contain missing data are 'Age' and 'Embarked'.\n\n**Age**: <br>\nThe approach taken to deal with missing values was to replace them with the median of all the age values present\n\n<b>Why the median?<\/b>\n\nBelow, a histrogram is plotted showing the distribution of ages of passengers. From the graph, it can be seen that the data is right-skewed, meaning that the distribution has a long right tail. In this case, it is better to use the median of the data over the mean, as the mean is affected more by extreme\/outlier data, or when data is skewed. Below, the mean age is higher than the median age, as can be seen with the 2 plotted vertical lines. Thus, the missing values will be replaced with the median of the values in the *'Age'* column.\n\n**Embarked**: <br>\nThe approach taken to deal with missing values was to delete entire observations where missing values were present in the 'Embarked' attribute\n","9d09f9d9":"<a class=\"anchor\" id=\"create_submission_csv\"><\/a>\n## Create sumbission csv","c1abf7ed":"<a class='anchor' id='clean_test_data'><\/a>\n## Cleaning and Processing Test Data\n\nThe testing dataset has to be prepared to resemble the training dataset in terms of structure, and deal with unexpected results.\n\n***All steps taken with checking, data-cleaning and data-transforming on the training data are repeated below on the test data***","48775764":"Fit RandomForestClassifier and produce predictions","44aaea15":"<a class=\"anchor\" id=\"kaggle_sub\"><\/a>\n# <p style=\"background-color:#0d101c;font-family:arial;color:#ffffff;font-size:150%;text-align:center;border-radius:20px;\">Kaggle Submission<\/p>","8ea50542":"<a class=\"anchor\" id=\"data_exploration\"><\/a>\n# <p style=\"background-color:#0d101c;font-family:arial;color:#ffffff;font-size:150%;text-align:center;border-radius:20px;\">Data Exploration<\/p>\n\nIn this section, the data that is being used is loaded and explored. This will entail gaining an understanding of the different attributes and their datatypes. Checks will be made to uncover any irregularities with data entries for attributes that are clearly categorical, and missing data.","834c2154":"Get result of predictions of optimised RandomForestClassifier on the test set","4ea17205":"## Removing Attributes\nThere are some attributes that do not appear to be useful when constructing models to make predictions. These columns were identified as \"PassengerId\", \"Name\", \"Ticket\" and \"Cabin\". They are to be removed from the final training dataset","044189ed":"Create RandomForestClassifier using optimal hyper-parameters","d3451f82":"<a class=\"anchor\" id=\"load_data\"><\/a>\n# <p style=\"background-color:#0d101c;font-family:arial;color:#ffffff;font-size:150%;text-align:center;border-radius:20px;\">Load Data<\/p>","afa588ad":"<a class=\"anchor\" id=\"import_modules\"><\/a>\n# <p style=\"background-color:#0d101c;font-family:arial;color:#ffffff;font-size:150%;text-align:center;border-radius:20px;\">Import Modules<\/p>\n","f48cb477":"Create submission file (.csv)","a3137649":"<img src=\"https:\/\/github.com\/datastrider\/titanic_svm\/blob\/main\/titanic_comp_pic_rf.jpg?raw=true\" ><\/img>\n# <p style=\"background-color:#0d101c;font-family:arial;color:#ffffff;font-size:150%;text-align:center;border-radius:20px;\">Table of Contents<\/p>\n1. [Import Modules](#import_modules)\n2. [Load Data](#load_data)\n3. [Data Exploration](#data_exploration)<br>\n4. [Data Cleaning](#data_cleaning)<br>\n5. [Data Transformation](#data_transformation)\n5. [Model Training](#model_train)\n6. [Kaggle Submission](#kaggle_sub)<br>\n    6.1 [Cleaning and Processing Test Data](#clean_test_data)<br>\n    6.2 [Create sumbission csv](#create_submission_csv)<br>","22406e16":"View best parameters after optimisation"}}