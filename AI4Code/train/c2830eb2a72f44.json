{"cell_type":{"1bb399c0":"code","52906527":"code","9d5829af":"code","b7ef93cc":"code","da1c7177":"code","29f7d544":"code","9f3d5b11":"code","3a550db7":"code","4b5175cd":"code","9a48675e":"code","d42e890c":"code","b9870709":"code","4ccfda0a":"code","029db5a5":"code","0be0c192":"code","4f0d5fe6":"code","237d22d9":"code","bf2733c2":"code","06177cfe":"code","46102304":"code","22725d5d":"code","d7b64acb":"code","eb0d97f6":"code","e0399f05":"code","380de57f":"code","51880aca":"code","abffdb29":"code","71756aae":"code","b5a3d87e":"code","5040286c":"code","bc9a780c":"code","956ae046":"code","c1430aac":"code","2ef4238d":"code","8d1629b0":"code","93282c36":"code","b6592937":"code","8cae3f09":"code","02096087":"code","b713dbc5":"code","1aa99c71":"code","bb85c63e":"code","fc8e5bc1":"code","671d3288":"code","a5833bd6":"code","6daa2eb5":"code","50bc2b6c":"code","80caae72":"code","8c41035d":"markdown"},"source":{"1bb399c0":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","52906527":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\ndf = pd.read_csv(\"\/kaggle\/input\/Kannada-MNIST\/train.csv\")\ndf","9d5829af":"# Save the label column in variable l\nl = df[\"label\"]\nl","b7ef93cc":"d = df.drop(\"label\",axis=1)\nd","da1c7177":"print(d.shape)\nprint(l.shape)","29f7d544":"# display or plot a number \nplt.figure(figsize=(10,10))\nidx=3502\n\ngrid_data = d.iloc[idx].as_matrix().reshape(28,28)\nplt.imshow(grid_data,interpolation=None,cmap=\"gray\")\n\nplt.show()\n\nprint(l[idx])","9f3d5b11":"d = df.drop(\"label\",axis=1)\nd","3a550db7":"l = df[\"label\"]\nl","4b5175cd":"# As first our data should be standardized \n# \n\nfrom sklearn.preprocessing import StandardScaler\nstandardized_data = StandardScaler().fit_transform(d)\nstandardized_data","9a48675e":"# Building a covariance matrix\nsample_data = standardized_data\nimport numpy as np\ncovar_matrix = np.matmul(sample_data.T,sample_data)\ncovar_matrix\nprint(\"Covariance matrix shape: \",covar_matrix.shape)","d42e890c":"from scipy.linalg import eigh\nvalues,vectors = eigh(covar_matrix,eigvals=(782,783))\n# print(values)\n# print(vectors)\nprint(vectors.shape)\nprint(values.shape)","b9870709":"# Storing transpose of vectors variable \n\nvectors = vectors.T\n#The shape of Transposed vector is \nprint(\"New shape is : \",vectors.shape)","4ccfda0a":"# sample_data.shape\n\n# creating new coordinates\nnew_cordinates = np.matmul(vectors,sample_data.T)\nnew_cordinates","029db5a5":"new_cordinates.shape","0be0c192":"import pandas as pd\nnew_cordinates = np.vstack((new_cordinates,l)).T\nnew_cordinates.shape","4f0d5fe6":"# Creating a Dataframe\ndataframe = pd.DataFrame(new_cordinates,columns=(\"ist principal\",\"2nd principal\",\"labels\"))\ndataframe","237d22d9":"import matplotlib.pyplot as plt\nimport seaborn as sns\n# Visualize the data using seaborn\nsns.FacetGrid(dataframe,hue=\"labels\",size=10)\\\n.map(plt.scatter,\"ist principal\",\"2nd principal\")\\\n.add_legend()","bf2733c2":"# ## Steps to be carried out are:\n# Step 1: split dataset into two parts one consisting of label column and second consisting of rest of the data\n# Step 2: As we know for pca first our data should be standardized therefore we will convert our data to column standardized\n# Step 3: After that we create a covariance matrix of our standardized_data i.e standardized_data^T*standardized_data\n# Step 4: Then we find eigen values and eigen vectors (As our covariance matrix is 784*784 dimensions therefore in total we will get 784 eigen values and 784 eigen vectors).The scipy eigh funcion provides this very easily and gives eigen values and vectors in ascending order as we are only interested in last two(highest) eigen values and eigen vectors therefore we pass eigvals as 782,783\n# Step 5: Taking traspose of this eigen vector so that dimention becomes (2,784)\n# Step 6: taking eigen_vector.standardized_data^T\n# step 7: creating a vstack with eigen_vector.standardized_data^T and labels\n# step 8: creating a dataframe of vstack with corresponding column names and visualizng data using seaborn\n# Step 9: checking the graph obtained","06177cfe":"## To directly do PCA by\nd = df.drop(\"label\",axis=1)\nd","46102304":"l = df[\"label\"]\nl","22725d5d":"## Standardizing the data\nfrom sklearn.preprocessing import StandardScaler\nstandardized_data = StandardScaler().fit_transform(d)\nstandardized_data.shape","d7b64acb":"from sklearn.decomposition import PCA\npca = PCA()\npca.n_components = 2\npca_fitdata = pca.fit_transform(standardized_data)\npca_fitdata","eb0d97f6":"pca_fitdata.shape","e0399f05":"l.shape","380de57f":"# data = np.vstack((pca_fitdata,l))\n# data","51880aca":"a = pd.DataFrame(pca_fitdata,columns=(\"1st principal\",\"2nd principal\"))\na","abffdb29":"b = l.to_frame(name=\"label\")\nb","71756aae":"df1 = pd.concat([a,b],axis=1)\ndf1","b5a3d87e":"# Visualizing the dataframe made using seaborn \nsns.FacetGrid(df1,hue=\"label\",size=10).map(plt.scatter,\"1st principal\",\"2nd principal\").add_legend();","5040286c":"# Standardized \nfrom sklearn.preprocessing import StandardScaler\nstandardized_data = StandardScaler().fit_transform(d)\nsample_data = standardized_data\n# PCA for dimentionality reduction\nfrom sklearn.decomposition import PCA\npca = PCA()\npca.n_component = 784\npca_data = pca.fit_transform(sample_data)\n\npercentage_var_explained = pca.explained_variance_\/np.sum(pca.explained_variance_)\n\ncumsum_percentage_var = np.cumsum(percentage_var_explained)\n\n# Plotting \n\nplt.figure(1,figsize=(10,10))\nplt.clf()\nplt.plot(cumsum_percentage_var,linewidth=2)\nplt.axis(\"tight\")\nplt.grid()\nplt.xlabel(\"n_component\")\nplt.ylabel(\"cumsum\")\nplt.show()","bc9a780c":"# print(pca.explained_variance_)\n# print(\"???????????????????????????????????????????????\")\n# print(pca.explained_variance_ratio_)\n# print(\"\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\")\n# print(pca.explained_variance_ratio_.cumsum())","956ae046":"df","c1430aac":"data = df.drop(\"label\",axis=1)\ndata","2ef4238d":"label = df[\"label\"]\nlabel","8d1629b0":"### As in case of PCA can only be applied to standardized data similar to that t-SNE \nfrom sklearn.preprocessing import StandardScaler\nstandardized_data = StandardScaler().fit_transform(data)\nstandardized_data","93282c36":"!pip install sklearn.manifold","b6592937":"from sklearn.manifold import TSNE\n\nmodel = TSNE(random_state=0)\nmodel.n_component=2\n### Configuring the parameters\n# the number of components = 2\n# the default perplexity= 30\n# default learning rate = 200\n# default maximum number of iterations for optimization = 1000\ntsne_data = model.fit_transform(standardized_data)","8cae3f09":"tsne_data","02096087":"### Converting label to dataframe\na = pd.DataFrame(tsne_data,columns=(\"1st dim\",\"2nd dim\"))\na","b713dbc5":"b = label.to_frame(name=\"label\")\nb","1aa99c71":"df1 = pd.concat([a,b],axis=1)\ndf1","bb85c63e":"# Visualizing the dataframe made using seaborn \nsns.FacetGrid(df1,hue=\"label\",size=10).map(plt.scatter,\"1st dim\",\"2nd dim\").add_legend();","fc8e5bc1":"### Tweaking the parameter of t-SNE\nfrom sklearn.manifold import TSNE\n\nmodel = TSNE(random_state=0,perplexity=50,n_iter=5000)\nmodel.n_component=2 #n_component basically means no of column we want in our t-SNE data to have\n### Configuring the parameters\n# the number of components = 2\n# the default perplexity= 30\n# default learning rate = 200\n# default maximum number of iterations for optimization = 1000\ntsne_data = model.fit_transform(standardized_data)","671d3288":"tsne_data","a5833bd6":"### Converting label to dataframe\na = pd.DataFrame(tsne_data,columns=(\"1st dim\",\"2nd dim\"))\na","6daa2eb5":"b = label.to_frame(name=\"label\")\nb","50bc2b6c":"df1 = pd.concat([a,b],axis=1)\ndf1","80caae72":"# Visualizing the dataframe made using seaborn \nsns.FacetGrid(df1,hue=\"label\",size=10).map(plt.scatter,\"1st dim\",\"2nd dim\").add_legend();","8c41035d":"# t-SNE(t-disb Stochastic Neighborhood Embedding):-  "}}