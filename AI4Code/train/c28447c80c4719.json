{"cell_type":{"a741a897":"code","028ffcd3":"code","e1fe5375":"code","41723359":"code","7c80371b":"code","a975ba8c":"code","fb137eb9":"code","4facec85":"code","53f8f06e":"code","f5af043f":"code","91e3b7b8":"code","145e6f4a":"code","a8d05d54":"code","d3144482":"code","6572c6c6":"code","a58b8af0":"code","c41bd142":"code","15e2516d":"code","27c861aa":"code","b1215c59":"code","8a3fd6ec":"code","29bbe964":"code","77d6a5c7":"code","b109857b":"code","51affbb7":"code","2c6c020a":"code","a3f824b7":"code","50ab54c3":"code","cdc57aa9":"code","550c374c":"code","108d4e5d":"code","0e5e2061":"code","22d35efa":"code","b62c05e2":"code","d77166bc":"code","49d10a6b":"code","475abd90":"code","183c9fbe":"code","3ddfaab9":"code","3735cef5":"code","3a2056bb":"code","e92c76de":"code","d27fcacb":"code","08e5a4bb":"code","41532118":"code","cd0921ce":"code","69b5aea8":"code","d939ddf1":"code","fa66758a":"code","4cb9cbcd":"code","156d55e5":"code","5b04c44e":"code","2d236c16":"code","758f81dc":"code","2903a3d4":"code","b1e50d42":"code","3db70119":"code","939256ef":"code","0bd1404a":"code","8cf158d6":"code","8f7a6986":"code","033165f8":"code","0e45f9d8":"code","a3e37f1a":"code","37ab36ee":"code","01bb44e4":"code","c81b5f5b":"code","523893ca":"code","20f50721":"code","c6902526":"code","eb0ae0af":"code","3d27a12f":"code","ff30e700":"code","c967ea0b":"code","bd44706f":"code","0a481759":"code","ff68335d":"code","a67d0f8e":"code","7fb38112":"code","f4fa72e6":"code","3c1f66bf":"code","8cc2e4e5":"code","a7c7d186":"code","0d652e46":"code","aa5ca9c5":"code","0ab35690":"code","8f942988":"code","2ae4ea9c":"code","297b46a1":"code","0d179e2a":"code","97a6d344":"code","ff535fde":"code","e76834cd":"code","4dfe8e7c":"code","e07cce05":"code","dec6b135":"code","00bbae2b":"code","e1078ad0":"code","02220160":"code","63e01b29":"code","25b5f73a":"code","6ee91027":"code","be83bd28":"code","a4a4b334":"code","9767fb78":"code","4e4d1f4b":"code","f4c4f03d":"code","66e34f11":"code","e49d15e4":"code","609ca0fc":"code","d7f11b46":"code","afc9c0b4":"code","303b2a83":"code","86b66772":"code","c153eabc":"code","c204f3bf":"code","9e779480":"code","e6de6e93":"code","2ad72382":"code","dbd296ca":"code","443369c8":"code","02ef80fa":"markdown","3307634c":"markdown","d0276844":"markdown"},"source":{"a741a897":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.neighbors import KNeighborsRegressor\nimport scipy.stats\nfrom itertools import combinations\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.naive_bayes import ComplementNB\nfrom pprint import pprint\n\n%matplotlib inline","028ffcd3":"# downloading and observing the data\ntrain = pd.read_csv('..\/input\/titanic\/train.csv')\ntrain.head()","e1fe5375":"test = pd.read_csv('..\/input\/titanic\/test.csv')\ntest.head()","41723359":"y = train.Survived\nId = test.PassengerId","7c80371b":"# distribution of survived and died passengers\nfig = plt.figure(figsize = (7, 7))\nsns.set(style='darkgrid')\nax = sns.countplot(x='Survived', data = train)","a975ba8c":"print('Number of people who survived: {}'.format(y.value_counts()[1]))\nprint('Number of people who died: {}'.format(y.value_counts()[0]))\nprint('Percentage of people who survived: {:.2f}%'.format(y.value_counts(normalize=True)[1]*100))\nprint('Percentage of people who died: {:.2f}%'.format(y.value_counts(normalize=True)[0]*100))","fb137eb9":"fig = plt.figure(figsize = (7, 7))\nsns.set(style='darkgrid')\nax = sns.countplot(x='Sex', data = train)","4facec85":"print('Number of males: {}'.format(train['Sex'].value_counts()[1]))\nprint('Number of females: {}'.format(train['Sex'].value_counts()[0]))","53f8f06e":"sns.catplot(x=\"Sex\",col=\"Survived\",\n                data=train, kind=\"count\",\n                height=6, aspect=.9)\n\n# print percentages of females vs. males that survive\nprint(\"Percentage of females who survived: {:.2f}%\".format (train[\"Survived\"][train[\"Sex\"] == 'female'].value_counts(normalize = True)[1]*100))\nprint(\"Percentage of males who survived: {:.2f}%\". format(train[\"Survived\"][train[\"Sex\"] == 'male'].value_counts(normalize = True)[1]*100))","f5af043f":"# females are much more likely to survive than male ","91e3b7b8":"fig = plt.figure(figsize = (7, 7))\nax = sns.countplot(x='Pclass', data=train)","145e6f4a":"sns.catplot(x=\"Pclass\",col=\"Survived\",\n                data=train, kind=\"count\",\n                height=7, aspect=.7)\n\n\nprint(\"Percentage of Pclass = 1 who survived: {:.2f}%\".format (train[\"Survived\"][train[\"Pclass\"] == 1].value_counts(normalize = True)[1]*100))\nprint(\"Percentage of Pclass = 2 who survived: {:.2f}%\".format (train[\"Survived\"][train[\"Pclass\"] == 2].value_counts(normalize = True)[1]*100))\nprint(\"Percentage of Pclass = 3 who survived: {:.2f}%\".format (train[\"Survived\"][train[\"Pclass\"] == 3].value_counts(normalize = True)[1]*100))","a8d05d54":"# we can see that 1 class are likely to survive than other classes which make sence","d3144482":"# Intuition: combine columns 'SibSp' and 'Parch' to a new column 'family'\n# (according to the description of the dataset) and look if family helps to survive or not","6572c6c6":"train['family'] = train['SibSp'] + train['Parch']\ntrain['family'] = [0 if x == 0 else 1 for x in train['family']]","a58b8af0":"sns.catplot(x=\"family\",col=\"Survived\",\n                data=train, kind=\"count\",\n                height=6, aspect=.9)","c41bd142":"print(\"Percentage of persons without family who survived: {:.2f}%\".format (train[\"Survived\"][train[\"family\"] == 0].value_counts(normalize = True)[1]*100))\nprint(\"Percentage of persons with family who survived: {:.2f}%\". format(train[\"Survived\"][train[\"family\"] == 1].value_counts(normalize = True)[1]*100))","15e2516d":"# this make sence\n# let's see how 'age' influence on survival rate","27c861aa":"train.groupby('Survived').Age.value_counts().plot(kind='hist')","b1215c59":"sns.jointplot(x = train['Age'], y=y)","8a3fd6ec":"train['Survived'][train['Age'] < 10.0].value_counts().plot(kind='barh')\nprint('Percentage of people with Age < 10 who survived: {:.2f}%'.format(train['Survived'][train['Age'] < 10.0].value_counts(normalize = True)[1]*100))","29bbe964":"train['Survived'][(train['Age'] >= 10.0) & (train['Age'] < 25.0)].value_counts().plot(kind='barh')\nprint('Percentage of people with Age >= 10 and Age < 25 who survived: {:.2f}%'.format(train['Survived'][(train['Age'] >= 10.0) & (train['Age'] < 25.0)].value_counts(normalize = True)[1]*100))","77d6a5c7":"train['Survived'][train['Age'] >= 45.0].value_counts().plot(kind='barh')\nprint('Percentage of people with Age >= 45 who survived: {:.2f}%'.format(train['Survived'][train['Age'] >= 45.0].value_counts(normalize = True)[1]*100))","b109857b":"# It's controversial question to remove this feature or not. Let's make iteractions after dealing with NaN and than decide","51affbb7":"# removing 'Survived' and 'family' columns\ntrain.drop(['Survived','family'], axis = 1, inplace = True)\ntrain.head()","2c6c020a":"# combining test and training data for cleaning dataset\ndataset = pd.concat([train, test], axis = 0 , sort = False, ignore_index = True)\ndataset.head()","a3f824b7":"# Intuition: Name, Ticket, PassengerId and Embarked are not informative features for prediction. So we can merely remove them from dataset","50ab54c3":"dataset.drop(['PassengerId', 'Ticket', 'Name', 'Embarked'], axis = 1, inplace = True)\ndataset['Family'] = dataset['SibSp'] + dataset['Parch']\ndataset['Family'] = [0 if x == 0 else 1 for x in dataset['Family']]\ndataset.drop(['SibSp', 'Parch'], axis = 1, inplace = True)","cdc57aa9":"dataset.head()","550c374c":"# It's time to handle with missing data","108d4e5d":"dataset.isna().sum()","0e5e2061":"print('Percentage of missing values of \"Age\" column : {:.2f}%'.format(dataset['Age'].isna().mean()*100))\nprint('Percentage of missing values of \"Cabin\" column : {:.2f}%'.format(dataset['Cabin'].isna().mean()*100))\nprint('Percentage of missing values of \"Fare\" column : {:.2f}%'.format(dataset['Fare'].isna().mean()*100))","22d35efa":"# 'Cabin' column has almost all missing values, so we have to delete this feature","b62c05e2":"dataset.drop('Cabin', axis = 1, inplace = True)\ndataset.columns","d77166bc":"# Let's look at 'Age' distributions and decide what to do with missing values","49d10a6b":"# visualizing distributions\nsns.distplot(dataset['Age'], kde = False, hist = True)","475abd90":"ax = sns.boxplot(x=dataset['Age'])","183c9fbe":"# Well, it seems that it is not a good idea to replace all NaN values in the 'Age' column with mean\/median\/mode\n# Let's create a knn model to predict Age\n# but before that I will create dummies for 'Sex' column","3ddfaab9":"dataset = pd.get_dummies(dataset, columns = ['Sex'])\ndataset = dataset.drop(['Sex_male'], axis = 1)\ndataset.head()","3735cef5":"# function for KNN model-based imputation of missing values using features without NaN as predictors\n\ndef impute_model(dataset):\n    cols_nan = dataset.columns[dataset.isna().any()].tolist()    \n    cols_no_nan = dataset.columns.difference(cols_nan).values            \n    for col in cols_nan:\n        test_data = dataset[dataset[col].isna()]\n        train_data = dataset.dropna()\n        knr = KNeighborsRegressor(n_neighbors=5).fit(train_data[cols_no_nan], train_data[col])\n        dataset.loc[dataset[col].isna(), col] = knr.predict(test_data[cols_no_nan])\n    return dataset","3a2056bb":"dataset = impute_model(dataset)","e92c76de":"dataset.head()","d27fcacb":"dataset.isna().sum()","08e5a4bb":"# Great! It's time to deal with 'Pclass' and 'Fare' column\n# Intuition: The higher price of your ticket the better class\n# Let's check if 'Pclass' and 'Fare' highly correlated","41532118":"# 0-hypothesis: 'Pclass' and 'Fare' are not correlated","cd0921ce":"scipy.stats.pearsonr(dataset['Pclass'], dataset['Fare'])    # Pearson's r","69b5aea8":"scipy.stats.spearmanr(dataset['Pclass'], dataset['Fare'])    # Spearman's rho","d939ddf1":"scipy.stats.kendalltau(dataset['Pclass'], dataset['Fare'])   # Kendall's tau","fa66758a":"# pvalue < 0.05 0-hypothesis is rejected -  these two features are highly correlated\n# We will remove 'Fare' ","4cb9cbcd":"dataset.drop('Fare', axis = 1, inplace = True)","156d55e5":"dataset.head()","5b04c44e":"# Now I will make a strange step\n# As we have already convinced first class are likely to survive than third\n# So I will do manual feature scaling","2d236c16":"dataset = dataset.replace({'Pclass':{3:0, 2:0.5}}) # I suppouse it's better for ML algorithm","758f81dc":"dataset.head()","2903a3d4":"dataset['ScaleredAge'] = [(x - min(dataset['Age'])) \/ (max(dataset['Age']) - min(dataset['Age'])) for x in dataset['Age']]\ndataset.drop(['Age'], axis = 1, inplace = True)\ndataset.head()","b1e50d42":"# making feature iteractions","3db70119":"def add_iteractions(dataset):\n    # Get feature names\n    combos = list(combinations(list(dataset.columns), 2))\n    colnames = list(dataset.columns) + ['_'.join(x) for x in combos]\n    \n    # Find interactions\n    poly = PolynomialFeatures(interaction_only = True, include_bias = False)\n    dataset = poly.fit_transform(dataset)\n    dataset = pd.DataFrame(dataset)\n    dataset.columns = colnames\n    \n    # Remove interactions terms with 0 values\n    noint_indices = [i for i, x in enumerate(list((dataset == 0).all())) if x]\n    dataset = dataset.drop(dataset.columns[noint_indices], axis = 1)\n    \n    return dataset","939256ef":"dataset = add_iteractions(dataset)\ndataset.head()","0bd1404a":"train = dataset[:len(train)]\ntrain.shape","8cf158d6":"test = dataset[len(train):]\ntest.shape","8f7a6986":"# Let's combine 'y' and 'train' to see correlation map\ntrain_data = pd.concat([y, train], axis = 1, sort = False)","033165f8":"#get correlations of each features in dataset\ncorrmat = train_data.corr()\ntop_corr_features = corrmat.index\nplt.figure(figsize=(20,20))\n#plot heat map\ng=sns.heatmap(train_data[top_corr_features].corr(),annot=True,cmap=\"RdYlGn\")","0e45f9d8":"# Features to delete: 'ScaleredAge', 'Family_ScaleredAge'","a3e37f1a":"train.drop(['ScaleredAge', 'Family_ScaleredAge'], axis = 1, inplace = True)","37ab36ee":"test.drop(['ScaleredAge', 'Family_ScaleredAge'], axis = 1, inplace = True)","01bb44e4":"# That's all for feature engineering","c81b5f5b":"# Models to build and compare : K-NN, Decision Trees, Random Forest, Logistic Regression and ComplementNB","523893ca":"# K-NN","20f50721":"knn = KNeighborsClassifier()","c6902526":"grid_params = {\n    'n_neighbors': list(range(1,16)),\n    'weights': ['uniform', 'distance'],\n    'metric': ['euclidean', 'manhattan']\n}","eb0ae0af":"cv = StratifiedKFold(n_splits=10, shuffle = True, random_state = 42) # best for classification","3d27a12f":"gs_knn = GridSearchCV(knn,grid_params, cv = cv, verbose=1, n_jobs = -1)","ff30e700":"gs_knn.fit(train, y)","c967ea0b":"gs_knn.best_score_","bd44706f":"def plot_search_results(grid):\n    \"\"\"\n    Params: \n        grid: A trained GridSearchCV object.\n    \"\"\"\n    ## Results from grid search\n    results = gs_knn.cv_results_\n    means_test = results['mean_test_score']\n    stds_test = results['std_test_score']\n    \n    \n\n    ## Getting indexes of values per hyper-parameter\n    masks=[]\n    masks_names= list(gs_knn.best_params_.keys())\n    for p_k, p_v in gs_knn.best_params_.items():\n        masks.append(list(results['param_'+p_k].data==p_v))\n\n    params=gs_knn.param_grid\n\n    ## Ploting results\n    fig, ax = plt.subplots(1,len(params),sharex='none', sharey='all',figsize=(20,5))\n    fig.suptitle('Score per parameter')\n    fig.text(0.04, 0.5, 'MEAN SCORE', va='center', rotation='vertical')\n    pram_preformace_in_best = {}\n    for i, p in enumerate(masks_names):\n        m = np.stack(masks[:i] + masks[i+1:])\n        pram_preformace_in_best\n        best_parms_mask = m.all(axis=0)\n        best_index = np.where(best_parms_mask)[0]\n        x = np.array(params[p])\n        y_1 = np.array(means_test[best_index])\n        e_1 = np.array(stds_test[best_index])\n\n        ax[i].errorbar(x, y_1, e_1, linestyle='--', marker='o', label='test')\n\n        ax[i].set_xlabel(p.upper())\n\n    plt.legend()\n    plt.show()","0a481759":"plot_search_results(gs_knn)","ff68335d":"best_knn = gs_knn.best_estimator_","a67d0f8e":"final_predictions_knn = best_knn.predict(test)","7fb38112":"output = pd.DataFrame({'PassengerId': Id, 'Survived': final_predictions_knn})\noutput.to_csv('submission1.csv', index=False)","f4fa72e6":"# Let's download submission to Kaggle and see the result","3c1f66bf":"print('Kaggle K-NN score : 0.74880')","8cc2e4e5":"# Decision Trees","a7c7d186":"dt = DecisionTreeClassifier()","0d652e46":"grid_params_tree = {\n    'min_samples_split' : [2, 3, 4, 5, 6],\n    'max_leaf_nodes': list(range(2, 100)),\n    'max_depth': list(range(1,20,2))\n}","aa5ca9c5":"gs_dt = GridSearchCV(dt,grid_params_tree, cv = cv, verbose=1, n_jobs = -1)","0ab35690":"gs_dt.fit(train, y)","8f942988":"gs_dt.best_score_","2ae4ea9c":"best_dt = gs_dt.best_estimator_","297b46a1":"from sklearn import tree\ntree.plot_tree(best_dt)","0d179e2a":"final_prediction_dt = best_dt.predict(test)","97a6d344":"output = pd.DataFrame({'PassengerId': Id, 'Survived': final_prediction_dt})\noutput.to_csv('submission2.csv', index=False)","ff535fde":"print('Kaggle DT score : 0.76555')","e76834cd":"# Random Forest","4dfe8e7c":"rf = RandomForestRegressor()","e07cce05":"# Look at parameters used by our current forest\nprint('Parameters currently in use:\\n')\npprint(rf.get_params())","dec6b135":"param_grid = {\n    'bootstrap': [True],\n    'max_depth': [100, 105, 110, 115, 120],\n    'max_features': ['sqrt'],\n    'min_samples_leaf': [5, 6, 7],\n    'min_samples_split': [4, 5, 6],\n    'n_estimators': [int(x) for x in np.linspace(start = 200, stop = 300, num = 5)]\n}\n\n# Instantiate the grid search model\ngrid_search_rf = GridSearchCV(estimator = rf, param_grid = param_grid, \n                          cv = cv, n_jobs = -1, verbose = 1)","00bbae2b":"grid_search_rf.fit(train, y)","e1078ad0":"grid_search_rf.best_params_","02220160":"best_grid_rf = grid_search_rf.best_estimator_","63e01b29":"final_predictions = best_grid_rf.predict(test)","25b5f73a":"print(final_predictions)","6ee91027":"final_predictions[final_predictions >= 0.5] = 1\nfinal_predictions[final_predictions < 0.5] = 0","be83bd28":"final_predictions = final_predictions.astype(int)\nprint(final_predictions[0:5])","a4a4b334":"output = pd.DataFrame({'PassengerId': Id, 'Survived': final_predictions})\noutput.to_csv('submission3.csv', index=False)","9767fb78":"print('Kaggle Random Forest score : 0.77033')","4e4d1f4b":"# Logistic regression","f4c4f03d":"grid_lr = {\"C\":np.logspace(-3,3,7), \"penalty\":[\"l1\",\"l2\"]}# l1 lasso l2 ridge\nlogreg=LogisticRegression()\nlogreg_cv=GridSearchCV(logreg,grid_lr,cv=cv, n_jobs = -1, verbose = 1)\nlogreg_cv.fit(train, y)\n\nprint(\"tuned hpyerparameters :(best parameters) \",logreg_cv.best_params_)","66e34f11":"best_score_lr = logreg_cv.best_estimator_","e49d15e4":"predictions_lr = best_score_lr.predict(test)","609ca0fc":"output = pd.DataFrame({'PassengerId': Id, 'Survived': predictions_lr})\noutput.to_csv('submission4.csv', index=False)","d7f11b46":"print('Kaggle Logistic Regression score : 0.77751')","afc9c0b4":"# Complement NB","303b2a83":"NB = ComplementNB()","86b66772":"params_NB = {'alpha':list(np.arange(0.1,10,0.5))}\ngrid_search_NB = GridSearchCV(estimator = NB, param_grid = params_NB, \n                          cv = cv, n_jobs = -1, verbose = 1)","c153eabc":"grid_search_NB.fit(train, y)","c204f3bf":"best_score_NB = grid_search_NB.best_estimator_","9e779480":"predictions_NB = best_score_NB.predict(test)","e6de6e93":"output = pd.DataFrame({'PassengerId': Id, 'Survived': predictions_NB})\noutput.to_csv('submission5.csv', index=False)","2ad72382":"print('Kaggle Complemet NB score : 0.76555')","dbd296ca":"# Let's summarize all results","443369c8":"print('K-NN score : 0.74880')\nprint('DT score : 0.76555')\nprint('Random Forest score : 0.77033')\nprint('Logistic Regression score : 0.77751')\nprint('Complemet NB score : 0.76555')","02ef80fa":"Let's observe all features.\n\n- **Survived**: Outcome of survival (0 = No; 1 = Yes)\n- **Pclass**: Socio-economic class (1 = Upper class; 2 = Middle class; 3 = Lower class)\n- **Name**: Name of passenger\n- **Sex**: Sex of the passenger\n- **Age**: Age of the passenger\n- **SibSp**: Number of siblings and spouses of the passenger aboard\n- **Parch**: Number of parents and children of the passenger aboard\n- **Ticket**: Ticket number of the passenger\n- **Fare**: Fare paid by the passenger\n- **Cabin** Cabin number of the passenger\n- **Embarked**: Port of embarkation of the passenger (C = Cherbourg; Q = Queenstown; S = Southampton)\n\nSince we're interested in the outcome of survival for each passenger or crew member, we can store the **Survived** feature as its own separate variable. And also we will store the **PassengerId** from the test dataset for submission","3307634c":"Aims of the notebook: 1) do intuitive feature engineering \n                      2) compare ML models (K-NN, Decision Trees, Random Forest, Logistic Regression and NB)","d0276844":" Hello everyone. My name Vladislav Shikhta. I want to become a data scientist and this is my first notebook. So, please, don't judge me strong if my code contains blunders. Let's go!"}}