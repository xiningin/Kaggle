{"cell_type":{"2250c8e4":"code","0031deac":"code","19c7735d":"code","816b8c54":"code","12890a6b":"code","c82cd1b1":"code","df962325":"code","ec0d10c7":"code","65584df6":"code","550726ae":"code","baf9605b":"code","46547e50":"code","0b65a47d":"code","47f3a037":"code","75dcefda":"code","9b0f9f8d":"code","0838a9fa":"markdown","b025c718":"markdown","d18bcc5f":"markdown","1bd3339b":"markdown","31f8af60":"markdown","d80edd78":"markdown","6a6fc840":"markdown"},"source":{"2250c8e4":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport pandas as pd\nfrom skimage import io\nimport torch.optim as optim\nimport torchvision.transforms as transforms\nfrom torch.utils.data import Dataset, DataLoader\nimport random\nimport math\nimport os\nfrom tqdm.notebook import tqdm\nfrom torch.autograd import Variable","0031deac":"class DepthWiseSeparableConv(nn.Module):\n    def __init__(self, in_features, out_channels, dw_kernel, dw_stride):\n        super(DepthWiseSeparableConv, self).__init__()\n        self.stride = dw_stride\n        self.dw_conv = nn.Conv2d(in_channels=1, out_channels=1, kernel_size=dw_kernel, stride=dw_stride, padding=1)\n        self.bn1 = nn.BatchNorm2d(num_features=in_features)\n        self.pw_conv = nn.Conv2d(in_channels=in_features, out_channels=out_channels, kernel_size=1, stride=1)\n        self.bn2 = nn.BatchNorm2d(num_features=out_channels)\n    \n    def forward(self, x):\n        dims = x.dim()\n        if dims < 4:\n            raise ValueError(\"Expected input to be atleast 4 dimensions, found: {}\".format(dims))\n        sizes = x.size()\n        # View (N, C, H, W) as a stack of matrices \/ images containing a single channel (stack of channels across batch)\n        x = x.view(-1, 1, sizes[2], sizes[3])\n        # Simply put, we could think of x being a batch of single channel images where batch_size (or) batch = N * C\n        x = self.dw_conv(x)\n        x = x.view(sizes[0], sizes[1], sizes[2] \/\/ self.stride, sizes[3] \/\/ self.stride)\n        x = F.relu(self.bn1(x))\n        # 1 x 1 Conv Module\n        x = F.relu(self.bn2(self.pw_conv(x)))\n        return x","19c7735d":"# Expects images of the shape (224, 224, 3)\nclass MobileNet(nn.Module):\n    def __init__(self, num_classes=37, bounding_box_points=4):\n        super(MobileNet, self).__init__()\n        self.conv1 = nn.Conv2d(in_channels=3, out_channels=32, kernel_size=3, stride=2, padding=1)\n        self.bn = nn.BatchNorm2d(num_features=32)\n        self.dw_conv1 = DepthWiseSeparableConv(in_features=32, out_channels=64, dw_kernel=3, dw_stride=1)\n        self.dw_conv2 = DepthWiseSeparableConv(in_features=64, out_channels=128, dw_kernel=3, dw_stride=2)\n        self.dw_conv3 = DepthWiseSeparableConv(in_features=128, out_channels=128, dw_kernel=3, dw_stride=1)\n        self.dw_conv4 = DepthWiseSeparableConv(in_features=128, out_channels=256, dw_kernel=3, dw_stride=2)\n        self.dw_conv5 = DepthWiseSeparableConv(in_features=256, out_channels=256, dw_kernel=3, dw_stride=1)\n        self.dw_conv6 = DepthWiseSeparableConv(in_features=256, out_channels=512, dw_kernel=3, dw_stride=2)\n        self.dw_conv7 = DepthWiseSeparableConv(in_features=512, out_channels=512, dw_kernel=3, dw_stride=1)\n        self.dw_conv8 = DepthWiseSeparableConv(in_features=512, out_channels=512, dw_kernel=3, dw_stride=1)\n        self.dw_conv9 = DepthWiseSeparableConv(in_features=512, out_channels=512, dw_kernel=3, dw_stride=1)\n        self.dw_conv10 = DepthWiseSeparableConv(in_features=512, out_channels=512, dw_kernel=3, dw_stride=1)\n        self.dw_conv11 = DepthWiseSeparableConv(in_features=512, out_channels=512, dw_kernel=3, dw_stride=1)\n        self.dw_conv12 = DepthWiseSeparableConv(in_features=512, out_channels=1024, dw_kernel=3, dw_stride=2)\n        self.dw_conv13 = DepthWiseSeparableConv(in_features=1024, out_channels=1024, dw_kernel=3, dw_stride=1)\n        \n        self.avg_pool = nn.MaxPool2d(kernel_size=7, stride=1)\n        self.fc_classification = nn.Linear(in_features=1 * 1 * 1024, out_features=num_classes)\n        self.fc_bounding_box = nn.Linear(in_features=1 * 1 * 1024, out_features=bounding_box_points)\n    \n    def forward(self, x):\n        x = F.relu(self.bn(self.conv1(x)))\n        x = self.dw_conv1(x)\n        x = self.dw_conv2(x)\n        x = self.dw_conv3(x)\n        x = self.dw_conv4(x)\n        x = self.dw_conv5(x)\n        x = self.dw_conv6(x)\n        x = self.dw_conv7(x)\n        x = self.dw_conv8(x)\n        x = self.dw_conv9(x)\n        x = self.dw_conv10(x)\n        x = self.dw_conv11(x)\n        x = self.dw_conv12(x)\n        x = torch.flatten(self.avg_pool(self.dw_conv13(x)), start_dim=1)\n        y_classification = self.fc_classification(x)\n        y_bounding_box = self.fc_bounding_box(x)\n        return [y_classification, y_bounding_box]","816b8c54":"device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nx = torch.zeros(1, 3, 224, 224, dtype=torch.float, requires_grad=False).to(device)\nmobilenet = MobileNet()\nmobilenet.to(device=device)\nmobilenet(x)","12890a6b":"# need to install torchviz dependency for visualization\n!pip install torchviz graphviz","c82cd1b1":"from torchviz import make_dot\nfrom graphviz import Source\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nx = torch.zeros(1, 3, 224, 224, dtype=torch.float, requires_grad=False).to(device)\ny_classification = torch.zeros(1, dtype=torch.long, requires_grad=False).to(device)\ny_bounding_box = torch.zeros(1, 4, dtype=torch.float, requires_grad=False).to(device)\nmobilenet = MobileNet()\nmobilenet.to(device=device)\nout = mobilenet(x)\nclassification_loss = nn.CrossEntropyLoss()(out[0], y_classification)\nbounding_box_regression_loss = nn.MSELoss()(out[1], y_bounding_box)\nloss = classification_loss + bounding_box_regression_loss\nmodel_arch = make_dot(loss)\nSource(model_arch).render(\"..\/working\/mobilenet_architecture_mulitple_loss\")","df962325":"from lxml import etree\ndef generate_dictionaries(root_dir=\"..\/input\/the-oxfordiiit-pet-dataset\/annotations\/annotations\/\", train_val_split=0.8):\n    train_ids = []\n    train_val_dict = {}\n    labels = {}\n    classes_set = set()\n    curr_len = len(classes_set)\n    idx2class = {}\n    with open(os.path.join(root_dir, \"trainval.txt\"), \"r\") as tv_file:\n            for line in tv_file:\n                try:\n                    if not line.startswith('#'):\n                        line_split = line.split(\" \")\n                        img_filename = line_split[0] + \".jpg\"\n                        breed_label = int(line_split[1])\n                        species_label = int(line_split[2]) - 1\n                        if 1 <= breed_label <= 25:\n                            species_name = \"cat\"\n                        else:\n                            species_name = \"dog\"\n                        classes_set.add(species_name)\n                        if len(classes_set) > curr_len:\n                            curr_len = len(classes_set)\n                            idx2class[species_label] = species_name\n                        with open(\"..\/input\/the-oxfordiiit-pet-dataset\/annotations\/annotations\/xmls\/{}.xml\".format(line_split[0])) as xml_file:\n                            xml = xml_file.read()\n                        root = etree.fromstring(xml)\n                        # just to check if the image exists\n                        with open(\"..\/input\/the-oxfordiiit-pet-dataset\/images\/images\/{}\".format(img_filename)) as f:\n                            l = []\n                        # iterate through xml tree structure\n                        for element in root.iter():\n                            l.append(element.text)\n                        l = l[-5:-1]\n                        l = [int(x)\/1. for x in l]\n                        train_ids.append(img_filename)\n                        labels[img_filename] = [species_label, l]\n                except Exception as e:\n#                     print(e)\n                    # print the files that don't have xml files\n                    print(line_split[0])\n                    continue\n    random.shuffle(x=train_ids)\n    train_val_dict['train'] = train_ids[:int(math.ceil(train_val_split*len(train_ids)))]\n    train_val_dict['val'] = train_ids[int(math.ceil(train_val_split*len(train_ids))):]\n    return train_val_dict, labels, idx2class","ec0d10c7":"# sanity check\ntrain_val_dict, labels, idx2class = generate_dictionaries()","65584df6":"len(train_val_dict['train']) + len(train_val_dict['val']), len(labels)","550726ae":"class PetsDataset(Dataset):\n    def __init__(self, list_ids, labels, idx2class, root_dir, transforms=None):\n        self.list_ids = list_ids\n        self.labels = labels\n        self.root_dir = root_dir\n        self.idx2class = idx2class\n        self.transforms = transforms\n    \n    def __len__(self):\n        return len(self.list_ids)\n    \n    def __getitem__(self, index):\n        file_name = self.list_ids[index]\n        class_label = torch.tensor(self.labels[file_name][0], dtype=torch.long)\n        bbox_ground_truth = torch.tensor(self.labels[file_name][1], dtype=torch.float32)\n        # To get image path, join root dir, class folder name, and file_name\n        img_path = os.path.join(self.root_dir, file_name)\n        image = io.imread(img_path)\n        old_x = image.shape[0]\n        old_y = image.shape[1]\n        if self.transforms:\n            image = self.transforms(image)\n        scaled_x = 224. \/ old_x\n        scaled_y = 224. \/ old_y\n        for idx, coords in enumerate(bbox_ground_truth):\n            if idx % 2 == 0:\n                bbox_ground_truth[idx] = bbox_ground_truth[idx] *  scaled_x\n            else:\n                bbox_ground_truth[idx] = bbox_ground_truth[idx] *  scaled_y\n        return [image, class_label, bbox_ground_truth]","baf9605b":"def check_accuracy(loader, model):\n    num_correct = 0\n    num_samples = 0\n    # Don't forget to toggle to eval mode!\n    model.eval()\n    \n    with torch.no_grad():\n        losses = []\n        for data, target_classify, target_bbox in tqdm(loader):\n            data = data.to(device=device)\n            target_classify = target_classify.to(device=device)\n            target_bbox = target_bbox.to(device=device)\n\n            scores = model(data)\n            loss2 = bounding_box_regression_loss(scores[1], target_bbox)\n            losses.append(loss2)\n            _, predictions = scores[0].max(1)\n            num_correct += (predictions == targets).sum()\n            num_samples += predictions.size(0)\n        print(\"Correct: {}, Total: {}, Accuracy: {}\".format(num_correct, num_samples, int(num_correct) \/ int(num_samples)))\n        print(\"Regression Loss: {}\".format(sum(losses) \/ len(losses)))\n    # Don't forget to toggle back to model.train() since you're done with evaluation\n    model.train()","46547e50":"if __name__ == '__main__':\n    \n    LEARNING_RATE = 0.001\n    BATCH_SIZE = 16\n    # Training for ~100 epochs might give you better performance\n    EPOCHS = 20\n    NUM_CLASSES = 2\n    BOUNDING_BOX_POINTS = 4\n    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n    \n    transform_img = transforms.Compose([\n        transforms.ToPILImage(),\n        transforms.Resize((224, 224)),\n        transforms.ToTensor()\n    ])\n    \n    train_val_dict, labels, idx2class = generate_dictionaries()\n    train_data = PetsDataset(list_ids=train_val_dict['train'], \n                             labels=labels,\n                             idx2class=idx2class, \n                             root_dir=\"..\/input\/the-oxfordiiit-pet-dataset\/images\/images\/\", \n                             transforms=transform_img)\n    train_loader = DataLoader(dataset=train_data, batch_size=BATCH_SIZE, shuffle=True)\n    \n    val_data = PetsDataset(list_ids=train_val_dict['val'],\n                             labels=labels,\n                             idx2class=idx2class, \n                             root_dir=\"..\/input\/the-oxfordiiit-pet-dataset\/images\/images\/\", \n                             transforms=transform_img)\n    val_loader = DataLoader(dataset=val_data, batch_size=BATCH_SIZE, shuffle=True)\n    \n    mobilenet = MobileNet(num_classes=NUM_CLASSES)\n    mobilenet.to(device)\n    classification_loss = nn.CrossEntropyLoss()\n    bounding_box_regression_loss = nn.MSELoss()\n    optimizer = optim.Adam(mobilenet.parameters(), lr=LEARNING_RATE)\n    data, target_classify, target_bbox = next(iter(train_loader))\n    for epoch in tqdm(range(EPOCHS)):\n        losses = []\n        with tqdm(total=len(train_val_dict['train']) \/\/ BATCH_SIZE) as pbar:\n            for batch_idx, (data, target_classify, target_bbox) in enumerate(train_loader):\n                data = data.to(device=device)\n                target_classify = target_classify.to(device=device)\n                target_bbox = target_bbox.to(device=device)\n\n                scores = mobilenet(data)\n                loss1 = classification_loss(scores[0], target_classify)\n                loss2 = bounding_box_regression_loss(scores[1], target_bbox)\n                loss = loss1 + loss2\n                losses.append(loss)\n\n                # backprop\n                optimizer.zero_grad()\n                loss.backward()\n                optimizer.step()\n                pbar.update(1)\n#                 print(loss.item())\n        print(\"Cost at epoch {} is {}\".format(epoch, sum(losses) \/ len(losses)))\n#         print(\"Calculating Train Accuracy...\")\n#         check_accuracy(train_loader, mobilenet)\n#         print(\"Calculating Validation Accuracy...\")\n#         check_accuracy(val_loader, mobilenet)","0b65a47d":"import matplotlib.pyplot as plt\nimport matplotlib.patches as patches\ndef see_prediction(path=\"..\/input\/the-oxfordiiit-pet-dataset\/images\/images\/Abyssinian_114.jpg\"):\n    image = plt.imread(path)\n    transform_img = transforms.Compose([\n        transforms.ToPILImage(),\n        transforms.Resize((224, 224)),\n        transforms.ToTensor()\n    ])\n\n    transformed_image = transform_img(image).unsqueeze(0).to(device=device)\n    out = mobilenet(transformed_image)\n    print(out[0])\n    _, prediction = out[0].max(1)\n    print(prediction)\n    print(idx2class[prediction.item()])\n#     breed_name = idx2class[prediction.item()]\n#     print(breed_name)\n\n    out_item = [i.item() for i in out[1][0]]\n    plot_scale_x = image.shape[0] \/ 224.\n    plot_scale_y = image.shape[1] \/ 224.\n    for idx, coords in enumerate(out_item):\n        if idx % 2 == 0:\n            out_item[idx] = out_item[idx] *  plot_scale_x\n        else:\n            out_item[idx] = out_item[idx] *  plot_scale_y\n\n    fig,ax = plt.subplots(1)\n    ax.imshow(image)\n    rect = patches.Rectangle((out_item[0],out_item[1]),abs(out_item[3] - out_item[1]), abs(out_item[2] - out_item[0]),linewidth=1,edgecolor='r',facecolor='none')\n    ax.add_patch(rect)\n    plt.show()","47f3a037":"see_prediction(\"..\/input\/the-oxfordiiit-pet-dataset\/images\/images\/Abyssinian_100.jpg\")","75dcefda":"see_prediction(\"..\/input\/the-oxfordiiit-pet-dataset\/images\/images\/Abyssinian_201.jpg\")","9b0f9f8d":"see_prediction(\"..\/input\/the-oxfordiiit-pet-dataset\/images\/images\/german_shorthaired_103.jpg\")","0838a9fa":"## Custom Depth Wise Separable Convolution - The fundamental module of MobileNet","b025c718":"## Utility to compute train \/ val \/ test accuracy","d18bcc5f":"## MobileNet architecture using the above module","1bd3339b":"## Utility to generate labels and train, val dictionaries to feed into custom dataset","31f8af60":"## Sanity check for this net we just built\n\n### Let's pass in a dummy tensor and see if we get two separate prediction tensors","d80edd78":"## Setting up DataLoaders and Training Loops","6a6fc840":"## Let Visualize the architecture"}}