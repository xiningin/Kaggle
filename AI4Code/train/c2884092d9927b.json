{"cell_type":{"04f400e1":"code","5c09e87d":"code","7319e057":"code","99b80271":"code","dd9d4d0d":"code","36a6f439":"code","20fa98f0":"code","f7717fb3":"code","9b3e8db4":"code","966872bf":"code","8547c817":"code","1b328834":"code","bdd4cc3c":"code","788b5943":"code","3a1f4285":"code","9b4b6999":"code","3f20ad94":"code","cccaf643":"code","319c5aa8":"code","c4b951b2":"code","4144198b":"code","ee67f458":"code","a7e16a0e":"code","3dc3c41a":"code","89ccfee2":"code","e0fb71f6":"code","2ccdeff2":"code","85974762":"code","dff5011f":"code","f8c4ef57":"code","e1ad9363":"code","2b2fad8a":"code","798c8ef0":"code","cd7db349":"code","1aa6a4c1":"code","4fb33815":"code","74fcefbd":"code","fce0d1e5":"code","e3a765c7":"code","47864ce4":"code","6c16a713":"code","83172fe2":"code","a319ee74":"code","77a89afb":"code","314c4cc0":"code","20cc0369":"code","d2339943":"code","e0f3e4a1":"code","7234725d":"code","7e475b62":"code","77fb9bbd":"code","c54fe7d4":"code","0e7ba767":"code","59bdf299":"code","bc11addc":"code","0e402028":"code","54dc9cb7":"code","6cd44b53":"code","4a03d180":"markdown","2d458889":"markdown","fed62e73":"markdown","f854c189":"markdown","c7b73ae8":"markdown","9bc41278":"markdown","848772d9":"markdown","b70f10e4":"markdown","49a15a7e":"markdown","98770b30":"markdown","914c0fde":"markdown"},"source":{"04f400e1":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","5c09e87d":"# import Libraries\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport seaborn as sns\nimport warnings\nwarnings.filterwarnings('ignore')\nfrom scipy import stats\nfrom scipy.stats import norm\nfrom sklearn.preprocessing import StandardScaler\n% matplotlib inline","7319e057":"# Read and load Data\ntrain = pd.read_csv(\"..\/input\/train.csv\")\ntest = pd.read_csv(\"..\/input\/test.csv\")","99b80271":"# check index of dataframe\ntrain.columns","dd9d4d0d":"train.SalePrice.describe()","36a6f439":"#PLot Histogram for 'SalePrice'\nsns.distplot(train['SalePrice'])","20fa98f0":"# Skewness and Kurtosis\nprint(\"Skewness : %f\" % train['SalePrice'].skew())\nprint(\"Kurtosis : %f\" % train['SalePrice'].kurt())","f7717fb3":"target = np.log(train.SalePrice)\nprint(\"Skewness : %f\" % target.skew())\nprint(\"Kurtosis : %f\" % target.kurt())","9b3e8db4":"numeric_features = train.select_dtypes(include=[np.number])\nnumeric_features.dtypes","966872bf":"print(train.describe(include=['number']).loc[['min','max','mean']].T.sort_values('max'))","8547c817":"corr = numeric_features.corr()\n\nprint (corr['SalePrice'].sort_values(ascending=False)[:5], '\\n')\nprint (corr['SalePrice'].sort_values(ascending=False)[-5:])","1b328834":"#'SalePrice' Correlation Matrix\nk = 10\ncols = corr.nlargest(k , 'SalePrice')['SalePrice'].index\ncm = np.corrcoef(train[cols].values.T)\nsns.set(font_scale = 1.00)\nhm = sns.clustermap(cm , cmap = \"Greens\",cbar = True,square = True,\n                 yticklabels = cols.values, xticklabels = cols.values)","bdd4cc3c":"quality_pivot = train.pivot_table(index='OverallQual',\n                                  values='SalePrice', aggfunc=np.median)\nquality_pivot.plot(kind='bar', color='blue')\nplt.xlabel('Overall Quality')\nplt.ylabel('Median Sale Price')\nplt.xticks(rotation=0)\nplt.show()","788b5943":"# Histogram and normal probability plot\nsns.distplot(train['SalePrice'], fit = norm)\nfig = plt.figure()\nres = stats.probplot(train['SalePrice'],plot = plt)","3a1f4285":"# Missing Data\ntotal = train.isnull().sum().sort_values(ascending = False)\npercent = (train.isnull().sum()\/train.isnull().count()).sort_values(ascending = False)\nmissing_data = pd.concat([total,percent], axis = 1, keys = ['Total', 'Percent'])\nmissing_data.head(20)","9b4b6999":"categoricals = train.select_dtypes(exclude=[np.number])\ncategoricals.describe()","3f20ad94":"# ref. https:\/\/github.com\/shan4224\/Kaggle_House_Prices\nn = categoricals\nfor c in n.columns:\n    print('{:<14}'.format(c), train[c].unique())","cccaf643":"from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n\ncols = ('MSZoning', 'Street', 'Alley', 'LotShape', 'LandContour', 'Utilities',\n       'LotConfig', 'LandSlope', 'Neighborhood', 'Condition1', 'Condition2',\n       'BldgType', 'HouseStyle', 'RoofStyle', 'RoofMatl', 'Exterior1st',\n       'Exterior2nd', 'MasVnrType', 'ExterQual', 'ExterCond', 'Foundation',\n       'BsmtQual', 'BsmtCond', 'BsmtExposure', 'BsmtFinType1', 'BsmtFinType2',\n       'Heating', 'HeatingQC', 'CentralAir', 'Electrical', 'KitchenQual',\n       'Functional', 'FireplaceQu', 'GarageType', 'GarageFinish', 'GarageQual',\n       'GarageCond', 'PavedDrive', 'PoolQC', 'Fence', 'MiscFeature',\n       'SaleType', 'SaleCondition')\n\n# process columns, apply LabelEncoder to categorical features\nfor c in cols:\n    lbl = LabelEncoder() \n    lbl.fit(list(train[c].values)) \n    train[c] = lbl.transform(list(train[c].values))\n    lbl.fit(list(test[c].values)) \n    test[c] = lbl.transform(list(test[c].values))","319c5aa8":"fig, axes = plt.subplots(ncols=5, nrows=2, figsize=(16, 4))\naxes = np.ravel(axes)\ncol_name = ['GrLivArea','TotalBsmtSF','1stFlrSF','BsmtFinSF1','LotArea']\nfor i, c in zip(range(5), col_name):\n    train.plot.scatter(ax=axes[i], x=c, y='SalePrice', sharey=True, colorbar=False, c='r')\n\n# delete outliers\nprint(train.shape)\ntrain = train[train['GrLivArea'] < 4500]\ntrain = train[train['LotArea'] < 100000]\ntrain = train[train['TotalBsmtSF'] < 3000]\ntrain = train[train['1stFlrSF'] < 2500]\ntrain = train[train['BsmtFinSF1'] < 2000]\n\nprint(train.shape)\n\nfor i, c in zip(range(5,10), col_name):\n    train.plot.scatter(ax=axes[i], x=c, y='SalePrice', sharey=True, colorbar=False, c='b')","c4b951b2":"# Deleting dominating features over 97%\ntrain=train.drop(columns=['Street', 'Utilities', 'Condition2', 'RoofMatl', 'Heating', 'PoolQC'])\ntest=test.drop(columns=['Street', 'Utilities', 'Condition2', 'RoofMatl', 'Heating', 'PoolQC'])","4144198b":"train = train.select_dtypes(include=[np.number])","ee67f458":"#train.drop(train[(train['OverallQual']<5) & (train['SalePrice']>200000)].index, inplace=True)\n#train.drop(train[(train['GrLivArea']>4000) & (train['SalePrice']<300000)].index, inplace=True)\n#train = train[train.GrLivArea < 4500]\n#train.shape","a7e16a0e":"train = train.dropna(thresh=0.70*len(train), axis=1)\n\ntest = test.dropna(thresh=0.70*len(test), axis=1)","3dc3c41a":"train = train.fillna(train.mean())\ntest = test.fillna(test.mean())","89ccfee2":"train.shape","e0fb71f6":"#from scipy import stats\n#import numpy as np\n#z = np.abs(stats.zscore(train))\n#print(z)","2ccdeff2":"#threshold = 2\n#print(np.where(z > 2))","85974762":"#train = train[(z < 2).all(axis=1)]","dff5011f":"#from pandas.api.types import is_numeric_dtype\n#from scipy import stats\n#for c in train.columns.values:\n#    if is_numeric_dtype(train[c]):\n#        z = np.abs(stats.zscore(train[c]))\n#        s = train[(z > 7) & (train[c] > 1000)].shape[0]\n#        if s <= 5 and s > 0:\n#            train.drop(train[(z > 7) & (train[c] > 1000)].index, inplace=True)\n#train.shape","f8c4ef57":"# function to detect outliers based on the predictions of a model\n#from sklearn.metrics import mean_squared_error\n#from lightgbm import LGBMRegressor\n#def find_outliers(model, X, y, sigma=3):\n#    model = lgb.LGBMRegressor(objective='regression', boosting_type = 'goss', \n#                               n_estimators =550, class_weight = 'balanced')\n#    # predict y values using model\n#    try:\n#        y_pred = pd.Series(model.predict(X), index=y.index)\n#    # if predicting fails, try fitting the model first\n#    except:\n#        model.fit(X,y)\n#        y_pred = pd.Series(model.predict(X), index=y.index)\n#        \n#    # calculate residuals between the model prediction and true y values\n#    resid = y - y_pred\n#    mean_resid = resid.mean()\n#    std_resid = resid.std()\n#\n#    # calculate z statistic, define outliers to be where |z|>sigma\n#    z = (resid - mean_resid)\/std_resid    \n#    outliers = z[abs(z)>sigma].index\n#    \n#    # print and plot the results\n#    print('R2=',model.score(X,y))\n#    print('rmse=',mean_squared_error(y, y_pred))\n#    print('---------------------------------------')\n#\n#    print('mean of residuals:',mean_resid)\n#    print('std of residuals:',std_resid)\n#    print('---------------------------------------')\n#\n#    print(len(outliers),'outliers:')\n#    print(outliers.tolist())\n#\n#    plt.figure(figsize=(15,5))\n#    ax_131 = plt.subplot(1,3,1)\n#    plt.plot(y,y_pred,'.')\n#    plt.plot(y.loc[outliers],y_pred.loc[outliers],'ro')\n#    plt.legend(['Accepted','Outlier'])\n#    plt.xlabel('y')\n#    plt.ylabel('y_pred');\n#\n#    ax_132=plt.subplot(1,3,2)\n#    plt.plot(y,y-y_pred,'.')\n#    plt.plot(y.loc[outliers],y.loc[outliers]-y_pred.loc[outliers],'ro')\n#    plt.legend(['Accepted','Outlier'])\n#    plt.xlabel('y')\n#    plt.ylabel('y - y_pred');\n#\n#    ax_133=plt.subplot(1,3,3)\n#    z.plot.hist(bins=50,ax=ax_133)\n#    z.loc[outliers].plot.hist(color='r',bins=50,ax=ax_133)\n#    plt.legend(['Accepted','Outlier'])\n#    plt.xlabel('z')\n#    \n#    plt.savefig('outliers.png')\n#    \n#    return outliers\n#\n#find_outliers(model, X, y, sigma=3)","e1ad9363":"#from sklearn.linear_model import Ridge\n## find and remove outliers using a Ridge model\n#outliers = find_outliers(Ridge(), X, y)\n#\n## permanently remove these outliers from the data\n#train = train.drop(outliers)\n","2b2fad8a":"train.shape","798c8ef0":"y = np.log(train.SalePrice)\nX = train.drop(['SalePrice', 'Id'], axis=1)\n","cd7db349":"# Threshold for removing correlated variables\nthreshold = 0.8\n\n# Absolute value correlation matrix\ncorr_matrix = X.corr().abs()\n\n# Upper triangle of correlations\nupper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(np.bool))\n\n# Select columns with correlations above threshold\nto_drop = [column for column in upper.columns if any(upper[column] > threshold)]\n\n# Remove the columns\nX = X.drop(columns = to_drop)","1aa6a4c1":"print(X.shape)\nprint(y.shape)","4fb33815":"from lightgbm import LGBMRegressor\nfrom sklearn.model_selection import train_test_split\n\ndef identify_zero_importance_features(X, y, iterations = 2):\n    \"\"\"\n    Identify zero importance features in a training dataset based on the \n    feature importances from a gradient boosting model. \n    \n    Parameters\n    --------\n    train : dataframe\n        Training features\n        \n    train_labels : np.array\n        Labels for training data\n        \n    iterations : integer, default = 2\n        Number of cross validation splits to use for determining feature importances\n    \"\"\"\n    \n    # Initialize an empty array to hold feature importances\n    feature_importances = np.zeros(X.shape[1])\n\n    # Create the model with several hyperparameters\n    model = LGBMRegressor(objective='regression',\n                              num_leaves=4,\n                              learning_rate=0.05, \n                              n_estimators=1250,\n                              max_bin=75, \n                              bagging_fraction=0.8,\n                              bagging_freq=9, \n                              feature_fraction=0.45,\n                              feature_fraction_seed=9, \n                              bagging_seed=12,\n                              min_data_in_leaf=3, \n                              min_sum_hessian_in_leaf=2)\n    \n    # Fit the model multiple times to avoid overfitting\n    for i in range(iterations):\n\n        # Split into training and validation set\n        train_features, valid_features, train_y, valid_y = train_test_split(X, y, \n                                                                            test_size = 0.25, \n                                                                            random_state = i)\n\n        # Train using early stopping\n        model.fit(train_features, train_y, early_stopping_rounds=100, \n                  eval_set = [(valid_features, valid_y)])\n\n        # Record the feature importances\n        feature_importances += model.feature_importances_ \/ iterations\n    \n    feature_importances = pd.DataFrame({'feature': list(X.columns), \n                            'importance': feature_importances}).sort_values('importance', \n                                                                            ascending = False)\n    \n    # Find the features with zero importance\n    zero_features = list(feature_importances[feature_importances['importance'] == 0.0]['feature'])\n    print('\\nThere are %d features with 0.0 importance' % len(zero_features))\n    \n    return zero_features, feature_importances\n\nzero_features, feature_importances = identify_zero_importance_features(X, y, iterations = 2)\nprint('zero_features:',zero_features)\nprint('feature_importances : ', feature_importances)","74fcefbd":"feature_importances.describe()","fce0d1e5":"#X = X.drop(zero_features, axis = 1)\n#test = test.drop(zero_features, axis =1)","e3a765c7":"pp =np.percentile(feature_importances['importance'], 20) \nprint(pp)","47864ce4":"to_drop = feature_importances[feature_importances['importance'] <= pp]['feature']\nX = X.drop(columns = to_drop)","6c16a713":"print(X.shape)\nprint(y.shape)","83172fe2":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)             ","a319ee74":"# Linear Regression\nfrom sklearn import linear_model\nfrom sklearn.metrics import r2_score\nfrom sklearn.metrics import mean_squared_error\n# Create linear regression object\nregr = linear_model.LinearRegression()\n\n# Train the model using the training sets\nregr.fit(X_train, y_train)\n\n# Make predictions using the testing set\ny_pred = regr.predict(X_test)\nprint('The accuracy of the Linear Regression is',r2_score(y_test,y_pred))\nprint ('RMSE is: \\n', mean_squared_error(y_test, y_pred))","77a89afb":"import xgboost as xgb\n\nxg_reg = xgb.XGBRegressor(learning_rate =0.01, n_estimators=5580, \n                                     max_depth=3,min_child_weight=0 ,\n                                     gamma=0, subsample=0.7,\n                                     colsample_bytree=0.7,\n                                     objective= 'reg:linear',nthread=4,\n                                     scale_pos_weight=1,seed=27, \n                                     reg_alpha=0.00006)\nxg_reg.fit(X_train,y_train)\n\npreds = xg_reg.predict(X_test)\nprint('The accuracy of the xgboost is',r2_score(y_test,preds))\nprint ('RMSE is: \\n', mean_squared_error(y_test,preds))","314c4cc0":"from sklearn.ensemble import GradientBoostingRegressor\ngbr_model = GradientBoostingRegressor(n_estimators=4780, learning_rate=0.01,\n                                   max_depth=10, max_features='sqrt',\n                                   min_samples_leaf=1, min_samples_split=250, \n                                   loss='huber', random_state =6).fit(X_train,y_train)\ngbr_preds = gbr_model.predict(X_test)\nprint('The accuracy of the Gradient boost is',r2_score(y_test,gbr_preds))\nprint ('RMSE is: \\n', mean_squared_error(y_test,gbr_preds))","20cc0369":"from lightgbm import LGBMRegressor\nfrom sklearn.preprocessing import RobustScaler\nfrom sklearn.pipeline import make_pipeline\nlgbm_model = LGBMRegressor(objective='regression',\n                              num_leaves=4,\n                              learning_rate=0.05, \n                              n_estimators=1250,\n                              max_bin=75, \n                              bagging_fraction=0.8,\n                              bagging_freq=9, \n                              feature_fraction=0.45,\n                              feature_fraction_seed=9, \n                              bagging_seed=12,\n                              min_data_in_leaf=3, \n                              min_sum_hessian_in_leaf=2).fit(X_train, y_train)\nlgbm_preds = lgbm_model.predict(X_test)\nprint('The accuracy of the lgbm Regressor is',r2_score(y_test,lgbm_preds))\nprint ('RMSE is: \\n', mean_squared_error(y_test,lgbm_preds))","d2339943":"from sklearn.linear_model import RidgeCV\nfrom sklearn.preprocessing import RobustScaler\nfrom sklearn.pipeline import make_pipeline\nr_alphas = [0.05, 0.1, 0.3, 1, 3, 5, 10, 15, 30]\nridge_model = make_pipeline(RobustScaler(), RidgeCV(alphas = r_alphas, cv =3)).fit(X_train, y_train)\n\nridge_preds = ridge_model.predict(X_test)\nprint('The accuracy of the ridge Regressor is',r2_score(y_test,ridge_preds))\nprint ('RMSE is: \\n', mean_squared_error(y_test,ridge_preds))   \n    ","e0f3e4a1":"from sklearn.linear_model import LassoCV\n\nalpha_lasso = np.logspace(-3, -1, 30)\n\n\nlasso_model = make_pipeline(RobustScaler(),\n                             LassoCV(max_iter=1e6,\n                                    alphas = alpha_lasso,\n                                    random_state = 1)).fit(X_train, y_train)\n\nlasso_preds = lasso_model.predict(X_test)\nprint('The accuracy of the lasso Regressor is',r2_score(y_test,lasso_preds))\nprint ('RMSE is: \\n', mean_squared_error(y_test,lasso_preds))   \n    ","7234725d":"from sklearn.linear_model import ElasticNetCV\n\ne_alphas = np.logspace(-3 -2, 30)\n\ne_l1ratio = [0.8, 0.85, 0.9, 0.95, 0.99, 1]\n\nelastic_model= make_pipeline(RobustScaler(), \n                           ElasticNetCV(max_iter=1e6, alphas=e_alphas, \n                                         l1_ratio=e_l1ratio)).fit(X_train, y_train)\n\nelastic_preds = elastic_model.predict(X_test)\nprint('The accuracy of the  Elastic Net CV is',r2_score(y_test,elastic_preds))\nprint ('RMSE is: \\n', mean_squared_error(y_test,elastic_preds))   \n    ","7e475b62":"from mlxtend.regressor import StackingCVRegressor\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import RobustScaler\n#setup models\nridge = make_pipeline(RobustScaler(), \n                      RidgeCV(alphas = r_alphas))\n\nlasso = make_pipeline(RobustScaler(),\n                      LassoCV(max_iter=1e6,\n                                    alphas = alpha_lasso,\n                                    random_state = 1)).fit(X_train,y_train)\n\nelasticnet = make_pipeline(RobustScaler(), \n                           ElasticNetCV(max_iter=1e6, alphas=e_alphas, \n                                         l1_ratio=e_l1ratio)).fit(X_train,y_train)\n\nlgbm_model = LGBMRegressor(objective='regression',\n                              num_leaves=4,\n                              learning_rate=0.05, \n                              n_estimators=1250,\n                              max_bin=75, \n                              bagging_fraction=0.8,\n                              bagging_freq=9, \n                              feature_fraction=0.45,\n                              feature_fraction_seed=9, \n                              bagging_seed=12,\n                              min_data_in_leaf=3, \n                              min_sum_hessian_in_leaf=2).fit(X_train,y_train)\n\ngbr_model = GradientBoostingRegressor(n_estimators=4780, learning_rate=0.01,\n                                   max_depth=10, max_features='sqrt',\n                                   min_samples_leaf=1, min_samples_split=250, \n                                   loss='huber', random_state =6).fit(X_train,y_train)\n\n#stack\nstack_gen = StackingCVRegressor(regressors=(ridge,\n                                            lasso, elasticnet, gbr_model,\n                                             lgbm_model), \n                               meta_regressor=gbr_model,\n                               use_features_in_secondary=True)\n\n#prepare dataframes\nstackX = np.array(X_train)\nstacky = np.array(y_train)\nstack_gen_model = stack_gen.fit(stackX, stacky)","77fb9bbd":"em_preds = elastic_model.predict(X_test)\nlasso_preds = lasso_model.predict(X_test)\nridge_preds = ridge_model.predict(X_test)\nstack_gen_preds = stack_gen_model.predict(X_test)\nlgbm_preds = lgbm_model.predict(X_test)\ngbr_preds = gbr_model.predict(X_test)","c54fe7d4":"print ('RMSE is: \\n', mean_squared_error(y_test,stack_gen_preds))\nprint('The accuracy of the stack is',r2_score(y_test,stack_gen_preds))\n","0e7ba767":"stack_preds_1 = ((0.1*em_preds) + (0.1*lasso_preds) + (0.1*ridge_preds) +(0.1 * gbr_preds ) \n               + (0.1*lgbm_preds) + (0.5*stack_gen_preds) )\nprint('The accuracy of the stack Regressor is',r2_score(y_test,stack_preds_1))\nprint ('RMSE is: \\n', mean_squared_error(y_test,stack_preds_1))   \n    ","59bdf299":"#feats = test.select_dtypes(include=[np.number]).interpolate().dropna()\nfeats = test.drop(['Id'], axis=1)\n\nfeats = feats[X_train.columns]","bc11addc":"lasso_preds = lasso_model.predict(feats)\nem_preds = elastic_model.predict(feats)\nridge_preds = ridge_model.predict(feats)\nstack_gen_preds = stack_gen_model.predict(feats)\nlgbm_preds = lgbm_model.predict(feats)\ngbr_preds = gbr_model.predict(feats)","0e402028":"stack_preds = ((0.1*em_preds) + (0.1*lasso_preds) + (0.1*ridge_preds) +(0.1 * gbr_preds ) \n               + (0.1*lgbm_preds) + (0.5*stack_gen_preds) )","54dc9cb7":"#predictions = model.predict(feats)\nfinal_predictions = np.exp(stack_preds)\nprint (\"Original predictions are: \\n\", stack_preds[:5], \"\\n\")\nprint (\"Final predictions are: \\n\", final_predictions[:5])","6cd44b53":"submission = pd.DataFrame()\nsubmission['Id'] = test.Id\nsubmission['SalePrice'] = final_predictions \nsubmission.to_csv('submission1.csv', index=False)","4a03d180":"Let's have a look at the distribution of 'SalePrice' by plotting a simple histogram","2d458889":"Cleaning missing data\n\nIn statistics, missing data, or missing values, occur when no data value is stored for the variable in an observation. Missing data are a common occurrence and can have a significant effect on the conclusions that can be drawn from the data. The goal of cleaning operations is to prevent problems caused by missing data that can arise when training a model.","fed62e73":"Working with Numeric Features","f854c189":"**2. Remove Collinear Variables:**\nCollinear variables are variables that are highly correlated with one another. ","c7b73ae8":"**1. Remove Missing Columns :**\nwe\u2019ll remove any columns with more than 70% missing values in either the training or testing set.\n\n","9bc41278":"Transforming and engineering features\nHere we are using label encoding.\nLabel encoding refers to transforming the word labels into numerical form so that the algorithms can understand how to operate on them.","848772d9":"**OUTLIERS** : -\n\nFocusing on outliers, defined by Gladwell as people who do not fit into our normal understanding of achievement. Outliers deals with exceptional people, especially those who are smart, rich, and successful, and those who operate at the extreme outer edge of what is statistically plausible. An outlier is a data point that is distant from other similar points. They may be due to variability in the measurement or may indicate experimental errors. If possible, outliers should be excluded from the data set. We'll do a quick analysis through the standard deviation of 'SalePrice' and a set of scatter plots.","b70f10e4":" non-numeric Features","49a15a7e":"use np.log() to transform train.SalePric and calculate the skewness and kurtosis again.","98770b30":"Skewness: -\n\nThe term \u2018skewness\u2019 is used to mean the absence of symmetry from the mean of the dataset. Skewness is used to indicate the shape of the distribution of data.In a skewed distribution, the curve is extended to either left or right side. So, when the plot is extended towards the right side more, it denotes positive skewness. On the other hand, when the plot is stretched more towards the left direction, then it is called as negative skewness.\n\nKurtosis:-\nIn statistics, kurtosis is defined as the parameter of relative sharpness of the peak of the probability distribution curve.It is used to indicate the flatness or peakedness of the frequency distribution curve and measures the tails or outliers of the distribution.Positive kurtosis represents that the distribution is more peaked than the normal distribution, whereas negative kurtosis shows that the distribution is less peaked than the normal distribution.","914c0fde":"**Drop Columns having features importance less than 20%**"}}