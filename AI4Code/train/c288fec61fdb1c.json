{"cell_type":{"272d83b1":"code","0e013f63":"code","672e7dc3":"code","265457dc":"code","0d613780":"code","890c55ba":"code","046694e6":"code","0391c7ab":"code","02722579":"code","ec2aa255":"code","7425381d":"code","7dd06058":"code","3fb27354":"code","88862298":"code","821bd788":"markdown","ad166fce":"markdown","a8430bf2":"markdown","420d311a":"markdown","5af2f283":"markdown","3bb4bb2d":"markdown","36327dcb":"markdown","68cfc665":"markdown","4a036c63":"markdown","77df871a":"markdown","d69fc0b2":"markdown","f1a32e39":"markdown","1b64401c":"markdown","7e15515f":"markdown"},"source":{"272d83b1":"!git clone https:\/\/github.com\/scaleapi\/pandaset-devkit.git\n!pip install pandaset-devkit\/python\/","0e013f63":"from pandaset import DataSet\n\ndataset = DataSet('\/kaggle\/input\/pandaset-dataset')\n\nprint(\"The sequences that have (cuboid + semantic) segmentation annotations:\\n\", dataset.sequences(with_semseg=True))","672e7dc3":"# Choosing the key value from the above result\nseq002 = dataset['002']\nprint(seq002.__dict__)\n","265457dc":"seq002.load()","0d613780":"#print(seq002.__dict__)\n\nimport plotly.graph_objects as go \n  \nheaderColor = 'rgb(116,0,4)'\n\nannon_class = []\nfor key in seq002.semseg.classes:\n    annon_class.append(seq002.semseg.classes[key])\n  \nfig = go.Figure(data=[go.Table( \n    header = dict(values=['<b>Semantic Segmentation Annotations for Sequence 002<\/b>'],\n                  line_color='darkslategray',\n                  fill_color=headerColor,\n                  align=['left','center'],\n                  font=dict(color='white', size=12)\n                 ), \n    cells = dict(values=[annon_class],\n                 line_color='darkslategray',\n                 align = ['left', 'center'],\n                 font = dict(color = headerColor, size = 11)\n                )) \n                     ]) \nfig.show() ","890c55ba":"# This is what lidar data looks like\n#print (seq002.lidar.__dict__)\nlidar_poses = seq002.lidar._poses\nprint(lidar_poses[0])\n\n#print(seq002.lidar._poses)\npoints3d_lidar_xyz = seq002.lidar.data#[1].to_numpy()[:, :3]","046694e6":"import csv\nimport pickle\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nimport os\n    \npfile = \"..\/input\/pandaset-dataset\/018\/lidar\/05.pkl\"\nwith open(pfile, \"rb\") as fin:\n    data = pickle.load(fin)\n\ndata.info()","0391c7ab":"import numpy as np\nimport matplotlib.pyplot as plt\nfrom mpl_toolkits import mplot3d\nimport random\n\nax = plt.axes(projection='3d')\n\nxyz = data[['x', 'y', 'z']].to_numpy()\ncolor_maps = [(random.random(), random.random(), random.random()) for _ in range(1000 + 1)]\nax.scatter(xyz[:,0], xyz[:,1], xyz[:,2])","02722579":"figsize = plt.rcParams.get('figure.figsize')\nfig = plt.figure(figsize=(figsize[0] * 2, figsize[1]))\nax1 = fig.add_subplot(1, 2, 1, projection = '3d')\nax2 = fig.add_subplot(1, 2, 2, projection = '3d')\nax1.axis(\"off\")\nax1.view_init(90, -90) # front view\nax1.scatter(xyz[:,0], xyz[:,1], xyz[:,2])\nax2.axis(\"off\")\nax2.view_init(90 + 90, -90) # top view\nax2.scatter(xyz[:,0], xyz[:,1], xyz[:,2])\nplt.show()","ec2aa255":"cameras = []\nfor key in seq002.camera.keys():\n    cameras.append(key)\n  \nfig = go.Figure(data=[go.Table( \n    header = dict(values=['<b>Cameras used for Sequence 002<\/b>'],\n                  line_color='darkslategray',\n                  fill_color=headerColor,\n                  align=['left','center'],\n                  font=dict(color='white', size=12)\n                 ), \n    cells = dict(values=[cameras],\n                 line_color='darkslategray',\n                 align = ['left', 'center'],\n                 font = dict(color = headerColor, size = 11)\n                )) \n                     ]) \nfig.show() ","7425381d":"img0 = seq002.camera['front_camera']._data_structure[0]\nimport PIL\nPIL.Image.open(img0)\n\n# To view image taken by the left camera uncomment this\n#PIL.Image.open(seq002.camera['left_camera']._data_structure[0])\n","7dd06058":"print (\"Coordinates for Sequence 002 [ Latitude: \", seq002.gps[0]['lat'], \", Longitude:\", seq002.gps[0]['long'], \"]\") \n","3fb27354":"# Uninstalling devkit since it changed the nbconvert version & caused exception while final conversion\n!echo \"y\" | pip uninstall pandaset\n!rm -rf pandaset-devkit","88862298":"#Reverting to original version\n!pip install nbconvert==6.0.7","821bd788":"# The PandaSet\n\nTo understand what is PandaSet, let us first see what are the different types of LiDAR.\n\n![types.png](attachment:types.png)\n\nThe above figure is easy to understand and shows where we should focus when it comes to autonomous driving, and that is <span style=\"font-size:14px;color:maroon\">Mobile Solid-state LiDAR<\/span>. This method can be employed to acquire millions of 3D design points per minute. The mobile laser can offer a data density of up to 150 points per square foot, making it the fastest way to acquire coordinates compared to all other LiDAR methods.\n\n<br>\n\n<span style=\"font-size:14px;color:maroon\">PandaSet is a public large-scale dataset for autonomous driving published by Scale AI<\/span>. It was collected using a Chrysler Pacifica minivan mounted with a suite of cameras and Hesai LiDARs. The cameras and sensors were positioned like this\n![cams.png](attachment:cams.png)\n\nAccording to the Scale AI team, the data was collected using a forward-facing LIDAR with image-like resolution called PandarGT as well as a mechanical spinning LIDAR called Pandar64. The collected data was annotated with a combination of cuboid and segmentation annotation that is called <b>Scale 3D Sensor Fusion Segmentation<\/b>.\n\n### Okay. Now what is the big deal about PandaSet?\nHere is the list of points that tips the scales towards PandaSet when we talk about LiDAR datasets:\n* It is the first open-source autonomous driving dataset made available for both academic and commercial use. It contains 48,000 camera images and 16,000 LiDAR sweeps\n* It was collected after meticulous planning of routes and scenes to include complex urban driving scenarios, with steep hills, construction, dense traffic and pedestrians. The two routes that were used: (1) San Francisco; and (2) El Camino Real from Palo Alto to San Mateo, met the criteria\n* A variety of times of day and lighting conditions typical of morning, afternoon, dusk and evening were covered\n* The biggest strength of this dataset: <span style=\"font-size:18px;color:maroon\">Annotation<\/span>\n<br><b>28 annotation classes and 37 semantic segmentation labels<\/b> were achieved using a combination of human work and its review with smart tools, statistical confidence checks and machine learning checks\n***\n<br>All right! So let us dive into the PandaSet data that has been hosted here.\n\n# Structure of PandaSet\nThe Pandaset contains 47 directories numbered 001 till 047, each corresponding to a scene. A scene in this case will have a <span style=\"color:maroon\">cuboid segmentation annotation, an optional semantic segmentation annotation, Pickle files of data collected by the LiDAR sensors, JPG images collected by the cameras, and the metadata relevant to the scene<\/span>.\nThe figure below shows what you can expect inside each of these directories.\n\n![dirstruc.png](attachment:dirstruc.png)\n\nIt should be noted that the semseg directory that contains semantic segmentation is available for specific scenes only. While directories cuboids, semseg and lidar contain .pkl and .pkl.gz files; camera sub-directories contains image files in jpg format.\n***","ad166fce":"Displaying one of the cool images captured by the front camera.","a8430bf2":"<span style=\"font-size:18px;color:maroon\">Metadata<\/span>\n\nIn addition to the sensor data, the loaded dataset also contains the following meta information:\n* GPS Positions\n* Timestamps\n\nThese can be directly accessed through the known list slicing operations, and read in their dict format","420d311a":"****The PandaSet dataset is a public large-scale for autonomous driving provided by Hesai and Scale AI.****\n<br>The intent behind this dataset is to encourage researchers and analysts like us to study urban driving situations. It was collected by a real self-driving car that was equipped with cameras and LiDAR sensors.\n<br>Now you would ask the million dollar question\n\n# What is LiDAR?\n<span style=\"font-size:14px;color:maroon\">Light Detection and Ranging (LiDAR) aka Laser Radar<\/span>\n<br>The Wikipedia definition of LiDAR says that it is a method for determining ranges (variable distance) by targeting an object with a laser and measuring the time for the reflected light to return to the receiver. It is commonly used to make high-resolution maps, with applications in surveying, geodesy, geomatics, archaeology, geography, geology, geomorphology, seismology, forestry, atmospheric physics, laser guidance, airborne laser swath mapping (ALSM), and laser altimetry. The technology is also used in control and navigation for some autonomous cars.\n<br>A typical LiDAR system consists of a transmitter that emits short pulses of ultra-violet, visible and infra-red electro-magnetic radiation that hit objects and get scattered. Parts of this radiation is received and the travel time of pulses is computed as a part of LiDAR. A bare bones explanation of a LiDAR system would look like this:\n![arch.png](attachment:arch.png)\n***\n<br><span style=\"font-size:14px;color:maroon\">Tracing the history of LiDAR<\/span>\n<br>The use of light beams for spatial measurements had been in use even before the invention of lasers. In the year 1962, MIT scientists measured the distance between Earth and the Moon using a reflected laser beam thereby marking a promising application of lasers. Three years later, Ronald Collins of Stanford Research Institute filed a patent for a laser-radar LIDAR system that could be used to study Earth's atmosphere and weather. In the year 1971, Apollo 15 astronauts used LIDAR to map the surface of the Moon. By the 1990s, thanks to the works of Optech, LIDAR was widely being used for geographical mapping.\n\nWhile LiDAR was applied to space research by NASA to study the atmosphere from space including that of Mars. It was in the year 2005, when the US military's DARPA Grand Challenge resulted in LIDAR systems finding their place as the eyes behind self-driving cars. Now it is not just self-driving cars but smart vacuums and even iPhone12 Pro that use this technology.\n\nLet me present to you a roomba that is doing its job to the fullest:\n\n![](https:\/\/media0.giphy.com\/media\/hmGQKkNaUIgHS\/200.gif)\n\n*******","5af2f283":"Now picking a sequence from the above list for further exploration, an instance of [Sequence](https:\/\/scaleapi.github.io\/pandaset-devkit\/sequence.html#pandaset.sequence.Sequence) class should be created.<br>Its contents would look like this","3bb4bb2d":"After installation, simply import and instantiate the [DataSet](https:\/\/scaleapi.github.io\/pandaset-devkit\/dataset.html#pandaset.dataset.DataSet) class using the directory path \/kaggle\/input\/\n\n<br>Let's find the scenes, henceforth called sequences, in the dataset that contain both, cuboid and semantic, segmentation annotations.","36327dcb":"Plotting the point clouds in 3-D space","68cfc665":"***\n\n<span style=\"font-size:18px;color:maroon\">Camera images<\/span>\n\n<br>The recording vehicle was equipped with multiple cameras and the captured images were also published as part of PandaSet.\n<br>Listing down which cameras were used to record the sequence 002","4a036c63":"<span style=\"font-size:18px;color:maroon\">The LiDAR point clouds<\/span>\n<br>The LiDAR points are stored in a world coordinate system.\n<br>This means that a common reference frame exists for all objects in the scene, therefore it is not required to transform point clouds using the vehicle's pose graph.\n\n\n<br>In addition to the LiDAR points, the lidar property also holds the sensor pose in world coordinate system for every LiDAR frame recorded.\nHere is an example of one of the sensor pose in lidar property of Sequence 002.","77df871a":"The pose data and the timestamp for the recorded LiDAR frames are also present in JSON format.\n<br>The name of the JSON file has been saved as ._poses_structure in the lidar property.\n\n<br>Another way to read the pointclouds of LiDAR frames is to read the pickle files provided in PandaSet.\n<br>This time let us check out what sequence numbered 018 holds. Randomly picking one pickle file and reading it in a Pandas dataframe.","d69fc0b2":"***\n\nSources:\n* [LIDAR](https:\/\/www.explainthatstuff.com\/lidar.html)\n\n* [LiDAR (Light Detection and Ranging)- Types, Architecture, How it works](https:\/\/electricalfundablog.com\/lidar-light-detection-and-ranging-types-architecture\/)\n* [Demystifying LiDAR \u2013 An overview of LiDAR technology, its types, and key applications](https:\/\/www.blickfeld.com\/blog\/what-is-lidar-technology\/)\n* [PandaSet](https:\/\/scale.com\/open-datasets\/pandaset)\n","f1a32e39":"Here is another way to make 3-D and 2-D plots of the point clouds","1b64401c":"The above access statement searched the sequence directory for available sensor data, metadata and annotations and prepared the directory to be loaded explicitly.<br>At this point, **no point clouds or images had been loaded into memory**. To execute the loading of sensor data and metadata into memory, we need to call the **load()** method on the sequence object. This will load all available sensor data and metadata.\n\n<br>Now let's load the sequence 002 and list the annontation classes for semantic segmentation.","7e15515f":"# The PandaSet Devkit\nScale AI have also published a [devkit](https:\/\/github.com\/scaleapi\/pandaset-devkit) for exploration and use of the PandaSet. \nIt can be cloned from the GIT repository and installed."}}