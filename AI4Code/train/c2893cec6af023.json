{"cell_type":{"b0e40b92":"code","6d7498bb":"code","6180794f":"code","0b213c4b":"code","2a884072":"code","2e26a389":"code","6b85972c":"code","7be19bdd":"code","8bbe5552":"code","489462d0":"code","7594306b":"code","337b3204":"code","8a30deff":"code","fa96857e":"code","8273acb1":"code","8e87db3b":"code","1dcc7877":"code","1d57cc70":"code","301002c2":"code","7ba665a6":"code","18f5e943":"code","7be904be":"code","306ed93d":"code","2b78e01a":"code","ed9387ca":"code","979fa868":"code","e7f317bd":"code","e79ea392":"code","c4215b9e":"code","8ee1defa":"code","f89ca89a":"code","b53bf39d":"code","a0b93f77":"code","d043d66e":"code","eba67273":"code","fadf19da":"code","bf8842c6":"code","adc92f7a":"code","cef56962":"code","0f01e746":"code","904c0363":"code","9565d7a5":"code","7db987a0":"code","6d9dcaf7":"code","06b1a780":"code","d18adc0c":"code","cf1c5609":"code","f9e9c8a7":"code","724c01f4":"code","eae507d9":"code","2bec9241":"code","6c4ef4b7":"code","defcd1d4":"code","9c510794":"code","9784d04c":"code","7a1cde06":"code","54e47b9f":"code","d4c596ff":"code","6ae1e5cb":"code","81f8f895":"code","b59274f8":"code","d6945132":"code","a9fc0eee":"code","6ec5644e":"code","9e3ef1c8":"code","1ce07c97":"code","bac1b687":"code","4781ab1a":"markdown","e25e9f95":"markdown","2f92582c":"markdown","d19da8fe":"markdown","224bdced":"markdown","5bd6373d":"markdown","a2ff8b8d":"markdown","d57f41e2":"markdown","b5ce7f0a":"markdown","dd23c202":"markdown","6b75edfa":"markdown","c84a2c87":"markdown","ae811766":"markdown","bb766087":"markdown","053485d4":"markdown","c9418fe3":"markdown","6dbdd838":"markdown","06da6a95":"markdown","9f69b7cb":"markdown","3173c28e":"markdown","c9b8db29":"markdown","ea21427c":"markdown","c74ca683":"markdown","30477322":"markdown","ec07125d":"markdown","8895d7ba":"markdown","f2379f35":"markdown","4ebae8c3":"markdown","816f3d1c":"markdown","faeac41d":"markdown","87975a49":"markdown","e59b48ea":"markdown","7b33883a":"markdown","73beee16":"markdown","21bea40f":"markdown","32cab44b":"markdown","3c9ebfe3":"markdown","35015ae7":"markdown"},"source":{"b0e40b92":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","6d7498bb":"import pandas as pd \nimport numpy as np \nfrom sklearn.ensemble import BaggingClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import GridSearchCV\nimport missingno as msno\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import classification_report, confusion_matrix\nfrom sklearn.svm import SVC\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.preprocessing import StandardScaler, MinMaxScaler, Normalizer","6180794f":"train=pd.read_csv('\/kaggle\/input\/titanic\/train.csv')\ntrain_label=train.Survived","0b213c4b":"test=pd.read_csv('\/kaggle\/input\/titanic\/test.csv')","2a884072":"train.info()","2e26a389":"train.describe().transpose()\n","6b85972c":"train.head()\n","7be19bdd":"train.tail()\n","8bbe5552":"test.info()","489462d0":"test.describe().transpose()","7594306b":"test.head()","337b3204":"test.tail()","8a30deff":"Total_DF=pd.concat([train,test])\nTotal_DF\n","fa96857e":"print('Train:',train.shape)\nprint('Test:',test.shape)","8273acb1":"train.columns","8e87db3b":"#msno.matrix(train)\n\n#The function is used to determine the type of the column:Categorical or Numerical  \ndef utils_recognize_type(dtf, col, max_cat):\n    if (dtf[col].dtype == \"O\") | (dtf[col].nunique() < max_cat):\n        return \"cat\"\n    else:\n        return \"num\"","1dcc7877":"dic_cols = {col:utils_recognize_type(train, col, max_cat=10) for col in train.columns}\nheatmap = train.isnull()\nfor k,v in dic_cols.items():\n    if v == \"num\":\n        heatmap[k] = heatmap[k].apply(lambda x: 0.5 if x is False else 1)\n    else:\n        heatmap[k] = heatmap[k].apply(lambda x: 0 if x is False else 1)\nsns.heatmap(heatmap, cbar=False).set_title('Train Dataset Overview')\nplt.show()\nprint(\"\\033[1;37;40m Categerocial \", \"\\033[1;30;41m Numeric \", \"\\033[1;30;47m NaN \")","1d57cc70":"dic_cols = {col:utils_recognize_type(test, col, max_cat=10) for col in test.columns}\nheatmap = test.isnull()\nfor k,v in dic_cols.items():\n    if v == \"num\":\n        heatmap[k] = heatmap[k].apply(lambda x: 0.5 if x is False else 1)\n    else:\n        heatmap[k] = heatmap[k].apply(lambda x: 0 if x is False else 1)\nsns.heatmap(heatmap, cbar=False).set_title('Test Dataset Overview')\nplt.show()\nprint(\"\\033[1;37;40m Categerocial \", \"\\033[1;30;41m Numeric \", \"\\033[1;30;47m NaN \")","301002c2":"print('Train: Number of tickets:', train.Ticket.nunique())\nprint('Train: Number of Embarkation Ports:',train.Embarked.nunique())\n######################################################################\nprint('Test: Number of tickets:', test.Ticket.nunique())\nprint('Test: Number of Embarkation Ports:',test.Embarked.nunique())\n","7ba665a6":"del train['Cabin']\ndel test['Cabin']","18f5e943":"train[\"Age\"] = train[\"Age\"].fillna(train[\"Age\"].mean())\ntest[\"Age\"] = test[\"Age\"].fillna(test[\"Age\"].mean())\ntest[\"Fare\"] = test[\"Fare\"].fillna(test[\"Fare\"].mean())","7be904be":"train.dtypes","306ed93d":"train.Sex = pd.Categorical(train.Sex)\ntrain['code_Sex'] = train.Sex.cat.codes\ntest.Sex = pd.Categorical(test.Sex)\ntest['code_Sex'] = test.Sex.cat.codes\n################################################\ntrain.Ticket = pd.Categorical(train.Ticket)\ntrain['code_Ticket'] = train.Ticket.cat.codes\ntest.Ticket = pd.Categorical(test.Ticket)\ntest['code_Ticket'] = test.Ticket.cat.codes\n################################################\ntrain.Embarked = pd.Categorical(train.Embarked)\ntrain['code_Embarked'] = train.Embarked.cat.codes\ntest.Embarked = pd.Categorical(test.Embarked)\ntest['code_Embarked'] = test.Embarked.cat.codes\n","2b78e01a":"del train['Survived']\ntrain=train[['PassengerId', 'Pclass', 'code_Sex', 'Age', 'SibSp', 'Parch','code_Ticket', 'Fare', 'code_Embarked']]\ntest=test[['PassengerId', 'Pclass', 'code_Sex', 'Age', 'SibSp', 'Parch','code_Ticket', 'Fare', 'code_Embarked']]","ed9387ca":"plt.subplot(1,2,1)\nplt.title('train')\nsns.boxplot(x=train['Age'])\nplt.subplot(1,2,2)\nsns.boxplot(x=train['Fare'])","979fa868":"plt.subplot(1,2,1)\nplt.title('test')\nsns.boxplot(x=test['Age'])\nplt.subplot(1,2,2)\nsns.boxplot(x=test['Fare'])","e7f317bd":"Q3=train.quantile(0.75)\nQ1=train.quantile(0.25)\nIQR=Q3-Q1\n\nclean_iqr_DF = train[~((train < (Q1 - 1.5 * IQR)) |(train > (Q3 + 1.5 * IQR))).any(axis=1)]\nclean_iqr_DF=clean_iqr_DF.reset_index(drop=True)\nclean_iqr_DF\n","e79ea392":"class_0=np.where(train_label==0)# Survived\nclass_1=np.where(train_label==1) # Died\n\nprint('The number of survived is :',len(class_1[0]))\nprint('The number of died is :',len(class_0[0]))\n","c4215b9e":"ax = train_label.value_counts().sort_values().plot(kind=\"barh\")\ntotals= []\nfor i in ax.patches:\n    totals.append(i.get_width())\ntotal = sum(totals)\nfor i in ax.patches:\n     ax.text(i.get_width()+.3, i.get_y()+.20, \n     str(round((i.get_width()\/total)*100, 2))+'%', \n     fontsize=10, color='black')\nax.grid(axis=\"x\")\nplt.suptitle('Class samples', fontsize=20)\nplt.show()\n","8ee1defa":"Total_DF_gr=Total_DF.groupby(['Age']).count()\nTotal_DF_gr=Total_DF_gr.reset_index(level=['Age'],drop=False)","f89ca89a":"DF_Survived=Total_DF[Total_DF.Survived==1]\nDF_Survived_gr=DF_Survived.groupby(['Age']).count()\nDF_Survived_gr=DF_Survived_gr.reset_index(level=['Age'],drop=False)","b53bf39d":"DF_Died=Total_DF[Total_DF.Survived==0]\nDF_Died_gr=DF_Died.groupby(['Age']).count()\nDF_Died_gr=DF_Died_gr.reset_index(level=['Age'],drop=False)","a0b93f77":"width = 0.8\nfig, ax = plt.subplots(figsize=(15,5))\n#ax.bar(Total_DF_gr.Age, Total_DF_gr.PassengerId,width, label='Total')\nax.bar(DF_Died_gr.Age, DF_Died_gr.PassengerId, width, label='Died')\nax.bar(DF_Survived_gr.Age, DF_Survived_gr.PassengerId, width, label='Survived')\n# Add some text for labels, title and custom x-axis tick labels, etc.\nax.set_ylabel('Number of passenger')\nax.set_title('Survived Vs Died passenger number')\nax.legend()","d043d66e":"list_ports=[c for c in DF_Survived.Embarked.unique()]\nlist_ports\n\nSurvived_gr_EP=DF_Survived.groupby(['Embarked']).count()\nSurvived_gr_EP=Survived_gr_EP.reset_index(level=['Embarked'],drop=False)\nDied_gr_EP=DF_Died.groupby(['Embarked']).count()\nDied_gr_EP=Died_gr_EP.reset_index(level=['Embarked'],drop=False)","eba67273":"width = 0.35\nx = np.arange(len(list_ports[:-1])) \nfig, ax = plt.subplots()\nax.bar(x - width\/2, Survived_gr_EP.PassengerId, width, label='Died')\nax.bar(x + width\/2, Died_gr_EP.PassengerId, width, label='Survived')\n# Add some text for labels, title and custom x-axis tick labels, etc.\nax.set_ylabel('Number of passenger')\nax.set_xticks(x)\nax.set_title('Survived Vs Died passenger number per Embarkation port')\nax.legend()\nax.set_xticklabels(list_ports[:-1])\nfig.tight_layout()\nplt.show()","fadf19da":"Su_gr_Sex=DF_Survived.groupby(['Sex']).count()\nSu_gr_Sex=Su_gr_Sex.reset_index(level=['Sex'],drop=False)\nDi_gr_Sex=DF_Died.groupby(['Sex']).count()\nDi_gr_Sex=Di_gr_Sex.reset_index(level=['Sex'],drop=False)","bf8842c6":"width = 0.35\nlabels=['female','male']\nx = np.arange(len(labels)) \nfig, ax = plt.subplots()\nax.bar(x - width\/2, Su_gr_Sex.PassengerId, width, label='Suvived')\nax.bar(x + width\/2, Di_gr_Sex.PassengerId, width, label='Died')\n# Add some text for labels, title and custom x-axis tick labels, etc.\nax.set_ylabel('Number of passenger')\nax.set_xticks(x)\nax.set_xticklabels(labels)\nax.set_title('Survived Vs Died passenger number per sex')\nax.legend()\nfig.tight_layout()\nplt.show()","adc92f7a":"Su_gr_class=DF_Survived.groupby(['Pclass']).count()\nSu_gr_class=Su_gr_class.reset_index(level=['Pclass'],drop=False)\nDi_gr_class=DF_Died.groupby(['Pclass']).count()\nDi_gr_class=Di_gr_class.reset_index(level=['Pclass'],drop=False)","cef56962":"width = 0.35\nlabels=[1,2,3 ]\nx = np.arange(len(labels)) \nfig, ax = plt.subplots()\nax.bar(x - width\/2, Su_gr_class.PassengerId, width, label='Suvived')\nax.bar(x + width\/2, Di_gr_class.PassengerId, width, label='Died')\n# Add some text for labels, title and custom x-axis tick labels, etc.\nax.set_ylabel('Number of passenger')\nax.set_xticks(x)\nax.set_xticklabels(labels)\nax.set_title('Survived Vs Died passenger number per Ticket class')\nax.legend()\nfig.tight_layout()\nplt.show()","0f01e746":"X_train, X_test, y_train, y_test = train_test_split(train, train_label, test_size=0.2, random_state=30)","904c0363":"normalizer = Normalizer()\nnormalized_train_X = normalizer.fit_transform(X_train)### we fit the normalizer on training data \nnormalized_train_X\n\nnormalized_test_X = normalizer.transform(X_test)\nnormalized_test_X","9565d7a5":"clf=DecisionTreeClassifier(criterion=\"gini\",random_state = 42) # implementation of the hierarchical\/decision trees\nclf.fit(X_train, y_train)\ny_predDT=clf.predict(X_test)\n# The score method returns the accuracy of the model\naccuracy_score(y_test,clf.predict(X_test))","7db987a0":"clf=DecisionTreeClassifier(criterion=\"gini\",random_state = 42) # implementation of the hierarchical\/decision trees\nclf.fit(normalized_train_X, y_train)\ny_predDT=clf.predict(normalized_test_X)\n# The score method returns the accuracy of the model\naccuracy_score(y_test,clf.predict(normalized_test_X))","6d9dcaf7":"clf1=DecisionTreeClassifier(random_state = 42) # implementation of the hierarchical\/decision trees\nparam_grid={'splitter': ['best','random'] ,'max_depth':[10,12,13,14,15],\n            'min_samples_split':[2,3,4,5], 'criterion':['gini', 'entropy'],\n           'max_features':[5,7,9,'sqrt','log2']}\ngrid_search=GridSearchCV(clf1, param_grid, n_jobs=-1,cv=5, scoring='accuracy')\ngrid_search.fit(X_train,y_train)\n\ngrid_search.best_params_ ","06b1a780":"best_CLF=DecisionTreeClassifier(criterion='entropy',random_state = 42,max_depth=12,\n                                max_features='sqrt',min_samples_split=3,splitter='random')\nbest_CLF.fit(X_train, y_train)\naccuracy_score(y_test,best_CLF.predict(X_test))","d18adc0c":"LR = LogisticRegression(solver='sag').fit(X_train, y_train)\ny_predLR=LR.predict(X_test)\naccuracy_score(y_test, y_predLR)\nprint('Logistic regression:',accuracy_score(y_test, y_predLR))","cf1c5609":"LR = LogisticRegression(solver='sag').fit(normalized_train_X, y_train)\ny_predLR=LR.predict(normalized_test_X)\naccuracy_score(y_test, y_predLR)\nprint('Logistic regression:',accuracy_score(y_test, y_predLR))","f9e9c8a7":"LR1 = LogisticRegression(random_state=42)\nparam_grid = {'C': [0.001, 0.01, 0.1, 1, 10, 100, 1000],'solver':['newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga'], 'penalty' : ['none', 'l1', 'l2', 'elasticnet'] }\ngrid_search=GridSearchCV(LR1, param_grid, n_jobs=-1,cv=5, scoring='accuracy')\ngrid_search.fit(X_train,y_train)","724c01f4":"grid_search.best_params_ ","eae507d9":"LR_best = LogisticRegression(random_state=42,C= 1, penalty='l1', solver='liblinear')\nLR_best.fit(X_train, y_train)\naccuracy_score(y_test,LR_best.predict(X_test))","2bec9241":"RF = RandomForestClassifier(criterion='gini')\nRF.fit(X_train, y_train.values.ravel())\npred_RF = RF.predict(X_test)\nprint('RF',accuracy_score(pred_RF,y_test))","6c4ef4b7":"RF = RandomForestClassifier(criterion='gini')\nRF.fit(normalized_train_X, y_train.values.ravel())\npred_RF = RF.predict(normalized_test_X)\nprint('RF',accuracy_score(pred_RF,y_test))","defcd1d4":"param_grid = { 'criterion' :['gini', 'entropy'], 'max_features': ['None', 'sqrt', 'log2'],\n              'n_estimators': [120,200,300],'random_state': [42,60,85]}\nRF_GS = RandomForestClassifier()\ngrid_search=GridSearchCV(RF_GS, param_grid, n_jobs=-1,cv=5, scoring='accuracy')\ngrid_search.fit(X_train,y_train)\ngrid_search.best_params_ ","9c510794":"RF_best = RandomForestClassifier(criterion='gini', random_state =60, max_features='sqrt', n_estimators=120)\nRF_best.fit(X_train, y_train)\naccuracy_score(y_test,RF_best.predict(X_test))","9784d04c":"svclassifier=SVC(kernel='linear',random_state=80, gamma='auto')\n# train the classifier\nsvclassifier.fit(X_train,y_train)\ny_predSVM=svclassifier.predict(X_test)\nprint('SVC:',accuracy_score(y_test, y_predSVM))","7a1cde06":"svclassifier=SVC(kernel='linear',random_state=80, gamma='auto')\n# train the classifier\nsvclassifier.fit(normalized_train_X,y_train)\ny_predSVM=svclassifier.predict(normalized_test_X)\nprint('SVC:',accuracy_score(y_test, y_predSVM))","54e47b9f":"param_grid = { 'kernel':['linear', 'rbf', 'sigmoid'], 'gamma':['scale', 'auto'],'C': [0.001, 0.10, 0.1, 10, 25]}","d4c596ff":"# SVC_GS = SVC(random_state=42)\n# grid_search=GridSearchCV(SVC_GS, param_grid, n_jobs=-1,cv=5, scoring='accuracy')\n# grid_search.fit(X_train,y_train)","6ae1e5cb":"# grid_search.best_params_","81f8f895":"# SVC_best = SVC(criterion='gini', random_state =60, max_features='sqrt', n_estimators=120)\n# SVC_best.fit(X_train, y_train)\n# accuracy_score(y_test,SVC_best.predict(X_test))","b59274f8":"knn = KNeighborsClassifier(n_neighbors=5)\nknn.fit(X_train, y_train.values.ravel())\npred_KNN = knn.predict(X_test)\nprint('KNN',accuracy_score(y_test, pred_KNN))","d6945132":"knn1 = KNeighborsClassifier(n_neighbors=5)\nknn1.fit(normalized_train_X, y_train.values.ravel())\npred_KNN = knn1.predict(normalized_test_X)\nprint('KNN',accuracy_score(y_test, pred_KNN))\n\n","a9fc0eee":"param_grid ={'n_neighbors':[5,6,7,8,9,10],'leaf_size':[1,2,3,5],'weights':['uniform', 'distance'],\n          'algorithm':['auto', 'ball_tree','kd_tree','brute']}\nKNN_GS = KNeighborsClassifier()\ngrid_search=GridSearchCV(KNN_GS, param_grid, n_jobs=-1,cv=5, scoring='accuracy')\ngrid_search.fit(X_train,y_train)\ngrid_search.best_params_ ","6ec5644e":"KNN_best = KNeighborsClassifier(algorithm='auto', n_neighbors=10, weights='uniform',leaf_size=1)\nKNN_best.fit(X_train, y_train)\naccuracy_score(y_test,KNN_best.predict(X_test))","9e3ef1c8":"from sklearn.ensemble import GradientBoostingClassifier\n# Define the model\nmodl_XG = GradientBoostingClassifier(random_state=0) # Your code here\n# Fit the model\nmodl_XG.fit(X_train,y_train) # Your code here","1ce07c97":"pred_XG = modl_XG.predict(X_test)\naccuracy_score(y_test,pred_XG)","bac1b687":"mdl_best = GradientBoostingClassifier(random_state=0)\nmdl_best.fit(train, train_label)\nprediction=mdl_best.predict(test)\nmysubmission=pd.DataFrame(columns=['PassengerId','Survived'])\nmysubmission['Survived'] = prediction\n\nmysubmission['PassengerId']=test.PassengerId\n\n# mysubmission = mysubmission[['PassengerId', 'Survived']]\n\nmysubmission.to_csv('Submission.csv',index=False)","4781ab1a":"##### Dimensions","e25e9f95":"### f.XGboost: boosting gradient","2f92582c":"##### Survived ","d19da8fe":"##### Testing","224bdced":"#### InterQuartile Range (IQR)","5bd6373d":"# 3- Data visualization \n## 3.1- Survived Vs Died\n\n##### The Age of the total persons ","a2ff8b8d":"##### Removing non important columns\n\n\nFrom the Figure below it is clear that the 'cabin' column is almost empty in both training and testing datasets ==> it is better to delete this column because it does not contain important information\n","d57f41e2":"# Submission \n\nThe best classification algorithm is : ","b5ce7f0a":"### d.Support Vector Machine","dd23c202":"#### Using GridSearch","6b75edfa":"#####  Replacing NaN values with the column's mean\n\nOnly 'Age' and 'Fare' columns have null values therefore, we have decided to replace them with the Column's mean. \n","c84a2c87":"## 3.2 Survived \/ Died Vs Embarkation port","ae811766":"## 2.3- Checking outliers","bb766087":"# 2- Exploratory data analysis \n## 2.1- Data description \n#### Training\n","053485d4":"Only 'Fare' and 'Age' are numerical columns so we can study their oulier variables. \n\n##### Training ","c9418fe3":"## 3.4- Survived\/Died Vs Ticket class ","6dbdd838":"### e.K-Nearest Neighbor ","06da6a95":"#### Using GridSearch ","9f69b7cb":"# 2- Machine Learning Pipline \n\n## 2.1-  Splitting to training and testing dataset\n","3173c28e":"## 2.3- Algorithms \n\n### a. Decision tree","c9b8db29":"# 1- Data acquisition\n\n### 1.1- Training ","ea21427c":"#####  Mapping Categorical columns into numerical ones","c74ca683":"### 1.2- Testing ","30477322":"##### Total dataset\n","ec07125d":"### c.Random Forest ","8895d7ba":"#### Testing \n","f2379f35":"## 3.3- Survived\/Died Vs Female\/Male ","4ebae8c3":"## 2.2- Does the data containing categorical columns and NaN values ","816f3d1c":"##### Dataset features","faeac41d":"From the Figures below categorical columns are the next: 'Survived', 'Pclass', 'Name', 'Sex', 'SibSp', 'Parch', 'Ticket', 'Embarked'\n","87975a49":"### b. Logistic Regression","e59b48ea":"#### Using GridSearch","7b33883a":"##### Died","73beee16":"#### Using GridSearch","21bea40f":"## 2.4- Checking Class balance ","32cab44b":"#### Using GridSearch","3c9ebfe3":"#### Using GridSearch","35015ae7":"## 2.2- Data standardization and normalization\n\n#In general a good idea is to normalize the data"}}