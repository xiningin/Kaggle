{"cell_type":{"8d4fb9fc":"code","d2ed888c":"code","d74798c5":"code","7a8bebc7":"code","5ace8b04":"code","4150ace1":"code","145ca8bb":"code","5d1f1cb1":"code","d16d303d":"code","be0afa00":"code","523b21f1":"code","d6674244":"code","5371d273":"code","4b98d8fd":"code","31913890":"code","231af733":"code","3b30cf05":"code","57c028c6":"code","991cd1f7":"code","ac355135":"code","e125bc98":"code","081c1228":"code","ae40a425":"code","44f4bf0d":"code","685ed333":"code","9790657c":"code","5290cf4d":"code","1ab6d8a7":"code","b5b727bf":"code","2b6f6de3":"code","58af2d91":"code","3d165f27":"code","bb076407":"code","4437860d":"code","7cb4fd5d":"code","b49a66ed":"code","774aad6a":"code","971ab6a9":"code","266ab81a":"code","67fe03de":"code","9ecbcb01":"code","cd17edc9":"code","4d6cdeb6":"code","49f4fae9":"code","3ddb01cd":"code","e3cd11e4":"code","f498b4fc":"code","3988572f":"code","388e85ce":"code","db8de755":"code","d2d50c3f":"code","41d451c8":"code","5c7f95cf":"code","527349a4":"markdown","66b4e0dd":"markdown","39099ade":"markdown","7931a5c8":"markdown","71b64565":"markdown","1d6bd6aa":"markdown","35fd672c":"markdown","bffd5bb5":"markdown","9d2415ce":"markdown","4231749a":"markdown","73a64828":"markdown","a866c2a3":"markdown","c3aa1c99":"markdown","174f251a":"markdown","c61c652b":"markdown","b6eb6c39":"markdown","13f62d84":"markdown","e1e94acc":"markdown","008e1849":"markdown","639d0036":"markdown","b7b791f8":"markdown","2539f810":"markdown","2382cc0d":"markdown","11e60cbb":"markdown","aee1ca1e":"markdown","d2b55480":"markdown","8e214c9f":"markdown","66389038":"markdown","334dac86":"markdown","922291a6":"markdown","6b5ce21a":"markdown","4638c8f0":"markdown","dcf7d6d0":"markdown","97ef8b77":"markdown","d9c1883c":"markdown","50fbe89f":"markdown"},"source":{"8d4fb9fc":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport seaborn as sns\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","d2ed888c":"import warnings\nwarnings.filterwarnings(\"ignore\")\n\nimport time\nfrom sklearn import linear_model\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom xgboost import XGBClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\n\nfrom sklearn import model_selection\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.model_selection import RandomizedSearchCV\nfrom sklearn.model_selection import train_test_split\n\nfrom sklearn.metrics import accuracy_score, confusion_matrix, classification_report\nfrom sklearn.ensemble import AdaBoostClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier","d74798c5":"# Read training dataset\ntrain_data = pd.read_csv('\/kaggle\/input\/titanic\/train.csv')\n# train_data = pd.read_csv('train.csv')\ntrain_data.head()","7a8bebc7":"train_data.shape","5ace8b04":"train_data[train_data['Survived'] == 0].shape","4150ace1":"# Read test dataset\ntest_data = pd.read_csv('\/kaggle\/input\/titanic\/test.csv')\n# test_data = pd.read_csv('test.csv')\ntest_data.head()","145ca8bb":"test_data.shape","5d1f1cb1":"# Identify the percentage of women survived.\nwomen = train_data.loc[train_data.Sex == 'female']['Survived']\nrate_women = sum(women)\/len(women)\nprint(\"% of women who survived:\", rate_women)","d16d303d":"# Identify the percentage of men survived.\nwomen = train_data.loc[train_data.Sex == 'male']['Survived']\nrate_men = sum(women)\/len(women)\nprint(\"% of men who survived:\", rate_men)","be0afa00":"# Train data Information\ntrain_data.info()","523b21f1":"print(\"Cabin Null Values :\", train_data.Cabin.isnull().sum())\nprint(\"Embarked Null Values :\", train_data.Embarked.isnull().sum())\nprint(\"Age Null Values :\", train_data.Age.isnull().sum())","d6674244":"# Use regex to extract title from Name, All the title values with low representation in dataset is marked as 'Misc'\nimport re\ndef split_it(data):\n    result = re.search('^.*,(.*)\\.\\s.*$', data)\n    if result.group(1) not in [' Mr', ' Miss', ' Mrs', ' Master']:\n        return ' Misc'\n    else:\n        return result.group(1)\n\ntrain_data['Title'] = train_data['Name'].apply(split_it)\ntrain_data.loc[train_data['PassengerId'] == 514, 'Title'] = ' Mrs'\n\n# Plot barplot\nfrom matplotlib import pyplot as plt\nplt.figure(figsize=(9,6))\nplt.tick_params(axis='x', rotation=90)\nsns.countplot(x=\"Title\", hue='Survived', data=train_data)\nplt.legend(loc='upper right')","5371d273":"# Plot graphs\nplt.figure(figsize=(18,6))\nplt.tick_params(axis='x', rotation=90)\nplt.subplot(1,2,1)\nsns.countplot(x=\"Parch\", hue='Survived', data=train_data)\nplt.legend(loc='upper right')\nplt.subplot(1,2,2)\nsns.countplot(x=\"SibSp\", hue='Survived', data=train_data)\nplt.legend(loc='upper right')","4b98d8fd":"bin_labels_p = ['Parch_0', 'Parch_1_2', 'Parch_3+']\ntrain_data['Parch_new'] = pd.cut(train_data['Parch'], [-1,0,2,20], labels = bin_labels_p)\n\nbin_labels_s = ['SibSp_0', 'SibSp_1_2', 'SibSp_3+']\ntrain_data['SibSp_new'] = pd.cut(train_data['SibSp'], [-1,0,2,20], labels = bin_labels_s)\n\nplt.figure(figsize=(18,6))\nplt.tick_params(axis='x', rotation=90)\nplt.subplot(1,2,1)\nsns.countplot(x=\"Parch_new\", hue='Survived', data=train_data)\nplt.legend(loc='upper right')\nplt.subplot(1,2,2)\nsns.countplot(x=\"SibSp_new\", hue='Survived', data=train_data)\nplt.legend(loc='upper right')","31913890":"# Check the details of passengers missing port of Embarkation\ntrain_data[train_data['Embarked'].isnull()]","231af733":"# Check the statistical mode of 'Embarked' feature\ntrain_data[~train_data['Embarked'].isnull()]['Embarked'].mode()","3b30cf05":"# As obtained from web research, impute the Missing values for 'Embarked' as 'S' Southampton (Statistical mode also giving 'S')\ntrain_data['Embarked'] = train_data['Embarked'].fillna(train_data['Embarked'].mode()[0])\ntrain_data[train_data['Embarked'].isnull()]","57c028c6":"# Percentage of Missing Values in Cabin Field\n(train_data['Cabin'].isnull().sum() \/ len(train_data['Cabin'])) * 100","991cd1f7":"train_data = train_data.drop(['Cabin'],axis=1)","ac355135":"# Histogram of Age\nsns.distplot(train_data[~train_data['Age'].isnull()]['Age'])","e125bc98":"bin_labels_a = ['Babies', 'Children', 'Young Adults', 'Middle-Aged Adults','Old Adults']\ntrain_data['Age_Group'] = pd.cut(train_data['Age'], [0,2,16,30,45,200], labels = bin_labels_a)\n\n# Plot boxplot of Age\nplt.figure(figsize=(18,6))\nplt.subplot(1,2,1)\nsns.boxplot(data=train_data['Age'])\n# Trends in Age groups and Survival\nplt.subplot(1,2,2)\nsns.countplot(x=\"Age_Group\", hue='Survived', data=train_data)","081c1228":"# Map Age\ntrain_data['Age'] = train_data['Age'].fillna(round(train_data.groupby(['Title','Parch_new'])['Age'].transform('mean')))","ae40a425":"bin_labels = ['very_low', 'low', 'medium', 'high']\ntrain_data['fare_bin'] = pd.qcut(train_data['Fare'], q=4, labels = bin_labels)\n\n# Plot boxplot of Fare\nplt.figure(figsize=(18,6))\nplt.subplot(1,2,1)\nsns.boxplot(data=train_data['Fare'])\n# Trends in Fare categories and Survival\nplt.subplot(1,2,2)\nsns.countplot(x=\"fare_bin\", hue='Survived', data=train_data)","44f4bf0d":"# Check for unique values\ntrain_data.nunique()","685ed333":"train_data = train_data.drop(['Name', 'Ticket', 'Age','Parch','SibSp','Fare'], axis=1)","9790657c":"# Convert categorical features to objects\ntrain_data['SibSp_new'] = train_data['SibSp_new'].astype(object)\ntrain_data['Parch_new'] = train_data['Parch_new'].astype(object)\ntrain_data['Pclass'] = train_data['Pclass'].astype(object)\n\ndummies = pd.get_dummies(train_data[['Age_Group','fare_bin','Sex','SibSp_new','Parch_new','Embarked','Pclass','Title']])\ntrain_data = train_data.join(dummies)\ntrain_data = train_data.drop(['Sex','Parch_new','SibSp_new','Embarked','Pclass','Age_Group','fare_bin','Title'],axis=1)","5290cf4d":"y = train_data['Survived'] # Target variable\nX = train_data.drop('Survived', axis=1)","1ab6d8a7":"plt.figure(figsize=(18,12))\nsns.heatmap(X.corr(), annot=True)","b5b727bf":"# Drop 'Cabin' 77% missing value\ntest_data = test_data.drop(['Cabin'],axis=1)\n\ntest_data['Title'] = test_data['Name'].apply(split_it)\ntest_data['Parch_new'] = pd.cut(test_data['Parch'], [-1,0,2,20], labels = bin_labels_p)\ntest_data['SibSp_new'] = pd.cut(test_data['SibSp'], [-1,0,2,20], labels = bin_labels_s)\ntest_data['Age_Group'] = pd.cut(test_data['Age'], [0,2,16,30,45,200], labels = bin_labels_a)\n\n# Treat Missing values 'Age'\ntest_data['Age'] = test_data['Age'].fillna(round(test_data.groupby(['Title','Parch_new'])['Age'].transform('mean')))\ntest_data['Fare'] = test_data['Fare'].fillna(round(test_data[test_data['Pclass']==3]['Fare'].mean(),2))\n\ntest_data['fare_bin'] = pd.qcut(test_data['Fare'], q=4, labels = bin_labels)\n\n# Drop Unwanted Features\ntest_data = test_data.drop(['Name', 'Ticket', 'Age','Parch','SibSp','Fare'], axis=1)\n\n# One-Hot Encoding\ntest_data['SibSp_new'] = test_data['SibSp_new'].astype(object)\ntest_data['Parch_new'] = test_data['Parch_new'].astype(object)\ntest_data['Pclass'] = test_data['Pclass'].astype(object)\n\ndummies = pd.get_dummies(test_data[['Age_Group','fare_bin','Sex','SibSp_new','Parch_new','Embarked','Pclass','Title']])\ntest_data = test_data.join(dummies)\ntest_data = test_data.drop(['Sex','Parch_new','SibSp_new','Embarked','Pclass','Age_Group','fare_bin','Title'],axis=1)\n\nX_test = test_data\n","2b6f6de3":"X_test.info()","58af2d91":"#!pip install mlxtend\n# importing the models\nfrom mlxtend.feature_selection import SequentialFeatureSelector as sfs","3d165f27":"# Calling Various models\n\nlreg = LogisticRegression()\ngb = GradientBoostingClassifier()\nrf = RandomForestClassifier()\nada = AdaBoostClassifier()\nXGB = XGBClassifier()\n\nsfs1 = sfs(rf, k_features=15, forward=True, verbose=2, scoring='accuracy')\n\n#fit\nsfs1 = sfs1.fit(X, y)","bb076407":"feat_names = list(sfs1.k_feature_names_)\nprint(feat_names)","4437860d":"X_new = X[['Age_Group_Babies', 'Age_Group_Children', 'Age_Group_Young Adults', 'Age_Group_Middle-Aged Adults', 'fare_bin_very_low', 'Sex_female', 'Sex_male', 'SibSp_new_SibSp_3+', 'Parch_new_Parch_0', 'Parch_new_Parch_1_2', 'Parch_new_Parch_3+', 'Pclass_3', 'Title_ Master', 'Title_ Miss', 'Title_ Mrs']]\nX_test = X_test[['Age_Group_Babies', 'Age_Group_Children', 'Age_Group_Young Adults', 'Age_Group_Middle-Aged Adults', 'fare_bin_very_low', 'Sex_female', 'Sex_male', 'SibSp_new_SibSp_3+', 'Parch_new_Parch_0', 'Parch_new_Parch_1_2', 'Parch_new_Parch_3+', 'Pclass_3', 'Title_ Master', 'Title_ Miss', 'Title_ Mrs']]","7cb4fd5d":"# To check model performance\ndef check_performance(y, predictions):\n    # View accuracy score\n    print(\"Accuracy : \",round(accuracy_score(y, predictions),2))\n    print(\" \")\n    \n    plt.figure(figsize=(14,5))\n    plt.subplot(1,2,1)\n    \n    #Confusion Matrix\n    cf_matrix =  confusion_matrix(y, predictions)      \n    sns.heatmap(cf_matrix, annot=True, annot_kws={\"size\": 10},fmt=\"d\", cmap='Spectral_r')\n    plt.ylabel('True label')\n    plt.xlabel('Predicted label')\n    \n    # Classification\n    print(classification_report(y, predictions))    ","b49a66ed":"# Hyperparameters for Logistic Regression\npenalty = ['l2'] # L2 Regularization\nC = [0.0001, 0.001, 0.01, 0.1, 1, 10, 100, 1000] \nsolver = ['newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga']\nmulti_class = ['ovr', 'multinomial','auto']\n\nparam_distributions = dict(penalty=penalty, C=C, solver=solver, multi_class=multi_class)","774aad6a":"# Logistic Regression - Grid Search CV\nfolds = KFold(n_splits=5, shuffle=True, random_state=42)\n\n# Perform Cross Validation\nlog_cv = GridSearchCV(estimator = LogisticRegression(),\n                      param_grid = param_distributions, \n                      #scoring= 'roc_auc', \n                      scoring= 'accuracy', \n                      cv = folds,                         \n                      n_jobs=6, verbose = 10,\n                      return_train_score=True)\n\n# Perform Hyperparameter Tuning\nlog_cv.fit(X_new, y)    \ncv_results = pd.DataFrame(log_cv.cv_results_)\ndisplay(cv_results)","971ab6a9":"print('Best score: ', log_cv.best_score_)\n#print the optimum value of hyperparameters\nprint('Best hyperparameters: ', log_cv.best_params_)    ","266ab81a":"#Train logistic regression model using the best parameters\n\nbest_model = LogisticRegression(C=1, multi_class='multinomial', penalty='l2', solver='newton-cg')\n\n# Predict on train data and check performance\nbest_model.fit(X_new, y)    \ny_pred_log = best_model.predict(X_new)\ncheck_performance(y, y_pred_log)\n\n# Predict on test data\ny_pred_log = best_model.predict(X_test)","67fe03de":"# Save results to CSV\noutput_logcsv = pd.DataFrame({'PassengerId': test_data.PassengerId, 'Survived': y_pred_log})\noutput_logcsv.to_csv('log_regression.csv', index=False)\nprint(\" Saved Successfully\")","9ecbcb01":"params = {\n    'max_depth': [5, 10, 20, 30, 40],\n #   'min_samples_leaf': [3, 4, 5],\n #   'n_estimators': [20, 30, 60, 80],\n #   'learning_rate': [0.01, 0.05, 0.1, 0.2], \n #   'subsample': [0.3, 0.6, 0.9]\n}","cd17edc9":"folds = 4\n\n# specify model\nxgb_model = XGBClassifier()\n\n# set up GridSearchCV()\nmodel_cv = GridSearchCV(estimator = xgb_model, \n                        param_grid = params, \n                        #scoring= 'roc_auc', \n                        scoring= 'accuracy', \n                        cv = folds, \n                        verbose = 10,\n                        n_jobs = 6,\n                        return_train_score=True)     \n# fit the model\nmodel_cv.fit(X_new, y)   \n\ncv_results = pd.DataFrame(model_cv.cv_results_)\n#cv_results['param_learning_rate'] = cv_results['param_learning_rate'].astype('float')\nprint(\"Cross Validation Results\")\nprint(\"------------------------\")\ndisplay(cv_results)\n\n#print the evaluation result by choosing a evaluation metric\nprint('Best ROC AUC score: ', model_cv.best_score_)\n#print the optimum value of hyperparameters\nprint('Best hyperparameters: ', model_cv.best_params_)\n","4d6cdeb6":"#Train XGBoost model using the best parameters\n#Model with optimal hyperparameters\n\nparams = {#'learning_rate': model_cv.best_params_['learning_rate'], \n          'max_depth': 20}  #10\n         # 'n_estimators':60,\n         # 'subsample':model_cv.best_params_['subsample'], \n         # 'min_samples_leaf': 3,\n         # 'objective':'binary:logistic'}\n\n# fit model on training data\nmodel = XGBClassifier(params = params)\nmodel.fit(X_new, y)\n\n# Predict on train data and check performance    \ny_pred_XG = model.predict(X_new)\ncheck_performance(y, y_pred_XG)\n\n# Predict on test data\ny_pred_XG = model.predict(X_test)","49f4fae9":"output2 = pd.DataFrame({'PassengerId': test_data.PassengerId, 'Survived': y_pred_XG})\noutput2.to_csv('XGB.csv', index=False)\nprint(\" Saved Successfully\")","3ddb01cd":"params = {\n#   'max_depth': [5, 8, 10, 11, 12, 20],\n#   'min_samples_leaf': [3, 4, 5, 6],\n    'n_estimators': [50,100, 110, 120, 200, 500, 1000]\n}\n\n# Instantiate the grid search model\nrf = RandomForestClassifier(random_state=42, n_jobs=-1)\n\ngrid_search = GridSearchCV(estimator = rf, \n                           param_grid = params, \n                           #scoring= 'roc_auc',\n                           scoring= 'accuracy',\n                           cv = 4, \n                           n_jobs=6,\n                           verbose = 10)\n\n# Fit the grid search to the data\ngrid_search.fit(X_new,y)\n\ncv_results = pd.DataFrame(grid_search.cv_results_)\nprint(\"Cross Validation Results\")\nprint(\"------------------------\")\ndisplay(cv_results)\n","e3cd11e4":"#print the evaluation result by choosing a evaluation metric\nprint('Best ROC AUC score: ', grid_search.best_score_)\n#print the optimum value of hyperparameters\nprint('Best hyperparameters: ', grid_search.best_params_)\n","f498b4fc":"best_model_rf = RandomForestClassifier(#random_state = 100,\n                                  n_estimators = 200 #1000\n                                  #max_depth=8\n                                  #min_samples_leaf=4\n                                  )\n\n# Train data\nbest_model_rf.fit(X_new, y)   \ny_pred_rf = best_model_rf.predict(X_new)\ncheck_performance(y,y_pred_rf)\n\n# Predict on Test data\ny_pred_rf = best_model_rf.predict(X_test)","3988572f":"output2 = pd.DataFrame({'PassengerId': test_data.PassengerId, 'Survived': y_pred_rf})\noutput2.to_csv('RandomForest.csv', index=False)\nprint(\" Saved Successfully\")","388e85ce":"abc = AdaBoostClassifier(base_estimator=best_model_rf,random_state=15)\nabc.fit(X_new, y)\ny_pred_abc = abc.predict(X_new)\ncheck_performance(y, y_pred_abc)","db8de755":"y_pred_abc = abc.predict(X_test)\n\noutput2 = pd.DataFrame({'PassengerId': test_data.PassengerId, 'Survived': y_pred_abc})\noutput2.to_csv('Adaboost.csv', index=False)\nprint(\" Saved Successfully\")","d2d50c3f":"gbc = GradientBoostingClassifier(random_state=15)\ngbc.fit(X_new,y)\ny_pred = gbc.predict(X_new)\ncheck_performance(y, y_pred)","41d451c8":"y_pred_gbc = gbc.predict(X_test)\n\noutput2 = pd.DataFrame({'PassengerId': test_data.PassengerId, 'Survived': y_pred_gbc})\noutput2.to_csv('GB.csv', index=False)\nprint(\" Saved Successfully\")","5c7f95cf":"#params = {'learning_rate': model_cv.best_params_['learning_rate'], \n#          'max_depth': 10, \n#          'n_estimators':80,\n#          'subsample':model_cv.best_params_['subsample'], \n#          'min_samples_leaf': 3,\n#          'objective':'binary:logistic'}\n\n#classifiers = []\n#classifiers.append(('LR',LogisticRegression(C=1, multi_class='multinomial', penalty='l2', solver='newton-cg')))\n#classifiers.append(('GB',GradientBoostingClassifier(random_state=15)))\n#classifiers.append(('AB',AdaBoostClassifier(base_estimator=best_model_rf,random_state=15)))\n#classifiers.append(('XG',XGBClassifier(params = params)))\n#classifiers.append(('RF',RandomForestClassifier(random_state = 100,\n#                                  n_estimators = 100,\n#                                  max_depth=8, \n#                                  min_samples_leaf=4\n#                                  )))\n\n#from sklearn.ensemble import VotingClassifier\n\n#vot_hard = VotingClassifier(estimators=classifiers, voting = 'soft')\n#vot_hard.fit(X,y)\n#pred_vot = vot_hard.predict(X)\n#check_performance(y, pred_vot)\n\n#pred_vot = vot_hard.predict(X_test)\n\n#op_vot = pd.DataFrame({'PassengerId': test_data.PassengerId, 'Survived': pred_vot})\n#op_vot.to_csv('voting_sub.csv', index=False)\n#print(\" Saved Successfully\")","527349a4":"__Apply all transformation's applied on train data to test Data__","66b4e0dd":"__4. Fare__","39099ade":"## __Load data__","7931a5c8":"### * XG Boost *","71b64565":"## Improve Model","1d6bd6aa":"__Fare - Split Fare into seperate bins__","35fd672c":"- Low survival rate observed for passenger's with name title 'Mr.'( i.e. Mostly young and middle aged men)","bffd5bb5":"__2. Cabin__","9d2415ce":"## Exploratory Data Analysis","4231749a":"Surprisingly, both the passengers are having same Ticket Number & shares same cabin. I have done some analysis to check relationship between Ticket Number and port of embarkation and failed to reach any inference.\n\nBut I was able to get an interesing information about both passengers from https:\/\/www.geni.com\/people\/Rose-Icard\/600000001582961.\n\n\"__Miss Amelie Icard, 38,__ was born in Vaucluse, France, where her father Marc Icard lived at Mafs \u00e1 Murs (?). __She boarded the Titanic at Southampton as maid to Mrs George Nelson Stone__. She travelled on Mrs Stone's ticket (__#113572__). Personal Maid to __Mrs Martha Evelyn Stone__\"","73a64828":"Nearly 77% of passenger details missing Cabin Number. So dropping entire column.","a866c2a3":"There 891 rows of data in training dataset","c3aa1c99":"### Voting Classifier","174f251a":"## Treating Missing Values","c61c652b":"__Age - Split Age filed into seperate bins__","b6eb6c39":"For imputing Age value, instead of proceeding with mean of entire population, following with below approach.\n\n1. Passenger names are addressed with title's such as Mr., Miss., Master. & Mrs.  People with similar title can falls similar age group and gender.\n\n2. Based on the title and 'Parch' feature, Passengers are grouped and impute missing age values with group's mean.","13f62d84":"### Gradient Boost","e1e94acc":"__1. Embarked__","008e1849":"Features __PassengerId, Name__ is unique for all the rows. Also, nearly 681 __ticket__ numbers are unique. We are good to remove __'Name', 'Ticket'__ from training because it wont help in the classification task.","639d0036":"- Babies and Childrens are having high survival rate","b7b791f8":"__Check Correlation__","2539f810":"1. __Extract title from 'Name' field__","2382cc0d":"### * Logistic Regression _ K-Fold Cross Validation *","11e60cbb":"__3. Age__","aee1ca1e":"__Check features with unique values & drop irrelevent feature__","d2b55480":"- __Split 'Parch' and 'SibSp' into bins__","8e214c9f":"- Many of the solo travellers in Titanic lost their life in the disaster.\n- Count of passengers who travelled with 3 or more family members are compartitevely less and survival statistics also not good. May be they haven't taken individual oppurtunity to escape leaving the entire family behind.\n- People who travelled along with 1 family member having good survival chances when companring with others.","66389038":"__Target Column__","334dac86":"### Forward Feature Selection","922291a6":"### Titanic Prediction ","6b5ce21a":"2. __SibSp (Siblings & Spouse) | Parch (Parents or Children)__","4638c8f0":"### * Ada Boost *","dcf7d6d0":"People who purchased costly tickets, probably the rich have higher survival rate.","97ef8b77":"Features __Age, Cabin, Embarked contains few null values__","d9c1883c":"### * Random Forest Hyperparameter Tuning *","50fbe89f":"__One-Hot encoding the categorical parameters using get_dummies()__"}}