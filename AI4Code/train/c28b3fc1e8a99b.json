{"cell_type":{"9e2ce16f":"code","eb0882c8":"code","7db1fe61":"code","ffab0e6f":"code","b925eebc":"code","d82d8793":"code","bf44d8ba":"code","58fde11f":"code","097e63f9":"code","0dd67354":"code","0e061f3c":"code","48568606":"code","863094a5":"code","2630a7f3":"code","66249f32":"code","04185f7d":"code","c6be79ba":"code","778445c3":"code","ec462ea6":"code","f9f6027a":"code","97c137f6":"code","f2712bd4":"code","2eaddcbe":"code","8429bf57":"code","0ab7b0bf":"code","cbf9bcfe":"code","fa5991a8":"code","f834ac8e":"code","132c8e41":"code","a9eb91db":"code","5c98c3a3":"code","41d2f4d7":"code","379edd1a":"code","77316e15":"code","2d6707a6":"code","9640ca91":"code","a1a7d024":"code","53c3d3bc":"markdown","f525027a":"markdown","138ce0cb":"markdown","8aeb34ae":"markdown","10333468":"markdown","710d7ade":"markdown","bcdca6f3":"markdown","ca8f0d61":"markdown","70e11115":"markdown","3b9647e9":"markdown","f3d596e8":"markdown","82cd7da5":"markdown","326e19c7":"markdown","9f96e25f":"markdown","aadbe7d1":"markdown","8f58bd0b":"markdown","aa1a5932":"markdown","53066a1c":"markdown"},"source":{"9e2ce16f":"pip install pycountry_convert","eb0882c8":"#import pandas and other visualization packages\nimport pandas as pd\nimport plotly.express as px\nimport plotly.graph_objects as go\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom scipy import stats\nimport numpy as np\n\n#set visualization theme for seaborn\nsns.set_theme(context='notebook', style='darkgrid', palette='deep', \n              font='sans-serif', font_scale=1, color_codes=True, rc=None)","7db1fe61":"#create a dataframe from the data cleaned up in Excel\nraw_data = pd.read_csv('..\/input\/life-expectancy-data\/final data.csv')\n\n#get the top 10 countries with highest life expectancies\ntop10_countries = raw_data[['Country', 'Life expectancy at birth, total (years)']].sort_values(by = 'Life expectancy at birth, total (years)', ascending=False).head(10)\ntop10_countries","ffab0e6f":"#get the bottom 10 countries with lowest life expectancies\n#bottom10_countries = raw_data[['Country', 'Life expectancy at birth, total (years)']].sort_values(by = 'Life expectancy at birth, total (years)', ascending=True).head(10)\nbottom10_countries = raw_data.loc[raw_data['Life expectancy at birth, total (years)'] < 60].sort_values(by = 'Life expectancy at birth, total (years)', ascending = True).copy(deep=True)\nbottom10_countries.head(13)","b925eebc":"#let's take a peek at all our features\nraw_data.head()","d82d8793":"#import package to get continents from country names\nfrom pycountry_convert import country_alpha2_to_continent_code, country_name_to_country_alpha2\n\n#create a dictionary that maps continent names to continent codes\ncontinents = {\n    'NA': 'North America',\n    'SA': 'South America', \n    'AS': 'Asia',\n    'OC': 'Australia',\n    'AF': 'Africa',\n    'EU': 'Europe'\n}\ncontinent = []\ni = 0\ncountries = dict()\n#get the continent code for each country in our data frame\nfor country in raw_data['Country']:\n\n    countries[i] = country\n    i+=1\n    try:\n        continent.append(continents[(country_alpha2_to_continent_code(country_name_to_country_alpha2(country)))])\n    except:\n        continent.append(\"Africa\")\n\n#add a column to our dataframe for continent\nraw_data[\"Continent\"] = continent\n\n","bf44d8ba":"avg_life = raw_data['Life expectancy at birth, total (years)'].mean()","58fde11f":"fig1 = px.scatter(raw_data, x=\"GDP per capita (constant 2010 US$)\", y=\"Life expectancy at birth, total (years)\",\n                 hover_name=\"Country\", log_x=True, size_max=50, color='Continent', title='Life expectancy vs GDP per Capita')\n\nfig2 = px.scatter(raw_data, x=\"Fertility rate, total (births per woman)\", y=\"Life expectancy at birth, total (years)\",\n                 hover_name=\"Country\", log_x=True, size_max=50, color='Continent', title='Life expectancy vs Fertility rate')\n\nfig3 = px.scatter(raw_data, x=\"Prevalence of current tobacco use (% of adults)\", y=\"Life expectancy at birth, total (years)\",\n                 hover_name=\"Country\", log_x=False, size_max=50, color='Continent', title='Life expectancy vs Tobacco use')\nfig1.show()\nfig2.show()\nfig3.show()","097e63f9":"plt.figure(figsize=(15,10))\ncmap = sns.diverging_palette(500, 10, as_cmap=True)\nsns.heatmap(raw_data.corr(), cmap =cmap, center=0, annot=False, square=True);","0dd67354":"#import pandas package to create dataframes; import sci-kit learn to create model\nimport pandas as pd\nimport sklearn as sc\n\n#for this project, it's safe to ignore the SettingWithoutCopyWarning, so we're\n#going to suppress it here\npd.options.mode.chained_assignment = None","0e061f3c":"#create a dataframe that separates labels from features\nlabels = raw_data[['Country', 'Life expectancy at birth, total (years)']].copy()\n\n#Delete our label from our features dataframe\ndel raw_data['Life expectancy at birth, total (years)']\n#del raw_data['Continent']\n#Check to see what our data looks like\nraw_data.head()","48568606":"#Give the index a name so we can reference it later\nraw_data.index.name = 'index'\n\n#Delete the Country column since we now have index\ndel raw_data['Country']\ndel raw_data['Continent']\n\n#Do the same for our labels dataframe\nlabels.index.name = 'index'\ndel labels['Country']\nraw_data2 = raw_data.copy(deep=True)","863094a5":"#import module from sci-kit learn\nfrom sklearn.model_selection import train_test_split\n\n#split data into training set and testing set\nX_train, X_test, Y_train, Y_test = sc.model_selection.train_test_split(raw_data, labels )","2630a7f3":"#Take a look at our data\nX_train.head()","66249f32":"#import the Random Forest Regressor from ski-kit learn\nfrom sklearn.ensemble import RandomForestRegressor\n\n#create a RandomForestRegressor object, which is our model\nbase_model = RandomForestRegressor() \n# \n#train our model\nbase_model.fit(X_train, Y_train.values.ravel())","04185f7d":"#import numpy for analysis\nimport numpy as np\n#import module to calculate r-squared\nimport sklearn.metrics as metrics\n\ndef evaluate(model, test_features, test_labels):\n    #make predictions using our model\n    predictions = model.predict(test_features)\n    \n    #convert test_labels to array\n    test_labels = np.array(test_labels)\n    \n    #calculate the absolute difference (in years) between the\n    #model's predictions and the true values\n    errors = abs(predictions - test_labels)\n    \n    #calculate the average percent difference between prediction and true values\n    mape = 100 * np.mean(errors \/ test_labels)\n    \n    #convert to \n    accuracy = 100 - mape\n    \n    #print results\n    print('Model Performance')\n    print('Average Error: {:0.4f} years.'.format(np.mean(errors)))\n    print('Accuracy = {:0.2f}%.'.format(accuracy))\n    print(\"R-squared = {:0.3f}\".format(metrics.r2_score(Y_test['Life expectancy at birth, total (years)'],predictions)))\n    print('Mean Absolute Error = {:0.3f}'.format(metrics.mean_absolute_error(Y_test['Life expectancy at birth, total (years)'],predictions)))\n    print('Mean Squared Error = {:0.3f}'.format(metrics.mean_squared_error(Y_test['Life expectancy at birth, total (years)'],predictions)))\n    print('Root Mean Squared Error = {:0.3f}'.format(np.sqrt(metrics.mean_squared_error(Y_test['Life expectancy at birth, total (years)'],predictions))))\n    \n    ","c6be79ba":"#call the function defined above to evaluate our base model\n\nevaluate(base_model, X_test, Y_test)\n","778445c3":"#a method to get the names of our features from the column number\ndef get_column_names(df): \n    column_names = dict()\n    j = 0\n    \n    for i in list(df.columns):\n        column_names[j] = i\n        j +=1\n","ec462ea6":"#a method that gets the important features of a model\ndef get_importance(model):# get importance\n    importance = model.feature_importances_\n\n    #create a dictionary of features and their importance scores\n    top_features = dict()\n    for i in range(0,len(importance)):\n        top_features[i] = importance[i]\n\n    #create a dataframe from dictionary    \n    df_features = pd.DataFrame.from_dict(top_features, orient='index')\n\n    #give the column a name\n    df_features.rename(columns={0: 'importance'}, inplace=True)\n    \n    column_names = dict()\n    j = 0\n    \n    if len(model.feature_importances_) == 18:\n        for i in list(raw_data2.columns):\n            column_names[j] = i\n            j +=1\n    else:    \n        for i in list(raw_data.columns):\n            column_names[j] = i\n            j +=1\n\n    #Sort by importance\n    df_features['feature'] = df_features.index.map(column_names)\n    df_features = df_features.sort_values(by=['importance'], \n                                          ascending=False)\n    df_features = df_features[['feature', 'importance']]\n    \n    #Print list\n    return df_features","f9f6027a":"get_importance(base_model)\n","97c137f6":"#delete bottom five features\n\n\ndel raw_data['Immunization, HepB3 (% of one-year-old children)']\ndel raw_data['Prevalence of current tobacco use (% of adults)']\ndel raw_data['PM2.5 air pollution, population exposed to levels exceeding WHO guideline value (% of total)']\ndel raw_data['Urban population (% of total population)']\ndel raw_data['Immunization, DPT (% of children ages 12-23 months)']\n","f2712bd4":"#create new testing and training data\nX_train2, X_test2, Y_train2, Y_test2 = sc.model_selection.train_test_split(raw_data, labels)","2eaddcbe":"#create a new model\nscrubbed_model = RandomForestRegressor() \n\n#train our model\nscrubbed_model.fit(X_train2, Y_train2)\n\n#evaluate our model\nevaluate(scrubbed_model, X_test2, Y_test2)","8429bf57":"#import our RandomizedSearchCV module from sci-kit learn\nfrom sklearn.model_selection import RandomizedSearchCV\n\n#sometimes these things take a while. We'll use this package later to alert us when our \n#code is finisihed running. (this will not work on mac machines)\n\n\n# Number of trees in random forest\nn_estimators = [int(x) for x in np.linspace(start = 1, stop = 17, num = 1)]\n# Number of features to consider at every split\nmax_features = ['auto', 'sqrt']\n# Maximum number of levels in tree\nmax_depth = [int(x) for x in np.linspace(4, 100, num = 1)]\nmax_depth.append(None)\n# Minimum number of samples required to split a node\nmin_samples_split = [2, 5, 10]\n# Minimum number of samples required at each leaf node\nmin_samples_leaf = [1, 2, 4]\n# Method of selecting samples for training each tree\nbootstrap = [True, False]\n\n# Create the random grid\nrandom_grid = {'n_estimators': n_estimators,\n               'max_features': max_features,\n               'max_depth': max_depth,\n               'min_samples_split': min_samples_split,\n               'min_samples_leaf': min_samples_leaf,\n               'bootstrap': bootstrap}\n\n#create the model\nrf = RandomForestRegressor()\n\n#pick random sets of parameters\nrf_random = RandomizedSearchCV(estimator = rf, param_distributions = random_grid, \n                               n_iter = 100, cv = 5, verbose=2, random_state=42, n_jobs = -1)\n\n\n#fit our model\nrf_random.fit(X_train, Y_train.values.ravel())\n\n","0ab7b0bf":"#create and train new model with the best parameters found above\nbest_random = rf_random.best_estimator_\n\n#evaluate model created above\nevaluate(best_random, X_test, Y_test['Life expectancy at birth, total (years)'])","cbf9bcfe":"from sklearn.model_selection import GridSearchCV\n\n# Create the parameter grid based on the results of random search \nparam_grid = {\n    'bootstrap': [True, False],\n    'max_depth': [10, 20, 50, None],\n    'max_features': [2, 3, 4, 'auto'],\n    'min_samples_leaf': [1, 3, 5],\n    'min_samples_split': [1, 2, 4, 8],\n    'n_estimators': [10, 30, 100, 120, 150]\n}\n\n# Create a based model\nrf = RandomForestRegressor()\n# Instantiate the grid search model\ngrid_search = GridSearchCV(estimator = rf, param_grid = param_grid, cv = 3, n_jobs = -1, verbose = 2)\n\n\ngrid_search.fit(X_train,Y_train.values.ravel())\n","fa5991a8":"#create and train a new model with the best parameters found above\nbest_grid = grid_search.best_estimator_\n\n#evaluate new model\nevaluate(best_grid, X_test, Y_test['Life expectancy at birth, total (years)'])","f834ac8e":"model_predictions = best_random.predict(X_test)\nresiduals = model_predictions - Y_test['Life expectancy at birth, total (years)']","132c8e41":"plt.figure(figsize=(20,5))\nsns.histplot(residuals, kde=True, color=\"orange\")\nplt.title('Residual Plot')\nplt.xlabel('Residuals: (Predictions - Actual)')\nplt.ylabel('Density');","a9eb91db":"#get importance for new models\n\nget_importance(best_grid)\n","5c98c3a3":"get_importance(best_random)\n","41d2f4d7":"bottom10_countries","379edd1a":"def plus_50(val):\n    return val + (val*0.5)\n\ndef minus_50(val):\n    return val - (val*0.5)\n\nbottom10_countries['Lifetime risk of maternal death (%)'] = bottom10_countries['Lifetime risk of maternal death (%)'].apply(minus_50)\n","77316e15":"bottom10_countries","2d6707a6":"bottom10_countries['Cause of death, by communicable diseases and maternal, prenatal and nutrition conditions (% of total)'] = bottom10_countries['Cause of death, by communicable diseases and maternal, prenatal and nutrition conditions (% of total)'].apply(minus_50)\nbottom10_countries['People using at least basic sanitation services (% of population)'] = bottom10_countries['People using at least basic sanitation services (% of population)'].apply(plus_50)\nbottom10_countries['Access to electricity (% of population)'] = bottom10_countries['Access to electricity (% of population)'].apply(plus_50)\npredict_bottom = bottom10_countries.copy(deep=True)\ndel predict_bottom['Country'], predict_bottom['Life expectancy at birth, total (years)']","9640ca91":"improved_bottom = list()\npredict_bottom = np.array(predict_bottom)\nfor i in predict_bottom:\n    a = best_grid.predict(i.reshape(1,-1))\n\n    improved_bottom.append(a)\n\nbottom_10np = np.array(bottom10_countries['Life expectancy at birth, total (years)'])\navg = 0  \ncountries = np.array(bottom10_countries['Country'])\nimproved = []\ninitial = []\nfor i in range(0,len(predict_bottom)):\n    change = improved_bottom[i][0] - bottom_10np[i]\n    print('Improvement: ', change)\n    avg += change\n    \n    improved.append(improved_bottom[i][0])\n    initial.append(bottom_10np[i])\n    \n    \nprint(avg \/ len(predict_bottom))\n    #print(bottom_10np[i])\n","a1a7d024":"from matplotlib.pyplot import figure\n\n\n\nlabels = countries\nmen_means = initial\nwomen_means = improved\n\nx = np.arange(len(labels))  # the label locations\nwidth = 0.35  # the width of the bars\n\nfig, ax = plt.subplots()\nrects1 = ax.bar(x - width\/2, men_means, width, label='Initial')\nrects2 = ax.bar(x + width\/2, women_means, width, label='Prediction')\n\n# Add some text for labels, title and custom x-axis tick labels, etc.\nax.set_ylabel('Life Expectancy at Birth (Years)')\nax.set_title('Life Expectancy Improvement for Bottom 10 Countries')\nax.set_xticks(x)\nax.set_xticklabels(labels)\nplt.xticks(rotation = 45)\nax.legend()\n\nax.bar_label(rects1, padding=1)\nax.bar_label(rects2, padding=3)\n\nfig.tight_layout()\nfig.set_size_inches(18,18)\nplt.show()","53c3d3bc":"<h5 style=\"background-color: #f7ffe7; border-color: #ddecbe; border-left: 5px solid #ddecbe; padding: 1.5em; color: #909f70\">\nNow that we've created our model, we must test it. The code below creates a function, evaluate, that we'll use now (and later) to calculate the accuracy for our test set.\n<\/h5>","f525027a":"<h3 style=\"background-color: #daebff; border-color: #bad5f6; border-left: 5px solid #bad5f6; padding: 1.5em; color: #6f89a9\">\nPart 1: Data Cleanup and Test\/Train Split\n<\/h3>","138ce0cb":"<h5 style=\"background-color: #f7ffe7; border-color: #ddecbe; border-left: 5px solid #ddecbe; padding: 1.5em; color: #909f70;\">\nNext, we'll create a correlation matrix. This shows multi-collinearities.\n<\/h5>","8aeb34ae":"<h5 style=\"background-color: #f7ffe7; border-color: #ddecbe; border-left: 5px solid #ddecbe; padding: 1.5em; color: #909f70\">\nOur accuracy has improved significantly, but our R-squared is worse than our base model. Let's instead do a full grid search. This takes much longer since it tests every combination of parameters. In our case, it will try 5,760 combinations.\n\n<\/h5>","10333468":"<h3 style=\"background-color: #daebff; border-color: #bad5f6; border-left: 5px solid #bad5f6; padding: 1.5em; color: #6f89a9\">\nPart 2: Creating, Training, and Testing our Base Model\n<\/h3>","710d7ade":"<h5 style=\"background-color: #f7ffe7; border-color: #ddecbe; border-left: 5px solid #ddecbe; padding: 1.5em; color: #909f70\">\nIt looks like our most important features are Lifetime risk of maternal death; People using at least basic drinking water services; Cause of death by communicable disease and maternal, prenatal, and nutrition conditions; and GDP per capita. On the other hand, some of our least important features include Immunization for HepB3; PM2.5 air pollution exposure; and Immunization for DPT. Let's try removing the five least important features of our model.\n    \n \n\n<\/h5>","bcdca6f3":"<h5 style=\"background-color: #f7ffe7; border-color: #ddecbe; border-left: 5px solid #ddecbe; padding: 1.5em; color: #909f70\">\n\nIt looks like our hyperparameter tuning had mixed success compared to our base model. Even though our accuracy significantly improved, our R-squared value has decreased.\n\n<\/h5>","ca8f0d61":"<h5 style=\"background-color: #f7ffe7; border-color: #ddecbe; border-left: 5px solid #ddecbe; padding: 1.5em; color: #909f70;\">\nThis project uses the sci-kit learn package to implement a Random Forest Regressor to predict a country's life expectancy at birth based on a variety of socioeconomic factors.\n<\/h5>","70e11115":"<h5 style=\"background-color: #f7ffe7; border-color: #ddecbe; border-left: 5px solid #ddecbe; padding: 1.5em; color: #909f70;\">\nFirst, let's examine some basics. We'll start by getting the top and bottom ten countries based on life expectancy at birth.\n<\/h5>","3b9647e9":"<h5 style=\"background-color: #f7ffe7; border-color: #ddecbe; border-left: 5px solid #ddecbe; padding: 1.5em; color: #909f70\">\nNow, we'll split the data into a training set and a testing set using sci-kit learn's built-in function. The default split is 80% training data and 20% testing data, and the function randomly splits the data unless specificed otherwise. X_train contains all our features for training, while Y_train contains all our labels for training. After we train our model, we'll use X_test and Y_test to understand the accuracy and precision of our model.\n<\/h5>","f3d596e8":"<h5 style=\"background-color: #f7ffe7; border-color: #ddecbe; border-left: 5px solid #ddecbe; padding: 1.5em; color: #909f70;\">\nNow, let's move on to some visualizations. Below are some interactive scatter plots that compares our label to some of our features.\n<\/h5>","82cd7da5":"<h5 style=\"background-color: #f7ffe7; border-color: #ddecbe; border-left: 5px solid #ddecbe; padding: 1.5em; color: #909f70\">\nNow, we will create several models and compare them. We'll start with the base_model which uses all the default parameters.\n<\/h5>","326e19c7":"<h3 style=\"background-color: #daebff; border-color: #bad5f6; border-left: 5px solid #bad5f6; padding: 1.5em; color: #6f89a9\">\nPart 0: Data Exploration and Visualization\n<\/h3>","9f96e25f":"<h5 style=\"background-color: #f7ffe7; border-color: #ddecbe; border-left: 5px solid #ddecbe; padding: 1.5em; color: #909f70\">\nOur accuracy and R-squared are okay...let's see if we can improve!\n<\/h5>","aadbe7d1":"<h5 style=\"background-color: #f7ffe7; border-color: #ddecbe; border-left: 5px solid #ddecbe; padding: 1.5em; color: #909f70\">\nSometimes more data makes our model less accurate, so let's see which features are of the most importance to our model. Luckily, sci-kit learn has a built-in function that calculates just this! The feature importance is calculated as a percentage. So if a feature has a score of 0.5, it accounts for 50% of the decision made by our model.\n<\/h5>","8f58bd0b":"<h3 style=\"background-color: #daebff; border-color: #bad5f6; border-left: 5px solid #bad5f6; padding: 1.5em; color: #6f89a9\">\nPart 3: Attempting to Fine-Tune Our Model\n<\/h3>","aa1a5932":"<h5 style=\"background-color: #f7ffe7; border-color: #ddecbe; border-left: 5px solid #ddecbe; padding: 1.5em; color: #909f70\">\nThe model we use expects all numeric inputs. Consequently, we cannot use the name of the country in our analyses. Instead, we'll use the native index that pandas generated for our dataframe since it's an integer.\n<h5>","53066a1c":"<h5 style=\"background-color: #f7ffe7; border-color: #ddecbe; border-left: 5px solid #ddecbe; padding: 1.5em; color: #909f70\">\nOur R-squared with the scrubbed_model is rather poor... Let's try some hyperparameter tuning of our base model to see if we can improve our accuracy. We'll start with a randomized search on our hyperparameters. With this method, we will create a grid of different parameters the model uses during the decision making process. These include the number of nodes, the maximum number of features considered at every split in the tree, etc. The RandomizedSearchCV is a quick way to perform hyperparemeter tuning because it selects different combinations of these parameters at random. Then, it trains the model using each combination of parameters and chooses the one that results in the best fit.\n\n<\/h5>"}}