{"cell_type":{"12c4397b":"code","5d518768":"code","ed4c6c94":"code","154de049":"code","6ac9dd4c":"code","97b45e3e":"code","839ef231":"code","6c58d6a4":"code","49a7aea0":"code","8ba89d3e":"code","c7269059":"code","8888cab5":"code","a08ea50d":"code","ba2109af":"code","8e88374c":"code","3c1a59bb":"code","d880a655":"code","78212b95":"code","37bda714":"code","c7c968a5":"code","298f0946":"code","9cd003ae":"code","2e82046a":"code","594df5b5":"code","d44e23a2":"code","978e5af4":"code","5e4b01e4":"code","8ad842f3":"code","c141fba1":"code","a4264165":"code","564df9a9":"code","67c53388":"code","1176b348":"code","a5242c4f":"code","1904ff3c":"code","3978ed7f":"code","6a893cc2":"code","1023918c":"code","c9fe97e2":"code","42cb1984":"code","a9782627":"code","e6657cad":"code","eef42286":"code","25384d0b":"code","48763ff4":"code","32f381c2":"code","1540dce8":"markdown","5999ccdc":"markdown","e98d7251":"markdown","6381768d":"markdown","c8f4c022":"markdown","a719a507":"markdown","cd29e7db":"markdown","83632340":"markdown","dd875248":"markdown","70914dca":"markdown","eca41b02":"markdown","3c1e16d3":"markdown","dab226c2":"markdown","8003e894":"markdown","84b5a732":"markdown","a93409b4":"markdown","5a57e34a":"markdown","3aa903ca":"markdown","7741ad03":"markdown","8681cc1e":"markdown","814d0663":"markdown","24289d3c":"markdown","c3d35d6d":"markdown","47e1cefb":"markdown","0f82b442":"markdown","b86a5065":"markdown","43937e8d":"markdown","cf879e6d":"markdown","afe91faa":"markdown"},"source":{"12c4397b":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n","5d518768":"from sklearn.feature_selection import SelectKBest, chi2\nfrom sklearn.ensemble import ExtraTreesClassifier, RandomForestClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score","ed4c6c94":"df = pd.read_csv('..\/input\/artificially-generatedsensorsdata\/task_data.csv')\ndf.head()","154de049":"df.info()","6ac9dd4c":"print(\"*_*\"*20,\"\\nData Set\")\nprint(f'There are {df.shape[0]} rows and {df.shape[1]} columns') # fstring \n#display(missing_df.sort_values(by='Missing', ascending=False))\nprint(f'There are {df.isnull().sum().mean()} missing values')\n#check shape and missing values  ","97b45e3e":"# rename it to get ride of space\ndf.rename(columns={\"sample index\":\"sample_index\"},inplace=True)","839ef231":"# Categorical features\ncat_col=df.select_dtypes(include='object').columns.to_list()\ncat_col\n","6c58d6a4":"# Numerical features\nnum_col=df.select_dtypes(include='number').columns.to_list()\nnum_col","49a7aea0":"ax = sns.countplot(df.class_label,label=\"Count\")       # M = 212, B = 357\nOne, NegOne = df.class_label.value_counts()\nprint('Number of Ones: ',One)\nprint('Number of -Ones : ',NegOne)","8ba89d3e":"df.describe()","c7269059":"df.describe().T.style.bar(subset=['mean'], color='#FF595E')\\\n                           .background_gradient(subset=['50%'], cmap='PiYG') # highlight median","8888cab5":"cols = 4\nrows = len(num_col) \/\/ cols+1\nfig, axs = plt.subplots(ncols=cols, nrows=rows, figsize=(19,30), sharex=False) #subplot with all rows\nplt.subplots_adjust(hspace = 0.4)\ni=0\n\nfor r in np.arange(0, rows, 1):\n    for c in np.arange(0, cols, 1):\n        if i >= len(num_col):\n            axs[r, c].set_visible(False)\n        else:\n            axs[r,c].hist(df[num_col[i]].values,\n                                   color=\"#59c8ff\",\n                                   edgecolor=\"black\",\n                                   alpha=0.7,\n                                   label=\"Train Dataset\",bins=40)\n            axs[r, c].set_title(num_col[i], fontsize=17, pad=4)\n            axs[r, c].tick_params(axis=\"y\", labelsize=11)\n            axs[r, c].tick_params(axis=\"x\", labelsize=11)\n            axs[r,c].spines['right'].set_visible(False)\n            axs[r,c].spines['top'].set_visible(False)\n\n        i+=1\n\nplt.show();","a08ea50d":"df['sample_index'].value_counts()\n\n# each sample count 1","ba2109af":"df.head()","8e88374c":"sns.jointplot(x=df.loc[:,'sensor0'], y=df.loc[:,'sensor1'], kind=\"reg\", color=\"#ce1414\")","3c1a59bb":"sns.jointplot(x=df.loc[:,'sensor0'], y=df.loc[:,'sensor2'], kind=\"reg\", color=\"#ce1414\")","d880a655":"sns.jointplot(x=df.loc[:,'sensor0'], y=df.loc[:,'sensor3'], kind=\"reg\", color=\"#ce1414\")","78212b95":"sns.jointplot(x=df.loc[:,'sensor0'], y=df.loc[:,'sensor4'], kind=\"reg\", color=\"#ce1414\")","37bda714":"sns.jointplot(x=df.loc[:,'sensor0'], y=df.loc[:,'sensor5'], kind=\"reg\", color=\"#ce1414\")","c7c968a5":"sns.jointplot(x=df.loc[:,'sensor0'], y=df.loc[:,'sensor6'], kind=\"reg\", color=\"#ce1414\")","298f0946":"sns.jointplot(x=df.loc[:,'sensor0'], y=df.loc[:,'sensor7'], kind=\"reg\", color=\"#ce1414\")","9cd003ae":"sns.jointplot(x=df.loc[:,'sensor0'], y=df.loc[:,'sensor8'], kind=\"reg\", color=\"#ce1414\")","2e82046a":"sns.jointplot(x=df.loc[:,'sensor0'], y=df.loc[:,'sensor9'], kind=\"reg\", color=\"#ce1414\")","594df5b5":"sns.set(style=\"white\")\n#df = x.loc[:,['radius_worst','perimeter_worst','area_worst']]\ng = sns.PairGrid(df, diag_sharey=False)\ng.map_lower(sns.kdeplot, cmap=\"Blues_d\")\ng.map_upper(plt.scatter)\ng.map_diag(sns.kdeplot, lw=3)","d44e23a2":"list = ['sample_index','class_label']\nx = df.drop(list,axis = 1 )\nx.head()","978e5af4":"#correlation map\nf,ax = plt.subplots(figsize=(18, 18))\nsns.heatmap(x.corr(), annot=True, linewidths=.5, fmt= '.1f',ax=ax)","5e4b01e4":"y = df.class_label","8ad842f3":"x.head()","c141fba1":"from sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import f1_score,confusion_matrix\nfrom sklearn.metrics import accuracy_score\n\n# split data train 70 % and test 30 %\nx_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.3, random_state=42)\n\n#random forest classifier with n_estimators=10 (default)\nclf_rf = RandomForestClassifier(random_state=43)      \nclr_rf = clf_rf.fit(x_train,y_train)\n\nac = accuracy_score(y_test,clf_rf.predict(x_test))\nprint('Accuracy is: ',ac)\ncm = confusion_matrix(y_test,clf_rf.predict(x_test))\nsns.heatmap(cm,annot=True,fmt=\"d\")","a4264165":"# feature importances based on analysis using random forest\n\nfeatureImp = pd.DataFrame({  \n                'feature': x_train.columns,\n                'Score': clf_rf.feature_importances_\n              })\n    \nsortedFeatureImp = featureImp.sort_values('Score', axis=0, ascending=False, inplace=False, kind='quicksort', na_position='last')\n# Feature importance\nsortedFeatureImp.style.highlight_max(axis=0)","564df9a9":"from sklearn.feature_selection import SelectKBest\nfrom sklearn.feature_selection import chi2\n# find best scored 5 features\nselect_feature_9ft = SelectKBest(chi2, k=9).fit(x_train, y_train)","67c53388":"#feature importances based on analysis using random forest\nfeature_select_9ft = pd.DataFrame({'feature': x_train.columns,\n                'Score': select_feature_9ft.scores_\n            })\nsortedFeatureImp = feature_select_9ft.sort_values('Score', axis=0, ascending=False, inplace=False, kind='quicksort', na_position='last')\nsortedFeatureImp.style.highlight_max(axis=0)","1176b348":"x_train_2 = select_feature_9ft.transform(x_train)\nx_test_2 = select_feature_9ft.transform(x_test)\n#random forest classifier with n_estimators=10 (default)\nclf_rf_2 = RandomForestClassifier()      \nclr_rf_2 = clf_rf_2.fit(x_train_2,y_train)\nac_2 = accuracy_score(y_test,clf_rf_2.predict(x_test_2))\nprint('Accuracy is: ',ac_2)\ncm_2 = confusion_matrix(y_test,clf_rf_2.predict(x_test_2))\nsns.heatmap(cm_2,annot=True,fmt=\"d\")","a5242c4f":"# find best scored 5 features\nselect_feature_5ft = SelectKBest(chi2, k=5).fit(x_train, y_train)","1904ff3c":"#feature importances based on analysis using random forest\nfeature_select_5ft = pd.DataFrame({'feature': x_train.columns,\n                'Score': select_feature_5ft.scores_\n            })\nsortedFeatureImp = feature_select_5ft.sort_values('Score', axis=0, ascending=False, inplace=False, kind='quicksort', na_position='last')\nsortedFeatureImp.style.highlight_max(axis=0)","3978ed7f":"x_train_2 = select_feature_5ft.transform(x_train)\nx_test_2 = select_feature_5ft.transform(x_test)\n#random forest classifier with n_estimators=10 (default)\nclf_rf_2 = RandomForestClassifier()      \nclr_rf_2 = clf_rf_2.fit(x_train_2,y_train)\nac_2 = accuracy_score(y_test,clf_rf_2.predict(x_test_2))\nprint('Accuracy is: ',ac_2)\ncm_2 = confusion_matrix(y_test,clf_rf_2.predict(x_test_2))\nsns.heatmap(cm_2,annot=True,fmt=\"d\")","6a893cc2":"# feature importances based on analysis using random forest\n\nfeatureImp = pd.DataFrame({  \n                'feature': ['Sensor8','Sensor4','Sensor0','Sensor3','Sensor1'],\n                'Score': clf_rf_2.feature_importances_\n              })\n    \nsortedFeatureImp = featureImp.sort_values('Score', axis=0, ascending=False, inplace=False, kind='quicksort', na_position='last')\n# Feature importance\nsortedFeatureImp.style.highlight_max(axis=0)","1023918c":"from sklearn.feature_selection import RFE\n# Create the RFE object and rank each pixel\nclf_rf_3 = RandomForestClassifier()      \nrfe = RFE(estimator=clf_rf_3, n_features_to_select=5, step=1)\nrfe = rfe.fit(x_train, y_train)","c9fe97e2":"print('Chosen best 5 feature by rfe:',x_train.columns[rfe.support_])\n","42cb1984":"from sklearn.feature_selection import RFECV\n\n# The \"accuracy\" scoring is proportional to the number of correct classifications\nclf_rf_4 = RandomForestClassifier() \nrfecv = RFECV(estimator=clf_rf_4, step=1, cv=5,scoring='accuracy')   #5-fold cross-validation\nrfecv = rfecv.fit(x_train, y_train)\n\nprint('Optimal number of features :', rfecv.n_features_)\nprint('Best features :', x_train.columns[rfecv.support_])","a9782627":"# Plot number of features VS. cross-validation scores\nimport matplotlib.pyplot as plt\nplt.figure()\nplt.xlabel(\"Number of features selected\")\nplt.ylabel(\"Cross validation score of number of selected features\")\nplt.plot(range(1, len(rfecv.grid_scores_) + 1), rfecv.grid_scores_)\nplt.show()","e6657cad":"clf_rf_5 = RandomForestClassifier()      \nclr_rf_5 = clf_rf_5.fit(x_train,y_train)\nimportances = clr_rf_5.feature_importances_\nstd = np.std([tree.feature_importances_ for tree in clf_rf.estimators_],\n             axis=0)\nindices = np.argsort(importances)[::-1]\n\n# Print the feature ranking\nprint(\"Feature ranking:\")\n\nfor f in range(x_train.shape[1]):\n    print(\"%d. feature %d (%f)\" % (f + 1, indices[f], importances[indices[f]]))\n\n# Plot the feature importances of the forest\n\nplt.figure(1, figsize=(14, 13))\nplt.title(\"Feature importances\")\nplt.bar(range(x_train.shape[1]), importances[indices],\n       color=\"g\", yerr=std[indices], align=\"center\")\nplt.xticks(range(x_train.shape[1]), x_train.columns[indices],rotation=90)\nplt.xlim([-1, x_train.shape[1]])\nplt.show()","eef42286":"# split data train 70 % and test 30 %\nx_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.3, random_state=42)\n#normalization\nx_train_N = (x_train-x_train.mean())\/(x_train.max()-x_train.min())\nx_test_N = (x_test-x_test.mean())\/(x_test.max()-x_test.min())\n\nfrom sklearn.decomposition import PCA\npca = PCA()\npca.fit(x_train_N)\n\nplt.figure(1, figsize=(14, 13))\nplt.clf()\nplt.axes([.2, .2, .7, .7])\nplt.plot(pca.explained_variance_ratio_, linewidth=2)\nplt.axis('tight')\nplt.xlabel('n_components')\nplt.ylabel('explained_variance_ratio_')","25384d0b":"yy = df.class_label\nlist = ['sample_index','class_label','sensor0', 'sensor1', 'sensor2',\n       'sensor3', 'sensor4', 'sensor5', 'sensor7', 'sensor9']\nXX_f = df.drop(list,axis = 1 )\nXX_f.head()\n","48763ff4":"xx_train, xx_test, yy_train, yy_test = train_test_split(XX_f, yy, test_size=0.3, random_state=42)\nclf_rf_final = RandomForestClassifier()      \nclf_rf_final = clf_rf_final.fit(xx_train,yy_train)\nimportances = clf_rf_final.feature_importances_\nstd = np.std([tree.feature_importances_ for tree in clf_rf.estimators_],\n             axis=0)\nindices = np.argsort(importances)[::-1]\n\n# Print the feature ranking\nprint(\"Feature ranking:\")\n\nfor f in range(xx_train.shape[1]):\n    print(\"%d. feature %d (%f)\" % (f + 1, indices[f], importances[indices[f]]))\n\n# Plot the feature importances of the forest\n\nplt.figure(1, figsize=(14, 13))\nplt.title(\"Feature importances\")\nplt.bar(range(xx_train.shape[1]), importances[indices],\n       color=\"g\", yerr=std[indices], align=\"center\")\nplt.xticks(range(xx_train.shape[1]), xx_train.columns[indices],rotation=90)\nplt.xlim([-1, xx_train.shape[1]])\nplt.show()","32f381c2":"ac_f = accuracy_score(yy_test,clf_rf_final.predict(xx_test))\nprint('Accuracy is: ',ac_f)\ncm_f = confusion_matrix(yy_test,clf_rf_final.predict(xx_test))\nsns.heatmap(cm_f,annot=True,fmt=\"d\")","1540dce8":"### 6) Feature Extraction with PCA\n\n\nhttp:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.decomposition.PCA.html We will use principle component analysis (PCA) for feature extraction. Before PCA, we need to normalize data for better performance of PCA.","5999ccdc":"Okey, now we have 9 features but what does they mean (not just Sensors meaning) or actually how much do we need to know about these features.\n\nThe answer is that we do not need to know meaning of these features however in order to imagine in our mind we should know something like variance, standart deviation, number of sample (count) or max min values. \n\nThese type of information helps to understand about what is going on data.","e98d7251":"__Observation:__ \n\n* According to variance ration, **2 component can be chosen.**","6381768d":"Finally, we find best 2 features that are `'sensor6', 'sensor8'` for best classification. Lets look at best accuracy with plot.","c8f4c022":"# Visualization","a719a507":"__Observation:__ Features are not correlated.","cd29e7db":"__Observation__ : the methodes varies and the Acc is the same with 9features","83632340":"There are 3 things that take my attention \n1) There is an Sample index that cannot be used for classificaiton \n\n2) `class_label` is our class label\n\n3) we have 9 sensors to do our classification.\n\nTherefore, drop sample_index feature.","dd875248":"# Conclusion\n\nShortly, I tried to show importance of feature selection and data visualization. \n\nDefault data includes 9 feature but after feature selection we drop this number from 9 to 2 with accuracy __99%__. \n\nIn this kernel we just tried basic things, I am sure with these data visualization and feature selection methods, you can easily ecxeed the % 99% accuracy. \n\nOr Maybe you can use other classification methods.","70914dca":"### 2) Univariate feature selection and random forest classification\n\nIn univariate feature selection, we will use SelectKBest that removes all but the k highest scoring features. http:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.feature_selection.SelectKBest.html#sklearn.feature_selection.SelectKBest\n\n","eca41b02":"__Observation__: the accuracy droped to 92.5% so these are not the best features.","3c1e16d3":"__Let's try 9 features like the previous method then afterward we will stick with 5 features only.__","dab226c2":"### 3) Recursive feature elimination (RFE) with random forest\n\nhttp:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.feature_selection.RFE.html Basically, it uses one of the classification methods (random forest in our example), assign weights to each of features. \nWhose absolute weights are the smallest are pruned from the current set features. That procedure is recursively repeated on the pruned set until the desired number of features","8003e894":"__Observation:__ Lets look at what we did up to this point. \nLets accept that guys this data is very easy to classification. However, our first purpose is actually not finding good accuracy. Our purpose is learning how to make feature selection and understanding data. Then last make our last feature selection method.","84b5a732":"Chosen 5 best features by rfe are :\n`'sensor0', 'sensor3', 'sensor4', 'sensor6', 'sensor8'`. \nThey are exactly similar with previous (selectkBest) method with a slight change 1 by 6 , & 3 by 2\nTherefore we do not need to calculate accuracy again. Shortly, we can say that we make good feature selection with rfe and selectkBest methods. However as you can see there is a problem, okey I except we find best 5 feature with two different method and these features are same but why it is 5.  Therefore lets see how many feature we need to use with __rfecv method.__","a93409b4":"Like previous method, we will use 5 features. However, which 5 features will we use ? We will choose them with RFE method.","5a57e34a":"## Feature Selection and Random Forest Classification\n\nToday our purpuse is to try new `coffee`. For example, we are finaly in the coffe shop and we want to drink different tastes. Therefore, we need to compare ingredients of drinks. If one of them includes Milk, after drinking it we need to eliminate other drinks which includes Milk so as to experience very different tastes.\n\nIn this part we will select feature with different methods that are:\n\n* feature selection with correlation, \n* univariate feature selection, \n* recursive feature elimination (RFE), \n* recursive feature elimination with cross validation (RFECV) and \n* tree based feature selection. \n\nWe will use random forest classification in order to train our model and predict","3aa903ca":"# Sensors Data which is Artificially Generated\n\n\nTask description\n----------------\nThe file task_data.csv contains an example data set that has been artificially\ngenerated. The set consists of 400 samples where for each sample there are 10\ndifferent sensor readings available. The samples have been divided into two\nclasses where the class label is either `1 or -1`. The class labels define to what\nparticular class a particular sample belongs.\n\nYour task is to rank the sensors according to their importance\/predictive power\nwith respect to the class labels of the samples. Your solution should be a\nPython script or a Jupyter notebook file that generates a ranking of the sensors\nfrom the provided CSV file. The ranking should be in decreasing order where the\nfirst sensor is the most important one.\n\nAdditionally, please include an analysis of your method and results, with\npossible topics including:\n\n* your process of thought, i.e., how did you come to your solution?\n* properties of the artificially generated data set\n* strengths of your method: why does it produce a reasonable result?\n* weaknesses of your method: when would the method produce inaccurate results?\n* scalability of your method with respect to number of features and\/or samples\n* alternative methods and their respective strengths, weaknesses, scalability","7741ad03":"## here we will try all the 9 sensors.","8681cc1e":"# You remarks and suggestions are much appreciated.  \ud83d\udca1\ud83d\udca1\ud83d\udca1\n\n## \u2b06\ufe0f\u2b06\ufe0fUpvote this Notebook if you find it usefull \u2b06\ufe0f\u2b06\ufe0f\n","814d0663":"Our data has 0 NANs.\n\n* Next let's check the Categorical\/ Numerical features.","24289d3c":"### 5) Tree based feature selection and random forest classification\n\nhttp:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.ensemble.RandomForestClassifier.html In random forest classification method there is a featureimportances attributes that is the feature importances (the higher, the more important the feature). \n\n__!!! To use feature_importance method, in training data there should not be correlated features. Random forest choose randomly at each iteration, therefore sequence of feature importance list can change.__","c3d35d6d":"__Observation:__ As you can seen in plot above, after 5 best features importance of features decrease. Therefore we can focus these 5 features. As I sad before, I give importance to understand features and find best of them.","47e1cefb":"* Our data is balances. ( each class contains 200).","0f82b442":"__Observation:__ \n\n* Like __RFECV Method__ & __Feature Extraction with PCA__ mentioned that `Sensor6 & Sensor8` are the best so let's use them and see if the Accuracy will rise.","b86a5065":"__Remark:__ if we reduce the K to 5, the Accuracy will drop meaningfully.\n\n\n__So lets se what happens if we use only these best scored 5 feature.__\nNow, I will not try all combinations but I will only choose k = 5 and then  find best 5 features.","43937e8d":"### 4) Recursive feature elimination with cross validation and random forest classification\n\nhttp:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.feature_selection.RFECV.html Now we will not only find best features but we also find how many features do we need for best accuracy.","cf879e6d":"__Observation__ : Accuracy is almost 98% and as it can be seen in confusion matrix, we make 2 wrong predictions. Now lets see other feature selection methods to find better results.","afe91faa":"### 1) Feature selection with correlation and random forest classification"}}