{"cell_type":{"bf6d036e":"code","2a59d11f":"code","3d0c6b68":"code","9b0c3418":"code","b4707d6c":"code","e69fdecc":"code","1d5f27eb":"code","9c714b23":"code","c8bbc41b":"code","7d4c0b40":"code","fb5f8055":"code","73524c60":"code","ce36081e":"code","b0d0dda9":"code","9d3b7812":"code","750de80c":"code","cf8ef19e":"code","5e7bd9db":"code","b4734fb9":"code","e18b8a28":"code","71b89d63":"code","9b141d62":"code","b08d8454":"code","4bf60abf":"code","1ef2c2db":"code","093b6f65":"code","60e05ad4":"code","93f7cba4":"code","064cdcc6":"code","d0173b49":"code","f2cf0cb3":"code","b85d069f":"code","24236594":"code","75638793":"code","08eca993":"code","7d29a275":"code","05d6e176":"code","2353f7fd":"code","7468028b":"code","5ae761eb":"code","5e789702":"code","d90c3b39":"code","23f9dd1a":"code","ae60604e":"code","04eeaf76":"code","64cc735d":"code","d2ef2ff3":"markdown","c470f209":"markdown","c21f966d":"markdown","3f7c8e3b":"markdown","9b4d3431":"markdown","2e50522d":"markdown","7ca53129":"markdown","770a1a26":"markdown","10ae7e39":"markdown","610355fd":"markdown","d860a3d9":"markdown","e86e73bb":"markdown","dee40b79":"markdown","5dd33c9f":"markdown","3219f57b":"markdown","3131aa16":"markdown","1809e0aa":"markdown","5e7a2979":"markdown","6bd17911":"markdown","87eb5901":"markdown","afb5f939":"markdown","02bf09a7":"markdown","e54b5c04":"markdown","40132e8e":"markdown","9fb13345":"markdown","aafc1e50":"markdown"},"source":{"bf6d036e":"!pip install --upgrade scikit-learn","2a59d11f":"import sklearn\nsklearn.__version__","3d0c6b68":"import pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.neighbors import LocalOutlierFactor\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.preprocessing import OrdinalEncoder\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.pipeline import Pipeline, FeatureUnion\nfrom sklearn.model_selection import StratifiedKFold, KFold\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import plot_confusion_matrix\nfrom sklearn.metrics import ConfusionMatrixDisplay\nfrom sklearn.tree import plot_tree\nimport joblib\n\nfrom sklearn.base import BaseEstimator, TransformerMixin\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.feature_selection import SelectKBest, f_classif, mutual_info_classif, chi2\n\nfrom sklearn.metrics import make_scorer\nfrom sklearn.metrics import accuracy_score\n\n# Rich visual representation of estimators (new 0.23.2)\nfrom sklearn import set_config\nset_config(display='diagram')\n","9b0c3418":"# columns used \ncolumns = ['age', 'workclass', 'fnlwgt', 'education', 'education_num',\n           'marital_status', 'occupation', 'relationship', 'race', \n           'sex','capital_gain', 'capital_loss', 'hours_per_week',\n           'native_country','high_income']\n# importing the dataset\nincome = pd.read_csv(\"https:\/\/archive.ics.uci.edu\/ml\/machine-learning-databases\/adult\/adult.data\",\n                   header=None,\n                   names=columns)\nincome.head()","b4707d6c":"# There are duplicated rows\nincome.duplicated().sum()","e69fdecc":"# Delete duplicated rows\nincome.drop_duplicates(inplace=True)\nincome.duplicated().sum()","1d5f27eb":"# Verify if columns[int64] has outliers (with data leakage!!!!!!!)\n\n# data\nx = income.select_dtypes(\"int64\")\n\n# identify outlier in the dataset\nlof = LocalOutlierFactor()\noutlier = lof.fit_predict(x)\nmask = outlier != -1\n\nprint(\"Income shape [original]: {}\".format(income.shape))\nprint(\"Income shape [outlier removal]: {}\".format(income.loc[mask,:].shape))\n\n# income with outliner\nincome_w = income.loc[mask,:].copy()\nincome_w.head()","9c714b23":"# define a categorical encoding for target variable\nle = LabelEncoder()\n\n# fit and transoform y_train\nincome_w[\"high_income\"] = le.fit_transform(income_w.high_income)","c8bbc41b":"le.classes_","7d4c0b40":"income_w.head()","fb5f8055":"#Custom Transformer that extracts columns passed as argument to its constructor \nclass FeatureSelector( BaseEstimator, TransformerMixin ):\n    #Class Constructor \n    def __init__( self, feature_names ):\n        self.feature_names = feature_names \n    \n    #Return self nothing else to do here    \n    def fit( self, X, y = None ):\n        return self \n    \n    #Method that describes what we need this transformer to do\n    def transform( self, X, y = None ):\n        return X[ self.feature_names ]","73524c60":"# Handling categorical features \nclass CategoricalTransformer( BaseEstimator, TransformerMixin ):\n  # Class constructor method that takes one boolean as its argument\n  def __init__(self, new_features=True):\n    self.new_features = new_features\n    self.colnames = None\n\n  #Return self nothing else to do here    \n  def fit( self, X, y = None ):\n    return self \n\n  def get_feature_names(self):\n        return self.colnames.tolist()\n\n  # Transformer method we wrote for this transformer \n  def transform(self, X , y = None ):\n    df = X.copy()\n\n    # customize feature?\n    # how can I identify this one? EDA!!!!\n    if self.new_features: \n      \n      # minimize the cardinality of native_country feature\n      df.loc[df['native_country']!=' United-States','native_country'] = 'non_usa' \n\n      # replace ? with Unknown\n      edit_cols = ['native_country','occupation','workclass']\n      for col in edit_cols:\n        df.loc[df[col] == ' ?', col] = 'unknown'\n\n      # decrease the cardinality of education feature\n      hs_grad = [' HS-grad',' 11th',' 10th',' 9th',' 12th']\n      elementary = [' 1st-4th',' 5th-6th',' 7th-8th']\n      # replace\n      df['education'].replace(to_replace = hs_grad,value = 'HS-grad',inplace = True)\n      df['education'].replace(to_replace = elementary,value = 'elementary_school',inplace = True)\n\n      # adjust marital_status feature\n      married= [' Married-spouse-absent',' Married-civ-spouse',' Married-AF-spouse']\n      separated = [' Separated',' Divorced']\n      # replace \n      df['marital_status'].replace(to_replace = married ,value = 'Married',inplace = True)\n      df['marital_status'].replace(to_replace = separated,value = 'Separated',inplace = True)\n\n      # adjust workclass feature\n      self_employed = [' Self-emp-not-inc',' Self-emp-inc']\n      govt_employees = [' Local-gov',' State-gov',' Federal-gov']\n      # replace elements in list.\n      df['workclass'].replace(to_replace = self_employed ,value = 'Self_employed',inplace = True)\n      df['workclass'].replace(to_replace = govt_employees,value = 'Govt_employees',inplace = True)\n\n    # update column names\n    self.colnames = df.columns      \n  \n    return df","ce36081e":"# \n# for validation purposes\n#\n\n#model = FeatureSelector(income_w.select_dtypes(\"object\").columns.to_list())\n#df = model.fit_transform(income_w)\n#df.head()","b0d0dda9":"# \n# for validation purposes\n#\n\n#model = CategoricalTransformer(new_features=True)\n#df_cat = model.fit_transform(df)\n#df_cat.head()","9d3b7812":"# check the cardinality before and after transformation\n#income_w.select_dtypes(\"object\").apply(pd.Series.nunique)","750de80c":"# check the cardinality before and after transformation\n#df_cat.apply(pd.Series.nunique)","cf8ef19e":"# transform numerical features\nclass NumericalTransformer( BaseEstimator, TransformerMixin ):\n  # Class constructor method that takes a model parameter as its argument\n  # model 0: minmax\n  # model 1: standard\n  # model 2: without scaler\n  def __init__(self, model = 0):\n    self.model = model\n    self.colnames = None\n\n  #Return self nothing else to do here    \n  def fit( self, X, y = None ):\n    return self\n\n  # return columns names after transformation\n  def get_feature_names(self):\n        return self.colnames \n        \n  #Transformer method we wrote for this transformer \n  def transform(self, X , y = None ):\n    df = X.copy()\n    \n    # update columns name\n    self.colnames = df.columns.tolist()\n    \n    # minmax\n    if self.model == 0: \n      scaler = MinMaxScaler()\n      # transform data\n      df = scaler.fit_transform(df)\n    elif self.model == 1:\n      scaler = StandardScaler()\n      # transform data\n      df = scaler.fit_transform(df)\n    else:\n      df = df.values\n\n    return df","5e7bd9db":"# \n# for validation purposes\n#\n\n#model = FeatureSelector(income_w.select_dtypes(\"int64\").columns.to_list()[:-1])\n#df = model.fit_transform(income_w)\n#df.head()","b4734fb9":"# \n# for validation purposes\n#\n\n#model = NumericalTransformer(model=2)\n#df_cat = model.fit_transform(df)\n#df_cat","e18b8a28":"# split-out train\/validation and test dataset\nX_train, X_test, y_train, y_test = train_test_split(income_w.drop(labels=\"high_income\",axis=1),\n                                                    income_w[\"high_income\"],\n                                                    test_size=0.20,\n                                                    random_state=41,\n                                                    shuffle=True,\n                                                    stratify=income_w[\"high_income\"])","71b89d63":"# Categrical features to pass down the categorical pipeline \ncategorical_features = X_train.select_dtypes(\"object\").columns.to_list()\n\n# Numerical features to pass down the numerical pipeline \nnumerical_features = X_train.select_dtypes(\"int64\").columns.to_list()\n\n# Defining the steps in the categorical pipeline \ncategorical_pipeline = Pipeline(steps = [('cat_selector', FeatureSelector(categorical_features)),\n                                         ('cat_transformer', CategoricalTransformer()),\n                                         ('cat_encoder','passthrough')\n                                         #('cat_encoder',OneHotEncoder(sparse=False,drop=\"first\"))\n                                         ]\n                                )\n\n# Defining the steps in the numerical pipeline     \nnumerical_pipeline = Pipeline(steps = [('num_selector', FeatureSelector(numerical_features)),\n                                       ('num_transformer', NumericalTransformer()) \n                                       ]\n                              )\n\n# Combining numerical and categorical piepline into one full big pipeline horizontally \n# using FeatureUnion\nfull_pipeline_preprocessing = FeatureUnion(transformer_list = [('cat_pipeline', categorical_pipeline),\n                                                               ('num_pipeline', numerical_pipeline)\n                                                               ]\n                                           )","9b141d62":"# \n# for validate purposes\n#\n\n#new_data = full_pipeline_preprocessing.fit_transform(X_train)\n#catnames = full_pipeline_preprocessing.get_params()[\"cat_pipeline\"][2].get_feature_names().tolist()\n#numnames = full_pipeline_preprocessing.get_params()[\"num_pipeline\"][1].get_feature_names()\n#df = pd.DataFrame(new_data,columns = catnames + numnames)\n#df.head()","b08d8454":"#df.shape","4bf60abf":"# global varibles\nseed = 15\nnum_folds = 10\nscoring = {'AUC': 'roc_auc', 'Accuracy': make_scorer(accuracy_score)}","1ef2c2db":"# See documentation for more info\n# https:\/\/scikit-learn.org\/stable\/auto_examples\/compose\/plot_compare_reduction.html#sphx-glr-auto-examples-compose-plot-compare-reduction-py\n\n# The full pipeline \npipe = Pipeline(steps = [('full_pipeline', full_pipeline_preprocessing),\n                         (\"fs\",SelectKBest()),\n                         (\"classifier\",DecisionTreeClassifier())])\n# create a dictionary with the hyperparameters\nsearch_space = [{\"classifier\":[DecisionTreeClassifier()],\n                 \"classifier__criterion\": [\"entropy\"],  #[\"gini\",\"entropy\"],\n                 \"classifier__splitter\": [\"best\"],\n                 \"fs__k\":[15],  # [10,15,20] \n                 \"fs__score_func\": [chi2],  #[f_classif, mutual_info_classif, chi2],\n                 \"full_pipeline__cat_pipeline__cat_encoder\": [OneHotEncoder(sparse=False,drop=\"first\")], #[OneHotEncoder(sparse=False,drop=\"first\"),\n                                                            #OrdinalEncoder()],\n                 \"full_pipeline__cat_pipeline__cat_transformer__new_features\":[True],\n                 \"full_pipeline__num_pipeline__num_transformer__model\": [0]}, #[0,2]},\n                {\"classifier\": [KNeighborsClassifier()],\n                \"classifier__n_neighbors\": [3],\n                 \"full_pipeline__cat_pipeline__cat_encoder\":[OneHotEncoder(sparse=False,drop=\"first\")]}]\n\n# create grid search\nkfold = StratifiedKFold(n_splits=num_folds,random_state=seed,shuffle=True)\n\n# see other scoring\n# https:\/\/scikit-learn.org\/stable\/modules\/model_evaluation.html#scoring-parameter\ngrid = GridSearchCV(estimator=pipe, \n                    param_grid=search_space,\n                    cv=kfold,\n                    scoring=scoring,\n                    return_train_score=True,\n                    n_jobs=-1,\n                    refit=\"Accuracy\")\n\n# fit grid search\nall_models = grid.fit(X_train,y_train)","093b6f65":"all_models","60e05ad4":"print(\"Best: %f using %s\" % (all_models.best_score_,all_models.best_params_))","93f7cba4":"result = pd.DataFrame(all_models.cv_results_)\nresult","064cdcc6":"# Just change column name from \"test\" to \"validation\" for not confuse\nresult_auc = result[['mean_train_AUC', 'std_train_AUC','mean_test_AUC', 'std_test_AUC',\"rank_test_AUC\"]].copy()\nfor col in result_auc.columns:\n  result_auc.rename(columns={col:col.replace(\"test\",\"validation\")}, inplace=True)\nresult_auc","d0173b49":"# Just change column name from \"test\" to \"validation\" for not confuse\nresult_acc = result[['mean_train_Accuracy', 'std_train_Accuracy','mean_test_Accuracy', 'std_test_Accuracy',\"rank_test_Accuracy\"]].copy()\nfor col in result_acc.columns:\n  result_acc.rename(columns={col:col.replace(\"test\",\"validation\")}, inplace=True)\nresult_acc","f2cf0cb3":"# final model\npredict = all_models.predict(X_test)","b85d069f":"# confusion matrix (we change the way to make equal to slides)\n#             true label\n#               1     0     \n# predict  1    TP    FP\n#          0    FN    TN\n#\n\nconfusion_matrix(predict,y_test,\n                 labels=[1,0])","24236594":"print(accuracy_score(y_test, predict))\nprint(classification_report(y_test,predict))","75638793":"fig, ax = plt.subplots(1,1,figsize=(7,4))\n\nConfusionMatrixDisplay(confusion_matrix(predict,y_test,labels=[1,0]),\n                       display_labels=[\">50k\",\"<=50k\"]).plot(values_format=\".0f\",ax=ax)\n\nax.set_xlabel(\"True Label\")\nax.set_ylabel(\"Predicted Label\")\nplt.show()","08eca993":"# columns used in the model (k columns)\nfeatures = all_models.best_estimator_.named_steps['fs']\nfeatures.get_support()","7d29a275":"# All information is trackable going back in the Pipeline\n# categorical columns\nfeatures_full = all_models.best_estimator_.named_steps['full_pipeline']\nfeatures_cat = features_full.get_params()[\"cat_pipeline\"]\nfeatures_cat[2].get_feature_names()","05d6e176":"features_full","2353f7fd":"# numerical columns\nfeatures_full.get_params()[\"num_pipeline\"][1].get_feature_names()","7468028b":"all_columns = features_cat[2].get_feature_names().tolist() + features_full.get_params()[\"num_pipeline\"][1].get_feature_names()\nall_columns","5ae761eb":"selected_columns = [value for (value, filter) in zip(all_columns, features.get_support()) if filter]\nselected_columns","5e789702":"fig, ax = plt.subplots(1,1,figsize=(12,8))\n\nxticks = [x for x in range(len(features.scores_))] \nax.bar(xticks, features.scores_)\nax.set_xticks(xticks)\nax.set_xticklabels(all_columns,rotation=90)\n#ax.set_xticks(ticks=xticks, labels=all_columns,rotation=90,fontsize=15)\nax.set_title(\"Feature Importance\")\nplt.show()","d90c3b39":"all_models.best_estimator_.named_steps['classifier']","23f9dd1a":"from sklearn.tree import plot_tree # to draw a classification tree\nplt.figure(figsize=(15, 7.5))\nplot_tree(all_models.best_estimator_.named_steps['classifier'], \n          filled=True, \n          rounded=True, \n          class_names=[\"<=50k\", \">50k\"],\n          feature_names=all_columns)","ae60604e":"# Save the model using joblib\nwith open('pipe.joblib', 'wb') as file:\n  joblib.dump(all_models, file)","04eeaf76":"# Under the production environment [joblib]\nwith open('pipe.joblib', 'rb') as file:\n  model = joblib.load(file)\n\n# final model\npredict = model.predict(X_test)","64cc735d":"print(accuracy_score(y_test, predict))\nprint(classification_report(y_test,predict))","d2ef2ff3":"This dataset contains a mix of **categorical (9 columns)** and **numerical (6 columns)** independent variables which as we know will need to pre-processed in different ways and separately.\n\nThis means that initially they\u2019ll have to go through **separate pipelines** to be pre-processed appropriately and then we\u2019ll combine them together. So the first step in both pipelines would have to be to extract the appropriate columns that need to be pushed down for pre-processing.","c470f209":"## 1.7 Finalize the model","c21f966d":"### 1.4.3 Encoding target variable","3f7c8e3b":"#### 1.5.4.1 Evaluate","9b4d3431":"1. https:\/\/towardsdatascience.com\/an-easier-way-to-encode-categorical-features-d840ff6b3900\n2. https:\/\/towardsdatascience.com\/machine-learning-feature-encoding-with-onehotencoder-inside-a-pipeline-76d440940f4b","2e50522d":"### 1.5.1 Column extractor","7ca53129":"#### 1.5.3.1 Evaluate","770a1a26":"### 1.5.3 Numerical transformation","10ae7e39":"### 1.4.2 Removal Outliers","610355fd":"- Other models using Ensemble","d860a3d9":"If a categorical target variable needs to be encoded for a classification predictive modeling problem, then the [LabelEncoder class](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.preprocessing.LabelEncoder.html) can be used.","e86e73bb":"## 1.8 Next","dee40b79":"# References","5dd33c9f":"### 1.4.1 Removal duplications","3219f57b":"# 1.0 An end-to-end classification problem\n\n","3131aa16":"## 1.2 Load Libraries","1809e0aa":"## 1.4  Clean, prepare and manipulate Data (feature engineering)","5e7a2979":"## 1.3 Get data & EDA","6bd17911":"### 1.3.1 Import the dataset","87eb5901":"### 1.5.4 Pipeline union (cat + num)","afb5f939":"### 1.5.2 Categorical transformation","02bf09a7":"#### 1.5.2.1 Evaluate","e54b5c04":"## 1.6 Algorithm tuning","40132e8e":"## 1.1 Dataset description","9fb13345":"\n\nWe'll be looking at individual income in the United States. The **data** is from the **1994 census**, and contains information on an individual's **marital status**, **age**, **type of work**, and more. The **target column**, or what we want to predict, is whether individuals make less than or equal to 50k a year, or more than **50k a year**.\n\nYou can download the data from the [University of California, Irvine's website](http:\/\/archive.ics.uci.edu\/ml\/datasets\/Adult).\n\nLet's take the following steps:\n\n1. Load Libraries\n2. Get data, including EDA\n3. Clean, prepare and manipulate Data (feature engineering)\n4. Modeling (train and test)\n5. Algorithm Tuning\n6. Finalizing the Model\n","aafc1e50":"## 1.5 Pipeline "}}