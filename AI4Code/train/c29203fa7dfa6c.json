{"cell_type":{"8e5bc570":"code","7983adbd":"code","bc8c368b":"code","67c5b003":"code","c89bf09e":"code","492e88d4":"code","26282225":"code","c2b3ee54":"code","52bb4bca":"code","a25df6e0":"code","c99bcd36":"code","f6537525":"markdown","95619a05":"markdown","01215090":"markdown","bc36d7eb":"markdown"},"source":{"8e5bc570":"# Download EfficientNet from LukeMK\n! pip install efficientnet-pytorch","7983adbd":"# Importing the libraries\nfrom fastai.vision import *\nfrom efficientnet_pytorch import EfficientNet","bc8c368b":"# Define the path\npath = Path('\/kaggle\/input\/flowers-recognition\/flowers')\npath.ls()","67c5b003":"# Create the data using fastai's Datablock API\nsrc = (ImageList.from_folder(path)\n                .split_by_rand_pct(0.2, seed=42)\n                .label_from_folder()\n                .transform(get_transforms(), size=300))\n\ndata = src.databunch(bs=8).normalize(imagenet_stats)","c89bf09e":"# Let's see some training examples\ndata.show_batch(rows=2, figsize=(9, 6))","492e88d4":"# Replace the fully connected layer at the end to fit our task\n# Pre-trained model based on adversarial training\narch = EfficientNet.from_pretrained(\"efficientnet-b3\", advprop=True)\narch._fc = nn.Linear(1536, data.c)","26282225":"# Define custom loss function\nloss_func = LabelSmoothingCrossEntropy()","c2b3ee54":"# Define the model\nlearn = Learner(data, arch, loss_func=loss_func, metrics=accuracy, model_dir='\/kaggle\/working')","52bb4bca":"# Train the model using 1 Cycle policy\nlearn.fit_one_cycle(3, slice(1e-3))","a25df6e0":"# Unfreeze the model and retrain\nlearn.unfreeze()\nlearn.fit_one_cycle(2, slice(1e-5))","c99bcd36":"# Let's see the result\nlearn.show_results(rows=2, figsize=(9, 6))","f6537525":"| Coefficient | Width | Depth | Resolution | Dropout | Last layer |\n|:-----------:|:-----:|:-----:|:----------:|:-------:|:----------:|\n|      b0     |  1.0  |  1.0  |     224    |   0.2   |1280|\n|      b1     |  1.0  |  1.1  |     240    |   0.2   |1280|\n|      b2     |  1.1  |  1.2  |     260    |   0.3   |1408|\n|      b3     |  1.2  |  1.4  |     300    |   0.3   |1536|\n|      b4     |  1.4  |  1.8  |     380    |   0.4   |1792|\n|      b5     |  1.6  |  2.2  |     456    |   0.4   |2048|\n|      b6     |  1.8  |  2.6  |     528    |   0.5   |2304|\n|      b7     |  2.0  |  3.1  |     600    |   0.5   |2560|","95619a05":"* EfficientNet comes with a variety of sub-models **from b0 to b7**\n* The larger the model, the higher the amount of **width, depth, resolution, and dropout**\n* We start out with image size of **224x224**, so let's use **b0** first as a baseline\n* We can use bigger images later with larger models","01215090":"* Normally, we can use a common model such as **ResNet** for this task!\n* However, I want to test out **EfficientNet** with fastai this time! :))","bc36d7eb":"* In conclusion, our model was able to reach **96-97% accuracy** using b3 with 300x300 images!\n* However, it seemed that the model **consumed a lot of memory and training time!** \n* I was not able to train with b7 using 600x600 images. \n* I'm pretty sure that the result **will be even better** if I could use that!\n* Hope that I will be able to find out the issue next time. :((\n* The good thing is that the model **converges very fast after just 1-2 epochs**! :))"}}