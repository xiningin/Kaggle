{"cell_type":{"3456c5ee":"code","90e82fad":"code","cab1ef34":"code","8693518d":"code","88ba9429":"code","fd74aeab":"code","391dc752":"code","76742759":"code","d6af6467":"code","dbe8ed1d":"code","888d4326":"code","7eb50773":"code","a4e9cc34":"code","60fcc224":"code","9d999ece":"code","58d108ce":"code","0841160d":"code","bb42dd5e":"code","c66a7a6c":"markdown","f3607f7b":"markdown","c1a3f886":"markdown","c70ce223":"markdown","5f9bab01":"markdown","096112c5":"markdown","43f4a61c":"markdown","d953864c":"markdown","56bf69d5":"markdown","2d10ee66":"markdown","234bb4fd":"markdown","242dea00":"markdown","91deb6e6":"markdown","719a484b":"markdown","370d15d4":"markdown","f602fdf5":"markdown","583bc0a7":"markdown","eec66847":"markdown","32fda67c":"markdown"},"source":{"3456c5ee":"#select Python Packages\nimport tensorflow as tf\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport IPython.display\nimport PIL.Image\nimport tensorflow_datasets as tfds\nfrom tensorflow import keras","90e82fad":"from tensorflow.keras.datasets import fashion_mnist","cab1ef34":"#import\/load the fashion-mnist dataset\n\n(train_images,train_labels), (test_images, test_labels) = fashion_mnist.load_data()","8693518d":"#Answer Questions 1-5 - Reshape & display 100 images from the fashion_mnist training dataset\n\n#Q1 - Determining the labels in the dataset\nprint('Question 1:')\nprint()\nprint('Our unique labels are: {}'.format(np.unique(train_labels)))\nprint('The total number of labels are %d' % np.ptp(train_labels,axis=0))\n\n#Q2 - Determining how many training images and test images\nprint()\nprint('Question 2:')\nprint()\nprint('There are %d training images' % len(train_images))\nprint('There are %d test images' % len(test_images))\n\n#Q3 - Determining the size of each image set\nprint()\nprint('Question 3:')\nprint()\nprint(train_images.shape)\nprint(test_images.shape)\nprint('The size of our images are 28 pixels by 28 pixels')\n\n#Q4 - Numeric range of the input\nprint()\nprint('Question 4:')\nprint()\nprint('The numerical range of our train_image data is %d' %np.ptp(train_images))\nprint('The numerical range of our test_image data is %d' %np.ptp(test_images))\nprint()\nprint('These results indicates that we should rescale our data to a range of 0 to 1, which we can do in our input pipeline task, dividing our data by 255')\n\n#Q5 - What are the shapes of the input and target tensors, do we need to reshape?\nprint()\nprint('Question 5:')\nprint()\nprint('Input tensor shape: {}'.format(train_images.shape))\nprint('Target tensor shape: {}'.format(test_images.shape))\nprint()\nprint(\"Both of our input and target tensors have the same image shape, of 28 by 28 pixels - however we do need to reshape our data to a 1D tensor in our pipeline\")","88ba9429":"#Display 100 images from the train_images dataset\n\nprint()\nplt.figure(figsize=(10,10))\nfor i in range(100):\n    plt.subplot(10,10, i+1)\n    plt.suptitle('100 Images Displayed in a 10x10 Matrix', color='k', size=20)\n    plt.xticks([])\n    plt.yticks([])\n    plt.grid(False)\n    plt.imshow(train_images[i], cmap= plt.cm.binary)\nplt.show();","fd74aeab":"\n#Creating validation dataset to go with training and test dataset imported\n\ntrain_images = train_images[6000:]\ntrain_labels = train_labels[6000:]\nval_images = train_images[:6000]\nval_labels = train_labels[:6000]\n\n#Print the shape of our image datasets\n\nprint(train_images.shape)\nprint(val_images.shape)\nprint(test_images.shape)\nprint()\n\n#Print the shape of our label datasets\nprint(train_labels.shape)\nprint(val_labels.shape)\nprint(test_labels.shape)","391dc752":"#Converting labels to categorical data\n\nfrom tensorflow.keras.utils import to_categorical\n\ntrain_labels = to_categorical(train_labels)\ntest_labels = to_categorical(test_labels)\nval_labels = to_categorical(val_labels)","76742759":"#Defining function to scale image pixels to have values between 0 and 1 - normalization pipeline\n\ndef normalize(example):\n    image = example\n    image = tf.math.divide(tf.math.subtract(image,tf.reduce_min(image)), tf.math.subtract(tf.reduce_max(image), tf.reduce_min(image)))\n    return image\n\n#Defining function to re-shape images to 784 pixels\n\ndef reshape(example):\n    image = example\n    image = tf.reshape(example, (len(image), 28 * 28))\n    return image\n\ntrain_images = reshape(train_images)\ntest_images = reshape(test_images)\nval_images = reshape(val_images)\n\ntrain_norm = normalize(train_images)\ntest_norm = normalize(test_images)\nval_norm = normalize(val_images)\n\nprint(train_norm.shape)\nprint(test_norm.shape)\nprint(val_norm.shape)\n","d6af6467":"train_tf = tf.data.Dataset.from_tensor_slices([train_norm, train_labels])\nval_tf = tf.data.Dataset.from_tensor_sclices([val_norm_val_labels])\ntest_tf = tf.data.Dataset.from_tensor_slices([test_norm, test_labels])","dbe8ed1d":"from tensorflow.keras import models, layers\n\n#Starting NN Model with 4 layers\n\nmodel = models.Sequential([\n        layers.Dense(512, activation='relu', input_shape=(28 * 28,)),\n        layers.Dense(256, activation='relu'),\n        layers.Dense(128, activation='relu'),\n        layers.Dense(10, activation='softmax')])\n\n#Compile the model\nmodel.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['accuracy'])","888d4326":"model.summary()","7eb50773":"#Plotting model structure\nfrom tensorflow.keras.utils import plot_model\nplot_model(model, show_shapes=True)","a4e9cc34":"!rm -rf .\/logs\/\n\nfrom datetime import datetime\nimport os\n\nroot_logdir = \"logs\"\nrun_id = datetime.now().strftime(\"%Y%m%d-%H%M%S\")\nlogdir = os.path.join(root_logdir, run_id)\n\ncallbacks = [\n    tf.keras.callbacks.TensorBoard(\n        log_dir=logdir,\n        histogram_freq=1,\n    )\n]","60fcc224":"#Fit the model to our training data\n\nhistory = model.fit(train_norm,\n                    train_labels,\n                    batch_size=100,\n                    epochs=1000,\n                    validation_data = (val_norm, val_labels),\n                    callbacks=[callbacks])\n\n#Evaluate test model and accuracy\n\ntest_loss, test_acc = model.evaluate(test_norm, test_labels)\nprint(f'test_loss: {test_loss}')\nprint(f'test_acc: {test_acc}')","9d999ece":"#Plot training and validation data performance\n\naccuracy = history.history['accuracy']\nval_acc = history.history['val_accuracy']\nloss = history.history['loss']\nval_loss = history.history['val_loss']\n\n\n#Plot 1 - Training vs Validation Loss\n\nepochs = range(1, len(loss) + 1)\n\nplt.figure(figsize= (8,8))\nplt.ylim(0,3)\nplt.plot(epochs, loss, 'ro', label='Training Loss')\nplt.plot(epochs, val_loss, 'r', label='Validation Loss')\nplt.title('Training vs Validation Loss')\nplt.xlabel('Epochs')\nplt.ylabel('Loss')\nplt.legend()\n\nplt.show()\n\n#Plot 2 - Training vs Validation Accuracy\n\nepochs1 = range(1, len(accuracy) +1)\n\nplt.figure(figsize= (8,8))\nplt.ylim(0.7,1)\nplt.plot(epochs1, accuracy, 'bo', label='Training Accuracy')\nplt.plot(epochs1, val_acc, 'b', label='Validation Accuracy')\nplt.title('Training vs Validation Loss')\nplt.xlabel('Epochs')\nplt.ylabel('Accuracy')\nplt.legend()\n\nplt.show()","58d108ce":"#Model 1 - Add dropout layer, optimiser - Adam\n\nmodel1 = models.Sequential([\n        layers.Dense(512, activation='relu', input_shape=(28 * 28,)),\n        layers.Dense(256, activation='relu'),\n        layers.Dropout(.2),\n        layers.Dense(128, activation='relu'),\n        layers.Dense(10, activation='softmax')])\n\nmodel1.compile(optimizer='Adam', loss='categorical_crossentropy', metrics=['accuracy'])\n\nhistory = model1.fit(train_norm,\n                    train_labels,\n                    batch_size=100, \n                    epochs=500,\n                    validation_data = (val_norm, val_labels))\n\ntest_loss1, test_acc1 = model1.evaluate(test_norm, test_labels)\n\n\n#Model 2 - Batch normalization, optimiser - rmsprop, learning rate - 0.005\n\nmodel2 = models.Sequential([\n        layers.Dense(512, activation='relu', input_shape=(28 * 28,)),\n        layers.BatchNormalization(),\n        layers.Dense(256, activation='relu'),\n        layers.Dense(128, activation='relu'),\n        layers.Dense(10, activation='softmax')])\n\nopt = keras.optimizers.RMSprop(learning_rate=0.005)\n\nmodel2.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy'])\n\nhistory = model2.fit(train_norm,\n                    train_labels,\n                    batch_size=100, \n                    epochs=500,\n                    validation_data = (val_norm, val_labels))\n                    \n\ntest_loss2, test_acc2 = model2.evaluate(test_norm, test_labels)\n\n\n#Model 3 - Add dropout, batch normalization, optimiser - Adam, learning rate - 0.005\n\nmodel3 = models.Sequential([\n        layers.Dense(512, activation='relu', input_shape=(28 * 28,)),\n        layers.BatchNormalization(),\n        layers.Dense(256, activation='relu'),\n        layers.Dropout(0.15),\n        layers.Dense(128, activation='relu'),\n        layers.Dense(10, activation='softmax')])\n\nopt2 = keras.optimizers.Adam(learning_rate=0.005)\n\nmodel3.compile(optimizer=opt2, loss='categorical_crossentropy', metrics=['accuracy'])\n\nhistory = model3.fit(train_norm,\n                    train_labels,\n                    batch_size=100, \n                    epochs=250,\n                    validation_data = (val_norm, val_labels))\n                    \n\ntest_loss3, test_acc3 = model3.evaluate(test_norm, test_labels)\n\n\n#Model 4 - Optimiser - rmsprop, learning rate - learning rate schedule\n\nmodel4 = models.Sequential([\n        layers.Dense(512, activation='relu', input_shape=(28 * 28,)),\n        layers.Dense(256, activation='relu'),\n        layers.Dense(128, activation='relu'),\n        layers.Dense(10, activation='softmax')])\n\ninitial_learning_rate = 0.01\nlr_schedule = keras.optimizers.schedules.ExponentialDecay(\n    initial_learning_rate, decay_steps=100000, decay_rate=0.96, staircase=True)\n\nopt3 = keras.optimizers.RMSprop(learning_rate=lr_schedule)\n\nmodel4.compile(optimizer=opt3, loss='categorical_crossentropy', metrics=['accuracy'])\n\nhistory = model4.fit(train_norm,\n                    train_labels,\n                    batch_size=100, \n                    epochs=250,\n                    validation_data = (val_norm, val_labels))\n                    \n\ntest_loss4, test_acc4 = model4.evaluate(test_norm, test_labels)\n\n\n#Model 5 - Add dropout, Optimiser - Adam, learning rate - learning rate schedule\n\nmodel5 = models.Sequential([\n        layers.Dense(512, activation='relu', input_shape=(28 * 28,)),\n        layers.Dense(256, activation='relu'),\n        layers.Dropout(0.1),\n        layers.Dense(128, activation='relu'),\n        layers.Dense(10, activation='softmax')])\n\ninitial_learning_rate = 0.01\nlr_schedule = keras.optimizers.schedules.ExponentialDecay(\n    initial_learning_rate, decay_steps=100000, decay_rate=0.96, staircase=True)\n\nopt4 = keras.optimizers.Adam(learning_rate=lr_schedule)\n\nmodel5.compile(optimizer=opt4, loss='categorical_crossentropy', metrics=['accuracy'])\n\nhistory = model5.fit(train_norm,\n                    train_labels,\n                    batch_size=100, \n                    epochs=100,\n                    validation_data = (val_norm, val_labels))\n                    \n\ntest_loss5, test_acc5 = model5.evaluate(test_norm, test_labels)\n\n\n#print experiment results\n\nprint('Experiment 1:')\nprint(f'test_loss: {test_loss1}')\nprint(f'test_acc: {test_acc1}')\nprint()\n\nprint('Experiment 2:')\nprint(f'test_loss: {test_loss2}')\nprint(f'test_acc: {test_acc2}')\nprint()\n\nprint('Experiment 3:')\nprint(f'test_loss: {test_loss3}')\nprint(f'test_acc: {test_acc3}')\nprint()\n\nprint('Experiment 4:')\nprint(f'test_loss: {test_loss4}')\nprint(f'test_acc: {test_acc4}')\nprint()\n\nprint('Experiment 5:')\nprint(f'test_loss: {test_loss5}')\nprint(f'test_acc: {test_acc5}')\n","0841160d":"! pip install nbconvert","bb42dd5e":"!jupyter nbconvert --to html 212140181_Assignment1_Solution.ipynb","c66a7a6c":"\nAnswer the following questions:\n\n1. Which configuration achieved the best test accuracy?\n\nThe configuration with a 20% dropout rate, using Adam as the optimiser, 500 epochs and a learning rate of 0.0001 produced the best test accuracy of my experiments, at 89.03%\n\n2. Which setting had the most impact and which one had the least impact?\n\nIt appears that the dropout rate had the highest impact, with each model using a dropout rate having an accuracy of > 86%.\n\nThe setting which had the least impact appears to be the number of epochs, given there was no correlation between a high\/low number of epochs and significantly improved test accuracy","f3607f7b":"## Task 3 Construct an input pipeline\n\n*(weight ~15%)*\n\nCreat train\/validate\/test data splits and construct tf.data pipelines. Make sure that the training data is batched.\n","c1a3f886":"Display 100 images from the train set in the form of 10x10 matrix.\n\nAnswer the following questions:\n\n1. What are the unique labels in this dataset?\n2. How many training images and how many test images?\n3. What is the size of each image?\n4. Find out the numeric range of the input. Do we need to rescale the input?\n5. In our problem, what are the shapes of input tensors and target tensors? Do you need to reshape the input?\n","c70ce223":"**Justifications**\n\n1. The training batch size\n\nA training batch size of 100 was used in order to give us enough samples per batch for our model to train the loss function and optimizers whilst still performing at a good run time\n\n\n2. The number of training epochs (at least 1,000 epochs recommended)\n\n\n1,000 epochs was used for our model, this was done to allow for a comprehensive analysis of our training and validation accuracy and loss, so we could determine the optimal number of epochs\n\n3. The learning rate. If you used momentum or a learning rate schedule, please report the configuration as well.\n\nThe default learning rate for the optimizer 'RMSProp' was used (lr=0.001), the model was attempted with a higher learning rate of 0.005, which resulted in significant model overfitting and worse performance\n","5f9bab01":"### Task 4.1 Setting up a model for training","096112c5":"Construct a deep feedforward neural network. You need to decide and report the following configurations:\n\n- Output layer: \n    - How many output nodes? \n    - Which activation function?\n- Hidden layers:\n    - How many hidden layers?\n    - How many nodes in each layer?\n    - Which activation function for each layer?\n- Input layer\n    - What is the input size?\n- The loss function\n- The metrics for model evaluation (which may be different from the loss function)\n- The optimiser\n\nJustify your model design decisions.\n\nPlot the model structure `using keras.utils.plot_model` or similar tools.","43f4a61c":"---\n**END OF ASSIGNMENT ONE**\n\n","d953864c":"## Task 5 Fine-tuning the model\n\n*(weight ~30%)*\n\nYou may see above that your model is overfitting. There are multiple things you can do. Below are some options:\n\n1. Add dropout\n2. Add Batch Normalisation\n3. Add layer-specific weight regularizations\n4. Change the learning rate\n\nApply different regularisation techniques to the model training. You may also try other techniques for improving training such as learning rate scheduling (see https:\/\/www.tensorflow.org\/guide\/keras\/train_and_evaluate#using_learning_rate_schedules).\n\nRun **five or more** experiments of different training configurations and record the test accuracy achieved in the Markdown table below. You may modify the table heading to match your experiment design.\n","56bf69d5":"## Assignment objective\n\n\n\nThis assignment is for you to demonstrate the knowledge in deep learning that you have acquired from the lectures and practical lab materials. Most tasks in this assignment are straightforward applications of the practical materials in weeks 1-5. Going through these materials before attempting this assignment is highly recommended.\n\nIn this assignment, you are going to work with the Fashion-MNIST dataset for image recognition. The dataset contains 10 classes of 28x28 grayscale images. You will see some examples in the visualization task below. \n\nThis assignment consists of five tasks.\n","2d10ee66":"** The training data has not been batched in our pipeline as the data is 'batched' using our 'fit' function for the neural network model","234bb4fd":"## Task 1 Load the data\n\n*(weight ~5%)*\n\nLoad the Fashion MNIST dataset (https:\/\/github.com\/zalandoresearch\/fashion-mnist). You may get the data via Keras (keras.datasets) or Tensorflow Datasets (tfds). ","242dea00":"**Justifications**\n\n\n---\n\n\n**Output Layer**\n\n  - How many output nodes? \n\nThere are 10 output nodes in the model, due to there being 10 classes for our model to classify images into (0-9)    \n\n  - Which activation function?\n\n'Softmax' was used as the activation function as it is more suitable for multiclass classification problems (where classifying for 'k' categories) than other activation functions, such as 'sigmoid' or 'tanh'\n    \n**Hidden layers**\n\n  - How many hidden layers?\n\nUsing 2 hidden layers, in order to provide greater reduction in our data\n\n  - How many nodes in each layer?\n\nThe first hidden layer has 256 and the second hidden layer has 128, reducing the dimensionality of our images down from the 784 nodes provided by our input layer\n\n  - Which activation function for each layer?\n\nEach of our hidden layers is using the activation function 'relu', relu is used to help center our data by bringing the mean of our data closer to 0\n\n**Input layer**\n\n  - What is the input size?\n\nThe input size is 28 * 28, in order to match the required input dimensions of our images from the Fashion MNIST dataset\n\n\n**The loss function**\n\nThe loss function deplpoyed is the categorical crossentropy loss function, as it works well for multiclass classification problems\n\n**The metrics for model evaluation (which may be different from the loss function)**\n\nAccuracy is used as the primary metric for our loss function, it is useful for classification functions as we can determine how accurate our predicted label is vs our actual image labels\n\n**The optimiser**\n\n\nRMSProp is the loss function, as it can provide a quicker convergence than functions such as 'gradient descent'","91deb6e6":"### Task 4.2 Fitting the model\n\nNow fit the model. Decide and report the following training setting:\n\n1. The training batch size\n2. The number of training epochs (at least 1,000 epochs recommended)\n3. The learning rate. If you used momentum or a learning rate schedule, please report the configuration as well.\n\nPlot the training and validation loss and accuracy. Answer the following questions:\n\n1. Do you see overfitting or underfitting? Why?\n2. If you see overfitting, at which epoch did it happen?","719a484b":"## Task 6 Generate HTML output\n\nUse *nbconvert* to convert your completed notebook into an HTML file and name it **[YourID]_assingment1_output.html**.","370d15d4":"## Task 2 Understand the data\n\n*(weight ~15%)*\n\n","f602fdf5":"## Task 4 Construct a deep forward neural network\n\n*(weight ~35%)*","583bc0a7":"# SIT744 Assignment 1: Image Classification with Deep Feedforward Neural Network\n\n\nThis is an <strong>individual<\/strong> assignment. It contributes <strong>30%<\/strong> to your final mark. Read the assignment instruction carefully.\n\n<h2> What to submit <\/h2>\n\n<p>\nThis assignment is to be completed individually and submitted to CloudDeakin. <strong>By the due date, you are required to submit the following files to the corresponding Assignment (Dropbox) in CloudDeakin<\/strong>:\n\n<ol>\n<li>\t<strong>[YourID]_assignment1_solution.ipynp<\/strong>:  This is your Python notebook solution source file. <\/li>\n<li>\t<strong>[YourID]_assingment1_output.html<\/strong>: This is the output of your Python notebook solution <emph>exported<\/emph> in HTML format.<\/li>\n<li>\tExtra files needed to complete your assignment, if any (e.g., images used in your answers).<\/li>\n<\/ol>\n<\/p>\n\n<p>\nFor example, if your student ID is: 123456, you will then need to submit the following files:\n<ul>\n<li> 123456_assignment1_solution.ipynp <\/li>\n<li> 123456_assignment1_output.html<\/li>\n<\/ul>\n<\/p>\n","eec66847":"\n|Dropout (rate)   | Batch Normalisation (Y\/N)  | Optimiser  | Learning Rate  | Number of Epochs   |  Test Accuracy |\n|---|---|---|---|---  |---|\n| 20%   | N | Adam  | 0.001  | 500 | 89.03%  |\n| N\/A  | Y  | RMSProp | 0.005   | 500 | 46.88%  |\n| 15%   | Y  | Adam  | 0.005 | 250 | 88.21%  |\n| N\/A  | N  | RMSProp | Schedule | 250 | 23.69%  |\n| 10%  | N  | Adam| Schedule  | 100 | 86.39%  |","32fda67c":"**Justifications**\n\n1. Do you see overfitting or underfitting? Why?\n\nIt appears that the model overfit, which is indicated by the validation loss gaining some separation from the general trend of the training loss in the first plot and (although less so) in the second plot with the validation accuracy gaining slight separation from the training accuracy as our training progressed.\n\n\n2. If you see overfitting, at which epoch did it happen?\n\nThe overtraining appears to commence at around the 100th epoch.\n"}}