{"cell_type":{"03946e03":"code","8131cb35":"code","73794b00":"code","9ad3e62a":"code","9d714ae6":"code","11c659db":"code","31616cb2":"code","fce1c7d0":"code","8f1a599e":"markdown","124a6f0a":"markdown","538c461b":"markdown","31ee4a32":"markdown","8dbca90f":"markdown","24d44bbd":"markdown","aa386861":"markdown","ae18bb63":"markdown"},"source":{"03946e03":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nfrom tqdm import tqdm\nimport warnings\nwarnings.filterwarnings(\"ignore\")\nfrom sklearn.preprocessing import LabelEncoder\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","8131cb35":"X_train = pd.read_csv('\/kaggle\/input\/career-con-2019\/X_train.csv')\n#X_test = pd.read_csv('\/kaggle\/input\/career-con-2019\/X_test.csv')\ny_train = pd.read_csv('\/kaggle\/input\/career-con-2019\/y_train.csv')\n#submission = pd.read_csv('\/kaggle\/input\/career-con-2019\/sample_submission.csv')\ndata_train = pd.read_csv('\/kaggle\/input\/252-features\/data_252_features.csv')\nprint('Read Finished')","73794b00":"import tsfresh as tsf\n\ndef feature_engineer(df):\n    measure_length = 128 \n    res = pd.DataFrame()\n    for i in tqdm(range(int(len(df)\/measure_length))):\n        ans_df = pd.Series()\n        sub_df = df.iloc[i*128:(i+1)*128].copy()\n        sub_df['sum_acc_linear'] = sum([sub_df['linear_acceleration_X']**2, sub_df['linear_acceleration_Y']**2, sub_df['linear_acceleration_Z']**2])**0.5\n        sub_df['sum_acc_angular'] = sum([sub_df['angular_velocity_X']**2, sub_df['angular_velocity_Y']**2, sub_df['angular_velocity_Z']**2])**0.5\n        for each in sub_df.columns:\n            if each in ['row_id','series_id','measurement_number']:\n                continue\n            ans_df[each+'_mean'] = sub_df[each].mean()\n            ans_df[each+'_mode'] = sub_df[each].mode()[0]\n            ans_df[each+'_median'] = sub_df[each].median()\n            ans_df[each+'_max'] = sub_df[each].max()\n            ans_df[each+'_min'] = sub_df[each].min()\n            ans_df[each+'_range'] = ans_df[each+'_max'] - ans_df[each+'_min']\n            ans_df[each+'_std'] = sub_df[each].std()\n            ans_df[each+'_mean'] = sub_df[each].mean()\n            ans_df[each+'count_above_mean'] = tsf.feature_extraction.feature_calculators.count_above_mean(sub_df[each])\n            ans_df[each+'count_below_mean'] = tsf.feature_extraction.feature_calculators.count_below_mean(sub_df[each])\n            \n            ans_df[each+'_abs_energy'] = tsf.feature_extraction.feature_calculators.abs_energy(sub_df[each])\n            ans_df[each+'_abs_sum_changes'] = tsf.feature_extraction.feature_calculators.absolute_sum_of_changes(sub_df[each])\n            ans_df[each+'_agg_autocorrelation'] = tsf.feature_extraction.feature_calculators.agg_autocorrelation(sub_df[each], [{'f_agg': 'mean', 'maxlag':2}])[0][1]\n            ans_df[each+'_approximate_entropy'] = tsf.feature_extraction.feature_calculators.approximate_entropy(sub_df[each], 16, 0.05)\n            ans_df[each+'_augmented_dickey_fuller'] = tsf.feature_extraction.feature_calculators.augmented_dickey_fuller(sub_df[each], [{'attr': 'pvalue'}])[0][1]\n            ans_df[each+'_binned_entropy'] = tsf.feature_extraction.feature_calculators.binned_entropy(sub_df[each], 16)\n            ans_df[each+'_cid_ce'] = tsf.feature_extraction.feature_calculators.cid_ce(sub_df[each], True)\n            ans_df[each+'_fft_real'] = list(tsf.feature_extraction.feature_calculators.fft_coefficient(sub_df[each], [{'coeff': 2, 'attr': 'real'}]))[0][1]\n            ans_df[each+'_fft_imag'] = list(tsf.feature_extraction.feature_calculators.fft_coefficient(sub_df[each], [{'coeff': 2, 'attr': 'imag'}]))[0][1]\n            ans_df[each+'_fft_abs'] = list(tsf.feature_extraction.feature_calculators.fft_coefficient(sub_df[each], [{'coeff': 2, 'attr': 'abs'}]))[0][1]\n            ans_df[each+'_fft_angle'] = list(tsf.feature_extraction.feature_calculators.fft_coefficient(sub_df[each], [{'coeff': 2, 'attr': 'angle'}]))[0][1]\n            ans_df[each+'_kurtosis'] = tsf.feature_extraction.feature_calculators.kurtosis(sub_df[each])\n            \n        res[i] = ans_df.T\n    return res.T\n\ntry:\n    print('Using feature engineered data.')\n    print('data_train shape: {}'.format(data_train.shape))\nexcept:\n    print('no data train,start feature engineering')\n    data_train = feature_engineer(X_train)\n    print('Feature Engineering Finished')\n\n","9ad3e62a":"from sklearn.preprocessing import LabelEncoder\nfrom sklearn.preprocessing import Normalizer\n\ndata_train_with_ori = data_train.copy()\ndata_train_without_ori = data_train.copy()\nfor each_feature in data_train.columns:\n    if 'orientation' in each_feature:\n        data_train_without_ori.drop(each_feature, axis = 1, inplace = True)\nprint('ori_data_train shape: {}, type: {}\\n\\nwithoutori_data_train shape:{}, type: {}\\n'.format(data_train_with_ori.shape, type(data_train_with_ori), data_train_without_ori.shape, type(data_train_without_ori)))\n\n\n\nle = LabelEncoder()\ny_train['surface'] = le.fit_transform(y_train['surface'])\ndata_label = y_train['surface']\ndata_label = data_label.astype(np.int8)\nprint('data_label shape: {}, type: {}\\n'.format(data_label.shape, type(data_label)))\n","9d714ae6":"#save_data = pd.concat([data_train, data_label], axis = 1)\n#data_train.to_csv('\/kaggle\/working\/data_252_features.csv', index = False)","11c659db":"from sklearn.model_selection import train_test_split\nX_train_with_ori, X_test_with_ori, y_train_with_ori, y_test_with_ori = train_test_split(data_train_with_ori, data_label, test_size=0.3,random_state=0)\nX_train_without_ori, X_test_without_ori, y_train_without_ori, y_test_without_ori = train_test_split(data_train_without_ori, data_label, test_size=0.3,random_state=0)\nprint('with_ori: {}, {}, {}, {}'.format(X_train_with_ori.shape, X_test_with_ori.shape, y_train_with_ori.shape, y_test_with_ori.shape))\nprint('without_ori: {}, {}, {}, {}'.format(X_train_without_ori.shape, X_test_without_ori.shape, y_train_without_ori.shape, y_test_without_ori.shape))\n\ndef normalize(data_train):\n    normalizer = Normalizer()\n    data_train = normalizer.fit_transform(data_train)\n    return data_train\n\nX_train_with_ori = normalize(X_train_with_ori)\nX_test_with_ori = normalize(X_test_with_ori)\nX_train_without_ori = normalize(X_train_without_ori)\nX_test_without_ori = normalize(X_test_without_ori)\n#print('ori type: {}, shape: {}\\nno_ori type: {}, shape: {}'.format(type(data_train_with_ori), data_train_with_ori.shape, type(data_train_without_ori), data_train_without_ori.shape))","31616cb2":"from sklearn.ensemble import RandomForestClassifier\nimport lightgbm as lgb\nfrom sklearn.model_selection import StratifiedKFold\ndef k_folds(X, y, X_test, k):\n    clf_dic = {'RFC': RandomForestClassifier(),\n               'LGB': lgb.LGBMClassifier(objective='multiclass')}\n    res = pd.DataFrame()\n    ensemble_test = np.zeros((X_test.shape[0], 9))\n    X = np.array(X)\n    y = np.array(y)\n    X_test = np.array(X_test)\n    for each in list(clf_dic.keys()):\n        print('Training {}'.format(each))\n        folds = StratifiedKFold(n_splits = k, shuffle=True, random_state=2019)\n        y_test = np.zeros((X_test.shape[0], 9))\n        y_oof = np.zeros((X.shape[0]))\n        score = 0\n        for i, (train_idx, val_idx) in  enumerate(folds.split(X, y)):\n            in_loop_train_data = X[train_idx]\n            in_loop_train_label = y[train_idx]\n            in_loop_val_data = X[val_idx]\n            in_loop_val_label = y[val_idx]\n            clf =  clf_dic[each]\n            clf.fit(in_loop_train_data, in_loop_train_label)\n            \n            y_oof[val_idx] = clf.predict(in_loop_val_data)\n            y_test += clf.predict_proba(X_test)\n            score += clf.score(in_loop_val_data, in_loop_val_label)\n            print('clf: {} Fold: {} score: {}'.format(each,i+1,clf.score(in_loop_val_data, in_loop_val_label)))\n        print('{} Avg Accuracy {}'.format(each, score\/folds.n_splits))\n        res[each] = np.argmax(y_test, axis=1)\n        ensemble_test += (y_test \/ folds.n_splits) * (score\/folds.n_splits)\n        print('\\n\\n')\n    return res, ensemble_test\nprint('Using ori data')\nres_with_ori, ensemble_test_with_ori = k_folds(X_train_with_ori, y_train_with_ori, X_test_with_ori, k = 7)\nprint('Using no ori data')\nres_without_ori, ensemble_test_without_ori = k_folds(X_train_without_ori, y_train_without_ori, X_test_without_ori, k = 7)","fce1c7d0":"from sklearn.metrics import accuracy_score\nfinal_with_ori = np.argmax(ensemble_test_with_ori, axis=1)\nfinal_without_ori = np.argmax(ensemble_test_without_ori, axis=1)\nacc_with_ori = accuracy_score(final_with_ori, y_test_with_ori)\nacc_without_ori = accuracy_score(final_without_ori, y_test_without_ori)\nprint('Use orientation data test acc: {}\\nNot Use orientation data test acc: {}'.format(acc_with_ori, acc_without_ori))","8f1a599e":"Evaluate my result\nActuacally I could not break to higher 0.6 without orientation data. Although we drop the leakage inforamtion, meanwhile we also drop some useful information that may contribute a lot.","124a6f0a":"Save Data after Feature Engineering\nI need to save my feature engineered data matrix because it takes too long (around 1.5 hours) to finished operating feature engineering function. In the kernal, I will read the feature engineered data for the next use.","538c461b":"Split Train and Test Dataset\nDetails:\n1. I apply a simple LableEncoder on y_train['surface'] which is categorical label.\n2. I apply my feature_engineer function on X_train dataset to generate 168 new features","31ee4a32":"Feature Engineering:\n* Leakage Analysis:\n* According to my explore in orientation data and the [Discussion](https:\/\/www.kaggle.com\/c\/career-con-2019\/discussion\/87239#latest-508136), I found that there is a very strong correlation between orientation data and surface. To be specific, Series m and Series n have similiar orientation data (vary a little), it turns out that the Series m and Series n are in high probability to have same surface.\n* So this phenomenon will cause that if we split our train data into train and val datasets and do not deal with orientation data, we will find quiet high CV score because we only hide the label of val data but did not hide the orientation data.\n* In me below code I check my guess with two different dataset. One is the whole dataset  including orientation data (Dataset 1) and the other is dataset without any orientation data (Dataset 2). I found that if we use Dataset 1 we can easily gei CV score (acc) higher that 0.90, but if we use Dataset 2 our CV score is around 0.60. However, there is an explian that \"maybe it means that the orientation data is an very important feature and if you drop them you can definitly get lower score!\". To check it I must submit my result to Kaggle platform to get my public leaderboard score. Unluckily, there must be something wrong with Kaggle evaluate system.... So I refer to these discussions and kernals:\n\n* https:\/\/www.kaggle.com\/c\/career-con-2019\/discussion\/84243\n\n* https:\/\/www.kaggle.com\/c\/career-con-2019\/discussion\/84434\n\n* https:\/\/www.kaggle.com\/friedchips\/the-missing-link\n\n* https:\/\/www.kaggle.com\/anjum48\/leakage-within-the-train-dataset\n\n* So it is quiet easy to get high score in CV but there will be a huge drop between CV and public leaderboard. **It means that if we evaluate our model performance with Dataset 1 it is useless. Dataset 2 will have a true reflection of your model performance.**\n\n* Here I generated several features including basic features and time series features.\n\n**Basic Features:**\n1. Mean\n2. Mode\n3. Median\n4. Max\n5. Min\n6. Range\n7. Sum Acceleration of Linear\n8. Sum Acceleration of Angular\n\n**Time Series Features**\n1. Absolute Energy: Describes the squared transient from the origin of time series data\n2. Absolute Sum of Changes: Describe absolute fluctuations between adjacent observations of time series data\n3. Agg Autocorrelation: Aggregate characteristics of differences between time series data. Describe different characteristics of time series data according to different aggregation functions\n4. Approximate Entropy: Ratio between adjacent entropy values\n5. Augmented Dickey Fuller: ADF test statistics\n6. Binned Entropy: Isometric grouping of time series data for entropy\n7. Cid Ce: Used to evaluate the complexity of time series. The more complex the series, the more valleys\n8. FFT: Fast Fourier Transform coefficient\n9. Kurtosis: Calculate kurtosis based on modified Fisher-Pearson moment statistics\n\nIn this project, each measurement contains 128 sample point which is time serries data.So I apply some time series algorithm to generate a high level time period features such as fast fourier transform. These time series features mainly focus on:\n* Fluctuation during the whole time\n* Entrophy of the serie\n* Complexity of the serie\n* Robust ability of value in the period\n* Transformation extract more information\n* Statis\n* Correlation of each sample","8dbca90f":"Training\n\nDetails:\n\nHere I choose RandomForestClassifier (RFC) and LightGBM (LGB) as my preference model.\n\nBrief Intro of RFC:\n\nIt's a classical bagging algorithm. It's composed by many Decision Trees. Every estimator will select m features with replacement in total N features to compose a Decision Tree which have a prediction. And the RFC will select the result with the highest vote of all estimators.\nWhy RFC:\n* It is very robust in Kaggle Classification Competition which can easily be regarded as start baseline.\n* It's fast for experiment iteration and low in memory occupation.\n* When in RFC algorithm, we need to split features, it take information gain into consideration (e.g. information entrophy), that could reflect feature importance.\n* It's not easy to overfit.\n* For unbalanced data sets, it can balance errors.\n\n\n\nBried Intro of LGB:\n\nIn NIPS 2017, Microsoft represented a light Grandient Boosting Decision Tree framework called LightGBM. All these boosting algorithm will focus on max information gain to find split point of features. LGB only needs a small number of samples to calculate the information gain, so it will be much faster. In order to minimize the information loss, LGB selects samples with large gradients to calculate the information gain. At the same time, some small gradient samples are kept randomly, and the information gain brought by the small gradient samples is amplified.\nAccording to some application of LGB in Kaggle, this algorithm express impressive speed and accuracy in many task.\n\n\n\nAfter several experiments (including other classifiers such as SVM, FullyConnected Neural Network etc.), I foud that the ensemble RandomForestClassifier(RFC) and LightGBMClassifier(LGB) with 7 folds Cross Validation (CV) will have the best performance. The ensemble formula is:\n\n* final_pred_prob = RFC_weights * RFC_pred_prob + LGB_wights * LGB_pred_prob\n* RFC_weights = sum_of_each_fold_CV_RFC_acc \/ k_folds\n* LGB_weights = sum_of_each_fold_CV_LGB_acc \/ k_folds\n* RFC_pred_prob = sum_of_each_fold_CV_RFC_pred_prob \/ k_folds\n* LGB_pred_prob = sum_of_each_fold_CV__pred_prob \/ k_folds\n\nSimply speaking: final prediction is voted by two Classifier. Each weight is there average acc in CV.","24d44bbd":"Read Original Data","aa386861":"Start split the dataset into train and test\n\nDetails:\n\nI lock the seed with 2020 and the test_size is 30% of all dataset.\n\n* I apply Normalization which make the data more friendly to the model and I choose L2 norm to avoid overfitting. Attention Here, why I do not apply Normalizer on the whole dataset? It's because we need to seperate train and test data. **If we apply normalization algorithm to the whole dataset, it convey the imformation of test data to train data via the normalization formula below.** Value_normalize_feature1 = Value_origional_feature1 \/ sum(Value of feature1)","ae18bb63":"**Name: Hang Chen**\n\n**Student Number: N18107496**\n\n**Class: Artificial Intelligence**\n\nImport some libs and prepare for the kernal."}}