{"cell_type":{"4bf80c8d":"code","19ea3371":"code","ad330652":"code","52e54898":"code","543017cd":"code","b1e88d53":"code","03972718":"code","750f6cf8":"code","d5f6fb72":"code","ca6090c9":"code","ee0932cc":"code","b228c02b":"code","ac86a0fe":"code","43a3c382":"code","3ece25fa":"code","c82bfdf9":"code","1a98b2d3":"code","2a05f758":"code","43a5d9df":"code","1735acd0":"code","dafaffb2":"code","8a171238":"code","0e5b7109":"code","21d02eb9":"code","03bfd689":"code","596ac667":"code","d7f97c3a":"code","d1ec20fe":"code","95ebcef2":"code","6fcd3a73":"code","7aeb3200":"code","98c6ab02":"code","594de459":"code","3c56673a":"code","63e3f2a2":"code","fb0f9e0f":"code","26023c70":"code","292ed55d":"code","a622bd75":"code","bf7710a3":"code","db975bda":"code","58d7c267":"code","3b9e0e75":"code","64029d63":"code","c1e8867e":"code","3572f01e":"code","835f1139":"code","a2f3d88f":"code","b1b58cc8":"code","273d531f":"code","c8c1a4b7":"code","11633a7f":"code","50d81b2b":"code","1affba91":"code","d69c3987":"code","42ecc4c7":"code","ec003dfd":"code","f6f04749":"code","c0cae2ff":"code","6897dd12":"code","3be508ee":"code","f9e262cc":"code","44456148":"code","f116cc77":"code","688fd89c":"code","1487c584":"code","ecf50664":"code","e9ab71a3":"code","a31fab2d":"code","b8f84fb3":"code","0474b2c3":"code","b90f0543":"code","bc0eb476":"code","d47352fa":"code","3fa917cf":"code","5e36f8b6":"code","bfc7a171":"code","f456621a":"code","fe3c2825":"code","4b173b5a":"code","bc68b86c":"code","296e247c":"code","1d7edab6":"markdown","fb01451d":"markdown","2622c7cc":"markdown","dc1bd404":"markdown","77960640":"markdown","4c02a210":"markdown","2489336f":"markdown","d73b8663":"markdown","d55ee814":"markdown","387d1eeb":"markdown","ff588ecb":"markdown","00ef4369":"markdown","c6037454":"markdown","ae3811a1":"markdown","2f830b83":"markdown","119c8816":"markdown","aef11046":"markdown","277e626c":"markdown","b94e04e3":"markdown","8ecf8153":"markdown","b6ff8933":"markdown","f087ab01":"markdown","5c06ee73":"markdown","da4b44b2":"markdown","6f5f0959":"markdown","0a5de0bf":"markdown","30166f6e":"markdown","611252f7":"markdown","861fb2b2":"markdown","7cfb814f":"markdown","61a9b348":"markdown","f0e7afb9":"markdown","a5bc0de6":"markdown","67f88897":"markdown","d2415342":"markdown","cd01e3a4":"markdown","9a7aae71":"markdown","00a27460":"markdown","b298af35":"markdown","ceccaed0":"markdown","c8b587bb":"markdown","e5a74032":"markdown","18e37d43":"markdown","5909b15c":"markdown","735106a9":"markdown","3374eb3e":"markdown","fcf514f5":"markdown","d07979ff":"markdown","24fd13f3":"markdown","bceeca1f":"markdown","aa89d6ea":"markdown","5cb216dc":"markdown","9a083208":"markdown","57741209":"markdown","03c2e260":"markdown","1971ad4a":"markdown","95916645":"markdown","402985a1":"markdown","afa9780e":"markdown","c5148d33":"markdown","b7f8f16e":"markdown","946c2f2a":"markdown","d3085252":"markdown","b6546a68":"markdown","5ae7b675":"markdown","11fbc223":"markdown","ed84e3a3":"markdown","c428731c":"markdown","401ec8bb":"markdown","34628d9f":"markdown","68fe5811":"markdown","ea16a8f8":"markdown","6ac258d4":"markdown","bde2a679":"markdown","9761549e":"markdown","ffc1adce":"markdown","7a87cc53":"markdown","8047defa":"markdown","b5286cc8":"markdown","067e8c75":"markdown","bb189a33":"markdown","972337d6":"markdown","9a172e8b":"markdown","783b9585":"markdown","33ec2646":"markdown","a739c191":"markdown","f8a2771c":"markdown","c6a0ff94":"markdown","78ad0f51":"markdown","11a5ebda":"markdown","2e545389":"markdown","3ef8e27e":"markdown","b6f42d96":"markdown"},"source":{"4bf80c8d":"# Data Analysis and visualization tools\nimport numpy as np\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport pandas as pd\nimport seaborn as sns\nimport plotly as py\nimport plotly.graph_objs as go\n\n#statistics tools\nimport statsmodels.api as sm\nimport scipy.stats as st\nfrom scipy.stats import shapiro, mannwhitneyu, chi2_contingency\n\n#scikit learn framework\nfrom sklearn import metrics\nfrom sklearn.model_selection import train_test_split\nfrom sklearn import svm\nfrom sklearn.naive_bayes import GaussianNB\n\nimport warnings\nwarnings.filterwarnings('ignore')","19ea3371":"# Reading Database\ndata = pd.read_csv('..\/input\/online-shoppers-intention\/online_shoppers_intention.csv')\n\n# shape of the data(number of rows vs number of column)\ndata.shape","ad330652":"type(data)","52e54898":"# Displaying some rows of the data\ndata.head()","543017cd":"# Information of data\ndata.info()","b1e88d53":"# description of the data\ndata.describe()","03972718":"# Null data checking \ndata.isnull().sum()","750f6cf8":"# missing percentage of the data\nmissing_percentage = data.isnull().sum()\/data.shape[0]\nprint(missing_percentage)","d5f6fb72":"data['Revenue'].value_counts()","ca6090c9":"# checking the Distribution of customers on Revenue\n\nplt.rcParams['figure.figsize'] = (13, 5)\n\nplt.subplot(1, 2, 1)\nsns.countplot(data['Revenue'], palette = 'pastel')\nplt.title('Buy or Not', fontsize = 15)\nplt.xlabel('Revenue or not', fontsize = 15)\nplt.ylabel('count', fontsize = 15)\nplt.show()","ee0932cc":"data['Weekend'].value_counts()","b228c02b":"# checking the Distribution of customers on Weekend\n\nplt.rcParams['figure.figsize'] = (13,5)\nplt.subplot(1, 2, 2)\nsns.countplot(data['Weekend'], palette = 'inferno')\nplt.title('Puchase on Weekends', fontsize = 30)\nplt.xlabel('Weekend or not', fontsize = 15)\nplt.ylabel('count', fontsize = 15)\n\nplt.show()","ac86a0fe":"# checking the no. of Os's is having\ndata['OperatingSystems'].value_counts()","43a3c382":"# plotting a pie chart for Operating Systems\n\nplt.rcParams['figure.figsize'] = (18, 7)\nsize = [6601, 2585, 2555, 589]\ncolors = ['violet', 'magenta', 'pink', 'blue']\nlabels = \"2\", \"1\", \"3\", \"others\"\n\nplt.subplot(1, 2, 2)\nplt.pie(size, colors = colors, labels = labels, shadow = True, autopct = '%.2f%%', startangle=90)\nplt.title('Different Operating Systems', fontsize = 30)\nplt.axis('off')\nplt.legend()\nplt.show()\n\n\n","3ece25fa":"# checking the no. of Browser is having\ndata['Browser'].value_counts()","c82bfdf9":"# Ploting a pie chart for operating systems\nplt.rcParams['figure.figsize'] = (18, 7)\n\nsize = [7961, 2462, 736, 467,174, 163, 300]\ncolors = ['orange', 'yellow', 'pink', 'crimson', 'lightgreen', 'cyan', 'blue']\nlabels = \"2\", \"1\", \"4\", \"5\", \"6\", \"10\", \"others\"\n\nplt.subplot(1, 2, 2)\nplt.pie(size, colors = colors, labels = labels, shadow = True, autopct = '%.1f%%', startangle = 90)\nplt.title('Different Browsers', fontsize = 30)\nplt.axis('off')\nplt.legend()\nplt.show()","1a98b2d3":"data['Month'].value_counts()","2a05f758":"# creating a donut chart for the months variations\n# plotting a pie chart for share of special days\nsize = [3364, 2998, 1907, 1727, 549, 448, 433, 432, 288, 184]\ncolors = ['yellow', 'pink', 'lightblue', 'crimson', 'lightgreen', 'orange', 'cyan', 'magenta', 'violet', 'pink', 'lightblue', 'red']\nlabels = \"May\", \"November\", \"March\", \"December\", \"October\", \"September\", \"August\", \"July\", \"June\", \"February\"\nexplode = [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n\ncircle = plt.Circle((0, 0), 0.6, color = 'white')\n\nplt.rcParams['figure.figsize'] = (18, 7)\nplt.pie(size, colors = colors, labels = labels, explode = explode, shadow = True, autopct = '%.2f%%')\nplt.title('Month', fontsize = 30)\np = plt.gcf()\np.gca().add_artist(circle)\nplt.axis('off')\nplt.legend()\nplt.show()","43a5d9df":"data['VisitorType'].value_counts()","1735acd0":"# plotting a pie chart for Visitors\n\nplt.rcParams['figure.figsize'] = (18, 7)\nsize = [10551, 1694, 85]\ncolors = ['lightGreen', 'green', 'pink']\nlabels = \"Returning Visitor\", \"New Visitor\", \"Others\"\nexplode = [0, 0, 0.1]\nplt.subplot(1, 2, 1)\nplt.pie(size, colors = colors, labels = labels, explode = explode, shadow = True, autopct = '%.2f%%')\nplt.title('Different Visitors', fontsize = 30)\nplt.axis('off')\nplt.legend()\nplt.show()","dafaffb2":"data['TrafficType'].value_counts()","8a171238":"# visualizing the distribution of different traffic around the TrafficType\nplt.rcParams['figure.figsize'] = (18, 7)\n\nplt.subplot(1, 2, 1)\nplt.hist(data['TrafficType'], color = 'lightblue')\nplt.title('Distribution of different Traffic', fontsize = 30)\nplt.xlabel('TrafficType Codes', fontsize = 15)\nplt.ylabel('Count', fontsize = 15)\nplt.grid()\nplt.show()","0e5b7109":"data['Region'].value_counts()","21d02eb9":"# visualizing the distribution of the users around the Region\nplt.rcParams['figure.figsize'] = (18, 7)\n\nplt.subplot(1, 2, 1)\nplt.hist(data['Region'], color = 'lightgreen')\nplt.title('Distribution of users(Customers)', fontsize = 30)\nplt.xlabel('Region Codes', fontsize = 15)\nplt.ylabel('Count', fontsize = 15)\n\nplt.show()","03bfd689":"data['SpecialDay'].value_counts()","596ac667":"# visualizing the distribution of the users around the SpecialDay\nplt.rcParams['figure.figsize'] = (18, 7)\n\nplt.subplot(1, 2, 1)\nplt.hist(data['SpecialDay'], color = 'lightblue')\nplt.title('Distribution of users(Customers)', fontsize = 30)\nplt.xlabel('SpecialDay', fontsize = 15)\nplt.ylabel('Count', fontsize = 15)\n\nplt.show()","d7f97c3a":"# boxenplot for Administrative duration vs revenue\nplt.rcParams['figure.figsize'] = (8, 5)\n\nsns.boxenplot(data['Administrative_Duration'], data['Revenue'], palette = 'pastel', orient='h')\nplt.title('Admin. duration vs Revenue', fontsize = 30)\nplt.xlabel('Admin. duration', fontsize = 15)\nplt.ylabel('Revenue', fontsize = 15)\nplt.show()","d1ec20fe":"# boxenplot for Informational duration vs revenue\nplt.rcParams['figure.figsize'] = (8, 5)\n\nsns.boxenplot(data['Informational_Duration'], data['Revenue'], palette = 'rainbow', orient = 'h')\nplt.title('Info. duration vs Revenue', fontsize = 30)\nplt.xlabel('Info. duration', fontsize = 15)\nplt.ylabel('Revenue', fontsize = 15)\n\nplt.show()","95ebcef2":"# boxen plot product related duration vs revenue\nplt.rcParams['figure.figsize'] = (8, 5)\n\nsns.boxenplot(data['ProductRelated_Duration'], data['Revenue'], palette = 'inferno', orient = 'h')\nplt.title('Product Related Duration vs Revenue', fontsize = 30)\nplt.xlabel('Product Related Duration', fontsize = 15)\nplt.ylabel('Revenue', fontsize = 15)\nplt.show()","6fcd3a73":"# boxenplot for exit rates vs revenue\nplt.rcParams['figure.figsize'] = (8, 5)\n\nsns.boxenplot(data['ExitRates'], data['Revenue'], palette = 'dark', orient = 'h')\nplt.title('Exit Rates vs Revenue', fontsize = 30)\nplt.xlabel('Exit Rates', fontsize = 15)\nplt.ylabel('Revenue', fontsize = 15)\nplt.show()","7aeb3200":"# strip plot for page values vs revenue\nplt.rcParams['figure.figsize'] = (8, 5)\n\nsns.stripplot(data['PageValues'], data['Revenue'], palette = 'spring', orient = 'h')\nplt.title('Page Values vs Revenue', fontsize = 30)\nplt.xlabel('PageValues', fontsize = 15)\nplt.ylabel('Revenue', fontsize = 15)\nplt.show()\n","98c6ab02":"# strip plot for bounce rates vs revenue\nplt.rcParams['figure.figsize'] = (8, 5)\n\nsns.stripplot(data['BounceRates'], data['Revenue'], palette = 'autumn', orient = 'h')\nplt.title('Bounce Rates vs Revenue', fontsize = 30)\nplt.xlabel('Bounce Rates', fontsize = 15)\nplt.ylabel('Revenue', fontsize = 15)\nplt.show()\n","594de459":"# bar plot for weekend vs Revenue\ndf = pd.crosstab(data['Weekend'], data['Revenue'])\ndf.div(df.sum(1).astype(float), axis = 0).plot(kind = 'bar', stacked = True, figsize = (15, 5), color = ['orange', 'crimson'])\nplt.title('Weekend vs Revenue', fontsize = 30)\nplt.show()","3c56673a":"# bar plot for traffic type vs revenue\n\ndf = pd.crosstab(data['TrafficType'], data['Revenue'])\ndf.div(df.sum(1).astype(float), axis = 0).plot(kind = 'bar', stacked = True, figsize = (15, 5), color = ['lightblue', 'blue'])\nplt.title('Traffic Type as Revenue', fontsize = 30)\nplt.show()","63e3f2a2":"# bar plot for visitor type vs revenue\ndf = pd.crosstab(data['VisitorType'], data['Revenue'])\ndf.div(df.sum(1).astype(float), axis=0).plot(kind = 'bar', stacked = True, figsize =(15, 5), color = ['lightgreen', 'green'])\nplt.title('Visitor Type vs Revenue', fontsize = 30)\nplt.show()","fb0f9e0f":"# bar plot for region vs revenue\n\ndf = pd.crosstab(data['Region'], data['Revenue'])\ndf.div(df.sum(1).astype(float), axis=0).plot(kind = 'bar', stacked = True, figsize = (15, 5), color = ['pink', 'yellow'])\nplt.title('Region vs Revenue', fontsize = 30)\nplt.show()","26023c70":"# boxplot for month vs pagevalues w.r.t. revenue\nplt.rcParams['figure.figsize'] = (10, 7)\nsns.boxplot(x = data['Month'], y = data['PageValues'], hue = data['Revenue'], palette = 'spring')\nplt.title('month vs pagevalues w.r.t. revenue', fontsize = 25)\nplt.show()","292ed55d":"# boxplot for month vs exitrates w.r.t. revenue\nplt.rcParams['figure.figsize'] = (10, 7)\nsns.boxplot(x = data['Month'], y = data['ExitRates'], hue = data['Revenue'], palette = 'inferno')\nplt.title('month vs exitrates w.r.t. revenue', fontsize = 25)\nplt.show()","a622bd75":"# boxplot for month vs bounceRates w.r.t. Revenue\nplt.rcParams['figure.figsize'] = (10, 7)\n\nsns.boxplot(x = data['Month'], y = data['BounceRates'], hue = data['Revenue'], palette = 'autumn')\nplt.title(\"month vs bounceRates w.r.t. Revenue\", fontsize = 25)\nplt.show()","bf7710a3":"# boxplot for visitorType vs BounceRates w.r.t. revenue\nplt.rcParams['figure.figsize'] = (10, 7)\n\nsns.boxplot(x = data['VisitorType'], y = data['BounceRates'], hue = data['Revenue'], palette = 'pastel')\nplt.title('visitor type vs BounceRates w.r.t. revenue', fontsize = 25)\nplt.show()","db975bda":"# violin plot for visitor type vs exit rates w.r.t revenue\nplt.rcParams['figure.figsize'] = (10, 7)\n\nsns.violinplot(x = data['VisitorType'], y = data['PageValues'], hue = data['Revenue'], palette = 'Reds')\nplt.title('visitor type vs exit rates w.r.t. revenue', fontsize = 25)\nplt.show()","58d7c267":"# violin plot for visitor type vs exit rates wrt revenue\nplt.rcParams['figure.figsize'] = (10, 7)\n\nsns.violinplot(x = data['VisitorType'], y = data['ExitRates'], hue = data['Revenue'], palette = 'Purples')\nplt.title('visitor type vs exit rates w.r.t. revenue', fontsize = 25)\nplt.show()","3b9e0e75":"# violin plot for region vs pagevalues w.r.t. revenue\nplt.rcParams['figure.figsize'] = (10, 7)\n\nsns.violinplot(x = data ['Region'], y = data['PageValues'], hue = data['Revenue'])\nplt.title('region vs pagevalues w.r.t. revenue', fontsize = 25)\nplt.show()\n","64029d63":"# violin plot for rigion vs exit rates w.r.t revenue\nplt.rcParams['figure.figsize'] = (10, 7)\nsns.violinplot(x = data['Region'], y = data['ExitRates'], hue = data['Revenue'], palette = 'Greens')\nplt.title(\"rigion vs exit rates w.r.t revenue\", fontsize = 25)\nplt.show()","c1e8867e":"multivariate_feature_analysis = [\n    ['month vs pagevalues', 'Revenue', 'Gaussian', 'High', 'Low', 'Low', 'High'],\n    ['month vs exitrates' , 'Revenue', 'Gaussian', 'Low', 'High', 'Medium', 'Medium'],\n    ['month vs bounceRates' , 'Revenue', 'Gaussian', 'Low', 'High', 'Medium', 'High'],\n    ['visitor type vs BounceRates' , 'Revenue', 'Exponential', 'Low', 'High', 'Low', 'High'],\n    ['visitor type vs exit rates' , 'Revenue', 'Exponential', 'Low', 'High', 'High', 'Medium'],\n    ['visitor type vs exit rates', 'Revenue', 'Exponential', 'High', 'Low', 'High', 'Medium'],\n    ['region vs pagevalues', 'Revenue', 'Exponential', 'Low', 'High', 'High', 'High'],\n    ['rigion vs exit rates', 'Revenue', 'Gaussian', 'High', 'High', 'High', 'Medium'] \n]\nfeature_summary = pd.DataFrame(multivariate_feature_analysis, columns=['Multivariate_features', 'W.R.T', 'Distribution', 'Revenue_True', 'Revenue_False', 'Outliers', 'Importance'])\nfeature_summary","3572f01e":"cat_cols=['Administrative','Informational','ProductRelated','Month','OperatingSystems', 'Browser', 'Region', 'TrafficType', 'VisitorType',\n       'Weekend', 'SpecialDay']","835f1139":"# check wheather Revenue is influenced by categorical column\n# Null Hypopthesis, H0 = proportion of revenue accross the category is same\n# Alternative Hypothesis, H1 = proportion of revenue at least in two category is different\nscol = []\nspval = []\nss = []\nfor n in cat_cols:\n    scol.append(n)\n    cp = chi2_contingency(pd.crosstab(data[n], data['Revenue']))[1]\n    spval.append(round(cp, 4))\n    if cp < 0.05:\n        # rejects Null Hypothesis\n        ss.append('*') # significant\n    else:\n        # Accept Null Hypothesis\n        ss.append('**') # not significant\n        ","a2f3d88f":"pd.DataFrame({'Feature': scol, 'P-Value': spval, 'Significance': ss})","b1b58cc8":"numerical_columns=['Administrative_Duration','Informational_Duration','ProductRelated_Duration','BounceRates', 'ExitRates', 'PageValues']","273d531f":"from scipy.stats import levene\n\n# Two-Sample T-test\ntcol = []\ntpval = []\nts = []\nfor n in numerical_columns:\n    tcol.append(n)\n    # splitting into 2 groups(Revenue = True, Revenue = False)\n    g1 = data[n][data['Revenue'] == False]\n    g2 = data[n][data['Revenue'] == True]\n    # Test for normality(Shapiro Test)\n    # H0: Data is normal\n    # H1: Data is not normal\n    # if p < 0.05 --- reject Null Hypothesis\n    for b in [g1]:\n        s, p = shapiro(b)\n    for c in [g2]:\n        s1, p1 = shapiro(c)\n    if p > 0.05 or p1 > 0.05:\n        w, lp = levene(g1, g2)\n    # If doesn't pass normality or variance test, we do non-parametric Test(mannwhitneyu Test)\n    if p <= 0.05 or p1 <= 0.05 or lp <= 0.05:\n        ms, mp = mannwhitneyu(g1, g2)\n        tpval.append(round(mp, 4))\n    if mp < 0.05:\n        ts.append('*')  # significant\n    else:\n        ts.append('**') # not significant\n        ","c8c1a4b7":"pd.DataFrame({'Feature': tcol, 'P-Value': tpval, 'Significance': ts})","11633a7f":"plt.figure(figsize=(62, 20))\ndata.boxplot();","50d81b2b":"# identify outliers with standard deviation\nfrom numpy.random import seed\nfrom numpy.random import randn\nfrom numpy import mean\nfrom numpy import std\nout_per=[]\nfor i in numerical_columns:\n    data_mean, data_std = mean(data[i]), std(data[i])\n    \n    # identify outliers\n    cut_off = data_std * 3\n    lower, upper = data_mean - cut_off, data_mean + cut_off\n    print(i, ': \\n')\n\n    # identify outliers\n    outliers = [x for x in data[i] if x < lower or x > upper]\n    \n    num_out = len(outliers)\n    print('Identified outliers: %d' %num_out)\n    outliers_removed = [x for x in data[i] if x >= lower and x <= upper]\n    num_nout = len(outliers_removed)\n    print('Non-outlier observations: %d' %num_nout)\n    outlier_percent = (num_out \/ (num_out + num_nout)) * 100\n    print('Percent of outliers:', outlier_percent, '\\n')\n    out_per.append(outlier_percent)","1affba91":"Outliers = pd.DataFrame({'Feature': numerical_columns, '% Of Outliers': out_per})\noutlier_sorted = Outliers.sort_values('% Of Outliers', ascending = False)\noutlier_sorted","d69c3987":"plt.rcParams['figure.figsize'] = (8, 5)\nsns.barplot(y = outlier_sorted['Feature'], x = outlier_sorted['% Of Outliers'], palette = 'GnBu_d')\nplt.title('Percent fo Outliers by columns')\nplt.ylabel('Column Name')\nplt.show()","42ecc4c7":"# Imputing Missing Values with 0\ndata.fillna(0, inplace = True)\n\n#checking the no. of null values after imputing\ndata.isnull().sum().sum()","ec003dfd":"# Time spent by the Users on website vs Bounce Rates\n# let's cluster Administrative duration and bounce Rates to different types of clusters in the dataset.\n# preparing the dataset\nx = data.iloc[:, [1, 6]].values\n\n# checking the shape of the dataset\nprint(\"Shape of the dataset: \", x.shape)\n\nfrom sklearn.cluster import KMeans\n\nwcss = []\nfor i in range(1, 11):\n    km = KMeans(n_clusters = i,\n               init = 'k-means++',\n               max_iter = 200,\n               n_init = 10, \n               random_state = 0,\n               algorithm = 'elkan',\n               tol = 0.001)\n    km.fit(x)\n    labels = km.labels_\n    wcss.append(km.inertia_)\n    \nplt.rcParams['figure.figsize'] = (15, 7)\nplt.plot(range(1, 11), wcss)\nplt.grid()\nplt.tight_layout()\nplt.xlabel('No. of Clusters')\nplt.ylabel('within-cluster sum of square')\nplt.show()","f6f04749":"km = KMeans(n_clusters = 3, init = 'k-means++', max_iter = 200, n_init = 10, random_state = 0)\ny_means = km.fit_predict(x)\n\nplt.scatter(x[y_means == 0, 0], x[y_means == 0, 1], s = 100, c = 'orange', label = 'Un-interested Customers')\nplt.scatter(x[y_means == 1, 0], x[y_means == 1, 1], s = 100, c = 'red', label = 'General Customers')\nplt.scatter(x[y_means == 2, 0], x[y_means == 2, 1], s = 100, c = 'cyan', label = 'Target Customers')\nplt.scatter(km.cluster_centers_[:, 0], km.cluster_centers_[:, 1], s = 50, c = 'blue', label = 'centeroid')\n\nplt.title('Administrative Duration vs BounceRates', fontsize = 20)\nplt.grid()\nplt.xlabel('Administrative Duration')\nplt.ylabel('Bounce Rates')\nplt.legend()\nplt.show()","c0cae2ff":"# cluster anaysis of Informational Duration vs Bounce Rates\nx = data.iloc[:, [3, 6]].values\n\nwcss = []\nfor i in range(1, 11):\n    km = KMeans(n_clusters = i,\n                init = 'k-means++',\n                max_iter = 200,\n                n_init = 10,\n                random_state = 0,\n                algorithm = 'elkan',\n                tol = 0.001)\n    km.fit(x)\n    labels = km.labels_\n    wcss.append(km.inertia_)\n    \nplt.rcParams['figure.figsize'] = (15, 7)\nplt.plot(range(1, 11), wcss)\nplt.grid()\nplt.tight_layout()\nplt.title('The Elbow Method', fontsize = 20)\nplt.xlabel('No. of Clusters')\nplt.ylabel('within-cluster sum of square')\nplt.show()","6897dd12":"km = KMeans(n_clusters = 2, init = 'k-means++', max_iter = 200, n_init = 10, random_state = 0)\ny_means = km.fit_predict(x)\n\nplt.scatter(x[y_means == 0, 0], x[y_means == 0, 1], s = 100, c = 'pink', label = 'Un-interested Customers')\nplt.scatter(x[y_means == 1, 0], x[y_means == 1, 1], s = 100, c = 'cyan', label = 'Target Customers')\nplt.scatter(km.cluster_centers_[:,0], km.cluster_centers_[:, 1], s = 50, c = 'blue' , label = 'centeroid')\n\nplt.title('Informational Duration vs Bounce Rates', fontsize = 20)\nplt.grid()\nplt.xlabel('Informational Duration')\nplt.ylabel('Bounce Rates')\nplt.legend()\nplt.show()","3be508ee":"# informational duration vs Bounce Rates\nx = data.iloc[:, [1, 7]].values\n\nwcss = []\nfor i in range(1, 11):\n    km = KMeans(n_clusters = i,\n               init = 'k-means++',\n               max_iter = 200,\n               n_init = 10, \n               random_state = 0,\n               algorithm = 'elkan',\n               tol = 0.001)\n    km.fit(x)\n    labels = km.labels_\n    wcss.append(km.inertia_)\n    \nplt.rcParams['figure.figsize'] = (15, 7)\nplt.plot(range(1, 11), wcss)\nplt.grid()\nplt.tight_layout()\nplt.xlabel('No. of Clusters')\nplt.ylabel('wcss')\nplt.show()","f9e262cc":"km = KMeans(n_clusters = 3, init = 'k-means++', max_iter = 200, n_init = 10, random_state = 0)\ny_means = km.fit_predict(x)\n\nplt.scatter(x[y_means == 0, 0], x[y_means == 0, 1], s = 100, c = 'magenta', label = 'Un-interested Customers')\nplt.scatter(x[y_means == 1, 0], x[y_means == 1, 1], s = 100, c = 'cyan', label = 'General Customers')\nplt.scatter(x[y_means == 2, 0], x[y_means == 2, 1], s = 100, c = 'pink', label = 'Target Customers')\nplt.scatter(km.cluster_centers_[:,0], km.cluster_centers_[:, 1], s = 50, c = 'blue', label = 'centroid')\n\nplt.title('Administrative Clustering vs Exit Rates', fontsize = 20)\nplt.grid()\nplt.xlabel('Administrative Duration')\nplt.ylabel('Exit Rates')\nplt.legend()\nplt.show()","44456148":"# Region vs TrafficType clustering\nx = data.iloc[:, [13, 14]].values\n\nwcss = []\nfor i in range(1, 11):\n    km = KMeans(n_clusters = i,\n                init = 'k-means++',\n                max_iter = 200,\n                n_init = 10,\n                random_state = 0,\n                algorithm = 'elkan',\n                tol = 0.001)\n    km.fit(x)\n    labels = km.labels_\n    wcss.append(km.inertia_)\n    \nplt.rcParams['figure.figsize'] = (15, 7)\nplt.plot(range(1, 11), wcss)\nplt.grid()\nplt.tight_layout()\nplt.title('The Elbow Method', fontsize = 20)\nplt.xlabel('No. of Cluster')\nplt.ylabel('within cluster sum of the square')\nplt.show()","f116cc77":"km = KMeans(n_clusters = 2, init = 'k-means++', max_iter = 200, n_init = 10, random_state = 0)\ny_means = km.fit_predict(x)\n\nplt.scatter(x[y_means == 0, 0], x[y_means == 0, 1], s = 100, c = 'pink', label = 'Un-iterested customers')\nplt.scatter(x[y_means == 1, 0], x[y_means == 1, 1], s = 100, c = 'lightgreen', label = 'Target Customers')\nplt.scatter(km.cluster_centers_[:, 0], km.cluster_centers_[:, 1], s = 50, c = 'blue', label = 'centroid')\n\nplt.title('Region vs Traffic Type', fontsize = 20)\nplt.xlabel('Region')\nplt.ylabel('Traffic')\nplt.legend()\nplt.grid()\nplt.show()","688fd89c":"# administrative duration vs bounce rates\nx = data.iloc[:, [1, 13]].values\n\nwcss = []\nfor i in range(1, 11):\n    km = KMeans(n_clusters = i,\n                init = 'k-means++',\n                max_iter = 300,\n                n_init = 10,\n                random_state = 0,\n                algorithm = 'elkan',\n                tol = 0.001)\n    km.fit(x)\n    labels = km.labels_\n    wcss.append(km.inertia_)\n    \nplt.rcParams['figure.figsize'] = (15, 7)\nplt.plot(range(1, 11), wcss)\nplt.grid()\nplt.tight_layout()\nplt.title('The Elbow Method', fontsize = 20)\nplt.xlabel('No. of Clusters')\nplt.ylabel('Within cluster sum of the square')\nplt.show()","1487c584":"km = KMeans(n_clusters = 2, init = 'k-means++', max_iter = 300, n_init = 10, random_state = 0)\ny_means = km.fit_predict(x)\n\nplt.scatter(x[y_means == 0, 0], x[y_means == 0, 1], s = 100, c = 'cyan', label = 'Unproductive Customers')\nplt.scatter(x[y_means == 1, 0], x[y_means == 1, 1], s = 100, c = 'magenta', label = 'Target Customers')\nplt.scatter(km.cluster_centers_[:,0], km.cluster_centers_[:, 1], s = 50, c = 'blue' , label = 'centeroid')\n\nplt.title('Adminstrative Duration vs Region', fontsize = 20)\nplt.grid()\nplt.xlabel('Administrative Duration')\nplt.ylabel('Region Type')\nplt.legend()\nplt.show()","ecf50664":"data.info()","e9ab71a3":"# one hot encoding\ndf1 = pd.get_dummies(data)\ndf1.head()","a31fab2d":"df1.info()","b8f84fb3":"# Label encoding of revenue\nfrom sklearn.preprocessing import LabelEncoder\nle = LabelEncoder()\ndf1['Revenue'] = le.fit_transform(df1['Revenue'])\ndf1['Revenue'].value_counts()","0474b2c3":"df1['Weekend'].value_counts()","b90f0543":"# Label encoding of weekend\n\ndf1['Weekend'] = le.fit_transform(df1['Weekend'])\ndf1['Weekend'].value_counts()","bc0eb476":"# Splitting dependent and independent variables(columns)\ny = df1['Revenue']\nx = df1.drop(['Revenue'], axis = 1)\n\n# checking the shapes\nprint(\"Shape of x: \", x.shape)\nprint(\"Shape of y: \", y.shape)","d47352fa":"# Splitting of the Data\n\nx_train, x_test, y_train, y_test = train_test_split(x, y, test_size =  0.2, random_state = 0)\n\n# checking the shapes\n\nprint(\"Shape of x_train :\", x_train.shape)\nprint(\"Shape of y_train :\", y_train.shape)\nprint(\"Shape of x_test :\", x_test.shape)\nprint(\"Shape of y_test :\", y_test.shape)","3fa917cf":"# model define and training\nmodel = svm.SVC()\nmodel.fit(x_train, y_train)\n\ny_pred = model.predict(x_test)\n\n# evaluating the model\nprint(\"Training Accuracy: \", model.score(x_train, y_train))\nprint(\"Testing Accuracy: \", model.score(x_test, y_test))","5e36f8b6":"# confusion matrix\ncm = metrics.confusion_matrix(y_test, y_pred)\nplt.rcParams['figure.figsize'] = (6, 6)\nsns.heatmap(cm, annot = True)\nplt.show()","bfc7a171":"# classification report\ncr = metrics.classification_report(y_test, y_pred)\nprint(cr)","f456621a":"# model define and training\nmodel = GaussianNB()\nmodel.fit(x_train, y_train)\n\ny_pred = model.predict(x_test)\n\n# evaluating the model\nprint(\"Training Accuracy: \", model.score(x_train, y_train))\nprint(\"Testing Accuracy: \", model.score(x_test, y_test))","fe3c2825":"# confusion matrix\ncm = metrics.confusion_matrix(y_test, y_pred)\nplt.rcParams['figure.figsize'] = (6, 6)\nsns.heatmap(cm, annot = True)\nplt.show()","4b173b5a":"# classification report\ncr = metrics.classification_report(y_test, y_pred)\nprint(cr)","bc68b86c":"precision = ['Naive Bayes', 'precision', 0.91, 0.42]\nrecall = ['Naive Bayes', 'recall', 0.83, 0.61]\nf1_score = ['Naive Bayes', 'f1_score', 0.87, 0.50 ]\nprecision2 = ['Support Vector Machine', 'precision', 0.83, 0.00 ]\nrecall2 = ['Support Vector Machine', 'recall', 1.0, 0.00 ]\nf1_score2 = ['Support Vector Machine', 'f1_score', 0.91, 0.00 ]\ntable = pd.DataFrame([precision, precision2, recall, recall2, f1_score, f1_score2])\ntable.columns = ['model_name', 'metrics', 'Is_Revenue(False)', 'Is_Revenue(True)']\ntable","296e247c":"table.hist()","1d7edab6":"# Clustering Analysis\n> **Trying to learn the user characteristics of in terms of time spent on the Website**\n- Administrative Duration vs Bounce Rate\n- Informative Duration vs Bounce Rates\n- Administrative Duration vs Exit Rates\n\n> **Where from the Users of the Website come?**\n- Region vs Traffic Type\n- Adminstrative Duration vs Region","fb01451d":"#### Shapiro-Wilk test\nThe null hypothesis for the Shapiro-Wilk test is that `a variable is normally distributed in some population.`\nA different way to say the same is that a variable\u2019s values are a simple random sample from a normal distribution. As a rule of thumb, we\nreject the null hypothesis if `p < 0.05`.","2622c7cc":"## VisitorType vs BounceRates w.r.t. revenue","dc1bd404":"**What is the observation Point here?**\n- We see here `ExitRates` is normally(gaussian) distributed for both purchased(`True`) or not puchased(`False`). \n- We also see there are so many outliers in not puchased(`False`) according to `ExitRates`.","77960640":"## Informational Duration vs Bounce Rates","4c02a210":"## Product Related Duration vs Revenue","2489336f":"## Weekend","d73b8663":"**What is the observation point here?**\n- We see at this clustering plot, we can say that the customers who is from Region 2, 4, 5 have less traffic than others.","d55ee814":"**What is the observation point here?**\n- According to the above plot, the maximum bend at index 3, that is number of optimal no. of Clusters for Administrative Duration and Exitrates is 2. Let's go the next step, i.e., Plotting the Clusters.\n","387d1eeb":"### month vs pagevalues w.r.t. revenue","ff588ecb":"## Administrative Duration vs Exit Rates","00ef4369":"## Bounce Rates vs Revenue","c6037454":"**What is the observation Point here?**\n- We see here `Informational_Duration` is exponentially distributed for both purchased(`True`) or not puchased(`False`). \n- We also see there are so many outliers in not puchased(`False`) according to `Informational_Duration`.","ae3811a1":"### Numerical Column vs Target","2f830b83":"## Visitor Type","119c8816":"The model which is just build by Naive Bayes(Gaussian) gives us training accuracy 79.83% and testing accuracy 79.03%","aef11046":"## visitor type vs exit rates w.r.t. revenue","277e626c":"## Column Descriptions:\n**Administrative:** This is the number of pages of this type (administrative) that the user visited.\n\n**Administrative_Duration:** This is the amount of time spent in this category of pages.\n\n**Informational:** This is the number of pages of this type (informational) that the user visited.\n\n**Informational_Duration:** This is the amount of time spent in this category of pages.\n\n**ProductRelated:** This is the number of pages of this type (product related) that the user visited.\n\n**ProductRelated_Duration:** This is the amount of time spent in this category of pages.\n\n**BounceRates:** The percentage of visitors who enter the website through that page and exit without triggering any additional tasks.\n\n**ExitRates:** The percentage of pageviews on the website that end at that specific page.\n\n**PageValues:** The average value of the page averaged over the value of the target page and\/or the completion of an eCommerce transaction. <br>\n[More information about how this is calculated](https:\/\/support.google.com\/analytics\/answer\/2695658?hl=en)\n\n**SpecialDay:** This value represents the closeness of the browsing date to special days or holidays (eg Mother's Day or Valentine's day) in which the transaction is more likely to be finalized. More information about how this value is calculated below.\n\n**Month:** Contains the month the pageview occurred, in string form.\n\n**OperatingSystems:** An integer value representing the operating system that the user was on when viewing the page.\n\n**Browser:** An integer value representing the browser that the user was using to view the page.\n\n**Region:** An integer value representing which region the user is located in.\n\n**TrafficType:** An integer value representing what type of traffic the user is categorized into. <br>\n[Read more about traffic types here.](https:\/\/www.practicalecommerce.com\/Understanding-Traffic-Sources-in-Google-Analytics)\n\n**VisitorType:** A string representing whether a visitor is New Visitor, Returning Visitor, or Other.\n\n**Weekend:** A boolean representing whether the session is on a weekend.\n\n**Revenue:** A boolean representing whether or not the user completed the purchase.\n","b94e04e3":"**What is observation point here?**\n- Top 3 Operating Systems are covered 95% of this dataset. So we should focus on them to increase our business.","8ecf8153":"## Visitor type vs revenue","b6ff8933":"**What is the observation Point here?**\n- We see here `Administrative_Duration` is exponentially distributed for both purchased(`True`) or not puchased(`False`). \n- We also see there are so many outliers in not puchased(`False`) according to `Administrative_Duration`.","f087ab01":"**What is observation point here?**\n- More than 85% visitors are returning vistors, This is huge. This information may helpful for marketing.","5c06ee73":"## Operating Systems","da4b44b2":"**What is the observation point here?**\n- According to the above plot, the maximum bend at index 2, that is number of optimal no. of Clusters for Administrative Duration and Region is 2. Let's go the next step, i.e., Plotting the Clusters.\n","6f5f0959":"# Modelling: Naive Bayes","0a5de0bf":"**What we observered here?**\n- From the above information we see that the distribution of `Revenue` and `Weekend` data are hightly imbalanced.","30166f6e":"**What is the observation Point here?**\n- We see here `Visitor Type` is also a categorical column. \n- In this visualization, every category is different than others. New_Visitors are highly influenced to buy a product.","611252f7":"## Informational duration vs Revenue","861fb2b2":"### month vs bounceRates w.r.t. Revenue","7cfb814f":"# Modelling : Support Vector Machine","61a9b348":"**What is the observation Point here?**\n- We see here `VisitorType` vs `PageValues` are exponentially distributed when users purchased a product or not.\n- There are lots of outlier also here.","f0e7afb9":"## Outliers","a5bc0de6":"**What is the observation Point here?**\n- We see here `PageValues` is exponentially distributed for both purchased(`True`) or not puchased(`False`). \n- We also see there are so many outliers in puchased(`True`) according to `ExitRates`.\n- Most important things is here `PageValues` are highly influenced to purchased(`True`) a product.","67f88897":"### Visualizing the Cluster using scatter plot","d2415342":"## Exit Rates vs Revenue","cd01e3a4":"### categorical column vs target column","9a7aae71":"**What is the observation Point here?**\n- We see here `VisitorType` vs `PageValues` are normally(Gaussian) distributed when users purchased a product or not.\n- There are lots of outlier also here.","00a27460":"**What is the observation Point here?**\n- We see here `Month` vs `BounceRates` are normally(Gaussian) distributed when users purchased a product for some `Month` but some them are exponentially distributed.\n- There are lots of outlier also here.","b298af35":"### Summary table of multivariate Feature Analysis","ceccaed0":"**What is the observation point here?**\n- According to the above plot, the maximum bend at index 3, that is number of optimal no. of Clusters for Administrative Duration and Revenue is 3. Let's go the next step, i.e., Plotting the Clusters.\n","c8b587bb":"### Necessary Imports","e5a74032":"## Administrative Duration vs Bounce Rates","18e37d43":"**What is the observation point here?**\n- Different type of users with respect to region are not normal(Gaussian) distributed.This Regional data is exponentially distributed. So we need to take care of this type distribution.\n- There are 9 different Region Codes here.","5909b15c":"A chi-squared test, also written as \u03c7\u00b2 test, is a statistical hypothesis test that is valid to perform when the test statistic is chi-squared distributed under the null hypothesis, specifically Pearson's chi-squared test and variants thereof. \n\n#### chi2_contingency\nChi-square test of independence of variables in a contingency table.\n\nThis function computes the chi-square statistic and p-value for the hypothesis test of independence of the observed frequencies in the contingency table observed. The expected frequencies are computed based on the marginal sums under the assumption of independence; see `scipy.stats.contingency.expected_freq`. The number of degrees of freedom is (expressed using numpy functions and attributes):\n\ndof = observed.size - sum(observed.shape) + observed.ndim - 1","735106a9":"## Page Values vs Revenue","3374eb3e":"**What is the observation point here?**\n- We see at this clustering plot, we can confidently say that the customers who spent a longer Administrative duration in a website are very less likely comers from 2, 4 Region Type.","fcf514f5":"## Bi-Variate Analysis with Visualization\n- Administrative duration vs revenue\n- Informational duration vs revenue\n- product related duration vs revenue\n- exit rate vs revenue\n- page values vs revenue\n- bounce rates vs revenue\n- weekend vs Revenue\n- Traffic Type vs Revenue\n- visitor type vs revenue\n- region vs Revenue","d07979ff":"**What is the observation point here?**\n- We see at this clustering plot, we can confidently say that the customers who spent a longer Administrative duration in a website are very less likely to Exit from the website that is nevigating away from the website.\n- There are Three groups, The Magenta group is a group of customers who stay for shortest Administrative duration and have highest chance for Navigating away from a website.\n","24fd13f3":"## Summary Table based on Two Learning Algorithm","bceeca1f":"**What is the observation Point here?**\n- We see here `Month` vs `ExitRates` are normally(Gaussian) distributed for both when users purchased a product or not.\n- There are lots of outlier also here.","aa89d6ea":"**What is the observation point here?**\n- According to the above plot, the maximum bend at index 2, that is number of optimal no. of Clusters for Informational Duration and Revenue is 2. Let's go the next step, i.e., Plotting the Clusters.\n","5cb216dc":"### Univariate Analysis with Visualization\n- Revenue\n- Weekend\n- Operating System\n- Browser\n- Month\n- VistorType\n- TrafficType\n- Region","9a083208":"**What is the observation point here?**\n- According to the above plot, the maximum bend at index 2, that is number of optimal no. of Clusters for Region and Traffic Type is 2. Let's go the next step, i.e., Plotting the Clusters.\n","57741209":"## Region vs Traffic Type","03c2e260":"## Browsers","1971ad4a":"## rigion vs exit rates w.r.t revenue","95916645":"**What is the observation Point here?**\n- We see here `Weekend` is also a boolean column. \n- There is nothing significant to describe here.","402985a1":"## Where from the Users of the Website come?","afa9780e":"## Month","c5148d33":"## Weekend vs Revenue","b7f8f16e":"## visitor type vs exit rates w.r.t. revenue","946c2f2a":"Above two models shows different type of accuracy. Though support vector machine gave us more accuracy I would choose naive bayes algorithm. Because, we see in the heatmap of confusion matrix that provide us important information which is support vector machine doesn't recognize any '`false-negative`'. It means support vector machine doesn't give us a any good solution. ","d3085252":"**What is the observation Point here?**\n- We see here `BounceRates` is exponentially distributed for both purchased(`True`) or not puchased(`False`). \n- We also see there are so many outliers in not puchased(`False`) according to `ExitRates`.\n- `BounceRates` is highly influenced to buy a product or not.","b6546a68":"**What is the observation Point here?**\n- We see here `VisitorType` vs `PageValues` are exponentially distributed when users purchased a product or not.\n- There are lots of outlier for Returning_Visitors.","5ae7b675":"## Region vs Revenue","11fbc223":"## Adminstrative Duration vs Region","ed84e3a3":"### Traffic Type","c428731c":"**Attribute Information:**\n\nThe dataset consists of 10 numerical and 8 categorical attributes. The 'Revenue' attribute can be used as the class label. Of the 12,330 sessions in the dataset, 84.5% (10,422) were negative class samples that did not end with shopping, and the rest (1908) were positive class samples ending with shopping.\n\n\"Administrative\", \"Administrative Duration\", \"Informational\", \"Informational Duration\", \"Product Related\" and \"Product Related Duration\" represent the number of different types of pages visited by the visitor in that session and total time spent in each of these page categories.\n\nThe values of these features are derived from the URL information of the pages visited by the user and updated in real time when a user takes an action, e.g. moving from one page to another.\n\nThe \"Bounce Rate\", \"Exit Rate\" and \"Page Value\" features represent the metrics measured by \"Google Analytics\" for each page in the e-commerce site. The value of \"Bounce Rate\" feature for a web page refers to the percentage of visitors who enter the site from that page and then leave (\"bounce\") without triggering any other requests to the analytics server during that session.\n\nThe value of \"Exit Rate\" feature for a specific web page is calculated as for all pageviews to the page, the percentage that were the last in the session. The \"Page Value\" feature represents the average value for a web page that a user visited before completing an e-commerce transaction.\n\nThe \"Special Day\" feature indicates the closeness of the site visiting time to a specific special day (e.g. Mother\u2019s Day, Valentine's Day) in which the sessions are more likely to be finalized with transaction.\n\nThe value of this attribute is determined by considering the dynamics of e-commerce such as the duration between the order date and delivery date. For example, for Valentina\u2019s day, this value takes a nonzero value between February 2 and February 12, zero before and after this date unless it is close to another special day, and its maximum value of 1 on February 8.\n\nThe dataset also includes operating system, browser, region, traffic type, visitor type as returning or new visitor, a Boolean value indicating whether the date of the visit is weekend, and month of the year.","401ec8bb":"The model which is just build by Support vector machine gives us training accuracy 99.5% and testing accuracy 82.85%","34628d9f":"**What is the observation Point here?**\n- We see here `ProductRelatedDuration` is exponentially distributed for both purchased(`True`) or not puchased(`False`). \n- We also see there are so many outliers in not puchased(`False`) according to `ProductRelatedDuration`.","68fe5811":"**What is the observation Point here?**\n- We see here `Traffic Type` is a categorical column. \n- In this visualization, every category is different than others. Some of them are highly influenced to buy a product such as (2, 7, 16, 20, etc).","ea16a8f8":"**What is the observation point here?**\n- Different type of Traffic are not normal(Gaussian) distributed. This data is exponentially distributed. So we need to take care of this type distribution.\n- There are 20 different Traffic Type Codes here.","6ac258d4":"**What is the observation Point here?**\n- We see here `VisitorType` vs `BounceRates` are normally(Gaussian) distributed when Returning_users purchased a product but New_users and others are exponentially distributed.\n- There are lots of outlier also here.","bde2a679":"**What is the observation point here?**\n- 90% users used only top 3 browser. ","9761549e":"### Why I used this dataset?\n\nThe data used in this analysis is an Online Shoppers Purchasing Intention data set provided on the UC Irvine\u2019s Machine Learning Repository. The data set was formed so that each session would belong to a different user in a 1-year period to avoid any tendency to a specific campaign, special day, user profile, or period.The primary purpose of the data set is to predict the purchasing intentions of a visitor to this particular store\u2019s website. This dataset has very few missing values and all features of the dataset are relevant to the purchasing intention based on inference. That's why I choose this dataset.","ffc1adce":"## Multi-variate Analysis\n- month vs pagevalues w.r.t. revenue\n- month vs exitrates w.r.t. revenue\n- month vs bounceRates w.r.t. Revenue\n- visitor type vs BounceRates w.r.t. revenue\n- visitor type vs exit rates w.r.t. revenue\n- visitor type vs exit rates w.r.t. revenue\n- region vs pagevalues w.r.t. revenue\n- rigion vs exit rates w.r.t. revenue","7a87cc53":"**What is the observation point here?**\n- We see at this clustering plot, we can confidently say that the customers who spent a longer administrative duration in a website are very less likely to bounce from the website that is nevigating away from the website just after navigating one page of that website.\n- There are Three groups, The Pink group is a group of customers who stay for shortest administrative duration and have highest chance for Navigating away from a website.\n","8047defa":"# Visualization of Outliers","b5286cc8":"# Data Preprocessing\n- One Hot and Label Encoding","067e8c75":"#### levene (scipy.stats)\nPerform Levene test for equal variances.\n\nThe Levene test tests the null hypothesis that all input samples are from populations with equal variances. Levene\u2019s test is an alternative to Bartlett\u2019s test bartlett in the case where there are significant deviations from normality.\n\n### Levene's test\nIn statistics, Levene's test is an inferential statistic used to assess the equality of variances for a variable calculated for two or more groups. Some common statistical procedures assume that variances of the populations from which different samples are drawn are equal. Levene's test assesses this assumption. ","bb189a33":"## Traffic Type vs Revenue","972337d6":"### month vs exitrates w.r.t. revenue","9a172e8b":"**What is the observation Point here?**\n- We see here `Region` is also a categorical column. \n- In this visualization, every category is almost similar to others.","783b9585":"## region vs pagevalues w.r.t. revenue","33ec2646":"**What is the observation point here?**\n- We see at this clustering plot, we can confidently say that the customers who spent a longer Informational duration in a website are very less likely to bounce from the website that is nevigating away from the website just after navigating one page of that website.\n- There are Two groups, The Pink group is a group of customers who stay for shortest Informational duration and have highest chance for Navigating away from a website.\n","a739c191":"## Statistical Tests\n- categorical column vs target column\n- Numerical column vs target column","f8a2771c":"**What is the observation Point here?**\n- We see here `Month` vs `PageValues` are normally(Gaussian) distributed when users purchased a product.\n- There are lots of outlier also here.","c6a0ff94":"**The Elbow Method to find out the maximum no. of Optimal Clusters**\n- Compute clustering algorithm(e.g., K-Means Clustering) for different values of k. For instance, by varying k from 1 to 10 clusters.\n- For each k, calculate the total within-cluster sum of square(WCSS).\n- plot the curve of WCSS according to the number of clusters k.\n- The location of a bend(Knee) in the plot is generally considered as an indicator of the appropriate number of clusters.\n","78ad0f51":"**What is the observation Point here?**\n- We see here `Region` vs `PageValues` are exponentially distributed when users purchased a product or not.\n- There are lots of outlier also here.","11a5ebda":"## Revenue","2e545389":"#### MannWhitneyu\nCompute the Mann-Whitney rank test on samples x and y.\n### Mann-Whitney rank test\nIn statistics, the Mann\u2013Whitney U test is a nonparametric test of the null hypothesis that, for randomly selected values X and Y from two populations, the probability of X being greater than Y is equal to the probability of Y being greater than X.","3ef8e27e":"### Administrative duration vs Revenue","b6f42d96":"## Region"}}