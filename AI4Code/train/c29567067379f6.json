{"cell_type":{"c180c21a":"code","04aeeea5":"code","7717dbc5":"code","d8d165c6":"code","15fdbd73":"code","c3587b94":"code","857fb992":"code","ac38bcbc":"code","a3dd6b23":"code","377fc975":"code","f123394e":"code","6db68c07":"code","fde8aefc":"code","3a52e414":"code","12495c5d":"code","a8f30ac1":"code","93bd23b6":"code","c9a3d8fc":"code","a093f5dd":"code","81f3a294":"code","1ebb5e17":"code","aada0e95":"code","3f359d8e":"code","05e92395":"code","1fd0d2a6":"code","d249955c":"code","393f6f3f":"code","350cd2b1":"code","c89264ab":"code","635e4c79":"code","9709db96":"code","6c623bba":"code","d28bf69c":"code","a22c6b44":"code","42747205":"code","f7cb68ed":"markdown","22a0eb5d":"markdown","cd9819e8":"markdown","312f6673":"markdown","5c2b4a2f":"markdown","c17ee222":"markdown","d763f994":"markdown","f587bd8d":"markdown","09bfe581":"markdown"},"source":{"c180c21a":"import numpy as np # linear algebra\nimport pandas as pd # data processing\nimport matplotlib.pyplot as plt\nimport os\nimport warnings\nwarnings.filterwarnings(\"ignore\")\nos.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n\nfrom PIL import Image\nfrom tqdm import tqdm\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import confusion_matrix\nfrom keras.utils.np_utils import to_categorical\n\nimport tensorflow as tf\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPool2D, BatchNormalization\nfrom tensorflow.keras.optimizers import Adam, SGD\nfrom keras.preprocessing.image import ImageDataGenerator\nfrom keras.callbacks import ReduceLROnPlateau, EarlyStopping\n%matplotlib inline\n\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    print(dirname)\n    #for filename in filenames:\n        #print(os.path.join(dirname, filename))","04aeeea5":"Image.open('\/kaggle\/input\/cat-and-dog\/training_set\/training_set\/dogs\/dog.682.jpg').resize((300, 300))","7717dbc5":"Image.open('\/kaggle\/input\/cat-and-dog\/training_set\/training_set\/cats\/cat.682.jpg').resize((300, 300))","d8d165c6":"# STANDARDIZE DATA!!","15fdbd73":"training_dir_dogs = '\/kaggle\/input\/cat-and-dog\/training_set\/training_set\/dogs'\ntraining_list_dogs = []\n\nfor d in tqdm(os.listdir(training_dir_dogs)):\n    if d == '_DS_Store':\n        continue\n    path = training_dir_dogs+'\/'+d\n    image = Image.open(path).resize((200, 200)).convert('L')\n    pixels = np.asarray(image).astype('float64')\n    pixels \/= 255\n    training_list_dogs.append(pixels)","c3587b94":"training_dir_cats = '\/kaggle\/input\/cat-and-dog\/training_set\/training_set\/cats'\ntraining_list_cats = []\n\nfor c in tqdm(os.listdir(training_dir_cats)):\n    if c == '_DS_Store':\n        continue\n    path = training_dir_cats+'\/'+c\n    image = Image.open(path).resize((200, 200)).convert('L')\n    pixels = np.asarray(image).astype('float64')\n    pixels \/= 255\n    training_list_cats.append(pixels)","857fb992":"test_dir_dogs = '\/kaggle\/input\/cat-and-dog\/test_set\/test_set\/dogs'\ntest_list_dogs = []\n\nfor d in tqdm(os.listdir(test_dir_dogs)):\n    if d == '_DS_Store':\n        continue\n    path = test_dir_dogs+'\/'+d\n    image = Image.open(path).resize((200, 200)).convert('L')\n    pixels = np.asarray(image).astype('float64')\n    pixels \/= 255\n    test_list_dogs.append(pixels)","ac38bcbc":"test_dir_cats = '\/kaggle\/input\/cat-and-dog\/test_set\/test_set\/cats'\ntest_list_cats = []\n\nfor c in tqdm(os.listdir(test_dir_cats)):\n    if c == '_DS_Store':\n        continue\n    path = test_dir_cats+'\/'+c\n    image = Image.open(path).resize((200, 200)).convert('L')\n    pixels = np.asarray(image).astype('float64')\n    pixels \/= 255\n    test_list_cats.append(pixels)","a3dd6b23":"X_train = np.array([*training_list_dogs, *training_list_cats])\ntrain_y_label_dog = np.zeros((len(training_list_dogs), 1)) # dogs labeled as 0\ntrain_y_label_cat = np.ones((len(training_list_cats), 1)) # cats labeled as 1\ny_train = np.concatenate((train_y_label_dog, train_y_label_cat))","377fc975":"X_test = np.array([*test_list_dogs, *test_list_cats])\ntest_y_label_dog = np.zeros((len(test_list_dogs), 1)) # dogs labeled as 0\ntest_y_label_cat = np.ones((len(test_list_cats), 1)) # cats labeled as 1\ny_test = np.concatenate((test_y_label_dog, test_y_label_cat))","f123394e":"del training_list_dogs\ndel training_list_cats\ndel test_list_dogs\ndel test_list_cats","6db68c07":"# One-hot encoding\ny_train = to_categorical(y_train, num_classes = 2)\ny_test = to_categorical(y_test, num_classes = 2)","fde8aefc":"y_train","3a52e414":"print(\"X_train shape:\",X_train.shape)\nprint(\"y_train shape:\",y_train.shape)\nprint(\"X_test shape:\",X_test.shape)\nprint(\"y_test shape:\",y_test.shape)","12495c5d":"X_train = X_train.reshape(-1, 200, 200, 1)\nX_test = X_test.reshape(-1, 200, 200, 1)\nprint(\"X_train reshaped:\", X_train.shape)\nprint(\"X_test reshaped:\",X_test.shape)","a8f30ac1":"X_train, X_val, Y_train, Y_val = train_test_split(X_train, y_train, test_size = 0.2, random_state = 31)\nprint(\"X_train shape:\",X_train.shape)\nprint(\"X_validation shape:\",X_val.shape)\nprint(\"Y_train shape:\",Y_train.shape)\nprint(\"Y_validation shape:\",Y_val.shape)","93bd23b6":"plt.figure(figsize=(8,6))\nplt.bar([\"X train\", \"X validation\", \"X test\"], [X_train.shape[0], X_val.shape[0], X_test.shape[0]], color=[\"maroon\", \"cyan\", \"seagreen\"], width=0.4)\nplt.xlabel(\"Data\")\nplt.ylabel(\"Numbers\")\nplt.show()","c9a3d8fc":"from keras import regularizers\n\nmodel = Sequential()\nmodel.add(Conv2D(filters=32, kernel_size=(3,3), padding='Same', activation='relu', input_shape=(200, 200, 1)))\nmodel.add(BatchNormalization(epsilon=1e-06, momentum=0.9))\nmodel.add(MaxPool2D(pool_size=(3,3), strides=3))\n#model.add(Dropout(0.2))\nmodel.add(Conv2D(filters=64, kernel_size=(3,3), padding='Same', activation='relu'))\nmodel.add(BatchNormalization(epsilon=1e-06, momentum=0.9))\nmodel.add(MaxPool2D(pool_size=(3,3), strides=3))\n#model.add(Dropout(0.2))\nmodel.add(Conv2D(filters=128, kernel_size=(3,3), padding='Same', activation='relu'))\nmodel.add(BatchNormalization(epsilon=1e-06, momentum=0.9))\nmodel.add(MaxPool2D(pool_size=(3,3), strides=3))\n#model.add(Dropout(0.2))\n\nmodel.add(Flatten())\n\nmodel.add(Dense(200, activation='relu', kernel_regularizer=regularizers.l2(0.001)))\nmodel.add(BatchNormalization(epsilon=1e-06, momentum=0.9))\n#model.add(Dropout(0.2))\nmodel.add(Dense(100, activation='relu', kernel_regularizer=regularizers.l2(0.001)))\nmodel.add(BatchNormalization(epsilon=1e-06, momentum=0.9))\n#model.add(Dropout(0.2))\nmodel.add(Dense(10, activation='relu', kernel_regularizer=regularizers.l2(0.001)))\nmodel.add(BatchNormalization(epsilon=1e-06, momentum=0.9))\n#model.add(Dropout(0.2))\nmodel.add(Dense(2, activation='sigmoid'))","a093f5dd":"model.summary()","81f3a294":"from tensorflow.keras.utils import plot_model\n\nplot_model(model, show_shapes = True,expand_nested = True,dpi = 80)","1ebb5e17":"from keras.callbacks import *\n\nclass CyclicLR(Callback):\n    \"\"\"This callback implements a cyclical learning rate policy (CLR).\n    The method cycles the learning rate between two boundaries with\n    some constant frequency, as detailed in this paper (https:\/\/arxiv.org\/abs\/1506.01186).\n    The amplitude of the cycle can be scaled on a per-iteration or \n    per-cycle basis.\n    This class has three built-in policies, as put forth in the paper.\n    \"triangular\":\n        A basic triangular cycle w\/ no amplitude scaling.\n    \"triangular2\":\n        A basic triangular cycle that scales initial amplitude by half each cycle.\n    \"exp_range\":\n        A cycle that scales initial amplitude by gamma**(cycle iterations) at each \n        cycle iteration.\n    For more detail, please see paper.\n    \n    # Example\n        ```python\n            clr = CyclicLR(base_lr=0.001, max_lr=0.006,\n                                step_size=2000., mode='triangular')\n            model.fit(X_train, Y_train, callbacks=[clr])\n        ```\n    \n    Class also supports custom scaling functions:\n        ```python\n            clr_fn = lambda x: 0.5*(1+np.sin(x*np.pi\/2.))\n            clr = CyclicLR(base_lr=0.001, max_lr=0.006,\n                                step_size=2000., scale_fn=clr_fn,\n                                scale_mode='cycle')\n            model.fit(X_train, Y_train, callbacks=[clr])\n        ```    \n    # Arguments\n        base_lr: initial learning rate which is the\n            lower boundary in the cycle.\n        max_lr: upper boundary in the cycle. Functionally,\n            it defines the cycle amplitude (max_lr - base_lr).\n            The lr at any cycle is the sum of base_lr\n            and some scaling of the amplitude; therefore \n            max_lr may not actually be reached depending on\n            scaling function.\n        step_size: number of training iterations per\n            half cycle. Authors suggest setting step_size\n            2-8 x training iterations in epoch.\n        mode: one of {triangular, triangular2, exp_range}.\n            Default 'triangular'.\n            Values correspond to policies detailed above.\n            If scale_fn is not None, this argument is ignored.\n        gamma: constant in 'exp_range' scaling function:\n            gamma**(cycle iterations)\n        scale_fn: Custom scaling policy defined by a single\n            argument lambda function, where \n            0 <= scale_fn(x) <= 1 for all x >= 0.\n            mode paramater is ignored \n        scale_mode: {'cycle', 'iterations'}.\n            Defines whether scale_fn is evaluated on \n            cycle number or cycle iterations (training\n            iterations since start of cycle). Default is 'cycle'.\n    \"\"\"\n\n    def __init__(self, base_lr=0.01, max_lr=0.07, step_size=2000., mode='triangular',\n                 gamma=1., scale_fn=None, scale_mode='cycle'):\n        super(CyclicLR, self).__init__()\n\n        self.base_lr = base_lr\n        self.max_lr = max_lr\n        self.step_size = step_size\n        self.mode = mode\n        self.gamma = gamma\n        if scale_fn == None:\n            if self.mode == 'triangular':\n                self.scale_fn = lambda x: 1.\n                self.scale_mode = 'cycle'\n            elif self.mode == 'triangular2':\n                self.scale_fn = lambda x: 1\/(2.**(x-1))\n                self.scale_mode = 'cycle'\n            elif self.mode == 'exp_range':\n                self.scale_fn = lambda x: gamma**(x)\n                self.scale_mode = 'iterations'\n        else:\n            self.scale_fn = scale_fn\n            self.scale_mode = scale_mode\n        self.clr_iterations = 0.\n        self.trn_iterations = 0.\n        self.history = {}\n\n        self._reset()\n\n    def _reset(self, new_base_lr=None, new_max_lr=None,\n               new_step_size=None):\n        \"\"\"Resets cycle iterations.\n        Optional boundary\/step size adjustment.\n        \"\"\"\n        if new_base_lr != None:\n            self.base_lr = new_base_lr\n        if new_max_lr != None:\n            self.max_lr = new_max_lr\n        if new_step_size != None:\n            self.step_size = new_step_size\n        self.clr_iterations = 0.\n        \n    def clr(self):\n        cycle = np.floor(1+self.clr_iterations\/(2*self.step_size))\n        x = np.abs(self.clr_iterations\/self.step_size - 2*cycle + 1)\n        if self.scale_mode == 'cycle':\n            return self.base_lr + (self.max_lr-self.base_lr)*np.maximum(0, (1-x))*self.scale_fn(cycle)\n        else:\n            return self.base_lr + (self.max_lr-self.base_lr)*np.maximum(0, (1-x))*self.scale_fn(self.clr_iterations)\n        \n    def on_train_begin(self, logs={}):\n        logs = logs or {}\n\n        if self.clr_iterations == 0:\n            K.set_value(self.model.optimizer.lr, self.base_lr)\n        else:\n            K.set_value(self.model.optimizer.lr, self.clr())        \n            \n    def on_batch_end(self, epoch, logs=None):\n        \n        logs = logs or {}\n        self.trn_iterations += 1\n        self.clr_iterations += 1\n\n        self.history.setdefault('lr', []).append(K.get_value(self.model.optimizer.lr))\n        self.history.setdefault('iterations', []).append(self.trn_iterations)\n\n        for k, v in logs.items():\n            self.history.setdefault(k, []).append(v)\n        \n        K.set_value(self.model.optimizer.lr, self.clr())\n        \n# reference = https:\/\/github.com\/bckenstler\/CLR\/blob\/master\/clr_callback.py","aada0e95":"epoch = 100\nbatch_size = 30","3f359d8e":"clr_triangular = CyclicLR(base_lr=0.02, mode='triangular', step_size=(8*epoch))\n#optimizer = Adam(learning_rate=0.002, beta_1=0.9, beta_2=0.999)\noptimizer = SGD(lr=0.02, momentum=0.9, decay=0.01, nesterov=True)\nreduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=5, mode='min', min_lr=0.001, verbose=1, cooldown=1)\nearly_stop = EarlyStopping(monitor='val_accuracy', verbose=1, mode='max', patience=20)","05e92395":"from keras import backend as K\n\ndef recall_m(y_true, y_pred):\n    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n    recall = true_positives \/ (possible_positives + K.epsilon())\n    return recall\n\ndef precision_m(y_true, y_pred):\n    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n    precision = true_positives \/ (predicted_positives + K.epsilon())\n    return precision\n\ndef f1_m(y_true, y_pred):\n    precision = precision_m(y_true, y_pred)\n    recall = recall_m(y_true, y_pred)\n    return 2*((precision*recall)\/(precision+recall+K.epsilon()))\n\n\n# reference = https:\/\/datascience.stackexchange.com\/questions\/45165\/how-to-get-accuracy-f1-precision-and-recall-for-a-keras-model","1fd0d2a6":"model.compile(optimizer = optimizer , loss = \"binary_crossentropy\", metrics=[\"accuracy\", \"mean_absolute_error\", f1_m, precision_m, recall_m, tf.keras.metrics.AUC()])","d249955c":"#DATA AUGMENTATION","393f6f3f":"datagen = ImageDataGenerator(\n    rotation_range = 40,\n    width_shift_range = 0.2,\n    height_shift_range = 0.2,\n    #shear_range = 0.2,\n    zoom_range = 0.2,\n    fill_mode='nearest',\n    #horizontal_flip = True,\n    vertical_flip = True)\n\n    \n#datagen.fit(X_train)","350cd2b1":"history = model.fit_generator(datagen.flow(X_train, Y_train, batch_size=batch_size), epochs=epoch, callbacks=[clr_triangular, reduce_lr, early_stop], verbose=1, validation_data=(X_val, Y_val))","c89264ab":"plt.plot(history.history[\"loss\"], color=\"blue\", label=\"Loss Values\")\nplt.xlabel(\"Epochs\")\nplt.ylabel(\"Loss\")\nplt.legend()\nplt.show()","635e4c79":"plt.figure(figsize=(8,5))\nplt.plot(history.history[\"accuracy\"], color=\"red\", label=\"Accuracy\")\nplt.plot(history.history[\"val_accuracy\"], color=\"yellow\", label=\"Validation Accuracy\")\nplt.xlabel(\"Epochs\")\nplt.ylabel(\"Accuracies\")\nplt.legend()\nplt.show()","9709db96":"fig = plt.figure(1, figsize=(10,10))\nax1 = fig.add_subplot(311)\nax2 = fig.add_subplot(312)\nax3 = fig.add_subplot(313)\nxx = list(range(1,65+1))\nax1.plot(xx, history.history[\"f1_m\"], color=\"b\", label=\"F1 score\")\nax1.xaxis.set_ticks(list(range(0,65+1, 5)))\nax1.yaxis.set_ticks(list(np.arange(0.55, 0.80, 0.05)))\nax1.legend()\n\nax2.plot(xx, history.history[\"precision_m\"], color=\"g\", label=\"Precision\")\nax2.xaxis.set_ticks(list(range(0,65+1, 5)))\nax2.yaxis.set_ticks(list(np.arange(0.55, 0.80, 0.05)))\nax2.legend()\n\nax3.plot(xx, history.history[\"recall_m\"], color=\"r\", label=\"Recall\")\nax3.xaxis.set_ticks(list(range(0,65+1,5)))\nax3.yaxis.set_ticks(list(np.arange(0.55, 0.80, 0.05)))\nax3.legend()\n\nfig.supxlabel('Epochs')\nfig.supylabel('Scores')\nplt.show()","6c623bba":"score = model.evaluate(X_test, y_test, verbose=0)","d28bf69c":"print('Test loss:', round(score[0], 3)) \nprint('Test accuracy:', round(score[1], 3))\nprint('Test MAE:', round(score[2], 3))\nprint('F1 score:', round(score[3], 3))\nprint('Precision:', round(score[4], 3))\nprint('Recall:', round(score[5], 3))\nprint(\"AUC:\", round(score[6], 3))","a22c6b44":"y_pred = model.predict(X_test)","42747205":"from sklearn.metrics import ConfusionMatrixDisplay\nfrom sklearn.metrics import confusion_matrix\n\ncm = confusion_matrix(np.argmax(y_test,axis = 1), np.argmax(y_pred,axis = 1))\ndisp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=[\"Dogs\", \"Cats\"])\nfig, ax = plt.subplots(figsize=(6, 6))\ndisp.plot(cmap=plt.cm.Blues, ax=ax)\nplt.show()","f7cb68ed":"<a id=\"2\"><\/a>\n## First Look at the Data","22a0eb5d":"<a id=\"8\"><\/a>\n## Resources\n\n* Resources that I benefited from:\n\n1. [Convolutional Neural Network (CNN) Tutorial][1]\n[1]:https:\/\/www.kaggle.com\/kanncaa1\/convolutional-neural-network-cnn-tutorial\n2. [Hey Siri!! is it a \ud83d\udc31 or\ud83d\udc36 !! \ud83c\udfaf Class-f1 (0.992)][2]\n[2]:https:\/\/www.kaggle.com\/bnymnsen\/hey-siri-is-it-a-or-class-f1-0-992\n3. [CNN Model Training Using Masking][3]\n[3]:https:\/\/www.kaggle.com\/accountstatus\/cnn-model-training-using-masking\n4. [A Comprehensive Guide to Convolutional Neural Networks][4]\n[4]:https:\/\/towardsdatascience.com\/a-comprehensive-guide-to-convolutional-neural-networks-the-eli5-way-3bd2b1164a53","cd9819e8":"<a id=\"5\"><\/a>\n## Training Data Visualization","312f6673":"<a id=\"7\"><\/a>\n## Conclusion\n\nIn my first attempts, without using Data Augmentation, my model achieved 99% accuracy on the training dataset. However, my test and validation accuracies were close 68%. It basically means that my CNN model was overfitted to training dataset. Therefore, I decided to use Data Augmentation to prevent overfitting. Nevertheless, despite numerous adjustments, accuracy values of the model freaked out and started  oscillating until I used Batch Normalization. For those who faced the same problem, I recommend to use Batch Normalization.\n\nIn conclusion, I believe that my current model can achieve 90+% accuracy by tuning hyperparameters.","5c2b4a2f":"<a id=\"4\"><\/a>\n## CNN Model Construction","c17ee222":"<a id=\"1\"><\/a>\n## Importing Libraries","d763f994":"## Convolutional Neural Networks (CNN)\n\n\n<font color='blue'>\n<h2 style = \"font-family: Arial; font-size: 30px; font-style: normal;\" >Content:<\/h2> \n\n* [Importing Libraries](#1)\n* [First Look at the Data](#2)\n* [Data Preparation](#3)\n* [CNN Model Construction](#4)\n* [Training Data Visualization](#5)\n* [Evaluation of the Model](#6)\n* [Conclusion](#7)\n* [Resources](#8)\n   \n","f587bd8d":"<a id=\"6\"><\/a>\n## Evaluation of the Model","09bfe581":"<a id=\"3\"><\/a>\n## Data Preparation"}}