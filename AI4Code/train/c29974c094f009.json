{"cell_type":{"17304b8d":"code","e48afde8":"code","2db2cc13":"code","66ab2c01":"code","01f52f63":"code","524c720b":"code","4c92bdb7":"code","7d2fb490":"code","d759d318":"code","106617dd":"code","0e21370c":"code","e42677ad":"code","50714d79":"code","1a0accfd":"code","738bd46f":"code","f9bf2421":"code","57be0971":"code","d9d8c125":"code","609f3a71":"code","0bfb3669":"code","26f9ad09":"code","8ab7c83f":"code","f8213f78":"code","9e81ddec":"code","cb6d7bce":"code","afbf65ff":"markdown","330d064c":"markdown","501d6025":"markdown","13de0c35":"markdown","7911c064":"markdown","fa901289":"markdown","cd3fec2f":"markdown","3cc7b0c0":"markdown","6c035b46":"markdown","0b36f64c":"markdown","9ac6cc8e":"markdown","db5bf504":"markdown"},"source":{"17304b8d":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\nimport plotly.express as px\nimport plotly.offline as py\nfrom plotly import tools, subplots\npy.init_notebook_mode(connected=True)\nfrom plotly.offline import init_notebook_mode, iplot\ninit_notebook_mode(connected=True)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","e48afde8":"df = pd.read_csv('..\/input\/hackathon\/task_2-owid_covid_data-21_June_2020.csv')\ndf.head()","2db2cc13":"# Data processing, metrics and modeling\nfrom sklearn.preprocessing import LabelEncoder, StandardScaler\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import StratifiedKFold\nfrom datetime import datetime\nfrom sklearn.metrics import precision_score, recall_score, confusion_matrix, accuracy_score, roc_auc_score, f1_score, roc_curve, auc,precision_recall_curve\nfrom sklearn import metrics\n# Lgbm\nimport lightgbm as lgb\nimport catboost\nfrom catboost import Pool\nimport xgboost as xgb\n\n# Suppr warning\nimport warnings\nwarnings.filterwarnings(\"ignore\")","66ab2c01":"def missing_data(data):\n    total = data.isnull().sum()\n    percent = (data.isnull().sum()\/data.isnull().count()*100)\n    tt = pd.concat([total, percent], axis=1, keys=['Total', 'Percent'])\n    types = []\n    for col in data.columns:\n        dtype = str(data[col].dtype)\n        types.append(dtype)\n    tt['Types'] = types\n    return(np.transpose(tt))","01f52f63":"%%time\nmissing_data(df)","524c720b":"import missingno as msno","4c92bdb7":"#Nullity Matrix. The msno.matrix nullity matrix is a data-dense display which lets you quickly visually analyse data completion.\nmsno.matrix(df.head(10000))","7d2fb490":"%%time\na = msno.heatmap(df, sort='ascending')\na","d759d318":"%%time\na2 = msno.dendrogram(df)\na2","106617dd":"# Number of unique classes in each object column\ndf.select_dtypes('object').apply(pd.Series.nunique, axis = 0)","0e21370c":"# Find correlations with the target and sort\ncorrelations = df.corr()['total_cases'].sort_values()\n\n# Display correlations\nprint('Most Positive Correlations:\\n', correlations.tail(15))\nprint('\\nMost Negative Correlations:\\n', correlations.head(15))","e42677ad":"%%time\nfeatures = df.columns.values[2:112]\ncorrs_ = df[features].corr().abs().unstack().sort_values(kind=\"quicksort\").reset_index()\ncorrs_ = corrs_[corrs_['level_0'] != corrs_['level_1']]\ncorrs_.head(10)","50714d79":"corrs_.head(10)","1a0accfd":"corrs_.tail(10)","738bd46f":"%%time\ncorrs = df.corr()\nplt.figure(figsize = (10, 6))\n# Heatmap of correlations\nsns.heatmap(corrs, cmap = plt.cm.RdYlBu_r, vmin = -0.25, annot = False, vmax = 0.8)\nplt.title('Clustermap');","f9bf2421":"def plot_dist_col(column):\n    pos__df = df[df['total_cases'] ==1]\n    neg__df = df[df['total_cases'] ==0]\n\n    '''plot dist curves for train and test weather data for the given column name'''\n    fig, ax = plt.subplots(figsize=(8, 8))\n    sns.distplot(pos__df[column].dropna(), color='green', ax=ax).set_title(column, fontsize=16)\n    sns.distplot(neg__df[column].dropna(), color='purple', ax=ax).set_title(column, fontsize=16)\n    plt.xlabel(column, fontsize=15)\n    plt.legend(['total_cases', 'total_tests'])\n    plt.show()\nplot_dist_col('total_tests')","57be0971":"#fill in mean for floats\nfor c in df.columns:\n    if df[c].dtype=='float16' or  df[c].dtype=='float32' or  df[c].dtype=='float64':\n        df[c].fillna(df[c].mean())\n\n#fill in -999 for categoricals\ndf = df.fillna(-999)\n# Label Encoding\nfor f in df.columns:\n    if df[f].dtype=='object': \n        lbl = LabelEncoder()\n        lbl.fit(list(df[f].values))\n        df[f] = lbl.transform(list(df[f].values))\n        \nprint('Labelling done.') ","d9d8c125":"from sklearn.base import BaseEstimator, TransformerMixin, RegressorMixin, clone\nclass AveragingModels(BaseEstimator, RegressorMixin, TransformerMixin):\n    def __init__(self, models):\n        self.models = models\n        \n    # we define clones of the original models to fit the data in\n    def fit(self, X, y):\n        self.models_ = [clone(x) for x in self.models]\n        \n        # Train cloned base models\n        for model in self.models_:\n            model.fit(X, y)\n\n        return self\n    \n    #Now we do the predictions for cloned models and average them\n    def predict(self, X):\n        predictions = np.column_stack([\n            model.predict(X) for model in self.models_\n        ])\n        return np.mean(predictions, axis=1)","609f3a71":"from sklearn.linear_model import ElasticNet, Lasso,  BayesianRidge, LassoLarsIC\nfrom sklearn.preprocessing import RobustScaler\nfrom sklearn.pipeline import make_pipeline\nENet = make_pipeline(RobustScaler(), ElasticNet(alpha=0.0005, l1_ratio=.9, random_state=3))","0bfb3669":"from sklearn.ensemble import RandomForestRegressor,  GradientBoostingRegressor\nGBoost = GradientBoostingRegressor(n_estimators=3000, learning_rate=0.05,\n                                   max_depth=4, max_features='sqrt',\n                                   min_samples_leaf=15, min_samples_split=10, \n                                   loss='huber', random_state =5)","26f9ad09":"from sklearn.kernel_ridge import KernelRidge\nKRR = KernelRidge(alpha=0.6, kernel='polynomial', degree=2, coef0=2.5)","8ab7c83f":"from sklearn.linear_model import ElasticNet, Lasso,  BayesianRidge, LassoLarsIC\nlasso = make_pipeline(RobustScaler(), Lasso(alpha =0.0005, random_state=1))","f8213f78":"from sklearn.model_selection import KFold, cross_val_score, train_test_split\n#Validation function\nn_folds = 5\n\ndef rmsle_cv(model):\n    kf = KFold(n_folds, shuffle=True, random_state=42).get_n_splits(df.values)\n    rmse= np.sqrt(-cross_val_score(model, df.values, y_train, scoring=\"neg_mean_squared_error\", cv = kf))\n    return(rmse)","9e81ddec":"#ntrain = train.shape[0]\n#ntest = test.shape[0]\ny_train = df.total_cases.values\n#all_data = pd.concat((train, test)).reset_index(drop=True)\na#ll_data.drop(['SalePrice'], axis=1, inplace=True)\n#print(\"all_data size is : {}\".format(all_data.shape))","cb6d7bce":"#averaged_models = AveragingModels(models = (ENet, GBoost, KRR, lasso))\n\n#score = rmsle_cv(averaged_models)\n#print(\" Averaged base models score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))","afbf65ff":"#Codes from Serigne https:\/\/www.kaggle.com\/serigne\/stacked-regressions-top-4-on-leaderboard\/notebook","330d064c":"The least correlated features.","501d6025":"#Label Encoding.\n\nIt assigns each unique value to a different integer.","13de0c35":"#Heatmap. The missingno correlation heatmap measures nullity correlation: how strongly the presence or absence of one variable affects the presence of another:\nDon't write anything before the per cent time, Otherwise it wouldn't work.","7911c064":"#Correlations clustermap","fa901289":"#Codes from Crisl\u00e2nio Macedo https:\/\/www.kaggle.com\/caesarlupum\/brazil-against-the-advance-of-covid-19\/notebook?select=h2oai-pystacknet-af571e0 Master and friend","cd3fec2f":"![](https:\/\/encrypted-tbn0.gstatic.com\/images?q=tbn%3AANd9GcQUWXStMee9eQjoH2a3K8qslB6z_PnVU5GEbQ&usqp=CAU)inet.ox.ac.uk","3cc7b0c0":"#Correlations","6c035b46":"The top most correlated features, besides the same feature pairs.","0b36f64c":"Das War's, Kaggle Notebook Runner: Mar\u00edlia Prata  @mpwolke.","9ac6cc8e":"I commented the snippet below because it was frozen then I couldn't perform anything else. I made \"Stacked Stucked OWID\" public it's showing like some \"inutility script\" all black. I worked on it for 3 days, trying to use PyStackNet then Stack. The worst, it only appears in my page though it's Public. It doesn't appear in the list of that Dataset. I simply don't know why. No clue. Maybe it was abducted and I was stucked on it. The \"fade away\" from the title is due to that situation. (The 1st attempt to Stack isn't listed in the Datasets challenge list, simply remains in my page, as a sign of shame. ","db5bf504":"#Dendrogram.The dendrogram allows you to more fully correlate variable completion, revealing trends deeper than the pairwise ones visible in the correlation heatmap:"}}