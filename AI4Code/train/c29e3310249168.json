{"cell_type":{"c21cf7fb":"code","fc3f7175":"code","3294a94e":"code","83b440d4":"code","4580a026":"code","3f0e3743":"code","6fa45c28":"code","6bdd5baf":"code","7bbff3b7":"code","f5a8ea94":"code","46fcf42c":"code","630a745e":"code","5cffe4e6":"code","da94af77":"code","ed773944":"code","6ab48cb5":"code","3263e3bd":"code","40d3e800":"code","31a1dae0":"code","6aa1a7b3":"code","d9b26290":"code","99c84fa9":"code","12a5776b":"code","ef7907db":"code","ec988fd3":"code","dcab2ca2":"code","5ed614b2":"code","bf5ec9e4":"code","22b45c59":"code","6d9991e8":"code","4ffbbab2":"code","1a9fabd6":"code","ceaa58f6":"markdown","79c29daa":"markdown","f7c69a61":"markdown","168434dc":"markdown","47813743":"markdown","78db1398":"markdown","74c3f3ac":"markdown"},"source":{"c21cf7fb":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","fc3f7175":"pip install mtcnn","3294a94e":"from matplotlib import pyplot as plt\nimport cv2\nfrom PIL import Image\n\nimport mtcnn\nfrom mtcnn.mtcnn import MTCNN\nfrom matplotlib.patches import Rectangle\n\nfrom os import listdir\nfrom tqdm import tqdm\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nimport seaborn as sns","83b440d4":"DIRECTORY = \"..\/input\/opencv-facial-recognition-lbph\/yalefaces\/\"\nDIRECTORY_train='..\/input\/opencv-facial-recognition-lbph\/yalefaces\/train\/'\nDIRECTORY_test='..\/input\/opencv-facial-recognition-lbph\/yalefaces\/test\/'","4580a026":"filename = \"..\/input\/opencv-facial-recognition-lbph\/yalefaces\/test\/subject03.glasses.gif\"\npixels = plt.imread(filename)\n\nrgb_pixels = np.stack((pixels, pixels, pixels), axis=2)\nprint(rgb_pixels.shape)\nplt.imshow(pixels)\nplt.show()","3f0e3743":"detector = MTCNN()\n\nresults = detector.detect_faces(rgb_pixels)\nresults","6fa45c28":"def draw_image_with_boxes(data, result_list):\n    plt.imshow(data)\n    ax = plt.gca()\n    for result in result_list:\n        x, y, width, height = result['box']\n        rect = Rectangle((x, y), width, height, fill=False, color='red')\n        ax.add_patch(rect)\n    plt.show()\n\ndraw_image_with_boxes(rgb_pixels, results)","6bdd5baf":"def extract_face_from_file(filename, required_size=(160, 160)):\n    image = Image.open(filename)\n    \n    return extract_face(image, required_size)\n\ndef extract_face(image, required_size=(160, 160)):\n    image = image.convert('RGB')\n    pixels = np.asarray(image)\n    results = detector.detect_faces(pixels)\n\n    x1, y1, width, height = results[0]['box']\n    \n    x1, y1 = abs(x1), abs(y1)\n    x2, y2 = x1 + width, y1 + height\n\n    face = pixels[y1:y2, x1:x2]\n\n    image = Image.fromarray(face)\n    image = image.resize(required_size)\n    face_array = np.asarray(image)\n    gray_face = cv2.cvtColor(face_array, cv2.COLOR_BGR2GRAY)\n    \n    return gray_face\n\ndetector = MTCNN()\n\nface_pixels = extract_face_from_file(\"..\/input\/opencv-facial-recognition-lbph\/yalefaces\/test\/subject03.glasses.gif\")\n\nplt.imshow(face_pixels)","7bbff3b7":"def list_files(directory, contains):\n    return list(f for f in listdir(directory) if contains in f)","f5a8ea94":"i = 1\nfaces = list()\nfor filename in tqdm(list_files(DIRECTORY_train, \"subject\")[0:16]):\n    # path\n    path = DIRECTORY_train + filename\n    # get face\n    face = extract_face_from_file(path)\n    # plot\n    plt.subplot(4, 4, i)\n    plt.axis('off')\n    plt.imshow(face)\n    faces.append(face)\n    i += 1\nplt.show()","46fcf42c":"filenames = pd.DataFrame(list_files(DIRECTORY_train, \"subject\"))\n\ndf_train = filenames[0].str.split(\".\", expand=True)\ndf_train[\"filename\"] = filenames\n\ndf_train = df_train.rename(columns = {0:\"subject\", 1:\"category\"})\ndf_train['subject'] = df_train.subject.str.replace('subject' , '')\ndf_train.apply(pd.to_numeric, errors='coerce').dropna()\ndf_train['subject'] = pd.to_numeric(df_train[\"subject\"])\ndf_train","630a745e":"filenames2 = pd.DataFrame(list_files(DIRECTORY_test, \"subject\"))\n\ndf_test = filenames2[0].str.split(\".\", expand=True)\ndf_test[\"filename\"] = filenames2\n\ndf_test = df_test.rename(columns = {0:\"subject\", 1:\"category\"})\ndf_test['subject'] = df_test.subject.str.replace('subject' , '')\ndf_test.apply(pd.to_numeric, errors='coerce').dropna()\ndf_test['subject'] = pd.to_numeric(df_test[\"subject\"])\ndf_test","5cffe4e6":"x_train=df_train.loc[:,['category','filename']]\nx_test=df_test.loc[:,['category','filename']]\ny_train=df_train.loc[:,['subject']]\ny_test=df_test.loc[:,['subject']]","da94af77":"y_train=y_train.to_numpy()\ny_test=y_test.to_numpy()","ed773944":"y_train = y_train.tolist()\ny_test = y_test.tolist()","6ab48cb5":"detector = MTCNN()\n\ndef load_dataset1(dataset):\n    faces = list()\n    for filename in tqdm(dataset[\"filename\"]):\n        path = DIRECTORY_train + filename\n        # get face\n        face = extract_face_from_file(path)\n        faces.append(face)\n    return np.asarray(faces)","3263e3bd":"detector = MTCNN()\n\ndef load_dataset2(dataset):\n    faces = list()\n    for filename in tqdm(dataset[\"filename\"]):\n        path = DIRECTORY_test + filename\n        # get face\n        face = extract_face_from_file(path)\n        faces.append(face)\n    return np.asarray(faces)","40d3e800":"x_test = load_dataset2(x_test)\nx_train = load_dataset1(x_train)\n\nprint(x_test.shape)\nprint(x_train.shape)","31a1dae0":"TRAINING_DATA_DIRECTORY = \"data\/train\"\nTESTING_DATA_DIRECTORY = \"data\/test\"\nNUM_CLASSES = 15\nEPOCHS = 25\nBATCH_SIZE = 20\nNUMBER_OF_TRAINING_IMAGES = 135\nNUMBER_OF_TESTING_IMAGES = 30\nIMAGE_HEIGHT = 160\nIMAGE_WIDTH = 160","6aa1a7b3":"import os \n\ndef save_keras_dataset(setname, dataset, labels, per_class):\n    data = sorted(list(zip(labels, dataset)), key=lambda x: x[0])\n\n    j = 0\n    for label, gray_img in tqdm(data):\n        j = (j% per_class) + 1\n        \n        directory = f\"data\/{setname}\/class_{label}\/\"\n        if not os.path.exists(directory):\n                os.makedirs(directory)\n        cv2.imwrite(f\"{directory}class_{label}_{j}.png\",gray_img)","d9b26290":"import shutil\nshutil.rmtree(r'data', ignore_errors=True)\n\n# Save datasets\nsave_keras_dataset(\"test\", x_test, y_test, 3)\nsave_keras_dataset(\"train\", x_train, y_train, 8)","99c84fa9":"from tensorflow.keras.preprocessing.image import ImageDataGenerator\ndef data_generator():\n    return ImageDataGenerator(\n        rescale=1.\/255,\n        \n    )\n\ndef add_noise(img):\n    \"\"\"Add random noise to an image\"\"\"\n    VARIABILITY = 35\n    deviation = VARIABILITY*random.random()\n    noise = np.random.normal(0, deviation, img.shape)\n    img += noise\n    np.clip(img, 0., 255.)\n    return img","12a5776b":"training_generator = data_generator().flow_from_directory(\n    TRAINING_DATA_DIRECTORY,\n    target_size=(IMAGE_WIDTH, IMAGE_HEIGHT),\n    batch_size=BATCH_SIZE,\n    class_mode='categorical',\n    color_mode='grayscale'\n)\n\ntesting_generator = data_generator().flow_from_directory(\n    TESTING_DATA_DIRECTORY,\n    target_size=(IMAGE_WIDTH, IMAGE_HEIGHT),\n    class_mode='categorical',\n    color_mode='grayscale'\n)\n\nvalidation_generator = data_generator().flow_from_directory(\n    TESTING_DATA_DIRECTORY,\n    target_size=(IMAGE_WIDTH, IMAGE_HEIGHT),\n    class_mode='categorical',\n    color_mode='grayscale',\n    shuffle=False\n)","ef7907db":"sample_images = testing_generator.next()[0]\n\nf, xyarr = plt.subplots(3,3)\nxyarr[0,0].imshow(sample_images[0])\nxyarr[0,1].imshow(sample_images[1])\nxyarr[0,2].imshow(sample_images[2])\nxyarr[1,0].imshow(sample_images[3])\nxyarr[1,1].imshow(sample_images[4])\nxyarr[1,2].imshow(sample_images[5])\nxyarr[2,0].imshow(sample_images[6])\nxyarr[2,1].imshow(sample_images[7])\nxyarr[2,2].imshow(sample_images[8])\nplt.show()","ec988fd3":"import keras\nclass MCDropout(keras.layers.Dropout):\n    def call(self, inputs):\n        return super().call(inputs, training=True)","dcab2ca2":"from tensorflow.keras import models\nfrom tensorflow.keras.layers import Activation, ZeroPadding2D, MaxPooling2D, Conv2D, Flatten, Dense, Dropout\nfrom tensorflow.keras import regularizers, constraints\n\nmodel = models.Sequential()\n\nmodel.add(Conv2D(32, kernel_size=(3, 3), activation='linear', input_shape=(IMAGE_WIDTH, IMAGE_HEIGHT, 1), padding='same'))\nmodel.add(MaxPooling2D((2, 2)))\n\nmodel.add(Conv2D(64, (3, 3), activation='relu', kernel_regularizer=regularizers.l2(l2=0.01)))\nmodel.add(MaxPooling2D(pool_size=(2, 2)))\n\nmodel.add(Conv2D(128, (3, 3), activation='relu', kernel_regularizer=regularizers.l2(l2=0.01)))\nmodel.add(MaxPooling2D(pool_size=(2, 2)))\n\nmodel.add(Flatten())\n\nmodel.add(Dense(512, activation='relu', kernel_initializer=\"glorot_uniform\", kernel_regularizer=regularizers.l2(l2=0.01)))\n\nmodel.add(MCDropout(rate=0.5))\n\nmodel.add(Dense(NUM_CLASSES, activation='softmax', kernel_initializer=\"glorot_uniform\"))","5ed614b2":"model.summary()","bf5ec9e4":"from keras.utils.vis_utils import plot_model\nplot_model(model, show_shapes=True, show_layer_names=True)","22b45c59":"from tensorflow.keras import optimizers, losses\nfrom tensorflow.keras.callbacks import EarlyStopping\nearly_stopping = EarlyStopping()\n\nmodel.compile(\n    loss=losses.CategoricalCrossentropy(from_logits=True),\n    optimizer=optimizers.Adam(learning_rate=0.0003),\n    metrics=[\"accuracy\"]\n)\n\nhistory = model.fit(\n    training_generator,\n    steps_per_epoch=(NUMBER_OF_TRAINING_IMAGES\/\/BATCH_SIZE ),\n    epochs=EPOCHS,\n    validation_data=testing_generator,\n    shuffle=True,\n    validation_steps=(NUMBER_OF_TESTING_IMAGES\/\/BATCH_SIZE),\n#     callbacks=[early_stopping]\n)","6d9991e8":"plot_folder = \"plot\"\nplt.plot(history.history['accuracy'], label='accuracy')\nplt.plot(history.history['val_accuracy'], label='val_accuracy')\nplt.xlabel('Epoch')\nplt.ylabel('Accuracy')\nplt.ylim([0.1, 1])\nplt.legend(loc='lower right')","4ffbbab2":"plot_folder = \"plot\"\nplt.plot(history.history['loss'], label='loss')\nplt.plot(history.history['val_loss'], label='val_loss')\nplt.xlabel('Epoch')\nplt.ylabel('Val Loss')\nplt.legend(loc='lower right')","1a9fabd6":"from sklearn.metrics import classification_report, confusion_matrix, precision_recall_fscore_support\n\nY_pred = model.predict(validation_generator)\ny_pred = np.argmax(Y_pred, axis=1)\nprint(classification_report(validation_generator.classes, y_pred))\nprint(validation_generator.classes)\nprint(y_pred)\nprint('Confusion Matrix')\nprint(confusion_matrix(validation_generator.classes, y_pred))","ceaa58f6":"# 2) MTCNN","79c29daa":"# 1) Data Sample","f7c69a61":"# 3) Extract and normalise the face pixels","168434dc":"# 2.Data set\u3000\u301c\u3000135 train datas and 30 test datas","47813743":"# 3. Convolutional Neural Network Model","78db1398":"# 1.Data Analysis","74c3f3ac":"# I refered to https:\/\/www.kaggle.com\/georgearnall\/yale-face-recognition\n# I learned a lot of things from this notebook. Thank you very much."}}