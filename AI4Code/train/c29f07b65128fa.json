{"cell_type":{"d5df7ac7":"code","d3c7999e":"code","3e695588":"code","12c5444e":"code","73fb7fa9":"code","2f0ac4cd":"code","6db0caa7":"code","bb87f1f8":"code","8a98ff18":"code","204a59c8":"code","85bb5392":"code","1c1467cc":"code","d67515d0":"code","ccbda11a":"code","aff877b3":"code","b9c85b3b":"code","58f76e04":"code","d5e5674d":"code","82471276":"code","3e737575":"code","e1084e1f":"code","93f95609":"code","174ce580":"code","81278d21":"code","638e439d":"code","49a99924":"code","726f008e":"code","2c611c96":"code","567b6297":"code","68216bed":"code","bdeac1cd":"code","2b2ad29b":"code","00b2d97d":"code","311417e7":"code","dcb750bc":"code","895746a5":"code","0a9bbec2":"code","9887d447":"code","691f07ff":"code","9c871710":"code","6e135440":"code","e8a71f4c":"code","90952159":"code","c9eba2dc":"code","e8cf7100":"code","b1a7085b":"code","1648e880":"code","573968f6":"code","a24e0df8":"code","409b1ab9":"code","179c2c65":"code","fe3711e6":"code","03f5ecbd":"code","eac80d6a":"code","d9f338b2":"code","a653b23b":"code","9d6abccd":"code","d408cc3c":"code","0c53c013":"code","b08af4a8":"code","9756560d":"code","e2196d06":"code","b7c06bbf":"code","719adbf5":"code","b3bd9a06":"code","1d813643":"code","430e2e5c":"code","fbb39bb8":"markdown","ed282150":"markdown","f151a598":"markdown","c8804461":"markdown","0fb9c1b0":"markdown","c349bd44":"markdown","0c97d4be":"markdown","869ee688":"markdown","6f2ae725":"markdown","687dc6e3":"markdown","b7fa6024":"markdown","f9f3ce72":"markdown","0d8d40d8":"markdown","011f0ddd":"markdown","1174d0e1":"markdown","5fecd60b":"markdown","f5d2db18":"markdown","929acffa":"markdown","a0600fed":"markdown","66dd4143":"markdown","b7b7aa5e":"markdown","ec1a0c79":"markdown","ca6ade67":"markdown","36384b91":"markdown","a8cd8737":"markdown","c9e1e0d5":"markdown","56f2ccde":"markdown","2bb9ad36":"markdown","f957348a":"markdown","96f644ce":"markdown","1f8e8b42":"markdown","cd3d079f":"markdown","46190cb4":"markdown","476265bf":"markdown","501e28a7":"markdown","61d7fe0d":"markdown","cf61cd04":"markdown","601bfe65":"markdown","cbb83c2b":"markdown","d064b24b":"markdown","1681a732":"markdown","0b9c5966":"markdown","9c082b8b":"markdown","25303273":"markdown","abfa9cb3":"markdown","03258d4b":"markdown"},"source":{"d5df7ac7":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","d3c7999e":"import pandas as pd\n\n#this data set has all females surviving and all males not surviving\n#get ball-park estimate about what feautures are important in the data set\ngender_submission = pd.read_csv(\"..\/input\/titanic\/gender_submission.csv\")\n\n#testing data has no labels (i.e., does not include 'Survived' variable)\ntest = pd.read_csv(\"..\/input\/titanic\/test.csv\")\n#training set has labels and is usde to train our model\ntrain = pd.read_csv(\"..\/input\/titanic\/train.csv\")","3e695588":"train = pd.read_csv(\"\/kaggle\/input\/titanic\/train.csv\")\n#show top 5 rows of the data set \ntrain.head()","12c5444e":"test = pd.read_csv(\"\/kaggle\/input\/titanic\/test.csv\")\ntest.head()","73fb7fa9":"#query train data by column name 'Sex', where it equals 'female'\nwomen = train.loc[train.Sex == 'female'][\"Survived\"]\n\n#sum of women that survived \/ total number of women\nrate_women = sum(women)\/len(women)\n\nprint(\"% of women who survived:\", rate_women)","2f0ac4cd":"men = train.loc[train.Sex == 'male'][\"Survived\"]\nrate_men = sum(men)\/len(men)\n\nprint(\"% of men who survived:\", rate_men)","6db0caa7":"#import Shallow Machine Learning library (i.e., sklearn)\n#import the Random Forest Algorithm\nfrom sklearn.ensemble import RandomForestClassifier\n\n#DEPENDENT VARIABLE\ny = train[\"Survived\"]\n\n#INDIPENDENT VARIABLES\n#the features we will include in our model\nfeatures = [\"Pclass\", \"Sex\", \"SibSp\", \"Parch\"]\nX = pd.get_dummies(train[features])\nX_test = pd.get_dummies(test[features])\n\n#100 random forest trees\nmodel = RandomForestClassifier(n_estimators=100, max_depth=5, random_state=1)\nmodel.fit(X, y) #fit the model\npredictions = model.predict(X_test) \n\noutput = pd.DataFrame({'PassengerId': test.PassengerId, 'Survived': predictions})\noutput.to_csv('my_submission.csv', index=False)\nprint(\"Your submission was successfully saved!\")","bb87f1f8":"#get an idea aboutt the data set\ntrain.shape","8a98ff18":"#Descriptive Statistics\ntrain.describe()","204a59c8":"#object data types are a type of data in Pandas\ntrain.describe(include=['O'])","85bb5392":"train.info()","1c1467cc":"#There are 177 rows with missing Age, 687 rows with missing Cabin and 2 rows with missing Embarked information.\ntrain.isnull().sum()","d67515d0":"print(train.shape)\ntest.shape","ccbda11a":"test.info()","aff877b3":"#There are 86 rows with missing Age, 327 rows with missing Cabin and 1 row with missing Fare information.\n\n#shows total number of missing values\ntest.isnull().sum()","b9c85b3b":"#shows number\/percent of survived\nsurvived = train[train['Survived'] == 1]\n#shows number\/percent of those that did not survived\nnot_survived = train[train['Survived'] == 0]\n\nprint (\"Survived: %i (%.1f%%)\"%(len(survived), float(len(survived))\/len(train)*100.0))\nprint (\"Not Survived: %i (%.1f%%)\"%(len(not_survived), float(len(not_survived))\/len(train)*100.0))\nprint (\"Total: %i\"%len(train))","58f76e04":"train.Pclass.value_counts()","d5e5674d":"#count number of survived\/did not survive per class\ntrain.groupby('Pclass').Survived.value_counts()","82471276":"#percent survived by class\ntrain[['Pclass', 'Survived']].groupby(['Pclass'], as_index=False).mean()","3e737575":"import seaborn as sns\nimport matplotlib.pyplot as plt\n\n#train.groupby('Pclass').Survived.mean().plot(kind='bar')\nsns.barplot(x='Pclass', y='Survived', data=train)","e1084e1f":"#train.groupby('Sex').Survived.mean().plot(kind='bar')\nsns.barplot(x='Sex', y='Survived', data=train)\n","93f95609":"#number of passengers by sex \ntrain.Sex.value_counts()","174ce580":"#Female ratio of surviavl indicates females more likely to have survived\ntrain.groupby('Sex').Survived.value_counts()","81278d21":"#mean values for survival \ntrain[['Sex', 'Survived']].groupby(['Sex'], as_index=False).mean()","638e439d":"tab = pd.crosstab(train['Pclass'], train['Sex'])\nprint (tab)\n\ntab.div(tab.sum(1).astype(float), axis=0).plot(kind=\"bar\", stacked=False)\nplt.xlabel('Pclass')\nplt.ylabel('Percentage')","49a99924":"# Shows that Males in all classes less likely to survive than Females in corresponding classes\n# Shows that Males in 2nd and 3rd classe less likely to survive than Males in 1st class\nsns.factorplot('Sex', 'Survived', hue='Pclass', size=4, aspect=3, data=train)","726f008e":"#embark = port of embarkment (C = Cherbourg, Q = Queenstown, S = Southampton)\nsns.factorplot(x='Pclass', y='Survived', hue='Sex', col='Embarked', data=train)","2c611c96":"#count for each port of embarkment (highest is S)\ntrain.Embarked.value_counts()","567b6297":"#appears embarked from Q less than 50% chance of survival (second worst)\n#emarkment S passengers much more likely to die (worst)\n#C passengers more likely to survive (least worst)\ntrain.groupby('Embarked').Survived.value_counts()\n","68216bed":"#mean survival chance by each port (C passengers over 50% chance of survival)\ntrain[['Embarked', 'Survived']].groupby(['Embarked'], as_index=False).mean()","bdeac1cd":"#different plot but decided to go with the one below \n#train.groupby('Embarked').Survived.mean().plot(kind='bar')\n\n#chose this plot because it is easier to decipher that C embarked passengers had a higher chance survival \nsns.barplot(x='Embarked', y='Survived', data=train)","2b2ad29b":"#the majority of passengers did not have parents \/ children\ntrain.Parch.value_counts()","00b2d97d":"#passengers with just 1 parents \/ children had over a 50% chance of survival\ntrain.groupby('Parch').Survived.value_counts()","311417e7":"#passengers with 1 to 3 parents \/ children had a 50% or above chance of survival\ntrain[['Parch', 'Survived']].groupby(['Parch'], as_index=False).mean()","dcb750bc":"#train.groupby('Parch').Survived.mean().plot(kind='bar')\n\n# ci=None will hide the error bar\n#decided to move it because it makes the barplot distracting\nsns.barplot(x='Parch', y='Survived', ci=None,data=train) ","895746a5":"#most did not have siblings\ntrain.SibSp.value_counts()","0a9bbec2":"train.groupby('SibSp').Survived.value_counts()","9887d447":"#higher survivial mean for passengers with just one sibling\/spouse \ntrain[['SibSp', 'Survived']].groupby(['SibSp'], as_index=False).mean()","691f07ff":"#train.groupby('SibSp').Survived.mean().plot(kind='bar')\nsns.barplot(x='SibSp', y='Survived', ci=None, data=train) # ci=None will hide the error bar","9c871710":"#use violin plot since Age is a continuous feature\n#1) Age on y-axis; 2) a. Embarked b. Pclass c. Sex on the x-axis per violin plot\n\nfig = plt.figure(figsize=(15,5))\nax1 = fig.add_subplot(131)\nax2 = fig.add_subplot(132)\nax3 = fig.add_subplot(133)\n\nsns.violinplot(x=\"Embarked\", y=\"Age\", hue=\"Survived\", data=train, split=True, ax=ax1)\nsns.violinplot(x=\"Pclass\", y=\"Age\", hue=\"Survived\", data=train, split=True, ax=ax2)\nsns.violinplot(x=\"Sex\", y=\"Age\", hue=\"Survived\", data=train, split=True, ax=ax3)","6e135440":"#shows negative and positive correlations\n#Correlations go from 1 to -1\n#positive numbers indicate a positive correlation among features \n#negative numbers indicate a negative correlation among features \n\n#dimension size for heat map\nplt.figure(figsize=(10,10))\n#make the heat map a squares and the heat spectrum vmax at 0.8\nsns.heatmap(train.drop('PassengerId',axis=1).corr(), vmax=0.8, square=True, annot=True)","e8a71f4c":"\n#merge the train and test data set\ncombined_data = [train, test] \n\n#create new column with name titles, such as Mr., Mrs., etc.\nfor dataset in combined_data:\n    dataset['NAME_TITLE'] = dataset.Name.str.extract(' ([A-Za-z]+)\\.')\n    \n    \n#replace uncommon name titles to a category named 'other'\nfor dataset in combined_data:\n    dataset['NAME_TITLE'] = dataset['NAME_TITLE'].replace(['Lady', 'Countess','Capt', 'Col', 'Don', 'Dr', 'Major', 'Rev', 'Sir', 'Jonkheer', 'Dona'], 'Other')\n\n    dataset['NAME_TITLE'] = dataset['NAME_TITLE'].replace('Mlle', 'Miss')\n    dataset['NAME_TITLE'] = dataset['NAME_TITLE'].replace('Ms', 'Miss')\n    dataset['NAME_TITLE'] = dataset['NAME_TITLE'].replace('Mme', 'Mrs')\n\n#show mean survival rate by name title \n#notice females ['Mrs.' and 'Miss'] have higher survival mean\ntrain[['NAME_TITLE', 'Survived']].groupby(['NAME_TITLE'], as_index=False).mean()","90952159":"#turn name titles into numerical values\nquant_title_name = {\"Mr\": 1, \"Miss\": 2, \"Mrs\": 3, \"Master\": 4, \"Other\": 5}\nfor dataset in combined_data:\n    dataset['NAME_TITLE'] = dataset['NAME_TITLE'].map(quant_title_name)\n    dataset['NAME_TITLE'] = dataset['NAME_TITLE'].fillna(0)\n    \n#column 'Name_Title' now has numerical values\ntrain.head()","c9eba2dc":"#males == 0\n#fmeales == 1\nfor dataset in combined_data:\n    dataset['Sex'] = dataset['Sex'].map( {'female': 1, 'male': 0} ).astype(int)","e8cf7100":"#notice the 'Sex' column \ntrain.head()","b1a7085b":"#check which category occurs more often\ntrain.Embarked.value_counts()","1648e880":"#since 'S' occcures more often, we will replace missing values with 'S'\nfor dataset in combined_data:\n    dataset['Embarked'] = dataset['Embarked'].fillna('S')\n    \ntrain.head()","573968f6":"#convert categories in 'Embarked' to numerical values\nfor dataset in combined_data:\n    dataset['Embarked'] = dataset['Embarked'].map( {'S': 0, 'C': 1, 'Q': 2} ).astype(int)\n    \ntrain.head()","a24e0df8":"#get the mean and standard deviation for age\n#count the number of null values in the age column \nfor dataset in combined_data:\n    age_avg = dataset['Age'].mean()\n    age_std = dataset['Age'].std()\n    null_age_count = dataset['Age'].isnull().sum()\n    \n    #add random numbers to nill_age_list that are the outcome of subtracting\/adding mean_age and std_age\n    null_age_count_list = np.random.randint(age_avg - age_std, age_avg + age_std, size=null_age_count)\n    dataset['Age'][np.isnan(dataset['Age'])] = null_age_count_list\n    dataset['Age'] = dataset['Age'].astype(int)\n    \n#create a new column AgeRange with \n#break it down into 5 age ranges \ntrain['AgeRange'] = pd.cut(train['Age'], 5)\n\n#see what age ranges were more likely to survive \nprint (train[['AgeRange', 'Survived']].groupby(['AgeRange'], as_index=False).mean())","409b1ab9":"#see new column AgeRange \ntrain.head()","179c2c65":"#break down Age column by AgeRange\nfor dataset in combined_data:\n    dataset.loc[ dataset['Age'] <= 16, 'Age'] = 0\n    dataset.loc[(dataset['Age'] > 16) & (dataset['Age'] <= 32), 'Age'] = 1\n    dataset.loc[(dataset['Age'] > 32) & (dataset['Age'] <= 48), 'Age'] = 2\n    dataset.loc[(dataset['Age'] > 48) & (dataset['Age'] <= 64), 'Age'] = 3\n    dataset.loc[ dataset['Age'] > 64, 'Age'] = 4","fe3711e6":"#age column now coded with numerical values \ntrain.head()","03f5ecbd":"#we chose median instead of mean b\/c of the huge std for Fare\nprint(train['Fare'].median())","eac80d6a":"#replace missing values with the median value \nfor dataset in combined_data:\n    dataset['Fare'] = dataset['Fare'].fillna(train['Fare'].median())","d9f338b2":"#create a Fare range\n#divided into four ranges\n#get mean values for each range\n\n#notice the higher the fare the more likely to survive\ntrain['FareRange'] = pd.qcut(train['Fare'], 4)\nprint (train[['FareRange', 'Survived']].groupby(['FareRange'], as_index=False).mean())","a653b23b":"#code Fare values by Fare ranges \nfor dataset in combined_data:\n    dataset.loc[ dataset['Fare'] <= 7.91, 'Fare'] = 0\n    dataset.loc[(dataset['Fare'] > 7.91) & (dataset['Fare'] <= 14.454), 'Fare'] = 1\n    dataset.loc[(dataset['Fare'] > 14.454) & (dataset['Fare'] <= 31), 'Fare']   = 2\n    dataset.loc[ dataset['Fare'] > 31, 'Fare'] = 3\n    dataset['Fare'] = dataset['Fare'].astype(int)","9d6abccd":"#Fare column now has numerical values for each range\ntrain.head()","d408cc3c":"#add values for SibSP and Parch and add 1 to account for lone tavelers\nfor dataset in combined_data:\n    dataset['Size_of_Family'] = dataset['SibSp'] +  dataset['Parch'] + 1\n\nprint (train[['Size_of_Family', 'Survived']].groupby(['Size_of_Family'], as_index=False).mean())","0c53c013":"#compare traveling alone vs traveling with a family \nfor dataset in combined_data:\n    dataset['Alone'] = 0\n    dataset.loc[dataset['Size_of_Family'] == 1, 'Alone'] = 1\n    \n#traveling alone == only 30 percent chance of survival \nprint (train[['Alone', 'Survived']].groupby(['Alone'], as_index=False).mean())","b08af4a8":"#drop passenger Id from training since we do not need it \n#but we kept for testing data, as it is required to submit it on Kaggle\ndrop_features = ['Name', 'SibSp', 'Parch', 'Ticket', 'Cabin', 'Size_of_Family']\ntrain = train.drop(drop_features, axis=1)\ntest = test.drop(drop_features, axis=1)\ntrain = train.drop(['PassengerId', 'AgeRange', 'FareRange'], axis=1)","9756560d":"#modified training data set\ntrain.head()","e2196d06":"#modified testing data set\ntest.head()","b7c06bbf":"X_train = train.drop('Survived', axis=1)\ny_train = train['Survived']\nX_test = test.drop(\"PassengerId\", axis=1).copy()\n\nX_train.shape, y_train.shape, X_test.shape","719adbf5":"# Importing Classifier Modules ()\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier","b3bd9a06":"clf = LogisticRegression()\nclf.fit(X_train, y_train)\ny_pred_log_reg = clf.predict(X_test)\nacc_log_reg = round( clf.score(X_train, y_train) * 100, 2)\n\nprint (str(acc_log_reg) + ' percent')","1d813643":"clf = DecisionTreeClassifier()\nclf.fit(X_train, y_train)\ny_pred_decision_tree = clf.predict(X_test)\nacc_decision_tree = round(clf.score(X_train, y_train) * 100, 2)\nprint (acc_decision_tree)","430e2e5c":"clf = RandomForestClassifier(n_estimators=100)\nclf.fit(X_train, y_train)\ny_pred_random_forest = clf.predict(X_test)\nacc_random_forest = round(clf.score(X_train, y_train) * 100, 2)\nprint (acc_random_forest)","fbb39bb8":"We did much better! However, Decision Trees are not the ideal algorithm, as they are susceptible to overfitting (when the model is unable to generalize effectively to new data). Thankfully, a more complicated algorith, Random Forests, takes care of this issue. ","ed282150":"The bar plot above indicates that first class passengers were more likely to survive than second or third class passengers.","f151a598":"### Relationship between Features and Survival\u00b6\n* In this section, we analyze relationship between different features with respect to Survival. We see how different feature values show different survival chance. We also plot different kinds of diagrams to visualize our data and findings.","c8804461":"From Pclass violinplot, we can see that:\n\n1st Pclass has very few children as compared to other two classes.\n\n1st Plcass has more old people as compared to other two classes.\n\nAlmost all children (between age 0 to 10) of 2nd Pclass survived.\n\nMost children of 3rd Pclass survived.\n\nYounger people of 1st Pclass survived as compared to its older people.\n\nFrom Sex violinplot, we can see that:\n\nMost male children (between age 0 to 14) survived.\n\nFemales with age between 18 to 40 have better survival chance.","0fb9c1b0":"### Name Feature\n","c349bd44":"To determine how many males and females there are in each Pclass. The bar diagram shows\nthat there are more males in the 3rd Pclass than females.","0c97d4be":"### Age vs. Survival\nAge = Age in years\t\n","869ee688":"### Selecting Relevant Features \n.We get rid of the features we do not need in our model.","6f2ae725":"The bar plot avobe indicates that female passengers were more likely to survive than male passengers.","687dc6e3":"### Sex Feature","b7fa6024":"There are missing entries for Age in Test dataset as well.\n\nOut of 418 rows in Test dataset, only 332 rows have Age value.\n\nCabin values are also missing in many rows. Only 91 rows out ot 418 have values for Cabin column.","f9f3ce72":"From the above plot, it can be seen that:\n\nAlmost all females from Pclass 1 and 2 survived from all ports of embarkment.\n\nFemales dying were mostly from 3rd Pclass, regardless of port of embarkment.\n\nMales from Pclass 1 only have slightly higher survival chance than Pclass 2 and 3, from all embarkments (slightly different from emarkemnt C, however).","0d8d40d8":"![Tiatnic!](https:\/\/wallpapertag.com\/wallpaper\/full\/d\/b\/8\/750236-titanic-sinking-wallpapers-2048x1536-phone.jpg)","011f0ddd":"We achived our goal of 80% or above accuracy! But let's see if we can do better with a different model.\n","1174d0e1":"![Tiatnic2!](https:\/\/kwmp.ca\/wp-content\/uploads\/2018\/04\/titanic-the-musical-1024x538.jpg)","5fecd60b":"In addition, the plot above shows that:\n\nWomen from 1st and 2nd Pclass have almost 100% survival chance.\nMen from 2nd and 3rd Pclass have only around 10% survival chance.","f5d2db18":"### Pclass vs. Survival\nHigher class passengers have better survival chance.","929acffa":"### Training our Classifier with Feature set","a0600fed":"### Parch vs. Survival\n\nParch = Number of parents \/ children aboard the Titanic\n","66dd4143":"![Tiatnic3!](https:\/\/titanicbelfast.com\/BlankSite\/media\/images\/Ship%20Fact%20Files\/1-Titanic-Stern-View.jpg?width=1100&height=745&ext=.jpg)","b7b7aa5e":"### Heatmap Correlations with the feature 'Survived'","ec1a0c79":"Survived column is not present in Test data. We have to train our classifier using the Train data and generate predictions (Survived) on Test data.","ca6ade67":"### Decision Trees\n\nDecision Trees are a sligtly more complicated classifier algortithm. It divides outcomes of features into a upside down tree. Each branch of tree is an outcome and each leaf is a class label. The overall path is basically the set of classification rules. ","36384b91":"We can see that Age value is missing for many rows.\n\nOut of 891 rows, the Age value is present only in 714 rows.\n\nSimilarly, Cabin values are also missing in many rows. Only 204 out of 891 rows have Cabin values.","a8cd8737":"### Random Forests \n\nRandom Forests is basically an algorithm that incorporates many decision trees. The process itself is complicated and beyond the scope of this project, however, as mentioned before, Random Forests alleviates the issue of overfitting present in Decision Trees. Thus, Random Forests is considered a superior algorithm to Decision Trees. ","c9e1e0d5":"### Class vs. Survival","56f2ccde":"### SibSp and Parch\ncombine these two columns to determine the size of the family ","2bb9ad36":"### Age ","f957348a":"This shows that there are duplicate Ticket number and Cabins shared. The highest number of duplicate ticket number is \"CA. 2343\". It has been repeated 7 times. Similarly, the highest number of people using the same cabin is 4. They are using cabin number \"C23 C25 C27\".\n\nWe also see that 644 people were embarked from port \"S\".\n\nAmong 891 rows, 577 were Male and the rest were Female.\n\nWe use info() method to see more information of our train dataset.","96f644ce":"### Fare ","1f8e8b42":"### Looking into the testing dataset\nTest data has 418 rows and 11 columns.\n\nTrain data rows = 891\n\nTest data rows = 418\n\nTotal rows = 891+418 = 1309\n\nWe can see that around 2\/3 of total data is set as Train data and around 1\/3 of total data is set as Test data.","cd3d079f":"### Import Classifier Shallow Machine Learning Algorithms\n\nProcedure: \n1. Train classifier with the train data set\n2. Use now trained classifier to predict the chances of survival in the test data set\n3. Determine how accurate the model was in predicting chances of survival ","46190cb4":"### Describing training dataset\n\ndescribe() method can show different values like count, mean, standard deviation, etc. of numeric data types.","476265bf":"Below is a brief information about each columns of the dataset:\n\nPassengerId: An unique index for passenger rows. It starts from 1 for first row and increments by 1 for every new rows.\n\nSurvived: Shows if the passenger survived or not. 1 stands for survived and 0 stands for not survived.\n\nPclass: Ticket class. 1 stands for First class ticket. 2 stands for Second class ticket. 3 stands for Third class ticket.\n\nName: Passenger's name. Name also contain title. \"Mr\" for man. \"Mrs\" for woman. \"Miss\" for girl. \"Master\" for boy.\n\nSex: Passenger's sex. It's either Male or Female.\n\nAge: Passenger's age. \"NaN\" values in this column indicates that the age of that particular passenger has not been recorded.\n\nSibSp: Number of siblings or spouses travelling with each passenger.\n\nParch: Number of parents of children travelling with each passenger.\nTicket: Ticket number.\nFare: How much money the passenger has paid for the travel journey.\nCabin: Cabin number of the passenger. \"NaN\" values in this column indicates that the cabin number of that particular passenger has not been recorded.\nEmbarked: Port from where the particular passenger was embarked\/boarded","501e28a7":"### Embarked Feature\n\nData for this feature has missing entries 'NaN'\nWe will convert these values into the most common embarked category","61d7fe0d":"### Sex vs. Survival","cf61cd04":"describe(include = ['O']) will show the descriptive statistics of object data types.\n\n","601bfe65":"### Extracting Relevant Features \nIn this section, we select the appropriate features to train our classifier. Here, we create new features based on existing features. We also convert categorical features into numeric form.","cbb83c2b":"### Pclass & Sex vs. Survival\n![](http:\/\/)","d064b24b":"### Logistic Regression \nLogistic Regression is one of the most commonly used classifiers in Machine Learning. It is one of the 'bread and butter' algortithms, along with linear regression. In this model, values range from '0' to '1', which classify the dependent variable (in this case, chance of survival) into an either 'survived\/did not survive' dichotomous decision. ","1681a732":"### Pclass, Sex & Embarked vs. Survival\n","0b9c5966":"### Conclusion \n\nRandom Forests gave us the same accuracy percentage as Decision Trees (86.98%). In addition, he Random Forest model was  superior to the Logistic Regression model, which gave as an accuracy of \"80.92%\". \n\nGiven our logic behind why Random Forests is a better option to Decision  Trees, and given that the Random Forest algorithm gave us a better accuracy rating than Logistic Rgeression, we choose Random Forest as our Machine Learning Algorithm for the Titanic Data Set!","9c082b8b":"### SibSp vs. Survival\n\nSibSp = Number of siblings \/ spouses aboard the Titanic\n","25303273":"# Titanic Machine Learning Project: Classifying Survival Likelihood Using the Passenger's Features ","abfa9cb3":"### Embarked vs. Survived\n","03258d4b":"### Total rows and columns\n\nWe can see that there are 891 rows and 12 columns in our training dataset."}}