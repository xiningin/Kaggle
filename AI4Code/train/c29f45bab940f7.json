{"cell_type":{"dfbdf6ed":"code","b83af442":"code","d6757937":"code","316e6257":"code","f553c984":"code","4e9a6432":"code","7eb72096":"code","13d8873c":"code","d5a290c2":"code","750da0ad":"code","725d236f":"code","293eff90":"code","9ea0caa4":"code","ae1a3170":"code","b4ccbf78":"code","237bcc5f":"code","c99c6e91":"code","26efc8f8":"code","53bd0f55":"code","e051f54d":"code","4bd3ff3c":"code","9b393021":"code","2f0bde99":"code","f9f03b21":"code","355e46dc":"code","504c7585":"code","0b242508":"code","6ff5b285":"code","17106b16":"code","4f4a8f55":"code","f1d13c33":"code","e6b76007":"code","0c009eb3":"code","a446ece5":"code","81d6d773":"code","9d981ef2":"code","76d1f121":"code","2a3238e6":"code","c3a186e8":"code","50c06a60":"code","898fb040":"code","85029625":"markdown","ba8a3133":"markdown","f7c35110":"markdown","de4add0c":"markdown","3001de16":"markdown"},"source":{"dfbdf6ed":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline","b83af442":"from sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import accuracy_score , confusion_matrix , classification_report\nfrom sklearn.neighbors import KNeighborsClassifier","d6757937":"#Import Ames House Dataset\ndf= pd.read_csv('\/kaggle\/input\/taitanictrain\/datasets_11657_16098_train.csv')","316e6257":"df.describe()","f553c984":"df.info()","4e9a6432":"df.head()","7eb72096":"sns.countplot(x='Survived',data=df)","13d8873c":"sns.countplot(x='Survived',hue='Sex',data=df,palette='winter')","d5a290c2":"sns.countplot(x='Survived',hue='Pclass',data=df)","750da0ad":"df['Age'].plot.hist()\n","725d236f":"sns.boxplot(data=df, x='Survived', y='Age')","293eff90":"sns.boxplot(data=df, x='Sex', y='Age')","9ea0caa4":"df['Fare'].plot.hist(bins=20,figsize=(10,5))","ae1a3170":"sns.countplot(x='SibSp',data=df)","b4ccbf78":"df['Parch'].plot.hist()","237bcc5f":"sns.countplot(x='Parch',data=df)","c99c6e91":"df.isnull().sum()","26efc8f8":"df.drop('Cabin',axis=1,inplace=True)","53bd0f55":"df.head()","e051f54d":"df.dropna(inplace=True)","4bd3ff3c":"df.shape","9b393021":"df.isnull().sum()","2f0bde99":"df['Sex'] = df['Sex'].map({'male':1,'female':0})\ndf.head()","f9f03b21":"df.drop(['PassengerId','Ticket','Embarked','Name'],axis=1,inplace=True)","355e46dc":"df.head()","504c7585":"df.info()","0b242508":"print('max_missing:', df.count().idxmin())","6ff5b285":"sns.heatmap(df.corr(), annot=True,cmap='RdYlGn')","17106b16":"sns.scatterplot(data=df, x='Age', y='Fare', hue='Survived')","4f4a8f55":"sns.countplot(data=df, x='Survived')","f1d13c33":"df['Survived'].value_counts()","e6b76007":"X= df.drop(['Survived'], axis=1)\ny=df['Survived']","0c009eb3":"X_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.3,random_state=42)","a446ece5":"scaler= StandardScaler()\nscaler.fit(X_train)","81d6d773":"X_train= scaler.transform(X_train)\nX_test= scaler.transform(X_test)","9d981ef2":"KNN=KNeighborsClassifier(n_neighbors=1)\nKNN.fit(X_train,y_train)","76d1f121":"y_pred=KNN.predict(X_test)\n#KNN.score(X_test,y_test)","2a3238e6":"pd.DataFrame({'Y_test': y_test , 'Y_pred':y_pred})","c3a186e8":"accuracy_score(y_test , y_pred)","50c06a60":"confusion_matrix(y_test, y_pred)","898fb040":"print(classification_report(y_test, y_pred))","85029625":"# Analysing data\nData analysis is a process of inspecting, cleansing, transforming, and modelling data with the goal of discovering useful information, informing conclusions, and supporting decision-making.[1] Data analysis has multiple facets and approaches, encompassing diverse techniques under a variety of names, and is used in different business, science, and social science domains.\nIn today's business world, data analysis plays a role in making decisions more scientific and helping businesses operate more effectively.\n\nData mining is a particular data analysis technique that focuses on statistical modelling and knowledge discovery for predictive rather than purely descriptive purposes, while business intelligence covers data analysis that relies heavily on aggregation, focusing mainly on business information.\nIn statistical applications, data analysis can be divided into descriptive statistics, exploratory data analysis (EDA), and confirmatory data analysis (CDA).\nEDA focuses on discovering new features in the data while CDA focuses on confirming or falsifying existing hypotheses.\nPredictive analytics focuses on the application of statistical models for predictive forecasting or classification, while text analytics applies statistical, linguistic, and structural techniques to extract and classify information from textual sources, a species of unstructured data. All of the above are varieties of data analysis.","ba8a3133":"# Data Cleaning\nOnce processed and organized, the data may be incomplete, contain duplicates, or contain errors.\nThe need for data cleaning will arise from problems in the way that the datum are entered and stored.\nData cleaning is the process of preventing and correcting these errors. Common tasks include record matching, identifying inaccuracy of data, overall quality of existing data, deduplication, and column segmentation.\nSuch data problems can also be identified through a variety of analytical techniques. For example; with financial information, the totals for particular variables may be compared against separately published numbers that are believed to be reliable.\nUnusual amounts, above or below predetermined thresholds, may also be reviewed. There are several types of data cleaning, that are dependent upon the type of data in the set; this could be phone numbers, email addresses, employers, or other values.\nQuantitative data methods for outlier detection, can be used to get rid of data that appears to have a higher likelihood of being input incorrectly.\nTextual data spell checkers can be used to lessen the amount of mis-typed words. However, it is harder to tell if the words themselves are correct.[","f7c35110":"# Predicting Test Data","de4add0c":"Accuracy of the model with K = 1 is 75 %. Compared to logistic regression, which gave us 79% accuracy, the KNN has an error of about 4%.\n\nSo It's more valid model than Logistic regression.","3001de16":"# k-nearest neighbors algorithm\nn statistics, the k-nearest neighbors algorithm (k-NN) is a non-parametric classification method first developed by Evelyn Fix and Joseph Hodges in 1951, and later expanded by Thomas Cover. It is used for classification and regression. In both cases, the input consists of the k closest training examples in data set. The output depends on whether k-NN is used for classification or regression:\n\nIn k-NN classification, the output is a class membership. An object is classified by a plurality vote of its neighbors, with the object being assigned to the class most common among its k nearest neighbors (k is a positive integer, typically small). If k = 1, then the object is simply assigned to the class of that single nearest neighbor.\nIn k-NN regression, the output is the property value for the object. This value is the average of the values of k nearest neighbors.\nk-NN is a type of classification where the function is only approximated locally and all computation is deferred until function evaluation. Since this algorithm relies on distance for classification, if the features represent different physical units or come in vastly different scales then normalizing the training data can improve its accuracy dramatically.\n\nBoth for classification and regression, a useful technique can be to assign weights to the contributions of the neighbors, so that the nearer neighbors contribute more to the average than the more distant ones. For example, a common weighting scheme consists in giving each neighbor a weight of 1\/d, where d is the distance to the neighbor.\n\nThe neighbors are taken from a set of objects for which the class (for k-NN classification) or the object property value (for k-NN regression) is known. This can be thought of as the training set for the algorithm, though no explicit training step is required.\n\nk-NN is a special case of a variable-bandwidth, kernel density \"balloon\" estimator with a uniform kernel.\n\nThe naive version of the algorithm is easy to implement by computing the distances from the test example to all stored examples, but it is computationally intensive for large training sets. Using an approximate nearest neighbor search algorithm makes k-NN computationally tractable even for large data sets. Many nearest neighbor search algorithms have been proposed over the years; these generally seek to reduce the number of distance evaluations actually performed.\n\nk-NN has some strong consistency results. As the amount of data approaches infinity, the two-class k-NN algorithm is guaranteed to yield an error rate no worse than twice the Bayes error rate (the minimum achievable error rate given the distribution of the data).Various improvements to the k-NN speed are possible by using proximity graphs."}}