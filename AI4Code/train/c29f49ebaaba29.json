{"cell_type":{"4940e403":"code","1b52f831":"code","69c8e886":"code","494a37f5":"code","6e19e2a2":"code","9e5989e6":"code","c50a4649":"code","144c9151":"code","98b91659":"code","38b1cc77":"code","54a2ff88":"code","324ef2c3":"markdown","9aca7bdb":"markdown","aba6a4e5":"markdown","dc35ec15":"markdown","d32299b0":"markdown","907d6071":"markdown","c33738bc":"markdown","4ee01798":"markdown","5de617ff":"markdown","22a9b71f":"markdown","e410b487":"markdown"},"source":{"4940e403":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport networkx as nx\nimport sklearn as sk\nfrom scipy import linalg\nfrom sklearn.cluster import KMeans","1b52f831":"n = 150\np_in = 0.9\np_out = 0.05\nsizes_assort = [n,n,n]\nprobs_assort = [[p_in,p_out,p_out],[p_out,p_in,p_out],[p_out,p_out,p_in]]\n\nsizes_disassort =[n,n]\nprobs_disassort = [[p_out,p_in],[p_in,p_out]]\n\np_core = 0.9\np_periph = 0.1\np_low = 0.001\n\nsizes_cperiph = [2*n,n]\nprobs_cperiph = [[p_core,p_periph],[p_periph,p_low]]\n\nG_disassort = nx.generators.community.stochastic_block_model(sizes_disassort,probs_disassort)\nA_disassort = nx.linalg.graphmatrix.adjacency_matrix(G_disassort)\n\nG_assort = nx.generators.community.stochastic_block_model(sizes_assort,probs_assort)\nA_assort = nx.linalg.graphmatrix.adjacency_matrix(G_assort)\n\nG_cperiph = nx.generators.community.stochastic_block_model(sizes_cperiph,probs_cperiph)\nA_cperiph = nx.linalg.graphmatrix.adjacency_matrix(G_cperiph)\nfig, axs = plt.subplots(1,3,figsize = (16,9))\naxs[0].spy(A_assort,markersize = 0.3,color = 'green')\naxs[1].spy(A_disassort, markersize = 0.3, color = 'red')\naxs[2].spy(A_cperiph,markersize = 0.3, color = 'blue')\n\naxs[0].title.set_text('Adjacency matrix for 3 assortative communities')\naxs[1].title.set_text('Adjacency matrix for 2 disassortative communities')\naxs[2].title.set_text('Adjacency matrix for core-periphery structure')","69c8e886":"'''Function for solving the quadratic eigenvalue problem listed at the end of the section on the \nnon-backtracking matrix. This is so we can calculate $\\sqrt{\\rho(B)}$. This code was adapted from https:\/\/stackoverflow.com\/questions\/8252428\/how-to-solve-the-polynomial-eigenvalue-in-python '''\ndef quadeig(A):\n    \n    #Forming diagonal degree matrix and Identity matrix.\n    n = len(A)\n    D = np.diagflat(A.sum(axis =0))\n    D =np.asmatrix(D)\n    zero_n = np.zeros((n,n))\n    I_n = np.eye(n)\n    \n    # Assemble matrices for 2n x2n eigenvalue problem\n    C = np.block([\n        [A, D-I_n],\n        [-I_n, zero_n]\n        ])\n    # Solve eigenvalue problem\n    e, X = linalg.eig(C);\n    if np.all(np.isreal(e)):\n        e=np.real(e)\n    X=X[n:,:]\n\n    # Sort eigenvalues\/vectors\n    #I = np.argsort(e)\n    #X = X[:,I]\n    #e = e[I]\n\n    # Scaling each mode by max\n    X \/= np.tile(np.max(np.abs(X),axis=0), (n,1))\n\n    return X, e","494a37f5":"#Our function that takes a network as input and provides the partitioning according to the Bethe Hessian spectral\n#clustering.\n\ndef BetheHess(G):\n    A = nx.adjacency_matrix(G)\n    A = A.todense()\n    X,e = quadeig(A)\n    specB = max(abs(e))\n    Rc = np.sqrt(specB)\n    BHassort = nx.linalg.bethehessianmatrix.bethe_hessian_matrix(G, r=Rc) #We assume assortative communities here.\n    BHassort = BHassort.todense()\n    eigvals_assort, eigvects_assort = np.linalg.eig(BHassort) # eigvects[:,i] is the eigenvector corresponding to the eigenvalue eigvals[i]\n    assort_count = np.sum(eigvals_assort<0)\n    indices_assort = eigvals_assort.argsort()[:assort_count]\n    W_assort = eigvects_assort[:,indices_assort]\n\n    BHdisassort = nx.linalg.bethehessianmatrix.bethe_hessian_matrix(G, r=-Rc) #We assume disassortative communities here.\n    BHdisassort = BHdisassort.todense()\n    eigvals_disassort, eigvects_disassort = np.linalg.eig(BHdisassort) # eigvects[:,i] is the eigenvector corresponding to the eigenvalue eigvals[i]\n    disassort_count = np.sum(eigvals_disassort<0)\n    indices_disassort = eigvals_disassort.argsort()[:disassort_count]\n    W_disassort = eigvects_disassort[:,indices_disassort]\n\n    W = np.hstack((W_assort,W_disassort)) #Joining the necessary eigenvectors of H to then use in K means. \n\n    kmeans = KMeans(n_clusters=assort_count+disassort_count).fit(W) # kmeans\n\n    #form partitions\n    partition = kmeans.labels_\n    #We return the partitioning as the output.\n    return partition","6e19e2a2":"def create_corr_network(G, corr_direction, min_correlation):\n    ##Creates an empty copy of the graph\n    H = nx.create_empty_copy(G)\n    \n    ##Checks all the edges and removes some based on corr_direction\n    for stock1, stock2, weight in G.edges(data=True):\n        ##if we only want to see the positive correlations      \n        if corr_direction == \"positive\":\n            if weight[\"weight\"] <0 or weight[\"weight\"] < min_correlation:\n                continue\n            elif stock1 == stock2:\n                continue\n            else:\n                H.add_edge(stock1,stock2)\n                    \n                 \n        ##If we'd like to create links that are based on negative OR positive correlations surpassing a threshold. \n        if corr_direction == \"both\":\n            if weight[\"weight\"] > -min_correlation and weight[\"weight\"] < min_correlation:\n                continue\n            elif stock1 == stock2:\n                continue\n            else:\n                H.add_edge(stock1,stock2)\n                    \n        ##If we'd like to create a network of links based on negative correlations surpassing a threshold.\n        else:\n\n            if weight[\"weight\"] >=0 or weight[\"weight\"] > min_correlation:\n                continue\n            elif stock1 == stock2:\n                continue\n            else:\n                H.add_edge(stock1,stock2)\n                    \n                \n    return H\n","9e5989e6":"###This is the code we used to produce the networks in section 3. The application to finance.\n#reads the csv\nstocks = pd.read_csv('..\/input\/techvsoil\/techvsoil.csv')\ncor_matrix = stocks.iloc[:,:].corr()\nstocks = cor_matrix.index.values\ncor_matrix = np.asmatrix(cor_matrix)\nG = nx.from_numpy_matrix(cor_matrix)\nG = nx.relabel_nodes(G,lambda x: stocks[x])\nG.edges(data=True)\nH = create_corr_network(G,'positive',0.8)\n\nH.remove_nodes_from(list(nx.isolates(H)))\n\n#Plotting H to see how it appears:\nd = nx.degree(H)\nnodelist, node_sizes = zip(*d)\n\npositions=nx.circular_layout(H)\n\nplt.figure(figsize=(10,10))\n\nnx.draw_networkx_nodes(H,positions,node_color='#DA70D6',nodelist=nodelist,\n                       node_size=tuple([x**3 for x in node_sizes]),alpha=0.8)\nnx.draw_networkx_labels(H, positions, font_size=14, \n                        font_family='sans-serif')\nnx.draw_networkx_edges(H, positions)\nplt.axis('off')\nplt.show()","c50a4649":"#We remove the disconnected component for some clarity:\nH.remove_node('DHT')\nH.remove_node('DSSI')\nH.remove_node('FRO')","144c9151":"### Displaying the communities found by the Bethe Hessian.\nfig = plt.figure(figsize = (16,9))\n\n\nax1 = fig.add_subplot(121)\nax1.title.set_text('Bethe Hessian')\npartition = BetheHess(H)\npos = nx.spring_layout(H) \nnx.draw_networkx_nodes(H, pos, node_size=100, cmap=plt.cm.RdYlBu, node_color=list(partition))\nnx.draw_networkx_edges(H, pos, alpha=0.3)\nnx.draw_networkx_labels(H, pos, font_size=14, font_family='sans-serif')\n\n\nK=3  #Change K manually to determine the number of communities to search for. \n\n\n#form partitions\n\n\n # compute graph layout\nax2 = fig.add_subplot(122)\nax2.title.set_text('Normalized Laplacian')\nnL = nx.linalg.normalized_laplacian_matrix(H)\nnL = nL.todense()\neigvals, eigvects = np.linalg.eig(nL) # eigvects[:,i] is the eigenvector corresponding to the eigenvalue eigvals[i]\nindices = eigvals.argsort()[:K]\nW = eigvects[:,indices]\nkmeans = KMeans(n_clusters=K).fit(W) # kmeans\npartition = kmeans.labels_\npos = nx.spring_layout(H) \nnx.draw_networkx_nodes(H, pos, node_size=100, cmap=plt.cm.RdYlBu, node_color=list(partition))\nnx.draw_networkx_edges(H, pos, alpha=0.3)\nnx.draw_networkx_labels(H, pos, font_size=14, font_family='sans-serif')","98b91659":"plt.scatter(range(len(eigvals)),sorted(list(eigvals)))\nplt.xlabel('sorted position')\nplt.ylabel('eigenvalue')","38b1cc77":"###This is the code we used to produce the networks in section 3. The application to finance.\n#reads the csv\nstocks = pd.read_csv('..\/input\/techvsoil\/techvsoil.csv')\ncor_matrix = stocks.iloc[:,:].corr()\nstocks = cor_matrix.index.values\ncor_matrix = np.asmatrix(cor_matrix)\nG = nx.from_numpy_matrix(cor_matrix)\nG = nx.relabel_nodes(G,lambda x: stocks[x])\nG.edges(data=True)\nH = create_corr_network(G,'negative',-0.3)\n\nH.remove_nodes_from(list(nx.isolates(H)))\n\n#Plotting H to see how it appears:\nd = nx.degree(H)\nnodelist, node_sizes = zip(*d)\n\npositions=nx.circular_layout(H)\n\nplt.figure(figsize=(10,10))\n\nnx.draw_networkx_nodes(H,positions,node_color='#DA70D6',nodelist=nodelist,\n                       node_size=tuple([x**3 for x in node_sizes]),alpha=0.8)\nnx.draw_networkx_labels(H, positions, font_size=14, \n                        font_family='sans-serif')\nnx.draw_networkx_edges(H, positions)\nplt.axis('off')\nplt.show()\n","54a2ff88":"### Displaying the communities found by the Bethe Hessian.\nfig = plt.figure(figsize = (16,9))\n\n\nax1 = fig.add_subplot(121)\nax1.title.set_text('Bethe Hessian')\npartition = BetheHess(H)\npos = nx.spring_layout(H) \nnx.draw_networkx_nodes(H, pos, node_size=100, cmap=plt.cm.RdYlBu, node_color=list(partition))\nnx.draw_networkx_edges(H, pos, alpha=0.3)\nnx.draw_networkx_labels(H, pos, font_size=14, font_family='sans-serif')\n\n\nK=2  #Change K manually to determine the number of communities to search for. \n\n\n#form partitions\n\n\n # compute graph layout\nax2 = fig.add_subplot(122)\nax2.title.set_text('Normalized Laplacian')\nnL = nx.linalg.normalized_laplacian_matrix(H)\nnL = nL.todense()\neigvals, eigvects = np.linalg.eig(nL) # eigvects[:,i] is the eigenvector corresponding to the eigenvalue eigvals[i]\nindices = eigvals.argsort()[:K]\nW = eigvects[:,indices]\nW = np.real(W) #we have to take real parts here since there are some numerical errors which lead to our values + 0.0j which cause an error\nkmeans = KMeans(n_clusters=K).fit(W) # kmeans\npartition = kmeans.labels_\npos = nx.spring_layout(H) \nnx.draw_networkx_nodes(H, pos, node_size=100, cmap=plt.cm.RdYlBu, node_color=list(partition))\nnx.draw_networkx_edges(H, pos, alpha=0.3)\nnx.draw_networkx_labels(H, pos, font_size=14, font_family='sans-serif')","324ef2c3":"Whether or not you agree with these community choices is subjective. I am not able to come up with a perfect explanation for these except that a lot of the tech companies that were in the third community with some of the oil companies are hardware based so their correlations may be a consequence of shipping of these companies requiring shipping.\n\nLets see what happens in the case of a disassortative community. I will now rerun the same code but where we search for negative correlations and the threshold will be -0.3 (The negative correlations were a lot weaker so I had to reduce the threshold to get a network that wasn't crazily sparse).","9aca7bdb":"As one can see, these results are identical (although the spring layout is random so it takes a moment to realise), the normalized Laplacian has done just as well. However, we had to state that K = 3 in the case of the normalized Laplacian which was not immediately obvious from the look of the graph. Nor was it particularly obvious from the eigenvalues of normalized Laplacian. Usually to determine $K$ we search through the eigenvalues of the normalized Laplacian and count the smallest eigenvalues before a jump. See below a plot of the eigenvalues in the case of this network. As you can see, there seems to be two jumps rather than just one so it is hard to say how many communities there should be!","aba6a4e5":"There appears to be two communities (after removing the smaller disconnected component of DHT,FRO and DSSI) from the visual although Bethe Hessian spectral clustering will detect 3. There is a third weaker one among some tech and oil companies. ","dc35ec15":"## Bethe Hessian Matrix\n\nWe have that the potentially informative eigenvalues $\\mu$ of the non-backtracking matrix are given by solving :\n$$ \\det ((\\mu^2 - 1)I - \\mu A + D) = 0$$\n\nThe matrix $H(r) := ((r^2 - 1)I - r A + D)$ is called the Bethe Hessian matrix. For large enough $r$, $H(r)$ for a given network with adjacency matrix $A$ and diagonal degree matrix $D$ will be positive definite. This is because of Gershgorin's circle theorem which states that the eigenvalues of any complex matrix $C$ lie in the union of complex circles with centres given by the diagonal elements of $C_{ii}$ and corresponding radii $\\sum_{j \\neq i} C_{ij}$. Thus in the case of $H(r)$, as $r$ is increased, the diagonal elements grow with order $r^2$ and the off-diagonal elements with order $r$. $H(r)$ is also symmetric so all its eigenvalues are real. Thus it follows that for large enough r, we obtain eigenvalues in a union of intervals that all lie contained in the positive real numbers.\nIf we start with large enough positive $r$ where $H(r)$ is positive definite then as $r$ decreases, $H(r)$ gains a negative eigenvalue exactly when $det(H(r))$ crosses the r-axis. But this is of course when $r$ has dropped past an eigenvalue of the non-backtracking matrix $B$. Therefore, the negative eigenvalues of $H(r), r > 0$ correspond to the real positive eigenvalues of the non-backtracking matrix $B$ that are greater than $r$. Noting that the diagonal term of $H(r)$ is invariant to the sign of $r$, the same idea applies where we take a large negative value of $r$ where guaranteeing $H(r)$ is positive definite by Gershgorin's circle Theorem and then consider increasing it until $H(r)$ has a negative eigenvalue. It follows that negative eigenvalues of $H(r)$ for negative $r$ correspond to negative real eigenvalues of $B$ less than $r$. \n\\newline\nNow as discussed previously, the number of real eigenvalues of $B$ outside the complex circle of radius $\\sqrt{\\rho(B)}$ is the number of communities, so $r = \\pm \\sqrt{\\rho\\left(B\\right)}$ is a good choice of substitution into $H(r)$ . The number of negative eigenvalues of $H(-\\sqrt{\\rho(B)})$ are the number of disassortative communities and in a similar fashion, $H(\\sqrt{\\rho(B)})$ gives the quantity of assortative communities. So the Bethe Hessian inherits $B$'s ability to detect communities right down to the threshold.It has been argued that the eigenvectors of $H(\\pm \\sqrt{\\rho(B)})$ provide 'the direction of the clusters'. In other words, they are highly correlated with the block assignments of the nodes!","d32299b0":"Clearly from the visual here, there is a disassortative community comprising of at least the companies DSSI, FRO and DHT (All oil tanker companies).","907d6071":"As one can see, the Bethe Hessian approach detects this disassortative community whereas normalized Laplacian fails to (it's not designed to find it). \n\n# Conclusions\nBethe Hessian spectral clustering is great at finding both assortative and disassortative communities. It is, in theory, very computationally expensive but thanks to Ihara's Theorem, we can reduce the size of the matrix that we need obtain the eigenvalues of. It also solves the issue of knowing how many communities to search for which up until its discovery, had been an unsolved issue in Networks (to my knowledge). \nThere are a lot of details that I have left out but would be willing to fill out if requested.","c33738bc":"# An example application of Bethe-Hessian Spectral Clustering\n\nI have taken a collection of technology and oil company stock prices then used the correlation matrix as the adjacency matrix. I applied a threshold rounding up positive correlations to produce my network of the companies. It's important to note that the Bethe Hessian does generalize to a weighted network:\n\n\\begin{equation}\n    H_{ij}(r) = \\delta_{ij}(1 + \\sum_{k \\in \\delta_i} \\frac{A_{ik}^2}{r^2 - A_{ik}^2}) - \\frac{rA_{ij}}{r^2 - A_{ij}^2}\n\\end{equation}\n\nWhere $\\delta_i$ is the set of neighbour nodes to node $i$. However, I won't use this form for now as the unweighted version works well enough.","4ee01798":"# Bethe Hessian Spectral clustering\nKaggle has a bustling and very active community of fantastic data scientists so I'm sure someone has covered this somewhere but I couldn't find it so I wanted to put this notebook together to bring it more to light.\n## Assortative and Disassortative communities\nCommunity detection within networks is a large part of on-going research in the applied field of networks. A community is typically defined as a grouping of nodes which has a higher density of connections between the nodes within the grouping than the density of out-going connections to nodes outside of the grouping. In simpler terms, it is like a clique within the network. Nodes have a lot of connections between each other within the clique and less to nodes that are not part of their clique. These types of communities are referred to as assortative.\n\nHowever, this definition, as relaxed as it is, does not encompass some newer definitions of what could also be described as a community. For example, we could have a group of nodes that all play the same 'role' in a network and have lots of connections to other nodes outside of their role but not between each other. They are a community but would not be detected by community-detection algorithms that only search for assortative communities. These communities are termed disassortative. \n\nWe also could have other community-like structure such as core-periphery structure which, is in loose terms, where we have a large portion of the nodes densely connected to each other (the core) but with a fair amount of out-going links to outside nodes (the periphery).\n\nThe umbrella term for these different types of communities is 'block-model' likely because of the heat-map generated by their adjacency matrices (possibly after applying a similiarity transformation permuting the rows and columns). ","5de617ff":"# How many communities?\nWith older (yet still brilliant) and newer methods for detecting communities with networks, one of the biggest issues faced is determining how many communities to actually search for! This often has to be given as a value which we shall denote by $K$ beforehand.\n## The Non-backtracking matrix\nLet $G$ be an undirected, connected network with $n$ nodes and $m$ edges. Let the adjacency matrix be denoted by $A$ and the diagonal matrix of the degrees of each node be given by $D$. Then non-backtracking matrix of $G$ is defined as $B \\in \\mathbb{R}^{2m \\times 2m}$ where:\n\n$$ B((u,v),(x,y)) = \\begin{array}{l l} \n1 & \\mbox{ if } v = x, \\\\\nu \\neq y & 0 \\mbox{ otherwise }\\end{array} $$\n\n\nWhere $(u,v)$ and $(x,y)$ are 'directed' edges of $G$. $G$ does not need to be a directed network, we just assign direction both ways to each edge so that $(u,v)$ is distinct from $(v,u)$ for nodes $u$ and $v$.\n\nIt has been shown that studying the spectral properties of $B$ can aid community detection. Indeed, its eigenvalues are far more well-behaved than those of the adjacency matrix with B having $2(m-n)$ eigenvalues that are $\\pm 1$ and the rest are given as the eigenvalues of a matrix $W$:\n\n\\begin{equation} W = \\left[ \\begin{array}{c c}\n                                A & D - I \\\\\n                                -I & 0            \\end{array} \\right] \\end{equation}\n                                \nThis is Ihara's Theorem. I have a great outline of this proof but will hold it for now so as not to overwhelm with the nitty gritty details.\n\nSuppose that an eigenvector of W is $\\left[ x , y \\right]^T$, $x,y \\in \\mathbb{R}^n$ with corresponding eigenvalue $\\mu$. Then, \n\n\\begin{equation} \\left[ \\begin{array}{c c}\n                                A & D - I \\\\\n                                -I & 0      \\end{array} \\right] \n \\left[   \\begin{array}{c}x \\\\ y    \\end{array} \\right] = \\left[ \\begin{array}{c}\n     \\mu x  \\\\ \\mu y\n \\end{array} \\right]\n \\label{eigenval B}\n                        \\end{equation}\n    Which implies that $x = -\\mu y$ and so consequently, every eigenpair of W is determined by the equation induced by the top row block:\n    \n$$-\\mu Ay + (D-I)y = -\\mu^2y$$.\n\n## The non-backtracking matrix's use in detecting block structure\n\nA good question to ask at this point is 'why is the non-backtracking matrix important in detecting community structure?' There are already plenty of spectral clustering methods that rely on different matrices such as the adjacency matrix, the Laplacian, the normalised Laplacian or the modularity matrix (although the last two are almost identical). These methods rely on the matrix's eigenvalues showing a clear distinction in how to determine the number of communities $K$. For the (normalized) Laplacian, the number of communities is often intuited by the smallest $K$ eigenvalues before there is a 'spectral gap'. In the case of a densely populated network with clear block structure this would be fine, but in the case of sparsely populated networks,these spectral methods tend to struggle. It has been theorised that there is an asymptotic limit for which communities can be detected from a blockmodel with equal sized communities and equal probabilities.\n\nSpectral methods exploiting the spectrum of the adjacency matrix, the (normalized) Laplacian or the modularity matrix fail to detect the correct number of communities before this limit is reached. This is therefore an issue for detecting block structure in sparse networks since they are more likely to have 'weaker', less clear structure with the out-link and in-link probability of communities closer in value. It has been demonstrated that the non-backtracking matrix has more clearly defined differences in it's eigenvalues corresponding to communities and it's uninformative excess eigenvalues.\n\nIt has been noted that the eigenvalues of the non-backtracking matrix B are mostly contained within the complex circle of radius $\\sqrt{(\\rho(B))}$ and centre at the origin with $\\rho(B)$ being the spectrum of $B$. The real eigenvalues that stray far from this circle are the number of communities in the network with negative real eigenvalues outside the circle corresponding to disassortative communities and positive real eigenvalues to assortative communities.","22a9b71f":"We now define a function to form our network based on the correlation network. The data I have collected is from the closing prices on Yahoo finance from 23rd March 2020 - 23rd March 2021 with null values removed.","e410b487":"Once we obtain the $K$ dominant eigenvectors of $H(\\sqrt{\\rho(B)})$, we used them lined up alongside each other as the coordinates of our nodes in $K$-dimensional euclidean space. We apply $K$-means to determine the $K$ clusters that will be our communities (We could use other clustering algorithms but K-means works well!)."}}