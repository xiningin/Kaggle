{"cell_type":{"c0efbb99":"code","34385c1b":"code","afcbf57e":"code","4159d041":"code","825aa4f9":"code","25030dc9":"code","c050b6e0":"code","46230e8b":"code","b4fa3b99":"code","f3ccc049":"code","cdc1959e":"code","28b504ca":"code","3ebeed71":"code","530e5baa":"code","1fe94198":"code","bac73d1d":"code","133add32":"code","f8dcc321":"code","48a6ddc8":"code","84db1efa":"code","cc0c0c68":"code","e0e2ddac":"code","cef6e476":"code","dd9893b7":"code","c9cfcb7f":"code","98e3a35d":"code","4a2d0921":"code","eabe25a6":"code","21192984":"code","6cc22740":"code","4e5d0d74":"markdown","4be75384":"markdown","feb863b0":"markdown","4e6ed441":"markdown","e8fd7192":"markdown","ca5bfecc":"markdown","7a19d1cc":"markdown","ef6ec2c8":"markdown","17a9c80b":"markdown","65fb87b5":"markdown","4c5138be":"markdown","1d862a9f":"markdown","ae7f2a8d":"markdown","a6aacfa8":"markdown","22bc4a1b":"markdown","470a31af":"markdown","ba0d2480":"markdown","aa067c01":"markdown","45c167c9":"markdown","d9f7a889":"markdown","c3bd0869":"markdown","34a58ca8":"markdown","6db75c03":"markdown","8d41541c":"markdown","88843b03":"markdown","3a102915":"markdown","39ecff8f":"markdown","3527e237":"markdown","731690ce":"markdown","40e388d6":"markdown","ab9d5e92":"markdown","989eace7":"markdown","5d5930ea":"markdown","26658da4":"markdown","c0ca4277":"markdown","ded8f1b7":"markdown","c10fe9f4":"markdown"},"source":{"c0efbb99":"#%% Imports\nimport numpy as np\nimport pandas as pd\nimport plotly.express as px\nimport matplotlib.pyplot as plt\nimport lightgbm as lgb\nimport shap\n%matplotlib inline\n\nfrom pprint import pprint\nfrom IPython.display import display \nfrom sklearn import model_selection\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import plot_confusion_matrix\nfrom sklearn.metrics import precision_recall_fscore_support\nfrom sklearn.metrics import classification_report, confusion_matrix, accuracy_score, recall_score, roc_auc_score","34385c1b":"#%% Read aug_train.csv\naug_train = pd.read_csv('..\/input\/hr-analytics-job-change-of-data-scientists\/aug_train.csv')\n#%% Initial Glance at Data\ndisplay(aug_train.info(verbose = True,null_counts=True))\nprint(aug_train.shape)","afcbf57e":"#%% count total number of unique values in enrollee_id column\nprint('Number of Unique Values: ' + str(aug_train['enrollee_id'].nunique()))","4159d041":"print('Number of Unique Values: ' + str(aug_train['city'].nunique()))\nprint('Number of NaN Values: ' + str(sum(aug_train['city'].isnull())))\n# top 10 cities \nprint((aug_train['city'].value_counts()[0:10]))","825aa4f9":"print(\"Number of Missing Values: \", aug_train['city_development_index'].isna().sum())\ndisplay(aug_train['city_development_index'].describe())\nboxplot = aug_train.boxplot(column ='city_development_index')","25030dc9":"print(\"Number of Missing Values: \", aug_train['gender'].isna().sum())\nfig = px.pie(aug_train['gender'].value_counts(), values='gender', names = aug_train['gender'].value_counts().index,title = 'gender',template='ggplot2')\nfig.show()","c050b6e0":"print(\"Number of Missing Values: \", aug_train['relevent_experience'].isna().sum())\nfig = px.pie(aug_train['relevent_experience'].value_counts(), values='relevent_experience', \n             names = aug_train['relevent_experience'].value_counts().index,title = 'relevent_experience',template='ggplot2')\nfig.show()","46230e8b":"print(\"Number of Missing Values: \", aug_train['education_level'].isna().sum())\nfig = px.pie(aug_train['education_level'].value_counts(), values='education_level', \n             names = aug_train['education_level'].value_counts().index,title = 'education_level',template='ggplot2')\nfig.show()","b4fa3b99":"print(\"Number of Missing Values: \", aug_train['major_discipline'].isna().sum())\nfig = px.pie(aug_train['major_discipline'].value_counts(), values='major_discipline', \n             names = aug_train['major_discipline'].value_counts().index,title = 'major_discipline',template='ggplot2')\nfig.show()","f3ccc049":"print(\"Number of Missing Values: \", aug_train['experience'].isna().sum())\nfig = px.pie(aug_train['experience'].value_counts(), values='experience', \n             names = aug_train['experience'].value_counts().index,title = 'experience',template='ggplot2')\nfig.show()","cdc1959e":"print(\"Number of Missing Values: \", aug_train['company_size'].isna().sum())\nfig = px.pie(aug_train['company_size'].value_counts(), values='company_size', \n             names = aug_train['company_size'].value_counts().index,title = 'company_size',template='ggplot2')\nfig.show()","28b504ca":"print(\"Number of Missing Values: \", aug_train['company_type'].isna().sum())\nfig = px.pie(aug_train['company_type'].value_counts(), values='company_type', \n             names = aug_train['company_type'].value_counts().index,title = 'company_type',template='ggplot2')\nfig.show()","3ebeed71":"print(\"Number of Missing Values: \", aug_train['last_new_job'].isna().sum())\nfig = px.pie(aug_train['last_new_job'].value_counts(), values='last_new_job', \n             names = aug_train['last_new_job'].value_counts().index,title = 'last_new_job',template='ggplot2')\nfig.show()","530e5baa":"print(\"Number of Missing Values: \", aug_train['training_hours'].isna().sum())\ndisplay(aug_train['training_hours'].describe())\naug_train.boxplot(column ='training_hours')","1fe94198":"print(\"Number of Missing Values: \", aug_train['target'].isna().sum())\nfig = px.pie(aug_train['target'].value_counts(), values='target', \n             names = aug_train['target'].value_counts().index,title = 'target',template='ggplot2')\nfig.show()","bac73d1d":"#%% Read aug_test.csv\naug_test = pd.read_csv('..\/input\/hr-analytics-job-change-of-data-scientists\/aug_test.csv')\n#%% Initial Glance at Data\ndisplay(aug_test.info(verbose = True,null_counts=True))\nprint(aug_test.shape)","133add32":"#%% Prepare Data for LightGBM\n\n# Seperate aug_train into target and features \ny = aug_train['target']\nX_aug_train = aug_train.drop('target',axis = 'columns')\n# save the index for X_aug_train \nX_aug_train_index = X_aug_train.index.to_list()\n\n# row bind aug_train features with aug_test features \n# this makes it easier to apply label encoding onto the entire dataset \nX_aug_total = X_aug_train.append(aug_test,ignore_index = True)\ndisplay(X_aug_total.info(verbose = True,null_counts=True))\n\n# save the index for X_aug_test \nX_aug_test_index = np.setdiff1d(X_aug_total.index.to_list() ,X_aug_train_index) ","f8dcc321":"#%% MultiColumnLabelEncoder\n# Code snipet found on Stack Exchange \n# https:\/\/stackoverflow.com\/questions\/24458645\/label-encoding-across-multiple-columns-in-scikit-learn\n# from sklearn.preprocessing import LabelEncoder\n\nclass MultiColumnLabelEncoder:\n    def __init__(self,columns = None):\n        self.columns = columns # array of column names to encode\n\n    def fit(self,X,y=None):\n        return self # not relevant here\n\n    def transform(self,X):\n        '''\n        Transforms columns of X specified in self.columns using\n        LabelEncoder(). If no columns specified, transforms all\n        columns in X.\n        '''\n        output = X.copy()\n        if self.columns is not None:\n            for col in self.columns:\n                # convert float NaN --> string NaN\n                output[col] = output[col].fillna('NaN')\n                output[col] = LabelEncoder().fit_transform(output[col])\n        else:\n            for colname,col in output.iteritems():\n                output[colname] = LabelEncoder().fit_transform(col)\n        return output\n\n    def fit_transform(self,X,y=None):\n        return self.fit(X,y).transform(X)\n\n# store the catagorical features names as a list      \ncat_features = X_aug_total.select_dtypes(['object']).columns.to_list()\n\n# use MultiColumnLabelEncoder to apply LabelEncoding on cat_features \n# uses NaN as a value , no imputation will be used for missing data\nX_aug_total_transform = MultiColumnLabelEncoder(columns = cat_features).fit_transform(X_aug_total)","48a6ddc8":"#%% Before and After LabelEncoding\ndisplay(X_aug_total)\ndisplay(X_aug_total_transform)","84db1efa":"##% Split X_aug_total_transform \nX_aug_train_transform = X_aug_total_transform.iloc[X_aug_train_index, :]\nX_aug_test_transform = X_aug_total_transform.iloc[X_aug_test_index, :].reset_index(drop = True) ","cc0c0c68":"##% Before and After LabelEncoding for aug_train \ndisplay(X_aug_train)\ndisplay(X_aug_train_transform)","e0e2ddac":"##% Before and After LabelEncoding for aug_test \ndisplay(aug_test)\ndisplay(X_aug_test_transform)","cef6e476":"#%%  train-test stratified split using a 80-20 split\n# drop enrollee_id for aug_train as it is a useless feature \ntrain_x, valid_x, train_y, valid_y = train_test_split(X_aug_train_transform.drop('enrollee_id',axis = 'columns'), y, test_size=0.2, shuffle=True, stratify=y, random_state=1301)\n\n# Create the LightGBM data containers\n# Make sure that cat_features are used\ntrain_data=lgb.Dataset(train_x,label=train_y, categorical_feature = cat_features)\nvalid_data=lgb.Dataset(valid_x,label=valid_y, categorical_feature = cat_features)\n\n#Select Hyper-Parameters\nparams = {'objective':'binary',\n          'metric' : 'auc',\n          'boosting_type' : 'gbdt',\n          'colsample_bytree' : 0.9234,\n          'num_leaves' : 13,\n          'max_depth' : -1,\n          'n_estimators' : 200,\n          'min_child_samples': 399, \n          'min_child_weight': 0.1,\n          'reg_alpha': 2,\n          'reg_lambda': 5,\n          'subsample': 0.855,\n          'verbose' : -1,\n          'num_threads' : 4\n}","dd9893b7":"#%% Train model on selected parameters and number of iterations\nlgbm = lgb.train(params,\n                 train_data,\n                 2500,\n                 valid_sets=valid_data,\n                 early_stopping_rounds= 30,\n                 verbose_eval= 10\n                 )","c9cfcb7f":"#%% Overall AUC\ny_hat = lgbm.predict(X_aug_train_transform.drop('enrollee_id',axis = 'columns'))\nscore = roc_auc_score(y, y_hat)\nprint(\"Overall AUC: {:.3f}\" .format(score))","98e3a35d":"#%% ROC Curve for training\/validation data\n# https:\/\/scikit-learn.org\/stable\/auto_examples\/model_selection\/plot_roc.html#sphx-glr-auto-examples-model-selection-plot-roc-py\ny_probas = lgbm.predict(valid_x) \nfrom sklearn.metrics import roc_curve, auc\nfpr, tpr, _ = roc_curve(valid_y, y_probas)\nroc_auc = auc(fpr, tpr)\n\nplt.figure()\nlw = 2\nplt.plot(fpr, tpr, color='darkorange',\n         lw=lw, label='ROC curve (area = %0.4f)' % roc_auc)\nplt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('Receiver operating characteristic for training data')\nplt.legend(loc=\"lower right\")\nplt.show()","4a2d0921":"##% Feature Importance \nlgb.plot_importance(lgbm)","eabe25a6":"##% Feature Importance using shap package \nlgbm.params['objective'] = 'binary'\nshap_values = shap.TreeExplainer(lgbm).shap_values(valid_x)\nshap.summary_plot(shap_values, valid_x)","21192984":"#%% Predictions for aug_test.csv\npredict = lgbm.predict(X_aug_test_transform.drop('enrollee_id',axis = 'columns')) \nsubmission = pd.DataFrame({'enrollee_id':X_aug_test_transform['enrollee_id'],'target':predict})\ndisplay(submission)","6cc22740":"##% Submit Predictions \nsubmission.to_csv('submission.csv',index=False)","4e5d0d74":"## target","4be75384":"## Initial Glance at Testing Data","feb863b0":"## Notes \n\nThe dataset is imbalanced.\n\nMost features are categorical (Nominal, Ordinal, Binary), some with high cardinality.\n\nMissing imputation can be a part of your pipeline as well.\n\n## Features\n\n**enrollee_id** : Unique ID for candidate\n\n**city**: City code\n\n**city_development_index** : Developement index of the city (scaled)\n\n**gender**: Gender of candidate\n\n**relevent_experience**: Relevant experience of candidate\n\n**enrolled_university**: Type of University course enrolled if any\n\n**education_level**: Education level of candidate\n\n**major_discipline** :Education major discipline of candidate\n\n**experience**: Candidate total experience in years\n\n**company_size**: No of employees in current employer's company\n\n**company_type** : Type of current employer\n\n**lastnewjob**: Difference in years between previous job and current job\n\n**training_hours**: training hours completed\n\n**target**: 0 \u2013 Not looking for job change, 1 \u2013 Looking for a job change","4e6ed441":"# Conclusions \/ Challenges","e8fd7192":"**Conclusion**\n* LightGBM is a great ML algorithim that handles catagorical features and missing values \n* This is a great dataset to work on and lots of knowledge can be gain from withing with this dataset \n* Researching and reading other Kaggle notebooks is essential for becoming a better data scientist\n\n**Challenges**\n* LightGBM has many parameters and other methods that can be utilize to better tune the parameters, this is my first time using LightGBM so mistakes might have occured \n* Working with catagorical features is difficult, especialy when using One-Hot Encoding, this leads to a messy dataframe and longer computational. This is why I opt for Label Encoding and LightGBM \n\n**Closing Remarks**  \nPlease comment and like the notebook if it of use to you! Have a wonderful year! \n\n1-8-2020\nJoseph Chan ","ca5bfecc":"## Train-Test Stratified Split","7a19d1cc":"# <center>JobChangeClassifier<\/center>\n<img src=\"https:\/\/www.personalcareermanagement.com\/wp-content\/uploads\/2014\/02\/change-300x213.jpg\" width=\"300\" height=\"200\" align=\"center\"\/>","ef6ec2c8":"## company_type\nCatagorical Variable ","17a9c80b":"## company_size\nOrdinal Catagorical variable has 5938 missing variables ","65fb87b5":"## education_level\nCatagorical Variable indicating education level of worker, has 460 missing values","4c5138be":"# Run LightGBM on train data","1d862a9f":"# ROC Curve for training\/validation data","ae7f2a8d":"# Prepare Data for LightGBM","a6aacfa8":"## relevent_experience\nBinary Variable with no missing values.","22bc4a1b":"## enrollee_id \nenrolle_id is an meaningless feature that is a unique value for each employee. ","470a31af":"# Read in Testing Data (aug_test.csv)","ba0d2480":"# Task Details\nThis dataset is designed to understand the factors that lead to a person to work for a different company(leaving current job), by model(s) that uses the current credentials\/demographics\/experience to predict the probability of a candidate to look for a new job or will work for the company.\n\nThe whole data divided to train and test. Sample submission has been provided correspond to enrollee_ id of test set (enrolle_ id | target)","aa067c01":"## Split X_aug_total_transform \n\nSplit X_aug_total_transform back into X_aug_train_transform and X_aug_test_transform by using the index we saved before.","45c167c9":"# Feature Importance \n","d9f7a889":"## city\ncity has 123 unique values and is a categorical variable. ","c3bd0869":"## gender\nCatagorical Variable: Male, Female, Other, or NaN","34a58ca8":"# Submit Predictions ","6db75c03":"# Read in Training Data (aug_train.csv)","8d41541c":"\n## training_hours\nContinous variable","88843b03":"target is the variable we are trying to predict and calculate the probablities for.   \n\n**0 \u2013 Not looking for job change, 1 \u2013 Looking for a job change.**\n\nIt is better to have a high recall to better target employees who are looking for a job change.  \nThis is an unbalanced classification problem as seen in the pie chart below.","3a102915":"# Data Visualization","39ecff8f":"## MultiColumnLabelEncoder","3527e237":"From both feature importance, we can see that **city** contributes a lot if a employee is looking to change jobs or not. The next feature that is also important is **company_size**. The shap package is prefer when finding feature importance as it preservces consistency and accuracy. You can read more about the shap package in the links provided below \n\n[https:\/\/towardsdatascience.com\/explain-your-model-with-the-shap-values-bc36aac4de3d](https:\/\/towardsdatascience.com\/explain-your-model-with-the-shap-values-bc36aac4de3d)  \n[https:\/\/towardsdatascience.com\/interpretable-machine-learning-with-xgboost-9ec80d148d27](https:\/\/towardsdatascience.com\/interpretable-machine-learning-with-xgboost-9ec80d148d27)","731690ce":"aug_train has 1**9,158 observations** with **13 features** and **1 target variable**. The dataset has **missing data** and must be handled properly. ","40e388d6":"## experience\nOrdinal Variable, can replace <1 with 0 and >20 with 21 ","ab9d5e92":"# Predictions for aug_test.csv","989eace7":"## city_development_index\nContinous Variable  \n[How to Calculate the CDI](https:\/\/en.wikipedia.org\/wiki\/City_development_index#:~:text=The%20City%20Development%20Index%20was,level%20of%20development%20in%20cities.&text=It%20was%20invented%20by%20Dr,analysis%20of%20city%20indicators%20data.)\n\n| Index | Formula   |\n|------|------|\n|   Infrastructure  | 25 x Water connections + 25 x Sewerage + 25 x Electricity + 25 x Telephone|\n|   Waste  | Wastewater treated x 50 + Formal solid waste disposal x 50|\n|   Health  | (Life expectancy - 25) x 50\/60 + (32 - Child mortality) x 50\/31.92|\n|   Education  | Literacy x 25 + Combined enrolment x 25 |\n|Product\t|(log City Product - 4.61) x 100\/5.99|\n|   City Development  | (Infrastructure index + Waste index + Education index + Health index + City Product index)\/5|","5d5930ea":"## Initial Glance at Training Data","26658da4":"## major_discipline\nCatagorical Variable indicating major discipline of worker, has 2813 missing values","c0ca4277":"Extract only the features from aug_train and aug_test and rowbind them. We then will perform label encoding so that the LightGBM can be used.","ded8f1b7":"# Importing Libraries","c10fe9f4":"\n## last_new_job\nCatagorical Variable "}}