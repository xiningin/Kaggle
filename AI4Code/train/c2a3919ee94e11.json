{"cell_type":{"9ec0386a":"code","0db8eca6":"code","980c19ed":"code","5c46eec2":"code","7b25ba87":"code","7083ffb1":"code","d28783e7":"code","f9cb4673":"code","8c8b8d54":"code","b964aeef":"code","6eee627b":"code","27a3628f":"code","521453bb":"code","5b53973a":"code","ccf5a7de":"code","9104ed58":"code","a5397bc7":"code","bb1e2127":"code","9142536f":"code","a8eddcdc":"code","3d8b3634":"code","53b26ea6":"code","2544b48a":"code","50826620":"code","ef562907":"code","ff50848a":"code","bdc0a3d5":"code","0ce183ae":"code","615a078a":"code","3dc6d461":"code","ad61e4d8":"code","09ef80bb":"markdown","188510a5":"markdown","35c2dfe1":"markdown","a047b47f":"markdown","68131ec6":"markdown","2a7f244b":"markdown","18b7387a":"markdown","b03ef8d7":"markdown","1a72e015":"markdown","f695a3f4":"markdown","5af89123":"markdown","0244b46d":"markdown","c3551359":"markdown","8e607cb2":"markdown","628caf5e":"markdown","26a111d4":"markdown","efa61124":"markdown","e88a6850":"markdown","e9d3a8bc":"markdown","0322d425":"markdown","bbf07f82":"markdown","1f6c1b4c":"markdown","97384108":"markdown","ddd1b819":"markdown","afa97834":"markdown","8ce60236":"markdown","8009a193":"markdown","f7a0e7e3":"markdown","8139187e":"markdown","0b6a9e30":"markdown","c8c73fa3":"markdown","2718f938":"markdown","08975caa":"markdown"},"source":{"9ec0386a":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn import metrics\nfrom sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\nfrom pandas.plotting import scatter_matrix\n\ndata = pd.read_csv('..\/input\/heart-failure-clinical-data\/heart_failure_clinical_records_dataset.csv')\ndata.head()","0db8eca6":"fig, ax = plt.subplots(figsize = (8, 8))\nsurvive = data.loc[(data.DEATH_EVENT == 0)].age\ndeath = data.loc[(data.DEATH_EVENT == 1)].age\nprint(survive.mean())\n\nsns.boxplot(data = data, x = 'DEATH_EVENT', y = 'age', hue = 'sex', width = 0.4, ax = ax, fliersize = 3, palette=sns.color_palette(\"pastel\"))\nsns.stripplot(data = data, x = 'DEATH_EVENT', y = 'age', hue = 'sex', size = 3, palette=sns.color_palette())\nax.set(xlabel = 'DEATH', ylabel=\"age\", title='The relationship between age and death')\nplt.show()","980c19ed":"fig, ax = plt.subplots(ncols=2, figsize = (15, 5))\nsns.violinplot(data=data, x='DEATH_EVENT', y='ejection_fraction', ax=ax[0], palette=sns.color_palette('Set2'))\nsns.violinplot(data=data, x='DEATH_EVENT', y='serum_creatinine', ax=ax[1], palette=sns.color_palette('Set2'))","5c46eec2":"corr_matrix = data.corr()\ncorr_matrix['DEATH_EVENT'].sort_values(ascending=False)","7b25ba87":"data.info()\ndata.isnull().sum()","7083ffb1":"data.describe()","d28783e7":"discrete_features, continuous_features = [], []\nfor feature in data.columns:\n    if feature == 'DEATH_EVENT':\n        label = feature\n    elif len(data[feature].unique()) > 2:\n        continuous_features.append(feature)\n    else:\n        discrete_features.append(feature)\n\nprint('Discrete: ', discrete_features, '\\n', 'Continuous:', continuous_features, '\\n', 'Label:', label)","f9cb4673":"data[discrete_features].head()","8c8b8d54":"data[continuous_features].head()","b964aeef":"from sklearn.preprocessing import MinMaxScaler\nscaler = MinMaxScaler(feature_range=(0,1))\nscaler.fit(data[continuous_features])\nscaled_continuous_features = scaler.transform(data[continuous_features])\nscaled_continuous_features = pd.DataFrame(data=scaled_continuous_features, columns=['age','creatinine_phosphokinase', 'ejection_fraction', 'platelets', 'serum_creatinine', 'serum_sodium', 'time' ])\nscaled_continuous_features.head()","6eee627b":"scaled_data = pd.concat([data[discrete_features], scaled_continuous_features, data['DEATH_EVENT']], axis=1)\nscaled_data.describe()","27a3628f":"X_train, X_test, y_train, y_test = train_test_split(scaled_data.iloc[:,:-1], scaled_data.iloc[:,-1], stratify = scaled_data['DEATH_EVENT'], test_size=0.3, random_state=42)","521453bb":"num_all = np.unique(scaled_data['DEATH_EVENT'], return_counts=True)\nprint('             The number of DEATH_EVENT')\nprint('                   0        |      1')\nprint('------------------------------------------')\nprint('All dataset      ', num_all[1][0], ' '*5, '|', ' '*4, num_all[1][1])\nnum_train = np.unique(y_train, return_counts=True)\nprint('Train set (70%)  ', num_train[1][0], ' '*5, '|', ' '*4, num_train[1][1])\nnum_test = np.unique(y_test, return_counts=True)\nprint('Test set (30%)    ', num_test[1][0], ' '*5, '|', ' '*4, num_test[1][1])","5b53973a":"from sklearn.tree import DecisionTreeClassifier, plot_tree\n\ntree_clf = DecisionTreeClassifier(criterion='gini')\ntree_clf = tree_clf.fit(X_train, y_train)\nprint('The maximum depth of the tree is ', tree_clf.get_depth())","ccf5a7de":"accuracies = []\nprint('Depth', ' Train Accuracy', ' Test Accuracy', ' CV Accuracy')\n\nfor d in range(1,9):\n    tree_clf = DecisionTreeClassifier(criterion='gini', max_depth = d)\n    tree_clf = tree_clf.fit(X_train, y_train)\n    \n    train_accuracy = tree_clf.score(X_train, y_train)\n    test_accuracy = tree_clf.score(X_test, y_test)\n    \n    cv_accuracy = np.mean(cross_val_score(tree_clf, X_train, y_train, cv=5))\n    accuracies.append([d, train_accuracy, test_accuracy, cv_accuracy])\n    \n    print(' ', d, ' '*6, '%.4f' % train_accuracy, ' '*8, '%.4f'% test_accuracy, ' '*5, '%.4f'% cv_accuracy)","9104ed58":"accuracies = pd.DataFrame( \n    data = accuracies, \n    columns = ['Depth', 'Train Accuracy','Test Accuracy', 'CV Accuracy']\n)    \n\naccuracies = pd.melt(\n    accuracies,\n    id_vars = ['Depth'],\n    var_name = 'Type',\n    value_name = 'Accuracy'\n) \nsns.lineplot(\n    x = 'Depth',\n    y = 'Accuracy',\n    hue = 'Type',\n    data = accuracies,\n\n).set_title('The relationship between the depth of the tree and accuracies')","a5397bc7":"tree_clf = DecisionTreeClassifier(criterion='gini', max_depth=3)\ntree_clf = tree_clf.fit(X_train,y_train)\nplt.subplots(figsize=(20,10))\nplot_tree(tree_clf, fontsize=13, feature_names=scaled_data.columns, class_names=['Not dead', 'Dead'], filled=True)\nplt.show()","bb1e2127":"from sklearn.ensemble import RandomForestClassifier\naccuracies = []\nprint('Depth', ' Train Accuracy', ' Test Accuracy', ' CV Accuracy')\n\nfor d in range(1,8):\n    rf_clf = RandomForestClassifier(n_estimators=100, max_depth=d, criterion='gini', random_state=42)\n    rf_clf = rf_clf.fit(X_train, y_train)\n    \n    train_accuracy = rf_clf.score(X_train, y_train)\n    test_accuracy = rf_clf.score(X_test, y_test)\n    cv_accuracy = np.mean(cross_val_score(rf_clf, X_train, y_train, cv=5))\n    accuracies.append([d, train_accuracy, test_accuracy, cv_accuracy])\n    \n    print(' ', d, ' '*6, '%.4f' % train_accuracy, ' '*8, '%.4f'% test_accuracy, ' '*5, '%.4f'% cv_accuracy )","9142536f":"accuracies = pd.DataFrame( \n    data = accuracies, \n    columns = ['Depth', 'Train Accuracy','Test Accuracy', 'CV Accuracy']\n)    \n\naccuracies = pd.melt(\n    accuracies,\n    id_vars = ['Depth'],\n    var_name = 'Type',\n    value_name = 'Accuracy'\n) \nsns.lineplot(\n    x = 'Depth',\n    y = 'Accuracy',\n    hue = 'Type',\n    data = accuracies,\n\n).set_title('The relationship between the depth of the tree and accuracies')","a8eddcdc":"param_grid = {\n    'criterion': ['gini', 'entropy'],\n    'n_estimators': [100, 200, 500],\n    'max_depth': [2,3,4],\n}\nclf_grid = GridSearchCV(estimator = RandomForestClassifier(random_state=42), param_grid=param_grid, cv=5)\nclf_grid.fit(X_train, y_train)\nclf_grid.best_params_","3d8b3634":"print('CV Accuracy: %.4f' % clf_grid.best_score_)\nprint('Test Accuracy: %.4f' % clf_grid.score(X_test, y_test))","53b26ea6":"rf_fi = RandomForestClassifier(n_estimators=200, max_depth=3, criterion='gini', random_state=42)\nrf_fi = rf_fi.fit(X_train, y_train)\n\nimportances = list(rf_fi.feature_importances_)\nscaled_data_col_name = list(scaled_data.columns[:-1].to_numpy())\n\nplt.bar(scaled_data_col_name , importances, color='lightblue')\nplt.xticks(ticks = scaled_data_col_name, labels = scaled_data_col_name, rotation = 'vertical')\nplt.ylabel('Importance')\nplt.xlabel('Feature')\nplt.title('Feature Importances')","2544b48a":"from sklearn.svm import SVC\nparam_grid = {\n    'kernel': ['linear', 'poly', 'rbf'],\n    'C': [0.001, 0.01, 0.1, 1, 10],\n    'degree': [2,3,4],\n    'gamma': [0.001, 0.01, 0.1, 1, 10]\n}\nclf_grid_SVC = GridSearchCV(estimator = SVC(random_state=42), param_grid=param_grid, cv=5)\nclf_grid_SVC.fit(X_train, y_train)\nclf_grid_SVC.best_params_","50826620":"print('CV Accuracy: %.4f' % clf_grid_SVC.best_score_)\nprint('Test Accuracy: %.4f' % clf_grid_SVC.score(X_test, y_test))","ef562907":"from sklearn.decomposition import PCA\n\npca_data = scaled_data.drop(columns='DEATH_EVENT')\npca = PCA()\npca.fit(pca_data)\n\npercentage_variance = np.round(pca.explained_variance_ratio_ * 100, decimals=2)\nxlabels = ['PC' + str(x) for x in range(1, len(percentage_variance)+1)]\n\nplt.plot(range(1,len(percentage_variance)+1), percentage_variance, '-o')\nplt.axvline(x=8, color='red', linestyle='--')\nplt.ylabel('Percentage of Explained Variance')\nplt.xlabel('Principal Component')\nplt.title('Scree Plot')\nplt.show()","ff50848a":"pca95 = PCA(n_components=0.95)\npca95.fit_transform(pca_data).shape","bdc0a3d5":"fig, ax = plt.subplots(nrows=4, ncols=2, figsize=(15, 18))\n\nrow = -1; col = 0\nfor i in range(0,8):\n    \n    if(i%2 == 0): \n        col = 0\n        row = row + 1\n    else: \n        col=1\n    \n    sns.barplot(x=abs(pca.components_[i]), y=pca_data.columns, orient='h', ax=ax[row][col])\n    ax[row][col].set_title('Principal Component ' + str(i+1))","0ce183ae":"print('                  Original           With PCA')\nprint('---------------------------------------------------')\nori_accuracy = []\npca_accuracy = []\n\n# Without PCA\ntree_clf = DecisionTreeClassifier(criterion='gini', max_depth = 1, random_state=42)\ntree_clf = tree_clf.fit(X_train, y_train)\nori_accuracy.append(np.mean(cross_val_score(tree_clf, X_train, y_train, cv=5)))\n\nrf_clf = RandomForestClassifier(n_estimators=200, max_depth=3, criterion='gini', random_state=42)\nrf_clf = rf_clf.fit(X_train, y_train)\nori_accuracy.append(np.mean(cross_val_score(rf_clf, X_train, y_train, cv=5)))\n\nsvc_clf = SVC(kernel='linear', C=10, random_state=42)\nsvc_clf = svc_clf.fit(X_train, y_train)\nori_accuracy.append(np.mean(cross_val_score(svc_clf, X_train, y_train, cv=5)))\n\n# With PCA\npca95 = PCA(n_components=0.95)\nX_train_reduced = pca95.fit_transform(X_train)\nX_test_reduced = pca95.transform(X_test)\n\ntree_clf_pca = DecisionTreeClassifier(criterion='gini', max_depth = 1, random_state=42)\ntree_clf_pca = tree_clf_pca.fit(X_train_reduced, y_train)\npca_accuracy.append(tree_clf_pca.score(X_test_reduced, y_test))\n\nrf_clf_pca = RandomForestClassifier(n_estimators=200, max_depth=3, criterion='gini', random_state=42)\nrf_clf_pca = rf_clf_pca.fit(X_train_reduced, y_train)\npca_accuracy.append(rf_clf_pca.score(X_test_reduced, y_test))\n\nsvc_clf_pca = SVC(kernel='linear', C=10)\nsvc_clf_pca = svc_clf_pca.fit(X_train_reduced, y_train)\npca_accuracy.append(svc_clf_pca.score(X_test_reduced, y_test))\n\nprint('Decision Tree      %.4f' % ori_accuracy[0], ' '*11, '%.4f'%pca_accuracy[0])\nprint('Random Forest      %.4f' % ori_accuracy[1], ' '*11, '%.4f'%pca_accuracy[1])\nprint('SVM', ' '*14, '%.4f'%ori_accuracy[2], ' '*11, '%.4f'%pca_accuracy[2])","615a078a":"from sklearn.cluster import KMeans\nlosses = []\nfor K in range(1, 15):\n    kmeans = KMeans(n_clusters=K)\n    kmeans.fit(scaled_data.iloc[:,:-1])\n    losses.append(kmeans.inertia_)\n    \nplt.plot(range(1,15), losses, '-o')\nplt.xlabel('Number of Clusters')\nplt.ylabel('Loss')","3dc6d461":"from sklearn.cluster import KMeans\n\nkmeans = KMeans(n_clusters=2, random_state=42)\nkmeans = kmeans.fit(scaled_data.iloc[:,:-1])\nkmeans_label = kmeans.labels_\nkmeans_label = pd.DataFrame(kmeans_label, columns=['K-Means label'])\npd.concat([kmeans_label, scaled_data.iloc[:, -1:]], axis=1).head()","ad61e4d8":"print('K-means accuracy: %.4f'%metrics.accuracy_score(kmeans_label, scaled_data.iloc[:, -1:]))","09ef80bb":"We compare the accuracy of models and the result shows that the model with the Random Forest classification technique without PCA performs the highest accuracy at 0.8662.\n\n| Models      | Accuracy \n| ------------| ----------- \n| Random Forest | 0.8662\n| Decision Tree | 0.8470\n| SVM with PCA | 0.8444\n| SVM | 0.8280\n| Random Forest with PCA | 0.8222\n| Decision Tree with PCA | 0.8111\n| K-means | 0.4448","188510a5":"## 4.2 Random Forest\n\nRandom Forest is the second technique used in building models. It is based on decision tree.\n\n### 4.2.1 Accuracy Comparison\n\nRandom forest also causes the overfitting issue, same as the decision tree. The higher number of depth increases, the higher accuracy closes to 1.0. `CV Accuracy` is applied to avoid overfitting. Random forest requires many parameters to be set for a classifier. To define the best parameters, we will discuss in Section 4.2.3. Now we define `n_estimators=100` which is an acceptable number for our computing resources. The higher number increases the computional time.","35c2dfe1":"### 4.2.4 Feature Importance\n\nFeature importance is a procedure to assign scores to features to indicate the relative importance of each feature when building a model. The scores provide insight into the dataset, telling us which features are the most or the least relevant. After building the model, we use `feature_importances_` to view the relative importance scores of each feature. We can interpret from the bar chart below that `time` is the most importance feature in prediction.","a047b47f":"## 1.2 Ejection Fraction\/Serum Cretinine and Death\n\nThe violin plots display the relationship between `ejection_fraction`(left)\/`seruum_creatinin`(right) and death. On the left hand side, there is a significant difference in `ejection_fraction` between survived and dead patients. Those who have a lower percentage of blood pumped out of the heart tend to be dead, whereas most patients who have over 35 of `ejection_fraction` survived.  On the other hand, in the chart on the right, there is no significant differece in the level of `serum_creatinine` between two types of patients. ","68131ec6":"### 2.2.2 Standardisation by MinMaxScaler\n\nAfter we obtain `continuous_feature`, we perform the standardisation by using `MinMaxScaler` to scale features. The scaled data lies between 0 and 1 and the standard deviation of every features is in the range of 0 and 1 as well. The dataset is now ready to be used in building models.","2a7f244b":"# 5. Model Comparison","18b7387a":"# 4. Unsupervised Learning\n## 4.1 Principal Component Analysis (PCA) \n\n### 4.1.1 Dimensionality Reduction\n\nOne of the ability of PCA is to reduce the dimensionality of the dataset to reduce the computational time. PCA is used to find features that explain the most of the variance (95-100%) in the dataset without using all features to compute. Our dataset contains 12 features, excluding the label, which are used to create 12 principal components (PC). Each PC produces eigenvectors `components_` and eigenvalues `explained_variance_`. The line graph below shows a sharp decresing trend in `explained_variance_` in percentage until it reaches at PC8 where the trend remains almost stable.","b03ef8d7":"### 3.1.3 Tree Plot\nWe use `plot_tree` to view the information of the split. The picture below shows the example split of the depth of 3. It is very useful to explain how the decision tree algorithm works to non-IT persons. It includes \n* Feature name and its value to split\n* gini = gini value\n* samples = The number of total samples\n* value = The number of `DEATH_EVENT`\n* class = `DEATH_EVENT`","1a72e015":"### 3.1.2 Accuracy by Depth of Tree\nThe line graph below illustrates `Train Accuracy`, `Test Accuracy` and `CV Accuracy`. We ignore the train accuracy because it causes the overfitting issue. We now focus on `Test accuracy` and `CV Accuracy`. Both accuracy keep decreasing when the depth increases. The test accuracy flutuates along the way while the CV accuracy drops smoothly. To choose the most accurate model, we consider a model with the highest accuracy. The highest accuracy is the tree with cross-validation at the depth of 1.\n\n##### The decision tree with the highest performance is the model with `max_depth`=1. The train and test accuracy are 0.8470 and 0.8222, respectively.","f695a3f4":"### 4.2.2 Accuracy Comparison\n\nIn this section, we compare the output from K-means clustering and the original output.  Given that we would like to cluster our patients into two groups: dead and survived patients. Thus we use k=2 for the model. The table below shows the output from K-means and the original dataset.","5af89123":"## 1.3 Data Correlation\nThe numbers below are the correlation between features and `DEATH_EVENT`. The correlation closed to 1.0 and -1.0 means the variable is positively and negatively correlated, respectively. The correlation closed to 0 is less correlated. \n* `serum_creatinine` and `age` are positively and highly correlated at 0.29 and 0.25, respectively\n* `time` and `ejection_fraction` are negatively and highly correlated at -0.53 and -0.27, respectively","0244b46d":"The eight barchart below illustrate eigenvectors and principal components. For example, `sex` and `smoking` are dominated in PC1 and `anaemia` is dominated in PC2, and so on.","c3551359":"# 6. Reference\n* Professor Soufiane Hayou's Jupyter notebook files\n* Heart Failure Dataset: https:\/\/www.kaggle.com\/andrewmvd\/heart-failure-clinical-data\n* Research in Heart Failure Dataset: https:\/\/bmcmedinformdecismak.biomedcentral.com\/articles\/10.1186\/s12911-020-1023-5#Sec3\n* Python Book: Python Data Science Handbook, Jake VanderPlas, O'Reilly\n* Python Book: Introduction to Machine Learning with Python, Andreas C. M\u00fcller & Sarah Guido, O'Reilly\n* Machine Learning Framework: https:\/\/scikit-learn.org\/\n* Machine Learning Information: https:\/\/machinelearningmastery.com\/\n\nOriginal project was completed on 21 November 2021.","8e607cb2":"##### The random forest with the highest performance is the model with gini, `max_depth`=3, `n_estimators`=200. The train and test accuracy are 0.8662 and 0.8444, respectively.","628caf5e":"## 2.3 Train and Test Set\n\nThe scaled dataset is split into a train and a test set in building machine learning models. We set 70% of the dataset as a train set and 30% of the dataset as a test set by using the `train_test_split` function. When we split the data, we `stratify` according to the label to ensure that both train and test data will be balance in terms of the number of `DEATH_EVENT`. In this project, `random_state=42` is used for reproducible. ","26a111d4":"## 1.1 Age and Death\nThe boxplot below illustrates the relationship between age and death. Overall the age of dead patients is older than those who survived. The average age of survived patients is around 60 years old for both genders, whereas the average age of dead male patients is 65 which is higher than of dead female patients at 60. ","efa61124":"<b>The accuracy of K-means model with two clusters is 0.4448<\/b>.","e88a6850":"## 2.2 Feature Scaling\n\n\nFeature Scaling is a preprocessing to scale data to a standard range before building machine learning models. Without feature scaling, one significant number can impact the model because of its large magnitude. This results the model to suffer from poor performance during learning. \n\n\n### 2.2.1 Binary and Non-binary Data\nAs mentioned in Section 1, dataset contains both binary and non-binary values. We now call the binary as `discrete_features` and the non-binary as `continuous_features`. The standard deviation of `discrete_features` is already small, ranging between `0` and `1`, so we do not have to scale them. Unlike `continuous_features`, the standard deviation varies from `1.03` to `97,804`. Hence, we have to scale these features. We categorise features before performing feature scaling as follows.","e9d3a8bc":"### 3.1.1 Decision Tree and Overfitting Issue\nWe calculate `Train Accuracy`, `Test Accuracy` of depth 1 to 8. `Train Accuracy` and `Test Accuracy` are the accuracy of the train and test set split in Section 2.3. We clearly see that the higher depth of tree, the closer of `Train Accuracy` reaches to 1. This happens because the model at the depth of 8 splits until left nodes are pure, resulting the model to be perfect in prediction. However, it causes an `overfitting` issue for a decision tree classifier.\n\nTo avoid the overfitting, the <b>`k-Fold Cross-Validation`<\/b> procedure is applied to mitigate this issue. The model is trained using k-1 of the folds as training data. The remaining part is used as testing data to measure the accuracy of each fold. Then, the average of accuracy is calculated as a result. After applying the cross-validation, the model does not suffer from overfitting anymore.","0322d425":"### 4.1.2 PCA with Supervised Learning Models\n\nPCA is applied to supervised learning models with the highest accuracy. Reduced components from PCA are used as a new train and test dataset in buliding models with PCA. We compare the accuracies of original models from Section 3 and the models with PCA. The accuracy of Decision Tree and Random Forest models decrease after PCA is applied. On the other hand, the accuracy of SVM with linear kernel increases when PCA is performed. SVM accuracy increases from 0.8280 to 0.8444.\n\nFrom this experiment, we can conclude that reducing the dimensionality of the dataset does not always perform a better result than using all dimensions. ","bbf07f82":"## 4.3 Support Vector Machine (SVM)\n\nSVM provides three kernels: linear, polynomial and rbf. We use `GridSearchCV` to select the best parameters for the support vectore classifier. The result is `kernel='linear'` and `C=10`. We can ignore degree and gamma because they are only used in poly and rbf kernel. The higest accuracy <b>0.8280<\/b> is a model performed with cross-validation.","1f6c1b4c":"## 4.2 K-Means\n\nK-Means is an algorithm for clustering unlabelled data into k groups and each datapoint is assigned to the closest centroid. In this section, we assume that our dataset does not have a label, so we build the algorithm with only 12 independent variables, no `DEATH_EVENT` includes. The performance of the model depends on the value of k. To find the optimal value of k, <b>elbow method<\/b> is performed. \n\n### 4.2.1 Elbow Method\nThe elbow method plots the value of `inertia`, or the loss function, and the number of clusters (k). `inertia` tells how far away the points within a cluster are. The number of cluster increases while the loss decreases. This means that each cluster has fewer datapoints and datapoints are closer to their centriods. The value of k in which the loss drops the most is called the elbow, and it is where we dividing more clusters. In this experiment, it is very difficult to identify the breaking point of the elbow since the graph has a smooth decreasing trend.","97384108":"To choose the right number of dimensions, we can input the number of components when constructing PCA (i.e. `PCA(n_components=8`). However, it is much simpler to indicate the ratio of variance, between 0.0 to 1.0, that we want to preserve. In this experiment, we want to preserve 95% of the dataset's variance, so we define `n_components=0.95`. After transformation, the dimensionality has been reduced from `12` to `8` components. \n\n`299` is the number of records and `8` is the number of reduced components.","ddd1b819":"### 4.2.3 Hyperparameter Tuning\n\nHyperparameter tuning is used to recommend to search the hyperparameter space for the highest cross-validation score. We construct `GridSearchCV` with given parameters in `param_grid`. The best parameter for the random forest is `{'criterion': 'gini', 'max_depth': 3, 'n_estimators': 200}` with the accuracy of <b>0.8662<\/b>.","afa97834":"##### The support vector machine with the highest performance is the model with linear kernel and `C`=10. The train and test accuracy are 0.8280 and 0.8222, respectively.","8ce60236":"# 3. Supervised Learning\n\nThe heart failure dataset has a label `DEATH_EVENT` with two distinct values: `0` and `1`. Thus, we use <b>classification algorithms<\/b> to build supervised machine learning models.\n\n## 3.1 Decision Tree\n\nFirst of all, we construct a classifier of Decision Tree using `gini` as a criterion without defining the `max_depth` parameter. The tree performs until all left nodes are pure, meaning there is no ability to split left nodes. The maximum depth of the tree is 8.","8009a193":"To discover more information from the dataset, we perform some data analysis to visualise the relationships between features (independent variables) and `DEATH_EVENT` in Section 1.1-1.3. First of all, we import relevant Python packages and load the dataset into this notebook.","f7a0e7e3":"# 2. Data Preparation\nIn this section, we will prepare the dataset for machine learning algorithms to perform the prediction.\n\n## 2.1. Data Cleaning\nData cleaning is the first step to prepare datasets to ensure that there is no missing values occurs during the application because most machine learning algorithms cannot work with missing features. Two common ways to deal with missing values are as follows:\n* Get rid of data with missing values by using `dropna()`.\n* Fill in some values to missing values by using `fillna()`. \n\nTo check whether the dataset contains missing values or null values (`NaN`), we use either `data.info()` or `data.isnull().sum()` to view the number of null values of each feature. `data.info()` shows that there are 299 entries (rows) and 13 features (columns). Every feature contains 299 non-null values. In the same way, `data.isnull().sum()` calculates the sum of null values and every feature has 0 null value. Thus, we do not need to clean this dataset since it is well-constructed.","8139187e":"### 4.2.2 Accuracy by Depth of Tree\nUnlike decision tree, random forest perform an increasing trend of accuracy when we increase the depth of the tree. `CV Accuracy` increases sharply until it reaches the depth of 3 and the accuracy remains quite stable. `Test Accuracy` is less flutuated than the decision tree. It increases significantly until the depth is 4 and drops at the depth of 5.","0b6a9e30":"# Heart Failure Prediction\nThis is my first individual machine learning project for university assignments. I hope it is useful for beginners who have just started to learn machine learning. The techiniques used in this project include as follows:\n* Supervised Learning (Classification)\n 1. Decision Tree\n 2. Random Forest\n 3. Support Vector Machine\n* Unsupervised Learning\n 1. Principal Component Analysis (PCA) \n 2. K-Means\n\nPlease do not copy my code for your assignments, you will learning nothing from this action.","c8c73fa3":"The table below shows that all features have the standard deviation between 0 and 1.","2718f938":"We categorise features by calculating the distinct values of each feature. If the feature contains more than 2 values, it is `continuous_features`. Otherwise, it is `discrete_features`. The output `DEATH_EVENT` is not in any types. The tables below show `discrete_features` and `continuous_features` separately.","08975caa":"# 1. Data Analysis and Visualisation\n\nThe dataset contains the medical records of 299 patients with previuos heart failures collected at the Faisalabad Institute of Cardiology and at the Allied Hospital in Pakistan. The dataset contains 13 features of physical, clinical and lifestyle information which can be categorised into two types of data: binary and non-binary data type. Binary features include only values of `0` (No) and `1` (Yes), and non-binary features are any real numbers. The `DEATH_EVENT` feature is the label (output) for classification algorithms.\n\nThe definition of features is in the table below.\n\n\n| Features      | Explanation | Data Type\n| ------------- | ----------- | -----\n| age           | Age of the patient | Any number\n| anaemia       | If a patient has a low level of red blood cells | 0 and 1\n| creatinine_phosphokinase | Level of this enzyme in the blood  | Any number\n| diabetes      | If a the patient has diabetes | 0 and 1\n| ejection_fraction  | Percentage of blood pumped out of the heart during a single contraction | Any number\n| high_blood_pressure | If a patient has a high blood pressure | 0 and 1\n| platelets     | Platelets in the blood | Any number\n| serum_creatinine | Level of creatinine in the blood | Any number\n| serum_sodium | Level of creatinine in the blood | Any number\n| sex | Female and Male | 0 (Female) and 1 (Male)\n| smoking | If a patient smokes | 0 and 1\n| time | Follow-up period (Days) | Any number\n| DEATH_EVENT | If a patient died before the end of the follow-up period | 0 and 1creatinine_phosphokinase "}}