{"cell_type":{"76c9b558":"code","9e59886c":"code","f8ae50b8":"code","24966ea4":"code","cb50d49c":"code","df928591":"code","e87492ac":"code","50c4d8d0":"code","87ea28ae":"code","022571c3":"code","7e97e09f":"code","60d0675a":"code","e9304699":"code","fe4ae014":"code","657ac937":"code","b9ebb937":"code","0d5f8030":"code","47c6540d":"code","74ad4d8a":"code","93cae617":"markdown","cdf0932d":"markdown","70bf4bcb":"markdown","80f0ad8b":"markdown","63dca7b7":"markdown","e31e0431":"markdown","16c6d1d2":"markdown","1b2f627e":"markdown"},"source":{"76c9b558":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","9e59886c":"import pandas as pd\npd.set_option(\"display.max_columns\",30)\npd.set_option('display.max_rows', 1000)\n\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.ensemble import RandomForestRegressor,RandomForestClassifier\nfrom sklearn.metrics import mean_absolute_error\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import classification_report\n\ntrain = pd.read_csv(r'..\/input\/titanic\/train.csv')\ntest = pd.read_csv(r'..\/input\/titanic\/test.csv')\n","f8ae50b8":"combined_data = [train,test]","24966ea4":"# info on the data\nprint(train.info())\n\nprint(train.head())","cb50d49c":"# check for null in training and test data\nprint(\"Null data before cleaning:\")\nfor data in combined_data:\n    print(data.isnull().sum())","df928591":"#imputing missing values\n\nfor data in combined_data:\n    data['Age'].fillna(value = data['Age'].mean(), inplace=True)\n    data['Embarked'].fillna(value = data['Embarked'].mode()[0],inplace=True)\n    data['Fare'].fillna(value = data['Fare'].mean(),inplace = True)\n\n    \n    \n# drop cabin column , too many values at null\n    \nfor data in combined_data:\n    data.drop(columns=['Cabin','Name'], inplace=True)\n\nprint(\"Null data after cleaning:\")\n\nfor data in combined_data:\n    print(data.isnull().sum())","e87492ac":"# features engineering\n    \nfor data in combined_data:\n    data['Family_size'] = data['SibSp']+data['Parch']\n    \n#Ticket\nfor data in combined_data:\n    data['Ticket_len'] = data['Ticket'].apply(lambda x: len(x))\n    data['First_digit_ticket'] = data['Ticket'].apply(lambda x : x[0])\n\n#drop ticket    \nfor data in combined_data:\n    data.drop(columns=['Ticket'], inplace=True)","50c4d8d0":"#survival rate by ticket lenght\n\nprint(\"Survival_rate_by_ticket_len\", train.groupby(['Ticket_len'])['Survived'].mean())\nprint(\"Survival rate by first digit ticket \", train.groupby(['First_digit_ticket'])['Survived'].mean())    \n\n","87ea28ae":"# suvivival for categorical data\nfor x in train:\n    if train[x].dtype ==  'object' and x != 'Ticket':\n        print(train[train['Survived']==1].groupby(x)['Survived'].count())","022571c3":"#distribution of survival per age   \nsns.kdeplot(data =train['Age'][train['Survived']==1] , shade=True,label = \"Distribution of age for survived\")\nsns.kdeplot(data =train['Age'][train['Survived']==0] , shade=True,label = \"Distribution of age for non survived\")\nplt.title(\"Distribution of survival by age\")\nplt.show()","7e97e09f":"#distrbibution of survived per age - pclass\nsns.jointplot(x=train['Pclass'][train['Survived']==1], y=train['Age'][train['Survived']==1], data=train, kind=\"kde\")\nplt.title(\"Distribution of survived by age and Pclass\")\nplt.show()","60d0675a":"#pairplot\n\nsns.boxplot(x=\"Embarked\", y=\"Fare\",hue =\"Survived\" , data=train)\nplt.show()","e9304699":"sns.relplot(x=\"Age\", y=\"Fare\", size=\"Fare\", sizes=(15, 200),hue=\"Survived\",  data=train);\nplt.show()","fe4ae014":"#ONE HOT ENCODING , to use categorical values in our model we need to hod encode them\ncolonne = ['Sex','Embarked']\n    \ntrain = pd.get_dummies(train, columns=colonne)\ntest = pd.get_dummies(test, columns=colonne)\n\n\n\n    \ncolumns_to_drop2 = ['Ticket_len']\n\ntrain.drop(columns=columns_to_drop2, inplace=True)\ntest.drop(columns=columns_to_drop2, inplace=True)\n    \n    \n\nlabel = LabelEncoder()\n#train['Age_10bins_code'] = label.fit_transform(train['Age_10bins'])\n#train['Fare_10bins_code'] = label.fit_transform(train['Fare_10bins'])\ntrain['First_digit_ticket_code'] = label.fit_transform(train['First_digit_ticket'])\ntest['First_digit_ticket_code'] = label.fit_transform(test['First_digit_ticket'])\n            \ncolumns_to_drop3 = ['First_digit_ticket']\n\ntrain.drop(columns=columns_to_drop3, inplace=True)\ntest.drop(columns=columns_to_drop3, inplace=True)   \n\n    \n             \nprint(\" training list\", train.columns.tolist())\nprint(\"test  lists\",test.columns.tolist())","657ac937":"features = [ 'Pclass', 'Age', 'SibSp', 'Parch', 'Fare', 'Family_size', 'Sex_female', 'Sex_male', 'Embarked_C', 'Embarked_Q', 'Embarked_S', 'First_digit_ticket_code']\n\ntarget = 'Survived'\nX = train[features]\ny = train[target]\n\nX_train, X_test, y_train, y_test = train_test_split(X,y,test_size = 0.3,random_state = 0 )\n","b9ebb937":"\n#random forest classifier\nprint(\"RANDOM FOREST CLASSIFIER\")\nrfc = RandomForestClassifier(max_features='auto',  n_jobs=-1)\nparams_rfc = {\"criterion\":['gini','entropy'],\"n_estimators\": [50, 100, 400, 700, 1000],\"bootstrap\":[True, False]}\ngrid_search_rfc = GridSearchCV(rfc, param_grid= params_rfc, cv=5, n_jobs=-1)\ngrid_search_rfc.fit(X_train,y_train)\nprint(\"score \", grid_search_rfc.score(X_train, y_train))\nprint(\"best parameter for random forest classifier \", grid_search_rfc.best_params_)","0d5f8030":"#random forest regressor\nprint(\"RANDOM FOREST REGRESSOR\")\nrfr = RandomForestRegressor(max_features='auto',  n_jobs=-1)\nparams_rfr = {\"n_estimators\": [50, 100, 400, 700, 1000],\"bootstrap\":[True, False]}\ngrid_search_rfr = GridSearchCV(rfr, param_grid= params_rfr, cv=5, n_jobs=-1)\ngrid_search_rfr.fit(X_train,y_train)\nprint(\"score \", grid_search_rfr.score(X_train, y_train))\nprint(\"best parameter for random forest classifier \", grid_search_rfr.best_params_)","47c6540d":"#Logistic regression\nprint(\"LOGISTIC REGRESSION\")\nlr = LogisticRegression()\nparams_lr = {\"penalty\": ['l1','l2'],\"C\":[0.001,0.01,0.1,1,10,100,1000]}\ngrid_search_lr = GridSearchCV(lr, param_grid= params_lr, cv=5, n_jobs=-1)\ngrid_search_lr.fit(X_train,y_train)\nprint(\"score \", grid_search_lr.score(X_train, y_train))\nprint(\"best parameter for random logistic regression \", grid_search_lr.best_params_)","74ad4d8a":"rfc2 = RandomForestClassifier(max_features='auto',   n_jobs=-1, n_estimators = 100, bootstrap = True, criterion ='entropy')\n\nrfc2.fit(X, y)\n\nx_test = test[features]\n\nY_prediction = rfc2.predict(x_test)\n\nmy_submission = pd.DataFrame({'PassengerId': test['PassengerId'], 'Survived': Y_prediction})","93cae617":"As you can see there are some missing values on Age and Cabin and only one missing value for Fare","cdf0932d":"Now we can build our models. I will use Random forest classifier, Random forest regressor and Logistic regressor. I will use grid search to find out the best parameters for our model.","70bf4bcb":"From the jointplto what we see is that  first class and third class have the highest probability of survival. \nFrom the pairplot we can see that people who embarked in port C have more probability of survival and from the last graph we can see that the highest is the fare the highest is the probability of survive.","80f0ad8b":"Load data and import libraries","63dca7b7":"Random forest regressor is the best model among the ones selected with a score of 0.98.  We can now test our model on test data with parameters {'bootstrap': True, 'criterion': 'entropy', 'n_estimators': 100} and submit the results.","e31e0431":"Now what I want to do is find the relationships with our variables and the target variable using some graphs.","16c6d1d2":"After cleaning data noone columns has missing values. The next step is feature engineering, that is trying to create new variables starting from the exsisting ones.","1b2f627e":"Now I put togheter train and test to work simultaneusly on both of them."}}