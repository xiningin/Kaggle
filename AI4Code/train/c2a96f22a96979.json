{"cell_type":{"ae071ee2":"code","7884a8f6":"code","58bf2073":"code","66e81af6":"code","b0f11e82":"code","d1efb6ab":"code","8d108cb9":"code","778650f3":"code","84a6889b":"code","da20b39a":"code","d5bf4e71":"code","acdaa376":"code","749e14d2":"code","fe08213e":"code","322c3f53":"code","e7bb85cb":"code","20964a21":"code","337d5eea":"code","d64d6444":"code","1ac37f2f":"code","f74446fa":"code","7e295c0d":"code","eccb092e":"code","234bceaa":"code","bf7c9597":"code","0cb36f06":"markdown","e6c2c4d4":"markdown","0a696bf5":"markdown","fcbde966":"markdown","669c0fe4":"markdown","736b0801":"markdown","384ad8cd":"markdown","c9a9d35a":"markdown","515f8258":"markdown","87fc8d0b":"markdown","3a16d2dd":"markdown","69275243":"markdown","18ce49ef":"markdown","8966bd71":"markdown","02b27e40":"markdown","0e30f557":"markdown","32739cef":"markdown"},"source":{"ae071ee2":"import pandas as pd\nimport numpy as np\n\nresults =  pd.DataFrame(np.array([['(Bayesian) SoftMax Regression',94.5, 95], \n                                  ['Decision Tree', 83.7, 88.4],\n                                  ['(Bayesian) 1 Parameter Log. Regression', 71.8, 69.9], \n                                  ['Random Forests', 71.4, 79.6],\n                                  ['PyTorch', np.nan, 63.6],\n                                  ['LGB', 50, 62.9]\n                                 ]),columns=['Model', 'Balanced Accuracy (%)', 'Accuracy (%)'])\n\nresults","7884a8f6":"import lightgbm as lgb\nfrom sklearn.metrics import mean_absolute_error, log_loss\nfrom sklearn import metrics\nfrom sklearn.model_selection import train_test_split\nimport category_encoders as ce\nimport torch\nimport torch.nn as nn\nfrom torch.autograd import Variable\nfrom torch.utils.data import DataLoader\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import MinMaxScaler\nimport pymc3 as pm\nimport theano\nimport theano.tensor as tt\nimport seaborn as sns\nimport arviz as az\nimport matplotlib.pyplot as plt\nfrom sklearn.metrics import accuracy_score, f1_score, confusion_matrix, balanced_accuracy_score\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.ensemble import RandomForestRegressor\n\nimport warnings\n\nwarnings.filterwarnings('ignore')\nwarnings.simplefilter('ignore')\n\nNFL1 = pd.read_csv(\"..\/input\/nfl-pbp\/pbp-2019.csv\")\nNFL2 = pd.read_csv(\"..\/input\/nfl-pbp\/pbp-2018.csv\")\nNFL3 = pd.read_csv(\"..\/input\/nfl-pbp\/pbp-2017.csv\")\nNFL = pd.concat([NFL2, NFL3, NFL1], axis=0)","58bf2073":"NFL = NFL.assign(outcome=(NFL['SeriesFirstDown'] == 'successful').astype(int))\ncat_features = ['Formation','PassType', 'PlayType', 'RushDirection'] \nencoder = ce.CountEncoder()\n\ncount_encoded = encoder.fit_transform(NFL[cat_features])\n\ndata = NFL[['SeriesFirstDown','ToGo', 'Down', 'YardLine', 'IsPass', 'IsRush', 'Yards', 'IsIncomplete']].join(count_encoded)\n\ninteraction1 = NFL1['Formation']+ \"_\" + NFL1['PlayType'] \n\nencoder = ce.CountEncoder()\ndata_interaction1 = data.assign(interact1=encoder.fit_transform(interaction1))\n\ndata_interaction1.head()","66e81af6":"corr = data_interaction1.corr()\nmask = np.tri(*corr.shape).T\nsns.set(rc={'figure.figsize':(9,7)})\nsns.heatmap(corr.abs(), mask=mask, annot=True, cmap='viridis');","b0f11e82":"df = data_interaction1[['ToGo', 'SeriesFirstDown']]\ndf = df[:1000]\n\ny_0 = pd.Categorical(df['SeriesFirstDown']).codes\nx_n = 'ToGo'\nx_0 = df[x_n].values\nx_c = x_0 - x_0.mean()","d1efb6ab":"with pm.Model() as model_0:\n    \u03b1 = pm.Uniform('\u03b1', lower=5, upper=10)\n    \u03b2 = pm.Normal('\u03b2', mu=8, sd=4)\n    \u03bc = \u03b1 + pm.math.dot(x_c, \u03b2)\n    \u03b8 = pm.Deterministic('\u03b8', pm.math.sigmoid(\u03bc))\n    bd = pm.Deterministic('bd', -\u03b1\/\u03b2)\n    yl = pm.Bernoulli('yl', p=\u03b8, observed=y_0)\n    trace_0 = pm.sample(1000)","8d108cb9":"theta = trace_0['\u03b8'].mean(axis=0)\nidx = np.argsort(x_c)\nplt.plot(x_c[idx], theta[idx], color='C2', lw=3)\nplt.vlines(trace_0['bd'].mean(), 0, 1, color='k')\nbd_hpd = az.hpd(trace_0['bd'])\nplt.fill_betweenx([0, 1], bd_hpd[0], bd_hpd[1], color='k', alpha=0.5)\nplt.scatter(x_c, np.random.normal(y_0, 0.02),\nmarker='.', color=[f'C{x}' for x in y_0])\naz.plot_hpd(x_c, trace_0['\u03b8'], color='C2')\nplt.xlabel(x_n)\nplt.ylabel('\u03b8', rotation=0)\n# use original scale for xticks\nlocs, _ = plt.xticks()\nplt.xticks(locs, np.round(locs + x_0.mean(), 1));","778650f3":"ppc = pm.sample_ppc(trace_0, model=model_0, samples=1000)\npreds = np.rint(ppc['yl'].mean(axis=0)).astype('int')\ncm = confusion_matrix(y_0, preds)","84a6889b":"print('Balanced Accuracy: ' + str(100*balanced_accuracy_score(preds, y_0)) + '%')\nprint('Accuracy: ' + str(100*accuracy_score(preds, y_0)) + '%')\nprint(' ')\nprint('Confusion Matrix:')\nprint(' ')\nprint(cm)","da20b39a":"df2 = NFL[['SeriesFirstDown','Yards','ToGo', 'IsIncomplete', 'Down']]\ndf2 = df2[:1000]\n\ny_1 = pd.Categorical(df2['SeriesFirstDown']).codes\nx_n = ['Yards','ToGo', 'IsIncomplete', 'Down']\nx_1 = df2[x_n].values\nx_c2 = (x_1 - x_1.mean(axis=0)) \/ x_1.std(axis=0)","d5bf4e71":"with pm.Model() as model_1:\n    \u03b1 = pm.Normal('\u03b1', mu=0, sd=5, shape=3)\n    \u03b2 = pm.Normal('\u03b2', mu=0, sd=5, shape=(4,3))\n    \u03bc = pm.Deterministic('\u03bc', \u03b1 + pm.math.dot(x_c2, \u03b2))\n    \u03b8 = tt.nnet.softmax(\u03bc)\n    yl = pm.Categorical('yl', p=\u03b8, observed=y_1)\n    trace_1 = pm.sample(2000)","acdaa376":"ppc2 = pm.sample_ppc(trace_1, model=model_1, samples=1000)\npreds2 = np.rint(ppc2['yl'].mean(axis=0)).astype('int')\ncm2 = confusion_matrix(y_1, preds2)","749e14d2":"print('Balanced Accuracy: ' + str(100*balanced_accuracy_score(preds2, y_1)) + '%')\nprint('Accuracy: ' + str(100*accuracy_score(preds2, y_1)) + '%')\nprint(' ')\nprint('Confusion Matrix:')\nprint(' ')\nprint(cm2)","fe08213e":"data2 = NFL[['SeriesFirstDown','IsIncomplete', 'ToGo', 'Down', 'Yards']]\nfeats = data2[[ 'IsIncomplete', 'ToGo', 'Down', 'Yards']]\n\nfeats.loc[(feats.ToGo >= 6),'ToGo']= 0\nfeats.loc[(feats.ToGo < 6),'ToGo']= 1\n\nfeats.loc[(feats.Down >= 3),'Down']= 0\nfeats.loc[(feats.Down < 3),'Down']= 1\n\nfeats.loc[(feats.Yards <= 5),'Yards']= 0\nfeats.loc[(feats.Yards > 5),'Yards']= 1\n\ntargs = data2[['SeriesFirstDown']]\ntargs = targs.to_numpy()\nfeats = feats.to_numpy()\n\nx_train2, x_test2, y_train2, y_test2 = train_test_split(feats,targs,test_size = 0.2,random_state = 42) \n\nfeaturesTrain = torch.from_numpy(x_train2)\ntargetsTrain = torch.from_numpy(y_train2).type(torch.LongTensor)\nfeaturesTest = torch.from_numpy(x_test2)\ntargetsTest = torch.from_numpy(y_test2).type(torch.LongTensor)\n\nbatch_size = 100\nn_iters = 10000\nnum_epochs = n_iters \/ (len(x_train2) \/ batch_size)\nnum_epochs = int(num_epochs)\n\ntrain2 = torch.utils.data.TensorDataset(featuresTrain,targetsTrain)\ntest2 = torch.utils.data.TensorDataset(featuresTest,targetsTest)\n\ntrain_loader = DataLoader(train2, batch_size = batch_size, shuffle = False)\ntest_loader = DataLoader(test2, batch_size = batch_size, shuffle = False)","322c3f53":"class LogisticRegressionModel(nn.Module):\n    def __init__(self, input_dim, output_dim):\n        super(LogisticRegressionModel, self).__init__()\n        self.linear = nn.Linear(input_dim, output_dim)\n    \n    def forward(self, x):\n        out = self.linear(x)\n        return out\n\ninput_dim = 4\noutput_dim = 1 \n\nmodel = LogisticRegressionModel(input_dim, output_dim)\n\nerror = nn.BCELoss()\n\nlearning_rate = 0.01\noptimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n","e7bb85cb":"for epoch in range(num_epochs):\n    for i, (images, labels) in enumerate(train_loader):\n        images = Variable(images.float())\n        labels = Variable(labels)\n        \n        optimizer.zero_grad()\n        outputs = model(images)\n        outputs2 = torch.round(torch.abs(outputs))\n\n        optimizer.step()        ","20964a21":"accuracy = []\n\nfor epoch in range(num_epochs):\n    correct = 0\n    total = 0\n    for images, labels in test_loader:\n        images = Variable(images.float())\n        outputs = model(images)\n        outputs2 = torch.round(torch.abs(outputs))\n        total += labels.size(0)\n        correct += (outputs2 == labels).sum()\n    \n        accuracy.append((100 * correct \/ total))","337d5eea":"print('Average Accuracy of the Model: ' + str(np.mean(accuracy)) + '%')","d64d6444":"valid_fraction = 0.1\nvalid_size = int(len(data)*valid_fraction)\n\ntrain = data_interaction1[:-2 * valid_size]\nvalid = data_interaction1[-2 * valid_size:-valid_size]\ntest = data_interaction1[-valid_size:]","1ac37f2f":"feature_cols = train.columns.drop('SeriesFirstDown')\n\ndtrain = lgb.Dataset(train[feature_cols], label=train['SeriesFirstDown'])\ndvalid = lgb.Dataset(valid[feature_cols], label=valid['SeriesFirstDown'])\n\nparam = {'num_leaves': 128, 'objective': 'binary', 'learning_rate': 0.01}\nparam['metric'] = 'binary_error'\nnum_round = 1000\nbst = lgb.train(param, dtrain, num_round, valid_sets=[dvalid], early_stopping_rounds=15, verbose_eval=False)","f74446fa":"ypred = np.rint(bst.predict(test[feature_cols]).astype(int))\n\nscore = metrics.accuracy_score(test['SeriesFirstDown'], ypred)\nscore2 = metrics.balanced_accuracy_score(test['SeriesFirstDown'], ypred)\n\nprint(f\"Accuracy: {100*score}\" + '%')\nprint(f\"Balanced Accuracy: {100*score2}\"+ '%')","7e295c0d":"X = NFL[['ToGo', 'Down', 'YardLine', 'IsPass', 'IsRush', 'Yards', 'IsIncomplete']] #.join(count_encoded)\ny = NFL['SeriesFirstDown']\n\nNFL_model = DecisionTreeRegressor(random_state=1)\nNFL_model.fit(X, y)\n\nprint(X.head(10))\nprint(\"Predictions:\")\nprint(NFL_model.predict(X.head(10)))\nprint(\"Actual Results:\")\nprint(y.head(10))","eccb092e":"predicted = np.rint(NFL_model.predict(X).astype(int))\n\nprint(\"Balanced Accuracy: \" + str(100*balanced_accuracy_score(y, predicted)) + '%')\nprint(\"Accuracy: \" + str(100*accuracy_score(y, predicted)) + '%')","234bceaa":"NFL_model_2 = RandomForestRegressor(random_state=1)\nNFL_model_2.fit(X,y)\npredicted_2 = np.rint(NFL_model_2.predict(X).astype(int))\n\nprint(\"Actual Results:\")\nprint(y.head(10))\nprint(\"Predictions:\")\nprint(predicted_2[:10])","bf7c9597":"print(\"Balanced Accuracy: \" + str(100*balanced_accuracy_score(y, predicted_2)) + '%')\nprint(\"Accuracy: \" + str(100*accuracy_score(y, predicted_2)) + '%')","0cb36f06":"# **Results**","e6c2c4d4":"# PyTorch","0a696bf5":"# **Imports & Data**","fcbde966":"The goal of this notebook is to experiment with feature engineering with NFL Savant's play-by-play data. I had previously tried to preform some elementary data analysis on this data; however, the data being largely categorical I found it difficult to study. Now, with various encoders I'll attempt to label the data and explore some simple statistics using Machine Learning.  \n\nThe problem I chose to tackle was: how can we use the data provided to predict whether or not a team will get a first down on a given set of downs?","669c0fe4":"# **Decision Tree**","736b0801":"Next, I'll try to add more features to improve the results of the Bayesian model. ","384ad8cd":"We'll start with the simplest model possible (using 1 feature) where the feature we'll use is the yards 'To Go' before a first down. As you can see below this is by far the feature most correlated to a First Down. Then we'll try to improve the model by adding other features. ","c9a9d35a":"# **LGB Model**","515f8258":"# PYMC3: Bayesian Logistic Regression","87fc8d0b":"# **Random Forests**","3a16d2dd":"Accuracy:","69275243":"I tackled this problem in 4 ways: Bayesian logistical regression, LGB binary model, Pytorch model and Decision Tree\/ Random forest model","18ce49ef":"That was fun; here you can 'imagine' several plays and their outcomes (i.e did the offense get a first down?)","8966bd71":"1 parameter logistic regression:","02b27e40":"Softmax logistic regression:","0e30f557":"To summarize the work below, here is a table with the accuracy scores for each model:","32739cef":"NFL play by play data is from NFL Savant: http:\/\/nflsavant.com\/about.php"}}