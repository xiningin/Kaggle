{"cell_type":{"557a46f0":"code","ddf9a16f":"code","9d63220e":"code","db8d3ba5":"code","34792776":"code","67d4210b":"code","9ec8a2ef":"code","d9c13f2f":"code","768aa462":"code","5184e929":"code","d6ad9595":"code","44d7145f":"code","0b64be52":"code","b0f70ae9":"code","af6396ff":"code","b574b973":"code","c7a7da4d":"code","1bd002e3":"code","461493e3":"markdown","8a4f2b39":"markdown","ed73393c":"markdown","752aef6d":"markdown","fdd3df34":"markdown","fe1a73b7":"markdown","1fd29a8e":"markdown","66e59904":"markdown","93b5172e":"markdown"},"source":{"557a46f0":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","ddf9a16f":"data = pd.read_csv('\/kaggle\/input\/iris\/Iris.csv')\ndata.head()","9d63220e":"data.drop('Id', axis = 1, inplace = True)","db8d3ba5":"data.describe()","34792776":"data.info()","67d4210b":"plt.scatter(data.PetalLengthCm, data.PetalWidthCm)\nplt.show()","9ec8a2ef":"sns.pairplot(data, hue = \"Species\")\nplt.plot()","d9c13f2f":"sns.countplot(data.Species)\nplt.show()","768aa462":"x = data.iloc[:,[2, 3]]\ny = data.iloc[:, 4]","5184e929":"plt.scatter(data.PetalLengthCm, data.PetalWidthCm, color = 'blue')\nplt.xlabel(\"PetalLengthCm\")\nplt.ylabel(\"PetalWidthCm\")\nplt.show()","d6ad9595":"from sklearn.cluster import KMeans\nwcss = []\n\nfor each in range(1, 15):\n    means = KMeans(n_clusters = each)\n    means.fit(x)\n    wcss.append(means.inertia_)\n    \nplt.plot(range(1, 15), wcss, color = 'blue')\nplt.xlabel(\"k values\")\nplt.ylabel(\"WCSS Scores\")\nplt.show()","44d7145f":"means3 = KMeans(n_clusters = 3)\nclusters = means3.fit_predict(x)\ndata[\"label\"] = clusters","0b64be52":"data.info()","b0f70ae9":"plt.scatter(data.PetalLengthCm[data.label == 0], data.PetalWidthCm[data.label == 0], color = 'blue')\nplt.scatter(data.PetalLengthCm[data.label == 1], data.PetalWidthCm[data.label == 1], color = 'red')\nplt.scatter(data.PetalLengthCm[data.label == 2], data.PetalWidthCm[data.label == 2], color = 'green')\nplt.xlabel(\"PetalLengthCm\")\nplt.ylabel(\"PetalWidthCm\")\nplt.show()","af6396ff":"x = data.iloc[:,0:4]\nx","b574b973":"from scipy.cluster.hierarchy import dendrogram, linkage\nmerg = linkage(x, method = 'ward')\ndendrogram(merg, leaf_rotation = 90)\nplt.xlabel(\"Data points\")\nplt.ylabel(\"Euclidean Distance\")\nplt.show()","c7a7da4d":"from sklearn.cluster import AgglomerativeClustering\n\nhierarchical_cluster = AgglomerativeClustering(n_clusters = 3,  affinity = \"euclidean\", linkage = \"ward\")\nclusters = hierarchical_cluster.fit_predict(x)\n\ndata[\"label\"] = clusters","1bd002e3":"plt.scatter(data.PetalLengthCm[data.label == 0], data.PetalWidthCm[data.label == 0], color = 'blue')\nplt.scatter(data.PetalLengthCm[data.label == 1], data.PetalWidthCm[data.label == 1], color = 'red')\nplt.scatter(data.PetalLengthCm[data.label == 2], data.PetalWidthCm[data.label == 2], color = 'green')\nplt.xlabel(\"PetalLengthCm\")\nplt.ylabel(\"PetalWidthCm\")\nplt.show()","461493e3":"<a id=\"3\"><\/a>\n## Hierarchical Clustering\n\nAgglomerative Hierarchical clustering Technique: In this technique, initially each data point is considered as an individual cluster. At each iteration, the similar clusters merge with other clusters until one cluster or K clusters are formed.\n\nThe basic algorithm of Agglomerative is straight forward.\n\n    Compute the proximity matrix\n    Let each data point be a cluster\n    Repeat: Merge the two closest clusters and update the proximity matrix\n    Until only a single cluster remains\n\nKey operation is the computation of the proximity of two clusters\n\n* The Hierarchical clustering Technique can be visualized using a Dendrogram.\n\n* A Dendrogram is a tree-like diagram that records the sequences of merges or splits.","8a4f2b39":"Ward Linkage Method\n\nThere are four methods for combining clusters in agglomerative approach. The one we choose to use is called Ward\u2019s Method. Unlike the others. Instead of measuring the distance directly, it analyzes the variance of clusters. Ward\u2019s is said to be the most suitable method for quantitative variables.\n\nWard\u2019s method says that the distance between two clusters, A and B, is how much the sum of squares will increase when we merge them:\n\n\u0394(A,B)=\u2211i\u2208A\u22c3B||xi\u2192\u2212m\u2192A\u22c3B||2\u2212\u2211i\u2208A||xi\u2192\u2212m\u2192A||2\u2212\u2211i\u2208B||xi\u2192\u2212m\u2192B||2=nAnBnA+nB||m\u2192A\u2212m\u2192B||2\n\nwhere m\u2192j\nis the center of cluster j, and nj is the number of points in it. \u0394\n\nis called the merging cost of combining the clusters A and B. With hierarchical clustering, the sum of squares starts out at zero (because every point is in its own cluster) and then grows as we merge clusters. Ward\u2019s method keeps this growth as small as possible.\n\nThe Euclidean distance is the \u201cordinary\u201d straight-line distance between two points in Euclidean space.\n","ed73393c":"Using the elbow rule, the optimal k value is 3.","752aef6d":"<a id=\"2\"><\/a>\n## K-means Clustering\n\n* A cluster refers to a collection of data points aggregated together because of certain similarities.\n* Target number k, which refers to the number of centroids you need in the dataset. A centroid is the imaginary or real location representing the center of the cluster.\n* Every data point is allocated to each of the clusters through reducing the in-cluster sum of squares.","fdd3df34":"# INTRODUCTION\n\nIn this kernel, I will make two unsupervised learning algorithms. K-means clustering and Hierarchical clustering.\n\nLet's begin with what is unsupervised? In supervised learning, we have labels in our data and we can calculate the accuracy of the model with using predicted labels and real labels. In unsupervised learning, there is no label in data. Algorithms make only clusters of the data.\n\n1. [Load and Inspect Data](#1)\n1. [K-means Clustering](#2)\n1. [Hierarchical Clustering](#3)","fe1a73b7":"<a id=\"1\"><\/a>\n## Load and Inspect Data","1fd29a8e":"First we try to find and optimal value of 'k'. When we try to find it we use WCSS : Within Cluster Sum of Squares","66e59904":"We won't use the 'id' column so, we can drop it.","93b5172e":"This data is balanced."}}