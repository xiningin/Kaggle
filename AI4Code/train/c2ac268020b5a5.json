{"cell_type":{"14f1a757":"code","58a16103":"code","92899831":"code","4e0e8dbc":"code","b7679c15":"code","5e469bb1":"code","bf7b2a09":"code","c434b25e":"code","a399adee":"markdown","fd29299f":"markdown","800b7ed3":"markdown","f193d236":"markdown","f349c9fb":"markdown","0fb09b86":"markdown","ddab975a":"markdown","89385dff":"markdown","739b1811":"markdown","e4e6ef65":"markdown","576c14ad":"markdown","7e9dd323":"markdown","3c745ffe":"markdown","786000b4":"markdown","fd55df62":"markdown"},"source":{"14f1a757":"# Dependencies\n\n!pip install --no-deps '..\/input\/timm-package\/timm-0.1.26-py3-none-any.whl' > \/dev\/null\n!pip install --no-deps '..\/input\/pycocotools\/pycocotools-2.0-cp37-cp37m-linux_x86_64.whl' > \/dev\/null\n\n\n\n\n\nimport sys\nsys.path.insert(0, \"..\/input\/timm-efficientdet-pytorch\")\nsys.path.insert(0, \"..\/input\/omegaconf\")\nsys.path.insert(0, \"..\/input\/weightedboxesfusion\")\n\n\n\nimport ensemble_boxes\nimport torch\nimport numpy as np\nimport pandas as pd\nfrom glob import glob\nfrom torch.utils.data import Dataset,DataLoader\nimport albumentations as A\nfrom albumentations.pytorch.transforms import ToTensorV2\nimport cv2\nimport gc\nfrom matplotlib import pyplot as plt\nfrom effdet import get_efficientdet_config, EfficientDet, DetBenchEval\nfrom effdet.efficientdet import HeadNet\n\n\n\n\ndef get_valid_transforms():\n    return A.Compose([\n            A.Resize(height=512, width=512, p=1.0),\n            ToTensorV2(p=1.0),\n        ], p=1.0)\n\n\n\n\nDATA_ROOT_PATH = '..\/input\/global-wheat-detection\/test'\n\n\n\n\nclass DatasetRetriever(Dataset):\n\n    def __init__(self, image_ids, transforms=None):\n        super().__init__()\n        self.image_ids = image_ids\n        self.transforms = transforms\n\n    def __getitem__(self, index: int):\n        image_id = self.image_ids[index]\n        image = cv2.imread(f'{DATA_ROOT_PATH}\/{image_id}.jpg', cv2.IMREAD_COLOR)\n        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB).astype(np.float32)\n        image \/= 255.0\n        image_size, width, _ = image.shape\n        if self.transforms:\n            sample = {'image': image}\n            sample = self.transforms(**sample)\n            image = sample['image']\n        return image, image_id, image_size\n\n    def __len__(self) -> int:\n        return self.image_ids.shape[0]\n    \n    \n    \n    \n    \ndataset = DatasetRetriever(\n    image_ids=np.array([path.split('\/')[-1][:-4] for path in glob(f'{DATA_ROOT_PATH}\/*.jpg')]),\n    transforms=get_valid_transforms()\n)\n\n\n\n\ndef collate_fn(batch):\n    return tuple(zip(*batch))\n\n\n\n\n\ndata_loader = DataLoader(\n    dataset,\n    batch_size=4,\n    shuffle=False,\n    num_workers=2,\n    drop_last=False,\n    collate_fn=collate_fn\n)\n\n\nclass BaseWheatTTA:\n    \"\"\" author: @shonenkov \"\"\"\n    image_size = 512\n\n    def augment(self, image):\n        raise NotImplementedError\n    \n    def batch_augment(self, images):\n        raise NotImplementedError\n    \n    def deaugment_boxes(self, boxes):\n        raise NotImplementedError\n\nclass TTAContrastBrightness(BaseWheatTTA):\n    \"\"\" author: @ffares \"\"\"\n\n    def augment(self, image):\n        alpha = round(random.uniform(1, 1.3),2) # Contrast control\/\/ alpha value [1.0-3.0]\n        beta = round(random.uniform(0, 0.2),2)  # Brightness control\/\/ beta value [0-100]\n        return torch.clamp(torch.add(torch.mul(image,alpha),beta),0, 1, out=None)\n    \n    def batch_augment(self, images):\n        alpha = round(random.uniform(1, 1.3),2) # Contrast control\/\/ alpha value [1.0-3.0]\n        beta = round(random.uniform(0, 0.2),2)  # Brightness control\/\/ beta value [0-100]\n        return torch.clamp(torch.add(torch.mul(images,alpha),beta),0, 255, out=None)\n    \n    def deaugment_boxes(self, boxes):\n        return boxes\n        \nclass TTAHorizontalFlip(BaseWheatTTA):\n    \"\"\" author: @shonenkov \"\"\"\n\n    def augment(self, image):\n        return image.flip(1)\n    \n    def batch_augment(self, images):\n        return images.flip(2)\n    \n    def deaugment_boxes(self, boxes):\n        boxes[:, [1,3]] = self.image_size - boxes[:, [3,1]]\n        return boxes\n\nclass TTAVerticalFlip(BaseWheatTTA):\n    \"\"\" author: @shonenkov \"\"\"\n    \n    def augment(self, image):\n        return image.flip(2)\n    \n    def batch_augment(self, images):\n        return images.flip(3)\n    \n    def deaugment_boxes(self, boxes):\n        boxes[:, [0,2]] = self.image_size - boxes[:, [2,0]]\n        return boxes\n    \nclass TTARotate90(BaseWheatTTA):\n    \"\"\" author: @shonenkov \"\"\"\n    \n    def augment(self, image):\n        return torch.rot90(image, 1, (1, 2))\n\n    def batch_augment(self, images):\n        return torch.rot90(images, 1, (2, 3))\n    \n    def deaugment_boxes(self, boxes):\n        res_boxes = boxes.copy()\n        res_boxes[:, [0,2]] = self.image_size - boxes[:, [1,3]]\n        res_boxes[:, [1,3]] = boxes[:, [2,0]]\n        return res_boxes\n    \nclass TTARotate180(BaseWheatTTA):\n    \"\"\" author: @shonenkov \"\"\"\n    \n    def augment(self, image):\n        return torch.rot90(image, 2, (1, 2))\n\n    def batch_augment(self, images):\n        return torch.rot90(images, 2, (2, 3))\n    \n    def deaugment_boxes(self, boxes):\n        boxes[:, [0,1,2,3]] = self.image_size - boxes[:, [2,3,0,1]]\n        return boxes\n    \nclass TTARotate270(BaseWheatTTA):\n    \"\"\" author: @shonenkov \"\"\"\n    \n    def augment(self, image):\n        return torch.rot90(image, 3, (1, 2))\n\n    def batch_augment(self, images):\n        return torch.rot90(images, 3, (2, 3))\n    \n    def deaugment_boxes(self, boxes):\n        res_boxes = boxes.copy()\n        res_boxes[:, [0,2]] = boxes[:, [1,3]]\n        res_boxes[:, [1,3]] = self.image_size - boxes[:, [2,0]]\n        return res_boxes\n    \nclass TTACompose(BaseWheatTTA):\n    \"\"\" author: @shonenkov \"\"\"\n    def __init__(self, transforms):\n        self.transforms = transforms\n        \n    def augment(self, image):\n        for transform in self.transforms:\n            image = transform.augment(image)\n        return image\n    \n    def batch_augment(self, images):\n        for transform in self.transforms:\n            images = transform.batch_augment(images)\n        return images\n    \n    def prepare_boxes(self, boxes):\n        result_boxes = boxes.copy()\n        result_boxes[:,0] = np.min(boxes[:, [0,2]], axis=1)\n        result_boxes[:,2] = np.max(boxes[:, [0,2]], axis=1)\n        result_boxes[:,1] = np.min(boxes[:, [1,3]], axis=1)\n        result_boxes[:,3] = np.max(boxes[:, [1,3]], axis=1)\n        return result_boxes\n    \n    def deaugment_boxes(self, boxes):\n        for transform in self.transforms[::-1]:\n            boxes = transform.deaugment_boxes(boxes)\n        return self.prepare_boxes(boxes)\n    \n    \n    \nfrom itertools import product\n\ntta_transforms = []\nfor tta_combination in product([TTAHorizontalFlip(), None], \n                               [TTAVerticalFlip(), None],\n                               [TTARotate90(), None]):\n    tta_transforms.append(TTACompose([tta_transform for tta_transform in tta_combination if tta_transform]))\n    \n\ndef load_net5(checkpoint_path):\n    config = get_efficientdet_config('tf_efficientdet_d5')\n    net = EfficientDet(config, pretrained_backbone=False)\n\n    config.num_classes = 1\n    config.image_size=512\n    net.class_net = HeadNet(config, num_outputs=config.num_classes, norm_kwargs=dict(eps=.001, momentum=.01))\n\n    checkpoint = torch.load(checkpoint_path)\n    net.load_state_dict(checkpoint['model_state_dict'])\n\n    del checkpoint\n    gc.collect()\n\n    net = DetBenchEval(net, config)\n    net.eval();\n    return net.cuda()\n\ndef load_net7(checkpoint_path):\n    config = get_efficientdet_config('tf_efficientdet_d7')\n    net = EfficientDet(config, pretrained_backbone=False)\n\n    config.num_classes = 1\n    config.image_size=512\n    net.class_net = HeadNet(config, num_outputs=config.num_classes, norm_kwargs=dict(eps=.001, momentum=.01))\n\n    checkpoint = torch.load(checkpoint_path)\n    net.load_state_dict(checkpoint['model_state_dict'])\n\n    del checkpoint\n    gc.collect()\n\n    net = DetBenchEval(net, config)\n    net.eval();\n    return net.cuda()\n\n\n# Models\nmodels =[    \n    #With Cleaning\/ Old Version fold 0\n    #load_net5('..\/input\/efficientdetearlierversionf0\/best-checkpoint-047epoch.bin'), \n    #With Cleaning\/ New Version fold 0\n    load_net5('..\/input\/efficientdet5f0\/best-checkpoint-028epoch.bin'), \n    #With CLEANING fold 1 fine tuned on half arvalis\n    #load_net5('..\/input\/efficientdet5f1finetunearv2\/best-checkpoint-001epoch.bin'), \n    #Without Cleaning fold 3\n    load_net7('..\/input\/training-efficientdet-f3\/effdet5-cutmix-augmix\/best-checkpoint-033epoch.bin'),\n    #Without Cleaning fold 4\n    #load_net7('..\/input\/training-efficientdet-f4\/effdet5-cutmix-augmix\/best-checkpoint-037epoch.bin'), \n]\n    \n\ndef make_predictions(images, score_threshold=0.1):\n    images = torch.stack(images).cuda().float()\n    predictions = []\n    with torch.no_grad():\n        det = net(images, torch.tensor([1]*images.shape[0]).float().cuda())\n        for i in range(images.shape[0]):\n            boxes = det[i].detach().cpu().numpy()[:,:4]    \n            scores = det[i].detach().cpu().numpy()[:,4]\n            indexes = np.where(scores > score_threshold)[0]\n            boxes = boxes[indexes]\n            boxes[:, 2] = boxes[:, 2] + boxes[:, 0]\n            boxes[:, 3] = boxes[:, 3] + boxes[:, 1]\n            predictions.append({\n                'boxes': boxes[indexes],\n                'scores': scores[indexes],\n            })\n    return [predictions]\n  \n    \ndef make_tta_predictions(images, score_threshold=0.1):\n    with torch.no_grad():\n        images = torch.stack(images).float().cuda()\n        predictions = []\n        for tta_transform in tta_transforms:\n            result = []\n            det = net(tta_transform.batch_augment(images.clone()), torch.tensor([1]*images.shape[0]).float().cuda())\n\n            for i in range(images.shape[0]):\n                boxes = det[i].detach().cpu().numpy()[:,:4]    \n                scores = det[i].detach().cpu().numpy()[:,4]\n                indexes = np.where(scores > score_threshold)[0]\n                boxes = boxes[indexes]\n                boxes[:, 2] = boxes[:, 2] + boxes[:, 0]\n                boxes[:, 3] = boxes[:, 3] + boxes[:, 1]\n                boxes = tta_transform.deaugment_boxes(boxes.copy())\n                result.append({\n                    'boxes': boxes,\n                    'scores': scores[indexes],\n                })\n            predictions.append(result)\n    return predictions\n\n\ndef make_tta_models_predictions(images, score_threshold=0.1):\n    images = torch.stack(images).float().cuda()\n    with torch.no_grad():\n        predictions = []\n        for tta_transform in tta_transforms:\n            for net in models:\n                result = []\n                det = net(tta_transform.batch_augment(images.clone()), torch.tensor([1]*images.shape[0]).float().cuda())\n                for i in range(images.shape[0]):\n                    boxes = det[i].detach().cpu().numpy()[:,:4]    \n                    scores = det[i].detach().cpu().numpy()[:,4]\n                    indexes = np.where(scores > score_threshold)[0]\n                    boxes = boxes[indexes]\n                    boxes[:, 2] = boxes[:, 2] + boxes[:, 0]\n                    boxes[:, 3] = boxes[:, 3] + boxes[:, 1]\n                    boxes = tta_transform.deaugment_boxes(boxes.copy())\n                    \n                    result.append({\n                        'boxes': boxes,\n                        'scores': scores[indexes],\n                    })\n                predictions.append(result)\n    return predictions\n\ndef make_models_predictions(images, score_threshold=0.1):\n    images = torch.stack(images).cuda().float()\n    predictions = []\n    for net in models:\n        with torch.no_grad():\n            det = net(images, torch.tensor([1]*images.shape[0]).float().cuda())\n            result = []\n            for i in range(images.shape[0]):\n                boxes = det[i].detach().cpu().numpy()[:,:4]    \n                scores = det[i].detach().cpu().numpy()[:,4]\n                indexes = np.where(scores > score_threshold)[0]\n                boxes = boxes[indexes]\n                boxes[:, 2] = boxes[:, 2] + boxes[:, 0]\n                boxes[:, 3] = boxes[:, 3] + boxes[:, 1]\n                result.append({\n                    'boxes': boxes[indexes],\n                    'scores': scores[indexes],\n                })\n            predictions.append(result)\n    return predictions\n\n\n# Inference \ndef format_prediction_string(boxes, scores):\n    pred_strings = []\n    for j in zip(scores, boxes):\n        pred_strings.append(\"{0:.4f} {1} {2} {3} {4}\".format(j[0], j[1][0], j[1][1], j[1][2], j[1][3]))\n    return \" \".join(pred_strings)\n\n\n\ndef resize_predicitions(predictions): \n    '''\n    Resize bboxes of efficient det as it predicts bboxes in the range of 512 \n    We need to double the predicitions\n    '''\n    for i in range(len(predictions)):\n        for j in range(len(predictions[i])):\n            predictions[i][j]['boxes']=predictions[i][j]['boxes']*2\n        \n    return predictions\n\n\ndef effdet_organize(predictions, image_index):\n    \n    boxes = [(prediction[image_index]['boxes']).tolist() for prediction in predictions]\n    scores = [prediction[image_index]['scores'].tolist() for prediction in predictions]\n    labels = [np.ones(prediction[image_index]['scores'].shape[0]).astype(int).tolist() for prediction in predictions]\n        \n    return boxes, scores, labels\n\n\ndef run_wbf_ensemble(boxes_effdet, boxes_yolo, scores_effdet, scores_yolo, labels_effdet, labels_yolo, iou_thr=0.6, skip_box_thr=0.5, weights=None):    \n        \n    boxes = boxes_effdet + boxes_yolo\n    scores = scores_effdet + scores_yolo\n    labels = labels_effdet + labels_yolo\n    \n    boxes, scores, labels = ensemble_boxes.ensemble_boxes_wbf.weighted_boxes_fusion(boxes, scores, labels, weights=weights, iou_thr=iou_thr, skip_box_thr=skip_box_thr)\n    return boxes, scores, labels\n\n\n\ndef run_wbf(boxes, scores, labels, iou_thr=0.5, skip_box_thr=0.3, weights=None):    \n    boxes, scores, labels = ensemble_boxes.ensemble_boxes_wbf.weighted_boxes_fusion(boxes, scores, labels, weights=weights, iou_thr=iou_thr, skip_box_thr=skip_box_thr)\n    return boxes, scores, labels  \n\n\ndef run_wbf_initial(boxes,scores, image_size=1024, iou_thr=0.41, skip_box_thr=0.4, weights=None):\n    labels0 = [np.ones(len(scores[idx])) for idx in range(len(scores))]\n    boxes, scores, labels = ensemble_boxes.ensemble_boxes_wbf.weighted_boxes_fusion(boxes, scores, labels0, weights=None, iou_thr=iou_thr, skip_box_thr=skip_box_thr)\n    return boxes, scores, labels","58a16103":"for images, image_ids, image_size in data_loader:\n    \n    #predictions = make_tta_predictions(images)\n    predictions = make_tta_models_predictions(images)\n    #predictions = make_predictions(images)\n    predictions=resize_predicitions(predictions)\n\n    for i, image in enumerate(images):\n        \n        image_id=image_ids[i]\n\n        \n        # boxes using efficientdet5\n        boxes_effdet, scores_effdet, labels_effdet = effdet_organize(predictions, image_index=i)\n        \n        boxes, scores, labels = run_wbf(boxes_effdet, scores_effdet, labels_effdet, iou_thr=0.4, skip_box_thr=0.43, weights=None)\n\n        boxes = (boxes\/2).round().astype(np.int32).clip(min=0, max=512)\n\n    \n    \n        fig, ax = plt.subplots(1, 1, figsize=(10, 10))\n    \n    \n        sample = images[i].permute(1,2,0).cpu().numpy()\n    \n        font = cv2.FONT_HERSHEY_SIMPLEX \n    \n        # fontScale \n        fontScale = 1\n\n        # Blue color in RGB \n        color = (0, 0, 1) \n\n        # Line thickness of 2 px \n        thickness = 2\n\n        for box,score in zip(boxes,scores):\n            cv2.rectangle(sample, (box[0], box[1]), (box[2], box[3]), (0, 0, 1), 1)\n            cv2.putText(sample, '{:.2}'.format(score), (box[0]+np.random.randint(20),box[1]), font, fontScale, color, thickness, cv2.LINE_AA)\n    \n        ax.set_axis_off()\n        ax.imshow(sample);\n    \n        break\n    break","92899831":"import torch.nn.functional as F\n\n\ndef get_valid_transforms_1024():\n    return A.Compose([\n            A.Resize(height=1024, width=1024, p=1.0),\n            ToTensorV2(p=1.0),\n        ], p=1.0)\n\n    \ndataset_1024 = DatasetRetriever(\n    image_ids=np.array([path.split('\/')[-1][:-4] for path in glob(f'{DATA_ROOT_PATH}\/*.jpg')]),\n    transforms=get_valid_transforms_1024()\n)\n\n\n\ndata_loader_1024 = DataLoader(\n    dataset_1024,\n    batch_size=1,\n    shuffle=False,\n    num_workers=0,\n    drop_last=False,\n    collate_fn=collate_fn\n)\n\n\nclass BaseWheatTTA_1024:\n    \"\"\" author: @shonenkov \"\"\"\n    image_size = 1024\n\n    def augment(self, image):\n        raise NotImplementedError\n    \n    def batch_augment(self, images):\n        raise NotImplementedError\n    \n    def deaugment_boxes(self, boxes):\n        raise NotImplementedError\n\n        \nclass TTAHorizontalFlip_1024(BaseWheatTTA_1024):\n    \"\"\" author: @shonenkov \"\"\"\n\n    def augment(self, image):\n        return image.flip(1)\n    \n    def batch_augment(self, images):\n        return images.flip(2)\n    \n    def deaugment_boxes(self, boxes):\n        boxes[:, [1,3]] = self.image_size - boxes[:, [3,1]]\n        return boxes\n\nclass TTAVerticalFlip_1024(BaseWheatTTA_1024):\n    \"\"\" author: @shonenkov \"\"\"\n    \n    def augment(self, image):\n        return image.flip(2)\n    \n    def batch_augment(self, images):\n        return images.flip(3)\n    \n    def deaugment_boxes(self, boxes):\n        boxes[:, [0,2]] = self.image_size - boxes[:, [2,0]]\n        return boxes\n    \nclass TTARotate90_1024(BaseWheatTTA_1024):\n    \"\"\" author: @shonenkov \"\"\"\n    \n    def augment(self, image):\n        return torch.rot90(image, 1, (1, 2))\n\n    def batch_augment(self, images):\n        return torch.rot90(images, 1, (2, 3))\n    \n    def deaugment_boxes(self, boxes):\n        res_boxes = boxes.copy()\n        res_boxes[:, [0,2]] = self.image_size - boxes[:, [1,3]]\n        res_boxes[:, [1,3]] = boxes[:, [2,0]]\n        return res_boxes\n    \nclass resize1024to512(BaseWheatTTA_1024):\n    \"\"\" author: @ffares \"\"\"\n    \n    def augment(self, image):\n        return F.interpolate(image, size=(512,512))\n\n    def batch_augment(self, images):\n        return F.interpolate(images, size=(512,512))\n    \n    def deaugment_boxes(self, boxes):\n        return boxes\n    \n    \nclass TTACompose_1024(BaseWheatTTA_1024):\n    \"\"\" author: @shonenkov \"\"\"\n    def __init__(self, transforms):\n        self.transforms = transforms\n        \n    def augment(self, image):\n        for transform in self.transforms:\n            image = transform.augment(image)\n        return image\n    \n    def batch_augment(self, images):\n        for transform in self.transforms:\n            images = transform.batch_augment(images)\n        return images\n    \n    def prepare_boxes(self, boxes):\n        result_boxes = boxes.copy()\n        result_boxes[:,0] = np.min(boxes[:, [0,2]], axis=1)\n        result_boxes[:,2] = np.max(boxes[:, [0,2]], axis=1)\n        result_boxes[:,1] = np.min(boxes[:, [1,3]], axis=1)\n        result_boxes[:,3] = np.max(boxes[:, [1,3]], axis=1)\n        return result_boxes\n    \n    def deaugment_boxes(self, boxes):\n        for transform in self.transforms[::-1]:\n            boxes = transform.deaugment_boxes(boxes)\n        return self.prepare_boxes(boxes)\n    \n    \n    \ntta_transforms_1024 = []\nfor tta_combination in product([TTAHorizontalFlip_1024(), None], \n                               [TTAVerticalFlip_1024(), None],\n                               [TTARotate90_1024(), None]):\n    tta_transforms_1024.append(TTACompose_1024([tta_transform for tta_transform in tta_combination if tta_transform]))\n    \n\ndef load_net5_1024(checkpoint_path):\n    config = get_efficientdet_config('tf_efficientdet_d5')\n    net = EfficientDet(config, pretrained_backbone=False)\n\n    config.num_classes = 1\n    config.image_size=1024\n    net.class_net = HeadNet(config, num_outputs=config.num_classes, norm_kwargs=dict(eps=.001, momentum=.01))\n\n    checkpoint = torch.load(checkpoint_path)\n    net.load_state_dict(checkpoint['model_state_dict'])\n\n    del checkpoint\n    gc.collect()\n\n    net = DetBenchEval(net, config)\n    net.eval();\n    return net.cuda()\n\n\n\n# Models\nmodels_1024 =[load_net5_1024('..\/input\/wheatweightsefficientdet51024v4\/best-checkpoint-006epoch.bin')]\n    \n\n\ndef make_tta_models_predictions_1024(images, score_threshold=0.1):\n    images = torch.stack(images).float().cuda()\n    with torch.no_grad():\n        predictions = []\n        for tta_transform in tta_transforms_1024:\n            for net in models_1024:\n                result = []\n                det = net(tta_transform.batch_augment(images.clone()), torch.tensor([1]*images.shape[0]).float().cuda())\n                for i in range(images.shape[0]):\n                    boxes = det[i].detach().cpu().numpy()[:,:4]    \n                    scores = det[i].detach().cpu().numpy()[:,4]\n                    indexes = np.where(scores > score_threshold)[0]\n                    boxes = boxes[indexes]\n                    boxes[:, 2] = boxes[:, 2] + boxes[:, 0]\n                    boxes[:, 3] = boxes[:, 3] + boxes[:, 1]\n                    boxes = tta_transform.deaugment_boxes(boxes.copy())\n                    \n                    result.append({\n                        'boxes': boxes,\n                        'scores': scores[indexes],\n                    })\n                predictions.append(result)\n    return predictions\n\n","4e0e8dbc":"for images, image_ids, image_size in data_loader_1024:\n    \n    predictions_1024 = make_tta_models_predictions_1024(images)\n\n\n    for i, image in enumerate(images):\n        \n        image_id=image_ids[i]\n\n        \n        # boxes using efficientdet5\n        boxes_effdet, scores_effdet, labels_effdet = effdet_organize(predictions_1024, image_index=i)\n        \n        boxes, scores, labels = run_wbf(boxes_effdet, scores_effdet, labels_effdet, iou_thr=0.4, skip_box_thr=0.43, weights=None)\n\n        boxes = (boxes).round().astype(np.int32).clip(min=0, max=1023)\n\n    \n    \n        fig, ax = plt.subplots(1, 1, figsize=(10, 10))\n    \n    \n        sample = images[i].permute(1,2,0).cpu().numpy()\n    \n        font = cv2.FONT_HERSHEY_SIMPLEX \n    \n        # fontScale \n        fontScale = 1\n\n        # Blue color in RGB \n        color = (0, 0, 1) \n\n        # Line thickness of 2 px \n        thickness = 2\n\n        for box,score in zip(boxes,scores):\n            cv2.rectangle(sample, (box[0], box[1]), (box[2], box[3]), (0, 0, 1), 1)\n            cv2.putText(sample, '{:.2}'.format(score), (box[0]+np.random.randint(20),box[1]), font, fontScale, color, thickness, cv2.LINE_AA)\n    \n        ax.set_axis_off()\n        ax.imshow(sample);\n    \n        break\n    break","b7679c15":"tta_transforms_512 = []\nfor tta_combination in product([resize1024to512()],\n                               [TTAHorizontalFlip(), None], \n                               [TTAVerticalFlip(), None],\n                               [TTARotate90(), None]):\n    tta_transforms_512.append(TTACompose([tta_transform for tta_transform in tta_combination if tta_transform]))\n    \ndef make_tta_models_predictions_512(images, score_threshold=0.1):\n    images = torch.stack(images).float().cuda()\n    with torch.no_grad():\n        predictions = []\n        for tta_transform in tta_transforms_512:\n            for net in models:\n                result = []\n                det = net(tta_transform.batch_augment(images.clone()), torch.tensor([1]*images.shape[0]).float().cuda())\n                for i in range(images.shape[0]):\n                    boxes = det[i].detach().cpu().numpy()[:,:4]    \n                    scores = det[i].detach().cpu().numpy()[:,4]\n                    indexes = np.where(scores > score_threshold)[0]\n                    boxes = boxes[indexes]\n                    boxes[:, 2] = boxes[:, 2] + boxes[:, 0]\n                    boxes[:, 3] = boxes[:, 3] + boxes[:, 1]\n                    boxes = tta_transform.deaugment_boxes(boxes.copy())\n                    \n                    result.append({\n                        'boxes': boxes,\n                        'scores': scores[indexes],\n                    })\n                predictions.append(result)\n    return predictions\n","5e469bb1":"results = []\n\nfor images, image_ids, image_sizes in data_loader_1024:\n    \n    predictions_1024 = make_tta_models_predictions_1024(images)\n    \n    predictions_512 = make_tta_models_predictions_512(images)\n    predictions_512= resize_predicitions(predictions_512)\n    \n    for i, image in enumerate(images):\n        \n        image_id=image_ids[i]\n        image_size= image_sizes[i]\n        \n        \n        # EfficientDet predictions\n        boxes_effdet_1024, scores_effdet_1024, labels_effdet_1024 = effdet_organize(predictions_1024, image_index=i)\n                \n        \n        # EfficientDet predictions\n        boxes_effdet_512, scores_effdet_512, labels_effdet_512 = effdet_organize(predictions_512, image_index=i)\n        \n        \n        # Fusion of both predictions\n        boxes, scores, labels = run_wbf_ensemble(boxes_effdet_1024, boxes_effdet_512, scores_effdet_1024, scores_effdet_512, labels_effdet_1024, \n                                                 labels_effdet_512, iou_thr=0.4, skip_box_thr=0.43, weights=None)\n        \n        boxes = (boxes*(image_size\/1024)).round().astype(np.int32).clip(min=0, max=image_size-1)\n        \n        \n        \n        boxes[:, 2] = boxes[:, 2] - boxes[:, 0]\n        boxes[:, 3] = boxes[:, 3] - boxes[:, 1]\n\n        result = {\n            'image_id': image_id,\n            'PredictionString': format_prediction_string(boxes, scores)\n        }\n        results.append(result)\n        ","bf7b2a09":"test_df = pd.DataFrame(results, columns=['image_id', 'PredictionString'])\ntest_df.to_csv('submission.csv', index=False)\ntest_df.head(10)","c434b25e":"for images, image_ids, image_sizes in data_loader_1024:\n    \n    predictions_1024 = make_tta_models_predictions_1024(images)\n    \n    predictions_512 = make_tta_models_predictions_512(images)\n    predictions_512= resize_predicitions(predictions_512)\n    \n    for i, image in enumerate(images):\n        \n        image_id=image_ids[i]\n        image_size= image_sizes[i]\n        \n        \n        # EfficientDet predictions\n        boxes_effdet_1024, scores_effdet_1024, labels_effdet_1024 = effdet_organize(predictions_1024, image_index=i)\n                \n        \n        # EfficientDet predictions\n        boxes_effdet_512, scores_effdet_512, labels_effdet_512 = effdet_organize(predictions_512, image_index=i)\n        \n\n        # Fusion of both predictions\n        boxes, scores, labels = run_wbf_ensemble(boxes_effdet_1024, boxes_effdet_512, scores_effdet_1024, scores_effdet_512, labels_effdet_1024, \n                                                 labels_effdet_512, iou_thr=0.4, skip_box_thr=0.43, weights=None)\n        \n        boxes = (boxes*(image_size\/1024)).round().astype(np.int32).clip(min=0, max=image_size-1)\n\n    \n    \n        fig, ax = plt.subplots(1, 1, figsize=(10, 10))\n    \n    \n        sample = images[i].permute(1,2,0).cpu().numpy()\n    \n        font = cv2.FONT_HERSHEY_SIMPLEX \n    \n        # fontScale \n        fontScale = 1\n\n        # Blue color in RGB \n        color = (0, 0, 1) \n\n        # Line thickness of 2 px \n        thickness = 2\n\n        for box,score in zip(boxes,scores):\n            cv2.rectangle(sample, (box[0], box[1]), (box[2], box[3]), (0, 0, 1), 1)\n            cv2.putText(sample, '{:.2}'.format(score), (box[0]+np.random.randint(20),box[1]), font, fontScale, color, thickness, cv2.LINE_AA)\n    \n        ax.set_axis_off()\n        ax.imshow(sample);\n    \n        break\n    break","a399adee":"![image.png](attachment:image.png)","fd29299f":"## EfficientDet5 1024x1024","800b7ed3":"Object detection is one of the most important topics of computer vision since it has many applications in several fields. One application of it is this amazing challenge.\n\nObject detection models can be improved thanks to ensemble techniques.\n\nHowever, the process of ensembling object detectors poses\nseveral challenges including the selection of models but most importantly the way of ensembling itself. Because different models have different output types with different confidence range which requires some 'work'. \n\nHere we ensembled EfficientDet models.","f193d236":"## EfficientDet Architecture ","f349c9fb":"## EfficientDet 512x512","0fb09b86":"## EfficientDet ModelFlops vs COCO accuracy","ddab975a":"![image.png](attachment:image.png)","89385dff":"![image.png](attachment:image.png)","739b1811":"# Global Wheat Head Detection ","e4e6ef65":"## Ensemble different EfficientDet Models\n### Different architectures (Different Input Size, Different Backbone Networks, Layers)\n### Different folds & using different augmentations\n","576c14ad":"## Thank you for reading my kernel!","7e9dd323":"![image.png](attachment:image.png)","3c745ffe":"## Ensemble ALL models","786000b4":"# Introduction","fd55df62":"# EfficientDet5"}}