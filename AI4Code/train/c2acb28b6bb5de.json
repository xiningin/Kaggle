{"cell_type":{"ee459b2b":"code","34855d7f":"code","6db349b9":"code","6bb6ad16":"code","0dc1f4c4":"code","06ab3a21":"code","773f908e":"code","69acbac0":"code","6c036e7d":"code","787206a9":"code","d0c833b6":"code","2b904b3f":"code","83df622e":"code","aee0cb1c":"code","cecda2f3":"code","96740163":"code","a5477439":"code","c24767b3":"code","018e412f":"code","86d2033b":"markdown","bad84412":"markdown","ce42c9b7":"markdown","3ea9bd0b":"markdown","58f822fe":"markdown","7973e215":"markdown","c2c26084":"markdown","445b6f2f":"markdown","b20fd31f":"markdown","1f639aea":"markdown"},"source":{"ee459b2b":"import pandas as pd # for data analytics\nimport numpy as np # for numerical computation\nfrom sklearn.metrics import precision_recall_fscore_support, classification_report,confusion_matrix, precision_recall_curve\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn import utils  \nfrom sklearn import svm\nfrom sklearn.model_selection import train_test_split\nfrom fastai.structured import *\nfrom fastai.column_data import *\nimport warnings\nwarnings.filterwarnings(\"ignore\")","34855d7f":"df = pd.read_csv('..\/input\/creditcard.csv')\ndf.loc[df['Class'] == 1, \"Class\"] = -1\ndf.loc[df['Class'] == 0, \"Class\"] = 1","6db349b9":"#getting random set of nonfraud data to train on\nnon_fraud = df[df['Class']==1]\ndf_train, val = train_test_split(non_fraud, test_size=0.20, random_state=42)\nfraud = df[df['Class']==-1]","6bb6ad16":"#fastai\ndf, _, nas, mapper = proc_df(df_train, 'Class', do_scale=True)\ndf_val, _, nas, mapper = proc_df(val, 'Class', mapper=mapper, na_dict=nas, do_scale=True)\ndf_fraud, _, nas, mapper = proc_df(fraud, 'Class', mapper=mapper, na_dict=nas, do_scale=True)","0dc1f4c4":"model = svm.OneClassSVM(kernel='rbf', nu=0.0005,gamma=0.007)\nmodel.fit(df)","06ab3a21":"#Creating a test set that contains both fraud and non fraud\ny_val = val['Class']\ny_fraud = fraud['Class']\ny_testval = pd.concat([y_val, y_fraud])\ny_testval = np.array(y_testval)\ndf_testval = pd.concat([df_val, df_fraud])","773f908e":"#predicting on test set, which consists of both fraud and non-fraud\npred_testval = model.predict(df_testval)","69acbac0":"print(classification_report(y_testval, pred_testval))","6c036e7d":"prec, rec, f2, _ = precision_recall_fscore_support(y_testval, pred_testval, beta=2, \n                                                   pos_label=-1, average='binary')\nprint(f'precision is {prec}, recall is {rec} and F2 score is {f2}')","787206a9":"roc = roc_auc_score(y_testval, pred_testval)\nprint(f'ROC score is {roc}')","d0c833b6":"df = pd.read_csv('..\/input\/creditcard.csv')\ndf.loc[df['Class'] == 1, \"Class\"] = -1\ndf.loc[df['Class'] == 0, \"Class\"] = 1","2b904b3f":"#227845 because that is 80% of dataset, just like training amount earlier\ndf_train = df.iloc[:227845,:]\nval = df.iloc[227845:,:]\nprint(df_train.shape, val.shape)","83df622e":"#fastai\ndf, _, nas, mapper = proc_df(df_train, 'Class', do_scale=True)\ndf_val, _, _, _ = proc_df(val, 'Class', mapper=mapper, na_dict=nas, do_scale=True)","aee0cb1c":"nu = df_train[df_train['Class']==-1].shape[0]\/df_train.shape[0]\nnu","cecda2f3":"model = svm.OneClassSVM(kernel='rbf', nu=nu,gamma=0.007)\nmodel.fit(df)","96740163":"y_val = val['Class']\npred_val = model.predict(df_val)","a5477439":"print(classification_report(y_val, pred_val))","c24767b3":"prec, rec, f2, _ = precision_recall_fscore_support(y_val, pred_val, beta=2, \n                                                   pos_label=-1, average='binary')\nprint(f'precision is {prec}, recall is {rec} and F2 score is {f2}')","018e412f":"roc = roc_auc_score(y_val, pred_val)\nprint(f'ROC score is {roc}')","86d2033b":"**References:**\n\n[1] A Survey of Outlier Detection Methodologies. http:\/\/eprints.whiterose.ac.uk\/767\/1\/hodgevj4.pdf \n\n[2] How and why to create a good valdiation set. https:\/\/www.fast.ai\/2017\/11\/13\/validation-sets\/","bad84412":"OC-SVM, or one class learning in general, is suitable when your **dataset is imbalanced**, because you only need the majority class to train your data, and you don't need to worry how the outliers are distributed\/modelled to create a useful decision boundary. This is useful when it is hard to create outliers. In many practical scenarios, it is hard and expensive to create anomalies [1], certainly the case for fraud detection which happens rarely. Also, if you have a new type of fraud that has never been encountered before, it would be easy for OC-SVM to detect it, whereas a classfication model may struggle to do so [1].","ce42c9b7":"### Exploring if data is leaked\nSince we are dealing with a time series, there is the concern that the results will not hold in production. A likely culprit would be that the model is benefiting from looking back and forward in time to create its decision boundary [2]. By doing random split using sklearn, the model may have learnt things that it will not be able to use when the model goes live.\n\nTo make sure we do not commit that crime and to see if there is indeed 'data leakage', we replicate the steps above, only difference being we train the model using first X rows, and test on the subsequent rows, so the model will not benefit from crystal balling into the future.\n\nOne key thing to note is that for OC-SVM, the training set should ideally be all non-fraud cases, which was done earlier. However to allow proper split between training and testing sets, we make a compromise and account for the 'training_error' in the modelling.","3ea9bd0b":"### Quick implementation of OC-SVM","58f822fe":"Where I differ from above section: Instead of a random split, I split by time","7973e215":"I will discuss the following:\n1. When One Class SVM(OC-SVM) is useful, \n1. A quick implementation of the model, and \n1. Investigation into whether there is any data leakage when random sampling is done.","c2c26084":"### When to use OC-SVM","445b6f2f":"#### Here I use fastai to work with my dataset. It's the easiest way for me to do some basic processing on structured data.","b20fd31f":"To set the training error, I check how many outliers are there in my train set. It is still very small(0.1%), so it should not affect the model adversely. In sklearn, nu represents the expected training error.","1f639aea":"So we can see that all the metrics have gone down, which is very likely due to data leakage."}}