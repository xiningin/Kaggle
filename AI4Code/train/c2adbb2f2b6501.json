{"cell_type":{"8bf561a2":"code","2020a6f9":"code","a0a1f4c1":"code","4df90c81":"code","b6555666":"code","897f3bf9":"code","c6462b5c":"code","82c0c950":"code","99be85bd":"code","c40513d7":"code","c5e04c03":"code","6e0a6a60":"code","9e4647ce":"code","0776614b":"markdown","0228433e":"markdown","775ebbf8":"markdown","8d0ec17d":"markdown","75d12b9a":"markdown"},"source":{"8bf561a2":"import os\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport nltk\nfrom gensim.models import KeyedVectors\nfrom sklearn.datasets import fetch_20newsgroups","2020a6f9":"print(os.listdir('..\/input'))","a0a1f4c1":"FILE_PATH = '..\/input\/fasttext-wikinews\/wiki-news-300d-1M.vec'","4df90c81":"# Let's read the first few lines \nwith open(FILE_PATH) as f:\n    for _ in range(5):\n        print(f.readline()[:80])","b6555666":"# This may take a few mins\nkeyed_vec = KeyedVectors.load_word2vec_format(FILE_PATH)","897f3bf9":"for word in ['hello', '!', '2', 'Turing', 'foobarz', 'hi!']:\n    print(word, \"is in the vocabulary:\", word in keyed_vec.vocab)","c6462b5c":"word_vec = keyed_vec.get_vector('foobar')\nprint(word_vec.shape)\nprint(word_vec[:25])","82c0c950":"keras_embedding = keyed_vec.get_keras_embedding()\nkeras_embedding.get_config()","99be85bd":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import f1_score","c40513d7":"def mean_fasttext(arr, embedding_dim=300):\n    '''\n    Create the average of the fasttext embeddings from each word in a document. \n    Very slow function, needs to be optimized for larger datasets\n    '''\n    mean_vectors = []\n    for document in arr:\n        tokens = nltk.tokenize.word_tokenize(document)\n        vectors = [keyed_vec.get_vector(token) for token in tokens if token in keyed_vec.vocab]\n        if vectors:\n            mean_vec = np.vstack(vectors).mean(axis=0)\n            mean_vectors.append(mean_vec)\n        else:\n            mean_vectors.append(np.zeros(embedding_dim))\n    embedding = np.vstack(mean_vectors)\n    return embedding","c5e04c03":"data_sample = pd.read_csv('..\/input\/quora-insincere-questions-classification\/train.csv', nrows=6000)\ntrain_sample = data_sample[:5000]\ntest_sample = data_sample[5000:]\ntrain_sample.head()","6e0a6a60":"X_train = mean_fasttext(train_sample[\"question_text\"].values)\nX_test = mean_fasttext(test_sample[\"question_text\"].values)\ny_train = train_sample['target'].values\ny_test = test_sample['target'].values\nprint(X_train.shape)\nprint(y_train.shape)","9e4647ce":"model = LogisticRegression(solver='lbfgs')\nmodel.fit(X_train, y_train)\nprint(\"Train Score:\", f1_score(y_train, model.predict(X_train)))\nprint(\"Test Score:\", f1_score(y_test, model.predict(X_test)))","0776614b":"### Retrieving a vector with the KeyedVector","0228433e":"# Starter Code for fastText English Word Vectors Embedding\n\nThis kernel intends to be a starter code for anyone using the fastText Embedding. It uses Gensim to create a `KeyedVector` object (behavior similar to a dictionary). An example of tokenizing the data is also given.","775ebbf8":"## Applied Example: Prediction with scikit-learn","8d0ec17d":"### Creating Keras Embeddings","75d12b9a":"## Load the embedding"}}