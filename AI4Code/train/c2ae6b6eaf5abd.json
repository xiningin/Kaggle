{"cell_type":{"15cb5b55":"code","39909acf":"code","081b73fd":"code","8f92b0f3":"code","d77fd71f":"code","2bd4adfb":"code","72b12cb9":"code","56de0311":"code","d98606e8":"code","305c0e03":"code","4ddf5263":"code","cc61e916":"code","d99079d2":"code","589c0c4b":"code","fd6b33d7":"code","9c0611a8":"code","9d5bcfe6":"code","a39cb528":"code","31aef9d4":"code","7252c9c5":"code","877474f8":"code","de59b9eb":"code","14479ffa":"code","ebba0a11":"code","a043b6c5":"code","bf979574":"code","806ba7e9":"code","9058a1fb":"code","b75fe828":"code","6d36e1cb":"code","d4b5340f":"code","8aeb4067":"code","2229129a":"markdown","937d488a":"markdown","4d0ee2fb":"markdown","89fab4dc":"markdown","d45dc1d8":"markdown","a1ca57b1":"markdown","b459c78b":"markdown","004d037c":"markdown","34885dd5":"markdown","895ec4b7":"markdown","3658eba5":"markdown","7894c205":"markdown","08ddc6d8":"markdown","dfb42045":"markdown","7811fb81":"markdown","21c4d6e3":"markdown","0ac91a65":"markdown","09402ece":"markdown","ad2bb793":"markdown","53c9b555":"markdown","bd074710":"markdown","4fa98cee":"markdown","4c4b732d":"markdown","f1f9f4f3":"markdown","bca4a428":"markdown","74066c48":"markdown","af2b1039":"markdown","7054659e":"markdown","f47dc4c7":"markdown"},"source":{"15cb5b55":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","39909acf":"import matplotlib.pyplot as plt\nimport seaborn as sns\nimport nltk\nfrom nltk.tokenize import word_tokenize \n\nplt.rcParams['figure.figsize'] = (13, 7)","081b73fd":"#take a look on the dataset\ndf = pd.read_csv(\"..\/input\/netflix-shows\/netflix_titles.csv\")\ndf.head(5)","8f92b0f3":"#showing info for each featrue\ndf.info()","d77fd71f":"df.head(1)","2bd4adfb":"def pie_chart(df, column ,explode , labels,title,no):\n    \n    plt.pie(df[column].value_counts(),\n            explode=explode,    #explode=[0.04,0]\n            startangle=90, \n            autopct='%1.1f%%',\n            labels=labels, #labels=['Males','Females']\n            colors = ['#66b3ff','#99ff99'],\n            pctdistance=.6,\n            textprops={'fontsize': 20})\n    plt.title(title)\n    plt.figure(no)\n\npie_chart(df, \"type\" ,[0.15,0.05], ['Movies',\"TV series\"],\"Movie and TV Shows count\",0)","72b12cb9":"Top10_movies_directors = df[df['type'] == 'Movie']\n\nbase_color = sns.color_palette()[1]\n\nsns.countplot(y = 'director',color= base_color, data = Top10_movies_directors, order = Top10_movies_directors.director.value_counts().head(10).index)\n\nplt.title(\"Top 10 Movie's Directors on Netflix\", fontsize = 20);","56de0311":"Top10_Series_directors = df[df['type'] == 'TV Show']\n\nbase_color = sns.color_palette()[2]\n\nsns.countplot(y = 'director',color= base_color, data = Top10_Series_directors, order = Top10_Series_directors.director.value_counts().head(10).index)\n\nplt.title(\"Top 10 TV Show's Directors on Netflix\", fontsize = 20);","d98606e8":"base_color = sns.color_palette()[3]\n\nsns.countplot(y = 'country',color= base_color, data = df, order = df.country.value_counts().head(10).index)\n\nplt.title(\"Top 10 Countries in film Production on Netflix\", fontsize = 20);","305c0e03":"#now checking the NaN values\ndf.isnull().sum()","4ddf5263":"df= df.dropna(subset=['cast','country'], axis = 0)\ndf['director'] = df['director'].fillna(\"Unknown\")\ndf = df.reset_index( drop=True)\n","cc61e916":"df.head(1)","d99079d2":"df.info()","589c0c4b":"overall_infos = []\nfor i in range(0, df.shape[0]):\n    overall_infos.append(df['type'][i]+' '+df['title'][i]+' '+df['director'][i]+' '+df['cast'][i]+' '+df['description'][i]+' '+df['country'][i])\ndf['overall_infos'] = overall_infos","fd6b33d7":"df.iloc[4505:4509]","9c0611a8":"df.insert(1, \"id\", list(range(1, 6659)), True) ","9d5bcfe6":"df.head(1)","a39cb528":"df_new = df[['id','overall_infos']]\ndf_new.head(1)","31aef9d4":"#Stopwords help us to get rid of unwanted words like: a, an, are, is, ...\nfrom nltk.corpus import stopwords\nstop = stopwords.words('english')","7252c9c5":"def text_preprocessing(column):\n    #make all words with lower letters\n    column = column.str.lower()\n    #getting rid of any punctution\n    column = column.str.replace('http\\S+|www.\\S+|@|%|:|,|', '', case=False)\n    #spliting each sentence to words to apply previous funtions on them \n    word_tokens = column.str.split()\n    keywords = word_tokens.apply(lambda x: [item for item in x if item not in stop])\n    #assemble words of each sentence again and assign them in new column\n    for i in range(len(keywords)):\n        keywords[i] = \" \".join(keywords[i])\n        column = keywords\n\n    return column","877474f8":"df_new['cleaned_infos'] = text_preprocessing(df_new['overall_infos'])\n","de59b9eb":"df['overall_infos'][6]","14479ffa":"df_new['cleaned_infos'][6]","ebba0a11":"from sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.metrics.pairwise import cosine_similarity\n\nCV = CountVectorizer()\nconverted_metrix = CV.fit_transform(df_new['cleaned_infos'])","a043b6c5":"cosine_similarity = cosine_similarity(converted_metrix)","bf979574":"cosine_similarity","806ba7e9":"#finding the correct name of a movie\ndf[df['title'].str.contains('Ninja')]","9058a1fb":"#this how we will get the id of the movie so we can check similarity between it and other movies\ntitle = 'Teenage Mutant Ninja Turtles'\n\nmovie_id = df[df['title'] == title]['id'].values[0]","b75fe828":"score = list(enumerate(cosine_similarity[movie_id]))","6d36e1cb":"#now sort the similar movies in descending order\nsorted_score = sorted(score, key=lambda x:x[1], reverse= True)\n#we will ignore the first score because it will give us a 100% score because it's the same movie \nsorted_score = sorted_score[1:]","d4b5340f":"sorted_score[0:10]","8aeb4067":"#now showing the top 5 movies similar to TMNT accroding to this algorithm\ni = 0\nfor item in sorted_score:\n    movie_title = df[df['id'] == item[0]]['title'].values[0]\n    print(i+1,movie_title)\n    i = i+1\n    if i > 4:\n        break","2229129a":"Because we don't have rating feature **(the rating feature provided in this dataset is only show the kind of the movie or series like who can watch them, and so on)** that clarifiy user's satisfication about the quality of the movie or series, the only why to calculate similartiy is using **Cosine Similarity**","937d488a":"Now, let's test it, we will see what recommendations would we get according to any movie or series","4d0ee2fb":"> For **rating**, **date_added** and **country** i'm not going to use them so i will just keep them like that \n\n> As **cast** and **country** are important features when calculating the similarity between movies, i won't be able to fill them with a good information so, i will drop NaNs of them \n\n> For **director**, if i dropped NaNs it will be a big loss of data so, i will fill it with **Unknown**","89fab4dc":"more visualization and EDA will be added soon to understanfd this dataset more but for now the main purpose of this notebook is to practice the implementation of a recommender system.\n\nThanks for reaching this section of my notebook, hope you liked it, if so support me with an upvote :)","d45dc1d8":"## Upcoming","a1ca57b1":"Now let's see Top 10 directors that have more in each movies and series ","b459c78b":"## References","004d037c":"## Introduction","34885dd5":"Now let's see the features that we will use","895ec4b7":"![1_UxsrvB1oWpTYxUgSRNW92g.jpeg](attachment:1_UxsrvB1oWpTYxUgSRNW92g.jpeg)","3658eba5":"## Text Preprocessing ","7894c205":"when trying to use specific movie we have to use **id**, the one provided with the dataset have a lot of missing ids but at the same time they aren't NaN values, \n\nlet me show you","08ddc6d8":">1 - [Recommender System Application development by Emre Havan](https:\/\/towardsdatascience.com\/recommender-system-application-development-part-1-of-4-cosine-similarity-f6dbcd768e83#:~:text=Similarity%20between%20two%20vectors%20(A,the%20angle%20between%20them%20decreases.))\n\n>2 - [Build A Movie Recommendation Engine Using Python by 'Computer Science' channel on Youtube](https:\/\/www.youtube.com\/watch?v=ueKXSupHz6Q)\n","dfb42045":"## Testing","7811fb81":"Now, let's create a new feature that we will use to calculate the similarity between movies, it's a combination of some other features \n","21c4d6e3":"Not a fancy suggestions but it should work according to the dataset we got","0ac91a65":"Cosine similarity is the measure of similarity between two vectors, by computing the cosine of the angle between two vectors projected into multidimensional space. It can be applied to items available on a dataset to compute similarity to one another via keywords or other metrics. Similarity between two vectors (A and B) is calculated by taking the dot product of the two vectors and dividing it by the magnitude value as shown in the equation below. We can simply say that the CS score of two vectors increases as the angle between them decreases.\n\n![1_nUNJaIHpUoPozJxe5HZ-fg.png](attachment:1_nUNJaIHpUoPozJxe5HZ-fg.png)","09402ece":"## Data Cleaning","ad2bb793":"As you can see the id after **s4960** is **s4962** , and there are alot of these mistakes, so i will make my own id column","53c9b555":"To apply Cosine similarity we need to transform each sentence into vector, **CountVectorizer** will do this for us and combine them all in one matrix","bd074710":"welcome to my notebook for [Netflix Movies and TV Shows](https:\/\/www.kaggle.com\/shivamb\/netflix-shows) dataset\n\nin this kernel i will use Netflix dataset to implement my first recommendation system using Cosine Similarity and doing some visualization to understand some features, this dataset contains 7787 record and 12 column, we won't use all of them as some rows will be dropped and we will work on specific coulmns for the recommender system, hope you all like this notebook \n\nif you have any suggest,advice or correction please don't hesitate to write it, i think it will be very helpful for me and if you like this notebook an upvote would be great.","4fa98cee":"## Feature Engineering","4c4b732d":"Let's see what is the matrix looks like","f1f9f4f3":"It's all clean now","bca4a428":"Which one is bigger? Movies or TV Shows","74066c48":"## Data Visualization","af2b1039":">Source : [Recommender System Application development](https:\/\/towardsdatascience.com\/recommender-system-application-development-part-1-of-4-cosine-similarity-f6dbcd768e83#:~:text=Similarity%20between%20two%20vectors%20(A,the%20angle%20between%20them%20decreases.))","7054659e":"Before calculating similarity, a text preprocessing is need to be applied on the new feature, to make it easier for the cosine similarity algorithm when deal with each row after transforming them into vectors","f47dc4c7":"Now let's see the difference before and after apply **text_preprocessing** function"}}