{"cell_type":{"9127b101":"code","19544539":"code","43181ce3":"code","d9745606":"code","531c7f1d":"code","069a560e":"code","bda154b8":"code","aad0a67b":"code","7bc20cd7":"code","6ab6644a":"code","7023893d":"code","b6b07407":"code","4ebe6d35":"code","c6150e4b":"code","9e83e003":"code","69331a0f":"code","75f02443":"code","769ea66f":"code","cffd8018":"code","e4d3922a":"code","56b273f1":"code","a54a3c86":"markdown","9c178607":"markdown","3e17f209":"markdown","1568472d":"markdown","fb67bc71":"markdown","eb54b0dc":"markdown","47c94ea7":"markdown","d99c83c7":"markdown"},"source":{"9127b101":"\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nfrom nltk.stem import PorterStemmer\nfrom nltk.sentiment.vader import SentimentIntensityAnalyzer\nfrom nltk.corpus import stopwords\nimport emoji\nimport string\nimport re\nimport os\nfrom sklearn.feature_extraction.text import TfidfVectorizer,CountVectorizer\nfrom PIL import Image\nfrom wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\nimport matplotlib.pyplot as plt\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.linear_model import RidgeClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.ensemble import BaggingClassifier\nfrom scipy.stats import ttest_ind\nplt.style.use('fivethirtyeight')","19544539":"train=pd.read_csv(\"\/kaggle\/input\/nlp-getting-started\/train.csv\")\ntest=pd.read_csv(\"\/kaggle\/input\/nlp-getting-started\/test.csv\")","43181ce3":"def preprocess(line):\n    ps=PorterStemmer()\n    remove_list=string.punctuation\n    remove_list+=''.join(emoji.UNICODE_EMOJI.keys())\n    translator = str.maketrans(remove_list, ' '*len(remove_list), '')\n    line=line.translate(translator)\n    line=re.sub(r'http(s)?:\\\/\\\/\\S*? ', \" \", line)\n    this_stopwords=set(stopwords.words('english'))\n    line=re.sub('https?:\/\/\\S+|www\\.\\S+', '', line)\n    this_stopwords.add(\"co\")\n    line = ' '.join(filter(lambda l: l not in this_stopwords, line.split(' ')))\n    line=line.replace(\"#\",' ').replace('  ','').lower()\n    return line","d9745606":"train['tokens']=train['text'].astype(str).apply(lambda x:preprocess(x).split(' '))\ntrain['text_prep']=train['text'].astype(str).apply(lambda x:preprocess(x))","531c7f1d":"true_tokens=np.array(train.loc[train.target==1,'tokens'].apply(lambda x:np.array(x)))\nfalse_tokens=np.array(train.loc[train.target==0,'tokens'].apply(lambda x:np.asarray(x)))","069a560e":"vectorizer1=CountVectorizer()\ntrue_tokens_v=vectorizer1.fit_transform(train.loc[train.target==1,'text_prep'].astype(str).apply(lambda x:preprocess(x)))\nvectorizer2=CountVectorizer()\nfalse_tokens_v=vectorizer2.fit_transform(train.loc[train.target==0,'text_prep'].astype(str).apply(lambda x:preprocess(x)))\ntrue_tokens_df=pd.DataFrame(columns=vectorizer1.get_feature_names(),data=true_tokens_v.toarray())\nfalse_tokens_df=pd.DataFrame(columns=vectorizer2.get_feature_names(),data=false_tokens_v.toarray())\ntrue_words=true_tokens_df.T[true_tokens_df.sum(axis=0)>5].sum(axis=1)\nfalse_words=false_tokens_df.T[false_tokens_df.sum(axis=0)>5].sum(axis=1)","bda154b8":"def check_emoji(line):\n    emoji_=''.join(emoji.UNICODE_EMOJI.keys())\n    emoji_flag=sum([i in emoji_ for i in line])>0\n    return emoji_flag\n\ndef check_capslock(line):\n    capslock_flag=len(re.findall(r'[A-Z][A-Z][A-Z]+',line))>1\n    return capslock_flag\n\ndef check_url(line):\n    url_flag=len(re.findall(r'http(s)?:\\\/\\\/\\S*? ',line))>1\n    return url_flag\n\nsid = SentimentIntensityAnalyzer()\ndef dict_max(scores):\n    if scores['pos']==max(scores.values()):\n        return 1\n    elif scores['neg']==max(scores.values()):\n        return -1\n    else:\n        return 0","aad0a67b":"train['tags']=train['text'].apply(lambda x:x.count('#'))\ntrain['emoji']=train['text'].apply(lambda x:float(check_emoji(x)))\ntrain['capslock']=train['text'].apply(lambda x:float(check_capslock(x)))\ntrain['url']=train['text'].astype(str).apply(lambda x:float(check_url(x)))\ntrain['sentiment_vader']=train['text'].astype(str).apply(lambda x:dict_max(sid.polarity_scores(x)))\ntrain=train.join(pd.DataFrame.from_records(train['text'].astype(str).apply(lambda x:sid.polarity_scores(x))))","7bc20cd7":"fig, ax = plt.subplots()\nplt.figure(figsize=(9, 3))\nax.bar(train.groupby(by='target')['emoji'].sum().index.values,\n        train.groupby(by='target')['emoji'].sum().values)\n\nax.set_title('Emoji by target')\nfig.show()\nt_emoji=ttest_ind(train[train.target==0].emoji.astype('int'),\n          train[train.target==1].emoji.astype('int'))\nprint('pvalue={}'.format(t_emoji.pvalue))","6ab6644a":"fig, ax = plt.subplots()\nplt.figure(figsize=(9, 3))\nax.bar(train.groupby(by='target')['capslock'].sum().index.values,\n        train.groupby(by='target')['capslock'].sum().values)\nax.set_title('Capslock by target')\nfig.show()\nt_emoji=ttest_ind(train[train.target==0].capslock.astype('int'),\n          train[train.target==1].capslock.astype('int'))\nprint('pvalue={}'.format(t_emoji.pvalue))","7023893d":"train[['target','tags']].boxplot(by='target',figsize=(12, 4),grid=False)\nt_emoji=ttest_ind(train[train.target==0].tags.astype('int'),\n          train[train.target==1].tags.astype('int'))\nprint('pvalue={}'.format(t_emoji.pvalue))","b6b07407":"fig, ax = plt.subplots()\nax.bar(train.groupby(by='target')['sentiment_vader'].sum().index.values,\n        train.groupby(by='target')['sentiment_vader'].sum().values)\nax.set_title('Sentiment by target')\nfig.show()\nt_emoji=ttest_ind(train[train.target==0].sentiment_vader.astype('int'),\n          train[train.target==1].sentiment_vader.astype('int'))\nprint('pvalue={}'.format(t_emoji.pvalue))","4ebe6d35":"fig, ax = plt.subplots()\nax.bar(train.groupby(by='target')['url'].sum().index.values,\n        train.groupby(by='target')['url'].sum().values)\nax.set_title('Url by target')\nfig.show()\nt_emoji=ttest_ind(train[train.target==0].url.astype('int'),\n          train[train.target==1].url.astype('int'))\nprint('pvalue={}'.format(t_emoji.pvalue))","c6150e4b":"wordcloud_true = WordCloud(background_color='white',width=800,height=600,margin=0)\nwordcloud_true.generate_from_frequencies(true_words.to_dict())\nwordcloud_false = WordCloud(width=800,height=600,margin=0)\nwordcloud_false.generate_from_frequencies(false_words.to_dict())\nfig, axes = plt.subplots(1, 2)\nfig.set_size_inches(16, 10, forward=True)\naxes[0].imshow(wordcloud_true, interpolation='bilinear')\naxes[1].imshow(wordcloud_false, interpolation='bilinear')\nplt.axis(\"off\")\nplt.show()","9e83e003":"vectorizer=CountVectorizer(train['text'].tolist(),preprocessor=preprocess,ngram_range=(1,1),min_df=2 ,max_df=0.9)\nvectorizer.fit(train['text'].astype(str).apply(lambda x:preprocess(x)))\ntokens_v=vectorizer.transform(train['text'].astype(str).apply(lambda x:preprocess(x)))\ntokens_v_featured=np.concatenate((tokens_v.toarray(),train[['url','neu','pos','neg']].values),axis=1)\nclf = MultinomialNB()\nclf.fit(tokens_v[:6000],train.target[:6000])\nscore1=clf.score(tokens_v[6000:],train.target[6000:])\nprint('MultinomialNB score: {}'.format(score1))\nclf_featured = MultinomialNB()\nclf_featured.fit(tokens_v_featured[:6000],train.target[:6000])\nscore2=clf_featured.score(tokens_v_featured[6000:],train.target[6000:])\nprint('MultinomialNB score with features: {}'.format(score2))\n","69331a0f":"rc = RidgeClassifier()\nrc.fit(tokens_v[:5000],train.target[:5000])\nscore1=rc.score(tokens_v[5000:],train.target[5000:])\nprint('RidgeClassifier score: {}'.format(score1))\nrc_featured = RidgeClassifier()\nrc_featured.fit(tokens_v_featured[:5000],train.target[:5000])\nscore2=rc_featured.score(tokens_v_featured[5000:],train.target[5000:])\nprint('RidgeClassifier score with features: {}'.format(score2))\n","75f02443":"svc = SVC(kernel='rbf')\nsvc.fit(tokens_v[:5000],train.target[:5000])\nscore1=clf.score(tokens_v[5000:],train.target[5000:])\nprint('Support Vector Machine score: {}'.format(score1))\nsvc_featured = SVC(kernel='rbf')\nsvc_featured.fit(tokens_v_featured[:5000],train.target[:5000])\nscore2=svc_featured.score(tokens_v_featured[5000:],train.target[5000:])\nprint('Support Vector Machine score with features: {}'.format(score2))","769ea66f":"bag = BaggingClassifier(base_estimator=clf,n_estimators=30)\nbag.fit(tokens_v[:6000],train.target[:6000])\nscore1=bag.score(tokens_v[6000:],train.target[6000:])\nprint('BaggingClassifier score: {}'.format(score1))\nbag_featured = BaggingClassifier(base_estimator=clf_featured,n_estimators=30)\nbag_featured.fit(tokens_v_featured[:5000],train.target[:5000])\nscore2=bag_featured.score(tokens_v_featured[5000:],train.target[5000:])\nprint('BaggingClassifier score with features: {}'.format(score2))","cffd8018":"submission=pd.read_csv(\"\/kaggle\/input\/nlp-getting-started\/sample_submission.csv\")\n","e4d3922a":"test=pd.merge(submission,test,on='id')[['id','text','target']]\ntest['tags']=test['text'].apply(lambda x:x.count('#'))\ntest['emoji']=test['text'].apply(lambda x:float(check_emoji(x)))\ntest['capslock']=test['text'].apply(lambda x:float(check_capslock(x)))\ntest['url']=test['text'].apply(lambda x:float(check_url(x)))\ntest['text_prep']=test['text'].astype(str).apply(lambda x:preprocess(x))\ntest=test.join(pd.DataFrame.from_records(test['text'].astype(str).apply(lambda x:sid.polarity_scores(x))))\n#vectorizer.fit(train['text_prep'].astype(str).apply(lambda x:preprocess(x)))\ntokens_v=vectorizer.transform(test['text_prep'].astype(str).apply(lambda x:preprocess(x)))\ntokens_v_featured=np.concatenate((tokens_v.toarray(),test[['url','neu','pos','neg']].values),axis=1)\n","56b273f1":"test.target=clf_featured.predict(tokens_v_featured)\ntest[['id','target']].to_csv('submission_6_nb.csv',index=False)\n","a54a3c86":"Wordclouds","9c178607":"Naive Bayes","3e17f209":"kernel SVC","1568472d":"Tokens and Words frequensies","fb67bc71":"\n*     The text of a tweet\n*     A keyword from that tweet (although this may be blank!)\n*     The location the tweet was sent from (may also be blank)\n","eb54b0dc":"New Features","47c94ea7":"BaggingClassifier","d99c83c7":"RidgeClassifier"}}