{"cell_type":{"2a9e7915":"code","d5ce260d":"code","1c5beee8":"code","d1c5c0b4":"code","d32ac581":"code","09d3cede":"code","d47e38bc":"code","11be2b86":"code","53478f45":"code","585054c1":"code","af468944":"code","e7e5ef11":"code","0523fcde":"code","b55e206c":"code","2907189f":"code","76fd785a":"code","56378deb":"code","8844e08f":"code","8fc9a961":"code","f07753aa":"code","d83241f2":"code","c31ccde0":"code","273cb4e0":"code","488073f2":"code","5d2f2d62":"code","c038b0da":"code","e2fa16f1":"code","8fb80b23":"code","3b7934cc":"code","690ccc34":"code","b9b435a2":"code","9e17b3e5":"code","e21dd8dd":"code","0aeabb3f":"code","11367b89":"code","030d253a":"code","a1eaad04":"code","c86b61b1":"code","d53001d7":"code","2a6244e2":"code","85526c0f":"code","ab1e267c":"code","760f2565":"code","7e2c968a":"code","b18a7a81":"code","96ab66a6":"code","b58e1287":"code","068d33b7":"code","bffbbffc":"markdown","0d5595ca":"markdown","de894a25":"markdown","acb4e2f6":"markdown"},"source":{"2a9e7915":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","d5ce260d":"import pandas as pd\nimport numpy as np\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns","1c5beee8":"df = pd.read_csv(\"\/kaggle\/input\/creditcardfraud\/creditcard.csv\")","d1c5c0b4":"df.head()","d32ac581":"df.info()","09d3cede":"df.describe().T","d47e38bc":"sns.countplot(x=df['Class'])","11be2b86":"df['Class'].value_counts()","53478f45":"df.columns","585054c1":"corr_mat = df.drop(\"Time\", axis=1).corr()\n\nplt.figure(figsize=(18,14))\nsns.heatmap(corr_mat)","af468944":"num_feat = [col for col in df.drop([\"Time\", \"Class\"], axis=1).columns]","e7e5ef11":"fig = plt.figure(figsize=(18, 30))\n\nfor i, col in enumerate(num_feat):\n    plt.subplot(12,4, i+1)\n    sns.kdeplot(x=df[col], color='b', shade=True) # histplot, boxplot\n    plt.tight_layout()\nfig.show()","0523fcde":"fig = plt.figure(figsize=(18,40))\n\nfor i, col in enumerate(num_feat):\n    plt.subplot(12,4, i+1)\n    sns.boxplot(x=df['Class'], y=df[col])\n    plt.tight_layout()\n\nfig.show()","b55e206c":"sns.distplot(x=df['Time'])","2907189f":"# Split the data into X\/y\nX = df.drop([\"Class\"], axis=1).values\ny = df['Class'].values","76fd785a":"X.shape","56378deb":"from tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, Dropout\nfrom tensorflow.keras.callbacks import EarlyStopping\nfrom sklearn.model_selection import KFold\nfrom tensorflow.keras.optimizers import Adamax, Adam","8844e08f":"def create_classifier():\n    \n    model = Sequential()\n    \n    # First layer\n    model.add(Dense(29, activation=\"relu\"))\n    model.add(Dropout(0.3))\n    # hidden layer\n    model.add(Dense(14, activation=\"relu\"))\n    model.add(Dropout(0.3))\n    # Output layer --> Binary Classification\n    model.add(Dense(1, activation=\"sigmoid\"))\n    \n    # Compile model\n    model.compile(optimizer=Adam(learning_rate=0.0001), loss=\"binary_crossentropy\")\n    \n    return model","8fc9a961":"# Use train\/test split for a base model\nfrom sklearn.model_selection import train_test_split\n\nX_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=101)","f07753aa":"from sklearn.preprocessing import MinMaxScaler","d83241f2":"scaler = MinMaxScaler()\nX_train = scaler.fit_transform(X_train)\nX_val = scaler.transform(X_val)","c31ccde0":"ann_model = create_classifier()","273cb4e0":"early_stop = EarlyStopping(monitor='val_loss', mode='min', patience=2)","488073f2":"ann_model.fit(x=X_train, y=y_train, epochs=20, batch_size=64, validation_data=(X_val, y_val), callbacks=[early_stop])","5d2f2d62":"losses = pd.DataFrame(ann_model.history.history)\nlosses.plot()","c038b0da":"base_y_pred = ann_model.predict(X_val)","e2fa16f1":"base_y_pred = np.where(base_y_pred > 0.5, 1,0)","8fb80b23":"from sklearn.metrics import classification_report, confusion_matrix, r2_score","3b7934cc":"print(classification_report(y_val, base_y_pred))","690ccc34":"confusion_matrix(y_val, base_y_pred)","b9b435a2":"r2_score(y_val, base_y_pred)","9e17b3e5":"from sklearn.model_selection import StratifiedKFold","e21dd8dd":"# Now let't apply StratifiedKFold and see if that improve our model\nskf = StratifiedKFold(n_splits=10, shuffle=True)\noof = np.zeros(len(X))\nscore_list = []\nfolds = 1\n\ny_pred_list = []\n\nfor train_idx, test_idx in skf.split(X, y):\n    X_train, X_val = X[train_idx], X[test_idx]\n    y_train, y_val = y[train_idx], y[test_idx]\n    \n    scaler = MinMaxScaler()\n    X_train = scaler.fit_transform(X_train)\n    X_val = scaler.transform(X_val)\n    \n    early_stop = EarlyStopping(monitor='val_loss', mode='min', patience=2)\n    \n    ann_model = create_classifier()\n    ann_model.fit(x=X_train,y=y_train,\n                  validation_data=(X_val, y_val), epochs=20, batch_size=64, callbacks=[early_stop])\n    \n    y_preds = ann_model.predict(X_val)\n    y_preds = np.where(y_preds > 0.5, 1,0)\n    \n    oof[test_idx] = y_preds.reshape(-1,)\n    score = r2_score(y_val, oof[test_idx])\n    score_list.append(score)\n    print(f\"Accuracy in folds {folds} : {score}\")\n    folds +=1\n    \nprint(score_list)","0aeabb3f":"print(np.mean(score))","11367b89":"print(classification_report(y, oof))","030d253a":"confusion_matrix(y, oof)","a1eaad04":"from imblearn.over_sampling import SMOTE","c86b61b1":"oversample= SMOTE()\n\nX, y = oversample.fit_resample(X,y)","d53001d7":"# Now let't apply StratifiedKFold and see if that improve our model\nskf = StratifiedKFold(n_splits=10, shuffle=True)\noof = np.zeros(len(X))\nscore_list = []\nfolds = 1\n\ny_pred_list = []\n\nfor train_idx, test_idx in skf.split(X, y):\n    X_train, X_val = X[train_idx], X[test_idx]\n    y_train, y_val = y[train_idx], y[test_idx]\n    \n    scaler = MinMaxScaler()\n    X_train = scaler.fit_transform(X_train)\n    X_val = scaler.transform(X_val)\n    \n    early_stop = EarlyStopping(monitor='val_loss', mode='min', patience=2)\n    \n    ann_model = create_classifier()\n    ann_model.fit(x=X_train,y=y_train,\n                  validation_data=(X_val, y_val), epochs=100, batch_size=64, callbacks=[early_stop])\n    \n    y_preds = ann_model.predict(X_val)\n    y_preds = np.where(y_preds > 0.5, 1,0)\n    \n    oof[test_idx] = y_preds.reshape(-1,)\n    score = r2_score(y_val, oof[test_idx])\n    score_list.append(score)\n    print(f\"Accuracy in folds {folds} : {score}\")\n    folds +=1\n    \nprint(score_list)","2a6244e2":"print(np.mean(score_list))","85526c0f":"#losses = pd.DataFrame(ann_model.history.history)\n#losses.plot()","ab1e267c":"from imblearn.metrics import classification_report_imbalanced","760f2565":"print(classification_report_imbalanced(y, oof))","7e2c968a":"confusion_matrix(y, oof)","b18a7a81":"X = df.drop([\"Class\"], axis=1).values\ny = df['Class'].values","96ab66a6":"from imblearn.over_sampling import SMOTE\nfrom imblearn.under_sampling import RandomUnderSampler\nfrom imblearn.pipeline import Pipeline\n\nover = SMOTE(sampling_strategy='not minority')\nunder = RandomUnderSampler()\n\nsteps = [(\"over\", over),\n         (\"under\", under)]\npipeline = Pipeline(steps=steps)\n\n# Transform the dataset\nX, y = pipeline.fit_resample(X, y)\n\n\n# Apply StratifiedKFold \nskf = StratifiedKFold(n_splits=10, shuffle=True)\noof = np.zeros(len(X))\nscore_list = []\nfolds = 1\n\ny_pred_list = []\n\nfor train_idx, test_idx in skf.split(X, y):\n    X_train, X_val = X[train_idx], X[test_idx]\n    y_train, y_val = y[train_idx], y[test_idx]\n    \n    scaler = MinMaxScaler()\n    X_train = scaler.fit_transform(X_train)\n    X_val = scaler.transform(X_val)\n    \n    early_stop = EarlyStopping(monitor='val_loss', mode='min', patience=2)\n    \n    ann_model = create_classifier()\n    ann_model.fit(x=X_train,y=y_train,\n                  validation_data=(X_val, y_val), epochs=100, batch_size=64, callbacks=[early_stop])\n    \n    y_preds = ann_model.predict(X_val)\n    y_preds = np.where(y_preds > 0.5, 1,0)\n    \n    oof[test_idx] = y_preds.reshape(-1,)\n    score = r2_score(y_val, oof[test_idx])\n    score_list.append(score)\n    print(f\"Accuracy in folds {folds} : {score}\")\n    folds +=1\n    \nprint(score_list)","b58e1287":"print(np.mean(score_list))","068d33b7":"print(classification_report_imbalanced(y, oof))","bffbbffc":"### SMOTE and RandomUnderSampling together","0d5595ca":"## SMOTE Unbalanced dataset","de894a25":"In this notebook I would like to improve accuracy of unbalanced dataset by first applying SMOTE technique and then combine it with RandomUnderSample Class as the orginal paper on SMOTE suggest.","acb4e2f6":"So, we have inbalance dataset which shouldn't be a suprise considering the problem we want to solve."}}