{"cell_type":{"449e455a":"code","8cbb4bb5":"code","4d11eeef":"code","a255c549":"code","6113f222":"code","79825b5c":"code","c35f26c7":"code","7f01cafa":"code","18b9aba5":"code","2f9286fc":"code","be1ed2af":"code","467d1a05":"code","85fb872d":"code","f3e55d43":"code","0f04188b":"code","765afeaf":"code","788b50e3":"code","759c094f":"code","459f8f43":"code","eb70c666":"code","c77b8437":"code","0972a77c":"markdown","17078757":"markdown","1833758a":"markdown","eeedcad6":"markdown","95712e32":"markdown","7ae5614e":"markdown","a3cc1363":"markdown","3b835170":"markdown","61ae99c2":"markdown","863ccfce":"markdown","2f82b551":"markdown","74b65b17":"markdown","83420f5c":"markdown","66691217":"markdown"},"source":{"449e455a":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn import linear_model\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.ensemble import GradientBoostingRegressor\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","8cbb4bb5":"train = pd.read_csv(\"\/kaggle\/input\/house-prices-advanced-regression-techniques\/train.csv\")\ntest= pd.read_csv(\"\/kaggle\/input\/house-prices-advanced-regression-techniques\/test.csv\")\nsample_submission = pd.read_csv(\"\/kaggle\/input\/house-prices-advanced-regression-techniques\/sample_submission.csv\") \n","4d11eeef":"\ndirt_train = train[train.columns[train.isna().any()]]\ndirt_train\n\n\n\n","a255c549":"dirty_features = list(dirt_train.columns)\ntrain.drop(dirty_features,inplace=True,axis =1)\ntest.drop(dirty_features,inplace=True,axis=1)\n\n","6113f222":"train.head(5)","79825b5c":"train_categorical = train.select_dtypes(include=\"object\")\ntest_categorical = test.select_dtypes(include= \"object\")\ncat_col_names = train_categorical.columns\ntrain_categorical.head(5)","c35f26c7":"sample_submission.head(5)","7f01cafa":"new =train.corr()\nnewcorr = new[(new[\"SalePrice\"]>0.4) ]\nnewcorr[\"SalePrice\"]\n\n","18b9aba5":"correlatedfeatures = [\"OverallQual\",\"YearBuilt\",\"YearRemodAdd\",\"TotalBsmtSF\",\"1stFlrSF\",\"1stFlrSF\",\"GrLivArea\",\"FullBath\",\"TotRmsAbvGrd\",\"Fireplaces\",\"GarageCars\",\"GarageArea\",\"SalePrice\"]\ncorrelatedfeatures2 = [\"OverallQual\",\"YearBuilt\",\"YearRemodAdd\",\"TotalBsmtSF\",\"1stFlrSF\",\"1stFlrSF\",\"GrLivArea\",\"FullBath\",\"TotRmsAbvGrd\",\"Fireplaces\",\"GarageCars\",\"GarageArea\"]\nnewdf_train = train[correlatedfeatures]\nnewdf_test = test[correlatedfeatures2]\nnewdfC = newdf_train.corr()","2f9286fc":"x = train[\"OverallQual\"]\ny = train[\"SalePrice\"]\nplt.scatter(x=x,y=y)","be1ed2af":"plt.figure(figsize=(10,10))\nsns.heatmap(data=newdfC,cmap=\"coolwarm\")","467d1a05":"newdf_train.head(5)","85fb872d":"cat_col_names","f3e55d43":"train_categorical.drop([\"MSZoning\",\"Condition2\",\"HouseStyle\",\"RoofMatl\",\"Exterior1st\",\"Heating\",\"KitchenQual\",\"Functional\",\"SaleType\"],inplace=True,axis=1)\ntest_categorical.drop([\"MSZoning\",\"Condition2\",\"HouseStyle\",\"RoofMatl\",\"Exterior1st\",\"Heating\",\"KitchenQual\",\"Functional\",\"SaleType\"],inplace=True,axis=1)\n","0f04188b":"cat_encoder = OneHotEncoder()\nencoded_train_cat = cat_encoder.fit_transform(train_categorical)\nencoded_test_cat = cat_encoder.fit_transform(test_categorical)\nencodedOH_train = pd.DataFrame(encoded_train_cat.toarray())\nencodedOH_test = pd.DataFrame(encoded_test_cat.toarray())\n\n#   N_set_train = set(train_categorical[i])\n  #  N_set_test = set(test_categorical[i])\n #   count1 = len(N_set_train)\n #   count2 = len(N_set_test)\n  #  print(i,count1,count2)\n#train_categorical[\"SaleType\"].value_counts()","765afeaf":"train_num= newdf_train.select_dtypes(exclude=\"object\")\ntest_num= newdf_test.select_dtypes(exclude=\"object\")\nready_train = pd.concat([train_num,encodedOH_train], axis=1)\nready_test = pd.concat([test_num,encodedOH_test], axis=1)\nready_train_labels = ready_train[\"SalePrice\"]\nready_train1 = ready_train.drop(\"SalePrice\",axis=1)\n\n","788b50e3":"\n#mean1 = ready_test[\"GarageCars\"].mean()\n#ready_test[\"GarageCars\"].fillna(mean1)\n#mean2 = ready_test[\"GarageArea\"].mean()\n#ready_test[\"GarageArea\"].fillna(mean2)\n#mean3 = ready_test[\"TotalBsmtSF\"].mean()\n#ready_test[\"TotalBsmtSF\"].fillna(mean3)\nready_test = ready_test.replace(np.nan,0)\nready_test.isna().sum().sum()\n","759c094f":"ready_test","459f8f43":"#regr = linear_model.LinearRegression()\n#regr.fit(ready_train1,ready_train_labels)\n\n#pred = regr.predict(ready_test)\n#preddf = pd.DataFrame(pred) \n#regr = RandomForestRegressor()\n#regr.fit(ready_train1,ready_train_labels)\n#pred = regr.predict(ready_test)\nregr = GradientBoostingRegressor(max_depth=100,n_estimators=200,learning_rate=0.5)\nregr.fit(ready_train1,ready_train_labels)\npred = regr.predict(ready_test)\n","eb70c666":"sub = pd.DataFrame({'Id':sample_submission[\"Id\"],'SalePrice':pred})","c77b8437":"sub.to_csv('submission.csv',index = False)","0972a77c":"Feature engineering,we have 81 features so we will be need to drop few of the un neccesary columns, here first deal with the numerical columns i.e (81-43)=38 columns. corr() will get rid of all the numerical variables","17078757":"Extracting categorical variables from the dataframe","1833758a":"We are done with the cleaning of numerical variables, now lets perform one-hot encoding on the categorical variables","eeedcad6":"imputing null values in the training set","95712e32":"Summary of what had been done in previous steps :-\n* exploratory data analysis\n* Data cleaning\n* Featue engineering\n\nTechniques Used :-\n* One hot encoding\n* Pearson's correlation coefficient\n* dat imputation \n* visualisation\n\nNext is fittin the model and predicting outcomes on unseen(test data)","7ae5614e":"Lets print out the cleaned numerical dataset","a3cc1363":"It looks highly correlated to each other, lets make a heat map to understand the relationship better","3b835170":"Checking the first few rows of training and sample_submission dataset","61ae99c2":"We have gotten ourselves a clean dataset. ","863ccfce":"Next Concatenating the  numerical and categorical variables and splitting dataframe into labels and features dataframe\n","2f82b551":"Plotting the scatter graph betwee highly correlated value i.e \"overallQual\" with \"SalePrice\" as they seem to be highly correlated","74b65b17":"The above variables dont play any significant role in determining the output i.e SalesPrics, so we better drop these column. This can be seen in the heatmap below to understand the role played by these variables are insignificant","83420f5c":"Reading the dataset in pandas dataframe :-\n1.  **train containing** the training dataset\n2.  **test containing** the test dataset\n3.  **sample_submission** containing the submission format","66691217":"Required solution format is as given below :-"}}