{"cell_type":{"3ed88afa":"code","717c1261":"code","6e69b4ad":"code","5654c20b":"code","ed6784b2":"code","a1b8259f":"code","895faca2":"code","0c28ec04":"code","2e72c934":"code","fc12ce22":"code","58e86303":"code","b3bf6a25":"code","3bd54c48":"code","15037894":"code","da480a70":"code","424c7a0b":"code","8503b683":"markdown","01043a9d":"markdown","7975ca39":"markdown","84bc7d51":"markdown","33afc70f":"markdown","497738fa":"markdown","bbb61d99":"markdown","a293f505":"markdown","a72d913d":"markdown","c89e0c4c":"markdown"},"source":{"3ed88afa":"from urllib.request import urlopen\n\n# Links to data\nMEN_LINK = \"https:\/\/www.worldathletics.org\/records\/all-time-toplists\/throws\/hammer-throw\/outdoor\/men\/senior?regionType=world&page=1&bestResultsOnly=false&firstDay=1899-12-31&lastDay=2021-02-09\"\nWOMEN_LINK = \"https:\/\/www.worldathletics.org\/records\/all-time-toplists\/throws\/hammer-throw\/outdoor\/women\/senior?regionType=world&page=1&bestResultsOnly=false&firstDay=1899-12-30&lastDay=2021-02-08\"","717c1261":"# For opening and readning urls\npage = urlopen(MEN_LINK)\npage","6e69b4ad":"page.read()[:1000]","5654c20b":"!pip install bs4","ed6784b2":"from bs4 import BeautifulSoup\n\n# When you use page.read() 2 times, it sometimes works incorrectly, so let's try to avoid that\npage = urlopen(MEN_LINK)\nsoup = BeautifulSoup(page.read(), 'html.parser')\n\nsoup","a1b8259f":"soup.find_all('td', {\"data-th\": \"Rank\"})","895faca2":"ranks = []\n\nfor tag in soup.find_all('td', {\"data-th\": \"Rank\"}):\n    ranks.append(int(tag.getText().strip()))\n\nranks","0c28ec04":"marks = []\n\nfor tag in soup.find_all('td', {\"data-th\": \"Mark\"}):\n    marks.append(float(tag.getText().strip()))\n\n\ncompetitors = []\n\nfor tag in soup.find_all('td', {\"data-th\": \"Competitor\"}):\n    competitors.append(tag.getText().strip())\n\ndobs = []\n\nfor tag in soup.find_all('td', {\"data-th\": \"DOB\"}):\n    dobs.append(tag.getText().strip())\n\nnationalities = []\n\nfor tag in soup.find_all('td', {\"data-th\": \"Nat\"}):\n    nationalities.append(tag.getText().strip())\n\npositions = []\n\nfor tag in soup.find_all('td', {\"data-th\": \"Pos\"}):\n    positions.append(int(tag.getText().strip()))\n\nvenues = []\n\nfor tag in soup.find_all('td', {\"data-th\": \"Venue\"}):\n    venues.append(tag.getText().strip())\n\ndates = []\n\nfor tag in soup.find_all('td', {\"data-th\": \"Date\"}):\n    dates.append(tag.getText().strip())\n\nresult_scores = []\n\nfor tag in soup.find_all('td', {\"data-th\": \"ResultScore\"}):\n    result_scores.append(int(tag.getText().strip()))","2e72c934":"len(ranks) == len(marks) == len(competitors) == len(dobs) == len(nationalities) == len(positions) == len(venues) == len(dates) == len(result_scores)","fc12ce22":"import pandas as pd\n\ndf_data = {\n    'Rank': ranks,\n    'Mark': marks,\n    'Competitor': competitors,\n    'DOB': dobs,\n    'Nationality': nationalities,\n    'Position': positions,\n    'Venue': venues,\n    'Date': dates,\n    'Result Score': result_scores    \n}\n\ndf = pd.DataFrame(df_data)\ndf","58e86303":"df.to_csv('Hammer Throw Men.csv')","b3bf6a25":"def get_and_save_data(url, destfile_name):\n    page = urlopen(url)\n    soup = BeautifulSoup(page.read(), 'html.parser')\n    \n    ranks = []\n\n    for tag in soup.find_all('td', {\"data-th\": \"Rank\"}):\n        ranks.append(int(tag.getText().strip()))\n    \n    marks = []\n\n    for tag in soup.find_all('td', {\"data-th\": \"Mark\"}):\n        marks.append(float(tag.getText().strip()))\n\n\n    competitors = []\n\n    for tag in soup.find_all('td', {\"data-th\": \"Competitor\"}):\n        competitors.append(tag.getText().strip())\n\n    dobs = []\n\n    for tag in soup.find_all('td', {\"data-th\": \"DOB\"}):\n        dobs.append(tag.getText().strip())\n\n    nationalities = []\n\n    for tag in soup.find_all('td', {\"data-th\": \"Nat\"}):\n        nationalities.append(tag.getText().strip())\n\n    positions = []\n\n    for tag in soup.find_all('td', {\"data-th\": \"Pos\"}):\n        positions.append(tag.getText().strip())\n\n    venues = []\n\n    for tag in soup.find_all('td', {\"data-th\": \"Venue\"}):\n        venues.append(tag.getText().strip())\n\n    dates = []\n\n    for tag in soup.find_all('td', {\"data-th\": \"Date\"}):\n        dates.append(tag.getText().strip())\n\n    result_scores = []\n\n    for tag in soup.find_all('td', {\"data-th\": \"ResultScore\"}):\n        result_scores.append(int(tag.getText().strip()))\n    \n    \n    df_data = {\n        'Rank': ranks,\n        'Mark': marks,\n        'Competitor': competitors,\n        'DOB': dobs,\n        'Nationality': nationalities,\n        'Position': positions,\n        'Venue': venues,\n        'Date': dates,\n        'Result Score': result_scores    \n    }\n\n    df = pd.DataFrame(df_data)\n    \n    df.to_csv(destfile_name)","3bd54c48":"get_and_save_data(WOMEN_LINK, 'Hammer Throw Women.csv')","15037894":"WOMEN = 'https:\/\/www.worldathletics.org\/records\/all-time-toplists\/throws\/javelin-throw\/outdoor\/women\/senior?regionType=world&page=1&bestResultsOnly=false&firstDay=1899-12-30&lastDay=2021-02-08'\nMEN = 'https:\/\/www.worldathletics.org\/records\/all-time-toplists\/throws\/javelin-throw\/outdoor\/men\/senior?regionType=world&page=1&bestResultsOnly=false&firstDay=1899-12-30&lastDay=2021-02-08'\n\nget_and_save_data(WOMEN, 'Javelin Throw Women.csv')\nget_and_save_data(MEN, 'Javelin Throw Men.csv')","da480a70":"WOMEN = 'https:\/\/www.worldathletics.org\/records\/all-time-toplists\/throws\/discus-throw\/outdoor\/women\/senior?regionType=world&page=1&bestResultsOnly=false&firstDay=1899-12-30&lastDay=2021-02-08'\nMEN = 'https:\/\/www.worldathletics.org\/records\/all-time-toplists\/throws\/discus-throw\/outdoor\/men\/senior?regionType=world&page=1&bestResultsOnly=false&firstDay=1899-12-30&lastDay=2021-02-08'\n\nget_and_save_data(WOMEN, 'Discus Trow Women.csv')\nget_and_save_data(MEN, 'Discus Throw Men.csv')","424c7a0b":"WOMEN = 'https:\/\/www.worldathletics.org\/records\/all-time-toplists\/throws\/shot-put\/outdoor\/women\/senior?regionType=world&page=1&bestResultsOnly=false&firstDay=1899-12-30&lastDay=2021-02-08'\nMEN = 'https:\/\/www.worldathletics.org\/records\/all-time-toplists\/throws\/shot-put\/outdoor\/men\/senior?regionType=world&page=1&bestResultsOnly=false&firstDay=1899-12-30&lastDay=2021-02-08'\n\nget_and_save_data(WOMEN, 'Shot Put Women.csv')\nget_and_save_data(MEN, 'Shot Put Men.csv')","8503b683":"As a little bonus, here is the function with all the process and some other links to get other competitions data.","01043a9d":"Now, this was kind of silly because we just got a sequence from 1 to 100, which we could've generated it in a lot of easier ways! But it was a really good way to explain a simple web scraper. Now we have to do this with all the other fields.","7975ca39":"We have the data, now, what do we do? Well, we should store it in order to not have to run this script every time we need the information. We'll store it in a .csv file, a really common format to store tabular data.","84bc7d51":"As we can see, using the urlopen method returns an object, not the html code itself. From this connection we have to get the html code.","33afc70f":"That's a lot clearer! But you may be saying \"It looks better, and what next\"? Well, now we can easily extract data by using soup methods, we want all the scores (from 1 to 100) and so we'll use the method **find_all()** (really simple huh?). In order to do this we need to identify where is the information we want, specifically the tags in which it is contained. Let's do a little test with the rank information as it is really easy to checl whether or not our scraper is working the way we intent it to, we should get a series from 1 to 100.  \n\nLet's use the **inspect element** in the information we want and see in which tags it is contained.\n\n![image.png](attachment:image.png)\n\nWe can see it is inside a **th** tag, but we need more information, we can see that the rank info is inside a **th** tag with the **data-th** parameter set to **Rank**. This is all we need to get the data. You'll see how this data is going to get passed to the **find_all()** method.","497738fa":"Ta da! We have the data we wanted! Well, not really, we got the tags in which the information we want is located. Now we have to extract the text inside these tags, we do this with the **getText()** method to later convert it to a integer.  \nWe'll take a longer way to do this, but it is way clearier to see what's going on, well do the following:  \n\n* Create an empty ranks list\n* Iterate through each tag\n* Get the text from the tag\n* Transform the text to an integer\n* Append the this integer to the rank list","bbb61d99":"Now, the next line is very long, but it is to convince you that the lists have all the same lenght (as they should!)","a293f505":"All ready to go! Now we'll just save it and do the same for the women link!","a72d913d":"And there is the html code we wanted! We could find a way to parse it from here, but we don't have to, this is were BeautifulSoup comes in handy! You can check the documentation [here](https:\/\/www.crummy.com\/software\/BeautifulSoup\/bs4\/doc\/). Basically, among some other things, BeautifulSoup allows us to parse html code to make it easier to get information we need.  \n\n**Note** in order for the parser to function in a good way, the website's hmtl code has to be organized (you'll see why in a moment), so we should always take a look at the page using the **inspect element** tool to make sure the html code is well structured.  \n\nNow, let's create a BeautifulSoup object by passing the html code we just got.","c89e0c4c":"# Simple web scraping with BeautifulSoup\n\nWeb scraping is a very useful technique to get data from websites, it is basically a way to parse the html code of a website in order to retrieve the information we want. For this, as you can imagine, you need to do the following:  \n\n* Establish connection with the website\n* Get the html code from the website\n* Analyze the sructure of the html code and locate the information we want to get\n\nFor this example, I'll get the hammer throw top 100 scores for men and women to then save them in a csv file for later analysis. The data will be coming from [World Athletics](https:\/\/www.worldathletics.org\/), take a look at the page, it has very interesting information.  \n\nLet's start with the code, we'll follow the steps I listed above.\n\n## Establish connection with the website\n\nThis is really easy, there are a lot of python packages that allow you to do this, we'll use **urllib**, you can check the documentation [here](https:\/\/docs.python.org\/3\/library\/urllib.html)."}}