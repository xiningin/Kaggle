{"cell_type":{"37fd3a9c":"code","f593906d":"code","b4815685":"code","9accac3a":"code","bcbc91d4":"code","45c65dad":"code","866f8712":"code","d59858af":"code","6447029e":"code","74ebcf60":"code","89de6195":"code","9229e4f5":"code","7a77f977":"code","aa12954d":"code","4bbef272":"code","6153b9b1":"code","561b0d0c":"code","99debd2d":"code","f5bf0766":"code","95a23379":"code","7585af19":"code","90fb5115":"code","8354fed9":"code","dbbe1306":"code","160fc782":"code","a9346534":"code","8d7be503":"code","9b83f698":"code","edb65204":"code","eeb24f12":"code","110e4957":"code","261104b2":"code","f43da886":"code","c6ecfe36":"markdown","0f4aceb4":"markdown","eeab12e3":"markdown","71fde86d":"markdown","3b12b775":"markdown","d40c529b":"markdown","08dd6a0e":"markdown","8143416f":"markdown","b37dfb00":"markdown"},"source":{"37fd3a9c":"import math\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nimport tensorflow as tf\nfrom tensorflow.keras.models import Model\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Dropout, Flatten, BatchNormalization\nfrom tensorflow.keras.optimizers import Adam\nfrom keras.callbacks import EarlyStopping\nfrom keras.callbacks import ReduceLROnPlateau\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split, cross_val_score, KFold\nfrom sklearn import metrics\nfrom sklearn.metrics import roc_auc_score, plot_roc_curve","f593906d":"INT8_MIN = np.iinfo(np.int8).min\nINT8_MAX = np.iinfo(np.int8).max\nINT16_MIN = np.iinfo(np.int16).min\nINT16_MAX = np.iinfo(np.int16).max\nINT32_MIN = np.iinfo(np.int32).min\nINT32_MAX = np.iinfo(np.int32).max\n\nFLOAT16_MIN = np.finfo(np.float16).min\nFLOAT16_MAX = np.finfo(np.float16).max\nFLOAT32_MIN = np.finfo(np.float32).min\nFLOAT32_MAX = np.finfo(np.float32).max\n\n\ndef memory_usage(data, detail = 1):\n    if detail:\n        display(data.memory_usage())\n    memory = data.memory_usage().sum() \/ (1024 * 1024)\n    print(\"Memory usage : {0:.2f}MB\".format(memory))\n    return memory\n\n\ndef compress_dataset(data):\n    memory_before_compress = memory_usage(data, 0)\n    print()\n    print('=' * 50)\n    for col in data.columns:\n        col_dtype = data[col][:100].dtype\n\n        if col_dtype != 'object':\n            print(\"Name: {0:24s} Type: {1}\".format(col, col_dtype))\n            col_series = data[col]\n            col_min = col_series.min()\n            col_max = col_series.max()\n\n            if col_dtype == 'float64':\n                print(\" variable min: {0:15s} max: {1:15s}\".format(str(np.round(col_min, 4)), str(np.round(col_max, 4))))\n                if (col_min > FLOAT16_MIN) and (col_max < FLOAT16_MAX):\n                    data[col] = data[col].astype(np.float16)\n                    print(\"  float16 min: {0:15s} max: {1:15s}\".format(str(FLOAT16_MIN), str(FLOAT16_MAX)))\n                    print(\"compress float64 --> float16\")\n                elif (col_min > FLOAT32_MIN) and (col_max < FLOAT32_MAX):\n                    data[col] = data[col].astype(np.float32)\n                    print(\"  float32 min: {0:15s} max: {1:15s}\".format(str(FLOAT32_MIN), str(FLOAT32_MAX)))\n                    print(\"compress float64 --> float32\")\n                else:\n                    pass\n                memory_after_compress = memory_usage(data, 0)\n                print(\"Compress Rate: [{0:.2%}]\".format((memory_before_compress-memory_after_compress) \/ memory_before_compress))\n                print('=' * 50)\n\n            if col_dtype == 'int64':\n                print(\" variable min: {0:15s} max: {1:15s}\".format(str(col_min), str(col_max)))\n                type_flag = 64\n                if (col_min > INT8_MIN \/ 2) and (col_max < INT8_MAX \/ 2):\n                    type_flag = 8\n                    data[col] = data[col].astype(np.int8)\n                    print(\"     int8 min: {0:15s} max: {1:15s}\".format(str(INT8_MIN), str(INT8_MAX)))\n                elif (col_min > INT16_MIN) and (col_max < INT16_MAX):\n                    type_flag = 16\n                    data[col] = data[col].astype(np.int16)\n                    print(\"    int16 min: {0:15s} max: {1:15s}\".format(str(INT16_MIN), str(INT16_MAX)))\n                elif (col_min > INT32_MIN) and (col_max < INT32_MAX):\n                    type_flag = 32\n                    data[col] = data[col].astype(np.int32)\n                    print(\"    int32 min: {0:15s} max: {1:15s}\".format(str(INT32_MIN), str(INT32_MAX)))\n                    type_flag = 1\n                else:\n                    pass\n                memory_after_compress = memory_usage(data, 0)\n                print(\"Compress Rate: [{0:.2%}]\".format((memory_before_compress-memory_after_compress) \/ memory_before_compress))\n                if type_flag == 32:\n                    print(\"compress (int64) ==> (int32)\")\n                elif type_flag == 16:\n                    print(\"compress (int64) ==> (int16)\")\n                else:\n                    print(\"compress (int64) ==> (int8)\")\n                print('=' * 50)\n\n    print()\n    memory_after_compress = memory_usage(data, 0)\n    print(\"Compress Rate: [{0:.2%}]\".format((memory_before_compress-memory_after_compress) \/ memory_before_compress))\n    \n    return data","b4815685":"df_train = pd.read_csv('..\/input\/tabular-playground-series-nov-2021\/train.csv')\ndf_train.head()","9accac3a":"df_train = df_train.drop('id', axis = 1)","bcbc91d4":"print(f'Train set shape:   {df_train.shape}')","45c65dad":"df_train.info()","866f8712":"df_train.describe()","d59858af":"df_train.isnull().sum().max() == 0","6447029e":"df_test = pd.read_csv('..\/input\/tabular-playground-series-nov-2021\/test.csv')\ndf_test.head()","74ebcf60":"df_test = df_test.drop('id', axis = 1)","89de6195":"print(f'Test set shape:   {df_test.shape}')","9229e4f5":"df_test.info()","7a77f977":"df_test.describe()","aa12954d":"df_test.isnull().sum().max() == 0","4bbef272":"plt.figure(figsize = (5,5))\nplt.pie(x = df_train['target'].value_counts(), labels = ['1', '0'], autopct = '%1.2f%%', \n        explode = [0.05, 0], startangle = 90)","6153b9b1":"df_train_sample = df_train.sample(n = 20000)\ndf_test_sample = df_test.sample(n = 20000)","561b0d0c":"fig, axes = plt.subplots(10,10, figsize = (30, 30))\naxes = axes.flatten()\n\nfor idx, ax in enumerate(axes):\n    \n    sns.kdeplot(data = df_train_sample, ax = ax, fill = True, x = f'f{idx}', \n                palette = ['#4DB6AC', 'red'])\n    sns.kdeplot(data = df_test_sample, ax = ax, fill = True, x = f'f{idx}', \n                palette = ['#4DB6AC', 'blue'])\n \n    ax.set_xticks([]); ax.set_yticks([]); ax.set_xlabel('')\n    ax.set_ylabel(''); ax.spines['left'].set_visible(False)\n    ax.set_title(f'f{idx}', loc = 'right', weight = 'bold', fontsize = 10)\n\nfig.supxlabel('Probability Density Function Estimation', ha = 'center', fontweight = 'bold')\nfig.tight_layout()\nplt.show()","99debd2d":"peaks = ['f0','f2','f4','f9','f12','f16','f19','f20','f23','f24','f27',\n    'f28','f30','f31','f32','f33','f35','f39','f42','f44','f46','f48',\n    'f49','f51','f52','f53','f56','f58','f59','f60','f61','f62','f63',\n    'f64','f68','f69','f72','f73','f75','f76','f78','f79','f81','f83',\n    'f84','f87','f88','f89','f90','f92','f93','f94','f95','f98','f99']\n\nno_peaks = [feats for feats in df_test.columns if feats not in peaks]\n\ndf_train['median_peaks'] = df_train[peaks].median(axis = 1)\ndf_train['median_no_peaks'] = df_train[no_peaks].median(axis = 1)\ndf_test['median_peaks'] = df_test[peaks].median(axis = 1)\ndf_test['median_no_peaks'] = df_test[no_peaks].median(axis = 1)\n\ndf_train['mean_peaks'] = df_train[peaks].mean(axis = 1)\ndf_train['mean_no_peaks'] = df_train[no_peaks].mean(axis = 1)\ndf_test['mean_peaks'] = df_test[peaks].mean(axis = 1)\ndf_test['mean_no_peaks'] = df_test[no_peaks].mean(axis = 1)\n\ndf_train['std_peaks'] = df_train[peaks].std(axis = 1)\ndf_train['std_no_peaks'] = df_train[no_peaks].std(axis = 1)\ndf_test['std_peaks'] = df_test[peaks].std(axis = 1)\ndf_test['std_no_peaks'] = df_test[no_peaks].std(axis = 1)\n\ndf_train['sum_peaks'] = df_train[peaks].sum(axis = 1)\ndf_train['sum_no_peaks'] = df_train[no_peaks].sum(axis = 1)\ndf_test['sum_peaks'] = df_test[peaks].sum(axis = 1)\ndf_test['sum_no_peaks'] = df_test[no_peaks].sum(axis = 1)\n\ndf_train['min_peaks'] = df_train[peaks].min(axis = 1)\ndf_train['min_no_peaks'] = df_train[no_peaks].min(axis = 1)\ndf_test['min_peaks'] = df_test[peaks].min(axis = 1)\ndf_test['min_no_peaks'] = df_test[no_peaks].min(axis = 1)\n\ndf_train['max_peaks'] = df_train[peaks].max(axis = 1)\ndf_train['max_no_peaks'] = df_train[no_peaks].max(axis = 1)\ndf_test['max_peaks'] = df_test[peaks].max(axis = 1)\ndf_test['max_no_peaks'] = df_test[no_peaks].max(axis = 1)\n\ndf_train['skew_peaks'] = df_train[peaks].skew(axis = 1)\ndf_train['skew_no_peaks'] = df_train[no_peaks].skew(axis = 1)\ndf_test['skew_peaks'] = df_test[peaks].skew(axis = 1)\ndf_test['skew_no_peaks'] = df_test[no_peaks].skew(axis = 1)","f5bf0766":"df_train.head()","95a23379":"df_test.head()","7585af19":"scaler = StandardScaler()\n\nfloat_columns = [feats for feats in df_train.select_dtypes('float')]\n\ndf_train[float_columns] = scaler.fit_transform(df_train[float_columns])\ndf_test = pd.DataFrame(scaler.transform(df_test), columns = df_test.columns)","90fb5115":"df_train.head()","8354fed9":"df_test.head()","dbbe1306":"df_train = compress_dataset(df_train)","160fc782":"df_test = compress_dataset(df_test)","a9346534":"X = df_train.select_dtypes('float16')\ny = df_train['target']","8d7be503":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 12)","9b83f698":"epochs = 200 \nbatch_size = 1024\n\nearly_stop = EarlyStopping(patience = 20, restore_best_weights = True, mode = 'min', verbose = 1)\nred_lr = ReduceLROnPlateau(patience = 5, factor = 0.2, verbose = 1)\n\nmodel = Sequential()\nmodel.add(BatchNormalization())\nmodel.add(Dropout(0.4))\nmodel.add(Flatten())\nmodel.add(BatchNormalization())\nmodel.add(Dense(128, activation = 'relu', input_shape = [X.shape[1]]))\nmodel.add(Dropout(0.4))\nmodel.add(BatchNormalization())\nmodel.add(Dense(64, activation = 'relu'))\nmodel.add(Dropout(0.4))\nmodel.add(BatchNormalization())\nmodel.add(Dense(32, activation = 'relu'))\nmodel.add(Dropout(0.4))\nmodel.add(BatchNormalization())\nmodel.add(Dense(8, activation = 'relu'))\nmodel.add(Dropout(0.4))\nmodel.add(BatchNormalization())\nmodel.add(Dense(1, activation = 'sigmoid'))\n\noptimizer = Adam(learning_rate = 0.002, beta_1 = 0.9, beta_2 = 0.999)\n\nmodel.compile(optimizer = optimizer, loss = 'binary_crossentropy', metrics = ['AUC'])","edb65204":"cnn = model.fit(X_train, y_train, validation_data = (X_test, y_test), epochs = epochs, \n                batch_size = batch_size, callbacks = [early_stop, red_lr])","eeb24f12":"scores = []\npreds_tests = []\ncv = KFold(n_splits = 10, shuffle = True)\n\nfor train_idx, test_idx in cv.split(X):\n    X_train, y_train = X.iloc[train_idx], y.iloc[train_idx]\n    X_test, y_test = X.iloc[test_idx], y.iloc[test_idx]\n\n    model.fit(X_train, y_train, validation_data = (X_test, y_test), epochs = epochs, \n            batch_size = batch_size, callbacks = [early_stop, red_lr])\n\n    preds = model.predict(X_test).reshape(1,-1)[0]\n    preds_test = model.predict(df_test).reshape(1,-1)[0]\n\n    score = roc_auc_score(y_test, preds)\n    scores.append(score)\n    preds_tests.append(preds_test)\n\nprint('************************************')    \nprint(f'Mean AUROC score:       {np.mean(scores)}')\nprint(f'Std AUROC:              {np.std(scores)}')","110e4957":"plt.figure(figsize = (15, 5))\n\nplt.plot(cnn.history['auc'])\nplt.plot(cnn.history['val_auc'])\n\nplt.title('Model AUC', size = 16)\nplt.xlabel('Epoch')\nplt.legend(['Train AUC', 'Test AUC'], loc = 4)\nplt.grid()\nplt.show()\n           \nplt.figure(figsize = (15, 5))\n\nplt.plot(cnn.history['loss'])\nplt.plot(cnn.history['val_loss'])\n\nplt.title('Model loss', size = 16)\nplt.xlabel('Epoch')\nplt.legend(['Train loss', 'Test loss'], loc = 7)\nplt.grid()\nplt.show()","261104b2":"sub = pd.read_csv('..\/input\/tabular-playground-series-nov-2021\/sample_submission.csv')\nsub['target'] = np.mean(preds_tests, axis = 0)\nsub.head()","f43da886":"sub.to_csv('nn_0.74733.csv', index = False)","c6ecfe36":"## Target summary","0f4aceb4":"## Submission","eeab12e3":"## CNN","71fde86d":"## Standard Scaler","3b12b775":"**Summary:**\n1. Train set contains **600 000** rows and **102** columns (including `id`)\n2. Test set contains **540 000** rows and **101** columns (including `id`)\n3. All columns are 'float' type (except `target`)\n4. There are **no missing values** in train set and test set.\n5. Classes in target column are **balansed**.","d40c529b":"## Feature engineering","08dd6a0e":"## Train set summary","8143416f":"## Test set summary","b37dfb00":"## Release memory"}}