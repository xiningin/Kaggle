{"cell_type":{"c29b0844":"code","d9b2e445":"code","ae4cc55a":"code","d5ccfd5d":"code","b48fb003":"code","884e9512":"code","84eccbd5":"code","86640f3d":"code","4f2535dd":"code","ef873377":"code","326cd967":"code","df28701e":"code","5576168e":"code","75f5684e":"code","2368dccf":"code","48f25c91":"code","ed306548":"code","1893f4d2":"code","ecd6a3ed":"code","d58aaff4":"code","442b5575":"code","9df3aeed":"code","43aedf68":"code","d912f294":"code","633dff5e":"code","fdec2cd2":"code","0238c148":"code","105e87d2":"code","cebd2830":"code","1da0274d":"code","548e21cd":"code","d0137522":"code","7bdd81a4":"code","bbad3347":"code","7d7ad531":"markdown","d96859b3":"markdown","7936d31c":"markdown","b3cc523e":"markdown","9b517e2a":"markdown","78e746cd":"markdown","3b6d97cc":"markdown","d035d47c":"markdown","def4e66b":"markdown","607c5da0":"markdown","2edbdc3e":"markdown","99130324":"markdown","1a55af16":"markdown","2120b1e7":"markdown","06dbba09":"markdown","a19fafdf":"markdown","4cb3a712":"markdown","42f69513":"markdown","27515823":"markdown","5edfd81a":"markdown","2c7faa11":"markdown"},"source":{"c29b0844":"import numpy as np \nimport pandas as pd \nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom sklearn import preprocessing\nfrom sklearn.preprocessing import MinMaxScaler","d9b2e445":"data = pd.read_csv('..\/input\/breast-cancer-prediction-dataset\/Breast_cancer_data.csv')\nprint('Dataset :',data.shape)\nx = data.iloc[:, [0, 1, 2, 3]].values\ndata.info()\ndata[0:10]","ae4cc55a":"data.diagnosis.value_counts()[0:30].plot(kind='bar')\nplt.show()","d5ccfd5d":"data = data[['mean_radius','mean_texture','mean_perimeter','mean_area', 'mean_smoothness','diagnosis']] #Subsetting the data\ncor = data.corr() #Calculate the correlation of the above variables\nsns.heatmap(cor, square = True) #Plot the correlation as heat map","b48fb003":"sns.set_style(\"ticks\")\nsns.pairplot(data,hue=\"diagnosis\",size=3);\nplt.show()","884e9512":"from sklearn.model_selection import train_test_split\nY = data['diagnosis']\nX = data.drop(columns=['diagnosis'])\nX_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=9)","84eccbd5":"print('X train shape: ', X_train.shape)\nprint('Y train shape: ', Y_train.shape)\nprint('X test shape: ', X_test.shape)\nprint('Y test shape: ', Y_test.shape)","86640f3d":"from sklearn.linear_model import LogisticRegression\n\n# We defining the model\nlogreg = LogisticRegression(C=10)\n\n# We train the model\nlogreg.fit(X_train, Y_train)\n\n# We predict target values\nY_predict1 = logreg.predict(X_test)","4f2535dd":"# The confusion matrix\nfrom sklearn.metrics import confusion_matrix\nimport seaborn as sns\n\nlogreg_cm = confusion_matrix(Y_test, Y_predict1)\nf, ax = plt.subplots(figsize=(5,5))\nsns.heatmap(logreg_cm, annot=True, linewidth=0.7, linecolor='red', fmt='g', ax=ax, cmap=\"BuPu\")\nplt.title('Logistic Regression Classification Confusion Matrix')\nplt.xlabel('Y predict')\nplt.ylabel('Y test')\nplt.show()","ef873377":"# Test score\nscore_logreg = logreg.score(X_test, Y_test)\nprint(score_logreg)","326cd967":"#precision and recall\nfrom sklearn.metrics import average_precision_score\naverage_precision = average_precision_score(Y_test, Y_predict1)\n\nprint('Average precision-recall score: {0:0.2f}'.format(\n      average_precision))","df28701e":"from sklearn.ensemble import BaggingClassifier\nfrom sklearn.multiclass import OneVsRestClassifier\nfrom sklearn.svm import SVC\n\n# We define the SVM model\nsvmcla = OneVsRestClassifier(BaggingClassifier(SVC(C=10,kernel='rbf',random_state=9, probability=True), \n                                               n_jobs=-1))\n\n# We train model\nsvmcla.fit(X_train, Y_train)\n\n# We predict target values\nY_predict2 = svmcla.predict(X_test)","5576168e":"# The confusion matrix\nsvmcla_cm = confusion_matrix(Y_test, Y_predict2)\nf, ax = plt.subplots(figsize=(5,5))\nsns.heatmap(svmcla_cm, annot=True, linewidth=0.7, linecolor='red', fmt='g', ax=ax, cmap=\"BuPu\")\nplt.title('SVM Classification Confusion Matrix')\nplt.xlabel('Y predict')\nplt.ylabel('Y test')\nplt.show()","75f5684e":"# Test score\nscore_svmcla = svmcla.score(X_test, Y_test)\nprint(score_svmcla)","2368dccf":"#precision and recall\nfrom sklearn.metrics import average_precision_score\naverage_precision = average_precision_score(Y_test, Y_predict2)\n\nprint('Average precision-recall score: {0:0.2f}'.format(\n      average_precision))","48f25c91":"from sklearn.naive_bayes import GaussianNB\n\n# We define the model\nnbcla = GaussianNB()\n\n# We train model\nnbcla.fit(X_train, Y_train)\n\n# We predict target values\nY_predict3 = nbcla.predict(X_test)","ed306548":"# The confusion matrix\nnbcla_cm = confusion_matrix(Y_test, Y_predict3)\nf, ax = plt.subplots(figsize=(5,5))\nsns.heatmap(nbcla_cm, annot=True, linewidth=0.7, linecolor='red', fmt='g', ax=ax, cmap=\"BuPu\")\nplt.title('Naive Bayes Classification Confusion Matrix')\nplt.xlabel('Y predict')\nplt.ylabel('Y test')\nplt.show()","1893f4d2":"# Test score\nscore_nbcla = nbcla.score(X_test, Y_test)\nprint(score_nbcla)","ecd6a3ed":"#precision and recall\nfrom sklearn.metrics import average_precision_score\naverage_precision = average_precision_score(Y_test, Y_predict3)\n\nprint('Average precision-recall score: {0:0.2f}'.format(\n      average_precision))","d58aaff4":"from sklearn.tree import DecisionTreeClassifier\n\n# We define the model\ndtcla = DecisionTreeClassifier(random_state=9)\n\n# We train model\ndtcla.fit(X_train, Y_train)\n\n# We predict target values\nY_predict4 = dtcla.predict(X_test)","442b5575":"# The confusion matrix\ndtcla_cm = confusion_matrix(Y_test, Y_predict4)\nf, ax = plt.subplots(figsize=(5,5))\nsns.heatmap(dtcla_cm, annot=True, linewidth=0.7, linecolor='red', fmt='g', ax=ax, cmap=\"BuPu\")\nplt.title('Decision Tree Classification Confusion Matrix')\nplt.xlabel('Y predict')\nplt.ylabel('Y test')\nplt.show()","9df3aeed":"# Test score\nscore_dtcla = dtcla.score(X_test, Y_test)\nprint(score_dtcla)","43aedf68":"#precision and recall\nfrom sklearn.metrics import average_precision_score\naverage_precision = average_precision_score(Y_test, Y_predict4)\n\nprint('Average precision-recall score: {0:0.2f}'.format(\n      average_precision))","d912f294":"from sklearn.ensemble import RandomForestClassifier\n\n# We define the model\nrfcla = RandomForestClassifier(n_estimators=100,random_state=9,n_jobs=-1)\n\n# We train model\nrfcla.fit(X_train, Y_train)\n\n# We predict target values\nY_predict5 = rfcla.predict(X_test)","633dff5e":"# The confusion matrix\nrfcla_cm = confusion_matrix(Y_test, Y_predict5)\nf, ax = plt.subplots(figsize=(5,5))\nsns.heatmap(rfcla_cm, annot=True, linewidth=0.7, linecolor='red', fmt='g', ax=ax, cmap=\"BuPu\")\nplt.title('Random Forest Classification Confusion Matrix')\nplt.xlabel('Y predict')\nplt.ylabel('Y test')\nplt.show()","fdec2cd2":"# Test score\nscore_rfcla = rfcla.score(X_test, Y_test)\nprint(score_rfcla)","0238c148":"#precision and recall\nfrom sklearn.metrics import average_precision_score\naverage_precision = average_precision_score(Y_test, Y_predict5)\n\nprint('Average precision-recall score: {0:0.2f}'.format(\n      average_precision))","105e87d2":"from sklearn.neighbors import KNeighborsClassifier\n\n# We define the model\nknncla = KNeighborsClassifier(n_neighbors=5,n_jobs=-1)\n\n# We train model\nknncla.fit(X_train, Y_train)\n\n# We predict target values\nY_predict6 = knncla.predict(X_test)","cebd2830":"# The confusion matrix\nknncla_cm = confusion_matrix(Y_test, Y_predict6)\nf, ax = plt.subplots(figsize=(5,5))\nsns.heatmap(knncla_cm, annot=True, linewidth=0.7, linecolor='red', fmt='g', ax=ax, cmap=\"BuPu\")\nplt.title('KNN Classification Confusion Matrix')\nplt.xlabel('Y predict')\nplt.ylabel('Y test')\nplt.show()","1da0274d":"# Test score\nscore_knncla= knncla.score(X_test, Y_test)\nprint(score_knncla)","548e21cd":"#precision and recall\nfrom sklearn.metrics import average_precision_score\naverage_precision = average_precision_score(Y_test, Y_predict6)\n\nprint('Average precision-recall score: {0:0.2f}'.format(\n      average_precision))","d0137522":"Testscores = pd.Series([score_logreg, score_svmcla, score_nbcla, score_dtcla, score_rfcla, score_knncla], \n                        index=['Logistic Regression Score', 'Support Vector Machine Score', 'Naive Bayes Score', 'Decision Tree Score', 'Random Forest Score', 'K-Nearest Neighbour Score']) \nprint(Testscores)","7bdd81a4":"fig = plt.figure(figsize=(15,15))\nax1 = fig.add_subplot(3, 3, 1) \nax1.set_title('Logistic Regression Classification') \nax2 = fig.add_subplot(3, 3, 2) \nax2.set_title('SVM Classification')\nax3 = fig.add_subplot(3, 3, 3)\nax3.set_title('Naive Bayes Classification')\nax4 = fig.add_subplot(3, 3, 4)\nax4.set_title('Decision Tree Classification')\nax5 = fig.add_subplot(3, 3, 5)\nax5.set_title('Random Forest Classification')\nax6 = fig.add_subplot(3, 3, 6)\nax6.set_title('KNN Classification')\nsns.heatmap(data=logreg_cm, annot=True, linewidth=0.7, linecolor='red',cmap=\"BuPu\" ,fmt='g', ax=ax1)\nsns.heatmap(data=svmcla_cm, annot=True, linewidth=0.7, linecolor='red',cmap=\"BuPu\" ,fmt='g', ax=ax2)  \nsns.heatmap(data=nbcla_cm, annot=True, linewidth=0.7, linecolor='red',cmap=\"BuPu\" ,fmt='g', ax=ax3)\nsns.heatmap(data=dtcla_cm, annot=True, linewidth=0.7, linecolor='red',cmap=\"BuPu\" ,fmt='g', ax=ax4)\nsns.heatmap(data=rfcla_cm, annot=True, linewidth=0.7, linecolor='red',cmap=\"BuPu\" ,fmt='g', ax=ax5)\nsns.heatmap(data=knncla_cm, annot=True, linewidth=0.7, linecolor='red',cmap=\"BuPu\" ,fmt='g', ax=ax6)\nplt.show()","bbad3347":"from sklearn.metrics import roc_curve\n\n# Logistic Regression Classification\nY_predict1_proba = logreg.predict_proba(X_test)\nY_predict1_proba = Y_predict1_proba[:, 1]\nfpr, tpr, thresholds = roc_curve(Y_test, Y_predict1_proba)\nplt.subplot(331)\nplt.plot([0,1],[0,1],'k--')\nplt.plot(fpr,tpr, label='ANN')\nplt.xlabel('fpr')\nplt.ylabel('tpr')\nplt.title('ROC Curve Logistic Regression')\nplt.grid(True)\n\n# SVM Classification\nY_predict2_proba = svmcla.predict_proba(X_test)\nY_predict2_proba = Y_predict2_proba[:, 1]\nfpr, tpr, thresholds = roc_curve(Y_test, Y_predict2_proba)\nplt.subplot(332)\nplt.plot([0,1],[0,1],'k--')\nplt.plot(fpr,tpr, label='ANN')\nplt.xlabel('fpr')\nplt.ylabel('tpr')\nplt.title('ROC Curve SVM')\nplt.grid(True)\n\n# Naive Bayes Classification\nY_predict3_proba = nbcla.predict_proba(X_test)\nY_predict3_proba = Y_predict3_proba[:, 1]\nfpr, tpr, thresholds = roc_curve(Y_test, Y_predict3_proba)\nplt.subplot(333)\nplt.plot([0,1],[0,1],'k--')\nplt.plot(fpr,tpr, label='ANN')\nplt.xlabel('fpr')\nplt.ylabel('tpr')\nplt.title('ROC Curve Naive Bayes')\nplt.grid(True)\n\n# Decision Tree Classification\nY_predict4_proba = dtcla.predict_proba(X_test)\nY_predict4_proba = Y_predict4_proba[:, 1]\nfpr, tpr, thresholds = roc_curve(Y_test, Y_predict4_proba)\nplt.subplot(334)\nplt.plot([0,1],[0,1],'k--')\nplt.plot(fpr,tpr, label='ANN')\nplt.xlabel('fpr')\nplt.ylabel('tpr')\nplt.title('ROC Curve Decision Tree')\nplt.grid(True)\n\n# Random Forest Classification\nY_predict5_proba = rfcla.predict_proba(X_test)\nY_predict5_proba = Y_predict5_proba[:, 1]\nfpr, tpr, thresholds = roc_curve(Y_test, Y_predict5_proba)\nplt.subplot(335)\nplt.plot([0,1],[0,1],'k--')\nplt.plot(fpr,tpr, label='ANN')\nplt.xlabel('fpr')\nplt.ylabel('tpr')\nplt.title('ROC Curve Random Forest')\nplt.grid(True)\n\n# KNN Classification\nY_predict6_proba = knncla.predict_proba(X_test)\nY_predict6_proba = Y_predict6_proba[:, 1]\nfpr, tpr, thresholds = roc_curve(Y_test, Y_predict6_proba)\nplt.subplot(336)\nplt.plot([0,1],[0,1],'k--')\nplt.plot(fpr,tpr, label='ANN')\nplt.xlabel('fpr')\nplt.ylabel('tpr')\nplt.title('ROC Curve KNN')\nplt.grid(True)\nplt.subplots_adjust(top=2, bottom=0.08, left=0.10, right=1.4, hspace=0.45, wspace=0.45)\nplt.show()","7d7ad531":"Data for training and testing\nTo select a set of training data that will be input in the Machine Learning algorithm, to ensure that the classification algorithm training can be generalized well to new data. For this study using a sample size of 20%, assumed it ideal ratio between training and testing","d96859b3":"### Test score","7936d31c":"**VISUALIZING THE DATA\n**","b3cc523e":"As you can see above, we obtain the heatmap of correlation among the variables. The color palette in the side represents the amount of correlation among the variables. The lighter shade represents a high correlation.","9b517e2a":"Breast cancer is a disease in which cells in the breast grow out of control. There are different kinds of breast cancer. The kind of breast cancer depends on which cells in the breast turn into cancer.\nsource information: https:\/\/www.cdc.gov\/cancer\/breast\/basic_info\/what-is-breast-cancer.htm","78e746cd":"**Distribution of diagnosis\n357 benign, 212 malignant**","3b6d97cc":"### ROC curve","d035d47c":"## 2. SVM (Support Vector Machine) classification\n\nSVMs (Support Vector Machine) have shown a rapid proliferation during the last years. The learning problem setting for SVMs corresponds to a some unknown and nonlinear dependency (mapping, function) $y = f(x)$ between some high-dimensional input vector $x$ and scalar output $y$. It is noteworthy that there is no information on the joint probability functions, therefore, a free distribution learning must be carried out. The only information available is a training data set $D = {(x_i, y_i) \u2208 X\u00d7Y }, i = 1$, $l$, where $l$ stands for the number of the training data pairs and is therefore equal to the size of the training data set $D$, additionally, $y_i$ is denoted as $d_i$, where $d$ stands for a desired (target) value. Hence, SVMs belong to the supervised learning techniques.\n\nFrom the classification approach, the goal of SVM is to find a hyperplane in an N-dimensional space that clearly classifies the data points. Thus hyperplanes are decision boundaries that help classify the data points. Data points falling on either side of the hyperplane can be attributed to different classes.\n\n\n\n","def4e66b":"## 3. Naive bayes classification\n\nThe naive Bayesian classifier is a probabilistic classifier based on Bayes' theorem with strong independence assumptions between the features. Thus, using Bayes theorem $\\left(P(X|Y)=\\frac{P(Y|X)P(X)}{P(Y)}\\right)$, we can find the probability of $X$ happening, given that $Y$ has occurred. Here, $Y$ is the evidence and $X$ is the hypothesis. The assumption made here is that the presence of one particular feature does not affect the other (the predictors\/features are independent). Hence it is called naive. In this case we will assume that we assume the values are sampled from a Gaussian distribution and therefore we consider a Gaussian Naive Bayes.","607c5da0":"## 1. Logistic regression classification\n\nLogistic regression is a technique that can be applied to binary classification problems. This technique uses the logistic function or sigmoid function, which is an S-shaped curve that can assume any real value number and assign it to a value between 0 and 1, but never exactly in those limits. Thus, logistic regression models the probability of the default class (the probability that an input $(X)$ belongs to the default class $(Y=1)$) $(P(X)=P(Y=1|X))$. In order to make the prediction of the probability, the logistic function is used, which allows us to obtain the log-odds or the probit. Thus, the model is a linear combination of the inputs, but that this linear combination relates to the log-odds of the default class.\n\nStarted from make an instance of the model setting the default values. Specify the inverse of the regularization strength in 10. Trained the logistic regression model with the training data, and then applied such model to the test data.","2edbdc3e":"Import Library","99130324":"From a Comparison of classification techniques, we plotting ROC to illustrates the diagnostic ability of a binary classifier system as its discrimination threshold is varied.","1a55af16":"## 6. K-Nearest Neighbor classification\n\nK-Nearest neighbors is a technique that stores all available cases and **classifies new cases based on a similarity measure (e.g., distance functions)**. This technique is non-parametric since there are no assumptions for the distribution of underlying data and it is lazy since it does not need any training data point model generation. All the training data used in the test phase. **This makes the training faster and the test phase slower and more costlier. In this technique, the number of neighbors k is usually an odd number if the number of classes is 2**. For finding closest similar points,  find the distance between points using distance measures such as Euclidean distance, Hamming distance, Manhattan distance and Minkowski distance.\n\n","2120b1e7":"As you can see above, a Comparison of classification techniques, we can evaluate that here Logistic Regression Classification and Random Forest have the most optimal result of the accuracy.","06dbba09":"![http:\/\/triplesteptowardthecure.org\/images\/slide1.jpg](http:\/\/triplesteptowardthecure.org\/images\/slide1.jpg)\n\n\n\n\n\nSource Image : http:\/\/triplesteptowardthecure.org\/understanding.php","a19fafdf":"### The confusion matrix\nConfusion matrix is a specific table layout that allows visualization of the performance of an algorithm, typically a supervised learning one.","4cb3a712":"## Data\n\n","42f69513":"# Comparison of classification techniques","27515823":"## 4. Decision tree classification\n\nA decision tree is a flowchart-like tree structure where an internal node represents feature, the branch represents a decision rule, and each leaf node represents the outcome. The decision tree analyzes a set of data to construct a set of rules or questions, which are used to predict a class, i.e., the goal of decision tree is to create a model that predicts the value of a target variable by learning simple decision rules inferred from the data features. In this sense the decision tree selects the best attribute using to divide the records, converting that attribute into a decision node and dividing the data set into smaller subsets, to finally start the construction of the tree repeating this process recursively. ","5edfd81a":"## 5. Random forest classification\n\nBased on the previous classification method, random forest is a supervised learning algorithm that creates a forest randomly. This forest, is a set of decision trees, most of the times trained with the bagging method. The essential idea of bagging is to average many noisy but approximately impartial models, and therefore reduce the variation. Each tree is constructed using the following algorithm:\n\n* Let $N$ be the number of test cases, $M$ is the number of variables in the classifier.\n* Let $m$ be the number of input variables to be used to determine the decision in a given node; $m<M$.\n* Choose a training set for this tree and use the rest of the test cases to estimate the error.\n* For each node of the tree, randomly choose $m$ variables on which to base the decision. Calculate the best partition of the training set from the $m$ variables.\n\nFor prediction a new case is pushed down the tree. Then it is assigned the label of the terminal node where it ends. This process is iterated by all the trees in the assembly, and the label that gets the most incidents is reported as the prediction. We define the number of trees in the forest in 100. ","2c7faa11":"In this study, we tried to predict Breast Cancer using 6 different algorithm:\n\n1. Logistic regression classification\n2. SVM (Support Vector Machine) classification\n3. Naive bayes classification\n4. Decision tree classification\n5. Random forest classification\n6. K-Nearest Neighbor classification\n\n\nPredictor variable use in classifying breast cancer, its features are computed for each cell nucleus:\n1. mean_radius (mean of distances from center to points on the perimeter)\n2. mean_texture (standard deviation of gray-scale values)\n3. mean_perimeter \n4. mean_area  \n5. mean_smoothness (local variation in radius lengths)\n\nSource Attribute Information:https:\/\/archive.ics.uci.edu\/ml\/datasets\/Breast+Cancer+Wisconsin+(Diagnostic)\n\nWe measure the study of 7 different algorithms using a **confusion matrix**. "}}