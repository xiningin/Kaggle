{"cell_type":{"9675d2a2":"code","d79080ac":"code","4d686f43":"code","c5c70b81":"code","14559b00":"code","90adf22d":"code","96bfd8db":"code","0b21fa45":"code","7295b35c":"code","58824a69":"code","718d70e5":"code","c758845c":"code","996840fd":"code","e44c4c00":"code","f0bb9279":"code","b6770d9e":"code","3a87d392":"code","b8a87c67":"code","093bc8ac":"code","185e2959":"code","68b2a920":"code","92b81e0c":"code","f4adf2b4":"code","d2b4c792":"code","50cd9ea6":"code","78b8788b":"markdown","88acbb98":"markdown","1a8e9274":"markdown"},"source":{"9675d2a2":"#codes from Rodrigo Lima  @rodrigolima82\nfrom IPython.display import Image\nImage(url = 'https:\/\/encrypted-tbn0.gstatic.com\/images?q=tbn%3AANd9GcQtFO4WaMLX_4jLojLC3UyAzwI4Et3_dMfE7GpLoqQ_IqZF1KCn&usqp=CAU',width=400,height=400)","d79080ac":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport tensorflow as tf\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","4d686f43":"ls ..\/input\/nerdy-personality-attributes-scale-responses\/","c5c70b81":"path_to_file = '..\/input\/nerdy-personality-attributes-scale-responses\/codebook.txt'","14559b00":"text = open(path_to_file, 'r',encoding='utf-8',\n                 errors='ignore').read()","90adf22d":"print(text[:1500])","96bfd8db":"# The unique characters in the file\nvocab = sorted(set(text))\nprint(vocab)\nlen(vocab)","0b21fa45":"char_to_ind = {u:i for i, u in enumerate(vocab)}\nind_to_char = np.array(vocab)\nencoded_text = np.array([char_to_ind[c] for c in text])\nseq_len = 250\ntotal_num_seq = len(text)\/\/(seq_len+1)\ntotal_num_seq","7295b35c":"# Create Training Sequences\nchar_dataset = tf.data.Dataset.from_tensor_slices(encoded_text)\n\nsequences = char_dataset.batch(seq_len+1, drop_remainder=True)\n\ndef create_seq_targets(seq):\n    input_txt = seq[:-1]\n    target_txt = seq[1:]\n    return input_txt, target_txt\n\ndataset = sequences.map(create_seq_targets)","58824a69":"# Batch size\nbatch_size = 128\n\n# Buffer size to shuffle the dataset so it doesn't attempt to shuffle\n# the entire sequence in memory. Instead, it maintains a buffer in which it shuffles elements\nbuffer_size = 10000\n\ndataset = dataset.shuffle(buffer_size).batch(batch_size, drop_remainder=True)\n\n# Length of the vocabulary in chars\nvocab_size = len(vocab)\n\n# The embedding dimension\nembed_dim = 64\n\n# Number of RNN units\nrnn_neurons = 2052","718d70e5":"from tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import LSTM,Dense,Embedding,Dropout,GRU\nfrom tensorflow.keras.losses import sparse_categorical_crossentropy","c758845c":"def sparse_cat_loss(y_true,y_pred):\n    return sparse_categorical_crossentropy(y_true, y_pred, from_logits=True)","996840fd":"def create_model(vocab_size, embed_dim, rnn_neurons, batch_size):\n    model = Sequential()\n    model.add(Embedding(vocab_size, embed_dim,batch_input_shape=[batch_size, None]))\n    model.add(GRU(rnn_neurons,return_sequences=True,stateful=True,recurrent_initializer='glorot_uniform'))\n    # Final Dense Layer to Predict\n    model.add(Dense(vocab_size))\n    model.compile(optimizer='adam', loss=sparse_cat_loss) \n    return model","e44c4c00":"model = create_model(\n  vocab_size = vocab_size,\n  embed_dim=embed_dim,\n  rnn_neurons=rnn_neurons,\n  batch_size=batch_size)","f0bb9279":"model.summary()","b6770d9e":"epochs = 50 ","3a87d392":"model.fit(dataset,epochs=epochs)","b8a87c67":"model.save('codebook')","093bc8ac":"from tensorflow.keras.models import load_model","185e2959":"model = create_model(vocab_size, embed_dim, rnn_neurons, batch_size=1)\n\nmodel.load_weights('..\/input\/nerdy-personality-attributes-scale-responses\/codebook.txt')\n\nmodel.build(tf.TensorShape([1, None]))","68b2a920":"model.summary()","92b81e0c":"def generate_text(model, start_seed,gen_size=100,temp=1.1):\n  num_generate = gen_size\n  input_eval = [char_to_ind[s] for s in start_seed]\n  input_eval = tf.expand_dims(input_eval, 0)\n  text_generated = []\n  temperature = temp\n  model.reset_states()\n  for i in range(num_generate):\n      predictions = model(input_eval)\n      predictions = tf.squeeze(predictions, 0)\n      predictions = predictions \/ temperature\n      predicted_id = tf.random.categorical(predictions, num_samples=1)[-1,0].numpy()\n      input_eval = tf.expand_dims([predicted_id], 0)\n      text_generated.append(ind_to_char[predicted_id])\n  return (start_seed + ''.join(text_generated))","f4adf2b4":"print(generate_text(model,\"Q14\",gen_size=1000))","d2b4c792":"print(generate_text(model,\"Q26\",gen_size=2000))","50cd9ea6":"#codes from Rodrigo Lima  @rodrigolima82\nfrom IPython.display import Image\nImage(url = 'https:\/\/encrypted-tbn0.gstatic.com\/images?q=tbn%3AANd9GcTs7mQNRAcbTcMrk9-9ztfAbPOc4n43JXdgkZYcuxzpyp-Tvij-&usqp=CAU',width=400,height=400)","78b8788b":"youtube.com","88acbb98":"Kaggle Notebook Runner: Mar\u00edlia Prata   @mpwolke","1a8e9274":"theenginneringmama"}}