{"cell_type":{"30a78255":"code","edb6fe81":"code","f6f0d735":"code","47adc7cb":"code","a988b30b":"code","5b861a02":"code","80f7e212":"code","c049982a":"code","6f7acea1":"code","1470883b":"code","e9a2bc73":"code","07bc08a6":"code","22995176":"code","d721f9b5":"code","e8480047":"code","5e193455":"code","d74055d2":"code","17824f49":"code","eab92e89":"code","dc522821":"code","c3c40e14":"code","4ca63ec7":"code","91a62b6a":"code","e799f97f":"code","265dbc7c":"code","800ad2a0":"code","92418189":"code","7daeda37":"code","4a6ca803":"code","e1d27814":"code","ef763613":"code","89297f75":"code","248ef28e":"code","e8938cb5":"code","5f6d8173":"markdown","93b4a2e7":"markdown","64d5c55f":"markdown","97e52547":"markdown","2b124b6c":"markdown","5ac877af":"markdown","e8c61b1b":"markdown","e96ec54e":"markdown","3b65c8f0":"markdown","11be310d":"markdown","8574f041":"markdown","eb14776d":"markdown","fd953765":"markdown","d725ce68":"markdown","6fd3e9d1":"markdown","5846b53f":"markdown","6ce580ae":"markdown","71824068":"markdown","dfa2fb98":"markdown","39d41063":"markdown","2dc387a1":"markdown","82a7c4be":"markdown"},"source":{"30a78255":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","edb6fe81":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nimport warnings\nwarnings.filterwarnings('ignore')","f6f0d735":"titanic = pd.read_csv('\/kaggle\/input\/titanic\/gender_submission.csv')","47adc7cb":"titanic.head()","a988b30b":"# data loading\ntrain = pd.read_csv('\/kaggle\/input\/titanic\/train.csv')\ntest = pd.read_csv('\/kaggle\/input\/titanic\/test.csv')\ndf = train.append(test, ignore_index=True)\npassenger_id = df[891:].PassengerId\ndf.head()","5b861a02":"def description(df):\n    print(f'Dataset Shape:{df.shape}')\n    summary = pd.DataFrame(df.dtypes, columns=['dtypes'])\n    summary = summary.reset_index()\n    summary['Name'] = summary['index']\n    summary = summary[['Name','dtypes']]\n    summary['Missing'] = df.isnull().sum().values   \n    summary['Uniques'] = df.nunique().values\n    return summary\nprint('Data Description:')\ndescription(df)","80f7e212":"sns.countplot('Survived', data=df, palette='Set2')\nplt.ylabel('Number of survivors')\nplt.title('Distribution of survivors');","c049982a":"sns.boxplot(x=\"Survived\", y=\"Age\", data=df, palette='rainbow');","6f7acea1":"sns.violinplot('Pclass','Age', hue='Survived',\n               data=df,palette=\"Set2\", split=True,scale=\"count\");","1470883b":"sns.swarmplot(x='Embarked', y='Fare', data=df);","e9a2bc73":"ax = sns.violinplot(x=\"Sex\", y=\"Age\", data=df, inner=None)\nax = sns.swarmplot(x=\"Sex\", y=\"Age\", data=df,\n                   color=\"white\", edgecolor=\"gray\")","07bc08a6":"sns.catplot(x=\"Pclass\", y=\"Fare\",\n            hue=\"Survived\", col=\"Sex\",\n            data=df, kind=\"swarm\");","22995176":"sns.jointplot(\"Age\", \"Pclass\", data=df,\n                  kind=\"kde\", space=0, color=\"g\");","d721f9b5":"plt.figure(figsize=(17,10))\nmatrix = np.triu(df.corr())\nsns.heatmap(df.corr(), annot=True, mask=matrix,cmap= 'coolwarm');","e8480047":"df = df.drop(['PassengerId','Name','Ticket','Cabin','Fare'], axis=1)\ndf.head()","5e193455":"# Remember what needs to be done\ndescription(df)","d74055d2":"from sklearn.preprocessing import LabelEncoder\nlabelEnc = LabelEncoder()\ndf.Sex=labelEnc.fit_transform(df.Sex)","17824f49":"df['Age'] = df.Age.fillna(df.Age.mean())","eab92e89":"df.Embarked.value_counts()","dc522821":"df['Embarked'] = df.Embarked.fillna('S')","c3c40e14":"Embarked = pd.get_dummies(df.Embarked , prefix='Embarked' )\nEmbarked.head()","4ca63ec7":"Pclass = pd.get_dummies(df.Pclass, prefix='Pclass')\nSibSp = pd.get_dummies(df.SibSp, prefix='SibSp')\nParch = pd.get_dummies(df.Parch, prefix='Parch')\ndf_new = pd.concat([df, Embarked, Pclass, SibSp, Parch], axis=1)","91a62b6a":"df_new = df_new.drop(['Pclass', 'SibSp','Parch', 'Embarked'], axis=1)\ndescription(df_new)","e799f97f":"df_new.shape","265dbc7c":"X_train = df_new.drop(['Survived'],axis=1)[ 0:891]\ny_train= df_new.Survived[ 0:891]\nX_test = df_new.drop(['Survived'],axis=1)[891:]\ny_test = titanic.Survived","800ad2a0":"# Preprocessing, modelling and evaluating\nfrom sklearn import preprocessing\nfrom sklearn.metrics import confusion_matrix, roc_auc_score\nfrom sklearn.model_selection import StratifiedKFold, cross_val_score, KFold\nfrom xgboost import XGBClassifier\nimport xgboost as xgb\n\n## Hyperopt modules\nfrom hyperopt import fmin, hp, tpe, Trials, space_eval, STATUS_OK, STATUS_RUNNING\nfrom functools import partial\n\nimport os\nimport gc","92418189":"from sklearn.model_selection import KFold,TimeSeriesSplit\nfrom sklearn.metrics import roc_auc_score\nfrom xgboost import plot_importance\nfrom sklearn.metrics import make_scorer\n\nimport time\ndef objective(params):\n    time1 = time.time()\n    params = {\n        'max_depth': int(params['max_depth']),\n        'gamma': \"{:.3f}\".format(params['gamma']),\n        'subsample': \"{:.2f}\".format(params['subsample']),\n        'reg_alpha': \"{:.3f}\".format(params['reg_alpha']),\n        'reg_lambda': \"{:.3f}\".format(params['reg_lambda']),\n        'learning_rate': \"{:.3f}\".format(params['learning_rate']),\n        'num_leaves': '{:.3f}'.format(params['num_leaves']),\n        'colsample_bytree': '{:.3f}'.format(params['colsample_bytree']),\n        'min_child_samples': '{:.3f}'.format(params['min_child_samples']),\n        'feature_fraction': '{:.3f}'.format(params['feature_fraction']),\n        'bagging_fraction': '{:.3f}'.format(params['bagging_fraction'])\n    }\n\n    print(\"\\n############## New Run ################\")\n    print(f\"params = {params}\")\n    FOLDS = 7\n    count=1\n    skf = StratifiedKFold(n_splits=FOLDS, shuffle=True, random_state=42)\n\n    tss = TimeSeriesSplit(n_splits=FOLDS)\n    y_preds = np.zeros(titanic.shape[0])\n    y_oof = np.zeros(X_train.shape[0])\n    score_mean = 0\n    for tr_idx, val_idx in tss.split(X_train, y_train):\n        clf = xgb.XGBClassifier(\n            n_estimators=600, random_state=4, verbose=True, \n           # tree_method='gpu_hist', \n            **params\n        )\n\n        X_tr, X_vl = X_train.iloc[tr_idx, :], X_train.iloc[val_idx, :]\n        y_tr, y_vl = y_train.iloc[tr_idx], y_train.iloc[val_idx]\n        \n        clf.fit(X_tr, y_tr)\n        #y_pred_train = clf.predict_proba(X_vl)[:,1]\n        #print(y_pred_train)\n        score = make_scorer(roc_auc_score, needs_proba=True)(clf, X_vl, y_vl)\n        # plt.show()\n        score_mean += score\n        print(f'{count} CV - score: {round(score, 4)}')\n        count += 1\n    time2 = time.time() - time1\n    print(f\"Total Time Run: {round(time2 \/ 60,2)}\")\n    gc.collect()\n    print(f'Mean ROC_AUC: {score_mean \/ FOLDS}')\n    del X_tr, X_vl, y_tr, y_vl, clf, score\n    return -(score_mean \/ FOLDS)\n\n\nspace = {\n    # The maximum depth of a tree, same as GBM.\n    # Used to control over-fitting as higher depth will allow model \n    # to learn relations very specific to a particular sample.\n    # Should be tuned using CV.\n    # Typical values: 3-10\n    'max_depth': hp.quniform('max_depth', 7, 23, 1),\n    \n    # reg_alpha: L1 regularization term. L1 regularization encourages sparsity \n    # (meaning pulling weights to 0). It can be more useful when the objective\n    # is logistic regression since you might need help with feature selection.\n    'reg_alpha':  hp.uniform('reg_alpha', 0.01, 0.4),\n    \n    # reg_lambda: L2 regularization term. L2 encourages smaller weights, this\n    # approach can be more useful in tree-models where zeroing \n    # features might not make much sense.\n    'reg_lambda': hp.uniform('reg_lambda', 0.01, .4),\n    \n    # eta: Analogous to learning rate in GBM\n    # Makes the model more robust by shrinking the weights on each step\n    # Typical final values to be used: 0.01-0.2\n    'learning_rate': hp.uniform('learning_rate', 0.01, 0.2),\n    \n    # colsample_bytree: Similar to max_features in GBM. Denotes the \n    # fraction of columns to be randomly samples for each tree.\n    # Typical values: 0.5-1\n    'colsample_bytree': hp.uniform('colsample_bytree', 0.3, .9),\n    \n    # A node is split only when the resulting split gives a positive\n    # reduction in the loss function. Gamma specifies the \n    # minimum loss reduction required to make a split.\n    # Makes the algorithm conservative. The values can vary depending on the loss function and should be tuned.\n    'gamma': hp.uniform('gamma', 0.01, .7),\n    \n    # more increases accuracy, but may lead to overfitting.\n    # num_leaves: the number of leaf nodes to use. Having a large number \n    # of leaves will improve accuracy, but will also lead to overfitting.\n    'num_leaves': hp.choice('num_leaves', list(range(20, 250, 10))),\n    \n    # specifies the minimum samples per leaf node.\n    # the minimum number of samples (data) to group into a leaf. \n    # The parameter can greatly assist with overfitting: larger sample\n    # sizes per leaf will reduce overfitting (but may lead to under-fitting).\n    'min_child_samples': hp.choice('min_child_samples', list(range(100, 250, 10))),\n    \n    # subsample: represents a fraction of the rows (observations) to be \n    # considered when building each subtree. Tianqi Chen and Carlos Guestrin\n    # in their paper A Scalable Tree Boosting System recommend \n    'subsample': hp.choice('subsample', [0.2, 0.4, 0.5, 0.6, 0.7, .8, .9]),\n    \n    # randomly select a fraction of the features.\n    # feature_fraction: controls the subsampling of features used\n    # for training (as opposed to subsampling the actual training data in \n    # the case of bagging). Smaller fractions reduce overfitting.\n    'feature_fraction': hp.uniform('feature_fraction', 0.4, .8),\n    \n    # randomly bag or subsample training data.\n    'bagging_fraction': hp.uniform('bagging_fraction', 0.4, .9)\n    \n    # bagging_fraction and bagging_freq: enables bagging (subsampling) \n    # of the training data. Both values need to be set for bagging to be used.\n    # The frequency controls how often (iteration) bagging is used. Smaller\n    # fractions and frequencies reduce overfitting.\n}","7daeda37":"# Set algoritm parameters\nbest = fmin(fn=objective,\n            space=space,\n            algo=tpe.suggest,\n            max_evals=27)\n\n# Print best parameters\nbest_params = space_eval(space, best)","4a6ca803":"print(\"BEST PARAMS: \", best_params)\n\nbest_params['max_depth'] = int(best_params['max_depth'])","e1d27814":"clf = xgb.XGBClassifier(\n    n_estimators=300,\n    **best_params\n    #tree_method='gpu_hist'\n)\n\nclf.fit(X_train, y_train)\n\npredict = clf.predict(X_test)","ef763613":"from sklearn.metrics import accuracy_score\naccuracy_score(y_test, predict)","89297f75":"feature_important = clf.get_booster().get_score(importance_type=\"weight\")\nkeys = list(feature_important.keys())\nvalues = list(feature_important.values())\n\ndata = pd.DataFrame(data=values, index=keys, columns=[\"score\"]).sort_values(by = \"score\", ascending=False)\n\n# Top 10 features\ndata.head(20)","248ef28e":"titanic['Survived'] = predict\ntitanic.to_csv('titanicpred.csv', index = False)","e8938cb5":"titanic","5f6d8173":"## Swarmplot + violin plot","93b4a2e7":"## Jointplot","64d5c55f":"# Starter libraries","97e52547":"# Data Preparation","2b124b6c":"## What we see from the data:\n1. There are unnecessary columns. We will remove them.\n2. The Sex field will be tediously converted to int format\n3. Supplement Age; empty values are present\n4. Convert Embarked field to int format\n5. Let's come back to this later.\n\n# Visualization!","5ac877af":"### Replace Sex with LabelEncoder","e8c61b1b":"### Trainning and Predicting with best Parameters","e96ec54e":"## Use Catplot to combine a swarmplot() and a FacetGrid.","3b65c8f0":"# Hello everyone!\nIn this kernel you will not see anything new, but you did it yourself.\nThank you and like it.","11be310d":"## Boxplot ","8574f041":"## Violinplot","eb14776d":"# One_hot","fd953765":"### Replace the average age","d725ce68":"# Let's go back to working with data","6fd3e9d1":"## Heatmap","5846b53f":"# Training model","6ce580ae":"## Countplot","71824068":"We remove the speakers that seem to us a little informative\n\n* PassengerID\n* Name\n* Ticket\n* Cabin\n* Fare","dfa2fb98":"## Replace the average Embarked","39d41063":"## Swarmplot","2dc387a1":"## Moving on to machine learning","82a7c4be":"### Best parameters"}}