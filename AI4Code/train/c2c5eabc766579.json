{"cell_type":{"9877f8b8":"code","cb972ea0":"code","2cf96afa":"code","2f134896":"code","1d953801":"code","7b3f3a30":"code","107d4ade":"code","361774c2":"code","ba161442":"code","927d1a16":"code","a0eb361c":"code","bc506fe1":"code","95622dfb":"code","34a700b9":"code","6b7c7302":"code","a0c30b6c":"code","92018022":"code","e1aff58d":"code","f5355716":"code","98e721ea":"code","54725817":"code","17f4a709":"code","985a992c":"code","1745d68e":"code","7f5fe13b":"markdown","907ee144":"markdown","21e699ed":"markdown","5980d2fd":"markdown","33680b77":"markdown","127a2895":"markdown","35844144":"markdown","4e663a67":"markdown","6fb88e63":"markdown","13f0c8d0":"markdown","2056ba95":"markdown","5d8c3fbf":"markdown","b29ea952":"markdown","2855920c":"markdown","dd6d6001":"markdown","7eb801e6":"markdown","5396d82f":"markdown"},"source":{"9877f8b8":"# Import libraries\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport json\nimport os\nfrom tqdm import tqdm, tqdm_notebook\nimport random\n\nimport tensorflow as tf\nfrom tensorflow.keras.models import Sequential, Model\nfrom tensorflow.keras.layers import *\nfrom tensorflow.keras.optimizers import *\nfrom tensorflow.keras.applications import *\nfrom tensorflow.keras.callbacks import *\nfrom tensorflow.keras.initializers import *\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\n\nfrom numpy.random import seed\nseed(1)\nfrom tensorflow import set_random_seed\nset_random_seed(1)","cb972ea0":"print(os.listdir(\"..\/input\"))","2cf96afa":"artists = pd.read_csv('..\/input\/artists.csv')\nartists.shape","2f134896":"# Sort artists by number of paintings\nartists = artists.sort_values(by=['paintings'], ascending=False)\n\n# Create a dataframe with artists having more than 200 paintings\nartists_top = artists[artists['paintings'] >= 200].reset_index()\nartists_top = artists_top[['name', 'paintings']]\n#artists_top['class_weight'] = max(artists_top.paintings)\/artists_top.paintings\nartists_top['class_weight'] = artists_top.paintings.sum() \/ (artists_top.shape[0] * artists_top.paintings)\nartists_top","1d953801":"# Set class weights - assign higher weights to underrepresented classes\nclass_weights = artists_top['class_weight'].to_dict()\nclass_weights","7b3f3a30":"# There is some problem recognizing 'Albrecht_D\u00fcrer' (don't know why, worth exploring)\n# So I'll update this string as directory name to df's\nupdated_name = \"Albrecht_Du\u0308rer\".replace(\"_\", \" \")\nartists_top.iloc[4, 0] = updated_name","107d4ade":"# Explore images of top artists\nimages_dir = '..\/input\/images\/images'\nartists_dirs = os.listdir(images_dir)\nartists_top_name = artists_top['name'].str.replace(' ', '_').values\n\n# See if all directories exist\nfor name in artists_top_name:\n    if os.path.exists(os.path.join(images_dir, name)):\n        print(\"Found -->\", os.path.join(images_dir, name))\n    else:\n        print(\"Did not find -->\", os.path.join(images_dir, name))","361774c2":"# Print few random paintings\nn = 5\nfig, axes = plt.subplots(1, n, figsize=(20,10))\n\nfor i in range(n):\n    random_artist = random.choice(artists_top_name)\n    random_image = random.choice(os.listdir(os.path.join(images_dir, random_artist)))\n    random_image_file = os.path.join(images_dir, random_artist, random_image)\n    image = plt.imread(random_image_file)\n    axes[i].imshow(image)\n    axes[i].set_title(\"Artist: \" + random_artist.replace('_', ' '))\n    axes[i].axis('off')\n\nplt.show()","ba161442":"# Augment data\nbatch_size = 16\ntrain_input_shape = (224, 224, 3)\nn_classes = artists_top.shape[0]\n\ntrain_datagen = ImageDataGenerator(validation_split=0.2,\n                                   rescale=1.\/255.,\n                                   #rotation_range=45,\n                                   #width_shift_range=0.5,\n                                   #height_shift_range=0.5,\n                                   shear_range=5,\n                                   #zoom_range=0.7,\n                                   horizontal_flip=True,\n                                   vertical_flip=True,\n                                  )\n\ntrain_generator = train_datagen.flow_from_directory(directory=images_dir,\n                                                    class_mode='categorical',\n                                                    target_size=train_input_shape[0:2],\n                                                    batch_size=batch_size,\n                                                    subset=\"training\",\n                                                    shuffle=True,\n                                                    classes=artists_top_name.tolist()\n                                                   )\n\nvalid_generator = train_datagen.flow_from_directory(directory=images_dir,\n                                                    class_mode='categorical',\n                                                    target_size=train_input_shape[0:2],\n                                                    batch_size=batch_size,\n                                                    subset=\"validation\",\n                                                    shuffle=True,\n                                                    classes=artists_top_name.tolist()\n                                                   )\n\nSTEP_SIZE_TRAIN = train_generator.n\/\/train_generator.batch_size\nSTEP_SIZE_VALID = valid_generator.n\/\/valid_generator.batch_size\nprint(\"Total number of batches =\", STEP_SIZE_TRAIN, \"and\", STEP_SIZE_VALID)","927d1a16":"# Print a random paintings and it's random augmented version\nfig, axes = plt.subplots(1, 2, figsize=(20,10))\n\nrandom_artist = random.choice(artists_top_name)\nrandom_image = random.choice(os.listdir(os.path.join(images_dir, random_artist)))\nrandom_image_file = os.path.join(images_dir, random_artist, random_image)\n\n# Original image\nimage = plt.imread(random_image_file)\naxes[0].imshow(image)\naxes[0].set_title(\"An original Image of \" + random_artist.replace('_', ' '))\naxes[0].axis('off')\n\n# Transformed image\naug_image = train_datagen.random_transform(image)\naxes[1].imshow(aug_image)\naxes[1].set_title(\"A transformed Image of \" + random_artist.replace('_', ' '))\naxes[1].axis('off')\n\nplt.show()","a0eb361c":"# Load pre-trained model\nbase_model = ResNet50(weights='imagenet', include_top=False, input_shape=train_input_shape)\n\nfor layer in base_model.layers:\n    layer.trainable = True","bc506fe1":"print(base_model.output)\nprint(Flatten()(base_model.output))","95622dfb":"# Add layers at the end\nX = base_model.output\nX = Flatten()(X)\n\nX = Dense(512, kernel_initializer='he_uniform')(X)\nX = Dropout(0.5)(X)\nX = BatchNormalization()(X)\nX = Activation('relu')(X)\n\nX = Dense(16, kernel_initializer='he_uniform')(X)\nX = Dropout(0.5)(X)\nX = BatchNormalization()(X)\nX = Activation('relu')(X)\n\noutput = Dense(n_classes, activation='softmax')(X)\n\nmodel = Model(inputs=base_model.input, outputs=output)","34a700b9":"optimizer = Adam(lr=0.0001)\nmodel.compile(loss='categorical_crossentropy',\n              optimizer=optimizer, \n              metrics=['accuracy'])","6b7c7302":"n_epoch = 10\n\nearly_stop = EarlyStopping(monitor='val_loss', patience=20, verbose=1, \n                           mode='auto', restore_best_weights=True)\n\nreduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=5, \n                              verbose=1, mode='auto')","a0c30b6c":"# Train the model - all layers\nhistory1 = model.fit_generator(generator=train_generator, steps_per_epoch=STEP_SIZE_TRAIN,\n                              validation_data=valid_generator, validation_steps=STEP_SIZE_VALID,\n                              epochs=n_epoch,\n                              shuffle=True,\n                              verbose=1,\n                              callbacks=[reduce_lr],\n                              use_multiprocessing=True,\n                              workers=16,\n                              class_weight=class_weights\n                             )","92018022":"# Freeze core ResNet layers and train again \nfor layer in model.layers:\n    layer.trainable = False\n\nfor layer in model.layers[:50]:\n    layer.trainable = True\n\noptimizer = Adam(lr=0.0001)\n\nmodel.compile(loss='categorical_crossentropy',\n              optimizer=optimizer, \n              metrics=['accuracy'])\n\nn_epoch = 50\nhistory2 = model.fit_generator(generator=train_generator, steps_per_epoch=STEP_SIZE_TRAIN,\n                              validation_data=valid_generator, validation_steps=STEP_SIZE_VALID,\n                              epochs=n_epoch,\n                              shuffle=True,\n                              verbose=1,\n                              callbacks=[reduce_lr, early_stop],\n                              use_multiprocessing=True,\n                              workers=16,\n                              class_weight=class_weights\n                             )","e1aff58d":"# Merge history1 and history2\nhistory = {}\nhistory['loss'] = history1.history['loss'] + history2.history['loss']\nhistory['acc'] = history1.history['acc'] + history2.history['acc']\nhistory['val_loss'] = history1.history['val_loss'] + history2.history['val_loss']\nhistory['val_acc'] = history1.history['val_acc'] + history2.history['val_acc']\nhistory['lr'] = history1.history['lr'] + history2.history['lr']","f5355716":"# Plot the training graph\ndef plot_training(history):\n    acc = history['acc']\n    val_acc = history['val_acc']\n    loss = history['loss']\n    val_loss = history['val_loss']\n    epochs = range(len(acc))\n\n    fig, axes = plt.subplots(1, 2, figsize=(15,5))\n    \n    axes[0].plot(epochs, acc, 'r-', label='Training Accuracy')\n    axes[0].plot(epochs, val_acc, 'b--', label='Validation Accuracy')\n    axes[0].set_title('Training and Validation Accuracy')\n    axes[0].legend(loc='best')\n\n    axes[1].plot(epochs, loss, 'r-', label='Training Loss')\n    axes[1].plot(epochs, val_loss, 'b--', label='Validation Loss')\n    axes[1].set_title('Training and Validation Loss')\n    axes[1].legend(loc='best')\n    \n    plt.show()\n    \nplot_training(history)","98e721ea":"# Prediction accuracy on train data\nscore = model.evaluate_generator(train_generator, verbose=1)\nprint(\"Prediction accuracy on train data =\", score[1])","54725817":"# Prediction accuracy on CV data\nscore = model.evaluate_generator(valid_generator, verbose=1)\nprint(\"Prediction accuracy on CV data =\", score[1])","17f4a709":"# Classification report and confusion matrix\nfrom sklearn.metrics import *\nimport seaborn as sns\n\ntick_labels = artists_top_name.tolist()\n\ndef showClassficationReport_Generator(model, valid_generator, STEP_SIZE_VALID):\n    # Loop on each generator batch and predict\n    y_pred, y_true = [], []\n    for i in range(STEP_SIZE_VALID):\n        (X,y) = next(valid_generator)\n        y_pred.append(model.predict(X))\n        y_true.append(y)\n    \n    # Create a flat list for y_true and y_pred\n    y_pred = [subresult for result in y_pred for subresult in result]\n    y_true = [subresult for result in y_true for subresult in result]\n    \n    # Update Truth vector based on argmax\n    y_true = np.argmax(y_true, axis=1)\n    y_true = np.asarray(y_true).ravel()\n    \n    # Update Prediction vector based on argmax\n    y_pred = np.argmax(y_pred, axis=1)\n    y_pred = np.asarray(y_pred).ravel()\n    \n    # Confusion Matrix\n    fig, ax = plt.subplots(figsize=(10,10))\n    conf_matrix = confusion_matrix(y_true, y_pred, labels=np.arange(n_classes))\n    conf_matrix = conf_matrix\/np.sum(conf_matrix, axis=1)\n    sns.heatmap(conf_matrix, annot=True, fmt=\".2f\", square=True, cbar=False, \n                cmap=plt.cm.jet, xticklabels=tick_labels, yticklabels=tick_labels,\n                ax=ax)\n    ax.set_ylabel('Actual')\n    ax.set_xlabel('Predicted')\n    ax.set_title('Confusion Matrix')\n    plt.show()\n    \n    print('Classification Report:')\n    print(classification_report(y_true, y_pred, labels=np.arange(n_classes), target_names=artists_top_name.tolist()))\n\nshowClassficationReport_Generator(model, valid_generator, STEP_SIZE_VALID)","985a992c":"# Prediction\nfrom keras.preprocessing import *\n\nn = 5\nfig, axes = plt.subplots(1, n, figsize=(25,10))\n\nfor i in range(n):\n    random_artist = random.choice(artists_top_name)\n    random_image = random.choice(os.listdir(os.path.join(images_dir, random_artist)))\n    random_image_file = os.path.join(images_dir, random_artist, random_image)\n\n    # Original image\n\n    test_image = image.load_img(random_image_file, target_size=(train_input_shape[0:2]))\n\n    # Predict artist\n    test_image = image.img_to_array(test_image)\n    test_image \/= 255.\n    test_image = np.expand_dims(test_image, axis=0)\n\n    prediction = model.predict(test_image)\n    prediction_probability = np.amax(prediction)\n    prediction_idx = np.argmax(prediction)\n\n    labels = train_generator.class_indices\n    labels = dict((v,k) for k,v in labels.items())\n\n    #print(\"Actual artist =\", random_artist.replace('_', ' '))\n    #print(\"Predicted artist =\", labels[prediction_idx].replace('_', ' '))\n    #print(\"Prediction probability =\", prediction_probability*100, \"%\")\n\n    title = \"Actual artist = {}\\nPredicted artist = {}\\nPrediction probability = {:.2f} %\" \\\n                .format(random_artist.replace('_', ' '), labels[prediction_idx].replace('_', ' '),\n                        prediction_probability*100)\n\n    # Print image\n    axes[i].imshow(plt.imread(random_image_file))\n    axes[i].set_title(title)\n    axes[i].axis('off')\n\nplt.show()","1745d68e":"# Predict from web - this is an image of Titian.\n# Replace 'url' with any image of one of the 11 artists above and run this cell.\nurl = 'https:\/\/www.gpsmycity.com\/img\/gd\/2081.jpg'\n\nimport imageio\nimport cv2\n\nweb_image = imageio.imread(url)\nweb_image = cv2.resize(web_image, dsize=train_input_shape[0:2], )\nweb_image = image.img_to_array(web_image)\nweb_image \/= 255.\nweb_image = np.expand_dims(web_image, axis=0)\n\n\nprediction = model.predict(web_image)\nprediction_probability = np.amax(prediction)\nprediction_idx = np.argmax(prediction)\n\nprint(\"Predicted artist =\", labels[prediction_idx].replace('_', ' '))\nprint(\"Prediction probability =\", prediction_probability*100, \"%\")\n\nplt.imshow(imageio.imread(url))\nplt.axis('off')\nplt.show()","7f5fe13b":"# Evaluate performance by predicting on random images from dataset","907ee144":"# This portion is just for fun :) Replace the variable `url` with an image of one of the 11 artists above and run this cell.","21e699ed":"# Introduction\nCreation of art is among the highest form of expression of human mind and imagination. The ability of communicating imagination sets us apart from all other beings. Painting, being an expression of visual language, have attracted and connected the brilliant human minds since the dawn of civilization - from early drawings on walls of caves to paper or glass paintings of modern times, from charcoals in prehistoric times to water, oil, or pastel colors of today. We have travelled a long way, and have finally reached a stage where not only humans but computers, another brilliant creation of human minds, is creating paintings.\n\nFor an enthusiast of arts, identifying the paintings of her favorite artists is not that difficult, given years of careful practice and research. Given a painting, she can easily identify if it was painted by a painter she is passionate about. But can a computer do the same? Can a machine without emotions identify who the genius is behind a mindblowing painting?\n\nIn this kernel, let us try to explore that direction, using techniques of deep learning.\n\n#### Special thanks to [Icaro](https:\/\/www.kaggle.com\/ikarus777) for sharing this wonderful dataset with us!","5980d2fd":"## Confusion Matrix. Look at the style of the artists which the model thinks are almost similar. ","33680b77":"### Print a random paintings and it's random augmented version","127a2895":"### Thank you for reading the  notebook! Please share your thoughts and feedback in comments section below.\n### Please \"upvote\" the kernel if you like it :)\n### Please take a look at http:\/\/supratimh.github.io for other projects I am working on. I will eagerly look forward to your feedback and suggestions.","35844144":"## Data Augmentation","4e663a67":"## Data Processing","6fb88e63":"# Objective: \nDevelop an algorithm which will identify the artist when provided with a painting, with state of the art precision.","13f0c8d0":"## Evaluate performance","2056ba95":"## Training graph","5d8c3fbf":"## Build Model","b29ea952":"# Let's implement \"DeepArtist\" :)","2855920c":"# \"I dream of painting and then I paint my dream.\" - Vincent Van Gogh\n<img src=\"https:\/\/www.vincentvangogh.org\/images\/paintings\/self-portrait-with-straw-hat.jpg\" width=\"400px\" height=\"200px\"\/>","dd6d6001":"# My high-level approach to solution:\n## Data processing:\n* There are paintings of 50 artists in the dataset. However only 11 artists have more than 200 paintings available here.\n* To reduce computation and better training, I decided to use the paintings of these 11 artists only.\n* Since this is an imbalanced datset (Van Gogh has 877 paintings whereas Marc Chagall has only 239), `class_weight` is important. Infact, it improved model performance substantially.\n* I used Keras `ImageDataGenerator` for data augmentation. This is not a traditional object detection problem, hence the augmentation approch should be used very carefully.\n* I couldn't experiment in detail, but so far `zoom_range` worked well. \n\n## Modelling and Training:\n* Use convolutional neural network based approach, with a pre-defined architecture as baseline.\n* I tried multiple architectures, however `ResNet50` worked well so far.\n* Pretrained weights on `imagenet` helped the model train better.\n* The objective is to identify artist and not objects in the images. So the model should understand the style of the image better rather than the final output. Hence, training of shallow layers is more important than the deeper layers.\n* The above statement is based on my understanding of the problem and experiments and observations.\n* Training the model for more iterations might improve the performance, at the cost of computation resource.\n\n## Predictions:\n* <b>The final model could identify the artists with an approximate accuracy of 99% on training set and 85% on cross-validation set.<\/b> ","7eb801e6":"## Read data","5396d82f":"### Print few random paintings"}}