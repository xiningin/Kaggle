{"cell_type":{"a33e5083":"code","89355a41":"code","c05d6648":"code","5667ff77":"code","f9126f7b":"code","9670ce63":"code","03ba7f52":"code","a84ed219":"code","1c2b4836":"code","0860fa20":"code","a291f33d":"code","20722298":"code","c588af30":"code","cbae5a93":"code","f579015d":"code","8ee572fd":"code","a2fd62b9":"code","430e1806":"code","41d4601d":"code","dd542810":"code","95cb3173":"code","764ad5a8":"code","3de7f485":"code","6885e334":"code","891d7c1c":"code","9f5cca17":"code","d3215c9e":"code","f5153b9f":"markdown","cf43606e":"markdown","d2f7fa0a":"markdown","6d26c37e":"markdown","4bde71e4":"markdown"},"source":{"a33e5083":"import os\nimport gc\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pylab as plt\nfrom tqdm.notebook import tqdm\n\nplt.style.use('ggplot')\nplt.rcParams['figure.figsize'] = [9, 9]\npd.set_option('display.max_columns', None)\npd.set_option('display.max_rows', 350)","89355a41":"PATH = '..\/input\/ubiquant-market-prediction-half-precision-pickle'","c05d6648":"os.listdir(PATH)","5667ff77":"%%time\n\ntrain = pd.read_pickle(f'{PATH}\/train.pkl')\n\nfor col in ['time_id', 'investment_id']:\n    train[col] = train[col].astype(int)","f9126f7b":"gc.collect()","9670ce63":"train.info()","03ba7f52":"N_ROWS = len(train)\nfeatures = [f'f_{i}' for i in range(300)]","a84ed219":"train","1c2b4836":"# no NAs at all\ntrain.isnull().sum().sum()","0860fa20":"train['investment_id'].value_counts()","a291f33d":"unique_investment_ids = train['investment_id'].unique()\nprint(unique_investment_ids)\nprint(unique_investment_ids.shape)","20722298":"# for x in unique_investment_ids[:20]:\n#     train[train['investment_id'] == x].plot('time_id', 'target', title=f'investment_id = {x}', figsize=(6, 6));","c588af30":"train['target'].plot.hist();","cbae5a93":"train[['target']].describe()","f579015d":"train.sample(int(N_ROWS * 0.02), random_state=1)[features].describe().T['mean'].plot.hist(title='0 mean');","8ee572fd":"train.sample(int(N_ROWS * 0.02), random_state=1)[features].describe().T['std'].plot.hist('1 std');","a2fd62b9":"# %%time\n\n# many high correlated features, so some type of models can struggle with this\n# train.sample(int(N_ROWS * 0.001), random_state=1)[['target'] + features].corr().style.background_gradient(axis=None)  ","430e1806":"train.sample(int(N_ROWS * 0.02), random_state=1)[['f_109', 'target']].plot.scatter('f_109', 'target');","41d4601d":"train.sample(int(N_ROWS * 0.02), random_state=1)[['f_108', 'target']].plot.scatter('f_108', 'target');","dd542810":"# take some investment ids \nN = int(len(unique_investment_ids) * 0.8)\ninvestments_to_use = train['investment_id'].value_counts()[:N]\nprint('Selecting:', N)\nprint('Before:', train.shape)\ntrain = train[train['investment_id'].isin(investments_to_use)].reset_index(drop=True)\nprint('After:', train.shape)\n\ntrain","95cb3173":"from sklearn import linear_model\nfrom sklearn import metrics\nfrom scipy.stats import pearsonr\nfrom sklearn.preprocessing import StandardScaler\nimport lightgbm as lgbm\nfrom sklearn import svm","764ad5a8":"from typing import Tuple\n\n\nclass GroupTimeSeriesSplit:\n    \"\"\"\n    From: https:\/\/www.kaggle.com\/c\/ubiquant-market-prediction\/discussion\/304036\n    Custom class to create a Group Time Series Split. We ensure\n    that the time id values that are in the testing data are not a part\n    of the training data & the splits are temporal\n    \"\"\"\n    def __init__(self, n_folds: int, holdout_size: int, groups: str) -> None:\n        self.n_folds = n_folds\n        self.holdout_size = holdout_size\n        self.groups = groups\n\n    def split(self, X) -> Tuple[np.array, np.array]:\n        # Take the group column and get the unique values\n        unique_time_ids = np.unique(self.groups.values)\n\n        # Split the time ids into the length of the holdout size\n        # and reverse so we work backwards in time. Also, makes\n        # it easier to get the correct time_id values per\n        # split\n        array_split_time_ids = np.array_split(\n            unique_time_ids, len(unique_time_ids) \/\/ self.holdout_size\n        )[::-1]\n\n        # Get the first n_folds values\n        array_split_time_ids = array_split_time_ids[:self.n_folds]\n\n        for time_ids in array_split_time_ids:\n            # Get test index - time id values that are in the time_ids\n            test_condition = X['time_id'].isin(time_ids)\n            test_index = X.loc[test_condition].index\n\n            # Get train index - The train index will be the time\n            # id values right up until the minimum value in the test\n            # data - we can also add a gap to this step by\n            # time id < (min - gap)\n            train_condition = X['time_id'] < (np.min(time_ids))\n            train_index = X.loc[train_condition].index\n\n            yield train_index, test_index","3de7f485":"gc.collect()","6885e334":"%%time\n\nFEATS = features + ['investment_id', 'time_id']\n\npearsons = []\nmodels = []\nscalers = []\n\nFOLDS = 5\ngtss = GroupTimeSeriesSplit(n_folds=FOLDS, holdout_size=20, groups=train['time_id'])\nfor fold, (tr, val) in enumerate(gtss.split(train)):\n    print('FOLD:', fold)\n    \n    # use a fraction to training\n    X_train = train.loc[tr, FEATS]\n    y_train = train.loc[tr, 'target']\n    del tr\n    gc.collect()\n    \n    X_val = train.loc[val, FEATS]\n    y_val = train.loc[val, 'target']\n    del val\n    gc.collect()\n    \n    print('Train time_id range:', X_train['time_id'].min(), '->', X_train['time_id'].max())\n    print('Val time_id range:', X_val['time_id'].min(), '->', X_val['time_id'].max())\n    \n    # store time_id to calculate Pearson correlation\n    time_ids_val = X_val['time_id'].values\n    \n    # standardize\n#     scaler = StandardScaler()\n#     X_train = pd.DataFrame(scaler.fit_transform(X_train), columns=X_train.columns)\n#     X_val = pd.DataFrame(scaler.transform(X_val), columns=X_val.columns)\n#     scalers.append(scaler)\n    \n#     model = lgbm.LGBMRegressor(\n#         random_state=1,\n#         max_depth=3\n#     )\n#     model = svm.LinearSVR(\n#         random_state=1,\n#         loss='squared_epsilon_insensitive',\n#         dual=False # when n_samples > n_features\n#     )\n\n    # fit\n    model = linear_model.LinearRegression(\n        n_jobs=-1\n    )\n    model.fit(X_train.drop(['investment_id', 'time_id'], axis=1), y_train)\n    models.append(model)\n    \n    del X_train, y_train\n    gc.collect()\n    \n    # metrics\n    # submissions are evaluated on the mean of the Pearson correlation coefficient for each time ID\n    X_val['y_pred'] = model.predict(X_val.drop(['investment_id', 'time_id'], axis=1))\n    X_val['y_true'] = y_val.values\n    X_val['time_id'] = time_ids_val\n    \n    del y_val, time_ids_val\n    gc.collect()\n    \n    pearson = X_val[['time_id', 'y_true', 'y_pred']].groupby('time_id').apply(lambda x: pearsonr(x['y_true'], x['y_pred'])[0]).mean()\n    print('Pearson:', pearson)\n    print()\n    pearsons.append(pearson)\n    \n    del X_val\n    gc.collect()\n    \nprint('-' * 30)\nprint('Mean:', np.mean(pearsons))\nprint('Std:', np.std(pearsons))","891d7c1c":"BEST_FOLD = np.argmax(pearsons)\nprint(BEST_FOLD)","9f5cca17":"# Linear Regression (GroupKFold) -> LB: 0.108\n# Mean: 0.12937332275514163\n# Std: 0.006988315782118249\n\n# Linear Regression (GroupTimeSeriesSplit) -> LB: 0.102\n# Mean: 0.11933183135552154\n# Std: 0.0416659840190710","d3215c9e":"import ubiquant\n\nenv = ubiquant.make_env()\niter_test = env.iter_test()\n\nfor (test_df, sample_prediction_df) in iter_test:\n    \n    # time_id is not present in test set \n    test_df['time_id'] = test_df['row_id'].apply(lambda x: int(x.split('_')[0]))\n        \n    # predict using each model\n    final_pred = models[BEST_FOLD].predict(test_df[features])\n    \n    # average\n    sample_prediction_df['target'] = final_pred\n    \n    env.predict(sample_prediction_df)\n    display(sample_prediction_df)","f5153b9f":"## Update\n\n- Changed the Pearson correlation calculation, from [here](https:\/\/www.kaggle.com\/c\/ubiquant-market-prediction\/discussion\/303627)\n- Changed GroupKFold to GroupTimeSeriesSplit from [here](https:\/\/www.kaggle.com\/c\/ubiquant-market-prediction\/discussion\/304036)\n- Using only 80% of most frequent investment ids due memory error  \n\n---\n\n- Using GroupKFold: \n    - CV: 0.12937332275514163\n    - LB: 0.108\n- Using GroupTimeSeriesSplit:\n    - CV: 0.11928179218531812\n    - LB: ?","cf43606e":"## EDA","d2f7fa0a":"## Submission","6d26c37e":"### On the target","4bde71e4":"## Modelling"}}