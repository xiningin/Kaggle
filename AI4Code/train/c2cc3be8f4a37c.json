{"cell_type":{"a606ba73":"code","eb3e5525":"code","eb2af62f":"code","aa5cff72":"code","e3d26e1f":"code","009bf225":"code","0121675a":"code","cc3e0f70":"code","9e716faa":"code","16c020d0":"code","c8718401":"code","aa2b4fb5":"code","ad1fba87":"code","7c42d5b0":"code","ce179c1a":"code","07a8f951":"code","355e9b5e":"code","7a872407":"code","364d65ea":"code","3b9299f3":"code","6367169a":"code","fa097513":"code","f3d89915":"markdown","d99fa688":"markdown","2bb43889":"markdown","2965eac8":"markdown"},"source":{"a606ba73":"import numpy as np \nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n# import shape\n\nfrom gc import collect\nfrom pdpbox import pdp\nfrom xgboost import XGBClassifier\nfrom lightgbm import LGBMClassifier\nfrom sklearn.model_selection import train_test_split, StratifiedKFold\nfrom sklearn.metrics import classification_report, log_loss, roc_curve, auc\nfrom sklearn.feature_selection import RFE\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.feature_selection import mutual_info_classif\nfrom statsmodels.stats.outliers_influence import variance_inflation_factor\n%matplotlib inline\nfrom warnings import filterwarnings\nfilterwarnings('ignore')\npd.set_option('max.columns', 200)","eb3e5525":"train = pd.read_csv('\/kaggle\/input\/tabular-playground-series-sep-2021\/train.csv')\ntest = pd.read_csv('\/kaggle\/input\/tabular-playground-series-sep-2021\/test.csv')","eb2af62f":"train.shape, test.shape","aa5cff72":"target = train.claim\ntrain = train.drop(['claim'], axis=1)","e3d26e1f":"mask = np.triu(np.ones_like(corr, dtype = bool))\n\nplt.figure(figsize = (15, 15))\nplt.title('Corelation matrix')\nsns.heatmap(corr, mask = mask, cmap = 'Greens', linewidths = .5)\nplt.show()","009bf225":"train_na = train.dropna()","0121675a":"vif_data = pd.DataFrame()\nvif_data[\"feature\"] = train_na.columns\n  \n# calculating VIF for each feature\nvif_data[\"VIF\"] = [variance_inflation_factor(train_na.values, i)\n                          for i in range(len(train_na.columns))]","cc3e0f70":"vif_data.query(\"VIF>1\")","9e716faa":"# filling NaN values\nimputer = SimpleImputer(missing_values=np.nan, strategy='mean')\ntrain_imp = imputer.fit_transform(train)\ntest_imp = imputer.transform(test)","16c020d0":"# select top 20 features\n# threshold = 20\n# high_score_features = []\n# feature_scores = mutual_info_classif(train_imp, target, random_state=0)\n# for score, f_name in sorted(zip(feature_scores, train_imp.columns), reverse=True)[:threshold]:\n#         print(f_name, score)\n#         high_score_features.append(f_name)\n# train_imp_mi = train_imp[high_score_features]\n# print(train_imp_mi.columns)","c8718401":"X_train, X_test, y_train, y_test = train_test_split(train_imp, target, test_size=0.2)","aa2b4fb5":"clf = LGBMClassifier(objective='binary', \n                     n_estimators=10000,\n                    learning_rate=0.16622575854184896,\n                    num_leaves=720,\n                    max_depth=9,\n                    min_data_in_leaf=3900,\n                    lambda_l1=40,\n                    lambda_l2=100,\n                    min_gain_to_split=3.011673053510428,\n                    bagging_fraction=0.5,\n                    bagging_freq=1,\n                     feature_fraction=0.30000000000000004\n                    )","ad1fba87":"# eval_set = [(X_test, y_test)]\n\nmodel_lgbm = clf.fit(train_imp, target)","7c42d5b0":"y_pred = model_lgbm.predict_proba(X_test)\n\nfpr, tpr, thresholds = roc_curve(y_test, y_pred[:, 1])\nprint('Valid AUC: ', auc(fpr, tpr))","ce179c1a":"# print(classification_report(y_test, y_pred))","07a8f951":"xgb_parama = {'n_estimators': 10000,\n              'max_depth': 5,\n              'colsample_bytree': 0.30,\n              'learning_rate': 0.01,\n              'reg_lambda': 18,\n              'reg_alpha': 18,\n              'random_state': 0,\n              'predictor': 'gpu_predictor',\n              'tree_method': 'gpu_hist',\n              'eval_metric': 'auc'}\n\nmodel_xgb = XGBClassifier(**xgb_parama)\n\nmodel_xgb.fit(X_train, y_train,\n          eval_set = eval_set,\n          early_stopping_rounds = 300,\n          verbose = 1000)","355e9b5e":"y_pred = model_lgbm.predict_proba(X_test)\n\nfpr, tpr, thresholds = roc_curve(y_test, y_pred[:, 1])\nprint('Valid AUC: ', auc(fpr, tpr))","7a872407":"test_ids = test['id'].values.flatten()\n\npredictions = model_lgbm.predict_proba(test)\n\nsubmission = pd.DataFrame({'id': test_ids, 'claim': predictions[:, 1]})","364d65ea":"submission.to_csv('lgbm_submission.csv', index=False)","3b9299f3":"import optuna\n\nfrom optuna.integration import LightGBMPruningCallback\n\n\ndef objective(trial, X, y):\n    param_grid = {\n#         \"device_type\": trial.suggest_categorical(\"device_type\", ['gpu']),\n        \"n_estimators\": trial.suggest_categorical(\"n_estimators\", [10000]),\n        \"learning_rate\": trial.suggest_float(\"learning_rate\", 0.01, 0.3),\n        \"num_leaves\": trial.suggest_int(\"num_leaves\", 20, 3000, step=20),\n        \"max_depth\": trial.suggest_int(\"max_depth\", 3, 12),\n        \"min_data_in_leaf\": trial.suggest_int(\"min_data_in_leaf\", 200, 10000, step=100),\n        \"lambda_l1\": trial.suggest_int(\"lambda_l1\", 0, 100, step=5),\n        \"lambda_l2\": trial.suggest_int(\"lambda_l2\", 0, 100, step=5),\n        \"min_gain_to_split\": trial.suggest_float(\"min_gain_to_split\", 0, 15),\n        \"bagging_fraction\": trial.suggest_float(\n            \"bagging_fraction\", 0.2, 0.95, step=0.1\n        ),\n        \"bagging_freq\": trial.suggest_categorical(\"bagging_freq\", [1]),\n        \"feature_fraction\": trial.suggest_float(\n            \"feature_fraction\", 0.2, 0.95, step=0.1\n        ),\n    }\n\n    cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=1121218)\n\n    cv_scores = np.empty(5)\n    for idx, (train_idx, test_idx) in enumerate(cv.split(X, y)):\n        X_train, X_test = X.iloc[train_idx], X.iloc[test_idx]\n        y_train, y_test = y[train_idx], y[test_idx]\n\n        model = LGBMClassifier(objective=\"binary\", **param_grid)\n        model.fit(\n            X_train,\n            y_train,\n            eval_set=[(X_test, y_test)],\n            eval_metric=\"binary_logloss\",\n            early_stopping_rounds=100,\n            callbacks=[\n                LightGBMPruningCallback(trial, \"binary_logloss\")\n            ],  # Add a pruning callback\n        )\n        preds = model.predict_proba(X_test)\n        cv_scores[idx] = log_loss(y_test, preds)\n\n    return np.mean(cv_scores)","6367169a":"study = optuna.create_study(direction=\"minimize\", study_name=\"LGBM Classifier\")\nfunc = lambda trial: objective(trial, X, y)\nstudy.optimize(func, n_trials=20)","fa097513":"print(f\"\\tBest value (rmse): {study.best_value:.5f}\")\nprint(f\"\\tBest params:\")\n\nfor key, value in study.best_params.items():\n    print(f\"\\t\\t{key}: {value}\")","f3d89915":"### **Using XGBoost**","d99fa688":"#### **Model tuning part**","2bb43889":"### **EDA**","2965eac8":"### **Model using LGBM**"}}