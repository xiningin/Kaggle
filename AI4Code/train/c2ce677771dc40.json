{"cell_type":{"7d4978e5":"code","dd08289d":"code","df436f13":"code","b6fa3d4e":"code","701d5e77":"code","5850bf01":"code","434d0c4f":"code","ba0649df":"code","d6aff5ce":"code","11c651cd":"code","f116388b":"code","f8ca6959":"code","f47a4017":"code","f8024cfc":"code","bf7de050":"code","315185b5":"code","7878b639":"code","3fa29dea":"code","68cf8c76":"code","e6b4d99a":"code","ba469d24":"code","3f0adb5e":"code","18850d7f":"code","7067f38a":"code","16d190ed":"code","1d4437d8":"code","19279beb":"code","4de47061":"code","eed95108":"code","78b5d061":"code","e1e9f3d5":"code","535e7387":"code","74d2e93a":"code","2b3499e9":"code","7a7772c0":"code","3f5f30c8":"code","9a51f01e":"code","7f382d3f":"code","01bde5a6":"code","e4ba461a":"code","a251da9e":"code","68a73eb4":"code","47a4a486":"code","85a73b94":"code","457ad1bd":"code","2b5488e1":"code","42098313":"code","47167366":"code","b1fb3e2d":"code","3e0b9390":"code","8fdea641":"code","f676539f":"code","7e6d09c6":"code","b0fd01ca":"code","3a619ef8":"code","9d0ba4c2":"code","ddfc0714":"code","b3ed15c5":"code","4e8938de":"code","a47e0057":"code","f199aff7":"code","610e1c59":"code","88063d00":"code","74cc2cde":"code","ec9be0ad":"code","39f957cb":"code","560e3854":"code","a0a429f8":"code","f201ef92":"code","4a9e1bd5":"code","12e2f17b":"code","d63c7051":"code","7025c4d5":"code","e6bf786f":"code","25cbb259":"code","a45017be":"code","c3486281":"code","f5388c8e":"code","725793b5":"code","e57aa18e":"code","ec31d3dc":"code","1b923758":"code","f3c7d811":"code","a2187624":"code","8976eda4":"code","2cb11da6":"code","acfff9c5":"code","d0828c2c":"code","01910274":"code","58c9db7e":"code","be0e10a8":"code","fd4425cb":"code","6ba1c566":"code","8be13dad":"code","aa7619a1":"code","3dff38d5":"code","393bcf08":"code","7cd3a0db":"code","f6734ef9":"code","d0749a05":"code","f4effc19":"code","a3dda480":"code","bcbadd4e":"code","d71e29c6":"code","9c6246c5":"code","6126cfe1":"code","9f9a0cf9":"code","300cbfc8":"code","69b385d2":"code","8861d247":"code","cca8b5d2":"code","42db7a51":"code","3acebd33":"code","0add79fa":"code","25b9c108":"code","a4b14f57":"code","6416ea46":"code","16939e24":"code","f6bfb34a":"code","1a31071b":"code","ccc54761":"code","9b916eb9":"code","dfef9731":"code","147309f8":"code","6b76edec":"code","9691a268":"code","e4eb935e":"code","8fafb8ec":"code","559c3a4b":"code","9b266cfc":"code","32b8458c":"code","e948372b":"code","d4908682":"code","66e639cb":"code","06589093":"code","ed952a0d":"code","9830d971":"markdown","591ed967":"markdown","0d53bd08":"markdown","cb9987fc":"markdown","010a09dc":"markdown","206b9074":"markdown","0b01a8df":"markdown","c49bb457":"markdown","c68b228a":"markdown","89235d53":"markdown","20219eb5":"markdown","5cece33e":"markdown","26b53cfb":"markdown","33865fcb":"markdown","6a2cd867":"markdown","b3972bb8":"markdown","30c72fb7":"markdown","2c77700d":"markdown","6bb6d881":"markdown","609a0e97":"markdown","e8af6a84":"markdown","b6740833":"markdown","68513c83":"markdown","27946efb":"markdown","9bc261de":"markdown","a0f7e292":"markdown","46b71bfc":"markdown","8d0cfc61":"markdown","0c86ccd7":"markdown","f96a5bba":"markdown","594e0dbd":"markdown","6de01f15":"markdown","0de1168c":"markdown","8fccd6d0":"markdown","5693dbbf":"markdown","cfdce137":"markdown","4edf03ea":"markdown","fd8f1f87":"markdown","dd52a577":"markdown","c02bca82":"markdown","40f34a38":"markdown","a7aa7dcd":"markdown","7d89b1b6":"markdown","10c7c8cf":"markdown","544135fc":"markdown","e582e184":"markdown","ea7a8e3a":"markdown","20026998":"markdown","8cddd0dc":"markdown","6a35334c":"markdown","15d669ff":"markdown","22ce9dc3":"markdown","fa43efd7":"markdown","359d08d4":"markdown","4b564447":"markdown","abe0dfc6":"markdown","e092b02c":"markdown","1e4b0b62":"markdown","e001b419":"markdown","73f83344":"markdown","efcde887":"markdown","f61f4816":"markdown","bf748294":"markdown","eb33b80d":"markdown","40e75020":"markdown","d623bf69":"markdown","b220cc1b":"markdown","1dc97b60":"markdown","1b2f3250":"markdown","3981c52e":"markdown","892b4553":"markdown","77490b1b":"markdown","83b2bf0a":"markdown","e16c56e0":"markdown","382722ab":"markdown","03abd54b":"markdown","cf6db135":"markdown","2fccd26f":"markdown","5c981a37":"markdown","efdb752a":"markdown","9abe23ca":"markdown","67e786af":"markdown","ffc72d50":"markdown","6ff1ab81":"markdown","c0a67d02":"markdown","0bc58775":"markdown","9f97b21b":"markdown","95a45bc4":"markdown","f3683afa":"markdown","bca372c4":"markdown","6fd61fa1":"markdown","3c22efda":"markdown","aa644e0f":"markdown","a44db0b4":"markdown","655fd387":"markdown","67d09a61":"markdown","9a613b95":"markdown","3e3d97fd":"markdown","32ac458d":"markdown","e28a2e1d":"markdown","017ee632":"markdown","2f507322":"markdown","a8bcb3b3":"markdown","71bbc021":"markdown","b9b7bef5":"markdown","d663b5d4":"markdown","184424eb":"markdown","a4aefbd0":"markdown","6629aa68":"markdown","ead23b83":"markdown","57ec1075":"markdown","d34b81b5":"markdown","bd9de7a7":"markdown","fddd6dbc":"markdown","03ed5f07":"markdown","7c4231d6":"markdown","8a5ff6a0":"markdown","66a7eee7":"markdown","d0ce093d":"markdown","fa829978":"markdown","5865ba6a":"markdown","bfc7c16a":"markdown","1ec35df6":"markdown","ed6013f3":"markdown","fef457f7":"markdown","ce5c9c4a":"markdown","bd521ad8":"markdown","6196fb89":"markdown","db75d1e7":"markdown","88c4c704":"markdown","6fde754b":"markdown","5edd719b":"markdown","6457ceb7":"markdown","f3322611":"markdown","7ea52198":"markdown","bf6ea8c2":"markdown","fb86839c":"markdown","adc15909":"markdown","894e3767":"markdown","b82c1aad":"markdown","045d56a6":"markdown","dcb6ffa8":"markdown","7d72ac18":"markdown","510e0ff4":"markdown","ebeaac19":"markdown","ac13fde0":"markdown","94fc18d6":"markdown","5c5eda5e":"markdown","6b18ab59":"markdown","7ed4b6db":"markdown","1158eeeb":"markdown","653b3fbc":"markdown","1a58c9e3":"markdown","97e9ae6a":"markdown"},"source":{"7d4978e5":"import pandas as pd\nimport numpy as np  \nimport seaborn as sns \nimport matplotlib.pyplot as plt\nfrom sklearn import preprocessing\nfrom sklearn.metrics import f1_score\nfrom sklearn.metrics import recall_score\nfrom sklearn.metrics import precision_score\nfrom sklearn.model_selection import train_test_split\nfrom sklearn import preprocessing\nfrom imblearn.over_sampling import SMOTE\nfrom prettytable import PrettyTable\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.ensemble import BaggingClassifier\nfrom xgboost import XGBClassifier\nfrom sklearn.ensemble import AdaBoostClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.model_selection import RandomizedSearchCV\nimport time\n%matplotlib inline\nimport warnings\nwarnings.filterwarnings(\"ignore\")","dd08289d":"data = pd.read_csv(\"\/kaggle\/input\/employees-evaluation-for-promotion\/employee_promotion.csv\", encoding = \"ISO-8859-1\", engine=\"python\")","df436f13":"data.head()","b6fa3d4e":"data.tail()","701d5e77":"data.columns","5850bf01":"print('lenght of data is', len(data))","434d0c4f":"data.shape","ba0649df":"data.info()","d6aff5ce":"data.dtypes","11c651cd":"np.sum(data.isnull().any(axis=1))","f116388b":"data.isnull().sum()","f8ca6959":"print('Count of columns in the data is:  ', len(data.columns))","f47a4017":"print('Count of rows in the data is:  ', len(data))","f8024cfc":"data.describe()","bf7de050":"data.hist(figsize=(20,20),bins = 20, color=\"#107009AA\")\nplt.title(\"Numeric Features Distribution\")\nplt.show()","315185b5":"sns.countplot(data= data, x = \"department\")\nplt.show()","7878b639":"data[\"department\"].value_counts().head(10).plot(kind = 'pie', autopct='%1.1f%%', figsize=(10, 10)).legend()","3fa29dea":"plt.figure(figsize=(12,10))\nsns.countplot(data.region)\nplt.xticks(rotation=90)","68cf8c76":"data[\"region\"].value_counts().head(10).plot(kind = 'pie', autopct='%1.1f%%', figsize=(10, 10), startangle=0).legend()","e6b4d99a":"sns.countplot(data= data, x = \"education\")\nplt.show()","ba469d24":"data[\"education\"].value_counts().head(7).plot(kind = 'pie', autopct='%1.1f%%', figsize=(10, 10), startangle=0).legend()","3f0adb5e":"sns.countplot(data= data, x = \"gender\")\nplt.show()","18850d7f":"data[\"gender\"].value_counts().head(7).plot(kind = 'pie', autopct='%1.1f%%', figsize=(10, 10), startangle=0).legend()","7067f38a":"sns.countplot(data= data, x = \"recruitment_channel\")\nplt.show()","16d190ed":"data[\"recruitment_channel\"].value_counts().head(7).plot(kind = 'pie', autopct='%1.1f%%', figsize=(10, 10), startangle=0).legend()","1d4437d8":"sns.countplot(data= data, x = \"no_of_trainings\")\nplt.show()","19279beb":"data[\"no_of_trainings\"].value_counts().head(10).plot(kind = 'pie', autopct='%1.1f%%', figsize=(10, 10), startangle=0).legend()","4de47061":"plt.figure(figsize=(12,10))\nsns.countplot(data.age)\nplt.xticks(rotation=90)","eed95108":"data[\"age\"].value_counts().head(10).plot(kind = 'pie', autopct='%1.1f%%', figsize=(10, 10), startangle=0).legend()","78b5d061":"sns.countplot(data= data, x = \"previous_year_rating\")\nplt.show()","e1e9f3d5":"data[\"previous_year_rating\"].value_counts().head(7).plot(kind = 'pie', autopct='%1.1f%%', figsize=(10, 10), startangle=0).legend()","535e7387":"plt.figure(figsize=(12,10))\nsns.countplot(data.length_of_service)\nplt.xticks(rotation=90)","74d2e93a":"data[\"length_of_service\"].value_counts().head(10).plot(kind = 'pie', autopct='%1.1f%%', figsize=(10, 10), startangle=0).legend()","2b3499e9":"sns.countplot(data= data, x = \"awards_won\")\nplt.show()","7a7772c0":"data[\"awards_won\"].value_counts().head(7).plot(kind = 'pie', autopct='%1.1f%%', figsize=(10, 10), startangle=0).legend()","3f5f30c8":"data[\"avg_training_score\"].value_counts().head(10).plot(kind = 'pie', autopct='%1.1f%%', figsize=(10, 10), startangle=0).legend()","9a51f01e":"sns.countplot(data= data, x = \"is_promoted\")\nplt.show()","7f382d3f":"data[\"is_promoted\"].value_counts().head(7).plot(kind = 'pie', autopct='%1.1f%%', figsize=(10, 10), startangle=0).legend()","01bde5a6":"data[\"is_promoted\"].value_counts()","e4ba461a":"plt.rcParams['figure.figsize'] = [10, 5]\nscore_bin = pd.crosstab(data.no_of_trainings,data.is_promoted,normalize='index')\nscore_bin.plot.bar(stacked=True)\nplt.legend(title='is_promoted',loc='upper left',bbox_to_anchor=(1, 0.5))","a251da9e":"plt.rcParams['figure.figsize'] = [10, 5]\nscore_bin = pd.crosstab(data.previous_year_rating,data.is_promoted,normalize='index')\nscore_bin.plot.bar(stacked=True)\nplt.legend(title='is_promoted',loc='upper left',bbox_to_anchor=(1, 0.5))","68a73eb4":"plt.rcParams['figure.figsize'] = [10, 5]\nscore_bin = pd.crosstab(data.length_of_service,data.is_promoted,normalize='index')\nscore_bin.plot.bar(stacked=True)\nplt.legend(title='is_promoted',loc='upper left',bbox_to_anchor=(1, 0.5))","47a4a486":"plt.rcParams['figure.figsize'] = [10, 5]\nscore_bin = pd.crosstab(data.awards_won,data.is_promoted,normalize='index')\nscore_bin.plot.bar(stacked=True)\nplt.legend(title='is_promoted',loc='upper left',bbox_to_anchor=(1, 0.5))","85a73b94":"plt.rcParams['figure.figsize'] = [10, 5]\nscore_bin = pd.crosstab(data.avg_training_score,data.is_promoted,normalize='index')\nscore_bin.plot.bar(stacked=True)\nplt.legend(title='is_promoted',loc='upper left',bbox_to_anchor=(1, 0.5))","457ad1bd":"data.select_dtypes(include='object')","2b5488e1":"pro= preprocessing.LabelEncoder()\nencpro=pro.fit_transform(data['department'])\ndata['department'] = encpro\n\npro= preprocessing.LabelEncoder()\nencpro=pro.fit_transform(data['region'])\ndata['region'] = encpro\n\npro= preprocessing.LabelEncoder()\nencpro=pro.fit_transform(data['education'].astype(str))\ndata['education'] = encpro\n\npro= preprocessing.LabelEncoder()\nencpro=pro.fit_transform(data['gender'])\ndata['gender'] = encpro\n\npro= preprocessing.LabelEncoder()\nencpro=pro.fit_transform(data['recruitment_channel'].astype(str))\ndata['recruitment_channel'] = encpro\n\npro= preprocessing.LabelEncoder()\nencpro=pro.fit_transform(data['recruitment_channel'])\ndata['recruitment_channel'] = encpro","42098313":"colormap = plt.cm.RdBu\nplt.figure(figsize=(14,12))\nplt.title('Pearson Correlation of Features', y=1.05, size=15)\nsns.heatmap(data.corr(),linewidths=0.1,vmax=1.0, \n            square=True, cmap=colormap, linecolor='white', annot=True)","47167366":"g = sns.FacetGrid(data, col='is_promoted')\ng.map(plt.hist, 'age', bins=20)","b1fb3e2d":"g = sns.FacetGrid(data, col='is_promoted')\ng.map(plt.hist, 'gender', bins=20)","3e0b9390":"g = sns.FacetGrid(data, col='is_promoted')\ng.map(plt.hist, 'recruitment_channel', bins=20)","8fdea641":"g = sns.FacetGrid(data, col='is_promoted')\ng.map(plt.hist, 'no_of_trainings', bins=20)","f676539f":"g = sns.FacetGrid(data, col='is_promoted')\ng.map(plt.hist, 'previous_year_rating', bins=20)","7e6d09c6":"g = sns.FacetGrid(data, col='is_promoted')\ng.map(plt.hist, 'length_of_service', bins=20)","b0fd01ca":"g = sns.FacetGrid(data, col='is_promoted')\ng.map(plt.hist, 'awards_won', bins=20)","3a619ef8":"g = sns.FacetGrid(data, col='is_promoted')\ng.map(plt.hist, 'avg_training_score', bins=20)","9d0ba4c2":"sns.pairplot(data,diag_kind='kde',hue='is_promoted')","ddfc0714":"current=len(data)\nprint('Rows of data before Delecting ', current)","b3ed15c5":"data=data.drop_duplicates()","4e8938de":"now=len(data)\nprint('Rows of data before Delecting ', now)","a47e0057":"diff=current-now\nprint('Duplicated rows deleted ', diff)","f199aff7":"data=data.drop(columns=['employee_id'])","610e1c59":"data.isnull().sum()","88063d00":"data.isnull().sum().sum()\/len(data)","74cc2cde":"data_total = data.isnull().sum()\ndata_percent = ((data.isnull().sum()\/data.shape[0])*100).round(2)\nmissing_data = pd.concat([data_total, data_percent],\n                                axis=1, \n                                keys=['Train_Total', 'Train_Percent %','Test_Total', 'Test_Percent %'],\n                                sort = True)\nmissing_data.style.bar(color = ['gold'])","ec9be0ad":"py=data[data['previous_year_rating'].isnull()]\npy.head()","39f957cb":"py['length_of_service'].value_counts()","560e3854":"data['previous_year_rating'].fillna(value=0,inplace=True)","a0a429f8":"data['education'] = data['education'].fillna(data['education'].mode()[0])\ndata['avg_training_score'] = data['avg_training_score'].fillna(data['avg_training_score'].mode()[0])","f201ef92":"data.isnull().sum()","4a9e1bd5":"X=data.drop(columns=['is_promoted'])\ny=data['is_promoted']","12e2f17b":"smt = SMOTE()\nX_up, y_up = smt.fit_resample(X, y)","d63c7051":"sns.countplot(data= data, x = y_up)","7025c4d5":"X_train, X_test, y_train, y_test = train_test_split(X_up, y_up, test_size=0.3, random_state=2)","e6bf786f":"LR=RandomForestClassifier()\nLR= LR.fit(X_train , y_train)\nLR","25cbb259":"print('Test set\\n  Accuracy: {:0.2f}'.format(LR.score(X_test, y_test))) #the accuracy of the model on test data is given below","a45017be":"y_pred = LR.predict(X_test) #getting predictions on the trained model","c3486281":"print('Precision',round(f1_score(y_test, y_pred, average='micro'),3),'%')","f5388c8e":"print('Recall',round(recall_score(y_test, y_pred, average='micro'),4),'%')","725793b5":"print('F1',round(f1_score(y_test, y_pred, average='micro'),2),'%')","e57aa18e":"data[\"is_promoted\"].value_counts()","ec31d3dc":"promoted=data[data[\"is_promoted\"]==1] #getting the promoted employees\nnot_promoted=data[data[\"is_promoted\"]==0] #getting not promoted employees\nnot_promoted=not_promoted.sample(4668) #getting only 4668 fromnot employees to down sample the data equal\ndownsample=pd.concat([promoted, not_promoted]) #now combining both\ndownsample","1b923758":"sns.countplot(data= data, x = downsample['is_promoted'])","f3c7d811":"X_train, X_test, y_train, y_test = train_test_split(downsample.drop(columns=['is_promoted']), downsample['is_promoted'], test_size=0.3, random_state=2)","a2187624":"LR=RandomForestClassifier()\nLR= LR.fit(X_train , y_train)\nLR","8976eda4":"print('Test set\\n  Accuracy: {:0.2f}'.format(LR.score(X_test, y_test))) #the accuracy of the model on test data is given below","2cb11da6":"y_pred = LR.predict(X_test) #getting predictions on the trained model","acfff9c5":"print('Precision',round(f1_score(y_test, y_pred, average='micro'),3),'%')","d0828c2c":"print('Recall',round(recall_score(y_test, y_pred, average='micro'),4),'%')","01910274":"print('F1',round(f1_score(y_test, y_pred, average='micro'),2),'%')","58c9db7e":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=2)","be0e10a8":"DT=DecisionTreeClassifier()\nDT= DT.fit(X_train , y_train)\nDT","fd4425cb":"dt=DT.score(X_test, y_test)\nprint('Test set\\n  Accuracy: {:0.2f}'.format(DT.score(X_test, y_test))) #the accuracy of the model on test data is given below","6ba1c566":"RN=RandomForestClassifier()\nRN= RN.fit(X_train , y_train)\nRN","8be13dad":"rn=RN.score(X_test, y_test)\nprint('Test set\\n  Accuracy: {:0.2f}'.format(RN.score(X_test, y_test))) #the accuracy of the model on test data is given below","aa7619a1":"BC=BaggingClassifier()\nBC= BC.fit(X_train , y_train)\nBC","3dff38d5":"bc=BC.score(X_test, y_test)\nprint('Test set\\n  Accuracy: {:0.2f}'.format(BC.score(X_test, y_test))) #the accuracy of the model on test data is given below","393bcf08":"XG=XGBClassifier(verbosity = 0)\nXG= XG.fit(X_train , y_train)\nXG","7cd3a0db":"xg=XG.score(X_test, y_test)\nprint('Test set\\n  Accuracy: {:0.2f}'.format(XG.score(X_test, y_test))) #the accuracy of the model on test data is given below","f6734ef9":"AD=AdaBoostClassifier()\nAD= AD.fit(X_train , y_train)\nAD","d0749a05":"ad=AD.score(X_test, y_test)\nprint('Test set\\n  Accuracy: {:0.2f}'.format(AD.score(X_test, y_test))) #the accuracy of the model on test data is given below","f4effc19":"GB=GradientBoostingClassifier()\nGB= GB.fit(X_train , y_train)\nGB","a3dda480":"gb=GB.score(X_test, y_test)\nprint('Test set\\n  Accuracy: {:0.2f}'.format(GB.score(X_test, y_test))) #the accuracy of the model on test data is given below","bcbadd4e":"x = PrettyTable()\nprint('\\n')\nx.field_names = [\"Model\", \"Accuracy\"]\nx.add_row([\"Decision Tree Model\", round(dt,2)])\nx.add_row([\"Random Forest Classifier Model\", round(rn,2)])\nx.add_row([\"Bagging Classifier Model\", round(bc,2)])\nx.add_row([\"XGB Classifierr Model\", round(xg,2)])\nx.add_row([\"AdaBoost Classifier Model\", round(ad,2)])\nx.add_row([\"Gradient Boosting Classifier Model\", round(gb,2)])\n\nprint(x)\nprint('\\n')","d71e29c6":"param_grid = { \n    'n_estimators': [200, 500],\n    'max_features': ['auto', 'sqrt', 'log2'],\n    'max_depth' : [4,5,6,7,8],\n    'criterion' :['gini', 'entropy']\n}","9c6246c5":"start = time.time()\nRN=RandomForestClassifier()\nGrid_RN = GridSearchCV(estimator=RN, param_grid=param_grid, cv= 2)\nGrid_RN.fit(X_train, y_train)\nend = time.time()\nRF_time1=end-start","6126cfe1":"Grid_RN.best_params_","9f9a0cf9":"print('Execution time is ', RF_time1)","300cbfc8":"y_pred = Grid_RN.predict(X_test) #getting predictions on the trained model\nrn1=round(recall_score(y_test, y_pred, average='micro'),3)\nprint('Recall',round(recall_score(y_test, y_pred, average='micro'),3),'%')","69b385d2":"params = {\n        'min_child_weight': [1, 5, 10],\n        'gamma': [0.5, 1, 1.5, 2, 5],\n        'subsample': [0.6, 0.8, 1.0],\n        'colsample_bytree': [0.6, 0.8, 1.0],\n        'max_depth': [3, 4, 5]\n        }","8861d247":"start = time.time()\nXGB=XGBClassifier(verbosity = 0)\nGrid_XG = GridSearchCV(estimator=XGB, param_grid=params, cv= 2)\nGrid_XG.fit(X_train, y_train)\nend = time.time()\nXG_time1=end-start","cca8b5d2":"Grid_XG.best_params_","42db7a51":"print('Execution time is ', XG_time1)","3acebd33":"y_pred = Grid_XG.predict(X_test) #getting predictions on the trained model\nxg1=round(recall_score(y_test, y_pred, average='micro'),3)\nprint('Recall',round(recall_score(y_test, y_pred, average='micro'),3),'%')","0add79fa":"param = {\n    \"learning_rate\": [0.01, 0.025],\n    \"min_samples_split\": np.linspace(0.1, 0.5),\n    \"min_samples_leaf\": np.linspace(0.1, 0.5),\n    }","25b9c108":"start = time.time()\nBC=GradientBoostingClassifier()\nGrid_BC = GridSearchCV(estimator=BC, param_grid=param, cv= 2)\nGrid_BC.fit(X_train, y_train)\nend = time.time()\nBC_time1=end-start","a4b14f57":"print('Execution time is ', BC_time1)","6416ea46":"y_pred = Grid_BC.predict(X_test) #getting predictions on the trained model\ngc1=round(recall_score(y_test, y_pred, average='micro'),3)\nprint('Recall',round(recall_score(y_test, y_pred, average='micro'),3),'%')","16939e24":"params = { \n    'n_estimators': [200, 500],\n    'max_features': ['auto', 'sqrt', 'log2'],\n    'max_depth' : [4,5,6,7,8],\n    'criterion' :['gini', 'entropy']\n}","f6bfb34a":"start = time.time()\nRN=RandomForestClassifier()\nGrid_RN = RandomizedSearchCV(estimator = RN, param_distributions = params, n_iter = 100, cv = 2, verbose=2, random_state=42, n_jobs = -1)\nGrid_RN.fit(X_train, y_train)\nend = time.time()\nRF_time2=end-start","1a31071b":"Grid_RN.best_params_","ccc54761":"print('Execution time is ', RF_time2)","9b916eb9":"y_pred = Grid_RN.predict(X_test) #getting predictions on the trained model\nrn2=round(recall_score(y_test, y_pred, average='micro'),3)\nprint('Recall',round(recall_score(y_test, y_pred, average='micro'),3),'%')","dfef9731":"params = {\n        'min_child_weight': [1, 5, 10],\n        'gamma': [0.5, 1, 1.5, 2, 5],\n        'subsample': [0.6, 0.8, 1.0],\n        'colsample_bytree': [0.6, 0.8, 1.0],\n        'max_depth': [3, 4, 5]\n        }","147309f8":"start = time.time()\nXGB=XGBClassifier(verbosity = 0)\nGrid_XG = RandomizedSearchCV(estimator = XGB, param_distributions = params, n_iter = 100, cv = 2, verbose=2, random_state=42, n_jobs = -1)\nGrid_XG.fit(X_train, y_train)\nend = time.time()\nXG_time2=end-start","6b76edec":"Grid_XG.best_params_","9691a268":"print('Execution time is ', XG_time2)","e4eb935e":"y_pred = Grid_XG.predict(X_test) #getting predictions on the trained model\nxg2=round(recall_score(y_test, y_pred, average='micro'),3)\nprint('Recall',round(recall_score(y_test, y_pred, average='micro'),3),'%')","8fafb8ec":"params = {\n    \"loss\":[\"deviance\"],\n    \"learning_rate\": [0.01, 0.025],\n    \"min_samples_split\": np.linspace(0.1, 0.5),\n    \"min_samples_leaf\": np.linspace(0.1, 0.5),\n    \"max_depth\":[3,5],\n    \"max_features\":[\"log2\",\"sqrt\"],\n    \"criterion\": [\"friedman_mse\",  \"mae\"],\n    \"subsample\":[0.5, 0.618],\n    \"n_estimators\":[10]\n    }","559c3a4b":"start = time.time()\nBC=GradientBoostingClassifier()\nGrid_BC = RandomizedSearchCV(estimator = BC, param_distributions = params, n_iter = 100, cv = 2, verbose=2, random_state=42, n_jobs = -1)\nGrid_BC.fit(X_train, y_train)\nend = time.time()\nBC_time2=end-start","9b266cfc":"Grid_BC.best_params_","32b8458c":"print('Execution time is ', BC_time2)","e948372b":"y_pred = Grid_BC.predict(X_test) #getting predictions on the trained model\nbc2=round(recall_score(y_test, y_pred, average='micro'),3)\nprint('Recall',round(recall_score(y_test, y_pred, average='micro'),3),'%')","d4908682":"x = PrettyTable()\nprint('\\n')\nx.field_names = [\"Model\", \"Recall Metric\"]\n\nx.add_row([\"Random Forest Classifier Model\", round(rn1,2)])\nx.add_row([\"XGB Classifierr Model\", round(xg1,2)])\nx.add_row([\"Gradient Boosting Classifier Model\", round(gc1,2)])\n\nprint(x)\nprint('\\n')","66e639cb":"x = PrettyTable()\nprint('\\n')\nx.field_names = [\"Model\", \"Execution Time\"]\n\nx.add_row([\"Random Forest Classifier Model\", round(RF_time1,2)])\nx.add_row([\"XGB Classifierr Model\", round(XG_time1,2)])\nx.add_row([\"Gradient Boosting Classifier Model\", round(BC_time1,2)])\n\nprint(x)\nprint('\\n')","06589093":"x = PrettyTable()\nprint('\\n')\nx.field_names = [\"Model\", \"Recall Metric\"]\n\nx.add_row([\"Random Forest Classifier Model\", round(rn2,2)])\nx.add_row([\"XGB Classifierr Model\", round(xg2,2)])\nx.add_row([\"Gradient Boosting Classifier Model\", round(bc2,2)])\n\nprint(x)\nprint('\\n')","ed952a0d":"x = PrettyTable()\nprint('\\n')\nx.field_names = [\"Model\", \"Execution Time\"]\n\nx.add_row([\"Random Forest Classifier Model\", round(RF_time2,2)])\nx.add_row([\"XGB Classifierr Model\", round(XG_time2,2)])\nx.add_row([\"Gradient Boosting Classifier Model\", round(BC_time2,2)])\n\nprint(x)\nprint('\\n')","9830d971":"#### Accuracy\n- Accuracy is the number of correctly classify promoted or not promoted. \n- Accuracy= Total number of correct predictions\/Total number of predictions","591ed967":"### avg_training_score distribution","0d53bd08":"### Checking categorical type columns in the data ","cb9987fc":"<div class=\"alert alert-block alert-danger\">  \n<h1><center><strong>Data Preparation \ud83d\udcdd<\/strong><\/center><\/h1>\n\n\n   \n        \n<\/div>","010a09dc":"### length_of_service distribution","206b9074":"<div class=\"alert alert-block alert-success\">  \n<h4>XGB Classifier Model<\/h4>\n        \n<\/div>","0b01a8df":"<div class=\"alert alert-block alert-success\">  \n<h4>Gradient Boosting Classifier Model<\/h4>\n        \n<\/div>","c49bb457":"### Only 2.3% employees won the awards and 97.7% did not won. Looking at awards, its very less ratio. ","c68b228a":"#### Length of data","89235d53":"- Libraries are important and we call them to perform the different actions on our data and for training the models.\n- Its a first step to load the library to perform the specific task","20219eb5":"#### There are almost 16% missing values in the data","5cece33e":"### We can clearly see that, the data is not balanced. The promoted employees are only 4668 and not promoted employees are 50140. 91% and 9% ratio is very unbalanced. ","26b53cfb":"<div class=\"alert alert-block alert-success\">  \n<h4>XGB Classifierr Model<\/h4>\n        \n<\/div>","33865fcb":"From no of trainings 1 to 6, employees are promoted. From no of trainings 7 to 10, employees are not promoted.","6a2cd867":"### Working on the Education and Previous_Year_rating","b3972bb8":"### awards_won plot with is_promoted","30c72fb7":"#### Precision Score on test data\n- Precision measure the number of positive class predictions that actually belong to the positive class","2c77700d":"#### Recall Score on test data","6bb6d881":"### If we look at the gender, males are too much again with amlost 38K data and females 16K. ","609a0e97":"#### Rows and columns in the dataset","e8af6a84":"<div class=\"alert alert-block alert-success\">  \n<h4>Gradient Boosting Classifier Model<\/h4>\n        \n<\/div>","b6740833":"<div class=\"alert alert-block alert-info\">  \n    <h1><strong>\ud83d\udc68\u200d\ud83d\udcbb Getting Started with Employee Promotion<\/strong><\/h1>\n    <i><\/i>\n<\/div>","68513c83":"# <img src=\"https:\/\/sherwoodlumber.com\/wp-content\/uploads\/2020\/02\/2016-10-24-employee-promotions-best-practices.png\">","27946efb":"### Distribution of promoted employees ratio across different previous_year_rating","9bc261de":"#### Recall Score on test data\n- Recall measures the number of positive class predictions made out of all positive records in the dataset","a0f7e292":"### is_promoted distribution","46b71bfc":"### department distribution","8d0cfc61":"The XGboost model is giving more than 94% recall but the training time is more. It would be great if the training time less.","0c86ccd7":"#### Recall Score on test data","f96a5bba":"### previous_year_rating plot with is_promoted","594e0dbd":"### If we look at the no_of_trainings, all employees participated one time with 81%. ","6de01f15":"<b> <h3> Inputs : <\/h3><\/b>  Inputs are the data features that we feed into model like in this project department, region,education,gender are the inputs. ","0de1168c":"#### Getting prediction of the test data and then we will compare the true Target\/classes of the data with predictions","8fccd6d0":"#### Data information","5693dbbf":"### previous_year_rating distribution","cfdce137":"<div class=\"alert alert-block alert-success\">  \n<h4>Execution Time<\/h4>\n        \n<\/div>","4edf03ea":"## Evaluation of Trained model on test data","fd8f1f87":"The XGboost model is giving more than 94% recall but the training time is more. It would be great if the training time less.","dd52a577":"### Rating 3 is most used for the employees in all data with 36%. 5 rating with 23%. ","c02bca82":"<div class=\"alert alert-block alert-success\">  \n<h4>Decision Tree Model<\/h4>\n        \n<\/div>","40f34a38":"#### Getting prediction of the test data and then we will compare the true Target\/classes of the data with predictions","a7aa7dcd":"More Training Score means more chances of promotions.","7d89b1b6":"## Evaluation of Trained model on test data","10c7c8cf":"#### Shape of data","544135fc":"The model is giving good results and taking reasonable time on training. 92% accuracy is good but still we can improve the model with different techniques of machine learning.","e582e184":"### recruitment_channel plot with is_promoted","ea7a8e3a":"### avg_training_score plot with is_promoted","20026998":"From year of service 1 to 23, employees promoted but in after 24 years of service, peoples are often promoted.","8cddd0dc":"Gradient Boosting model outform over random Forest modelbut XGboost is still good. Also it took very long time to train the model.It is not a good thing to take a lot of time on training. ","6a35334c":"#### Checking Null values \/ missing values","15d669ff":"As we prepared all the data, now we are separating\/splitting the all data into training data and testing data.\n- 70% data will be used in the training \n- 30% data will be used to test the performance of the model.","22ce9dc3":"### recruitment_channel distribution","fa43efd7":"XGBoost and Gradient Decent models are giving highest accuracy with 94% which is good. But Decision tree model is lower than others with only 88% accuracy. Random forest,Bagging and Adaboost also performed well with 93% accuracy. ","359d08d4":"### Length of service is from 1 to 10 years. After 10 years, there are few employees.","4b564447":"<div class=\"alert alert-block alert-success\">  \n<h4>Random Forest Classifier Model<\/h4>\n        \n<\/div>","abe0dfc6":"<div class=\"alert alert-block alert-success\">  \n<h4>Evaluation<\/h4>\n        \n<\/div>","e092b02c":"Now we dont have any any missing values in the features.","1e4b0b62":"### lets calculate the total missing values in the each column","e001b419":"#### F1 Measure Score on test data\n- F-Measure is the average of the precision and recall. ","73f83344":"Previous year training matter for employee promotion. As we can see that more years training means more employees promoted.","efcde887":"<b> <h3> Target : <\/h3><\/b>  Target are the Results like in this project 1 and 0 are Target. ","f61f4816":"#### Counts of missing values in each column","bf748294":"<div class=\"alert alert-block alert-success\">  \n<h4>Execution Time<\/h4>\n        \n<\/div>","eb33b80d":"### age distribution","40e75020":"Same performance and comments on the random search because we got the same kind of results. ","d623bf69":"### Encoding these categorical features into numeric type","b220cc1b":"#### Numeric features distrubution ","1dc97b60":"### region distribution","1b2f3250":"#### Recall Score on test data\n- Recall measures the number of positive class predictions made out of all positive records in the dataset","3981c52e":"## Univariate Analysis ","892b4553":"### The Pairplot is showing the relationship of all with each other. In some comparisons, the features are distributed more and that is the case of continuous.","77490b1b":"As we can see that, we encoded the categorical features into numeric form so we can use them in the models.","83b2bf0a":"On Random Search,the models took less training time and got the same accuracy on all models as compared to Grid search.","e16c56e0":"<div class=\"alert alert-block alert-success\">  \n<h4>Bagging Classifier Model<\/h4>\n        \n<\/div>","382722ab":"#### promoted target is good correlated with the following features:\n- employee Id\n- deparment \n- region\n- education\n- recruitement_channel\n- previous year rating \n- awards won\n- avg training score\n\n#### promoted target is not good correlated with the following features:\n- gender \n- no of training \n- age \n- length of service ","03abd54b":"### As we can see above, there are the values ranges of all features. Every feature have different distribution of values. To understand the features better and deeper, we are going to look at each one separately. ","cf6db135":"<div class=\"alert alert-block alert-info\">  \n<h2><center><strong> Hyperparameter Tuning using Grid Search<\/strong><\/center><\/h2>\n    <li> Random Forest Model<\/li>\n    <li> XGB Classifierr Model<\/li>\n    <li> Gradient Boosting Classifier Model<\/li>\n        \n<\/div>","2fccd26f":"### gender distribution","5c981a37":"<div class=\"alert alert-block alert-info\">  \n<h1><center><strong>Actionable Insights & Recommendations \ud83d\udcdd<\/strong><\/center><\/h1>\n    <p>\n<li>We used the Employee promotion dataset and explored the data with different ways.<\/li>\n        <li>While Technology department had highest percentage of employees getting promoted, Legal department has the least number. But we don't see major differences in terms of percentages.\n<\/li>\n        <li>According to the data, percentage of promotions is higher among the employees who got recruited through referrals.\n<\/li>\n        <li>The ratio of promoted employees increases with previous year rating which is quite obvious.\n <\/li>\n        <li>Promotions ratio increases with the score and the ratio is very high in 90-100 range which means getting promoted is highly dependent on the average score\n <\/li>\n          <li>Ratio doesn't vary much with the employees age.<\/li>\n        <li>Promotions are majorly dependent on the training score and not on age<\/li>\n        <li>It is recommneded to manage the data of employees with all checkpoints like previous year rating is important but it contains a lot of missing values. <\/li>\n        <li>The data is not balanced with promoted employees and not employee but the above facts clealry tell us that which employees can be promoted.<\/li>\n         <li>We trained the model with over sampling but the results were not satisfactory because we over sampled the promoted records 98%.<\/li>\n        <li>We trained the model with under sampling but the results were not satisfactory again and it was because we under sampled the not promoted records 48%<\/li>\n        <li>We trained the models with Bagging and Boosting and got the good results. <\/li>\n        <li>After that we applied the grid search and random search techniques to train the models.  <\/li>\n        <li>We evaluated th model with different evaluation measures to check the performance on all techniques that we applied. <\/li>\n           <li>XGBoost Model gave us the good results with 94% as well as took very less time as compare to other models.  <\/li>\n         <li>HR department can use the final model of XGBoost and can feed the previous record of the employee and can get prediction that the employee should be promoted or not. <\/li>\n        <\/p>\n<\/div>","efdb752a":"### In the recruitment_channel, other are 55%,referred cases are 2% and sourcing 42%.","9abe23ca":"<div class=\"alert alert-block alert-success\">  \n<h4>XGB Classifier Model<\/h4>\n        \n<\/div>","67e786af":"### If we look at the region, region_2 is more in counts with 32% and region_22 is 16% and region_7 is 12%. It means that these three regions cover almost 60% data.  ","ffc72d50":"#### Five top records of data","6ff1ab81":"#### Coloumns\/features in data","c0a67d02":"### length_of_service plot with is_promoted","0bc58775":"### education distribution","9f97b21b":"#### Recall Score on test data","95a45bc4":"#### Training the  Random Forest Model","f3683afa":"### We can see that, Sales and marketing department employees data is more than other departments. Its 30% in whole data and on the second number, Oprations department is 20%. ","bca372c4":"<div class=\"alert alert-block alert-success\">  \n<h4>Random Forest Classifier Model<\/h4>\n        \n<\/div>","6fd61fa1":"#### Precision Score on test data\n- Precision measure the number of positive class predictions that actually belong to the positive class","3c22efda":"### Distribution of promoted employees ratio across different avg_training_score","aa644e0f":"### Pair plot of data","a44db0b4":"The model is giving good results and taking more time on training. 92% accuracy is good but still we can improve the model with different techniques of machine learning.","655fd387":"Gradient Boosting model outform over random Forest modelbut XGboost is still good. Also it took very long time to train the model.It is not a good thing to take a lot of time on training. ","67d09a61":"<div class=\"alert alert-block alert-info\">  \n<h2><center><strong> Model building - Bagging and Boosting<\/strong><\/center><\/h2>\n        \n<\/div>","9a613b95":"<h4>How we can install the libraries in python?<\/h4>","3e3d97fd":"### In the age, most ages are between 27 to 36 years. ","32ac458d":"### Age plot with is_promoted","e28a2e1d":"The model is giving good results and taking reasonable time on training. 92% accuracy is good but still we can improve the model with different techniques of machine learning.","017ee632":"#### Recall Score on test data","2f507322":"<div class=\"alert alert-block alert-danger\">  \n<h2><center><strong>Exploratory data analysis \ud83d\udd0e \ud83d\udcca<\/strong><\/center><\/h2>\n        \n<\/div>","a8bcb3b3":"### If we look at the education of employees, Bacherlors are too much with 70% data, 28% Master and 1.5 below secondary education. ","71bbc021":"#### Separating the 70% data for training data and 30% for testing data","b9b7bef5":"### Distribution of promoted employees ratio across different awards_won","d663b5d4":"#### Five last records of data","184424eb":"#### Recall Score on test data","a4aefbd0":"### Dropping the Employee_Id in the data because it is not useful and helpful for training and helping to predict the employee promotion. ","6629aa68":"The model is giving good results and taking more time on training. 92% accuracy is good but still we can improve the model with different techniques of machine learning.","ead23b83":"<div class=\"alert alert-block alert-info\">  \n<h1><center><strong>Model Performances<\/strong><\/center><\/h1>\n        \n<\/div>","57ec1075":"### Deleting the duplicate rows","d34b81b5":"<div class=\"alert alert-block alert-info\">  \n<h4>Make logistic regression with upsampled data<\/h4>\n        \n<\/div>","bd9de7a7":"<div class=\"alert alert-block alert-info\">  \n<h2><center><strong> Hyperparameter Tuning using Random Search<\/strong><\/center><\/h2>\n    <li> Random Forest Model<\/li>\n    <li> XGB Classifierr Model<\/li>\n    <li> Gradient Boosting Classifier Model<\/li>\n        \n<\/div>","fddd6dbc":"<div class=\"alert alert-block alert-danger\">  \n<h1><center><strong>Comparison of Bagging and Boosting Models using Hyperparameter Tuning using Grid Search<\/strong><\/center><\/h1>\n        \n<\/div>","03ed5f07":"### working on the previous_year_rating ","7c4231d6":"#### Accuracy\n- Accuracy is the number of correctly classify the promoted and notpromoted. \n- Accuracy= Total number of correct predictions\/Total number of predictions","8a5ff6a0":"#### Separating the 70% data for training data and 30% for testing data","66a7eee7":"Wining awards means more chances to get promoted. as we can see that employees with awards have higher count of promotions.","d0ce093d":"<b> <h3> Testing Data <\/h3><\/b>  We use testing data after training the model. We use this data to evalaute the performance that how the model perform after training. So in this way first we get predictions from the trained model without giving the Target and then we compare the true Target with predictions and get the performance of the model.","fa829978":"The results of gradient boosting are not good as compare to other models as XGBoost giving more than 94% accuracy.","5865ba6a":"### the more ages are with not promoted and less ages are with promoted employees.","bfc7c16a":"<div class=\"alert alert-block alert-info\">  \n<h4>Logistic regression with downsampled data<\/h4>\n        \n<\/div>","1ec35df6":"<h4>To install the python library is very easy<\/h4>\n- pip install name_of_library \n<h5> Like if you wanted to install numpy? <\/h5>\n- pip install numpy","ed6013f3":"### Missing value Treatment","fef457f7":"<div class=\"alert alert-block alert-info\">  \n<h2><center><strong> Model building - Logistic Regression<\/strong><\/center><\/h2>\n        \n<\/div>","ce5c9c4a":"<b> <h3> Training Data <\/h3><\/b>  We use training data when we train the models. We feed train data to tensorflow model so that model can learn from the data.","bd521ad8":"<div class=\"alert alert-block alert-danger\">  \n<h1><center><strong>Comparison of Bagging and Boosting Models using Hyperparameter Tuning using Random Search<\/strong><\/center><\/h1>\n        \n<\/div>","6196fb89":"The XGboost model is giving more than 94% recall and also taking less time as compare to other models.","db75d1e7":"<div class=\"alert alert-block alert-danger\">  \n<h2><center><strong>Importing Python Libraries \ud83d\udcd5 \ud83d\udcd7 \ud83d\udcd8 \ud83d\udcd9<\/strong><\/center><\/h2>\n        \n<\/div>","88c4c704":"<div class=\"alert alert-block alert-success\">  \n<h4>AdaBoost Classifier Model<\/h4>\n        \n<\/div>","6fde754b":"#### Data Description","5edd719b":"<div class=\"alert alert-block alert-success\">  \n<h4>Evaluation<\/h4>\n        \n<\/div>","6457ceb7":"<div class=\"alert alert-block alert-danger\">  \n<h1><center><strong>Comparison of Bagging and Boosting Models<\/strong><\/center><\/h1>\n        \n<\/div>","f3322611":"#### Recall Score on test data","7ea52198":"### Distribution of promoted employees ratio across different length_of_service","bf6ea8c2":"#### Training the Random Forest  Model","fb86839c":"### awards_won distribution","adc15909":"The XGboost model is giving more than 94% recall and also taking less time as compare to other models.","894e3767":"The results of gradient boosting are not good as compare to other models as XGBoost giving more than 94% accuracy.","b82c1aad":"Since the length of service is 1 for all the employees with previous year rating as null.,which means they are the new recruits with 1 year experience. So they may not be having the previous year rating.We impute 0 for the null values.","045d56a6":"<div class=\"alert alert-block alert-success\">  \n<h4>Random Forest Classifier Model<\/h4>\n        \n<\/div>","dcb6ffa8":"<div class=\"alert alert-block alert-success\">  \n<h4>Gradient Boosting Classifier Model<\/h4>\n        \n<\/div>","7d72ac18":"### no_of_trainings distribution","510e0ff4":"#### F1 Measure Score on test data\n- F-Measure is the average of the precision and recall. ","ebeaac19":"### no_of_trainings plot with is_promoted","ac13fde0":"<div class=\"alert alert-block alert-danger\">  \n<h2><center><strong>Loading the data \ud83d\udcc1 \ud83d\udcc2<\/strong><\/center><\/h2>\n        \n<\/div>","94fc18d6":"<h4> We are uisng the following versions of the libraries:<\/h4>\n\n- numpy == 1.18.5 \n\n- pandas == 1.1.3\n\n- seaborn ==0.11.0","5c5eda5e":"### The counts of gender are more in not promoted and less in promoted. Males are more in both cases.","6b18ab59":"## Bivariate Analysis ","7ed4b6db":"### gender plot with is_promoted","1158eeeb":"### Distribution of promoted employees ratio across different no_of_trainings","653b3fbc":"####  Separating input feature and label","1a58c9e3":"#### Data types of all coloumns","97e9ae6a":"As we prepared all the data, now we are separating\/splitting the all data into training data and testing data.\n- 70% data will be used in the training \n- 30% data will be used to test the performance of the model."}}