{"cell_type":{"1341d261":"code","a2b4737e":"code","dfbb506b":"code","e0c68e9b":"code","0c9be5ac":"code","8623a86e":"code","3aae7fee":"code","b5a7ee85":"code","156334c2":"code","73da447e":"code","b40c4f58":"code","9e42d7c4":"code","d94000cf":"code","e514fa04":"code","200ab6f6":"code","d5334a60":"code","063aa55e":"code","5a6e389b":"code","54d82fed":"code","bc0233b5":"code","bdf69770":"code","e310281e":"code","54b34c81":"code","b1fdbd56":"markdown","a2912486":"markdown","239e3790":"markdown","f401cb51":"markdown","2e7de673":"markdown","f7fa0ceb":"markdown","fb3b68a3":"markdown","de814aa6":"markdown","34027244":"markdown","f36eafa8":"markdown","6eedc80e":"markdown","807add5d":"markdown","e0014586":"markdown","d4d82925":"markdown","0e1d63f7":"markdown","245da92d":"markdown","d73b3978":"markdown","fa2d158c":"markdown","25fb8381":"markdown"},"source":{"1341d261":"#import in libraries\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline","a2b4737e":"#read in train and test data\ntrain = pd.read_csv(r'..\/input\/titanic\/train.csv')\ntest = pd.read_csv(r'..\/input\/titanic\/test.csv')","dfbb506b":"train.info()","e0c68e9b":"train.describe()","0c9be5ac":"#print value counts for each categorical feature excluding Name \ncat_feats = ['Sex', 'Ticket', 'Cabin', 'Embarked']\n\nfor i in cat_feats:\n    print(i)\n    print(train[i].value_counts())\n    print('\\n\\n')\n    continue\n","8623a86e":"train.head()","3aae7fee":"train.tail()","b5a7ee85":"train.shape","156334c2":"sns.pairplot(data=train, hue='Survived')","73da447e":"plt.figure(figsize=(12,10))\nsns.heatmap(train.corr(), annot=True)","b40c4f58":"fig, axes = plt.subplots(2, 3, figsize=(18,10))\n\nfig.suptitle('Survived by most features ')\n\nsns.countplot(ax=axes[0, 0], data=train, x='Survived', hue='Pclass')\nsns.countplot(ax=axes[0, 1], data=train, x='Survived', hue='Sex')\nsns.countplot(ax=axes[0, 2], data=train, x='Survived', hue='SibSp')\nsns.countplot(ax=axes[1, 0], data=train, x='Survived', hue='Parch')\nsns.countplot(ax=axes[1, 1], data=train, x='Survived', hue='Embarked')\n","9e42d7c4":"fig, axes = plt.subplots(2, 2, figsize=(18,10))\n\nfig.suptitle('Misc Plots')\n\n\nsns.countplot(ax=axes[0, 0], data=train, x='Embarked',  hue='Pclass')\nsns.countplot(ax=axes[0, 1], data=train, x='Pclass')\nsns.histplot(ax=axes[1, 0],data=train, x='Fare', hue='Pclass', bins=10)\nsns.histplot(ax=axes[1, 1],data=train, x='Age', hue='Survived')","d94000cf":"test_passenger_ids = test['PassengerId']\n\ndef clean_data(data): \n    \n    #replace null values in Embarked column with Southampton location\n    data['Embarked'].loc[(data['Embarked'].isna())] = 'S'\n    \n    #replace null values in Age column with median age by Sex and Pclass\n    data['Age'].loc[(data['Sex']=='female') & (data['Pclass'] == 1) & (data['Age'].isna())] = data['Age'].loc[(data['Sex']=='female') & (data['Pclass'] == 1)].median()\n    data['Age'].loc[(data['Sex']=='female') & (data['Pclass'] == 2) & (data['Age'].isna())] = data['Age'].loc[(data['Sex']=='female') & (data['Pclass'] == 2)].median()\n    data['Age'].loc[(data['Sex']=='female') & (data['Pclass'] == 3) & (data['Age'].isna())] = data['Age'].loc[(data['Sex']=='female') & (data['Pclass'] == 3)].median()\n    data['Age'].loc[(data['Sex']=='male') & (data['Pclass'] == 1) & (data['Age'].isna())] = data['Age'].loc[(data['Sex']=='male') & (data['Pclass'] == 1)].median()\n    data['Age'].loc[(data['Sex']=='male') & (data['Pclass'] == 2) & (data['Age'].isna())] = data['Age'].loc[(data['Sex']=='male') & (data['Pclass'] == 2)].median()\n    data['Age'].loc[(data['Sex']=='male') & (data['Pclass'] == 3) & (data['Age'].isna())] = data['Age'].loc[(data['Sex']=='male') & (data['Pclass'] == 3)].median()\n       \n    #replace null fare data with mean fare cost\n    data['Fare'] = data['Fare'].fillna(8.05)\n  \n    #replace null cabin values with U (unknown)\n    data['Cabin'] = data['Cabin'].replace(np.nan, 'U')\n\n    #remove numbers from cabin column\n    data['Cabin'] = data['Cabin'].str.replace('\\d+','')\n\n    data['Cabin'].loc[data['Cabin'].str.contains('B')] ='B'\n    data['Cabin'].loc[data['Cabin'].str.contains('C')] ='C'\n    data['Cabin'].loc[data['Cabin'].str.contains('D')] ='D'\n    data['Cabin'].loc[data['Cabin'].str.contains('E')] ='E'\n    data['Cabin'].loc[data['Cabin'].str.contains('F')] ='F'\n    data['Cabin'].loc[data['Cabin'].str.contains('G')] ='G'\n     \n    return data\n\ntrain = clean_data(train)\ntest = clean_data(test)\n","e514fa04":"#show data after cleaning \ntrain","200ab6f6":"def engineer_features(data):\n    #create title column from splitting data in Name column\n    titles = []\n    for s in data['Name']:\n        id1 = s.index(\", \")\n        id2 = s.index(\".\")\n        res = s[id1 + len(\"\") + 1: id2]\n        titles.append(res)\n    data['title'] = titles\n    data['title'] = data['title'].str.lstrip()\n    replacement_dic = {\n        'Mlle':'Miss',\n        'Mme':'Miss', \n        'Master':'Other',\n        'Col':'Other', \n        'Major':'Other', \n        'Capt':'Other',\n        'Don':'Other',\n        'the Countess':'Other',\n        'Lady':'Other',\n        'Jonkheer':'Other',\n        'Sir':'Other',\n        'Rev':'Other'\n        }\n    data['title'] = data['title'].replace(replacement_dic, value=None)\n\n    #create family_size column by combineing SibSp and Parch columns\n    data['family_size'] = data['SibSp'] + data['Parch']\n\n    #create is_alone column from family_size column\n    data['is_alone'] = 1\n    data['is_alone'].loc[(data['family_size'] > 0 )] = 0\n\n    #remove columns\n    data = data.drop(['PassengerId', 'Name', 'Ticket', 'SibSp', 'Parch'], axis=1)\n\n    return data\n\ntrain = engineer_features(train)\ntest = engineer_features(test)","d5334a60":"#show data after cleaning and feature engineering\ntrain","063aa55e":"from sklearn.preprocessing import StandardScaler\nfrom sklearn.preprocessing import OrdinalEncoder\n\ndef data_preprocessing(data):\n\n    #group Age column in 10 year periods\n    data['Age'].loc[(data['Age'] <= 10)] = 0\n    data['Age'].loc[(data['Age'] > 10) & (data['Age'] <= 20)] = 1\n    data['Age'].loc[(data['Age'] > 20) & (data['Age'] <= 30)] = 2\n    data['Age'].loc[(data['Age'] > 30) & (data['Age'] <= 40)] = 3\n    data['Age'].loc[(data['Age'] > 40) & (data['Age'] <= 50)] = 4\n    data['Age'].loc[(data['Age'] > 50) & (data['Age'] <= 60)] = 5\n    data['Age'].loc[(data['Age'] > 60) & (data['Age'] <= 70)] = 6\n    data['Age'].loc[(data['Age'] > 70) & (data['Age'] <= 80)] = 7\n    data['Age'].loc[(data['Age'] > 80)] = 8\n\n    #group Fare column into 10 groups\n    data['Fare'].loc[data['Fare'] < 7.55 ] = 0 \n    data['Fare'].loc[(data['Fare'] < 7.854) & (data['Fare'] >= 7.55) ] = 1\n    data['Fare'].loc[(data['Fare'] < 8.05) & (data['Fare'] >= 7.854 ) ] = 2\n    data['Fare'].loc[(data['Fare'] < 10.5) & (data['Fare'] >= 8.05) ] = 3\n    data['Fare'].loc[(data['Fare'] < 14.454) & (data['Fare'] >= 10.5 ) ] = 4\n    data['Fare'].loc[(data['Fare'] < 21.679) & (data['Fare'] >= 14.454) ] = 5\n    data['Fare'].loc[(data['Fare'] < 27) & (data['Fare'] >= 21.679) ] = 6\n    data['Fare'].loc[(data['Fare'] < 39.688) & (data['Fare'] >= 27.0) ] = 7\n    data['Fare'].loc[(data['Fare'] < 77.958) & (data['Fare'] >= 39.688) ] = 8\n    data['Fare'].loc[(data['Fare'] >= 77.958) ] = 9\n    \n    #dummy encode Sex column, drop sex_female column\n    data = pd.get_dummies(data, columns=['Sex'], prefix=['sex_'], dtype=np.int64)\n    data = data.drop('sex__female', axis=1)\n\n    #ordinal encode title, Embarked and Cabin columns\n    ord_encoder = OrdinalEncoder(dtype=int)\n    ord_encoder.fit(data[['title','Embarked', 'Cabin']])\n    data[[ 'title','Embarked','Cabin']] = ord_encoder.transform(data[['title', 'Embarked', 'Cabin']])\n     \n    return data\n\ntrain = data_preprocessing(train)\ntest = data_preprocessing(test)","5a6e389b":"#show data after cleaning, feature engineering and preprocessing\ntrain","54d82fed":"#read in sklean libraries\nfrom sklearn.linear_model import LogisticRegression, SGDClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.naive_bayes import GaussianNB\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score","bc0233b5":"#split training data \nX = train.drop('Survived', axis=1)\ny = train['Survived']\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)","bdf69770":"#Logistic Regression\nlog_reg = LogisticRegression(solver='liblinear', max_iter=1000)\nlog_reg.fit(X_train, y_train)\nlog_reg_pred = log_reg.predict(X_test)\nlog_reg_acc = accuracy_score(y_test, log_reg_pred)\n\n#K Nearest Neighbors\nknn = KNeighborsClassifier(n_neighbors=19)\nknn.fit(X_train, y_train)\nknn_pred = knn.predict(X_test)\nknn_acc = accuracy_score(y_test, knn_pred)\n\n#Support Vector Machine\nsvm = SVC()\nsvm.fit(X_train, y_train)\nsvm_pred =svm.predict(X_test)\nsvm_acc = accuracy_score(y_test, svm_pred)\n\n#Random Forest\nrfc = RandomForestClassifier()\nrfc.fit(X_train, y_train)\nrfc_pred = rfc.predict(X_test)\nrfc_acc = accuracy_score(y_test, rfc_pred)\n\n#Guassian\nnb = GaussianNB()\nnb.fit(X_train, y_train)\nnb_pred = nb.predict(X_test)\nnb_acc = accuracy_score(y_test, nb_pred)\n\n#Gradient Descent\ngd = SGDClassifier()\ngd.fit(X_train, y_train)\ngd_pred = gd.predict(X_test)\ngd_acc = accuracy_score(y_test, gd_pred)","e310281e":"# run n_neighbors from 1-100\nerror_rate = []\nfor i in range(1,100):\n    \n    knn = KNeighborsClassifier(n_neighbors=i)\n    knn.fit(X_train,y_train)\n    pred_i = knn.predict(X_test)\n    error_rate.append(np.mean(pred_i != y_test))\n    \nplt.figure(figsize=(10,12))\nplt.plot(error_rate)\nplt.title('K-Value Optimization')\nplt.xlabel('K-Value')\nplt.ylabel('Error Rate')","54b34c81":"#print accuracy score for each model\nprint('Logistic Regression: ', '\\t', log_reg_acc)\nprint('KNN: ', '\\t\\t\\t', knn_acc)\nprint('SVM: ', '\\t\\t\\t', svm_acc)\nprint('Random Forest: ', '\\t', rfc_acc)\nprint('Naive Bayes: ', '\\t\\t', nb_acc)\nprint('Gradient Decent: ', '\\t', gd_acc)","b1fdbd56":"## 2. EDA","a2912486":"## 7. Results","239e3790":"## 4. Feature Engineering","f401cb51":"In 1912 the HMS Titanic famously sunk in the Atlantic Ocean after colliding with an iceburg. Over 1000 people lost their lives as a result of the Titanic sinking in the frigid waters. \n\nThis project is aimed at prediciting which passengers survivied based on data we know about each passenger. This data includes but is not limited to their sex, passenger class, cost of their ticket, name, etc. \n\n#### **Model Results**\n- On training data: **81-83%**\n- Competition Submission:       **78.7%**     \n\n\n","2e7de673":"### 2.2 Plots ","f7fa0ceb":"- The model is ~2-5% more accurate when tested against training data than compared to the test data\n- Scored in the middle of the pack of competition particpants\n- Improvements:\n- - Use classification to predict null Cabin data\n- - Change how Age and Fare columns are grouped","fb3b68a3":"- Non-Survivor trends:\n- - 3rd Class (Pclass=3)\n- - Male\n- - Low Fare price\n\n- General Trends:\n- - Majority of passengers have No Sibling\/Spouse or Parents onboard\n- - Southhampton is the most popular embarked location\n- - Most 3rd class passengers embarked from Southhampton\n- - Almost no 1st class passengers embarked from Queensland\n- - 3rd class is almost half the distribution of passengers\n- - Majority of ticket cost fall between 0-100 dollars\n- - Age is somewhat normally distributed, with a large number of children under 5 years old","de814aa6":"### 2.1 Basic EDA","34027244":"## 5. Data Preprocessing","f36eafa8":"### 2.3 EDA Findings","6eedc80e":"- Is there a negative correlation between being in 3rd class or Male and survival?\n- Is the ticket number releveant or arbitrary?\n- - Online research may need to be done\n- Assume cabin data would be a good indicator of survival but it looks like a lot of null values exist\n- See if correlation between last name and survival exists","807add5d":"## 8. Conclusion","e0014586":"## 1. Planning","d4d82925":"## Introduction","0e1d63f7":"## 3. Data Cleaning","245da92d":"### 6.1 K-value Optimization","d73b3978":"1. Planning \n2. Exploratory Data Analysis (EDA)\n3. Data Cleaning\n4. Feature Engineering\n5. Data Preprocessing\n6. Modeling\n7. Results\n8. Conclusion","fa2d158c":"## 6. Modeling","25fb8381":"## Overview"}}