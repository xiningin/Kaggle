{"cell_type":{"dcb6f923":"code","d53c361e":"code","8364ef9d":"code","9b4ed147":"code","8d303cb7":"code","561c3168":"code","dd6e646c":"code","34f1aca5":"code","1003d313":"code","ab8ab04e":"code","64bb17c4":"code","b1debeab":"code","4498e7e9":"code","1ca99685":"code","2214e026":"code","b25ca770":"code","39dc0286":"code","03819eb0":"code","28dfc7ce":"code","04422526":"code","45390be3":"code","7e3b71a0":"code","a2903b47":"code","55e54132":"code","4a82680f":"code","cc906cc1":"code","43853144":"code","3dee1672":"code","72c9d7bf":"code","639cd1c6":"code","7eb6493f":"code","4abf8ca2":"code","1d2f5da7":"code","d3d67b1b":"code","0430d885":"code","febdf7ab":"code","e92cf2e4":"code","a2ad3401":"code","56635da5":"code","b9e00142":"code","4827b215":"code","6d607834":"code","042ee1a6":"code","d20e4282":"code","30e774c4":"code","b94ee145":"code","057d0424":"code","b92ebe19":"code","f3e41427":"code","dbe158d7":"code","57340b7f":"code","f209545e":"code","8e9be5c6":"code","12aa045e":"code","7342c9b7":"markdown","e84fd589":"markdown","a8033ce7":"markdown","8a2a9f1a":"markdown","c5665ef6":"markdown","e10d76f9":"markdown","07f3dc79":"markdown","9ad8306e":"markdown","60fe138b":"markdown","595dee60":"markdown","5a1f0fc5":"markdown","aadbab26":"markdown","9f74ef18":"markdown","47797e7e":"markdown","7988f267":"markdown","ced277fe":"markdown","afe87221":"markdown","4459c18d":"markdown","a176cfb7":"markdown","792727e0":"markdown","e8ccebde":"markdown","a88265ba":"markdown","2367add1":"markdown","dac8a130":"markdown","bff1345f":"markdown","d5fd2fa4":"markdown","9888a30a":"markdown","33e6bd3f":"markdown","442d1fc0":"markdown","f2db644b":"markdown","a0fd7767":"markdown","3445ba40":"markdown","5f568abc":"markdown"},"source":{"dcb6f923":"import pandas as pd\nimport numpy as np\nimport plotly.graph_objects as go\nimport plotly.express as px\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.tree import DecisionTreeClassifier \nfrom sklearn.model_selection import train_test_split \nfrom sklearn import tree\nfrom sklearn.preprocessing import OneHotEncoder, LabelEncoder\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import roc_curve, auc\n%matplotlib inline","d53c361e":"bank = pd.read_csv('..\/input\/bank-marketing-dataset\/bank.csv')","8364ef9d":"bank.head()","9b4ed147":"bank.info()","8d303cb7":"bank.describe()","561c3168":"bank.shape","dd6e646c":"# Checking if null values are present in the dataset\nbank.isnull().sum()","34f1aca5":"# Check if the dataset is balanced or not\nplt.figure(figsize = (8,6))\ntotal = len(bank[\"deposit\"])\nax = sns.countplot(x = 'deposit', data = bank)\nfor p in ax.patches:\n    ax.annotate('{:.1f}%'.format(100*p.get_height()\/total), (p.get_x()+0.1, p.get_height()+5))\nplt.show()","1003d313":"plt.figure(figsize = (14,12))\ntotal = len(bank[\"job\"])\nax = sns.countplot(x = 'job', data = bank, hue = 'deposit', palette = 'Set2')\nfor p in ax.patches:\n    ax.annotate('{:.1f}%'.format(100*p.get_height()\/total), (p.get_x()+0.1, p.get_height()+5))\nplt.show()","ab8ab04e":"plt.figure(figsize = (8,6))\ntotal = len(bank[\"marital\"])\nax = sns.countplot(x = 'marital', data = bank, hue = 'deposit', palette = 'Set1')\nfor p in ax.patches:\n    ax.annotate('{:.1f}%'.format(100*p.get_height()\/total), (p.get_x()+0.1, p.get_height()+5))\nplt.show()","64bb17c4":"plt.figure(figsize = (8,6))\ntotal = len(bank[\"education\"])\nax = sns.countplot(x = 'education', data = bank, hue = 'deposit')\nfor p in ax.patches:\n    ax.annotate('{:.1f}%'.format(100*p.get_height()\/total), (p.get_x()+0.1, p.get_height()+5))\nplt.show()","b1debeab":"plt.figure(figsize = (8,6))\ntotal = len(bank[\"default\"])\nsns.set_palette(\"Paired\")\nax = sns.countplot(x = 'default', data = bank, hue = 'deposit')\nfor p in ax.patches:\n    ax.annotate('{:.1f}%'.format(100*p.get_height()\/total), (p.get_x()+0.1, p.get_height()+5))\nplt.show()","4498e7e9":"plt.figure(figsize = (8,6))\ntotal = len(bank[\"housing\"])\nax = sns.countplot(x = 'housing', data = bank, hue = 'deposit')\nfor p in ax.patches:\n    ax.annotate('{:.1f}%'.format(100*p.get_height()\/total), (p.get_x()+0.1, p.get_height()+5))\nplt.show()","1ca99685":"plt.figure(figsize = (8,6))\ntotal = len(bank[\"loan\"])\nax = sns.countplot(x = 'loan', data = bank, hue = 'deposit', palette = 'Set1')\nfor p in ax.patches:\n    ax.annotate('{:.1f}%'.format(100*p.get_height()\/total), (p.get_x()+0.1, p.get_height()+5))\nplt.show()","2214e026":"plt.figure(figsize = (8,6))\ntotal = len(bank[\"poutcome\"])\nax = sns.countplot(x = 'poutcome', data = bank, hue = 'deposit', palette='Set2')\nfor p in ax.patches:\n    ax.annotate('{:.1f}%'.format(100*p.get_height()\/total), (p.get_x()+0.1, p.get_height()+5))\nplt.show()","b25ca770":"fig = px.histogram(bank, x = \"day\", y = \"day\", color = \"deposit\")\nfig.show()","39dc0286":"plt.figure(figsize = (16,14))\ntotal = len(bank[\"month\"])\nax = sns.countplot(x = 'month', data = bank, hue = 'deposit', palette='Set1')\nfor p in ax.patches:\n    ax.annotate('{:.1f}%'.format(100*p.get_height()\/total), (p.get_x()+0.1, p.get_height()+5))\nplt.show()","03819eb0":"fig = px.box(x = bank[\"deposit\"], y = bank[\"age\"])\nfig.show()","28dfc7ce":"plt.figure(figsize = (10,8))\nsns.distplot(bank[\"age\"])","04422526":"fig = px.box(x = bank[\"deposit\"], y = bank[\"balance\"])\nfig.show()","45390be3":"fig = px.box(x = bank[\"deposit\"], y = bank[\"duration\"])\nfig.show()","7e3b71a0":"plt.figure(figsize = (10,8))\nsns.distplot(bank[\"duration\"])","a2903b47":"fig = px.box(x = bank[\"deposit\"], y = bank[\"campaign\"])\nfig.show()","55e54132":"fig = px.violin(bank, x=bank[\"pdays\"], y=bank[\"deposit\"], color = bank[\"deposit\"])\nfig.show()","4a82680f":"fig = px.histogram(bank, x=\"pdays\", color = bank[\"deposit\"], nbins = 15)\nfig.show()","cc906cc1":"fig = px.histogram(bank, x=\"previous\", color = \"deposit\", nbins = 10)\nfig.show()","43853144":"plt.figure(figsize = (10,8))\ncorr = bank.corr()\nsns.heatmap(corr, annot = True, cmap = \"Greens\")\nplt.title(\"Pearson Correlation of Features\", size = 15)","3dee1672":"bank[bank.duplicated()].sum()","72c9d7bf":"# Remove contact column as it does not affect much in predicting target feature\nbank = bank.drop(columns = \"contact\", axis = 1)\nbank.head()","639cd1c6":"from sklearn.preprocessing import LabelEncoder\nlabelEncoder = LabelEncoder()\nbank['deposit'] = labelEncoder.fit_transform(bank['deposit'])\nbank['marital'] = labelEncoder.fit_transform(bank['marital'])\nbank['education'] = labelEncoder.fit_transform(bank['education'])\nbank['default'] = labelEncoder.fit_transform(bank['default'])\nbank['housing'] = labelEncoder.fit_transform(bank['housing'])\nbank['loan'] = labelEncoder.fit_transform(bank['loan'])\nbank['poutcome'] = labelEncoder.fit_transform(bank['poutcome'])\n","7eb6493f":"bank[\"month\"].value_counts().sort_values(ascending = False).head(20)","4abf8ca2":"bank[\"job\"].value_counts().sort_values(ascending = False).head(20)","1d2f5da7":"# Get whole set of dummy variables for all categorical columns\ndef one_hot_encode_top(df, col, top_6_labels):\n    for label in top_6_labels:\n        df[col+'_'+label] = np.where(df[col] == label,1,0)\nbank_new = pd.read_csv('..\/input\/bank-marketing-dataset\/bank.csv', usecols = ['job', 'month']) ","d3d67b1b":"# Encode for job column\ntop_6_labels = [x for x in bank[\"job\"].value_counts().sort_values(ascending = False).head(6).index]\none_hot_encode_top(bank, \"job\", top_6_labels)\nbank.head()","0430d885":"# Encode for month column\ntop_6_labels = [x for x in bank[\"month\"].value_counts().sort_values(ascending = False).head(6).index]\none_hot_encode_top(bank, \"month\", top_6_labels)\nbank.head()","febdf7ab":"# Drop initial job and month categorical column\nbank.drop(columns = [\"job\", \"month\"], axis =1, inplace = True)\nbank.head()","e92cf2e4":"bank.shape","a2ad3401":"bank.info()","56635da5":"# Rearranging columns\nbank = bank[[\"age\", \"balance\", \"day\", \"campaign\", \"duration\", \"pdays\", \"previous\", \"marital\", \"education\", \n             \"default\", \"housing\", \"loan\", \"poutcome\", \"job_management\", \"job_blue-collar\", \"job_technician\",\n             \"job_admin.\", \"job_services\", \"job_retired\", \"month_may\", \"month_aug\", \"month_jul\", \"month_jun\", \"month_nov\",\n             \"month_apr\", \"deposit\"]]","b9e00142":"X = bank.iloc[:, :25]\ny = bank.iloc[:, -1]","4827b215":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2)\nprint(\"X_train: \", X_train.shape)\nprint(\"X_test: \", X_test.shape)\nprint(\"y_train: \", y_train.shape)\nprint(\"y_test: \", y_test.shape)","6d607834":"clf = DecisionTreeClassifier(random_state=0)\nclf.fit(X_train, y_train)\ny_pred_train = clf.predict(X_train)\ny_pred_test = clf.predict(X_test)\nacc_train = accuracy_score(y_train, y_pred_train)\nacc_test = accuracy_score(y_test, y_pred_test)\nprint(\"Accuracy score of training data: \", acc_train)\nprint(\"Accuracy score of test data: \", acc_test)","042ee1a6":"plt.figure(figsize = (18, 16))\ntree.plot_tree(clf)\nplt.show()","d20e4282":"# Using cost_complexity_pruning technique to prune the branches of decision tree\npath=clf.cost_complexity_pruning_path(X_train,y_train)\n#path variable gives two things ccp_alphas and impurities\nccp_alphas,impurities=path.ccp_alphas,path.impurities\nprint(\"ccp alpha values :\",ccp_alphas)\nprint()\nprint(\"Impurities in Decision Tree :\",impurities)","30e774c4":"# Taking ccp_alphas as one of the parameter in DecisionTreeClassifier()\nclfs=[]   #will store all the models here\nfor ccp_alpha in ccp_alphas:\n    clf=DecisionTreeClassifier(random_state=0,ccp_alpha=ccp_alpha)\n    clf.fit(X_train,y_train)\n    clfs.append(clf)\nprint(\"Last node in Decision tree is {} and ccp_alpha for last node is {}\".format(clfs[-1].tree_.node_count,ccp_alphas[-1]))","b94ee145":"# Visualizing the accuracy score for train and test set.\ntrain_scores = [clf.score(X_train, y_train) for clf in clfs]\ntest_scores = [clf.score(X_test, y_test) for clf in clfs]\nfig, ax = plt.subplots()\nax.set_xlabel(\"alpha\")\nax.set_ylabel(\"accuracy\")\nax.set_title(\"Accuracy vs alpha for training and testing sets\")\nax.plot(ccp_alphas, train_scores, marker='o', label=\"train\",drawstyle=\"steps-post\")\nax.plot(ccp_alphas, test_scores, marker='o', label=\"test\",drawstyle=\"steps-post\")\nax.legend()\nplt.show()","057d0424":"clf=DecisionTreeClassifier(random_state=0,ccp_alpha=0.001)\nclf.fit(X_train,y_train)\nplt.figure(figsize=(14,10))\ntree.plot_tree(clf,rounded=True,filled=True)\nplt.show()","b92ebe19":"accuracy_score(y_test,clf.predict(X_test))","f3e41427":"y_score = clf.predict_proba(X_test)[:, 1]\nfpr, tpr, thresholds = roc_curve(y_test, y_score)\n\nfig = px.area(\nx=fpr, y=tpr,\ntitle=f'ROC Curve (Auc = {auc(fpr, tpr):.4f})',\nlabels=dict(x = 'False Positive Rate', y = 'True Positive Rate'),\nwidth = 700, height = 500)\nfig.add_shape(\ntype = 'line', line = dict(dash='dash'),\nx0=0, x1=1, y0=0, y1=1)","dbe158d7":"# Using hyperparameter tuning\nfrom sklearn.model_selection import GridSearchCV\n#from sklearn.grid_search import GridSearchCV\ngrid_param={\"criterion\":[\"gini\",\"entropy\"],\n             \"splitter\":[\"best\",\"random\"],\n             \"max_depth\":range(5,15,1),\n             \"min_samples_leaf\":range(5,15,1),\n             \"min_samples_split\":range(5,15,1) \n            }\ngrid_search=GridSearchCV(estimator=clf,param_grid=grid_param,cv=5,n_jobs=-1)\ngrid_search.fit(X_train,y_train)","57340b7f":"print(grid_search.best_params_)","f209545e":"clf=DecisionTreeClassifier(criterion = 'gini', max_depth = 4, min_samples_leaf = 1, min_samples_split = 2, splitter = 'best')\nclf.fit(X_train,y_train)\nplt.figure(figsize=(20,12))\ntree.plot_tree(clf,rounded=True,filled=True)\nplt.show()","8e9be5c6":"y_pred = clf.predict(X_test)\naccuracy_score(y_test,clf.predict(X_test))","12aa045e":"y_score = clf.predict_proba(X_test)[:, 1]\nfpr, tpr, thresholds = roc_curve(y_test, y_score)\n\nfig = px.area(\nx=fpr, y=tpr,\ntitle=f'ROC Curve (Auc = {auc(fpr, tpr):.4f})',\nlabels=dict(x = 'False Positive Rate', y = 'True Positive Rate'),\nwidth = 700, height = 500)\nfig.add_shape(\ntype = 'line', line = dict(dash='dash'),\nx0=0, x1=1, y0=0, y1=1)","7342c9b7":"## Univariate Analaysis on Numerical features","e84fd589":"<p>Number of contacts performed before this campaign and for this client or previous feature is not a good indicator of target variable.<\/p>","a8033ce7":"### Post Pruning Operation","8a2a9f1a":"<p> People who have not taken loan are more in number.Therefore people who have not taken loan are more likely to\n    subscribe to term deposit.<\/p>","c5665ef6":"<p>We can see that number of days passed by after the client was last contacted from a previous campaign (numeric; 999 means client was not previously contacted)is between 3 - 6 months then there is a high chance of client subscribing to term deposit.If the client is contacted many days ago or not contacted at all then the client subscribing to term deposit is likely less.Therefore to some extent this feature will help in determing target feature.<\/p>","e10d76f9":"<p>From the above plot we can see that, the duration (last contact duration) of a customer can be useful for predicting the target variable. It is expected because it is already mentioned in the data overview that this field highely affects the target variable and should only be used for benchmark purposes.<\/p>","07f3dc79":"auc = 0.87 which means there is a 87% chance that our model will be able to distinguish between positive class and negative class.","9ad8306e":"<p>we know that we will not be able to include this feature in our final models, as obviously we want to create a realistic predictive model that can be used by the business. But, we will surely implement a basic model with the duration feature just to see how much of an impact this feature makes. So with that, let\u2019s look into the box plot and pdf of this feature.<\/p>","60fe138b":"<p> All categorical columns are converted into numeric columns<\/p>","595dee60":"<p>We can infer that, number of contacts performed during this campaign for a client does not contribute much in predicting wether he will subscribe or not subscribe to term deposit.","5a1f0fc5":"<p>From the above boxplot we know that for both the customers that subscibed or didn\u2019t subscribe a term deposit, has a median age of around 38\u201339. And the boxplot for both the classes overlap quite a lot, which means that age isn\u2019t necessarily a good indicator for which customer will subscribe and which customer will not.<\/p>","aadbab26":"auc = 0.86 which means there is a 86% chance that our model will be able to distinguish between positive class and negative class.","9f74ef18":"#### Check if there are any duplicate values","47797e7e":"### Splitting dataset into train and test","7988f267":"### Visualizing Decision Tree","ced277fe":"<p> Customers who have a job of management has higher rate of subscribing to term deposit, but they are also the \n highest when it comes to not subscribing. This is simply because we have more customers working as \n management than any other profession.<\/p>\n","afe87221":"bank client data:\n1. age (numeric)\n2. job : type of job (categorical: \u2018admin.\u2019,\u2019blue-collar\u2019,\u2019entrepreneur\u2019,\u2019housemaid\u2019,\u2019management\u2019,\u2019retired\u2019,\u2019self-employed\u2019,\u2019services\u2019,\u2019student\u2019,\u2019technician\u2019,\u2019unemployed\u2019,\u2019unknown\u2019)\n3. marital : marital status (categorical: \u2018divorced\u2019,\u2019married\u2019,\u2019single\u2019,\u2019unknown\u2019; note: \u2018divorced\u2019 means divorced or widowed)\n4. education (categorical: \u2018basic.4y\u2019,\u2019basic.6y\u2019,\u2019basic.9y\u2019,\u2019high.school\u2019,\u2019illiterate\u2019,\u2019professional.course\u2019,\u2019university.degree\u2019,\u2019unknown\u2019)\n5. default: has credit in default? (categorical: \u2018no\u2019,\u2019yes\u2019,\u2019unknown\u2019)\n6. balance (numeric)\n7. housing: has housing loan? (categorical: \u2018no\u2019,\u2019yes\u2019,\u2019unknown\u2019)\n8. loan: has personal loan? (categorical: \u2018no\u2019,\u2019yes\u2019,\u2019unknown\u2019)\nRelated with the last contact of the current campaign:\n9. contact: contact communication type (categorical: \u2018cellular\u2019,\u2019telephone\u2019)\n10. month: last contact month of year (categorical: \u2018jan\u2019, \u2018feb\u2019, \u2018mar\u2019, \u2026, \u2018nov\u2019, \u2018dec\u2019)\n11. day_of_week: last contact day of the week (categorical: \u2018mon\u2019,\u2019tue\u2019,\u2019wed\u2019,\u2019thu\u2019,\u2019fri\u2019)\n12. duration: last contact duration, in seconds (numeric). Important note: this attribute highly affects the output target (e.g., if duration=0 then y=\u2019no\u2019). Yet, the duration is not known before a call is performed. Also, after the end of the call y is obviously known. Thus, this input should only be included for benchmark purposes and should be discarded if the intention is to have a realistic predictive model.\nother attributes:\n13. campaign: number of contacts performed during this campaign and for this client (numeric, includes last contact)\n14. pdays: number of days that passed by after the client was last contacted from a previous campaign (numeric; 999 means client was not previously contacted)\n15. previous: number of contacts performed before this campaign and for this client (numeric)\n16. poutcome: outcome of the previous marketing campaign (categorical: \u2018failure\u2019,\u2019nonexistent\u2019,\u2019success\u2019)\n17. deposit: yes, no - Dependent variable","4459c18d":"Here we are able to prune infinitely grown tree.Let\u2019s check the accuracy score again","a176cfb7":"<p>From the above plot we can see that it is a uniform distribution plot.Thus, we can conclude that this feature will not be very helpful in predicting the target variable.<\/p> ","792727e0":"<p> There is a uniform distribution among customers who do no have credit in default.Therefore this feature contribute less in predecting weather a customer will subscribe to term deposit or not.<\/p> ","e8ccebde":"<p> People who have a secondary education subscribe less to term deposit followed by tertiary, unknown and primary education.<\/p> ","a88265ba":"If we folow bias and variance tradeoff we will choose that point which will have low bias(low training error) and low variance(low test error).Here we get that point at a value of alpha=0.001.","2367add1":"<p> In the month of May followed by August, July and April more number of people subscribe to term deposit.<\/p>","dac8a130":"<p> Pdays and Previous feature have high correlation of 0.51.<\/p>","bff1345f":"<p> From the above plot we can say that the dataset is almost balanced.<\/p>","d5fd2fa4":"<p>Duration attribute highly affects the target attribute(e.g., if duration=0 then y=\u2019no\u2019). Yet, the duration is not known before a call is performed. Also, after the end of the call, the target variable y is obviously known. Thus, this input should only be included for benchmark purposes and should be discarded if the intention is to have a realistic predictive model.<\/p>","9888a30a":"<p> Customers who have housing loan has lower rate of subscribing to term deposit.<\/p>","33e6bd3f":"### Data Preprocessing","442d1fc0":"<p> Majority of the customers are married followed by single and divorced.<\/p>","f2db644b":"## Univariate Analysis of Categorical Variables","a0fd7767":"<p> Similar to age feature even balance feature doesn't contribute much to target feature.<\/p>","3445ba40":"<p> Majority of the people who have subscribed to term deposit does not have previous marketing outcome which means \nthat they are new customers.Also, customers who had a successful outcome from the previous campaign, majority of those\ncustomers did subscribe for a term deposit.From this, we can make an assumption, that this feature may hold some value in predicting the target variable, specially poutcome_success category.<\/p>","5f568abc":"### Pre - Pruning"}}