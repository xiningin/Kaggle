{"cell_type":{"791934aa":"code","f6c97ad1":"code","91eefb17":"code","3f5265a8":"code","8e3c0fa8":"code","926a85b4":"code","1dd01d06":"code","54061a15":"code","8c683266":"code","81dfb0e4":"code","cd8d00fd":"code","429c9738":"code","11a0c134":"code","ef319061":"markdown","007c49d6":"markdown","e4376f24":"markdown","a1528072":"markdown","21fca2ee":"markdown","40a14277":"markdown","c354dff1":"markdown"},"source":{"791934aa":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","f6c97ad1":"# Reading the data\ntest = pd.read_csv(\"\/kaggle\/input\/digit-recognizer\/test.csv\")\ntrain = pd.read_csv(\"\/kaggle\/input\/digit-recognizer\/train.csv\")\nx_train = train.iloc[:,1:]\ny_train = train.iloc[:,:1]","91eefb17":"# Normalizing the pixels values\nx_train = x_train\/255.0\ntest = test\/255.0\n\n# Bringing dimensions in Keras expected form (examples, hx, hy, channels)\nx_train = x_train.to_numpy().reshape(x_train.shape[0],28,28,1)\ntest = test.to_numpy().reshape(test.shape[0],28,28,1)\n\nprint(\"Train Data Shape is \", x_train.shape)\nprint(\"Train Label data Shape is \", y_train.shape)\nprint(\"Test Data Shape is \", test.shape)","3f5265a8":"import tensorflow as tf\nfrom tensorflow import keras\n\nmodel = keras.Sequential([\n    keras.layers.Conv2D(input_shape =(28,28,1), filters=16, kernel_size=(3,3), activation = 'relu',\n                        strides=(1,1), data_format = \"channels_last\",),\n    \n    keras.layers.Conv2D(filters=32, kernel_size=(3,3), activation = 'relu',\n                        strides=(1,1), data_format = \"channels_last\"),\n    keras.layers.MaxPool2D(2,2),\n    \n    keras.layers.Conv2D(filters=64, kernel_size=(3,3), padding = 'same', activation = 'relu',\n                        strides=(1,1), data_format = \"channels_last\"),\n    keras.layers.Dropout(0.2),\n    \n    keras.layers.Conv2D(filters=128, kernel_size=(3,3), padding = 'same', activation = 'relu',\n                        strides=(1,1), data_format = \"channels_last\"),\n    keras.layers.MaxPool2D(2,2),\n    keras.layers.Dropout(0.3),\n    \n    keras.layers.Conv2D(filters=256, kernel_size=(3,3), activation = 'relu',\n                        strides=(1,1), data_format = \"channels_last\"),\n    keras.layers.Dropout(0.3),\n    keras.layers.BatchNormalization(),\n    \n#     keras.layers.Conv2D(filters=256, kernel_size=(3,3), \n#                         strides=(1,1), data_format = \"channels_last\"),\n#     keras.layers.BatchNormalization(),\n    \n    keras.layers.Flatten(),\n    \n    keras.layers.Dense(128, activation=\"relu\"),\n    keras.layers.Dense(10, activation=\"softmax\")\n])","8e3c0fa8":"from tensorflow.keras.optimizers import Adam\nadm = Adam(learning_rate = 0.002)\nmodel.compile(optimizer='adam',\n              loss='sparse_categorical_crossentropy',\n              metrics=['accuracy'])","926a85b4":"model.summary()","1dd01d06":"from keras.preprocessing.image import ImageDataGenerator\ndata_gen = ImageDataGenerator(\n    rotation_range=20,\n    zoom_range = 0.1,  \n    width_shift_range=0.1, \n    height_shift_range=0.1\n)\n\ndata_gen.fit(x_train)\n\n","54061a15":"len(x_train)","8c683266":"from keras.callbacks import ReduceLROnPlateau\nreduce_LR = ReduceLROnPlateau(monitor='acc', \n                                            patience=2, \n                                            verbose=1, \n                                            factor=0.5, \n                                            min_lr=0.00005)","81dfb0e4":"history = model.fit_generator(data_gen.flow(x_train,y_train, batch_size=64),\n                    epochs = 50, \n                    verbose = 1,\n                    callbacks = [reduce_LR])\n# model.fit(x_train, y_train, epochs=10)","cd8d00fd":"import matplotlib.pyplot as plt\n\n# Plot training & validation accuracy values\nplt.plot(history.history['acc'])\nplt.title('Model accuracy')\nplt.ylabel('Accuracy')\nplt.xlabel('Epoch')\nplt.legend(['Train', 'Test'], loc='upper left')\nplt.show()\n\n# Plot training & validation loss values\nplt.plot(history.history['loss'])\nplt.title('Model loss')\nplt.ylabel('Loss')\nplt.xlabel('Epoch')\nplt.legend(['Train', 'Test'], loc='upper left')\nplt.show()","429c9738":"predictions = model.predict(test)\n\n# select the indix with the maximum probability\npredictions = np.argmax(predictions,axis = 1)\n\npredictions = pd.Series(predictions,name=\"Label\")","11a0c134":"submission = pd.concat([pd.Series(range(1,28001),name = \"ImageId\"), predictions],axis = 1)\n\nsubmission.to_csv(\"cnn_mnist.csv\",index=False)","ef319061":"## Compiling the Model","007c49d6":"## Normalizing and Formatting the Data","e4376f24":"## Fitting the Model on Generator","a1528072":"## Data Augmentation\ncreating ImageDataGenerator Object and fitting this to the original data to augment it.","21fca2ee":"## Reading the Data","40a14277":"## Reducing Learning rate on Plateau","c354dff1":"## Defining the Model Architecture"}}