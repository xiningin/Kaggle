{"cell_type":{"a348fa8f":"code","e707bfbc":"code","761d293c":"code","4ad8e3d4":"code","b528e938":"code","bdc1fd74":"code","e44b7fbf":"code","d9676131":"code","6ce8a9e7":"code","ecf5b5d5":"code","ff6a0b52":"code","a50c9608":"code","271fe1bf":"code","85d981d3":"code","387aceb7":"code","1bab2142":"code","f4cb1b49":"code","7073e798":"code","87d62266":"code","2345b0b2":"code","ae697d96":"code","7343e653":"code","310a242f":"code","3dbec391":"code","01baec4e":"code","7b9ccae9":"code","24e41252":"code","28221ed3":"code","4088628e":"code","30eeb677":"code","15c8b271":"code","5967e104":"code","2ebfb29e":"code","db15100a":"code","18aafc2b":"code","c2a431a5":"markdown","ea798894":"markdown","5724c25e":"markdown","11bae774":"markdown","03243f89":"markdown","4643039f":"markdown","a3626ee5":"markdown","459c501b":"markdown","65b10976":"markdown","017fbb48":"markdown","aee89ce6":"markdown","faf0464d":"markdown","be2225a1":"markdown","7e3ffd26":"markdown","03b3c52f":"markdown","6c2521f0":"markdown","708a5eb9":"markdown","6554d438":"markdown","0adc9191":"markdown","fb8ec387":"markdown","e887e4db":"markdown","9629e12d":"markdown","e5cbe5d5":"markdown"},"source":{"a348fa8f":"rm(list = ls())\n\nlibrary(caret)\nlibrary(doParallel)\nlibrary(rpart.plot)\n","e707bfbc":"data = read.csv(\"..\/input\/titanic\/train.csv\")\n\n","761d293c":"colsToDelete = c(\"Name\", \"Cabin\", \"Ticket\", \"PassengerId\")\n\nfor (i in colsToDelete) {\n  data = data[, -which(names(data) == i)]\n}\n\nrm(list = setdiff(ls(), c(\"data\")))\n","4ad8e3d4":"toFactor = c(\"Survived\", \"Sex\", \"Embarked\")\n\nfor (i in toFactor) {\n  data[, which(names(data) == i)] = factor(data[, which(names(data) == i)])\n}\n\nlevels(data$Survived) = c(\"no\", \"yes\")\n","b528e938":"length(which(!complete.cases(data)))\n\n","bdc1fd74":"hist(rowMeans(is.na(data)), xlab = c(\"Missing values average by rows\"), main = c())\n\n","e44b7fbf":"indexesEmptyCols = which(colMeans(is.na(data)) != 0)\nprint(indexesEmptyCols)\n","d9676131":"data = data[-which(is.na(data[, indexesEmptyCols]) == T), ]\n\n","6ce8a9e7":"length(which(!complete.cases(data)))\n\n","ecf5b5d5":"prop.table(table(data$Embarked))\n\n","ff6a0b52":"indexEmpty = which(data$Embarked == \"\")\ndata = data[-indexEmpty, ]\n\nlevels(data$Embarked) = droplevels(data$Embarked)\n","a50c9608":"prop.table(table(data$Embarked))\n\n","271fe1bf":"rm(list = setdiff(ls(), c(\"data\")))\n\n","85d981d3":"indexTest = createDataPartition(data$Survived, p = 0.15, list = FALSE)\n\ntest = data[indexTest, ]\ntrain = data[-indexTest, ]\n","387aceb7":"rm(list = setdiff(ls(), c(\"data\", \"train\", \"test\")))\n\n","1bab2142":"getPrediction = function(model, test) {\n  pred = predict(model, test[, -1])\n  \n  confMatrix = confusionMatrix(pred, test[, 1])\n  return(confMatrix)\n}\n\nfitModel = function(formula, data, method, family = NULL, grid = NULL, test = NULL, preProcess = NULL) {\n  \n  env <- foreach:::.foreachGlobals # remove previous parallel computing instances.\n  rm(list=ls(name=env), pos=env)\n  \n  trainControl = trainControl(method=\"repeatedcv\", number = 5, repeats = 3,\n                                savePredictions=TRUE, classProbs=TRUE)\n  \n  cores = detectCores() - 1\n  cl = makeCluster(cores)\n  registerDoParallel(cl)\n  \n  if (is.null(family)) {\n    model = train(formula, data, method = method, trControl = trainControl, tuneGrid = grid, preProcess = preProcess)\n  } else {\n    model = train(formula, data, method = method, trControl = trainControl, tuneGrid = grid, family = family, preProcess = preProcess)\n  }\n  \n  stopCluster(cl)\n  \n  confMatrix = getPrediction(model, test)\n  \n  print(confMatrix)\n\n  \n  return(list(model, confMatrix))\n}\n","f4cb1b49":"dt_grid = expand.grid(cp = seq(0.2, 2, 0.2))\n\nmodel.dt = fitModel(Survived ~ ., train, \"rpart\", grid = dt_grid, test = test)\n","7073e798":"rpart.plot(model.dt[[1]]$finalModel, digits = 3)\n\n","87d62266":"model.lr = fitModel(Survived ~ ., train, \"glm\", test = test, family = \"binomial\")\n\n","2345b0b2":"plot(varImp(model.lr[[1]], scale = F), scales = list(y = list(cex = .95)))\n\n","ae697d96":"model.lda = fitModel(Survived ~ . - Embarked, train, \"lda\", test = test)\n\n","7343e653":"model.qda = fitModel(Survived ~ . - Embarked, train, \"qda\", test = test)\n\n","310a242f":"knn_grid = expand.grid(k = seq(2, 50, 4))\n\nmodel.knn = fitModel(Survived ~ ., train, \"knn\", test = test, grid = knn_grid, preProcess = c(\"center\", \"scale\"))\n","3dbec391":"model.plr = fitModel(Survived ~ . , train, \"plr\", test = test)\n\n","01baec4e":"rf_grid = expand.grid(mtry = seq(1, 7, 1))\n\nmodel.rf = fitModel(Survived ~ ., train, \"rf\", test = test, grid = rf_grid, preProcess = c(\"center\", \"scale\"))\n","7b9ccae9":"svm_grid = expand.grid(sigma = seq(0.6, 0.8, 0.05),\n                       C = seq(0.4, 0.6, 0.08))\n\nmodel.svm = fitModel(Survived ~ ., train, \"svmRadial\", test = test, grid = svm_grid, preProcess = c(\"center\", \"scale\"))\n","24e41252":"spline_grid = expand.grid(df = seq(1, 6, 1))\n\nmodel.spline = fitModel(Survived ~ ., train, \"gamSpline\", test = test, grid = spline_grid, preProcess = c(\"center\", \"scale\"))\n","28221ed3":"data.test = read.csv(\"..\/input\/titanic\/test.csv\")\n\n","4088628e":"colsToDelete = c(\"Name\", \"Cabin\", \"Ticket\")\n\nfor (i in colsToDelete) {\n  data.test = data.test[, -which(names(data.test) == i)]\n}\n","30eeb677":"toFactor = c(\"Sex\", \"Embarked\")\n\nfor (i in toFactor) {\n  data.test[, which(names(data.test) == i)] = factor(data.test[, which(names(data.test) == i)])\n}\n","15c8b271":"colsToFill = c(\"Age\", \"Fare\")\n\nfor (i in colsToFill) {\n  data = data.test[, which(names(data.test) == i)]\n  indexToFill = which(is.na(data))\n  dataToFill = median(data[-indexToFill])\n  data.test[indexToFill, which(names(data.test) == i)] = dataToFill\n}\n","5967e104":"mode = function(v) {\n  uniqv = unique(v)\n  uniqv[which.max(tabulate(match(v, uniqv)))]\n}\n","2ebfb29e":"finalPrediction = function(data) {\n  pred.knn = predict(model.knn[[1]], newdata = data)\n  pred.qda = predict(model.qda[[1]], newdata = data)\n  pred.rf = predict(model.rf[[1]], newdata = data)\n\n  predictions = apply(data.frame(pred.knn, pred.qda, pred.rf), 1, mode)\n    \n  finalPred = data.frame(PassengerId = data$PassengerId,\n                         Survived = predictions)\n  return(finalPred)\n}\n","db15100a":"prediction = finalPrediction(data.test)\nprediction$Survived = ifelse(prediction$Survived == \"no\", 0, 1)\n","18aafc2b":"write.csv(prediction, \"predictions.csv\", row.names = FALSE)\n\n","c2a431a5":"Using more complex machine learning tools does in fact increase the kappa by a lot! That is pretty interesting, and could mean that this models will perform better on our testing set.\n\n## SVM\n","ea798894":"# Titanic - Ensambled Models\n\n## Introduciton\n\nThe goal for this project is learn the basics of the main statistical tool models and use some more complex machine learning tools to predict better. Moreover, combine them together to try to predict more accurately as some models will focus on one set of characteristics and others in other.\n\nTake care, this data set is not that big so we will take care for overfitting and also we won't be able to achieve high accuracy.\n\nFirst of all we set up our environment and the libraries that we will be using.\n","5724c25e":"Now, that all the columns are clear, let's look a the factors as there are sometimes hidden NAs.\n\n","11bae774":"Here SVM, is not really that usefull so we will no care about it. We have tried other SVM such as linear or polynomial but they do not perform any better. \n\n## Splines\n","03243f89":"It really does not have a really bad prediction, however due to the high bias, let's try to find other models that will hopefully perform better.\n\n## Logistic Regression\n","4643039f":"## Preprocessing\n\nWe delete useless columns.\n","a3626ee5":"Logistic regression is a little better than a decision tree.\n\n","459c501b":"## Decission Tree\n\nWe start simple to see what is the most important variable.\n","65b10976":"### Ensambled (QDA, KNN, RF)\n\nWe will use the median to predict.\n","017fbb48":"As there are some missing values, we will use the median to fill the empty data.\n\n","aee89ce6":"Here we see that there are some rows with empty values. We could use the median or the mean to predict them as they are maily in the Age column, but this will introduce some noise in the training set that we want to avoid.\n\nTherefore, let's delete them.\n","faf0464d":"LDA struggles a little bit in this data set and the most probable reason is that the data is not that normal (if EDA is done and used PCA we will see that the data is more radial base on the first two PCs).\n\n## QDA\n","be2225a1":"We convert columns that are strings to factors to better interpret them.\n\n","7e3ffd26":"## LDA\n\n","03b3c52f":"KNN is amazing. That is because of the nature of the model, it does not care about the shape it just cares about the closes data points. This could indicate that the separation between survived and dead people is pretty big, therefore kkn can be effective.\n\n## Penalized Logistic Regression\n","6c2521f0":"## Automate Model\n\nHere we will use this functions to predict, this will speed up our process in predicting.\n","708a5eb9":"Splines perform good, but not as good as QDA, therefore we will skip them.\n\n## Test Prediction\n","6554d438":"PLR is pretty much the same as logistic regression, this is because mainly we do not have a lot of predictors, therefore, there is not columns that add a lot of noise to the model.\n\n## Random Forest\n","0adc9191":"QDA is the best one yet, and this could be because it can model better non linear relationships that the data could have with the varaible that we want to predict.\n\n## KNN\n","fb8ec387":"Here we see that all the data is clean now, so let's start predicting. Note: we would do some data exploratory analysis, however this is not the main focus of this notebook. We assume that you have worked on previously with this data set.\n\n","e887e4db":"What about missing values in the training set? Let's take a look.\n\n","9629e12d":"Let's clean first the data.\n\n","e5cbe5d5":"## Set Up (Classification)\n\nLet's separate the data in test and train.\n"}}