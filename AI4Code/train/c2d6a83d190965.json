{"cell_type":{"d13da197":"code","6ae4e635":"code","5c00bde8":"code","84337831":"code","bf193080":"code","12114aa2":"code","c998783f":"code","4de44309":"code","d0b962f3":"code","c8a4cbf4":"code","dd93c0ea":"code","5bd4a481":"code","141b63b4":"code","413a81a9":"code","84f80dcf":"code","37995464":"code","921780a7":"code","a0714318":"code","84187054":"code","17c43966":"code","e077c141":"code","d2e1e10e":"code","87bdd107":"code","4ca48437":"code","27b7b6a8":"code","3c76983f":"code","5eb779ea":"code","42a4c233":"code","842e16b0":"code","6060d09f":"code","3cdee6a8":"markdown","afb8f5b5":"markdown","2aaaf2bf":"markdown","ed1173a7":"markdown","dfd843af":"markdown","7969fcd4":"markdown","26d4cc16":"markdown","afeb81a6":"markdown","db84f42e":"markdown","e04be323":"markdown","ed16634d":"markdown","349b7b92":"markdown","859153f4":"markdown","9bb13274":"markdown","5dc12ec4":"markdown","a23d9f9d":"markdown","8e684803":"markdown","194b11ed":"markdown","01e59acd":"markdown","0e6a3c18":"markdown","87dc31e7":"markdown","f4793778":"markdown","7e55c65f":"markdown"},"source":{"d13da197":"import tensorflow as tf\nimport keras.backend as K\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Dropout, LSTM\nimport pandas as pd\nimport numpy as np\nimport os\nimport math\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import MinMaxScaler","6ae4e635":"### Adding additional indicators\n### from https:\/\/journals.plos.org\/plosone\/article\/file?id=10.1371\/journal.pone.0155133&type=printable\n\ndef OBV(df):\n    prev = 0\n    OBV_list = [0]\n    for i in range(1, len(df)):\n        if df['Close'][i] - df['Close'][i-1] > 0:\n            prev += df['Volume'][i]\n        else:\n            prev -= df['Volume'][i]\n        OBV_list.append(prev)\n    \n    df['OBV'] = OBV_list\n    return df\n\ndef MA(df, time):\n    MA_list = [0 for i in range(time-1)]\n        \n    for i in range(time-1, len(df)):\n        summ = 0\n        for j in range(i-time+1, i+1):\n            summ += df['Close'][j]\n        summ \/= time\n        MA_list.append(summ)\n    \n    df['MA' + str(time)] = MA_list\n    return df\n\ndef BIAS(df, time):\n    BIAS_list = [0 for i in range(time-1)]\n    \n    for i in range(time-1, len(df)):\n        avg = 0\n        for j in range(i-time+1, i+1):\n            avg += df['Close'][j]\n        avg \/= time\n        \n        BS = 100*(df['Close'][i]-avg)\/avg\n        BIAS_list.append(BS)\n        \n    df['BIAS' + str(time)] = BIAS_list\n    return df\n\ndef PSY(df, time):\n    PSY_list = [0 for i in range(time)]\n        \n    for i in range(time, len(df)):\n        tot = 0\n        for j in range(i-time+1, i+1):\n            if df['Close'][j] - df['Close'][j-1] > 0:\n                tot += 1\n        \n        tot *= 100\/(time)\n        PSY_list.append(tot)\n        \n    df['PSY' + str(time)] = PSY_list\n    return df\n\ndef ASY(df, time):\n    ASY_list = [0 for i in range(time)]\n    \n    for i in range(time, len(df)):\n        summ = (np.log(df['Close'][i])-np.log(df['Close'][i-time]))*100\/time\n        ASY_list.append(summ)\n    \n    df['ASY' + str(time)] = ASY_list\n    return df\n\ndef stochK(df):\n    K_list = []\n    for i in range(len(df)):\n        K_list.append(100*(df['Close'][i] - df['Low'][i]) \/ (df['High'][i] - df['Low'][i]))\n        \n    df['stochK'] = K_list\n    return df\n\ndef stochD(df, time):\n    ### Can easily be optimized\n    D_list = [0 for i in range(time-1)]\n    \n    for i in range(time-1, len(df)):\n        summ = 0\n        for j in range(i-time+1, time+1):\n            summ += 100*(df['Close'][j] - df['Low'][j]) \/ (df['High'][j] - df['Low'][j])\n        D_list.append(summ)\n        \n    df['stochD' + str(time)] = D_list\n    return df\n\ndef momentum(df, time):\n    mom_list = [0 for i in range(time)]\n    \n    for i in range(time, len(df)):\n        mom_list.append(df['Close'][i] - df['Close'][i-time])\n    \n    df['momentum' + str(time)] = mom_list\n    return df\n\ndef OSCP(df, time1, time2):\n    OSCP_list = [0 for i in range(time2-1)]\n    \n    for i in range(time2-1, len(df)):\n        MA_short = 0\n        MA_long = 0\n        for j in range(i-time1+1, i+1):\n            MA_short += df['Close'][j]\n        \n        for j in range(i-time2+1, i+1):\n            MA_long += df['Close'][j]\n            \n        MA_short \/= time1\n        MA_long \/= time2\n        \n        OSCP_list.append((MA_short-MA_long)\/MA_short)\n    \n    df['OSCP' + str(time1)] = OSCP_list\n    return df\n\ndef RSI(df, time):\n    RSI_list = [0 for i in range(time)]\n    \n    for i in range(time, len(df)):\n        up = 0 \n        down = 0\n        for j in range(i-time+1, i+1):\n            diff = df['Close'][j] - df['Close'][j-1]\n            if diff >= 0:\n                up += diff\n            else:\n                down += abs(diff)\n        \n        RSI_list.append(100-100\/(1+(up\/down)))\n        \n    df['RSI' + str(time)] = RSI_list\n    return df\n\ndef EMA(df, time, param):\n    k = 2\/(time+1)\n    EMA_list = [0 for i in range(time-1)]\n    summ = 0\n    \n    for i in range(time):\n        summ += df[param][i]\n        \n    summ \/= time\n    EMA = summ\n    EMA_list.append(summ)\n    for i in range(time, len(df)):\n        EMA = df[param][i]*k+EMA*(1-k)\n        EMA_list.append(EMA)\n        \n    df['EMA' + str(time)] = EMA_list\n    return df\n\ndef MACD(df):\n    testdf = EMA(df, 12, 'Close')\n    testdf = EMA(testdf, 26, 'Close')\n    tmp_list = testdf['EMA12'] - testdf['EMA26']\n    df['MACD'] = tmp_list\n    signaldf = EMA(df, 9, 'MACD')\n    df['Signal'] = signaldf['EMA9']\n    df['MACDDiff'] = df['MACD'] - df['Signal']\n    df1 = df.drop(columns='EMA9')\n    \n    return df1\n\ndef convert(data):\n    data = OBV(data)\n    data = MA(data, 5)\n    data = BIAS(data, 6)\n    data = PSY(data, 12)\n    data = ASY(data, 5)\n    data = ASY(data, 4)\n    data = ASY(data, 3)\n    data = ASY(data, 2)\n    data = ASY(data, 1)\n    data = stochK(data)\n    data = momentum(data, 4)\n    data = OSCP(data, 5, 10)\n    data = RSI(data, 14)\n    data = MACD(data)\n    \n    return data","5c00bde8":"kwarg_list = ['Close', 'Open', 'Low', 'High', 'OBV', 'MA5', 'BIAS6', 'PSY12', 'ASY5', 'ASY4', 'ASY3', 'ASY2', 'ASY1', 'stochK', \n             'momentum4', 'OSCP5', 'RSI14', 'MACD', 'Signal', 'MACDDiff']\n\ndji = convert(pd.read_excel(\"\/kaggle\/input\/djitestfile3\/DJI.xlsx\"))\nfb = convert(pd.read_csv(\"\/kaggle\/input\/faang-companies\/FB.csv\"))\naapl = convert(pd.read_csv(\"\/kaggle\/input\/faang-companies\/AAPL.csv\"))\namzn = convert(pd.read_csv(\"\/kaggle\/input\/faang-companies\/AMZN.csv\"))\ngoog = convert(pd.read_csv(\"\/kaggle\/input\/faang-companies\/GOOG.csv\"))\nnflx = convert(pd.read_csv(\"\/kaggle\/input\/faang-companies\/NFLX (1).csv\"))\nmsft = convert(pd.read_csv(\"\/kaggle\/input\/faang-companies\/MSFT.csv\"))\nford = convert(pd.read_csv(\"\/kaggle\/input\/carcompanies\/F.csv\"))\ntoyota = convert(pd.read_csv(\"\/kaggle\/input\/carcompanies\/TM.csv\"))\nhonda = convert(pd.read_csv(\"\/kaggle\/input\/carcompanies\/HMC.csv\"))\nlincoln = convert(pd.read_csv(\"\/kaggle\/input\/carcompanies\/LNC.csv\"))","84337831":"comp_names = [\"Amazon\", \"Apple\", \"Netflix\", \"Google\"]\nplt.figure(figsize=(15,10))\nplt.plot(amzn['Close'], color='yellow')\nplt.plot(aapl['Close'], color='gray')\nplt.plot(nflx['Close'], color='red')\nplt.plot(goog['Close'], color='blue')\nplt.title(\"Non-Scaled Comparison of Large Tech Companies\", fontsize=24)\nplt.xlabel(\"Days Passed Since 2010\", fontsize=18)\nplt.ylabel(\"Value of Stock, in USD\", fontsize=18)\nplt.legend(comp_names, prop={'size': 16})\nplt.xlim(0, 2265)\nplt.show()","bf193080":"scaled_prices = [[[amzn['Close'][i]] for i in range(len(amzn['Close']))], [[aapl['Close'][i]] for i in range(len(aapl['Close']))],\n    [[nflx['Close'][i]] for i in range(len(nflx['Close']))], [[goog['Close'][i]] for i in range(len(goog['Close']))]]\n\nscaler1 = MinMaxScaler(feature_range=(0,1))\ncolors_list = ['yellow', 'gray', 'red', 'blue', 'green']\n\nplt.figure(figsize=(15,10))\nr = 0\nfor thing in scaled_prices:\n    test_set = scaler1.fit_transform(thing)\n    plt.plot(test_set, color=colors_list[r])\n    r += 1\n    \nplt.title(\"Scaled Comparison of Large Tech Companies\", fontsize=24)\nplt.xlabel(\"Days Passed Since 2010\", fontsize=18)\nplt.ylabel(\"Value of Stock, in USD, Scaled\", fontsize=18)\nplt.legend(comp_names, prop={'size': 16})\nplt.xlim(0, 2265)\nplt.ylim(0, 1)\nplt.show()","12114aa2":"other_comp_names = ['Toyota', 'Honda', 'Ford', 'Lincoln']\nplt.figure(figsize=(15,10))\nplt.plot(toyota['Close'], color='gray')\nplt.plot(honda['Close'], color='blue')\nplt.plot(ford['Close'], color='red')\nplt.plot(lincoln['Close'], color='green')\n\nplt.title(\"Non-Scaled Comparison of Automobile Companies\", fontsize=24)\nplt.xlabel(\"Days Passed Since 2010\", fontsize=18)\nplt.ylabel(\"Value of Stock, in USD\", fontsize=18)\nplt.legend(other_comp_names, prop={'size': 16})\nplt.xlim(0, 2265)\nplt.show()","c998783f":"other_scaled_prices = [[[dji['Close'][i]] for i in range(len(dji['Close']))], [[toyota['Close'][i]] for i in range(len(toyota['Close']))],\n    [[honda['Close'][i]] for i in range(len(honda['Close']))], [[ford['Close'][i]] for i in range(len(ford['Close']))], \n                      [[lincoln['Close'][i]] for i in range(len(lincoln['Close']))]]\n\nscaler1 = MinMaxScaler(feature_range=(0,1))\ncolors_list = ['yellow', 'gray', 'blue', 'red', 'green']\n\nplt.figure(figsize=(15,10))\nr = 0\nfor thing in other_scaled_prices:\n    test_set = scaler1.fit_transform(thing)\n    plt.plot(test_set, color=colors_list[r])\n    r += 1\n    \nplt.title(\"Scaled Comparison of DJI\/Automobile Companies\", fontsize=24)\nplt.xlabel(\"Days Passed Since 2010\", fontsize=18)\nplt.ylabel(\"Value of Stock, in USD, Scaled\", fontsize=18)\nplt.legend(other_comp_names, prop={'size': 12}, loc='upper left')\nplt.xlim(0, 2265)\nplt.ylim(0, 1)\nplt.show()","4de44309":"from scipy.stats import pearsonr, spearmanr\nimport plotly.graph_objects as go\n\ncomp_names = [\"Amazon\", \"Apple\", \"Netflix\", \"Google\"]\ntech_prices = [amzn['Close'], aapl['Close'], nflx['Close'], goog['Close']]\n\ndef convert_list(all_company_prices):\n    scaling_prices = []\n    for comp in all_company_prices:\n        comp_list = []\n        for x in comp:\n            comp_list.append(x)\n        scaling_prices.append(comp_list)\n    return scaling_prices\n\nscaling_prices = convert_list(tech_prices)\npearson_list = []\nspearman_list = []\npearson_avg = 0\nspearman_avg = 0\ncounter = 0\nfor i in range(len(scaling_prices)):\n    tmp_p_list = []\n    tmp_s_list = []\n    for j in range(len(scaling_prices)):\n        pearson_val = pearsonr(scaling_prices[i], scaling_prices[j])[0]\n        spearman_val = spearmanr(scaling_prices[i], scaling_prices[j])[0]\n        tmp_p_list.append(round(pearson_val, 3))\n        tmp_s_list.append(round(spearman_val, 3))\n        if i > j:\n            pearson_avg += pearson_val\n            spearman_avg += spearman_val\n            counter += 1\n    pearson_list.append(tmp_p_list)\n    spearman_list.append(tmp_s_list)\n\npearson_avg \/= counter\nspearman_avg \/= counter\nprint(\"Average of Pearson Coefficients %.3f\" % pearson_avg)\nprint(\"Average of Spearman Coefficients %.3f\" % spearman_avg)","d0b962f3":"def colors(n):\n    red_col = 10*round(255*(1-n))\n    green_col = 255*n\n    blue_col = 0\n    color = \"rgb(%d, %d, %d)\" % (red_col, green_col, blue_col)\n    return color\n\ndef create_table(name_list, colors, comp_names_x, comp_names_y, title, height_val):\n    tmp_name_list = [comp_names_y] + name_list\n    tmp_comp_names_x = [''] + comp_names_x\n    \n    color_map = []\n    for l in tmp_name_list:\n        tmp_color_list = []\n        for x in l:\n            if type(x) != str:\n                tmp_color_list.append(colors(x))\n            else:\n                tmp_color_list.append('rgb(245, 245, 245)')\n        color_map.append(tmp_color_list)\n    \n    layout = go.Layout(\n            title = go.layout.Title(\n                text=title,\n                x=0.5,\n                font=dict(color='rgb(0,0,0)', size=18)\n            ),\n              margin=go.layout.Margin(\n                    l=0, #left margin\n                    r=50, #right margin\n                    b=0, #bottom margin\n                    t=40  #top margin\n                ), \n              height = height_val\n            )\n    \n    trace = dict(header=dict(values=tmp_comp_names_x, fill=dict(color=['rgb(245, 245, 245)']), font = dict(color=['rgb(0,0,0)'])),\n            cells=dict(values=tmp_name_list, fill = dict(color=color_map), font = dict(color=['rgb(0,0,0)'])\n                    )\n                )\n            \n    fig = go.Figure(data=[go.Table(trace)], layout=layout)\n    fig.show()\n\ncreate_table(pearson_list, colors, comp_names, comp_names, \"Pearson's Coefficient Comparison of Tech Companies\", 180)\ncreate_table(spearman_list, colors, comp_names, comp_names, \"Spearman's Coefficient Comparison of Tech Companies\", 180)","c8a4cbf4":"other_companies = [dji['Close'], toyota['Close'], honda['Close'], ford['Close'], lincoln['Close']]\nother_prices = convert_list(other_companies)\n\ndef correlation_data(scaling_prices, other_prices):\n    pearson_list_o = []\n    spearman_list_o = []\n    pearson_avg_o = 0\n    spearman_avg_o = 0\n    counter_o = 0\n    for i in range(len(scaling_prices)):\n        tmp_p_list = []\n        tmp_s_list = []\n        for j in range(len(other_prices)):\n            pearson_val = pearsonr(scaling_prices[i], other_prices[j])[0]\n            spearman_val = spearmanr(scaling_prices[i], other_prices[j])[0]\n            tmp_p_list.append(round(pearson_val, 3))\n            tmp_s_list.append(round(spearman_val, 3))\n            if i >= j:\n                pearson_avg_o += pearson_val\n                spearman_avg_o += spearman_val\n                counter_o += 1\n        pearson_list_o.append(tmp_p_list)\n        spearman_list_o.append(tmp_s_list)\n\n    pearson_avg_o \/= counter_o\n    spearman_avg_o \/= counter_o\n    print(\"Average of Pearson Coefficients %.3f\" % pearson_avg_o)\n    print(\"Average of Spearman Coefficients %.3f\" % spearman_avg_o)\n    return pearson_list_o, spearman_list_o\n\npearson_list_o, spearman_list_o = correlation_data(scaling_prices, other_prices)","dd93c0ea":"def other_colors(n):\n    red_col = 255 if n < 0 else 0\n    green_col = 255*n if n > 0 else 255*(n+1)\n    blue_col = 100*abs(n)\n    color = \"rgb(%d, %d, %d)\" % (red_col, green_col, blue_col)\n    return color\n\ncreate_table(pearson_list_o, other_colors, comp_names, other_comp_names, \"Pearson Coefficients Comparison with Other Companies\", 190)\ncreate_table(spearman_list_o, other_colors, comp_names, other_comp_names, \"Spearman Coefficients Comparison with Other Companies\", 190)","5bd4a481":"def colors3(n):\n    red_col = 255*(1-abs(n)) if n > 0 else 255\n    green_col = 255*abs(n) if n > 0.5 else 150*(1-abs(n))\n    blue_col = 0\n    color = \"rgb(%d, %d, %d)\" % (red_col, green_col, blue_col)\n    return color\n\npearson_list_oc, spearman_list_oc = correlation_data(other_prices, other_prices)\ncreate_table(pearson_list_oc, colors3, other_comp_names, other_comp_names, \"Pearson's Coefficient Comparison of Control\/Car Companies\", 180)\ncreate_table(spearman_list_oc, colors3, other_comp_names, other_comp_names, \"Pearson's Coefficient Comparison of Control\/Car Companies\", 180)","141b63b4":"test_data = [fb['Close']]\ntest_list = convert_list(test_data)\ntest_comp_names = ['Facebook', 'Microsoft']\nnew_comb_names = comp_names + other_comp_names\nnew_comb_list = scaling_prices + other_companies\n\nnew_data_list = []\nfor comb_list in new_comb_list:\n    tmp_list = []\n    for i in range(600, len(comb_list)):\n        tmp_list.append(comb_list[i])\n    new_data_list.append(tmp_list)\n    \nmsft_data = [msft['Close']]\nmsft_list = convert_list(msft_data)\n    \npearson_list_t, spearman_list_t = correlation_data(new_data_list, test_list)\npearson_list_tmp, spearman_list_tmp = correlation_data(new_comb_list, msft_list)\n\npearson_list_f = [[x[0], y[0]] for x, y in zip(pearson_list_t, pearson_list_tmp)] \nspearman_list_f = [[x[0], y[0]] for x, y in zip(spearman_list_t, spearman_list_tmp)]\n\ncreate_table(pearson_list_f, other_colors, new_comb_names, test_comp_names, \"Pearson Coefficients When Comparing Input Data and Test Data\", 120)\ncreate_table(spearman_list_f, other_colors, new_comb_names, test_comp_names, \"Spearman Coefficients When Comparing Input Data and Test Data\", 120)","413a81a9":"dense_layer = 14\nlstm_layer = 45\nperiod = 14\niterations = 4*3","84f80dcf":"def get_cases(data, start_index, end_index, period):\n    large_set = []\n    stock_set = []\n    for i in range(start_index, end_index):\n        tmp_list = []\n        stock_set.append([data['Close'][i]])\n        for kwarg in kwarg_list:\n            tmp_list.append(data[kwarg][i])\n\n        large_set.append(tmp_list)\n    \n    scaler = MinMaxScaler(feature_range=(0,1))\n    input_set = scaler.fit_transform(large_set)\n    test_scaler = MinMaxScaler(feature_range=(0,1))\n    throwaway = test_scaler.fit_transform(stock_set) #code so the test_scaler is scaled onto just the stock values,\n    #used for inverse transform later\n\n    train_set, test_set = [], []\n    for i in range(len(input_set)):\n        test_set.append(input_set[i][:])\n\n    test_set = np.array(test_set)\n\n    x_test, y_test = [], [] #Input, what we want\n    for i in range(period, test_set.shape[0]):\n        x_test.append(test_set[i-period:i, :])\n        y_test.append([test_set[i, 0]])\n\n    x_test, y_test = np.array(x_test), np.array(y_test)\n    return x_test, y_test, test_scaler","37995464":"test_cases_list = []\nstart_end_list = [[600, 901], [1100, 1401], [1200, 1501], [1700, 2001]]\ncnt = 0\nprint(len(msft)-len(fb))\n\nfor start_end in start_end_list:\n    if cnt < 2:\n        x_test, y_test, test_scaler = get_cases(fb, start_end[0], start_end[1], period)\n    else:\n        x_test, y_test, test_scaler = get_cases(msft, start_end[0], start_end[1], period)\n    test_cases_list.append([x_test, y_test, test_scaler])\n    cnt += 1\n    \nvalidation_x, validation_y, validation_scaler = get_cases(msft, 1200, 1501, period)","921780a7":"def solve(data, model, epochs, period):\n    break_pt = 2117\n\n    large_set = []\n    for i in range(period+1, len(data)):\n        tmp_list = []\n        for kwarg in kwarg_list:\n            tmp_list.append(data[kwarg][i])\n\n        large_set.append(tmp_list)\n    \n    scaler = MinMaxScaler(feature_range=(0,1))\n    input_set = scaler.fit_transform(large_set)\n\n    train_set, test_set = [], []\n    for i in range(len(input_set)):\n        if i < break_pt:\n            train_set.append(input_set[i][:])\n        else:\n            test_set.append(input_set[i][:])\n\n    train_set, test_set = np.array(train_set), np.array(test_set)\n\n    x_train, y_train, x_test, y_test = [], [], [], [] #Input, what we want\n    for i in range(period, train_set.shape[0]):\n        x_train.append(train_set[i-period:i, :])\n        y_train.append([train_set[i, 0]])\n        if i < test_set.shape[0]:\n            x_test.append(test_set[i-period:i, :])\n            y_test.append([test_set[i, 0]])\n\n    x_train, y_train = np.array(x_train), np.array(y_train)\n    x_test, y_test = np.array(x_test), np.array(y_test)\n\n    model.compile(loss='mse', optimizer='adam')\n    history = model.fit(x_train, y_train, epochs=epochs, batch_size=1, \n              validation_data=(validation_x, validation_y))\n            \n    return model, history","a0714318":"def double_dense_lstm_model(dense_layer, lstm_layer, second_dense_layer):\n    model = Sequential()\n    model.add(Dense(dense_layer))\n    model.add(LSTM(lstm_layer))\n    model.add(Dense(second_dense_layer))\n    model.add(Dense(1))\n    return model\n\ndef dense_lstm_model(dense_layer, lstm_layer):\n    model = Sequential()\n    model.add(Dense(dense_layer))\n    model.add(LSTM(lstm_layer))\n    model.add(Dense(1))\n    return model\n    \ndef lstm_model(lstm_layer):\n    model = Sequential()\n    model.add(LSTM(lstm_layer))\n    model.add(Dense(1))\n    return model\n\ndef encoder_decoder_lstm(first_dense_layer, lstm_layer, second_dense_layer):\n    model = Sequential()\n    model.add(Dense(first_dense_layer))\n    model.add(LSTM(lstm_layer, return_sequences=True))\n    model.add(LSTM(lstm_layer))\n    model.add(Dense(second_dense_layer))\n    model.add(Dense(1))\n    return model","84187054":"\"\"\" Main Training Block\ndense_layer = 14\nlstm_layer = 45\niterations = 4*3\n\ndef standard_metrics(x_test, y_test):\n    c = 0\n    for i in range(1, len(x_test)):\n        if x_test[i]-y_test[i-1] > 0:\n            if y_test[i]-y_test[i-1] > 0:\n                c += 1\n        else:\n            if y_test[i]-y_test[i-1] < 0:\n                c += 1\n    \n    mse = np.mean(np.square(y_test-x_test))\n    dpa = c\/len(x_test)\n    #print(\"DPA: %.4f \\n MSE: %.4f\" % (dpa, mse))\n    return mse, dpa\n\ndji_array = np.array([float(0) for x in range(8)]).reshape(4, 2)\nstorage_list = []\ncomp_list = [aapl, amzn, goog, nflx]\ncar_comp = [toyota, honda, ford, lincoln]\ntimes = 30\nfor i in range(times):\n    tmp_list = []\n    dji_model = dense_lstm_model(dense_layer, lstm_layer)\n    dji_model, history1 = solve(dji, dji_model, iterations, period)\n    cnt = 0\n    for test_cases in test_cases_list:\n        x = dji_model.predict(np.array(test_cases[0]))\n        y = test_cases[1]\n        test_scaler = test_cases[2]\n        x = test_scaler.inverse_transform(x)\n        y = test_scaler.inverse_transform(y)\n        mse, dpa = standard_metrics(x, y)\n        dji_array[cnt][0] += mse\n        dji_array[cnt][1] += dpa\n        tmp_list.append([mse, dpa])\n        cnt += 1\n    storage_list.append(tmp_list)\n\"\"\"","17c43966":"history2_list = []\nhistory3_list = []\n\ndense_layer = 14\nlstm_layer = 45\niterations = 4*3\n\ndji_model = dense_lstm_model(dense_layer, lstm_layer)\ndji_model, history1 = solve(dji, dji_model, iterations, period)\n\ncomp_list = [aapl, amzn, goog, nflx]\ncar_comp = [toyota, honda, ford, lincoln]\nmodel_seq = dense_lstm_model(dense_layer, lstm_layer)\nfor x in car_comp:\n    model_seq, history2 = solve(x, model_seq, int(iterations\/4), period)\n    history2_list.append(history2)\n   \nmodel_iter = dense_lstm_model(dense_layer, lstm_layer)\nfor i in range(int(iterations\/4)):\n    for x in car_comp:\n        model_iter, history3 = solve(x, model_iter, 1, period)\n        history3_list.append(history3)","e077c141":"history2_loss = []\nhistory2_valloss = []\nfor history2 in history2_list:\n    for x in history2.history['loss']:\n        history2_loss.append(x)\n    for x in history2.history['val_loss']:\n        history2_valloss.append(x)\n\nhistory3_loss = []\nhistory3_valloss = []\nfor history3 in history3_list:\n    history3_loss.append(history3.history['loss'][0])\n    history3_valloss.append(history3.history['val_loss'][0])","d2e1e10e":"model_names = [\"DJI Model\", \"Sequential Model\", \"Iterative Model\"]\n\nfig, ax = plt.subplots(1, 2, figsize=(20, 8))\nax[0].plot([x for x in range(1, iterations+1)], history1.history['loss'], color='blue', linewidth=2)\nax[0].plot([x for x in range(1, iterations+1)], history2_loss, color='orange', linewidth=2)\nax[0].plot([x for x in range(1, iterations+1)], history3_loss, color='green', linewidth=2)\n\nax[1].plot([x for x in range(1, iterations+1)], history1.history['val_loss'], color='blue', linewidth=2)\nax[1].plot([x for x in range(1, iterations+1)], history2_valloss, color='orange', linewidth=2)\nax[1].plot([x for x in range(1, iterations+1)], history3_valloss, color='green', linewidth=2)\n\nax[0].set_title('Training Loss (MSE)', fontsize=18)\nax[1].set_title('Validation Loss (MSE)', fontsize=18)\nax[0].set_xlabel('Epochs', fontsize=14)\nax[1].set_xlabel('Epochs', fontsize=14)\nax[0].set_ylabel('Loss', fontsize=14)\nax[1].set_ylabel('Loss', fontsize=14)\nax[0].legend(model_names, prop={'size': 14})\nax[1].legend(model_names, prop={'size': 14})\nax[0].set_xlim(1, iterations)\nax[1].set_xlim(1, iterations)\nax[0].set_xticks([x for x in range(1, iterations+1)])\nax[1].set_xticks([x for x in range(1, iterations+1)])\n\nfig.show()","87bdd107":"fig, ax = plt.subplots(2, 2, figsize=(15, 10))\nax[0][0].plot(fb['Close'][600:901])\nax[0][1].plot(fb['Close'][1100:1401])\nax[1][0].plot(msft['Close'][1200:1501])\nax[1][1].plot(msft['Close'][1700:2001])\n\nax[0][0].set_title(\"FB 1 Actual Stock Prices\")\nax[0][1].set_title(\"FB 2 Actual Stock Prices\")\nax[1][0].set_title(\"MSFT 1 Actual Stock Prices\")\nax[1][1].set_title(\"MSFT 2 Actual Stock Prices\")","4ca48437":"def standard_metrics(x_test, y_test):\n    c = 0\n    for i in range(1, len(x_test)):\n        if x_test[i]-y_test[i-1] > 0:\n            if y_test[i]-y_test[i-1] > 0:\n                c += 1\n        else:\n            if y_test[i]-y_test[i-1] < 0:\n                c += 1\n    \n    mse = np.mean(np.square(y_test-x_test))\n    dpa = c\/len(x_test)\n    print(\"DPA: %.4f \\n MSE: %.4f\" % (dpa, mse))\n    return mse, dpa","27b7b6a8":"comp_list_names = ['aapl', 'amzn', 'goog', 'nflx']\n\nmodels_list = [dji_model, model_seq, model_iter]\n\ndef stock_results(models_list, test_cases_list, inverse=True):\n    fig, ax = plt.subplots(len(test_cases_list), 3, figsize=(20, 20))\n    cnt = 0\n    for test_cases in test_cases_list:\n        for r in range(len(test_cases)):\n            stock_model = models_list[r]\n            x = stock_model.predict(np.array(test_cases[0]))\n            y = test_cases[1]\n\n            if inverse:\n                test_scaler = test_cases[2]\n                x = test_scaler.inverse_transform(x)\n                y = test_scaler.inverse_transform(y)\n\n            ax[cnt][r].plot(x, color='red')\n            ax[cnt][r].plot(y, color='gray')\n            ax[cnt][r].set_xlim(0, len(x))\n            mse, dpa = standard_metrics(x, y)\n            ax[cnt][r].set_title(\"%s, Error: %.4f, DPA: %.4f\" % (model_names[r], mse, dpa))\n        cnt += 1\n        \n    fig.show()\n\nstock_results(models_list, test_cases_list)","3c76983f":"msft_test_cases = []\nford_test_cases = []\nfor start_end in start_end_list:\n    x_test, y_test, test_scaler = get_cases(msft, start_end[0], start_end[1], period)\n    msft_test_cases.append([x_test, y_test, test_scaler])\n    x_test, y_test, test_scaler = get_cases(ford, start_end[0], start_end[1], period)\n    ford_test_cases.append([x_test, y_test, test_scaler])","5eb779ea":"dji_arr = np.load(\"\/kaggle\/input\/nparrays\/car_iter_large.npy\")\ntmp_list = []\navg_mse = 0\navg_dpa = 0\nf_list = []\ng_list = []\nfor i in range(len(dji_arr)):\n    tmp_list.append(dji_arr[i, 3, 1])\n    for x in range(4):\n        avg_mse += dji_arr[i, x, 0]\n        avg_dpa += dji_arr[i, x, 1]\n        f_list.append(dji_arr[i, x, 0])\n        g_list.append(dji_arr[i, x, 1])\n    \nprint(str(round(np.mean(tmp_list), 3)) + ' \\pm ' + str(round(2*np.std(tmp_list), 3)))\nprint(str(round(avg_mse\/len(dji_arr)\/4, 3)) + ' \\pm ' + str(round(2*np.std(f_list), 3)))\nprint(str(round(avg_dpa\/len(dji_arr)\/4, 3)) + ' \\pm ' + str(round(2*np.std(g_list), 3)))","42a4c233":"dji_arr = np.load(\"\/kaggle\/input\/nparrays\/dji2.npy\")\ncar_iter = np.load(\"\/kaggle\/input\/nparrays\/car_iter.npy\")\ncar_seq = np.load(\"\/kaggle\/input\/nparrays\/car_seq.npy\")\ntech_iter = np.load(\"\/kaggle\/input\/nparrays\/tech_iter.npy\")\ntech_seq = np.load(\"\/kaggle\/input\/nparrays\/tech_seq (1).npy\")\ndata_list = [dji_arr, tech_seq, tech_iter, car_seq, car_iter]","842e16b0":"plot_list = []\nfor data in data_list:\n    tmp_list = []\n    for i in range(len(data)):\n        tmp_list.append(data[i, 0, 1])\n    plot_list.append(tmp_list)\n\nimport seaborn as sns\nnames_list = ['DJI', 'Sequential - Tech', 'Iterative - Tech', 'Sequential - Car', 'Iterative - Car']\nplt.figure(figsize=(15, 10))\nfor x in range(len(plot_list)):\n    sns.distplot(plot_list[x], hist = False, kde = True,\n                 kde_kws = {'shade': True, 'linewidth': 3}, \n                  label = names_list[x])\nplt.xlim(0.4, 0.6)\nplt.title(\"Density Plot of Mean Squared Error for FB 1 Case\", fontsize=20)\nplt.ylabel(\"Distribution\", fontsize=20)\nplt.xlabel(\"Mean Squared Error\", fontsize=20)\nplt.legend(prop={'size': 14})\nplt.show()","6060d09f":"### Code for deleting outputs\nimport os\nimport shutil\n#os.remove(\"\/kaggle\/working\/\" + \"C:\\Users\\Admin\\Desktop\\stockproj\\dense_lstm\\dji_model\\L64I20P14.h5\") #File\n#shutil.rmtree(\"\/kaggle\/working\/dense\") #Directory\n#os.rmdir(\"\/kaggle\/working\/dense_lstm\")","3cdee6a8":"Now we can start graphing\/using our models to predict.","afb8f5b5":"Finally, we can compare the automobile company cluster.","2aaaf2bf":"And compare it with the test data.","ed1173a7":"Next we define the function that can be iterated to train the neural network.","dfd843af":"Let's test correlation between these clusters of data.","7969fcd4":"We will begin with the important import statements:","26d4cc16":"Here, we can create a color map.","afeb81a6":"Now process the data and graph the data in a scaled version (better visual of what we are inputting into the neural network).","db84f42e":"Aside from looking at Facebook, we can look at Microsoft and Ford.","e04be323":"We can initialize our models in the next code block.","ed16634d":"Now let's get some test cases.","349b7b92":"# The Model\n\nLet's initialize our parameters.","859153f4":"Scaled version of automobile companies:","9bb13274":"We can also graph the other cluster of data (Automobile companies).","5dc12ec4":"Load in the data.","a23d9f9d":"# Processing for Research","8e684803":"# Preliminary Analysis\nFirst, let's graph the test data against itself.","194b11ed":"# Stock Market Prediction with Cluster Analysis\n\nKernel for research paper, linked here: https:\/\/drive.google.com\/file\/d\/1g5G_LkgcnCrpZLzMuPzwthcjbhkd9t_6\/view?usp=sharing.\n\nFuture research will be optimizing model architecture and input variables, as well as adding more stock data with my web application: https:\/\/stock-scraper-multi.herokuapp.com\/.","01e59acd":"Let's compare the correlation of the tech companies with the other companies (DJI + car).","0e6a3c18":"We have metrics for MSE\/DPA that we can put into a function.","87dc31e7":"Now we can train our models.","f4793778":"Define the stock market indicators:","7e55c65f":"Initialize loss histories."}}