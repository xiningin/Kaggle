{"cell_type":{"3c9a6a6e":"code","97df52c0":"code","91fd8587":"code","70b67af8":"code","85b68fae":"code","0aec8330":"code","fbde8721":"code","c3cadd9d":"code","16d026b1":"code","cf1d6272":"code","72a58d93":"code","f945fc8b":"code","b8f6ef69":"code","58ced2d2":"code","5478ba6f":"code","76cd4ad7":"code","c8f6ecd5":"markdown","f652f348":"markdown","6148186b":"markdown","5efa1008":"markdown","1d2776f6":"markdown","6402b3a7":"markdown","c7600a0f":"markdown","bfe14f0a":"markdown","37b1b432":"markdown","3bb4b726":"markdown","18943570":"markdown","755a94ca":"markdown","a4684c0d":"markdown","b5016013":"markdown"},"source":{"3c9a6a6e":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\nimport os\nprint(os.listdir(\"..\/input\"))\n# Any results you write to the current directory are saved as output.\nfrom pandas import read_csv\n#Lets load the dataset and sample some\ncolumn_names = ['CRIM', 'ZN', 'INDUS', 'CHAS', 'NOX', 'RM', 'AGE', 'DIS', 'RAD', 'TAX', 'PTRATIO', 'B', 'LSTAT', 'MEDV']\ndata = read_csv('..\/input\/housing.csv', header=None, delimiter=r\"\\s+\", names=column_names)\nprint(data.head(5))","97df52c0":"# Dimension of the dataset\nprint(np.shape(data))","91fd8587":"# Let's summarize the data to see the distribution of data\nprint(data.describe())","70b67af8":"import seaborn as sns\nimport matplotlib.pyplot as plt\nfrom scipy import stats\n\nfig, axs = plt.subplots(ncols=7, nrows=2, figsize=(20, 10))\nindex = 0\naxs = axs.flatten()\nfor k,v in data.items():\n    sns.boxplot(y=k, data=data, ax=axs[index])\n    index += 1\nplt.tight_layout(pad=0.4, w_pad=0.5, h_pad=5.0)","85b68fae":"    for k, v in data.items():\n        q1 = v.quantile(0.25)\n        q3 = v.quantile(0.75)\n        irq = q3 - q1\n        v_col = v[(v <= q1 - 1.5 * irq) | (v >= q3 + 1.5 * irq)]\n        perc = np.shape(v_col)[0] * 100.0 \/ np.shape(data)[0]\n        print(\"Column %s outliers = %.2f%%\" % (k, perc))\n    \n    ","0aec8330":"data = data[~(data['MEDV'] >= 50.0)]\nprint(np.shape(data))","fbde8721":"fig, axs = plt.subplots(ncols=7, nrows=2, figsize=(20, 10))\nindex = 0\naxs = axs.flatten()\nfor k,v in data.items():\n    sns.distplot(v, ax=axs[index])\n    index += 1\nplt.tight_layout(pad=0.4, w_pad=0.5, h_pad=5.0)","c3cadd9d":"plt.figure(figsize=(20, 10))\nsns.heatmap(data.corr().abs(),  annot=True)","16d026b1":"from sklearn import preprocessing\n# Let's scale the columns before plotting them against MEDV\nmin_max_scaler = preprocessing.MinMaxScaler()\ncolumn_sels = ['LSTAT', 'INDUS', 'NOX', 'PTRATIO', 'RM', 'TAX', 'DIS', 'AGE']\nx = data.loc[:,column_sels]\ny = data['MEDV']\nx = pd.DataFrame(data=min_max_scaler.fit_transform(x), columns=column_sels)\nfig, axs = plt.subplots(ncols=4, nrows=2, figsize=(20, 10))\nindex = 0\naxs = axs.flatten()\nfor i, k in enumerate(column_sels):\n    sns.regplot(y=y, x=x[k], ax=axs[i])\nplt.tight_layout(pad=0.4, w_pad=0.5, h_pad=5.0)\n","cf1d6272":"y =  np.log1p(y)\nfor col in x.columns:\n    if np.abs(x[col].skew()) > 0.3:\n        x[col] = np.log1p(x[col])","72a58d93":"from sklearn import datasets, linear_model\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import KFold\nimport numpy as np\n\nl_regression = linear_model.LinearRegression()\nkf = KFold(n_splits=10)\nmin_max_scaler = preprocessing.MinMaxScaler()\nx_scaled = min_max_scaler.fit_transform(x)\nscores = cross_val_score(l_regression, x_scaled, y, cv=kf, scoring='neg_mean_squared_error')\nprint(\"MSE: %0.2f (+\/- %0.2f)\" % (scores.mean(), scores.std()))\n\nscores_map = {}\nscores_map['LinearRegression'] = scores\nl_ridge = linear_model.Ridge()\nscores = cross_val_score(l_ridge, x_scaled, y, cv=kf, scoring='neg_mean_squared_error')\nscores_map['Ridge'] = scores\nprint(\"MSE: %0.2f (+\/- %0.2f)\" % (scores.mean(), scores.std()))\n\n# Lets try polinomial regression with L2 with degree for the best fit\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import PolynomialFeatures\n#for degree in range(2, 6):\n#    model = make_pipeline(PolynomialFeatures(degree=degree), linear_model.Ridge())\n#    scores = cross_val_score(model, x_scaled, y, cv=kf, scoring='neg_mean_squared_error')\n#    print(\"MSE: %0.2f (+\/- %0.2f)\" % (scores.mean(), scores.std()))\nmodel = make_pipeline(PolynomialFeatures(degree=3), linear_model.Ridge())\nscores = cross_val_score(model, x_scaled, y, cv=kf, scoring='neg_mean_squared_error')\nscores_map['PolyRidge'] = scores\nprint(\"MSE: %0.2f (+\/- %0.2f)\" % (scores.mean(), scores.std()))\n","f945fc8b":"from sklearn.svm import SVR\nfrom sklearn.model_selection import GridSearchCV\n\nsvr_rbf = SVR(kernel='rbf', C=1e3, gamma=0.1)\n#grid_sv = GridSearchCV(svr_rbf, cv=kf, param_grid={\"C\": [1e0, 1e1, 1e2, 1e3], \"gamma\": np.logspace(-2, 2, 5)}, scoring='neg_mean_squared_error')\n#grid_sv.fit(x_scaled, y)\n#print(\"Best classifier :\", grid_sv.best_estimator_)\nscores = cross_val_score(svr_rbf, x_scaled, y, cv=kf, scoring='neg_mean_squared_error')\nscores_map['SVR'] = scores\nprint(\"MSE: %0.2f (+\/- %0.2f)\" % (scores.mean(), scores.std()))\n","b8f6ef69":"from sklearn.tree import DecisionTreeRegressor\n\ndesc_tr = DecisionTreeRegressor(max_depth=5)\n#grid_sv = GridSearchCV(desc_tr, cv=kf, param_grid={\"max_depth\" : [1, 2, 3, 4, 5, 6, 7]}, scoring='neg_mean_squared_error')\n#grid_sv.fit(x_scaled, y)\n#print(\"Best classifier :\", grid_sv.best_estimator_)\nscores = cross_val_score(desc_tr, x_scaled, y, cv=kf, scoring='neg_mean_squared_error')\nscores_map['DecisionTreeRegressor'] = scores\nprint(\"MSE: %0.2f (+\/- %0.2f)\" % (scores.mean(), scores.std()))","58ced2d2":"from sklearn.neighbors import KNeighborsRegressor\n\nknn = KNeighborsRegressor(n_neighbors=7)\nscores = cross_val_score(knn, x_scaled, y, cv=kf, scoring='neg_mean_squared_error')\nscores_map['KNeighborsRegressor'] = scores\n#grid_sv = GridSearchCV(knn, cv=kf, param_grid={\"n_neighbors\" : [2, 3, 4, 5, 6, 7]}, scoring='neg_mean_squared_error')\n#grid_sv.fit(x_scaled, y)\n#print(\"Best classifier :\", grid_sv.best_estimator_)\nprint(\"KNN Accuracy: %0.2f (+\/- %0.2f)\" % (scores.mean(), scores.std()))","5478ba6f":"from sklearn.ensemble import GradientBoostingRegressor\n\ngbr = GradientBoostingRegressor(alpha=0.9,learning_rate=0.05, max_depth=2, min_samples_leaf=5, min_samples_split=2, n_estimators=100, random_state=30)\n#param_grid={'n_estimators':[100, 200], 'learning_rate': [0.1,0.05,0.02], 'max_depth':[2, 4,6], 'min_samples_leaf':[3,5,9]}\n#grid_sv = GridSearchCV(gbr, cv=kf, param_grid=param_grid, scoring='neg_mean_squared_error')\n#grid_sv.fit(x_scaled, y)\n#print(\"Best classifier :\", grid_sv.best_estimator_)\nscores = cross_val_score(gbr, x_scaled, y, cv=kf, scoring='neg_mean_squared_error')\nscores_map['GradientBoostingRegressor'] = scores\nprint(\"MSE: %0.2f (+\/- %0.2f)\" % (scores.mean(), scores.std()))\n","76cd4ad7":"plt.figure(figsize=(20, 10))\nscores_map = pd.DataFrame(scores_map)\nsns.boxplot(data=scores_map)","c8f6ecd5":"Columns like CRIM, ZN, RM, B seems to have outliers. Let's see the outliers percentage in every column.","f652f348":"Let's plot k-fold results to see which model has better distribution of results. Let's have a look at the MSE distribution of these models with k-fold=10","6148186b":"Another interesing fact on the dataset is the max value of MEDV. From the original data description, it says: Variable #14 seems to be censored at 50.00 (corresponding to a median price of $50,000). Based on that, values above 50.00 may not help to predict MEDV. Let's plot the dataset and see interesting trends\/stats.","5efa1008":"The Liner Regression with and without L2 regularization does not make significant difference is MSE score. However polynomial regression with degree=3 has a better MSE. Let's try some non prametric regression techniques: SVR with kernal rbf, DecisionTreeRegressor, KNeighborsRegressor etc.","1d2776f6":"Let's try Linear, Ridge Regression on dataset first.","6402b3a7":"The models SVR and GradientBoostingRegressor show better performance with -11.62 (+\/- 5.91) and -12.39 (+\/- 5.86).\n\nThis is my first kernel and thanks to https:\/\/www.kaggle.com\/vikrishnan for the dataset and the well writtten kernel that provdies great pointers into this dataset.","c7600a0f":"From correlation matrix, we see TAX and RAD are highly correlated features. The columns LSTAT, INDUS, RM, TAX, NOX, PTRAIO has a correlation score above 0.5 with MEDV which is a good indication of using as predictors. Let's plot these columns against MEDV. ","bfe14f0a":"Let's see how these features plus MEDV distributions looks like","37b1b432":"From get-go,  two data coulmns show interesting summeries. They are : ZN (proportion of residential land zoned for lots over 25,000 sq.ft.)  with 0 for 25th, 50th percentiles. Second, CHAS: Charles River dummy variable (1 if tract bounds river; 0 otherwise) with 0 for 25th, 50th and 75th percentiles. These summeries are understandable as both variables are conditional + categorical variables. First assumption would be that these coulms may not be useful in regression task such as predicting MEDV (Median value of owner-occupied homes).","3bb4b726":"So with these analsis, we may try predict MEDV with 'LSTAT', 'INDUS', 'NOX', 'PTRATIO', 'RM', 'TAX', 'DIS', 'AGE' features. Let's try to remove the skewness of the data trough log transformation.\n\n","18943570":"The histogram also shows that columns CRIM, ZN, B has highly skewed distributions. Also MEDV looks to have a normal distribution (the predictions) and other colums seem to have norma or bimodel ditribution of data except CHAS (which is a discrete variable).\n\nNow let's plot the pairwise  correlation on data.","755a94ca":"The Boston Housing Dataset\n\nThe Boston Housing Dataset is a derived from information collected by the U.S. Census Service concerning housing in the area of [ Boston MA](http:\/\/www.cs.toronto.edu\/~delve\/data\/boston\/bostonDetail.html). The following describes the dataset columns:\n\n* CRIM - per capita crime rate by town\n* ZN - proportion of residential land zoned for lots over 25,000 sq.ft.\n* INDUS - proportion of non-retail business acres per town.\n* CHAS - Charles River dummy variable (1 if tract bounds river; 0 otherwise)\n* NOX - nitric oxides concentration (parts per 10 million)\n* RM - average number of rooms per dwelling\n* AGE - proportion of owner-occupied units built prior to 1940\n* DIS - weighted distances to five Boston employment centres\n* RAD - index of accessibility to radial highways\n* TAX - full-value property-tax rate per \\$10,000\n* PTRATIO - pupil-teacher ratio by town\n* B - 1000(Bk - 0.63)^2 where Bk is the proportion of blacks by town\n* LSTAT - % lower status of the population\n* MEDV - Median value of owner-occupied homes in \\$1000's\n\n","a4684c0d":"Compared to three models which are shosen through grid search, SVR performes better. Let's try an ensemble method finally.","b5016013":"Let's remove MEDV outliers (MEDV = 50.0) before plotting more distributions"}}