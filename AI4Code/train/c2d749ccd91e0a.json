{"cell_type":{"eb1c9dd4":"code","f091fc91":"code","8910200a":"code","b9d2d10e":"code","15ba55b2":"code","e07f0d85":"code","69fe83f5":"code","cc2e2c8b":"code","f51512e1":"code","9ab10b08":"code","00c0c8b4":"code","88b1f584":"code","327a6eb1":"code","26302f0a":"code","7070f300":"code","6483d605":"code","d68748bf":"code","fdc65b5d":"code","2dcb42fe":"code","23af3bea":"code","3f73ff02":"markdown","c90285e3":"markdown","dc4e182e":"markdown","804f3ba4":"markdown","0d688b28":"markdown","827fc291":"markdown","c221e35d":"markdown","0e6bfb64":"markdown","20cfced6":"markdown","cafaf002":"markdown","9e627aa9":"markdown","2c90a13a":"markdown","8afca74f":"markdown","7aeb6f1d":"markdown","d9baa4f7":"markdown","cc9b89a1":"markdown","14e6c8ce":"markdown","74d5d658":"markdown","786c1ceb":"markdown","ac447772":"markdown","8c12bd36":"markdown","3c1967b5":"markdown","dfd4fd9b":"markdown"},"source":{"eb1c9dd4":"import random\nimport seaborn\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport matplotlib.cm as cm\n\nseaborn.set(style='whitegrid'); seaborn.set_context('talk')\n%matplotlib inline\n%config InlineBackend.figure_format = 'retina'\n\nfrom sklearn.datasets import load_iris\niris_data = load_iris()","f091fc91":"print(iris_data['DESCR'])","8910200a":"n_samples, n_features = iris_data.data.shape\n\nplt.subplot(1, 2, 1)\nscatter_plot = plt.scatter(iris_data.data[:,0], iris_data.data[:,1], alpha=0.5, \n                           c=iris_data.target) \nplt.colorbar(ticks=([0, 1, 2]))\nplt.title('Sepal Sample')\n\nplt.subplot(1, 2, 2)\nscatter_plot_2 = plt.scatter(iris_data.data[:,2], iris_data.data[:,3], alpha=0.5, \n                           c=iris_data.target)\nplt.colorbar(ticks=([0, 1, 2]))\nplt.title('Petal Sample')","b9d2d10e":"import pandas\nfrom pandas.plotting import scatter_matrix\n\n\ndataset = pandas.read_csv('..\/input\/iris\/Iris.csv')\nscatter_matrix(dataset, alpha=0.5, figsize=(20, 20))\nplt.show()","15ba55b2":"dataset.hist(alpha=0.5, figsize=(20, 20), color='red')\nplt.show()","e07f0d85":"dataset.plot(subplots=True, figsize=(10, 10), sharex=False, sharey=False)\nplt.show()","69fe83f5":"random.seed(123)\n\ndef separate_data():\n    A = iris_dataset[0:40]\n    tA = iris_dataset[40:50]\n    B = iris_dataset[50:90]\n    tB = iris_dataset[90:100]\n    C = iris_dataset[100:140]\n    tC = iris_dataset[140:150]\n    train = np.concatenate((A,B,C))\n    test =  np.concatenate((tA,tB,tC))\n    return train,test\n\ntrain_porcent = 80 # Porcent Training \ntest_porcent = 20 # Porcent Test\niris_dataset = np.column_stack((iris_data.data,iris_data.target.T)) #Join X and Y\niris_dataset = list(iris_dataset)\nrandom.shuffle(iris_dataset)\n\nFiletrain, Filetest = separate_data()\n\ntrain_X = np.array([i[:4] for i in Filetrain])\ntrain_y = np.array([i[4] for i in Filetrain])\ntest_X = np.array([i[:4] for i in Filetest])\ntest_y = np.array([i[4] for i in Filetest])","cc2e2c8b":"import matplotlib.pyplot as plt\nimport matplotlib.cm as cm\n\n\nplt.subplot(1, 2, 1)\nplt.scatter(train_X[:,0],train_X[:,1],c=train_y,cmap=cm.viridis)\nplt.xlabel(iris_data.feature_names[0])\nplt.ylabel(iris_data.feature_names[1])\n\nplt.subplot(1, 2, 2)\nplt.scatter(train_X[:,2],train_X[:,3],c=train_y,cmap=cm.viridis)\nplt.xlabel(iris_data.feature_names[2])\nplt.ylabel(iris_data.feature_names[3])","f51512e1":"plt.subplot(1, 2, 1)\nplt.scatter(test_X[:,0],test_X[:,1],c=test_y,cmap=cm.viridis)\nplt.xlabel(iris_data.feature_names[0])\nplt.ylabel(iris_data.feature_names[1]) \n\nplt.subplot(1, 2, 2)\nplt.scatter(test_X[:,2],test_X[:,3],c=test_y,cmap=cm.viridis)\nplt.xlabel(iris_data.feature_names[2])\nplt.ylabel(iris_data.feature_names[3])","9ab10b08":"x = 0 \nativation = {(lambda x: 1\/(1 + np.exp(-x)))}\nderiv = {(lambda x: x*(1-x))}","00c0c8b4":"activation_tang = {(lambda x: np.tanh(x))}\nderiv_tang = {(lambda x: 1-x**2)}\n  ","88b1f584":"activation_ReLU = {(lambda x: x*(x > 0))}\nderiv_ReLU = {(lambda x: 1 * (x>0))}","327a6eb1":"from sklearn.base import BaseEstimator, ClassifierMixin, RegressorMixin\nimport random\n\nclass MultiLayerPerceptron(BaseEstimator, ClassifierMixin): \n    def __init__(self, params=None):     \n        if (params == None):\n            self.inputLayer = 4                        # Input Layer\n            self.hiddenLayer = 5                       # Hidden Layer\n            self.outputLayer = 3                       # Outpuy Layer\n            self.learningRate = 0.005                  # Learning rate\n            self.max_epochs = 600                      # Epochs\n            self.iasHiddenValue = -1                   # Bias HiddenLayer\n            self.BiasOutputValue = -1                  # Bias OutputLayer\n            self.activation = self.ativacao['sigmoid'] # Activation function\n            self.deriv = self.derivada['sigmoid']\n        else:\n            self.inputLayer = params['InputLayer']\n            self.hiddenLayer = params['HiddenLayer']\n            self.OutputLayer = params['OutputLayer']\n            self.learningRate = params['LearningRate']\n            self.max_epochs = params['Epocas']\n            self.BiasHiddenValue = params['BiasHiddenValue']\n            self.BiasOutputValue = params['BiasOutputValue']\n            self.activation = self.ativacao[params['ActivationFunction']]\n            self.deriv = self.derivada[params['ActivationFunction']]\n        \n        'Starting Bias and Weights'\n        self.WEIGHT_hidden = self.starting_weights(self.hiddenLayer, self.inputLayer)\n        self.WEIGHT_output = self.starting_weights(self.OutputLayer, self.hiddenLayer)\n        self.BIAS_hidden = np.array([self.BiasHiddenValue for i in range(self.hiddenLayer)])\n        self.BIAS_output = np.array([self.BiasOutputValue for i in range(self.OutputLayer)])\n        self.classes_number = 3 \n        \n    pass\n    \n    def starting_weights(self, x, y):\n        return [[2  * random.random() - 1 for i in range(x)] for j in range(y)]\n\n    ativacao = {\n         'sigmoid': (lambda x: 1\/(1 + np.exp(-x))),\n            'tanh': (lambda x: np.tanh(x)),\n            'Relu': (lambda x: x*(x > 0)),\n               }\n    derivada = {\n         'sigmoid': (lambda x: x*(1-x)),\n            'tanh': (lambda x: 1-x**2),\n            'Relu': (lambda x: 1 * (x>0))\n               }\n \n    def Backpropagation_Algorithm(self, x):\n        DELTA_output = []\n        'Stage 1 - Error: OutputLayer'\n        ERROR_output = self.output - self.OUTPUT_L2\n        DELTA_output = ((-1)*(ERROR_output) * self.deriv(self.OUTPUT_L2))\n        \n        arrayStore = []\n        'Stage 2 - Update weights OutputLayer and HiddenLayer'\n        for i in range(self.hiddenLayer):\n            for j in range(self.OutputLayer):\n                self.WEIGHT_output[i][j] -= (self.learningRate * (DELTA_output[j] * self.OUTPUT_L1[i]))\n                self.BIAS_output[j] -= (self.learningRate * DELTA_output[j])\n      \n        'Stage 3 - Error: HiddenLayer'\n        delta_hidden = np.matmul(self.WEIGHT_output, DELTA_output)* self.deriv(self.OUTPUT_L1)\n \n        'Stage 4 - Update weights HiddenLayer and InputLayer(x)'\n        for i in range(self.OutputLayer):\n            for j in range(self.hiddenLayer):\n                self.WEIGHT_hidden[i][j] -= (self.learningRate * (delta_hidden[j] * x[i]))\n                self.BIAS_hidden[j] -= (self.learningRate * delta_hidden[j])\n                \n    def show_err_graphic(self,v_erro,v_epoca):\n        plt.figure(figsize=(9,4))\n        plt.plot(v_epoca, v_erro, \"m-\",color=\"b\", marker=11)\n        plt.xlabel(\"Number of Epochs\")\n        plt.ylabel(\"Squared error (MSE) \");\n        plt.title(\"Error Minimization\")\n        plt.show()\n\n    def predict(self, X, y):\n        'Returns the predictions for every element of X'\n        my_predictions = []\n        'Forward Propagation'\n        forward = np.matmul(X,self.WEIGHT_hidden) + self.BIAS_hidden\n        forward = np.matmul(forward, self.WEIGHT_output) + self.BIAS_output\n                                 \n        for i in forward:\n            my_predictions.append(max(enumerate(i), key=lambda x:x[1])[0])\n            \n        array_score = []\n        for i in range(len(my_predictions)):\n            if my_predictions[i] == 0: \n                array_score.append([i, 'Iris-setosa', my_predictions[i], y[i]])\n            elif my_predictions[i] == 1:\n                 array_score.append([i, 'Iris-versicolour', my_predictions[i], y[i]])\n            elif my_predictions[i] == 2:\n                 array_score.append([i, 'Iris-virginica', my_predictions[i], y[i]])\n                    \n        dataframe = pd.DataFrame(array_score, columns=['_id', 'class', 'output', 'hoped_output'])\n        return my_predictions, dataframe\n\n    def fit(self, X, y):  \n        count_epoch = 1\n        total_error = 0\n        n = len(X); \n        epoch_array = []\n        error_array = []\n        W0 = []\n        W1 = []\n        while(count_epoch <= self.max_epochs):\n            for idx,inputs in enumerate(X): \n                self.output = np.zeros(self.classes_number)\n                'Stage 1 - (Forward Propagation)'\n                self.OUTPUT_L1 = self.activation((np.dot(inputs, self.WEIGHT_hidden) + self.BIAS_hidden.T))\n                self.OUTPUT_L2 = self.activation((np.dot(self.OUTPUT_L1, self.WEIGHT_output) + self.BIAS_output.T))\n                'Stage 2 - One-Hot-Encoding'\n                if(y[idx] == 0): \n                    self.output = np.array([1,0,0]) #Class1 {1,0,0}\n                elif(y[idx] == 1):\n                    self.output = np.array([0,1,0]) #Class2 {0,1,0}\n                elif(y[idx] == 2):\n                    self.output = np.array([0,0,1]) #Class3 {0,0,1}\n                \n                square_error = 0\n                for i in range(self.OutputLayer):\n                    erro = (self.output[i] - self.OUTPUT_L2[i])**2\n                    square_error = (square_error + (0.05 * erro))\n                    total_error = total_error + square_error\n         \n                'Backpropagation : Update Weights'\n                self.Backpropagation_Algorithm(inputs)\n                \n            total_error = (total_error \/ n)\n            if((count_epoch % 50 == 0)or(count_epoch == 1)):\n                print(\"Epoch \", count_epoch, \"- Total Error: \",total_error)\n                error_array.append(total_error)\n                epoch_array.append(count_epoch)\n                \n            W0.append(self.WEIGHT_hidden)\n            W1.append(self.WEIGHT_output)\n             \n                \n            count_epoch += 1\n        self.show_err_graphic(error_array,epoch_array)\n        \n        plt.plot(W0[0])\n        plt.title('Weight Hidden update during training')\n        plt.legend(['neuron1', 'neuron2', 'neuron3', 'neuron4', 'neuron5'])\n        plt.ylabel('Value Weight')\n        plt.show()\n        \n        plt.plot(W1[0])\n        plt.title('Weight Output update during training')\n        plt.legend(['neuron1', 'neuron2', 'neuron3'])\n        plt.ylabel('Value Weight')\n        plt.show()\n\n        return self","26302f0a":"def show_test():\n    ep1 = [0,100,200,300,400,500,600,700,800,900,1000,1500,2000]\n    h_5 = [0,60,70,70,83.3,93.3,96.7,86.7,86.7,76.7,73.3,66.7,66.7]\n    h_4 = [0,40,70,63.3,66.7,70,70,70,70,66.7,66.7,43.3,33.3]\n    h_3 = [0,46.7,76.7,80,76.7,76.7,76.6,73.3,73.3,73.3,73.3,76.7,76.7]\n    plt.figure(figsize=(10,4))\n    l1, = plt.plot(ep1, h_3, \"--\",color='b',label=\"node-3\", marker=11)\n    l2, = plt.plot(ep1, h_4, \"--\",color='g',label=\"node-4\", marker=8)\n    l3, = plt.plot(ep1, h_5, \"--\",color='r',label=\"node-5\", marker=5)\n    plt.legend(handles=[l1,l2,l3], loc=1)\n    plt.xlabel(\"number of Epochs\")\n    plt.ylabel(\"% Hits\")\n    plt.title(\"Number of Hidden Layers - Performance\")\n    \n    ep2 = [0,100,200,300,400,500,600,700]\n    tanh = [0.18,0.027,0.025,0.022,0.0068,0.0060,0.0057,0.00561]\n    sigm = [0.185,0.0897,0.060,0.0396,0.0343,0.0314,0.0296,0.0281]\n    Relu = [0.185,0.05141,0.05130,0.05127,0.05124,0.05123,0.05122,0.05121]\n    plt.figure(figsize=(10,4))\n    l1 , = plt.plot(ep2, tanh, \"--\",color='b',label=\"Hyperbolic Tangent\",marker=11)\n    l2 , = plt.plot(ep2, sigm, \"--\",color='g',label=\"Sigmoide\", marker=8)\n    l3 , = plt.plot(ep2, Relu, \"--\",color='r',label=\"ReLu\", marker=5)\n    plt.legend(handles=[l1,l2,l3], loc=1)\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Error\")\n    plt.title(\"Activation Functions - Performance\")\n    \n    fig, ax = plt.subplots()\n    names = [\"Hyperbolic Tangent\",\"Sigmoide\",\"ReLU\"]\n    x1 = [2.0,4.0,6.0]\n    plt.bar(x1[0], 53.4,0.4,color='b')\n    plt.bar(x1[1], 96.7,0.4,color='g')\n    plt.bar(x1[2], 33.2,0.4,color='r')\n    plt.xticks(x1,names)\n    plt.ylabel('% Hits')\n    plt.title('Hits - Activation Functions')\n    plt.show()","7070f300":"show_test()","6483d605":"dictionary = {'InputLayer':4, 'HiddenLayer':5, 'OutputLayer':3,\n              'Epocas':700, 'LearningRate':0.005,'BiasHiddenValue':-1, \n              'BiasOutputValue':-1, 'ActivationFunction':'sigmoid'}\n\nPerceptron = MultiLayerPerceptron(dictionary)\nPerceptron.fit(train_X,train_y)","d68748bf":"prev, dataframe = Perceptron.predict(test_X, test_y)\nhits = n_set = n_vers = n_virg = 0\nscore_set = score_vers = score_virg = 0\nfor j in range(len(test_y)):\n    if(test_y[j] == 0): n_set += 1\n    elif(test_y[j] == 1): n_vers += 1\n    elif(test_y[j] == 2): n_virg += 1\n        \nfor i in range(len(test_y)):\n    if test_y[i] == prev[i]: \n        hits += 1\n    if test_y[i] == prev[i] and test_y[i] == 0:\n        score_set += 1\n    elif test_y[i] == prev[i] and test_y[i] == 1:\n        score_vers += 1\n    elif test_y[i] == prev[i] and test_y[i] == 2:\n        score_virg += 1    \n         \nhits = (hits \/ len(test_y)) * 100\nfaults = 100 - hits","fdc65b5d":"dataframe","2dcb42fe":"graph_hits = []\nprint(\"Porcents :\",\"%.2f\"%(hits),\"% hits\",\"and\",\"%.2f\"%(faults),\"% faults\")\nprint(\"Total samples of test\",n_samples)\nprint(\"*Iris-Setosa:\",n_set,\"samples\")\nprint(\"*Iris-Versicolour:\",n_vers,\"samples\")\nprint(\"*Iris-Virginica:\",n_virg,\"samples\")\n\ngraph_hits.append(hits)\ngraph_hits.append(faults)\nlabels = 'Hits', 'Faults';\nsizes = [96.5, 3.3]\nexplode = (0, 0.14)\n\nfig1, ax1 = plt.subplots();\nax1.pie(graph_hits, explode=explode,colors=['green','red'],labels=labels, autopct='%1.1f%%',\nshadow=True, startangle=90)\nax1.axis('equal')\nplt.show()","23af3bea":"acc_set = (score_set\/n_set)*100\nacc_vers = (score_vers\/n_vers)*100\nacc_virg = (score_virg\/n_virg)*100\nprint(\"- Acurracy Iris-Setosa:\",\"%.2f\"%acc_set, \"%\")\nprint(\"- Acurracy Iris-Versicolour:\",\"%.2f\"%acc_vers, \"%\")\nprint(\"- Acurracy Iris-Virginica:\",\"%.2f\"%acc_virg, \"%\")\nnames = [\"Setosa\",\"Versicolour\",\"Virginica\"]\nx1 = [2.0,4.0,6.0]\nfig, ax = plt.subplots()\nr1 = plt.bar(x1[0], acc_set,color='orange',label='Iris-Setosa')\nr2 = plt.bar(x1[1], acc_vers,color='green',label='Iris-Versicolour')\nr3 = plt.bar(x1[2], acc_virg,color='purple',label='Iris-Virginica')\nplt.ylabel('Scores %')\nplt.xticks(x1, names);plt.title('Scores by iris flowers - Multilayer Perceptron')\nplt.show()","3f73ff02":"# <center> Machine Learning <\/center>\n# <center> Multilayer Perceptron from Scratch <\/center>","c90285e3":"## 4.2. Plot our test Samples","dc4e182e":"## Step 3. Accuracy and precision the Multilayer Perceptron","804f3ba4":"# Training the Artificial Neural Network(MLP)\n\n## Step 1: training our MultiLayer Perceptron","0d688b28":"# 6. Implementation the Multilayer Perceptron in Python\n","827fc291":"### Step 4. Backpropagation Algorithm\n<img src=\"https:\/\/sebastianraschka.com\/images\/faq\/visual-backpropagation\/backpropagation.png\">\n### In Output Layer,  $L = 2:$\n   - **Step 1**: Calculate error in output layer: $\\delta^{(L2)} = -({d_j}^{(L2)} - {y_j}^{(L2)})\\cdot\n   g'({S_j}^{(L2)})$\n   \n      `\n      ERROR_output = self.OUTPUT - self.OUTPUT_L2\n      DELTA_output = ((-1)*(ERROR_output) * self.deriv(self.OUTPUT_L2))\n      `\n      \n\n   - **Step 2**: Update all weight between hidden and output layer: $W^{(L2)} = W^{(L2)} -\\gamma \\cdot(\\delta^{(L2)}  - {S_j}^{(L1)})$\n   \n         for i in range(self.hiddenLayer):`\n           ` for j in range(self.OutputLayer):`\n               ` self.WEIGHT_output[i][j] -= (self.learningRate * (DELTA_output[j] * self.output_l1[i]))`\n               ` self.BIAS_output[j] -= (self.learningRate * DELTA_output[j])`\n               \n   - **Step 3**: Update bias value in output layer: $bias^{(L2)} = bias^{(L2)} - \\gamma \\cdot \\delta^{(L2)}$\n   \n### In Input Layer , $L = 1$:\n   - **Step 4**: Calculate error in hidden layer: $\\delta^{(L1)} = W^{(L2)} \\cdot \\delta^{(L2)} \\cdot g'({S_j}^{(L1)})$\n     \n   `delta_hidden = np.matmul(self.WEIGHT_output, DELTA_output) * self.deriv(self._l1)`\n   - **Step 5**: Update all weight between hidden and output layer: $W^{(L1)} = W^{(L1)} -\\gamma \\cdot(\\delta^{(L1)}  - {X_i})$\n         `for i in range(self.OutputLayer):`\n           `for j in range(self.hiddenLayer):`\n               `self.WEIGHT_hidden[i][j] -= (self.learningRate * (DELTA_hidden[j] * INPUT[i]))`\n               `self.BIAS_hidden[j] -= (self.learningRate * DELTA_hidden[j])`\n   - **Step 6**: Update bias value in output layer: $bias^{(L1)} = bias^{(L1)} - \\gamma \\cdot \\delta^{(L1)}$","c221e35d":"## 3.2. An analysis about the Iris Flower Dataset\n","0e6bfb64":"### ReLU Function:","20cfced6":"\n\n# 5. Multilayer Perceptron\n\n<p style=\"text-align: justify;\">Artificial neural networks (ANNs) or connectionist systems are computing systems inspired by the biological neural networks that constitute animal brains. Such systems learn (progressively improve performance) to do tasks by considering examples, generally without task-specific programming. For example, in image recognition, \"they might learn to identify images that contain cats by analyzing example images that have been manually labeled as \"cat\" or \"no cat\" and using the analytic results to identify cats in other images\".<\/p>\n\n<p style=\"text-align: justify;\">They have found most use in applications difficult to express in a traditional computer algorithm using rule-based programming. An ANN is based on a collection of connected units called artificial neurons, (analogous to axons in a biological brain). Each connection (synapse) between neurons can transmit a signal to another neuron. The receiving (postsynaptic) neuron can process the signal(s) and then signal downstream neurons connected to it.<\/p>\n\n<p style=\"text-align: justify;\"> More information here: [Artificial Neural Network](https:\/\/en.wikipedia.org\/wiki\/Artificial_neural_network)<\/p>\n\n<img src=\"https:\/\/miro.medium.com\/max\/1072\/1*DOkHU_dgXMCybA6WWXrp4g.gif\"\/>\n\n<p style=\"text-align: justify;\">The Multilayer Perceptron Networks are characterized by the presence of many intermediate layers (hidden) in your structure, located between input layer and output layer. With this, such networks have the advantage of being able to classify more than two different classes and It also solve non-linearly separable problems.<\/p>\n<div class=\"container-fluid\"><div class=\"row\">\n      <div class=\"col-md-2\" align='center'><\/div>\n      <div class='col-md-8' align='center'>\n           <img src='http:\/\/ffden-2.phys.uaf.edu\/212_fall2003.web.dir\/Keith_Palchikoff\/multilayer%20perceptron.JPG' \/>\n      <\/div><div class=\"col-md-2\" align='center'><\/div>\n  <\/div>\n<\/div>","cafaf002":"# About this notebook\n\nThis notebook kernel was created to help you understand more about machine learning. I intend to create tutorials with several machine learning algorithms from basic to advanced. I hope I can help you with this data science trail. For any information, you can contact me through the link below.\n\nContact me: https:\/\/www.linkedin.com\/in\/vitorgamalemos\/\n\n**Other noteboks about neural networks:** \n  - Simple Perceptron: https:\/\/www.kaggle.com\/vitorgamalemos\/neural-network-01-simple-perceptron\n  - Multilayer Perceptron: https:\/\/www.kaggle.com\/vitorgamalemos\/neural-network-02-multilayer-perceptron\n  - Convolutional neural network: https:\/\/www.kaggle.com\/vitorgamalemos\/object-recognition-using-convolutional-network\n  - GANs: https:\/\/www.kaggle.com\/vitorgamalemos\/generating-digits-with-gans\n  \n  \n\n# 1. Introduction about Iris Flower \n\n<p style=\"text-align: justify;\">The Iris Flower Dataset, also called Fisher\u2019s Iris, is a dataset introduced by Ronald Fisher, a British statistician, and biologist, with several contributions to science. Ronald Fisher has well known worldwide for his paper The use of multiple measurements in taxonomic problems as an example of linear discriminant analysis. It was in this paper that Ronald Fisher introduced the Iris flower dataset.<\/p>\n\n<p style=\"text-align: justify;\">The iris database consists of 50 samples distributed among three different species of iris. Each of these samples has specific characteristics, which allows them to be classified into three categories: Iris Setosa, Iris Virginica, and Iris versicolor. In this tutorial, we will use multilayer perceptron to separate and classify the iris samples.<\/p>\n\n- The data set consists of 50 samples from each of three species of Iris (Iris setosa, Iris virginica, and Iris versicolor).\n\n- Four features were measured from each sample, the length and the width of the sepals and petals, in centimeters.\n\n- Based on the combination of these four features, Fisher developed a linear discriminant model to distinguish the species from each other.\n\nFor this example, I will implement a multilayer perceptron without any Python libraries. However, to help us format and manipulate the iris data set, we will use numpy, matplotlib, seaborn, and scikit-learn libraries.","9e627aa9":"# 2. Artificial Neural Networks\n\n<p style=\"text-align: justify;\">Artificial Neural Networks are mathematical models inspired by the human brain, specifically the ability to learn, process, and perform tasks. The Artificial Neural Networks are powerful tools that assist in solving complex problems linked mainly in the area of combinatorial optimization and machine learning. In this context, artificial neural networks have the most varied applications possible, as such models can adapt to the situations presented, ensuring a gradual increase in performance without any human interference. We can say that the Artificial Neural Networks are potent methods can give computers a new possibility, that is, a machine does not get stuck to preprogrammed rules and opens up various options to learn from its own mistakes. <\/p>\n","2c90a13a":"### Step 2: Calculation our Erro function \n<img src=\"https:\/\/miro.medium.com\/max\/920\/1*jYQYuHpHdkZqNFQKJSuDTw.png\">\nIt is used to measure performance locality associated with the results produced by the neurons in output layer and the expected result.\n$$\nE(k) = \n\\frac{1}{2} \\sum_{k=1}^{K}({{d_j(k)}} - {y_j}{(k)})^2.\n$$","8afca74f":"## Step 4. Score for each one of the samples","7aeb6f1d":"<img src=\"https:\/\/thumbs.gfycat.com\/FickleHorribleBlackfootedferret-small.gif\">","d9baa4f7":" ### Hyperbolic Tangent Function:","cc9b89a1":"# 4. Manually separating our dataset\n\nIt is here that we will select our samples to train and test the algorithms: **80% Training Samples and 20% Test**\n<div class=\"container-fluid\">\n  <div class=\"row\">\n      <div class=\"col-md-2\" align='center'>\n      <\/div>\n      <div class='col-md-8' align='center'>\n      <\/div>\n      <div class=\"col-md-2\" align='center'><\/div>\n  <\/div>\n<\/div>","14e6c8ce":"## 3.1. Some Python Libraries \n\n<p style=\"text-align: justify;\">In the first place, Let's define some libraries to help us in the manipulation the data set, such as `numpy`, `matplotlib`, `seaborn` and `scikit-learn`. In this tutorial, I am implementing a Multilayer Perceptron without any framework like Keras or similar ones. The goal here is to be as simple as possible! So to help you with this task, we implementing the neural network without using ready-made libraries. You can use numpy to work with array operations! There is no problem it! <\/p>","74d5d658":"## Step 2: testing our results ","786c1ceb":"## Finding the best parameters \n\n<p style=\"text-align: justify;\">For find the best parameters, it was necessary to realize various tests using different values to the parameters. The graphs below denote all tests made to select the best configuration for the multilayer perceptron. These tests were important in selecting the best settings and ensuring the best accuracy. The graph was drawn manually, but you can change the settings and note the results obtained. The tests involve different activation functions and the number of neurons for each layer.<\/p>","ac447772":"# 3. How implement a Multilayer Perceptron","8c12bd36":"## 4.1. Plot our training Samples","3c1967b5":"## 5.1. How does Multilayer Perceptron work? \n\n<p style=\"text-align: justify;\"> We can summarize the operation of the perceptron as follows it:<\/p>\n\n  - **Step 1**: Initialize the weights and bias with small-randomized values;\n  - **Step 2**: Propagate all values in the input layer until output layer(Forward Propagation)\n  - **Step 3**: Update weight and bias in the inner layers(Backpropagation)\n  - **Step 4**: Do it until that the stop criterion is satisfied !\n  \n### Step 1: Forward propagation Algorithm\n<img src=\"https:\/\/sebastianraschka.com\/images\/faq\/visual-backpropagation\/forward-propagation.png\">\n\nIn order to proceed we need to improve the notation we have been using. That for, for each layer $1\\geq l\\geq L$, the activations and outputs are calculated as:\n\n$$\n\\text{L}^l_j = {\\sum_i w^l_{ji} x^l_i\\, = w^l_{j,0} x^l_0 + w^l_{j,1} x^l_1 + w^l_{j,2} x^l_2 + ... + w^l_{j,n}} x^l_n,\n$$\n$$Y^l_j = g^l(\\text{L}^l_j)\\,,$$\n\n$$\\{y_{i},\\,x_{i1},\\ldots ,x_{ip}\\}_{i=1}^{n}$$\n\nwhere:\n\n* $y^l_j$ is the $j-$th output of layer $l$,\n* $x^l_i$ is the $i$-th input to layer $l$,\n* $w^l_{ji}$ is the weight of the $j$-th neuron connected to input $i$,\n* $\\text{L}^l_{j}$ is called net activation, and\n* $g^l(\\cdot)$ is the activation function of layer $l$.","dfd4fd9b":"### Step 3. Activation Functions\n<img src=\"https:\/\/miro.medium.com\/max\/1192\/1*4ZEDRpFuCIpUjNgjDdT2Lg.png\">\n\n### Sigmoid Function:"}}