{"cell_type":{"a0bdb3ea":"code","dfee8983":"code","4c678861":"code","f7b4a97c":"code","34f52dd5":"code","00d6228d":"code","27321037":"code","54b4b8df":"code","8792248b":"code","48c844d3":"code","bc5eb808":"code","3fa17d6f":"code","5ed17d21":"code","521392e0":"code","56feb8c6":"code","f1858197":"code","0a4df41e":"code","69a720ef":"code","c8c2945e":"code","10222ead":"code","01a4f219":"code","6e43c362":"code","6a4cd67f":"code","a67c181a":"code","beee3276":"code","acfe204a":"code","32cb5679":"code","ab283274":"code","aae6248d":"code","0e219f60":"code","c9af9e32":"code","fac2546d":"code","11820777":"code","52767e5f":"code","99c75352":"code","14004baa":"code","fb83bdf4":"code","c29c9afb":"code","1c9d07a5":"code","eee78924":"code","f958101b":"code","53818e06":"code","fc83cf4c":"code","4dae0a4e":"code","a3157569":"code","16af62b5":"code","35c50880":"code","72eb22a6":"code","02171718":"code","2513cedd":"code","8abf3808":"code","1b4ebfc6":"code","02a43011":"code","63f0b0f3":"code","3c38bed8":"code","5b6bc32a":"code","62f88f9b":"code","d5a2c7c9":"code","ff75d9ec":"code","2e7c9e97":"code","efdbaa2a":"code","57c3fa95":"code","51c5c8da":"code","ae7756bf":"code","8682cffc":"code","c33d202a":"code","72e08abf":"code","76d4dee3":"code","8434c955":"code","c804fe50":"code","7fd40a36":"markdown","7eeb2261":"markdown","1b5e1446":"markdown","739cc4a0":"markdown","af5d73cf":"markdown","e7cc8156":"markdown","44ae9197":"markdown","d7beacf6":"markdown","0af8d081":"markdown","ac66501f":"markdown","74df8ebd":"markdown","4931775c":"markdown","dc2f47a7":"markdown","8415157a":"markdown","5589f665":"markdown","2555bd48":"markdown","6e8ed211":"markdown","e782a18f":"markdown","aefe5245":"markdown","452e7c02":"markdown","618db49d":"markdown","0777fe7c":"markdown","2eedd6b9":"markdown","c0a0c81c":"markdown","5b73fb67":"markdown","80f08f02":"markdown","ba8c2beb":"markdown","27aeda5d":"markdown","85dfd6f5":"markdown","70d4e212":"markdown","46672449":"markdown","1b47daca":"markdown","980e76fc":"markdown","ed6b39a7":"markdown","17b42b70":"markdown","2aaba0f0":"markdown","31cf9e78":"markdown","a5a0b7a9":"markdown","0569c149":"markdown","b1e89f23":"markdown","12118460":"markdown","ce5e411d":"markdown","7571f087":"markdown","7f019d97":"markdown","19551193":"markdown","941fb77b":"markdown","4c6c1d46":"markdown","0a4408f8":"markdown","9b002d73":"markdown","269cddeb":"markdown","5050d96b":"markdown","6f4f6e3c":"markdown","0ee27b27":"markdown","13d24a58":"markdown","00820366":"markdown","a66040a3":"markdown","66242a36":"markdown","576b4ed0":"markdown","6acd0e20":"markdown","4bd5b7e7":"markdown","99f14869":"markdown","da72bdd0":"markdown","afb49ee5":"markdown","e9e67581":"markdown","7025026e":"markdown","cbc26890":"markdown","e42bd5cb":"markdown","b4729dfb":"markdown","893fcad5":"markdown"},"source":{"a0bdb3ea":"# libs\nimport pandas as pd\nimport numpy as np\nimport geopandas as gpd\nimport os\nimport re\nimport math\nimport folium\nimport pickle\nfrom collections import namedtuple\nimport plotly.plotly as py\nimport plotly.graph_objs as go\nfrom plotly.offline import download_plotlyjs, init_notebook_mode, plot, iplot\nfrom functools import reduce\nfrom shapely.geometry import Point\nfrom geopy.geocoders import Nominatim\nfrom geopy.distance import vincenty\nfrom IPython.display import Image, display, HTML\nfrom IPython.core.pylabtools import figsize\nimport matplotlib.pyplot as plt\nimport matplotlib\nfrom scipy.spatial import ConvexHull\nimport seaborn as sns\nfrom tqdm import tqdm_notebook as tqdm\nimport dill\n\npd.options.display.max_rows = 10\n\ninit_notebook_mode(connected=True)\n%matplotlib inline","dfee8983":"# Custom pipe Functions\ndef csnap(df, fn=lambda x: x.shape, msg=None):\n    \"\"\" Custom Help function to print things in method chaining.\n        Returns back the df to further use in chaining.\n    \"\"\"\n    if msg:\n        print(msg)\n    display(fn(df))\n    return df\n    \ndef cfilter(df, fn, axis='rows'):\n    \"\"\" Custom Filters based on a condition and returns the df.\n        function - a lambda function that returns a binary vector\n        thats similar in shape to the dataframe\n        axis = rows or columns to be filtered.\n        A single level indexing\n    \"\"\"\n    if axis == 'rows':\n        return df[fn(df)]\n    elif axis == 'columns':\n        return df.iloc[:, fn(df)]\n    \ndef ccol(df, string, sep=';'):\n    \"\"\" Custom column filtering that works in multi level indexing.\n    \"\"\"\n    return df.iloc[:, reduce(lambda x, y: x & y,\n                          [(df.columns.get_level_values(i).to_series()\n                                 .str.contains(string, case=False)\n                                 .fillna(True).values)\n                            for i, string in enumerate(string.split(sep))])]\n\ndef setcols(original, fn=lambda x: x, cols=None):\n    \"\"\"Sets the column of the data frame to the passed column list.\n    \"\"\"\n    df = original.copy()\n    if cols:\n        df.columns = cols\n    elif isinstance(df.columns, pd.core.index.MultiIndex):\n        df.columns = ['_'.join(map(fn, args)).strip('_') \n                         for args in zip(*[df.columns.get_level_values(i).fillna('') \n                                           for i in range(len(df.columns.levels))])]\n    elif not isinstance(df.columns, pd.core.index.MultiIndex):\n        df.columns = fn(df)\n    return df","4c678861":"# data codes\ndc = \\\n{\n    'edu': \"S1501\",\n    'edu_25': \"B15003\",\n    'housing': \"S2502\",\n    'poverty': \"S1701\",\n    'rsa': \"DP05\",\n    'employement': \"S2301\",\n    'income': \"S1903\"\n}\n\n# city codes\ncc = \\\n{\n    'boston': \"11-00091\",\n    'indianapolis': \"23-00089\",\n    'minneapolis': \"24-00013\",\n    'st_paul': \"24-00098\",\n    'orlando': \"35-00016\",\n    'charlotte': \"35-00103\",\n    'austin': \"37-00027\",\n    'dallas': \"37-00049\",\n    'seattle': \"49-00009\",\n    'los_angeles': \"49-00033\",\n    'oakland': \"49-00035\",\n    'san_francisco': \"49-00081\"\n}\n\ncity_state = [\"boston, Massachusetts\", \"indianapolis, indiana\", \"charlotte, north carolina\",\n              \"austin, texas\", \"dallas, texas\", \"seattle, washington\", \"minneapolis, minnesota\",\n             \"st paul, minnesota\", \"orlando, florida\", \"los angeles, california\",\n              \"oakland, california\", \"san francisco, california\"]\n\n# data containers\ndata_list = list(dc.keys())\n\ndepts = dict()\nuof = dict()\n\nis_uof_present = dict()\n\ndept = namedtuple('dept', data_list)\nacs_data = namedtuple('acs_data', ['data', 'meta'])","f7b4a97c":"# reading data\ndata_dir = '..\/input\/data-science-for-good\/cpe-data\/'\ndept_paths = [f.path for f in os.scandir(data_dir)\n                         if f.is_dir()]\nndepts = len(dept_paths)\ndept_list = [None] * len(dept_paths)\nprint(f\"Processing {ndepts} PD's\")\nshape_folder = []\n\ndept_cnt = 0\nfor path in tqdm(dept_paths):\n    try:\n        dept_folder = path.split('\/')[-1]\n        folder = ''\n        dept_code = re.findall(r\"_(\\d{2}-\\d{5})\", dept_folder)[0]\n        dept_list[dept_cnt] = dept_code\n\n        acs_folder = [f.path for f in os.scandir(path)\n                      if f.is_dir()\n                      and (f.path).lower().find('acs') > 0][0]\n        shape_folder.append([f.path for f in os.scandir(path)\n                        if f.is_dir()\n                        and (f.path).lower().find('shape') > 0][0])\n\n        acs_dl = [None] * len(dc.keys())\n        for folder in os.scandir(acs_folder):\n            if folder.is_dir():\n                file_code = ''\n                for sub_folder in os.scandir(folder.path):\n                    if (sub_folder.path).split('\\\\')[-1][0] != \"_\":\n                        file_code = re.findall(r\"5YR_([A-Z\\d]*)_\",\n                                               sub_folder.path,\n                                               flags=re.IGNORECASE)\n                    if sub_folder.path.lower().find('meta') > 0:\n                        meta = pd.read_csv(sub_folder.path,\n                                           low_memory=False)\n                    elif sub_folder.path.lower().find('ann') > 0:\n                        data = pd.read_csv(sub_folder.path,\n                                           skiprows=1,\n                                           low_memory=False)\n                        \n                data_meta = acs_data(data, meta)\n\n                if len(file_code) == 1:\n                    for k,v in dc.items():\n                        if v == file_code[0]:\n                            acs_dl[data_list.index(k)] = data_meta\n        \n        # Checks for a file in the folder and sees if it has known headers.\n        uof_path = [f.path for f in os.scandir(path)\n                      if f.is_file()\n                      and (f.path).lower().find('csv') > 0]\n        \n        #temp = pd.read_csv(uof_path[0])\n        #if temp.columns.str.contains('incident|subject', case=False)\n        \n        if len(uof_path) == 1:\n            uof[dept_code] = pd.read_csv(uof_path[0],\n                                         low_memory=False)\n            is_uof_present[dept_code] = True\n        else:\n            is_uof_present[dept_code] = False\n                \n        depts[dept_code] = dept(*acs_dl)\n        dept_cnt += 1\n    except:\n        print(f\"{path} - {folder} has unexpected structure\")","34f52dd5":"depts[cc[\"dallas\"]].poverty.data.head(2)","00d6228d":"dept_shapes, has_req_files, is_consistent = dict(), dict(), dict()","27321037":"# shape file load\nfor sub_folder in shape_folder:\n    req_file = True\n    consistent_flag = False\n    try:\n        dept_code = re.findall(r\"_(\\d{2}-\\d{5})\", sub_folder)[0]\n        file = ''\n        has_req_files[dept_code] = len([file.path for file in os.scandir(sub_folder)\n                               for ext in '.shp|.shx|.dbf'.split('|')\n                               if (file.path).endswith(ext)]) >= 3\n\n        for file in os.scandir(sub_folder):\n            if (file.path).endswith('.shp'):\n                dept_shapes[dept_code] = gpd.read_file(file.path)   \n                if (~((dept_shapes[dept_code].geometry.type == 'Polygon') |\n                        (dept_shapes[dept_code].geometry.type == 'MultiPolygon'))).sum() != 0:\n                    is_consistent[dept_code] = False\n                    print(f\"{file.path} has inconsistent geometry\")\n                else:\n                    is_consistent[dept_code] = True\n    except:\n        print(f\"{sub_folder} - {file} has incorrect structure\")\n","54b4b8df":"dept_shapes[cc[\"charlotte\"]]= gpd.read_file('..\/input\/cpe-external-adiamaan\/external\/CMPD_Police_Divisions\/CMPD_Police_Divisions.shp')","8792248b":"census = dict()","48c844d3":"#read census\ndest_crs = {'init': 'epsg:4326'}\nfor folder in os.scandir('..\/input\/cpe-external-adiamaan\/external\/census'):\n    if folder.path.split('\/')[-1].startswith('cb'):\n        state = re.findall(r'cb_2017_(.*)_tract', folder.path, flags=re.IGNORECASE)\n        for file in os.scandir(folder):\n            if (file.path).endswith('.shp'):\n                sp = gpd.read_file(file.path)  \n                print(f\"{file.path} = {sp.crs}\")\n                if sp.crs:\n                    sp = sp.to_crs(dest_crs)\n                if (~((sp.geometry.type == 'Polygon') |\n                        (sp.geometry.type == 'MultiPolygon'))).sum() != 0:\n                    print(f\"{files} has inconsistent geometry\")\n                else:\n                    census[state[0]] = sp","bc5eb808":"census = \\\n(\n    pd.concat(census.values())\n        .pipe(setcols, lambda x: x.columns.str.lower())\n        .pipe(csnap, lambda x: x.sample(5))\n)","3fa17d6f":"is_acs_data_consistent = dict()","5ed17d21":"for data in data_list:\n    is_acs_data_consistent[cc['boston']] = True\n    for x,y in zip(['boston']*len(cc), list(cc.keys())[1:]):\n        consistent = True\n        if (getattr(depts[cc[x]], data).meta != \\\n                getattr(depts[cc[y]], data).meta).sum().sum() == 0 :\n            consistent = False\n        is_acs_data_consistent[cc[y]] = True","521392e0":"data_dict = ()","56feb8c6":"data_dict = \\\n{data:pd.concat(\n[(getattr(depts[cc[x]], data).data\n              .assign(City = x)) for x in list(cc.keys())])\n    for data in data_list}","f1858197":"for _, x in data_dict.items():\n    x.columns = x.columns.str.split(r';\\s|\\s-\\s', expand=True)","0a4df41e":"for _, df in data_dict.items():\n    char_cols = (df.columns.get_level_values(0)\n                     .str.contains('geo|id|city',\n                                   case=False))\n    df.iloc[:, ~char_cols] = (df.iloc[:, ~char_cols]\n                                .apply(lambda x: \n                                       pd.to_numeric(x, errors=\"coerce\"),\n                                       axis=1))","69a720ef":"for key, value in data_dict.items():\n    data_dict[key] = \\\n    (value.assign(id2_str = lambda x: x.Id2.astype(str))\n        .assign(statefp = lambda x: x.id2_str.str[:2],\n                countyfp = lambda x: x.id2_str.str[2:5],\n                tractce = lambda x: x.id2_str.str[5:]))","c8c2945e":"orig_crs = dict()\norig_crs = {city:dept_shapes[cc[city]].crs\n                for city,_ in cc.items()}\n[print(f\"{k} = {dept_shapes[cc[k]].crs}\")\n    for k,_ in cc.items()];","10222ead":"[print(f\"{k} = {dept_shapes[cc[k]].geometry.head(1)}\")\n     for k,_ in cc.items()];","01a4f219":"epsg = \\\n(\n    pd.read_csv('..\/input\/cpe-external-adiamaan\/external\/epsg_code.csv')    \n        .pipe(csnap, lambda x: x.sample(5))\n)","6e43c362":"(\n    dept_shapes[cc[\"boston\"]]\n            .pipe(csnap, lambda x: x.head(2),\n                  msg=\"Original\")\n            .to_crs(epsg='4326')\n            .pipe(csnap, lambda x: x.head(2),\n                  msg=\"In GCS\")\n            .to_crs(epsg='2249')\n            .pipe(csnap, lambda x: x.head(2),\n                  msg=\"Original with negligible loss\")\n);","6a4cd67f":"city_cs = dict()\ncoord_system = 'gcs'\nfor city in cc.keys():\n    sp = dept_shapes[cc[city]]\n    ndigits = max([math.floor(math.log10(abs(n))) + 1\n                     for n in sp.geometry.bounds.max().values])\n    if ndigits in [2,3]:\n        city_cs[city] = 'gcs'\n    elif ndigits in [6,7,8]:\n        city_cs[city] = 'pcs'\n    else:\n        print(f\"{city} unrecognized coordinate system\")\n            \n    if sp.crs:\n        sp = sp.to_crs(dest_crs)\n    elif not sp.crs and city_cs[city] == 'gcs':\n        sp = sp.to_crs(dest_crs)\n\n    dept_shapes[cc[city]] = sp","a67c181a":"city_cs","beee3276":"locations = dict()","acfe204a":"#uncomment code to automatically fetch the lat\/long of the cities\n# %%time\n# geolocator = Nominatim(user_agent=\"cpe_mak_kaggle_kernel\")\n# for i, loc in enumerate(city_state):\n#     city = loc.split(\",\")[0]\n#     locations[city] = geolocator.geocode(loc)\n\n#comment below\nwith open('..\/input\/cpe-external-adiamaan\/locations.pkl', 'rb') as handle:\n    locations = pickle.load(handle)","32cb5679":"locations","ab283274":"# logic demo\nboston_bounds =  dept_shapes[cc[\"boston\"]].bounds\nminx, miny, maxx, maxy = \\\nboston_bounds.minx.min(), boston_bounds.miny.min(),\\\nboston_bounds.maxx.max(), boston_bounds.maxy.max()\ncorners = [(miny, minx), (miny, maxx),\n           (maxy, minx), (maxy, maxx)]\n\nffig = folium.Figure(height=400)\nfmap = folium.Map(location=[42.3248716,-71.102893],\n                  zoom_start=10.45, tiles= \"CartoDB positron\").add_to(ffig)\n\n(folium.GeoJson(dept_shapes[cc[\"boston\"]],\n               style_function=lambda x: {'fillColor': 'grey',\n                                       'color': 'Black',\n                                       'fillOpacity': 0.30,\n                                       'weight':0.75},)\n    .add_to(fmap))\n\nfolium.CircleMarker(location=[locations['boston'].latitude, locations['boston'].longitude],\n                     radius=10,\n                    popup='Boston City', \n                    fill_color='blue', fill_opacity= 1.0).add_to(fmap)\n\nfor corner in corners:\n    folium.CircleMarker(location=[corner[0], corner[1]],\n                     radius=10,\n                    fill_color='red', fill_opacity= 1.0).add_to(fmap)\n    folium.PolyLine([[corner[0], corner[1]], \n                    [locations['boston'].latitude,\n                     locations['boston'].longitude]],\n                   color='blue').add_to(fmap)\n\nfolium.PolyLine([[miny, minx], [maxy, minx],\n                 [maxy, maxx], [miny, maxx],\n                [miny, minx]], color=\"red\").add_to(fmap)\n    \nfmap","aae6248d":"%%time\nfor city in cc.keys():\n    sp = dept_shapes[cc[city]].copy()\n    if city_cs[city] == 'pcs' and not sp.crs:\n            total_distance = [None] * epsg.shape[0]\n            for i,code in enumerate(epsg.crs_code):\n                sp = dept_shapes[cc[city]].copy()\n                crs = {'init': 'epsg:' + str(code)}\n                sp.crs = crs\n                sp = sp.to_crs(dest_crs)\n                bounds = sp.bounds\n                minx, miny, maxx, maxy = \\\n                bounds.minx.min(), bounds.miny.min(),\\\n                bounds.maxx.max(), bounds.maxy.max()\n                corners = [(miny, minx), (miny, maxx),\n                           (maxy, minx), (maxy, maxx)]\n                distance = 0\n                for lat, long in corners:\n                    distance += vincenty((locations[city].latitude,\n                                         locations[city].longitude),\n                                         (lat, long)).miles\n                total_distance[i] = distance\/4\n            print(epsg.iloc[total_distance.index(min(total_distance)), 0], city)\n            correct_crs_code = epsg.iloc[total_distance.index(min(total_distance)), 0]\n            correct_crs = {'init': 'epsg:' + str(correct_crs_code)}\n            orig_crs[city] = correct_crs\n            dept_shapes[cc[city]].crs = correct_crs\n            dept_shapes[cc[city]] = dept_shapes[cc[city]].to_crs(dest_crs)","0e219f60":"[print(f\"{k} = {dept_shapes[cc[k]].crs}\")\n     for k,_ in cc.items()];","c9af9e32":"# distance calc\nfor city in cc.keys():\n    loc_city = city.replace('_', ' ')\n    bounds = dept_shapes[cc[city]].bounds\n    minx, miny, maxx, maxy = \\\n    bounds.minx.min(), bounds.miny.min(),\\\n    bounds.maxx.min(), bounds.maxy.min()\n    corners = [(miny, minx), (miny, maxx),\n               (maxy, minx), (maxy, maxx)]\n    total_distance = 0\n    for lat, long in corners:\n        total_distance += vincenty((locations[loc_city].latitude,\n                                    locations[loc_city].longitude),\n                                   (lat, long)).miles\n    print(f\"{city} - {total_distance\/4:.2f} miles\")","fac2546d":"# wrapping plotters\ndef folium_map(data, style_function, **kwargs):\n    fig_pars = ['height', 'width']\n    fig_args = {k:kwargs[k] for k in kwargs.keys() if k in fig_pars}\n    kwargs = {k:kwargs[k] for k in kwargs.keys() if k not in fig_pars}\n    \n    ffig = folium.Figure(**fig_args)\n    fmap = folium.Map(**kwargs).add_to(ffig)\n    \n    (folium.GeoJson(data,\n               style_function=style_function)\n        .add_to(fmap))\n    \n    display(fmap)\n    return fmap\n\ndef cplotly(df, traces, **kwargs):\n    config={'showLink': False}\n    \n    layout = go.Layout(**kwargs)\n    fig = go.Figure(traces(df), layout)\n    \n    display(iplot(fig, config=config))\n    return df\n\ndef cmatplot(df, **kwargs):\n    display(df.plot(**kwargs))\n    return df","11820777":"# codes\nmap_latlong = dict(boston = [42.3248716,-71.102893],\n                     indianapolis = [39.81, -86.15],\n                     charlotte = [35.2030728,-80.8799136],\n                     dallas = [32.8203525,-96.811731],\n                     austin = [30.308179,-97.8184848],\n                     seattle = [47.6129432,-122.3821475],\n                     minneapolis = [44.9706756,-93.3315183],\n                     st_paul = [44.9396219,-93.2461368],\n                     orlando = [28.4810971,-81.5088354],\n                     los_angeles = [34.0201613,-118.6919205],\n                     oakland = [37.7583925,-122.3754124],\n                     san_francisco = [37.7576171,-122.5776844])\n\nmap_color = dict(boston = 'red',\n                     indianapolis = 'blue',\n                     charlotte = 'green',\n                     dallas = 'yellow',\n                     austin = 'beige',\n                     seattle = 'pink',\n                     minneapolis = 'brown',\n                     st_paul = 'teal',\n                     orlando = 'purple',\n                     los_angeles = 'orange', \n                     oakland = 'lightgreen',\n                     san_francisco = 'lightblue')","52767e5f":"ncities = len(cc.keys())\naxis_list =  ['axis_' + str(i) for i in range(ncities)]\nfigure, axis_list = plt.subplots(nrows=ncities\/\/3, ncols=3)\n\nfor i, city in enumerate(cc.keys()):\n    j, k = i%3, i%4\n    dept_shapes[cc[city]].plot(ax=axis_list[k][j],\n                               color=map_color[city],\n                               edgecolor='black')\n    axis_list[k][j].set_title(city.upper().replace('_', ' '),\n                              fontsize=15)\n    axis_list[k][j].set_axis_off()\n\nfigure.set_size_inches(14, 14)","99c75352":"uof_present = [key for key, value in cc.items()\n                       if is_uof_present[value]]\nuof_present","14004baa":"[(uof[cc[city]]\n      .pipe(csnap, msg=f\"{city}\")\n      .pipe(csnap, lambda x: x.head(2)))\n     for city in uof_present[:3]];","fb83bdf4":"common_cols = ['INCIDENT_DATE',  'LOCATION_DISTRICT',\n               'LOCATION_LATITUDE', 'LOCATION_LONGITUDE',\n               'SUBJECT_RACE']","c29c9afb":"import warnings\nwarnings.filterwarnings('ignore')","1c9d07a5":"# normalize uof_data\nuof_list = []\nfor city in tqdm(uof_present):\n    data = uof[cc[city]].iloc[1:,]\n    data = data.loc[:, common_cols]\n    \n    match_cols = uof[cc[city]].iloc[1:,].columns.str.contains('^x|^y|coord',\n                                                              case=False)\n    filt_data = uof[cc[city]].iloc[1:,].iloc[:, match_cols]\n    \n    if sum(match_cols) == 2:\n        data['COORDINATES'] = list(zip(pd.to_numeric(filt_data.iloc[:, 0],\n                                                     errors=\"coerce\"),\n                                         pd.to_numeric(filt_data.iloc[:, 1],\n                                                       errors=\"coerce\")))\n        crs = orig_crs[city]\n\n    elif sum(match_cols) == 0:\n        data['COORDINATES'] = list(zip(data.LOCATION_LONGITUDE.astype(np.float16),\n                                       data.LOCATION_LATITUDE.astype(np.float16)))\n        crs = dest_crs\n    \n    data['COORDINATES'] = data['COORDINATES'].apply(Point)\n    data = gpd.GeoDataFrame(data, geometry='COORDINATES')\n    data.crs = crs\n    data = data.to_crs(dest_crs)\n\n    data = data.reset_index().rename(columns={'index': 'ID'})\n        \n    data = data.assign(city = city,\n                      INCIDENT_DATE = lambda x: pd.to_datetime(x.INCIDENT_DATE))\n    \n    uof_list.append(data.loc[:, common_cols+['COORDINATES', 'city']])\nuof_agg = pd.concat(uof_list).reset_index(drop=True)","eee78924":"for city in cc.keys():\n    dept_shapes[cc[city]] = (dept_shapes[cc[city]]\n                                 .reset_index()\n                                 .rename(columns={'index': 'UUID'}))","f958101b":"district_name = dict(orlando = 'Sector',\n                    charlotte = 'DNAME')","53818e06":"do_point_in_polygon = False","fc83cf4c":"if do_point_in_polygon:\n    uof_agg['SHAPE_ID'] = np.nan\n    for i in tqdm(range(uof_agg.shape[0])):\n        lat = float(uof_agg.at[i, 'LOCATION_LATITUDE'])\n        long = float(uof_agg.at[i, 'LOCATION_LONGITUDE'])\n        if pd.notna(lat) and pd.notna(long) and lat != 0 and long != 0:\n            city = uof_agg.at[i, 'city']\n            PD = dept_shapes[cc[city]]\n            for j in range(PD.shape[0]):\n                if PD.at[j, 'geometry'].contains(uof_agg.at[i, 'COORDINATES']):\n                    uof_agg.at[i, 'SHAPE_ID'] = PD.at[j, 'UUID']\n    uof_agg.to_pickle('uof_agg_with_PD.pkl')\nelse:\n    uof_agg = pd.read_pickle('..\/input\/cpe-external-adiamaan\/external\/uof_agg_with_PD.pkl')","4dae0a4e":"# find state and county code from data\nstate_city_data = dict()\ncounty_city_data = dict()\nfor city in cc.keys():\n    state_city_data[city] = list()\n    county_city_data[city] = list()\n    for key, value in data_dict.items():\n        data = (data_dict['edu']\n                     .pipe(setcols)\n                     .query(\"City == @city\"))\n        state_city_data[city].append(data.statefp.unique())\n        county_city_data[city].append(data.countyfp.unique())     \n        \nstate_city_data = {key:(np.unique(value).astype(int))\n                       for key, value in state_city_data.items()}\ncounty_city_data = {key:(np.unique(value).astype(int))\n                        for key, value in county_city_data.items()}        ","a3157569":"census = \\\n(\n    census\n        .assign(statefp = lambda x: pd.to_numeric(x.statefp),\n                countyfp = lambda x: pd.to_numeric(x.countyfp))\n)","16af62b5":"overlay, is_missing_overlay = dict(), dict()","35c50880":"overlay_working = False","72eb22a6":"if overlay_working:\n    for city in tqdm(cc.keys()):\n        statefp, countyfp = state_city_data[city], county_city_data[city]\n        overlay[city] = gpd.overlay((census.query(\"statefp in @statefp \\\n                                                and countyfp in @countyfp\")),\n                                    dept_shapes[cc[city]], how='intersection')\n        if overlay[city].shape[0] > 0:\n            is_missing_overlay[city] = False\n        else:\n            is_missing_overlay[city] = True\n        overlay[city].crs = dest_crs\nelse:\n    with open('..\/input\/cpe-external-adiamaan\/overlay.pkl', 'rb') as handle:\n        overlay = pickle.load(handle)","02171718":"ncities = len(cc.keys())\naxis_list =  ['axis_' + str(i) for i in range(ncities)]\nfigure, axis_list = plt.subplots(nrows=ncities\/\/3, ncols=3)\nfigure.subplots_adjust(hspace=0.5)\n\nfor i, city in enumerate(cc.keys()):\n    j, k = i%3, i%4\n    axis_list[k][j].set_title(city.upper().replace('_', ' '),\n                                  fontsize=15)\n    axis_list[k][j].get_xaxis().set_ticklabels([])\n    axis_list[k][j].get_yaxis().set_ticklabels([])\n    if overlay[city].shape[0] > 0:\n        overlay[city].plot(ax=axis_list[k][j],\n                                   color=map_color[city],\n                                   edgecolor='black')\n        axis_list[k][j].set_axis_off()\n\nfigure.set_size_inches(12, 12)\n#figure.set_size_inches(10.5, 10.5)","2513cedd":"fields = ['statefp', 'countyfp', 'tractce']\nfor city in cc.keys():\n    if overlay[city].shape[0] > 0:\n        overlay[city].loc[:, fields] = \\\n        (overlay[city]\n            .loc[:, fields]\n            .apply(lambda x: pd.to_numeric(x, errors=\"coerce\")))\n    for key in data_dict.keys():\n            data_dict[key].loc[:, fields] = \\\n            (data_dict[key]\n                .loc[:, fields]\n                .apply(lambda x: pd.to_numeric(x, errors=\"coerce\")))\n        ","8abf3808":"flag_dicts = [is_uof_present, has_req_files, is_consistent,\n              is_acs_data_consistent]     ","1b4ebfc6":"(\npd.concat([\n(\npd.DataFrame\n    .from_dict([is_uof_present, has_req_files,\n                is_consistent,is_acs_data_consistent])\n    .rename(columns={value:key\n                     for key, value in cc.items()})\n),\n(\npd.DataFrame.from_dict([is_missing_overlay,\n                        state_city_data,\n                        county_city_data])\n)]\n).assign(flag = ['is_uof_preset', 'has_required_shape_files',\n                 'is_shape_file_consistent', 'is_acs_data_consistent',\n                 'is_not_overlaying', 'state_city_code', 'county_city_code'])\n .set_index('flag')\n)","02a43011":"# For brevity\nrace_naming = ((r\"American Indian (?:or|and) Alaska Native\\s?\",\n                'Native'),\n               (r\"Native Hawaiian (?:or|and) Other Pacific Islander\\s?\",\n                'Pacific'),\n               (r\"Black (?:or|and) African American\\s?\",\n                \"Black\"))\n\nedu_naming = ((r\"Bachelor's degree or higher\",\n               \"Bach\"),\n              (r\"High school graduate or higher\",\n               \"HS\"))","63f0b0f3":"races = ['Native', 'Asian', 'Black',\n         'Pacific', 'White']","3c38bed8":"#edu\nedu = data_dict[\"edu\"].copy()\nedu = edu.pipe(ccol, 'total|city|state|county|tract;'\n                      'Estimate|^$;alone|^$;')\nedu.columns = edu.columns.droplevel([1, -1])\nedu = \\\n(edu.pipe(setcols)\n     .pipe(setcols, lambda x: x.columns.str.replace('Total_|alone|, '\n                                                    'not Hispanic or Latino', ''))\n     .pipe(setcols, lambda x: [reduce(lambda a, kv: re.sub(*kv, a),\n                                      race_naming, cname)\n                                for cname in x.columns])\n     .pipe(setcols, lambda x: [reduce(lambda a, kv: a.replace(*kv),\n                                      edu_naming, cname)\n                                for cname in x.columns])\n     .pipe(cfilter, lambda x: (~x.columns.str\n                                 .contains('Some', case = False)),\n           axis='columns')\n     .pipe(setcols, lambda x: x.columns.str.replace('\\s+', ''))\n)\n\nfor race in races:\n    for education in ['Bach', 'HS']:\n        edu[race + '_' + education] = \\\n            edu[race + '_' + education]\/edu[race]\nedu = (edu.pipe(cfilter, lambda x: ~x.columns.isin(races),\n                 axis='columns'))","5b6bc32a":"(\n    edu\n        .groupby('City', as_index=False).mean()\n        .pipe(cfilter, lambda x: x.columns.str.contains('City|HS'), axis = 'columns')\n        .pipe(setcols, lambda x: x.columns.str.replace('_HS', ''))\n        .assign(City = lambda x: x.City.str.upper().str.replace('_', ' '))\n        .set_index('City')\n        .sort_index(axis=1)\n        .style.format(\"{:.0%}\").bar()\n)","62f88f9b":"# poverty\npoverty = \\\n(\n data_dict[\"poverty\"]\n     .pipe(ccol, 'city|state|county|tract|perc;'\n                 'est|^$;^race|^$;alone|^$')\n     .pipe(setcols)\n     .pipe(setcols, lambda x: (x.columns\n                                .str.replace('Percent below poverty level_Estimate_', '')\n                                .str.replace('RACE AND HISPANIC OR LATINO ORIGIN_|'\n                                              ', not Hispanic or Latino|'\n                                              '\\(of any race\\)|alone', '')))\n     .pipe(setcols, lambda x: [reduce(lambda a, kv: re.sub(*kv, a),\n                                  race_naming, cname)\n                            for cname in x.columns])\n     .pipe(setcols, lambda x: x.columns.str.strip())\n     .pipe(cfilter, lambda x: (~x.columns.str\n                             .contains('Some', case = False)),\n           axis='columns')\n\n)","d5a2c7c9":"(\n    poverty\n        .pipe(cfilter, lambda x: ~x.columns.str.contains('state|county|tract'),\n             axis = 'columns')\n        .groupby('City', as_index=False).mean()\n        .assign(City = lambda x: x.City.str.upper().str.replace('_', ' '))\n        .set_index('City')\n        .sort_index(axis=1)\n        .style.format(\"{:.0f}%\").bar()\n)","ff75d9ec":"# employment\nemployement = data_dict['employement']\nemployement = \\\n(\n    employement\n        .pipe(ccol, 'unempl|city|state|county|tract;'\n                    'est|^$;^race|white|alone|^$')\n        .pipe(setcols)\n        .pipe(setcols,\n              lambda x: (x.columns\n                          .str.replace('Unemployment rate_Estimate_', '')\n                          .str.replace('RACE AND HISPANIC OR LATINO ORIGIN_|'\n                                       ', not Hispanic or Latino|'\n                                       ' \\(of any race\\)|alone', '')\n                          .str.strip()))\n        .pipe(setcols, lambda x: [reduce(lambda a, kv: re.sub(*kv, a),\n                                  race_naming, cname)\n                            for cname in x.columns])\n)","2e7c9e97":"(\n    employement\n        .pipe(cfilter, lambda x: ~x.columns.str.contains('state|county|tract'),\n         axis = 'columns')\n        .groupby('City', as_index=False).mean()\n        .assign(City = lambda x: x.City.str.upper().str.replace('_', ' '))\n        .set_index('City')\n        .pipe(cfilter, lambda x: x.columns.isin(races), axis='columns')\n        .iloc[:, 1:]\n        .sort_index(axis=1)\n        .style.format(\"{:.0f}%\").bar()\n)","efdbaa2a":"#rsa\nrsa = data_dict[\"rsa\"].copy()\nrsa = \\\n(\n    rsa.pipe(ccol, 'Percent$|city|state|county|tract;'\n                   '^race|^$;one|^$;' +\n                   '|'.join(races)+ '|^$' + ';^$')\n        .pipe(setcols)\n        .iloc[:,1:]\n        .pipe(setcols,\n              lambda x: (x.columns\n                         .str.replace('Percent_RACE_One race_|\\(of any race\\)', '')\n                         .str.replace('Percent_HISPANIC OR LATINO AND RACE_Total population_', '')))\n        .pipe(setcols, lambda x: [reduce(lambda a, kv: re.sub(*kv, a),\n                              race_naming, cname)\n                            for cname in x.columns])\n)","57c3fa95":"(\n    rsa\n        .pipe(cfilter, lambda x: ~x.columns.str.contains('state|county|tract'),\n         axis = 'columns')\n        .groupby('City', as_index=False).mean()\n        .assign(City = lambda x: x.City.str.upper().str.replace('_', ' '))\n        .set_index('City')\n        .pipe(cfilter, lambda x: x.columns.isin(races), axis='columns')\n        .sort_index(axis=1)\n        .style.format(\"{:.0f}%\").bar()\n)","51c5c8da":"# income\nincome = data_dict[\"income\"].copy()\nincome = \\\n(\n    income.pipe(ccol, 'median|city|state|county|tract;'\n                      'estim|^$;^hispanic|^Households$|^$')\n        .pipe(setcols)\n        .pipe(setcols,\n              lambda x: (x.columns\n                          .str.replace('Median income \\(dollars\\)_Estimate_Households_One race--_', '')\n                          .str.replace('Median income \\(dollars\\)_Estimate_|\\(of any race\\)', '')))\n        .pipe(setcols, lambda x: [reduce(lambda a, kv: re.sub(*kv, a),\n                          race_naming, cname)\n                        for cname in x.columns])\n        .iloc[:, list(range(1, 6)) + list(range(8, 13))]\n)\n","ae7756bf":"(\n    income\n        .pipe(cfilter, lambda x: ~x.columns.str.contains('state|county|tract'),\n         axis = 'columns')\n        .groupby('City', as_index=False).median()\n        .assign(City = lambda x: x.City.str.upper().str.replace('_', ' '))\n        .set_index('City')\n        .pipe(cfilter, lambda x: x.columns.isin(races), axis='columns')\n        .sort_index(axis=1)\n        .apply(lambda x: x\/1000)\n        .style.format(\"${:.2f} K\").bar()\n)","8682cffc":"austin_uof = \\\n(\n    uof_agg.query(\"city == 'austin'\").pipe(csnap)\n        .pipe(cfilter, lambda x: x.SHAPE_ID.notnull())\n        .pipe(csnap)\n        .assign(SHAPE_ID = lambda x: x.SHAPE_ID.astype(int))\n)","c33d202a":"# austin with crime data\ncity = 'austin'\nboston_bounds =  dept_shapes[cc[city]].bounds\nminx, miny, maxx, maxy = \\\nboston_bounds.minx.min(), boston_bounds.miny.min(),\\\nboston_bounds.maxx.max(), boston_bounds.maxy.max()\ncorners = [(miny, minx), (miny, maxx),\n           (maxy, minx), (maxy, maxx)]\n\nffig = folium.Figure(height=400)\nfmap = folium.Map(location=map_latlong[city],\n                  zoom_start=10.45, tiles= \"CartoDB positron\").add_to(ffig)\n\n(folium.GeoJson(dept_shapes[cc[city]].assign(geometry = lambda x: x.geometry.simplify(0.0001, preserve_topology=True)),\n               style_function=lambda x: {'fillColor': 'green',\n                                       'color': 'Black',\n                                       'fillOpacity': 0.75,\n                                       'weight':0.75})\n    .add_to(fmap))\n\nfor corner in austin_uof.COORDINATES:\n    folium.CircleMarker(location=[corner.y, corner.x],\n                         radius=0.125,\n                        fill_color='red',\n                        fill_opacity= .75,\n                       color='red').add_to(fmap)\n    \nfmap","72e08abf":"austin_uof = \\\n(\n    pd.DataFrame(\n        austin_uof\n            .groupby(['SHAPE_ID', 'SUBJECT_RACE'],\n                 as_index=False).size().reset_index()\n            .rename(columns={0:'narrests'}))\n        .pivot(index='SHAPE_ID',\n              columns='SUBJECT_RACE',\n            values='narrests').reset_index()\n)","76d4dee3":"# combining all the data\ncity = 'austin'\naustin_results = \\\n(\n    dept_shapes[cc[city]]\n        .loc[:, ['UUID', 'geometry']]\n        .pipe(csnap)\n        .merge(overlay[city]\n                .loc[:, ['statefp', 'countyfp',\n                 'tractce', 'UUID']]\n                .merge(rsa,\n                       on = ['statefp', 'countyfp', 'tractce'])\n                .pipe(csnap)\n                .merge(poverty,\n                       on = ['statefp', 'countyfp', 'tractce'],\n                       suffixes = ('_POP', '_POV'))\n                .merge(austin_uof,\n                       left_on = 'UUID',\n                       right_on = 'SHAPE_ID')\n                .fillna(0)\n                .groupby('UUID', as_index=False).mean())\n)","8434c955":"for race in races:\n    if race in austin_results.columns:\n        austin_results[race+'_ADJ_ARR'] = \\\n        (austin_results[race] *\n            (1 - austin_results[race + '_POV']\/100) *\n            (1 - austin_results[race + '_POP']\/100))","c804fe50":"austin_races = [col for col in austin_results.columns\n                   if col in races]\nfor i, race in enumerate(austin_races):\n    ax = austin_results.plot(column = race+'_ADJ_ARR',legend = True,\n                             figsize = (6,6), k=4, cmap = 'viridis')\n    ax.set_title(race+' Adjusted Arrests',\n                 fontsize=18)\n    plt.axis('equal')\n    ax.set_axis_off()","7fd40a36":"\n### Coordinate system\nLets understand what a Coordinate system is. Learning by example makes it easier to grasp. One of the well known coordinate system is the cartesian coordinate system. Using this as an analogy, for a coordinate system to work, it needs an origin and a way to measure a point relative to the origin. In case of cartesian system, we use (0, 0) as origin and euclidean distance as a way to measure. While cartesian is a 2D system, we are dealing with a 3D spheroid (earth). The process of conversion of this 3D to a 2D system is called projections which we will come back to later. It should be noted that datum is not the same as a coordinate system. Based on how you select the origin, there are two types of coordinate systems in GIS.\n-  Geographic Coordinate System(GCS), which is a universal system that uses a Prime Meridian as the origin and uses an angular unit of measure (latitude and longitude) relative to the meridians.\n-  Projected Coordinate Sytem(PCS), which is a very narrow local coordinate system unlike GCS. This becomes very useful when the area of interest is narrow like a country or a state. In this case, the region of interest becomes a square with the origin at the center. For example, if we are dealing only with a particular state in US like dallas as our region of interest, we pick a sqare boundary large enough just to encapsulate the region and pick the center and define all the points with reference to this center. The points can be measured in several ways from the origin, such as feet, metres etc.,","7eeb2261":"Logic for fixing missing .prj files,","1b5e1446":"##  Aggregating ACS data across cities.\n\nLets make sure the ACS data structure is the same for all cities using boston as a reference. The below code asserts that,","739cc4a0":"The U.S. Census Bureau considers race\nand ethnicity to be two separate and\ndistinct concepts. <br>\n> What is race? <br>\nThe Census Bureau defines race as a person\u2019s\nself-identification with one or more social groups.\nAn individual can report as White, Black or African\nAmerican, Asian, American Indian and Alaska Native,\nNative Hawaiian and Other Pacific Islander, or some\nother race. Survey respondents may report\nmultiple races.\n\n>What is ethnicity? <br>\nEthnicity determines whether a person is of Hispanic\norigin or not. For this reason, ethnicity is broken out\nin two categories, Hispanic or Latino and Not\nHispanic or Latino. ** Hispanics may report as any race.**","af5d73cf":"### EPSG (European Petroleum Survey Group) standard:\nEPSG has standardized some of the Coordinate Reference System (CRS). <br>\nFor example, the `EPSG4326` standard is defined with datum `WGS84`,  Prime Meridian as `Greenwich` , origin as (0.00000000, 0.00000000) and bounds as (-180, -90) to (180, 90) which makes it a GCS.<br>\n`EPSG2249` which is used as a CRS for boston in this problem is defined with datum `NAD83`, projection as `LCC` and bounds as (73.5100, 41.3500) to (-69.8600, 42.8900) with unit of measurement in US feet making it a PCS.","e7cc8156":"The question poised in this challenge is to find the **racial disparity shown by number of arrests not explainable by poverty rate**. We also have to normalize based on population. Assuming a fair soceity, where there is no racial disparity then everyone is equally likely to commit crime and arrested. But since the population of a particular race varies across regions we need to weight it based on population. More the population of a particular race, more the change of a person in that race committing a crime when all other factors are considered equal. <br><br> There are several studies that show the relationship between poverty and crime rate.\nOne such [study](http:\/\/economics.fundamentalfinance.com\/povertycrime.php), show a simple linear regression model, where a 1% increase in population under the poverty level will increase the total crimes by 134 per 100,000 inhabitants. So it is critical to normalize the number of arrests based on poverty level.","44ae9197":"<a id='normalize_arrests'><\/a>\n### Normalize Arrests by Population and Poverty rate","d7beacf6":"<center> \n\n# A robust, scalable solution for data cleaning and an objective look at  racial disparity <br> in policing unexplained by crime rate and poverty\n\n<\/center>\n***\n## Data Science for Good: Center for Policing Equity\n### Problem\nCPE needs to automate the combination of Police Department data and Census data and find a way to identify the bias based on Race that is not explainable by crime rate and poverty level.<br>\nWe have to merge the following two data pipelines, <br>\n1.  Census data containing socio economic statistics which are given in ACS folder in this challenge and<br>\n    census tract shape file found [here](https:\/\/www.census.gov\/geo\/maps-data\/data\/tiger-line.html).<br>    \n   \n2.  Police Department data containing arrest information and shape files about police districts. ","0af8d081":"<a id='pipe_demo'><\/a>\nAs expected they are from the same projection, lets combine them together,","ac66501f":"####  High School Graduates by City and Race","74df8ebd":"All the data are read in a **robust** way  so that unexpected folder structure or naming are handled properly. The only **manual work** needed in the code to scale the solution is to add the codes in the below dictionaries and add the state to the city name in the list. `city_state`.","4931775c":"Looking at the file, it is clear that each of them has two headers. The first header seems to be the result of normalizing the column names, which reduces our work significanlty. <br> We also need to normalize all the locations to `EPSG4326`. The coordinate system of the points follow the coordinate system of its shape files. Lets define some common columns that are vital for the analysis,","dc2f47a7":"<a id = 'point_in_polygon'>\n<\/a>\n### Mapping UOF to PD\nCreate a unique index in all the shape files.","8415157a":"### Reading census shape files","5589f665":"Combining all the socio economic data across cities to a single data frame each for `edu, edu_25, poverty, housing, rsa, income, employment`","2555bd48":"Kaggle kernel fails to make the call to geopy. When running locally uncomment the below lines to fetch latitude and longitude automatically. As a workaround, reading from the pickle file.","6e8ed211":"<a id='overlay'><\/a>\n### Mapping census tracts to PD","e782a18f":"As seen below the UOF data has more problems like non-matching shapes, columns and content. Normalizing this is important, as files from each PD is bound to have its own format. <br>","aefe5245":"***","452e7c02":"### Reading Police Districts (PD's) shape files","618db49d":"<a id = 'solution_starts'>\n<\/a>\n## Heuristics\n\nInorder to standardize the CRS across all shape files, lets assume NAD83 and WGS84 datum are same. The next step is to identify if a shape file is in GCS or PCS. If a shape file is in GCS, then the geometries are defined as latitude and longitude, else if it is in PCS then they should be in feet or metres. We can detect this with the number of digits to the left of the decimal point. <br>","0777fe7c":"<a id='uof_normalize'><\/a>\n### Normalize UOF Data\nShape files are now clean. Lets normalize use of Force files. This file is available only for,","2eedd6b9":"<a id='disclaimer'><\/a>\nDISCLAIMER: The coding style may make readability an issue for some, but once you get used to method chaining, you will never look back. The GIS explanations are crude, which reflects my own understanding and an effort to keep it simple. I expect them to have mistakes and will be more than happy to be pointed out. ","c0a0c81c":"**TL; DR** <br>\n>  Two kinds of **Coordinate System**, GCS - expressed in lat\/long and PCS - expressed in feet\/metre from a reference point. <br>\nGCS is akin to looking at the entire map, whereas PCS is zooming to an area of interest. <br>\n**Datum** specifies the origin point for CRS. Common datum is to use the center of mass of the earth. <br>\n**Projections** is converting 3D earth to a 2D map. Similar to flattening an orange peel -it will rip. <br>\n**EPSG** standardizes a CRS which is a collection of datum, projections and few other things that we dont care about. <br>","5b73fb67":"####  Race in Total population by City","80f08f02":"Looking at the CRS for the shape files we got, as given in the problem statement, we have a mixture of CRS(sometimes is missing). We also see that in case of dallas and seattle they are in different projection systems like LCC and Mercator with no epsg code given.","ba8c2beb":"If the shape files are PCS then they need to follow some standards thats usually used by that state.\nLets look at all the epsg codes that are used in US states. You can get this data here (add link to  epsg.io). This gives us all the state level epsg code to find the best one that matches our shape file. ","27aeda5d":"## _Tale of Austin_\nLooking at Austin for the sake of showing an example. <br>\n`overlay` Now has the overlay of census tract files on police districts. We also have tract level Race Data on education, poverty , income, population.","85dfd6f5":"<a id='data_cleaning'><\/a>\n## Data Cleaning Summary","70d4e212":"### *One Plot to show 'em All*\nFor Dallas only the EPIC focus area is given. Hence the isolated polygons.","46672449":"> ** How to normalize number of arrests by poverty and population? ** <br>\nNumber of arrests is directly proportional to poverty rate and population.\nWe want to account for that and a simpler way is to inverse this effect by taking an inverse.<br><br>\n$X' = X * (1 - POV) * (1 - POP)$ <br><br>\nX' - Adjusted number of Arrests, <br>\nX - Number of Arrests, <br>\nPOV - Below Poverty Rate, <br>\nPOP - Percentage in Population. <br><br> \n","1b47daca":"Boston, Los Angeles, Oakland and San Francisco Police shape files and Census shape files doesnt overlap anywhere.","980e76fc":"We can again use the same process to see how far is the city from the extremes of the shape file given","ed6b39a7":"Creating state, county and tract codes, for later joining with census shape files,","17b42b70":"<a id = 'data_container'><\/a>\n##  Data containers\n\nEvery city has 5 different ACS data folders containing socio economic information on Education, Housing, Poverty, Race - Sex and Age.\nwe will be referring them using the abbreviations, ` ['edu', 'edu_25', 'housing', 'poverty', 'rsa']` for brevity.<br><br>\nAll the ACS data are stored in a dictionary called `dept` with the key being dept code of the format `XX-XXXXX`.\nBoth the ACS data folders and cities have an alias for meaningful reference defined with the dictionaries `cc` and `dc`. \nThe value in dept is a named tuple, providing access to all the ACS data using the `.` operator. Each ACS data in turn is a named tuple, giving access to either data(ann) or meta.\n\nIf none of the above make sense, the ** key take-away ** is that all these abstractions helps us in accessing all the data as simple as follows, <br>\neg : access Dallas poverty meta data as `depts[cc['dallas']].poverty.data`\nThe dictionaries are a way to add aliases to the department and the data code. `uof` dictionary for storing Use of Force data. the only requirement for this kind of file to be processed is to ensure that it has the string `uof` on its names.\n","2aaba0f0":"Lets see the logic in action. We pick the extremes in our shape file and form a bounding box. This is to mimic the Projected Coordinate System's act of bounding the region of interest and when we find the distance between these corners with our city coordinate reference (indicated with the blue line) and find the epsg code that gives a minimum distance out of them all.","31cf9e78":"### Conversion between CRS - GCS and PCS\nFrom the above it's clear that we have a combination of GCS and LCS. We should have no problems converting from a PCS to GCS or GCS to PCS. In the below we can see a round trip of conversion, leads to a trivial change in footage. Its clear that as long as the .prj file is present, we can safely convert between PCS and GCS.","a5a0b7a9":"Lets load all the shape files and store it in the data container `dept_shapes` accessed with the key similar to `depts`. The code also looks for Mandatory files in a shape folder `.shp, .shx, .dbf`, for consistent geometries. It should be noted that the shape files should have polygons or Multipolygons as their geometries. Polygons are just set of points that starts and ends with the same x,y which ensures a closed shape.<br><br>\n_Note: `.prj` is not mandatory for a shape file format. [1] (add wiki reference)_","0569c149":"##  Custom Pipe functions\nPandas provides [pipe functions](https:\/\/pandas.pydata.org\/pandas-docs\/stable\/generated\/pandas.DataFrame.pipe.html) to let users create custom functions and helps in creating method chaining <br>\nLook at the [demo](#pipe_demo) to see how it helps.","b1e89f23":"### Reading Data","12118460":"####  Unemployment rate by City and Race","ce5e411d":"####  Poverty rate by City and Race","7571f087":"CMPD shape file has points instead of polygons. Reading it from external source,","7f019d97":"**The census tract files need to be named with state for easy access and storage in a dictionary**.","19551193":"### Census tracts intersection with Police Districts","941fb77b":"### Solution    \n\n**The primary motive of this solution is to make the solution scalable across several PD's so that CPE can take this solution and use it in production with minimal rewrite.**\n#### Ease of Use\nFirst and foremost we have to organize all the data. This is very important for the **scalability** of the solution. Few data [containers](#data_container) are introduced so that accessing any data across departments is made easy, while letting users create aliases for convenient usage. <br>\n#### Data Validation\nOne of the important aspect of this competition is data validation. Given a department folder, we have to make sure that it contains all the necessary information to combine it seemlessly with the census data. The census data and shape files are collected by a single organization, Census Bureau and by the sample files that are given, it looks consistent. **The main focus is on validating and fixing the PD shape files and Use of Force\/Arrest data**, which by far has multitudes of issues which is expected given its disparate data sources. Some of the data issues that are seen in the given files,\n1. Points instead of polygons in Police district shape files.\n2. Missing .prj files.\n3. Missing UOF (Use of force) files\/ not geocoded.\n4. Wrong ACS data for a PD.\n\nWhile some of the above issues are fixable some or not. (Such as automating the search and download of use of Force file).\nA [Summary](#data_cleaning) is created to quickly glance through the issues so that the user is able to \nquickly point out the inconsistencies.\n#### Data correction\nA [quick intro](#intro_crs) on datum, projections, coordinate systems and epsg standard helps in understanding the issue. With this information, we can fix our main problem, which is missing or inconsistent CRS. **The entire process is automated to completely eliminate user interference**.\n#### Aggregating Data\n[Use of Force data is normalized](#uof_normalize) first and all the data are aggregated at PD level for easy analysis. The Use of Force data is mapped to the PD shape files using [point in polygon](#point_in_polygon) method and the census files are [overlayed](#overlay) on top of the PD shape files.\n#### Normalizing Arrests\nThe [number of arrests have to be normalized based on population of a particular race in a PD and poverty rate](#normalize_arrests) since they are directly proportional to the number of arrests. A simple normalization is implemented after which the PD's are compared to see if there really is a Racial Bias in that PD.\n","4c6c1d46":"### Datum\nDatum is a part of the coordinate system, The sole purpose of datum is to specify the origin. In case of GCS there are two widely used datums. \n-  WGS84 (World Geodetic System 1984) which defines the Earth's center of mass as the origin.\n-  NAD83 (North American Datum 1983) which defines the center to remain constant over the North American Plate.\n\nThough both the datums are similar, its been found that they have been diverging at a rate if 1 to 2 cm per year. For our intents and purposes, we can assume that they are analogues.","0a4408f8":"Since we have all the possible epsg state level code in US, we can loop through and find the best fit for the given shape file thats missing a .prj file.<br>\n\n> **How can we quantitatively tell a epsg\/crs is better than others? **<br>\nOne way is to encapsulate the shape files in a rectangle and then find the distance between the corners of this rectangle with the city's latitude and longitude that we get from a reference such as google maps\/open street maps. The epsg code with the minimum distance is the winner.\n\nNote: Both Google Maps and Open street maps use WGS84 as datum. [**Vincenty** ](https:\/\/en.wikipedia.org\/wiki\/Vincenty%27s_formulae) is used for accurate distance measure\n","9b002d73":"Identifying a Coordinate system,","269cddeb":"Fixing PD shape files and Data, ","5050d96b":"### Projections\nConverting earth as a sphere or a spheroid into a planar(cartesian) 2D coordinate system is called projection. A spheroid cannot be flatenned to a plane any easier than a piece of orange peel can be flattened- it will rip. Representing the earth's surface in two dimensions causes distortion in the shape, area, distance or direction of the data. Naturally when a system is not perfect, you are expected to have several attemps to solve it. Some of the frequently used projections are, Robinsons, Transverse Mercator, Lambert Conformat Conical (LCC) ","6f4f6e3c":"Load all the data files into the data containers. Every folder has a ACS folder, shape folder and UOF if present.","0ee27b27":"We take a lat\/long and do a point in polygon analysis to find the PD the event happened. This is a costly operation and since it is _embarrasingly parallel_ the speed can be improved very easily while deploying. The following is just a naive implementation of the logic.","13d24a58":"The below code normalizes the UOF across PD's, adds a UID if present and changes the points from PCS to GCS.","00820366":"\nNow that we have done all the hardwork, lets try out the example,","a66040a3":"Since we need to automate the entire process, we can use geopy (open street maps) to fetch the lat\/long of the cities of interest as shown below","66242a36":"### Handling missing .prj file\nIf we have a missing .prj file then we can initiate it to a default like `EPSG4326`, if its a GCS.<br>\nBut if its a PCS, then we need to find out which state epsg code matches the shape file the best.<br><br>\n\nBelow, we find whether the shape file follows GCS or PCS. If its GCS or has a .prj file it is converted into `EPSG4326`. ","576b4ed0":"From the above log, its clear that the city is within reasonable miles of the bounds of the shape files extreme points.\nThis make sure that all the above conversions are right.\n<font color=\"red\">Note: Seattle fails above because of geopandas version difference in kaggle kernel  from my local.<\/font>","6acd0e20":"### [_Talk is cheap. Show me the code._](#solution_starts)","4bd5b7e7":"### _To Tidy, or not to Tidy_\nThis problem has a unique data style, almost like cross tables. From [Census Bureau](https:\/\/factfinder.census.gov\/faces\/tableservices\/jsf\/pages\/productview.xhtml?src=bkmk) we can see that each ACS data folder like `edu` is a combination of multiple tables. Hence we got this very wide table. Since this format seems to be widely used and looks intuitive, the format is retained. Few helpful functions are added to weild them easily.","99f14869":"### Cities overview\nSince the main motive is to address racism in policing, lets gather Race related Data","da72bdd0":"Splitting column names to create a multi-index,","afb49ee5":"#### Normalize `statefp, countyfp and tractce`","e9e67581":"#### Median Income by City and Race","7025026e":"Lets load all the census shape files into a `census` data frame. <br>\nAs expected all the files are from the same CRS. <br>\n`EPSG4269` is a CRS used for the entire North America onshore and offshore. \nWe will see about projections in a bit.","cbc26890":"<a id='intro_crs'><\/a\/>\n## A quick intro to CRS\nThis is a [naive](#disclaimer) explanation of CRS. ","e42bd5cb":"Converting all the non character fields to numeric,","b4729dfb":"All the shape files have been converted into a standardized form,","893fcad5":"### Answering racial disparity in policing"}}