{"cell_type":{"a4c17652":"code","79d29f6e":"code","9eee3f81":"code","cde25428":"code","6814a2de":"code","5f3fa2a9":"code","2b9f02f6":"code","8c7c3058":"code","6d981de2":"code","49980178":"code","46c63e32":"code","4dbbdf13":"code","2db2111a":"code","ad494076":"code","8ba89540":"code","9f7b05e2":"code","59102977":"code","0d18a6e6":"code","79136068":"code","3fdbdcc3":"code","ec873ebb":"code","b22b5ff6":"code","ad2ec9ec":"code","36a4befe":"code","1d5deff3":"code","b3a09a5c":"markdown","6c4c2ac6":"markdown","8f109782":"markdown","38442b6c":"markdown","46255246":"markdown","adb82730":"markdown","290e3293":"markdown","9d36189f":"markdown","3d73631b":"markdown","ae7f72fc":"markdown","9e8c7b5c":"markdown","a4c95973":"markdown","d2d9499f":"markdown","9400cf18":"markdown","dff128d0":"markdown","05fae047":"markdown","9e747088":"markdown"},"source":{"a4c17652":"import os\nimport pandas as pd\nimport numpy as np\nimport random\nimport time\nimport cv2\n\nfrom typing import List, Tuple\nfrom numpy import ndarray\nfrom copy import deepcopy\nfrom scipy.special import logsumexp\nimport matplotlib.pyplot as plt\n\nimport torch\nfrom torch import Tensor\nfrom torch import nn\nfrom torch.utils.cpp_extension import load_inline\nfrom torch.utils.data import Dataset, DataLoader","79d29f6e":"class EmotionsDataset(Dataset):\n    def __init__(self, path, type_folder):\n        self.path = path\n        self.type_folder = type_folder\n        self.emotions = list(os.listdir(path + type_folder))\n        self.idx_to_emos = {k:v for k,v in enumerate(self.emotions)}\n        self.emos_to_idx = {v:k for k,v in enumerate(self.emotions)}\n        self.df = self.get_df()\n        \n    def absoluteFilePaths(self, directory):\n        for dirpath,_,filenames in os.walk(directory):\n            for f in filenames:\n                yield os.path.abspath(os.path.join(dirpath, f))\n    \n    def get_df(self):\n        files = []\n        labels = []\n\n        for emotion in self.emotions:\n            for file in self.absoluteFilePaths(self.path + self.type_folder + '\/' + emotion):\n                files.append(file)\n                labels.append(self.emos_to_idx[emotion])\n\n        return pd.DataFrame({'file': files, 'label': labels})\n    \n    def normalize(self, img):\n        return img \/ 255.\n    \n    def __getitem__(self, idx):\n        iloc = self.df.iloc[idx]\n        img = cv2.imread(iloc.file)\n        \n        # normalize\n        img = self.normalize(img)\n        \n        img = np.moveaxis(np.moveaxis(img, 2, 1), 1, 0)\n        lbl = np.eye(len(self.idx_to_emos))[iloc.label]\n        \n        return img, lbl\n    \n    def __len__(self):\n        return len(self.df)","9eee3f81":"path = '..\/input\/emotion-detection-fer\/'\ntype_folder = ['train', 'test']","cde25428":"def collate_fn(batch):\n    imgs, lbls = zip(*batch)\n    return np.array(imgs), np.array(lbls)","6814a2de":"ds_train = EmotionsDataset(path, type_folder[0])\ntrain_batches = DataLoader(ds_train, batch_size=16, shuffle=True, collate_fn=collate_fn)\n\nds_test = EmotionsDataset(path, type_folder[0])\ntest_batches = DataLoader(ds_test, batch_size=16, shuffle=True, collate_fn=collate_fn)","5f3fa2a9":"imgs, lbls = next(iter(train_batches))","2b9f02f6":"class Operation(object):\n\n    def __init__(self):\n        pass\n\n    def forward(self,\n                input_: ndarray,\n                inference: bool=False) -> ndarray:\n\n        self.input_ = input_\n\n        self.output = self._output(inference)\n\n        return self.output\n\n    def backward(self, output_grad: ndarray) -> ndarray:\n\n        assert_same_shape(self.output, output_grad)\n\n        self.input_grad = self._input_grad(output_grad)\n\n        assert_same_shape(self.input_, self.input_grad)\n\n        return self.input_grad\n\n    def _output(self, inference: bool) -> ndarray:\n        raise NotImplementedError()\n\n    def _input_grad(self, output_grad: ndarray) -> ndarray:\n        raise NotImplementedError()","8c7c3058":"class ParamOperation(Operation):\n\n    def __init__(self, param: ndarray) -> ndarray:\n        super().__init__()\n        self.param = param\n\n    def backward(self, output_grad: ndarray) -> ndarray:\n\n        assert_same_shape(self.output, output_grad)\n\n        self.input_grad = self._input_grad(output_grad)\n        self.param_grad = self._param_grad(output_grad)\n\n        assert_same_shape(self.input_, self.input_grad)\n\n        return self.input_grad\n\n    def _param_grad(self, output_grad: ndarray) -> ndarray:\n        raise NotImplementedError()","6d981de2":"class Linear(Operation):\n    '''\n    Linear activation function\n    '''\n    def __init__(self) -> None:\n        super().__init__()\n\n    def _output(self, inference: bool) -> ndarray:\n        return self.input_\n\n    def _input_grad(self, output_grad: ndarray) -> ndarray:\n        return output_grad\n\n\nclass Sigmoid(Operation):\n    '''\n    Sigmoid activation function\n    '''\n    def __init__(self) -> None:\n        super().__init__()\n\n    def _output(self, inference: bool) -> ndarray:\n        return 1.0\/(1.0+np.exp(-1.0 * self.input_))\n\n    def _input_grad(self, output_grad: ndarray) -> ndarray:\n        sigmoid_backward = self.output * (1.0 - self.output)\n        input_grad = sigmoid_backward * output_grad\n        return input_grad\n\n\nclass Tanh(Operation):\n    '''\n    Hyperbolic tangent activation function\n    '''\n    def __init__(self) -> None:\n        super().__init__()\n\n    def _output(self, inference: bool) -> ndarray:\n        return np.tanh(self.input_)\n\n    def _input_grad(self, output_grad: ndarray) -> ndarray:\n\n        return output_grad * (1 - self.output * self.output)\n\n\nclass ReLU(Operation):\n    '''\n    Hyperbolic tangent activation function\n    '''\n    def __init__(self) -> None:\n        super().__init__()\n\n    def _output(self, inference: bool) -> ndarray:\n        return np.clip(self.input_, 0, None)\n\n    def _input_grad(self, output_grad: ndarray) -> ndarray:\n\n        mask = self.output >= 0\n        return output_grad * mask\n    \n    \ndef softmax(x, axis=None):\n    return np.exp(x - logsumexp(x, axis=axis, keepdims=True))","49980178":"class Layer(object):\n\n    def __init__(self,\n                 neurons: int) -> None:\n        self.neurons = neurons\n        self.first = True\n        self.params: List[ndarray] = []\n        self.param_grads: List[ndarray] = []\n        self.operations: List[Operation] = []\n\n    def _setup_layer(self, input_: ndarray) -> None:\n        pass\n\n    def forward(self, input_: ndarray,\n                inference=False) -> ndarray:\n\n        if self.first:\n            self._setup_layer(input_)\n            self.first = False\n\n        self.input_ = input_\n\n        for operation in self.operations:\n\n            input_ = operation.forward(input_, inference)\n\n        self.output = input_\n\n        return self.output\n\n    def backward(self, output_grad: ndarray) -> ndarray:\n\n        assert_same_shape(self.output, output_grad)\n\n        for operation in self.operations[::-1]:\n            output_grad = operation.backward(output_grad)\n\n        input_grad = output_grad\n\n        assert_same_shape(self.input_, input_grad)\n\n        self._param_grads()\n\n        return input_grad\n\n    def _param_grads(self) -> None:\n\n        self.param_grads = []\n        for operation in self.operations:\n            if issubclass(operation.__class__, ParamOperation):\n                self.param_grads.append(operation.param_grad)\n\n    def _params(self) -> None:\n\n        self.params = []\n        for operation in self.operations:\n            if issubclass(operation.__class__, ParamOperation):\n                self.params.append(operation.param)","46c63e32":"class WeightMultiply(ParamOperation):\n\n    def __init__(self, W: ndarray):\n        super().__init__(W)\n\n    def _output(self, inference: bool) -> ndarray:\n        return np.matmul(self.input_, self.param)\n\n    def _input_grad(self, output_grad: ndarray) -> ndarray:\n        return np.matmul(output_grad, self.param.transpose(1, 0))\n\n    def _param_grad(self, output_grad: ndarray) -> ndarray:\n        return np.matmul(self.input_.transpose(1, 0), output_grad)\n   \n\nclass BiasAdd(ParamOperation):\n\n    def __init__(self,\n                 B: ndarray):\n        super().__init__(B)\n\n    def _output(self, inference: bool) -> ndarray:\n        return self.input_ + self.param\n\n    def _input_grad(self, output_grad: ndarray) -> ndarray:\n        return np.ones_like(self.input_) * output_grad\n\n    def _param_grad(self, output_grad: ndarray) -> ndarray:\n        output_grad_reshape = np.sum(output_grad, axis=0).reshape(1, -1)\n        param_grad = np.ones_like(self.param)\n        return param_grad * output_grad_reshape\n\n\nclass Dense(Layer):\n\n    def __init__(self,\n                 neurons: int,\n                 activation: Operation = Linear(),\n                 conv_in: bool = False,\n                 dropout: float = 1.0,\n                 weight_init: str = \"standard\") -> None:\n        super().__init__(neurons)\n        self.activation = activation\n        self.conv_in = conv_in\n        self.dropout = dropout\n        self.weight_init = weight_init\n\n    def _setup_layer(self, input_: ndarray) -> None:\n        np.random.seed(self.seed)\n        num_in = input_.shape[1]\n\n        if self.weight_init == \"glorot\":\n            scale = 2\/(num_in + self.neurons)\n        else:\n            scale = 1.0\n\n        # weights\n        self.params = []\n        self.params.append(np.random.normal(loc=0,\n                                            scale=scale,\n                                            size=(num_in, self.neurons)))\n\n        # bias\n        self.params.append(np.random.normal(loc=0,\n                                            scale=scale,\n                                            size=(1, self.neurons)))\n\n        self.operations = [WeightMultiply(self.params[0]),\n                           BiasAdd(self.params[1]),\n                           self.activation]\n\n        if self.dropout < 1.0:\n            self.operations.append(Dropout(self.dropout))\n\n        return None","4dbbdf13":"class Loss(object):\n\n    def __init__(self):\n        pass\n\n    def forward(self,\n                prediction: ndarray,\n                target: ndarray) -> float:\n\n        # batch size x num_classes\n        assert_same_shape(prediction, target)\n\n        self.prediction = prediction\n        self.target = target\n\n        self.output = self._output()\n\n        return self.output\n\n    def backward(self) -> ndarray:\n\n        self.input_grad = self._input_grad()\n\n        assert_same_shape(self.prediction, self.input_grad)\n\n        return self.input_grad\n\n    def _output(self) -> float:\n        raise NotImplementedError()\n\n    def _input_grad(self) -> ndarray:\n        raise NotImplementedError()\n","2db2111a":"class MeanSquaredError(Loss):\n\n    def __init__(self,\n                 normalize: bool = False) -> None:\n        super().__init__()\n        self.normalize = normalize\n\n    def _output(self) -> float:\n\n        if self.normalize:\n            self.prediction = self.prediction \/ self.prediction.sum(axis=1, keepdims=True)\n\n        loss = np.sum(np.power(self.prediction - self.target, 2)) \/ self.prediction.shape[0]\n\n        return loss\n\n    def _input_grad(self) -> ndarray:\n\n        return 2.0 * (self.prediction - self.target) \/ self.prediction.shape[0]\n\n\nclass SoftmaxCrossEntropy(Loss):\n    def __init__(self, eps: float=1e-9) -> None:\n        super().__init__()\n        self.eps = eps\n        self.single_class = False\n\n    def _output(self) -> float:\n\n        # if the network is just outputting probabilities\n        # of just belonging to one class:\n        if self.target.shape[1] == 0:\n            self.single_class = True\n\n        # if \"single_class\", apply the \"normalize\" operation defined above:\n        if self.single_class:\n            self.prediction, self.target = \\\n            normalize(self.prediction), normalize(self.target)\n\n        # applying the softmax function to each row (observation)\n        softmax_preds = softmax(self.prediction, axis=1)\n\n        # clipping the softmax output to prevent numeric instability\n        self.softmax_preds = np.clip(softmax_preds, self.eps, 1 - self.eps)\n\n        # actual loss computation\n        softmax_cross_entropy_loss = (\n            -1.0 * self.target * np.log(self.softmax_preds) - \\\n                (1.0 - self.target) * np.log(1 - self.softmax_preds)\n        )\n\n        return np.sum(softmax_cross_entropy_loss) \/ self.prediction.shape[0]\n\n    def _input_grad(self) -> ndarray:\n\n        # if \"single_class\", \"un-normalize\" probabilities before returning gradient:\n        if self.single_class:\n            return unnormalize(self.softmax_preds - self.target)\n        else:\n            return (self.softmax_preds - self.target) \/ self.prediction.shape[0]","ad494076":"class LayerBlock(object):\n\n    def __init__(self, layers: List[Layer]):\n        super().__init__()\n        self.layers = layers\n\n    def forward(self,\n                X_batch: ndarray,\n                inference=False) ->  ndarray:\n\n        X_out = X_batch\n        for layer in self.layers:\n            X_out = layer.forward(X_out, inference)\n\n        return X_out\n\n    def backward(self, loss_grad: ndarray) -> ndarray:\n\n        grad = loss_grad\n        for layer in reversed(self.layers):\n            grad = layer.backward(grad)\n\n        return grad\n\n    def params(self):\n        for layer in self.layers:\n            yield from layer.params\n\n    def param_grads(self):\n        for layer in self.layers:\n            yield from layer.param_grads\n\n    def __iter__(self):\n        return iter(self.layers)\n\n    def __repr__(self):\n        layer_strs = [str(layer) for layer in self.layers]\n        return f\"{self.__class__.__name__}(\\n  \" + \",\\n  \".join(layer_strs) + \")\"\n\n\nclass NeuralNetwork(LayerBlock):\n    '''\n    Just a list of layers that runs forwards and backwards\n    '''\n    def __init__(self,\n                 layers: List[Layer],\n                 loss: Loss = MeanSquaredError,\n                 seed: int = 1):\n        super().__init__(layers)\n        self.loss = loss\n        self.seed = seed\n        if seed:\n            for layer in self.layers:\n                setattr(layer, \"seed\", self.seed)\n\n    def forward_loss(self,\n                     X_batch: ndarray,\n                     y_batch: ndarray,\n                     inference: bool = False) -> float:\n\n        prediction = self.forward(X_batch, inference)\n        return self.loss.forward(prediction, y_batch)\n\n    def train_batch(self,\n                    X_batch: ndarray,\n                    y_batch: ndarray,\n                    inference: bool = False) -> float:\n\n        prediction = self.forward(X_batch, inference)\n\n        batch_loss = self.loss.forward(prediction, y_batch)\n        loss_grad = self.loss.backward()\n\n        self.backward(loss_grad)\n\n        return batch_loss","8ba89540":"class Optimizer(object):\n    def __init__(self,\n                 lr: float = 0.01,\n                 final_lr: float = 0,\n                 decay_type: str = None) -> None:\n        self.lr = lr\n        self.final_lr = final_lr\n        self.decay_type = decay_type\n        self.first = True\n\n    def _setup_decay(self) -> None:\n\n        if not self.decay_type:\n            return\n        elif self.decay_type == 'exponential':\n            self.decay_per_epoch = np.power(self.final_lr \/ self.lr,\n                                       1.0 \/ (self.max_epochs - 1))\n        elif self.decay_type == 'linear':\n            self.decay_per_epoch = (self.lr - self.final_lr) \/ (self.max_epochs - 1)\n\n    def _decay_lr(self) -> None:\n\n        if not self.decay_type:\n            return\n\n        if self.decay_type == 'exponential':\n            self.lr *= self.decay_per_epoch\n\n        elif self.decay_type == 'linear':\n            self.lr -= self.decay_per_epoch\n\n    def step(self,\n             epoch: int = 0) -> None:\n\n        for (param, param_grad) in zip(self.net.params(),\n                                       self.net.param_grads()):\n            self._update_rule(param=param,\n                              grad=param_grad)\n\n    def _update_rule(self, **kwargs) -> None:\n        raise NotImplementedError()","9f7b05e2":"class SGD(Optimizer):\n    def __init__(self,\n                 lr: float = 0.01,\n                 final_lr: float = 0,\n                 decay_type: str = None) -> None:\n        super().__init__(lr, final_lr, decay_type)\n\n    def _update_rule(self, **kwargs) -> None:\n\n        update = self.lr*kwargs['grad']\n        kwargs['param'] -= update\n\n        \nclass SGDMomentum(Optimizer):\n    def __init__(self,\n                 lr: float = 0.01,\n                 final_lr: float = 0,\n                 decay_type: str = None,\n                 momentum: float = 0.9) -> None:\n        super().__init__(lr, final_lr, decay_type)\n        self.momentum = momentum\n\n    def step(self) -> None:\n        if self.first:\n            self.velocities = [np.zeros_like(param)\n                               for param in self.net.params()]\n            self.first = False\n\n        for (param, param_grad, velocity) in zip(self.net.params(),\n                                                 self.net.param_grads(),\n                                                 self.velocities):\n            self._update_rule(param=param,\n                              grad=param_grad,\n                              velocity=velocity)\n\n    def _update_rule(self, **kwargs) -> None:\n\n            # Update velocity\n            kwargs['velocity'] *= self.momentum\n            kwargs['velocity'] += self.lr * kwargs['grad']\n\n            # Use this to update parameters\n            kwargs['param'] -= kwargs['velocity']","59102977":"class Flatten(Operation):\n    def __init__(self):\n        super().__init__()\n\n    def _output(self, inference: bool = False) -> ndarray:\n        return self.input_.reshape(self.input_.shape[0], -1)\n\n    def _input_grad(self, output_grad: ndarray) -> ndarray:\n        return output_grad.reshape(self.input_.shape)","0d18a6e6":"class Dropout(Operation):\n\n    def __init__(self,\n                 keep_prob: float = 0.8):\n        super().__init__()\n        self.keep_prob = keep_prob\n\n    def _output(self, inference: bool) -> ndarray:\n        if inference:\n            return self.input_ * self.keep_prob\n        else:\n            self.mask = np.random.binomial(1, self.keep_prob,\n                                           size=self.input_.shape)\n            return self.input_ * self.mask\n\n    def _input_grad(self, output_grad: ndarray) -> ndarray:\n        return output_grad * self.mask","79136068":"def assert_dim(t: Tensor,\n               dim: Tensor):\n    assert len(t.shape) == dim, \\\n        '''\n        Tensor expected to have dimension {0}, instead has dimension {1}\n        '''.format(dim, len(t.shape))\n    return None\n\n\ndef assert_same_shape(output: np.ndarray,\n                      output_grad: np.ndarray):\n    assert output.shape == output_grad.shape, \\\n        '''\n        Two tensors should have the same shape;\n        instead, first Tensor's shape is {0}\n        and second Tensor's shape is {1}.\n        '''.format(tuple(output_grad.shape), tuple(output.shape))\n    return None","3fdbdcc3":"class Conv2D_Op(ParamOperation):\n\n    def __init__(self, W: ndarray):\n        super().__init__(W)\n        self.param_size = W.shape[2]\n        self.param_pad = self.param_size \/\/ 2\n\n    def _pad_1d(self, inp: ndarray) -> ndarray:\n        z = np.array([0])\n        z = np.repeat(z, self.param_pad)\n        return np.concatenate([z, inp, z])\n\n    def _pad_1d_batch(self,\n                      inp: ndarray) -> ndarray:\n        outs = [self._pad_1d(obs) for obs in inp]\n        return np.stack(outs)\n\n    def _pad_2d_obs(self,\n                    inp: ndarray):\n        '''\n        Input is a 2 dimensional, square, 2D Tensor\n        '''\n        inp_pad = self._pad_1d_batch(inp)\n\n        other = np.zeros((self.param_pad, inp.shape[0] + self.param_pad * 2))\n\n        return np.concatenate([other, inp_pad, other])\n\n\n    def _pad_2d_channel(self,\n                        inp: ndarray):\n        '''\n        inp has dimension [num_channels, image_width, image_height]\n        '''\n        return np.stack([self._pad_2d_obs(channel) for channel in inp])\n\n    def _get_image_patches(self,\n                           input_: ndarray):\n        imgs_batch_pad = np.stack([self._pad_2d_channel(obs) for obs in input_])\n        patches = []\n        img_height = imgs_batch_pad.shape[2]\n        for h in range(img_height-self.param_size+1):\n            for w in range(img_height-self.param_size+1):\n                patch = imgs_batch_pad[:, :, h:h+self.param_size, w:w+self.param_size]\n                patches.append(patch)\n        return np.stack(patches)\n\n    def _output(self,\n                inference: bool = False):\n        '''\n        conv_in: [batch_size, channels, img_width, img_height]\n        param: [in_channels, out_channels, fil_width, fil_height]\n        '''\n        assert_dim(self.input_, 4)\n        \n        batch_size = self.input_.shape[0]\n        img_height = self.input_.shape[2]\n        img_size = self.input_.shape[2] * self.input_.shape[3]\n        patch_size = self.param.shape[0] * self.param.shape[2] * self.param.shape[3]\n\n        patches = self._get_image_patches(self.input_)\n\n        patches_reshaped = (patches\n                            .transpose(1, 0, 2, 3, 4)\n                            .reshape(batch_size, img_size, -1))\n\n        param_reshaped = (self.param\n                          .transpose(0, 2, 3, 1)\n                          .reshape(patch_size, -1))\n\n        output_reshaped = (\n            np.matmul(patches_reshaped, param_reshaped)\n            .reshape(batch_size, img_height, img_height, -1)\n            .transpose(0, 3, 1, 2))\n\n        return output_reshaped\n\n\n    def _input_grad(self, output_grad: np.ndarray) -> np.ndarray:\n\n        batch_size = self.input_.shape[0]\n        img_size = self.input_.shape[2] * self.input_.shape[3]\n        img_height = self.input_.shape[2]\n\n        output_patches = (self._get_image_patches(output_grad)\n                          .transpose(1, 0, 2, 3, 4)\n                          .reshape(batch_size * img_size, -1))\n\n        param_reshaped = (self.param\n                          .reshape(self.param.shape[0], -1)\n                          .transpose(1, 0))\n\n        return (\n            np.matmul(output_patches, param_reshaped)\n            .reshape(batch_size, img_height, img_height, self.param.shape[0])\n            .transpose(0, 3, 1, 2)\n        )\n\n\n    def _param_grad(self, output_grad: ndarray) -> ndarray:\n\n        batch_size = self.input_.shape[0]\n        img_size = self.input_.shape[2] * self.input_.shape[3]\n        in_channels = self.param.shape[0]\n        out_channels = self.param.shape[1]\n\n        in_patches_reshape = (\n            self._get_image_patches(self.input_)\n            .reshape(batch_size * img_size, -1)\n            .transpose(1, 0)\n            )\n\n        out_grad_reshape = (output_grad\n                            .transpose(0, 2, 3, 1)\n                            .reshape(batch_size * img_size, -1))\n\n        return (np.matmul(in_patches_reshape,\n                          out_grad_reshape)\n                .reshape(in_channels, self.param_size, self.param_size, out_channels)\n                .transpose(0, 3, 1, 2))","ec873ebb":"\nclass Conv2D(Layer):\n    '''\n    Once we define all the Operations and the outline of a layer,\n    all that remains to implement here is the _setup_layer function!\n    '''\n    def __init__(self,\n                 out_channels: int,\n                 param_size: int,\n                 dropout: int = 1.0,\n                 weight_init: str = \"normal\",\n                 activation: Operation = Linear(),\n                 flatten: bool = False) -> None:\n        super().__init__(out_channels)\n        self.param_size = param_size\n        self.activation = activation\n        self.flatten = flatten\n        self.dropout = dropout\n        self.weight_init = weight_init\n        self.out_channels = out_channels\n\n    def _setup_layer(self, input_: ndarray) -> ndarray:\n\n        self.params = []\n        in_channels = input_.shape[1]\n\n        if self.weight_init == \"glorot\":\n            scale = 2\/(in_channels + self.out_channels)\n        else:\n            scale = 1.0\n\n        conv_param = np.random.normal(loc=0,\n                                      scale=scale,\n                                      size=(input_.shape[1],  # input channels\n                                     self.out_channels,\n                                     self.param_size,\n                                     self.param_size))\n\n        self.params.append(conv_param)\n\n        self.operations = []\n        self.operations.append(Conv2D_Op(conv_param))\n        self.operations.append(self.activation)\n\n        if self.flatten:\n            self.operations.append(Flatten())\n\n        if self.dropout < 1.0:\n            self.operations.append(Dropout(self.dropout))\n\n        return None","b22b5ff6":"class Trainer(object):\n    '''\n    Just a list of layers that runs forwards and backwards\n    '''\n    def __init__(self,\n                 net: NeuralNetwork,\n                 optim: Optimizer) -> None:\n        self.net = net\n        self.optim = optim\n        self.best_loss = 1e9\n        setattr(self.optim, 'net', self.net)\n\n    def fit(self, train_batches: DataLoader,\n            test_batches: DataLoader,\n            epochs: int=100,\n            eval_every: int=10,\n            seed: int = 1,\n            single_output: bool = False,\n            restart: bool = True,\n            early_stopping: bool = True,\n            conv_testing: bool = False)-> None:\n\n        setattr(self.optim, 'max_epochs', epochs)\n        self.optim._setup_decay()\n\n        np.random.seed(seed)\n        if restart:\n            for layer in self.net.layers:\n                layer.first = True\n\n            self.best_loss = 1e9\n            \n\n        epoch_losses = {}\n        \n        for e in range(epochs):\n\n            if (e+1) % eval_every == 0:\n\n                last_model = deepcopy(self.net)\n\n            batch_losses = []\n            valid_losses = []\n            \n            for ii, (X_batch, y_batch) in enumerate(train_batches):\n                \n                self.net.train_batch(X_batch, y_batch)\n\n                self.optim.step()\n\n                if conv_testing:\n                    if ii % eval_every == 0:\n                        test_preds = self.net.forward(X_batch, inference=True)\n                        batch_loss = self.net.loss.forward(test_preds, y_batch)\n                        batch_losses.append(batch_loss \/ 100.0)\n                        \n                        X_test, y_test = next(iter(test_batches))\n                        valid_losses.append(np.equal(np.argmax(self.net.forward(X_test, inference=True), axis=1), np.argmax(y_test, axis=1)).sum() \/ X_test.shape[0])\n                        \n                        print(\"|-- Epoch:{0:3d},   Batch:{1:4d},   Loss:{2:1.4f},   Value_Loss:{3:1.4f}\".format((e + 1), ii, batch_losses[-1], valid_losses[-1]))\n\n            batch_losses = np.array(batch_losses)\n            valid_losses = np.array(valid_losses)\n            \n            epoch_losses[e] = batch_losses, valid_losses\n            \n            print('---------------------------------------------------')\n            print('Epoch:{0:3d}, Epoch Loss:{1:3f}, Valid Loss:{2:3f}'.format((e + 1), np.mean(batch_losses), np.mean(valid_losses)))\n            print('---------------------------------------------------')\n                \n            if self.optim.final_lr:\n                self.optim._decay_lr()\n                \n        return epoch_losses","ad2ec9ec":"model = NeuralNetwork(\n            layers=[Conv2D(out_channels=16,\n                           param_size=5,\n                           dropout=0.3,\n                           weight_init=\"glorot\",\n                           flatten=False,\n                           activation=Tanh()),\n                    Conv2D(out_channels=32,\n                           param_size=5,\n                           dropout=0.3,\n                           weight_init=\"glorot\",\n                           flatten=True,\n                           activation=Tanh()),\n                    Dense(neurons=7, \n                           activation=Linear())],\n            loss = SoftmaxCrossEntropy(), \n                   seed=20190406)\n\ntrainer = Trainer(model, SGDMomentum(lr=0.001, momentum=0.9))\nlosses = trainer.fit(train_batches,\n                     test_batches,\n                     epochs = 1,\n                     eval_every = 100,\n                     seed=201904026,\n                     conv_testing=True)","36a4befe":"def draw(losses_values):\n    fig, ax = plt.subplots()\n    ax.plot(losses_values[0], '-bo', label='Losses')\n    ax.plot(losses_values[1], '--ro', label='Valid Losses')\n    ax.axis('equal')\n    leg = ax.legend();","1d5deff3":"losses_values = np.array(list(losses.values()))\n\ndraw(losses_values[0])","b3a09a5c":"# Neural Network","6c4c2ac6":"# Conclusion\n\nWe can see clearly that our model is training well but it's super slow due to CPU usage.","8f109782":"<h1 align='center' style='color:blue; border:1px dotted blue;'>Deep Learning Framework<\/h1>","38442b6c":"# Dataset","46255246":"# Utils","adb82730":"# Trainer","290e3293":"# Loss Functions","9d36189f":"# Dense","3d73631b":"# Dropout","ae7f72fc":"# Layer","9e8c7b5c":"# Optimizer","a4c95973":"# Conv2D","d2d9499f":"# Reshape","9400cf18":"# Analyze Training","dff128d0":"<h1 align='center' style='color:green; border:1px dotted green;'>Emotion Detection<\/h1>","05fae047":"# Goal\n\nThe goal of this notebook is to show how you can implement a Deep Learning Framework from scratch.","9e747088":"# Activation Functions"}}