{"cell_type":{"ae7c176e":"code","38b08e08":"code","b99b1523":"code","71efbf6b":"code","ac6f6666":"code","b08724ab":"code","4dfa8410":"code","b4695649":"code","ae65fdcf":"code","47cd4ef4":"code","43e5ad60":"code","7bea108d":"code","1dbca98b":"code","e38033c0":"code","aad0791b":"code","278c0643":"code","ea64ab4e":"code","42c009ff":"code","38d7a8d5":"code","f3e3de9e":"code","842dae9d":"code","bc4b1035":"code","a5f2a6db":"code","0785b190":"code","74e30c05":"code","46157391":"code","92d4fb7e":"code","59a366a0":"code","1e7e8827":"code","d04e38ee":"code","e56e89d5":"code","ce417e5b":"code","acc9e273":"code","b938874d":"code","897e81e5":"code","7fea3998":"code","acaf680d":"code","c7cd9c12":"code","1872d776":"code","0801bf6f":"code","b06764f3":"code","5f48803a":"code","066bc346":"code","1c565a07":"code","3b8bb105":"code","a5cfa5ea":"code","71abd0b0":"code","bdaaa0e1":"code","e5322991":"code","422e8349":"code","8a5fb656":"markdown","a5fa724a":"markdown","41e68b16":"markdown","1d586daa":"markdown","fd5d2342":"markdown","d63b2e74":"markdown","79145cf7":"markdown","c072ff62":"markdown","6fffad61":"markdown","af29adb7":"markdown","6c6801eb":"markdown","da6cb40f":"markdown","0bebf905":"markdown","cedfc96d":"markdown","e7af7067":"markdown","f149d528":"markdown","7d4195ba":"markdown","270b5a2b":"markdown","b1d3c812":"markdown","4c211dc5":"markdown","77d21001":"markdown","8ab19b40":"markdown","8713c9b1":"markdown","0e030e2d":"markdown","2b62930c":"markdown"},"source":{"ae7c176e":"import numpy as np \nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nplt.style.use('seaborn')\n# sns.set(font_scale = 2.5)\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n%matplotlib inline","38b08e08":"from sklearn.tree import DecisionTreeClassifier\nfrom sklearn.datasets import load_iris\nfrom sklearn.model_selection import train_test_split","b99b1523":"dt_clf = DecisionTreeClassifier()","71efbf6b":"iris_data = load_iris()\nX_train, X_test, y_train, y_test = train_test_split(iris_data.data, iris_data.target,\\\n                                                   test_size=0.2, random_state=2019)","ac6f6666":"dt_clf.fit(X_train, y_train)","b08724ab":"from sklearn.tree import export_graphviz\n\nexport_graphviz(dt_clf, out_file='tree.dot', class_names=iris_data.target_names, \\\n               feature_names = iris_data.feature_names, impurity=True, filled=True)","4dfa8410":"import graphviz\nwith open('tree.dot') as f:\n    dot_graph = f.read()\ngraphviz.Source(dot_graph)","b4695649":"print('Feature Importance: \\n{}'.format(np.round(dt_clf.feature_importances_, 3)))\n\nfor name, value in zip(iris_data.feature_names, dt_clf.feature_importances_):\n    print('{0} : {1:.3f}'.format(name, value))\n\nsns.barplot(x=dt_clf.feature_importances_, y=iris_data.feature_names)","ae65fdcf":"from sklearn.datasets import make_classification\n\nplt.title('3 Class values with 2 Features Sample data creation')\n\nX_features, y_labels = make_classification(n_features=2, n_redundant=0, n_informative=2, n_classes=3, n_clusters_per_class=1, random_state=2019)\n\nplt.scatter(X_features[:, 0], X_features[:, 1], marker='o', c=y_labels, s=25, edgecolor='k' )","47cd4ef4":"def visualize_boundary(model, X, y):\n    fig,ax = plt.subplots()\n    \n    # \ud559\uc2b5 \ub370\uc774\ud0c0 scatter plot\uc73c\ub85c \ub098\ud0c0\ub0b4\uae30\n    ax.scatter(X[:, 0], X[:, 1], c=y, s=25, cmap='rainbow', edgecolor='k',\n               clim=(y.min(), y.max()), zorder=3)\n    ax.axis('tight')\n    ax.axis('off')\n    xlim_start , xlim_end = ax.get_xlim()\n    ylim_start , ylim_end = ax.get_ylim()\n    \n    # \ud638\ucd9c \ud30c\ub77c\ubbf8\ud130\ub85c \ub4e4\uc5b4\uc628 training \ub370\uc774\ud0c0\ub85c model \ud559\uc2b5 . \n    model.fit(X, y)\n    # meshgrid \ud615\ud0dc\uc778 \ubaa8\ub4e0 \uc88c\ud45c\uac12\uc73c\ub85c \uc608\uce21 \uc218\ud589. \n    xx, yy = np.meshgrid(np.linspace(xlim_start,xlim_end, num=200),np.linspace(ylim_start,ylim_end, num=200))\n    Z = model.predict(np.c_[xx.ravel(), yy.ravel()]).reshape(xx.shape)\n    \n    # contourf() \ub97c \uc774\uc6a9\ud558\uc5ec class boundary \ub97c visualization \uc218\ud589. \n    n_classes = len(np.unique(y))\n    contours = ax.contourf(xx, yy, Z, alpha=0.3,\n                           levels=np.arange(n_classes + 1) - 0.5,\n                           cmap='rainbow', clim=(y.min(), y.max()),\n                           zorder=1)","43e5ad60":"dt_clf = DecisionTreeClassifier().fit(X_features, y_labels)\nvisualize_boundary(dt_clf, X_features, y_labels)","7bea108d":"dt_clf = DecisionTreeClassifier(min_samples_leaf=6).fit(X_features, y_labels)\nvisualize_boundary(dt_clf, X_features, y_labels)","1dbca98b":"feature_name_df = pd.read_csv('..\/input\/features.txt',sep='\\s+', header=None, names=['column_index', 'column_name'])\nfeature_name = feature_name_df.iloc[:, 1].values.tolist()\nprint('\uc804\uccb4 \ud53c\ucc98\uba85\uc5d0\uc11c 10\uac1c\ub9cc \ucd94\ucd9c: ', feature_name[:10])","e38033c0":"def get_human_dataset():\n    feature_name_df = pd.read_csv('..\/input\/features.txt',sep='\\s+', header=None, names=['column_index', 'column_name'])\n    feature_name = feature_name_df.iloc[:, 1].values.tolist()\n    \n    X_train = pd.read_csv('..\/input\/X_train.txt', sep='\\s+', names=feature_name)\n    X_test = pd.read_csv('..\/input\/X_test.txt', sep='\\s+', names=feature_name)\n    \n    y_train = pd.read_csv('..\/input\/y_train.txt', sep='\\s+', header=None, names=['action'])\n    y_test = pd.read_csv('..\/input\/y_test.txt', sep='\\s+', header=None, names=['action'])\n    \n    return X_train, X_test, y_train, y_test\n\nX_train, X_test, y_train, y_test = get_human_dataset()","aad0791b":"X_train.info()","278c0643":"X_train.head()","ea64ab4e":"X_train.describe()","42c009ff":"y_train.info()","38d7a8d5":"y_train['action'].value_counts()","f3e3de9e":"from sklearn.tree import DecisionTreeClassifier\nfrom sklearn.metrics import accuracy_score\n\ndt_clf = DecisionTreeClassifier(random_state=2019)\ndt_clf.fit(X_train, y_train)\npred = dt_clf.predict(X_test)\naccuracy = accuracy_score(y_test, pred)\nprint('Decision Tree Accuracy: {:.4f}'.format(accuracy))\n\nprint('Decision Tree Basic Hyper-parameters: \\n', dt_clf.get_params())","842dae9d":"from sklearn.model_selection import GridSearchCV\n\nparams = {'max_depth':[6,8,10,12,16,20,24]}\n\ngrid_cv = GridSearchCV(dt_clf, param_grid=params, scoring='accuracy', cv=5, verbose=1)\ngrid_cv.fit(X_train, y_train)\nprint('GridSearchCV Best Average Accuracy: {:.4f}'.format(grid_cv.best_score_))\nprint('GridSearchCV Optimal Hyper-parameter: ', grid_cv.best_params_)","bc4b1035":"cv_results_df = pd.DataFrame(grid_cv.cv_results_)\ncv_results_df[['param_max_depth', 'mean_test_score', 'mean_train_score']]","a5f2a6db":"max_depths = [6,8,10,12,16,20,24]\n\nfor depth in max_depths:\n    dt_clf = DecisionTreeClassifier(max_depth=depth, random_state=2019)\n    dt_clf.fit(X_train, y_train)\n    pred = dt_clf.predict(X_test)\n    accuracy = accuracy_score(y_test, pred)\n    print('Max_depth = {0} Accuracy: {1:.4f}'.format(depth, accuracy))","0785b190":"params = {'max_depth':[6,8,10,12,16,20,24], 'min_samples_split':[16, 24]}\n\ngrid_cv = GridSearchCV(dt_clf, param_grid=params, scoring='accuracy', cv=5, verbose=1)\ngrid_cv.fit(X_train, y_train)\nprint('GridSearchCV Best Average Accuracy: {:.4f}'.format(grid_cv.best_score_))\nprint('GridSearchCV Optimal Hyper-parameter: ', grid_cv.best_params_)","74e30c05":"best_df_clf = grid_cv.best_estimator_\npred1 = best_df_clf.predict(X_test)\naccuracy = accuracy_score(y_test, pred1)\nprint('Decision Tree Accuracy: {:.4f}'.format(accuracy))","46157391":"ftr_importances_values = best_df_clf.feature_importances_\nftr_importances = pd.Series(ftr_importances_values, index=X_train.columns)\n\nftr_top20 = ftr_importances.sort_values(ascending=False)[:20]\nplt.figure(figsize=(8,6))\nplt.title('Feature Importances Top 20')\nsns.barplot(x=ftr_top20, y=ftr_top20.index)\nplt.show()","92d4fb7e":"from sklearn.ensemble import VotingClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.datasets import load_breast_cancer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score\n\ncancer = load_breast_cancer()","59a366a0":"data_df = pd.DataFrame(cancer.data, columns=cancer.feature_names)\ndata_df.head()","1e7e8827":"lr_clf = LogisticRegression()\nknn_clf= KNeighborsClassifier(n_neighbors=8)\n\nvo_clf = VotingClassifier(estimators=[('LR', lr_clf), ('KNN', knn_clf)], voting='soft')\n\nX_train, X_test, y_train, y_test = train_test_split(cancer.data, cancer.target, test_size=0.2, random_state=2019)\n\nvo_clf.fit(X_train, y_train)\npred = vo_clf.predict(X_test)\nprint('Voting Classifier Accuracy: {:.4f}'.format(accuracy_score(pred, y_test)))","d04e38ee":"classifiers = [lr_clf, knn_clf]\nfor classifier in classifiers:\n    classifier.fit(X_train, y_train)\n    pred = classifier.predict(X_test)\n    class_name = classifier.__class__.__name__\n    print('{0} \uc815\ud655\ub3c4: {1:.4f}'.format(class_name, accuracy_score(pred, y_test)))","e56e89d5":"from sklearn.ensemble import RandomForestClassifier","ce417e5b":"X_train, X_test, y_train, y_test = get_human_dataset()","acc9e273":"rf_clf = RandomForestClassifier(random_state=2019)\nrf_clf.fit(X_train, y_train)\npred=rf_clf.predict(X_test)\nprint('Random Forest Accuracy: ', accuracy_score(pred, y_test))","b938874d":"from sklearn.model_selection import GridSearchCV\n\nparams = {'n_estimators':[100], 'max_depth':[6,8,10,12], 'min_samples_leaf':[8,12,18], 'min_samples_split': [8, 16, 20]}\n\nrf_clf = RandomForestClassifier(random_state=2019, n_jobs=1)\ngrid_cv = GridSearchCV(rf_clf, param_grid=params, cv=2, n_jobs=1)\ngrid_cv.fit(X_train, y_train)\n\nprint('Optimal hyper-parameters: \\n', grid_cv.best_params_)\nprint('Best Accuracy: {:.4f}'.format(grid_cv.best_score_))","897e81e5":"rf_clf1 = RandomForestClassifier(n_estimators=300, max_depth=8, min_samples_leaf=8, min_samples_split=20, random_state=2019)\nrf_clf1.fit(X_train, y_train)\npred = rf_clf1.predict(X_test)\nprint('Accuracy: ', accuracy_score(pred, y_test))","7fea3998":"ftr_importances_values = rf_clf1.feature_importances_\nftr_importances = pd.Series(ftr_importances_values, index=X_train.columns)\nftr_top20 = ftr_importances.sort_values(ascending=False)[:20]\n\nplt.figure(figsize=(8, 6))\nplt.title('Feature Importances Top 20')\nsns.barplot(x=ftr_top20, y=ftr_top20.index)\nplt.show()","acaf680d":"from sklearn.ensemble import GradientBoostingClassifier\nimport time\n\nX_train, X_test, y_training, y_test = get_human_dataset()\n\nstart_time = time.time()\n\ngb_clf = GradientBoostingClassifier(random_state=2019)\ngb_clf.fit(X_train, y_train)\ngb_pred = gb_clf.predict(X_test)\ngb_accuracy =  accuracy_score(gb_pred, y_test)\n\nprint('GBM Accuracy: ', gb_accuracy)\nprint('GBM Time: {:.1f} seconds'.format(time.time() - start_time))","c7cd9c12":"from sklearn.datasets import load_breast_cancer\nfrom sklearn.model_selection import train_test_split","1872d776":"dataset = load_breast_cancer()\nX_features = dataset.data\ny_label = dataset.target\ncancer_df = pd.DataFrame(data=X_features, columns=dataset.feature_names)\ncancer_df['target'] = y_label\ncancer_df.head()","0801bf6f":"cancer_df['target'].value_counts()","b06764f3":"X_train, X_test, y_train, y_test = train_test_split(X_features, y_label, test_size=0.2, random_state=2019)\nprint(X_train.shape, X_test.shape)","5f48803a":"from xgboost import XGBClassifier\n\nxgb_wrapper = XGBClassifier(n_estimator=400, learning_rate=0.1, max_depth=3)\nxgb_wrapper.fit(X_train, y_train)\nw_preds = xgb_wrapper.predict(X_test)","066bc346":"from sklearn.metrics import confusion_matrix, accuracy_score\nfrom sklearn.metrics import precision_score, recall_score\nfrom sklearn.metrics import f1_score, roc_auc_score\n\ndef get_clf_eval(y_test , pred):\n    confusion = confusion_matrix(y_test, pred)\n    accuracy = accuracy_score(y_test , pred)\n    precision = precision_score(y_test , pred)\n    recall = recall_score(y_test , pred)\n    f1 = f1_score(y_test,pred)\n    roc_auc = roc_auc_score(y_test, pred)\n    print('\uc624\ucc28 \ud589\ub82c')\n    print(confusion)\n    print('\uc815\ud655\ub3c4: {0:.4f}, \uc815\ubc00\ub3c4: {1:.4f}, \uc7ac\ud604\uc728: {2:.4f},\\\n    F1: {3:.4f}, AUC:{4:.4f}'.format(accuracy, precision, recall, f1, roc_auc))","1c565a07":"get_clf_eval(y_test, w_preds)","3b8bb105":"from xgboost import XGBClassifier\n\nxgb_wrapper = XGBClassifier(n_estimator=400, learning_rate=0.1, max_depth=3)\nevals=[(X_test, y_test)]\nxgb_wrapper.fit(X_train, y_train, early_stopping_rounds=100, eval_metric='logloss', eval_set=evals, verbose=True)\nws100_preds = xgb_wrapper.predict(X_test)","a5cfa5ea":"get_clf_eval(y_test, ws100_preds)","71abd0b0":"from xgboost import plot_importance\n\nfig, ax = plt.subplots(figsize=(10, 12))\nplot_importance(xgb_wrapper, ax=ax)","bdaaa0e1":"from lightgbm import LGBMClassifier\n\ndataset = load_breast_cancer()\nftr = dataset.data\ntarget = dataset.target\n\nX_train, X_test, y_train, y_test = train_test_split(ftr, target, test_size=0.2, random_state=2019)\n\nlgbm_wrapper = LGBMClassifier(n_estimator=400)\n\nevals=[(X_test, y_test)]\nlgbm_wrapper.fit(X_train, y_train, early_stopping_rounds=100, eval_metric='logloss', eval_set=evals, verbose=True)\npreds = lgbm_wrapper.predict(X_test)","e5322991":"get_clf_eval(y_test, preds)","422e8349":"from lightgbm import plot_importance\n\nfig, ax = plt.subplots(figsize=(10, 12))\nplot_importance(lgbm_wrapper, ax=ax)","8a5fb656":"- Generate Decision Classifer","a5fa724a":"- Let's check the influence that Tree Depth has on Accuracy!","41e68b16":"- Load iris data and divide the iris data into train and test data sets","1d586daa":"### Voting Classifier","fd5d2342":"- Learning","d63b2e74":"### Decision Tree Overfitting Issue","79145cf7":"# Random Forest","c072ff62":"- modification of 'min_samples_leaf'","6fffad61":"# XGBoost (eXtra Gradient Boost)","af29adb7":"### Tuning hyper-parameters with using GridSearchCV","6c6801eb":"The 'petal length' is the most important feature!","da6cb40f":"### Feature Importances","0bebf905":"# GBM (Gradient Boosting Machine)\n\n- AdaBoost\n\n- Gradient Boost Machine","cedfc96d":"- Check graph using Graphviz","e7af7067":"The deeper DT is, the more overfitting arises!\n\nSo, we have to control the deep with using hyper-parameters!\n","f149d528":"# Decision Tree","7d4195ba":"### Another Example: UCI Human Activity Recognition Using Smartphones","270b5a2b":"- Voting: between different classifiers to the same data set (Hard voting vs Soft voting)\n- Vagging: between one classifiers to the respective sampling data (bootstrapping method) (ex. RandomForest)\n- Boosting: between different classifiers with adding weights to wrong-predictions (ex. XGBoost, LightGBM)\n- Stacking: making predictions from different classifiers into training data and retraining such training data","b1d3c812":"\\>.<  Voting Clasifer is not always good!","4c211dc5":"# LightGBM","77d21001":"- Feature Importance","8ab19b40":"\\>.< It takes too long...","8713c9b1":"# Ensemble","0e030e2d":"### Feature Importances","2b62930c":"- Let's make a new Voting Classifier with using the soft-voting method based on LogisticRegression and KNN!"}}