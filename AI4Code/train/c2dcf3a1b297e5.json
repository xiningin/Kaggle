{"cell_type":{"62cef194":"code","efce3405":"code","aa04efdb":"code","8e3e4e09":"code","7dc44388":"code","454b1276":"code","ec6e14ea":"code","efca5927":"code","0f860a01":"code","2afb397c":"code","642a0c38":"code","f9dc072c":"code","2adc06cd":"code","b70e3d45":"code","89d6c8b0":"code","f33cca06":"code","62abe2ef":"code","eddf1a58":"code","25afe694":"code","d9861311":"code","b96cefbf":"code","cb4bfca4":"code","9e52d539":"code","08fe8bd5":"code","100188a7":"code","e52e94ce":"code","2e83637b":"code","e98c52f4":"code","279d5ba4":"code","a2f0dcec":"code","96421c17":"code","f778b069":"code","b0054b1f":"code","e001e5c5":"code","77e5b4f6":"code","1a38930b":"code","331eb0d9":"code","df753b53":"code","6282c0b9":"code","7a0a3b38":"code","f5e2c48e":"code","ba8318b5":"code","8ae2c4a1":"markdown","050ff5d6":"markdown","d6f2e70b":"markdown","7795fb6e":"markdown","7c22d149":"markdown","342a9f1a":"markdown","4560c1b3":"markdown","8fc34d00":"markdown","cf0efe01":"markdown","e49d8c37":"markdown","ab548796":"markdown","a3aa53fe":"markdown","bd55d03a":"markdown"},"source":{"62cef194":"#Importing the Required Librarires\nimport warnings\nwarnings.filterwarnings('ignore')\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns","efce3405":"#Loading Data Set\n\ndf = pd.read_csv(\"..\/input\/spam-text-message-classification\/SPAM text message 20170820 - Data.csv\")\ndf.head()","aa04efdb":"spam = df[df.Category == 'spam']\nspam.head()","8e3e4e09":"ham = df[df.Category == 'ham']\nham.head()","7dc44388":"# Spam Word clouds\nfrom os import path\nfrom PIL import Image\nfrom wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\ntext = \" \".join(review for review in spam.Message)\nwordcloud = WordCloud(max_font_size=50, max_words=100, background_color=\"white\").generate(text)\nfig = plt.figure(figsize = (20, 6)) \nplt.imshow(wordcloud, interpolation=\"bilinear\")\nplt.axis(\"off\")\nplt.show()","454b1276":"# Ham Word clouds\nfrom os import path\nfrom PIL import Image\nfrom wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\ntext = \" \".join(review for review in ham.Message)\nwordcloud = WordCloud(max_font_size=50, max_words=100, background_color=\"white\").generate(text)\nfig = plt.figure(figsize = (20, 6)) \nplt.imshow(wordcloud, interpolation=\"bilinear\")\nplt.axis(\"off\")\nplt.show()","ec6e14ea":"#ham and spam distributions\ndf_Spam=df[['Category','Message']].groupby('Category').count().reset_index()\ndf_Spam.columns=['Category','count']\ndf_Spam['percentage']=(df_Spam['count']\/df_Spam['count'].sum())*100\ndf_Spam","efca5927":"#Encoding dependent variable\n\nencode = {\n            \"Category\":     {\"ham\": 0, \"spam\": 1}\n         \n         }\n\ndf = df.replace(encode)\ndf.head()","0f860a01":"# Split data into training and test sets\nfrom sklearn.model_selection import train_test_split\nX = df['Message']\ny = df['Category']\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify = None)","2afb397c":"#importing CountVectorizer\nfrom sklearn.feature_extraction.text import CountVectorizer\n\nvectorizer = CountVectorizer()\n","642a0c38":"\nfrom sklearn.naive_bayes import MultinomialNB\nNB = MultinomialNB()\nNB.fit(vectorizer.fit_transform(X_train), y_train)\n","f9dc072c":"#Prediction on train set\nprediction = NB.predict(vectorizer.transform(X_train))","2adc06cd":"from sklearn.metrics import accuracy_score, confusion_matrix, classification_report","b70e3d45":"#Confusion matrix\nprint(confusion_matrix(y_train, prediction))","89d6c8b0":"#Classification Report\nprint(classification_report(y_train, prediction))","f33cca06":"#Prediction on test set\nprediction_test = NB.predict(vectorizer.transform(X_test))\n#Confusion matrix\nprint(confusion_matrix(y_test, prediction_test))","62abe2ef":"#Classification Report\nprint(classification_report(y_test, prediction_test))","eddf1a58":"#Importing Library\nfrom sklearn.linear_model import LogisticRegression","25afe694":"logreg = LogisticRegression(random_state=42)","d9861311":"#Building Logistic Regression  Model\nlogreg.fit(vectorizer.fit_transform(X_train), y_train)","b96cefbf":"#Prediction on train set\nprediction_logistic = logreg.predict(vectorizer.transform(X_train))\n#Confusion matrix\nprint(confusion_matrix(y_train, prediction_logistic))","cb4bfca4":"#classification_report\nprint(classification_report(y_train, prediction_logistic))","9e52d539":"#Evaluating on test data\n\nprediction_logistic_test = logreg.predict(vectorizer.transform(X_test))\n#Confusion matrix\nprint(confusion_matrix(y_test, prediction_logistic_test))\nprint(classification_report(y_test, prediction_logistic_test))","08fe8bd5":"from sklearn.svm import SVC\nsvclassifier = SVC(kernel='linear')\nsvclassifier.fit(vectorizer.fit_transform(X_train), y_train)","100188a7":"pred_svm_test = svclassifier.predict(vectorizer.transform(X_test))\n","e52e94ce":"print(confusion_matrix(y_test, pred_svm_test))\nprint(classification_report(y_test, pred_svm_test))","2e83637b":"from sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.naive_bayes import MultinomialNB\n\nvectorizer = TfidfVectorizer()\nNB = MultinomialNB()\nNB.fit(vectorizer.fit_transform(X_train), y_train)\n","e98c52f4":"#Prediction on train set\nprediction = NB.predict(vectorizer.transform(X_train))","279d5ba4":"from sklearn.metrics import accuracy_score, confusion_matrix, classification_report","a2f0dcec":"#Confusion matrix\nprint(confusion_matrix(y_train, prediction))","96421c17":"#Classification Report\nprint(classification_report(y_train, prediction))","f778b069":"#Prediction on test set\nprediction_test = NB.predict(vectorizer.transform(X_test))\n#Confusion matrix\nprint(confusion_matrix(y_test, prediction_test))","b0054b1f":"#Classification Report\nprint(classification_report(y_test, prediction_test))","e001e5c5":"#Importing Library\nfrom sklearn.linear_model import LogisticRegression","77e5b4f6":"logreg = LogisticRegression(random_state=42)","1a38930b":"#Building Logistic Regression  Model\nlogreg.fit(vectorizer.fit_transform(X_train), y_train)","331eb0d9":"#Prediction on train set\nprediction_logistic = logreg.predict(vectorizer.transform(X_train))\n#Confusion matrix\nprint(confusion_matrix(y_train, prediction_logistic))","df753b53":"#classification_report\nprint(classification_report(y_train, prediction_logistic))","6282c0b9":"#Evaluating on test data\n\nprediction_logistic_test = logreg.predict(vectorizer.transform(X_test))\n#Confusion matrix\nprint(confusion_matrix(y_test, prediction_logistic_test))\nprint(classification_report(y_test, prediction_logistic_test))","7a0a3b38":"from sklearn.svm import SVC\nsvclassifier = SVC(kernel='linear')\nsvclassifier.fit(vectorizer.fit_transform(X_train), y_train)","f5e2c48e":"pred_svm_test = svclassifier.predict(vectorizer.transform(X_test))\n","ba8318b5":"print(confusion_matrix(y_test, pred_svm_test))\nprint(classification_report(y_test, pred_svm_test))","8ae2c4a1":"By checking Countvectorizer, TF-IDF methods to convert messages into vectors Countvectorizer is more efficient(for this problem) as it gives better performance than TF-IDF ","050ff5d6":"### Model Evaluation","d6f2e70b":"Types of Navie Baye's Classifier::\n\nGaussian: It is used in classification and it assumes that features follow a normal distribution.\n\nMultinomial: suitable for classification with discrete features (e.g., word counts for text classification). The multinomial distribution normally requires integer feature counts. \u201cnumber of times outcome number x_i is observed over the n trials\u201d.\n\nBernoulli: The binomial model is useful if your feature vectors are binary (i.e. zeros and ones). One application would be text classification with \u2018bag of words\u2019 model where the 1s & 0s are \u201cword occurs in the document\u201d and \u201cword does not occur in the document\u201d respectively.\n","7795fb6e":"##### Statement Regarding the Problem:\nWe are going to build a Spam Message Detection Model to detect spam messages, we have both spam and ham message in the data we have to interpret whether the message is spam or not","7c22d149":"* ","342a9f1a":"### Logistic Regression Model","4560c1b3":"#### SVM","8fc34d00":"##### Results\n\n##### SVM :\n\nrecall: 89%\n\naccuracy: 98%\n\n##### Multinomial Navie Bayes :\n\nrecall: 92%\n\naccuracy: 99%\n\n##### Logistic Regression :\n\nrecall: 86%\n\naccuracy: 98%\n","cf0efe01":"### SVM Classifier","e49d8c37":"### Building Multinomial Navie Baye's Classifier","ab548796":"##### TF-IDF method:","a3aa53fe":"### CountVectorizer\nCountVectorizer tokenizes(tokenization means breaking down a sentence or paragraph or any text into words) the text along with performing very basic preprocessing like removing the punctuation marks, converting all the words to lowercase, etc.\n\n\nThe vocabulary of known words is formed which is also used for encoding unseen text later.\n\nAn encoded vector is returned with a length of the entire vocabulary and an integer count for the number of times each word appeared in the document","bd55d03a":"#### Logistic Regression"}}