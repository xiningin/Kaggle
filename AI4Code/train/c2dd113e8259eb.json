{"cell_type":{"182ee2d0":"code","ecd07b96":"code","b623f656":"code","e2bbc87a":"code","de3493ad":"code","9283f9e4":"code","39411533":"code","ecc84394":"code","d1456e78":"code","8341d836":"code","c252138a":"code","77fcae19":"code","a485c50c":"code","e4c76973":"code","6dcd5e72":"code","32fb35d1":"code","9dbaa12c":"code","88a88110":"code","2ec0bb92":"code","dab552fa":"code","a30f54dd":"code","f84cfcbb":"code","941220dc":"code","0f4f3be8":"markdown","fa53c7bf":"markdown","7296cfdf":"markdown","ea5de3b7":"markdown","15b80791":"markdown","075dbfae":"markdown","c95a9bc4":"markdown","87324084":"markdown","f0918f45":"markdown","eee3bce1":"markdown","b6e10ee5":"markdown","dac869e9":"markdown","f41b2211":"markdown","56ea1983":"markdown","8b0109be":"markdown","51946399":"markdown"},"source":{"182ee2d0":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","ecd07b96":"# import data\ntrain = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/train.csv')\ntest = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/test.csv')\n\ntrain.head()","b623f656":"print(train.dtypes)","e2bbc87a":"print(train['SaleCondition'].unique())","de3493ad":"# string label to categorical values\nfrom sklearn.preprocessing import LabelEncoder\n\nfor i in range(train.shape[1]):\n    if train.iloc[:,i].dtypes == object:\n        lbl = LabelEncoder()\n        lbl.fit(list(train.iloc[:,i].values) + list(test.iloc[:,i].values))\n        train.iloc[:,i] = lbl.transform(list(train.iloc[:,i].values))\n        test.iloc[:,i] = lbl.transform(list(test.iloc[:,i].values))\n\nprint(train['SaleCondition'].unique())","9283f9e4":"# search for missing data\nimport missingno as msno\nmsno.matrix(df=train, figsize=(20,14), color=(0.5,0,0))","39411533":"# keep ID for submission\ntrain_ID = train['Id']\ntest_ID = test['Id']\n\ny_train = train['SalePrice']\nX_train = train.drop(['Id','SalePrice'], axis=1)\nX_test = test.drop('Id', axis=1)\n\nXmat = pd.concat([X_train, X_test])\n\nXmat.isnull().sum()","ecc84394":"# dealing with missing data\n\nXmat = Xmat.drop(['LotFrontage','MasVnrArea','GarageYrBlt'], axis=1)\nXmat = Xmat.fillna(Xmat.median())\n\n# check whether there are still nan\nimport missingno as msno\nmsno.matrix(df=Xmat, figsize=(20,14), color=(0.5,0,0))","d1456e78":"print(Xmat.columns.values)\nprint(str(Xmat.shape[1]) + ' columns')","8341d836":"# add a new feature 'total sqfootage'\nXmat['TotalSF'] = Xmat['TotalBsmtSF'] + Xmat['1stFlrSF'] + Xmat['2ndFlrSF']\nprint('There are currently ' + str(Xmat.shape[1]) + ' columns.')","c252138a":"# normality check for the target\nax = sns.distplot(y_train)\nplt.show()","77fcae19":"# log-transform the dependent variable for normality\ny_train = np.log(y_train)\n\nax = sns.distplot(y_train)\nplt.show()","a485c50c":"# train and test\nX_train = Xmat.iloc[:train.shape[0],:]\nX_test = Xmat.iloc[train.shape[0]:,:]\n\n# Compute the correlation matrix\ncorr = X_train.corr()\n\n\nf, ax = plt.subplots(figsize=(11, 9))\n\n\n\n# Draw the heatmap with the mask and correct aspect ratio\nsns.heatmap(corr, square=True)\n\nplt.show()","e4c76973":"# feature importance using random forest\nfrom sklearn.ensemble import RandomForestRegressor\nrf = RandomForestRegressor(n_estimators=80, max_features='auto')\nrf.fit(X_train, y_train)\nprint('Training done using Random Forest')\n\nranking = np.argsort(-rf.feature_importances_)\nf, ax = plt.subplots(figsize=(11, 9))\nsns.barplot(x=rf.feature_importances_[ranking], y=X_train.columns.values[ranking], orient='h')\nax.set_xlabel(\"feature importance\")\nplt.tight_layout()\nplt.show()","6dcd5e72":"# use the top 30 features only\nX_train = X_train.iloc[:,ranking[:30]]\nX_test = X_test.iloc[:,ranking[:30]]\n\n# zscoring\nX_train = (X_train - X_train.mean())\/X_train.std()\nX_test = (X_test - X_test.mean())\/X_test.std()\n    \n# heatmap\nf, ax = plt.subplots(figsize=(11, 5))\ncmap = sns.cubehelix_palette(light=1, as_cmap=True)\nsns.heatmap(X_train, cmap=cmap)\nplt.show()","32fb35d1":"# relation to the target\nfig = plt.figure(figsize=(12,7))\nfor i in np.arange(30):\n    ax = fig.add_subplot(5,6,i+1)\n    sns.regplot(x=X_train.iloc[:,i], y=y_train)\n\nplt.tight_layout()\nplt.show()","9dbaa12c":"# outlier deletion\nXmat = X_train\nXmat['SalePrice'] = y_train\nXmat = Xmat.drop(Xmat[(Xmat['TotalSF']>5) & (Xmat['SalePrice']<12.5)].index)\nXmat = Xmat.drop(Xmat[(Xmat['GrLivArea']>5) & (Xmat['SalePrice']<13)].index)\n\n# recover\ny_train = Xmat['SalePrice']\nX_train = Xmat.drop(['SalePrice'], axis=1)","88a88110":"import xgboost as xgb\nfrom sklearn.model_selection import GridSearchCV\n\nprint(\"Parameter optimization\")\nxgb_model = xgb.XGBRegressor()\nreg_xgb = GridSearchCV(xgb_model,{'max_depth': [2,4,6,8,10], 'n_estimators': [50,100,150,200]}, verbose=1)\nreg_xgb.fit(X_train, y_train)\nprint(reg_xgb.best_score_)\nprint(reg_xgb.best_params_)","2ec0bb92":"from keras.models import Sequential\nfrom keras.layers import Dense\nfrom keras.wrappers.scikit_learn import KerasRegressor\n\ndef create_model(optimizer='adam'):\n    model = Sequential()\n    model.add(Dense(X_train.shape[1], input_dim=X_train.shape[1], kernel_initializer='normal', activation='relu'))\n    model.add(Dense(16, kernel_initializer='normal', activation='relu'))\n    model.add(Dense(1, kernel_initializer='normal'))\n\n    model.compile(loss='mean_squared_error', optimizer=optimizer)\n    return model\n\nmodel = KerasRegressor(build_fn=create_model, verbose=0)\n# define the grid search parameters\noptimizer = ['SGD','Adam']\nbatch_size = [10, 30, 50, 70]\nepochs = [10, 50, 70, 100]\nparam_grid = dict(optimizer=optimizer, batch_size=batch_size, epochs=epochs)\nreg_dl = GridSearchCV(estimator=model, param_grid=param_grid, n_jobs=-1)\nreg_dl.fit(X_train, y_train)\n\nprint(reg_dl.best_score_)\nprint(reg_dl.best_params_)","dab552fa":"# SVR\nfrom sklearn.svm import SVR\n\nreg_svr = GridSearchCV(SVR(kernel='rbf', gamma=0.1), cv=5, param_grid={\"C\": [1e0, 1e1, 1e2, 1e3, 1e4], \"gamma\": np.logspace(-5, -2, 2, 5)})\nreg_svr.fit(X_train, y_train)\n\nprint(reg_svr.best_score_)\nprint(reg_svr.best_params_)","a30f54dd":"# second feature matrix\nX_train2 = pd.DataFrame( {'XGB': reg_xgb.predict(X_train),  'NN': reg_dl.predict(X_train).ravel(), 'SVR': reg_svr.predict(X_train) })\nX_train2.head()","f84cfcbb":"# second-feature modeling using linear regression\nfrom sklearn.linear_model import LinearRegression\n\nreg = LinearRegression()\nreg.fit(X_train2, y_train)\n\n# prediction using the test set\nX_test2 = pd.DataFrame( {'XGB': reg_xgb.predict(X_test), 'DL': reg_dl.predict(X_test).ravel(), 'SVR': reg_svr.predict(X_test)})\n\n# Don't forget to convert the prediction back to non-log scale\ny_pred = np.exp(reg.predict(X_test2))","941220dc":"# submission\nsubmission = pd.DataFrame({\n    \"Id\": test_ID,\n    \"SalePrice\": y_pred\n})\nsubmission.to_csv('houseprice.csv', index=False)","0f4f3be8":"There are no nan any more! Let's check our columns now","fa53c7bf":"Which features are important? Let a random forest regressor tell us about it.","7296cfdf":"**XGBOOST**","ea5de3b7":"Let's make a new matrix where each column represents prediction by each model","15b80791":"We have used a grid-search to optimize hyperparameters, but it is still not good enough because explored parameter spaces are still narrow. \n\nWe could still optimize hyperparameters and go further up!!","075dbfae":"The white elements have nan there. There seems to be some columns containing nan.","c95a9bc4":"We can see clear linear relationships in many panels. We do not see a lot of outliers, which is good.\n\nAlso, we will remove some outlier data points found in the 'totalSF' and 'GrLivArea', which seem out of the linear regression line","87324084":"We may be able to get better prediction performance if we average predictions by multiple models, rather than relying on a single model. There are always pros and cons in machine learning algorithms, so averaging multiple model predictions may compensate one another.\n\nHere we use three models: XGBoost, Neural Network, and SVR","f0918f45":"**Neural Network**: we simply use Keras for easy implementation of multi-layer perceptron.\n\nWe can again take advantage of grid search, but that requires Keras' wrapper 'KerasRegressor' (or 'KerasClassifier') to utilize scikit_learn's gird search.","eee3bce1":"As it is right-skewed, we use log-transform to make them normally distributed.","b6e10ee5":"**SVR (support vector regressior)** :  We can do that easily using sklearn. Again, we use grid search to optimize some of the SVR's hyperparameters.","dac869e9":"There are many '...SF'. Maybe we make a new feature which takes the sum of the all.","f41b2211":"**MISSING VALUE TREATMENT**","56ea1983":"Now let's have a look at the target distribution. As this is a regression task, we want the target to be normally distributed.","8b0109be":"Let's have a look at correlation matrix now","51946399":"Surprisingly, only two features are dominant: 'OverallQual' and 'TotalSF'. So instead of using all the 77 features, maybe just using the top few features is good enough (dimensionality reduction).\n\nHere, we normalize the data via z-scoring."}}