{"cell_type":{"5892b0ad":"code","8bbf8899":"code","ca4ed09c":"code","f0d07975":"code","153ed542":"code","79ea15e5":"code","f31a5cb4":"code","80fc297d":"code","7bbe5b6c":"code","ac65c080":"code","78102646":"code","2e427c33":"code","5bd09e6b":"code","ab8cc93e":"code","63a57cf8":"code","38066f59":"code","5c3913fb":"markdown","3dbd5d07":"markdown","83fe4913":"markdown","4c9c821e":"markdown","1430ac15":"markdown","1e16592e":"markdown","753fb1b6":"markdown","3de5ec91":"markdown","76f9643d":"markdown","27816554":"markdown","88771538":"markdown"},"source":{"5892b0ad":"# Importing all the required libraries\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport cv2\nimport os\nfrom keras.utils import Sequence, to_categorical, plot_model\nfrom keras.layers import Conv2D, Dropout, MaxPooling2D, UpSampling2D, concatenate, Input\nfrom keras.models import Model\nfrom sklearn.model_selection import train_test_split\nfrom keras.callbacks import ModelCheckpoint, EarlyStopping","8bbf8899":"path = '\/kaggle\/input\/semantic-drone-dataset\/dataset\/semantic_drone_dataset\/'\nimg = cv2.imread(path + 'original_images\/001.jpg')\nimg = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\nmask = cv2.imread(path + 'label_images_semantic\/001.png', cv2.IMREAD_GRAYSCALE)\n#mask = mask.cvtColor(img, cv2.COLOR_BGR2RGB)\nfig, axs = plt.subplots(1, 2, figsize=(20, 10))\naxs[0].imshow(img)\naxs[1].imshow(mask)","ca4ed09c":"print('Image Dimensions are: ', img.shape)\nprint('Label Dimensions are: ', mask.shape)","f0d07975":"# Prepare the Images\nX = []\nfor filename in sorted(os.listdir(path + 'original_images\/')):\n    a = cv2.imread(path + 'original_images\/' + filename)\n    a = cv2.cvtColor(a, cv2.COLOR_BGR2RGB)\n    a = cv2.resize(a, (256, 256))\n    a = a \/ 255\n    X.append(a)\n    \nX = np.array(X)\n\n# Prepare the Labels\nY = []\nfor filename in sorted(os.listdir(path + 'label_images_semantic\/')):\n    a = cv2.imread(path + 'label_images_semantic\/' + filename, cv2.IMREAD_GRAYSCALE)\n    a = cv2.resize(a, (256, 256))\n    Y.append(a)\n    \nY = np.array(Y)\nYc = to_categorical(Y)","153ed542":"print(X.shape)\nprint(Yc.shape)\nfig, axs = plt.subplots(1, 2, figsize=(20, 10))\naxs[0].imshow(X[1])\naxs[1].imshow(Y[1])","79ea15e5":"test_image1 = X[-1]\ntest_label1 = Yc[-1]\ntest_image2 = X[-2]\ntest_label2 = Yc[-2]\nx_train, x_val, y_train, y_val = train_test_split(X[0:-2], Yc[0:-2], test_size = 0.1)","f31a5cb4":"print(x_train.shape)\nprint(y_train.shape)\nprint(x_val.shape)\nprint(y_val.shape)","80fc297d":"fig, axs = plt.subplots(1, 2, figsize=(20, 10))\naxs[0].imshow(x_train[50])\naxs[1].imshow(np.argmax(y_train[50], axis=2))","7bbe5b6c":"def unet(num_classes = 23, image_shape = (256, 256, 3)):\n    # Input\n    inputs = Input(image_shape)\n    # Encoder Path\n    conv1 = Conv2D(64, 3, activation='relu', kernel_initializer = 'he_normal', padding='same')(inputs)\n    conv1 = Conv2D(64, 3, activation='relu', kernel_initializer = 'he_normal', padding='same')(conv1)\n    pool1 = MaxPooling2D((2,2))(conv1)\n    \n    conv2 = Conv2D(128, 3, activation='relu', kernel_initializer = 'he_normal', padding='same')(pool1)\n    conv2 = Conv2D(128, 3, activation='relu', kernel_initializer = 'he_normal', padding='same')(conv2)\n    pool2 = MaxPooling2D((2,2))(conv2)\n\n    conv3 = Conv2D(256, 3, activation='relu', kernel_initializer = 'he_normal', padding='same')(pool2)\n    conv3 = Conv2D(256, 3, activation='relu', kernel_initializer = 'he_normal', padding='same')(conv3)\n    pool3 = MaxPooling2D((2,2))(conv3)\n    \n    conv4 = Conv2D(512, 3, activation='relu', kernel_initializer = 'he_normal', padding='same')(pool3)\n    conv4 = Conv2D(512, 3, activation='relu', kernel_initializer = 'he_normal', padding='same')(conv4)\n    drop4 = Dropout(0.5)(conv4)\n    pool4 = MaxPooling2D((2,2))(drop4)\n    \n    conv5 = Conv2D(1024, 3, activation='relu', kernel_initializer = 'he_normal', padding='same')(pool4)\n    conv5 = Conv2D(1024, 3, activation='relu', kernel_initializer = 'he_normal', padding='same')(conv5)\n    drop5 = Dropout(0.5)(conv5)\n    \n    # Decoder Path\n    up6 = Conv2D(512, 2, activation='relu', kernel_initializer='he_normal', padding='same')(UpSampling2D(size=(2,2))(drop5))\n    merge6 = concatenate([up6, conv4], axis = 3)\n    conv6 = Conv2D(512, 3, activation='relu', kernel_initializer='he_normal', padding='same')(merge6)\n    conv6 = Conv2D(512, 3, activation='relu', kernel_initializer='he_normal', padding='same')(conv6)\n    \n    up7 = Conv2D(256, 2, activation='relu', kernel_initializer='he_normal', padding='same')(UpSampling2D(size=(2,2))(conv6))\n    merge7 = concatenate([up7, conv3], axis = 3)\n    conv7 = Conv2D(256, 3, activation='relu', kernel_initializer='he_normal', padding='same')(merge7)\n    conv7 = Conv2D(256, 3, activation='relu', kernel_initializer='he_normal', padding='same')(conv7)\n    \n    up8 = Conv2D(128, 2, activation='relu', kernel_initializer='he_normal', padding='same')(UpSampling2D(size=(2,2))(conv7))\n    merge8 = concatenate([up8, conv2], axis = 3)\n    conv8 = Conv2D(128, 3, activation='relu', kernel_initializer='he_normal', padding='same')(merge8)\n    conv8 = Conv2D(128, 3, activation='relu', kernel_initializer='he_normal', padding='same')(conv8)\n    \n    up9 = Conv2D(64, 2, activation='relu', kernel_initializer='he_normal', padding='same')(UpSampling2D(size=(2,2))(conv8))\n    merge9 = concatenate([up9, conv1], axis = 3)\n    conv9 = Conv2D(64, 3, activation='relu', kernel_initializer='he_normal', padding='same')(merge9)\n    conv9 = Conv2D(64, 3, activation='relu', kernel_initializer='he_normal', padding='same')(conv9)\n    \n    conv10 = Conv2D(num_classes, (1, 1), padding='same', activation='softmax')(conv9)\n    \n    model = Model(inputs, conv10)\n    \n    return model","ac65c080":"model = unet()\nmodel.summary()","78102646":"plot_model(model)","2e427c33":"model_checkpoint = ModelCheckpoint('unet_model.hdf5', monitor='val_loss', verbose=1, save_best_only=True)\nmodel_earlyStopping = EarlyStopping(min_delta= 0.001, patience=30)\n\nmodel.compile(optimizer='adam', loss=['categorical_crossentropy'], metrics=['accuracy'])\n\nhistory = model.fit(x=x_train, y=y_train,\n              validation_data=(x_val, y_val),\n              batch_size=16, epochs=200,\n              callbacks=[model_checkpoint, model_earlyStopping])","5bd09e6b":"acc = history.history['accuracy']\nval_acc = history.history['val_accuracy']\nloss = history.history['loss']\nval_loss = history.history['val_loss']\nepochs_range = range(len(acc))\n\nplt.figure(figsize=(15, 15))\nplt.subplot(2, 2, 1)\nplt.plot(epochs_range, acc, label='Training Accuracy')\nplt.plot(epochs_range, val_acc, label='Validation Accuracy')\nplt.legend(loc='lower right')\nplt.title('Training and Validation Accuracy')\n\nplt.subplot(2, 2, 2)\nplt.plot(epochs_range, loss, label='Training Loss')\nplt.plot(epochs_range, val_loss, label='Validation Loss')\nplt.legend(loc='upper right')\nplt.title('Training and Validation Loss')\nplt.show()","ab8cc93e":"m1 = test_image1\npred = model.predict(np.expand_dims(m1, 0))\npred_mask = np.argmax(pred, axis=-1)\nprint(pred_mask.shape)\npred_mask = pred_mask[0]\nprint(pred_mask.shape)\n\nm2 = test_image2\npred2 = model.predict(np.expand_dims(m2, 0))\npred_mask2 = np.argmax(pred2, axis=-1)\nprint(pred_mask2.shape)\npred_mask2 = pred_mask2[0]\nprint(pred_mask2.shape)","63a57cf8":"fig, axs = plt.subplots(1, 3, figsize=(20, 10))\naxs[0].imshow(m1)\naxs[0].set_title('Image')\naxs[1].imshow(np.argmax(test_label1, axis=-1))\naxs[1].set_title('Ground Truth')\naxs[2].imshow(pred_mask)\naxs[2].set_title('Prediction')","38066f59":"fig, axs = plt.subplots(1, 3, figsize=(20, 10))\naxs[0].imshow(m2)\naxs[0].set_title('Image')\naxs[1].imshow(np.argmax(test_label2, axis=-1))\naxs[1].set_title('Ground Truth')\naxs[2].imshow(pred_mask2)\naxs[2].set_title('Prediction')","5c3913fb":"**DATA Preprocessing**","3dbd5d07":"Now, Let's visualize the images in the dataset","83fe4913":"So, Let's first import the dependencies.","4c9c821e":"**NOW, We are ready to create the Model, U-NET**","1430ac15":"As the first step in any computer vision task, we are going to prepare the dataset. \n\nIn this notebook, we will use [this dataset](https:\/\/www.kaggle.com\/bulentsiyah\/semantic-drone-dataset)","1e16592e":"Let's Prepare our dataset for the training","753fb1b6":"Now, Let's evaluate our Model. I will use the test Image that the model had never seen before.","3de5ec91":"Although accuracy is not the better metric while doing semantic segmentation, we will just go through this notebook.","76f9643d":"* **In this Notebook, we are trying to approach the task of semantic segmentation, classifying each pixel in an image from a set of classes that are previously defined.**\n\n* **Our main goal in this task is to take an image of size (Width x Height x 3) and generate a (Width x Height) matrix containing the predicted class corresponding to each pixel in the image.**","27816554":"Reaching that point successfully, we are now ready to complie and train our model","88771538":"Now, Let's Split our data into training and validation.\n\nI will keep the last image as a test image to test the Model"}}