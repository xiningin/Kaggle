{"cell_type":{"daf3ab41":"code","7a04cd6b":"code","0ce5d6d1":"code","5fbb34f5":"code","c8a7b85a":"code","2ee392b7":"code","3061c8ab":"code","cd455ae2":"code","69d5991c":"code","26526944":"code","674bb40a":"code","35284bf3":"code","55f49cb7":"code","d34f079d":"code","fe767b25":"code","ddfa729d":"code","df343121":"code","58179018":"code","22460348":"code","8af65b12":"code","a0a06603":"code","8f4455be":"code","09801a11":"code","8f4019fe":"code","454ab4c8":"code","b02ffd62":"code","887eb7b0":"code","8cf0cbb2":"code","dd3433be":"code","c695316c":"markdown"},"source":{"daf3ab41":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\n# for dirname, _, filenames in os.walk('\/kaggle\/input'):\n#     for filename in filenames:\n#         print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","7a04cd6b":"ls \/kaggle\/input\/urbansound8k # the files are already in 10 folders for 10 fold validation etc","0ce5d6d1":"# source\n\n# https:\/\/everythingtutorial.com\/pytorch-audio-tutorial\n# https:\/\/colab.research.google.com\/github\/pytorch\/tutorials\/blob\/gh-pages\/_downloads\/audio_classifier_tutorial.ipynb\n# https:\/\/colab.research.google.com\/drive\/1g_33vGuSy_3SGV7Tc8REOLe8oBz1UA8w#scrollTo=sYyRFlZuOVvI","5fbb34f5":"%matplotlib inline","c8a7b85a":"! pip install torchaudio","2ee392b7":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom torchvision import datasets, transforms\nfrom torch.utils.data import Dataset\nimport torchaudio\nimport pandas as pd\nimport numpy as np","3061c8ab":"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(device)","cd455ae2":"csvData = pd.read_csv('\/kaggle\/input\/urbansound8k\/UrbanSound8K.csv')\nprint(csvData.iloc[0, :])","69d5991c":"csvData.head()","26526944":"import IPython.display as ipd\nipd.Audio('\/kaggle\/input\/urbansound8k\/fold1\/108041-9-0-5.wav')","674bb40a":"ipd.Audio('\/kaggle\/input\/urbansound8k\/fold5\/100852-0-0-19.wav')","35284bf3":"path = '\/kaggle\/input\/urbansound8k\/fold5\/100852-0-0-19.wav'","55f49cb7":"sound = torchaudio.load(path, out = None, normalization = True)\n#load returns a tensor with the sound data and the sampling frequency (44.1kHz for UrbanSound8K)\n# soundData = self.mixer(sound[0]) #Down mix mono doesnt exist anymore\n# https:\/\/github.com\/pytorch\/audio\/issues\/363","d34f079d":" sound, sound [0] , sound [1] # an array and freq","fe767b25":"soundData = torch.mean(sound[0], dim=0).unsqueeze(1)\n# was as below\n# soundData = torch.mean(sound[0], dim=0).unsqueeze(0)# add a dim at idx 0 <-unsqueeze? \n# dim 0 is where the second channel comes in\nsoundData, soundData.shape","ddfa729d":"soundData.numel()","df343121":"#downsample the audio to ~8kHz\ntempData = torch.zeros([160000, 1]) #tempData accounts for audio clips that are too short\ntempData.shape","58179018":"\nif soundData.numel() < 160000:\n    tempData[:soundData.numel()] = soundData[:]\nelse:\n    tempData[:] = soundData[:160000]\n\n","22460348":"soundData = tempData\nsoundFormatted = torch.zeros([32000, 1])\nsoundFormatted[:32000] = soundData[::5] #take every fifth sample of soundData\nsoundFormatted.shape","8af65b12":"soundFormatted = soundFormatted.permute(1, 0)","a0a06603":"soundFormatted.shape","8f4455be":"torch.max(soundFormatted),torch.min(soundFormatted)","09801a11":"class UrbanSoundDataset(Dataset):\n#rapper for the UrbanSound8K dataset\n    # Argument List\n    #  path to the UrbanSound8K csv file\n    #  path to the UrbanSound8K audio files\n    #  list of folders to use in the dataset\n    \n    def __init__(self, csv_path, file_path, folderList):\n        csvData = pd.read_csv(csv_path)\n        #initialize lists to hold file names, labels, and folder numbers\n        self.file_names = []\n        self.labels = []\n        self.folders = []\n        #loop through the csv entries and only add entries from folders in the folder list\n        for i in range(0,len(csvData)):\n            if csvData.iloc[i, 5] in folderList:\n                self.file_names.append(csvData.iloc[i, 0]) # file name see df.head() above\n                self.labels.append(csvData.iloc[i, 6]) # class id column\n                self.folders.append(csvData.iloc[i, 5]) # folder\/fold num\n                \n        self.file_path = file_path\n#         self.mixer = torchaudio.transforms.DownmixMono() #UrbanSound8K uses two channels, this will convert them to one\n#         self.mixer = torch.mean(waveform, dim=0).unsqueeze(0)\n        self.folderList = folderList\n        \n    def __getitem__(self, index):\n        #format the file path and load the file\n        path = self.file_path + \"fold\" + str(self.folders[index]) + \"\/\" + self.file_names[index]\n        sound = torchaudio.load(path, out = None, normalization = True)\n        #load returns a tensor with the sound data and the sampling frequency (44.1kHz for UrbanSound8K)\n        # soundData = self.mixer(sound[0]) #Down mix mono doesnt exist anymore\n        # https:\/\/github.com\/pytorch\/audio\/issues\/363\n        soundData = torch.mean(sound[0], dim=0).unsqueeze(1) # was .unsqueeze(0) \n        #downsample the audio to ~8kHz\n        tempData = torch.zeros([160000, 1]) #tempData accounts for audio clips that are too short\n        if soundData.numel() < 160000:\n            tempData[:soundData.numel()] = soundData[:]\n        else:\n            tempData[:] = soundData[:160000]\n        \n        soundData = tempData\n        soundFormatted = torch.zeros([32000, 1])\n        soundFormatted[:32000] = soundData[::5] #take every fifth sample of soundData\n        soundFormatted = soundFormatted.permute(1, 0)\n        return soundFormatted, self.labels[index]\n    \n    def __len__(self):\n        return len(self.file_names)\n","8f4019fe":"    \ncsv_path = '\/kaggle\/input\/urbansound8k\/UrbanSound8K.csv'\nfile_path = '\/kaggle\/input\/urbansound8k\/'\n\ntrain_set = UrbanSoundDataset(csv_path, file_path, range(1,10))\ntest_set = UrbanSoundDataset(csv_path, file_path, [10])\nprint(\"Train set size: \" + str(len(train_set)))\nprint(\"Test set size: \" + str(len(test_set)))\n\nkwargs = {'num_workers': 1, 'pin_memory': True} if device == 'cuda' else {} #needed for using datasets on gpu\n\ntrain_loader = torch.utils.data.DataLoader(train_set, batch_size = 128, shuffle = True, **kwargs)\ntest_loader = torch.utils.data.DataLoader(test_set, batch_size = 128, shuffle = True, **kwargs)","454ab4c8":"class Net(nn.Module):\n    def __init__(self):\n        super(Net, self).__init__()\n        self.conv1 = nn.Conv1d(1, 128, 80, 4)\n        self.bn1 = nn.BatchNorm1d(128)\n        self.pool1 = nn.MaxPool1d(4)\n        self.conv2 = nn.Conv1d(128, 128, 3)\n        self.bn2 = nn.BatchNorm1d(128)\n        self.pool2 = nn.MaxPool1d(4)\n        self.conv3 = nn.Conv1d(128, 256, 3)\n        self.bn3 = nn.BatchNorm1d(256)\n        self.pool3 = nn.MaxPool1d(4)\n        self.conv4 = nn.Conv1d(256, 512, 3)\n        self.bn4 = nn.BatchNorm1d(512)\n        self.pool4 = nn.MaxPool1d(4)\n        self.avgPool = nn.AvgPool1d(30) #input should be 512x30 so this outputs a 512x1\n        self.fc1 = nn.Linear(512, 10)\n        \n    def forward(self, x):\n        x = self.conv1(x)\n        x = F.relu(self.bn1(x))\n        x = self.pool1(x)\n        x = self.conv2(x)\n        x = F.relu(self.bn2(x))\n        x = self.pool2(x)\n        x = self.conv3(x)\n        x = F.relu(self.bn3(x))\n        x = self.pool3(x)\n        x = self.conv4(x)\n        x = F.relu(self.bn4(x))\n        x = self.pool4(x)\n        x = self.avgPool(x)\n        x = x.permute(0, 2, 1) #change the 512x1 to 1x512\n        x = self.fc1(x)\n        return F.log_softmax(x, dim = 2)\n\nmodel = Net()\nmodel.to(device)\nprint(model)","b02ffd62":"optimizer = optim.Adam(model.parameters(), lr = 0.01, weight_decay = 0.0001)\nscheduler = optim.lr_scheduler.StepLR(optimizer, step_size = 20, gamma = 0.1)","887eb7b0":"def train(model, epoch):\n    model.train()\n    for batch_idx, (data, target) in enumerate(train_loader):\n        optimizer.zero_grad()\n        data = data.to(device)\n        target = target.to(device)\n        data = data.requires_grad_() #set requires_grad to True for training\n        output = model(data)\n        output = output.permute(1, 0, 2) #original output dimensions are batchSizex1x10 \n        loss = F.nll_loss(output[0], target) #the loss functions expects a batchSizex10 input\n        loss.backward()\n        optimizer.step()\n        if batch_idx % log_interval == 0: #print training stats\n            print('Train Epoch: {} [{}\/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n                epoch, batch_idx * len(data), len(train_loader.dataset),\n                100. * batch_idx \/ len(train_loader), loss))","8cf0cbb2":"def test(model, epoch):\n    model.eval()\n    correct = 0\n    for data, target in test_loader:\n        data = data.to(device)\n        target = target.to(device)\n        output = model(data)\n        output = output.permute(1, 0, 2)\n        pred = output.max(2)[1] # get the index of the max log-probability\n        correct += pred.eq(target).cpu().sum().item()\n    print('\\nTest set: Accuracy: {}\/{} ({:.0f}%)\\n'.format(\n        correct, len(test_loader.dataset),\n        100. * correct \/ len(test_loader.dataset)))","dd3433be":"log_interval = 20\nfor epoch in range(1, 2): # use 41\n    if epoch == 31:\n        print(\"First round of training complete. Setting learn rate to 0.001.\")\n    scheduler.step()\n    train(model, epoch)\n    test(model, epoch)","c695316c":" air_conditioner, car_horn, children_playing, dog_bark, drilling, enginge_idling, gun_shot, jackhammer, siren, and street_music   \n are the classes"}}