{"cell_type":{"98b52b3c":"code","c85861dd":"code","174f9c8f":"code","12d33c40":"code","4dc7e89a":"code","6da7627b":"code","9b0fc912":"code","238d7046":"code","3551bbe5":"code","21241760":"code","7df05fb7":"code","9287923d":"code","02d79322":"code","a7c5c751":"code","2ca60b45":"code","2a622595":"code","69048d30":"code","20e3b64a":"code","13d0c349":"code","7b4c95c6":"code","fb2e9577":"code","6c7acd50":"code","6094c271":"code","6c53bd2c":"code","7468438c":"code","7ce4d2f6":"code","41611d73":"code","9d67e951":"code","12e77c03":"code","b6c2be1c":"code","ab1089c5":"code","7e266462":"code","21c243a1":"code","3bd2931e":"code","af8c9a0c":"code","1397c685":"code","224f173c":"code","deb40315":"code","432a8d6e":"code","b33380dc":"code","45590d0d":"code","9ab46f9b":"code","fcb615e2":"code","1c3a3a44":"code","40703deb":"code","5aaa4577":"code","8489580f":"code","5fc43ef2":"code","b5f062d7":"code","e145ae9c":"code","228516b9":"code","9fb0a28d":"code","34132a03":"code","f78be8ce":"code","72863896":"markdown","04610232":"markdown","7435603b":"markdown","0d98578f":"markdown","0d839b7b":"markdown","7793cbe5":"markdown","0f75ccac":"markdown","43e8f910":"markdown","7e7ab931":"markdown","4f49d973":"markdown","4a275522":"markdown","34b43f62":"markdown","20f3e04d":"markdown","9e905583":"markdown","3e8e5cd3":"markdown","c17eb997":"markdown","23e9de09":"markdown","467bef72":"markdown","0cc6b9db":"markdown","399bdc83":"markdown","ac9e265e":"markdown","75594ac9":"markdown","6fa9d489":"markdown","74b67e7a":"markdown","7498ef43":"markdown","954b6fd1":"markdown","39379e81":"markdown"},"source":{"98b52b3c":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","c85861dd":"data=pd.read_csv('\/kaggle\/input\/real-estate-dataset\/data.csv')","174f9c8f":"data.head()","12d33c40":"data.info()","4dc7e89a":"tab_desc=data.describe()\ntab_desc","6da7627b":"tab_desc = tab_desc.iloc[1:]\ntab_desc","9b0fc912":"tab_desc=np.array(tab_desc)\n","238d7046":"\nimport matplotlib.pyplot as plot\n\nimport numpy as np\n\nplot.boxplot((tab_desc[:,0], tab_desc[:,1], tab_desc[:,2],tab_desc[:,3],tab_desc[:,4],tab_desc[:,5],tab_desc[:,6]))\n\nplot.show()","3551bbe5":"%matplotlib inline\nimport matplotlib.pyplot as plt\ndata.hist(bins=50,figsize=(20,15))\nplt.show()","21241760":"from zlib import crc32\ndef test_set_check(identifier,test_ratio):\n    return crc32(np.int64(identifier))& 0xffffffff < test_ratio * 2 **32\ndef split_train_test_by_id(data, test_ratio, id_column):\n    ids=data[id_column]\n    in_test_set= ids.apply(lambda id_ : test_set_check(id_, test_ratio))\n    return data.loc[~in_test_set],data.loc[in_test_set]","7df05fb7":"data_with_id= data.reset_index() ## add an 'index' column\ntrain_set, test_set = split_train_test_by_id(data_with_id,0.2,'index')","9287923d":"data_with_id['id']= data['CRIM']*1000 + data['ZN']\ntrain_set,test_set = split_train_test_by_id(data_with_id,0.2,'id')","02d79322":"test_set.describe()","a7c5c751":"import pandas as pd\ndata['Tax_marge'] = pd.cut(data['TAX'],bins=[0.0,200.0 ,350.0 ,400.0  ,650.0 ,np.inf],labels=[1,2,3,4,5])","2ca60b45":"data['Tax_marge'].hist()","2a622595":"from sklearn.model_selection import StratifiedShuffleSplit\nsplit = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=42)\nfor train_index,test_index in split.split(data,data[\"Tax_marge\"]):\n    strat_train_set=data.loc[train_index]\n    strat_test_set=data.loc[test_index]","69048d30":"strat_test_set[\"Tax_marge\"].value_counts() \/ len(strat_test_set)","20e3b64a":"for set_ in (strat_train_set, strat_test_set):\n    set_.drop(\"Tax_marge\", axis=1, inplace=True)","13d0c349":"data = strat_train_set.copy()","7b4c95c6":"dataTest=strat_test_set.copy()","fb2e9577":"data.info()","6c7acd50":"dataTest.info()","6094c271":"data.plot(kind=\"scatter\",x=\"ZN\",y=\"LSTAT\")","6c53bd2c":"corr_matrix= data.corr()","7468438c":"corr_matrix","7ce4d2f6":"corr_matrix['MEDV'].sort_values(ascending=False)","41611d73":"from pandas.plotting import scatter_matrix\nattributs=['RM','PTRATIO','TAX','LSTAT']\nscatter_matrix(data[attributs],figsize=(12,8))\n","9d67e951":"data.plot(kind=\"scatter\",x=\"RM\",y=\"MEDV\")","12e77c03":"data.plot(kind=\"scatter\",x=\"RM\",y=\"MEDV\",alpha=0.1)","b6c2be1c":"data['TAX_RM']= data['TAX']\/data['RM']\n","ab1089c5":"corr_matrix=data.corr()\ncorr_matrix['MEDV'].sort_values(ascending=False)","7e266462":"data=strat_train_set.drop(\"MEDV\",axis=1)\ndata_labels= strat_train_set['MEDV'].copy()","21c243a1":"data.info()","3bd2931e":"#data.dropna(subset=[\"RM\"])","af8c9a0c":"#data.drop(\"RM\", axis=1)","1397c685":"median= data[\"RM\"].median()\n## we have to conserve the value of the median to fill the empty values \n### in the test set  \ndata[\"RM\"].fillna(median,inplace=True)","224f173c":"### Scikit-learn provides a handy class to handle missing values.\nfrom sklearn.impute import SimpleImputer\nimputer = SimpleImputer(strategy=\"median\")\nimputer.fit(data) \n## it's safer to apply the imputer to all the numerical attributes.","deb40315":"##stored the result in the instance called statistics_\nimputer.statistics_","432a8d6e":"data.median().values","b33380dc":"# replacing the missed values with the learned medians:\n\nx=imputer.transform(data)\n\n# result is a plain Numpy array","45590d0d":"data_transform=pd.DataFrame(x,columns=data.columns, index=data.index)\ndata_transform","9ab46f9b":"data_transform.describe()","fcb615e2":"## for numerical attributes\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import StandardScaler\n\nnum_pipeline=Pipeline([\n    ('imputer',SimpleImputer(strategy=\"median\")),\n   ########## u can other transformers.\n    ('std_scaler', StandardScaler())\n])","1c3a3a44":"data_prepared=num_pipeline.fit_transform(data)","40703deb":"from sklearn.svm import SVR\nsvr_rbf = SVR(kernel='rbf', C=100, gamma=0.1, epsilon=.1)\nsvr_rbf.fit(data_prepared, data_labels)","5aaa4577":"data=pd.DataFrame(data)\nsome_data = data.iloc[:5]\nsome_labels = data_labels.iloc[:5]\nsome_prepared_data = num_pipeline.transform(some_data)\nsvr_rbf.predict(some_prepared_data)","8489580f":"print(\"Labels: \",list(some_labels))","5fc43ef2":"## let's Measure this regression model's RMSE on the whole training set using Scikit_Learn's mean_squared_error() function\n\nfrom sklearn.metrics import mean_squared_error\n\ndata_predictions=svr_rbf.predict(data_prepared)\nsvr_mse=mean_squared_error(data_labels,data_predictions)\nsvr_rmse=np.sqrt(svr_mse)\nsvr_rmse","b5f062d7":"from sklearn.model_selection import cross_val_score\nscores= cross_val_score(svr_rbf,data_prepared,data_labels,scoring = 'neg_mean_squared_error',cv=10)\n##cv split data set into 10 distinct subsets called folders.\nsvr_rmse_scores=np.sqrt(-scores)","e145ae9c":"def display_scores(scores):\n    print(\"Scores:\", scores)\n    print(\"Mean:\", scores.mean())\n    print(\"Standard deviation:\",scores.std())\ndisplay_scores(svr_rmse_scores)","228516b9":"### we can easly save scikit-learn models by using Python's pickle module or serializing a large umpy array.\n\nimport joblib\njoblib.dump(svr_rbf,\"my_model.pkl\")\n## and later\n#my_model_loaded=joblib.load(\"my_model.pkl\")","9fb0a28d":"from sklearn.model_selection import RandomizedSearchCV\nfrom scipy.stats import uniform\ndistributions = dict(C=uniform(loc=0, scale=4))\nclf = RandomizedSearchCV(svr_rbf, distributions, random_state=0)\nsearch = clf.fit(data_prepared, data_labels)\nsearch.best_params_","34132a03":"final_model = search.best_estimator_\nx_test=strat_test_set.drop(\"MEDV\", axis= 1)\ny_test=strat_test_set[\"MEDV\"].copy()\nX_test_prepared=num_pipeline.transform(x_test)\nfinal_predictions=final_model.predict(X_test_prepared)\nfinal_mse=mean_squared_error(y_test,final_predictions)\nfinal_rmse=np.sqrt(final_mse) #=> evaluates to 47,730.2\n","f78be8ce":"final_rmse","72863896":"### 4. Prepare the data for Machine Learning algorithms","04610232":"Date cleaning\n<br>\nwe cannot work with missing features. to fix this we can :\n<br>\n<ol>\n    <li>Get rid of the corresponding districs.<\/li>\n<li>Get rid of the whole attribute.<\/li>\n<li>Set the values to some value (zero, the mean , the median , etc.)<\/li>\n<\/ol>","7435603b":"\n### Real Estate Median Price Prediction.\n\nGiven data about real estate in Boston, let's try to predict the median values of a given home.\n\n### Predict the crime per capita of this dataset .\n\n\n\n#### Frame Your Problem\n\nthe problem: is supervised,\nIs a regression task\natch learning or online learning techniques?\n\n#### Select a Performance Measure\n###### MSE (Mean-squared error)\nA typical performance measure for regression problems is the Root Mean Square Error (RMSE). \nAnother such measure is called, Mean Absolute Error (MAE). \nBoth the RMSE and the MAE are ways to measure the distance between two vectors: the vector of predictions and the vector of target values. \nRMSE is more sensitive to outliers than MAE. But when outliers are expotentially rare, the RMSE performs very well and is generally preferred.","0d98578f":"### 3. Discover and visualize the data to gain insights[](http:\/\/)","0d839b7b":"## One of the best plots that we can draw to visualize outliers, is a Box and Whisker plot.\n\nYou are required to:\n1. Draw a Box and Whisker plot for the numerical attributes of our dataset.\n2. Draw a Scatter plot for longitude and latitude columns of our dataset.","7793cbe5":"We are required to:\n1. Read the data from the above URL and store it in a variable.\n2. Print the top most five rows in your loaded dataset (HINT: using head() method).","0f75ccac":"<p style='color: green; font-weight:bold '>Visualizing Geographical Data<\/p>","43e8f910":"<p style='color: green; font-weight:bold '>Fine-Tune My Model<\/p>","7e7ab931":"<p style='color: green; font-weight:bold '>Feature Scaling<\/p>","4f49d973":"### 2. Get the Data","4a275522":"### 1. Look at the Big Picture","34b43f62":"<p style='color: green; font-weight:bold '>Discover and Visualize the Data to Gain Insights <\/p>","20f3e04d":"## Exploring the data","9e905583":"<p style='color: green; font-weight:bold '>Looking for correlation <\/p>","3e8e5cd3":"We have null values in the attribute RM \"average number of rooms per dwelling\"[](http:\/\/)","c17eb997":"## few ways to do fine-tuning\n\n<ul>\n    <li>Grid Search<\/li>\n    <li>Randomized Search<\/li>\n    <li>Ensemble Methodes<\/li>\n    \n  <ul>\n","23e9de09":"<p style='color: green; font-weight:bold '> Evaluate Your System On the Test Set <\/p>","467bef72":"<p style='color: green; font-weight:bold '>Experimenting with Attribute Combinations<\/p>","0cc6b9db":"Note: The describe() method has ignored the null values in total_bedrooms data column.","399bdc83":"## revert to clean training set","ac9e265e":"\nwe are required to:\n1. Using the describe() method, find a summary of the numerical attributes of our dataset.","75594ac9":"## create testset ","6fa9d489":"<p style='color: green; font-weight:bold '>Select and Train a Model<\/p>","74b67e7a":"The info() method is useful to get a quick description of the data, in particular the total number of rows, and each attribute\u2019s type and number of non-null values.\n\nWe are required to:\n1. Use the info() method to find out the type of data fields present in our dateset and identify fields with null values.","7498ef43":"### min_max scaling (normalisation) and standardization.\n### transformation Pipelines.","954b6fd1":"**Getting Our Hands Dirty with Data\n**\nIn this section, we will go through an example project related to Real Estates. The following are the main steps we will go through in this section:\n1. Look at the big picture.\n2. Get the data.\n3. Discover and visualize the data to gain insights.\n4. Prepare the data for Machine Learning algorithms.\n","39379e81":"<p style='color: green; font-weight:bold '>Better Evaluation Using Cross-validation<\/p>"}}