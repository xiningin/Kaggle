{"cell_type":{"e80fd1d0":"code","b12d26ff":"code","a6c17aef":"code","ff5b117f":"code","05496de3":"code","c4997fcf":"code","051d5774":"code","7e4993bd":"code","85b634cd":"code","9e5f80bf":"code","e71feb91":"code","aee1dbdb":"code","cae6f329":"code","a8b04d29":"code","1dccc88e":"code","c1e6787d":"code","3a18f2bf":"code","38afc06b":"code","d7c01dd4":"code","b4cb878b":"code","d550174f":"code","f339c121":"code","858c3949":"code","4fa6cc15":"code","d71900ec":"code","c6157f6d":"code","49fb091c":"code","8805460c":"code","678093a6":"code","1f712b91":"code","e7bda846":"markdown","6736bd7c":"markdown","dcd57851":"markdown","cfa57e8c":"markdown","b865031a":"markdown","c72d28c9":"markdown","e1831dfb":"markdown","5a20df56":"markdown"},"source":{"e80fd1d0":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","b12d26ff":"import pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n#Load Dataset\ndf = pd.read_csv('\/kaggle\/input\/red-wine-quality-cortez-et-al-2009\/winequality-red.csv')\ndf.head()","a6c17aef":"#Explore dataset\ndf.info()","ff5b117f":"corr = df.corr()\ncorr","05496de3":"df.quality.unique()","c4997fcf":"sns.countplot(x='quality', data = df)","051d5774":"columns = df.columns\ncolumns","7e4993bd":"plt.figure(figsize = (10, 8))\nsns.boxplot('quality', 'fixed acidity', data = df)\nsns.barplot('quality', 'fixed acidity', data = df)","85b634cd":"plt.figure(figsize = (10, 8))\nsns.boxplot('quality', 'volatile acidity', data = df)\nsns.barplot('quality', 'volatile acidity', data = df)","9e5f80bf":"plt.figure(figsize = (10, 8))\nsns.boxplot('quality', 'citric acid', data = df)\nsns.barplot('quality', 'citric acid', data = df)","e71feb91":"plt.figure(figsize = (10, 8))\nsns.boxplot('quality', 'residual sugar', data = df)\nsns.barplot('quality', 'residual sugar', data = df)","aee1dbdb":"plt.figure(figsize = (10, 8))\nsns.boxplot('quality', 'chlorides', data = df)\nsns.barplot('quality', 'chlorides', data = df)","cae6f329":"plt.figure(figsize = (10, 8))\nsns.boxplot('quality', 'free sulfur dioxide', data = df)\nsns.barplot('quality', 'free sulfur dioxide', data = df)","a8b04d29":"plt.figure(figsize = (10, 8))\nsns.boxplot('quality', 'total sulfur dioxide', data = df)\nsns.barplot('quality', 'total sulfur dioxide', data = df)\n","1dccc88e":"plt.figure(figsize = (10, 8))\nsns.boxplot('quality', 'density', data = df)","c1e6787d":"plt.figure(figsize = (10, 8))\nsns.barplot(x = 'quality', y = 'density', data = df)","3a18f2bf":"plt.figure(figsize = (10, 8))\nsns.boxplot('quality', 'pH', data = df)\nsns.barplot('quality', 'pH', data = df)","38afc06b":"plt.figure(figsize = (10, 8))\nsns.boxplot('quality', 'sulphates', data = df)\nsns.barplot('quality', 'sulphates', data = df)\n","d7c01dd4":"plt.figure(figsize = (10, 8))\nsns.boxplot('quality', 'alcohol', data = df)\nsns.barplot('quality', 'alcohol', data = df)","b4cb878b":"from sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import KFold\nfrom sklearn.preprocessing import StandardScaler, LabelEncoder\nfrom sklearn.model_selection import LeaveOneOut\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.linear_model import RidgeClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.ensemble import ExtraTreesClassifier\nfrom numpy import mean","d550174f":"bins = (2, 6.5, 8)\ngroup_names = ['bad', 'good']\ndf['quality'] = pd.cut(df['quality'], bins = bins, labels = group_names)\ndf.head()","f339c121":"df['quality'].value_counts()","858c3949":"X = df.drop('quality', axis = 1)\nlb_en  = LabelEncoder()\ny = lb_en.fit_transform(df['quality'])","4fa6cc15":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 50)\n\nsc = StandardScaler()\nX_train = sc.fit_transform(X_train)\nX_test = sc.fit_transform(X_test)","d71900ec":"# retreive the dataset\ndef get_dataset(n_samples=100):\n    X, y = X_train, y_train\n    return X, y\n\nmodel = RandomForestClassifier()\n \n# evaluate the model using a given test condition\ndef evaluate_model(cv, model):\n    # get the dataset\n    X, y = get_dataset()\n    # evaluate the model\n    scores = cross_val_score(model, X, y, scoring='accuracy', cv=cv, n_jobs=-1)\n    # return scores\n    return np.mean(scores), scores.min(), scores.max()\n \n# calculate the ideal test condition\nideal, _, _ = evaluate_model(LeaveOneOut(), model)\nprint('Ideal: %.3f' % ideal)\n# define folds to test\nfolds = range(2,31)\n# record mean and min\/max of each set of results\nmeans, mins, maxs = list(),list(),list()\n# evaluate each k value\nfor k in folds:\n    # define the test condition\n    cv = KFold(n_splits=k, shuffle=True, random_state=1)\n    # evaluate k value\n    k_mean, k_min, k_max = evaluate_model(cv, model)\n    # report performance\n    print('> folds=%d, accuracy=%.3f (%.3f,%.3f)' % (k, k_mean, k_min, k_max))\n    # store mean accuracy\n    means.append(k_mean)\n    # store min and max relative to the mean\n    mins.append(k_mean - k_min)\n    maxs.append(k_max - k_mean)\n# line plot of k mean values with min\/max error bars\nplt.errorbar(folds, means, yerr=[mins, maxs], fmt='o')\n# plot the ideal case in a separate color\nplt.plot(folds, [ideal for _ in range(len(folds))], color='r')\n# show the plot\nplt.show()","c6157f6d":"#Retreive Models for Kfold validation:\n\ndef get_models():\n    models = []\n    models.append(('LR',LogisticRegression()))\n    models.append(('RC',RidgeClassifier()))\n    models.append(('SVC',SVC()))\n    models.append(('NB', GaussianNB()))\n    models.append(('RC',RandomForestClassifier()))\n    models.append(('ET',ExtraTreesClassifier()))\n    return models","49fb091c":"# define test conditions\ncv = KFold(n_splits=11, shuffle=True, random_state=1)\n\n# get the list of models to consider\nmodels = get_models()\n\n# collect results\nnames = []\ncv_results = []","8805460c":"for name, model in models:\n    results = cross_val_score(model, X_train, y_train, cv=cv, scoring='accuracy')\n    cv_results.append(results)\n    names.append(name)\n    print('%s: %f (%f)' % (name, results.mean(), results.std()))\n# Compare Algorithms\nplt.boxplot(cv_results, labels=names)\nplt.title('Algorithm Comparison')\nplt.show()","678093a6":"# train the model and predict \net = ExtraTreesClassifier()\net.fit(X_train, y_train)\npredictions = et.predict(X_test)","1f712b91":"# Evaluate predictions\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import classification_report\n\nprint(accuracy_score(y_test, predictions))\nprint(confusion_matrix(y_test, predictions))\nprint(classification_report(y_test, predictions))","e7bda846":"# Data Visualization","6736bd7c":"# Explore Dataset","dcd57851":"**Kfold Cross validation** is a statistical method to estimate the learning capabilities of the different machine learning models.\n\n   Kfold algorithm splits the entire dataset into random number of train and test sets (number of splits are specified as \"K\" in Kwargs. We will in the below notebook identify the best \"K\" for the dataset and validate the number of models and select the best performing model for the given K value.\n\nCheck bellow illustration to understand how Kfold validation work:\n\n![image.png](attachment:image.png)\n","cfa57e8c":"# Load Dataset","b865031a":"# Model Selection","c72d28c9":"# Create Target Variable & Split Data set","e1831dfb":"This dataset is also available from the UCI machine learning repository, https:\/\/archive.ics.uci.edu\/ml\/datasets\/wine+quality , I just shared it to kaggle for convenience. (If I am mistaken and the public license type disallowed me from doing so, I will take this down if requested.)\nContent\n\nFor more information, read [Cortez et al., 2009].\n\nInput variables (based on physicochemical tests):\n\n1 - fixed acidity\n\n2 - volatile acidity\n\n3 - citric acid\n\n4 - residual sugar\n\n5 - chlorides\n\n6 - free sulfur dioxide\n\n7 - total sulfur dioxide\n\n8 - density\n\n9 - pH\n\n10 - sulphates\n\n11 - alcohol\n\nOutput variable (based on sensory data):\n\n12 - quality (score between 0 and 10)","5a20df56":"# Train and Predict"}}