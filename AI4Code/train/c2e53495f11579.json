{"cell_type":{"3727489d":"code","692d9e74":"code","54a44efa":"code","28431c64":"code","5168c504":"code","67db6754":"code","3b3fef35":"code","59c5e238":"code","80a518fb":"code","81c06c22":"code","2ad94a6e":"code","6f70831d":"code","e8dd0f0c":"code","da27639f":"code","ee67fe48":"code","64c8f563":"code","67c74f37":"code","5e6f985f":"code","2702f5ac":"markdown","8a7fbd0f":"markdown","f116cda0":"markdown","7c4c075f":"markdown","059422c1":"markdown","1630a7d9":"markdown","580cf6fe":"markdown","539ec54c":"markdown","ea4ea688":"markdown","436e5b79":"markdown"},"source":{"3727489d":"# Carregamento do dataset 20 Newsgroups\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nfrom sklearn.datasets import fetch_20newsgroups\n\nnewsgroups_train = fetch_20newsgroups(subset='train', remove=('headers', 'footers', 'quotes'))\nnewsgroups_test = fetch_20newsgroups(subset='test', remove=('headers', 'footers', 'quotes'))\n\nX_train = np.array(newsgroups_train.data)\ny_train = np.array(newsgroups_train.target)\nX_test = np.array(newsgroups_test.data)\ny_test = np.array(newsgroups_test.target)","692d9e74":"# EDA - An\u00e1lise Explorat\u00f3ria de Dados\n\n#Verfica\u00e7\u00e3o de um dos exemplos\n\nprint(X_train[0])","54a44efa":"newsgroups_train.target_names[y_train[0]]","28431c64":"# Contagem de documentos de treino e teste por label\n\ndef conta_labels(y_train, y_test):\n    \"\"\"Returna dataframe com os total de documentos em cada classe\n    de treinamento e teste. Ref.: Cachopo (2007)\"\"\"\n    y_train_classes = pd.DataFrame([newsgroups_train.target_names[i] for i in newsgroups_train.target])[0]\n    y_test_classes = pd.DataFrame([newsgroups_test.target_names[i] for i in newsgroups_test.target])[0]\n    \n    contagem_df = pd.concat([y_train_classes.value_counts(),\n                             y_test_classes.value_counts()],\n                            axis=1, \n                            keys=[\"# docs treino\", \"# docs teste\"], \n                            sort=False)\n    \n    contagem_df[\"# total docs\"] = contagem_df.sum(axis=1)\n    contagem_df.loc[\"Total\"] = contagem_df.sum(axis=0)\n    \n    return contagem_df\n\nnewsgroups_df_labels = conta_labels(y_train, y_test)\nnewsgroups_df_labels","5168c504":"# Exibi\u00e7\u00e3o no formato de gr\u00e1fico\n%matplotlib inline\n\nnewsgroups_df_labels.iloc[:-1,:-1].plot.barh(stacked=True, \n                                    figsize=(10, 8),\n                                    title=\"N\u00famero de documentos de treinamento e teste por classe\");","67db6754":"# Carregamento das bibliotecas comuns do sklearn\n\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.feature_extraction.text import TfidfTransformer\nfrom sklearn import metrics","3b3fef35":"# Classe de Pre-Processamento de textos utilizando a biblioteca NLTK\n\nimport string\nimport re\nimport nltk\n\nclass NLTKTokenizer():\n    \"\"\"Classe que recebe documentos como entrada e devolve realizado lematiza\u00e7\u00e3o\n    e retirando stopwords e pontuacoes.\n    Ref.: https:\/\/scikit-learn.org\/stable\/modules\/feature_extraction.html\n    \"\"\"    \n    def __init__(self):\n        self.lemmatizer = nltk.stem.WordNetLemmatizer()\n        self.stopwords = nltk.corpus.stopwords.words('english')\n        self.english_words = set(nltk.corpus.words.words())\n        self.pontuacao = string.punctuation\n\n    def __call__(self, doc):\n        # ETAPA 1 - Limpeza de texto\n        # Converte para min\u00fasculo\n        doc = doc.lower()       \n        \n        # Trocar numeros pela string numero\n        doc = re.sub(r'[0-9]+', 'numero', doc)\n        \n        # Trocar underlines por underline\n        doc = re.sub(r'[_]+', 'underline', doc)\n\n        # Trocar URL pela string httpaddr\n        doc = re.sub(r'(http|https):\/\/[^\\s]*', 'httpaddr', doc)\n        \n        # Trocar Emails pela string emailaddr\n        doc = re.sub(r'[^\\s]+@[^\\s]+', 'emailaddr', doc) \n        \n        # Remover caracteres especiais\n        doc = re.sub(r'\\\\r\\\\n', ' ', doc)\n        doc = re.sub(r'\\W', ' ', doc)\n\n        # Remove caracteres simples de uma letra\n        doc = re.sub(r'\\s+[a-zA-Z]\\s+', ' ', doc)\n        doc = re.sub(r'\\^[a-zA-Z]\\s+', ' ', doc) \n\n        # Substitui multiplos espa\u00e7os por um unico espa\u00e7o\n        doc = re.sub(r'\\s+', ' ', doc, flags=re.I)\n        \n        # ETAPA 2 - Tratamento da cada palavra\n        palavras = []\n        for word in nltk.word_tokenize(doc):\n            if word in self.stopwords:\n                continue\n            if word in self.pontuacao:\n                continue\n            if word not in self.english_words:\n                continue\n            \n            word = self.lemmatizer.lemmatize(word)\n            palavras.append(word)\n        \n        return palavras","59c5e238":"# Vetores de caracter\u00edsticas sem tratamento com NLTK e Express\u00f5es Regulares\n\nvetorizador = CountVectorizer()\nv1 = vetorizador.fit_transform(X_train)\n\nfeatures = vetorizador.get_feature_names()\nv1_df = pd.DataFrame(v1.toarray(), columns = features)\nv1_df","80a518fb":"# Vetores de caracter\u00edsticas com NLTK (lematiza\u00e7\u00e3o, remo\u00e7\u00e3o de stopwords e palavras desconhecidas)\n\nvetorizador_tratado = CountVectorizer(tokenizer=NLTKTokenizer())\nv2 = vetorizador_tratado.fit_transform(X_train)\n\nfeatures = vetorizador_tratado.get_feature_names()\nv2_df = pd.DataFrame(v2.toarray(), columns = features)\nv2_df","81c06c22":"# Regress\u00e3o Log\u00edstica\n\nfrom sklearn.linear_model import LogisticRegression\n\ntext_clf_logistic_regression = Pipeline([('vect', CountVectorizer(tokenizer=NLTKTokenizer())),\n                     ('tfidf', TfidfTransformer()),\n                     ('clf', LogisticRegression(penalty='l2', \n                                                dual=False, \n                                                tol=0.001, \n                                                C=1.0, \n                                                fit_intercept=True, \n                                                intercept_scaling=1, \n                                                class_weight=None, \n                                                random_state=None, \n                                                solver='lbfgs', \n                                                max_iter=1000, \n                                                multi_class='multinomial', \n                                                verbose=0, \n                                                warm_start=False, \n                                                n_jobs=None, \n                                                l1_ratio=None)),\n                     ])\n\n#text_clf_logistic_regression.fit(X_train, y_train)\n#predicted = text_clf_logistic_regression.predict(X_test)\n#print(metrics.classification_report(y_test, predicted))","2ad94a6e":"# Naive Bayes\n\nfrom sklearn.naive_bayes import MultinomialNB\n\ntext_clf_naive_bayes = Pipeline([('vect', CountVectorizer(tokenizer=NLTKTokenizer())),\n                     ('tfidf', TfidfTransformer()),\n                     ('clf', MultinomialNB(alpha=1.0, \n                                           fit_prior=True, \n                                           class_prior=None)),\n                     ])\n\n#text_clf_naive_bayes.fit(X_train, y_train)\n#predicted = text_clf_naive_bayes.predict(X_test)\n#print(metrics.classification_report(y_test, predicted))","6f70831d":"# KNN\n\nfrom sklearn.neighbors import KNeighborsClassifier\n\ntext_clf_knn = Pipeline([('vect', CountVectorizer(tokenizer=NLTKTokenizer())),\n                     ('tfidf', TfidfTransformer()),\n                     ('clf', KNeighborsClassifier(n_neighbors=5, \n                                                  weights='uniform', \n                                                  algorithm='auto', \n                                                  leaf_size=30, \n                                                  p=2, \n                                                  metric='minkowski', \n                                                  metric_params=None, \n                                                  n_jobs=None)),\n                     ])\n\n#text_clf_knn.fit(X_train, y_train)\n#predicted = text_clf_knn.predict(X_test)\n#print(metrics.classification_report(y_test, predicted))","e8dd0f0c":"# \u00c1rvore de Decis\u00e3o\n\nfrom sklearn.tree import DecisionTreeClassifier\n\ntext_clf_decision_tree = Pipeline([('vect', CountVectorizer(tokenizer=NLTKTokenizer())),\n                         ('tfidf', TfidfTransformer()),\n                         ('clf', DecisionTreeClassifier(criterion='gini', \n                                                        splitter='best', \n                                                        max_depth=None, \n                                                        min_samples_split=2, \n                                                        min_samples_leaf=1, \n                                                        min_weight_fraction_leaf=0.0, \n                                                        max_features=None, \n                                                        random_state=None, \n                                                        max_leaf_nodes=None, \n                                                        min_impurity_decrease=0.0, \n                                                        min_impurity_split=None, \n                                                        class_weight=None, \n                                                        presort=False)),\n                         ])\n\n#text_clf_decision_tree.fit(X_train, y_train)\n#predicted = text_clf_decision_tree.predict(X_test)\n#print(metrics.classification_report(y_test, predicted))","da27639f":"# Random Forest\n\nfrom sklearn.ensemble import RandomForestClassifier\n\ntext_clf_rf = Pipeline([('vect', CountVectorizer(tokenizer=NLTKTokenizer())),\n                     ('tfidf', TfidfTransformer()),\n                     ('clf', RandomForestClassifier(n_estimators=100, \n                                                    criterion='gini', \n                                                    max_depth=None, \n                                                    min_samples_split=2, \n                                                    min_samples_leaf=1, \n                                                    min_weight_fraction_leaf=0.0, \n                                                    max_features='auto', \n                                                    max_leaf_nodes=None, \n                                                    min_impurity_decrease=0.0, \n                                                    min_impurity_split=None, \n                                                    bootstrap=True, \n                                                    oob_score=False, \n                                                    n_jobs=None, \n                                                    random_state=None, \n                                                    verbose=0, \n                                                    warm_start=False, \n                                                    class_weight=None)),\n                     ])\n\n#text_clf_rf.fit(X_train, y_train)\n#predicted = text_clf_rf.predict(X_test)\n#print(metrics.classification_report(y_test, predicted))","ee67fe48":"# Classe avaliadora de performance dos classificadores (com base no exemplo do Prof. Boldt)\n\nfrom sklearn.model_selection import KFold\n\nclass PerformanceEvaluator():\n    \"\"\" Classe avaliadora de performance dos classificadores (com base no exemplo do Prof. Boldt) \"\"\"\n    def __init__(self, X_train, y_train):\n        self.X_train = X_train\n        self.y_train = y_train\n        self.kf = KFold(n_splits=5)\n    \n    def score(self, clf):\n        scores = []\n        for train, validate in self.kf.split(self.X_train):\n            clf.fit(self.X_train[train],self.y_train[train])\n            scores.append(clf.score(self.X_train[validate],self.y_train[validate]))\n        return np.mean(scores), np.std(scores)\n    \n    def treinar(self, clfs):\n        print(f'{\"\":>20}  M\u00e9dia \\t Desvio Padr\u00e3o')\n        for name,clf in clfs:\n            score_mean, score_std = self.score(clf)\n            print(f'{name:>20}: {score_mean:.4f} \\t {score_std:.4f}')\n\n    def testar(self, clfs, X_test, y_test):\n        # Testa os classificadores em dados de teste (n\u00e3o vistos no treinamento)\n        for name,clf in clfs:\n            score = clf.score(X_test, y_test)\n            print(f'{name:>20}: {score:.4f}')","64c8f563":"# Avalia\u00e7\u00e3o de todos os classificadores\n\nclfs = [\n    ('Logistic Regression', text_clf_logistic_regression),\n    ('Naive Bayes', text_clf_naive_bayes),\n    ('KNN', text_clf_knn),\n    ('Decision Tree', text_clf_decision_tree),\n    ('Random Forest', text_clf_rf),\n]\n\npe = PerformanceEvaluator(X_train,y_train)","67c74f37":"%%time\n# Treina os classificadores usando valida\u00e7\u00e3o cruzada\n\npe.treinar(clfs)","5e6f985f":"%%time\n# Testa os classificadores em dados de teste (n\u00e3o vistos no treinamento)\n\npe.testar(clfs, X_test, y_test)","2702f5ac":"# An\u00e1lise Explorat\u00f3ria dos Dados","8a7fbd0f":"# An\u00e1lises  \n\nPela an\u00e1lise pode-se concluir que o classificador mais apropriado neste caso espec\u00edfico foi a regress\u00e3o log\u00edstica.\n\nA avalia\u00e7\u00e3o dos modelos sem e com o preprocessador NLTKTokenizer mostrou que neste caso espec\u00edfico a aplica\u00e7\u00e3o de express\u00f5es regulares e tratamento de palavras piorou o resultado tanto em treinamento quanto em teste. ","f116cda0":"# Teste dos Modelos","7c4c075f":"# Obten\u00e7\u00e3o dos Dados","059422c1":"# Avalia\u00e7\u00e3o dos Modelos\n\nElaborada uma classe avaliadora de performance dos classificadores (com base no exemplo do Prof. Boldt) que utiliza valida\u00e7\u00e3o cruzada.","1630a7d9":"# Prepara\u00e7\u00e3o dos Modelos","580cf6fe":"# Treinamento dos modelos","539ec54c":"# Prepara\u00e7\u00e3o dos Dados","ea4ea688":"# Introdu\u00e7\u00e3o\n\n__Base de dados utilizada:__  \n- 20-Newsgroups: composta por 18.846 documentos em 20 classes  [(link)](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.datasets.fetch_20newsgroups.html).  \n\n__Classificadores testados:__  \n- Regress\u00e3o Log\u00edstica  \n- Naive Bayes  \n- KNN  \n- \u00c1rvore de Decis\u00e3o  \n- Random Forest  ","436e5b79":"## An\u00e1lise dos vetores de caracter\u00edsticas"}}