{"cell_type":{"1aadcaea":"code","3fad8489":"code","b4f25f11":"code","70f6cfba":"code","3f46882c":"code","7750b87b":"code","0cc06f08":"code","6ccfefbd":"code","1b0c232c":"code","333272ea":"code","6204c397":"code","c1927dc1":"code","ffcc8694":"code","f3ed7614":"code","1f02b161":"code","e5ce1c9a":"markdown","039e181d":"markdown"},"source":{"1aadcaea":"import numpy as np \nimport pandas as pd \nimport re\n\nimport numpy as np \nimport requests\nfrom PIL import Image\nfrom io import BytesIO \n\nimport nltk\nfrom nltk.tokenize import word_tokenize\nfrom nltk.stem import PorterStemmer\nfrom nltk.stem import WordNetLemmatizer\n\nfrom collections import Counter\nfrom wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\nsns.set(rc={'figure.figsize':(12,8)})\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","3fad8489":"df = pd.read_csv('\/kaggle\/input\/schopenhauer-work-corpus\/Schopenhauer_works_corpus.csv')\ndf","b4f25f11":"# Isolate the book wanted: Beyond Good and Evil\nwill = df[df['book_title']=='The World As Will And Idea (Vol. 1 of 3)']['text_clean'][8]\ntokens = word_tokenize(will)\nfreq = Counter(tokens)\nsorted_freq = dict(sorted(freq.items(), key=lambda x: x[1], reverse=True))\ntop_25_words = list(sorted_freq.keys())[:25]\ntop_25_freq = list(sorted_freq.values())[:25]\nsns.barplot(y=top_25_words, x=top_25_freq)","70f6cfba":"def plot_cloud(wordcloud):\n    # Set figure size\n    plt.figure(figsize=(12, 8))\n    # Display image\n    plt.imshow(wordcloud) \n    # No axis details\n    plt.axis(\"off\");\n    \n\nwordcloud = WordCloud(width = 3000, height = 2000, random_state=1, \n                      background_color='black', colormap='Set2', \n                      collocations=False, stopwords = STOPWORDS)\nwordcloud.generate_from_frequencies(sorted_freq)\nplot_cloud(wordcloud)","3f46882c":"will = df[df['book_title']=='The World As Will And Idea (Vol. 1 of 3)']['text'][8]\nwill = will.encode(encoding=\"ascii\", errors=\"ignore\").decode()\nwill = re.sub(\"[\\r\\n]\", \" \", will)","7750b87b":"import numpy as np\nimport pandas as pd\nimport nltk\nnltk.download('punkt') # one time execution\nimport re","0cc06f08":"from nltk.tokenize import sent_tokenize\nsentences = []\ntext = will.split(\". \")\nfor s in text:\n    sentences.append(sent_tokenize(s))\n\nsentences = [y for x in sentences for y in x] # flatten list","6ccfefbd":"!wget http:\/\/nlp.stanford.edu\/data\/glove.6B.zip\n!unzip glove*.zip","1b0c232c":"# Extract word vectors\nword_embeddings = {}\nf = open('glove.6B.100d.txt', encoding='utf-8')\nfor line in f:\n    values = line.split()\n    word = values[0]\n    coefs = np.asarray(values[1:], dtype='float32')\n    word_embeddings[word] = coefs\nf.close()","333272ea":"len(word_embeddings)","6204c397":"# remove punctuations, numbers and special characters\nclean_sentences = pd.Series(sentences).str.replace(\"[^a-zA-Z]\", \" \")\n\n# make alphabets lowercase\nclean_sentences = [s.lower() for s in clean_sentences]","c1927dc1":"# function to remove stopwords\nfrom nltk.corpus import stopwords\nstop_words = stopwords.words('english')\ndef remove_stopwords(sen):\n    sen_new = \" \".join([i for i in sen if i not in stop_words])\n    return sen_new\n# remove stopwords from the sentences\nclean_sentences = [remove_stopwords(r.split()) for r in clean_sentences]","ffcc8694":"sentence_vectors = []\nfor i in clean_sentences:\n    if len(i) != 0:\n        v = sum([word_embeddings.get(w, np.zeros((100,))) for w in i.split()])\/(len(i.split())+0.001)\n    else:\n        v = np.zeros((100,))\n    sentence_vectors.append(v)","f3ed7614":"# from sklearn.metrics.pairwise import cosine_similarity\n# sim_mat = np.zeros([len(sentences), len(sentences)])\n# for i in range(len(sentences)):\n#     for j in range(len(sentences)):\n#         if i != j:\n#             sim_mat[i][j] = cosine_similarity(sentence_vectors[i].reshape(1,100), \n#                                               sentence_vectors[j].reshape(1,100))[0,0]","1f02b161":"# import networkx as nx\n\n# nx_graph = nx.from_numpy_array(sim_mat)\n# scores = nx.pagerank(nx_graph)","e5ce1c9a":"# Text Summarization","039e181d":"# Wordcloud"}}