{"cell_type":{"7078c1d3":"code","a473cf68":"code","c69041eb":"code","b91ff9b0":"code","423cc29f":"code","e506ba0a":"code","296c0188":"code","041f829d":"code","cca2d48d":"code","15cea82d":"code","b7b0b113":"code","59e783a2":"code","1da52b5f":"markdown","374d62d0":"markdown","d5502354":"markdown","0ac60e30":"markdown"},"source":{"7078c1d3":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)","a473cf68":"from xgboost import XGBRegressor\nfrom sklearn.ensemble import RandomForestRegressor\nfrom array import array\ndef get_model():\n    model = XGBRegressor(n_estimators=300, learning_rate=0.03, n_jobs=4)\n    return model","c69041eb":"# Load data\nX_full = pd.read_csv('..\/input\/sdfadgdadfda\/medium_data.csv')","b91ff9b0":"# Pairplot\nimport seaborn as sns\nsns.pairplot(X_full)","423cc29f":"from sklearn.model_selection import train_test_split\n\nX_full.dropna(axis=0, subset=['claps'], inplace=True)\ny = X_full['claps']\nX_full.drop(['claps', 'responses', 'id'], axis=1, inplace=True)\nfeatures = [\"reading_time\"]\n\n\n# Break off validation set from training data\nX_train_full, X_valid_full, y_train, y_valid = train_test_split(X_full, y, \n                                                                train_size=.8, test_size=.2,\n                                                                random_state=0)\n# \"Cardinality\" means the number of unique values in a column\n# Select categorical columns with relatively low cardinality (convenient but arbitrary)\ncategorical_cols = [cname for cname in X_train_full.columns if\n                    X_train_full[cname].nunique() < 10 and \n                    X_train_full[cname].dtype == \"object\"]\n\n# Select numerical columns\nnumerical_cols = [cname for cname in X_train_full.columns if \n                X_train_full[cname].dtype in ['int64', 'float64']]\n\n# Keep selected columns only\nmy_cols = categorical_cols + numerical_cols\nX_train = X_train_full[my_cols].copy()\nX_valid = X_valid_full[my_cols].copy()","e506ba0a":"from sklearn.compose import ColumnTransformer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.metrics import mean_absolute_error\n\n# Preprocessing for numerical data\nnumerical_transformer = SimpleImputer(strategy='median')\n\n# Preprocessing for categorical data\ncategorical_transformer = Pipeline(steps=[\n    ('imputer', SimpleImputer(strategy='constant')),\n    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n])\n\n# Bundle preprocessing for numerical and categorical data\npreprocessor = ColumnTransformer(\n    transformers=[\n        ('num', numerical_transformer, numerical_cols),\n        ('cat', categorical_transformer, categorical_cols)\n    ])","296c0188":"# Define model\nmodel = get_model()","041f829d":"# Bundle preprocessing and modeling code in a pipeline\nmy_pipeline = Pipeline(steps=[('preprocessor', preprocessor),\n                              ('model', model)\n                             ])\n\n# Preprocessing of training data, fit model \nmy_pipeline.fit(X_train, y_train)\n\n# Preprocessing of validation data, get predictions\npreds = my_pipeline.predict(X_valid)\n\n# Evaluate the model\nscore = mean_absolute_error(y_valid, preds)\nprint('MAE:', score)","cca2d48d":"X_valid.head","15cea82d":"data = {'publication': ['Estimaciones y riesgo', 'By the book', 'Nuestra capacidad de cumplir'], 'reading_time': [2, 5, 4]}\n# Create DataFrame.\ndf = pd.DataFrame(data)\n# Print the output.\nprint(df)","b7b0b113":"preds = my_pipeline.predict(df)\ndf[\"claps\"] = preds","59e783a2":"df","1da52b5f":"Based on my medium: https:\/\/medium.com\/@fabian-bozoglian\n* 'Estimaciones y riesgo': 5 \n* 'By the book': 14\n* 'Nuestra capacidad de cumplir': 12\n\nMaybe this difference relies in the fact that most of the training set comes from Medium publishers instead of individual contributors. **What do you think?**","374d62d0":"# Lets make some predictions","d5502354":"# Results","0ac60e30":"I would like to use ML to write better articles in Medium. I'm gonna use this dataset to find out what are the features of a clapped article."}}