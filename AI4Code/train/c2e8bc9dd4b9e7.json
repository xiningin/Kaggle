{"cell_type":{"c79029e1":"code","bb64dae3":"code","bdc11b65":"code","56013c81":"code","bade3a65":"code","da581512":"code","6ecc496c":"code","4573940b":"code","c5a364f6":"code","800576f7":"code","2a413799":"code","0b1505d1":"code","161e4ac3":"code","e801525a":"code","1aa67228":"code","d97da5d9":"code","d41d1e5a":"code","faaf1b70":"code","793ef39d":"code","e8a3be1e":"code","f787c341":"code","abd59660":"code","0ade3a1c":"code","a636dc88":"code","501775e5":"code","921a54e5":"code","61e15fd1":"code","5956f7a1":"code","f1fc6ba7":"code","f7fe579a":"code","19e3ea16":"code","1f61bc9f":"code","d1bde717":"code","c725150e":"code","5188c2c5":"code","1dbb5f18":"code","60d5eeaf":"code","c329df0d":"code","fff54aa3":"code","5dc6d394":"code","9023c407":"code","4bd2ec8b":"code","b7a56d04":"code","39dbf510":"code","dd0f26cb":"code","507182b3":"code","85fe952a":"code","59fd4c02":"markdown","dca3adfa":"markdown","af5b9736":"markdown","cb373682":"markdown","cc00c3fd":"markdown","08dbf407":"markdown","c46e1bfa":"markdown","620f3787":"markdown","f51fc594":"markdown","d8e36b7b":"markdown","bf73c075":"markdown","cfe7c54c":"markdown","8af1866e":"markdown","d9e3921f":"markdown","cd302fdd":"markdown","39622497":"markdown","90f3c094":"markdown","59e42f60":"markdown","3ef7ec40":"markdown","fe491877":"markdown","870437f9":"markdown","ca7f6b77":"markdown","f3669069":"markdown","cd56aeed":"markdown","0b5cc0ce":"markdown","3cec276f":"markdown","7b019f2e":"markdown"},"source":{"c79029e1":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import confusion_matrix, accuracy_score\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.svm import SVC\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom xgboost import XGBClassifier\nfrom sklearn.ensemble import AdaBoostClassifier, GradientBoostingClassifier, VotingClassifier\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.neural_network import MLPClassifier\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","bb64dae3":"train_data = pd.read_csv('\/kaggle\/input\/titanic\/train.csv') # Train Dataset\ntest_data = pd.read_csv('\/kaggle\/input\/titanic\/test.csv')   # Test Dataset\n\ntrain_data.head(10) # Disply first 10 rows","bdc11b65":"train_data.info()","56013c81":"test_data.info()","bade3a65":"missing_val_data = train_data[train_data['Age'].isnull()]\nnonmissing_val_data = train_data[train_data['Age'].notnull()]\n\n# missing_val_data[(missing_val_data['Survived'] == 1) & (missing_val_data['Sex'] == 'female')]\n\n# train_data[(train_data['Survived'] == 0) &\n# (train_data['Age'] < 16) &\n# (train_data['Sex'] == 'male') &\n# (train_data['Pclass'] == 1)]\n\n# Plot - Survival of Age Missing Data\n\nsns.barplot(x = missing_val_data['Survived'].unique(), \n            y = missing_val_data['Survived'].value_counts()) \\\n.set(title = 'Survival of Age Missing Data',\n    xlabel = 'Survival')","da581512":"# Plot - Survival of Age Missing Data\n\nsns.catplot(x = \"Sex\", \n            y = \"Survived\", \n            hue = \"Pclass\", \n            kind = \"bar\", \n            data = missing_val_data) \\\n.set(title = 'Survival of Age Missing Data')","6ecc496c":"# Plot - Survival of Age Non Missing Data\n\nsns.catplot(x = \"Sex\", \n            y = \"Survived\", \n            hue = \"Pclass\", \n            kind = \"bar\", \n            data = nonmissing_val_data) \\\n.set(title = 'Survival of Age Non Missing Data')","4573940b":"sns.catplot(x = \"Pclass\", \n            y = \"Survived\", \n            hue = \"Sex\", \n            kind = \"bar\", \n            data = nonmissing_val_data) \\\n.set(title = 'Survival rate of Each Class')","c5a364f6":"child_data = train_data[train_data['Age']<= 16]\nsns.catplot(x = \"Pclass\", \n            y = \"Survived\", \n            hue = \"Sex\", \n            kind = \"bar\", \n            data = child_data) \\\n.set(title = 'Survival rate of Each Class')","800576f7":"train_data.describe()","2a413799":"# Drop unwanted Columns\n    \ntrain_data = train_data.drop(['PassengerId','Ticket','Cabin'], axis = 1)\ntrain_data.info()","0b1505d1":"obj_imputer = SimpleImputer(missing_values = np.nan,strategy = 'most_frequent') # For Categorical\ntrain_data['Embarked'] = obj_imputer.fit_transform(train_data[['Embarked']])\n# train_data.info()\ntrain_data","161e4ac3":"def set_title(name):\n    if name == ' Don' \\\n    or name == ' Rev' \\\n    or name == ' Dr' \\\n    or name == ' Mme' \\\n    or name == ' Major' \\\n    or name == ' Sir' \\\n    or name == ' Col' \\\n    or name == ' Capt' \\\n    or name == ' Jonkheer' \\\n    or name == ' Ms' \\\n    or name == ' Lady' \\\n    or name == ' Mlle' \\\n    or name == ' the Countess' \\\n    or name == ' Dona'\\\n    or name == ' Master':\n        return \"1\"\n    elif name == ' Miss':\n        return \"2\"\n    else:\n        return \"3\"","e801525a":"split_one = train_data['Name'].str.split('.', n=1, expand = True)\ntrain_data['Name'] = split_one[0]\nsplit_two = train_data['Name'].str.split(',', n=1, expand = True)\ntrain_data['Name'] = split_two[1]\n\ntrain_data['Title'] = train_data['Name'].apply(set_title)\n\ntrain_data","1aa67228":"# Drop Name Columns\n    \ntrain_data = train_data.drop(['Name'], axis = 1)\ntrain_data.info()","d97da5d9":"col_names = ['Female','Male','St_C','St_Q','St_S'] # Hot Encode Data\ncol_names.extend((list(train_data.columns)))\ncol_names.remove('Embarked')\ncol_names.remove('Sex')\n\nct = ColumnTransformer(transformers=[('encoder', OneHotEncoder(), [2,7])], \n                       remainder='passthrough') # Corder, HotEncoder and Indexes for Transform\ntrain_data = pd.DataFrame(data = np.array(ct.fit_transform(train_data.values)),\n                         columns = col_names)\ntrain_data","d41d1e5a":"missing_val_data = train_data[train_data['Age'].isnull()] # Age Missing\nnonmissing_val_data = train_data[train_data['Age'].notnull()] # Age Not Missing\n\nmissing_val_data.info()","faaf1b70":"X_MD = missing_val_data.drop(['Survived','Age'], axis = 1).values\ny_MD = missing_val_data['Survived'].values.astype('int')\nr_state = 3\nmd_colnames = missing_val_data.drop(['Survived','Age'], axis = 1).columns\n\nX_train, X_test, y_train, y_test = train_test_split(X_MD, \n                                                    y_MD, \n                                                    test_size = 0.3, \n                                                    random_state = r_state)\n\nX_train","793ef39d":"pd.DataFrame(X_train)","e8a3be1e":"sc_MD = StandardScaler()\nX_train[:, 5:] = sc_MD.fit_transform(X_train[:, 5:])\nX_test[:, 5:] = sc_MD.transform(X_test[:, 5:])\n\nX_train = pd.DataFrame(X_train)\nX_test = pd.DataFrame(X_test)\nprint(X_train.head(3))\nprint(X_test.head(3))","f787c341":"classifier_LR = LogisticRegression(random_state = r_state)\nclassifier_LR.fit(X_train, y_train)\n\ny_pred_LR = classifier_LR.predict(X_test)\naccuracy_score(y_test, y_pred_LR)","abd59660":"classifier_KNN = KNeighborsClassifier(n_neighbors = 5,\n                                      metric = 'minkowski',\n                                      p = 2)\nclassifier_KNN.fit(X_train, y_train)\ny_pred_KNN = classifier_KNN.predict(X_test)\naccuracy_score(y_test, y_pred_KNN)","0ade3a1c":"knn_prams = [{'n_neighbors': [1 , 2, 3, 4, 5, 6, 7], \n              'metric': ['minkowski'], \n              'p': [2]}]\ngrid_search_KNN = GridSearchCV(estimator = classifier_KNN,\n                               param_grid = knn_prams,\n                               scoring = 'accuracy',\n                               cv = 10,\n                               n_jobs = -1)\ngrid_search_KNN.fit(X_train, y_train)\nbest_accuracy_KNN = grid_search_KNN.best_score_\nbest_parameters_KNN = grid_search_KNN.best_params_\nprint(\"Best Accuracy: {:.2f} %\".format(best_accuracy_KNN*100))\nprint(\"Best Parameters:\", best_parameters_KNN)","a636dc88":"classifier_KNN = KNeighborsClassifier(n_neighbors = 2,\n                                      metric = 'minkowski',\n                                      p = 2)\nclassifier_KNN.fit(X_train, y_train)\ny_pred_KNN = classifier_KNN.predict(X_test)\naccuracy_score(y_test, y_pred_KNN)","501775e5":"classifier_SVM = SVC(kernel = 'linear', \n                     random_state = r_state)\nclassifier_SVM.fit(X_train, y_train)\ny_pred_SVM = classifier_SVM.predict(X_test)\naccuracy_score(y_test, y_pred_SVM)","921a54e5":"svm_params = [{'C': [0.25, 0.5, 0.75, 0.85, 1.0], \n               'kernel': ['rbf'], \n               'gamma': [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]}]\ngrid_search_SVM = GridSearchCV(estimator = classifier_SVM,\n                               param_grid = svm_params,\n                               scoring = 'accuracy',\n                               cv = 10,\n                               n_jobs = -1)\ngrid_search_SVM.fit(X_train, y_train)\nbest_accuracy_SVM = grid_search_SVM.best_score_\nbest_parameters_SVM = grid_search_SVM.best_params_\nprint(\"Best Accuracy: {:.2f} %\".format(best_accuracy_SVM*100))\nprint(\"Best Parameters:\", best_parameters_SVM)\n\nclassifier_KSVM = SVC(C = 0.75,\n                      kernel = 'rbf', \n                      random_state = r_state,\n                      gamma = 0.3)\nclassifier_KSVM.fit(X_train, y_train)\ny_pred_KSVM = classifier_KSVM.predict(X_test)\naccuracy_score(y_test, y_pred_KSVM)","61e15fd1":"classifier_NB = GaussianNB()\nclassifier_NB.fit(X_train, y_train)\n\ny_pred_NB = classifier_NB.predict(X_test)\naccuracy_score(y_test, y_pred_NB)","5956f7a1":"classifier_DT = DecisionTreeClassifier(criterion = 'gini', \n                                       random_state = r_state)\nclassifier_DT.fit(X_train, y_train)\n\ny_pred_DT = classifier_DT.predict(X_test)\naccuracy_score(y_test, y_pred_DT)","f1fc6ba7":"classifier_RF = RandomForestClassifier(n_estimators = 10, \n                                       criterion = 'gini', \n                                       random_state = r_state)\nclassifier_RF.fit(X_train, y_train)\n\ny_pred_RF = classifier_RF.predict(X_test)\naccuracy_score(y_test, y_pred_RF)","f7fe579a":"classifier_XGB = XGBClassifier()\nclassifier_XGB.fit(X_train.to_numpy(), y_train)\n\ny_pred_XGB = classifier_XGB.predict(X_test.to_numpy())\naccuracy_score(y_test, y_pred_XGB)\n\n","19e3ea16":"classifier_GB = GradientBoostingClassifier()\nclassifier_GB.fit(X_train, y_train)\ny_pred_GB = classifier_GB.predict(X_test)\naccuracy_score(y_test, y_pred_GB)","1f61bc9f":"classifier_NN = MLPClassifier(random_state = r_state)\n\nclassifiers = [classifier_LR, \n               classifier_KNN,\n               classifier_SVM, \n               classifier_KSVM, \n               classifier_NB, \n               classifier_DT, \n               classifier_RF, \n               classifier_XGB, \n               classifier_GB, \n               classifier_NN]\nclassifiers_names = ['Linear Regression',\n                     'KNN', \n                     'SVM',\n                     'Kernel SVM',\n                     'Naive Bayes',\n                     'Decision Tree',\n                     'Random Forest',\n                     'XG Boost',\n                     'Gradient Boost',\n                     'NN']\naccuracy_mean = []\n\nfor cl in classifiers :\n    accuracies = cross_val_score(estimator = cl, \n                                 X = X_train.to_numpy(), \n                                 y = y_train, \n                                 cv = 10)\n    accuracy_mean.append(accuracies.mean()*100)\n\n    \naccuracy_df = pd.DataFrame({'Classifier': classifiers_names,\n                           'Accuracy Mean': accuracy_mean})\naccuracy_df\n","d1bde717":"age_missing_model1 = classifier_KSVM\nage_missing_model1.fit(X_train, y_train)\n\n\nage_missing_model2 = classifier_SVM\nage_missing_model2.fit(X_train, y_train)\n\n\nage_missing_model3 = classifier_NB\nage_missing_model3.fit(X_train, y_train)\n\nvoting_cl_MD = VotingClassifier(estimators = [('KSVM', age_missing_model1),\n                                              ('SVM',age_missing_model2),\n                                              ('NB',age_missing_model3)], \n                                voting = 'hard')\nvoting_cl_MD.fit(X_train, y_train)\n# y_pred_SVM = classifier_SVM.predict(X_test)","c725150e":"X_NMD = nonmissing_val_data.drop(['Survived'], axis = 1).values\ny_NMD = nonmissing_val_data['Survived'].values.astype('int')\nr_state = 3\nmd_colnames = nonmissing_val_data.drop(['Survived'], axis = 1).columns\n\nX_train, X_test, y_train, y_test = train_test_split(X_NMD, \n                                                    y_NMD, \n                                                    test_size = 0.3, \n                                                    random_state = r_state)\n\nX_train","5188c2c5":"pd.DataFrame(X_train)","1dbb5f18":"sc_NMD = StandardScaler()\nX_train[:, 5:] = sc_NMD.fit_transform(X_train[:, 5:])\nX_test[:, 5:] = sc_NMD.transform(X_test[:, 5:])\n\nX_train = pd.DataFrame(X_train,\n                       columns = md_colnames)\nX_test = pd.DataFrame(X_test,\n                      columns = md_colnames)\nprint(X_train.head(3))\nprint(X_test.head(3))","60d5eeaf":"# Run below code to identify optimal settings for Gradient Boosting\n\n# classifier_GB_NM = GradientBoostingClassifier()\n\n# gb_params = [{'n_estimators' : [100,200,500],\n#               'learning_rate': [0.5, 0.1, 0.05, 0.01],\n#               'max_depth': [3, 5, 9],\n#               'min_samples_leaf': [1,10,50,100]\n#               }]\n# grid_search_GB = GridSearchCV(estimator = classifier_GB_NM,\n#                               param_grid = gb_params,\n#                               scoring = 'accuracy',\n#                               cv = 10,\n#                               n_jobs = -1)\n# grid_search_GB.fit(X_train, y_train)\n# best_accuracy_GB = grid_search_GB.best_score_\n# best_parameters_GB = grid_search_GB.best_params_\n# print(\"Best Accuracy: {:.2f} %\".format(best_accuracy_GB*100))\n# print(\"Best Parameters:\", best_parameters_GB)\n\noptimal_GB_classifier = GradientBoostingClassifier(learning_rate = 0.1,\n                                                   max_depth = 3,\n                                                   min_samples_leaf = 1,\n                                                   n_estimators = 200)","c329df0d":"classifier_NN = MLPClassifier(random_state = r_state)\n\nclassifiers = [classifier_LR, \n               classifier_KNN,\n               classifier_SVM, \n               classifier_KSVM, \n               classifier_NB, \n               classifier_DT, \n               classifier_RF, \n               classifier_XGB, \n               optimal_GB_classifier,\n               classifier_NN]\nclassifiers_names = ['Linear Regression',\n                     'KNN', \n                     'SVM',\n                     'Kernel SVM',\n                     'Naive Bayes',\n                     'Decision Tree',\n                     'Random Forest',\n                     'XG Boost',\n                     'Gradient Boost',\n                     'NN']\naccuracy_mean = []\n\nfor cl in classifiers :\n    accuracies = cross_val_score(estimator = cl, \n                                 X = X_train.to_numpy(), \n                                 y = y_train, \n                                 cv = 10)\n    accuracy_mean.append(accuracies.mean()*100)\n\n    \naccuracy_df = pd.DataFrame({'Classifier': classifiers_names,\n                           'Accuracy Mean': accuracy_mean})\naccuracy_df\n","fff54aa3":"# age_nonmissing_model = GradientBoostingClassifier(learning_rate = 0.1,\n#                                                   max_depth = 3,\n#                                                   min_samples_leaf = 1,\n#                                                   n_estimators = 200)\n\n# age_nonmissing_model.fit(X_train, y_train)\n# y_pred_SVM = classifier_SVM.predict(X_test)\n\nage_nonmissing_model1 = optimal_GB_classifier\nage_nonmissing_model1.fit(X_train, y_train)\n\n\nage_missing_model2 = classifier_NN\nage_missing_model2.fit(X_train, y_train)\n\n\nage_missing_model3 = classifier_KSVM\nage_missing_model3.fit(X_train, y_train)\n\nvoting_cl_NMD = VotingClassifier(estimators = [('GB', age_missing_model1),\n                                              ('MLP',age_missing_model2),\n                                              ('KVSM',age_missing_model3)], \n                                voting = 'hard')\nvoting_cl_NMD.fit(X_train, y_train)\n\n# age_nonmissing_model = MLPClassifier(random_state = r_state)\n# age_nonmissing_model.fit(X_train, y_train)","5dc6d394":"test_data = pd.read_csv('\/kaggle\/input\/titanic\/test.csv')   # Test Dataset\n\n# Remove Unwanted Col\ntest_data = test_data.drop(['Ticket','Cabin'], axis = 1)\ntest_data.info()","9023c407":"obj_imputer_test = SimpleImputer(missing_values = np.nan,\n                                 strategy = 'most_frequent') # For Categorical\nfare_imputer_test = SimpleImputer(missing_values = np.nan,\n                                 strategy = 'mean')\ntest_data['Embarked'] = obj_imputer.fit_transform(test_data[['Embarked']])\ntest_data['Fare'] = fare_imputer_test.fit_transform(test_data[['Fare']])\n# train_data.info()\ntest_data","4bd2ec8b":"split_one = test_data['Name'].str.split('.', n=1, expand = True)\ntest_data['Name'] = split_one[0]\nsplit_two = test_data['Name'].str.split(',', n=1, expand = True)\ntest_data['Name'] = split_two[1]\n\ntest_data['Title'] = test_data['Name'].apply(set_title)\n\ntest_data","b7a56d04":"# Drop Name Columns\n    \ntest_data = test_data.drop(['Name'], axis = 1)\ntest_data.info()","39dbf510":"col_names = ['Female','Male','St_C','St_Q','St_S'] # Hot Encode Data\ncol_names.extend((list(test_data.columns)))\ncol_names.remove('Embarked')\ncol_names.remove('Sex')\n\nct_test = ColumnTransformer(transformers=[('encoder', OneHotEncoder(), [2,7])], \n                            remainder='passthrough') # Corder, HotEncoder and Indexes for Transform\ntest_data = pd.DataFrame(data = np.array(ct_test.fit_transform(test_data.values)),\n                        columns = col_names)\ntest_data\n","dd0f26cb":"missing_val_data_test = test_data[test_data['Age'].isnull()]\nnonmissing_val_data_test = test_data[test_data['Age'].notnull()]\n\nnonmissing_val_data_test.info()","507182b3":"missing_val_data_test.info()","85fe952a":"MD_ID = missing_val_data_test['PassengerId']\nNMD_ID = nonmissing_val_data_test['PassengerId']\n\nX_MD_test = missing_val_data_test.drop(['Age','PassengerId'], axis = 1).values\nX_NMD_test = nonmissing_val_data_test.drop(['PassengerId'], axis = 1).values\n\n\nX_MD_test[:, 5:] = sc_MD.transform(X_MD_test[:, 5:])\nX_NMD_test[:, 5:] = sc_NMD.transform(X_NMD_test[:, 5:])\n\ny_pred_missing_age = []\ny_pred_nonmissing_age = []\n\ny_pred_nonmissing = voting_cl_NMD.predict(X_NMD_test)\ny_pred_missing_age = voting_cl_MD.predict(X_MD_test)\n\n\npred_df1 =  pd.DataFrame(\n    {\"PassengerId\" : MD_ID,\n     \"Survived\" : y_pred_missing_age})\npred_df2 =  pd.DataFrame(\n    {\"PassengerId\" : NMD_ID,\n     \"Survived\" : y_pred_nonmissing})\nframes = [pred_df1, pred_df2]\nresult = pd.concat(frames)\n\nfile_name = \"Submission_New.csv\"\nresult.to_csv(file_name, index = False)","59fd4c02":"**Kernel SVM**","dca3adfa":"**Train and Test Data split**","af5b9736":"Now We need to add Object imputer since we have missing values on 'Embarked' Column","cb373682":"**Feature Scaling**","cc00c3fd":"**Best Model For Age Non Missing Data are Gradient Boost , Neural network and Kernel SVM. Then we could use Voting Classifier for Age non missing values.**","08dbf407":"**Decision Tree**","c46e1bfa":"# Cross Validation on Age Missing Data","620f3787":"**KNN Grid Search to find the best parameters**","f51fc594":"# Models For Age Missing Dataset","d8e36b7b":"**XG Boost**","bf73c075":"**Random Forest**","cfe7c54c":"**Support Vector Machine - SVM**","8af1866e":"It's clear that **Age**, **Fare** and **Cabin** have some missing values. My first though was to replace Age column missing values with 'Mean' age value of the column. But I thought that it's better to have a look about those missing Age data first before taking a decision. ","d9e3921f":"**Now we need to add Hot Encoder to 'Embarked' Column**","cd302fdd":"**Optimal Settings for Gradient Boosting**","39622497":"**Gradient Boosting**","90f3c094":"# **Importing Titanic Dataset**\n","59e42f60":"**Best 'n_neighbors' value would be 2. So let's use 2 Neighbors.**","3ef7ec40":"Drop PassengerId, Ticket Number , Cabin and Name","fe491877":"# Test Results File Generation","870437f9":"**Best Models For Age Missing Data are Kernel SVM , SVM and Naive Bayes. Then we could use Voting Classifier Age missing values.**","ca7f6b77":"**K Nearest Neighbors**","f3669069":"**Naive Bayes**","cd56aeed":"# Models For Age Non Missing Dataset","0b5cc0ce":"**Logistic Regression**","3cec276f":"Next we could check about the Non-Null data count and their datatype spread.","7b019f2e":"**Feature Scaling**"}}