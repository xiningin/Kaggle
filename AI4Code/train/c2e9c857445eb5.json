{"cell_type":{"49644c55":"code","2ffafc4a":"code","c1c58370":"code","fcefb3a8":"code","66f4202e":"code","75c43023":"code","bd5a935b":"code","5ca61300":"code","ddfc041d":"code","b8054019":"code","fa3a1183":"code","4ed22ce4":"code","36ed0139":"code","a54078b7":"code","d8014e8c":"code","c2e79638":"code","84fd9fc0":"code","2f44135a":"code","6cef9275":"code","676e8d14":"code","8e433eca":"code","22c5834d":"code","fb32f7f0":"code","d6ae1efc":"code","23432347":"code","15913ede":"code","d6ca5551":"code","8df2a938":"code","042c22f4":"code","f6edaf85":"code","1724d77b":"code","9758b2ca":"code","6afdc157":"code","8207bdf8":"code","0934b677":"code","6e0604d3":"code","4fd03d6c":"markdown","4da494a9":"markdown","6fb23a29":"markdown","84987aff":"markdown","3c5a8f0c":"markdown","75b37128":"markdown","41a2f8fd":"markdown","4478a6af":"markdown","96bdf5d7":"markdown","6edb2472":"markdown","ac182f42":"markdown","183fa2fc":"markdown","77426e14":"markdown","9e2019f9":"markdown","2ca88657":"markdown","018ce537":"markdown","e07c271f":"markdown","277f59a8":"markdown","da7e0467":"markdown","4cce45da":"markdown","95f021a5":"markdown","7d9816bd":"markdown","3fc4fd15":"markdown","1c6a1556":"markdown","a5569497":"markdown","75091618":"markdown","30d2b126":"markdown","43dab3a6":"markdown"},"source":{"49644c55":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","2ffafc4a":"import matplotlib.pyplot as plt\nimport plotly.express as px\nimport seaborn as sns\n\nimport math\nfrom sklearn import preprocessing\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom sklearn import linear_model\nfrom sklearn.metrics import r2_score","c1c58370":"px.defaults.template = \"plotly_dark\"\nsns.set_theme()","fcefb3a8":"df = pd.read_csv('\/kaggle\/input\/bigmartsalesdataset\/bigmart.csv')","66f4202e":"print(df.shape)\ndf.head()","75c43023":"df.info()","bd5a935b":"df['Outlet_Establishment_Year'] = df.Outlet_Establishment_Year.astype('object')","5ca61300":"df.describe()","ddfc041d":"df.isnull().sum()","b8054019":"df[['Outlet_Type', 'Outlet_Location_Type', 'Outlet_Size']].value_counts(dropna=False).to_frame().sort_index()","fa3a1183":"df[['Item_Type', 'Item_Weight']].value_counts(dropna=False)","4ed22ce4":"new_weights = []\nfor x, row in df.iterrows():\n    if math.isnan(row[1]):\n        new_weights.append(df[df['Item_Identifier'] == row[0]].Item_Weight.max())\n    else:\n        new_weights.append(row[1])\n\ndf['Item_Weight'] = new_weights\ndf.isnull().sum()","36ed0139":"# dropping rows with missing values\nto_drop = df.iloc[list(df['Item_Weight'].isnull())].index\ndf.drop(to_drop, axis=0, inplace=True)\nto_drop = df.iloc[list(df['Outlet_Size'].isnull())].index\ndf.drop(to_drop, axis=0, inplace=True)","a54078b7":"print(df.shape)\ndf.isnull().sum()","d8014e8c":"df.describe(include='all')","c2e79638":"df.Item_Fat_Content.value_counts()","84fd9fc0":"df.Item_Fat_Content.replace(to_replace=['LF', 'low fat'], value='Low Fat', inplace=True)\ndf.Item_Fat_Content.replace(to_replace='reg', value='Regular', inplace=True)\ndf.Item_Fat_Content.value_counts()","2f44135a":"for col in df.columns:\n    print(col)\n    df[col].hist()\n    plt.show()","6cef9275":"sns.regplot(data=df, x='Item_MRP', y='Item_Outlet_Sales')","676e8d14":"sns.regplot(data=df, x='Item_Visibility', y='Item_Outlet_Sales')","8e433eca":"sns.boxplot(data=df, x='Item_Fat_Content', y='Item_Outlet_Sales')","22c5834d":"sns.boxplot(data=df, x='Outlet_Establishment_Year', y='Item_Outlet_Sales')","fb32f7f0":"sns.boxplot(data=df, x='Outlet_Location_Type', y='Item_Outlet_Sales')","d6ae1efc":"sns.boxplot(data=df, x='Outlet_Type', y='Item_Outlet_Sales')","23432347":"item_fat_content = pd.get_dummies(data=df['Item_Fat_Content']).drop('Regular', axis=1)\noutlet_location_type = pd.get_dummies(data=df['Outlet_Location_Type'])\noutlet_type = pd.get_dummies(data=df['Outlet_Type'])\nitem_type = pd.get_dummies(data=df['Item_Type'])\noutlet_size = pd.get_dummies(data=df['Outlet_Size'])\nX = pd.concat(objs=[outlet_location_type,\n                    outlet_type,\n                    df.Outlet_Establishment_Year.astype('float'),\n                    outlet_size,\n                    item_fat_content,\n                    df.Item_Visibility,\n                    item_type,\n                    df.Item_Weight,\n                    df.Item_MRP],\n              axis=1)","15913ede":"corr_df = X.corr()\nX = preprocessing.StandardScaler().fit(X).transform(X)\ny = df.Item_Outlet_Sales.to_numpy()\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=2)\nprint('Train set:', X_train.shape,  y_train.shape)\nprint('Test set:', X_test.shape,  y_test.shape)","d6ca5551":"z = corr_df.to_numpy()\nz_text = np.around(z, decimals=2)  # Only show rounded value (full value on hover)\n\nx = corr_df.columns.to_list()\ny = corr_df.columns.to_list()\n\nfig = px.imshow(z, x=x, y=y,\n                title='Correlation Heatmap',\n                labels=dict(\n                    # x=\"Day of Week\",\n                    # y=\"Time of Day\",\n                    color=\"Correlation\"),\n                color_continuous_scale='Matter_r',\n                width=600, height=600,\n                )\n# fig.update_xaxes(side=\"top\")\nfig.update_xaxes(tickangle=-90)\nfig.show()","8df2a938":"LR = linear_model.LinearRegression()\nLR.fit(X_train, y_train)\nprint('Coefficients: ', LR.coef_)\nprint('Intercept: ', LR.intercept_)","042c22f4":"y_hat = LR.predict(X_test)\nprint(\"Mean absolute error: %.2f\" % np.mean(np.absolute(y_hat - y_test)))\nprint(\"Residual sum of squares (MSE): %.2f\" % np.mean((y_hat - y_test) ** 2))\nprint(\"R2-score: %.2f\" % r2_score(y_test, y_hat))","f6edaf85":"i = 2\nPR = PolynomialFeatures(degree=i)\nX_train_poly = PR.fit_transform(X_train)\nclf = linear_model.LinearRegression()\ny_hat = clf.fit(X_train_poly, y_train)\n# The coefficients\nprint('Coefficients:', clf.coef_[:10], '...')\nprint('Intercept:', clf.intercept_)","1724d77b":"X_test_poly = PR.fit_transform(X_test)\ny_hat = clf.predict(X_test_poly)\nprint(\"Mean absolute error: %.2f\" % np.mean(np.absolute(y_hat - y_test)))\nprint(\"Residual sum of squares (MSE): %.2f\" % np.mean((y_hat - y_test) ** 2))\nprint(f'Degree={i}')\nprint(\"R2-score: %.2f\" % r2_score(y_test, y_hat))","9758b2ca":"df = pd.read_csv('\/kaggle\/input\/bigmartsalesdataset\/bigmart.csv')\n\ndf.drop(['Outlet_Size'], axis=1, inplace=True)\n\n# Replacing each missing Item_Weight based on its corresponding Item_Identifier:\nnew_weights = []\nfor x, row in df.iterrows():\n    if math.isnan(row[1]):\n        new_weights.append(df[df['Item_Identifier'] == row[0]].Item_Weight.max())\n    else:\n        new_weights.append(row[1])\n\ndf['Item_Weight'] = new_weights\n# dropping rows with missing values from Item_Weight\nto_drop = df.iloc[list(df['Item_Weight'].isnull())].index\ndf.drop(to_drop, axis=0, inplace=True)\n\ndf.Item_Fat_Content.replace(to_replace=['LF', 'low fat'], value='Low Fat', inplace=True)\ndf.Item_Fat_Content.replace(to_replace='reg', value='Regular', inplace=True)\n\nitem_fat_content = pd.get_dummies(data=df['Item_Fat_Content']).drop('Regular', axis=1)\noutlet_location_type = pd.get_dummies(data=df['Outlet_Location_Type'])\noutlet_type = pd.get_dummies(data=df['Outlet_Type'])\nitem_type = pd.get_dummies(data=df['Item_Type'])\nX = pd.concat(objs=[outlet_location_type,\n                    outlet_type,\n                    df.Outlet_Establishment_Year.astype('float'),\n                    item_fat_content,\n                    df.Item_Visibility,\n                    item_type,\n                    df.Item_MRP],\n              axis=1)\n\nX = preprocessing.StandardScaler().fit(X).transform(X)\ny = df.Item_Outlet_Sales.to_numpy()\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=2)\nprint('Train set:', X_train.shape,  y_train.shape)\nprint('Test set:', X_test.shape,  y_test.shape)","6afdc157":"LR = linear_model.LinearRegression()\nLR.fit(X_train, y_train)\nprint(f'Coefficients (n={len(LR.coef_)}):', LR.coef_)\nprint()\nprint('Intercept: ', LR.intercept_)","8207bdf8":"y_hat = LR.predict(X_test)\nprint(\"Mean absolute error: %.2f\" % np.mean(np.absolute(y_hat - y_test)))\nprint(\"Residual sum of squares (MSE): %.2f\" % np.mean((y_hat - y_test) ** 2))\nprint(\"R2-score: %.2f\" % r2_score(y_test, y_hat))","0934b677":"i = 2\nPR = PolynomialFeatures(degree=i)\nX_train_poly = PR.fit_transform(X_train)\nclf = linear_model.LinearRegression()\ny_hat = clf.fit(X_train_poly, y_train)\n# The coefficients\nprint(f'Coefficients (n={len(clf.coef_)}):', clf.coef_[:10], '...')\nprint('Intercept:', clf.intercept_)","6e0604d3":"X_test_poly = PR.fit_transform(X_test)\ny_hat = clf.predict(X_test_poly)\nprint(\"Mean absolute error: %.2f\" % np.mean(np.absolute(y_hat - y_test)))\nprint(\"Residual sum of squares (MSE): %.2f\" % np.mean((y_hat - y_test) ** 2))\nprint(f'Degree={i}')\nprint(\"R2-score: %.2f\" % r2_score(y_test, y_hat))","4fd03d6c":"Dropping the rows with missing values from `Outlet_Size` and `Item_Weight`:","4da494a9":"_ | Details\n--- | ---\nTasks | Perform EDA on [Bigmart Sales Dataset](https:\/\/www.kaggle.com\/yasserh\/bigmartsalesdataset) dataset.\nOwner | yasserh\nID | yasserh\/bigmartsalesdataset\nTags | business, tabular data, e-commerce services, regression, linear regression \nSubtitle | ML Project Datset on BigMart Sales Prediction\nDescription | The data scientists at BigMart have collected 2013 sales data for numerous products across many stores in different cities. Also, certain attributes of each product and store have been defined.<br><br>The aim is to build a predictive model and find out the sales of each product at a particular store.<br><br>Using this model, BigMart will try to understand the properties of products and stores which play a key role in increasing sales.\nLicense | CC0: Public Domain","6fb23a29":"### Polynomial Regression","84987aff":"Inspecting `Item_Fat_Content` and replacing irregular values","3c5a8f0c":"## Setup","75b37128":"The dataset appears to have missing values, and the datatypes can easily be modified to suit a regression model.","41a2f8fd":"## Polynomial Regression","4478a6af":"## Re-engineering Features","96bdf5d7":"## Conclusion\nAlthough the $R^2$ score improved, the dataset appears to be insufficient for modeling. Further features engineering or additional data may be required to have an appropriate prediction for sales.","6edb2472":"## Heatmap Visualization","ac182f42":"### Inspecting each variable and its distribution","183fa2fc":"### Visualizing the distribution of each variable","77426e14":"## Dataset Inspection","9e2019f9":"### Inspecting \/ Handling Missing Values","2ca88657":"#### Histograms","018ce537":"#### `Item_Weight`","e07c271f":"It appears that dropping the missing values from `Outlet_Size` and `Item_Weight` lead to a very low `r2_score`. Reloading the dataset and dropping the entire `Outlet_Size` column instead to see if the test scores improve.","277f59a8":"#### `Outlet_Size`","da7e0467":"Attempting to replace each missing `Item_Weight` based on its corresponding `Item_Identifier`:","4cce45da":"## Multiple Linear Regression","95f021a5":"### Multiple Linear Regression","7d9816bd":"The coefficient of determination $R^2=(1 - \\frac{u}{v})$, where:\n> $u$ is the residual sum of squares $\\sum{(y - \\hat{y})}^2$\n> <br>and $v$ is the total sum of squares $\\sum{(y - \\bar{y})^2}$.\n* The best possible score is 1.0 and it can be negative (because the model can be arbitrarily worse). \n* A constant model that always predicts the expected value of `y`, disregarding the input features, would get a $R^2$ score of 0.0.","3fc4fd15":"## Features Engineering","1c6a1556":"Typecasting the Year feature to ensure it is a dimension rather than a measure","a5569497":"#### Scatterplots","75091618":"## EDA","30d2b126":"# Bigmart Sales Dataset - EDA, LR","43dab3a6":"The missing data in `Outlet_Size` appears somewhere within a conceptual hierarchy for the Outlet dimensions. There seems to be no simple way to get a mean for this feature."}}