{"cell_type":{"961167aa":"code","b404d75e":"code","e68693ce":"code","7cfebb42":"code","8056b8cf":"code","1913a766":"code","ff37904f":"code","ab7c5632":"code","3a556cab":"code","9cf3d91f":"code","c448e4d2":"code","ac56687c":"code","f7e2e007":"code","d8760f7c":"code","b3ec28a2":"code","2c7fdf4a":"code","6d2b5a07":"code","7322a50a":"code","1fbb9436":"code","120010d8":"code","f70922d8":"code","3b1c9c62":"code","556179d2":"code","d0e955b1":"code","abdc879e":"code","f14cd5ba":"code","28fddce5":"code","2db29b94":"code","7cbefbac":"code","9e478e94":"code","1579e786":"code","2c054612":"code","77b9bf0d":"code","feff5abc":"markdown","a6f9e85c":"markdown","40811f65":"markdown","121c6faf":"markdown","5b82db02":"markdown","371c783c":"markdown","1fa892f6":"markdown","9a9b0a11":"markdown","37893f50":"markdown","0ebad347":"markdown","940998d7":"markdown","64da2d86":"markdown","256451c7":"markdown","c649ee5d":"markdown","ecd1f99e":"markdown","0de852f8":"markdown"},"source":{"961167aa":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","b404d75e":"import numpy as np\nimport pandas as pd\nimport pandas_datareader as web\nimport matplotlib.pyplot as plt\nplt.style.use('fivethirtyeight')","e68693ce":"# BBRI\nbbri = web.DataReader('bbri.jk', 'yahoo', '2008-01-01', '2020-05-31')\nbbri","7cfebb42":"# Let's visualize the Closing Price\n\nrolling_mean20 = bbri['Close'].rolling(window=20).mean()\nrolling_mean50 = bbri['Close'].rolling(window=50).mean()\nrolling_mean200 = bbri['Close'].rolling(window=200).mean()\n\nplt.figure(figsize = (16,8))\nplt.plot(bbri['Close'], color = 'b', label = 'BBRI Close')\nplt.plot(rolling_mean20, color = 'r', linewidth = 2.5, label = 'MA20')\nplt.plot(rolling_mean50, color = 'y', linewidth = 2.5, label = 'MA50')\nplt.plot(rolling_mean200, color = 'c',linewidth = 2.5, label = 'MA200')\n\nplt.xlabel('Date', fontsize = 15)\nplt.ylabel('Closing Price in Rupiah (Rp)', fontsize = 18)\nplt.title('Closing Price of BBRI')\nplt.legend(loc = 'lower right')","8056b8cf":"# Don't forget to using 'values' attribute before apply it into Neural Network!\n\nX = bbri.drop('Close', axis = 1).values\ny = bbri['Close'].values","1913a766":"from sklearn.model_selection import train_test_split","ff37904f":"# Split the Data into Train and Test dataset with 20% test size.\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=101)","ab7c5632":"X_train.shape","3a556cab":"# Scale it with MinMaxScaler\nfrom sklearn.preprocessing import MinMaxScaler","9cf3d91f":"scaler = MinMaxScaler()","c448e4d2":"X_train = scaler.fit_transform(X_train)\nX_test = scaler.fit_transform(X_test)","ac56687c":"y_train = y_train.reshape(-1,1)\ny_test = y_test.reshape(-1,1)","f7e2e007":"# Reshape it into 3-dimensional before input it into LSTM Model.\n# Reshape the data to be 3-dimensional in the form [number of samples, number of time steps, and number of features].\n\nX_train = np.reshape(X_train, (X_train.shape[0], X_train.shape[1], 1))\nX_test = np.reshape(X_test, (X_test.shape[0], X_test.shape[1], 1))","d8760f7c":"from tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, Dropout, LSTM","b3ec28a2":"# just to remember our dataset shape\n\nprint(X_train.shape)\nprint(y_train.shape)\nprint(X_test.shape)\nprint(y_test.shape)","2c7fdf4a":"model = Sequential()\n\n# input layer\nmodel.add(LSTM(5, return_sequences=True, input_shape=(X_train.shape[1],1)))\nmodel.add(LSTM(5, return_sequences= False))\n\n# hidden layer\nmodel.add(Dense(5, activation='relu'))\nmodel.add(Dense(5, activation='relu'))\n\n# output layer\nmodel.add(Dense(1))\n\n# compiler\nmodel.compile(optimizer='adam', loss = 'mse')","6d2b5a07":"from tensorflow.keras.callbacks import EarlyStopping","7322a50a":"early_stop = EarlyStopping(monitor = 'val_loss', mode = 'min', verbose = 1, patience = 25)","1fbb9436":"# Let's train our model!\n\nmodel.fit(x = X_train,\n          y = y_train,\n          validation_data = (X_test, y_test),\n          epochs = 500, \n          callbacks = [early_stop])","120010d8":"# So there are 162 iterations. Let's recap our training loss vs validation loss, and make it to DataFrame.\n\nmodel_loss = pd.DataFrame(model.history.history)\nmodel_loss","f70922d8":"plt.figure(figsize = (12,6))\nmodel_loss.plot()\nplt.xlabel('n of Epochs')","3b1c9c62":"from sklearn.metrics import mean_squared_error, explained_variance_score","556179d2":"predictions = model.predict(X_test)","d0e955b1":"# MSE\nmean_squared_error(y_test, predictions)","abdc879e":"# RMSE\nnp.sqrt(mean_squared_error(y_test, predictions))","f14cd5ba":"# Explained variance regression score function\n\nexplained_variance_score(y_test, predictions)\n# Best possible score is 1.0, lower values are worse (sklearn).","28fddce5":"plt.figure(figsize=(12,6))\nplt.scatter(y_test,predictions)\nplt.plot(y_test, y_test, 'r', linewidth = 2.5)\nplt.xlabel('y_test')\nplt.ylabel('predictions')\nplt.title('LSTM Model Prediction Evaluation')","2db29b94":"# BBRI\nbbri2 = web.DataReader('bbri.jk', 'yahoo', '2020-06-01', '2020-06-01')\nbbri2","7cbefbac":"bbri2 = bbri2.drop('Close', axis = 1)","9e478e94":"bbri2 = bbri2.values","1579e786":"bbri2 = scaler.transform(bbri2)","2c054612":"bbri2 = bbri2.reshape(-1, 5, 1)","77b9bf0d":"model.predict(bbri2)","feff5abc":"Still can't believe this that our model's predictions just fit with y_test dataset. It shows fit very well with few outliers there. It's okay. Let's try to predict closing price of BBRI on June 2nd.","a6f9e85c":"## 3) Make a LSTM Model\nLong Short Term Memory networks \u2013 usually just called \u201cLSTMs\u201d \u2013 are a special kind of RNN, capable of learning long-term dependencies. They were introduced by Hochreiter & Schmidhuber (1997), and were refined and popularized by many people in following work.1 They work tremendously well on a large variety of problems, and are now widely used.\n\nLSTMs are explicitly designed to avoid the long-term dependency problem. Remembering information for long periods of time is practically their default behavior, not something they struggle to learn!\n\nAll recurrent neural networks have the form of a chain of repeating modules of neural network. In standard RNNs, this repeating module will have a very simple structure, such as a single tanh layer. ","40811f65":"So, it's already splitted with 80:20. Notice our X_train size is 2457 (80% of the dataset).","121c6faf":"I set epochs to 500 iterations, but at the same time I do believe this model won't iterate more than 500 cause EarlyStopping will save us.","5b82db02":"Our model predicts 3124 of BBRI's closing price on June 2nd, 2020 while the actual close price is 3180. Well, that's not bad though since our model's RMSE is about 34 units and the evaluation just fit between its predictions and y_test dataset.","371c783c":"I can't believe this our model get \"just right\" form along the training. We see that 'training loss' and 'validation loss' walk along the side. It's beautiful graph that you could see.","1fa892f6":"## 1) Data Preparation\nUsing Pandas DataReader, we can get dataset about the stock that we want to use. Of course we're using some libraries to make it happen.","9a9b0a11":"## 5) Predict with Another Data on June 2nd, 2020\nFirst we get our dataset, then prepare it up into our model's prediction.","37893f50":"## 2) Data Pre-processing\nNow let's straight to the pre-processing. We have to prepare up like splitting our dataset into Train and Test dataset. We do this by call some libraries from sklearn.","0ebad347":"# Predicting Closing Price of BBRI's Stock\n\nBBRI is the stock name of Bank Rakyat Indonesia. PT Bank Rakyat Indonesia (Persero) Tbk (People's Bank of Indonesia, commonly known as BRI or Bank BRI) is one of the largest banks in Indonesia. It specialises in small scale and microfinance style borrowing from and lending to its approximately 30 million retail clients through its over 4,000 branches, units and rural service posts. It also has a comparatively small, but growing, corporate business. As of 2010 it is the second largest bank in Indonesia by asset.\n\nThis dataset can be found using [Pandas DataReader](https:\/\/pandas-datareader.readthedocs.io\/en\/latest\/) and will pick from January 2008 to the end of May 2020. It will be more than 12 years. In this session, we will predict the closing price of BBRI with Long Short-Term Memory.\n\nLong short-term memory (LSTM) is an artificial recurrent neural network (RNN) architecture used in the field of deep learning. Unlike standard feedforward neural networks, LSTM has feedback connections. It can not only process single data points (such as images), but also entire sequences of data (such as speech or video). For example, LSTM is applicable to tasks such as unsegmented, connected handwriting recognition, speech recognition and anomaly detection in network traffic or IDS's (intrusion detection systems).","940998d7":"\nHere the visualization of BBRI's closing price since 2008. Based on this graph, it's been a good money when you invest your money into this stock. Just saying. I'm showing you the Moving Average (MA) too withing 20, 50 and 200 days to make it more technical with these indicators.","64da2d86":"## 4) Predictions\nTraining is done, let's get our model to predict and compare it with y_test dataset. We will see the value of RMSE and linear regression graph.","256451c7":"We use 5 neurons in input layer because we have 5 features along training our model. We add 1 hidden layer with 5 neurons and also ReLU activation. Finally adding 1 layer for output. We choose Adam optimizer with mean squared error for calculating model's loss.","c649ee5d":"We see that we get the value of RMSE is 34.533. It's very small according to the context of stocks. Look at the 'explained variance score', we get 0.9995. Our model just did a great job. So what if we plot our valid dataset with model's prediction? Could we get some nice line of regression?","ecd1f99e":"I love efficiency, so I add Callbacks feature to the model with EarlyStopping from TensorFlow to avoid overfitting and improve generalization.","0de852f8":"After scaling our X dataset, we should reshape our dataset into proper one. Especially on X dataset, we should turn it into 3-dimensional data before apply it to LSTM NN model or you'll get some advices from the notebook."}}