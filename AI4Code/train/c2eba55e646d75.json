{"cell_type":{"e0125e2f":"code","f5084825":"code","679336f9":"code","bb2b8863":"code","8f2d00a8":"code","8db827b4":"code","13246c72":"code","3e98a42d":"code","b9158422":"code","ae2693f4":"code","390516a4":"code","67736e8b":"code","239650f0":"code","cdd681b8":"code","1ceb2a5c":"code","282f85a5":"code","bf1c2d61":"code","93464184":"code","8d790d98":"code","77dae93b":"code","1c8264ec":"code","7ca79096":"code","81619a13":"code","c93accde":"code","bcd7df3f":"code","33c5673e":"code","12b202ba":"code","6e00b9e8":"code","b273cb27":"code","0b991704":"code","d9f6a7e7":"code","7d8c5c41":"code","3dabd372":"code","92fa96f2":"code","e80481a7":"code","b6681fac":"code","de13265a":"code","a6a58164":"code","1c4c686d":"code","e30346b6":"code","e9775045":"markdown","eb6158b7":"markdown","3c5a1a9f":"markdown","3bcadbed":"markdown","14d7b4bd":"markdown","66e16f38":"markdown","0dcbd722":"markdown","0ebe7037":"markdown","31421c0c":"markdown","f5ff2590":"markdown","af3fba89":"markdown","da80dd5d":"markdown","8a74cfda":"markdown","0332ba84":"markdown","10e9c48f":"markdown","4a4a01db":"markdown","c4455909":"markdown","bd8cbe2f":"markdown","44b22c69":"markdown","46ab8feb":"markdown","b32784d1":"markdown","df4b17ee":"markdown","44123d0a":"markdown","99d831ea":"markdown","64d3708e":"markdown","92958a91":"markdown","dd275e38":"markdown","65a8288f":"markdown","54a65fe8":"markdown","69ed1b1a":"markdown","8a477b99":"markdown","58501746":"markdown","0988b540":"markdown","0987a997":"markdown","794a32ee":"markdown","eb1ec961":"markdown","30869919":"markdown","f0dcb6a9":"markdown","d315dc55":"markdown","efefa650":"markdown","c5f08079":"markdown","72fc66b4":"markdown","07f749e1":"markdown","d3cfb817":"markdown","9c3074b6":"markdown","99c4da0a":"markdown","8493eef7":"markdown","46feeeee":"markdown","cf75215d":"markdown","170e19b6":"markdown"},"source":{"e0125e2f":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom imblearn.over_sampling import SMOTE\nfrom sklearn.model_selection import train_test_split\nimport xgboost as xgb\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom catboost import CatBoostClassifier, Pool\nfrom sklearn.ensemble import AdaBoostClassifier\nimport time\nfrom lightgbm import LGBMClassifier\nprint('Imported')","f5084825":"df = pd.read_csv('\/kaggle\/input\/banksim1\/bs140513_032310.csv')\ndf","679336f9":"plt.scatter(x=\"amount\", y=\"fraud\", data=df)","bb2b8863":"# let us confirm if all customers have multiple transactions\n\nprint('Number of duplicated rows: {}.'.format(df['customer'].duplicated().shape)) # Yes all customers have multiple transactions","8f2d00a8":"# intiate dictionary to store the maximum transaction amount for each customer\namount_df = df[['customer','amount']].groupby('customer').max().reset_index()\nMax_amount = {}\n     \nfor i,row in amount_df.iterrows():\n    Max_amount[row.customer] = row.amount\n\n# dividing the value for each transaction by maximum transaction amount for the same customer        \nfor i,row in df.iterrows():\n    df.at[i,'amount'] = df.at[i,'amount']\/Max_amount[row.customer]\n\ndf","8db827b4":"# Confirm if all records are processed \nlen(Max_amount)","13246c72":"bins = np.linspace(df.step.min(), df.step.max(), 10)\ng = sns.FacetGrid(df, col=\"gender\", hue=\"fraud\", palette=\"Set1\", col_wrap=4)\ng.map(plt.hist,'gender', bins=bins, ec=\"k\")\n\ng.axes[0].legend()\nplt.show()","3e98a42d":"bins = np.linspace(df.step.min(), df.step.max(), 10)\ng = sns.FacetGrid(df, col=\"category\", hue=\"fraud\", palette=\"Set1\", col_wrap=4)\ng.map(plt.hist,'category', bins=bins, ec=\"k\")\n\ng.axes[0].legend()\ng.axes[1].legend()\ng.axes[3].legend()\ng.axes[5].legend()\ng.axes[8].legend()\ng.axes[9].legend()\ng.axes[10].legend()\nplt.show()","b9158422":"bins = np.linspace(df.step.min(), df.step.max(), 10)\ng = sns.FacetGrid(df, col=\"age\", hue=\"fraud\", palette=\"Set1\", col_wrap=4)\ng.map(plt.hist,'age', bins=bins, ec=\"k\")\n\ng.axes[0].legend()\ng.axes[1].legend()\ng.axes[2].legend()\ng.axes[3].legend()\ng.axes[4].legend()\ng.axes[5].legend()\ng.axes[6].legend()\ng.axes[7].legend()\nplt.show()","ae2693f4":"bins = np.linspace(df.step.min(), df.step.max(), 10)\ng = sns.FacetGrid(df, col=\"merchant\", hue=\"fraud\", palette=\"Set1\", col_wrap=4)\ng.map(plt.hist,'merchant', bins=bins, ec=\"k\")\n\ng.axes[0].legend()\ng.axes[1].legend()\ng.axes[2].legend()\ng.axes[3].legend()\ng.axes[4].legend()\ng.axes[5].legend()\ng.axes[6].legend()\ng.axes[7].legend()\n\nplt.show()","390516a4":"bins = np.linspace(df.step.min(), df.step.max(), 10)\ng = sns.FacetGrid(df, col=\"step\", hue=\"fraud\", palette=\"Set1\", col_wrap=4)\ng.map(plt.hist,'step', bins=bins, ec=\"k\")\n\ng.axes[0].legend()","67736e8b":"df.isna().sum()","239650f0":"for i,row in df.iterrows():\n    df.at[i,'category'] = row.category[4:-1]\n    df.at[i,'merchant'] = row.merchant[2:-1]\n    df.at[i,'customer'] = row.customer[2:-1]\ndf","cdd681b8":"df['age'].replace({\"'U'\": \"'2'\"},inplace=True)","1ceb2a5c":"df['gender'].replace({\"'U'\":\"'M'\"},inplace=True)","282f85a5":"df['customer'] = df['customer'].astype('float64')","bf1c2d61":"df['customer'] = df['customer']\/df['customer'].max()","93464184":"df.drop(['zipcodeOri', 'zipMerchant','step'],axis=1,inplace=True)","8d790d98":"df = pd.get_dummies(df, columns = ['gender'])\ndf = pd.get_dummies(df, columns = ['category'])\ndf = pd.get_dummies(df, columns = ['merchant'])\ndf = pd.get_dummies(df, columns = ['age'])\ndf.head(2)","77dae93b":"# Check the number of columns (features) after hot encoding\nlen(df.columns)","1c8264ec":"df_training = df.iloc[0:500000,:]\ndf_production = df.iloc[500000:594643,:]","7ca79096":"# check the shape for production dataset\ndf_production.shape","81619a13":"# Check the number of fraudulent transactions in training and testing dataset\ndf_training.loc[df_training['fraud'] == 1].shape","c93accde":"# Check the number of fraudulent transactions in the production dataset\ndf_production.loc[df_production['fraud'] == 1].shape","bcd7df3f":"X = df_training[['customer', 'amount', \"gender_'F'\", \"gender_'M'\",\"gender_'E'\",\n       'category_barsandrestaurants', 'category_contents', 'category_fashion',\n       'category_food', 'category_health', 'category_home',\n       'category_hotelservices', 'category_hyper', 'category_leisure',\n       'category_otherservices', 'category_sportsandtoys', 'category_tech',\n       'category_transportation', 'category_travel',\n       'category_wellnessandbeauty', 'merchant_1053599405',\n       'merchant_117188757', 'merchant_1198415165', 'merchant_1294758098',\n       'merchant_1313686961', 'merchant_1352454843', 'merchant_1353266412',\n       'merchant_1400236507', 'merchant_1416436880', 'merchant_151143676',\n       'merchant_1535107174', 'merchant_1600850729', 'merchant_1649169323',\n       'merchant_1726401631', 'merchant_17379832', 'merchant_1741626453',\n       'merchant_1748431652', 'merchant_1788569036', 'merchant_1823072687',\n       'merchant_1842530320', 'merchant_1872033263', 'merchant_1873032707',\n       'merchant_1888755466', 'merchant_1913465890', 'merchant_1946091778',\n       'merchant_2011752106', 'merchant_2080407379', 'merchant_209847108',\n       'merchant_2122776122', 'merchant_348875670', 'merchant_348934600',\n       'merchant_349281107', 'merchant_3697346', 'merchant_45060432',\n       'merchant_480139044', 'merchant_495352832', 'merchant_50039827',\n       'merchant_547558035', 'merchant_677738360', 'merchant_692898500',\n       'merchant_732195782', 'merchant_78078399', 'merchant_840466850',\n       'merchant_855959430', 'merchant_857378720', 'merchant_85975013',\n       'merchant_923029380', 'merchant_933210764', 'merchant_97925176',\n       'merchant_980657600', \"age_'0'\", \"age_'1'\", \"age_'2'\", \"age_'3'\",\n       \"age_'4'\", \"age_'5'\", \"age_'6'\"]]","33c5673e":"y = df_training['fraud']","12b202ba":"X_prod = df_production[['customer', 'amount', \"gender_'F'\", \"gender_'M'\", \"gender_'E'\",\n       'category_barsandrestaurants', 'category_contents', 'category_fashion',\n       'category_food', 'category_health', 'category_home',\n       'category_hotelservices', 'category_hyper', 'category_leisure',\n       'category_otherservices', 'category_sportsandtoys', 'category_tech',\n       'category_transportation', 'category_travel',\n       'category_wellnessandbeauty', 'merchant_1053599405',\n       'merchant_117188757', 'merchant_1198415165', 'merchant_1294758098',\n       'merchant_1313686961', 'merchant_1352454843', 'merchant_1353266412',\n       'merchant_1400236507', 'merchant_1416436880', 'merchant_151143676',\n       'merchant_1535107174', 'merchant_1600850729', 'merchant_1649169323',\n       'merchant_1726401631', 'merchant_17379832', 'merchant_1741626453',\n       'merchant_1748431652', 'merchant_1788569036', 'merchant_1823072687',\n       'merchant_1842530320', 'merchant_1872033263', 'merchant_1873032707',\n       'merchant_1888755466', 'merchant_1913465890', 'merchant_1946091778',\n       'merchant_2011752106', 'merchant_2080407379', 'merchant_209847108',\n       'merchant_2122776122', 'merchant_348875670', 'merchant_348934600',\n       'merchant_349281107', 'merchant_3697346', 'merchant_45060432',\n       'merchant_480139044', 'merchant_495352832', 'merchant_50039827',\n       'merchant_547558035', 'merchant_677738360', 'merchant_692898500',\n       'merchant_732195782', 'merchant_78078399', 'merchant_840466850',\n       'merchant_855959430', 'merchant_857378720', 'merchant_85975013',\n       'merchant_923029380', 'merchant_933210764', 'merchant_97925176',\n       'merchant_980657600', \"age_'0'\", \"age_'1'\", \"age_'2'\", \"age_'3'\",\n       \"age_'4'\", \"age_'5'\", \"age_'6'\"]]","6e00b9e8":"y_prod = df_production['fraud']","b273cb27":"sm = SMOTE(random_state=42)\nX_res, y_res = sm.fit_resample(X, y)","0b991704":"# number of records have been doubeld \nX_res.shape","d9f6a7e7":"X_train, X_test, y_train, y_test = train_test_split(X_res,y_res,test_size=0.3,random_state=42,stratify=y_res)","7d8c5c41":"# building the model\ntest_data = catboost_pool = Pool(X_test,y_test)\n\nmodel = CatBoostClassifier(iterations=2,\n                           depth=2,\n                           learning_rate=1,\n                           loss_function='Logloss',\n                           verbose=True)\n# train the model\nmodel.fit(X_train, y_train)\n# make the prediction using the resulting model\nyhat = model.predict(test_data)\npreds_proba = model.predict_proba(test_data)\nprint(\"class = \", yhat)\nprint(\"proba = \", preds_proba)\nprint(\"Classification Report for CatBoost: \\n\", classification_report(y_test, yhat))","3dabd372":"# Plot confusion matrix function:\nimport itertools\ndef plot_confusion_matrix(cm, classes,\n                          normalize=False,\n                          title='Confusion matrix',\n                          cmap=plt.cm.Blues):\n\n    yhat = classes\n    y_test = cm\n    \"\"\"\n    This function prints and plots the confusion matrix.\n    Normalization can be applied by setting `normalize=True`.\n    \"\"\"\n    if normalize:\n        cm = cm.astype('float') \/ cm.sum(axis=1)[:, np.newaxis]\n        print(\"Normalized confusion matrix\")\n    else:\n        print('Confusion matrix, without normalization')\n\n    print(cm)\n\n    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n    plt.title(title)\n    plt.colorbar()\n    tick_marks = np.arange(len(classes))\n    plt.xticks(tick_marks, classes, rotation=45)\n    plt.yticks(tick_marks, classes)\n\n    fmt = '.2f' if normalize else 'd'\n    thresh = cm.max() \/ 2.\n    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n        plt.text(j, i, format(cm[i, j], fmt),\n                 horizontalalignment=\"center\",\n                 color=\"white\" if cm[i, j] > thresh else \"black\")\n\n    plt.tight_layout()\n    plt.ylabel('True label')\n    plt.xlabel('Predicted label')\n\nprint(confusion_matrix(y_test,yhat, labels=[1,0]))","92fa96f2":"# Evalute the model\ntime_zero = time.time()\nprod_data = catboost_pool = Pool(X_prod)\nyhatCat = model.predict(prod_data)\nduration = time.time() - time_zero\nprint(\"Classification Report for CatBoost: \\n\", classification_report(y_prod, yhatCat))\nprint('Duration: {}.'.format(duration))\n# Compute confusion matrix\ncnf_matrix = confusion_matrix(y_prod, yhatCat, labels=[1,0])\nnp.set_printoptions(precision=2)\n\n\n# Plot non-normalized confusion matrix\nplt.figure()\nplot_confusion_matrix(cnf_matrix, classes=['fraud','true'],normalize= False,  title='Confusion matrix')","e80481a7":"# building the model\nclf = AdaBoostClassifier(n_estimators=100, random_state=0)\nclf.fit(X_train, y_train)\nyhat = clf.predict(X_test)\nprint(\"Classification Report for AdaBoost: \\n\", classification_report(y_test, yhat))","b6681fac":"# Evalute the model\ntime_zero = time.time()\nyhatAda = clf.predict(X_prod)\nduration = time.time() - time_zero\nprint(\"Classification Report for AdaBoost: \\n\", classification_report(y_prod, yhatAda))\nprint('Duration: {}.'.format(duration))\n\n# Compute confusion matrix\ncnf_matrix = confusion_matrix(y_prod, yhatAda, labels=[1,0])\nnp.set_printoptions(precision=2)\n\n\n# Plot non-normalized confusion matrix\nplt.figure()\nplot_confusion_matrix(cnf_matrix, classes=['fraud','true'],normalize= False,  title='Confusion matrix')","de13265a":"# building the model\ntime_zero = time.time()\n\nlgbm = LGBMClassifier()\n\nlgbm.fit(X_train,y_train)\n\nyhat = lgbm.predict(X_test)\n\n\nduration = time.time() - time_zero\n\nprint(\"Classification Report for LGBBoost: \\n\", classification_report(y_test, yhat))\n\nprint('Duration: {}.'.format(duration))\n","a6a58164":"# Evalute the model\ntime_zero = time.time()\nyhatLgb = lgbm.predict(X_prod)\nduration = time.time() - time_zero\nprint(\"Classification Report for LgbBoost: \\n\", classification_report(y_prod, yhatLgb))\nprint('Duration: {}.'.format(duration))\n\n# Compute confusion matrix\ncnf_matrix = confusion_matrix(y_prod, yhatLgb, labels=[1,0])\nnp.set_printoptions(precision=2)\n\n\n# Plot non-normalized confusion matrix\nplt.figure()\nplot_confusion_matrix(cnf_matrix, classes=['fraud','true'],normalize= False,  title='Confusion matrix')","1c4c686d":"# Building the model\nXgboost = xgb.XGBClassifier(max_depth=6, learning_rate=0.05, n_estimators=400, \n                                objective=\"binary:hinge\", booster='gbtree', \n                                n_jobs=-1, nthread=None, gamma=0, min_child_weight=1, max_delta_step=0, \n                                subsample=1, colsample_bytree=1, colsample_bylevel=1, reg_alpha=0, reg_lambda=1, \n                                scale_pos_weight=1, base_score=0.5, random_state=42, verbosity=1)\n\ntime_zero = time.time()\nXgboost.fit(X_train,y_train)\n\nyhat = Xgboost.predict(X_test)\nduration = time.time() - time_zero\n\nprint(\"Classification Report for XGBoost: \\n\", classification_report(y_test, yhat))\nprint('Duration: {}.'.format(duration))","e30346b6":"# Evalute the model\ntime_zero = time.time()\nyhatXgb = Xgboost.predict(X_prod)\nduration = time.time() - time_zero\nprint(\"Classification Report for XGBoost: \\n\", classification_report(y_prod, yhatXgb))\nprint('Duration: {}.'.format(duration))\n\n# Compute confusion matrix\ncnf_matrix = confusion_matrix(y_prod, yhatXgb, labels=[1,0])\nnp.set_printoptions(precision=2)\n\n\n# Plot non-normalized confusion matrix\nplt.figure()\nplot_confusion_matrix(cnf_matrix, classes=['fraud','true'],normalize= False,  title='Confusion matrix')","e9775045":"Insert the data and create the dataframe:","eb6158b7":"<a name='xgbboost'\/>\nXGBBoost","3c5a1a9f":"2- Gender, let us see if one gender has preference based on number of fraudulent transactions","3bcadbed":"<a name='lgbboost'\/>\nLGBBoost","14d7b4bd":"<b>Context<\/b><br>\nBankSim is an agent-based simulator of bank payments based on a sample of aggregated transactional data provided by a bank in Spain. The main purpose of BankSim is the generation of synthetic data that can be used for fraud detection research. Statistical and a Social Network Analysis (SNA) of relations between merchants and customers were used\nto develop and calibrate the model. Our ultimate goal is for BankSim to be usable to model relevant scenarios that combine normal payments and injected known fraud signatures. The data sets generated by BankSim contain no personal information or disclosure of legal and private customer transactions. Therefore, it can be shared by academia, and others, to develop and reason about fraud detection methods. Synthetic data has the added benefit of being easier to acquire, faster and at less cost, for experimentation even for those that have access to their own data. We argue that BankSim generates data that usefully approximates the relevant aspects of the real data.\n\n<b>Content<\/b><br>\nWe ran BankSim for 180 steps (approx. six months), several times and calibrated the parameters in order to obtain a distribution that get close enough to be reliable for testing. We collected several log files and selected the most accurate. We injected thieves that aim to steal an average of three cards per step and perform about two fraudulent transactions per day. We produced 594643 records in total. Where 587443 are normal payments and 7200 fraudulent transactions. Since this is a randomized simulation the values are of course not identical to original data.\n","66e16f38":"<a name='recomendation'\/>\nRecomendation","0dcbd722":"Extract approximately 20% percent of dataset for production to simulate the actual data and will be used to test the model, This dataset is different than testing dataset which also used for purpose of testing the model","0ebe7037":"<a name='adaboost'\/>\n AdaBoost","31421c0c":"Customer id\/ maximum customer id","f5ff2590":"whereas merchants with high number of transactions have no fraudulent transactions, whereas other merchants with low number of transactions have high number of fraudulent transactions so let us add it to the featuers","af3fba89":"Remove the columns (zipcodeOri, zipmerchant and step) becuase:<br>\n1- zipcodeOri and zipmerchant each one of them has one value only! so they can't be added to the featuers<br>\n2- step also can't be added to the deatuers because the distribution for fraudulent transactions are symmetrical among different steps","da80dd5d":"3- Category","8a74cfda":"<a name='discussion'\/>\nDiscussion","0332ba84":"Change the \"U\" to 2 in age column","10e9c48f":"Comparing to the number numbers of males and females it looks there is littel bit preference for Female gender so let us add it to our featuers list","4a4a01db":"<h1 align=center><font size = 5>Detect the fraudulent E-payment transactions by using supervised machine learning models<\/font><\/h1>","c4455909":"4- Age","bd8cbe2f":"<a name='DataExploration' \/>\nData Exploration","44b22c69":"6-step","46ab8feb":"-Accuracy(by using testing dataset) = 98%<br>\n-Accuracy(by using production dataset) = 98%<br>\n-990 fraud transactions out of 1020 fraud transactions have been classified correctly by model<br>\n-2004 true transactions out of 93623 true transactions have been classied by model as fraud transactions<br>\n-Estimated time to proceed production dataset consist of 94643 rows is 0.18203353881835938s (time slightly varies from one run to another)","b32784d1":"## Table of Contents\n\n<div class=\"alert alert-block alert-info\" style=\"margin-top: 20px\">\n\n<font size = 3>\n\n<a href=#aboutdataset>About the Dataset<\/a>\n\n<a href=#DataExploration>Data Exploration<\/a>\n\n<a href=#DataProcessing>Data Processing<\/a>\n\n<a href=#hotencoding>Hot Encoding<\/a>\n\n<a href=#splitingdata>Spliting Data<\/a>\n    \n\n<a href=#Trainingdataset>Training Dataset<\/a>\n    \n<a href=#oversampling>Oversampling<\/a>\n <br>\n \n   \n    \n\n<a href=#catboost>CatBoost Model<\/a>\n    \n<a href=#adaboost>AdaBoost Model<\/a>\n    \n<a href=#lgbboost>LGBBoost Model<\/a>\n    \n<a href=#xgbboost>XGBBoost Model<\/a>\n    \n    \n<a href=#results>Results<\/a> \n    \n<a href=#discussion>Discussion<\/a>    \n\n<a href=#recomendation>Recomendation<\/a> \n    \n<\/font>\n<\/div>","df4b17ee":"Different models will be evaluted to choose the optimal one","44123d0a":"There are some categories where most of fradulent transactions fall under them so let us add it to the featuers list","99d831ea":"AdaBoost and LGBBoost are the recomended models for this dataset","64d3708e":"-Accuracy(by using testing dataset) = 98% <br>\n-Accuracy(by using production dataset) = 98% <br>\n-1003 fraud transactions out of 1020 fraud transactions have been classified correctly by model! <br>\n-2335 true transactions out of 93623 true transactions have been classied by model as fraud transactions<br>\n-Estimated time to proceed production dataset consist of 94643 rows is 5.924099922180176s (time slightly varies from one run to another)","92958a91":"<a name='catboost'\/>\nCatBoost","dd275e38":"<a name='aboutdataset'\/>\nAbout the Dataset","65a8288f":"-Accuracy(by using testing dataset) = 96% <br>\n-Accuracy(by using production dataset) = 94% <br>\n-1020 fraud transactions out of 1020 fraud transactions have been classified correctly by model! <br>\n-5534 true transactions out of 93623 true transactions have been classied by model as fraud transactions<br>\n-Estimated time to proceed production dataset consist of 94643 rows is 0.028914451599121094s (time slightly varies from one run to another)","54a65fe8":"\n1-Though Catboost has the minimum latecy and it has predicted all fraud transactions however it has predecited 5534 true transactions out of 93623 true transactions as fraud transactions which unfortunatly reduced its accuracy and move it away from top of list<br>\n2-XGBBoost and LGBBoost have predicted correctly 990 fraudulent transactions out of 1020 fradulent transactions however XGBBoost latency time is more than LGBBoost and it has predicted 2264 true transactions as fraudulent transactions whereas LGB has predicted 2004 true transactions as fraudulent transactions which gives LGBBoost preference over XGBBoost<br>\n3- AdaBoost and LGBBoost are similar, LGBBoost latency is better than AdaBoost on the other hand AdaBoost has predicted correctly more fraudulent transactions than LGBBoost which make both of them are the optimal option.","69ed1b1a":"<a name='Trainingdataset'\/>\nTraining Dataset","8a477b99":"We see that all steps have approxiamtly the same ratio of fraudulent transactions so let us excluded it from our featuers","58501746":"Oversampling aims to increase the number of fraudulent transactions to be close to the number of truthfull transactions, this process is high requiered to balance the data, without balancing the data, model will be more more biosed to truthfull transaction and less sensitive to fraudulent transactions and you will end up with model with low accuracy ","0988b540":"<table>\n  <thead>\n    <tr>\n      <th>Model Name<\/th>\n      <th>Accuracy%(Using production Dataset)<\/th>\n      <th>Number of true predicted fraud transactions (Out of 1020)<\/th>\n      <th>Number of wrong predicted fraud transactions (Out of 93623)<\/th>\n       <th>Latency(s)<\/th>\n    <\/tr>\n  <\/thead>\n  <tbody>\n    <tr>\n      <td>CatBoost<\/td>\n      <td>94%<\/td>\n      <td>1020<\/td>\n      <td>5534<\/td>\n      <td>0.028914451599121094<\/td>\n    <\/tr>\n    <tr>\n      <td>AdaBoost<\/td>\n      <td>98%<\/td>\n      <td>1003<\/td>\n      <td>2335<\/td>\n      <td>5.924099922180176<\/td>\n    <\/tr>\n    <tr>\n      <td>LGBBoost<\/td>\n      <td>98%<\/td>\n      <td>990<\/td>\n      <td>2004<\/td>\n      <td>0.18203353881835938<\/td>\n    <\/tr>\n    <tr>\n      <td>XGBBoost<\/td>\n      <td>98%<\/td>\n      <td>990<\/td>\n      <td>2262<\/td>\n      <td>0.5125706195831299<\/td>\n    <\/tr>\n\n  <\/tbody>\n<\/table>","0987a997":"<b>CatBoost:<\/b><br>\n-Accuracy(by using testing dataset) = 96% <br>\n-Accuracy(by using production dataset) = 94% <br>\n-1020 fraud transactions out of 1020 fraud transactions have been classified correctly by model<br>\n-5534 true transactions out of 93623 true transactions have been classied by model as fraud transactions<br>\n-Estimated time to proceed production dataset consist of 94643 rows is 0.028914451599121094s (time slightly varies from one run to another)\n<br><br>\n\n<b>AdaBoost:<\/b><br>\n-Accuracy(by using testing dataset) = 98% <br>\n-Accuracy(by using production dataset) = 98% <br>\n-1003 fraud transactions out of 1020 fraud transactions have been classified correctly by model! <br>\n-2335 true transactions out of 93623 true transactions have been classied by model as fraud transactions<br>\n-Estimated time to proceed production dataset consist of 94643 rows is 5.924099922180176s (time slightly varies from one run to another)\n\n<br><br>\n\n<b>LGBBoost:<\/b><br>\n-Accuracy(by using testing dataset) = 98%<br>\n-Accuracy(by using production dataset) = 98%<br>\n-990 fraud transactions out of 1020 fraud transactions have been classified correctly by model<br>\n-2004 true transactions out of 93623 true transactions have been classied by model as fraud transactions<br>\n-Estimated time to proceed production dataset consist of 94643 rows is 0.18203353881835938s (time slightly varies from one run to another)\n    \n<br><br>\n<b> XGBBoost:<\/b><br>\n-Accuracy(by using testing dataset) = 98%<br>\n-Accuracy(by using production dataset) = 98%<br>\n-990 fraud transactions out of 1020 fraud transactions have been classified correctly by model<br>\n-2262 true transactions out of 93623 true transactions have been classied by model as fraud transactions<br>\n-Estimated time to proceed production dataset consist of 94643 rows is 0.5125706195831299s (time slightly varies from one run to another)\n\n","794a32ee":"5- Merchant","eb1ec961":"Remove the characters from category,merchant and customer columns","30869919":"All ages have fraudulent transactions","f0dcb6a9":"Check if there is any null value in dataframe","d315dc55":"<a name='DataProcessing' \/>\nData Processing","efefa650":"The transactions with high amount are more likley be fraudulant, however comparing the amounts of 2 transactions for different customers doesn't sound right since the value of the transactions amounts vary from customer to other and what considered low for one customer could be high for another one's so let us compare the transactions for each customer by dividing the transaction value for each customer by maximum transaction amount which belong to the same customer, this apprach will enable the model to figure out the abnormal behavoiur for each customer transaction, the good thing that this dataset has 4112 customers and each one of them has many transactions, by standrazied the transaction amount in this way then each transaction amount will be between 0 and 1 ","c5f08079":"Looks like all ages have fraudulent transactions with differents ratios so let us add it this column to the featuers list","72fc66b4":"-Accuracy(by using testing dataset) = 98%<br>\n-Accuracy(by using production dataset) = 98%<br>\n-990 fraud transactions out of 1020 fraud transactions have been classified correctly by model<br>\n-2262 true transactions out of 93623 true transactions have been classied by model as fraud transactions<br>\n-Estimated time to proceed production dataset consist of 94643 rows is 0.5125706195831299s (time slightly varies from one run to another)","07f749e1":"Change the type of customer column to float","d3cfb817":"<a name='splitingdata'\/>\nSpliting Data","9c3074b6":"Change the \"U\" to \"M\" in gender column","99c4da0a":"<a name='results'\/>\nResults","8493eef7":"<a name='hotencoding'\/>\nHot Encoding","46feeeee":"Block of below code will standardize the amount for each transactions as explained above","cf75215d":"<a name ='oversampling'\/>\nOversampling","170e19b6":"Let us explore the featuers and determine which one's will be selected as a featuers for our model<br>\n 1- Start with amount:"}}