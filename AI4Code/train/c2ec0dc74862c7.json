{"cell_type":{"7c326d76":"code","09d22292":"code","4eee90f7":"code","591c3a62":"code","7c33c883":"code","90339a16":"code","e7f49d0f":"code","033e0caf":"code","71c451d5":"code","1400d44b":"code","9e5d81f2":"code","b9f7a045":"code","201a5027":"code","40ea2873":"code","84032a85":"code","3b6bd3c9":"code","8c3ff8b9":"code","0970db7e":"code","e37da9e2":"code","9b3bb3a6":"code","84d0a0e9":"code","93037549":"code","d480c479":"code","eb94db9b":"code","aded4edd":"code","dca0675a":"code","aa51484e":"code","faa4bd2c":"code","b386e515":"code","03d7fe0c":"code","43d46eae":"code","071beae1":"code","4bb0a7e4":"code","76a3df61":"code","32170bc6":"code","0372535d":"code","03443206":"code","6d8b9dfd":"code","a9c14501":"markdown","ff739ca4":"markdown","2a99f513":"markdown","195e6e2e":"markdown","4cb39d68":"markdown","59ca0dd5":"markdown","9e10541f":"markdown","7e5040da":"markdown","5a2684ef":"markdown","bf8f38b1":"markdown","3ce2c84a":"markdown","007add34":"markdown"},"source":{"7c326d76":"import numpy as np \nimport pandas as pd \npd.set_option('display.max_colwidth', -1)\nfrom time import time\nimport re\nimport string\nimport os\nfrom pprint import pprint\nimport collections\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set(style=\"darkgrid\")\nsns.set(font_scale=1.3)\n\nfrom sklearn.base import BaseEstimator, TransformerMixin\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.pipeline import Pipeline, FeatureUnion\nfrom sklearn.metrics import classification_report\n\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.externals import joblib\n\nfrom nltk.corpus import stopwords\nfrom nltk.stem import PorterStemmer\nfrom nltk.tokenize import word_tokenize\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\nnp.random.seed(37)","09d22292":"from nltk.tokenize import TweetTokenizer\nimport datetime\nimport lightgbm as lgb\nfrom scipy import stats\nfrom scipy.sparse import hstack, csr_matrix\nfrom sklearn.model_selection import train_test_split, cross_val_score\nfrom wordcloud import WordCloud\nfrom collections import Counter\nfrom nltk.corpus import stopwords\nfrom nltk.util import ngrams\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import LinearSVC\nfrom sklearn.multiclass import OneVsRestClassifier","4eee90f7":"from nltk.stem.snowball import SnowballStemmer\nstemmer = SnowballStemmer(\"english\")","591c3a62":"import spacy","7c33c883":"PATH = '..\/input\/'","90339a16":"df_train = pd.read_csv(PATH + \"train.tsv\", sep = '\\t')\ndf_test = pd.read_csv(PATH + \"test.tsv\", sep = '\\t')","e7f49d0f":"df_train.head(10)","033e0caf":"sns.factorplot(x = \"Sentiment\", data = df_train, kind = 'count', size = 6)","71c451d5":"df_train.Sentiment.value_counts()","1400d44b":"print (\"Number of sentences is {0:.0f}.\".format(df_train.SentenceId.count()))\n\nprint (\"Number of unique sentences is {0:.0f}.\".format(df_train.SentenceId.nunique()))\n\nprint (\"Number of phrases is {0:.0f}.\".format(df_train.PhraseId.count()))","9e5d81f2":"print (\"The average length of phrases in the training set is {0:.0f}.\".format(np.mean(df_train['Phrase'].apply(lambda x: len(x.split(\" \"))))))\n\nprint (\"The average length of phrases in the test set is {0:.0f}.\".format(np.mean(df_test['Phrase'].apply(lambda x: len(x.split(\" \"))))))","b9f7a045":"text = ' '.join(df_train.loc[df_train.Sentiment == 0, 'Phrase'].values)","201a5027":"Counter([i for i in ngrams(text.split(), 3)]).most_common(5)","40ea2873":"print (df_train.info())","84032a85":"df_train.Phrase.str.len().sort_values(ascending = False)","3b6bd3c9":"df_train.loc[105155, 'Phrase']","8c3ff8b9":"df_train.Sentiment.dtype","0970db7e":"nlp = spacy.load('en',disable=['parser', 'tagger', 'ner'])\ndef tokenizer(s): \n    return [w.text.lower() for w in nlp(s)]","e37da9e2":"## stemming sentences\nsentences = list(df_train.Phrase.values) + list(df_test.Phrase.values)\nsentences2 = [[stemmer.stem(word) for word in sentence.split(\" \")] for sentence in sentences]\nfor i in range(len(sentences2)):sentences2[i] = ' '.join(sentences2[i])","9b3bb3a6":"tfidf = TfidfVectorizer(strip_accents = 'unicode', tokenizer = tokenizer, encoding='utf-8', ngram_range = (1,2), max_df = 0.75, min_df = 3, sublinear_tf = True)","84d0a0e9":"_ = tfidf.fit(sentences2)","93037549":"train_phrases2 = [[stemmer.stem(word) for word in sentence.split(\" \")] for sentence in list(df_train.Phrase.values)]\nfor i in range(len(train_phrases2)):train_phrases2[i] = ' '.join(train_phrases2[i])\ntrain_df_flags = tfidf.transform(train_phrases2)","d480c479":"test_phrases2 = [[stemmer.stem(word) for word in sentence.split(\" \")] for sentence in list(df_test.Phrase.values)]\nfor i in range(len(test_phrases2)):test_phrases2[i] = ' '.join(test_phrases2[i])\ntest_df_flags = tfidf.transform(test_phrases2)","eb94db9b":"X_train_tf = train_df_flags[0:125000]\nX_valid_tf = train_df_flags[125000:]\ny_train_tf = (df_train[\"Sentiment\"])[0:125000]\ny_valid_tf = (df_train[\"Sentiment\"])[125000:]\n\nprint(\"X_train shape: \", X_train_tf.shape)\nprint(\"X_valid shape: \",X_valid_tf.shape)\nprint(\"Y_train shape: \",len(y_train_tf))\nprint(\"Y_valid shape: \",len(y_valid_tf))","aded4edd":"from sklearn.linear_model import LogisticRegression","dca0675a":"scores = cross_val_score(LogisticRegression(C=4, dual=True), X_train_tf, y_train_tf, cv=5)","aa51484e":"scores","faa4bd2c":"np.mean(scores), np.std(scores)","b386e515":"logistic = LogisticRegression(C=4, dual=True)\novrm = OneVsRestClassifier(logistic)\novrm.fit(X_train_tf, y_train_tf)","03d7fe0c":"scores = cross_val_score(ovrm, X_train_tf, y_train_tf, scoring='accuracy', n_jobs=-1, cv=3)","43d46eae":"print (np.mean(scores))\nprint (np.std(scores))","071beae1":"print (\"train accuracy:\", ovrm.score(X_train_tf, y_train_tf ))\nprint (\"valid accuracy:\", ovrm.score(X_valid_tf, y_valid_tf))","4bb0a7e4":"df_test.head()","76a3df61":"df_test_logistic = df_test.copy()[[\"PhraseId\"]]\ndf_test_logistic['Sentiment'] = ovrm.predict(test_df_flags)\n\ndf_test_logistic.head()","32170bc6":"svc = LinearSVC(dual=False)\nsvc.fit(X_train_tf, y_train_tf)","0372535d":"print (\"train accuracy:\", svc.score(X_train_tf, y_train_tf ))\nprint (\"valid accuracy:\", svc.score(X_valid_tf, y_valid_tf))","03443206":"df_test_svm = df_test.copy()[[\"PhraseId\"]]\ndf_test_svm['Sentiment'] = svc.predict(test_df_flags)\ndf_test_svm.head()","6d8b9dfd":"df_test_logistic.to_csv(\"submission_tfidf_logistic.csv\", index = False)","a9c14501":"Common trigrams","ff739ca4":"There are 5 different sentiments in the training set.\n* 0 - negative\n* 1 - somewhat negative\n* 2 - neutral\n* 3 - somewhat positive\n* 4 - positive","2a99f513":"SVC is similar to SVM classifier - but is more flexible with parameter tuning. Although, I didn't do much tuning myself. I don't believe that would get us very far. need to explore advanced models","195e6e2e":"Using Logistic Regression, through One Vs Rest classifier. This is just a way to model five separate functions\/algorithms\/models that predicts each class vs the others","4cb39d68":"choosing to go with logistic regression predictions, for no particular reason. Don't expect to see much of a difference","59ca0dd5":"Both models perform incredibly well on the training set but not as much on the validation set. If our test set is any similar to the training and validation sets, the accuracy shouldn't be much farther than the 50is %, if not less","9e10541f":"TFIDF to vectorizer the tokens. uni and bigrams used.","7e5040da":"Around 8529 reviews are split into nearly 156k phrases and each phrase is tagged from 0 to 4. And so, we see far more neutral phrases than polarizing ones. It is likely that the phrases in groups 0 and 4 likely had the most effect on the movie reviews","5a2684ef":"Creating separate dataframes for test predictions using logistic classifier and svc classifiers","bf8f38b1":"Unlike traditional test_train_split using random initialization, here I am splitting the dataset into train and validation datasets in order.\nEvery sentence is broken down into multiple phrases, and so a random split would ensure that starkly similar phrases from the training set would land in the validation set, thereby the validation set performance misleading us into believing that the model generalized well, while all it did was encounter a validation dataset that was mostly a subset of the training dataset.","3ce2c84a":"Linear SVC","007add34":"Some basic EDA. Not much"}}