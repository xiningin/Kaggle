{"cell_type":{"ac3f59d4":"code","4eab1e40":"code","64768003":"code","bf712524":"code","78b66b0a":"code","149b9d41":"code","a3af7984":"code","14ecfca9":"code","1a72571f":"code","1ec7a2c5":"code","bef37064":"code","063a46d0":"code","401f4e0b":"code","e5d7fe6e":"code","0c432e0a":"code","623c4d07":"code","1b002776":"code","22a15d85":"code","d2add8cd":"code","6e89c635":"code","1090faea":"code","8ab1006e":"code","f35f1ef2":"code","00630a55":"code","2fa9ab2c":"code","f6c6d288":"code","da1863e0":"code","42fa6f1f":"code","5b2ac3cc":"code","0c235d8a":"markdown","f16e10fb":"markdown","ef5cf0db":"markdown","d761805f":"markdown","e832e269":"markdown","606891f6":"markdown","9fbbf3d0":"markdown","0481f420":"markdown","0fac41d4":"markdown","de6f456c":"markdown","eb34ff4f":"markdown","edb894cd":"markdown","2fa439f6":"markdown","8917be7c":"markdown","74e95b0b":"markdown","f93ae0b8":"markdown","5257e858":"markdown","427aba39":"markdown","1753f975":"markdown","50a90e0d":"markdown","9424e6d1":"markdown"},"source":{"ac3f59d4":"import numpy as np\nimport pandas as pd \nimport os\nimport shutil\nimport pydicom\nfrom pydicom.pixel_data_handlers.util import apply_voi_lut\nimport cv2\nimport matplotlib.pyplot as plt\nfrom skimage import exposure\nfrom glob import glob\n\nfrom scipy.io import wavfile\nimport warnings\nwarnings.filterwarnings('ignore')\nfrom sklearn.model_selection import GroupKFold\n","4eab1e40":"dataset_dir = '..\/input\/vinbigdata-chest-xray-abnormalities-detection'","64768003":"def dicom2array(path, voi_lut=True, fix_monochrome=True):\n    dicom = pydicom.read_file(path)\n    # VOI LUT (if available by DICOM device) is used to\n    # transform raw DICOM data to \"human-friendly\" view\n    if voi_lut:\n        data = apply_voi_lut(dicom.pixel_array, dicom)\n    else:\n        data = dicom.pixel_array\n    # depending on this value, X-ray may look inverted - fix that:\n    if fix_monochrome and dicom.PhotometricInterpretation == \"MONOCHROME1\":\n        data = np.amax(data) - data\n    data = data - np.min(data)\n    data = data \/ np.max(data)\n    data = (data * 255).astype(np.uint8)\n    return data\n        \n    \ndef plot_img(img, size=(7, 7), is_rgb=True, title=\"\", cmap='gray'):\n    plt.figure(figsize=size)\n    plt.imshow(img, cmap=cmap)\n    plt.suptitle(title)\n    plt.show()\n\n\ndef plot_imgs(imgs, cols=4, size=7, is_rgb=True, title=\"\", cmap='gray', img_size=(500,500)):\n    rows = len(imgs)\/\/cols + 1\n    fig = plt.figure(figsize=(cols*size, rows*size))\n    for i, img in enumerate(imgs):\n        if img_size is not None:\n            img = cv2.resize(img, img_size)\n        fig.add_subplot(rows, cols, i+1)\n        plt.imshow(img, cmap=cmap)\n    plt.suptitle(title)\n    plt.show()","bf712524":"dicom_paths = glob(f'{dataset_dir}\/train\/*.dicom')\nimgs = [dicom2array(path) for path in dicom_paths[:8]]\nplot_imgs(imgs)","78b66b0a":"invert = 256 - np.array(imgs)\nplot_imgs(invert)","149b9d41":"hf1 = np.array([[-1, -1, -1], [-1, 9, -1], [-1, -1, -1]])\n\nafter_hf1 = [cv2.filter2D(img, -1, hf1) for img in imgs]\nplot_imgs(after_hf1)","a3af7984":"hist_eq = [cv2.equalizeHist(img) for img in imgs]\nplot_imgs(hist_eq)","14ecfca9":"def clahe(image):\n    clahe = cv2.createCLAHE(\n        clipLimit = 2., \n        tileGridSize = (10, 10)\n    )\n    \n    image = clahe.apply(image) \n    #image = tf.expand_dims(image, axis = 2)\n    \n    return image","1a72571f":"clahe_ = [clahe(img) for img in imgs]\nplot_imgs(clahe_)","1ec7a2c5":"dim = 512 #1024, 256, 'original'\ntest_dir = f'\/kaggle\/input\/vinbigdata-{dim}-image-dataset\/vinbigdata\/test'\nweights_dir = '\/kaggle\/input\/vinbigdata-cxr-ad-yolov5-14-class-train\/yolov5\/runs\/train\/exp\/weights\/best.pt'","bef37064":"train_df = pd.read_csv(f'..\/input\/vinbigdata-{dim}-image-dataset\/vinbigdata\/train.csv')\ntrain_df.head()","063a46d0":"train_df['image_path'] = f'\/kaggle\/input\/vinbigdata-{dim}-image-dataset\/vinbigdata\/train\/'+train_df.image_id+('.png' if dim!='original' else '.jpg')\ntrain_df.head()","401f4e0b":"train_df = train_df[train_df.class_id!=14].reset_index(drop = True)","e5d7fe6e":"fold = 4\ngkf  = GroupKFold(n_splits = 5)\ntrain_df['fold'] = -1\nfor fold, (train_idx, val_idx) in enumerate(gkf.split(train_df, groups = train_df.image_id.tolist())):\n    train_df.loc[val_idx, 'fold'] = fold\nval_df = train_df[train_df['fold']==4]\nval_df.head()","0c432e0a":"train_files = []\nval_files   = []\nval_files += list(train_df[train_df.fold==fold].image_path.unique())\ntrain_files += list(train_df[train_df.fold!=fold].image_path.unique())\nlen(train_files), len(val_files)","623c4d07":"os.makedirs('\/kaggle\/working\/vinbigdata\/labels\/train', exist_ok = True)\nos.makedirs('\/kaggle\/working\/vinbigdata\/labels\/val', exist_ok = True)\nos.makedirs('\/kaggle\/working\/vinbigdata\/images\/train', exist_ok = True)\nos.makedirs('\/kaggle\/working\/vinbigdata\/images\/val', exist_ok = True)\nlabel_dir = '\/kaggle\/input\/vinbigdata-yolo-labels-dataset\/labels'\nfor file in train_files:\n    shutil.copy(file, '\/kaggle\/working\/vinbigdata\/images\/train')\n    filename = file.split('\/')[-1].split('.')[0]\n    shutil.copy(os.path.join(label_dir, filename+'.txt'), '\/kaggle\/working\/vinbigdata\/labels\/train')\n    \nfor file in val_files:\n    shutil.copy(file, '\/kaggle\/working\/vinbigdata\/images\/val')\n    filename = file.split('\/')[-1].split('.')[0]\n    shutil.copy(os.path.join(label_dir, filename+'.txt'), '\/kaggle\/working\/vinbigdata\/labels\/val')\n    \nval_dir = f'\/kaggle\/working\/vinbigdata\/images\/val'","1b002776":"os.makedirs('\/kaggle\/working\/vinbigdata_processed\/images\/train', exist_ok = True)\nshutil.copytree('\/kaggle\/working\/vinbigdata\/labels\/train','\/kaggle\/working\/vinbigdata_processed\/labels\/train')\nshutil.copytree('\/kaggle\/working\/vinbigdata\/labels\/val','\/kaggle\/working\/vinbigdata_processed\/labels\/val')\nshutil.copytree('\/kaggle\/working\/vinbigdata\/images\/val','\/kaggle\/working\/vinbigdata_processed\/images\/val')","22a15d85":"for file in train_files:\n    shutil.copy(file, '\/kaggle\/working\/vinbigdata\/images\/train')\n    filename = file.split('\/')[-1].split('.')[0]\n    shutil.copy(os.path.join(label_dir, filename+'.txt'), '\/kaggle\/working\/vinbigdata_processed\/labels\/train')\n    ","d2add8cd":"path = '\/kaggle\/working\/vinbigdata\/images\/train'\nsave_path = '\/kaggle\/working\/vinbigdata_processed\/images\/train'\nimages = os.listdir(path)\n\nfor image in images:\n    img_path = os.path.join(path, image)\n    ori_img = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)\n    dst_img = clahe(ori_img)\n\n    cv2.imwrite(os.path.join(save_path, image), dst_img)\n\n    \n","6e89c635":"print(len(os.listdir(save_path)))\nsample = cv2.imread(os.path.join(save_path, images[0]))\nplt.imshow(sample)","1090faea":"class_ids, class_names = list(zip(*set(zip(train_df.class_id, train_df.class_name))))\nclasses = list(np.array(class_names)[np.argsort(class_ids)])\nclasses = list(map(lambda x: str(x), classes))\nclasses","8ab1006e":"from os import listdir\nfrom os.path import isfile, join\nimport yaml\n\ncwd = '\/kaggle\/working\/'\n\nwith open(join( cwd , 'train.txt'), 'w') as f:\n    for path in glob('\/kaggle\/working\/vinbigdata_processed\/images\/train\/*'):\n        f.write(path+'\\n')\n            \nwith open(join( cwd , 'val.txt'), 'w') as f:\n    for path in glob('\/kaggle\/working\/vinbigdata_processed\/images\/val\/*'):\n        f.write(path+'\\n')\n\ndata = dict(\n    train =  join( cwd , 'train.txt') ,\n    val   =  join( cwd , 'val.txt' ),\n    nc    = 14,\n    names = classes\n    )\n\nwith open(join( cwd , 'vinbigdata.yaml'), 'w') as outfile:\n    yaml.dump(data, outfile, default_flow_style=False)\n\nf = open(join( cwd , 'vinbigdata.yaml'), 'r')\nprint('\\nyaml:')\nprint(f.read())","f35f1ef2":"shutil.copytree('\/kaggle\/input\/yolov5-official-v31-dataset\/yolov5', '\/kaggle\/working\/yolov5')\nos.chdir('\/kaggle\/working\/yolov5') # install dependencies\n\nimport torch\nfrom IPython.display import Image, clear_output  # to display images\n\nclear_output()\nprint('Setup complete. Using torch %s %s' % (torch.__version__, torch.cuda.get_device_properties(0) if torch.cuda.is_available() else 'CPU'))","00630a55":"!WANDB_MODE=\"dryrun\" python train.py --img 640 --batch 16 --epochs 30 --data \/kaggle\/working\/vinbigdata.yaml --weights yolov5x.pt --cache\n","2fa9ab2c":"!python detect.py --weights '\/kaggle\/working\/yolov5\/runs\/train\/exp\/weights\/best.pt'\\\n--img 640\\\n--conf 0.15\\\n--iou 0.5\\\n--source \/kaggle\/working\/vinbigdata_processed\/images\/val\\\n--exist-ok","f6c6d288":"import matplotlib.pyplot as plt\nfrom mpl_toolkits.axes_grid1 import ImageGrid\nimport numpy as np\nimport random\nimport cv2\nfrom glob import glob\nfrom tqdm import tqdm\n\nfiles = glob('runs\/detect\/exp\/*')\n\nfor _ in range(1):\n    row = 4\n    col = 4\n    grid_files = files[:16]\n    images     = []\n    for image_path in tqdm(grid_files):\n        img          = cv2.cvtColor(cv2.imread(image_path), cv2.COLOR_BGR2RGB)\n        images.append(img)\n\n    fig = plt.figure(figsize=(col*5, row*5))\n    grid = ImageGrid(fig, 111,  # similar to subplot(111)\n                     nrows_ncols=(col, row),  # creates 2x2 grid of axes\n                     axes_pad=0.05,  # pad between axes in inch.\n                     )\n\n    for ax, im in zip(grid, images):\n        # Iterating over the grid returns the Axes.\n        ax.imshow(im)\n        ax.set_xticks([])\n        ax.set_yticks([])\n    plt.show()","da1863e0":"weights_dir = '\/kaggle\/input\/vinbigdata-cxr-ad-yolov5-14-class-train\/yolov5\/runs\/train\/exp\/weights\/best.pt'\n# train 30 epochs\n# val mAP_0.5 : 0.31919\n# val mAP_0.5:0.95 : 0.14161","42fa6f1f":"!python detect.py --weights $weights_dir\\\n--img 640\\\n--conf 0.15\\\n--iou 0.5\\\n--source \/kaggle\/working\/vinbigdata_processed\/images\/val\\\n--save-txt --save-conf --exist-ok\n","5b2ac3cc":"import matplotlib.pyplot as plt\nfrom mpl_toolkits.axes_grid1 import ImageGrid\nimport numpy as np\nimport random\nimport cv2\nfrom glob import glob\nfrom tqdm import tqdm\n\nfiles = glob('runs\/detect\/exp\/*')\n\nfor _ in range(1):\n    row = 4\n    col = 4\n    grid_files = files[:16]\n    images     = []\n    for image_path in tqdm(grid_files):\n        img          = cv2.cvtColor(cv2.imread(image_path), cv2.COLOR_BGR2RGB)\n        images.append(img)\n\n    fig = plt.figure(figsize=(col*5, row*5))\n    grid = ImageGrid(fig, 111,  # similar to subplot(111)\n                     nrows_ncols=(col, row),  # creates 2x2 grid of axes\n                     axes_pad=0.05,  # pad between axes in inch.\n                     )\n\n    for ax, im in zip(grid, images):\n        # Iterating over the grid returns the Axes.\n        ax.imshow(im)\n        ax.set_xticks([])\n        ax.set_yticks([])\n    plt.show()","0c235d8a":"# Enhancement trial","f16e10fb":"# Compare with non-clahe image train models","ef5cf0db":"# Setups for training yolo v5","d761805f":"# Goal of this experiment\n(Written in Korean, but I will edit in English asap)\n\n* Because X-ray imaging devices cannot focus like optical lenses, The resulting image generally tends to be slightly blurred.\n* Radiologists diagnose X-ray images with their own eyes, so it is determined that the black\/white ratio and shape (blood vessels, lungs) are the criteria, which are determined by the edge.\n* Therefore, I think it would be helpful to emphasize this edge information on the image through pre-processing.\n* Image enhancement was experimented using several basic methods\n* And train yolo v5 model with enhanced images to see if this affects the actual performance.\n* Also, preprocessed image datasets (abnormal classes) are also provided as a result.\n\n","e832e269":"# Compare with GT labels","606891f6":"# Setting for enhanced images","9fbbf3d0":"# Data Settings","0481f420":"# Histogram Equalization","0fac41d4":"* Enhance only train images (not for val images)","de6f456c":"# References\n\n1. https:\/\/www.kaggle.com\/dschettler8845\/visual-in-depth-eda-vinbigdata-competition-data\n\n2. https:\/\/www.kaggle.com\/trungthanhnguyen0502\/eda-vinbigdata-chest-x-ray-abnormalities\n\n3. https:\/\/www.kaggle.com\/bhallaakshit\/dicom-wrangling-and-enhancement\n\n4. https:\/\/www.kaggle.com\/awsaf49\/vinbigdata-cxr-ad-yolov5-14-class-train\n\nThanks for above great works!","eb34ff4f":"# Inference","edb894cd":"# CLAHE (Contrast Limited Adaptive Histogram Equalization)\n","2fa439f6":"Train : 3515, val : 879","8917be7c":"# filters\n1) 3x3 High pass filter","74e95b0b":"# Invert","f93ae0b8":"It looks best.","5257e858":"# Origninal images","427aba39":"# Save Enhancemented Images with .png format","1753f975":"It seems worse than originals..\n* noise\n* appear wave patterns","50a90e0d":"Save train images after clahe","9424e6d1":"Data split"}}