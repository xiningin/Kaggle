{"cell_type":{"e5e9ad05":"code","665b852a":"code","368f8807":"code","9b3f6791":"code","0f77800c":"code","dab9be52":"code","8822644e":"code","a0c9df1f":"code","3fada4a6":"code","064dc170":"code","63088cb1":"code","38e2184f":"code","ecc5aa8c":"code","1b55adf3":"code","d7bc1fb8":"code","5f3e8492":"code","399781a5":"code","5bfc235e":"code","ce1492b8":"code","0d68c814":"code","db1cddca":"code","0b0fb336":"code","991903da":"code","a16a7fb4":"code","582197cb":"code","0a2a795f":"code","a2bc1f06":"code","bdb4a99f":"code","7af5e971":"code","285da415":"code","70bada41":"markdown","4c7dc7f0":"markdown","25797421":"markdown","28296241":"markdown","5e483b30":"markdown","eafda11f":"markdown","e6fae770":"markdown","c00c4f67":"markdown","c42ae4c7":"markdown","86461f34":"markdown","bd7df3e2":"markdown","60a1d71c":"markdown","8d1020ae":"markdown","7c02c4c9":"markdown","49717066":"markdown","64c47f6c":"markdown"},"source":{"e5e9ad05":"!pip install --upgrade transformers\n!pip install simpletransformers","665b852a":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport os\nimport re\nimport string\nimport tqdm\nimport nltk\nnltk.download('stopwords')\nnltk.download('wordnet')\nfrom wordcloud import WordCloud\nfrom wordcloud import STOPWORDS\nfrom nltk.corpus import stopwords\nfrom tqdm.notebook import tqdm\nimport tensorflow as tf\nfrom tensorflow.keras.layers import Dense, Input\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.callbacks import ModelCheckpoint\nimport tensorflow_hub as hub\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder\n\nimport gc\nfrom tqdm.autonotebook import tqdm\n\nfrom sklearn.metrics import accuracy_score,f1_score\n\nimport sklearn\n\nimport torch\nfrom simpletransformers.classification import ClassificationModel\n","368f8807":"train_data = pd.read_csv(\"..\/input\/tweets-with-sarcasm-and-irony\/train.csv\")\ntest_data = pd.read_csv(\"..\/input\/tweets-with-sarcasm-and-irony\/test.csv\")","9b3f6791":"train_tweets=train_data['tweets'].tolist()\ntest_tweets=test_data['tweets'].tolist()","0f77800c":"def keep_uniques(array, df):\n    dels=[]\n    for i in array:\n        if array.count(i)>1:\n            dels.append(i)\n    dels=list(set(dels))\n    for i in dels:\n        df.drop( df[ df['tweets'] == i ].index, inplace=True)\n    return df","dab9be52":"train_data=keep_uniques(train_tweets, train_data)\ntest_data=keep_uniques(test_tweets, test_data)","8822644e":"len(train_data['tweets'].unique())","a0c9df1f":"len(test_data['tweets'].unique())","3fada4a6":"train_data.describe().T","064dc170":"train_data = train_data.sample(frac = 1)\ntest_data = test_data.sample(frac = 1)","63088cb1":"train_data['class'].value_counts()","38e2184f":"temp=train_data.loc[train_data['class'] == 'regular']\nlis=temp['tweets'].tolist()\n\nimport random\nreg_del=[]\nvisited=set()\nfor _ in range(3600):\n    n=random.randint(0,18556)\n    if n not in visited:\n        reg_del.append(lis[n])\n        \n        \nfor i in reg_del:\n    train_data.drop( train_data[ train_data['tweets'] == i ].index, inplace=True)","ecc5aa8c":"train_data['class'].value_counts()","1b55adf3":"test_data['class'].value_counts()","d7bc1fb8":"test_data = test_data.dropna()","5f3e8492":"def clean(tweet): \n    \n\n    # Special characters\n    tweet = re.sub(r\"\\x89\u00db_\", \"\", tweet)\n    tweet = re.sub(r\"\\x89\u00db\u00d2\", \"\", tweet)\n    tweet = re.sub(r\"\\x89\u00db\u00d3\", \"\", tweet)\n    tweet = re.sub(r\"\\x89\u00db\u00cfWhen\", \"When\", tweet)\n    tweet = re.sub(r\"\\x89\u00db\u00cf\", \"\", tweet)\n    tweet = re.sub(r\"China\\x89\u00db\u00aas\", \"China's\", tweet)\n    tweet = re.sub(r\"let\\x89\u00db\u00aas\", \"let's\", tweet)\n    tweet = re.sub(r\"\\x89\u00db\u00f7\", \"\", tweet)\n    tweet = re.sub(r\"\\x89\u00db\u00aa\", \"\", tweet)\n    tweet = re.sub(r\"\\x89\u00db\\x9d\", \"\", tweet)\n    tweet = re.sub(r\"\u00e5_\", \"\", tweet)\n    tweet = re.sub(r\"\\x89\u00db\u00a2\", \"\", tweet)\n    tweet = re.sub(r\"\\x89\u00db\u00a2\u00e5\u00ca\", \"\", tweet)\n    tweet = re.sub(r\"from\u00e5\u00cawounds\", \"from wounds\", tweet)\n    tweet = re.sub(r\"\u00e5\u00ca\", \"\", tweet)\n    tweet = re.sub(r\"\u00e5\u00c8\", \"\", tweet)\n    tweet = re.sub(r\"Jap\u00cc_n\", \"Japan\", tweet)    \n    tweet = re.sub(r\"\u00cc\u00a9\", \"e\", tweet)\n    tweet = re.sub(r\"\u00e5\u00a8\", \"\", tweet)\n    tweet = re.sub(r\"Suru\u00cc\u00a4\", \"Suruc\", tweet)\n    tweet = re.sub(r\"\u00e5\u00c7\", \"\", tweet)\n    tweet = re.sub(r\"\u00e5\u00a33million\", \"3 million\", tweet)\n    tweet = re.sub(r\"\u00e5\u00c0\", \"\", tweet)\n    \n    #emojis\n    emoji_pattern = re.compile(\n        '['\n        u'\\U0001F600-\\U0001F64F'  # emoticons\n        u'\\U0001F300-\\U0001F5FF'  # symbols & pictographs\n        u'\\U0001F680-\\U0001F6FF'  # transport & map symbols\n        u'\\U0001F1E0-\\U0001F1FF'  # flags\n        u'\\U00002702-\\U000027B0'\n        u'\\U000024C2-\\U0001F251'\n        ']+',\n        flags=re.UNICODE)\n    tweet =  emoji_pattern.sub(r'', tweet)\n    \n    # usernames mentions like \"@abc123\"\n    ment = re.compile(r\"(@[A-Za-z0-9]+)\")\n    tweet =  ment.sub(r'', tweet)\n    \n    # Contractions\n    tweet = re.sub(r\"he's\", \"he is\", tweet)\n    tweet = re.sub(r\"there's\", \"there is\", tweet)\n    tweet = re.sub(r\"We're\", \"We are\", tweet)\n    tweet = re.sub(r\"That's\", \"That is\", tweet)\n    tweet = re.sub(r\"won't\", \"will not\", tweet)\n    tweet = re.sub(r\"they're\", \"they are\", tweet)\n    tweet = re.sub(r\"Can't\", \"Cannot\", tweet)\n    tweet = re.sub(r\"wasn't\", \"was not\", tweet)\n    tweet = re.sub(r\"don\\x89\u00db\u00aat\", \"do not\", tweet)\n    tweet = re.sub(r\"aren't\", \"are not\", tweet)\n    tweet = re.sub(r\"isn't\", \"is not\", tweet)\n    tweet = re.sub(r\"What's\", \"What is\", tweet)\n    tweet = re.sub(r\"haven't\", \"have not\", tweet)\n    tweet = re.sub(r\"hasn't\", \"has not\", tweet)\n    tweet = re.sub(r\"There's\", \"There is\", tweet)\n    tweet = re.sub(r\"He's\", \"He is\", tweet)\n    tweet = re.sub(r\"It's\", \"It is\", tweet)\n    tweet = re.sub(r\"You're\", \"You are\", tweet)\n    tweet = re.sub(r\"I'M\", \"I am\", tweet)\n    tweet = re.sub(r\"shouldn't\", \"should not\", tweet)\n    tweet = re.sub(r\"wouldn't\", \"would not\", tweet)\n    tweet = re.sub(r\"i'm\", \"I am\", tweet)\n    tweet = re.sub(r\"I\\x89\u00db\u00aam\", \"I am\", tweet)\n    tweet = re.sub(r\"I'm\", \"I am\", tweet)\n    tweet = re.sub(r\"Isn't\", \"is not\", tweet)\n    tweet = re.sub(r\"Here's\", \"Here is\", tweet)\n    tweet = re.sub(r\"you've\", \"you have\", tweet)\n    tweet = re.sub(r\"you\\x89\u00db\u00aave\", \"you have\", tweet)\n    tweet = re.sub(r\"we're\", \"we are\", tweet)\n    tweet = re.sub(r\"what's\", \"what is\", tweet)\n    tweet = re.sub(r\"couldn't\", \"could not\", tweet)\n    tweet = re.sub(r\"we've\", \"we have\", tweet)\n    tweet = re.sub(r\"it\\x89\u00db\u00aas\", \"it is\", tweet)\n    tweet = re.sub(r\"doesn\\x89\u00db\u00aat\", \"does not\", tweet)\n    tweet = re.sub(r\"It\\x89\u00db\u00aas\", \"It is\", tweet)\n    tweet = re.sub(r\"Here\\x89\u00db\u00aas\", \"Here is\", tweet)\n    tweet = re.sub(r\"who's\", \"who is\", tweet)\n    tweet = re.sub(r\"I\\x89\u00db\u00aave\", \"I have\", tweet)\n    tweet = re.sub(r\"y'all\", \"you all\", tweet)\n    tweet = re.sub(r\"can\\x89\u00db\u00aat\", \"cannot\", tweet)\n    tweet = re.sub(r\"would've\", \"would have\", tweet)\n    tweet = re.sub(r\"it'll\", \"it will\", tweet)\n    tweet = re.sub(r\"we'll\", \"we will\", tweet)\n    tweet = re.sub(r\"wouldn\\x89\u00db\u00aat\", \"would not\", tweet)\n    tweet = re.sub(r\"We've\", \"We have\", tweet)\n    tweet = re.sub(r\"he'll\", \"he will\", tweet)\n    tweet = re.sub(r\"Y'all\", \"You all\", tweet)\n    tweet = re.sub(r\"Weren't\", \"Were not\", tweet)\n    tweet = re.sub(r\"Didn't\", \"Did not\", tweet)\n    tweet = re.sub(r\"they'll\", \"they will\", tweet)\n    tweet = re.sub(r\"they'd\", \"they would\", tweet)\n    tweet = re.sub(r\"DON'T\", \"DO NOT\", tweet)\n    tweet = re.sub(r\"That\\x89\u00db\u00aas\", \"That is\", tweet)\n    tweet = re.sub(r\"they've\", \"they have\", tweet)\n    tweet = re.sub(r\"i'd\", \"I would\", tweet)\n    tweet = re.sub(r\"should've\", \"should have\", tweet)\n    tweet = re.sub(r\"You\\x89\u00db\u00aare\", \"You are\", tweet)\n    tweet = re.sub(r\"where's\", \"where is\", tweet)\n    tweet = re.sub(r\"Don\\x89\u00db\u00aat\", \"Do not\", tweet)\n    tweet = re.sub(r\"we'd\", \"we would\", tweet)\n    tweet = re.sub(r\"i'll\", \"I will\", tweet)\n    tweet = re.sub(r\"weren't\", \"were not\", tweet)\n    tweet = re.sub(r\"They're\", \"They are\", tweet)\n    tweet = re.sub(r\"Can\\x89\u00db\u00aat\", \"Cannot\", tweet)\n    tweet = re.sub(r\"you\\x89\u00db\u00aall\", \"you will\", tweet)\n    tweet = re.sub(r\"I\\x89\u00db\u00aad\", \"I would\", tweet)\n    tweet = re.sub(r\"let's\", \"let us\", tweet)\n    tweet = re.sub(r\"it's\", \"it is\", tweet)\n    tweet = re.sub(r\"can't\", \"cannot\", tweet)\n    tweet = re.sub(r\"don't\", \"do not\", tweet)\n    tweet = re.sub(r\"you're\", \"you are\", tweet)\n    tweet = re.sub(r\"i've\", \"I have\", tweet)\n    tweet = re.sub(r\"that's\", \"that is\", tweet)\n    tweet = re.sub(r\"i'll\", \"I will\", tweet)\n    tweet = re.sub(r\"doesn't\", \"does not\", tweet)\n    tweet = re.sub(r\"i'd\", \"I would\", tweet)\n    tweet = re.sub(r\"didn't\", \"did not\", tweet)\n    tweet = re.sub(r\"ain't\", \"am not\", tweet)\n    tweet = re.sub(r\"you'll\", \"you will\", tweet)\n    tweet = re.sub(r\"I've\", \"I have\", tweet)\n    tweet = re.sub(r\"Don't\", \"do not\", tweet)\n    tweet = re.sub(r\"I'll\", \"I will\", tweet)\n    tweet = re.sub(r\"I'd\", \"I would\", tweet)\n    tweet = re.sub(r\"Let's\", \"Let us\", tweet)\n    tweet = re.sub(r\"you'd\", \"You would\", tweet)\n    tweet = re.sub(r\"It's\", \"It is\", tweet)\n    tweet = re.sub(r\"Ain't\", \"am not\", tweet)\n    tweet = re.sub(r\"Haven't\", \"Have not\", tweet)\n    tweet = re.sub(r\"Could've\", \"Could have\", tweet)\n    tweet = re.sub(r\"youve\", \"you have\", tweet)  \n    tweet = re.sub(r\"don\u00e5\u00abt\", \"do not\", tweet)   \n            \n    # Character entity references\n    tweet = re.sub(r\"&amp;\", \"&\", tweet)\n    \n    # html tags\n    html = re.compile(r'<.*?>|&([a-z0-9]+|#[0-9]{1,6}|#x[0-9a-f]{1,6});')\n    tweet = re.sub(html, '', tweet)\n    \n    # Urls\n    tweet = re.sub(r\"https?:\\\/\\\/t.co\\\/[A-Za-z0-9]+\", \"\", tweet)\n    tweet = re.sub(r'https?:\/\/\\S+|www\\.\\S+','', tweet)\n        \n    #Punctuations and special characters\n    \n    tweet = re.sub('[%s]' % re.escape(string.punctuation),'',tweet)\n    \n    tweet = tweet.lower()\n    \n    splits = tweet.split()\n    splits = [word for word in splits if word not in set(nltk.corpus.stopwords.words('english'))]\n    tweet = ' '.join(splits)\n    \n    \n    return tweet","399781a5":"tqdm.pandas() \n\ntrain_data['cleaned_text']= train_data['tweets'].progress_apply((lambda x: clean(x))) \ntest_data['cleaned_text'] = test_data['tweets'].progress_apply((lambda x: clean(x)))","5bfc235e":"train_data.head()","ce1492b8":"sns.set(rc={'figure.figsize':(10,10)})\nsns.countplot(train_data['class'])","0d68c814":"from wordcloud import WordCloud\nstopwords = nltk.corpus.stopwords.words('english')\n\nplt.figure(figsize=(12,6))\ntext = ' '.join(train_data.cleaned_text[train_data['class']=='regular'])\nwc = WordCloud(background_color='white',stopwords=stopwords).generate(text)\nplt.imshow(wc)\nplt.axis('off')\nplt.title('Regular Tweets',fontsize=25)\n\nplt.figure(figsize=(12,6))\ntext = ' '.join(train_data.cleaned_text[train_data['class']=='irony'])\nwc1 = WordCloud(background_color='white',stopwords=stopwords).generate(text)\nplt.imshow(wc1)\nplt.axis('off')\nplt.title('Irony Tweets',fontsize=25)\n\nplt.figure(figsize=(12,6))\ntext = ' '.join(train_data.cleaned_text[train_data['class']=='sarcasm'])\nwc2 = WordCloud(background_color='white',stopwords=stopwords).generate(text)\nplt.imshow(wc2)\nplt.axis('off')\nplt.title('Sarcasm Tweets',fontsize=25)\n\nplt.figure(figsize=(12,6))\ntext = ' '.join(train_data.cleaned_text[train_data['class']=='figurative'])\nwc3 = WordCloud(background_color='white',stopwords=stopwords).generate(text)\nplt.imshow(wc3)\nplt.axis('off')\nplt.title('Figurative Tweets',fontsize=25)","db1cddca":"def encode_target(t_class):\n    t_class=str(t_class)\n    class_dict = {\n        'irony':0,\n        'sarcasm':1,\n        'regular':2,\n        'figurative':3\n    }\n    return class_dict[t_class]","0b0fb336":"train_data[\"target\"] = train_data['class'].apply(lambda x: encode_target(x))\ntest_data[\"target\"] = test_data['class'].apply(lambda x: encode_target(x))","991903da":"train = train_data[['cleaned_text','target']]\ntrain.columns = ['text','labels']\n\ntest = test_data[['cleaned_text','target']]\ntest.columns = ['text','labels']\n","a16a7fb4":"train.head()","582197cb":"test.head()","0a2a795f":"\nmodel_type = 'distilbert'\nmodel_name = 'distilbert-base-uncased'\nseed = 100\nmodel_args =  {'fp16': False,\n               'train_batch_size': 128,\n               'gradient_accumulation_steps': 2,\n#                'do_lower_case': True,\n               'learning_rate': 1e-5,\n               'overwrite_output_dir': True,\n               'manual_seed': seed,\n               'num_train_epochs': 4}","a2bc1f06":"model = ClassificationModel(model_type, model_name,num_labels=4, args=model_args) ","bdb4a99f":"model.train_model(train,acc=accuracy_score)","7af5e971":"result, model_outputs, wrong_predictions = model.eval_model(test,acc=accuracy_score)","285da415":"print(\"TEST SET EVALUATION:\")\nprint(\"====================================\")\nprint(\"%s: %.2f%%\" % ('Accuracy', result['acc']*100))\nprint(\"%s: %.5f\" % ('Final Loss', result['eval_loss']))","70bada41":"## Data Cleaning & Preprocessing","4c7dc7f0":"### Import necessary packages","25797421":"### Training model","28296241":"### Encode our text classes","5e483b30":"## EDA and Visualization","eafda11f":"## Building the model\n\n### Setting up the model arguments","e6fae770":"### Loading data","c00c4f67":"### Please upvote if this helped :)","c42ae4c7":"### Install simpletransformers","86461f34":"# Using Keras and distilBERT to classify tweets","bd7df3e2":"Here, we see that the `regular` class has 18k tweets, which causes our dataset to be imbalanced. So we shall delete some tweets from this class","60a1d71c":"### Exploring Dataset","8d1020ae":"### Evaluate on the test set","7c02c4c9":"### Remove recurring tweets to prevent ambiguity","49717066":"### Defining model","64c47f6c":"### Preparing our train and test sets"}}