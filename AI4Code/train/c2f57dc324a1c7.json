{"cell_type":{"e5f77963":"code","66897603":"code","145a9dc4":"code","cfff7b02":"code","3668d9ee":"code","88f48b2d":"code","ac1dfbcd":"code","ccc2bc4b":"code","ab6fdcdf":"code","f20267a4":"code","cabc4329":"code","a27e274a":"code","a531ef42":"code","3c1309fb":"code","ca6b268e":"code","cf3b23a1":"code","ffe87dd8":"code","9b956eeb":"code","0ed4e348":"code","606ae738":"code","b41dc725":"code","908c40b4":"code","0b4d5e73":"code","fe488d9b":"code","11775985":"code","1e0e0069":"code","d4ac7f66":"code","9269dd29":"code","c86a0ad5":"code","f0b994a7":"code","fcad7ce9":"code","8e1f0307":"code","02daffa7":"code","ac2caafb":"code","59912821":"code","d9db76e4":"code","4111b791":"code","dbbbd5b2":"code","1b583471":"code","d5f49232":"code","0760e2a0":"code","a95e5862":"code","e5e46822":"code","0b5ffd22":"code","4212348d":"code","b824c1c7":"code","7da70bed":"code","5627653b":"code","b4be62d8":"code","6dce680e":"code","a12679e9":"code","5f810d8e":"code","7dc5d7d7":"code","13e3188b":"code","9cf5472e":"code","8973cf1b":"code","3a0c61b3":"code","34e69be1":"code","fc3d49dc":"code","f9f79e2b":"code","6ed02256":"code","c8662ffa":"code","6a579764":"code","3fc363c2":"code","4fd197ac":"code","83ede75a":"code","cd20a253":"code","96b3a561":"code","971ce4f4":"code","ea2c2c1b":"code","d346659f":"code","2d7cc40c":"code","d98aadf9":"code","8aa4bdcd":"code","64cb497e":"code","0ff4a281":"code","eb2966a0":"code","49a68ce3":"code","35df1506":"code","a37a23e1":"code","64bd2d7f":"code","fdc91077":"code","d649dd74":"code","f8797169":"code","db835fa0":"code","857ed955":"code","fa288ef3":"code","b2dde534":"code","01f30acf":"code","f8cbd7df":"code","94d8db27":"code","731c785f":"code","78533f31":"code","b39111fe":"code","e9a8dc68":"code","98883060":"code","c4757e85":"code","3c524e01":"code","e4ffc505":"code","f49a5120":"code","9cd3409a":"code","ade11e5d":"code","3e45fdfd":"code","941fda6a":"code","5ade5581":"code","f3686c8d":"code","afc72557":"code","b221ce6a":"code","89e8427b":"code","9c211368":"code","19ac314e":"code","e4b8b22e":"code","5ace9b1a":"code","1e67cad2":"code","9264d7b6":"code","36ce7559":"code","de3ebdd3":"code","5aae83b3":"code","569cae0f":"code","2c8a99a5":"code","6a39e288":"code","de9b88fb":"code","d23702f8":"code","2528ffb8":"code","441f1215":"code","6150971c":"code","0eba083a":"code","da47c97d":"code","0ec7535c":"code","02a4f49f":"code","786e4ab3":"code","824cffad":"code","d744a2eb":"code","5ef6646b":"code","c73bab2a":"code","c79c0cad":"code","cf997779":"code","faea8b2f":"code","dcd0e7c3":"code","a4c828c9":"code","3db79cf8":"code","efd89f46":"code","bec22bc4":"code","8a8474ad":"code","0ae169d1":"code","3337da56":"code","f93cd38f":"code","dd15f5ee":"code","110eb582":"code","d90bff04":"code","5aa6f67e":"code","65b94aca":"code","36884bae":"code","c1e5c57f":"code","029f270b":"code","6cd27570":"code","461ea65e":"code","1aaa472c":"code","a71e57e9":"code","626dee87":"code","06de0cac":"code","1bfe7ecc":"markdown","0d5ec0ed":"markdown","0835d88a":"markdown","043082a7":"markdown","49f81cc6":"markdown","5da1ff26":"markdown","d9b3a579":"markdown","18b1fe95":"markdown","604ba690":"markdown","783f8dc7":"markdown","34c26639":"markdown","e2f8bec8":"markdown","425963c2":"markdown","5b95e606":"markdown","fe0765a7":"markdown","6c66d521":"markdown","91cf6dc8":"markdown","1a21c9b9":"markdown","f437a995":"markdown","aecf442c":"markdown","3616d988":"markdown","d4f6d8b8":"markdown","eb7ef7dc":"markdown","26c491b2":"markdown"},"source":{"e5f77963":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n%matplotlib inline\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport numpy as np\nfrom sklearn import preprocessing\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.svm import SVR\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.model_selection import RandomizedSearchCV\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","66897603":"train=pd.read_csv(\"\/kaggle\/input\/house-prices-advanced-regression-techniques\/train.csv\")\n","145a9dc4":"fffff=pd.read_csv('\/kaggle\/input\/house-prices-advanced-regression-techniques\/sample_submission.csv')","cfff7b02":"test=pd.read_csv(\"\/kaggle\/input\/house-prices-advanced-regression-techniques\/test.csv\")","3668d9ee":"ty=test[\"LotFrontage\"]","88f48b2d":"ty=ty.to_dict()","ac1dfbcd":"for i,j in ty.items():\n    test[\"LotFrontage\"].iloc[i]=j","ccc2bc4b":"train.head()","ab6fdcdf":"train.info()","f20267a4":"a=[]\ndef missvalue(t):\n    for i in t.columns:\n        if(t[i].isnull().sum()>0):\n              a.append(i)\n            ","cabc4329":"missvalue(train)\na","a27e274a":"a=[]\nmissvalue(test)\na","a531ef42":"a=[]\nmissvalue(train)\n","3c1309fb":"num=[]\ncat=[]\ndef separate(train,p):\n    for i in p:\n        if(train[i].dtypes=='O'):\n            cat.append(i)\n        else:\n            num.append(i)\n    ","ca6b268e":"separate(train,a)","cf3b23a1":"cat","ffe87dd8":"num","9b956eeb":"num2=[]\nnum1=[]\ndef cor(k):\n    corr=train.corr()\n    for l in k:\n        m=0\n        t=corr[l].sort_values(ascending=False)\n        t=t.to_dict()\n        for i,j in t.items():\n            if(j>0.5):\n                print(i,j)\n                m=m+1\n        if(m>1):\n            num1.append(l)\n        else:\n            num2.append(l)\n                \n           \n        print(\"*\"*40)","0ed4e348":"cor(num)","606ae738":"num1","b41dc725":"num2","908c40b4":"def fig(k,c):\n     for i in k:\n        sns.boxplot(i,data=train)\n        plt.show()\n        \n\n","0b4d5e73":"fig(num2,train)","fe488d9b":"plt.scatter(train['GarageYrBlt'],train['YearBuilt'])","11775985":"def discrete(a,b,c):\n    t=b[a].isnull()\n    t=t.to_dict()\n    k=[]\n    for i,j in t.items():\n        if j==True:\n            k.append(i)\n    p=pd.read_csv(\"\/kaggle\/input\/house-prices-advanced-regression-techniques\/train.csv\")\n    m=pd.DataFrame(p[c].loc[k])\n    p.drop(k,inplace=True)\n    u=p[c].mode()\n    p.fillna(u,inplace=True)\n    t1=p[c].values\n    t2=p[a].values\n    t1=t1.reshape(-1,1)\n    from sklearn.linear_model import LinearRegression\n    lin_reg=LinearRegression()\n    lin_reg.fit(t1,t2)\n    l=m.values\n    y=lin_reg.predict(l)\n    y = [round(x) for x in y]\n    q=0\n    for i,j in t.items():\n        if j==True:\n               b[a].iloc[i]=y[q]\n               q=q+1  ","1e0e0069":"discrete(\"GarageYrBlt\",train,\"YearBuilt\")","d4ac7f66":"train[\"GarageYrBlt\"].isnull().sum()","9269dd29":"train['LotFrontage'].fillna(69,inplace=True)\ntrain['LotFrontage'].isnull().sum()","c86a0ad5":"train['MasVnrArea'].mean()","f0b994a7":"train['MasVnrArea'].fillna(0,inplace=True)","fcad7ce9":"cat","8e1f0307":"for i in cat:\n    sns.countplot(i,data=train)\n    plt.show()\n    t=train[i].isnull().sum()\n    print(t)","02daffa7":"train.drop(['MiscFeature','Fence','PoolQC','Alley'],axis=1,inplace=True)","ac2caafb":"k=['MiscFeature','Fence','PoolQC','Alley']\n\ncat = [element for element in cat if element not in k]","59912821":"cat","d9db76e4":"len(train.columns)","4111b791":"def categorical(ca,tttt):\n    for al in ca:\n        t=tttt[al].isnull()\n        t=t.to_dict()  \n        k=[]\n        for i,j in t.items():\n            if j==True:\n                 k.append(i)\n        p=pd.read_csv(\"\/kaggle\/input\/house-prices-advanced-regression-techniques\/train.csv\")\n        p.drop(['MiscFeature','Fence','PoolQC','Alley'],axis=1,inplace=True)\n        z=pd.DataFrame(p.iloc[k])\n        z.drop([al],axis=1,inplace=True)\n        p.drop(k,inplace=True)\n        q=p[al].value_counts()\n        q=q.to_dict()\n        m=1\n        for i,j in q.items():\n            q[i]=m\n            m=m+1\n        p.replace({al:q},inplace=True)\n        t1=p[al].values\n        p.drop([al],axis=1,inplace=True)\n        p.fillna(0,inplace=True)\n        def split(a):\n            num=a.select_dtypes(include=[np.number]) \n            cat=a.select_dtypes(exclude=[np.number])\n            cat=pd.get_dummies(cat)\n            return num,cat\n         \n        x,y=split(p)\n        scaler = preprocessing.StandardScaler()\n        def scaling(x,y):\n             features_scaled=scaler.fit_transform(x.values)\n             q=y.values\n             vk=np.concatenate((q,features_scaled),axis=1)\n             return vk\n        features=scaling(x,y)\n        (x_train,x_test,y_train,y_test) = train_test_split(features,t1, train_size=0.75, random_state=42)\n        rnd_clf = RandomForestClassifier(n_estimators=500, max_leaf_nodes=100, random_state=42)\n        rnd_clf.fit(x_train,y_train)\n        z.fillna(0,inplace=True)\n        r,s=split(z)\n        for w in y.columns:\n            for o in s.columns:\n                if(w==o):\n                    c=1\n                    break\n                else:\n                    c=2\n            if(c==2):\n                s[w]=0\n        for o in s.columns:\n                for w in y.columns:\n                    if(o==w):\n                        c=1\n                        break\n                    else:\n                        c=2\n                if(c==2):\n                    s.drop(o,axis=1,inplace=True)\n     \n        for oe in r.columns:\n            for we in x.columns:\n                if(we==oe):\n                    c=1\n                    break\n                else:\n                    c=2\n            if(c==2):\n                r.drop(oe,axis=1,inplace=True)\n        for we in x.columns:\n            for oe in r.columns:\n                if(we==oe):\n                    c=1\n                    break\n                else:\n                    c=2\n            if(c==2):\n                r[we]=0\n            \n                    \n       \n        features=scaling(r,s)\n        y_pred=rnd_clf.predict(features)\n        h=[]\n        for l in range(len(y_pred)):\n                u=[i for i,j in q.items() if j==y_pred[l]]\n                for tyyy in u:\n                    h.append(tyyy)\n        x=0\n        for i,j in t.items():\n                if j==True:\n                       tttt[al].iloc[i]=h[x]\n                       x=x+1  \n        ","dbbbd5b2":"categorical(cat,train)","1b583471":"len(train.columns)","d5f49232":"train.isnull()","0760e2a0":"test.info()","a95e5862":"a=[]\nmissvalue(test)\nnum=[]\ncat=[]\nseparate(test,a)\nnum2=[]\nnum1=[]\ncor(num)\n","e5e46822":"discrete('BsmtFinSF1',test,\"BsmtFullBath\")\n","0b5ffd22":"discrete('TotalBsmtSF',test,\"1stFlrSF\")\ndiscrete('BsmtFullBath',test,\"BsmtFinSF1\")","4212348d":"test[\"YearBuilt\"].mode()","b824c1c7":"\ndiscrete('GarageCars',test,\"GarageArea\")","7da70bed":"discrete('GarageArea',test,\"GarageCars\")","5627653b":"test[\"YearBuilt\"].isnull().sum()","b4be62d8":"test[\"GarageYrBlt\"].isnull().sum()","6dce680e":"train[\"GarageYrBlt\"].unique()","a12679e9":"train[\"YearBuilt\"].unique()","5f810d8e":"test[\"GarageYrBlt\"].fillna(2005,inplace=True)","7dc5d7d7":"fig(num2,train)","13e3188b":"test[\"LotFrontage\"]","9cf5472e":"test[\"LotFrontage\"].fillna(test[\"LotFrontage\"].median(),inplace=True)","8973cf1b":"test[\"MasVnrArea\"].fillna(test[\"MasVnrArea\"].median(),inplace=True)","3a0c61b3":"test[\"BsmtFinSF2\"].fillna(test[\"BsmtFinSF2\"].mode(),inplace=True)\ntest[\"BsmtUnfSF\"].fillna(test[\"BsmtUnfSF\"].median(),inplace=True)\n","34e69be1":"test[\"BsmtHalfBath\"].fillna(test[\"BsmtHalfBath\"].mode(),inplace=True)","fc3d49dc":"cat","f9f79e2b":"for i in cat:\n    sns.countplot(i,data=test)\n    plt.show()\n    t=test[i].isnull().sum()\n    print(t)","6ed02256":"test.drop(['MiscFeature','Fence','PoolQC','Alley'],axis=1,inplace=True)\n","c8662ffa":"k=['MiscFeature','Fence','PoolQC','Alley']\n\ncat = [element for element in cat if element not in k]","6a579764":"cat","3fc363c2":"len(test.columns)","4fd197ac":"test['MasVnrType'].value_counts()\n","83ede75a":"def categori(ca,tttt):\n    for al in ca:\n        t=tttt[al].isnull()\n        t=t.to_dict()  \n        k=[]\n        for i,j in t.items():\n            if j==True:\n                 k.append(i)\n        p=pd.read_csv(\"\/kaggle\/input\/house-prices-advanced-regression-techniques\/test.csv\")\n        p.drop(['MiscFeature','Fence','PoolQC','Alley'],axis=1,inplace=True)\n        z=pd.DataFrame(p.iloc[k])\n        z.drop([al],axis=1,inplace=True)\n        p.drop(k,inplace=True)\n        q=p[al].value_counts()\n        q=q.to_dict()\n        m=1\n        for i,j in q.items():\n            q[i]=m\n            m=m+1\n        p.replace({al:q},inplace=True)\n        t1=p[al].values\n        p.drop([al],axis=1,inplace=True)\n        p.fillna(0,inplace=True)\n        def split(a):\n            num=a.select_dtypes(include=[np.number]) \n            cat=a.select_dtypes(exclude=[np.number])\n            cat=pd.get_dummies(cat)\n            return num,cat\n         \n        x,y=split(p)\n        scaler = preprocessing.StandardScaler()\n        def scaling(x,y):\n             features_scaled=scaler.fit_transform(x.values)\n             q=y.values\n             vk=np.concatenate((q,features_scaled),axis=1)\n             return vk\n        features=scaling(x,y)\n        (x_train,x_test,y_train,y_test) = train_test_split(features,t1, train_size=0.75, random_state=42)\n        rnd_clf = RandomForestClassifier(n_estimators=500, max_leaf_nodes=100, random_state=42)\n        rnd_clf.fit(x_train,y_train)\n        z.fillna(0,inplace=True)\n        r,s=split(z)\n        for w in y.columns:\n            for o in s.columns:\n                if(w==o):\n                    c=1\n                    break\n                else:\n                    c=2\n            if(c==2):\n                s[w]=0\n        for o in s.columns:\n                for w in y.columns:\n                    if(o==w):\n                        c=1\n                        break\n                    else:\n                        c=2\n                if(c==2):\n                    s.drop(o,axis=1,inplace=True)\n     \n        for oe in r.columns:\n            for we in x.columns:\n                if(we==oe):\n                    c=1\n                    break\n                else:\n                    c=2\n            if(c==2):\n                r.drop(oe,axis=1,inplace=True)\n        for we in x.columns:\n            for oe in r.columns:\n                if(we==oe):\n                    c=1\n                    break\n                else:\n                    c=2\n            if(c==2):\n                r[we]=0\n            \n                    \n       \n        features=scaling(r,s)\n        y_pred=rnd_clf.predict(features)\n        h=[]\n        for l in range(len(y_pred)):\n                u=[i for i,j in q.items() if j==y_pred[l]]  \n                for tyyy in u:\n                    h.append(tyyy)\n        x=0\n        for i,j in t.items():\n                if j==True:\n                       tttt[al].iloc[i]=h[x]\n                       x=x+1  \n        ","cd20a253":"categori(cat,test)","96b3a561":"len(test.columns)","971ce4f4":"num=[]\ncat=[]\ndef sep(tv):\n    for i in tv.columns:\n        if(tv[i].dtypes=='O'):\n            cat.append(i)\n        else:\n            num.append(i)","ea2c2c1b":"sep(train)","d346659f":"cont=[]\ndisc=[]\ndef numsep():\n    for i in num:\n        t=train[i].value_counts()\n        qw=len(t)\n        if qw>20:\n            cont.append(i)\n        else:\n            disc.append(i)\n        ","2d7cc40c":"\nnumsep()","d98aadf9":"cont","8aa4bdcd":"disc","64cb497e":"for i in cont:\n      train[i].hist()\n      plt.xlabel(i)\n      plt.show()","0ff4a281":"sss=['LotFrontage','LotArea','1stFlrSF','GrLivArea']\nfor f in sss:\n    train[f]=np.log(train[f])","eb2966a0":"disc","49a68ce3":"cont","35df1506":"train['LotFrontage'].hist()","a37a23e1":"num=[]\ncat=[]\nsep(test)\ncont=[]\ndisc=[]\nnumsep()","64bd2d7f":"for i in cont:\n    test[i].hist()\n    plt.xlabel(i)\n    plt.show()","fdc91077":"\nss=['LotFrontage','LotArea','1stFlrSF','GrLivArea']\nfor q in ss:\n    test[q]=np.log(test[q])","d649dd74":"pp=[]\ncorr=train.corr()\nt=corr[\"SalePrice\"].sort_values(ascending=False)\nt=t.to_dict()\nfor i,j in t.items():\n    if j>0.5 or (0.2>j and j>-0.2):\n        print(i,j)\n        print(\"*\"*40)\n        if 0.2>j and j>-0.2:\n            pp.append(i)\n","f8797169":"pp","db835fa0":"num=[]\nsep(train)","857ed955":"for i in num:\n    for j in num:\n        train[\"new\"]=train[j]\/train[i]\n        corr=train.corr()\n        t=corr[\"new\"].sort_values(ascending=False)\n        t=t.to_dict()\n        for r,s in t.items():\n            if (s>0.6 or s<-0.6) and r=='SalePrice':\n                print(i,j)\n                print(s)\n                print(\"*\"*40)\n\n                \n       \n\n        \n","fa288ef3":"train.drop(['new'],axis=1,inplace=True)","b2dde534":"def add(w):\n    w[\"LotFrontage OverallQual\"]=w[\"OverallQual\"]\/w[\"LotFrontage\"]\n    w[\"LotArea OverallQual\"]=w[\"OverallQual\"]\/w[\"LotArea\"]\n    w[\"OverallCond OverallQual\"]=w[\"OverallQual\"]\/w[\"OverallCond\"]\n    w[\"YearBuilt OverallQual\"]=w[\"OverallQual\"]\/w[\"YearBuilt\"]\n    w[\"YearBuilt TotalBsmtSF\"]=w[\"TotalBsmtSF\"]\/w[\"YearBuilt\"]\n    w[\"YearBuilt GarageCars\"]=w[\"GarageCars\"]\/w[\"YearBuilt\"]\n    w[\"YearBuilt GarageArea\"]=w[\"GarageArea\"]\/w[\"YearBuilt\"]\n    w[\"YearRemodAdd OverallQual\"]=w[\"OverallQual\"]\/w[\"YearRemodAdd\"]\n    w[\"YearRemodAdd TotalBsmtSF\"]=w[\"TotalBsmtSF\"]\/w[\"YearRemodAdd\"]\n    w[\"YearRemodAdd GrLivArea\"]=w[\"GrLivArea\"]\/w[\"YearRemodAdd\"]\n    w[\"YearRemodAdd GarageCars\"]=w[\"GarageCars\"]\/w[\"YearRemodAdd\"]\n    w[\"YearRemodAdd GarageArea\"]=w[\"GarageArea\"]\/w[\"YearRemodAdd\"]\n    w[\"1stFlrSF OverallQual\"]=w[\"OverallQual\"]\/w[\"1stFlrSF\"]\n    w[\"1stFlrSF TotalBsmtSF\"]=w[\"TotalBsmtSF\"]\/w[\"1stFlrSF\"]\n    w[\"LowQualFinSF ScreenPorch\"]=w[\"ScreenPorch\"]\/w[\"LowQualFinSF\"]\n    w[\"GrLivArea OverallQual\"]=w[\"OverallQual\"]\/w[\"GrLivArea\"]\n    w[\"GrLivArea YrSold\"]=w[\"YrSold\"]\/w[\"GrLivArea\"]\n  \n  \n    w[\"GarageYrBlt OverallQual\"]=w[\"OverallQual\"]\/w[\"GarageYrBlt\"]\n    w[\"GarageYrBlt TotalBsmtSF\"]=w[\"TotalBsmtSF\"]\/w[\"GarageYrBlt\"]\n    w[\"GarageYrBlt GarageCars\"]=w[\"GarageCars\"]\/w[\"GarageYrBlt\"]\n    w[\"GarageYrBlt GarageArea\"]=w[\"GarageArea\"]\/w[\"GarageYrBlt\"]\n  \n    w[\"YrSold OverallQual\"]=w[\"OverallQual\"]\/w[\"YrSold\"]\n    w[\"YrSold TotalBsmtSF\"]=w[\"TotalBsmtSF\"]\/w[\"YrSold\"]\n    w[\"YrSold GrLivArea\"]=w[\"GrLivArea\"]\/w[\"YrSold\"]\n    w[\"YrSold GarageArea\"]=w[\"GarageArea\"]\/w[\"YrSold\"]\n    w[\"GarageYrBlt TotalBsmtSF\"]=w[\"TotalBsmtSF\"]\/w[\"GarageYrBlt\"]\n    w[\"GarageYrBlt GarageCars\"]=w[\"GarageCars\"]\/w[\"GarageYrBlt\"] \n    ","01f30acf":"add(train)","f8cbd7df":"train.drop(pp,axis=1,inplace=True)","94d8db27":"add(test)","731c785f":"len(test.columns)","78533f31":"test.drop(pp,axis=1,inplace=True)","b39111fe":"len(test.columns)","e9a8dc68":"len(train.columns)","98883060":"label=train[\"SalePrice\"].values\nlabel","c4757e85":"train.drop([\"SalePrice\"],axis=1,inplace=True)","3c524e01":"trtr=[\"LowQualFinSF ScreenPorch\"]\ntrain.drop(trtr,axis=1,inplace=True)","e4ffc505":"num=[]\ncat=[]\nsep(train)","f49a5120":"def split(a):\n    num=a.select_dtypes(include=[np.number]) \n    cat=a.select_dtypes(exclude=[np.number])\n    cat=pd.get_dummies(cat)\n    return num,cat","9cd3409a":"x,y=split(train)","ade11e5d":"scaler=StandardScaler()\ndef scaling(x,y):\n     features_scaled=scaler.fit_transform(x.values)\n     q=y.values\n     vk=np.concatenate((q,features_scaled),axis=1)\n     return vk","3e45fdfd":"features=scaling(x,y)","941fda6a":"np.random.seed(1234)\n(x_train,x_test,y_train,y_test) = train_test_split(features,label, train_size=0.75, random_state=42)","5ade5581":"from sklearn.linear_model import LinearRegression\n\nlin_reg = LinearRegression()\nlin_reg.fit(x_train, y_train)","f3686c8d":"scores = cross_val_score(lin_reg, x_test,y_test,\n                         scoring=\"neg_mean_squared_error\", cv=10)\ntree_rmse_scores = np.sqrt(-scores)\ntree_rmse_scores.mean()","afc72557":"from sklearn.tree import DecisionTreeRegressor\ntree_reg = DecisionTreeRegressor(random_state=42)\ntree_reg.fit(x_train,y_train)","b221ce6a":"y_pred = tree_reg.predict(x_test)\nlin_mse = mean_squared_error(y_test,y_pred)\nlin_rmse = np.sqrt(lin_mse)\nlin_rmse","89e8427b":"scores = cross_val_score(tree_reg, x_test,y_test,\n                         scoring=\"neg_mean_squared_error\", cv=10)\ntree_rmse_scores = np.sqrt(-scores)","9c211368":"tree_rmse_scores.mean()","19ac314e":"from sklearn.ensemble import RandomForestRegressor\nforest_reg = RandomForestRegressor(n_estimators=100, random_state=42)\nforest_reg.fit(x_train,y_train)","e4b8b22e":"y_pred = forest_reg.predict(x_test)\nlin_mse = mean_squared_error(y_test,y_pred)\nlin_rmse = np.sqrt(lin_mse)\nlin_rmse","5ace9b1a":"from sklearn.svm import SVR\nsvm_reg = SVR(kernel=\"linear\")\nsvm_reg.fit(x_train, y_train)\ny_pred = svm_reg.predict(x_test)\nsvm_mse = mean_squared_error(y_test,y_pred)\nsvm_rmse = np.sqrt(svm_mse)\nsvm_rmse","1e67cad2":"from sklearn.model_selection import GridSearchCV\n\nparam_grid = [\n    # try 12 (3\u00d74) combinations of hyperparameters\n    {'n_estimators': [3, 10, 30], 'max_features': [2, 4, 6, 8]},\n    # then try 6 (2\u00d73) combinations with bootstrap set as False\n    {'bootstrap': [False], 'n_estimators': [3, 10], 'max_features': [2, 3, 4]},\n  ]\n\nforest_reg = RandomForestRegressor(random_state=42)\n# train across 5 folds, that's a total of (12+6)*5=90 rounds of training \ngrid_search = GridSearchCV(forest_reg, param_grid, cv=5,\n                           scoring='neg_mean_squared_error',\n                           return_train_score=True)\ngrid_search.fit(x_train,y_train)","9264d7b6":"yt=grid_search.best_estimator_","36ce7559":"yt","de3ebdd3":"y_pred = yt.predict(x_test)\nlin_mse = mean_squared_error(y_test,y_pred)\nlin_rmse = np.sqrt(lin_mse)\nlin_rmse","5aae83b3":"k=[1,2,3,4,5,100]\nfor i in k:\n    svm=SVR(kernel=\"linear\",epsilon=i)\n    svm.fit(x_train,y_train)\n    y_pred = svm_reg.predict(x_test)\n    svm_mse = mean_squared_error(y_test,y_pred)\n    svm_rmse = np.sqrt(svm_mse)\n    print(svm_rmse,i)\n    print(\"*\"*40)","569cae0f":"\ndeg=[2,3,4,5,6]\nc=[1,3,10,15,20,100]\n\nfor j in deg:\n    for ci in c:\n            svm=SVR(kernel=\"rbf\",degree=j,C=ci,gamma=\"scale\")\n            svm.fit(x_train,y_train)\n            scores = cross_val_score(svm, x_test,y_test,\n                         scoring=\"neg_mean_squared_error\", cv=10)\n            rmse_scores = np.sqrt(-scores)\n            print(rmse_scores.mean(),j,ci)","2c8a99a5":"deg=[2,3,4,5,6]\nc=[1,3,10,15,20,100]\nepsilon=[1.5,2,3]\nfor j in deg:\n    for ci in c:\n        for e in epsilon:\n                svm=SVR(kernel=\"rbf\",degree=j,C=ci,epsilon=e,gamma=\"scale\")\n                svm.fit(x_train,y_train)\n                scores = cross_val_score(svm, x_test,y_test,\n                             scoring=\"neg_mean_squared_error\", cv=10)\n                rmse_scores = np.sqrt(-scores)\n                print(rmse_scores.mean(),j,ci,e)","6a39e288":"deg=[2,3,4,5,6]\nc=[1,3,10,15,20,100]\nfor j in deg:\n    for ci in c:\n                svm=SVR(kernel=\"rbf\",degree=j,C=ci,epsilon=0.1,gamma=\"scale\")\n                svm.fit(x_train,y_train)\n                scores = cross_val_score(svm, x_test,y_test,\n                             scoring=\"neg_mean_squared_error\", cv=10)\n                rmse_scores = np.sqrt(-scores)\n                print(rmse_scores.mean(),j,ci)","de9b88fb":"deg=[2,3,4,5,6]\nc=[1,3,10,15,20,100]\ngamma=[0.01, 0.03, 0.1]\nfor j in deg:\n    for ci in c:\n        for k in gamma:\n                svm=SVR(kernel=\"rbf\",degree=j,C=ci,gamma=k)\n                svm.fit(x_train,y_train)\n                scores = cross_val_score(svm, x_test,y_test,\n                             scoring=\"neg_mean_squared_error\", cv=10)\n                rmse_scores = np.sqrt(-scores)\n                print(rmse_scores.mean(),j,ci,k)","d23702f8":"deg=[2,3,4,5,6]\nc=[1,3,10,15,20,100]\ngamma=[0.01, 0.03, 0.1]\nfor j in deg:\n    for ci in c:\n        for k in gamma:\n                svm=SVR(kernel=\"linear\",degree=j,C=ci,gamma=k)\n                svm.fit(x_train,y_train)\n                scores = cross_val_score(svm, x_test,y_test,\n                             scoring=\"neg_mean_squared_error\", cv=10)\n                rmse_scores = np.sqrt(-scores)\n                print(rmse_scores.mean(),j,ci,k)","2528ffb8":"svm=SVR(kernel=\"linear\",C=100)\nsvm.fit(x_train,y_train)\nscores = cross_val_score(svm, x_test,y_test,\n                             scoring=\"neg_mean_squared_error\", cv=10)\nrmse_scores = np.sqrt(-scores)\nprint(rmse_scores.mean(),j,ci,k)","441f1215":"c=[100,200,300,400,500,600,700,800,900,1000,2000,3000,4000,5000,5500,5550]\nfor i in c:\n    svm=SVR(kernel=\"linear\",C=i)\n    svm.fit(x_train,y_train)\n    scores = cross_val_score(svm, x_test,y_test,\n                                 scoring=\"neg_mean_squared_error\", cv=10)\n    rmse_scores = np.sqrt(-scores)\n    print(rmse_scores.mean(),i)","6150971c":"tree_reg = DecisionTreeRegressor(max_depth=2,random_state=42)\ntree_reg.fit(x_train,y_train)\nscores = cross_val_score(tree_reg, x_test,y_test,\n                                 scoring=\"neg_mean_squared_error\", cv=10)\nrmse_scores = np.sqrt(-scores)\nprint(rmse_scores.mean())","0eba083a":"m=[1,2,3,4,5,6,7,8,9,100,150,200,250,300]\n\nfor i in m:\n    tree_reg = DecisionTreeRegressor(max_depth=i,random_state=42)\n    tree_reg.fit(x_train,y_train)\n    scores = cross_val_score(tree_reg, x_test,y_test,\n                                 scoring=\"neg_mean_squared_error\", cv=10)\n    rmse_scores = np.sqrt(-scores)\n    print(rmse_scores.mean(),i)","da47c97d":"m=[1,2,3,4,5,6,7,8,9,100,150,200,250,300]\nmi=[2,3,4]\nfor i in m:\n    for j in mi:\n        tree_reg = DecisionTreeRegressor(max_depth=i,min_samples_split=j,random_state=42)\n        tree_reg.fit(x_train,y_train)\n        scores = cross_val_score(tree_reg, x_test,y_test,\n                                     scoring=\"neg_mean_squared_error\", cv=10)\n        rmse_scores = np.sqrt(-scores)\n        print(rmse_scores.mean(),i,j)","0ec7535c":"mi=[2,3,4]\nfor j in mi:\n    for k in range(2,100):\n            tree_reg = DecisionTreeRegressor(min_samples_split=j,max_leaf_nodes=k,random_state=42)\n            tree_reg.fit(x_train,y_train)\n            scores = cross_val_score(tree_reg, x_test,y_test,\n                                         scoring=\"neg_mean_squared_error\", cv=10)\n            rmse_scores = np.sqrt(-scores)\n            print(rmse_scores.mean(),j,k)","02a4f49f":"mi=[2,3,4]\nmax=[1,2,3,4,5,6,7,8,9,10,11,12,13]\nfor m in max:\n    for j in mi:\n        for k in range(10,20):\n                tree_reg = DecisionTreeRegressor(min_samples_split=j,max_leaf_nodes=k,max_depth=m,random_state=42)\n                tree_reg.fit(x_train,y_train)\n                scores = cross_val_score(tree_reg, x_test,y_test,\n                                             scoring=\"neg_mean_squared_error\", cv=10)\n                rmse_scores = np.sqrt(-scores)\n                print(rmse_scores.mean(),j,k)","786e4ab3":"rnd_clf = RandomForestRegressor(n_estimators=500, max_leaf_nodes=16, n_jobs=-1,random_state=42)\nrnd_clf.fit(x_train, y_train)\n","824cffad":"scores = cross_val_score(rnd_clf, x_test,y_test,scoring=\"neg_mean_squared_error\", cv=10)\nrmse_scores = np.sqrt(-scores)\nprint(rmse_scores.mean())","d744a2eb":"rnd_clf = RandomForestRegressor(n_estimators=500, n_jobs=-1,random_state=42)\nrnd_clf.fit(x_train, y_train)\nscores = cross_val_score(rnd_clf, x_test,y_test,scoring=\"neg_mean_squared_error\", cv=10)\nrmse_scores = np.sqrt(-scores)\nprint(rmse_scores.mean())","5ef6646b":"n_estimators=[10,20,30,40]\nfor i in n_estimators:\n    rnd_clf = RandomForestRegressor(n_estimators=i,random_state=42)\n    rnd_clf.fit(x_train, y_train)\n    scores = cross_val_score(rnd_clf, x_test,y_test,scoring=\"neg_mean_squared_error\", cv=10)\n    rmse_scores = np.sqrt(-scores)\n    print(rmse_scores.mean())","c73bab2a":"n_estimators=[50,60,70,80,90,100]\nmax=[8,9,10,11,12]\nfor i in n_estimators:\n    for j in max:\n        rnd_clf = RandomForestRegressor(n_estimators=i,random_state=42,max_features=j)\n        rnd_clf.fit(x_train, y_train)\n        scores = cross_val_score(rnd_clf, x_test,y_test,scoring=\"neg_mean_squared_error\", cv=10)\n        rmse_scores = np.sqrt(-scores)\n        print(rmse_scores.mean())","c79c0cad":"import xgboost","cf997779":"xgb_reg = xgboost.XGBRegressor(random_state=42)\nxgb_reg.fit(x_train, y_train)","faea8b2f":"scores = cross_val_score(xgb_reg, x_test,y_test,scoring=\"neg_mean_squared_error\", cv=10)\nrmse_scores = np.sqrt(-scores)\nprint(rmse_scores.mean())","dcd0e7c3":"params={\n    \"learning_rate\":[0.05,0.10,0.15,0.20,0.30],\n    \"max_depth\":[3,4,5,6,7,8,9,10,12,15,20,30,40,50],\n    \"min_child_weight\":[1,3,5,7,9,11,12,13,14,16,19,25,30],\n    \"gamma\":[0.0,0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,1],\n    \"colsample_bytree\":[0.3,0.4,0.5,0.6,0.7,0.8],\n}","a4c828c9":"rs=RandomizedSearchCV(xgb_reg,param_distributions=params,n_iter=10,scoring='neg_mean_squared_error',cv=5)","3db79cf8":"rs.fit(x_train,y_train)","efd89f46":"rs.best_params_","bec22bc4":"rs.best_estimator_","8a8474ad":"xgb=xgboost.XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n             colsample_bynode=1, colsample_bytree=0.4, gamma=0.1, gpu_id=-1,\n             importance_type='gain', interaction_constraints='',\n             learning_rate=0.2, max_delta_step=0, max_depth=12,\n             min_child_weight=30, monotone_constraints='()',\n             n_estimators=100, n_jobs=0, num_parallel_tree=1, random_state=42,\n             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,\n             tree_method='exact', validate_parameters=1, verbosity=None)","0ae169d1":"scores = cross_val_score(xgb, x_test,y_test,scoring=\"neg_mean_squared_error\", cv=10)\nrmse_scores = np.sqrt(-scores)\nprint(rmse_scores.mean())","3337da56":"test.drop(trtr,axis=1,inplace=True)","f93cd38f":"num=[]\ncat=[]\nsep(test)\nx1,y1=split(test)\n","dd15f5ee":"num=[]\ncat=[]\nsep(test)\nx,y=split(train)\n","110eb582":"a=[]\nfor i in y.columns:\n    n=-1\n    for j in y1.columns:\n        n=n+1\n        if i==j:\n            break\n        elif n==222:\n            a.append(i)","d90bff04":"for i in a:\n    y1[i]=y[i]","5aa6f67e":"len(y1.columns)","65b94aca":"features1=scaling(x1,y1)","36884bae":"xgb.fit(x_train,y_train)","c1e5c57f":"y_pred=xgb.predict(features1)","029f270b":"fffff.head()","6cd27570":"r = pd.Series(y_pred,name=\"SalePrice\")","461ea65e":"fffff.tail","1aaa472c":"r","a71e57e9":"submission = pd.concat([pd.Series(range(1461,2920),name = \"Id\"),r],axis = 1)","626dee87":"submission","06de0cac":"submission.to_csv(\"regression.csv\",index=False)","1bfe7ecc":"with increasing c accuracy getting better(C=5500 is best value further increase in c will result in decrease in accuracy)","0d5ec0ed":"There r too many outliers so we should stick with median","0835d88a":"Lets Plot hist graph of continous curve","043082a7":"# Handling missing values","49f81cc6":"We clearly see a line btw these two variable we will use linear regression model to predict those null values","5da1ff26":"Lets see the feature contaning null values","d9b3a579":"Lets Draw Boxplot of LotFrontage and MasVnrArea to understand whether mean or median to be used to fill null values","18b1fe95":"# Feature Selection\/Data Preprocessing","604ba690":"Decision tree is acting like a dumbed","783f8dc7":"test dataset has more null value than train set(this observeation will be help us in future while dealing with missing value)","34c26639":"From all analysis we came to conclusion that our svm model is the best","e2f8bec8":"Quite a huge dataset with 81 features","425963c2":"lets try some more values of c","5b95e606":"# Continuos Data Visulization And Transformation\n","fe0765a7":"Lets Separate Continous and discrete numerical variables","6c66d521":"Lets do it for train dataset first\n1-separate categorical and numerical missing feature","91cf6dc8":"Now we will transform skewed data into Normal Distribution(using log (remember pick that feature which didn't contain zero))","1a21c9b9":"Dealing with discrete numerical attribute(using linear regression)","f437a995":"Now we will repeat that for test dataset","aecf442c":"Its better to use ML models to predict missing value(we r gonna use knearest neighbours)","3616d988":"SVM Linear CLASSIFIER IS PERFORMING A LOT BETTER THAN RBF","d4f6d8b8":"Dealing with missing categorical attributes","eb7ef7dc":"LotFrontage and MasVnrArea are not so correlated but GarageYrBlt is highly correlated on other features","26c491b2":"Lets deal with numerical attribute first\n,Here LotFrontage and MasVnrArea are continous Variable and GarageYrBlt is descrete Variable\n.Lets check that if these are correlated with some other variable"}}