{"cell_type":{"76775d38":"code","96b599b0":"code","93167dd4":"code","0e65493d":"code","b3802f8b":"code","a8ee60ab":"code","6ef8f857":"code","3c98eaa5":"code","0dd8e2a8":"code","519bb279":"code","c6b21c8d":"code","e7002a9f":"code","83e76187":"code","a16008ca":"code","6231cae5":"code","2a7121a4":"code","2d3e2308":"code","0f73c190":"markdown","f5656dc6":"markdown","dad190be":"markdown","655b4126":"markdown","452583c1":"markdown","02b9e970":"markdown","8e99158b":"markdown","2940e044":"markdown","1fb415d9":"markdown","bcdce781":"markdown","8d93f6c7":"markdown","85558962":"markdown","4725f73b":"markdown","da777fb1":"markdown","2633eef3":"markdown","7ca2777a":"markdown","89e5e4d9":"markdown","a87f059e":"markdown","49fabb67":"markdown","7c0124fb":"markdown","d5f8f1a9":"markdown","d1ee4e7b":"markdown","f9e2c968":"markdown","fe08c0f3":"markdown","9f417936":"markdown","5f753b0a":"markdown","31d9fbb8":"markdown","7ec2b4ee":"markdown","73377862":"markdown"},"source":{"76775d38":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","96b599b0":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport re\nimport os\nimport nltk\n#nltk.download('stopwords')\nfrom nltk.corpus import stopwords\nfrom nltk.stem.porter import PorterStemmer\nfrom nltk.stem import WordNetLemmatizer","93167dd4":"messages = pd.read_csv('..\/input\/sms-spam-collection-dataset\/spam.csv',encoding='latin-1')\nmessages.head()","0e65493d":"messages.drop(['Unnamed: 2', 'Unnamed: 3', 'Unnamed: 4'],axis=1,inplace=True)","b3802f8b":"messages.rename(columns={'v1':'Label','v2':'Message'},inplace = True)\nmessages.info()","a8ee60ab":"sns.countplot(messages.Label,palette=\"twilight\")\nplt.xlabel('Label')\nplt.title('Number of ham and spam messages')","6ef8f857":"ax = sns.countplot(y=\"Label\", data=messages,palette=\"twilight\")\nplt.xlabel('Count')\nplt.title('Number of ham and spam messages')\n\ntotal = len(messages['Label'])\nfor p in ax.patches:\n        percentage = '{:.1f}%'.format(100 * p.get_width()\/total)\n        x = p.get_x() + p.get_width() + 0.02\n        y = p.get_y() + p.get_height()\/2\n        ax.annotate(percentage, (x, y))\n\nplt.show()","3c98eaa5":"ps = PorterStemmer # You can use stemming but here I am using Lemmatization.\nlmr = WordNetLemmatizer\ncorpus = []\nfor i in range( 0, len(messages)):\n    review = re.sub('[^a-zA-Z]', ' ', messages['Message'][i]) # Cleaning all special characters and numbers, keeping words only\n    review = review.lower() # Making all the words to lower case.\n    review = review.split()\n    \n    review = [lmr.lemmatize('word',word) for word in review if word not in set(stopwords.words('english'))]\n    # This is list compreshession adding the words which are not avaibale in stopwords.\n    review = ' '.join(review)\n    corpus.append(review) # adding all words to Corpus","0dd8e2a8":"# Creating a Bag Of Words model:\nfrom sklearn.feature_extraction.text import CountVectorizer\ncv = CountVectorizer(max_features=5000) # Taking the 5000 important words.\nX = cv.fit_transform(corpus).toarray() # Fitting the model to Corpus and converting it to array.","519bb279":"Y = pd.get_dummies(messages['Label'])\n# No need to specify teo catregorical column here we can just define one culumn (ie:if 0-> Ham or if ->1 Spam):\nY = Y.iloc[:,1].values","c6b21c8d":"X.shape","e7002a9f":"Y.shape","83e76187":"# Spliting the Data into train and test:\nfrom sklearn.model_selection import train_test_split\nX_train,X_test,Y_train,Y_test = train_test_split(X, Y, test_size= 0.2,random_state = 5)","a16008ca":"# Training the model with Naive Bayes Classifier:\nfrom sklearn.naive_bayes import MultinomialNB\nspam_detection_model=MultinomialNB().fit(X_train,Y_train)","6231cae5":"# The predected output of our model\nY_pred = spam_detection_model.predict(X_test)","2a7121a4":"from sklearn.metrics import confusion_matrix\nconfusion_m = confusion_matrix(Y_test,Y_pred) #It shows the total Right Predictions (960+140) and Wrong Predictions(10+5).\nprint(confusion_m)","2d3e2308":"from sklearn.metrics import accuracy_score\naccuracy = accuracy_score(Y_test,Y_pred) # It shows the accuracy of the model.\nprint (accuracy)","0f73c190":"## 04-Data Cleaning and Preprocessing:","f5656dc6":"It looks like we got a good accuracy just using a simple Classifier, we an accuracy approximate to 98 %.OKKK it's GOOD .","dad190be":"Lets see Our X, Is it looking Handsome?\nAs it is the independent Variable I am taking it as Hero of Our Movie.(Don't aks me if there are more than one one independent variables, they might be side Heros or Fraiends of Hero)- Got it You Dirty Mind","655b4126":"## 06-Training Model:","452583c1":"Finally Accuracy of Our model.","02b9e970":"## 03- Visualization:","8e99158b":"Test_size = 0.2 mean I am taking 20% of all data to test our model and random_state means that 20% data will be cloose randomly from the main dataset.","2940e044":"Confusion matrix- It show the True +ve,False +ve,True -ve and False -ve values. It mainly helps to find the accuracy of the Classificaton model.","1fb415d9":"Stemming->\"Stemming is the process of reducing infected or derived words to there word stem, base or root form\"-Wikipedia\nBut the problem in stemming is some of stemming words dont have any actual meaning.(for example- \"intelligence,intelligent,intelligently\" becomes \"intelgen\" which dont have any meaning.\nin otherhand Lemmatization make it \"intelligent\" which have an actual meaning.","bcdce781":"## 01-Importing Libraries:","8d93f6c7":"It seems like there are some columns, who dont have any value in the data set.we can drop them.\nHii Friends\" Do you guys Love Cricket\",Let me know in the comment section. I am from India and I love Cricket.Let me Know about You.","85558962":"Lets's Create Dummi Values for Ham and Spam","4725f73b":"![](http:\/\/)Loading Data ","da777fb1":"## 02-Load Data:","2633eef3":"Possible Palettes are: Accent, Accent_r, Blues, Blues_r, BrBG, BrBG_r, BuGn, BuGn_r, BuPu, BuPu_r, CMRmap, CMRmap_r, Dark2, Dark2_r, GnBu, GnBu_r, Greens, Greens_r, Greys, Greys_r, OrRd, OrRd_r, Oranges, Oranges_r, PRGn, PRGn_r, Paired, Paired_r, Pastel1, Pastel1_r, Pastel2, Pastel2_r, PiYG, PiYG_r, PuBu, PuBuGn, PuBuGn_r, PuBu_r, PuOr, PuOr_r, PuRd, PuRd_r, Purples, Purples_r, RdBu, RdBu_r, RdGy, RdGy_r, RdPu, RdPu_r, RdYlBu, RdYlBu_r, RdYlGn, RdYlGn_r, Reds, Reds_r, Set1, Set1_r, Set2, Set2_r, Set3, Set3_r, Spectral, Spectral_r, Wistia, Wistia_r, YlGn, YlGnBu, YlGnBu_r, YlGn_r, YlOrBr, YlOrBr_r, YlOrRd, YlOrRd_r, afmhot, afmhot_r, autumn, autumn_r, binary, binary_r, bone, bone_r, brg, brg_r, bwr, bwr_r, cividis, cividis_r, cool, cool_r, coolwarm, coolwarm_r, copper, copper_r, cubehelix, cubehelix_r, flag, flag_r, gist_earth, gist_earth_r, gist_gray, gist_gray_r, gist_heat, gist_heat_r, gist_ncar, gist_ncar_r, gist_rainbow, gist_rainbow_r, gist_stern, gist_stern_r, gist_yarg, gist_yarg_r, gnuplot, gnuplot2, gnuplot2_r, gnuplot_r, gray, gray_r, hot, hot_r, hsv, hsv_r, icefire, icefire_r, inferno, inferno_r, jet, jet_r, magma, magma_r, mako, mako_r, nipy_spectral, nipy_spectral_r, ocean, ocean_r, pink, pink_r, plasma, plasma_r, prism, prism_r, rainbow, rainbow_r, rocket, rocket_r, seismic, seismic_r, spring, spring_r, summer, summer_r, tab10, tab10_r, tab20, tab20_r, tab20b, tab20b_r, tab20c, tab20c_r, terrain, terrain_r, twilight, twilight_r, twilight_shifted, twilight_shifted_r, viridis, viridis_r, vlag, vlag_r, winter, winter_r","7ca2777a":"Above line of code drops the unnessary columns,Now lets make Our DataFrame more Interesting by giving some meaningful column names.","89e5e4d9":"Thank You Guys.ALL the very Best for your Data Science Career.","a87f059e":"As I am a Big Fan \"Twilight\" , I am using this palette style ,You can use as your wish , I have given a set of Palette styles down ,Play aound","49fabb67":"Woew She looks Beautyful...#bornsexy","7c0124fb":"Ok it's becoming little intense ,,, Let's have some fun.... Anyone can tell me \"Pandora\" is a name used for one Planate in a very famous Science Friction Movie ,What is the name of the movie ? # Answer it in the comment section","d5f8f1a9":"Lets create a Bag Of words using the messages and consider the 5000 important words.","d1ee4e7b":"## 05-Spliting Data into train and test:","f9e2c968":"## 07-Accuracy:","fe08c0f3":"Here some information about our created dataframe, It seems like there is no NaN or null values present,Let me know if you gus find some. We can clean them .Right...","9f417936":"Lets add some percentages to more understanding.","5f753b0a":"Now it time for Our Heroine, Y","31d9fbb8":"Ok now Let's Visualize Our Data.","7ec2b4ee":"I will try to make you understand evry line,Please read every comments written after every line of code ","73377862":"Okk answer me onething guys ---Which one is more Dengerious ? (False +ve or False -ve or Depends on the situation) If you can explane your ans!!!"}}