{"cell_type":{"670fad67":"code","9ca4138e":"code","bd5ad343":"code","fd41f127":"code","4016525d":"code","7731a739":"code","bb43f77d":"code","9ef93f59":"code","b69a0ff9":"markdown","6c457df8":"markdown","ed4e656e":"markdown","e5219d04":"markdown","78860407":"markdown"},"source":{"670fad67":"# Familiar imports\nimport numpy as np\nimport pandas as pd\n\n# For one hot encoding categorical variables, splitting data\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.model_selection import train_test_split\n\n# For training XGBoost model\nfrom xgboost import XGBRegressor\nfrom sklearn.metrics import mean_squared_error","9ca4138e":"train = pd.read_csv(\"..\/input\/30-days-of-ml\/train.csv\", index_col=0)\nX_test = pd.read_csv(\"..\/input\/30-days-of-ml\/test.csv\", index_col=0)\n\ny = train['target']\nX = train.drop(['target'], axis=1)\n\nX.head()","bd5ad343":"X_train, X_valid, y_train, y_valid = train_test_split(X, y,\n                                                      train_size=0.8, test_size=0.2,\n                                                      random_state=0)","fd41f127":"# object columns\nobject_cols = [col for col in X_train.columns if X_train[col].dtype == \"object\"]\n\n# Columns that will be one-hot encoded\nlow_cardinality_cols = [col for col in object_cols if X_train[col].nunique() < 10]\n\n# Columns that will be dropped from the dataset\nhigh_cardinality_cols = list(set(object_cols)-set(low_cardinality_cols))\n\nprint('Categorical columns that will be one-hot encoded:', low_cardinality_cols)\nprint('\\nCategorical columns that will be dropped from the dataset:', high_cardinality_cols)","4016525d":"encoder = OneHotEncoder(handle_unknown='ignore',sparse=False)\nOH_X_train = pd.DataFrame(encoder.fit_transform(X_train[low_cardinality_cols]))\nOH_X_valid = pd.DataFrame(encoder.transform(X_valid[low_cardinality_cols]))\n\n# review: OneHotEncoder strips index\nOH_X_train.index = X_train.index\nOH_X_valid.index = X_valid.index\n\nnum_X_train = X_train.drop(object_cols, axis=1)\nnum_X_valid = X_valid.drop(object_cols, axis=1)\n\nOH_X_train = pd.concat([num_X_train, OH_X_train], axis=1)\nOH_X_valid = pd.concat([num_X_valid, OH_X_valid], axis=1)","7731a739":"# I prapare helper functions. pramaters is same as below notebook but I use GPU so I add 'gpu_hist'\n# https:\/\/www.kaggle.com\/ryopenguin\/apply-xgboost-preprocessing-not-changed\n\nmodel = XGBRegressor(n_estimators=1000, learning_rate=0.01, n_jobs=4, tree_method = 'gpu_hist')\n# the data is large so n_jobs may work\nmodel.fit(OH_X_train, y_train,\n      early_stopping_rounds=5,\n      eval_set=[(OH_X_valid, y_valid)],\n      verbose=False)\npreds_valid = model.predict(OH_X_valid)\n\nprint(mean_squared_error(y_valid, preds_valid, squared=False))","bb43f77d":"OH_X_test = pd.DataFrame(encoder.transform(X_test[low_cardinality_cols]))\n\nOH_X_test.index = X_test.index\nnum_X_test = X_test.drop(object_cols, axis=1)\nOH_X_test = pd.concat([num_X_test, OH_X_test], axis=1)","9ef93f59":"predictions = model.predict(OH_X_test)\n\noutput = pd.DataFrame({'Id': X_test.index,\n                       'target': predictions})\noutput.to_csv('submission.csv', index=False)","b69a0ff9":"## Load the data","6c457df8":"- It performs pretty similar with result by ordinal encoding...\n- Anyway I submit this result ","ed4e656e":"## Investigating cardinality","e5219d04":"## What I will do next\n- Experiment with various models and different preprocessing patterns(ordinal \/ one-hot)\n- Hyper-paramater tuning\n- Learn feature engineering","78860407":"## this is (wrote this before training model)\n- I try to adopt one hot encoding to this compe's data\n- Maybe it performs worse because I need to drop high cardinality columns\n- Let's go ahead\n\n### disclaimer\n- I'm a machine learning newbie\n- So if you find odd part in this notebook, please leave the comments."}}