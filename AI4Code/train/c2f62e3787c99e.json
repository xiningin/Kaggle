{"cell_type":{"87798d74":"code","e7868eb4":"code","c8982e41":"code","9e95abe3":"code","ec93a28a":"code","d7bf4d31":"code","64ee8f1b":"code","fc98c8c7":"code","84fa5f26":"code","51556e0d":"code","f3cbfea5":"code","701501a7":"code","b9fd963f":"code","63ca0f23":"code","614a98e5":"code","ad88b51c":"code","97702dce":"code","4405b8a3":"code","a45256f8":"code","9ceac097":"code","d9af782e":"code","0f88f965":"code","0f64679b":"code","f34d51a1":"code","9b455a58":"code","2fcaff5b":"code","291dd8d7":"code","8186dcc7":"code","0e3a69dd":"code","9ecee5cb":"code","9db763c3":"code","6c9bc0a2":"code","67bba74b":"code","da7e832a":"code","8ec7d391":"code","da58f24a":"code","bf1928a4":"code","23a80ebd":"code","4b8c0e83":"code","e85e61e4":"code","2dc3338f":"code","04a1751a":"code","23a45295":"code","42669b81":"code","ac4e3080":"code","87d05e38":"code","db94103e":"code","fde804cf":"code","374eccfd":"code","6ffeb8ef":"code","fe070847":"code","71d26927":"code","16cb0c9f":"code","2169bf08":"code","7470fcd8":"code","f2c13661":"code","55027718":"code","5b1b787a":"code","194cde3c":"code","d064f403":"code","b85a9a7f":"code","03a086b8":"code","df212df4":"code","049518a1":"code","26d88640":"code","0eb22b01":"code","ab138c8a":"code","7e846d56":"code","516218f3":"code","8ae92694":"code","531fc799":"code","c4620203":"code","9a558a17":"code","a77a32f5":"code","beafe0dd":"code","69dea35a":"code","83e3312e":"code","6e63638d":"code","5a105142":"code","7c384c30":"code","b51512b7":"code","f463d0f7":"code","75d521b1":"code","800beac6":"code","48aff1c3":"code","c37e4e9a":"code","1914d447":"code","f3f783c7":"code","3752871b":"code","5cb39f61":"code","233e004a":"code","5e6953e8":"code","b7a58553":"code","691911ad":"code","15347387":"code","6e6891ad":"code","dac4f617":"code","f27fbd0e":"code","fd84a453":"code","74ceeaa9":"code","638d4a8f":"code","46cf1618":"code","60cfa37b":"code","4c71b426":"code","c607d5a8":"code","eec6653f":"code","f5b635bf":"code","e13592c7":"code","8cc7aa2b":"code","5c52eaa8":"code","ad2b0d04":"code","51b3d48a":"code","c461cce3":"code","edb029a8":"code","25bcc50d":"code","76e34648":"code","5439dcb4":"code","b6e04f39":"code","8a0823a5":"code","6fa0090f":"code","784693fb":"code","f6987f54":"code","89bbc082":"code","36dfce22":"code","db922535":"code","be520747":"code","c2945f70":"code","09fa982e":"code","3be7eb92":"code","97846f2f":"code","675565e5":"code","ed1678ea":"code","fc4302c0":"code","c4e0ae13":"code","7fd8e15f":"code","7ffa8690":"code","40070b22":"code","6f16bc00":"code","9ba43c2d":"code","afc1d111":"code","0a7ced22":"code","37a445a4":"code","a7334087":"code","d7c11426":"code","5c3a0f7d":"code","40854c18":"code","8d697301":"code","a226ff41":"code","a96d65ce":"code","3f868061":"code","63918fd4":"code","4e0a3dbd":"code","497cbd6c":"code","45db5a25":"code","2173a17c":"code","3fef5bf3":"code","f342c4d4":"code","d4252c0c":"code","df3dfaac":"code","258932cd":"code","0492279b":"code","00a99494":"code","bef5126f":"code","ef8a0626":"code","f58bf464":"code","60487b16":"code","7c50d079":"code","91781be5":"code","5d1afb70":"code","f052c76b":"code","7a8ef311":"code","f35876fb":"code","7979f408":"code","e80add4c":"code","3b44d188":"code","d6c886f3":"code","dabfab69":"code","caeb57b6":"code","650015d4":"code","b90cfb51":"code","dfb17a58":"code","507ad37d":"code","a5b789b9":"code","406d1446":"code","90634153":"code","28cae46b":"code","5699430e":"code","213ed9ab":"code","62d70dbe":"code","0c2b9383":"code","f4c4bc67":"code","c3a073de":"code","b997ff6f":"code","279724b6":"code","31c41bf8":"code","d178cddf":"code","b62b3ba1":"code","39d3f643":"markdown","d2491936":"markdown","432390d8":"markdown","071ec6fa":"markdown","b951015f":"markdown","df60c988":"markdown","a34755a3":"markdown","bcdf1ecf":"markdown","ce91081c":"markdown","f4816aa0":"markdown","facbe035":"markdown","2703d2f2":"markdown","2c1f7406":"markdown","91afda37":"markdown","b03d7332":"markdown","9dd630ab":"markdown","be17de45":"markdown","dd1d5d4f":"markdown","39041d00":"markdown","32293e4f":"markdown","b8051666":"markdown","9f1824ff":"markdown","fdc85e44":"markdown","c3899bc1":"markdown","61d32a18":"markdown","0d450032":"markdown","32ac3b17":"markdown","a2fef6e2":"markdown","97fa6785":"markdown","fc344c72":"markdown","7988c8cd":"markdown","22385cd3":"markdown","06841983":"markdown","61506b8d":"markdown","a96c8d34":"markdown","c6335d82":"markdown","9b18f5b1":"markdown","5700ca8a":"markdown","d3492242":"markdown","cbd534be":"markdown","48638305":"markdown","cea7b5f8":"markdown","60436b5e":"markdown","485a0a63":"markdown","70ab7005":"markdown","71bcfb56":"markdown","c63d8ec4":"markdown","c2a50fac":"markdown","62cda067":"markdown","e70e3b62":"markdown","9c919ae2":"markdown","d9eb238f":"markdown","941de837":"markdown","34b5fd4a":"markdown","b1884552":"markdown","60ea76ac":"markdown","0531f346":"markdown","22257438":"markdown","d0fde40f":"markdown","e9c2c4ba":"markdown","8ec9e52c":"markdown","d9dfafe2":"markdown","d252404b":"markdown","3c1669b0":"markdown","e1986f4d":"markdown","19ca978c":"markdown","713c62de":"markdown","51563496":"markdown","47966783":"markdown","96a36918":"markdown","a6cdb784":"markdown","86c3d032":"markdown","aae653dd":"markdown","9b0cc563":"markdown","fcfa317e":"markdown","60d34d8d":"markdown","79d0206e":"markdown","68b7f4a7":"markdown","02721411":"markdown","b44d37d3":"markdown","4c803322":"markdown","35a639f3":"markdown","c143d4b0":"markdown","de4477e6":"markdown","97c68ba4":"markdown","8650ed3e":"markdown","4d50ab2b":"markdown","3cc36532":"markdown","e6b30c12":"markdown","db2eec40":"markdown","80911eb1":"markdown","93218222":"markdown","6656723c":"markdown","7a1dcb19":"markdown","6eece8d5":"markdown","dfd1c63e":"markdown","fa6ed0a4":"markdown","11ccc7f3":"markdown","1d90a7df":"markdown","f7699ed6":"markdown","05567192":"markdown","751c160f":"markdown","90392d44":"markdown","f876db93":"markdown","435cef52":"markdown","b4165ad8":"markdown","6e30be9f":"markdown","06736cc7":"markdown","77472757":"markdown","132c4404":"markdown","508d55cf":"markdown","76b0b015":"markdown","2fc0162c":"markdown","a9f2b3a0":"markdown","fc13e8a3":"markdown","f8c9afad":"markdown","d1252e85":"markdown","1a9a3564":"markdown","e71c1457":"markdown","9840c828":"markdown","ef4dfac4":"markdown","7fafb6b3":"markdown","5e4c1805":"markdown","e7ebca32":"markdown","74d1711b":"markdown","9f08fb89":"markdown","31378464":"markdown","95680efc":"markdown","e83c161c":"markdown","240560fe":"markdown","9e028e5b":"markdown","92c949b5":"markdown","9b704519":"markdown","2b3216a2":"markdown","65f97cb0":"markdown","ffcd496f":"markdown","9e09c79f":"markdown","25079a57":"markdown","7c567062":"markdown","8c5c525a":"markdown","77121002":"markdown","790b3305":"markdown","23b83b7d":"markdown","91407587":"markdown","bb2bfd50":"markdown","b6e6ae49":"markdown","f93f70e7":"markdown","022bc99b":"markdown","ac8e39c3":"markdown","6a6fd030":"markdown","35e0663b":"markdown"},"source":{"87798d74":"import numpy as np\nimport pandas as pd","e7868eb4":"dataset = pd.read_json('\/kaggle\/input\/news-category-dataset\/News_Category_Dataset_v2.json', lines=True)","c8982e41":"#news_data_sample = dataset.sample(2000)","9e95abe3":"dataset.info()","ec93a28a":"dataset.describe().transpose()","d7bf4d31":"dataset.columns","64ee8f1b":"dataset.head()","fc98c8c7":"dataset.tail()","84fa5f26":"dataset['category'].value_counts()","51556e0d":"len(dataset['category'].value_counts())","f3cbfea5":"pd.DataFrame(dataset['category'].value_counts()[:10])","701501a7":"top_categories = dataset['category'].value_counts().index[:10]\ndataset = dataset[dataset['category'].isin(top_categories)]","b9fd963f":"dataset","63ca0f23":"dataset['category'].value_counts()","614a98e5":"len(dataset)","ad88b51c":"dataset.sample(7)","97702dce":"dataset['headline'].nunique()","4405b8a3":"len(dataset['headline'])- dataset['headline'].nunique() ","a45256f8":"dataset['headline'].value_counts() ","9ceac097":"dataset[dataset['headline']=='Sunday Roundup']['short_description'].nunique()","d9af782e":"len(dataset[dataset['headline']=='Sunday Roundup']['short_description']) - dataset[dataset['headline']=='Sunday Roundup']['short_description'].nunique()","0f88f965":"dataset['short_description'].value_counts()","0f64679b":"dataset[dataset['short_description'] == 'Welcome to the HuffPost Rise Morning Newsbrief, a short wrap-up of the news to help you start your day.']","f34d51a1":"from dateutil import parser\n\n\ndef is_valid_date(date_str):\n    try:\n        parser.parse(date_str)\n        return True\n    except:\n        return False\n\ndataset['short_description'] = [' '.join([w for w in line.split() if not is_valid_date(w)]) for line in dataset['short_description']]\ndataset['headline'] = [' '.join([w for w in line.split() if not is_valid_date(w)]) for line in dataset['headline']]","9b455a58":"dataset['short_description'].value_counts()","2fcaff5b":"dataset['headline'].value_counts()","291dd8d7":"dataset[dataset['short_description'] == 'Welcome to the HuffPost Rise Morning Newsbrief, a short wrap-up of the news to help you start your day.']['headline'].value_counts()","8186dcc7":"dataset[dataset['short_description'] == 'The stress and strain of constantly being connected can sometimes take your life -- and your well-being -- off course. GPS']","0e3a69dd":"dataset[dataset['short_description'] == 'The stress and strain of constantly being connected can sometimes take your life -- and your well-being -- off course. GPS']['headline'].value_counts()","9ecee5cb":"len(dataset['link'].value_counts())","9db763c3":"dataset['authors'].value_counts()","6c9bc0a2":"len(dataset[dataset['authors'] == \"\"])","67bba74b":"dataset.drop(['authors', 'link', 'date'], axis = 1, inplace = True)","da7e832a":"dataset.drop_duplicates(subset=['short_description','headline'], keep='first', inplace=True, ignore_index=True)","8ec7d391":"len(dataset) ","da58f24a":"dataset['headline'].value_counts()","bf1928a4":"len(dataset[(dataset['short_description'] == \"\") & (dataset['headline'] == \"\")])","23a80ebd":"len(dataset[dataset['headline'] == \"\"])","4b8c0e83":"dataset = dataset[(dataset['short_description']!=\"\") & (dataset['headline']!=\"\")]","e85e61e4":"dataset[dataset['headline'] == \"\"]","2dc3338f":"dataset[dataset['short_description'] == \"\"]","04a1751a":"len(dataset)","23a45295":"dataset['short_description'].value_counts()","42669b81":"dataset['short_description'].nunique()","ac4e3080":"dataset[dataset['short_description'] == 'The stress and strain of constantly being connected can sometimes take your life -- and your well-being -- off course. GPS']['category'].value_counts()","87d05e38":"dataset[dataset['short_description'] == 'The stress and strains of our always-connected lives can sometimes take us off course. GPS For The Soul can help you find']['category'].value_counts()","db94103e":"dataset[dataset['short_description'] == 'Want more? Be sure to check out HuffPost Style on Twitter, Facebook, Tumblr, Pinterest and Instagram at @HuffPostStyle. -- Do']['category'].value_counts()","fde804cf":"dataset[dataset['short_description'] == 'Want more HuffPost Style beauty content? Check us out on Twitter, Facebook, Tumblr, Pinterest and Instagram at @HuffPostBeauty']['category'].value_counts()","374eccfd":"dataset['headline'].nunique()  # unique number of headlines","6ffeb8ef":"dataset['headline'].value_counts()","fe070847":"nn = pd.DataFrame(dataset['headline'].value_counts())","71d26927":"nn.head(50)","16cb0c9f":"nn.values","2169bf08":"nn.index","7470fcd8":"index_list = nn.index.tolist()\nindex_list","f2c13661":"value_list = nn.values.tolist()\nvalue_list","55027718":"value_list = np.array(value_list)\nvalue_list","5b1b787a":"value_list = value_list.flatten()\nvalue_list","194cde3c":"value_list = np.array(value_list)\nindex_list = np.array(index_list)\ntemp_dataset = pd.DataFrame({'number': value_list, 'headline': list(index_list)}, columns=['number', 'headline'])","d064f403":"temp_dataset.head(10)","b85a9a7f":"len(temp_dataset)","03a086b8":"temp_dataset = temp_dataset[temp_dataset['number'] > 1]","df212df4":"len(temp_dataset)","049518a1":"def ret_more_categories(dataframe, news_data):\n    \n    for i in range(0, len(dataframe)):\n\n        headline = dataframe.iloc[i]['headline']\n\n        df_category = news_data[news_data['headline'] == headline]\n        \n        unique_categories =  df_category['category'].unique()\n        v = len(unique_categories)\n        \n        news_data = news_data[(news_data['headline'] != headline)]\n\n        if v > 0:\n            for i in range(0, v):\n\n                first = df_category[(df_category['headline'] == headline) & (df_category['category'] == unique_categories[i])].iloc[0]\n\n                news_data = news_data.append({'category' : unique_categories[i], 'headline' : headline, 'short_description': first['short_description']}, ignore_index = True)\n            \n        else:\n            v= 0\n        \n    return news_data","26d88640":"dataset = ret_more_categories(temp_dataset, dataset)","0eb22b01":"len(dataset)","ab138c8a":"dataset[dataset['headline'] == 'News Roundup for']","7e846d56":"dataset[dataset['headline'] == 'Roundup']['category'].value_counts() # just one category","516218f3":"# DATA PREPROCESSING - TEXT PREPROCESSING\n#1. convert to lower case\n\ndataset['category'] = dataset['category'].apply(lambda x: str(x).lower())\ndataset['headline'] = dataset['headline'].apply(lambda x: str(x).lower())\ndataset['short_description'] = dataset['short_description'].apply(lambda x: str(x).lower())","8ae92694":"dataset.sample(3)","531fc799":"contractions = { \n\"ain't\": \"am not\",\n\"aren't\": \"are not\",\n\"can't\": \"cannot\",\n\"can't've\": \"cannot have\",\n\"'cause\": \"because\",\n\"could've\": \"could have\",\n\"couldn't\": \"could not\",\n\"couldn't've\": \"could not have\",\n\"didn't\": \"did not\",\n\"doesn't\": \"does not\",\n\"don't\": \"do not\",\n\"hadn't\": \"had not\",\n\"hadn't've\": \"had not have\",\n\"hasn't\": \"has not\",\n\"haven't\": \"have not\",\n\"he'd\": \"he would\",\n\"he'd've\": \"he would have\",\n\"he'll\": \"he will\",\n\"he'll've\": \"he will have\",\n\"he's\": \"he is\",\n\"how'd\": \"how did\",\n\"how'd'y\": \"how do you\",\n\"how'll\": \"how will\",\n\"how's\": \"how does\",\n\"i'd\": \"i would\",\n\"i'd've\": \"i would have\",\n\"i'll\": \"i will\",\n\"i'll've\": \"i will have\",\n\"i'm\": \"i am\",\n\"i've\": \"i have\",\n\"isn't\": \"is not\",\n\"it'd\": \"it would\",\n\"it'd've\": \"it would have\",\n\"it'll\": \"it will\",\n\"it'll've\": \"it will have\",\n\"it's\": \"it is\",\n\"let's\": \"let us\",\n\"ma'am\": \"madam\",\n\"mayn't\": \"may not\",\n\"might've\": \"might have\",\n\"mightn't\": \"might not\",\n\"mightn't've\": \"might not have\",\n\"must've\": \"must have\",\n\"mustn't\": \"must not\",\n\"mustn't've\": \"must not have\",\n\"needn't\": \"need not\",\n\"needn't've\": \"need not have\",\n\"o'clock\": \"of the clock\",\n\"oughtn't\": \"ought not\",\n\"oughtn't've\": \"ought not have\",\n\"shan't\": \"shall not\",\n\"sha'n't\": \"shall not\",\n\"shan't've\": \"shall not have\",\n\"she'd\": \"she would\",\n\"she'd've\": \"she would have\",\n\"she'll\": \"she will\",\n\"she'll've\": \"she will have\",\n\"she's\": \"she is\",\n\"should've\": \"should have\",\n\"shouldn't\": \"should not\",\n\"shouldn't've\": \"should not have\",\n\"so've\": \"so have\",\n\"so's\": \"so is\",\n\"that'd\": \"that would\",\n\"that'd've\": \"that would have\",\n\"that's\": \"that is\",\n\"there'd\": \"there would\",\n\"there'd've\": \"there would have\",\n\"there's\": \"there is\",\n\"they'd\": \"they would\",\n\"they'd've\": \"they would have\",\n\"they'll\": \"they will\",\n\"they'll've\": \"they will have\",\n\"they're\": \"they are\",\n\"they've\": \"they have\",\n\"to've\": \"to have\",\n\"wasn't\": \"was not\",\n\" u \": \" you \",\n\" ur \": \" your \",\n\" n \": \" and \",\n\"won't\": \"would not\",\n\"you're\": \"you are\",\n\"you'll\": \"you will\",\n\"you've\": \"you have\",\n\"you'd\": \"you would\",\n\"you'd've\": \"you would have\",\n\"you'll've\": \"you will have\",\n\"here's\" : \"here is\",\n\"there's\" : \"there is\",\n\"where's\": \"where is\"\n}","c4620203":"def cont_to_exp(x):\n    if type(x) is str:\n        for key in contractions:\n            value = contractions[key]\n            x = x.replace(key, value)\n        return x\n    else:\n        return x","9a558a17":"dataset['headline'] = dataset['headline'].apply(lambda x: cont_to_exp(x))\ndataset['short_description'] = dataset['short_description'].apply(lambda x: cont_to_exp(x))","a77a32f5":"import re ","beafe0dd":"dataset['headline'] = dataset['headline'].apply(lambda x: re.sub(r'(http|https|ftp|ssh):\/\/([\\w_-]+(?:(?:\\.[\\w_-]+)+))([\\w.,@?^=%&:\/~+#-]*[\\w@?^=%&\/~+#-])?', '' , x))\ndataset['short_description'] = dataset['short_description'].apply(lambda x: re.sub(r'(http|https|ftp|ssh):\/\/([\\w_-]+(?:(?:\\.[\\w_-]+)+))([\\w.,@?^=%&:\/~+#-]*[\\w@?^=%&\/~+#-])?', '' , x))","69dea35a":"dataset['headline'] = dataset['headline'].apply(lambda x: re.sub(r'[^\\w ]+', \"\", x))\ndataset['short_description'] = dataset['short_description'].apply(lambda x: re.sub(r'[^\\w ]+', \"\", x))","83e3312e":"dataset['headline'] = dataset['headline'].apply(lambda x: ' '.join(x.split()))\ndataset['short_description'] = dataset['short_description'].apply(lambda x: ' '.join(x.split()))","6e63638d":"import unicodedata","5a105142":"def remove_accented_char(x):\n    x = unicodedata.normalize('NFKD', x).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n    return x","7c384c30":"dataset['headline'] = dataset['headline'].apply(lambda x: remove_accented_char(x))\ndataset['short_description'] = dataset['short_description'].apply(lambda x: remove_accented_char(x))","b51512b7":"import spacy","f463d0f7":"from spacy.lang.en.stop_words import STOP_WORDS as stopwords","75d521b1":"# removing stopwords\n\ndataset['headline'] = dataset['headline'].apply(lambda x: ' '.join([t for t in x.split() if t not in stopwords]))\ndataset['short_description'] = dataset['short_description'].apply(lambda x: ' '.join([t for t in x.split() if t not in stopwords]))","800beac6":"dataset.sample(5)","48aff1c3":"import nltk\n\nw_tokenizer = nltk.tokenize.WhitespaceTokenizer()\nlemmatizer = nltk.stem.WordNetLemmatizer()","c37e4e9a":"def lemmatize_tokenize_text(text):\n    return [lemmatizer.lemmatize(w) for w in w_tokenizer.tokenize(text)]","1914d447":"dataset['headline_lemmatized_tokenized'] = dataset.headline.apply(lemmatize_tokenize_text)\ndataset['short_description_lemmatized_tokenized'] = dataset.short_description.apply(lemmatize_tokenize_text)","f3f783c7":"dataset.sample(5)","3752871b":"dataset['category'].value_counts()","5cb39f61":"import plotly.express as px\nimport plotly.graph_objs as go\nfrom plotly.offline import iplot\n\nlabels = dataset['category'].value_counts().index\nvalues = dataset['category'].value_counts().values\ncolors = dataset['category']\n\nfig = go.Figure(data = [go.Pie(labels = labels, values = values, textinfo = \"label+percent\", marker = dict(colors = colors))])\nfig.show()","233e004a":"dataset['category'].value_counts(normalize=True)","5e6953e8":"value_frame = dataset['category'].value_counts().to_frame()\nlast_value = value_frame.iloc[value_frame.shape[0]-1].values[0]\nlast_value","b7a58553":"import random;\n\nvalue_frame = dataset['category'].value_counts().to_frame()\ncategory_number = len(dataset['category'].value_counts())\n\nfor i in range(0, category_number):\n    category_name = value_frame.index[i]\n    category_indices = dataset[dataset['category'] == category_name].index\n    row_num = len(category_indices)\n    remove_rows_num = row_num - last_value;\n    random_indices = random.sample(list(category_indices), remove_rows_num)\n    dataset.drop(random_indices , inplace=True)","691911ad":"dataset.reset_index(inplace = True, drop=True)","15347387":"dataset.shape[0]","6e6891ad":"dataset['category'].value_counts()","dac4f617":"# we can show the same Figure as before, but now with our balanced values\nbalanced_values = dataset['category'].value_counts().values\n\nbalanced_fig = go.Figure(data = [go.Pie(labels = labels, values = balanced_values, textinfo = \"label+percent\", marker = dict(colors = colors))])\nbalanced_fig.show()","f27fbd0e":"from wordcloud import WordCloud\nimport matplotlib.pyplot as plt\n%matplotlib inline","fd84a453":"text = ' '.join(dataset['headline'])\n\nwc = WordCloud(width=800, height=400).generate(text)\nplt.imshow(wc)\nplt.axis('off')\nplt.show()","74ceeaa9":"text = ' '.join(dataset['short_description'])\n\nwc = WordCloud(width=800, height=400).generate(text)\nplt.imshow(wc)\nplt.axis('off')\nplt.show()","638d4a8f":"from sklearn.feature_extraction import text\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.cluster import KMeans\nfrom nltk.tokenize import RegexpTokenizer\nfrom nltk.stem.snowball import SnowballStemmer\n%matplotlib inline\n\nfrom sklearn.cluster import KMeans\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import normalized_mutual_info_score, adjusted_rand_score\n\nfrom sklearn.decomposition import PCA\n\nfrom yellowbrick.cluster import KElbowVisualizer\nfrom yellowbrick.cluster import SilhouetteVisualizer\n\nimport seaborn as sns\n\nfrom sklearn import metrics\nfrom sklearn.metrics import pairwise_distances\nfrom sklearn.metrics import silhouette_samples, silhouette_score\n\nfrom sklearn.cluster import MeanShift, estimate_bandwidth","46cf1618":"# Get top labels by calculating average scores for clusters (means), sort them and take top 10 (number_of_labels)\n\ndef get_top_labels(data, prediction, number_of_labels):\n    dfs = []\n    labels = np.unique(prediction)\n    for label in labels:\n        cluster_index = np.where(prediction==label)\n        means = np.mean(data[cluster_index], axis = 0)\n        sorted_means = np.argsort(means)[::-1][:number_of_labels]\n        features = vectorizer.get_feature_names()\n        best_features = [(features[i], means[i], label) for i in sorted_means]\n        df = pd.DataFrame(best_features, columns = ['features', 'score', 'label'])\n        dfs.append(df)\n    return dfs ","60cfa37b":"dataset_test = dataset.sample(20000)\n\ndataset_test.reset_index(inplace = True, drop=True)\n\ny_unsup = dataset_test['category']\nX_unsup = dataset_test['headline'].values + \" \" + dataset_test['short_description'].values\n\nX_unsupervised_train, X_unsupervised_test, y_unsupervised_train, y_unsupervised_test = train_test_split(X_unsup, y_unsup, test_size = 0.2)","4c71b426":"# stop_words is set to \"english\" to remove all common pronouns (\"a\", \"the\", ...) to reduce the number of noisy features\n# norm is set to l2, to ensure all our feature vectors have a euclidian norm of 1\n# min_df is the minimum numbers of documents a word must be present in to be kept","c607d5a8":"vectorizer = TfidfVectorizer(sublinear_tf= True, min_df=10, norm='l2', ngram_range=(1, 2), stop_words='english')\nX_unsup_train = vectorizer.fit_transform(X_unsupervised_train)\nX_unsup_train","eec6653f":"pd.DataFrame(X_unsup_train.toarray(), columns=vectorizer.get_feature_names()).head()","f5b635bf":"original_dfs = get_top_labels(X_unsup_train.toarray(), y_unsupervised_train, 10)\nfor i in range(0, len(original_dfs)):\n    plt.figure(figsize=(8, 2))\n    plt.title((\"Top labels in Category {}\".format(original_dfs[i]['label'][0])), fontsize=10, fontweight='bold')\n    sns.barplot(x='score', y='features', data= original_dfs[i][:10])","e13592c7":"unsupervised_df = pd.DataFrame({'headline and description': X_unsupervised_train, 'category': y_unsupervised_train})\nunsupervised_df.reset_index(inplace = True, drop=True)\nunsupervised_df","8cc7aa2b":"!pip install --upgrade kneed","5c52eaa8":"disortion_model = KMeans()\ndisortion_visualizer = KElbowVisualizer(disortion_model, k=(1,15), timings=False, locate_elbow=True)\ndisortion_visualizer.fit(X_unsup_train.toarray())\ndisortion_visualizer.show()","ad2b0d04":"k_clusters = disortion_visualizer.elbow_value_\nk_clusters","51b3d48a":"sklearn_pca = PCA(n_components = 2)\nY_sklearn = sklearn_pca.fit_transform(X_unsup_train.toarray())\nkmeans = KMeans(n_clusters=k_clusters, max_iter=600, algorithm = 'auto')\nfitted = kmeans.fit(Y_sklearn)\nprediction = kmeans.predict(Y_sklearn)","c461cce3":"# Creating a new column to display the predicted result (cluster number)\nunsupervised_df[\"k_means_cluster_number\"] = prediction\nunsupervised_df.head(20)","edb029a8":"plt.figure(figsize=(14, 7))\nplt.scatter(Y_sklearn[:, 0], Y_sklearn[:, 1], c=prediction, s=40, cmap='viridis', linewidths=5)\n\ncenters = fitted.cluster_centers_\nplt.scatter(centers[:, 0], centers[:, 1],c='black', s=200, alpha=0.6);","25bcc50d":"score = silhouette_score(Y_sklearn, kmeans.labels_, metric='euclidean')\nscore","76e34648":"print('K-Means Silhouette Score: %.3f' % score)","5439dcb4":"dfs = get_top_labels(X_unsup_train.toarray(), prediction, 10)","b6e04f39":"for i in range(0, len(dfs)):\n    plt.figure(figsize=(8, 2))\n    plt.title((\"Top labels in Cluster {}\".format(i)), fontsize=10, fontweight='bold')\n    sns.barplot(x='score', y='features', data= dfs[i][:10])","8a0823a5":"meanshift_pca = PCA(n_components = 2)\nY_meanshift = meanshift_pca.fit_transform(X_unsup_train.toarray())","6fa0090f":"bandwidth = estimate_bandwidth(Y_meanshift, quantile=0.2)","784693fb":"ms = MeanShift(bandwidth=bandwidth)\nms.fit(Y_meanshift)","f6987f54":"labels = ms.labels_\ncluster_centers = ms.cluster_centers_","89bbc082":"labels_unique = np.unique(labels)\nn_clusters_ = len(labels_unique)","36dfce22":"print(\"number of estimated clusters : %d\" % n_clusters_)","db922535":"plt.figure(figsize=(14, 7))  \nplt.scatter(Y_meanshift[:, 0], Y_meanshift[:, 1],  c=ms.labels_, s=40, cmap='viridis', linewidths=5) \nplt.scatter(cluster_centers[:, 0], cluster_centers[:, 1], marker ='x', color ='black', s = 200, alpha=0.6)\nplt.title('Estimated number of clusters: %d' % n_clusters_)\nplt.show()","be520747":"# Creating a new column to display the predicted result (cluster number)\nunsupervised_df[\"ms_cluster_number\"] = ms.labels_\nunsupervised_df","c2945f70":"ms_score = silhouette_score(Y_meanshift, ms.labels_, metric='euclidean')\nprint('Mean Shift Silhouette Score: %.3f' % ms_score)","09fa982e":"ms_dfs = get_top_labels(X_unsup_train.toarray(), ms.labels_, 10)\nfor i in range(0, len(ms_dfs)):\n    plt.figure(figsize=(8, 2))\n    plt.title((\"Top labels in Cluster {}\".format(i)), fontsize=10, fontweight='bold')\n    sns.barplot(x='score', y='features', data= ms_dfs[i][:10])","3be7eb92":"sample_list = random.sample(list(X_unsupervised_test), 5)\nsample_list","97846f2f":"# Prepare our sample data\n\nsample_vectorizer = TfidfVectorizer(sublinear_tf= True, norm='l2', ngram_range=(1, 2), stop_words='english')\nX_sample = sample_vectorizer.fit_transform(sample_list)\nsample_pca = PCA(n_components = 2)\nY_sample = sklearn_pca.fit_transform(X_sample.toarray())","675565e5":"# Prediction with our k-means model\n\nkmeans_predicted = kmeans.predict(Y_sample)\nkmeans_predicted","ed1678ea":"# Prediction with our Mean Shift model\n\nms_predicted = ms.predict(Y_sample)\nms_predicted","fc4302c0":"# With k_means_cluster_number column and ms_cluster_number column - to have everything from Training in one place\nunsupervised_df.head(20)","c4e0ae13":"# Creating our Prediction Dataframe for the samples from our Test dataset\n# set \"sample_list\", \"kmeans_predicted\" and \"ms_predicted\" columns\n# set empty \"kmeans_labels\" and \"ms_labels\" columns\n\nprediction_df = pd.DataFrame({'sample_list': list(sample_list), 'kmeans_predicted': list(kmeans_predicted), 'ms_predicted': list(ms_predicted) }, columns=['sample_list', 'kmeans_predicted', 'kmeans_labels', 'ms_predicted', 'ms_labels'])\nprediction_df","7fd8e15f":"# Add labels to corresponding predicted category numbers and set \"kmeans_labels\" and \"ms_labels\" columns\n\ndef label_kmeans_categories (row):\n    return np.asarray(dfs[row['kmeans_predicted']]['features'])\ndef label_ms_categories (row):\n    return np.asarray(ms_dfs[row['ms_predicted']]['features'])\n\n\nprediction_df['kmeans_labels'] = prediction_df.apply(lambda row: label_kmeans_categories(row), axis=1)\nprediction_df['ms_labels'] = prediction_df.apply(lambda row: label_ms_categories(row), axis=1)\n\nprediction_df","7ffa8690":"from sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.svm import SVC\n\nfrom sklearn.metrics import accuracy_score, classification_report, confusion_matrix\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.feature_extraction.text import TfidfVectorizer","40070b22":"news_data = dataset.sample(20000)\n#news_data = dataset.copy()","6f16bc00":"news_data.reset_index(inplace = True, drop=True)","9ba43c2d":"plt.figure(figsize=(12, 8))\nplt.hist(news_data['category'], bins = 30, alpha = 0.7)\nplt.legend()\nplt.show()","afc1d111":"import seaborn as sns","0a7ced22":"news_data","37a445a4":"news_data['sum'] = news_data['headline_lemmatized_tokenized'] + news_data['short_description_lemmatized_tokenized']","a7334087":"news_data.sample(5)","d7c11426":"news_data.reset_index(inplace = True, drop=True)","5c3a0f7d":"news_data['sum'][0]","40854c18":"# Function to convert   \ndef listToString(s):  \n    \n    # initialize an empty string \n    str1 = \" \" \n    \n    # return string   \n    return (str1.join(s))","8d697301":"news_data['sum'] = news_data['sum'].apply(lambda x: ' '.join([str(elem) for elem in x]))","a226ff41":"news_data.sample(3)","a96d65ce":"news_data['sum'][0]","3f868061":"import spacy\nnlp = spacy.load('en_core_web_lg')","63918fd4":"def get_vec(x):\n    doc = nlp(x)\n    vec = doc.vector\n    return vec","4e0a3dbd":"news_data['vec'] = news_data['sum'].apply(lambda x: get_vec(x))","497cbd6c":"len(news_data.iloc[0]['vec'])","45db5a25":"news_data.iloc[0]['vec']","2173a17c":"X = news_data['vec'].to_numpy()\nX = X.reshape(-1, 1)","3fef5bf3":"X = np.concatenate(np.concatenate(X, axis = 0), axis = 0).reshape(-1, 300)","f342c4d4":"X.shape","d4252c0c":"#X = X[:20000]","df3dfaac":"y = news_data['category']","258932cd":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, stratify = y)  # , stratify = y - optional","0492279b":"X_train.shape, X_test.shape, y_train.shape, y_test.shape","00a99494":"from sklearn.svm import LinearSVC\nclf = LinearSVC()","bef5126f":"clf.fit(X_train, y_train)","ef8a0626":"y_pred = clf.predict(X_test)","f58bf464":"print(confusion_matrix(y_test, y_pred))","60487b16":"print(classification_report(y_test, y_pred))","7c50d079":"names = news_data.category.unique()","91781be5":"from sklearn.metrics import accuracy_score, confusion_matrix, precision_recall_fscore_support\n\nimport matplotlib.pyplot as plt\n\n","5d1afb70":"cm = confusion_matrix(y_test, y_pred) \n\n# Transform to df for easier plotting\ncm_df = pd.DataFrame(cm,\n                     index = names, \n                     columns = names)\n\nplt.figure(figsize=(12,12))\nsns.heatmap(cm_df, annot=True, cmap = 'viridis', fmt='g')\nplt.title('LinearSVC classifier \\nAccuracy:{0:.3f}'.format(accuracy_score(y_test, y_pred)))\nplt.ylabel('True label')\nplt.xlabel('Predicted label')\nplt.show()","f052c76b":"from sklearn.linear_model import LogisticRegression","7a8ef311":"clf1 = LogisticRegression(solver = 'liblinear')","f35876fb":"clf1.fit(X_train, y_train)","7979f408":"y_pred = clf1.predict(X_test)","e80add4c":"print(classification_report(y_test, y_pred))","3b44d188":"print(confusion_matrix(y_test, y_pred))","d6c886f3":"cm = confusion_matrix(y_test, y_pred) \n\n# Transform to df for easier plotting\ncm_df = pd.DataFrame(cm,\n                     index = names, \n                     columns = names)\n\nplt.figure(figsize=(12,12))\nsns.heatmap(cm_df, annot=True, cmap = 'plasma', fmt='g')\nplt.title('LinearSVC classifier \\nAccuracy:{0:.3f}'.format(accuracy_score(y_test, y_pred)))\nplt.ylabel('True label')\nplt.xlabel('Predicted label')\nplt.show()","dabfab69":"clf_report = classification_report(y_test, y_pred,labels=names,target_names=names,output_dict=True)","caeb57b6":"sns.heatmap(pd.DataFrame(clf_report).iloc[:-1, :].T, annot=True)","650015d4":"tfidf = TfidfVectorizer()\nX = tfidf.fit_transform(news_data['sum'])","b90cfb51":"X = X.toarray()","dfb17a58":"X.shape","507ad37d":"y = news_data['category']","a5b789b9":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 0, stratify = y)","406d1446":"from sklearn.ensemble import RandomForestClassifier","90634153":"X_train.shape, X_test.shape, y_train.shape, y_test.shape","28cae46b":"clf2 = RandomForestClassifier(n_estimators=100, n_jobs= -1)","5699430e":"clf2.fit(X_train, y_train)","213ed9ab":"y_pred = clf2.predict(X_test)","62d70dbe":"print(confusion_matrix(y_test, y_pred))","0c2b9383":"print(classification_report(y_test, y_pred))","f4c4bc67":"cm = confusion_matrix(y_test, y_pred) \n\n# Transform to df for easier plotting\ncm_df = pd.DataFrame(cm,\n                     index = names, \n                     columns = names)\n\nplt.figure(figsize=(12,12))\nsns.heatmap(cm_df, annot=True, cmap = 'magma', fmt='g')\nplt.title('LinearSVC classifier \\nAccuracy:{0:.3f}'.format(accuracy_score(y_test, y_pred)))\nplt.ylabel('True label')\nplt.xlabel('Predicted label')\nplt.show()","c3a073de":"clf1 = LogisticRegression(solver = 'liblinear')","b997ff6f":"clf1.fit(X_train, y_train)","279724b6":"y_pred = clf1.predict(X_test)","31c41bf8":"print(classification_report(y_test, y_pred))","d178cddf":"print(confusion_matrix(y_test, y_pred))","b62b3ba1":"cm = confusion_matrix(y_test, y_pred) \n\n# Transform to df for easier plotting\ncm_df = pd.DataFrame(cm,\n                     index = names, \n                     columns = names)\n\nplt.figure(figsize=(12,12))\nsns.heatmap(cm_df, annot=True, cmap = 'plasma', fmt='g')\nplt.title('LinearSVC classifier \\nAccuracy:{0:.3f}'.format(accuracy_score(y_test, y_pred)))\nplt.ylabel('True label')\nplt.xlabel('Predicted label')\nplt.show()","39d3f643":"# Creating and Training models (supervised and unsupervised)\n***","d2491936":"Now, we need to find predictions from our model for the test data and compare this to the actual categories","432390d8":"## Top 10 words in our predicted clusters - after Mean Shift clustering has been done \n***","071ec6fa":"First, let's check out some more samples with same descriptions","b951015f":"It is a centroid-based algorithm, which works by updating candidates for centroids to be the mean of the points within a given region.  \nThese candidates are then filtered in a post-processing stage to eliminate near-duplicates to form the final set of centroids.  \n  \nIt assigns the data points to the clusters iteratively by shifting points towards the mode - also known as the Mode-seeking algorithm  \n  \nUnlike the popular K-Means cluster algorithm, mean-shift does not require specifying the number of clusters in advance.  \nThe number of clusters is determined by the algorithm with respect to the data.  \n\nNote: The downside to Mean Shift is that it is computationally expensive O(n\u00b2).","df60c988":"Removing multiple spaces - in case of a typo","a34755a3":"### Creating the model with n_clusters = computed clusters (k_clusters) and fitting it to our X data","bcdf1ecf":"Lets see the shape of the X variable - 20000 rows with size 300 for the vectors","ce91081c":"DataFrame with information about headline and description - which is used for tagging, and original category for each row","f4816aa0":"We will just leave in the rows which don't have empty strings for headline or short_description","facbe035":"1 - First lets try with using the spacy model (en_core_web_lg). It is the largest English model of spaCy with size 788 MB.","2703d2f2":"If we take a look at the categories we can see that our dataset is still not balanced enough. This is because of the large difference between the 1st and 10th category entry count (for the first one we have 28942 rows, and for the tenth category we have 5049 rows)","2c1f7406":"We can see that we don't have a balanced set, because the top 3 categories are much more frequent than the other categories (> 15 000 rows), so this could definately create some problems with the accuracy - when printing the report at the end of supervised learning, we should take a look at the accuracy and the recall values together to properly evaluate our model. In the cell below, we will find how many different categories are there.","91afda37":"For this, we will use the classification report and the confusion matrix to evaluate our results","b03d7332":"Do they have the same category? - Yes, it's WELLNESS - 22 rows","9dd630ab":"Sample - returns a random sample of items from an axis of object","be17de45":"Fitting our model with the training data","dd1d5d4f":"Lets now see the look of this for the first row, and also lets see the size of this vector - it is 300","39041d00":"K-Means Clustering algorithm which groups data points into a set amount of clusters  \nIt works well with unlabeled data (like text), but can be pretty computationally expensive  \nWhen doing natural language processing, words must be converted into vectors that machine learning algorithms can make use of","32293e4f":"First, we can notice that we have an empty string as the most repeated short description. The best approach would be to delete these rows in the data, because only a small part of our data is consisted of these cases, so the overall model performance should not be too much affected by it.","b8051666":"Now we fit the model with our training data, which is the X_train and y_train","9f1824ff":"These headlines are all unique","fdc85e44":"We can also take a look at the tail of our data, which is the last 5 rows of the table. We can see that the rows are indexed from 0 to 200852","c3899bc1":"For the classification report below, we can see that the overall accuracy is 0.72, which is not too bad. Also this report shows us the accuracy of the prediction for each category. \n\n##### Precision (also called positive predictive value) is the fraction of relevant instances among the retrieved instances.\n\n##### Recall (also known as sensitivity) is the fraction of retrieved relevant instances among all relevant instances. Both precision and recall are therefore based on an understanding and measure of relevance.\n\n##### F-score or F-measure is a measure of a test's accuracy. It is calculated from the precision and recall of the test, where the precision is the number of correctly identified positive results divided by the number of all positive results, including those not identified correctly, and the recall is the number of correctly identified positive results divided by the number of all samples that should have been identified as positive.","61d32a18":"* Set categories as labels\n* Set data as headline + short_description\n* Splitted train-test data as 80%-20% \n* Working with 20 000 samples, because of memory issues","0d450032":"Next step is to find urls of pages and remove them from the text - for this we need to import library for regular expressions","32ac3b17":"The column 'category' is of most interest to us, since we will try to make predictions that put news articles in the correct categories. Because of this, we used value_counts() function to see how many news articles (of the whole dataset) are in each category.","a2fef6e2":"We can view our results with the confusion_matrix and classification_report.\n\n##### A Confusion matrix is an N x N matrix used for evaluating the performance of a classification model, where N is the number of target classes. The matrix compares the actual target values with those predicted by the machine learning model.\n\n##### The classification report visualizer displays the precision, recall, F1, and support scores for the model.","97fa6785":"We can now see that each category has the same number of rows - 5049","fc344c72":"## Top 10 words in our predicted clusters - after K-means clustering has been done \n***","7988c8cd":"For this, we will use the classification report and the confusion matrix to evaluate our results","22385cd3":"Next, lets check out for this description that is repeated: \"Welcome to the HuffPost Rise Morning Newsbrief, a short wrap-up of the news to help you start your day.\"\n\nIf the headline is the same for every sample, we can just leave one value in our dataframe for this","06841983":"**The goal of this project was to try and train a model to accurately predict the category of the news article topic, by just giving it the title and the short summary of the article. Two approaches were used to achieve this: supervised and unsupervised training of the model.**","61506b8d":"TFIDF (term frequency\u2013inverse document frequency) is a numerical statistic that is intended to reflect how important a word is to a document in a collection  \nTFIDF value increases proportionally to the number of times a word appears in the document","a96c8d34":"**Term Frequency\u2013Inverse Document Frequency**","c6335d82":"Next, we can look for the number of unique headlines - 119181","9b18f5b1":"We will now truncate this dataframe to a one that contains headlines that occur more than once","5700ca8a":"We can see that with this classifier, the overall accuracy is 0.73, but we got a higher recall and precision, so this model seems to perform better","d3492242":"First, we will take a sample of 7 random rows from the dataset and see what we can do with this","cbd534be":"Also, for the confusion matrix, we can notice that we have higher numbers on the diagonal, which confirms that this model did a better job with predictiong the values.","48638305":"We can see that a lot of the articles have no authors named? (=20699)","cea7b5f8":"Lets see these duplicated headlines - duplicated are some weekly news articles, which is fine ('Sunday Roundup' , 'Weekly Roundup'...)","60436b5e":"To have consecutive indices starting from 0","485a0a63":"Select 5 random samples from the test data (not seen by our model)","70ab7005":"Rows with empty headlines - 4 rows - delete these too","71bcfb56":"## Silhouette Score Concepts \n***  \n\nSilhouette score is used to evaluate the quality of clusters created using clustering algorithms such as K-Means in terms of how well samples are clustered with other samples that are similar to each other.  \n\nThe Silhouette Coefficient (sklearn.metrics.silhouette_score) is an example of such an evaluation, where a higher Silhouette Coefficient score relates to a model with better defined clusters.  \nSmall coefficient can mean that the clusters are overlapping.","c63d8ec4":"To successfully apply a classifier to this column, we need to do some reshaping. We will create a new variable X to store the information about the vectors here. This X will be a numpy array type","c2a50fac":"For the confusion_matrix in our case, we can see that we have a 10x10 matrix, where 10 is the number of categories we have. The numbers on the diagonal represent how many news articles from each category were predicted correctly. We can see that we did a pretty good job, except for the wellness and healthy living categories, they seem to be less distinguished by our algorithm.","62cda067":"Our y will be the value we are trying to predict - which is the category of each news article","e70e3b62":"Now, in order to train our model correctly, we will make a new column called 'sum', where we will combine the text from the headline and the short summary into one text. We will then just feed our model with this column from the data to create predictions","9c919ae2":"After observing our dataset and collectiong the most relevant information, we should do some data preprocessing in the next part. This will help us prepare our data and to make our models perfom the best they can.","d9eb238f":"Now we can move on to text preprocessing - we star off by converting all the text to lowercase (for all three columns - 'category', 'headline', 'short_description')","941de837":"We do the splitting of train and test data like before","34b5fd4a":"We can get a list of all column names with the following code line: dataset.columns. \n\nOur columns are: 'category', 'headline', 'authors', 'link', 'short_description', 'date'","b1884552":"Now, we need to find predictions from our model for the test data and compare this to the actual categories","60ea76ac":"We can find the difference between the total number of headlines and unique headlines, so we can find the number of repeated headlines, which is 827","0531f346":"We first start off by reading the data from a json file into a pandas Dataframe","22257438":"We will use a function in Sklearn model selection for splitting data arrays into two subsets: for training data and for testing data. With this function, you don't need to divide the dataset manually.\n\nBy default, Sklearn train_test_split will make random partitions for the two subsets. However, you can also specify a random state for the operation.","d0fde40f":"Lets take a look at the same descriptions now","e9c2c4ba":"Values are the number of accurances","8ec9e52c":"Make a function to remove accented chars - in case of non-english names and surnames","d9dfafe2":"To visualize the most repeated words in the dataset - the headline","d252404b":"Lets check the new results","3c1669b0":"We dropped almost 460 values - duplicates\n\nNow lets see the unique headlines again","e1986f4d":"Fitting our model with the training data","19ca978c":"Next, we need to see the predictions for our test data and store them in a variable called y_pred","713c62de":"We can see that with this classifier, the overall accuracy is 0.73, but we got a higher recall and precision, so this model seems to perform better","51563496":"### Silhouette Score calculation","47966783":"## 2. Mean Shift","96a36918":"# DATA PREPROCESSING","a6cdb784":"Now lets take a look at the length of our dataset - we now have 120008 rows left","86c3d032":"There are 41 different categories with big differences in the number of articles they each have (the top one has 32739 rows, and the last one has 1004, which is 32 times less data). Since we do require more data for this task, we will focus on the categories that have enough articles that the model can learn from. This means that we will work with top 10 categories from this dataset, that have the most news articles in them. This will help us to improve our accuracy and to have a more balanced final version of our dataset.","aae653dd":"Lets go back to examples of same description and different headline, to see what we can do for this case","9b0cc563":"Top 50 most repeated headlines","fcfa317e":"## Top 10 words in our 10 categories - before any training has been done ","60d34d8d":"We can plot a histogram to see how many occurances are in each category","79d0206e":"Lets also take a look at the unique headlines for this short description","68b7f4a7":"Now we can focus on the 'short_description' column, since the text in this column is usually longer than the headline. We will try to find if there are some descriptions that are repeated","02721411":" Next, we can do a contraction to expansion for abbreviated words.","b44d37d3":"## 1. K-Means clustering","4c803322":"# INTRODUCTION","35a639f3":"Indices contain the headlines","c143d4b0":"# Unsupervised\n***","de4477e6":"We will try again with the LogisticRegression classifier, since RandomForest performed really porely in terms of precision and recall","97c68ba4":"First, we create the model","8650ed3e":"Make a function to convert to expansions","4d50ab2b":"To visualize the most repeated words in the dataset - the short_description","3cc36532":"We now have the number of repetitions and the headline in one temporary dataframe","e6b30c12":"Lets take a look at the headlines","db2eec40":"Now, our prediction_df contains the predicted category numbers and the corresponding top labels from our trained models  \nWe can evaluate our predictions by comparing the samples (sample_list column) with the corresponding label values\n","80911eb1":"Apply this function to the headline and short description columns","93218222":"One of the most importaint steps - removing the stopwords\n\n##### Stop Words: A stop word is a commonly used word (such as \u201cthe\u201d, \u201ca\u201d, \u201can\u201d, \u201cin\u201d) that a search engine has been programmed to ignore, both when indexing entries for searching and when retrieving them as the result of a search query. Stopwords are the most common words in any natural language. For the purpose of analyzing text data and building NLP models, these stopwords might not add much value to the meaning of the document.","6656723c":"We need to convert X to an array, so we could feed it in correct shape to the classifier for training","7a1dcb19":"Lets check our shape of the train and test data","6eece8d5":"For the 'Sunday Roundup' they are","dfd1c63e":"We can now keep just one instance of each of these headlines - we will do that later with drop duplicates","fa6ed0a4":"Seems like they do. Lets just keep one instance of these for each duplicated description - we can change this later if the accuracy is too bad. We will make a function for this in the following cells","11ccc7f3":"Now lets see the size of our dataset - 110274 rows","1d90a7df":"Next, a good thing would be to do lemmatization - Convert into root form of word, example running = run\n\nWe will use the nltk library for this","f7699ed6":"# 3) Supervised\n***","05567192":"Apply this function for the headline and short_description columns","751c160f":"We can try to use another type of classifier aswell - A random forest classifier.\n\nA random forest is a meta estimator that fits a number of decision tree classifiers on various sub-samples of the dataset and uses averaging to improve the predictive accuracy and control over-fitting. The sub-sample size is controlled with the max_samples parameter if bootstrap=True (default), otherwise the whole dataset is used to build each tree.","90392d44":"We can see that the headlines are basically the same, just that the date is changining. This means that we can try to remove the dates and see the number of unique values after this","f876db93":"We have 125 rows with this description. Now lets see about the uniqueness of the headlines","435cef52":"Function that takes the headlines from the temporary dataframe, and takes just the first short description wich matches this headline, ignoring the others","b4165ad8":"We can do some exploring of the data by finding out more information - there are 200 853 values in our dataset, all of them are non-null values, but we still need to check for empty strings later on","6e30be9f":"We can try this again with a different classifier - Logistic regression\n\nLogistic Regression (aka logit, MaxEnt) classifier.\n\nIn the multiclass case, the training algorithm uses the one-vs-rest (OvR) scheme if the \u2018multi_class\u2019 option is set to \u2018ovr\u2019, and uses the cross-entropy loss if the \u2018multi_class\u2019 option is set to \u2018multinomial\u2019. (Currently the \u2018multinomial\u2019 option is supported only by the \u2018lbfgs\u2019, \u2018sag\u2019, \u2018saga\u2019 and \u2018newton-cg\u2019 solvers.)","06736cc7":"We will use the stopwords from the spacy library","77472757":"Now that we know what our data roughly looks like, we can take the head of our data, which is the first 5 examples to take a look of the structure of each news article (which is each row in the table)","132c4404":"Now the question is - If we have a same headline and different short descriptions or same short descriptions but different headlines, are the both cases going to result in the same category? If this is the case, we could just leave the headlines and one sample of the description for our model, to avoid redundant data\n\n##### Proposal for future work: we can leave this as it is\n\nFirst lets remove the duplicates where the headlines and the descriptions are the same","508d55cf":"No more empty headlines or empty descriptions","76b0b015":"Removing special chars and punctuation - in case of a typo","2fc0162c":"For our evaluation we will use 2 different classifiers and 2 different word representations - one with word2vec model, and the other one with TfIdf","a9f2b3a0":"### **Elbow method to select number of clusters**  \n\n\n\nThis method looks at the percentage of variance explained as a function of the number of clusters  \nOne should choose a number of clusters so that adding another cluster doesn't give much better modeling of the data    \n***\nThe ***KElbowVisualizer*** implements the \u201celbow\u201d method to select the optimal number of clusters by fitting the model with a range of values for K  \nBy default, the scoring parameter metric is set to ***distortion*** (computes the sum of squared distances from each point to its assigned center)","fc13e8a3":"Now the length of our temporary dataframe is 517","f8c9afad":"Number of unique short descriptions - 109008","d1252e85":"2 - We can now try to train these same type of classifiers but with a different representation of the data - instead of word2vec, we could use TfIdf","1a9a3564":"Rows with empty headline and description - we should delete them (1 row)","e71c1457":"We will create a function that gets the vector representation for the given string input","9840c828":"Now we will create a function that converts the sum column into a string, instead of a list of strings","ef4dfac4":"Also, for the confusion matrix, we can notice that we have higher numbers on the diagonal, which confirms that this model did a better job with predictiong the values.","7fafb6b3":"Lets create a pie chart again to cofirm this - each category takes 10% of the pie chart, which is just what we wanted","5e4c1805":"Do they have the same category? - Yes, it's STYLE & BEAUTY - 91 rows","e7ebca32":"For example, **(prediction_df['sample_list'][12])** : \"***trainers share worst fitness advice heard lots questionable flatout false fitness advice floating gym online tv magazines asked trainers share worst advice heard straight gym floor reported***\" is predicted as category 8 of the kmeans model and category 0 of the Mean Shift model.  \n\nCategory 8 of the kmeans model has the following most common labels (***list(dfs[8]['features'])***):  \n* 'life',\n* 'time',\n* 'people',\n* 'know',\n* 'kids',\n* 'need',\n* 'things',\n* 'like',\n* 'health',\n* 'way'  \n  \nCategory 13 of the Mean Shift model has the following most common labels (***list(ms_dfs[13]['features'])***):  \n* 'new',\n* 'photos',\n* 'time',\n* 'like',\n* 'people',\n* 'day',\n* 'best',\n* 'life',\n* 'love',\n* 'know' ","74d1711b":"Lets check the shape of our X - 20000 rows and each is a vector of size 31801","9f08fb89":"We need to reset the indices again","31378464":"Convert two numpy array to dataframe","95680efc":"We have reduced our categories to have the same number of occurrences by removing random indexed rows","e83c161c":"The computed number of clusters (k_clusters)","240560fe":"Bag-of-words (BOW) model, which is a common way in NLP to create vectors out of text.  \nEach document is represented as a vector.  \n\nThe output is a sparse matrix - a data type that is optimized for matrices with only a few non-zero elements. It only keeps track of the non-zero elements reducing the memory load","9e028e5b":"We will create two new columns in the table, named 'headline_lemmatized_tokenized' and 'short_description_lemmatized_tokenized' where we will store the tokenized and lemmatized text from columns headline and short_description","92c949b5":"Now we can try to find if all of the descriptions for these same headlines are different","9b704519":"We have transformed the data into a string","2b3216a2":"Next, we will only work with data entries whose category column is present in the previous mentioned TOP 10 categories","65f97cb0":"Lets see how our data looks like now","ffcd496f":"* Principal component analysis (PCA) is a fast and flexible unsupervised method for dimensionality reduction in data  \n* The fit learns some quantities from the data, most importantly the \"components\" and \"explained variance\"  \n* fit_transform(X[, y]) - Fit the model with X and apply the dimensionality reduction on X  ","9e09c79f":"y is again the attribute we want to predict - the category of the news articles","25079a57":"# Predictions - Unsupervised learning \n\n  \nEvaluating the performance of an algorithm requires a label that represents the expected value and a predicted value to compare it with  \nUnsupervised learning model - you don\u2019t know what the expected values are and you don\u2019t give labels to the clustering algorithm.","7c567062":"### Visualize the clusters  \n***","8c5c525a":"Do they have the same category? - Yes, it's STYLE & BEAUTY - 27 rows","77121002":"First we need to import necessary libraries for our model creation and training","790b3305":"Best thing to do would be to drop te author, date and the link columns - since they don't give us that much information\n\n##### Proposal for future work: Try and fill it in with known authors for the specific category,We could also plot some features and their corellation to really see how each column is connected to each of the other columns and try to use that information to optimize the models even more","23b83b7d":"Setting up the model","91407587":"Next, we will create a new column in our dataframe, called 'vec', which will contain the vector representation of the 'sum' column","bb2bfd50":"For the first classifier, we will use LinearSVC. \n\n### It is a Linear Support Vector Classification. Similar to SVC with parameter kernel=\u2019linear\u2019, but implemented in terms of liblinear rather than libsvm, so it has more flexibility in the choice of penalties and loss functions and should scale better to large numbers of samples. This class supports both dense and sparse input and the multiclass support is handled according to a one-vs-the-rest scheme.","b6e6ae49":"Extract all unique headlines from the dataframe","f93f70e7":"## Imports and Methods","022bc99b":"Above we can see a pie chart that shows this huge difference in different categories. We will reduce all selected categories to have 5049 occurrences (since that is the number of rows for the 10th category) and work with that dataset for training","ac8e39c3":"Do they have the same category? - Yes, it's WELLNESS - 125 rows","6a6fd030":"Next, we can take a look at the link column and find how many unique links are there - they are all unique","35e0663b":"Lets see the shape of our train and test data"}}