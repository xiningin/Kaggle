{"cell_type":{"c0273bf2":"code","85dedc92":"code","1ed47b5d":"code","20f4f3d9":"code","431d6d90":"code","4f7e2b42":"code","4582f670":"code","f0562d88":"code","2176301c":"code","ca792ebe":"code","556bedb0":"code","f834b08f":"code","4b9dcc5c":"code","0f55a879":"code","5acfe420":"code","d9e8afde":"code","c28a08a6":"code","809bb69b":"code","c243c63c":"code","50e7751d":"code","bcda2206":"code","3c1f0547":"code","4438362b":"code","0b6f6fee":"code","76cb2b4b":"markdown","c167efb1":"markdown"},"source":{"c0273bf2":"!wget https:\/\/github.com\/google-research-datasets\/gap-coreference\/raw\/master\/gap-development.tsv -q\n!wget https:\/\/github.com\/google-research-datasets\/gap-coreference\/raw\/master\/gap-test.tsv -q\n!wget https:\/\/github.com\/google-research-datasets\/gap-coreference\/raw\/master\/gap-validation.tsv -q","85dedc92":"!pip install pytorch-pretrained-bert\n!pip install https:\/\/github.com\/ceshine\/pytorch_helper_bot\/archive\/0.0.4.zip","1ed47b5d":"import os\n\n# This variable is used by helperbot to make the training deterministic\nos.environ[\"SEED\"] = \"420\"\n\nimport logging\nfrom pathlib import Path\n\nimport torch\nimport torch.nn as nn\nimport numpy as np\nimport pandas as pd\nfrom torch.utils.data import Dataset, DataLoader\nfrom pytorch_pretrained_bert import BertTokenizer\nfrom pytorch_pretrained_bert.modeling import BertModel\n\nfrom helperbot import BaseBot, TriangularLR, WeightDecayOptimizerWrapper\n","20f4f3d9":"BERT_MODEL = 'bert-large-uncased'\nCASED = False","431d6d90":"def insert_tag(row):\n    \"\"\"Insert custom tags to help us find the position of A, B, and the pronoun after tokenization.\"\"\"\n    to_be_inserted = sorted([\n        (row[\"A-offset\"], \" [A] \"),\n        (row[\"B-offset\"], \" [B] \"),\n        (row[\"Pronoun-offset\"], \" [P] \")\n    ], key=lambda x: x[0], reverse=True)\n    text = row[\"Text\"]\n    for offset, tag in to_be_inserted:\n        text = text[:offset] + tag + text[offset:]\n    return text\n\ndef tokenize(text, tokenizer):\n    \"\"\"Returns a list of tokens and the positions of A, B, and the pronoun.\"\"\"\n    entries = {}\n    final_tokens = []\n    for token in tokenizer.tokenize(text):\n        if token in (\"[A]\", \"[B]\", \"[P]\"):\n            entries[token] = len(final_tokens)\n            continue\n        final_tokens.append(token)\n    return final_tokens, (entries[\"[A]\"], entries[\"[B]\"], entries[\"[P]\"])\n\nclass GAPDataset(Dataset):\n    \"\"\"Custom GAP Dataset class\"\"\"\n    def __init__(self, df, tokenizer, labeled=True):\n        self.labeled = labeled\n        if labeled:\n            tmp = df[[\"A-coref\", \"B-coref\"]].copy()\n            tmp[\"Neither\"] = ~(df[\"A-coref\"] | df[\"B-coref\"])\n            self.y = tmp.values.astype(\"bool\")\n\n        # Extracts the tokens and offsets(positions of A, B, and P)\n        self.offsets, self.tokens = [], []\n        for _, row in df.iterrows():\n            text = insert_tag(row)\n            tokens, offsets = tokenize(text, tokenizer)\n            self.offsets.append(offsets)\n            self.tokens.append(tokenizer.convert_tokens_to_ids(\n                [\"[CLS]\"] + tokens + [\"[SEP]\"]))\n        \n    def __len__(self):\n        return len(self.tokens)\n\n    def __getitem__(self, idx):\n        if self.labeled:\n            return self.tokens[idx], self.offsets[idx], self.y[idx]\n        return self.tokens[idx], self.offsets[idx]\n    \ndef collate_examples(batch, truncate_len=500):\n    \"\"\"Batch preparation.\n    \n    1. Pad the sequences\n    2. Transform the target.\n    \"\"\"\n    transposed = list(zip(*batch))\n    max_len = min(\n        max((len(x) for x in transposed[0])),\n        truncate_len\n    )\n    tokens = np.zeros((len(batch), max_len), dtype=np.int64)\n    for i, row in enumerate(transposed[0]):\n        row = np.array(row[:truncate_len])\n        tokens[i, :len(row)] = row\n    token_tensor = torch.from_numpy(tokens)\n    # Offsets\n    offsets = torch.stack([\n        torch.LongTensor(x) for x in transposed[1]\n    ], dim=0) + 1 # Account for the [CLS] token\n    # Labels\n    if len(transposed) == 2:\n        return token_tensor, offsets, None\n    one_hot_labels = torch.stack([\n        torch.from_numpy(x.astype(\"uint8\")) for x in transposed[2]\n    ], dim=0)\n    _, labels = one_hot_labels.max(dim=1)\n    return token_tensor, offsets, labels\n\ndef collate_examples_no_labels(batch, truncate_len=500):\n    \"\"\"Batch preparation.\n    \n    1. Pad the sequences\n    2. Transform the target.\n    \"\"\"\n    transposed = list(zip(*batch))\n    print(\"transposed size is\", transposed.size())\n    max_len = min(\n        max((len(x) for x in transposed[0])),\n        truncate_len\n    )\n    tokens = np.zeros((len(batch), max_len), dtype=np.int64)\n    for i, row in enumerate(transposed[0]):\n        row = np.array(row[:truncate_len])\n        tokens[i, :len(row)] = row\n    token_tensor = torch.from_numpy(tokens)\n    # Offsets\n    offsets = torch.stack([\n        torch.LongTensor(x) for x in transposed[1]\n    ], dim=0) + 1 # Account for the [CLS] token\n    # Labels\n    if len(transposed) == 2:\n        return token_tensor, offsets, None\n    one_hot_labels = torch.stack([\n        torch.from_numpy(x.astype(\"uint8\")) for x in transposed[2]\n    ], dim=0)\n    _, labels = one_hot_labels.max(dim=1)\n    return token_tensor, offsets, labels\n\nclass Head(nn.Module):\n    \"\"\"The MLP submodule\"\"\"\n    def __init__(self, bert_hidden_size: int):\n        super().__init__()\n        self.head_hidden_size = 1024\n        self.bert_hidden_size = bert_hidden_size\n        self.fc = nn.Sequential(\n            nn.BatchNorm1d(bert_hidden_size * 3),\n            nn.Dropout(0.5),\n            nn.Linear(bert_hidden_size * 3, self.head_hidden_size),\n            nn.ReLU(),\n            nn.BatchNorm1d(self.head_hidden_size),\n            nn.Dropout(0.5),\n            nn.Linear(self.head_hidden_size, self.head_hidden_size),\n            nn.ReLU(),\n            nn.BatchNorm1d(self.head_hidden_size),\n            nn.Dropout(0.5),\n            nn.Linear(self.head_hidden_size, self.head_hidden_size),\n            nn.ReLU(),\n            nn.BatchNorm1d(self.head_hidden_size),\n            nn.Dropout(0.5),\n            nn.Linear(self.head_hidden_size, 3)\n        )\n        for i, module in enumerate(self.fc):\n            if isinstance(module, (nn.BatchNorm1d, nn.BatchNorm2d)):\n                nn.init.constant_(module.weight, 1)\n                nn.init.constant_(module.bias, 0)\n                print(\"Initing batchnorm\")\n            elif isinstance(module, nn.Linear):\n                if getattr(module, \"weight_v\", None) is not None:\n                    nn.init.uniform_(module.weight_g, 0, 1)\n                    nn.init.kaiming_normal_(module.weight_v)\n                    print(\"Initing linear with weight normalization\")\n                    assert model[i].weight_g is not None\n                else:\n                    nn.init.kaiming_normal_(module.weight)\n                    print(\"Initing linear\")\n                nn.init.constant_(module.bias, 0)\n                \n    def forward(self, bert_outputs, offsets):\n        assert bert_outputs.size(2) == self.bert_hidden_size\n        extracted_outputs = bert_outputs.gather(\n            1, offsets.unsqueeze(2).expand(-1, -1, bert_outputs.size(2))\n        ).view(bert_outputs.size(0), -1)\n        return self.fc(extracted_outputs)\n\n    \nclass GAPModel(nn.Module):\n    \"\"\"The main model.\"\"\"\n    def __init__(self, bert_model: str, device: torch.device):\n        super().__init__()\n        self.device = device\n        if bert_model in (\"bert-base-uncased\", \"bert-base-cased\"):\n            self.bert_hidden_size = 768\n        elif bert_model in (\"bert-large-uncased\", \"bert-large-cased\"):\n            self.bert_hidden_size = 1024\n        else:\n            raise ValueError(\"Unsupported BERT model.\")\n        self.bert = BertModel.from_pretrained(bert_model).to(device)\n        self.head = Head(self.bert_hidden_size).to(device)\n    \n    def forward(self, token_tensor, offsets):\n        token_tensor = token_tensor.to(self.device)\n        bert_outputs, _ =  self.bert(\n            token_tensor, attention_mask=(token_tensor > 0).long(), \n            token_type_ids=None, output_all_encoded_layers=False)\n        head_outputs = self.head(bert_outputs, offsets.to(self.device))\n        return head_outputs            \n\n    \ndef children(m):\n    return m if isinstance(m, (list, tuple)) else list(m.children())\n\n\ndef set_trainable_attr(m, b):\n    m.trainable = b\n    for p in m.parameters():\n        p.requires_grad = b\n\n\ndef apply_leaf(m, f):\n    c = children(m)\n    if isinstance(m, nn.Module):\n        f(m)\n    if len(c) > 0:\n        for l in c:\n            apply_leaf(l, f)\n\n            \ndef set_trainable(l, b):\n    apply_leaf(l, lambda m: set_trainable_attr(m, b))\n    \n    \nclass GAPBot(BaseBot):\n    def __init__(self, model, train_loader, val_loader, *, optimizer, clip_grad=0,\n        avg_window=100, log_dir=\".\/cache\/logs\/\", log_level=logging.INFO,\n        checkpoint_dir=\".\/cache\/model_cache\/\", batch_idx=0, echo=False,\n        device=\"cuda:0\", use_tensorboard=False):\n        super().__init__(\n            model, train_loader, val_loader, \n            optimizer=optimizer, clip_grad=clip_grad,\n            log_dir=log_dir, checkpoint_dir=checkpoint_dir, \n            batch_idx=batch_idx, echo=echo,\n            device=device, use_tensorboard=use_tensorboard\n        )\n        self.criterion = torch.nn.CrossEntropyLoss()\n        self.loss_format = \"%.6f\"\n        \n    def extract_prediction(self, tensor):\n        return tensor\n    \n    def snapshot(self):\n        \"\"\"Override the snapshot method because Kaggle kernel has limited local disk space.\"\"\"\n        loss = self.eval(self.val_loader)\n        loss_str = self.loss_format % loss\n        self.logger.info(\"Snapshot loss %s\", loss_str)\n        self.logger.tb_scalars(\n            \"losses\", {\"val\": loss},  self.step)\n        target_path = (\n            self.checkpoint_dir \/ \"best.pth\")        \n        if not self.best_performers or (self.best_performers[0][0] > loss):\n            torch.save(self.model.state_dict(), target_path)\n            self.best_performers = [(loss, target_path, self.step)]\n            self.logger.info(\"Saving checkpoint %s...\", target_path)\n        else:\n            new_loss_str = self.loss_format % self.best_performers[0][0]\n            self.logger.info(\"This performance:%s is not as a good as our previously saved:%s\", loss_str,new_loss_str )\n        assert Path(target_path).exists()\n        return loss","4f7e2b42":"df_train = pd.read_csv(\"gap-test.tsv\", delimiter=\"\\t\")\ndf_val = pd.read_csv(\"gap-validation.tsv\", delimiter=\"\\t\")\ndf_test = pd.read_csv(\"..\/input\/test_stage_2.tsv\", delimiter=\"\\t\")\nsample_sub = pd.read_csv(\"..\/input\/sample_submission_stage_2.csv\")\nassert sample_sub.shape[0] == df_test.shape[0]","4582f670":"print(len(df_train))\ndf_train.head().transpose()","f0562d88":"print(len(df_test))\ndf_test.head().transpose()","2176301c":"tokenizer = BertTokenizer.from_pretrained(\n    BERT_MODEL,\n    do_lower_case=CASED,\n    never_split = (\"[UNK]\", \"[SEP]\", \"[PAD]\", \"[CLS]\", \"[MASK]\", \"[A]\", \"[B]\", \"[P]\")\n)\n# These tokens are not actually used, so we can assign arbitrary values.\ntokenizer.vocab[\"[A]\"] = -1\ntokenizer.vocab[\"[B]\"] = -1\ntokenizer.vocab[\"[P]\"] = -1","ca792ebe":"train_ds = GAPDataset(df_train, tokenizer)\nval_ds = GAPDataset(df_val, tokenizer)\ntest_ds = GAPDataset(df_test, tokenizer, labeled=False)\ntrain_loader = DataLoader(\n    train_ds,\n    collate_fn = collate_examples,\n    batch_size=20,\n    num_workers=2,\n    pin_memory=True,\n    shuffle=True,\n    drop_last=True\n)\nval_loader = DataLoader(\n    val_ds,\n    collate_fn = collate_examples,\n    batch_size=128,\n    num_workers=2,\n    pin_memory=True,\n    shuffle=False\n)\ntest_loader = DataLoader(\n    test_ds,\n    collate_fn = collate_examples,\n    batch_size=128,\n    num_workers=2,\n    pin_memory=True,\n    shuffle=False\n)","556bedb0":"train_ds","f834b08f":"len(train_loader), len(test_loader), len(val_loader)","4b9dcc5c":"next(iter(test_loader))","0f55a879":"model = GAPModel(BERT_MODEL, torch.device(\"cuda:0\"))\n# You can unfreeze the last layer of bert by calling set_trainable(model.bert.encoder.layer[23], True)\nset_trainable(model.bert, False)\nset_trainable(model.head, True)","5acfe420":"lr=1e-3\nweight_decay=5e-3\noptimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n\nbot = GAPBot(\n    model, train_loader, val_loader,\n    optimizer=optimizer, echo=True,\n    avg_window=25\n)","d9e8afde":"steps_per_epoch = len(train_loader) \nn_steps = steps_per_epoch * 27\nbot.train(\n    n_steps,\n    log_interval=steps_per_epoch \/\/ 4,\n    snapshot_interval=steps_per_epoch,\n    scheduler=TriangularLR(\n        optimizer, 20, ratio=2, steps_per_cycle=n_steps)\n)","c28a08a6":"# Load the best checkpoint\nbot.load_model(bot.best_performers[0][1])","809bb69b":"# Evaluate on the test dataset\n#bot.eval(test_loader)\n#0.48949944019317626","c243c63c":"def predict(loader, *, return_y=False):\n    model.eval()\n    outputs, y_global = [], []\n    with torch.set_grad_enabled(False):\n        for input_tensors in loader:\n            input_tensors = [x.to(model.device) for x in input_tensors if x is not None]\n            outputs.append(bot.predict_batch(input_tensors).cpu())\n        outputs = torch.cat(outputs, dim=0)\n    return outputs","50e7751d":"preds = predict(test_loader)","bcda2206":"len(preds)","3c1f0547":"\nfrom torch.autograd import Variable\n#make_dot(model(test_ds), params=dict(model.named_parameters()))","4438362b":"def exx(row):\n    if(row.A > row.B and row.A > row.NEITHER):\n          row.A = 1\n          row.B = 0\n          row.NEITHER = 0    \n    elif(row.B > row.A and row.B > row.NEITHER):\n          row.A = 0\n          row.B = 1\n          row.NEITHER = 0 \n    else:\n          row.A = 0\n          row.B = 0\n          row.NEITHER = 1\n    return row","0b6f6fee":"# Create submission file\ndf_sub = pd.DataFrame(torch.softmax(preds, -1).cpu().numpy().clip(1e-3, 1-1e-3), columns=[\"A\", \"B\", \"NEITHER\"])\ndf_sub[\"ID\"] = df_test.ID\ndf_sub.to_csv(\"submission.csv\", index=False)\ndf_sub.head()","76cb2b4b":"This is not my work.  It was forked from the pytorch bert baseline with .54 score.\n\nI have messed around with epochs, weight decay, and adding another fully connected layer in the head.","c167efb1":"\"pytorch_helper_bot\" is a thin abstraction of some common PyTorch training routines. It can easily be replaced, so you can mostly ignore it and focus on the preprocessing and model definition instead."}}