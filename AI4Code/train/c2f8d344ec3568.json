{"cell_type":{"ca44c653":"code","d28544ce":"code","850a0a55":"code","28194999":"code","856844ce":"code","99a91eda":"code","7a4e2908":"code","d4464d39":"code","c49d2648":"code","7e2211a7":"code","a9780437":"code","ee86696b":"code","58a6367b":"code","0cf0e2ea":"code","5a4412f4":"code","22a23546":"code","d8731498":"code","6cfaf8cb":"code","9012f18c":"code","c52de6c4":"code","484aa305":"code","fc876c6d":"code","42f58f22":"code","798bf1d2":"code","1c1e36ed":"code","6a54f233":"code","e4aa04b4":"code","98a82f0c":"code","cc294ed7":"code","7c638d2f":"code","58629ac7":"code","badc9505":"code","bec33c8b":"code","00a09fe6":"code","5fb3e1b0":"code","b997fa5e":"code","7cd6f30a":"code","d744899e":"code","4e70f128":"code","2f397b16":"code","426fb6ef":"code","95c5fed5":"code","a4bfc8b2":"code","0409dc0f":"code","eb1c1d49":"code","89481ece":"code","3937bf3c":"code","188c9147":"code","7224b3fc":"code","7feb1ff9":"code","7a506705":"code","77ff9063":"code","34f3b45d":"code","3df981c2":"code","ae2b8ec3":"code","1afdb8b5":"code","97bf9b06":"code","30818e11":"code","4c0673c6":"code","c049b97f":"code","1053b321":"code","1ea8d73e":"code","6e1ee5de":"code","d6eb0783":"code","6b721e0c":"code","d12faed3":"code","9d2f4dcc":"code","b70592ea":"code","5a2fa9fc":"code","693c99d1":"code","a81d88b7":"code","8d9faae0":"code","cdca986e":"code","ec9b5ec2":"code","db7d0823":"code","b9abc1db":"code","38c897e3":"code","a90f6f32":"code","c918f129":"code","e9c05c8d":"code","4473a942":"code","b8f61d71":"code","fa82d5a9":"code","467c95db":"code","7837da80":"code","fc5d8421":"code","a8388930":"code","ea9d3b96":"code","fbc08fc5":"code","1a79f7f1":"code","014d1765":"code","a63b1ffa":"code","5bf1917d":"code","309e17d9":"code","0cab66e6":"code","58b8e63e":"code","852d0b07":"code","2dfcc1ff":"code","a102375c":"code","2f3674bb":"code","40d604fe":"code","c2396742":"code","62060217":"code","9b6cd135":"code","48e2de3e":"code","5cc19db2":"code","378539db":"code","af94f0ec":"code","9141757b":"code","a5544ab0":"code","dfbd4019":"code","5b854f21":"code","ef68155b":"code","a4e80246":"code","55293026":"code","8e2e8d4c":"code","3b99ce71":"code","1de670e8":"code","aa84c529":"code","d120e307":"code","b97a78b2":"code","8cc59197":"code","f11fb1f5":"code","85929087":"code","543bb403":"code","5a8447c6":"code","01a6d6b0":"code","7de09dbe":"code","0c05f06c":"markdown","74b1c9b5":"markdown","46b18304":"markdown","f2e0cbd9":"markdown","2b8e73c6":"markdown","bfd2e6e3":"markdown","c8763cb4":"markdown","1085a55a":"markdown","f038afdd":"markdown","c6212b1d":"markdown","07651f46":"markdown","47dbb098":"markdown","4ae77570":"markdown","8ae2741c":"markdown","7548863c":"markdown","97064fb5":"markdown","621edecd":"markdown","063ad0bb":"markdown","a3607aae":"markdown","4918e289":"markdown","4c1553b8":"markdown","1baeb84e":"markdown","6960805d":"markdown","b5aa605c":"markdown","e959ef7e":"markdown","119e87f3":"markdown","218071f9":"markdown","2bc6aecf":"markdown","943f3230":"markdown","cc1fee72":"markdown","ca7a96bb":"markdown","3ebb7570":"markdown","e44905dd":"markdown","e55a708d":"markdown","ae1ab921":"markdown","21095e1a":"markdown","2f02464c":"markdown","6ae97b92":"markdown","275979f2":"markdown","d996cf46":"markdown","9a3eb990":"markdown","49bf76ea":"markdown","92de2194":"markdown","6f6468e4":"markdown","1bcc5bf4":"markdown","0f408d02":"markdown","78b35940":"markdown","d080e473":"markdown","8c3fa241":"markdown","d425c773":"markdown","7bf0409e":"markdown","7171aa17":"markdown","67e67295":"markdown","6b9a4790":"markdown","4e420ef2":"markdown","4059f2c1":"markdown","4e2be74d":"markdown","b8b1215b":"markdown","f4a63ce1":"markdown","e59a4a51":"markdown","9fcefdc1":"markdown","e4d2ed1f":"markdown"},"source":{"ca44c653":"import pandas as pd\nimport seaborn as sns\nfrom colorama import Fore\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport re\nimport scikitplot as skplt \n\nfrom sklearn.feature_selection import VarianceThreshold\nfrom sklearn.decomposition import PCA\nfrom sklearn.model_selection import train_test_split as split\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.metrics import accuracy_score, confusion_matrix, classification_report, f1_score\nfrom sklearn.linear_model import LogisticRegression, LogisticRegressionCV\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.ensemble import VotingClassifier, BaggingClassifier, AdaBoostClassifier, GradientBoostingClassifier, RandomForestClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\n\nfrom sklearn.cluster import KMeans\nfrom sklearn.preprocessing import StandardScaler, MinMaxScaler\n\nimport xgboost as xgb\n\nfrom IPython.display import Image\n%matplotlib inline\n\nimport warnings\nwarnings.filterwarnings('ignore')\n","d28544ce":"def report(y_true, y_pred, labels):\n    f1 = f1_score(y_true=y_true, y_pred=y_pred, labels=labels, average='macro', pos_label=1)\n    \n    acc = accuracy_score(y_true=y_true, y_pred=y_pred)\n    \n    cm = pd.DataFrame(confusion_matrix(y_true=y_true, \n                                       y_pred=y_pred), \n                      index=labels, \n                      columns=labels)\n    rep = classification_report(y_true=y_true, \n                                y_pred=y_pred)\n    \n    return (f'F1 Macro = {f1:.4f}\\n\\nAccuracy = {acc:.4f}\\n\\nConfusion Matrix:\\n{cm}\\n\\nClassification Report:\\n{rep}')","850a0a55":"data = pd.read_csv('..\/input\/train.csv', index_col='idhogar')\ndata = data.rename(columns = {'v2a1':'rent','hacdor':'crowd_bedroom', 'hacapo': 'crowd_room', 'v14a': 'bathroom' })\nprint(data.shape)\ndata.tail(7)","28194999":"data.info(verbose=False)","856844ce":"data.describe()","99a91eda":"data = data.reset_index()\ndata.Target.value_counts(normalize=True)","7a4e2908":"print((data['agesq'] == data['SQBage']).all())\nprint((data['tamhog'] == data['hhsize']).all())\nprint((data['hhsize'] == data['hogar_total']).all())\ndata.drop(columns=['agesq', 'tamhog','hhsize'], inplace=True)\ndata.shape","d4464d39":"issues_list = ['epared', 'etecho', 'elimbasu', 'eviv', 'tipovivi', 'area'] #good, normal, bad etc.\n\nfor issue in issues_list:\n\n    col_list = list(data.columns[data.columns.str.startswith(issue)])\n    df = data[col_list]\n    \n    s = pd.Series(df.columns[np.where(df!=0)[1]])\n    s = s.apply(lambda x: x[-1]).astype('int64')\n    s = s.rename(issue+'_cat', copy=True)\n    \n    data = pd.concat([data, s], axis=1)\n    data = data.drop(columns=col_list)\ndata.head()","c49d2648":"data.shape","7e2211a7":"#educational level\n\ndef instlevel_cat(row):\n    col_list = list(data.columns[data.columns.str.startswith('instlevel')])\n    \n    for i in col_list:\n        if row[i] == 1:\n            return (int(i[-1]))","a9780437":"# water provision\n\ndef water_cat(row):\n    if row.abastaguadentro == 1:\n        return 1\n    if row.abastaguafuera == 1:\n        return 2\n    if row.abastaguano == 1:\n        return 3\n\n# source of energy used for cooking\ndef energcocinar_cat(row):\n    if row.energcocinar1 == 1:\n        return 1\n    if row.energcocinar4 == 1:\n        return 2\n    if row.energcocinar3 == 1:\n        return 3\n    if row.energcocinar2 == 1:\n        return 4","ee86696b":"data['water_cat'] = data.apply(water_cat, axis=1)\ndata['energcocinar_cat'] = data.apply(energcocinar_cat, axis=1)\ndata['instlevel_cat'] = data.apply(instlevel_cat, axis=1)\n\ndata.shape","58a6367b":"print('nulls: ', data['instlevel_cat'].isnull().sum())","0cf0e2ea":"data[data['instlevel_cat'].isnull()]['escolari']","5a4412f4":"data.instlevel_cat.fillna(1, inplace=True)","22a23546":"import re\n\ncolumns_list = list(data.columns)\ncol_drop = []\n\nfor i in columns_list:\n    if re.match(\"instlevel\\d\", i) or re.match(\"abastagua\", i) or re.match(\"energcocinar\\d\", i):\n        col_drop.append(i)\nprint(col_drop)","d8731498":"print('Before:',data.shape)\ndata.drop(columns=col_drop, inplace=True)\nprint('After:',data.shape)","6cfaf8cb":"data[['female', 'male']].head(3)","9012f18c":"data = data.rename(columns={'female': 'sex'}).drop(columns='male')\ndata.shape","c52de6c4":"data[['dependency','edjefe','edjefa']].head(5)\n","484aa305":"for col in ['dependency','edjefe','edjefa']:\n    print(col.capitalize(),':\\n', data[col].unique(),'\\n')\n    ","fc876c6d":"for col in ['dependency','edjefe','edjefa']:\n    data[col].replace({'yes': 1, 'no': 0 }, inplace=True)\n    data[col] = data[col].astype('float64')\n    if data[col].dtype == 'float64':\n        print(col,': fixed')","42f58f22":"columns_list = list(data.columns)\nprint(len(columns_list))\nprint(columns_list)","798bf1d2":"print('Before:',data.shape)\n\ncol_drop = []\n\nfor i in columns_list:\n    if re.match(\"SQB\", i):\n        col_drop.append(i)\nprint(col_drop)\n\ndata.drop(columns=col_drop, inplace=True)\n\nprint('After:',data.shape)","1c1e36ed":"data.isnull().sum().sort_values(ascending=False).head(7)","6a54f233":"data.rez_esc.value_counts()","e4aa04b4":"ages = list(data[data['rez_esc'].isnull()]['age'].unique())\nprint(sorted(ages))","98a82f0c":"data[['rez_esc', 'age']].head()","cc294ed7":"data['rez_esc'] = data['rez_esc'].fillna(0)","7c638d2f":"data.isnull().sum().sort_values(ascending=False).head()","58629ac7":"data.v18q1.describe(np.arange(0,1.1,0.2))","badc9505":"data.v18q1.value_counts()","bec33c8b":"data[['v18q1', 'v18q']].head()","00a09fe6":"print(data[data['v18q1'].isnull()]['v18q'].sum())\ndata['v18q1'] = data['v18q1'].fillna(0)","5fb3e1b0":"data.isnull().sum().sort_values(ascending=False).head()","b997fa5e":"data[['rent', 'tipovivi_cat']].head() #own the house or no house to pay rent for","7cd6f30a":"data[data['rent'].isnull()]['tipovivi_cat'].unique()","d744899e":"data['rent'] = data['rent'].fillna(0)","4e70f128":"data.isnull().sum().sort_values(ascending=False).head()","2f397b16":"meaneduc_df = data.groupby(['idhogar', 'Id'])['age', 'escolari'].sum()\nmeaneduc_df.head()","426fb6ef":"def fix_meaneduc(df): \n    counter=0\n    escolari=0\n    \n    list_age = list(df.age) \n    list_escolari = list(df.escolari) \n    \n    for i in range(len(list_age)):\n        if (list_age[i] >= 18):\n            counter +=1\n            escolari += list_escolari[i]\n    if counter==0:\n        return 0\n    return (escolari\/counter)\n    ","95c5fed5":"meaneduc_new = data.groupby('idhogar').apply(fix_meaneduc)\nmeaneduc_new.isnull().any()\n","a4bfc8b2":"meaneduc_new.head() #series, each row is for idhogar","0409dc0f":"print(meaneduc_new.shape) \nprint((data.parentesco1==1).sum()) ","eb1c1d49":"data[data['meaneduc'].isnull()]","89481ece":"# family example:\nprint(data.shape)\ndata[data['idhogar'] == '2b58d945f']","3937bf3c":"data = data.drop(columns=['meaneduc','mobilephone', 'v18q'])\ndata.shape","188c9147":"# Houseowners Example:\nowners_data = data[data['parentesco1']==1]\nprint(owners_data.shape)\nowners_data.head()","7224b3fc":"train, test = split(owners_data, test_size=0.3, random_state=44, stratify=owners_data.Target)\ntrain.shape, test.shape","7feb1ff9":"train.Target.value_counts(normalize=True)","7a506705":"rf = RandomForestClassifier(class_weight='balanced', n_estimators=300, \n                            max_depth=30, min_samples_leaf=10, max_features=30, random_state=111)\n\nX = train.drop(columns=['Target','Id','idhogar'])\ny = train.Target\n\nparams = {'max_depth': range(25,30),\n          'min_samples_leaf': range(7,10)}\n\ngs = GridSearchCV(rf, params, cv=5, scoring='f1_macro')\ngs.fit(X,y)","77ff9063":"model1 = gs.best_estimator_\ngs.best_params_","34f3b45d":"X_test = test.drop(columns=['Target','Id','idhogar'])\ny_test = test.Target\n\nprint(Fore.BLUE+report(y_test, model1.predict(X_test), model1.classes_))\n","3df981c2":"my_list = list(zip(model1.feature_importances_ ,X.columns))\nmy_list.sort(key=lambda tup: tup[0],reverse=True)\n# for item in my_list:\n#     if item[0]> 0.01:\n#         print(item)","ae2b8ec3":"importances = pd.DataFrame(data=None,columns=['importance', 'feature'])\n\nimportance_l = []\nfeature_l = []\n\nfor t in my_list:\n    importance_l.append(t[0])\n    feature_l.append(t[1])\n    \nimportances['importance'] = importance_l\nimportances['feature'] = feature_l","1afdb8b5":"plt.figure(figsize=(20,5))\nplt.xticks(rotation=90)\nplt.title('Feature Importances')\nsns.barplot(x=\"feature\", y=\"importance\", data=importances)","97bf9b06":"owners_data_only = owners_data.copy()","30818e11":"# adding meaneduc_new\nowners_data = owners_data.merge(meaneduc_new.to_frame(), left_on='idhogar', right_index=True)\\\n                        .rename(index=str, columns={0: \"meaneduc_new\"})\n\nowners_data.head()","4c0673c6":"print(list(data.columns))","c049b97f":"# Individual Features:\n\nfamily_data = data[['idhogar', 'Id','escolari', 'rez_esc','dis','instlevel_cat','age',\n                           'estadocivil1', 'estadocivil2', 'estadocivil3', 'estadocivil4', \n                           'estadocivil5', 'estadocivil6', 'estadocivil7', \n                           'parentesco1', 'parentesco2', 'parentesco3', 'parentesco4', \n                           'parentesco5', 'parentesco6', 'parentesco7', 'parentesco8', \n                           'parentesco9', 'parentesco10', 'parentesco11', 'parentesco12']]\nprint(owners_data.shape, family_data.shape)\nfamily_data.head()","1053b321":"family_data[family_data.idhogar == '2b58d945f']","1ea8d73e":"print(f'There are {(len(family_data.idhogar.unique()))-(len(owners_data.idhogar.unique()))} family members with no related homeowner')\n","6e1ee5de":"wrong_idhogars = []\nfor id_ in list(family_data.idhogar.unique()):\n    if id_ not in owners_data.idhogar.unique():\n        wrong_idhogars.append(id_)\nprint(wrong_idhogars)","d6eb0783":"print(family_data.shape)\nfor id_ in wrong_idhogars:\n    family_data = family_data[family_data['idhogar'] != id_]\nprint('Number of family members in the dataset (rows): ',family_data.shape)","6b721e0c":"print('Number of families in the dataset: ',owners_data.shape)\n\nowners_data = owners_data.sort_values(by='idhogar')\nfamily_data = family_data.sort_values(by='idhogar')\n\ndf = family_data.groupby('idhogar')","d12faed3":"#school min, max and mean per family\n\nowners_data['escolari_max_fam'] = np.array(df.escolari.max())\nowners_data['escolari_min_fam'] = np.array(df.escolari.min())\nowners_data['escolari_mean_fam'] = np.array(df.escolari.mean())\n\nowners_data.shape","9d2f4dcc":"#age min, max and mean per family\n\nowners_data['age_max_fam'] = np.array(df.age.max())\nowners_data['age_min_fam'] = np.array(df.age.min())\nowners_data['age_mean_fam'] = np.array(df.age.mean())\nowners_data['age_median_fam'] = np.array(df.age.median())\n\nowners_data.shape","b70592ea":"#rez_esc max and mean per family\n\nowners_data['rez_esc_max'] = np.array(df.rez_esc.max())\nowners_data['rez_esc_mean'] = np.array(df.rez_esc.mean())\n\nowners_data.shape","5a2fa9fc":"# sum of disabled in the family\nowners_data['dis_fam'] = np.array(df.rez_esc.sum())\nowners_data.shape","693c99d1":"#mean of family members that studied in each education level:\nowners_data['instlevel_mean_fam'] = np.array(df.instlevel_cat.mean())\nowners_data.shape","a81d88b7":"col_names = pd.Series(owners_data.columns)\nprint(list(col_names))","8d9faae0":"#sum of family members that are married, divorced etc.:\n\ncol_list = owners_data.columns[col_names.str.startswith('estadocivil')]\n\nfor col in col_list:\n    owners_data[col+'_fam'] = np.array(df[col].sum())\n\nprint(owners_data.shape)\nowners_data.head()","cdca986e":"#sum of family members in each family category (spouse\/partner, son\/doughter, mother\/father etc.):\n\ncol_names = pd.Series(owners_data.columns)\n#col_names\ncol_list = owners_data.columns[col_names.str.startswith('parentesco')]\nfor col in col_list:\n    owners_data[col+'_fam'] = np.array(df[col].sum())\n\nprint(owners_data.shape)\nowners_data.head()","ec9b5ec2":"print(list(owners_data.columns))","db7d0823":"print('Before:',owners_data.shape)\n\nowners_data.drop(columns=['Id','rez_esc',\n                   'estadocivil1', 'estadocivil2', 'estadocivil3', 'estadocivil4', \n                   'estadocivil5', 'estadocivil6', 'estadocivil7', \n                   'parentesco1','parentesco1_fam','parentesco2', 'parentesco3', 'parentesco4', 'parentesco5',\n                   'parentesco6', 'parentesco7', 'parentesco8', 'parentesco9', 'parentesco10',\n                   'parentesco11', 'parentesco12' ], inplace=True)\n\nprint('After:',owners_data.shape)","b9abc1db":"owners_data = owners_data.set_index('idhogar')\nprint(owners_data.shape)\nowners_data.head()","38c897e3":"owners_data_all_features = owners_data.copy()\nowners_data.tail()","a90f6f32":"owners_data['instlevel_cat*age_mean_fam'] = owners_data['instlevel_cat']*owners_data['age_mean_fam']\nowners_data['instlevel_cat*escolari_mean_fam'] = owners_data['instlevel_cat']*owners_data['escolari_mean_fam']\nowners_data['v18q1\/r4t2'] = owners_data['v18q1']\/owners_data['r4t2'] #tablets\/people\nowners_data['qmobilephone\/r4t2'] = owners_data['qmobilephone'] \/  owners_data['r4t2'] #phones\/people\n\nowners_data['SQescolari_mean_fam'] = np.square(owners_data['escolari_mean_fam'])\nowners_data['SQdependency'] = np.square(owners_data['dependency'])\nowners_data['SQmeaneduc_new'] =  np.square(owners_data['meaneduc_new'])\nowners_data['SQescolari_mean_fam'] = np.square(owners_data.escolari_mean_fam)\nowners_data['SQescolari'] = np.square(owners_data['escolari'])\nowners_data['SQage'] = np.square(owners_data.age)\nowners_data['SQhogar_total'] = np.square(owners_data['hogar_total'])\nowners_data.shape","c918f129":"plt.figure(figsize=(25,7))\nowners_data.nunique().sort_values(ascending=False).plot(kind='bar')","e9c05c8d":"corrmat = owners_data.corr()\ncols = corrmat.nlargest(30, 'Target')['Target'].index\ncm = np.corrcoef(owners_data[cols].values.T)\nsns.set(font_scale=1.5)\nplt.figure(figsize=(20, 20))\n\nhm = sns.heatmap(cm, cbar=False, annot=True, square=True, fmt='.2f', annot_kws={'size': 10}, yticklabels=cols.values, xticklabels=cols.values)\nplt.yticks(rotation=0)\nplt.xticks(rotation=90)\nplt.show()","4473a942":"plt.figure(figsize=(12,7))\nplt.title('Mean Years of Education per Family by Poverty Level')\nax = sns.boxplot(x=\"Target\", y=\"escolari_mean_fam\", data=owners_data)","b8f61d71":"plt.figure(figsize=(12,7))\nplt.title('Median age per Family by Poverty Level')\nax = sns.boxplot(x=\"Target\", y=\"age_median_fam\", data=owners_data)","fa82d5a9":"colors = {1: 'b', 2:'orange', 3: 'g', 4:'r'}\nplt.figure(figsize = (12, 3))\n\n# overcrowding - persons per room\n# meaneduc_new - mean years of education of adults (>=18)\n\nfor i, col in enumerate(['overcrowding', 'meaneduc_new']):\n    ax = plt.subplot(2, 1, i + 1)\n    \n    for poverty_level, color in colors.items():\n        sns.kdeplot(owners_data.loc[owners_data['Target'] == poverty_level, col], \n                     ax = ax, color = color, label = poverty_level)\n        \n    plt.title(f'{col.capitalize()} Distribution'); plt.xlabel(f'{col}'); plt.ylabel('Density')\nplt.subplots_adjust(top = 4)","467c95db":"# from target, how many appeared in each instlevel\n\nmap_df = pd.crosstab(columns=owners_data.Target, \n                          index=owners_data.instlevel_cat, \n                          normalize='columns')\n\nfig, ax = plt.subplots(figsize=(10, 5))\nsns.heatmap(ax=ax, data=map_df, cmap='coolwarm')","7837da80":"X = owners_data.drop(columns='Target')\ny = owners_data.Target","fc5d8421":"X_train, X_test, y_train, y_test = split(X,y, random_state=555, test_size=0.3, stratify = y)\nX_train.shape, X_test.shape, y_train.shape, y_test.shape","a8388930":"scaler = MinMaxScaler()\nX_train_scaled = scaler.fit_transform(X_train)\nX_test_scaled = scaler.transform(X_test)","ea9d3b96":"train_scores = []\n# test_scores = []\nfor k in range(1, X.shape[1]):\n    scaled_pca_transformer = PCA(n_components=k).fit(X_train_scaled)\n        \n    X_train_scaled_pca = scaled_pca_transformer.transform(X_train_scaled)\n        \n    clf = KNeighborsClassifier().fit(X_train_scaled_pca, y_train)\n    \n    X_test_scaled_pca = scaled_pca_transformer.transform(X_test_scaled)\n    \n    train_scores.append(clf.score(X_train_scaled_pca, y_train))\n#    test_scores.append(clf.score(X_test_scaled_pca, y_test))","fbc08fc5":"plt.figure(figsize=(20,7))\n# plt.plot(list(zip(train_scores, test_scores)), linewidth=5)\nplt.plot(train_scores, linewidth=5)\nplt.xticks(ticks=np.arange(0, len(X.columns)+1, 2.0))\nplt.title('Model score vs. number of components')\nplt.xlabel('n_components')\nplt.ylabel('Score (accuracy)')\nplt.legend(['train score', 'test score'], loc='best')","1a79f7f1":"pca_transformer = PCA(n_components=2).fit(X_train_scaled)\nX_train_scaled_pca = pca_transformer.transform(X_train_scaled)\nX_test_scaled_pca = pca_transformer.transform(X_test_scaled)\nX_train_scaled_pca[:1]","014d1765":"plt.figure(figsize=(15,7))\nsns.scatterplot(x=X_train_scaled_pca[:, 0], \n                y=X_train_scaled_pca[:, 1], \n                hue=y_train, \n                sizes=100,\n                palette=\"Accent\") ","a63b1ffa":"X = owners_data.drop(columns='Target')\ny = owners_data.Target","5bf1917d":"X_train, X_test, y_train, y_test = split(X, y, random_state=555, test_size=0.3, stratify=y)","309e17d9":"print('train:')\nprint(y_train.value_counts(normalize=True))\nprint('test:')\nprint(y_test.value_counts(normalize=True))","0cab66e6":"scaler = MinMaxScaler()\nX_train_scaled = scaler.fit_transform(X_train)\nX_test_scaled = scaler.transform(X_test) \n\nX_train_scaled = pd.DataFrame(X_train_scaled, index=X_train.index, columns=X_train.columns)\nX_test_scaled = pd.DataFrame(X_test_scaled,index=X_test.index, columns=X_test.columns)\nX_train_scaled.head()","58b8e63e":"selector = VarianceThreshold(0.008)\nselector.fit(X_train_scaled)","852d0b07":"print(f'There are {len(X_train_scaled.columns[selector.get_support()])} columns left')","2dfcc1ff":"dropped_features = X_train_scaled.columns[~selector.get_support()]\ndropped_features","a102375c":"selected_data_train = selector.transform(X_train_scaled)\nselected_data_test = selector.transform(X_test_scaled)\n\n\nselected_data_train = pd.DataFrame(selected_data_train, \n                             columns=X_train_scaled.columns[selector.get_support()])\nselected_data_test = pd.DataFrame(selected_data_test, \n                             columns=X_test_scaled.columns[selector.get_support()])\n\nselected_data_train.sample(5)","2f3674bb":"pca_transformer = PCA(n_components=50).fit(selected_data_train)\nX_train_scaled_pca = pca_transformer.transform(selected_data_train)\nX_test_scaled_pca = pca_transformer.transform(selected_data_test)","40d604fe":"classifiers = [('LR', LogisticRegression(solver='lbfgs', multi_class='auto', \n                            max_iter=1000, class_weight='balanced',\n                            random_state=555)), \n               ('KNC', KNeighborsClassifier(n_neighbors=5, metric='manhattan'))] #no class_weights\n\n\nmodel2 = VotingClassifier(estimators=classifiers, voting='soft')\nmodel2.fit(X_train_scaled_pca, y_train)","c2396742":"print(Fore.BLUE+report(y_test, model2.predict(X_test_scaled_pca), model2.classes_))","62060217":"classifiers = [('LR', LogisticRegression(solver='lbfgs', multi_class='auto', \n                            max_iter=1000, class_weight='balanced',\n                            random_state=555)), \n               ('KNC', KNeighborsClassifier(n_neighbors=5, metric='manhattan'))] #no class_weights\n\n\nmodel4 = VotingClassifier(estimators=classifiers, voting='soft')\nmodel4.fit(selected_data_train, y_train)","9b6cd135":"print(Fore.BLUE+report(y_test, model4.predict(selected_data_test), model4.classes_))","48e2de3e":"dt = DecisionTreeClassifier(max_depth=5, class_weight='balanced', random_state=555 )\n\nmodel8 = AdaBoostClassifier(base_estimator=dt, learning_rate=0.01, n_estimators=100, random_state=222)\nmodel8.fit(selected_data_train ,y_train)","5cc19db2":"print(Fore.BLUE+report(y_test, model8.predict(selected_data_test), model8.classes_))","378539db":"svc = SVC(class_weight='balanced',random_state=222, kernel='rbf', C= 1.0, gamma='auto')\n\nparams = {'kernel': ['linear', 'poly', 'rbf', 'sigmoid'],\n          'C': [0.1, 1.0, 10]}\n\ngs = GridSearchCV(svc, params, cv=3, return_train_score=False, scoring='f1_macro')\ngs.fit(selected_data_train, y_train)","af94f0ec":"print(gs.best_params_)\nsvc_best = gs.best_estimator_","9141757b":"model10 = BaggingClassifier(base_estimator=svc_best, n_estimators=100, n_jobs=4, max_features=30, random_state=555)\n\nmodel10.fit(selected_data_train, y_train)","a5544ab0":"print(Fore.BLUE+report(y_test, model10.predict(selected_data_test), model10.classes_))","dfbd4019":"lr = LogisticRegression(solver='lbfgs', multi_class='auto', \n                            max_iter=10000, class_weight='balanced',\n                            random_state=555)\nparams = {'C': [0.1, 1.0, 10]}\n\ngs = GridSearchCV(lr, params, scoring='f1_macro', cv=5, return_train_score=False)\ngs.fit(selected_data_train, y_train)","5b854f21":"gs.best_params_","ef68155b":"model5 = gs.best_estimator_\nprint(Fore.BLUE+report(y_test, model5.predict(selected_data_test), model5.classes_))","a4e80246":"rf = RandomForestClassifier(class_weight='balanced', n_estimators=50000, #max_depth=30, \n                            min_samples_leaf=5, max_features=30, n_jobs=4, random_state=222)\n\nparams = {#'max_depth': range(25,30),\n          'min_samples_leaf': range(5,7)}\n\ngs = GridSearchCV(rf, params, cv=5, return_train_score=False, scoring='f1_macro')\ngs.fit(selected_data_train,y_train)","55293026":"print(gs.best_params_)\nmodel6 = gs.best_estimator_","8e2e8d4c":"print(Fore.RED+report(y_train, model6.predict(selected_data_train), model6.classes_))","3b99ce71":"print(Fore.BLUE+report(y_test, model6.predict(selected_data_test), model6.classes_))","1de670e8":"grid_results = pd.DataFrame(gs.cv_results_)\ngrid_results.sort_values(by='rank_test_score').head(4)","aa84c529":"df_scores = grid_results.sort_values(by='rank_test_score')[['params', 'mean_test_score', 'std_test_score' ]]\n\ndf_scores[['mean_test_score',  'std_test_score']].plot(kind='scatter', x='mean_test_score', y='std_test_score', \n                                                       color='lightblue', figsize=(10,5))\n\nP = [df_scores.iloc[0,1] , df_scores.iloc[0,2]]\nplt.plot(P[0], P[1], marker='o', markersize=5, color=\"darkblue\")","d120e307":"my_list = list(zip(model6.feature_importances_ ,selected_data_test.columns))\nmy_list.sort(key=lambda tup: tup[0],reverse=True)\n# for item in my_list:\n#     if item[0]> 0.009:\n#         print(item)","b97a78b2":"importances = pd.DataFrame(data=None,columns=['importance', 'feature'])\n\nimportance_l = []\nfeature_l = []\n\nfor t in my_list:\n    importance_l.append(t[0])\n    feature_l.append(t[1])\n    \nimportances['importance'] = importance_l\nimportances['feature'] = feature_l","8cc59197":"plt.figure(figsize=(20,5))\nplt.xticks(rotation=90)\nplt.title('Feature Importances')\nsns.barplot(x=\"feature\", y=\"importance\", data=importances)","f11fb1f5":"model13 = KMeans(n_clusters=4).fit(selected_data_train)\ny_pred = model13.predict(selected_data_train)\nprint(f1_score(y_true=y_train, y_pred=y_pred, average='macro',labels=np.unique(y_pred)))","85929087":"print(f1_score(y_true=y_test, \n               y_pred=model13.predict(selected_data_test), \n               average='macro', \n               labels=np.unique(model13.predict(selected_data_test))))","543bb403":"selected_data_train1 = selected_data_train.copy()\nselected_data_test1= selected_data_test.copy()\n\nmodel13 = KMeans(n_clusters=4).fit(selected_data_train1)\n\nselected_data_train1['clustering'] = model13.predict(selected_data_train1)\nselected_data_test1['clustering'] = model13.predict(selected_data_test1)\n","5a8447c6":"rf = RandomForestClassifier(class_weight='balanced', n_estimators=50000, \n                            min_samples_leaf=5, max_features=30, n_jobs=4, random_state=222)\n\nparams = {#'max_depth': range(25,30),\n          'min_samples_leaf': range(5,7)}\n\ngs = GridSearchCV(rf, params, cv=5, return_train_score=False, scoring='f1_macro')\ngs.fit(selected_data_train1,y_train)","01a6d6b0":"print(gs.best_params_)\nmodel14 = gs.best_estimator_","7de09dbe":"print(Fore.BLUE+report(y_test, model14.predict(selected_data_test1), model14.classes_))","0c05f06c":"### Clustering as a model:","74b1c9b5":"* Clustering doesn't improve the f1 macro score. The previous model was better.\n","46b18304":"### First Model: Houseowners' features only","f2e0cbd9":"# 7. Models","2b8e73c6":"* according to the PCA graph above - I chose to use 50 conponents.","bfd2e6e3":"# Outline:","c8763cb4":"## Models without using PCA","1085a55a":"# Poverty Level Prediction of Costa Rican Households","f038afdd":"The category 0 doesn't exist, so I checked it with the boolean feature v18q:","c6212b1d":"* Dropping meaneduc - family feature\n* Dropping columns that repeat themselves in a boolean and ordinal manner ('mobilephone', 'v18q').","07651f46":"### v18q1 - Number of tablets in the household","47dbb098":"* \"Many social programs have a hard time making sure the right people are given enough aid. It\u2019s especially tricky when a program focuses on the poorest segment of the population. The world\u2019s poorest typically can\u2019t provide the necessary income and expense records to prove that they qualify.\n* In Latin America, one popular method uses an algorithm to verify income qualification. It\u2019s called the Proxy Means Test (or PMT). With PMT, agencies use a model that considers a family\u2019s observable household attributes like the material of their walls and ceiling, or the assets found in the home to classify them and predict their level of need.\n\n* While this is an improvement, accuracy remains a problem as the region\u2019s population grows and poverty declines\". \n* Source: https:\/\/www.kaggle.com\/c\/costa-rican-household-poverty-prediction\/data","4ae77570":"### Target Variable: Poverty level\n1. <u>extreme poverty<\/u> \n2. moderate poverty \n3. vulnerable households \n4. non vulnerable households\n\nThe main class I would like to identify and predict is the extreme poverty households - class 1. ","8ae2741c":"### BaggingClassifier (SVC):","7548863c":"Since I would like to create a new table where each row represents a household, I will drop meaneduc and replace it with meaneduc_new later on.","97064fb5":"### This is the best model so far, which I chose to work with.","621edecd":"### Variance Threshold:","063ad0bb":"# Thank you!","a3607aae":"### Filling Null Values:","4918e289":"### LogisticRegression","4c1553b8":"# 2. Data Cleaning","1baeb84e":"# 3. Houseowners Model","6960805d":"There are families who own the house or does't have one, so there are no rental fees.","b5aa605c":"I will compare the next models to this model, in order to understand if we actually need the features of the family or not.","e959ef7e":"### test:","119e87f3":"# PCA For Visualization","218071f9":"1. Introduction\n2. Data Cleaning\n3. Base Model\n4. Adding Features\n5. Visualization\n6. Feature Selection\n7. Models","2bc6aecf":"This feature is available between the ages of 11-17","943f3230":"The evaluation is based on the <b> F1 macro score<\/b>, which is the <b>harmonic mean of precision and recall<\/b>.","cc1fee72":"### rez_esc - Years behind in school","ca7a96bb":"* Each row represents one family member.\n* Each row contains individual features and household features.\n* According to kaggles file description, only predictions for heads of household are scored.","3ebb7570":"### Imbalances data - I will use the balanced class_weight","e44905dd":"### Models using PCA","e55a708d":"### Creating a dataset of houseowners (where parentesco1==1)\n* This dataset includes houseowners individual features and house features.","ae1ab921":"* The data is imbalanced","21095e1a":"# 6. Feature Selection","2f02464c":"### meaneduc - Average years of education for adults (18+)","6ae97b92":"### Dropping duplicated features:","275979f2":"# 5. Visualization","d996cf46":"* Dropping these dummies:","9a3eb990":"# 4. Adding Features\n","49bf76ea":"### Dropping all squared parameters:\n* I will create new ones without null values or calculation mistakes","92de2194":"* No feature is highly correlated with the target","6f6468e4":"### Dropping  family members without a houseowner:","1bcc5bf4":"### VotingClassifier (LogisticRegression and KNeighborsClassifier)","0f408d02":"### Adding Features and Interactions:","78b35940":"### VotingClassifier (LogisticRegression and KNeighborsClassifier)","d080e473":"### Creating a dataframe of individual features:","8c3fa241":"* most of the features are binary, the rest are ordinal or continuous","d425c773":"# 1. Introduction:","7bf0409e":"### Creating family features:\n* Aggregating individual features of each family member to one family feature","7171aa17":"Before dropping these categorical features, I found that there are null values in instlevel_cat:","67e67295":"### train:","6b9a4790":"### rent - house rental fees ","4e420ef2":"recommendations for future research:\n* Improve feature selection\n* Compare models by other score methods except f1 score","4059f2c1":"### RandomForestClassifier with clustering as a parameter:","4e2be74d":"### Adding meaneduc_new:","b8b1215b":"### Dropping part of the individual features of houseowners:","f4a63ce1":"### Fixing mixed Scales:","e59a4a51":"I will drop them since the score is given to the homeowners.","9fcefdc1":"### Creating Ordinal Features from dummies:","e4d2ed1f":"### AdaBoostClassifier (DecisionTreeClassifier)"}}