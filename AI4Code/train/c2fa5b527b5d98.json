{"cell_type":{"30e0a83b":"code","5231a156":"code","2925b922":"code","c9b912b9":"code","5ec08987":"code","3a357649":"code","4c255e6f":"code","9ae215b0":"code","f11058c0":"code","fe47b3bc":"code","5d421b5d":"code","d735f8dd":"code","d4993efe":"code","f31b012e":"code","73e2b130":"code","5468a0ff":"code","67deb28a":"code","6ea57906":"code","d25650d1":"code","e9c445e1":"code","f16a8f6f":"code","8e531613":"code","911ddfd7":"code","557881e0":"code","6137593e":"code","8a5c5d99":"code","794ebe2e":"code","e4d70ab5":"code","d4494fef":"code","fc24ce4a":"code","702ddbe4":"code","29739110":"code","3717c30b":"code","bbac3879":"code","30646a9e":"code","44130882":"code","451ce191":"code","e7b03f53":"code","d1cf7d7f":"code","61203f79":"code","79911322":"code","df7b8e93":"code","82abbcf3":"code","b63b46fb":"code","e5ab6939":"code","dd436a1d":"code","ca4c9028":"code","acef95fc":"code","56f86b53":"code","b2a7aafd":"code","5a5eda64":"code","16bba43a":"code","a303e741":"markdown","2a0ecc38":"markdown","398f00ab":"markdown","a1fce2fb":"markdown","0ebcade5":"markdown","03587c98":"markdown","baa5195f":"markdown","16c45a6a":"markdown","cb311058":"markdown","2b0babe9":"markdown"},"source":{"30e0a83b":"import os\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport cv2\nfrom sklearn.preprocessing import LabelEncoder, OneHotEncoder\nimport sklearn\nfrom skimage.feature import hog\nimport skimage.io\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.decomposition import PCA\nfrom scipy.sparse.linalg.eigen.arpack import eigsh as largest_eigsh\nfrom sklearn.metrics import confusion_matrix,classification_report,accuracy_score\nfrom tensorflow.keras.layers import Dense, Input, Conv2D,MaxPooling2D, GlobalMaxPooling2D, GaussianNoise, BatchNormalization,Flatten,Dropout\nfrom tensorflow.keras.models import Model,Sequential\nfrom tensorflow.keras.layers.experimental.preprocessing import Normalization\nfrom tensorflow.keras.callbacks import EarlyStopping\nfrom tensorflow.keras.layers import LeakyReLU\nfrom keras.utils.np_utils import to_categorical\nfrom tensorflow.keras.optimizers import RMSprop, Adam\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.svm import SVC, LinearSVC","5231a156":"#list the contents of the input directory\nos.listdir(\"..\/input\/chinese-mnist\")","2925b922":"#read the csv file\ndata =pd.read_csv('..\/input\/chinese-mnist\/chinese_mnist.csv')","c9b912b9":"data.info()","5ec08987":"data.sample(1000).head() #randomly show the samples from the dataframe","3a357649":"data.groupby([\"value\",\"character\"]).size()","4c255e6f":"#function to identify missing data\ndef missing_data(data):\n    null_total = data.isnull().sum().sort_values(ascending = False)\n    null_percent = (data.isnull().sum()\/data.isnull().count()*100).sort_values(ascending = False)\n    data_miss = pd.concat([null_total, null_percent], axis=1, keys=['Total', 'Percent'])\n    return data_miss","9ae215b0":"missing_data(data)","f11058c0":"image_files = list(os.listdir(\"..\/input\/chinese-mnist\/data\/data\"))\nprint(\"Number of image files: {}\".format(len(image_files)))","fe47b3bc":"#function to create file name for each image in the input directory\ndef create_image_file_name(x):\n    file_name = f\"input_{x[0]}_{x[1]}_{x[2]}.jpg\"\n    return file_name","5d421b5d":"data[\"image_file\"] = data.apply(create_image_file_name, axis=1) #append column for file names to original data","d735f8dd":"#function to get image size\ndef image_sizes(file_name):\n    image = skimage.io.imread(\"..\/input\/chinese-mnist\/data\/data\/\" + file_name)\n    return list(image.shape)","d4993efe":"image_size = np.stack(data['image_file'].apply(image_sizes))\nimage_size_df = pd.DataFrame(image_size,columns=['w','h'])\ndata = pd.concat([data,image_size_df],axis=1, sort=False)\ndata.head() #data after adding new columns ","f31b012e":"file_names = list(data['image_file'])\nprint(\"There are {} matching image names\".format(len(set(file_names).intersection(image_files))))","73e2b130":"num_duplicates = data.duplicated().sum() #identify duplicates\nprint('There are {} duplicate rows present in the dataset'.format(num_duplicates))","5468a0ff":"#creating dataset\nprint(\"Number of suites in the dataset: {}\".format(data.suite_id.nunique()))\nprint(\"Unique samples in the dataset: {}\".format(sorted(data.sample_id.unique())))","67deb28a":"char_value=sorted(data['value'].unique())\nprint(\"Unique character values: {}\".format(char_value))","6ea57906":"#create a function to read images\ndef read_image(image_file_name):\n    image = skimage.io.imread(\"..\/input\/chinese-mnist\/data\/data\/\" + image_file_name)\n    return image","d25650d1":"# get images and labels\nX = np.stack(data['image_file'].apply(read_image)) #image data\ny = data['value'].to_numpy() #target_labels","e9c445e1":"#encoding our labels\nle = LabelEncoder()\ny_enc = le.fit_transform(y)","f16a8f6f":"np.unique(y)","8e531613":"np.unique(y_enc)","911ddfd7":"#Overview of the dataset\nnum_pixels = X.shape[1]**2\nimg_dim = np.sqrt(num_pixels).astype(int)\nnum_classes = len(set(y))\nnum_samples = len(X)\nprint(10*\"-\",\"Overview of the Dataset\",10*\"-\")\nprint(\"Size of each image: {} x {}\".format(img_dim,img_dim))\nprint(\"Number of pixels in each image:\",num_pixels)\nprint(\"Number of classes:\",num_classes)\nprint(\"Total number of images:\",num_samples)\n","557881e0":"#Perform normalization\nX = X\/255.0 #perform normalization to reduce illumination differences","6137593e":"#Plotting images for each sign\nplt.figure(figsize=(20,6))\n\nfor i,j in enumerate([6000,7000,8000,9000,10000,11000,12000,13000,14000,0,1000,2000,3000,4000,5000]):\n    plt.subplot(3,5,i+1)\n    plt.subplots_adjust(top = 2, bottom = 1)\n    img = X[j]\n    plt.imshow(img,cmap = 'gray')\n    plt.title(y[j])\n    plt.axis('off')\nplt.suptitle('Images from the dataset',fontsize = 20)\nplt.show()","8a5c5d99":"# function to extract HOG features \ndef HOG_features(data):\n    sample_size = data.shape[0]\n    hog_features = []\n    for i in range(sample_size):\n        image = data[i]\n        feature = hog(image, orientations=9, pixels_per_cell=(8, 8), cells_per_block=(3, 3))\n        hog_features.append(feature)\n    return np.array(hog_features)","794ebe2e":"#Extract hog features\nX_hog = HOG_features(X)","e4d70ab5":"dim = np.sqrt(X_hog.shape[1]).astype(int)\nhog_images = X_hog.reshape(-1,dim,dim)\nperc_red = np.round((1- X_hog.shape[1]\/num_pixels)*100,2)","d4494fef":"print('The percentage reduction in size of the features is: {} %'.format(perc_red))","fc24ce4a":"#some image processing to be done at a later stage.\n#Plotting images for each sign\nplt.figure(figsize=(20,6))\n\nfor i,j in enumerate([6000,7000,8000,9000,10000,11000,12000,13000,14000,0,1000,2000,3000,4000,5000]):\n    plt.subplot(3,5,i+1)\n    plt.subplots_adjust(top = 2, bottom = 1)\n    plt.imshow(hog_images[i],cmap = 'gray')\n    plt.title(y[j])\n    plt.axis('off')\nplt.suptitle('HOG of images from the dataset',fontsize = 20)\nplt.show()","702ddbe4":"def get_data(X,y):\n    #Splitting the data into training and test set\n    y = to_categorical(y)\n    Xtrain,Xtest,ytrain,ytest = train_test_split(X,y,test_size = 0.2,random_state = 2)\n    #Splitting the training data into training and validation set\n    Xtrain,Xval,ytrain,yval = train_test_split(Xtrain,ytrain,test_size = 0.2,random_state = 2)\n    return Xtrain,Xval,Xtest,ytrain,yval,ytest","29739110":"Xtrain,Xval,Xtest,ytrain,yval,ytest = get_data(X_hog,y_enc)\nytrain_labels = np.argmax(ytrain,axis = -1) \nytest_labels = np.argmax(ytest,axis = -1) \nyval_labels = np.argmax(yval,axis = -1) ","3717c30b":"print(\"Number of samples in training set:{}\".format(len(ytrain)))\nprint(\"Number of samples in validation set:{}\".format(len(yval)))\nprint(\"Number of samples in testing set:{}\".format(len(ytest)))","bbac3879":"def plotLearningCurve(history,epochs):\n  epochRange = range(1,epochs+1)\n  fig , ax = plt.subplots(1,2,figsize = (10,5))\n  \n  ax[0].plot(epochRange,history.history['accuracy'],label = 'Training Accuracy')\n  ax[0].plot(epochRange,history.history['val_accuracy'],label = 'Validation Accuracy')\n  ax[0].set_title('Training and Validation accuracy')\n  ax[0].set_xlabel('Epoch')\n  ax[0].set_ylabel('Accuracy')\n  ax[0].legend()\n  ax[1].plot(epochRange,history.history['loss'],label = 'Training Loss')\n  ax[1].plot(epochRange,history.history['val_loss'],label = 'Validation Loss')\n  ax[1].set_title('Training and Validation loss')\n  ax[1].set_xlabel('Epoch')\n  ax[1].set_ylabel('Loss')\n  ax[1].legend()\n  fig.tight_layout()\n  plt.show()","30646a9e":"np.random.seed(12)\ni = Input(shape=len(Xtrain.T))\nx = Dense(256, activation=LeakyReLU(0.05))(i)\nx = Dropout(0.2)(x)\nx = Dense(128, activation=LeakyReLU(0.05))(x)\nx = Dropout(0.2)(x)\nx = Dense(64, activation=LeakyReLU(0.05))(x)\nx = Dropout(0.2)(x)\nx = Dense(32,activation=LeakyReLU(0.05))(x)\nx = Dropout(0.2)(x)\nx = Flatten()(x)\nx = Dense(15, activation='softmax')(x)\nmodel = Model(i, x)\nmodel.summary()\n","44130882":"model.compile(loss='categorical_crossentropy', optimizer=Adam(0.001),metrics=['accuracy']) # use categorical_crossentropy when one hot encoding is used","451ce191":" r  = model.fit(x=Xtrain, \n          y=ytrain, \n          epochs=100,\n          validation_data=(Xval, yval),\n          )","e7b03f53":"print(\"Train score:\", model.evaluate(Xtrain,ytrain))\nprint(\"Validation score:\", model.evaluate(Xval,yval))","d1cf7d7f":"plotLearningCurve(r,100)","61203f79":"ypred = model.predict(Xtest)\nypred = np.argmax(ypred,axis = 1) ","79911322":"#plot confusion matrix\ncm = confusion_matrix(ytest_labels,ypred)\nplt.figure(figsize=(10,10))\nsns.heatmap(cm,cmap = 'Blues',annot = True,fmt = 'g')\nplt.xlabel('Predicted value')\nplt.ylabel('True')\nplt.show()","df7b8e93":"accuracy = np.round(accuracy_score(ytest_labels,ypred),4)\nprint(\"The accuracy of ANN on test set is {} %\".format(accuracy*100))","82abbcf3":"hog_images_1 = np.expand_dims(hog_images,3)","b63b46fb":"Xtrain,Xval,Xtest,ytrain,yval,ytest = get_data(hog_images_1,y_enc)","e5ab6939":"np.random.seed(12)\ni = Input(shape=hog_images_1[1].shape)\nx = BatchNormalization()(i)\nx = Conv2D(128, 3,padding = \"same\",activation = \"relu\")(i)\nx = MaxPooling2D(pool_size=(4,4), strides = 2)(x)\nx = Conv2D(64, 3,padding = \"same\",activation = \"relu\")(x)\nx = MaxPooling2D(pool_size=(2,2), strides = 2)(x)\nx = Flatten()(x)\nx = Dense(128, activation='relu')(x)\nx = Dense(15, activation='softmax')(x)\nmodel = Model(i, x)\nmodel.summary()","dd436a1d":"model.compile(loss='categorical_crossentropy', optimizer=Adam(0.001),metrics=['accuracy']) # use categorical_crossentropy when one hot encoding is used","ca4c9028":" r  = model.fit(x=Xtrain, \n          y=ytrain, \n          epochs=100,\n          validation_data=(Xval, yval),\n          )","acef95fc":"print(\"Train score:\", model.evaluate(Xtrain,ytrain))\nprint(\"Validation score:\", model.evaluate(Xval,yval))","56f86b53":"plotLearningCurve(r,100)","b2a7aafd":"ypred = model.predict(Xtest)\nypred = np.argmax(ypred,axis = 1) ","5a5eda64":"#plot confusion matrix\ncm = confusion_matrix(ytest_labels,ypred)\nplt.figure(figsize=(10,10))\nsns.heatmap(cm,cmap = 'Blues',annot = True,fmt = 'g')\nplt.xlabel('Predicted value')\nplt.ylabel('True')\nplt.show()","16bba43a":"accuracy = np.round(accuracy_score(ytest_labels,ypred),4)\nprint(\"The accuracy of CNN on test set is {} %\".format(accuracy*100))","a303e741":"* **orientations** refers to the number of bins for the histogram.\n* **pixels_per_cell** refers the patch size or size of the cell for which we create the histograms. \n* **cells_per_block** refers to the size of the block over which histogram is normalized and is represented terms of number of cells included in each block.","2a0ecc38":"There are no missing values in our dataset.","398f00ab":"# **4. Performing Classification using ANN**","a1fce2fb":"# **3. HOG for feature extraction and dimension reduction**","0ebcade5":"# **2. Data Handling and Exploration**","03587c98":"Histogram of Oriented Gradients (HOG) is a feature descriptor that helps us finding the explicit and robust local features of an image. A feature descriptor is a representation of an image or an image patch that is a simplified version obtained by extracting useful information and throwing away extraneous information from the original image. In the HOG feature descriptor, the distribution (histograms) of directions of gradients (oriented gradients) are used as features. Gradients (x and y derivatives) of an image are useful because the magnitude of gradients is large around edges\/corners which contain abrupt changes in intensity which contains lot of information regarding the object shape as compared to flatter regions. In HOG,feature extraction is performed by first dividng the complete image into smaller regions and calculating the gradients and orientation for each of these regions. This is followed by normalization over blocks of cells to reduce lighting variation since gradients of image are sensitive to overall lighting. \n","baa5195f":"# **1. Importing Libraries**","16c45a6a":"# **5. Performing Classification using CNN**","cb311058":"### Dataset Information\n\nThe Chinese MNIST dataset consists of 15000 images, each representing one character from a set of 15 characters (grouped in samples, grouped in suites, with 10 samples\/volunteer and 100 volunteers). \n\nThe dataset contains the following:\n\n* an index file named chinese_mnist.csv\n* a folder with 15,000 jpg images, sized 64 x 64. \n\nThe index file contains the following fields:  \n\n* suite_id - each suite corresponds to a set of handwritten samples by one volunteer,so in total we have 100 suits  \n* sample_id - Each sample contains a complete set of 15 characters for Chinese numbers.\n* code - Code used for each chinese character with values from 1 to 15.\n* value - The actual numerical value associated with the Chinese character for number.\n* character - The actual Chinese character.\nThe files in the dataset are indexed using a file name by combining suite_id, sample_id and code. The pattern for each image is as follows:\n\n> \"input_{suite_id}_{sample_id}_{code}.jpg\"\n\n### Problem Statement\nCorrectly classify the chinese characters from the dataset\n\n### Overview\nIn this kernel, Histogram of Oriented Gradients (HOG) is used to extract features from Chinese characters. The HOG in this context not only helps in extracting robust and critical features of the characters but also reduces the overall dimension of features from 64 x 64 = 4096 to 54 x 54 = 2916 i.e. around 28 % reduction in the number of features. Following feature extraction using HOG, the transformed data is then fed to the ANN. An accuracy of around 91-92% is obtained on the test data which can be improved using hyperparameter optimization.\n\n### References\nN. Dalal and B. Triggs, \"Histograms of oriented gradients for human detection,\" 2005 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR'05), 2005, pp. 886-893 vol. 1, doi: 10.1109\/CVPR.2005.177.\n\nPlease upvote if you like my work. Thank you!","2b0babe9":"There are 1000 images per characters, which means that the dataset is balanced."}}