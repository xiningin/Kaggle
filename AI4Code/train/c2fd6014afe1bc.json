{"cell_type":{"dfb64921":"code","dba49730":"code","fd2a8c7b":"code","08820eb3":"code","7ab16a32":"code","3dc857f8":"code","89db32eb":"code","3da56704":"code","48eb7667":"code","cc7305c3":"code","cc04b74d":"code","b39d4363":"code","4574484f":"code","b6995690":"code","e43b9f3a":"code","6c94ca35":"code","46b468af":"code","37bc32db":"markdown","5afd586e":"markdown","9f476806":"markdown","35a8fa3e":"markdown","940c115d":"markdown","b3437584":"markdown","22602424":"markdown","0fc920a6":"markdown","2abc11b6":"markdown","a07d133c":"markdown","63f93945":"markdown","c5480686":"markdown","3cf0dc86":"markdown","c6ee5d8c":"markdown","09cca2ee":"markdown","47f4c787":"markdown","621c350c":"markdown"},"source":{"dfb64921":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plot\nimport seaborn as sns","dba49730":"data = pd.read_csv(\"..\/input\/creditcard.csv\")\n\ndata.describe()","fd2a8c7b":"print(data.head())","08820eb3":"corr = data.corr()\n\nplot.figure(figsize=(30,30))\nsns.heatmap(corr, annot=True)","7ab16a32":"corr1 = corr[corr>0.1]\nplot.figure(figsize=(20,20))\nsns.heatmap(corr1, annot=True)","3dc857f8":"from sklearn.decomposition import PCA\n\n#dropping the solution\npca_data = data.drop(\"Class\", 1)\n\npca = PCA(n_components=5)\npca.fit(pca_data)\n\npca_data = pd.DataFrame(pca.transform(pca_data))\nprint(pca_data.shape)","89db32eb":"means = []\nstds = []\n\nfor col in range(pca_data.shape[1]):\n        mn = np.mean(pca_data.iloc[:,col])\n        st = np.mean(pca_data.iloc[:,col].std())\n        \n        #storing statistical data for later\n        means.append(mn)\n        stds.append(st)\n        \n        pca_data.iloc[:,col] = (pca_data.iloc[:,col]-mn)\/st\n        pca_data.iloc[:,col] = np.nan_to_num(pca_data.iloc[:,col])\n        \npca_data.describe()","3da56704":"test_ratio=0.2\n\n#combining with solutions to keep order\nnew_data = pd.concat([pca_data, data[\"Class\"]],1)\n\ntest_data = new_data.sample(frac=test_ratio)\ntrain_data = new_data.drop(test_data.index)\n\ntest_sols = test_data[\"Class\"]\ntest_data = test_data.drop(\"Class\", 1)\n\ntrain_sols = train_data[\"Class\"]\ntrain_data = train_data.drop(\"Class\", 1)\n\n\n\n#get dummies for better classification\ntrain_sols = pd.get_dummies(train_sols, prefix=\"Class\")\ntest_sols = pd.get_dummies(test_sols, prefix=\"Class\")\n\nprint(new_data.shape)\nprint(train_data.shape)\nprint(test_data.shape)\n\nprint(train_sols.head())","48eb7667":"from keras.models import Model, load_model\nfrom keras.layers import Input, Dropout, Dense, LeakyReLU\nfrom keras.callbacks import ModelCheckpoint\n\ninp = Input(shape=(5,))\nx = Dense(64)(inp)\nx = LeakyReLU()(x)\nx = Dropout(0.2)(x)\nx = Dense(32)(x)\nx = LeakyReLU()(x)\nx = Dropout(0.2)(x)\nx = Dense(8)(x)\nx = LeakyReLU()(x)\nx = Dropout(0.2)(x)\nx = Dense(2, activation=\"softmax\")(x)\n\nmodel = Model(inputs=inp, outputs=x)\nmodel.compile(optimizer=\"adam\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])","cc7305c3":"weight0 = 1.0\/train_sols[\"Class_0\"].sum()\nweight1 = 1.0\/train_sols[\"Class_1\"].sum()\n\n_sum = weight0+weight1\n\nweight0 \/= _sum\nweight1 \/= _sum\n\nprint(weight0)\nprint(weight1)","cc04b74d":"callback = [ModelCheckpoint(\"check.h5\", save_best_only=True, monitor=\"val_acc\", verbose=0)]","b39d4363":"model.fit(train_data, train_sols, batch_size=500, epochs=100, verbose=0, callbacks=callback, validation_split=0.2, shuffle=True, \n         class_weight={0:weight0, 1: weight1})","4574484f":"best_model = load_model(\"check.h5\")","b6995690":"score = model.evaluate(test_data, test_sols)\nprint(score[1])","e43b9f3a":"test_data2 = pd.concat([test_data, test_sols],1)\ntest_data2 = test_data2.loc[test_data2[\"Class_1\"]==1]\n\ntest_sols2 = test_data2[[\"Class_0\", \"Class_1\"]]\ntest_data2 = test_data2.drop([\"Class_0\", \"Class_1\"],1)\n\nscore2 = model.evaluate(test_data2, test_sols2)\nprint(score2[1])","6c94ca35":"from sklearn.ensemble import GradientBoostingClassifier\n\nxgmodel = GradientBoostingClassifier(n_estimators=200)\n\n#weighting the samples\nxgweight0 = train_sols[\"Class_0\"].values*weight0\nxgweight1 = train_sols[\"Class_1\"].values*weight1\n\nxgweights = xgweight0+xgweight1\nprint(xgweights.shape)\nprint(train_sols[\"Class_1\"].shape)\n#fit\nxgmodel.fit(train_data, train_sols[\"Class_1\"], xgweights)\n\nxgscore = xgmodel.score(test_data, test_sols[\"Class_1\"])\nxgscore2 = xgmodel.score(test_data2, test_sols2[\"Class_1\"])","46b468af":"print(\"Overall score:\\t\\t\"+str(xgscore))\nprint(\"Rare case score:\\t\"+str(xgscore2))","37bc32db":"Ha! The model obviously performs worse for these cases but it is still quite good. Neat!\n\nFinally I would like to compare our neural network to a gradient boosting classifier. Therefore we are using the gradient boosting classifier of sklearn.","5afd586e":"The accuracy seems to be high right? Careful! Keep in mind that we have a highly unbalanced dataset with many zeros and only a few ones! Let us check how the model performs for ones only.","9f476806":"This is a lot of data. We can see that mostly there are very low correlations (0.1 or less). Let us filter out only the large ones.","35a8fa3e":"In order to minimize the influence of the heavily unbalanced dataset we are going to calculate class weights, that will penalize wrong detection of the minority class more than of the majority class. Therefore we are calculating the inverse number of elements in each category.","940c115d":"Next we are going to normalize our data, which helps our neural network to perform better.","b3437584":"We can see that the means are 0 and the standard deviations are 1 now. Perfect!\n\nNow we have reduced the dimensionality of our data significantly. We are now splitting off data to evaluate our model later on.","22602424":"Let us now look at correlations to find out more about our data. For that we are using the heatmap plot of searborn.","0fc920a6":"To evaluate our model we can use the evaluate model provided by keras.","2abc11b6":"Let us now define a model checkpoint, which saves the model for the best accuracy.","a07d133c":"We can now finally fit our model.","63f93945":"We are provided with Time information, 24 columns of anonymous data, the amount of the fraud and the class (1=fraud). By looking at the mean of the class we can see that we are dealing with a heavily unbalanced dataset. In order to avoid the network predicting no fraud all the time we have to take some measures. This is going to be discussed later on.","c5480686":"**Fraud detection (classification) using neural networks**\n\nIn this tutorial we are going to build a simple fraud classification model for credit card data. For data loading, preparation and visualization we are going to use the following modules.\n* numpy\n* pandas\n* matplotlib\n* seaborn\n\nLet us import those modules.","3cf0dc86":"Perfect! Let us build the model now. We are using keras as the frontend and tensorflow as our backend. The network will be a simple dense neural network. For introducing non-linearity and improving convergence we are using the Leaky-Rectified-Linear (LeakyReLU) activation function for intermediate layers. For the last layer we are going to use softmax.","c6ee5d8c":"At the first glance it looks like the gradient boosting classifier outperforms the neural network, but when having a closer look at the rare cases we see that the neural network gives much a litte more accurate predictions.\n\nA few interesting ideas to play with:\n* How does the number of principal components affect the accuracy?\n* How does the complexity of the neural network afferct the accuracy and training speed?\n* Does binary_crossentropy and only one column for the class perform also that well?\n* Can some kind of feature engineering improve the performance?\n* How does the number of estimators influence the performance of the gradient boosting classifier?\n* ...\n\nYou see that there are still many open questions one could try to answer in order to optimize the model. I am for now happy with the current model.","09cca2ee":"We can now see that most of the variables are not really correlated with the class. It might be useful to perform a principal component analysis here (PCA). Therefore we are importing sklearn. We want roughly 5 components to remain.\n\nWe are dropping the solution from the data before fitting the PCA.","47f4c787":"Let us now load the best model of our training epochs.","621c350c":"The first step is to load the data and gain a basic insight. Therefore we are going to use the describe method of the pandas dataframe. We are also going to print the first 5 lines using the head method."}}