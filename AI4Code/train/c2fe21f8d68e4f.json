{"cell_type":{"8fc0a8f2":"code","212a656a":"code","38186097":"code","4de2cd87":"code","a6e6d2be":"code","f54cbb64":"code","fc2d612b":"code","257dea24":"code","3c743b3f":"code","42ef889d":"code","3c6b8d67":"code","958fa784":"code","52206cad":"code","102e6795":"code","61d90ea8":"code","45bde441":"markdown","f657109b":"markdown","ddb18252":"markdown","d619ce44":"markdown","414b3b54":"markdown","06268bcf":"markdown","fb7446b1":"markdown","28db5b28":"markdown","4031ccfe":"markdown","9454f2eb":"markdown"},"source":{"8fc0a8f2":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport os\nimport sys\nfrom multiprocessing import cpu_count\nimport copy\nimport pickle\nimport warnings\nfrom datetime import datetime, timedelta\nfrom time import time, sleep, mktime\nfrom matplotlib import font_manager as fm, rc, rcParams\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom tqdm import tqdm\nimport re\nimport random as rnd\n\nimport numpy as np\nfrom numpy import array, nan, where\nimport pandas as pd\nfrom pandas import DataFrame as dataframe, Series as series, isna, read_csv\nfrom pandas.tseries.offsets import DateOffset\nimport statsmodels.api as sm\nfrom scipy.stats import f_oneway\n\nfrom sklearn import preprocessing as prep\nfrom sklearn.impute import KNNImputer\nfrom sklearn.model_selection import train_test_split as tts, GridSearchCV as GridTuner, StratifiedKFold, KFold\nfrom sklearn.feature_selection import SelectFromModel\nfrom sklearn.preprocessing import OneHotEncoder, StandardScaler, MinMaxScaler, RobustScaler\nfrom sklearn import metrics\nfrom sklearn.pipeline import make_pipeline\n\nfrom sklearn import linear_model as lm\nfrom sklearn import svm\nimport lightgbm as lgb\nimport xgboost as xgb\nimport catboost as cat\nfrom sklearn import neighbors as knn\nfrom sklearn import ensemble\nfrom sklearn.preprocessing import KBinsDiscretizer\n\n# display setting\nrcParams['axes.unicode_minus'] = False\nos.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\nos.environ[\"WANDB_API_KEY\"] = \"6f810b088fcc6b9eaaa56c1e52cfd37836606240\"\n\n# ===== tensorflow =====\nimport tensorflow as tf\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras import layers\nfrom tensorflow.keras.layers.experimental.preprocessing import Rescaling\nfrom tensorflow.keras import activations\nfrom tensorflow.keras import optimizers\nfrom tensorflow.keras import metrics as tf_metrics\nfrom tensorflow.keras import callbacks as tf_callbacks\nfrom tqdm.keras import TqdmCallback\nimport tensorflow_addons as tfa\nimport keras_tuner as kt\nfrom keras_tuner import HyperModel\nimport tensorflow_hub as tf_hub\nfrom tensorflow.keras.utils import plot_model\nfrom tensorflow.keras.models import load_model\n\n!pip install --upgrade wandb\nimport wandb\nfrom wandb.keras import WandbCallback\n\nimport shutil\n# GPU check\ndevice_name = tf.test.gpu_device_name()\nif device_name != '\/device:GPU:0':\n  raise SystemError('GPU device not found')\nprint('Found GPU at: {}'.format(device_name))\n\ngpus = tf.config.list_physical_devices('GPU')\nif gpus:\n  try:\n    tf.config.experimental.set_memory_growth(gpus[0], True)\n  except RuntimeError as e:\n    # \ud504\ub85c\uadf8\ub7a8 \uc2dc\uc791\uc2dc\uc5d0 \uba54\ubaa8\ub9ac \uc99d\uac00\uac00 \uc124\uc815\ub418\uc5b4\uc57c\ub9cc \ud569\ub2c8\ub2e4\n    print(e)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","212a656a":"def seed_everything(seed=42):\n    rnd.seed(seed)\n    np.random.seed(seed)\n    tf.random.set_seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\ndef which(bool_list):\n    idx_array = where(bool_list)[0]\n    return idx_array[0] if len(idx_array) == 1 else idx_array\ndef easyIO(x=None, path=None, op=\"r\"):\n    tmp = None\n    if op == \"r\":\n        with open(path, \"rb\") as f:\n            tmp = pickle.load(f)\n        return tmp\n    elif op == \"w\":\n        tmp = {}\n        print(x)\n        if type(x) is dict:\n            for k in x.keys():\n                if \"MLP\" in k:\n                    tmp[k] = {}\n                    for model_comps in x[k].keys():\n                        if model_comps != \"model\":\n                            tmp[k][model_comps] = copy.deepcopy(x[k][model_comps])\n                    print(F\"INFO : {k} model is removed (keras)\")\n                else:\n                    tmp[k] = x[k]\n        if input(\"Write [y \/ n]: \") == \"y\":\n            with open(path, \"wb\") as f:\n                pickle.dump(tmp, f)\n            print(\"operation success\")\n        else:\n            print(\"operation fail\")\n    else:\n        print(\"Unknown operation type\")\ndef diff(first, second):\n    second = set(second)\n    return [item for item in first if item not in second]\ndef findIdx(data_x, col_names):\n    return [int(i) for i, j in enumerate(data_x) if j in col_names]\ndef orderElems(for_order, using_ref):\n    return [i for i in using_ref if i in for_order]\n# concatenate by row\ndef ccb(df1, df2):\n    if type(df1) == series:\n        tmp_concat = series(pd.concat([dataframe(df1), dataframe(df2)], axis=0, ignore_index=True).iloc[:,0])\n        tmp_concat.reset_index(drop=True, inplace=True)\n    elif type(df1) == dataframe:\n        tmp_concat = pd.concat([df1, df2], axis=0, ignore_index=True)\n        tmp_concat.reset_index(drop=True, inplace=True)\n    elif type(df1) == np.ndarray:\n        tmp_concat = np.concatenate([df1, df2], axis=0)\n    else:\n        print(\"Unknown Type: return 1st argument\")\n        tmp_concat = df1\n    return tmp_concat\ndef change_width(ax, new_value):\n    for patch in ax.patches :\n        current_width = patch.get_width()\n        adj_value = current_width - new_value\n        # we change the bar width\n        patch.set_width(new_value)\n        # we recenter the bar\n        patch.set_x(patch.get_x() + adj_value * .5)\ndef week_of_month(date):\n    month = date.month\n    week = 0\n    while date.month == month:\n        week += 1\n        date -= timedelta(days=7)\n    return week\ndef createFolder(directory):\n    try:\n        if not os.path.exists(directory):\n            os.makedirs(directory)\n    except OSError:\n        print('Error: Creating directory. ' + directory)\ndef softmax(x):\n    max = np.max(x, axis=1, keepdims=True)  # returns max of each row and keeps same dims\n    e_x = np.exp(x - max)  # subtracts each row with its max value\n    sum = np.sum(e_x, axis=1, keepdims=True)  # returns sum of each row and keeps same dims\n    f_x = e_x \/ sum\n    return f_x\ndef sigmoid(x):\n    return 1\/(1 + np.exp(-x))\ndef dispPerformance(result_dic):\n    perf_table = dataframe()\n    index_names = []\n    for k, v in result_dic.items():\n        index_names.append(k)\n        perf_table = pd.concat([perf_table, series(v[\"performance\"]).to_frame().T], ignore_index=True, axis=0)\n    perf_table.index = index_names\n    perf_table.sort_values(perf_table.columns[0], inplace=True)\n    print(perf_table)\n    return perf_table\ndef tf_loss_rmse(y_true, y_pred):\n    return tf.sqrt(tf.reduce_mean((y_true - y_pred) ** 2))\nclass MyLabelEncoder:\n    def __init__(self, preset={}):\n        # dic_cat format -> {\"col_name\": {\"value\": replace}}\n        self.dic_cat = preset\n    def fit_transform(self, data_x, col_names):\n        tmp_x = copy.deepcopy(data_x)\n        for i in col_names:\n            # type check\n            if not ((tmp_x[i].dtype.name == \"object\") or (tmp_x[i].dtype.name == \"category\")):\n                print(F\"WARNING : {i} is not object or category\")\n            # if key is not in dic, update dic\n            if i not in self.dic_cat.keys():\n                tmp_dic = dict.fromkeys(sorted(set(tmp_x[i]).difference([nan])))\n                label_cnt = 0\n                for j in tmp_dic.keys():\n                    tmp_dic[j] = label_cnt\n                    label_cnt += 1\n                self.dic_cat[i] = tmp_dic\n            # transform value which is not in dic to nan\n            tmp_x[i] = tmp_x[i].astype(\"object\")\n            conv = tmp_x[i].replace(self.dic_cat[i])\n            for conv_idx, j in enumerate(conv):\n                if j not in self.dic_cat[i].values():\n                    conv[conv_idx] = nan\n            # final return\n            tmp_x[i] = conv.astype(\"float\")\n        return tmp_x\n    def transform(self, data_x):\n        tmp_x = copy.deepcopy(data_x)\n        for i in list(self.dic_cat.keys()):\n            if not ((tmp_x[i].dtype.name == \"object\") or (tmp_x[i].dtype.name == \"category\")):\n                print(F\"WARNING : {i} is not object or category\")\n            # transform value which is not in dic to nan\n            tmp_x[i] = tmp_x[i].astype(\"object\")\n            conv = tmp_x[i].replace(self.dic_cat[i])\n            for conv_idx, j in enumerate(conv):\n                if j not in self.dic_cat[i].values():\n                    conv[conv_idx] = nan\n            # final return\n            tmp_x[i] = conv.astype(\"float\")\n        return tmp_x\n    def clear(self):\n        self.dic_cat = {}\nclass MyOneHotEncoder:\n    def __init__(self, label_preset={}):\n        self.dic_cat = {}\n        self.label_preset = label_preset\n    def fit_transform(self, data_x, col_names):\n        tmp_x = dataframe()\n        for i in data_x:\n            if i not in col_names:\n                tmp_x = pd.concat([tmp_x, dataframe(data_x[i])], axis=1)\n            else:\n                if not ((data_x[i].dtype.name == \"object\") or (data_x[i].dtype.name == \"category\")):\n                    print(F\"WARNING : {i} is not object or category\")\n                self.dic_cat[i] = OneHotEncoder(sparse=False, handle_unknown=\"ignore\")\n                conv = self.dic_cat[i].fit_transform(dataframe(data_x[i])).astype(\"int\")\n                col_list = []\n                for j in self.dic_cat[i].categories_[0]:\n                    if i in self.label_preset.keys():\n                        for k, v in self.label_preset[i].items():\n                            if v == j:\n                                col_list.append(str(i) + \"_\" + str(k))\n                    else:\n                        col_list.append(str(i) + \"_\" + str(j))\n                conv = dataframe(conv, columns=col_list)\n                tmp_x = pd.concat([tmp_x, conv], axis=1)\n        return tmp_x\n    def transform(self, data_x):\n        tmp_x = dataframe()\n        for i in data_x:\n            if not i in list(self.dic_cat.keys()):\n                tmp_x = pd.concat([tmp_x, dataframe(data_x[i])], axis=1)\n            else:\n                if not ((data_x[i].dtype.name == \"object\") or (data_x[i].dtype.name == \"category\")):\n                    print(F\"WARNING : {i} is not object or category\")\n                conv = self.dic_cat[i].transform(dataframe(data_x[i])).astype(\"int\")\n                col_list = []\n                for j in self.dic_cat[i].categories_[0]:\n                    if i in self.label_preset.keys():\n                        for k, v in self.label_preset[i].items():\n                            if v == j: col_list.append(str(i) + \"_\" + str(k))\n                    else:\n                        col_list.append(str(i) + \"_\" + str(j))\n                conv = dataframe(conv, columns=col_list)\n                tmp_x = pd.concat([tmp_x, conv], axis=1)\n        return tmp_x\n    def clear(self):\n        self.dic_cat = {}\n        self.label_preset = {}\nclass MyKNNImputer:\n    def __init__(self, k=5):\n        self.imputer = KNNImputer(n_neighbors=k)\n        self.cat_dic = {}\n    def fit_transform(self, x, y, cat_vars=None):\n        naIdx = dict.fromkeys(cat_vars)\n        for i in cat_vars:\n            self.cat_dic[i] = diff(list(sorted(set(x[i]))), [nan])\n            naIdx[i] = list(which(array(x[i].isna()))[0])\n        x_imp = dataframe(self.imputer.fit_transform(x, y), columns=x.columns)\n\n        # if imputed categorical value are not in the range, adjust the value\n        for i in cat_vars:\n            x_imp[i] = x_imp[i].apply(lambda x: int(round(x, 0)))\n            for j in naIdx[i]:\n                if x_imp[i][j] not in self.cat_dic[i]:\n                    if x_imp[i][j] < self.cat_dic[i][0]:\n                        x_imp[i][naIdx[i]] = self.cat_dic[i][0]\n                    elif x_imp[i][j] > self.cat_dic[i][0]:\n                        x_imp[i][naIdx[i]] = self.cat_dic[i][len(self.cat_dic[i]) - 1]\n        return x_imp\n    def transform(self, x):\n        naIdx = dict.fromkeys(self.cat_vars)\n        for i in self.cat_dic.keys():\n            naIdx[i] = list(which(array(x[i].isna())))\n        x_imp = dataframe(self.imputer.transform(x), columns=x.columns)\n\n        # if imputed categorical value are not in the range, adjust the value\n        for i in self.cat_dic.keys():\n            x_imp[i] = x_imp[i].apply(lambda x: int(round(x, 0)))\n            for j in naIdx[i]:\n                if x_imp[i][j] not in self.cat_dic[i]:\n                    if x_imp[i][j] < self.cat_dic[i][0]:\n                        x_imp[i][naIdx[i]] = self.cat_dic[i][0]\n                    elif x_imp[i][j] > self.cat_dic[i][0]:\n                        x_imp[i][naIdx[i]] = self.cat_dic[i][len(self.cat_dic[i]) - 1]\n        return x_imp\n    def clear(self):\n        self.imputer = None\n        self.cat_dic = {}\ndef doCAT(train_x, train_y, test_x=None, test_y=None, categoIdx=None, boostingType=\"Plain\", ntrees=5000, eta=5e-2,\n          depthSeq=[4, 6, 8], bagTempSeq=[0.2, 0.8], colsampleSeq=[0.6, 0.8], l2Seq=[0.1, 1.0, 5.0], random_strength=[0.1, 1.0],\n          kfolds=KFold(10, shuffle=True, random_state=2323), model_export=False, preTrained=None, seed=33, tuningMode=True,\n          targetType=\"numeric\", targetTask=\"binary\", cut_off=0.5, class_levels=(0, 1, 2), kfolds_prediction=True):\n    result_dic = {}\n    result_dic[\"best_params\"] = {}\n    patientRate = 0.2\n    seed_everything()\n    tuner_params = {\"max_depth\": depthSeq, \"bagging_temperature\": bagTempSeq, \"rsm\": colsampleSeq,\n                    \"l2_leaf_reg\": l2Seq, \"random_strength\": random_strength}\n\n    runStart = time()\n    if targetType == \"numeric\":\n        if preTrained is not None:\n            result_dic[\"model\"] = preTrained\n            if train_x is not None:\n                result_dic[\"model\"].fit(train_x, train_y, cat_features=categoIdx)\n        else:\n            if tuningMode:\n                if kfolds_prediction:\n                    pass\n                else:\n                    cat_model = cat.CatBoostRegressor(boosting_type=boostingType, loss_function=\"RMSE\",\n                                                      n_estimators=int(ntrees * patientRate), learning_rate=eta \/ 10,\n                                                      logging_level=\"Silent\", thread_count=None, random_state=seed)\n                    model_tuner = GridTuner(cat_model, param_grid=tuner_params,\n                                            cv=kfolds.split(train_x, train_y), refit=False,\n                                            n_jobs=cpu_count(),\n                                            scoring=\"neg_root_mean_squared_error\")\n                    model_tuner.fit(train_x, train_y, cat_features=categoIdx)\n\n                    result_dic[\"best_params\"] = model_tuner.best_params_\n\n                    result_dic[\"model\"] = cat.CatBoostRegressor(boosting_type=boostingType, loss_function=\"RMSE\",\n                                                                n_estimators=ntrees, learning_rate=eta,\n                                                                max_depth=model_tuner.best_params_[\"max_depth\"],\n                                                                bagging_temperature=model_tuner.best_params_[\"bagging_temperature\"],\n                                                                rsm=model_tuner.best_params_[\"rsm\"],\n                                                                l2_leaf_reg=model_tuner.best_params_[\"l2_leaf_reg\"],\n                                                                random_strength=model_tuner.best_params_[\"random_strength\"],\n                                                                thread_count=cpu_count(),\n                                                                logging_level=\"Silent\", random_state=seed)\n\n                    result_dic[\"best_params\"][\"best_trees\"] = 0\n                    for nonkIdx, kIdx in kfolds.split(train_x, train_y):\n                        result_dic[\"model\"].fit(train_x.iloc[nonkIdx, :], train_y.iloc[nonkIdx], cat_features=categoIdx,\n                                                eval_set=[(train_x.iloc[kIdx, :], train_y.iloc[kIdx])], use_best_model=True,\n                                                early_stopping_rounds=int(ntrees * patientRate))\n                        result_dic[\"best_params\"][\"best_trees\"] += result_dic[\"model\"].best_iteration_ \/ kfolds.get_n_splits()\n                    result_dic[\"best_params\"][\"best_trees\"] = int(result_dic[\"best_params\"][\"best_trees\"])\n\n                    print(\"Tuning Result --->\", result_dic[\"best_params\"])\n\n                    result_dic[\"model\"] = cat.CatBoostRegressor(boosting_type=boostingType, loss_function=\"RMSE\",\n                                                                n_estimators=result_dic[\"best_params\"][\"best_trees\"], learning_rate=eta,\n                                                                max_depth=model_tuner.best_params_[\"max_depth\"],\n                                                                bagging_temperature=model_tuner.best_params_[\"bagging_temperature\"],\n                                                                rsm=model_tuner.best_params_[\"rsm\"],\n                                                                l2_leaf_reg=model_tuner.best_params_[\"l2_leaf_reg\"],\n                                                                random_strength=model_tuner.best_params_[\"random_strength\"],\n                                                                thread_count=cpu_count(),\n                                                                logging_level=\"Silent\", random_state=seed)\n            else:\n                result_dic[\"best_params\"][\"best_trees\"] = ntrees\n                result_dic[\"best_params\"][\"max_depth\"] = depthSeq\n                result_dic[\"best_params\"][\"bagging_temperature\"] = bagTempSeq\n                result_dic[\"best_params\"][\"rsm\"] = colsampleSeq\n                result_dic[\"best_params\"][\"l2_leaf_reg\"] = l2Seq\n                result_dic[\"best_params\"][\"random_strength\"] = random_strength\n\n                result_dic[\"model\"] = cat.CatBoostRegressor(boosting_type=boostingType, loss_function=\"RMSE\",  learning_rate=eta,\n                                                            n_estimators=result_dic[\"best_params\"][\"best_trees\"],\n                                                            max_depth=result_dic[\"best_params\"][\"max_depth\"],\n                                                            bagging_temperature=result_dic[\"best_params\"][\"bagging_temperature\"],\n                                                            rsm=result_dic[\"best_params\"][\"rsm\"],\n                                                            l2_leaf_reg=result_dic[\"best_params\"][\"l2_leaf_reg\"],\n                                                            random_strength=result_dic[\"best_params\"][\"random_strength\"],\n                                                            thread_count=cpu_count(),\n                                                            logging_level=\"Silent\", random_state=seed)\n            result_dic[\"model\"].fit(train_x, train_y, cat_features=categoIdx)\n\n        if test_x is not None:\n            result_dic[\"pred\"] = result_dic[\"model\"].predict(test_x)\n            if test_y is not None:\n                mae = metrics.mean_absolute_error(test_y, result_dic[\"pred\"])\n                rmse = metrics.mean_squared_error(test_y, result_dic[\"pred\"], squared=False)\n                result_dic[\"performance\"] = {\"MAE\": mae,\n                                             \"MAPE\": metrics.mean_absolute_percentage_error(test_y, result_dic[\"pred\"]),\n                                             \"NMAE\": (mae \/ test_y.abs().mean()),\n                                             \"RMSE\": rmse,\n                                             \"NRMSE\": (rmse \/ test_y.abs().mean()),\n                                             \"R2\": metrics.r2_score(test_y, result_dic[\"pred\"])}\n            else:\n                result_dic[\"performance\"] = None\n\n    if not model_export: result_dic[\"model\"] = None\n    result_dic[\"running_time\"] = round(time() - runStart, 3)\n    print(f\"Running Time ---> {result_dic['running_time']} sec\")\n    return result_dic\ndef doLGB(train_x, train_y, test_x=None, test_y=None, categoIdx=None, boostingType=\"goss\", ntrees=5000, eta=5e-3,\n          subsampleFreq=1, leavesSeq=[pow(2, i) - 1 for i in [4, 6, 8]], subsampleSeq=[0.6, 0.8], gammaSeq=[0.0, 0.2],\n          colsampleSeq=[0.6, 0.8], l2Seq=[0.1, 1.0, 5.0], mcsSeq=[5, 10, 20], mcwSeq=[5e-2, 1e-3, 5e-3], kfolds_prediction=True,\n          kfolds=KFold(10, shuffle=True, random_state=2323), model_export=False, preTrained=None, seed=22, tuningMode=True,\n          targetType=\"numeric\", targetTask=\"binary\", cut_off=0.5, class_levels=(0, 1, 2)):\n    result_dic = {}\n    result_dic[\"best_params\"] = {}\n    patientRate = 0.2\n    seed_everything()\n    tuner_params = {\"num_leaves\": leavesSeq, \"subsample\": subsampleSeq, \"colsample_bytree\": colsampleSeq,\n                    \"reg_lambda\": l2Seq, \"min_child_samples\": mcsSeq, \"min_child_weight\": mcwSeq, \"min_split_gain\": gammaSeq}\n\n    runStart = time()\n    if targetType == \"numeric\":\n        if preTrained is not None:\n            result_dic[\"model\"] = preTrained\n            if train_x is not None:\n                result_dic[\"model\"].fit(train_x, train_y, categorical_feature=categoIdx, callbacks=[lgb.log_evaluation(show_stdv=False)])\n        else:\n            if tuningMode:\n                if boostingType == \"rf\":\n                    pass\n                elif boostingType == \"goss\":\n                    if kfolds_prediction:\n                        pass\n                    else:\n                        lgb_model = lgb.LGBMRegressor(boosting_type=boostingType, objective=\"regression\",\n                                                      n_estimators=int(ntrees * patientRate), learning_rate=eta \/ 10,\n                                                      n_jobs=None, random_state=seed)\n                        model_tuner = GridTuner(lgb_model, param_grid=tuner_params,\n                                                cv=kfolds.split(train_x, train_y), refit=False,\n                                                n_jobs=cpu_count(),\n                                                scoring=\"neg_root_mean_squared_error\")\n                        model_tuner.fit(train_x, train_y, categorical_feature=categoIdx, verbose=False)\n\n                        result_dic[\"best_params\"] = model_tuner.best_params_\n\n                        result_dic[\"model\"] = lgb.LGBMRegressor(boosting_type=boostingType, objective=\"regression\",\n                                                                n_estimators=ntrees, learning_rate=eta,\n                                                                num_leaves=model_tuner.best_params_[\"num_leaves\"],\n                                                                subsample=model_tuner.best_params_[\"subsample\"],\n                                                                colsample_bytree=model_tuner.best_params_[\"colsample_bytree\"],\n                                                                reg_lambda=model_tuner.best_params_[\"reg_lambda\"],\n                                                                min_child_weight=model_tuner.best_params_[\"min_child_weight\"],\n                                                                min_child_samples=model_tuner.best_params_[\"min_child_samples\"],\n                                                                min_split_gain=model_tuner.best_params_[\"min_split_gain\"],\n                                                                n_jobs=cpu_count(), random_state=seed)\n\n                        result_dic[\"best_params\"][\"best_trees\"] = 0\n                        for nonkIdx, kIdx in kfolds.split(train_x, train_y):\n                            result_dic[\"model\"].fit(train_x.iloc[nonkIdx, :], train_y.iloc[nonkIdx], categorical_feature=categoIdx,\n                                                    eval_set=[(train_x.iloc[kIdx, :], train_y.iloc[kIdx])], eval_metric=\"rmse\",\n                                                    early_stopping_rounds=int(ntrees * patientRate), verbose=False)\n                            result_dic[\"best_params\"][\"best_trees\"] += result_dic[\"model\"].best_iteration_ \/ kfolds.get_n_splits()\n                        result_dic[\"best_params\"][\"best_trees\"] = int(result_dic[\"best_params\"][\"best_trees\"])\n                        result_dic[\"best_params\"][\"best_trees\"] = 1 if result_dic[\"best_params\"][\"best_trees\"] == 0 else result_dic[\"best_params\"][\"best_trees\"]\n\n                        print(\"Tuning Result --->\", result_dic[\"best_params\"])\n\n                        result_dic[\"model\"] = lgb.LGBMRegressor(boosting_type=boostingType, objective=\"regression\",\n                                                                n_estimators=result_dic[\"best_params\"][\"best_trees\"], learning_rate=eta,\n                                                                num_leaves=model_tuner.best_params_[\"num_leaves\"],\n                                                                subsample=model_tuner.best_params_[\"subsample\"],\n                                                                colsample_bytree=model_tuner.best_params_[\"colsample_bytree\"],\n                                                                reg_lambda=model_tuner.best_params_[\"reg_lambda\"],\n                                                                min_child_weight=model_tuner.best_params_[\"min_child_weight\"],\n                                                                min_child_samples=model_tuner.best_params_[\"min_child_samples\"],\n                                                                min_split_gain=model_tuner.best_params_[\"min_split_gain\"],\n                                                                n_jobs=cpu_count(), random_state=seed)\n            else: # not tuning mode\n                pass\n            result_dic[\"model\"].fit(train_x, train_y, categorical_feature=categoIdx, verbose=False)\n\n        if test_x is not None:\n            result_dic[\"pred\"] = result_dic[\"model\"].predict(test_x)\n            if test_y is not None:\n                mae = metrics.mean_absolute_error(test_y, result_dic[\"pred\"])\n                rmse = metrics.mean_squared_error(test_y, result_dic[\"pred\"], squared=False)\n                result_dic[\"performance\"] = {\"MAE\": mae,\n#                                              \"MAPE\": metrics.mean_absolute_percentage_error(test_y, result_dic[\"pred\"]),\n                                             \"NMAE\": (mae \/ test_y.abs().mean()),\n                                             \"RMSE\": rmse,\n                                             \"NRMSE\": (rmse \/ test_y.abs().mean()),\n                                             \"R2\": metrics.r2_score(test_y, result_dic[\"pred\"])}\n            else:\n                result_dic[\"performance\"] = None\n\n    if not model_export: result_dic[\"model\"] = None\n    result_dic[\"running_time\"] = round(time() - runStart, 3)\n    print(f\"Running Time ---> {result_dic['running_time']} sec\")\n    return result_dic\ndef tf_rmse(y_true, y_pred):\n    return tf.sqrt(tf.reduce_mean((y_true -  y_pred) ** 2))","38186097":"# Global setting\nAUTOTUNE = tf.data.experimental.AUTOTUNE\nimg_size = 224\nchannels = 3\ntarget_var = \"Pawpularity\"\n\n# Directory for dataset\nfull_dir = \"\/kaggle\/\/input\/petfinder-pawpularity-score\/test\"\ntest_dir = \"\/kaggle\/\/input\/petfinder-pawpularity-score\/test\"","4de2cd87":"# Reading dataset full, test in df and df_test respectively\nfull_meta = pd.read_csv(\"\/kaggle\/input\/petfinder-pawpularity-score\/train.csv\")\ntest_meta = pd.read_csv(\"\/kaggle\/input\/petfinder-pawpularity-score\/test.csv\")\n\n# Converting Id column for taking images\nfull_img = full_meta[\"Id\"].apply(lambda x : \"\/kaggle\/input\/petfinder-pawpularity-score\/train\/\" + x + \".jpg\")\nfull_y = full_meta[target_var].astype(dtype=\"float32\")\nfull_meta.drop([\"Id\", target_var], axis=1, inplace=True)\nfull_meta = full_meta.astype(dtype=\"float32\")\ntest_img = test_meta[\"Id\"].apply(lambda x : \"\/kaggle\/input\/petfinder-pawpularity-score\/test\/\" + x + \".jpg\")\ntest_meta.drop([\"Id\"], axis=1, inplace=True)\ntest_meta = test_meta.astype(dtype=\"float32\")","a6e6d2be":"# widthVec = []\n# heightVec = []\n# for i in os.listdir(imgPath_full):\n#     imgDic_full[\"id\"].append(i)\n#     pil_obj = Image.open(imgPath_full + i)\n#     img_to_array = tf.keras.preprocessing.image.img_to_array(pil_obj)\n#     widthVec.append(img_to_array.shape[0])\n#     heightVec.append(img_to_array.shape[1])\n\n# print(\"Mean width :\", np.mean(widthVec))\n# print(\"Mean height :\", np.mean(heightVec))\n\n# Mean width : 904.2843018563358\n# Mean height : 804.4262510088781","f54cbb64":"# Augmenting the image\ndef image_preprocessing(augment):\n    def image_read(x, y):\n        # image read\n        image = tf.io.read_file(x[0])\n        image = tf.image.decode_jpeg(image, channels=channels)\n        image = tf.image.resize(image, (img_size, img_size), method=\"lanczos3\")\n        image = tf.cast(image, tf.float32)\n        image = Rescaling(1\/255)(image)\n        return (image, x[1]), y\n    def image_read_augment(x, y):\n        # image read\n        image = tf.io.read_file(x[0])\n        image = tf.image.decode_jpeg(image, channels=channels)\n        image = tf.image.resize(image, (img_size, img_size), method=\"lanczos3\")\n        image = tf.cast(image, tf.float32)\n        # image augmentation\n        image = tf.image.random_saturation(image, 0.8, 1.2)\n        image = tf.image.random_contrast(image, 0.8, 1.2)\n        image = tf.image.random_brightness(image, 0.4)\n        image = tf.image.random_crop(image, size=(int(img_size*0.8), int(img_size*0.8), channels))\n        image = tf.image.resize(image, (img_size, img_size), method=\"lanczos3\")\n        image = Rescaling(1\/255)(image)\n        return (image, x[1]), y\n    return image_read_augment if augment else image_read\n\n# Creating the dataset\ndef create_dataset(img_x=None, cat_x=None, y=None, batch_size=None, augment=False, shuffle=False):\n    dataset = tf.data.Dataset.from_tensor_slices(((img_x, cat_x), y))\n    if img_x is None and cat_x is None:\n        print(\"ERROR : Either image and categorical data should not be none\")\n        return None\n    if img_x is None:\n        dataset = dataset.map(lambda x, y: (x[1], y), num_parallel_calls=AUTOTUNE)\n    elif cat_x is None:\n        dataset = dataset.map(image_preprocessing(augment), num_parallel_calls=AUTOTUNE)\n        dataset = dataset.map(lambda x, y: (x[0], y), num_parallel_calls=AUTOTUNE)\n    else:\n        dataset = dataset.map(image_preprocessing(augment), num_parallel_calls=AUTOTUNE)\n    dataset = dataset.shuffle(1024, reshuffle_each_iteration=True) if shuffle else dataset\n    dataset = dataset.batch(batch_size)\n    dataset = dataset.prefetch(AUTOTUNE)\n    return dataset","fc2d612b":"# NN model parameter setting\nnCols = full_meta.shape[1]\nhiddenLayers = 128\ndropoutRate = 1\/2**2\npretrained_model = tf_hub.KerasLayer(\"..\/input\/pawpularity-pretrained\/resnet50_pretrained_\" + str(img_size), name=\"BiT\", trainable=False)\n\ndef createNN_image(): \n    B0_input = layers.Input(shape=(img_size, img_size, channels), name=\"B0_image_data_input\")\n    \n    x = pretrained_model(B0_input)\n    x = layers.Dropout(dropoutRate)(x)\n    \n    x = layers.Dense(int(hiddenLayers * 8))(x)\n    x = layers.PReLU()(x)\n    x = layers.Dropout(dropoutRate)(x)\n    \n    x = layers.Dense(int(hiddenLayers * 4))(x)\n    x1_last_layer = layers.PReLU()(x)\n\n    return Model(B0_input, x1_last_layer)","257dea24":"# Categorical Embedding Layer\ndef createNN_categoricalEmbedding():\n    # ----------- Embedding layers ----------------------\n    B0_input = layers.Input(shape=(nCols), name=\"B0_cat_input\")\n    B0_embedding = layers.Embedding(input_dim=128,\n                                    output_dim=16,\n                                    name=\"B0_cat_embedding\")(B0_input)\n    # ----------- Convolution layers ----------------------\n    B1_conv1d = layers.Conv1D(4, 1, name=\"B1_cat_conv1d\")(B0_embedding)\n    layer_final = layers.Flatten(name=\"extract\")(B1_conv1d)\n    return Model(B0_input, layer_final)","3c743b3f":"def createNN_ConcatLearning(model_image, model_cat):\n    B0_concat = layers.Concatenate(name=\"B0_img_cat_input\")([model_image.get_layer(index=-1).output, model_cat.get_layer(index=-1).output])\n    \n    # # === learning layers ===\n    B0_dense_regularizer = layers.Dense(units=hiddenLayers * 2, kernel_regularizer=\"l2\", activation=\"relu\", name=\"B0_dense_regularizer\")(B0_concat)\n\n    B1_dense = tfa.layers.WeightNormalization(\n        layers.Dense(units=hiddenLayers, activation=\"selu\", kernel_initializer=\"lecun_normal\"), name=\"B1_dense\"\n    )(B0_dense_regularizer)\n    B1_concat = layers.Concatenate(name=\"B1_concat\")([B0_dense_regularizer, B1_dense])\n    B1_dropout = layers.Dropout(rate=dropoutRate, name=\"B1_dropout\")(B1_concat)\n\n    B2_dense = tfa.layers.WeightNormalization(\n        layers.Dense(units=hiddenLayers, activation=\"relu\", kernel_initializer=\"lecun_normal\"), name=\"B2_dense\"\n    )(B1_dropout)\n    B2_concat = layers.Concatenate(name=\"B2_concat\")([B0_dense_regularizer, B1_dense, B2_dense])\n    B2_dropout = layers.Dropout(rate=dropoutRate, name=\"B2_dropout\")(B2_concat)\n\n    layer_final = tfa.layers.WeightNormalization(\n        layers.Dense(units=hiddenLayers, activation=\"elu\", kernel_initializer=\"lecun_normal\"), name=\"B3_dense\"\n    )(B2_dropout)\n\n    layer_regressor = layers.Dense(1, name=\"regressor\")(layer_final)\n    return Model([model_image.get_layer(index=0).output, model_cat.get_layer(index=0).output], layer_regressor)","42ef889d":"plot_model(createNN_ConcatLearning(createNN_image(), createNN_categoricalEmbedding()), show_shapes=True)","3c6b8d67":"# learning parameter setting\nepochs = 30\npatient_epochs = 10\nbatch_size = 64\neta = 5e-4\nweight_decay = 1e-3\nmodel_name = \"BiT_LP_Try1\"\nfolder_path = \".\/\"\nwb_project_name = \"kaggle_pawpularity\"\ncheckpoint_filepath = '.\/tmp_checkpoint\/'\n\n# fold spliter setting\nkfolds_spliter = StratifiedKFold(10, random_state=4242, shuffle=True)\nstratifed_vector = KBinsDiscretizer(10, strategy=\"quantile\", encode=\"ordinal\").fit_transform(full_y.to_frame()).flatten()\n\n# result container setting\nval_pred = {}\nval_pred[model_name] = np.zeros((full_meta.shape[0],1))\n\ntest_pred = {}\ntest_pred[model_name] = np.zeros((test_meta.shape[0],1))\n\nval_perf = {}\nval_perf[model_name] = []","958fa784":"if os.path.isdir(checkpoint_filepath): shutil.rmtree(checkpoint_filepath)\nseed_everything()\n# fold training\nfor fold, (nonkIdx, kIdx) in enumerate(kfolds_spliter.split(full_meta, stratifed_vector)):\n    print(\"\\n===== Fold\", fold+1, \"=====\\n\")\n    tf.keras.backend.clear_session()\n    \n#     fold_dir = \"fold_\" + str(fold)\n#     log_dir = folder_path + \"models\/\" + model_name + \"\/logs\/\" + fold_dir\n#     if os.path.isdir(log_dir): shutil.rmtree(log_dir)\n#     createFolder(log_dir)\n#     tensorboard_callback = tf_callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)\n    \n#     try:\n#         wandb.tensorboard.patch(root_logdir=log_dir)\n#     except:\n#         pass\n    \n#     wandb.init(\n#         project=wb_project_name,\n#         name=fold_dir,\n#         group=model_name\n#     )\n\n    print(F\"1. {model_name}\")\n    tmp_time = time()\n    train_ds = create_dataset(full_img.iloc[nonkIdx], full_meta.iloc[nonkIdx], full_y.iloc[nonkIdx],\n                              batch_size=batch_size, augment=True, shuffle=True)\n    val_ds = create_dataset(full_img.iloc[kIdx], full_meta.iloc[kIdx], full_y.iloc[kIdx],\n                            batch_size=batch_size, augment=False, shuffle=False)\n    test_ds = create_dataset(test_img, test_meta, None,\n                             batch_size=batch_size, augment=False, shuffle=False)\n    \n    cb_reduceLR = tf_callbacks.ReduceLROnPlateau(patience=1, factor=0.6, min_lr=1e-5)\n    cb_earlyStopping = tf_callbacks.EarlyStopping(patience=patient_epochs, monitor='val_loss', mode='min')\n    cb_modelsave = tf_callbacks.ModelCheckpoint(filepath=checkpoint_filepath, monitor='val_loss', mode='min',\n                                                save_weights_only=True, save_best_only=True)\n\n    model = createNN_ConcatLearning(createNN_image(), createNN_categoricalEmbedding())\n    model.compile(\n        optimizer=tfa.optimizers.AdamW(learning_rate=eta, weight_decay=weight_decay),\n        loss=copy.deepcopy(tf_loss_rmse),\n        metrics=tf_metrics.RootMeanSquaredError(name=\"rmse\")\n    )\n    model.fit(train_ds, validation_data=val_ds, epochs=epochs, verbose=0,\n              callbacks=[cb_reduceLR, cb_earlyStopping, cb_modelsave, TqdmCallback(verbose=0)])\n    model.load_weights(checkpoint_filepath)\n\n    val_pred[model_name][kIdx] = model.predict(val_ds)\n    val_perf[model_name].append(metrics.mean_squared_error(full_y.iloc[kIdx], val_pred[model_name][kIdx], squared=False))\n    test_pred[model_name] += model.predict(test_ds) \/ kfolds_spliter.get_n_splits()\n\n    print(\"Running Time --->\", round(time() - tmp_time, 3),\"sec\")\n    print(F\"{model_name} Fold {fold+1} Performance ---> {val_perf[model_name][-1]}\\n\")\n    print(F\"{model_name} Average Performance ---> {np.mean(val_perf[model_name])}\\n\")\n#     wandb.finish()\n    shutil.rmtree(checkpoint_filepath)","52206cad":"# print(\"===== Meta Learning =====\\n\")\n# for i in meta_learner.keys():\n#     if i == \"Linear\":\n#         meta_learner[i][\"model\"] = lm.LinearRegression(n_jobs=cpu_count())\n#         meta_learner[i][\"model\"].fit(val_pred_x, full_y)\n#         meta_learner[i][\"val_pred\"] = meta_learner[i][\"model\"].predict(val_pred_x)\n#         meta_learner[i][\"val_perf\"] = metrics.mean_squared_error(full_y,  meta_learner[i][\"val_pred\"], squared=False)\n#         meta_learner[i][\"test_pred\"] = meta_learner[i][\"model\"].predict(test_pred_x)\n#         print(\"Linear RMSE :\", meta_learner[i][\"val_perf\"])\n#         print()\n#     elif i == \"MLP_1D\":\n#         meta_learner[i][\"model\"] = Sequential([\n#             layers.Input(shape=(val_pred_x.shape[1])),\n#             layers.Dense(8, use_bias=True, kernel_constraint=tf.keras.constraints.MinMaxNorm(min_value=0.0, max_value=1.0, rate=1.0)),\n#             layers.Dense(1)\n#         ])\n#         epochs = 100\n#         batch_size = 32\n#         cb_earlystopping = tf_callbacks.EarlyStopping(monitor=\"rmse\", patience=20, restore_best_weights=True)\n#         cb_reduceLR = tf_callbacks.ReduceLROnPlateau(patience=2, factor=0.8, min_lr=5e-4)\n#         meta_learner[i][\"model\"].compile(optimizer=optimizers.Adam(learning_rate=eta), loss=\"mean_squared_error\",\n#                                          metrics=tf_metrics.RootMeanSquaredError(name=\"rmse\"))\n#         meta_learner[i][\"model\"].fit(val_pred_x, full_y, epochs=epochs, batch_size=batch_size, verbose=0,\n#                                      callbacks=[cb_earlystopping, cb_reduceLR, TqdmCallback(verbose=0)])\n#         meta_learner[i][\"val_pred\"] = meta_learner[i][\"model\"].predict(val_pred_x)[:,0]\n#         meta_learner[i][\"val_perf\"] = metrics.mean_squared_error(full_y,  meta_learner[i][\"val_pred\"], squared=False)\n#         meta_learner[i][\"test_pred\"] = meta_learner[i][\"model\"].predict(test_pred_x)[:,0]\n#         print(\"MLP_1D RMSE :\", meta_learner[i][\"val_perf\"])\n#         print()","102e6795":"print(\"===== 10-Folds Average RMSE =====\\n\")\nfor k, v in val_perf.items():\n    print(k, \"--->\", np.mean(v))\n\n# print(\"\\n\\n===== Meta Learning Models RMSE =====\\n\")\n# min_rmse_model = (\"None\", np.inf, [0])\n# for k, v in meta_learner.items():\n#     print(k, \"--->\", v[\"val_perf\"])\n#     if v[\"val_perf\"] < min_rmse_model[1]:\n#         min_rmse_model = (k, v[\"val_perf\"], v[\"test_pred\"])","61d90ea8":"submission = read_csv(\"..\/input\/petfinder-pawpularity-score\/sample_submission.csv\")\nsubmission.iloc[:,1] = test_pred[model_name].flatten()\nsubmission.to_csv(\".\/submission.csv\", index=False)","45bde441":"# Image loading and processing functions\n\n* I trasformed the codes a little from this kernel (refer to it)\n\n> https:\/\/www.kaggle.com\/lazybuttryingfinal\/paw-effib4-10fold <- upvote :)","f657109b":"# Validation score summary","ddb18252":"# Loading pretrained model and Defining NN model","d619ce44":"# Utility functions","414b3b54":"# Final Learning Layer\n\n* I trasformed the NN architecture a little from this kernel (refer to it)\n\n<!-- > https:\/\/www.kaggle.com\/pourchot\/neural-network-2-inputs-numerical-categorical <- upvote :) -->\n\n> https:\/\/www.kaggle.com\/pourchot\/simple-neural-network <- upvote :)","06268bcf":"# 10-Folds Prediction","fb7446b1":"# Model Visualization","28db5b28":"# Importing","4031ccfe":"* I used EfficientNet V2 B0 for extracting features from image data\n\n<!-- > Refer to tensorflow hub https:\/\/tfhub.dev\/google\/collections\/bit\/1 -->\n\n> Refer to tensorflow hub https:\/\/tfhub.dev\/google\/collections\/efficientnet_v2\/1\n\n<!-- > Refer to tensorflow hub https:\/\/tfhub.dev\/google\/collections\/efficientnet_v2\/1 -->","9454f2eb":"# Submission"}}