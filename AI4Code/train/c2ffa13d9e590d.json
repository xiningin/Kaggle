{"cell_type":{"24f95a19":"code","9eb7a776":"code","77afa7c0":"code","5b9188c6":"code","fbd3b98b":"code","792f2219":"code","62815edc":"code","ea413cd6":"code","8f771985":"code","a9d57616":"code","43ad5be5":"code","55475366":"code","96e0f788":"code","e97bba75":"code","c9361d0f":"code","611ecdf8":"code","e85c10f8":"code","b51b0397":"code","933e6d5a":"code","e44807db":"code","277f3e0a":"code","2569472b":"code","7e16e377":"code","27b2114a":"code","6dfe4e40":"code","fbc1abd7":"code","c19ffd33":"code","d992150b":"code","3535b8b7":"markdown","50a7e9bd":"markdown","583d0d30":"markdown","33c99be8":"markdown","3866314f":"markdown"},"source":{"24f95a19":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)","9eb7a776":"# Read the data and seperate the predictor variables from the predicted\ndata = pd.read_csv(\"..\/input\/house-prices-advanced-regression-techniques\/train.csv\")\nX = data.drop(['SalePrice'], axis=1)\ny = data.SalePrice\nX_test = pd.read_csv(\"..\/input\/house-prices-advanced-regression-techniques\/test.csv\")","77afa7c0":"X.head()","5b9188c6":"total = X.isnull().sum().sort_values(ascending = False)\npercent = (X.isnull().sum()\/X.isnull().count()).sort_values(ascending = False)\nmissing_data = pd.concat([total, percent], axis =1, keys=['Total', 'Percentage'])\nmissing_data[(missing_data.Percentage > 0)]","fbd3b98b":"X = X.drop(missing_data[missing_data.Percentage > 0.1].index, axis = 1)","792f2219":"X_test = X_test.drop(missing_data[missing_data.Percentage > 0.1].index, axis = 1)","62815edc":"X = X.drop(['SaleType','SaleCondition','Id','MoSold', 'YrSold'], axis = 1)","ea413cd6":"X_test = X_test.drop(['SaleType','SaleCondition','Id','MoSold', 'YrSold'], axis = 1)","8f771985":"numerical_cols = [cname for cname in X.columns if X[cname].dtype in ['int64', 'float64']]\ncategorical_cols = [cname for cname in X.columns if X[cname].nunique() < 10 and \n                    X[cname].dtype == \"object\"]\nmy_cols = categorical_cols + numerical_cols\nprint(categorical_cols,numerical_cols)","a9d57616":"from sklearn.compose import ColumnTransformer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import OneHotEncoder\n\n# Preprocessing for numerical data\nnumerical_transformer = SimpleImputer(strategy='median')\n# Preprocessing for categorical data\ncategorical_transformer = Pipeline(steps=[\n    ('imputer', SimpleImputer(strategy='most_frequent')),\n    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n])\n# Bundle preprocessing for numerical and categorical data\npreprocessor = ColumnTransformer(\n    transformers=[\n        ('num', numerical_transformer, numerical_cols),\n        ('cat', categorical_transformer, categorical_cols)\n    ])","43ad5be5":"X = X[numerical_cols]\nX_test = X_test[numerical_cols]","55475366":"X = numerical_transformer.fit_transform(X)\nX_test = numerical_transformer.fit_transform(X_test)","96e0f788":"print('X shape: ' + str(X.shape))\nprint('X_test shape:  ' + str(X_test.shape))","e97bba75":"from sklearn.feature_selection import SelectKBest, chi2","c9361d0f":"Classifier = SelectKBest(chi2, k=10)\nClassifier.fit(X, y)","611ecdf8":"mask = Classifier.get_support(indices = True)\nX_extracted = X[:,mask]","e85c10f8":"X_extracted.shape","b51b0397":"X_extracted_test = X_test[:,mask]","933e6d5a":"from sklearn.ensemble import RandomForestRegressor\nfrom sklearn.model_selection import RandomizedSearchCV\n\n# Number of trees in random forest\nn_estimators = [int(x) for x in np.linspace(start = 200, stop = 2000, num = 10)]\n# Number of features to consider at every split\nmax_features = ['auto', 'sqrt']\n# Maximum number of levels in tree\nmax_depth = [int(x) for x in np.linspace(10, 110, num = 11)]\nmax_depth.append(None)\n# Minimum number of samples required to split a node\nmin_samples_split = [2, 5, 10]\n# Minimum number of samples required at each leaf node\nmin_samples_leaf = [1, 2, 4]\n# Method of selecting samples for training each tree\nbootstrap = [True, False]\n\n# Create the random grid\nrandom_grid = {'n_estimators': n_estimators,\n               'max_features': max_features,\n               'max_depth': max_depth,\n               'min_samples_split': min_samples_split,\n               'min_samples_leaf': min_samples_leaf,\n               'bootstrap': bootstrap}\n\nprint('Random grid created')\nprint(random_grid)","e44807db":"# Use the random grid to search for best hyperparameters\n# First create the base model to tune\nmodel = RandomForestRegressor()\n# Random search of parameters, using 3 fold cross validation, \n# search across 100 different combinations, and use all available cores\nmodel_hyper = RandomizedSearchCV(estimator = model, param_distributions = random_grid, n_iter = 100, cv = 3, verbose=10, random_state=42, \n                                 n_jobs = -1)","277f3e0a":"model_hyper.fit(X_extracted ,y)","2569472b":"PARAMS = model_hyper.best_params_","7e16e377":"model = RandomForestRegressor(**PARAMS)","27b2114a":"model.fit(X_extracted, y)","6dfe4e40":"predictions = model.predict(X_extracted_test)","fbc1abd7":"pipeline = Pipeline(steps=[('preprocessor', preprocessor),\n                              ('model', model)\n                             ])","c19ffd33":"X_test = pd.read_csv(\"..\/input\/house-prices-advanced-regression-techniques\/test.csv\")","d992150b":"output = pd.DataFrame({'Id': X_test.Id,\n                       'SalePrice': predictions})\noutput.to_csv('submission.csv', index=False)","3535b8b7":"### Creating of the pipeline\nInserting the preprocessor, the model and assessing the quality using cross validation.","50a7e9bd":"### Model\nDefining of a model.","583d0d30":"### Missing Values\nDrop columns that have more than 10 percent missing values. Impute the other columns with median strategy","33c99be8":"### Find out if you only need to transform X_ext and how this goes to X_test","3866314f":"### Preprocess\nImputation of missing values & encoding of catergorical variables"}}