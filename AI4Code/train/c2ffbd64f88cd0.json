{"cell_type":{"242f0536":"code","fce4756e":"code","11159de2":"code","05fe67b5":"code","72dd4b65":"code","1d72565d":"code","58ba53d7":"code","2039f26f":"code","9f03c81a":"code","ad068aa4":"code","cf5aeb70":"code","8350b241":"code","52cc1277":"code","08ccf8d8":"code","916dc996":"code","b59cf1db":"code","9b0e2341":"code","04383af9":"code","ea07db04":"code","73481164":"code","1c44a0e2":"code","ce499431":"code","3c32d74b":"code","1764a445":"code","f4a26436":"code","b2570866":"code","4ad8d6bd":"code","5487f868":"code","5a4ec6d1":"code","c09f98db":"code","1d9b4c46":"code","10dc96f0":"code","c08bc607":"code","2692572b":"code","b563d9d1":"code","5a01ebc7":"code","e39a4208":"code","ec4f94b1":"code","96e3b7a6":"code","486ec9f7":"code","659848d7":"code","13027e54":"code","6148bf79":"code","8a4d6d15":"code","264f5d76":"code","97d9d5ca":"code","b96f1b37":"code","d715910f":"code","c92005bd":"code","93790c43":"code","da095f24":"code","c6987ec0":"code","c26fb907":"code","0fd8c4a1":"code","c5054a62":"code","f793e6f7":"code","838f2750":"code","7ee7a157":"code","a038e7a4":"code","e2d41e24":"code","e53c1f24":"code","348c6e43":"code","4773a26d":"code","b8d0132d":"code","b168bdea":"code","da8a322f":"code","1c0eb193":"code","314bfb55":"code","b593855a":"code","89ae540a":"code","6dffd146":"code","4fa6763d":"code","0e4c728b":"code","06d421e3":"code","6c06aed4":"code","71fa24ec":"code","982e8b1a":"code","18c6bc23":"code","cf9685b8":"code","ac00f6c3":"code","ee015b48":"code","ef5a81c2":"code","b0a43fed":"code","42a9d73f":"code","61043e20":"code","5d90019e":"code","0abcb402":"code","7119ab38":"code","f6efe236":"code","f4bb8865":"code","6d415f77":"code","64762314":"code","445b1ec2":"code","2b453347":"code","160e8a55":"code","7d8dc61a":"code","f2365fb4":"code","085b7d18":"code","5969eb68":"code","fb740b88":"code","2649611f":"code","084fb13f":"code","6ba9582a":"code","5e4f72fa":"code","cd2326de":"code","ff7e646c":"code","047de282":"code","5d8fb8b3":"code","dbfa6433":"code","bcaa4a21":"code","61d1b968":"code","8c1f3214":"code","ea2f1fa2":"code","7f5402dc":"code","9de03ac8":"code","fe2aad89":"code","90ce39f0":"code","f9da20bc":"code","31674fe1":"code","04e2f477":"code","8a79c868":"code","227ea7ed":"code","e913f23a":"code","3aaa5d4f":"code","2ed50916":"code","31efd768":"code","f57d6ca2":"code","6ff90359":"code","56aa8a80":"code","7fa9474f":"code","7e552dbf":"code","c8212a21":"code","ba261ba6":"code","b1096142":"code","bd878fbd":"code","24f2218c":"code","9e5bd701":"code","13d8cc13":"code","187a58c7":"code","d5e22a62":"code","dcc5f0ae":"code","85667323":"code","28b6627a":"code","a5812940":"code","694a5183":"code","e029c603":"code","54d2ac52":"code","4a6e0d91":"markdown","999fa1f1":"markdown","07369878":"markdown","b4d83270":"markdown","72eed189":"markdown","cf046f70":"markdown","b3a18c21":"markdown","17d4b4ca":"markdown","1f12fa16":"markdown","9f49dfd6":"markdown","61bd1ed2":"markdown","eaec301e":"markdown","32dad48b":"markdown","58aa2824":"markdown","b94b326a":"markdown","d10f2082":"markdown","a96685b1":"markdown","14535d22":"markdown","93fee2c1":"markdown","2775ff45":"markdown","c778d246":"markdown","97ea5755":"markdown","8c88e6af":"markdown","8699c51c":"markdown","30d0aba6":"markdown","e9de1d3a":"markdown","90a40e94":"markdown","bb94c11c":"markdown","9ae5f841":"markdown","45f3e869":"markdown","b815cc44":"markdown","da78c307":"markdown","0f8e9a8a":"markdown","00e637a2":"markdown","34654c03":"markdown","07b15515":"markdown","6c259d45":"markdown","c2a067a7":"markdown","c7de9e95":"markdown","e3a8e8ee":"markdown","64d5c1f4":"markdown","cac548d4":"markdown","3f6efb3b":"markdown","657f4c6a":"markdown","79c75af5":"markdown","3d905170":"markdown","258dffe4":"markdown","5bb0c1bd":"markdown","53636777":"markdown","c7fe9b90":"markdown","f4ad86e5":"markdown","f432adc6":"markdown","66f1248c":"markdown","bc5760a0":"markdown","ca5e6b62":"markdown","d3dcb539":"markdown","2b846a61":"markdown","e03d9989":"markdown","29aa0b1a":"markdown","d236bc51":"markdown","771c465e":"markdown","7358c17b":"markdown","382d99ac":"markdown","51253f10":"markdown","e38e4ce9":"markdown","070efc7d":"markdown","582ea256":"markdown","b1567e1e":"markdown","fa66679d":"markdown","6546eac9":"markdown","c03639b0":"markdown","f009dbc0":"markdown","5a66947e":"markdown","3ffa63dd":"markdown","edf9bfdc":"markdown","0ee7ee61":"markdown","77b6c024":"markdown","db8cd98e":"markdown","a7d03468":"markdown","a62aa000":"markdown","713d558c":"markdown","b62070f3":"markdown","d8f2f450":"markdown","24b7aec9":"markdown","f958428b":"markdown","e893ee71":"markdown","83e7049f":"markdown","4d05ec14":"markdown","83dfe739":"markdown","38a75214":"markdown","a6323c24":"markdown","37c32e88":"markdown","498c344d":"markdown","2c4cd1f2":"markdown","f46b50f1":"markdown","364ddfe6":"markdown","6e110a2a":"markdown","021dea00":"markdown","28c9eafd":"markdown","6085ac75":"markdown","a657de6d":"markdown","7c28219e":"markdown","e3b2d4a5":"markdown","4ef39c06":"markdown","c2831690":"markdown","548823a1":"markdown","f33a367a":"markdown","ac91f738":"markdown","00993120":"markdown","605acddc":"markdown","c05d1973":"markdown","3980a403":"markdown","6bb52cf8":"markdown","4c3e9b7a":"markdown","87d95478":"markdown","ce8dda22":"markdown","e34ce10d":"markdown","f3882c6b":"markdown","5c8bdc16":"markdown","3ca57beb":"markdown","726ac2df":"markdown","5952ed7f":"markdown","4234cfa3":"markdown","4ed6230f":"markdown","1d22ad9f":"markdown","bedc67cc":"markdown","9f60c81a":"markdown","91ab8249":"markdown","8de9ef2a":"markdown","91e8f362":"markdown","1ecee5e2":"markdown","efa1e9ea":"markdown","6ec83c56":"markdown","7108c873":"markdown","3c68ce9b":"markdown","0a855e8b":"markdown","0aef1d08":"markdown","f5f6a254":"markdown","eba7372e":"markdown","5283daf9":"markdown","9fd9a9ec":"markdown","20be0de2":"markdown","df297e91":"markdown","f1276c7b":"markdown","a87e726f":"markdown","c829d0ec":"markdown","9a45617e":"markdown","7c75abbf":"markdown","014b3700":"markdown","a8562d9e":"markdown","7514a5be":"markdown","46596364":"markdown","6e358e36":"markdown","806495a1":"markdown","e2b4c221":"markdown","1cdee02a":"markdown","56a1490d":"markdown","d5530ca0":"markdown","c615b591":"markdown","e8f8e3c5":"markdown","d73c432c":"markdown","9c14df28":"markdown","2ebb955b":"markdown","754b7c01":"markdown","47ee22c3":"markdown","58561231":"markdown","89cd170e":"markdown","96cf6501":"markdown","9b78bcc3":"markdown","c39b7e5e":"markdown","24b2e8b0":"markdown","04815ec5":"markdown","d7058433":"markdown","ab5b1576":"markdown","41b589f3":"markdown","b947f5fd":"markdown","379478bc":"markdown","30123e8b":"markdown","91ba7df9":"markdown","d1a5e744":"markdown","bc502a53":"markdown","8af34361":"markdown","3be8737a":"markdown","545e2d42":"markdown","7166dae8":"markdown","1d8935ef":"markdown","4c51f424":"markdown","3e84132f":"markdown","99d65212":"markdown","112630f8":"markdown","eab0abf9":"markdown","96fdb2c2":"markdown","7083a3f0":"markdown","38cbb1bc":"markdown","826eba2a":"markdown","bcb7095a":"markdown","e5408174":"markdown","a4428298":"markdown","7b8c014c":"markdown","cd32ad67":"markdown","6b6eac61":"markdown","0522211b":"markdown","6ac35769":"markdown","074c275c":"markdown","6ff09b48":"markdown","48925e06":"markdown","f2599bd9":"markdown","9293827e":"markdown","67ef8275":"markdown","145d47b9":"markdown","2335ba18":"markdown","6a143868":"markdown","d7044b5c":"markdown","14a13b01":"markdown","3f5b52a2":"markdown","c52319d2":"markdown","699d6b0c":"markdown","d1da4733":"markdown","5933d79a":"markdown","bfcf333d":"markdown","fd418d68":"markdown","c2f83215":"markdown","039d3935":"markdown","d6744ef2":"markdown"},"source":{"242f0536":"import numpy as np   #numpy for matehmatical computations\nimport pandas as pd  #pandas for data computations\n\n\nimport warnings      #warnings to supress second and subsequent repeated warnings\nwarnings.filterwarnings(\"ignore\")\n\n#datetime imports for date - time computations\nimport time\nimport datetime as dt\nfrom datetime import date\nfrom datetime import datetime\nfrom datetime import timedelta\n\n\nimport json #json for handling json data\n\n#Visualization Imports\nimport seaborn as sns\nsns.set_palette('Pastel2')\nimport matplotlib.pyplot as plt\n%matplotlib inline\nfrom plotly import __version__\n#%matplotlib inline\n\nfrom plotly.tools import FigureFactory as FF\nimport cufflinks as cf\nimport plotly as py\nimport plotly.offline as pyo\nimport plotly.graph_objs as go\nimport folium\nimport plotly.express as px\nimport plotly.tools as tls\nimport cufflinks as cf\nfrom plotly.offline import download_plotlyjs, init_notebook_mode, plot, iplot\ninit_notebook_mode(connected=True)\ncf.go_offline()\n\n#StandardScaler Data Normalization Imports\nfrom sklearn.preprocessing import StandardScaler\n\n#Kmeans cluter algorithm import\nfrom sklearn.cluster import KMeans\n\n#Identify Silhouette score for best KMeans cluster import\nfrom sklearn.metrics import silhouette_score\n\n# Supress Scientific notation in python import\npd.set_option('display.float_format', lambda x: '%.2f' % x)\n\n# Display all columns of long dataframe\npd.set_option('display.max_columns', None)","fce4756e":"path = \"..\/input\/input-datasets\/\"","11159de2":"# Import input datasets\ntrain = pd.read_excel(path+'train.xlsx', parse_dates=['InvoiceDate'])\ntest = pd.read_excel(path+'test.xlsx', parse_dates=['InvoiceDate'])","05fe67b5":"#Bonus Steps : Import Country Dataset - For Visualization in Tableau have added a Country to Country Code Mapping\ncountries = pd.read_excel(path+'Country.xlsx')","72dd4b65":"# Check the shape of dataframe\ntrain.shape","1d72565d":"train.head()","58ba53d7":"# Check the shape of dataframe\ntest.shape","2039f26f":"test.head()","9f03c81a":"data = pd.concat([train, test], ignore_index=True)\ndata.reset_index(drop=True,inplace = True)\ndata.shape","ad068aa4":"data.head()","cf5aeb70":"data.info()","8350b241":"data.describe()","52cc1277":"dataDuplicates = pd.DataFrame(columns=['Type','Count'])\ndataDuplicates = dataDuplicates.append({'Type': \"Duplicates\",'Count' : data.duplicated().sum()}, ignore_index=True)\ndataDuplicates = dataDuplicates.append({'Type': \"Unique Value\",'Count' : data.count()[0] - data.duplicated().sum()}, ignore_index=True)\ndataDuplicates\n# Set notebook mode to work in offline\npyo.init_notebook_mode()\n# Use `hole` to create a donut-like pie chart\nfig = go.Figure(data=[go.Pie(labels=dataDuplicates.Type, values=dataDuplicates.Count, title = \"Duplicates Spread\",hole=.8)])\nfig.show()","08ccf8d8":"data.drop_duplicates(inplace=True)\ndata.reset_index(drop=True,inplace = True)\ndata.shape","916dc996":"data.CustomerID.notnull().sum()","b59cf1db":"dataCustomerID = pd.DataFrame(columns=['Type','Count'])\ndataCustomerID = dataCustomerID.append({'Type': \"Null Values\",'Count' : data.count()[0] - data.CustomerID.notnull().sum()}, \n                                       ignore_index=True)\ndataCustomerID = dataCustomerID.append({'Type': \"Populated Values\",'Count' : data.CustomerID.notnull().sum()}, ignore_index=True)\ndataCustomerID\n# Set notebook mode to work in offline\npyo.init_notebook_mode()\n# Use `hole` to create a donut-like pie chart\nfig = go.Figure(data=[go.Pie(labels=dataCustomerID.Type, values=dataCustomerID.Count, \n                             title = \"Customer ID Missing Values\",hole=.8)])\nfig.show()","9b0e2341":"# We don't need records with Null customer id for RFM analysis so let's remove those first\ndata = data[data.CustomerID.notnull()]\ndata.reset_index(drop=True,inplace = True)\ndata.shape","04383af9":"data.info()","ea07db04":"# title function in python to convert the character column to title case or proper case\n \ndata['Description'] = data['Description'].str.title()\ndata.head()","73481164":"# Convert CustomerID to int type\ndata.CustomerID = (data.CustomerID).astype(int)\ndata.head(5)","1c44a0e2":"# Convert CustomerId to str type\ndata.CustomerID = (data.CustomerID).astype(str)\ndata.info()","ce499431":"groupByCountry = data.groupby([\"Country\"], as_index=False)['InvoiceNo'].count().rename(columns={'Country':'Country','InvoiceNo' : 'Count'})\ngroupByCountry.head()","3c32d74b":"cf.set_config_file(theme='white')\ngroupByCountry.iplot(kind = 'bar', x = 'Country', y = 'Count', title = 'Transaction Count of Purchases by Country',\n                     xTitle='Country', yTitle='#Purchased')","1764a445":"data.Country.value_counts(normalize=True).head(3).mul(100).round(1).astype(str) + '%'","f4a26436":"# Create feature total cost of the transactions\ndata['TotalCost'] = data.Quantity * data.UnitPrice\ndata.head()","b2570866":"groupBySumTransactionCountry = data.groupby(['Country'], as_index=False)['TotalCost'].sum().rename(columns={'Country':'Country','TotalCost' : 'SumPurchase'})\ngroupBySumTransactionCountry = groupBySumTransactionCountry.sort_values(by='SumPurchase', ascending=False)\ngroupBySumTransactionCountry.head()\ncf.set_config_file(theme='white')\ngroupBySumTransactionCountry.iplot(kind = 'bar', x = 'Country', y = 'SumPurchase', title = 'Sum Purchases By Country',\n                     xTitle='Country', yTitle='Purchase Sum in \u00a3')","4ad8d6bd":"groupByAverageTransactionCountry = data.groupby(['Country'], as_index=False)['TotalCost'].mean().rename(columns={'Country':'Country','TotalCost' : 'MeanPurchase'})\ngroupByAverageTransactionCountry = groupByAverageTransactionCountry.sort_values(by='MeanPurchase', ascending=False)\ngroupByAverageTransactionCountry.head()\ncf.set_config_file(theme='white')\ngroupByAverageTransactionCountry.iplot(kind = 'bar', x = 'Country', y = 'MeanPurchase', title = 'Mean Purchases By Country',\n                     xTitle='Country', yTitle='Purchase Mean in \u00a3 Per Transaction')","5487f868":"# Count of transactions in different years\ngroupYear = data.InvoiceDate.dt.year.value_counts(sort=False).rename_axis('Year').reset_index(name='TransactionCount')\ngroupYear.Year = groupYear.Year.astype(str)\ngroupYear.Year = 'Year ' + groupYear.Year\ncf.set_config_file(theme='white')\ngroupYear.iplot(kind = 'barh', x = 'Year', y = 'TransactionCount', title = 'Transactions By Year',\n                     yTitle='Year of Sale', xTitle='# of Transactions',text = 'TransactionCount')","5a4ec6d1":"# Count of transactions in 2011\ngroupMonth = data[data.InvoiceDate.dt.year==2011].InvoiceDate.dt.month.value_counts(sort=False).rename_axis('Month').reset_index(name='TransactionCount')\ngroupMonth\ngroupMonth.Month = groupMonth.Month.astype(str) + '-2011'\ngroupMonth\ncf.set_config_file(theme='white')\ngroupMonth.iplot(kind = 'barh', x = 'Month', y = 'TransactionCount', title = 'Transactions By Month in 2011',\n                     yTitle='Month of Sale', xTitle='# of Transactions',text = 'TransactionCount')","c09f98db":"groupByDescription = data.groupby([\"Description\"], as_index=False)['InvoiceNo'].count().rename(columns={'Description':'Description','InvoiceNo' : 'Count'})\ngroupByDescription = groupByDescription.sort_values(by='Count', ascending=False).head(20)\ncf.set_config_file(theme='white')\ngroupByDescription.iplot(kind = 'barh', x = 'Description', y = 'Count', title = 'Top 20 Spread of Purchases by Description',\n                     yTitle='Description', xTitle='#Purchased',text = 'Count')","1d9b4c46":"groupByTotalCost = data.groupby([\"InvoiceNo\"], as_index=False)['TotalCost'].sum().rename(columns={'InvoiceNo':'InvoiceNo','TotalCost' : 'PurchaseTotal'})\ngroupByTotalCost.InvoiceNo = groupByTotalCost.InvoiceNo.astype(str)\ngroupByTotalCost = groupByTotalCost.sort_values(by='PurchaseTotal', ascending=False).head(20)\ngroupByTotalCost.InvoiceNo = \"INV \" + groupByTotalCost.InvoiceNo\ngroupByTotalCost.head()","10dc96f0":"cf.set_config_file(theme='white')\ngroupByTotalCost.iplot(kind = 'barh', x = 'InvoiceNo', y = 'PurchaseTotal', title = 'Top 20 Purchases by Invoice Number',\n                     yTitle='Invoice Number', xTitle='Purchase Total')","c08bc607":"groupByTotalCost2 = data.groupby([\"InvoiceNo\"], as_index=False)['TotalCost'].sum().rename(columns={'InvoiceNo':'InvoiceNo','TotalCost' : 'PurchaseTotal'})\ngroupByTotalCost2.InvoiceNo = groupByTotalCost2.InvoiceNo.astype(str)\ngroupByTotalCost2 = groupByTotalCost2.sort_values(by='PurchaseTotal', ascending=True).head(20)\ngroupByTotalCost2.InvoiceNo = \"INV \" + groupByTotalCost2.InvoiceNo\ngroupByTotalCost2.head()","2692572b":"cf.set_config_file(theme='white')\ngroupByTotalCost2.iplot(kind = 'barh', x = 'InvoiceNo', y = 'PurchaseTotal', title = 'Bottom 20 Purchases by Invoice Number',\n                     yTitle='Invoice Number', xTitle='Purchase Total')","b563d9d1":"groupByTotalCost1 = data.groupby([\"CustomerID\"], as_index=False)['TotalCost'].sum().rename(columns={'CustomerID':'CustomerID','TotalCost' : 'PurchaseTotal'})\ngroupByTotalCost1.CustomerID = groupByTotalCost1.CustomerID.astype(str)\ngroupByTotalCost1 = groupByTotalCost1.sort_values(by='PurchaseTotal', ascending=False).head(20)\ngroupByTotalCost1.CustomerID = \"CUSID \" + groupByTotalCost1.CustomerID\ngroupByTotalCost1.head()\ncf.set_config_file(theme='white')\ngroupByTotalCost1.iplot(kind = 'barh', x = 'CustomerID', y = 'PurchaseTotal', title = 'Top 20 Customer ID by Purchases',\n                     yTitle='Customer ID', xTitle='Purchase Total')","5a01ebc7":"sns.set(style=\"whitegrid\")\nax1 = sns.boxplot(x=data[\"Quantity\"],orient=\"h\", palette=\"Pastel2\")","e39a4208":"#Outliers InterQuartile Range (IQR) 1.5 IQR Rule for Numeric Data\ndef outlierDetection(datacolumn):\n    #Sort the data in ascending order\n    #GET Q1 and Q3\n    sorted(datacolumn)\n    Q1,Q3 = np.percentile(datacolumn, [25,75])\n    \n    #Calc IQR\n    IQR = Q3 - Q1\n    \n    #Calc LowerRange\n    lr = Q1 - (1.5 * IQR)\n    #Calc Upper Range\n    ur = Q3 + (1.5 * IQR)\n    #return 1,2\n    return lr,ur\n\nQuantityOutliersDataFrame = pd.DataFrame(columns=['FeatureUniqueValues',\n                                         'lowerRange','upperRange','OutlierLower','OutlierUpper','OutlierFoundStatus'])\nlowerRange,upperRange = outlierDetection(data['Quantity'])\noutlier_upper = data['Quantity'] > upperRange \noutlier_lower = data['Quantity'] < lowerRange\nif outlier_upper.any() or outlier_lower.any():\n    OutlierFoundStatus = True\nelse:\n    OutlierFoundStatus = False\nQuantityOutliersDataFrame = QuantityOutliersDataFrame.append({'FeatureUniqueValues': data['Quantity'].nunique(),\n                                                              'lowerRange' : lowerRange, \n                                                              'upperRange' : upperRange,\n                                                              'OutlierLower' : data['Quantity'].min(),\n                                                              'OutlierUpper' :data['Quantity'].max(),\n                                                              'OutlierFoundStatus' : OutlierFoundStatus\n                                                             }, ignore_index=True)\nQuantityOutliersDataFrame","ec4f94b1":"dataQuantity = pd.DataFrame(columns=['Type','Count'])\ndataQuantity = dataQuantity.append({'Type': \"Negative Values\",'Count' : (data.Quantity < 0).sum()}, \n                                       ignore_index=True)\ndataQuantity = dataQuantity.append({'Type': \"Zero Values\",'Count' : (data.Quantity == 0).sum()}, \n                                       ignore_index=True)\ndataQuantity = dataQuantity.append({'Type': \"Positive Values\",'Count' : (data.Quantity > 0).sum()}, \n                                       ignore_index=True)\n\n# Set notebook mode to work in offline\npyo.init_notebook_mode()\n# Use `hole` to create a donut-like pie chart\nfig = go.Figure(data=[go.Pie(labels=dataQuantity.Type, values=dataQuantity.Count, title = \"Quantity Classification\",hole=.8)])\nfig.show()","96e3b7a6":"data[(data.Quantity < -20000) | (data.Quantity > 20000)]","486ec9f7":"# We don't need records with Quantity < 0 and positive outliers which are cancelled for RFM analysis \n#so let's remove those records\ndata5 = data[(data.Quantity <=0) | (data.Quantity > 20000)]\ndata5.reset_index(drop=True,inplace = True)\n\ndata = data[(data.Quantity>0) & (data.Quantity <= 20000)]\ndata.reset_index(drop=True,inplace = True)\ndata.shape","659848d7":"sns.set(style=\"whitegrid\")\nax1 = sns.boxplot(x=data[\"Quantity\"],orient=\"h\", palette=\"Pastel2\")","13027e54":"sns.set(style=\"whitegrid\")\nax1 = sns.boxplot(x=data[\"UnitPrice\"],orient=\"h\", palette=\"Pastel2\")","6148bf79":"UnitPriceOutliersDataFrame = pd.DataFrame(columns=['FeatureUniqueValues',\n                                         'lowerRange','upperRange','OutlierLower','OutlierUpper','OutlierFoundStatus'])\nlowerRange,upperRange = outlierDetection(data['UnitPrice'])\noutlier_upper = data['UnitPrice'] > upperRange \noutlier_lower = data['UnitPrice'] < lowerRange\nif outlier_upper.any() or outlier_lower.any():\n    OutlierFoundStatus = True\nelse:\n    OutlierFoundStatus = False\nUnitPriceOutliersDataFrame = UnitPriceOutliersDataFrame.append({'FeatureUniqueValues': data['UnitPrice'].nunique(),\n                                                              'lowerRange' : lowerRange, \n                                                              'upperRange' : upperRange,\n                                                              'OutlierLower' : data['UnitPrice'].min(),\n                                                              'OutlierUpper' :data['UnitPrice'].max(),\n                                                              'OutlierFoundStatus' : OutlierFoundStatus\n                                                             }, ignore_index=True)\nUnitPriceOutliersDataFrame","8a4d6d15":"dataUnitPrice = pd.DataFrame(columns=['Type','Count'])\ndataUnitPrice = dataUnitPrice.append({'Type': \"Negative Values\",'Count' : (data.UnitPrice < 0).sum()}, \n                                       ignore_index=True)\ndataUnitPrice = dataUnitPrice.append({'Type': \"Zero Values\",'Count' : (data.UnitPrice == 0).sum()}, \n                                       ignore_index=True)\ndataUnitPrice = dataUnitPrice.append({'Type': \"Positive Values\",'Count' : (data.UnitPrice > 0).sum()}, \n                                       ignore_index=True)\n\n# Set notebook mode to work in offline\npyo.init_notebook_mode()\n# Use `hole` to create a donut-like pie chart\nfig = go.Figure(data=[go.Pie(labels=dataUnitPrice.Type, values=dataUnitPrice.Count, \n                             title = \"UnitPrice Classification\",hole=.8)])\nfig.show()","264f5d76":"# We don't need records with UnitPrice < 0 for RFM analysis so let's remove those records\ndata = data[data.UnitPrice > 0]\ndata.reset_index(drop=True,inplace = True)\ndata.shape","97d9d5ca":"sns.set(style=\"whitegrid\")\nax1 = sns.boxplot(x=data[\"UnitPrice\"],orient=\"h\", palette=\"Pastel2\")","b96f1b37":"dataInvoiceDate = pd.DataFrame(columns=['Type','Count'])\ndataInvoiceDate = dataInvoiceDate.append({'Type': \"From December 2011\",'Count' \n                                          : (data.InvoiceDate >= \"2011-12-01 00:00:00\").sum()}, ignore_index=True)\ndataInvoiceDate = dataInvoiceDate.append({'Type': \"Before December 2011\",'Count' \n                                          : (data.InvoiceDate < \"2011-12-01 00:00:00\").sum()},ignore_index=True)\n# Set notebook mode to work in offline\npyo.init_notebook_mode()\n# Use `hole` to create a donut-like pie chart\nfig = go.Figure(data=[go.Pie(labels=dataInvoiceDate.Type, values=dataInvoiceDate.Count, \n                             title = \"Invoice Date Classification\",hole=.8)])\nfig.show()","d715910f":"#We need only 1 year of data, removing data of December 2011 which has partial month data\ndata = data[data.InvoiceDate < \"2011-12-01 00:00:00\"]\ndata.reset_index(drop=True,inplace = True)\ndata.shape","c92005bd":"groupByTotalCost3 = data.groupby([\"CustomerID\"], as_index=False)['TotalCost'].sum().rename(columns={'CustomerID':'CustomerID','TotalCost' : 'PurchaseTotal'})\ngroupByTotalCost3.CustomerID = groupByTotalCost3.CustomerID.astype(str)\ngroupByTotalCost3 = groupByTotalCost3.sort_values(by='PurchaseTotal', ascending=False).head(20)\ngroupByTotalCost3.CustomerID = \"CUSID \" + groupByTotalCost3.CustomerID\ngroupByTotalCost3.head()\ncf.set_config_file(theme='white')\ngroupByTotalCost3.iplot(kind = 'barh', x = 'CustomerID', y = 'PurchaseTotal', title = 'Top 20 Customer ID by Purchases After EDA',\n                     yTitle='Customer ID', xTitle='Purchase Total')","93790c43":"countries.info()","da095f24":"countries[countries.Country_Code.isna()]","c6987ec0":"countries = countries.dropna()\ncountries.reset_index(drop=True,inplace = True)\ncountries.shape","c26fb907":"#Create Dataframe dataTableau\ndataTableau = pd.merge(data,countries,on=\"Country\", how='left')\ndataTableau.head()                ","0fd8c4a1":"dataTableau['Region'] = 'Others'\ndataTableau.Region[dataTableau.Country ==\"United Kingdom\"] = 'UK'\ndataTableau.head()","c5054a62":"##### Create MonthYear Column for Visualization\ndef obtain_TableauMonthYear(InvoiceDate):\n    return dt.datetime(InvoiceDate.year,InvoiceDate.month,1) \n\n##### Compute column InvoiceMonth column applying function obtain_CohortMonth on the datetime column InvoiceDate\ndataTableau['InvoiceMonth'] = dataTableau['InvoiceDate'].apply(obtain_TableauMonthYear) \ndataTableau['MonthYear'] = dataTableau['InvoiceMonth'].dt.strftime('%Y-%m')\ndataTableau.drop(['InvoiceMonth'], axis=1, inplace=True)\ndataTableau.head()","f793e6f7":"dataTableau.info()","838f2750":"dataRefundCalc = data5[(data5.Quantity <=0)]\ndataRefundCalc.reset_index(drop=True,inplace = True)\ngroupByNegativeInvoices = dataRefundCalc.groupby([\"InvoiceNo\"], as_index=False)['TotalCost'].sum().rename(columns={'InvoiceNo':'InvoiceNo','TotalCost' : 'RefundValue'})\ngroupByNegativeInvoices = groupByNegativeInvoices.sort_values(by='RefundValue', ascending=False).head(20)\ngroupByNegativeInvoices.shape","7ee7a157":"refundedInvoices = pd.DataFrame(columns=['RefundedInvoices'])\nrefundedInvoices = refundedInvoices.append({'RefundedInvoices': len(groupByNegativeInvoices)},ignore_index=True)\nrefundedInvoices","a038e7a4":"cohortdata = data.copy()\ncohortdata.shape","e2d41e24":"cohortdata.head()","e53c1f24":"##### Date Parsing Function obtain_CohortMonth\ndef obtain_CohortMonth(InvoiceDate):\n    return dt.datetime(InvoiceDate.year,InvoiceDate.month,1) \n\n##### Compute column InvoiceMonth column applying function obtain_CohortMonth on the datetime column InvoiceDate\ncohortdata['InvoiceMonth'] = cohortdata['InvoiceDate'].apply(obtain_CohortMonth) \n\n###### Create a Grouping cohortGrouping on the CustomerID and InvoiceMonth\ncohortGrouping = cohortdata.groupby('CustomerID')['InvoiceMonth'] \n\n###### Compute the CohortMonth (month of first transaction) for each Customer by identifying first transaction\ncohortdata['CohortMonth'] = cohortGrouping.transform('min')\n\ncohortdata.head()","348c6e43":"def obtainDateInterval(dataframe, column):\n    year = dataframe[column].dt.year\n    month = dataframe[column].dt.month\n    return year, month","4773a26d":"# Parse Year & Month of InvoiceMonth column into invoiceYear & invoiceMonth columns using function obtainDateInterval\ninvoiceYear, invoiceMonth = obtainDateInterval(cohortdata,'InvoiceMonth')\n\n# Parse Year & Month of CohortMonth column into cohortYear & cohortMonth columns using function obtainDateInterval\ncohortYear, cohortMonth = obtainDateInterval(cohortdata,'CohortMonth')","b8d0132d":"# Calculate difference between invoiceYear and the assigned cohortYear in years for every row item\ndiffYears = invoiceYear - cohortYear\n\n# Calculate difference between invoiceMonth and the assigned cohortMonth in months for every row item\ndiffMonths = invoiceMonth - cohortMonth\n\n# Set CohortIndex as diffYears*12 + diffMonths + 1 : \n#1 is added so that CohortIndex is never 0 if the first and subsequent transaction for customer are in same month\ncohortdata['CohortIndex'] = diffYears.mul(12) + diffMonths + 1\n\ncohortdata.head()","b168bdea":"#Create Individual Months List for cohort analysis\nmapMonths = cohortdata.groupby([\"InvoiceMonth\"], as_index=False)['TotalCost'].sum().rename(columns={'InvoiceMonth':'InvoiceMonth','TotalCost' : 'PurchaseTotal'})\nmapMonths =mapMonths.sort_values(by='InvoiceMonth')\nmapMonths.reset_index(drop=True,inplace = True)\nmapMonths['MonthYear'] = mapMonths['InvoiceMonth'].dt.strftime('%b-%Y')\nmapMonthsList = mapMonths['MonthYear'].to_list()\nmapMonthsList","da8a322f":"# Create a groupby object MonthlyActiveCustomerGroup and pass the monthly cohort and cohort index as a list\nMonthlyActiveCustomerGroup = cohortdata.groupby(['CohortMonth', 'CohortIndex']) \n\n# Calculate the sum of the TotalCost column\nMonthlyCustomers = MonthlyActiveCustomerGroup['CustomerID'].apply(pd.Series.nunique)\n\n# Reset the index of cohort_data\nMonthlyCustomers = MonthlyCustomers.reset_index()\n\n# Create a pivot \nTotalMonthlyCustomers = MonthlyCustomers.pivot(index='CohortMonth', columns='CohortIndex', values='CustomerID')\n\n# Display Monthly Customer Cohort count\nTotalMonthlyCustomers","1c0eb193":"# Initialize plot figure\nplt.figure(figsize=(20, 15))\n\n# Add a title\nplt.title('Active Customer Count by Cohort Month',fontsize=18)\n\n# Create the heatmap\nsns.heatmap(data = TotalMonthlyCustomers,\n            annot=True,\n            vmin = 0.0,\n            cmap='Pastel2',\n            vmax = list(TotalMonthlyCustomers.max().sort_values(ascending = False))[1]+3,\n            fmt = '.1f',\n            linewidth = 0.3,\n            yticklabels=mapMonthsList)\nplt.show();","314bfb55":"# Create a groupby object TotalCostGroup and pass the monthly cohort and cohort index as a list\nTotalCostGroup = cohortdata.groupby(['CohortMonth', 'CohortIndex']) \n\n# Calculate the sum of the TotalCost column\nPurchaseTotal = TotalCostGroup['TotalCost'].sum()\n\n# Reset the index of cohort_data\nPurchaseTotal = PurchaseTotal.reset_index()\n\n# Create a pivot \nTotalPurchase = PurchaseTotal.pivot(index='CohortMonth', columns='CohortIndex', values='TotalCost')\nTotalPurchase","b593855a":"cf.set_config_file(theme='white')\nmapMonths.iplot(kind = 'barh', x = 'MonthYear', y = 'PurchaseTotal', title = 'Purchases By Month',\n                     yTitle='Monthly Purchases in \u00a3', xTitle='Month Year of Purchase',text = 'PurchaseTotal')","89ae540a":"# Initialize plot figure\nplt.figure(figsize=(20, 15))\n\n# Add a title\nplt.title('Total Items Purchased Spread by Cohort Month',fontsize=18)\n\n# Create the heatmap\nsns.heatmap(data = TotalPurchase,\n            annot=True,\n            vmin = 0.0,\n            cmap='Pastel2',\n            vmax = list(TotalPurchase.max().sort_values(ascending = False))[1]+3,\n            fmt = '.1f',\n            linewidth = 0.3,\n            yticklabels=mapMonthsList)\nplt.show();","6dffd146":"# Create a groupby quantityGroup object and pass the monthly cohort and cohort index as a list\nquantityGroup = cohortdata.groupby(['CohortMonth', 'CohortIndex']) \n\n# Calculate the average of the Quantity column\nquantityMean = quantityGroup['Quantity'].mean()\n\n# Reset the index of cohort_data\nquantityMean = quantityMean.reset_index()\n\n# Create a pivot \nMeanQuantity = quantityMean.pivot(index='CohortMonth', columns='CohortIndex', values='Quantity')\n\n# Initialize plot figure\nplt.figure(figsize=(15, 12))\n\n# Add a title\nplt.title('Mean Quantity of Items Purchased Spread by Cohort Month',fontsize=18)\n\n# Create the heatmap\nsns.heatmap(data = MeanQuantity,\n            annot=True,\n            vmin = 0.0,\n            cmap='Pastel2',\n            vmax = list(MeanQuantity.max().sort_values(ascending = False))[1]+3,\n            fmt = '.1f',\n            linewidth = 0.3,\n            yticklabels=mapMonthsList)\nplt.show();","4fa6763d":"cohortGroup = cohortdata.groupby(['CohortMonth', 'CohortIndex'])","0e4c728b":"# Count the number of unique values per customer ID\ncohort_data = cohortGroup['CustomerID'].apply(pd.Series.nunique).reset_index()\n\n# Create a pivot \ncohortCounts = cohort_data.pivot(index='CohortMonth', columns='CohortIndex', values='CustomerID')\n\n# Select the first column and store it to cohort_sizes\ncohortSizes = cohortCounts.iloc[:,0]\n\n# Divide the cohort count by cohort sizes along the rows\nretention = cohortCounts.divide(cohortSizes, axis=0).mul(100)","06d421e3":"# Initialize inches plot figure\nplt.figure(figsize=(15,15))\n\n# Add a title\nplt.title('Customer Retention Rate Spread by Cohort Month in Percentage (%)',fontsize=18)\n\n# Create the heatmap\nsns.heatmap(data=retention,\n            annot = True,\n            cmap = \"Pastel2\",\n            vmin = 0.0,\n            vmax = list(retention.max().sort_values(ascending = False))[1]+3,\n            fmt = '.1f',\n            linewidth = 0.3,\n            yticklabels=mapMonthsList)\nplt.show();","6c06aed4":"RFMData = data.copy()\nRFMData.shape","71fa24ec":"RFMData.head()","982e8b1a":"# Set variable current_date to this max of Invoice date in dataframe RFMData\ncurrentDateTime = RFMData['InvoiceDate'].max() +timedelta(days=1)\ncurrentYear = currentDateTime.year\ncurrentMonth = currentDateTime.month\ncurrentDay = currentDateTime.day\ncurrentDate = dt.date(currentYear,currentMonth,currentDay)\ncurrentDate","18c6bc23":"# Lets create a date column RecentPurchaseDate for date part only of InvoiceDate\nRFMData['RecentPurchaseDate'] = RFMData.InvoiceDate.dt.date\nRFMData.head()","cf9685b8":"recency = RFMData.groupby('CustomerID')['RecentPurchaseDate'].max().reset_index()\nrecency.head()","ac00f6c3":"# Create column currentDate in dataframe recency\nrecency['CurrentDate'] = currentDate\nrecency.head()","ee015b48":"# Compute Recency as difference between current date and RecentPurchaseDate\nrecency['Recency'] = recency.RecentPurchaseDate.apply(lambda elapsed: (currentDate - elapsed).days)\nrecency.head()","ef5a81c2":"# Data Clean Up - Drop Columns RecentPurchaseDate and CurrentDate\nrecency.drop(['RecentPurchaseDate','CurrentDate'], axis=1, inplace=True)\nrecency.head()","b0a43fed":"recency.shape","42a9d73f":"frequency = RFMData.groupby('CustomerID').InvoiceNo.count().reset_index().rename(columns={'InvoiceNo':'Frequency'})\nfrequency.head()","61043e20":"frequency.shape","5d90019e":"monetary = RFMData.groupby('CustomerID').TotalCost.sum().reset_index().rename(columns={'TotalCost':'Monetary'})\nmonetary.head()","0abcb402":"monetary.shape","7119ab38":"intermediateRFMerge = recency.merge(frequency, on='CustomerID')\nRFMModel = intermediateRFMerge.merge(monetary, on='CustomerID')\nRFMModel.head()","f6efe236":"RFMModel.shape","f4bb8865":"data.CustomerID.unique().shape","6d415f77":"RFMModel.set_index('CustomerID',inplace=True)\nRFMModel.head()","64762314":"# Match RFMData Customer ID and RFMModel Index and display the head of the comparison result in RFMData\nRFMData[RFMData.CustomerID == RFMModel.index[0]].head()","445b1ec2":"# Check if the number difference of days from the purchase date in original record is same as shown in RFMModel\n(currentDate - RFMData[RFMData.CustomerID == RFMModel.index[0]].iloc[0].RecentPurchaseDate).days == RFMModel.iloc[0,0]","2b453347":"# RFM Quartiles\nRFMQuantiles = RFMModel.quantile(q=[0.25,0.5,0.75]).to_dict()\nRFMQuantiles","160e8a55":"RFMQuantilesDF = pd.DataFrame(RFMModel.quantile(q=[0,0.25,0.5,0.75,1]))\nRFMQuantilesDF","7d8dc61a":"plt.figure(figsize=(15, 10))\nsns.set(style=\"whitegrid\")\nax1 = sns.boxplot(x=RFMQuantilesDF[\"Recency\"],orient=\"h\", palette=\"Pastel2\")\n# Set title\nplt.title('Recency Quantiles Distribution Boxplot',fontsize=18)","f2365fb4":"plt.figure(figsize=(15, 10))\nsns.set(style=\"whitegrid\")\nax1 = sns.boxplot(x=RFMQuantilesDF[\"Frequency\"],orient=\"h\", palette=\"Pastel2\")\n# Set title\nplt.title('Frequency Quantiles Distribution Boxplot',fontsize=18)","085b7d18":"plt.figure(figsize=(15, 10))\nsns.set(style=\"whitegrid\")\nax1 = sns.boxplot(x=RFMQuantilesDF[\"Monetary\"],orient=\"h\", palette=\"Pastel2\")\n# Set title\nplt.title('Monetary Quantiles Distribution Boxplot',fontsize=18)","5969eb68":"RFMQuantilesDFSubPlot = RFMQuantilesDF.head(4).plot.bar(rot=0, subplots=True,figsize=(12, 8))\nRFMQuantilesDFSubPlot[1].legend(loc=2)  \n# Set title\nplt.title('Quantiles Distribution Bar Spread 0 - 25 - 50 - 75',fontsize=18)","fb740b88":"RFMSegment = RFMModel.copy()\nRFMSegment.shape","2649611f":"RFMSegment.head()","084fb13f":"# RScore Function Arguments Getting Passed (S = Score, P = Recency, Q = quantiles dict)\ndef RScore(S,P,Q):\n    if S > Q[P][0.75]:\n        return 1\n    elif S > Q[P][0.50]:\n        return 2\n    elif S > Q[P][0.25]: \n        return 3\n    else:\n        return 4\n    \nRFMSegment['R'] = RFMSegment['Recency'].apply(RScore, args=('Recency',RFMQuantiles,))\nRFMSegment.head()","6ba9582a":"# FMScore Function Arguments Getting Passed (S = Score, P = Frequency\/Monetary, Q = quantiles dict)\ndef FMScore(S,P,Q):\n    if S > Q[P][0.75]:\n        return 4\n    elif S > Q[P][0.50]:\n        return 3\n    elif S > Q[P][0.25]: \n        return 2\n    else:\n        return 1\n    \nRFMSegment['F'] = RFMSegment['Frequency'].apply(FMScore, args=('Frequency',RFMQuantiles,))\nRFMSegment.head()","5e4f72fa":"RFMSegment['M'] = RFMSegment['Monetary'].apply(FMScore, args=('Monetary',RFMQuantiles,))\nRFMSegment.head()","cd2326de":"# Compute RFM Score as a String\nRFMSegment['RFM_Segment'] = RFMSegment.R.astype(str) +  RFMSegment.F.astype(str) + RFMSegment.M.astype(str)\nRFMSegment['RFM_Score'] = RFMSegment[['R', 'F', 'M']].sum(axis = 1)\nRFMSegment.head()","ff7e646c":"print(RFMSegment['RFM_Segment'].unique())","047de282":"RFMSegment.info()","5d8fb8b3":"# Reset the index to create customerID column\nRFMSegment.reset_index(inplace=True)\nRFMSegment.shape","dbfa6433":"RFMSegment.head()","bcaa4a21":"#Create Customer Segment Function\ndef segment_customer(Dataframe):\n    if Dataframe['R'] >3:\n        if Dataframe['F'] >3:\n            if Dataframe['M'] >3:\n                return 'Best Customer'\n            elif Dataframe['M'] >2:\n                return 'Medium\/High Spending Active Loyal Customer'\n            else:\n                return 'Low-Spending Active Loyal Customer'\n        elif Dataframe['F'] >2:\n            if Dataframe['M'] >2:\n                return 'Medium\/High Spending Active Loyal Customer'\n            else:\n                return 'Low-Spending Active Loyal Customer'\n        else:\n            if Dataframe['M'] >2:\n                return 'Medium\/High Spending Active Customer'\n            else:\n                return 'Low-Spending New Customer'\n    elif Dataframe['R'] >2:\n        if Dataframe['F'] >3:\n            if Dataframe['M'] >3:\n                return 'Inactive Best Customer'\n            elif Dataframe['M'] >2:\n                return 'Medium\/High Spending Active Loyal Customer'\n            else:\n                return 'Low-Spending Inactive Loyal Customer'\n        elif Dataframe['F'] >2:\n            if Dataframe['M'] >2:\n                return 'Medium\/High Spending Active Loyal Customer'\n            else:\n                return 'Low-Spending Inactive Loyal Customer'\n        else:\n            if Dataframe['M'] >2:\n                return 'Medium\/High Spending Active Customer'\n            else:\n                return 'Low Spending Inactive Customer'\n    else:\n        if Dataframe['F'] >3:\n            if Dataframe['M'] >3:\n                return 'Churned Best Customer'\n            elif Dataframe['M'] >2:\n                return 'Churned Loyal Customer'\n            else:\n                return 'Churned Frequent Customer'\n        elif Dataframe['F'] >2:\n            if Dataframe['M'] >2:\n                return 'Churned Loyal Customer'\n            else:\n                return 'Churned Frequent Customer'\n        else:\n            if Dataframe['M'] >2:\n                return 'Churned Medium-High Spender'\n            else:\n                return 'Churned Low Spending Infrequent Customer'   \nRFMSegment['CustomerSegment'] = RFMSegment.apply(segment_customer, axis=1)\nRFMSegment['CustomerSegment'].value_counts()","61d1b968":"# Check head of RFMSegment Dataframe for Data Integrity\nRFMSegment.head()","8c1f3214":"RFMSegment.CustomerSegment.value_counts(normalize=True).mul(100).round(1).astype(str) + '%'","ea2f1fa2":"RFMSegment.sample(10)","7f5402dc":"#Create Market Segment Function\ndef segment_market(Dataframe):\n    if Dataframe['RFM_Score'] > 11:\n        return 'Platinum'\n    elif Dataframe['RFM_Score'] > 8:\n        return 'Gold'\n    elif (Dataframe['RFM_Score'] > 5):\n        return 'Silver'\n    else:\n        return 'Bronze'\nRFMSegment['MarketSegment'] = RFMSegment.apply(segment_market, axis=1)\nRFMSegment.groupby('MarketSegment').agg({\n    'Recency': 'mean',\n    'Frequency': 'mean',\n    'Monetary': ['mean', 'count']}).round(1)","9de03ac8":"groupByCustomerSegment = RFMSegment.groupby([\"CustomerSegment\"], as_index=False)['CustomerID'].count().rename(columns={'CustomerSegment' : 'CustomerSegment','CustomerID':'NoofCustomers'})\ngroupByCustomerSegment.NoofCustomers = groupByCustomerSegment.NoofCustomers.astype(str)\ngroupByCustomerSegment = groupByCustomerSegment.sort_values(by='NoofCustomers', ascending=False)\ngroupByCustomerSegment.head()\ncf.set_config_file(theme='white')\ngroupByCustomerSegment.iplot(kind = 'barh', x = 'CustomerSegment', y = 'NoofCustomers', title = 'Customer Segment Spread Categories',\n                     yTitle='Customer Segment', xTitle='# of Customers',text = 'NoofCustomers',textposition = 'auto')","fe2aad89":"# Validate Distribution Skewness using distplot - subplots\nfig, axes = plt.subplots(3, 1, figsize=(15, 15))\nsns.distplot(RFMModel.Recency , color=\"chocolate\", ax=axes[0], axlabel='Recency')\nsns.distplot(RFMModel.Frequency, color=\"blue\", ax=axes[1], axlabel='Frequency')\nsns.distplot(RFMModel.Monetary, color=\"green\", ax=axes[2], axlabel='Monetary')\n# Set title\nplt.title('Subplot Distribution to validate Skewness',fontsize=18)\nplt.show();","90ce39f0":"# Let's describe the table to see if there are any negative values\nRFMModel.describe()","f9da20bc":"# Transform the data before K-Means clustering\n\n\n# Taking log first because normalization forces data for negative values\nRFMModelLog = np.log(RFMModel)\n\n# Normalize the data for uniform averages and means in the distribution.\nscaler = StandardScaler()\nRFMModelScaled = scaler.fit_transform(RFMModelLog)\nRFMModelScaled = pd.DataFrame(data=RFMModelScaled, index=RFMModel.index, columns=RFMModel.columns)\nRFMModelScaled.head()","31674fe1":"# Revalidate Distribution Subplot to check data skewness after log transformation\nfig, axes = plt.subplots(3, 1, figsize=(15, 15))\nsns.distplot(RFMModelScaled.Recency , color=\"chocolate\", ax=axes[0], axlabel='Recency')\nsns.distplot(RFMModelScaled.Frequency , color=\"blue\", ax=axes[1], axlabel='Frequency')\nsns.distplot(RFMModelScaled.Monetary , color=\"green\", ax=axes[2], axlabel='Monetary')\n# Set title\nplt.title('Subplot Distribution after Log Transformation',fontsize=18)\nplt.show();","04e2f477":"# Compute Within Cluster Sum of Squares(WCSS) Error through Elbow Method\nWCSSKmeansOut = pd.DataFrame(columns=['Clusters','WCSSErrorScore'])\n# Choose range of 1 to 14 as we have 13 Customer Segments\nfor loopCounter in range(1,14):\n    kmeans = KMeans(n_clusters=loopCounter,random_state=1, init='k-means++')\n    kmeans.fit(RFMModelScaled)\n    WCSSKmeansOut = WCSSKmeansOut.append({'Clusters': loopCounter,'WCSSErrorScore' : kmeans.inertia_},ignore_index=True)\nWCSSKmeansOut.set_index('Clusters',inplace=True)\nWCSSKmeansOut","8a79c868":"# Plot WCSS Elbow Graph\nplt.figure(figsize=(20,15));\nplt.title('WCSS Error Score Across Clusters 1 to 13 - Kmeans',fontsize=18)\nWCSSKmeansOut.WCSSErrorScore.plot(marker='o')","227ea7ed":"#Optimum Cluster # Validation using Silhouette Score\n#Using 3 as lower range number as typically best score for Silhouette is obtained for 2 clusters \n#With 13 segments selected - 2 clusters is not ideal\nWCSSKmeansSilhouetteOut = pd.DataFrame(columns=['Clusters','silhouetteScore'])\nfor loopCounter2 in range(3,14):\n    SilhouetteKMeansModel = KMeans(n_clusters=loopCounter2, random_state=1,init='k-means++').fit(RFMModelScaled)\n    preds = SilhouetteKMeansModel.predict(RFMModelScaled)\n    silhouetteScore = silhouette_score(RFMModelScaled,preds)\n    WCSSKmeansSilhouetteOut = WCSSKmeansSilhouetteOut.append({'Clusters': loopCounter2,'silhouetteScore' : silhouetteScore},ignore_index=True)\n# plot Silhouette graph\nax4 = sns.scatterplot(x=\"Clusters\", y=\"silhouetteScore\", data=WCSSKmeansSilhouetteOut)\n# Set title\nplt.title('Silhouette Score Across Clusters',fontsize=18)","e913f23a":"WCSSKmeansSilhouetteOut","3aaa5d4f":"ComputedClusters = 4\n#Build KMeans Model with 4 Clusters\nKMeansModel = KMeans(n_clusters=ComputedClusters, random_state=1, init='k-means++')\nKMeansModel.fit(RFMModelScaled)\nclusterLabels = KMeansModel.labels_\nclusterLabels","2ed50916":"KMeansModel","31efd768":"clusterLabels.shape","f57d6ca2":"RFMSegment.shape","6ff90359":"# Assign the clusters as column to each customer\nClusterData = RFMSegment.assign(Cluster = clusterLabels)\nClusterData.head()","56aa8a80":"ClusterData.shape","7fa9474f":"# Compute counts of CustomerIDs assigned to different clusters\nClusterDataCount = ClusterData.Cluster.value_counts().sort_index().rename_axis('Cluster').reset_index(name='CustomerIDCount')\nClusterDataCount.Cluster = ClusterDataCount.astype(str)\nClusterDataCount.Cluster = \"Cluster \" + ClusterDataCount.Cluster\nClusterDataCount.head()","7e552dbf":"cf.set_config_file(theme='white')\nClusterDataCount.iplot(kind = 'barh', x = 'Cluster', y = 'CustomerIDCount', title = 'CustomerID Spread by Clusters',\n                     xTitle='Cluster #', yTitle='# Of Customers')","c8212a21":"#Sample the Cluster Table data to check data correctness\nClusterData.sample(10)","ba261ba6":"ClusterData[ClusterData.Cluster == 3].sample(10)","b1096142":"ClusterData[ClusterData.Cluster == 2].sample(10)","bd878fbd":"ClusterData[ClusterData.Cluster == 1].sample(10)","24f2218c":"ClusterData[ClusterData.Cluster == 0].sample(10)","9e5bd701":"# Plot 2D plots of RF, FM and RM\nTwoDPlot = RFMModelScaled.iloc[:,0:3].values\nTwoDCount=TwoDPlot.shape[1]\nfor loopCounter01 in range(0,TwoDCount):\n    for loopCounter02 in range(loopCounter01+1,TwoDCount):\n        plt.figure(figsize=(20,10));\n        plt.suptitle('Scatter Plot Visualization',fontsize=18)\n        plt.scatter(TwoDPlot[clusterLabels == 0, loopCounter01], TwoDPlot[clusterLabels == 0, loopCounter02], s = 10, c = 'grey', label = 'Cluster0')\n        plt.scatter(TwoDPlot[clusterLabels == 1, loopCounter01], TwoDPlot[clusterLabels == 1, loopCounter02], s = 10, c = 'chocolate', label = 'Cluster1')\n        plt.scatter(TwoDPlot[clusterLabels == 2, loopCounter01], TwoDPlot[clusterLabels == 2, loopCounter02], s = 10, c = 'cyan', label = 'Cluster2')\n        plt.scatter(TwoDPlot[clusterLabels == 3, loopCounter01], TwoDPlot[clusterLabels == 3, loopCounter02], s = 10, c = 'lightgreen', label = 'Cluster3')\n        \n        plt.scatter(KMeansModel.cluster_centers_[:,loopCounter01], KMeansModel.cluster_centers_[:,loopCounter02], s = 50, c = 'black', label = 'Centroids')\n        plt.xlabel(RFMModelScaled.columns[loopCounter01])\n        plt.ylabel(RFMModelScaled.columns[loopCounter02])\n        plt.legend()       \n        plt.show();","13d8cc13":"# Assign Cluster values to each customer in normalized dataframe\nRFMModelScaled = RFMModelScaled.assign(Cluster = clusterLabels)\n\n# Melt normalized dataframe into long form to have all metric in same column\nRFMModelScaledMelt = pd.melt(RFMModelScaled.reset_index(),\n                      id_vars=['CustomerID','Cluster'],\n                      value_vars=['Recency', 'Frequency', 'Monetary'],\n                      var_name='Metric',\n                      value_name='Value')\nRFMModelScaledMelt.head()","187a58c7":"RFMModelScaledMelt.shape","d5e22a62":"# RFM Snake Plot Visualization\nplt.figure(figsize=(15,10))\npalette = sns.color_palette(\"Pastel2\", 6)\nsns.lineplot(x = 'Metric',\n             y = 'Value',\n             hue = 'Cluster',\n             data = RFMModelScaledMelt,\n             palette = \"Pastel2\")\nplt.title(\"Snake Plot of RFM\",fontsize=18)\nplt.legend()\nplt.show();","dcc5f0ae":"# Assign Cluster labels to RFMModelCluster table\nRFMModelCluster = RFMModel.assign(Cluster = clusterLabels)\n\n# Average attributes for each cluster\nclusterMean = RFMModelCluster.groupby(['Cluster']).mean() \n\n# Calculate the population average\npopulationMean = RFMModel.mean()\n\n# Calculate relative importance of attributes by \nAttributeInterDependence = (clusterMean \/ populationMean) - 1\nAttributeInterDependence","85667323":"plt.figure(figsize=(12, 5))\nplt.title('Inter Dependence of RFM Attributes Across Clusters')\nsns.heatmap(data=AttributeInterDependence, annot=True, fmt='.2f', cmap='Pastel2')\nplt.show();","28b6627a":"ClusterData[ClusterData.Cluster == 0].CustomerSegment.value_counts()","a5812940":"ClusterData[ClusterData.Cluster == 1].CustomerSegment.value_counts()","694a5183":"ClusterData[ClusterData.Cluster == 2].CustomerSegment.value_counts()","e029c603":"ClusterData[ClusterData.Cluster == 3].CustomerSegment.value_counts()","54d2ac52":"# Export datasets\n#with pd.ExcelWriter(outputPath+'Online_Retail_EDA.xlsx',engine='xlsxwriter', mode='w') as writer:\nwith pd.ExcelWriter('Online_Retail_EDA.xlsx',engine='xlsxwriter', mode='w') as writer:\n    dataTableau.to_excel(writer,sheet_name='Online_Retail',index=False)\n    ClusterData.to_excel(writer,sheet_name='Cluster_Data',index=False)\n    WCSSKmeansOut.to_excel(writer,sheet_name='WCSS_Data',index=False)\n    WCSSKmeansSilhouetteOut.to_excel(writer,sheet_name='Silhouette_Data',index=False)\n    refundedInvoices.to_excel(writer,sheet_name='Refunded_Invoices',index=False)","4a6e0d91":"<h5>Inference(s):<\/h5>\n<ul>\n    <li>Customer 16446 from UK has shopped for 168.5K \u00a3 on a single transaction 581483 for Birdie Paper Craft\n    <\/li>\n<\/ul>","999fa1f1":"<h5>Inference(s):\n<\/h5>\n<ul>\n    <li>Frequency Quantile Distribution Boxplot shows outliers beyond 75th percentile\n    <\/li>\n<\/ul>","07369878":"<h5>Inference(s):\n<\/h5>\n<ul>\n    <li>ClusterData Table Bhas 4296 rows equal to unique customer ID's and has 12 columns\n    <\/li>\n    <li>Data Quality Check Validated Fine while verifying the first few rows  for 12 columns in previous step\n    <\/li>\n  <\/ul>\n<h4  style=\"color:Navy;text-align:left;\">Identify Split of customer ID's by Cluster<\/h4>","b4d83270":"<h5> Inference(s) Drawn from Customer Retention Rate Spread by Cohort Month in Percentage (%) Heatmap<\/h5>\n<table style=\"width:100%;text-align:left;\">\n    <tr style=\"color:Crimson;\">\n        <th style=\"text-align:left;\">#<\/th>\n        <th style=\"text-align:left;\">Description<\/th>\n        <th style=\"text-align:left;\">Affected Cohorts<\/th>\n        <th style=\"text-align:left;\">Remarks<\/th>\n    <\/tr>\n    <tr>\n        <td style=\"text-align:left;\"><div style=\"word-wrap: break-word;\">1<\/div><\/td>\n        <td style=\"text-align:left;\"><div style=\"word-wrap: break-word;\">Maximum Retention Rate Cohort<\/div><\/td>\n        <td style=\"text-align:left;\"><div style=\"word-wrap: break-word;\">December 2010<\/div><\/td>\n        <td style=\"text-align:left;\"><div style=\"word-wrap: break-word;\">Average Retention of ~36%<\/div><\/td>\n    <\/tr>\n    <tr>\n        <td style=\"text-align:left;\"><div style=\"word-wrap: break-word;\">2<\/div><\/td>\n        <td style=\"text-align:left;\"><div style=\"word-wrap: break-word;\">Minimum Retention Rate Cohort<\/div><\/td>\n        <td style=\"text-align:left;\"><div style=\"word-wrap: break-word;\">May 2011<\/div><\/td>\n        <td style=\"text-align:left;\"><div style=\"word-wrap: break-word;\"><\/div>Average Retention of ~20%<\/td>\n    <\/tr>\n    <tr>\n        <td style=\"text-align:left;\"><div style=\"word-wrap: break-word;\">3<\/div><\/td>\n        <td style=\"text-align:left;\"><div style=\"word-wrap: break-word;\">Average Retention Rate Across Cohorts<\/div><\/td>\n        <td style=\"text-align:left;\"><div style=\"word-wrap: break-word;\">All Cohorts<\/div><\/td>\n        <td style=\"text-align:left;\"><div style=\"word-wrap: break-word;\">~25% except for seasonal changes in October and November 2011<\/div><\/td>\n    <\/tr>\n<\/table>","72eed189":"<h5>Inference(s):<\/h5>\n<ul>\n    <li>20 Unique refund invoices found\n    <\/li>\n<\/ul>\n<h4  style=\"color:Navy;\">Create Dataframe refundedInvoices to compute count of Distinct Refunded Invoices for Tableau Visualization KPI <\/h4>   ","cf046f70":"<h3 style=\"color:MediumBlue;text-align:left;\">Validate RFM Table Integrity from Customer ID, R, F am M perspectives\n<\/h3>\n<h4 style=\"color:Navy;text-align:left;\"> Match RFMData Customer ID and RFMModel Index and display the head of the comparison result in RFMData\n<\/h4>","b3a18c21":"<h5>Inferences:\n<\/h5>\n<ul>\n    <li>ClusterDataCount Split Computed\n    <\/li>\n    <li>Four clusters created with sizes equal to\n        <ul>\n            <li>Cluster 0: \n                <b>838<\/b>\n            <\/li>\n            <li>Cluster 1: \n                <b>1206<\/b>\n            <\/li>\n            <li>Cluster 2: \n                <b>963<\/b>\n            <\/li>\n            <li>Cluster 3: \n                <b>1289<\/b>\n            <\/li>\n        <\/ul>\n    <\/li>\n<\/ul>","17d4b4ca":"<h5>Inference(s):<\/h5>\n<ul>\n    <li>\n        <b>groupByTotalCost2 <\/b>\n        dataframe summarized at InvoiceNo level aggregating sum of TotalCost for Bottom 20 Invoices\n    <\/li>\n    <li> Data quality check successful on first 5 rows validation\n    <\/li>\n<\/ul>\n<h4 style=\"color:Navy;\">Plot Bar Plot Bottom 20 Purchases by Invoice Number<\/h4>    ","1f12fa16":"<h5>Inference(s):\n<\/h5>\n<ul>\n    <li>RFMSegment data quality checked validating first 5 rows of data\n    <\/li>\n<\/ul>\n<h4  style=\"color:Navy;\">Check the distribution of CustomerSegment<\/h4>","9f49dfd6":"<h5>Inference(s):<\/h5>\n<ul>\n    <li>Variable CurrentDate computed as 1st December 2011 - Hypothetical Snapshot Date<\/li>\n<\/ul>\n<h4  style=\"color:Navy;\">Compute Recent Purchase Date column to have Date part alone of Invoice date<\/h4>    ","61bd1ed2":"<h5>Inference(s):\n<\/h5>\n<ul>\n    <li>EDA on Countries showed missing values of country code for 4 countries and they were removed\n    <\/li>\n<\/ul>\n<h4  style=\"color:Navy;\">Create Dataframe dataTableau merging EDA completed dataframe data and EDA completed dataframe countries on Country using left join\n<\/h4>","eaec301e":"<h5>Inference(s):<\/h5>\n<ul>\n    <li>data Dataframe displays fine and has 8 columns\n    <\/li>\n    <li>Description column is in Capitals and can be converted to Title\/Proper Case after numeric EDA\n    <\/li>\n    <li> Data quality check successful on first 5 rows validation\n    <\/li>\n<\/ul>\n<h4  style=\"color:Navy;\"> Identify Missing Values and Type of Data using data.info()<\/h4>","32dad48b":"<h5>Inference(s):\n<\/h5>\n<ul>\n    <li>CurrentDate column added to Recency Dataframe\n    <\/li>\n    <li> Data quality check successful on first 5 rows validation\n    <\/li>\n<\/ul>\n<h4 style=\"color:Navy;text-align:left;\">Compute Recency Column as difference between current date and RecentPurchaseDate\n<\/h4>","58aa2824":"<h5>Inference(s):\n<\/h5>\n<ul>\n    <li>InvoiceMonth Column computed from InvoiceDate having only the date part\n    <\/li>\n    <li>CohortMonth Column computed based on month of first purchase\n    <\/li>\n<\/ul>","b94b326a":"<h5>Inference(s):\n<\/h5>\n<ul>\n    <li>Above 4% of overall data is part of December 2011 where we have partial month data\n    <\/li>\n<\/ul>","d10f2082":"<h3 style=\"color:MediumBlue;text-align:left;\">Data Modeling Completed For:\n<\/h3>\n<ul>\n    <li>1. Building a RFM (Recency Frequency Monetary) model. <\/li>\n    <li>2. Calculating RFM metrics.<\/li>\n    <li>3. Building RFM Segments. Giving recency, frequency, and monetary scores individually by dividing them into quartiles\n        <ul>    \n            <li>a. Combinining three ratings to get a RFM segment (as strings).\n            <\/li>\n            <li>b. Getting the RFM score by adding up the three ratings.\n            <\/li>\n            <li>c. Analyzing the RFM segments by summarizing them and comment on the findings.\n            <\/li>\n        <\/ul>\n    <\/li>\n<\/ul>","a96685b1":"<h4  style=\"color:Navy;\"> Records with Quantity < 0 and records with Quantity > 20000 which are cancelled are removed <\/h4>","14535d22":"<h4 style=\"color:Navy;text-align:left;\">Create Quartiles Dictionary for RFM<\/h4>","93fee2c1":"<h5>Inference(s):<\/h5>\n<ul>\n        <li>data Dataframe has 541909 rows of information and 8 columns<\/li>\n<\/ul>\n<h4  style=\"color:Navy;\"> Validate First Few Records of data Dataframe<\/h4>","2775ff45":"<h4 style=\"color:Navy;text-align:left;\">Plot Quantiles Distribution Bar Spread 0 - 25 - 50 - 75\n<\/h4>","c778d246":"<h5>Inference(s):\n<\/h5>\n<ul>\n    <li>Recency Dataframe Computed with CustomerID and RecentPurchaseDate\n    <\/li>\n    <li>Data quality check successful on first 5 rows validation\n    <\/li>\n<\/ul>\n<h4 style=\"color:Navy;text-align:left;\">Add column CurrentDate column to Recency Dataframe\n<\/h4>","97ea5755":"<h5>Inference(s):<\/h5>\n<ul>\n    <li>More than 2% quantity is negative indicating returns\/refunds\n    <\/li>\n<\/ul>","8c88e6af":"<h5>Inference(s):<\/h5>\n<ul>\n    <li>401604 rows out of 536641 have CustomerID Populated<\/li>\n<\/ul>\n<h4 style=\"color:Navy;\">Validate percentage of missing customer IDs<\/h4>","8699c51c":"<h5>Inference(s):\n<\/h5>\n<ul>\n    <li>RFMSegment data quality checked validating first 5 rows of data\n    <\/li>\n    <li>Index became numeric and CustomerID became a column\n    <\/li>\n<\/ul>","30d0aba6":"<h5> Inference(s)\n<\/h5>\n<ul>\n    <li>KMeans model is built on 4 clusters and Cluster Labels are generated\n    <\/li>\n<\/ul>","e9de1d3a":"<h5>Inference(s):<\/h5>\n<ul>\n    <li>TotalMonthlyCustomers Monthly Cohort Count by customer computed\n    <\/li>\n    <li>It has nan values below the diagonal which is as expected\n    <\/li>\n<\/ul>","90a40e94":"<h4  style=\"color:Navy;text-align:left;\"> Sum Total of Items Purchased Spread by CohortMonth\n<\/h4>","bb94c11c":"<h5>Inference(s):\n<\/h5>\n<ul>\n    <li>RFMSegment has 8 columns of size 4296 = number of Unique Customer IDs\n    <\/li>\n     <li>Recency, Frequency are Integer values while Monetary is a float value\n    <\/li>\n    <li>R Quantile, F Quantile and M Quantile are Integer values\n    <\/li>\n    <li>RFM_Segment is a string object\n    <\/li>\n    <li>RFM_Score is a integer object\n    <\/li>\n<\/ul>\n <h4 style=\"color:Navy;text-align:left;\">Reset the index of RFMSegment Dataframe\n<\/h4> ","9ae5f841":"<h5>Inference(s):<\/h5>\n<ul>\n    <li>2011 had more than 375K transactions\n    <\/li>\n<\/ul>","45f3e869":"<h5>Inference(s):<\/h5>\n<ul>\n    <li>\n        <b>TotalCost <\/b>\n        column added as last column of dataframe data and first 5 rows displays right computation as multiplication product of UnitPrice and Quantity\n    <\/li>\n    <li> Data quality check successful on first 5 rows validation\n    <\/li>\n<\/ul>    ","b815cc44":"<h5>Inference(s):<\/h5>\n<ul>\n    <li>data Dataframe displays fine and has 8 columns\n    <\/li>\n    <li>Description column converted to Title\/Proper Case\n    <\/li>\n    <li> Data quality check successful on first 5 rows validation\n    <\/li>\n<\/ul>\n<h4  style=\"color:Navy;\"> Change the DType of CustomerID to Integer<\/h4>","da78c307":"<h4  style=\"color:Navy;text-align:left;\"> Compute Ordered Month List for usage on HeatMaps on CohortMonth\n<\/h4>","0f8e9a8a":"<h5>Inference(s):\n<\/h5>\n<ul>\n    <li>RFMModel Dataframe computed and it has 4 columns CustomerID,Recency, Frequency and Monetary\n    <\/li>\n    <li>Data quality check successful on first 5 rows validation\n    <\/li>\n    <\/ul>\n<h4  style=\"color:Navy;\">Data Shape Quality Check on RFMModel Dataframe\n<\/h4>  ","00e637a2":"<h5>Inference(s):\n<\/h5>\n<ul>\n    <li>Quantiles Dictionary List created for Recency, Frequency, Monetary for 25th, 50th and 75th percentile\n    <\/li>\n<\/ul>\n<h4 style=\"color:Navy;text-align:left;\">Create Quantiles Dataframe for RFM\n<\/h4>    ","34654c03":"<h5>Inference(s):\n<\/h5>\n<ul>\n    <li style=\"color:Purple;\">December 2011 Partial Month data removed<\/li>\n<\/ul>\n<h4  style=\"color:Navy;\">Post EDA Validation of  Top 20 Customers by Spend<\/h4>        ","07b15515":"<h4 style=\"color:Navy;text-align:left;\">Build RFM Segment and RFM Score\n<\/h4>\n<ul>\n    <li>Concatenate RFM quartile values to RFM_Segment\n    <\/li>\n    <li>Sum RFM quartiles values to RFM_Score\n    <\/li>\n<\/ul>    ","6c259d45":"<h5>Inference(s):<\/h5>\n<ul>\n    <li>RFM Quantile Dataframe computed\n    <\/li>\n    <li>Data quality check successful on all rows validation\n    <\/li>\n    <li>Significant Outliers found in Recency, Frequency and Monetary beyond 75th percentile\n    <\/li>\n<\/ul>  ","c2a067a7":"<h3 style=\"color:MediumBlue;text-align:left;\">Data Modeling :\n<\/h3>\n<ul>\n    <li>1. Create clusters using k-means clustering algorithm.\n        <ul>    \n            <li>a. Prepare the data for the algorithm. If the data is asymmetrically distributed, manage the skewness with appropriate transformation. Standardize the data.\n            <\/li>\n            <li>b. Decide the optimum number of clusters to be formed.\n            <\/li>\n            <li>c. Analyze these clusters and comment on the results.\n            <\/li>\n        <\/ul>\n    <\/li>\n<\/ul>","c7de9e95":"<h4 style=\"color:Navy;text-align:left;\">Validate Recency for first Customer ID\n<\/h4>","e3a8e8ee":"<h4  style=\"color:Navy;text-align:left;\"> Sum Total of Gift Items Purchased Spread by CohortMonth<\/h4>","64d5c1f4":"<h4  style=\"color:Navy;\">Remove Partial Month Data of December 2011<\/h4>","cac548d4":"<h5> Inference(s):\n<\/h5>\n<ul>\n    <li>Data Visualization shows Highest Silhouette score value is seen for cluster 4\n    <\/li>\n    <li>Lets confirm it once by looking at underlying scores\n    <\/li>\n<\/ul>\n<h4  style=\"color:Navy;text-align:left;\">Check Silhouette score Dataframe to Validate Cluster with maximum score\n<\/h4>","3f6efb3b":"<h4  style=\"color:Navy;\"> Purchase Transactions Split By Country<\/h4>","657f4c6a":"<h4  style=\"color:Navy;text-align:left;\">Compute Within Cluster Sum of Squares(WCSS) Error through Elbow Method<\/h4>","79c75af5":"<h3 style=\"color:MediumBlue;text-align:left;\">Recency<\/h3>","3d905170":"<h5> Inference(s) Drawn from Total Items Purchased Spread by Cohort Month Heatmap<\/h5>\n<table style=\"width:100%;text-align:left;\">\n    <tr style=\"color:Crimson;\">\n        <th style=\"text-align:left;\">#<\/th>\n        <th style=\"text-align:left;\">Description<\/th>\n        <th style=\"text-align:left;\">Affected Cohorts<\/th>\n        <th style=\"text-align:left;\">Remarks<\/th>\n    <\/tr>\n    <tr>\n        <td style=\"text-align:left;\"><div style=\"word-wrap: break-word;\">1<\/div><\/td>\n        <td style=\"text-align:left;\"><div style=\"word-wrap: break-word;\">Maximum Items Procured Cohort<\/div><\/td>\n        <td style=\"text-align:left;\"><div style=\"word-wrap: break-word;\">December 2010<\/div><\/td>\n        <td style=\"text-align:left;\"><div style=\"word-wrap: break-word;\">Four months over 450K plus purchases<\/div><\/td>\n    <\/tr>\n    <tr>\n        <td style=\"text-align:left;\"><div style=\"word-wrap: break-word;\">2<\/div><\/td>\n        <td style=\"text-align:left;\"><div style=\"word-wrap: break-word;\">Minimum Items Procured Cohort<\/div><\/td>\n        <td style=\"text-align:left;\"><div style=\"word-wrap: break-word;\">Feb 2011<\/div><\/td>\n        <td style=\"text-align:left;\"><div style=\"word-wrap: break-word;\">Eight months under 64K purchases<\/div><\/td>\n    <\/tr>\n    <tr>\n        <td style=\"text-align:left;\"><div style=\"word-wrap: break-word;\">3<\/div><\/td>\n        <td style=\"text-align:left;\"><div style=\"word-wrap: break-word;\">Promising Cohort by Items<\/div><\/td>\n        <td style=\"text-align:left;\"><div style=\"word-wrap: break-word;\">January 2011<\/div><\/td>\n        <td style=\"text-align:left;\"><div style=\"word-wrap: break-word;\">Greater than 50K purchases every month<\/div><\/td>\n    <\/tr>\n<\/table>","258dffe4":"<h5> Inference(s) Drawn from Monthly Cohort Customer Counts Heatmap<\/h5>\n<table style=\"width:100%;text-align:left;\">\n    <tr style=\"color:Crimson;\">\n        <th style=\"text-align:left;\">#<\/th>\n        <th style=\"text-align:left;\">Description<\/th>\n        <th style=\"text-align:left;\">Affected Cohorts<\/th>\n        <th style=\"text-align:left;\">Remarks<\/th>\n    <\/tr>\n    <tr>\n        <td style=\"text-align:left;\"><div style=\"word-wrap: break-word;\">1<\/div><\/td>\n        <td style=\"text-align:left;\"><div style=\"word-wrap: break-word;\">First Cohort<\/div><\/td>\n        <td style=\"text-align:left;\"><div style=\"word-wrap: break-word;\">December 2010<\/div><\/td>\n        <td style=\"text-align:left;\"><div style=\"word-wrap: break-word;\">885 out of overall 4296 which is over 20% of all customers across cohorts<\/div><\/td>\n    <\/tr>\n    <tr>\n        <td style=\"text-align:left;\"><div style=\"word-wrap: break-word;\">2<\/div><\/td>\n        <td style=\"text-align:left;\"><div style=\"word-wrap: break-word;\">Last Cohort<\/div><\/td>\n        <td style=\"text-align:left;\"><div style=\"word-wrap: break-word;\">November 2011<\/div><\/td>\n        <td style=\"text-align:left;\"><div style=\"word-wrap: break-word;\">323 New Customers<\/div><\/td>\n    <\/tr>\n    <tr>\n        <td style=\"text-align:left;\"><div style=\"word-wrap: break-word;\">3<\/div><\/td>\n        <td style=\"text-align:left;\"><div style=\"word-wrap: break-word;\">High Intake Of New Customers<\/div><\/td>\n        <td style=\"text-align:left;\"><div style=\"word-wrap: break-word;\">January 2011 and March 2011<\/div><\/td>\n        <td style=\"text-align:left;\"><div style=\"word-wrap: break-word;\">Greater than 400 new customers each in the cohort, December 2010 excluded while computing this as existing customers vs new customer data unknown<\/div><\/td>\n    <\/tr>\n    <tr>\n        <td style=\"text-align:left;\"><div style=\"word-wrap: break-word;\">4<\/div><\/td>\n        <td style=\"text-align:left;\"><div style=\"word-wrap: break-word;\">Low Intake Of New Customers<\/div><\/td>\n        <td style=\"text-align:left;\"><div style=\"word-wrap: break-word;\">July 2011 and August 2011<\/div><\/td>\n        <td style=\"text-align:left;\"><div style=\"word-wrap: break-word;\">Less than 200 new customers in each cohort<\/div><\/td>\n    <\/tr>\n    <tr>\n        <td style=\"text-align:left;\"><div style=\"word-wrap: break-word;\">5<\/div><\/td>\n        <td style=\"text-align:left;\"><div style=\"word-wrap: break-word;\">Shopping increased in October 2011 - November 2011<\/div><\/td>\n        <td style=\"text-align:left;\"><div style=\"word-wrap: break-word;\">All Cohorts<\/div><\/td>\n        <td style=\"text-align:left;\"><div style=\"word-wrap: break-word;\">Probably pre holiday season purchases<\/div><\/td>\n    <\/tr>\n<\/table>","5bb0c1bd":"<h5>Inference(s):<\/h5>\n<ul>\n    <li>\n        <b>groupByCountry<\/b>\n        dataframe summarized at Country level aggregating count on InvoiceNo<\/li>\n<\/ul>\n<h4 style=\"color:Navy;\">Plot Bar Plot Transaction Count of Purchases by Country<\/h4>    ","53636777":"<h5>Inference(s):\n<\/h5>\n<ul>\n    <li>CustomerSegment sample showed a variety of RFM scores and CustomerSegments showing up - data quality check verified and passed\n    <\/li>\n<\/ul>\n<h4  style=\"color:Navy;\">Grouping by RFM Score for Market Campaigns (To be used in visualization):\n<\/h4>","c7fe9b90":"<h5>Inference(s):<\/h5>\n<ul>\n    <li>\n        <b>United Kingdom (UK) <\/b> \n        tops on transactions with\n        <b>over 356K <\/b>\n        transactions<\/li>\n<\/ul>    \n<h4 style=\"color:Navy;\">Summarize Top 3 Countries Transactions Count as Percentage<\/h4>    ","f4ad86e5":"<h5>Inference(s):<\/h5>\n<ul>\n    <li>Merged Dataframe dataTableau created and the first 5 rows validated fine\n    <\/li>\n<\/ul>\n<h4  style=\"color:Navy;\">Add Region column based on Country value to classify as UK and Others for Visualization\n<\/h4>","f432adc6":"<h5>Inference(s):\n<\/h5>\n<ul>\n    <li>Recency Quantile Distribution Boxplot shows outliers beyond 75th percentile\n    <\/li>\n<\/ul>  ","66f1248c":"<h5>Inference(s):<\/h5>\n<ul>\n    <li>Missing values are found on CustomerID and Description columns<\/li>\n    <li>4 Object Columns : InvoiceNo, StockCode, Description, Country <\/li>\n    <li>1 Integer Columns : Quantity <\/li>                \n    <li>2 Float Columns : UnitPrice, CustomerID <\/li>\n    <li>1 Datetime Columns : InvoiceDate <\/li>\n<\/ul>\n<h4  style=\"color:Navy;\">Identify statistics of numeric data using describe()<\/h4>","bc5760a0":"<h5>Inference(s):<\/h5>\n<ul>\n    <li>Recency column computed and added to Recency Dataframe\n    <\/li>\n    <li> Data quality check successful on first 5 rows validation\n    <\/li>\n<\/ul>","ca5e6b62":"<h5>Inference(s):\n<\/h5>\n<ul>\n    <li>Boxplot on quantity shows positive distribution with 1 valid high value\n    <\/li>\n<\/ul>\n<h4  style=\"color:Navy;\">Perform Boxplot Test for Outliers on UnitPrice<\/h4>","d3dcb539":"<h4 style=\"color:Navy;\">Copy cleaned EDA dataframe data into cohortdata dataframe for cohort analysis\n<\/h4>","2b846a61":"<h3 style=\"color:MediumBlue;text-align:left;\">Build RFM Table\n<\/h3>\n<h4 style=\"color:Navy;text-align:left;\">Combine Recency, Frequency and Monetary into aggregated RFMModel dataframe\n<\/h4>","e03d9989":"<h4  style=\"color:Navy;\">Identify Duplicates<\/h4>","29aa0b1a":"<h4 style=\"color:Navy;text-align:left;\">Monetary Quantiles Distribution<\/h4>","d236bc51":"<h5>Inference(s):<\/h5>\n<ul>\n    <li>Feb 2011 ranked lowest on Purchases in 2011 with ~19800 transactions\n    <\/li>\n    <li>Nov 2011 ranked highest on Purchases in 2011 with ~64000 transactions\n    <\/li>\n    <li>Dec 2011 had partial month data and hence had significantly lower volume of transactions\n    <\/li>\n<\/ul>","771c465e":"<p><b>Note:<\/b> Rate \u201crecency\" for customer who has been active more recently higher than the less recent customer, because each company wants its customers to be recent.<\/p>\n<p><b>Note:<\/b> Rate \u201cfrequency\" and \u201cmonetary\" higher, because the company wants the customer to visit more often and spend more money<\/p>","7358c17b":"<h5>Inference(s):\n<\/h5>\n<ul>\n    <li>Maximum volume of Churned Loyal Customer<\/li>\n    <li>Good volume of Churned Low Spending Infrequent Customer\n    <\/li>\n    <li>Good volume of Churned Medium-High Spender\n    <\/li>\n    <li>Good volume of Churned Frequent Customer\n    <\/li>\n    <li>Good volume of Medium\/High Spending Active Loyal Customer\n    <\/li>\n    <li>Notable volume of Churned Best Customer\n    <\/li>\n<\/ul>","382d99ac":"<h5>Inference(s):\n<\/h5>\n<ul>\n    <li>monetary dataframe has 2 columns with 4296 rows - 1 pertaining to each customer ID\n    <\/li>\n    <li>Data Integrity shape test successful on monetary dataframe\n    <\/li>\n<\/ul>","51253f10":"<h5>Inference(s):\n<\/h5>\n<ul>\n    <li>Maximum volume of Churned Low Spending Infrequent Customer\n    <\/li>\n    <li>Good volume of Low Spending Inactive Customer\n    <\/li>\n<\/ul>\n<h4  style=\"color:Navy;text-align:left;\">Check Customer Segment Spread for Cluster 2\n<\/h4>","e38e4ce9":"<h4  style=\"color:Navy;text-align:left;\">Check Customer Segment Spread for Cluster 0\n<\/h4>","070efc7d":"<h5>Inference(s):\n<\/h5>\n<ul>\n    <li>Sample Data validated fine and gave some intersting insights\n        <ul>\n            <li>Cluster Data Sample has records across clusters and segments\n            <\/li>\n            <li>Cluster 0 sample has: \n                <b>No customers<\/b>\n            <\/li>\n            <li>Cluster 1 sample has \n                <b>Bronze customers<\/b>\n            <\/li>\n            <li>Cluster 2 sample has \n                <b>Gold customers<\/b>\n            <\/li>\n            <li>Cluster 3 sample has: \n                <b>Gold and Silver customers<\/b>\n            <\/li>\n        <\/ul>\n    <\/li>\n<\/ul>","582ea256":"<h5>Inference(s):\n<\/h5>\n<ul>\n    <li>Column size increased from 8 to 9 for the same 4296 rows\n    <\/li>\n    <li>Lets check head operation to validate data integrity\n    <\/li>\n<\/ul>\n<h4  style=\"color:Navy;\">Data Integrity Check on RFMSegment\n<\/h4>","b1567e1e":"<h5>Inference(s):\n<\/h5>\n<ul>\n    <li>TotalPurchase Monthly Cohort Spend computed \n    <\/li>\n    <li>First cohort December2010 has maximum Sales as expected\n    <\/li>\n    <li>It has nan values below the diagonal which is as expected\n    <\/li>\n<\/ul>\n<h4  style=\"color:Navy;\">Plot Bar Graph on dataframe TotalPurchase\n<\/h4>","fa66679d":"<h4  style=\"color:Navy;text-align:left;\">Dataframe Melt\n<\/h4>","6546eac9":"<h2  style=\"color:DodgerBlue;text-align:left;\"> Data Cleaning\n<\/h2>\n<h2  style=\"color:MediumBlue;text-align:left;\"> Data Pre-processing\n<\/h2>","c03639b0":"<h4  style=\"color:Navy;text-align:left;\">Sample the Cluster Table data to check correctness<\/h4>","f009dbc0":"<h4  style=\"color:Navy;text-align:left;\">Create BoxPlot on Cluster Count Spread<\/h4>","5a66947e":"<h5>Inference(s):\n<\/h5>\n<ul>\n    <li>M quantile segment Values created in RFMSegment Dataframe\n    <\/li>\n    <li>RFMSegment data quality checked validating first 5 rows of data\n    <\/li>\n<\/ul>","3ffa63dd":"<h5>Inference(s):\n<\/h5>\n<ul>\n    <li>F quantile segment Values created in RFMSegment Dataframe\n    <\/li>\n    <li>RFMSegment data quality checked validating first 5 rows of data\n    <\/li>\n<\/ul>\n<h4 style=\"color:Navy;text-align:left;\">Compute M Quantile\n<\/h4> ","edf9bfdc":"<h5> Inference(s) Drawn from Mean Quantity of Items Purchased Spread by Cohort Month Heatmap<\/h5>\n<table style=\"width:100%;text-align:left;\">\n    <tr style=\"color:Crimson;\">\n        <th style=\"text-align:left;\">#<\/th>\n        <th style=\"text-align:left;\">Description<\/th>\n        <th style=\"text-align:left;\">Affected Cohorts<\/th>\n        <th style=\"text-align:left;\">Remarks<\/th>\n    <\/tr>\n    <tr>\n        <td style=\"text-align:left;\"><div style=\"word-wrap: break-word;\">1<\/div><\/td>\n        <td style=\"text-align:left;\"><div style=\"word-wrap: break-word;\">Maximum Mean Items Procured Cohort<\/div><\/td>\n        <td style=\"text-align:left;\"><div style=\"word-wrap: break-word;\">December 2010<\/div><\/td>\n        <td style=\"text-align:left;\"><div style=\"word-wrap: break-word;\">Consistently over 12 items<\/div><\/td>\n    <\/tr>\n    <tr>\n        <td style=\"text-align:left;\"><div style=\"word-wrap: break-word;\">2<\/div><\/td>\n        <td style=\"text-align:left;\"><div style=\"word-wrap: break-word;\">Minimum Mean Items Procured Cohort<\/div><\/td>\n        <td style=\"text-align:left;\"><div style=\"word-wrap: break-word;\">August 2011<\/div><\/td>\n        <td style=\"text-align:left;\"><div style=\"word-wrap: break-word;\">Consistently less than 11 items with minimum mean of 5.4 in October<\/div><\/td>\n    <\/tr>\n    <tr>\n        <td style=\"text-align:left;\"><div style=\"word-wrap: break-word;\">3<\/div><\/td>\n        <td style=\"text-align:left;\"><div style=\"word-wrap: break-word;\">Consistent Cohort by Mean Items Procured<\/div><\/td>\n        <td style=\"text-align:left;\"><div style=\"word-wrap: break-word;\">December 2010 to March 2011, June 2011 <\/div><\/td>\n        <td style=\"text-align:left;\"><div style=\"word-wrap: break-word;\">Atleast 9.6 mean items procured every month<\/div><\/td>\n    <\/tr>\n<\/table>","0ee7ee61":"<h5>Inference(s):\n<\/h5>\n<ul>\n    <li>R quantile segment Values created in RFMSegment Dataframe\n    <\/li>\n    <li>RFMSegment data quality checked validating first 5 rows of data\n    <\/li>\n<\/ul>\n <h4 style=\"color:Navy;text-align:left;\">Compute F Quantile\n<\/h4> ","77b6c024":"<h5>Inference(s):<\/h5>\n<ul>\n    <li>UnitPrice has negative values and potential outliers beyond 75% percentile<\/li>\n    <li>Quantity has negative values (refunds) and has potential outliers beyond 75% percentile and below 25th percentile<\/li>\n    <li>Let's ignore CustomerID describe statistics as its actually unique ordinal<\/li>\n  <\/ul>","db8cd98e":"<h5>Inference(s):\n<\/h5>\n<ul>\n    <li>AttributeInterDependence computed for each cluster for Recency, Frequency and Monetary\n    <\/li>\n    <li>Cluster 2 alone has high values on Frequency and Monetary Interdependence\n    <\/li>\n    <li>Cluster 0  and Cluster 2 have high negative values in Recency\n    <\/li>\n    <li>Cluster 0  and Cluster 1 have high negative values in Monetary\n    <\/li>\n    <li>Let's plot heatmap visualization to get more insights\n    <\/li>\n<\/ul>\n<h4  style=\"color:Navy;text-align:left;\">Clusterwise Related Dependence Heatmap<\/h4>","a7d03468":"<h5>Inference(s):<\/h5>\n<ul>\n    <li>data dataframe now has 536641 unique rows and 8 columns after removing duplicates<\/li>\n<\/ul>\n<h4  style=\"color:Navy;\">Identify count of CustomerID which have Valid Data<\/h4>","a62aa000":"<h4  style=\"color:Navy;\"> Boxplot Validation on UnitPrice after removing inappropriate data<\/h4>","713d558c":"<h5>Inference(s):\n<\/h5>\n<ul>\n    <li>Ordered List of 12 unique Months created from Dec 2010 to Nov 2011\n    <\/li>\n<\/ul>","b62070f3":"<h5>Inference(s):\n<\/h5>\n<ul>\n    <li>4 significant outlier records with 2 on either side of the distribution are present in quantity which was earlier highlighted in boxplot\n    <\/li>\n<\/ul>","d8f2f450":"<h4 style=\"color:Navy;text-align:left;\">RFM Quantile Summary <\/h4> \n<table style=\"width:100%;text-align:left;\">\n  <tr style=\"color:Purple;\">\n    <th style=\"text-align:center;\">RFM Type<\/th>\n      <th style=\"text-align:center;\">0th Percentile<\/th>\n      <th style=\"text-align:center;\">25th Percentile<\/th>\n      <th style=\"text-align:center;\">50th Percentile<\/th>\n        <th style=\"text-align:center;\">75th Percentile<\/th>\n        <th style=\"text-align:center;\">100th Percentile<\/th>\n        <th style=\"text-align:center;\">Outliers Comments<\/th>\n      <\/tr>\n  <tr>\n    <td style=\"text-align:center;\"><div style=\"word-wrap: break-word;\">Recency<\/div><\/td>\n    <td style=\"text-align:right;\"><div style=\"word-wrap: break-word;\">1<\/div><\/td>\n    <td style=\"text-align:right;\"><div style=\"word-wrap: break-word;\">16<\/div><\/td>\n    <td style=\"text-align:right;\"><div style=\"word-wrap: break-word;\">50<\/div><\/td>\n    <td style=\"text-align:right;\"><div style=\"word-wrap: break-word;\">144.50<\/div><\/td>\n    <td style=\"text-align:right;\"><div style=\"word-wrap: break-word;\">365<\/div><\/td>\n    <td style=\"text-align:right;\"><div style=\"word-wrap: break-word;\">Found Above 75th Percentile<\/div><\/td>\n      <\/tr>\n  <tr>\n     <td style=\"text-align:center;\"><div style=\"word-wrap: break-word;\">Frequency<\/div><\/td>\n    <td style=\"text-align:right;\"><div style=\"word-wrap: break-word;\">1<\/div><\/td>\n    <td style=\"text-align:right;\"><div style=\"word-wrap: break-word;\">17<\/div><\/td>\n    <td style=\"text-align:right;\"><div style=\"word-wrap: break-word;\">40<\/div><\/td>\n    <td style=\"text-align:right;\"><div style=\"word-wrap: break-word;\">97<\/div><\/td>\n    <td style=\"text-align:right;\"><div style=\"word-wrap: break-word;\">7288<\/div><\/td>\n    <td style=\"text-align:right;\"><div style=\"word-wrap: break-word;\">Found Above 75th Percentile<\/div><\/td>\n    <\/tr>\n    <tr>\n    <td style=\"text-align:center;\"><div style=\"word-wrap: break-word;\">Monetary<\/div><\/td>\n    <td style=\"text-align:right;\"><div style=\"word-wrap: break-word;\">2.90<\/div><\/td>\n    <td style=\"text-align:right;\"><div style=\"word-wrap: break-word;\">303.90<\/div><\/td>\n    <td style=\"text-align:right;\"><div style=\"word-wrap: break-word;\">653.02<\/div><\/td>\n    <td style=\"text-align:right;\"><div style=\"word-wrap: break-word;\">1594.62<\/div><\/td>\n    <td style=\"text-align:right;\"><div style=\"word-wrap: break-word;\">268478<\/div><\/td>\n    <td style=\"text-align:right;\"><div style=\"word-wrap: break-word;\">Found Above 75th Percentile<\/div><\/td>\n    <\/tr>\n    <tr>\n<\/table>    ","24b7aec9":"<h4  style=\"color:Navy;\"> Check Shape of Input Train Dataset<\/h4>","f958428b":"<h5>Inference(s):\n<\/h5>\n<ul>\n    <li>Heatmap executed between cluster Mean and Population Mean across Metrics of R, F and M to identify the interdependence\n    <\/li>\n    <li>\n        <b>Cluster 0<\/b>    \n        <ul>\n            <li>Highest Negative Value of -0.83 for R- \n                <b>Insight is most recent<\/b>\n            <\/li>\n            <li>Medium Negative Value of -0.5 for F - \n                <b>Insight is not so frequent<\/b>\n            <\/li>\n            <li>Medium to High Negative Value of -0.67 for M- \n                <b>Insight is low spending customer base<\/b>\n            <\/li>\n            <li>High amount of silver customers and mix of low monetary gold customers is due to this\n            <\/li>\n        <\/ul>\n    <li>\n        <b>Cluster 1<\/b>    \n        <ul>        \n            <li>Highest Positive Value of 0.94 for R - \n                <b>Insight is least recent and churned<\/b>\n            <\/li>\n            <li>Highest Negative Value of -0.86 for F across clusters -- \n                <b>Insight is least frequent and least active<\/b>\n            <\/li>\n            <li>Highest Negative Value of -0.87 for M across clusters -- \n                <b>Insight is low spending customer base<\/b>\n            <\/li>\n            <li>Highest amount of churned  customers (bronze) and small mix of low spending new customers with RFM close to median value of 6(silver) is due to this\n            <\/li>\n        <\/ul>\n    <li>\n        <b>Cluster 2<\/b>    \n        <ul>        \n            <li>High Negative Value of -0.8 for R - \n                <b>Insight is most recent with few cases above median<\/b>\n            <\/li>\n            <li>Highest Positive Value of 1.92 for F - \n                <b>Insight is most active customer base<\/b>\n            <\/li>\n            <li>Highest Positive Value of 2.23 for M - \n                <b>Insight is maximum spending customer base<\/b>\n            <\/li>\n            <li>Highest amount of best customers (Platinum) with few infrequent best customers and few churned best customers and medium-high spending loyal active customers make this segment (Gold)\n            <\/li>\n        <\/ul>\n    <li>\n        <b>Cluster 3<\/b>    \n        <ul>        \n           <li>Medium Positive Value of 0.26 for R- \n               <b>Insight is near the median values and not recent<\/b>\n            <\/li>\n           <li>Medium Negative Value of -0.31 for F - \n               <b>Insight is not so frequent with values near the median<\/b>\n            <\/li>\n           <li>Medium Negative Value of -0.42 for M- \n               <b>Insight is low to medium spending customer base with values mostly over median varying between 2 and 3<\/b>\n            <\/li>\n            <li>High amount of silver customers and mix of low monetary gold customers is due to this. This group will have cross over of some bronze customers if Monetary value is lower as the other 2 parameters are lower\n            <\/li>\n        <\/ul>\n    <\/li>\n<\/ul>","e893ee71":"<h5>Inference(s):\n<\/h5>\n<ul>\n    <li>Log Transformation completed\n    <\/li>\n    <li>Scaled Output stored in dataframe RFMModelScaled\n    <\/li>\n    <li>First 5 values of RFMModelScaled checked and passed data quality check\n    <\/li>\n<\/ul>\n<h4  style=\"color:Navy;text-align:left;\">Revalidate Distribution Subplot to check data skewness after log transformation and scaling\n<\/h4>","83e7049f":"<h5>Inference(s):\n<\/h5>\n<ul>\n    <li>13 unique customer segments mapped\n    <\/li>\n    <li>Churned Low Spending Infrequent Customer segment had 1233 of 4296 customers \n    <\/li>\n    <li>Low-Spending Active Loyal Customer segment had 91 of 4296 customers \n    <\/li>\n<\/ul>\n<h4 style=\"color:Navy;text-align:left;\">Check head of RFMSegment Dataframe for Data Integrity\n<\/h4> ","4d05ec14":"<h5>Inference(s):\n<\/h5>\n<ul>\n    <li>Maximum volume of Best customers\n    <\/li>\n    <li>Maximum volume of Inactive Best customers\n    <\/li>\n    <li>High volume of Medium\/High Spending Active Loyal Customer\n    <\/li>\n<\/ul>\n<h4  style=\"color:Navy;text-align:left;\">Check Customer Segment Spread for Cluster 3<\/h4>","83dfe739":"<h4 style=\"color:Navy;\"> Top 20 Customers by Spending <\/h4>","38a75214":"<h5>Inference(s):<\/h5>\n<ul>\n    <li>\n        <b>United Kingdom <\/b>\n        has approximately\n        <b> 89% <\/b>\n        of Purchase Sales Split By Country<\/li>\n<\/ul>    ","a6323c24":"<h4  style=\"color:Navy;text-align:left;\"> Compute Cohort Index using the year and month parts of cohort and invoice dates<\/h4>","37c32e88":"<h4 style=\"color:Navy;text-align:left;\">Data Clean Up - Drop Columns RecentPurchaseDate and CurrentDate<\/h4>","498c344d":"<h3 style=\"color:MediumBlue;text-align:left;\">Data Reporting Completed<\/h3>\n\n<ul>\n    <li>1. Created a dashboard in tableau by choosing appropriate chart types and metrics useful for the business. The dashboard must entail the following:\n        <ul>    \n            <li>a. Country-wise analysis to demonstrate average spend. Used a bar chart to show the monthly figures - \n                <b> Check Sales Dash filtering on Month and Region and use  reset filters to reset<\/b>\n            <\/li>\n            <li>b. Bar graph of top 15 products which are mostly ordered by the users to show the number of products sold -  \n                <b> Check Sales Dash filtering on Month and Region and use parameter top N and use  reset filters to reset filters, no reset on parameter<\/b>\n            <\/li>\n            <li>c. Bar graph to show the count of orders vs. hours throughout the day - \n                <b> Check Sales Dash filtering on Month and Region and use  reset filters to reset<\/b>\n            <\/li>\n            <li>d. Plot the distribution of RFM values using histogram and frequency charts - \n                <b> Check Cluster Dash filtering on Cluster and use  reset filters to reset<\/b>\n            <\/li>\n            <li>e. Plot error (cost) vs. number of clusters selected - \n                <b> Check Cluster Dash filtering on Cluster and use reset filters to reset<\/b>\n            <\/li>\n            <li>f. Visualize to compare the RFM values of the clusters using heatmap - \n                <b> Check Customer Dash filtering on CustomerID and use  reset filters to reset<\/b>\n            <\/li>\n        <\/ul>\n    <\/li>\n<\/ul>","2c4cd1f2":"<h3 style=\"color:MediumBlue;text-align:left;\">Data Modeling Completed for:<\/h3>\n<ul>\n    <li>1. Creating clusters using k-means clustering algorithm.\n        <ul>    \n            <li>a. Preparing the data for the algorithm. If the data is asymmetrically distributed, manage the skewness with appropriate transformation. Standardize the data.\n            <\/li>\n            <li>b. Deciding the optimum number of clusters to be formed.\n            <\/li>\n            <li>c. Analyzing these clusters and comment on the results.\n            <\/li>\n        <\/ul>\n    <\/li>\n<\/ul>","f46b50f1":"<h4  style=\"color:Navy;text-align:left;\">Print the KMeans Model<\/h4>","364ddfe6":"<h4  style=\"color:Navy;text-align:left;\">Validate silhouette Score\n<\/h4>","6e110a2a":"<h4 style=\"color:Navy;text-align:left;\">Export Dataframes Data and ClusterData to Excel<\/h4>","021dea00":"<h4  style=\"color:Navy;text-align:left;\"> Cohort Mean Quantity of Gift Items Purchased Spread by CohortMonth<\/h4>","28c9eafd":"<h5>Inference(s):\n<\/h5>\n<ul>\n    <li>Cluster 1 sample has: \n        <b>Bronze customers<\/b>\n    <\/li>\n    <li>RFM Score is 5 or lower in the sample\n    <\/li>\n    <li>Sample had mostly churned customers or Low Spending Inactive Customers\n    <\/li>\n    <li>Monetary capitalization of this cluster is very low of this cluster is very low\n    <\/li>\n<\/ul>\n<h4  style=\"color:Navy;text-align:left;\">Sample the Cluster Table data for Cluster 0\n<\/h4>","6085ac75":"<h3  style=\"color:MediumBlue;text-align:left;\"> Cohort Analysis : \n<\/h3>\n<p>What is Cohort Analysis?\n<\/p>\n<p>A cohort is a group of subjects who share a defining characteristic. We can observe how a cohort behaves across time and compare it to other cohorts. \n<\/p>\n<ul>\n    <li> Mutually exclusive segments - cohorts\n    <\/li>\n    <li>Compare metrics across product lifecycle\n    <\/li>\n    <li>Compare metrics across customer lifecycle\n    <\/li>\n<\/ul>\n\n<h4  style=\"color:Navy;text-align:left;\"> Types of cohorts:  \n<\/h4>\n<ul>\n    <li>\n        <b>Time Cohorts \n        <\/b>\n        are customers who signed up for a product or service during a particular time frame. Analyzing these cohorts shows the customers\u2019 behavior depending on the time they started using the company\u2019s products or services. The time may be monthly or quarterly even daily. \n    <\/li>\n    <li>\n        <b>Behavior cohorts\n        <\/b>\n        are customers who purchased a product or subscribed to a service in the past. It groups customers by the type of product or service they signed up. Customers who signed up for basic level services might have different needs than those who signed up for advanced services. Understanding the needs of the various cohorts can help a company design custom-made services or products for particular segments.\n    <\/li>\n    <li>\n        <b>Size cohorts\n        <\/b>\n        refer to the various sizes of customers who purchase company\u2019s products or services. This categorization can be based on the amount of spending in some periodic time after acquisition or the product type that the customer spent most of their order amount in some period of time.\n    <\/li>\n<\/ul>\n<h4  style=\"color:Navy;text-align:left;\"> For cohort analysis, there are a few labels that we have to create:<\/h4>\n<ul>\n    <li>\n        <b>Invoice period:\n        <\/b>\n        A string representation of the year and month of a single transaction\/invoice. \n    <\/li>\n    <li>\n        <b>Cohort group:\n        <\/b>\n        A string representation of the year and month of a customer\u2019s first purchase. This label is common across all invoices for a particular customer. \n    <\/li>\n    <li>\n        <b>Cohort period\/Index: \n        <\/b>\n        An integer representation a customer\u2019s stage in its \u201clifetime\u201d. The number represents the number of months passed since the first purchase.\n    <\/li>\n<\/ul>","a657de6d":"<h5>Inference(s):\n<\/h5>\n<ul>\n    <li>Validation of Data Integrity <b>successful<\/b> between clusterLabels and  RFMSegment\n    <\/li>\n    <li>4296 was the number of rows returned for each which is correct and equal to unique Customer ID's\n    <\/li>\n<\/ul>\n<h4  style=\"color:Navy;text-align:left;\">Build ClusterData Table\n<\/h4>","7c28219e":"<h5>Inference(s):<\/h5>\n<ul>\n    <li>Customer 14646 from Netherlands has shopped for 279K \u00a3 during the given period which is the maximum\n    <\/li>\n<\/ul>","e3b2d4a5":"<h5> Inference(s):\n<\/h5>\n<ul>\n    <li>Cluster 4 has best Silhouette score with 0.3\n    <\/li>\n    <li>Lets plot KMeans with 4 clusters\n    <\/li>\n<\/ul>\n<h4  style=\"color:Navy;text-align:left;\">Decide the optimum number of clusters to be formed as 4\n<\/h4>\n<h4  style=\"color:Navy;text-align:left;\">Build KMeans Model with 4 Clusters and fix random state as 1 to ensure no variation during rerun\n<\/h4>","4ef39c06":"<h4 style=\"color:Navy;\"> Average Purchase Per Transaction by Country <\/h4>","c2831690":"<h5>Inference(s):\n<\/h5>\n<ul>\n    <li>RFM Segments are created\n    <\/li>\n    <li>RFMSegment data quality checked validating first 5 rows of data\n    <\/li>\n    <li>\n        <b>Best Recency score: <\/b>\n        4\n    <\/li>\n    <li>\n        <b>Best Frequency score: <\/b>\n        4\n    <\/li>\n    <li> \n        <b>Best Monetary score: <\/b>\n        4\n    <\/li>\n    <li>\n        <b>Best RFM Segment: <\/b>\n        444\n    <\/li>\n    <li>\n        <b>Best RFM Score: <\/b>\n        12\n    <\/li>\n<\/ul>\n<h4 style=\"color:Navy;text-align:left;\">Print RFM Unique String Values\n<\/h4> ","548823a1":"<h5>Inference(s):<\/h5>\n<ul>\n    <li>Quantity has outliers spread on both sides and needs further preprocessing\n    <\/li>\n<\/ul>\n<h4  style=\"color:Navy;\"> InterQuartile Range (IQR) 1.5 IQR Rule on Quantity to Identify Outliers in Data Distribution<\/h4>","f33a367a":"<h5>Inference(s):<\/h5>\n<ul>\n    <li>Less than 1% duplicates found - let's drop them before moving forward<\/li>\n<\/ul>\n<h4  style=\"color:Navy;\">Drop Identified Duplicates<\/h4>","ac91f738":"<h4  style=\"color:Navy;text-align:left;\">Customer Segment Spread by Categories<\/h4>","00993120":"<h5>Inference(s):\n<\/h5>\n<ul>\n    <li>obtainDateInterval function run on InvoiceMonth and CohortMonth to parse the year and month parts\n    <\/li>\n<\/ul>","605acddc":"<h5>Inference(s):<\/h5>\n<ul>\n    <li>Quantity outliers found based on IQR test on both sides\n    <\/li>\n<\/ul>","c05d1973":"<h5>Inference(s):\n<\/h5>\n<ul>\n    <li>cohortdata dataframe has 375665 rows of data spread across 9 columns which matches the data dataframe\n    <\/li>\n<\/ul>\n<h4 style=\"color:Navy;\">Perform head operation on cohortdata to check data integrity\n<\/h4>        ","3980a403":"<h5>Inference(s):\n<\/h5>\n<ul>\n    <li>TotalPurchase Monthly Cohort Spend computed shown in bar plot\n    <\/li>\n    <li>November 2011 had maximum Cumulative Sales\n    <\/li>\n<\/ul>\n<h4  style=\"color:Navy;\">Create Heatmap on Total Items Purchased Spread by Cohort Month\n<\/h4>","6bb52cf8":"<h5>Inference(s):\n<\/h5>\n<ul>\n    <li>After EDA preprocessing, Customer 14646 from Netherlands has shopped for 268K \u00a3 during the given period which is the maximum\n    <\/li>\n<\/ul>\n<h4  style=\"color:Navy;\">EDA on Countries Dataframe<\/h4>","4c3e9b7a":"<h3 style=\"color:MediumBlue;text-align:left;\">Frequency\n<\/h3>\n\n<p>\n    <b>Frequency <\/b>\n    is about the number of purchase in a given period. It could be 3 months, 6 months or 1 year. So we can understand this value as for how often or how many a customer used the product of a company. The bigger the value is, the more engaged the customers are\n<\/p>","87d95478":"<h5>Inference(s):\n<\/h5>\n<ul>\n    <li>monetary Dataframe computed and it has 2 columns CustomerID and Monetary\n    <\/li>\n    <li>Data quality check successful on first 5 rows validation\n    <\/li>\n    <\/ul>\n<h4  style=\"color:Navy;\">Data Shape Quality Check on monetary Dataframe\n<\/h4>  ","ce8dda22":"<h4 style=\"color:Navy;text-align:left;\">Create segment_customer function on RFMtable created above to map RFM segments to customer segments\n<\/h4> ","e34ce10d":"<h5>Inference(s):\n<\/h5>\n<ul>\n    <li>RecentPurchaseDate and CurrentDate columns dropped from recency dataframe\n    <\/li>\n    <li> Data quality check successful on first 5 rows validation\n    <\/li>\n<\/ul>\n\n<h4 style=\"color:Navy;text-align:left;\">Data Integrity Shape Check on Recency Dataframe\n<\/h4>","f3882c6b":"<h5>Inference(s):\n<\/h5>\n<ul>\n    <li>First 5 Matched rows of RFMData Customer ID and RFMModel Index displayed from dataframe RFMData\n    <\/li>\n    <li>Data quality check successful on first 5 rows validation\n    <\/li>\n <\/ul>","5c8bdc16":"<h5>Inference(s):\n<\/h5>\n<ul>\n    <li>ClusterData Table Built from RFMSegment with Cluster column getting added\n    <\/li>\n    <li>Data Quality Check Validated Fine while verifying the first few rows\n    <\/li>\n<\/ul>\n<h4  style=\"color:Navy;text-align:left;\">Check Shape of Dataframe ClusterData<\/h4>","3ca57beb":"<h5>Inference(s):\n<\/h5>\n<ul>\n    <li>Dataframe dataTableau has 375665 rows of information which matches the count of rows on data Dataframe and there are no missing values - ready for Tableau Visualization with additional data of Country_Code,Region and InvoiceMonth\n    <\/li>\n<\/ul>\n<h4  style=\"color:Navy;\">Compute Number of Refunded Invoices for Tableau Visualization KPI\n<\/h4>        ","726ac2df":"<h3 style=\"color:MediumBlue;text-align:left;\">Data Reporting:<\/h3>\n\n<ul>\n    <li>1. Create a dashboard in tableau by choosing appropriate chart types and metrics useful for the business. The dashboard must entail the following:\n        <ul>    \n            <li>a. Country-wise analysis to demonstrate average spend. Use a bar chart to show the monthly figures\n            <\/li>\n            <li>b. Bar graph of top 15 products which are mostly ordered by the users to show the number of products sold\n            <\/li>\n            <li>c. Bar graph to show the count of orders vs. hours throughout the day\n            <\/li>\n            <li>d. Plot the distribution of RFM values using histogram and frequency charts\n            <\/li>\n            <li>e. Plot error (cost) vs. number of clusters selected\n            <\/li>\n            <li>f. Visualize to compare the RFM values of the clusters using heatmap\n            <\/li>\n        <\/ul>\n    <\/li>\n<\/ul>","5952ed7f":"<h5>Inference(s):<\/h5>\n<ul>\n    <li>data Dataframe has 8 columns and 401604 rows\n    <\/li>\n    <li>CustomerID column converted to String Type\n    <\/li>   \n<\/ul>","4234cfa3":"<h5>Inference(s):\n<\/h5>\n<ul>\n    <li>Validation of Recency computation between RFMData and RFMModel was successful and return positive boolean output for first CustomerID on RFMModel dataframe\n    <\/li>\n<\/ul>\n<h4  style=\"color:Navy;\">RFM (Recency Frequency Monetary) model RFMModel created\n<\/h4>","4ed6230f":"<h5>Inference(s):\n<\/h5>\n<ul>\n    <li>Good volume of Medium\/High Spending Active Loyal Customer\n    <\/li>\n    <li>Good volume of recent Low-Spending New Customer and recent Low Spending Inactive Customer \n    <\/li>\n    <li>Good volume of Medium\/High Spending Active Customer \n    <\/li>\n    <li>Good volume of Low-Spending Active Loyal Customer\n    <\/li>\n<\/ul>\n<h4  style=\"color:Navy;text-align:left;\">Check Customer Segment Spread for Cluster 1\n<\/h4>","1d22ad9f":"<h3  style=\"color:MediumBlue;text-align:left;\">Analyse the retention rate of customers<\/h3>\n\n<p>Customer retention is a very useful metric to understand how many of all the customers are still active. It gives you the percentage of active customers compared to the total number of customers <\/p>\n<h4  style=\"color:Navy;text-align:left;\">Create summarization cohortGroup on CohortMonth and CohortIndex<\/h4>","bedc67cc":"<h5>Inference(s):\n<\/h5>\n<ul>\n    <li>Single Metric computed each for R, F amd M for customer\n    <\/li>\n    <li>New variable Metric\n    <\/li>\n    <li>Data quality check validated okay - lets check the shape\n    <\/li>\n<\/ul>\n<h4  style=\"color:Navy;text-align:left;\">Check shape of RFMModelScaledMelt dataframe\n<\/h4>","9f60c81a":"<h4 style=\"color:Navy;text-align:left;\">Copy EDA completed Data dataframe into RFMData for RFM Analysis<\/h4>","91ab8249":"<h5>Inference(s):\n<\/h5>\n<ul>\n    <li>12888 rows of data with 4 columns is the shape\n    <\/li>\n    <li>Each of the 4296 CustomerID's now has 3 metric values - one for Recency, Frequency and Monetary but we have only 4 columns\n    <\/li>\n    <li>Melt is successful - let's do snake plot visualization\n    <\/li>\n<\/ul>\n<h4  style=\"color:Navy;text-align:left;\">RFM Snake Plot Visualization\n<\/h4>","8de9ef2a":"<h4 style=\"color:Navy;text-align:left;\">Create a hypothetical snapshot currentDate data as if we're doing analysis recently<\/h4>","91e8f362":"<h5>Inference(s):\n<\/h5>\n<ul>\n    <li>RFMSegment data quality checked validating first 5 rows of data\n    <\/li>\n<\/ul>\n<h4 style=\"color:Navy;text-align:left;\">Compute R, F and M Quantiles for each Customer ID\n<\/h4>\n<ul>\n    <li>Process of calculating percentiles:\n        <ul>\n            <li>1. Sort customers based on that metric\n            <\/li>\n            <li>2. Break customers into a pre-defined number of groups of equal size\n            <\/li>\n            <li>3. Assign a label to each group\n            <\/li>\n        <\/ul>\n    <\/li>\n<\/ul>\n<h4 style=\"color:Navy;text-align:left;\">Compute R Quantile\n<\/h4>\n<ul>\n    <li>Highest score to the best metric - best is not always highest e.g. recency\n    <\/li>\n    <li>In this case, the label is inverse - the more recent the customer, the better\n    <\/li>\n<\/ul>","1ecee5e2":"<h5> Inferences<\/h5>\n<ul>\n    <li>WCSS scores computed for clusters 1 to 13\n    <\/li>\n    <li>4 or 5 would be a good number considering the WCSS error tapers down after this\n    <\/li>\n    <li>Let's plot WCSS elbow graph to check our inference\n    <\/li>\n<\/ul>\n<h4  style=\"color:Navy;text-align:left;\">Plot WCSS Elbow Graph from computed WCSS Inertia Dataframe WCSSKmeansOut\n<\/h4>","efa1e9ea":"<h5>Inference(s):\n<\/h5>\n<ul>\n    <li>IQR shows unit price high positive outlier - lets ignore as its valid\n    <\/li>\n<\/ul>\n<h4  style=\"color:Navy;\">Visualise the UnitPrice Distribution<\/h4>","6ec83c56":"<h4  style=\"color:Navy;text-align:left;\">Heat Map<\/h4>\n\n<p>Heat map to visualize the relative importance of each attributes across 4 clusters.<\/p>\n<p>The farther a ratio is from 0, the more important that attribute is for a segment relative to the total population.<\/p>","7108c873":"<h5>Inference(s):<\/h5>\n<ul>\n    <li>No negative values - proceed with log transformation.\n    <\/li>\n<\/ul>\n<h4  style=\"color:Navy;text-align:left;\">Log Transformation of RFMModel and Scaling the Transformed Log Model into Scaled Model RFMModelScaled\n<\/h4>","3c68ce9b":"<h4  style=\"color:Navy;\"> Top 20 Purchase Split By Item Description<\/h4>","0a855e8b":"<h3 style=\"color:MediumBlue;text-align:left;\">Monetary\n<\/h3>\n<p>\n    <b>Monetary <\/b>\n    is the total amount of money a customer spent in that given period. Therefore big spenders will be differentiated with other customers such as MVP or VIP. \n<\/p>\n<h4 style=\"color:Navy;\">Compute Monetary Value  Dataframe grouping on Customer ID by sum of TotalCost\n<\/h4> ","0aef1d08":"<h4  style=\"color:Navy;text-align:left;\">Below is a table with key RFM segments:\n<\/h4>\n\n<table style=\"width:100%;text-align:left;\">\n  <tr style=\"color:Crimson;\">\n    <th style=\"text-align:left;\">Customer Segment<\/th>\n      <th style=\"text-align:left;\">RFM Segment<\/th>\n      <th style=\"text-align:left;\">Criteria<\/th>\n      <th style=\"text-align:left;\">Marketing<\/th>\n      <\/tr>\n  <tr>\n      <td style=\"text-align:left;\"><div style=\"word-wrap: break-word;\">Best Customers<\/div><\/td>\n      <td style=\"text-align:left;\"><div style=\"word-wrap: break-word;\">444<\/div><\/td>\n      <td style=\"text-align:left;\"><div style=\"word-wrap: break-word;\">Recency : Highest ; Frequency Highest; Monetary Value Highest<\/div><\/td>\n      <td style=\"text-align:left;\"><div style=\"word-wrap: break-word;\">No price incentives, new products and loyalty program<\/div><\/td>\n      <\/tr>\n  <tr>\n       <td style=\"text-align:left;\"><div style=\"word-wrap: break-word;\">Churned Best Customer<\/div><\/td>\n      <td style=\"text-align:left;\"><div style=\"word-wrap: break-word;\">144,244<\/div><\/td>\n      <td style=\"text-align:left;\"><div style=\"word-wrap: break-word;\">Recency : Below Median ; Frequency Highest; Monetary Value Highest<\/div><\/td>\n      <td style=\"text-align:left;\"><div style=\"word-wrap: break-word;\">Incentivise via price to buy in near future and also frequently, new products and loyalty program<\/div><\/td>\n    <\/tr>\n    <tr>\n        <td style=\"text-align:left;\"><div style=\"word-wrap: break-word;\">Churned Frequent Customer<\/div><\/td>\n        <td style=\"text-align:left;\"><div style=\"word-wrap: break-word;\">131,132,141,142,231,232,241,242<\/div><\/td>\n        <td style=\"text-align:left;\"><div style=\"word-wrap: break-word;\">Recency : Below Median ; Frequency Below Median; Monetary Value Above Median<\/div><\/td>\n        <td style=\"text-align:left;\"><div style=\"word-wrap: break-word;\">Incentivise via price to buy in near future and also frequently, new economy products and loyalty program<\/div><\/td>\n    <\/tr>\n    <tr>\n        <td style=\"text-align:left;\"><div style=\"word-wrap: break-word;\">Churned Low Spending Infrequent Customer<\/div><\/td>\n        <td style=\"text-align:left;\"><div style=\"word-wrap: break-word;\">111,112,121,122,211,212,221,222<\/div><\/td>\n        <td style=\"text-align:left;\"><div style=\"word-wrap: break-word;\"> Recency : Below Median; Frequency Below Median; Monetary Value Below Median<\/div><\/td>\n        <td style=\"text-align:left;\"><div style=\"word-wrap: break-word;\">Limited campaign to buy new economy products with price incentives, check casually on loyalty program interest<\/div><\/td>\n    <\/tr>\n    <tr>\n    <td style=\"text-align:left;\"><div style=\"word-wrap: break-word;\">Churned Loyal Customer<\/div><\/td>\n        <td style=\"text-align:left;\"><div style=\"word-wrap: break-word;\">133,134,143,233,234,243<\/div><\/td>\n        <td style=\"text-align:left;\"><div style=\"word-wrap: break-word;\">Recency : Below Median ; Frequency Above Median; Monetary Value Above Median<\/div><\/td>\n        <td style=\"text-align:left;\"><div style=\"word-wrap: break-word;\">Focus on Loyalty Program - Incentivise to Buy<\/div><\/td>\n      <\/tr>\n    <tr>\n    <td style=\"text-align:left;\"><div style=\"word-wrap: break-word;\">Churned Medium-High Spender<\/div><\/td>\n        <td style=\"text-align:left;\"><div style=\"word-wrap: break-word;\">113,114,123,124,213,214,223,224<\/div><\/td>\n        <td style=\"text-align:left;\"><div style=\"word-wrap: break-word;\">Recency : Below Median ; Frequency Below Median; Monetary Value Above Median<\/div><\/td>\n        <td style=\"text-align:left;\"><div style=\"word-wrap: break-word;\">Focus on Loyalty Program - Incentivise to Buy Again with new products<\/div><\/td>\n      <\/tr>\n     <tr>\n    <td style=\"text-align:left;\"><div style=\"word-wrap: break-word;\">Inactive Best Customer<\/div><\/td>\n        <td style=\"text-align:left;\"><div style=\"word-wrap: break-word;\">344<\/div><\/td>\n        <td style=\"text-align:left;\"><div style=\"word-wrap: break-word;\">Recency : 3 ; Frequency Highest; Monetary Value Highest<\/div><\/td>\n        <td style=\"text-align:left;\"><div style=\"word-wrap: break-word;\">No price incentives, new products and loyalty program<\/div><\/td>\n      <\/tr>\n     <tr>\n    <td style=\"text-align:left;\"><div style=\"word-wrap: break-word;\">Low Spending Inactive Customer<\/div><\/td>\n        <td style=\"text-align:left;\"><div style=\"word-wrap: break-word;\">311,312,321,322<\/div><\/td>\n        <td style=\"text-align:left;\"><div style=\"word-wrap: break-word;\">Recency : Above Median ; Frequency Below Median; Monetary Value Below Median<\/div><\/td>\n        <td style=\"text-align:left;\"><div style=\"word-wrap: break-word;\">High price incentives, new products and loyalty program<\/div><\/td>\n      <\/tr>\n    <tr>\n    <td style=\"text-align:left;\"><div style=\"word-wrap: break-word;\">Low-Spending Active Loyal Customer<\/div><\/td>\n        <td style=\"text-align:left;\"><div style=\"word-wrap: break-word;\">431,432,441,442<\/div><\/td>\n        <td style=\"text-align:left;\"><div style=\"word-wrap: break-word;\">Recency : Highest ; Frequency Above Median; Monetary Value Below Median<\/div><\/td>\n        <td style=\"text-align:left;\"><div style=\"word-wrap: break-word;\">High price incentives, market niche products in economy segment increasing spend<\/div><\/td>\n    <\/tr>\n    <tr>\n    <td style=\"text-align:left;\"><div style=\"word-wrap: break-word;\">Low-Spending Inactive Loyal Customer<\/div><\/td>\n        <td style=\"text-align:left;\"><div style=\"word-wrap: break-word;\">331,332,341,342<\/div><\/td>\n        <td style=\"text-align:left;\"><div style=\"word-wrap: break-word;\">Recency : 3; Frequency Above Median; Monetary Value Below Median<\/div><\/td>\n        <td style=\"text-align:left;\"><div style=\"word-wrap: break-word;\">High price incentives, market niche products in economy segment increasing spend<\/div><\/td>\n    <\/tr>\n    <tr>\n    <td style=\"text-align:left;\"><div style=\"word-wrap: break-word;\">Low-Spending New Customer<\/div><\/td>\n        <td style=\"text-align:left;\"><div style=\"word-wrap: break-word;\">411,412,421,422<\/div><\/td>\n        <td style=\"text-align:left;\"><div style=\"word-wrap: break-word;\">Recency : Highest ; Frequency Below Median; Monetary Value Below Median<\/div><\/td>\n        <td style=\"text-align:left;\"><div style=\"word-wrap: break-word;\">High price incentives, market niche products in economy segment increasing spend, try loyalty program<\/div><\/td>\n    <\/tr>\n    <tr>\n    <td style=\"text-align:left;\"><div style=\"word-wrap: break-word;\">Medium\/High Spending Active Customer<\/div><\/td>\n        <td style=\"text-align:left;\"><div style=\"word-wrap: break-word;\">313,314,323,324,413,414,423,424<\/div><\/td>\n        <td style=\"text-align:left;\"><div style=\"word-wrap: break-word;\">Recency : Above Median ; Frequency Below Median; Monetary Value Above Median<\/div><\/td>\n        <td style=\"text-align:left;\"><div style=\"word-wrap: break-word;\">Low price incentives, market niche products in premium segment increasing spend, go for loyalty program<\/div><\/td>\n    <\/tr>\n    <tr>\n    <td style=\"text-align:left;\"><div style=\"word-wrap: break-word;\">Medium\/High Spending Active Loyal Customer<\/div><\/td>\n        <td style=\"text-align:left;\"><div style=\"word-wrap: break-word;\">333,334,343,433,434,443<\/div><\/td>\n        <td style=\"text-align:left;\"><div style=\"word-wrap: break-word;\">Recency : Above Median ; Frequency Above Median; Monetary Value Above Median<\/div><\/td>\n        <td style=\"text-align:left;\"><div style=\"word-wrap: break-word;\">Low price incentives, market niche products in premium segment, market associated products based on previous items surfed\/purchased<\/div><\/td>\n    <\/tr>\n<\/table>    ","f5f6a254":"<p>To plot this we should have normalized data distribution and all the attributes in a single column. We will use pandas melt facility on RFMModelScaled to achieve that<\/p>","eba7372e":"<h5>Inference(s):<\/h5>\n<ul>\n    <li>test Dataframe has 2 nan value on CustomerID Column in the first 5 records\n    <\/li>\n    <li> Data quality check successful on first 5 rows validation\n    <\/li>\n<\/ul>\n<h4  style=\"color:Navy;\">Concatenate train and test Dataframes into Single Merged dataframe data<\/h4>","5283daf9":"<h5>Inference(s):<\/h5>\n<ul>\n    <li>\n        <b>groupByTotalCost <\/b>\n        dataframe summarized at InvoiceNo level aggregating sum of TotalCost for Top 20 Invoices\n    <\/li>\n    <li> Data quality check successful on first 5 rows validation\n    <\/li>\n<\/ul>\n<h4 style=\"color:Navy;\">Plot Bar Plot Top 20 Purchases by Invoice Number<\/h4>    ","9fd9a9ec":"<h5>Inference(s):\n<\/h5>\n<ul>\n    <li>Recency dataframe has 2 columns with 4296 rows - 1 pertaining to each customer ID\n    <\/li>\n    <li>Data Integrity shape test successful on recency dataframe\n    <\/li>\n<\/ul>","20be0de2":"<h5>Inference(s):<\/h5>\n<ul>\n    <li>data Dataframe displays fine and has 8 columns\n    <\/li>\n    <li>CustomerID column converted to Integer Type\n    <\/li>\n    <li> Data quality check successful on first 5 rows validation\n    <\/li>\n<\/ul>\n<h4  style=\"color:Navy;\"> Change the DType of CustomerID to String<\/h4>","df297e91":"<h5>Inference(s):\n<\/h5>\n<ul>\n    <li>cohortdata dataframe passed data integrity test on examining head of the data\n    <\/li>\n<\/ul>","f1276c7b":"<h4  style=\"color:Navy;\"> Boxplot on Quantity to Identify Outliers in Data Distribution<\/h4>","a87e726f":"<h3 style=\"color:MediumBlue;text-align:left;\">Build RFM Segments. Give recency, frequency, and monetary scores individually by dividing them into quartiles.<\/h3>\n<ul>\n    <li>a. Combine three ratings to get a RFM segment (as strings)\n    <\/li>\n    <li>b. Get the RFM score by adding up the three ratings\n    <\/li>\n    <li>c. Analyze the RFM segments by summarizing them and comment on the findings\n    <\/li>\n<\/ul>\n<p>Note: Rate \u201crecency\" for customer who has been active more recently higher than the less recent customer, because each company wants its customers to be recent.<\/p>\n<p>Note: Rate \u201cfrequency\" and \u201cmonetary\" higher, because the company wants the customer to visit more often and spend more money<\/p>\n\n<h4 style=\"color:Navy;text-align:left;\">Create RFM Score for R, F and M on a scale of 1 to 4 and concatenate as string to assign Customer Segment<\/h4>","c829d0ec":"<h5>Inference(s):\n<\/h5>\n<ul>\n    <li>40 items with unit price = 0 removed\n    <\/li>\n<\/ul>","9a45617e":"<h5>Inference(s):\n<\/h5>\n<ul>\n    <li>RFMModel dataframe has 4 columns with 4296 rows - 1 pertaining to each customer ID\n    <\/li>\n    <li>Data Integrity shape test successful on RFMModel dataframe\n    <\/li>\n<\/ul>\n<h4  style=\"color:Navy;\">Data Integrity Check on Unique Customers on Baselined Data dataframe\n<\/h4>    ","7c75abbf":"<h4 style=\"color:Navy;text-align:left;\">Compute Frequency Dataframe grouping on Customer ID by count of InvoiceNo\n<\/h4>","014b3700":"<h5>Inference(s):<\/h5>\n<ul>\n    <li>test Dataframe has 162573 rows of information and 8 columns<\/li>\n<\/ul>\n<h4  style=\"color:Navy;\"> Validate First Few Records of Input Test Dataframe<\/h4>","a8562d9e":"<h4  style=\"color:Navy;\">Transaction Spread by Month for Year 2011<\/h4>","7514a5be":"<h3  style=\"color:MediumBlue;text-align:left;\">Create clusters using k-means clustering algorithm\n<\/h3>\n<h4  style=\"color:Navy;text-align:left;\">Prepare the data for the algorithm. If the data is asymmetrically distributed, manage the skewness with appropriate transformation. Standardize the data.\n<\/h4>","46596364":"<h2 style=\"color:DodgerBlue;text-align:left;\"> Problem Statement <\/h2>\n<p>It is a critical requirement for business to understand the value derived from a customer. RFM is a method used for analyzing customer value.\nCustomer segmentation is the practice of segregating the customer base into groups of individuals based on some common characteristics such as age, gender, interests, and spending habits\nPerform customer segmentation using RFM analysis. The resulting segments can be ordered from most valuable (highest recency, frequency, and value) to least valuable (lowest recency, frequency, and value).<\/p> ","6e358e36":"<h4  style=\"color:Navy;text-align:left;\">Compute time offset in months for InvoiceMonth and CohortMonth to obtain Year and Month\n<\/h4>","806495a1":"<h5>Inference(s):<\/h5>\n<ul>\n    <li>Train Dataframe has nan value on CustomerID Column for first record\n    <\/li>\n    <li> Data quality check successful on first 5 rows validation\n    <\/li>\n<\/ul>\n<h4  style=\"color:Navy;\">Check the shape of Test Dataframe<\/h4>","e2b4c221":"<h5>Inference(s):\n<\/h5>\n<ul>\n    <li>MarketSegment field imputed\n    <\/li>\n    <li>On an average the 398 platinum customers spent 9484 \u00a3 while the 1259 bronze customers spent on an average ~264 \u00a3 showing a huge disparity between the High Spending and the low spending customers\n    <\/li>\n    <li>On an average the 398 platinum customers visited 7 days ago and did 363 transactions against average of 190 days and only ~15 transactions per person for the 1259 bronze customers\n    <\/li>\n<\/ul>","1cdee02a":"<h4 style=\"color:Navy;\"> Overall Purchase by Country <\/h4>","56a1490d":"<h4 style=\"color:Navy;\">Set relative path of input files<\/h4>","d5530ca0":"<h5>Inference(s):<\/h5>\n<ul>\n    <li>Monetary Quantile Distribution Boxplot shows outliers beyond 75th percentile\n    <\/li>\n<\/ul> ","c615b591":"<h4  style=\"color:Navy;\"> Create Feature Total Cost as Quantity * UnitPrice<\/h4>","e8f8e3c5":"<h5>Inference(s):\n<\/h5>\n<ul>\n    <li>Data Visualization shows 1233 Churned Low Spending Infrequent Customers - ~29% of customer base<\/li>\n    <li>Data Visualization shows 619 Medium\/High Spending Active Loyal Customers - ~14% of customer base<\/li>\n    <li>Data Visualization shows 398 Best Customers - ~9% of customer base<\/li>\n    <li>Data Visualization shows 382 customers across Churned\/Inactive Best Customers - ~9% of customer base<\/li>\n    <li>Data Visualization shows 354 Churned Loyal Customer - ~8% of customer base<\/li>\n<\/ul>","d73c432c":"<h3 style=\"color:MediumBlue;text-align:left;\">Build a RFM (Recency Frequency Monetary) model. <\/h3>","9c14df28":"<h5>Inference(s):<\/h5>\n<ul>\n        <li>train dataframe has 379336 rows of information and 8 columns<\/li>\n<\/ul>\n<h4  style=\"color:Navy;\"> Validate First Few Records of Input train dataframe<\/h4>","2ebb955b":"<h5>Inference(s):\n<\/h5>\n<ul>\n    <li>Cluster 3 sample has: \n        <b>Gold, Silver and Bronze customers<\/b>\n    <\/li>\n    <li>RFM Score for this cluster sample varied from value of 5 (Bronze) to 10 (Gold)\n    <\/li>\n    <li>Recency, Frequency and Monetary varied from 1 to 4 considering minimum RFMScore for the sample was 5(below median value)\n    <\/li>\n    <li>Churned customers (Recency), Infrequent customers (Frequency) and Low Spending customers (Monetary) were all spotted in the sample\n    <\/li>\n    <li>Medium\/High Spending Active Loyal Customers (Gold) were all in the cluster 3 sample mix\n    <\/li>\n<\/ul> \n<h4  style=\"color:Navy;text-align:left;\">Sample the Cluster Table data for Cluster 2<\/h4>","754b7c01":"<h5>Inference(s):\n<\/h5>\n<ul>\n    <li>Cluster 0 sample has: \n        <b>Gold and Silver customers<\/b>\n    <\/li>\n    <li>RFM Scores are above the median of 6 with sample values till 10\n    <\/li>\n    <li>Cluster accounts for the frequent customers with all samples above median value of 2 and the recency is also above median value for this cluster while the monetary value of the cluster varies from median value of 2 to above median value of 3.\n    <\/li>\n    <li>Based on sample - this cluster can be targeted with new niche economy products for improving market capitalization \n    <\/li>\n<\/ul>\n<h4  style=\"color:Navy;text-align:left;\">Scatter Plot\n<\/h4>\n<p>These will help us visualize the division of customers into different segments based on the RFM atributes.\n<\/p>","47ee22c3":"<h5>Inference(s):\n<\/h5>\n<ul>\n    <li>frequency Dataframe computed and it has 2 columns CustomerID and Frequency\n    <\/li>\n    <li>Data quality check successful on first 5 rows validation\n    <\/li>\n<\/ul>\n<h4  style=\"color:Navy;\">Data Shape Quality Check on Frequency Dataframe\n<\/h4>    ","58561231":"<h5>Inference(s):\n<\/h5>\n<ul>\n    <li>MonthYear column added to dataTableau dataframe and validated fine while checking the head\n    <\/li>\n<\/ul>\n<h4  style=\"color:Navy;\">Check Info of dataTableau dataframe to validate DType and Missing Values\n<\/h4>","89cd170e":"<h5>Inference(s):\n<\/h5>\n<ul>\n    <li>Churned Low Spending Infrequent Customer segment has 28.7% - highest\n    <\/li>\n    <li>Low-Spending Active Loyal Customer segment has 2.1% - lowest\n    <\/li>\n<\/ul>\n<h4  style=\"color:Navy;\">Random Sample on RFMSegment to check data<\/h4>","96cf6501":"<h5>Inference(s):\n<\/h5>\n<ul>\n    <li>There are about 0.01% items where unit price is 0 (free items)\n    <\/li>\n<\/ul>","9b78bcc3":"<h5>Inference(s):\n<\/h5>\n<ul>\n    <li>Boxplot shows unit price positive distribution with 1 high value (valid outlier)\n    <\/li>\n<\/ul>","c39b7e5e":"<h4 style=\"color:Navy;\">Import Input Datasets<\/h4>","24b2e8b0":"<h5>Inference(s):\n<\/h5>\n<ul>\n    <li>KMeans Model is built with 4 clusters and we have used random state 1\n    <\/li>\n<\/ul>\n<p>KMeans(algorithm='auto', copy_x=True, init='k-means++', max_iter=300,\n       n_clusters=4, n_init=10, n_jobs=None, precompute_distances='auto',\n       random_state=1, tol=0.0001, verbose=0)\n<\/p>\n<h4  style=\"color:Navy;text-align:left;\">Validate if # of entries cluster label array matches RFMSegment dataframe size in terms of rows - should be equal to count of customer IDs\n<\/h4>","04815ec5":"<h5>Inference(s):\n<\/h5>\n<ul>\n    <li>61 unique RFM Segment Values Created based on RFM values\n    <\/li>\n<\/ul>\n <h4 style=\"color:Navy;text-align:left;\">Validate RFMSegment Information\n<\/h4> ","d7058433":"<h4 style=\"color:Navy;text-align:left;\">Plot Recency Quantiles Distribution\n<\/h4>","ab5b1576":"<h2 style=\"color:DodgerBlue;text-align:left;\"> Dataset Description <\/h2>\n<p>This is a transnational data set which contains all the transactions that occurred between 01\/12\/2010 and 09\/12\/2011 for a UK-based and registered non-store online retail. The company mainly sells unique and all-occasion gifts. <\/p>\n\n<h3 style=\"color:MediumBlue;text-align:left;\"> Variables\tDescription <\/h3>\n <table style=\"width:100%;text-align:left;\">\n  <tr style=\"color:Crimson;\">\n    <th style=\"text-align:center;\">Item<\/th>\n      <th style=\"text-align:center;\">Purpose<\/th>\n      <th style=\"text-align:center;\">Type<\/th>\n    <th style=\"text-align:center;\">Metadata Description<\/th>\n  <\/tr>\n  <tr>\n    <td style=\"text-align:left;\">InvoiceNo<\/td>\n      <td style=\"text-align:left;\">Invoice number<\/td>\n      <td style=\"text-align:left;\">Nominal<\/td>\n    <td style=\"text-align:left;\">A six digit integral number uniquely assigned to each transaction. If this code starts with letter 'c', it indicates a cancellation<\/td>\n  <\/tr>\n  <tr>\n    <td style=\"text-align:left;\">StockCode<\/td>\n      <td style=\"text-align:left;\">Product (item) code.<\/td>\n      <td style=\"text-align:left;\">Nominal<\/td>\n    <td style=\"text-align:left;\"> A five digit integral number uniquely assigned to each distinct product<\/td>\n  <\/tr>\n    <tr>\n    <td style=\"text-align:left;\">Description<\/td>\n        <td style=\"text-align:left;\">Product (item) name<\/td>\n        <td style=\"text-align:left;\">Nominal<\/td>\n    <td style=\"text-align:left;\">Product (item) name<\/td>\n  <\/tr>\n    <tr>\n    <td style=\"text-align:left;\">Quantity<\/td>\n        <td style=\"text-align:left;\">The quantities of each product (item) per transaction<\/td>\n        <td style=\"text-align:left;\">Numeric<\/td>\n    <td style=\"text-align:left;\">The quantities of each product (item) per transaction<\/td>\n  <\/tr>\n    <tr>\n    <td style=\"text-align:left;\">InvoiceDate<\/td>\n        <td style=\"text-align:left;\">Invoice Date and time<\/td>\n        <td style=\"text-align:left;\">Numeric<\/td>\n    <td style=\"text-align:left;\">The day and time when each transaction was generated<\/td>\n  <\/tr>\n    <tr>\n    <td style=\"text-align:left;\">UnitPrice<\/td>\n        <td style=\"text-align:left;\">Unit price<\/td>\n        <td style=\"text-align:left;\">Numeric<\/td>\n    <td style=\"text-align:left;\">Product price per unit in sterling<\/td>\n  <\/tr>\n        <tr>\n    <td style=\"text-align:left;\">CustomerID<\/td>\n        <td style=\"text-align:left;\">Customer number<\/td>\n        <td style=\"text-align:left;\">Nominal<\/td>\n    <td style=\"text-align:left;\">A six digit integral number uniquely assigned to each customer<\/td>\n  <\/tr>\n    <tr>\n    <td style=\"text-align:left;\">Country<\/td>\n        <td style=\"text-align:left;\">Country name<\/td>\n        <td style=\"text-align:left;\">Nominal<\/td>\n    <td style=\"text-align:left;\">The name of the country where each customer resides<\/td>\n  <\/tr>\n<\/table>","41b589f3":"<h4  style=\"color:Navy;\"> Bottom 20 Purchases by Invoices<\/h4>","b947f5fd":"<h5>Inference(s):\n<\/h5>\n<ul>\n    <li>Data Visualization shows skewness for Recency, Frequency and Monetary\n    <\/li>\n    <li>Let's check for negative values before doing log transformation.\n    <\/li>\n<\/ul>\n<h4  style=\"color:Navy;text-align:left;\">Check Description of Dataframe RFMModel to identify negative values\n<\/h4>","379478bc":"<h3 style=\"color:MediumBlue;text-align:left;\"> Data Transformation: <\/h3>","30123e8b":"<h3  style=\"color:DarkBlue;\">Handling Inappropriate Data<\/h3>","91ba7df9":"<h5>Inference(s):\n<\/h5>\n<ul>\n    <li>Distribution seems to follow a normal pattern for Frequency and Monetary<\/li>\n    <li>Recency shows mild skewness which we will ignore for the moment<\/li>\n<\/ul>\n<h4  style=\"color:Navy;text-align:left;\">Decide the optimum number of clusters to be formed using Kmeans.\n<\/h4>","d1a5e744":"<h4 style=\"color:Navy;\">Import Bonus Country - Country Code Mapping Dataset for Tableau Visualization<\/h4>","bc502a53":"<h5>Inference(s):<\/h5>\n<ul>\n    <li>RecentPurchaseDate column created and data quality check successful on first 5 rows validation<\/li>\n<\/ul>\n<h4 style=\"color:Navy;text-align:left;\">Compute Recency by Grouping RFMData on CustomerID and Maximum of RecentPurchaseDate<\/h4>","8af34361":"<h5>Inference(s):\n<\/h5>\n<ul>\n    <li>Dataframe refundedInvoices created with 1 row having RefundedInvoices count\n    <\/li>\n<\/ul>\n<h3  style=\"color:MediumBlue;\">Data Cleaning Completed<\/h3>\n<ul>\n    <li>1. Performed preliminary data inspection and data cleaning\n        <ul>\n            <li>a. Checked for missing data and formulated an apt strategy to treat them\n            <\/li>\n            <li>b. Removed duplicate data records\n            <\/li>\n            <li>c. Performed descriptive analytics on the given data\n            <\/li>\n        <\/ul>\n    <\/li>\n<\/ul>","3be8737a":"<h5>Inference(s):\n<\/h5>\n<ul>\n    <li>RFM Segment Dataframe created from RFMModel Dataframe\n    <\/li>\n    <li>RFM Segment Dataframe has 4296 rows and 3 columns which match the RFMModeldataframe\n    <\/li>\n<\/ul>\n<h4 style=\"color:Navy;text-align:left;\">Perform head operation on RFMSegment to check data integrity\n<\/h4>","545e2d42":"<h4 style=\"color:Navy;text-align:left;\">Copy RFMModel dataframe into RFMSegment for RFM Segment Table\n<\/h4>","7166dae8":"<h5>Inference(s):\n<\/h5>\n<ul>\n    <li>Data Dataframe had 4296 unique customers which matches the RFMModel Dataframe size\n    <\/li>\n<\/ul>\n<h4 style=\"color:Navy;text-align:left;\">Set Customer ID as index for RFMModel dataframe\n<\/h4> ","1d8935ef":"<h3 style=\"color:MediumBlue;text-align:left;\"> RFM Analysis <\/h3>\n<p>\n    <b>RFM analysis <\/b>\n    is a customer segmentation technique that uses past purchase behavior to divide customers into groups. RFM helps divide customers into various categories or clusters to identify customers who are more likely to respond to promotions and also for future personalization services.<\/p>\n<table style=\"width:100%;text-align:left;\">\n  <tr style=\"color:Crimson;\">\n    <th style=\"text-align:left;\">Item<\/th>\n      <th style=\"text-align:left;\">Abbreviation<\/th>\n      <th style=\"text-align:left;\">Definition<\/th>\n      <\/tr>\n  <tr>\n    <td style=\"text-align:left;\">Recency<\/td>\n      <td style=\"text-align:left;\">(R)<\/td>\n      <td style=\"text-align:left;\">Days since last customer transaction<\/td>\n      <\/tr>\n  <tr>\n    <td style=\"text-align:left;\">Frequency<\/td>\n      <td style=\"text-align:left;\">(F)<\/td>\n      <td style=\"text-align:left;\">Number of transactions in the last 12 months<\/td>\n    <\/tr>\n    <tr>\n    <td style=\"text-align:left;\">Monetary<\/td>\n      <td style=\"text-align:left;\">(M)<\/td>\n      <td style=\"text-align:left;\">Total spend in the last 12 months<\/td>\n    <\/tr>\n<\/table>    \n\n<h4 style=\"color:Navy;text-align:left;\">Benefits of RFM analysis <\/h4> \n<ul>\n    <li> Increased customer retention <\/li>\n    <li>Increased response rate <\/li>\n    <li>Increased conversion rate <\/li>\n    <li>Increased revenue <\/li>\n<\/ul>\n<p>To perform RFM analysis, we divide customers into four equal groups according to the distribution of values for recency, frequency, and monetary value. Four equal groups across three variables create 64 (4x4x4) different customer segments, which is a manageable number.<\/p>\n\n<p>For example, let\u2019s look at a customer who: <\/p>\n\n<ul>\n    <li> is within the group who purchased most recently (R=4), <\/li>\n    <li>is within the group who purchased most quantity (F=4), <\/li>\n    <li>is within the group who spent the most (M=4) <\/li>\n    <li>Increased revenue <\/li>\n<\/ul>\n\n<p>This customer belongs to RFM segment **4-4-4 (Best Customer)**, (R=4, F=4, M=4) <\/p>","4c51f424":"<h5>Inference(s):\n<\/h5>\n<ul>\n    <li>Boxplot shows unit price positive distribution with 1 high value (outlier)\n    <\/li>\n<\/ul>\n<h4  style=\"color:Navy;\">Perform IQR Test for Outliers on UnitPrice<\/h4>","3e84132f":"<h4 style=\"color:Navy;\">Import Libraries<\/h4>","99d65212":"<h5>Inference(s):\n<\/h5>\n<ul>\n    <li>Distribution RFM sub plot to show the distribution across first 3 quantile splits\n    <\/li>\n    <li>Top quantile not plotted as it had outliers and plot would have miniscule values for first 3 quantiles\n    <\/li>\n<\/ul>","112630f8":"<h5>Inference(s):\n<\/h5>\n<ul>\n    <li>2 percent negative quantity items removed\n    <\/li>\n    <li>2 Positive outlier quantities > 20000 which were part of refunds are removed\n    <\/li>\n<\/ul>","eab0abf9":"<h4  style=\"color:Navy;text-align:left;\">Snake Plot\n<\/h4>\n<ul>\n    <li>Market research technique to compare different segments\n    <\/li>\n    <li>Visual representation of each segment's attributes\n    <\/li>\n    <li>>Need to first normalize data (center & scale)\n    <\/li>\n    <li>Plot each cluster's average normalized values of each attribute\n    <\/li>\n<\/ul>","96fdb2c2":"<h4  style=\"color:Navy;\">Transaction Spread by Year<\/h4>","7083a3f0":"<h5>Inference(s):\n<\/h5>\n<ul>\n    <li>ClusterData Plot Summary : # of Records in Four created clusters by KMeans algorithm are\n        <ul>\n            <li>Cluster 0: \n                <b>838<\/b>\n            <\/li>\n            <li>Cluster 1: \n                <b>1206<\/b>\n            <\/li>\n            <li>Cluster 2: \n                <b>963<\/b>\n            <\/li>\n            <li>Cluster 3: \n                <b>1289<\/b>\n            <\/li>\n        <\/ul>\n    <\/li>\n<\/ul>","38cbb1bc":"<h5>Inference(s):<\/h5>\n<ul>\n    <li>USA ranked lowest on Mean Purchases:  ~6 \u00a3 per transaction\n    <\/li>\n    <li>Netherlands ranked highest on Mean Purchases : greater than 120 \u00a3 per transaction\n    <\/li>\n<\/ul>","826eba2a":"<h4 style=\"color:Navy;text-align:left;\">Check Input Data Spread until and after December 2011<\/h4>","bcb7095a":"<h4  style=\"color:Navy;text-align:left;\">Analyze these clusters and comment on the results.\n<\/h4>\n<h4  style=\"color:Navy;text-align:left;\">Sample the Cluster Table data for Cluster 3\n<\/h4>","e5408174":"<h5>Inference(s):<\/h5>\n<ul>\n    <li>cohortGroup summarization created on CohortMonth and CohortIndex<\/li>\n<\/ul>\n<h4  style=\"color:Navy;text-align:left;\">Compute Retention Rate<\/h4>","a4428298":"<h4  style=\"color:Navy;\">Validate Quantity Distribution after removing inappropriate data<\/h4>","7b8c014c":"<h5>Inference(s):<\/h5>\n<ul>\n    <li>\n        <b>No<\/b> \n        missing values found in data dataframe for any of the \n        <b>8<\/b>\n         Columns\n    <\/li>\n<\/ul>\n<h4  style=\"color:Navy;\"> Change the case of column Description from Upper to Title\/Proper for better readability<\/h4>","cd32ad67":"<h5>Inference(s):\n<\/h5>\n<ul>\n    <li>Cluster 2 sample has: \n        <b>Gold and Platinum customers<\/b>\n        - Elite Cluster\n    <\/li>\n    <li>The RFM scores are all 10 and higher in the sampler and this cluster would account for maximum monetary Sales and also account for most frequent and recent transactions\n    <\/li>\n    <li>Inactive and Churned Best Customers also made the sample mix under Gold Category\n    <\/li>\n<\/ul>\n<h4  style=\"color:Navy;text-align:left;\">Sample the Cluster Table data for Cluster 1\n<\/h4>","6b6eac61":"<h4  style=\"color:Navy;text-align:left;\"> Cohort Monthly Customer Count Heatmap\n<\/h4>\n<ul>\n    <li> Easiest way to visualize cohort analysis\n    <\/li>\n    <li> Includes both data and visuals\n    <\/li>\n<\/ul>","0522211b":"<h5>Inference(s):<\/h5>\n<ul>\n    <li>RFMData dataframe has 375665 rows of data spread across 9 columns which matches the data dataframe<\/li>\n<\/ul>\n<h4 style=\"color:Navy;\">Perform head operation on RFMData to check data integrity<\/h4>","6ac35769":"<h5>Inference(s):<\/h5>\n<ul>\n    <li>Most Procured Item :\n        <b> White Hanging Heart T-Light Holder<\/b>\n    <\/li>\n<\/ul>","074c275c":"<h4 style=\"color:Navy;text-align:left;\">Tableau Visualization\n<\/h4>\n<a href=\"https:\/\/public.tableau.com\/views\/OnlineRetailRFMAnalysis_RishikeshSreedhar\/SalesDash?:language=en&:display_count=y&publish=yes&:origin=viz_share_link\">Tableau Visualization Dashboards\n<\/a>\n","6ff09b48":"<h5>Inference(s):\n<\/h5>\n<ul>\n    <li>Centroids and distribution seem even for RF and RM Plots\n    <\/li>\n    <li>RF and RM Plot has best values for cluster 2 and the worst values for cluster 1\n    <\/li>\n    <li>RF and RM Plot has lower recency values for cluster 3 than cluster 2\n    <\/li>\n    <li>RF and RM Plot has lower frequency values for cluster 0 than cluster 2\n    <\/li>\n    <li>Centroids for cluster 0 and 3 are closer for FM Plot and distribution seem well distributed except for segments 0 and 3 which overlap\n    <\/li>\n    <li>FM Plot has lowest values for cluster 1, medium for clusters 0 and 3 which overlap and high values for cluster 2\n    <\/li>\n<\/ul>","48925e06":"<h5>Inference(s):\n<\/h5>\n<ul>\n    <li>Region column added to dataTableau dataframe and validated fine while checking the head\n    <\/li>\n<\/ul>\n<h4  style=\"color:Navy;\">Add MonthYear column based on InvoiceDate to filter for Visualization - Tableau gives MonthYear in long format and take space - hence export in short form from Python\n<\/h4>","f2599bd9":"<h5>Inference(s):<\/h5>\n<ul>\n    <li>Saudi Arabia ranked lowest on Overall Purchases\n    <\/li>\n    <li>UK ranked highest on Overall Purchases :\n        <b> greater than 6.75 Million \u00a3<\/b>\n    <\/li>\n<\/ul>","9293827e":"<h5>Inference(s):\n<\/h5>\n<ul>\n    <li>frequency dataframe has 2 columns with 4296 rows - 1 pertaining to each customer ID\n    <\/li>\n    <li>Data Integrity shape test successful on frequency dataframe\n    <\/li>\n<\/ul>","67ef8275":"<h3  style=\"color:MediumBlue;\">Data Transformation Completed<\/h3>\n<ul>\n    <li>Performed cohort analysis (a cohort is a group of subjects that share a defining characteristic). Observeed how a cohort behaves across time and compare it to other cohorts.\n        <ul>\n            <li>a. Created month cohorts and analyze active customers for each cohort.<\/li>\n        <li>b. Analyze the retention rate of customers.<\/li>\n        <\/ul>\n    <\/li>\n<\/ul>","145d47b9":"<h5>Inference(s):<\/h5>\n    <li>data Dataframe has <b>401604<\/b> records retained with <b>8<\/b> Columns<\/li>\n<h4 style=\"color:Navy;\">Recheck the missing values information on data dataframe<\/h4>    ","2335ba18":"<h3 style=\"color:MediumBlue;text-align:left;\">Data Modeling : <\/h3>\n<ul>\n    <li>1. Build a RFM (Recency Frequency Monetary) model. Recency means the number of days since a customer made the last purchase. Frequency is the number of purchase in a given period. It could be 3 months, 6 months or 1 year. Monetary is the total amount of money a customer spent in that given period. Therefore, big spenders will be differentiated among other customers such as MVP (Minimum Viable Product) or VIP.<\/li>\n    <li>2. Calculate RFM metrics.<\/li>\n    <li>3. Build RFM Segments. Give recency, frequency, and monetary scores individually by dividing them into quartiles.\n        <ul>    \n            <li>a.Combine three ratings to get a RFM segment (as strings).<\/li>\n            <li>b.Get the RFM score by adding up the three ratings.<\/li>\n            <li>c.Analyze the RFM segments by summarizing them and comment on the findings.<\/li>\n        <\/ul>\n    <\/li>\n<\/ul>","6a143868":"<h1 style=\"color:SteelBlue;text-align:center;\">Retail Capstone Project<\/h1>","d7044b5c":"<h4  style=\"color:Navy;\"> Records with UnitPrice Less than 0 are removed\n<\/h4>","14a13b01":"<h5> Inference(s) based on WCSS Elbow Graph\n<\/h5>\n<ul>\n    <li>Elbow graph WCSS Line starts smoothening from cluster 4<\/li>\n    <li>Lets validate with Silhouette score -  a measure of how similar an object is to its own cluster (cohesion) compared to other clusters (separation). The silhouette ranges from \u22121 to +1, where a high value indicates that the object is well matched to its own cluster and poorly matched to neighboring clusters.\n    <\/li>\n<\/ul>","3f5b52a2":"<h5>Inference(s):<\/h5>\n<ul>\n    <li>Snakeplot executed to identify RFM attributes for each cluster and will for basis for market capitalization by executing specific focused campaigns for the cluster to improve Sales and loyalty\n    <\/li>\n    <li>\n        <b>Cluster 0\n        <\/b>\n        <ul>\n            <li>Cluster 0 has negative values closer to 0 on F and M and negative values farther away on R from 0 implying recency above median with the other 2 values varying from 1 to 4.\n            <\/li>\n            <li>It has a small population of low spending customers with recency 3 who are under bronze category with RFM score less than 6, major share of silver customers with good mix of active and loyal customers who have various levels of frequency and monetary and a gold share of gold customers with high recency loyal customers. \n            <\/li>\n            <li>Strategy for this cluster is to lure customers with new economy products with discounts and good recommendations to keep the transaction count high and plan for loyalty programs.\n            <\/li>\n            <li>Exciting low spend category who have shopped recently\n            <\/li>\n        <\/ul>\n    <\/li>\n    <li>\n        <b>Cluster 1<\/b>\n        <ul>\n            <li>Cluster 1 has negative values significantly further away from 0 for F, further away from 0 for M and high positive values for R implying all these 3 values are below the median values for R, F and M mostly. \n            <\/li>\n            <li>This is understandable as this cluster has mostly customers with RFM scores below median score of 6 and a few just over the median. Most of these customers in this cluster are churned low spending customers, churned loyal customers, churned medium spenders and inactive not so frequent low spending customers. \n            <\/li>\n            <li>It also has a few new customers who are spending lower and have lower frequency. \n            <\/li>\n            <li>This is the cluster which need be prioritized for campaigns as a tactical approach as the monetary capitalization and the RFM analysis show, this cluster can have minimum campaigns - loyalty program or encouraging spend here may not get much traction.\n            <\/li>\n        <\/ul>\n    <\/li>\n    <li>\n        <b>Cluster 2<\/b>\n        <ul>\n            <li>Cluster 2 has high negative values for R and high positive values for F and highest positive values for M implying this is the cluster with high RFM and caters to all platinum and gold with maximum sales, most recent and most frequent.\n            <\/li>\n            <li>This cluster accounts for more than 6 million sales in the 1 year and for this cluster - the customers need to be marketed with new niche gift premium products which appeal to the sensibilities to ensure consistent monetary capitalization.\n            <\/li>\n            <li>No price discounts need to be planned for this cluster campaign.\n            <\/li>\n        <\/ul>\n    <\/li>\n    <li>\n        <b>Cluster 3<\/b>\n        <ul>\n           <li>Cluster 3 has positive values closer to 0 for F and M and positive values slightly further from 0 for R indicating this is the cluster with RFM scores above median and has a combination of Bronze, Silver and Gold and is our target cluster to improve Sales and Loyalty.\n            <\/li>\n            <li>This cluster needs to be looked at closely and more than 1 strategy devised to improve frequency where applicable, improve spend for new customers and target churned loyal customers and churned medium to high spenders.\n            <\/li>\n            <li>Exciting sector for marketing as it has a plethora of opportunities and needs a comprehensive SWOT analysis to improve capitalization and see how these customers can be moved up the value chain\n            <\/li>   \n        <\/ul>\n    <\/li>\n<\/ul>","c52319d2":"<h5>Inference(s):<\/h5>\n<ul>\n    <li>First 5 rows of RFMData checked and passed data quality check<\/li>\n<\/ul>","699d6b0c":"<h5>Inference(s):<\/h5>\n<ul>\n    <li>Retention percentage computed across Months for cohorts<\/li>\n<\/ul>\n<h4  style=\"color:Navy;\">Create Heatmap Customer Retention Rate Spread by Cohort Month in Percentage (%)<\/h4>    ","d1da4733":"<h5>Inference(s):\n<\/h5>\n<ul>\n    <li>Cohort Index generates fine and head data validated fine\n    <\/li>\n<\/ul>","5933d79a":"<h5>Inference(s)<\/h5>\n<ul>\n    <li><b>25.1%<\/b> records have <b>Customer ID<\/b> Missing<\/li>\n    <li>Apt strategy is to <b>remove<\/b> these records <\/li>\n<\/ul>\n<h4 style=\"color:Navy;\">Retain only records where CustomerID is populated and drop the remaining 25%<\/h4>    ","bfcf333d":"<h4  style=\"color:Navy;\"> Top 20 Purchases by Invoices<\/h4>","fd418d68":"<h5>Inference(s):<\/h5>\n<ul>\n    <li>We infer that the top 2 invoices were cancelled and refunded\n    <\/li>\n<\/ul>","c2f83215":"<h4 style=\"color:Crimson;text-align:left;\">Plot Frequency Quantiles Distribution<\/h4>","039d3935":"<h4  style=\"color:Navy;text-align:left;\"> Elements of cohort analysis<\/h4>\n<ul>\n    <li>Pivot table\n    <\/li>\n    <li>Assigned cohort in rows\n    <\/li>\n    <li>Cohort Index in columns\n    <\/li>\n    <li>Metrics in the table\n    <\/li>\n<\/ul>","d6744ef2":"<h3  style=\"color:MediumBlue;text-align:left;\">Create month cohorts and analyze active customers for each cohort<\/h3>\n<h4  style=\"color:Navy;text-align:left;\"> Monthly Cohort Assignment:<\/h4>\n\n<p>Setting up a cohort is the pre cursor to perform cohort analysis. Based on 12 month data provided - assigning customers to cohorts based on date of first purchase based on data provided for performig cohort analysis<\/p>"}}