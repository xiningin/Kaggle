{"cell_type":{"4413b726":"code","c2e53737":"code","960d7085":"code","f7cf2172":"code","ff8247cb":"code","4133d95e":"code","d67840c7":"code","14f322cd":"code","2b8d76a5":"code","7bc83f56":"code","ccb3960e":"code","6ea3580e":"code","e73cfb0a":"code","50f2c158":"code","47c3a63a":"code","006ddd16":"code","e4e13d6d":"code","144d5554":"code","ea16b3f3":"code","b1f6e40b":"code","15572d31":"code","ecb57ec7":"code","07ca31f9":"code","25490e15":"code","d58c3d47":"code","063f0d81":"code","b383cd47":"code","0ae20c56":"code","6a1106f4":"code","0bab038c":"code","8986e9a4":"code","d8cedb2f":"code","40cca0ee":"code","811f8d0d":"code","8c2e37a8":"code","1edff218":"code","1a47bc22":"code","b7f1a62e":"code","9091f076":"code","4fc8794c":"code","0a54094b":"code","e0ae1e8a":"code","c4f073ae":"code","dfc449c6":"code","9119a470":"code","b212b924":"code","bced7135":"code","246da59b":"code","83a09ec8":"code","ba9f209b":"code","6e38429f":"code","986f08f6":"code","678a32c6":"code","9629fad4":"code","e6cbbd67":"code","ad9305b8":"code","9c60ae0c":"code","e7350993":"code","96194993":"code","f9f7980c":"code","53c76169":"code","192719a4":"code","a2cceb85":"code","fea72e98":"code","927e0b91":"code","a72172d0":"code","882e624b":"code","ed27c0ba":"code","c5f0f670":"code","37f3b113":"code","3ce06754":"code","2f074101":"code","fd594b02":"code","4742d4fe":"code","de2bed5d":"code","b8d568bf":"code","e12ea9da":"code","7b818436":"code","e07b952c":"code","b560b671":"code","eb35591b":"code","83531377":"code","8f983ad8":"code","0f75f97c":"code","8bb8a135":"code","24b139cd":"code","7838c54e":"markdown","2f6b082b":"markdown","30f6bf4b":"markdown","0627aef0":"markdown","54599824":"markdown","c29c144f":"markdown","c270befd":"markdown","4c55242c":"markdown","b932980b":"markdown","2e43d342":"markdown","fe4cf2b4":"markdown","40e35b83":"markdown","51f1fe1f":"markdown","09762aa1":"markdown","8127deec":"markdown","45b45228":"markdown","27d88ea7":"markdown","245a87a8":"markdown","db491804":"markdown","dd7272b1":"markdown","f500d7ae":"markdown","8e087c37":"markdown","8a19860b":"markdown","87825dcc":"markdown","9917be37":"markdown","d55d9d15":"markdown","0bcc25cc":"markdown","d1f86aca":"markdown","c9b19134":"markdown","dc8d69e1":"markdown","41e411b7":"markdown","e9703c96":"markdown","15930d6d":"markdown","9084719d":"markdown","a38b68ee":"markdown","a8880e92":"markdown","97729bb7":"markdown","b35cf86e":"markdown","f2fb5e74":"markdown","83633df5":"markdown","7b0b7b7b":"markdown","44c1811d":"markdown","3365ce2b":"markdown","7d23b602":"markdown","b73c9bba":"markdown","1089fb38":"markdown","0404afb9":"markdown","c783779a":"markdown","991bfb03":"markdown","677615c5":"markdown","35fb2afc":"markdown","1228f9e9":"markdown","8c7df883":"markdown","21c39971":"markdown","2648616e":"markdown","dd07cfd1":"markdown","008ff812":"markdown","4afd979e":"markdown","cd07d970":"markdown","8a795100":"markdown","90971a1b":"markdown","81597970":"markdown","16548600":"markdown","17e6f8c8":"markdown","04449651":"markdown","55437ac3":"markdown"},"source":{"4413b726":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","c2e53737":"data=pd.read_csv(\"\/kaggle\/input\/automobile-dataset\/Automobile_data.csv\")","960d7085":"data.head()","f7cf2172":"data.describe()","ff8247cb":"data.price.value_counts()","4133d95e":"data[data.price==\"?\"]","d67840c7":"#drop by rows\ndata.drop([9,44,45,129],inplace=True)\ndata.head(10)","14f322cd":"data.price=data.price.astype(\"int64\")","2b8d76a5":"data.info()","7bc83f56":"data.price.describe()","ccb3960e":"data.price.hist()","6ea3580e":"data.price.sort_values(ascending=False).head()","e73cfb0a":"data.loc[data[\"price\"]>16500,\"target\"]=\">16500\"\ndata.loc[data[\"price\"]<=16500,\"target\"]=\"<=16500\"\ndata[\"target\"].value_counts()","50f2c158":"import matplotlib.pyplot as plt\ndata[\"target\"].value_counts().plot(kind = \"bar\",color = \"orange\")\nplt.show()","47c3a63a":"data.isnull().any()","006ddd16":"data[\"num-of-doors\"].value_counts()","e4e13d6d":"data[data[\"make\"]==\"dodge\"]","144d5554":"#Splitting variables in numerical and objects\ndata.select_dtypes(include=object).columns,data.select_dtypes(include=np.number).columns","ea16b3f3":"categorical_columns = ['normalized-losses', 'make', 'fuel-type', 'aspiration', 'num-of-doors',\n        'body-style', 'drive-wheels', 'engine-location', 'engine-type',\n        'num-of-cylinders', 'fuel-system', 'bore', 'stroke', 'horsepower',\n        'peak-rpm', 'target'\n    ]\nnumerical_columns = ['symboling', 'wheel-base', 'length', 'width', 'height', 'curb-weight',\n        'engine-size', 'compression-ratio', 'city-mpg', 'highway-mpg', 'price']\nall_columns = numerical_columns + categorical_columns + [\n    \"target\"]","b1f6e40b":"print(f\"The dataset contains {data.shape[0]} samples and \"\n      f\"{data.shape[1]} columns\")\nprint(f\"The dataset contains {data.shape[1] - 1} features.\")","15572d31":"print(f\"The dataset contains {data.shape[1] - 1} features.\")","ecb57ec7":"data[\"normalized-losses\"].value_counts()","07ca31f9":"data.make.value_counts()","25490e15":"data[\"fuel-type\"].value_counts()","d58c3d47":"data.aspiration.value_counts()","063f0d81":"data[\"num-of-doors\"].value_counts()","b383cd47":"data[\"body-style\"].value_counts()","0ae20c56":"data[\"drive-wheels\"].value_counts()","6a1106f4":"data[\"engine-location\"].value_counts()","0bab038c":"data[\"engine-type\"].value_counts()","8986e9a4":"data[\"num-of-cylinders\"].value_counts()","d8cedb2f":"data[\"fuel-system\"].value_counts()","40cca0ee":"data[\"bore\"].value_counts()","811f8d0d":"data[\"stroke\"].value_counts()","8c2e37a8":"data[\"horsepower\"].value_counts()","1edff218":"data[\"peak-rpm\"].value_counts()","1a47bc22":"data[\"num-of-doors\"]=data[\"num-of-doors\"].replace(\"four\",4)\ndata[\"num-of-doors\"]=data[\"num-of-doors\"].replace(\"two\",2)\ndata[\"num-of-doors\"].value_counts()\n","b7f1a62e":"#We can see that between the most frequents value for this variable we have the \"?\"\ndata[\"normalized-losses\"].value_counts().head()","9091f076":"from numpy import nan\ndata[\"normalized-losses\"]=data[\"normalized-losses\"].replace(\"?\",np.nan)\n\n#now we can check the first 5 rows\ndata.head()","4fc8794c":"data.fillna(data.median(),inplace=True)","0a54094b":"data.head()","e0ae1e8a":"data.dtypes.head()","c4f073ae":"#Conversion of a variable from object in float type\ndata['normalized-losses']=pd.to_numeric(data['normalized-losses'],downcast=\"float\")\n\n\n#if you prefer you can convert from float32 to float64\ndata['normalized-losses']=data['normalized-losses'].astype(\"float64\")\n\n#We can now control \ndata.dtypes.head()","dfc449c6":"var_to_convert=[\"normalized-losses\",\"num-of-doors\",\"bore\",\"stroke\",\"horsepower\",\"peak-rpm\"]\nfor i in var_to_convert:\n    data[i]=data[i].replace(\"?\",np.nan)\n    data.fillna(data.median(),inplace=True)\n    data[i]=pd.to_numeric(data[i],downcast=\"float\")\n    data[i]=data[i].astype(\"float64\")\n    \n    \n    ","9119a470":"data.info()","b212b924":"#Splitting variables in numerical and objects\ndata.select_dtypes(include=object).columns,data.select_dtypes(include=np.number).columns","bced7135":"categorical_columns = ['make', 'fuel-type', 'aspiration', 'body-style', 'drive-wheels',\n        'engine-location', 'engine-type', 'num-of-cylinders', 'fuel-system']    \nnumerical_columns = ['symboling', 'normalized-losses', 'num-of-doors', 'wheel-base',\n        'length', 'width', 'height', 'curb-weight', 'engine-size', 'bore',\n        'stroke', 'compression-ratio', 'horsepower', 'peak-rpm', 'city-mpg',\n        'highway-mpg', 'price']","246da59b":"import seaborn as sns\n\n# Outlier detection using boxplots \nplt.figure(figsize= (20,15))\n\nplt.subplot(4,4,1)\nsns.boxplot(data['normalized-losses'])\n\nplt.subplot(4,4,2)\nsns.boxplot(data['symboling'])\n\nplt.subplot(4,4,3)\nsns.boxplot(data['wheel-base'])\n\nplt.subplot(4,4,4)\nsns.boxplot(data['length'])\n\nplt.subplot(4,4,5)\nsns.boxplot(data['width'])\n\nplt.subplot(4,4,6)\nsns.boxplot(data['curb-weight'])\n\nplt.subplot(4,4,7)\nsns.boxplot(data['engine-size'])\n\nplt.subplot(4,4,8)\nsns.boxplot(data['city-mpg'])\n\nplt.subplot(4,4,9)\nsns.boxplot(data['highway-mpg'])\n\nplt.subplot(4,4,10)\nsns.boxplot(data['height'])\n\nplt.subplot(4,4,11)\nsns.boxplot(data['compression-ratio'])\n\nplt.subplot(4,4,12)\nsns.boxplot(data['bore'])\n\nplt.subplot(4,4,13)\nsns.boxplot(data['stroke'])\n\nplt.subplot(4,4,14)\nsns.boxplot(data['horsepower'])\n\nplt.subplot(4,4,15)\nsns.boxplot(data['peak-rpm'])\n\nplt.subplot(4,4,16)\nsns.boxplot(data['num-of-doors'])","83a09ec8":"data1=data.copy()\n#Substitution for the variable normalized-lossses.\ndata1['normalized-losses']=data1['normalized-losses'].clip(lower=data1['normalized-losses'].quantile(0.05), upper=data1['normalized-losses'].quantile(0.95))","ba9f209b":"#We can compare the difference in the distributions before and after the removing of outliers\nplt.figure(figsize= (10,10))\n\n#Now we don't have anymore outliers for normalized losses\nplt.subplot(2,1,1)\nsns.boxplot(data['normalized-losses'])\n\nplt.subplot(2,1,2)\nsns.boxplot(data1['normalized-losses'])","6e38429f":"plt.figure(figsize=(8,5))\nsns.countplot(data[\"target\"])\nplt.show()","986f08f6":"for i in categorical_columns:      \n    plt.figure(figsize=(10,7))\n    sns.countplot(data[i])\n    plt.show()","678a32c6":"plt.figure(figsize=(10,7))\nax=sns.countplot(data[\"make\"])\nax.set_xticklabels(\n    ax.get_xticklabels(),\n    rotation=45,\n    horizontalalignment='right'\n)\nplt.show()","9629fad4":"_ =data.hist(figsize=(20,12))","e6cbbd67":"sns.set_style(\"white\")\nsns.displot(x=\"horsepower\",data=data)","ad9305b8":"sns.displot(x=\"horsepower\",data=data, kde=True)\nplt.show()","9c60ae0c":"sns.displot(x=\"horsepower\",data=data, kind =\"kde\")","e7350993":"# now we can have a better idea of which variable is a distribution bell shaped\nfor i in numerical_columns:\n    plt.figure(figsize=(10,7))\n    sns.displot(data[i],kde=True)\n    plt.show()\n    \n    ","96194993":"sns.displot(x=\"horsepower\",data=data, kind =\"ecdf\")\nplt.show()","f9f7980c":"pd.crosstab(columns=data['target'],\n            index=data['make'])","53c76169":"data.info()","192719a4":"#Choose a subgroup of quantitative variables and see graphically their relations\n\nimport seaborn as sns\ntarget=data[\"target\"]\ncolumns = ['num-of-doors', 'horsepower', 'engine-size','price']\n_ = sns.pairplot(data=data,vars= columns, hue= \"target\")\n","a2cceb85":"sns.pairplot(data,diag_kind=\"kde\")\nplt.tight_layout()","fea72e98":"sns.displot(x='horsepower', data=data, hue='fuel-type');","927e0b91":"sns.displot(x='horsepower', data=data, hue='target')\nplt.show()","a72172d0":"sns.displot(x='horsepower', data=data, kind = \"kde\",hue='target')\nplt.show()","882e624b":"#pairplot_figure = sns.pairplot(data, hue=\"target\")","ed27c0ba":"data.corr()","c5f0f670":"#To have a specific glance for the variable price\ndata.corr()[16:17].T\n","37f3b113":"#Remember!!! The stronger the color, the larger the correlation magnitude.\n\nplt.figure(figsize=(13,8))\nax=sns.heatmap(data.corr(),\n            cmap=sns.diverging_palette(20, 220, n=200),\n    square=True)\n\nax.set_xticklabels(\n    ax.get_xticklabels(),\n    rotation=45,\n    horizontalalignment='right'\n)\nplt.show()","3ce06754":"_ = sns.scatterplot(\n    x=\"horsepower\", y=\"length\", data=data,\n    hue=\"target\", alpha=0.5,\n)","2f074101":"plt.figure(figsize=(10,7))\n_ = sns.scatterplot(\n    x=\"horsepower\", y=\"length\", data=data,\n    hue=\"target\", alpha=0.5,\n)\n\nhp_limit = 94\nplt.axvline(x=hp_limit, ymin=0, ymax=1, color=\"black\", linestyle=\"--\")\n\nlength_limit = 188\nplt.axhline(\n    y=length_limit, xmin=0, xmax=1, color=\"black\", linestyle=\"--\"\n)\n\nplt.annotate(\"<=16500\", (70, 150), rotation=90, fontsize=35)\nplt.annotate(\"Uncertain\", (125, 160), fontsize=35)\n_ = plt.annotate(\">16500\", (120, 195), fontsize=35)\nplt.title(\"Decision boundary at hand\")\nplt.show()","fd594b02":"#Example of correlation matrix sized\n\ndef heatmap(x, y, size):\n    fig, ax = plt.subplots()\n    \n    # Mapping from column names to integer coordinates\n    x_labels = [v for v in sorted(x.unique())]\n    y_labels = [v for v in sorted(y.unique())]\n    x_to_num = {p[1]:p[0] for p in enumerate(x_labels)} \n    y_to_num = {p[1]:p[0] for p in enumerate(y_labels)} \n    \n    size_scale = 500\n    ax.scatter(\n        x=x.map(x_to_num), # Use mapping for x\n        y=y.map(y_to_num), # Use mapping for y\n        s=size * size_scale, # Vector of square sizes, proportional to size parameter\n        marker='s' # Use square as scatterplot marker\n    )\n    # Show column labels on the axes\n    ax.set_xticks([x_to_num[v] for v in x_labels])\n    ax.set_xticklabels(x_labels, rotation=45, horizontalalignment='right')\n    ax.set_yticks([y_to_num[v] for v in y_labels])\n    ax.set_yticklabels(y_labels)\n    x_labels = [v for v in sorted(x.unique())]\n    y_labels = [v for v in sorted(y.unique())]\n    x_to_num = {p[1]:p[0] for p in enumerate(x_labels)} \n    y_to_num = {p[1]:p[0] for p in enumerate(y_labels)} \n    ax.set_xlim([-0.5, max([v for v in x_to_num.values()]) + 0.5]) \n    ax.set_ylim([-0.5, max([v for v in y_to_num.values()]) + 0.5])\n\n    \ndata = data\ncolumns = ['bore', 'stroke', 'compression-ratio', 'horsepower', 'city-mpg', 'price'] \ncorr = data[columns].corr()\ncorr = pd.melt(corr.reset_index(), id_vars='index') # Unpivot the dataframe, so we can get pair of arrays for x and y\ncorr.columns = ['x', 'y', 'value']\nheatmap(\n    x=corr['x'],\n    y=corr['y'],\n    size=corr['value'].abs()\n)\nax.grid(False, 'major')\nax.grid(True, 'minor')\n  \nplt.show()\n\n\n\n","4742d4fe":"#This are the values from we create the matrix\ncorr","de2bed5d":"!pip install heatmapz","b8d568bf":"# Import the two methods from heatmap library\nfrom heatmap import heatmap, corrplot\n\nplt.figure(figsize=(8, 8))\ncorrplot(data.corr(), size_scale=300);","e12ea9da":"\n\n#we have to drop both the variables price and target because are the same thing expressed in different way.\nX = data.drop([\"target\",\"price\"], axis= 1)\ny = data[\"target\"].values.reshape(-1,1)\nfrom sklearn.model_selection import train_test_split\n(x_train, x_test, y_train, y_test) = train_test_split(X, y, test_size= 0.3, random_state= 42)\nprint(\"Shape of train_X: \", x_train.shape)\nprint(\"Shape of test_X: \", x_test.shape)","7b818436":"for i in categorical_columns:\n    print(i)\n    print(data[i].nunique())","e07b952c":"\nfeatures=[ 'fuel-type', 'aspiration','wheel-base', 'length', 'width', 'curb-weight','engine-size', 'bore','horsepower', 'city-mpg',\n       'highway-mpg']\nx_train= pd.get_dummies(x_train[features])\nx_test = pd.get_dummies(x_test[features])","b560b671":"from sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import confusion_matrix, precision_score, recall_score, f1_score, confusion_matrix, precision_recall_curve, accuracy_score\ndef metrics(actuals, predictions):\n    print(\"Accuracy: {:.5f}\".format(accuracy_score(actuals, predictions)))\n    print(\"Precision: {:.5f}\".format(precision_score(actuals, predictions,pos_label=\">16500\")))\n    print(\"Recall: {:.5f}\".format(recall_score(actuals, predictions,pos_label=\">16500\")))\n    print(\"F1-score: {:.5f}\".format(f1_score(actuals, predictions,pos_label=\">16500\")))","eb35591b":"rf = RandomForestClassifier()\nrf.fit(x_train, y_train)\n\n\ny_pred = rf.predict(x_test)\nrf_cnf_matrix = confusion_matrix(y_test, y_pred)\nprint('Evaluation of Random Forest')\nprint(rf_cnf_matrix)\nmetrics(y_test, y_pred)\nrf_f1_score = f1_score(y_test, y_pred,pos_label=\">16500\")\nsns.heatmap(pd.DataFrame(rf_cnf_matrix), annot=True, cmap=\"YlGnBu\", fmt='g')\nplt.ylabel('Actual Label')\nplt.xlabel('Predicted Label')\n","83531377":"from sklearn import set_config\nset_config(display='diagram')","8f983ad8":"from sklearn.neighbors import KNeighborsClassifier\n\nknn = KNeighborsClassifier()\nknn.fit(x_train, y_train)","0f75f97c":"y_pred = knn.predict(x_test)\nknn_cnf_matrix = confusion_matrix(y_test, y_pred)\nprint('Evaluation of Random Forest')\nprint(knn_cnf_matrix)\nmetrics(y_test, y_pred)\nrf_f1_score = f1_score(y_test, y_pred,pos_label=\">16500\")\nsns.heatmap(pd.DataFrame(knn_cnf_matrix), annot=True, cmap=\"YlGnBu\", fmt='g')\nplt.ylabel('Actual Label')\nplt.xlabel('Predicted Label')","8bb8a135":"from sklearn.linear_model import LinearRegression\nlr=LinearRegression()","24b139cd":"\n(x_train, x_test, y_train, y_test) = train_test_split(X, y, test_size= 0.3, random_state= 42)","7838c54e":"And now we can split again the variables in the corrected way:","2f6b082b":"*Categorical Variables*\n----\n\nFor categorical variables the most intuitive things to do is using the value_counts function to see how they are distributed.","30f6bf4b":"Correlation Matrix\n----\n\nWe can see the Correlation matrix to see the correlation between variables. Remember that it can vary from -1 ( maximum negative correlation) to 1 (maximum positive correlation). When we have a matrix with all numbers near to 0 we can say that the features are not correlated. If two or more variables are values far from 0 we will have a multicollinearity problem and we can mind to cut some variables from the forecasting model.\n\nTo know more about Correlation Matrix it let's see this intersting article:\n\nhttps:\/\/towardsdatascience.com\/better-heatmaps-and-correlation-matrix-plots-in-python-41445d0f2bec\n","0627aef0":"The Eda is not still finished...we can scale the values for numeric variables if we want to predict the values with some models but we will do it later. For the classification task with tree bases methods is not necessary for the moment but we have to use it for the regression tasks.","54599824":"<h1 style='background:#E8DCCC; border:0; color:#CF673A'><center>Now we can say that we solve the missing problem!!!<\/center><\/h1> ","c29c144f":"We can do the same with all the other variables changing the upper and lower quantile.","c270befd":"I have to transform the variable price in quantitative but before i have to drop the value equal to '?'","4c55242c":"Machine Learning Predictions\n===","b932980b":"<h1 style='background:lightblue; border:0; color:#Black'><center>Data Visualization<\/center><\/h1> \n\n\nLet's start from the Target Variable.\n\nWe have to note that the dataset is quite unbalanced","2e43d342":"First glance to the dataset.","fe4cf2b4":"We can improve the details on the first graph using the rotation function to read better the names of the different makes.","40e35b83":"<h1 style='background:lightblue; border:0; color:#Black'><center>Missing Value Detection and Treatment<\/center><\/h1> \n","51f1fe1f":"Let's take a look to the 4 missing values.\nWe could choose to impute this values (using mean or median or trying to estimate it maybe with a linear regression)or just drop it.","09762aa1":"**Note**\n\nIn the previous cell, we used the following pattern: _ = func(). We do this to avoid showing the output of func() which in this case is not that useful. We actually assign the output of func() into the variable _ (called underscore). By convention, in Python the underscore variable is used as a \"garbage\" variable to store results that we are not interested in.","8127deec":"Heatmap\n----\n\nThe Heatmap graph is more intuitive respect to the correlation matrix. It can help at a first eye to detect faster the correlations. You can create it using the correlation matrix and adding some visual details.\n\nWe have to pay attention to the last rows in particular if we want to see which features are more related with the price.\n\nIf you want to know more you can read this interesting article","45b45228":"We can see that for normalized losses ? is the most frequent values and we will replace it with the median.\n\n<h1 style='background:lightgreen; border:0; color:#Black'><center>How to face this problem?<\/center><\/h1> \nHow to face this problem?\n---\n\nWe will use the nan library and we will substitute the \"?\" with np.nan values with the function **replace()**..","27d88ea7":"We want now to create a new variable to make a classification for the cars. We want to separate it in 2 classes (\"<=16500\" and \">16500\") who is exactly the threshold for the 3rd quartile.\nThis value is just an example to separate the cheapest cars from the most expensive. We could choos an higher threshold but in this case we don't want a too unbalanced dataset.\n\nClass imbalance happens often in practice and may need special techniques when building a predictive model.","245a87a8":"In this graph we can clearly see that with an higher value for horsepower we can easily find in presence of a high value car. The most of the right side of the distribution has a yellow color while the left side is in prevalence blue. \n\nFor more aestetic details i suggest you this video with a well explained example on a similar dataset:\nhttps:\/\/www.youtube.com\/watch?v=4DA_dgc521o&t=17s\n\nAnd the following code :\nhttps:\/\/github.com\/kimfetti\/Videos\/blob\/master\/Seaborn\/19_displot.ipynb","db491804":"Let's see now the bivariate relations between variables and let's begin with the target respect to the others","dd7272b1":"Doing so we discover that normalized losses is in practice a variable to be converted in numeric as well","f500d7ae":"We can add ","8e087c37":"<h1 style='background:lightblue; border:0; color:#Black'><center>Creating decision rules by hand<\/center><\/h1> \n\nBy looking at the previous plots, we could create some hand-written rules that predicts whether someone has a high- or low-price car. For instance, we could focus on the combination of the horsepower and length features.","8a19860b":" <h1 style='background:lightblue; border:0; color:#Black'><center>Missing Value Detection and Treatment<\/center><\/h1> ","87825dcc":"let's do it for the other variables","9917be37":"This is the distribution of the price variables. We can see how it's skewed to the right as we could suspect. There are some extreme values. ","d55d9d15":"In this case we will drop it","0bcc25cc":"Pairplot\n---","d1f86aca":"we will include some categorical variables ","c9b19134":"It's generally hard to make an analysis for all variables when we have many. Sometimes we can just the most interesting aspects. \n\nThe most of this variables are not normally distributed. **Highway-mpg**, **wheel-base** and **curb-weight** has a bell shape and are almost skewed.\n\n**Height** has two peaks and a bimodal distribution.\n\n**Symboling** is a discrete variables and has his maximum for the value 0.\n...\n","dc8d69e1":"The dataset contains both numerical and categorical data. Numerical values take continuous values, for example Price. Categorical values can have a finite number of values, for example body-styles.\nWe will leave the original type of distinction eventhough some variables could change type, just to simplify. For example the number of doors could be treated as numeric but in this case we have just 3 modality(\"four\",\"two\" and unknown) so we could treat it as categorical as well. ","41e411b7":"*Univariate Distributions*\n===","e9703c96":"We can remove the outliers using the clip() function to substitute the lower and upper data with some specific quantiles (for examples 0.05 & 0.95).\n\nWe will just do it in a copy of the dataset apart.\n\nWe have outliers in every quantitatibe variables except for symboling, bore, curb-weight and height.\nWe just escluded the num-of-doors variable because we have just 2 options (2 or 4)\n","15930d6d":"Let's apply the Random Forest model","9084719d":"*Numeric Variables*\n---","a38b68ee":"It seems that we don't have missing values but if we look at the first 5 rows we can clearly note that missing are indicates with '?' instead of nan.\n\nWe can also check that some quantitative variable is represented as object in this dataframe","a8880e92":"*note that the scale is changed (now is 80-180).*","97729bb7":" <h1 style='background:LightBlue; border:0; color:black'><center>Outlier Detection and Treatment<\/center><\/h1> ","b35cf86e":"Now we can convert the price column in numeric ( int64).","f2fb5e74":"At this point the most practical things to do is to substitute the missing values with the median of the same variable. The median is preferable to the mean because of its robustness to the outliers.\n\nWe will use the function fillna() to fill the missing data.","83633df5":"Linear Regression\n====","7b0b7b7b":"Or if we prefer we can try to understand the distribution of the target variable based on the interactions between the variables","44c1811d":" <h1 style='background:LightBlue; border:0; color:black'><center>A first look at the Dataset<\/center><\/h1> ","3365ce2b":"we can atleast see the cumulative Empirical cumulative distribution of the variables in this way:","7d23b602":"Or if we want just Kernel the density estimation of the variable:","b73c9bba":"If we want to have a general idea of all the relations between all the couple of variables we can include all the variables.\nThe option tight_layout helps us to give specific padding adjusting between and around subplot and it's useful when we have a lot of variables.","1089fb38":"Let's start with a simple prediction with a Random forest classifier.\nWe will make predictions for the variable target who divide the cars in 2 classes depend on the threshold of price.\nWe have to split the dataset in training and test. This model doesn't require particular operations of scaling and it's robust to outliers. \nWe have 140 rows for the training set and we will try to predict the values on the remaining 61 cars included in the test set. We set the random state to can compare more different models. ","0404afb9":"We can have an idea of outliers in the distribution with the command pd.describe() but if we want to have a visual idea of their presence the first things you have to learn is the use of Boxplots.\n\nLet's have an example of boxplot for our dataset. We will produce subplot for the quantitative variable. A Box and Whisker Plot (or Box Plot) is a convenient way of visually displaying the data distribution through their quartiles.The boxplot give us information about the 5 important numbers of the distribution (min,1st quartile,median,3rd quartile and max).\n\nThe values inside the box are between the 1st and 3rd quartile. The point out of the whiskers that we can see in the graphs represents the outliers. Outliers may be plotted as individual points\n\nHere are the types of observations one can make from viewing a Box Plot:\n\nWhat the key values are, such as: the average, median 25th percentile etc.\n\nIf there are any outliers and what their values are.\n\nIs the data symmetrical.\n\nHow tightly is the data grouped.\n\nIf the data is skewed and if so, in what direction.\n\nWhen the median is in the middle of the box, and the whiskers are about the same on both sides of the box, then the distribution is symmetric. When the median is closer to the bottom of the box, and if the whisker is shorter on the lower end of the box, then the distribution is positively skewed (skewed right).\n\nFrom above the upper quartile, a distance of 1.5 times the IQR is measured out and a whisker is drawn up to the largest observed point from the dataset that falls within this distance. Similarly, a distance of 1.5 times the IQR is measured out below the lower quartile and a whisker is drawn up to the lower observed point from the dataset that falls within this distance. All other observed points are plotted as outliers","c783779a":"This dataset is more focused on EDA in a classical dataset like Automobile Dataset and it will face many aspects about EDA and Data Visualization in the first part.\nIn the second we will try to use Machine Learning algorithm to solve problem in both cases ( Classification and Regression) for the price. \nWe used the price variable as target. For the Classification task we choose a threshold to splite the cars in 2 category of price. For the Regression task we just converted the original variable in numeric.","991bfb03":"<div style=\"display:fill;\n            border-radius: false;\n            border-style: solid;\n            border-color:#000000;\n            border-style: false;\n            border-width: 2px;\n            color:black;\n            font-size:15px;\n            font-family: Georgia;\n            background-color:lightblue;\n            text-align:center;\n            letter-spacing:0.1px;\n            padding: 0.1em;\">\n\n**<h2> Example of  Exploratory Data Analysis for the Automobile Dataset**\n    \n    \n","677615c5":"We could introduse a decision boundary  ","35fb2afc":"Ok we can use the pairplot to have an idea of the relations between two numerical feature but how if we want to analyze the relation between a categorical with a numerical feature?\n\nOne way to proceed is still using the *displot* function of seaborn adding the *hue* parameter ","1228f9e9":"Ricordati che normalized losses \u00e8 da modificare in numerica","8c7df883":"We can have a look at the distribution of variables with more tecnics like\n\n- Pairplot\n- Correlation Matrix\n- Heatmap\n\nWe can also after using this tecnique trying to design at hand a decision role before to apply Machine learning Algorithms.","21c39971":"See the sns heatmap example\n","2648616e":"*A different visualization way*\n---","dd07cfd1":"We can have the kernel density estimation as well:","008ff812":"We can use the Countplot for the Categorical features to have an idea of how they are distributed.","4afd979e":"<h1 style='background:#E8DCCC; border:0; color:#CF673A'><center>OOPS we have a problem!!!<\/center><\/h1> \n\nOOPS!!! We just discovered that some variables are not really categorical. Some of them could be converted in numeric replacing some values.\n\nThese variables are:\n- normalized-losses (37)\n- num-of doors  (2)\n- bore (4)\n- stroke (4)\n- horsepower (2)\n- peak-rpm (2)\n\nWe could choose to drop it but the dataset is already not too big so we should try to impute in coherent way.\nWe could replacing values using the median or trying in some cases at hand looking at the data. \nExcept num-of-doors all the variables are written in numeric charachter. We could leave the number of doors as charachter and just replacing with some criteria the missing values.\n\n","cd07d970":"Ok cool but it's not still finish. If we check the type of variable we can note that normalized-losses is still a numeric variable. We have to convert it in numerical feature with the function to_numeric() and we want to make it float.","8a795100":"Horsepower, engine-size and curb-weithg are the most correlated (all of them in in positive way) variables with the price. We have many other strong correlated both in positive (weight,length,bore) and negative way ( highway-mpgm,city-mpg). ","90971a1b":"<h1 style='background:lightblue; border:0; color:#Black'><center>To be continued.....<\/center><\/h1> ","81597970":"Finally we can treat it as a numeric variable","16548600":"let's control that we have corretly changed the type of variable for price","17e6f8c8":"*Bivariate distributions*\n===","04449651":"Another way to have a glance of the univariate distribution for numeric features is by using the *Displot* who is another tool of seaborn to visualize the distribution of a numeric variable. It stands for *distribution plot*.","55437ac3":"We can also add color installing a new package *heatmapz* using his function *corrplot*"}}