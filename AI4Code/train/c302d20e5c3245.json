{"cell_type":{"18e15d79":"code","80a33077":"code","365dfcad":"code","7888c4b5":"code","cb98ad3d":"code","a65bf9fd":"code","3ae32e8e":"code","fb7eb88c":"code","ef84deee":"code","64a5aedd":"code","70d9954e":"code","9ac6c83b":"markdown","b40c6cf7":"markdown","f98c68bd":"markdown","087aa3fe":"markdown","ea093ca8":"markdown","9aed6fd7":"markdown","7d6c2509":"markdown","6b759e79":"markdown","61036913":"markdown","01c34c09":"markdown"},"source":{"18e15d79":"import pandas as pd\nimport numpy as np\nimport tensorflow as tf\nimport tensorflow_addons as tfa\nimport glob, warnings\nimport matplotlib.pyplot as plt\nfrom sklearn.metrics import confusion_matrix, classification_report\nimport seaborn as sns\n\nwarnings.filterwarnings('ignore')\nprint('TensorFlow Version ' + tf.__version__)","80a33077":"IMAGE_SIZE = 224\nBATCH_SIZE = 16\nEPOCHS = 7\n\nTRAIN_PATH = '\/kaggle\/input\/cassava-leaf-disease-classification\/train_images'\nTEST_PATH = '\/kaggle\/input\/cassava-leaf-disease-classification\/test_images'\n\nDF_TRAIN = pd.read_csv('\/kaggle\/input\/cassava-leaf-disease-classification\/train.csv', dtype='str')\nTEST_IMAGES = glob.glob(TEST_PATH + '\/*.jpg')\nDF_TEST = pd.DataFrame(TEST_IMAGES, columns = ['image_path'])\n\nclasses = {0 : \"Cassava Bacterial Blight (CBB)\",\n           1 : \"Cassava Brown Streak Disease (CBSD)\",\n           2 : \"Cassava Green Mottle (CGM)\",\n           3 : \"Cassava Mosaic Disease (CMD)\",\n           4 : \"Healthy\"}","365dfcad":"def data_augment(image):\n    p_spatial = tf.random.uniform([], 0, 1.0, dtype = tf.float32)\n    p_rotate = tf.random.uniform([], 0, 1.0, dtype = tf.float32)\n    p_pixel_1 = tf.random.uniform([], 0, 1.0, dtype = tf.float32)\n    p_pixel_2 = tf.random.uniform([], 0, 1.0, dtype = tf.float32)\n    p_pixel_3 = tf.random.uniform([], 0, 1.0, dtype = tf.float32)\n    \n    # Flips\n    image = tf.image.random_flip_left_right(image)\n    image = tf.image.random_flip_up_down(image)\n    \n    if p_spatial > .75:\n        image = tf.image.transpose(image)\n        \n    # Rotates\n    if p_rotate > .75:\n        image = tf.image.rot90(image, k = 3) # rotate 270\u00ba\n    elif p_rotate > .5:\n        image = tf.image.rot90(image, k = 2) # rotate 180\u00ba\n    elif p_rotate > .25:\n        image = tf.image.rot90(image, k = 1) # rotate 90\u00ba\n        \n    # Pixel-level transforms\n    if p_pixel_1 >= .4:\n        image = tf.image.random_saturation(image, lower = .7, upper = 1.3)\n    if p_pixel_2 >= .4:\n        image = tf.image.random_contrast(image, lower = .8, upper = 1.2)\n    if p_pixel_3 >= .4:\n        image = tf.image.random_brightness(image, max_delta = .1)\n        \n    return image","7888c4b5":"datagen = tf.keras.preprocessing.image.ImageDataGenerator(rescale = 1.\/255,\n                                                          samplewise_center = True,\n                                                          samplewise_std_normalization = True,\n                                                          validation_split = 0.2,\n                                                          preprocessing_function = data_augment)\n\ntrain_gen = datagen.flow_from_dataframe(dataframe = DF_TRAIN,\n                                        directory = TRAIN_PATH,\n                                        x_col = 'image_id',\n                                        y_col = 'label',\n                                        subset = 'training',\n                                        batch_size = BATCH_SIZE,\n                                        seed = 1,\n                                        color_mode = 'rgb',\n                                        shuffle = True,\n                                        class_mode = 'categorical',\n                                        target_size = (IMAGE_SIZE, IMAGE_SIZE))\n\nvalid_gen = datagen.flow_from_dataframe(dataframe = DF_TRAIN,\n                                        directory = TRAIN_PATH,\n                                        x_col = 'image_id',\n                                        y_col = 'label',\n                                        subset = 'validation',\n                                        batch_size = BATCH_SIZE,\n                                        seed = 1,\n                                        color_mode = 'rgb',\n                                        shuffle = False,\n                                        class_mode = 'categorical',\n                                        target_size = (IMAGE_SIZE, IMAGE_SIZE))\n\ntest_gen = datagen.flow_from_dataframe(dataframe = DF_TEST,\n                                       x_col = 'image_path',\n                                       y_col = None,\n                                       batch_size = BATCH_SIZE,\n                                       seed = 1,\n                                       color_mode = 'rgb',\n                                       shuffle = False,\n                                       class_mode = None,\n                                       target_size = (IMAGE_SIZE, IMAGE_SIZE))","cb98ad3d":"images = [train_gen[0][0][i] for i in range(16)]\nfig, axes = plt.subplots(3, 5, figsize = (10, 10))\n\naxes = axes.flatten()\n\nfor img, ax in zip(images, axes):\n    ax.imshow(img.reshape(IMAGE_SIZE, IMAGE_SIZE, 3))\n    ax.axis('off')\n\nplt.tight_layout()\nplt.show()","a65bf9fd":"!pip install --quiet vit-keras\n\nfrom vit_keras import vit","3ae32e8e":"vit_model = vit.vit_b32(\n        image_size = IMAGE_SIZE,\n        activation = 'softmax',\n        pretrained = True,\n        include_top = False,\n        pretrained_top = False,\n        classes = 5)","fb7eb88c":"from vit_keras import visualize\n\nx = test_gen.next()\nimage = x[0]\n\nattention_map = visualize.attention_map(model = vit_model, image = image)\n\n# Plot results\nfig, (ax1, ax2) = plt.subplots(ncols = 2)\nax1.axis('off')\nax2.axis('off')\nax1.set_title('Original')\nax2.set_title('Attention Map')\n_ = ax1.imshow(image)\n_ = ax2.imshow(attention_map)","ef84deee":"model = tf.keras.Sequential([\n        vit_model,\n        tf.keras.layers.Flatten(),\n        tf.keras.layers.BatchNormalization(),\n        tf.keras.layers.Dense(11, activation = tfa.activations.gelu),\n        tf.keras.layers.BatchNormalization(),\n        tf.keras.layers.Dense(5, 'softmax')\n    ],\n    name = 'vision_transformer')\n\nmodel.summary()","64a5aedd":"learning_rate = 1e-4\n\noptimizer = tfa.optimizers.RectifiedAdam(learning_rate = learning_rate)\n\nmodel.compile(optimizer = optimizer, \n              loss = tf.keras.losses.CategoricalCrossentropy(label_smoothing = 0.2), \n              metrics = ['accuracy'])\n\nSTEP_SIZE_TRAIN = train_gen.n \/\/ train_gen.batch_size\nSTEP_SIZE_VALID = valid_gen.n \/\/ valid_gen.batch_size\n\nreduce_lr = tf.keras.callbacks.ReduceLROnPlateau(monitor = 'val_accuracy',\n                                                 factor = 0.2,\n                                                 patience = 2,\n                                                 verbose = 1,\n                                                 min_delta = 1e-4,\n                                                 min_lr = 1e-6,\n                                                 mode = 'max')\n\nearlystopping = tf.keras.callbacks.EarlyStopping(monitor = 'val_accuracy',\n                                                 min_delta = 1e-4,\n                                                 patience = 5,\n                                                 mode = 'max',\n                                                 restore_best_weights = True,\n                                                 verbose = 1)\n\ncheckpointer = tf.keras.callbacks.ModelCheckpoint(filepath = '.\/model.hdf5',\n                                                  monitor = 'val_accuracy', \n                                                  verbose = 1, \n                                                  save_best_only = True,\n                                                  save_weights_only = True,\n                                                  mode = 'max')\n\ncallbacks = [earlystopping, reduce_lr, checkpointer]\n\nmodel.fit(x = train_gen,\n          steps_per_epoch = STEP_SIZE_TRAIN,\n          validation_data = valid_gen,\n          validation_steps = STEP_SIZE_VALID,\n          epochs = EPOCHS,\n          callbacks = callbacks)\n\nmodel.save('model.h5', save_weights_only = True)","70d9954e":"predicted_classes = np.argmax(model.predict(valid_gen, steps = valid_gen.n \/\/ valid_gen.batch_size + 1), axis = 1)\ntrue_classes = valid_gen.classes\nclass_labels = list(valid_gen.class_indices.keys())  \n\nconfusionmatrix = confusion_matrix(true_classes, predicted_classes)\nplt.figure(figsize = (16, 16))\nsns.heatmap(confusionmatrix, cmap = 'Blues', annot = True, cbar = True)\n\nprint(classification_report(true_classes, predicted_classes))","9ac6c83b":"### Visualizing Attention Maps of Sample Test Image","b40c6cf7":"# Model Results","f98c68bd":"# Libraries and Configurations","087aa3fe":"## 1. ViT B32 Model","ea093ca8":"# Overview\n\nThis notebook implements Vision Transformer (ViT) model by Alexey Dosovitskiy et al for image classification, and demonstrates it on the Cassava Leaf Disease Classification dataset.\n\nFor from scratch implementation of ViT check out this notebook: <br>\nhttps:\/\/www.kaggle.com\/raufmomin\/vision-transformer-vit-from-scratch\n\nResearch Paper: https:\/\/arxiv.org\/pdf\/2010.11929.pdf <br>\nGithub (Official) Link: https:\/\/github.com\/google-research\/vision_transformer <br>\nGithub (Keras) Link: https:\/\/github.com\/faustomorales\/vit-keras\n\n#### Highlights of this notebook:\n1. Pre-trained Vision Transformer (vit_b32) on imagenet21k dataset\n2. Label Smoothing of 0.3\n3. Custom data augmentation for ImageDataGenerator\n4. RectifiedAdam Optimizer\n\n\n# Model Architecture\n\n![figure1.png](attachment:figure1.png)\n\n# Available models\nThere are models pre-trained on imagenet21k for the following architectures:\nViT-B\/16, ViT-B\/32, ViT-L\/16, ViT-L\/32 and ViT-H\/14. \nThere are also the same models pre-trained on imagenet21k and fine-tuned on imagenet2012.\n\n![image.png](attachment:image.png)","9aed6fd7":"# Data Generator","7d6c2509":"# Data Augmentations","6b759e79":"## 2. Fine-tuning the Model","61036913":"# Building the Model","01c34c09":"# Training the Model"}}