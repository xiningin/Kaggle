{"cell_type":{"e77362c4":"code","d006d527":"code","d07a5e61":"code","cee27ab8":"code","68335e4f":"code","ea410e3c":"code","affdc811":"code","758bbd88":"code","a1d33563":"code","fe57ba2a":"code","18602f60":"code","b85fd5fa":"code","983c0714":"code","167feb50":"code","89acb387":"code","16d23c3c":"markdown","6832854f":"markdown","65e576f2":"markdown"},"source":{"e77362c4":"import random\nimport os\nimport time\nimport pandas as pd\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader\nfrom torch.cuda.amp import autocast, GradScaler\nfrom tqdm import tqdm\nfrom sklearn.model_selection import *\nfrom transformers import *\n","d006d527":"CFG = {\n    'fold_num': 5, \n    'seed': 42,\n    'model': '..\/input\/roberta-base',\n    'max_len': 512,\n    'epochs': 3,\n    'train_bs': 16,\n    'valid_bs': 32,\n    'lr': 1e-4,\n    'num_workers': 0,\n    'weight_decay': 1e-5,\n}","d07a5e61":"def seed_everything(seed):\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = False\n\nseed_everything(CFG['seed'])\n\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')","cee27ab8":"label_list = ['o', 'B-Lead', 'I-Lead', 'B-Position', 'I-Position', 'B-Claim', 'I-Claim', 'B-Counterclaim', 'I-Counterclaim', \n          'B-Rebuttal', 'I-Rebuttal', 'B-Evidence', 'I-Evidence', 'B-Concluding Statement', 'I-Concluding Statement']\nlabel_encoding_dict = {'o': 1,\n                       'B-Lead': 2,\n                       'I-Lead': 3,\n                       'B-Position': 4,\n                       'I-Position': 5,\n                       'B-Claim': 6,\n                       'I-Claim': 7,\n                       'B-Counterclaim': 8, 'I-Counterclaim': 9,\n                       'B-Rebuttal': 10, 'I-Rebuttal': 11,\n                       'B-Evidence': 12, 'I-Evidence': 13, 'B-Concluding Statement': 14,\n                       'I-Concluding Statement' :15\n                       \n                      }\n","68335e4f":"tokenizer = AutoTokenizer.from_pretrained(CFG['model'], add_prefix_space=True)","ea410e3c":"train=pd.read_csv('..\/input\/feedback-train\/finaldata.csv')","affdc811":"train_names, train_texts = [], []\nfor f in tqdm(list(os.listdir('..\/input\/feedback-prize-2021\/train'))):\n    train_names.append(f.replace('.txt', ''))\n    train_texts.append(open('..\/input\/feedback-prize-2021\/train\/' + f, 'r').read())\ntrain_texts = pd.DataFrame({'id': train_names, 'text': train_texts})\ntrain_texts['text'] = train_texts['text'].apply(lambda x:x.split())","758bbd88":"\ntrain_texts=pd.merge(train_texts, train[['id','token_class_merged']], how='left', on =['id'])\ntrain_texts['token_class_merged']=train_texts['token_class_merged'].apply(lambda x:(str(x)[1:-1]).split(','))\ntrain_texts.head(2)","a1d33563":"train_df=train_texts.iloc[1:13000,]\ntest_df=train_texts.iloc[13001:,]","fe57ba2a":"label_to_id = {l: i for i, l in enumerate(label_list)}\nb_to_i_label = []\nfor idx, label in enumerate(label_list):\n    if label.startswith(\"B-\") and label.replace(\"B-\", \"I-\") in label_list:\n        b_to_i_label.append(label_list.index(label.replace(\"B-\", \"I-\")))\n    else:\n        b_to_i_label.append(idx)","18602f60":"def tokenize_and_align_labels(examples):\n    label_all_tokens = True\n    tokenized_inputs = tokenizer(list(examples[\"text\"]), truncation=True, is_split_into_words=True,\n                                 max_length=CFG['max_len'])\n\n    labels = []\n    for i, label in enumerate(examples['token_class_merged']):\n        word_ids = tokenized_inputs.word_ids(batch_index=i)\n        previous_word_idx = None\n        label_ids = []\n        for word_idx in word_ids:\n            if word_idx is None:\n                label_ids.append(-100)\n            elif label[word_idx] == '0':\n                label_ids.append(0)\n            elif word_idx != previous_word_idx:\n                label_ids.append(label_to_id[label[word_idx]])\n            else:\n                label_ids.append(label_to_id[label[word_idx]] if label_all_tokens else -100)\n            previous_word_idx = word_idx\n        labels.append(label_ids)\n        \n    tokenized_inputs[\"labels\"] = labels\n    return tokenized_inputs\n","b85fd5fa":"!pip install seqeval","983c0714":"#uncomment for training\nfrom datasets import Dataset\ntrain_dataset = Dataset.from_pandas(train_df)\ntest_dataset = Dataset.from_pandas(test_df)\ntrain_tokenized_datasets = train_dataset.map(tokenize_and_align_labels, batched=True)\ntest_tokenized_datasets = test_dataset.map(tokenize_and_align_labels, batched=True)","167feb50":"## training loop\nfrom datasets import load_metric\nos.environ[\"WANDB_DISABLED\"] = \"true\"\nmodel = AutoModelForTokenClassification.from_pretrained(CFG['model'], num_labels=len(label_list))\nargs = TrainingArguments(\n    \"test-ner\",\n    evaluation_strategy = \"epoch\",\n    learning_rate=CFG['lr'],\n    per_device_train_batch_size=CFG['train_bs'],\n    per_device_eval_batch_size=CFG['valid_bs'],\n    num_train_epochs=CFG['epochs'],\n    weight_decay=CFG['weight_decay'],\n)\n\ndata_collator = DataCollatorForTokenClassification(tokenizer)\nmetric = load_metric(\"seqeval\")\n\n\ndef compute_metrics(p):\n    predictions, labels = p\n    predictions = np.argmax(predictions, axis=2)\n\n    true_predictions = [[label_list[p] for (p, l) in zip(prediction, label) if l != -100] for prediction, label in zip(predictions, labels)]\n    true_labels = [[label_list[l] for (p, l) in zip(prediction, label) if l != -100] for prediction, label in zip(predictions, labels)]\n\n    results = metric.compute(predictions=true_predictions, references=true_labels)\n    return {\"precision\": results[\"overall_precision\"], \"recall\": results[\"overall_recall\"], \"f1\": results[\"overall_f1\"], \"accuracy\": results[\"overall_accuracy\"]}\n    \ntrainer = Trainer(\n    model,\n    args,\n    train_dataset=train_tokenized_datasets,\n    eval_dataset=test_tokenized_datasets,\n    data_collator=data_collator,\n    tokenizer=tokenizer,\n    compute_metrics=compute_metrics\n)\n\ntrainer.train()\n","89acb387":"trainer.evaluate()\ntrainer.save_model('roberta_v1.model')\ntorch.save(model.state_dict(), '.\/roberta-baseline.pt')\n","16d23c3c":"#### A simple Roberta hugging face NER based approach","6832854f":"I have taken ideas from this notebook\nhttps:\/\/www.kaggle.com\/zzy990106\/pytorch-ner-infer.\n\nI have created the baseline traning pipeline based on roberta for token classification.\n\nThis is the training notebook.\n\nFind the prediction notebook here: https:\/\/www.kaggle.com\/revathiprakash\/feedback-prize-baseline-roberta-pytorch-wip\/edit\/run\/82697901\n","65e576f2":"\n### Please upvote if you find the notebook useful"}}