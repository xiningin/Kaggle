{"cell_type":{"4a8895e3":"code","704fec56":"code","5658bc12":"code","040c54d7":"code","c08ef99d":"code","6f5d6b48":"code","24ccc257":"code","a499247c":"code","0c58784e":"code","da7826f3":"markdown","49bc95de":"markdown","b2a011fa":"markdown"},"source":{"4a8895e3":"!pip install -U spacy\n!pip install https:\/\/s3-us-west-2.amazonaws.com\/ai2-s2-scispacy\/releases\/v0.2.4\/en_core_sci_lg-0.2.4.tar.gz","704fec56":"from __future__ import unicode_literals, print_function\nfrom pathlib import Path\nfrom spacy.util import minibatch, compounding\nfrom tqdm import tqdm\nfrom sklearn.model_selection import train_test_split\n\nimport itertools\nimport json\nimport nltk.data\nimport numpy as np\nimport os\nimport pandas as pd\nimport random\nimport spacy","5658bc12":"# CONFIG\n\n# Data\nDIR_DATA_INPUT = os.path.join('\/kaggle', 'input', 'CORD-19-research-challenge')\nDIR_BIORXIV = os.path.join(DIR_DATA_INPUT, 'biorxiv_medrxiv', 'biorxiv_medrxiv', 'pdf_json')\nDIR_COMM = os.path.join(DIR_DATA_INPUT, 'comm_use_subset', 'comm_use_subset', 'pdf_json')\nDIR_CUSTOM = os.path.join(DIR_DATA_INPUT, 'custom_license', 'custom_license', 'pdf_json')\nDIR_NONCUSTOM = os.path.join(DIR_DATA_INPUT, 'noncomm_use_subset', 'noncomm_use_subset', 'pdf_json')\n\nDIR_DATA_OUTPUT = os.path.join('\/kaggle', 'working')\nPATH_AGG_JSON = os.path.join(DIR_DATA_OUTPUT, 'agg_data.json')","040c54d7":"def extract_jsons_to_list(folder):\n    \"\"\"\n    Extracting 4 fields ('abstract', 'text', 'paper_id', 'title') from orginal Json file\n    :folder String, to location with Jsons\n    :return: Lists, with selected params\n    \"\"\"\n    results = []\n\n    files = os.listdir(folder)\n    for filename in tqdm(files, f'parsing {folder}'):\n        json_file = os.path.join(folder, filename)\n        file = json.load(open(json_file, 'rb'))\n        agg_abstract_file = ' '.join(\n            [abstract['text'] for abstract in file['abstract']])\n        text = ' '.join(\n            [text['text'] for text in file['body_text']])\n        results.append({\n            'abstract': agg_abstract_file,\n            'text': text,\n            'paper_id': file['paper_id'], \n            'title': file['metadata']['title']\n        })\n\n    return results\n\n\ndef save_json(file_to_save, path_to_save):\n    \"\"\"\n    Save in relevant Json format\n    :file_to_save DataFrame, file to save\n    :path_to_save String, lacation to save a file\n    \"\"\"\n    df = pd.DataFrame(file_to_save)\n    \n    df['json_output'] = df.apply(lambda x: {\n        'text': x.text, \"meta\":{'paper_id':x.paper_id, 'title': x.title}\n    }, axis=1)\n    df['json_output'].to_json(path_to_save, orient='records', lines=True)\n    \n\ndef filtr_covid_and_risk_factor(file_to_save, path_to_save):\n    \"\"\"\n    List filtering in abstact and text (filters: 'COVID-19' or 'SARS-CoV-2')\n    :file_to_save List, file to save\n    :path_to_save String, lacation to save a file\n    :return: DataFrame, valid data\n    \"\"\"\n    df = pd.DataFrame(file_to_save)\n    mask = df['abstract'].str.contains('COVID-19') | df['text'].str.contains('COVID-19') \\\n     | df['abstract'].str.contains('SARS-CoV-2') | df['text'].str.contains('SARS-CoV-2')\n    \n    abstracts = text_2_sentance(df[mask], 'abstract')\n    text = text_2_sentance(df[mask], 'text')\n    abstracts.extend(text)\n\n    save_json(abstracts, path_to_save)\n    \n    return df\n\n\ntokenizer = nltk.data.load('tokenizers\/punkt\/english.pickle')\ndef text_2_sentance(df, column):\n    \"\"\"\n    Save 5 senctance before and after sentance which contains `risk factor` expression\n    :df DataFrame, with text data\n    :column String, column name to process\n    :return: List, valid sentance\n    \"\"\"\n    df['sentances'] = df.apply(lambda x: tokenizer.tokenize(x[column]), axis = 1)\n    \n    valid_sentance = []\n    for _, row in tqdm(df.iterrows()):\n        for index, singiel_sentance in enumerate(row['sentances']):\n            if 'risk factor' in singiel_sentance.lower():\n                sentance_range = [valid_index for valid_index in range(index-5, index+6) if (valid_index >=0) and (valid_index < len(row['sentances']))]\n                valid_sentance.append({\n                    'text': row['sentances'][sentance_range[0]: (sentance_range[-1]+1)],\n                    'paper_id': row['paper_id'], \n                    'title': row['title']\n                })\n                \n    return valid_sentance\n","c08ef99d":"# Generate Json for Marek\n\nbio = extract_jsons_to_list(DIR_BIORXIV)\ncomm = extract_jsons_to_list(DIR_COMM)\ncus = extract_jsons_to_list(DIR_CUSTOM)\nnon = extract_jsons_to_list(DIR_NONCUSTOM)\n\nlist_agg = bio + comm + cus + non\nresults = filtr_covid_and_risk_factor(list_agg, PATH_AGG_JSON)\n","6f5d6b48":"!head $PATH_AGG_JSON","24ccc257":"!wget https:\/\/raw.githubusercontent.com\/chopeen\/CORD-19\/master\/data\/annotated\/cord_19_rf_sentences_merged.json\n!ls -1","a499247c":"\n\nnew_list = []\nfile = json.load(open('cord_19_rf_sentences_merged.json', 'rb'))\n\ndf = pd.DataFrame(file)\n\nX_train, X_test = train_test_split(\n    df, test_size=0.2, random_state=42)\n\nX_train.to_json('train_abstract_teach.json', orient='records')\nX_test.to_json('test_abstract_teach.json', orient='records')","0c58784e":"!spacy train en models\/ train_abstract_teach.json test_abstract_teach.json --pipeline ner --base-model en_core_sci_lg  --replace-components","da7826f3":"# Split dataset for train and test sets","49bc95de":"# Train NER model","b2a011fa":"# Download data for training"}}