{"cell_type":{"e1387d8c":"code","ad1df415":"code","c764774e":"code","54e8ad09":"code","8fed37a3":"code","ce69d92e":"code","28f9c8d7":"code","90204518":"code","78d1b8c4":"code","7512b069":"code","71ab6c46":"code","badf147a":"code","7a599810":"code","dcf84386":"code","2db0b218":"code","0ca9faaf":"code","46b15358":"code","c2bf0e33":"code","660b2e39":"code","30ea9e57":"code","9c3dae04":"code","f9895f15":"code","75dce32a":"code","2e46db5b":"code","7224847d":"markdown","f0e1a463":"markdown","e7a03d98":"markdown","debf6831":"markdown","608b31cb":"markdown","6607e4ec":"markdown","c3f45c6a":"markdown","e62df6b0":"markdown","272618c8":"markdown","ace57c10":"markdown"},"source":{"e1387d8c":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","ad1df415":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set()\nimport warnings\nwarnings.filterwarnings('ignore')\n%matplotlib inline","c764774e":"#Loading the dataset\ndiabetes_data = pd.read_csv(\"..\/input\/pima-indians-diabetes-database\/diabetes.csv\")\n\n#Print the first 5 rows of the dataframe.\ndiabetes_data.head()","54e8ad09":"# gives information about the data types,columns, null value counts, memory usage etc\ndiabetes_data.info()","8fed37a3":"diabetes_data.describe()\n## basic statistic details about the data (note only numerical columns would be displayed here unless parameter include=\"all\")","ce69d92e":"diabetes_data.describe().T","28f9c8d7":"diabetes_data_copy = diabetes_data.copy(deep = True)\ndiabetes_data_copy[['Glucose','BloodPressure','SkinThickness','Insulin','BMI']] = diabetes_data_copy[['Glucose','BloodPressure','SkinThickness','Insulin','BMI']].replace(0,np.NaN)\n\n## showing the count of Nans\nprint(diabetes_data_copy.isnull().sum())","90204518":"p = diabetes_data.hist(figsize = (20,20))","78d1b8c4":"diabetes_data_copy['Glucose'].fillna(diabetes_data_copy['Glucose'].mean(), inplace = True)\ndiabetes_data_copy['BloodPressure'].fillna(diabetes_data_copy['BloodPressure'].mean(), inplace = True)\ndiabetes_data_copy['SkinThickness'].fillna(diabetes_data_copy['SkinThickness'].median(), inplace = True)\ndiabetes_data_copy['Insulin'].fillna(diabetes_data_copy['Insulin'].median(), inplace = True)\ndiabetes_data_copy['BMI'].fillna(diabetes_data_copy['BMI'].median(), inplace = True)","7512b069":"p = diabetes_data_copy.hist(figsize = (20,20))","71ab6c46":"diabetes_data.shape","badf147a":"import seaborn as sns\nsns.pairplot(diabetes_data)\n","7a599810":"p=sns.pairplot(diabetes_data_copy, hue = 'Outcome')","dcf84386":"plt.figure(figsize=(12,10))  # on this line I just set the size of figure to 12 by 10.\np=sns.heatmap(diabetes_data.corr(), annot=True) ","2db0b218":"diabetes_data.corr()","0ca9faaf":"plt.figure(figsize=(12,10))  # on this line I just set the size of figure to 12 by 10.\np=sns.heatmap(diabetes_data_copy.corr(), annot=True,cmap ='RdYlGn')  # seaborn has very simple solution for heatmap","46b15358":"diabetes_data_copy.corr()","c2bf0e33":"X=diabetes_data_copy.drop('Outcome',axis=1)\ny=diabetes_data_copy['Outcome']","660b2e39":"import pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import confusion_matrix\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20,random_state=1)\nknn=KNeighborsClassifier(n_neighbors=4)\nsc=StandardScaler()\nsc.fit(X_train)\nscaledX_train = sc.transform(X_train)\n# scaledX_train = sc.fit_transform(X_train)\nscaledX_test = sc.transform(X_test)\nknn.fit(scaledX_train,y_train)\nprint(\"What is the Testing Accuracy\")\nprint(knn.score(scaledX_test,y_test))\nprint(\"What is the Training Accuracy\")\nprint(knn.score(scaledX_train,y_train))\npredicted = knn.predict(scaledX_test)\nprint(confusion_matrix(y_test,predicted))","30ea9e57":"from sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.ensemble import AdaBoostClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier","9c3dae04":"from matplotlib import pyplot as plt\nimport numpy as np\n\nneighbors = np.arange(1, 60)\ntrain_accuracy_plot = np.empty(len(neighbors))\ntest_accuracy_plot = np.empty(len(neighbors))\n# Loop over different values of k\nfor i, k in enumerate(neighbors):\n    train = []\n    test = []\n    for j in range(10):\n        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20,random_state=j)\n        sc=StandardScaler()\n        scaledX_train = sc.fit_transform(X_train)\n        scaledX_test = sc.transform(X_test)\n        ad = AdaBoostClassifier(n_estimators=k)\n        ad.fit(scaledX_train,y_train)\n        train.append(ad.score(scaledX_train,y_train))\n        test.append(ad.score(scaledX_test,y_test))\n    #Compute accuracy on the training set\n    train_accuracy_plot[i] = np.mean(train)\n    #Compute accuracy on the testing set\n    test_accuracy_plot[i] = np.mean(test)\n# Generate plot\nplt.title('AdaBoostClassifier: Varying Number of Depth')\nplt.plot(neighbors, test_accuracy_plot, label = 'Testing Accuracy')\nplt.plot(neighbors, train_accuracy_plot, label = 'Training Accuracy')\nplt.legend()\nplt.xlabel('n_estimator')\nplt.ylabel('Accuracy')\nplt.show()\nprint(np.mean(train))\nprint(np.mean(test))","f9895f15":"from matplotlib import pyplot as plt\nimport numpy as np\nfrom sklearn.ensemble import GradientBoostingClassifier\nneighbors = np.arange(1, 60)\ntrain_accuracy_plot = np.empty(len(neighbors))\ntest_accuracy_plot = np.empty(len(neighbors))\n# Loop over different values of k\nfor i, k in enumerate(neighbors):\n    train = []\n    test = []\n    for j in range(10):\n        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20,random_state=j)\n        sc=StandardScaler()\n        scaledX_train = sc.fit_transform(X_train)\n        scaledX_test = sc.transform(X_test)\n        knn = GradientBoostingClassifier(n_estimators=k)\n        knn.fit(scaledX_train,y_train)\n        train.append(knn.score(scaledX_train,y_train))\n        test.append(knn.score(scaledX_test,y_test))\n    #Compute accuracy on the training set\n    train_accuracy_plot[i] = np.mean(train)\n    #Compute accuracy on the testing set\n    test_accuracy_plot[i] = np.mean(test)\n# Generate plot\nplt.title('GradientBoostClassifier: Varying Number of Depth')\nplt.plot(neighbors, test_accuracy_plot, label = 'Testing Accuracy')\nplt.plot(neighbors, train_accuracy_plot, label = 'Training Accuracy')\nplt.legend()\nplt.xlabel('n_estimator')\nplt.ylabel('Accuracy')\nplt.show()\nprint(\"Train Accuracy\")\nprint(np.mean(train))\nprint(\"Test Accuracy\")\nprint(np.mean(test))","75dce32a":"from matplotlib import pyplot as plt\nimport numpy as np\n\nneighbors = np.arange(1, 60)\ntrain_accuracy_plot = np.empty(len(neighbors))\ntest_accuracy_plot = np.empty(len(neighbors))\n# Loop over different values of k\nfor i, k in enumerate(neighbors):\n    train = []\n    test = []\n    for j in range(10):\n        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20,random_state=j)\n        sc=StandardScaler()\n        scaledX_train = sc.fit_transform(X_train)\n        scaledX_test = sc.transform(X_test)\n        knn = RandomForestClassifier(n_estimators=k)\n        knn.fit(scaledX_train,y_train)\n        train.append(knn.score(scaledX_train,y_train))\n        test.append(knn.score(scaledX_test,y_test))\n    #Compute accuracy on the training set\n    train_accuracy_plot[i] = np.mean(train)\n    #Compute accuracy on the testing set\n    test_accuracy_plot[i] = np.mean(test)\n# Generate plot\nplt.title('Random Forest: Number of Estimators')\nplt.plot(neighbors, test_accuracy_plot, label = 'Testing Accuracy')\nplt.plot(neighbors, train_accuracy_plot, label = 'Training Accuracy')\nplt.legend()\nplt.xlabel('Number of estimators')\nplt.ylabel('Accuracy')\nplt.show()\nprint(\"Train Accuracy\")\nprint(np.mean(train))\nprint(\"Test accuracy\")\nprint(np.mean(test))","2e46db5b":"#import classification_report\nfrom sklearn.metrics import classification_report\nprint(classification_report(y_test,predicted))","7224847d":"**GRADIENT DESCENT**","f0e1a463":"### Basic EDA and statistical analysis","e7a03d98":"Using KNN Algorithm","debf6831":"**clean data**","608b31cb":"**ADABOOST**","6607e4ec":"The Question creeping out of this summary\nCan minimum value of below listed columns be zero (0)?\nOn these columns, a value of zero does not make sense and thus indicates missing value.\n\nFollowing columns or variables have an invalid zero value:\n\n* Glucose\n* BloodPressure\n* SkinThickness\n* Insulin\n* BMI\nIt is better to replace zeros with nan since after that counting them would be easier and zeros need to be replaced with suitable values","c3f45c6a":"**Random Forest**","e62df6b0":"Now, let's understand the statistics that are generated by the describe() method:\n\n* count tells us the number of NoN-empty rows in a feature.\n* mean tells us the mean value of that feature.\n* std tells us the Standard Deviation Value of that feature.\n* min tells us the minimum value of that feature.\n* 25%, 50%, and 75% are the percentile\/quartile of each features. This quartile information helps us to detect Outliers.\n* max tells us the maximum value of that feature.","272618c8":"Scaling the data\ndata Z is rescaled such that \u03bc = 0 and \ud835\uded4 = 1, and is done through this formula:","ace57c10":"To fill these Nan values the data distribution needs to be understood"}}