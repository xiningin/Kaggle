{"cell_type":{"736d0bdc":"code","f67056ab":"code","74de19e8":"code","cd8e4798":"code","39c93dc7":"code","c58ed4e9":"code","5d55e002":"code","f7998883":"code","520de482":"code","26923887":"code","220d4c18":"code","48862d4c":"code","5fd00714":"code","57aa6c73":"code","dc91cfb6":"code","cf76da42":"code","d370c7bd":"code","10c4a0ae":"code","360c9455":"code","117305ed":"code","03e70147":"code","0bbfec46":"code","b5b68678":"code","d93493e3":"code","397fb99d":"code","d07368e2":"code","afb7a3b8":"code","b1cb1e61":"code","cc889842":"code","00884871":"code","7068078d":"code","0d2508ba":"code","f90c1d81":"code","b5bf7648":"code","1ed6cb33":"code","25e04f12":"code","46c5503a":"code","9079489f":"code","97ed42ed":"code","2f8580bc":"code","ed798da9":"code","b5c9f51f":"markdown","94ee89b8":"markdown","25fecb16":"markdown","88d19fd2":"markdown","ba24c278":"markdown","b96902d8":"markdown","f3e0568d":"markdown","cdce544a":"markdown","933b4665":"markdown","a9e855df":"markdown","46fc9b79":"markdown","4246085c":"markdown","0237bf56":"markdown","f1b9bb6d":"markdown","4a35768f":"markdown","5fe63132":"markdown","ccd12f24":"markdown"},"source":{"736d0bdc":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nimport statsmodels.api as sm","f67056ab":"df= pd.read_csv('..\/input\/creditcard-fraud-detection\/creditcard.csv')\ndf.head()","74de19e8":"from scipy.stats import zscore\ndf1=df[['Time','Amount']].apply(zscore)\ndf.drop(['Time','Amount'],axis=1,inplace=True)\ndf.head()","cd8e4798":"df1.head()\ndf2=pd.concat((df,df1),axis=1)\ndf2.head()","39c93dc7":"# the no of rows and columns:\ndf2.shape","c58ed4e9":"# the descriptive statistics\ndf2.describe().T","5d55e002":"# the target\ndf2['Class'].value_counts(normalize=True)","f7998883":"# Checking for missing Values\ndf2.isnull().sum()","520de482":"df3=df2.drop('Class',axis=1)","26923887":"# distribution of the features and target\ncols= list(df3.columns)\nfor i in cols:\n    sns.boxplot(y=i,x=df2['Class'],data=df3)\n    plt.show()","220d4c18":"x=df3\ny=df2['Class']\nx_const = sm.add_constant(x)","48862d4c":"model1=sm.Logit(y,x_const).fit()\nmodel1.summary()","5fd00714":"y_pred_proba=model1.predict(x_const)","57aa6c73":"def pro(y_pred):\n    if y_pred <0.5:\n        y_pred=0\n    elif y_pred>0.5:\n        y_pred=1\n    return y_pred","dc91cfb6":"y_pred=y_pred_proba.apply(pro)","cf76da42":"from sklearn.metrics import accuracy_score,roc_auc_score,roc_curve\nprint('accuracy_ score :',accuracy_score(y,y_pred))","d370c7bd":"print('roc_auc_score:',roc_auc_score(y,y_pred_proba))","10c4a0ae":" # removing the insignificant features and making a model","360c9455":"x_const=x_const.drop(['V2','V3','V6','V7','V11','V12','V15','V16','V17','V18','V19','V23','V24','V25','V26','Time'],axis=1)","117305ed":"model2= sm.Logit(y,x_const).fit()\nmodel2.summary()","03e70147":"y_pred_proba = model2.predict(x_const)\ny_pred=y_pred_proba.apply(pro)","0bbfec46":"print('accuracy_score:', accuracy_score(y_pred,y))","b5b68678":"print('roc_auc_score : ',roc_auc_score(y,y_pred_proba))","d93493e3":"x_train,x_test,y_train,y_test = train_test_split(x,y,test_size=0.3,random_state=1)","397fb99d":"lr= LogisticRegression()\nlr.fit(x_train,y_train)","d07368e2":"y_pred_train=lr.predict(x_train)\ny_pred_test=lr.predict(x_test)\ny_train_prob = lr.predict_proba(x_train)[:,1]\ny_test_proba = lr.predict_proba(x_test)[:,1]","afb7a3b8":"from sklearn.metrics import accuracy_score,roc_auc_score,roc_curve,confusion_matrix,classification_report","b1cb1e61":"print('accuracy score for train :',accuracy_score(y_train,y_pred_train))\nprint('accuracy score for test :',accuracy_score(y_test,y_pred_test))\nprint('roc_auc score for train : ',roc_auc_score(y_train,y_train_prob))\nprint('roc_auc score for test : ',roc_auc_score(y_test,y_test_proba))","cc889842":"from statsmodels.stats.outliers_influence import variance_inflation_factor\ncols=list(x.columns)\nvif= [variance_inflation_factor(x.values,i) for i in range(len(cols))]\npd.DataFrame(vif,cols)","00884871":"x1=x.copy()","7068078d":"x1=x1.drop('Amount',axis=1)","0d2508ba":"X_train,X_test,Y_train,Y_test = train_test_split(x1,y,test_size=0.3,random_state=True)","f90c1d81":"lr1=LogisticRegression()\nlr1.fit(X_train,Y_train)","b5bf7648":"y_pred_train=lr1.predict(X_train)\ny_proba_train = lr1.predict_proba(X_train)[:,1]\ny_pred_test =lr1.predict(X_test)\ny_proba_test = lr1.predict_proba(X_test)[:,1]\n","1ed6cb33":"print('accuracy score for train :',accuracy_score(y_train,y_pred_train))\nprint('accuracy score for test :',accuracy_score(y_test,y_pred_test))\nprint('roc_auc score for train : ',roc_auc_score(y_train,y_proba_train))\nprint('roc_auc score for test : ',roc_auc_score(y_test,y_proba_test))","25e04f12":"from statsmodels.stats.outliers_influence import variance_inflation_factor\ncols=list(x1.columns)\nvif= [variance_inflation_factor(x1.values,i) for i in range(len(cols))]\npd.DataFrame(vif,cols)","46c5503a":"cols = list(x.columns)\npmax = 0\nwhile (len(cols)>1):\n   \n    X_1 = x[cols]\n    X_1 = sm.add_constant(X_1)\n    model2 = sm.Logit(y,X_1).fit()\n    p = model2.pvalues     \n    pmax = max(p)\n    feature_with_p_max = p.idxmax()\n    if(pmax>0.05):\n        cols.remove(feature_with_p_max)\n    else:\n        break\nselected_features_BE = cols\nprint(selected_features_BE)","9079489f":"x2=x[['V1', 'V4', 'V5', 'V7', 'V8', 'V9', 'V10', 'V13', 'V14', 'V16', 'V20', 'V21', 'V22', 'V23', 'V27', 'V28', 'Amount']]\nx2_const = sm.add_constant(x2)\nmodel3= sm.Logit(y,x2_const).fit()\nmodel3.summary()","97ed42ed":"y_pred_proba = model2.predict(x2_const)\ny_pred=y_pred_proba.apply(pro)","2f8580bc":"print('roc_auc_score : ',roc_auc_score(y,y_pred_proba))\nprint('accuracy_score:',accuracy_score(y,y_pred))","ed798da9":"fpr,tpr,threshold = roc_curve(y_train,y_proba_train)\n\nplt.plot(fpr,tpr)\nplt.plot(fpr,fpr,'-r')\nplt.show()","b5c9f51f":"MACHINE LEARNING MODEL","94ee89b8":"Amount has the highest vif ,hence we can remove it and try modeling","25fecb16":"All the above methods gets almost similar accuracy and Roc-Auc score hence not much parameter tuning or feature elimination is required to improve the performance of the model","88d19fd2":"Making the first model using stats","ba24c278":" Hence this is an imbalanced dataset with majority of the class belonging to class 0","b96902d8":"## ROC Curve","f3e0568d":" The p-value of the model is <0.05 (considering alpha to be 0.05)  and hence the model is significant.\n The p-value of individual features shows that  the fetures V2,V3,V6,V7,V11,V12,V15,V16,V17,V18,V19,V23,V24,V25,V26,Time are insignificant as the p-values are greater than the alpha.","cdce544a":"Hence no missing values in the data","933b4665":"From the box plot ,looks like most of the features have huge amount of outliers ,hence we will first keep the ouliers and make the model","a9e855df":"### Importing the libraries to be used","46fc9b79":" All the variables\/ features  are standardised apart from amount and time , hence standardsing it","4246085c":"##### checking multicollinearity using vif\n","0237bf56":"According to vif, all other features don't exhibit mulicollinearity among each other\nAlso the previous accuracy and roc-auc score  shows that the model performance is good for both train and test data","f1b9bb6d":"From the p-value we  can find that the variables V1,V9, and amount are also insignificant","4a35768f":"The accuracy score and roc-auc score shows that their is no significant increase in the performance after removing insignificant features as well","5fe63132":"### Import the dataset","ccd12f24":"#### Backward elimination to check significant features"}}