{"cell_type":{"7901c56c":"code","d31f0426":"code","b5cd276c":"code","2d34e07a":"code","fb3f6bd9":"code","282e54b2":"code","34c0a06a":"code","61e65144":"code","30895bb6":"code","9c2f0e9e":"code","e9e9d9fe":"code","47b70656":"code","24c775d7":"code","f795ec06":"code","b878f1cd":"code","7c470fa1":"code","61e6c05f":"code","059a626a":"code","7029de7a":"code","7ffaf3a1":"code","6b06d751":"code","ffeb43c2":"code","4c2a690b":"code","8a91dafc":"code","3570aa6f":"code","074f966e":"code","5f961114":"code","30f5e183":"code","8c968530":"code","c5ce2c93":"code","0f15a058":"code","7912cd48":"code","6eb7d380":"code","bd91749f":"code","c9e8b277":"code","7a55dad2":"code","1a6befff":"code","9037659f":"code","03806d14":"code","a6bc7f54":"code","f9395031":"code","1f8407e1":"code","3ccd01ef":"code","0c339b6b":"code","8344b1e3":"code","3a293478":"code","3b1bf7ec":"code","2349c840":"code","76d6be35":"code","07d07b2f":"code","a3a2b61f":"code","565ebca9":"code","d4c4a370":"code","6f1ac778":"code","070f646b":"code","e27227c2":"code","d61e31f7":"code","e30741da":"code","966cb9bc":"code","c55d2aa7":"code","7a04f0b0":"code","77022eaa":"code","bcdb1fac":"code","ebfbcc3c":"code","8abb4f4f":"code","4bb5cf66":"code","d8227b62":"code","6ea5d1ab":"code","96c0984f":"code","6babcaa3":"code","caab6bae":"code","84b48138":"code","f041eab2":"code","32bdd72c":"code","46994bc0":"code","44923e20":"code","e70affd7":"code","f632871d":"code","1457981a":"code","0315e694":"code","ccab9997":"code","8efd7b55":"code","b55e501c":"code","34f01d21":"code","3f2fa1c2":"code","8820ec07":"code","2502dd4b":"code","5081145b":"code","02201865":"code","56a03595":"code","f7c86449":"code","362e622b":"code","ef2a4339":"code","5e385ce1":"markdown","ffa4dd25":"markdown","8d76dd50":"markdown","f1b21982":"markdown","03d36e2d":"markdown","0b21753a":"markdown","44a1954b":"markdown","2be30c1d":"markdown","50de6153":"markdown","a9b4d71c":"markdown","7c8c7c89":"markdown","e1a86833":"markdown","d3833c5d":"markdown","15cc24c9":"markdown","c8b76289":"markdown","251b639c":"markdown","57096aa3":"markdown","c234b28b":"markdown","853fa609":"markdown","8c2e21b3":"markdown","c5ee6f31":"markdown","cf38638c":"markdown","f0fa3884":"markdown","b41fc6cf":"markdown","997f4dec":"markdown","b55714ee":"markdown","8f98de68":"markdown","fd4f1dcd":"markdown","0306d609":"markdown","87bce70c":"markdown","fde92917":"markdown","a604b2cc":"markdown","ed648590":"markdown","f060036e":"markdown","1513375f":"markdown","977cd293":"markdown","7ff2e253":"markdown","0dbd994f":"markdown","3e9338ed":"markdown","07e823d6":"markdown","8d12480b":"markdown","6024279f":"markdown","c84d3463":"markdown","2a12f552":"markdown","6f932a23":"markdown","49839877":"markdown","991d987d":"markdown","9deee93d":"markdown","35d0158c":"markdown","c42f492e":"markdown","842dd782":"markdown","3b0382de":"markdown","fd205424":"markdown","19e6e4a2":"markdown","e3fa9d68":"markdown","6ca7c9c4":"markdown","e385589b":"markdown","9ae1424b":"markdown","76c3c39a":"markdown","9e4c579a":"markdown"},"source":{"7901c56c":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport math\nplt.style.use('fivethirtyeight')\n\ndf=pd.read_csv('..\/input\/time-series-forecasting-with-yahoo-stock-price\/yahoo_stock.csv')","d31f0426":"from keras.optimizers import Adam\nimport statistics","b5cd276c":"df.head()","2d34e07a":"df.shape","fb3f6bd9":"len(df['Date'].unique())","282e54b2":"df.info()","34c0a06a":"df.dtypes","61e65144":"df.isnull().sum()","30895bb6":"df.Date=pd.to_datetime(df['Date'])","9c2f0e9e":"df.Date.min(),df.Date.max()","e9e9d9fe":"df.Date.max()-df.Date.min()","47b70656":"df.set_index('Date',inplace=True)","24c775d7":"df.head()","f795ec06":"df[['High','Low','Open','Close']].plot(figsize = (15, 5), alpha = 0.5)","b878f1cd":"df.High.hist(bins=50)","7c470fa1":"np.round(df.shape[0]\/10,0)","61e6c05f":"from statistics import stdev\nmean=[]\nstd=[]\n    \nfor i in range(0,10):\n    mean.append(df['High'].iloc[(i*182):(i*182)+182].mean())\n    std.append(stdev(df['High'].iloc[(i*182):(i*182)+182]))","059a626a":"pd.concat([pd.DataFrame(mean,columns=['mean']),pd.DataFrame(std,columns=['std'])], axis=1)","7029de7a":"from statsmodels.tsa.seasonal import seasonal_decompose","7ffaf3a1":"decompose_add=seasonal_decompose(df['High'], model='additive', period=12)\nplt.figure(figsize=(15,15))\nplt.subplot(411)\nplt.plot(df['High'], label='Original TS')\nplt.legend(loc='best')\nplt.subplot(412)\nplt.plot(decompose_add.trend, label='Trend')\nplt.legend(loc='best')\nplt.subplot(413)\nplt.plot(decompose_add.seasonal,label='Seasonality')\nplt.legend(loc='best')\nplt.subplot(414)\nplt.plot(decompose_add.resid, label='Residual')\nplt.legend(loc='best')","6b06d751":"decompose_mul=seasonal_decompose(df['High'], model='multiplicative', period=12)\nplt.figure(figsize=(15,15))\nplt.subplot(411)\nplt.plot(df['High'], label='Original TS')\nplt.legend(loc='best')\nplt.subplot(412)\nplt.plot(decompose_mul.trend, label='Trend')\nplt.legend(loc='best')\nplt.subplot(413)\nplt.plot(decompose_mul.seasonal,label='Seasonality')\nplt.legend(loc='best')\nplt.subplot(414)\nplt.plot(decompose_mul.resid, label='Residual')\nplt.legend(loc='best')","ffeb43c2":"from statsmodels.graphics.tsaplots import plot_acf\nfrom statsmodels.graphics.tsaplots import plot_pacf","4c2a690b":"plt.rc(\"figure\", figsize=(10,5))\nplot_acf(df['High'])\nprint()","8a91dafc":"plt.rc(\"figure\", figsize=(10,5))\nplot_pacf(df['High'])\nprint()","3570aa6f":"from statsmodels.tsa.stattools import adfuller","074f966e":"result = adfuller(df['High'])\nprint('ADF Statistic: %f' % result[0])\nprint('p-value: %f' % result[1])\nprint('Critical Values:')\nfor key, value in result[4].items():\n    print('\\t%s: %.3f' % (key, value))","5f961114":"from numpy import log\n\nresult = adfuller(log(df['High']))\nprint('ADF Statistic: %f' % result[0])\nprint('p-value: %f' % result[1])\nprint('Critical Values:')\nfor key, value in result[4].items():\n    print('\\t%s: %.3f' % (key, value))","30f5e183":"new_df=df['High'].iloc[:-4]","8c968530":"train_len = math.ceil(len(new_df)*0.8)\ntrain_len","c5ce2c93":"window=10","0f15a058":"train_data = new_df[0:train_len]\n\nX_train=[]\nY_train=[]\n\nfor i in range(window, len(train_data)):\n    X_train.append(train_data[i-window:i])\n    Y_train.append(train_data[i])","7912cd48":"X_train, Y_train= np.array(X_train), np.array(Y_train)","6eb7d380":"X_train = np.reshape(X_train, (X_train.shape[0], X_train.shape[1], 1))\nX_train.shape","bd91749f":"X_train","c9e8b277":"from keras.models import Sequential\nfrom keras.layers import Dense, LSTM, Dropout","7a55dad2":"model=Sequential()\nmodel.add(LSTM(50, activation='relu', input_shape=(X_train.shape[1],1)))\nmodel.add(Dense(25))\nmodel.add(Dense(1))\nmodel.compile(loss='mean_squared_error', optimizer='adam')\nmodel.summary()\nmodel.fit(X_train, Y_train, epochs=10, batch_size=10, verbose=0)","1a6befff":"test_data = new_df[train_len-window:]\n\nX_val=[]\nY_val=[] \n\nfor i in range(window, len(test_data)):\n    X_val.append(test_data[i-window:i])\n    Y_val.append(test_data[i])","9037659f":"X_val, Y_val = np.array(X_val), np.array(Y_val)\nX_val = np.reshape(X_val, (X_val.shape[0], X_val.shape[1],1))","03806d14":"X_val.shape, Y_val.shape","a6bc7f54":"prediction = model.predict(X_val)","f9395031":"len(prediction), len(Y_val)","1f8407e1":"from sklearn.metrics import mean_squared_error\n\nlstm_train_pred = model.predict(X_train)\nlstm_valid_pred = model.predict(X_val)\nprint('Train rmse:', np.sqrt(mean_squared_error(Y_train, lstm_train_pred)))\nprint('Validation rmse:', np.sqrt(mean_squared_error(Y_val, lstm_valid_pred)))","3ccd01ef":"valid = pd.DataFrame(new_df[train_len:])\nvalid['Predictions']=lstm_valid_pred\nvalid","0c339b6b":"plt.plot(valid[['High','Predictions']])\nplt.legend(['Validation','Predictions'])\nplt.show()","8344b1e3":"train = new_df[:train_len]\nvalid = pd.DataFrame(new_df[train_len:])\nvalid['Predictions']=lstm_valid_pred\n\nplt.figure(figsize=(16,8))\nplt.title('Model LSTM')\nplt.xlabel('Date')\nplt.ylabel('Close Price USD')\nplt.plot(train)\nplt.plot(valid[['High','Predictions']])\nplt.legend(['Train','Val','Predictions'])\nplt.show()","3a293478":"train_error=[]\nval_error=[]\n\nwindow_number=[5,8,10,15,20,30,40]\nfor i in window_number:\n    #_____________________________________________________________________\n    train_data = new_df[0:train_len]\n\n    X_train=[]\n    Y_train=[]\n\n    for i in range(window, len(train_data)):\n        X_train.append(train_data[i-window:i])\n        Y_train.append(train_data[i])\n\n    X_train, Y_train= np.array(X_train), np.array(Y_train)\n    X_train = np.reshape(X_train, (X_train.shape[0], X_train.shape[1], 1))\n    #______________________________________________________________________\n    test_data = new_df[train_len-window:]\n\n    X_val=[]\n    Y_val=[] \n\n    for i in range(window, len(test_data)):\n        X_val.append(test_data[i-window:i])\n        Y_val.append(test_data[i])\n\n    X_val, Y_val = np.array(X_val), np.array(Y_val)\n    X_val = np.reshape(X_val, (X_val.shape[0], X_val.shape[1],1))\n    #______________________________________________________________________\n    model=Sequential()\n    model.add(LSTM(50, activation='relu', input_shape=(X_train.shape[1],1)))\n    model.add(Dense(25))\n    model.add(Dense(1))\n    model.compile(loss='mean_squared_error', optimizer='adam')\n    model.fit(X_train, Y_train, epochs=10, batch_size=10, verbose=0)\n    #______________________________________________________________________\n    lstm_train_pred = model.predict(X_train)\n    lstm_valid_pred = model.predict(X_val)\n    train_error.append(np.sqrt(mean_squared_error(Y_train, lstm_train_pred)))\n    val_error.append(np.sqrt(mean_squared_error(Y_val, lstm_valid_pred)))","3b1bf7ec":"train_error","2349c840":"val_error","76d6be35":"pd.concat([pd.DataFrame(train_error,columns=['train_error']),\n           pd.DataFrame(val_error,columns=['val_error']),\n           pd.DataFrame([5,8,10,15,20,30,40],columns=['window'])], axis=1).set_index('window')","07d07b2f":"window=10\n\ntrain_data = new_df[0:train_len]\nX_train=[]\nY_train=[]\nfor i in range(window, len(train_data)):\n    X_train.append(train_data[i-window:i])\n    Y_train.append(train_data[i])\n\nX_train, Y_train= np.array(X_train), np.array(Y_train)\nX_train = np.reshape(X_train, (X_train.shape[0], X_train.shape[1], 1))\n#____________________________________________________________________\ntest_data = new_df[train_len-window:]\nX_val=[]\nY_val=[] \nfor i in range(window, len(test_data)):\n    X_val.append(test_data[i-window:i])\n    Y_val.append(test_data[i])\n    \nX_val, Y_val = np.array(X_val), np.array(Y_val)\nX_val = np.reshape(X_val, (X_val.shape[0], X_val.shape[1],1))","a3a2b61f":"model=Sequential()\nmodel.add(LSTM(50,return_sequences=True, activation='relu', input_shape=(X_train.shape[1],1)))\nmodel.add(LSTM(50,return_sequences=False,activation='relu'))\nmodel.add(Dense(100))\nmodel.add(Dense(25))\nmodel.add(Dense(1))\nopt1=Adam(learning_rate=0.001,beta_1=0.9,beta_2=0.999)\nmodel.compile(loss='mean_squared_error', optimizer=opt1)\nmodel.summary()\nmodel.fit(X_train, Y_train, epochs=100, batch_size=10, verbose=0)","565ebca9":"lstm_train_pred = model.predict(X_train)\nlstm_valid_pred = model.predict(X_val)\nprint('Train rmse:', np.sqrt(mean_squared_error(Y_train, lstm_train_pred)))\nprint('Validation rmse:', np.sqrt(mean_squared_error(Y_val, lstm_valid_pred)))","d4c4a370":"valid = pd.DataFrame(new_df[train_len:])\nvalid['Predictions']=lstm_valid_pred","6f1ac778":"plt.plot(valid[['High','Predictions']])\nplt.legend(['Validation','Predictions'])\nplt.show()","070f646b":"r1=[]\nr2=[]\n\nfor i in range(0,10):\n    model=Sequential()\n    model.add(LSTM(50,return_sequences=True, activation='relu', input_shape=(X_train.shape[1],1)))\n    model.add(LSTM(50,return_sequences=False,activation='relu'))\n    model.add(Dense(100))\n    model.add(Dense(25))\n    model.add(Dense(1))\n    opt1=Adam(learning_rate=0.001,beta_1=0.9,beta_2=0.999)\n    model.compile(loss='mean_squared_error', optimizer=opt1)\n    model.fit(X_train, Y_train, epochs=100, batch_size=10,verbose=0)\n\n    lstm_train_pred = model.predict(X_train)\n    lstm_valid_pred = model.predict(X_val)\n    r1.append(np.round(np.sqrt(mean_squared_error(Y_train, lstm_train_pred)),2))\n    r2.append(np.round(np.sqrt(mean_squared_error(Y_val, lstm_valid_pred)),2))","e27227c2":"r1, statistics.mean(r1), statistics.stdev(r1)","d61e31f7":"r2, statistics.mean(r2), statistics.stdev(r2)","e30741da":"r1=[]\nr2=[]\n\nfor i in range(0,10):\n    model=Sequential()\n    model.add(LSTM(50,return_sequences=True, activation='relu', input_shape=(X_train.shape[1],1),recurrent_dropout=0.2))\n    model.add(LSTM(50,return_sequences=False,activation='relu'))\n    model.add(Dense(100))\n    model.add(Dropout(0.2))\n    model.add(Dense(25))\n    model.add(Dense(1))\n    opt1=Adam(learning_rate=0.001,beta_1=0.9,beta_2=0.999)\n    model.compile(loss='mean_squared_error', optimizer=opt1)\n    model.fit(X_train, Y_train, epochs=100, batch_size=10,verbose=0)\n\n    lstm_train_pred = model.predict(X_train)\n    lstm_valid_pred = model.predict(X_val)\n    r1.append(np.round(np.sqrt(mean_squared_error(Y_train, lstm_train_pred)),2))\n    r2.append(np.round(np.sqrt(mean_squared_error(Y_val, lstm_valid_pred)),2))","966cb9bc":"r1, statistics.mean(r1), statistics.stdev(r1)","c55d2aa7":"r2, statistics.mean(r2), statistics.stdev(r2)","7a04f0b0":"from keras.layers import SimpleRNN","77022eaa":"r1=[]\nr2=[]\n\nfor i in range(0,10):\n    model=Sequential()\n    model.add(SimpleRNN(50,return_sequences=True, activation='relu', input_shape=(X_train.shape[1],1)))\n    model.add(SimpleRNN(50,return_sequences=False,activation='relu'))\n    model.add(Dense(100))\n    model.add(Dense(25))\n    model.add(Dense(1))\n    opt1=Adam(learning_rate=0.001,beta_1=0.9,beta_2=0.999)\n    model.compile(loss='mean_squared_error', optimizer=opt1)\n    model.fit(X_train, Y_train, epochs=100, batch_size=10,verbose=0)\n\n    lstm_train_pred = model.predict(X_train)\n    lstm_valid_pred = model.predict(X_val)\n    r1.append(np.round(np.sqrt(mean_squared_error(Y_train, lstm_train_pred)),2))\n    r2.append(np.round(np.sqrt(mean_squared_error(Y_val, lstm_valid_pred)),2))","bcdb1fac":"r1, statistics.mean(r1), statistics.stdev(r1)","ebfbcc3c":"r2, statistics.mean(r2), statistics.stdev(r2)","8abb4f4f":"valid = pd.DataFrame(new_df[train_len:])\nvalid['Predictions']=lstm_valid_pred\nvalid","4bb5cf66":"plt.plot(valid[['High','Predictions']])\nplt.legend(['Validation','Predictions'])\nplt.show()","d8227b62":"import tensorflow \n\nr1=[]\nr2=[]\n\nmodel=Sequential()\nmodel.add(SimpleRNN(50,return_sequences=True, activation='relu', input_shape=(X_train.shape[1],1)))\nmodel.add(SimpleRNN(50,return_sequences=False,activation='relu'))\nmodel.add(Dense(100))\nmodel.add(Dense(25))\nmodel.add(Dense(1))\nlr_schedule = tensorflow.keras.callbacks.LearningRateScheduler(\n    lambda epoch: 1e-5 * 10**(epoch \/ 85))\nopt1=Adam(learning_rate=1e-5,beta_1=0.9,beta_2=0.7)\nmodel.compile(loss='mean_squared_error', optimizer=opt1)\nhistory=model.fit(X_train, Y_train, epochs=100, batch_size=10,verbose=2, callbacks=[lr_schedule])\n\nlstm_train_pred = model.predict(X_train)\nlstm_valid_pred = model.predict(X_val)\nr_train_new=np.round(np.sqrt(mean_squared_error(Y_train, lstm_train_pred)),2)\nr_val_new=np.round(np.sqrt(mean_squared_error(Y_val, lstm_valid_pred)),2)","6ea5d1ab":"plt.semilogx(history.history[\"lr\"], history.history[\"loss\"])\nplt.axis([1e-5, 5e-4, 0, 1000])","96c0984f":"r1=[]\nr2=[]\n\nfor i in range(0,10):\n    model=Sequential()\n    model.add(SimpleRNN(50,return_sequences=True, activation='relu', input_shape=(X_train.shape[1],1)))\n    model.add(SimpleRNN(50,return_sequences=False,activation='relu'))\n    model.add(Dense(100))\n    model.add(Dense(25))\n    model.add(Dense(1))\n    opt1=Adam(learning_rate=1e-4,beta_1=0.9,beta_2=0.7)\n    model.compile(loss='mean_squared_error', optimizer=opt1)\n    model.fit(X_train, Y_train, epochs=100, batch_size=10,verbose=0)\n\n    lstm_train_pred = model.predict(X_train)\n    lstm_valid_pred = model.predict(X_val)\n    r1.append(np.round(np.sqrt(mean_squared_error(Y_train, lstm_train_pred)),2))\n    r2.append(np.round(np.sqrt(mean_squared_error(Y_val, lstm_valid_pred)),2))","6babcaa3":"r1, statistics.mean(r1), statistics.stdev(r1)","caab6bae":"r2, statistics.mean(r2), statistics.stdev(r2)","84b48138":"valid = pd.DataFrame(new_df[train_len:])\nvalid['Predictions']=lstm_valid_pred\nvalid","f041eab2":"plt.plot(valid[['High','Predictions']])\nplt.legend(['Validation','Predictions'])\nplt.show()","32bdd72c":"last_10_days=new_df[-10:].values\nX_test=[]\nX_test.append(last_10_days)\nX_test=np.array(X_test)\nX_test=np.reshape(X_test,(X_test.shape[0],X_test.shape[1],1))\npred_price=model.predict(X_test)\nprint(pred_price)","46994bc0":"df['High'].iloc[-4]","44923e20":"df['High'].iloc[-4]-pred_price","e70affd7":"df.High.tail(14)","f632871d":"last_9_days=new_df[-9:].values\nX_test=[]\nX_test=np.append(last_9_days,pred_price)\nX_test=np.array(X_test)\nX_test\nX_test=np.reshape(X_test,(1,X_test.shape[0],1))\npred_price2=model.predict(X_test)\nprint(pred_price2)","1457981a":"df['High'].iloc[-3]","0315e694":"df['High'].iloc[-3]-pred_price2","ccab9997":"last_8_days=new_df[-8:].values\nX_test=[]\nX_test=np.append(last_8_days,pred_price)\nX_test=np.append(X_test,pred_price2)\nX_test=np.array(X_test)\nX_test\nX_test=np.reshape(X_test,(1,X_test.shape[0],1))\npred_price3=model.predict(X_test)\nprint(pred_price3)","8efd7b55":"df['High'].iloc[-2]","b55e501c":"df['High'].iloc[-2]-pred_price3","34f01d21":"last_7_days=new_df[-7:].values\nX_test=[]\nX_test=np.append(last_7_days, pred_price)\nX_test=np.append(X_test, pred_price2)\nX_test=np.append(X_test, pred_price3)\nX_test=np.array(X_test)\nX_test\nX_test=np.reshape(X_test,(1,X_test.shape[0],1))\npred_price4=model.predict(X_test)\nprint(pred_price4)","3f2fa1c2":"df['High'].iloc[-1]","8820ec07":"df['High'].iloc[-1]-pred_price4","2502dd4b":"df.High.iloc[-4], df.High.iloc[-3], df.High.iloc[-2], df.High.iloc[-1]","5081145b":"pred_price, pred_price2, pred_price3, pred_price4","02201865":"actual_prices=np.array([df.High.iloc[-4], df.High.iloc[-3], df.High.iloc[-2], df.High.iloc[-1]])\npred_prices=np.array([float(pred_price),float(pred_price2),float(pred_price3),float(pred_price4)])","56a03595":"actual_prices, pred_prices","f7c86449":"data={'Actual values':[df.High.iloc[-4],df.High.iloc[-3],df.High.iloc[-2],df.High.iloc[-1]],\n      'Predicted values':[float(pred_price),float(pred_price2),float(pred_price3),float(pred_price4)]}\n\npd.DataFrame(data, index=['2020-11-17','2020-11-18','2020-11-19','2020-11-20'])","362e622b":"fut=pd.DataFrame(data, index=['2020-11-17','2020-11-18','2020-11-19','2020-11-20']).plot()\nfut.set(ylim=(3540,3700))","ef2a4339":"np.round(np.sqrt(mean_squared_error(actual_prices, pred_prices)),2)","5e385ce1":"## 4th comparison:  \nUntil now every model built does not offer us a good accuracy, as we are dealing with the budget of a company this results can lead to a completely wrong decision, but as I said there is still a room of improvement and different approaches must be taken. We were forgeting the simplest type of RNN which is known for being more affected by vanishing gradient, however for our dataset it worked as follows (Again the model will be run 10 times):","ffa4dd25":"Above, the last 4 values correspond to the actual values we want to predict, and the time steps used does not comprehend these rather will be used the predicted ones as we run the model.  \nAs we predicted one value (pred_price) the time steps used to predict a new one will take the last 9 of new_df and pred_price, as follows:","8d76dd50":"Nice!, let's see what was the actual value:","f1b21982":"The difference is less than $18 USD and the model seems to be working very good.","03d36e2d":"Finally, the plot showing training, validation and prediction curves:","0b21753a":"This time the difference was:","44a1954b":"Now that I have the best hyperparameters I will build again the model and evaluate its performance 15 times:","2be30c1d":"Actual value:","50de6153":"The four had almost same behaviour troughout time and based on this assumption in this project we will only focus on one of them 'High' in order to build a model which could predict future values and then as possible suggestions extrapolate such model to other indicators.  \nFirstly, we will demonstrate if the time-series problem corresponds to a Non-Stationary type, which characterizes for having:  \n- Non-constant variance.  \n- Non-constant mean.  \n- Seasonality.  \n- High autocorrelation. \n\nThere are four well known ways to evaluate if the serie meets the criteria just mentioned, these corresponds to:  \n- Sequence visualization.\n- Histogram, Autocorrelation and Partial Aurocorrelation plots.\n- Statistical summary of chunks.\n- Adfuller test.\n\n","a9b4d71c":"Above we see how both measures are clearly different in each chunk. However looking at the plot we could assume a constant variance as the ripple in the curve, but without a doubt trend is the most outstanding feature. Until now can assume it's a non-stationary serie, but to know more about it we will evaluate two other methods.  \nSeasonal decomposition is a function from statsmodels library which allows us to decompose the serie into trend, seasonal and residual, either additive or multiplicative:","7c8c7c89":"In the summary above we can see that the total parameters of this new model is three times or even more than the prior which is indicative of the complexity and the time it takes to train.","e1a86833":"As the date column is wrongly set as object type, let's change it to Datetime type:","d3833c5d":"The following code will help us visualize these values in a table and plot:","15cc24c9":"Clearly, the gap was reduced significantly and the predicted curve even responded quickly to the 'high frequencies' or a.k.a. ripple in the actual curve. As this model corresponds to the best one we will continue working with this.\n\nIn order to get a bit better results from the previous model I will use a callback to find the learning rate wich offers the lowest MSE loss. I have already shorten the range to [1e-5 - 1e-3] and found that around 1e-4 and beta2 = 0.7 occurrs the lowest error, therefore these were chosen to rebuild the model:","c8b76289":"# Modeling:  \nIn order to choose the best model 4 changes in hyperparameters and architecture will be applied and then evaluate each one by computing their corresponding error metrics. The purpose of this method is to find the appropriate characteristics of the model by looking at the effect of regularization, window length, number of epochs and type of cell. Each step will be denominated \u2018comparison\u2019 and are detailed as follows:  \n\n\u25cf 1st comparison, model with 7 different window length.  \n\u25cf 2nd comparison, model with more layers, neurons and epochs.  \n\u25cf 3rd comparison, model with regularization.  \n\u25cf 4th comparison, model using SimpleRNN vs LSTM.  \n\nFirstly, we will define a new dataset equal to the existing one, but omitting the last four records, later we will use the model to predict such values.","251b639c":"We can see how validation errors are higher than training and also more spreaded because of such stardard deviation.","57096aa3":"Now, let's find the corresponding actual value in df:","c234b28b":"Let's use 10 as a random window to be used in the model to build:","853fa609":"Our dataset contains 6 columns of indicators and date columns which will be then set as index. Now let's see if the dates are unique or if there are duplicate values.","8c2e21b3":"The following lines create the training sets, as we now the first row takes the first 10 time steps and then the second row takes time steps shifted in one and so on and so forth:","c5ee6f31":"Based on every method applied we can endorse the serie clearly corresponds to a non-stationary type, which means that we have to convert this to stationary by using filters and complex transformations. However, deep learning approaches can lead us to build models which takes into account all of these characteristics and predict future values modestly, but as disadvantage it would take significantly more time to train such models. ","cf38638c":"To compute a new prediction let's use the same logic as before:","f0fa3884":"## 1st comparison:  \nIn order to tackle down this 'shift' and thus reduce the error we have to find the suit window number, for this we will compute the RMSE for the following number of windows: 5,8,10,15,20,30,40. Then compare results and find the lowest.  ","b41fc6cf":"Finally, we will make the Augmented Dickey-Fuller test, using 5% as significance level and declaring the following hypothesis:\n- H0= Serie corresponds to non-stationary type.\n- H1= Serie corresponds to stationary type.","997f4dec":"**Wow!, even less than half the prior error metrics and what is more important is the fact that stardard deviation is much lower, which tells us that the model offer a more stable performance using RNNs rather than LSTMs.**  \nTranslating this to a more meaningful way the average error in the validation set reaches 41 USD with a std a bit higher than 4 USD.   \nLet's plot the actual and predicted values for the validation set and see how reduced was the 'shift':","b55714ee":"## Key findings:  \nDefinitely Time-series forecasting is incredibly complex, despite the fact that using Deep Learning approaches somehow increases the accuracy it also makes more difficult the interpretability and particularly for this dataset it did not work stably, I mean every time I run this code the results were much significantly different, even more than what I am used to see. Nevertheless is the long time it took to train the models and in order to build a more complex architecture as a hypothesis to increase accuracy it must be taken into account.  \n\nOne outstanding detail about the performance of the best model chosen is the low variance in the error metrics, which makes this the most stable system and gave us the key factor in this project. Also the gap seems to have been dropped to an insignificant level, but do not misinterpret this as looking at the plot both curves looks almost the same, however they have a big difference quantified as RMSE reaching 40 USD which makes a big difference in the budget.  \n\nThe prediction of 'future' values is a task whose risk increases exponentially as we want to predict more time steps becoming non-reliable, this is because we are comprehending predicted values which obviously has certain error and trying to predict new ones adds even more error in this case 'additive'. Once we understand this effect in our context would not be recommended to predict more than two time steps so as to not lead to wrong decisions. \n\nIn the last plot of actual and predicted future values shows that our prediction had a linear behaviour keeping a constant slope through time which indicates the model needs some regularization or dropout which could add non-linearity. This idea was implemented to the current best model, but the result were horribly bad, even worse than LSTMs using Dropout, making us again take different approaches if we wish to improve the model.  \n\nFinally, the current model is only recommended for learning purposes as particularly I would not use it to predict economical indicators of any company, having said this the model and work achieved can be extrapolated to the other 3 indicators in the original dataset. \n","8f98de68":"We can see above how these values either training or validation were higher than without dropout, which is definitely not good.  \n**Note:** Different levels of dropout were used above from 0.1 to 0.6 obtaining similar or higher errors. Now that we saw dropout did not reduce the errors we can not attribute the problem to overfitting and a different approach must be taken. For your consideration I have also changed the optimizer to Adadelta, Adamax, RMSProp and SGD but everyone had higher error or simply did not work, the activation function was changed to tanh and sigmoid obtaining similar errors.\n","fd4f1dcd":"Both errors are high, if we specifically see the validation one we could say it's too much and we aim to improve this as we build a robust model in the next steps.  \nBelow is a table which merged the actual and predicted values, therefore we can see for each record how differ these two and have an idea of how sidetracked we currently are.","0306d609":"The following is the first model to be used, this is relatively simple with one LSTM layer with relu activation function and one hidden fully connected layer, the optimizer used is Adam, number of epochs=10, batch_size=10 and loss function=Mean squared error:","87bce70c":"The following is the architecture of the model to be used (2 LSTM layers and 2 hidden fully-connected layers), the optimizer is more specific and the number of epochs was increased to 100:","fde92917":"## 3rd comparison:   \nAs the validation errors were still considerably higher than training we think one reason could be due to overfitting and for this problem we have to add regularization to the model chosen by using Dropout after Fully Connected layers and LSTM cells. In this step we will run a regularized model again 10 times in order to see the variation in the results:","a604b2cc":"We can't assume a uniform nor normal distribution in the plot above, because of that the statistical summary can helps us more, for this we will split the data into 10 chunks and compute mean and variance of each one.","ed648590":"As there are 1825 unique dates means that each one corresponds to unique records in the table, therefore we don't have duplicated or inconsistent values.","f060036e":"# Main Objectives:  \nThe scope of this project is to build several deep learning algorithms based on RNN\ntechniques which can predict future values of an indicator using Time-Series Forecasting methods in order to achieve the highest possible accuracy. This can be broken down into the following milestones:  \n\n1. Data Exploration and evaluation of Stationarity.  \n2. Modeling and selection of best model.  \n3. Prediction of future values.  ","1513375f":"As p-value is greater than 0.05 we fail to reject the null hypothesis, therefore there is not enough evidence to reject that we are working with a non-stationary series. Let's apply the log transformation to the serie and test again.","977cd293":"As we have values until 16-11-2020 we can predict the next one using the model built and compare the outcome with the actual value saved in the original dataframe 'df':","7ff2e253":"The plot above shows how sidetracked our prediction is from the actual values, this will impact our conclussion about complexity of model vs reliable forecasting, now will be computed the correspoding RMSE to see this difference quantified:","0dbd994f":"# Predicting 'future' values: ","3e9338ed":"Let's plot four of the indicators in the table and differentiate their corresponding curves by colours.","07e823d6":"Now that our validation set has the correct shape we can use it in the model to predict the next value.","8d12480b":"The following table shows the RMSE of train and validation sets for each of the 7 windows:","6024279f":"Now, let's see the four actual and predicted future values:","c84d3463":"The following histogram plot should have a uniform distribution meaning non-constant mean or 'trend':","2a12f552":"## Suggestions:  \nHaving the scope of improve the prediction I would take into account some approaches of Linear Regression as these comprehend several features ideally highly correlated to the label, so a good idea and future work would be to implement or combine Lasso\/Ridge Regression with Time-Series Forecasting. I am relatively novice in ML, really don't know if this already exist, but in the following weeks I will be researching more to do exactly this.","6f932a23":"# Evaluating Stationarity:","49839877":"Below we can see the RMSE for each time, note that these are considerably different, being still more impactful in the validation set.","991d987d":"Creating the validation set using same logic as training:","9deee93d":"The four metrics were reduced by chosing the right learning rate and betas, this made our model a bit more reliable as we will see next in the prediction:","35d0158c":"Reshaping the training set to (number of records-window, number of time steps, 1):","c42f492e":"Predicting 4 future values:","842dd782":"Below we can see a simple plot showing both curves, in which clearly there is a difference and a sort of shift to the right or delay in the prediction curve.","3b0382de":"As our prediction and actual values have the same shape we can use these sets to compute the error metrics, in this case we will use RMSE.","fd205424":"Let's compute the amount of days between the limits in the table, i.e. 2020-11-20\/2015-11-23, take into account that the result will give us the days-1:","19e6e4a2":"Looking carefully to the original curve we can say the additive decomposition makes more sense as the trend does not seem to be changing by the multiplication with seasonal component, rather every component seems to being added up to create the original curve. Independent of this there is a trend and seasonal component.  \nBelow we can see the ACF and PACF plots:","e3fa9d68":"Setting the date as index will make our time series plots much more understandable.","6ca7c9c4":"Difference: ","e385589b":"## 2nd comparison:  \nOne efficient way to improve the accuracy is by simply increasing the complexity of the model and this can be achieved adding more layers and more LSTM cells. Therefore in this step we will build a multilayered model and compare its performance with the prior.  \nFirstly, let's create again the training and validation sets for 10 windows to be used.\n","9ae1424b":"Despite the fact that error metrics were lower than before in the plot above still can see a gap between the actual and predicted values. Again, I've found that each time I ran this model the results were different and the current output corresponds to the very best one, but in order to better vizualize this I will run 10 times the same model and compute the errors in each one.","76c3c39a":"We define the length of the training set as 80% of the total records (specifically the first 80% of data, i.e.: from record 0 to record 1456):","9e4c579a":"**Note: Each time that I ran the prior code of 7 windows the outcome was different**, but in general the behaviour was as in the table above, having window=10 the lowest RMSE for validation set. Thus this number was chosen as the best predictor. Even though before we have seen how for this value the shift was significant for all other windows this difference was much more, but there are still more hyperparameters to tune in order to improve the accuracy."}}