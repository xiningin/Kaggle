{"cell_type":{"cac6b6c3":"code","38c364c6":"code","ba133df3":"code","203f44f1":"code","4a3beb60":"code","5e261fcb":"code","4969c20e":"code","1d25815f":"code","356f6b53":"code","9600d9e3":"code","fbb4cf74":"code","56452c29":"code","4d8600e9":"code","553dabb8":"code","0683e74e":"code","24c04387":"markdown","0563f086":"markdown","f0d9362f":"markdown","15756f86":"markdown","892b8b55":"markdown","0bb7f27f":"markdown","3e9571d5":"markdown","722952ce":"markdown","d0b93ecb":"markdown","532a77d0":"markdown","a2c71fa6":"markdown","41bd2779":"markdown","f552c323":"markdown","64956384":"markdown","eac8009b":"markdown","dc2bca44":"markdown","ec8f5784":"markdown","c2e6519d":"markdown","66f13f62":"markdown","9b312395":"markdown","d97cf38a":"markdown","509bc301":"markdown","6fabd9c9":"markdown"},"source":{"cac6b6c3":"import numpy as np\nimport pandas as pd \nfrom collections import Counter\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.pipeline import Pipeline\nfrom imblearn.under_sampling import RandomUnderSampler\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.metrics import classification_report, roc_auc_score, accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","38c364c6":"df = pd.read_csv('\/kaggle\/input\/creditcardfraud\/creditcard.csv')\ndf","ba133df3":"sns.countplot(data=df, x='Class')","203f44f1":"df.isnull().sum()","4a3beb60":"df.describe()","5e261fcb":"df.dtypes","4969c20e":"print(df.shape)\ndf.drop_duplicates(keep='first', inplace=True)\nprint(df.shape)","1d25815f":"y = df['Class']\nX = df.drop('Class', axis=1)","356f6b53":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)","9600d9e3":"print('Original dataset shape %s' % Counter(y_train))\nrus = RandomUnderSampler(sampling_strategy='all', random_state=42)\nX_res, y_res = rus.fit_resample(X_train, y_train)\nprint('Resampled dataset shape %s' % Counter(y_res))","fbb4cf74":"clf_pipe = Pipeline(steps=[\n    ('scaler', MinMaxScaler()),\n    ('clf', MLPClassifier(hidden_layer_sizes=(100,), max_iter= 500, activation='relu', solver='adam',random_state=42))\n])","56452c29":"clf_pipe.fit(X_res, y_res)","4d8600e9":"y_pred = clf_pipe.predict(X_test)","553dabb8":"def evaluate_model(clf, y_test, y_pred):\n    s1 = accuracy_score(y_test, y_pred).round(3)\n    s2 = precision_score(y_test, y_pred, average='weighted').round(3)\n    s3 = recall_score(y_test, y_pred, average='weighted').round(3)\n    s4 = f1_score(y_test, y_pred, average='weighted').round(3)\n    s5 = roc_auc_score(y_test, clf.predict_proba(X_test)[:,1], average='weighted').round(3)\n    print(\"Accuracy \", s1)\n    print(\"Precision \", s2)\n    print(\"Recall \", s3)\n    print(\"f1 score \", s4)\n    print(\"ROC_AUC \", s5)\n    cm = confusion_matrix(y_test, y_pred, normalize='true')\n    sns.heatmap(cm, annot=True)","0683e74e":"evaluate_model(clf_pipe, y_test, y_pred)","24c04387":"The pre-processing done and the model implemented has combined to produce a very good performance on the test set as reflected by the various scoring metrics. ","0563f086":"It is important to remove duplicate values in the dataset - so that the data is not polluted by false\/duplicate entries which can affect the distribution of the data and the class frequency. This im turn affects the convergence of the optimization function. \n\nAbove, I scanned the dataset for duplicate\/repeated patterns. For this, pandas has the handy drop_duplicates() function, which identifies repetitive patterns and removes them from the dataset. I have set it to keep the first instance of the duplicate","f0d9362f":"**REMOVE DUPLICATES**","15756f86":"The high difference between 75th percentile and MAX value of each feature means that there is quite a bit of outliers in the dataset. The different MIN and MAX values across the features means that the scale of the features are different - We would require Outlier removal and feature scaling to be performed. \n\nThe datatypes of various features are listed as float while the target feature 'Class' is int - No conversions or mainpulations required\n","892b8b55":"To balance the dataset, I have used a simple Random Under Sampling technique. This randomly reduces the majority class instances in the input until it matches the no.of samples in the minority class. \n\nRemember, do not apply resampling to your test set. They are supposed to mimick the real-world\/deployment data and should be untouched.","0bb7f27f":"# Data Analysis","3e9571d5":"**UNDERSTANDING DATA**","722952ce":"I built the above function as an easy wrapper to produce the most common evaluation metrics for the model. Please feel free to adapt or improve the function according to your needs","d0b93ecb":"**MISSING DATA VERIFICATION**","532a77d0":"**ADDRESSING CLASS BALANCE**","a2c71fa6":"Now to the core part of this notebook. \n\n**PERCEPTRON**\n\nIt is a simple neural network consisting of only 1 neuron. Here, the number of nodes in the input layer is the no.of features in the dataset. Each input is multiplied with a weight and passed to the activation function. This function processes the data and produces the output\n\n**MULTI-LAYER PERCEPTRON**\n\nA single neuron is not powerful enough to model complex non-linear relationships in the dataset. For this, we need multiple layers of nuerons and hence we arrive at the concept of multi-layer perceptron\n\nThere are 2 phases in training a MLP:\n\n--> FEED FORWARD:\n\n        First, the weights are randomly initialized. A bias value is added to the summation of input and weights to avoid null values. The neurons have activation function which are applied to its inputs (Eg. Relu, Sigmoid etc). The output of the first hidden layer is multipied with the weights of the next hidden layer and processed. This continues until the output layer is reached. \n        \n--> BACK PROPAGATION:\n\n        First, the error is calculated as the difference between the predicted and expected output. This error is sometimes called loss, and the function used to calculate this is called as loss function. The goal now is to reduce\/minimize this error. For this, the partial derivative of the error with respect to the weights and biases is calculated. This partial derivative is called gradient descent. The derivates are used to find the slope of the fucntion. The function used to perform this is called the optimization fucntion. \n\nOne cycle of feed-forward and back-propagation is called as an epoch\/iteration. This process continues for the set number of iterations. ","41bd2779":"# MOTIVATION & FORE-WORD\n\nThis is an *introductory notebook* for anyone looking to get their hands dirty with the very basic of neural networks. One such network is **MULTI-LAYER PERCEPTRON**. This particular netowrk was chosen to explain neural network basics as it is a part of the SKLearn library - as most of the ML Scientists know this library already\n\nFollow along the notebook to learn the Neural Networks 101 using the Credit Fraud dataset","f552c323":"Here, most of the columns are marked as V1, V2 etc. We have access to columns like time and amount which put the rest of the columns in perspective. But without knowing the context of the majority of columns, we can only perform limited EDA of this dataset ","64956384":"Above, I built a pipeline to implement the model. To understand more about pipeline and its merits, please visit my previous notebook here - https:\/\/www.kaggle.com\/anirudhg15\/stackingregressor-pipelines-predict-house-price","eac8009b":"# Hope I contributed something valueable and worthy of your time. If so, upvote and comment your thoughts\/suggestions\n\n**Remember, learning is a life-long commitment and sharing knowledge accelarates the process**\n\n**Cheers !!**","dc2bca44":"**CHEKCING CLASS BALANCE**","ec8f5784":"# DATA PRE-PROCESSING","c2e6519d":"In my Implementation of MLP, I have used:\n- 100 hidden layers, meaning 100 layers of neurons\n- 500 iterations of feed-forward and back-propogations to perform\n- Rectified Linear Unit (Relu) activation function\n- Cross-Entropy loss fucntion (Sklearn uses this in the background)\n- Adaptive moment estimation (Adam) optimization function\n- Set random state to initialize the layers with the same weights for every run, to maintain reproducability","66f13f62":"**TRAIN TEST SPLIT**","9b312395":"This is a highly imbalanced dataset. Therefore data resampling techniques should be applied before being input to the model. Otherwise, the result will be biased\/skewed towards the majority class and might even sometimes ignore the minority class altogether while classifying","d97cf38a":"There are no NULL or missing values in this dataset. This is an ideal scenario - but in live data, there will be lots of missing data and also the data will require a lot of understanding and preprocessing. \n\nTo get started on dealing with missing data, check out my other notebook here for more details - https:\/\/www.kaggle.com\/anirudhg15\/top-8-randomforest-mice-predict-titanic-survival","509bc301":"# SETUP","6fabd9c9":"# CLASSIFICATION PIPELINE"}}