{"cell_type":{"22618abd":"code","77603f84":"code","c33d05bd":"code","435bac68":"code","14a7d5be":"code","0c89b3c6":"code","91f5897f":"code","6fd51ad6":"markdown","230cd2d7":"markdown","80eed88d":"markdown","2a7c2dcf":"markdown","d6c59c0c":"markdown","288d393d":"markdown","27d308c2":"markdown","6d6c5e7c":"markdown","c1310542":"markdown"},"source":{"22618abd":"import numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.linear_model import LinearRegression\nfrom itertools import permutations\nimport pandas as pd","77603f84":"class BestSubset:\n    def __init__(self, metric='Cp'):\n        self.metric = metric\n        self.modelContainer = []\n    \n    def fit(self, X, y):\n        nSamples, nFeats = X.shape\n        Mo = LinearRegression().fit(np.zeros((nSamples,1)),y) # this will return mean always\n        self.modelContainer.append([]) #empty predictors\n        \n        for k in range(1,nFeats+1):\n            print(f'Training for {k} predictors')\n            tmp = list(permutations(range(nFeats),r=k))\n            tmp = [tuple(sorted(x)) for x in tmp]\n            feature_combinations = list(set(tmp))\n            tmpModelContainer = []\n            \n            for t in range(len(feature_combinations)):\n                p = np.array(feature_combinations[t])\n#                 print(p.shape, p)\n                tmpX = X[:,p]\n                model = LinearRegression().fit(tmpX, y)\n                r2 = model.score(tmpX, y)\n                tmpModelContainer.append((p,r2))\n            \n            tmpModelContainer = sorted(tmpModelContainer, key=lambda x: x[1], reverse=True)\n            self.modelContainer.append(tmpModelContainer[0][0]) # only best model's predictors\n        \n        print(\"Comparing final K models\", X.shape)\n        Cp = []\n        for k in range(nFeats):\n            predictors = self.modelContainer[k]\n            print(f\"{k}th model: \",len(predictors))\n            \n            if len(predictors) == 0:\n                model = LinearRegression().fit(np.zeros((nSamples, 1)), y)\n                yhat = model.predict(np.zeros((nSamples, 1)))\n            else:\n                model = LinearRegression().fit(X[:, predictors], y)\n                yhat = model.predict(X[:, predictors])\n                \n            rss = np.sum((y - yhat)**2)\n            est_var = np.var( (y-yhat) )\n            _Cp = ((rss + (2 * len(predictors) * est_var)) \/ nSamples)\n            Cp.append(_Cp)\n            \n        idxmin = np.argmin(Cp)\n        return self.modelContainer[idxmin], Cp","c33d05bd":"from sklearn.preprocessing import OrdinalEncoder","435bac68":"df = pd.read_csv(\"..\/input\/ISLR-Auto\/Credit.csv\")\ndf.drop(['Unnamed: 0'], axis=1, inplace=True)\nX = df.loc[:,df.columns!='Rating'].values\ny = df['Rating'].values\npredictiors = df.columns[df.columns!='Rating']\noe = OrdinalEncoder().fit(X)\nX = oe.transform(X)","14a7d5be":"%%time\nbs = BestSubset()\nbest_predictors, scores = bs.fit(X,y)\nprint(f\"Best predictors are {df.columns[best_predictors].values}\")","0c89b3c6":"class ForwardStepwise:\n    def __init__(self):\n        pass\n    \n    def fit(self, X, y):\n        nSamples, nFeats = X.shape\n        predictors = [] # predictors[0:i] will have best model for i number of predictors.\n        for k in range(0, nFeats):\n            unused_indices = [i for i in range(nFeats) if i not in predictors]\n            tmp_scores = []\n            for j in unused_indices:\n                p = predictors + [j]\n                model = LinearRegression().fit(X[:, p], y)\n                score = model.score(X[:, p], y)\n                tmp_scores.append(score)\n            mx = np.argmax(tmp_scores)\n            predictors.append(unused_indices[mx])\n            \n        Cp = []\n        \n        for k in range(nFeats):\n            if k == 0:\n                p = []\n                model = LinearRegression().fit(np.zeros((nSamples,1)), y)\n                yhat = model.predict(np.zeros((nSamples,1)))\n            else:\n                p = predictors[0:k]\n                model = LinearRegression().fit(X[:, p], y)\n                yhat = model.predict(X[:, p])\n                \n            rss = np.sum((y - yhat)**2)\n            est_var = np.var( (y-yhat) )\n            _Cp = ((rss + (2 * len(p) * est_var)) \/ nSamples)\n            Cp.append(_Cp)\n            \n        mn = np.argmin(Cp)\n        best_predictors = predictors[:mn]\n        return best_predictors","91f5897f":"%%time\nfs = ForwardStepwise()\nbest = fs.fit(X, y)\nprint(f\"Best predictors are {df.columns[best].values}\")","6fd51ad6":"## Trying out the model\non Credit dataset used in the ISLR book","230cd2d7":"## Stepwise Selection\nIt does a similar job of comparing models but computationally efficient than best subset. Stepwise selection are of two type,\n- Forward Stepwise Selection\n- Backward Stepwise Selection\n\nForward stepwise begins with no predictors, then adds predictors that contribute the most. Here is the algorithm, \n1. $M_0$ is the null model without any predictors and only returns mean of target variable.\n2. For $k=0, ... ,p-1$:\n    1. Consider all $p-k$ predictor that are not in Mk. Augment Mk with each of them, one a time and train.\n    2. Choose the best among those $p-k$ models, and call it $M_{k+1}$.\n3. Select the best model from $M_0 ... M_p$ models using Cp or other metrics.\n\nIn each iteration inside $(2)$, we train $p-k$ models. That gives us total $M_0 + \\sum_{k=0}^{p-1} \\ k(p-k)$ models to train, which is less than the $2^p$ models that Best Subset would have to train.\n\n### Implementation","80eed88d":"As we can see that *Best Subset* and *Forward Stepwise* both has given us the same result but the latter took less time.","2a7c2dcf":"## Best Subset\nIt is just training all predictor combination possible and taking the best one by error rates. Computationally, this is a bad approach, but none the less, we should at least learn it once. For $p$ predictors, there is going to be $2^p$ models.\n### Algorithm\n\n- Define Mo as the null model. It contains no predictors and predicts sample mean as output.\n- For k = 1,2,...,p:\n    - Fit all 2^k models among p predictors models that contain exactly k predictors.\n    - Pick the best among those models, call it Mk. Best is defiend by accuracy metrics.\n- Select a single best model from among Mo,...,Mk models using cross validated prediction error, Cp (AIC) or adjusted R^2.      \n\n### Implementation","d6c59c0c":"[![Group-1subset-selection.png](https:\/\/i.postimg.cc\/ZRBGJ8Xh\/Group-1subset-selection.png)](https:\/\/postimg.cc\/njxRkDHd)","288d393d":"## Why?\nSelecting a subset of predictors is handy in certain situations. Given that underlying relation between predictors, $X$ and target, $y$ is linear, in following situation we might need subset of predictors. \n* **Accuracy**. When relation is linear between X and y, the model will have low bias. If number of samples is greater than predictors, then a model will also have low variance. But as those numbers becomes closer, variance starts to increase. Which the model will solve by overfitting and poor performance on the test set. Where predictors are more than there is samples, least squares fit won't be able to uniquely estimate coefficients. If we reduce the number of predictors, then we will have a better model.\n* **Interpretability**. Not all predictors affects the target values equally. Some of which are negligible in the output for a less complex and interpretable model.\n\nReducing or eliminating predictors can be done in three main ways,\n- Subset Selection\n- Shrinkage\n- Dimension Reduction\n\nShrinkage is methods like Ridge Regression, Lasso Regression. Dimension Reduction techniques includes Principle Component Analysis etc. ","27d308c2":"## Backward Stepwise\nIt is the same as forward stepwise but reverse. We begin with using all features as predictors in step $(1)$, and then in step $(2)$, we remove features.","6d6c5e7c":"# Subset Selection\nThere are a few approach to take here. We will begin with the simplest one.","c1310542":"# Reference\n1. James, Gareth, et al. An introduction to statistical learning. Vol. 112. New York: springer, 2013."}}