{"cell_type":{"bab94dfe":"code","868c1b76":"code","0c39930d":"code","8d04b984":"code","d8db924b":"code","6ff9d3a6":"code","e4b78838":"code","7441e135":"code","fd312297":"code","3ecbd609":"code","98ae8007":"code","976dad76":"code","c9119251":"code","0f70b5c7":"code","f673bd12":"code","1a642eeb":"code","5fb65234":"code","c2fd7b33":"code","4b55b4f0":"code","64917c96":"code","0f599b64":"code","a39f1362":"code","bb68899d":"code","d7ece2a2":"code","d9ccb30a":"code","61fcdf55":"code","d12b4073":"code","9f85b99d":"code","54164887":"code","3c3116b2":"code","bfce4492":"code","79dbc813":"code","d836cdeb":"code","f5ec45f7":"code","675ce43e":"code","01e81918":"code","7e54f5b9":"code","850f9749":"code","1fc65ac2":"code","96a815ef":"code","285de8cc":"code","1060742d":"code","c1b06c4e":"code","7eedfd94":"code","8dd20a35":"markdown","724757a4":"markdown","98f7bd29":"markdown","788ad075":"markdown","0a9c8f38":"markdown","5b94074a":"markdown","06c51edd":"markdown","29bba4a9":"markdown","51a5e441":"markdown","d1748752":"markdown","08de13a0":"markdown","50f2fd7a":"markdown","e069c898":"markdown","ac1e6d46":"markdown","2725c26a":"markdown","16f4d897":"markdown","0be5153b":"markdown","992bea97":"markdown","99fc8532":"markdown"},"source":{"bab94dfe":"import pandas as pd \nimport numpy as np\nimport matplotlib.pyplot as plt\nimport random\nimport seaborn as sns\nfrom sklearn.preprocessing import LabelEncoder\n\nsns.set_style(\"darkgrid\")\n%matplotlib inline","868c1b76":"df=pd.read_csv(\"https:\/\/raw.githubusercontent.com\/kkehoe1985\/ga_data_science_final_project\/master\/combined_data.csv\")","0c39930d":"##column with anomaly is population column\ndf.select_dtypes(['object'])","8d04b984":"##finding the <bound method Series.mean in the data\n\ndf[pd.to_numeric(df['Population'], errors='coerce').isna()]['Population']\n","d8db924b":"##converting the column to integer and filling it with zero\ndf['Population']=pd.to_numeric(df['Population'],errors='coerce')\ndf['Population'].fillna(0,inplace=True)","6ff9d3a6":"##spliting fips\ndf['fips']=df['fips'].astype(str)\n##some fips have 4 digits since state code is missing a zero at beginning\ndf['fips']","e4b78838":"##splitting to county and state\ndf['county']=df['fips'].str[-3:]\ndf['state']=df['fips'].str[:-3]\n","7441e135":"##eduaction categorical feature\ndf['Education']=df.iloc[:,2:6].idxmax(axis=1)\ndf['Education']","fd312297":"##Religion categorical feature\n\ndf['Religion']=df.iloc[:,8:25].idxmax(axis=1)\ndf['Religion']","3ecbd609":"##age groups\ndf['Young']=df[['0-4_rate', '5-9_rate', '10-14_rate', '15-19_rate']].sum(axis=1)\ndf['Adult']=df[[ '20-24_rate', '25-29_rate', '30-34_rate', '35-39_rate', '40-44_rate', '45-49_rate',\n       '50-54_rate', '55-59_rate', '60-64_rate']].sum(axis=1)\ndf['Old']=df[['65-69_rate', '70-74_rate',\n       '75-79_rate', '80-84_rate', '85+_rate']].sum(axis=1)","98ae8007":"##creating ethnic categories\ndf['EthnicMale']=df[['WHITE_MALE_rate', 'BLACK_MALE_rate', 'NATIVE_AMERICAN_MALE_rate', 'ASIAN_MALE_rate',\n       'HAWAIIAN_PACIFIC_MALE_rate', 'MULTI_MALE_rate']].idxmax(axis=1)\ndf['EthnicFemale']=df[[ 'WHITE_FEMALE_rate', \n       'BLACK_FEMALE_rate',  'NATIVE_AMERICAN_FEMALE_rate', 'ASIAN_FEMALE_rate',\n        'HAWAIIAN_PACIFIC_FEMALE_rate','MULTI_FEMALE_rate']].idxmax(axis=1)","976dad76":"##Remove the following features\ndf=df.drop(columns=['PovertyUnder18Pct2014', 'Deep_Pov_Children', 'Housing units', 'Area in square miles - Water area','Area in square miles - Land area',\n'Density per square mile of land area - Housing units', 'age_total_pop'])","c9119251":"df.rename(columns={'Density per square mile of land area - Population': 'PopDensity', \n                   'Percent of adults with less than a high school diploma, 2011-2015': 'adultsLessHighSchool',\n                   'Percent of adults with a high school diploma only, 2011-2015':'adultsWithHighSchool', \n                   'Percent of adults completing some college or associate\\'s degree, 2011-2015':'adultsWithCollege',\n                    'Percent of adults with a bachelor\\'s degree or higher, 2011-2015':'adultsWithBachelorDegree',\n                    'Unemployment_rate_2015':'unemploymentRate2015',\n                    'POP_ESTIMATE_2015':'POP ESTIMATE 2015',\n                   'Area in square miles - Total area':'totalArea', \n                   },inplace=True)\ndf.columns","0f70b5c7":"df.drop(columns=['Amish', 'Buddhist', 'Catholic',\n       'Christian Generic', 'Eastern Orthodox', 'Hindu', 'Jewish',\n       'Mainline Christian', 'Mormon', 'Muslim', 'Non-Catholic Christian',\n       'Other', 'Other Christian', 'Other Misc', 'Pentecostal \/ Charismatic',\n       'Protestant Denomination', 'Zoroastrian','adultsLessHighSchool', 'adultsWithHighSchool',\n       'adultsWithCollege', 'adultsWithBachelorDegree','0-4_rate', '5-9_rate', '10-14_rate',\n       '15-19_rate', '20-24_rate', '25-29_rate', '30-34_rate', '35-39_rate',\n       '40-44_rate', '45-49_rate', '50-54_rate', '55-59_rate', '60-64_rate',\n       '65-69_rate', '70-74_rate', '75-79_rate', '80-84_rate', '85+_rate',\n       'TOT_MALE_rate', 'TOT_FEMALE_rate', 'WHITE_MALE_rate',\n       'WHITE_FEMALE_rate', 'BLACK_MALE_rate', 'BLACK_FEMALE_rate',\n       'NATIVE_AMERICAN_MALE_rate', 'NATIVE_AMERICAN_FEMALE_rate',\n       'ASIAN_MALE_rate', 'ASIAN_FEMALE_rate', 'HAWAIIAN_PACIFIC_MALE_rate',\n       'HAWAIIAN_PACIFIC_FEMALE_rate', 'MULTI_MALE_rate', 'MULTI_FEMALE_rate',\n       'WHITE_rate', 'BLACK_rate', 'NATIVE_AMERICAN_rate',\n       'HAWAIIAN_PACIFIC_rate', 'MULTI_rate'],inplace=True)","f673bd12":"##Normalize all population-related. income-related, and area-related features with z-score normalization\n\nfor col in  ['MedHHInc2014', 'PerCapitaInc','POP ESTIMATE 2015', 'Population', 'totalArea', 'PopDensity']:\n    df[col]=(df[col]-df[col].min())\/df[col].max()\n","1a642eeb":"df[['MedHHInc2014', 'PerCapitaInc','POP ESTIMATE 2015', 'Population', 'totalArea', 'PopDensity']]","5fb65234":"##New name, mean and standard deviation of POP ESTIMATE 2015 andPopulation. Also report their correlation score.\ndf[['POP ESTIMATE 2015','Population']].describe().loc[['mean','std'],:]","c2fd7b33":"##the columns are highly correlated 0.99\ndf[['POP ESTIMATE 2015','Population']].corr()","4b55b4f0":"from scipy.stats import iqr\n##New name, median, quartiles, and IQR of PerCapitaInc and PovertyAllAgesPct2014 features\ndf[['PerCapitaInc','PovertyAllAgesPct2014']].median()","64917c96":"##the quartiles\ndf[['PerCapitaInc','PovertyAllAgesPct2014']].quantile([0.25,0.5,0.75,1])","0f599b64":"##inter quartile range\niqr(df[['PerCapitaInc','PovertyAllAgesPct2014']],axis=0)","a39f1362":"## Mode of combined Religion and EthnicMale feature\ndf[[ 'Religion', 'EthnicMale']].mode()","bb68899d":"##histogram of the following features: Religion, EthnicMale, EthnicFemale, Education\nplt.figure(figsize=(16,12))\nfor i,col in enumerate(['Religion', 'EthnicMale', 'EthnicFemale', 'Education']):\n    plt.subplot(4, 1, i + 1)\n    df1 = pd.Series(df[col].value_counts())\n    plt.tight_layout(h_pad=0.5, w_pad=0.5, pad=2.5)\n    plt.title(col)\n    plt.bar(range(len(df1)), df1.values, align='center')\n    plt.xticks(range(len(df1)), df1.index.values, size='small',rotation=30)\nplt.legend(loc='lower right', borderpad=0, handletextpad=0)\nplt.axis(\"tight\")\nplt.figure()","d7ece2a2":"##2D scatter plot of Area and Population feature\nplt.figure(figsize=(12,8))\nplt.scatter(df[\"totalArea\"], df[\"Population\"])\nplt.xlabel('Totalarea')\nplt.xlabel('Population')\n\nplt.show()","d9ccb30a":"\n##Box plot of normalized PerCapitaInc and PovertyAllAgesPct2014 features\nfig, ax = plt.subplots(figsize=(12, 7))\n# Remove top and right border\nax.spines['top'].set_visible(False)\nax.spines['right'].set_visible(False)\nax.spines['left'].set_visible(False)\n# Remove y-axis tick marks\nax.yaxis.set_ticks_position('none')\n# Add major gridlines in the y-axis\nax.grid(color='grey', axis='y', linestyle='-', linewidth=0.25, alpha=0.5)\n# Set plot title\nax.set_title('Distribution of percapita nad poveryallages')\n# Set species names as labels for the boxplot\ndataset = [df[\"PerCapitaInc\"], (df[\"PovertyAllAgesPct2014\"]-df[\"PovertyAllAgesPct2014\"].min())\/df[\"PovertyAllAgesPct2014\"].max()]\n\nax.boxplot(dataset, labels=['PerCapitaInc','PovertyAllAgesPct2014'])\nplt.show()","61fcdf55":"##Are there any contradicting samples in the dataset? Are there any nonsensical samples? What do they look like? How many of them are there\n\n# the data with population zero in 5 rows is an anomaly since fips population cant be zero.\ndf[df['Population']==0]","d12b4073":"df.describe()","9f85b99d":"##result should be saved as elections clean.csv.\ndf.to_csv('elections clean.csv',index=False)","54164887":"#Import the dataset and define the feature as well as the target datasets \/ columns#\ndataset = pd.read_csv('elections clean.csv')\ndataset['Education']=dataset['Education'].str.replace(' ','_')\ndataset['Religion']=dataset['Religion'].str.replace(' ','_')\ndataset.head()\n\n","3c3116b2":"##taking only categorical \ndata=dataset.drop(columns=['Democrat'])\ndata['fips']=data['fips'].astype('str')\ndata['Democrat']=dataset['Democrat']\ndata","bfce4492":"for col in ['Education', 'Religion','EthnicMale',\n       'EthnicFemale', 'Democrat']:\n    encoder=LabelEncoder()\n    data[col]=encoder.fit_transform(data[col])","79dbc813":"##shuffle and split the data\ndef train_test_split(df, test_size):\n    \n    if isinstance(test_size, float):\n        test_size = round(test_size * len(df))\n\n    indices = df.index.tolist()\n    test_indices = random.sample(population=indices, k=test_size)\n\n    test_df = df.loc[test_indices]\n    train_df = df.drop(test_indices)\n    \n    return train_df, test_df\n\n\n\n    \n###Use 70% of the shuffled dataset for training and the rest for\n\n\ntraining_data,testing_data = train_test_split(data, 0.3)","d836cdeb":"# 1.1 Data pure?\ndef check_purity(data):\n    \n    label_column = data[:, -1]\n    unique_classes = np.unique(label_column)\n\n    if len(unique_classes) == 1:\n        return True\n    else:\n        return False\n    \n# 1.2 Create Leaf\ndef create_leaf(data, ml_task):\n    \n    label_column = data[:, -1]\n    if ml_task == \"regression\":\n        leaf = np.mean(label_column)\n\n    # classfication    \n    else:\n        unique_classes, counts_unique_classes = np.unique(label_column, return_counts=True)\n        index = counts_unique_classes.argmax()\n        leaf = unique_classes[index]\n\n    return leaf\n\n\n# 1.3 Determine potential splits\ndef get_potential_splits(data):\n    \n    potential_splits = {}\n    _, n_columns = data.shape\n    for column_index in range(n_columns - 1): # excluding the last column which is the label\n        values = data[:, column_index]\n        unique_values = np.unique(values)\n        \n        potential_splits[column_index] = unique_values\n    \n    return potential_splits\n\n\n# 1.4 Determine Best Split\ndef calculate_entropy(data):\n    \n    label_column = data[:, -1]\n    _, counts = np.unique(label_column, return_counts=True)\n\n    probabilities = counts \/ counts.sum()\n    entropy = sum(probabilities * -np.log2(probabilities))\n     \n    return entropy\n\n\n\n\ndef calculate_overall_metric(data_below, data_above, metric_function):\n    \n    n = len(data_below) + len(data_above)\n    p_data_below = len(data_below) \/ n\n    p_data_above = len(data_above) \/ n\n\n    overall_metric =  (p_data_below * metric_function(data_below) \n                     + p_data_above * metric_function(data_above))\n    \n    return overall_metric\n\n\ndef determine_best_split(data, potential_splits, ml_task):\n    \n    first_iteration = True\n    for column_index in potential_splits:\n        for value in potential_splits[column_index]:\n            data_below, data_above = split_data(data, split_column=column_index, split_value=value)\n            \n          \n            current_overall_metric = calculate_overall_metric(data_below, data_above, metric_function=calculate_entropy)\n\n            if first_iteration or current_overall_metric <= best_overall_metric:\n                first_iteration = False\n                \n                best_overall_metric = current_overall_metric\n                best_split_column = column_index\n                best_split_value = value\n    return best_split_column, best_split_value\n\n\n# 1.5 Split data\ndef split_data(data, split_column, split_value):\n    \n    split_column_values = data[:, split_column]\n\n    type_of_feature = FEATURE_TYPES[split_column]\n    if type_of_feature == \"categorical\":\n        data_below = data[split_column_values == split_value]\n        data_above = data[split_column_values != split_value]\n        \n    \n    # feature is continous   \n    else:\n        data_below = data[split_column_values <= split_value]\n        data_above = data[split_column_values >  split_value]\n    \n    return data_below, data_above\n\n\n# 2. Decision Tree Algorithm\n# 2.1 Helper Function\ndef determine_type_of_feature(df):\n    \n    feature_types = []\n    n_unique_values_treshold = 15\n    for feature in df.columns:\n        if feature != \"Democrat\":\n            unique_values = df[feature].unique()\n            example_value = unique_values[0]\n\n            if (isinstance(example_value, str)) or (len(unique_values) <= n_unique_values_treshold):\n                feature_types.append(\"categorical\")\n            else:\n                feature_types.append(\"continuous\")\n    \n    return feature_types\n\n\n# 2.2 Algorithm\ndef decision_tree_algorithm(df, ml_task, counter=0, preprune= False, max_depth=3):\n    \n    # data preparations\n    if counter == 0:\n        global COLUMN_HEADERS, FEATURE_TYPES\n        COLUMN_HEADERS = df.columns\n        FEATURE_TYPES = determine_type_of_feature(df)\n        data = df.values\n        data[[]]\n    else:\n        data = df   \n        \n    if (check_purity(data)):\n        leaf = create_leaf(data, ml_task)\n        return leaf\n    ##checking pruning\n    if preprune and (counter == max_depth):\n        leaf = create_leaf(data, ml_task)\n        return leaf\n\n    \n    # recursive part\n    else:    \n        counter += 1\n        \n        # helper functions \n        potential_splits = get_potential_splits(data)\n        \n        split_column, split_value = determine_best_split(data, potential_splits, ml_task)\n        data_below, data_above = split_data(data, split_column, split_value)\n        \n        # check for empty data\n        if len(data_below) == 0 or len(data_above) == 0:\n            leaf = create_leaf(data, ml_task)\n            return leaf\n        \n        # determine question\n        feature_name = COLUMN_HEADERS[split_column]\n        type_of_feature = FEATURE_TYPES[split_column]\n        if type_of_feature == \"continuous\":\n            question = \"{} <= {}\".format(feature_name, split_value)\n            \n        # feature is categorical\n        else:\n            question = \"{} = {}\".format(feature_name, split_value)\n        \n        # instantiate sub-tree\n        sub_tree = {question: []}\n        \n        # find answers (recursion)\n        yes_answer = decision_tree_algorithm(data_below, ml_task, counter,preprune, max_depth)\n        no_answer = decision_tree_algorithm(data_above, ml_task, counter,preprune, max_depth)\n        \n        # If the answers are the same, then there is no point in asking the qestion.\n        # This could happen when the data is classified even though it is not pure\n        # yet (min_samples or max_depth base case).\n        if yes_answer == no_answer:\n            sub_tree = yes_answer\n        else:\n            sub_tree[question].append(yes_answer)\n            sub_tree[question].append(no_answer)\n        \n        return sub_tree\n\n\n# 3. Make predictions\n# 3.1 One example\ndef predict_example(example, tree):\n    \n    # tree is just a root node\n    if not isinstance(tree, dict):\n        return tree\n    \n    question = list(tree.keys())[0]\n    feature_name, comparison_operator, value = question.split(\" \")\n\n    # ask question\n    if comparison_operator == \"<=\":\n        if example[feature_name] <= float(value):\n            answer = tree[question][0]\n        else:\n            answer = tree[question][1]\n    \n    # feature is categorical\n    else:\n        if str(example[feature_name]) == value:\n            answer = tree[question][0]\n        else:\n            answer = tree[question][1]\n\n    # base case\n    if not isinstance(answer, dict):\n        return answer\n    \n    # recursive part\n    else:\n        residual_tree = answer\n        return predict_example(example, residual_tree)\n\n    \n# 3.2 All examples of a dataframe\ndef make_predictions(df, tree):\n    \n    if len(df) != 0:\n        predictions = df.apply(predict_example, args=(tree,), axis=1)\n    else:\n        # \"df.apply()\"\" with empty dataframe returns an empty dataframe,\n        # but \"predictions\" should be a series instead\n        predictions = pd.Series()\n        \n    return predictions\n\n\n# 3.3 Accuracy\ndef calculate_accuracy(df, tree):\n    predictions = make_predictions(df, tree)\n    predictions_correct = predictions == df.Democrat\n    accuracy = predictions_correct.mean()\n    \n    return accuracy","f5ec45f7":"##creating tree, only the categirucal variables are taken in making the tree\ntree = decision_tree_algorithm(training_data[['Education', 'Religion','EthnicMale','EthnicFemale', 'Democrat']], ml_task=\"classification\")","675ce43e":"##calculate accuarcy on testing dataset\naccuracy=calculate_accuracy(testing_data[['Education', 'Religion','EthnicMale',\n       'EthnicFemale', 'Democrat']], tree)\naccuracy","01e81918":"##pretty printing the tree\n##the root node is Education\nfrom pprint import pprint\npprint(tree)","7e54f5b9":"##education is the root of the tree with more information gain\n","850f9749":"test=testing_data.sample(200)\npreds= make_predictions(test, tree)","1fc65ac2":"\nn_classes = 2\nplot_colors = \"ryb\"\ntest['fips']=test['fips'].astype(int)\nxx, yy = np.meshgrid(test['fips'].astype(int).unique(),\n                     test['Education'].unique())\nplt.tight_layout(h_pad=0.5, w_pad=0.5, pad=2.5)","96a815ef":"\nZ = np.concatenate([[preds.values]] * yy.shape[0], axis=0)\ncs = plt.contourf(xx, yy, Z, cmap=plt.cm.RdYlBu)\n\nplt.xlabel('Fips')\nplt.ylabel('EthnicFemale')\n\n# Plot the training points\nfor i, color in zip(range(n_classes), plot_colors):\n    idx = np.where(test['Democrat'] == i)\n    plt.scatter(test.iloc[idx]['fips'].values, test.iloc[idx]['Education'].values)\n\nplt.suptitle(\"Decision surface of a decision tree using paired features\")\nplt.legend(loc='lower right', borderpad=0, handletextpad=0)\nplt.axis(\"tight\")\n\nplt.figure()\nplt.show()","285de8cc":"##the depth of the tree is reduced to specified max depth\ntree3 = decision_tree_algorithm(training_data[['Education', 'Religion','EthnicMale','EthnicFemale', 'Democrat']], ml_task=\"classification\", preprune= True, max_depth=3)\ntree5 = decision_tree_algorithm(training_data[['Education', 'Religion','EthnicMale','EthnicFemale', 'Democrat']], ml_task=\"classification\", preprune= True, max_depth=5)\ntree7 = decision_tree_algorithm(training_data[['Education', 'Religion','EthnicMale','EthnicFemale', 'Democrat']], ml_task=\"classification\", preprune= True, max_depth=7)","1060742d":"##finding training and testing errors of the tree of varying depths 3,5,7\n##training data is 70% and testing data is 30%\ntesterror3=1-calculate_accuracy(testing_data[['Education', 'Religion','EthnicMale',\n       'EthnicFemale', 'Democrat']], tree3)\ntesterror5=1-calculate_accuracy(testing_data[['Education', 'Religion','EthnicMale',\n       'EthnicFemale', 'Democrat']], tree5)\ntesterror7=1-calculate_accuracy(testing_data[['Education', 'Religion','EthnicMale',\n       'EthnicFemale', 'Democrat']], tree7)\n\ntrainerror3=1-calculate_accuracy(training_data[['Education', 'Religion','EthnicMale',\n       'EthnicFemale', 'Democrat']], tree3)\ntrainerror5=1-calculate_accuracy(training_data[['Education', 'Religion','EthnicMale',\n       'EthnicFemale', 'Democrat']], tree5)\ntrainerror7=1-calculate_accuracy(training_data[['Education', 'Religion','EthnicMale',\n       'EthnicFemale', 'Democrat']], tree7)","c1b06c4e":"depths=[3, 5, 7]\ntrainerrors=[trainerror3,trainerror5,trainerror7]\ntesterrors=[testerror3,testerror5,testerror7]","7eedfd94":"plt.plot(depths, trainerrors, label = \"trainerrors\")\nplt.plot(depths, testerrors, label = \"trainerrors\")\nplt.legend()\nplt.xlabel('Depth variation')\nplt.ylabel('Error')\n\nplt.show()","8dd20a35":"# 2","724757a4":"\n\n## 2.2","98f7bd29":"## 1 Problem 1","788ad075":"## What do you think the name of the label vector is?\n\n\nThe label vector is the democrat ","0a9c8f38":"Implement ID3 decision-tree inference algorithm from scratch","5b94074a":"## Prepruning with depths","06c51edd":"We can see that training and testing error decreases as the depth of the tree increases in prepruning .THe best depth is at 5 due to less traning and test error.","29bba4a9":"#### 1.3","51a5e441":"## 1.5","d1748752":"#### 1.4","08de13a0":"## 2.3 Making the plot ","50f2fd7a":"## 1.8","e069c898":"## 2.1","ac1e6d46":"#### 1.1","2725c26a":"## 1.6","16f4d897":"## 1.7","0be5153b":"#### 1.2","992bea97":"## 1.9","99fc8532":"# 3"}}