{"cell_type":{"9b34759d":"code","6a8f643d":"code","044f1542":"code","28ad4438":"code","c3c71871":"markdown","ee8cb71b":"markdown","4931d94c":"markdown","7bf50f08":"markdown","5b8c9825":"markdown","ef4acf1b":"markdown"},"source":{"9b34759d":"#!pip install nb_black\n#%load_ext nb_black\nimport numpy as np\nnp.random.seed(42)","6a8f643d":"# Distribution\ndistribution = {\n    \"mean\": np.random.rand(1, 3),\n    \"covariance\": [[1, 0.1, 0], [0.1, 1, 0], [0, 0, 1]],\n}\n\n# Point\npoint = np.random.rand(1, 3)\n\n\ndef mahalanobis_distance(distribution: \"dict\", point: \"np.array()\") -> int:\n    \"\"\" Estimate Mahalanobis Distance \n    \n    Args:\n        distribution: a sample gaussian distribution\n        point: a deterministic point\n    \n    Returns:\n        Mahalanobis distance\n    \"\"\"\n    mean = distribution[\"mean\"]\n    cov = distribution[\"covariance\"]\n    return np.sqrt((point - mean) @ np.linalg.inv(cov) @ (point - mean).T)[0][0]\n\n\n# Our implementation\ndistance = mahalanobis_distance(distribution, point)\nprint(f\"Ours : {distance}\")\n\n# scipy inbuilt\nfrom scipy.spatial.distance import mahalanobis\n\ndistance = mahalanobis(\n    point, distribution[\"mean\"], np.linalg.inv(distribution[\"covariance\"])\n)\nprint(f\"Scipy: {distance}\")","044f1542":"# Distribution 1\ndistribution1 = {\n    \"mean\": np.array([[1,3,1]]),\n    \"covariance\": np.array([[1, 0.1, 0], [0.1, 1, 0], [0, 0, 1]]),\n}\n\n\n# Distribution 2\ndistribution2 = {\n    \"mean\": np.array([[1,3,1]]),\n    \"covariance\": np.array([[1, 0.5, 0], [0.5, 1, 0], [0, 0, 1]]),\n}\n\n# Distribution 3\nd1 = np.random.rand(1,1000)\np1 = np.histogram(d1,100)[0]\np1 = p1 \/ np.sum(p1)\n\n# Distribution 4\nd2 = np.random.rand(1,1000)\np2 = np.histogram(d2,100)[0]\np2 = p1 \/ np.sum(p2)\n\ndef bhattacharyya_gaussian_distance(distribution1: \"dict\", distribution2: \"dict\",) -> int:\n    \"\"\" Estimate Bhattacharyya Distance (between Gaussian Distributions)\n    \n    Args:\n        distribution1: a sample gaussian distribution 1\n        distribution2: a sample gaussian distribution 2\n    \n    Returns:\n        Bhattacharyya distance\n    \"\"\"\n    mean1 = distribution1[\"mean\"]\n    cov1 = distribution1[\"covariance\"]\n\n    mean2 = distribution2[\"mean\"]\n    cov2 = distribution2[\"covariance\"]\n\n    cov = (1 \/ 2) * (cov1 + cov2)\n\n    T1 = (1 \/ 8) * (\n        np.sqrt((mean1 - mean2) @ np.linalg.inv(cov) @ (mean1 - mean2).T)[0][0]\n    )\n    T2 = (1 \/ 2) * np.log(\n        np.linalg.det(cov) \/ np.sqrt(np.linalg.det(cov1) * np.linalg.det(cov2))\n    )\n\n    return T1 + T2\n\ndef bhattacharyya_distance(distribution1: \"dict\", distribution2: \"dict\",) -> int:\n    \"\"\" Estimate Bhattacharyya Distance (between General Distributions)\n    \n    Args:\n        distribution1: a sample distribution 1\n        distribution2: a sample distribution 2\n    \n    Returns:\n        Bhattacharyya distance\n    \"\"\"\n    sq = 0\n    for i in range(len(distribution1)):\n        sq  += np.sqrt(distribution1[i]*distribution2[i])\n    \n    return -np.log(sq)\n    \n    \n# Our implementation (Gaussian)\ndistance = bhattacharyya_gaussian_distance(distribution1, distribution2)\nprint(f\"Ours (Gaussian) : {distance}\")\n\n# Our implementation (General)\ndistance = bhattacharyya_distance(p1, p2)\nprint(f\"Ours (General)  : {distance}\")\n","28ad4438":"# Distribution 1\nd1 = np.random.rand(1,1000)\np1 = np.histogram(d1,100)[0] + 0.000001\np1 = p1 \/ np.sum(p1)\n\n# Distribution 2\nd2 = np.random.randn(1,1000)\np2 = np.histogram(d2,100)[0] + 0.000001\np2 = p2 \/ np.sum(p2)\n\n\ndef KL_divergence(distribution1: \"dict\", distribution2: \"dict\",) -> int:\n    \"\"\" Estimate KL-Divergence (from distribution1 to distribution2)\n    \n    Args:\n        distribution1: a sample distribution 1\n        distribution2: a sample distribution 2\n    \n    Returns:\n        KL-Divergence distance\n    \"\"\"\n    s = 0\n    for i in range(len(distribution2)):\n        p = distribution2[i]\n        q = distribution1[i]\n        s += p*np.log(q\/p)\n    \n    return s\n        \n# Our implementation\ndistance = KL_divergence(p1, p2)\nprint(f\"Ours (1 (Uniform) ->2 (Gaussian)): {distance}\")\n    \ndistance = KL_divergence(p2, p1)\nprint(f\"Ours (2 (Gaussian)->1 (Uniform)): {distance}\")\n    \n","c3c71871":"# Mahalanobis Distance\n---\n\nThe Mahalanobis distance is a measure of the distance between a point P and a distribution D, introduced by P. C. Mahalanobis in 1936. It is a multi-dimensional generalization of the idea of measuring how many standard deviations away P is from the mean of D. This distance is zero if P is at the mean of D, and grows as P moves away from the mean along each principal component axis. [2]\n\nIf the principle axes are re-scaled to have unit variance, then the Mahalanobis distance corresponds to standard Euclidean distance in the transformed space. \n\nThe Mahalanobis distance is unitless and scale-invariant, and takes into account the correlations of the data set.\n\nThe Mahalanobis distance of an observation ${\\displaystyle {\\vec {x}}=(x_{1},x_{2},x_{3},\\dots ,x_{N})^{T}}$ from a set of observations with mean ${\\displaystyle {\\vec {\\mu }}=(\\mu _{1},\\mu _{2},\\mu _{3},\\dots ,\\mu _{N})^{T}}$ and covariance matrix $S$ is defined as:\n\n<center>${\\displaystyle D_{M}({\\vec {x}})={\\sqrt {({\\vec {x}}-{\\vec {\\mu }})^{T}S^{-1}({\\vec {x}}-{\\vec {\\mu }})}}.\\,}$<\/center>\n\nMahalanobis distance (or \"generalized squared interpoint distance\" for its squared value) can also be defined as a dissimilarity measure between two random vectors ${\\displaystyle {\\vec {x}}}$ and ${\\displaystyle {\\vec {y}}}$ of the same distribution with the covariance matrix $S$:\n\n<center>${\\displaystyle d({\\vec {x}},{\\vec {y}})={\\sqrt {({\\vec {x}}-{\\vec {y}})^{T}S^{-1}({\\vec {x}}-{\\vec {y}})}}.\\,}$<\/center>\n\n### Code:","ee8cb71b":"# Bhattacharyya Distance\n---\n\nIn statistics, the Bhattacharyya distance measures the similarity of two probability distributions. It is closely related to the Bhattacharyya coefficient which is a measure of the amount of overlap between two statistical samples or populations. Both measures are named after Anil Kumar Bhattacharya, a statistician who worked in the 1930s at the Indian Statistical Institute.[3]\n\nThe coefficient can be used to determine the relative closeness of the two samples being considered. It is used to measure the separability of classes in classification and it is considered to be more reliable than the Mahalanobis distance.\n\nMahalanobis distance is a particular case of the Bhattacharyya distance when the standard deviations of the two classes are the same. Consequently, when two classes have similar means but different standard deviations, the Mahalanobis distance would tend to zero, whereas the Bhattacharyya distance grows depending on the difference between the standard deviations.\n\nFor multivariate normal distributions ${\\displaystyle p_{i}={\\mathcal {N}}({\\boldsymbol {\\mu }}_{i},\\,{\\boldsymbol {\\Sigma }}_{i})}$\n\n<center>${\\displaystyle D_{B}={1 \\over 8}({\\boldsymbol {\\mu }}_{1}-{\\boldsymbol {\\mu }}_{2})^{T}{\\boldsymbol {\\Sigma }}^{-1}({\\boldsymbol {\\mu }}_{1}-{\\boldsymbol {\\mu }}_{2})+{1 \\over 2}\\ln \\,\\left({\\det {\\boldsymbol {\\Sigma }} \\over {\\sqrt {\\det {\\boldsymbol {\\Sigma }}_{1}\\,\\det {\\boldsymbol {\\Sigma }}_{2}}}}\\right),}$<\/center>\n\nwhere ${\\displaystyle {\\boldsymbol {\\mu }}_{i}}$ and ${\\displaystyle {\\boldsymbol {\\Sigma }}_{i}}$ are the means and covariances of the distributions, and\n\n${\\displaystyle {\\boldsymbol {\\Sigma }}={{\\boldsymbol {\\Sigma }}_{1}+{\\boldsymbol {\\Sigma }}_{2} \\over 2}}$.\n\nNote that, in this case, the first term in the Bhattacharyya distance is related to the Mahalanobis distance.","4931d94c":"## References:\n\n[1] https:\/\/en.wikipedia.org\/wiki\/Statistical_distance\n\n[2] https:\/\/en.wikipedia.org\/wiki\/Mahalanobis_distance\n\n[3] https:\/\/en.wikipedia.org\/wiki\/Bhattacharyya_distance\n\n[4] https:\/\/en.wikipedia.org\/wiki\/Kullback%E2%80%93Leibler_divergence","7bf50f08":"## Import Packages","5b8c9825":"# Kullback\u2013Leibler Divergence\n---\n\nIn mathematical statistics, the Kullback\u2013Leibler divergence (also called relative entropy) is a measure of how one probability distribution is different from a second, reference probability distribution. \n\nApplications include characterizing the relative (Shannon) entropy in information systems, randomness in continuous time-series, and information gain when comparing statistical models of inference. \n\nIn contrast to variation of information, it is a distribution-wise asymmetric measure and thus does not qualify as a statistical metric of spread - it also does not satisfy the triangle inequality. \n\nIn the simple case, a Kullback\u2013Leibler divergence of 0 indicates that the two distributions in question are identical. In simplified terms, it is a measure of surprise, with diverse applications such as applied statistics, fluid mechanics, neuroscience and machine learning. [4]","ef4acf1b":"# <center> An Introduction to Statistical Distances <\/center>\n\n## Definition\n---\nIn statistics, probability theory, and information theory, a **statistical distance** quantifies the distance between two statistical objects, which can be two random variables, or two probability distributions or samples, or the distance can be between an individual sample point and a population or a wider sample of points. [1]\n\nStatistical distance measures are mostly not metrics and they need not be symmetric. Some types of distance measures are referred to as (statistical) divergences. **KL divergence** is a well know statistical distance used in *variational autoencoders*, and note that it is not symmetric (i.e. distance between A and B and between B and A may not be same) violating property of \"metrics\".\n\nJust for a refresher, a metric on a set X is a function (called the distance function or simply distance). Say, d : X \u00d7 X \u2192 R+ (where R+ is the set of non-negative real numbers) [1]. For all x, y, z in X, this function is required to satisfy the following conditions:\n1. d(x, y) \u2265 0     (non-negativity)\n2. d(x, y) = 0   if and only if   x = y     (identity of indiscernibles. Note that condition 1 and 2 together produce positive definiteness)\n3. d(x, y) = d(y, x)     (symmetry)\n4. d(x, z) \u2264 d(x, y) + d(y, z)     (subadditivity \/ triangle inequality).\n\n---\n\nSome statistical distances we are going to discuss here are:\n1. Mahalanobis distance\n2. Bhattacharyya distance\n3. Kullback\u2013Leibler divergence\n"}}