{"cell_type":{"a913db1b":"code","23015776":"code","a8c80c19":"code","2e4d76c9":"code","d9b0340d":"code","82b9d128":"code","a4a3a206":"code","12a4c508":"code","4551a216":"code","c396e55b":"code","a998738b":"code","58176982":"code","ce4f52cf":"code","00b58472":"code","03b8acc9":"code","a9061054":"code","dc8aef39":"code","67e8e3fe":"code","2436c344":"code","65ffa434":"code","e4974f0c":"code","dad8af56":"code","49f7061e":"markdown","61b590b3":"markdown","90df5fa7":"markdown","0155281f":"markdown","e203395a":"markdown","6090a740":"markdown","aaad1cfe":"markdown","6ba51aef":"markdown","70f00ad6":"markdown","0b0426f8":"markdown","cdddbb05":"markdown","5e0b14e2":"markdown","310a0f32":"markdown","5004cc08":"markdown","47e43e76":"markdown","1b74d8ff":"markdown"},"source":{"a913db1b":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","23015776":"# Libraries \n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom matplotlib import pyplot\nimport seaborn as sns\nfrom scipy import stats\nfrom numpy import asarray\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import r2_score,mean_absolute_error,mean_squared_error","a8c80c19":"# Lets look at the data\n\ndf_train = pd.read_csv('\/kaggle\/input\/random-linear-regression\/train.csv')\ndf_test = pd.read_csv('\/kaggle\/input\/random-linear-regression\/test.csv')","2e4d76c9":"df_train.head()","d9b0340d":"df_train.describe().transpose()","82b9d128":"df_test.head()","a4a3a206":"df_test.describe().transpose()","12a4c508":"# we detect one missing value in train dataset, lets look at it by the isnull function\n\ndf_train.isnull().sum()","4551a216":"df_train[df_train.isna().any(axis=1)]","c396e55b":"# Missing value in terms of ratio in the dataset\n\ndf_train.y.isnull().sum()\/len(df_train.y)","a998738b":"# Lets drop the missin value. Only one value is missing, so there is no need to imputation\n\ndf_train.dropna(inplace=True)\n\ndf_train.shape","58176982":"# calculate mean and std of data\n\ndata_std_Y=df_train.y.std()\ndata_mean_Y=df_train.y.mean()\n\ndata_std_X=df_train.x.std()\ndata_mean_X=df_train.x.mean()\n\n\n# we use 3 stardart deviation to determine cut off point\n\nlower_y, upper_y = data_mean_Y- 3 * data_std_Y, data_mean_Y + 3 * data_std_Y\n\nlower_x, upper_x = data_mean_X- 3 * data_std_X, data_mean_X + 3 * data_std_X\n\nprint(\"For Y dimension; Std : {}, Mean : {}, Lower : {}, Upper : {}\".format(data_std_Y,data_mean_Y,lower_y,upper_y))\n\nprint(\"For X dimension; Std : {}, Mean : {}, Lower : {}, Upper : {}\".format(data_std_X,data_mean_X,lower_x,upper_x))\n\n# detect outlier\n\noutliers_std_X = [x for x in df_train.x if x < lower_x or x > upper_x]\n\noutliers_std_Y = [x for x in df_train.y if x < lower_y or x > upper_y]","ce4f52cf":"outliers_std_X","00b58472":"outliers_std_Y","03b8acc9":"# simple box-plot visualization\n\nplt.boxplot([df_train.x,df_train.y],   boxprops=dict(color='blue'), labels=['x','y'])","a9061054":"# Reshape train and test data to apply regression\n\nX_train = df_train['x'].values.reshape(-1,1)\nY_train = df_train['y'].values.reshape(-1,1)\n\nX_test = df_test['x'].values.reshape(-1,1)\nY_test = df_test['y'].values.reshape(-1,1)","dc8aef39":"# Call Linear Regression fuction\n\nfrom sklearn.linear_model import LinearRegression\n\nlr = LinearRegression() # create lr object to fit our data\n\nlr.fit(X_train,Y_train) # fit the formula our train dataset","67e8e3fe":"# Make Prediction Test Data\n\ny_pred = lr.predict(X_test)\n\ndf_test['Predicted'] = y_pred\n\ndf_test","2436c344":"#Calculation of Mean Squared Error (MSE)\n\n# Test Data\nTest_data_MSE = mean_squared_error(Y_test,df_test.Predicted)\n\nTest_data_MSE","65ffa434":"# y and y_pred visualiation\n\nplt.scatter(X_test, Y_test, color = 'red')\nplt.plot(X_test,df_test.Predicted, color = 'blue')","e4974f0c":"#Calculation of Mean Absolute Error (MAE)\n\n# Test Data\nTest_data_MAE = mean_absolute_error(Y_test,df_test.Predicted)\n\nTest_data_MAE","dad8af56":"#Calculation of r2_score (R2)\n\n# Test Data\nTest_data_R2 = r2_score(Y_test,df_test.Predicted)\n\nTest_data_R2","49f7061e":"So far now, we made prediction. Then, we evaluate performance of our model. For that, there are a lot metrics, including;\n\n**Mean Squared Error**\n\n**Mean Absolute Error**\n\n**R-squared****\n\nRoot Mean-squared Error\n\nRoot Mean Squared Log Error\n\nAdjusted R Squared\n\n\n..\n\nWe will look at above first 3 metrics to understand success of our model.","61b590b3":"When we look at the data by quick way with the describe function, wee see one missing value in the \"y\" column in the train dataset. ","90df5fa7":"In the test dataset, there is no any missing value.","0155281f":"We have generated a stong prediction model and catch strong linear regression about our data; we can show the r sqaured score (0.99) as the evidence for that. ","e203395a":"# **3. Model Evaluation**","6090a740":"First of all, we reshape our data to apply Linear Regression formula, then we call Linear Regression function from Sckit-Learn library and predict our data.","aaad1cfe":"# 1. **Data Preprocessing**","6ba51aef":"**C) Handle with Outliers**","70f00ad6":"There ara a lot ways to determine outliers, including\n* Standart Deviation\n* Quartile Range\n* Z score\n* Winsorization\n* Local Outlier Factor\n...\n\nWe will use **Standart Deviation** method to find outliers in the analysis.","0b0426f8":"**B) Handle with Missing Values**","cdddbb05":"**B) Mean Absolute Error**","5e0b14e2":"#  **Linear Regression**\n\n**1. Data Preprocessing**\n\n    A) First Glimpse at Data\n    \n    B) Handle with missing value\n    \n    C) Handle with outliers\n        \n**2. Regression with Sckit-Learn**\n\n**3. Model Evaluation**\n\n    A) Mean Squared Error\n    \n    B) Mean Absolute Error\n    \n    C) R-squared\n    ","310a0f32":"# **2. Regression with Sckit-Learn**","5004cc08":"**A) Mean Squared Error -- MSE**","47e43e76":"**A) **First Glimpse at Data****","1b74d8ff":"We see that our outlier list for X columns return empty, so there is no outlier in the dataset for X dimension. Also, we reach same result for Y dimension. Therefore, we do not need to remove outliers from our dataset. However, lets look at data by a visualization"}}