{"cell_type":{"8f325d79":"code","da2a8e2a":"code","87f75864":"code","426940d3":"code","561b3226":"code","d5e89d27":"code","c1679ace":"code","a3bf558a":"code","9bd2fe67":"code","06b0a55e":"code","a26e8e19":"code","a887f6b9":"code","6e81015f":"code","50d62ee3":"code","7edd72db":"code","d535aafc":"code","822fa996":"code","60f03954":"code","3b8ada87":"code","44c3c8b0":"code","830e2ff2":"code","fa596b73":"code","0c21004f":"code","929baac2":"code","2a20d73a":"code","19d1511f":"code","6e3d621e":"code","c1ec598d":"code","82695ab3":"code","dd70b421":"code","1418e8c6":"code","da09a453":"code","725a0229":"code","611cd467":"code","0adec907":"code","70d1882b":"code","109be745":"code","42ceb86d":"code","9535b072":"code","f12e09a5":"code","e5d5fd81":"code","123e005e":"code","d72e1812":"code","e388a4a0":"code","7d9935f9":"code","c1bf0e5a":"code","6e6364e8":"code","09242620":"code","0823af36":"code","1f7e69ac":"code","342a2ba6":"code","bb27d06c":"code","e5ab2c92":"code","00598b7b":"code","321658a4":"code","02256eeb":"code","e15003e4":"code","66d2b175":"code","50a252a7":"code","1a32a32b":"code","5d1a5cdb":"code","7443d07d":"code","117e675e":"code","36df38f6":"code","58c13666":"code","83339315":"code","7dae3d7c":"code","0ae770a0":"code","3292a8e4":"code","7759177e":"code","00db416d":"code","b6a81460":"code","51309a70":"code","34ef6ede":"code","882d454e":"code","667aac35":"code","d7a8815f":"code","0f3d5cd8":"code","944d92cd":"code","06823925":"code","07e7269e":"code","8306f485":"code","6704f517":"code","d1221c88":"code","c92a89fe":"code","04817ed1":"code","b019de63":"code","4fddde55":"code","3f3e7f95":"code","e3cc04ef":"code","45642557":"code","3c749585":"code","7b103884":"code","cd744df2":"code","6af83d03":"code","5ec00d45":"code","140b3519":"markdown","2b60eff5":"markdown","6388eb33":"markdown","1352a11f":"markdown","e64ffe7c":"markdown","7ca6b150":"markdown","7b66fd4c":"markdown","6d1c9cbf":"markdown","2e1f1af9":"markdown","d5e03d97":"markdown","bcb7aada":"markdown","bfb38a2b":"markdown","c6ceebc3":"markdown","6dc11548":"markdown","9e0aada8":"markdown","fe9f3fcd":"markdown","8133fdcb":"markdown","559a2aed":"markdown","3cdfa127":"markdown","584a3e2b":"markdown","2255a02c":"markdown","afed5c87":"markdown","75582c8a":"markdown","a378d0ad":"markdown","d63ce7d5":"markdown","65dbaabf":"markdown","856859cb":"markdown","f62f737e":"markdown","c15ff2f4":"markdown","2399b8ba":"markdown","21b1b0af":"markdown","a5f6c554":"markdown","e7e96b4d":"markdown","d7eea935":"markdown","5f47ca55":"markdown","8694450f":"markdown","87e20197":"markdown","6f96bfde":"markdown","d409a323":"markdown","95fe46a4":"markdown","fce7c4d2":"markdown","0e633016":"markdown","8d607fca":"markdown","1c63b0d8":"markdown","8a48c5c4":"markdown","041ea5bd":"markdown","2b6bbf32":"markdown","137af028":"markdown","0d8b51de":"markdown","52132496":"markdown","80189720":"markdown","276e12e5":"markdown","04ac4183":"markdown","864bf57f":"markdown","88287d56":"markdown","c6a9a61d":"markdown","2b63671e":"markdown","d43b0d2b":"markdown","c6d36d48":"markdown","2bf85dd4":"markdown","7a0e53e7":"markdown","01d9208c":"markdown","4415c82b":"markdown","a07d60cd":"markdown","2d96a8e6":"markdown","be225cbb":"markdown","74473f12":"markdown","3e109ee7":"markdown","d4ef29ab":"markdown"},"source":{"8f325d79":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nplt.style.use('fivethirtyeight')\nmatplotlib.rcParams['font.family'] = \"Arial\"\n\nfrom plotly.offline import download_plotlyjs, init_notebook_mode, plot, iplot\nimport plotly as py\nimport plotly.graph_objs as go\nfrom plotly.subplots import make_subplots\n\ninit_notebook_mode(connected=True)\n\nimport collections\nimport itertools\n\nimport scipy.stats as stats\nfrom scipy.stats import norm\nfrom scipy.special import boxcox1p\n\nimport statsmodels\nimport statsmodels.api as sm\n#print(statsmodels.__version__)\n\nfrom sklearn.preprocessing import scale, StandardScaler, RobustScaler, OneHotEncoder\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV, KFold, StratifiedKFold, RandomizedSearchCV\nfrom sklearn.linear_model import Ridge, RidgeCV, Lasso, LassoCV, LinearRegression, ElasticNet,  HuberRegressor\nfrom sklearn.metrics import mean_squared_error, r2_score\nfrom xgboost import XGBRegressor\nfrom sklearn.gaussian_process import GaussianProcessRegressor\nfrom sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\nfrom sklearn.svm import SVR\nfrom sklearn.kernel_ridge import KernelRidge\nfrom sklearn.utils import resample\n\nfrom xgboost import XGBRegressor\n\n#Model interpretation modules\nimport eli5\nimport lime\nimport lime.lime_tabular\nimport shap\nshap.initjs()\n\nimport warnings\nwarnings.filterwarnings(\"ignore\", category=FutureWarning)","da2a8e2a":"Combined_data = pd.read_csv('..\/input\/new-york-city-airbnb-open-data\/AB_NYC_2019.csv')\nCombined_data.head()","87f75864":"print('Number of features: {}'.format(Combined_data.shape[1]))\nprint('Number of examples: {}'.format(Combined_data.shape[0]))","426940d3":"#for c in df.columns:\n#    print(c, dtype(df_train[c]))\nCombined_data.dtypes","561b3226":"Combined_data['last_review'] = pd.to_datetime(Combined_data['last_review'],infer_datetime_format=True) ","d5e89d27":"total = Combined_data.isnull().sum().sort_values(ascending=False)\npercent = (Combined_data.isnull().sum())\/Combined_data.isnull().count().sort_values(ascending=False)\nmissing_data = pd.concat([total, percent], axis=1, keys=['Total','Percent'], sort=False).sort_values('Total', ascending=False)\nmissing_data.head(40)","c1679ace":"Combined_data.drop(['host_name','name'], axis=1, inplace=True)","a3bf558a":"Combined_data[Combined_data['number_of_reviews']== 0.0].shape","9bd2fe67":"Combined_data['reviews_per_month'] = Combined_data['reviews_per_month'].fillna(0)","06b0a55e":"earliest = min(Combined_data['last_review'])\nCombined_data['last_review'] = Combined_data['last_review'].fillna(earliest)\nCombined_data['last_review'] = Combined_data['last_review'].apply(lambda x: x.toordinal() - earliest.toordinal())","a26e8e19":"total = Combined_data.isnull().sum().sort_values(ascending=False)\npercent = (Combined_data.isnull().sum())\/Combined_data.isnull().count().sort_values(ascending=False)\nmissing_data = pd.concat([total, percent], axis=1, keys=['Total','Percent'], sort=False).sort_values('Total', ascending=False)\nmissing_data.head(40)","a887f6b9":"fig, axes = plt.subplots(1,3, figsize=(21,6))\nsns.distplot(Combined_data['price'], ax=axes[0])\nsns.distplot(np.log1p(Combined_data['price']), ax=axes[1])\naxes[1].set_xlabel('log(1+price)')\nsm.qqplot(np.log1p(Combined_data['price']), stats.norm, fit=True, line='45', ax=axes[2]);","6e81015f":"Combined_data = Combined_data[np.log1p(Combined_data['price']) < 8]\nCombined_data = Combined_data[np.log1p(Combined_data['price']) > 3]","50d62ee3":"fig, axes = plt.subplots(1,3, figsize=(21,6))\nsns.distplot(Combined_data['price'], ax=axes[0])\nsns.distplot(np.log1p(Combined_data['price']), ax=axes[1])\naxes[1].set_xlabel('log(1+price)')\nsm.qqplot(np.log1p(Combined_data['price']), stats.norm, fit=True, line='45', ax=axes[2]);","7edd72db":"Combined_data['price'] = np.log1p(Combined_data['price'])","d535aafc":"print(Combined_data.columns)","822fa996":"print('In this dataset there are {} unique hosts renting out  a total number of {} properties.'.format(len(Combined_data['host_id'].unique()), Combined_data.shape[0]))","60f03954":"Combined_data = Combined_data.drop(['host_id', 'id'], axis=1)","3b8ada87":"sns.catplot(x='neighbourhood_group', kind='count' ,data=Combined_data)\nfig = plt.gcf()\nfig.set_size_inches(12, 6)","44c3c8b0":"fig, axes = plt.subplots(1,3, figsize=(21,6))\nsns.distplot(Combined_data['latitude'], ax=axes[0])\nsns.distplot(Combined_data['longitude'], ax=axes[1])\nsns.scatterplot(x= Combined_data['latitude'], y=Combined_data['longitude'])","830e2ff2":"sns.catplot(x='room_type', kind='count' ,data=Combined_data)\nfig = plt.gcf()\nfig.set_size_inches(8, 6)","fa596b73":"fig, axes = plt.subplots(1,2, figsize=(21, 6))\n\nsns.distplot(Combined_data['minimum_nights'], rug=False, kde=False, color=\"green\", ax = axes[0])\naxes[0].set_yscale('log')\naxes[0].set_xlabel('minimum stay [nights]')\naxes[0].set_ylabel('count')\n\nsns.distplot(np.log1p(Combined_data['minimum_nights']), rug=False, kde=False, color=\"green\", ax = axes[1])\naxes[1].set_yscale('log')\naxes[1].set_xlabel('minimum stay [nights]')\naxes[1].set_ylabel('count')","0c21004f":"Combined_data['minimum_nights'] = np.log1p(Combined_data['minimum_nights'])","929baac2":"fig, axes = plt.subplots(1,2,figsize=(18.5, 6))\nsns.distplot(Combined_data[Combined_data['reviews_per_month'] < 17.5]['reviews_per_month'], rug=True, kde=False, color=\"green\", ax=axes[0])\nsns.distplot(np.sqrt(Combined_data[Combined_data['reviews_per_month'] < 17.5]['reviews_per_month']), rug=True, kde=False, color=\"green\", ax=axes[1])\naxes[1].set_xlabel('ln(reviews_per_month)')","2a20d73a":"fig, axes = plt.subplots(1,1, figsize=(21,6))\nsns.scatterplot(x= Combined_data['availability_365'], y=Combined_data['reviews_per_month'])","19d1511f":"Combined_data['reviews_per_month'] = Combined_data[Combined_data['reviews_per_month'] < 17.5]['reviews_per_month']","6e3d621e":"fig, axes = plt.subplots(1,1,figsize=(18.5, 6))\nsns.distplot(Combined_data['availability_365'], rug=False, kde=False, color=\"blue\", ax=axes)\naxes.set_xlabel('availability_365')\naxes.set_xlim(0, 365)","c1ec598d":"Combined_data['all_year_avail'] = Combined_data['availability_365']>353\nCombined_data['low_avail'] = Combined_data['availability_365']< 12\nCombined_data['no_reviews'] = Combined_data['reviews_per_month']==0","82695ab3":"corrmatrix = Combined_data.corr()\nf, ax = plt.subplots(figsize=(15,12))\nsns.heatmap(corrmatrix, vmax=0.8, square=True)\nsns.set(font_scale=0.8)","dd70b421":"# sns.pairplot(Combined_data.select_dtypes(exclude=['object']))","1418e8c6":"categorical_features = Combined_data.select_dtypes(include=['object'])\nprint('Categorical features: {}'.format(categorical_features.shape))","da09a453":"categorical_features_one_hot = pd.get_dummies(categorical_features)\ncategorical_features_one_hot.head()","725a0229":"Combined_data['reviews_per_month'] = Combined_data['reviews_per_month'].fillna(0)","611cd467":"numerical_features =  Combined_data.select_dtypes(exclude=['object'])\ny = numerical_features.price\nnumerical_features = numerical_features.drop(['price'], axis=1)\nprint('Numerical features: {}'.format(numerical_features.shape))","0adec907":"X = np.concatenate((numerical_features, categorical_features_one_hot), axis=1)\nX_df = pd.concat([numerical_features, categorical_features_one_hot], axis=1)\n#print('Dimensions of the design matrix: {}'.format(X.shape))\n#print('Dimension of the target vector: {}'.format(y.shape))","70d1882b":"Processed_data = pd.concat([X_df, y], axis = 1)\nProcessed_data.to_csv('NYC_Airbnb_Processed.dat')","109be745":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)","42ceb86d":"print('Dimensions of the training feature matrix: {}'.format(X_train.shape))\nprint('Dimensions of the training target vector: {}'.format(y_train.shape))\nprint('Dimensions of the test feature matrix: {}'.format(X_test.shape))\nprint('Dimensions of the test target vector: {}'.format(y_test.shape))","9535b072":"scaler = RobustScaler()\nX_train = scaler.fit_transform(X_train)\nX_test = scaler.fit_transform(X_test)","f12e09a5":"n_folds = 5\n\n# squared_loss\ndef rmse_cv(model):\n    kf = KFold(n_folds, shuffle=True, random_state = 91).get_n_splits(numerical_features)\n    return cross_val_score(model, X_train, y_train, scoring='neg_mean_squared_error', cv=kf)\n\ndef rmse_lv_cv(model):\n    kf = KFold(n_folds, shuffle=True, random_state = 91).get_n_splits(numerical_features)\n    return cross_val_score(model, Xlv_train, y_train, scoring='neg_mean_squared_error', cv=kf)","e5d5fd81":"for Model in [LinearRegression, Ridge, Lasso, ElasticNet, RandomForestRegressor, XGBRegressor, HuberRegressor]:\n    if Model == XGBRegressor: cv_res = rmse_cv(XGBRegressor(objective='reg:squarederror'))\n    else: cv_res = rmse_cv(Model())\n    print('{}: {:.5f} +\/- {:5f}'.format(Model.__name__, -cv_res.mean(), cv_res.std()))","123e005e":"alphas1 = [0.05, 0.1, 0.3, 1, 3, 5, 10, 15, 30, 50, 75] \ncv_ridge1 = [-rmse_cv(Ridge(alpha = alpha)).mean() for alpha in alphas1]","d72e1812":"alphas2 = [0.5*i for i in range(4,12)]\ncv_ridge2 = [-rmse_cv(Ridge(alpha = alpha)).mean() for alpha in alphas2]","e388a4a0":"cv_ridge1 = pd.Series(cv_ridge1, index = alphas1) \ncv_ridge2 = pd.Series(cv_ridge2, index = alphas2) \n\nfig, axes = plt.subplots(1,2,figsize=(21, 8))\ncv_ridge1.plot(title = \"Ridge Regression Cross-Validation\", style='-o', ax = axes[0]) \naxes[0].set_xlabel(\"alpha\") \naxes[0].set_ylabel(\"rmse\")\naxes[0].set_xscale('log')\n\ncv_ridge2.plot(title = \"Ridge Regression Cross-Validation\", style='-o', ax = axes[1]) \naxes[1].set_xlabel(\"alpha\") \naxes[1].set_ylabel(\"rmse\")\naxes[1].set_xscale('log')\n\n#RR_best = Ridge(alpha = np.argmin(cv_ridge)) RR_best.fit(X_train, y_train) predicted_prices = RR_best.predict(test_data)\n\n","7d9935f9":"fig = go.Figure()\n\nfig.add_trace(\n    go.Scatter(\n        x = alphas1,\n        y= cv_ridge1,\n        line = dict(color='royalBlue', width=2)\n        ),\n)\n\n\n\nfig.update_layout(\n   \n    xaxis=go.layout.XAxis(\n        title=go.layout.xaxis.Title(\n            text=\"Penalty\",\n            font=dict(\n                size=16\n            )\n        )\n    ),\n    \n    yaxis=go.layout.YAxis(\n        title=go.layout.yaxis.Title(\n            text=\"Cross-validation error\",\n            font=dict(\n                size=16\n            )\n        )\n    ),\n)\n\n\nfig.update_layout(height=400,\n                width = 600,\n                title = 'Telescopic Search: Coarse level', \n                  xaxis_type=\"log\", \n                  showlegend=False)\n\nfig.show()","c1bf0e5a":"fig = go.Figure()\n\nfig.add_trace(\n    go.Scatter(\n        x = alphas2,\n        y= cv_ridge2,\n        line = dict(color='crimson', width=2)\n        ),\n)\n\n\n\nfig.update_layout(\n   \n    xaxis=go.layout.XAxis(\n        title=go.layout.xaxis.Title(\n            text=\"Penalty\",\n            font=dict(\n                size=16\n            )\n        )\n    ),\n    \n    yaxis=go.layout.YAxis(\n        title=go.layout.yaxis.Title(\n            text=\"Cross-validation error\",\n            font=dict(\n                size=16\n            ),\n        )\n    ),\n)\n\n\nfig.update_layout(height=400,\n                width = 600,\n                title = 'Telescopic Search: Fine level', \n                  xaxis_type=\"log\", \n                  showlegend=False)\n\nfig.show()","6e6364e8":"best_alpha = alphas2[np.argmin(cv_ridge2.values)]\nRR_CV_best = -rmse_cv(Ridge(alpha = best_alpha))\nRR = Ridge(alpha = best_alpha) \nRR.fit(X_train, y_train) \ny_train_RR = RR.predict(X_train)\ny_test_RR = RR.predict(X_test)\nridge_results = pd.DataFrame({'algorithm':['Ridge Regression'],\n            'CV error': RR_CV_best.mean(), \n            'CV std': RR_CV_best.std(),\n            'training error': [mean_squared_error(y_train, y_train_RR)],\n            'test error': [mean_squared_error(y_test_RR, y_test_RR)],\n            'training_r2_score': [r2_score(y_train, y_train_RR)],\n            'test_r2_score': [r2_score(y_test, y_test_RR)]})\nridge_results","09242620":"explainer = lime.lime_tabular.LimeTabularExplainer(X_train, feature_names=X_df.columns, class_names=['price'], verbose=True, mode='regression')","0823af36":"i=25\nexp = explainer.explain_instance(X_test[i], RR.predict, num_features=5)\nitem = pd.DataFrame(scaler.inverse_transform(X_test[i].reshape(1,-1))[0], index=X_df.columns)\nitem.loc['minimum_nights'] = np.expm1(item.loc['minimum_nights'])\nitem.loc['true_price'] = np.exp(y_test.iloc[i])\n#print(exp.intercept)\n#print(exp.local_pred)\nitem.loc['predicted_price'] = np.exp(exp.local_pred)\nitem[(item.select_dtypes(include=['number']) != 0).any(1)]","1f7e69ac":"exp.show_in_notebook(show_table=True)","342a2ba6":"i=0\nexp = explainer.explain_instance(X_test[i], RR.predict, num_features=5)\nitem = pd.DataFrame(scaler.inverse_transform(X_test[i].reshape(1,-1))[0], index=X_df.columns)\nitem.loc['minimum_nights'] = np.expm1(item.loc['minimum_nights'])\nitem.loc['true_price'] = np.exp(y_test.iloc[i])\n#print(exp.intercept)\n#print(exp.local_pred)\nitem.loc['predicted_price'] = np.exp(exp.local_pred)\nitem[(item.select_dtypes(include=['number']) != 0).any(1)]","bb27d06c":"exp.show_in_notebook(show_table=True)","e5ab2c92":"i=78\nexp = explainer.explain_instance(X_test[i], RR.predict, num_features=5)\nitem = pd.DataFrame(scaler.inverse_transform(X_test[i].reshape(1,-1))[0], index=X_df.columns)\nitem.loc['minimum_nights'] = np.expm1(item.loc['minimum_nights'])\nitem.loc['true_price'] = np.exp(y_test.iloc[i])\n#print(exp.intercept)\n#print(exp.local_pred)\nitem.loc['ridge_prediction_price'] = np.exp(exp.local_pred)\nitem[(item.select_dtypes(include=['number']) != 0).any(1)]","00598b7b":"exp.show_in_notebook(show_table=True)","321658a4":"i=395\nexp = explainer.explain_instance(X_test[i], RR.predict, num_features=5)\nitem = pd.DataFrame(scaler.inverse_transform(X_test[i].reshape(1,-1))[0], index=X_df.columns)\nitem.loc['minimum_nights'] = np.expm1(item.loc['minimum_nights'])\nitem.loc['true_price'] = np.exp(y_test.iloc[i])\n#print(exp.intercept)\n#print(exp.local_pred)\nitem.loc['ridge_prediction_price'] = np.exp(exp.local_pred)\nitem[(item.select_dtypes(include=['number']) != 0).any(1)]","02256eeb":"exp.show_in_notebook(show_table=True)","e15003e4":"explainer_sh = shap.LinearExplainer(RR, X_train, feature_dependence='independent')\nshap_values = explainer_sh.shap_values(X_test)\nshap.summary_plot(shap_values, X_test, feature_names=X_df.columns)","66d2b175":"alphas = [0.0001, 0.001, 0.005,0.01, 0.05, 0.1, 0.3, 1] \ncv_lasso = [-rmse_cv(Lasso(alpha = alpha, max_iter=2000)).mean() for alpha in alphas]","50a252a7":"cv_lasso = pd.Series(cv_lasso, index = alphas) \ncv_lasso.plot(title = \"LASSO Regression Cross-Validation\", style='-+') \nplt.xlabel(\"alpha\") \nplt.ylabel(\"rmse\") \nplt.xscale('log')","1a32a32b":"fig = go.Figure()\n\nfig.add_trace(\n    go.Scatter(\n        x = alphas,\n        y= cv_lasso,\n        line = dict(color='crimson', width=2)\n        ),\n)\n\n\n\nfig.update_layout(\n   \n    xaxis=go.layout.XAxis(\n        title=go.layout.xaxis.Title(\n            text=\"Penalty\",\n            font=dict(\n                size=16\n            )\n        )\n    ),\n    \n    yaxis=go.layout.YAxis(\n        title=go.layout.yaxis.Title(\n            text=\"Cross-validation error\",\n            font=dict(\n                size=16\n            ),\n        )\n    ),\n)\n\n\nfig.update_layout(height=400,\n                width = 600,\n                title = 'Lasso penalty optimization', \n                  xaxis_type=\"log\", \n                  showlegend=False)\n\nfig.show()","5d1a5cdb":"best_alpha = alphas[np.argmin(cv_lasso.values)]\nlasso_CV_best = -rmse_cv(Lasso(alpha = best_alpha))\nlasso = Lasso(alpha = best_alpha) \nlasso.fit(X_train, y_train) \ny_train_lasso = lasso.predict(X_train)\ny_test_lasso = lasso.predict(X_test)\nlasso_results = pd.DataFrame({'algorithm':['LASSO Regression'],\n            'CV error': lasso_CV_best.mean(), \n            'CV std': lasso_CV_best.std(),\n            'training error': [mean_squared_error(y_train_lasso, y_train)],\n            'test error': [mean_squared_error(y_test_lasso, y_test)],\n            'training_r2_score': [r2_score(y_train, y_train_lasso)],\n            'test_r2_score': [r2_score(y_test, y_test_lasso)]})\nlasso_results","7443d07d":"features = list(categorical_features_one_hot.columns) + list(numerical_features.columns)\ncoef = pd.Series(lasso.coef_, index = features)\nprint(\"Lasso picked \" + str(sum(coef != 0)) + \" variables and eliminated the other \" +  str(sum(coef == 0)) + \" variables\")","117e675e":"coef","36df38f6":"imp_coef = pd.concat([coef.sort_values().iloc[:10],\n                     coef.sort_values().iloc[-10:]])\nmatplotlib.rcParams['figure.figsize'] = (8.0, 10.0)\nimp_coef.plot(kind = \"barh\")\nplt.title(\"Coefficients in the Lasso Model\")","58c13666":"i=25\nexp = explainer.explain_instance(X_test[i], lasso.predict, num_features=5)","83339315":"exp.show_in_notebook(show_table=True)","7dae3d7c":"explainer = shap.LinearExplainer(lasso, X_train, feature_dependence='independent')\nshap_values = explainer.shap_values(X_test)\nshap.summary_plot(shap_values, X_test, feature_names=X_df.columns)","0ae770a0":"alphas = [0.0001, 0.001, 0.005,0.01, 0.05, 0.1, 0.3, 1] \n#cv_huber = [-rmse_cv(HuberRegressor(alpha = alpha, max_iter=2000)).mean() for alpha in alphas]\ncv_huber = [0.20051906841425277, 0.20044833042114646, 0.20048899799050565, 0.200533996471012, 0.20051788009059482, 0.2005294886778608, 0.20052011204607623, 0.2004070661477452]","3292a8e4":"cv_huber = pd.Series(cv_huber, index = alphas) \ncv_huber.plot(title = \"Huber Regression Cross-Validation\", style='-o') \nplt.xlabel(\"alpha\") \nplt.ylabel(\"rmse\") \nplt.xscale('log')\n","7759177e":"fig = go.Figure()\n\nfig.add_trace(\n    go.Scatter(\n        x = alphas,\n        y= cv_huber,\n        line = dict(color='crimson', width=2)\n        ),\n)\n\n\n\nfig.update_layout(\n   \n    xaxis=go.layout.XAxis(\n        title=go.layout.xaxis.Title(\n            text=\"Penalty\",\n            font=dict(\n                size=16\n            )\n        )\n    ),\n    \n    yaxis=go.layout.YAxis(\n        title=go.layout.yaxis.Title(\n            text=\"Cross-validation error\",\n            font=dict(\n                size=16\n            ),\n        )\n    ),\n)\n\n\nfig.update_layout(height=400,\n                width = 600,\n                title = 'Lasso penalty optimization', \n                  xaxis_type=\"log\", \n                  showlegend=False)\n\nfig.show()","00db416d":"best_alpha = alphas[np.argmin(cv_huber.values)]\nhuber_CV_best = -rmse_cv(HuberRegressor(alpha=best_alpha))\nhuber = HuberRegressor(alpha=best_alpha)\nhuber.fit(X_train, y_train) \ny_train_huber = huber.predict(X_train)\ny_test_huber = huber.predict(X_test)\nhuber_results = pd.DataFrame({'algorithm':['Huber Regression'],\n            'CV error': huber_CV_best.mean(), \n            'CV std': huber_CV_best.std(),\n            'training error': [mean_squared_error(y_train, y_train_huber)],\n            'test error': [mean_squared_error(y_test, y_test_huber)],\n            'training_r2_score': [r2_score(y_train, y_train_huber)],\n            'test_r2_score': [r2_score(y_test, y_test_huber)]})\nhuber_results","b6a81460":"lasso_coef = coef[coef!=0]\nXlv = X_df[list(lasso_coef.index)]\n#X_lasso_vars.shape\nXlv_train, Xlv_test, y_train, y_test = train_test_split(Xlv, y, test_size=0.2, random_state=42)\nprint('Dimensions of the training feature matrix for lasso variable selection: {}'.format(Xlv_train.shape))\nprint('Dimensions of the test feature matrix for lasso variable selection: {}'.format(Xlv_test.shape))","51309a70":"for Model in [LinearRegression, Ridge, Lasso, ElasticNet, RandomForestRegressor, XGBRegressor, HuberRegressor]:\n    if Model == XGBRegressor: cv_res = rmse_cv(XGBRegressor(objective='reg:squarederror'))\n    else: cv_res = rmse_lv_cv(Model())\n    print('{}: {:.5f} +\/- {:5f}'.format(Model.__name__, -cv_res.mean(), cv_res.std()))","34ef6ede":"alphas1 = [0.05, 0.1, 0.3, 1, 3, 5, 10, 15, 30, 50, 75] \ncv_ridge1 = [-rmse_lv_cv(Ridge(alpha = alpha)).mean() for alpha in alphas1]","882d454e":"alphas2 = [1.4+0.05*i for i in range(1,12)]\ncv_ridge2 = [-rmse_lv_cv(Ridge(alpha = alpha)).mean() for alpha in alphas2]","667aac35":"cv_ridge1 = pd.Series(cv_ridge1, index = alphas1) \ncv_ridge2 = pd.Series(cv_ridge2, index = alphas2) \n\nfig, axes = plt.subplots(1,2,figsize=(21, 8))\ncv_ridge1.plot(title = \"Ridge Regression Cross-Validation\", style='-o', ax = axes[0]) \naxes[0].set_xlabel(\"alpha\") \naxes[0].set_ylabel(\"rmse\")\naxes[0].set_xscale('log')\n\ncv_ridge2.plot(title = \"Ridge Regression Cross-Validation\", style='-o', ax = axes[1]) \naxes[1].set_xlabel(\"alpha\") \naxes[1].set_ylabel(\"rmse\")\n#axes[1].set_xscale('log')\n\n#RR_best = Ridge(alpha = np.argmin(cv_ridge)) RR_best.fit(X_train, y_train) predicted_prices = RR_best.predict(test_data)","d7a8815f":"best_alpha = alphas2[np.argmin(cv_ridge2.values)]\nRR_lassoVars_CV_best = -rmse_lv_cv(Ridge(alpha = best_alpha))\nRR_lassoVars = Ridge(alpha = best_alpha) \nRR_lassoVars.fit(Xlv_train, y_train) \ny_train_RR_lassoVars = RR_lassoVars.predict(Xlv_train)\ny_test_RR_lassoVars = RR_lassoVars.predict(Xlv_test)\nridge_lassoVars_results = pd.DataFrame({'algorithm':['Ridge Regression with LASSO variable selection'],\n            'CV error': RR_lassoVars_CV_best.mean(), \n            'CV std': RR_lassoVars_CV_best.std(),\n            'training error': [mean_squared_error(y_train, y_train_RR_lassoVars)],\n            'test error': [mean_squared_error(y_test, y_test_RR_lassoVars)],\n            'training_r2_score': [r2_score(y_train, y_train_RR_lassoVars)],\n            'test_r2_score': [r2_score(y_test, y_test_RR_lassoVars)]})","0f3d5cd8":"rfr_CV_baseline = -rmse_cv(RandomForestRegressor(random_state=42))\nrfr_baseline = RandomForestRegressor(random_state=42)\nrfr_baseline.fit(X_train, y_train) \ny_train_rfr = rfr_baseline.predict(X_train)\ny_test_rfr = rfr_baseline.predict(X_test)\nrfr_baseline_results = pd.DataFrame({'algorithm':['Random Forest Regressor [baseline]'],\n            'CV error': rfr_CV_baseline.mean(), \n            'CV std': rfr_CV_baseline.std(),\n            'training error': [mean_squared_error(y_train_rfr, y_train)],\n            'test error': [mean_squared_error(y_test_rfr, y_test)]})","944d92cd":"rfr_baseline_results","06823925":"print(rfr_baseline.estimators_)","07e7269e":"eli5.show_weights(rfr_baseline, feature_names=list(X_df.columns))","8306f485":"rf = RandomForestRegressor(random_state=42)\nfrom pprint import pprint\nprint('Parameters currently in use: \\n')\npprint(rf.get_params())","6704f517":"#Number of trees in the forest\nn_estimators = [int(x) for x in np.linspace(start = 200, stop=2000,num=10)]\nmax_features = ['auto', 'sqrt']\nmax_depth = [int(x) for x in np.linspace(10, 110, num=11)]\nmax_depth.append(None)\nmin_samples_split = [2,5,10]\nmin_samples_leaf = [1, 2, 4]\nbootstrap = [True, False]\n\nrandom_grid = {'n_estimators': n_estimators,\n        'max_features': max_features,\n        'max_depth': max_depth,\n        'min_samples_split': min_samples_split,\n        'min_samples_leaf': min_samples_leaf,\n        'bootstrap': bootstrap}\n\npprint(random_grid)","d1221c88":"rf_random = RandomizedSearchCV(estimator=rf, param_distributions = random_grid, n_iter=10, cv = 3, verbose=2, random_state=42, n_jobs=-1)\n\nrf_random.fit(X_train, y_train)","c92a89fe":"#best_random = rf_random.best_estimator_\nbest_random = RandomForestRegressor(bootstrap=True, criterion='mse', max_depth=30,\n                      max_features='sqrt', max_leaf_nodes=None,\n                      min_impurity_decrease=0.0, min_impurity_split=None,\n                      min_samples_leaf=1, min_samples_split=5,\n                      min_weight_fraction_leaf=0.0, n_estimators=1400,\n                      n_jobs=None, oob_score=False, random_state=42, verbose=0,\n                      warm_start=False)\nrfr_CV_best = -rmse_cv(best_random)\nbest_random.fit(X_train, y_train) \ny_train_rfr = best_random.predict(X_train)\ny_test_rfr = best_random.predict(X_test)\nrfr_best_results = pd.DataFrame({'algorithm':['Random Forest Regressor'],\n            'CV error': rfr_CV_best.mean(), \n            'CV std': rfr_CV_best.std(),\n            'training error': [mean_squared_error(y_train, y_train_rfr)],\n            'test error': [mean_squared_error(y_test, y_test_rfr)],\n            'training_r2_score': [r2_score(y_train, y_train_rfr)],\n            'test_r2_score': [r2_score(y_test, y_test_rfr)]})\nrfr_best_results","04817ed1":"eli5.show_weights(best_random, feature_names=list(X_df.columns))","b019de63":"xgb_baseline = XGBRegressor(n_estimators=1000, learning_rate=0.05, early_stopping=5)\nkf = KFold(n_folds, shuffle=True, random_state = 91).get_n_splits(numerical_features)\ncv_res = cross_val_score(xgb_baseline, X_train, y_train, scoring='neg_mean_squared_error', cv=kf)\nxgb_baseline.fit(X_train, y_train)\ny_train_xgb_base = xgb_baseline.predict(X_train)\ny_test_xgb_base = xgb_baseline.predict(X_test)\nxgb_baseline_results = pd.DataFrame({'algorithm':['XGBRegressor[baseline]'],\n            'CV error': cv_res.mean(), \n            'CV std': cv_res.std(),\n            'training error': [mean_squared_error(y_train_xgb_base, y_train)]})","4fddde55":"print(xgb_baseline)\nxgb_baseline_results","3f3e7f95":"d = {'Learning Rate':[],\n            'Mean CV Error': [],\n            'CV Error Std': [],\n            'Training Error': []}\nfor lr in [0.01, 0.05, 0.1, 0.5]:\n    continue\n    xgb_model = XGBRegressor(n_estimators=1000, learning_rate=lr, early_stopping=5)\n    cv_res = -cross_val_score(xgb_model, X_train, y_train, scoring='neg_mean_squared_error', cv=kf)\n    xgb_model.fit(X_train, y_train)\n    y_train_xgb = xgb_model.predict(X_train)\n    d['Learning Rate'].append(lr)\n    d['Mean CV Error'].append(cv_res.mean())\n    d['CV Error Std'].append(cv_res.std())\n    # makes no sense to look at max\/min when we only have 3 CV folds\n    #d['Max CV Error'].append(max(cv_res)\n    #d['Min CV Error'].append(max(cv_res)\n    d['Training Error'].append(mean_squared_error(y_train_xgb, y_train))\n\n# to run the search, comment out 'continue' in the for loop\n# here are the results I got\nd = {'Learning Rate':[0.01, 0.05, 0.1, 0.5],\n        'Mean CV Error': [0.184223, 0.177748, 0.175002, 0.188239],\n        'CV Error Std': [0.00626211, 0.00575213, 0.00544426, 0.00525595],\n        'Training Error': [0.179093, 0.164874, 0.154238, 0.109885]}\n\nxgb_tuning_1 = pd.DataFrame(d)\nxgb_tuning_1","e3cc04ef":"fig, ax = plt.subplots(1, 1, figsize=(20,6))\n\nax.plot(xgb_tuning_1['Learning Rate'], xgb_tuning_1['Mean CV Error'], color='red')\nax.plot(xgb_tuning_1['Learning Rate'], xgb_tuning_1['Mean CV Error'], 'o', color='black')\nax.fill_between(xgb_tuning_1['Learning Rate'], xgb_tuning_1['Mean CV Error'] - xgb_tuning_1['CV Error Std'], xgb_tuning_1['Mean CV Error'] + xgb_tuning_1['CV Error Std'], color='r', alpha=.1)\nax.plot(xgb_tuning_1['Learning Rate'], xgb_tuning_1['Training Error'], color='blue')\nax.plot(xgb_tuning_1['Learning Rate'], xgb_tuning_1['Training Error'], 'o', color='black')\nax.legend(fontsize=12, loc = 'center right');\nax.set_ylim(0.1, 0.2)\nax.set_xlabel('Learning Rate')\nax.set_ylabel('Mean Squared Error')\n#ax.set_title('')","45642557":"d = {'max_depth':[],\n             'min_child_weight': [],\n            'Mean CV Error': [],\n            'CV Error Std': [],\n            'Training Error': []}\nxgbreg = XGBRegressor(n_estimators=2, learning_rate=0.05, early_stopping=5)\nparams2 = {'max_depth': list(range(3,10,2)), 'min_child_weight': list(range(1,6,2))}\n#print(params2)\n#xgb_random.fit(X_train, y_train)\nkf = KFold(n_folds, shuffle=True, random_state = 91).get_n_splits(X_train)\nfor md in params2['max_depth']:\n    for mcw in params2['min_child_weight']:\n        continue\n        xgb_model = XGBRegressor(n_estimators=1000, learning_rate=0.1, early_stopping=5, max_depth=md, min_child_weight=mcw )\n        cv_res = -cross_val_score(xgb_model, X_train, y_train, scoring='neg_mean_squared_error', cv=kf)\n        xgb_model.fit(X_train, y_train)\n        y_train_xgb = xgb_model.predict(X_train)\n        d['max_depth'].append(md)\n        d['min_child_weight'].append(mcw)\n        d['Mean CV Error'].append(cv_res.mean())\n        d['CV Error Std'].append(cv_res.std())\n        # makes no sense to look at max\/min when we only have 3 CV folds\n        #d['Max CV Error'].append(max(cv_res)\n        #d['Min CV Error'].append(max(cv_res)\n        d['Training Error'].append(mean_squared_error(y_train_xgb, y_train))\n\n#print(d)\n\n# to run the search, comment out 'continue' in the for loop\n# here are the results I got\n\nd = {'max_depth': [3, 3, 3, 5, 5, 5, 7, 7, 7, 9, 9, 9], 'min_child_weight': [1, 3, 5, 1, 3, 5, 1, 3, 5, 1, 3, 5], \n 'Mean CV Error': [0.1750024956601357, 0.17483011840929769, 0.17493846554576997, 0.17309889297300166, 0.17316622731288867, \n        0.17351576928079232, 0.17662213266155447, 0.17623539711716868, 0.17586167155362295, 0.18027062402369495, 0.1795815552171006, 0.1794402792605232], \n 'CV Error Std': [0.0054442612607845196, 0.005346726848155686, 0.005781224325978589, 0.0047992091315554805, 0.005078460548746871, 0.0055470435006580825, \n                  0.004522282538112627, 0.005521088520254507, 0.005182127039391581, 0.00548502303198156, 0.0056636180606624885, 0.005837983614899652],\n 'Training Error': [0.15423828100740364, 0.1548338435116449, 0.15489721899341147, 0.1174713383813709, 0.11768836644071619, 0.11962286723882598, \n                    0.07157996439924702, 0.07249081997317249, 0.0809473890478948, 0.03364907441870936, 0.03787025803370217, 0.045449523400453724]}\n        \nxgb_tuning_2 = pd.DataFrame(d)\nxgb_tuning_2","3c749585":"fig, axes = plt.subplots(1, 2, figsize=(20,6))\n\ncolors = ['orange','green','blue','red']\n\nfor i, md in enumerate(params2['max_depth']):\n    color = colors[i]\n    xgb_tuning_3 = xgb_tuning_2[xgb_tuning_2['max_depth']==md]\n    axes[0].plot(xgb_tuning_3['min_child_weight'], xgb_tuning_3['Mean CV Error'], color=color, label= 'max_depth='+str(md))\n    axes[0].plot(xgb_tuning_3['min_child_weight'], xgb_tuning_3['Mean CV Error'], 'o', color='black', label='_nolegend_')\n    #ax.fill_between(xgb_tuning_3['Max_depth'], xgb_tuning_3['Mean CV Error'] - xgb_tuning_3['CV Error Std'], \n                    #xgb_tuning_3['Mean CV Error'] + xgb_tuning_3['CV Error Std'], color='r', alpha=.1, label='_nolegend_')\n    axes[1].plot(xgb_tuning_3['min_child_weight'], xgb_tuning_3['Training Error'], color=color, label='max_depth='+str(md))\n    axes[1].plot(xgb_tuning_3['min_child_weight'], xgb_tuning_3['Training Error'], 'o', color='black', label='_nolegend_')\n\nfor ax in axes:\n    ax.set_xlabel('min_child_weight')\n    ax.set_ylabel('Mean Squared Error')\n    \naxes[0].set_title('CV Error')\naxes[1].set_title('Training Error')\n\naxes[0].legend(fontsize=12, bbox_to_anchor=(0.6, .52, .7, .602), loc=3,);\naxes[0].set_ylim([0.172, 0.181])","7b103884":"print('Optimal parameter values are: ')\nbest = xgb_tuning_2.iloc[xgb_tuning_2.idxmin()['Mean CV Error']]\nprint('max_depth: {}'.format(int(best['max_depth'])))\nprint('min_child_weight: {}'.format(int(best['min_child_weight'])))","cd744df2":"xbgreg_best = XGBRegressor(n_estimators=1000, learning_rate=0.1, early_stopping=5, max_depth=5, min_child_weight=1 )\nxbgreg_CV_best = -rmse_cv(xbgreg_best)\nxbgreg_best.fit(X_train, y_train) \ny_train_xgbreg = xbgreg_best.predict(X_train)\ny_test_xgbreg = xbgreg_best.predict(X_test)\nxgb_best_results = pd.DataFrame({'algorithm':['XGBRegressor'],\n            'CV error': xbgreg_CV_best.mean(), \n            'CV std': xbgreg_CV_best.std(),\n            'training error': [mean_squared_error(y_train, y_train_xgbreg)],\n            'test error': [mean_squared_error(y_test, y_test_xgbreg)],\n            'training_r2_score': [r2_score(y_train, y_train_xgbreg)],\n            'test_r2_score': [r2_score(y_test, y_test_xgbreg)]})\nxgb_best_results","6af83d03":"eli5.show_weights(xgbreg_best, feature_names=list(X_df.columns))","5ec00d45":"pd.concat([ridge_results, lasso_results, ridge_lassoVars_results, huber_results, rfr_best_results, xgb_best_results], axis=0, ignore_index=True)","140b3519":"### 1.4.5 Minimum nights","2b60eff5":"I notice that Statten Island and the Bronx are highly underrepresented in this dataset. For Statten Island, the reason is that the population of the island is small. However, this can't be the case for the Bronx which has a population comparable (~1.4mln) to Manhattan (~1.6mln) or for for Brooklyn \/Queens with their populations of ~2.5mln and ~2.4mln, respectively. \n\nThis makes sense: Queens, the Bronx  and, to a fair extent Brooklyn, are residential neighborhoods unlike Manhattan which is a business centre as well as a tourist destination.","6388eb33":"### 1.4.2 Neighbourhood group","1352a11f":"### 2.7.1 Baseline model (default parameters)","e64ffe7c":"### Feature engineering","7ca6b150":"I find the best value of the L2 penalty hyperparameter with a telescopic search based on cross-validation scores. I then train the Ridge model on the entire training set and test how it performs on the held-out test set.","7b66fd4c":"### 1.5.1 PairPlot","6d1c9cbf":"I'm going to split the data into a test set and a training set. I will hold out the test set until the very end and use the error on those data as an unbiased estimate of how my models did. \n\nI might perform a further split later on the training set into training set proper and a validation set or I might cross-validate.","2e1f1af9":"Something that people say one should try is rerunning algorithms on the feature subset selected by LASSO. \n\nOn this dataset this doesn't seem to bring improvement: in the fact the values of the loss go up slighly for all algorithms or they don't change.","d5e03d97":"The distribution of the number of reviews per month is highly skewed however way we cut it. This is because there is a large weight on small numbers: there are a lot of properties which only get a few reviews and a rather fat tail of properties which get a lot of reviews. \n\nOne explanation would be that the properties which are available a larger fraction of the year get more reviews. However, a scatter plot of reviews_per_month and availability_365 variables shows no evidence of a relationship so that explanation would appear to not be valid.","bcb7aada":"I will score models based on K-fold cross-validation with 5 folds.","bfb38a2b":"Longitude and latitude are somewhat correlated with each other. This is because the locations of properties tend to come from clusters.","c6ceebc3":"## 1.9 Rescale the design matrix","6dc11548":"I now scale the design matrix with sklearn's RobustScaler() so that each predictor has zero mean and unit variance. This helps the convergence of machine learning algorithms such as linear regression.\n\nI avoid data snooping by defining the scaleing transformation based on the training data not the test data.","9e0aada8":"## 1.3 Price distribution","fe9f3fcd":"### 2.3.2 Variables selected","8133fdcb":"The training error for LASSO is somewhat higher than for Ridge (~0.198 vs ~0.197). However, the difference is quite small so that LASSO is still useful - for one as, a variable selector!","559a2aed":"## 1.0 Missing data","3cdfa127":"1. Penalized regression: https:\/\/www.kaggle.com\/aaron7sun\/you-got-this-feature-engineering-and-lasso\n2. Hyperparameter tuning for random forests: https:\/\/towardsdatascience.com\/hyperparameter-tuning-the-random-forest-in-python-using-scikit-learn-28d2aa77dd74","584a3e2b":"### 2.3.0 Hyperparameter optimization","2255a02c":"### 1.4.7 Availability_365","afed5c87":"Since the number of unique hosts is close to the total number of examples, we're not going to use hostname in our regression analysis since it would cause the number of parameters in our model to baloon! \n\nIn real-life, when there is more data and perhaps some feature data on hosts, I expect past history of a host and of a property to be a strong predictor of price!","75582c8a":"# 1. Preprocessing and EDA","a378d0ad":"### 2.7.3 max_depth and min_child_weight optimization","d63ce7d5":"## 1.5 Bivariate correlations","65dbaabf":"### Model interpretability","856859cb":"## 1.4. Predictor distributions","f62f737e":"### 2.2.2 Interpretation of instance predictions with Lime","c15ff2f4":"### 2.2.1 Best Ridge Regression model","2399b8ba":" ### 0.0 Load modules","21b1b0af":"## 1.7 Save transformed dataframe for future use","a5f6c554":"## 2.6 Random Forest Regressor","e7e96b4d":"## 1.2 Choosing a prediction target [Smart Pricing Regressor]","d7eea935":"# 2. Models","5f47ca55":"1. Kevin Lemagnen's 2018 NYC PyData talk: [Open the Black Box: an Introduction to Model Interpretability with LIME and SHAP](https:\/\/www.youtube.com\/watch?v=C80SQe16Rao)\n2. Lime Regression example can be found in [this notebook](https:\/\/marcotcr.github.io\/lime\/tutorials\/Using%2Blime%2Bfor%2Bregression.html)\n3. [The original SHAP paper](https:\/\/arxiv.org\/abs\/1705.07874) is quite readable","8694450f":"### 1.4.1 Host_id","87e20197":"#### Interpretation","6f96bfde":"Let's take a look at the settings and the results.","d409a323":"## 2.0 Cross-validation routine","95fe46a4":"## 1.8 Train-test split","fce7c4d2":"## 2.3 LASSO Regression","0e633016":"There don't appear to exist obvious, strong correlations between these variables. \n\nHowever, the number of reviews per month is fairly (40%) correlated with the total number of reviews and the the total number of reviews is correlated (at 30%) with the availability of the property. Both of these correlations make sense.\n\nIt's also interesting that the longitude is anticorrelated (at 20%) with the price. That also makes sense - property in the Bronx and in Queens is cheaper than Manhattan and Brooklyn.","8d607fca":"#### Ridge Regression Hyperparameter Optimization","1c63b0d8":"# Acknowledgements","8a48c5c4":"### 2.2.0 Hyperparameter optimization","041ea5bd":"### 1.5.0 Pearson correlation matrix","2b6bbf32":"As far as room types, this dataset is balanced away from 'Shared room' properties. The proportions of private room and entire home\/apt rentals are close, with entire home\/apt dominating prive room by <10%.","137af028":"# SUMMARY","0d8b51de":"### 0.1 Load data","52132496":"## 2.7 XGBoost Regressor","80189720":"## 2.4 Huber regression","276e12e5":"#### Hyperparameter tuning","04ac4183":"## 2.1 Scoring basic models (no parameter tuning)","864bf57f":"The NaN values in the last_review and reviews_per_month columns all occur for examples where no reviews were given in the first place. \n\nFor reviews_per_month, I will fill those values with 0's.","88287d56":"### 1.4.4 Room type","c6a9a61d":"\n### 2.7.2 Learning rate optimization","2b63671e":"### 1.4.3 Longitude and latitude","d43b0d2b":"One of the machine learning models at AirBNB is Smart Pricing. After a client has entered the details of their rental, AirBNB suggests an appropriate price. The aim of this notebook is to build and train a Smart Pricing model for this dataset.","c6d36d48":"This distribution is highly skewed towards the low and high end. The dataset contains a hiuge number of properties that are available only for a couple of days each year, and a decent number that are available for > 300 days. ","2bf85dd4":"I notice that the target has a highly skewed distribution. This can cause problems for machine learning algorithms such as linear regression. A log transformation and removal of outliers makes the distribution look much closer to normal.","7a0e53e7":"### 1.4.6 Reviews per month","01d9208c":"### 2.7.4 Best model","4415c82b":"We see that the training error is nearly an order of magnitude smaller than the test error, and the training error for any of the other algorithms. The baseline model is overfitting massively! Let's look at the baseline model's parameter values.","a07d60cd":"### 1.4.0 A list of predictors","2d96a8e6":"Many machine learning algorithms do badly when acting on inputs with missing data. To deal with this, we start by taking a count of missing values in each column.","be225cbb":"## 2.2 Ridge Regression","74473f12":"### 2.2.3 Summary of local interpretation on test set","3e109ee7":"## 1.6 Encoding categorical features","d4ef29ab":"## 2.5 Ridge with variables selected by LASSO"}}