{"cell_type":{"81e9a3d9":"code","ff68b5d8":"code","e44f36eb":"code","27c126c0":"code","3bf943ac":"code","4d2942da":"code","3ea2d04a":"code","2f457ca8":"code","b8ae4077":"code","dde8817c":"code","70ab7ae9":"code","7e2665bf":"code","7a10fe75":"code","a4f019f1":"code","8eccd6a5":"code","1ed0084a":"code","487b9e28":"code","78068b94":"code","6c1f4328":"code","f7e754f5":"code","92f87541":"code","eeae07d3":"code","455a17a6":"code","a8f7a82f":"code","58fbe515":"code","09d72d51":"code","c5637295":"code","6f38808e":"code","cc2f03dc":"code","1adf6067":"code","4b6b8ed0":"code","6245a2cd":"code","919b1705":"markdown","d975539f":"markdown","29f9d3e2":"markdown","3d654980":"markdown"},"source":{"81e9a3d9":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport re\nimport string\nimport nltk\nimport warnings\n%matplotlib inline\nwarnings.filterwarnings('ignore')","ff68b5d8":"df = pd.read_csv('..\/input\/twitter-sentiments-analysis-nlp\/Twitter Sentiments.csv')","e44f36eb":"df.head()","27c126c0":"df['tweet']","3bf943ac":"df['label'].value_counts()","4d2942da":"df.info()","3ea2d04a":"# removes pattern in the umput text\ndef remove_pattern(input_txt, pattern):\n    r = re.findall(pattern, input_txt)\n    for word in r:\n        input_txt = re.sub(word, \"\",input_txt)\n    return input_txt","2f457ca8":"#remove twitter handels (@user)\ndf['clean_tweet'] = np.vectorize(remove_pattern)(df['tweet'],\"@[\\w]*\")\ndf.head()","b8ae4077":"# remove special characters , number and punctuations\ndf['clean_tweet'] = df['clean_tweet'].str.replace(\"[^a-zA-Z#]\", \" \")\ndf.head()","dde8817c":"#remove short words\ndf['clean_tweet'] = df['clean_tweet'].apply(lambda x: \" \".join([w for w in x.split() if len(w)>3]))\ndf.head()","70ab7ae9":"#individual words considered as tokens\ntokenized_tweet = df['clean_tweet'].apply(lambda x : x.split())\ntokenized_tweet.head()","7e2665bf":"#stem the words\nfrom nltk.stem.porter import PorterStemmer\nstemmer = PorterStemmer()\n\ntokenized_tweet = tokenized_tweet.apply(lambda sentence:[stemmer.stem(word) for word in sentence])\ntokenized_tweet.head()","7a10fe75":"# combine words into single sentence\nfor i in range(len(tokenized_tweet)):\n    tokenized_tweet[i] = \" \".join(tokenized_tweet[i])\ndf['clean_tweet'] = tokenized_tweet\ndf.head()","a4f019f1":"# # frequent words visualization for -ve\n# all_words = \" \".join([sentence for sentence in df['clean_tweet'][df['label']==1]])\n\n# wordcloud = WordCloud(width=800, height=500, random_state=42, max_font_size=100).generate(all_words)\n\n# # plot the graph\n# plt.figure(figsize=(15,8))\n# plt.imshow(wordcloud, interpolation='bilinear')\n# plt.axis('off')\n# plt.show()","8eccd6a5":"# extract the hashtag\ndef hashtag_extract(tweets):\n    hashtags = []\n    # loop words in the tweet\n    for tweet in tweets:\n        ht = re.findall(r\"#(\\w+)\", tweet)\n        hashtags.append(ht)\n    return hashtags    ","1ed0084a":"# extract hashtags from non-racist\/sexist tweets\nht_positive = hashtag_extract(df['clean_tweet'][df['label']==0])\n\n# extract hashtags from racist\/sexist tweets\nht_negative = hashtag_extract(df['clean_tweet'][df['label']==1])","487b9e28":"ht_positive[:5]","78068b94":"# unnest list\nht_positive = sum(ht_positive, [])\nht_negative = sum(ht_negative, [])","6c1f4328":"ht_positive[:5]","f7e754f5":"freq = nltk.FreqDist(ht_positive)\nd = pd.DataFrame({'Hashtag': list(freq.keys()),\n                 'Count': list(freq.values())})\nd.head()","92f87541":"# select top 10 hashtags\nd = d.nlargest(columns='Count', n=10)\nplt.figure(figsize=(15,9))\nsns.barplot(data=d, x='Hashtag', y='Count')\nplt.show()","eeae07d3":"freq = nltk.FreqDist(ht_negative)\nd = pd.DataFrame({'Hashtag': list(freq.keys()),\n                 'Count': list(freq.values())})\nd.head()","455a17a6":"# select top 10 hashtags\nd = d.nlargest(columns='Count', n=10)\nplt.figure(figsize=(15,9))\nsns.barplot(data=d, x='Hashtag', y='Count')\nplt.show()","a8f7a82f":"# feature extraction\nfrom sklearn.feature_extraction.text import CountVectorizer\nbow_vectorizer = CountVectorizer(max_df=0.90, min_df=2, max_features=1000, stop_words='english')\nbow = bow_vectorizer.fit_transform(df['clean_tweet'])","58fbe515":"from sklearn.model_selection import train_test_split\nx_train, x_test, y_train, y_test = train_test_split(bow, df['label'], random_state=42, test_size=0.25)","09d72d51":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import f1_score, accuracy_score","c5637295":"# training\nmodel = LogisticRegression()\nmodel.fit(x_train, y_train)","6f38808e":"# testing\npred = model.predict(x_test)\nf1_score(y_test, pred)","cc2f03dc":"accuracy_score(y_test,pred)","1adf6067":"# use probability to get output\npred_prob = model.predict_proba(x_test)\npred = pred_prob[:, 1] >= 0.3\npred = pred.astype(np.int)\n\nf1_score(y_test, pred)","4b6b8ed0":"accuracy_score(y_test,pred)","6245a2cd":"pred_prob[0][1] >= 0.3","919b1705":"Model Training","d975539f":"#Exploratory Data Analysis","29f9d3e2":"Input Split","3d654980":"#preprocessing the dataset"}}