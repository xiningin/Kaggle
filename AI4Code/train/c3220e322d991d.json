{"cell_type":{"221e7ede":"code","070a789f":"code","b99546e9":"code","d5d6efbc":"code","d1be808e":"code","c543784a":"code","215fb5b3":"code","47974c14":"code","afaaaf5c":"code","e531ebe2":"code","70508b31":"code","bcfcac7c":"code","291fb50c":"code","92451baf":"code","da307f90":"code","5284b008":"code","d583e097":"code","63cb6ff3":"markdown","79992973":"markdown","50498dfd":"markdown","552add06":"markdown","58a6217a":"markdown","d612fbac":"markdown","d4a60602":"markdown","4411760c":"markdown","e4fbac06":"markdown","8a9db5b1":"markdown","77634db8":"markdown","c0de6957":"markdown","ea33d309":"markdown","f4afa7f7":"markdown"},"source":{"221e7ede":"import statistics\n\nimport pandas as pd\nimport numpy as np\nfrom sklearn import model_selection\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.preprocessing import OneHotEncoder, StandardScaler\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.metrics import roc_auc_score\nimport warnings\nfrom sklearn.ensemble import RandomForestClassifier\n\nwarnings.filterwarnings('ignore')","070a789f":"class Config:\n    RANDOM_SEED = 42\n    NUM_FOLDS = 10\n    TARGET_COL_NAME = \"song_popularity\"\n    CATEGORICAL_COLS = [\"audio_mode\", \"time_signature\", \"key\"]\n\nDATA_PATH = \"\/kaggle\/input\/song-popularity-prediction\/\"","b99546e9":"df_train = pd.read_csv(DATA_PATH + \"train.csv\")\ndf_test = pd.read_csv(DATA_PATH + \"test.csv\")","d5d6efbc":"# split the training dataframe into kfolds for cross validation. We do this before any processing is done\n# on the data. We use stratified kfold if the target distribution is unbalanced\ndef strat_kfold_dataframe(df, target_col_name, num_folds=5):\n    # we create a new column called kfold and fill it with -1\n    df[\"kfold\"] = -1\n    # randomize of shuffle the rows of dataframe before splitting is done\n    df = df.sample(frac=1, random_state=Config.RANDOM_SEED).reset_index(drop=True)\n    # get the target data\n    y = df[target_col_name].values\n    skf = model_selection.StratifiedKFold(n_splits=num_folds, shuffle=True, random_state=42)\n    for fold, (train_index, val_index) in enumerate(skf.split(X=df, y=y)):\n        df.loc[val_index, \"kfold\"] = fold    \n    return df     \n\ndf_train = strat_kfold_dataframe(df_train, target_col_name=Config.TARGET_COL_NAME, num_folds=Config.NUM_FOLDS)\ndf_train.head()","d1be808e":"cont_cols = ['song_duration_ms', 'acousticness', 'danceability', 'energy', \n            'instrumentalness', 'liveness', 'loudness', 'speechiness', 'tempo', 'audio_valence']","c543784a":"def add_missing_col(df, cols_with_nulls):\n    for col_name in cols_with_nulls:        \n        df[col_name + \"_missing\"] = [int(item) for item in df[col_name].isna().values]\n    return df        \n\ntrain_cols_withnulls = [col for col in df_train.columns if df_train[col].isnull().any()]\ntest_cols_withnulls = [col for col in df_test.columns if df_test[col].isnull().any()]\ndf_train = add_missing_col(df_train, train_cols_withnulls)\ndf_test = add_missing_col(df_test, test_cols_withnulls)","215fb5b3":"def impute_df_col(df, col_name, imputer):\n    imputed_col = imputer.fit_transform(df[col_name].to_numpy().reshape(-1, 1))\n    return pd.Series(imputed_col.reshape(-1))    ","47974c14":"def impute_missing_values(df, cols, col_type=\"cont\"):    \n    if col_type == \"cont\":\n        imputer = SimpleImputer(missing_values=np.nan, strategy=\"mean\")\n    elif col_type == \"cat\":\n        imputer = SimpleImputer(missing_values=np.nan, strategy=\"most_frequent\")            \n    for col in cols:\n        df[col] = impute_df_col(df, col, imputer)\n    return df\n\ndf_train = impute_missing_values(df_train, Config.CATEGORICAL_COLS, col_type=\"cat\")\ndf_train = impute_missing_values(df_train, cont_cols, col_type=\"cont\")\ndf_test = impute_missing_values(df_test, Config.CATEGORICAL_COLS, col_type=\"cat\")\ndf_test = impute_missing_values(df_test, cont_cols, col_type=\"cont\")","afaaaf5c":"def col_one_hot_encode(df, cols):    \n    one_hot_enc = OneHotEncoder(handle_unknown=\"ignore\", sparse=False)\n    one_hot_enc.fit(df[cols])\n    return one_hot_enc.transform(df[cols])","e531ebe2":"def get_input_features(df):\n    non_cont_cols = Config.CATEGORICAL_COLS + [\"id\", \"kfold\", \"song_popularity_proba\", Config.TARGET_COL_NAME]\n    cont_col_names = [item for item in df.columns.values.tolist() if item not in non_cont_cols]     \n    X_cont = df[cont_col_names].to_numpy()       \n    X_cat_one_hot = col_one_hot_encode(df, Config.CATEGORICAL_COLS)    \n    X = np.concatenate((X_cont, X_cat_one_hot), axis=1)    \n    return X\n\ndef get_fold_data(fold, df):\n    df_train = df[df.kfold != fold]\n    df_val = df[df.kfold == fold]\n    X_train = get_input_features(df_train)\n    y_train = df_train[Config.TARGET_COL_NAME].to_numpy()\n    X_val = get_input_features(df_val)\n    y_val = df_val[Config.TARGET_COL_NAME].to_numpy()\n    return X_train, y_train, X_val, y_val","70508b31":"def run_training(model, train_X, train_y, val_X, val_y):        \n    scaler = StandardScaler()\n    train_X_scaled = scaler.fit_transform(train_X)    \n    val_X_scaled = scaler.fit_transform(val_X)    \n    model.fit(train_X_scaled, train_y.ravel())\n    val_y_pred_proba = model.predict_proba(val_X_scaled)\n    auc = roc_auc_score(val_y, val_y_pred_proba[:, 1], average=\"weighted\")\n    return auc, model, val_y_pred_proba[:, 1]","bcfcac7c":"# import optuna\n\n# def rf_objective(trial):       \n#     params = {        \n#         \"n_estimators\": trial.suggest_int(\"n_estimators\", 100, 1000),\n#         \"max_depth\": trial.suggest_int(\"max_depth\", 4, 20),\n#         \"min_samples_leaf\": trial.suggest_categorical(\"min_samples_leaf\", [1, 2, 4]),\n#         \"min_samples_split\": trial.suggest_categorical(\"min_samples_split\", [2, 4, 8]),\n#         \"max_features\": trial.suggest_categorical(\"max_features\", [\"auto\", \"log2\"])\n#     }\n#     rf_model = RandomForestClassifier(\n#                 n_estimators=params[\"n_estimators\"],                 \n#                 max_depth=params[\"max_depth\"],\n#                 min_samples_leaf=params[\"min_samples_leaf\"],\n#                 min_samples_split=params[\"min_samples_split\"],\n#                 max_features=params[\"max_features\"],\n#                 random_state=Config.RANDOM_SEED,\n#                 n_jobs=-1\n#             )   \n\n#     fold_auc = []\n#     for fold in range(Config.NUM_FOLDS):\n#         train_X, train_y, val_X, val_y = get_fold_data(fold, df_train)\n#         auc_score, _, _ = run_training(rf_model, train_X, train_y, val_X, val_y)\n#         fold_auc.append(auc_score)\n#     mean_auc = statistics.mean(fold_auc)                \n#     return mean_auc\n\n# study = optuna.create_study(direction=\"maximize\", study_name=\"RFModelTuning\")    \n# study.optimize(rf_objective, n_trials=20)\n# print(\"Best trial:\")\n# print(study.best_params)","291fb50c":"rf_model_params = {'n_estimators': 703, 'max_depth': 10, 'min_samples_leaf': 4, 'min_samples_split': 4, 'max_features': 'log2'}\n#rf_model_params = study.best_params\nrf_model = RandomForestClassifier(\n                n_estimators=rf_model_params[\"n_estimators\"],                 \n                max_depth=rf_model_params[\"max_depth\"],\n                min_samples_leaf=rf_model_params[\"min_samples_leaf\"],\n                min_samples_split=rf_model_params[\"min_samples_split\"],\n                max_features=rf_model_params[\"max_features\"],\n                random_state=Config.RANDOM_SEED,\n                n_jobs=-1\n            )     ","92451baf":"fold_metrics_model = []\ntest_preds = {}\ndf_train[\"song_popularity_proba\"] = 0.0\nfor fold in range(Config.NUM_FOLDS):\n    X_train, y_train, X_val, y_val = get_fold_data(fold, df_train)\n    fold_auc_score, model, fold_val_preds = run_training(rf_model, X_train, y_train, X_val, y_val)\n    print(f\"fold {fold } auc score = {fold_auc_score}\")\n    # add the validation probability predictions for the fold to a new column in train data\n    df_train.loc[df_train.kfold == fold, \"song_popularity_proba\"] = fold_val_preds    \n    X_test = get_input_features(df_test)\n    scaler = StandardScaler()\n    X_test_scaled = scaler.fit_transform(X_test)\n    fold_test_preds = model.predict_proba(X_test_scaled)[:, 1]\n    pred_col_name = f\"fold_{fold}_test_preds\"\n    test_preds[pred_col_name] = fold_test_preds    \n    fold_metrics_model.append((round(fold_auc_score, 4), model))","da307f90":"fold_metrics = [item[0] for item in fold_metrics_model]\nprint(f\"auc scores = {fold_metrics}\")    \ncv_auc_mean = statistics.mean(fold_metrics)\ncv_auc_stdev = statistics.stdev(fold_metrics)\nprint(f\"mean auc across folds = {cv_auc_mean}, auc stdev across folds = {cv_auc_stdev}\")","5284b008":"df_test_preds = pd.DataFrame(test_preds)\ntest_pred_cols = [f\"fold_{fold}_test_preds\" for fold in range(Config.NUM_FOLDS)]\ndf_test_preds[\"mean_test_pred\"] = df_test_preds[test_pred_cols].mean(axis=1)\nprint(f\"Completed prediction for {len(df_test)} test rows\")\ndf_submission = pd.read_csv(DATA_PATH + 'sample_submission.csv')\ndf_submission['song_popularity']= df_test_preds[\"mean_test_pred\"]\ndf_submission.to_csv('submission_rf.csv',index=False)\ndf_submission.head()","d583e097":"rf_val_preds = df_train[[\"id\", \"song_popularity_proba\", \"song_popularity\"]]\nrf_val_preds.to_csv(\"rf_val_preds.csv\")\nprint(\"Saved validation predictions for all folds to csv\")","63cb6ff3":"### The cross validation statistics","79992973":"### The training loop","50498dfd":"### Split the train data into k folds","552add06":"### Get train and validation data for a fold","58a6217a":"### Save OOF preds to csv (in case you want to use these for model blending later)","d612fbac":"### Creater marker columns for missing values <br>\nHoping it provides more signal to the model","d4a60602":"### Create model using tuned params","4411760c":"### Configurations for model training","e4fbac06":"Best trial: {'n_estimators': 703, 'max_depth': 10, 'min_samples_leaf': 4, 'min_samples_split': 4, 'max_features': 'log2'}","8a9db5b1":"### Load the train and test data","77634db8":"### Predictions on the test set\n( Using \"average\" ensembling from the models returned by the training fold )","c0de6957":"### Hyperparameter tuning\nUncomment the cell below in case you want to run hyper parameter tuning","ea33d309":"### One hot encoding of categorical variables","f4afa7f7":"### Use \"mean\" imputing strategy for continuous and \"most frequent\" for categorical features"}}