{"cell_type":{"b1cecdf2":"code","4c7cdc46":"code","2f45646b":"code","acc701dd":"code","9c492e85":"code","30c4120a":"code","da96c0f6":"code","f62aa393":"code","5f91bbb8":"code","39ea43e3":"code","bd7a37d4":"code","93583188":"code","c358ce20":"code","ed47cb5b":"code","9aee01af":"code","9c8ab9b0":"code","6a6de804":"code","ce4c65eb":"code","33752d8b":"code","8d182337":"code","a5285a8b":"code","d97a7ce8":"code","39ebbc76":"code","e5be6164":"code","5bc6d60e":"code","1e483099":"code","0dc86cfa":"code","c6300ae5":"code","380337b7":"code","aedfada5":"code","99b1b09f":"code","73a147d1":"code","33747294":"code","62355f5f":"code","0b38b470":"code","cd81fce6":"code","c79e688f":"code","82d56cad":"code","b9505a1f":"code","160e3ada":"code","f5f4cd14":"code","9b3b3e0b":"code","0ecd20a4":"code","52783971":"code","9180ec57":"code","90c52acc":"code","c99ac8b2":"code","5402d8c3":"code","cd86e946":"code","b420fe06":"code","70e4cd91":"code","8e2e5616":"code","a4f72465":"code","052c8492":"code","e8be9002":"code","d7ee4c69":"code","b35925a6":"code","c130f968":"code","02e8af77":"code","3bd6ce64":"code","626a1263":"code","cd97db4a":"code","053c1e4b":"code","8b337ef9":"code","35fb95ce":"code","d8d8e4a3":"code","dd9a25fa":"code","ff97b7ac":"code","9b953d6a":"code","aa092dc0":"code","9865a374":"code","13fcabfe":"code","a3700721":"code","de441024":"code","385488a9":"code","b38cb691":"code","135843e5":"code","c4dec4f0":"code","123611e7":"code","413ef1c9":"code","701422fe":"code","8ff438a8":"code","d87a13f5":"code","df81c7e0":"code","9c3df6cf":"code","25826ba8":"code","d1317b3a":"code","6b3950dc":"code","8755633e":"code","00492d92":"code","3a3e2bd4":"code","fb8e07b8":"code","0e2c781f":"code","ec16a644":"code","43394b5f":"code","b57483ad":"code","d67b8244":"code","9d89fab9":"code","e3bffdf3":"code","456ce8ad":"code","c0251866":"code","d232d72d":"code","5d88bcc2":"code","42a4981e":"code","335989ed":"code","5710af56":"code","c378d85e":"code","8207af2b":"code","cf3c6f14":"code","0e3bcee0":"code","0c0285b7":"code","6f5f4d7b":"code","a9827947":"code","a1a598d1":"code","66ddcaba":"code","f78c39c0":"code","524a7360":"code","9ac40fab":"code","5783dd91":"code","38b076a4":"code","e4cefcad":"code","08ba08a3":"code","50019e3b":"code","868b6eb5":"code","cc7773a4":"code","d54e753b":"code","bc25ea24":"code","7b8660bf":"code","49d507d8":"code","d519b600":"code","4a54f525":"code","96a1f405":"code","4ea53b99":"code","34911781":"code","7e055644":"code","58a7e4b6":"code","0ae0c256":"code","c4388028":"code","1972b995":"code","202b75fb":"code","edf9a7e9":"code","9ea176d7":"code","0e609f1e":"code","ffb25f59":"code","47929cc1":"code","4f900be0":"code","d8266b91":"code","5817a247":"code","1ecc8821":"code","db65bc14":"code","29bc67d9":"code","45b4e1f6":"code","d379deb7":"code","d2504288":"code","3f6941db":"code","b0b746a8":"code","8ee9a86d":"code","8d747b1d":"code","e57fd596":"code","e45f143e":"code","389517fc":"code","7ad9bb43":"code","e8c3ed67":"code","f28fc36c":"code","5dbba43a":"code","ebf457ab":"code","bcb9cc2c":"code","808bf328":"code","dac1ba7a":"code","632f6108":"code","6f4d534c":"code","024ce8de":"code","1726a4a0":"code","f81e3460":"code","40123800":"code","7ea4ca14":"code","e5ac3bb3":"code","aa60a971":"code","14148e2b":"code","70cb8cfc":"code","36e5da9a":"code","e89af614":"code","fad2a6bb":"code","a0fe7136":"code","a20ff9f3":"code","aeaf52e4":"code","cfd08fdd":"code","91108226":"code","e1874397":"code","d86155c6":"code","d5ae633a":"code","35d03fac":"code","6e398569":"code","91acf77a":"code","3534c233":"code","6d200a37":"code","d6d35609":"code","b469572f":"code","2aed0d25":"code","f7b41d25":"code","c2faaf4d":"code","901f4b0a":"code","53cfd8c9":"code","ffa99fc8":"code","ae3c1e55":"code","db1ca64a":"code","1edb182b":"code","484365e4":"code","e2011914":"code","0eb6093c":"code","507a8d5f":"code","ba404181":"code","177a0352":"code","e3e4e888":"code","c9d81926":"code","3cc200ad":"code","925a576d":"code","86255499":"code","9d7e3e02":"code","ab991057":"code","ed17c1af":"code","88889b85":"code","d60c6bbf":"code","c72cbb83":"markdown","724c37ff":"markdown","cdf6a432":"markdown","e80186c5":"markdown","3f60e7a8":"markdown","2654425d":"markdown","59c8c19e":"markdown","cea2f795":"markdown","5814421b":"markdown","74641648":"markdown","ff8c7848":"markdown","7ac5a8cf":"markdown","fe4df977":"markdown","17e22e60":"markdown","7c8be00d":"markdown","a0114de5":"markdown","c5d0f37f":"markdown","86ae1b72":"markdown","5891b025":"markdown","9b8fc2b3":"markdown","853588fa":"markdown","eb9c0166":"markdown","2f1ee03f":"markdown","ca75029e":"markdown","50098084":"markdown","cb523e71":"markdown","c66ce9e5":"markdown","be3cf985":"markdown","bbd8032f":"markdown","f787fb9d":"markdown","b714d8d2":"markdown","465f22de":"markdown","e707abdf":"markdown","43b9ed68":"markdown","8a34eb29":"markdown","b3281266":"markdown","38f38afb":"markdown","5636a60a":"markdown","6420a06b":"markdown","4d91a784":"markdown","fec4ade4":"markdown","899fcde7":"markdown","b6532efd":"markdown","44418e3c":"markdown","a3015ab6":"markdown","6830b74f":"markdown","a03951f2":"markdown","bc852773":"markdown","1de7e037":"markdown","3cd957ed":"markdown","0b754aad":"markdown","e14a0504":"markdown","71ba4e22":"markdown","f45e5802":"markdown","45a38430":"markdown","9c1581d1":"markdown","cfe2d965":"markdown","89979391":"markdown","2b5cd8fb":"markdown","f2d00cba":"markdown","04752d3e":"markdown","be1c71e4":"markdown","0b915d2d":"markdown","094532a4":"markdown","1b9fc1fe":"markdown","03385ab7":"markdown","c5d01731":"markdown","d54621c3":"markdown","798c515c":"markdown"},"source":{"b1cecdf2":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nimport warnings\nwarnings.filterwarnings('ignore')","4c7cdc46":"leads = pd.read_csv('\/kaggle\/input\/lead-scoring-x-online-education\/Leads X Education.csv')","2f45646b":"leads.head()","acc701dd":"leads.columns","9c492e85":"leads.info()","30c4120a":"leads.shape","da96c0f6":"# target variable\nleads['Converted'].value_counts()","f62aa393":"leads.describe()","5f91bbb8":"# checking for the number of null values in each column \n\nleads.isnull().sum(axis = 0)","39ea43e3":"# checking for the percentage of null values in each column \n\nround((leads.isnull().sum(axis = 0)\/ len(leads.index))*100 , 2)","bd7a37d4":"leads.columns","93583188":"# Dropping the columns 'Asymmetrique Activity Index' and 'Asymmetrique Profile Index' as there is score column for both\n\nleads = leads.drop(['Asymmetrique Activity Index','Asymmetrique Profile Index'], axis = 1)","c358ce20":"leads['Asymmetrique Activity Score'].value_counts()","ed47cb5b":"sum(leads['Asymmetrique Activity Score'].isnull())","9aee01af":"leads['Asymmetrique Activity Score'] = leads['Asymmetrique Activity Score'].fillna('Unknown')\nprint(leads['Asymmetrique Activity Score'].value_counts())\nprint('\\n')\nprint('Number of null values = ',sum(leads['Asymmetrique Activity Score'].isnull()))","9c8ab9b0":"leads['Asymmetrique Profile Score'] = leads['Asymmetrique Profile Score'].fillna('Unknown')\nleads['Asymmetrique Profile Score'].value_counts()","6a6de804":"leads['Lead Quality'].value_counts()","ce4c65eb":"sum(leads['Lead Quality'].isnull())","33752d8b":"leads['Lead Quality'].fillna(\"Unknown\", inplace = True)\nleads['Lead Quality'].value_counts()","8d182337":"# Tags column\nleads['Tags'].value_counts()","a5285a8b":"sum(leads['Tags'].isnull())","d97a7ce8":"leads['Tags'] = leads['Tags'].fillna('Unknown')\nleads['Tags'].value_counts()","39ebbc76":"sum(leads['Tags'].isnull())","e5be6164":"# Country column \nsum(leads['Country']=='India')\/len(leads.index)","5bc6d60e":"leads['Country'] = leads['Country'].apply(lambda x: 'India' if x=='India' else 'Foreign Country')\nleads['Country'].value_counts()","1e483099":"# Total visits column\nleads['TotalVisits'].value_counts() ","0dc86cfa":"leads['TotalVisits'].median() #Since the above column has lot of outliers we will impute with the median value","c6300ae5":"leads['TotalVisits'].replace(np.NaN, leads['TotalVisits'].median(), inplace =True)","380337b7":"# Page Views Per Visit column null values are similarly imputed using the median values\n\nleads['Page Views Per Visit'].replace(np.NaN, leads['Page Views Per Visit'].median(), inplace =True)","aedfada5":"leads['Last Activity'].value_counts()","99b1b09f":"sum(leads['Last Activity'].isnull())","73a147d1":"leads['Last Activity'].fillna(\"Unknown\", inplace = True)\nleads['Last Activity'].value_counts()","33747294":"leads['Specialization'].value_counts()","62355f5f":"sum(leads['Specialization'].isnull())","0b38b470":"leads['Specialization'].replace('Select', 'Unknown', inplace =True)\nleads['Specialization'].value_counts()","cd81fce6":"leads['Specialization'].fillna(\"Unknown\", inplace = True)\nleads['Specialization'].value_counts()","c79e688f":"leads['How did you hear about X Education'].value_counts()","82d56cad":"leads = leads.drop('How did you hear about X Education', axis=1)","b9505a1f":"leads['What is your current occupation'].value_counts()","160e3ada":"sum(leads['What is your current occupation'].isnull())","f5f4cd14":"leads['What is your current occupation'].fillna(\"Unknown\", inplace = True)\nleads['What is your current occupation'].value_counts()","9b3b3e0b":"leads['What matters most to you in choosing a course'].value_counts()","0ecd20a4":"sum(leads['What matters most to you in choosing a course'].isnull())","52783971":"leads = leads.drop('What matters most to you in choosing a course', axis = 1)","9180ec57":"leads['Lead Profile'].value_counts()","90c52acc":"sum(leads['Lead Profile'].isnull())","c99ac8b2":"leads['Lead Profile'].replace('Select', 'Unknown', inplace =True)\nleads['Lead Profile'].value_counts()","5402d8c3":"leads['Lead Profile'].fillna(\"Unknown\", inplace = True)\nleads['Lead Profile'].value_counts()","cd86e946":"# City column\nleads['City'].value_counts()","b420fe06":"sum(leads['City'].isnull())","70e4cd91":"leads['City'].fillna(\"Unknown\", inplace = True) # Replacing null values with 'NotSpecified' \nleads['City'].value_counts()","8e2e5616":"leads['City'].replace('Select', 'Unknown', inplace =True)\nleads['City'].value_counts()","a4f72465":"# re-checking for the percentage of null values in each column \n\nround((leads.isnull().sum(axis = 0)\/ len(leads.index))*100 , 2)","052c8492":"leads.shape","e8be9002":"# removing all the rows with null values\n\nleads = leads.dropna()","d7ee4c69":"leads.shape","b35925a6":"# checking again for missing values in the dataframe \n\nround((leads.isnull().sum(axis = 0)\/ len(leads.index))*100 , 2)","c130f968":"leads.head()","02e8af77":"leads.columns","3bd6ce64":"for col in leads.columns:\n    print(col, ':', leads[col].nunique())\n    print('\\n')","626a1263":"# Prospect ID and Lead Number are the same thing so having both the columsn is redundant so we will drop the Prospect ID column\n\nleads = leads.drop('Prospect ID',axis=1)\n\n# Also a lot of the columns have just one unique value so they are of no use as they do not provide any information so dropping them as well\nleads = leads.drop(['Magazine','Receive More Updates About Our Courses',\n                    'Update me on Supply Chain Content','Get updates on DM Content',\n                    'I agree to pay the amount through cheque'], axis=1)","cd97db4a":"leads.head()","053c1e4b":"print(leads.shape)","8b337ef9":"leads.columns","35fb95ce":"def mapping(x):\n    return x.map({'Yes':1, 'No':0})","d8d8e4a3":"col_list = ['Search',\n            'Do Not Email',\n            'Do Not Call',\n            'Newspaper Article',\n            'X Education Forums',\n            'Newspaper',\n            'Digital Advertisement',\n            'Through Recommendations',\n            'A free copy of Mastering The Interview']","dd9a25fa":"leads[col_list] = leads[col_list].apply(mapping)","ff97b7ac":"leads.head()","9b953d6a":"leads.columns","aa092dc0":"leads.info()","9865a374":"# creating dummy variables for some of the other categorical columns \nleads = pd.get_dummies(leads, columns=['Lead Origin', 'Lead Source', 'Country', 'Last Notable Activity'], drop_first=True)","13fcabfe":"# Creating dummmy variables for the rest of the columns and dropping the level called 'Unknown'\n\n\n# Creating dummy variables for the variable 'City'\ndummy = pd.get_dummies(leads['Asymmetrique Activity Score'], prefix='Asymmetrique Activity Score')\nfinal_dummy = dummy.drop(['Asymmetrique Activity Score_Unknown'], 1)\nleads = pd.concat([leads,final_dummy], axis=1)\n\n# Creating dummy variables for the variable 'City'\ndummy = pd.get_dummies(leads['Asymmetrique Profile Score'], prefix='Asymmetrique Profile Score')\nfinal_dummy = dummy.drop(['Asymmetrique Profile Score_Unknown'], 1)\nleads = pd.concat([leads,final_dummy], axis=1)\n\n# Creating dummy variables for the variable 'Last Activity'\ndummy = pd.get_dummies(leads['Last Activity'], prefix='Last Activity')\nfinal_dummy = dummy.drop(['Last Activity_Unknown'], 1)\nleads = pd.concat([leads,final_dummy], axis=1)\n\n# Creating dummy variables for the variable 'What is your current occupation'\ndummy = pd.get_dummies(leads['What is your current occupation'], prefix='What is your current occupation')\nfinal_dummy = dummy.drop(['What is your current occupation_Unknown'], 1)\nleads = pd.concat([leads,final_dummy], axis=1)\n\n# Creating dummy variables for the variable 'Lead Profile'\ndummy = pd.get_dummies(leads['Lead Profile'], prefix='Lead Profile')\nfinal_dummy = dummy.drop(['Lead Profile_Unknown'], 1)\nleads = pd.concat([leads,final_dummy], axis=1)\n\n# Creating dummy variables for the variable 'Specialization'\ndummy = pd.get_dummies(leads['Specialization'], prefix='Specialization')\nfinal_dummy = dummy.drop(['Specialization_Unknown'], 1)\nleads = pd.concat([leads,final_dummy], axis=1)\n\n# Creating dummy variables for the variable 'City'\ndummy = pd.get_dummies(leads['City'], prefix='City')\nfinal_dummy = dummy.drop(['City_Unknown'], 1)\nleads = pd.concat([leads,final_dummy], axis=1)\n\n# Creating dummy variables for the variable 'City'\ndummy = pd.get_dummies(leads['Lead Quality'], prefix='Lead Quality')\nfinal_dummy = dummy.drop(['Lead Quality_Unknown'], 1)\nleads = pd.concat([leads,final_dummy], axis=1)\n\n# Creating dummy variables for the variable 'City'\ndummy = pd.get_dummies(leads['Tags'], prefix='Tags')\nfinal_dummy = dummy.drop(['Tags_Unknown'], 1)\nleads = pd.concat([leads,final_dummy], axis=1)","a3700721":"leads.shape","de441024":"leads = leads.drop(['Lead Quality','Asymmetrique Profile Score','Asymmetrique Activity Score','Last Activity', \n                    'What is your current occupation', 'Lead Profile','Specialization','City','Tags'],axis=1)","385488a9":"leads.shape","b38cb691":"leads.head()","135843e5":"leads.info()","c4dec4f0":"# checking for outliers in the continuous variables\n\nnumerical = leads[['TotalVisits','Total Time Spent on Website', 'Page Views Per Visit']]","123611e7":"numerical.describe()","413ef1c9":"plt.figure(figsize=(20,10))\n\nplt.subplot(2,2,1)\nsns.boxplot(numerical['TotalVisits'])\n\nplt.subplot(2,2,2)\nsns.boxplot(numerical['Total Time Spent on Website'])\n\nplt.subplot(2,2,3)\nsns.boxplot(numerical['Page Views Per Visit'])","701422fe":"# removing outliers using the IQR\n\nQ1 = leads['TotalVisits'].quantile(0.25)\nQ3 = leads['TotalVisits'].quantile(0.75)\nIQR = Q3 - Q1\nleads = leads.loc[(leads['TotalVisits'] >= Q1 - 1.5*IQR) & (leads['TotalVisits'] <= Q3 + 1.5*IQR)]\n\nQ1 = leads['Page Views Per Visit'].quantile(0.25)\nQ3 = leads['Page Views Per Visit'].quantile(0.75)\nIQR = Q3 - Q1\nleads=leads.loc[(leads['Page Views Per Visit'] >= Q1 - 1.5*IQR) & (leads['Page Views Per Visit'] <= Q3 + 1.5*IQR)]","8ff438a8":"plt.figure(figsize=(20,10))\n\nplt.subplot(2,2,1)\nsns.boxplot(leads['TotalVisits'])\n\nplt.subplot(2,2,2)\nsns.boxplot(leads['Total Time Spent on Website'])\n\nplt.subplot(2,2,3)\nsns.boxplot(leads['Page Views Per Visit'])","d87a13f5":"leads.shape","df81c7e0":"# Lets look at the head of the dataframe again\nleads.head()","9c3df6cf":"# Lets look at the info of the dataframe again\nleads.info()","25826ba8":"from sklearn.model_selection import train_test_split","d1317b3a":"X = leads.drop(['Lead Number', 'Converted'], axis = 1)\ny = leads['Converted']","6b3950dc":"X.head()","8755633e":"y.head()","00492d92":"# Splitting the data into train and test sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.7, test_size=0.3, random_state=100)","3a3e2bd4":"from sklearn.preprocessing import StandardScaler","fb8e07b8":"scaler = StandardScaler()","0e2c781f":"X_train[['TotalVisits','Total Time Spent on Website','Page Views Per Visit']] = scaler.fit_transform(X_train[['TotalVisits','Total Time Spent on Website','Page Views Per Visit']])","ec16a644":"X_train.head()","43394b5f":"y.head()","b57483ad":"round((y.sum()\/len(y))*100,2) ","d67b8244":"import statsmodels.api as sm","9d89fab9":"# logistic regression model\n\nlogm1 = sm.GLM(y_train,(sm.add_constant(X_train)), family=sm.families.Binomial())\nlogm1.fit().summary()","e3bffdf3":"from sklearn.linear_model import LogisticRegression","456ce8ad":"logreg = LogisticRegression()","c0251866":"from sklearn.feature_selection import RFE\nrfe = RFE(logreg, 20) # running RFE with 20 variables\nrfe = rfe.fit(X_train,y_train)","d232d72d":"rfe.support_","5d88bcc2":"list(zip(X_train.columns, rfe.support_, rfe.ranking_))","42a4981e":"col = X_train.columns[rfe.support_]","335989ed":"X_train.columns[~rfe.support_] # rfe.support_ = false ","5710af56":"X_train_sm = sm.add_constant(X_train[col])\nlogm2 = sm.GLM(y_train,X_train_sm, family = sm.families.Binomial())\nres = logm2.fit()\nres.summary()","c378d85e":"# Getting the predicted values on the train set\ny_train_pred = res.predict(X_train_sm)\ny_train_pred[:10]","8207af2b":"y_train_pred = y_train_pred.values.reshape(-1)\ny_train_pred[:10]","cf3c6f14":"y_train_pred_final = pd.DataFrame({'Converted':y_train.values, 'Conversion_Prob':y_train_pred})\ny_train_pred_final['LeadID'] = y_train.index\ny_train_pred_final.head()","0e3bcee0":"y_train_pred_final['Predicted'] = y_train_pred_final.Conversion_Prob.map(lambda x: 1 if x > 0.5 else 0)\ny_train_pred_final.head()","0c0285b7":"from sklearn import metrics","6f5f4d7b":"# Confusion matrix \nconfusion = metrics.confusion_matrix(y_train_pred_final.Converted, y_train_pred_final.Predicted)\nprint(confusion)","a9827947":"# Let's check the overall accuracy.\nprint(metrics.accuracy_score(y_train_pred_final.Converted, y_train_pred_final.Predicted))","a1a598d1":"from statsmodels.stats.outliers_influence import variance_inflation_factor","66ddcaba":"# Create a dataframe that will contain the names of all the feature variables and their respective VIFs\nvif = pd.DataFrame()\nvif['Features'] = X_train[col].columns\nvif['VIF'] = [variance_inflation_factor(X_train[col].values, i) for i in range(X_train[col].shape[1])]\nvif['VIF'] = round(vif['VIF'], 2)\nvif = vif.sort_values(by = \"VIF\", ascending = False)\nvif","f78c39c0":"col = col.drop('Tags_Diploma holder (Not Eligible)', 1)\ncol","524a7360":"# Let's re-run the model using the selected variables\nX_train_sm = sm.add_constant(X_train[col])\nlogm3 = sm.GLM(y_train,X_train_sm, family = sm.families.Binomial())\nres = logm3.fit()\nres.summary()","9ac40fab":"# Getting the predicted values on the train set\ny_train_pred = res.predict(X_train_sm)\ny_train_pred[:10]","5783dd91":"y_train_pred = y_train_pred.values.reshape(-1)\ny_train_pred[:10]","38b076a4":"y_train_pred_final = pd.DataFrame({'Converted':y_train.values, 'Conversion_Prob':y_train_pred})\ny_train_pred_final['LeadID'] = y_train.index\ny_train_pred_final.head()","e4cefcad":"y_train_pred_final['Predicted'] = y_train_pred_final.Conversion_Prob.map(lambda x: 1 if x > 0.5 else 0)\ny_train_pred_final.head()","08ba08a3":"# Confusion matrix \nconfusion = metrics.confusion_matrix(y_train_pred_final.Converted, y_train_pred_final.Predicted)\nprint(confusion)","50019e3b":"# Let's check the overall accuracy.\nprint(metrics.accuracy_score(y_train_pred_final.Converted, y_train_pred_final.Predicted))","868b6eb5":"#### Checking VIFs again\nvif = pd.DataFrame()\nvif['Features'] = X_train[col].columns\nvif['VIF'] = [variance_inflation_factor(X_train[col].values, i) for i in range(X_train[col].shape[1])]\nvif['VIF'] = round(vif['VIF'], 2)\nvif = vif.sort_values(by = \"VIF\", ascending = False)\nvif","cc7773a4":"col = col.drop('Tags_wrong number given', 1)\n\n# Let's re-run the model using the selected variables\nX_train_sm = sm.add_constant(X_train[col])\nlogm4 = sm.GLM(y_train,X_train_sm, family = sm.families.Binomial())\nres = logm4.fit()\nres.summary()","d54e753b":"y_train_pred = res.predict(X_train_sm)\ny_train_pred[:10]","bc25ea24":"y_train_pred = y_train_pred.values.reshape(-1)\ny_train_pred[:10]","7b8660bf":"y_train_pred_final = pd.DataFrame({'Converted':y_train.values, 'Conversion_Prob':y_train_pred})\ny_train_pred_final['LeadID'] = y_train.index\ny_train_pred_final.head()","49d507d8":"y_train_pred_final['Predicted'] = y_train_pred_final.Conversion_Prob.map(lambda x: 1 if x > 0.5 else 0)\ny_train_pred_final.head()","d519b600":"# Confusion matrix \nconfusion = metrics.confusion_matrix(y_train_pred_final.Converted, y_train_pred_final.Predicted)\nprint(confusion)","4a54f525":"# Let's check the overall accuracy.\nprint(metrics.accuracy_score(y_train_pred_final.Converted, y_train_pred_final.Predicted))","96a1f405":"#### Checking VIFs again\nvif = pd.DataFrame()\nvif['Features'] = X_train[col].columns\nvif['VIF'] = [variance_inflation_factor(X_train[col].values, i) for i in range(X_train[col].shape[1])]\nvif['VIF'] = round(vif['VIF'], 2)\nvif = vif.sort_values(by = \"VIF\", ascending = False)\nvif","4ea53b99":"col = col.drop('Tags_number not provided', 1)\n\n# Let's re-run the model using the selected variables\nX_train_sm = sm.add_constant(X_train[col])\nlogm5 = sm.GLM(y_train,X_train_sm, family = sm.families.Binomial())\nres = logm5.fit()\nres.summary()","34911781":"y_train_pred = res.predict(X_train_sm)\ny_train_pred[:10]","7e055644":"y_train_pred = y_train_pred.values.reshape(-1)\ny_train_pred[:10]","58a7e4b6":"y_train_pred_final = pd.DataFrame({'Converted':y_train.values, 'Conversion_Prob':y_train_pred})\ny_train_pred_final['LeadID'] = y_train.index\ny_train_pred_final.head()","0ae0c256":"y_train_pred_final['Predicted'] = y_train_pred_final.Conversion_Prob.map(lambda x: 1 if x > 0.5 else 0)\ny_train_pred_final.head()","c4388028":"# Confusion matrix \nconfusion = metrics.confusion_matrix(y_train_pred_final.Converted, y_train_pred_final.Predicted)\nprint(confusion)","1972b995":"# Let's check the overall accuracy.\nprint(metrics.accuracy_score(y_train_pred_final.Converted, y_train_pred_final.Predicted))","202b75fb":"#### Checking VIFs again\nvif = pd.DataFrame()\nvif['Features'] = X_train[col].columns\nvif['VIF'] = [variance_inflation_factor(X_train[col].values, i) for i in range(X_train[col].shape[1])]\nvif['VIF'] = round(vif['VIF'], 2)\nvif = vif.sort_values(by = \"VIF\", ascending = False)\nvif","edf9a7e9":"# correlation matrix \nplt.figure(figsize = (20,10),dpi=200)  \nsns.heatmap(X_train[col].corr(),annot = True)\nplt.show()\n\nplt.savefig('corr.png')","9ea176d7":"TP = confusion[1,1] # true positive \nTN = confusion[0,0] # true negatives\nFP = confusion[0,1] # false positives\nFN = confusion[1,0] # false negatives","0e609f1e":"# Let's see the sensitivity of our logistic regression model\nTP \/ float(TP+FN)","ffb25f59":"# Let us calculate specificity\nTN \/ float(TN+FP)","47929cc1":"# Calculate false postive rate - predicting churn when customer does not have churned\nprint(FP\/ float(TN+FP))","4f900be0":"# positive predictive value \nprint (TP \/ float(TP+FP))","d8266b91":"# Negative predictive value\nprint (TN \/ float(TN+ FN))","5817a247":"def draw_roc( actual, probs ):\n    fpr, tpr, thresholds = metrics.roc_curve( actual, probs,\n                                              drop_intermediate = False )\n    auc_score = metrics.roc_auc_score( actual, probs )\n    plt.figure(figsize=(5, 5))\n    plt.plot( fpr, tpr, label='ROC curve (area = %0.2f)' % auc_score )\n    plt.plot([0, 1], [0, 1], 'k--')\n    plt.xlim([0.0, 1.0])\n    plt.ylim([0.0, 1.05])\n    plt.xlabel('False Positive Rate or [1 - True Negative Rate]')\n    plt.ylabel('True Positive Rate')\n    plt.title('Receiver operating characteristic (RoC) curve')\n    plt.legend(loc=\"lower right\")\n    plt.show()\n\n    return None","1ecc8821":"fpr, tpr, thresholds = metrics.roc_curve( y_train_pred_final.Converted, y_train_pred_final.Conversion_Prob, \n                                         drop_intermediate = False )","db65bc14":"draw_roc(y_train_pred_final.Converted, y_train_pred_final.Conversion_Prob)","29bc67d9":"# Let's create columns with different probability cutoffs \nnumbers = [float(x)\/10 for x in range(10)]\nfor i in numbers:\n    y_train_pred_final[i]= y_train_pred_final.Conversion_Prob.map(lambda x: 1 if x > i else 0)\ny_train_pred_final.head(10)","45b4e1f6":"# Now let's calculate accuracy sensitivity and specificity for various probability cutoffs.\ncutoff_df = pd.DataFrame( columns = ['prob','accuracy','sensi','speci'])\nfrom sklearn.metrics import confusion_matrix\n\n# TP = confusion[1,1] # true positive \n# TN = confusion[0,0] # true negatives\n# FP = confusion[0,1] # false positives\n# FN = confusion[1,0] # false negatives\n\nnum = [0.0,0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9]\nfor i in num:\n    cm1 = metrics.confusion_matrix(y_train_pred_final.Converted, y_train_pred_final[i] )\n    total1=sum(sum(cm1))\n    accuracy = (cm1[0,0]+cm1[1,1])\/total1\n    \n    speci = cm1[0,0]\/(cm1[0,0]+cm1[0,1])\n    sensi = cm1[1,1]\/(cm1[1,0]+cm1[1,1])\n    cutoff_df.loc[i] =[ i ,accuracy,sensi,speci]\nprint(cutoff_df)","d379deb7":"# Let's plot accuracy sensitivity and specificity for various probabilities.\n\ncutoff_df.plot.line(x='prob', y=['accuracy','sensi','speci'])\nplt.vlines(x=0.34, ymax=1, ymin=0, colors=\"r\", linestyles=\"--\")\nplt.show()","d2504288":"y_train_pred_final['final_predicted'] = y_train_pred_final.Conversion_Prob.map( lambda x: 1 if x > 0.34 else 0)\ny_train_pred_final.head()","3f6941db":"# Let's check the overall accuracy.\nmetrics.accuracy_score(y_train_pred_final.Converted, y_train_pred_final.final_predicted)","b0b746a8":"# Confusion matrix\nconfusion2 = metrics.confusion_matrix(y_train_pred_final.Converted, y_train_pred_final.final_predicted)\nconfusion2","8ee9a86d":"TP = confusion2[1,1] # true positive \nTN = confusion2[0,0] # true negatives\nFP = confusion2[0,1] # false positives\nFN = confusion2[1,0] # false negatives","8d747b1d":"# Let's see the sensitivity of our logistic regression model\nTP \/ float(TP+FN)","e57fd596":"# Let us calculate specificity\nTN \/ float(TN+FP)","e45f143e":"# Calculate false postive rate - predicting churn when customer does not have churned\nprint(FP\/ float(TN+FP))","389517fc":"# Positive predictive value \nprint (TP \/ float(TP+FP))","7ad9bb43":"# Negative predictive value\nprint (TN \/ float(TN+ FN))","e8c3ed67":"confusion2[1,1]\/(confusion2[0,1]+confusion2[1,1])","f28fc36c":"confusion2[1,1]\/(confusion2[1,0]+confusion2[1,1])","5dbba43a":"from sklearn.metrics import precision_score, recall_score","ebf457ab":"precision_score(y_train_pred_final.Converted, y_train_pred_final.Predicted)","bcb9cc2c":"recall_score(y_train_pred_final.Converted, y_train_pred_final.Predicted)","808bf328":"from sklearn.metrics import precision_recall_curve","dac1ba7a":"p, r, thresholds = precision_recall_curve(y_train_pred_final.Converted, y_train_pred_final.Conversion_Prob)","632f6108":"plt.plot(thresholds, p[:-1], \"g-\")\nplt.plot(thresholds, r[:-1], \"r-\")\nplt.show()","6f4d534c":"from sklearn.metrics import classification_report","024ce8de":"print(classification_report(y_train_pred_final.Converted, y_train_pred_final.final_predicted))","1726a4a0":"X_test[['TotalVisits','Total Time Spent on Website','Page Views Per Visit']] = scaler.transform(X_test[['TotalVisits','Total Time Spent on Website','Page Views Per Visit']])\nX_test.head()","f81e3460":"X_test = X_test[col]\nX_test.head()","40123800":"# adding constant for statsmodel\nX_test_sm = sm.add_constant(X_test)","7ea4ca14":"# making prediction on the test set\ny_test_pred = res.predict(X_test_sm)","e5ac3bb3":"y_test_pred[:10]","aa60a971":"# Converting y_pred to a dataframe which is an array\ny_pred = pd.DataFrame(y_test_pred)","14148e2b":"y_pred.head()","70cb8cfc":"# Converting y_test to dataframe\ny_test_df = pd.DataFrame(y_test)","36e5da9a":"y_test_df.head()","e89af614":"# Putting LeadID to index\ny_test_df['LeadID'] = y_test_df.index\ny_test_df.head()","fad2a6bb":"# concatenating both the prediction and the orginal labels\ny_pred_final = pd.concat([y_test_df, y_pred],axis=1)","a0fe7136":"y_pred_final.head()","a20ff9f3":"# Renaming the column \ny_pred_final= y_pred_final.rename(columns={ 0 : 'Conversion_Prob'})","aeaf52e4":"# Rearranging the columns\ny_pred_final = y_pred_final[['LeadID','Converted','Conversion_Prob']]","cfd08fdd":"y_pred_final.head()","91108226":"y_pred_final['Predicted'] = y_pred_final.Conversion_Prob.map(lambda x: 1 if x > 0.34 else 0)","e1874397":"y_pred_final.head()","d86155c6":"# Let's check the overall accuracy.\naccuracy_score=metrics.accuracy_score(y_pred_final.Converted, y_pred_final.Predicted)\naccuracy_score","d5ae633a":"confusion_test_set = metrics.confusion_matrix(y_pred_final.Converted, y_pred_final.Predicted)\nprint(confusion_test_set)","35d03fac":"TP = confusion_test_set[1,1] # true positive \nTN = confusion_test_set[0,0] # true negatives\nFP = confusion_test_set[0,1] # false positives\nFN = confusion_test_set[1,0] # false negatives","6e398569":"# Let's see the sensitivity of our logistic regression model\nTP \/ float(TP+FN)","91acf77a":"# Let us calculate specificity\nTN \/ float(TN+FP)","3534c233":"# Calculate false postive rate - predicting converion when customer does not have converted\nprint(FP\/ float(TN+FP))","6d200a37":"# Positive predictive value \nprint (TP \/ float(TP+FP))","d6d35609":"# Negative predictive value\nprint (TN \/ float(TN+ FN))","b469572f":"#precision\nconfusion_test_set[1,1]\/(confusion_test_set[0,1]+confusion_test_set[1,1])","2aed0d25":"#recall\nconfusion_test_set[1,1]\/(confusion_test_set[1,0]+confusion_test_set[1,1])","f7b41d25":"from sklearn.metrics import classification_report","c2faaf4d":"print(classification_report(y_pred_final.Converted, y_pred_final.Predicted))","901f4b0a":"from sklearn.metrics import precision_recall_curve","53cfd8c9":"p, r, thresholds = precision_recall_curve(y_pred_final.Converted, y_pred_final.Conversion_Prob)","ffa99fc8":"plt.plot(thresholds, p[:-1], \"g-\")\nplt.plot(thresholds, r[:-1], \"r-\")\nplt.show()","ae3c1e55":"def draw_roc( actual, probs ):\n    fpr, tpr, thresholds = metrics.roc_curve( actual, probs,\n                                              drop_intermediate = False )\n    auc_score = metrics.roc_auc_score( actual, probs )\n    plt.figure(figsize=(5, 5))\n    plt.plot( fpr, tpr, label='ROC curve (area = %0.2f)' % auc_score )\n    plt.plot([0, 1], [0, 1], 'k--')\n    plt.xlim([0.0, 1.0])\n    plt.ylim([0.0, 1.05])\n    plt.xlabel('False Positive Rate or [1 - True Negative Rate]')\n    plt.ylabel('True Positive Rate')\n    plt.title('Receiver operating characteristic (ROC) curve')\n    plt.legend(loc=\"lower right\")\n    plt.show()\n\n    return fpr,tpr, thresholds\n","db1ca64a":"fpr, tpr, thresholds = metrics.roc_curve(y_pred_final.Converted, y_pred_final.Conversion_Prob, drop_intermediate = False)","1edb182b":"draw_roc(y_pred_final.Converted, y_pred_final.Conversion_Prob)","484365e4":"y_pred_final.head()","e2011914":"y_pred_final['Lead Score'] = y_pred_final['Conversion_Prob']*100\ny_pred_final.head()","0eb6093c":"y_pred_final = pd.merge(leads[['Lead Number']], y_pred_final,how='inner',left_index=True, right_index=True)","507a8d5f":"y_pred_final.head()  # test dataset with all the Lead Score values","ba404181":"y_train_pred_df = y_train_pred_final[['Converted', 'Conversion_Prob', 'LeadID','Predicted']]\ny_train_pred_df.head()","177a0352":"y_train_pred_df = pd.merge(leads[['Lead Number']], y_train_pred_df,how='inner',left_index=True, right_index=True)\ny_train_pred_df.head()","e3e4e888":"y_train_pred_df['Lead Score'] = y_train_pred_df['Conversion_Prob']*100","c9d81926":"y_train_pred_df.head()     # train dataset with all the Lead Score values","3cc200ad":"final_df_lead_score = pd.concat([y_train_pred_df,y_pred_final],axis=0)\nfinal_df_lead_score.head()","925a576d":"final_df_lead_score = final_df_lead_score.set_index('LeadID')\n\nfinal_df_lead_score = final_df_lead_score[['Lead Number','Converted','Conversion_Prob','Predicted','Lead Score']]","86255499":"final_df_lead_score.head()  # final dataframe with all the Lead Scores","9d7e3e02":"final_df_lead_score.shape","ab991057":"# coefficients of our final model \n\npd.options.display.float_format = '{:.2f}'.format\nnew_params = res.params[1:]\nnew_params","ed17c1af":"# Getting a relative coeffient value for all the features wrt the feature with the highest coefficient\n\nfeature_importance = new_params\nfeature_importance = 100.0 * (feature_importance \/ feature_importance.max())\nfeature_importance","88889b85":"# Sorting the feature variables based on their relative coefficient values\n\nsorted_idx = np.argsort(feature_importance,kind='quicksort',order='list of str')","d60c6bbf":"feature_importance_df = pd.DataFrame(feature_importance).reset_index().sort_values(by=0,ascending=False)\nfeature_importance_df = feature_importance_df.rename(columns={'index':'Variables', 0:'Relative coeffient value'})\nfeature_importance_df = feature_importance_df.reset_index(drop=True)\nfeature_importance_df.head(3)","c72cbb83":"As we can see we have 38% conversion rate","724c37ff":"##### Area under the ROC curve is 0.97","cdf6a432":"##### Lead Profile column","e80186c5":"#### Dropping the columns for which we have created dummy variables","3f60e7a8":"We will drop this columns as most of the values in this column is 'Select' which does not add any information to our model","2654425d":"##### Asymmetrique Activity Score column","59c8c19e":"Again the accuracy hasn't dropped after dropping the 'Tags_wrong number given' feature column","cea2f795":"# Metrics beyond simply accuracy","5814421b":"#### Negative Predicted Value","74641648":"##### Total visits column","ff8c7848":"## Feature Scaling","7ac5a8cf":"Our model has about 92% accuracy","fe4df977":"#### From the above curve, 0.34 seems to be the optimum point to take as the cutoff probability","17e22e60":"## Determining Feature Importance of our final model","7c8be00d":"##### Asymmetrique Profile Score column","a0114de5":"Since maximum number of values in the country columns have \"India\" we are going to create 2 values for the country columns one being 'India' and the other being 'Foreign Country'","c5d0f37f":"### Mapping 'Yes' and 'No' to '1' and '0'","86ae1b72":"#### 1. We can see that now most of our P-values for all our variables are equal to 'Zero' which indicates that these variables are statistically significant so we do not need to drop more feature variables\n#### 2. Also the accuracy of our model hasn't dropped even after removing so many of the feature columns at around 91.6%","5891b025":"##### What matters most to you in choosing a course column","9b8fc2b3":"##### Our variables do not have high VIF which is good as it indicates we do not have multicolinearity issues to deal with","853588fa":"The variable 'Tags_Diploma holder (Not Eligible)' has high high P-value. So let's start by dropping that.","eb9c0166":"#### Positive Predicted Value","2f1ee03f":"#### Precision","ca75029e":"#### Creating a dataframe with the actual churn flag and the predicted probabilities","50098084":"## Splitting the data into Training and Test datasets","cb523e71":"# Project Brief\n\n#### An education company named X Education sells online courses to industry professionals. On any given day, many professionals who are interested in the courses land on their website and browse for courses. \n\n#### The company markets its courses on several websites and search engines like Google. Once these people land on the website, they might browse the courses or fill up a form for the course or watch some videos. When these people fill up a form providing their email address or phone number, they are classified to be a lead. Moreover, the company also gets leads through past referrals. Once these leads are acquired, employees from the sales team start making calls, writing emails, etc. Through this process, some of the leads get converted while most do not. The typical lead conversion rate at X education is around 30%. \n\n#### Now, although X Education gets a lot of leads, its lead conversion rate is very poor. For example, if, say, they acquire 100 leads in a day, only about 30 of them are converted. To make this process more efficient, the company wishes to identify the most potential leads, also known as \u2018Hot Leads\u2019. If they successfully identify this set of leads, the lead conversion rate should go up as the sales team will now be focusing more on communicating with the potential leads rather than making calls to everyone. \n\n#### There are a lot of leads generated in the initial stage but only a few of them come out as paying customers at the last stage. In the middle stage, you need to nurture the potential leads well (i.e. educating the leads about the product, constantly communicating etc. ) in order to get a higher lead conversion.\n\n#### X Education has appointed you to help them select the most promising leads, i.e. the leads that are most likely to convert into paying customers. The company requires you to build a model wherein you need to assign a lead score to each of the leads such that the customers with higher lead score have a higher conversion chance and the customers with lower lead score have a lower conversion chance. ","c66ce9e5":"#### Top three variables in your model which contribute most towards the probability of a lead getting converted","be3cf985":"### Checking VIFs","bbd8032f":"#### The top 3 variables are:\n1. Tags_Lost to EINS\t\n2. Tags_Closed by Horizzon\n3. Lead Source_Welingak Website","f787fb9d":"# Finding Optimal Cutoff Point","b714d8d2":"#### Creating new column 'predicted' with 1 if Churn_Prob > 0.5 else 0","465f22de":"Precision: \nTP \/ TP + FP","e707abdf":"#### As we can see some of the columns have substantial number of null or missing values. If we drop all these columns we will lose a lot of information so instead of dropping them, for some of the feature variables we will create a new value as 'Unknown' i.e. not specified to replace null or missing values.","43b9ed68":"##### Last Activity column","8a34eb29":"### Final dataframe with all the Lead Scores","b3281266":"## Feature selestion using RFE","38f38afb":"##### Specialization column","5636a60a":"#### False Postive Rate","6420a06b":"We will remove the rows with missing values","4d91a784":"So the overall accuracy hasn't dropped after dropping the 'Tags_Diploma holder (Not Eligible)' column ","fec4ade4":"#### Specificity","899fcde7":"# Plotting the ROC Curve","b6532efd":"#### Recall","44418e3c":"Recall: TP \/ TP + FN","a3015ab6":"#### Assessing the model with StatsModels","6830b74f":"##### Country column ","a03951f2":"Again the model accuracy hasn't decreased after removing the variable 'Tags_number not provided'","bc852773":"##### Tags column","1de7e037":"# Model building","3cd957ed":"##### Page views per visit column","0b754aad":"#### Classification Report","e14a0504":"We have removed most of the outliers and so we can proceed with model building","71ba4e22":"##### Lead Quality column","f45e5802":"## Imputing missing values and Dropping columns where imputation is not possible","45a38430":"##### What is your current occupation column","9c1581d1":"The variable 'Tags_wrong number given' has very high P-value. So we will drop that","cfe2d965":"# Precision and Recall","89979391":"##### How did you hear about X Education","2b5cd8fb":"#### Plotting the ROC Curve for Test Dataset","f2d00cba":"#### Precision recall curve","04752d3e":"We will drop this column as most of the values in this column belong to one category and others are null","be1c71e4":"## Final dataframe with the Lead Scores for all the LeadID","0b915d2d":"#### Confusion matrix","094532a4":"### Creating dummy variables for categorical variables","1b9fc1fe":"##### Area under the ROC curve is around 0.96 which means our model seems to be doing well on the test set as well","03385ab7":"The variable 'Tags_number not provided' has very high P-value. So we will drop that","c5d01731":"#### Sensitivity","d54621c3":"# Making predictions on the test set","798c515c":"##### City column"}}