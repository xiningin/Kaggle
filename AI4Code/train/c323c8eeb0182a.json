{"cell_type":{"3214292e":"code","501c4539":"code","70ce38c2":"code","729e70d4":"code","cd0d838f":"code","c4494d0f":"code","5285c5c9":"code","47f73104":"code","1cd273f1":"code","e6645940":"code","1c7ab466":"code","50cba697":"code","234213f6":"code","979bfd2e":"code","9d3ca502":"code","90eb0a1c":"code","a993bf84":"code","ce9a3048":"code","b39e38b1":"code","1bf81797":"code","ab6a45e2":"code","9d157735":"code","df14debf":"code","81134384":"code","909f839d":"code","09862b8f":"code","40aa9d0d":"code","ef4bed21":"code","a8b45777":"code","72aff4d3":"code","6d102d20":"code","ac2a1b67":"code","7b8b3182":"code","4cf2f427":"markdown","8aca5baa":"markdown","571d5b49":"markdown","c1a7e62d":"markdown","d9e94fd5":"markdown","9959823e":"markdown","159b1189":"markdown","3cbba27f":"markdown"},"source":{"3214292e":"import numpy as np \nimport pandas as pd\nimport random\nfrom sklearn.utils import shuffle\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import train_test_split\nfrom tensorflow.keras.layers import Conv2D, Dense, Dropout, Flatten\nfrom tensorflow.keras import layers\nfrom xgboost import XGBRegressor\nfrom sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\nimport xgboost as xgb\nimport lightgbm as lgb\nfrom xgboost import XGBRegressor\nfrom lightgbm import LGBMRegressor\nfrom catboost import CatBoostRegressor\nfrom sklearn.model_selection import KFold \nfrom sklearn.model_selection import cross_val_score\n\n# %matplotlib inline   #\uc96c\ud53c\ud130\ub178\ud2b8\ubd81\uc5d0\uc11c \uc774\ubbf8\uc9c0 \ud45c\uc2dc\uac00\ub2a5\ud558\uac8c \ud558\ub294 \uc96c\ud53c\ud130\ub178\ud2b8\ubd81 \ub9e4\uc9c1\ud568\uc218\nimport matplotlib.pyplot as plt \nimport seaborn as sns    \nfrom pathlib import Path\nimport tensorflow as tf\n\n\nimport warnings\nwarnings.filterwarnings('ignore')","501c4539":"IMG_SIZE = 224\nCHANNELS = 3\nBATCH_SIZE = 16\nEPOCHS = 10\nSEED = 2021\n\nDATA_DIR = '..\/input\/petfinder-pawpularity-score\/'\nTRAIN_DIR = DATA_DIR + 'train\/'\nTEST_DIR = DATA_DIR + 'test\/'","70ce38c2":"# Configure Strategy. Assume TPU...if not set default for GPU\/CPU\ntpu = None\ntry:\n    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n    tf.config.experimental_connect_to_cluster(tpu)\n    tf.tpu.experimental.initialize_tpu_system(tpu)\n    strategy = tf.distribute.experimental.TPUStrategy(tpu)\nexcept ValueError:\n    # Enable XLA\n    tf.config.optimizer.set_jit(enabled = \"autoclustering\")\n    strategy = tf.distribute.get_strategy()","729e70d4":"# Load Train Data\nsample_df = pd.read_csv(f'{DATA_DIR}train.csv')\nsample_df['Id'] = sample_df['Id'].apply(lambda x: f'{TRAIN_DIR}{x}.jpg')\n\n# Label value to be used for feature model 'classification' training.\nsample_df['target_value'] = sample_df['Pawpularity'] \/ 100.","cd0d838f":"sample_df = shuffle(sample_df, random_state=SEED)\ntrain_size = int(len(sample_df)*0.8)\ntrain_df = sample_df[:train_size]\nvalidation_df = sample_df[train_size:]\ntrain_df.head()","c4494d0f":"training_data = tf.data.Dataset.from_tensor_slices((train_df['Id'].values, train_df['target_value'].values))\nvalidation_data = tf.data.Dataset.from_tensor_slices((validation_df['Id'].values, validation_df['target_value'].values))","5285c5c9":"def load_image_and_label_from_path(image_path, label):\n    img = tf.io.read_file(image_path)\n    img = tf.image.decode_jpeg(img, channels=CHANNELS)\n    img = tf.image.resize_with_pad(img, IMG_SIZE, IMG_SIZE)\n    img = tf.image.adjust_brightness(img, 0.5)\n    img = tf.image.adjust_saturation(img, 3)\n    return img, label\n\nAUTOTUNE = tf.data.experimental.AUTOTUNE #\uba54\ubaa8\ub9ac \ub3d9\uc801 \ud560\ub2f9\uc744 \uc704\ud55c AUTOTUNE\ntraining_data = training_data.map(load_image_and_label_from_path, num_parallel_calls=AUTOTUNE) #train \ub370\uc774\ud130\ub97c \ubd88\ub7ec\uc634\nvalidation_data = validation_data.map(load_image_and_label_from_path,num_parallel_calls=AUTOTUNE) #validation \ub370\uc774\ud130\ub97c \ubd88\ub7ec\uc634","47f73104":"#train \ubc0f validation \ub370\uc774\ud130\ub97c \ud6c8\ub828\ud558\uae30 \uc88b\uac8c batch\ub85c \uc790\ub984\ntraining_data_batches = training_data.shuffle(buffer_size=1000).batch(BATCH_SIZE).prefetch(buffer_size=AUTOTUNE)\nvalidation_data_batches = validation_data.shuffle(buffer_size=1000).batch(BATCH_SIZE).prefetch(buffer_size=AUTOTUNE)","1cd273f1":"model = tf.keras.Sequential()\neffnet = tf.keras.applications.EfficientNetB0(include_top=False, classes=None, \n                                              weights=\"..\/input\/efficientnet-keras-noisystudent-weights-b0b7\/noisystudent\/noisy.student.notop-b0.h5\",\n                                              input_shape = (IMG_SIZE, IMG_SIZE, CHANNELS))\neffnet.trainable = True\nmodel.add(effnet)\nmodel.add(tf.keras.layers.Dropout(0.25))\nmodel.add(tf.keras.layers.BatchNormalization())  \nmodel.add(tf.keras.layers.Flatten())\nmodel.add(tf.keras.layers.Dense(128, activation='relu'))\nmodel.add(tf.keras.layers.Dense(256, activation='relu'))\nmodel.add(tf.keras.layers.Dense(128, activation='relu'))\nmodel.add(tf.keras.layers.Dense(156, activation='relu'))\nmodel.add(tf.keras.layers.Dense(128, activation='relu'))\nmodel.add(tf.keras.layers.Dense(1000, activation='relu'))\nmodel.add(tf.keras.layers.Dense(186, activation='relu'))\nmodel.add(tf.keras.layers.Dense(300, activation='relu'))\nmodel.add(tf.keras.layers.Dense(560, activation='relu'))\nmodel.add(tf.keras.layers.Dense(186, activation='relu'))\nmodel.add(tf.keras.layers.Dense(186, activation='relu'))\nmodel.add(tf.keras.layers.Dense(356, activation='relu'))\nmodel.add(tf.keras.layers.Dense(186, activation='relu'))\nmodel.add(tf.keras.layers.Dense(186, activation='relu'))\nmodel.add(tf.keras.layers.Dense(186, activation='relu'))\nmodel.add(tf.keras.layers.Dense(254, activation='relu'))\nmodel.add(tf.keras.layers.Dense(186, activation='relu'))\nmodel.add(tf.keras.layers.Dense(186, activation='relu'))\nmodel.add(tf.keras.layers.Dense(186, activation='relu'))\nmodel.add(tf.keras.layers.Dense(1, activation='sigmoid'))\nmodel.summary()\n","e6645940":"model.compile(optimizer='adam', loss = tf.keras.losses.BinaryCrossentropy(), metrics=[tf.keras.metrics.RootMeanSquaredError('rmse')])","1c7ab466":"checkpoint =  tf.keras.callbacks.ModelCheckpoint(f'feature_model.h5',\n                                              verbose = 1, \n                                              monitor = 'val_loss', \n                                              mode = 'min', \n                                              save_weights_only = True,\n                                              save_best_only = True)\nmodel.fit(training_data_batches, epochs = EPOCHS,callbacks = [checkpoint], validation_data = validation_data_batches, verbose=1 )","50cba697":"def make_tabular_data(df, features, drop_features = ['index', 'Id', 'Action', 'Human',  'Pawpularity']):\n    features = pd.Series(np.squeeze(features), name='features')\n    df = pd.concat([df.reset_index(), features], axis=1)\n    df = df.drop(drop_features, axis=1)\n    return df\n\n# 'Action', 'Collage', 'Group',","234213f6":"tabular_train = make_tabular_data(train_df, model.predict(training_data_batches))\ntabular_valid = make_tabular_data(validation_df, model.predict(validation_data_batches))","979bfd2e":"y_train = tabular_train['target_value']\nX_train = tabular_train.drop(['target_value'], axis=1)\ny_valid = tabular_valid['target_value']\nX_valid = tabular_valid.drop(['target_value'], axis=1)","9d3ca502":"t = pd.DataFrame(data={\"col\": train_df.dtypes.index, \"type\": train_df.dtypes}).reset_index(drop=True)\n","90eb0a1c":"plt.figure(figsize=(15,15))\nsns.heatmap(data = train_df.corr(), annot=True, \nfmt = '.2f', linewidths=.5, cmap='Blues')","a993bf84":"model_xgb = xgb.XGBRegressor(colsample_bytree=0.4603, gamma=0.0468, \n                             learning_rate=0.08, max_depth=3, \n                             min_child_weight=1.7817, n_estimators=2200,\n                             reg_alpha=0.4640, reg_lambda=0.8571,\n                             subsample=0.5213, silent=1,\n                             random_state =7, nthread = -1)","ce9a3048":"model_lgb = lgb.LGBMRegressor(objective='regression',num_leaves=5,\n                              learning_rate=0.08, n_estimators=20000,\n                              max_bin = 55, bagging_fraction = 0.8,\n                              bagging_freq = 5, feature_fraction = 0.2319,\n                              feature_fraction_seed=9, bagging_seed=9,\n                              min_data_in_leaf =6, min_sum_hessian_in_leaf = 11\n                             )","b39e38b1":"model_CBR = CatBoostRegressor(iterations=1900,\n                              learning_rate=0.08,\n                              depth=4,\n                              l2_leaf_reg=20,\n                              bootstrap_type='Bernoulli',\n                              subsample=0.6, eval_metric='RMSE',\n                              metric_period=50, od_type='Iter',\n                              od_wait=45, random_seed=17,\n                              allow_writing_files=False, verbose=False\n                                )","1bf81797":"model_xgb.fit(X_train, y_train)\nmodel_lgb.fit(X_train, y_train)\nmodel_CBR.fit(X_train, y_train)","ab6a45e2":"pred_xgb = model_xgb.predict(X_valid)\npred_lgb = model_lgb.predict(X_valid)\npred_CBR = model_CBR.predict(X_valid)","9d157735":"X_valid.head()","df14debf":"pred_ensemble_val = (pred_xgb + pred_lgb + pred_CBR)\/3","81134384":"# rmse\nprint(np.sqrt(mean_squared_error(pred_ensemble_val, y_valid)))\n         \n# xgb hyper parameter tune -> 0.207384233279449\n# ensemble -> 0.20971427318649655 -> 20.96696","909f839d":"# Load Test Data\ntest_df = pd.read_csv(f'{DATA_DIR}test.csv')\ntest_df['Id'] = test_df['Id'].apply(lambda x: f'{TEST_DIR}{x}.jpg')\ntest_df['Pawpularity'] = 0\n\n# Summary\nprint(f'test_df: {test_df.shape}')\ntest_df.head()","09862b8f":"def load_image(image_path):\n    img = tf.io.read_file(image_path)\n    img = tf.image.decode_jpeg(img, channels=CHANNELS)\n    img = tf.image.resize_with_pad(img, IMG_SIZE, IMG_SIZE)\n    img = np.reshape(img, [-1,IMG_SIZE, IMG_SIZE, CHANNELS])\n    return img","40aa9d0d":"def test_predict(filepath):\n    local_image = load_image(filepath)\n    prediction = model.predict(local_image)\n    return float(prediction)*100","ef4bed21":"def predictions_over_image(filepath):\n    predictions = []\n    for path in filepath:\n        predictions.append(test_predict(path))\n    return predictions","a8b45777":"test_feature = predictions_over_image(test_df['Id'])\ntest_feature = pd.Series(test_feature, name='features')\ntabular_test = pd.concat([test_df.reset_index(), test_feature], axis=1)\ntabular_test = tabular_test.drop(['index', 'Id','Action', 'Human', 'Pawpularity'], axis=1)","72aff4d3":"pred_xgb = model_xgb.predict(tabular_test)\npred_lgb = model_lgb.predict(tabular_test)\npred_CBR = model_CBR.predict(tabular_test)","6d102d20":"pred_ensemble_real = (pred_xgb + pred_lgb + pred_CBR)\/3","ac2a1b67":"submission = pd.DataFrame(columns={\"Id\", \"Pawpularity\"})\nsubmission['Id'] = test_df['Id'].map(lambda i : i.split('\/')[-1].split('.')[0])\nsubmission['Pawpularity'] = pred_ensemble_real\nsubmission['Pawpularity'] = submission['Pawpularity'].map(lambda i: i*100)\nsubmission.head()","7b8b3182":"submission.to_csv('submission.csv', index = False)","4cf2f427":"# **\uc774\ubbf8\uc9c0 \ub370\uc774\ud130 \ud6c8\ub828**","8aca5baa":"# **Data preparing**","571d5b49":"\uc608\uce21\ud558\ub824\ub294 \ud0c0\uac9f \ubca8\ub958\uc640 column\ub4e4\uc774 \uadf8\ub807\uac8c \uad00\ub828\uc774 \uc5c6\ub2e4\n\uc774\ubbf8\uc9c0\ub97c \uc5b4\ub5a4\ubc29\uc2dd\uc73c\ub85c \ucc98\ub9ac\ud558\ub294\uac00\uac00 \uac00\uc7a5\uc88b\uc74c","c1a7e62d":"# **EDA**","d9e94fd5":"# **tabular data + cnn pawpularity**","9959823e":"# Modeling","159b1189":"# Evaluate","3cbba27f":"# **Ensemble**"}}