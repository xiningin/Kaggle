{"cell_type":{"a3c6f184":"code","4cbfba70":"code","05057c05":"code","4e6e5ed6":"code","a8310e26":"code","c55ed6a4":"code","78bc0ada":"code","7d98939f":"code","1ae5f438":"code","dbdbc1cb":"code","b90a0d6c":"code","a1142ac6":"code","9ff8f94b":"code","82384ac0":"code","ba7be39a":"code","f3d6e271":"code","acda2598":"code","c2c19e6c":"code","0d090ee0":"code","ee6b602a":"code","ef8276f2":"code","027e3411":"code","9154e582":"markdown","dbf891e9":"markdown","c03eb0d1":"markdown","a7e76b95":"markdown","24fdde84":"markdown","799dc0d0":"markdown","30dde2cd":"markdown","e109f744":"markdown","461c9907":"markdown"},"source":{"a3c6f184":"from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\nfrom plotly.subplots import make_subplots\nimport plotly.graph_objects as go\nimport plotly.offline as py\nimport cufflinks as cf\nimport plotly\nimport matplotlib.pyplot as plt\nfrom scipy.signal import butter\nfrom scipy import signal\nimport seaborn as sns\nimport pandas as pd \nimport numpy as np\nimport numpy.fft as fft\nimport warnings\nimport os\nfrom sklearn.metrics import confusion_matrix, f1_score\n  \ncf.go_offline()\npy.init_notebook_mode(connected=True)\ncf.getThemes()\ncf.set_config_file(theme='ggplot')\nwarnings.simplefilter('ignore')\npd.plotting.register_matplotlib_converters()\nsns.mpl.rc('figure',figsize=(16, 6))\nplt.style.use('ggplot')\nsns.set_style('darkgrid')","4cbfba70":"base = os.path.abspath('\/kaggle\/input\/')\n\ntrain = pd.read_csv(os.path.join(base+ '\/liverpool-ion-switching\/train.csv'))\ntest  = pd.read_csv(os.path.join(base+ '\/liverpool-ion-switching\/test.csv'))\n\ntrain_clean = pd.read_csv(os.path.join(base+ '\/data-without-drift\/train_clean.csv'))\ntest_clean  = pd.read_csv(os.path.join(base+ '\/data-without-drift\/test_clean.csv'))\n\ntrain_clean_Kalman = pd.read_csv(os.path.join(base+ '\/clean-kalman\/train_clean_kalman.csv'))","05057c05":"def add_batch_to_data(df):\n    batches = df.shape[0] \/\/ 500000\n    df['batch'] = 0\n    for i in range(batches):\n        idx = np.arange(i*500000, (i+1)*500000)\n        df.loc[idx, 'batch'] = i + 1\n    return df\n\ntrain = add_batch_to_data(train)\ntrain_clean = add_batch_to_data(train_clean)\ntrain_clean_Kalman = add_batch_to_data(train_clean_Kalman)\n\ntest = add_batch_to_data(test)\ntest_clean = add_batch_to_data(test_clean)","4e6e5ed6":"def plots(train, train_clean, batches, title0='signal', title1='signal_clean'):\n    legs = ['signal','open_channels','signal_diff','signal-open_chn']\n    fig, axs = plt.subplots(nrows=2,ncols=1,figsize=(20,10),sharex=True); res = 100\n\n    axs[0].plot(range(0,train.shape[0],res), train.signal[0::res],marker='o',ms=0.01,label=legs[0])\n    axs[0].plot(range(0,train.shape[0],res), train.open_channels[0::res],alpha=0.4,label=legs[1])\n\n    for i in range(batches+1): \n        axs[0].plot([i*500000,i*500000],[-5,12.5],'r')\n    for j in range(batches): \n        axs[0].text(j*500000+200000,10,str(j+1),size=20)\n\n    axs[1].plot(range(0,train_clean.shape[0],res), train_clean.signal[0::res],marker='o',ms=0.01,label=legs[0],alpha=0.4)\n    axs[1].plot(range(0,train_clean.shape[0],res), train_clean.open_channels[0::res],alpha=0.4,label=legs[1])\n\n    for i in range(batches): \n        axs[1].plot([i*500000,i*500000],[-5,12.5],'r')\n\n    axs[0].set_ylabel('Signal',size=16); axs[1].set_ylabel('Signal',size=16); axs[1].set_xlabel('Row',size=16);\n    axs[0].set_title(title0,size=20); axs[1].set_title(title1,size=20)\n    axs[0].legend(); axs[1].legend()\n    plt.show()\n    \nplots(train, train_clean, batches=10, title0='Train - 10 batches', title1='Train_clean')","a8310e26":"def plots1(test, test_clean, batches, res=100, title0='Test', title1='Test_clean'):\n    legs = ['signal','open_channels','signal_diff','signal-open_chn']\n    fig, axs = plt.subplots(nrows=2,ncols=1,figsize=(20,8),sharex=True)\n\n    axs[0].plot(range(0,test.shape[0],res), test.signal[0::res],marker='o',ms=0.01,label=legs[0])\n    for i in range(batches+1): \n        axs[0].plot([i*500000,i*500000],[-5,12.5],'r')\n    for j in range(batches): \n        axs[0].text(j*500000+200000,10,str(j+1),size=20)\n        \n    axs[1].plot(range(0,test_clean.shape[0],res), test_clean.signal[0::res], marker='o',ms=0.01,label=legs[0],alpha=0.4)\n\n    for i in range(batches): \n        axs[1].plot([i*500000,i*500000],[-5,12.5],'r')\n\n    axs[0].set_ylabel('Signal',size=16); axs[1].set_ylabel('Signal',size=16); axs[1].set_xlabel('Row',size=16);\n    axs[0].set_title(title0,size=20); axs[1].set_title(title1,size=20)\n    axs[0].legend(); axs[1].legend()\n    plt.show()\nplots1(test, test_clean, 4) ","c55ed6a4":"def plot_5hist_signal(train, train_clean, batch_no):\n    from scipy.stats import norm\n    \n    a = (batch_no-1)*500000; b = batch_no*500000; res=500\n    print('#'*25)\n    print('### Batch NO.%d'%batch_no)\n    print('#'*25)\n    \n    fig, axs = plt.subplots(nrows=1,ncols=5,figsize=(30,5))\n    \n    sns.distplot(train.signal[a:b],kde=False,ax=axs[0])\n    sns.distplot(train_clean.signal[a:b],kde=False,ax=axs[1])\n    \n    chn_mu_sig = pd.DataFrame(columns=['chn', 'mu', 'sigma'])\n    for i in range(11):\n        if train_clean.signal[a:b][train.open_channels[a:b]==i].shape[0]>1000:\n            #print(i)\n            (mu, sigma) = norm.fit(train_clean.signal[a:b][train.open_channels[a:b]==i])\n            sns.distplot(train_clean.signal[a:b][train.open_channels[a:b]==i],kde=False,ax=axs[2],\n                         label=str(i)+', {0:.4g}, {1:.4f}'.format(mu, sigma))\n            chn_mu_sig = chn_mu_sig.append({'chn': i, 'mu': mu,'sigma': sigma}, ignore_index=True)\n    sns.regplot(x='chn',y='mu', data=chn_mu_sig, ax=axs[3]); \n\n    ab = np.polyfit(chn_mu_sig.chn, chn_mu_sig.mu, 1); axs[3].text(0.1,0.85,'slop:{0:.4f}, intercept:{1:.4f}'.format(ab[0],ab[1]),size=15,transform=axs[3].transAxes)\n\n    # Get the fitted parameters used by sns\n    sns.distplot((train_clean.signal[a:b]- ab[0]*train_clean.open_channels[a:b]-ab[1]),fit=norm,kde=False,ax=axs[4])\n    (mu, sigma) = norm.fit((train_clean.signal[a:b]- ab[0]*train_clean.open_channels[a:b]-ab[1]),loc=0, scale=0.25)\n\n    axs[0].set_xlabel('signal')\n    axs[1].set_xlabel('clean_signal')\n    axs[2].set_xlabel('clean_signal_by_chn');\n    axs[4].set_xlabel('signal_clean - (a*open_chn+b)')\n    \n    # Legend and labels \n    axs[4].legend([\"normal dist. fit\\n ($\\mu=${0:.4g},$\\sigma=${1:.4f})\".format(mu, sigma)])\n    axs[2].legend(title='chn,  $\\mu$,  $\\sigma$')\n    \n    plt.show()","78bc0ada":"plot_5hist_signal(train, train_clean, 1)\nplot_5hist_signal(train, train_clean, 2)\nplot_5hist_signal(train, train_clean, 3)\nplot_5hist_signal(train, train_clean, 7)","7d98939f":"plot_5hist_signal(train, train_clean, 4)\nplot_5hist_signal(train, train_clean, 8)\nplot_5hist_signal(train, train_clean, 6)\nplot_5hist_signal(train, train_clean, 9)","1ae5f438":"plot_5hist_signal(train, train_clean, 5)\nplot_5hist_signal(train, train_clean, 10)","dbdbc1cb":"def plot_hist_time_channel(train): \n    fig, axs = plt.subplots(nrows=2,ncols=5,figsize=(25,10),gridspec_kw={'hspace':0.3})\n    for i in range(10):\n        a = i*500000; b = (i+1)*500000\n        for j in range(11):\n            if train.time[a:b][train.open_channels[a:b]==j].empty != True:\n                sns.distplot(train.iloc[a:b].loc[train.iloc[a:b].open_channels==j, 'time'], kde=False, ax=axs[i\/\/5,np.remainder(i,5)],label='open_chn {}'.format(j))\n                axs[i\/\/5,np.remainder(i,5)].set_title('Batch No {}'.format(i+1))\n        if i==10: \n            axs[i\/\/5,np.remainder(i,5)].legend()\n    plt.show()\nplot_hist_time_channel(train_clean)","b90a0d6c":"def compare_abSignal_chn(df, df_Kalman=None, batch=10,start=5000, end=5500, res=1):\n    ab = [[1.23681698, -2.72065304],[1.23750154, -2.73096669],[1.23409502, -2.73621808],[1.23065603, -2.73611005],[1.23429745, -5.46470623],\n          [1.23436182, -2.73395716],[1.23282196, -2.73299525],[1.22087484, -2.71416059],[1.23557789, -2.75182785],[1.22429227, -5.39251544]]\n    slop, intercept = ab[batch-1]\n    fig = go.Figure(data=go.Line(x=df.loc[df.batch==batch, 'time'][start:end:res], y=(df.loc[df.batch==batch,'signal'][start:end:res]-intercept)\/slop, name='(signal-b)\/a'))\n    fig.add_trace(go.Line(x=df.loc[df.batch==batch, 'time'][start:end:res], y=df.loc[df.batch==batch,'open_channels'][start:end:res],name='open_chn' ))\n    fig.add_trace(go.Line(x=df.loc[df.batch==batch, 'time'][start:end:res], y=(df.loc[df.batch==batch,'signal'][start:end:res]-intercept)\/slop - \n                          df.loc[df.batch==batch,'open_channels'][start:end:res],name='(signal-b)\/a - open_chn' ))\n    if df_Kalman is not None:\n        fig.add_trace(go.Line(x=df_Kalman.loc[df.batch==batch, 'time'][start:end:res], y=(df_Kalman.loc[df_Kalman.batch==batch,'signal'][start:end:res]-intercept)\/slop, name='(signal_Kalman-b)\/a' ))\n        fig.add_trace(go.Line(x=df_Kalman.loc[df.batch==batch, 'time'][start:end:res], \n                              y=df_Kalman.loc[df_Kalman.batch==batch,'signal'][start:end:res]-df.loc[df.batch==batch,'signal'][start:end:res], name='signal-signal_Kalman' ))\n        fig.add_trace(go.Line(x=df.loc[df_Kalman.batch==batch, 'time'][start:end:res], y=(df_Kalman.loc[df_Kalman.batch==batch,'signal'][start:end:res]-intercept)\/slop - \n                              df.loc[df.batch==batch,'open_channels'][start:end:res],name='(signal_Kalman-b)\/a - open_chn' ))\n    \n    fig.show()\n    return","a1142ac6":"compare_abSignal_chn(train_clean, train_clean_Kalman, batch=5)","9ff8f94b":"# Plot: acf(abSignal-open_channels), acf(open_channels), pacf(open_channels), acf(abSignal_Kalman-open_channels)\ndef plot_acf_noise(df, df_Kalman=None, batch=10, start=5000,end=10000, res=1, lags=500 ):\n    ab = [[1.23681698, -2.72065304],[1.23750154, -2.73096669],[1.23409502, -2.73621808],[1.23065603, -2.73611005],[1.23429745, -5.46470623],\n          [1.23436182, -2.73395716],[1.23282196, -2.73299525],[1.22087484, -2.71416059],[1.23557789, -2.75182785],[1.22429227, -5.39251544]]\n    slop, intercept = ab[batch-1]\n    batch_sampled_diff = (df.loc[df.batch==batch,'signal'][start:end:res]-intercept)\/slop - df.loc[df.batch==batch,'open_channels'][start:end:res]\n    if df_Kalman is not None:\n        fig, axs = plt.subplots(2,2,figsize=(16,8))\n        axs = axs.flatten()\n    else:\n        fig, axs = plt.subplots(1,3,figsize=(25,3))\n    \n    plot_acf(batch_sampled_diff, lags=lags, title='acf_noise', ax=axs[0])\n    plot_acf(df.loc[df.batch==batch,'open_channels'][start:end:res], lags=lags, title='acf_open_channels', ax=axs[1])\n    plot_pacf(df.loc[df.batch==batch,'open_channels'][start:end:res], lags=lags, title='pacf_open_channels', ax=axs[2])\n    \n    if df_Kalman is not None:\n        plot_acf((df_Kalman.loc[df.batch==batch,'signal'][start:end:res]-intercept)\/slop - df_Kalman.loc[df.batch==batch,'open_channels'][start:end:res], \n                 lags=lags, title='acf_noise_Kalman', ax=axs[3])\n    \nplot_acf_noise(train_clean, train_clean_Kalman, start=0, end=20000, batch=5)","82384ac0":"def fft_noise(df, batch=10):\n    ab = [[1.23681698, -2.72065304],[1.23750154, -2.73096669],[1.23409502, -2.73621808],[1.23065603, -2.73611005],[1.23429745, -5.46470623],\n      [1.23436182, -2.73395716],[1.23282196, -2.73299525],[1.22087484, -2.71416059],[1.23557789, -2.75182785],[1.22429227, -5.39251544]]\n    slop, intercept = ab[batch-1]\n    \n    noise = ((df.loc[df.batch==batch,'signal']-intercept)\/slop - df.loc[df.batch==batch,'open_channels']) \n\n    F = fft.fft(noise)\n    w = np.linspace(0, 1, len(F))\n    plt.figure(figsize=(5,4))\n    plt.plot(w, np.abs(F))\n    plt.xlim(0, 0.01)\n    plt.show()\nfft_noise(train_clean, batch=5)","ba7be39a":"def bandstop(x, samplerate = 1000000, fp = np.array([9850, 10150]), fs = np.array([9600, 10400])):\n    fn = samplerate \/ 2   # Nyquist frequency\n    wp = fp \/ samplerate\n    ws = fs \/ samplerate\n    gpass = 1\n    gstop = 10.0\n\n    N, Wn = signal.buttord(wp, ws, gpass, gstop)\n    b, a = signal.butter(N, Wn, \"bandstop\")\n    y = signal.filtfilt(b, a, x)\n    return y\n\ndef bandpass(x, samplerate = 1000000, fp = np.array([9850, 10150]), fs = np.array([9600, 10400])):\n    fn = samplerate \/ 2   # Nyquist frequency\n    wp = fp \/ samplerate\n    ws = fs \/ samplerate\n    gpass = 1\n    gstop = 10.0\n\n    N, Wn = signal.buttord(wp, ws, gpass, gstop)\n    b, a = signal.butter(N, Wn, \"bandpass\")\n    y = signal.filtfilt(b, a, x)\n    return y\n\ndef noise_spectrogram_plot(df, batch):\n    # slop and intercept for the train dataset\n    ab = [[1.23681698, -2.72065304],[1.23750154, -2.73096669],[1.23409502, -2.73621808],[1.23065603, -2.73611005],[1.23429745, -5.46470623],\n          [1.23436182, -2.73395716],[1.23282196, -2.73299525],[1.22087484, -2.71416059],[1.23557789, -2.75182785],[1.22429227, -5.39251544]]\n    slop, intercept = ab[batch-1]\n    \n    fig, ax = plt.subplots(5, 1, figsize=(8, 12), gridspec_kw={\"height_ratios\": [3,1, 1, 3,3]})\n    ax = ax.flatten()\n    \n    # noise spectrogram\n    fs = 10_000 # sampling rate is 10kHz of the signal: 0.0001sec step\n    noise = (df.loc[df.batch==batch,'signal']-intercept)\/slop - df.loc[df.batch==batch,'open_channels']\n    f, t, Sxx = signal.spectrogram(noise , fs, nperseg=5000)\n    ax[0].pcolormesh(t, f, -np.log(Sxx), cmap=\"plasma\")\n    ax[0].set_ylabel('Frequency [Hz]')\n    ax[0].set_ylim([0, 500])\n    ax[0].set_xlabel('Time [sec]')\n    plt.tight_layout()\n    \n    # Power histogram (collapsed across time)\n    ax[1].plot(f, np.log(np.mean(Sxx, axis=1)), color=\"g\")\n    ax[1].set_xlabel(\"Frequency [Hz]\")\n    ax[1].set_xlim([0, 500])\n    ax[1].set_ylabel(\"Power\")\n    print('power max at ', f[np.argmax(np.mean(Sxx, axis=1))], 'Hz')\n    \n    # Power vs time for 50Hz\n    ax[2].plot(t, Sxx[np.argmax(np.mean(Sxx, axis=1))], color=\"r\")\n    ax[2].set_xlabel('Time [sec]')\n    ax[2].set_ylabel('Power for 50Hz')\n    ax[2].set_xlim([0, 50])\n    \n    # Noise vs time and smoothed, filtered, removed\n    start, end, res = 0, 5000, 1\n    times = np.arange(start,end,res)*0.0001\n    ax[3].plot(times,noise[start:end:res],label = \"original\",alpha=1)\n    ax[3].plot(times,signal.savgol_filter(noise[start:end], 81, 3)[start:end:res],alpha=1,label='Smoothed',zorder=10)\n    ax[3].set_xlabel('Time [sec]')\n    ax[3].set_ylabel('Noise')\n    ax[3].set_ylim([-1, 1])\n    \n    sig_pass = bandstop(noise)\n    ax[3].plot(times, sig_pass[start:end:res], label = \"Kept\",alpha=1)\n    sig_remove = bandpass(noise)\n    ax[3].plot(times, sig_remove[start:end:res], label = \"Removed\",alpha=1)\n    ax[3].legend()\n    \n    plot_acf(noise[0:20000], lags=500, label='acf_noise', ax=ax[4])\n    plot_acf(sig_pass[0:20000], lags=500, label='acf_after', ax=ax[4])\n    ax[4].set_ylim([-0.06,0.06])\n    ax[4].legend()","f3d6e271":"noise_spectrogram_plot(train_clean, batch=5)","acda2598":"ab = [[1.23681698, -2.72065304],[1.23750154, -2.73096669],[1.23409502, -2.73621808],[1.23065603, -2.73611005],[1.23429745, -5.46470623],\n      [1.23436182, -2.73395716],[1.23282196, -2.73299525],[1.22087484, -2.71416059],[1.23557789, -2.75182785],[1.22429227, -5.39251544]]\n\nul = [2,2,2,4,11,6,2,4,6,11] # upper limits\n\ntrain_clean['open_channels_pred'] = np.nan\n\nf1_s = []\n\nfor i in [1,2,3,7,\n          4,8,\n          6,9,\n          5,10]:\n    y_true = train_clean.loc[train.batch==i, 'open_channels']\n    y_pred_raw = ( (train_clean.loc[train_clean.batch==i,'signal']-ab[i-1][1]) \/ ab[i-1][0] )\n    y_pred = y_pred_raw.round().astype('int')\n    y_pred[y_pred<=-1] = 0\n    y_pred[y_pred>=ul[i-1]] = ul[i-1]-1\n    \n    train_clean.loc[train_clean.batch==i,'open_channels_pred_raw'] = y_pred_raw\n    train_clean.loc[train_clean.batch==i,'open_channels_pred'] = y_pred\n    \n    f1 = f1_score(y_true, y_pred, average='macro')\n    print('Batch %1d'%i,'Accuracy', np.sum(y_true==y_pred)\/500000,'f1 score', f1)\n    \n    f1_s.extend([f1])\n    \n    cm = confusion_matrix(y_true,y_pred,normalize='true')\n    fig, ax = plt.subplots(figsize=(5,5))\n    sns.heatmap(cm, annot=True, ax=ax)\n    ax.set_aspect('equal')\n    \nprint('Final f1 score for the trainning data',f1_score(train_clean['open_channels'],train_clean['open_channels_pred'],average='macro'))","c2c19e6c":"from functools import partial\nimport scipy as sp\nclass OptimizedRounder(object):\n    \"\"\"\n    An optimizer for rounding thresholds\n    to maximize F1 (Macro) score\n    # https:\/\/www.kaggle.com\/naveenasaithambi\/optimizedrounder-improved\n    \"\"\"\n    def __init__(self):\n        self.coef_ = 0\n\n    def _f1_loss(self, coef, X, y):\n        \"\"\"\n        Get loss according to\n        using current coefficients\n        \n        :param coef: A list of coefficients that will be used for rounding\n        :param X: The raw predictions\n        :param y: The ground truth labels\n        \"\"\"\n        X_p = pd.cut(X, [-np.inf] + list(np.sort(coef)) + [np.inf], labels = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10])\n\n        return -f1_score(y, X_p, average = 'macro')\n\n    def fit(self, X, y):\n        \"\"\"\n        Optimize rounding thresholds\n        \n        :param X: The raw predictions\n        :param y: The ground truth labels\n        \"\"\"\n        loss_partial = partial(self._f1_loss, X=X, y=y)\n        initial_coef = [0.5, 1.5, 2.5, 3.5, 4.5, 5.5, 6.5, 7.5, 8.5, 9.5]\n        self.coef_ = sp.optimize.minimize(loss_partial, initial_coef, method='nelder-mead')\n\n    def predict(self, X, coef):\n        \"\"\"\n        Make predictions with specified thresholds\n        \n        :param X: The raw predictions\n        :param coef: A list of coefficients that will be used for rounding\n        \"\"\"\n        return pd.cut(X, [-np.inf] + list(np.sort(coef)) + [np.inf], labels = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10])\n\n\n    def coefficients(self):\n        \"\"\"\n        Return the optimized coefficients\n        \"\"\"\n        return self.coef_['x']","0d090ee0":"# a few minuts to run\noptR = OptimizedRounder()\noptR.fit(train_clean['open_channels_pred_raw'], train_clean['open_channels'])\ncoefficients = optR.coefficients()\nprint(coefficients)","ee6b602a":"def optimize_prediction(prediction):\n    prediction[prediction <= coefficients[0]] = 0\n    prediction[np.where(np.logical_and(prediction > coefficients[0], prediction <= coefficients[1]))] = 1\n    prediction[np.where(np.logical_and(prediction > coefficients[1], prediction <= coefficients[2]))] = 2\n    prediction[np.where(np.logical_and(prediction > coefficients[2], prediction <= coefficients[3]))] = 3\n    prediction[np.where(np.logical_and(prediction > coefficients[3], prediction <= coefficients[4]))] = 4\n    prediction[np.where(np.logical_and(prediction > coefficients[4], prediction <= coefficients[5]))] = 5\n    prediction[np.where(np.logical_and(prediction > coefficients[5], prediction <= coefficients[6]))] = 6\n    prediction[np.where(np.logical_and(prediction > coefficients[6], prediction <= coefficients[7]))] = 7\n    prediction[np.where(np.logical_and(prediction > coefficients[7], prediction <= coefficients[8]))] = 8\n    prediction[np.where(np.logical_and(prediction > coefficients[8], prediction <= coefficients[9]))] = 9\n    prediction[prediction > coefficients[9]] = 10\n    \n    return prediction","ef8276f2":"y_pred = optimize_prediction(train_clean['open_channels_pred_raw'].values)\nf1_score(train_clean['open_channels'], y_pred, average = 'macro')","027e3411":"cm = confusion_matrix(train_clean['open_channels'],y_pred,normalize='true')\nfig, ax = plt.subplots(figsize=(10,10))\nsns.heatmap(cm, annot=True, ax=ax)\nax.set_aspect('equal')","9154e582":"### The noise has frequencies: \n\n1. AC electricity frequency 50Hz, and one weak 350Hz\n2. The power varies as a function of time\n3. The 0.02 sec periodicity is quite clear to see in the noise","dbf891e9":"> ### Enlightened by the recent EDA notebooks, here are some extended exlorarations, includeing\uff1a\n> 1. Linear relation between singal and open_channel by fitting signal distributions (one slop, two intercepts amoung ten batches)\n> 2. Pure noise is obtained by Open_channel - (signal-intercept)\/slop\n> 3. FFT and spectrogram of noise: strong 50Hz and weak 350Hz signal, power varies over time\n> 4. F1 score 0.922 for train data, predict just by rounding\/OptimizedRounder (signal-intercept)\/slop\n\n\n> ### Happy kaggling: learn, practise and creat!!!\n\n### Acknowledges:\n\ndata_without_drift, clean_kalman\n\nhttps:\/\/www.kaggle.com\/cdeotte\/one-feature-model-0-930\n\nhttps:\/\/www.kaggle.com\/siavrez\/simple-eda-model\n\nhttps:\/\/www.kaggle.com\/sirishks\/0-918-only-signal-no-model\n\nhttps:\/\/www.kaggle.com\/kakoimasataka\/remove-pick-up-electric-noise\n\nhttps:\/\/www.kaggle.com\/code1110\/visualizing-added-noise-by-spectrogram","c03eb0d1":"## Optimize the rounder\nhttps:\/\/www.kaggle.com\/teejmahal20\/physically-possible-optimized-rounder\n\nshows bias to the dominante classes","a7e76b95":"# ACF of noise, channels, sginal","24fdde84":"# Compare (signal-b)\/a, (signal_Kalman-b)\/a with Open_channels","799dc0d0":"# y_pred = round((signal-b)*a) for the train data","30dde2cd":"Reflections:\n1. There is a linear relation between signal and open_channel, the slop is nearly the same for all batches, but the intercepts are different: -2.73, -5.46 (bactches 5 and 10)\n2. The sigal has a normal distribution for different channels with increased wdith for batchs with more open_channels","e109f744":"# FFT and spectrogram of the noise\n\nAcknowledgements:\n\nhttps:\/\/www.kaggle.com\/kakoimasataka\/remove-pick-up-electric-noise\n\nhttps:\/\/www.kaggle.com\/code1110\/visualizing-added-noise-by-spectrogram","461c9907":"### Distribution of of open_channels over time"}}