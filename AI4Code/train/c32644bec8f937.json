{"cell_type":{"c2391af2":"code","d06a96cb":"code","c9e27c3e":"code","00f6d883":"code","6c8e3f8d":"code","073d5277":"code","6806b807":"code","a0a8705a":"code","7dde0044":"code","0983fd14":"code","b537e106":"code","bf633e7f":"code","5bbb2eac":"code","2d832e0b":"code","2a7342a8":"code","54add4e8":"code","bfdabdd5":"code","73cda7f7":"code","8387f3dc":"code","a6ee1223":"code","5586b87f":"code","f576ff13":"code","5da10972":"code","82431b76":"code","00a92d59":"code","87c6fd61":"code","764afb57":"code","9262823b":"code","4dd27565":"code","7d24420a":"code","e459e348":"code","04fdca38":"code","e07e99ca":"code","3467f559":"code","26e7fa11":"code","7e5fd5ad":"code","a78ea70b":"code","ead6e869":"code","e509382b":"code","f4285147":"code","1585a9cf":"code","af616fde":"code","1f1e2668":"code","ece20434":"code","8ce56c1b":"code","ee046039":"code","dd4f385d":"code","d546b270":"code","0794fc40":"code","fad49153":"code","84de2887":"code","678aaca7":"code","382b5ea3":"code","649c6905":"code","fc7aa901":"code","b9223457":"code","dc169c93":"code","615f6dc8":"code","3a9d13d9":"markdown","efd93f56":"markdown","f7fbdba9":"markdown","7756d457":"markdown","e3cd9df3":"markdown","d8dd085f":"markdown","99c8944c":"markdown","a0861850":"markdown","8b8a89f4":"markdown","19520795":"markdown","32185b4b":"markdown","f1b6cf21":"markdown","7f935d1d":"markdown","61b3a7d7":"markdown","a2562d2e":"markdown","9b7cfa96":"markdown","e2ccd5e5":"markdown","c00cd27f":"markdown","0c2a31d0":"markdown","4fae45bb":"markdown","9a28482d":"markdown","37cbdc43":"markdown"},"source":{"c2391af2":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","d06a96cb":"#This Librarys is to work with matrics\nimport pandas as pd\n#This Librarys is to work with vectors\nimport numpy as np\n#This Librarys is to work with visualization\nimport seaborn as sns\n#to render the graphics\nimport matplotlib.pyplot as plt\n#import module to set some plotting parameters\nfrom matplotlib import rcParams\n#Library to work with Regular Expression\nimport re\n#This Function make plot to show directly in browser\n%matplotlib inline\n#Setting parameters for the plot\nrcParams['figure.figsize'] = 10,8","c9e27c3e":"df_train = pd.read_csv('..\/input\/titanic\/train.csv')\ndf_test = pd.read_csv('..\/input\/titanic\/test.csv')\n\nprint(df_train.info())\nprint(df_test.info())","00f6d883":"df_train.describe()","6c8e3f8d":"df_train.head()","073d5277":"df_train[\"Name\"].head(3)","6806b807":"#GettingLooking of Prefix of all passenger\ndf_train[\"Title\"] = df_train.Name.apply(lambda x: re.search('([A-Z][a-z]+)\\.',x).group(1))\n\nplt.figure(figsize=(12,5))\nsns.countplot(x='Title', data=df_train)\nplt.xlabel(\"Title\", fontsize=16)\nplt.ylabel(\"Count\", fontsize=16)\nplt.title(\"Title Name Count\", fontsize=20)\nplt.xticks(rotation=45)\nplt.show()","a0a8705a":"#Doing the same on df_test with regular expressions\ndf_test[\"Title\"] = df_test.Name.apply(lambda x: re.search('([A-Z][a-z]+)\\.',x).group(1))","7dde0044":"#Now, I will identify the social status of each title\nTitle_Dictionary = {\n        \"Capt\":       \"Officer\",\n        \"Col\":        \"Officer\",\n        \"Major\":      \"Officer\",\n        \"Dr\":         \"Officer\",\n        \"Rev\":        \"Officer\",\n        \"Jonkheer\":   \"Royalty\",\n        \"Don\":        \"Royalty\",\n        \"Sir\" :       \"Royalty\",\n        \"the Countess\":\"Royalty\",\n        \"Dona\":       \"Royalty\",\n        \"Lady\" :      \"Royalty\",\n        \"Mme\":        \"Mrs\",\n        \"Ms\":         \"Mrs\",\n        \"Mrs\" :       \"Mrs\",\n        \"Mlle\":       \"Miss\",\n        \"Miss\":       \"Miss\",\n        \"Mr\":         \"Mr\",\n        \"Master\":     \"Master\"\n                }\n\n# we map each title to correct category\ndf_train['Title'] = df_train.Title.map(Title_Dictionary)\ndf_test['Title'] = df_test.Title.map(Title_Dictionary)","0983fd14":"print(\"Chances to survive based on titles: \")\nprint(df_train.groupby(\"Title\")[\"Survived\"].mean())","b537e106":"plt.figure(figsize=(12,5))\n\nsns.countplot(x=\"Title\", data=df_train, hue=\"Survived\")\nplt.xlabel(\"Titles\", fontsize=16)\nplt.ylabel(\"Count\", fontsize=16)\nplt.title(\"Title Grouped Count\", fontsize=20)\nplt.xticks(rotation=45)\nplt.show()","bf633e7f":"age_high_zero_died = df_train[(df_train[\"Age\"]> 0)& (df_train[\"Survived\"]==0)]\nage_high_zero_surv = df_train[(df_train[\"Age\"]> 0)& (df_train[\"Survived\"]==1)]\n\nplt.figure(figsize=(10,5))\nsns.distplot(age_high_zero_surv[\"Age\"], bins=24, color='g', kde=False)\nsns.distplot(age_high_zero_died[\"Age\"], bins=24, color='r', kde=False)\nplt.title(\"Distribution and density by Age\", fontsize=20)\nplt.xlabel(\"Age\", fontsize=15)\nplt.ylabel(\"Distribution Died and Survived\", fontsize=15)\nplt.show()","5bbb2eac":"age_group = df_train.groupby([\"Sex\",\"Pclass\",\"Title\"])[\"Age\"]\nprint(age_group.median())","2d832e0b":"# printing the total of nulls in Age Feature\ndf_train[\"Age\"].isnull().sum()","2a7342a8":"# using the groupby to transform this variables\ndf_train.loc[df_train.Age.isnull(), 'Age'] = df_train.groupby(['Sex','Pclass','Title']).Age.transform('median')","54add4e8":"df_train[\"Age\"].isnull().sum()","bfdabdd5":"# Let's see the results of imputation\n\nplt.figure(figsize=(12,5))\n\nsns.distplot(df_train[\"Age\"], bins=24, kde=False)\nplt.xlabel(\"Age\")\nplt.ylabel(\"No. of passengers\")\nplt.title(\"Distribution and density of Age:\")\nplt.show()","73cda7f7":"plt.figure(figsize=(12,5))\ng = sns.FacetGrid(df_train, col=\"Survived\", size=5)\ng = g.map(sns.distplot, \"Age\", kde=False)\nplt.show()","8387f3dc":"# Using categorical Variable for different Age's\ninterval = (0, 5, 12, 18, 25, 35, 60, 120)\n\ncats = ['babies', 'Children', 'Teen', 'Student', 'Young', 'Adult','Senior']\n\ndf_train[\"Age_cat\"] = pd.cut(df_train.Age, interval,labels=cats)\n\ndf_train[\"Age_cat\"].head()","a6ee1223":"#Do the same to test dataset\ninterval = (0, 5, 12, 18, 25, 35, 60, 120)\n\ncats = ['babies', 'Children', 'Teen', 'Student', 'Young', 'Adult','Senior']\n\ndf_test[\"Age_cat\"] = pd.cut(df_test.Age, interval,labels=cats)\n\ndf_test[\"Age_cat\"].head()","5586b87f":"# Describe of categorical Age\n# Using pd.crosstab to understand the Survived rate by Age category's\n\nprint(pd.crosstab(df_train.Age_cat, df_train.Survived))\n\n# Setting the figure Size\nplt.figure(figsize=(12, 10))\n\n# plotting the result\nplt.subplot(2,1,1)\nsns.countplot(\"Age_cat\", data=df_train, hue=\"Survived\", palette=\"hls\")\nplt.xlabel(\"Age Category's\", fontsize=18)\nplt.ylabel(\"Count\", fontsize=18)\nplt.title(\"Age Distribution\", fontsize=20)\n\nplt.subplot(2,1,2)\nsns.swarmplot(x='Age_cat', y=\"Fare\", data=df_train, hue=\"Survived\", palette=\"hls\")\nplt.xlabel(\"Age Category's\", fontsize=18)\nplt.ylabel(\"Fare Distribution\", fontsize=18)\nplt.title(\"Fare Distribution by Age Category's\", fontsize=20)\nplt.subplots_adjust(hspace=0.5, top=0.9)\nplt.show()","f576ff13":"# Setting the figure size\nplt.figure(figsize=(12,5))\n\n# Understanding the Fare Distribution\nsns.distplot(df_train[df_train.Survived == 0][\"Fare\"],bins=50, color='r', kde=False)\nsns.distplot(df_train[df_train.Survived == 1][\"Fare\"],bins=50, color='g', kde=False)\nplt.title(\"Fare Distribution by Survived\", fontsize=20)\nplt.xlabel(\"Fare\", fontsize=18)\nplt.ylabel(\"Density\", fontsize=18)\nplt.show()","5da10972":"df_train.Fare = df_train.Fare.fillna(-0.5)\n\n#intervals to categorize\nquant = (-1, 0, 8, 15, 31, 600)\n\n#Labels without input values\nlabel_quants = ['NoInf', 'quart_1', 'quart_2', 'quart_3', 'quart_4']\n\n#doing the cut in fare and puting in a new column\ndf_train[\"Fare_cat\"] = pd.cut(df_train.Fare, quant, labels=label_quants)\n\n#Description of transformation\nprint(pd.crosstab(df_train.Fare_cat, df_train.Survived))\n\nplt.figure(figsize=(12,5))\n\n#Plotting the new feature\nsns.countplot(x=\"Fare_cat\", hue=\"Survived\", data=df_train, palette=\"hls\")\nplt.title(\"Count of survived x Fare expending\",fontsize=20)\nplt.xlabel(\"Fare Cat\",fontsize=15)\nplt.ylabel(\"Count\",fontsize=15)\nplt.show()","82431b76":"# Replicate the same to df_test\ndf_test.Fare = df_test.Fare.fillna(-0.5)\n\nquant = (-1, 0, 8, 15, 31, 1000)\nlabel_quants = ['NoInf', 'quart_1', 'quart_2', 'quart_3', 'quart_4']\n\ndf_test[\"Fare_cat\"] = pd.cut(df_test.Fare, quant, labels=label_quants)","00a92d59":"#Now lets drop the variable Fare, Age and ticket that is irrelevant now\ndel df_train[\"Name\"]\ndel df_train[\"Fare\"]\ndel df_train[\"Ticket\"]\ndel df_train[\"Age\"]\ndel df_train[\"Cabin\"]\n#same in df_test\ndel df_test[\"Fare\"]\ndel df_test[\"Ticket\"]\ndel df_test[\"Age\"]\ndel df_test[\"Cabin\"]\ndel df_test[\"Name\"]","87c6fd61":"df_train.head()","764afb57":"df_test.head()","9262823b":"# Let see how many people die or survived\nprint(\"Total of Survived or not: \")\nprint(df_train.groupby(\"Survived\")[\"PassengerId\"].count())\n\nplt.figure(figsize=(12,5))\nsns.countplot(x=\"Survived\", data=df_train, palette=\"hls\")\nplt.title('Total Distribuition by survived or not', fontsize=22)\nplt.xlabel('Target Distribuition', fontsize=18)\nplt.ylabel('Count', fontsize=18)\n\nplt.show()","4dd27565":"print(pd.crosstab(df_train.Survived, df_train.Sex))\nplt.figure(figsize=(12, 5))\nsns.countplot(x=\"Sex\", data=df_train, hue=\"Survived\", palette=\"hls\")\nplt.title('Sex Distribuition by survived or not', fontsize=20)\nplt.xlabel('Sex Distribuition',fontsize=17)\nplt.ylabel('Count', fontsize=17)\n\nplt.show()","7d24420a":"print(pd.crosstab(df_train.Pclass, df_train.Embarked))\nplt.figure(figsize=(12,5))\nsns.countplot(x=\"Embarked\", data=df_train, hue=\"Pclass\", palette=\"hls\")\nplt.title('Embarked x Pclass Count', fontsize=20)\nplt.xlabel('Embarked with Pclass', fontsize=17)\nplt.ylabel(\"Count\", fontsize=17)\nplt.show()","e459e348":"#lets input the NA's with highest frequency\ndf_train[\"Embarked\"] = df_train[\"Embarked\"].fillna('S')","04fdca38":"print(pd.crosstab(df_train.Survived, df_train.Embarked))\n\nplt.figure(figsize=(12,5))\n\nsns.countplot(x=\"Embarked\", data=df_train, hue=\"Survived\",palette=\"hls\")\nplt.title('Class Distribuition by survived or not',fontsize=20)\nplt.xlabel('Embarked',fontsize=17)\nplt.ylabel('Count', fontsize=17)\n\nplt.show()","e07e99ca":"# Exploring Survivors vs Pclass\nprint(pd.crosstab(df_train.Survived, df_train.Pclass))\n\nplt.figure(figsize=(12,5))\n\nsns.countplot(x=\"Pclass\", data=df_train, hue=\"Survived\",palette=\"hls\")\nplt.xlabel('PClass',fontsize=17)\nplt.ylabel('Count', fontsize=17)\nplt.title('Class Distribuition by Survived or not', fontsize=20)\n\nplt.show()","3467f559":"g = sns.factorplot(x=\"SibSp\",y=\"Survived\",data=df_train,\n                   kind=\"bar\", height = 5, aspect= 1.6, palette = \"hls\")\ng.set_ylabels(\"Probability(Survive)\", fontsize=15)\ng.set_xlabels(\"SibSp Number\", fontsize=15)\n\nplt.show()","26e7fa11":"# Explore Parch feature vs Survived\ng = sns.factorplot(x=\"Parch\", y=\"Survived\", data=df_train, kind=\"bar\", size=6, palette=\"hls\")\ng = g.set_ylabels(\"Survival Probability\")","7e5fd5ad":"#Create a new column and sum the Parch + SibSp + 1 that refers the people self\ndf_train[\"FSize\"] = df_train[\"Parch\"] + df_train[\"SibSp\"] + 1\n\ndf_test[\"FSize\"] = df_test[\"Parch\"] + df_test[\"SibSp\"] + 1","a78ea70b":"print(pd.crosstab(df_train.FSize, df_train.Survived))\nsns.factorplot(x=\"FSize\",y=\"Survived\", data=df_train, kind=\"bar\",size=6, aspect=1.6)\nplt.show()","ead6e869":"del df_train[\"SibSp\"]\ndel df_train[\"Parch\"]\n\ndel df_test[\"SibSp\"]\ndel df_test[\"Parch\"]","e509382b":"df_train.head()","f4285147":"df_test.head()","1585a9cf":"df_train = pd.get_dummies(df_train, columns=[\"Sex\",\"Embarked\",\"Age_cat\",\"Fare_cat\",\"Title\"],\\\n                          prefix=[\"Sex\",\"Emb\",\"Age\",\"Fare\",\"Prefix\"], drop_first=True)\n\ndf_test = pd.get_dummies(df_test, columns=[\"Sex\",\"Embarked\",\"Age_cat\",\"Fare_cat\",\"Title\"],\\\n                         prefix=[\"Sex\",\"Emb\",\"Age\",\"Fare\",\"Prefix\"], drop_first=True)","af616fde":"df_train.head()","1f1e2668":"plt.figure(figsize=(15,12))\nplt.title('Correlation of Features for train set')\nsns.heatmap(df_train.astype(float).corr(), vmax=1.0, annot=True)\nplt.show()","ece20434":"df_train.shape","8ce56c1b":"train = df_train.drop([\"Survived\",\"PassengerId\"],axis=1)\ntrain_ = df_train[\"Survived\"]\n\ntest_ = df_test.drop([\"PassengerId\"],axis=1)\n\nX_train = train.values\ny_train = train_.values\n\nX_test = test_.values\nX_test = X_test.astype(np.float64, copy=False)","ee046039":"from sklearn.preprocessing import StandardScaler\nsc = StandardScaler()\nX_train = sc.fit_transform(X_train)\nX_test = sc.fit_transform(X_test)","dd4f385d":"from keras.models import Sequential\nfrom keras.layers import Dense, Activation, Dropout\nimport keras\nfrom keras.optimizers import SGD\nimport graphviz","d546b270":"# Creating the model\nmodel = Sequential()\n# Inputing the first layer input dimensions\nmodel.add(Dense(18, activation='relu', input_dim=20,\n               kernel_initializer='uniform'))\n# The argument being passed to each Dense layer(18) is the number of hidden units of the layer\n# A hidden unit is a dimensiion in the representatiion space of fhe layer\n# Stacks of Dense layers with relu activations can solve a wide range of problems\n#(including sentiment classification), and you'll likely use tehm frequently\n\n# Addding an Dropout Layer toprevine from overfitting\nmodel.add(Dropout(0.50))\n\n#Adding second hidden layer\nmodel.add(Dense(60, kernel_initializer='uniform',activation='relu'))\n\n# Adding another Dropout layer\nmodel.add(Dropout(0.50))\n\n#Adding the output layer that is binary [0,1]\nmodel.add(Dense(1, kernel_initializer='uniform', activation='sigmoid'))\n\n# visualize the model\nmodel.summary()\n\n","0794fc40":"#Creating an stochastic Gradient Descent\nsgd = SGD(lr=0.01, momentum=.09)\n\n# Compiling our model\nmodel.compile(optimizer=sgd, loss='binary_crossentropy',\n             metrics = ['accuracy'])\n\n#optimizers list\n#optimizers['SGD', 'RMSprop', 'Adagrad', 'Adadelta', 'Adam', 'Adamax', 'Nadam']\n\n# fitting the ANN to the training set\nmodel.fit(X_train, y_train, batch_size=60,epochs=30,verbose=2)","fad49153":"y_preds = model.predict(X_test)\n\nsubmission = pd.read_csv(\"..\/input\/titanic\/gender_submission.csv\", index_col='PassengerId')\nsubmission['Survived'] = y_preds.astype(int)\nsubmission.to_csv('output.csv')","84de2887":"scores = model.evaluate(X_train, y_train, batch_size=30)\nprint(\"%s: %.2f%%\" % (model.metrics_names[1], scores[1]*100))","678aaca7":"history = model.fit(X_train, y_train, validation_split=0.20, epochs=180, batch_size=10, verbose=0)\nprint(history.history.keys())","382b5ea3":"plt.plot(history.history['accuracy'])\nplt.plot(history.history['val_accuracy'])\nplt.title('Model Accuracy')\nplt.ylabel('Accuracy')\nplt.xlabel('Epoch')\nplt.legend(['train', 'test'], loc='upper left')\nplt.show()","649c6905":"plt.plot(history.history['loss'])\nplt.plot(history.history['val_loss'])\nplt.title('model loss')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.legend(['train', 'test'], loc='upper left')\nplt.show()","fc7aa901":"y_preds = model.predict(X_test)","b9223457":"# Trying to implementing the TensorBoard to evaluate the model\n\ncallbacks = [\n    keras.callbacks.TensorBoard(log_dir='my_log_dir',\n                                histogram_freq=1,\n                                embeddings_freq=1,\n                               )\n]","dc169c93":"# Importing the auxiliar and preprocessing librarys\nfrom sklearn.metrics import accuracy_score, confusion_matrix, classification_report\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.preprocessing import LabelEncoder, StandardScaler\nfrom sklearn.pipeline import Pipeline\n\nfrom sklearn.model_selection import train_test_split, KFold, cross_validate\nfrom sklearn.metrics import accuracy_score\n#Models \nfrom sklearn.cluster import KMeans\nfrom sklearn.decomposition import PCA\nfrom sklearn.svm import SVC\nfrom sklearn.linear_model import RidgeClassifier, SGDClassifier,LogisticRegression\nfrom sklearn.svm import SVC, LinearSVC\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom xgboost import XGBClassifier\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.feature_selection import SelectFromModel\nfrom sklearn.ensemble import RandomForestClassifier,GradientBoostingClassifier, ExtraTreesClassifier, BaggingClassifier, VotingClassifier, RandomTreesEmbedding","615f6dc8":"clfs = []\nseed = 3\nclfs.append((\"LogReg\", Pipeline([(\"Scaler\", StandardScaler()),(\"LogReg\", LogisticRegression())])))\n\nclfs.append((\"XGBClassifier\", Pipeline([(\"Scaler\", StandardScaler()),(\"XGB\", XGBClassifier())])))\n\nclfs.append((\"KNN\", Pipeline([(\"Scaler\", StandardScaler()),(\"KNN\", KNeighborsClassifier())])))\n\nclfs.append((\"DecisionTreeClassifier\", Pipeline([(\"Scaler\", StandardScaler()),(\"DecisionTrees\", DecisionTreeClassifier())])))\n\nclfs.append((\"RandomForestClassifier\", Pipeline([(\"Scaler\", StandardScaler()),(\"RandomForest\", RandomForestClassifier())])))\n\nclfs.append((\"GradientBoostingClassifier\", \n             Pipeline([(\"Scaler\", StandardScaler()),\n                       (\"GradientBoosting\", GradientBoostingClassifier(max_features=15, n_estimators=150))]))) \n\nclfs.append((\"RidgeClassifier\", \n             Pipeline([(\"Scaler\", StandardScaler()),\n                       (\"RidgeClassifier\", RidgeClassifier())])))\n\nclfs.append((\"BaggingRidgeClassifier\",\n             Pipeline([(\"Scaler\", StandardScaler()),\n                       (\"BaggingClassifier\", BaggingClassifier())])))\n\nclfs.append((\"ExtraTreesClassifier\",\n             Pipeline([(\"Scaler\", StandardScaler()),\n                       (\"ExtraTrees\", ExtraTreesClassifier())])))\n\n#'neg_mean_absolute_error', 'neg_mean_squared_error','r2'\nscoring = 'accuracy'\nn_folds = 10\n\nresults, names  = [], [] \n\nfor name, model  in clfs:\n    kfold = KFold(n_splits=n_folds, random_state=seed)\n    cv_results = cross_val_score(model, X_train, y_train, cv= 5, scoring=scoring, n_jobs=-1)    \n    names.append(name)\n    results.append(cv_results)    \n    msg = \"%s: %f (+\/- %f)\" % (name, cv_results.mean(), \ncv_results.std())\n    print(msg)\n    \n# boxplot algorithm comparison\nfig = plt.figure(figsize=(15,6))\nfig.suptitle('Classifier Algorithm Comparison', fontsize=22)\nax = fig.add_subplot(111)\nsns.boxplot(x=names, y=results)\nax.set_xticklabels(names)\nax.set_xlabel(\"Algorithmn\", fontsize=20)\nax.set_ylabel(\"Accuracy of Models\", fontsize=18)\nax.set_xticklabels(ax.get_xticklabels(),rotation=45)\nplt.show()","3a9d13d9":"### Looking the graphs, is clear that 3st class and Embarked at Southampton have a high probabilities to not survive","efd93f56":"### I will create a categorical variable to treat the Fare expend","f7fbdba9":"## Evaluating the model","7756d457":"**Because you\u2019re facing a binary classification problem and the output of your network is a probability (you end your network with a single-unit layer with a sigmoid activation), it\u2019s best to use the binary_crossentropy loss.**","e3cd9df3":"## It's my first deep learning implementation.. I am studying about this and i will continue editing this kernel to improve the results","d8dd085f":"## Exploring the data","99c8944c":"Now let categorize them","a0861850":"## Modelling","8b8a89f4":"### Lets Cross our Pclass with the Age_cats","19520795":"## Preprocessing ","32185b4b":"## Now I will handle the Age variable that has a high number of NaN's, using some columns to correctly input he missing Age's","f1b6cf21":"**Titanic survivors prediction:\na binary classification example\nTwo-class classification, or binary classification, may be the most widely applied kind of machine-learning problem.**","7f935d1d":"## Title grouped","61b3a7d7":"## We can look that % dies to mens are much higher than female","a2562d2e":"Stacks of Dense layers with relu activations can solve a wide range of problems(including sentiment classification) and you'll likely use them frequently.","9b7cfa96":"## Import the Library's","e2ccd5e5":"## Validation","c00cd27f":"## Now, lets do some exploration in Pclass and Embarked to see if might have some information to build the model","0c2a31d0":"## Anatomy of a neural network:\nAs you saw in the previous chapters, training a neural network revolves around the following objects:\n\n* Layers, which are combined into a network (or model)\n* The input data and corresponding targets\n* The loss function, which defines the feedback signal used for learning\n* The optimizer, which determines how learning proceeds","4fae45bb":"### To complete this part, I will now work on \"Names\"","9a28482d":"## Predicting X_test","37cbdc43":"## Look at the data"}}