{"cell_type":{"ce52eb8a":"code","7b3177b2":"code","ece33d2a":"code","7d1052b3":"code","d6c25d5e":"code","d27b57dd":"code","89436bf4":"code","8af50047":"code","33135f8f":"code","7da5b933":"code","bf1ce401":"code","2266e08e":"code","475dca71":"code","ea1bc605":"code","e016cd9a":"code","a532fbc3":"code","1f4627f4":"code","aa641230":"code","6ffaf423":"code","9db23b83":"code","1c4c7475":"code","0a010ad2":"code","5a235f3f":"code","200e2b01":"code","8b0294ba":"code","952a2a6a":"code","109a4f88":"code","69245b77":"code","9555c7a6":"code","3b349321":"code","24fba678":"code","6c2f2aa4":"code","4f1365e1":"code","539490ae":"code","9d0116b9":"code","1bf2947e":"code","c580cc34":"code","2ff4019c":"code","aa5d36dc":"code","01265d4e":"code","8ef61ee3":"code","415b4a98":"code","a70dd678":"code","59377b3e":"code","cab1eb7b":"code","4b8b8a57":"code","4ca6e4b9":"code","ad74e0ba":"code","432949c1":"code","97f5ddc3":"code","7fcd773b":"code","e9447555":"code","0eadfa8a":"code","afcfb5fa":"code","a67f5b44":"code","6a588fc4":"code","e8e3da36":"code","852e9836":"code","96a4eb68":"code","b856ca03":"code","4c3180b6":"code","b9767560":"code","303d7953":"markdown","0de244c8":"markdown","d4713b5f":"markdown","be7d413f":"markdown","cd9abe5b":"markdown","35d46e66":"markdown","36a9c4d9":"markdown","9f034b88":"markdown","896b47ed":"markdown","7d56066c":"markdown","804dd37a":"markdown","f10959a2":"markdown","b39460ef":"markdown","57fb17e8":"markdown","748793b1":"markdown","e6fbc3ef":"markdown","00df2136":"markdown","7560e5e2":"markdown","99c09151":"markdown","ac7159f7":"markdown","214b6f43":"markdown","56773f02":"markdown","4db744c7":"markdown","26fdd1fd":"markdown","83133faf":"markdown","fd8bfb03":"markdown","002def75":"markdown","dc86e24a":"markdown","6efd4d97":"markdown","f19a3f40":"markdown","9fb0a141":"markdown","358474a9":"markdown","fae0b0a2":"markdown","f64c0686":"markdown","5b12b92e":"markdown","040f7146":"markdown","26a37792":"markdown","9e086904":"markdown","80006079":"markdown","a1ead9d7":"markdown","eb6e9160":"markdown","eadbfb48":"markdown","5e7d8785":"markdown","df15a0b9":"markdown","e73bc02e":"markdown","ea965510":"markdown","29323012":"markdown","b125ae9f":"markdown","ea4cdf2a":"markdown","c35b2184":"markdown","c6e4d4ae":"markdown","1a1a4b68":"markdown","94d62343":"markdown","1ffda0d1":"markdown","49c126d7":"markdown","d8f76b29":"markdown","877be446":"markdown","f7142954":"markdown","dc1c33ff":"markdown","2cf2801a":"markdown","33fa7378":"markdown","d404e87d":"markdown","704f3733":"markdown","88a342fd":"markdown","7d435c67":"markdown","68f0eed5":"markdown","aef99e5b":"markdown","dc80adb0":"markdown","2ab44b43":"markdown","79e02806":"markdown","175fa54e":"markdown","ebbd9f5d":"markdown","2b9f5119":"markdown","393bde78":"markdown","9203a348":"markdown","abe57fb9":"markdown","552374cf":"markdown","d13610c2":"markdown","cc0aded8":"markdown","845b2c40":"markdown","8a16f55d":"markdown","36cec3ed":"markdown","239819bb":"markdown","7aa8de21":"markdown","7680d3dd":"markdown","cdfe6788":"markdown","9436a836":"markdown","a6265cee":"markdown"},"source":{"ce52eb8a":"# Third party\nfrom sklearn.dummy import DummyClassifier\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import KBinsDiscretizer\nfrom sklearn.tree import DecisionTreeClassifier\n\nimport md_grupoS_practica1_utils as utils","7b3177b2":"seed = 562","ece33d2a":"filepath = \"..\/input\/pima-indians-diabetes-database\/diabetes.csv\"\n# index = \"Id\"\ntarget = \"Outcome\"\npima_diabetes = utils.load_data(filepath,target, \"\")\npima_diabetes.shape","7d1052b3":"pima_diabetes.sample(5, random_state=seed)","d6c25d5e":"(X,y) = utils.divide_dataset(pima_diabetes, target = \"Outcome\")","d27b57dd":"X.sample(5, random_state=seed)","89436bf4":"y.sample(5, random_state=seed)","8af50047":"train_size = 0.7  #70% entrenamiento y 30% test\n(X_train, X_test, y_train, y_test) = train_test_split(X, y,\n                                                      stratify=y,\n                                                      random_state=seed,\n                                                      train_size=train_size)","33135f8f":"X_train.sample(5, random_state=seed)","7da5b933":"X_test.sample(5, random_state=seed)","bf1ce401":"y_train.sample(5, random_state=seed)","2266e08e":"y_test.sample(5, random_state=seed)","475dca71":"data_train = utils.join_dataset(X_train, y_train)\ndata_test = utils.join_dataset(X_test, y_test)","ea1bc605":"data_train.sample(5, random_state=seed)","e016cd9a":"data_test.sample(5, random_state=seed)","a532fbc3":"data_train.shape","1f4627f4":"data_test.shape","aa641230":"data_train.info(memory_usage=False)","6ffaf423":"y_train.cat.categories\n","9db23b83":"data_train.describe()","1c4c7475":"utils.plot_conditional_histogram(data_train, \"Outcome\")","0a010ad2":"utils.missing_values(data_train,[\"Glucose\",\"BloodPressure\", \"SkinThickness\",\"Insulin\",\"BMI\"])","5a235f3f":"utils.plot_heatmap(data_train.corr(), data_train.columns[:-2])","200e2b01":"from sklearn.compose import make_column_transformer\nfrom sklearn.impute import SimpleImputer\n\ncolumn_transformer = make_column_transformer((\"drop\",[\"SkinThickness\",\"Insulin\"]),(SimpleImputer(missing_values = 0, strategy=\"mean\"), [\"Glucose\",\"BloodPressure\",\"BMI\"]), remainder=\"passthrough\")","8b0294ba":"zero_r_model = DummyClassifier(strategy=\"most_frequent\")","952a2a6a":"tree_model = DecisionTreeClassifier(random_state=seed)","109a4f88":"pipeline = make_pipeline(column_transformer, tree_model)","69245b77":"utils.evaluate(zero_r_model, X_train, X_test, y_train, y_test)","9555c7a6":"utils.evaluate(pipeline, X_train, X_test, y_train, y_test)","3b349321":"from sklearn.model_selection import train_test_split\nfrom sklearn.dummy import DummyClassifier\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import KBinsDiscretizer\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.tree import DecisionTreeClassifier\n\n# Aplicaci\u00f3n local\nimport md_grupoS_practica1_utils as utils","24fba678":"seed = 1","6c2f2aa4":"# filepath = \"..\/input\/wisconsin.csv\"\nindex = \"id\"\ntarget = \"diagnosis\"\n\ndata = utils.load_data(\"..\/input\/breast-cancer-wisconsin-data\/data.csv\", target, index)","4f1365e1":"data.sample(5, random_state=seed)","539490ae":"del data[\"Unnamed: 32\"]\ndata.sample(5, random_state=seed)","9d0116b9":"(X,y) = utils.divide_dataset(data, target = target)","1bf2947e":"X.sample(5, random_state=seed)","c580cc34":"y.sample(5, random_state=seed)","2ff4019c":"train_size = 0.7\n(X_train, X_test, y_train, y_test) = train_test_split(X, y,\n                                                      stratify=y,\n                                                      random_state=seed,\n                                                      train_size=train_size)","aa5d36dc":"data_train = utils.join_dataset(X_train, y_train)\ndata_test = utils.join_dataset(X_test, y_test)","01265d4e":"data_train.sample(5, random_state=seed)","8ef61ee3":"data_test.sample(5, random_state=seed)","415b4a98":"data.shape","a70dd678":"data_train.shape","59377b3e":"data_test.shape","cab1eb7b":"data_train.shape","4b8b8a57":"data_train.info(memory_usage=False)","4ca6e4b9":"y_train.cat.categories","ad74e0ba":"data_train.describe(include=\"number\")","432949c1":"data_train.describe(include=\"category\")","97f5ddc3":"utils.plot_conditional_histogram(data_train, \"diagnosis\")","7fcd773b":"utils.plot_heatmap(X_train.corr(), X_train.iloc[:,:10].columns)","e9447555":"utils.plot_heatmap(X_train.corr(), X_train.iloc[:,10:20].columns)","0eadfa8a":"utils.plot_heatmap(X_train.corr(), X_train.iloc[:,20:30].columns)","afcfb5fa":"from sklearn.compose import make_column_transformer\n\ncolumn_transformer = make_column_transformer((\"drop\",[\"area_mean\", \"perimeter_mean\", \"area_se\", \"perimeter_se\", \n                                                      \"area_worst\", \"perimeter_worst\", \"concavity_mean\", \"compactness_mean\", \n                                                      \"concavity_se\", \"compactness_se\", \"concavity_worst\", \"compactness_worst\"]),\n                                             remainder=\"passthrough\")","a67f5b44":"column_transformer_discretized = make_column_transformer((\"drop\",[\"area_mean\", \"perimeter_mean\", \"area_se\", \"perimeter_se\", \n                                                      \"area_worst\", \"perimeter_worst\", \"concavity_mean\", \"compactness_mean\", \n                                                      \"concavity_se\", \"compactness_se\", \"concavity_worst\", \"compactness_worst\"]),\n                                             ( KBinsDiscretizer(n_bins=2, strategy=\"uniform\"), [\"radius_mean\",\"radius_worst\",\n                                                        \"radius_se\", \"concave points_mean\",\"concave points_worst\",\"concave points_se\"]),\n                                             remainder=\"passthrough\")","6a588fc4":"zero_r_model = DummyClassifier(strategy=\"most_frequent\")","e8e3da36":"tree_model = DecisionTreeClassifier(random_state=seed)","852e9836":"no_columns = make_pipeline(column_transformer, tree_model)","96a4eb68":"discretize_tree_model = make_pipeline(column_transformer_discretized, tree_model)","b856ca03":"utils.evaluate(zero_r_model,\n               X_train, X_test,\n               y_train, y_test)","4c3180b6":"utils.evaluate(no_columns,\n               X_train, X_test,\n               y_train, y_test)","b9767560":"utils.evaluate(discretize_tree_model,\n               X_train, X_test,\n               y_train, y_test)","303d7953":"Comprobamos como siempre que todo se ha realizado correctamente.","0de244c8":"Viendo este mapa de correlaci\u00f3n podemos confirmamos que `perimeter_mean` y `area_mean` dependen linealmente de `radius_mean`.\n\nLo mismo pasa con `concavity_mean`, `compactness_mean` y `concave points_mean` siento esta \u00faltima la que tiene m\u00e1s correlaci\u00f3n con otras variables.","d4713b5f":"Construimos el modelo de \u00e1rbol y metemos todo al pipeline que se encargar\u00e1 de ejecutar los pasos que le asignemos.","be7d413f":"Por lo general los datos de cada variable no son muy dispares, siendo las desviaciones est\u00e1ndar bajas salvo en el caso de las **areas** que suele ser bastante alta. Algo que entendemos dado los valores m\u00ednimos y m\u00e1ximos de estas. ","cd9abe5b":"Y despu\u00e9s el conjunto de datos de prueba:","35d46e66":"#### Visualizaci\u00f3n de las variables\n\nComo el an\u00e1lisis num\u00e9rico es complicado, la manera m\u00e1s simple de sacar conclusiones r\u00e1pidas acerca de los datos es mediante representaciones gr\u00e1ficas de estos. \n\nAl tener relativamente pocas variables, usaremos m\u00e9todos **univariados** para visualizarlos.","36a9c4d9":"### 3. An\u00e1lisis exploratorio de datos","9f034b88":"Vuelva a ocurrir lo mismo que antes, pero esta vez con los valores del error est\u00e1ndar. Algo que por intuici\u00f3n pod\u00edamos deducir, ya que el error est\u00e1ndar proviene de las medias de las variables.","896b47ed":"Pasemos ahora a la visualizaciones gr\u00e1ficas que son m\u00e1s sencillas de entender que solo mirando n\u00fameros. Para ello empezamos con los histogramas condicionados de cada variable.","7d56066c":"### 2. Acceso y almacenamiento de datos","804dd37a":"#### Descripci\u00f3n del conjunto de datos\n\nVeamos el tipo de variables que tenemos que manejar. Esto nos dar\u00e1 pistas posteriormente sobre qu\u00e9 m\u00e9todos podemos usar para su debido procesamiento.\n\nAntes hemos revisado que hay un total de `768` casos y unas `9` variables, de las cuales `8` son predictores y `1` de clase. Pasemos a comprobar el tipo de variables de las que se trata.","f10959a2":"Comprobamos si se ha realizado correctamente la funci\u00f3n antes de seguir.","b39460ef":"Con esto tendr\u00edamos los **datos crudos** preprocesados, en caso de necesitar otro tipo de preprocesamiento posteriormente tan solo habr\u00eda que a\u00f1adirlo al pipeline. ","57fb17e8":"### 1. Preliminares","748793b1":"Antes de realizar el preprocesamiento, observaremos las distintas variables que conforman la base de datos y las posibles relaciones entre ellas, mediante el apoyo de m\u00e9todos gr\u00e1ficos y estad\u00edsticos.","e6fbc3ef":"### Preprocesamiento de datos","00df2136":"### Evaluaci\u00f3n","7560e5e2":"Comenzamos con una descripci\u00f3n m\u00e1s detallada de cada variable:","99c09151":"### Evaluaci\u00f3n de los modelos","ac7159f7":"Con la funci\u00f3n `info` nos muestra las variables y de que tipo son:","214b6f43":"#### Descripci\u00f3n del conjunto de datos","56773f02":"Cargamos nuestro conjunto de datos, en este caso el de `wisconsin` especificando que variable corresponde al identificador de las instancias del conjunto de datos y cual corresponde a la variable clase.","4db744c7":"Pasamos a la fase mas importante del proceso. Para ello haremos uso del **pipeline** que se encargar\u00e1 de aplicar las transformaciones que creemos convenientes al conjunto de datos para despu\u00e9s poder aprender un modelo concreto a partir \u00e9l.\n\nLo primero de todo ser\u00e1 eliminar las variables **SkinThickness** e **Insulin** tal y como especificamos en la parte del an\u00e1lisis exploratorio. Para ello recurrimos a uno de los transformadores del paquete de `sci-kit`.","26fdd1fd":"Como podemos ver las 569 instancias del conjunto de datos original se han separado en:\n* 398 para el conjunto de datos de entrenamiento\n* 171 para el conjunto de datos de prueba","83133faf":"#### Preprocesamiento","fd8bfb03":"La gr\u00e1fica nos muestra el porcentaje de valores perdidos en las variables de las que hemos sospechado desde un principio. Tal y como se dijo en clase, en el prepocesamiento habr\u00e1 que eliminar a **SkinThickness** e **Insulin**, ya que poseen m\u00e1s de un 20% de valores perdidos. Por tanto estas variables no las consideraremos a la hora de aprender nuestros modelos.","002def75":"### \u00c1rbol de decisi\u00f3n","dc86e24a":"#### Visualizaci\u00f3n estad\u00edstica de los datos","6efd4d97":"Con la funci\u00f3n `describe(include=\"number\")` del paquete `Pandas` podemos analizar variables num\u00e9ricas.","f19a3f40":"Todo correcto, ya que la suma de ambas **instancias** es igual al n\u00famero total de instancias mostradas al cargar los datos por primera vez. Una vez listos los preliminares podemos proceder con el an\u00e1lisis exploratorio de la base de datos.","9fb0a141":"### Zero-R\n\nTal y como se especifico usaremos este algoritmo \"tonto\" como punto de partida (baseline) para poder medir la efectividad de nuestro clasificador","358474a9":"Esta base de datos est\u00e1 ideada en torno a variables m\u00e9dicas que se suelen utilizar para la detecci\u00f3n de c\u00e1ncer de mama, as\u00ed que el fin con el que se construy\u00f3 no es m\u00e1s que otro que intentar predecir su aparici\u00f3n de forma m\u00e1s precisa a trav\u00e9s de t\u00e9cnias de machine learning.","fae0b0a2":"Este apartado lo dividiremos en dos subpartes para poder entender con qu\u00e9 tipos de datos estamos tratando. Para ello analizaremos sus variables y la correlaci\u00f3n entre ellas.","f64c0686":"Podemos observar que todas son num\u00e9ricas, la mayor\u00eda de tipo entero y algunas de coma flotante. Esto nos da pistas de que posiblemente despu\u00e9s tendresea necesario discretizarlas para poder obtener informaci\u00f3n relevante.\n\nPor otro lado, las clases en las que se divide la variable objetivo son las siguientes:","5b12b92e":"Para ser un clasificador \"tonto\" ha conseguido un porcentaje de aciertos importante, aun as\u00ed no es suficiente, pero nos valdr\u00e1 como punto de partida. Por otro lado, hemos hecho uso del an\u00e1lisis ROC que nos indica claramente de que se trata de un mal clasificador al ser el mismo ejemplo visto en clase.","040f7146":"Entenderemos que `0` indica que **no** hay diabetes y un `1` a que **si**. ","26a37792":"Bien ahora que sabemos nuestras **variables**, es una buena pr\u00e1ctica separarlas en **predictoras** y **objetivo** para que su tratamiento sea m\u00e1s f\u00e1cil y c\u00f3modo. Por convenci\u00f3n, **X** son las predictoras e **y** las objetivo.","9e086904":"Como podemos observar en la muestra, tenemos una columna sin nombre (`Unnamed: 32`) que no tiene datos para niguna de las instancias, por lo tanto la eliminaremos puesto que no nos aporta nada.","80006079":"Antes de empezar con el an\u00e1lisis exploratorio separaremos nuestro conjunto de datos en conjunto de **entrenamiento** y  de **validaci\u00f3n** para evitar  un **sobre-ajuste** del modelo a los datos. A esto se le llama *holdout* y para realzarlo usaremos un m\u00e9todo del paquete de `sci-kit learn`.\n","a1ead9d7":"Viendo esta informaci\u00f3n podemos afirmar que las 30 variables predictoras son num\u00e9ricas (`float64`), y que la variable clase (`diagnosis`) es categ\u00f3rica (`category`). Otra de las cosas que nos percatamos es que en realidad se trata de 10 variables solamente, pero que se han convertido en 30 dado que son las diversas medidas tomadas de una misma: la media, el error est\u00e1ndar y por lo que hemos entendido en la descripci\u00f3n del conjunto de datos `worst` es la media de los tres valores m\u00e1s altos de cada variable.\n\nLos estados de la la variable clase son:","eb6e9160":"Fijamos una semilla para poder reproducir el experimento al igual que antes","eadbfb48":"Como dijimos anteriormente, en este conjunto de datos carece de sentido discretizar por lo que nos ahorramos dicha acci\u00f3n. Sin embargo, sigue habiendo un porcentaje de valores perdidos de algunas variables. En este caso lo que haremos es imputarlos (rellenar huecos). Tan solo lo aplicamos a las variables correspondientes, ya que si lo ejecutamos en todo el conjunto afectar\u00eda a otra variables en las que los valores nulos no son valores perdidos, digase el ejemplo de **Pregnancies**.","5e7d8785":"Hay dos clases y damos por entendido que **B** es benigno y **M** maligno.","df15a0b9":"Desde un principio hemos tenido dudas acerca de si llevar a cabo la discretizacion, puesto que desde el an\u00e1lisis exploratorio se intu\u00eda de que aplicarla ser\u00eda en vano por la distribuci\u00f3n superpuesta de las clases en todas las variables. Asi que, al f\u00edn nuestra hipotesis se cumple al ver que la precisi\u00f3n del clasificador disminuye respecto al modelo sin discretizar, generando m\u00e1s falsos positivos y negativos. Aun as\u00ed la precisi\u00f3n es bastante alta.","e73bc02e":"Volvemos a importar las librer\u00edas que nos har\u00e1n falta para \"reiniciar\" el entorno despu\u00e9s de la secci\u00f3n anterior.","ea965510":"### Algoritmos de clasificaci\u00f3n","29323012":"Analizando los datos extraidos de cada variable hay algo importante de lo que nos debemos percatar. Muchas variables tienen como valor m\u00ednimo el cero, algo que en la realidad es imposible dado el significado intrinseco de dichas variables m\u00e9dicas. Se trata de las siguientes caracter\u00edsitcas: **Glucose**, **BloodPressure**, **SkinThickness**, **Insulin** y **BMI**. Esto puede deberse a valores perdidos que luego se deber\u00e1n tratar. Por otro lado, tenemos constancia de que la desviaci\u00f3n media de **Insulin** es demasiado alta, lo que nos advierte de que hay mucho ruido en sus valores.","b125ae9f":"Veamos una muestra aleatoria del conjunto de datos para ver que todo est\u00e1 en orden. Al ser aleatoria conseguimos m\u00e1s significancia estad\u00edstica y evitamos las **muestras sesgadas** que juegan una mala pasada si queremos hacer alg\u00fan tipo de verificaci\u00f3n.","ea4cdf2a":"## Breast Cancer Wisconsin","c35b2184":"Hasta ahora hemos analizado las variables una por una, el siguiente paso es averiguar si hay alg\u00fan tipo de relaci\u00f3n entre ellas. Para ello nos valdremos de m\u00e9todos **multivariados** para determinar cuales son las m\u00e1s relevantes en la predicci\u00f3n de diabetes al contraponerlas unas con otras. En nuestro caso, al no ser muy expertos creemos que un `pairplot` con tantas variables no nos dejar\u00eda nada claro, por eso recurrimos a su versi\u00f3n simplificada: el mapa de calor  o `heatmap`.","c6e4d4ae":"Y por otro la variable clase","1a1a4b68":"Veamos ahora como se traducen estos n\u00fameros en histogramas y diagramas de barras. Para ellos nos valemos de m\u00e9todos que hemos construido con la librer\u00eda `plotly` en nuestro archivo de utilidades.","94d62343":"El gr\u00e1fico nos ayuda a ver la correlaci\u00f3n entre distintas variables, eso si, siempre es lineal con lo cual no nos dar\u00eda informaci\u00f3n si hay alg\u00fan otro tipo de dependencia entre ellas. A grosso modo se observa que las que mayor relaci\u00f3n tienen son el par **Insulin* y **SkinThickness**, junto con el par **BMI** y **SkinThickness**. Por el otro lado, las dem\u00e1s correlaciones son muy bajas o casi nulas con lo cual da a entender que a simple vista es dif\u00edcil percatarnos de cuales tienen un mayor poder predictivo.","1ffda0d1":"Como hemos podido ver, podemos diferenciar entre tres tipos de variables `mean`, `se` y `worst`, por lo tanto diseccionaremos nuestra base de datos para mostrar los mapas de calor ya que con treinta variables es dificil de visualizar si hay alguna correlaci\u00f3n entre ellas.","49c126d7":"Con esto vemos que de los 398 casos, 250 son del tipo B y 148 del tipo M, lo que ya nos advierte de que se trata de un problema **desbalanceado**. Lo cual requerir\u00e1 m\u00e9todos de evaluaci\u00f3n espec\u00edficos que comentaremos en la \u00faltima secci\u00f3n. ","d8f76b29":"Podr\u00edamos usar la funci\u00f3n `head` para mostrar las primeras **n instancias** del conjunto de datos, pero ser\u00eda una **muestra sesgada**. Para impedir que eso ocurra obtendremos una muestra aleatoria con la funci\u00f3n `sample`","877be446":"Hay un importante mejora en la precisi\u00f3n del clasificador al crear el modelo a partir de datos ya procesados. Lo cual nos muestra de nuevo la importancia de esta fase en el proceso que hemos llevado a cabo. Por otro lado, si nos fijamos en la matriz de confusi\u00f3n vemos que tenemos una tasa baja tanto de falsos positivos como negativo. El an\u00e1lisis ROC pasa con creces el punto de partida del que hemos empezado, esto se debe a lo que hemos dicho antes de las bajas tasa de falsos positivos y el incremento de los verdaderos positivos. Por tanto, hemso obtenido un muy buen clasificador.","f7142954":"Puesto que no tenemos un conjunto de datos de prueba y para no provocar un sobre-ajuste al usar el mismo conjunto de datos para entrenar y realizar la prueba, lo separaremos como m\u00ednimo en dos:\n* Una muestra de entrenamiento (70%)\n* Una muestra de prueba (30%)","dc1c33ff":"# Pr\u00e1ctica 1 Miner\u00eda de Datos\n# An\u00e1lisis exploratorio de datos, preprocesamiento y validaci\u00f3n de modelos de clasificaci\u00f3n\n\nRealizada por el **grupo S** formado por **Cristian Stanimirov Petrov** y **Nikola Svetlozarov Dyulgerov**\n\n","2cf2801a":"Otra de las transformaciones que haremos ser\u00e1 discretizar, aunque sea un poco contradictorio en base al an\u00e1lisis exploratorio, ya que la \u00fanica variables candidata para ser discretizada era el **radio** y tampoco estaba tan clara. De todas maneras, nuestro objetivo es ver si la hip\u00f3tesis formulada es correcta o no.","33fa7378":"### An\u00e1lisis exploratorio","d404e87d":"## Pima Indians Diabetes","704f3733":"Con la funci\u00f3n `describe(include=\"category\")` podemos analizar variables num\u00e9ricas.","88a342fd":"Lo mismo con las variables objetivo de entrenamiento y prueba:","7d435c67":"Esta base de datos ha sido construida con el objetivo de analizar una serie de **variables** para ver como est\u00e1n relacionadas con la aparici\u00f3n de diabetes. De esta manera lo que se pretende es poder predecir si un paciente tiene o no tiene dicha enfermedad en base a los factores descritos en el conjunto de datos.","68f0eed5":"#### Visualizaci\u00f3n gr\u00e1fica de los datos","aef99e5b":"Para poder realizar el an\u00e1lisis exploratorio de una manera m\u00e1s simple, volveremos a juntar las variables predictoras con la objetivo para ambos conjuntos: entrenamiento y test","dc80adb0":"Comprobamos ambos conjuntos","2ab44b43":"Pasamos a la fase mas importante del proceso. Para ello haremos uso del **pipeline** que se encargar\u00e1 de aplicar las transformaciones que creemos convenientes al conjunto de datos para despu\u00e9s poder aprender un modelo concreto a partir \u00e9l.\n\nLo primero de todo ser\u00e1 eliminar las variables , tal y como especificamos en la parte del an\u00e1lisis exploratorio. Eliminamos las derivadas del **radio** porque son las que dependen linealmente de este debido a la f\u00f3rmula matem\u00e1tica. Son las siguientes: **area_mean**,**perimeter_mean**,**area_se**,**perimeter_se**,**area_worst**,**perimeter_worst**. Mientras que por el otro lado dejamos **concave_points** eliminando las que son similares a el: **concavity_mean**,**compactness_mean**,**concavity_se**,**compactness_se**,**concavity_worst** y **compactness_worst**. Con dejar una nos vale, ya que si son muy parecidad no nos aportan nada nuevo. \n\nPara realizar este trabajo recurrimos a uno de los transformadores del paquete de sci-kit.","79e02806":"Lo primero de todo importamos las librer\u00edas comunes con las que trabajaremos a lo largo de la pr\u00e1ctica. Despu\u00e9s importaremos alguna m\u00e1s especifica para apartados concretos, pero de momento con estas es suficiente.\n\n**Fichero con nuestras funciones personalizadas**","175fa54e":"Hay un importante mejora en la precisi\u00f3n del clasificador al crear el modelo a partir de datos ya procesados. Lo cual nos muestra la importancia de esta fase en el proceso que hemos llevado a cabo. Por otro lado, si nos fijamos en la matriz de confusi\u00f3n vemos que las tasas de falsos positivos y negativo es bastante alta por lo que podemos seguir mejorando intentando rebajar dicha cifras. La curva del an\u00e1lisis ROC ha mejorado notablemente tambi\u00e9n al haber disminuido los falsos positivos y aunmentado los verdaderos positivos, aun as\u00ed sigue quedandose a medio camino de ser un muy buen clasificador. Como para esta pr\u00e1ctica no veremos m\u00e1s tipos de preprocesamiento, no podremos ver c\u00f3mo podr\u00eda merjorar todav\u00eda mas.","ebbd9f5d":"Vamos a separar nuestro conjunto de datos en dos subconjuntos porque es util tener por un lado las variables predictoras y por otro lado la variable objetivo.","2b9f5119":"Una vez m\u00e1s la similitud con los mapas de calor anteriores es evidente a simple vista.","393bde78":"Establecemos una semilla para que se puedan reproducir los experimentos m\u00e1s adelante:","9203a348":"Viendo las histogramas de todas las variables predictoras llegamos a las siguientes conclusiones:\n\n\n*   Lo primero de todo es que discretizar la mayor\u00eda de variables ser\u00e1 en vano, ya que las clases se superponen en todo momento. Algunas como el **radio** y sus derivadas parece que ser\u00edan una buena opci\u00f3n para discretizar porque es donde m\u00e1s separaci\u00f3n se ve, pero aun as\u00ed no es del todo clara. Lo mismo ocurre con **concave points**.\n*   Se observan variables con cierta tendencia a una distribici\u00f3n normal, como **Smoothness_mean** y **Symmetry_mean**.\n*   La oblicuidad a la derecha de todas las variables del error est\u00e1ndar.\n*   La aparici\u00f3n de **outliers** en las variables del **radio** y las derivadas, se ve como se \"arrastra\" dicho ruido. Tambi\u00e9n los valores an\u00f3malos est\u00e1n presentes en **Compactness**, **Smoothness** y **Fractal Dimension**\n\nPor \u00faltimo, en general es notorio el desbalance del problema comentado anteriormente. Siempre la distribuci\u00f3n de los casos benignos es superior al de malignos.","abe57fb9":"Usaremos al igual que en la base de datos anterior el **Zero R** como modelo de base para evaluar los nuestros con los datos preprocesados.","552374cf":"Juntamos todo los pasos del preprocesamiento y el \u00e1rbol de decisi\u00f3n en el pipeline","d13610c2":"## Preparando entorno","cc0aded8":"Y comprobamos que el conjunto de datos esta correctamente separado. Para ello usamos la funci\u00f3n `shape` que nos muestra (n\u00famero de instancias, n\u00famero de variables)","845b2c40":"Y comprobamos que se haya realizado completamente. Primero, las variables predictoras:","8a16f55d":"De nuevo y sin extra\u00f1arnos el modelo creado por el **Zero R** es un mal clasificador obteniendo un rendimiento bajo y una \"l\u00ednea recta\" en el an\u00e1lisis ROC. Es nuestro punto a partida a batir.","36cec3ed":"Como hab\u00edamos visto en el apartado anterior el conjunto de datos de entrenamiento esta formado por 398 instacias y 31 variables (30 variables predictoras y 1 variable clase).","239819bb":"Para facilitar el an\u00e1lisis exploratorio de datos, volvemos a juntar las variables predictoras con la variable clase. Comenzamos con el conjunto de datos de entrenamiento y luego procedemos igual para el conjunto de datos de prueba:","7aa8de21":"Distinguiremos dos modelos, uno con el \u00e1rbol de clasificaci\u00f3n sin discretizar tan solo con las variables eliminadas y otro con discretizado con el fin de vez cual tiene mejor rendimiento.","7680d3dd":"Viendo los histogramas condicionados por la vairable de clase llegamos a las siguientes conclusiones:\n\n*   **Glucose**, **BloodPressure** y **BMI** son variables que parece que intentan seguir una distribuci\u00f3n normal aunque los `outliers` provocan una visualizaci\u00f3n distorsionada de esta.\n*   Confirmamos la existencia de valores perdidos en algunas variables porque tienen puntos nulos que carecen de sentido: **SkinThickness**, **Insulin**, **BMI**, **Glucose** y **BloodPressure**.\n*   La oblicuidad positiva o la derecha de las variables **DiabetesPedigreeFunction**, **Insulin** y **Pregnancies**. \n*   En el \u00faltimo histograma, el de la variable de clase, se ve claramente de que estamos lidiando con un problema desbalanceado. Esto nos har\u00e1 recurrir a m\u00e9todos de evaluaci\u00f3n de clasificadores espec\u00edficos para estas situaciones.\n*   Se ha observado que en todas o casi todas las variables las distribuciones de cada categor\u00eda se superponen. Esto supone que la t\u00e9cnica de discretizado tendr\u00e1 un efecto casi nulo, ya que carece de poder discriminatorio.\n\n","cdfe6788":"Ambas tablas son particiones de la original, veamos si la separaci\u00f3n se ha ejecutado correctamente.","9436a836":"Comprobamos que se han juntado correctamente. Primero el conjunto de datos de entrenamiento:","a6265cee":"Empezamos cargando los datos `pima_diabetes` para poder trabajar con ellos:"}}