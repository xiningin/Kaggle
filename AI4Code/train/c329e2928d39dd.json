{"cell_type":{"1abd0c58":"code","81ec433b":"code","f4f474ba":"code","782d751a":"code","4ac871b6":"code","b70dac3f":"code","0c8a29d5":"code","6bb38591":"code","faddf056":"code","0236d196":"code","eaafac38":"code","db571882":"code","283690b9":"code","e7f8bce5":"code","afc3efbb":"code","e9e818e5":"code","4b8609e6":"code","d0854f64":"code","535c2d5c":"code","49a58a59":"code","450c8224":"code","11ed010b":"code","78e4cc5c":"code","ba712c53":"code","5aa5e9d1":"markdown","9861ae26":"markdown","bbcedc5d":"markdown","a773cdf7":"markdown","4425c11d":"markdown","bca0c6d0":"markdown"},"source":{"1abd0c58":"import pandas as pd","81ec433b":"df = pd.read_csv('..\/input\/titanic-machine-learning-from-disaster\/train.csv')\ndf.head()","f4f474ba":"df['Age'].isnull().sum()","782d751a":"import seaborn as sns","4ac871b6":"sns.distplot(df['Age'].dropna())","b70dac3f":"sns.distplot(df['Age'].fillna(100))","0c8a29d5":"figure = df.Age.hist(bins=50)\nfigure.set_title('Age')\nfigure.set_xlabel('Age')\nfigure.set_ylabel('No of passenger')","6bb38591":"figure = df.boxplot(column='Age')","faddf056":"df['Age'].describe()","0236d196":"# assuming Age follows a gaussian distribution we will calculate the boundaries which differentiates the outliers\nupper_boundary = df['Age'].mean() + 3*df['Age'].std()\nlower_boundary = df['Age'].mean() - 3*df['Age'].std()\nprint(lower_boundary), print(upper_boundary), print(df['Age'].mean())","eaafac38":"figure = df.Fare.hist(bins=50)\nfigure.set_title('Fare')\nfigure.set_xlabel('Fare')\nfigure.set_ylabel('No of passenger')","db571882":"df.boxplot(column='Fare')","283690b9":"df['Fare'].describe()","e7f8bce5":"#lets compute the interquantile range to calculate the boundaries\nIQR = df.Fare.quantile(0.75)-df.Fare.quantile(0.25)","afc3efbb":"lower_bridge = df['Fare'].quantile(0.25)-(IQR*1.5)\nupper_bridge = df['Fare'].quantile(0.75)+(IQR*1.5)\nprint(lower_bridge), print(upper_bridge)","e9e818e5":"#extreme outliers\nlower_bridge = df['Fare'].quantile(0.25)-(IQR*3)\nupper_bridge = df['Fare'].quantile(0.75)+(IQR*3)\nprint(lower_bridge), print(upper_bridge)","4b8609e6":"data = df.copy()","d0854f64":"data.loc[data['Age']>=73,'Age']=73","535c2d5c":"data.loc[data['Fare']>=100,'Fare']=100","49a58a59":"figure=data.Age.hist(bins=50)\nfigure.set_title('Age')\nfigure.set_xlabel('Age')\nfigure.set_ylabel('No of passenger')","450c8224":"figure=data.Fare.hist(bins=50)\nfigure.set_title('Fare')\nfigure.set_xlabel('Fare')\nfigure.set_ylabel('No of passenger')","11ed010b":"from sklearn.model_selection import train_test_split\nX_train,X_test,y_train,y_test=train_test_split(data[['Age','Fare']].fillna(0),data['Survived'],test_size=0.3)","78e4cc5c":"# Applying Logistic Regression\nfrom sklearn.linear_model import LogisticRegression\nclassifier=LogisticRegression()\nclassifier.fit(X_train,y_train)\ny_pred=classifier.predict(X_test)\ny_pred1=classifier.predict_proba(X_test)\n\nfrom sklearn.metrics import accuracy_score,roc_auc_score\nprint(\"Accuracy_score: {}\".format(accuracy_score(y_test,y_pred)))\nprint(\"roc_auc_score: {}\".format(roc_auc_score(y_test,y_pred1[:,1])))","ba712c53":"# Applying Logistic Regression\nfrom sklearn.ensemble import RandomForestClassifier\nclassifier=RandomForestClassifier()\nclassifier.fit(X_train,y_train)\ny_pred=classifier.predict(X_test)\ny_pred1=classifier.predict_proba(X_test)\n\nfrom sklearn.metrics import accuracy_score,roc_auc_score\nprint(\"Accuracy_score: {}\".format(accuracy_score(y_test,y_pred)))\nprint(\"roc_auc_score: {}\".format(roc_auc_score(y_test,y_pred1[:,1])))","5aa5e9d1":"### Which Machine Learning Models are sensitive to outliers?","9861ae26":"### If features are skewed we use the below technique ","bbcedc5d":"### If the Data is normally Distributed we use this","a773cdf7":"### Gaussian Distributed","4425c11d":"# Discussion Related with Outliers and Impact on Machine Learning ","bca0c6d0":"1. Naive Bayes Classifier----------------Not Sensitive to Outliers\n2. SVM----------------------------------Not Sensitive to Outliers\n3. Linear Regression---------------------Sensitive to Outliers\n4. Logistic Regression-------------------Sensitive to Outliers\n5. Decision Tree Regressor or Classifier-Not Sensitive to Outliers\n6. Ensemble(RF, XGBoost, GB)-------------Not Sensitive to Outliers\n7. KNN-----------------------------------Not Sensitive to Outliers\n8. Kmeans--------------------------------Sensitive to Outliers\n9. Hierarchical--------------------------Sensitive to Outliers\n10. PCA----------------------------------Sensitive to Outliers\n11. Neural Networks----------------------Sensitive to Outliers"}}