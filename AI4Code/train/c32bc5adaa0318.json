{"cell_type":{"90f57ef0":"code","68493538":"code","77f35e4c":"code","a8633b50":"code","3bc71366":"code","d0c27502":"code","3b490bc3":"code","ce8d52be":"code","91656f8f":"code","09a96f04":"code","2ad6d467":"code","62f8c694":"code","e5dbecaa":"code","9e505ead":"code","db7ce47f":"code","385eae99":"code","55645ff9":"code","e9c07452":"code","48386be0":"code","9d47afcd":"code","1cce744c":"code","61cd7183":"code","0abb2d7a":"code","9853a5a7":"code","1a99b065":"code","779ad26f":"code","aaa99b32":"code","1a665746":"code","565404ff":"code","abebcf96":"code","f75cb178":"code","931825c9":"code","2898c373":"code","ea03e362":"code","2cbb2189":"code","17b99391":"markdown","94e1e991":"markdown","023445ba":"markdown","8ddbbbde":"markdown","8277a5fb":"markdown","bd813355":"markdown","fbc5be85":"markdown","586753a5":"markdown","cb94c0db":"markdown"},"source":{"90f57ef0":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nfrom tqdm import tqdm\ntqdm.pandas()\n%matplotlib inline\n\nfrom plotly import tools\nimport plotly.offline as py\npy.init_notebook_mode(connected=True)\nimport plotly.graph_objs as go\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\nimport math\nfrom sklearn.model_selection import train_test_split\nfrom sklearn import metrics\nimport gensim.models.keyedvectors as word2vec\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.layers import Dense, Input, LSTM, Embedding, Dropout, Activation, CuDNNGRU, Conv1D, CuDNNLSTM, concatenate\nfrom keras.layers import Bidirectional, GlobalMaxPool1D, Dropout, SpatialDropout1D, GlobalAveragePooling1D, GlobalMaxPooling1D\nfrom keras.models import Model\nfrom keras import initializers, regularizers, constraints, optimizers, layers\nimport os\nprint(os.listdir(\"..\/input\"))\nimport gensim.models.keyedvectors as word2vec\nimport gc\n# Any results you write to the current directory are saved as output.","68493538":"train=pd.read_csv(\"..\/input\/train.csv\")\ntest=pd.read_csv(\"..\/input\/test.csv\")","77f35e4c":"train.head(10)","a8633b50":"def build_vocab(sentences,verbose=True):\n    vocab={}\n    \n    for sentence in tqdm(sentences,disable=(not verbose)):\n        for word in sentence:\n            try:\n                vocab[word] += 1\n            except KeyError:\n                vocab[word] = 1\n    return vocab\n","3bc71366":"sentences = train[\"question_text\"].progress_apply(lambda x: x.split()).values\n\nvocab = build_vocab(sentences)\nprint({k: vocab[k] for k in list(vocab)[:5]})","d0c27502":"from gensim.models import KeyedVectors\nnews_path = '..\/input\/embeddings\/GoogleNews-vectors-negative300\/GoogleNews-vectors-negative300.bin'\nword2vecDict= KeyedVectors.load_word2vec_format(news_path, binary=True)\n","3b490bc3":"import operator \n\ndef check_coverage(vocab,word2vecDict):\n    a = {}\n    oov = {}\n    k = 0\n    i = 0\n    for word in tqdm(vocab):\n        try:\n            a[word] = word2vecDict[word]\n            k += vocab[word]\n        except:\n\n            oov[word] = vocab[word]\n            i += vocab[word]\n            pass\n\n    print('Found embeddings for {:.2%} of vocab'.format(len(a) \/ len(vocab)))\n    print('Found embeddings for  {:.2%} of all text'.format(k \/ (k + i)))\n    sorted_x = sorted(oov.items(), key=operator.itemgetter(1))[::-1]\n\n    return sorted_x","ce8d52be":"oov = check_coverage(vocab,word2vecDict)","91656f8f":"oov[:15]","09a96f04":"def clean_text(x):\n    x = str(x)\n    for punct in \"\/-'\":\n        x = x.replace(punct, ' ')\n    for punct in '&':\n        x = x.replace(punct, f' {punct} ')\n    for punct in '?!.,\"#$%\\'()*+-\/:;<=>@[\\\\]^_`{|}~' + '\u201c\u201d\u2019':\n        x = x.replace(punct, '')\n    return x\n                    ","2ad6d467":"train_df[\"question_text\"] = train[\"question_text\"].progress_apply(lambda x: clean_text(x))\ntest_df[\"question_text\"] = test[\"question_text\"].progress_apply(lambda x: clean_text(x))\nsentences = train[\"question_text\"].apply(lambda x: x.split())\nvocab = build_vocab(sentences)","62f8c694":"oov = check_coverage(vocab,word2vecDict)","e5dbecaa":"oov[:16]","9e505ead":"for i in range(10):\n    print(word2vecDict.index2entity[i])","db7ce47f":"import re\n\ndef clean_numbers(x):\n    x = re.sub('[0-9]{5,}', '#####', x)\n    x = re.sub('[0-9]{4}', '####', x)\n    x = re.sub('[0-9]{3}', '###', x)\n    x = re.sub('[0-9]{2}',\"##\",x)\n    return x\ntrain_df[\"question_text\"] = train[\"question_text\"].progress_apply(lambda x: clean_numbers(x))\ntest_df[\"question_text\"] = test[\"question_text\"].progress_apply(lambda x: clean_numbers(x))","385eae99":"sentences = train_df[\"question_text\"].progress_apply(lambda x: x.split())\nvocab = build_vocab(sentences)","55645ff9":"oov = check_coverage(vocab,word2vecDict)","e9c07452":"oov[:20]","48386be0":"def _get_mispell(mispell_dict):\n    mispell_re = re.compile('(%s)' % '|'.join(mispell_dict.keys()))\n    return mispell_dict, mispell_re\n\n\nmispell_dict = {'colour':'color',\n                'centre':'center',\n                'didnt':'did not',\n                'doesnt':'does not',\n                'isnt':'is not',\n                'shouldnt':'should not',\n                'favourite':'favorite',\n                'travelling':'traveling',\n                'counselling':'counseling',\n                'theatre':'theater',\n                'cancelled':'canceled',\n                'labour':'labor',\n                'organisation':'organization',\n                'wwii':'world war 2',\n                'citicise':'criticize',\n                'instagram': 'social medium',\n                'whatsapp': 'social medium',\n                'snapchat': 'social medium'\n\n                }\nmispellings, mispellings_re = _get_mispell(mispell_dict)\n\ndef replace_typical_misspell(text):\n    def replace(match):\n        return mispellings[match.group(0)]\n\n    return mispellings_re.sub(replace, text)","9d47afcd":"train_df[\"question_text\"] = train[\"question_text\"].progress_apply(lambda x: replace_typical_misspell(x))\ntest_df[\"question_text\"] = test[\"question_text\"].progress_apply(lambda x: replace_typical_misspell(x))\nsentences = train[\"question_text\"].progress_apply(lambda x: x.split())\nto_remove = ['a','to','of','and']\nsentences = [[word for word in sentence if not word in to_remove] for sentence in tqdm(sentences)]\nvocab = build_vocab(sentences)","1cce744c":"oov = check_coverage(vocab,word2vecDict)","61cd7183":"oov[:20]\n","0abb2d7a":"del(oov)\n\ngc.collect()\ntrain.head(20)","9853a5a7":"embed_size = 300\nmaxlen = 200\nmax_features = 20000\ntokenizer = Tokenizer(num_words=max_features)\n\ntrain_x,val_x=train_test_split(train_df, test_size=0.1, random_state=2018)\ntrain_X=train_x[\"question_text\"].fillna(\"_na_\").values\nval_X=val_x[\"question_text\"].fillna(\"_na_\").values\ntest_X=test_df[\"question_text\"].fillna(\"_na_\").values\n\ntokenizer = Tokenizer(num_words=max_features)\ntokenizer.fit_on_texts(list(train_X))\ntrain_X = tokenizer.texts_to_sequences(train_X)\nval_X = tokenizer.texts_to_sequences(val_X)\ntest_X = tokenizer.texts_to_sequences(test_X)\n\n## Pad the sentences \ntrain_X = pad_sequences(train_X, maxlen=maxlen)\nval_X = pad_sequences(val_X, maxlen=maxlen)\ntest_X = pad_sequences(test_X, maxlen=maxlen)\n\n## Get the target values\ntrain_y = train_x['target'].values\nval_y = val_x['target'].values","1a99b065":"#word2vecDict = word2vec.KeyedVectors.load_word2vec_format(\"..\/input\/embeddings\/GoogleNews-vectors-negative300\/GoogleNews-vectors-negative300.bin\", binary=True)\n","779ad26f":"embeddings_index = dict()\nfor word in word2vecDict.wv.vocab:\n    embeddings_index[word] = word2vecDict.word_vec(word)","aaa99b32":"\nword_index = tokenizer.word_index\nnb_words = min(max_features, len(word_index))\nembedding_matrix = (np.random.rand(nb_words, embed_size) - 0.5) \/ 5.0\nfor word, i in word_index.items():\n    if i >= max_features: continue\n    if word in word2vecDict:\n        embedding_vector = word2vecDict.get_vector(word)\n        embedding_matrix[i] = embedding_vector\n        \ndel word2vecDict; gc.collect()   ","1a665746":"#inp = Input(shape=(maxlen,))\n#x = Embedding(max_features, embed_size, weights=[embedding_matrix])(inp)\n#x = Bidirectional(CuDNNGRU(64, return_sequences=True))(x)\n#x = GlobalMaxPool1D()(x)\n#x = Dense(16, activation=\"relu\")(x)\n#x = Dropout(0.1)(x)\n#x = Dense(1, activation=\"sigmoid\")(x)\n#model = Model(inputs=inp, outputs=x)\n#model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\nembed_size = 300 # how big is each word vector\nmax_features = 20000 # how many unique words to use (i.e num rows in embedding vector)\nmaxlen = 200 # max number of words in a question to use\n\nS_DROPOUT = 0.4\nDROPOUT = 0.1\n\n\ninp = Input(shape=(maxlen,))\nx = Embedding(max_features, embed_size , weights=[embedding_matrix_3])(inp)\nx = SpatialDropout1D(S_DROPOUT)(x)\nx = Bidirectional(CuDNNGRU(128, return_sequences=True))(x)\navg_pool = GlobalAveragePooling1D()(x)\nmax_pool = GlobalMaxPooling1D()(x)\nconc = concatenate([avg_pool, max_pool])\nx = Dense(16, activation=\"relu\")(conc)\nx = Dropout(DROPOUT)(x)\nx = Dense(1, activation=\"sigmoid\")(x)\nmodel = Model(inputs=inp, outputs=x)\nmodel.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n\nprint(model.summary())","565404ff":"EMBEDDING_FILE = '..\/input\/embeddings\/glove.840B.300d\/glove.840B.300d.txt'\ndef get_coefs(word,*arr): return word, np.asarray(arr, dtype='float32')\nembeddings_index = dict(get_coefs(*o.split(\" \")) for o in open(EMBEDDING_FILE))\n\n","abebcf96":"embed_size = 300 \nmax_features = 50000 \nmaxlen = 100 \n\ntrain_x,val_x=train_test_split(train, test_size=0.1, random_state=42)\ntrain_X=train_x[\"question_text\"].fillna(\"_na_\").values\nval_X=val_x[\"question_text\"].fillna(\"_na_\").values\ntest_X=test[\"question_text\"].fillna(\"_na_\").values\ntokenizer = Tokenizer(num_words=max_features)\n\ntokenizer.fit_on_texts(list(train_X))\ntrain_X = tokenizer.texts_to_sequences(train_X)\nval_X = tokenizer.texts_to_sequences(val_X)\ntest_X = tokenizer.texts_to_sequences(test_X)\n\n## Pad the sentences \ntrain_X = pad_sequences(train_X, maxlen=maxlen)\nval_X = pad_sequences(val_X, maxlen=maxlen)\ntest_X = pad_sequences(test_X, maxlen=maxlen)\n\n## Get the target values\ntrain_y = train_x['target'].values\nval_y = val_x['target'].values\n","f75cb178":"\nall_embs = np.stack(embeddings_index.values())\nemb_mean,emb_std = all_embs.mean(), all_embs.std()\nembed_size = all_embs.shape[1]\n\nword_index = tokenizer.word_index\nnb_words = min(max_features, len(word_index))\nembedding_matrix = np.random.normal(emb_mean, emb_std, (nb_words, embed_size))\nfor word, i in word_index.items():\n    if i >= max_features: continue\n    embedding_vector = embeddings_index.get(word)\n    if embedding_vector is not None: embedding_matrix[i] = embedding_vector\n        \ninp = Input(shape=(maxlen,))\nx = Embedding(max_features, embed_size, weights=[embedding_matrix])(inp)\nx = Bidirectional(CuDNNGRU(64, return_sequences=True))(x)\nx = GlobalMaxPool1D()(x)\nx = Dense(16, activation=\"relu\")(x)\nx = Dropout(0.1)(x)\nx = Dense(1, activation=\"sigmoid\")(x)\nmodel = Model(inputs=inp, outputs=x)\nmodel.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\nprint(model.summary())","931825c9":"model.fit(train_X, train_y, batch_size=1024, epochs=2, validation_data=(val_X, val_y))","2898c373":"pred_test_y = model.predict([val_X], batch_size=512, verbose=1)\nfor thresh in np.arange(0.1, 0.501, 0.01):\n    thresh = np.round(thresh, 2)\n    print(\"F1 score at threshold {0} is {1}\".format(thresh, metrics.f1_score(val_y, (pred_test_y>thresh).astype(int))))","ea03e362":"pred_y = model.predict([test_X], batch_size=512, verbose=1)","2cbb2189":"\npred_y = (pred_y>0.35).astype(int)\nout= pd.DataFrame({\"qid\":test[\"qid\"].values})\nout['prediction'] = pred_y\nout.to_csv(\"submission.csv\", index=False)","17b99391":"\nNow the Data is processed and was almost 60% of the embedding.\nNow we move into data insights.\n","94e1e991":"Now going to model","023445ba":"Looks like numbers are denoted with \"#\" so we need to reolace numbers with \"#\" as per the google embeddings.","8ddbbbde":"Lets look at the data words in data","8277a5fb":"From this we can clearly see that only 24.3% of the data words are present in the embeddings. This shows there is a need for preprocessing the data to eleminate the puncuation marks, numbers, spaces. We go along the steps to clean the data. ","bd813355":"Now considering for the missplled words and replacing with correct words and removing the words like \"a\",\"to\",\"of\",\"and\"","fbc5be85":"Counting the frequency of words in our data","586753a5":"Now importing the embeddings and checking the percentage of words in data thats are in embeddings.","cb94c0db":"Now punctuation are the reson for less percentage of words in embeddings. Lets remove these punctuations."}}