{"cell_type":{"00de01c6":"code","963d9f24":"code","bc730a1f":"code","e75065b7":"markdown","dc2e4f31":"markdown"},"source":{"00de01c6":"''' Code structure\n1 imports\n2 def # (original)\n        decode_image\n        read_labeled_tfrecord\n        read_unlabeled_tfrecord\n        load_dataset\n        data_augment\n        get_training_dataset\n        get_validation_dataset\n        get_test_dataset\n        count_data_items\n   Data Augmentation\n        get_mat\n        transform_ori\n        lrfn\n   ANN functions\n        get_model\n        train_cross_validate\n        train_and_predict\n3 def # Modeifications, our work\n    \n4 main function\n    train_and_predict\n5 Confusion Matrix and Validation Score   '''","963d9f24":"import time\nprint('Start: '+time.strftime(\"%Y-%m-%d-%H_%M_%S\", time.localtime()))\n\nimport random, re, math\nimport numpy as np, pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import KFold\nfrom sklearn.metrics import f1_score, precision_score, recall_score, confusion_matrix\n\nimport tensorflow as tf, tensorflow.keras.backend as K\nprint('    Tensorflow version ' + tf.__version__)\nfrom tensorflow.keras.applications import DenseNet201\nfrom kaggle_datasets import KaggleDatasets\n\n\ndef decode_image(image_data):\n    image = tf.image.decode_jpeg(image_data, channels=3)\n    image = tf.cast(image, tf.float32) \/ 255.0  # convert image to floats in [0, 1] range\n    image = tf.reshape(image, [*IMAGE_SIZE, 3]) # explicit size needed for TPU\n    return image\n\ndef read_labeled_tfrecord(example):\n    LABELED_TFREC_FORMAT = {\n        \"image\": tf.io.FixedLenFeature([], tf.string), # tf.string means bytestring\n        \"class\": tf.io.FixedLenFeature([], tf.int64),  # shape [] means single element\n    }\n    example = tf.io.parse_single_example(example, LABELED_TFREC_FORMAT)\n    image = decode_image(example['image'])\n    label = tf.cast(example['class'], tf.int32)\n    return image, label # returns a dataset of (image, label) pairs\n\ndef read_unlabeled_tfrecord(example):\n    UNLABELED_TFREC_FORMAT = {\n        \"image\": tf.io.FixedLenFeature([], tf.string), # tf.string means bytestring\n        \"id\": tf.io.FixedLenFeature([], tf.string),  # shape [] means single element\n        # class is missing, this competitions's challenge is to predict flower classes for the test dataset\n    }\n    example = tf.io.parse_single_example(example, UNLABELED_TFREC_FORMAT)\n    image = decode_image(example['image'])\n    idnum = example['id']\n    return image, idnum # returns a dataset of image(s)\n\ndef load_dataset(filenames, labeled = True, ordered = False):\n    # Read from TFRecords. For optimal performance, reading from multiple files at once and\n    # Diregarding data order. Order does not matter since we will be shuffling the data anyway\n    ignore_order = tf.data.Options()\n    if not ordered:\n        ignore_order.experimental_deterministic = False # disable order, increase speed\n        \n    dataset = tf.data.TFRecordDataset(filenames, num_parallel_reads = AUTO) # automatically interleaves reads from multiple files\n    dataset = dataset.with_options(ignore_order) # use data as soon as it streams in, rather than in its original order\n    dataset = dataset.map(read_labeled_tfrecord if labeled else read_unlabeled_tfrecord, num_parallel_calls = AUTO) # returns a dataset of (image, label) pairs if labeled = True or (image, id) pair if labeld = False\n    return dataset\n\ndef data_augment(image, label):\n    # data augmentation. Thanks to the dataset.prefetch(AUTO) statement in the next function (below),\n    # this happens essentially for free on TPU. Data pipeline code is executed on the \"CPU\" part\n    # of the TPU while the TPU itself is computing gradients.\n    image = tf.image.random_flip_left_right(image)\n    return image, label   \n\ndef get_training_dataset(dataset,do_aug=True):\n    dataset = dataset.map(data_augment, num_parallel_calls=AUTO)\n    if do_aug: dataset = dataset.map(transform, num_parallel_calls=AUTO)\n    dataset = dataset.repeat() # the training dataset must repeat for several epochs\n    dataset = dataset.shuffle(2048)\n    dataset = dataset.batch(BATCH_SIZE)\n    dataset = dataset.prefetch(AUTO) # prefetch next batch while training (autotune prefetch buffer size)\n    return dataset\n\ndef get_validation_dataset(dataset):\n    dataset = dataset.batch(BATCH_SIZE)\n    dataset = dataset.cache()\n    dataset = dataset.prefetch(AUTO) # prefetch next batch while training (autotune prefetch buffer size)\n    return dataset\n\ndef get_test_dataset(ordered=False):\n    dataset = load_dataset(TEST_FILENAMES, labeled=False, ordered=ordered)\n    dataset = dataset.batch(BATCH_SIZE)\n    dataset = dataset.prefetch(AUTO) # prefetch next batch while training (autotune prefetch buffer size)\n    return dataset\n\ndef count_data_items(filenames): # V\n    # the number of data items is written in the name of the .tfrec files, i.e. flowers00-230.tfrec = 230 data items\n    n = [int(re.compile(r\"-([0-9]*)\\.\").search(filename).group(1)) for filename in filenames]\n    return np.sum(n)\n\n###############     ##############\n# # Data Augmentation # \u6570\u636e\u589e\u5f3a\u7528\u51fd\u6570\n    # \u4ee5\u4e0b\u4ee3\u7801\u4f7f\u7528GPU\/TPU\u8fdb\u884c\u968f\u673a\u65cb\u8f6c\uff0c\u526a\u5207\uff0c\u7f29\u653e\u548c\u79fb\u4f4d\u3002 \u5f53\u56fe\u50cf\u4ece\u663e\u793a\u7a7a\u767d\u7684\u8fb9\u7f18\u79fb\u5f00\u65f6\uff0c\u901a\u8fc7\u62c9\u4f38\u539f\u59cb\u8fb9\u7f18\u4e0a\u7684\u989c\u8272\u6765\u586b\u5145\u7a7a\u767d\u3002 \n    # \u5728\u4e0b\u9762\u7684\u51fd\u6570\u201c transform\uff08\uff09\u201d\u4e2d\u66f4\u6539\u53d8\u91cf\uff0c\u4ee5\u63a7\u5236\u6240\u9700\u7684\u6269\u5145\u91cf\u3002 \ndef get_mat(rotation, shear, height_zoom, width_zoom, height_shift, width_shift):\n    # returns 3x3 transformmatrix which transforms indicies  \u8fd4\u56de\u7528\u4e8e\u8f6c\u6362\u7d22\u5f15\u76843x3\u8f6c\u6362\u77e9\u9635\n    # CONVERT DEGREES TO RADIANS\n    rotation = math.pi * rotation \/ 180.\n    shear = math.pi * shear \/ 180.\n    # ROTATION MATRIX\n    c1 = tf.math.cos(rotation)\n    s1 = tf.math.sin(rotation)\n    one = tf.constant([1],dtype='float32')\n    zero = tf.constant([0],dtype='float32')\n    rotation_matrix = tf.reshape( tf.concat([c1,s1,zero, -s1,c1,zero, zero,zero,one],axis=0),[3,3] )   \n    # SHEAR MATRIX\n    c2 = tf.math.cos(shear)\n    s2 = tf.math.sin(shear)\n    shear_matrix = tf.reshape( tf.concat([one,s2,zero, zero,c2,zero, zero,zero,one],axis=0),[3,3] )    \n    # ZOOM MATRIX\n    zoom_matrix = tf.reshape( tf.concat([one\/height_zoom,zero,zero, zero,one\/width_zoom,zero, zero,zero,one],axis=0),[3,3] )\n    # SHIFT MATRIX\n    shift_matrix = tf.reshape( tf.concat([one,zero,height_shift, zero,one,width_shift, zero,zero,one],axis=0),[3,3] )\n    return K.dot(K.dot(rotation_matrix, shear_matrix), K.dot(zoom_matrix, shift_matrix))\n\ndef transform_ori(image,label):\n    # input image - is one image of size [dim,dim,3] not a batch of [b,dim,dim,3]\n    # output - image randomly rotated, sheared, zoomed, and shifted \u968f\u673a\u65cb\u8f6c\uff0c\u526a\u5207\uff0c\u7f29\u653e\u548c\u79fb\u52a8\n    DIM = IMAGE_SIZE[0]\n    XDIM = DIM%2 #fix for size 331\n    rot = 15. * tf.random.normal([1],dtype='float32')\n    shr = 5. * tf.random.normal([1],dtype='float32') \n    h_zoom = 1.0 + tf.random.normal([1],dtype='float32')\/10.\n    w_zoom = 1.0 + tf.random.normal([1],dtype='float32')\/10.\n    h_shift = 16. * tf.random.normal([1],dtype='float32') \n    w_shift = 16. * tf.random.normal([1],dtype='float32') \n    # GET TRANSFORMATION MATRIX \n    m = get_mat(rot,shr,h_zoom,w_zoom,h_shift,w_shift)\n    # LIST DESTINATION PIXEL INDICES\n    x = tf.repeat( tf.range(DIM\/\/2,-DIM\/\/2,-1), DIM )\n    y = tf.tile( tf.range(-DIM\/\/2,DIM\/\/2),[DIM] )\n    z = tf.ones([DIM*DIM],dtype='int32')\n    idx = tf.stack( [x,y,z] )\n    # ROTATE DESTINATION PIXELS ONTO ORIGIN PIXELS\n    idx2 = K.dot(m,tf.cast(idx,dtype='float32'))\n    idx2 = K.cast(idx2,dtype='int32')\n    idx2 = K.clip(idx2,-DIM\/\/2+XDIM+1,DIM\/\/2)\n    # FIND ORIGIN PIXEL VALUES           \n    idx3 = tf.stack( [DIM\/\/2-idx2[0,], DIM\/\/2-1+idx2[1,]] )\n    d = tf.gather_nd(image,tf.transpose(idx3));#d: tf.Tensor(shape=(262144, 3), dtype=float32)  (for 512px image)        \n    return tf.reshape(d,[DIM,DIM,3]),label\n\n# \u8ba1\u7b97\u5b66\u4e60\u7387\u7528\u51fd\u6570\ndef lrfn(epoch):\n    if epoch < LR_RAMPUP_EPOCHS:\n        lr = (LR_MAX - LR_START) \/ LR_RAMPUP_EPOCHS * epoch + LR_START\n    elif epoch < LR_RAMPUP_EPOCHS + LR_SUSTAIN_EPOCHS:\n        lr = LR_MAX\n    else:\n        lr = (LR_MAX - LR_MIN) * LR_EXP_DECAY**(epoch - LR_RAMPUP_EPOCHS - LR_SUSTAIN_EPOCHS) + LR_MIN\n    return lr\n\n# ANN functions #\u795e\u7ecf\u7f51\u7edc\u7528\u51fd\u6570\ndef get_model():\n    print('    -------------get_model is running')\n    with strategy.scope():\n        rnet = DenseNet201(\n            input_shape=(IMAGE_SIZE[0], IMAGE_SIZE[1], 3),weights='imagenet',include_top=False)\n        # trainable rnet\n        rnet.trainable = True\n        model = tf.keras.Sequential([rnet,tf.keras.layers.GlobalAveragePooling2D(),\n            tf.keras.layers.Dense(len(CLASSES), activation='softmax',dtype='float32') ])\n    model.compile(optimizer='adam',loss = 'sparse_categorical_crossentropy',metrics=['sparse_categorical_accuracy'])\n    return model\n\ndef train_cross_validate(folds = 5):\n    print('    -------------train_cross_validate is running')\n    histories = [];   models = [];\n    early_stopping = tf.keras.callbacks.EarlyStopping(monitor = 'val_loss', patience = 3)\n    kfold = KFold(folds, shuffle = True, random_state = SEED)\n    for f, (trn_ind, val_ind) in enumerate(kfold.split(TRAINING_FILENAMES)):\n        print('    '+'#'*25);print('    ### FOLD',f+1);\n        train_dataset = load_dataset(list(pd.DataFrame({'TRAINING_FILENAMES': TRAINING_FILENAMES}).loc[trn_ind]['TRAINING_FILENAMES']), labeled = True)\n        val_dataset = load_dataset(list(pd.DataFrame({'TRAINING_FILENAMES': TRAINING_FILENAMES}).loc[val_ind]['TRAINING_FILENAMES']), labeled = True, ordered = True)\n        model = get_model()\n        if no_training_mode:\n            print('    No training mode is on, training is skipped: no_training_mode == True')\n            models.append(model)\n            return histories, models\n        else:\n            history = model.fit(\n                get_training_dataset(train_dataset), \n                steps_per_epoch = STEPS_PER_EPOCH,epochs = EPOCHS,\n                callbacks = [lr_callback],#, early_stopping],\n                validation_data = get_validation_dataset(val_dataset),\n                verbose=1)\n            models.append(model)\n            histories.append(history)\n        if no_folds:\n            print('    No folds mode is on, the other folds is skipped: no_training_mode == no_folds')\n            return histories, models\n    return histories, models\n\ndef train_and_predict(folds = 5):\n    print('    -------------train_and_predict is running')\n    test_ds = get_test_dataset(ordered=True) \n        # since we are splitting the dataset and iterating separately on images and ids, order matters.\n    test_images_ds = test_ds.map(lambda image, idnum: image)\n    \n    # \u6b63\u5f0f\u8bad\u7ec3\n    print('Start training %i folds'%folds)\n    histories, models = train_cross_validate(folds = folds)\n    \n    print('Computing predictions...')\n    # get the mean probability of the folds models \u4ea4\u53c9\u9a8c\u8bc1\u5f97\u5230\u5e73\u5747\u6982\u7387\n    if no_folds: # \u5feb\u901f\u6d4b\u8bd5\u65f6\u4ec5\u4ec5\u53ea\u8dd1\u4e00\u6b21fold\uff0c\u800c\u975e\u4ea4\u53c9\u9a8c\u8bc1\uff08[models[0]\uff09\n        probabilities = np.average([models[0].predict(test_images_ds)], axis = 0)\n    else:\n        probabilities = np.average([models[i].predict(test_images_ds) for i in range(folds)], axis = 0)\n        \n    predictions = np.argmax(probabilities, axis=-1)\n    print('Generating submission.csv file...')\n    test_ids_ds = test_ds.map(lambda image, idnum: idnum).unbatch()\n    test_ids = next(iter(test_ids_ds.batch(NUM_TEST_IMAGES))).numpy().astype('U') # all in one batch\n    np.savetxt('submission.csv', np.rec.fromarrays([test_ids, predictions]), fmt=['%s', '%d'], delimiter=',', header='id,label', comments='')\n    return histories, models\n    \n######################### Modified Functions ######################### Our work\ndef plot_history(history, label, epcohs):\n    data = {}; \n    data[label] = [0]; data[label].extend(history.history[label]) \n    data['val_' + label] = [0];  data['val_' + label].extend(history.history['val_' + label])\n    pd.DataFrame(data).plot(figsize=(8, 5))\n    plt.grid(True);plt.axis([1, epochs, 0, data[label][1]*1.5]);plt.show()\n    \ndef contrastImg_ts(img_file1,alpha,beta,gamma):  #Adjust Contrast & Brightness randomly\n    #\u968f\u673a\u8c03\u6574\u5bf9\u6bd4\u5ea6\u53ca\u4eae\u5ea6\\\n    alpha = tf.random.uniform([1],dtype='float32')*0.3+0.7;\n    gamma = (tf.random.uniform([1],dtype='float32')-0.5)*0.8;\n    beta = 1.0 - alpha;#     print(IMAGE_SIZE)\n    img2 = tf.zeros([IMAGE_SIZE[0],IMAGE_SIZE[1],3],dtype='float32');\n    print(\"\u539f\u56fe\u6743\u91cd=\",alpha);\n    contrasted = img_file1*alpha + img2*beta + gamma;\n    return contrasted\n    \ndef transform(image,label): # Basicly same as the original\n    # input image - is one image of size [dim,dim,3] not a batch of [b,dim,dim,3]\n    # output - image randomly rotated, sheared, zoomed, and shifted\n    DIM = IMAGE_SIZE[0];XDIM = DIM%2 #fix for size 331\n    \n    rot = 15. * tf.random.normal([1],dtype='float32');\n    shr = 5. * tf.random.normal([1],dtype='float32') ;\n    h_zoom = 1.0 + tf.random.normal([1],dtype='float32')\/10.;\n    w_zoom = 1.0 + tf.random.normal([1],dtype='float32')\/10.;\n    h_shift = 16. * tf.random.normal([1],dtype='float32') ;\n    w_shift = 16. * tf.random.normal([1],dtype='float32') ;\n  \n    # GET TRANSFORMATION MATRIX \n    m = get_mat(rot,shr,h_zoom,w_zoom,h_shift,w_shift);\n\n    # LIST DESTINATION PIXEL INDICES\n    x = tf.repeat( tf.range(DIM\/\/2,-DIM\/\/2,-1), DIM );\n    y = tf.tile( tf.range(-DIM\/\/2,DIM\/\/2),[DIM] );\n    z = tf.ones([DIM*DIM],dtype='int32');\n    idx = tf.stack( [x,y,z] );\n    \n    # ROTATE DESTINATION PIXELS ONTO ORIGIN PIXELS\n    idx2 = K.dot(m,tf.cast(idx,dtype='float32'));\n    idx2 = K.cast(idx2,dtype='int32');\n    idx2 = K.clip(idx2,-DIM\/\/2+XDIM+1,DIM\/\/2);\n    \n    # FIND ORIGIN PIXEL VALUES           \n    idx3 = tf.stack( [DIM\/\/2-idx2[0,], DIM\/\/2-1+idx2[1,]] );\n    img_changed = contrastImg_ts(image,alpha=1,beta=0,gamma=0);    # NEW\n    d = tf.gather_nd(img_changed,tf.transpose(idx3));\n    \n    return tf.reshape(d,[DIM,DIM,3]),label\n\n####   main function # \u4e3b\u51fd\u6570\nif __name__ == \"__main__\":\n    upload_mode = True\n    # Configuration \u8bbe\u7f6e \u53ca \u8d85\u53c2\u6570 \n    print('Configuring: ' + time.strftime(\"%Y-%m-%d-%H_%M_%S\", time.localtime()))\n    if upload_mode:\n        IMAGE_SIZE =[512,512]# [331,331]#[512,512];# [192,192];# \n        EPOCHS = 15\n        FOLDS = 2;        SEED = 777;\n        # BATCH_SIZE = 16 * strategy.num_replicas_in_sync   # \u5728\u4e0b\u65b9\uff0c\u6b64\u5904strategy\u672a\u751f\u6210\n        quick_test = False;no_training_mode = False; show_augmented_exampe = True; no_folds = False;\n    else:\n        IMAGE_SIZE = [512,512]#[192,192];\n        EPOCHS = 20\n        FOLDS = 2\n        SEED = 777\n        no_folds = True; # \u8df3\u8fc7folds\u6b65\u9aa4\n        quick_test = False; # \u52a0\u8f7d\u5c11\u91cf\u6570\u636e\u8fdb\u884c\u8bad\u7ec3\n        no_training_mode = False;#True; \n        show_augmented_exampe = False;#True; \n        \n    MIXED_PRECISION = False;     XLA_ACCELERATE = False\n\n    # Configurations \u786c\u4ef6\u914d\u7f6e\n    print('Detecting Hardware:'+time.strftime(\"%Y-%m-%d-%H_%M_%S\", time.localtime()))\n    # Detect hardware, return appropriate distribution strategy \u68c0\u6d4b\u786c\u4ef6\uff0c\u8fd4\u56de\u9002\u5f53\u7684\u5206\u914d\u7b56\u7565\n    try:\n        tpu = tf.distribute.cluster_resolver.TPUClusterResolver()# Cluster Resolver for Google Cloud TPUs.\u9002\u7528\u4e8eGoogleCloudTPU\u7684\u96c6\u7fa4\u89e3\u6790\u5668\u3002\n        # TPU detection. No parameters necessary if TPU_NAME TPU\u68c0\u6d4b\u3002 \u5982\u679cTPU_NAME\uff0c\u5219\u4e0d\u9700\u8981\u4efb\u4f55\u53c2\u6570\n        #    environment variable is set. On Kaggle this is always the case. \u73af\u5883\u53d8\u91cf\u5df2\u8bbe\u7f6e\u3002 \u5728Kaggle\u4e0a\uff0c\u60c5\u51b5\u603b\u662f\u5982\u6b64\u3002\n        print('    Running on TPU ', tpu.master())\n    except ValueError:\n        tpu = None\n\n    if tpu:\n        tf.config.experimental_connect_to_cluster(tpu)# Connects to the given cluster.\n        tf.tpu.experimental.initialize_tpu_system(tpu)# Initialize the TPU devices.\n        strategy = tf.distribute.experimental.TPUStrategy(tpu)# TPU distribution strategy implementation.\n    else:\n        strategy = tf.distribute.get_strategy() \n        # default distribution strategy in Tensorflow. Works on CPU and single GPU.\n\n    BATCH_SIZE = 16 * strategy.num_replicas_in_sync # \n    AUTO = tf.data.experimental.AUTOTUNE\n    print(\"    REPLICAS: \", strategy.num_replicas_in_sync) # \u591a\u7ebf\u7a0b\uff1f\n    \n\n    # Mixed Precision and\/or XLA \n    # \u4ee5\u4e0b\u5e03\u5c14\u503c\u53ef\u4ee5\u5728GPU \/ TPU\u4e0a\u542f\u7528\u6df7\u5408\u7cbe\u5ea6\u548c\/\u6216XLA\u3002 \u9ed8\u8ba4\u60c5\u51b5\u4e0b\uff0cTPU\u5df2\u7ecf\u4f7f\u7528\u4e86\u67d0\u79cd\u6df7\u5408\u7cbe\u5ea6\uff0c\u4f46\u6211\u4eec\u53ef\u4ee5\u6dfb\u52a0\u66f4\u591a\u7cbe\u5ea6\u3002 \n    #\u8fd9\u4e9b\u4f7fGPU \/ TPU\u5185\u5b58\u53ef\u4ee5\u5904\u7406\u66f4\u5927\u7684\u6279\u5904\u7406\u5927\u5c0f\uff0c\u5e76\u53ef\u4ee5\u52a0\u5feb\u8bad\u7ec3\u8fc7\u7a0b\u3002 \n    #Nvidia V100 GPU\u5177\u6709\u7279\u6b8a\u7684Tensor\u6838\u5fc3\uff0c\u5728\u542f\u7528\u6df7\u5408\u7cbe\u5ea6\u540e\u4f1a\u88ab\u5229\u7528\u3002 Kaggle\u7684Nvidia P100 GPU\u6ca1\u6709Tensor Core\u6765\u52a0\u901f\u3002\n    if MIXED_PRECISION:\n        print('    Mixed precision enabled:'+time.strftime(\"%Y-%m-%d-%H_%M_%S\", time.localtime()))\n        from tensorflow.keras.mixed_precision import experimental as mixed_precision\n        if tpu: policy = tf.keras.mixed_precision.experimental.Policy('mixed_bfloat16')\n        else: policy = tf.keras.mixed_precision.experimental.Policy('mixed_float16')\n        mixed_precision.set_policy(policy)\n        print('    Mixed precision enabled')\n        \n    if XLA_ACCELERATE:\n        print('   Accelerated Linear Algebra:'+time.strftime(\"%Y-%m-%d-%H_%M_%S\", time.localtime()))\n        tf.config.optimizer.set_jit(True)\n        print('    Accelerated Linear Algebra enabled')\n        \n# # Data Directories \u6570\u636e\u8def\u5f84\u4e0e\u83b7\u53d6\n    # Data access # \u6570\u636e\u8def\u5f84\n    print('Accessing Data: '+time.strftime(\"%Y-%m-%d-%H_%M_%S\", time.localtime()))\n    GCS_DS_PATH = KaggleDatasets().get_gcs_path('flower-classification-with-tpus')\n    # available image sizes :\n    GCS_PATH_SELECT = { \n        192: GCS_DS_PATH + '\/tfrecords-jpeg-192x192',224: GCS_DS_PATH + '\/tfrecords-jpeg-224x224',\n        331: GCS_DS_PATH + '\/tfrecords-jpeg-331x331',512: GCS_DS_PATH + '\/tfrecords-jpeg-512x512'}\n    print('    IMAGE_SIZE:' + str(IMAGE_SIZE)) # \u56fe\u50cf\u5c3a\u5bf8\n    GCS_PATH = GCS_PATH_SELECT[IMAGE_SIZE[0]]\n    TRAINING_FILENAMES = tf.io.gfile.glob(GCS_PATH + '\/train\/*.tfrec') + tf.io.gfile.glob(GCS_PATH + '\/val\/*.tfrec')\n    # predictions on this dataset should be submitted for the competition\n    TEST_FILENAMES = tf.io.gfile.glob(GCS_PATH + '\/test\/*.tfrec')\n    \n    if quick_test: # \u6570\u636e\u622a\u53d6(\u51cf\u5c11\u6570\u636e\u52a0\u901f\u6d4b\u8bd5)\n        print('\u6570\u636e\u622a\u53d6\u4e2d(\u51cf\u5c11\u6570\u636e\u52a0\u901f\u6d4b\u8bd5): '+ time.strftime(\"%Y-%m-%d-%H_%M_%S\", time.localtime()))\n        TRAINING_FILENAMES = TRAINING_FILENAMES[0:2];#print(TRAINING_FILENAMES);\n        TEST_FILENAMES = TEST_FILENAMES[0:1];#print(TEST_FILENAMES)\n        print('\\n    QUICK TEST MODEL!!! very little amount of data \\n')\n    else:\n        print('\\n    FULL TEST MODEL!!! ALL data\\n')\n    \n    # Classes \u5b9a\u4e49\u82b1\u6735\u7c7b\u522b\n    CLASSES = ['pink primrose',    'hard-leaved pocket orchid', 'canterbury bells', 'sweet pea',     'wild geranium',     'tiger lily',           'moon orchid',              'bird of paradise', 'monkshood',        'globe thistle',         # 00 - 09\n               'snapdragon',       \"colt's foot\",               'king protea',      'spear thistle', 'yellow iris',       'globe-flower',         'purple coneflower',        'peruvian lily',    'balloon flower',   'giant white arum lily', # 10 - 19\n               'fire lily',        'pincushion flower',         'fritillary',       'red ginger',    'grape hyacinth',    'corn poppy',           'prince of wales feathers', 'stemless gentian', 'artichoke',        'sweet william',         # 20 - 29\n               'carnation',        'garden phlox',              'love in the mist', 'cosmos',        'alpine sea holly',  'ruby-lipped cattleya', 'cape flower',              'great masterwort', 'siam tulip',       'lenten rose',           # 30 - 39\n               'barberton daisy',  'daffodil',                  'sword lily',       'poinsettia',    'bolero deep blue',  'wallflower',           'marigold',                 'buttercup',        'daisy',            'common dandelion',      # 40 - 49\n               'petunia',          'wild pansy',                'primula',          'sunflower',     'lilac hibiscus',    'bishop of llandaff',   'gaura',                    'geranium',         'orange dahlia',    'pink-yellow dahlia',    # 50 - 59\n               'cautleya spicata', 'japanese anemone',          'black-eyed susan', 'silverbush',    'californian poppy', 'osteospermum',         'spring crocus',            'iris',             'windflower',       'tree poppy',            # 60 - 69\n               'gazania',          'azalea',                    'water lily',       'rose',          'thorn apple',       'morning glory',        'passion flower',           'lotus',            'toad lily',        'anthurium',             # 70 - 79\n               'frangipani',       'clematis',                  'hibiscus',         'columbine',     'desert-rose',       'tree mallow',          'magnolia',                 'cyclamen ',        'watercress',       'canna lily',            # 80 - 89\n               'hippeastrum ',     'bee balm',                  'pink quill',       'foxglove',      'bougainvillea',     'camellia',             'mallow',                   'mexican petunia',  'bromelia',         'blanket flower',        # 90 - 99\n               'trumpet creeper',  'blackberry lily',           'common tulip',     'wild rose']                                                                                                                                               # 100 - 102\n    \n    # \u751f\u6210\u5b66\u4e60\u7387\u7b56\u7565\n    print('Generating Learning Rate: '+time.strftime(\"%Y-%m-%d-%H_%M_%S\", time.localtime()))\n    # # Custom LR scheduler \u5b66\u4e60\u7387\u7b56\u7565\n    # From starter [kernel][1]: https:\/\/www.kaggle.com\/mgornergoogle\/getting-started-with-100-flowers-on-tpu\n    # TPU\uff0cGPU\u548cCPU\u7684\u5b66\u4e60\u7387\u8ba1\u5212\u3002 #\u4f7f\u7528LR\u52a0\u901f\u662f\u56e0\u4e3a\u5fae\u8c03\u4e86\u9884\u5148\u8bad\u7ec3\u7684\u6a21\u578b\u3002# \u4ece\u9ad8LR\u5f00\u59cb\u4f1a\u7834\u574f\u9884\u8bad\u7ec3\u7684\u6743\u91cd\u3002\n    \n    LR_START = 0.00001\n    LR_MAX = 0.00005 * strategy.num_replicas_in_sync\n    LR_MIN = 0.00001\n    LR_RAMPUP_EPOCHS = 5\n    LR_SUSTAIN_EPOCHS = 0\n    LR_EXP_DECAY = .8\n    \n    lr_callback = tf.keras.callbacks.LearningRateScheduler(lrfn, verbose = True)\n    \n    rng = [i for i in range(25 if EPOCHS<25 else EPOCHS)]\n    y = [lrfn(x) for x in rng]\n    if show_augmented_exampe:\n        plt.plot(rng, y) \n    print(\"    Learning rate schedule: {:.3g} to {:.3g} to {:.3g}\".format(y[0], max(y), y[-1]))\n    \n    # # Dataset Functions # \u83b7\u53d6\u6570\u636e\n    # From starter [kernel][1]: https:\/\/www.kaggle.com\/mgornergoogle\/getting-started-with-100-flowers-on-tpu\n    \n    # \u6c42\u8bad\u7ec3\u3001\u9a8c\u8bc1\u3001\u6d4b\u8bd5 \u6570\u91cf\n    NUM_TRAINING_IMAGES = int( count_data_items(TRAINING_FILENAMES) * (FOLDS-1.)\/FOLDS )\n    NUM_VALIDATION_IMAGES = int( count_data_items(TRAINING_FILENAMES) * (1.\/FOLDS) )\n    NUM_TEST_IMAGES = count_data_items(TEST_FILENAMES)\n    STEPS_PER_EPOCH = NUM_TRAINING_IMAGES \/\/ BATCH_SIZE\n    print('    Dataset: {} training images, {} validation images, {} unlabeled test images'.format(NUM_TRAINING_IMAGES, NUM_VALIDATION_IMAGES, NUM_TEST_IMAGES))\n    \n    # # Display Example Augmentation \u5c55\u793a\u589e\u5f3a\u6548\u679c\n    # \u4ee5\u4e0b\u662f3\u4e2a\u8bad\u7ec3\u56fe\u50cf\u7684\u793a\u4f8b\uff0c\u5176\u4e2d\u6bcf\u4e2a\u56fe\u50cf\u968f\u673a12\u6b21\u589e\u5f3a\u3002\n    if show_augmented_exampe:\n        print('Plotting augmented exampe: '+time.strftime(\"%Y-%m-%d-%H_%M_%S\", time.localtime()))\n        row = 3; col = 4;\n        for ex_No in range(2): # Examples\n            all_elements = get_training_dataset(load_dataset(TRAINING_FILENAMES),do_aug=False).unbatch()\n            one_element = tf.data.Dataset.from_tensors( next(iter(all_elements)) )\n            augmented_element = one_element.repeat().map(transform).batch(row*col)\n            for (img,label) in augmented_element:\n                plt.figure(figsize=(15,int(15*row\/col)))\n                for j in range(row*col):\n                    plt.subplot(row,col,j+1);plt.axis('off');\n                    plt.imshow(img[j,]);plt.show()\n                break\n        \n    # run train and predict \u8fd0\u884c\u8bad\u7ec3\n    print('Start training: '+time.strftime(\"%Y-%m-%d-%H_%M_%S\", time.localtime()))\n    histories, models = train_and_predict(folds = FOLDS) # Original code\n    \n    # Plot loss descrease history # \u7ed8\u5236loss\u4e0b\u964d\u66f2\u7ebf\n    epochs = EPOCHS;plot_history(histories[0], 'loss', epochs);\n\nprint('Everything End at: '+ time.strftime(\"%Y-%m-%d-%H_%M_%S\", time.localtime()))","bc730a1f":"# Confusion Matrix and Validation Score   \nprint('Evaluating:'+time.strftime(\"%Y-%m-%d-%H_%M_%S\", time.localtime()))\n\n# # Confusion Matrix and Validation Score \u6df7\u6dc6\u77e9\u9635 \u4e0e \u9a8c\u8bc1\u5206\u6570\u751f\u6210\ndef display_confusion_matrix(cmat, score, precision, recall):\n    plt.figure(figsize=(15,15))\n    ax = plt.gca()\n    ax.matshow(cmat, cmap='Reds')\n    ax.set_xticks(range(len(CLASSES)))\n    ax.set_xticklabels(CLASSES, fontdict={'fontsize': 7})\n    plt.setp(ax.get_xticklabels(), rotation=45, ha=\"left\", rotation_mode=\"anchor\")\n    ax.set_yticks(range(len(CLASSES)))\n    ax.set_yticklabels(CLASSES, fontdict={'fontsize': 7})\n    plt.setp(ax.get_yticklabels(), rotation=45, ha=\"right\", rotation_mode=\"anchor\")\n    titlestring = \"\"\n    if score is not None:\n        titlestring += 'f1 = {:.3f} '.format(score)\n    if precision is not None:\n        titlestring += '\\nprecision = {:.3f} '.format(precision)\n    if recall is not None:\n        titlestring += '\\nrecall = {:.3f} '.format(recall)\n    if len(titlestring) > 0:\n        ax.text(101, 1, titlestring, fontdict={'fontsize': 18, 'horizontalalignment':'right', 'verticalalignment':'top', 'color':'#804040'})\n    plt.show()\n    \nall_labels = []; all_prob = []; all_pred = []\nkfold = KFold(FOLDS, shuffle = True, random_state = SEED)\nfor j, (trn_ind, val_ind) in enumerate( kfold.split(TRAINING_FILENAMES) ):\n    print('    Inferring fold',j+1,'validation images...'+time.strftime(\"%Y-%m-%d-%H_%M_%S\", time.localtime()))\n    VAL_FILES = list(pd.DataFrame({'TRAINING_FILENAMES': TRAINING_FILENAMES}).loc[val_ind]['TRAINING_FILENAMES'])\n    NUM_VALIDATION_IMAGES = count_data_items(VAL_FILES)\n    cmdataset = get_validation_dataset(load_dataset(VAL_FILES, labeled = True, ordered = True))\n    images_ds = cmdataset.map(lambda image, label: image)\n    labels_ds = cmdataset.map(lambda image, label: label).unbatch()\n    all_labels.append( next(iter(labels_ds.batch(NUM_VALIDATION_IMAGES))).numpy() ) # get everything as one batch\n\n    if quick_test or no_folds:\n        prob = models[0].predict(images_ds)\n        all_prob.append( prob )\n        all_pred.append( np.argmax(prob, axis=-1) )\n        break\n    else:\n        prob = models[j].predict(images_ds)\n        all_prob.append( prob )\n        all_pred.append( np.argmax(prob, axis=-1) )\n\nprint('Calculating Scores:'+time.strftime(\"%Y-%m-%d-%H_%M_%S\", time.localtime()))\ncm_correct_labels = np.concatenate(all_labels)\ncm_probabilities = np.concatenate(all_prob)\ncm_predictions = np.concatenate(all_pred)\n# show predicted labels\nif quick_test or no_folds:\n    print(\"    Correct   labels: \", cm_correct_labels.shape, cm_correct_labels[0:10]);\n    print(\"    Predicted labels: \", cm_predictions.shape, cm_predictions[0:10]);\nelse: \n    print(\"    Correct   labels: \", cm_correct_labels.shape, cm_correct_labels);\n    print(\"    Predicted labels: \", cm_predictions.shape, cm_predictions);\n# cal cmat, precision, recall\ncmat = confusion_matrix(cm_correct_labels, cm_predictions, labels=range(len(CLASSES)))\nscore = f1_score(cm_correct_labels, cm_predictions, labels=range(len(CLASSES)), average='macro')\nprecision = precision_score(cm_correct_labels, cm_predictions, labels=range(len(CLASSES)), average='macro')\nrecall = recall_score(cm_correct_labels, cm_predictions, labels=range(len(CLASSES)), average='macro')\\\n# display confusion matrix\nif not no_training_mode:\n    display_confusion_matrix(cmat, score, precision, recall)\n\nprint('f1 score: {:.3f}, precision: {:.3f}, recall: {:.3f}'.format(score, precision, recall));   ","e75065b7":"# Flower Classification with Data Augmentations & Densenet201\nWe added and tested random contrast and bright change as data augmentation methods in this notebook.\n\nThis notebook is based on this work, thire augmentation methods is kept.\nhttps:\/\/www.kaggle.com\/cdeotte\/rotation-augmentation-gpu-tpu-0-96","dc2e4f31":"# Test Area\nuseless"}}