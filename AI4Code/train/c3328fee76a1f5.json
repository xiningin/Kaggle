{"cell_type":{"dffd8304":"code","76092087":"code","3ad045ab":"code","c52e8d06":"code","3b7d2ef2":"code","0f7a6916":"code","a4284c1a":"code","74533563":"code","47b7ead7":"code","7dabb757":"code","336f2236":"code","eb090ed2":"code","c37aa0ae":"code","037dbba6":"code","86fb921b":"code","7095a0b8":"code","57959a79":"code","8edb64f1":"code","81ac48ba":"code","82a99b1b":"code","0e62b138":"code","6932d73c":"code","8c4a81b2":"markdown","aa93cbc7":"markdown","badac9dd":"markdown","0c460974":"markdown","7013d6fc":"markdown","228aac0a":"markdown","0e6de556":"markdown","8d37939b":"markdown","05abf9d0":"markdown"},"source":{"dffd8304":"import numpy as np\nimport pandas as pd\nfrom pathlib import Path\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n        \nimport matplotlib.pyplot as plt\nimport optuna\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error, roc_auc_score\nfrom sklearn.ensemble import RandomForestRegressor\nfrom xgboost import XGBRegressor\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import StandardScaler","76092087":"train = pd.read_csv('..\/input\/tpssep2021dataset10folds\/train_10_folds.csv', index_col='id')\nprint(train.shape)\ntrain.head()","3ad045ab":"train.describe()","c52e8d06":"test = pd.read_csv('\/kaggle\/input\/tabular-playground-series-sep-2021\/test.csv', index_col='id')\nprint(test.shape)\ntest.head()","3b7d2ef2":"test.describe()","0f7a6916":"submission = pd.read_csv('\/kaggle\/input\/tabular-playground-series-sep-2021\/sample_solution.csv')\nsubmission.head()","a4284c1a":"# Adding the number of missing values in a row as a feature increases the score significantly\ntrain[\"missing_value_cnt\"] = train.isnull().sum(axis=1)\ntest[\"missing_value_cnt\"] = test.isnull().sum(axis=1)\n\ntrain.head()","74533563":"def imputation(X_train, X_valid, X_test = None):\n    imputer = SimpleImputer(strategy='mean')\n\n    imputed_X_train = pd.DataFrame(imputer.fit_transform(X_train))\n    imputed_X_valid = pd.DataFrame(imputer.transform(X_valid))\n    imputed_X_test = None\n\n    # Imputation removed column names; put them back\n    imputed_X_train.columns = X_train.columns\n    imputed_X_valid.columns = X_valid.columns\n    \n    if X_test is not None:\n        imputed_X_test = pd.DataFrame(imputer.transform(X_test))\n        imputed_X_test.columns = X_test.columns\n    \n    return imputed_X_train, imputed_X_valid, imputed_X_test\n","47b7ead7":"def feature_scaling(X_train, X_valid, X_test = None):\n    standardScaler = StandardScaler()\n    \n    scaled_X_train = pd.DataFrame(standardScaler.fit_transform(X_train))\n    scaled_X_valid = pd.DataFrame(standardScaler.transform(X_valid))\n    scaled_X_test = None\n    \n    # Scaling removed column names; put them back\n    scaled_X_train.columns = X_train.columns\n    scaled_X_valid.columns = X_valid.columns\n    \n    \n    if X_test is not None:\n        scaled_X_test = pd.DataFrame(standardScaler.transform(X_test))\n        scaled_X_test.columns = X_test.columns\n    \n    return scaled_X_train, scaled_X_valid, scaled_X_test","7dabb757":"# def objective(trial):\n    \n#     y = train.claim\n#     X = train.drop(columns = ['claim'])\n#     X_train, X_valid, y_train, y_valid = train_test_split(X, y, train_size = 0.8, random_state = 1234)\n    \n#     # Perform imputation\n#     imputed_X_train, imputed_X_valid, _ = imputation(X_train, X_valid)\n    \n#     # Perform Feature Scaling\n#     scaled_X_train, scaled_X_valid, _ = feature_scaling(imputed_X_train, imputed_X_valid)  \n\n#     X_train = scaled_X_train.copy()\n#     X_valid = scaled_X_valid.copy()\n\n#     params = {\n#         \"n_estimators\": trial.suggest_int(\"n_estimators\", 300, 10000),\n#         \"max_depth\": trial.suggest_int(\"max_depth\", 3, 12),\n#         \"min_child_weight\": trial.suggest_int(\"min_child_weight\", 5, 12),\n#         \"learning_rate\": trial.suggest_float(\"learning_rate\", 1e-3, 0.5, log=True),\n#         \"gamma\": trial.suggest_float(\"gamma\", 1e-9, 1e-5, log=True),\n#         \"reg_lambda\": trial.suggest_float(\"lambda\", 1e-8, 1e-1, log=True),\n#         \"reg_alpha\": trial.suggest_float(\"alpha\", 1e-8, 1e-1, log=True),\n#         \"subsample\": trial.suggest_float(\"subsample\", 0.2, 1.0),\n#         \"colsample_bytree\": trial.suggest_float(\"colsample_bytree\", 0.2, 1.0),\n#         \"grow_policy\": trial.suggest_categorical(\"grow_policy\", [\"depthwise\", \"lossguide\"])\n#     }\n\n#     xgb_regressor = XGBRegressor(**params,\n#                                  tree_method=\"gpu_hist\",\n#                                  random_state=1234,\n#                                  gpu_id=0,\n#                                  predictor=\"gpu_predictor\",\n#                                  verbosity=0)\n\n#     pruning_callback = optuna.integration.XGBoostPruningCallback(trial, \"validation_0-rmse\")\n    \n#     xgb_regressor.fit(X_train, \n#                       y_train,\n#                       eval_set=[(X_valid, y_valid)],\n#                       eval_metric=\"rmse\",\n#                       early_stopping_rounds=15,\n#                       verbose=False,\n#                       callbacks=[pruning_callback])\n\n#     y_pred = xgb_regressor.predict(X_valid)\n#     roc_auc = roc_auc_score(y_valid, y_pred)\n\n#     return roc_auc","336f2236":"# pruner = optuna.pruners.MedianPruner(n_warmup_steps=15)\n# study = optuna.create_study(pruner= pruner, study_name=\"xgbr-study\", direction=\"maximize\")\n# study.optimize(objective, n_trials=100, timeout=3600)","eb090ed2":"# print(\"Number of finished trials: \", len(study.trials))\n# trial = study.best_trial\n# print(\"Best trial validation score: {}\".format(trial.value))\n\n# print(\"The best parameters are: \")\n# study.best_params","c37aa0ae":"best_params = {'n_estimators': 9727,\n 'max_depth': 5,\n 'min_child_weight': 6,\n 'learning_rate': 0.011278075450219378,\n 'gamma': 1.9201053461331828e-07,\n 'lambda': 3.2282518444851405e-06,\n 'alpha': 1.2871612752393361e-06,\n 'subsample': 0.604103572661558,\n 'colsample_bytree': 0.9805549632981628,\n 'grow_policy': 'lossguide'}\n\nprint(best_params)","037dbba6":"all_test_predictions = []\nvalid_predictions = pd.DataFrame(np.zeros(train.index.shape), index = train.index, columns=['XGB_preds'])\n# print(valid_predictions.shape)\nauc_scores = []\n\nfor fold in range(10):\n    X_train =  train[train.fold != fold]\n    X_valid = train[train.fold == fold]\n    X_test = test.copy()\n    \n    valid_ids = X_valid.index.tolist()\n\n    y_train = X_train.claim\n    y_valid = X_valid.claim\n    \n    X_train = X_train.drop(columns=['claim', 'fold'])\n    X_valid = X_valid.drop(columns=['claim', 'fold'])\n    \n    # Perform imputation\n    X_train, X_valid, X_test = imputation(X_train, X_valid, X_test)\n\n    # Perform Feature Scaling\n    X_train, X_valid, X_test = feature_scaling(X_train, X_valid, X_test) \n    \n    model = XGBRegressor(**best_params,\n                         verbosity = 0,\n                         tree_method=\"gpu_hist\",\n                         random_state=1234,\n                         predictor=\"gpu_predictor\")\n\n    model.fit(X_train, y_train)\n    valid_preds = model.predict(X_valid)\n    test_preds = model.predict(X_test)\n    all_test_predictions.append(test_preds)\n    valid_predictions.loc[valid_ids, 'XGB_preds'] = valid_preds\n    \n    roc_auc = roc_auc_score(y_valid, valid_preds)\n    print(\"Validation score for fold {}: {}\".format(fold, roc_auc))\n    auc_scores.append(roc_auc)\n\nprint(\"Validation scores mean : {} and Standard deviation : {}\".format(np.mean(auc_scores), np.std(auc_scores)))","86fb921b":"valid_predictions = valid_predictions.reset_index()\nvalid_predictions.columns = [\"id\", \"XGB_preds\"]\nvalid_predictions.to_csv(\"XGB_train_predictions.csv\", index=False)","7095a0b8":"print(valid_predictions.shape)\nvalid_predictions.head()","57959a79":"submission.claim = np.mean(np.array(all_test_predictions), axis=0)\n# submission.columns = [\"id\", \"XGB_preds\"]\nsubmission.to_csv(\"XGB_test_predictions.csv\", index=False)","8edb64f1":"print(submission.shape)\nsubmission.head()","81ac48ba":"# X_train = train.copy()\n# X_test = test.copy()\n\n# y_train = train.claim\n# X_train = X_train.drop(columns = ['claim'])\n\n# # Perform imputation\n# imputed_X_train, imputed_X_test, _ = imputation(X_train, X_test)\n\n# # Perform Feature Scaling\n# scaled_X_train, scaled_X_test, _ = feature_scaling(imputed_X_train, imputed_X_test) ","82a99b1b":"# model = XGBRegressor(**best_params,\n#                      verbosity = 0,\n#                      tree_method=\"gpu_hist\",\n#                      random_state=1234,\n#                      predictor=\"gpu_predictor\")\n\n# model.fit(scaled_X_train, y_train)\n# test_predictions = model.predict(scaled_X_test)","0e62b138":"# print(roc_auc_score(y_train, model.predict(scaled_X_train)))","6932d73c":"# submission['claim'] = test_predictions\n# submission.to_csv('rf_output.csv', index = False)","8c4a81b2":"## Submission","aa93cbc7":"## Training model with 10 Fold Cross Validation","badac9dd":"## Import Necessary Libraries","0c460974":"## HyperParameter Tuning using Optuna","7013d6fc":"## Read the data files","228aac0a":"## Training Model with Whole Training Data","0e6de556":"## Introducing Additional Features","8d37939b":"## Feature Scaling","05abf9d0":"## Imputation for Handling Missing Values"}}