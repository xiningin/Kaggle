{"cell_type":{"ec67955d":"code","ec5c0646":"code","cb77918e":"code","2d559c97":"code","90431c1b":"code","1b1f0e1c":"code","18ff1b88":"code","5171b051":"code","c699ee74":"code","b830878a":"code","cd495c0e":"code","1406ccb4":"code","41a4fe35":"code","06559f0f":"code","ef1d34c7":"code","f99224a7":"code","7f6aea64":"code","c77671dd":"code","6f5a51c3":"code","8c8b7110":"code","926cb01d":"code","6d57316a":"code","d6d3d8f6":"code","e4d8ecd4":"code","484a691a":"code","f4c91e84":"code","c74d98ec":"code","128d71da":"code","d12616d3":"code","33649333":"code","baa03906":"code","ffbf3bcd":"code","7783034b":"code","5366c299":"code","5f43f453":"markdown","f449132e":"markdown","48663542":"markdown"},"source":{"ec67955d":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","ec5c0646":"# train: https:\/\/raw.githubusercontent.com\/qodatecnologia\/titanic-dataset\/master\/train.csv\n# test: https:\/\/raw.githubusercontent.com\/qodatecnologia\/titanic-dataset\/master\/test.csv\nimport pandas as pd\n\ndf_train = pd.read_csv('..\/input\/titanic\/train.csv')\ndf_test = pd.read_csv('..\/input\/titanic\/test.csv')\n\nprint(df_train.info())","cb77918e":"# Bibliotecas\nimport pandas as pd \nimport matplotlib \nimport numpy as np \nimport scipy as sp \nimport IPython\nfrom IPython import display \nimport sklearn \nimport random\nimport time\nimport warnings\nwarnings.filterwarnings('ignore')","2d559c97":"# Modelos preditivos\nfrom sklearn import svm, tree, linear_model, neighbors, naive_bayes, ensemble, discriminant_analysis, gaussian_process\nfrom xgboost import XGBClassifier\n# Auxiliares\nfrom sklearn.preprocessing import OneHotEncoder, LabelEncoder\nfrom sklearn import feature_selection\nfrom sklearn import model_selection\nfrom sklearn import metrics\n# DataViz\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\nimport matplotlib.pylab as pylab\nimport seaborn as sns\nfrom pandas.plotting import scatter_matrix\n\n# Dataviz Config\nmpl.style.use('ggplot')\nsns.set_style('white')\npylab.rcParams['figure.figsize'] = 12,8","90431c1b":"data_raw = pd.read_csv('..\/input\/titanic\/train.csv')\ndata_val  = pd.read_csv('..\/input\/titanic\/test.csv')\n\n# criamos copia destes dados\ndata1 = data_raw.copy(deep = True)\n# e concatenamos os datasets para altera-los juntos, caso necess\u00e1rio\ndata_cleaner = [data1, data_val]","1b1f0e1c":"print('Null values TREINO:\\n', data1.isnull().sum())\nprint()\nprint('Null values TESTE:\\n', data_val.isnull().sum())","18ff1b88":"# LIMPEZA DE DADOS\nfor dataset in data_cleaner:    \n    #MEDIANA\n    dataset['Age'].fillna(dataset['Age'].median(), inplace = True)\n\n    #MODA (em caso de uso da mediana, erro de tipo: str)\n    dataset['Embarked'].fillna(dataset['Embarked'].mode()[0], inplace = True)\n\n    #MEDIANA\n    dataset['Fare'].fillna(dataset['Fare'].median(), inplace = True)\n    \n# dropar colunas para dataset de treino\ndrop_column = ['PassengerId','Cabin', 'Ticket']\ndata1.drop(drop_column, axis=1, inplace = True)\n\nprint(data1.isnull().sum())\nprint(\"-\"*10)\nprint(data_val.isnull().sum())","5171b051":"# FEATURE ENGINEERING: cria\u00e7\u00e3o de features\nfor dataset in data_cleaner:    \n    # nova feature: FamilySize\n    dataset['FamilySize'] = dataset ['SibSp'] + dataset['Parch'] + 1\n    \n    #nova feature: IsAlone\n    dataset['IsAlone'] = 1 #initialize to yes\/1 is alone\n    dataset['IsAlone'].loc[dataset['FamilySize'] > 1] = 0 # now update to no\/0 if family size is greater than 1\n\n    # nova feature: \"Title\"\n    # Divis\u00e3o r\u00e1pida para \"Name\" com m\u00e9todo .split()\n    dataset['Title'] = dataset['Name'].str.split(\", \", expand=True)[1].str.split(\".\", expand=True)[0]\n\n\n    # nova feature: FareBin, \"cortamos os dados em 4(quartil)\"; Segmentamos\/classificamos em 4 grupos\n    dataset['FareBin'] = pd.qcut(dataset['Fare'], 4)\n\n    #nova feature: AgeBin para melhor distribuir os dados de idade por grupos, neste caso, 5. Segmentamos\/classificamos em 5 grupos\n    dataset['AgeBin'] = pd.cut(dataset['Age'].astype(int), 5)\n\ndata1.sample(10)","c699ee74":"# Limpar Title names raros(diferentes de Mr, Miss, Mrs, Master)\nprint(data1['Title'].value_counts())\ntitle_names = (data1['Title'].value_counts() < 10) # S\u00e9rie de true\/false do TitleName como index, quando aparece menos que 10x no dataset","b830878a":"title_names","cd495c0e":"# Fun\u00e7\u00e3o lambda para economizar linhas de c\u00f3digo e substituir por \"Misc\" qualquer TitleName raro, com menos de 10 apari\u00e7\u00f5es\ndata1['Title'] = data1['Title'].apply(lambda x: 'Misc' if title_names.loc[x] == True else x)\nprint(data1['Title'].value_counts())","1406ccb4":"# Visualizar limpeza\ndata1.info()\nprint(\"====================================================================================\")\ndata_val.info()\nprint(\"====================================================================================\")\ndata1.sample(10)","41a4fe35":"# Converter\n\nlabel = LabelEncoder()\nfor dataset in data_cleaner:    \n    dataset['Sex_Code'] = label.fit_transform(dataset['Sex'])\n    dataset['Embarked_Code'] = label.fit_transform(dataset['Embarked'])\n    dataset['Title_Code'] = label.fit_transform(dataset['Title'])\n    dataset['AgeBin_Code'] = label.fit_transform(dataset['AgeBin'])\n    dataset['FareBin_Code'] = label.fit_transform(dataset['FareBin'])","06559f0f":"# Definir variavel target\nTarget = ['Survived']","ef1d34c7":"# Definir features ja alteradas\ndata1_x = ['Sex','Pclass', 'Embarked', 'Title','SibSp', 'Parch', 'Age', 'Fare', 'FamilySize', 'IsAlone'] # Nomes para gr\u00e1ficos\ndata1_x_calc = ['Sex_Code','Pclass', 'Embarked_Code', 'Title_Code','SibSp', 'Parch', 'Age', 'Fare'] # modelos preditivos\ndata1_xy =  Target + data1_x\ndata1_xy","f99224a7":"# Vari\u00e1veis a substituir, com LabelEncoder\ndata1_x_bin = ['Sex_Code','Pclass', 'Embarked_Code', 'Title_Code', 'FamilySize', 'AgeBin_Code', 'FareBin_Code']\ndata1_xy_bin = Target + data1_x_bin\ndata1_xy_bin","7f6aea64":"# Vari\u00e1veis dummy\ndata1_dummy = pd.get_dummies(data1[data1_x])\ndata1_x_dummy = data1_dummy.columns.tolist()\ndata1_xy_dummy = Target + data1_x_dummy\ndata1_dummy.head()","c77671dd":"# Divis\u00e3o TREINO\/TESTE\n\ntrain1_x, test1_x, train1_y, test1_y = model_selection.train_test_split(data1[data1_x_calc], data1[Target], random_state = 0)\ntrain1_x_bin, test1_x_bin, train1_y_bin, test1_y_bin = model_selection.train_test_split(data1[data1_x_bin], data1[Target] , random_state = 0)\ntrain1_x_dummy, test1_x_dummy, train1_y_dummy, test1_y_dummy = model_selection.train_test_split(data1_dummy[data1_x_dummy], data1[Target], random_state = 0)\n\n\nprint(\"Dataset: {}\".format(data1.shape))\nprint(\"Treino: {}\".format(train1_x.shape))\nprint(\"Teste: {}\".format(test1_x.shape))\n\ntrain1_x_bin.head()","6f5a51c3":"data1_x","8c8b7110":"# Correla\u00e7\u00e3o por sobreviv\u00eancia\n# https:\/\/pandas.pydata.org\/pandas-docs\/stable\/generated\/pandas.DataFrame.groupby.html\nfor x in data1_x:\n    if data1[x].dtype != 'float64' :\n        print('Correla\u00e7\u00e3o sobreviv\u00eancia:', x)\n        print(data1[[x, Target[0]]].groupby(x, as_index=False).mean())\n        print('-'*10, '\\n')\n        \n\n#crosstabs: https:\/\/pandas.pydata.org\/pandas-docs\/stable\/generated\/pandas.crosstab.html\nprint(pd.crosstab(data1['Title'],data1[Target[0]]))","926cb01d":"data1[[x, Target[0]]]","6d57316a":"data1[[x, Target[0]]].groupby(x, as_index=False).mean()","d6d3d8f6":"# Distribui\u00e7\u00e3o por idade: sobreviventes\/n\u00e3o-sobreviventes\na = sns.FacetGrid( data1, hue = 'Survived', aspect=4 )\na.map(sns.kdeplot, 'Age', shade= True )\na.set(xlim=(0 , data1['Age'].max()))\na.add_legend()","e4d8ecd4":"# Comparativo Sexo\/Classe\/Idade\nh = sns.FacetGrid(data1, row = 'Sex', col = 'Pclass', hue = 'Survived')\nh.map(plt.hist, 'Age', alpha = .75)\nh.add_legend()","484a691a":"# Heatmap de Correla\u00e7\u00e3o\ndef correlation_heatmap(df):\n    _ , ax = plt.subplots(figsize =(14, 12))\n    colormap = sns.diverging_palette(220, 10, as_cmap = True)\n    \n    _ = sns.heatmap(\n        df.corr(), \n        cmap = colormap,\n        square=True, \n        cbar_kws={'shrink':.9 }, \n        ax=ax,\n        annot=True, \n        linewidths=0.1,vmax=1.0, linecolor='white',\n        annot_kws={'fontsize':12 }\n    )\n    \n    plt.title('Heatmap Correla\u00e7\u00e3o Pearson', y=1.05, size=15)\n\ncorrelation_heatmap(data1)","f4c91e84":"# Instanciar diversos algoritmos de CLASSIFICA\u00c7\u00c3O\nMLA = [\n    #Ensemble Methods\n    ensemble.AdaBoostClassifier(),\n    ensemble.BaggingClassifier(),\n    ensemble.ExtraTreesClassifier(),\n    ensemble.GradientBoostingClassifier(),\n    ensemble.RandomForestClassifier(),\n\n    #Gaussian Processes\n    gaussian_process.GaussianProcessClassifier(),\n    \n    #GLM\n    linear_model.LogisticRegressionCV(),\n    linear_model.PassiveAggressiveClassifier(),\n    linear_model.RidgeClassifierCV(),\n    linear_model.SGDClassifier(),\n    linear_model.Perceptron(),\n    \n    #Navies Bayes\n    naive_bayes.BernoulliNB(),\n    naive_bayes.GaussianNB(),\n    \n    #Nearest Neighbor\n    neighbors.KNeighborsClassifier(),\n    \n    #SVM\n    svm.SVC(probability=True),\n    svm.NuSVC(probability=True),\n    svm.LinearSVC(),\n    \n    #Trees    \n    tree.DecisionTreeClassifier(),\n    tree.ExtraTreeClassifier(),\n    \n    #Discriminant Analysis\n    discriminant_analysis.LinearDiscriminantAnalysis(),\n    discriminant_analysis.QuadraticDiscriminantAnalysis(),\n\n    #xgboost: http:\/\/xgboost.readthedocs.io\/en\/latest\/model.html\n    XGBClassifier()    \n    ]","c74d98ec":"# splitter http:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.model_selection.ShuffleSplit.html#sklearn.model_selection.ShuffleSplit\ncv_split = model_selection.ShuffleSplit(n_splits = 10, test_size = .3, train_size = .6, random_state = 0 ) \n\n# comparar m\u00e9tricas\nMLA_columns = ['MLA Nome', 'MLA Parametros','Acur\u00e1cia m\u00e9dia TREINO', 'Acur\u00e1cia m\u00e9dia TESTE', 'TEMPO UTILIZADO(m\u00e9dia)']\nMLA_compare = pd.DataFrame(columns = MLA_columns)\n\n# comparar predi\u00e7\u00f5es\nMLA_predict = data1[Target]\n\n# salvamos performance na tabela\nrow_index = 0","128d71da":"for alg in MLA:\n\n    # setamos nome do algoritmo e parametros\n    MLA_name = alg.__class__.__name__\n    MLA_compare.loc[row_index, 'MLA Nome'] = MLA_name\n    MLA_compare.loc[row_index, 'MLA Parametros'] = str(alg.get_params())\n    \n    # resultados com cross validation\n    cv_results = model_selection.cross_validate(alg, data1[data1_x_bin], data1[Target], cv=cv_split, return_train_score=True)\n\n    MLA_compare.loc[row_index, 'TEMPO UTILIZADO(m\u00e9dia)'] = cv_results['fit_time'].mean()\n    MLA_compare.loc[row_index, 'Acur\u00e1cia m\u00e9dia TREINO'] = cv_results['train_score'].mean()\n    MLA_compare.loc[row_index, 'Acur\u00e1cia m\u00e9dia TESTE'] = cv_results['test_score'].mean()     \n\n    # salvamos as predi\u00e7\u00f5es, algoritmo por algoritmo\n    alg.fit(data1[data1_x_bin], data1[Target])\n    MLA_predict[MLA_name] = alg.predict(data1[data1_x_bin])\n    \n    row_index+=1","d12616d3":"MLA_compare.sort_values(by = ['Acur\u00e1cia m\u00e9dia TESTE'], ascending = False, inplace = True)\nMLA_compare","33649333":"MLA_predict","baa03906":"#barplot seaborn https:\/\/seaborn.pydata.org\/generated\/seaborn.barplot.html\nsns.barplot(x='Acur\u00e1cia m\u00e9dia TESTE', y = 'MLA Nome', data = MLA_compare, color = 'm')","ffbf3bcd":"#barplot seaborn https:\/\/seaborn.pydata.org\/generated\/seaborn.barplot.html\nsns.barplot(x='Acur\u00e1cia m\u00e9dia TESTE', y = 'MLA Nome', data = MLA_compare, color = 'm')\n\n# pyplot: https:\/\/matplotlib.org\/api\/pyplot_api.html\nplt.title('Score por Algoritmos \\n')\nplt.xlabel('Accur\u00e1cia(%)')\nplt.ylabel('Algoritmo')","7783034b":"submit_xgb = XGBClassifier()\nsubmit_xgb.fit(data1[data1_x_bin], data1[Target])\ndata_val['Survived'] = submit_xgb.predict(data_val[data1_x_bin])","5366c299":"#submit file\nsubmit = data_val[['PassengerId','Survived']]\nsubmit.to_csv(\"submit.csv\", index=False)","5f43f453":"##5. Validar e implementar modelo preditivo","f449132e":"## 4. Modelo preditivo\n![texto alternativo](https:\/\/scikit-learn.org\/stable\/_static\/ml_map.png)\n\n","48663542":"##3. An\u00e1lise explorat\u00f3ria: estat\u00edstica e dataviz!"}}