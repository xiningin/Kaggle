{"cell_type":{"3a073824":"code","af1a8225":"code","0e2b6f1b":"code","6a5e4698":"code","2a39b817":"code","eb96df7b":"code","be3299c8":"code","10d63b0a":"code","c0162c05":"code","596ab3fa":"code","a0abb6dc":"code","0bfb0f5f":"code","41e4a47b":"code","0faf6bbd":"code","497b6329":"code","d819be1e":"code","6e772db3":"code","2d2a3a04":"code","a3bc4e3a":"code","953b5c25":"code","38ecdadb":"code","d467433b":"code","5192a531":"code","2de49b12":"code","edb5196d":"code","34fcebb9":"code","1893f472":"code","9e96b127":"code","3798dd11":"code","62b6e1b3":"code","5e92c378":"code","4051a51b":"code","605150ec":"code","4719a5bf":"code","5b2a5728":"code","0055e150":"code","ab9bf303":"code","d508d40e":"code","33cf6aba":"code","6274eb4b":"code","21fdd628":"code","5e3c6eae":"code","79780afe":"code","00b2a9e9":"code","4dd7c40d":"code","77cb8b20":"code","d85f1ad3":"code","a3ece144":"code","1fddd94b":"code","2e856f4a":"code","d55bdf59":"code","5d77b40f":"code","302f4660":"code","0073f965":"code","40eb596a":"code","22356c4b":"code","fd2d5f04":"code","48941e0c":"code","62dfd151":"code","ca3b00d8":"code","7d62e173":"code","9fc52875":"code","092bb1d4":"code","77be6330":"code","2001c721":"code","cc47ccc3":"code","3fd735a6":"code","e222b876":"markdown","95370389":"markdown","f07a436c":"markdown","9b2c35ca":"markdown","b45cdb52":"markdown","f44fa3b9":"markdown","c5716104":"markdown","5897f9ff":"markdown","19a8c44a":"markdown","dc61e06a":"markdown","38311e8c":"markdown","bdc34e2d":"markdown","0ad0df97":"markdown","67d35295":"markdown","a3ba1cf5":"markdown","1012c93c":"markdown","af468e91":"markdown","2c323db8":"markdown","85c8ab24":"markdown","3a9828fb":"markdown","e5244902":"markdown","227bd441":"markdown","20e24dfe":"markdown","74bccd48":"markdown","37a25de5":"markdown","abd466d4":"markdown","c5150f40":"markdown","3502bf42":"markdown","161afe60":"markdown","5adcc4cd":"markdown","25066c92":"markdown","29f23853":"markdown","89e55712":"markdown","5421cdb3":"markdown"},"source":{"3a073824":"# Supress Warnings\n\nimport warnings\nwarnings.filterwarnings('ignore')","af1a8225":"# Importing the Libraries\nimport pandas as pd\nimport numpy as np\nimport random as rnd\n\n# Importing Library for visualization\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\n\n","0e2b6f1b":"#Importing the train dataset and reading\ntrain = pd.read_csv('..\/input\/train.csv', )\n\ntrain.head()\n","6a5e4698":"#Importing and reading the test dataset\n\ntest = pd.read_csv('..\/input\/test.csv')\n\ntest.head()","2a39b817":"# Getting a summary of the dataframe using 'describe()'\ntrain.describe()\n","eb96df7b":"test.head()\n","be3299c8":"# Check the number of rows and columns in the dataframe\n\ntrain.shape\n","10d63b0a":"test.shape","c0162c05":"# Check the column-wise info of the dataframe\n\ntrain.info()\n","596ab3fa":"test.info()","a0abb6dc":"# Get a summary of the dataframe using 'describe()'\n\ntrain.describe()\n","0bfb0f5f":"test.describe()","41e4a47b":"train['Survived'].value_counts(normalize=True)\n","0faf6bbd":"train['Survived'].groupby(train['Pclass']).mean","497b6329":"\n\nsns.barplot(x='Pclass', y='Survived', data=train)\n","d819be1e":"train['Sex'].value_counts(normalize=True)\n\ntrain['Survived'].groupby(train['Sex']).mean()\n","6e772db3":"sns.barplot(x='Sex', y='Survived', data=train)\nplt.show()","2d2a3a04":"train['Survived'].groupby(train['SibSp']).mean()\n","a3bc4e3a":"sns.barplot(x='SibSp', y='Survived', data=train)\n","953b5c25":"train['Survived'].groupby(train['Parch']).mean()\n","38ecdadb":"sns.barplot(x='Parch', y='Survived', data=train)\n","d467433b":"sns.barplot(x='Embarked', y='Survived', data=train)","5192a531":"sns.barplot(x='Pclass', y='Survived', data=train)\n","2de49b12":"# Get the column-wise Null count using 'is.null()' alongwith the 'sum()' function\ntrain.isnull().sum()\n","edb5196d":"train = train.drop(['Ticket','Name','Cabin'],axis=1)\n","34fcebb9":"round(100*(train.isnull().sum()\/len(train.index)), 2)","1893f472":"test.isnull().sum()","9e96b127":"# Get the percentages by dividing the sum obtained previously by the total length, multiplying it by 100 and rounding it off to\n# two decimal places\n\nround(100*(train.isnull().sum()\/len(train.index)), 2)","3798dd11":"test = test.drop(columns=['Cabin','Ticket','Name'],axis=1)\n# dropping the cabin column as i am not going use this for predicting the survived","62b6e1b3":"#calculating the mean of the age of train data set and test\nAge_mean= train['Age'].mean()\ntrain['Age'] = train['Age'].fillna(Age_mean)\n#","5e92c378":"#most occuring embarked and filling the missing row of embarked\nsns.barplot(x='Embarked', y='Survived', data=train)\n","4051a51b":"#As we can see Embarked C is high so can fill missing values as C\ntrain['Embarked'] = train['Embarked'].fillna('S')\n","605150ec":"# checking again the null values now the train dataset is clean and can do the same to the test dataset\nround(100*(train.isnull().sum()\/len(train.index)), 2)","4719a5bf":"round(100*(test.isnull().sum()\/len(test.index)), 2)","5b2a5728":"#Imputing mean values in the fare column\nFare_mean = test['Fare'].mean()\ntest['Fare'] = test['Fare'].fillna(Fare_mean)","0055e150":"round(100*(test.isnull().sum()\/len(test.index)), 2)","ab9bf303":"gender={'male':0,'female':1}\n\ntrain['Sex']=train['Sex'].apply(lambda x:gender[x])","d508d40e":"Embarked_map={'S':1,'C':2,'Q':3}","33cf6aba":"train['Embarked']=train['Embarked'].apply(lambda x:Embarked_map[x])","6274eb4b":"test['Embarked']=test['Embarked'].apply(lambda x:Embarked_map[x])","21fdd628":"test['Sex']=test['Sex'].apply(lambda x:gender[x])","5e3c6eae":"test['Sex'].head()","79780afe":"test['FamilySize'] =  test['SibSp'] + test['Parch']\n\n   ","00b2a9e9":"#Imputing mean values in the age column\nAge_mean = test['Age'].mean()\ntest['Age'] = test['Age'].fillna(Age_mean)","4dd7c40d":"train['AgeBand'] = pd.cut(train['Age'], 5)\ntrain[['AgeBand', 'Survived']].groupby(['AgeBand'], as_index=False).mean().sort_values(by='AgeBand', ascending=True)\n","77cb8b20":"combine= [train,test]\nfor dataset in combine:   \n    dataset.loc[ dataset['Age'] <= 16, 'Age'] = 0\n    dataset.loc[(dataset['Age'] > 16) & (dataset['Age'] <= 32), 'Age'] = 1\n    dataset.loc[(dataset['Age'] > 32) & (dataset['Age'] <= 48), 'Age'] = 2\n    dataset.loc[(dataset['Age'] > 48) & (dataset['Age'] <= 64), 'Age'] = 3\n    dataset.loc[ dataset['Age'] > 64, 'Age']\ntest.head()\n","d85f1ad3":"test.head()","a3ece144":"train['FareBand'] = pd.qcut(train['Fare'], 4)\ntrain[['FareBand', 'Survived']].groupby(['FareBand'], as_index=False).mean().sort_values(by='FareBand', ascending=True)\n","1fddd94b":"\nfor dataset in combine:\n    dataset.loc[ dataset['Fare'] <= 7.91, 'Fare'] = 0\n    dataset.loc[(dataset['Fare'] > 7.91) & (dataset['Fare'] <= 14.454), 'Fare'] = 1\n    dataset.loc[(dataset['Fare'] > 14.454) & (dataset['Fare'] <= 31), 'Fare']   = 2\n    dataset.loc[ dataset['Fare'] > 31, 'Fare'] = 3\n    dataset['Fare'] = dataset['Fare'].astype(int)\n\ntrain = train.drop(['FareBand'], axis=1)\ncombine = [train, test]\n    \ntrain.head(10)\n","2e856f4a":"for dataset in combine:\n    dataset['FamilySize'] = dataset['SibSp'] + dataset['Parch'] + 1\n    \npd.crosstab(train['FamilySize'], train['Survived']).plot(kind='bar', stacked=True, title=\"Survived by family size\")","d55bdf59":"train.head()","5d77b40f":"train=train[['PassengerId','Age','Survived','Pclass','Sex','Fare','FamilySize','Embarked']]","302f4660":" from sklearn.model_selection import train_test_split\n\npredictors = train.drop(['Survived', 'PassengerId'], axis=1)\ntarget = train[\"Survived\"]\nx_train, x_val, y_train, y_val = train_test_split(predictors, target, test_size = 0.2, random_state = 0)\n","0073f965":"# Gaussian Naive Bayes\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.metrics import accuracy_score\n\ngaussian = GaussianNB()\ngaussian.fit(x_train, y_train)\ny_pred = gaussian.predict(x_val)\nacc_gaussian = round(accuracy_score(y_pred, y_val) * 100, 2)\nprint(acc_gaussian)\n","40eb596a":"# Logistic Regression\nfrom sklearn.linear_model import LogisticRegression\n\nlogreg = LogisticRegression()\nlogreg.fit(x_train, y_train)\ny_pred = logreg.predict(x_val)\nacc_logreg = round(accuracy_score(y_pred, y_val) * 100, 2)\nprint(acc_logreg)\n","22356c4b":"# Support Vector Machines\nfrom sklearn.svm import SVC\n\nsvc = SVC()\nsvc.fit(x_train, y_train)\ny_pred = svc.predict(x_val)\nacc_svc = round(accuracy_score(y_pred, y_val) * 100, 2)\nprint(acc_svc)\n","fd2d5f04":"# Linear SVC\nfrom sklearn.svm import LinearSVC\n\nlinear_svc = LinearSVC()\nlinear_svc.fit(x_train, y_train)\ny_pred = linear_svc.predict(x_val)\nacc_linear_svc = round(accuracy_score(y_pred, y_val) * 100, 2)\nprint(acc_linear_svc)\n","48941e0c":"#Decision Tree\nfrom sklearn.tree import DecisionTreeClassifier\n\ndecisiontree = DecisionTreeClassifier()\ndecisiontree.fit(x_train, y_train)\ny_pred = decisiontree.predict(x_val)\nacc_decisiontree = round(accuracy_score(y_pred, y_val) * 100, 2)\nprint(acc_decisiontree)\n","62dfd151":"from sklearn.ensemble import RandomForestClassifier\n\nrandomforest = RandomForestClassifier()\nrandomforest.fit(x_train, y_train)\ny_pred = randomforest.predict(x_val)\nacc_randomforest = round(accuracy_score(y_pred, y_val) * 100, 2)\nprint(acc_randomforest)\n","ca3b00d8":"from sklearn.neighbors import KNeighborsClassifier\n\nknn = KNeighborsClassifier()\nknn.fit(x_train, y_train)\ny_pred = knn.predict(x_val)\nacc_knn = round(accuracy_score(y_pred, y_val) * 100, 2)\nprint(acc_knn)\nfrom sklearn.linear_model import SGDClassifier\n\n\n","7d62e173":"from sklearn.linear_model import SGDClassifier\n\nsgd = SGDClassifier()\nsgd.fit(x_train, y_train)\ny_pred = sgd.predict(x_val)\nacc_sgd = round(accuracy_score(y_pred, y_val) * 100, 2)\nprint(acc_sgd)\n","9fc52875":"from sklearn.ensemble import GradientBoostingClassifier\n\ngbk = GradientBoostingClassifier()\ngbk.fit(x_train, y_train)\ny_pred = gbk.predict(x_val)\nacc_gbk = round(accuracy_score(y_pred, y_val) * 100, 2)\nprint(acc_gbk)\n","092bb1d4":"models = pd.DataFrame({\n    'Model': ['Support Vector Machines', 'KNN', 'Logistic Regression', \n              'Random Forest', 'Naive Bayes', 'Linear SVC', \n              'Decision Tree', 'Stochastic Gradient Descent', 'Gradient Boosting Classifier'],\n    'Score': [acc_svc, acc_knn, acc_logreg, \n              acc_randomforest, acc_gaussian,acc_linear_svc, acc_decisiontree,\n              acc_sgd, acc_gbk]})\nmodels.sort_values(by='Score', ascending=False)\n","77be6330":"#Checking test data\ntest.head()","2001c721":"#saving passenger id for submission\ntest_ids = test['PassengerId']\n","cc47ccc3":"#only taking the columns required for prediction\ntest=test[['Age','Pclass','Sex','Fare','FamilySize','Embarked']]\ntest_predictions =randomforest.predict(test)\n","3fd735a6":"submission = pd.DataFrame({\n        \"PassengerId\": test_ids,\n        \"Survived\":test_predictions })\n\nsubmission.to_csv('submission.csv', index=False)\n","e222b876":"8. Create Submission File\n","95370389":"High Fare has more chances of survival","f07a436c":"Let us create Age bands and determine correlations with Survived.\n\n","9b2c35ca":"Sex\n","b45cdb52":"Visualisation of columns which would help us in prediction the Survivalfor eg,Age,sex,Pclass,Fare","f44fa3b9":"let us plot and see how columns are related to each and to anlayse the survival rate","c5716104":"Now we can see the Age of train data set has 86 and cabin 327  null values.i will drop the cabin as it has high null values and will deal with itin next submission\nAnd will impute the mean values in the Age column","5897f9ff":"#In Sklearn we cannot pass the string so we have map integer values\nto the Gender(Male  as   0 and female as 1)","19a8c44a":"Titanic: Machine Learning from Disaster-Beginner\n","dc61e06a":"Importing data\nThe Python Pandas packages helps us work with our datasets. We start by acquiring the training and testing datasets into Pandas DataFrames. .\n\n","38311e8c":"It seems that for families from 1 to 4 people, family size increases survival rates. But for families of 5 and up, survival rates is much lower.\n\n","bdc34e2d":"Survived\nSo we can see that 62% of the people in the training set died. .\n\n","0ad0df97":"Pclass\n\nClass played a critical role in survival, as the survival rate decreased drastically for the lowest class. This variable is both useful and clean\n\n","67d35295":"We can see that Female has more survival chance compared to male","a3ba1cf5":" Modelling\nChoosing the best model\nSplitting the Training Data\nWe will use part of our training data (20% in this case) to test the accuracy of our different models.\n","1012c93c":"Fare :As we know the People with high fare had more chances of survival","af468e91":"Testing Different Models\nI will be testing the following models with my training data:\n\nGaussian Naive Bayes\nLogistic Regression\nSupport Vector Machines\nDecision Tree Classifier\nRandom Forest Classifier\nKNN or k-Nearest Neighbors\nStochastic Gradient Descent\nGradient Boosting Classifier\nFor each model, we set the model, fit it with 80% of our training data, predict for other 20% of the training data and check the accuracy.\nReferred from\nhttps:\/\/www.kaggle.com\/sisakk\/titanic-survival-prediction-model-building\n","2c323db8":"Let us deal with familysize","85c8ab24":"Titanic:This is my 2nd submission and kernel where i could improve my score from .765 to.794\n Here I worked on the Agegroup survival and familysize versus survival\nContents:\nImporting Required Libraries\nImporting and Analysing the Data\nData Visualization\nCleaning Data\nChoosing the Best Model\nCreating Submission File\nAny and all feedback is welcome!\n\n","3a9828fb":"Inspecting the dataframe's columns, shapes, variable types etc.","e5244902":" 2: Cleaning the Data\n\n-   Inspect Null values\nFind out the number of Null values in all the columns and rows.\nAlso, find the percentage of Null values in each column. Round-off \nthe percentages upto two decimal places.","227bd441":"Now we can see the Age of train data set has 177 and emabarked 2 null values.We need to impute the data for those null rows as these the required fields to predict the survival.\nCabin for this submission i will drop the column","20e24dfe":"As we need only numberical data to test the model and we have done the required cleaning by filling the missing values and converting the gender to 0 and 1","74bccd48":"Importing Required Libraries\n","37a25de5":"Ticket :this feature I am going to drop because it doesnt show any correlation to survival","abd466d4":"Testing Different Models\nI will be testing the following models with my training data:\n\nGaussian Naive Bayes\nLogistic Regression\nSupport Vector Machines\nDecision Tree Classifier\nRandom Forest Classifier\nKNN or k-Nearest Neighbors\nStochastic Gradient Descent\nGradient Boosting Classifier\nFor each model, we set the model, fit it with 80% of our training data, predict for other 20% of the training data and check the accuracy.\n\n","c5150f40":"SibSp\n","3502bf42":" Name Column i am going to drop as I do not see much correlation to survival","161afe60":"Cleaning the data by imputing the mean values in the missing age  ","5adcc4cd":"Parch\n","25066c92":"Splitting the Training Data\nWe will use part of our training data (22% in this case) to test the accuracy of our different models.\n\n","29f23853":"Cabin:this column i am going to drop as it has more null values maybe in next version and submission i will try to deal with it","89e55712":"Cleaning Test Data set and imputing the values in age and embarked column as it has null values and we need it for prediction","5421cdb3":"Sex is the one of feature which we will use to analyse the survival.We can see from the train dataset that 74 percent female survived.Hence,female had more survival chances"}}