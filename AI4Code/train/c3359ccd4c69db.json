{"cell_type":{"27ffe26c":"code","c5f7b4b0":"code","cd79a18d":"code","d687e445":"code","a4fc4424":"code","c847ef88":"code","7afa32e1":"code","a470b5c1":"code","c5c15d32":"code","a29a35fc":"code","4513c56f":"code","3e63bba8":"code","0b01e049":"code","75d794e8":"code","38966726":"code","660e3f43":"code","1bf89772":"code","acda3516":"code","48dc69e9":"code","d259b2df":"code","f391a484":"code","574a5bb9":"code","27e5f4d7":"code","6428ffe4":"code","96d06782":"markdown","9ed312c6":"markdown","e908cb2e":"markdown","f43ad482":"markdown","ea30aa79":"markdown","91f32473":"markdown","33ce10a3":"markdown"},"source":{"27ffe26c":"import pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport string\nfrom tqdm import tqdm\nimport re\n\n# nltk\nfrom nltk.tokenize import word_tokenize\nfrom nltk.corpus import stopwords\n\n\n# sklearn\nfrom sklearn.model_selection import train_test_split\n\n# keras and tensorflow\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\n\nfrom keras.models import Sequential\nfrom keras.layers import Embedding,LSTM,Dense,SpatialDropout1D\nfrom keras.initializers import Constant\nfrom keras.optimizers import Adam\n\nimport tensorflow as tf\n\nstop = set(stopwords.words('english'))","c5f7b4b0":"df_train = pd.read_csv('..\/input\/nlp-getting-started\/train.csv')\ndf_test = pd.read_csv('..\/input\/nlp-getting-started\/test.csv')","cd79a18d":"df_train.head()","d687e445":"df_train.tail()","a4fc4424":"# shape\nprint(f\"There are {df_train.shape[0]} rows and {df_train.shape[1]} columns. \")","c847ef88":"# Class Distribution\n# 0 (Non Disaster) is more than 1 (Disaster) Tweets\nclass_dist = df_train.target.value_counts()\nsns.barplot(class_dist.index,class_dist)","7afa32e1":"# Misssing vs Non Missing\n# 'keyword' & 'location' columns have missing values. Looks like 'location' column is very dirty so lets not use it. Lets also ignore 'keyword' column for now.\nnull_vals = df_train.isnull().sum()\nsns.barplot(null_vals.index,null_vals)","a470b5c1":"# Removing <> tags\n\ndef remove_spec(text):\n    text = re.sub('<.*?>+', '', text)\n    text = text.lower()\n    return text\n\n# Rmoving puntuctions\n\ndef remove_punctuation(text):\n    table = str.maketrans('','',string.punctuation)\n    return text.translate(table)\n\n# Removing URL\n\ndef remove_urls(text):\n    text = re.sub(r\"https?:\\\/\\\/t.co\\\/[A-Za-z0-9]+\",'',text)\n    return text\n\n# Removing Emojis\n\ndef remove_emoji(text):\n    emojis = re.compile(\"[\"\n                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n                           u\"\\U00002702-\\U000027B0\"\n                           u\"\\U000024C2-\\U0001F251\"\n                           \"]+\", flags=re.UNICODE)\n    \n    text = re.sub(emojis,'',text)\n    return text\n","c5c15d32":"df_train['cleaned_text'] = df_train['text'].apply(lambda x : remove_punctuation(x))\ndf_train['cleaned_text'] = df_train['cleaned_text'].apply(lambda x : remove_urls(x))\ndf_train['cleaned_text'] = df_train['cleaned_text'].apply(lambda x : remove_emoji(x))\ndf_train['cleaned_text'] = df_train['cleaned_text'].apply(lambda x : remove_spec(x))","a29a35fc":"# Creating Words Corpus\n\ndef create_corpus(dataset):\n    corpus = []\n    for review in tqdm(dataset['cleaned_text']):\n        words = [ word.lower() for word in word_tokenize(review) if (word.isalpha() == 1 ) & (word not in stop) ]\n        corpus.append(words)\n\n    return corpus\n\ncorpus = create_corpus(df_train)\n","4513c56f":"# Creating Embedding Dictionary\n\nembedding_dict={}\nwith open('..\/input\/glove-100d\/glove.6B.100d.txt','r', encoding='utf8') as f:\n    for line in f:\n        values=line.split()\n        word=values[0]\n        vectors=np.asarray(values[1:],'float32')\n        embedding_dict[word]=vectors\nf.close()","3e63bba8":"# Tokenize : break the sentence into single word\/token\n# texts_to_sequences : convert tokenized word into an encoded sequnce\n# pad_sequence : change the length of sequence by either adding or truncating\n\nMAX_LEN = 20 \ntokenizer = Tokenizer()\ntokenizer.fit_on_texts(corpus)\n\nsequences = tokenizer.texts_to_sequences(corpus)\n\ncorpus_pad = pad_sequences(sequences, maxlen=MAX_LEN, truncating='post', padding='post')","0b01e049":"# Unique words present\nword_index = tokenizer.word_index\nprint(f\"Number of unique words : {len(word_index)}\")","75d794e8":"# Creating embedding matrix with GloVe using enbedding_dict we created above\nnum_words=len(word_index)+1\nembedding_matrix=np.zeros((num_words,100))\n\nfor word,i in tqdm(word_index.items()):\n    if i > num_words:\n        continue\n    \n    emb_vec=embedding_dict.get(word)\n    if emb_vec is not None:\n        embedding_matrix[i]=emb_vec","38966726":"# Long Short Term Memory network.\n\n# We need sequential model to process sequence of text data\nmodel=Sequential()\n\n# Embedding(input_dimension, output_dimension,embeddings_initializer = initialize the embedding matrix we created, trainable = do not train)\nembedding=Embedding(num_words,100,\n                    embeddings_initializer=Constant(embedding_matrix),\n                    input_length=MAX_LEN,\n                    trainable=False)\n# Adding Embedding Layer\nmodel.add(embedding)\n\n# Drops 40% of entire row\nmodel.add(SpatialDropout1D(0.4))\n\n# Recurrent Layer LSTM(dimensionality of the output space, dropout = 20%, recurrent_dropout = 20%) \nmodel.add(LSTM(64, dropout=0.2, recurrent_dropout=0.2))\n\n# Decide what we are going to output Dense(units, activation function)\nmodel.add(Dense(1, activation='sigmoid'))\n\n# Compile the model compile(loss = binary crossentropy, use Adam(adaptive moment estimation) optimizer with learning rate 1e-3,evaluate based on accuracy)\nmodel.compile(loss='binary_crossentropy',optimizer=Adam(learning_rate=1e-4),metrics=['accuracy'])\n\nmodel.summary()","660e3f43":"X_train,X_test,y_train,y_test = train_test_split(corpus_pad, df_train['target'].values, test_size = 0.25, random_state = 0 )\n\nprint('Shape of train',X_train.shape)\nprint(\"Shape of Validation \",X_test.shape)\n\nprint('Shape of train',y_train.shape)\nprint(\"Shape of Validation \",y_test.shape)","1bf89772":"history=model.fit(X_train,y_train,batch_size=32,epochs=30,validation_data=(X_test,y_test),verbose=2)","acda3516":"# Accuracy vs Epoch\nplt.title('Accuracy')\nplt.plot(history.history['accuracy'], label='train')\nplt.plot(history.history['val_accuracy'], label='test')\nplt.legend()\nplt.show();","48dc69e9":"# Loss vs Epoch\n# Visualize learning curve. Here learning curve is not ideal. It should be much smoother as it decreases.\n\nepoch_count = range(1, len(history.history['loss']) + 1)\nplt.plot(epoch_count, history.history['loss'], 'r--')\nplt.plot(epoch_count, history.history['val_loss'], 'b-')\nplt.legend(['Training Loss', 'Validation Loss'])\nplt.xlabel('Epoch')\nplt.ylabel('Loss')\nplt.show()","d259b2df":"# Clean Test data\n\ndf_test['cleaned_text'] = df_test['text'].apply(lambda x : remove_punctuation(x))\ndf_test['cleaned_text'] = df_test['cleaned_text'].apply(lambda x : remove_urls(x))\ndf_test['cleaned_text'] = df_test['cleaned_text'].apply(lambda x : remove_emoji(x))\ndf_test['cleaned_text'] = df_test['cleaned_text'].apply(lambda x : remove_spec(x))","f391a484":"# Creating corpus\ntest_corpus = create_corpus(df_test)","574a5bb9":"# Encoding Test Text to Sequences\ntest_sequences = tokenizer.texts_to_sequences(test_corpus)\n\ntest_corpus_pad = pad_sequences(test_sequences, maxlen=MAX_LEN, truncating='post', padding='post')","27e5f4d7":"# Predictions\npredictions = model.predict(test_corpus_pad)\npredictions = np.round(predictions).astype(int).reshape(3263)","6428ffe4":"# Creating submission file \nsubmission = pd.DataFrame({'id' : df_test['id'], 'target' : predictions})\nsubmission.to_csv('final_submission.csv', index=False)\n\nsubmission.head()","96d06782":"### Prediction","9ed312c6":"### Word corpus and Embedding Dictionary","e908cb2e":"### Basic EDA","f43ad482":"### Cleaning the Dataset\n\n- \"When you have pre-trained embeddings, doing standard preprocessing steps might not be a good idea because some of the valuable information can be lost. It is better to get vocabulary as close to embeddings as possible. In order to do that, train vocab and test vocab are created by counting the words in tweets.\" -- Gunes Evitan, Dieter","ea30aa79":"## Whats in this Notebook?\n- Exploratory Data Analysis (EDA)\n- Simple Data Cleaning\n- Using pre-trained model like GloVe\n- Why RNN is good for Text Processing?\n- Making sense out of Different Layers of RNN\n- Building a RNN model\n- Model Evaluation\n\n-- Detailed EDA is not included. Focused more on RNN step-by-step details","91f32473":"### Loading Data\n- Given : Training Dataset , Testing Dataset\n- Format : .csv","33ce10a3":"### Special Thanks to : \n- Gunes Evitan [https:\/\/www.kaggle.com\/gunesevitan]\n- Dieter[https:\/\/www.kaggle.com\/christofhenkel]\n- Shahules786[https:\/\/www.kaggle.com\/shahules]\n\n#### For their contribution and encouraging to learn more."}}