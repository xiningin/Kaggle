{"cell_type":{"a6d897f5":"code","25e8f05b":"code","fb5b4505":"code","4b2f7db1":"code","c3fe788b":"code","ef600e10":"code","8ee80eb4":"code","ef263a5f":"code","0e09f4d5":"code","e313d0f8":"code","a40d7972":"code","88ee73f5":"code","e6423b7c":"code","1426cdfe":"code","cb92d697":"code","e093ca78":"code","2107074d":"code","3256b181":"code","d42698fb":"code","6d4b49b3":"code","641e1f70":"code","9ffbc815":"code","c39079ea":"markdown","ef8986cf":"markdown","c683cb7a":"markdown","70716e53":"markdown","c457c2bc":"markdown","90c7f512":"markdown","9037327d":"markdown","7b37c033":"markdown","0ff2f3ea":"markdown","019dae5a":"markdown","11430f1e":"markdown","ae71e932":"markdown","b19a3a54":"markdown","853da569":"markdown","0e573525":"markdown","3aabea04":"markdown","89937752":"markdown","6ccb37ea":"markdown","baf1fbb0":"markdown","b790df11":"markdown","f146d796":"markdown","32bcb884":"markdown"},"source":{"a6d897f5":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","25e8f05b":"df = pd.read_csv(\"\/kaggle\/input\/heart-disease-uci\/heart.csv\")  # patients data frame","fb5b4505":"df.info()","4b2f7db1":"df.head()","c3fe788b":"# libraries for Visualization\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt","ef600e10":"labels = ['M' if i == 1 else 'F' for i in df.sex.value_counts().index]\ncolors = ['gray','red']\nexplode = [0,0.1]\nsizes = [df.sex.value_counts().values]\n\n# visual\nplt.figure(figsize = (8,8))\nplt.pie(sizes, explode=explode, labels=labels, colors=colors, autopct='%1.1f%%')   # matplot methodu , 'autopct' ile 1 tane ondalik kismini gostermek icin\nplt.title('Sex Ratio of Patients',color = 'blue',fontsize = 15)\nplt.show()","8ee80eb4":"plt.figure(figsize=(15,10))\nsns.countplot(df.age)\nplt.title(\"Age Distribution\",color = 'black',fontsize=20)\nplt.show()","ef263a5f":"\nheart_disease = ['Has Heart Disease' if i == 0 else 'No Heart Disease' for i in df.target]\n\ndf_1 =  pd.DataFrame({'target':heart_disease})\n\nplt.figure(figsize=(10,7))\n\nsns.countplot(x = df_1.target)\nplt.ylabel('Count')\nplt.xlabel('Has Heart Disease or No Heart Disease')\nplt.title('Heart Disease Count',color = 'blue',fontsize=15)\nplt.show()","0e09f4d5":"y = df.target.values\nx_data = df.drop([\"target\"],axis=1)\n\n# normalization \nx = ( x_data - np.min(x_data) ) \/ ( np.max(x_data) - np.min(x_data) ).values","e313d0f8":"# %% split data\nfrom sklearn.model_selection import train_test_split\n\nx_train, x_test, y_train, y_test = train_test_split(x,y,test_size = 0.2, random_state = 42) # test data = 0.2 data","a40d7972":"accuracies = {} # Create a dictionary to save algorithms accuracies.\n\ndef save_score(name,score):\n    \"\"\"\n    Parameters\n    ----------\n    name : Algorithm name\n    score : Algorithm test score    \n\n    Returns\n    -------\n    None.\n    \"\"\"\n    accuracies[name] = score*100\n    \n    return None","88ee73f5":"from sklearn.linear_model import LogisticRegression\n\nlr = LogisticRegression()\n\nlr.fit(x_train,y_train)\n\nprint(\"Test Accuracy {}\".format(lr.score(x_test,y_test)))\n\nsave_score( \"Logistic Regression Classification\",lr.score(x_test,y_test) )","e6423b7c":"# knn model\n\nfrom sklearn.neighbors import KNeighborsClassifier\n\nknn = KNeighborsClassifier(n_neighbors = 3) # n_neighbors = k\n\nknn.fit(x_train,y_train)\n\nprediction = knn.predict(x_test)\n\nprint(\" {} nn score: {} \".format(3,knn.score(x_test,y_test)))\n\nsave_score( \"K-Nearest Neighbour (KNN) Classification\",knn.score(x_test,y_test) )","1426cdfe":"# find k value\n\nscore_list = []\n\nfor each in range(1,50):\n    \n    knn2 = KNeighborsClassifier(n_neighbors = each)\n    \n    knn2.fit(x_train,y_train)\n    \n    score_list.append(knn2.score(x_test,y_test))\n    \nplt.figure(figsize=(30,6))   \nplt.plot(range(1,50),score_list)\nplt.xlabel(\"k values\")\nplt.ylabel(\"accuracy\")\nplt.show()","cb92d697":"# SVM\n\nfrom sklearn.svm import SVC\n\nsvm = SVC(random_state = 1)\n\nsvm.fit(x_train,y_train)\n\nprint(\"print accuracy of svm algo: \",svm.score(x_test,y_test))\n\nsave_score( \"Support Vector Machine (SVM) Classification\",svm.score(x_test,y_test) )","e093ca78":"from sklearn.naive_bayes import GaussianNB\n\nnb = GaussianNB()\n\nnb.fit(x_train,y_train)\n\nprint(\"Accuracy of Naive Bayes Algo: \",nb.score(x_test,y_test))\n\nsave_score( \"Naive Bayes Classification\",nb.score(x_test,y_test) )","2107074d":"from sklearn.tree import DecisionTreeClassifier\n\ndt = DecisionTreeClassifier()\n\ndt.fit(x_train,y_train)\n\nprint(\"score: \", dt.score(x_test,y_test))\n\nsave_score( \"Decision Tree Classification\",dt.score(x_test,y_test) )","3256b181":"from sklearn.ensemble import RandomForestClassifier\n\nrf = RandomForestClassifier(n_estimators = 100,random_state = 1)  # n_estimater = kac agac olacak\n\nrf.fit(x_train,y_train)\n\nprint(\"random forest algo result: \",rf.score(x_test,y_test))\n\naccuracies[\"Random Forest Classification\"] = [rf.score(x_test,y_test)*100]\n\nsave_score( \"Random Forest Classification\",rf.score(x_test,y_test) )","d42698fb":"plt.figure(figsize=(30,8))\nsns.barplot(x = list( accuracies.keys() ), y = list( accuracies.values() ) )\nplt.show()","6d4b49b3":"# Predictions\n\ny_lr_pred = lr.predict(x_test)\ny_knn_pred = knn.predict(x_test)\ny_svm_pred = svm.predict(x_test)\ny_nb_pred = nb.predict(x_test)\ny_dt_pred = dt.predict(x_test)\ny_rf_pred = rf.predict(x_test)\n\ny_true = y_test","641e1f70":"# confusion matrix\n\nfrom sklearn.metrics import confusion_matrix\n\ncm_lr = confusion_matrix(y_true,y_lr_pred)\ncm_knn = confusion_matrix(y_true,y_knn_pred)\ncm_svm = confusion_matrix(y_true,y_svm_pred)\ncm_nb = confusion_matrix(y_true,y_nb_pred)\ncm_dt = confusion_matrix(y_true,y_dt_pred)\ncm_rf = confusion_matrix(y_true,y_rf_pred)","9ffbc815":"# cm visualization\n\nplt.figure(figsize=(24,12))\n\nplt.suptitle(\"Confusion Matrixes\",fontsize=24)\nplt.subplots_adjust(wspace = 0.4, hspace= 0.4)\n\nplt.subplot(2,3,1)\nplt.title(\"Logistic Regression Confusion Matrix\")\nsns.heatmap(cm_lr,annot=True,cmap=\"Blues\",fmt=\"d\",cbar=False, annot_kws={\"size\": 24})\n\nplt.subplot(2,3,2)\nplt.title(\"K Nearest Neighbors Confusion Matrix\")\nsns.heatmap(cm_knn,annot=True,cmap=\"Blues\",fmt=\"d\",cbar=False, annot_kws={\"size\": 24})\n\nplt.subplot(2,3,3)\nplt.title(\"Support Vector Machine Confusion Matrix\")\nsns.heatmap(cm_svm,annot=True,cmap=\"Blues\",fmt=\"d\",cbar=False, annot_kws={\"size\": 24})\n\nplt.subplot(2,3,4)\nplt.title(\"Naive Bayes Confusion Matrix\")\nsns.heatmap(cm_nb,annot=True,cmap=\"Blues\",fmt=\"d\",cbar=False, annot_kws={\"size\": 24})\n\nplt.subplot(2,3,5)\nplt.title(\"Decision Tree Classifier Confusion Matrix\")\nsns.heatmap(cm_dt,annot=True,cmap=\"Blues\",fmt=\"d\",cbar=False, annot_kws={\"size\": 24})\n\nplt.subplot(2,3,6)\nplt.title(\"Random Forest Confusion Matrix\")\nsns.heatmap(cm_rf,annot=True,cmap=\"Blues\",fmt=\"d\",cbar=False, annot_kws={\"size\": 24})\n\nplt.show()","c39079ea":"<a id = '5'><\/a><br>\n## Logistic Regression Classification","ef8986cf":"<font color = 'red'>\nLogistic Regression Classification algorithm accuracy is %85.24.","c683cb7a":"<font color = 'red'>\nRandom Forest Classification algorithm accuracy is %85.24 .","70716e53":"<a id = '11'><\/a><br>\n## Comparison of Accuracy","c457c2bc":"<a id = '7'><\/a><br>\n## Support Vector Machine (SVM) Classification","90c7f512":"<a id = '1'><\/a><br>\n# Load and Check Data","9037327d":"<a id = '12'><\/a><br>\n## Confusion Matrix","7b37c033":"<a id = '2'><\/a><br>\n# Variable Description","0ff2f3ea":"The data was normalized. We will split the data as test data and train data.","019dae5a":"<a id = '8'><\/a><br>\n## Naive Bayes Classification","11430f1e":"<a id = '10'><\/a><br>\n## Random Forest Classification","ae71e932":"# Introduction\n\nHeart disease describes a range of conditions that affect your heart. Diseases under the heart disease umbrella include blood vessel diseases, such as coronary artery disease; heart rhythm problems (arrhythmias); and heart defects you're born with (congenital heart defects), among others.\n\nWe have patients data that shows us a few types of symptoms about heart diseases. We want to classify who has heart disease or not. \nSo, we can use some supervised learning techniques.\n\n<font color = 'blue'>\nContent:\n    \n1. [Load and Check Data](#1)\n1. [Variable Description](#2)\n1. [Take a Look Data](#3)\n1. [Supervised Learning](#4)\n    * [Logistic Regression Classification](#5)\n    * [K-Nearest Neighbour (KNN) Classification](#6)\n    * [Support Vector Machine (SVM) Classification](#7)\n    * [Naive Bayes Classification](#8)\n    * [Decision Tree Classification](#9)\n    * [Random Forest Classification](#10)\n1. [Comparison of Accuracy](#11)\n1. [Confusion Matrix](#12)\n1. [Conclusion](#13)\n    ","b19a3a54":"- **age:** age in years \n- **sex:** (1 = male; 0 = female) \n- **cp:** chest pain type (4 values)\n- **trestbps:** resting blood pressure (in mm Hg on admission to the hospital) \n- **chol:** serum cholestoral in mg\/dl\n- **fbs:** fasting blood sugar > 120 mg\/dl (1 = true; 0 = false) \n- **restecg:** resting electrocardiographic results (values 0,1,2)\n- **thalach:** maximum heart rate achieved\n- **exang:** exercise induced angina (1 = yes; 0 = no) \n- **oldpeak:** ST depression induced by exercise relative to rest \n- **slope:** the slope of the peak exercise ST segment\n- **ca:** number of major vessels (0-3) colored by flourosopy\n- **thal:** 3 = normal; 6 = fixed defect; 7 = reversable defect\n- **target:** 0 = has heart disease; 1 = no heart disease.","853da569":"<a id = '3'><\/a><br>\n# Take a Look Data","0e573525":"<font color = 'red'>\nK-Nearest Neighbour (KNN) Classification algorithm  accuracy is %83.60 .","3aabea04":"<a id = '4'><\/a><br>\n# Supervised Learning\n\nIn this section, we'll use some of the widely used supervised learning classification algorithms. But first we'll prepare data.","89937752":"<a id = '13'><\/a><br>\n## Conclusion\nI am new with ML and just trying to apply what I have just learned. Thanks for reading.","6ccb37ea":"<font color = 'red'>\nSupport Vector Machine (SVM) Classification algorithm accuracy is %83.60 .","baf1fbb0":"<font color = 'red'>\nDecision Tree Classification algorithm accuracy is %78.68 .","b790df11":"<font color = 'red'>\nNaive Bayes Classification algorithm accuracy is %86.88 .","f146d796":"<a id = '6'><\/a><br>\n## K-Nearest Neighbour (KNN) Classification","32bcb884":"<a id = '9'><\/a><br>\n## Decision Tree Classification"}}