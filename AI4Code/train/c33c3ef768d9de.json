{"cell_type":{"fa0e0539":"code","9ab18ba7":"code","e8ee2957":"code","52289131":"code","13a630cb":"code","74754ae5":"code","0be142f1":"code","7839ee5e":"code","adc17e59":"code","1fe16492":"code","29879a05":"code","73ea70ec":"code","41d16a3d":"code","9ebf445b":"code","cff6b08f":"code","7a831c27":"code","9bfe43ac":"code","f480e749":"code","d000e1c5":"code","ad1dd46d":"code","d154da36":"code","1a1a31d3":"code","7be51fd9":"code","c6355ef0":"code","79a20560":"code","1f5cd570":"code","edaf71be":"code","e9a2029d":"code","5cafeb2e":"code","229ccb32":"code","3a1b9fcb":"code","85773b49":"code","197787a5":"code","85abafd2":"code","bdaff3ce":"code","7cb7f42b":"code","ab8c4cd4":"code","29617a84":"code","43955d3d":"code","1f03e1a2":"code","1dd71278":"code","dbff232f":"code","a3155288":"code","23f0e4c0":"code","ed0b6fd7":"code","5e392ddb":"code","f937217d":"markdown","59a21c73":"markdown","60a497a7":"markdown","aa453847":"markdown","a4baba1d":"markdown","f710659d":"markdown","e24fd544":"markdown","400850c8":"markdown","efa0b90f":"markdown","1a013fe2":"markdown","f921085a":"markdown","116e7d12":"markdown","345d308a":"markdown","13e6f9a4":"markdown","ecaf9b7b":"markdown","8f5cf9ae":"markdown","8563bdfd":"markdown","a804688e":"markdown","706c9beb":"markdown","ac7ceb80":"markdown","9e23ef37":"markdown","43073abf":"markdown","5efdcf7f":"markdown","2818b81f":"markdown","fed57404":"markdown","57399ed6":"markdown","cef8ce6d":"markdown","42acccb5":"markdown","7d5a2a90":"markdown","ec14df53":"markdown","22f53e43":"markdown","418e572f":"markdown","31e30fde":"markdown","f50d05c8":"markdown","7ac87902":"markdown","91639131":"markdown","b9ba7b77":"markdown"},"source":{"fa0e0539":"import pandas as pd \nimport numpy as np\nimport nltk\nimport re \nimport os \nimport random \nimport matplotlib.pyplot as plt\nimport seaborn as sns \nfrom sklearn.feature_extraction.text import CountVectorizer","9ab18ba7":"clean_data = pd.read_csv('..\/input\/twitter-airline-sentiment\/Tweets.csv')","e8ee2957":"clean_data.head()","52289131":"clean_data.info()","13a630cb":"sns.countplot(x = \"airline_sentiment\", data = clean_data)","74754ae5":"# First of all let's drop the columns which we don't required\n\nwaste_col = ['tweet_id', 'airline_sentiment_confidence',\n       'negativereason', 'negativereason_confidence', 'airline',\n       'airline_sentiment_gold', 'name', 'negativereason_gold',\n       'retweet_count', 'tweet_coord', 'tweet_created',\n       'tweet_location', 'user_timezone']\n\ndata = clean_data.drop(waste_col, axis = 1)","0be142f1":"data.head()","7839ee5e":"def sentiment(x):\n    if x == 'positive':\n        return 1\n    elif x == 'negative':\n        return -1\n    else:\n        return 0","adc17e59":"nltk.download('stopwords')\nfrom nltk.corpus import stopwords\nfrom nltk.stem import SnowballStemmer \nfrom nltk.tokenize import RegexpTokenizer\n\nstopwords = stopwords.words('english')\nstemmer = SnowballStemmer('english')\ntokenizer = RegexpTokenizer(r'\\w+')\n# As this dataset is fetched from twitter so it has lots of people tag in tweets\n# we will remove them \ntags = r\"@\\w*\"\n\n\ndef preprocess_text(sentence, stem = False):\n    \n    sentence = [re.sub(tags, \"\", sentence)]\n    text = []\n    for word in sentence:\n        \n        if word not in stopwords:\n            \n            if stem:\n                text.append(stemmer.stem(word).lower())\n            else:\n                text.append(word.lower())\n    return tokenizer.tokenize(\" \".join(text))","1fe16492":"print(f\"Orignal Text : {data.text[11]}\")\nprint()\nprint(f\"Preprocessed Text : {preprocess_text(data.text[11])}\")","29879a05":"data.text = data.text.map(preprocess_text)\ndata.head()","73ea70ec":"#this is an example vocabulary just to make concept clear\nsample_vocab = ['the', 'cat', 'sat', 'on', 'mat', 'dog', 'run', 'green', 'tree']","41d16a3d":"# vocabulary of words present in dataset\ndata_vocab = []\nfor text in data.text:\n    for word in text:\n        if word not in data_vocab:\n            data_vocab.append(word)","9ebf445b":"#function to return one-hot representation of passed text\ndef get_onehot_representation(text, vocab = data_vocab):\n    onehot_encoded = []\n    for word in text:\n        temp = [0]*len(vocab)\n        temp[vocab.index(word)-1] = 1\n        onehot_encoded.append(temp)\n    return onehot_encoded\n\nprint(\"One Hot Representation for sentence \\\"the cat sat on the mat\\\" :\")\nget_onehot_representation(['the', 'cat', 'sat', 'on', 'the', 'mat'], sample_vocab)","cff6b08f":"print(f'Length of Vocabulary : {len(data_vocab)}')\nprint(f'Sample of Vocabulary : {data_vocab[302 : 312]}')","7a831c27":"sample_one_hot_rep = get_onehot_representation(data.text[7], data_vocab)\nprint(f\"Shapes of a single sentence : {np.array(sample_one_hot_rep).shape}\")","9bfe43ac":"#one-hot representation for dataset sentences \n\n# data.loc[:, 'one_hot_rep'] = data.loc[:, 'text'].map(get_onehot_representation)\n\n#if you run this cell it will give you a memory error\n","f480e749":"data.head()","d000e1c5":"from sklearn.feature_extraction.text import CountVectorizer\n\nsample_bow = CountVectorizer()\n\n# sample_corpus = [['the', 'cat', 'sat'], \n#                  ['the', 'cat', 'sat', 'in', 'the', 'hat'],\n#                  ['the', 'cat', 'with', 'the', 'hat']]\n\nsample_corpus = [\"the cat sat\", \"the cat sat in the hat\", \"the cat with the hat\"]\n\nsample_bow.fit(sample_corpus)\n\ndef get_bow_representation(text):\n        return sample_bow.transform(text)\n    \nprint(f\"Vocabulary mapping for given sample corpus : \\n {sample_bow.vocabulary_}\")\nprint(\"\\nBag of word Representation of sentence 'the cat cat sat in the hat'\")\nprint(get_bow_representation([\"the cat cat sat in the hat\"]).toarray())","ad1dd46d":"sample_bow = CountVectorizer(binary = True)\n\nsample_corpus = [\"the cat sat\", \"the cat sat in the hat\", \"the cat with the hat\"]\n\nsample_bow.fit(sample_corpus)\n\ndef get_bow_representation(text):\n        return sample_bow.transform(text)\n    \nprint(f\"Vacabulary mapping for given sample corpus : \\n {sample_bow.vocabulary_}\")\nprint(\"\\nBag of word Representation of sentence 'the the the the cat cat sat in the hat'\")\nprint(get_bow_representation([\"the the the the cat cat sat in the hat\"]).toarray())","d154da36":"# generate bag of word representation for given dataset\n\nbow = CountVectorizer()\nbow_rep = bow.fit_transform(data.loc[:, 'text'].astype('str'))","1a1a31d3":"# intrested one can see vocabulary of given corpus by uncommenting below code line\n\n# bow.vocabulary_","7be51fd9":"print(f\"Shape of Bag of word representaion matrix : {bow_rep.toarray().shape}\")","c6355ef0":"# Bag of 1-gram (unigram)\nfrom sklearn.feature_extraction.text import CountVectorizer\n\nsample_boN = CountVectorizer(ngram_range = (1, 1))\n\nsample_corpus = [\"the cat sat\", \"the cat sat in the hat\", \"the cat with the hat\"]\n\nsample_boN.fit(sample_corpus)\n\ndef get_boN_representation(text):\n        return sample_boN.transform(text)\n    \nprint(f\"Unigram Vocabulary mapping for given sample corpus : \\n {sample_boN.vocabulary_}\")\nprint(\"\\nBag of 1-gram (unigram) Representation of sentence 'the cat cat sat in the hat'\")\nprint(get_boN_representation([\"the cat cat sat in the hat\"]).toarray())","79a20560":"# Bag of 2-gram (bigram)\nfrom sklearn.feature_extraction.text import CountVectorizer\n\nsample_boN = CountVectorizer(ngram_range = (2, 2))\n\nsample_corpus = [\"the cat sat\", \"the cat sat in the hat\", \"the cat with the hat\"]\n\nsample_boN.fit(sample_corpus)\n\ndef get_boN_representation(text):\n        return sample_boN.transform(text)\n    \nprint(f\"Bigram Vocabulary mapping for given sample corpus : \\n {sample_boN.vocabulary_}\")\nprint(\"\\nBag of 2-gram (bigram) Representation of sentence 'the cat cat sat in the hat'\")\nprint(get_boN_representation([\"the cat cat sat in the hat\"]).toarray())","1f5cd570":"# Bag of 3-gram (trigram)\nfrom sklearn.feature_extraction.text import CountVectorizer\n\nsample_boN = CountVectorizer(ngram_range = (3, 3))\n\nsample_corpus = [\"the cat sat\", \"the cat sat in the hat\", \"the cat with the hat\"]\n\nsample_boN.fit(sample_corpus)\n\ndef get_boN_representation(text):\n        return sample_boN.transform(text)\n    \nprint(f\"Trigram Vocabulary mapping for given sample corpus : \\n {sample_boN.vocabulary_}\")\nprint(\"\\nBag of 3-gram (trigram) Representation of sentence 'the cat cat sat in the hat'\")\nprint(get_boN_representation([\"the cat cat sat in the hat\"]).toarray())","edaf71be":"from sklearn.feature_extraction.text import TfidfVectorizer\n\ntfidf = TfidfVectorizer()\n\nsample_corpus = [\"the cat sat\", \"the cat sat in the hat\", \"the cat with the hat\"]\ntfidf_rep = tfidf.fit_transform(sample_corpus)\nprint(f\"IDF Values for sample corpus : {tfidf.idf_}\")\n\n\nprint(\"TF-IDF Representation for sentence 'the cat sat in the hat' :\") \nprint(tfidf.transform([\"the cat sat in the hat\"]).toarray())","e9a2029d":"from gensim.models import Word2Vec, KeyedVectors\npretrained_path = \"..\/input\/googles-trained-word2vec-model-in-python\/GoogleNews-vectors-negative300.bin\"\n\nWord2VecModel = KeyedVectors.load_word2vec_format(pretrained_path, binary = True)","5cafeb2e":"print(Word2VecModel.most_similar('good'))","229ccb32":"print(Word2VecModel['good'])","3a1b9fcb":"from gensim.test.utils import common_texts\nfrom gensim.models import Word2Vec\n\nprint(\"Sentences on Which We are gonna train our CBOW Word2Vec Model:\\n\")\nprint(common_texts)\n\nOur_CBOW_Word2Vec_Model = Word2Vec(common_texts, vector_size = 10, window = 5, min_count = 1, workers = 8, sg = 0)\nOur_CBOW_Word2Vec_Model.save(\"Our_CBOW_Word2Vec_Model.w2v\")\nprint(\"Model Saved\")","85773b49":"Our_CBOW_Word2Vec_Model.wv.most_similar('human', topn = 5)","197787a5":"Our_CBOW_Word2Vec_Model.wv['human']","85abafd2":"Glove_path = \"..\/input\/glove6b\/glove.6B.100d.txt\"\nfrom gensim.scripts.glove2word2vec import glove2word2vec\nword2vec_output_file = 'glove.6B.100d.txt.word2vec'\nglove2word2vec(Glove_path, word2vec_output_file)","bdaff3ce":"from gensim.models import KeyedVectors\n# load the Stanford GloVe model\nfilename = '.\/glove.6B.100d.txt.word2vec'\nGlove_model = KeyedVectors.load_word2vec_format(filename, binary=False)","7cb7f42b":"print(\"Most similar words to word 'human' : \")\nGlove_model.most_similar('human')","ab8c4cd4":"print(\"Glove Word Embeddings of word 'human' \")\nGlove_model['human']","29617a84":"from gensim.models.fasttext import load_facebook_model\nfrom gensim.models import FastText, KeyedVectors\n\nfasttext_model = KeyedVectors.load_word2vec_format('..\/input\/fasttext-wikinews\/wiki-news-300d-1M.vec')\n\n# fasttext_model = FastText.load_fasttext_format('..\/input\/fasttext-crawl-300d-2m\/crawl-300d-2M.vec')\nprint(\"Most similar words to word 'human' : \")\nfasttext_model.most_similar('human')","43955d3d":"print(\"Glove Word Embeddings of word 'human' \")\nfasttext_model['human']","1f03e1a2":"from gensim.models import FastText\nfrom gensim.test.utils import common_texts\n\nour_fasttext_model = FastText(common_texts, vector_size = 100, min_count = 1, window = 5, sg = 1)","1dd71278":"print(\"Most Similar words of word 'computer' : \")\nour_fasttext_model.wv.most_similar('computer')","dbff232f":"#Word Embedding for Word \"Computer\"\n\nour_fasttext_model.wv['computer']","a3155288":"# Visualizing Word2vec Word Embedding \n\nkeys = ['India', 'good', 'friday', 'science', 'Twitter', 'masters', 'computer', 'election', 'costly',\n        'learning', 'finance', 'machine', 'android', 'peace', 'nature', 'war']\n\nwords_clusters = []\nembeddings_clusters = []\n\nfor word in keys:\n    \n    words = []\n    embeddings = []\n    \n    for similar_word, _ in Word2VecModel.most_similar(word, topn = 30):\n        words.append(similar_word)\n        embeddings.append(Word2VecModel[word])\n    words_clusters.append(words)\n    embeddings_clusters.append(embeddings)\n","23f0e4c0":"    \nfrom sklearn.manifold import TSNE\n\nembedding_array = np.array(embeddings_clusters)\nn, m, k = embedding_array.shape\n\ntsne_2d_model = TSNE(perplexity = 15, n_components = 2, n_iter = 4000, random_state = 11, init = 'pca')\ntsne_embeddings = np.array(tsne_2d_model.fit_transform(embedding_array.reshape(n * m, k))).reshape(n, m, 2)","ed0b6fd7":"    \nimport matplotlib.pyplot as plt \nimport matplotlib.cm as cm\n%matplotlib inline\n\ndef plot_most_similar_words(labels, embedding_cluster, word_cluster, title):\n    \n    colors = cm.rainbow(np.linspace(0, 1, len(labels)))\n    plt.figure(figsize = (16,9))\n    for label, embeddings, words, color in zip(labels, embedding_cluster, word_cluster, colors):\n        x = embeddings[:, 0]\n        y = embeddings[:, 1]\n        plt.scatter(x, y, c=color, alpha=0.7, label=label)\n    plt.legend(loc = 4)\n    plt.title(title)\n    plt.grid(True)\n    plt.show()","5e392ddb":"plot_most_similar_words(keys, tsne_embeddings, words_clusters, \"Visualizing Word2vec Word Embedding\")","f937217d":"A potential problem with both approaches is that they do not take the context of words into account. Take, for example, the sentences \u201ccat sat on mat\u201d and \u201cmat sat on cat.\u201d Both receive the same representation in these approaches, but they obviously have very different meanings. Let\u2019s look at another approach, **Doc2vec**, which allows us to directly learn the representations for texts of arbitrary lengths (phrases, sentences, paragraphs, and documents) by taking the context of words in the text into account.\n\nYou can Explore Doc2Vec in great detail and I will also try to bring its details in future versions","59a21c73":"<h1  style=\"text-align: center\" class=\"list-group-item list-group-item-action active\">TF-IDF<\/h1><a id = \"7\" ><\/a>\n\n\nIn all the three approaches we\u2019ve seen so far, all the words in the text are treated as equally important\u2014there\u2019s no notion of some words in the document being more important than others. TF-IDF, or term frequency-inverse document frequency, addresses this issue. It aims to quantify the importance of a given word relative to other words in the document and in the corpus.\n\nThe intuition behind TF-IDF is as follows: if a word w appears many times in a sentence S1 but does not occur much in the rest of the Sentences Sn in the corpus, then the word w must be of great importance to the Sentence S1. The importance of w should increase in proportion to its frequency in S1 (how many times that word occurs in sentence S1), but at the same time, its importance should decrease in proportion to the word\u2019s frequency in other Sentence Sn in the corpus. **Mathematically, this is captured using two quantities: TF and IDF. The two are then multiplied to arrive at the TF-IDF score.**\n\n**TF (term frequency) measures how often a term or word occurs in a given document.**\n\nMathematical Expression of TF\n\n![image-2.png](attachment:image-2.png)\n\n**IDF (inverse document frequency)** measures the importance of the term across a corpus. In computing TF, all terms are given equal importance (weightage). However, it\u2019s a well-known fact that stop words like is, are, am, etc., are not important, even though they occur frequently. To account for such cases, IDF weighs down the terms that are very common across a corpus and weighs up the rare terms. IDF of a term t is calculated as follows:\n\n![image.png](attachment:image.png)\n\nThe TF-IDF score is a product of these two terms. Thus, TF-IDF score = TF * IDF. Let\u2019s consider an example.\n\nSentence A = The Car is Driven on the Road <br>\nSentence B = The Truck is Driven on the highway <br>\n\nComputation of TF-IDF scores are shown below\n\n![](https:\/\/cdn-media-1.freecodecamp.org\/images\/1*q3qYevXqQOjJf6Pwdlx8Mw.png)","60a497a7":"You can visualize and analyze every embedding discussed in this notebook and can also suggest to me some new visualization techniques for Word Embeddings.","aa453847":"In the output of the above code cell, we can see a 300-dimensional real-valued vector for the word \"good\".\n\nThe above few cells were about using pre-trained word2vec representation now in upcoming cells we will focus on learning\/Training our own word2vec representations.","a4baba1d":"<h1  style=\"text-align: center\" class=\"list-group-item list-group-item-action active\">Basic Text Pre-Processing<\/h1><a id = \"3\" ><\/a>\n\nText preprocessing steps include a few essential tasks to further clean the available text data. It includes tasks like:-\n\n**1. Stop-Word Removal** : In English words like a, an, the, as, in, on, etc. are considered as stop-words so according to our requirements we can remove them to reduce vocabulary size as these words don't have some specific meaning\n\n**2. Lower Casing** : Convert all words into the lower case because the upper or lower case may not make a difference for the problem.\nAnd we are reducing vocabulary size by doing so. \n\n**3. Stemming** : Stemming refers to the process of removing suffixes and reducing a word to some base form such that all different variants of that word can be represented by the same form (e.g., \u201cwalk\u201d and \u201cwalking\u201d are both reduced to \u201cwalk\u201d).\n\n**4. Tokenization** : NLP software typically analyzes text by breaking it up into words (tokens) and sentences.\n\nPre-processing of the text is not the main objective of this notebook that's why I am just covering a few basic steps in a brief\n","f710659d":"But before moving on to the Text representation step first we have to get a cleaned dataset which then has to be preprocessed. In this notebook, I will be using only a few basic steps to preprocess the text data","e24fd544":"**Training our own embeddings**\n\nNow we\u2019ll focus on training our own word embeddings. For this, we\u2019ll look at two architectural variants that were proposed in the original Word2vec approach. The two variants are:\n\n1. Continuous bag of words (CBOW)\n2. SkipGram\n\nBoth of these have a lot of similarities in many respects. \n\nThroughout this section, we\u2019ll use the sentence \u201cThe quick brown fox jumps over the lazy dog\u201d as our example text.\n\n**1. Continuous bag of words (CBOW)**\n\nIn CBOW, the primary task is to build a language model that correctly predicts the center word given the context words in which the center word appears. Consider our example sentence we take the word \u201cjumps\u201d as the center word, then its context is formed by words in its vicinity. If we take the context size of 2, then for our example, the context is given by brown, fox, over, the. CBOW uses the context words to predict the target word\u2014jumps\u2014as shown in the below figure \n<br><br>\n\n![image-1.png](attachment:image.png)\n\n<br><br>\nNow next task is to create a training sample of the form (X, Y) for this task where X will be context words and Y will be Center word. We define the value of context window = 2 in this case. \n![image-3.png](attachment:image-2.png)\n\n<br><br>\nNow that we have the training data ready, let\u2019s focus on the model. For this, we construct a shallow net (it\u2019s shallow since it has a single hidden layer). We assume we want to learn D-dim word embeddings. Further, let V be the vocabulary of the text corpus","400850c8":"<h1  style=\"text-align: center\" class=\"list-group-item list-group-item-action active\">Bag of N-Grams<\/h1><a id = \"6\" ><\/a>\n\nAll the representation schemes we\u2019ve seen so far treat words as independent units. There is no notion of phrases or word order. The bag-of-n-grams (BoN) approach tries to remedy this. It does so by breaking text into chunks of n contiguous words (or tokens). This can help us capture some context, which earlier approaches could not do. Each chunk is called an n-gram.\n\n**One can simply say Bag of words (BoW) is a special case of the Bag of n-grams having n = 1.**\n\nThe corpus vocabulary, V, is then nothing but a collection of all unique n-grams across the text corpus. Then, each document in the corpus is represented by a vector of length |V|. This vector simply contains the frequency counts of n-grams present in the document and zero for the n-grams that are not present.\n\nConsider an Example: \n\n![](https:\/\/i.stack.imgur.com\/8ARA1.png)\n\n\nThe following code cell shows an example of a BoN representation considering 1\u20133 n-gram word features to represent the corpus that we\u2019ve used so far.\n","efa0b90f":"The Above cells show code implementation of the CBOW model one can experiment with tunning hyperparameters. Interested one can experiment by setting sg hyperparameter to 1 then the model will become a skip-gram model.","1a013fe2":"As we learned Word2vec do have a similar vector representation for words with the same meaning so let's check the similar words for the word \"good\"","f921085a":"We have 14276 different words in a given dataset thus this implies each word representation for one-hot encoding schema will be of 14276-dimensional vector mark that this much big representation is just for a single word if we consider the representation of a sentence which consist of let say 20 words in it then it will be represented with (20,14276) sized matrix.","116e7d12":"**2. SkipGram** \n\nSkipGram is very similar to CBOW, with some minor changes. In Skip\u2010 Gram, the task is to predict the context words from the center word. For our toy corpus with context size 2, using the center word \u201cjumps,\u201d we try to predict every word in context\u2014\u201cbrown,\u201d \u201cfox,\u201d \u201cover,\u201d \u201cthe\u201d\u2014as shown in the Figure below \n\n![image-5.png](attachment:image.png)\n\nNow we will create a training sample of the form (X, Y) for this task where X will be the center word and Y will be Context words. \n\n<br>\n<br>\n\n![image-6.png](attachment:image-2.png)\n\n<br>\n<br>\n\n![image-9.png](attachment:image-3.png)\n\n<br>\n<br>\n\nThe shallow network used to train the SkipGram model, shown in the below Figure, is very similar to the network used for CBOW, with some minor changes. In the input layer, the index of the word in the target is used to fetch the corresponding row from the embedding matrix E|V| x d. The vectors fetched are then passed to the next layer. The next layer simply takes this d vector and multiplies it with another matrix E\u2019d x |V|. This gives a 1 x |V| vector, which is fed to a softmax function to get probability distribution over the vocabulary space. This distribution is compared with the label and uses backpropagation to update both the matrices E and E\u2019 accordingly. At the end of the training, E is the embedding matrix we wanted to learn.","345d308a":"<h1  style=\"text-align: center\" class=\"list-group-item list-group-item-action active\">Visualizing Embeddings<\/h1><a id = \"11\" ><\/a>\n\n\nSo far, we\u2019ve seen various vectorization techniques for representing text. The vectors obtained are used as features for the NLP task at hand. An important aspect of any ML project is feature exploration. Visual exploration is a very important aspect of any data-related problem. Even though embeddings are low-dimensional vectors, even\n100 or 300 dimensions are too high to visualize.\n\nt-SNE, or t-distributed Stochastic Neighboring Embedding help us solve this problem. It\u2019s a technique used for visualizing high-dimensional data like embeddings by reducing them to two or three-dimensional data. The technique takes in the embeddings (or any data) and looks at how to best represent the input data using lesser dimensions, all while maintaining the same data distributions in original high-dimensional input space and low-dimensional output space. This, therefore, enables us to plot and visualize the input data. It helps to get a feel for the space of word embedding.\n\nNow lets plot scatter plots of most-similar words in the vocabulary of different embedding schemes. ","13e6f9a4":"**Important Points Regarding Word Embeddings**\n\n1. All text representations are **inherently biased** based on what they saw in training data. For example, an embedding model trained heavily on technology news or articles is likely to identify Apple as being closer to, say, Microsoft or Facebook than to an orange or pear.\n\n2. Unlike the basic vectorization approaches, pre-trained embeddings are generally **large-sized files (several gigabytes)**, which may pose problems in certain deployment scenarios. This is something we need to address while using them, otherwise, it can become an engineering bottleneck in performance. The Word2vec model takes ~4.5 GB RAM.","ecaf9b7b":"Sometimes, we don\u2019t care about the frequency of occurrence of words in the text and we only want to represent whether a word exists in the text or not. In such cases, we just initialize CountVectorizer with the binary=True","8f5cf9ae":"**Here are the main advantages and disadvantages of TF-IDF Representation:**\n\n1. Its Implementation is not that easy as compared to techniques discussed above \n2. We have a fixed-length encoding for any sentence of arbitrary length.\n3. The feature vectors are high-dimensional representations. The dimensionality increases with the size of the vocabulary.\n4. It did capture a bit of the semantics of the sentence. \n5. They too cannot handle OOV words.\n\nWith this, we come to the end of basic vectorization approaches. Now, let\u2019s start looking at distributed representations.","8563bdfd":"**Advantages of this Bag of words(BoW) encoding** :\n\n1. Like one-hot encoding, BoW is fairly simple to understand and implement.\n\n2. With this representation, documents having the same words will have their vector representations closer to each other in Euclidean space as compared to documents with completely different words.\n\n    Consider an example Where \n\n    S1 = \"cat on the mat\" --> BoW Representation --> {0 1 1 0 1 0 1} <br>\n    S2 = \"mat on the cat\" --> BoW Representation --> {0 1 1 0 1 0 1} <br>\n    S3 = \"dog in the mat\" --> BoW Representation --> {0 1 0 1 1 1 0} <br>\n\n    The distance between S1 and S2 is 0 as compared to the distance between S1 and S3, which is 2. Thus, the vector space resulting from the BoW scheme captures the semantic similarity of documents. So if two documents have a similar vocabulary, they\u2019ll be closer to each other in the vector space and vice versa.\n\n3. We have a fixed-length encoding for any sentence of arbitrary length.\n\n**Disadvantages of this Bag of words(BoW) encoding** :\n\n1. The size of the vector increases with the size of the vocabulary as in our case it is 14238 dimensional. Thus, sparsity continues to be a problem. One way to control it is by limiting the vocabulary to n number of the most frequent words.\n\n2. It does not capture the similarity between different words that mean the same thing. Say we have three documents: \u201cwalk\u201d, \u201cwalked\u201d, and \u201cwalking\u201d. BoW vectors of all three documents will be equally apart.\n\n3. This representation does not have any way to handle **out of vocabulary (OOV)** words (i.e., new words that were not seen in the corpus that was used to build the vectorizer).\n\n4. As the name indicates, it is a \u201cbag\u201d of words\u2014word order information is lost in this representation. Both S1 and S2 will have the same representation in this scheme.\n","a804688e":"Despite the ease of using Powerfull Word Embeddings like Word2vec or any such word embedding to do feature extraction from texts, **we don\u2019t have a good way of handling OOV words yet**.\n\nWhat We can do to solve this Problem?\n\n1. A simple approach that often works is to exclude those words from the feature extraction process so we don\u2019t have to worry about how to get their representations.\n\n2. Another way to deal with the OOV problem for word embeddings is to create vectors that are initialized randomly.\n\n3. There are also other approaches that handle the OOV problem by modifying the training process by bringing in characters and other subword-level linguistic components. Let\u2019s look at one such approach now. The key idea is that one can potentially handle the OOV problem by using subword information, such as morphological properties (e.g., prefixes, suffixes, word endings, etc.), **or by using character representations. fastText, from Facebook AI research**, is one of the popular algorithms that follows this approach.\n\n","706c9beb":"![image-4.png](attachment:image.png)\n\n\n<br><br>\nThe objective is to learn an embedding matrix E|V| x d.To begin with, we initialize the matrix randomly. Here, |V| is the size of corpus vocabulary and d is the dimension of the embedding. Let\u2019s break down the shallow net in Figure layer by layer. In the input layer, indices of the words in context are used to fetch the corresponding rows from the embedding matrix E|V| x d. The vectors fetched are then added to get a single D-dim vector, and this is passed to the next layer. The next layer simply takes this d vector and multiplies it with another matrix E\u2019d x |V|.. This gives a 1 x |V| vector, which is fed to a softmax function to get probability distribution over the vocabulary space. This distribution is compared with the label and uses backpropagation to update both the matrices E and E\u2019 accordingly. At the end of the training, E is the embedding matrix we wanted to learn.\n<br><br>","ac7ceb80":"<h1  style=\"text-align: center\" class=\"list-group-item list-group-item-action active\">One-Hot Encoding<\/h1><a id = \"4\" ><\/a>\n\n\nIn one-hot encoding, each word w in the corpus vocabulary is given a unique integer ID (wid) that is between 1 and |V|, where V is the set of the corpus vocabulary. Each word is then represented by a V dimensional binary vector of 0s and 1s. This is done via a |V| dimension vector filled with all 0s barring the index, where index = wid. At this index, we simply put a 1. The representation for individual words is then combined to form a sentence representation.\n\nConsider an Example \n\n![](https:\/\/miro.medium.com\/max\/886\/1*_da_YknoUuryRheNS-SYWQ.png)","9e23ef37":"**Advantages of Word2Vec:**\n\n1. The idea is very intuitive, which transforms the unlabeled raw corpus into labeled data (by mapping the target word to its context word), and learns the representation of words in a classification task.\n2. The data can be fed into the model in an online way and needs little preprocessing, thus requiring little memory.\n3. the mapping between the target word to its context word implicitly embeds the sub-linear relationship into the vector space of words, so that relationships like \u201cking:man as queen: woman\u201d can be inferred by word vectors.\n3. It is simple for a freshman to understand the principle and do the implementation.\n\n**Disadvantages of Word2Vec:**\n\n1. The sub-linear relationships are not explicitly defined. There is little theoretical support behind such characteristics.\n2. The model could be very difficult to train if use the softmax function, since the number of categories is too large (the size of vocabulary). Though approximation algorithms like negative sampling (NEG) and hierarchical softmax (HS) are proposed to address the issue, other problems happen. For example, the word vectors by NEG are not distributed uniformly, they are located within a cone in the vector space hence the vector space is not sufficiently utilized.\n3. It still provides no way to address the **out of vocabulary(OOV)** problem.\n\n**Advantages of Glove:**\n\n1. The goal of Glove is very straightforward, i.e., to enforce the word vectors to capture sub-linear relationships in the vector space. Thus, it proves to perform better than Word2vec in the word analogy tasks.\n2. Glove adds some more practical meaning to word vectors by considering the relationships between word pair and word pair rather than word and word.\n3. Glove gives lower weight for highly frequent word pairs to prevent the meaningless stop words like \u201cthe\u201d, \u201can\u201d will not dominating the training progress.\n\n**Disadvantages of Glove:**\n\n1. The model is trained on the co-occurrence matrix of words, which takes a lot of memory for storage. Especially, if you change the hyper-parameters related to the co-occurrence matrix, you have to reconstruct the matrix again, which is very time-consuming.\n\n**Both Word2vec and Glove do not solve the problems like:**\n\n1. How to learn the representation for out-of-vocabulary words.\n2. How to separate some opposite word pairs. For example, \u201cgood\u201d and \u201cbad\u201d are usually located very close to each other in the vector space, which may limit the performance of word vectors in NLP tasks like sentiment analysis.\n\nContent of this cell is taken from [Quora](https:\/\/www.quora.com\/What-are-the-advantages-and-disadvantages-of-Word2vec-and-GloVe)\n","43073abf":"<h1 style=\"text-align: center;\" class=\"list-group-item list-group-item-action active\">Table of Contents<\/h1>\n<a class=\"list-group-item list-group-item-action\" data-toggle=\"list\" href = \"#1\" role=\"tab\" aria-controls=\"settings\">1. Introduction<span class=\"badge badge-primary badge-pill\">1<\/span><\/a>\n<a class=\"list-group-item list-group-item-action\" data-toggle=\"list\" href = \"#2\" role=\"tab\" aria-controls=\"settings\">2. Load a Clean Dataset<span class=\"badge badge-primary badge-pill\">2<\/span><\/a>\n<a class=\"list-group-item list-group-item-action\"  data-toggle=\"list\" href=\"#3\" role=\"tab\" aria-controls=\"settings\">3. Basic Text Pre-Processing<span class=\"badge badge-primary badge-pill\">3<\/span><\/a>\n   <a class=\"list-group-item list-group-item-action\"  data-toggle=\"list\" href=\"#4\" role=\"tab\" aria-controls=\"settings\">4. One-Hot Encoding<span class=\"badge badge-primary badge-pill\">4<\/span><\/a>\n  <a class=\"list-group-item list-group-item-action\"  data-toggle=\"list\" href=\"#5\" role=\"tab\" aria-controls=\"settings\">5. Bag of Words<span class=\"badge badge-primary badge-pill\">5<\/span><\/a>\n  <a class=\"list-group-item list-group-item-action\"  data-toggle=\"list\" href=\"#6\" role=\"tab\" aria-controls=\"settings\">6. Bag of N grams<span class=\"badge badge-primary badge-pill\">6<\/span><\/a>\n  <a class=\"list-group-item list-group-item-action\"  data-toggle=\"list\" href=\"#7\" role=\"tab\" aria-controls=\"settings\">7. TF-IDF<span class=\"badge badge-primary badge-pill\">7<\/span><\/a>\n  <a class=\"list-group-item list-group-item-action\"  data-toggle=\"list\" href=\"#8\" role=\"tab\" aria-controls=\"settings\">8. Word2vec Word Embeddings<span class=\"badge badge-primary badge-pill\">8<\/span><\/a>\n  <a class=\"list-group-item list-group-item-action\"  data-toggle=\"list\" href=\"#9\" role=\"tab\" aria-controls=\"settings\">9. Glove Word Embeddings<span class=\"badge badge-primary badge-pill\">9<\/span><\/a>\n  <a class=\"list-group-item list-group-item-action\"  data-toggle=\"list\" href=\"#10\" role=\"tab\" aria-controls=\"settings\">10. FastText Word Embeddings<span class=\"badge badge-primary badge-pill\">10<\/span><\/a>\n  <a class=\"list-group-item list-group-item-action\"  data-toggle=\"list\" href=\"#11\" role=\"tab\" aria-controls=\"settings\">11.   Visualizing Embeddings<span class=\"badge badge-primary badge-pill\">11<\/span><\/a>\n  \n","5efdcf7f":"One-hot encoding is intuitive to understand and straightforward to implement. However, it has lots of disadvantages listed below\n\n1. The size of a one-hot vector is directly proportional to the size of the vocabulary and if we consider a real-world vocabulary size it may be in millions so we can not represent a single word with a million-dimensional vector. \n\n2. One-hot representation does not give a fixed-length representation for text, i.e., the sentence with 32 words in it and 40 words in it has variable length representation. But for most learning algorithms, we need the feature vectors to be of the same length.\n\n3. One-Hot representation gives each word the same weight whether that word is important for the task or not.\n\n4. One-Hot representation does not represent the meaning of the word in a proper numerical manner as embedding vectors do. Consider an example word read, reading should have similar real-valued vector representation but in this case, they have different representations. \n\n5. Let say we train the model on some article and get the vocabulary of size 10000 but what if we use this vocabulary on that text which contains words that are not present in learned vocabulary. This is Known as **Out Of Vocabulary (OOV)** problem.\n","2818b81f":"We can observe results obtained are not very good because I didn't tune the hyperparameters nicely, you can do this task and if you got good results then let me know as well.","fed57404":"<h1  style=\"text-align: center\" class=\"list-group-item list-group-item-action active\">Bag of words<\/h1><a id = \"5\" ><\/a>\n\nBag of words (BoW) is a classical text representation technique that has been used commonly in NLP, especially in text classification problems. The key idea behind it is as follows: represent the text under consideration as a bag (collection) of words while ignoring the order and context.\n\nSimilar to one-hot encoding, BoW maps words to unique integer IDs between 1 and |V|. Each document in the corpus is then converted into a vector of |V| dimensions were in the ith component of the vector, i = wid, is simply the number of times the word w occurs in the document, i.e., we simply score each word in V by their occurrence count in the document.\n\nConsider an example:\n\nlet say we have a vocabulary **V consisting of words --> {the, cat, sat, in, hat, with}** then the bag of word representation of a few sentences will be given as \n\n![](https:\/\/miro.medium.com\/max\/1400\/1*3IACMnNpwVlCl8kSTJocPA.png)","57399ed6":"<h1  style=\"text-align: center\" class=\"list-group-item list-group-item-action active\">Introduction<\/h1><a id = \"1\" ><\/a>\n\n\nIn Natural Language Processing (NLP) the conversion of raw-text to numerical form is called <b>Text Representation<\/b> and believe me this step is one of the most important steps in the NLP pipeline as if we feed in poor features in ML Model, we will get poor results. In computer science, this is often called \u201cgarbage in, garbage out.\u201d\n\n<b>I observed in NLP feeding a good text representation to an ordinary algorithm will get you much farther compared to applying a topnotch algorithm to an ordinary text representation.<\/b>\n\nIn this notebook, I will discuss various text-representation schemes with their advantages and disadvantages so that you can choose one of the schemes which suit your task most. Our main objective is to transform a given text into numerical form so that it can be fed\ninto NLP and ML algorithms.\n\n![](https:\/\/www.oreilly.com\/library\/view\/practical-natural-language\/9781492054047\/assets\/pnlp_0301.png)\n\nIn this notebook, the focus will be on the dotted box in the figure \n\n\nhere write a para on the flow of the notebook later\n","cef8ce6d":"<h1  style=\"text-align: center\" class=\"list-group-item list-group-item-action active\">FastText Word Embeddings<\/h1><a id = \"10\" ><\/a>\n\nA word can be represented by its constituent character ngrams. Following a similar architecture to Word2vec, fastText learns embeddings for words and character n-grams together and views a word\u2019s embedding vector as an aggregation of its constituent character n-grams. This makes it possible to generate embeddings even for words that are not present in the vocabulary. Say there\u2019s a word, \u201cgregarious,\u201d that\u2019s not found in the embedding\u2019s word vocabulary. We break it into character n-grams\u2014gre, reg, ega, \u2026.ous\u2014and combine these embeddings of the ngrams to arrive at the embedding of \u201cgregarious.\u201d\n\nHow FastText Works?\n\nFastText is a modified version of word2vec (i.e.. Skip-Gram and CBOW). The only difference between fastText vs word2vec is its pooling strategies (what are the input, output, and dictionary of the model). In word2vec each word is represented as a bag of words but in FastText each word is represented as a bag of character n-gram.\n\n**character n-grams** the contiguous sequence of n items from a given sample of a character or word. It may be bigram, trigram, etc.\nFor example character trigram (n = 3) of the word \u201cwhere\u201d will be:\n\n<wh, whe, her, ere, re>\n\nIn FastText architecture, they have also included the word itself with the character n-gram. That means input data to the model for the word \u201ceating\u201d will be:\n\n![](https:\/\/amitness.com\/images\/fasttext-center-word-embedding.png)\n\nNow the model I am referring same is word2vec which is a shallow neural network with one hidden layer as discussed above.\n\n\nNow to prepare training data for the (Skip-Gram-based) FastText model, we define \u201ccontext word\u201d as the word which follows a given word in the text (which will be our \u201ctarget word\u201d). That means we will be predicting the surrounding word for a given word.\n\nNote: FastText word embeddings support both Continuous Bag of Words (CBOW) and Skip-Gram models. I will explain and implement the skip-gram model in the below cell to learn vector representation (FastText word embeddings). Now let\u2019s construct our training examples (like Skip-Gram), scanning through the text with a window will prepare a context word and a target word.\n\nConsider the sentence : \n\n<div style = \"text-align:center\"><b> i like natural language processing<\/b><\/div>\n\n![](https:\/\/secureservercdn.net\/45.40.148.234\/um0.ec8.myftpupload.com\/wp-content\/uploads\/2020\/10\/Picture2.png)\n\nFor the above example, for context words \u201ci\u201d and \u201cnatural\u201d the target word will be \u201clike\u201d. Full training data for FastText word embedding will look like below. By observing the below training data, your confusion of fastText vs word2vec should be clear.\n\n\nNow you know in word2vec (skip-gram) each word is represented as a bag of words but in FastText each word is represented as a bag of character n-gram. This training data preparation is the only difference between FastText word embeddings and skip-gram (or CBOW) word embeddings.\n\nAfter training data preparation of FastText, training the word embedding, finding word similarity, etc. are the same as the word2vec model (for our example similar to the skip-gram model).\n\nNow let\u2019s see how to implement FastText word embeddings in python using Gensim library.\n","42acccb5":"<h1  style=\"text-align: center\" class=\"list-group-item list-group-item-action active\">Word2vec Word Embeddings<\/h1><a id = \"8\" ><\/a>\n\n**Word Embeddings** : They are a real-valued vector representation of words that allows words with the same meaning to have similar representation. Thus we can say word embeddings are the projection of meanings of words in a real-valued vector \n\nWord2vec is a Word Embedding Technique published in 2013. The word2vec algorithm uses a neural network model to learn word associations from a large corpus of text.\n\nIt is the representation of words that allows words with the same meaning to have similar representation, Word2vec operationalizes this by projecting the meaning of the words in a vector space where words with similar meanings will tend to cluster together, and works with very different meanings are far from one another.\n\n**Using Pre-trained word2vec word embeddings** <br>\nTraining your own word embeddings is a pretty expensive process (in terms of both time and computing). Thankfully, for many scenarios, it\u2019s not necessary to train your own embeddings Someone has done the hard work of training word embeddings on a large corpus, such as Wikipedia, news articles, or even the entire web, and has put words and their corresponding vectors on the web. These embeddings\ncan be downloaded and used to get the vectors for the words you want.  \n\nSome of the most popular pre-trained embeddings are Word2vec by Google, GloVe by Stanford, and fasttext embeddings by Facebook, to name a few.\n\nBelow code, cell demonstrates how to use pre-trained word2vec word embeddings.","7d5a2a90":"**Here are the main advantages and disadvantages of BoN Representation:**\n\n1. It captures some context and word-order information in the form of n-grams.\n\n2. Thus, the resulting vector space can capture some semantic similarity. Documents having the same n-grams will have their vectors closer to each other in Euclidean space as compared to documents with completely different n-grams.\n\n3. As n increases, dimensionality (and therefore sparsity) only increases rapidly.\n\n4. It still provides no way to address the **out of vocabulary(OOV)** problem.","ec14df53":"Similar to BoW, we can use the TF-IDF vectors to calculate the similarity between two texts using a similarity measure like Euclidean distance or cosine similarity. TF-IDF is a commonly used representation in application scenarios such as information\nretrieval and text classification. However, even though TF-IDF is better than the vectorization methods we saw earlier in terms of capturing similarities between words, **it still suffers from the curse of high dimensionality.**","22f53e43":"As we know One-hot representation does not give a fixed-length representation for text but from the output of the above cell, we can interpret that bag of the word has a fixed-length vector representation (14238 dimensional) for each word but it is too big again in this case also.","418e572f":"Now we have preprocessed textual data so now we can proceed further in this notebook and discuss various text representation approaches in detail","31e30fde":"<h1  style=\"text-align: center\" class=\"list-group-item list-group-item-action active\">Load a Clean Dataset<\/h1><a id = \"2\" ><\/a>\n\nKaggle Datasets is one of the best sources to get a clean dataset for this notebook I will be using [Twitter US Airline Sentiment](https:\/\/www.kaggle.com\/crowdflower\/twitter-airline-sentiment) dataset.\n","f50d05c8":"<h1  style=\"text-align: center\" class=\"list-group-item list-group-item-action active\">Glove Word Embeddings<\/h1><a id = \"9\" ><\/a>\n\nGloVe Stands for Global Vectors for word representation is another word embedding technique that was developed as an open-source project at Stanford and was launched in 2014. Just to refresh, word vectors put words to a nice vector space, where similar words cluster together and different words repel. The advantage of GloVe is that, unlike Word2vec, GloVe does not rely just on local statistics (local context information of words), but incorporates global statistics (word co-occurrence) to obtain word vectors. But keep in mind that there\u2019s quite a bit of synergy between the GloVe and Word2vec. The gloVe can be used to find relations between words like synonyms, company-product relations, zip codes, and cities, etc.\n\nThe question may arise Why do we need Glove if we have word2vec as a good word embedding technique Because Word2vec relies only on local information of language. That is, the semantics learned for a given word, are only affected by the surrounding words.\n\nFor example, take the sentence,\n\nThe cat sat on the mat\n\nIf you use Word2vec, it wouldn\u2019t capture information like,\n\nis \u201cthe\u201d a special context of the words \u201ccat\u201d and \u201cmat\u201d ?\n\nor\n\nis \u201cthe\u201d just a stopword?\n\nThis can be suboptimal, especially in the eye of theoreticians.\n\nGloVe method is built on an important idea, You can derive semantic relationships between words from the co-occurrence matrix. Given a corpus having V words, the co-occurrence matrix X will be a V x V matrix, where the i th row and j th column of X, X_ij denotes how many times word i has co-occurred with word j. An example co-occurrence matrix might look as follows.\n\n![](https:\/\/miro.medium.com\/max\/434\/1*QWcK8CIDs8kMkOwsOxvywA.png)\n\nThe co-occurrence matrix for the sentence \u201cthe cat sat on the mat\u201d with a window size of 1. As you probably noticed it is a symmetric matrix.\n\nFor detailed knowledge about Glove word embedding, you can refer [This article](https:\/\/towardsdatascience.com\/light-on-math-ml-intuitive-guide-to-understanding-glove-embeddings-b13b4f19c010)","7ac87902":"Next few code cells shows implementation of CBOW","91639131":"Training our own fasttext model using python's gensim library by settings up below listed hyperparameters:\n\n\n- size: Dimensionality of the word vectors. window=window_size,\n- min_count: The model ignores all words with total frequency lower than this.\n- sample: The threshold for configuring which higher-frequency words are randomly down sampled, useful range is (0, 1e-5).\n- workers: Use these many worker threads to train the model (=faster training with multicore machines).\n- sg: Training algorithm: skip-gram if sg=1, otherwise CBOW.\n- iter: Number of iterations (epochs) over the corpus.","b9ba7b77":"**Implementation** : \n\nOne of the most commonly used implementations is with gensim. We have to choose several hyperparameters (i.e., the variables that need to be set before starting the training process). Let\u2019s look at two examples.\n\nDimensionality of the word vectors\n\nAs the name indicates, this decides the space of the learned embeddings. While there is no ideal number, it\u2019s common to construct word vectors with dimensions in the range of 50\u2013500 and evaluate them on the task we\u2019re using them for to choose the best option. In gensim we do this by setting the \"vector_size\" parameter to the size we want. \n\nContext window\n\nHow long or short the context we look for to learn the vector representation is. In gensim we do this by setting the \"window\" parameter to the size we want.\n\nThere are also other choices we make, such as whether to use CBOW or SkipGram to\nlearn the embeddings."}}