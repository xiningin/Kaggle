{"cell_type":{"b7a0af2c":"code","d79c4137":"code","e2a33d0e":"code","c8cb0b22":"code","44ed40e8":"code","c928d04f":"code","182a3b0d":"code","163e21df":"code","c5b1d20c":"code","cebda384":"code","a8acd929":"code","189fc52d":"code","ef0e242a":"code","04adb074":"code","e55b2bf3":"code","49837dcd":"code","85041eb2":"code","3a8513c1":"code","715289fc":"code","35de493e":"code","d519eaef":"code","718fc29d":"markdown","56f84e02":"markdown","8b0b7b00":"markdown"},"source":{"b7a0af2c":"import torch","d79c4137":"x = torch.ones(1)\ny = torch.rand(1)\n\nw = torch.zeros(1,requires_grad = True)","e2a33d0e":"print(x)\nprint(y)\nprint(w)","c8cb0b22":"y_hat = x*w\ns = y_hat - y\nloss = s**2\nprint(loss)","44ed40e8":"loss.backward()","c928d04f":"print(w.grad)","182a3b0d":"import numpy as np","163e21df":"x = np.array([1,2,3,4,5])\ny = np.array([2,4,6,8,10]) ## y = 3*x\n# weight initialization\nw = np.array([0.],dtype=np.float32)","c5b1d20c":"def forward_pass(x):\n    return w*x\n\ndef loss(y,y_predicted):\n    return ((y-y_predicted)**2).mean()\n\ndef gradient_descent(x,y,y_predicted):\n    return (1\/len(x))*(np.dot(2*x,(y_predicted-y)))","cebda384":"y_pred = forward_pass(x)\nprint(\"Predicted value with initial weights: {:.3f}\",y_pred)\n\nepochs = 10\nlearning_rate = 0.01\nfor epoch in range(epochs):\n    y_pred = forward_pass(x)\n    loss_value = loss(y,y_pred)\n    dw = gradient_descent(x,y,y_pred)\n    w = w - learning_rate*dw\n    print(f\"Predicted value at epoch number : {epoch}: {w[0]:.3f} and loss = {loss_value:.3f}\")","a8acd929":"import torch","189fc52d":"x = torch.tensor([1,2,3,4,5],dtype=torch.float32)\ny = torch.tensor([3,6,9,12,15],dtype=torch.float32)\nw = torch.tensor([0.],requires_grad = True)","ef0e242a":"def forward(x):\n    return w*x\n\ndef loss(y,y_pred):\n    return ((y-y_pred)**2).mean()","04adb074":"epochs = 100\nlearning_rate = 0.002\n\nfor epoch in range(epochs):\n    y_pred = forward_pass(x)\n    \n    l = loss(y,y_pred)\n    \n    l.backward() #dl\/dw\n    \n    #update weights\n    ## ---------IMPORTANT STEP ----------------------\n    with torch.no_grad():\n        w -= learning_rate*w.grad\n    \n    w.grad.zero_()\n    \n    if (epoch +1)%10 == 0:\n        print(f\"Prediction after epoch -> {epoch}: weights : {w[0]:.3f}, loss: {l:.3f}\")","e55b2bf3":"import torch\nimport torch.nn as nn","49837dcd":"## Inputs now must be in the shape of rows and features, see below for more understanding\nx = torch.tensor([[1],[3],[5],[7]],dtype=torch.float32)\ny = torch.tensor([[2],[6],[10],[14]],dtype=torch.float32)","85041eb2":"nsamples,nfeatures = x.shape\nprint(x.shape)","3a8513c1":"input_size = nfeatures\noutput_size = nfeatures","715289fc":"model = nn.Linear(input_size,output_size)\nlearning_rate = 0.01\nepochs = 200","35de493e":"loss = nn.MSELoss()\noptimizer = torch.optim.SGD(model.parameters(),lr=learning_rate)","d519eaef":"for epoch in range(epochs):\n    y_pred = model(x)\n    \n    l = loss(y,y_pred)\n    \n    l.backward()\n    \n    optimizer.step()\n    \n    optimizer.zero_grad()\n    \n    if epoch %20 == 0:\n        w,b = model.parameters()\n        print(f\"prediction after epoch : {epoch} : weight : {w[0][0]:.3f},  loss: {l:.3f}\")\n    ","718fc29d":"# Gradient Descent with Pytorch from scratch","56f84e02":"## Gradient Descent From Scratch","8b0b7b00":"# Implementation of Feedforward Neural network using torch.nn"}}