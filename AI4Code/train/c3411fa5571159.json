{"cell_type":{"c20324db":"code","a7f43e1c":"code","647d4153":"code","7f01bd5f":"code","9a632fba":"code","5ee0d9ad":"code","dd7880f9":"code","46e1c9e0":"code","e7b14a0f":"code","d052e7ee":"code","7c8d1d35":"code","46d340c6":"code","350821b5":"code","262ebcf5":"code","06a450a1":"code","2c42df5c":"code","662fc8da":"code","20fe7b6a":"code","16901ad5":"code","22cede67":"code","4cfabf2e":"code","4eae63f1":"code","fca543be":"code","e7d612bd":"code","b29fe6b8":"code","1c579572":"code","ed00add5":"code","72e9f897":"code","ca244bf0":"code","3cf9a9be":"code","bdda8047":"code","7cf0bbdb":"code","89826a65":"code","6b22c1a4":"code","df720922":"code","e9815251":"code","afe65d8d":"code","e344ce23":"code","51de58f8":"code","61f638e5":"code","dc39b9c6":"code","eccbdbd1":"code","affd2330":"code","bf291745":"code","c411d9f7":"code","6e5ee317":"code","e1d4bea5":"code","0c90ae1a":"code","ba251b35":"code","9dd93ba9":"code","efc24a64":"code","9a610e7f":"code","85fa9b12":"code","6d064f0a":"code","73d9384f":"code","8b59d113":"code","626b695d":"code","03191dc9":"code","c0e79435":"code","a50712a0":"code","9a591c33":"code","b094cd44":"code","2856a33a":"code","90fa81de":"code","80780f48":"code","66e9a0ae":"code","f312a6de":"code","20cf77af":"code","7462cc1b":"code","fd3d9ed3":"code","279474e5":"code","97ffca7f":"code","e52eff84":"code","16d8c60d":"code","1fef0696":"code","ebd8727e":"code","647bf854":"code","5caae13f":"code","84447762":"code","bc4b0a98":"code","303aaec4":"code","29c6ea95":"code","68f8e0a8":"markdown","e526f69f":"markdown","ce158f84":"markdown","044da5da":"markdown","15c0d95e":"markdown","d6b95476":"markdown","9c60d491":"markdown","fce8130f":"markdown","86002cf3":"markdown","97d373a2":"markdown","13b3289a":"markdown","44a83412":"markdown","0976a0af":"markdown","af2a9a81":"markdown","eaae9087":"markdown","bc13e37e":"markdown","f4f8c087":"markdown","d75e9d76":"markdown","61c739d6":"markdown","c289efd7":"markdown","558a71bd":"markdown","bfaa3677":"markdown","1c5ec2ae":"markdown","c0b31e30":"markdown","efc4ea9c":"markdown","79d69100":"markdown","9484a7a7":"markdown","453a4528":"markdown","149f09d7":"markdown","30b41f71":"markdown","673fba15":"markdown","5fa6ffc2":"markdown","c9db19c9":"markdown","c90da3cb":"markdown","a11dd4d7":"markdown","f2f735f6":"markdown","662bc2ad":"markdown","4834816c":"markdown","b152911d":"markdown","747e1608":"markdown","3b132125":"markdown","45bb998f":"markdown","eae81239":"markdown","639107fd":"markdown","1d896cbf":"markdown","15c2ae51":"markdown","5a626909":"markdown","29904ace":"markdown","3cb0bbb1":"markdown"},"source":{"c20324db":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Some visualization libraries\nfrom matplotlib import pyplot as plt\nimport seaborn as sns\n\n## Some other snippit of codes to get the setting right \n## This is so that the chart created by matplotlib can be shown in the jupyter notebook. \n%matplotlib inline \n%config InlineBackend.figure_format = 'retina' ## This is preferable for retina display. \n\n#import warnings ## importing warnings library. \n#warnings.filterwarnings('ignore') ## Ignore warning\n\nimport os ## imporing os\nprint(os.listdir(\"..\/input\/\"))","a7f43e1c":"## Importing the datasets\ntrain = pd.read_csv(\"..\/input\/train.csv\")\ntest = pd.read_csv(\"..\/input\/test.csv\")\n\n","647d4153":"train.head()","7f01bd5f":"print (\"The shape of the train data is (row, column):\"+ str(train.shape))\nprint (train.info())\nprint (\"The shape of the test data is (row, column):\"+ str(test.shape))\nprint (test.info())","9a632fba":"##!!  this block is for delete. I think.\n\n## saving passenger id in advance in order to submit later. \npassengerid = test.PassengerId\n## We will drop PassengerID and Ticket since it will be useless for our data. \n#train.drop(['PassengerId'], axis=1, inplace=True)\n#test.drop(['PassengerId'], axis=1, inplace=True)","5ee0d9ad":"total = train.isnull().sum().sort_values(ascending = False)\npercent = round(train.isnull().sum().sort_values(ascending = False)\/len(train)*100, 2)\npd.concat([total, percent], axis = 1,keys= ['Total', 'Percent'])","dd7880f9":"total = test.isnull().sum().sort_values(ascending = False)\npercent = round(test.isnull().sum().sort_values(ascending = False)\/len(test)*100, 2)\npd.concat([total, percent], axis = 1,keys= ['Total', 'Percent'])","46e1c9e0":"## Concat train and test into a variable \"all_data\"\nsurvivers = train.Survived\n\ntrain.drop([\"Survived\"],axis=1, inplace=True)\n\nall_data = pd.concat([train,test], ignore_index=False)","e7b14a0f":"## were carefull with all_data. data leackage possible if use mean() and so on\n\n\n\n## assign most appropriate value according to charts\nall_data.Embarked.fillna(\"C\", inplace=True)\n\n\n\n## we do not touch \"Age\" feature with Null here, as it is very important","d052e7ee":"## Assign all the null values to N\nall_data.Cabin.fillna(\"N\", inplace=True)\n\n# multiple entries in Cabin grouped into one by first letter\nall_data.Cabin = [i[0] for i in all_data.Cabin]\n\nwith_N = all_data[all_data.Cabin == \"N\"]\n\nwithout_N = all_data[all_data.Cabin != \"N\"]\n\nall_data.groupby(\"Cabin\")['Fare'].mean().sort_values()\n\n\ndef cabin_estimator(i):\n    a = 0\n    if i<16:\n        a = \"G\"\n    elif i>=16 and i<27:\n        a = \"F\"\n    elif i>=27 and i<38:\n        a = \"T\"\n    elif i>=38 and i<47:\n        a = \"A\"\n    elif i>= 47 and i<53:\n        a = \"E\"\n    elif i>= 53 and i<54:\n        a = \"D\"\n    elif i>=54 and i<116:\n        a = 'C'\n    else:\n        a = \"B\"\n    return a\n\n##applying cabin estimator function. \nwith_N['Cabin'] = with_N.Fare.apply(lambda x: cabin_estimator(x))\n\n## getting back train. \nall_data = pd.concat([with_N, without_N], axis=0)","7c8d1d35":"## getting back train. \n\n## PassengerId helps us separate train and test. \nall_data.sort_values(by = 'PassengerId', inplace=True)\n\n## Separating train and test from all_data. \ntrain = all_data[:891]\n\ntest = all_data[891:]\n\n## replace \"fare\" null values with mean vallue for similar records\nmissing_value = test[(test.Pclass == 3) & (test.Embarked == 'S') & (test.Sex == 'male')].Fare.mean()\ntest.Fare.fillna(missing_value, inplace=True)\n\n# adding saved target variable with train. \ntrain['Survived'] = survivers\n\n# Placing 0 for female and \n# 1 for male in the \"Sex\" column. \ntrain['Sex'] = train['Sex'].apply(lambda x: 0 if x == \"female\" else 1)\ntest['Sex'] = test['Sex'].apply(lambda x: 0 if x == \"female\" else 1)\n","46d340c6":"print(train.shape,test.shape)","350821b5":"## a lot of charts here","262ebcf5":"train.describe()","06a450a1":"#many other statistics here","2c42df5c":"pd.DataFrame(abs(train.corr()['Survived']).sort_values(ascending = False))","662fc8da":"## get the most important variables. \ncorr = train.corr()**2\ncorr.Survived.sort_values(ascending=False)","20fe7b6a":"## heatmeap to see the correlation between features. \n# Generate a mask for the upper triangle (taken from seaborn example gallery)\nmask = np.zeros_like(train.corr(), dtype=np.bool)\n#mask[np.triu_indices_from(mask)] = True\n\nplt.subplots(figsize = (15,12))\nsns.heatmap(train.corr(), \n            annot=True,\n            #mask = mask,\n            cmap = 'RdBu_r',\n            linewidths=0.1, \n            linecolor='white',\n            vmax = .9,\n            square=True)\nplt.title(\"Correlations Among Features\", y = 1.03,fontsize = 20);","16901ad5":"# Hypothesis testing (null hypothesis) and p-value","22cede67":"train['name_length'] = [len(i) for i in train.Name]\ntest['name_length'] = [len(i) for i in test.Name]\n\ndef name_length_group(size):\n    a = ''\n    if (size <=20):\n        a = 'short'\n    elif (size <=35):\n        a = 'medium'\n    elif (size <=45):\n        a = 'good'\n    else:\n        a = 'long'\n    return a\n\n\ntrain['nLength_group'] = train['name_length'].map(name_length_group)\ntest['nLength_group'] = test['name_length'].map(name_length_group)\n\n","4cfabf2e":"print(train.shape,test.shape)","4eae63f1":"## get the title from the name\ntrain[\"title\"] = [i.split('.')[0] for i in train.Name]\ntrain[\"title\"] = [i.split(',')[1] for i in train.title]\ntest[\"title\"] = [i.split('.')[0] for i in test.Name]\ntest[\"title\"]= [i.split(',')[1] for i in test.title]\n\n#rare_title = ['the Countess','Capt','Lady','Sir','Jonkheer','Don','Major','Col']\n#train.Name = ['rare' for i in train.Name for j in rare_title if i == j]\n## train Data\ntrain[\"title\"] = [i.replace('Ms', 'Miss') for i in train.title]\ntrain[\"title\"] = [i.replace('Mlle', 'Miss') for i in train.title]\ntrain[\"title\"] = [i.replace('Mme', 'Mrs') for i in train.title]\ntrain[\"title\"] = [i.replace('Dr', 'rare') for i in train.title]\ntrain[\"title\"] = [i.replace('Col', 'rare') for i in train.title]\ntrain[\"title\"] = [i.replace('Major', 'rare') for i in train.title]\ntrain[\"title\"] = [i.replace('Don', 'rare') for i in train.title]\ntrain[\"title\"] = [i.replace('Jonkheer', 'rare') for i in train.title]\ntrain[\"title\"] = [i.replace('Sir', 'rare') for i in train.title]\ntrain[\"title\"] = [i.replace('Lady', 'rare') for i in train.title]\ntrain[\"title\"] = [i.replace('Capt', 'rare') for i in train.title]\ntrain[\"title\"] = [i.replace('the Countess', 'rare') for i in train.title]\ntrain[\"title\"] = [i.replace('Rev', 'rare') for i in train.title]\n\n\n\n#rare_title = ['the Countess','Capt','Lady','Sir','Jonkheer','Don','Major','Col']\n#train.Name = ['rare' for i in train.Name for j in rare_title if i == j]\n## test data\ntest['title'] = [i.replace('Ms', 'Miss') for i in test.title]\ntest['title'] = [i.replace('Dr', 'rare') for i in test.title]\ntest['title'] = [i.replace('Col', 'rare') for i in test.title]\ntest['title'] = [i.replace('Dona', 'rare') for i in test.title]\ntest['title'] = [i.replace('Rev', 'rare') for i in test.title]","fca543be":"print(train.shape,test.shape)","e7d612bd":"## Family_size seems like a good feature to create\ntrain['family_size'] = train.SibSp + train.Parch+1\ntest['family_size'] = test.SibSp + test.Parch+1\n\ndef family_group(size):\n    a = ''\n    if (size <= 1):\n        a = 'loner'\n    elif (size <= 4):\n        a = 'small'\n    else:\n        a = 'large'\n    return a\n\ntrain['family_group'] = train['family_size'].map(family_group)\ntest['family_group'] = test['family_size'].map(family_group)\n\n","b29fe6b8":"print(train.shape,test.shape)","1c579572":"train['is_alone'] = [1 if i<2 else 0 for i in train.family_size]\ntest['is_alone'] = [1 if i<2 else 0 for i in test.family_size]\n\n","ed00add5":"print(train.shape,test.shape)","72e9f897":"#don't know how to manage this features, so drop it.\n\ntrain.drop(['Ticket'], axis=1, inplace=True)\ntest.drop(['Ticket'], axis=1, inplace=True)\n\ntrain.drop(['PassengerId'], axis=1, inplace=True)\ntest.drop(['PassengerId'], axis=1, inplace=True)\n\n","ca244bf0":"print(train.shape,test.shape)","3cf9a9be":"## Calculating fare based on family size. \ntrain['calculated_fare'] = train.Fare\/train.family_size\ntest['calculated_fare'] = test.Fare\/test.family_size\n\n","bdda8047":"print(train.shape,test.shape)","7cf0bbdb":"def fare_group(fare):\n    a= ''\n    if fare <= 4:\n        a = 'Very_low'\n    elif fare <= 10:\n        a = 'low'\n    elif fare <= 20:\n        a = 'mid'\n    elif fare <= 45:\n        a = 'high'\n    else:\n        a = \"very_high\"\n    return a\n\ntrain['fare_group'] = train['calculated_fare'].map(fare_group)\ntest['fare_group'] = test['calculated_fare'].map(fare_group)\n\n#train['fare_group'] = pd.cut(train['calculated_fare'], bins = 5, labels=('Very_low','low','mid','high','very_high'))\n\n","89826a65":"print(train.shape,test.shape)","6b22c1a4":"#way of moving from categorical variables to numbers\n\ntrain = pd.get_dummies(train, columns=['title',\"Pclass\", 'Cabin','Embarked','nLength_group', 'family_group', 'fare_group'], drop_first=False)\ntest = pd.get_dummies(test, columns=['title',\"Pclass\",'Cabin','Embarked','nLength_group', 'family_group', 'fare_group'], drop_first=False)\ntrain.drop(['family_size','Name', 'Fare','name_length'], axis=1, inplace=True)\ntest.drop(['Name','family_size',\"Fare\",'name_length'], axis=1, inplace=True)","df720922":"print(train.shape,test.shape)\ntrain.head()","e9815251":"## rearranging the columns so that I can easily use the dataframe to predict the missing age values. \ntrain = pd.concat([train[[\"Survived\", \"Age\", \"Sex\",\"SibSp\",\"Parch\"]], train.loc[:,\"is_alone\":]], axis=1)\ntest = pd.concat([test[[\"Age\", \"Sex\"]], test.loc[:,\"SibSp\":]], axis=1)\n\n## Importing RandomForestRegressor\nfrom sklearn.ensemble import RandomForestRegressor\n\n## writing a function that takes a dataframe with missing values and outputs it by filling the missing values. \ndef completing_age(df):\n    ## gettting all the features except survived\n    age_df = df.loc[:,\"Age\":] \n    \n    temp_train = age_df.loc[age_df.Age.notnull()] ## df with age values\n    temp_test = age_df.loc[age_df.Age.isnull()] ## df without age values\n    \n    y = temp_train.Age.values ## setting target variables(age) in y \n    x = temp_train.loc[:, \"Sex\":].values\n    \n    rfr = RandomForestRegressor(n_estimators=1500, n_jobs=-1)\n    rfr.fit(x, y)\n    \n    predicted_age = rfr.predict(temp_test.loc[:, \"Sex\":])\n    \n    df.loc[df.Age.isnull(), \"Age\"] = predicted_age\n    \n\n    return df\n\n## Implementing the completing_age function in both train and test dataset. \ncompleting_age(train)\ncompleting_age(test)","afe65d8d":"## create bins for age\ndef age_group_fun(age):\n    a = ''\n    if age <= 1:\n        a = 'infant'\n    elif age <= 4: \n        a = 'toddler'\n    elif age <= 13:\n        a = 'child'\n    elif age <= 18:\n        a = 'teenager'\n    elif age <= 35:\n        a = 'Young_Adult'\n    elif age <= 45:\n        a = 'adult'\n    elif age <= 55:\n        a = 'middle_aged'\n    elif age <= 65:\n        a = 'senior_citizen'\n    else:\n        a = 'old'\n    return a\n        \n## Applying \"age_group_fun\" function to the \"Age\" column.\ntrain['age_group'] = train['Age'].map(age_group_fun)\ntest['age_group'] = test['Age'].map(age_group_fun)\n\n## Creating dummies for \"age_group\" feature. \ntrain = pd.get_dummies(train,columns=['age_group'], drop_first=True)\ntest = pd.get_dummies(test,columns=['age_group'], drop_first=True);\n\n","e344ce23":"print(train.shape,test.shape)","51de58f8":"X = train.drop(['Survived'], axis = 1)\ny = train[\"Survived\"]\n\n\n","61f638e5":"print(X.shape,y.shape)","dc39b9c6":"from sklearn.model_selection import train_test_split\ntrain_x, test_x, train_y, test_y = train_test_split(X,y,test_size = .33, random_state = 0)","eccbdbd1":"print(train_x.shape, test_x.shape)","affd2330":"headers = train_x.columns \n\ntrain_x.head()","bf291745":"\n\n# Feature Scaling\n## We will be using standardscaler to transform\nfrom sklearn.preprocessing import StandardScaler\nsc = StandardScaler()\n\n## transforming \"train_x\"\ntrain_x = sc.fit_transform(train_x)\n## transforming \"test_x\"\ntest_x = sc.transform(test_x)\n\n## transforming \"The testset\"\ntest = sc.transform(test)","c411d9f7":"pd.DataFrame(train_x, columns=headers).head()","6e5ee317":"#why this?...\ntrain.calculated_fare = train.calculated_fare.astype(float)","e1d4bea5":"# import LogisticRegression model in python. \nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import mean_absolute_error, accuracy_score\n\n## call on the model object\nlogreg = LogisticRegression(solver='liblinear')\n\n## fit the model with \"train_x\" and \"train_y\"\nlogreg.fit(train_x,train_y)\n\n## Once the model is trained we want to find out how well the model is performing, so we test the model. \n## we use \"test_x\" portion of the data(this data was not used to fit the model) to predict model outcome. \ny_pred = logreg.predict(test_x)\n\n## Once predicted we save that outcome in \"y_pred\" variable.\n## Then we compare the predicted value( \"y_pred\") and actual value(\"test_y\") to see how well our model is performing. \n\nprint (\"So, Our accuracy Score is: {}\".format(round(accuracy_score(y_pred, test_y),4)))","0c90ae1a":"from sklearn.metrics import roc_curve, auc\n#plt.style.use('seaborn-pastel')\ny_score = logreg.decision_function(test_x)\n\nFPR, TPR, _ = roc_curve(test_y, y_score)\nROC_AUC = auc(FPR, TPR)\nprint (ROC_AUC)\n\nplt.figure(figsize =[11,9])\nplt.plot(FPR, TPR, label= 'ROC curve(area = %0.2f)'%ROC_AUC, linewidth= 4)\nplt.plot([0,1],[0,1], 'k--', linewidth = 4)\nplt.xlim([0.0,1.0])\nplt.ylim([0.0,1.05])\nplt.xlabel('False Positive Rate', fontsize = 18)\nplt.ylabel('True Positive Rate', fontsize = 18)\nplt.title('ROC for Titanic survivors', fontsize= 18)\nplt.show()","ba251b35":"from sklearn.metrics import precision_recall_curve\n\ny_score = logreg.decision_function(test_x)\n\nprecision, recall, _ = precision_recall_curve(test_y, y_score)\nPR_AUC = auc(recall, precision)\n\nplt.figure(figsize=[11,9])\nplt.plot(recall, precision, label='PR curve (area = %0.2f)' % PR_AUC, linewidth=4)\nplt.xlabel('Recall', fontsize=18)\nplt.ylabel('Precision', fontsize=18)\nplt.title('Precision Recall Curve for Titanic survivors', fontsize=18)\nplt.legend(loc=\"lower right\")\nplt.show()","9dd93ba9":"## Using StratifiedShuffleSplit\n## We can use KFold, StratifiedShuffleSplit, StratiriedKFold or ShuffleSplit, They are all close cousins. look at sklearn userguide for more info.   \nfrom sklearn.model_selection import StratifiedShuffleSplit, cross_val_score\ncv = StratifiedShuffleSplit(n_splits = 10, test_size = .25, random_state = 0 ) # run model 10x with 60\/30 split intentionally leaving out 10%\n## Using standard scale for the whole dataset.\nX = sc.fit_transform(X)\naccuracies = cross_val_score(LogisticRegression(), X,y, cv  = cv)\nprint (\"Cross-Validation accuracy scores:{}\".format(accuracies))\nprint (\"Mean Cross-Validation accuracy score: {}\".format(round(accuracies.mean(),5)))","efc24a64":"from sklearn.model_selection import GridSearchCV, StratifiedKFold\n## C_vals is the alpla value of lasso and ridge regression(as alpha increases the model complexity decreases,)\n## remember effective alpha scores are 0<alpha<infinity \nC_vals = [0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9,1,2,3,4,5,6,7,8,9,10,12,13,14,15,16,16.5,17,17.5,18]\n## Choosing penalties(Lasso(l1) or Ridge(l2))\npenalties = ['l1','l2']\n## Choose a cross validation strategy. \ncv = StratifiedShuffleSplit(n_splits = 10, test_size = .25)\n\n## setting param for param_grid in GridSearchCV. \nparam = {'penalty': penalties, 'C': C_vals}\n\nlogreg = LogisticRegression(solver='liblinear')\n## Calling on GridSearchCV object. \ngrid = GridSearchCV(estimator=LogisticRegression(), \n                           param_grid = param,\n                           scoring = 'accuracy',\n                            n_jobs =-1,\n                           cv = cv\n                          )\n## Fitting the model\ngrid.fit(X, y)","9a610e7f":"## Getting the best of everything. \nprint (grid.best_score_)\nprint (grid.best_params_)\nprint(grid.best_estimator_)","85fa9b12":"### Using the best parameters from the grid-search.\nlogreg_grid = grid.best_estimator_\nlogreg_grid.score(X,y)","6d064f0a":"## Importing the model. \nfrom sklearn.neighbors import KNeighborsClassifier\n## calling on the model oject. \nknn = KNeighborsClassifier(metric='minkowski', p=2)\n## knn classifier works by doing euclidian distance \n\n\n## doing 10 fold staratified-shuffle-split cross validation \ncv = StratifiedShuffleSplit(n_splits=10, test_size=.25, random_state=2)\n\naccuracies = cross_val_score(knn, X,y, cv = cv, scoring='accuracy')\nprint (\"Cross-Validation accuracy scores:{}\".format(accuracies))\nprint (\"Mean Cross-Validation accuracy score: {}\".format(round(accuracies.mean(),3)))","73d9384f":"## Search for an optimal value of k for KNN.\nk_range = range(1,31)\nk_scores = []\nfor k in k_range:\n    knn = KNeighborsClassifier(n_neighbors=k)\n    scores = cross_val_score(knn, X,y, cv = cv, scoring = 'accuracy')\n    k_scores.append(scores.mean())\nprint(\"Accuracy scores are: {}\\n\".format(k_scores))\nprint (\"Mean accuracy score: {}\".format(np.mean(k_scores)))","8b59d113":"from matplotlib import pyplot as plt\nplt.plot(k_range, k_scores)","626b695d":"from sklearn.model_selection import GridSearchCV\n## trying out multiple values for k\nk_range = range(1,31)\n## \nweights_options=['uniform','distance']\n# \nparam = {'n_neighbors':k_range, 'weights':weights_options}\n## Using startifiedShufflesplit. \ncv = StratifiedShuffleSplit(n_splits=10, test_size=.30, random_state=15)\n# estimator = knn, param_grid = param, n_jobs = -1 to instruct scikit learn to use all available processors. \ngrid = GridSearchCV(KNeighborsClassifier(), param,cv=cv,verbose = False, n_jobs=-1)\n## Fitting the model. \ngrid.fit(X,y)","03191dc9":"print (grid.best_score_)\nprint (grid.best_params_)\nprint(grid.best_estimator_)","c0e79435":"### Using the best parameters from the grid-search.\nknn_grid= grid.best_estimator_\nknn_grid.score(X,y)","a50712a0":"from sklearn.model_selection import RandomizedSearchCV\n## trying out multiple values for k\nk_range = range(1,31)\n## \nweights_options=['uniform','distance']\n# \nparam = {'n_neighbors':k_range, 'weights':weights_options}\n## Using startifiedShufflesplit. \ncv = StratifiedShuffleSplit(n_splits=10, test_size=.30, random_state=15)\n# estimator = knn, param_grid = param, n_jobs = -1 to instruct scikit learn to use all available processors. \n## for RandomizedSearchCV, \ngrid = RandomizedSearchCV(KNeighborsClassifier(), param,cv=cv,verbose = False, n_jobs=-1, n_iter=40)\n## Fitting the model. \ngrid.fit(X,y)","9a591c33":"print (grid.best_score_)\nprint (grid.best_params_)\nprint(grid.best_estimator_)","b094cd44":"### Using the best parameters from the grid-search.\nknn_ran_grid = grid.best_estimator_\nknn_ran_grid.score(X,y)","2856a33a":"# Gaussian Naive Bayes\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.metrics import accuracy_score\n\ngaussian = GaussianNB()\ngaussian.fit(X, y)\ny_pred = gaussian.predict(test_x)\ngaussian_accy = round(accuracy_score(y_pred, test_y), 3)\nprint(gaussian_accy)","90fa81de":"from sklearn.svm import SVC\nCs = [0.001, 0.01, 0.1, 1,1.5,2,2.5,3,4,5, 10] ## penalty parameter C for the error term. \ngammas = [0.0001,0.001, 0.01, 0.1, 1]\nparam_grid = {'C': Cs, 'gamma' : gammas}\ncv = StratifiedShuffleSplit(n_splits=10, test_size=.30, random_state=15)\ngrid_search = GridSearchCV(SVC(kernel = 'rbf', probability=True), param_grid, cv=cv) ## 'rbf' stands for gaussian kernel\ngrid_search.fit(X,y)","80780f48":"print(grid_search.best_score_)\nprint(grid_search.best_params_)\nprint(grid_search.best_estimator_)","66e9a0ae":"# using the best found hyper paremeters to get the score. \nsvm_grid = grid_search.best_estimator_\nsvm_grid.score(X,y)","f312a6de":"from sklearn.tree import DecisionTreeClassifier\nmax_depth = range(1,30)\nmax_feature = [21,22,23,24,25,26,28,29,30,'auto']\ncriterion=[\"entropy\", \"gini\"]\n\nparam = {'max_depth':max_depth, \n         'max_features':max_feature, \n         'criterion': criterion}\ngrid = GridSearchCV(DecisionTreeClassifier(), \n                                param_grid = param, \n                                 verbose=False, \n                                 cv=StratifiedKFold(n_splits=20, random_state=15, shuffle=True),\n                                n_jobs = -1)\ngrid.fit(X, y) ","20cf77af":"print( grid.best_params_)\nprint (grid.best_score_)\nprint (grid.best_estimator_)","7462cc1b":"dectree_grid = grid.best_estimator_\n## using the best found hyper paremeters to get the score. \ndectree_grid.score(X,y)","fd3d9ed3":"import graphviz\n\nfrom sklearn import tree\n\ndot_data = tree.export_graphviz(dectree_grid, out_file=None)\n\ngraph = graphviz.Source(dot_data)\n\ngraph.render(\"house\")\n\ngraph","279474e5":"from sklearn.ensemble import BaggingClassifier\nBaggingClassifier = BaggingClassifier()\nBaggingClassifier.fit(X, y)\ny_pred = BaggingClassifier.predict(test_x)\nbagging_accy = round(accuracy_score(y_pred, test_y), 3)\nprint(bagging_accy)","97ffca7f":"from sklearn.ensemble import RandomForestClassifier\nn_estimators = [90,95,100,105,110]\nmax_depth = range(1,30)\ncv = StratifiedShuffleSplit(n_splits=10, test_size=.30, random_state=15)\n\n\nparameters = {'n_estimators':n_estimators, \n         'max_depth':max_depth, \n        }\ngrid = GridSearchCV(RandomForestClassifier(),\n                                 param_grid=parameters,\n                                 cv=cv,\n                                 n_jobs = -1)\ngrid.fit(X,y) ","e52eff84":"print (grid.best_score_)\nprint (grid.best_params_)\nprint (grid.best_estimator_)","16d8c60d":"rf_grid = grid.best_estimator_\nrf_grid.score(X,y)","1fef0696":"# Gradient Boosting Classifier\nfrom sklearn.ensemble import GradientBoostingClassifier\n\ngradient = GradientBoostingClassifier()\ngradient.fit(X, y)\ny_pred = gradient.predict(test_x)\ngradient_accy = round(accuracy_score(y_pred, test_y), 3)\nprint(gradient_accy)","ebd8727e":"from xgboost import XGBClassifier\nXGBClassifier = XGBClassifier()\nXGBClassifier.fit(X, y)\ny_pred = XGBClassifier.predict(test_x)\nXGBClassifier_accy = round(accuracy_score(y_pred, test_y), 3)\nprint(XGBClassifier_accy)","647bf854":"from sklearn.ensemble import AdaBoostClassifier\nadaboost = AdaBoostClassifier()\nadaboost.fit(X, y)\ny_pred = adaboost.predict(test_x)\nadaboost_accy = round(accuracy_score(y_pred, test_y), 3)\nprint(adaboost_accy)","5caae13f":"from sklearn.ensemble import ExtraTreesClassifier\nExtraTreesClassifier = ExtraTreesClassifier()\nExtraTreesClassifier.fit(X, y)\ny_pred = ExtraTreesClassifier.predict(test_x)\nextraTree_accy = round(accuracy_score(y_pred, test_y), 3)\nprint(extraTree_accy)","84447762":"from sklearn.gaussian_process import GaussianProcessClassifier\nGaussianProcessClassifier = GaussianProcessClassifier()\nGaussianProcessClassifier.fit(X, y)\ny_pred = GaussianProcessClassifier.predict(test_x)\ngau_pro_accy = round(accuracy_score(y_pred, test_y), 3)\nprint(gau_pro_accy)","bc4b0a98":"from sklearn.ensemble import VotingClassifier\n\nvoting_classifier = VotingClassifier(estimators=[\n    ('logreg_grid', logreg_grid),\n    ('svc', svm_grid),\n    ('random_forest', rf_grid),\n    ('gradient_boosting', gradient),\n    ('decision_tree_grid',dectree_grid),\n    ('knn_grid', knn_grid),\n    ('XGB Classifier', XGBClassifier),\n    ('BaggingClassifier', BaggingClassifier),\n    ('ExtraTreesClassifier', ExtraTreesClassifier),\n    ('gaussian',gaussian),\n    ('gaussian process classifier', GaussianProcessClassifier)\n    ], voting='soft')\n\nvoting_classifier = voting_classifier.fit(train_x,train_y)\n\ny_pred = voting_classifier.predict(test_x)\nvoting_accy = round(accuracy_score(y_pred, test_y), 3)\nprint(voting_accy)","303aaec4":"all_models = [voting_classifier\n              ,knn_grid\n              ,GaussianProcessClassifier, gaussian, ExtraTreesClassifier, BaggingClassifier\n              , XGBClassifier,  dectree_grid, gradient, rf_grid, svm_grid, logreg_grid\n             ]\n\nc = {}\nfor i in all_models:\n    a = i.predict(test_x)\n    b = accuracy_score(a, test_y)\n    c[i] = b","29c6ea95":"test_prediction = (max(c, key=c.get)).predict(test)\nsubmission = pd.DataFrame({\n        \"PassengerId\": passengerid,\n        \"Survived\": test_prediction\n    })\n\nsubmission.PassengerId = submission.PassengerId.astype(int)\nsubmission.Survived = submission.Survived.astype(int)\n\nsubmission.to_csv(\"titanic1_submission.csv\", index=False)","68f8e0a8":"Getting back train and test datasets","e526f69f":"7.2. K-Nearest Neighbor classifier(KNN)","ce158f84":"Cleaning data:","044da5da":"7.8. Gradient Boosting Classifier","15c0d95e":"AUC & ROC Curve","d6b95476":"missing data in Train dataset","9c60d491":"7.10. AdaBoost Classifier","fce8130f":"7.12. Gaussian Process Classifier","86002cf3":"Age feature (some magic that fills empty age with predicted age using RandomForestRegressor)","97d373a2":"Part 1.2. importing datasets","13b3289a":"7.6. Bagging Classifier","44a83412":"5.1. Creation of new features","0976a0af":"4.1. Correlation Matrix and Heatmap","af2a9a81":"before scaling","eaae9087":"Positive Correlation Features:\n* Fare and Survived: 0.26\n\nNegative Correlation Features:\n* Fare and Pclass: -0.6\n* Sex and Survived: -0.55\n* Pclass and Survived: -0.33","bc13e37e":"cross validation","f4f8c087":"7.5. Decision Tree Classifier","d75e9d76":"**Part 7: Modeling the Data**","61c739d6":"2.1. Cleaning data","c289efd7":"Grid Search on Logistic Regression","558a71bd":"**Part 4: Statistical Overview**","bfaa3677":"**Part 3. Visualization and Feature Relations**","1c5ec2ae":"6.1. Separating dependent and independent variables","c0b31e30":"Using RandomizedSearchCV","efc4ea9c":"Combine two data sets (test and train) and save \"answer\" to variable.","79d69100":"**Part 5: Feature Engineering**","9484a7a7":"Part 6: Pre-Modeling Tasks","453a4528":"7.3. Gaussian Naive Bayes","149f09d7":"Click \"+Add Dataset\" to add data from competition","30b41f71":"Some magic with \"Cabin\" feature","673fba15":"Part 1.1. import of libraries","5fa6ffc2":"7.9. XGBClassifier","c9db19c9":"1.3. Look into data","c90da3cb":"Missing values in test set.","a11dd4d7":"5.2. Creating dummy variables","f2f735f6":"!(need to update) Charts and group by, to understand missing data","662bc2ad":"**Part 8: Submit test predictions**","4834816c":"6.3. Feature Scaling","b152911d":"**Part 2: Overview and Cleaning the Data**","747e1608":"grid search","3b132125":"choosing best model","45bb998f":"7.11. Extra Trees Classifier","eae81239":"7.1. Logistic Regression","639107fd":"**Part1. Import modules and data**","1d896cbf":"7.7 Random Forest Classifier","15c2ae51":"after scaling","5a626909":"6.2. Splitting the training data","29904ace":"7.4. Support Vector Machines(SVM)","3cb0bbb1":"manual search:"}}