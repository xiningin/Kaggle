{"cell_type":{"caca0f93":"code","c851d918":"code","7a590910":"code","2a9e4ee9":"code","a2a1ec5e":"code","b6627395":"code","ed7f2aa0":"code","373a827f":"code","d289c01c":"code","baf78e0f":"code","39d4c3f2":"code","b2b1a4c7":"code","34dfd0ec":"code","0c49c43b":"code","58489015":"code","8f7188d4":"code","376bb6c9":"code","da8cd079":"code","b1d59c0e":"code","b88a704e":"code","7efb0647":"code","2126df60":"code","87f1e243":"code","4e641de0":"code","1fe4d2e0":"code","9e8e556c":"code","d515849d":"code","2c4adcc3":"code","422eea68":"code","362beb29":"code","a4f98281":"code","c4466429":"code","976abb1e":"code","33e1af83":"code","af66eee1":"code","8d473d4a":"code","0ce7b87c":"code","a486f5a8":"code","29dc65ae":"code","7a3cbd19":"code","d8b79073":"code","880fe34c":"code","1c0a59cd":"markdown","5c2e80a1":"markdown","6e960927":"markdown","ffaf4ffa":"markdown","fc4172d6":"markdown","f48b49ce":"markdown","8bdc9b7e":"markdown","664f1900":"markdown","ff9b2512":"markdown","2ce7ddf3":"markdown","08ee07ec":"markdown","61d1c0c7":"markdown","f856a5bb":"markdown","89e4d6bc":"markdown"},"source":{"caca0f93":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session\n\n","c851d918":"from zipfile import ZipFile \n  \nwith ZipFile('\/kaggle\/input\/quora-insincere-questions-classification\/embeddings.zip', 'r') as embd_zip: \n    print(embd_zip.namelist())\n","7a590910":"def DEBUG_DICTIONARY(dct, limit=10):\n    for i, key in enumerate(dct.keys()):\n        if i > limit: break\n        print(key, dct[key])","2a9e4ee9":"from sklearn.model_selection import train_test_split\n\n# configure train and validation data\ntrain_data, val_data = train_test_split(pd.read_csv('\/kaggle\/input\/quora-insincere-questions-classification\/train.csv'), test_size=0.2, random_state=42)\nsentences, targets = train_data['question_text'], train_data['target']\nval_sentences, val_targets = val_data['question_text'], val_data['target']","a2a1ec5e":"train_data.head()","b6627395":"targets.value_counts(), val_targets.value_counts()","ed7f2aa0":"# for each word - counts how many times it occurs totally in sentences \ndef configure_sentences(sentences, lower = True):\n    words = {}\n    for sentence in sentences:\n        for word in sentence.split():\n            if lower: word = word.lower()\n            words[word] = words.get(word, 0) + 1\n    return words\n\nwords = configure_sentences(sentences)\n# look to frequencies of words in sentences\nDEBUG_DICTIONARY(words)","373a827f":"# find out what kind of words are frequently used\nDEBUG_DICTIONARY({word: cnt for word, cnt in sorted(words.items(), key=lambda item: item[1], reverse=True)})","d289c01c":"# indexing words - in case needed\ndef configure_words(words):\n    vocabulary = {}\n    for i, word in enumerate(words.keys()):\n        vocabulary[word] = i # vocabulary[i] = word\n    return vocabulary\n\nvocabulary = configure_words(words)\nDEBUG_DICTIONARY(vocabulary)","baf78e0f":"import matplotlib.pyplot as plt\n%matplotlib inline","39d4c3f2":"# returns min, avrg and max sentence length - and also displays plot(histogram) for length distribution \ndef configure_sentence_statistic(sentences):\n    def sentence_len(s):\n        return len(s.split())\n    \n    sentences.apply(sentence_len).plot(title='Sentence Length Distribution',y='Length Frequency',kind='hist', colormap='autumn', logy=True);\n    return np.min(sentences.apply(sentence_len)), np.round(np.mean(sentences.apply(sentence_len))), np.max(sentences.apply(sentence_len))\n\nmin, avrg, max = configure_sentence_statistic(sentences)\n\nprint('minimum sentence length {} - average sentence length {} - maximum sentence length {}'.format(min, avrg, max))","b2b1a4c7":"HIDDEN_SIZE = 30","34dfd0ec":"import plotly.offline as py\nimport plotly.graph_objs as go\npy.init_notebook_mode(connected=True)\n\n# display how the targets are distributed\ndef configure_target_statistic(targets):\n    trg_cnt = targets.value_counts()\n    labels, sizes = (np.array(trg_cnt.index)), (np.array(100*(trg_cnt\/trg_cnt.sum())))\n    py.iplot(go.Figure(data=[go.Pie(labels=labels, values=sizes)], layout=go.Layout(title='Target Distribution',font=dict(size=15),width=500, height=500)))\n    return trg_cnt\n\nconfigure_target_statistic(targets)","0c49c43b":"# filters data according to given parameters\ndef filter_and_display_data(sentences, targets, target=0, min_len=5, max_len=30, limit=3):\n    result = []\n    for i, sentence in enumerate(sentences):\n        sent_len = len(sentence.split(' '))\n        if min_len <= sent_len and sent_len <= max_len:\n            if targets[i] == target:\n                result.append(sentence)\n                if len(result) >= limit: break\n    \n    if(len(result) ==- 0):\n        print('no such sequencies found.')\n        return\n    \n    print('{} {} sentences with length between {}-{}:\\n'.format(limit, 'GOOD' if target == 0 else 'BAD', min_len, max_len))\n    for i, s in enumerate(result):\n        print(str(i+1)+\")\",s)\n","58489015":"filter_and_display_data(sentences, np.asarray(targets, dtype='int'), target=0)","8f7188d4":"filter_and_display_data(sentences, np.asarray(targets, dtype='int'), target=1)","376bb6c9":"filter_and_display_data(sentences, np.asarray(targets, dtype='int'), target=0, min_len=120, max_len=140)","da8cd079":"filter_and_display_data(sentences, np.asarray(targets, dtype='int'), target=1, min_len=120, max_len=140)","b1d59c0e":"# reads and returns dictionary - key: word; value: word's embedding vector (vec. length=300)\ndef confnigure_embeddings(embd_path):\n    word2vecs = {}\n    with ZipFile('\/kaggle\/input\/quora-insincere-questions-classification\/embeddings.zip') as embd_zip:\n        for embd in embd_zip.open(embd_path, 'r'):\n            word2vec = embd.decode().split(' ')\n            word2vecs[word2vec[0]] = np.asarray(word2vec[1:], dtype='float32')\n    return word2vecs\n            \nword2vecs = confnigure_embeddings('glove.840B.300d\/glove.840B.300d.txt')\nDEBUG_DICTIONARY(word2vecs, limit=1)","b88a704e":"# in each sentence replaces words with its own embedding vectors \ndef configure_word2vecs(sentences, word2vecs):\n    def configure_sentence(sentence, len=HIDDEN_SIZE):\n        return ([word2vecs.get(word.lower(), np.zeros(300)) for word in sentence.split()] + [np.zeros(300)]*len)[:len] \n    \n    return [configure_sentence(sentence) for sentence in sentences]\n\n# embedding_sentences = configure_word2vecs(sentences, word2vecs)\n# print(embedding_sentences[:10])","7efb0647":"import torch\nimport torch.nn as nn\n\nimport seaborn as sns\nimport numpy as np\nimport pandas as pd","2126df60":"BATCH_SIZE = 256\nBATCHES = (len(sentences)+BATCH_SIZE-1)\/\/BATCH_SIZE\n\nEPOCHS = 2 # gpu :(\nEMBD_SIZE = 300","87f1e243":"gpu = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\ngpu, torch.cuda.is_available()","4e641de0":"# long short-term memory is best suit for this case i think - as we got 1M+ train data and embedding vectors with length 300, \n# if we just convert evrything once in tensors we need more than 16gb ram and much more resources to train this data.\n# also use linear layer should be good enough as there is no hard dependences - as if sentence contains 'bad' word its target is most likly 1.\n# dropout layer would be good also - but as we are using only one lstm layer bc of cpu - we dont...\n\nclass LSTM(nn.Module):\n    def __init__(self, input_dim=1, emb_dim=EMBD_SIZE, hid_dim=HIDDEN_SIZE, n_layers=1, output_dim=1, dropout=0.3):\n        super().__init__()\n        self.hid_dim, self.n_layers = hid_dim, n_layers\n        \n        # nn's\n        self.lstm = nn.LSTM(emb_dim, hid_dim, n_layers, batch_first=True)\n        self.linear = nn.Linear(hid_dim, output_dim)\n        \n        ### NOTE ### for dropout lstm layers has to be more than 1 - but bc of my code works only cpu i got one layer :( so it doesn't works...\n#         self.dropout = nn.Dropout(dropout)\n        \n        \n    def forward(self, src):\n        outputs, (hidden, cell) = self.lstm(src)\n        return self.linear(hidden.reshape(-1, self.hid_dim))\n","1fe4d2e0":"# craete model - with lstm and linear layers\nmodel = LSTM().to(gpu)\n\n# init loss function\nloss_function = nn.BCEWithLogitsLoss().to(gpu) #nn.MSELoss()\n\n# init optimizer with learning rate 0.001\noptimizer = torch.optim.Adam(model.parameters(), lr=0.001)","9e8e556c":"model","d515849d":"# evaluates and returns accuracy for predicted Y by model \ndef acc_function(y_pred, y_test):\n    y_pred = torch.round(torch.sigmoid(y_pred).to(gpu)).to(gpu)\n    correct = (y_pred == y_test).sum().float()\n    return torch.round(100*(correct\/y_pred.shape[0]))\n\n# generates and returns idx-th batch as torch tensor according to given data(sentences and targets)\ndef get_batch(sentences, targets, idx):\n    src = configure_word2vecs(sentences[BATCH_SIZE*idx:BATCH_SIZE*(idx+1)], word2vecs)\n    trg = np.asarray(targets[BATCH_SIZE*idx:BATCH_SIZE*(idx+1)], dtype='bool')\n    return torch.FloatTensor(src).to(gpu), torch.FloatTensor(trg).to(gpu)\n\n# evaluates and returns f1 score for predicted Y by model \ndef f1_score(y_pred, y_test):\n    tp = (y_test * y_pred).sum().to(torch.float32)\n    tn = ((1 - y_test) * (1 - y_pred)).sum().to(torch.float32)\n    fp = ((1 - y_test) * y_pred).sum().to(torch.float32)\n    fn = (y_test * (1 - y_pred)).sum().to(torch.float32)\n    \n    epsilon = 1e-7 # for avoid crash\n    precision, recall = tp \/ (tp + fp + epsilon), tp \/ (tp + fn + epsilon)\n    \n    return 2*(precision*recall)\/(precision + recall + epsilon)\n    ","2c4adcc3":"# ready for training\nmodel.train()\n\nVALIDATION_BATCHES = 10\n# init validation data for accuracy while training - but taking only VALIDATION_BATCHES while whole data is too big.\nval_sents = configure_word2vecs(val_sentences[:VALIDATION_BATCHES*BATCH_SIZE], word2vecs)\nval_targs = np.asarray(val_targets[:VALIDATION_BATCHES*BATCH_SIZE], dtype='bool')\n\nval_batch = torch.FloatTensor(val_sents).to(gpu)\nval_target = torch.FloatTensor(val_targs).to(gpu)\nprint(type(val_batch), val_batch.shape, type(val_targets), val_targets.shape)","422eea68":"BATCHES, BATCH_SIZE, get_batch(sentences, targets, 0)[0].shape, get_batch(sentences, targets, 0)[1].shape","362beb29":"# training\nfor e in range(EPOCHS):\n    # save epoch loss and accuracy\n    epoch_loss, epoch_acc = 0, 0\n    for b in range(BATCHES):\n        # get current batch from data\n        X_batch, y_batch = get_batch(sentences, targets, b)\n        \n        # set the gradients to zero, before starting to do backpropragation - avoiding gradient miss direction for minimum. \n        optimizer.zero_grad()\n\n        # predict targets for current batch and learn by comparing it to real targets with loss func.\n        y_pred = model(X_batch)\n        loss = loss_function(y_pred, y_batch.unsqueeze(1))\n        \n        # predict targets for validation data and eval. accuracy\n        val_pred = model(val_batch)\n        acc = acc_function(val_pred, val_target.unsqueeze(1))\n\n        # gradients are \"stored\" by the tensors themselves - once call backward on the loss.\n        loss.backward()\n        \n        # updates the model parameters\n        optimizer.step()\n        \n        # add batch loss and acc to evaluate epoch loss\/acc\n        epoch_loss += loss.item()\n        epoch_acc += acc.item()\n        \n        if b == 0 or (b+1) % 100 == 0:\n            print(f'Epoch {(e+1)+0:03} | Batch {(b+1)+0:04}: | Loss: {epoch_loss\/(b+1):.5f} | Acc: {epoch_acc\/(b+1):.3f} | F1: {f1_score(val_pred, val_target.unsqueeze(1)):.3f}')\n            # print(next(model.parameters()).is_cuda, X_batch.get_device(), y_batch.get_device(), y_pred.get_device(), val_pred.get_device())\n\n    print(f'Epoch {(e+1)+0:03}: | Epoch Loss: {epoch_loss\/BATCHES:.5f} | Epoch Acc: {epoch_acc\/BATCHES:.3f}')","a4f98281":"# init test data\ntest_data = pd.read_csv('\/kaggle\/input\/quora-insincere-questions-classification\/test.csv')\nsentences, targets = test_data['question_text'], []\nTEST_BATCHES = (len(sentences)+BATCH_SIZE-1)\/\/BATCH_SIZE","c4466429":"test_data.head()","976abb1e":"min, avrg, max = configure_sentence_statistic(sentences)\n\nprint('minimum sentence length {} - average sentence length {} - maximum sentence length {}'.format(min, avrg, max))","33e1af83":"len(sentences), len(targets), TEST_BATCHES","af66eee1":"model.eval()\nwith torch.no_grad():\n    for b in range(TEST_BATCHES):\n        # get current batch\n        X_batch = torch.FloatTensor(configure_word2vecs(sentences[BATCH_SIZE*b:BATCH_SIZE*(b+1)], word2vecs)).to(gpu)\n        \n        # predict batch according to our trained model\n        trg = torch.round(torch.sigmoid(model(X_batch))).cpu().numpy().squeeze()\n        targets.extend(trg)\n        \n        if b == 0 or (b+1) % 100 == 0: print(f'Batch {(b+1)+0:04} predicted')","8d473d4a":"# save data to submit\ntest_targets = (np.array(targets) >= 0.5).astype(np.int)\n\nsubmit = pd.DataFrame({\"qid\": test_data['qid'], \"prediction\": test_targets})\nsubmit.to_csv(\"submission.csv\", index=False)","0ce7b87c":"# display results\nsubmit.head()","a486f5a8":"configure_target_statistic(submit['prediction'])","29dc65ae":"filter_and_display_data(sentences, np.asarray(targets, dtype='int'), target=0)","7a3cbd19":"filter_and_display_data(sentences, np.asarray(targets, dtype='int'), target=1)","d8b79073":"filter_and_display_data(sentences, np.asarray(targets, dtype='int'), target=1, min_len=100, max_len=150)","880fe34c":"filter_and_display_data(sentences, np.asarray(targets, dtype='int'), target=1, min_len=100, max_len=150)","1c0a59cd":"**START WORKING ON TEST DATA**","5c2e80a1":"As it seems % of our prediction is almost like train data. that seems good ^^ ","6e960927":"it seems that only 6.2% of targets are 1. so we need some careful model to dont go overboard - overfit ^^","ffaf4ffa":"**START WORKING ON DATA**","fc4172d6":"lets see some examples of our prediction","f48b49ce":"lets see some examples of our data","8bdc9b7e":"for length <= 30 - its seems our data is normally.","664f1900":"**PREDICT TEST DATA ACCORDING TO OUR MODEL**","ff9b2512":"if there are no long sequencies in test set - it's just fine.","2ce7ddf3":"in one look - it seems our model is working well.","08ee07ec":"**START TRAINING OF MODEL**","61d1c0c7":"Average length is 13, but if we take length as hiden dimension about 30-35 it should be better - as we see these lengths(30-35) are in the middle of data.\nif we got 13 - most of sentences would be cut and their 70-80% would be lost...  also the cut parts should be important for the final target, but i think its no need to filter them - as start of 30 word must have enough content to predict final target - if dont, some almost same sentences should have different targets and i think this will help to learn still the right way.","f856a5bb":"for length 120-140 its seems our data is some kind of hard - to determine its target even by human and for target = 1 we dont have any examples - so its bad for train data to dont have all kind of basic examples, but nvm.","89e4d6bc":"**START WORKING ON MODEL**"}}