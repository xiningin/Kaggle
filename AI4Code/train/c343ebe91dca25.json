{"cell_type":{"a93a1ca8":"code","b072c3ca":"code","0194580b":"code","85103c0a":"code","d1b6e1fc":"code","439ea44e":"code","793fbada":"code","76d6cb16":"code","31de3d0e":"code","9701e681":"code","bb0e0a42":"code","49dd0120":"code","f615cc69":"code","e08f84d9":"code","0d41374f":"code","503f1454":"code","88d7ca7d":"code","20875711":"code","aa56ff74":"code","12e50732":"code","694fb5d5":"code","a692e9c8":"code","52feb95d":"code","9e1712f3":"code","492de4ca":"code","43ae088f":"code","190be396":"code","791c7e5b":"code","beaa29c8":"code","401cc9e8":"code","dc90526f":"code","d28ea2b2":"code","43849aa4":"code","254c51eb":"code","4677abc0":"code","ee4de6c6":"markdown","84d674d0":"markdown","eddf8f78":"markdown","3bf2248e":"markdown","e0d7c4a5":"markdown","9477e611":"markdown","07165bf9":"markdown","04a6aef8":"markdown","843fbe78":"markdown","9ba39005":"markdown","3be41acc":"markdown","3ebd03fe":"markdown","35f90225":"markdown","2ea28b08":"markdown","2cfa61dc":"markdown"},"source":{"a93a1ca8":"import numpy as np \nimport pandas as pd \nimport matplotlib.pyplot as plt\n%matplotlib inline \n\nimport seaborn as sns\nfrom keras.utils import to_categorical  \n\nimport os\nimport cv2 \nfrom subprocess import check_output\nprint(check_output([\"ls\", \"..\/input\"]).decode(\"utf8\"))","b072c3ca":"temp_df=pd.read_csv('..\/input\/diabetic-retinopathy-detection\/trainLabels.csv.zip') \nprint(temp_df.head()) \nimage=temp_df['image'].str.split('_',n=1,expand=True)  \ndf = pd.DataFrame()","0194580b":"df['eye_side']=list(image[1]) \ndf.head()","85103c0a":"df['patient_id']=list(image[0])\ndf.head()","d1b6e1fc":"df['path']='..\/input\/diabetic-retinopathy-detection\/' \ndf['path']=df['path'].str.cat(temp_df['image']+'.jpeg') \ndf.head()","439ea44e":"df['level']=temp_df['level']\ndf.head()","793fbada":"df['level_cat'] = df['level'].map(lambda x: to_categorical(x, 1+df['level'].max()))\ndf.head()","76d6cb16":"sizes = df['level'].values \nprint(sizes[0:5])","31de3d0e":"sns.distplot(sizes, kde=False)","9701e681":"pd.value_counts(sizes) ","bb0e0a42":"sum_E=0\nfor i in range (1,5):\n    L1_df=pd.DataFrame()\n    L1_df =df [df.level==i]\n    x=len(L1_df)\n    sum_E=x+sum_E\nprint(sum_E)","49dd0120":"B_df=pd.read_csv('..\/input\/prepossessed-arrays-of-binary-data\/1000_Binary Dataframe')\nB_df=B_df.drop('Unnamed: 0',axis=1)\nB_df.head(10)","f615cc69":"sizes =B_df['level'].values\nsns.distplot(sizes, kde=False); # Visualizing levels in dataset","e08f84d9":"Binary_90 = np.load('..\/input\/prepossessed-arrays-of-binary-data\/1000_Binary_images_data_90.npz')\nX_90=Binary_90['a']\nBinary_128 = np.load('..\/input\/prepossessed-arrays-of-binary-data\/1000_Binary_images_data_128.npz')\nX_128=Binary_128['a']\nBinary_264 = np.load('..\/input\/prepossessed-arrays-of-binary-data\/1000_Binary_images_data_264.npz')\nX_264=Binary_264['a']\ny=B_df['level'].values\n\n\nprint(X_90.shape)\nprint(X_128.shape)\nprint(X_264.shape)\nprint(y.shape)","0d41374f":"print(\"Shape before reshaping X_90\" +str(X_90.shape))\nX_90=X_90.reshape(1000,90,90,3)\nprint(\"Shape after reshaping X_90\" +str(X_90.shape))\nprint(\"\\n\\n\")\n\nprint(\"Shape before reshaping X_128\" +str(X_128.shape))\nX_128=X_128.reshape(1000,128,128,3)\nprint(\"Shape after reshaping X_128\" +str(X_128.shape))\nprint(\"\\n\\n\")\n\nprint(\"Shape before reshaping X_264\" +str(X_264.shape))\nX_264=X_264.reshape(1000,264,264,3)\nprint(\"Shape after reshaping X_264\" +str(X_264.shape))\n","503f1454":"plt.title(\"128*128*3 Image\")\nplt.imshow(X_128[1])\nplt.show()\n\nplt.title(\"264*264*3 Image\")\nplt.imshow(X_264[1])\nplt.show()","88d7ca7d":"y.shape","20875711":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(\n    X_128,y, test_size=0.10, random_state=42)\ny_train = to_categorical(y_train, num_classes=2)\ny_test_Categorical=to_categorical(y_test)\ny_categorical =to_categorical(y)","aa56ff74":"from keras.models import Sequential,Model\nfrom keras.layers import Conv2D,MaxPooling2D,Dense,Flatten,Dropout,Activation\nfrom keras import losses\nfrom keras.optimizers import Adam, Adagrad\nfrom keras.callbacks import EarlyStopping\nfrom keras import regularizers\nfrom sklearn.model_selection import GridSearchCV\nimport keras\n#import talos as ta","12e50732":"\ndef Talos_Model(X_train, y_train, X_test, y_test, params):\n    #parameters defined\n    lr = params['lr']\n    epochs=params['epochs']\n    dropout_rate=params['dropout']\n    optimizer=params['optimizer']\n    loss=params['loss']\n    last_activation=params['last_activation']\n    activation=params['activation']\n    clipnorm=params['clipnorm']\n    decay=params['decay']\n    momentum=params['momentum']\n    l1=params['l1']\n    l2=params['l2']\n    No_of_CONV_and_Maxpool_layers=params['No_of_CONV_and_Maxpool_layers']\n    No_of_Dense_Layers =params['No_of_Dense_Layers']\n    No_of_Units_in_dense_layers=params['No_of_Units_in_dense_layers']\n    Kernal_Size=params['Kernal_Size']\n    Conv2d_filters=params['Conv2d_filters']\n    pool_size_p=params['pool_size']\n    padding_p=params['padding']\n    \n    model=Sequential()\n    \n    for i in range(0,No_of_CONV_and_Maxpool_layers):\n        model.add(Conv2D(Conv2d_filters, Kernal_Size ,padding=padding_p))\n        model.add(Activation(activation))\n        model.add(MaxPooling2D(pool_size=pool_size_p,strides=(2,2)))\n    \n    \n    model.add(Flatten())\n    \n    for i in range (0,No_of_Dense_Layers):\n        model.add(Dense(units=No_of_Units_in_dense_layers,activation=activation, kernel_regularizer=regularizers.l2(l2),\n                  activity_regularizer=regularizers.l1(l1)))\n    \n    \n    model.add(Dense(units=20,activation=activation))\n    \n    model.add(Dense(units=2,activation=activation))\n    if optimizer==\"Adam\":\n        opt=keras.optimizers.Adam(lr=lr, decay=decay, beta_1=0.9, beta_2=0.999)\n    if optimizer==\"Adagrad\":\n        opt=keras.optimizers.Adagrad(lr=lr, epsilon=None, decay=decay)\n    if optimizer==\"sgd\":\n        opt=keras.optimizers.SGD(lr=lr, momentum=momentum, decay=decay, nesterov=False)\n    \n    model.compile(loss=loss,optimizer=opt,\n                 metrics=['accuracy'])\n    \n    out = model.fit(X_train, y_train, epochs=params['epochs'])\n\n    return out,model","694fb5d5":"params = {'lr': (0.1, 0.01,1 ),\n     'epochs': [10,5,15],\n     'dropout': (0, 0.40, 0.8),\n     'optimizer': [\"Adam\",\"Adagrad\",\"sgd\"],\n     'loss': [\"binary_crossentropy\",\"mean_squared_error\",\"mean_absolute_error\"],\n     'last_activation': [\"softmax\",\"sigmoid\"],\n     'activation' :[\"relu\",\"selu\",\"linear\"],\n     'clipnorm':(0.0,0.5,1),\n     'decay':(1e-6,1e-4,1e-2),\n     'momentum':(0.9,0.5,0.2),\n     'l1': (0.01,0.001,0.0001),\n     'l2': (0.01,0.001,0.0001),\n     'No_of_CONV_and_Maxpool_layers':[2,3],\n     'No_of_Dense_Layers': [2,3,4],\n     'No_of_Units_in_dense_layers':[128,64,32,256],\n     'Kernal_Size':[(2,2),(4,4),(6,6)],\n     'Conv2d_filters':[60,40,80,120],\n     'pool_size':[(2,2),(4,4)],\n     'padding':[\"valid\",\"same\"]\n    }","a692e9c8":"\ndef Randomized_Model(lr=0.01,dropout=0.5,optimizer=\"adam\",loss='mean_squared_error',\n                    last_activation=\"softmax\",activation=\"relu\",clipnorm=0.1,\n                    decay=1e-2,momentum=0.5,l1=0.01,l2=0.001,No_of_CONV_and_Maxpool_layers=3,\n                    No_of_Dense_Layers=3,No_of_Units_in_dense_layers=24,Conv2d_filters=60):\n       \n    \n    \n    #model sequential\n    model=Sequential()\n    \n    for i in range(0,No_of_CONV_and_Maxpool_layers):\n        model.add(Conv2D(Conv2d_filters, (2,2) ,padding=\"same\"))\n        model.add(Activation(activation))\n        model.add(MaxPooling2D(pool_size=(2,2),strides=(2,2)))\n    \n    \n    model.add(Flatten())\n    \n    for i in range (0,No_of_Dense_Layers):\n        model.add(Dense(units=No_of_Units_in_dense_layers,activation=activation, kernel_regularizer=regularizers.l2(l2),\n                  activity_regularizer=regularizers.l1(l1)))\n    \n    model.add(Dropout(dropout))\n    model.add(Dense(units=20,activation=activation))\n    \n    model.add(Dense(units=2,activation=activation))\n    if optimizer==\"Adam\":\n        opt=keras.optimizers.Adam(lr=lr, decay=decay, beta_1=0.9, beta_2=0.999)\n    if optimizer==\"Adagrad\":\n        opt=keras.optimizers.Adagrad(lr=lr, epsilon=None, decay=decay)\n    if optimizer==\"sgd\":\n        opt=keras.optimizers.SGD(lr=lr, momentum=momentum, decay=decay, nesterov=False)\n    \n    model.compile(loss=loss,optimizer=opt,\n                 metrics=['accuracy'])\n    \n    \n\n    return model","52feb95d":"\n\nparams = {'lr': (0.1, 0.01,1,0.001 ),\n     'epochs': [10,5,15,30],\n     'dropout': (0, 0.40, 0.8),\n     'optimizer': [\"Adam\",\"Adagrad\",\"sgd\"],\n     'loss': [\"binary_crossentropy\",\"mean_squared_error\",\"mean_absolute_error\"],\n     'last_activation': [\"softmax\",\"sigmoid\"],\n     'activation' :[\"relu\",\"selu\",\"linear\"],\n     'clipnorm':(0.0,0.5,1),\n     'decay':(1e-6,1e-4,1e-2),\n     'momentum':(0.9,0.5,0.2),\n     'l1': (0.01,0.001,0.0001),\n     'l2': (0.01,0.001,0.0001),\n     'No_of_CONV_and_Maxpool_layers':[2,3],\n     'No_of_Dense_Layers': [2,3,4,5],\n     'No_of_Units_in_dense_layers':[128,64,32,256],\n     \n     'Conv2d_filters':[60,40,80,120,220]\n     \n     \n    }\n","9e1712f3":"from keras.wrappers.scikit_learn import KerasClassifier\nfrom sklearn.model_selection import RandomizedSearchCV, KFold\nfrom sklearn.metrics import make_scorer\n# model class to use in the scikit random search CV \nmodel = KerasClassifier(build_fn=Randomized_Model, epochs=10, batch_size=20, verbose=1)\ngrid = RandomizedSearchCV(estimator=model, cv=KFold(3), param_distributions=params, \n                          verbose=20,  n_iter=10, n_jobs=1)\n","492de4ca":"grid_result = grid.fit(X_train, y_train)","43ae088f":"best_params=grid_result.best_params_\nbest_params","190be396":"\nfrom sklearn.metrics import accuracy_score\n\ny=grid_result.predict(X_test)\nrandom=accuracy_score(y, y_test)\nprint(\"Base Accuracy \",random)\n\nbest_random = grid_result.best_estimator_\ny1=best_random.predict(X_test)\nBest=accuracy_score(y1, y_test)\nprint(\"Best Accuracy \" ,Best)\n\n\nprint('Improvement of {:0.2f}%.'.format( 100 * (Best - random) \/ random))","791c7e5b":"def Best_param_Model(best_params):\n       \n    lr=best_params[\"lr\"]\n    dropout=best_params[\"dropout\"]\n    optimizer=best_params[\"optimizer\"]\n    loss=best_params[\"loss\"]\n    last_activation=best_params[\"last_activation\"]\n    activation=best_params[\"activation\"]\n    clipnorm=best_params[\"clipnorm\"]\n    decay=best_params[\"decay\"]\n    momentum=best_params[\"momentum\"]\n    l1=best_params[\"l1\"]\n    l2=best_params[\"l2\"]\n    No_of_CONV_and_Maxpool_layers=best_params[\"No_of_CONV_and_Maxpool_layers\"]\n    No_of_Dense_Layers=best_params[\"No_of_Dense_Layers\"]\n    No_of_Units_in_dense_layers=best_params[\"No_of_Units_in_dense_layers\"]\n    Conv2d_filters=best_params[\"Conv2d_filters\"]\n    \n    #model sequential\n    model=Sequential()\n    \n    for i in range(0,No_of_CONV_and_Maxpool_layers):\n        model.add(Conv2D(Conv2d_filters, (2,2) ,padding=\"same\"))\n        model.add(Activation(activation))\n        model.add(MaxPooling2D(pool_size=(2,2),strides=(2,2)))\n    \n    \n    model.add(Flatten())\n    \n    for i in range (0,No_of_Dense_Layers):\n        model.add(Dense(units=No_of_Units_in_dense_layers,activation=activation, kernel_regularizer=regularizers.l2(l2),\n                  activity_regularizer=regularizers.l1(l1)))\n    \n    \n    model.add(Dense(units=20,activation=activation))\n    \n    model.add(Dense(units=2,activation=activation))\n    if optimizer==\"Adam\":\n        opt=keras.optimizers.Adam(lr=lr, decay=decay, beta_1=0.9, beta_2=0.999)\n    if optimizer==\"Adagrad\":\n        opt=keras.optimizers.Adagrad(lr=lr, epsilon=None, decay=decay)\n    if optimizer==\"sgd\":\n        opt=keras.optimizers.SGD(lr=lr, momentum=momentum, decay=decay, nesterov=False)\n    \n    model.compile(loss=loss,optimizer=opt,\n                 metrics=['accuracy'])\n    \n    \n\n    return model","beaa29c8":"\nBinary_model=Best_param_Model(best_params)\nhistory =Binary_model.fit(X_train, y_train, epochs=100, validation_data=(X_test,y_test_Categorical))\n\n# Plot training & validation accuracy values\nplt.plot(history.history['acc'])\nplt.plot(history.history['val_acc'])\n\n\nplt.title('Model accuracy')\nplt.ylabel('Accuracy')\nplt.xlabel('Epoch')\nplt.legend(['Train', 'Test'], loc='upper left')\nplt.show()\n\n# Plot training & validation loss values\nplt.plot(history.history['loss'])\nplt.plot(history.history['val_loss'])\n\nplt.title('Model loss')\nplt.ylabel('Loss')\nplt.xlabel('Epoch')\nplt.legend(['Train', 'Test'], loc='upper left')\nplt.show()","401cc9e8":"Binary_model.evaluate(X_test,y_test_Categorical)","dc90526f":"y=B_df['level'].values\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(\n    X_128,y, test_size=0.10, random_state=42)\ny_train = to_categorical(y_train, num_classes=2)\ny_test_Categorical=to_categorical(y_test)","d28ea2b2":"\nmodel = Sequential()\nmodel.add(Conv2D(16,kernel_size = (5,5),activation = 'relu', activity_regularizer=regularizers.l2(1e-8)))\nmodel.add(Conv2D(32,kernel_size = (5,5),activation = 'relu', activity_regularizer = regularizers.l2(1e-8)))\nmodel.add(MaxPooling2D(3,3))\nmodel.add(Conv2D(64,kernel_size = (5,5),activation = 'relu', activity_regularizer = regularizers.l2(1e-8)))\nmodel.add(MaxPooling2D(3,3))\nmodel.add(Conv2D(128,activation = 'relu',kernel_size = (3,3),activity_regularizer = regularizers.l2(1e-8)))\nmodel.add(Flatten())\nmodel.add(Dropout(0.4))\nmodel.add(Dense(64,activation = 'tanh',activity_regularizer = regularizers.l2(1e-8)))\nmodel.add(Dropout(0.2))\nmodel.add(Dense(16,activation = 'tanh',activity_regularizer = regularizers.l2(1e-8)))\nmodel.add(Dropout(0.2))\nmodel.add(Dense(2,activation = 'softmax'))\nmodel.compile(loss=keras.losses.binary_crossentropy, optimizer=\"adam\", metrics=[\"accuracy\"])\nmodel.fit(X_train,y_train, epochs = 10 ,batch_size = 16,validation_data=(X_test,y_test_Categorical))\nmodel.summary()\n","43849aa4":"from sklearn.metrics import confusion_matrix\nprediction=model.predict(X_test)\ny_pred=[]\nfor i in prediction:\n    y_pred.append(i.argmax())\ny_pred=np.asarray(y_pred)\ntrue_negative,false_positive,false_negative,true_positive=confusion_matrix(y_test, y_pred).ravel()\n\nprint(\"true_negative: \",true_negative)\nprint(\"false_positive: \",false_positive)\nprint(\"false_negative: \",false_negative)\nprint(\"true_positive: \",true_positive)\nprint(\"\\n\\n Accuracy Measures\\n\\n\")\nSensitivity=true_positive\/(true_positive+false_negative)\nprint(\"Sensitivity: \",Sensitivity)\n\nFalse_Positive_Rate=false_positive\/(false_positive+true_negative)\nprint(\"False_Positive_Rate: \",False_Positive_Rate)\n\nSpecificity=true_negative\/(false_positive + true_negative)\nprint(\"Specificity: \",Specificity)\n\n#FDR \u00e0 0 means that very few of our predictions are wrong\nFalse_Discovery_Rate=false_positive\/(false_positive+true_positive)\nprint(\"False_Discovery_Rate: \",False_Discovery_Rate)\n\nPositive_Predictive_Value =true_positive\/(true_positive+false_positive)\nprint(\"Positive_Predictive_Value: \",Positive_Predictive_Value)\n\n","254c51eb":"a=np.expand_dims( X_train[10],axis=0)\na.shape\nlayer_outputs = [layer.output for layer in model.layers]\nactivation_model = Model(inputs=model.input, outputs=layer_outputs)\nactivations = activation_model.predict(a)","4677abc0":"top_layer = model.layers[0]\nplt.imshow(top_layer.get_weights()[0][:, :, :,15 ])","ee4de6c6":"##### **Installing Libraries**","84d674d0":"**Confirming that image path on 1st index of data frame is same as the image at 1st index of the Np Arrays**","eddf8f78":"**Best Params of Randomized search**","3bf2248e":"## CNN Model","e0d7c4a5":"**Visualization**\n","9477e611":"**Visualizing my CNN model**","07165bf9":"**loading My numpy array which I saved**","04a6aef8":"**Now I have my X and Y. Now its time for spliting and training**","843fbe78":"Accuracy Measures\n[Helping Material](http:\/\/comprna.upf.edu\/courses\/Master_AGB\/2_ClassificationAlgorithms\/Lecture_Accuracy.pdf)","9ba39005":"**loading my saving Binary data**","3be41acc":"**Parameters of Randomized Search**","3ebd03fe":"**Its seem to be same now thats good for us**","35f90225":"**Acheiving 76% accuracy till now on 1000 examples**","2ea28b08":"**Talos Model For HyperParameter Optimization**","2cfa61dc":"**Randomized Search For Hyperparameter tuning**"}}