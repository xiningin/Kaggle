{"cell_type":{"b54b3388":"code","a0d14bc3":"code","1b2aad86":"code","ed9acbd3":"code","0f093974":"code","d9e18e7c":"code","e6da339f":"code","029c6e28":"code","07005fd2":"code","9b2cb075":"code","4613bf90":"code","94634f83":"code","4d55cc8c":"code","1c4e80e6":"code","76908321":"code","25d2da43":"code","caf3599e":"code","81429bb9":"code","56d88e2a":"code","97b0f14c":"code","be8e9b68":"code","25f2fb67":"code","859fe256":"code","b9263bcf":"code","70681599":"code","80f056ac":"code","6077ff28":"code","2cb0f41d":"code","4d1e3bb0":"code","57146824":"code","35a7abc6":"code","7176f07d":"code","c6cbfc47":"code","bc9f1aaa":"code","28e0453c":"code","df43252b":"code","48dd0b47":"code","a66d1cfe":"code","4206fd70":"code","0caef20d":"code","869b0101":"code","952e60a2":"code","82402811":"code","a66fc75f":"markdown","3fe3e6e6":"markdown","e6c3490d":"markdown","9dca078e":"markdown"},"source":{"b54b3388":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom nltk.corpus import stopwords\n\nfrom keras.models import Sequential\nfrom keras.layers import Embedding, Dense, LSTM, Dropout, Flatten, TimeDistributed, BatchNormalization, SpatialDropout1D\nfrom keras.layers.wrappers import Bidirectional\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\n\nfrom gensim.models.keyedvectors import KeyedVectors\n\nfrom sklearn.metrics import classification_report, confusion_matrix, f1_score\n\nimport tensorflow as tf\nimport math\nimport warnings\nfrom tqdm import tqdm\nimport scipy\nimport statistics ","a0d14bc3":"# train_df_master = pd.read_csv('..\/input\/complete-data-for-sentiment-analysis\/Complete Data for Sentiment Analysis.csv')\n# train_df_master = pd.read_csv('..\/input\/originaldataset\/Train_data_cleaned.csv')\ntrain_df_master = pd.read_csv('..\/input\/originaldatasetcombined\/Combined_Train_data_cleaned.csv')\ntest_df_master = pd.read_csv('..\/input\/test-data\/Test_data_cleaned.csv')","1b2aad86":"train_df = train_df_master.copy()\nprint('Training data has {} rows and {} columns'.format(train_df.shape[0], train_df.shape[1]))\n\ntest_df = test_df_master.copy()\nprint('Testing data has {} rows and {} columns'.format(test_df.shape[0], test_df.shape[1]))","ed9acbd3":"sns.countplot(train_df['sentiment_class']).set_title('Sentiment Class distribution')\nplt.show()","0f093974":"# define the training and testing lengths to keep a track on encoded data\ntrain_length = len(train_df)\ntest_length = len(test_df)","d9e18e7c":"\"\"\" Although the data in this file is already cleaned, we still perform a double check \"\"\"\n\ndef tokenize(text):\n    stop_words = stopwords.words('english')    \n    \n    tokenized_text = []\n    \n    for txt in text:\n        txt = str(txt)\n        words = txt.split(' ')\n        tokenized_string = ''\n\n        for word in words:\n            # check for name handles and unwanted text\n            if len(word) > 1 and word[0] != '@' and word not in stop_words:\n                # if the word is a hastag, remove #\n                if word[0] == '#':\n                    word = word[1:]\n\n                tokenized_string += word + ' '\n\n        tokenized_text.append(tokenized_string)\n    \n    return tokenized_text","e6da339f":"\"\"\" encode text -> translate text to a sequence of numbers \"\"\"\n\ndef encode_text(text):\n    tokenizer = Tokenizer(filters='!\"#$%&()*+,-.\/:;<=>?@[\\\\]^_`{|}~\\t\\n', split=\" \", lower=True)\n    tokenizer.fit_on_texts(text)\n    \n    return tokenizer, tokenizer.texts_to_sequences(text)","029c6e28":"\"\"\" Apply padding to dataset and convert labels (-1, 0, 1) to bitmaps \"\"\"\n\ndef format_data(encoded_text, max_length, sent_labels):\n    x = pad_sequences(encoded_text, maxlen= max_length, padding='post')\n    y = []\n    \n    for label in sent_labels:\n        bit_vec = np.zeros(3)\n        bit_vec[label+1] = 1\n        y.append(bit_vec)\n        \n    y = np.asarray(y)\n    return x, y","07005fd2":"\"\"\" create weight matrix from pre trained embeddings \"\"\"\n\ndef create_weight_matrix(vocab, raw_embeddings, dim):\n    vocab_size = len(vocab) + 1\n    weight_matrix = np.zeros((vocab_size, dim))\n    \n    for word, idx in vocab.items():\n        if word in raw_embeddings:\n            weight_matrix[idx] = raw_embeddings[word]\n    \n    return weight_matrix","9b2cb075":"## 43.84\n\n\"\"\" final model 1\"\"\"\ndef final_model_1(weight_matrix, vocab_size, max_length):\n    embedding_layer = Embedding(vocab_size, 50, weights=[weight_matrix], input_length=max_length, trainable=True, \n                                mask_zero=True)\n    model = Sequential()\n    model.add(embedding_layer)\n    \n    model.add(Bidirectional(LSTM(128, return_sequences=True)))\n    model.add(Dropout(0.2))\n    model.add(BatchNormalization())\n    \n    model.add(Bidirectional(LSTM(64, return_sequences=True)))\n    model.add(Dropout(0.2))\n    model.add(BatchNormalization())\n    \n    model.add(Bidirectional(LSTM(32)))\n    model.add(Dropout(0.2))\n    model.add(BatchNormalization())\n    \n    model.add(Dense(64, activation='softmax'))\n    model.add(Dense(64, activation='softmax'))\n    \n    model.add(Dropout(0.5))\n    model.add(Dense(3, activation='softmax'))\n    \n    \n    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n    \n    return model","4613bf90":"## 42.69292\n\n\"\"\" final model 2\"\"\"\ndef final_model_2(weight_matrix, vocab_size, max_length):\n    embedding_layer = Embedding(vocab_size, 300, weights=[weight_matrix], input_length=max_length, trainable=True, \n                                mask_zero=True)\n    model = Sequential()\n    model.add(embedding_layer)\n    \n    model.add(Bidirectional(LSTM(256, dropout=0.2, return_sequences=True)))\n    model.add(Bidirectional(LSTM(128, dropout=0.2, return_sequences=True)))\n    model.add(Bidirectional(LSTM(64, dropout=0.2, return_sequences=True)))\n    model.add(Bidirectional(LSTM(32)))\n    \n    model.add(Dense(64, activation = 'softmax'))\n    \n    model.add(Dropout(0.5))\n    \n    model.add(Dense(3, activation='softmax'))\n    \n    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n    \n    return model","94634f83":"# 42.57 score\n\n\"\"\" final model 3\"\"\"\ndef final_model_3(weight_matrix, vocab_size, max_length):\n    embedding_layer = Embedding(vocab_size, 50, weights=[weight_matrix], input_length=max_length, trainable=True, \n                                mask_zero=True)\n    model = Sequential()\n    model.add(embedding_layer)\n    \n    model.add(SpatialDropout1D(0.2))\n    model.add(LSTM(128, dropout=0.2, recurrent_dropout=0.2))\n    model.add(Dense(3, activation='softmax'))\n    \n    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n    \n    return model","4d55cc8c":"\"\"\" Tokenize all the training and testing text \"\"\"\n\ntokenized_text = tokenize(train_df['text'])\ntokenized_text += tokenize(test_df['text'])\n\nmax_length = math.ceil(sum([len(s.split(\" \")) for s in tokenized_text])\/len(tokenized_text))\n\ntokenizer, encoded_text = encode_text(tokenized_text)\n\nmax_length, len(tokenized_text)","1c4e80e6":"\"\"\" Apply padding and format data \"\"\"\n\nx, y = format_data(encoded_text[:train_length], max_length, train_df['sentiment_class'])\nprint('For train data: ', len(x), len(y))\n\nx_test = pad_sequences(encoded_text[train_length:], maxlen= max_length, padding='post')\nprint('For test data: ', len(x_test))","76908321":"\"\"\" Clearing vocabulary \"\"\"\n\nvocab = tokenizer.word_index\nvocab, len(vocab)","25d2da43":"# load the GloVe vectors in a dictionary:\n\nembeddings_index = {}\nf = open('\/kaggle\/input\/glove840b300dtxt\/glove.840B.300d.txt','r',encoding='utf-8')\nfor line in tqdm(f):\n    values = line.split(' ')\n    word = values[0]\n    coefs = np.asarray([float(val) for val in values[1:]])\n    embeddings_index[word] = coefs\nf.close()\n\nprint('Found %s word vectors.' % len(embeddings_index))","caf3599e":"# load the GloVe vectors in a dictionary:\n\nembeddings_index_50 = {}\nf = open('\/kaggle\/input\/glove6b50dtxt\/glove.6B.50d.txt','r',encoding='utf-8')\nfor line in tqdm(f):\n    values = line.split(' ')\n    word = values[0]\n    coefs = np.asarray([float(val) for val in values[1:]])\n    embeddings_index_50[word] = coefs\nf.close()\n\nprint('Found %s word vectors.' % len(embeddings_index_50))","81429bb9":"\"\"\" Create the weight matrix \"\"\"\n\nweight_matrix_300 = create_weight_matrix(vocab, embeddings_index, dim = 300)\nlen(weight_matrix_300)","56d88e2a":"\"\"\" Create the weight matrix \"\"\"\n\nweight_matrix_50 = create_weight_matrix(vocab, embeddings_index_50, dim = 50)\nlen(weight_matrix_50)","97b0f14c":"model1 = final_model_1(weight_matrix_50, len(vocab)+1, max_length)\n\nhistory1 = model1.fit(x, y, epochs = 20, validation_split = 0.1, verbose = 1)","be8e9b68":"model1.summary()","25f2fb67":"model2 = final_model_2(weight_matrix_300, len(vocab)+1, max_length)\n\nhistory2 = model2.fit(x, y, epochs = 20, validation_split = 0.1, verbose = 1)","859fe256":"model2.summary()","b9263bcf":"model3 = final_model_3(weight_matrix_50, len(vocab)+1, max_length)\n\nhistory3 = model3.fit(x, y, epochs = 30, validation_split = 0.1, verbose = 1)","70681599":"model3.summary()","80f056ac":"train_pred_prob_1 = model1.predict(x)\ntrain_pred_prob_1","6077ff28":"train_pred_1 = [np.argmax(pred)-1 for pred in train_pred_prob_1]\nlen(train_pred_1)","2cb0f41d":"train_pred_prob_2 = model2.predict(x)\ntrain_pred_prob_2","4d1e3bb0":"train_pred_2 = [np.argmax(pred)-1 for pred in train_pred_prob_2]\nlen(train_pred_2)","57146824":"train_pred_prob_3 = model3.predict(x)\ntrain_pred_prob_3","35a7abc6":"train_pred_3 = [np.argmax(pred)-1 for pred in train_pred_prob_3]\nlen(train_pred_3)","7176f07d":"train_pred_comb = np.concatenate((train_pred_1, train_pred_2, train_pred_3), axis = 0).reshape(-1, len(train_pred_1))\nprint(train_pred_comb)","c6cbfc47":"train_pred_comb[:, 0]","bc9f1aaa":"final_pred = []\nfor i in range(len(train_pred_1)):\n    try:\n        final_pred.append(statistics.mode(train_pred_comb[:, i]))\n    except:\n        final_pred.append(train_pred_1[i])\n\nfinal_pred[:5]","28e0453c":"train_pred = final_pred","df43252b":"train_f1_score = f1_score(train_df['sentiment_class'], train_pred, average='weighted')\n\nprint('F1 Score: ', train_f1_score)\nprint()\nprint('Confusion Matrix: \\n', confusion_matrix(train_df['sentiment_class'], train_pred))\nprint()\nprint('Classification Report: \\n', classification_report(train_df['sentiment_class'], train_pred))","48dd0b47":"test_pred_prob_1 = model1.predict(x_test)\ntest_pred_prob_2 = model2.predict(x_test)\ntest_pred_prob_3 = model3.predict(x_test)","a66d1cfe":"test_pred_1 = [np.argmax(pred)-1 for pred in test_pred_prob_1]\ntest_pred_2 = [np.argmax(pred)-1 for pred in test_pred_prob_2]\ntest_pred_3 = [np.argmax(pred)-1 for pred in test_pred_prob_3]","4206fd70":"\ntest_pred_combined = np.concatenate((test_pred_1, test_pred_2, test_pred_3), axis = 0).reshape(-1, len(test_pred_1))\nfinal_pred = []\nfor i in range(len(test_pred_1)):\n    try:\n        final_pred.append(statistics.mode(test_pred_combined[:, i]))\n    except:\n        final_pred.append(test_pred_1[i])\n\nfinal_pred[:5]","0caef20d":"test_pred = final_pred","869b0101":"org_test_df = pd.read_csv('..\/input\/original-dataset\/test.csv')\norg_test_df.head()","952e60a2":"submission_df = org_test_df.copy()\nsubmission_df.drop(['original_text', 'lang', 'retweet_count', 'original_author'], axis = 1, inplace = True)\nsubmission_df['sentiment_class'] = test_pred\n\nprint(submission_df['sentiment_class'].value_counts())\nsns.countplot(submission_df['sentiment_class']).set_title('Class distribution')\nsubmission_df.to_csv('checking model for submissions final.csv', index = False)\nsubmission_df.head()","82402811":"submission_df.head(20)","a66fc75f":"### Predicting on Test Data","3fe3e6e6":"### Preparing Data","e6c3490d":"### Start the model training","9dca078e":"### Check on Training data"}}