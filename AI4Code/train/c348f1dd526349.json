{"cell_type":{"dd5b2f39":"code","43751ebc":"code","cddbafe3":"code","3683a6af":"code","10266a0b":"code","8225a268":"code","931615f3":"code","4ab5e20e":"code","53c43037":"code","c9840655":"code","610624f5":"markdown","bb98cc8c":"markdown","9185f9bb":"markdown","393b21ca":"markdown","9c552295":"markdown","ae4cb929":"markdown"},"source":{"dd5b2f39":"import os\nimport re\nimport sys\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom nltk.corpus import stopwords\nimport json\nimport time\nimport warnings \nwarnings.filterwarnings('ignore')\nimport datetime\n\ndf = pd.read_csv('\/kaggle\/input\/CORD-19-research-challenge\/2020-03-13\/all_sources_metadata_2020-03-13.csv')\nprint(df.shape)","43751ebc":"df.info()\n","cddbafe3":"col='abstract'\nkeep = df.dropna(subset=[col])\nprint(keep.shape)\ndocs = keep[col].tolist()","3683a6af":"# Code adaptead from https:\/\/radimrehurek.com\/gensim\/auto_examples\/tutorials\/run_lda.html\n\nfrom nltk.tokenize import RegexpTokenizer\nfrom nltk.stem.wordnet import WordNetLemmatizer\nfrom gensim.corpora import Dictionary\n\ntokenizer = RegexpTokenizer(r'\\w+')\nfor idx in range(len(docs)):\n    # Convert to lowercase.\n    docs[idx] = docs[idx].lower()  \n    # Split into words.\n    docs[idx] = tokenizer.tokenize(docs[idx])  \n\n# Remove numbers\ndocs = [[token for token in doc if not token.isnumeric()] for doc in docs]\n\n# Remove one-character words\ndocs = [[token for token in doc if len(token) > 1] for doc in docs]\n\n# Remove stopwords \nstop_words = stopwords.words(\"english\")\ndocs = [[token for token in doc if token not in stop_words] for doc in docs]\n\n# Lemmatize\nlemmatizer = WordNetLemmatizer()\ndocs = [[lemmatizer.lemmatize(token) for token in doc] for doc in docs]\n\n# Create a dictionary representation of the documents\ndictionary = Dictionary(docs)\n\n# Filter out words that occur less than 20 documents, or more than 50% of the documents\ndictionary.filter_extremes(no_below=20, no_above=0.5)\n\n# Create Bag-of-words representation of the documents\ncorpus = [dictionary.doc2bow(doc) for doc in docs]\n\nprint('Number of unique tokens: %d' % len(dictionary))\nprint('Number of documents: %d' % len(corpus))","10266a0b":"from gensim.models import LdaModel, LdaMulticore\n\n# Set training parameters.\nnum_topics = 10\n\n# Make a index to word dictionary.\ntemp = dictionary[0]  # This is only to \"load\" the dictionary.\nid2word = dictionary.id2token\n\nmodel = LdaMulticore(\n    corpus=corpus,\n    id2word=id2word,\n    chunksize=2000,\n    eta='auto',\n    iterations=10,\n    num_topics=num_topics,\n    passes=10,\n    eval_every=None,\n    workers=4\n)","8225a268":"top_topics = model.top_topics(corpus) \nfor i, (topic, sc) in enumerate(top_topics): \n    print(\"\\nTopic {}: \".format(i) + \", \".join([w for score,w in topic]))","931615f3":"def split_to_sentences(text):\n    return re.split(r'(?<!\\w\\.\\w.)(?<![A-Z][a-z]\\.)(?<=\\.|\\?|\\!)\\s', text)","4ab5e20e":"has_question = keep[keep.abstract.dropna().map(lambda x: '?' in x)]\nhas_question.shape","53c43037":"df.publish_time.dropna().map(lambda x: x.split()[0].split('-')[0]).value_counts()","c9840655":"for i, row in has_question.dropna(subset=['publish_time']).iterrows():\n    if not '2020' in row.publish_time:\n        continue\n    print(\"\\nTITLE: {}\".format(row.title))\n    print(\"\\tDATE:  {}\".format(row.publish_time))\n    for sent in split_to_sentences(row.abstract):\n        if sent[-1] == '?':\n            print(\"Q:\\t{}\".format(sent))","610624f5":"In this notebook we aim to get a high-level picture of CORD-19 by (1) training an unsupervised topic model (LDA) on paper abstracts and (2) printing questions asked by the authors in the abstracts of the papers. ","bb98cc8c":"# Print Questions Asked in the Abstracts of Papers from 2020","9185f9bb":"# Print Topics","393b21ca":"# Tokenize the documents.\n","9c552295":"# Training LDA on paper abstracts","ae4cb929":"# Train LDA model"}}