{"cell_type":{"18d309cf":"code","422999af":"code","c7ca13e5":"code","ea715e90":"code","767539a5":"code","11c663f0":"code","9f19839e":"code","54254359":"code","347cadac":"code","d9daaf4e":"code","e03a6505":"code","3b85ee07":"code","a9a6a1c0":"code","52f9b510":"code","b502a989":"code","4fa31cd7":"code","8c09935b":"code","37c63cba":"code","c9261f02":"code","a20ce3c0":"code","85aacb12":"code","1d2c5309":"code","2118efba":"code","ab643305":"code","f521dbb7":"code","794d5ed0":"code","3d8a77af":"code","c8a4cbe2":"code","7110313c":"code","a970503c":"code","8e904904":"code","2d321348":"code","037f08f2":"code","a922d03d":"code","4f267301":"code","111b5751":"code","e0a01271":"code","d3276b57":"code","e8c9b7fc":"code","359b7ab6":"code","24f0cb72":"code","3d27c745":"code","9eb4e697":"code","00b36e42":"code","8154a11b":"code","1288529e":"code","7bc2ab73":"code","2cc4f0ba":"code","6a9d25a8":"code","a4fcf89d":"markdown","c15dcbee":"markdown","c8d6b757":"markdown","ba9988b6":"markdown","a2824439":"markdown","2471ba22":"markdown","f0892dcc":"markdown","a39878a1":"markdown","d98a87ef":"markdown","c9e2d822":"markdown","8ad6ea20":"markdown","8ab48c16":"markdown","bc0525f9":"markdown","b01b0bc5":"markdown","7e7328d9":"markdown","722e1702":"markdown","af360b0c":"markdown","ff86bdcc":"markdown","800b1b1b":"markdown","8f9dc0b9":"markdown","c9fe3845":"markdown","b83bbf1f":"markdown","e55805f3":"markdown","7b5ecfbc":"markdown","751056b3":"markdown","d91c0d26":"markdown","f7a9fd73":"markdown","d3298fa5":"markdown","e2ec653a":"markdown","9e54254d":"markdown","ebfceac5":"markdown","9bcca195":"markdown","7468fe75":"markdown","11f049bf":"markdown","fc38c710":"markdown","fb129766":"markdown","cd9563da":"markdown","1290a732":"markdown","dcfc2bc1":"markdown","e904cd74":"markdown","935969c6":"markdown","3b83c81b":"markdown","8d57dd70":"markdown","21868331":"markdown","39ff074f":"markdown","b8226a97":"markdown","6faafa19":"markdown","d0672e46":"markdown","804d4a13":"markdown"},"source":{"18d309cf":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","422999af":"import seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline","c7ca13e5":"data=pd.read_csv(\"..\/input\/sms-spam-collection-dataset\/spam.csv\",encoding=\"latin\")\ndata.head()","ea715e90":"data.columns","767539a5":"data=data.drop(columns=[\"Unnamed: 2\",\"Unnamed: 3\",\"Unnamed: 4\"])","11c663f0":"data=data.rename(\n{\n    \"v1\":\"Category\",\n    \"v2\":\"Message\"\n},\n    axis=1\n)","9f19839e":"data.head()","54254359":"data.isnull().sum()","347cadac":"data.info()","d9daaf4e":"data[\"Message Length\"]=data[\"Message\"].apply(len)","e03a6505":"fig=plt.figure(figsize=(12,8))\nsns.histplot(\n    x=data[\"Message Length\"],\n    hue=data[\"Category\"]\n)\nplt.title(\"ham & spam messege length comparision\")\nplt.show()","3b85ee07":"ham_desc=data[data[\"Category\"]==\"ham\"][\"Message Length\"].describe()\nspam_desc=data[data[\"Category\"]==\"spam\"][\"Message Length\"].describe()\n\nprint(\"Ham Messege Length Description:\\n\",ham_desc)\nprint(\"************************************\")\nprint(\"Spam Message Length Description:\\n\",spam_desc)","a9a6a1c0":"data.describe(include=\"all\")","52f9b510":"data[\"Category\"].value_counts()","b502a989":"sns.countplot(\n    data=data,\n    x=\"Category\"\n)\nplt.title(\"ham vs spam\")\nplt.show()","4fa31cd7":"ham_count=data[\"Category\"].value_counts()[0]\nspam_count=data[\"Category\"].value_counts()[1]\n\ntotal_count=data.shape[0]\n\nprint(\"Ham contains:{:.2f}% of total data.\".format(ham_count\/total_count*100))\nprint(\"Spam contains:{:.2f}% of total data.\".format(spam_count\/total_count*100))","8c09935b":"#compute the length of majority & minority class\nminority_len=len(data[data[\"Category\"]==\"spam\"])\nmajority_len=len(data[data[\"Category\"]==\"ham\"])\n\n#store the indices of majority and minority class\nminority_indices=data[data[\"Category\"]==\"spam\"].index\nmajority_indices=data[data[\"Category\"]==\"ham\"].index\n\n#generate new majority indices from the total majority_indices\n#with size equal to minority class length so we obtain equivalent number of indices length\nrandom_majority_indices=np.random.choice(\n    majority_indices,\n    size=minority_len,\n    replace=False\n)\n\n#concatenate the two indices to obtain indices of new dataframe\nundersampled_indices=np.concatenate([minority_indices,random_majority_indices])\n\n#create df using new indices\ndf=data.loc[undersampled_indices]\n\n#shuffle the sample\ndf=df.sample(frac=1)\n\n#reset the index as its all mixed\ndf=df.reset_index()\n\n#drop the older index\ndf=df.drop(\n    columns=[\"index\"],\n)\n","37c63cba":"df.shape","c9261f02":"df[\"Category\"].value_counts()","a20ce3c0":"sns.countplot(\n    data=df,\n    x=\"Category\"\n)\nplt.title(\"ham vs spam\")\nplt.show()","85aacb12":"df.head()","1d2c5309":"df[\"Label\"]=df[\"Category\"].map(\n    {\n        \"ham\":0,\n        \"spam\":1\n    }\n)","2118efba":"df.head()","ab643305":"import re\nimport nltk\nfrom nltk.corpus import stopwords\nfrom nltk.stem import PorterStemmer\n\nstemmer=PorterStemmer()","f521dbb7":"#declare empty list to store tokenized message\ncorpus=[]\n\n#iterate through the df[\"Message\"]\nfor message in df[\"Message\"]:\n    \n    #replace every special characters, numbers etc.. with whitespace of message\n    #It will help retain only letter\/alphabets\n    message=re.sub(\"[^a-zA-Z]\",\" \",message)\n    \n    #convert every letters to its lowercase\n    message=message.lower()\n    \n    #split the word into individual word list\n    message=message.split()\n    \n    #perform stemming using PorterStemmer for all non-english-stopwords\n    message=[stemmer.stem(words)\n            for words in message\n             if words not in set(stopwords.words(\"english\"))\n            ]\n    #join the word lists with the whitespace\n    message=\" \".join(message)\n    \n    #append the message in corpus list\n    corpus.append(message)","794d5ed0":"from tensorflow.keras.preprocessing.text import one_hot\nvocab_size=10000\n\noneHot_doc=[one_hot(words,n=vocab_size)\n           for words in corpus\n           ]","3d8a77af":"df[\"Message Length\"].describe()","c8a4cbe2":"fig=plt.figure(figsize=(12,8))\nsns.kdeplot(\n    x=df[\"Message Length\"],\n    hue=df[\"Category\"]\n)\nplt.title(\"ham & spam messege length comparision\")\nplt.show()","7110313c":"from tensorflow.keras.preprocessing.sequence import pad_sequences\nsentence_len=200\nembedded_doc=pad_sequences(\n    oneHot_doc,\n    maxlen=sentence_len,\n    padding=\"pre\"\n)","a970503c":"extract_features=pd.DataFrame(\n    data=embedded_doc\n)\ntarget=df[\"Label\"]","8e904904":"df_final=pd.concat([extract_features,target],axis=1)","2d321348":"df_final.head()","037f08f2":"X=df_final.drop(\"Label\",axis=1)\ny=df_final[\"Label\"]","a922d03d":"from sklearn.model_selection import train_test_split","4f267301":"X_trainval,X_test,y_trainval,y_test=train_test_split(\n    X,\n    y,\n    random_state=42,\n    test_size=0.15\n)\n","111b5751":"X_train,X_val,y_train,y_val=train_test_split(\n    X_trainval,\n    y_trainval,\n    random_state=42,\n    test_size=0.15\n)","e0a01271":"from tensorflow.keras.layers import LSTM\nfrom tensorflow.keras.layers import Dense\nfrom tensorflow.keras.layers import Embedding\nfrom tensorflow.keras.models import Sequential\n","d3276b57":"model=Sequential()","e8c9b7fc":"feature_num=100\nmodel.add(\n    Embedding(\n        input_dim=vocab_size,\n        output_dim=feature_num,\n        input_length=sentence_len\n    )\n)\nmodel.add(\n    LSTM(\n    units=128\n    )\n)\n\nmodel.add(\n    Dense(\n        units=1,\n        activation=\"sigmoid\"\n    )\n)","359b7ab6":"from tensorflow.keras.optimizers import Adam\nmodel.compile(\n    optimizer=Adam(\n    learning_rate=0.001\n    ),\n    loss=\"binary_crossentropy\",\n    metrics=[\"accuracy\"]\n)","24f0cb72":"model.fit(\n    X_train,\n    y_train,\n    validation_data=(\n        X_val,\n        y_val\n    ),\n    epochs=10\n)","3d27c745":"y_pred=model.predict(X_test)\ny_pred=(y_pred>0.5)","9eb4e697":"from sklearn.metrics import accuracy_score,confusion_matrix","00b36e42":"score=accuracy_score(y_test,y_pred)\nprint(\"Test Score:{:.2f}%\".format(score*100))","8154a11b":"cm=confusion_matrix(y_test,y_pred)\nfig=plt.figure(figsize=(12,8))\nsns.heatmap(\n    cm,\n    annot=True,\n)\nplt.title(\"Confusion Matrix\")\ncm","1288529e":"#The function take model and message as parameter\ndef classify_message(model,message):\n    \n    #We will treat message as a paragraphs containing multiple sentences(lines)\n    #we will extract individual lines\n    for sentences in message:\n        sentences=nltk.sent_tokenize(message)\n        \n        #Iterate over individual sentences\n        for sentence in sentences:\n            #replace all special characters\n            words=re.sub(\"[^a-zA-Z]\",\" \",sentence)\n            \n            #perform word tokenization of all non-english-stopwords\n            if words not in set(stopwords.words('english')):\n                word=nltk.word_tokenize(words)\n                word=\" \".join(word)\n    \n    #perform one_hot on tokenized word            \n    oneHot=[one_hot(word,n=vocab_size)]\n    \n    #create an embedded documnet using pad_sequences \n    #this can be fed to our model\n    text=pad_sequences(oneHot,maxlen=sentence_len,padding=\"pre\")\n    \n    #predict the text using model\n    predict=model.predict(text)\n    \n    #if predict value is greater than 0.5 its a spam\n    if predict>0.5:\n        print(\"It is a spam\")\n    #else the message is not a spam    \n    else:\n        print(\"It is not a spam\")","7bc2ab73":"message1=\"I am having a bad day and I would like to have a break today\"\nmessage2=\"This is to inform you had won a lottery and the subscription will end in a week so call us.\"\n","2cc4f0ba":"classify_message(model,message1)","6a9d25a8":"classify_message(model,message2)","a4fcf89d":"Display the **description** of length of **ham** and **spam** messages seperately on an individual series.\n\nFrom the statistics of the two description we can see that the ham contains the longest message of 910 length. However more than 70% of the ham messages contains messages of length less than 90.\n\nOn the other hand 75% spam messages have messages length more than 130. Hence can conclude than the spam messages are usually lenthier ","c15dcbee":"Perform one_hot on the corpus","c8d6b757":"We will use **pad_sequences** from keras to perform **word embedding**.\n\nThis will make every list to an equal length(**sentence length**) which we can later be fed to our model.","ba9988b6":"Since our dataset is imbalanced I have used undersampling technique to make a balanced dataset.\n\n**Undersampling**: It is a technique of obtaining an equivalent sample from the dataset by simply **deleting** some of the examples of **majority** class","a2824439":"**Visualize** the Messages length using **histogram**\n\nIt is evident form the plot that **spam** messages on an average are usually **lengthier** than the **non-spam(ham)** messages.","2471ba22":"import libraries to create **model**","f0892dcc":"This is a project to classify spam and ham messages using LSTM. This is a basic and simple model that will take one through a detailed steps to approach a NLP problem.","a39878a1":"Perform word **tokenization** using the below block of code","d98a87ef":"We will now split the dataset for **training**,**validataing** and **testing** sets.","c9e2d822":"Check if the dataset contains any **null** values, luckily we got dataset without any null values","8ad6ea20":"Drop these three columns","8ab48c16":"I have concatenated two dataframes to get the final dataframe","bc0525f9":"Created new column **Label** and encode **ham** as **0** and **spam** as **1**","b01b0bc5":"Lets **compile** the model built above.\n\nI have used a **adam** optimizer with **learning rate of 0.001** & **binary_crossentropy** as loss funtion","7e7328d9":"Display the head of new **df**","722e1702":"The dataset contains 5 columns of which 3 Unnamed columns are not important for us","af360b0c":"The model predicts pretty well on the **test** data as evident from the accuracy score.","ff86bdcc":"Lets import **metrics** to **evaluate** our **model**","800b1b1b":"**visualizing the Messages length using kdeplot.**\n\nFrom the plot we can see that spam messages usually are lengthier than ham messages, however both the messages are concentrated bw the lenth of 0-200, of course there are few messages with lengths more than 200. However we will take a length of 200 to make an even distribution amongst the two messages.","8f9dc0b9":"The model predicts **message1** as **not a spam message**","c9fe3845":"Once the model is complied we will **fit** the model using **train** and **validation** dataset.","b83bbf1f":"I have initialized the **vocabulary** size to **10,000.**\n\n**oneHot_doc** will contain the list of indices of words in the corpus whose indices will range in bw **0-10,000**.","e55805f3":"Since the model is fitted using required datasets, its time that how our model predict test data we have isolated earlier.\n\nThe prediction will be stored in array of boolean where prediction value **greater than** **0.5** will be assigned **True(Spam)** else **lesser than 0.5 will be False(Ham)**","7b5ecfbc":"display head to see the new column","751056b3":"This is the final code(**function**) that would take a **raw message** and classfiy the message using the model.","d91c0d26":"The Resulting dataframe contains 201 columns where 200 are independent features and 1 is our target class.","f7a9fd73":"The model predicts **message2** as **spam message**","d3298fa5":"Initialize the **Sequential** model","e2ec653a":"create a model using **Embedding layer->LSTM->Dense**\n\n**Embedding_layer**: We will declare input_dimesion as our vocaburaly size(10,000),input_length as the sentence length and output dimension as 100.\n\n**LSTM**: Add **128** units to the layers whose output will be fed as an input to our output Dense layer.\n\n**Dense**: Add 1 unit(neurons) to the dense layers and with an **sigmoid** activation, since we have binary classification problem. Else if you are to performing multi-class classification problem **softmax** activation with **units=no. of classes** would perform pretty well.","9e54254d":"Now we can see that the category cotains **747** entries for **ham** and **spam** and now we have a **balanced** dataset.","ebfceac5":"columns **v1** & **v2** does not make any sense so rename the columns into meaningful **Category** & **Message** names","9bcca195":"**Steps involved in the Project:**\n\n*  **Data Cleaning:**\n1. Removing unwanted columns.\n2. Exploring & comparing length of messages.\n3. Performing undersampling on dataset.\n\n* **Text preparation:**\n1. Tokenization of Messages.\n2. One hot implementation on tokenized message(corpus)\n3. Perform word embedding\n\n* **Data preparation\/Data Splitting:**\n1. Split the data into training+validation(85%) & testing(15%) data.\n2. Further split the training+validation data into training(85%) and validation(15%) data.\n\n* **Model Building:**\n1. Build a Sequential model: Embedding Layer->LSTM->Dense(output layer)\n2. Fit and Validate model on training and validation model\n\n* **Model Evaluation:**\n1. Evaluate the model on test dataset.\n2. Get the model accuracy score and visualize confusion matrix\n\n\n* **Model Testing:**\n1. Created a function that would classifiy the messages using the model\n","7468fe75":"The two unique values are **ham** & **spam** and **ham** contains **4825** & **spam** with **747** entries which is a vast difference","11f049bf":"From the overall data Description we knew that label **Category** contains 2 unique values and hence is a categorical variable and a binary classification problem","fc38c710":"Import seaborn and matplotlib for visualization","fb129766":"Split the dataframe into **dependent(y)** & **independent(X)** variables ","cd9563da":"Lets **visualize** our **confusion_matrix** using **heatmap**\n\nOur Model also gives a better generalization since the number of **False Positive(FP)** and **False Negative(FN)** are relatively lesser than the **True Postive(TP)** and **True Negative(TN)**","1290a732":"The resulting dataframes have **1494** rows and **4** columns ","dcfc2bc1":"Import libraries to perform word **tokenization**","e904cd74":"The code will split the whole data into two parts **trainval** & **testing**.\n\n**Trainval** dataset will contain the dataset for training and validation sets, which will constitute 85% of the data and rest as test data(15%).\n\nThis is done so to stop **data leakage**. If we use test data as both validation and testing purpose its very likely that our model will just **memorize** the dataset while validating using test data and we will get a **overfitted model**.\n\nSo I just wanted to completely **isolate** the **test** data from **train** and **validation** dataset, so we get a model with better **generilization**.","935969c6":"Visualizing the **Category** using countplot shows our **spam** messages are relatively less compared to the **ham** messages in the dataset","3b83c81b":"Display head of the new dataset","8d57dd70":"Let us make a data frame using embedded document, and a target using **Label** column from df","21868331":"After **one_hot** we will then perform **word embedding** \n\nThe resulting list of one_hot will contain uneven indices length because of uneven length of tokenized words in the corpus.\n\nTo perform word embedding we have to consider a sentence length and hence to define a fixed sentence length for our dataset we will try to visualize and understand the patterns of Messege length of dataset.","39ff074f":"Read the .csv file and display the first entries of the data","b8226a97":"We have splitted **85**% dataset into **training** and **validataion** so lets futher split our **trainval** data into **training**(**85**%) and **validation**(**15**%) dataset.","6faafa19":"Create new column called **Message Length** that would compute the message lengths","d0672e46":"Both **ham** and **spam** message bars are now **equal** and hence obtained a **balanced** dataset.","804d4a13":"**Ham** contains **86.6%** while **spam** constitute only **13.4%** of the total dataset, and thus we can conclude that the data is **imbalanced**.\n\n**Imbalanced Data:** Imbalanced data are dataset with an unequal class distribution. Since the ham contains more than 86% of dataset, the model can plainly have an accuracy score of **86%** by just classifying all the entries as ham message. However we just dont want to have a more accuracy score but also a model that can **generalize** well.\n\n"}}