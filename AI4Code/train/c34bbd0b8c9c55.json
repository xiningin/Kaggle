{"cell_type":{"6c25934d":"code","7c798bc9":"code","12413fac":"code","b5147240":"code","60b821ea":"code","cdaeeb23":"code","13709e8d":"code","f85d5813":"code","47666007":"code","f06d0944":"code","0fa4e6e4":"code","4135475e":"code","654e423a":"code","140f7e2d":"code","1f502081":"code","b99eda0a":"code","af2fb7d4":"code","038032c6":"code","ce0f2098":"code","7f9e0142":"code","7c88b329":"code","8bfc012e":"code","0b6fdb5e":"code","fc339526":"code","39f652d1":"code","1576f1d5":"code","8f202196":"code","2ce4c69e":"code","b031e70e":"code","3d33719f":"code","1b4373c1":"code","86be4405":"code","e3affcca":"code","6d9d2621":"code","17957a2f":"code","2a57d5c0":"code","2ea22065":"code","1679d9c9":"code","f762e1a6":"code","2ef1d32f":"code","96b3d62f":"markdown","87c668de":"markdown","9aee67f6":"markdown","8175b17e":"markdown","2449d69a":"markdown","94d7d4be":"markdown","42a5fe56":"markdown","3272415c":"markdown","abf01f31":"markdown","96600117":"markdown","182d8e72":"markdown","46103240":"markdown","94985506":"markdown","e7ffa8f9":"markdown","75705970":"markdown","31afb01e":"markdown","fec21f42":"markdown","f3fde9db":"markdown","61899d05":"markdown","efef35be":"markdown","a31e1e43":"markdown","463eeb36":"markdown","0b9173dd":"markdown","2ce094cf":"markdown","6c39fcf3":"markdown","faa02519":"markdown","9307234c":"markdown","55ad9831":"markdown","2a9c663d":"markdown","ee375a9e":"markdown","1df89bb5":"markdown","475c9b64":"markdown","7102560c":"markdown","d5a6ce26":"markdown","a0dc34cb":"markdown","6579ad86":"markdown","5bceb353":"markdown","6b9f15df":"markdown","64f2c773":"markdown","55e84e6b":"markdown","e98cadb0":"markdown","d11b21d1":"markdown","6a6ddca1":"markdown","1365c1b3":"markdown","aed7fb86":"markdown","40161010":"markdown","d16db732":"markdown","084e3044":"markdown","9ac02622":"markdown"},"source":{"6c25934d":"from sklearn.preprocessing import StandardScaler\nimport matplotlib.pyplot as plt \nimport os\nimport numpy as np\nimport pandas as pd \nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import confusion_matrix, classification_report\nfrom sklearn.metrics import jaccard_similarity_score\nfrom sklearn import metrics\nfrom sklearn import preprocessing\nimport itertools\nfrom sklearn.model_selection import validation_curve\nfrom xgboost import XGBClassifier\nimport xgboost as xgb\nfrom xgboost import plot_importance\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.metrics import roc_curve, auc\nfrom sklearn.inspection.partial_dependence import partial_dependence, plot_partial_dependence\nfrom sklearn.ensemble import GradientBoostingClassifier\nimport eli5\nfrom eli5.sklearn import PermutationImportance\nimport shap\nfrom mpl_toolkits.mplot3d import Axes3D","7c798bc9":"print(os.listdir('..\/input'))","12413fac":"filename = \"..\/input\/heart.csv\"\ndf = pd.read_csv(filename)\ndf.head()","b5147240":"df_target = df.groupby(\"target\").size()\ndf_target","60b821ea":"plt.pie(df_target.values, labels = [\"target 0\", \"target 1\"], autopct='%1.1f%%', radius = 1.5, textprops = {\"fontsize\" : 16}) \nplt.show()","cdaeeb23":"df_sex = df.groupby([\"sex\",\"target\"]).size()\ndf_sex","13709e8d":"plt.pie(df_sex.values, labels = [\"sex_0,target_0\", \"sex_0,target_1\", \"sex_1,target_0\", \"sex_1,target_1\"],autopct='%1.1f%%',radius = 1.5, textprops = {\"fontsize\" : 16})\nplt.show()","f85d5813":"plt.hist([df[df.target==0].age, df[df.target==1].age], bins = 20, alpha = 0.5, label = [\"no_heart_disease\",\"with heart disease\"])\nplt.xlabel(\"age\")\nplt.ylabel(\"percentage\")\nplt.legend()\nplt.show()","47666007":"plt.hist([df[df.target==0].chol, df[df.target==1].chol], bins = 20, alpha = 0.5, label = [\"no_heart_disease\",\"with heart disease\"])\nplt.xlabel(\"chol\")\nplt.ylabel(\"percentage\")\nplt.legend()\nplt.show()","f06d0944":"plt.hist([df[df.target==0].trestbps, df[df.target==1].trestbps], bins = 20, alpha = 0.5, label = [\"no_heart_disease\",\"with heart disease\"])\nplt.xlabel(\"trestbps\")\nplt.ylabel(\"percentage\")\nplt.legend()\nplt.show()","0fa4e6e4":"plt.hist([df[df.target==0].thalach, df[df.target==1].thalach], bins = 20, alpha = 0.5, label = [\"no_heart_disease\",\"with heart disease\"])\nplt.xlabel(\"thalach\")\nplt.ylabel(\"percentage\")\nplt.legend()\nplt.show()","4135475e":"df_1 = df[[\"age\", \"trestbps\", \"chol\", \"thalach\", \"oldpeak\"]]\ndf_1.describe()","654e423a":"for item in df_1.columns:\n    plt.subplot(3,2,list(df_1.columns).index(item)+1)\n    plt.boxplot(df_1[item], patch_artist=True, labels = [item])\n    plt.ylabel(\"value\")\nplt.tight_layout()\nplt.show()","140f7e2d":"infor = df.describe()\n\ndf2 = df[df.trestbps < infor.loc[\"mean\", \"trestbps\"] + 3 * infor.loc[\"std\", \"trestbps\"]]\ndf3 = df2[df.chol < infor.loc[\"mean\", \"chol\"] + 3 * infor.loc[\"std\", \"chol\"]]\ndf4 = df3[df.thalach > infor.loc[\"mean\", \"thalach\"] - 3 * infor.loc[\"std\", \"thalach\"]]\ndf_new = df4[df.oldpeak < infor.loc[\"mean\", \"oldpeak\"] + 3 * infor.loc[\"std\", \"oldpeak\"]]\ndf_new.head()","1f502081":"df_new.cp = df_new.cp.map({0:\"asymptomatic\", 1: \"typical angina\", 2:\"atypical angina\", 3:\"non-anginal pain\"})\ndf_new.sex = df_new.sex.map({0:\"Female\", 1:\"Male\"}) \ndf_new.exang = df_new.exang.map({0:\"exercise did not induce angina\", 1:\"exercise induced angina\"})\ndf_new.slope = df_new.slope.map({1:\"upsloping\", 2:\"flat\", 3:\"downsloping\"})\ndf_new.thal = df_new.thal.map({1:\"normal\",2:\"fixed defect\", 3:\"reversable defect\"})\ndf_new = pd.get_dummies(df_new, drop_first = True)\ndf_new.head(10)","b99eda0a":"X = df_new.drop(\"target\", 1).values\ny = df_new[\"target\"].astype(\"int\").values\nX_train, X_test, y_train, y_test = train_test_split( X, y, test_size=0.25, random_state=10)","af2fb7d4":"model = XGBClassifier()\nparam = dict(model_max_depth=[3,5,7], model_learning_rate=[0.001,0.01, 0.1], model_n_estimators=[100,500])\n\ncv = GridSearchCV(model, param_grid=param, cv=10, iid = True)\ncv","038032c6":"cv.fit(X_train, y_train)\ntest_pred = cv.predict(X_test)\ncv.best_estimator_","ce0f2098":"max_depth_of_model = cv.best_estimator_.max_depth\nbest_learning_rate = cv.best_estimator_.learning_rate\nbest_estimator = cv.best_estimator_.n_estimators\nbest_reg_lambda = cv.best_estimator_.reg_lambda\n\nmodel = XGBClassifier(max_depth=max_depth_of_model, learning_rate=best_learning_rate, n_estimators= best_estimator,n_jobs=1,)\nmodel.fit(X_train, y_train)\nyhat = model.predict(X_test)\ny_proba = model.predict_proba(X_test)[:, 1]\naccuracy_score(yhat,y_test)","7f9e0142":"importances = model.feature_importances_\nimportances","7c88b329":"inducies = np.argsort(importances)[::-1]\ninducies","8bfc012e":"feature_dict = dict()\nfor idx in inducies:\n    feature_dict[list(df_new.drop(\"target\",1).columns)[idx]] = float(importances[idx])\nfeature_dict","0b6fdb5e":"y_pos = np.arange(len(feature_dict.keys()))\nplt.bar(y_pos, list(feature_dict.values()), align = \"center\",color = \"lightgreen\")\nplt.xticks(y_pos, list(feature_dict.keys()), rotation = 90)\nplt.xlabel(\"feature\")\nplt.ylabel(\"ratio\")\nplt.title(\"feature importances\")\nplt.show()","fc339526":"tp,fn,fp,tn = confusion_matrix(y_test, yhat, labels=[1,0]).ravel()\ntp,tn,fp,fn","39f652d1":"def plot_confusion_matrix(cm, classes,\n                          normalize=False,\n                          title='Confusion matrix',\n                          cmap=plt.cm.Blues):\n    \n    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n    plt.title(title)\n    plt.colorbar()\n    tick_marks = np.arange(len(classes))\n    plt.xticks(tick_marks, classes, rotation=45)\n    plt.yticks(tick_marks, classes)\n\n    fmt = '.2f' if normalize else 'd'\n    thresh = cm.max() \/ 2.\n    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n        plt.text(j, i, format(cm[i, j], fmt),\n                 horizontalalignment=\"center\",\n                 color=\"white\" if cm[i, j] > thresh else \"black\")\n\n    plt.tight_layout()\n    plt.ylabel('True label')\n    plt.xlabel('Predicted label')\n    plt.show()\n    return \"\"","1576f1d5":"cmx = confusion_matrix(y_test, yhat, labels=[1,0])\nprint(plot_confusion_matrix(cmx, classes=['target=1','target=0'],normalize= False,  title='Confusion matrix'))","8f202196":"precision_rate = tp \/ (tp + fp)\nrecall_rate = tp \/ (tp + fn)\nprint(\"The precision rate is: \", precision_rate)\nprint(\"The recall rate is: \", recall_rate)","2ce4c69e":"fpr, tpr, threshold =roc_curve(y_test, y_proba)\nfig, ax = plt.subplots()\nax.plot(fpr,tpr)\nax.plot([0,1], [0,1], transform = ax.transAxes, ls=\"--\", c=\"0.3\")\nplt.xlim(0.0, 1.0)\nplt.ylim(0.0, 1.0)\nplt.xlabel(\"FPR or 1-Specificity\")\nplt.ylabel(\"TPR or Sensitivity\")\nplt.rcParams[\"font.size\"] = 10\nplt.grid(True)\nplt.show()","b031e70e":"auc(fpr, tpr)","3d33719f":"perm = PermutationImportance(model, random_state=1).fit(X_test, y_test)\neli5.show_weights(perm, feature_names = df_new.drop(\"target\", 1).columns.tolist())","1b4373c1":"clf = GradientBoostingClassifier(learning_rate = best_learning_rate, n_estimators = best_estimator,\n                                 max_depth = max_depth_of_model).fit(X_train, y_train)\n\nfeature_dict = dict(enumerate(df_new.drop(\"target\", 1).columns))\nfeature_dict","86be4405":"my_plots = plot_partial_dependence(clf, features = [0], X = X_test, \n                                   feature_names = df_new.drop(\"target\", 1).columns[:1], grid_resolution=5)     \n\nplt.subplots_adjust(top=1.0, right=1.0)\nmy_plots","e3affcca":"my_plots = plot_partial_dependence(clf, features = [7], X = X_test, \n                                   feature_names = df_new.drop(\"target\", 1).columns[:8], grid_resolution=5)     \n\nplt.subplots_adjust(top=1.0, right=1.0)\nmy_plots","6d9d2621":"my_plots = plot_partial_dependence(clf, features = [5], X = X_test, \n                                   feature_names = df_new.drop(\"target\", 1).columns[:6], grid_resolution=5)     \n\nplt.subplots_adjust(top=1.0, right=1.0)\nmy_plots","17957a2f":"my_plots = plot_partial_dependence(clf, features = [1], X = X_test, \n                                   feature_names = df_new.drop(\"target\", 1).columns[:2], grid_resolution=5)     \n\nplt.subplots_adjust(top=1.0, right=1.0)\nmy_plots","2a57d5c0":"my_plots = plot_partial_dependence(clf, features = [6], X = X_test, \n                                   feature_names = df_new.drop(\"target\", 1).columns[:7], grid_resolution=5)     \n\nplt.subplots_adjust(top=1.0, right=1.0)\nmy_plots","2ea22065":"fig = plt.figure()\ntarget_feature = (6, 13)\npdp, axes = partial_dependence(clf, X_train, target_feature,  grid_resolution=50)\nXX, YY = np.meshgrid(axes[0], axes[1])\nZ = pdp[0].reshape(list(map(np.size, axes))).T\nax = Axes3D(fig)\nsurf = ax.plot_surface(XX, YY, Z, rstride=1, cstride=1, cmap=plt.cm.BuPu, edgecolor='k')\nax.set_xlabel(feature_dict[target_feature[0]])\nax.set_ylabel(feature_dict[target_feature[1]])\nax.set_zlabel('Partial dependence')\n\nax.view_init(elev=20, azim=72)\nplt.colorbar(surf)\nplt.subplots_adjust(top=0.9)\nplt.show()","1679d9c9":"explainer = shap.TreeExplainer(model)\nshap_values = explainer.shap_values(X_test)\nshap.summary_plot(shap_values, X_test, feature_names = list(feature_dict.values()), plot_type = \"bar\", color = \"lightblue\")","f762e1a6":"shap.summary_plot(shap_values, X_test, feature_names = list(feature_dict.values()))","2ef1d32f":"def shap_force_plot_of_data(model, dataset):\n    explainer = shap.TreeExplainer(model)\n    shap_value_for_sample = explainer.shap_values(dataset)\n    shap.initjs()\n    drivein_force = shap.force_plot(explainer.expected_value, shap_value_for_sample, dataset)\n    return drivein_force\n\nperson_is_monitored = pd.DataFrame(X_test, columns = list(feature_dict.values()))\nshap_force_plot_of_data(model, person_is_monitored[person_is_monitored[\"sex_Male\"]==1])\n","96b3d62f":"Replace the name of category data and seperate them into different columns.","87c668de":"Index the features as dictionary.","9aee67f6":"Great! We got the learning rate equals to 0.1, max depth equals to 3 and number of trees equals to 100. Use the best condition to implement the XGBoost model. ","8175b17e":"Obviously, we find some features are seperated completely, like ca(number of major vessels colored by flourosopy), reversable defect, atypical angina, slope_upsloping, restecg, sex_male etc. It's easy to see the trend of each features. Most of these features are expected but sex_male and exercise induced angina. From our basic knowledge, if someone has angina while exerciseing, this person may be at risk. But the shap plot shows us the opposite results, that's interesting.","2449d69a":"There is a formula that can calculate a person's maximun heart rate: 220 - age\n\nBut, see the partial dependence results, the faster the rate is, the higher the risk is.\nDoes it mean that the young persons are in the high risk of heart disease? Not really!\nIf the max heart rate become faster year by year, it is possible that the heart become weaker. ","94d7d4be":"There is 1 csv file in the current version of the dataset:\n","42a5fe56":"Maximum heart rate is negatively related to the age. Let's see the distribution.","3272415c":"Let's see how percentage of the people who has heart disease.","abf01f31":"We all think intuitively that high blood pressure causes heart disease, is it real!? ","96600117":"As we mentioned in introduction, we hope the recall accuracy is high. Before getting the score, we calculate four parts of confusion matrix. \n\nFirst one is true positive. It means that the sample is positive example, and the prediction is also positive.\nSecond one is false negative. This means that the sample is positive example, but the prediction is negative, and this is what we care about.\nThird one is false positive. This one is relatived to precision of model. Most time we care about precision more, but this time the recall rate is more important.\nLast but not least, true negative is that the predicition and sample are all negative. It's a bit the same as true positive.","182d8e72":"To see which factors are significantly affect the heart disease, Xgboost library is a easy way to predict the result.\nWe should optimize the parameters first, so we assume different max depth of classification, learning rate and number of trees. \nSecondly, use crossvalid method to make sure the parameters is optimised.","46103240":"Obviously, \"ca\" and \"reversable defect\" are highly effective to heart disease, and male also has higher possibility to get heart disease. Maximum heart rate plays an important role in getting heart disease, the result surprised me because it is positive related to the heart disease. The other result surprising me is trestbps(resting blood pressure), its weight is not higher than we think.\n\nThe \"age\" is negative related to the heart disease. So let's take a look at the partial dependence plot to see what happen.","94985506":"The accuracy of model is 82.4%, it's not bad.\nAfter that, let's see the importance of each features.","e7ffa8f9":"We can figure out the relationship between oldpeak and slope.","75705970":"From this chart, a person with asymptomatic st-depression and flat slope is under high risk of getting heart disease.","31afb01e":"How about the resting blood pressure?","fec21f42":"Arrange the features as dictionary data structure.","f3fde9db":"## Conclusion\nCardiovascular diseases is the top one killer for many years. I think that the reasons are our lack of knowledge about heart disease and the life habits. According to the model and features analysis, we know which features that we can do regular self-examinations. \n\nI think the most obvious symptom is chest pain. There are three types of chest pain, but only atypical angina is strongly related to the heart disease. No matter which type of chest pain you have, go to the doctor in case.\n\nIn addition, everyone should always keep an eye on the resting blood pressure. The ideal resting blood pressure is lower than 120mmHg, but if your blood pressure is much lower than the 120mmHg, it means that you are under high risk of heart disease. Besides, the problem will not be only heart when the blood pressure is higher than 150mmHg.\n\nLots and lots of electronic devices that can measure heart rate, so it's easier to monitor your own. Record max heart rate to make sure that your heart is still healthy. Once the rate rises year by year, there must be something wrong with you. \n\nNo matter how healthy we are, we must do annual examination because another features can not be taken care of by ourselves. Finally, don't forget the older we are, the higher the risks are.","61899d05":"Finally, let's plot the shap summary to know the data distribution of each features.","efef35be":"According to the medicine knowledge, if the major vessels colored by flourosopy get more, it means that the risk of heart disease will be lower. So, vessel flourosopy examination for everyone is a very important process to diagnose whether the people has heart disease or not.","a31e1e43":"Next, we see how ca(number of major vessels colored by flourosopy) affects the heart disease.","463eeb36":"Sort the features on the basic of importances. ","0b9173dd":"Let's see how distribution of age is.","2ce094cf":"Surprisingly! Although the resting blood pressure is still positive related to the risk of heart disease, a person who has lower resting blood pressure has higher possibility to get heart disease.\nFrom this article, \"low blood pressure that causes an inadequate flow of blood to the body's organs can cause strokes, heart attacks, and kidney failure. The most severe form is shock.\"\n  Reference :\u3000https:\/\/www.medicinenet.com\/low_blood_pressure\/article.htm\n\nBesides, the risk becomes much lower once the resting blood pressure is above 150 mmHg, that's also a suprising outcome.","6c39fcf3":"The quantity of false negative is 1. Seems the recall rate will be above 90%. Let's plot the confusion matrix.","faa02519":"Define the dataset for training and testing.","9307234c":"Over 50% people who was diagnosed heart disease","55ad9831":"Next, we figure out the ROC curve performance.\nThe two main parameters of ROC curve is TPR and FPR. TPR is called true positive rate, and also called hit rate or sensitivity. The meaning of TPR is the ratio of positive sample being determined correctly. On the other hand, FPR is the ratio of negative sample being determined incorrectly. That is, the formula is defined (1 - Specificity). Let's see the auc.","2a9c663d":"## Exploratory Analysis\nThere are thirteen features and one target as below:\n* age: The person's age in years\n* sex: The person's sex (1 = male, 0 = female)\n* cp: The chest pain experienced (Value 1: typical angina, Value 2: atypical angina, Value 3: non-anginal pain, Value 4: asymptomatic)\n* trestbps: The person's resting blood pressure (mm Hg on admission to the hospital)\n* chol: The person's cholesterol measurement in mg\/dl\n* fbs: The person's fasting blood sugar (> 120 mg\/dl, 1 = true; 0 = false)\n* restecg: Resting electrocardiographic measurement (0 = normal, 1 = having ST-T wave abnormality, 2 = showing probable or definite left ventricular hypertrophy by Estes' criteria)\n* thalach: The person's maximum heart rate achieved\n* exang: Exercise induced angina (1 = yes; 0 = no)\n* oldpeak: ST depression induced by exercise relative to rest \n* slope: the slope of the peak exercise ST segment (Value 1: upsloping, Value 2: flat, Value 3: downsloping)\n* ca: The number of major vessels (0-3)\n* thal: A blood disorder called thalassemia (3 = normal; 6 = fixed defect; 7 = reversable defect)\n    \n* target: Heart disease (0 = no, 1 = yes)\n\n","ee375a9e":"Once we know the detail of the confusion matrix, we can calculate some types of accuracy.\nIn this type of dataset, the recall rate is much important than precision rate, because no one want to misdiagnose the patient who has heart disease.","1df89bb5":"The ratio of male has heart disease is 30.7%, a little bit higher than female.","475c9b64":"Let's see which condition is the best.","7102560c":"The ratio get higher over the age of forty. That is, people who is over forty is under high risk of heart disease.","d5a6ce26":"Before modeling the dataset, let's find out if there are outliers inside the dataset.\nWe check the contineous data by boxplot. ","a0dc34cb":"Next, let's see the weight of each features to know which one strongly affects the possibility.","6579ad86":"ST segment is an indicated features of electrocardiogram. It depresses during exercising, and doctors always diagonose the heart situation by the depression. ","5bceb353":"If the AUC is in the range of 0.5~1, that means the model has value of prediction. The higher the score is, the better the model is. Moreover, the model will be a perfect classifier if the AUC equals to one. We got the 0.9 of the AUC score, not bad!","6b9f15df":"The recall rate is about 97%, that is, still 3% of patient who has heart disease will be misdiagnosed. But now it's good enough.","64f2c773":"The ideal blood pressure should be lower than 120 mmHg. Whether the patients have heart disease or not , over 50% patients have higher blood pressure.","55e84e6b":"As the figure shown, the probability of getting heart disease is higher whether the age is higher, that's reasonable. So we must focus on heart disease because we are getting older.","e98cadb0":"Interesting outcome. It seems that the medium of patient with heart disease is higher. ","d11b21d1":"When the st segment depresses too much, that's means that the heart is lack of oxygen. On the other hand, a person with asymptomatic ST-depression during exercise has high risk of sudden cardiac death. This result is impressive.","6a6ddca1":"Let's see what happen to \"age\".","1365c1b3":"OK! Now we see \"thal_normal\" is not an important feature, that is, we can delete the column from dataset. \nOn the other hand, reversable defect and ca are much higher than others.  \nWe talked about the influence of features later.","aed7fb86":"There are outliers in each features except age. We define the outliers that are not in the range of 3 sigma. Then remove the outliers.","40161010":"Also, amounts of people having heart disease are over 200mg\/dl of chol. According to the research, the normal value of chol should be lower than 200mg\/dl.","d16db732":"How abot the max heart rate??","084e3044":"Also, we plot the shap force plot of lots of persons at once. We can easily to understandard what causes each person toward heart disease. As shown below, we choose all the male to see the distribution.","9ac02622":"## Introduction\n  WHO announced that cardiovascular diseases is the top one killer over the world. There are seventeen million people died from it every year, especially heart disease. Prevention is better than cure. If we can evaluate the risk of every patient who probably has heart disease, that is, not only patients but also everyone can do something earlier to keep illness away.\n\nThis dataset is a real data including important features of patients. This time we will build the predictable model by XGBoost library. Before predict the test dataset, use concept of crossvalid to find the optimised parameters. after that, the model can calculate the weight of each features, so we can easily understand which feature is more influent than others.\n\nConfusion matrix is a common technique to figure out the accuracy of the model. From the standpoint of medicine, the recall rate is more important than precision rate because no one want to be misdiagnosed if the one actually have heart disease. So we will check the recall performance. After that, roc curve can help us evaluate the model, and then we'll explore the features if the model is good enough.\n\n\"Shap\" is a powerful library to know whether each feature is positive or negative relationship with heart disease. At the end of the report, we double confirm the result, and we can also know which feature affect each patient the most."}}