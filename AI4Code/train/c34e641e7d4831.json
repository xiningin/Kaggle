{"cell_type":{"d045cf46":"code","e8c099fa":"code","59848ca8":"code","41c55ca2":"code","9983df4c":"code","2b547430":"markdown","448b922c":"markdown","2ccbe927":"markdown","b8a40e63":"markdown","b0c856fc":"markdown"},"source":{"d045cf46":"import numpy as np\n\nR = np.array([[4,np.NaN,np.NaN,2,np.NaN],\n             [np.NaN,5,np.NaN,3,1],\n             [np.NaN,np.NaN,3,4,4],\n             [5,2,1,2,np.NaN]])\n\nnum_users, num_items = R.shape\nK=3\n\nnp.random.seed(1)\nP=np.random.normal(scale=1.\/K, size=(num_users,K))\nQ=np.random.normal(scale=1.\/K, size=(num_items,K))","e8c099fa":"print(\"R matrix\\n\",R)\nprint(\"P matrix\\n\",P)\nprint(\"Q matrix\\n\",Q)","59848ca8":"from sklearn.metrics import mean_squared_error\n\ndef get_rmse(R, P, Q, non_zeros):\n    error=0\n    \n    full_pred_matrix = np.dot(P, Q.T)\n    \n    x_non_zero_ind=[non_zero[0] for non_zero in non_zeros]\n    y_non_zero_ind=[non_zero[1] for non_zero in non_zeros]\n    R_non_zeros=R[x_non_zero_ind, y_non_zero_ind]\n    full_pred_matrix_non_zeros=full_pred_matrix[x_non_zero_ind, y_non_zero_ind]\n    mse = mean_squared_error(R_non_zeros,full_pred_matrix_non_zeros)\n    rmse = np.sqrt(mse)\n    \n    return rmse","41c55ca2":"non_zeros = [(i,j,R[i,j]) for i in range(num_users) for j in range(num_items) if R[i,j]>0]\n\nsteps = 1000\nlearning_rate = 0.01\nr_lambda = 0.01\n\nfor step in range(steps):\n    for i,j,r in non_zeros:\n        eij = r-np.dot(P[i,:],Q[j,:].T)\n        P[i,:]=P[i,:]+learning_rate*(eij*Q[j,:]-r_lambda*P[i,:])\n        Q[j,:]=Q[j,:]+learning_rate*(eij*P[i,:]-r_lambda*Q[j,:])\n        \n        rmse = get_rmse(R,P,Q,non_zeros)\n        \n    if(step % 50)==0:\n        print(\"###iteration step: \",step, \" rmse: \",rmse)","9983df4c":"pred_matrix=np.dot(P,Q.T)\nprint('\uc608\uce21\ud589\ub82c:\\n',np.round(pred_matrix,3))","2b547430":"# Stochastic Gradient Descent\nAccording to this method, we can optimize the value that we want to get. It change the random value P,Q gradientlly, and fix the dot of P,Q.T as R. But we know some of the part of R, so what we want to get is the rest of the R value. So as changed P and Q, we can know the rest of the R value\n","448b922c":"Finally, we can get the predicted matirx of R.\n\nYou can find that what we already know in R matirx are simillar with predicted.","2ccbe927":"First, we will make the R matirx. There are some NaN value, so we can figure out what is NaN value according to the SGD.\n\nWe make the P and Q as random value.","b8a40e63":"And now iterate it. I fix the number of iteration as steps value. So I will repeat 1000 times to get more smaller value of rmse.\n\nTo avoid overfitting problem, We updata P,Q according to L2 regularization","b0c856fc":"Now, we will make get_rmse function to get rhe rmse. RMSE means root mean square error. It is good as RMSE become smaller. So we gradientlly change P,Q value to get smaller value of RMSE."}}