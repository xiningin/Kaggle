{"cell_type":{"786dd834":"code","676bdee3":"code","5a34be37":"code","83f625d2":"code","e59e41d0":"code","406495c2":"code","de5d66e8":"code","d85b8f88":"code","0cbb2f0c":"code","7a04bb30":"code","a06f51c5":"code","a7a79c88":"code","30946bdc":"code","2f254d57":"code","c572ce55":"code","58fce476":"code","5f622f6d":"code","2378d315":"code","c6a680c5":"code","0ddf0e5c":"code","8e345259":"code","634577de":"code","9ae13fe8":"code","2ab8452b":"code","3e0022ae":"code","3d19de64":"code","04564ab2":"code","f47191ba":"code","b7e4c9cd":"code","50172fd5":"code","6d641ab5":"code","6840ed9a":"code","4fb34e7b":"code","7f3c01c3":"code","6ca51c7a":"code","dfbea5c8":"markdown","09e2a2fe":"markdown","f7dc84b3":"markdown","119738ee":"markdown","34667588":"markdown","55cbd8f0":"markdown","ded3d576":"markdown","8f83d093":"markdown","2c4ceaa0":"markdown","3f38b467":"markdown","dba65bf9":"markdown","71f18c6f":"markdown","30a8cf6c":"markdown","390359f1":"markdown","9e67e126":"markdown","009642d7":"markdown","ae2b141a":"markdown","fff2c6ac":"markdown","1608be15":"markdown","2f68fcc9":"markdown","9e118475":"markdown","5cb0501d":"markdown","3f33ad27":"markdown","73af8777":"markdown","8a3db019":"markdown","f2b3f3b2":"markdown"},"source":{"786dd834":"import pandas as pd \nimport os\nimport numpy as np \nimport seaborn as sns\nfrom matplotlib import pyplot as plt\n%matplotlib inline\nimport warnings\nfrom sklearn import tree\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.externals.six import StringIO  \nfrom IPython.display import Image  \nfrom sklearn.tree import export_graphviz\nimport pydotplus\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import f1_score\nfrom sklearn import metrics\nfrom sklearn.model_selection import RandomizedSearchCV\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import GridSearchCV\nimport warnings","676bdee3":"df = pd.read_csv('..\/input\/WA_Fn-UseC_-Telco-Customer-Churn.csv')\ndf.head(5)","5a34be37":"print(df.isnull().sum())\ndf['Churn'].replace('Yes',1,inplace=True)\ndf['Churn'].replace('No',0,inplace=True)\n","83f625d2":"#There is an issue with the Total Charges colummns (the data is stored as a string)\nprint(' The data type for the Total Charges Column is:',type(df['TotalCharges'].loc[4]))\n#While attempting to convert this to a numeric type, ran into another problem at some positions,empty strings\nprint(df['TotalCharges'][(df['TotalCharges'] == ' ')])\n","e59e41d0":"# Drop rows where there is no value for Total Charges \nindex = [488,753,936,1082,1340,3331,3826,4380,5218,6670,6754]\nfor i in index: \n    df.drop(i,axis=0,inplace=True)","406495c2":"# Convert from str to float\ndf['TotalCharges'].apply(float)\n","de5d66e8":"# Inspecting frequency in the different demographic variables that are not related to the service\ndem = ['gender','SeniorCitizen','Partner','Dependents']\n\nfor i in dem: \n    sns.barplot(x = df.groupby(str(i))['Churn'].sum().reset_index()[str(i)]\\\n                , y = df.groupby(str(i))['Churn'].sum().reset_index()['Churn'],)\n    plt.show()\n    print(df.groupby(str(i))['customerID'].count().reset_index())\n","d85b8f88":"cat = ['PhoneService','MultipleLines','InternetService','OnlineSecurity','OnlineBackup','DeviceProtection','TechSupport','StreamingTV','StreamingMovies']\n\nfor i in cat: \n    sns.barplot(x = df.groupby(str(i))['Churn'].sum().reset_index()[str(i)]\\\n                , y = df.groupby(str(i))['Churn'].sum().reset_index()['Churn'],)\n    plt.show()\n    print(df.groupby(str(i))['customerID'].count().reset_index())\n    ","0cbb2f0c":"pay = ['Contract','PaperlessBilling','PaymentMethod']\n\nfor i in pay: \n    sns.barplot(x = df.groupby(str(i))['Churn'].sum().reset_index()[str(i)]\\\n                , y = df.groupby(str(i))['Churn'].sum().reset_index()['Churn'])\n    plt.show()\n    print(df.groupby(str(i))['customerID'].count().reset_index())\n\n","7a04bb30":"# Convert Binary Categories to 0's and 1's\ndf['Partner'].replace('Yes',1,inplace=True)\ndf['Partner'].replace('No',0,inplace=True)\ndf['Dependents'].replace('Yes',1,inplace=True)\ndf['Dependents'].replace('No',0,inplace=True)\ndf['gender'].replace('Male',1,inplace=True)\ndf['gender'].replace('Female',0,inplace=True)\ndf['PhoneService'].replace('Yes',1,inplace=True)\ndf['PhoneService'].replace('No',0,inplace=True)\ndf['PaperlessBilling'].replace('Yes',1,inplace=True)\ndf['PaperlessBilling'].replace('No',0,inplace=True)\n","a06f51c5":"## Prepare Categorical Variables with more than 2 categories\ncat_X = df[['MultipleLines','InternetService','OnlineSecurity','OnlineBackup',\\\n            'DeviceProtection','TechSupport','StreamingTV','StreamingMovies',\\\n           'Contract','PaymentMethod']]\n# Dummy Categorical Variables \nfor i in cat_X: \n    cat_X = pd.concat([cat_X,pd.get_dummies(cat_X[str(i)],\\\n                                            drop_first=True,prefix=str(i))],axis=1)\n\n\ncat_X = cat_X.drop(columns=['MultipleLines','InternetService','OnlineSecurity','OnlineBackup',\\\n            'DeviceProtection','TechSupport','StreamingTV','StreamingMovies',\\\n           'Contract','PaymentMethod'])\n","a7a79c88":"features = pd.concat([df[['tenure','Partner','Dependents','gender','PhoneService',\\\n                          'PaperlessBilling','MonthlyCharges','TotalCharges']],cat_X],axis=1)\n","30946bdc":"# Used stratified split as the classes are imbalanced\nX=features\ny= df['Churn']\nX_train, X_test, y_train, y_test = train_test_split(features, df['Churn'], \\\n                                                    test_size=0.33, random_state=42,stratify=y)\nmy_DT = tree.DecisionTreeClassifier(max_depth=3)\nmy_DT.fit(X_train, y_train)\ndot_data = StringIO()\nexport_graphviz(my_DT, out_file=dot_data,feature_names=features.columns,\n                filled=True, rounded=True,\n                special_characters=True)\ngraph = pydotplus.graph_from_dot_data(dot_data.getvalue())  \nImage(graph.create_png())\n","2f254d57":"X = features\ny = df['Churn']\ndepth_range = np.arange(1,50,1)\nval_scores = []\nfor d in depth_range:\n    my_DT = tree.DecisionTreeClassifier(max_depth=d)\n    scores = cross_val_score(my_DT, X, y, cv=10, scoring='accuracy')\n    val_scores.append(scores.mean())\nprint(val_scores)\n","c572ce55":"#Plot results from cross-validation\nfig,(ax1,ax2) = plt.subplots(ncols=2,figsize=(10,5))\nax1.plot(depth_range, val_scores)\nax1.set_xlabel('Max_Depth Values')\nax1.set_ylabel('Cross-Validated Accuracy Scores')\n\n# A more zoomed in version of the first plot\nax2.plot(depth_range,val_scores)\nax2.set_xlim(1,15)\nax2.set_xlabel('Max_Depth Values')\nax2.set_ylabel('Cross-Validated Accuracy Scores')\n\n","58fce476":"my_DT = tree.DecisionTreeClassifier(max_depth=3)\nmy_DT.fit(X_train,y_train)\nprint(my_DT.score(X_train,y_train))\nprint(my_DT.score(X_test,y_test))\n","5f622f6d":"# What are the 10 most important features for classification ? \nimp = pd.DataFrame(my_DT.feature_importances_).sort_values(by=0,ascending=False).\\\nhead(10).index.values\nimp_vals = pd.DataFrame(my_DT.feature_importances_).sort_values(by=0,ascending=False).\\\nhead(10)\n\nfor i,j in zip(imp,imp_vals[0]):\n    print(features.columns[i],j)\n    ","2378d315":"y_pred = my_DT.predict(X_test)\ndef cm(pred):\n    cm = confusion_matrix(y_test, pred)\n    fig = plt.plot(figsize=(8,5))\n    sns.heatmap(cm,annot=True,cmap='Blues',fmt='g')\n    plt.xlabel('Predicted')\n    plt.ylabel('Actual')\n    print(f1_score(y_test,pred))\n    return plt.show()\ncm(y_pred)\n","c6a680c5":"y_proba_DT = my_DT.predict_proba(X_test)","0ddf0e5c":"def roc_auc(prediction,model):\n    fpr, tpr, thresholds = metrics.roc_curve(y_test,prediction)\n    auc = metrics.auc(fpr, tpr)\n\n    plt.title('Receiver Operating Characteristic '+str(model))\n    plt.plot(fpr, tpr, color='blue', label = 'AUC = %0.2f' % auc)\n    plt.legend(loc = 'lower right')\n    plt.plot([0, 1], [0, 1],'--',color='red')\n    plt.xlim([0, 1])\n    plt.ylim([0, 1])\n    plt.ylabel('True Positive Rate')\n    plt.xlabel('False Positive Rate')\n    return plt.show()","8e345259":"roc_auc(y_proba_DT[:, 1],'Decision Tree Classifier')","634577de":"from sklearn.ensemble import RandomForestClassifier\nRF = RandomForestClassifier()\nRF.fit(X_train,y_train)\nprint(RF.score(X_train,y_train))\nprint(RF.score(X_test,y_test))","9ae13fe8":"warnings.filterwarnings('ignore')\n# Number of trees in random forest\nn_estimators = np.arange(10,1000,10)\n# Number of features to consider at every split\n# Maximum number of levels in tree\nmax_depth = np.arange(1,25,2)\n# Minimum number of samples required to split a node\nmin_samples_split = [2,4,8]\n# Minimum number of samples required at each leaf node\nmin_samples_leaf = [1, 2, 4]\n# Method of selecting samples for training each tree\nbootstrap = [True, False]\n# Create the random grid\ngrid = {'n_estimators': n_estimators,\n               'max_depth': max_depth,\n               'min_samples_split': min_samples_split,\n               'min_samples_leaf': min_samples_leaf,\n               'bootstrap': bootstrap}\nrf_random = RandomizedSearchCV(estimator = RF, param_distributions = grid, n_iter = 100, cv = 3, verbose=2, random_state=42, n_jobs = -1)\n# Fit the random search model\nrf_random.fit(X_train, y_train)\n\n\n","2ab8452b":"print(rf_random.best_score_)\nprint(rf_random.best_params_)","3e0022ae":"RFR=RandomForestClassifier(bootstrap= True, max_depth= 11, min_samples_split=\\\n 2, n_estimators=30,min_samples_leaf= 4)\nRFR.fit(X_train,y_train)\nprint(RFR.score(X_train,y_train))\nprint(RFR.score(X_test,y_test))\ny_pred_rf = RFR.predict(X_test)","3d19de64":"cm(y_pred_rf)","04564ab2":"y_proba_rf = RFR.predict_proba(X_test)","f47191ba":"roc_auc(y_proba_rf[:,1],'Random Forest Classifier')","b7e4c9cd":"lr = LogisticRegression()\nprint(lr.fit(X_train,y_train))\nprint(lr.score(X_train,y_train))\nprint(lr.score(X_test,y_test))","50172fd5":"penalty = ['l1', 'l2']\nC = np.logspace(0, 4, 10)\nhyperparameters = dict(C=C, penalty=penalty)\nclf = GridSearchCV(lr, hyperparameters, cv=5, verbose=0)\ngrid_model = clf.fit(X_train, y_train)","6d641ab5":"print('Best Penalty:', grid_model.best_estimator_.get_params()['penalty'])\nprint('Best C:', grid_model.best_estimator_.get_params()['C'])","6840ed9a":"print(grid_model.score(X_train,y_train))\nprint(grid_model.score(X_test,y_test))\ny_pred_lr = grid_model.predict(X_test)","4fb34e7b":"cm(y_pred_lr)","7f3c01c3":"y_proba_lr = grid_model.predict_proba(X_test)","6ca51c7a":"roc_auc(y_proba_lr[:,1],'Logistic Regression')","dfbea5c8":"The first tree is just a proof of concept to get an idea of which features really minimize confusion during the learning process for the tree (gini impurity). \n   \n   - Tenure and Fiber Optic Services seem to be really important features to classify whether a customer will leave or remain. ","09e2a2fe":"<a id='ml' ><\/a>","f7dc84b3":"   #### Demographic\nFew things that stood out that seem to be quite different at the demographic level: \n- Number of Senior Citizens \n- Number of Individuals with Dependents \n\nIn terms of churn, one interesting thing stands out, is that although the distribution of individuals with partners is equal, individuals without a partner are more likely to leave the company while individuals with a partner are more likely to remain as customers. This is a factor that is out of the company's control\n\n","119738ee":"<a id='explo' ><\/a>\n","34667588":"#### ROC-AUC Curve Logistic Regression","55cbd8f0":"### Cleaning ","ded3d576":"### Exploratory Analysis\n\n- For my EDA, I split up the data into 3 categories. The first categroy is demographic which contains features like gender, partner etc.. ","8f83d093":"<a id='ml' ><\/a>","2c4ceaa0":"The random forest scores slightly better and brings down the number of false negatives from 419 to 321 but .. results in twice the amount of false positives. ","3f38b467":"### Decision Tree Classifier","dba65bf9":"#### ROC-AUC Curve Logistic Regression","71f18c6f":"Cross-Validation to find optimal value for the max_depth","30a8cf6c":"#### Confusion Matrix Decision Tree Classifier\n  - The x axis shows the predicted values and the y axis show the true class values. On the top right we have the \"false positives\" which are the X values that are actually 0s but were classified (predicted) as 1s. On the bottom left we have the \"false negatives\" which are values that are actually 1s (churn) but were classified (predicted) as 0s.   \n  \n  - The F1 score shown above the confusion matrix represents the harmonic mean of precision and recall. It receives equal contribution from precision and recall, hence the higher the score (closer to 1) the better the model is at classifying. ","390359f1":" #### Service-Specific \n\n- One thing that stood out may seem to uncover a service that may be causing high churn rates. Looking at the Internet Service, it seems that people with Fiber Optic service are more likely to leave the company. Although individuals with fiber optic services make up a large proportion of internet service customers, DSL customers have a much lower churn rate and make up almost the same number of individuals\/customers.\n   - Proportion of individuals with DSL leaving the company 459\/2416 * 100 = 19%\n   - Proportion of individuals with Fiber Optic leaving the company 1297\/3096 * 100 = 42%\n \n \n- OnlineSecurity seems to be another factor causing high churn rates \n  - Proportion of individuals with Online Security leaving the company = 295\/3497 *100 = 8.4%\n  - Proportion of individuals without Online Security leaving the company = 1461\/2015*100= 73%\n  \n  \n- OnlineBackup,Device Protection and Tech Support also follow the same pattern as online security where individuals without these services are more likely to leave the company. \n\n\n\n##### Streaming Services \n  - More than 40% of individuals with streaming TV and streaming movies service are also unsubscribing which may indicate that the streaming services could also be improved","9e67e126":"#### ROC-AUC Random Forest Classifier","009642d7":"### Machine Learning\n- Start off converting variables to their appropriate format. Convert Binary Variables to 1 for Yes and 0 for no. For variables with more than 2 categories I used pd.get_dummies.","ae2b141a":"#### Confusion Matrix Random Forest Classifier","fff2c6ac":"<a id='cleaning' ><\/a>","1608be15":"### Random Forest Regressor ","2f68fcc9":"Decision Tree with Optimized Hyperparameters","9e118475":"### Logistic Regression","5cb0501d":"#### Payment-Specific \n   - One thing that stood out was that the majority of customers pay for Telco services on a month to month contract basis. \n\n","3f33ad27":"#### ROC-AUC Decision Tree Classifier\n - The red line (random predictor) is used as a baseline to see whether the model is useful.\n - The blue line demonstrates the TPR and FPR at varying thresholds. \n - The greater the Area under the Curve (AUC), the better the model is at classifying. ","73af8777":"Farther down, I ran into an issue with the Total Charges Column as highlighted below. In the next few cells, I just adjust this column by removing these empty strings and reformatting the column to a numeric type","8a3db019":"The dataset is very clean, there are no null values. Here, I replaced the binary outcome variable from Yes and No into 1 and 0. ","f2b3f3b2":"# Telco Customer Churn \n\n## Part 1 \n### Data Cleaning \n- [Cleaning](#cleaning) \n\n## Part 2\n### Exploratory Analysis \n- [Exploratory Analysis](#explo)\n\nFor EDA, I do some very simple exploratory analysis to try and pull out interesting factors that may be affecting customer churn. \n\n\n\n## Part 3\n- [Machine Learning](#ml)\n\nI then fit a decision tree classifier and a random forest classifier to classify the data with highest possible accuracy without overfitting (maintaing a good bias variance tradeoff !)\n\n\n   - Decision Tree Classifier \n   - Random Forest Classifier\n   - Logistic Regression  \n\n\n## Recommendation\n\n### Demographic \n- One thing I noted is that individuals without partners\/dependents are more likely to leave the company. In my opinion, individuals 'with' partners and dependents are more likely  to remain as most telecom companies offer special deals for families, couples etc.. As a recommendation, I would recommend the telecom company to maybe have a special promotion\/marketing campaign targeted towards individuals without partners\/dependents. \n\n\n### Service-Specific\n- Based off the feature importances and the EDA, I can conclude that the company needs to relaunch\/remodel their FiberOptic Service. \n\n\n- On the other hand, a lot of customers that are leaving the company seem to also not be registered for any support-like services (security, protection etc..) I would recommend the company to start offering these as bundles with services as customers who are signed up for these security-like services are more likely to remain with the company. (or make it mandatory to have these or just make them included) \n\n### Payment-Specific\n- No recommendations "}}