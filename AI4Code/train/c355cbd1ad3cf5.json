{"cell_type":{"d145f492":"code","c02ed3ea":"code","e4f8f407":"code","f1677920":"code","b2e51f2d":"code","cab67159":"code","55a97ccd":"code","1def22a9":"code","9d64de1f":"code","c63e3fbf":"code","87269a1d":"markdown","80bb8f1a":"markdown","7af02759":"markdown","8bfef80c":"markdown","4ba3dc86":"markdown","7a45a069":"markdown","71101e3f":"markdown","e0e53f8c":"markdown","f4323c9f":"markdown","9f087cab":"markdown","ba8d826c":"markdown","8584b06c":"markdown"},"source":{"d145f492":"import os,re\nfrom bs4 import BeautifulSoup\nimport nltk\nfrom nltk.tokenize import word_tokenize","c02ed3ea":"files = os.listdir('..\/input\/html-text-files')\ndata = ''\nfor i in files:\n  obj = open('..\/input\/html-text-files\/' + i)\n  dat = obj.read()\n  soup = BeautifulSoup(dat)\n  dat = soup.find('text').text\n  data += dat","e4f8f407":"print(\"Printing only first 2000 words of text for better display:\\n\\n\",data[0:2000])","f1677920":"data = re.sub('[!\\\"#$%&\\'()*+,-.\/:;<=>?@[\\]^_`{|}~0-9\\n]','',data)\nprint(\"Printing only first 2000 words of text after removing Puntuation for better display:\\n\\n\",data[0:2000])","b2e51f2d":"nltk.download('punkt')\ntokens = word_tokenize(data)\nprint(\"\\n\\nTokens are as follows, and I have displayed 100 tokens for better visual display:\\n\\n\",tokens[1:100])","cab67159":"tokens = [item.lower() for item in tokens]\nprint(\"Resulting tokens are as follows,  i have displayed 100 tokens for better visual display:\\n\\n\",tokens[1:100])\nprint(\"\\n\\nNumber of tokens are : \",len(tokens))","55a97ccd":"from nltk.corpus import stopwords\nfor i in tokens:\n  if i in stopwords.words('english'):\n    tokens.remove(i) \nprint(\"Tokens after removing stopwords are, i have displayed 100 tokens for better visual display:\\n\",tokens[1:100],\"\\n\\nNumber of tokens are :\",len(tokens),sep='\\n')    ","1def22a9":"stem_porter = []\nfrom nltk.stem import PorterStemmer\nporter = PorterStemmer()\nfor word in tokens:\n  stem_porter.append(porter.stem(word))","9d64de1f":"stem_lemma = []\nnltk.download('wordnet')\nfrom nltk.stem import WordNetLemmatizer\nlemmatizer = WordNetLemmatizer()\n\nfor word in tokens:\n  stem_lemma.append(lemmatizer.lemmatize(word,pos='v'))","c63e3fbf":"print(\"Original_Token      Lemmatization_Token       Porter_Stemmer_Token\\n\\n\")\n\nfor word in tokens[1:300]:\n  stem_lemma.append(lemmatizer.lemmatize(word,pos='v'))\n  print(\"{0:25}{1:25}{2:25}\".format(word,lemmatizer.lemmatize(word,pos='v'),porter.stem(word)))","87269a1d":"0. ># Preprocessing on Text is important step before any Information Retrieval Application. In this notebook, preprocessing is explained step wise, which are as follows:\n> * Combining data from all files into single string using BeautifulSoup\n> * Removing Punctuations from text\n> * Tokenization using NLTK library\n> * Removing Stopwords\n> * Stemming shown using both Porter Stemmer & Lemmatization\n> * Conclusion\/Compare roots formed by Porter Stemmer and those obtained by Lemmatization ","80bb8f1a":"2. >  # **Appending data from all utf files into single string \"data\"**","7af02759":"6. > # **Removing stopwords**","8bfef80c":"> # Printing Data","4ba3dc86":"4. > # Tokenization","7a45a069":"3. > # Removing Punctuation","71101e3f":"5. > # **Converting all text to Lowercase letters**","e0e53f8c":"> # **Conclusion what i can make is Lemmatization normalise to roots\/stem that do exist in language, while Porter stemmer identifies stem\/root which may not exist as a real word in language, language being refered here is English.**\n> # Comparsion of roots obtained using Lemmatization and Porter Stemming can be seen below","f4323c9f":"7. > # **Stemming using Porter Stemmer**","9f087cab":"1. > # Importing Libaries","ba8d826c":"* > Using NLTK library","8584b06c":"7. > # **Stemming Using Lemmatization**"}}