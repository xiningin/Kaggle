{"cell_type":{"5dce9e87":"code","a0ffa58d":"code","5baf9f60":"code","dbe05928":"code","7c45f84b":"code","422205dd":"code","adca88df":"code","52d42f84":"code","15c4e257":"code","ab51e91d":"code","cc198d6b":"code","5d0c021d":"code","6823b2d5":"code","a8955418":"code","5af6616d":"code","cc6ade08":"code","2ff10129":"code","363df0d7":"markdown","58bd4987":"markdown","bd6084fc":"markdown","3cd39387":"markdown","f4f14b4f":"markdown","72295499":"markdown","5b9c52d2":"markdown","d8f8b271":"markdown","ec3f0fb2":"markdown","75a1643b":"markdown","6b86105e":"markdown","ff56ba56":"markdown"},"source":{"5dce9e87":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n# Load everything that may be needed\nimport numpy as np\nimport copy\nimport matplotlib.pyplot as plt\nimport h5py\nimport scipy\nimport imageio\nimport cv2 \nimport os\nfrom PIL import Image\nfrom scipy import ndimage\nfrom scipy import misc","a0ffa58d":"train_messy = \"\/kaggle\/input\/messy-vs-clean-room\/images\/train\/messy\/\"\ntrain_clean= \"\/kaggle\/input\/messy-vs-clean-room\/images\/train\/clean\"\ntest_messy= \"\/kaggle\/input\/messy-vs-clean-room\/images\/val\/messy\"\ntest_clean= \"\/kaggle\/input\/messy-vs-clean-room\/images\/val\/clean\"","5baf9f60":"def load_dataset():\n    \n    train_set_x_orig_list,train_set_y_list, test_set_x_orig_list, test_set_y_list=[],[],[],[]\n\n    for image in (os.listdir(train_messy)): \n        path = os.path.join(train_messy, image)\n        img = cv2.imread(path) \n        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n        train_set_x_orig_list.append(img)\n        train_set_y_list.append(0)\n\n    for image in (os.listdir(train_clean)): \n        path = os.path.join(train_clean, image)\n        img = cv2.imread(path) \n        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n        train_set_x_orig_list.append(img)\n        train_set_y_list.append(1)\n\n    for image in (os.listdir(test_messy)): \n        path = os.path.join(test_messy, image)\n        img = cv2.imread(path) \n        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n        test_set_x_orig_list.append(img)\n        test_set_y_list.append(0)\n\n    for image in (os.listdir(test_clean)): \n        path = os.path.join(test_clean, image)\n        img = cv2.imread(path) \n        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n        test_set_x_orig_list.append(img)\n        test_set_y_list.append(1)\n\n    train_set_x_orig=np.array(train_set_x_orig_list)\n    train_set_y=np.array(train_set_y_list)\n    train_set_y=train_set_y.reshape((1,train_set_y.shape[0]))\n\n    test_set_x_orig=np.array(test_set_x_orig_list)\n    test_set_y=np.array(test_set_y_list)\n    test_set_y=test_set_y.reshape((1,test_set_y.shape[0]))\n\n    classes_list=[b'messy',b'clean']\n    classes=np.array(classes_list)\n    \n    return train_set_x_orig, train_set_y, test_set_x_orig, test_set_y, classes","dbe05928":"train_set_x_orig, train_set_y, test_set_x_orig, test_set_y,classes=load_dataset()","7c45f84b":"index = 100\nplt.imshow(train_set_x_orig[index])\nprint (\"y = \" + str(train_set_y[:, index]) + \", it's a '\" + classes[np.squeeze(train_set_y[:, index])].decode(\"utf-8\") +  \"' picture.\")","422205dd":"index = 10\nplt.imshow(train_set_x_orig[index])\nprint (\"y = \" + str(train_set_y[:, index]) + \", it's a '\" + classes[np.squeeze(train_set_y[:, index])].decode(\"utf-8\") +  \"' picture.\")","adca88df":"m_train = train_set_x_orig.shape[0]\nm_test = test_set_x_orig.shape[0]\nnum_px = train_set_x_orig.shape[1]\n\nprint (\"Number of training examples: m_train = \" + str(m_train))\nprint (\"Number of testing examples: m_test = \" + str(m_test))\nprint (\"Height\/Width of each image: num_px = \" + str(num_px))\nprint (\"Each image is of size: (\" + str(num_px) + \", \" + str(num_px) + \", 3)\")\nprint (\"train_set_x shape: \" + str(train_set_x_orig.shape))\nprint (\"train_set_y shape: \" + str(train_set_y.shape))\nprint (\"test_set_x shape: \" + str(test_set_x_orig.shape))\nprint (\"test_set_y shape: \" + str(test_set_y.shape))","52d42f84":"train_set_x_flatten = train_set_x_orig.reshape(train_set_x_orig.shape[0], num_px*num_px*3).T\ntest_set_x_flatten = test_set_x_orig.reshape(test_set_x_orig.shape[0], -1).T\nprint (\"train_set_x_flatten shape: \" + str(train_set_x_flatten.shape))\nprint (\"train_set_y shape: \" + str(train_set_y.shape))\nprint (\"test_set_x_flatten shape: \" + str(test_set_x_flatten.shape))\nprint (\"test_set_y shape: \" + str(test_set_y.shape))\n# print (\"sanity check after reshaping: \" + str(train_set_x_flatten[0:5,0]))","15c4e257":"train_set_x = train_set_x_flatten\/255.\ntest_set_x = test_set_x_flatten\/255.\nprint('number of train datasets =' + str(train_set_x.shape))\nprint('number of test datasets =' + str (test_set_x.shape))","ab51e91d":"print('% of Messy in the training data: ', 100*np.sum(train_set_y == 0)\/len(train_set_y[0]))\nprint('% of Clean in the training data: ', 100*np.sum(train_set_y == 1)\/len(train_set_y[0]))","cc198d6b":"def sigmoid(z):\n    s = 1\/(1+np.exp(-z))\n    return s\n\ndef initialize_with_zeros(dim):\n    w = np.zeros((dim,1))\n    b = 0.\n    return w, b\n\ndef propagate(w, b, X, Y):\n    m = X.shape[1]\n    A = sigmoid(np.dot(w.T,X)+b)\n    A = A.astype(np.float64)\n    cost = -1\/m*np.sum(np.nan_to_num(Y*np.log(A)+(1-Y)*np.log(1-A)),axis=1)  \n    dw = 1\/m*np.dot(X,(A-Y).T)\n    db = 1\/m*np.sum(A-Y)  \n    cost = np.squeeze(np.array(cost))\n    grads = {\"dw\": dw,\n             \"db\": db}    \n    return grads, cost\n\ndef optimize(w, b, X, Y, num_iterations=100, learning_rate=0.009, print_cost=False):\n    w = copy.deepcopy(w)\n    b = copy.deepcopy(b)  \n    costs = []    \n    for i in range(num_iterations): \n        grads, cost = propagate(w,b,X,Y)\n        dw = grads[\"dw\"]\n        db = grads[\"db\"]\n        w = w-learning_rate*dw\n        b = b-learning_rate*db \n        if i % 100 == 0:\n            costs.append(cost)\n            if print_cost:\n                print (\"Cost after iteration %i: %f\" %(i, cost))\n    params = {\"w\": w,\n              \"b\": b}\n    grads = {\"dw\": dw,\n             \"db\": db}\n    return params, grads, costs\n\ndef predict(w, b, X):\n    m = X.shape[1]\n    Y_prediction = np.zeros((1, m))\n    w = w.reshape(X.shape[0], 1)\n    A = sigmoid(np.dot(w.T,X)+b)\n    for i in range(A.shape[1]):\n        if A[0,i]<=0.5:\n            Y_prediction[0,i]=0\n        else:\n            Y_prediction[0,i]=1\n    return Y_prediction\n\ndef model(X_train, Y_train, X_test, Y_test, num_iterations=2000, learning_rate=0.5, print_cost=False):\n    w, b = initialize_with_zeros(X_train.shape[0])\n    parameters, grads, costs = optimize(w,b,X_train,Y_train,num_iterations,learning_rate,print_cost)\n    w = parameters[\"w\"]\n    b = parameters[\"b\"]\n    Y_prediction_test = predict(w,b,X_test)\n    Y_prediction_train = predict(w,b,X_train)\n    if print_cost:\n        print(\"train accuracy: {} %\".format(100 - np.mean(np.abs(Y_prediction_train - Y_train)) * 100))\n        print(\"test accuracy: {} %\".format(100 - np.mean(np.abs(Y_prediction_test - Y_test)) * 100))\n    d = {\"costs\": costs,\n         \"Y_prediction_test\": Y_prediction_test, \n         \"Y_prediction_train\" : Y_prediction_train, \n         \"w\" : w, \n         \"b\" : b,\n         \"learning_rate\" : learning_rate,\n         \"num_iterations\": num_iterations}\n    return d","5d0c021d":"d = model(train_set_x, train_set_y, test_set_x, test_set_y, num_iterations = 1500, learning_rate = 0.01, print_cost = True)","6823b2d5":"# Plot learning curve (with costs)\ncosts = np.squeeze(d['costs'])\nplt.plot(costs)\nplt.ylabel('cost')\nplt.xlabel('iterations (per hundreds)')\nplt.title(\"Learning rate =\" + str(d[\"learning_rate\"]))\nplt.show()","a8955418":"def ownimage(my_image):\n    # We preprocess the image to fit your algorithm.\n    fname = my_image\n    image = np.array(Image.open(fname).resize((num_px, num_px)))\n    plt.imshow(image)\n    image = image \/ 255.\n    image = image.reshape((1, num_px * num_px * 3)).T\n    my_predicted_image = predict(d[\"w\"], d[\"b\"], image)\n\n    print(\"y = \" + str(np.squeeze(my_predicted_image)) + \", your algorithm predicts a \\\"\" + classes[int(np.squeeze(my_predicted_image)),].decode(\"utf-8\") +  \"\\\" picture.\")","5af6616d":"# wrongly Classified\nownimage(\"\/kaggle\/input\/messy-vs-clean-room\/images\/test\/0.png\")","cc6ade08":"# correctly Classified\nownimage(\"\/kaggle\/input\/messy-vs-clean-room\/images\/test\/1.png\")","2ff10129":"# correctly Classified\nownimage(\"\/kaggle\/input\/messy-vs-clean-room\/images\/test\/2.png\")","363df0d7":"##### Plot the cost function and the gradients.","58bd4987":"### Train Accuracy of 100.0%\n### Test Accuracy of 60.0%","bd6084fc":"#### To estimate the percentages","3cd39387":"Reshape the training and test data sets so that images of size (num_px, num_px, 3) are flattened into single vectors of shape (num_px $*$ num_px $*$ 3, 1).\n\nA trick when you want to flatten a matrix X of shape (a,b,c,d) to a matrix X_flatten of shape (b$*$c$*$d, a) is to use:\n\nX_flatten = X.reshape(X.shape[0], -1).T      # X.T is the transpose of X","f4f14b4f":"Find the values for:\n\n- m_train (number of training examples)\n- m_test (number of test examples)\n- num_px (= height = width of a training image)\nRemember that train_set_x_orig is a numpy-array of shape (m_train, num_px, num_px, 3). For instance, you can access m_train by writing train_set_x_orig.shape[0].","72295499":"#### To check for a clean room","5b9c52d2":"#### To load the datasets\nThe train and test images are taken and stored as 'X'. Also, there corresponding 'Y' values are stored","d8f8b271":"#### To check for a messy room","ec3f0fb2":"#### Store the paths for the datasets","75a1643b":"\nTo represent color images, the red, green and blue channels (RGB) must be specified for each pixel, and so the pixel value is actually a vector of three numbers ranging from 0 to 255.\n\nOne common preprocessing step in machine learning is to center and standardize your dataset, meaning that you substract the mean of the whole numpy array from each example, and then divide each example by the standard deviation of the whole numpy array. But for picture datasets, it is simpler and more convenient and works almost as well to just divide every row of the dataset by 255 (the maximum value of a pixel channel).\n\nLet's standardize our dataset.","6b86105e":"# Test your own image","ff56ba56":"# Logistic Regression\n\nThe main steps for building a Neural Network are:\n\nDefine the model structure (such as number of input features)\nInitialize the model's parameters\nLoop:\nCalculate current loss (forward propagation)\nCalculate current gradient (backward propagation)\nUpdate parameters (gradient descent)\nYou often build 1-3 separately and integrate them into one function we call model()."}}