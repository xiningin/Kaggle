{"cell_type":{"c2c9883e":"code","0c31880a":"code","14ffcc5b":"code","0e539565":"code","73bb9b3b":"code","bec5d875":"code","da2c0fe0":"code","0ae4edc8":"code","f36d79d2":"code","b0478404":"code","b3bb9c35":"markdown","e9433407":"markdown","c0e8b066":"markdown","58e6990e":"markdown","f179d9f7":"markdown","bda20843":"markdown","de52d0d8":"markdown","face3d43":"markdown","798916ae":"markdown","e42ed1b5":"markdown","eb01980a":"markdown","b9887768":"markdown","8e4a2068":"markdown","03857bfb":"markdown"},"source":{"c2c9883e":"#basic tools \nimport os\nimport numpy as np\nimport pandas as pd\nimport warnings\n\n#tuning hyperparameters\nfrom bayes_opt import BayesianOptimization\nfrom skopt  import BayesSearchCV \n\n#graph, plots\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n#building models\nimport lightgbm as lgb\nimport xgboost as xgb\nfrom sklearn.model_selection import train_test_split, StratifiedKFold, cross_val_score\nimport time\nimport sys\n\n#metrics \nfrom sklearn.metrics import roc_auc_score, roc_curve\nimport shap\nwarnings.simplefilter(action='ignore', category=FutureWarning)","0c31880a":"def reduce_mem_usage(df, verbose=True):\n    numerics = ['int8','int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n    start_mem = df.memory_usage().sum() \/ 1024**2    \n    for col in df.columns:\n        col_type = df[col].dtypes\n        if col_type in numerics:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)  \n            else:\n                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                    df[col] = df[col].astype(np.float16)\n                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)    \n    end_mem = df.memory_usage().sum() \/ 1024**2\n    if verbose: print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'.format(end_mem, 100 * (start_mem - end_mem) \/ start_mem))\n    return df","14ffcc5b":"%%time\ntrain= reduce_mem_usage(pd.read_csv(\"..\/input\/train.csv\"))\ntest= reduce_mem_usage(pd.read_csv(\"..\/input\/test.csv\"))\nprint(\"Shape of train set: \",train.shape)\nprint(\"Shape of test set: \",test.shape)","0e539565":"y=train['target']\nX=train.drop(['ID_code','target'],axis=1)","73bb9b3b":"%%time\n\ndef bayes_parameter_opt_lgb(X, y, init_round=15, opt_round=25, n_folds=3, random_seed=6,n_estimators=10000, output_process=False):\n    # prepare data\n    train_data = lgb.Dataset(data=X, label=y, free_raw_data=False)\n    # parameters\n    def lgb_eval(learning_rate,num_leaves, feature_fraction, bagging_fraction, max_depth, max_bin, min_data_in_leaf,min_sum_hessian_in_leaf,subsample):\n        params = {'application':'binary', 'metric':'auc'}\n        params['learning_rate'] = max(min(learning_rate, 1), 0)\n        params[\"num_leaves\"] = int(round(num_leaves))\n        params['feature_fraction'] = max(min(feature_fraction, 1), 0)\n        params['bagging_fraction'] = max(min(bagging_fraction, 1), 0)\n        params['max_depth'] = int(round(max_depth))\n        params['max_bin'] = int(round(max_depth))\n        params['min_data_in_leaf'] = int(round(min_data_in_leaf))\n        params['min_sum_hessian_in_leaf'] = min_sum_hessian_in_leaf\n        params['subsample'] = max(min(subsample, 1), 0)\n        \n        cv_result = lgb.cv(params, train_data, nfold=n_folds, seed=random_seed, stratified=True, verbose_eval =200, metrics=['auc'])\n        return max(cv_result['auc-mean'])\n     \n    lgbBO = BayesianOptimization(lgb_eval, {'learning_rate': (0.01, 1.0),\n                                            'num_leaves': (24, 80),\n                                            'feature_fraction': (0.1, 0.9),\n                                            'bagging_fraction': (0.8, 1),\n                                            'max_depth': (5, 30),\n                                            'max_bin':(20,90),\n                                            'min_data_in_leaf': (20, 80),\n                                            'min_sum_hessian_in_leaf':(0,100),\n                                           'subsample': (0.01, 1.0)}, random_state=200)\n\n    \n    #n_iter: How many steps of bayesian optimization you want to perform. The more steps the more likely to find a good maximum you are.\n    #init_points: How many steps of random exploration you want to perform. Random exploration can help by diversifying the exploration space.\n    \n    lgbBO.maximize(init_points=init_round, n_iter=opt_round)\n    \n    model_auc=[]\n    for model in range(len( lgbBO.res)):\n        model_auc.append(lgbBO.res[model]['target'])\n    \n    # return best parameters\n    return lgbBO.res[pd.Series(model_auc).idxmax()]['target'],lgbBO.res[pd.Series(model_auc).idxmax()]['params']\n\nopt_params = bayes_parameter_opt_lgb(X, y, init_round=5, opt_round=10, n_folds=3, random_seed=6,n_estimators=10000)","bec5d875":"opt_params[1][\"num_leaves\"] = int(round(opt_params[1][\"num_leaves\"]))\nopt_params[1]['max_depth'] = int(round(opt_params[1]['max_depth']))\nopt_params[1]['min_data_in_leaf'] = int(round(opt_params[1]['min_data_in_leaf']))\nopt_params[1]['max_bin'] = int(round(opt_params[1]['max_bin']))\nopt_params[1]['objective']='binary'\nopt_params[1]['metric']='auc'\nopt_params[1]['is_unbalance']=True\nopt_params[1]['boost_from_average']=False\nopt_params=opt_params[1]\nopt_params\n","da2c0fe0":"%%time \n\ntarget=train['target']\nfeatures= [c for c in train.columns if c not in ['target','ID_code']]\n\n\nfolds = StratifiedKFold(n_splits=10, shuffle=True, random_state=31416)\noof = np.zeros(len(train))\npredictions = np.zeros(len(test))\nfeature_importance_df = pd.DataFrame()\n\nfor fold_, (trn_idx, val_idx) in enumerate(folds.split(train.values, target.values)):\n    print(\"Fold {}\".format(fold_))\n    trn_data = lgb.Dataset(train.iloc[trn_idx][features], label=target.iloc[trn_idx])\n    val_data = lgb.Dataset(train.iloc[val_idx][features], label=target.iloc[val_idx])\n\n    num_round = 15000\n    clf = lgb.train(opt_params, trn_data, num_round, valid_sets = [trn_data, val_data], verbose_eval=500, early_stopping_rounds = 250)\n    oof[val_idx] = clf.predict(train.iloc[val_idx][features], num_iteration=clf.best_iteration)\n    \n    fold_importance_df = pd.DataFrame()\n    fold_importance_df[\"Feature\"] = features\n    fold_importance_df[\"importance\"] = clf.feature_importance()\n    fold_importance_df[\"fold\"] = fold_ + 1\n    feature_importance_df = pd.concat([feature_importance_df, fold_importance_df], axis=0)\n    \n    predictions += clf.predict(test[features], num_iteration=clf.best_iteration) \/ folds.n_splits\n\nprint(\"CV score: {:<8.5f}\".format(roc_auc_score(target, oof)))\n","0ae4edc8":"cols = (feature_importance_df[[\"Feature\", \"importance\"]]\n        .groupby(\"Feature\")\n        .mean()\n        .sort_values(by=\"importance\", ascending=False)[:20].index)\nbest_features = feature_importance_df.loc[feature_importance_df.Feature.isin(cols)]\n\nplt.figure(figsize=(20,28))\nsns.barplot(x=\"importance\", y=\"Feature\", data=best_features.sort_values(by=\"importance\",ascending=False))\nplt.title('Features importance (averaged\/folds)')\nplt.tight_layout()\nplt.savefig('Feature_Importance.png')","f36d79d2":"explainer = shap.TreeExplainer(clf)\nshap_values = explainer.shap_values(X)\n\nshap.summary_plot(shap_values, X)","b0478404":"#tree visualization\ngraph = lgb.create_tree_digraph(clf, tree_index=3, name='Tree3' )\ngraph.graph_attr.update(size=\"110,110\")\ngraph","b3bb9c35":"# **Work in Progress..** <br><br>\n\nAlthough I feel confident that I can implement Bayesian Optimization with LightGBM by Python and understand what my Python code does, I do not feel so comfortable with math behind it...\ud83d\ude25  I will keep working on researching more about math behind Bayesian Optimization and share with you! <br><br>\nHere are some academic papers about Bayesian Optimization just in case you are interested in:<br>\n1) http:\/\/papers.nips.cc\/paper\/4522-practical-bayesian-optimization-of-machine-learning-algorithms.pdf<br>\n2) https:\/\/arxiv.org\/pdf\/1012.2599v1.pdf\n\n<br>\n<center>**I hope you guys enjoyed my kernel and do not forget to upvote if you think that it's helpful!**<\/center> <br>\n\n![](https:\/\/media.giphy.com\/media\/osjgQPWRx3cac\/giphy.gif)\n","e9433407":"<br>\n# ** CONTENTS**\n\n1. [What Is Bayesian Optimization and Why Do We Care?](#1)\n2. [Loading Library and Dataset](#2)\n3. [Bayesian Optimization with LightGBM](#3)\n4. [Training LightGBM](#4)\n5. [Understanding the Model Better](#5)\n","c0e8b066":"By Changing the data type of each column, I reduced memory usages by 75%. By taking the minimum and the maximum of each column, the function assigns which numeric data type is optimal for the column and change the data type. If you want to know more about how it works, I suggest you to read [Eryk's article](https:\/\/towardsdatascience.com\/make-working-with-large-dataframes-easier-at-least-for-your-memory-6f52b5f4b5c4)! ","58e6990e":"<a id=\"1\"><\/a> <br>\n\n# **What Is Bayesian Optimization and Why Do We Care?**","f179d9f7":"<a id=\"5\"><\/a> <br>\n# **Understanding the Model Better**\n<br>\n\nTo get an overview of which features are most important for a model, we can plot the SHAP values of every feature for every sample. The plot below sorts features by the sum of SHAP value magnitudes over all samples, and uses SHAP values to show the distribution of the impacts each feature has on the model output. [(source)](https:\/\/github.com\/slundberg\/shap) The color represents the feature value (red high, blue low). This reveals for example that a high var_139 lowers the probability of being a customer who will make a specific transaction in the future. ","bda20843":"![](https:\/\/github.com\/fmfn\/BayesianOptimization\/blob\/master\/examples\/func.png?raw=true)","de52d0d8":"<a id=\"4\"><\/a> <br>\n# **Training LightGBM** <br> <br>\n\nBased on the parameter from the previous step, I am going to train LightGBM. ","face3d43":"# **Ready, Set, Go!** <br>\n\nBefore starting the kernel, I guarantee that tuning hyperparameter process will not take more than 10 minutes. And the whole process (loading dataset, tuning the hyperparameter, and training LightGBM) will not take more than 20 minutes.  **Time is important!!** <br>\n![](https:\/\/media.giphy.com\/media\/3oz8xKaR836UJOYeOc\/giphy.gif)\n","798916ae":"We can also plot a tree from the model and see each tree! ","e42ed1b5":"Here is my optimal parameter for LightGBM. ","eb01980a":"<a id=\"3\"><\/a> <br>\n# **Bayesian Optimization with LightGBM**\n<br>\nNow I am going to prepare data for modeling and a Baysian Optimization function. You can put more parameters (ex. lambda_l1 and lambda_l2) into the function.  ","b9887768":"<a id=\"2\"><\/a> <br>\n# **Loading Library and Dataset**\n","8e4a2068":"# **Introduction**\n<br>\nHi guys! <br>\nHave you guys experienced some frustrating moments when you tuned hyperparameters with Grid Search and Random Search?  <br>\nWell...I have! I waited for two hours or more to run codes for both methods and ended up losing my focus by watching Youtube videos...\n<br>\n![](https:\/\/media.giphy.com\/media\/qjF9Akev3QPNC\/giphy.gif)\n<br>\nSo, I thought that there must be a faster way to tune hyperparameters and did some research about the new method of tuning, which is called Baysian Optimization.<br>\nIf you have done Kaggle for a while or are an expert in this field, you probably have used or heard the Baysian Optimization. <br>\nThis kernel will give you a good idea of Baysian Optimization and the simple implementation of Baysian Optimization with BayesianOptimization, which I found that it is faster than other Baysian Optimization functions in Python (ex. Scikit-Optimize and Hyperopt) for my model.  <br> \n\n**The goal is to identify which customers will make a specific transaction in the future and maximize the evaluation function (AUC)**\n<br> <br>\n\n# **Special Thanks to:** <br> \n\nI recommend you to see  [Fayzur's kernel](https:\/\/www.kaggle.com\/fayzur\/lgb-bayesian-parameters-finding-rank-average) and [sz8416's kernel](https:\/\/www.kaggle.com\/sz8416\/simple-bayesian-optimization-for-lightgbm) if you are interested in seeing what other people have done. Their kernels were very helpful to understand about the Baysian optimization process in Python. ","03857bfb":"**Bayesian Optimization** is a probabilistic model based approach for finding the minimum of any function that returns a real-value metric. [(source)](https:\/\/towardsdatascience.com\/an-introductory-example-of-bayesian-optimization-in-python-with-hyperopt-aae40fff4ff0)<br> It is very effective with real-world applications in high-dimensional parameter-tuning for complex machine learning algorithms. Bayesian optimization utilizes the Bayesian technique of setting a prior over the objective function and\ncombining it with evidence to get a posterior function. I attached one graph that demonstrates Bayes\u2019 theorem below. <br> <br>\n\n<img src=\"https:\/\/www.analyticsvidhya.com\/wp-content\/uploads\/2016\/06\/12-768x475.jpg\"  alt=\"Drawing\" style=\"width: 600px;\"\/>\n<br> <br> \n The prior belief is our belief in parameters before modeling process. The posterior belief is our belief in our parameters after observing the evidence.\n<br> Another way to present the Bayes\u2019 theorem is: \n\n<img src=\"https:\/\/www.maths.ox.ac.uk\/system\/files\/attachments\/Bayes_0.png\"  alt=\"Drawing\" style=\"width: 600px;\"\/> <br> \n\nFor continuous functions, Bayesian optimization typically works by assuming the unknown function was sampled from a Gaussian process and maintains a posterior distribution for this function as observations are made, which means that we need to give range of values of hyperparameters (ex. learning rate range from 0.1 to 1).  So, in our case, the Gaussian process gives us a prior distribution on functions. Gaussian process approach is a non-parametric approach, in that it finds a distribution over the possible functions \nf(x) that are consistent with the observed data. Gaussian processes have proven to be useful surrogate models for computer experiments and good\npractices have been established in this context for sensitivity analysis, calibration and prediction While these strategies are not considered in the context of optimization, they can be useful to researchers in machine learning who wish to understand better the sensitivity of their models to various hyperparameters. [(source)](http:\/\/papers.nips.cc\/paper\/4522-practical-bayesian-optimization-of-machine-learning-algorithms.pdf)\n\n<br>\n**Well..why do we care about it?** <br> \nAccording to the [study](http:\/\/proceedings.mlr.press\/v28\/bergstra13.pdf), hyperparameter tuning by Bayesian Optimization of machine learnin models is more efficient than Grid Search and Random Search. Bayesian Optimization has better overall performance on the test data and takes less time for optimization. Also, we do not need to set a certain values of parameters like we do in Random Search and Grid Search. For Bayesian Optimization tuning, we just give a range of a hyperparameter. \n\n"}}