{"cell_type":{"04b3a0c2":"code","1e9b9ef7":"code","34b76c41":"code","031ccc70":"code","de69e4bb":"code","5ec265d9":"code","d562be08":"code","83bf4abd":"code","a06da5b1":"code","37aae603":"code","a1ce5c59":"code","90c1b4f8":"code","d6101be0":"code","ab56d4b1":"code","8b42f12e":"code","dbbb60a1":"code","262c9021":"code","7d5e9323":"code","1c7bbf1c":"code","bff283e0":"code","c8803ddb":"code","1b15ae8c":"code","a1ad9c36":"code","2c816d7d":"code","06638cb3":"code","25cc75d8":"code","929016d8":"code","395ac680":"code","ac5f72ea":"code","f4d1cf7a":"code","174b1245":"code","d57c1654":"code","f73efaed":"code","073973d7":"code","648bc510":"code","f6595713":"code","ce43bf06":"markdown","9e4b7e1d":"markdown","a9576938":"markdown","c5c51da8":"markdown","a991ab08":"markdown","826d090f":"markdown","c769959f":"markdown","1ec8f9f1":"markdown","6b08fbc9":"markdown","3e0bcf44":"markdown","93303354":"markdown","c91e2e51":"markdown","a9ea1f76":"markdown","f7cca570":"markdown","3c1ca79a":"markdown","42dfb22a":"markdown","c4e8d5cd":"markdown","b3a9ca2e":"markdown","8280a0d4":"markdown","ef3fb441":"markdown","3a56888e":"markdown","3b583e1c":"markdown","2c9a7437":"markdown","529582cd":"markdown","8e133e31":"markdown","f51cd11c":"markdown","35f4fdcc":"markdown","e5f9b8ff":"markdown","970500b8":"markdown","cb94b4fd":"markdown","3f30e17a":"markdown","304045ef":"markdown","789d21a8":"markdown","4193857b":"markdown","35efb993":"markdown","4b8962d9":"markdown","3d3f10df":"markdown","d7556365":"markdown","13e5622a":"markdown","cad89bcd":"markdown","6069d9a1":"markdown","8c475e52":"markdown"},"source":{"04b3a0c2":"# import modules needed\n%matplotlib inline\nimport matplotlib.pyplot as plt\nimport seaborn as sns; sns.set()\nimport numpy as np\nimport pandas as pd\nimport sklearn as sk\nfrom math import sqrt\nimport warnings\nwarnings.filterwarnings('ignore')","1e9b9ef7":"# read in our data\ndata = pd.read_excel('..\/input\/diet-dataclean\/SWAPMEAT.xlsx')\ndata.head(10)","34b76c41":"data.shape","031ccc70":"data.dtypes","de69e4bb":"df = data['Baseline']","5ec265d9":"# plot the distribution of the bounce times - this creates the object that will be plotted when matplotlib.pyplot\nsns.distplot(data.bounce_time)\nplt.show()","d562be08":"# plot the distribution of the ages\nsns.distplot(data.age, kde=False)\nplt.show()","83bf4abd":"# lets use the scale function from the preprocess package within sklearn\nfrom sklearn import preprocessing\ndata[\"age_scaled\"] = preprocessing.scale(data.age.values)","a06da5b1":"# plot the distribution\nsns.distplot(data.age_scaled, kde=False)\nplt.show()","37aae603":"# let's use the lmplot function within seaborn\nsns.lmplot(x = \"age\", y = \"bounce_time\", data = data)","a1ce5c59":"from sklearn.linear_model import LinearRegression\n\n# construct our linear regression model\nmodel = LinearRegression(fit_intercept=True)\nx = data.age_scaled\ny = data.bounce_time\n\n# fit our model to the data\nmodel.fit(x[:, np.newaxis], y)\n\n# and let's plot what this relationship looks like \nxfit = np.linspace(-3, 3, 1000)\nyfit = model.predict(xfit[:, np.newaxis])\nplt.scatter(x, y)\nplt.plot(xfit, yfit);","90c1b4f8":"print(\"Model slope:    \", model.coef_[0])\nprint(\"Model intercept:\", model.intercept_)","d6101be0":"# and let's store the rmse\ny_predict = model.predict(x.values.reshape(-1,1))\nRMSE = sqrt(((y-y_predict)**2).values.mean())\n\nresults = pd.DataFrame()\nresults[\"Method\"] = [\"Linear Regression\"]\nresults[\"RMSE\"] = RMSE\nresults","ab56d4b1":"#!conda install -c districtdatalabs yellowbrick\nimport yellowbrick\nfrom sklearn.linear_model import Ridge\nfrom yellowbrick.regressor import ResidualsPlot\n\n# Instantiate the linear model and visualizer\nvisualizer = ResidualsPlot(model = model)\n\nvisualizer.fit(x[:, np.newaxis], y)  # Fit the training data to the model\nvisualizer.poof()                    # Draw\/show\/poof the data","8b42f12e":"ax = sns.residplot(x = \"age_scaled\", y= \"bounce_time\", data = data, lowess = True)\nax.set(ylabel='Observed - Prediction')\nplt.show()","dbbb60a1":"sns.catplot(x=\"county\", y=\"bounce_time\", data=data, kind = \"swarm\")","262c9021":"# let's use the lmplot function within seaborn\ngrid = sns.lmplot(x = \"age_scaled\", y = \"bounce_time\", col = \"county\", sharex=False, col_wrap = 4, data = data, height=4)","7d5e9323":"sns.catplot(x=\"location\", y=\"bounce_time\", col=\"county\", col_wrap=4, sharey=False, data=data, kind = \"swarm\")","1c7bbf1c":"# make a new data frame with one hot encoded columns for the counties\ncounties = data.county.unique()\ndata_new = pd.concat([data,pd.get_dummies(data.county)],axis=1)\ndata_new.head()","bff283e0":"# construct our linear regression model\nmodel = LinearRegression(fit_intercept=True)\nx = data_new.loc[:,np.concatenate(([\"age_scaled\"],counties))]\ny = data.bounce_time\n\n# fit our model to the data\nmodel.fit(x, y)\n\n# and let's plot what this relationship looks like \nvisualizer = ResidualsPlot(model = model)\nvisualizer.fit(x, y)  # Fit the training data to the model\nvisualizer.poof()     ","c8803ddb":"# and let's plot the predictions\nperformance = pd.DataFrame()\nperformance[\"residuals\"] = model.predict(x) - data.bounce_time\nperformance[\"age_scaled\"] = data.age_scaled\nperformance[\"predicted\"] = model.predict(x)\n\nax = sns.residplot(x = \"age_scaled\", y = \"residuals\", data = performance, lowess=True)\nax.set(ylabel='Observed - Prediction')\nplt.show()","1b15ae8c":"data_new[\"y_predict\"] = model.predict(x)\ngrid = sns.lmplot(x = \"age_scaled\", y = \"y_predict\", col = \"county\", sharey=False, col_wrap = 4, data = data_new, height=4)\ngrid.set(xlim=(-3,3))","a1ad9c36":"# and let's store the rmse\ny_predict = model.predict(x)\nRMSE = sqrt(((y-y_predict)**2).values.mean())\nresults.loc[1] = [\"Fixed\", RMSE]\nresults","2c816d7d":"# coefficient for age and the counties\npd.DataFrame.from_records(list(zip(np.concatenate(([\"age_scaled\"],counties)), model.coef_)))","06638cb3":"#!conda install -c conda-forge statsmodels -y\nimport statsmodels.api as sm\nimport statsmodels.formula.api as smf\n\n# construct our model, with our county now shown as a group\nmd = smf.mixedlm(\"bounce_time ~ age_scaled\", data, groups=data[\"county\"])\nmdf = md.fit()\nprint(mdf.summary())","25cc75d8":"# and let's plot the predictions\nperformance = pd.DataFrame()\nperformance[\"residuals\"] = mdf.resid.values\nperformance[\"age_scaled\"] = data.age_scaled\nperformance[\"predicted\"] = mdf.fittedvalues\n\nsns.lmplot(x = \"predicted\", y = \"residuals\", data = performance)","929016d8":"ax = sns.residplot(x = \"age_scaled\", y = \"residuals\", data = performance, lowess=True)\nax.set(ylabel='Observed - Prediction')\nplt.show()","395ac680":"# and let's store the rmse\ny_predict = mdf.fittedvalues\nRMSE = sqrt(((y-y_predict)**2).values.mean())\nresults.loc[2] = [\"Mixed\", RMSE]\nresults","ac5f72ea":"# construct our model, but this time we will have a random interecept AND a random slope with respect to age\nmd = smf.mixedlm(\"bounce_time ~ age_scaled\", data, groups=data[\"county\"], re_formula=\"~age_scaled\")\nmdf = md.fit()\nprint(mdf.summary())","f4d1cf7a":"# and let's plot the predictions\nperformance = pd.DataFrame()\nperformance[\"residuals\"] = mdf.resid.values\nperformance[\"age_scaled\"] = data.age_scaled\nperformance[\"predicted\"] = mdf.fittedvalues\n\nsns.lmplot(x = \"predicted\", y = \"residuals\", data = performance)","174b1245":"ax = sns.residplot(x = \"age_scaled\", y = \"residuals\", data = performance, lowess=True)\nax.set(ylabel='Observed - Prediction')\nplt.show()","d57c1654":"# and let's store the rmse\ny_predict = mdf.fittedvalues\nRMSE = sqrt(((y-y_predict)**2).values.mean())\nresults.loc[3] = [\"Mixed_Random_Slopes\", RMSE]\nresults","f73efaed":"from scipy import stats\n\n# construct our linear regression model\nlm = LinearRegression(fit_intercept=True)\nx = data.age\ny = data.bounce_time\n\n# fit our model to the data\nlm.fit(x[:, np.newaxis], y)\n\n# let's get our fitted parameters for the intercept and coefficient and what our predictions are\nparams = np.append(lm.intercept_,lm.coef_)\npredictions = lm.predict(x.values.reshape(-1, 1))\n\n# and let's simulate some new data for the model and then compare what the error is for these \nnewx = pd.DataFrame({\"Constant\":np.ones(len(x))}).join(pd.DataFrame(x))\nMSE = (sum((y-predictions)**2))\/(len(newx)-len(newx.columns))\n\n# and whats the variance, standard deviation, t values and p-values\nvar_b = MSE*(np.linalg.inv(np.dot(newx.T,newx)).diagonal())\nsd_b = np.sqrt(var_b)\nts_b = params\/ sd_b\np_values =[2*(1-stats.t.cdf(np.abs(i),(len(newx)-1))) for i in ts_b]\n\n# and let's group it together\nnames = [\"intercept\", \"age\"]\nsummary = pd.DataFrame()\nsummary[\"names\"],summary[\"Coefficients\"],summary[\"Standard Errors\"] = [names,params,sd_b]\nsummary[\"t values\"],summary[\"Probabilites\"] = [ts_b,p_values]\nprint(summary)","073973d7":"from sklearn.metrics import mean_squared_error\nfrom math import sqrt\n\nmodel = LinearRegression(fit_intercept=True)\nrms = np.empty(counties.size)\nmse = np.empty(counties.size)\n\nfor i in range(counties.size):\n    county = counties[i]\n    x = data.age[data.county == county].values.reshape(-1,1)\n    y = data.bounce_time[data.county == county]\n    fit = model.fit(x, y)\n    predict = model.predict(x)\n    mse[i] = mean_squared_error(y, predict)\n    rms[i] = sqrt(mse[i])\n\nsqrt(mse.sum())\n","648bc510":"# construct our model, with our county now shown as a group\ndata[\"location_county\"] = data[\"location\"] + \"_\" + data[\"county\"]\ndata.head()\n\nmd = smf.mixedlm(\"bounce_time ~ age_scaled\", data, groups=data[\"location_county\"], re_formula=\"~age_scaled\")\nmdf = md.fit()\nprint(mdf.summary())","f6595713":"# and let's store the rmse\ny_predict = mdf.fittedvalues\nRMSE = sqrt(((y-y_predict)**2).mean())\nresults.loc[3] = [\"Nested_Mixed\", RMSE]\nresults","ce43bf06":"#### 2. Calculate the RMSE for the individual linear regressions\n\nIn section 3 we constructed mutliple linear regression models on our data. But how did they perform? Try and estimate the RMSE for each model.","9e4b7e1d":"### 1. Interacting with the data","a9576938":"... or if you fancy there is some more material for those interested.","c5c51da8":"Yup we can see that the probability that the intercept and age are not equal to 0 is is 0 (in fact it's <2e-16 but hey ho).","a991ab08":"### 3a. Separate Linear Regression","826d090f":"---\n\n### 2. Linear Regression","c769959f":"Before we carry on it is often a good idea to **scale** our independent (explanatory) variables so that they are **standardised**. This is useful as it means that any estimated coefficient from our regression model later on are all on the same scale. So, for our dataset this would be the age, and let's create a new variable called **age_scaled**, which is the age scaled to have zero mean and unit variance:","1ec8f9f1":"___\n### 3b. Modelling county as a fixed effect\n\nOne way to incorporate the impact of county is to bring the county in to our equation for the linear model by treating it as a fixed effect. This would look like: \n\n$$\ntime_{c_{i}} = b_{0} + b_{1}age + c_i\n$$\n\ni.e. each county has it's own additional term that wil change the intercept. To model this we will need to alter our dataframe to encode our counties as numeric variables. So we will use **one-hot encoding**, where we will make a new column for each county:","6b08fbc9":".. just kidding ...\n\nbut you have now covered loads of material and can now have a go at using a mixed effect model. \n\n---","3e0bcf44":"---\n\n### 5. Wrap Up\n\nWell done for getting this far! You know have an understanding of what a mixed effect model is, and how it alters the coefficients within regression models so you account for variation stemming from random variables. Reward yourselves with pizza!!!\n\n![](https:\/\/i.imgflip.com\/2gu0c8.jpg)","93303354":"#### 3. Add in locations for the mixed effect model\n\nIn section 4 we constructed our mixed effect model, treating county as a random effect. However, what about location? This will really improve our predictions.\n\n**Hint: You will need to create a new variable that considers how the random effects are nested. For example there is nothing important about the locations a, b, and c that link location a in London with that in Essex. Therefore explicitly nest them**","c91e2e51":"##### Simple Linear Regression\n\nWe will start with the most familiar linear regression, a straight-line fit to data.\nA straight-line fit is a model of the form\n\n$$\ny = b_0 + b_1x \n$$\n\nwhere $b_1$ is commonly known as the *gradient*, and $b_0$ is commonly known as the *intercept*. So for our dataset, `y` would be the bounce time, `x` their age, `b_1` the rate of change of the bounce time with respect to `age` and `b_0` the intercept when `age` is 0.\n\n$$\ntime = b_0 + b_1age\n$$","a9ea1f76":"But what model have we actually fitted here? Is it what we wanted to investigate initially? The  model is estimating the difference in bounce times between the counties now as well, and we aren't actually interested in that. If you were then this is correct to do so, but the website company just wanted to know whether age affects the bounce times. And by including county in we are obscuring this. So to look at the impact of age on bounce times we need to control for the variation between the different counties (as well as between the locations). So to do that, we have to treat our counties as **random effects**, and build a **mixed effect model**!\n\nHurray for getting here! \n\n![](https:\/\/memegenerator.net\/img\/instances\/57364565\/hurray-time-to-close-your-laptops.jpg)","f7cca570":"The first thing we might want to do to see if the bounce time is dependent on the age is to plot this data, and then fit a linear regression to the data:","3c1ca79a":"Now we can se that the coefficient for the gradient given to age is substnantially smaller, and is likely no longer significant. Let's just check that by making a table of the coefficients:","42dfb22a":"### Take home challenges","c4e8d5cd":"Okay, so perhaps its not ideal...\n\nLet's check some of the other assumptions. For a good list of the assumptions (with code in R), have a look here http:\/\/r-statistics.co\/Assumptions-of-Linear-Regression.html. \n\nOne of the other key assumption is that the observations of our data are independent of the other data. When we collected our data we were doing it in 8 different counties and in 3 locations within each county. So we could check this by comparing the bounce times for each county. To do this let's use one of the categorical plots in seaborn:","b3a9ca2e":"### Dataset\n\nToday we will be using a dataset that looks at the bounce rates of users of a website with cooking recipes. A bounce rate is a measure of how quickly someone leaves a website, e.g. the number of seconds after which a user first accesses a webpage from the website and then leaves. Most websites want individuals to stay on their websites for a long time as they are more likely to read another article, buy one of their products, click on some of the sponsored links etc. As such, it can be useful to understand why some users leave the website quicker than others. \n\nTo investigate the bounce rate of the website, we chose three locations in 8 counties in England, and got members of the public of all ages to undertake a survey\/test questionaire. In the test we asked them to use our search engine to query for something they want to eat this evening. The search engine listed our website first, as well as other websites that would be returned. The users that clicked on our website were then timed and their bounce time recorded. We also recorded their age, and made a note of the county and location.\n\nWe have been asked by the website to work out if younger individuals are more likely to leave the website quicker.","8280a0d4":"#### Fitting our mixed effect lm\n\nWe will now fit our model using the `statsmodels` library. In this initial model we will look at how the bounce time relates to the scaled ages, while controlling for the impact of counties by allowing for a random intercept for each country, i.e. we are saying that each county has its own random intercept but that the slopes are still the same with respect to age.","ef3fb441":"#### 4. glm mixed effect for logistic\n\nWait, there's more data! Yup. In the other dataset, \"data2.csv\", you will know whether the individual chose a recipe from the website. So that's then a binary outcome. So wouldn't it be nice to fit a logistic regression, with the mixed effects? \n\nYes it would! Fortunately, there is also a `statsmodels` for that that allows you to construct generalised linear mixed models, or GLMMs, so you can then specify what the relationship is between your data and the response (logistic). \n\nhttps:\/\/www.statsmodels.org\/stable\/generated\/statsmodels.genmod.bayes_mixed_glm.BinomialBayesMixedGLM.html#statsmodels.genmod.bayes_mixed_glm.BinomialBayesMixedGLM\n\nAnd if you want an example of how it's implemented, and some alternative ways of doing GLMMs, check out this great repo:\n\nhttps:\/\/github.com\/junpenglao\/GLMM-in-Python","3a56888e":"#### Fixed vs random?\n\nSo what makes a variable a fixed or random effect. Hmmm, it's tricky and there are lots of answers out there. In brief, we view fixed effects as the variables that we are interested in. We wanted to know about age so we recorded data on that and wanted to see how it impacted the response variable. County was not we were interested in, but we recorded it as we were aware that our sampling methodology could lead to clustering in our data that could invalidate the linear model from before. If we hadn't recorded that someone who was given this dataset to analyse may have incorrectly said that  age was an important predictor of bounce rate.\n\nRandom effects are often our groups we are trying to control for, like county in our example. In particualr twe control for counties when we have not exhausted all the available groups - we only had 8 counties in England. If we wanted to make predicitons about the counties, then we would have tried to sample them better firstly, and also treat them as a fixed effect. \n\nFurther reading for those who want a better answer!:\n\nhttps:\/\/dynamicecology.wordpress.com\/2015\/11\/04\/is-it-a-fixed-or-random-effect\/\n\nAnd see the discussion in the paper linked in the take home challenge 3. ","3b583e1c":"---\n\n\nSo our mixed model with the random slopes is now performing much better, with our residuals much better ditributed. Crucially though, we can see that age does not impact the bounce rate, with the confidence intervals for the gradient with respect to age spanning -2.184 - 2.505 after we have controlled for the random variation caused by the county properly, i.e. with a random slope and intercept. \n\n```\n--------------------------------------------------------------------------\n                              Coef.  Std.Err.   z    P>|z|  [0.025  0.975]\n--------------------------------------------------------------------------\nIntercept                    202.140    8.356 24.190 0.000 185.762 218.518\nage_scaled                     0.161    1.196  0.134 0.893  -2.184   2.505\nIntercept RE                 558.143                              \nIntercept RE x age_scaled RE -51.614                                 \nage_scaled RE                  8.621                                      \n==========================================================================\n```\n\nWhat we have seen in our data was that individuals in certain counties took longer on the website, and that they happened to also be old. However, to get to this distinction we had to first treat county as a random effect. But what about location? Eh? Forgot about that. See the take home material 4. for that one.","2c9a7437":"Huh, that's strange - it's very similar. In fact the residuals plot looks alomst identical to the previous one where we were treating the county as a fixed effect. Why? \n\nWell, have a think about what we have actually just implemented. All we have done is say in the last one that the county has a randome intercept, but the same slope. This is very similar to the fixed effect approach, where county was included as a term that would impact the intercept. All we have changed is sayng that the intercepts for each county are probably drawn from a similar distribution. To ensure that each county has its own random slope we need to include this in our random effects forumla, like so:","529582cd":"data_new$Time <- rep(c(\"Baseline\",\"Red Meat W8\",\"Plant W8\"),each=3185)\n","8e133e31":"Actually not toooooo bad - you could say that there are more positive residuals than negative residuals at the highest and lowest predicted value ranges, e.g. predicted value < 195, and >210 there are more positive residuals than negative. You can have a look at this also using seaborn's `residplot`:","f51cd11c":"The residuals are much better than before, being more evenly distributed with respect to age, and if we look at the predictions with each county:","35f4fdcc":"#### 5. Read on the strengths of a mutlievel (mixed-effects) modelling vs random forests.\n\nStill here?? Really, well here is some reading by the godfather himself, Andrew Gelman, detailing what multilevel models can do:\n\nhttp:\/\/www.stat.columbia.edu\/~gelman\/research\/published\/multi2.pdf\n\nAnd for those who want to know how this stacks up against random forests:\n\nhttps:\/\/epub.ub.uni-muenchen.de\/39955\/1\/TR.pdf","e5f9b8ff":"#### Acknowledgements: \n\nFirstly, it's right to pay thanks to the blogs and sources I have used in writing this tutorial. A similar tutoral in R formed much of the framework by [Gabriela K Hajduk](https:\/\/twitter.com\/AmidstScience), [Liam Bailey](https:\/\/twitter.com\/ldbailey255) and the brilliant https:\/\/ourcodingclub.github.io\/ - if you are an R developer check it out. Also big thanks to Junpeng Lao (https:\/\/github.com\/junpenglao) for detailing the many different ways you can implement mixed-effect models in python (as there is always more than one), so look in the extension material in the end for links to one of his notebooks.","970500b8":"\n### 4. Mixed effects models\n\nAs we have discussed above, a mixed effects model is ideal here as it will allow us to both use all the data we have (higher sample size) and better acknowledge the correlations between data coming from the counties and locations. We will also estimate fewer parameters and avoid problems with multiple comparisons that we would encounter while using separate regressions.\n\nSo in this model we treat our `age`, which is what we are interested in, as a **fixed effect**, and `county` and `location` as a **random effect**.\n\nBut what does that mean? Well it's sort of a middle ground here between assuming our coefficient for the gradient and intercept are all the same (what we did in the first regression) and assuming they are all independent and different. If we look back at our equations, we now may assume that while $b_0$ and $b_1$ are different for each county, the coefficients all come from a common group distribution, such as a normal distribution:\n\n$$\n\\begin{align*}\ntime_c = b_{c0} + b_{c1}.age  \\\\ \\\\\nb_{c0} \\sim N(\\mu_{b_0}, \\sigma^2_{b_0}) \\\\\nb_{c1} \\sim N(\\mu_{b_1}, \\sigma^2_{b_1})\n\\end{align*}\n$$\n\nSo we now assume the intercepts $b_0$ and gradients $b_1$  come from a normal distribution centered around their respective group mean $\\mu$ with a certain standard deviation $\\sigma^2$, the values of which we also estimate.","cb94b4fd":"We can use Scikit-Learn's ``LinearRegression`` estimator to fit this data and construct the best-fit line:","3f30e17a":"This scaling won't impact the statistical findings in our example but you may find in more complex models than the one in this tutorial this can be very helpful in speeding up your models fitting. It is also makes comparison between a continous variable like age, and a binary variable (0, 1) more fair. Thanks to a couple of the s2ds people from last night for asking about this!","304045ef":"So from this we would say quite confidently that as age increases so does the bounce time. (To test if this increase is significant see the take home challenge 1.)\n\nHowever, to run a linear regression a number of assumptions about the data need to be met. One of these is that the residuals are **homoscedastic**, which means that the residuals are normally distributed in relation to the predicted value, i.e. are our predictions equally bad (or good) across our predicted values. We can look at this by plotting the residuals, using my new favourite package yellowbrick:\n","789d21a8":"In this summary of the model, we can clearly see that the `age_scaled` is having a more noticable impact than in the fixed model earlier (coefficient of 0.136 rather than 0.048), however, importantly it is still not significantly different to 0, with the 95% interval for this coefficient spanning -1.065 - 1.336.\n\nAnd let's look at the predictions.","4193857b":"#### 1. Test the initial linear regression was significant\n\nIn section 2 we constructed a linear regression model on our data. It looked as if the coefficient for the gradient was significant, i.e. there was in fact a positive relationship between age and bounce time. However, to be sure we could use a statistical test, such as the **t.test** to test this. This can be often useful and can be used to produce **p.values** which gives us a measure of how confident we are about something. In this we will test whether the intercept and mean are significantly different from 0.","35efb993":"## Mixed models","4b8962d9":"It would seem, from this simple analysis that older people do spend longer time on the website. To go into this further let's look at what the coefficients estimated are within our linear model. We looked at similar things n a previous meetup on generalised linear models and at lasso and ridge regression (https:\/\/www.meetup.com\/central_london_data_science\/events\/249149833\/), so without repeating too much of the material from that meetup (github here - https:\/\/github.com\/central-ldn-data-sci\/regression2000), here is a quick overview again of what a linear regression model is","3d3f10df":"Same as before, we are going to fit a linear regression to the data, but first separating it by county. Mathematically, this now looks like:\n\n$$\ntime_c = b_{c0} + b_{c1}age\n$$\n\nwhere the subscript *c* here represents the county. As a result we will be estimating 8 different intercept and 8 different gradients, one for each county. Let's have a look at that using the facet grid options within `sns.lmplot`.","d7556365":"We can have a look at these parameters within the model object, with the relevant parameters being ``coef_`` and ``intercept_``:","13e5622a":"#### 6. Compare performance to random forest\n\nSeriously? Take a break...\n\n... or how about comparing the performance of the mixed-effect models against a random forest for this dataset?\n\n... nah just have pizza. ","cad89bcd":"Ah... clearly there is substantial grouping - us Londoners seem to have short attention spans (maybe...). So we can definitely say that our data is not independent, and thus it is inappropriate to use a linear model for this data. \n\nWhat next? Well maybe we could do a separate regression for each county.\n\n---","6069d9a1":"So now we have 8 different analyses. This is fine, but we have started to reduce our sample size a lot already as a result, and we are now perhaps going too far in the other direction from before. Before we were saying that all counties were identical, whereas here we are saying that the impact of `age` on `bounce_time` share no similarities between counties, which is probably again not true.\n\nIn addition, what about our location variable, maybe our data is also not independent when looking at location. Let's have a look at this too, using a swarm plot:","8c475e52":"So we could carry on and then do an individual regression for each location within each county... hopefully you can see why we can't always just do an individual regression for each new group. If we did we would have to to estimate a slope and intercept parameter for each regression. That\u2019s two parameters, three locations and eight counties, which means 48 parameter estimates (2 x 3 x 8 = 48). \n\nAlso we would now be taking our nice dataset of 480 observations, that presumably took a long time to collect, and effectively reducing it to lots of sample sizes of 20. This really decreases our statistical power, and thus also increases our chances of a Type I Error (where you falsely reject the null hypothesis) by carrying out multiple comparisons.\n\nSo what could we do? Well we could modify the model to account for the different counties, and add it to our linear model."}}