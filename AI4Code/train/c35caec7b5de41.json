{"cell_type":{"d332a8b0":"code","19d158ab":"code","8661631f":"code","224e5caa":"code","11ec96b5":"code","22539891":"code","56b6b941":"code","09650cb1":"code","5da004f8":"code","2cc31b9e":"code","9c5cfda4":"code","7897f119":"code","a8ba9267":"code","e83a3798":"code","28cf631e":"code","d4f928eb":"code","832d660f":"code","99b5eabf":"code","15d41587":"code","95548ac7":"code","d5ab4332":"code","1aed38a9":"code","45963826":"code","e85c84af":"code","4208fe06":"code","a9eff424":"code","8df56f4a":"code","45c7c34c":"code","3b76a92a":"code","31175f4d":"markdown","a3b6a960":"markdown","1f3ad928":"markdown","122a81ee":"markdown","d33fe6c0":"markdown","15e06f6b":"markdown","52a06347":"markdown","3be02fe0":"markdown","0bee7e23":"markdown","1237b1a5":"markdown","09e50b3a":"markdown","9fb2dc49":"markdown","94cd5afe":"markdown","c15db5f7":"markdown","91d76f8e":"markdown"},"source":{"d332a8b0":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","19d158ab":"%matplotlib inline\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport pandas as pd\nimport warnings\nimport sklearn.exceptions\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split\nimport xgboost as xgb\nfrom xgboost import XGBRegressor\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.model_selection import RandomizedSearchCV\nfrom sklearn.feature_selection import RFE\nfrom sklearn.pipeline import Pipeline\nwarnings.filterwarnings('ignore', category=DeprecationWarning)\nwarnings.filterwarnings('ignore', category=FutureWarning)\nwarnings.filterwarnings('ignore', category=RuntimeWarning)\nwarnings.filterwarnings('ignore', category=UserWarning)\nwarnings.filterwarnings(\"ignore\", category=sklearn.exceptions.UndefinedMetricWarning)","8661631f":"train = pd.read_csv('\/kaggle\/input\/tabular-playground-series-aug-2021\/train.csv')\ntest = pd.read_csv('\/kaggle\/input\/tabular-playground-series-aug-2021\/test.csv')","224e5caa":"print(f'Train shape: {train.shape}')\nprint(f'Test shape: {test.shape}')","11ec96b5":"train.info()","22539891":"train.head()","56b6b941":"train.drop(['id'], axis=1, inplace=True)","09650cb1":"train['loss'].describe()","5da004f8":"fig = plt.figure(figsize=(14,6))\ntarget_cnt = train['loss'].value_counts().sort_index()\nsns.barplot(x=target_cnt.index, y=target_cnt)","2cc31b9e":"train.drop('loss', axis=1).describe()","9c5cfda4":"target = train['loss']\ntrain_features = train.drop('loss', axis=1)","7897f119":"train_features.shape","a8ba9267":"X_train, X_test, y_train, y_test = train_test_split(train_features, target, test_size=0.2, random_state=42)","e83a3798":"ss = StandardScaler()\nX_train_ss = ss.fit_transform(X_train)\nX_test_ss = ss.transform(X_test)","28cf631e":"# transform test data\ntest_ss = ss.transform(test.drop(['id'], axis=1))","d4f928eb":"fig = plt.figure(figsize=(25,25))\n\ntrain_corr = train_features.corr()\ntrain_mask = np.triu(np.ones_like(train_corr, dtype=bool))\n\nsns.heatmap(train_corr, \n            square=True, \n            linewidth=0.2,\n            mask=train_mask,\n            annot=False,\n            center=0,\n            cmap=sns.diverging_palette(240, 10),\n           )","832d660f":"param_dist = {\n    'objective':'reg:squarederror',\n    'n_estimators':5,\n    'seed': 123,\n    \n}\n\nxgb_rr = XGBRegressor(**param_dist)\n\nxgb_rr.fit(X_train_ss, y_train, verbose=True)","99b5eabf":"y_preds = xgb_rr.predict(X_test_ss)","15d41587":"mean_squared_error(y_test, y_preds, squared=False)","95548ac7":"fig = plt.figure(figsize=(24,34))\nax = plt.axes()\nxgb.plot_importance(xgb_rr, ax)","d5ab4332":"params = {\n    'n_estimators': [2, 3, 5, 10, 25, 50, 100, 150, 200],\n    'max_depth':np.arange(3,12),\n    'min_child_weight': np.arange(1,12),\n    'eta':[.3, .2, .1, .05, .01, .005],\n    'subsample': [0.3, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0],\n    'colsample_bytree': [0.3, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0],\n}","1aed38a9":"xgb_rr = XGBRegressor(objective='reg:squarederror', tree_method='gpu_hist', gpu_id=0, predictor= \"gpu_predictor\")\nrandom_rmse = RandomizedSearchCV(xgb_rr, param_distributions=params, n_iter=20,\n                                 verbose=2, cv=3,\n                                 scoring='neg_root_mean_squared_error', random_state=123)","45963826":"random_rmse.fit(X_train_ss, y_train)","e85c84af":"-random_rmse.best_score_","4208fe06":"random_rmse.best_params_","a9eff424":"param_dist = {\n    'objective':'reg:squarederror',\n    'n_estimators':50,\n    'seed': 123,\n    'min_child_weight': 6,\n    'subsample': 0.8,\n    'max_depth': 3,\n    'eta': 0.2,\n    'colsample_bytree': 0.7\n    \n}\n\nxgb_rr = XGBRegressor(**param_dist)","8df56f4a":"rfe = RFE(xgb_rr, 75)\nrfe = rfe.fit(X_train_ss, y_train)","45c7c34c":"predictions = rfe.predict(test_ss)","3b76a92a":"submission = pd.DataFrame({\n    'id': np.asarray(test.id), \n    'loss': predictions.astype(int)\n})\nsubmission.to_csv('my_submission.csv', index=False)","31175f4d":"# Submission","a3b6a960":"# Training: XGBoost","1f3ad928":"# Feature Importance\n","122a81ee":"## Scaling Data","d33fe6c0":"**We can see that there are 43 unique values in the target variable and out of all these values 0 (categorical value) is in considerable amount**","15e06f6b":"**95 columns have float64 dtype whereas 7 columns have int64 dtypes**","52a06347":"## Splitting our data into train and test set","3be02fe0":"# Hyperparameter Tuning","0bee7e23":"# Target Column","1237b1a5":"# Feature Elimination","09e50b3a":"The above parameters are best for our xgboost model and we can now see that after hyperparameter optimization we have dropped down our RMSE value from 7.95 to 7.89","9fb2dc49":"**The data shows almost no correlation among themselves as all of them are in pinky shade**","94cd5afe":"# Testing on Test data","c15db5f7":"**We can see that there is a huge difference in the values of almost every feature hence we will have to scale down our data**","91d76f8e":"# Correlation Matrix"}}