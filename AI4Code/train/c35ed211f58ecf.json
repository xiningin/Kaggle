{"cell_type":{"62ac07cd":"code","f89b6361":"code","1a49343d":"code","7106d42b":"code","ac8db1d3":"code","900f1063":"code","f4a58ac4":"code","408d8b91":"code","dc364b35":"code","57933a4d":"code","7e93103e":"code","8db35af4":"code","b531bc0a":"code","f6ade878":"code","16bfeed8":"code","7b650034":"markdown","8063bae9":"markdown","31a44100":"markdown","d5574a72":"markdown","5c05a84d":"markdown","7a6a0074":"markdown","f11aa715":"markdown","c416f438":"markdown","354ff740":"markdown","b08deca5":"markdown","898dbc1f":"markdown"},"source":{"62ac07cd":"!pip install stable-baselines3 kaggle-environments > \/dev\/null 2>&1","f89b6361":"%%writefile greedy-goose.py\nfrom kaggle_environments.envs.hungry_geese.hungry_geese import Observation, Configuration, Action, row_col, translate, adjacent_positions, min_distance\nimport random as rand\nfrom enum import Enum, auto\n\n\ndef opposite(action):\n    if action == Action.NORTH:\n        return Action.SOUTH\n    if action == Action.SOUTH:\n        return Action.NORTH\n    if action == Action.EAST:\n        return Action.WEST\n    if action == Action.WEST:\n        return Action.EAST\n    raise TypeError(str(action) + \" is not a valid Action.\")\n\n    \n\n#Enconding of cell content to build states from observations\nclass CellState(Enum):\n    EMPTY = 0\n    FOOD = auto()\n    GOOSE = auto()\n\n\n#This class encapsulates mos of the low level Hugry Geese stuff    \n#This class encapsulates mos of the low level Hugry Geese stuff    \nclass BornToNotMedalv2:    \n    def __init__(self):\n        self.DEBUG=False\n        self.rows, self.columns = -1, -1        \n        self.my_index = -1\n        self.my_head, self.my_tail = -1, -1\n        self.geese = []\n        self.heads = []\n        self.tails = []\n        self.food = []\n        self.cell_states = []\n        self.actions = [action for action in Action]\n        self.previous_action = None\n        self.step = 1\n\n        \n    def _adjacent_positions(self, position):\n        return adjacent_positions(position, self.columns, self.rows)\n \n\n    def _min_distance_to_food(self, position, food=None):\n        food = food if food!=None else self.food\n        return min_distance(position, food, self.columns)\n\n    \n    def _row_col(self, position):\n        return row_col(position, self.columns)\n    \n    \n    def _translate(self, position, direction):\n        return translate(position, direction, self.columns, self.rows)\n        \n        \n    def preprocess_env(self, observation, configuration):\n        observation = Observation(observation)\n        configuration = Configuration(configuration)\n        \n        self.rows, self.columns = configuration.rows, configuration.columns        \n        self.my_index = observation.index\n        self.hunger_rate = configuration.hunger_rate\n        self.min_food = configuration.min_food\n\n        self.my_head, self.my_tail = observation.geese[self.my_index][0], observation.geese[self.my_index][-1]        \n        self.my_body = [pos for pos in observation.geese[self.my_index][1:-1]]\n\n        \n        self.geese = [g for i,g in enumerate(observation.geese) if i!=self.my_index  and len(g) > 0]\n        self.geese_cells = [pos for g in self.geese for pos in g if len(g) > 0]\n        \n        self.occupied = [p for p in self.geese_cells]\n        self.occupied.extend([p for p in observation.geese[self.my_index]])\n        \n        \n        self.heads = [g[0] for i,g in enumerate(observation.geese) if i!=self.my_index and len(g) > 0]\n        self.bodies = [pos  for i,g in enumerate(observation.geese) for pos in g[1:-1] if i!=self.my_index and len(g) > 2]\n        self.tails = [g[-1] for i,g in enumerate(observation.geese) if i!=self.my_index and len(g) > 1]\n        self.food = [f for f in observation.food]\n        \n        self.adjacent_to_heads = [pos for head in self.heads for pos in self._adjacent_positions(head)]\n        self.adjacent_to_bodies = [pos for body in self.bodies for pos in self._adjacent_positions(body)]\n        self.adjacent_to_tails = [pos for tail in self.tails for pos in self._adjacent_positions(tail)]\n        self.adjacent_to_geese = self.adjacent_to_heads + self.adjacent_to_bodies\n        self.danger_zone = self.adjacent_to_geese\n        \n        #Cell occupation\n        self.cell_states = [CellState.EMPTY.value for _ in range(self.rows*self.columns)]\n        for g in self.geese:\n            for pos in g:\n                self.cell_states[pos] = CellState.GOOSE.value\n        for pos in self.heads:\n                self.cell_states[pos] = CellState.GOOSE.value\n        for pos in self.my_body:\n            self.cell_states[pos] = CellState.GOOSE.value\n                \n        #detect dead-ends\n        self.dead_ends = []\n        for pos_i,_ in enumerate(self.cell_states):\n            if self.cell_states[pos_i] != CellState.EMPTY.value:\n                continue\n            adjacent = self._adjacent_positions(pos_i)\n            adjacent_states = [self.cell_states[adj_pos] for adj_pos in adjacent if adj_pos!=self.my_head]\n            num_blocked = sum(adjacent_states)\n            if num_blocked>=(CellState.GOOSE.value*3):\n                self.dead_ends.append(pos_i)\n        \n        #check for extended dead-ends\n        new_dead_ends = [pos for pos in self.dead_ends]\n        while new_dead_ends!=[]:\n            for pos in new_dead_ends:\n                self.cell_states[pos]=CellState.GOOSE.value\n                self.dead_ends.append(pos)\n            \n            new_dead_ends = []\n            for pos_i,_ in enumerate(self.cell_states):\n                if self.cell_states[pos_i] != CellState.EMPTY.value:\n                    continue\n                adjacent = self._adjacent_positions(pos_i)\n                adjacent_states = [self.cell_states[adj_pos] for adj_pos in adjacent if adj_pos!=self.my_head]\n                num_blocked = sum(adjacent_states)\n                if num_blocked>=(CellState.GOOSE.value*3):\n                    new_dead_ends.append(pos_i)                                    \n        \n                \n    def strategy_random(self, observation, configuration):\n        if self.previous_action!=None:\n            action = rand.choice([action for action in Action if action!=opposite(self.previous_action)])\n        else:\n            action = rand.choice([action for action in Action])\n        self.previous_action = action\n        return action.name\n                        \n                        \n    def safe_position(self, future_position):\n        return (future_position not in self.occupied) and (future_position not in self.adjacent_to_heads) and (future_position not in self.dead_ends)\n    \n    \n    def valid_position(self, future_position):\n        return (future_position not in self.occupied) and (future_position not in self.dead_ends)    \n\n    \n    def free_position(self, future_position):\n        return (future_position not in self.occupied) \n    \n                        \n    def strategy_random_avoid_collision(self, observation, configuration):\n        dead_end_cell = False\n        free_cell = True\n        actions = [action \n                   for action in Action \n                   for future_position in [self._translate(self.my_head, action)]\n                   if self.valid_position(future_position)] \n        if self.previous_action!=None:\n            actions = [action for action in actions if action!=opposite(self.previous_action)] \n        if actions==[]:\n            dead_end_cell = True\n            actions = [action \n                       for action in Action \n                       for future_position in [self._translate(self.my_head, action)]\n                       if self.free_position(future_position)]\n            if self.previous_action!=None:\n                actions = [action for action in actions if action!=opposite(self.previous_action)] \n            #no alternatives\n            if actions==[]:\n                free_cell = False\n                actions = self.actions if self.previous_action==None else [action for action in self.actions if action!=opposite(self.previous_action)] \n\n        action = rand.choice(actions)\n        self.previous_action = action\n        if self.DEBUG:\n            aux_pos = self._row_col(self._translate(self.my_head, self.previous_action))\n            dead_ends = \"\" if not dead_end_cell else f', dead_ends={[self._row_col(p1) for p1 in self.dead_ends]}, occupied={[self._row_col(p2) for p2 in self.occupied]}'\n            if free_cell:\n                print(f'{id(self)}({self.step}): Random_ac_move {action.name} to {aux_pos} dead_end={dead_end_cell}{dead_ends}', flush=True)\n            else:\n                print(f'{id(self)}({self.step}): Random_ac_move {action.name} to {aux_pos} free_cell={free_cell}', flush=True)\n        return action.name\n    \n    \n    def strategy_greedy_avoid_risk(self, observation, configuration):        \n        actions = {  \n            action: self._min_distance_to_food(future_position)\n            for action in Action \n            for future_position in [self._translate(self.my_head, action)]\n            if self.safe_position(future_position)\n        }\n  \n        if self.previous_action!=None:\n            actions.pop(opposite(self.previous_action), None)\n        if any(actions):\n            action = min(actions.items(), key=lambda x: x[1])[0]\n            self.previous_action = action\n            if self.DEBUG:\n                aux_pos = self._row_col(self._translate(self.my_head, self.previous_action))\n                print(f'{id(self)}({self.step}): Greedy_ar_move {action.name} to {aux_pos}', flush=True)\n            self.previous_action = action\n            return action.name\n        else:\n            return self.strategy_random_avoid_collision(observation, configuration)\n    \n    \n    #Redefine this method\n    def agent_strategy(self, observation, configuration):\n        action = self.strategy_greedy_avoid_risk(observation, configuration)\n        return action\n    \n    \n    def agent_do(self, observation, configuration):\n        self.preprocess_env(observation, configuration)\n        move = self.agent_strategy(observation, configuration)\n        self.step += 1\n        #if self.DEBUG:\n        #    aux_pos = self._translate(self.my_head, self.previous_action), self._row_col(self._translate(self.my_head, self.previous_action))\n        #    print(f'{id(self)}({self.step}): Move {move} to {aux_pos} internal_vars->{vars(self)}', flush=True)\n        return move\n\n    \n    \ndef agent_singleton(observation, configuration):\n    global gus    \n    \n    try:\n        gus\n    except NameError:\n        gus = BornToNotMedalv2()\n            \n    action = gus.agent_do(observation, configuration)\n\n    \n    return action","1a49343d":"import gym\nfrom gym import spaces\n\nfrom kaggle_environments.envs.hungry_geese.hungry_geese import Observation, Configuration, Action, adjacent_positions, row_col, translate, min_distance\nfrom kaggle_environments import make\n\nfrom enum import Enum, auto\nimport numpy as np\n\n\nclass CellState(Enum):\n    EMPTY = 0\n    FOOD = auto()\n    HEAD = auto()\n    BODY = auto()\n    TAIL = auto()\n    MY_HEAD = auto()\n    MY_BODY = auto()\n    MY_TAIL = auto()\n    ANY_GOOSE = auto()\n    \n\nclass ObservationProcessor:\n    \n    def __init__(self, rows, columns, hunger_rate, min_food, debug=False, center_head=True):\n        self.debug = debug\n        self.rows, self.columns = rows, columns\n        self.hunger_rate = hunger_rate\n        self.min_food = min_food\n        self.previous_action = -1\n        self.last_action = -1\n        self.last_min_distance_to_food = self.rows*self.columns #initial max value to mark no food seen so far\n        self.center_head = center_head\n\n    #***** BEGIN: utility functions ******   \n    \n    def opposite(self, action):\n        if action == Action.NORTH:\n            return Action.SOUTH\n        if action == Action.SOUTH:\n            return Action.NORTH\n        if action == Action.EAST:\n            return Action.WEST\n        if action == Action.WEST:\n            return Action.EAST\n        raise TypeError(str(action) + \" is not a valid Action.\")\n        \n        \n    def _adjacent_positions(self, position):\n        return adjacent_positions(position, self.columns, self.rows)\n\n    \n    def _min_distance_to_food(self, position, food=None):\n        food = food if food!=None else self.food\n        return min_distance(position, food, self.columns)\n    \n    \n    def _row_col(self, position):\n        return row_col(position, self.columns)\n\n    \n    def _translate(self, position, direction):\n        return translate(position, direction, self.columns, self.rows)     \n\n    \n    def _preprocess_env(self, obs):\n        observation = Observation(obs)\n        \n        self.my_index = observation.index\n\n        if len (observation.geese[self.my_index])>0:\n            self.my_head = observation.geese[self.my_index][0]\n            self.my_tail = observation.geese[self.my_index][-1]        \n            self.my_body = [pos for pos in observation.geese[self.my_index][1:-1]]\n        else:\n            self.my_head = -1\n            self.my_tail = -1\n            self.my_body = []\n\n        \n        self.geese = [g for i,g in enumerate(observation.geese) if i!=self.my_index and len(g) > 0]\n        self.geese_cells = [pos for g in self.geese for pos in g if len(g) > 0]\n        \n        self.occupied = [p for p in self.geese_cells]\n        self.occupied.extend([p for p in observation.geese[self.my_index]])\n        \n        \n        self.heads = [g[0] for i,g in enumerate(observation.geese) if i!=self.my_index and len(g) > 0]\n        self.bodies = [pos  for i,g in enumerate(observation.geese) for pos in g[1:-1] if i!=self.my_index and len(g) > 2]\n        self.tails = [g[-1] for i,g in enumerate(observation.geese) if i!=self.my_index and len(g) > 1]\n        self.food = [f for f in observation.food]\n        \n        self.adjacent_to_heads = [pos for head in self.heads for pos in self._adjacent_positions(head)]\n        self.adjacent_to_bodies = [pos for body in self.bodies for pos in self._adjacent_positions(body)]\n        self.adjacent_to_tails = [pos for tail in self.tails for pos in self._adjacent_positions(tail)]\n        self.adjacent_to_geese = self.adjacent_to_heads + self.adjacent_to_bodies\n        self.danger_zone = self.adjacent_to_geese\n        \n        #Cell occupation\n        self.cell_states = [CellState.EMPTY.value for _ in range(self.rows*self.columns)]\n        for g in self.geese:\n            for pos in g:\n                self.cell_states[pos] = CellState.ANY_GOOSE.value\n        for pos in self.heads:\n                self.cell_states[pos] = CellState.ANY_GOOSE.value\n        for pos in self.my_body:\n            self.cell_states[pos] = CellState.ANY_GOOSE.value\n        self.cell_states[self.my_tail] = CellState.ANY_GOOSE.value\n                \n        #detect dead-ends\n        self.dead_ends = []\n        for pos_i,_ in enumerate(self.cell_states):\n            if self.cell_states[pos_i] != CellState.EMPTY.value:\n                continue\n            adjacent = self._adjacent_positions(pos_i)\n            adjacent_states = [self.cell_states[adj_pos] for adj_pos in adjacent if adj_pos!=self.my_head]\n            num_blocked = sum(adjacent_states)\n            if num_blocked>=(CellState.ANY_GOOSE.value*3):\n                self.dead_ends.append(pos_i)\n        \n        #check for extended dead-ends\n        new_dead_ends = [pos for pos in self.dead_ends]\n        while new_dead_ends!=[]:\n            for pos in new_dead_ends:\n                self.cell_states[pos]=CellState.ANY_GOOSE.value\n                self.dead_ends.append(pos)\n            \n            new_dead_ends = []\n            for pos_i,_ in enumerate(self.cell_states):\n                if self.cell_states[pos_i] != CellState.EMPTY.value:\n                    continue\n                adjacent = self._adjacent_positions(pos_i)\n                adjacent_states = [self.cell_states[adj_pos] for adj_pos in adjacent if adj_pos!=self.my_head]\n                num_blocked = sum(adjacent_states)\n                if num_blocked>=(CellState.ANY_GOOSE.value*3):\n                    new_dead_ends.append(pos_i)    \n                    \n                        \n    def safe_position(self, future_position):\n        return (future_position not in self.occupied) and (future_position not in self.adjacent_to_heads) and (future_position not in self.dead_ends)\n    \n    \n    def valid_position(self, future_position):\n        return (future_position not in self.occupied) and (future_position not in self.dead_ends)    \n\n    \n    def free_position(self, future_position):\n        return (future_position not in self.occupied)  \n    \n    #***** END: utility functions ******\n    \n    \n    def process_env_obs(self, obs):\n        self._preprocess_env(obs)\n        \n        EMPTY = .4\n        HEAD = -1\n        BODY = MY_BODY = -.8\n        TAIL = MY_TAIL = -.5\n        MY_HEAD = 0\n        FOOD = 1\n        RISK = -.5\n        \n        #Example: {'remainingOverageTime': 12, 'step': 0, 'geese': [[62], [50]], 'food': [7, 71], 'index': 0}\n        #observation = [[CellState.EMPTY.value for _ in range(self.columns)] for _ in range(self.rows)]\n        observation = [[EMPTY for _ in range(self.columns)] for _ in range(self.rows)]\n        \n        #Other agents\n        for pos in self.heads:\n            r, c = self._row_col(pos)\n            observation[r][c] = HEAD #CellState.HEAD.value\n        for pos in self.bodies:\n            r, c = self._row_col(pos)\n            observation[r][c] = BODY #CellState.BODY.value\n        for pos in self.tails:\n            r, c = self._row_col(pos)\n            observation[r][c] = TAIL #CellState.TAIL.value\n\n        #Me\n        r, c = self._row_col(self.my_head)\n        observation[r][c] = MY_HEAD #-1 #CellState.MY_HEAD.value\n        if self.my_head != self.my_tail:\n            r, c = self._row_col(self.my_tail)\n            observation[r][c] = MY_TAIL #CellState.MY_TAIL.value\n        for pos in self.my_body:\n            r, c = self._row_col(pos)\n            observation[r][c] = MY_BODY #CellState.MY_BODY.value\n            \n        #Food\n        for pos in self.food:\n            r, c = self._row_col(pos)\n            observation[r][c] = FOOD #CellState.FOOD.value\n        \n        \n        if (self.previous_action!=-1):\n            aux_previous_pos = self._translate(self.my_head, self.opposite(self.previous_action))\n            r, c = self._row_col(aux_previous_pos)\n            if observation[r][c]>0:\n                observation[r][c] = MY_BODY * .5 #Marked to avoid opposite moves\n        \n        #Add risk mark\n        for pos in self.adjacent_to_heads:\n            r, c = self._row_col(pos)\n            if observation[r][c] > 0:\n                    observation[r][c] = RISK\n\n        #Add risk mark\n        for pos in self.dead_ends:\n            r, c = self._row_col(pos)\n            if observation[r][c] > 0:\n                    observation[r][c] = RISK\/2\n        \n        \n        if self.center_head:\n            #NOTE: assumes odd number of rows and columns\n            head_row, head_col = self._row_col(self.my_head)\n            v_center = (self.columns \/\/ 2) # col 5 on 0-10 (11 columns)\n            v_roll = v_center - head_col\n            h_center = (self.rows \/\/ 2) # row 3 on 0-7 (7 rows)\n            h_roll = h_center - head_row\n            observation = np.roll(observation, v_roll, axis=1)\n            observation = np.roll(observation, h_roll, axis=0)\n\n        return np.array([observation])\n    \n    \n    def common_sense_rewards(self, action):\n        if self.my_head==-1:\n            if self.debug:\n                print(\"DIED!!\")\n            return -2\n        \n        reward = 0\n        future_position = self._translate(self.my_head, action)\n        check_opposite = (self.previous_action!=-1)\n        \n        if future_position in self.occupied:\n            if self.debug:\n                print(\"Move to occupied\")\n            reward = -2 #this action meant death        \n        elif check_opposite and (self.previous_action==self.opposite(action)): #opposite is currently a patch until Action.opposite works...\n            if self.debug:\n                print(\"Move to opposite direction, previous\", self.previous_action, \"vs now\",action)\n            reward = -2 #this action meant death\n        elif (future_position in self.food) and (future_position not in self.adjacent_to_heads):\n            if self.debug:\n                print(\"Safe move to EAT!\")\n            reward = 2 #eating is good! \n        elif future_position in self.dead_ends:\n            if self.debug:\n                print(\"Move to dead end\")\n            reward = 0\n        else:\n            min_distance_to_food = self._min_distance_to_food(future_position)\n            \n            if min_distance_to_food<=self.last_min_distance_to_food:\n                if self.debug:\n                    print(\"Move to food\")\n                #Removed positive rewards here, eating reward will be considered via gamma (future rewards) if agent gets to food\n                if future_position in self.danger_zone:\n                    reward = 0 #0.1 \n                else:\n                    reward = 0 #0.2 \n            else:\n                #ignore might be moving away, but also the nearest food could have been eaten... NO PENALTY HERE!\n                reward = 0 \n                \n            self.last_min_distance_to_food=min_distance_to_food\n                \n        self.previous_action = self.last_action\n        self.last_action = action\n        return reward\n    \n    \n#Initial template from: https:\/\/stable-baselines.readthedocs.io\/en\/master\/guide\/custom_env.html\nclass HungryGeeseEnv(gym.Env):\n    \n    def __init__(self, dummy_env=False, opponent=['greedy','greedy','greedy-goose.py'], action_offset=1, debug=False, defaults=[7,11,10,2]):\n        super(HungryGeeseEnv, self).__init__()\n        self.num_envs = 1\n        self.num_previous_observations = 0\n        self.debug=debug\n        self.actions = [action for action in Action]\n        self.action_offset=action_offset\n        if not dummy_env:\n            self.env = make(\"hungry_geese\", debug=self.debug)\n            self.rows = self.env.configuration.rows\n            self.columns = self.env.configuration.columns\n            self.hunger_rate = self.env.configuration.hunger_rate\n            self.min_food = self.env.configuration.min_food\n            self.trainer = self.env.train([None, *opponent])\n        else:\n            self.env = None\n            self.rows = defaults[0]\n            self.columns = defaults[1]\n            self.hunger_rate = defaults[2]\n            self.min_food = defaults[3]\n\n        # Define action and observation space\n        # They must be gym.spaces objects        \n        self.action_space = spaces.Discrete(len(self.actions))\n        self.observation_space = spaces.Box(low=-1, high=1,\n                                            shape=(self.num_previous_observations+1, self.rows, self.columns), dtype=np.int8)\n        self.reward_range = (-4, 1)\n        self.step_num=1        \n        self.observation_preprocessor = ObservationProcessor(self.rows, self.columns, self.hunger_rate, self.min_food, debug=self.debug, center_head=True)\n        self.observation = []\n        self.previous_observation = []\n    \n    \n    def step(self, action):        \n        action += self.action_offset\n        action = Action(action)        \n        cs_rewards = self.observation_preprocessor.common_sense_rewards(action)\n        if self.debug:\n            if cs_rewards!=0:\n                print(\"CS reward\", action.name, self.observation, cs_rewards)\n            else:\n                print(\"CS ok\", action.name)\n        \n\n        obs, reward, done, _ = self.trainer.step(action.name)\n\n        if len(self.observation)>0:\n            #Not initial step, t=0\n            self.previous_observation.append(self.observation)\n            #Keep list constrained to max length\n            if len(self.previous_observation)>self.num_previous_observations:\n                del self.previous_observation[0]\n            \n        self.observation = self.observation_preprocessor.process_env_obs(obs)\n        \n        if len(self.previous_observation)==0:\n            #Initial step, t=0\n            self.previous_observation = [self.observation for _ in range(self.num_previous_observations)]\n        \n        info = {}\n        #if self.debug:\n        #    print(action, reward, cs_rewards, done, \"\\n\"+\"\\n\".join([str(o) for o in self.observation]))\n        \n        env_reward = reward\n        if len(self.previous_observation)>0:\n            unique_before, counts_before = np.unique(self.previous_observation[-1], return_counts=True)\n            unique_now, counts_now = np.unique(self.observation, return_counts=True)\n            before = dict(zip(unique_before, counts_before))\n            now = dict(zip(unique_now, counts_now))\n            count_length = lambda d: d.get(CellState.MY_HEAD.value, 0) + d.get(CellState.MY_BODY.value, 0) + d.get(CellState.MY_TAIL.value, 0)\n            if count_length(now)>count_length(before):\n                reward = 2 #Ate\n            else:\n                reward = 0 #Just moving\n            if self.debug:\n                print(f'{self.step_num} {count_length(now)} {count_length(before)} R {reward}')\n        else:\n            reward = 0 # no way to check previuos length use common sense reward on move to food instead ;-)\n        if done:\n            #game ended            \n            if self.observation_preprocessor.my_head == -1:\n                #DIED, but what final ranking?\n                rank = len(self.observation_preprocessor.geese)+1\n                if self.debug:\n                    print(\"Rank on end\", rank, \"geese\", self.observation_preprocessor.geese)\n                if rank == 4:\n                    reward = -2\n                elif rank == 3:\n                    reward = 0\n                elif rank == 2:\n                    reward = 0\n                else:\n                    reward = 100\n            else:\n                reward = 1 #survived the game!?\n        elif reward<1:\n            reward=1.1 #0 #set to 0 if staying alive is not enough\n        elif reward>1:\n            #ate something!!! :-)\n            reward = 1\n        \n        if self.debug and done:\n            print(\"DONE!\", self.observation, env_reward, reward, cs_rewards)\n        \n        reward = cs_rewards if cs_rewards<0 else cs_rewards+reward #if cs_reward<0 use it only      \n        self.step_num += 1\n        \n        if self.num_previous_observations>0:\n            observations = np.concatenate((*self.previous_observation, self.observation), axis=0)\n            return observations, reward, done, info\n        else:\n            return self.observation, reward, done, info\n\n        \n    def reset(self):\n        self.observation_preprocessor = ObservationProcessor(self.rows, self.columns, self.hunger_rate, self.min_food, debug=self.debug, center_head=True)\n        obs = self.trainer.reset()\n        self.observation = self.observation_preprocessor.process_env_obs(obs)\n        self.previous_observation = [self.observation for _ in range(self.num_previous_observations)]\n        return self.observation\n    \n    \n    def render(self,**kwargs):\n        self.env.render(**kwargs)","7106d42b":"env = HungryGeeseEnv(opponent=['greedy','greedy','greedy-goose.py'], debug=False) #Train vs 2 basic greedy and 1 improved greedy averse","ac8db1d3":"\"\"\"\nfrom stable_baselines3.common.env_checker import check_env\ncheck_env(env)\n\"\"\"","900f1063":"from stable_baselines3 import DQN\nfrom stable_baselines3.common.monitor import Monitor\nfrom stable_baselines3.common.torch_layers import BaseFeaturesExtractor\n\nimport torch as th\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nmodel_name = \"dqnv1\"\nm_env = Monitor(env, model_name, allow_early_resets=True) \n\n\npolicy_kwargs = dict(\n    net_arch = [2000, 1000, 500, 1000, 500, 100]\n)\n\nTRAIN_STEPS = 1e6\nalpha_0 = 1e-6\nalpha_end = 1e-9\n\ndef learning_rate_f(process_remaining):\n    #default =  1e-4\n    initial = alpha_0\n    final = alpha_end\n    interval = initial-final\n    return final+interval*process_remaining\n\nparams ={\n    'gamma': .9,\n    'batch_size': 100,\n     #'train_freq': 500,\n    'target_update_interval': 10000,\n    'learning_rate': learning_rate_f,\n    'learning_starts': 1000,\n    'exploration_fraction': .2,\n    'exploration_initial_eps': .05,\n    'tau': 1,\n    'exploration_final_eps': .01,\n    'buffer_size': 100000,\n    #'verbose': 1,\n}\n\n#coment **params for default parameters\ntrainer = DQN('MlpPolicy', m_env, policy_kwargs=policy_kwargs, **params)\n\n#You can check policy architecture with:\n#print(trainer.policy.net_arch) #prints: [64, 64] for default DQN policy\n#Or check model.policy\nprint(trainer.policy)","f4a58ac4":"trainer.learn(total_timesteps=TRAIN_STEPS, callback=None)","408d8b91":"import pandas as pd\nimport seaborn as sns\nfrom matplotlib import pyplot as plt\n\ndf = pd.read_csv(f'{model_name}.monitor.csv', header=1, index_col='t')\n\ndf.rename(columns = {'r':'Episode Reward', 'l':'Episode Length'}, inplace = True) \nplt.figure(figsize=(20,5))\nsns.regplot(data=df, y='Episode Reward', x=df.index)","dc364b35":"plt.figure(figsize=(20,5))\nsns.regplot(data=df, y='Episode Length', x=df.index, color=\"orange\")","57933a4d":"state_dict = trainer.policy.to('cpu').state_dict()\nprint(\"\\n\".join(state_dict.keys())) #use this to check keys ;-)","7e93103e":"\"\"\" If yout don't do this an Error will be raised due to different naming when loading model outside stable_baselines3\nMissing key(s) in state_dict: \"conv1.weight\", \"conv1.bias\", \"conv2.weight\", \"conv2.bias\", \"fc1.weight\", \"fc1.bias\", \"fc2.weight\", \"fc2.bias\", \"fc3.weight\", \"fc3.bias\". \nKey names have format:\nq_net.q_net.0.weight\nq_net.q_net.0.bias\n...\nq_net_target.q_net.0.weight\nq_net_target.q_net.0.bias\n...\n\"\"\"\n    \nadapted_state_dict ={\n    new_key : state_dict[old_key]\n    for old_key in state_dict.keys()\n    for new_key in [\"layer\"+\".\".join(old_key.split(\".\")[-2:])] #use last 3 components of name\n    if old_key.find(\"q_net_target.\") != -1 #we only want the policy weights\n}\nprint(adapted_state_dict.keys())\nth.save(adapted_state_dict, f'{model_name}.pt')\n","8db35af4":"%%writefile main.py\nfrom kaggle_environments.envs.hungry_geese.hungry_geese import Observation, Configuration, Action, adjacent_positions, row_col, translate, min_distance\nfrom kaggle_environments import make\n\nimport gym\nfrom gym import spaces\n\nimport torch as th\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nfrom enum import Enum, auto\nimport numpy as np\nimport os\nimport random as rand\n\n\nclass CellState(Enum):\n    EMPTY = 0\n    FOOD = auto()\n    HEAD = auto()\n    BODY = auto()\n    TAIL = auto()\n    MY_HEAD = auto()\n    MY_BODY = auto()\n    MY_TAIL = auto()\n    ANY_GOOSE = auto()\n    \n\nclass ObservationProcessor:\n    \n    def __init__(self, rows, columns, hunger_rate, min_food, debug=False, center_head=True):\n        self.debug = debug\n        self.rows, self.columns = rows, columns\n        self.hunger_rate = hunger_rate\n        self.min_food = min_food\n        self.previous_action = -1\n        self.last_action = -1\n        self.last_min_distance_to_food = self.rows*self.columns #initial max value to mark no food seen so far\n        self.center_head = center_head\n\n    #***** BEGIN: utility functions ******   \n    \n    def opposite(self, action):\n        if action == Action.NORTH:\n            return Action.SOUTH\n        if action == Action.SOUTH:\n            return Action.NORTH\n        if action == Action.EAST:\n            return Action.WEST\n        if action == Action.WEST:\n            return Action.EAST\n        raise TypeError(str(action) + \" is not a valid Action.\")\n        \n        \n    def _adjacent_positions(self, position):\n        return adjacent_positions(position, self.columns, self.rows)\n\n    \n    def _min_distance_to_food(self, position, food=None):\n        food = food if food!=None else self.food\n        return min_distance(position, food, self.columns)\n    \n    \n    def _row_col(self, position):\n        return row_col(position, self.columns)\n\n    \n    def _translate(self, position, direction):\n        return translate(position, direction, self.columns, self.rows)     \n\n    \n    def _preprocess_env(self, obs):\n        observation = Observation(obs)\n        \n        self.my_index = observation.index\n\n        if len (observation.geese[self.my_index])>0:\n            self.my_head = observation.geese[self.my_index][0]\n            self.my_tail = observation.geese[self.my_index][-1]        \n            self.my_body = [pos for pos in observation.geese[self.my_index][1:-1]]\n        else:\n            self.my_head = -1\n            self.my_tail = -1\n            self.my_body = []\n\n        \n        self.geese = [g for i,g in enumerate(observation.geese) if i!=self.my_index and len(g) > 0]\n        self.geese_cells = [pos for g in self.geese for pos in g if len(g) > 0]\n        \n        self.occupied = [p for p in self.geese_cells]\n        self.occupied.extend([p for p in observation.geese[self.my_index]])\n        \n        \n        self.heads = [g[0] for i,g in enumerate(observation.geese) if i!=self.my_index and len(g) > 0]\n        self.bodies = [pos  for i,g in enumerate(observation.geese) for pos in g[1:-1] if i!=self.my_index and len(g) > 2]\n        self.tails = [g[-1] for i,g in enumerate(observation.geese) if i!=self.my_index and len(g) > 1]\n        self.food = [f for f in observation.food]\n        \n        self.adjacent_to_heads = [pos for head in self.heads for pos in self._adjacent_positions(head)]\n        self.adjacent_to_bodies = [pos for body in self.bodies for pos in self._adjacent_positions(body)]\n        self.adjacent_to_tails = [pos for tail in self.tails for pos in self._adjacent_positions(tail)]\n        self.adjacent_to_geese = self.adjacent_to_heads + self.adjacent_to_bodies\n        self.danger_zone = self.adjacent_to_geese\n        \n        #Cell occupation\n        self.cell_states = [CellState.EMPTY.value for _ in range(self.rows*self.columns)]\n        for g in self.geese:\n            for pos in g:\n                self.cell_states[pos] = CellState.ANY_GOOSE.value\n        for pos in self.heads:\n                self.cell_states[pos] = CellState.ANY_GOOSE.value\n        for pos in self.my_body:\n            self.cell_states[pos] = CellState.ANY_GOOSE.value\n        self.cell_states[self.my_tail] = CellState.ANY_GOOSE.value\n                \n        #detect dead-ends\n        self.dead_ends = []\n        for pos_i,_ in enumerate(self.cell_states):\n            if self.cell_states[pos_i] != CellState.EMPTY.value:\n                continue\n            adjacent = self._adjacent_positions(pos_i)\n            adjacent_states = [self.cell_states[adj_pos] for adj_pos in adjacent if adj_pos!=self.my_head]\n            num_blocked = sum(adjacent_states)\n            if num_blocked>=(CellState.ANY_GOOSE.value*3):\n                self.dead_ends.append(pos_i)\n        \n        #check for extended dead-ends\n        new_dead_ends = [pos for pos in self.dead_ends]\n        while new_dead_ends!=[]:\n            for pos in new_dead_ends:\n                self.cell_states[pos]=CellState.ANY_GOOSE.value\n                self.dead_ends.append(pos)\n            \n            new_dead_ends = []\n            for pos_i,_ in enumerate(self.cell_states):\n                if self.cell_states[pos_i] != CellState.EMPTY.value:\n                    continue\n                adjacent = self._adjacent_positions(pos_i)\n                adjacent_states = [self.cell_states[adj_pos] for adj_pos in adjacent if adj_pos!=self.my_head]\n                num_blocked = sum(adjacent_states)\n                if num_blocked>=(CellState.ANY_GOOSE.value*3):\n                    new_dead_ends.append(pos_i)    \n                    \n                        \n    def safe_position(self, future_position):\n        return (future_position not in self.occupied) and (future_position not in self.adjacent_to_heads) and (future_position not in self.dead_ends)\n    \n    \n    def valid_position(self, future_position):\n        return (future_position not in self.occupied) and (future_position not in self.dead_ends)    \n\n    \n    def free_position(self, future_position):\n        return (future_position not in self.occupied)  \n    \n    #***** END: utility functions ******\n    \n    \n    def process_env_obs(self, obs):\n        self._preprocess_env(obs)\n        \n        EMPTY = .4\n        HEAD = -1\n        BODY = MY_BODY = -.8\n        TAIL = MY_TAIL = -.5\n        MY_HEAD = 0\n        FOOD = 1\n        RISK = -.5\n        \n        #Example: {'remainingOverageTime': 12, 'step': 0, 'geese': [[62], [50]], 'food': [7, 71], 'index': 0}\n        #observation = [[CellState.EMPTY.value for _ in range(self.columns)] for _ in range(self.rows)]\n        observation = [[EMPTY for _ in range(self.columns)] for _ in range(self.rows)]\n        \n        #Other agents\n        for pos in self.heads:\n            r, c = self._row_col(pos)\n            observation[r][c] = HEAD #CellState.HEAD.value\n        for pos in self.bodies:\n            r, c = self._row_col(pos)\n            observation[r][c] = BODY #CellState.BODY.value\n        for pos in self.tails:\n            r, c = self._row_col(pos)\n            observation[r][c] = TAIL #CellState.TAIL.value\n\n        #Me\n        r, c = self._row_col(self.my_head)\n        observation[r][c] = MY_HEAD #-1 #CellState.MY_HEAD.value\n        if self.my_head != self.my_tail:\n            r, c = self._row_col(self.my_tail)\n            observation[r][c] = MY_TAIL #CellState.MY_TAIL.value\n        for pos in self.my_body:\n            r, c = self._row_col(pos)\n            observation[r][c] = MY_BODY #CellState.MY_BODY.value\n            \n        #Food\n        for pos in self.food:\n            r, c = self._row_col(pos)\n            observation[r][c] = FOOD #CellState.FOOD.value\n        \n        \n        if (self.previous_action!=-1):\n            aux_previous_pos = self._translate(self.my_head, self.opposite(self.previous_action))\n            r, c = self._row_col(aux_previous_pos)\n            if observation[r][c]>0:\n                observation[r][c] = MY_BODY * .5 #Marked to avoid opposite moves\n        \n        #Add risk mark\n        for pos in self.adjacent_to_heads:\n            r, c = self._row_col(pos)\n            if observation[r][c] > 0:\n                    observation[r][c] = RISK\n\n        #Add risk mark\n        for pos in self.dead_ends:\n            r, c = self._row_col(pos)\n            if observation[r][c] > 0:\n                    observation[r][c] = RISK\/2        \n        \n        if self.center_head:\n            #NOTE: assumes odd number of rows and columns\n            head_row, head_col = self._row_col(self.my_head)\n            v_center = (self.columns \/\/ 2) # col 5 on 0-10 (11 columns)\n            v_roll = v_center - head_col\n            h_center = (self.rows \/\/ 2) # row 3 on 0-7 (7 rows)\n            h_roll = h_center - head_row\n            observation = np.roll(observation, v_roll, axis=1)\n            observation = np.roll(observation, h_roll, axis=0)\n\n        return np.array([observation])\n    \n    \nclass MyNN(nn.Module):\n    def __init__(self):\n        super(MyNN, self).__init__()\n        \"\"\"use names generated on adapted saved_dict\n        dict_keys(['layer0.weight', 'layer0.bias', 'layer2.weight', 'layer2.bias', ...])\n\n        net_arch as seen before:\n          (q_net): QNetwork(\n            (features_extractor): FlattenExtractor(\n              (flatten): Flatten(start_dim=1, end_dim=-1)\n            )\n            (q_net): Sequential(\n              (0): Linear(...)\n              (1): ReLU()\n              ...\n            )\n          )\n        \"\"\"\n        #net_arch = [2000, 1000, 500, 1000, 500, 100]\n        self.layer0 = nn.Linear(77, 2000)\n        self.layer2 = nn.Linear(2000, 1000)\n        self.layer4 = nn.Linear(1000, 500)\n        self.layer6 = nn.Linear(500, 1000)\n        self.layer8 = nn.Linear(1000, 500)\n        self.layer10 = nn.Linear(500, 100)\n        self.layer12 = nn.Linear(100, 4)\n\n    def forward(self, x):\n        x = nn.Flatten()(x)  # no feature extractor means flatten (check policy arch on DQN creation)\n        for layer in [self.layer0, self.layer2, self.layer4, self.layer6, self.layer8, self.layer10]:\n            x = F.relu(layer(x))\n        x = self.layer12(x)\n        return x\n            \n        \ndef my_dqn(observation, configuration):\n    global model, obs_prep, last_action, last_observation, previous_observation\n\n    tgz_agent_path = '\/kaggle_simulations\/agent\/'\n    normal_agent_path = '\/kaggle\/working'\n    model_name = \"dqnv1\"\n    num_previous_observations = 0\n    epsilon = 0\n    init = False\n    debug = False\n\n    try:\n        model\n    except NameError:\n        init=True\n    else:\n        if model==None:\n            init = True \n            initializing\n    if init:\n        #initializations\n        defaults = [configuration.rows,\n                    configuration.columns,\n                    configuration.hunger_rate,\n                    configuration.min_food]\n\n        model = MyNN()\n        last_action = -1\n        last_observation = []\n        previous_observation = []\n        \n        file_name = os.path.join(normal_agent_path, f'{model_name}.pt')\n        if not os.path.exists(file_name):\n            file_name = os.path.join(tgz_agent_path, f'{model_name}.pt')\n            \n        model.load_state_dict(th.load(file_name))\n        obs_prep = ObservationProcessor(configuration.rows, configuration.columns, configuration.hunger_rate, configuration.min_food)\n    \n    #maintaint list of  last observations\n    if num_previous_observations>0 and len(last_observation)>0:\n        #Not initial step, t=0\n        previous_observation.append(last_observation)\n        #Keep list constrained to max length\n        if len(previous_observation)>num_previous_observations:\n            del previous_observation[0]\n            \n    #Convert to grid encoded with CellState values\n    aux_observation = [obs_prep.process_env_obs(observation)] \n    last_observation = aux_observation\n\n    if num_previous_observations>0 and len(previous_observation)==0:\n        #Initial step, t=0\n        previous_observation = [last_observation for _ in range(num_previous_observations)]\n\n    if num_previous_observations>0:\n        aux_observation = np.concatenate((*previous_observation, last_observation), axis=0)\n    else:\n        aux_observation = last_observation\n        \n    #predict with aux_observation.shape = (last_observations x rows x cols)\n    tensor_obs = th.Tensor([aux_observation])\n    n_out = model(tensor_obs) #Example: tensor([[0.2742, 0.2653, 0.2301, 0.2303]], grad_fn=<SoftmaxBackward>) \n    \n    #choose probabilistic next move based on prediction outputs\n    #with epsilon probability of fully random, always avoid opposite of last move\n    actions = [action.value for action in Action]\n    weights = list(n_out[0].detach().numpy())\n    if last_action!=-1:\n        #Avoid dying by stupidity xD\n        remove_index = actions.index(obs_prep.opposite(Action(last_action)).value)\n        del actions[remove_index]\n        del weights[remove_index]    \n    random=False\n\n    min_value = abs(min(weights))\n    weights = [min_value+w+1e-5 for w in weights] #Total of weights must be greater than zero  \n\n    \n    #Reduce weight to penalize bad moves (collisions, etc...)\n    weights_changed = False\n    weights_before = [w for w in weights]\n    for index, action in enumerate(actions):\n        future_position = obs_prep._translate(obs_prep.my_head, Action(action))\n        if not obs_prep.free_position(future_position):\n            weights[index] = min(weights[index], 1e-8) #Collision is worst case\n            weights_changed = True\n        elif future_position in obs_prep.dead_ends:\n            weights[index] = min(weights[index],1e-2) #dead ends\n            weights_changed = True\n        elif future_position in obs_prep.adjacent_to_heads:\n            weights[index] = min(weights[index],1e-8) #adjacent to heads\n            weights_changed = True\n    \n    \n    \n    if debug and weights_changed:\n        print(aux_observation)\n        print(f'Adapted weights: before {weights_before} and after {weights} for actions {[Action(a).name for a in actions]}')\n    #elif debug and not weights_changed:\n    #    print(f'Action weights {weights}')\n\n    if rand.random() < epsilon:\n        prediction = rand.choice(actions)\n        random=True\n    else:\n        prediction = rand.choices(actions, weights=weights)[0] \n    action_predicted = Action(prediction).name\n    \n    #print(observation) #Uncomment to debug a bit too much...\n    #if (last_action!=-1) and debug:\n    #    print(last_observation)\n    #    print(f'valid_actions={actions}, w={weights}, chose={Action(prediction).name}, rand={random}',\n    #          f'previous={Action(last_action).name}, opposite={Action(obs_prep.opposite(Action(last_action)).value).name}') \n    \n    last_action = prediction\n    return action_predicted #return action","b531bc0a":"!tar cvzf submission.tar.gz main.py dqnv1.pt","f6ade878":"\"\"\"\nimport kaggle_environments\nfrom kaggle_environments import make, evaluate, utils\n\nenv = make(\"hungry_geese\", debug=False) #set debug to True to see agent internals each step\n\nenv.reset()\nenv.run([\"main.py\",\"greedy\",\"greedy\", \"greedy-goose.py\"])\nenv.render(mode=\"ipython\", width=700, height=500)\n\"\"\"","16bfeed8":"import kaggle_environments\nfrom kaggle_environments import evaluate\n\nevaluate(\n    \"hungry_geese\",\n    [\"main.py\", \"greedy\",\"greedy\", \"greedy-goose.py\"],\n    num_episodes=10\n)","7b650034":"Lets see a game, so far not learning much :-(\n\n**EDIT:** Since game render is problematic with most browsers under certain scenarios, the code is commented on long trainings","8063bae9":"# Results\n\nLets see how the training goes","31a44100":"# Save our DQN model\n\nHere I use a little trick to avoid writting by-hand all the key mappings for model save\/load ;-)\n\nThis is due to the fack that stable-baselines uses different names than the ones pytorch will look for when loading later outside of stable-baselines","d5574a72":"## Creating our custom environment for sample-baselines\n\nCreate our custom training environment connecting kaggle's hugry-geese env with gym.Env.\n\nMain processes are done by:\n* **_preprocess_env()**: basic preprocessing of observation (copied from base template class)\n* **process_env_obs()**: processing raw environment observation to matrix-like spaces.Box, each cell has an int value in [-1,1]\n    * food -> 1\n    * empty -> 0.4\n    * tails -> -.5\n    * bodies -> -.8\n    * heads -> -1\n    * my_head -> 0, observation is always centered on it\n    * if agent's length is 1, previous position is marked as BODY*.5 (-.4) otherwise without previous observations learning of oppositve moves for length 1 is confusing...\n    * adjacent to heads and dead ends -> -0.5\n    \n* **common_sense_rewards()**: common sense rewards and penalties (avoid collisions, search food...)\n* step: environment step\n\nYou can edit the number of previous observations (if set to zero remember to set move to eat common sense reward to 1 because lenght can't be compared with previuos move to detect growth).","5c05a84d":"# Our agent for submission and test game","7a6a0074":"## Training DQN\n\nFrom stable_baselines3.DQN documentation\n* > class stable_baselines3.dqn.DQN(policy, env, learning_rate=0.0001, buffer_size=1000000, learning_starts=50000, batch_size=32, tau=1.0, gamma=0.99, train_freq=4, gradient_steps=1, n_episodes_rollout=- 1, optimize_memory_usage=False, target_update_interval=10000, exploration_fraction=0.1, exploration_initial_eps=1.0, exploration_final_eps=0.05, max_grad_norm=10, tensorboard_log=None, create_eval_env=False, policy_kwargs=None, verbose=0, seed=None, device='auto', _init_setup_model=True)\n    * > learning_rate (Union[float, Callable[[float], float]]) \u2013 The learning rate, it can be a function of the current progress remaining (from 1 to 0)\n    * > learning_starts (int) \u2013 how many steps of the model to collect transitions for before learning starts\n    * > batch_size (Optional[int]) \u2013 Minibatch size for each gradient update\n    * > buffer_size (int) \u2013 size of the replay buffer\n    * > gamma (float) \u2013 the discount factor\n    * > target_update_interval (int) \u2013 update the target network every target_update_interval environment steps.\n    * > exploration_fraction (float) \u2013 fraction of entire training period over which the exploration rate is reduced\n    * > exploration_initial_eps (float) \u2013 initial value of random action probability\n    * > exploration_final_eps (float) \u2013 final value of random action probability\n    \n### Custom Policy\n\nFrom: https:\/\/stable-baselines3.readthedocs.io\/en\/master\/guide\/custom_policy.html\n\n* using policy_kwargs use net_arch=[l1,..,ln]\n* > custom feature extractor (e.g. custom CNN when using images), you can define class that derives from BaseFeaturesExtractor and then pass it to the model when training","f11aa715":"# Hungry geese: Reinforcement Learning with stable-baselines3\n\n\nThis notebook is meant as a lab for testing Reinforcement Learning solutions on Kaggle's \"hungry-geese\" environment using stable-baselines3 for training but only pytorch later for submission agent. Consider this notebook as a general guide on how to work with stable-baselines3 and torch to implement a custom policy network for \"hungry-geese\". Play with it and modify to your liking ;-)\n\nCheck: \n* stable-baselines3 docs: https:\/\/stable-baselines3.readthedocs.io\/en\/master\/\n* DQN paper: https:\/\/arxiv.org\/abs\/1312.5602, https:\/\/www.nature.com\/articles\/nature14236\n* Thanks for the example on how to use pytorch custom policy to: https:\/\/www.kaggle.com\/toshikazuwatanabe\/connect4-make-submission-with-stable-baselines3\n\n\n## Important note\n\nTraining this method for enough steps on kaggle's notebook environment sometimes fails on game render (if you see error 137 after a failed run means \"out of memory\", try changing replay buffer size). I think DQN needs long trainings on this environment so train on your own server for better results.\n\nEnjoy and please notify me any mistakes you detect, thanks!\n\n\n## Changelog\n* v125: Fixed bug on observation space had wrong data type after changed to [-1,1] range. Thanks to \"Mahesh Abnave\" (@maheshabnave999) for reporting it!\n* v122: Fixed bug after observation space changed to [-1,1] range. Thanks to \"Mahesh Abnave\" (@maheshabnave999) for reporting it!\n* v119:\n    * Removed negative reward for just moving, ends up turning into suicidal agent. Now reward for move is 1.1, eat is 2\n    * Added to observation cells marked with RISK (-0.5) for: dead_ends and adjacent to heads\n* v116:\n    * Changed observation to properly mark as visited previous position when agent's lenght is only one\n    * Fixed bug on previous move negative reward, it was checked against the wrong action\n* v100: fixed some bug on submission agent\n    * fixed final weight adjutment\n    * fixed main.py used old version of observation processor\n* v95: fixed bug on final agent output weights adjustment\n* v92: observation input for policy is been simplified to grid of values [0,1] see environment description for encoding\n* v90: Major bugfix, fixed missing save\/load of policy network (only feature_extractor net was saved\/loaded and used).\n    * Now code is simpler since you just specify layers and sizes on creation\n* v84: added observation centered on head, various bugfixes, changed 2 of the opponents to vanilla greedy agents and some other minor modifications\n* v75: FIXED HUGE BUG on environment reward that led to the agent assuming every move was rewarded, sorry for not detecting it earlier :-(\n* v59: removed -100 rewards (seemed to prevent from learning)\n* v54: added policy action weight modification based on common sense (collision is bad, same for dead ends and reduce weight of dangerous moves)\n* v46: modified reward values to\n* v41: added back opposite action rewards for opposite move\n* v40: added reward based on final ranking on game end\n* v39:\n    * KNOWN ISSUES: policy still seems to converge on training to predicting almost always the same action :-( \n    * added positive reward when surviving a game\n    * added extra -1 reward when losing\n    * removed negative reward for opposite move\n    * removed negative reward for moving away from previously nearest food (might have been eaten or be a bad move to go there)\n    * (fixed bug) removed a mistake on a negative reward \n* v37: fixed bug on negative cs_reward incorporation\n* v25: added previous n grid observations to dqp policy (n=2), total input dimensions are n-previous + 1 for current observation\n* v23: final agent now avoids performing the opposite move to the last taken\n* v21: added dropout and changed action choice on agent to probabilistic based on policy outputs\n* v17: fixes on policy training\n\n\n## Preliminary preparations\n\n* Install library\n* Save our improved training opponent to disk. Here I'm using greedy risk averse goose from: https:\/\/www.kaggle.com\/victordelafuente\/hungry-geese-template-class-greedy-risk-averse\n* Part of the same template class code is used, with some minor modifications, at the \"utility functions\" section\n\n","c416f438":"Prepare the upload: submission.tar.gz","354ff740":"# Using our custom environment\n\n1. First we instantiate the environment","b08deca5":"Now let's put it all together. (See file for full source)\n\nI've tried to minimize necesary changes to the policy network to make copy to submission code easier. Our submission agent will use:\n* CellState & ObservationProcessor: to process the observation into a grid of states to use as input for our policy network\n* the MyNN pytorch network with architecture as inspected from \"trainer.policy\" with adapted layer names\n* my_dqn: main loop, chose action probabilisticaly based on outputs (could apply epsilon-greedy or other selection) that's done with the \"choices()\" with epsilon chance of fully random choice\n\nOnly changes to MyNN (apart from removing dropouts):\n> class MyNN(nn.Module):\n>\n>     def __init__(self):\n>         super(MyNN, self).__init__()\n\n\nAs for the \"main-loop\", general structure is as follows:\n         \n> def my_dqn(observation, configuration):\n>\n>      [initializations]\n>      ...    \n>      [maintaint list of  last observations]\n>      ...    \n>      #Convert to grid encoded with CellState     \n>      aux_observation = obs_prep.process_env_obs(observation)  values\n>      #predict with aux_observation.shape = (last_observations x rows x cols)\n>      ...    \n>      tensor_obs = th.Tensor([aux_observation])\n>      n_out = model(tensor_obs) #Example: tensor([[0.2742, 0.2653, 0.2301, 0.2303]], ...) \n>      ...    \n>      [edit action weights to take into account common sense, collide is bad, etc...]\n>      [choose probabilistic next move based on prediction outputs\n>      with epsilon probability of fully random, always avoid opposite of last move]\n>      ...   \n>      return action_predicted","898dbc1f":"Results are far from good but... keep working!!"}}