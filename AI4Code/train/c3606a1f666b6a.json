{"cell_type":{"104e79bd":"code","f2ca58e6":"code","a867d4e1":"code","b5c35381":"code","6862bd07":"code","fca2d9bf":"code","d9aaed64":"code","332858a2":"code","4b85d642":"code","610eca83":"code","49957c03":"code","833ed307":"code","042c00c5":"code","f6040582":"code","3a64f25b":"code","9afbd1aa":"code","72333111":"code","03fa24f7":"code","c9fdfbc0":"code","e26808c9":"code","56092ff2":"code","89c81a25":"code","04bb8ce0":"code","61810c63":"code","d48d60da":"code","d6cecf30":"code","a88e57ea":"code","3d7ccfbe":"code","c5b9dbfd":"code","e0e3c5d2":"code","8244db5c":"code","2b2792e0":"markdown","723ae4aa":"markdown","42f9d988":"markdown","3f6a95ce":"markdown","282c8b1c":"markdown","8d117a2b":"markdown","300c19e6":"markdown","bb16b5c1":"markdown","a1175575":"markdown","89e51ec4":"markdown","20942958":"markdown","3df6bd5f":"markdown","312f5a4d":"markdown"},"source":{"104e79bd":"import numpy as np\nimport pandas as pd\nfrom sklearn.manifold import TSNE\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport glob\nfrom urllib.parse import urlparse\n%matplotlib inline\nimport re\nfrom category_encoders.ordinal import OrdinalEncoder\nimport spacy\nimport gc\nimport gensim\nfrom nltk.tokenize import TweetTokenizer\nimport datetime\nfrom scipy import stats\nfrom scipy.sparse import hstack, csr_matrix\nfrom sklearn.model_selection import train_test_split, cross_val_score, KFold\nfrom wordcloud import WordCloud\nfrom collections import Counter\nfrom nltk.corpus import stopwords\nfrom nltk.util import ngrams\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.linear_model import LogisticRegression, LinearRegression, Ridge\nfrom sklearn.svm import LinearSVC\nfrom sklearn.multiclass import OneVsRestClassifier\nimport time\npd.set_option('max_colwidth',400)\nfrom scipy.stats import spearmanr\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\n\nimport scipy as sp\nimport random\nfrom sklearn.preprocessing import OneHotEncoder\nimport os\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader\nfrom torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\nfrom torch.autograd import Variable\nimport torch.utils.data\n\n\nimport torch.utils.data\nfrom torch.optim import lr_scheduler\nfrom scipy.stats import spearmanr\nimport numpy as np\nimport time\nfrom tqdm import tqdm\nimport pickle\nfrom nltk.stem import PorterStemmer\nps = PorterStemmer()\nfrom nltk.stem.lancaster import LancasterStemmer\nlc = LancasterStemmer()\nfrom nltk.stem import SnowballStemmer\nsb = SnowballStemmer(\"english\")\nimport tensorflow_hub as hub\nimport keras.backend as K\nimport sys\n!pip install ..\/input\/sacremoses\/sacremoses-master\/ > \/dev\/null\nsys.path.insert(0, \"..\/input\/transformers\/transformers-master\/\")\nimport transformers","f2ca58e6":"# added preprocessing from https:\/\/www.kaggle.com\/wowfattie\/3rd-place\/data\n\ndef sigmoid(x):\n    return 1 \/ (1 + np.exp(-x))\n\n\npuncts = [',', '.', '\"', ':', ')', '(', '-', '!', '?', '|', ';', \"'\", '$', '&', '\/', '[', ']', '>', '%', '=', '#', '*', '+', '\\\\', '\u2022',  '~', '@', '\u00a3',\n '\u00b7', '_', '{', '}', '\u00a9', '^', '\u00ae', '`',  '<', '\u2192', '\u00b0', '\u20ac', '\u2122', '\u203a',  '\u2665', '\u2190', '\u00d7', '\u00a7', '\u2033', '\u2032', '\u00c2', '\u2588', '\u00bd', '\u00e0', '\u2026', '\\n', '\\xa0', '\\t',\n '\u201c', '\u2605', '\u201d', '\u2013', '\u25cf', '\u00e2', '\u25ba', '\u2212', '\u00a2', '\u00b2', '\u00ac', '\u2591', '\u00b6', '\u2191', '\u00b1', '\u00bf', '\u25be', '\u2550', '\u00a6', '\u2551', '\u2015', '\u00a5', '\u2593', '\u2014', '\u2039', '\u2500', '\\u3000', '\\u202f',\n '\u2592', '\uff1a', '\u00bc', '\u2295', '\u25bc', '\u25aa', '\u2020', '\u25a0', '\u2019', '\u2580', '\u00a8', '\u2584', '\u266b', '\u2606', '\u00e9', '\u00af', '\u2666', '\u00a4', '\u25b2', '\u00e8', '\u00b8', '\u00be', '\u00c3', '\u22c5', '\u2018', '\u221e', '\u00ab',\n '\u2219', '\uff09', '\u2193', '\u3001', '\u2502', '\uff08', '\u00bb', '\uff0c', '\u266a', '\u2569', '\u255a', '\u00b3', '\u30fb', '\u2566', '\u2563', '\u2554', '\u2557', '\u25ac', '\u2764', '\u00ef', '\u00d8', '\u00b9', '\u2264', '\u2021', '\u221a', ]\nmispell_dict = {\"aren't\" : \"are not\",\n\"can't\" : \"cannot\",\n\"couldn't\" : \"could not\",\n\"couldnt\" : \"could not\",\n\"didn't\" : \"did not\",\n\"doesn't\" : \"does not\",\n\"doesnt\" : \"does not\",\n\"don't\" : \"do not\",\n\"hadn't\" : \"had not\",\n\"hasn't\" : \"has not\",\n\"haven't\" : \"have not\",\n\"havent\" : \"have not\",\n\"he'd\" : \"he would\",\n\"he'll\" : \"he will\",\n\"he's\" : \"he is\",\n\"i'd\" : \"I would\",\n\"i'd\" : \"I had\",\n\"i'll\" : \"I will\",\n\"i'm\" : \"I am\",\n\"isn't\" : \"is not\",\n\"it's\" : \"it is\",\n\"it'll\":\"it will\",\n\"i've\" : \"I have\",\n\"let's\" : \"let us\",\n\"mightn't\" : \"might not\",\n\"mustn't\" : \"must not\",\n\"shan't\" : \"shall not\",\n\"she'd\" : \"she would\",\n\"she'll\" : \"she will\",\n\"she's\" : \"she is\",\n\"shouldn't\" : \"should not\",\n\"shouldnt\" : \"should not\",\n\"that's\" : \"that is\",\n\"thats\" : \"that is\",\n\"there's\" : \"there is\",\n\"theres\" : \"there is\",\n\"they'd\" : \"they would\",\n\"they'll\" : \"they will\",\n\"they're\" : \"they are\",\n\"theyre\":  \"they are\",\n\"they've\" : \"they have\",\n\"we'd\" : \"we would\",\n\"we're\" : \"we are\",\n\"weren't\" : \"were not\",\n\"we've\" : \"we have\",\n\"what'll\" : \"what will\",\n\"what're\" : \"what are\",\n\"what's\" : \"what is\",\n\"what've\" : \"what have\",\n\"where's\" : \"where is\",\n\"who'd\" : \"who would\",\n\"who'll\" : \"who will\",\n\"who're\" : \"who are\",\n\"who's\" : \"who is\",\n\"who've\" : \"who have\",\n\"won't\" : \"will not\",\n\"wouldn't\" : \"would not\",\n\"you'd\" : \"you would\",\n\"you'll\" : \"you will\",\n\"you're\" : \"you are\",\n\"you've\" : \"you have\",\n\"'re\": \" are\",\n\"wasn't\": \"was not\",\n\"we'll\":\" will\",\n\"didn't\": \"did not\",\n\"tryin'\":\"trying\"}\n\n\ndef clean_text(x):\n    x = str(x)\n    for punct in puncts:\n        x = x.replace(punct, f' {punct} ')\n    return x\n\n\ndef clean_numbers(x):\n    x = re.sub('[0-9]{5,}', '#####', x)\n    x = re.sub('[0-9]{4}', '####', x)\n    x = re.sub('[0-9]{3}', '###', x)\n    x = re.sub('[0-9]{2}', '##', x)\n    return x\n\n\ndef _get_mispell(mispell_dict):\n    mispell_re = re.compile('(%s)' % '|'.join(mispell_dict.keys()))\n    return mispell_dict, mispell_re\n\n\ndef replace_typical_misspell(text):\n    mispellings, mispellings_re = _get_mispell(mispell_dict)\n\n    def replace(match):\n        return mispellings[match.group(0)]\n\n    return mispellings_re.sub(replace, text)\n\n\ndef clean_data(df, columns: list):\n    for col in columns:\n        df[col] = df[col].apply(lambda x: clean_numbers(x))\n        df[col] = df[col].apply(lambda x: clean_text(x.lower()))\n        df[col] = df[col].apply(lambda x: replace_typical_misspell(x))\n\n    return df","a867d4e1":"\ndef get_coefs(word,*arr):\n    return word, np.asarray(arr, dtype='float32')\n\n\ndef load_embeddings(path):\n    with open(path,'rb') as f:\n        emb_arr = pickle.load(f)\n    return emb_arr\n\ndef build_matrix_adv(embedding_path: str = '',\n                 embedding_path_spellcheck: str = r'f:\\embeddings\\wiki-news-300d-1M\\wiki-news-300d-1M.vec',\n                 word_dict: dict = None, lemma_dict: dict = None, max_features: int = 100000,\n                 embed_size: int= 300, ):\n    spell_model = gensim.models.KeyedVectors.load_word2vec_format(embedding_path_spellcheck)\n    words = spell_model.index2word\n    w_rank = {}\n    for i, word in enumerate(words):\n        w_rank[word] = i\n    WORDS = w_rank\n\n    def P(word):\n        \"Probability of `word`.\"\n        # use inverse of rank as proxy\n        # returns 0 if the word isn't in the dictionary\n        return - WORDS.get(word, 0)\n\n    def correction(word):\n        \"Most probable spelling correction for word.\"\n        return max(candidates(word), key=P)\n\n    def candidates(word):\n        \"Generate possible spelling corrections for word.\"\n        return (known([word]) or known(edits1(word)) or [word])\n\n    def known(words):\n        \"The subset of `words` that appear in the dictionary of WORDS.\"\n        return set(w for w in words if w in WORDS)\n\n    def edits1(word):\n        \"All edits that are one edit away from `word`.\"\n        letters = 'abcdefghijklmnopqrstuvwxyz'\n        splits = [(word[:i], word[i:]) for i in range(len(word) + 1)]\n        deletes = [L + R[1:] for L, R in splits if R]\n        transposes = [L + R[1] + R[0] + R[2:] for L, R in splits if len(R) > 1]\n        replaces = [L + c + R[1:] for L, R in splits if R for c in letters]\n        inserts = [L + c + R for L, R in splits for c in letters]\n        return set(deletes + transposes + replaces + inserts)\n\n    def edits2(word):\n        \"All edits that are two edits away from `word`.\"\n        return (e2 for e1 in edits1(word) for e2 in edits1(e1))\n\n    def singlify(word):\n        return \"\".join([letter for i, letter in enumerate(word) if i == 0 or letter != word[i - 1]])\n\n\n    # embedding_index = dict(get_coefs(*o.strip().split(\" \")) for o in open(embedding_path, encoding='utf-8'))\n\n    # embedding_index = dict(get_coefs(*o.strip().split(\" \")) for o in open(embedding_path, encoding='utf-8', errors='ignore'))\n    embedding_index = load_embeddings(embedding_path)\n\n    nb_words = min(max_features, len(word_dict))\n    embedding_matrix = np.zeros((nb_words + 1, embed_size))\n    unknown_words = []\n    for word, i in word_dict.items():\n        key = word\n        embedding_vector = embedding_index.get(word)\n        if embedding_vector is not None:\n            embedding_matrix[i] = embedding_vector\n            continue\n        embedding_vector = embedding_index.get(word.lower())\n        if embedding_vector is not None:\n            embedding_matrix[i] = embedding_vector\n            continue\n        embedding_vector = embedding_index.get(word.upper())\n        if embedding_vector is not None:\n            embedding_matrix[i] = embedding_vector\n            continue\n        embedding_vector = embedding_index.get(word.capitalize())\n        if embedding_vector is not None:\n            embedding_matrix[i] = embedding_vector\n            continue\n        word = ps.stem(key)\n        embedding_vector = embedding_index.get(word)\n        if embedding_vector is not None:\n            embedding_matrix[word_dict[key]] = embedding_vector\n            continue\n        word = lc.stem(key)\n        embedding_vector = embedding_index.get(word)\n        if embedding_vector is not None:\n            embedding_matrix[word_dict[key]] = embedding_vector\n            continue\n        word = sb.stem(key)\n        embedding_vector = embedding_index.get(word)\n        if embedding_vector is not None:\n            embedding_matrix[word_dict[key]] = embedding_vector\n            continue\n        word = lemma_dict[key]\n        embedding_vector = embedding_index.get(word)\n        if embedding_vector is not None:\n            embedding_matrix[word_dict[key]] = embedding_vector\n            continue\n        if len(key) > 1:\n            word = correction(key)\n            embedding_vector = embedding_index.get(word)\n            if embedding_vector is not None:\n                embedding_matrix[word_dict[key]] = embedding_vector\n                continue\n        unknown_words.append(key)\n\n    print(f'{len(unknown_words) * 100 \/ len(word_dict):.4f}% words are not in embeddings')\n    return embedding_matrix, nb_words, unknown_words\n\n\ndef get_word_lemma_dict(full_text: list = None, ):\n    nlp = spacy.load('en_core_web_lg', disable=['parser','ner','tagger'])\n    nlp.vocab.add_flag(lambda s: s.lower() in spacy.lang.en.stop_words.STOP_WORDS, spacy.attrs.IS_STOP)\n    word_dict = {}\n    word_index = 1\n    lemma_dict = {}\n    docs = nlp.pipe(full_text, n_threads = os.cpu_count())\n    for doc in docs:\n        for token in doc:\n            if (token.text not in word_dict) and (token.pos_ is not \"PUNCT\"):\n                word_dict[token.text] = word_index\n                word_index += 1\n                lemma_dict[token.text] = token.lemma_\n\n    return lemma_dict, word_dict\n\n\ndef build_matrix(embedding_path: str = '',\n                 embedding_path_spellcheck: str = r'f:\\embeddings\\wiki-news-300d-1M\\wiki-news-300d-1M.vec',\n                 word_dict: dict = None, max_features: int = 100000,\n                 embed_size: int= 300, ):\n\n    # embedding_index = dict(get_coefs(*o.strip().split(\" \")) for o in open(embedding_path, encoding='utf-8'))\n    embedding_index = load_embeddings(embedding_path)\n\n    nb_words = min(max_features, len(word_dict))\n    embedding_matrix = np.zeros((nb_words + 1, embed_size))\n    unknown_words = []\n    for word, i in word_dict.items():\n        key = word\n        embedding_vector = embedding_index.get(word)\n        if embedding_vector is not None:\n            embedding_matrix[i] = embedding_vector\n            continue\n        embedding_vector = embedding_index.get(word.lower())\n        if embedding_vector is not None:\n            embedding_matrix[i] = embedding_vector\n            continue\n        embedding_vector = embedding_index.get(word.upper())\n        if embedding_vector is not None:\n            embedding_matrix[i] = embedding_vector\n            continue\n        embedding_vector = embedding_index.get(word.capitalize())\n        if embedding_vector is not None:\n            embedding_matrix[i] = embedding_vector\n            continue\n        unknown_words.append(key)\n\n    print(f'{len(unknown_words) * 100 \/ len(word_dict):.4f}% words are not in embeddings')\n    return embedding_matrix, nb_words, unknown_words\n\n\ndef seed_everything(seed=1234):\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True","b5c35381":"def chunks(l, n):\n    \"\"\"Yield successive n-sized chunks from l.\"\"\"\n    for i in range(0, len(l), n):\n        yield l[i:i + n]\n        \ndef fetch_vectors(string_list, batch_size=64, max_len = 512):\n    # inspired by https:\/\/jalammar.github.io\/a-visual-guide-to-using-bert-for-the-first-time\/\n    DEVICE = torch.device(\"cuda\")\n    tokenizer = transformers.DistilBertTokenizer.from_pretrained(\"..\/input\/distilbertbaseuncased\/\")\n    model = transformers.DistilBertModel.from_pretrained(\"..\/input\/distilbertbaseuncased\/\")\n    model.to(DEVICE)\n\n    fin_features = []\n    for data in chunks(string_list, batch_size):\n        tokenized = []\n        for x in data:\n            x = \" \".join(x.strip().split()[:500])\n            tok = tokenizer.encode(x, add_special_tokens=True)\n            tokenized.append(tok[:max_len])\n\n        \n        padded = np.array([i + [0] * (max_len - len(i)) for i in tokenized])\n        attention_mask = np.where(padded != 0, 1, 0)\n        input_ids = torch.tensor(padded).to(DEVICE)\n        attention_mask = torch.tensor(attention_mask).to(DEVICE)\n\n        with torch.no_grad():\n            last_hidden_states = model(input_ids, attention_mask=attention_mask)\n\n        features = last_hidden_states[0][:, 0, :].cpu().numpy()\n        fin_features.append(features)\n\n    fin_features = np.vstack(fin_features)\n    return fin_features","6862bd07":"def get_embedding_features(train, test, input_columns, only_test = False, batch_size = 4):\n    \"\"\"\n    https:\/\/www.kaggle.com\/ragnar123\/simple-lgbm-solution-baseline?scriptVersionId=24198335\n    \"\"\"\n    \n    # load universal sentence encoder model to get sentence ambeddings\n    module_url = \"..\/input\/universalsentenceencoderlarge4\/\"\n    embed = hub.load(module_url)\n    \n    # create empty dictionaries to store final results\n    if not only_test:\n        embedding_train = {}\n    embedding_test = {}\n\n    # iterate over text columns to get senteces embeddings with the previous loaded model\n    for text in input_columns:\n    \n        print(text)\n        if not only_test:\n            train_text = train[text].str.replace('?', '.').str.replace('!', '.').tolist()\n        test_text = test[text].str.replace('?', '.').str.replace('!', '.').tolist()\n    \n        # create empy list to save each batch\n        curr_train_emb = []\n        curr_test_emb = []\n    \n        # define a batch to transform senteces to their correspinding embedding (1 X 512 for each sentece)\n        if not only_test:\n            ind = 0\n            while ind * batch_size < len(train_text):\n                curr_train_emb.append(embed(train_text[ind * batch_size: (ind + 1) * batch_size])['outputs'].numpy())\n                ind += 1\n        \n        ind = 0\n        while ind * batch_size < len(test_text):\n            curr_test_emb.append(embed(test_text[ind * batch_size: (ind + 1) * batch_size])['outputs'].numpy())\n            ind += 1\n\n        # stack arrays to get a 2D array (dataframe) corresponding with all the sentences and dim 512 for columns (sentence encoder output)\n        if not only_test:\n            embedding_train[text + '_embedding'] = np.vstack(curr_train_emb)\n        embedding_test[text + '_embedding'] = np.vstack(curr_test_emb)\n    \n    del embed\n    K.clear_session()\n    gc.collect()\n    \n    if only_test:\n        return embedding_test\n    else:\n        return embedding_train, embedding_test\n\ndef get_dist_features(embedding_train, embedding_test):\n    \n    # define a square dist lambda function were (x1 - y1) ^ 2 + (x2 - y2) ^ 2 + (x3 - y3) ^ 2 + ... + (xn - yn) ^ 2\n    # with this we get one vector of dimension 6079\n    l2_dist = lambda x, y: np.power(x - y, 2).sum(axis = 1)\n    \n    # define a cosine dist lambda function were (x1 * y1) ^ 2 + (x2 * y2) + (x3 * y3) + ... + (xn * yn)\n    cos_dist = lambda x, y: (x * y).sum(axis = 1)\n    \n    # transpose it because we have 6 vector of dimension 6079, need 6079 x 6\n    dist_features_train = np.array([\n        l2_dist(embedding_train['question_title_embedding'], embedding_train['answer_embedding']),\n        l2_dist(embedding_train['question_body_embedding'], embedding_train['answer_embedding']),\n        l2_dist(embedding_train['question_body_embedding'], embedding_train['question_title_embedding']),\n        cos_dist(embedding_train['question_title_embedding'], embedding_train['answer_embedding']),\n        cos_dist(embedding_train['question_body_embedding'], embedding_train['answer_embedding']),\n        cos_dist(embedding_train['question_body_embedding'], embedding_train['question_title_embedding'])]).T\n    \n    # transpose it because we have 6 vector of dimension 6079, need 6079 x 6\n    dist_features_test = np.array([\n        l2_dist(embedding_test['question_title_embedding'], embedding_test['answer_embedding']),\n        l2_dist(embedding_test['question_body_embedding'], embedding_test['answer_embedding']),\n        l2_dist(embedding_test['question_body_embedding'], embedding_test['question_title_embedding']),\n        cos_dist(embedding_test['question_title_embedding'], embedding_test['answer_embedding']),\n        cos_dist(embedding_test['question_body_embedding'], embedding_test['answer_embedding']),\n        cos_dist(embedding_test['question_body_embedding'], embedding_test['question_title_embedding'])]).T\n    \n    return dist_features_train, dist_features_test","fca2d9bf":"# training the model\n\ndef train_model(model, train_loader, valid_loader, n_epochs=5, lr=0.001):\n    optimizer = torch.optim.Adam(model.parameters(), lr)\n    patience = 2\n\n    scheduler = lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=1, factor=0.1)\n    \n    loss_fn = torch.nn.BCEWithLogitsLoss(reduction='mean').cuda()\n    best_score = 0\n\n    for epoch in range(n_epochs):\n        start_time = time.time()\n        model.train()\n        avg_loss = 0.\n\n        for question, answer, title, category, host, use_emb_q, use_emb_a, use_emb_t, dist_feature, y_batch in tqdm(train_loader, disable=True):\n            question = question.long().cuda()\n            answer = answer.long().cuda()\n            title = title.long().cuda()\n            category = category.long().cuda()\n            host = host.long().cuda()\n            use_emb_q = use_emb_q.cuda()\n            use_emb_a = use_emb_a.cuda()\n            use_emb_t = use_emb_t.cuda()\n            dist_feature = dist_feature.cuda()\n            \n            y_batch = y_batch.cuda()\n            y_pred = model(question, answer, title, category, host, use_emb_q, use_emb_a, use_emb_t, dist_feature)\n\n            loss = loss_fn(y_pred.double(), y_batch)\n\n            optimizer.zero_grad()\n\n            loss.backward()\n\n            optimizer.step()\n            avg_loss += loss.item() \/ len(train_loader)\n\n        model.eval()\n\n        avg_val_loss = 0.\n        preds = []\n        original = []\n        for i, (question, answer, title, category, host, use_emb_q, use_emb_a, use_emb_t, dist_feature, y_batch) in enumerate(valid_loader):\n            question = question.long().cuda()\n            answer = answer.long().cuda()\n            title = title.long().cuda()\n            category = category.long().cuda()\n            host = host.long().cuda()\n            use_emb_q = use_emb_q.cuda()\n            use_emb_a = use_emb_a.cuda()\n            use_emb_t = use_emb_t.cuda()\n            dist_feature = dist_feature.cuda()\n            \n            y_batch = y_batch.cuda()\n            y_pred = model(question, answer, title, category, host, use_emb_q, use_emb_a, use_emb_t, dist_feature).detach()\n\n            avg_val_loss += loss_fn(y_pred.double(), y_batch).item() \/ len(valid_loader)\n            preds.append(y_pred.cpu().numpy())\n            original.append(y_batch.cpu().numpy())\n            \n        score = 0\n        for i in range(30):\n            score += np.nan_to_num(\n                spearmanr(np.concatenate(original)[:, i], np.concatenate(preds)[:, i]).correlation \/ 30)\n        elapsed_time = time.time() - start_time\n        \n        print('Epoch {}\/{} \\t loss={:.4f} \\t val_loss={:.4f} \\t spearman={:.2f} \\t time={:.2f}s'.format(\n            epoch + 1, n_epochs, avg_loss, avg_val_loss, score, elapsed_time))\n\n        scheduler.step(avg_val_loss)\n\n        valid_score = score\n        if valid_score > best_score:\n            best_score = valid_score\n            p = 0\n            torch.save(model.state_dict(), 'model.pt')\n\n        # check if validation loss didn't improve\n        if valid_score <= best_score:\n            p += 1\n            print(f'{p} epochs of non improving score')\n            if p > patience:\n                print('Stopping training')\n                stop = True\n                break\n                \n        model.load_state_dict(torch.load('model.pt'))\n                \n    return model\n\n\ndef make_prediction(test_loader: DataLoader = None, model = None):\n    prediction = np.zeros((len(test_loader.dataset), 30))\n    model.eval()\n    for i, (question, answer, title, category, host, use_emb_q, use_emb_a, use_emb_t, dist_feature, _) in enumerate(test_loader):\n\n        start_index = i * test_loader.batch_size\n        end_index   = min(start_index + test_loader.batch_size, len(test_loader.dataset))\n        question = question.long().cuda()\n        answer = answer.long().cuda()\n        title = title.long().cuda()\n        category = category.long().cuda()\n        host = host.long().cuda()\n        use_emb_q = use_emb_q.cuda()\n        use_emb_a = use_emb_a.cuda()\n        use_emb_t = use_emb_t.cuda()\n        dist_feature = dist_feature.cuda()\n        y_pred = model(question, answer, title, category, host, use_emb_q, use_emb_a, use_emb_t, dist_feature).detach()\n        y_pred = torch.sigmoid(y_pred)\n        prediction[start_index:end_index, :] +=  y_pred.detach().cpu().numpy()\n        \n    return prediction","d9aaed64":"class Attention(nn.Module):\n    def __init__(self, feature_dim, step_dim, bias=True, **kwargs):\n        super(Attention, self).__init__(**kwargs)\n\n        self.supports_masking = True\n\n        self.bias = bias\n        self.feature_dim = feature_dim\n        self.step_dim = step_dim\n        self.features_dim = 0\n\n        weight = torch.zeros(feature_dim, 1)\n        nn.init.xavier_uniform_(weight)\n        self.weight = nn.Parameter(weight)\n\n        if bias:\n            self.b = nn.Parameter(torch.zeros(step_dim))\n\n    def forward(self, x, mask=None):\n        feature_dim = self.feature_dim\n        step_dim = self.step_dim\n\n        eij = torch.mm(\n            x.contiguous().view(-1, feature_dim),\n            self.weight\n        ).view(-1, step_dim)\n\n        if self.bias:\n            eij = eij + self.b\n\n        eij = torch.tanh(eij)\n        a = torch.exp(eij)\n\n        if mask is not None:\n            a = a * mask\n\n        a = a \/ torch.sum(a, 1, keepdim=True) + 1e-10\n\n        weighted_input = x * torch.unsqueeze(a, -1)\n        return torch.sum(weighted_input, 1)\n\n    \nclass GELU(nn.Module):\n    def forward(self, x):\n        return x * torch.sigmoid(1.702 * x)\n\n\nclass SpatialDropout(nn.Dropout2d):\n    def forward(self, x):\n        x = x.unsqueeze(2)    # (N, T, 1, K)\n        x = x.permute(0, 3, 2, 1)  # (N, K, 1, T)\n        x = super(SpatialDropout, self).forward(x)  # (N, K, 1, T), some features are masked\n        x = x.permute(0, 3, 2, 1)  # (N, T, 1, K)\n        x = x.squeeze(2)  # (N, T, K)\n        return x\n    \n    \nclass Mish(nn.Module):\n    \"\"\"\n    Mish - \"Mish: A Self Regularized Non-Monotonic Neural Activation Function\"\n    https:\/\/arxiv.org\/abs\/1908.08681v1\n    implemented for PyTorch \/ FastAI by lessw2020 \n    github: https:\/\/github.com\/lessw2020\/mish\n    \n    \"\"\"\n\n    def __init__(self):\n        super().__init__()\n\n    def forward(self, x):\n        #inlining this saves 1 second per epoch (V100 GPU) vs having a temp x and then returning x(!)\n        return x *( torch.tanh(F.softplus(x)))\n    \n    \n    \nclass NeuralNet5(nn.Module):\n    def __init__(self,\n                 hidden_size: int = 128,\n                 max_len: int = 500,\n                 max_len_title: int = 30,\n                 n_cat: int = 3,\n                 cat_emb: int = 6,\n                 n_host: int = 55,\n                 host_emb: int = 28,\n                 additional_embedding_shape: int = 512,\n                 embedding_matrix=None):\n        super(NeuralNet5, self).__init__()\n\n        self.embedding = nn.Embedding(*embedding_matrix.shape)\n        self.embedding.weight = nn.Parameter(torch.tensor(embedding_matrix, dtype=torch.float32))\n        self.embedding.weight.requires_grad = False\n\n        self.embedding = nn.Embedding(*embedding_matrix.shape)\n        self.embedding.weight = nn.Parameter(torch.tensor(embedding_matrix, dtype=torch.float32))\n        self.embedding.weight.requires_grad = False\n\n        self.embedding_dropout = SpatialDropout(0.3)\n        \n        self.category_embedding = nn.Embedding(n_cat, int(cat_emb))\n        self.host_embedding = nn.Embedding(n_host, int(host_emb))\n\n        self.linear_q_add = nn.Linear(300, 128)\n        self.linear_q_add1 = nn.Linear(128, 30)\n        self.bilinear_add = nn.Bilinear(30, 30, 30)\n\n        self.lstm_q = nn.LSTM(300, hidden_size, bidirectional=True, batch_first=True)\n        self.gru_q = nn.GRU(hidden_size * 2, hidden_size, bidirectional=True, batch_first=True)\n\n        self.lstm_a = nn.LSTM(300, hidden_size, bidirectional=True, batch_first=True)\n        self.gru_a = nn.GRU(hidden_size * 2, hidden_size, bidirectional=True, batch_first=True)\n\n        self.lstm_t = nn.LSTM(300, hidden_size, bidirectional=True, batch_first=True)\n        self.gru_t = nn.GRU(hidden_size * 2, hidden_size, bidirectional=True, batch_first=True)\n\n        self.lstm_attention_q = Attention(hidden_size * 2, max_len)\n        self.gru_attention_q = Attention(hidden_size * 2, max_len)\n\n        self.lstm_attention_a = Attention(hidden_size * 2, max_len)\n        self.gru_attention_a = Attention(hidden_size * 2, max_len)\n\n        self.lstm_attention_t = Attention(hidden_size * 2, max_len_title)\n        self.gru_attention_t = Attention(hidden_size * 2, max_len_title)\n\n        self.linear_q = nn.Linear(1024, 64)\n        self.relu_q = Mish()\n\n        self.linear_a = nn.Linear(1024, 64)\n        self.relu_a = Mish()\n\n        self.linear_t = nn.Linear(1024, 64)\n        self.relu_t = Mish()\n        \n        self.linear_q_emb = nn.Linear(additional_embedding_shape, 64)\n        self.relu_q_emb = Mish()\n\n        self.linear_a_emb = nn.Linear(additional_embedding_shape, 64)\n        self.relu_a_emb = Mish()\n\n        self.linear_t_emb = nn.Linear(additional_embedding_shape, 64)\n        self.relu_t_emb = Mish()\n\n        self.linear1 = nn.Sequential(nn.Linear(256 + int(cat_emb) + int(host_emb) + 6, 64),\n                                     nn.BatchNorm1d(64),\n                                     nn.ReLU(inplace=True),\n                                     nn.Dropout(0.5))\n\n        self.linear_q_out = nn.Linear(64, 21)\n\n        self.bilinear = nn.Bilinear(64, 64, 64)\n        self.bilinear_emb = nn.Bilinear(64, 64, 64)\n        self.linear2 = nn.Sequential(nn.Linear(390, 64),\n                                     nn.BatchNorm1d(64),\n                                     nn.ReLU(inplace=True),\n                                     nn.Dropout(0.5))\n\n        self.linear_aq_out = nn.Linear(64, 9)\n\n    def forward(self, question, answer, title, category, host, use_emb_q, use_emb_a, use_emb_t, dist_feature):\n        h_embedding_q = self.embedding(question)\n        h_embedding_q = self.embedding_dropout(h_embedding_q)\n\n        h_lstm_q, _ = self.lstm_q(h_embedding_q)\n        h_gru_q, _ = self.gru_q(h_lstm_q)\n\n        h_lstm_atten_q = self.lstm_attention_q(h_lstm_q)\n        h_gru_atten_q = self.gru_attention_q(h_gru_q)\n\n        avg_pool_q = torch.mean(h_gru_q, 1)\n        max_pool_q, _ = torch.max(h_gru_q, 1)\n\n        h_embedding_a = self.embedding(answer)\n        h_embedding_a = self.embedding_dropout(h_embedding_a)\n\n        h_lstm_a, _ = self.lstm_a(h_embedding_a)\n        h_gru_a, _ = self.gru_a(h_lstm_a)\n\n        h_lstm_atten_a = self.lstm_attention_a(h_lstm_a)\n        h_gru_atten_a = self.gru_attention_a(h_gru_a)\n\n        avg_pool_a = torch.mean(h_gru_a, 1)\n        max_pool_a, _ = torch.max(h_gru_a, 1)\n\n        h_embedding_t = self.embedding(title)\n        h_embedding_t = self.embedding_dropout(h_embedding_t)\n\n        h_lstm_t, _ = self.lstm_t(h_embedding_t)\n        h_gru_t, _ = self.gru_t(h_lstm_t)\n\n        h_lstm_atten_t = self.lstm_attention_t(h_lstm_t)\n        h_gru_atten_t = self.gru_attention_t(h_gru_t)\n\n        avg_pool_t = torch.mean(h_gru_t, 1)\n        max_pool_t, _ = torch.max(h_gru_t, 1)\n\n        category = self.category_embedding(category)\n        host = self.host_embedding(host)\n        \n        add = torch.cat((h_embedding_q, h_embedding_a, h_embedding_t), 1)\n        add = self.linear_q_add(torch.mean(add, 1))\n        add = self.linear_q_add1(add)\n\n        q = torch.cat((h_lstm_atten_q, h_gru_atten_q, avg_pool_q, max_pool_q), 1)\n        a = torch.cat((h_lstm_atten_a, h_gru_atten_a, avg_pool_a, max_pool_a), 1)\n        t = torch.cat((h_lstm_atten_t, h_gru_atten_t, avg_pool_t, max_pool_t), 1)\n        \n        q = self.relu_q(self.linear_q(q))\n        a = self.relu_a(self.linear_a(a))\n        t = self.relu_t(self.linear_t(t))\n\n        q_emb = self.relu_q_emb(self.linear_q_emb(use_emb_q))\n        a_emb = self.relu_a_emb(self.linear_a_emb(use_emb_a))\n        t_emb = self.relu_t_emb(self.linear_t_emb(use_emb_t))\n        \n        hidden_q = self.linear1(torch.cat((q, t, q_emb, t_emb, category, host, dist_feature), 1))\n        q_result = self.linear_q_out(hidden_q)\n\n        bil_sim = self.bilinear(q, a)\n        bil_sim_emb = self.bilinear_emb(q_emb, a_emb)\n        \n        hidden_aq = self.linear2(torch.cat((q, a, q_emb, a_emb, bil_sim, bil_sim_emb, dist_feature), 1))\n        aq_result = self.linear_aq_out(hidden_aq)\n\n        out = torch.cat([q_result, aq_result], 1)\n        out = self.bilinear_add(out, add)\n\n        return out","332858a2":"class TextDataset(Dataset):\n\n    def __init__(self, question_data, answer_data, title_data, category_data, host_data, use_embeddings, dist_features, idxs, targets=None):\n        self.question_data = question_data[idxs]\n        self.answer_data = answer_data[idxs]\n        self.title_data = title_data[idxs]\n        self.category_data = category_data[idxs]\n        self.host_data = host_data[idxs]\n        self.use_embeddings_q = use_embeddings['question_body_embedding'][idxs]\n        self.use_embeddings_a = use_embeddings['answer_embedding'][idxs]\n        self.use_embeddings_t = use_embeddings['question_title_embedding'][idxs]\n        self.dist_features = dist_features[idxs]\n        self.targets = targets[idxs] if targets is not None else np.zeros((self.question_data.shape[0], 30))\n\n    def __getitem__(self, idx):\n        question = self.question_data[idx]\n        answer = self.answer_data[idx]\n        title = self.title_data[idx]\n        category = self.category_data[idx]\n        host = self.host_data[idx]\n        use_emb_q = self.use_embeddings_q[idx]\n        use_emb_a = self.use_embeddings_a[idx]\n        use_emb_t = self.use_embeddings_t[idx]\n        dist_feature = self.dist_features[idx]\n        target = self.targets[idx]\n\n        return question, answer, title, category, host, use_emb_q, use_emb_a, use_emb_t, dist_feature, target\n\n    def __len__(self):\n        return len(self.question_data)","4b85d642":"pd.set_option('max_rows', 500)\npd.set_option('max_columns', 500)\npath = '\/kaggle\/input\/google-quest-challenge'\nsample_submission = pd.read_csv(f'{path}\/sample_submission.csv')\ntest = pd.read_csv(f'{path}\/test.csv').fillna(' ')\ntrain = pd.read_csv(f'{path}\/train.csv').fillna(' ')","610eca83":"train = clean_data(train, ['answer', 'question_body', 'question_title'])\ntest = clean_data(test, ['answer', 'question_body', 'question_title'])","49957c03":"seed_everything()","833ed307":"%%time\nembedding_test = get_embedding_features(train, test, ['answer', 'question_body', 'question_title'], only_test=True)\nembedding_train = {}\nembedding_train['answer_embedding'] = np.load('\/kaggle\/input\/qa-labeling-files-for-inference\/embedding_train_answer_embedding.npy', allow_pickle=True)\nembedding_train['question_body_embedding'] = np.load('\/kaggle\/input\/qa-labeling-files-for-inference\/embedding_train_question_body_embedding.npy', allow_pickle=True)\nembedding_train['question_title_embedding'] = np.load('\/kaggle\/input\/qa-labeling-files-for-inference\/embedding_train_question_title_embedding.npy', allow_pickle=True)","042c00c5":"%%time\ndist_features_train, dist_features_test  = get_dist_features(embedding_train, embedding_test)\n","f6040582":"tokenizer = Tokenizer()\nfull_text = list(train['question_body']) + \\\n                       list(train['answer']) + \\\n                       list(train['question_title']) + \\\n                       list(test['question_body']) + \\\n                       list(test['answer']) + \\\n                       list(test['question_title'])\ntokenizer.fit_on_texts(full_text)","3a64f25b":"embed_size=300\nembedding_path = \"\/kaggle\/input\/pickled-crawl300d2m-for-kernel-competitions\/crawl-300d-2M.pkl\"","9afbd1aa":"%%time\nlemma_dict, word_dict = get_word_lemma_dict(full_text)","72333111":"%%time\nembedding_matrix, nb_words, unknown_words = build_matrix(embedding_path, '\/kaggle\/input\/wikinews300d1mvec\/wiki-news-300d-1M.vec', tokenizer.word_index,\n                                              100000, embed_size)\n# embedding_matrix, nb_words, unknown_words = build_matrix_adv(embedding_path, '\/kaggle\/input\/wikinews300d1mvec\/wiki-news-300d-1M.vec', word_dict, lemma_dict,\n#                                               100000, embed_size)","03fa24f7":"# tk.word_index = {k: v for k, v in tk.word_index.items() if k in word_dict.keys()}\n# train['host'] = train['host'].apply(lambda x: x.split('.')[-2])\n# test['host'] = test['host'].apply(lambda x: x.split('.')[-2])\nunique_hosts = list(set(train['host'].unique().tolist() + test['host'].unique().tolist()))\nhost_dict = {i + 1: e for i, e in enumerate(unique_hosts)}\nhost_dict_reverse = {v: k for k, v in host_dict.items()}\n\nunique_categories = list(set(train['category'].unique().tolist() + test['category'].unique().tolist()))\ncategory_dict = {i + 1: e for i, e in enumerate(unique_categories)}\ncategory_dict_reverse = {v: k for k, v in category_dict.items()}\nmax_len = 500\nmax_len_title = 30\ntrain_question_tokenized = pad_sequences(tokenizer.texts_to_sequences(train['question_body']), maxlen = max_len)\ntrain_answer_tokenized = pad_sequences(tokenizer.texts_to_sequences(train['answer']), maxlen = max_len)\ntrain_title_tokenized = pad_sequences(tokenizer.texts_to_sequences(train['question_title']), maxlen = max_len_title)\n\ntest_question_tokenized = pad_sequences(tokenizer.texts_to_sequences(test['question_body']), maxlen = max_len)\ntest_answer_tokenized = pad_sequences(tokenizer.texts_to_sequences(test['answer']), maxlen = max_len)\ntest_title_tokenized = pad_sequences(tokenizer.texts_to_sequences(test['question_title']), maxlen = max_len_title)\n\ntrain_host = train['host'].apply(lambda x: host_dict_reverse[x]).values\ntrain_category = train['category'].apply(lambda x: category_dict_reverse[x]).values\n\ntest_host = test['host'].apply(lambda x: host_dict_reverse[x]).values\ntest_category = test['category'].apply(lambda x: category_dict_reverse[x]).values","c9fdfbc0":"y = train[sample_submission.columns[1:]].values","e26808c9":"num_workers = 0\nbs = 16\nn_cat = len(category_dict) + 1\ncat_emb = min(np.ceil((len(category_dict)) \/ 2), 50)\nn_host = len(host_dict)+1\nhost_emb = min(np.ceil((len(host_dict)) \/ 2), 50)","56092ff2":"bs_test = 16\ntest_loader = DataLoader(TextDataset(test_question_tokenized, test_answer_tokenized, test_title_tokenized,\n                                     test_category, test_host, embedding_test, dist_features_test, test.index),\n                          batch_size=bs_test, shuffle=False, num_workers=num_workers)","89c81a25":"folds = KFold(n_splits=5, random_state=42)\npreds = np.zeros((len(test), 30))\nfor fold_n, (train_index, valid_index) in enumerate(folds.split(train)):\n    print(f'Fold {fold_n + 1} started at {time.ctime()}')\n    train_loader = DataLoader(TextDataset(train_question_tokenized, train_answer_tokenized, train_title_tokenized, train_category, train_host, embedding_train,\n                                          dist_features_train, train_index, y),\n                              batch_size=bs, shuffle=True, num_workers=num_workers, pin_memory=True)\n    valid_loader = DataLoader(TextDataset(train_question_tokenized, train_answer_tokenized, train_title_tokenized, train_category, train_host, embedding_train,\n                                          dist_features_train, valid_index, y),\n                              batch_size=bs, shuffle=False, num_workers=num_workers, pin_memory=True)\n        \n    model = NeuralNet5(embedding_matrix=embedding_matrix,\n                       n_cat=n_cat,\n                       cat_emb=cat_emb,\n                       n_host=n_host,\n                       host_emb=host_emb)\n    model.cuda()\n\n    model = train_model(model, train_loader, valid_loader, n_epochs=10, lr=0.001)\n    prediction = make_prediction(test_loader, model)\n    preds += prediction \/ folds.n_splits \/ 2\n    \n    gc.collect()\n    torch.cuda.empty_cache()\n    print()","04bb8ce0":"del embedding_train\ndel embedding_test\ndel model\ngc.collect()\ntorch.cuda.empty_cache()","61810c63":"%%time\n# bert_embeddings_train = {}\nbert_embeddings_test = {}\nfor col in ['answer', 'question_body', 'question_title']:\n    # bert_embeddings_train[f'{col}_embedding'] = fetch_vectors(train[col].values, batch_size=4)\n    bert_embeddings_test[f'{col}_embedding'] = fetch_vectors(train[col].values, batch_size=4)","d48d60da":"%%time\nbert_embeddings_train = {}\nfor col in ['answer', 'question_body', 'question_title']:\n    bert_embeddings_train[f'{col}_embedding'] = np.load(f'\/kaggle\/input\/qa-labeling-files-for-inference\/distill_train_{col}.npy')","d6cecf30":"bs_test = 16\ntest_loader = DataLoader(TextDataset(test_question_tokenized, test_answer_tokenized, test_title_tokenized,\n                                     test_category, test_host, bert_embeddings_test, dist_features_test, test.index),\n                          batch_size=bs_test, shuffle=False, num_workers=num_workers)","a88e57ea":"folds = KFold(n_splits=5, random_state=42)\nfor fold_n, (train_index, valid_index) in enumerate(folds.split(train)):\n    print(f'Fold {fold_n + 1} started at {time.ctime()}')\n    train_loader = DataLoader(TextDataset(train_question_tokenized, train_answer_tokenized, train_title_tokenized, train_category, train_host, bert_embeddings_train,\n                                          dist_features_train, train_index, y),\n                              batch_size=bs, shuffle=True, num_workers=num_workers, pin_memory=True)\n    valid_loader = DataLoader(TextDataset(train_question_tokenized, train_answer_tokenized, train_title_tokenized, train_category, train_host, bert_embeddings_train,\n                                          dist_features_train, valid_index, y),\n                              batch_size=bs, shuffle=False, num_workers=num_workers, pin_memory=True)\n        \n    model = NeuralNet5(embedding_matrix=embedding_matrix,\n                       n_cat=n_cat,\n                       cat_emb=cat_emb,\n                       n_host=n_host,\n                       host_emb=host_emb,\n                       additional_embedding_shape=768)\n    model.cuda()\n\n    model = train_model(model, train_loader, valid_loader, n_epochs=10, lr=0.001)\n    prediction = make_prediction(test_loader, model)\n    preds += prediction \/ folds.n_splits \/ 2\n    \n    gc.collect()\n    torch.cuda.empty_cache()\n    print()","3d7ccfbe":"# clipping is necessary or we will get an error\nsample_submission.loc[:, 'question_asker_intent_understanding':] = np.clip(preds, 0.00001, 0.999999)\nsample_submission.to_csv('submission.csv', index=False)","c5b9dbfd":"sample_submission.head()","e0e3c5d2":"a = list(model.children())\ng = a[3].weight.cpu().detach().numpy()\ntsne = TSNE(n_components=2, verbose=1, perplexity=60, n_iter=1000, method='exact')\ntsne_results = tsne.fit_transform(g)\nfig, ax = plt.subplots(figsize=(12, 8))\nax.scatter(tsne_results[:, 0], tsne_results[:, 1])\n\nfor i, txt in enumerate(host_dict.values()):\n    ax.annotate(txt, (tsne_results[i, 0], tsne_results[i, 1]))","8244db5c":"g = a[2].weight.cpu().detach().numpy()\ntsne = TSNE(n_components=2, verbose=1, perplexity=60, n_iter=1000, method='exact')\ntsne_results = tsne.fit_transform(g)\nfig, ax = plt.subplots(figsize=(12, 8))\nax.scatter(tsne_results[:, 0], tsne_results[:, 1])\n\nfor i, txt in enumerate(category_dict.values()):\n    ax.annotate(txt, (tsne_results[i, 0], tsne_results[i, 1]))","2b2792e0":"### Model","723ae4aa":"## Training model","42f9d988":"### Building an embedding matrix","3f6a95ce":"## Helper functions and classes","282c8b1c":"### Creating additional features","8d117a2b":"### Training second model\n\nThe second model will use embeddings from bert.","300c19e6":"### Dataset class","bb16b5c1":"## Loading and preparing data","a1175575":"## Pytorch approach\n\nIn this kernel I work with data from Google QUEST Q&A Labeling competition.\nIn this challenge we work with... opinions. This could help Q&A systems, so let's try!\nCode will, of course, be in Pytorch\n\n#### Change log\n* V25: Adding USE like in https:\/\/www.kaggle.com\/ldm314\/universal-sentence-encoder-keras-nn\/\n* V26: changing preprocessing\n* V32: reverting to previous preprocessing approach. Use more models for predictions.\n* v33: fixed model paths\n* it doesn't work, so revert to previous version.\n* V37: adding bert embeddings (not using bert itself for training) like in this kernel: https:\/\/www.kaggle.com\/abhishek\/distilbert-use-features-oof\n* V40: limit the number of epochs for training (due to 2h limit). Use some preprocessed embeddings instead of creating them in kernel.","89e51ec4":"## Trained categorical encodings visualization\n\nJust a fun visualization showing how trained categorical embeddings look like.","20942958":"### Training and predicting functions","3df6bd5f":"## Importing libraries","312f5a4d":"### Text processing"}}