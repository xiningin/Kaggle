{"cell_type":{"c44e241a":"code","cda1cbf5":"code","729794e8":"code","fe78e248":"code","43003ae3":"code","dd38e90c":"code","834d05b8":"code","b8ea69e2":"code","2655e00a":"code","9650750c":"code","4df8e4cc":"code","cf88a4f6":"code","da8c34f9":"code","9f9057c4":"code","c0110d25":"markdown"},"source":{"c44e241a":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","cda1cbf5":"pd.set_option('mode.chained_assignment', None)\ntest = pd.read_csv(\"..\/input\/covid19-global-forecasting-week-3\/test.csv\")\ntrain = pd.read_csv(\"..\/input\/covid19-global-forecasting-week-3\/train.csv\")\ntrain['Province_State'].fillna('', inplace=True)\ntest['Province_State'].fillna('', inplace=True)\ntrain['Date'] =  pd.to_datetime(train['Date'])\ntest['Date'] =  pd.to_datetime(test['Date'])\ntrain = train.sort_values(['Country_Region','Province_State','Date'])\ntest = test.sort_values(['Country_Region','Province_State','Date'])\n\n# Fix error in train data\ntrain[['ConfirmedCases', 'Fatalities']] = train.groupby(['Country_Region', 'Province_State'])[['ConfirmedCases', 'Fatalities']].transform('cummax') ","729794e8":"from tqdm import tqdm\nimport warnings\n\ndef RMSLE(pred,actual):\n    return np.sqrt(np.mean(np.power((np.log(pred+1)-np.log(actual+1)),2)))\n\nfeature_day = [1,20,50,100,200,500,1000,5000,10000,15000,20000,50000,100000,200000]\ndef CreateInput(data):\n    feature = []\n    for day in feature_day:\n        #Get information in train data\n        data.loc[:,'Number day from ' + str(day) + ' case'] = 0\n        if (train[(train['Country_Region'] == country) & (train['Province_State'] == province) & (train['ConfirmedCases'] < day)]['Date'].count() > 0):\n            fromday = train[(train['Country_Region'] == country) & (train['Province_State'] == province) & (train['ConfirmedCases'] < day)]['Date'].max()        \n        else:\n            fromday = train[(train['Country_Region'] == country) & (train['Province_State'] == province)]['Date'].min()       \n        for i in range(0, len(data)):\n            if (data['Date'].iloc[i] > fromday):\n                day_denta = data['Date'].iloc[i] - fromday\n                data['Number day from ' + str(day) + ' case'].iloc[i] = day_denta.days \n        feature = feature + ['Number day from ' + str(day) + ' case']\n    \n    return data[feature]\n","fe78e248":"!pip install pmdarima","43003ae3":"import pmdarima as pm\n\npred_data_all = pd.DataFrame()\nwith tqdm(total=len(train['Country_Region'].unique())) as pbar:\n    for country in train['Country_Region'].unique():\n    #for country in ['Vietnam']:\n        for province in train[(train['Country_Region'] == country)]['Province_State'].unique():\n            with warnings.catch_warnings():\n                warnings.filterwarnings(\"ignore\")\n                df_train = train[(train['Country_Region'] == country) & (train['Province_State'] == province)]\n                df_test = test[(test['Country_Region'] == country) & (test['Province_State'] == province)]\n                X_train = CreateInput(df_train)\n                y_train_confirmed = df_train['ConfirmedCases'].ravel()\n                y_train_fatalities = df_train['Fatalities'].ravel()\n                X_pred = CreateInput(df_test)\n\n                # Define feature to use by X_pred\n                feature_use = X_pred.columns[0]\n                for i in range(X_pred.shape[1] - 1,0,-1):\n                    if (X_pred.iloc[0,i] > 10):\n                        feature_use = X_pred.columns[i]\n                        break\n                idx = X_train[X_train[feature_use] == 0].shape[0]          \n                adjusted_X_train = X_train[idx:][feature_use].values.reshape(-1, 1)\n                adjusted_y_train_confirmed = y_train_confirmed[idx:]\n                adjusted_y_train_fatalities = y_train_fatalities[idx:] #.values.reshape(-1, 1)\n\n                pred_data = test[(test['Country_Region'] == country) & (test['Province_State'] == province)]\n                max_train_date = train[(train['Country_Region'] == country) & (train['Province_State'] == province)]['Date'].max()\n                min_test_date = pred_data['Date'].min()            \n\n                model = pm.auto_arima(adjusted_y_train_confirmed, suppress_warnings=True, seasonal=False, error_action=\"ignore\")            \n                y_hat_confirmed = model.predict(pred_data[pred_data['Date'] > max_train_date].shape[0])\n                y_train_confirmed = train[(train['Country_Region'] == country) & (train['Province_State'] == province) & (train['Date'] >=  min_test_date)]['ConfirmedCases'].values\n                y_hat_confirmed = np.concatenate((y_train_confirmed,y_hat_confirmed), axis = 0)                        \n\n                model = pm.auto_arima(adjusted_y_train_fatalities, suppress_warnings=True, seasonal=False, error_action=\"ignore\")            \n                y_hat_fatalities = model.predict(pred_data[pred_data['Date'] > max_train_date].shape[0])\n                y_train_fatalities = train[(train['Country_Region'] == country) & (train['Province_State'] == province) & (train['Date'] >=  min_test_date)]['Fatalities'].values\n                y_hat_fatalities = np.concatenate((y_train_fatalities,y_hat_fatalities), axis = 0)            \n\n                pred_data['ConfirmedCases_hat'] = y_hat_confirmed\n                pred_data['Fatalities_hat'] = y_hat_fatalities\n                pred_data_all = pred_data_all.append(pred_data)\n        pbar.update(1)\n    \ndf_val = pd.merge(pred_data_all,train[['Date','Country_Region','Province_State','ConfirmedCases','Fatalities']],on=['Date','Country_Region','Province_State'], how='left')\ndf_val.loc[df_val['Fatalities_hat'] < 0,'Fatalities_hat'] = 0\ndf_val.loc[df_val['ConfirmedCases_hat'] < 0,'ConfirmedCases_hat'] = 0\n\ndf_val_1 = df_val.copy()","dd38e90c":"from bokeh.plotting import figure, show, output_notebook\nfrom bokeh.models import NumeralTickFormatter\nfrom bokeh.palettes import Spectral11\noutput_notebook()\ndef plotCountry(df_country, name):\n    p = figure(title=name + \" Confirmed Cases Forecast\", x_axis_label='Date', x_axis_type='datetime', y_axis_label='Confirmed Cases')\n    p.line(df_country['Date'], df_country['ConfirmedCases_hat'], legend_label=\"Confirmed Cases\", line_width=2)\n    p.legend.location = \"top_left\"\n    p.yaxis.formatter=NumeralTickFormatter(format=\"\u20180.0a\")    \n    show(p)\n\n    p = figure(title=name + \" Fatalities Forecast\", x_axis_label='Date', x_axis_type='datetime', y_axis_label='Fatalities Cases')\n    p.line(df_country['Date'], df_country['Fatalities_hat'], legend_label=\"Fatalities \", line_width=2)\n    p.legend.location = \"top_left\"\n    p.yaxis.formatter=NumeralTickFormatter(format=\"\u20180.0a\")    \n    show(p)\n\ndef plotTop(df_val):\n    df_now = train.groupby(['Date','Country_Region']).sum().sort_values(['Country_Region','Date']).reset_index()\n    df_now['New Cases'] = df_now['ConfirmedCases'].diff()\n    df_now['New Fatalities'] = df_now['Fatalities'].diff()\n    df_now = df_now.groupby('Country_Region').apply(lambda group: group.iloc[-1:]).reset_index(drop = True)\n\n    p = figure(title=\" Top 5 Confirmed Cases Forecast\", x_axis_label='Date', x_axis_type='datetime', y_axis_label='Confirmed Cases')\n    mypalette=Spectral11[0:5]\n    i = 0\n    for country in df_now.sort_values('ConfirmedCases', ascending=False).head(5)['Country_Region'].values:\n        df_country = df_val[df_val['Country_Region'] == country].groupby(['Date','Country_Region']).sum().reset_index()\n        idx = df_country[((df_country['ConfirmedCases'].isnull() == False) & (df_country['ConfirmedCases'] > 0))].shape[0]\n        p.line(df_country['Date'], df_country['ConfirmedCases_hat'], legend_label= country + \" Confirmed Cases\", line_color=mypalette[i], line_width=2)\n        p.legend.location = \"top_left\"\n        p.yaxis.formatter=NumeralTickFormatter(format=\"\u20180.0a\")    \n        i = i+1\n\n    show(p)        ","834d05b8":"country = \"Vietnam\"\ndf_country = df_val[df_val['Country_Region'] == country].groupby(['Date','Country_Region']).sum().reset_index()\nplotCountry(df_country,country)","b8ea69e2":"df_country = df_val.groupby(['Date']).sum().reset_index()\nplotCountry(df_country,'World')","2655e00a":"from statsmodels.tsa.statespace.sarimax import SARIMAX\n\npred_data_all = pd.DataFrame()\nwith tqdm(total=len(train['Country_Region'].unique())) as pbar:\n    for country in train['Country_Region'].unique():\n    #for country in ['Spain']:\n        for province in train[(train['Country_Region'] == country)]['Province_State'].unique():\n            with warnings.catch_warnings():\n                warnings.filterwarnings(\"ignore\")\n                df_train = train[(train['Country_Region'] == country) & (train['Province_State'] == province)]\n                df_test = test[(test['Country_Region'] == country) & (test['Province_State'] == province)]\n                X_train = CreateInput(df_train)\n                y_train_confirmed = df_train['ConfirmedCases'].ravel()\n                y_train_fatalities = df_train['Fatalities'].ravel()\n                X_pred = CreateInput(df_test)\n\n                # Define feature to use by X_pred\n                feature_use = X_pred.columns[0]\n                for i in range(X_pred.shape[1] - 1,0,-1):\n                    if (X_pred.iloc[0,i] > 10):\n                        feature_use = X_pred.columns[i]\n                        break\n                idx = X_train[X_train[feature_use] == 0].shape[0]          \n                adjusted_X_train = X_train[idx:][feature_use].values.reshape(-1, 1)\n\n                adjusted_y_train_confirmed = y_train_confirmed[idx:]\n                adjusted_y_train_fatalities = y_train_fatalities[idx:] #.values.reshape(-1, 1)\n                \n                # Log to forecast Not log because of decrease trending\n                #adjusted_y_train_confirmed = np.log1p(adjusted_y_train_confirmed + 1)\n                #adjusted_y_train_fatalities = np.log1p(adjusted_y_train_fatalities + 1)\n                \n\n                pred_data = test[(test['Country_Region'] == country) & (test['Province_State'] == province)]\n                max_train_date = train[(train['Country_Region'] == country) & (train['Province_State'] == province)]['Date'].max()\n                min_test_date = pred_data['Date'].min()            \n\n                #model = pm.auto_arima(adjusted_y_train_confirmed, suppress_warnings=True, seasonal=False, error_action=\"ignore\")            \n                #y_hat_confirmed = model.predict(pred_data[pred_data['Date'] > max_train_date].shape[0])\n                model = SARIMAX(adjusted_y_train_confirmed, order=(1,1,0),\n                                #seasonal_order=(1,1,0,12),\n                                measurement_error=True).fit(disp=False)\n                y_hat_confirmed = model.forecast(pred_data[pred_data['Date'] > max_train_date].shape[0])\n                # inverse log\n                #y_hat_confirmed = np.expm1(y_hat_confirmed)\n                \n                y_train_confirmed = train[(train['Country_Region'] == country) & (train['Province_State'] == province) & (train['Date'] >=  min_test_date)]['ConfirmedCases'].values\n                y_hat_confirmed = np.concatenate((y_train_confirmed,y_hat_confirmed), axis = 0)                        \n\n                #model = pm.auto_arima(adjusted_y_train_fatalities, suppress_warnings=True, seasonal=False, error_action=\"ignore\")\n                #y_hat_fatalities = model.predict(pred_data[pred_data['Date'] > max_train_date].shape[0])\n                # inverse log\n                #y_hat_fatalities = np.expm1(y_hat_fatalities)\n                \n                model = SARIMAX(adjusted_y_train_fatalities, order=(1,1,0),\n                                #seasonal_order=(1,1,0,12),\n                                measurement_error=True).fit(disp=False)\n                y_hat_fatalities = model.forecast(pred_data[pred_data['Date'] > max_train_date].shape[0])\n                                \n                y_train_fatalities = train[(train['Country_Region'] == country) & (train['Province_State'] == province) & (train['Date'] >=  min_test_date)]['Fatalities'].values\n                y_hat_fatalities = np.concatenate((y_train_fatalities,y_hat_fatalities), axis = 0)            \n\n                pred_data['ConfirmedCases_hat'] = y_hat_confirmed\n                pred_data['Fatalities_hat'] = y_hat_fatalities\n                pred_data_all = pred_data_all.append(pred_data)\n        pbar.update(1)\n    \ndf_val = pd.merge(pred_data_all,train[['Date','Country_Region','Province_State','ConfirmedCases','Fatalities']],on=['Date','Country_Region','Province_State'], how='left')\ndf_val.loc[df_val['Fatalities_hat'] < 0,'Fatalities_hat'] = 0\ndf_val.loc[df_val['ConfirmedCases_hat'] < 0,'ConfirmedCases_hat'] = 0\n\ndf_val_2 = df_val.copy()","9650750c":"country = \"Vietnam\"\ndf_country = df_val[df_val['Country_Region'] == country].groupby(['Date','Country_Region']).sum().reset_index()\nplotCountry(df_country,country)","4df8e4cc":"df_country = df_val.groupby(['Date']).sum().reset_index()\nplotCountry(df_country,'World')","cf88a4f6":"plotTop(df_val_1)","da8c34f9":"plotTop(df_val_2)","9f9057c4":"df_val = df_val_2\nsubmission = df_val[['ForecastId','ConfirmedCases_hat','Fatalities_hat']]\nsubmission.columns = ['ForecastId','ConfirmedCases','Fatalities']\nsubmission = submission.round({'ConfirmedCases': 0, 'Fatalities': 0})\nsubmission.to_csv('submission.csv', index=False)\nsubmission","c0110d25":"**Introduction**\n\nIn the week 1 version, I thought the best algorithm is ARIMA. In the week 2, I have some feature engineering technique. It approved my score. Because of early submission, do not update the latest train data, I only was top 10%. However, some fork version from me can reach to top 5% without change code. My week 2 version include SARIMAX and Exponential (with poly feature). Both algorithm have same score but now SARIMAX is better.\n\nI had some ideal to approve score in week 3:\n* Try to find correlations between country base on spread transmission\n* Create some new feature because now we have more data than before. The new feature include other country information\n* Build complex neural network focus to exponential forecast\n\nHowever, due to limit of time and the effect of ARIMA, in week 3 I only use two approach:\n\n* Auto ARIMA (same to week 1) with feature engineering in week 2\n* Keep ARIMAX with longer train data (I don't use log target. Beause, in my opinion, We are going to peak of pandamic. So transmison will be slowly. SARIMAX is better than ARIMA)\n\nLink to week 1 version:\n* Data Visualization: Matplotlib\n* Focus to ARIMA, compare to many difference algorithm https:\/\/www.kaggle.com\/binhlc\/sars-cov-2-voting-regressor\n\nLink to week 2 version:\n* Data Visualization: Plotly\n* Focus to Exponential https:\/\/www.kaggle.com\/binhlc\/sars-cov-2-exponential-model-week-2\n\nWeek 3 version:\n* Data Visualization: Bokeh\n* Aprrove Auto Arima and SARIMAX"}}