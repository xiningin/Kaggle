{"cell_type":{"6ae77996":"code","14324553":"code","4712632b":"code","bbedecf9":"code","655d77c4":"code","6df97b26":"code","b28efb7e":"code","3c7b5e62":"code","ad01b413":"code","19d1194e":"code","2e86ae80":"code","f1fc2a2d":"code","d5dbb797":"code","0243ea70":"code","256ea552":"code","bfebaf8d":"code","51c9867e":"code","754f55ba":"code","e25edba6":"code","e6dfb23f":"code","550626ff":"code","202da965":"code","069095de":"code","22685178":"code","97701054":"code","ae45050b":"code","9e7f382d":"code","c8548d6b":"code","532238cd":"code","9e92ee4f":"code","0cd6e2be":"code","27611789":"code","555d9aed":"code","49c688fc":"code","f72702af":"code","247ea1c7":"code","57d1147d":"code","33222fe4":"code","0db30307":"code","32b3ad42":"code","d09d19a6":"code","200c1910":"code","8fb5335a":"code","6883cf0f":"code","b6bf3c54":"code","953a379f":"code","2b0eb44d":"code","c67be17e":"code","5d5197c4":"code","83f6c4b0":"code","b8f1a2d8":"code","fec45525":"code","7de347cb":"code","d0f944e0":"code","60b7b50d":"code","dde5644d":"code","a00fa32e":"code","bd3d1dea":"code","0b476a13":"code","e852564f":"code","8d48a72a":"code","9f3c2091":"code","730b008d":"code","d9246e58":"code","f82493f8":"code","2b0c0860":"code","2cd7e30e":"code","f7cbf922":"code","e12fe394":"code","571e6ff8":"code","61b86a3e":"code","d4847081":"code","89231d65":"code","a091d568":"code","dc417393":"code","ea41897d":"code","54db9672":"code","46b91106":"code","c8920b50":"markdown","824e8648":"markdown","eedf24d7":"markdown","d3796c38":"markdown","5b9789fb":"markdown","7eb3d374":"markdown","74cb8e60":"markdown","ef794648":"markdown","8773950f":"markdown","c1263e07":"markdown","5fd24253":"markdown","dd866cbf":"markdown","8e8ac30c":"markdown","5f8ccc55":"markdown","74024870":"markdown","76f996b4":"markdown","31f48795":"markdown","2270ccfe":"markdown","bcc2b08a":"markdown","00fb14c0":"markdown","230c0a2c":"markdown","996d609f":"markdown","0c46a720":"markdown","a2eef271":"markdown","3e9e387a":"markdown","a095f3fe":"markdown","1ef04c9e":"markdown","b1bb2ef6":"markdown","eb47d178":"markdown","56a387f3":"markdown","07566bde":"markdown","a7d6c336":"markdown","075d153b":"markdown","0d491716":"markdown","2d38d0e5":"markdown","3b678524":"markdown"},"source":{"6ae77996":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","14324553":"df = pd.read_csv('..\/input\/breast-cancer-wisconsin-data\/data.csv')\ndf","4712632b":"df.dtypes","bbedecf9":"df.isna().sum()","655d77c4":"df.info()","6df97b26":"cols_to_drop = ['id', 'Unnamed: 32']\ndf = df.drop(cols_to_drop, axis=1)","b28efb7e":"#Wrapper for 'diagnosis' (i.e. M = 0, B = 1)\nwrapper = {'M':0, 'B':1}\ndf.diagnosis = df.diagnosis.replace(wrapper)","3c7b5e62":"df","ad01b413":"from sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import classification_report\nimport seaborn as sns\nimport matplotlib.pyplot as plt","19d1194e":"def split_data(df):\n    X = df.drop('diagnosis', axis=1)\n    y = df['diagnosis']\n    return X, y","2e86ae80":"X, y = split_data(df)\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.25, random_state=42)","f1fc2a2d":"scaler = MinMaxScaler()\nmodel = LogisticRegression()","d5dbb797":"pipe = Pipeline(steps=[('scaler', scaler), ('model', model)])","0243ea70":"pipe.fit(X_train, y_train)","256ea552":"y_preds = pipe.predict(X_test)\ny_preds","bfebaf8d":"from sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import accuracy_score","51c9867e":"print(f'The accuracy between X_test and y_test is: {pipe.score(X_test, y_test)}\\nThe accuracy between X_train and y_train is: {pipe.score(X_train, y_train)}')","754f55ba":"print(classification_report(y_test, y_preds, target_names=['Metastasis', 'Benign']))","e25edba6":"matrix = confusion_matrix(y_test, y_preds)\nsns.heatmap(matrix, annot=True, cbar=None, cmap='Blues')\nplt.title('Confusion Matrix'), plt.tight_layout()\nplt.ylabel('True Class'), plt.xlabel('Predicted Class')\nplt.show()\nprint('0: Metastasis (M)\\n1: Benign (B)')","e6dfb23f":"import matplotlib.pyplot as plt\nfrom scipy.stats import zscore","550626ff":"sns.countplot(df.diagnosis);","202da965":"from scipy.stats import zscore","069095de":"cols = df.columns[1:] #Exclude the first column (since it's diagnosis)\nfor col in cols:\n    df.boxplot(col)\n    plt.show()","22685178":"from scipy.stats import zscore","97701054":"# z_df = pd.DataFrame() #Making a new DataFrame for which to gather all the z-scores\n# def gen_zscores(df, col, df2):\n#     df2['zscore_'+col] = zscore(df[col])\n    \n# [gen_zscores(df=df, col=nc, df2=z_df) for nc in df.columns[1:]] # This will add the new columns all at the end of the current df\n\n# z_df","ae45050b":"# columns_to_examine = ['zscore_fractal_dimension_worst', 'zscore_symmetry_worst', 'zscore_fractal_dimension_se', 'zscore_symmetry_se', 'zscore_concave points_se', 'zscore_concavity_se', 'zscore_compactness_se', 'zscore_smoothness_se', 'zscore_area_se', 'zscore_perimeter_se', 'zscore_texture_se', 'zscore_radius_se']","9e7f382d":"# drop = [] #We use this list to keep track of indices we want to get rid of\n\n# for i, column in enumerate(columns_to_examine):\n#     drop.append(z_df.index[z_df[columns_to_examine[i]] > 9].tolist())\n    \n# drop","c8548d6b":"# # Convert the list 'drop' from a list of lists into a set\n# outer_list = []\n# def lists_to_list(nested_lists): \n#     for el in nested_lists: \n#         if type(el) == list: \n#             lists_to_list(el) \n#         else: \n#             outer_list.append(el)\n#     return set(outer_list)\n\n# indices_to_drop = lists_to_list(drop)\n# print(f'Set of indices to drop: {indices_to_drop}, \\nLength of set: {len(indices_to_drop)}')","532238cd":"# # Dropping all the rows from 'df' with indices from the above set\n# df = df.drop(indices_to_drop, axis=0)","9e92ee4f":"from statsmodels.stats.outliers_influence import variance_inflation_factor as vif","0cd6e2be":"# Create a function to generate the VIF's for each feature\ndef gen_VIF(columns, df=df):\n    df = df.copy()\n    feature_df = df[columns]\n    vif_data_out = pd.DataFrame()\n    vif_data_out['Feature'] = feature_df.columns\n    vif_data_out['VIF'] = [vif(feature_df.values, i) for i in range(len(feature_df.columns))]\n    \n    return vif_data_out","27611789":"X_var_mean = ['radius_mean','texture_mean','perimeter_mean', 'area_mean', 'smoothness_mean', 'compactness_mean', 'concavity_mean', 'concave points_mean', 'symmetry_mean', 'fractal_dimension_mean']\nX_var_se = ['radius_se', 'texture_se', 'perimeter_se', 'area_se' ,'smoothness_se' ,'compactness_se' ,'concavity_se', 'concave points_se', 'symmetry_se', 'fractal_dimension_se']\nX_var_worst = ['radius_worst', 'texture_worst', 'perimeter_worst', 'area_worst', 'smoothness_worst', 'compactness_worst', 'concavity_worst', 'concave points_worst', 'symmetry_worst', 'fractal_dimension_worst']","555d9aed":"gen_VIF(X_var_mean)","49c688fc":"# df = df.drop('radius_mean', axis=1)","f72702af":"gen_VIF(X_var_se)","247ea1c7":"# df = df.drop('radius_se', axis=1)","57d1147d":"gen_VIF(X_var_worst)","33222fe4":"# df = df.drop('radius_worst', axis=1)","0db30307":"from sklearn.feature_selection import SelectKBest, f_classif","32b3ad42":"def ANOVA_F_val_scores(df, k):\n\n    X, y = split_data(df)\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = .25, random_state=12)\n\n    # Feature selection using ANOVA F-value\n    def select_features(X_train, y_train, X_test, k):\n\n    #     fs = SelectKBest(score_func=f_classif, k='all')\n        fs = SelectKBest(score_func=f_classif, k=k) #'k' best features\n\n        fs.fit(X_train, y_train)\n\n        X_train_fs = fs.transform(X_train)\n        X_test_fs = fs.transform(X_test)\n\n        return X_train_fs, X_test_fs, fs\n    \n    #Running the function above\n    X_train_fs, X_test_fs, fs = select_features(X_train, y_train, X_test, k=k)\n    \n    #Feed into pipeline\n    pipe = Pipeline(steps=[('scaler', MinMaxScaler()), ('model', LogisticRegression(solver='liblinear'))])\n    \n    #Fit the pipeline on our data that was selected using 'SelectKBest' and the defined 'k'\n    pipe.fit(X_train_fs, y_train)\n    \n    #Output the pipeline accuracy score\n    return pipe.score(X_test_fs, y_test)","d09d19a6":"score = ANOVA_F_val_scores(df, k='all')\nscore","200c1910":"score_25 = ANOVA_F_val_scores(df, k=25)\nscore_25","8fb5335a":"print(f'The score for the pipeline with k = 25 is: {score_25}\\nThe score for the pipeline with k = all is: {score}')","6883cf0f":"ks = range(10,31)\nscores = []\n[scores.append(ANOVA_F_val_scores(df, k)) for k in ks]","b6bf3c54":"sns.set(style=\"darkgrid\")\nsns.lineplot(x=ks, y=scores);\nplt.title('Plot of model scores vs K');\nplt.xlabel('K')\nplt.ylabel('Score');","953a379f":"X, y = split_data(df)\n\nf = SelectKBest(score_func=f_classif, k=25)\n\nf.fit(X, y)","2b0eb44d":"f_pvals = pd.DataFrame(np.round(f.pvalues_, 6))\nfeatures = pd.DataFrame(X.columns)\nf_scores = pd.DataFrame(f.scores_)\nscore_df = pd.concat([features, f_scores, f_pvals], axis=1)\n    \n#Assign column names\nscore_df.columns=['Input Features', 'F-scores', 'P-values']\n\n#Display the score_df and filter the rows based on p-values which are greater than .05 (i.e. not significant)\nscore_df[score_df['P-values'] > .05]","c67be17e":"from scipy.stats import pointbiserialr as pbs","5d5197c4":"#Re-initiating the original dataframe\n\noriginal_df = pd.read_csv('..\/input\/breast-cancer-wisconsin-data\/data.csv')\n\nwrapper = {'M':0, 'B':1}\noriginal_df.diagnosis = original_df.diagnosis.replace(wrapper)\n\noriginal_df = original_df.drop(['id', 'Unnamed: 32'], axis=1)","83f6c4b0":"# List comprehension to run the pointbiserialr SciPy function on all columns of the dataset\npbs_out = [pbs(y, original_df[col]) for col in original_df.columns[1:]]","b8f1a2d8":"pbs_r, pbs_pval = [], []\n\n#Creating a DataFrame to concatenate the results and view together (extracting the important values out from each tuple of pbs output)\npbs_r = pd.DataFrame([pbs_out[i][0] for i in range(len(pbs_out))])\npbs_pval = pd.DataFrame([pbs_out[i][1] for i in range(len(pbs_out))])\n\n#Concatenating the results\ndf_pbs = pd.concat([features, pbs_r, np.round(pbs_pval, 5)], axis = 1)\ndf_pbs.columns=['Input Features', 'R value (correlation coeff.)', 'P-value']\n\n#Filtering to view the rows with p-values < .05\ndf_pbs[df_pbs['P-value'] > .05]","fec45525":"#Get all the input feature row names, this will us to drop these columns from the main dataframe (df)\ncolumns_to_drop = df_pbs[df_pbs['P-value'] > .05]['Input Features'].tolist()\n\n#Perform the drop\noriginal_df = original_df.drop(columns_to_drop, axis=1)","7de347cb":"from sklearn.preprocessing import RobustScaler, StandardScaler\nfrom sklearn.model_selection import GridSearchCV\n\n\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.svm import SVC\nimport xgboost as xgb","d0f944e0":"def scale_data(scaler, df):\n    \n    #Split the data\n    X, y = split_data(df)\n    \n    #Split into training and test splits\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.20, random_state = 1)\n    \n    #Instantiate scaler object\n    if scaler == 'MinMaxScaler':\n        sc = MinMaxScaler()\n    elif scaler == 'StandardScaler':\n        sc = StandardScaler()\n    elif scaler == 'RobustScaler':\n        sc = RobustScaler()\n        \n    #Transform using given scaler\n    X_train = sc.fit_transform(X_train)\n    X_test = sc.transform(X_test)\n\n    #Output\n    return X_train, X_test, y_train, y_test","60b7b50d":"model_params = {\n    'svm': {\n        'model': SVC(gamma='auto'),\n        'params' : {\n            'C': [1,20],\n            'kernel': ['rbf','linear']\n        }  \n    },\n    'random_forest': {\n        'model': RandomForestClassifier(),\n        'params' : {\n            'n_estimators': [1,10,50],\n            'criterion':['gini', 'entropy']\n        }\n    },\n    'logistic_regression' : {\n        'model': LogisticRegression(multi_class='auto'),\n        'params': {\n            'C': [1,10,20,50],\n            'solver': ['newton-cg', 'liblinear', 'saga']\n        }\n    },\n    'decision_tree':{\n        'model': DecisionTreeClassifier(),\n        'params': {\n            'criterion':['gini', 'entropy'],\n        }\n    }\n}","dde5644d":"def run_gridsearch(X_train, y_train, X_test, y_test, model_params = model_params):\n    scores = []\n    \n    for model_name, mp in model_params.items():\n        clf =  GridSearchCV(mp['model'], mp['params'], cv=5, return_train_score=False)\n        clf.fit(X_train, y_train)\n        scores.append({\n            'model': model_name,\n            'best_score': clf.best_score_,\n            'best_params': clf.best_params_\n        })\n\n    df_model_scores = pd.DataFrame(scores,columns=['model','best_score','best_params'])\n    \n    return df_model_scores","a00fa32e":"X, y = split_data(original_df)\n    \nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.20)","bd3d1dea":"df_model_noscale_featureselection = run_gridsearch(X_train, y_train, X_test, y_test)","0b476a13":"df_model_noscale_featureselection","e852564f":"X, y = split_data(df)\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = .20)","8d48a72a":"df_model_noscale_defaultdata = run_gridsearch(X_train, y_train, X_test, y_test)","9f3c2091":"df_model_noscale_defaultdata","730b008d":"def run_GS_scale(scale, df):\n    X, y = split_data(df)\n\n    X_train, X_test, y_train, y_test = scale_data(scale, df)\n\n    df_scores = run_gridsearch(X_train, y_train, X_test, y_test)\n    \n    return df_scores","d9246e58":"df_MinMaxScaler_scores = run_GS_scale('MinMaxScaler', original_df)","f82493f8":"df_MinMaxScaler_scores","2b0c0860":"df_StandardScaler_scores = run_GS_scale('StandardScaler', original_df)","2cd7e30e":"df_StandardScaler_scores","f7cbf922":"df_RobustScaler_scores = run_GS_scale('RobustScaler', original_df)","e12fe394":"df_RobustScaler_scores","571e6ff8":"df_StandardScaler_scores","61b86a3e":"df_RobustScaler_scores","d4847081":"df_MinMaxScaler_scores","89231d65":"final_model = LogisticRegression(C = 20, solver = 'saga')","a091d568":"#Instantiate model with best hyperparameters\nmodel = LogisticRegression(C = 20, solver= 'saga')\n\n#Split the data and scale\nX_train, X_test, y_train, y_test = scale_data('RobustScaler', original_df)\n\n#Fit the model\nmodel.fit(X_train, y_train)\n\n#Score the model\nmodel.score(X_test, y_test)","dc417393":"y_preds = model.predict(X_test)","ea41897d":"print(f'The accuracy between X_test and y_test is: {model.score(X_test, y_test)}\\nThe accuracy between X_train and y_train is: {model.score(X_train, y_train)}')","54db9672":"print(classification_report(y_test, y_preds, target_names=['Metastasis', 'Benign']))","46b91106":"matrix = confusion_matrix(y_test, y_preds)\nsns.heatmap(matrix, annot=True, cbar=None, cmap='Blues')\nplt.title('Confusion Matrix'), plt.tight_layout()\nplt.ylabel('True Class'), plt.xlabel('Predicted Class')\nplt.show()\nprint('0: Metastasis (M)\\n1: Benign (B)')","c8920b50":"#### From this we can proceed, the best scaling is RobustScaler and the best model to use is a Logistic Regression with the parameters of C=20, and 'solver' = 'saga'. We initially tested with the Logistic Regression algorthim and we can continue to use this algorithim as it is out-performing many of the other classification algorithims such as SVM, and even tree-based algorithims like decision tree classifier and random forests","824e8648":"## Multicollinearity Analysis","eedf24d7":"'radius_mean' and 'perimeter_mean' show signs of multi-colinearity","d3796c38":"I've decided on picking a z-score of anything greater than 5.0 to be excluded from our model training. For reference, a z-score of 3 encompasses nearly 99% of the population.","5b9789fb":"#### Variance Inflation Factor (VIF)\nhttps:\/\/towardsdatascience.com\/everything-you-need-to-know-about-multicollinearity-2f21f082d6dc\n\nhttps:\/\/www.youtube.com\/watch?v=Mq2sce8TjLw","7eb3d374":"Let's start by dropping the 'id' column and the 'Unnamed: 32' column","74cb8e60":"### We're trying to predict the 'diagnosis' (i.e. is it Malignant, M, or is it Benign, B)","ef794648":"Same score, less features.\n\nTo summarize, the ANOVA F-Value was used here to reduce the number of features (from 30 to 25) whilst maintaining optimum model accuracy","8773950f":"### Let's improve on this!","c1263e07":"#### Output all three dataframes and view results","5fd24253":"# Model Development","dd866cbf":"To conclude, we have definitely improved upon the accuracy of this model via the help of some outlier analysis techniques and feature selection techniques. Briefly summarizing, we performed brief EDA and dived deeper into the dataset by finding out which features were correlated strongly enough with the dependent variable ('diagnosis' in this case).\n\n\nThe model accuracy at the start with no hyperparameter tuning was improved due to hyperparameter tuning and feature selection techniques","8e8ac30c":"## Additional Feature Exploration methods","5f8ccc55":"No signs of extreme over-fitting either, so that's good","74024870":"'radius_worst' and 'perimeter_worst' show signs of multi-colinearity","76f996b4":"#### Z-score analysis\n(make new DF's for z-scores and VIF's and then filter the original DF based on those [i.e., if it falls within the threshold or not])","31f48795":"# Loading data in","2270ccfe":"## Outlier analysis\n#### Box plots","bcc2b08a":"# Model Evaluation \/ Classification Report","00fb14c0":"#### Let's make our final model with the hyperparameters listed above and run some additional analysis","230c0a2c":"#### Point-Biserial Correlation\nhttps:\/\/medium.com\/@outside2SDs\/an-overview-of-correlation-measures-between-categorical-and-continuous-variables-4c7f85610365\n\nhttps:\/\/www.youtube.com\/watch?v=AlFZlhVLdJs\n\nhttps:\/\/towardsdatascience.com\/point-biserial-correlation-with-python-f7cd591bd3b1","996d609f":"#### XGBoost performed the best with our feature selection embedded (i.e. ANOVA F-value & PBC) let's re-run our pipeline now with scaling implemented.\n1. StandardScaler()\n2. RobustScaler()\n3. MinMaxScaler()","0c46a720":"#### Metrics for assessing model\nHyperparameter optimization","a2eef271":"While there are lots of values that are seemingly outliers, they are inherently not. However, some of the columns that we can choose to look in to are of the following:\n\nfractal_dimension_worst, symmetry_worst, fractal_dimension_se, symmetry_se, concave points_se, concavity_se, compactness_se, smoothness_se, area_se, perimeter_se, texture_se, radius_se\n","3e9e387a":"Comparing this result to the previous DataFrame up above (ANOVA F-Value) we actually see nearly identical results which makes sense. These are the features in our model data that are of no use to us and can be safely discarded (i.e. they have little to no correlation to the diagnosis)","a095f3fe":"#### The goal is simply to remove any features which are seemingly unnecessary to model performance. In this case we will get rid 5 features since they have p-values greater than .05 and fail to reject the hypothesis.","1ef04c9e":"Like the ANOVA F-value, the point-biserial correlation tells a story of the data and the correlations within it. Ideally, it's used in the cases in which you have a binary variable to predict (i.e. in this case, it's diagnosis) and we have continuous data. \n\nWe'll be using the SciPy library for this as it has a very nice built in function. \n\n\"The point biserial correlation is used to measure the relationship between a binary variable, x, and a continuous variable, y. Like other correlation coefficients, this one varies between -1 and +1 with 0 implying no correlation. Correlations of -1 or +1 imply a determinative relationship.\" - SciPy Documentation (https:\/\/docs.scipy.org\/doc\/scipy-0.14.0\/reference\/generated\/scipy.stats.pointbiserialr.html)","b1bb2ef6":"#### You can see here that as 'k' increases our model score tends to increase. In this case however, our peak model performance is reached at k=25. So anything beyond ~25 doesn't really help us increase our model accuracy. So we should set our 'k' in SelectKBest to be 25.","eb47d178":"#### ANOVA F-Value Analysis","56a387f3":"# Back to the drawing board - EDA","07566bde":"#### Making pipeline","a7d6c336":"'radius_se' and 'perimeter_se' show signs of multi-colinearity","075d153b":"# Pre-processing","0d491716":"## Metrics for evaluation","2d38d0e5":"## Splitting and scaling data","3b678524":"# Creating a baseline model for which to improve upon later"}}