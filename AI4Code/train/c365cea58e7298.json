{"cell_type":{"5dac6dc4":"code","613e5ec2":"code","e9a505ec":"code","ddbc0676":"code","39bb31ca":"code","ae4ff580":"code","187a3af8":"code","9a31f007":"code","14934f49":"code","11394d32":"code","a7eb1f4f":"code","93ba1793":"code","308cb5c7":"code","938e36a0":"code","82e4f68b":"code","2e0eae6c":"code","ad69d3fe":"code","08c0c0ef":"code","b2a30460":"code","bfbcc1c9":"code","717fdf65":"code","bca0a6cb":"code","08288232":"code","2faae7c2":"code","7f2fddb5":"code","8b2e78da":"code","74005ba7":"code","0dba749f":"code","7b2ca85e":"code","3bcf6d76":"code","2d91d42a":"code","35c5354b":"code","a85e3135":"code","a4734db2":"code","a7a75449":"code","f7485821":"code","a81245a3":"code","5243395f":"code","5e224632":"code","8f83f9e0":"code","942a7fbc":"code","f9239ab9":"code","caee0641":"code","f5cf63d8":"code","16661dee":"code","a40ce4e9":"code","ce80d061":"code","6795e406":"code","94f4e78e":"code","4014f857":"code","fefc62ea":"code","999edf0e":"code","9a1a7bb1":"code","ffd30d59":"code","af8b6f89":"code","930a6340":"code","56444f8b":"code","0bdf1076":"code","8e0ca0d8":"code","2d3e3ec8":"code","0197f89a":"code","e97506e1":"code","fb1ca7a0":"code","f3a02181":"code","140fda27":"code","dd472291":"code","a222a691":"code","b102fff4":"code","28de7a24":"code","ca473c4d":"code","df8cdc91":"code","679ee6ff":"code","722aebfb":"code","c957a7a8":"code","0f26a117":"code","e0f8bd92":"code","3bc341b0":"code","142a3800":"code","ab93f2c7":"code","d769d24e":"code","4a56546b":"code","e63e5a2b":"code","f63022da":"code","1c69fa64":"code","b990553a":"code","ece6d16f":"code","db25a36d":"code","29ea78c8":"code","e0d37ac2":"code","a1d897ac":"code","d3ed2a83":"code","ed8f6727":"code","a858ec9a":"code","a8923ed1":"code","5d672913":"code","ae0026af":"code","4eb10be0":"code","2d0caff8":"code","c28278c9":"code","249d9ca2":"code","5cc5cc38":"code","f62ecd58":"code","538b710f":"code","b299ae81":"code","8805905a":"code","b0329533":"code","0a295d3d":"code","c1c82db4":"code","3f5fd35e":"code","c7a47e0b":"code","3b11ca83":"code","bed8563e":"code","3f989af0":"code","ba06ec42":"code","d8431ed3":"code","2e4a69a1":"code","a649d7bd":"code","d11e18e6":"code","5a79aa79":"code","05a05d4c":"code","df760f4e":"code","55ca7ea4":"code","96e0c679":"code","38117678":"code","d38a95a0":"code","1c6392f0":"code","5ae81ce6":"code","57dc5c5a":"code","5b1df96a":"code","ce43d8f0":"code","734bb01d":"code","d4845a39":"code","594e96bd":"code","5495abbf":"code","d685e925":"code","002f583b":"code","c84a046a":"code","8e216873":"code","b932505c":"code","29a9de0b":"code","16a7e51b":"code","69475e88":"markdown","4389399f":"markdown","7195a569":"markdown","7bd456af":"markdown","c79fd839":"markdown","2a754708":"markdown","b4987838":"markdown","88c5b650":"markdown","eb55ba67":"markdown","b5909040":"markdown","aa8fb6b4":"markdown","18c8f4c6":"markdown","371e1363":"markdown","be524b24":"markdown","44e27eaa":"markdown","503ba6cb":"markdown","606a2d4d":"markdown","3754a81f":"markdown","870c7613":"markdown","6bcea560":"markdown","5634b692":"markdown","b7d85683":"markdown","838aa830":"markdown","26b13048":"markdown","091a2406":"markdown","12020037":"markdown","56d54a56":"markdown","a7aa2163":"markdown","ed4cbd88":"markdown","ba94d488":"markdown","9d9aecc1":"markdown","aa57d205":"markdown","0880bf78":"markdown","9c420bcc":"markdown","c9d0340b":"markdown","f42a31eb":"markdown","851736e2":"markdown","1429f866":"markdown","d1235d61":"markdown","f5ab2c74":"markdown","58ae4a36":"markdown","551e2091":"markdown","33324a0f":"markdown","915fd943":"markdown"},"source":{"5dac6dc4":"pwd","613e5ec2":"import warnings\nwarnings.filterwarnings('ignore')\nimport os\nimport pandas as pd\nfrom pandas import DataFrame\nimport pylab as pl\nimport numpy as np\nimport seaborn as sns\nfrom scipy import stats\nimport matplotlib.pyplot as plt\n%matplotlib inline","e9a505ec":"import plotly.express as px\nimport plotly.graph_objects as go\nimport plotly.figure_factory as ff\nfrom plotly.colors import n_colors\nfrom plotly.subplots import make_subplots","ddbc0676":"kaggle=pd.read_csv(\"..\/input\/house-prices-advanced-regression-techniques\/train.csv\")\n#kaggle_test=pd.read_csv(\"C:\\\\Users\\\\ARPIT\\\\Desktop\\\\New folder\\\\Kaggle Housing\\\\test_housing.csv\")","39bb31ca":"kaggle.head()","ae4ff580":"kaggle.info()","187a3af8":"numeric_data = kaggle.select_dtypes(include=[np.number])\ncategorical_data = kaggle.select_dtypes(exclude=[np.number])\nprint(\"Numeric_Column_Count =\", numeric_data.shape)\nprint(\"Categorical_Column_Count =\", categorical_data.shape)","9a31f007":"kag=categorical_data.nunique()\nfig = go.Figure(go.Bar(x=categorical_data,y=kag,))\nfig.update_layout(title_text='Unique values in Categorical Columns',xaxis_title=\"Category\",yaxis_title=\"Count of Unique Values\")\nfig.show()","14934f49":"for k, v in numeric_data.items():\n    q1 = v.quantile(0.25)\n    q3 = v.quantile(0.75)\n    irq = q3 - q1\n    v_col = v[(v <= q1 - 1.5 * irq) | (v >= q3 + 1.5 * irq)]\n    perc = np.shape(v_col)[0] * 100.0 \/ np.shape(numeric_data)[0]\n    print(\"Column %s outliers = %.2f%%\" % (k, perc))","11394d32":"plt.figure(figsize=(15,10))\nkaggle.boxplot(patch_artist=True,vert=False)","a7eb1f4f":"import missingno as msno\nmsno.bar(kaggle)\ntotal = kaggle.isnull().sum().sort_values(ascending=False)\npercent_1 = kaggle.isnull().sum()\/kaggle.isnull().count()*100\npercent_2 = (round(percent_1, 1)).sort_values(ascending=False)\nmissing_data = pd.concat([total, percent_2], axis=1, keys=['Total','%'])","93ba1793":"def msv1(data, thresh=20, color='black', edgecolor='black', width=15, height=5):\n    plt.figure(figsize=(width,height))\n    percentage=(kaggle.isnull().mean())*100\n    percentage.sort_values(ascending=False).plot.bar(color=color, edgecolor=edgecolor)\n    plt.axhline(y=thresh, color='r', linestyle='-')\n    plt.title('Missing values percentage per column', fontsize=20, weight='bold' )\n    plt.text(len(data.isnull().sum()\/len(data))\/1.7, thresh+12.5, f'Columns with more than {thresh}% missing values', fontsize=12, color='crimson',\n         ha='left' ,va='top')\n    plt.text(len(data.isnull().sum()\/len(data))\/1.7, thresh - 5, f'Columns with less than {thresh}% missing values', fontsize=12, color='green',\n         ha='left' ,va='top')\n    plt.xlabel('Columns', size=15, weight='bold')\n    plt.ylabel('Missing values percentage')\n    plt.yticks(weight ='bold')\n    \n    return plt.show()\nmsv1(kaggle, 20, color=('black', 'deeppink'))\n### ","308cb5c7":"total_missing=kaggle.isnull().sum().sort_values(ascending=False)\npercent=(kaggle.isnull().sum()\/kaggle.isnull().count()).sort_values(ascending=False)\nmissing_data=pd.concat([total_missing,percent],axis=1,keys=['Missing_Total','Percent'])\nmissing_data.head(5)","938e36a0":"allna = (kaggle.isnull().sum() \/ len(kaggle))*100\nallna = allna.drop(allna[allna == 0].index).sort_values()\nNA=kaggle[allna.index.to_list()]\nNAcat=NA.select_dtypes(include='object')\nNAnum=NA.select_dtypes(exclude='object')\nprint(f'We have :{NAcat.shape[1]} categorical features with missing values')\nprint(f'We have :{NAnum.shape[1]} numerical features with missing values')","82e4f68b":"#all_data_na = (kaggle.isnull().sum() \/ len(kaggle)) * 100\n#all_data_na = all_data_na.drop(all_data_na[all_data_na == 0].index).sort_values(ascending=False)[:30]\n#missing_data = pd.DataFrame({'Missing Ratio' :all_data_na})\n#missing_data.head(20)","2e0eae6c":"my_corr=kaggle.corr()\nplt.figure(figsize=(20,20))\nsns.set(font_scale=0.8)\nsns.heatmap(my_corr, cbar=False, annot=True, square=True, fmt='.2f', annot_kws={'size': 10},linewidth=0.8)\nplt.show()","ad69d3fe":"num=kaggle.select_dtypes(exclude='object')\nnumcorr=num.corr()\nf,ax=plt.subplots(figsize=(20,1))\nsns.heatmap(numcorr.sort_values(by=['SalePrice'], ascending=False).head(1), cmap='Blues')\nplt.title(\" Numerical features correlation with the sale price\", weight='bold', fontsize=18)\nplt.xticks(weight='bold')\nplt.yticks(weight='bold', color='dodgerblue', rotation=0)\nplt.show()","08c0c0ef":"cor_target = abs(my_corr[\"SalePrice\"])\n#Selecting highly correlated features\nrelevant_features = cor_target[cor_target>0.5]\nrelevant_features","b2a30460":"cor_target =kaggle.corr().abs()\nTarget_Corr = cor_target.corr()['SalePrice'].to_frame().reset_index() #Feature Correlation related to SalePrice\nFeature_corr =cor_target.unstack().to_frame(name='Correlation') # Feature Relation\nFeature = Feature_corr[(Feature_corr['Correlation']>=0.8)&(Feature_corr['Correlation']<1)].sort_values(by='Correlation', ascending = False).reset_index()\nFeature.head(10)\n","bfbcc1c9":"kaggle.groupby('Neighborhood', as_index=True)['SalePrice'].mean()","717fdf65":"plt.figure(figsize=(20,12))\nplt.yticks(weight='bold')\nkaggle['Neighborhood'].value_counts().plot(kind=\"barh\",color='coral', fontsize=16)\nplt.title('Most frequent neighborhoods',fontsize=22, loc='center')","bca0a6cb":"import plotly.graph_objects as go\nfrom plotly.subplots import make_subplots\n\nlabels = kaggle['BldgType']\nfig = make_subplots(rows=1, cols=2, specs=[[{'type':'domain'}, {'type':'domain'}]])\nvalues = kaggle['LotFrontage']\nvalues1= kaggle['MSSubClass']\nfig.add_trace(go.Pie(labels=labels, values=values, name=\"Lot Frontage\"),1, 1)\nfig.add_trace(go.Pie(labels=labels, values=values1, name=\"MSSubClass\"),1, 2)\nfig.update_traces(hole=.5, hoverinfo=\"label+percent+name\")\nfig.update_layout(title_text=\"Class& Frontage dependency on Building Type\")\nannotations=([dict(text='LotFrontage', x=0.18, y=0.3, font_size=20, showarrow=False),\n                 dict(text='MSSubClass', x=0.82, y=0.3, font_size=20, showarrow=False)])\nfig.update_layout(margin=dict(t=0, b=0, l=0, r=0))\nfig.show()\n#fig = go.Figure(data=[go.Pie(labels=labels, values=values,hole=.5)])\n","08288232":"import plotly.graph_objects as go\nlabels = kaggle['SaleType']\nvalues = kaggle['GrLivArea']\nfig = go.Figure(data=[go.Pie(labels=labels, values=values,hole=0.0)])\nfig.update_layout(margin=dict(t=0, b=0, l=0, r=0))\nfig.show()","2faae7c2":"import plotly.graph_objects as go\nlabels = kaggle['Foundation']\nvalues = kaggle['SalePrice']\nfig = go.Figure(data=[go.Pie(labels=labels, values=values,hole=0.0)])\nfig.update_layout(margin=dict(t=0, b=0, l=0, r=0))\nfig.show()","7f2fddb5":"import plotly.graph_objects as go\nlabels = kaggle['MoSold']\nvalues = kaggle['SalePrice']\nfig = go.Figure(data=[go.Pie(labels=labels, values=values,hole=0.5)])\nfig.update_layout(margin=dict(t=0, b=0, l=0, r=0))\nfig.show()","8b2e78da":"pd.crosstab([kaggle.Heating,kaggle.Electrical],kaggle.KitchenAbvGr,margins=True).style.background_gradient(cmap='Wistia')","74005ba7":"pearson_coef, p_value = stats.pearsonr(kaggle['LotArea'], kaggle['SalePrice'])\nprint(\"The Pearson Correlation Coefficient of LotArea is\", pearson_coef, \" with a P-value of P =\", p_value)  \nsns.regplot(x=\"LotArea\", y=\"SalePrice\", data=kaggle)\nplt.ylim(0,)","0dba749f":"pearson_coef, p_value = stats.pearsonr(kaggle['GarageArea'], kaggle['SalePrice'])\nprint(\"The Pearson Correlation Coefficient of GarageArea is\", pearson_coef, \" with a P-value of P =\", p_value)  \nsns.regplot(x=\"GarageArea\", y=\"SalePrice\", data=kaggle)\nplt.ylim(0,)","7b2ca85e":"pearson_coef, p_value = stats.pearsonr(kaggle['GrLivArea'], kaggle['SalePrice'])\nprint(\"The Pearson Correlation Coefficient of GrLivArea is\", pearson_coef, \" with a P-value of P =\", p_value)  \nsns.regplot(x=\"GrLivArea\", y=\"SalePrice\", data=kaggle)\nplt.ylim(0,)","3bcf6d76":"import matplotlib.gridspec as gridspec\ndef multi_plotting (kaggle, feature): \n\n    fig = plt.figure(constrained_layout=True, figsize=(12,8))\n    grid = gridspec.GridSpec(ncols=3, nrows=3, figure=fig)\n\n    ax1 = fig.add_subplot(grid[0, :2])\n    ax1.set_title('Histogram')\n    sns.distplot(kaggle.loc[:,feature], norm_hist=True, ax = ax1)\n\n    ax2 = fig.add_subplot(grid[1, :2])\n    ax2.set_title('QQ_plot')\n    stats.probplot(kaggle.loc[:,feature], plot = ax2)\n\n    ax3 = fig.add_subplot(grid[:, 2])\n    ax3.set_title('Box Plot')\n    sns.boxplot(kaggle.loc[:,feature], orient='v', ax = ax3 );\n\n    print(\"Skewness: \"+ str(kaggle['SalePrice'].skew().round(3))) \n    print(\"Kurtosis: \" + str(kaggle['SalePrice'].kurt().round(3)))","2d91d42a":"multi_plotting (kaggle,'SalePrice')","35c5354b":"kaggle[\"SalePrice\"] = np.log1p(kaggle[\"SalePrice\"])\nmulti_plotting (kaggle,'SalePrice')","a85e3135":"from scipy import stats\nimport numpy as np\nz = np.abs(stats.zscore(numeric_data))\nprint(z)","a4734db2":"threshold = 3\n#print(np.where(z > 3))","a7a75449":"kaggle['SalePrice'].quantile([0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1])","f7485821":"import matplotlib.gridspec as gridspec\ndef multi_plotting (kaggle, feature): \n\n    fig = plt.figure(constrained_layout=True, figsize=(12,8))\n    grid = gridspec.GridSpec(ncols=3, nrows=3, figure=fig)\n\n    ax1 = fig.add_subplot(grid[0, :2])\n    ax1.set_title('Histogram')\n    sns.distplot(kaggle.loc[:,feature], norm_hist=True, ax = ax1)\n\n    ax2 = fig.add_subplot(grid[1, :2])\n    ax2.set_title('QQ_plot')\n    stats.probplot(kaggle.loc[:,feature], plot = ax2)\n\n    ax3 = fig.add_subplot(grid[:, 2])\n    ax3.set_title('Box Plot')\n    sns.boxplot(kaggle.loc[:,feature], orient='v', ax = ax3 );\n\n    print(\"Skewness: \"+ str(kaggle['LotArea'].skew().round(3))) \n    print(\"Kurtosis: \" + str(kaggle['LotArea'].kurt().round(3)))\nmulti_plotting (kaggle,'LotArea')","a81245a3":"from yellowbrick.regressor import CooksDistance\n# Load the regression dataset\nX = kaggle[\"LotArea\"].values.reshape(-1,1)\nY = kaggle[\"SalePrice\"]\n# Instantiate and fit the visualizer\nvisualizer = CooksDistance()\nvisualizer.fit(X, Y)\nvisualizer.show()","5243395f":"#kaggle['LotArea'].quantile([0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1])","5e224632":"q = kaggle['LotArea'].quantile(0.97)\nq","8f83f9e0":"kaggle[kaggle.LotArea > q]","942a7fbc":"avg =kaggle['LotArea'].mean()","f9239ab9":"r=kaggle.loc[(kaggle[\"LotArea\"]>q), 'LotArea'] = avg # Replacing the Row with Base Price less than q with Average Price\n#r ","caee0641":"#sns.boxplot( y=kaggle[\"LotArea\"])\nfig = go.Figure(go.Box(y=kaggle[\"LotArea\"],name=\"Lot Area\")) # to get Horizonal plot change axis :  x=germany_score\nfig.update_layout(title=\"Distribution of Datapoints\")\nfig.show()","f5cf63d8":"kaggle=kaggle.drop(['PoolQC','MiscFeature','Alley','Fence','LotFrontage'],axis=1,inplace=False)","16661dee":"kaggle=kaggle.drop(['GarageCars','GarageYrBlt','TotRmsAbvGrd','YearRemodAdd'],axis=1,inplace=False) #Not of USe","a40ce4e9":"kaggle.shape","ce80d061":"kaggle['House_Age']=kaggle['YrSold']-kaggle['YearBuilt']","6795e406":"kaggle=kaggle.drop(['YrSold','YearBuilt'],axis=1,inplace=False)","94f4e78e":"kaggle.shape","4014f857":"kaggle['MasVnrArea']=kaggle.MasVnrArea.fillna(0)","fefc62ea":"kaggle.head()","999edf0e":"kaggle['TotalLivingSF'] = kaggle['GrLivArea'] + kaggle['TotalBsmtSF'] - kaggle['LowQualFinSF']","9a1a7bb1":"kaggle=kaggle.drop(['BsmtFinSF1','BsmtFinSF2','GrLivArea','TotalBsmtSF','LowQualFinSF','FireplaceQu'],axis=1,inplace=False)\nkaggle.shape","ffd30d59":"kaggle=kaggle.drop(['1stFlrSF','2ndFlrSF'],axis=1,inplace=False)\nkaggle.shape","af8b6f89":"from yellowbrick.regressor import CooksDistance\n# Load the regression dataset\nX = kaggle[\"TotalLivingSF\"].values.reshape(-1,1)\nY = kaggle[\"SalePrice\"]\n# Instantiate and fit the visualizer\nvisualizer = CooksDistance()\nvisualizer.fit(X, Y)\nvisualizer.show()","930a6340":"fig = go.Figure(data=go.Violin(y=kaggle[\"TotalLivingSF\"], box_visible=True, line_color='black',\n                               meanline_visible=True, fillcolor='lightseagreen', opacity=0.8,\n                               x0='Total Living Area'))\nfig.update_layout(yaxis_zeroline=False,title=\"Distribution of Derived Feature-Total Living Area \")\nfig.show()","56444f8b":"q = kaggle['TotalLivingSF'].quantile(0.98)\nq","0bdf1076":"kaggle[kaggle.TotalLivingSF > q]","8e0ca0d8":"avg =kaggle['TotalLivingSF'].mean()","2d3e3ec8":"r=kaggle.loc[(kaggle[\"TotalLivingSF\"]>q), 'TotalLivingSF'] = avg # Replacing the Row with Base Price less than q with Average Price\nr ","0197f89a":"fig = go.Figure(go.Box(y=kaggle[\"TotalLivingSF\"],name=\"Lot Area\")) # to get Horizonal plot change axis :  x=germany_score\nfig.update_layout(title=\"Distribution of Datapoints\")\nfig.show()","e97506e1":"kaggle['MSZoning'] = kaggle.groupby(['Neighborhood', 'MSSubClass'])['MSZoning'].apply(lambda x: x.fillna(x.value_counts().index[0]))\nkaggle['Utilities'] = kaggle.groupby(['Neighborhood', 'MSSubClass'])['Utilities'].apply(lambda x: x.fillna(x.value_counts().index[0]))\nkaggle['Exterior1st'] = kaggle.groupby(['Neighborhood', 'MSSubClass'])['Exterior1st'].apply(lambda x: x.fillna(x.value_counts().index[0]))\nkaggle['Exterior2nd'] = kaggle.groupby(['Neighborhood', 'MSSubClass'])['Exterior2nd'].apply(lambda x: x.fillna(x.value_counts().index[0]))\nkaggle['MasVnrType'] = kaggle.groupby(['Neighborhood', 'MSSubClass'])['MasVnrType'].apply(lambda x: x.fillna(x.value_counts().index[0]))\nfor col in ['BsmtQual','BsmtCond','BsmtExposure','BsmtFinType1','BsmtFinType2','GarageType','GarageFinish','GarageQual','GarageCond']:\n    kaggle[col] = kaggle[col].fillna('None')\nkaggle['Electrical'] = kaggle.groupby(['Neighborhood','MSSubClass' ])['Electrical'].apply(lambda x: x.fillna(x.value_counts().index[0]))\nkaggle['KitchenQual'] = kaggle.groupby(['Neighborhood','MSSubClass' ])['KitchenQual'].apply(lambda x: x.fillna(x.value_counts().index[0]))\nkaggle['Functional'] = kaggle.groupby(['Neighborhood', 'MSSubClass'])['Functional'].apply(lambda x: x.fillna(x.value_counts().index[0]))\nkaggle['SaleType'] = kaggle.groupby(['Neighborhood', 'MSSubClass'])['SaleType'].apply(lambda x: x.fillna(x.value_counts().index[0]))","fb1ca7a0":"numeric_data = kaggle.select_dtypes(include=[np.number])\ncategorical_data = kaggle.select_dtypes(exclude=[np.number])\nprint(\"Numeric_Column_Count =\", numeric_data.shape)\nprint(\"Categorical_Column_Count =\", categorical_data.shape)","f3a02181":"from sklearn.preprocessing import LabelEncoder","140fda27":"labelencoder = LabelEncoder()","dd472291":"categorical_data=categorical_data.apply(LabelEncoder().fit_transform)","a222a691":"#X=X.apply(LabelEncoder().fit_transform)","b102fff4":"categorical_data.head()","28de7a24":"categorical_data=categorical_data.astype(int)","ca473c4d":"#categorical_data.dtypes","df8cdc91":"Kag_new = pd.concat([numeric_data, categorical_data],axis=1)","679ee6ff":"Kag_new.info()","722aebfb":"Kag_new=Kag_new.drop(['Id'],axis=1,inplace=False)","c957a7a8":"X=Kag_new.drop(['SalePrice'],axis=1,inplace=False)\nY=Kag_new['SalePrice']","0f26a117":"from statsmodels.tools.tools import add_constant\nfrom statsmodels.stats.outliers_influence import variance_inflation_factor\nX_vif = add_constant(X)\nvif = pd.Series([variance_inflation_factor(X_vif.values, i) \n               for i in range(X_vif.shape[1])], \n              index=X_vif.columns)","e0f8bd92":"print(vif.sort_values(ascending = False).head(20))","3bc341b0":"from sklearn import preprocessing\nscaler = preprocessing.StandardScaler()","142a3800":"X = scaler.fit_transform(X)\nX","ab93f2c7":"import sklearn.linear_model as linear_model\nimport sklearn.model_selection as model_selection\nfrom sklearn.linear_model import Ridge\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.ensemble import GradientBoostingRegressor\nfrom sklearn.ensemble import AdaBoostRegressor\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n","d769d24e":"from sklearn.linear_model import LinearRegression","4a56546b":"from sklearn.model_selection import train_test_split","e63e5a2b":"X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.30, random_state=123)","f63022da":"from sklearn.linear_model import LinearRegression","1c69fa64":"my_model = LinearRegression(normalize=True)  #Create an object of LinearRegression class.","b990553a":"my_model.fit(X_train, Y_train)               #Fitting the linear regression model to our training set.","ece6d16f":"predictions = my_model.predict(X_test)       #Make predictions on the test set","db25a36d":"pd.DataFrame({'actual value': Y_test, 'predictions':predictions}).sample(5)   #Compare a sample of 5 actual Y values from test set and corresponding predicted values ","29ea78c8":"linear=my_model.score(X_test, Y_test)           #Check the  R2  value\nlinear","e0d37ac2":"my_model.coef_","a1d897ac":"my_model.intercept_","d3ed2a83":"from sklearn import metrics","ed8f6727":"print('MAE',metrics.mean_absolute_error(Y_test,predictions))\nprint('MSE',metrics.mean_squared_error(Y_test,predictions))\nprint('RMSE',np.sqrt(metrics.mean_squared_error(Y_test,predictions)))","a858ec9a":"actuals = Y_test.values","a8923ed1":"## Actual vs Predicted plot\nplt.plot(actuals,\"b\")\nplt.plot(predictions,\"g\")\nplt.title('Actual vs Predicted')\nplt.show()\n\n# Visualzing actual vs predicted \nfig, ax = plt.subplots()\nax.scatter(Y_test, predictions, edgecolors=(0, 0, 0))\nax.plot([Y_test.min(), Y_test.max()], [Y_test.min(), Y_test.max()], 'k--', lw=4)\nax.set_xlabel('Actual')\nax.set_ylabel('Predicted')\nax.set_title(\"Actual vs Predicted\")\nplt.show()","5d672913":"from yellowbrick.regressor import ResidualsPlot\nvisualizer = ResidualsPlot(my_model,hist=False)\nvisualizer.fit(X_train, Y_train)  # Fit the training data to the visualizer\nvisualizer.score(X_test,Y_test)  # Evaluate the model on the test data\nvisualizer.show() ","ae0026af":"regR=linear_model.Ridge(normalize=True)\nregR=regR.fit(X_train,Y_train)","4eb10be0":"GregR=model_selection.GridSearchCV(regR,param_grid={'alpha':np.arange(1,1000,100)})\nGregR=GregR.fit(X_train,Y_train)","2d0caff8":"GregR.best_params_","c28278c9":"metrics.r2_score(Y_test,GregR.predict(X_test))","249d9ca2":"print(metrics.mean_squared_error(Y_test,GregR.predict(X_test)))","5cc5cc38":"reg=linear_model.Lasso(max_iter=1000,normalize=True)\nreg=reg.fit(X_train,Y_train)","f62ecd58":"Greg=model_selection.GridSearchCV(reg,param_grid={'alpha':np.arange(0.1,100,1).tolist()})\nGreg=Greg.fit(X_train,Y_train)","538b710f":"Greg.best_params_","b299ae81":"Greg.cv_results_","8805905a":"print(metrics.mean_squared_error(Y_test,GregR.predict(X_test)))","b0329533":"nEstimator = [140,160,180,200,220,240,260,280,300,320,340,360]\ndepth = [10,15,20,25,30,35,40,45,50,55,60]","0a295d3d":"RF = RandomForestRegressor()\nhyperParam = [{'n_estimators':nEstimator,'max_depth': depth}]\ngsv = GridSearchCV(RF,hyperParam,cv=5,verbose=1,scoring='r2',n_jobs=-1)\ngsv.fit(X_train, Y_train)\nprint(\"Best HyperParameter: \",gsv.best_params_)\nprint(gsv.best_score_)\nscores = gsv.cv_results_['mean_test_score'].reshape(len(nEstimator),len(depth))\nplt.figure(figsize=(8, 8))\nplt.subplots_adjust(left=.2, right=0.95, bottom=0.15, top=0.95)\nplt.imshow(scores, interpolation='nearest', cmap=plt.cm.hot)\nplt.xlabel('n_estimators')\nplt.ylabel('max_depth')\nplt.colorbar()\nplt.xticks(np.arange(len(nEstimator)), nEstimator)\nplt.yticks(np.arange(len(depth)), depth)\nplt.title('Grid Search r^2 Score')\nplt.show()\nmaxDepth=gsv.best_params_['max_depth']\nnEstimators=gsv.best_params_['n_estimators']","c1c82db4":"model = RandomForestRegressor(n_estimators = nEstimators,max_depth=maxDepth)\nmodel.fit(X_train, Y_train)","3f5fd35e":"y_pred = model.predict(X_test)\nprint('Root means score', np.sqrt(mean_squared_error(Y_test, y_pred)))\nprint('Variance score: %.2f' % r2_score(Y_test, y_pred))\nprint(\"Result :\",model.score(X_test, Y_test))\nRF_HyperScore = r2_score(Y_test, y_pred)\nRF_HyperScore","c7a47e0b":"gb_model = GradientBoostingRegressor(max_depth=30, n_estimators=250, learning_rate=0.01, random_state=123)","3b11ca83":"gb_model.fit(X_train, Y_train)","bed8563e":"predictions = gb_model.predict(X_test)","3f989af0":"Ensemble_GB_Score = r2_score(Y_test, predictions)\nEnsemble_GB_Score","ba06ec42":"print('RMSE',np.sqrt(metrics.mean_squared_error(Y_test,predictions)))","d8431ed3":"# Visualzing actual vs predicted \nfig, ax = plt.subplots()\nax.scatter(Y_test, predictions, edgecolors=(0, 0, 0))\nax.plot([Y_test.min(), Y_test.max()], [Y_test.min(), Y_test.max()], 'k--', lw=4)\nax.set_xlabel('Actual')\nax.set_ylabel('Predicted')\nax.set_title(\"Actual vs Predicted\")\nplt.show()","2e4a69a1":"gb_model.feature_importances_","a649d7bd":"ada_model = AdaBoostRegressor(DecisionTreeRegressor(max_depth=8), n_estimators=200, learning_rate=0.01, random_state=123)","d11e18e6":"ada_model.fit(X_train, Y_train)","5a79aa79":"predictions = ada_model.predict(X_test)","05a05d4c":"AdaBoostScore=r2_score(Y_test, predictions)\nAdaBoostScore","df760f4e":"print('RMSE',np.sqrt(metrics.mean_squared_error(Y_test,predictions)))","55ca7ea4":"tree = DecisionTreeRegressor(max_depth=10)","96e0c679":"tree.fit(X_train, Y_train)","38117678":"predictions = tree.predict(X_test)","d38a95a0":"actuals = Y_test.values","1c6392f0":"## Actual vs Predicted plot\nplt.plot(actuals,\"b\")\nplt.plot(predictions,\"g\")\nplt.title('Actual vs Predicted')\nplt.show()\n\n# Visualzing actual vs predicted \nfig, ax = plt.subplots()\nax.scatter(Y_test, predictions, edgecolors=(0, 0, 0))\nax.plot([Y_test.min(), Y_test.max()], [Y_test.min(),Y_test.max()], 'k--', lw=4)\nax.set_xlabel('Actual')\nax.set_ylabel('Predicted')\nax.set_title(\"Actual vs Predicted\")\nplt.show()","5ae81ce6":"DecisonTreeRegressor_Score = r2_score(Y_test, predictions)\nDecisonTreeRegressor_Score","57dc5c5a":"print('RMSE',np.sqrt(metrics.mean_squared_error(Y_test,predictions)))","5b1df96a":"tree_score = tree.score(X_train,Y_train)\ntree_score","ce43d8f0":"param_grid = [{\"max_depth\":[3, 4, 5,6,7,8,9,10, None], \"max_features\":[5,6,7,8,9,10,11,12,13,14,15,16]}]","734bb01d":"gs = GridSearchCV(estimator=DecisionTreeRegressor(random_state=123),\\\n                 param_grid = param_grid,\\\n                 cv=15)","d4845a39":"gs.fit(X_train, Y_train)","594e96bd":"gs.cv_results_['params']","5495abbf":"gs.best_params_","d685e925":"gs.cv_results_['rank_test_score']","002f583b":"gs.best_estimator_","c84a046a":"predictions2 = gs.predict(X_test)","8e216873":"DTGridSearchCGScore = r2_score(Y_test, predictions2)\nDTGridSearchCGScore","b932505c":"np.any(np.isnan(Test))","29a9de0b":"Test.fillna(0,inplace=True)","16a7e51b":"np.all(np.isfinite(Test))\n","69475e88":"### Random Forest with Hyper Parameter Tuning","4389399f":"#### GridSearchCV in Decision Tree","7195a569":"#### Checking for Count of Numeric and Categorical variables Count in our Dataset","7bd456af":"http:\/\/rasbt.github.io\/mlxtend\/user_guide\/feature_selection\/SequentialFeatureSelector\/","c79fd839":"#### Brief Description of variables of Our Dataset","2a754708":"### Regularization in the model","b4987838":"### ADABoost","88c5b650":"### Standard Scaling","eb55ba67":"```Applying Grid Search in Ridge Regressor\n```","b5909040":"### Feature Engineering","aa8fb6b4":"### Sale Price","18c8f4c6":"Use Rmarkdown to learn advanced regression techniques like \n```\n1. **Random forests\n2. Implement **LASSO regression** to avoid **multicollinearity**\n3. Includes **linear regression, random forest, and XGBoost** models as well\n4. **Ensemble Modeling: Stack Model Example**\n5. Use \"ensembling\" to combine the predictions of several models\n6. Comprehensive **Data Exploration** with Python\n7. Understand how **variables are distributed** and how they interact\n8. Apply different **transformations** before training machine learning models House Prices EDA\n9. Learn to use visualization techniques to study **missing data and distributions.**\n10.Includes correlation **heatmaps, pairplots, and t-SNE** to help inform appropriate inputs to      a linear model\n11.Demonstrate effective tactics for **feature engineering**\n12.Explore linear regression with different regularization methods including ridge, LASSO, and      ElasticNet using scikit-learn\n13.**Regularized Linear Models**\n14. Build a **basic linear model**","371e1363":"### Splitting X & Y","be524b24":"##### Dropping ID","44e27eaa":"Outliers in Column ```LotArea```","503ba6cb":"### ElasticNet","606a2d4d":"```Applying Grid Search in Lasso Regressor\n```","3754a81f":"```\nEven though the point from the graph does not have a Cook\u2019s distance of more than 0.25 nor 0.50, we could tell that it is indeed an outlier.\nThe graph shows that the outlier has the index near 200 to 400. We can check for the maximum value in column \u201cLotArea\u201d and check the its index so that we can remove outlier.\n\n```","870c7613":"### Lot Area","6bcea560":"#### Checking for unique Values present in each Columns","5634b692":"### Using Cooks Distance","b7d85683":"#### Checking Correlation","838aa830":"### Encoding","26b13048":"#### Checking Outliers in our Columns with Continuous datapoints","091a2406":"### Linear Regression","12020037":"#### Calling the Dataset","56d54a56":"#### Dropping Columns having missing values Gr. than 20%","a7aa2163":"Dropping Highly Correlated Columns Directly\n```\ncorr_kag=kaggle.corr().abs()\nupper = corr_kag.where(np.triu(np.ones(corr_kag.shape), k=1).astype(np.bool))\nupper\nto_drop = [column for column in upper.columns if any(upper[column] > 0.95)]\n\n```\nhttps:\/\/chrisalbon.com\/machine_learning\/feature_selection\/drop_highly_correlated_features\/","ed4cbd88":"### Ensemble Gradient Boosting Regressor","ba94d488":"##### Outliers Detection Using Z Score","9d9aecc1":"``` \nData with approx 1400 rows won't have such a huge number of outliers. Let's Check it with Cooks Distance\nkaggle = kaggle[(z < 3).all(axis=1)]\n```","aa57d205":"### Decision Tree Regressor","0880bf78":"Here's a brief version of what you'll find in the data description file.\n\n1. SalePrice - the property's sale price in dollars. This is the target variable that you're trying to predict.\n2. MSSubClass: The building class\n3. MSZoning: The general zoning classification\n4. LotFrontage: Linear feet of street connected to property\n5. LotArea: Lot size in square feet\n6. Street: Type of road access\n7. Alley: Type of alley access\n8. LotShape: General shape of property\n9. LandContour: Flatness of the property\n10. Utilities: Type of utilities available\n11. LotConfig: Lot configuration\n12. LandSlope: Slope of property\n13. Neighborhood: Physical locations within Ames city limits\n14. Condition1: Proximity to main road or railroad\n15. Condition2: Proximity to main road or railroad (if a second is present)\n16. BldgType: Type of dwelling\n17. HouseStyle: Style of dwelling\n18. OverallQual: Overall material and finish quality\n19. OverallCond: Overall condition rating\n20. YearBuilt: Original construction date\n21. YearRemodAdd: Remodel date\n22. RoofStyle: Type of roof\n23. RoofMatl: Roof material\n24. Exterior1st: Exterior covering on house\n25. Exterior2nd: Exterior covering on house (if more than one material)\n26. MasVnrType: Masonry veneer type\n27. MasVnrArea: Masonry veneer area in square feet\n28. ExterQual: Exterior material quality\n29. ExterCond: Present condition of the material on the exterior\n30. Foundation: Type of foundation\n31. BsmtQual: Height of the basement\n32. BsmtCond: General condition of the basement\n33. BsmtExposure: Walkout or garden level basement walls\n34. BsmtFinType1: Quality of basement finished area\n35. BsmtFinSF1: Type 1 finished square feet\n36. BsmtFinType2: Quality of second finished area (if present)\n37. BsmtFinSF2: Type 2 finished square feet\n38. BsmtUnfSF: Unfinished square feet of basement area\n39. TotalBsmtSF: Total square feet of basement area\n40. Heating: Type of heating\n41. HeatingQC: Heating quality and condition\n42. CentralAir: Central air conditioning\n43. Electrical: Electrical system\n44. 1stFlrSF: First Floor square feet\n45. 2ndFlrSF: Second floor square feet\n46. LowQualFinSF: Low quality finished square feet (all floors)\n47. GrLivArea: Above grade (ground) living area square feet\n48. BsmtFullBath: Basement full bathrooms\n49. BsmtHalfBath: Basement half bathrooms\n50. FullBath: Full bathrooms above grade\n51. HalfBath: Half baths above grade\n52. Bedroom: Number of bedrooms above basement level\n53. Kitchen: Number of kitchens\n54. KitchenQual: Kitchen quality\n55. TotRmsAbvGrd: Total rooms above grade (does not include bathrooms)\n56. Functional: Home functionality rating\n57. Fireplaces: Number of fireplaces\n58. FireplaceQu: Fireplace quality\n59. GarageType: Garage location\n60. GarageYrBlt: Year garage was built\n61. GarageFinish: Interior finish of the garage\n62. GarageCars: Size of garage in car capacity\n63. GarageArea: Size of garage in square feet\n64. GarageQual: Garage quality\n65. GarageCond: Garage condition\n66. PavedDrive: Paved driveway\n67. WoodDeckSF: Wood deck area in square feet\n68. OpenPorchSF: Open porch area in square feet\n69. EnclosedPorch: Enclosed porch area in square feet\n70. 3SsnPorch: Three season porch area in square feet\n71. ScreenPorch: Screen porch area in square feet\n72. PoolArea: Pool area in square feet\n73. PoolQC: Pool quality\n74. Fence: Fence quality\n75. MiscFeature: Miscellaneous feature not covered in other categories\n76. MiscVal: $Value of miscellaneous feature\n77. MoSold: Month Sold\n78. YrSold: Year Sold\n79. SaleType: Type of sale\n80. SaleCondition: Condition of sale\n\n\nhttp:\/\/jse.amstat.org\/v19n3\/decock\/DataDocumentation.txt\n\nhttps:\/\/cran.r-project.org\/web\/packages\/AmesHousing\/AmesHousing.pdf","9c420bcc":"#### Importing Packages","c9d0340b":"#### Knowing about Datatypes and Checking for nulls ","f42a31eb":"#### Visualization of Missing Values ","851736e2":"### Data Cleaning & Feature Engineering","1429f866":"### VIF","d1235d61":"#### Getting %age of Missing values in Each Columns","f5ab2c74":"https:\/\/python-graph-gallery.com\/90-heatmaps-with-various-input-format\/","58ae4a36":"### Ridge Regressor","551e2091":"### Lasso Regression","33324a0f":"### Actual vs Predicted Plot","915fd943":"#### Adding New column of Age of House derived from Year Built and Year Sold"}}