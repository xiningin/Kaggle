{"cell_type":{"0ab3fd09":"code","d8ab3ab0":"code","d4ef38c8":"code","1b307889":"code","2b5a1a4f":"code","31c0ad81":"code","2735ba14":"code","d1db0d53":"code","890b6c5d":"code","61c525d3":"code","72771adc":"code","e3463cd4":"code","c776ba96":"code","0e6386b6":"code","f59b1b37":"code","7899df4b":"code","a6c5fe7f":"code","ff5d3abd":"code","fbf5cf90":"code","95fdafce":"code","9b2aa849":"code","eecb2004":"code","5b1d5848":"code","e8136a02":"code","78625348":"code","694ab458":"code","38cd256e":"code","a8b73079":"code","f6bd6bac":"code","ecbe34e4":"code","f4a63452":"code","3de2171c":"code","5bae0e88":"code","166e04c0":"code","403ea130":"code","13d1ab78":"code","a0879ab3":"code","d049ed28":"code","6daf2452":"code","7e56a231":"code","13adbdab":"code","dca63195":"code","62e7fdfb":"code","679c6797":"code","1844a75f":"code","24341839":"code","02980528":"code","c5545c6c":"code","d05ec94c":"code","fca15e32":"code","184c4f6a":"code","fd6f488e":"code","085cf6d8":"code","9f2cd07f":"code","f7f4dc97":"code","4e849d88":"code","fc8e3800":"code","af2be78e":"code","fcd3e7fc":"code","7a036bf7":"code","2917bd0b":"code","8af456e0":"code","83c42047":"code","7a548ab6":"code","8b7025ae":"code","41fabffc":"code","384e8ecc":"code","34a767b0":"code","36be8a63":"code","26df60d9":"code","b0194dba":"code","2ad1b5c8":"code","57f3c43f":"code","4be5263d":"markdown","5292a9ae":"markdown","32ae0437":"markdown","d79e7028":"markdown","42d15fe4":"markdown","0b758986":"markdown","484c0188":"markdown","49a4e33b":"markdown","0107af2e":"markdown","6fbfa43c":"markdown","164411bc":"markdown","4656bc42":"markdown","cdb077be":"markdown","4e2a18ec":"markdown","495631d8":"markdown","b1ebaa2b":"markdown","aa57874d":"markdown","b2024b11":"markdown","9d41dbc3":"markdown","91a1183e":"markdown","ab314cfa":"markdown","61597691":"markdown","f8cbcfee":"markdown","0692489c":"markdown","b8212088":"markdown","a7a701e7":"markdown","66dd2cc4":"markdown","8bdee191":"markdown","799243f9":"markdown","7a5db9c2":"markdown","319c99f4":"markdown","70bb3f0d":"markdown","3454f644":"markdown","2ffa8f1b":"markdown","d4f17541":"markdown"},"source":{"0ab3fd09":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.\nfrom tqdm import tqdm_notebook as tqdm\nimport re","d8ab3ab0":"genre_df = pd.read_csv(\"..\/input\/myanimelist-comment-dataset\/animeListGenres.csv\",error_bad_lines=False, warn_bad_lines=False, sep=\",\")\nreview_df = pd.read_csv(\"..\/input\/myanimelist-comment-dataset\/animeReviewsOrderByTime.csv\",error_bad_lines=False, warn_bad_lines=False, sep=\",\")","d4ef38c8":"nRowsRead = 'None' # specify 'None' if want to read whole file\n# animeReviewsOrderByTime.csv has 135201 rows in reality, but we are only loading\/previewing the first 1000 rows\n\n\nwith open('..\/input\/myanimelist-comment-dataset\/animeReviewsOrderByTime.csv', 'r', encoding='utf-8') as f:\n    headers = f.readline().replace('\"','').replace('\\n','').split(',')\n    print(headers)\n    print('The number of column: ', len(headers))\n    dataFormat = dict()\n    for header in headers:\n        dataFormat[header] = list()\n\n    for idx, line in enumerate(tqdm(f.readlines(), desc='Now parsing... ')):\n        \n        if idx == 67:\n            yee = line\n        \n        if line != '':\n            line = line.replace('\\n','')\n            indices = [i for i, x in enumerate(line) if x == ',']\n            idxStart = 0\n            for i in range(len(headers)):\n                if i < len(headers) - 1:\n                    dataFormat[headers[i]].append(line[idxStart + 1:indices[i] - 1])\n                    idxStart = indices[i] + 1\n                elif i == len(headers) - 1:\n                    dataFormat[headers[i]].append(line[idxStart + 1:-1])\n                else:\n                    break\n        if nRowsRead is not None and nRowsRead == idx + 1:\n            print('We read only', nRowsRead, 'lines.')\n            break","1b307889":"review_df = pd.DataFrame(dataFormat)","2b5a1a4f":"review_df.head()","31c0ad81":"df = review_df.drop(columns = [\"id\", \"workId\", \"reviewId\", \"postTime\", \"author\"])\ndf.head()","2735ba14":"df = df[df.overallRating != '0']\ndf = df[df.overallRating != '11']","d1db0d53":"stoplist = set(\"\"\"for a of what can about don't these them any much each well any these\n               doesn't that's when show series character characters how o y e \n               get up out then do only we it's which there because even neither \n               nor my were la the and to in is that i you was it this her with but \n               their its not while they are like very as who be an his her she he him just \n               really on it\\'s de que no or are anime have all so has at from by more \n               some one me if would other also into it's will being your than most many\n               few none where does while through way such think had good story make say me\n               our own why know time off both first around may through things something thing give \n               want many\"\"\".split())\n\nprint(stoplist)","890b6c5d":"from sklearn.model_selection import train_test_split\n\nx_train, x_test, y_train, y_test = train_test_split(\n    df['review'], \n    df['overallRating'], \n    test_size=0.1, random_state=2019\n)","61c525d3":"print(x_train.shape)\nprint(x_test.shape)\nprint(y_train.shape)\nprint(y_test.shape)","72771adc":"print(np.unique(y_train))","e3463cd4":"df2 = df[['workName','overallRating']]","c776ba96":"df2['overallRating'] = df2['overallRating'].astype(float)\ndf2.head()","0e6386b6":"df3 = df2.groupby(['workName']).mean().reset_index()\ndf4 = df2.groupby(['workName']).count().reset_index()\nprint(df3.columns)\nprint(df4.head())","f59b1b37":"df4[df4['overallRating']==df4['overallRating'].max()] #max number of ratings \nperfect_anime = df3[df3['overallRating']==df3['overallRating'].max()] #max average rating\n\nperfect_anime.head()\ndf5 = pd.merge(df4, perfect_anime, on='workName')\ndf5.head()\nfrequency_for_10_scorers = list(df5['overallRating_x'])\n","7899df4b":"import numpy as np\nimport matplotlib.mlab as mlab\nimport matplotlib.pyplot as plt\n\nnum_bins = 5\nn, bins, patches = plt.hist(frequency_for_10_scorers, num_bins, facecolor='blue', alpha=0.5)\nplt.show()","a6c5fe7f":"anime_df = df3\nanime_df['freq'] = df4['overallRating']\nanime_df['averageRating'] = anime_df['overallRating']\nanime_df = anime_df.drop(columns = 'overallRating')\nanime_df.head()\n\nv = anime_df['freq'] #frequency\nm = 100 #minum number of ratings to be taken into consideration \nR = anime_df['averageRating'] #average\nC = np.mean(anime_df['averageRating'])#mean score across all anime\n\nanime_df['score'] = (v\/(v+m))*R + (m\/(v+m))*C\n\n#(v \u00f7 (v+m)) \u00d7 R + (m \u00f7 (v+m)) \u00d7 C\n# R = average for the movie (mean) = (Rating)\n# v = number of votes for the movie = (votes)\n# m = minimum votes required to be listed in the Top 250 (currently 3000)\n# C = the mean vote across the whole report (currently 6.9)","ff5d3abd":"anime_df.sort_values(by=['score'],ascending=False)[0:10]","fbf5cf90":"import gensim","95fdafce":"print(df['review'][0:5])","9b2aa849":"corpus = []\nfor i in df['review']:\n    corpus.append(i)","eecb2004":"print(len(corpus))","5b1d5848":"# remove common words and tokenize\ntexts = [\n    [word for word in document.lower().split() if word not in stoplist]\n    for document in corpus\n]\n\ntexts = [\n    [token for token in text if token.isalnum()]\n        for text in texts\n]\n\nfrom collections import defaultdict\n# remove words that appear only once\nfrequency = defaultdict(int)\nfor text in texts:\n    for token in text:\n        frequency[token] += 1\n\ntexts = [\n    [token for token in text if frequency[token] > 1]\n    for text in texts\n]\n\nfrom gensim import corpora\ndictionary = corpora.Dictionary(texts)\ncorpus = [dictionary.doc2bow(text) for text in texts]\n","e8136a02":"print(texts[0:2])","78625348":"print(corpus[0])","694ab458":"from gensim import corpora, models, similarities\ntfidf = models.TfidfModel(corpus) \ncorpus_tfidf = tfidf[corpus]","38cd256e":"NUM_TOPICS = 5\nlda = models.ldamodel.LdaModel(corpus,num_topics = NUM_TOPICS, id2word=dictionary)\ncorpus_lda = lda[corpus_tfidf]\n\nlda.show_topics(NUM_TOPICS,5)","a8b73079":"import pyLDAvis.gensim\nimport matplotlib.pyplot as plt\nimport sklearn\n%matplotlib inline\n\n\npyLDAvis.enable_notebook()\n","f6bd6bac":"panel = pyLDAvis.gensim.prepare(lda, corpus_lda, dictionary, mds='mmds')#\npanel","ecbe34e4":"pyLDAvis.save_html(panel,'vis.html')","f4a63452":"from PIL import Image\nfrom wordcloud import WordCloud, STOPWORDS, ImageColorGenerator \nimport matplotlib.colors as mcolors\n\ncols = [color for name, color in mcolors.TABLEAU_COLORS.items()]  # more colors: 'mcolors.XKCD_COLORS'\n\ncloud = WordCloud(stopwords=stoplist,\n                  background_color='white',\n                  width=2500,\n                  height=1800,\n                  max_words=20,\n                  colormap='tab10',\n                  color_func=lambda *args, **kwargs: cols[i],\n                  prefer_horizontal=1.0)\n\ntopics = lda.show_topics(formatted=False)\n\nfig, axes = plt.subplots(1, 5, figsize=(15,15), sharex=True, sharey=True)\n\nfor i, ax in enumerate(axes.flatten()):\n    fig.add_subplot(ax)\n    topic_words = dict(topics[i][1])\n    cloud.generate_from_frequencies(topic_words, max_font_size=300)\n    plt.gca().imshow(cloud)\n    plt.gca().set_title('Topic ' + str(i), fontdict=dict(size=16))\n    plt.gca().axis('off')\n\n\nplt.subplots_adjust(wspace=0, hspace=0)\nplt.axis('off')\nplt.margins(x=0, y=0)\nplt.tight_layout()\nplt.show()","3de2171c":"df.head()","5bae0e88":"!pip install BeautifulSoup4\nfrom bs4 import BeautifulSoup  \nimport re","166e04c0":"def review_to_words( raw_review ):\n    # Function to convert a raw review to a string of words\n    # The input is a single string (a raw movie review), and \n    # the output is a single string (a preprocessed movie review)\n    #\n    # 1. Remove HTML\n    review_text = BeautifulSoup(raw_review).get_text() \n    #\n    # 2. Remove non-letters        \n    letters_only = re.sub(\"[^a-zA-Z]\", \" \", review_text) \n    #\n    # 3. Convert to lower case, split into individual words\n    words = letters_only.lower().split()                             \n    #\n    # 4. In Python, searching a set is much faster than searching\n    #   a list, so convert the stop words to a set\n    stops = set(stoplist)                  \n    # \n    # 5. Remove stop words\n    meaningful_words = [w for w in words if not w in stops]   \n    #\n    # 6. Join the words back into one string separated by space, \n    # and return the result.\n    return( \" \".join( meaningful_words ))   ","403ea130":"target = df['overallRating']\ndf = df.drop(columns=['overallRating','storyRating','animationRating','soundRating','enjoymentRating','characterRating'])\ndf.head()","13d1ab78":"from sklearn.model_selection import train_test_split\n\nx_train, x_test, y_train, y_test = train_test_split(\n    df, \n    target, \n    test_size=0.2, random_state=2019\n)","a0879ab3":"# Get the number of reviews based on the dataframe column size\nnum_reviews = x_train[\"review\"].size\n\n# Initialize an empty list to hold the clean reviews\nclean_train_reviews = []\n\n# Loop over each review; create an index i that goes from 0 to the length\n# of the movie review list \nfor i in x_train[\"review\"]:\n    # Call our function for each one, and add the result to the list of\n    # clean reviews\n    clean_train_reviews.append( review_to_words( i) )","d049ed28":"# Get the number of reviews based on the dataframe column size\nnum_reviews = x_test[\"review\"].size\n\n# Initialize an empty list to hold the clean reviews\nclean_test_reviews = []\n\n# Loop over each review; create an index i that goes from 0 to the length\n# of the movie review list \nfor i in x_test[\"review\"]:\n    # Call our function for each one, and add the result to the list of\n    # clean reviews\n    clean_test_reviews.append( review_to_words( i) )","6daf2452":"print(clean_train_reviews[0:2])","7e56a231":"from keras.preprocessing import text, sequence\nCHARS_TO_REMOVE = '!\"#$%&()*+,-.\/:;<=>?@[\\\\]^_`{|}~\\t\\n\u201c\u201d\u2019\\'\u221e\u03b8\u00f7\u03b1}\u2022\u00e0\u2212\u03b2\u2205\u00b3\u03c0\u2018\u20b9\u00b4\u00b0\u00a3\u20ac\\\u00d7\u2122\u221a\u00b2\u2014'\ntokenizer = text.Tokenizer(filters=CHARS_TO_REMOVE)\ntokenizer.fit_on_texts(clean_train_reviews)","13adbdab":"rtok = tokenizer.texts_to_sequences(clean_train_reviews)\nrtok = sequence.pad_sequences(rtok, maxlen=256)","dca63195":"from keras.preprocessing import text, sequence\nCHARS_TO_REMOVE = '!\"#$%&()*+,-.\/:;<=>?@[\\\\]^_`{|}~\\t\\n\u201c\u201d\u2019\\'\u221e\u03b8\u00f7\u03b1}\u2022\u00e0\u2212\u03b2\u2205\u00b3\u03c0\u2018\u20b9\u00b4\u00b0\u00a3\u20ac\\\u00d7\u2122\u221a\u00b2\u2014'\ntokenizer = text.Tokenizer(filters=CHARS_TO_REMOVE)\ntokenizer.fit_on_texts(clean_test_reviews)","62e7fdfb":"rtok2 = tokenizer.texts_to_sequences(clean_test_reviews)\nrtok2 = sequence.pad_sequences(rtok2, maxlen=256)","679c6797":"print(rtok.shape)\nprint(rtok[0])","1844a75f":"len(rtok)\n","24341839":"len(y_train)","02980528":"y_train = [ int(x) for x in y_train ]\nprint(type(y_train[0]))\ny_test = [ int(x) for x in y_test ]","c5545c6c":"#convert y_train to a binary classification problem\n#1 for good review, 0 for bad review\nfor i in range(len(y_train)):\n    if y_train[i] >=6:\n        y_train[i] = 1\n    elif y_train[i] <6:\n        y_train[i] = 0","d05ec94c":"#convert y_test to a binary classification problem\n#1 for good review, 0 for bad review\nfor i in range(len(y_test)):\n    if y_test[i] >=6:\n        y_test[i] = 1\n    elif y_test[i] <6:\n        y_test[i] = 0","fca15e32":"from sklearn.ensemble import RandomForestClassifier\n\nclf=RandomForestClassifier(n_estimators=100)\n\n#Train the model using the training sets y_pred=clf.predict(X_test)\nclf.fit(rtok,y_train)","184c4f6a":"print(len(rtok2))","fd6f488e":"print(len(y_test))","085cf6d8":"y_pred = clf.predict(rtok2)\n\nmatch = (y_pred == y_test)\nacc = sum(match)\/len(match)","9f2cd07f":"print(\"accuracy is: \", acc)","f7f4dc97":"from sklearn.metrics import confusion_matrix\ncm = confusion_matrix(y_test, y_pred)\nimport seaborn as sns \nsns.heatmap(cm,cmap=\"YlGnBu\",linewidths=.5, annot=True)","4e849d88":"from sklearn.model_selection import train_test_split\n\nx_train, x_test, y_train, y_test = train_test_split(\n    df, \n    target, \n    test_size=0.2, random_state=2019\n)","fc8e3800":"# Get the number of reviews based on the dataframe column size\nnum_reviews = x_train[\"review\"].size\n\n# Initialize an empty list to hold the clean reviews\nclean_train_reviews = []\n\n# Loop over each review; create an index i that goes from 0 to the length\n# of the movie review list \nfor i in x_train[\"review\"]:\n    # Call our function for each one, and add the result to the list of\n    # clean reviews\n    clean_train_reviews.append( review_to_words( i) )","af2be78e":"# Get the number of reviews based on the dataframe column size\nnum_reviews = x_test[\"review\"].size\n\n# Initialize an empty list to hold the clean reviews\nclean_test_reviews = []\n\n# Loop over each review; create an index i that goes from 0 to the length\n# of the movie review list \nfor i in x_test[\"review\"]:\n    # Call our function for each one, and add the result to the list of\n    # clean reviews\n    clean_test_reviews.append( review_to_words( i) )","fcd3e7fc":"from keras.preprocessing import text, sequence\nCHARS_TO_REMOVE = '!\"#$%&()*+,-.\/:;<=>?@[\\\\]^_`{|}~\\t\\n\u201c\u201d\u2019\\'\u221e\u03b8\u00f7\u03b1}\u2022\u00e0\u2212\u03b2\u2205\u00b3\u03c0\u2018\u20b9\u00b4\u00b0\u00a3\u20ac\\\u00d7\u2122\u221a\u00b2\u2014'\ntokenizer = text.Tokenizer(filters=CHARS_TO_REMOVE)\ntokenizer.fit_on_texts(clean_train_reviews)","7a036bf7":"rtok = tokenizer.texts_to_sequences(clean_train_reviews)\nrtok = sequence.pad_sequences(rtok, maxlen=256)","2917bd0b":"from keras.preprocessing import text, sequence\nCHARS_TO_REMOVE = '!\"#$%&()*+,-.\/:;<=>?@[\\\\]^_`{|}~\\t\\n\u201c\u201d\u2019\\'\u221e\u03b8\u00f7\u03b1}\u2022\u00e0\u2212\u03b2\u2205\u00b3\u03c0\u2018\u20b9\u00b4\u00b0\u00a3\u20ac\\\u00d7\u2122\u221a\u00b2\u2014'\ntokenizer = text.Tokenizer(filters=CHARS_TO_REMOVE)\ntokenizer.fit_on_texts(clean_test_reviews)\nrtok2 = tokenizer.texts_to_sequences(clean_test_reviews)\nrtok2 = sequence.pad_sequences(rtok2, maxlen=256)","8af456e0":"y_train = [ int(x) for x in y_train ]\nprint(type(y_train[0]))\ny_test = [ int(x) for x in y_test ]","83c42047":"import numpy as np\nimport math\nfrom matplotlib import pyplot as plt\n\nbins = np.linspace(math.ceil(min(y_train)), \n                   math.floor(max(y_train)),\n                   10) # fixed number of bins\n\nplt.xlim([min(y_train), max(y_train)])\n\nplt.hist(y_train, bins=bins, alpha=0.5)\nplt.title('Y_train (fixed number of bins)')\nplt.xlabel('variable X (10 evenly spaced bins)')\nplt.ylabel('count')\n\nplt.show()","7a548ab6":"import numpy as np\nimport math\nfrom matplotlib import pyplot as plt\n\nbins = np.linspace(math.ceil(min(y_test)), \n                   math.floor(max(y_test)),\n                   10) # fixed number of bins\n\nplt.xlim([min(y_test), max(y_test)])\n\nplt.hist(y_test, bins=bins, alpha=0.5)\nplt.title('Y_test (fixed number of bins)')\nplt.xlabel('variable X (10 evenly spaced bins)')\nplt.ylabel('count')\n\nplt.show()","8b7025ae":"#convert y_train to a binary classification problem\n#1 for good review, 0 for bad review\nfor i in range(len(y_train)):\n    if y_train[i] <=5:\n        y_train[i] = 0 #very bad\n    elif y_train[i] == 6 or y_train[i] == 7 or y_train[i] == 8 or y_train[i] == 9:\n        y_train[i] = 1 #average\n    elif y_train[i] == 10:\n        y_train[i] = 2 #excellent","41fabffc":"#convert y_train to a binary classification problem\n#1 for good review, 0 for bad review\nfor i in range(len(y_test)):\n    if y_test[i] <=5:\n        y_test[i] = 0 #very bad\n    elif y_test[i] == 6 or y_test[i] == 7 or y_test[i] == 8 or y_test[i] == 9:\n        y_test[i] = 1 #average\n    elif y_test[i] == 10:\n        y_test[i] = 2 #excellent","384e8ecc":"uni = np.unique(y_train)\nfor i in uni:\n    print(\"unique \", i, \": \", y_train.count(i))","34a767b0":"print(type(y_train))","36be8a63":"uni = np.unique(y_test)\nfor i in uni:\n    print(\"unique \", i, \": \", y_test.count(i))","26df60d9":"from sklearn.ensemble import RandomForestClassifier\n\nclf=RandomForestClassifier(n_estimators=100)\n\n#Train the model using the training sets y_pred=clf.predict(X_test)\nclf.fit(rtok,y_train)","b0194dba":"y_pred = clf.predict(rtok2)\n\nmatch = (y_pred == y_test)\nacc = sum(match)\/len(match)\nprint(\"accuracy is: \", acc)","2ad1b5c8":"cm = confusion_matrix(y_test, y_pred)\nsns.heatmap(cm,cmap=\"YlGnBu\",linewidths=.5, annot=True)","57f3c43f":"importances = clf.feature_importances_\n\n\nprint(\"Top 10 Features:\")\n\nfor f in range(10):\n    print(\"%d. feature %d (%f)\" % (f + 1, indices[f], importances[indices[f]]))\n    print([key for key, value in tokenizer.word_index.items() if value == indices[f]])","4be5263d":"No show with a perfect 10 had more than 4 users rating it... we should find a metric to scale highest overall score with number of ratings.","5292a9ae":"We use the same shrinkage score that sites like myanimelist and IMDB use for movie ratings: https:\/\/stats.stackexchange.com\/questions\/6418\/rating-system-taking-account-of-number-of-votes","32ae0437":"From: https:\/\/scikit-learn.org\/stable\/auto_examples\/ensemble\/plot_forest_importances.html","d79e7028":"![image.png](attachment:image.png)","42d15fe4":"First we try binary classification to predict if a review is good or bad. \n\nWe are going to feed the tokenized reviews into a random forest classifier model so that we can uncover feature importances behind certain encoded words.","0b758986":"Using data parser written by Nishant Kumar (https:\/\/www.kaggle.com\/nishantkumar95\/sentiment-classification-for-anime-comment-dataset)","484c0188":"We tokenize the reviews here using Keras' bag of words implementation.","49a4e33b":"Now we do some feature engineering to find max number of ratings and max average ratings of each anime. We find that perfectly rated 10\/10 anime have very few reviewers, as with a large popular anime surely someone wouldn't give it a perfect 10. ","0107af2e":"Top 10 anime based on shrinkage score","6fbfa43c":"Latent Dirichlet Allocation is an algorithm used to generate \"Topics\" or clusters from a corpus of text documents. [This article](https:\/\/towardsdatascience.com\/light-on-math-machine-learning-intuitive-guide-to-latent-dirichlet-allocation-437c81220158) explains the algorithm well, but I will briefly summarize. \n\nLDA represents documents as a random mixture of topics (think soft clusters) where each topic is a distribution over words. Both of these distributions are represented as Dirchilet distrubitions. We sample words and topics from these distributions and find the probability that a given topic generates a word. This is done generatively until good probabilities are found. LDA is latent because topics are latent variables (we don't actually obeserve them in the documents or corpus). LDA uses Dirchilet distributions as a prior for the word-topic and topic-document distributions because the Dirchilet distribution is a multivariate generalization of the beta distribution.  (https:\/\/www.quora.com\/Why-does-the-LDA-model-use-the-Dirichlet-distribution-as-a-prior)","164411bc":"We feature encode the text reviews as vectors using the bag of words approach (https:\/\/machinelearningmastery.com\/gentle-introduction-bag-words-model\/) and then use the Latent Dirichlet Allocation (LDA) algorithm to cluster frequent words into clusters called \"topics\". This is topic modeling. I will talk a bit more about LDA below. \n","4656bc42":"Clean the train and test reviews.","cdb077be":"Hello everyone, this notebook will be a naive exploration into some of the techniques used in NLP and also to familiarize myself with the types of analyses (sentiment, topic modeling, etc.) that people can use when given an NLP problem. The dataset I chose to use was a subset of myanimelist reviews on anime.","4e2a18ec":"Now we examine the most important words in predicting score. These words potentially indicate sentiment about an anime the best.","495631d8":"We additionally create wordmaps to show the most common words in each of the topics. From reading I found this was a controversial visualization\/representation to use.","b1ebaa2b":"pyLDAvis allows us to create this visualization that shows the topic clusters and how far away they are from one another. We can see the most common keywords for each topic by hovering over the cluster, as well as the saliency of each word based on the relevance. Salience is used to describe the contribution of a word to the overall meaning of a text. Relevance is based on lift- which is the ratio of a term's probability within a topic to across the corpus. Relevance is the weight assigned to the probability of a term in a topic relative to its lift. So relevance is basically how important the term being in a topic is compared to its overall incidence in the corpus. (https:\/\/www.objectorientedsubject.net\/2018\/08\/experiments-on-topic-modeling-pyldavis\/) ","aa57874d":"This is what the tabular dataset looks like. For the purposes of NLP experiments I will focus only on the ratings, the anime name and the text reviews.","b2024b11":"# Supervised Learning","9d41dbc3":"First we create a corpus from our reviews. This is adapted from the gensim documentation (https:\/\/radimrehurek.com\/gensim\/auto_examples\/core\/run_topics_and_transformations.html#sphx-glr-auto-examples-core-run-topics-and-transformations-py). ","91a1183e":"# Topic Modeling","ab314cfa":"True is y-axis, pred is x-axis. So our model seems to be overfitting on predicting class 2 (rating of 8-9)","61597691":"We define a function to preprocess the reviews in a different way. (From https:\/\/www.kaggle.com\/c\/word2vec-nlp-tutorial\/overview\/part-1-for-beginners-bag-of-words)","f8cbcfee":"First, we will try to use a topic modeling approach using LDA implementation from the gensim python library. This algorithm aims to find a set of \"topics\" based on a corpus of documents. This will let us identify common topics in the anime reviews that reviewers seem to focus on.","0692489c":"So these are the top 5 keywords of each of the 5 topics","b8212088":"I set up a stoplist of words to be filtered out in the reviews. These are words that likely have no sentiment or meaning but could occur quite frequently.","a7a701e7":"This seems to have a big class imbalance as there are only 15 \"bad\" reviews in our dataset as opposed to thousands of good ones. Let's instead bucket our classes into 4 or so classes instead of 10 or 2. {1-5} for very bad, {6-7} for below average, {8-9} for above average and {10} for excellent.","66dd2cc4":"Todo: Next we will try training the random forest on a downsampled training\/testing set to see if performance increases, and then explore the feature importances of said RF.","8bdee191":"Split the reviews into training and testing sets. Our input is the reviews and the output is the overall Ratings. Perhaps we can predict scores from review sentiment to some extent.","799243f9":"LONG RUN TIME!!","7a5db9c2":"from https:\/\/stackoverflow.com\/questions\/24809757\/how-to-make-a-histogram-from-a-list-of-data","319c99f4":"So we come up with our top 10 anime list based on this score! Now let's compare it with MyAnimeLists' actual top 10 list:","70bb3f0d":"Now we repeat the problem using a multiclassification problem to hopefully have more balanced classes. We group the scores into 3-4 buckets to see the impact on classification accuracy.","3454f644":"Our Top 10 looks similar to MyAnimeList... the difference might be a different shrinkage estimation formula or perhaps our dataset contains a limited number of reviews.\n\nI was also informed MAL doesn't take into account scores of those who have watched less than 20% of the anime. This is probably a significant contributor to the discrepancy between the top anime lists, but we won't filter out those 20% reviews for sentiment analysis because dropping a show early is a strong indicator of negative sentiment.","2ffa8f1b":"## Preprocessing","d4f17541":"Now we remove unecessary columns and do some preprocessing. I remove reviews with ratings of 0 and 11 to just have a 1-10 scale. Based on examinign the frequencies of these ratings in the dataset, they are just a few mislabeled\/misrepresentative data that seem best to be omitted\/cleaned. "}}