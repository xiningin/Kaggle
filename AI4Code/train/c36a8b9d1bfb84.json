{"cell_type":{"f77c206c":"code","05c67d14":"code","3f25d15f":"code","fc00de56":"code","e401e08e":"code","786fac45":"code","b18e01dc":"code","88a0760a":"code","acea62f6":"code","aae1a56a":"code","139ec882":"code","8c410f37":"code","fc805de3":"code","06342b30":"markdown","dc96beb0":"markdown","1561221c":"markdown","f64656c2":"markdown"},"source":{"f77c206c":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","05c67d14":"!pip install tensorflow-gpu==2.1.0","3f25d15f":"#import non tensorflow libraries\nimport matplotlib.pyplot as plt\nimport os\nimport numpy as np\nfrom PIL import Image\nimport pathlib\nimport IPython.display as display\n\n#import tensorflow libraries\nimport tensorflow as tf\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator","fc00de56":"#initialise class names and view them\nwith open(\"\/kaggle\/input\/food-101\/food-101\/food-101\/meta\/classes.txt\", \"r\") as f:\n    CLASS_NAMES = [item.strip() for item in f]\nCLASS_NAMES = np.array(CLASS_NAMES)\nprint(len(CLASS_NAMES))","e401e08e":"#initialising necessary properties\nBATCH_SIZE = 32\nIMG_HEIGHT = 224\nIMG_WIDTH = 224\nSTEPS_PER_EPOCH = np.ceil(10000\/BATCH_SIZE)\n\n#prepare train data generator with necessary augmentations and validation split\ntrain_datagen = ImageDataGenerator(\n        rotation_range=40,\n        width_shift_range=0.2,\n        height_shift_range=0.2,\n        rescale=1.\/255,\n        zoom_range=0.2,\n        horizontal_flip=True,\n        fill_mode='nearest',\n        validation_split=0.25)","786fac45":"#initialise train directory\ndata_dir = '\/kaggle\/input\/food-101\/food-101\/food-101\/images\/'\ndata_dir = pathlib.Path(data_dir)","b18e01dc":"#generate training data\nprint('Train Data')\ntrain_data = train_datagen.flow_from_directory(\n    str(data_dir),\n    batch_size=BATCH_SIZE,\n    shuffle=True,\n    target_size=(IMG_HEIGHT, IMG_WIDTH),\n    classes = list(CLASS_NAMES),\n    subset='training')\n\n#generate validation data\nprint('\\nValidation Data')\nvalid_data = train_datagen.flow_from_directory(\n    str(data_dir),\n    target_size=(IMG_HEIGHT, IMG_WIDTH),\n    batch_size=BATCH_SIZE,\n    subset='validation')","88a0760a":"#vizualise the loaded images\ndef show_batch(image_batch, label_batch):\n  fig = plt.figure(figsize=(10,10))\n  fig.patch.set_facecolor('white')\n  for n in range(25):\n      ax = plt.subplot(5,5,n+1)\n      plt.imshow(image_batch[n])\n      plt.title(CLASS_NAMES[label_batch[n]==1][0].title(), fontsize=14)\n      plt.axis('off')\nimage_batch, label_batch = next(train_data)\nshow_batch(image_batch, label_batch)","acea62f6":"#import model \nfrom tensorflow.keras.applications.mobilenet_v2 import MobileNetV2","aae1a56a":"#initialise base model\nIMG_SHAPE = (IMG_HEIGHT, IMG_WIDTH, 3)\nbase_model = MobileNetV2(input_shape=IMG_SHAPE, input_tensor=None,\n                                                include_top=False, \n                                                weights='imagenet')\nbase_model.trainable = True","139ec882":"#define model\nmodel = tf.keras.Sequential()\nmodel.add(base_model)\nmodel.add(tf.keras.layers.GlobalAveragePooling2D())\nmodel.add(tf.keras.layers.Dense(320, activation='relu'))\nmodel.add(tf.keras.layers.Dropout(0.2))\nmodel.add(tf.keras.layers.Dense(320, activation='relu'))\nmodel.add(tf.keras.layers.Dropout(0.2))\nmodel.add(tf.keras.layers.Dense(101, activation='softmax'))\n#compile model\nmodel.compile(optimizer=tf.keras.optimizers.SGD(lr=0.001, momentum=0.9), \n      loss = tf.keras.losses.CategoricalCrossentropy(from_logits = True), \n      metrics=['accuracy'])\nprint(model.summary())","8c410f37":"epochs = 30\n\n# Reduce learning rate when there is a change lesser than <min_delta> in <val_accuracy> for more than <patience> epochs\nreduce_lr = tf.keras.callbacks.ReduceLROnPlateau(monitor = 'val_accuracy',\n                                                 mode = 'max',\n                                                 min_delta = 0.01,\n                                                 patience = 3,\n                                                 factor = 0.25,\n                                                 verbose = 1,\n                                                 cooldown = 0,\n                                                 min_lr = 0.00000001)\n\n# Stop the training process when there is a change lesser than <min_delta> in <val_accuracy> for more than <patience> epochs\nearly_stopper = tf.keras.callbacks.EarlyStopping(monitor = 'val_accuracy',\n                                                 mode = 'max',\n                                                 min_delta = 0.005,\n                                                 patience = 10,\n                                                 verbose = 1,\n                                                 restore_best_weights = True)\n#fit the model\nhistory = model.fit(train_data, \n                    epochs=epochs,\n                    validation_data = valid_data,\n                    callbacks=[early_stopper, reduce_lr])","fc805de3":"#get results\nacc = history.history['accuracy']\nval_acc = history.history['val_accuracy']\nloss = history.history['loss']\nval_loss = history.history['val_loss']\n\n#plot results\n#accuracy\nplt.figure(figsize=(8, 8))\nplt.rcParams['figure.figsize'] = [16, 9]\nplt.rcParams['font.size'] = 14\nplt.rcParams['axes.grid'] = True\nplt.rcParams['figure.facecolor'] = 'white'\nplt.subplot(2, 1, 1)\nplt.plot(acc, label='Training Accuracy')\nplt.plot(val_acc, label='Validation Accuracy')\nplt.legend(loc='lower right')\nplt.ylabel('Accuracy')\nplt.title(f'MobileNetV2 \\nTraining and Validation Accuracy. \\nTrain Accuracy: {str(acc[-1])}\\nValidation Accuracy: {str(val_acc[-1])}')\n\n#loss\nplt.subplot(2, 1, 2)\nplt.plot(loss, label='Training Loss')\nplt.plot(val_loss, label='Validation Loss')\nplt.legend(loc='upper right')\nplt.ylabel('Cross Entropy')\nplt.title(f'Training and Validation Loss. \\nTrain Loss: {str(loss[-1])}\\nValidation Loss: {str(val_loss[-1])}')\nplt.xlabel('epoch')\nplt.tight_layout(pad=3.0)\nplt.show()","06342b30":"Fine Tuning the model by adding extra classification layers and training the entire models with imagenet weights.","dc96beb0":"## **MobileNetV2**","1561221c":"## **Data Preparation**","f64656c2":"### **Importing libraries**"}}