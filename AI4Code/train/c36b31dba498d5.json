{"cell_type":{"84257e52":"code","73e4c8e6":"code","c380e9b3":"code","30b17283":"code","affe6ba6":"code","8732a451":"code","873495aa":"code","1e6b6fe1":"code","793425d3":"code","8e6866d1":"code","8a766f27":"code","04e8431d":"code","87bb6932":"code","e0f3c886":"code","36663942":"code","cbbf5fae":"markdown","200a4655":"markdown","b932350a":"markdown","16a76811":"markdown","059f7800":"markdown","38dad1d1":"markdown","2ea0f425":"markdown","136f0af8":"markdown"},"source":{"84257e52":"from nltk.tokenize import RegexpTokenizer\nfrom stop_words import get_stop_words\nfrom nltk.stem.wordnet import WordNetLemmatizer\nfrom gensim import corpora, models\nimport pandas as pd\nimport gensim\nimport pyLDAvis.gensim\n","73e4c8e6":"pattern = r'\\b[^\\d\\W]+\\b'\ntokenizer = RegexpTokenizer(pattern)\nen_stop = get_stop_words('en')\nlemmatizer = WordNetLemmatizer()","c380e9b3":"from nltk.corpus import stopwords \n","30b17283":"remove_words = list(stopwords.words('english'))","affe6ba6":"# remove_words","8732a451":"!ls ..\/input\/news-category-dataset\/","873495aa":"# Input from csv\ndf = pd.read_json('..\/input\/news-category-dataset\/News_Category_Dataset_v2.json',lines = True)\n\n# sample data\n","1e6b6fe1":"df.head()","793425d3":"df.shape","8e6866d1":"df = df.sample(frac = 0.1)","8a766f27":"df[\"Description\"] = df[\"headline\"]+\". \" +df[\"short_description\"]","04e8431d":"# list for tokenized documents in loop\ntexts = []\n\n# loop through document list\nfor i in df['Description'].iteritems():\n    # clean and tokenize document string\n    raw = str(i[1]).lower()\n    tokens = tokenizer.tokenize(raw)\n\n    # remove stop words from tokens\n    stopped_tokens = [raw for raw in tokens if not raw in en_stop]\n    \n    # remove stop words from tokens\n    stopped_tokens_new = [raw for raw in stopped_tokens if not raw in remove_words]\n    \n    # lemmatize tokens\n    lemma_tokens = [lemmatizer.lemmatize(tokens) for tokens in stopped_tokens_new]\n    \n    # remove word containing only single char\n    new_lemma_tokens = [raw for raw in lemma_tokens if not len(raw) == 1]\n    \n    # add tokens to list\n    texts.append(new_lemma_tokens)\n\n# sample data\nprint(texts[0])","87bb6932":"# turn our tokenized documents into a id <-> term dictionary\ndictionary = corpora.Dictionary(texts)\n# convert tokenized documents into a document-term matrix\ncorpus = [dictionary.doc2bow(text) for text in texts]","e0f3c886":"ldamodel = gensim.models.ldamodel.LdaModel(corpus, num_topics=15, id2word = dictionary, passes=20)\nimport pprint\npprint.pprint(ldamodel.top_topics(corpus,topn=5))","36663942":"pyLDAvis.enable_notebook()\npyLDAvis.gensim.prepare(ldamodel, corpus, dictionary)","cbbf5fae":"# Topic Modeling \n\nTopic modeling is a statistical model to discover the abstract \"topics\" that occur in a collection of documents.  \nIt is commonly used in text document. But nowadays, in social media analysis, topic modeling is an emerging research area.  \nOne of the most popular algorithms used is Latent Dirichlet Allocation which was proposed by  \n[David Blei et al in 2003](http:\/\/www.jmlr.org\/papers\/volume3\/blei03a\/blei03a.pdf).   \nHere, I want to perform topic modeling for the upvoted kaggle dataset. \n\nSome notes on topic modeling:   \n* To determine the number topics, it is common to use [elbow method](https:\/\/en.wikipedia.org\/wiki\/Elbow_method_(clustering) with [perplexity score](http:\/\/qpleple.com\/perplexity-to-evaluate-topic-models\/) as its cost function.   \n* To evaluate the models, we can calculate [topic coherence](http:\/\/qpleple.com\/topic-coherence-to-evaluate-topic-models\/).   \n* Finally, to interpret the topics, as studied in social science research, there is [triangulation method](http:\/\/www.federica.eu\/users\/9\/docs\/amaturo-39571-01-Triangulation.pdf).  ","200a4655":"## Create term dictionary and document-term matrix","b932350a":"## Import libraries\n\nI used LDA model from gensim. Other option is using sklearn.","16a76811":"## Visualize the topic model\n\nUsing pyLDAvis, we can create an interactive visualization.","059f7800":"## Initiating Tokenizer and Lemmatizer\n\nInitiate the tokenizer, stop words, and lemmatizer from the libraries.\n\n* Tokenizer is used to split the sentences into words.  \n* Lemmatizer (a quite similar term to Stemmer) is used to reduce words to its base form.   \nThe simple difference is that Lemmatizer considers the meaning while Stemmer does not. \n","38dad1d1":"## Read the data","2ea0f425":"## Perform Tokenization, Words removal, and Lemmatization","136f0af8":"## Generate LDA model\n\nI used pre-determined number of topics. It will better calculating perplexity to find the optimum number of topics.    \n*top_topics* shows the sorted topics based on the topic coherence."}}