{"cell_type":{"b7ce8c0d":"code","1ed2ee73":"code","8df4c8f4":"code","e73ee136":"code","8a193e3b":"code","b6aa2d53":"code","01c1bc97":"code","d0301179":"code","e782a3fb":"code","b7e5df28":"code","d9c66a8b":"code","d0db3300":"code","5d3d0f06":"code","049a4942":"code","553a58e4":"code","866e5e25":"code","636340bc":"code","5f8856e3":"code","a0ea76f0":"code","3f6bd97a":"code","28bf861a":"code","b7d7b1cf":"code","7d124244":"code","95bd0925":"code","dab92910":"code","1eed1d84":"code","d5553f62":"code","b5ee9ffd":"code","c8448062":"code","6c4103c0":"code","af92a415":"code","abf4378f":"code","278a06e9":"code","060fac4f":"code","fdc62450":"code","7b675dc8":"code","6090602e":"code","ed57b0b9":"code","f8c880ce":"code","25730361":"code","91acb0ab":"code","b02eb74f":"code","9005108e":"code","b49667b1":"code","0bebd7ef":"code","465aad20":"code","dcbee8e3":"code","85287560":"code","0d222777":"code","76057d14":"code","3f70bd8a":"code","3eeb2430":"code","a7602fae":"code","3e91a1b6":"code","e1965f8b":"code","3a5f69c1":"code","78e864c7":"code","7d4d7eca":"code","c79a71de":"code","ca373cd0":"code","6569672e":"code","eea36197":"code","269fbe5e":"code","0c2947ee":"code","c13a0de5":"code","5c61db7f":"code","6b488142":"code","2bd4ec14":"code","7b453438":"code","b60bbd78":"code","f9b7df61":"markdown","77978f0f":"markdown","b8fd3b6c":"markdown","5df72525":"markdown","766c63fd":"markdown","7368ede4":"markdown","1217f940":"markdown","c179ba7e":"markdown","c1530095":"markdown","e6222d69":"markdown","9a7e45c8":"markdown","7d3278f2":"markdown","b52f54df":"markdown","e824c584":"markdown","d45f9340":"markdown","1a50e314":"markdown","19ad44a3":"markdown","a3b1ab13":"markdown","7682d026":"markdown","2e203ea0":"markdown","40b98b50":"markdown","55e10e5c":"markdown","02f97dc7":"markdown","eac2671d":"markdown","81303c6e":"markdown","6c5eaace":"markdown","98ef58a4":"markdown","cf515a7d":"markdown","2f27fd98":"markdown","d2e174e5":"markdown","5710092e":"markdown","717322fb":"markdown","9fb78ebb":"markdown","7efca376":"markdown","9f8e3717":"markdown","76a9643e":"markdown","3ea1c815":"markdown","069a892b":"markdown","ff390a17":"markdown","0cfe278e":"markdown","f484a589":"markdown","bcfd16d6":"markdown","dc518a0a":"markdown","36669e31":"markdown","13c928d8":"markdown","e8561f25":"markdown","b9058d49":"markdown","4bcb1e33":"markdown","d532c2db":"markdown","bc5fcfef":"markdown","ceab115f":"markdown","04473432":"markdown","a8763ba6":"markdown","824e1bc9":"markdown","b7f51f2f":"markdown","599c2b26":"markdown"},"source":{"b7ce8c0d":"import pandas as pd\nimport numpy as np\nfrom numpy import loadtxt\nfrom numpy import sort\nimport matplotlib.pyplot as plt\n%matplotlib inline\nfrom matplotlib.ticker import PercentFormatter\nimport matplotlib.ticker as mtick\nfrom wordcloud import WordCloud, STOPWORDS \nfrom random import sample\nimport seaborn as sns\nimport xgboost as xgb\nfrom xgboost import XGBClassifier\nfrom sklearn import svm\nfrom sklearn.metrics import classification_report\nfrom sklearn import metrics\nfrom sklearn import preprocessing\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.datasets import make_classification\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\nfrom sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\nfrom sklearn.utils import resample\nfrom sklearn.model_selection import StratifiedShuffleSplit\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.decomposition import PCA\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.feature_selection import SelectFromModel","1ed2ee73":"# import warnings filter\nfrom warnings import simplefilter\n# ignore all future warnings\nsimplefilter(action='ignore', category=FutureWarning)","8df4c8f4":"df_rating = pd.read_csv('..\/input\/corporate-credit-rating\/corporate_rating.csv')","e73ee136":"# Display the dimensions\nprint(\"The credit rating dataset has\", df_rating.shape[0], \"records, each with\", df_rating.shape[1],\n    \"attributes\")","8a193e3b":"# Display the structure\ndf_rating.info()","b6aa2d53":"df_rating.head()","01c1bc97":"df_rating.Rating.value_counts()","d0301179":"rating_dict = {'AAA':'Lowest Risk', \n               'AA':'Low Risk',\n               'A':'Low Risk',\n               'BBB':'Medium Risk', \n               'BB':'High Risk',\n               'B':'High Risk',\n               'CCC':'Highest Risk', \n               'CC':'Highest Risk',\n               'C':'Highest Risk',\n               'D':'In Default'}\n\ndf_rating.Rating = df_rating.Rating.map(rating_dict)","e782a3fb":"ax = df_rating['Rating'].value_counts().plot(kind='bar',\n                                             figsize=(8,4),\n                                             title=\"Count of Rating by Type\",\n                                             grid=True)","b7e5df28":"df_rating = df_rating[df_rating['Rating']!='Lowest Risk'] # filter Lowest Risk\ndf_rating = df_rating[df_rating['Rating']!='In Default']  # filter In Default\ndf_rating.reset_index(inplace = True, drop=True) # reset index","d9c66a8b":"# Statistical summary \ndf_rating.describe()","d0db3300":"column_list = list(df_rating.columns[6:31])\ncolumn_list = sample(column_list,4) \nprint(column_list)","5d3d0f06":"figure, axes = plt.subplots(nrows=2, ncols=4, figsize=(9,5))\n\naxes[0, 0].hist(df_rating[column_list[0]])\naxes[0, 1].hist(df_rating[column_list[1]])\naxes[1, 0].hist(df_rating[column_list[2]])\naxes[1, 1].hist(df_rating[column_list[3]])\n\naxes[0, 2].boxplot(df_rating[column_list[0]])\naxes[1, 2].boxplot(df_rating[column_list[1]])\naxes[0, 3].boxplot(df_rating[column_list[2]])\naxes[1, 3].boxplot(df_rating[column_list[3]])\n\nfigure.tight_layout()","049a4942":"df_rating.skew(axis=0)","553a58e4":"for c in df_rating.columns[6:31]:\n\n    q1 = df_rating[c].quantile(0.25)\n    q3 = df_rating[c].quantile(0.75)\n    iqr = q3 - q1 #Interquartile range\n    fence_low  = q3-1.5*iqr\n    fence_high = q1+1.5*iqr\n    lower_out = len(df_rating.loc[(df_rating[c] < fence_low)  ,c])\n    upper_out = len(df_rating.loc[(df_rating[c] > fence_high)  ,c])\n    outlier_count = upper_out+lower_out\n    prop_out = outlier_count\/len(df_rating)\n    print(c, \": \"+\"{:.2%}\".format(prop_out))\n","866e5e25":"df_rating_outlier = df_rating.copy()\n\nfor c in df_rating_outlier.columns[6:31]:\n    \n    q1 = df_rating_outlier[c].quantile(0.25)\n    q3 = df_rating_outlier[c].quantile(0.75)\n    iqr = q3 - q1 #Interquartile range\n    fence_low  = q3-1.5*iqr\n    fence_high = q1+1.5*iqr\n    \n    for i in range(len(df_rating_outlier)):\n        \n        if df_rating.loc[i,c] < fence_low or df_rating.loc[i,c] > fence_high: # if Outlier\n            \n            df_rating_outlier.loc[i,c] = 1\n        \n        else: # Not Outlier\n            df_rating_outlier.loc[i,c] = 0","636340bc":"df_rating_outlier.head()","5f8856e3":"df_rating_outlier[\"total\"] = df_rating_outlier.sum(axis=1)\ndf_rating_outlier.total.hist(bins = 20)","a0ea76f0":"from sklearn import preprocessing\nmin_max_scaler = preprocessing.MinMaxScaler()\n\nfor c in df_rating.columns[6:31]:\n\n    df_rating[[c]] = min_max_scaler.fit_transform(df_rating[[c]].to_numpy())*1000\n    df_rating[[c]] = df_rating[c].apply(lambda x: np.log10(x+0.01))","3f6bd97a":"figure, axes = plt.subplots(nrows=2, ncols=4, figsize=(9,5))\n\naxes[0, 0].hist(df_rating[column_list[0]])\naxes[0, 1].hist(df_rating[column_list[1]])\naxes[1, 0].hist(df_rating[column_list[2]])\naxes[1, 1].hist(df_rating[column_list[3]])\n\naxes[0, 2].boxplot(df_rating[column_list[0]])\naxes[1, 2].boxplot(df_rating[column_list[1]])\naxes[0, 3].boxplot(df_rating[column_list[2]])\naxes[1, 3].boxplot(df_rating[column_list[3]])\n\nfigure.tight_layout()","28bf861a":"df_rating_no_out = df_rating.copy()\n\nfor c in df_rating_no_out.columns[6:31]:\n\n    q05 = df_rating_no_out[c].quantile(0.10)\n    q95 = df_rating_no_out[c].quantile(0.90)\n    iqr = q95 - q05 #Interquartile range\n    fence_low  = q05-1.5*iqr\n    fence_high = q95+1.5*iqr\n    df_rating_no_out.loc[df_rating_no_out[c] > fence_high,c] = df_rating_no_out[c].quantile(0.25)\n    df_rating_no_out.loc[df_rating_no_out[c] < fence_low,c] = df_rating_no_out[c].quantile(0.75)\n    ","b7d7b1cf":"figure, axes = plt.subplots(nrows=8, ncols=3, figsize=(20,44))\n\ni = 0 \nj = 0\n\nfor c in df_rating_no_out.columns[6:30]:\n    \n    sns.boxplot(x=df_rating_no_out.Rating, y=df_rating_no_out[c], palette=\"Set3\", ax=axes[i, j])\n    \n    if j == 2:\n        j=0\n        i+=1\n    else:\n        j+=1    \n","7d124244":"df_rating.colors = 'a'\ndf_rating_no_out.loc[df_rating_no_out['Rating'] == 'Lowest Risk', 'color'] = 'r'\ndf_rating_no_out.loc[df_rating_no_out['Rating'] == 'Low Risk', 'color'] = 'g'\ndf_rating_no_out.loc[df_rating_no_out['Rating'] == 'Medium Risk', 'color'] = 'b'\ndf_rating_no_out.loc[df_rating_no_out['Rating'] == 'High Risk','color'] = 'y'\ndf_rating_no_out.loc[df_rating_no_out['Rating'] == 'Highest Risk', 'color'] = 'm'","95bd0925":"column_list = list(df_rating.columns[6:31])\ncolumn_list = sample(column_list,12) ","dab92910":"figure, axes = plt.subplots(nrows=3, ncols=2, figsize=(14,14))\n\ni = 0 \nj = 0\n\nfor c in range(0,12, 2):\n\n    sns.scatterplot(x = column_list[c], y=column_list[c+1], hue=\"color\", data=df_rating_no_out, ax=axes[j,i])\n    \n    if i == 1:\n        i = 0\n        j +=1\n    \n    else:\n        i+=1","1eed1d84":"le = preprocessing.LabelEncoder()\nle.fit(df_rating.Sector)\ndf_rating.Sector = le.transform(df_rating.Sector) # encode sector\nle.fit(df_rating.Rating)\ndf_rating.Rating = le.transform(df_rating.Rating) # encode rating","d5553f62":"df_train, df_test = train_test_split(df_rating, test_size=0.2, random_state = 1234)","b5ee9ffd":"X_train, y_train = df_train.iloc[:,5:31], df_train.iloc[:,0]\nX_test, y_test = df_test.iloc[:,5:31], df_test.iloc[:,0]","c8448062":"XGB_model = xgb.XGBRegressor(objective ='multi:softmax', num_class =4)\nXGB_model.fit(X_train, y_train)\ny_pred_XGB = XGB_model.predict(X_test)\nAccuracy_XGB = metrics.accuracy_score(y_test, y_pred_XGB)\nprint(\"XGB Accuracy:\",Accuracy_XGB)","6c4103c0":"GBT_model = GradientBoostingClassifier(random_state=123)\nGBT_model.fit(X_train, y_train)\ny_pred_GBT = GBT_model.predict(X_test)\nAccuracy_GBT = metrics.accuracy_score(y_test, y_pred_GBT)\nprint(\"GBT Accuracy:\",Accuracy_GBT)","af92a415":"RF_model = RandomForestClassifier(random_state=1234)\nRF_model.fit(X_train,y_train)\ny_pred_RF = RF_model.predict(X_test)\nAccuracy_RF = metrics.accuracy_score(y_test, y_pred_RF)\nprint(\"RF Accuracy:\",Accuracy_RF)","abf4378f":"SVC_model = svm.SVC(kernel='rbf', gamma= 2, C = 5, random_state=1234)\nSVC_model.fit(X_train, y_train)\ny_pred_SVM = SVC_model.predict(X_test)\nAccuracy_SVM = metrics.accuracy_score(y_test, y_pred_SVM)\nprint(\"SVM Accuracy:\",Accuracy_SVM)","278a06e9":"MLP_model = MLPClassifier(hidden_layer_sizes=(5,5,5), activation='logistic', solver='adam', max_iter=1500)\nMLP_model.fit(X_train, y_train)\ny_pred_MLP = MLP_model.predict(X_test)\nAccuracy_MLP = metrics.accuracy_score(y_test, y_pred_MLP)\nprint(\"MLP Accuracy:\",Accuracy_MLP)","060fac4f":"GNB_model = GaussianNB()\nGNB_model.fit(X_train, y_train)\ny_pred_GNB = GNB_model.predict(X_test)\nAccuracy_GNB = metrics.accuracy_score(y_test, y_pred_GNB)\nprint(\"GNB Accuracy:\",Accuracy_GNB)","fdc62450":"LDA_model = LinearDiscriminantAnalysis()\nLDA_model.fit(X_train,y_train)\ny_pred_LDA = LDA_model.predict(X_test)\nAccuracy_LDA = metrics.accuracy_score(y_test, y_pred_LDA)\nprint(\"LDA Accuracy:\",Accuracy_LDA)","7b675dc8":"QDA_model = QuadraticDiscriminantAnalysis()\nQDA_model.fit(X_train,y_train)\ny_pred_QDA = QDA_model.predict(X_test)\nAccuracy_QDA = metrics.accuracy_score(y_test, y_pred_QDA)\nprint(\"QDA Accuracy:\",Accuracy_QDA)","6090602e":"KNN_model = KNeighborsClassifier(n_neighbors = 3)\nKNN_model.fit(X_train,y_train)\ny_pred_KNN = KNN_model.predict(X_test)\nAccuracy_KNN = metrics.accuracy_score(y_test, y_pred_KNN)\nprint(\"KNN Accuracy:\",Accuracy_KNN)","ed57b0b9":"LR_model = LogisticRegression(random_state=1234 , multi_class='multinomial', solver='newton-cg')\nLR_model = LR_model.fit(X_train, y_train)\ny_pred_LR = LR_model.predict(X_test)\nAccuracy_LR = metrics.accuracy_score(y_test, y_pred_LR)\nprint(\"LR Accuracy:\",Accuracy_LR)","f8c880ce":"accuracy_list = [Accuracy_XGB, Accuracy_GBT, Accuracy_RF, Accuracy_SVM, Accuracy_MLP, Accuracy_GNB, \n                 Accuracy_LDA, Accuracy_QDA, Accuracy_KNN, Accuracy_LR]\n\nmodel_list = ['XGBboost', 'Gradient Boosting', 'Random Forest', 'Support Vector Machine', \n              \"Neural Network\", 'Naive Bayes', 'Linear Discriminat', 'Quadratic Discriminat', \n              'KNN', 'Logistic Regression']\n\ndf_accuracy = pd.DataFrame({'Model': model_list, 'Accuracy': accuracy_list})","25730361":"order = list(df_accuracy.sort_values('Accuracy', ascending=False).Model)\ndf_accuracy = df_accuracy.sort_values('Accuracy', ascending=False).reset_index().drop(['index'], axis=1)\n\nplt.figure(figsize=(12,8))\n# make barplot and sort bars\nx = sns.barplot(x='Model', y=\"Accuracy\", data=df_accuracy, order = order, palette=\"rocket\")\nplt.xlabel(\"Model\", fontsize=20)\nplt.ylabel(\"Accuracy\", fontsize=20)\nplt.title(\"Accuracy by Model\", fontsize=20)\nplt.grid(linestyle='-', linewidth='0.5', color='grey')\nplt.xticks(rotation=70, fontsize=12)\nplt.ylim(0,1)\nplt.gca().yaxis.set_major_formatter(mtick.PercentFormatter(1))\n\nfor i in range(len(model_list)):\n    plt.text(x = i, y = df_accuracy.loc[i, 'Accuracy'] + 0.05, s = str(round((df_accuracy.loc[i, 'Accuracy'])*100, 2))+'%', \n             fontsize = 14, color='black',horizontalalignment='center')\n\ny_value=['{:,.2f}'.format(x) + '%' for x in ax.get_yticks()]\nax.set_yticklabels(y_value)\n\nplt.tight_layout()\n","91acb0ab":"dtrain = xgb.DMatrix(X_train, label=y_train)\ndtest = xgb.DMatrix(X_test, label=y_test)","b02eb74f":"params = XGB_model.get_xgb_params()","9005108e":"params","b49667b1":"params['eval_metric'] = \"merror\"","0bebd7ef":"num_boost_round = 1000","465aad20":"model = xgb.train(\n    params,\n    dtrain,\n    num_boost_round=num_boost_round,\n    evals=[(dtest, \"Test\")],\n    early_stopping_rounds=50,\n    verbose_eval=30)\n\nprint(\"Best merror: {:.2f} with {} rounds\".format(\n                 model.best_score,\n                 model.best_iteration+1))","dcbee8e3":"cv_results = xgb.cv(\n    params,\n    dtrain,\n    num_boost_round=num_boost_round,\n    seed=42,\n    nfold=5,\n    metrics={'merror'},\n    early_stopping_rounds=50,\n    verbose_eval=30\n)\ncv_results.tail()","85287560":"cv_results['test-merror-mean'].min()","0d222777":"gridsearch_params = [\n    (max_depth, min_child_weight)\n    for max_depth in range(5,12)\n    for min_child_weight in range(5,8)\n]","76057d14":"# Define initial best params and MAE\nmin_merror = float(\"Inf\")\nbest_params = None\nfor max_depth, min_child_weight in gridsearch_params:\n    print(\"CV with max_depth={}, min_child_weight={}\".format(\n                             max_depth,\n                             min_child_weight))\n    \n    # Update our parameters\n    params['max_depth'] = max_depth\n    params['min_child_weight'] = min_child_weight\n    # Run CV\n    cv_results = xgb.cv(\n        params,\n        dtrain,\n        num_boost_round=num_boost_round,\n        seed=42,\n        nfold=5,\n        metrics={'merror'},\n        early_stopping_rounds=50,\n        verbose_eval=False\n\n    )\n    # Update best merror\n    mean_merror = cv_results['test-merror-mean'].min()\n    boost_rounds = cv_results['test-merror-mean'].argmin()\n    print(\"\\tMAE {} for {} rounds\".format(mean_merror, boost_rounds))\n    if mean_merror < min_merror:\n        min_merror = mean_merror\n        best_params = (max_depth,min_child_weight)\nprint(\"Best params: {}, {}, merror: {}\".format(best_params[0], best_params[1], min_merror))","3f70bd8a":"params['max_depth'] = 7\nparams['min_child_weight'] = 5","3eeb2430":"gridsearch_params = [\n    (subsample, colsample)\n    for subsample in [i\/10. for i in range(7,11)]\n    for colsample in [i\/10. for i in range(7,11)]\n]","a7602fae":"# Define initial best params and MAE\nmin_merror = float(\"Inf\")\nbest_params = None\n\nfor subsample, colsample in reversed(gridsearch_params):\n    print(\"CV with subsample={}, colsample={}\".format(\n                             subsample,\n                             colsample))\n    # We update our parameters\n    params['subsample'] = subsample\n    params['colsample_bytree'] = colsample\n    \n    # Run CV\n    cv_results = xgb.cv(\n        params,\n        dtrain,\n        num_boost_round=num_boost_round,\n        seed=42,\n        nfold=5,\n        metrics={'merror'},\n        early_stopping_rounds=10,\n        verbose_eval=False\n    )\n    \n    # Update best MAE\n    mean_merror = cv_results['test-merror-mean'].min()\n    boost_rounds = cv_results['test-merror-mean'].argmin()\n    print(\"\\tMAE {} for {} rounds\".format(mean_merror, boost_rounds))\n    if mean_merror < min_merror:\n        min_merror = mean_merror\n        best_params = (subsample,colsample)\n        \nprint(\"Best params: {}, {}, merror: {}\".format(best_params[0], best_params[1], min_merror))","3e91a1b6":"params['subsample'] =0.9\nparams['colsample_bytree'] = 0.7","e1965f8b":"%time\n# This can take some time\u2026\nmin_merror = float(\"Inf\")\nbest_params = None\n\nfor eta in [.3, .2, .1, .05, .01, .005]:\n    print(\"CV with eta={}\".format(eta))\n    # We update our parameters\n    params['eta'] = eta\n    # Run and time CV\n    cv_results = xgb.cv(\n            params,\n            dtrain,\n            num_boost_round=num_boost_round,\n            seed=42,\n            nfold=5,\n            metrics=['merror'],\n            early_stopping_rounds=10\n)\n    # Update best score\n    mean_mae = cv_results['test-merror-mean'].min()\n    boost_rounds = cv_results['test-merror-mean'].argmin()\n    print(\"\\tMAE {} for {} rounds\\n\".format(mean_mae, boost_rounds))\n    if mean_merror < min_merror:\n        min_merror = mean_merror\n        best_params = eta\nprint(\"Best params: {}, merror: {}\".format(best_params, min_merror))","3a5f69c1":"params['eta'] = .3","78e864c7":"params","7d4d7eca":"model = xgb.train(\n    params,\n    dtrain,\n    num_boost_round=num_boost_round,\n    evals=[(dtest, \"Test\")],\n    early_stopping_rounds=1000,\n    verbose_eval=100\n)","c79a71de":"num_boost_round = model.best_iteration + 1\nbest_model = xgb.train(\n    params,\n    dtrain,\n    verbose_eval=100,\n    num_boost_round=num_boost_round,\n    evals=[(dtest, \"Test\")]\n)","ca373cd0":"metrics.accuracy_score(best_model.predict(dtest), y_test)","6569672e":"cm = confusion_matrix(y_test, y_pred_XGB)","eea36197":"fig, ax = plt.subplots(figsize=(8,8))\n\nsns.heatmap(cm, annot = True, ax = ax, vmin=0, vmax=150, fmt=\"d\", linewidths=.5, linecolor = 'white', cmap=\"Reds\") # annot=True to annotate cells\n\n# labels, title and ticks\nax.set_xlabel('Predicted labels')\nax.set_ylabel('True labels'); \nax.set_title('Confusion Matrix'); \nax.xaxis.set_ticklabels(['Medium Risk','Highest Risk', 'Low Risk', 'High Risk'])\nax.yaxis.set_ticklabels(['Medium Risk','Highest Risk', 'Low Risk', 'High Risk']);\n\n# This part is to correct a bug from the heatmap funciton from pyplot\nb, t = plt.ylim() # discover the values for bottom and top\nb += 0.5 # Add 0.5 to the bottom\nt -= 0.5 # Subtract 0.5 from the top\nplt.ylim(b, t) # update the ylim(bottom, top) values\n\nplt.show()","269fbe5e":"print(classification_report(y_test, y_pred_XGB, target_names = ['Medium Risk','Highest Risk', 'Low Risk', 'High Risk']))","0c2947ee":"thresholds = sort(XGB_model.feature_importances_)\nfor thresh in thresholds:\n    # select features using threshold\n    selection = SelectFromModel(XGB_model, threshold=thresh, prefit=True)\n    select_X_train = selection.transform(X_train)\n    # train model\n    selection_model = XGBClassifier()\n    selection_model.fit(select_X_train, y_train)\n    # eval model\n    select_X_test = selection.transform(X_test)\n    y_pred = selection_model.predict(select_X_test)\n    predictions = [round(value) for value in y_pred]\n    accuracy = accuracy_score(y_test, predictions)\n    print(\"Thresh=%.3f, n=%d, Accuracy: %.2f%%\" % (thresh, select_X_train.shape[1], accuracy*100.0))","c13a0de5":"from xgboost import plot_importance\n\nfig, ax = plt.subplots(figsize=(8, 8))\n# xgboost.plot_importance(..., ax=ax)\n\nplot_importance(model, ax=ax)\nplt.show()","5c61db7f":"def WCloud(dataframe, column, rating):\n    \n    words = ''\n    \n    # iterate through the csv file \n    for val in dataframe.loc[dataframe['Rating'] == rating, column]:\n      \n        # typecaste each val to string \n        val = str(val)\n        val = val.replace(\".\", \"\")\n        val = val.replace(\",\", \"\")\n\n        # split the value \n        tokens = val.split()\n\n        #Converts each token into lowercase \n        for i in range(len(tokens)): \n            tokens[i] = tokens[i].lower() \n\n        words += \" \".join(tokens) + \" \"\n        \n    return words","6b488142":"stop_words = ['global', 'incorporated', 'corporation', ' corp', 'industries', 'technologies', 'co', 'inc', 'limited', 'ltd', 'technology', 'resources', 'corp', 'group', 'communications',\n             'holdings',' holding', 'plc', 'group', 'oil', 'resource', 'company','international', 'states', 'ag', ' sa', 'pty', 'international', 'united', 'states', 'partners', 'group', \n             'spa', 'se', 'lp', '(the)', 'the', 'LLC', 'n.v', 'service', 'products', 'companies', 'company', 'energy','corporation', 'holdings', 'company', 'limited',\n             'holding', 'partners', 'industries', 'nv', 'semiconductor', 'rr', 'usa', 'homes', 'eletric', 'petroleum']","2bd4ec14":"comment_wordsHR = WCloud(df_rating, 'Name', 0)\nclean_text = [word for word in comment_wordsHR.split() if word not in stop_words]\ncomment_wordsHR = ' '.join([str(elem) for elem in clean_text])\n\ncomment_wordsHRest = WCloud(df_rating, 'Name', 1)\nclean_text = [word for word in comment_wordsHRest.split() if word not in stop_words]\ncomment_wordsHRest = ' '.join([str(elem) for elem in clean_text])\n\ncomment_wordsLR = WCloud(df_rating, 'Name', 2)\nclean_text = [word for word in comment_wordsLR.split() if word not in stop_words]\ncomment_wordsLR = ' '.join([str(elem) for elem in clean_text])\n\ncomment_wordsMR = WCloud(df_rating, 'Name', 3)\nclean_text = [word for word in comment_wordsMR.split() if word not in stop_words]\ncomment_wordsMR = ' '.join([str(elem) for elem in clean_text])","7b453438":"wordcloudMR = WordCloud(background_color ='white', colormap=\"twilight\", max_font_size = 25,\n                min_font_size = 10).generate(comment_wordsHR) \n\nwordcloudLR = WordCloud(background_color ='white', colormap=\"twilight\", max_font_size = 25,\n                min_font_size = 10).generate(comment_wordsLR) \n\nwordcloudHR = WordCloud(background_color ='white',  colormap=\"ocean\",max_font_size = 25,\n                min_font_size = 10).generate(comment_wordsHR) \n\nwordcloudHRest = WordCloud(background_color ='white', colormap=\"gnuplot2\",max_font_size = 25,\n                min_font_size = 10).generate(comment_wordsHRest) \n","b60bbd78":"fig = plt.figure(figsize = (17,10))\naxes = fig.subplots(nrows=2, ncols=2)\n\nplt.subplot(2, 2, 1)\nplt.imshow(wordcloudMR, interpolation=\"bilinear\") \nplt.axis(\"off\") \nplt.margins(x=0, y=0)\nplt.title('Medium Risk Companies', fontsize = 27)\n\nplt.subplot(2, 2, 2)\nplt.imshow(wordcloudLR, interpolation=\"bilinear\") \nplt.axis(\"off\") \nplt.margins(x=0, y=0)\nplt.title('Low Risk Companies', fontsize = 27)\n\nplt.subplot(2, 2, 3)\nplt.imshow(wordcloudHR, interpolation=\"bilinear\") \nplt.axis(\"off\") \nplt.margins(x=0, y=0)\nplt.title('High Risk Companies', fontsize = 27)\n\nplt.subplot(2, 2, 4)\nplt.imshow(wordcloudHRest, interpolation=\"bilinear\") \nplt.axis(\"off\") \nplt.margins(x=0, y=0)\nplt.title('Highest Risk Companies', fontsize = 27, fontweight = 2)","f9b7df61":"#### Random Forest","77978f0f":"#### Linear Discriminant Analysis","b8fd3b6c":"Now that we have this dataframe we can use it use it to observe the data from a different angle. We will be able to observe the distribution that was hidden by the outliers. The first step:\n   - Plot all columns (boxplot) by each label:`High Risk`, `Low Risk`, `Medium Risk`, `Highest Risk`.","5df72525":"The names of companies usually have different suffixes that are not interesting for us, such as \"LLC\", \"plc\", \"holding\" etc. Therefore we will consider it as stopwords.","766c63fd":"#### K Nearest Neighbours\n","7368ede4":"We observe that the dataset is very unbalanced. We have 671 triple-Bs (BBB) but only 1 D. However, we are working with Ratings from different companies such as `Moody's`, `Standard & Poor's` and more. Therefore it is preferred to simplify the labels according to this table from the website [investopedia](https:\/\/www.investopedia.com\/terms\/c\/corporate-credit-rating.asp). We will classify our labels according to the grading risk and not the rate. \n\n| Bond Rating |                   |          |            |              |\n|-------------|-------------------|----------|------------|--------------|\n| Moody's     | Standard & Poor's | Fitch    | Grade      | Risk         |\n| Aaa         | AAA               | AAA      | Investment | Lowest Risk  |\n| Aa          | AA                | AA       | Investment | Low Risk     |\n| A           | A                 | A        | Investment | Low Risk     |\n| Baa         | BBB               | BBB      | Investment | Medium Risk  |\n| Ba, B       | BB, B             | BB, B    | Junk       | High Risk    |\n| Caa\/Ca      | CCC\/CC\/C          | CCC\/CC\/C | Junk       | Highest Risk |\n| C           | D                 | D        | Junk       | In Default   |\n\n\nTo do it we will replace with a dictonary each of this ratings. ","1217f940":"#### Neural Network","c179ba7e":"This is a very interesting plot. We can see that only up to 400 rows don't have any outliers. Most rows have outliers and maybe they will be useful in the further classification tasks. Therefore we see no value in excluding the outliers from the dataset. However we will perform a transformation on the data so we can reduce its negative impact.\n\n#### Data reshaping\n\nWe will now perform the following steps in each of the numerical data. \n1. Normalize the data between 0 and 1 (and multiply by 1.000).\n2. Apply log on base 10 on each of the variables. ","c1530095":"The num_boost_round which corresponds to the maximum number of boosting rounds that we allow. ","e6222d69":"# Machine Learning \n\nIs it possible to predict what creidt profile a company will receive from a rating agency based on its financial indicators? If so, what are the most important predictors? Apparently not much work has been done with regards to this question. This academic [paper](https:\/\/www.researchgate.net\/publication\/331386740_Credit_Rating_Forecasting_Using_Machine_Learning_Techniques) was the only work found about it. It is worth checking it out. As we will do it, it tests most ML algorithms and identifies the most important features. \n\nIn the following steps we will perform the following:\n\n1. Prepare the dataset \n    - Split in train and test\n    - Transform\/Encode the features kand labels\n2. Test a wide range of ML models (Tree-based, Probabilistic and so on). \n3. Compare the accuracry of all models. \n4. Choose our winning model and tune hyperparameters to target a higher accuracy.\n5. Make a more profound evaluation of the result with a confusion matrix and different measures. \n6. identify the most important features to predict the rating. \n\n\n\n## Prepare the Dataset","9a7e45c8":"As predicted, the data is comtaminated by outliers. We canot observe real behaviour of the distribution because some points differ too much from the others. We will use the function `.skew` from pandas in all columns. It should return between 0 and 1 if a column is normally distributed. ","7d3278f2":"Unfortunately, given the lack of Credit Ratings classified as `Lowest Risk` and `In Default` we will have to eliminate then from the table. However, the dataset will keep unbalanced and if needed we will have to adress this issue in further steps. ","b52f54df":"The most interesting point about the previous plots is the fact that they clearly show a difference in the medians and distribution according to the rating (Risk). It points to a scenario where the variables will have good predictive power for classification. Following with our analysis we will create scatter plots to see if we can observe who the variables relate to each other and how labels can be observer in respect to it.","e824c584":"### Analyse Labels\n\nAs we know we are working with ordinal labels. That means there is a scale from more secure to less secure ratings. For instance, the triple-A (AAA) is the most secure rating a company can receive. On the other hand, the rating D is the less secure. It means the company will likely default on its creditors. Let's have a first look at the how many reatings we have of each in the dataset. \n","d45f9340":"We finally plot all together....","1a50e314":"We did not arrive in an enhanced model with this tunning. Anyone is welcome to continue this tunning and achieve a superior accuracy.","19ad44a3":"#### Import Dataset","a3b1ab13":"We have 26 columns of numerical data and 6 descriptive columns (one of which is the label).There are no missing values.\n<br> A first look at the data:","7682d026":"### Again the plots","2e203ea0":"# Fit Models\n\nNow we will test a range of models. In each we will fit the model in the train data, make predictons for the test data and  obtain the accuracy. In later steps we will compare the accuracy of all the models. We will use primarily the library `sklearn` but also `XGBoost`.  \n\n#### XGBoost","40b98b50":"#### Gradient Boosting Classifier","55e10e5c":"We have a problem with respect to vizualisation of the data. The impact of the outliers is so big that we cannot observe the patterns in the data. To enhance our visualization we will from now ignore outliers. We will replace then by values with lower impact such as the lower hinge. In this way we will be able to continue with our EDA. To preserve our dataset we will use a new table called `df_rating_no_out`.","02f97dc7":"We observe this is a generalized problem. As we can see almost all columns are extremely skewed. We will now go deeper in the investigation of outliers. The following code will return the proportion of outliers in each column . The definition of outlier will be the one from the boxplot - above or bellow `1.5 x IQR`.","eac2671d":"We have our winner. XGboost is the best performing model. \n\n## XGBoost Hyperparameter Tunning\n\nThe XGboost model has achieved a very high accuracy given that we have 4 different classes. Now we will try to increase the performance even more. We will use a cross-validation approach and we will follow similar steps to this [tutorial](https:\/\/blog.cambridgespark.com\/hyperparameter-tuning-in-xgboost-4ff9100a3b2f). First we load the train and test data into DMatrices. `DMatrix` is a data structure used by XGBoost to optimize both memory efficiency and training speed.","81303c6e":"#### The params dictionary\nWe create a dictonary with the parameters from our previous XGboost model.","6c5eaace":"Now we will be able to count how many outliers each row has and plot it. ","98ef58a4":"## Compare Results","cf515a7d":"In this step we use the function `wordcloud` to create the plot.","2f27fd98":"### Skewness and Outliers\n\nWe observe a lot of skewness in the data with this first exploration. In this case, it means that most variables in the dataset may strong presence of outliers. Taking as observation the table above the first column:\n\n- `currentRatio`: This 50% of its variables between `1.071` and `2.166891`. The minimum value is `-0.932005` however the maximum value is `1725.505005`. It means, in other words, there is a giant outlier that is extremely distant from most points from the data (currentRatio). \n\nThe same pattern can be observed in the following columns such as `quickRatio`,\t`cashRatio`, `daysOfSalesOutstanding`, `netProfitMargin` and so on.\n\nTo observe how this reflect on the distribution of the data lets make some plots of variables chose randomly.","d2e174e5":"## Exploratory Data Analysis\n\nOur first step is to perform an exploratory data analysis to understand the charateristics of dataset. Here are some quesitons we will try to adress:\n\n- What are the dimensions of the data?\n- How do predictors relate to each other?\n- What are the classes of the data?\n- How are the predictors distributed?\n- How are the labels distributed?\n- Do we have missing values?\n- Are outliers are relevant?\n- Are there any transformations that must be done with the dataset?","5710092e":"## Confusion Matrix\n\nWe will now analyse according to each class the performance of the model. The best way to do it is with a confusion matrix. We can see how many points were missclassified and where were then classified to if not the right rating.","717322fb":"# Corporate Credit Rating Forecasting \n\n\n\n### Introduction\n\nThis notebook contains the results of the data analysis performed on a set of corporate credit ratings given by ratings agencies to a set of companies. The aim of the data analysis is to build a machine learning model from the rating data that can be used to predict the rating a company will receive.\n\nThe first section section of the notebook shows the exploratory data analysis (EDA) performed to explore and understand the data. It looks at each attribute (variable) in the data to understand the nature and distribution of the attribute values. It also examines the correlation between the variables through visual analysis. A summary at the end highlights the key findings of the EDA.\n\nThe second section shows the development of a machine learning model. Many diffferent models are tested and the performance of all models are compared. Subsequently, a winner is selected and we do hyperparameter tunning.\n\nIn the model evaluation step we use different techniques such as a confusion matrix and scores as F1, Precision and Recall to understand different aspects of the performance of the model. We also perform feature selection to know what financial indicators are more relevant for the rating agencies. \n\n\n### The Dataset\n\nThere are 30 features for every company of which 25 are financial indicators. \nThey can be divided in: <br>\n- **Liquidity Measurement Ratios**: currentRatio, quickRatio, cashRatio, daysOfSalesOutstanding\n- **Profitability Indicator Ratios**: grossProfitMargin, operatingProfitMargin, pretaxProfitMargin, netProfitMargin, effectiveTaxRate, returnOnAssets, returnOnEquity, returnOnCapitalEmployed\n- **Debt Ratios**: debtRatio, debtEquityRatio\n- **Operating Performance Ratios**: assetTurnover\n- **Cash Flow Indicator Ratios**: operatingCashFlowPerShare, freeCashFlowPerShare, cashPerShare, operatingCashFlowSalesRatio, freeCashFlowOperatingCashFlowRatio \n\n\n### Libraries used:\n- pandas\n- numpy\n- matplotlib\n- seaborn\n- random\n- sklearn\n- xgboost\n- wordcloud","9fb78ebb":"Now we are ready to start tuning. We will first tune our parameters to minimize the merror on cross-validation, and then check the performance of our model on the test dataset. \n\n#### Parameters `max_depth` and `min_child_weight`. ","7efca376":"### Descriptive Statistics\n\nNow we will use statistical tools, especially from pandas to improve the understanding from the dataset, especially the numerical features. We have seen there are 25 numerical columns in the dataset, all of each are financial indicators from the companies. The function `describe()` returns information about the distribution of the data such as `quantiles`, `min` and `max`.","9f8e3717":"#### Support Vector Machine\n","76a9643e":"#### Ignore Warnings","3ea1c815":"We will use the `merror` error parameter from classification. It is basic an accuracy. ","069a892b":"# Visualize Companies\n\n**Bonus**: In this dataset we are working exclusevely with companies that are traded in the stock exchanges from the US. \nNow, we will visualize which companies are considered secure to lend money according to agencies. We will make 4 different wordclouds, one for each rating of risk. \n \n #### Create a function to generate text for the word cloud\n ","ff390a17":"Apparetly the fact that we have more labels in the edium Risk has enhanced its classification. However, overall we have achieved good classification scaores for most, with the exception of Highest Risk.\n\n\n## Feature Selection\nIn our tast task we will identify which features were the most valuable for our model. In our first step we will check if by any chance we can increase the accuracy of our model extracting a feature.","0cfe278e":"#### Using XGBoost\u2019s CV\n\nIn order to tune the other hyperparameters, we will use the cv function from XGBoost. It allows us to run cross-validation on our training dataset and returns a mean merror score. We will use a `k = 5` for every parameter.\n","f484a589":"#### Parameter `ETA`\n","bcfd16d6":"We will now use the function `.info()` to see the classes of columns and search missing values.  ","dc518a0a":"It is not the case. Now lets visualize which are the most relevant features. ","36669e31":"Most columns have a significant number of outliers. However it is not clear for us if there are a few rows that all outliers or each of the rows may be contributing individually with some outliers. We will now check by row the distribution of outliers. We will create a new dataframe that `df_rating_outlier` that will be used with this purpose. In this dataframe every cell will 1 one if the corresponding cell is an outlier in `df_raint` and 0 if it is not.","13c928d8":"#### Parameters `subsample` and `colsample_bytree`","e8561f25":"#### Naive Bayes","b9058d49":"We generate the texts and create remove stopwords.","4bcb1e33":"In fact, we are working with a dataset that has a big numer of dimensions. With two variables it would not be possible to make any predictions. However this is not the case. Unfortunately we are not able to vizualise the data in all its dimensions, but luckely we will be able to perform accurate classificaitons. ","d532c2db":"Load the Libraries used in the notebook","bc5fcfef":"#### Quadratic Discriminant Analysis","ceab115f":"#### Analysis\nGiven the fact that the dataset is very unbalanced, with have achieved a very low accuracy (actually 0) for very risky companies. To deal with it we wiould have to apply upsampling techniques which we may in a future work. Now we analyse other metrics as Precision, recall and F1 from our targets.","04473432":"You may observe that some companies may be in different plots. Thats because they have been rated in different times with different rates.","a8763ba6":"We get the best score with a max_depth of 9 and min_child_weight of 6, so let's","824e1bc9":"#### Results\n\nThis are the final parameters of our tunned model.","b7f51f2f":"Let\u2019s train a model with it and see how well it does on our test set!","599c2b26":"#### Logistic Regression\n"}}