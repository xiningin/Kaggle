{"cell_type":{"326168c5":"code","016b1aa3":"code","5df4151a":"code","a1bbf5c9":"code","f6b08827":"code","5af46b60":"code","aa5c7737":"code","47a0827e":"code","6e110b4c":"code","b4324bdb":"code","f93e0fe7":"code","05ffe896":"code","419746a7":"code","bc2dbbae":"code","dbbd462c":"code","bf314bf0":"code","4d8b2a30":"code","594cf4c4":"code","e64e452a":"code","db7a2adb":"code","f69413cd":"code","df034f28":"code","63d2214c":"code","86004f9f":"code","ac5e89ae":"code","1ed1dd57":"code","25b416cd":"code","4251190a":"code","f3cb1741":"markdown","00033dfd":"markdown","184478b8":"markdown","546b58fd":"markdown","39ae2845":"markdown","cc320c80":"markdown","6dec9f05":"markdown","41a9728c":"markdown","a5df764b":"markdown","f65b8bb4":"markdown","60869249":"markdown","0c3460fb":"markdown","d6952637":"markdown","3fc48401":"markdown","8e4f3a48":"markdown","c56bd145":"markdown","09a9c211":"markdown","28b0813a":"markdown","06a14c8a":"markdown","cf8cfc27":"markdown","2a9d2e20":"markdown","9c97cf49":"markdown","b843728a":"markdown","a7f729dc":"markdown","a331f335":"markdown","206d2933":"markdown","1260ba90":"markdown","defe23eb":"markdown","b8828942":"markdown","9bd91f64":"markdown","75d54411":"markdown","30ecbefa":"markdown","62b4fa16":"markdown","a17018d6":"markdown","08cb1f19":"markdown","9adb13a2":"markdown","2128c3cc":"markdown","63064bbf":"markdown","21e2c548":"markdown","de14035f":"markdown","ed3dfbc7":"markdown","bdf64a98":"markdown"},"source":{"326168c5":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","016b1aa3":"#Reading the csv file using pandas\ndata = pd.read_csv('..\/input\/timeseries\/Train.csv')","5df4151a":"#Checking the first  values of the  dataset\ndata.head()","a1bbf5c9":"#Checking the shape of the dataset\ndata.shape","f6b08827":"#Defining the train and test in 80-20 split\ntrain = data[0:14630]\ntest = data[14630:]","5af46b60":"#First five values of the train dataset\ntrain.head()","aa5c7737":"#First five values of test dataset\ntest.head()","47a0827e":"#Function to do some processing\ndef processing(df):\n        df.Timestamp = pd.to_datetime(df.Datetime,format='%d-%m-%Y %H:%M') \n        df.index = df.Timestamp \n        df = df.resample('D').mean()\n        df.drop('ID',1,inplace = True)\n        return df","6e110b4c":"#Passing through the function\ntrain = processing(train)\ntest =processing(test)\ndata = processing(data)","b4324bdb":"#Ploting the seasonal decompose\nimport matplotlib.pyplot as plt\nfrom statsmodels.tsa.seasonal import seasonal_decompose\ndecompose = seasonal_decompose(data, model='multiplicative')\nfig = plt.figure()\nfig = decompose.plot(),","f93e0fe7":"#Deriving the function of Dickey fuller test for checking the stationarity\nfrom statsmodels.tsa.stattools import adfuller\ndef adf_test(series):\n    result = adfuller(series.dropna(),autolag='AIC')\n    if result[1] <= 0.05:\n        print(\"Data  is stationary\")\n    else:\n        print(\"Data  is non-stationary\")","05ffe896":"adf_test(data[\"Count\"])","419746a7":"#Seasonal difference\ndata[\"Count diff\"] = data[\"Count\"]- data[\"Count\"].shift(12)\ndata[\"Count diff\"].dropna(inplace=True)","bc2dbbae":"#Checking the stationarity again\nadf_test(data[\"Count diff\"])","dbbd462c":"#Auto correlation plot and Partial correlation plot\nfrom statsmodels.graphics.tsaplots import plot_acf, plot_pacf\nplot_acf(data[\"Count diff\"], lags= 60, alpha=0.05);\nplot_pacf(data[\"Count diff\"], lags= 60, alpha=0.05);","bf314bf0":"import seaborn as sns\nimport matplotlib.pyplot as plt\n#Plotting data\ntrain.Count.plot(figsize=(15,8), title= 'Daily Commuters', fontsize=14)\ntest.Count.plot(figsize=(15,8), title= 'Daily Commuters', fontsize=14)\nplt.show()","4d8b2a30":"#Code for naive method\nimport numpy as np\ndd= np.asarray(train.Count)\ny_hat = test.copy()\ny_hat['naive'] = dd[len(dd)-1]\nplt.figure(figsize=(12,8))\nplt.plot(train.index, train['Count'], label='Train')\nplt.plot(test.index,test['Count'], label='Test')\nplt.plot(y_hat.index,y_hat['naive'], label='Naive Forecast')\nplt.legend(loc='best')\nplt.title(\"Naive Forecast\")\nplt.show()\n","594cf4c4":"from sklearn.metrics import mean_squared_error\nfrom math import sqrt\nrms = sqrt(mean_squared_error(test.Count, y_hat.naive))\nprint(rms)","e64e452a":"y_hat_avg = test.copy()\ny_hat_avg['avg_forecast'] = train['Count'].mean()\nplt.figure(figsize=(12,8))\nplt.plot(train['Count'], label='Train')\nplt.plot(test['Count'], label='Test')\nplt.plot(y_hat_avg['avg_forecast'], label='Average Forecast')\nplt.legend(loc='best')\nplt.show()","db7a2adb":"rms = sqrt(mean_squared_error(test.Count, y_hat_avg.avg_forecast))\nprint(rms)","f69413cd":"y_hat_avg = test.copy()\ny_hat_avg['moving_avg_forecast'] = train['Count'].rolling(60).mean().iloc[-1]\nplt.figure(figsize=(16,8))\nplt.plot(train['Count'], label='Train')\nplt.plot(test['Count'], label='Test')\nplt.plot(y_hat_avg['moving_avg_forecast'], label='Moving Average Forecast')\nplt.legend(loc='best')\nplt.show()","df034f28":"rms = sqrt(mean_squared_error(test.Count, y_hat_avg.moving_avg_forecast))\nprint(rms)","63d2214c":"from statsmodels.tsa.api import ExponentialSmoothing, SimpleExpSmoothing, Holt\ny_hat_avg = test.copy()\nfit2 = SimpleExpSmoothing(np.asarray(train['Count'])).fit(smoothing_level=0.1,optimized=False)\ny_hat_avg['SES'] = fit2.forecast(len(test))\nplt.figure(figsize=(16,8))\nplt.plot(train['Count'], label='Train')\nplt.plot(test['Count'], label='Test')\nplt.plot(y_hat_avg['SES'], label='SES')\nplt.legend(loc='best')\nplt.show()","86004f9f":"rms = sqrt(mean_squared_error(test.Count, y_hat_avg.SES))\nprint(rms)","ac5e89ae":"import statsmodels.api as sm\nsm.tsa.seasonal_decompose(train.Count).plot()\nresult = sm.tsa.stattools.adfuller(train.Count)\nplt.show()","1ed1dd57":"y_hat_avg = test.copy()\n\nfit1 = Holt(np.asarray(train['Count'])).fit(smoothing_level = 0.3,smoothing_slope = 0.1)\ny_hat_avg['Holt_linear'] = fit1.forecast(len(test))\n\nplt.figure(figsize=(16,8))\nplt.plot(train['Count'], label='Train')\nplt.plot(test['Count'], label='Test')\nplt.plot(y_hat_avg['Holt_linear'], label='Holt_linear')\nplt.legend(loc='best')\nplt.show()","25b416cd":"y_hat_avg = test.copy()\nfit1 = ExponentialSmoothing(np.asarray(train['Count']) ,seasonal_periods=7 ,trend='add', seasonal='add',).fit()\ny_hat_avg['Holt_Winter'] = fit1.forecast(len(test))\nplt.figure(figsize=(16,8))\nplt.plot( train['Count'], label='Train')\nplt.plot(test['Count'], label='Test')\nplt.plot(y_hat_avg['Holt_Winter'], label='Holt_Winter')\nplt.legend(loc='best')\nplt.show()","4251190a":"#order = (p,d,q)\n#p- acf plot lag\n#q- pacf plot lag\n#d- diffencing\ny_hat_avg = test.copy()\nfit1 = sm.tsa.statespace.SARIMAX(train.Count, order=(2,1,4),seasonal_order=(0,1,1,7)).fit()\ny_hat_avg['SARIMA'] = fit1.predict(start=\"2014-04-26\", end=\"2014-09-25\", dynamic=True)\nplt.figure(figsize=(16,8))\nplt.plot( train['Count'], label='Train')\nplt.plot(test['Count'], label='Test')\nplt.plot(y_hat_avg['SARIMA'], label='SARIMA')\nplt.legend(loc='best')\nplt.show()","f3cb1741":"<a id = 'avg'><\/a>\n# Simple Average","00033dfd":"* <B>If you liked the worked, an upvote will be nice!<\/B>\n* <B>Any suggestions and feedback are always welcome<\/B>\n* <B>If you have any doubt, let's dicuss in the comments<\/B>","184478b8":"<a id = 'dataset'><\/a>\n# Importing the Dataset and First look","546b58fd":"<a id = 'expo'><\/a>\n# Simple Exponential smoothing","39ae2845":"<img src = 'https:\/\/cdn.analyticsvidhya.com\/wp-content\/uploads\/2018\/01\/Screen-Shot-2018-01-25-at-7.59.43-PM.png'><\/img>","cc320c80":"<a id = 'preprocess'><\/a>\n# Preprocessing the dataset","6dec9f05":"* As silly as it sounds,in this approach.\n* This simply assumes that the forecast will have the value through the rest of the period. \n* Such forecasting technique which assumes that the next expected point is equal to the last observed point is called Naive Method.\n* Clearly this is not helping, this could be usefull if were are using a stable data ","41a9728c":"<a id = 'visualisation'><\/a>\n# Visualisation","a5df764b":"<img src = 'https:\/\/cdn.analyticsvidhya.com\/wp-content\/uploads\/2018\/01\/Screen-Shot-2018-01-25-at-7.47.33-PM.png'><\/img>","f65b8bb4":"* Forecasting technique which uses window of time period for calculating the average is called Moving Average technique. Calculation of the moving average involves what is sometimes called a \u201csliding window\u201d of size n.","60869249":"<a id = 'station'><\/a>\n# Stationarity and ADFuller Test","0c3460fb":"<a id = 'split'><\/a>\n# Splitting the dataset","d6952637":"* A stationary time series is one whose properties do not depend on the time at which the series is observed. Thus, time series with trends, or with seasonality, are not stationary \u2014 the trend and seasonality will affect the value of the time series at different times.\n* The observations in a stationary time series are not dependent on time.\n* Time series are stationary if they do not have trend or seasonal effects. Summary statistics calculated on the time series are consistent over time, like the mean or the variance of the observations.\n* When a time series is stationary, it can be easier to model. Statistical modeling methods assume or require the time series to be stationary to be effective.\n* In statistics, the Dickey\u2013Fuller test tests the null hypothesis that a unit root is present in an autoregressive model. The alternative hypothesis is different depending on which version of the test is used, but is usually stationarity or trend-stationarity.\n* Statsmodels provide a way to the test.\n* Below is the function for determining the stationarity","3fc48401":"<a id = 'conclude'><\/a>\n# Conclusion","8e4f3a48":"<a id = 'model'><\/a>\n# Models in Time series forecasting","c56bd145":"<a id = 'Time'><\/a>\n# Time Series Analysis","09a9c211":"* For the time series problems there are some preprocessing that is important and needs to be done\n* First is checking whether the datatype of the date column should be in acceptable date format in python, if not so  we can use to pandas to_datetime to convert it into requiered format\n* We convert the index to the date column this helps a bigtime for processing in dataset\n* Now with the time series data we want consistency with the data, it should be either in years,months or day there has to be consistency in the data. To deal with this the resample comes handy and it's like creating the missing dates and filling it with some function here i have taken mean value\n* Finally we dropped the ID column which serve no purpose\n* This all is taken care with the help of function below","28b0813a":"* Use these above mentioned techniques in differenct problem statements and see what works for your problem\n* Remeber one thing each of these models can outperform others on a particular dataset. \n* Therefore it doesn\u2019t mean that one model which performs best on one type of dataset will perform the same for all others too.","06a14c8a":"<a id = 'visualise'><\/a>\n# Decomposition","cf8cfc27":"<a id = 'trend'><\/a>\n# Holt's Linear trend method","2a9d2e20":"# Table of Contents\n\n# Content:\n\n* [Time Series Analysis](#Time)\n* [Dataset](#dataset)\n* [Splitting the dataset into train and test](#split)\n* [Data preprocessing](#preprocess)\n* [Decomposition](#visualise)\n* [Stationarity and ADFuller test](#station)\n* [ACF and PACF plot](#acf)\n* [Visualisation](#visualisation)\n* **[Model in Time series Forecasting](#model)**<br>\n    * [Naive Approach](#naive)\n    * [Simple Average](#avg)\n    * [Moving Average](#ma)\n    * [Simple Exponential smoothing](#expo)\n    * [Holt's Linear trend method](#trend)\n    * [Holt's Winter](#winter)\n    * [ARIMA](#arima)\n* [Conclusion](#conclude)","9c97cf49":"The Holt-Winters seasonal method comprises the forecast equation and three smoothing equations \u2014 one for the level \u2113t, one for trend bt and one for the seasonal component denoted by st, with smoothing parameters \u03b1, \u03b2 and \u03b3.","b843728a":"* Time series analysis comprises methods for analyzing time series data in order to extract meaningful statistics and other characteristics of the data. Time series forecasting is the use of a model to predict future values based on previously observed values.\n\n* Time series forecasting is an important area of machine learning that is often neglected.\n \n* It is important because there are so many prediction problems that involve a time component. \n \n* These problems are neglected because it is this time component that makes time series problems more difficult to handle.","a7f729dc":"* We will now calculate RMSE to check to accuracy of our model on test data set.\n* We can infer from the RMSE value and the graph above, that Naive method isn\u2019t suited for datasets with high variability\n* Root Mean Square Error (RMSE) is a standard way to measure the error of a model in predicting quantitative data","a331f335":"<a id = 'naive'><\/a>\n# Naive Approach","206d2933":"<a id = 'ma'><\/a>\n# Moving Average","1260ba90":"<H2 align = 'center'>Time Series Forecasting<\/H2>\n<img src = 'https:\/\/media-exp1.licdn.com\/dms\/image\/C4E12AQE5yxWYO_UzsQ\/article-cover_image-shrink_720_1280\/0?e=1606953600&v=beta&t=4qGJznnLlbdDqLhzqVhZLw49ESLmjVZlowmLLESNKGU'><\/img>","defe23eb":"* Holt extended simple exponential smoothing to allow forecasting of data with a trend. \n* It is nothing more than exponential smoothing applied to both level(the average value in the series) and trend. \n* To express this in mathematical notation we now need three equations: one for level, one for the trend and one to combine the level and trend to get the expected forecast y\u0302","b8828942":"<img src = 'https:\/\/cdn.analyticsvidhya.com\/wp-content\/uploads\/2018\/01\/eq.png' ><\/img>","9bd91f64":"* Forecasting technique which forecasts the expected value equal to the average of all previously observed points is called Simple Average technique.\n* Calculate the average and take it as the next value. Of course it won\u2019t be it exact, but somewhat close. As a forecasting method, there are actually situations where this technique works the best.","75d54411":"<a id = 'acf'><\/a>\n# ACF and PACF plots","30ecbefa":"Let's start with visualising  the train and test data and observe how it varies with time","62b4fa16":"* Forecasts are calculated using weighted averages where the weights decrease exponentially as observations come from further in the past, the smallest weights are associated with the oldest observations where 0\u2264 \u03b1 \u22641 is the smoothing parameter\n","a17018d6":"<a id = 'winter'><\/a>\n# Holt's Winter ","08cb1f19":"* To make the data stationary, we do the seasonal differencing which is substracting the data next ","9adb13a2":"* Time Series problem involving prediction of number of commuters of JetRail, a new high speed rail service by Unicorn Investors.\n* We are provided with 2 years of data(Aug 2012-Sept 2014) and using this data we have to forecast the number of commuters","2128c3cc":"* Decomposition is primarily used for time series analysis, and as an analysis tool it can be used to inform forecasting models on your problem.\n* It provides a structured way of thinking about a time series forecasting problem, both generally in terms of modeling complexity and specifically in terms of how to best capture each of these components in a given model.\n* The statsmodels library provides an implementation of the naive, or classical, decomposition method in a function called seasonal_decompose(). It requires that you specify whether the model is additive or multiplicative.\n* The seasonal_decompose() function returns a result object. The result object contains arrays to access four pieces of data from the decomposition.\n* Multiplicative decomposition is done by dividing the series by the trend values. Next, seasonal factors are estimated using the de-trended series\n* There is seasonality in the below data, there is also trend\n* The trend is the component of a time series that represents variations of low frequency in a time series, the high and medium frequency fluctuations having been filtered out\n* Seasonality is a characteristic of a time series in which the data experiences regular and predictable changes that recur every calendar year. Any predictable fluctuation or pattern that recurs or repeats over a one-year period is said to be seasonal\n* The \u201cresiduals\u201d in a time series model are what is left over after fitting a model","63064bbf":"* ACF is an (complete) auto-correlation function which gives us values of auto-correlation of any series with its lagged value\n* PACF is a partial auto-correlation function. Basically instead of finding correlations of present with lags like ACF, it finds correlation of the residuals (which remains after removing the effects which are already explained by the earlier lag(s)) with the next lag value hence \u2018partial\u2019 and not \u2018complete\u2019 as we remove already found variations before we find the next correlation\n* Why do we bother about the plots? it will give us the hyperparametes of time series model for prediction\n* You will know what i mean at the end of the notebook","21e2c548":"* Another common Time series model that is very popular among the Data scientists is ARIMA. It stand for Autoregressive Integrated Moving average. While exponential smoothing models were based on a description of trend and seasonality in the data, ARIMA models aim to describe the correlations in the data with each other. An improvement over ARIMA is Seasonal ARIMA. It takes into account the seasonality of dataset just like Holt\u2019 Winter method\n* The hyperparameter in the p,d,q and s are taken arbitarily by checking the lags in the ACF and PACF plot.\n* I highly encourage you to learn as much as you can about the lag from the plots, it is mostly choosen arbitarily by experience\n* Also you should check this <a href='https:\/\/www.statsmodels.org\/dev\/generated\/statsmodels.tsa.statespace.sarimax.SARIMAX.html'>Link<\/a> SARIMAX document","de14035f":"<a id = 'arima'><\/a>\n# ARIMA","ed3dfbc7":"* <B>If you liked the worked, an upvote will be nice!<\/B>\n* <B>Any suggestions and feedback are always welcome<\/B>\n* <B>If you have any doubt, let's dicuss in the comments<\/B>","bdf64a98":"<img src = 'https:\/\/cdn.analyticsvidhya.com\/wp-content\/uploads\/2018\/02\/eq-768x317.png'><\/img>"}}