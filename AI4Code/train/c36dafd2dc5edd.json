{"cell_type":{"1326fed0":"code","f93dc3f5":"code","e852eb09":"code","b8d21373":"code","2389d006":"code","39af7bb5":"code","375431ae":"code","8cdb16fd":"code","cb69815f":"code","83eef178":"code","fa6b91f5":"code","98fd63ba":"code","43d94861":"code","c3db0049":"code","b276c8c1":"code","6afbb882":"code","8610ea96":"code","4e8a1e3c":"code","3674bb1a":"code","4b984be2":"code","f6a39be9":"code","e815fcba":"code","81b0b462":"code","8aadfb0d":"code","dd12f47e":"code","bc9d1aa9":"code","5e8564c1":"markdown","f933c417":"markdown","7cbe6121":"markdown","a9ca074e":"markdown","ad0accd6":"markdown","365d79d7":"markdown","0d3e082b":"markdown","15ae13ec":"markdown","fde9048b":"markdown","b47e8023":"markdown","018d0ceb":"markdown","14134c65":"markdown","10ffd7d2":"markdown","fc64e185":"markdown","e9c7ecc9":"markdown","803c3b38":"markdown","a0e84238":"markdown","34fc29fe":"markdown","78b4d784":"markdown","5599977b":"markdown","9225e4ed":"markdown","61e50ea9":"markdown"},"source":{"1326fed0":"import numpy as np\nimport pandas as pd\n\nfrom sklearn import metrics\nfrom sklearn.model_selection import train_test_split\n\nimport tensorflow as tf\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, Activation\n\nimport warnings\nwarnings.simplefilter('ignore')","f93dc3f5":"data = pd.read_csv(\"..\/input\/gene-expression-cancer-rnaseq\/data.csv\")\nlabels = pd.read_csv(\"..\/input\/gene-expression-cancer-rnaseq\/labels.csv\")","e852eb09":"data","b8d21373":"labels","2389d006":"data = data.drop(columns=['Unnamed: 0'])\nlabels = labels.drop(columns=['Unnamed: 0'])","39af7bb5":"print(data.shape)\nprint(labels.shape)","375431ae":"labels.Class.value_counts()","8cdb16fd":"data.isnull().values.any()","cb69815f":"(data.min() == data.max()).value_counts()","83eef178":"(data.max() == 0.0).value_counts()","fa6b91f5":"data = data.loc[:, (data.max() != data.min())]\ndata.shape","98fd63ba":"df, df_test = train_test_split(data, test_size=0.25, random_state=21)\n\ntest_index = list(df_test.index)\ndf_test = df_test.values","43d94861":"brca_mask = labels['Class'] == 'BRCA'\nkirc_mask = labels['Class'] == 'KIRC'\nluad_mask = labels['Class'] == 'LUAD'\nprad_mask = labels['Class'] == 'PRAD'\ncoad_mask = labels['Class'] == 'COAD'\n\ndf_brca = df[brca_mask]\ndf_kirc = df[kirc_mask]\ndf_luad = df[luad_mask]\ndf_prad = df[prad_mask]\ndf_coad = df[coad_mask]\n\nx_brca = df_brca.values\nx_kirc = df_kirc.values\nx_luad = df_luad.values\nx_prad = df_prad.values\nx_coad = df_coad.values","c3db0049":"def Model():\n    model = Sequential()\n    model.add(Dense(250, input_dim=x_brca.shape[1], activation='relu'))\n    model.add(Dense(21, activation='relu'))\n    model.add(Dense(250, activation='relu'))\n    model.add(Dense(x_brca.shape[1]))\n    model.compile(loss='mean_squared_error', optimizer='adam')\n    return model","b276c8c1":"from keras.callbacks import EarlyStopping\nearly_stopping = EarlyStopping(monitor = 'val_loss',\n                               min_delta = 0,\n                               patience = 5,\n                               verbose = 1,\n                               restore_best_weights = True)","6afbb882":"def evaluate(model):\n    return (np.sqrt(metrics.mean_squared_error(model.predict(x_brca), x_brca)),\n            np.sqrt(metrics.mean_squared_error(model.predict(x_kirc), x_kirc)),\n            np.sqrt(metrics.mean_squared_error(model.predict(x_luad), x_luad)),\n            np.sqrt(metrics.mean_squared_error(model.predict(x_prad), x_prad)),\n            np.sqrt(metrics.mean_squared_error(model.predict(x_coad), x_coad)))","8610ea96":"x_brca_train, x_brca_test = train_test_split(x_brca, test_size=0.25, random_state=21)\n\nbrca_model = Model()\nbrca_model.fit(x_brca_train, x_brca_train, validation_data=(x_brca_test, x_brca_test), verbose=1, epochs=100, callbacks=[early_stopping])\nevaluate(brca_model)","4e8a1e3c":"x_kirc_train, x_kirc_test = train_test_split(x_kirc, test_size=0.25, random_state=21)\n\nkirc_model = Model()\nkirc_model.fit(x_kirc_train, x_kirc_train, validation_data=(x_kirc_test, x_kirc_test), verbose=1, epochs=100, callbacks=[early_stopping])\nevaluate(kirc_model)","3674bb1a":"x_luad_train, x_luad_test = train_test_split(x_luad, test_size=0.25, random_state=21)\n\nluad_model = Model()\nluad_model.fit(x_luad_train, x_luad_train, validation_data=(x_luad_test, x_luad_test), verbose=1, epochs=100, callbacks=[early_stopping])\nevaluate(luad_model)","4b984be2":"x_prad_train, x_prad_test = train_test_split(x_prad, test_size=0.25, random_state=21)\n\nprad_model = Model()\nprad_model.fit(x_prad_train, x_prad_train, validation_data=(x_prad_test, x_prad_test), verbose=1, epochs=100, callbacks=[early_stopping])\nevaluate(prad_model)","f6a39be9":"x_coad_train, x_coad_test = train_test_split(x_coad, test_size=0.25, random_state=21)\n\ncoad_model = Model()\ncoad_model.fit(x_coad_train, x_coad_train, validation_data=(x_coad_test, x_coad_test), verbose=1, epochs=100, callbacks=[early_stopping])\nevaluate(coad_model)","e815fcba":"models = [brca_model, kirc_model, luad_model, prad_model, coad_model]","81b0b462":"def get_pred(df_test, models):\n    pred_class = []\n    for i in range(len(df_test)):\n        loss = []\n        x = df_test[i].reshape(1, 20264)\n        for model in models:\n            loss.append(np.sqrt(metrics.mean_squared_error(model.predict(x), x)))\n        pred_class.append(loss.index(min(loss)))\n    return pred_class","8aadfb0d":"def get_label(test_index):\n    num_label = []\n    for l in range(len(test_index)):\n        _ = labels.values[test_index[l]][0][0]\n        if _ == 'B': num_label.append(0)\n        elif _ == 'K': num_label.append(1)\n        elif _ == 'L': num_label.append(2)\n        elif _ == 'P': num_label.append(3)\n        elif _ == 'C': num_label.append(4)\n    return num_label","dd12f47e":"pred_correct = 0\nnum_label = get_label(test_index)\npred_class = get_pred(df_test, models)\n\nfor i in range(len(num_label)):\n    if num_label[i] == pred_class[i]: pred_correct += 1","bc9d1aa9":"print('Accuracy:', pred_correct\/len(num_label) * 100)","5e8564c1":"### Acquaintance with data","f933c417":"That means, there are 267 features that has the same minimum and maximum. General intuition is all of the values in these features are zeros. Let's check.","7cbe6121":"Our proposition is Correct! Getting rid of those features.","a9ca074e":"Evaluation function for all models using mean squared error.","ad0accd6":"### Creating the Model\n\nNow we create our Fully-connected Deep AutoEncoder (FcDAE). In short-hand notation, the model looks like this: \n\n**20264-250-21-250-20264**.","365d79d7":"List of FcDAE models:","0d3e082b":"There are 5 classes. Though it's not a balanced dataset, we don't care much as we are to classify these building AutoEncoders.","15ae13ec":"Now we create five FcDAE, one for each class and train them. Notice that, for every FcDAE, the corresponding class gets the lowest loss.","fde9048b":"We have a total of 20531 gene expression as feature to classify a total of 5 types of cancer e.g. BRCA, KIRC, LUAD, PRAD & COAD. The classes are denoted in **labels**.","b47e8023":"Early stopping callback.","018d0ceb":"Getting the prediction score, Accuracy:","14134c65":"**Reference:**\n\n[1] P. Garc\u00eda-D\u00edaz, I. S\u00e1nchez-Berriel, J.A. Mart\u00ednez-Rojas, et al., Unsupervised feature selection algorithm for multiclass cancer classification of gene expression RNA-Seq data, Genomics (2018), https:\/\/doi.org\/10.1016\/j.ygeno.2019.11.004\n\n[2] https:\/\/www.kaggle.com\/c\/applications-of-deep-learningwustl-spring-2020","10ffd7d2":"So, there is no null value. \n\nNow, check if there is any value that is equal or zero to all, which means that feature will not have any influence to any of the classes.","fc64e185":"Our aim is to build classification technique using **AutoEncoder**. Though it is not great for lots of classes, but for this problem it brings the **State of the art** accuracy. Previously studied best model [1] classified cancer with 98.81% accuracy where this AutoEncoder based technique gives 99.50% accuracy i.e. 1 error in 201 test data; rarely it drops to 99.00% i.e. 2 errors.\n\n**Algorithm:** The algorithms is preety simple.\n* We create an AutoEncoder for each class.\n* Measure the loss for test data for each model.\n* Model with the lowest loss provides the class.\n\nThe similar technique was used in this [2] kaggle in-class competition (binary classification) getting a perfect score of **log loss 0.0000**.","e9c7ecc9":"Lets examine the **data** if there is any null or some sort of value that is not so helpful.","803c3b38":"### Reading Dataset","a0e84238":"Performence of the models:\n\n    loss for\n        brca_model: 1.120480030764103\n        kirc_model: 0.9781646085729507\n        luad_model: 1.137179904715911\n        prad_model: 0.8989728895676945\n        coad_model: 1.0059484052906127\n\nAll models working quiet well and their lowest loss corresponds to the respective classes.","34fc29fe":"### Data Cleaning\n\nBefore diving into AutoEncoders, we perform some data cleaning.\n\n**Unnamed: 0** column is not a feature or any sort of data. So, dropping it.","78b4d784":"### Spliting the Dataset","5599977b":"Function that provides the prediction for the test data. This may take some time as it examins the loss for all classes and predicts the class with respect to lowest loss.","9225e4ed":"### Importing Libraries","61e50ea9":"Prepating the data for our model."}}