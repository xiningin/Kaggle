{"cell_type":{"de0ade8e":"code","253b153e":"code","9d62ff55":"code","224eae63":"code","f7595619":"code","7790e8c1":"code","1a63c025":"code","b269f1b4":"code","98ef87e4":"code","cfd24576":"code","bc264a6f":"code","7bfcc4b0":"code","c91396f5":"code","692881ca":"code","7d312987":"code","f7dacb61":"code","bb1f7f1c":"code","0c23b464":"code","4ed66976":"code","867f93ed":"code","9b8c1339":"markdown","30f35675":"markdown","d76c974e":"markdown","8164e0cd":"markdown","1aee3c88":"markdown","31f1b7bb":"markdown","1448716e":"markdown","9f930f96":"markdown","33cd70da":"markdown","e2cfe5d2":"markdown","650d51dc":"markdown","71c9e67d":"markdown","67c92406":"markdown","723cd965":"markdown","620554a5":"markdown","831eb067":"markdown","da81e914":"markdown","e14b170a":"markdown","110e097f":"markdown","f92363cc":"markdown","cdfcdc42":"markdown","1916de15":"markdown","8a5983a8":"markdown"},"source":{"de0ade8e":"# tests help notebooks stay managable\nimport doctest\nimport copy\nimport functools\n\ndef autotest(func):\n    globs = copy.copy(globals())\n    globs.update({func.__name__: func})\n    doctest.run_docstring_examples(\n        func, globs, verbose=True, name=func.__name__)\n    return func","253b153e":"@autotest\ndef add_1(x):\n  \"\"\"Add 1 to a value\"\"\"\n  return x+1","9d62ff55":"@autotest\ndef add_1(x):\n  \"\"\"Add 1 to a value\n  >>> add_1(4)\n  \"\"\"\n  return x+1","224eae63":"@autotest\ndef add_1(x):\n  \"\"\"Add 1 to a value\n  >>> add_1(4)\n  5\n  \"\"\"\n  return x+1","f7595619":"@autotest\ndef add_1(x):\n  \"\"\"Add 1 to a value.\n  :param x: a number to add 1 to\n  :return: the number plus 1\n  \n  >>> add_1(4)\n  5\n  >>> add_1(\"bob\")\n  Traceback (most recent call last):\n       ...\n  TypeError: must be str, not int\n  \"\"\"\n  return x+1","7790e8c1":"help(add_1)","1a63c025":"!pip install -q webcolors\nimport webcolors\n@autotest\ndef closest_color(requested_color):\n    # type: (Tuple[float, float, float]) -> str\n    \"\"\"\n    Finds the closest color to a given item\n    :param requested_color: r,g,b values for a color\n    :return: name of the closest color\n    \n    Examples:\n    >>> closest_color((128, 0, 255))\n    'darkviolet'\n    >>> closest_color((255, 128, 128))\n    'salmon'\n    >>> closest_color([0])\n    Traceback (most recent call last):\n        ...\n    ValueError: Invalid size\n    \"\"\"\n    if len(requested_color)!=3:\n        raise ValueError('Invalid size')\n    min_colors = {}\n    for key, name in webcolors.css3_hex_to_names.items():\n        r_c, g_c, b_c = webcolors.hex_to_rgb(key)\n        rd = (r_c - requested_color[0]) ** 2\n        gd = (g_c - requested_color[1]) ** 2\n        bd = (b_c - requested_color[2]) ** 2\n        min_colors[(rd + gd + bd)] = name\n    return min_colors[min(min_colors.keys())]","b269f1b4":"from warnings import warn\n@autotest\ndef try_default(errors=(Exception,), default_value='', verbose=False,\n                warning=False):\n    \"\"\"\n    A decorator for making failing functions easier to use\n    :param errors:\n    :param default_value:\n    :param verbose:\n    :return:\n    >>> int('a')\n    Traceback (most recent call last):\n         ...\n    ValueError: invalid literal for int() with base 10: 'a'\n    >>> safe_int = try_default(default_value=-1, verbose=True, warning=False)(int)\n    >>> safe_int('a')\n    Failed calling int with :('a',),{}, because of ValueError(\"invalid literal for int() with base 10: 'a'\",)\n    -1\n    >>> quiet_int = try_default(default_value=0, verbose=False, warning=False)(int)\n    >>> quiet_int('a')\n    0\n    \"\"\"\n\n    def decorator(func):\n        def new_func(*args, **kwargs):\n            try:\n                return func(*args, **kwargs)\n            except errors as e:\n                if verbose:\n                    out_msg = \"Failed calling {} with :{},{}, because of {}\".format(\n                        func.__name__,\n                        args, kwargs, repr(e))\n                    if warning:\n                        warn(out_msg, RuntimeWarning)\n                    else:\n                        print(out_msg)\n                return default_value\n\n        return new_func\n\n    return decorator","98ef87e4":"import numpy as np\n# make outputs look nicer\npprint = lambda x, p=2: print(np.array_str(x, max_line_width=80, precision=\np))","cfd24576":"@autotest\ndef in_nd(x, y):\n    # type: (np.ndarray, np.ndarray) -> np.ndarray\n    \"\"\"\n    A simple wrapper for the in1d function to work on ND data\n    :param x:\n    :param y:\n    :return:\n    >>> t_img = np.arange(6).reshape((2,3))\n    >>> pprint(t_img)\n    [[0 1 2]\n     [3 4 5]]\n    >>> pprint(in_nd(t_img, [4,5]))\n    [[False False False]\n     [False  True  True]]\n    \"\"\"\n    return np.in1d(x.ravel(), y).reshape(x.shape)","bc264a6f":"@autotest\ndef meshgridnd_like(in_img,\n                    rng_func=range):\n    \"\"\"\n    Makes a n-d meshgrid in the shape of the input image.\n    \n    >>> import numpy as np\n    >>> xx, yy = meshgridnd_like(np.ones((3,2)))\n    >>> xx.shape\n    (3, 2)\n    >>> xx\n    array([[0, 0],\n           [1, 1],\n           [2, 2]])\n    >>> xx[:,0]\n    array([0, 1, 2])\n    >>> yy\n    array([[0, 1],\n           [0, 1],\n           [0, 1]])\n    >>> yy[0,:]\n    array([0, 1])\n    >>> xx, yy, zz = meshgridnd_like(np.ones((2,3,4)))\n    >>> xx.shape\n    (2, 3, 4)\n    >>> xx[:,0,0]\n    array([0, 1])\n    >>> yy[0,:,0]\n    array([0, 1, 2])\n    >>> zz[0,0,:]\n    array([0, 1, 2, 3])\n    >>> zz.astype(int)\n    array([[[0, 1, 2, 3],\n            [0, 1, 2, 3],\n            [0, 1, 2, 3]],\n    <BLANKLINE>\n           [[0, 1, 2, 3],\n            [0, 1, 2, 3],\n            [0, 1, 2, 3]]])\n    \"\"\"\n    new_shape = list(in_img.shape)\n    all_range = [rng_func(i_len) for i_len in new_shape]\n    return tuple([x_arr.swapaxes(0, 1) for x_arr in np.meshgrid(*all_range)])","7bfcc4b0":"@autotest\ndef diffpad(in_x, \n            n=1,\n            axis=0,\n            starting_value=0):\n  \"\"\"\n  Run diff and pad the results to keep the same \n  If the starting_value is the same then np.cumsum should exactly undo diffpad\n  >>> diffpad([1, 2, 3], axis=0)\n  array([0, 1, 1])\n  >>> np.cumsum(diffpad([1, 2, 3], axis=0, starting_value=1))\n  array([1, 2, 3])\n  >>> diffpad(np.cumsum([0, 1, 2]), axis=0)\n  array([0, 1, 2])\n  >>> diffpad(np.eye(3), axis=0)\n  array([[ 0.,  0.,  0.],\n         [-1.,  1.,  0.],\n         [ 0., -1.,  1.]])\n  >>> diffpad(np.eye(3), axis=1)\n  array([[ 0., -1.,  0.],\n         [ 0.,  1., -1.],\n         [ 0.,  0.,  1.]])\n  \"\"\"\n  if axis<0: \n    raise ValueError(\"Axis must be nonneggative\")\n  d_x = np.diff(in_x, n=n, axis=axis)\n  return np.pad(d_x, [(n, 0) if i==axis else (0,0) \n                      for i, _ in enumerate(np.shape(in_x))], \n                mode='constant', constant_values=starting_value)","c91396f5":"import json\n@autotest\nclass NumpyAwareJSONEncoder(json.JSONEncoder):\n    \"\"\"\n    A JSON plugin that allows numpy data to be serialized \n    correctly (if inefficiently)\n    >>> json.dumps(np.eye(3))\n    Traceback (most recent call last):\n        ...\n    TypeError: Object of type 'ndarray' is not JSON serializable\n    >>> json.dumps(np.eye(3).astype(int), cls=NumpyAwareJSONEncoder)\n    '[[1, 0, 0], [0, 1, 0], [0, 0, 1]]'\n    \"\"\"\n\n    def default(self, obj):\n        if isinstance(obj, np.ndarray):  # and obj.ndim == 1:\n            return obj.tolist()\n        if isinstance(obj, np.number):\n            return obj.tolist()\n        return json.JSONEncoder.default(self, obj)\n  ","692881ca":"from keras import models, layers\nimport tensorflow as tf\ndef _setup_and_test(in_func, *in_arrs, is_list=False, round=False):\n    \"\"\"\n    For setting up a simple graph and testing it\n    :param in_func:\n    :param in_arrs:\n    :param is_list:\n    :return:\n    \"\"\"\n    with tf.Graph().as_default() as g:\n        in_vals = [tf.placeholder(dtype=tf.float32, shape=in_arr.shape) for\n                   in_arr in in_arrs]\n        out_val = in_func(*in_vals)\n        if not is_list:\n            print('setup_net', [in_arr.shape for in_arr in in_arrs],\n                  out_val.shape)\n            out_list = [out_val]\n        else:\n            out_list = list(out_val)\n    with tf.Session(graph=g) as c_sess:\n        sess_out = c_sess.run(fetches=out_list,\n                              feed_dict={in_val: in_arr\n                                         for in_val, in_arr in\n                                         zip(in_vals, in_arrs)})\n        if is_list:\n            o_val = sess_out\n        else:\n            o_val = sess_out[0]\n        if round:\n            return (np.array(o_val) * 100).astype(int) \/ 100\n        else:\n            return o_val","7d312987":"@autotest\ndef spatial_gradient_2d_tf(in_img):\n    # type: (tf.Tensor) -> Tuple[tf.Tensor, tf.Tensor, tf.Tensor]\n    \"\"\"\n    Calculate the 2d spatial gradient in x,y using tensorflow\n    The channel dimension is completely ignored and the batches are kept\n    consistent.\n    :param in_img: a 4d tensor sized as batch, x, y, channel\n    :return:\n    NOTE:: the doctests are written to only test the main region, due to\n    boundary issues the edges are different (not massively) between\n    np.gradient and this function, eventually a better edge scaling should\n    be implemented\n    >>> _testimg = np.ones((4, 4))\n    >>> _testimg = np.sum(np.power(np.stack(meshgridnd_like(_testimg),-1),2),-1)\n    >>> _testimg = np.expand_dims(np.expand_dims(_testimg,0),-1)\n    >>> dx, dy = _setup_and_test(spatial_gradient_2d_tf, _testimg, is_list=True)\n    >>> dx.shape, dy.shape\n    ((1, 4, 4, 1), (1, 4, 4, 1))\n    >>> ndx, ndy = np.gradient(_testimg[0,:,:,0])\n    >>> (ndx.shape, ndy.shape)\n    ((4, 4), (4, 4))\n    >>> [(a,b) for a,b in zip(ndx[:,0],dx[0,1:-1,0,0])]\n    [(1.0, 2.0), (2.0, 4.0)]\n    >>> [(a,b) for a,b in zip(ndy[0,:],dy[0,0,1:-1,0])]\n    [(1.0, 2.0), (2.0, 4.0)]\n    >>> np.sum(ndx-dx[0,:,:,0],(1))\n    array([ 2.,  0.,  0., 10.])\n    >>> np.sum(ndy-dy[0,:,:,0], (0))\n    array([ 2.,  0.,  0., 10.])\n    \"\"\"\n    with tf.variable_scope('spatial_gradient_2d'):\n        pad_r = tf.pad(in_img, [[0, 0], [1, 1], [1, 1], [0, 0]],\n                       \"SYMMETRIC\")\n        dx_img = pad_r[:, 2:, 1:-1, :] - pad_r[:, 0:-2, 1:-1, :]\n        dy_img = pad_r[:, 1:-1, 2:, :] - pad_r[:, 1:-1, 0:-2, :]\n        return (0.5 * dx_img, 0.5 * dy_img)\n","f7dacb61":"@autotest\ndef diff_filter_layer(length=2, depth=1):\n  \"\"\"Calculates difference of input in 1D.\n  \n  >>> c_layer = diff_filter_layer()\n  >>> t_in = layers.Input((5, 1))\n  >>> t_out = c_layer(t_in)\n  >>> t_model = models.Model(inputs=[t_in], outputs=[t_out])\n  >>> t_model.predict(np.ones((1, 5, 1)))[0, :, 0]\n  array([0., 0., 0., 0., 0.], dtype=float32)\n  >>> t_model.predict(np.arange(5).reshape((1, 5, 1)))[0, :, 0]\n  array([0., 1., 1., 1., 1.], dtype=float32)\n  >>> c_layer_3d = diff_filter_layer(depth=3)\n  >>> t_in_3d = layers.Input((4, 3))\n  >>> t_out_3d = c_layer_3d(t_in_3d)\n  >>> t_model_3d = models.Model(inputs=[t_in_3d], outputs=[t_out_3d])\n  >>> t_model_3d.predict(np.ones((1, 4, 3)))[0, :, 0]\n  array([0., 0., 0., 0.], dtype=float32)\n  >>> fake_in = np.arange(12).reshape((1, 4, 3))\n  >>> fake_in[0]\n  array([[ 0,  1,  2],\n         [ 3,  4,  5],\n         [ 6,  7,  8],\n         [ 9, 10, 11]])\n  >>> t_model_3d.predict(fake_in)[0]\n  array([[0., 0., 0.],\n         [3., 3., 3.],\n         [3., 3., 3.],\n         [3., 3., 3.]], dtype=float32)\n  \"\"\"\n  coef = np.zeros((length, depth, depth))\n  i=length\/\/2-1 # offset in middle\n  for j in range(depth):\n    coef[i,j,j] = -1\n    coef[i+1,j,j] = 1\n  c_layer = layers.Conv1D(depth, \n                          (coef.shape[0],), \n                          weights=[coef],\n                          use_bias=False,\n                          activation='linear',\n                          padding='valid',\n                          name='diff'\n               )\n  c_layer.trainable = False\n  def _diff_module(x):\n    diff_x = c_layer(x)\n    needed_padding = length-1\n    right_pad = np.clip(needed_padding\/\/2-1, 0, 1e99).astype(int)\n    left_pad = needed_padding-right_pad\n    return layers.ZeroPadding1D((left_pad, right_pad), name='PaddingDiffEdges')(diff_x)\n  return _diff_module","bb1f7f1c":"from keras import models, layers\n@autotest\ndef build_counting_model(\n    input_shape, \n    layer_count=2,\n    depth=3,\n    max_output=5\n    ):\n  \"\"\"Builds a regression model that counts things.\n  >>> basic_shape = (9, 9, 1)\n  >>> tf.random.set_random_seed(0)\n  >>> np.random.seed(0)\n  >>> my_model = build_counting_model(basic_shape)\n  >>> len(my_model.layers)\n  6\n  >>> my_model.compile(optimizer='adam', loss='mse')\n  >>> x_data = np.random.uniform(size=(5,)+basic_shape)\n  >>> y_data = np.linspace(-4, 4, 5)\n  >>> _ = my_model.fit(x_data, y_data, epochs=100, verbose=False)\n  >>> my_model.evaluate(x_data, y_data, verbose=False)<0.1\n  True\n  \"\"\"\n  cnt_model = models.Sequential()\n  cnt_model.add(layers.BatchNormalization(input_shape=input_shape))\n  for i in range(layer_count):\n    cnt_model.add(layers.Conv2D(depth*2**i, kernel_size=(3, 3), padding='same'))\n  cnt_model.add(layers.Flatten())\n  cnt_model.add(layers.Dense(2*max_output, activation='tanh'))\n  cnt_model.add(layers.Dense(1, activation='linear', use_bias=False))\n  return cnt_model\n  ","0c23b464":"from scipy.signal import argrelextrema\n@autotest\ndef get_local_maxi(s_vec, \n                   jitter_amount=1e-5, \n                   min_width=5, \n                   cutoff=None # type: Optional[float]\n                  ): \n  # type: (...) -> List\n  \"\"\"Get the local maximums.\n  \n  The standard functions struggle with flat peaks\n  \n  >>> np.random.seed(2019)\n  >>> get_local_maxi([0, 1, 1, 0])\n  array([2])\n  >>> get_local_maxi([1, 1, 1, 0])\n  array([0])\n  >>> get_local_maxi([1, 1, 1, 0, 1])\n  array([0])\n  >>> get_local_maxi([1, 0, 0, 0, 0, 1], min_width=1)\n  array([0, 2, 5])\n  >>> get_local_maxi([1, 0, 0, 0, 0, 1], min_width=1, cutoff=0.5)\n  array([0, 5])\n  \"\"\"\n  s_range = np.max(s_vec)-np.min(s_vec)\n  if s_range==0:\n    return []\n  \n  j_vec = np.array(s_vec)+\\\n    jitter_amount*s_range*np.random.uniform(-1, 1, size=np.shape(s_vec))\n  max_idx = argrelextrema(j_vec, np.greater_equal, order=min_width)[0]\n  if cutoff is not None:\n    max_idx = np.array([k for k in max_idx if s_vec[k]>cutoff])\n  return max_idx","4ed66976":"from scipy.signal import detrend\n@autotest\ndef scale_wobble(\n    in_vec, # type: np.ndarray\n    wobble_scale, # type: float\n    axis=0\n):\n  # type: (...) -> np.ndarray\n  \"\"\"Scale the amount of bobble in headpose.\n  \n  :param wobble_scale: scalar for how much to multiply the amplitude\n  >>> scale_wobble([0, 1, 0, 1, 0], 2)\n  array([-0.4,  1.6, -0.4,  1.6, -0.4])\n  \"\"\"\n  amp_vec = detrend(in_vec, axis=axis)\n  trend_vec = in_vec-amp_vec\n  return amp_vec*wobble_scale+trend_vec","867f93ed":"from scipy.ndimage import grey_dilation\n@autotest\ndef idx_dilation(\n    in_img,\n    in_mask=None,\n    step_size=(3, 3),\n    max_iter=100,\n    pre_erode=None,  # type: Optional[Tuple[int, int]]\n):\n    \"\"\"Index based dilation\n    >>> idx_dilation(np.diag(range(3)))\n    array([[1, 1, 1],\n           [1, 1, 2],\n           [1, 2, 2]])\n    >>> idx_dilation(np.diag(range(5)))\n    array([[1, 1, 1, 2, 2],\n           [1, 1, 2, 2, 3],\n           [1, 2, 2, 3, 3],\n           [2, 2, 3, 3, 4],\n           [2, 3, 3, 4, 4]])\n    \"\"\"\n    if in_mask is None:\n        in_mask = np.ones_like(in_img)\n    cur_img = in_img.astype(int)\n\n    if pre_erode is not None:\n        cur_img = grey_erosion(cur_img, pre_erode)\n\n    out_img = cur_img.copy()\n    matches = False\n    iter_cnt = 0\n    while not matches:\n        out_img = grey_dilation(cur_img, step_size) * in_mask\n        out_img[cur_img > 0] = cur_img[\n            cur_img > 0\n        ]  # preserve the last labels where necessary\n        matches = np.sum(np.abs(out_img - cur_img)) < 1\n        cur_img = out_img\n        iter_cnt += 1\n        if iter_cnt > max_iter:\n            print(\"Did not converge\")\n            print(cur_img)\n            print(out_img)\n            matches = True\n    return out_img","9b8c1339":"#### Getting Started\n\nInteractive `doctest` snippet. The snippet below lets us use a simple annotation (`@autotest`) above any function to run the doctest immediately. In real projects you can run doctest seperately using `doctest` or as part of `py.test` using the correct `setup.cfg` file or command line arguments. You will thus not need the annotation here. For notebooks the annotation makes it much easier to build tests on the fly","30f35675":"# Deep Learning Examples\nDeep learning can be a bit messier and require graphs, setups, CPU\/GPU configs and so is often less well suited to doctest-style testing. With a few simple helper functions it can be made easier and so we show a few examples below","d76c974e":"## Real Sample\n\nHere is a slightly more complicated real sample (requires webcolors library) which finds the closest color to a given entry","8164e0cd":"## Overfit Model on One Dataset\nOne of the points Andre Karpathy makes in his [recipe blog post](http:\/\/karpathy.github.io\/2019\/04\/25\/recipe\/) is your model should be able to overfit on a single test batch. This can also be incorporated as a unit test for the model which runs to make sure the model, loss, and training are properly implemented. We show here the model overfitting on a random batch over a -5 and 5 range.","1aee3c88":"# Scientific Computing Examples\nHere are a few general scientific computing examples","31f1b7bb":"## Difference Padding\nHere we want to take the difference along a dimension and then pad it so that it retains the same dimensions","1448716e":"## Numpy Aware JSON Encoder\nJSON typically chokes on numpy arrays since it does not have a default encoder for them (and many arrays would be massive if just naively converted to JSON). Here we implement the naive conversion for exporting small arrays. We see here that it also works on classes","9f930f96":"now everything works","33cd70da":"# Simple Examples\nJust to get started with doctest and how it generally works","e2cfe5d2":"## Create a layer which takes the difference of the input\nWe use a hard-coded convolution weight and disable `.trainable`","650d51dc":"## In-ND\nThe `np.in1d` function in numpy works quite well to see which elements in a 1D array ($x$) are inside a list ($y$). The result is a boolean array the same size as the input where each element is if the item at that index in $x$ was in $y$. The function below works on ND arrays.","71c9e67d":"we can add one test by adding the `>>> add_1(4)` line, but we see that it fails because we don't specify any output","67c92406":"You can even check exceptions by using traceback, elipses and then the error type","723cd965":"## More complicated exampe - Try\nHere is a more complicated example where we make and test another decorator which trys a function and returns a default value if it fails. This is particularly useful for parsing messy data where some columns that should contain numbers contain bizzare strings like (empty or just \"\")","620554a5":"## Spatial Gradient 2D\nThe function serves to add the equivalent of the `numpy.gradient` command to tensorflow. The edges are different but the results are largely the same.","831eb067":"## Meshgrid-like\nHere is a simple function which makes a meshgrid with the same dimensions as the input","da81e914":"# Motivation\nThe motivation for putting this together was [@JoelGru's](https:\/\/twitter.com\/joelgrus) presentation called [\"I don't like notebooks\"](https:\/\/conferences.oreilly.com\/jupyter\/jup-ny\/public\/schedule\/detail\/68282) at JupyterCon. He made a number of very good points and one of the most critical ones was about bad habits and lack of testing. I attempt to address this in this work\n\n## Overview\nThe idea for this document is to provide a practical introduction to using doctests (having tests inside the docstring). It is not meant to replace the [main documentation](https:\/\/docs.python.org\/3\/library\/doctest.html) but rather to show data-scientists and deep-learning people how to use it effectively. If you have any changes or suggestions please make comments to the document or reach out on Kaggle\n\n","e14b170a":"## Trivial Sample \/ Intro\nno >>> code means nothing actually happens","110e097f":"## Get Local Maxima\nHere we have a function to get the local maxima and ignore multiple values above a threshold","f92363cc":"# Matrix Examples\nSince matrices, arrays, tensors, ... come up quite frequently in real-world python use-cases, we show some examples of testing matrix code even though the output is often ugly","cdfcdc42":"## Add a wobble to a signal\nHere we add a wobble to a signal","1916de15":"The largest benefit of doctests is when using the `help` or `?` tools inside IPython\/Jupyter you can see all of the tests, results and along with the general function info. This makes it substantially easier to figure out how to use a function since ready-to-go examples have already been prepared.","8a5983a8":"## Perform a dilation on an labelmap\nTaken a labeled image (the value of a pixel is a label) and expand each region until the entire image is filled"}}