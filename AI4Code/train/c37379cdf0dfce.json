{"cell_type":{"4afeae9f":"code","aa3c8afd":"code","7112f229":"code","7cf02f10":"code","28e4a954":"code","402c9d56":"code","314312df":"code","71cb8ce3":"code","6f94eac7":"code","69c82ae9":"code","32c29c2d":"code","80f3dbb1":"code","271c9566":"code","b3603f77":"code","a52b63f5":"code","4a5960c0":"code","befced0a":"code","a1701c13":"code","7b8a7c2e":"code","2f5f2740":"code","7cf8a385":"code","cfbeba07":"code","2fec8ad5":"code","7cf79630":"code","4fe803a5":"code","f70bdff7":"code","3e0d8670":"code","c50f8812":"code","b43085ef":"code","32229451":"code","5f2e7f38":"code","73420933":"code","5c248481":"code","f479c5ab":"code","e47c438e":"code","4e021eda":"code","dd10e896":"code","9b043457":"code","a04cec30":"code","072b7b89":"code","66c7b0a5":"code","1c74b0b4":"markdown","5d814135":"markdown","36bcbf0b":"markdown","fd7f17a8":"markdown","15695565":"markdown","1beccf30":"markdown","df9459ed":"markdown","f5ee4a86":"markdown","8b0aa921":"markdown","019f52e7":"markdown"},"source":{"4afeae9f":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","aa3c8afd":"from string import punctuation\n\nimport re\n\ndef remove_emojis(data):\n    emoj = re.compile(\"[\"\n        u\"\\U00002700-\\U000027BF\"  # Dingbats\n        u\"\\U0001F600-\\U0001F64F\"  # Emoticons\n        u\"\\U00002600-\\U000026FF\"  # Miscellaneous Symbols\n        u\"\\U0001F300-\\U0001F5FF\"  # Miscellaneous Symbols And Pictographs\n        u\"\\U0001F900-\\U0001F9FF\"  # Supplemental Symbols and Pictographs\n        u\"\\U0001FA70-\\U0001FAFF\"  # Symbols and Pictographs Extended-A\n        u\"\\U0001F680-\\U0001F6FF\"  # Transport and Map Symbols\n                      \"]+\", re.UNICODE)\n    return re.sub(emoj, '', data)\n\n\ndef remove_punctuation_emoji(value):\n    value = remove_emojis(value)\n    return ''.join([c for c in value if (c not in punctuation)])","7112f229":"data = {\n    \"train\":{\n        \"<id>\":{\n            \"emotion\":\"\",\n            \"emotion_class\":1,\n            \"text\":\"\",\n            \"text_calss\":[],\n        }  \n    },\n    \"test\":{\n        \"<id>\":{\n            \"text\":\"\",\n            \"text_calss\":[],\n        },\n    },\n    \"emotion_map\":{},\n    \"tweet_map\":{},\n    \"max_voc\":1,\n    \"max_l\":1,\n}","7cf02f10":"import json\nwith open(\"..\/input\/dm2021-lab2-hw2\/tweets_DM.json\",\"r\") as file:\n    tweets_json_list = [eval(f) for f in file.readlines()]","28e4a954":"tweets_user = {u['_source'][\"tweet\"][\"tweet_id\"]:remove_punctuation_emoji(u['_source'][\"tweet\"][\"text\"]) for u in tweets_json_list}","402c9d56":"import csv\n\nemotion = None\nwith open(\"..\/input\/dm2021-lab2-hw2\/emotion.csv\",newline='') as csvfile:\n    reader = csv.reader(csvfile, delimiter=',')\n    _ = next(reader)\n    emotion = {rows[0]:rows[1] for rows in reader} #row[0]:key row[1]:data, dict as a key value","314312df":"data_identification = None\n\nwith open(\"..\/input\/dm2021-lab2-hw2\/data_identification.csv\",newline='') as csvfile:\n    reader = csv.reader(csvfile, delimiter=',')\n    _ = next(reader) # iterate once to skip the first line\n    data_identification = {rows[0]: rows[1] for rows in reader} ","71cb8ce3":"for k in data_identification.keys():\n    if data_identification[k] == \"train\":\n        data[\"train\"][k] = {}\n    elif data_identification[k] == \"test\":\n        data[\"test\"][k] = {}","6f94eac7":"key_to_remove = []\nfor k in data[\"train\"].keys():\n    if k in tweets_user:\n        data[\"train\"][k][\"text\"] = tweets_user[k]\n    else:\n        key_to_remove.append(k)\n\nfor k in key_to_remove:\n    data[\"train\"].pop(k)","69c82ae9":"key_to_remove = []\nfor k in data[\"test\"].keys():\n    if k in tweets_user:\n        data[\"test\"][k][\"text\"] = tweets_user[k]\n    else:\n        key_to_remove.append(k)\n\nfor k in key_to_remove:\n    data[\"test\"].pop(k)","32c29c2d":"key_to_remove = []\nfor k in data[\"train\"].keys():\n    if k in emotion:\n        data[\"train\"][k][\"emotion\"] = emotion[k]\n    else:\n        key_to_remove.append(k)\n    \nfor k in key_to_remove:\n    data[\"train\"].pop(k)","80f3dbb1":"class_emotion = []\n\nfor m in emotion.keys():\n    if emotion[m] not in class_emotion:\n        class_emotion.append(emotion[m])","271c9566":"emotion_map = {v:i for i,v in enumerate(class_emotion)}","b3603f77":"for k in data[\"train\"].keys():\n    data[\"train\"][k][\"emotion_class\"] = emotion_map[data[\"train\"][k][\"emotion\"]]","a52b63f5":"data[\"emotion_map\"] = emotion_map","4a5960c0":"from tqdm import tqdm ","befced0a":"tweets_data = {}\nfor k in tqdm(tweets_user.keys()):\n    users_text_list = tweets_user[k].split(\" \")\n    for t in users_text_list:\n        t = t.lower()\n        if t not in tweets_data:\n            tweets_data[t] = 1\n        else:\n            tweets_data[t] = tweets_data[t] + 1","a1701c13":"remove_key = []\nfor k in tweets_data.keys():\n    if '\\r' in k:\n        remove_key.append(k)\n    elif r'\\u' in k:\n        remove_key.append(k)\n    elif '\"\\\"' in k:\n        remove_key.append(k)\n    elif k.isdigit():\n        remove_key.append(k)\n    elif k == '':\n        remove_key.append(k)\n    elif k[0].isdigit():\n        remove_key.append(k)\n    elif len(k)>10:\n        remove_key.append(k)\n    elif '0' in k:\n        remove_key.append(k)\n    elif '1' in k:\n        remove_key.append(k)\n    elif '2' in k:\n        remove_key.append(k)\n    elif '3' in k:\n        remove_key.append(k)\n    elif '4' in k:\n        remove_key.append(k)\n    elif '5' in k:\n        remove_key.append(k)\n    elif '6' in k:\n        remove_key.append(k)\n    elif '7' in k:\n        remove_key.append(k)\n    elif '8' in k:\n        remove_key.append(k)\n    elif '9' in k:\n        remove_key.append(k)     \n        \nfor i in remove_key:\n    tweets_data.pop(i)","7b8a7c2e":"sorted_tweets_key = sorted(tweets_data.keys())","2f5f2740":"tweet_map  = {'':0}\nfor i,v in enumerate(sorted_tweets_key):\n    tweet_map[v] = i + 1","7cf8a385":"l_max = 0\nfor k in tqdm(tweets_user.keys()):\n    users_text_list = tweets_user[k].split(\" \")\n    embedded_text = []\n    for t in users_text_list:\n        t = t.lower()\n        if t in tweet_map:\n            embedded_text.append(tweet_map[t])\n    if len(embedded_text)>l_max:\n        l_max = len(embedded_text)\n        ","cfbeba07":"for k in tqdm(data[\"train\"].keys()):\n    users_text_list = data[\"train\"][k][\"text\"].split(\" \")\n    embedded_text = []\n    for t in users_text_list:\n        t = t.lower()\n        if t in tweet_map:\n            embedded_text.append(tweet_map[t])\n    while len(embedded_text)<l_max:\n        embedded_text.append(0)\n    data[\"train\"][k][\"text_class\"] = embedded_text","2fec8ad5":"for k in tqdm(data[\"test\"].keys()):\n    users_text_list = data[\"test\"][k][\"text\"].split(\" \")\n    embedded_text = []\n    for t in users_text_list:\n        t = t.lower()\n        if t in tweet_map:\n            embedded_text.append(tweet_map[t])\n    while len(embedded_text)<l_max:\n        embedded_text.append(0)\n    data[\"test\"][k][\"text_class\"] = embedded_text\n        ","7cf79630":"data[\"tweet_map\"] = tweet_map\ndata[\"max_voc\"] = len(tweet_map)\ndata[\"max_l\"] = l_max","4fe803a5":"import torch \n\ntorch.load(\"..\/input\/traineddmhw2\/model_full.pt\")","f70bdff7":"import torch\nfrom torch.utils.data import DataLoader, Dataset\nfrom torch.utils.data import SubsetRandomSampler\nfrom torch import nn","3e0d8670":"class TWEETDataset_test(Dataset):\n    def __init__(self, name, feature):\n        self.name = name\n        self.feature = [torch.tensor(i) for i in feature]\n\n    def __len__(self):\n        return len(self.feature)\n\n    def __getitem__(self, idx):\n        return self.name[idx], self.feature[idx]","c50f8812":"# This script was modified from\n# https:\/\/github.com\/TalwalkarLab\/leaf\/blob\/master\/models\/shakespeare\/stacked_lstm.py\nimport torch\n\nclass LSTM_model(torch.nn.Module):\n    def __init__(self, n_vocab=80, output = 10, embedding_dim=64, hidden_dim_1=256, nb_layers_1=1):\n        super(LSTM_model, self).__init__()\n\n        self.embedding_dim = embedding_dim\n        self.hidden_dim_1 = hidden_dim_1\n\n        self.embeddings = torch.nn.Embedding(n_vocab, embedding_dim)\n        self.lstm_1 = torch.nn.LSTM(embedding_dim, hidden_dim_1, nb_layers_1)\n        self.hidden2out = torch.nn.Linear(hidden_dim_1, output)\n\n    def forward(self, seq_in):\n        embeddings = self.embeddings(seq_in.t())\n        lstm_out, h_state1 = self.lstm_1(embeddings)\n        \n        ht = lstm_out[-1]\n        out = self.hidden2out(ht)\n\n        return out","b43085ef":"def test(model, test_data):\n    model.to(device)\n    model.eval()\n    \n    names = []\n    predicts = []\n\n    with torch.no_grad():\n        for name, data in tqdm(test_data):\n\n            data = data.to(device)\n            pred = model(data)\n            _, predicted = torch.max(pred, -1)\n            \n            names.append(name)\n            predicts.append(predicted)\n\n    return names, predicts","32229451":"device = torch.device(\"cuda:0\")\nbatch_size = 4\nLR = 0.001\nepoch = 2","5f2e7f38":"train_x = []\ntrain_y = []\n\ntest_name = []\ntest_x = []","73420933":"for k in data[\"train\"].keys():\n    train_x.append(data[\"train\"][k]['text_class'])\n    train_y.append(data[\"train\"][k]['emotion_class'])\n        \nfor k in data[\"test\"].keys():\n    test_name.append(k)\n    test_x.append(data[\"test\"][k]['text_class'])","5c248481":"print(\"Total train data: {}\".format(len(train_x)))\nprint(\"Total test data: {}\".format(len(test_x)))","f479c5ab":"test_dataset = TWEETDataset_test(name=test_name, feature=test_x)\ntest_dataloader = DataLoader(test_dataset, batch_size=batch_size)","e47c438e":"model = LSTM_model(n_vocab=data[\"max_voc\"], output = 8, embedding_dim=128, hidden_dim_1=256, nb_layers_1=1)","4e021eda":"name, predict = tqdm(test(model=model,test_data=test_dataloader))","dd10e896":"def get_emotion_by_class(value):\n    return list(data[\"emotion_map\"] .keys())[list(data[\"emotion_map\"].values()).index(value)]","9b043457":"result = {}\nfor n,p in zip(name, predict):\n    p = p.cpu().tolist()\n    for i,v in enumerate(n):\n        result[v] = get_emotion_by_class(p[i])","a04cec30":"result_id = list(result.keys())\nresult_values = list(result.values())","072b7b89":"import pandas","66c7b0a5":"save_data = pandas.DataFrame({\"id\":result_id, \"emotion\":result_values})\nsave_data.to_csv(\".\/Submission.csv\",index=False)","1c74b0b4":"# Load Trained Data","5d814135":"# Import Json","36bcbf0b":"# Import Emotion","fd7f17a8":"# Create Data","15695565":"# Analyze Text","1beccf30":"# Submission","df9459ed":"# Encode Emotion","f5ee4a86":"# Import Data Identification","8b0aa921":"# Remove Emoji","019f52e7":"# Build Data Structure"}}