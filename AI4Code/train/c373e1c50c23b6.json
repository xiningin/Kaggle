{"cell_type":{"a0aba945":"code","dc2296a4":"code","9d1daa40":"code","70e5d7fc":"code","cce61e14":"code","b25f326b":"code","10979b9e":"code","9c9b1aa3":"code","25a81677":"code","05f9ef76":"code","13012931":"code","a69f68da":"code","5e026bd6":"code","2a039750":"code","9c751836":"code","051a2aca":"code","6446ea0a":"code","56fc086d":"code","401f274e":"code","4f0f0183":"code","8679f0c9":"markdown","b26cd645":"markdown","2e08fe83":"markdown","4712953b":"markdown","9b7c78a1":"markdown","bb701c23":"markdown","bd63bed0":"markdown","a6c3e32b":"markdown","b442bc6b":"markdown"},"source":{"a0aba945":"from collections import Counter\nimport numpy as np\nimport re\nimport os","dc2296a4":"def open_file(filename, mode='r'):\n    return open(filename, mode, encoding='utf-8', errors='ignore')","9d1daa40":"def clean_str(string):\n    string = re.sub(r\"[^A-Za-z0-9(),!?\\'\\`]\", \" \", string)\n    string = re.sub(r\"\\'s\", \" \\'s\", string)\n    string = re.sub(r\"\\'ve\", \" \\'ve\", string)\n    string = re.sub(r\"n\\'t\", \" n\\'t\", string)\n    string = re.sub(r\"\\'re\", \" \\'re\", string)\n    string = re.sub(r\"\\'d\", \" \\'d\", string)\n    string = re.sub(r\"\\'ll\", \" \\'ll\", string)\n    string = re.sub(r\",\", \" , \", string)\n    string = re.sub(r\"!\", \" ! \", string)\n    string = re.sub(r\"\\(\", \" \\( \", string)\n    string = re.sub(r\"\\)\", \" \\) \", string)\n    string = re.sub(r\"\\?\", \" \\? \", string)\n    string = re.sub(r\"\\s{2,}\", \" \", string)\n    return string.strip().lower()","70e5d7fc":"def build_vocab(data, vocab_dir, vocab_size=8000):\n    print('Building vocabulary...')\n\n    all_data = []  # group all data\n    for content in data:\n        all_data.extend(content.split())\n\n    counter = Counter(all_data)  # count and get the most common words\n    count_pairs = counter.most_common(vocab_size - 1)\n    words, _ = list(zip(*count_pairs))\n\n    words = ['<PAD>'] + list(words)  # add a padding with id 0 to pad the sentence to same length\n    open_file(vocab_dir, 'w').write('\\n'.join(words) + '\\n')","cce61e14":"def read_vocab(vocab_file):\n    \"\"\"\n    Read vocabulary from file.\n    \"\"\"\n    words = open_file(vocab_file).read().strip().split('\\n')\n    word_to_id = dict(zip(words, range(len(words))))\n    return words, word_to_id","b25f326b":"def process_text(text, word_to_id, max_length, clean=True):\n    if clean:  # if the data needs to be cleaned\n        text = clean_str(text)\n    text = text.split()\n\n    text = [word_to_id[x] for x in text if x in word_to_id]\n    if len(text) < max_length:\n        text = [0] * (max_length - len(text)) + text\n    return text[:max_length]","10979b9e":"class Corpus(object):\n    def __init__(self, pos_file, neg_file, vocab_file, dev_split=0.1, max_length=50, vocab_size=8000):\n        # loading data\n        pos_examples = [clean_str(s.strip()) for s in open_file(pos_file)]\n        neg_examples = [clean_str(s.strip()) for s in open_file(neg_file)]\n        x_data = pos_examples + neg_examples\n        y_data = [0.] * len(pos_examples) + [1.] * len(neg_examples)  # 0 for pos and 1 for neg\n\n        if not os.path.exists(vocab_file):\n            build_vocab(x_data, vocab_file, vocab_size)\n\n        self.words, self.word_to_id = read_vocab(vocab_file)\n\n        for i in range(len(x_data)):  # tokenizing and padding\n            x_data[i] = process_text(x_data[i], self.word_to_id, max_length, clean=False)\n\n        x_data = np.array(x_data)\n        y_data = np.array(y_data)\n\n        # shuffle\n        indices = np.random.permutation(np.arange(len(x_data)))\n        x_data = x_data[indices]\n        y_data = y_data[indices]\n\n        # train\/dev split\n        num_train = int((1 - dev_split) * len(x_data))\n        self.x_train = x_data[:num_train]\n        self.y_train = y_data[:num_train]\n        self.x_test = x_data[num_train:]\n        self.y_test = y_data[num_train:]\n    def __str__(self):\n        return 'Training: {}, Testing: {}, Vocabulary: {}'.format(len(self.x_train), len(self.x_test), len(self.words))","9c9b1aa3":"import mxnet as mx\nfrom mxnet import gluon, autograd, metric\nfrom mxnet import ndarray as nd\nfrom mxnet.gluon import nn\nfrom mxnet.gluon.data import Dataset, DataLoader\nimport numpy as np\nfrom sklearn import metrics\nimport os\nimport time\nfrom datetime import timedelta\nimport pandas as pd\n\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","25a81677":"pos_file = os.path.join(\"..\/input\/rt-polarity.pos.txt\")\nneg_file = os.path.join(\"..\/input\/rt-polarity.neg.txt\")\nvocab_file = os.path.join(\"..\/input\/rt-polarity.vocab.txt\")\n\n# model save path\nsave_path = 'checkpoints'\nif not os.path.exists(save_path):\n    os.mkdir(save_path)\nmodel_file = os.path.join(save_path, 'mr_cnn_mxnet.params')","05f9ef76":"def try_gpu():\n    try:\n        ctx = mx.gpu()\n        _ = nd.array([0], ctx=ctx)\n    except:\n        ctx = mx.cpu()\n    return ctx","13012931":"class TCNNConfig(object):\n    embedding_dim = 128  # embedding vector size\n    seq_length = 50  # maximum length of sequence\n    vocab_size = 8000  # most common words\n\n    num_filters = 100  # number of the convolution filters (feature maps)\n    kernel_sizes = [3, 4, 5]  # three kinds of kernels (windows)\n\n    dropout_prob = 0.5  # dropout rate\n    learning_rate = 1e-3  # learning rate\n    batch_size = 50  # batch size for training\n    num_epochs = 10    # total number of epochs\n\n    num_classes = 2  # number of classes\n\n    dev_split = 0.1  # percentage of dev data","a69f68da":"class Conv_Max_Pooling(nn.Block):\n    def __init__(self, channels, kernel_size, **kwargs):\n        super(Conv_Max_Pooling, self).__init__(**kwargs)\n\n        with self.name_scope():\n            self.conv = nn.Conv1D(channels, kernel_size)\n            self.pooling = nn.GlobalMaxPool1D()\n\n    def forward(self, x):\n        output = self.pooling(self.conv(x))\n        return nd.relu(output).flatten()","5e026bd6":"class TextCNN(nn.Block):\n    def __init__(self, config, **kwargs):\n        super(TextCNN, self).__init__(**kwargs)\n\n        V = config.vocab_size\n        E = config.embedding_dim\n        Nf = config.num_filters\n        Ks = config.kernel_sizes\n        C = config.num_classes\n        Dr = config.dropout_prob\n\n        with self.name_scope():\n            self.embedding = nn.Embedding(V, E)  # embedding layer\n\n            # three different convolutional layers\n            self.conv1 = Conv_Max_Pooling(Nf, Ks[0])\n            self.conv2 = Conv_Max_Pooling(Nf, Ks[1])\n            self.conv3 = Conv_Max_Pooling(Nf, Ks[2])\n            self.dropout = nn.Dropout(Dr)  # a dropout layer\n            self.fc1 = nn.Dense(C)  # a dense layer for classification\n\n    def forward(self, x):\n        x = self.embedding(x).transpose((0, 2, 1))  # Conv1D takes in NCW as input\n        o1, o2, o3 = self.conv1(x), self.conv2(x), self.conv3(x)\n        outputs = self.fc1(self.dropout(nd.concat(o1, o2, o3)))\n\n        return outputs","2a039750":"def get_time_dif(start_time):\n    end_time = time.time()\n    time_dif = end_time - start_time\n    return timedelta(seconds=int(round(time_dif)))\n","9c751836":"class MRDataset(Dataset):\n    def __init__(self, x, y):\n        super(MRDataset, self).__init__()\n        self.x = x\n        self.y = y\n\n    def __getitem__(self, index):\n        return self.x[index].astype(np.float32), self.y[index].astype(np.float32)\n\n    def __len__(self):\n        return len(self.x)\n","051a2aca":"def evaluate(data_loader, data_len, model, loss, ctx):\n    total_loss = 0.0\n    acc = metric.Accuracy()\n\n    for data, label in data_loader:\n        data, label = data.as_in_context(ctx), label.as_in_context(ctx)\n\n        with autograd.record(train_mode=False):  # set the training_mode to False\n            output = model(data)\n            losses = loss(output, label)\n\n        total_loss += nd.sum(losses).asscalar()\n        predictions = nd.argmax(output, axis=1)\n        acc.update(preds=predictions, labels=label)\n    return acc.get()[1], total_loss \/ data_len","6446ea0a":"def train():\n    print(\"Loading data...\")\n    start_time = time.time()\n    config = TCNNConfig()\n    corpus = Corpus(pos_file, neg_file, vocab_file, config.dev_split, config.seq_length, config.vocab_size)\n    print(corpus)\n    config.vocab_size = len(corpus.words)\n\n    print(\"Configuring CNN model...\")\n    ctx = try_gpu()\n    model = TextCNN(config)\n    model.collect_params().initialize(ctx=ctx)\n    print(\"Initializing weights on\", ctx)\n    print(model)\n\n    # optimizer and loss function\n    loss = gluon.loss.SoftmaxCrossEntropyLoss()\n    trainer = gluon.Trainer(model.collect_params(), 'adam', {'learning_rate': config.learning_rate})\n\n    batch_size = config.batch_size\n    train_loader = DataLoader(MRDataset(corpus.x_train, corpus.y_train), batch_size=batch_size, shuffle=True)\n    test_loader = DataLoader(MRDataset(corpus.x_test, corpus.y_test), batch_size=batch_size, shuffle=False)\n\n    print(\"Training and evaluating...\")\n    best_acc = 0.0\n    for epoch in range(config.num_epochs):\n        for data, label in train_loader:\n            data, label = data.as_in_context(ctx), label.as_in_context(ctx)\n\n            with autograd.record(train_mode=True):  # set the model in training mode\n                output = model(data)\n                losses = loss(output, label)\n\n            # backward propagation and update parameters\n            losses.backward()\n            trainer.step(len(data))\n\n        # evaluate on both training and test dataset\n        train_acc, train_loss = evaluate(train_loader, len(corpus.x_train), model, loss, ctx)\n        test_acc, test_loss = evaluate(test_loader, len(corpus.x_test), model, loss, ctx)\n\n        if test_acc > best_acc:\n            # store the best result\n            best_acc = test_acc\n            improved_str = '*'\n            model.save_params(model_file)\n        else:\n            improved_str = ''\n\n        time_dif = get_time_dif(start_time)\n        msg = \"Epoch {0:3}, Train_loss: {1:>7.2}, Train_acc {2:>6.2%}, \" \\\n              + \"Test_acc {4:>6.2%}, Time: {5} {6}\"\n        print(msg.format(epoch + 1, train_loss, train_acc, test_loss, test_acc, time_dif, improved_str))\n\n    test(model, test_loader, ctx)","56fc086d":"def test(model, test_loader, ctx):\n    print(\"Testing...\")\n    start_time = time.time()\n    model.load_params(model_file, ctx=ctx)   # restore the best parameters\n\n    y_pred, y_true = [], []\n    for data, label in test_loader:\n        data, label = data.as_in_context(ctx), label.as_in_context(ctx)\n        with autograd.record(train_mode=False):  # set the training_mode to False\n            output = model(data)\n        pred = nd.argmax(output, axis=1).asnumpy().tolist()\n        y_pred.extend(pred)\n        y_true.extend(label.asnumpy().tolist())\n\n    test_acc = metrics.accuracy_score(y_true, y_pred)\n    test_f1 = metrics.f1_score(y_true, y_pred, average='macro')\n    print(\"Test accuracy: {0:>7.2%}, F1-Score: {1:>7.2%}\".format(test_acc, test_f1))\n\n    print(\"Precision, Recall and F1-Score...\")\n    print(metrics.classification_report(y_true, y_pred, target_names=['POS', 'NEG']))\n\n    print('Confusion Matrix...')\n    cm = metrics.confusion_matrix(y_true, y_pred)\n    print(cm)\n\n    print(\"Time usage:\", get_time_dif(start_time))\n","401f274e":"def predict(text):\n    # load config and vocabulary\n    print('\\npredicting....')\n    config = TCNNConfig()\n    _, word_to_id = read_vocab(vocab_file)\n    labels = ['POS', 'NEG']\n\n    # load model\n    ctx = try_gpu()\n    model = TextCNN(config)\n    model.load_params(model_file, ctx=ctx)\n\n    # process text\n    print(text)\n    text = process_text(text, word_to_id, config.seq_length)\n    text = nd.array([text]).as_in_context(ctx)\n\n    output = model(text)\n    pred = nd.argmax(output, axis=1).asscalar()\n    print(\"Polarity:\",output[0,0])\n    print(\"class:\",labels[int(pred)])","4f0f0183":"if __name__ == '__main__':\n    train()\n    predict(\"If you sometimes like to go to the movies to have fun, wasabi is a good place to start\")\n    predict(\"This is a film well worth seeing, talking and singing heads and all\")\n    predict(\"One of the greatest family oriented, fantasy- adventure movies ever\")\n    predict(\"sadly , though many of the actors throw off a spark or two when they first appear , they can't generate enough heat in this cold vacuum of a comedy to start a reaction\")\n    predict(\"It was a taut, intelligent and psychological drama\")\n    predict(\"it's so laddish and juvenile , only teenage boys could possibly find it funny\")\n    predict(\"I really hate this movie\")\n    predict(\"This is a good movie\")","8679f0c9":"**Commonly used file reader and writer, change this to switch between python2 and python3.\n:param filename: filename\n:param mode: 'r' and 'w' for read and write respectively**","b26cd645":"**Preprocessing training data.**","2e08fe83":"**BUILDING DATA CLEANING FUNCTIONS**","4712953b":"**Tokenization\/string cleaning for all datasets except for SST.\n    Original taken from https:\/\/github.com\/yoonkim\/CNN_sentence\/blob\/master\/process_data.py**","9b7c78a1":"*Importing Datasets*","bb701c23":"*Try GPU*","bd63bed0":"**Build vocabulary file from training data.**","a6c3e32b":"**tokenizing and padding**","b442bc6b":"**IMPORTING DATA AND BUILDING CNN MODEL**"}}