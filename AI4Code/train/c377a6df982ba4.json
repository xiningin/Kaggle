{"cell_type":{"45119555":"code","b4716290":"code","5797e90c":"code","f44f83b8":"code","f5aab27e":"code","23372365":"code","f1477e57":"code","88f70eca":"code","090c44df":"code","5a85b3b9":"code","c20dc198":"code","ca658355":"code","6de9dd34":"code","da9a48a2":"code","56a6cc6b":"code","09c669c9":"code","77d7dabb":"code","1bcfb90f":"code","21820ffc":"code","465c0ae2":"code","d421017c":"code","c2c5c8be":"code","d6c042c8":"code","42cb28ee":"code","0fcb2904":"code","948a237b":"code","5f13c6da":"code","5f4a91cf":"code","34ad6b9e":"code","d85e9374":"code","4432430b":"code","dd5ea5aa":"code","07f1db59":"code","0efdd2e4":"code","eb12caae":"code","24b70783":"code","82fd5cad":"code","c269289b":"markdown","96d1426b":"markdown","f32bc7ed":"markdown","bab1f6ee":"markdown","13f1aac2":"markdown","bd70f226":"markdown","e33f1c3c":"markdown","a6036ad6":"markdown","b2ce86f2":"markdown","67cac9b5":"markdown","897e6e60":"markdown","60ae956c":"markdown","4f363a2b":"markdown","09f22f91":"markdown","48b2534d":"markdown","8cb83742":"markdown","eae55fbd":"markdown","00338938":"markdown","434ea0e2":"markdown","15f2a8a6":"markdown","3fa852fd":"markdown","c5dbc9a2":"markdown","2fe08f5c":"markdown","2c51d264":"markdown","d20dd354":"markdown","1d6fdb0d":"markdown","00fca460":"markdown","75669dce":"markdown","9ee1f983":"markdown","60536960":"markdown","bf1e3863":"markdown","fda480d0":"markdown","071acc72":"markdown","3016e830":"markdown","d1319454":"markdown","15881cca":"markdown","39bef95c":"markdown","1d93f360":"markdown","4a92864c":"markdown","d0d163f3":"markdown","44d05518":"markdown","16100b3b":"markdown"},"source":{"45119555":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nfrom sklearn.impute import SimpleImputer\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","b4716290":"tr_data = pd.read_csv('\/kaggle\/input\/titanic\/train.csv')\ntr_data.head()","5797e90c":"te_data = pd.read_csv('\/kaggle\/input\/titanic\/test.csv')\nte_data.head()","f44f83b8":"cols = tr_data.columns\nprint(cols)","f5aab27e":"tr_data.info()","23372365":"tr_data['Age'].describe()","f1477e57":"mean_age = tr_data[\"Age\"].mean()\nprint(mean_age)\ntr_data['Age'] = tr_data['Age'].fillna(mean_age) \nte_data['Age'] = te_data['Age'].fillna(mean_age) ","88f70eca":"\ntr_data.loc[ tr_data['Age'] <= 16, \"Age\"] = 1\ntr_data.loc[(tr_data['Age'] > 16) & (tr_data['Age'] <= 32), \"Age\"] = 2\ntr_data.loc[(tr_data['Age'] > 32) & (tr_data['Age'] <= 48), \"Age\"] = 3\ntr_data.loc[(tr_data['Age'] > 48) & (tr_data['Age'] <= 64), \"Age\"] = 4\ntr_data.loc[ tr_data['Age'] > 64, \"Age\"] = 5\n\nte_data.loc[ te_data['Age'] <= 16, \"Age\"] = 1\nte_data.loc[(te_data['Age'] > 16) & (te_data['Age'] <= 32), \"Age\"] = 2\nte_data.loc[(te_data['Age'] > 32) & (te_data['Age'] <= 48), \"Age\"] = 3\nte_data.loc[(te_data['Age'] > 48) & (te_data['Age'] <= 64), \"Age\"] = 4\nte_data.loc[ te_data['Age'] > 64, \"Age\"] = 5\n    \ntr_data[\"Age\"].isnull().sum()\nte_data[\"Age\"].isnull().sum()\n","090c44df":"women = tr_data.loc[tr_data['Sex']=='female'][\"Survived\"]\nrate_women = sum(women)\/len(women)\n\nprint('% of women who survived:', rate_women)","5a85b3b9":"men = tr_data.loc[tr_data['Sex']=='male']['Survived']\nrate_men = sum(men)\/len(men)\n\nprint('% of men who survived:', rate_men)","c20dc198":"name = tr_data['Name']\n\n#for tr_data\ndataset_title = [i.split(\",\")[1].split(\".\")[0].strip() for i in tr_data[\"Name\"]]\ntr_data[\"Title\"] = pd.Series(dataset_title)\ntr_data[\"Title\"] = tr_data[\"Title\"].replace([\"Master\", \"Dr\", \"Rev\", \"Col\", \"Major\", \"the Countess\", \"Capt\", \"Jonkheer\", \"Lady\", \"Sir\",\"Don\", \"Dona\", \"Mlle\", \"Ms\", \"Mme\"], 'Rare')\ntr_data[\"Title\"] = tr_data[\"Title\"].replace([\"Mr\", \"Miss\", \"Mrs\"], 'Common')\n\n#for te_data\ndataset_title = [i.split(\",\")[1].split(\".\")[0].strip() for i in te_data[\"Name\"]]\nte_data[\"Title\"] = pd.Series(dataset_title)\nte_data[\"Title\"] = te_data[\"Title\"].replace([\"Master\", \"Dr\", \"Rev\", \"Col\", \"Major\", \"the Countess\", \"Capt\", \"Jonkheer\", \"Lady\", \"Sir\", \"Don\", \"Dona\", \"Mlle\", \"Ms\", \"Mme\"], 'Rare')\nte_data[\"Title\"] = te_data[\"Title\"].replace([\"Mr\", \"Miss\", \"Mrs\"], 'Common')\n     \nprint(tr_data[\"Title\"].value_counts())\nprint(te_data[\"Title\"].value_counts())","ca658355":"# #for train set\n# tr_data['FamilySize'] = tr_data['SibSp'] + tr_data['Parch'] +1\n\n# #for test set\n# te_data['FamilySize'] = tr_data['SibSp'] + tr_data['Parch'] +1\n\n# tr_data[['FamilySize', 'Survived']].groupby(['FamilySize'], as_index=False).mean().sort_values(by='Survived', ascending=False)","6de9dd34":"\n# list = []\n\n# for i in tr_data['FamilySize']:\n#     if i in [4, 3, 2, 7]:\n#         list.append(\"ideal\")\n#     elif i ==1:\n#         list.append(\"alone\")\n#     else:\n#         list.append(\"non_ideal\")\n        \n# tr_data[\"IdealFamilySize\"] = list\n\n# #for te_data\n# list2 = []\n\n# for i in te_data['FamilySize']:\n#     if i in [4, 3, 2, 7]:\n#         list2.append(\"ideal\")\n#     elif i ==1:\n#         list2.append(\"alone\")\n#     else:\n#         list2.append(\"non_ideal\")\n        \n# te_data[\"IdealFamilySize\"] = list2\n        \n# print(te_data['FamilySize'].head(10))\n# print(te_data['IdealFamilySize'].head(10))","da9a48a2":"#for tr_data\ncabin_list = []\n\ntr_data['Cabin'] = tr_data['Cabin'].fillna('None')\n\nfor i in tr_data['Cabin']:\n    if i == 'None':\n        cabin_list.append('None')\n    else:\n        cabin_list.append('Cabin')\n\ntr_data['Cabin'] = cabin_list\n\n#for te_data\ncabin_list2 = []\n\nte_data['Cabin'] = te_data['Cabin'].fillna('None')\n\nfor i in te_data['Cabin']:\n    if i == 'None':\n        cabin_list2.append('None')\n    else:\n        cabin_list2.append('Cabin')\n\nte_data['Cabin'] = cabin_list2\n\nte_data['Cabin'].value_counts()","56a6cc6b":"#tr_data['Fare'] = pd.qcut(tr_data['Fare'], 4)\n#tr_data[['Fare', 'Survived']].groupby(['Fare'], as_index=False).mean().sort_values(by='Fare', ascending=True)\n\nmap(float(), tr_data['Fare'])\n\nprint(tr_data['Fare'].head())","09c669c9":"#for tr_data\nfare_list = []\n\nfor i in tr_data['Fare']:\n    i = float(i)\n    if i <= 7.91:\n        fare_list.append(1)\n    elif i <= 14.454:\n        fare_list.append(2)\n    elif i <= 31.0:\n        fare_list.append(3)\n    else:\n        fare_list.append(4)\n        \ntr_data['Fare'] = fare_list\n\n#for te_data\nfare_list2 = []\n\nfor i in te_data['Fare']:\n    i = float(i)\n    if i <= 7.91:\n        fare_list2.append(1)\n    elif i <= 14.454:\n        fare_list2.append(2)\n    elif i <= 31.0:\n        fare_list2.append(3)\n    else:\n        fare_list2.append(4)\n        \nte_data['Fare'] = fare_list2\n\nprint(tr_data['Fare'].head())\nprint(te_data['Fare'].head())","77d7dabb":"tr_data = tr_data.drop(['Name', 'Ticket', 'Cabin'], axis=1)\nte_data = te_data.drop(['Name', 'Ticket', 'Cabin'], axis=1)","1bcfb90f":"features = ['Pclass', 'Sex', 'SibSp', 'Parch','Embarked', \"Title\", \"Fare\"]\n\nfor i in features:\n    tr_data [i] = pd.get_dummies(tr_data[i])\n    te_data [i] = pd.get_dummies(te_data[i])\n\nprint(tr_data.head())","21820ffc":"df = pd.DataFrame(tr_data,columns=['Survived', 'Pclass', 'Sex', 'Age', 'SibSp','Parch', 'Fare','Embarked', \"Title\"])\ncorrMatrix = df.corr()\nprint (corrMatrix)","465c0ae2":"# tr_data['IntAgeParch'] = tr_data['Age'] * tr_data['Parch']\n# tr_data['IntSibParch'] = tr_data['SibSp'] * tr_data['Parch']\ntr_data['IntSibFare'] = tr_data['Age'] * tr_data['Parch']\nte_data['IntSibFare'] = te_data['Age'] * te_data['Parch']","d421017c":"print(tr_data['Survived'].head(10))","c2c5c8be":"from sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.svm import SVC\n\ny = tr_data['Survived']\n\nfeatures = ['Pclass', 'Sex', 'Age', 'SibSp','Parch','Embarked', \"Title\"]\nX = tr_data[features]\nX_test = te_data[features]\n\nx_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)","d6c042c8":"#rfc=RandomForestClassifier(random_state=42)","42cb28ee":"# param_grid = { \n#     'n_estimators': [200, 500],\n#     'max_features': ['auto', 'sqrt', 'log2'],\n#     'max_depth' : [4,5,6,7,8],\n#     'criterion' :['gini', 'entropy']\n# }","0fcb2904":"# CV_rfc = GridSearchCV(estimator=rfc, param_grid=param_grid, cv= 5)\n# CV_rfc.fit(x_train, y_train)","948a237b":"# CV_rfc.best_params_","5f13c6da":"# model = RandomForestClassifier(max_depth= 4, max_features = 'auto', n_estimators = 500, criterion= 'entropy')\n# model.fit(X, y)\n# predictions = model.predict(X_test)\n\n# output = pd.DataFrame({'PassengerId': te_data.PassengerId, 'Survived':predictions})\n# output.to_csv('my_submission.csv', index=False)\n# print(\"Your submission was successfully save! :D\")","5f4a91cf":"import tensorflow as tf\nimport numpy as np\nfrom tensorflow import keras","34ad6b9e":"model = tf.keras.models.Sequential([tf.keras.layers.Flatten(), \n                                    tf.keras.layers.Dense(128, activation=tf.nn.relu), \n                                    tf.keras.layers.Dense(2, activation=tf.nn.softmax)])","d85e9374":"model.compile(optimizer='sgd', loss='mean_squared_error')","4432430b":"x_train_list = x_train.values.tolist()\ny_train_list = y_train.values.tolist()\nx_test_list = x_test.values.tolist()\ny_test_list = y_test.values.tolist()","dd5ea5aa":"X_list = x_train.values.tolist()\ny_list = y_train.values.tolist()","07f1db59":"model.fit(X_list, y_list, epochs=100)","0efdd2e4":"X_test_list = X_test.values.tolist()\nprint(len(X_test_list))","eb12caae":"predictions = model.predict(X_test_list)","24b70783":"df_pred = pd.DataFrame(predictions)\n\npred_final = []\n\nfor i, j in df_pred.iterrows():\n    if j[0] > j[1]:\n         pred_final.append(1)\n    else:\n         pred_final.append(0)\n        \nprint(pred_final[1:10])","82fd5cad":"output = pd.DataFrame({'PassengerId': te_data.PassengerId, 'Survived':pred_final})\noutput.to_csv('my_submission.csv', index=False)\nprint(\"Your submission was successfully save! :D\")","c269289b":"In order to make my RandomForest work, I need dummies. Hence here, I just split the age in 5 groups to create dummies. I created five equal layer between 0 and 80, so one per 16 years.","96d1426b":"From the above analysis, we see that it's not, the bigger, the better. We will thus create three groups: ideal family size, non-ideal familsy size and alone.\n\nEdit: this code below decreased my score, I'll remove it.","f32bc7ed":"# 4. Modelling","bab1f6ee":"So we can see that the best parameters for the features 'Pclass', 'Sex', 'Age', 'SibSp','Parch', 'Fare','Embarked', \"Title\" is {'criterion': 'entropy',\n 'max_depth': 4,\n 'max_features': 'log2',\n 'n_estimators': 500}","13f1aac2":"Based on the correlation table, I thought that creating interraction terms would help. But it didn't, so I end up removing it from my final model.","bd70f226":"let's drop useless variable: Name, ticket, PassengerId and Cabin","e33f1c3c":"## 3.1. Tackling the Age problem","a6036ad6":"## 3.8. Correlation and interaction terms","b2ce86f2":"# 2. Data collection\nUploading and creating the training and test datasets","67cac9b5":"### 3.1.3 Replacing age by dummies","897e6e60":"We see that the most important variables are Sex, Pclass, Fare, Embarked, Parch, Age, SibSp. \nOn the other hand Title and Age have minor effects.\nNow we also have 3 big correlation: Age\/Parch, SibSp\/Parch, SibSp\/Fare. we will thus create interaction term for those in 9.","60ae956c":"Let's jolt a Neural Network and see if we have any improvement.","4f363a2b":"## 3.5. Cabins vs no-cabins","09f22f91":"Ok we ran into a problem: ValueError: Please provide as model inputs either a single array or a list of arrays. You passed: inputs=     Pclass  Sex  Age  SibSp  Parch  Embarked  Title. So we transform in list.","48b2534d":"### 4.3.1 Training the model\nWe can still use the x_train, x_test, y_train, y_test of the 1.1 paragraph","8cb83742":"I am here working on the name column, and splitting it in pieces in order to isolate the title part.","eae55fbd":"## 3.7. Drop useless and creating the dummies","00338938":"## 3.2.Analysis of the survival ratio per gender","434ea0e2":"Now let's analyze the correlations","15f2a8a6":"We see that the minimum is less than one year, and that the maximum is 80. That seems correct!","3fa852fd":"## 4.3 Neural network test","c5dbc9a2":"We realize from the analysis that 3 variables have missing info: Age, Cabin and Embarked.\n1. Age: the age of the passengers seems to be important variable of survival in general. But has we have some missing, we shoudl try to find a proxy to replace the missing value. For instance, the price of the picket might have some correlation with the age.\n2. Cabin: We can guess that a lot of people didn't have cabins. The next step is to see if it's correlated with the fare.\n3. Embarked: It contains only two values missing values. This doesn't affect much the analysis","2fe08f5c":"So we see 4 different groups that we will use to create 4 categories: Fare1, Fare2, Fare3, Fare4. This division is comes from the pd.qcut(tr_data['Fare'], 4)","2c51d264":"I tried finding proxy true classes or so on, but it only decreased the final score. So I just went for the average.","d20dd354":"## 3.6. Fare \nLet's try to see if fare brings something good.","1d6fdb0d":"## 3.3. Creating a \"title of nobility\" column","00fca460":"Test 1 (x_train, y_train, x_test, y_test):The below does work, but I have, for 5 epochs, a loss: 0.2510. Let's try more. For 100, I land loss: 0.2502. Not great improvement. And changing the optimizer 'sdg' to tf.optimizers.Adam() only makes it go until 0.2500\n\nTest 2 (X, y)","75669dce":"This is to prove that the gender has a strong predictive input and that we shoudl consider it.","9ee1f983":"Also lowered my score: to be removed.","60536960":"We know from afterwards that Gridsearch decreased my score. I have no idea why it is so... I'll temporary offset this section.","bf1e3863":"# 3. EDA","fda480d0":"### 3.1.2 Changing the missing values","071acc72":"## 4.2 Random Forest & Grid search","3016e830":"This whole section is rubbish and just decreased my score.","d1319454":"Actually the split in x_train, y_train, x_test, y_test was useful for the GridSearch, but it's not now. So we'll try with just X and y.","15881cca":"# 1. Importing packages\nWe start by importing the neede packages","39bef95c":"## 3.4. Creating family size","1d93f360":"# 5.Testing","4a92864c":"### 3.1.1 Do the current values look correct?","d0d163f3":"## 4.1 Split dep & ind features","44d05518":"We are using the class as a proxy for age, considering that older you are, the richer, and thus the higher class.","16100b3b":"While creating the model, I tried first the model as such, then add a GridSearch with few parameters to improve the score. But it didn't help, it jus decreased the score."}}