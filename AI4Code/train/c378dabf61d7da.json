{"cell_type":{"6dfaa24d":"code","47f31fad":"code","49f0b5a1":"code","ed4e4620":"code","b717ff63":"code","6923031c":"code","911f9269":"code","ed8f76b7":"code","33ad9b08":"code","8d329b1e":"code","95ef2ede":"code","fbbfdb4f":"code","89dfd577":"code","c2e2b5ae":"code","685b5202":"code","2db41f4b":"code","f9f158f6":"code","35e5a729":"code","f3279b99":"code","781ddf5f":"code","65747f81":"code","ef33e19b":"code","fce90d41":"code","722f24f5":"code","defc2b18":"code","a47bb57d":"code","a1921235":"code","5dc7b869":"code","92f00563":"markdown","4fa5cc39":"markdown"},"source":{"6dfaa24d":"import numpy as np\nimport pandas as pd\n\nimport cv2\nimport json\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\n\nfrom PIL import Image\n\nfrom wordcloud import WordCloud\n\nimport warnings\nwarnings.filterwarnings(action='ignore')","47f31fad":"path = '..\/input\/'","49f0b5a1":"train_df = pd.read_csv(path + 'train.csv')\ntrain_df.head()","ed4e4620":"label_description = open(path + 'label_descriptions.json').read()\nlabel_description = json.loads(label_description)\nlabel_description.keys()","b717ff63":"label_description_info = label_description['info']\nlabel_description_categories = pd.DataFrame(label_description['categories'])\nlabel_description_attributes = pd.DataFrame(label_description['attributes'])","6923031c":"label_description_info","911f9269":"label_description_categories","ed8f76b7":"label_description_attributes","33ad9b08":"train_df_ImageId_count = train_df['ImageId'].value_counts()\nplt.figure(figsize=(20, 7))\nplt.title('image labels count', size=20)\nplt.xlabel('', size=15);plt.ylabel('', size=15);\nsns.countplot(train_df_ImageId_count)\nplt.show()","8d329b1e":"label_description_categories.shape, label_description_attributes.shape","95ef2ede":"train_classid = pd.DataFrame({'ClassId':train_df['ClassId'].apply(lambda x: x[:2].replace('_', ''))})\nlabel_merge = label_description_categories[['id', 'name']].astype(str).astype(object)","fbbfdb4f":"train_df_name = train_classid.merge(label_merge, left_on='ClassId', right_on='id', how='left')\nsum1 = train_df_name.shape[0]\nratio1 = np.round(train_df_name.groupby(['ClassId', 'name']).count().sort_values(by='id', ascending=False).rename(columns = {'id':'count'})\/sum1 * 100, 2)","89dfd577":"train_df_name_stat = train_df_name.groupby(['ClassId', 'name']).count().sort_values(by='id', ascending=False).rename(columns = {'id':'count'}).reset_index()\ntrain_df_name_stat['ratio(%)'] = ratio1.values\ntrain_df_name_stat","c2e2b5ae":"text = ''\nfor idx, name in enumerate(train_df_name_stat['name']):\n    text += (name + ' ') * train_df_name_stat.loc[idx, 'count']\ntext = text[:-1]\n\nwordcloud = WordCloud(max_font_size=50, width=600, height=300, background_color='white').generate(text)\nplt.figure(figsize=(15,8))\nplt.imshow(wordcloud)\nplt.title(\"label_description_attributes in the images\", fontsize=15)\nplt.axis(\"off\")\nplt.show() ","685b5202":"train_classid = pd.DataFrame({'ClassId':[j for i in train_df['ClassId'][train_df['ClassId'].apply(lambda x: '_' in x)].apply(lambda x: x.split('_')[1:]) for j in i]})\nlabel_merge = label_description_attributes[['id', 'name']].astype(str).astype(object)","2db41f4b":"train_df_name = train_classid.merge(label_merge, left_on='ClassId', right_on='id', how='left')\nsum1 = train_df_name.shape[0]\nratio1 = np.round(train_df_name.groupby(['ClassId', 'name']).count().sort_values(by='id', ascending=False).rename(columns = {'id':'count'})\/sum1 * 100, 3)","f9f158f6":"train_df_name_stat = train_df_name.groupby(['ClassId', 'name']).count().sort_values(by='id', ascending=False).rename(columns = {'id':'count'}).reset_index()\ntrain_df_name_stat['ratio(%)'] = ratio1.values\ntrain_df_name_stat","35e5a729":"text = ''\nfor idx, name in enumerate(train_df_name_stat['name']):\n    text += (name + ' ') * train_df_name_stat.loc[idx, 'count']\ntext = text[:-1]\n\nwordcloud = WordCloud(max_font_size=50, width=600, height=300, background_color='white').generate(text)\nplt.figure(figsize=(15,8))\nplt.imshow(wordcloud)\nplt.title(\"label_description_attributes in the images\", fontsize=15)\nplt.axis(\"off\")\nplt.show() ","f3279b99":"import os","781ddf5f":"sub = pd.read_csv(path + 'sample_submission.csv')","65747f81":"print('The number of training image is {}.'.format(len(os.listdir(path + 'train'))))\nprint('train data unique length is {}.\\n'.format(len(np.unique(train_df['ImageId']))))\nprint('The number of test image is {}.'.format(len(os.listdir(path + 'test'))))\nprint('test data unique length is {}.'.format(len(np.unique(sub['ImageId']))))","ef33e19b":"f, axes = plt.subplots(3, 1, figsize=(20, 15))\nplt.subplot(3, 1, 1)\nsns.distplot(train_df.groupby('ImageId').mean().reset_index()['Height'].tolist(), bins=150).set_title('Height Histogram')\n\nplt.subplot(3, 1, 2)\nsns.distplot(train_df.groupby('ImageId').mean().reset_index()['Width'].tolist(), bins=150).set_title('Width Histogram')\n\nplt.subplot(3, 1, 3)\nsns.distplot(train_df.groupby('ImageId').mean().reset_index()['Height'] * train_df.groupby('ImageId').mean().reset_index()['Width']\/10000, bins=300).set_title('Area Histogram(\/10000)')\n\nplt.show()","fce90d41":"img_name = np.argmin(train_df.groupby('ImageId').mean()['Height'] * train_df.groupby('ImageId').mean()['Width'])\nprint('minimum area picture \\nHeight : {} \\nWidth : {}'.format(train_df.loc[train_df['ImageId'] == img_name, 'Height'].mean(), train_df.loc[train_df['ImageId'] == img_name, 'Width'].mean()))\nimg = np.asarray(Image.open(path + \"train\/\" + img_name))\nplt.imshow(img)\nplt.show()","722f24f5":"img_name = np.argmax(train_df.groupby('ImageId').mean()['Height'] * train_df.groupby('ImageId').mean()['Width'])\nprint('maximum area picture \\nHeight : {} \\nWidth : {}'.format(train_df.loc[train_df['ImageId'] == img_name, 'Height'].mean(), train_df.loc[train_df['ImageId'] == img_name, 'Width'].mean()))\nimg = np.asarray(Image.open(path + \"train\/\" + img_name))\nplt.imshow(img)\nplt.show()","defc2b18":"img_name = np.argmin(train_df.groupby('ImageId').mean()['Height'])\nprint('minimum height picture \\nHeight : {} \\nWidth : {}'.format(train_df.loc[train_df['ImageId'] == img_name, 'Height'].mean(), train_df.loc[train_df['ImageId'] == img_name, 'Width'].mean()))\nimg = np.asarray(Image.open(path + \"train\/\" + img_name))\nplt.imshow(img)\nplt.show()","a47bb57d":"img_name = np.argmax(train_df.groupby('ImageId').mean()['Height'])\nprint('maximum height picture \\nHeight : {} \\nWidth : {}'.format(train_df.loc[train_df['ImageId'] == img_name, 'Height'].mean(), train_df.loc[train_df['ImageId'] == img_name, 'Width'].mean()))\nimg = np.asarray(Image.open(path + \"train\/\" + img_name))\nplt.imshow(img)\nplt.show()","a1921235":"img_name = np.argmin(train_df.groupby('ImageId').mean()['Width'])\nprint('minimum width picture \\nHeight : {} \\nWidth : {}'.format(train_df.loc[train_df['ImageId'] == img_name, 'Height'].mean(), train_df.loc[train_df['ImageId'] == img_name, 'Width'].mean()))\nimg = np.asarray(Image.open(path + \"train\/\" + img_name))\nplt.imshow(img)\nplt.show()","5dc7b869":"img_name = np.argmax(train_df.groupby('ImageId').mean()['Width'])\nprint('maximum width picture \\nHeight : {} \\nWidth : {}'.format(train_df.loc[train_df['ImageId'] == img_name, 'Height'].mean(), train_df.loc[train_df['ImageId'] == img_name, 'Width'].mean()))\nimg = np.asarray(Image.open(path + \"train\/\" + img_name))\nplt.imshow(img)\nplt.show()","92f00563":"# Image Data","4fa5cc39":"![](https:\/\/user-images.githubusercontent.com\/40379485\/56939165-c72a5f80-6b41-11e9-82e6-8e0c24495d14.png)\n\n\n# Fine-grained segmentation task for fashion and apparel\n\niMaterialist (Fashion) 2019 at FGVC6 with [dataset](https:\/\/www.kaggle.com\/c\/imaterialist-fashion-2019-FGVC6\/overview\/description)\n\nDesigners know what they are creating, but what, and how, do people really wear their products? What combinations of products are people using? In this competition, we challenge you to develop algorithms that will help with an important step towards automatic product detection \u2013 to accurately assign segmentations and attribute labels for fashion images.\n\nVisual analysis of clothing is a topic that has received increasing attention in recent years. Being able to recognize apparel products and associated attributes from pictures could enhance the shopping experience for consumers, and increase work efficiency for fashion professionals.\n\nWe present a new clothing dataset with the goal of introducing a novel fine-grained segmentation task by joining forces between the fashion and computer vision communities. The proposed task unifies both categorization and segmentation of rich and complete apparel attributes, an important step toward real-world applications.\n\n\n"}}