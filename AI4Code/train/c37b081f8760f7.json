{"cell_type":{"da49f73f":"code","254f8727":"code","33875b94":"code","c2d1fa7c":"code","8f766477":"code","90f21a76":"code","240d65e8":"code","0a012a74":"code","5fc8577a":"code","fd67d956":"code","1b14f7c2":"code","8fafb7c7":"code","bed2ecb2":"markdown","68c0c8d7":"markdown","cb183c70":"markdown","0d4a1dd1":"markdown","ead3de13":"markdown","b3991980":"markdown","0253a7f6":"markdown","fad515fb":"markdown","fb60a0bf":"markdown","e78ffc46":"markdown","c51e21ce":"markdown","1e536170":"markdown"},"source":{"da49f73f":"import json\nimport re\nimport unidecode\nimport numpy as np\nimport pandas as pd\nfrom collections import defaultdict\nfrom nltk.stem import WordNetLemmatizer\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.linear_model import LogisticRegression, SGDClassifier\nfrom sklearn.metrics import accuracy_score, classification_report\nfrom sklearn.model_selection import cross_validate\nfrom sklearn.multiclass import OneVsRestClassifier\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.pipeline import make_pipeline, make_union\nfrom sklearn.preprocessing import FunctionTransformer, LabelEncoder\nfrom tqdm import tqdm\ntqdm.pandas()","254f8727":"train = pd.read_json('..\/input\/train.json')\ntest = pd.read_json('..\/input\/test.json')","33875b94":"train['num_ingredients'] = train['ingredients'].apply(len)\ntrain = train[train['num_ingredients'] > 1]","c2d1fa7c":"lemmatizer = WordNetLemmatizer()\ndef preprocess(ingredients):\n    ingredients_text = ' '.join(ingredients)\n    ingredients_text = ingredients_text.lower()\n    ingredients_text = ingredients_text.replace('-', ' ')\n    words = []\n    for word in ingredients_text.split():\n        if re.findall('[0-9]', word): continue\n        if len(word) <= 2: continue\n        if '\u2019' in word: continue\n        word = lemmatizer.lemmatize(word)\n        if len(word) > 0: words.append(word)\n    return ' '.join(words)\n\nfor ingredient, expected in [\n    ('Eggs', 'egg'),\n    ('all-purpose flour', 'all purpose flour'),\n    ('pur\u00e9e', 'pur\u00e9e'),\n    ('1% low-fat milk', 'low fat milk'),\n    ('half & half', 'half half'),\n    ('safetida (powder)', 'safetida (powder)')\n]:\n    actual = preprocess([ingredient])\n    assert actual == expected, f'\"{expected}\" is excpected but got \"{actual}\"'","8f766477":"train['x'] = train['ingredients'].progress_apply(preprocess)\ntest['x'] = test['ingredients'].progress_apply(preprocess)\ntrain.head()","90f21a76":"vectorizer = make_pipeline(\n    TfidfVectorizer(sublinear_tf=True),\n    FunctionTransformer(lambda x: x.astype('float16'), validate=False)\n)\n\nx_train = vectorizer.fit_transform(train['x'].values)\nx_train.sort_indices()\nx_test = vectorizer.transform(test['x'].values)","240d65e8":"label_encoder = LabelEncoder()\ny_train = label_encoder.fit_transform(train['cuisine'].values)\ndict(zip(label_encoder.classes_, label_encoder.transform(label_encoder.classes_)))","0a012a74":"estimator = SVC(\n    C=50,\n    kernel='rbf',\n    gamma=1.4,\n    coef0=1,\n    cache_size=500,\n)\nclassifier = OneVsRestClassifier(estimator, n_jobs=-1)","5fc8577a":"%%time\nscores = cross_validate(classifier, x_train, y_train, cv=3)\nscores['test_score'].mean()","fd67d956":"%%time\nclassifier.fit(x_train, y_train)","1b14f7c2":"y_pred = label_encoder.inverse_transform(classifier.predict(x_train))\ny_true = label_encoder.inverse_transform(y_train)\n\nprint(f'accuracy score on train data: {accuracy_score(y_true, y_pred)}')\n\ndef report2dict(cr):\n    rows = []\n    for row in cr.split(\"\\n\"):\n        parsed_row = [x for x in row.split(\"  \") if len(x) > 0]\n        if len(parsed_row) > 0: rows.append(parsed_row)\n    measures = rows[0]\n    classes = defaultdict(dict)\n    for row in rows[1:]:\n        class_label = row[0]\n        for j, m in enumerate(measures):\n            classes[class_label][m.strip()] = float(row[j + 1].strip())\n    return classes\nreport = classification_report(y_true, y_pred)\npd.DataFrame(report2dict(report)).T","8fafb7c7":"y_pred = label_encoder.inverse_transform(classifier.predict(x_test))\ntest['cuisine'] = y_pred\ntest[['id', 'cuisine']].to_csv('submission.csv', index=False)\ntest[['id', 'cuisine']].head()","bed2ecb2":"## 6. Train model\n\nIf I become to be confident in the model, I train it with the whole train data for submission.","68c0c8d7":"## 1. Load dataset","cb183c70":"That's it! Don't trust what I've done here. The score can be better. Please let me know if you find a better approach.","0d4a1dd1":"I need to tune the parameters of TfidfVectorizer later.","ead3de13":"## 7. Check predicted values\n\nCheck if the model fitted enough.","b3991980":"Encode cuisines to numeric values using LabelEncoder.","0253a7f6":"## 6. Make submission\n\nIt seems to be working well. Let's make a submission.","fad515fb":"## 3. Preprocess\n\nCurrently, the preprocess is like below.\n\n- convert to lowercase\n- remove hyphen\n- remove numbers\n- remove words which consist of less than 2 characters\n- lemmatize\n\nThis process can be better.","fb60a0bf":"## 2. Remove outliers\n\nI saw weird recipes in the dataset .\n\n- water => Japanese\n- butter => Indian\n- butter => French\n\nLet's filter such single-ingredient recipes and see how it goes.","e78ffc46":"## 5. Check local CV\n\nTRUST YOUR LOCAL CV. TRUST YOUR LOCAL CV. TRUST YOUR LOCAL CV. I repeated 3 times since this is the most important thing.\n\nTry different prprocesses and parameters while looking at the local CV.","c51e21ce":"# Let's cook model\n\nLet's combine what we've found so far.\n\n- [What are ingredients?](https:\/\/www.kaggle.com\/rejasupotaro\/what-are-ingredients) (Preprocessing & Feature extraction)\n- [Representations for ingredients](https:\/\/www.kaggle.com\/rejasupotaro\/representations-for-ingredients)\n\nSteps are below.\n\n1. Load dataset\n2. Remove outliers\n3. Preprocess\n4. Create model\n5. Check local CV\n6. Train model\n7. Check predicted values\n8. Make submission","1e536170":"## 4. Create model\n\nI've tried LogisticRegression, GaussianProcessClassifier, GradientBoostingClassifier, MLPClassifier, LGBMClassifier, SGDClassifier, Keras but SVC works better so far.\n\nI need to take a look at models and the parameters more closely."}}