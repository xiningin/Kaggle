{"cell_type":{"d7bfb334":"code","75e13fbc":"code","43353043":"code","c95a63fe":"code","c29a3aa5":"code","94b4f83a":"code","0d7a28d5":"code","09f232d9":"code","192a6f77":"code","d7703065":"code","b716550b":"code","e7a09846":"code","6435afe5":"code","9a605790":"code","32269fa8":"code","467a949f":"markdown","9e55c042":"markdown","a56f19a5":"markdown","1d85e0d0":"markdown","d025f5bb":"markdown","f8860086":"markdown","11bfa8fa":"markdown","d8eb6ba7":"markdown","6693a5de":"markdown","9d1124a7":"markdown","6e529114":"markdown","18a6bd25":"markdown"},"source":{"d7bfb334":"#import necessary modules\n\nimport os\nimport gc\nimport pandas as pd\nimport numpy as np\nimport lightgbm as lgbm\nimport featuretools as ft\nfrom sklearn.preprocessing import LabelEncoder\nimport warnings\nwarnings.filterwarnings('ignore')","75e13fbc":"# Please note that you could have read it with simple read_csv, without using os (operating system commands...)\nsample_size = 30000 \n\"\"\" Load and process inputs \"\"\"\ninput_dir = os.path.join(os.pardir, 'input')\nprint('Input files:\\n{}'.format(os.listdir(input_dir)))\nprint('Loading data sets...')\napp_train_df = pd.read_csv(os.path.join(input_dir,'application_train.csv'), nrows=sample_size)\napp_test_df = pd.read_csv(os.path.join(input_dir,'application_test.csv'))\nprev_app_df = pd.read_csv(os.path.join(input_dir,'previous_application.csv'), nrows=sample_size)\nbureau_df = pd.read_csv(os.path.join(input_dir,'bureau.csv'), nrows=sample_size)\nbureau_balance_df = pd.read_csv(os.path.join(input_dir,'bureau_balance.csv'), nrows=sample_size)\ninstallments_df = pd.read_csv(os.path.join(input_dir,'installments_payments.csv'), nrows=sample_size)\ncc_balance_df = pd.read_csv(os.path.join(input_dir,'credit_card_balance.csv'), nrows=sample_size)\npos_balance_df = pd.read_csv(os.path.join(input_dir,'POS_CASH_balance.csv'), nrows=sample_size)\n\nprint('Data loaded.\\nMain application training data set shape = {}'.format(app_train_df.shape))\nprint('Main application test data set shape = {}'.format(app_test_df.shape))\nprint('Positive target proportion = {:.2f}'.format(app_train_df['TARGET'].mean()))","43353043":"# Merge the datasets into a single one for training\napp_both = pd.concat([app_train_df, app_test_df])","c95a63fe":"# A lot of the continuous days variables have integers as missing value indicators.\nprev_app_df['DAYS_LAST_DUE'].replace(365243, np.nan, inplace=True)\nprev_app_df['DAYS_TERMINATION'].replace(365243, np.nan, inplace=True)\nprev_app_df['DAYS_FIRST_DRAWING'].replace(365243, np.nan, inplace=True)\nprev_app_df['DAYS_FIRST_DUE'].replace(365243, np.nan, inplace=True)\nprev_app_df['DAYS_LAST_DUE_1ST_VERSION'].replace(365243, np.nan, inplace=True)","c29a3aa5":"#Add new features\n# Amount loaned relative to salary\napp_both['LOAN_INCOME_RATIO'] = app_both['AMT_CREDIT'] \/ app_both['AMT_INCOME_TOTAL']\napp_both['ANNUITY_INCOME_RATIO'] = app_both['AMT_ANNUITY'] \/ app_both['AMT_INCOME_TOTAL']\n    \n# Number of overall payments (I think!)\napp_both['ANNUITY LENGTH'] = app_both['AMT_CREDIT'] \/ app_both['AMT_ANNUITY']\n    \n# Social features\napp_both['WORKING_LIFE_RATIO'] = app_both['DAYS_EMPLOYED'] \/ app_both['DAYS_BIRTH']\napp_both['INCOME_PER_FAM'] = app_both['AMT_INCOME_TOTAL'] \/ app_both['CNT_FAM_MEMBERS']\napp_both['CHILDREN_RATIO'] = app_both['CNT_CHILDREN'] \/ app_both['CNT_FAM_MEMBERS']","94b4f83a":"# Create new entityset\nes = ft.EntitySet(id='home_credit_default_risk')\n\n\n# Create an entity from the applications (app_both) dataframe\n# This dataframe already has an index\nes = es.entity_from_dataframe(entity_id='applications',\n                              \n                              dataframe=app_both, index='SK_ID_CURR')\n\n\n# Create an entity from the bureau dataframe\n# This dataframe already has an index\nes = es.entity_from_dataframe(entity_id='bureau', \n                            \n                              dataframe=bureau_df, index='SK_ID_BUREAU')\n\n# Create an entity from the bureau balance dataframe\nes = es.entity_from_dataframe(entity_id='bureau_balance', \n                             \n                              make_index = True,\n                              dataframe=bureau_balance_df, index='bureau_balance_id')\n\n# Create an entity from the installments dataframe\nes = es.entity_from_dataframe(entity_id='installments',\n                              make_index = True,\n                              dataframe=installments_df, index='installment_id')\n\n\n\n# Create an entity from the previous applications dataframe\nes = es.entity_from_dataframe(entity_id='previous_application',\n                             \n                              make_index = True,\n                              dataframe=prev_app_df, index='prev_app_id')\n\n# Create an entity from the credit card balance dataframe\nes = es.entity_from_dataframe(entity_id='cc_balance',\n                         \n                              make_index = True,\n                              dataframe=cc_balance_df, index='cc_balance_id')\n\n# Create an entity from the POS Cash balance dataframe\nes = es.entity_from_dataframe(entity_id='pos_balance',\n\n                              make_index = True,\n                              dataframe=pos_balance_df, index='pos_balance_id')","0d7a28d5":"# Relationship between applications and credits bureau\nr_applications_bureau = ft.Relationship(es['applications']['SK_ID_CURR'],\n                                    es['bureau']['SK_ID_CURR'])\nes = es.add_relationship(r_applications_bureau)\n\n# Relationship between applications and credits bureau\nr_applications_installment = ft.Relationship(es['applications']['SK_ID_CURR'],\n                                    es['installments']['SK_ID_CURR'])\nes = es.add_relationship(r_applications_installment)\n\n# Relationship between applications and credits bureau\nr_bureau_bureaubalance = ft.Relationship(es['bureau']['SK_ID_BUREAU'],\n                                    es['bureau_balance']['SK_ID_BUREAU'])\nes = es.add_relationship(r_bureau_bureaubalance)\n\n# Relationship between applications and previous applications\nr_applications_prev_apps = ft.Relationship(es['applications']['SK_ID_CURR'],\n                                    es['previous_application']['SK_ID_CURR'])\nes = es.add_relationship(r_applications_prev_apps)\n\n# Relationship between applications and credit card balance\nr_applications_cc_balance = ft.Relationship(es['applications']['SK_ID_CURR'],\n                                    es['cc_balance']['SK_ID_CURR'])\nes = es.add_relationship(r_applications_cc_balance)\n\n# Relationship between applications and POS cash balance\nr_applications_pos_balance = ft.Relationship(es['applications']['SK_ID_CURR'],\n                                    es['pos_balance']['SK_ID_CURR'])\nes = es.add_relationship(r_applications_pos_balance)\n\nprint(es)","09f232d9":"\"\"\"\nDeep Feature Synthesis (DFS) is an automated method for performing feature engineering on relational and transactional data.\nhttps:\/\/docs.featuretools.com\/automated_feature_engineering\/afe.html\n\"\"\"\n# Create new features using specified primitives\nfeature_matrix, feature_defs = ft.dfs(entityset = es, target_entity = 'applications',\n                                      drop_contains=['SK_ID_PREV'], max_depth=2, verbose=True)","192a6f77":"feature_matrix.head(5)","d7703065":"def process_dataframe(input_df, encoder_dict=None):\n    \"\"\" Process a dataframe into a form useable by LightGBM \"\"\"\n\n    # Label encode categoricals\n    print('Label encoding categorical features...')\n    categorical_feats = input_df.columns[input_df.dtypes == 'object']\n    for feat in categorical_feats:\n        encoder = LabelEncoder()\n        input_df[feat] = encoder.fit_transform(input_df[feat].fillna('NULL'))\n    print('Label encoding complete.')\n\n    return input_df, categorical_feats.tolist(), encoder_dict","b716550b":"feature_matrix_enc, categorical_feats, encoder_dict = process_dataframe(input_df=feature_matrix)","e7a09846":"all_data_na = (feature_matrix_enc.isnull().sum() \/ len(feature_matrix_enc)) * 100\nall_data_na = all_data_na.drop(all_data_na[all_data_na == 0].index).sort_values(ascending=False)[:30]\nmissing_data = pd.DataFrame({'Missing Ratio' :all_data_na})\nmissing_data.head(20)","6435afe5":"# Separate into train and test\ntrain_df = feature_matrix_enc[feature_matrix_enc['TARGET'].notnull()].copy()\n\ntest_df = feature_matrix_enc[feature_matrix_enc['TARGET'].isnull()].copy()\ntest_df.drop(['TARGET'], axis=1, inplace=True)\n\ndel feature_matrix, feature_defs, feature_matrix_enc\ngc.collect()","9a605790":"\"\"\" Train the model \"\"\"\ntarget = train_df.pop('TARGET')\n\nlgbm_train = lgbm.Dataset(data=train_df,\n                          label=target,\n                          categorical_feature=categorical_feats,\n                          free_raw_data=False)\nlgbm_params = {\n    'boosting': 'dart',\n    'application': 'binary',\n    'learning_rate': 0.1,\n    'min_data_in_leaf': 30,\n    'num_leaves': 31,\n    'max_depth': -1,\n    'feature_fraction': 0.5,\n    'scale_pos_weight': 2,\n    'drop_rate': 0.02\n}\n\ncv_results = lgbm.cv(train_set=lgbm_train,\n                     params=lgbm_params,\n                     nfold=5,\n                     num_boost_round=600,\n                     early_stopping_rounds=50,\n                     verbose_eval=20,\n                     metrics=['auc'])\n\noptimum_boost_rounds = np.argmax(cv_results['auc-mean'])\nprint('Optimum boost rounds = {}'.format(optimum_boost_rounds))\nprint('Best CV result = {}'.format(np.max(cv_results['auc-mean'])))\n\nclf = lgbm.train(train_set=lgbm_train,\n                 params=lgbm_params,\n                 num_boost_round=optimum_boost_rounds)\n\n\"\"\" Predict on test set and create submission \"\"\"\ny_pred = clf.predict(test_df)","32269fa8":"out_df = pd.DataFrame({'SK_ID_CURR': test_df.index, 'TARGET': y_pred})\nout_df.to_csv('submission.csv', index=False)","467a949f":"**Label encoding** Making it machine readable","9e55c042":"**NaN imputation** will be skipped in this tutorial.","a56f19a5":"Let us split the variables one more time.","1d85e0d0":"Load in the data, NOTE: datasets are huge, working on them will be computationally costly. In order to avoid it we can introduce some limited sample size.","d025f5bb":"**Feature primitives** Basically which functions are we going to use to create features. Since we did not specify it we will be using standard ones (check doc) There is a option to define own ones or to just select some of the standards.","f8860086":"**Want to find out how to do the feature enigeering step automatically in a concise and compact tutorial, applying it on a binary classification problem using lightGBM?---look no further **\n\nAfter finding out that data scientists created a tool that \"replaces\" data-scientists I had to try it out. Thank you [https:\/\/docs.featuretools.com\/](http:\/\/)! Feature-engineering is tiresome, and takes the biggest amount of time do it. What if we can make it a one liner. Well now it seems we can. Even more appropriately we will be working on Home Credit Default Risk. A set of datasets where all of them are in a relationship with one-another and from all of them some information should be extracted. Featuretools makes it easy! Our goal in the end is simple. Predict whether the customer will default or not.","11bfa8fa":"***Featuretools*** is an open-source Python library for automatically creating features out of a set of related tables using a technique called deep feature synthesis. Automated feature engineering, like many topics in machine learning, is a complex subject built upon a foundation of simpler ideas. By going through these ideas one at a time, we can build up our understanding of how featuretools which will later allow for us to get the most out of it.\n\nThere are a few concepts that we will cover along the way:\n\n1.  Entities and EntitySets\n2. Relationships between tables\n3. Feature primitives: aggregations and transformations\n4. Deep feature synthesis","d8eb6ba7":"**NOTE** This NaN handling is just for the sake of it. It is by no-means complete and there are lot of them underneath (function is built that shows us percentage). But there is a specific way that GBM (light and xBGM) handle missing values. So even tough it would be better we want to focus on algortihm and automatic feature engineering!","6693a5de":"**NOTE** Even tough it is automatic, we can incorporate some manual features. IF we know some domain specific information.","9d1124a7":"**2. Relationships betweeen the sets**","6e529114":"If we merge datasets now we can perofrm neccesary operations and seperate them later.","18a6bd25":"**Train** the model, predict, etc."}}