{"cell_type":{"eb0cf08a":"code","d2f1da45":"code","ff2e4164":"code","7945f02d":"code","0c2b80db":"code","1f2c0f2b":"code","5041eaa5":"code","0eaa4808":"code","571916fc":"code","4c4b4807":"code","81e76581":"code","6c3e26f4":"code","9f0e6741":"code","c7bb35b3":"markdown","77b19678":"markdown","718c23bf":"markdown","a1367e6a":"markdown","1c6ad52e":"markdown","d9a61ddc":"markdown","bf6e436f":"markdown"},"source":{"eb0cf08a":"import numpy as np\nimport os\nimport tensorflow as tf\nfrom tensorflow import keras\nimport pathlib\nimport pandas as pd\nimport matplotlib.pyplot as plt","d2f1da45":"data_dir = pathlib.Path(\"..\/input\/meat-quality-assessment-based-on-deep-learning\/\")\npaths = list(data_dir.glob('*\/*.jpg'))\ndef load_paths(path):\n    path = path.absolute().as_posix()\n    if \"Fresh\" in path:return str(path),1\n    else:return path,0\nds = pd.DataFrame(map(load_paths,paths),columns =['Path','Fresh']).sample(frac=1).reset_index(drop=True)\n\n# Keeping Out 16 images to test Later\nds = ds[:-16]\ntest = ds[-16:]","ff2e4164":"ds.head()","7945f02d":"ds['Fresh'].value_counts().plot.bar()","0c2b80db":"def load_img(path,fresh):\n    img = tf.io.decode_jpeg(tf.io.read_file(path),channels=3)\n    img = tf.cast(img, tf.float32)\n    img = tf.image.resize(img, [224,224])\n    img = keras.applications.vgg19.preprocess_input(img)\n    return img,fresh\n\ndef load_test_img(path):\n    img = tf.io.decode_jpeg(tf.io.read_file(path),channels=3)\n    img = tf.cast(img, tf.float32)\n    img = tf.image.resize(img, [224,224])\n    img = keras.applications.vgg19.preprocess_input(img)\n    return img","1f2c0f2b":"def data_augment(image, label):\n    p_spatial = tf.random.uniform([], 0, 1.0, dtype=tf.float32)\n    p_rotate = tf.random.uniform([], 0, 1.0, dtype=tf.float32)\n    p_pixel_1 = tf.random.uniform([], 0, 1.0, dtype=tf.float32)\n    p_pixel_2 = tf.random.uniform([], 0, 1.0, dtype=tf.float32)\n    p_pixel_3 = tf.random.uniform([], 0, 1.0, dtype=tf.float32)\n    p_crop = tf.random.uniform([], 0, 1.0, dtype=tf.float32)\n    image = tf.image.random_flip_left_right(image)\n    image = tf.image.random_flip_up_down(image)\n    if p_spatial > .75:image = tf.image.transpose(image)\n    if p_rotate > .75:image = tf.image.rot90(image, k=3) \n    elif p_rotate > .5:image = tf.image.rot90(image, k=2) \n    elif p_rotate > .25:image = tf.image.rot90(image, k=1) \n\n    if p_pixel_1 >= .4:image = tf.image.random_saturation(image, lower=.7, upper=1.3)\n    if p_pixel_2 >= .4:image = tf.image.random_contrast(image, lower=.8, upper=1.2)\n    if p_pixel_3 >= .4:image = tf.image.random_brightness(image, max_delta=.1)\n\n    if p_crop > .7:\n        if p_crop > .9:image = tf.image.central_crop(image, central_fraction=.7)\n        elif p_crop > .8:image = tf.image.central_crop(image, central_fraction=.8)\n        else:image = tf.image.central_crop(image, central_fraction=.9)\n    elif p_crop > .4:\n        crop_size = tf.random.uniform([], int(224*.8),224, dtype=tf.int32)\n        image = tf.image.random_crop(image, size=[crop_size, crop_size, 3])\n    \n    image = tf.image.resize(image, [224,224])\n    return image,label","5041eaa5":"dataset = tf.data.Dataset.from_tensor_slices((ds.Path.values,ds.Fresh.values))","0eaa4808":"train_ds = dataset.take(int(0.8*len(ds)))\nval_ds = dataset.skip(int(0.2*len(ds)))\nAUTOTUNE = tf.data.AUTOTUNE\n\ntrain_ds = train_ds.map(load_img,num_parallel_calls=AUTOTUNE)\ntrain_ds = train_ds.repeat(4).map(data_augment,num_parallel_calls=AUTOTUNE)\ntrain_ds = train_ds.batch(32).prefetch(buffer_size=AUTOTUNE)\n\nval_ds = val_ds.map(load_img,num_parallel_calls=AUTOTUNE).batch(32).prefetch(buffer_size=AUTOTUNE)","571916fc":"base_model = keras.applications.VGG19(weights=\"imagenet\",include_top=False)\navg = keras.layers.GlobalAveragePooling2D()(base_model.output)\noutput = keras.layers.Dense(2, activation=\"softmax\")(avg)\nmodel = keras.Model(inputs=base_model.input, outputs=output)\n\nfor layer in base_model.layers:\n    layer.trainable = False","4c4b4807":"model.compile(loss=\"sparse_categorical_crossentropy\",optimizer=\"adam\",metrics=[\"accuracy\"])\nhistory = model.fit(train_ds, epochs=5, validation_data=val_ds,verbose=1)","81e76581":"model.save(\"model.h5\")","6c3e26f4":"acc = history.history['accuracy']\nval_acc = history.history['val_accuracy']\nloss = history.history['loss']\nval_loss = history.history['val_loss']\nepochs_range = range(len(history.history['val_loss']))\nplt.figure(figsize=(8, 8))\nplt.subplot(1, 2, 1)\nplt.plot(epochs_range, acc, label='Training Accuracy')\nplt.plot(epochs_range, val_acc, label='Validation Accuracy')\nplt.legend(loc='lower right')\nplt.title('Training and Validation Accuracy')\nplt.subplot(1, 2, 2)\nplt.plot(epochs_range, loss, label='Training Loss')\nplt.plot(epochs_range, val_loss, label='Validation Loss')\nplt.legend(loc='upper right')\nplt.title('Training and Validation Loss')\nplt.show()","9f0e6741":"test_ds = tf.data.Dataset.from_tensor_slices((test.Path.values))\ntest_ds = test_ds.map(load_test_img).batch(3)\nprediction = model.predict(test_ds)\n\nplt.figure(figsize=(15,15))\ni=0\nfor index,row in test.iterrows():\n    img = tf.io.decode_jpeg(tf.io.read_file(row['Path']),channels=3)\n    plt.subplot(4,4,i+1)\n    plt.imshow(img.numpy().astype(\"uint8\"),aspect='auto')\n    if prediction[i][0]>prediction[i][1]:\n        if row['Fresh']==0:plt.title(\"Not Fresh\", color=\"green\")\n        else:plt.title(\"Not Fresh\", color=\"red\")\n    else:\n        if row['Fresh']==0:plt.title(\"Fresh\", color=\"red\")\n        else:plt.title(\"Fresh\", color=\"green\")\n    plt.xticks([]), plt.yticks([])\n    i+=1    \n\nplt.show()    ","c7bb35b3":"# Test Cases\n\n**Below Image's titles show the value predicted by the Classifier<br>\nAnd the Font Color show the correctness of value<br>**\n* **Green** [Correct Prediction]<br>\n* **Red** [Incorrect Prediction]<br>","77b19678":"# Training","718c23bf":"# Define Model","a1367e6a":"# Import Data & Packages","1c6ad52e":"# Meat Quality Assesment \n\n### Contents\n#### 1. [Import Data & Packages](#Import-Data-&-Packages)\n#### 2. [Data Loading and Augmentation](#Data-Loading-and-Augmentation)\n#### 3. [Define Model](#Define-Model)\n#### 4. [Training](#Training)\n#### 5. [Test Cases](#Test-Cases)","d9a61ddc":"# Data Loading and Augmentation","bf6e436f":"## We can see that the model classifies all of the images correctly, with high confidence."}}