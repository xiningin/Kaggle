{"cell_type":{"2ff74be8":"code","4cbc5ca7":"code","783daea7":"code","6dd5c765":"code","567e8f60":"code","b39e9c35":"code","2d4a4cfe":"code","01bacefa":"code","bf6dab39":"code","74514b34":"code","220afbe4":"code","bdc646e4":"code","fc150d3d":"code","c1a3f768":"code","6002431d":"code","62ae91b5":"code","1a6ff852":"code","00a61f01":"code","a6c77335":"code","d5843268":"code","029000ef":"code","00af4d51":"code","570b5fb6":"code","68588cd2":"code","325211b2":"code","f7446f4c":"code","9ca5a34e":"code","721962e7":"code","a16febad":"code","4206f6cf":"code","77afc680":"code","b70d31b4":"code","faee3313":"code","973a4b9a":"code","61c9c281":"code","c7cc12ee":"code","e9cce389":"code","ed909eac":"code","bcdf1458":"code","2242cdef":"code","0c475611":"code","13c0cfbe":"code","4a7232f4":"code","a34b786f":"code","5545a6a5":"code","fcd732f5":"code","19be82f1":"code","b68d03b6":"code","c2c68542":"code","fd137c96":"markdown","da01efc2":"markdown","5483ba81":"markdown","2466449b":"markdown","375e09e1":"markdown","618de07a":"markdown","936a4b2b":"markdown","6eaee717":"markdown","b7e30d13":"markdown","c806e3da":"markdown","f55245ca":"markdown","faecb774":"markdown","796e16a9":"markdown"},"source":{"2ff74be8":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","4cbc5ca7":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\npd.set_option('display.max_rows', 100)\npd.set_option('display.max_columns', 500)\n\nfrom sklearn.model_selection import train_test_split,RandomizedSearchCV\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import StratifiedKFold\nimport xgboost as xgb\nimport lightgbm as lgb\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.model_selection import cross_val_score,cross_val_predict","783daea7":"train = pd.read_csv(\"..\/input\/cat-in-the-dat\/train.csv\")\ntest = pd.read_csv(\"..\/input\/cat-in-the-dat\/test.csv\")","6dd5c765":"train.shape, test.shape","567e8f60":"train.dtypes.value_counts()","b39e9c35":"test.dtypes.value_counts()","2d4a4cfe":"train.drop(\"id\",axis=1,inplace=True)\nSubmission = test[['id']]\ntest.drop('id',axis=1,inplace=True)","01bacefa":"train.head()","bf6dab39":"for col in train.columns:\n    if train[col].nunique() == 2:\n        print (\"Value Counts of {} Binary Variable:\\n\".format(col),train[col].value_counts())\n        print (\"---------------------------------------\")","74514b34":"cols = ['bin_0','bin_1','bin_2','bin_3','bin_4']\nfor ind,col in enumerate(train[cols]):\n    plt.figure(ind)\n    sns.countplot(x=col,data=train,hue='target')","220afbe4":"for col in ['bin_0','bin_1','bin_2','bin_3','bin_4']:\n    print (\"Value Count of {} Variable grouped by the target variable:\\n\".format(col),train.groupby(col)['target'].value_counts())","bdc646e4":"nominal_cols = [col for col in train.columns if col.startswith(\"nom\")]\ntrain[nominal_cols].head()","fc150d3d":"for col in nominal_cols:\n    print (\"Unique Values in {} Nominal Variable:\".format(col),train[col].nunique())\n    print (\"-------------------------------------------------------\")","c1a3f768":"cols = ['nom_0','nom_1','nom_2','nom_3','nom_4']\nfor ind,col in enumerate(train[cols]):\n    plt.figure(ind)\n    sns.countplot(x=col,data=train)","6002431d":"train.groupby('nom_0')['target'].value_counts().unstack().plot(kind='bar')\nplt.title(\"Distribution of Target variable by nom_0 variable\")\nplt.xlabel(\"nom_0 variable\")\nplt.ylabel(\"Count\")","62ae91b5":"train.groupby('nom_1')['target'].value_counts().unstack().plot(kind='bar')\nplt.title(\"Distribution of Target variable by nom_1 variable\")\nplt.xlabel(\"nom_1 variable\")\nplt.ylabel(\"Count\")","1a6ff852":"train.groupby('nom_2')['target'].value_counts().unstack().plot(kind='bar')\nplt.title(\"Distribution of Target variable by nom_2 variable\")\nplt.xlabel(\"nom_2 variable\")\nplt.ylabel(\"Count\")","00a61f01":"train.groupby('nom_3')['target'].value_counts().unstack().plot(kind='bar')\nplt.title(\"Distribution of Target variable by nom_3 variable\")\nplt.xlabel(\"nom_3 variable\")\nplt.ylabel(\"Count\")","a6c77335":"train.groupby('nom_4')['target'].value_counts().unstack().plot(kind='bar')\nplt.title(\"Distribution of Target variable by nom_4 variable\")\nplt.xlabel(\"nom_4 variable\")\nplt.ylabel(\"Count\")","d5843268":"high_card_nominal = [col for col in train.columns if((col.startswith(\"nom\"))&(train[col].nunique()>10))]\ntrain[high_card_nominal].head()","029000ef":"for col in high_card_nominal:\n    print (\"Cardinality of {} nominal variable is:\".format(col),train[col].nunique())","00af4d51":"for col in high_card_nominal:\n    print (\"Value Count of Top 5 values in the {} nominal variable:\".format(col),train[col].value_counts().sort_values(ascending=False).head())\n    print (\"---------------------------------------------------------------------------------\")","570b5fb6":"ordinal_cols = [col for col in train.columns if col.startswith(\"ord\")]\ntrain[ordinal_cols].head()","68588cd2":"for col in ordinal_cols:\n    print (\"Number of Unique values in {} Ordinal Variable are:\".format(col),train[col].nunique())","325211b2":"for col in ordinal_cols:\n    print (\"Value Count of Top 5 values in the {} ordinal variable:\\n\".format(col),train[col].value_counts().sort_values(ascending=False).head())\n    print (\"---------------------------------------------------------------------------------\")","f7446f4c":"train.groupby('ord_0')['target'].value_counts().unstack().plot(kind='bar')\nplt.title(\"Distribution of Target Variable by ord_0 variable\")\nplt.xlabel(\"ord_0 variable\")\nplt.ylabel(\"Counts\")","9ca5a34e":"train.groupby('ord_1')['target'].value_counts().sort_values().unstack().plot(kind='bar')\nplt.title(\"Distribution of Target Variable by ord_1 variable\")\nplt.xlabel(\"ord_1 variable\")\nplt.ylabel(\"Counts\")","721962e7":"train.groupby('ord_2')['target'].value_counts().sort_values().unstack().plot(kind='bar')\nplt.title(\"Distribution of Target Variable by ord_2 variable\")\nplt.xlabel(\"ord_2 variable\")\nplt.ylabel(\"Counts\")","a16febad":"train.groupby('ord_3')['target'].value_counts().sort_values().unstack().plot(kind='bar')\nplt.title(\"Distribution of Target Variable by ord_3 variable\")\nplt.xlabel(\"ord_3 variable\")\nplt.ylabel(\"Counts\")","4206f6cf":"train.groupby('ord_4')['target'].value_counts().unstack().plot(kind='bar',figsize=(10,6))\nplt.title(\"Distribution of Target Variable by ord_4 variable\")\nplt.xlabel(\"ord_4 variable\")\nplt.ylabel(\"Counts\")","77afc680":"train[['day','month']].head()","b70d31b4":"for col in ['day','month']:\n    print (\"Unique Values in {} variable are:\\n\".format(col),train[col].nunique())","faee3313":"train.groupby('day')['target'].value_counts().sort_values().unstack().plot(kind='bar')\nplt.title(\"Distribution of Target Variable by Day\")\nplt.xlabel(\"Day of the Week\")\nplt.ylabel(\"Count\")","973a4b9a":"train.groupby('month')['target'].value_counts().sort_values().unstack().plot(kind='bar')\nplt.title(\"Distribution of Target Variable by Month\")\nplt.xlabel(\"Month\")\nplt.ylabel(\"Count\")","61c9c281":"# Converting bin_3 and bin_4 variables in the form of 0's and 1's\nmapping = {\"T\":1,\"F\":0,\"Y\":1,\"N\":0}\ntrain['bin_4'] = train['bin_4'].map(mapping)\ntrain['bin_3'] = train['bin_3'].map(mapping)\n\n# Converting ordinal columns ord_0,ord_1,ord_2,ord_3,ord_4 to category data type with the assumed ordering\ntrain['ord_0'] = train['ord_0'].astype('category')\ntrain['ord_0'] = train['ord_0'].cat.set_categories([1,2,3],ordered=True)\ntrain['ord_0'] = train['ord_0'].cat.codes\n\ntrain['ord_1'] = train['ord_1'].astype('category')\ntrain['ord_1'] = train['ord_1'].cat.set_categories([\"Novice\",\"Contributor\",\"Expert\",\"Master\",\"Grandmaster\"],ordered=True)\ntrain['ord_1'] = train['ord_1'].cat.codes\n\n\ntrain['ord_2'] = train['ord_2'].astype('category')\ntrain['ord_2'] = train['ord_2'].cat.set_categories([\"Freezing\",\"Cold\",\"Warm\",\"Hot\",\"Boiling Hot\",\"Lava Hot\"],ordered=True)\ntrain['ord_2'] = train['ord_2'].cat.codes\n\ntrain['ord_3'] = train['ord_3'].astype('category')\ntrain['ord_3'] = train['ord_3'].cat.set_categories([\"a\",\"b\",\"c\",\"d\",\"e\",\"f\",\"g\",\"h\",\"i\",\"j\",\"k\",\"l\",\"m\",\"n\",\"o\"],ordered=True)\ntrain['ord_3'] = train['ord_3'].cat.codes\n\ntrain['ord_4'] = train['ord_4'].astype('category')\ntrain['ord_4'] = train['ord_4'].cat.set_categories([\"A\",\"B\",\"C\",\"D\",\"E\",\"F\",\"G\",\"H\",\"I\",\"J\",\"K\",\"L\",\"M\",\"N\",\"O\",\"P\",\"Q\",\"R\",\"S\",\"T\",\"U\",\"V\",\"W\",\"X\",\"Y\",\"Z\"],ordered=True)\ntrain['ord_4'] = train['ord_4'].cat.codes\n\ntrain['ord_5'] = train['ord_5'].astype('category')\ntrain['ord_5'] = train['ord_5'].cat.codes\n\n# Converting day and month variables to category data type\ntrain['day'] = train['day'].astype('category')\ntrain['day'] = train['day'].cat.codes\ntrain['month'] = train['month'].astype('category')\ntrain['month'] = train['month'].cat.codes\n\n# Dummy encoding\nnom_0_dummy = pd.get_dummies(train['nom_0'],prefix=\"nom_0\",)\ntrain = pd.concat([train,nom_0_dummy],axis=1)\ntrain.drop(\"nom_0\",axis=1,inplace=True)\n\nnom_1_dummy = pd.get_dummies(train['nom_1'],prefix=\"nom_1\")\ntrain = pd.concat([train,nom_1_dummy],axis=1)\ntrain.drop(\"nom_1\",axis=1,inplace=True)\n\nnom_2_dummy = pd.get_dummies(train['nom_2'],prefix=\"nom_2\")\ntrain = pd.concat([train,nom_2_dummy],axis=1)\ntrain.drop(\"nom_2\",axis=1,inplace=True)\n\nnom_3_dummy = pd.get_dummies(train['nom_3'],prefix=\"nom_3\")\ntrain = pd.concat([train,nom_3_dummy],axis=1)\ntrain.drop(\"nom_3\",axis=1,inplace=True)\n\nnom_4_dummy = pd.get_dummies(train['nom_4'],prefix=\"nom_4\")\ntrain = pd.concat([train,nom_4_dummy],axis=1)\ntrain.drop(\"nom_4\",axis=1,inplace=True)\n\nday_dummy = pd.get_dummies(train['day'],prefix=\"day\")\ntrain = pd.concat([train,day_dummy],axis=1)\ntrain.drop(\"day\",axis=1,inplace=True)\n\nmonth_dummy = pd.get_dummies(train['month'],prefix=\"month\")\ntrain = pd.concat([train,month_dummy],axis=1)\ntrain.drop(\"month\",axis=1,inplace=True)\n\n\nnom_5_freq_encoding = train['nom_5'].value_counts().to_dict()\ntrain['nom_5_freq_encoding'] = train['nom_5'].map(nom_5_freq_encoding)\n\nnom_6_freq_encoding = train['nom_6'].value_counts().to_dict()\ntrain['nom_6_freq_encoding'] = train['nom_6'].map(nom_6_freq_encoding)\n\nnom_7_freq_encoding = train['nom_7'].value_counts().to_dict()\ntrain['nom_7_freq_encoding'] = train['nom_7'].map(nom_7_freq_encoding)\n\nnom_8_freq_encoding = train['nom_8'].value_counts().to_dict()\ntrain['nom_8_freq_encoding'] = train['nom_8'].map(nom_8_freq_encoding)\n\nnom_9_freq_encoding = train['nom_9'].value_counts().to_dict()\ntrain['nom_9_freq_encoding'] = train['nom_9'].map(nom_9_freq_encoding)\n\n\nnom_5_target_encoding = np.round(train.groupby('nom_5')['target'].mean(),decimals=2).to_dict()\ntrain['nom_5_target_encoding'] = train['nom_5'].map(nom_5_target_encoding)\n\nnom_6_target_encoding = np.round(train.groupby('nom_6')['target'].mean(),decimals=2).to_dict()\ntrain['nom_6_target_encoding'] = train['nom_6'].map(nom_6_target_encoding)\n\nnom_7_target_encoding = np.round(train.groupby('nom_7')['target'].mean(),decimals=2).to_dict()\ntrain['nom_7_target_encoding'] = train['nom_7'].map(nom_7_target_encoding)\n\nnom_8_target_encoding = np.round(train.groupby('nom_8')['target'].mean(),decimals=2).to_dict()\ntrain['nom_8_target_encoding'] = train['nom_8'].map(nom_8_target_encoding)\n\nnom_9_target_encoding = np.round(train.groupby('nom_9')['target'].mean(),decimals=2).to_dict()\ntrain['nom_9_target_encoding'] = train['nom_9'].map(nom_9_target_encoding)","c7cc12ee":"# Converting bin_3 and bin_4 variables in the form of 0's and 1's\nmapping = {\"T\":1,\"F\":0,\"Y\":1,\"N\":0}\ntest['bin_4'] = test['bin_4'].map(mapping)\ntest['bin_3'] = test['bin_3'].map(mapping)\n\n# Converting ordinal columns ord_0,ord_1,ord_2,ord_3,ord_4 to category data type with the assumed ordering\ntest['ord_0'] = test['ord_0'].astype('category')\ntest['ord_0'] = test['ord_0'].cat.set_categories([1,2,3],ordered=True)\ntest['ord_0'] = test['ord_0'].cat.codes\n\ntest['ord_1'] = test['ord_1'].astype('category')\ntest['ord_1'] = test['ord_1'].cat.set_categories([\"Novice\",\"Contributor\",\"Expert\",\"Master\",\"Grandmaster\"],ordered=True)\ntest['ord_1'] = test['ord_1'].cat.codes\n\n\ntest['ord_2'] = test['ord_2'].astype('category')\ntest['ord_2'] = test['ord_2'].cat.set_categories([\"Freezing\",\"Cold\",\"Warm\",\"Hot\",\"Boiling Hot\",\"Lava Hot\"],ordered=True)\ntest['ord_2'] = test['ord_2'].cat.codes\n\ntest['ord_3'] = test['ord_3'].astype('category')\ntest['ord_3'] = test['ord_3'].cat.set_categories([\"a\",\"b\",\"c\",\"d\",\"e\",\"f\",\"g\",\"h\",\"i\",\"j\",\"k\",\"l\",\"m\",\"n\",\"o\"],ordered=True)\ntest['ord_3'] = test['ord_3'].cat.codes\n\ntest['ord_4'] = test['ord_4'].astype('category')\ntest['ord_4'] = test['ord_4'].cat.set_categories([\"A\",\"B\",\"C\",\"D\",\"E\",\"F\",\"G\",\"H\",\"I\",\"J\",\"K\",\"L\",\"M\",\"N\",\"O\",\"P\",\"Q\",\"R\",\"S\",\"T\",\"U\",\"V\",\"W\",\"X\",\"Y\",\"Z\"],ordered=True)\ntest['ord_4'] = test['ord_4'].cat.codes\n\ntest['ord_5'] = test['ord_5'].astype('category')\ntest['ord_5'] = test['ord_5'].cat.codes\n\n# Converting day and month variables to category data type\ntest['day'] = test['day'].astype('category')\ntest['day'] = test['day'].cat.codes\ntest['month'] = test['month'].astype('category')\ntest['month'] = test['month'].cat.codes\n\n# Dummy encoding\nnom_0_dummy = pd.get_dummies(test['nom_0'],prefix=\"nom_0\",)\ntest = pd.concat([test,nom_0_dummy],axis=1)\ntest.drop(\"nom_0\",axis=1,inplace=True)\n\nnom_1_dummy = pd.get_dummies(test['nom_1'],prefix=\"nom_1\")\ntest = pd.concat([test,nom_1_dummy],axis=1)\ntest.drop(\"nom_1\",axis=1,inplace=True)\n\nnom_2_dummy = pd.get_dummies(test['nom_2'],prefix=\"nom_2\")\ntest = pd.concat([test,nom_2_dummy],axis=1)\ntest.drop(\"nom_2\",axis=1,inplace=True)\n\nnom_3_dummy = pd.get_dummies(test['nom_3'],prefix=\"nom_3\")\ntest = pd.concat([test,nom_3_dummy],axis=1)\ntest.drop(\"nom_3\",axis=1,inplace=True)\n\nnom_4_dummy = pd.get_dummies(test['nom_4'],prefix=\"nom_4\")\ntest = pd.concat([test,nom_4_dummy],axis=1)\ntest.drop(\"nom_4\",axis=1,inplace=True)\n\nday_dummy = pd.get_dummies(test['day'],prefix=\"day\")\ntest = pd.concat([test,day_dummy],axis=1)\ntest.drop(\"day\",axis=1,inplace=True)\n\nmonth_dummy = pd.get_dummies(test['month'],prefix=\"month\")\ntest = pd.concat([test,month_dummy],axis=1)\ntest.drop(\"month\",axis=1,inplace=True)\n\n\nnom_5_freq_encoding = test['nom_5'].value_counts().to_dict()\ntest['nom_5_freq_encoding'] = test['nom_5'].map(nom_5_freq_encoding)\n\nnom_6_freq_encoding = test['nom_6'].value_counts().to_dict()\ntest['nom_6_freq_encoding'] = test['nom_6'].map(nom_6_freq_encoding)\n\nnom_7_freq_encoding = test['nom_7'].value_counts().to_dict()\ntest['nom_7_freq_encoding'] = test['nom_7'].map(nom_7_freq_encoding)\n\nnom_8_freq_encoding = test['nom_8'].value_counts().to_dict()\ntest['nom_8_freq_encoding'] = test['nom_8'].map(nom_8_freq_encoding)\n\nnom_9_freq_encoding = test['nom_9'].value_counts().to_dict()\ntest['nom_9_freq_encoding'] = test['nom_9'].map(nom_9_freq_encoding)\n\n\ntest['nom_5_target_encoding'] = test['nom_5'].map(nom_5_target_encoding)\ntest['nom_6_target_encoding'] = test['nom_6'].map(nom_6_target_encoding)\ntest['nom_7_target_encoding'] = test['nom_7'].map(nom_7_target_encoding)\ntest['nom_8_target_encoding'] = test['nom_8'].map(nom_8_target_encoding)\ntest['nom_9_target_encoding'] = test['nom_9'].map(nom_9_target_encoding)","e9cce389":"train.drop(['nom_5','nom_6','nom_7','nom_8','nom_9'],axis=1,inplace=True)\ntest.drop(['nom_5','nom_6','nom_7','nom_8','nom_9'],axis=1,inplace=True)","ed909eac":"train.head()","bcdf1458":"test.head()","2242cdef":"X = train[[col for col in train.columns if col!='target']]\ny = train['target']","0c475611":"kf = StratifiedKFold(n_splits=5,shuffle=True,random_state=42)\nfor train_index,test_index in kf.split(X,y):\n    X_Train,X_Valid = X.loc[train_index],X.loc[test_index]\n    y_Train,y_Valid = y.loc[train_index],y.loc[test_index]\nprint (X_Train.shape)\nprint (y_Train.shape)\nprint (X_Valid.shape)\nprint (y_Valid.shape)","13c0cfbe":"clf_1 = lgb.LGBMClassifier(boosting_type='goss',objective='binary',random_state=42,n_jobs=-1,verbose=1,class_weight='balanced')\nparams = {\"max_depth\":[3,4,5,6,7,-1],\n          \"learning_rate\":[0.01,0.05,0.1,0.3],\n          \"subsample\":[0.5,0.6,0.7,0.8,0.9],\n          \"colsample_bytree\":[0.5,0.6,0.7,0.8,0.9],\n          \"reg_alpha\":[0.5,1,2,5,10],\n          \"reg_lambda\":[0.5,1,2,5,10],\n          \"num_leaves\":[7,15,31,63,127],\n          \"n_estimators\":list(range(50,500,50)),\n          \"min_data_in_leaf\":[1,3,5,10,15,25]}\nrandom_search_1 = RandomizedSearchCV(estimator=clf_1,param_distributions=params,cv=10,scoring='roc_auc')\nrandom_search_1.fit(X_Train,y_Train)","4a7232f4":"random_search_1.best_estimator_,random_search_1.best_score_,random_search_1.best_params_","a34b786f":"ser = pd.Series(random_search_1.best_estimator_.feature_importances_,X_Train.columns).sort_values()\nser.plot(kind='bar',figsize=(10,6))","5545a6a5":"lst = list(ser[ser>0].index)\nX_Train = X_Train[lst]\ntest = test[lst]","fcd732f5":"clf_1 = lgb.LGBMClassifier(boosting_type='goss',objective='binary',random_state=42,n_jobs=-1,verbose=1,class_weight='balanced')\nparams = {\"max_depth\":[3,4,5,6,7,-1],\n          \"learning_rate\":[0.01,0.05,0.1,0.3],\n          \"subsample\":[0.5,0.6,0.7,0.8,0.9],\n          \"colsample_bytree\":[0.5,0.6,0.7,0.8,0.9],\n          \"reg_alpha\":[0.5,1,2,5,10],\n          \"reg_lambda\":[0.5,1,2,5,10],\n          \"num_leaves\":[7,15,31,63,127],\n          \"n_estimators\":list(range(50,500,50)),\n          \"min_data_in_leaf\":[1,3,5,10,15,25]}\nrandom_search_1 = RandomizedSearchCV(estimator=clf_1,param_distributions=params,cv=10,scoring='roc_auc')\nrandom_search_1.fit(X_Train,y_Train)","19be82f1":"random_search_1.best_estimator_,random_search_1.best_score_,random_search_1.best_params_","b68d03b6":"\"\"\"clf_2 = xgb.XGBClassifier(random_state=42,n_jobs=-1,verbosity=1)\nparams = {\"max_depth\":[3,4,5,6,7,8,9],\n          \"n_estimators\":list(range(50,500,50)),\n          \"learning_rate\":[0.01,0.05,0.1,0.3],\n          \"subsample\":[0.5,0.6,0.7,0.8,0.9],\n          \"colsample_bytree\":[0.5,0.6,0.7,0.8,0.9],\n          \"reg_alpha\":[0.5,1,2,5,10],\n          \"reg_lambda\":[0.5,1,2,5,10]}\nrandom_search_2 = RandomizedSearchCV(estimator=clf_2,param_distributions=params,cv=10,scoring='roc_auc')\nrandom_search_2.fit(X_Train,y_Train)\"\"\"","c2c68542":"Submission['target']=random_search_1.predict_proba(test)[:,1]\nSubmission.to_csv(\"Latest.csv\",index=None)","fd137c96":"#### Time Dependent Variables","da01efc2":"1. nom_0,nom_1,nom_2,nom_3,nom_4 variables have fairly low cardinality.\n2. Other nominal variables have very high cardinality. ","5483ba81":"#### Binary Variables","2466449b":"#### Ordinal Variables","375e09e1":"1. Since the day variable has only 7 unique values, it can be safely assumed that it is referring to Day Of The Week (Monday to Sunday).\n2. month variable represents the number of Months in a year.","618de07a":"1. Out of the Nominal variables, there is no single value that dominates the variables.\n2. Nominal variables do not have high cardinality.","936a4b2b":"1. bin_0 to bin_4 and target variables are Binary in nature.\n2. Binary columns either have values 0\/1, T\/F meaning True\/False, Y\/N meaning Yes\/No.\n","6eaee717":"Lion has significantly higher proportion as compared to other values.","b7e30d13":"1. There are no missing values in the dataset as stated in the competition statement.\n2. Most of the columns are of object data type followed by integer.\n3. Target is the dependent variable","c806e3da":"1. For variable ord_0 we will assume the ordering is as follows, 0>1>2.\n2. For variable ord_1 we will assume the ordering as follows, Novice<Contributor<Expert<Master<Grandmaster (As per Kaggle's ordering :)).\n3. For variable ord_2 we will assume the ordering as Freezing<Cold<Warm<Hot<BoilingHot<LavaHot.\n4. For variable ord_3 we will have to assume ordering as per alphabets either we have to consider a is the least or the highest. This is applicable to ord_4 variable as well.\n5. Since ord_5 is a combination of alphabets, we will consider that it is already in an ordered state.","f55245ca":"Trapexoid dominates nom_1 variable to certain extent. ","faecb774":"#### High Cardinality Nominal Variables","796e16a9":"#### Nominal Variables"}}