{"cell_type":{"962797cf":"code","842167a1":"code","358d9ab0":"code","d06ef3c1":"code","832012d0":"code","8b7295b6":"code","25b73c18":"code","29e4460f":"code","16977b1d":"code","158bbbbd":"code","4a05312b":"code","2bbbc362":"code","236c4ffe":"code","d0d8126f":"code","deba5bef":"code","f7743784":"code","8cb821ff":"code","a5896297":"code","078eb960":"code","44de6c60":"code","8fdac33d":"code","594f0ecd":"code","8c9844bc":"code","8ae73d59":"code","2571650b":"code","c3aeae67":"code","c65e544c":"code","5fee4fd0":"code","59b78b3e":"code","5656295f":"code","1971f981":"code","f4a2b40d":"code","127a3420":"code","1a409119":"code","78fee2fc":"code","155d49d7":"code","071ba2e9":"code","c5eb2829":"code","0f12073e":"code","ed079ef7":"code","d5a72ddd":"code","ee31686d":"code","1b754c3c":"code","c1d8b223":"code","19dc8956":"code","b8d05c72":"code","3eb909dc":"code","568c40b9":"code","2dcc3eb7":"code","27082064":"code","e4f2784b":"code","b4afe455":"code","97f75f56":"code","9562f7ca":"code","1956079d":"code","3778a52b":"code","ba1abfe2":"code","6d10382d":"code","bd9f3409":"code","8b03ef59":"code","e065bd93":"code","6788b26a":"code","d1e19a8d":"code","e649ce90":"code","a75af365":"code","825ebf49":"code","a2535bb5":"code","5fe84c03":"code","ab34ba91":"code","e310fec2":"code","1a8a13ca":"code","a673b5dd":"code","87c4aa40":"code","ecb9fa9e":"code","35038924":"code","caf1574a":"code","2e83b0da":"code","1bf88dcc":"code","3d6d4f50":"code","a773c197":"code","fc63c5f2":"code","bea09554":"code","1edb3a37":"code","03994b22":"code","0bd55cd2":"code","bc7cfc38":"code","08dab792":"code","42ab6982":"code","7990c9b7":"code","0351039b":"code","54f263a1":"code","a0439b2c":"code","86978f22":"code","a68e2735":"code","e318e6f6":"code","85acf39b":"code","624e894c":"code","f5941e8e":"code","3ee1e0f1":"code","e5a1ccce":"code","73ca4bc2":"code","c316ac58":"code","70ea1bd7":"code","75e6de7e":"code","c1e9d9b9":"code","b931d1ba":"code","dcb9bf1a":"code","ece3bd5b":"code","add2b857":"code","57da9015":"markdown","c75b73e4":"markdown","4e4af300":"markdown","6a3fa359":"markdown","8c0ca573":"markdown","5a58c1d8":"markdown","601b5810":"markdown","222b7bf8":"markdown","dd44bf7c":"markdown","7ca750f9":"markdown","223c3227":"markdown","c7c475b6":"markdown","f88b3b88":"markdown","7ab86b2b":"markdown","0b6d2f59":"markdown","4557260a":"markdown","05731ec8":"markdown","58dd0dcf":"markdown","91177528":"markdown","509ae6d0":"markdown","ec18c369":"markdown","8adb5371":"markdown","9a0edb48":"markdown","edb42d84":"markdown","c6a235ab":"markdown","ef4ef062":"markdown","e66d8895":"markdown","e7d11339":"markdown","f5881b6c":"markdown","61551ccc":"markdown","9790ba18":"markdown","9d595164":"markdown","ccf739fb":"markdown","a5ce3e06":"markdown","d6267c09":"markdown","1042cd93":"markdown","e3237e44":"markdown","6ebef300":"markdown","ba0c7fd0":"markdown","b98d2a05":"markdown","23ad434c":"markdown","2d50d3d9":"markdown","3788b100":"markdown","301b02b7":"markdown","dafb85e5":"markdown","e12f8713":"markdown","52a9d523":"markdown","8c72271e":"markdown","33bb8b96":"markdown","55c1ab05":"markdown","ccd35be2":"markdown","70017178":"markdown","0945105d":"markdown","77a60d3d":"markdown","71e72996":"markdown","8b8acd1b":"markdown"},"source":{"962797cf":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport math\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport sklearn.preprocessing\nfrom sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score, RandomizedSearchCV, StratifiedShuffleSplit, StratifiedKFold\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, BaggingClassifier, VotingClassifier\nimport sklearn.ensemble\nfrom sklearn.metrics import accuracy_score, roc_auc_score\nimport xgboost as xgb\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","842167a1":"df_train = pd.read_csv(\"\/kaggle\/input\/titanic\/train.csv\")\ndf_test = pd.read_csv(\"\/kaggle\/input\/titanic\/test.csv\")","358d9ab0":"df_train.head()\n","d06ef3c1":"df_train.info()","832012d0":"df_train.describe()","8b7295b6":"df_train.describe(include=['O'])","25b73c18":"df_test.info()","29e4460f":"df_test.describe()","16977b1d":"df_test.describe(include='O')","158bbbbd":"total = df_train.Survived.count()\nsurvived = df_train.Survived[df_train.Survived == 1].count() \/ total * 100\nnon_survived = df_train.Survived[df_train.Survived == 0].count() \/ total * 100\nprint('% survivors = ' + str(survived))\nprint('% non-survivors = ' + str(non_survived))","4a05312b":"plt.bar( ['survivors', 'non-survivors'], [survived, non_survived])\n","2bbbc362":"df_train.columns","236c4ffe":"fig, axes = plt.subplots(nrows=1, ncols=5, figsize = (20,8))\ndf_train.Sex.value_counts().plot('bar', ax=axes[0], title='Sex')\ndf_train.Embarked.value_counts().plot('bar', ax=axes[1], title='Embarked')\ndf_train.Pclass.value_counts().plot('bar', ax=axes[2], title='Pclass')\ndf_train.SibSp.value_counts().plot('bar', ax=axes[3], title='SibSp')\ndf_train.Parch.value_counts().plot('bar', ax=axes[4], title='Parch')","d0d8126f":"fig, axes = plt.subplots(nrows=1, ncols=2, figsize = (20,8))\ndf_train.Age.plot('hist', bins=20, ax=axes[0], title='Age')\ndf_train.Fare.plot('hist', bins=20, ax=axes[1], title='Fare')\n","deba5bef":"df_train[['Sex', 'Survived']].groupby('Sex').mean()","f7743784":"df_train[['Embarked', 'Survived']].groupby('Embarked').mean()","8cb821ff":"df_train[['Pclass', 'Survived']].groupby('Pclass').mean()","a5896297":"df_train[['SibSp', 'Survived']].groupby('SibSp').mean().sort_values('Survived')","078eb960":"df_train[['Parch', 'Survived']].groupby('Parch').mean().sort_values('Survived')","44de6c60":"g = sns.FacetGrid(df_train, col=\"Survived\", height=4, aspect=2)\ng.map(plt.hist, \"Age\", bins=20)","8fdac33d":"plt.figure(figsize=(15,8))\ndf_train.Age[df_train.Survived == 1].plot('hist')\ndf_train.Age[df_train.Survived == 0].plot('hist', alpha=0.5)\nplt.legend(['survived', 'not survived'])\nplt.title('Age')","594f0ecd":"fig, axes = plt.subplots(nrows=1, ncols=2, figsize = (20,8))\ndf_train.Age[(df_train.Survived == 1) & (df_train.Sex == 'male')].plot('hist', ax=axes[0])\ndf_train.Age[(df_train.Survived == 0) & (df_train.Sex == 'male')].plot('hist', ax = axes[0], alpha=0.5, title='Age (Male)')\n\ndf_train.Age[(df_train.Survived == 1) & (df_train.Sex == 'female')].plot('hist', ax=axes[1])\ndf_train.Age[(df_train.Survived == 0) & (df_train.Sex == 'female')].plot('hist', ax = axes[1], alpha=0.5, title='Age (Female)')\n\nfig.legend(['survived', 'not survived'])","8c9844bc":"g = sns.FacetGrid(df_train, col=\"Survived\", height=4, aspect=2)\ng.map(plt.hist, \"Fare\", bins=20)","8ae73d59":"plt.figure(figsize=(15,8))\ndf_train.Fare[df_train.Survived == 1].plot('hist')\ndf_train.Fare[df_train.Survived == 0].plot('hist', alpha=0.5)\nplt.legend(['survived', 'not survived'])\nplt.title('Fare')\n","2571650b":"fig, axes = plt.subplots(nrows=1, ncols=3, figsize = (20,8))\ndf_train.Fare[(df_train.Survived == 1) & (df_train.Pclass == 1)].plot('hist', ax=axes[0])\ndf_train.Fare[(df_train.Survived == 0) & (df_train.Pclass == 1)].plot('hist', ax = axes[0], alpha=0.5, title='Fare (Pclass = 1)')\n\ndf_train.Fare[(df_train.Survived == 1) & (df_train.Pclass == 2)].plot('hist', ax=axes[1])\ndf_train.Fare[(df_train.Survived == 0) & (df_train.Pclass == 2)].plot('hist', ax = axes[1], alpha=0.5, title='Fare (Pclass = 2)')\n\ndf_train.Fare[(df_train.Survived == 1) & (df_train.Pclass == 3)].plot('hist', ax=axes[2])\ndf_train.Fare[(df_train.Survived == 0) & (df_train.Pclass == 3)].plot('hist', ax = axes[2], alpha=0.5, title='Fare (Pclass = 3)')\n\nfig.legend(['survived', 'not survived'])","c3aeae67":"fig, axes = plt.subplots(nrows=1, ncols=3, figsize = (20,8))\ndf_train.Embarked[(df_train.Survived == 1) & (df_train.Pclass == 1)].value_counts().plot('bar', ax=axes[0])\ndf_train.Embarked[(df_train.Survived == 0) & (df_train.Pclass == 1)].value_counts().plot('bar', ax = axes[0], color='r', alpha=0.5, title='Embarked (Pclass = 1)')\n\ndf_train.Embarked[(df_train.Survived == 1) & (df_train.Pclass == 2)].value_counts().plot('bar', ax=axes[1])\ndf_train.Embarked[(df_train.Survived == 0) & (df_train.Pclass == 2)].value_counts().plot('bar', ax = axes[1], color='r', alpha=0.5, title='Embarked (Pclass = 2)')\n\ndf_train.Embarked[(df_train.Survived == 1) & (df_train.Pclass == 3)].value_counts().plot('bar', ax=axes[2])\ndf_train.Embarked[(df_train.Survived == 0) & (df_train.Pclass == 3)].value_counts().plot('bar', ax = axes[2], color='r', alpha=0.5, title='Embarked (Pclass = 3)')\n\nfig.legend(['survived', 'not survived'])","c65e544c":"df_train.Fare","5fee4fd0":"df_train.boxplot(by='Survived', column = ['Fare'])\n","59b78b3e":"df_train.boxplot(by='Survived', column = ['Age'])","5656295f":"df_train.plot.scatter(x = 'Age', y = 'Fare', c='Survived', colormap = 'cool', figsize=(10,10))","1971f981":"df_train[df_train.Pclass == 1].sort_values('Fare')","f4a2b40d":"df_train.Embarked = df_train.Embarked.fillna(df_train.Embarked.value_counts().index[0])","127a3420":"missing_age = df_train.Age.isnull()\nsample = df_train.Age.dropna().sample(missing_age.sum(), replace=True).values\ndf_train.loc[missing_age, 'Age'] = sample","1a409119":"missing_age2 = df_test.Age.isnull()\nsample = df_test.Age.dropna().sample(missing_age2.sum(), replace=True).values\ndf_test.loc[missing_age2, 'Age'] = sample","78fee2fc":"missing_fare = df_test.Fare.isnull()\nsample = df_test.Fare.dropna().sample(missing_fare.sum(), replace=True).values\ndf_test.loc[missing_fare, 'Fare'] = sample","155d49d7":"fig, axes = plt.subplots(nrows=2, ncols=2, figsize = (12,12))\n\ndf_train.Age.plot('hist', ax=axes[0,0], title='Age (train set)')\ndf_test.Age.plot('hist', ax=axes[0,1], title='Age (test set)')\n\ndf_train.Fare.plot('hist', ax=axes[1,0], title='Fare (train set)')\ndf_test.Fare.plot('hist', ax=axes[1,1], title='Fare (test set)')","071ba2e9":"xform_age = sklearn.preprocessing.PowerTransformer(method='box-cox').fit(np.asarray(df_train.Age).reshape(len(df_train.Age),1))\nxform_fare = sklearn.preprocessing.PowerTransformer(method='box-cox').fit(np.asarray(df_train.Fare + 1).reshape(len(df_train.Fare),1))\n\nage_T = xform_age.transform(np.asarray(df_train.Age).reshape(len(df_train.Age),1))\nfare_T = xform_fare.transform(np.asarray(df_train.Fare + 1).reshape(len(df_train.Fare),1))\n\nfig, (ax1, ax2) = plt.subplots(nrows=1, ncols=2)\nax1.hist(age_T)\nax1.title.set_text('Age (tranformed)')\nax2.hist(fare_T)\nax2.title.set_text('Fare (tranformed)')","c5eb2829":"age_T = pd.DataFrame(age_T)\nage_T.columns = ['Age']\nquantile_age = xform_age.transform(np.asarray([0.1, 12, 20, 40, 80]).reshape(-1,1))\nquantile_age = quantile_age.reshape(-1)\nage_T['Quantile'] = pd.cut(age_T.Age, bins=quantile_age, labels = False, retbins=False)\n\nfare_T = pd.DataFrame(fare_T)\nfare_T.columns = ['Fare']\nquantile_fare = xform_fare.transform(np.asarray([1, 32, 100, 350, 601]).reshape(-1,1))\nquantile_fare = quantile_fare.reshape(-1)\nfare_T['Quantile'] = pd.cut(fare_T.Fare, bins=quantile_fare, labels = False, retbins=False)\n","0f12073e":"plt.hist(age_T.Quantile)","ed079ef7":"plt.hist(fare_T.Quantile)","d5a72ddd":"df_train.head()","ee31686d":"input_train = df_train.drop(columns = ['PassengerId', 'Name', 'Ticket', 'Cabin'])\ninput_train.head()","1b754c3c":"encoder_sex = sklearn.preprocessing.LabelEncoder().fit(input_train.Sex.to_numpy().reshape(len(input_train),1))\nsex_encoded = encoder_sex.transform(input_train.Sex.to_numpy().reshape(len(input_train),1))\nprint(sex_encoded[0:5])\n","c1d8b223":"encoder_embark = sklearn.preprocessing.OneHotEncoder().fit(input_train.Embarked.to_numpy().reshape(len(input_train),1))\nembarked_encoded = encoder_embark.transform(input_train.Embarked.to_numpy().reshape(len(input_train),1)).toarray()\nprint(encoder_embark.categories_)\nprint(embarked_encoded[0:5])\nencoder_embark.get_feature_names(['Port'])","19dc8956":"input_Port = pd.DataFrame(embarked_encoded)\ninput_Port.columns = encoder_embark.get_feature_names(['Port'])","b8d05c72":"input_Sex = pd.DataFrame(sex_encoded)\ninput_Sex.columns = ['Sex']\n","3eb909dc":"missing_fare = fare_T.Quantile[fare_T.Quantile.isnull()]","568c40b9":"fare_T.iloc[missing_fare.index, :]","2dcc3eb7":"fare_T.fillna(0, inplace=True)","27082064":"fare_T.isnull().sum()","e4f2784b":"input_train.Sex = input_Sex\n\ninput_train = input_train.drop(columns=['Embarked'])\ninput_train[list(encoder_embark.get_feature_names(['Port']))] = input_Port\n\ninput_train.Age = age_T.Quantile\ninput_train.Fare = fare_T.Quantile\n\ninput_train.head()\n","b4afe455":"input_test = df_test.drop(columns = ['PassengerId', 'Name', 'Ticket', 'Cabin'])\n\nsex_encoded_test = encoder_sex.transform(input_test.Sex.to_numpy().reshape(len(input_test),1))\nembarked_encoded_test = encoder_embark.transform(input_test.Embarked.to_numpy().reshape(len(input_test),1)).toarray()\n\ninput_Sex_test = pd.DataFrame(sex_encoded_test)\ninput_Sex_test.columns = ['Sex']\ninput_Port_test = pd.DataFrame(embarked_encoded_test)\n\ninput_test.Sex = input_Sex_test\n\ninput_test = input_test.drop(columns=['Embarked'])\ninput_test[list(encoder_embark.get_feature_names(['Port']))] = input_Port_test\n\ninput_age_T = xform_age.transform(input_test.Age.to_numpy().reshape(len(input_test.Age),1))\ninput_fare_T = xform_fare.transform((input_test.Fare + 1).to_numpy().reshape(len(input_test.Fare),1))\n\nbinned_age_T = pd.cut(input_age_T.reshape(len(input_age_T),), bins=quantile_age, labels=[0,1,2,3])  \nbinned_fare_T = pd.cut(input_fare_T.reshape(len(input_fare_T),), bins=quantile_fare, labels=[0,1,2,3])                                \n                                    \ninput_test.Age = pd.DataFrame(binned_age_T).iloc[:,0]\ninput_test.Fare = pd.DataFrame(binned_fare_T).iloc[:,0]\n\ninput_test.head()","97f75f56":"idx_age = input_test.Age[input_test.Age.isnull()].index\n","9562f7ca":"idx_fare = input_test.Fare[input_test.Fare.isnull()].index","1956079d":"quantile_age","3778a52b":"quantile_fare","ba1abfe2":"for i in range(len(idx_age)):\n    if binned_age_T[idx_age[i]] <= quantile_age[0]:\n        input_test.Age[idx_age[i]] = 0\n    else: \n        input_test.Age[idx_age[i]] = 3\n\nfor i in range(len(idx_fare)):\n    if binned_fare_T[idx_fare[i]] <= quantile_fare[0]:\n        input_test.Fare[idx_fare[i]] = 0\n    else: \n        input_test.Fare[idx_fare[i]] = 3","6d10382d":"input_train = input_train.astype(int)\ninput_test = input_test.astype(int)","bd9f3409":"X = input_train.drop(columns=['Survived'])\ny = input_train.Survived","8b03ef59":"logR = LogisticRegression()\nlogR.fit(X,y)","e065bd93":"score_logR = logR.score(X,y)\nprint('LogR score is ' + str(score_logR))","6788b26a":"input_test","d1e19a8d":"logR_coef = pd.DataFrame(logR.coef_.transpose())\nlogR_coef.columns = ['weights']\nlogR_coef.insert(0, 'feature', input_test.columns.values)\nlogR_coef.sort_values('weights')","e649ce90":"logR_pred = logR.predict(input_test)","a75af365":"submit = pd.DataFrame(logR_pred)\nsubmit.columns = ['Survived']\n\nsubmit.insert(0, 'PassengerId', df_test.PassengerId.values)","825ebf49":"submit.to_csv('logR_baseline.csv', index = False)","a2535bb5":"svc = SVC()\nsvc.fit(X,y)\n","5fe84c03":"score_svc = svc.score(X,y)\nprint('SVC score is ' + str(score_svc))","ab34ba91":"svc_pred = svc.predict(input_test)\nsubmit = pd.DataFrame(svc_pred)\nsubmit.columns = ['Survived']\nsubmit.insert(0, 'PassengerId', df_test.PassengerId.values)\nsubmit.to_csv('svc_baseline.csv', index = False)","e310fec2":"tree = DecisionTreeClassifier()\ntree.fit(X,y)\nprint(tree)\nscore_tree = tree.score(X,y)\nprint('Decision tree score is ' + str(score_tree))\n\ntree_pred = tree.predict(input_test)\nsubmit = pd.DataFrame(tree_pred)\nsubmit.columns = ['Survived']\nsubmit.insert(0, 'PassengerId', df_test.PassengerId.values)\nsubmit.to_csv('tree_baseline.csv', index = False)","1a8a13ca":"knn = KNeighborsClassifier()\nknn.fit(X,y)\nprint(knn)\nscore_knn = knn.score(X,y)\nprint('KNN score is ' + str(score_knn))\n\nknn_pred = knn.predict(input_test)\nsubmit = pd.DataFrame(knn_pred)\nsubmit.columns = ['Survived']\nsubmit.insert(0, 'PassengerId', df_test.PassengerId.values)\nsubmit.to_csv('knn_baseline.csv', index = False)","a673b5dd":"mlp = MLPClassifier()\nmlp.fit(X,y)\nprint(mlp)\nscore_mlp = mlp.score(X,y)\nprint('MLP score is ' + str(score_mlp))\n\nmlp_pred = mlp.predict(input_test)\nsubmit = pd.DataFrame(knn_pred)\nsubmit.columns = ['Survived']\nsubmit.insert(0, 'PassengerId', df_test.PassengerId.values)\nsubmit.to_csv('mlp_baseline.csv', index = False)","87c4aa40":"summary = pd.DataFrame([score_logR, score_svc, score_tree, score_knn, score_mlp])\nsummary.columns = ['self score']\nbaseline = ['logR', 'SVC', 'DecisionT', 'KNN', 'MLP']\n\nsummary.insert(0, 'baseline model', baseline)\nsummary.head()","ecb9fa9e":"X","35038924":"def feature_weights(model):\n    try: \n        weight = model.coef_\n    except AttributeError:\n        weight = model.feature_importances_\n    finally:\n        df_weight = pd.DataFrame(weight.transpose())\n        df_weight.columns = ['weights']\n        df_weight.insert(0, 'feature', input_test.columns.values)\n    return df_weight.sort_values('weights')","caf1574a":"logR_coef = feature_weights(logR)\nlogR_coef","2e83b0da":"tree_coef = feature_weights(tree)\ntree_coef","1bf88dcc":"X['Family'] = X.SibSp + X.Parch\nX.drop(columns=['SibSp', 'Parch'], inplace=True)\n","3d6d4f50":"input_test['Family'] = input_test.SibSp + input_test.Parch\ninput_test.drop(columns=['SibSp', 'Parch'], inplace=True)","a773c197":"X","fc63c5f2":"def all_but_one(estimators, X, y, estimator_names):\n    score= []\n    for i in range(len(estimators)):\n        score_all = []\n        model = estimators[i].fit(X,y)\n        score_base = model.score(X,y)\n        col_nam = X.columns.values\n        score_all.append(score_base)\n\n        for col in range(len(col_nam)):\n            X_remain = X.drop(columns=[col_nam[col]])\n            model_remain = estimators[i].fit(X_remain, y)\n            score_remain = model_remain.score(X_remain, y)\n            score_all.append(score_remain)\n        score.append(score_all)\n    score = pd.DataFrame(np.asarray(score).transpose())\n    col_nam = list('w\/o ' + col_nam)\n    col_nam.insert(0, 'baseline')\n    score.columns = list('self score ' + np.asarray(estimator_names, dtype=object))\n    score.insert(0, 'feature', col_nam)\n    return score","bea09554":"feature_importances = all_but_one([logR, svc, tree, knn, mlp], X.drop(columns=['Port_C','Port_Q','Port_S']), y, ['logR', 'SVC', 'DecisionT', 'KNN', 'MLP'])\nfeature_importances","1edb3a37":"polyfeat = sklearn.preprocessing.PolynomialFeatures(include_bias=False, interaction_only=True)\ninteract = polyfeat.fit_transform(X.loc[:,['Pclass', 'Fare']])[:,1]\ninteract = interact.astype('int')","03994b22":"X","0bd55cd2":"X_new = X.drop(columns=['Port_C','Port_Q','Port_S']).copy()\nX_new.drop(columns=['Pclass', 'Fare'], inplace=True)\nX_new.insert(1, 'Pclass_Fare', interact)\nX_new","bc7cfc38":"feature_importances2 = all_but_one([logR, svc, tree, knn, mlp], X_new, y, ['logR', 'SVC', 'DecisionT', 'KNN', 'MLP'])\nfeature_importances2","08dab792":"param_logR = {'penalty' : ['l1', 'l2'], 'tol' : [1e-3, 1e-4, 1e-5], \n              'C' : [0.3, 0.6, 1]}\nparam_svc = {'kernel' : ['rbf', 'linear', 'poly'], 'gamma' : ['auto', 'scale'],\n             'C' : [0.3, 0.6, 1],  'tol' : [1e-3, 1e-4, 1e-5]}\nparam_tree = {'criterion' : ['gini', 'entropy'], 'max_depth' : [2, 4, 6], 'max_leaf_nodes' : [2, 3]}\nparam_knn =  {'n_neighbors' : [3, 4, 5], 'metric' : ['minkowski', 'euclidean'], 'leaf_size' : [25, 30 ,35]}\nparam_mlp = {'hidden_layer_sizes' : [3, 6], 'activation' : ['relu', 'logistic'], 'solver' : ['lbfgs', 'sgd', 'adam'], \n             'alpha' : [0.0002, 0.0001, 0.0005], 'learning_rate' : ['constant', 'adaptive'],  'tol' : [1e-3, 1e-4, 1e-5]}","42ab6982":"rng = 0\nX_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.1, random_state=rng)","7990c9b7":"def optimiser(estimator, param_grid, X_train, y_train, X_test, y_test, rng_instance):\n    model = RandomizedSearchCV(estimator, param_distributions=param_grid, scoring='roc_auc', verbose=0,random_state=rng_instance, cv=10)\n    model.fit(X_train, y_train)\n    y_pred = model.predict(X_test)\n    roc_score = roc_auc_score(y_test, y_pred)\n    print(model.best_estimator_)\n    print(model.best_score_)\n    print(roc_score)\n    return model.best_estimator_, y_pred","0351039b":"tuned_logR ,y_pred_logR = optimiser(logR, param_logR, X_train, y_train, X_test, y_test, 0)\ntuned_logR","54f263a1":"tuned_svc ,y_pred_svc = optimiser(SVC(probability=True), param_svc, X_train, y_train, X_test, y_test, 0)\ntuned_svc","a0439b2c":"tuned_tree ,y_pred_tree = optimiser(tree, param_tree, X_train, y_train, X_test, y_test, 0)\ntuned_tree","86978f22":"tuned_knn ,y_pred_knn = optimiser(knn, param_knn, X_train, y_train, X_test, y_test, 0)\ntuned_knn","a68e2735":"tuned_mlp ,y_pred_mlp = optimiser(MLPClassifier(max_iter=1000), param_mlp, X_train, y_train, X_test, y_test, 0)\ntuned_mlp","e318e6f6":"voting = VotingClassifier(estimators=[('logR', tuned_logR), ('svc', tuned_svc), ('tree', tuned_tree), ('mlp', tuned_mlp)], voting='soft')\nvoting.fit(X_train,y_train)\nroc_auc = roc_auc_score(y_test, voting.predict(X_test))\nprint(roc_auc)\nvoting_final = voting.fit(X, y)\ny_voting = voting_final.predict(input_test)\n\nsubmit_voting = pd.DataFrame(y_voting)\nsubmit_voting.columns = ['Survived']\nsubmit_voting.insert(0, 'PassengerId', df_test.PassengerId.values)\nsubmit_voting.to_csv('Voting_ensemble.csv', index = False)","85acf39b":"def Stacking(estimator, X, y, test, n_fold):\n    folds = StratifiedKFold(n_fold,random_state=0)\n    train_pred = np.empty((0,1),float)\n    test_pred = np.empty((n_fold,test.shape[0]),float)\n    for i, (train_idx,val_idx) in enumerate(folds.split(X,y)):\n        x_train, x_val = X.iloc[train_idx], X.iloc[val_idx]\n        y_train, y_val = y.iloc[train_idx],y.iloc[val_idx]\n        estimator.fit(X = x_train, y = y_train)\n        train_pred = np.append(train_pred, estimator.predict(x_val))\n        test_pred[i] = estimator.predict(test)\n    test_pred = np.mean(test_pred, axis=0)\n    return pd.DataFrame(test_pred.reshape(-1,1)), pd.DataFrame(train_pred)","624e894c":"svc_test, svc_train = Stacking(tuned_svc, X, y, input_test, n_fold=10)\ntree_test, tree_train = Stacking(tuned_tree, X, y, input_test, n_fold=10)\n#logR_test, logR_train = Stacking(tuned_logR, X, y, input_test, n_fold=10)\n#mlp_test, mlp_train = Stacking(tuned_mlp, X, y, input_test, n_fold=10)\nknn_test, knn_train = Stacking(tuned_knn, X, y, input_test, n_fold=10)","f5941e8e":"X_stack_train = pd.concat([svc_train, tree_train, knn_train], axis=1)\nX_stack_test = pd.concat([svc_test, tree_test, knn_test], axis=1)\nX_stack_train.columns = X_stack_test.columns = ['svc', 'tree', 'knn']","3ee1e0f1":"X_stack_test","e5a1ccce":"X_stack_train","73ca4bc2":"param_xgb = {'n_estimators' : [500, 1000, 2000, 4000], 'eta' : [0.05, 0.1, 0.2, 0.3] , 'max_depth' : [2, 4, 6, 8], 'min_child_weight' : [1, 2, 3], 'gamma' : [0.3, 0.6, 0.9], 'subsample' : [0.5, 0.8, 1], \n              'colsample_bytree' : [0.6, 0.8, 1], 'lambda' : [0, 0.5, 0.8, 1], 'learning_rate' : [0.05, 0.1, 0.2, 0.3],  'alpha' : [0, 0.5, 0.8, 1]}","c316ac58":"X_train, X_test, y_train, y_test = train_test_split(X_stack_train, y, test_size=0.1, random_state=rng)","70ea1bd7":"tuned_stack, tuned_stack_pred = optimiser(xgb.XGBClassifier(), param_xgb, X_train, y_train, X_test, y_test, rng)","75e6de7e":"tuned_stack.fit(X_stack_train, y)\nstack_pred = tuned_stack.predict(X_stack_test)","c1e9d9b9":"StackingSubmission = pd.DataFrame({ 'PassengerId': df_test.PassengerId,\n                            'Survived': stack_pred})\nStackingSubmission.to_csv(\"Stacking_ensemble.csv\", index=False)","b931d1ba":"param_bag = {'base_estimator' : [tuned_logR, tuned_svc, tuned_tree, tuned_mlp], 'n_estimators' : [1000, 1500, 2000, 2500], 'max_samples': [0.25, 0.5, 0.75, 1],\n            'max_features' : [0.25, 0.5, 1]}","dcb9bf1a":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=rng)","ece3bd5b":"#tuned_bag, tuned_bag_pre = optimiser(BaggingClassifier(), param_bag, X_train, y_train, X_test, y_test, rng)","add2b857":"#tuned_bag.fit(X, y)\n#bag_pred = tuned_bag.predict(input_test)\n#BaggingSubmission = pd.DataFrame({ 'PassengerId': df_test.PassengerId,\n                          #  'Survived': bag_pred})\n#BaggingSubmission.to_csv(\"Bagging_Submission.csv\", index=False)","57da9015":"We try decision tree next.","c75b73e4":"**Insight**\n\nOur initial hypothesis that port S could be the port where 3rd class passengers mainly board at runs contrary to what is shown here. We see that in all classes, port S is the most popular so it is unlikely that port S is mainly populated by passengers of a certain class. Port S could just be popular due to its geographic reasons and in fact port may not be that important a feature in predicting survival. We will check it out after we build our baseline model.","4e4af300":"Support vector classification, k-nearest neighbours and multi layer perceptron algorithms have no feature importance attribute.","6a3fa359":"Somehow my x label (Age) is not appearing...\n\nPersonally, I am not well-versed with interpreting scatterplots so in this case here, I wonder what insights could be derived...\n\nCan anyone reading this provide some clues or resources that might help me? ","8c0ca573":"**Why do data visualisation?**\n\nData visualisation is a method used to summarise our data into informative pictures which helps us make conclusions about our data. \n\nSo why do we want to make conclusions about our data? \n\nThe end goal here is to identify features for our training set which we think will best allow our models do predict accurately. Thus, we need to determine the dependencies between features and target (qualitative) and also to determine the strength of these dependencies (quantitative). This will guide us as we explore our data using visualisations.\n\n**Questions to ask as we go along with data visualisation**:\n1. What conclusion can I make from this picture?\n2. How will this conclusion affect the quality of my training set? (which will affect the quality of our model)\n3. What can do done to solve the problem?\n","5a58c1d8":"**Insight**\n\nWe see that are some missing values in 'Age'. When there are missing values, we can either drop the column if there are too many missing values or generate values to fill in the blanks. How many missing values are too many? There's no clear answer of course! When in doubt just run two models, with and without the particular feature and then decide. \n\nFurthermore, we know age will probably be correlated to survival so we keep it in this case.","601b5810":"From this histogram alone, I can't really extract anything exciting so let's dive in deeper by considering the different categorical varbiales together with age.","222b7bf8":"**Insight from bar chart of target variable**\n\n1. We see here that we are dealing with an imbalanced dataset. \n2. Imbalanced datasets will cause a bias in our model as one class is under-represented by the model.\n3. Converting this into a balanced dataset or employing methods to mitigate the effects of an imbalanced dataset will probably improve our predictions.  My plan for now is  keep this at the back of my head and come back to this later when I have finished testing my baseline model. ","dd44bf7c":"We managed to remove some skewness!\n\n\nWe added a constant of + 1 to all values in 'Fare' because the box-cox transform does not deal well with the values of 0.","7ca750f9":"**Part 1: Data Visualistion**","223c3227":"**Ensemble Method: Stacking**","c7c475b6":"Next, we do bivariate analysis.","f88b3b88":"From the feature importance of the decision tree model, we see that the feature relating to Ports (Q\/C\/S) are low individually. This is because of dummy variables created when we used one-hot encoding. We actually have to consider all 3 together for this feature to make sense.","7ab86b2b":"We see that the baseline model has a higher score throughout. This means that we cannot drop any of the current features as they contain information about our target.","0b6d2f59":"What we have so far,","4557260a":"Wow! It is not surprising that a significantly larger number of females than males survived across all ages, with the exception of around ages 6~15, which could be an anormaly. But the overal trend for females seems to be that they had higher survival rate than males regardless of age. The converse is true for the survival rates of males. This ties in well with our hypothesis mentioned above.","05731ec8":"We try combining SibSp and Parch.","58dd0dcf":"Next, we try support vector classification.","91177528":"**Part 4: Feature Engineering**","509ae6d0":"**Conclusion for Part 1: Data Visualisation**\n\n1. Exploring numerical statistics from dataset by .describe() and .info()\n2. Univariate analysis; forming some inital hypotheses.\n3. Bivariate analysis; conditional probability to probe at underlying relationships and confirming some of the inital hypotheses.\n4. Exploring higher order relationshis to possibly gain more in-depth insights?\n\nAll in all, we explore the dataset using data visualisation to gain a feel for what features will likely be more important for use in our model later on.","ec18c369":"Looking at the magnitude of the weights, it seems that SibSp, Age and Parch seem to have a low impact on predicting survival. We shall do feature engineering later on to improve on this.","8adb5371":"**Part 3: Data Preparation for Model**\n\nWe will do the following:\n\n1. Apply some form of power transformation to 'Age' and 'Fare' to check if we are able to get a normal distribution.\n2. Split 'Age' and 'Fare' into bins.\n4. Convert categorical string variables into numerical variables.\n5. Deploy baseline model.","9a0edb48":"When using preprocessing methods, the general consensus is to fit the methods to the train set and apply it on the test set to prevent data leakage. In this case, I would think it is more reasonable to go against the general consensus and actually fill in the missing values in the test set from random samples within the test set itself. \n\nIn actual fact, I would think that the following two ways would result in a similar end product:\n\n1. Fill missing values in test set by random samples from train set.\n2. Fill missing values in test set by random samples from test set itself.\n\nThis is because the data in train set and test set would have come from a similar distribution before being split and provided to us by Kaggle. We check this by plotting the distribution now.","edb42d84":"**Insight**\n\nWe see that the passengers who had 1 or 2 SibSp had higher survival rate than those who had none and then beyond that the survival rate fall. Let's think about this. \n\n1. Is it possible that those who had 1 or 2 SibSp would have been able to rely on each other, and through team effort increase their chance for survival? Conversely, those who had no SipSp may have had to struggle for survival alone thus reducing their survival rate.\n\n2. In contrast, with 3 or 4 SipSp we see that their survival rate actually decreased. Is it possible that this is too large a group such that coordination is made more difficult and thus survival rate is reduced?","c6a235ab":"**My Data Science Journey**\n\nThis will be a 28 day journey where I aim to learn something new each day about classification and regression problems. My goal at the end of it is to place in the top 40% of submissions. Looking at the list of featured competitions (past and present), I have chosen 1 competition from each of the 2 catagories based on my interests. On days my willpower falters, my interest in these topics will hopefully help me remain focussed on my goal. \n\n* Classification problem:    https:\/\/www.kaggle.com\/c\/jigsaw-toxic-comment-classification-challenge\n* Regression problem:    https:\/\/www.kaggle.com\/c\/elo-merchant-category-recommendation\n\nedit: have not been updating my kernels due to external datascience commitments! The journey is still on, albeit not on Kaggle. I've been too busy with my machine learning project in school so I've been unable to play on Kaggle :(\n\nAfter snooping around on Kaggle for a couple of days, this is what I have gathered so far about core competecies:\n1. Data visualisation\n    * determine correlation between target and variables\n    * analyse distribution of data\n2. Data cleaning: \n    * anomalies\n    * missing data\n    * wrong datatypes\n3. Data preparation for model\n    * train\/test split\n    * data transformation: normalisation, standardisation, log-transform\n    * noise filtering\n    * principal compenent analysis\n4. Feature selection \/ Feature engineering\n    * manual feature engineering (domain knowledge)\n    * automated feature engineering (brute force)\n    * feature importance\n    * permutation importanace\n    * feature interaction\n5. Final model preparation\n    * cross validation\n    * optimise hyperparameters\n    * ensemble methods: stacking, bagging, boosting\n    * evaluation metrics\n    \nSteps 1 and 2 are done concurrently. Likewise for steps 4 and 5!","ef4ef062":"**Ensemble Method: Voting**","e66d8895":"**Insight**\n\nWe see that passengers who embarked at port S had the lowest survival rate. I would think if mainly 3rd class passengers embarked at port S as previously hypothesised, then it would make sense that their survival rate is lower compared to higher class passengers as they would have gotten priority in evacuation.","e7d11339":"\nWe first check the distribution of our target variable.\n","f5881b6c":"We see that only the baseline score of Decision Tree model is decreased, the other models remain approximately unaffected. In this case, our new feature did not give us more information about the target. Hence, we drop the new feature.","61551ccc":"With that, I concludes this notebook for now. Time to try out some past competitions!","9790ba18":"These two boxplots show that there is quite a significant number of outliers in our data when we compare 'Fare' with survival. We certainly do not want to discard all these outliers as this will result in sigificant information loss. When deploy our models, I would think that a model that is more robust to outliers will perform better. We will check this later on.","9d595164":"The next step would be to determine the bin size. I am disinclined to use fix bin widths for either distribution. The reason for this is because there are regions where the data is densely populated and some regions that are sparsely populated. We can attempt to split the bins based on how different age groups of people are defined generally. We do the following:\n\n* Infant + Toddlers\n* Young adults\n* Adults\n* Everyone else who is older","ccf739fb":"Next, we try k-nearest neighbours.","a5ce3e06":"For 'Age' and 'Fare', we will randomly sample values from the current distribution of values so that we can retain the overall distribution even after filling in the missing values. ","d6267c09":"We see that higher paying passengers had higher survival. Let's break it down further according to Pclass.","1042cd93":"**Part 5: Final Model Preparation**\n\n1. We first perform cross validation and hyper-parameter optimsation for the models.\n2. Select the best 3 to create an ensemble.","e3237e44":"Intuitively, we would think Pclass and Fare would be correlated as tickets for a higher class would have a higher fare. We create a simple interaction term between the 2 as a new feature to see if this gives us more information about whether a passenger survives or not.","6ebef300":"Now we can try some baseline models! We start with logistic regression.","ba0c7fd0":"**Part 2: Data Cleaning**\n\nThings to look out for:\n\n1. Anomalies\n2. Missing values\n3. Zero values (is the value really zero or is zero the dafault entry in the event of an info gap)\n4. Incorrect datatypes\n\n","b98d2a05":"A significant amount of 'age' data is missing in both the test and training dataset. We need to decide how we will deal with this.\n\nWe have a strong feel that age will be an important feature in predicting survival so we prioritise filling in the missing values as opposed to dropping the rows with missing 'age' data.","23ad434c":"We now prepare the input features for our baseline model.\n\nA quick look at what we have so far.","2d50d3d9":"**Insights**\n\n1. Majority of people are in Pclass = 3 (3rd class passengers). We also see that majority of people embarked at S. I would start wondering, is there a correlation between the high percentage of people who embarked at port S and the high percentage of 3rd class passengers?\n\n2. Considering bar plots of SibSp and Parch together, majority of passengers traveled alone. One could think that a passenger who traveled alone would be able to quickly attempt to escape once the Titanic struck the iceberg while a passenger traveling with family would have to assist the family in escaping. There is possible a correlation between traveling alone and survival.\n\n3. Sex could also be correlated with survival. During the time period of the Titanic, chivalry probably still existed culturally so women and children would get priority in getting into life boats as passengers evacuated the sinking ship.","3788b100":"From here, we will do feature engineering followed by optimsation of hyper-parameters of our baseline model and lastly try out some ensemble methods. \n\nNote that the aim is to improve from our baseline model but not the point where we are overfitting our model to the training data. This is really just one huge optimsation problem were we consider the tradeoff between our model's ability to generalise to unseen data and the amount of overfitting to training data.","301b02b7":"Since some of the algorithms have no feature importance API available, we can try the following method to see the effect of a feature on the model's predicitive power.","dafb85e5":"**Insight**\n\nWe see a nice direct correlation between survival and passenger class. This would probably be an important feature to prediction survival.","e12f8713":"We then do a univariate analysis of our input variables.\n\n**Discrete variables**\n* Nominal variables: Sex, Embarked\n* Ordinal: Pclass\n* Interval based: SibSp, Parch\n\n**Continuous variables**\n* Age\n* Fare","52a9d523":"**Insights**\n\n1. We see a sharp spike at around age = 2, but there's no cause for alarm as this just means there are quite a few infant passengers. Similar to possible correlation between sex and survival, I would think that here is a correlation between age and survival as infants, elderly and women probably got priority in evacuation.\n\n2. We see the majority of the fare is around the cheaper end, which agrees which the fact that majority of passengers were 3rd class (3rd class tickets are the cheapest of course). \n\n3. In the histogram for Fares, we see an outlier in the far right end. This really expensive ticket was probably only for a very selective group of individuals. We will decide later whether we will drop this outlier or not.\n\n4. Age histogram follows a weibull distribution somewhat and the Fare historgram follows a lognormal distribution.","8c72271e":"Lastly, we try MLPClassifier.","33bb8b96":"**Ensemble Method: Bagging**","55c1ab05":"We check if our binning process was successful","ccd35be2":"This shows the conditional probably: given that a passenger is male\/female, what is the probably that the passenger survived?","70017178":"**Missing Values**\n\n1. training set: Age\n2. training set: Embarked\n3. test set: Age\n4. test set: Fare\n","0945105d":"These transformed values are probably out of range of the bins used to fit the train set. We will manually consider these cases and correct them.","77a60d3d":"For categaorical variable 'Embarked', we will correct for missing value by using the most frequent value.","71e72996":"**Insight**\n\nWe see that there are some case where Fare is 0. Could the passengers have gotten a free ride onto the Titanic? Possible as these group of people could be the employees (cabin crew, service staff, etc).","8b8acd1b":"**Insight**\n\nWe see that passengers traveling with 3 Parch had the highest survival rate. Adding on to the insights from SibSp vs survival rate, although I hypothesised that traveling in too large a group would decrease survival rate, but there can be an exception in the case of parents and children. \n\nLet us consider the case of a family with say, a father, a mother and a child. I am inclined to think that the mother and child would have a higher survival rate collectively as they would have gotten priority in evacuation. Well, it is unlikely that a mother and child would be separately evacuated."}}