{"cell_type":{"cb18eae8":"code","3d2d31d9":"code","0e7dab2f":"code","c51fd2f2":"code","48ae0088":"code","ba279c18":"code","298dfe20":"code","f19f891c":"code","59651760":"code","cbec40ca":"code","c283d9ed":"code","efe832a7":"markdown","124fabbe":"markdown"},"source":{"cb18eae8":"import sys\nsys.path.append('..\/input\/iterativestratification')\nfrom iterstrat.ml_stratifiers import MultilabelStratifiedKFold\nimport os\nimport gc\nimport random\nimport math\nimport time\nimport glob\n\nimport numpy as np\nimport pandas as pd\n\nfrom sklearn.model_selection import StratifiedKFold, GroupKFold, KFold\nfrom sklearn.metrics import log_loss\n\nimport category_encoders as ce\n\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import DataLoader, Dataset\nimport torch.optim as optim\nimport torch.nn.functional as F\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')","3d2d31d9":"def get_logger(filename='log'):\n    from logging import getLogger, INFO, StreamHandler, FileHandler, Formatter\n    logger = getLogger(__name__)\n    logger.setLevel(INFO)\n    handler1 = StreamHandler()\n    handler1.setFormatter(Formatter(\"%(message)s\"))\n    handler2 = FileHandler(filename=f\"{filename}.log\")\n    handler2.setFormatter(Formatter(\"%(message)s\"))\n    logger.addHandler(handler1)\n    logger.addHandler(handler2)\n    return logger\n\nlogger = get_logger()\n\n\ndef seed_everything(seed=42):\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True\n    \nseed_everything(seed=42)","0e7dab2f":"train_features = pd.read_csv('..\/input\/lish-moa\/train_features.csv')\ntrain_targets_scored = pd.read_csv('..\/input\/lish-moa\/train_targets_scored.csv')\ntrain_targets_nonscored = pd.read_csv('..\/input\/lish-moa\/train_targets_nonscored.csv')\ntest_features = pd.read_csv('..\/input\/lish-moa\/test_features.csv')\nsubmission = pd.read_csv('..\/input\/lish-moa\/sample_submission.csv')\n# ref: https:\/\/www.kaggle.com\/c\/lish-moa\/discussion\/180165\n# check if labels for 'ctl_vehicle' are all 0.\ntrain = train_features.merge(train_targets_scored, on='sig_id')\ntarget_cols = [c for c in train_targets_scored.columns if c not in ['sig_id']]\ncols = target_cols + ['cp_type']\ntrain[cols].groupby('cp_type').sum().sum(1)","c51fd2f2":"# constrcut train&test except 'cp_type'=='ctl_vehicle' data\nprint(train_features.shape, test_features.shape)\ntrain = train[train['cp_type']!='ctl_vehicle'].reset_index(drop=True)\ntest = test_features[test_features['cp_type']!='ctl_vehicle'].reset_index(drop=True)\nprint(train.shape, test.shape)","48ae0088":"folds = train.copy()\nFold = MultilabelStratifiedKFold(n_splits=5, shuffle=True, random_state=42)\nfor n, (train_index, val_index) in enumerate(Fold.split(folds, folds[target_cols])):\n    folds.loc[val_index, 'fold'] = int(n)\nfolds['fold'] = folds['fold'].astype(int)\nprint(folds.shape)","ba279c18":"class TrainDataset(Dataset):\n    def __init__(self, df, num_features, cat_features, labels):\n        self.cont_values = df[num_features].values\n        self.cate_values = df[cat_features].values\n        self.labels = labels\n        \n    def __len__(self):\n        return len(self.cont_values)\n\n    def __getitem__(self, idx):\n        cont_x = torch.FloatTensor(self.cont_values[idx])\n        cate_x = torch.LongTensor(self.cate_values[idx])\n        label = torch.tensor(self.labels[idx]).float()\n        \n        return cont_x, cate_x, label\n    \n\nclass TestDataset(Dataset):\n    def __init__(self, df, num_features, cat_features):\n        self.cont_values = df[num_features].values\n        self.cate_values = df[cat_features].values\n        \n    def __len__(self):\n        return len(self.cont_values)\n\n    def __getitem__(self, idx):\n        cont_x = torch.FloatTensor(self.cont_values[idx])\n        cate_x = torch.LongTensor(self.cate_values[idx])\n        \n        return cont_x, cate_x\ncat_features = ['cp_time', 'cp_dose']\n#num_features = [c for c in train.columns if train.dtypes[c] != 'object']\n#num_features = [c for c in num_features if c not in cat_features]\n#num_features = [c for c in num_features if c not in target_cols]\n\nnum_features = ['g-0','g-2','g-3','g-5','g-6','g-7','g-8'\n,'g-9','g-11','g-12','g-13','g-15','g-16','g-17','g-18','g-20','g-21'\n,'g-22','g-24','g-25','g-26','g-27','g-28','g-29','g-30','g-31','g-32'\n,'g-33','g-34','g-36','g-37','g-38','g-39','g-41','g-42','g-43','g-45'\n,'g-47','g-48','g-49','g-50','g-51','g-52','g-53','g-54','g-55','g-56'\n,'g-57','g-58','g-60','g-61','g-62','g-63','g-65','g-66','g-67','g-68'\n,'g-69','g-70','g-71','g-72','g-73','g-75','g-76','g-77','g-78','g-79'\n,'g-80','g-81','g-83','g-84','g-85','g-86','g-87','g-89','g-90','g-91'\n,'g-92','g-93','g-94','g-96','g-97','g-98','g-100','g-101','g-102','g-103'\n,'g-104','g-105','g-106','g-107','g-108','g-109','g-110','g-111','g-112'\n,'g-113','g-114','g-115','g-116','g-117','g-118','g-119','g-120','g-121'\n,'g-122','g-123','g-124','g-125','g-126','g-127','g-129','g-130','g-131'\n,'g-132','g-133','g-134','g-135','g-136','g-137','g-138','g-139','g-140'\n,'g-141','g-142','g-143','g-144','g-146','g-147','g-148','g-149','g-150'\n,'g-151','g-152','g-154','g-156','g-157','g-158','g-160','g-161','g-162'\n,'g-163','g-164','g-165','g-166','g-167','g-169','g-170','g-172','g-173'\n,'g-174','g-175','g-177','g-178','g-179','g-180','g-181','g-183','g-184'\n,'g-185','g-186','g-187','g-188','g-189','g-190','g-192','g-194','g-195'\n,'g-196','g-199','g-200','g-202','g-203','g-205','g-206','g-207','g-208'\n,'g-209','g-210','g-211','g-212','g-215','g-216','g-217','g-218','g-219'\n,'g-221','g-222','g-224','g-225','g-226','g-227','g-228','g-229','g-230'\n,'g-231','g-233','g-235','g-236','g-237','g-238','g-239','g-240','g-241'\n,'g-242','g-243','g-245','g-246','g-247','g-248','g-250','g-251','g-252'\n,'g-253','g-254','g-255','g-256','g-257','g-258','g-260','g-262','g-263'\n,'g-265','g-267','g-268','g-269','g-270','g-272','g-273','g-274','g-276'\n,'g-279','g-280','g-283','g-284','g-285','g-286','g-287','g-291','g-292'\n,'g-293','g-294','g-296','g-297','g-298','g-299','g-300','g-301','g-302'\n,'g-303','g-305','g-306','g-307','g-308','g-309','g-310','g-312','g-313'\n,'g-314','g-317','g-318','g-319','g-321','g-322','g-323','g-324','g-325'\n,'g-326','g-327','g-328','g-329','g-330','g-331','g-332','g-335','g-336'\n,'g-337','g-338','g-340','g-341','g-342','g-343','g-344','g-346','g-347'\n,'g-348','g-349','g-350','g-352','g-353','g-354','g-355','g-356','g-357'\n,'g-358','g-359','g-360','g-361','g-362','g-363','g-365','g-366','g-367'\n,'g-368','g-369','g-371','g-372','g-373','g-374','g-375','g-376','g-377'\n,'g-379','g-380','g-381','g-382','g-383','g-384','g-385','g-386','g-387'\n,'g-388','g-389','g-390','g-391','g-392','g-394','g-395','g-396','g-397'\n,'g-398','g-400','g-402','g-403','g-404','g-405','g-407','g-408','g-409'\n,'g-410','g-411','g-412','g-414','g-415','g-416','g-417','g-418','g-419'\n,'g-420','g-421','g-422','g-423','g-424','g-425','g-426','g-427','g-428'\n,'g-429','g-430','g-431','g-432','g-433','g-434','g-435','g-438','g-439'\n,'g-440','g-441','g-442','g-443','g-444','g-445','g-446','g-447','g-449'\n,'g-450','g-451','g-453','g-454','g-455','g-456','g-457','g-458','g-459'\n,'g-460','g-461','g-462','g-463','g-465','g-466','g-468','g-469','g-470'\n,'g-471','g-472','g-473','g-474','g-475','g-476','g-479','g-480','g-482'\n,'g-483','g-484','g-485','g-486','g-488','g-489','g-491','g-492','g-493'\n,'g-497','g-498','g-499','g-500','g-502','g-503','g-504','g-506','g-507'\n,'g-508','g-509','g-510','g-511','g-513','g-514','g-515','g-516','g-518'\n,'g-520','g-522','g-523','g-524','g-525','g-526','g-527','g-528','g-529'\n,'g-530','g-531','g-533','g-534','g-535','g-536','g-537','g-538','g-539'\n,'g-540','g-541','g-542','g-543','g-544','g-546','g-547','g-550','g-551'\n,'g-552','g-553','g-554','g-555','g-556','g-557','g-558','g-559','g-560'\n,'g-561','g-562','g-563','g-564','g-566','g-567','g-568','g-569','g-570'\n,'g-571','g-572','g-574','g-577','g-578','g-579','g-580','g-583','g-584'\n,'g-587','g-588','g-589','g-590','g-592','g-593','g-594','g-595','g-596'\n,'g-597','g-598','g-599','g-600','g-602','g-604','g-605','g-606','g-608'\n,'g-609','g-610','g-611','g-612','g-613','g-614','g-616','g-619','g-620'\n,'g-622','g-624','g-627','g-628','g-629','g-630','g-631','g-632','g-634'\n,'g-635','g-636','g-639','g-640','g-641','g-642','g-643','g-644','g-646'\n,'g-647','g-648','g-649','g-651','g-652','g-655','g-656','g-657','g-658'\n,'g-659','g-660','g-661','g-663','g-664','g-665','g-666','g-667','g-669'\n,'g-671','g-672','g-673','g-674','g-675','g-677','g-678','g-679','g-681'\n,'g-682','g-683','g-684','g-685','g-686','g-688','g-689','g-691','g-692'\n,'g-693','g-694','g-696','g-697','g-698','g-699','g-700','g-701','g-702'\n,'g-704','g-705','g-706','g-708','g-709','g-710','g-711','g-712','g-713'\n,'g-714','g-720','g-722','g-724','g-725','g-726','g-727','g-728','g-729'\n,'g-731','g-733','g-734','g-735','g-736','g-737','g-738','g-739','g-740'\n,'g-741','g-742','g-743','g-744','g-745','g-746','g-747','g-748','g-749'\n,'g-750','g-751','g-752','g-753','g-755','g-756','g-757','g-758','g-759'\n,'g-760','g-761','g-762','g-763','g-764','g-766','g-767','g-768','g-769'\n,'g-771','c-0','c-5','c-6','c-7','c-8','c-9','c-10','c-12','c-13','c-15'\n,'c-18','c-20','c-22','c-24','c-25','c-26','c-30','c-33','c-34','c-36'\n,'c-37','c-38','c-41','c-44','c-45','c-46','c-47','c-48','c-50','c-51'\n,'c-52','c-54','c-56','c-57','c-58','c-59','c-60','c-62','c-63','c-64'\n,'c-65','c-66','c-67','c-69','c-70','c-71','c-72','c-73','c-75','c-76'\n,'c-77','c-79','c-80','c-81','c-83','c-85','c-86','c-87','c-89','c-92'\n,'c-93','c-95','c-96','c-98','c-99']\n\ntarget = train[target_cols].values\n\ndef cate2num(df):\n    df['cp_time'] = df['cp_time'].map({24: 0, 48: 1, 72: 2})\n    df['cp_dose'] = df['cp_dose'].map({'D1': 3, 'D2': 4})\n    return df\n\ntrain = cate2num(train)\ntest = cate2num(test)","298dfe20":"class CFG:\n    max_grad_norm=1000\n    gradient_accumulation_steps=1\n    hidden_size=512\n    dropout=0.45\n    lr=1e-3\n    weight_decay=1e-5\n    batch_size=128\n    epochs=50\n    #total_cate_size=5\n    #emb_size=4\n    num_features=num_features\n    cat_features=cat_features\n    target_cols=target_cols\n    \nclass TabularNN(nn.Module):\n    def __init__(self, cfg):\n        super().__init__()\n        self.mlp = nn.Sequential(\n                          nn.Linear(len(cfg.num_features), cfg.hidden_size),\n                          nn.BatchNorm1d(cfg.hidden_size),\n                          nn.Dropout(cfg.dropout),\n                          nn.PReLU(),\n                          nn.Linear(cfg.hidden_size, cfg.hidden_size),\n                          nn.BatchNorm1d(cfg.hidden_size),\n                          nn.Dropout(cfg.dropout),\n                          nn.PReLU(),\n                          nn.Linear(cfg.hidden_size, len(cfg.target_cols)),\n                          )\n\n    def forward(self, cont_x, cate_x):\n        # no use of cate_x yet\n        x = self.mlp(cont_x)\n        return x\n    \ndef train_fn(train_loader, model, optimizer, epoch, scheduler, device):\n    \n    losses = AverageMeter()\n\n    model.train()\n\n    for step, (cont_x, cate_x, y) in enumerate(train_loader):\n        \n        cont_x, cate_x, y = cont_x.to(device), cate_x.to(device), y.to(device)\n        batch_size = cont_x.size(0)\n\n        pred = model(cont_x, cate_x)\n        \n        loss = nn.BCEWithLogitsLoss()(pred, y)\n        losses.update(loss.item(), batch_size)\n\n        if CFG.gradient_accumulation_steps > 1:\n            loss = loss \/ CFG.gradient_accumulation_steps\n\n        loss.backward()\n        \n        grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), CFG.max_grad_norm)\n\n        if (step + 1) % CFG.gradient_accumulation_steps == 0:\n            scheduler.step()\n            optimizer.step()\n            optimizer.zero_grad()\n        \n    return losses.avg\n\n\ndef validate_fn(valid_loader, model, device):\n    \n    losses = AverageMeter()\n\n    model.eval()\n    val_preds = []\n\n    for step, (cont_x, cate_x, y) in enumerate(valid_loader):\n        \n        cont_x, cate_x, y = cont_x.to(device), cate_x.to(device), y.to(device)\n        batch_size = cont_x.size(0)\n\n        with torch.no_grad():\n            pred = model(cont_x, cate_x)\n            \n        loss = nn.BCEWithLogitsLoss()(pred, y)\n        losses.update(loss.item(), batch_size)\n\n        val_preds.append(pred.sigmoid().detach().cpu().numpy())\n\n        if CFG.gradient_accumulation_steps > 1:\n            loss = loss \/ CFG.gradient_accumulation_steps\n\n    val_preds = np.concatenate(val_preds)\n        \n    return losses.avg, val_preds\n\n\ndef inference_fn(test_loader, model, device):\n\n    model.eval()\n    preds = []\n\n    for step, (cont_x, cate_x) in enumerate(test_loader):\n\n        cont_x,  cate_x = cont_x.to(device), cate_x.to(device)\n\n        with torch.no_grad():\n            pred = model(cont_x, cate_x)\n\n        preds.append(pred.sigmoid().detach().cpu().numpy())\n\n    preds = np.concatenate(preds)\n\n    return preds\n\n\nclass AverageMeter(object):\n    \"\"\"Computes and stores the average and current value\"\"\"\n    def __init__(self):\n        self.reset()\n\n    def reset(self):\n        self.val = 0\n        self.avg = 0\n        self.sum = 0\n        self.count = 0\n\n    def update(self, val, n=1):\n        self.val = val\n        self.sum += val * n\n        self.count += n\n        self.avg = self.sum \/ self.count\ndef run_single_nn(cfg, train, test, folds, num_features, cat_features, target, device, fold_num=0, seed=42):\n    \n    # Set seed\n    logger.info(f'Set seed {seed}')\n    seed_everything(seed=seed)\n\n    # loader\n    trn_idx = folds[folds['fold'] != fold_num].index\n    val_idx = folds[folds['fold'] == fold_num].index\n    train_folds = train.loc[trn_idx].reset_index(drop=True)\n    valid_folds = train.loc[val_idx].reset_index(drop=True)\n    train_target = target[trn_idx]\n    valid_target = target[val_idx]\n    train_dataset = TrainDataset(train_folds, num_features, cat_features, train_target)\n    valid_dataset = TrainDataset(valid_folds, num_features, cat_features, valid_target)\n    train_loader = DataLoader(train_dataset, batch_size=cfg.batch_size, shuffle=True, \n                              num_workers=4, pin_memory=True, drop_last=True)\n    valid_loader = DataLoader(valid_dataset, batch_size=cfg.batch_size, shuffle=False, \n                              num_workers=4, pin_memory=True, drop_last=False)\n\n    # model\n    model = TabularNN(cfg)\n    model.to(device)\n    optimizer = optim.Adam(model.parameters(), lr=cfg.lr)\n    scheduler = optim.lr_scheduler.OneCycleLR(optimizer=optimizer, pct_start=0.1, div_factor=1e3, \n                                              max_lr=1e-2, epochs=cfg.epochs, steps_per_epoch=len(train_loader))\n\n    # log\n    log_df = pd.DataFrame(columns=(['EPOCH']+['TRAIN_LOSS']+['VALID_LOSS']) )\n\n    # train & validate\n    best_loss = np.inf\n    for epoch in range(cfg.epochs):\n        train_loss = train_fn(train_loader, model, optimizer, epoch, scheduler, device)\n        valid_loss, val_preds = validate_fn(valid_loader, model, device)\n        log_row = {'EPOCH': epoch, \n                   'TRAIN_LOSS': train_loss,\n                   'VALID_LOSS': valid_loss,\n                  }\n        log_df = log_df.append(pd.DataFrame(log_row, index=[0]), sort=False)\n        #logger.info(log_df.tail(1))\n        if valid_loss < best_loss:\n            logger.info(f'epoch{epoch} save best model... {valid_loss}')\n            best_loss = valid_loss\n            oof = np.zeros((len(train), len(cfg.target_cols)))\n            oof[val_idx] = val_preds\n            torch.save(model.state_dict(), f\"fold{fold_num}_seed{seed}.pth\")\n\n    # predictions\n    test_dataset = TestDataset(test, num_features, cat_features)\n    test_loader = DataLoader(test_dataset, batch_size=cfg.batch_size, shuffle=False, \n                             num_workers=4, pin_memory=True)\n    model = TabularNN(cfg)\n    model.load_state_dict(torch.load(f\"fold{fold_num}_seed{seed}.pth\"))\n    model.to(device)\n    predictions = inference_fn(test_loader, model, device)\n    \n    # del\n    torch.cuda.empty_cache()\n\n    return oof, predictions\n\n\ndef run_kfold_nn(cfg, train, test, folds, num_features, cat_features, target, device, n_fold=5, seed=42):\n\n    oof = np.zeros((len(train), len(cfg.target_cols)))\n    predictions = np.zeros((len(test), len(cfg.target_cols)))\n\n    for _fold in range(n_fold):\n        logger.info(\"Fold {}\".format(_fold))\n        _oof, _predictions = run_single_nn(cfg,\n                                           train,\n                                           test,\n                                           folds,\n                                           num_features, \n                                           cat_features,\n                                           target, \n                                           device,\n                                           fold_num=_fold,\n                                           seed=seed)\n        oof += _oof\n        predictions += _predictions \/ n_fold\n\n    score = 0\n    for i in range(target.shape[1]):\n        _score = log_loss(target[:,i], oof[:,i])\n        score += _score \/ target.shape[1]\n    logger.info(f\"CV score: {score}\")\n    \n    return oof, predictions\n\ndef just_predict(cfg, test_loader, model_paths, device):\n    test_dataset = TestDataset(test, num_features, cat_features)\n    test_loader = DataLoader(test_dataset, batch_size=cfg.batch_size, shuffle=False, \n                             num_workers=4, pin_memory=True)\n    predictions = np.zeros((len(test), len(cfg.target_cols)))\n    for path in model_paths:\n        model = TabularNN(cfg)\n        model.load_state_dict(torch.load(path))\n        model.to(device)\n        prediction = inference_fn(test_loader, model, device)\n        predictions += prediction \/ len(model_paths)\n        print('Done! ', path)\n    return predictions","f19f891c":"model_paths = glob.glob('..\/input\/moa-model-weights\/pth_0908_dr45\/*.pth')\npredictions = just_predict(CFG, test, model_paths, device)\ntest[target_cols] = predictions\ntest","59651760":"'''# Seed Averaging for solid result\noof = np.zeros((len(train), len(CFG.target_cols)))\npredictions = np.zeros((len(test), len(CFG.target_cols)))\n\nSEED = [0, 1, 2]\nfor seed in SEED:\n    _oof, _predictions = run_kfold_nn(CFG, \n                                      train, test, folds, \n                                      num_features, cat_features, target,\n                                      device,\n                                      n_fold=5, seed=seed)\n    oof += _oof \/ len(SEED)\n    predictions += _predictions \/ len(SEED)\n\nscore = 0\nfor i in range(target.shape[1]):\n    _score = log_loss(target[:,i], oof[:,i])\n    score += _score \/ target.shape[1]\nlogger.info(f\"Seed Averaged CV score: {score}\")'''","cbec40ca":"'''train[target_cols] = oof\ntrain[['sig_id']+target_cols].to_csv('oof.csv', index=False)\n\ntest[target_cols] = predictions\ntest[['sig_id']+target_cols].to_csv('pred.csv', index=False)\n# Final result with 'cp_type'=='ctl_vehicle' data\n\nresult = train_targets_scored.drop(columns=target_cols)\\\n            .merge(train[['sig_id']+target_cols], on='sig_id', how='left').fillna(0)\ny_true = train_targets_scored[target_cols].values\ny_pred = result[target_cols].values\nscore = 0\nfor i in range(y_true.shape[1]):\n    _score = log_loss(y_true[:,i], y_pred[:,i])\n    score += _score \/ y_true.shape[1]\nlogger.info(f\"Final result: {score}\")'''","c283d9ed":"sub = submission.drop(columns=target_cols).merge(test[['sig_id']+target_cols], on='sig_id', how='left').fillna(0)\nsub.to_csv('submission.csv', index=False)\nsub","efe832a7":"Based on [Baseline NN with K-Folds][1] and [keras Multilabel Neural Network v1.2][2]\n\nTry StratifiedKFold\n\n[1]: https:\/\/www.kaggle.com\/stanleyjzheng\/baseline-nn-with-k-folds.\n[2]: https:\/\/www.kaggle.com\/simakov\/keras-multilabel-neural-network-v1-2\n","124fabbe":"# Introduction\n\nThis notebook is maily from https:\/\/www.kaggle.com\/yasufuminakama\/moa-pytorch-nn-starter by @yasufuminakama and https:\/\/www.kaggle.com\/andypenrose\/moa-pytorch-nn-starter-with-weight-decay by @andypenrose. The idea of top_feats is from https:\/\/www.kaggle.com\/simakov\/keras-multilabel-neural-network-v1-2 by @simakov, and I used the features as they are.\n\nI just added just_predict() function to upload submission.csv more rapidly with pretrained models. I have changed dropout from 0.50 to 0.45, because the change improved my local CV slightly (from 0.014961461740598384 to 0.014953314701419588). I also changed nfolds from 5 to 7. Although my local CV got better, the change worsened LB score.\n\nThis notebook is my first public notebook, I apologize for any rudeness.\n"}}