{"cell_type":{"57e89914":"code","227f5ed2":"code","eab3015a":"code","3c300c78":"markdown","ba53893a":"markdown","d9c4a0fc":"markdown"},"source":{"57e89914":"import pandas as pd\nimport numpy  as np","227f5ed2":"sample = pd.read_csv('..\/input\/lish-moa\/sample_submission.csv')\nsample.iloc[:,1:] = 0.0\nsample.to_csv('submission.csv',index=False)","eab3015a":"# read in \nscored     = pd.read_csv('..\/input\/lish-moa\/train_targets_scored.csv')\nsample     = pd.read_csv('..\/input\/lish-moa\/sample_submission.csv')\n\n# calculate\npredictions = []\nfor target_name in list(scored)[1:]:\n    rate = float(sum(scored[target_name])) \/ len(scored)\n    predictions.append(rate)\npredictions = np.array( [predictions] * len(sample) )\n\n# write out\nsample.iloc[:,1:] = predictions\nsample.to_csv('submission.csv',index=False)","3c300c78":"## MoA Baseline calculation\nThe very first thing one should do in any competition is produce a **baseline score** against which one can measure the performance of our model. In a classification problem a good baseline to start from is to calculate the score produced what is known as the [Zero Rule](https:\/\/machinelearningcatalogue.com\/algorithm\/alg_zero-rule.html) classifier (aka. *ZeroR* or *0-R*), and it simply consists of the majority class (i.e. the mode) of the dataset. For example in this case hypothesizing that it is more likely than not that there is no positive response to anything:","ba53893a":"Which produces a score of:\n![image.png](attachment:image.png)\n\nHowever, in this particular competition we are asked to predict probabilities, and not classes. In the book [\"Deep Learning with Python\"](https:\/\/www.manning.com\/books\/deep-learning-with-python) by [Fran\u00e7ois Chollet](https:\/\/fchollet.com\/) he suggests making a *common-sense* baseline. Given that in this competition we are asked to predict probabilities, and we are given a file of scored MoA targets by the competition organizers, this would be a good place to start. In his own words\n\n> [Baseline] *...the highest score you can reach without looking at the test features (or validation features in this case). Let's use the positivity rate of each target as measured in the training subset to generate predictions for the validation subset.*\n\nThe following snippet of code is adapted from Chollet's excellent notebook [\"MoA: Keras + KerasTuner best practices\"](https:\/\/www.kaggle.com\/fchollet\/moa-keras-kerastuner-best-practices):","d9c4a0fc":"Which gives a baseline of:\n![image.png](attachment:image.png)\n\nIt is now our objective in this competition to develop a model that results in a score that is *lower* than this value (but without overfitting!)"}}