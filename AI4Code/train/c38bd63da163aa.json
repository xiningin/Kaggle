{"cell_type":{"8a52245d":"code","fce0c711":"code","ba2cbc4e":"code","0acd4abc":"code","68bcbb2a":"code","5add78fb":"code","5bc48d2c":"code","9c7c739e":"code","96ba7f1b":"code","c1ee7285":"code","a84d7e4f":"code","8733d104":"code","34a4197b":"code","01dccced":"code","80a83d03":"code","1f308f4f":"code","2e6d63a1":"code","78aa26c8":"code","6b39be2d":"code","e1668cb2":"code","82e829d2":"code","9f230d1e":"code","bb0916df":"code","1e1f0082":"code","5c380eab":"code","d015e857":"code","c1e9503a":"code","394844a2":"code","49a3c81b":"code","d088c4d6":"code","d1df91fc":"code","864abb69":"code","d2aba105":"markdown","d178604a":"markdown","72f60b04":"markdown","d5a09aa4":"markdown","c9c95864":"markdown","da1b50d6":"markdown","111babca":"markdown","e3ece12f":"markdown","44b9479a":"markdown","535c9517":"markdown","73063bfd":"markdown","41121237":"markdown","23392672":"markdown","916509a4":"markdown","2dacde7b":"markdown"},"source":{"8a52245d":"import numpy as np\nimport pandas as pd \nimport statsmodels.api as sm\nfrom statsmodels.tools.eval_measures import rmse\nimport statsmodels.formula.api as smf\nimport seaborn as sns\nfrom sklearn.preprocessing import scale \nfrom imblearn.over_sampling import SMOTE\n\nfrom sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score\nfrom sklearn.metrics import confusion_matrix, accuracy_score, classification_report\nfrom sklearn.metrics import roc_auc_score,roc_curve\nimport statsmodels.formula.api as smf\nimport matplotlib.pyplot as plt\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn import tree\n\nfrom sklearn.multiclass import OneVsRestClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom xgboost import XGBClassifier\nfrom lightgbm import LGBMClassifier\nfrom catboost import CatBoostClassifier\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.svm import SVC\n\nfrom sklearn.svm import SVR\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom sklearn.ensemble import GradientBoostingRegressor\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.neural_network import MLPRegressor\nfrom sklearn.ensemble import AdaBoostRegressor\nfrom catboost import CatBoostRegressor\n\nfrom sklearn import metrics\nfrom sklearn.metrics import mean_squared_error \nfrom sklearn.metrics import r2_score\nfrom sklearn.metrics import recall_score\nfrom sklearn.metrics import f1_score\nfrom statsmodels.stats.outliers_influence import variance_inflation_factor\nfrom scipy.stats import shapiro\nfrom sklearn import preprocessing\n\nfrom warnings import filterwarnings\nfilterwarnings('ignore')\n\nfrom sklearn import metrics\nfrom sklearn.tree import plot_tree\nimport matplotlib.pyplot as plt\n\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","fce0c711":"data = pd.read_csv(\"\/kaggle\/input\/real-estate-dataset\/data.csv\")\n\ndf = data.copy()\ndf.head()","ba2cbc4e":"def Missing_Values(data):\n    variable_name=[]\n    total_value=[]\n    total_missing_value=[]\n    missing_value_rate=[]\n    unique_value_list=[]\n    total_unique_value=[]\n    data_type=[]\n    for col in data.columns:\n        variable_name.append(col)\n        data_type.append(data[col].dtype)\n        total_value.append(data[col].shape[0])\n        total_missing_value.append(data[col].isnull().sum())\n        missing_value_rate.append(round(data[col].isnull().sum()\/data[col].shape[0],3))\n        unique_value_list.append(data[col].unique())\n        total_unique_value.append(len(data[col].unique()))\n    missing_data=pd.DataFrame({\"Variable\":variable_name,\"Total_Value\":total_value,\\\n                             \"Total_Missing_Value\":total_missing_value,\"Missing_Value_Rate\":missing_value_rate,\n                             \"Data_Type\":data_type,\"Unique_Value\":unique_value_list,\\\n                               \"Total_Unique_Value\":total_unique_value})\n    return missing_data.sort_values(\"Missing_Value_Rate\",ascending=False)","0acd4abc":"Missing_Values(df)","68bcbb2a":"df.describe().T","5add78fb":"df.drop(\"CHAS\", axis = 1,inplace = True)","5bc48d2c":"Q1 = df.quantile(0.25)\nQ3 = df.quantile(0.75)\nIQR = Q3-Q1\nIQR","9c7c739e":"minn = Q1 - 3 * IQR\nmaxx = Q3 + 3 * IQR\nout_lier = df[(df < minn)  | (df > maxx)].count()\nout_lier","96ba7f1b":"outlier_ratio  = out_lier\/df.count()\noutlier_ratio","c1ee7285":"df_out_lier_cleaned = df[~((df < minn)  | (df > maxx)).any(axis=1)]\ndf_out_lier_cleaned","a84d7e4f":"Missing_Values(df_out_lier_cleaned)","8733d104":"df_out_lier_cleaned[\"RM\"] = df_out_lier_cleaned[\"RM\"].fillna((df_out_lier_cleaned[\"RM\"].median()))","34a4197b":"Missing_Values(df_out_lier_cleaned)","01dccced":"df_out_lier_cleaned.corr()","80a83d03":"X = df_out_lier_cleaned.drop(\"CRIM\", axis = 1)\ny = df_out_lier_cleaned[\"CRIM\"]\nX = sm.add_constant(X)\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 42)\n\nx_train_shape = X_train.shape\nx_test_shape = X_test.shape\ny_train_shape = y_train.shape\ny_test_shape = y_test.shape\n\nprint(\"X train shape: {}\\nX test shape: {}\\n\\ny train shape: {}\\ny test shape: {}\".format(x_train_shape, x_test_shape, y_train_shape, y_test_shape))","1f308f4f":"lin_reg = LinearRegression()\nlin_reg_model = lin_reg.fit(X_train, y_train)\n\ny_pred = lin_reg_model.predict(X_test)\n\n\ntest_set_rmse = (np.sqrt(mean_squared_error(y_test, y_pred)))\ntest_set_r2 = r2_score(y_test, y_pred)\n\nprint(\"R2: {}\\nRMSE: {}\".format(test_set_r2, test_set_rmse))","2e6d63a1":"sns.pairplot(df_out_lier_cleaned, kind = \"reg\")","78aa26c8":"def normal_errors_assumption(model, features, label, p_value_thresh=0.05):\n    \"\"\"\n    Normality: Assumes that the error terms are normally distributed. If they are not,\n    nonlinear transformations of variables may solve this.\n               \n    This assumption being violated primarily causes issues with the confidence intervals\n    \"\"\"\n    from statsmodels.stats.diagnostic import normal_ad\n    print('Assumption 2: The error terms are normally distributed', '\\n')\n    \n    # Calculating residuals for the Anderson-Darling test\n    df_results = calculate_residuals(model, features, label)\n    \n    print('Using the Anderson-Darling test for normal distribution')\n\n    # Performing the test on the residuals\n    p_value = normal_ad(df_results['Residuals'])[1]\n    print('p-value from the test - below 0.05 generally means non-normal:', p_value)\n    \n    # Reporting the normality of the residuals\n    if p_value < p_value_thresh:\n        print('Residuals are not normally distributed')\n    else:\n        print('Residuals are normally distributed')\n    \n    # Plotting the residuals distribution\n    plt.subplots(figsize=(12, 6))\n    plt.title('Distribution of Residuals')\n    sns.distplot(df_results['Residuals'])\n    plt.show()\n    \n    print()\n    if p_value > p_value_thresh:\n        print('Assumption satisfied')\n    else:\n        print('Assumption not satisfied')\n        print()\n        print('Confidence intervals will likely be affected')\n        print('Try performing nonlinear transformations on variables')\n        \n        \n        \ndef calculate_residuals(model, features, label):\n    \"\"\"\n    Creates predictions on the features with the model and calculates residuals\n    \"\"\"\n    predictions = model.predict(features)\n    df_results = pd.DataFrame({'Actual': label, 'Predicted': predictions})\n    df_results['Residuals'] = abs(df_results['Actual']) - abs(df_results['Predicted'])\n    \n    return df_results        ","6b39be2d":"normal_errors_assumption(lin_reg_model, X_train, y_train)","e1668cb2":"plt.figure(figsize = (10,8))\nsns.heatmap(pd.DataFrame(df_out_lier_cleaned).corr(), annot=True)\nplt.title('Correlation of Variables')\nplt.show()","82e829d2":"def variance_inflation_factor(exog, exog_idx):\n    \"\"\"\n    exog : ndarray, (nobs, k_vars)\n        design matrix with all explanatory variables, as for example used in\n        regression\n    exog_idx : int\n        index of the exogenous variable in the columns of exog\n    \"\"\"\n    k_vars = exog.shape[1]\n    x_i = exog[:, exog_idx]\n    mask = np.arange(k_vars) != exog_idx\n    x_noti = exog[:, mask]\n    r_squared_i = sm.OLS(x_i, x_noti).fit().rsquared\n    vif = 1. \/ (1. - r_squared_i)\n    return vif\n\ndef calculate_vif_(X, thresh=10.0):\n    variables = list(range(X.shape[1]))\n    dropped = True\n    while dropped:\n        dropped = False\n        vif = [variance_inflation_factor(X.iloc[:, variables].values, ix)\n               for ix in range(X.iloc[:, variables].shape[1])]\n\n        maxloc = vif.index(max(vif))\n        if max(vif) > thresh:\n            print('dropping \\'' + X.iloc[:, variables].columns[maxloc] +\n                  '\\' at index: ' + str(maxloc))\n            del variables[maxloc]\n            dropped = True\n\n    print('Remaining variables:')\n    print(X.columns[variables])\n    return X.iloc[:, variables]","9f230d1e":"calculate_vif_(X_train)","bb0916df":"X = df_out_lier_cleaned.drop([\"CRIM\",\"RM\", \"B\", \"NOX\", \"PTRATIO\", \"TAX\", \"AGE\"], axis = 1)\ny = df_out_lier_cleaned[\"CRIM\"]\n#X = sm.add_constant(X)\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 42)\n\nx_train_shape = X_train.shape\nx_test_shape = X_test.shape\ny_train_shape = y_train.shape\ny_test_shape = y_test.shape\n\nprint(\"X train shape: {}\\nX test shape: {}\\n\\ny train shape: {}\\ny test shape: {}\".format(x_train_shape, x_test_shape, y_train_shape, y_test_shape))","1e1f0082":"ols_model = sm.OLS(y_train, X_train)\nols_results = ols_model.fit()\nprint(ols_results.summary())","5c380eab":"X = df_out_lier_cleaned.drop([\"CRIM\",\"RM\", \"B\", \"NOX\", \"PTRATIO\", \"TAX\", \"AGE\", \"INDUS\", \"ZN\"], axis = 1)\ny = df_out_lier_cleaned[\"CRIM\"]\n#X = sm.add_constant(X)\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 42)\n\nx_train_shape = X_train.shape\nx_test_shape = X_test.shape\ny_train_shape = y_train.shape\ny_test_shape = y_test.shape\n\nprint(\"X train shape: {}\\nX test shape: {}\\n\\ny train shape: {}\\ny test shape: {}\".format(x_train_shape, x_test_shape, y_train_shape, y_test_shape))","d015e857":"ols_model = sm.OLS(y_train, X_train)\nols_results = ols_model.fit()\ny_pred = ols_results.predict(X_test)\n\n# calc rmse\nols_rmse = rmse(y_test, y_pred)\n\nprint(ols_results.summary())","c1e9503a":"ols_rmse","394844a2":"def autocorrelation_assumption(model, features, label):\n    \"\"\"\n    Autocorrelation: Assumes that there is no autocorrelation in the residuals. If there is\n                     autocorrelation, then there is a pattern that is not explained due to\n                     the current value being dependent on the previous value.\n                     This may be resolved by adding a lag variable of either the dependent\n                     variable or some of the predictors.\n    \"\"\"\n    from statsmodels.stats.stattools import durbin_watson\n    print('Assumption 4: No Autocorrelation', '\\n')\n    \n    # Calculating residuals for the Durbin Watson-tests\n    df_results = calculate_residuals(model, features, label)\n\n    print('\\nPerforming Durbin-Watson Test')\n    print('Values of 1.5 < d < 2.5 generally show that there is no autocorrelation in the data')\n    print('0 to 2< is positive autocorrelation')\n    print('>2 to 4 is negative autocorrelation')\n    print('-------------------------------------')\n    durbinWatson = durbin_watson(df_results['Residuals'])\n    print('Durbin-Watson:', durbinWatson)\n    if durbinWatson < 1.5:\n        print('Signs of positive autocorrelation', '\\n')\n        print('Assumption not satisfied')\n    elif durbinWatson > 2.5:\n        print('Signs of negative autocorrelation', '\\n')\n        print('Assumption not satisfied')\n    else:\n        print('Little to no autocorrelation', '\\n')\n        print('Assumption satisfied')","49a3c81b":"autocorrelation_assumption(ols_results, X_train, y_train)","d088c4d6":"def homoscedasticity_assumption(model, features, label):\n    \"\"\"\n    Homoscedasticity: Assumes that the errors exhibit constant variance\n    \"\"\"\n    print('Assumption 5: Homoscedasticity of Error Terms', '\\n')\n    \n    print('Residuals should have relative constant variance')\n        \n    # Calculating residuals for the plot\n    df_results = calculate_residuals(model, features, label)\n\n    # Plotting the residuals\n    plt.subplots(figsize=(12, 6))\n    ax = plt.subplot(111)  # To remove spines\n    plt.scatter(x=df_results.index, y=df_results.Residuals, alpha=0.5)\n    plt.plot(np.repeat(0, df_results.index.max()), color='darkorange', linestyle='--')\n    ax.spines['right'].set_visible(False)  # Removing the right spine\n    ax.spines['top'].set_visible(False)  # Removing the top spine\n    plt.title('Residuals')\n    plt.show()  ","d1df91fc":"homoscedasticity_assumption(ols_results, X_train, y_train)","864abb69":"normal_errors_assumption(ols_results, X_train, y_train)","d2aba105":"### No Multicollinearity among Predictors","d178604a":"Let's apply train test split to data","72f60b04":"Fill na values","d5a09aa4":"Extreme Cleaning:\nlower_threshold: Below the first quartile (Q1) \u2212 3 \u2217 interquartile range (IQR)\nupper_threshold: Above the third quartile (Q3) + 3 \u2217 interquartile range (IQR)\n    \nStandard Cleaning:\nlower_threshold: Below the first quartile (Q1) \u2212 1.5 \u2217 interquartile range (IQR)\nupper_threshold: Above the third quartile (Q3) + 1.5 \u2217 interquartile range (IQR)\nwhere 'IQR' is equal to [Q3-Q1]=[third_quartile - first_quartile]. This technique has disadvantages while dealing with sparse columns, since taking the middle lenght (75%-25%) may bring zero distanced IQR.\n","c9c95864":"### Homoscedasticity","da1b50d6":"# Linear Regression Assumptions\n\n## * Linearity \n## * Normality of the Error Terms \n## * No Multicollinearity among Predictors \n## * No Autocorrelation of the Error Terms\n## * Homoscedasticity","111babca":"### Linearity","e3ece12f":"Dropped ZN and INDUS because P value bigger than 0.05 so they do not contribute significantly to the model","44b9479a":"Some functions belong to **Jeff Macaluso**\n\n\nI wonder your thoughts about the notebook","535c9517":"### Normality of the Error Terms","73063bfd":"**We have to solve the  normal errorr assumption**","41121237":"### No Autocorrelation of the Error Terms","23392672":"#### Handling outlier","916509a4":"The notebook will be updated in the future","2dacde7b":"\na quick look at the data"}}