{"cell_type":{"cb3bc89b":"code","dce9b373":"code","a5913ecd":"code","cca1577b":"code","d32d6630":"code","1ae93a29":"code","01f76900":"code","b00529c6":"code","eb76b871":"code","3414c39a":"code","f00be38d":"code","6f915e2b":"code","68f36218":"code","c4a0dfd9":"code","e8f9198d":"code","967c7c25":"code","558ab2aa":"code","43fa4ce7":"code","6f004f5a":"code","211c4575":"code","119217ae":"code","4414d474":"code","08075661":"code","ea7438dc":"code","b8f747fa":"code","c76e64a9":"code","a557c022":"code","637bb432":"code","ef02265c":"code","aa54973b":"code","0dcbfc97":"code","b481baa2":"code","b46d4391":"code","ad0bac02":"markdown","93903d52":"markdown","6c77fabd":"markdown","9206fe90":"markdown","4c9486d3":"markdown","9fce037d":"markdown","3f6302e9":"markdown","944b7db0":"markdown","d7feeffa":"markdown","afca92bf":"markdown","7062c432":"markdown","820a04c0":"markdown","eba1667b":"markdown","5c0eb4fa":"markdown","1d9bd01b":"markdown","2af68416":"markdown","0c983bc2":"markdown","7c5f4a04":"markdown"},"source":{"cb3bc89b":"!pip install scispacy\n!pip install https:\/\/s3-us-west-2.amazonaws.com\/ai2-s2-scispacy\/releases\/v0.2.4\/en_core_sci_lg-0.2.4.tar.gz\n","dce9b373":"import numpy as np \nimport pandas as pd\nimport scispacy\nimport spacy\nimport en_core_sci_lg\nfrom spacy.matcher import PhraseMatcher\nfrom sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\nfrom tqdm import tqdm","a5913ecd":"import numpy as np\nimport pandas as pd\n\nroot_path = '\/kaggle\/input\/CORD-19-research-challenge\/'\nmetadata_path = f'{root_path}\/metadata.csv'\nmeta_df = pd.read_csv(metadata_path)\nmeta_df.head()","cca1577b":"covid_research_papers = meta_df[meta_df['abstract'].astype(str).str.contains('COVID-19|SARS-CoV-2|2019-nCov|SARS Coronavirus 2|2019 Novel Coronavirus')]\ncovid_abstract = covid_research_papers.abstract\ncovid_abstract.shape","d32d6630":"nlp = en_core_sci_lg.load()\n\n#Tokenizing and simple preprocessing of the documents to remove stop words, stemming and lemmatization of the words.\n\ndef spacy_tokenizer(sentence):\n    return [word.lemma_ for word in nlp(sentence) if not (word.like_num or word.is_stop or word.is_punct or word.is_space or len(word)==1)]\n","1ae93a29":"vectorizer = TfidfVectorizer(tokenizer = spacy_tokenizer, min_df=2)\ndata_vectorized = vectorizer.fit_transform(tqdm(covid_abstract.values.astype('U')))\ndata_vectorized.shape","01f76900":"# most frequent words\nword_count = pd.DataFrame({'word': vectorizer.get_feature_names(), 'sum of tf-idf': np.asarray(data_vectorized.sum(axis=0))[0]})\n\nword_count.sort_values('sum of tf-idf', ascending=False).set_index('word')[:20].sort_values('sum of tf-idf', ascending=True).plot(kind='barh')","b00529c6":"def compute_cosine_similarity(doc_features, corpus_features, top_n=10):\n    # get document vectors\n    doc_features = doc_features.toarray()[0]\n    corpus_features = corpus_features.toarray()\n    # compute similarites\n    similarity = np.dot(doc_features, corpus_features.T)\n    # get docs with highest similarity scores\n    top_docs = similarity.argsort()[::-1][:top_n]\n    top_docs_with_score = [(index, round(similarity[index], 3)) for index in top_docs]\n    \n    return top_docs_with_score","eb76b871":"from IPython.display import display, HTML\nimport numpy as np\n#Find the 10 most releveant papers to a given query and display them\ndef SearchDocuments(Query):\n    query_docs_tfidf = vectorizer.transform(Query) #Vectorizing and calculating tf-idf for the query\n\n    for index, doc in enumerate(Query):\n        doc_tfidf = query_docs_tfidf[index]\n        #Computing Cosine similarty between the query and the abstracts and get the 10 most relevant\n        top_similar_docs = compute_cosine_similarity(doc_tfidf, data_vectorized, top_n=10)\n        \n        df = pd.DataFrame()\n        Score=[]\n        for doc_index, sim_score in top_similar_docs :\n            #Getting the full data of the 10 most relevant papers and add them to the dataframe\n            data =meta_df.loc[meta_df['cord_uid'] == covid_research_papers.cord_uid.values[doc_index]]\n            Score.append(str(sim_score))\n            df = df.append(data)\n\n        df['Score']=Score\n        # Display the relevant papers in a table\n        DisplayTable(df)\n        \ndef AnswerSearchQuery(Query):\n    query_docs_tfidf = vectorizer.transform(Query) #Vectorizing and calculating tf-idf for the query\n\n    for index, doc in enumerate(Query):\n        doc_tfidf = query_docs_tfidf[index]\n        #Computing Cosine similarty between the query and the abstracts and get the 10 most relevant\n        top_similar_docs = compute_cosine_similarity(doc_tfidf, data_vectorized, top_n=1)\n        result = covid_abstract.values[top_similar_docs[0][0]].split('Results: ')\n        if(len(result)==1):\n            print(covid_abstract.values[top_similar_docs[0][0]])\n        else:\n            print(result[1])","3414c39a":"\n#Displaying the dataframe in a table and styling\ndef DisplayTable(df):\n    df = df.replace(np.nan, '', regex=True)\n    df['Title'] = df['title'] + '#' + df['url']\n    df =df[['Title','publish_time','abstract','Score']]\n    dfStyler =df.style.format({'Title': make_clickable_both,'text-align': 'right'})\n    dfStyler = dfStyler.set_properties(**{'text-align': 'left'})\n    dfStyler=dfStyler.set_table_styles([dict(selector='th', props=[('text-align', 'center')])])\n    display(HTML(dfStyler.render()))\n#Making the title of the paper in the table as Hyperlink to get access to the full text paper    \ndef make_clickable_both(val): \n    \n    name, url = val.split('#')\n    if(url==''):\n        return name\n    return f'<a href=\"{url}\">{name}<\/a>'\n","f00be38d":"SearchDocuments(['COVID-19 risk factors'])","6f915e2b":"SearchDocuments(['Data on potential risks factors'])","68f36218":"SearchDocuments(['Risk factors such as Smoking, pre-existing pulmonary disease'])","c4a0dfd9":"SearchDocuments(['Risk factors such as Co-infections (determine whether co-existing respiratory\/viral infections make the virus more transmissible or virulent) and other co-morbidities'])","e8f9198d":"SearchDocuments(['Risk factors for Neonates and pregnant women'])","967c7c25":"SearchDocuments(['Risk factors for Socio-economic and behavioral factors to understand the economic impact of the virus and whether there were differences'])","558ab2aa":"SearchDocuments(['Transmission dynamics of the virus, including the basic reproductive number, incubation period, serial interval, modes of transmission and environmental factors'])","43fa4ce7":"SearchDocuments(['Severity of disease, including risk of fatality among symptomatic hospitalized patients, and high-risk patient groups'])","6f004f5a":"SearchDocuments(['Susceptibility of populations'])","211c4575":"SearchDocuments(['Public health mitigation measures that could be effective for control'])","119217ae":"SearchDocuments(['antiviral treatment'])","4414d474":"SearchDocuments(['risk factors such as age'])","08075661":"SearchDocuments(['risk factors such as pollution'])","ea7438dc":"SearchDocuments(['risk factors such as population density'])","b8f747fa":"SearchDocuments(['risk factors such as humidity'])","c76e64a9":"SearchDocuments(['risk factors such as heart risks'])","a557c022":"SearchDocuments(['risk factors such as temperature'])","637bb432":"AnswerSearchQuery(['Risk factors such as Smoking, pre-existing pulmonary disease'])","ef02265c":"AnswerSearchQuery(['Transmission dynamics of the virus, including the basic reproductive number, incubation period, serial interval, modes of transmission and environmental factors'])","aa54973b":"AnswerSearchQuery(['Risk factors for Neonates and pregnant women'])","0dcbfc97":"AnswerSearchQuery(['COVID-19 risk factors'])","b481baa2":"print('Please Write down your own Query')","b46d4391":"Query= input()\nSearchDocuments([Query])","ad0bac02":"**Research Goal**\n\nMain goal of this research is to analyze the data and find Risk Factors of COVID-19 <hr\/>","93903d52":"# **Queries About Covid-19 Risk Factors**","6c77fabd":"# **Extracting data for COVID-19 risk factors  ** <hr\/> \n","9206fe90":"# Graph TF-IDF ","4c9486d3":"# Answering queries\nWe noticed that data preprocessing is done so well that our system returns answers to most of the queries. Below are examples for queries with the corresponding answer returned by our system.","9fce037d":"# Installations\nInstalling the full spaCy pipeline for biomedical data with a large vocabulary and 600k word vectors. Other smaller and similar models are available also. Check [sciSpacy documentation](https:\/\/allenai.github.io\/scispacy\/) for more details","3f6302e9":"**Methodology**: \n1. Due to the huge size of the data and our specific main goal concerning Covid-19, we extracted a number of abstracts that openly studied Covid-19 and its derivatives.\n2. Tokenization and data preprocessing such as stemming, lemmatization and removing stop words was done using sciSpacy on the abstracts. [scispaCy](https:\/\/allenai.github.io\/scispacy\/) is a Python package containing spaCy models for processing biomedical, scientific or clinical text.\n3. The abstracts were embedded into a TF-IDF Model to calculate the TF-IDF vectors. \n4. The cosine similarity of a **dynamic user query** was calculated against each of the abtracts. \n5. The next step is to sort the most similar papers to the given query and display the 10 most relevant papers. \n\nThis allows the medical research community, governments, and decision-makers to rapidly consult the latest findings and discoveries in a given knowledge area. **Also the dynamic search queries will help researchers to search for anything related to Covid-19 **. Links to the full-text research paper are also embedded and directly clickable in the output dataframe. <hr\/>","944b7db0":"# Define method to get top 10 documents for a search query and a method that answers any query","d7feeffa":"# Training the model on the research papers using Tf-Idf Vectorizer","afca92bf":"# Define Methods to display the results in a table","7062c432":"# Imports","820a04c0":"# Load Sci Model and Define Tokenizer","eba1667b":"# Load Metadata\nLoading the Metadata as we will do all the work on the abstract of the research papers only.","5c0eb4fa":"**This notebook has been used to provide insights into the ongoing struggle against this infectious disease through various natural language processing (NLP) techniques. The rapid growth of new literature on coronaviruses is making it extremely difficult for the medical research community  to keep up with the recent updates. As a result, there is a huge demand for these approaches in order to help the scientific research community.\n\nText data is not random, but has linguistic properties, which makes it very understandable for other people and computer-processable <hr\/>\n","1d9bd01b":"# **User Dynamic Quer**y","2af68416":"**Pros:**\n1. An efficient query answering system that returns the most relevant literature.\n1. Easy and Rapid Access to the Latest Findings in a Given knowledge area.\n\n\n**Cons:**\n\n1. An abstract of a research paper is a only partial description.\n1. Reduced scope: 10% research papers were analyzed as they were the Covid-19 related research papers.\n\n<hr\/>","0c983bc2":"# Extract relevant research papers only \nExtracting research papers related to Covid-19 and its derivates as this is what we are interested in.","7c5f4a04":"# Define Cosine Similirity to get top n documents"}}