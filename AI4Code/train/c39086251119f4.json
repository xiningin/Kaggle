{"cell_type":{"4d06cc5c":"code","f44d6854":"code","ce9ac880":"code","0c39b2a2":"code","69651d04":"code","7145cceb":"code","bcfa003f":"code","85430cc3":"code","38630b64":"code","e78aafe7":"code","81cef440":"code","d690d161":"code","993bfe38":"code","e3b20efa":"code","d6be84b4":"code","3ae72da2":"code","8fdea461":"code","3cffc662":"code","31e4f39f":"code","10fc4737":"code","c3d02024":"code","a4cca3f2":"code","4cee4f70":"code","77d3817c":"code","85d38de6":"code","9eefd60f":"code","979d72cc":"code","719c5ad0":"code","34469a20":"code","36b5118d":"code","74e4c857":"code","64706f26":"code","28404b30":"code","fc68be03":"code","15114e25":"code","8af368f0":"markdown","2cbc22a4":"markdown","46869dee":"markdown","1041250c":"markdown","e8fe69ba":"markdown","56576862":"markdown","3116b141":"markdown","56bc786a":"markdown","93fb6ecb":"markdown","7c60df59":"markdown","3cd606a6":"markdown","e57579e6":"markdown","027bedf0":"markdown","64da54cf":"markdown","b4c29b49":"markdown","1fad8912":"markdown","7a3132f9":"markdown","8a2e52b9":"markdown","493e23f5":"markdown","5bce9e54":"markdown","eca5ce43":"markdown","deaab5ab":"markdown"},"source":{"4d06cc5c":"import numpy as np\nimport pandas as pd\nimport seaborn as sns\nfrom pylab import rcParams\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport lightgbm as lgb\nfrom sklearn.model_selection import KFold, cross_val_score\nfrom sklearn.preprocessing import OrdinalEncoder\nfrom sklearn.model_selection import RandomizedSearchCV\n","f44d6854":"train = pd.read_csv('..\/input\/30-days-of-ml\/train.csv')\ntest = pd.read_csv('..\/input\/30-days-of-ml\/test.csv')\nsample = pd.read_csv('..\/input\/30-days-of-ml\/sample_submission.csv')\n\n# dropping Id column\ntrain.drop(columns = 'id', inplace = True)\ntest.drop(columns = 'id', inplace = True)","ce9ac880":"print('Train Shape', train.shape, '\\nTest Shape', test.shape)","0c39b2a2":"train.info()","69651d04":"train.describe().T","7145cceb":"print('Null in Train', train.isnull().sum().sum())\nprint('Null in Test', test.isnull().sum().sum())\nprint('Train Duplicate data', train.duplicated().sum())\nprint('Test Duplicate data', test.duplicated().sum())","bcfa003f":"# lets segregate the cont and cat datasets for data visualisation and cat encoding\n\nprint(train.columns, '\\n')\ntrain_cont = train.select_dtypes(include=[np.number])\ntrain_cat = train.select_dtypes(exclude=[np.number])\n\nprint('cat shape', train_cat.shape)\nprint('cont shape', train_cont.shape)","85430cc3":"hist = train_cont.hist(figsize = (25, 12), bins=50, grid = False,\n                       xlabelsize=8, ylabelsize=8, layout = (5,3))\nplt.tight_layout()","38630b64":"plt.figure(figsize=(30,15))\n\nfor i, column in enumerate(train_cont.columns, 1):\n    plt.subplot(3,5,i)\n    sns.histplot(train_cont[column], kde = True)\nplt.suptitle(\"Feature distribution\", fontsize=30)\nplt.tight_layout()","e78aafe7":"plt.figure(figsize=(18,6))\nax = sns.boxplot(data=train_cont, orient = 'h', palette=\"Set3\")\nplt.title('Box plot of numerical columns', fontsize=16);","81cef440":"print(train.columns, '\\n')\ntrain_cont = train.select_dtypes(include=[np.number])\ntrain_cat = train.select_dtypes(exclude=[np.number])\n\nprint('train cat shape', train_cat.shape)\nprint('train cont shape', train_cont.shape)","d690d161":"print(test.columns, '\\n')\n\ntest_cont = test.select_dtypes(include = [np.number])\ntest_cat = test.select_dtypes(exclude = [np.number])\n\nprint('test cat shape', test_cat.shape)\nprint('test cont shape', test_cont.shape)","993bfe38":"from sklearn.preprocessing import StandardScaler\nss = StandardScaler()\nscale_train_cont = pd.DataFrame(ss.fit_transform(train_cont.drop('target', axis = 1)), \n                                columns = train_cont.columns[:-1])\n# scale_train_cont","e3b20efa":"y = train.iloc[:,-1]\n\nplt.figure(figsize=(8,5))\nplt.title('target')\nsns.histplot(y, kde = True);","d6be84b4":"# log transformation\n\ny_log = np.log(y)\n\nplt.figure(figsize=(8,5))\nplt.title('target')\nsns.histplot(y_log, kde = True);","3ae72da2":"from sklearn.preprocessing import OneHotEncoder\noh_encoder = OneHotEncoder(handle_unknown='ignore', sparse=False)\ntrain_cat = train.select_dtypes(exclude=[np.number])\noh_cat = pd.DataFrame(oh_encoder.fit_transform(train_cat))\n# oh_cat.head(5)","8fdea461":"# Non-Transformed x \noh_x_nscaled = pd.concat([oh_cat, train_cont.drop(columns = 'target')], axis = 1)\n#oh_x_nscaled.head(5)\n\n# scaled x \noh_x_scaled = pd.concat([oh_cat, scale_train_cont], axis = 1)\n#oh_x_scaled.head(5)","3cffc662":"# prepare the cross-validation procedure\ncv = KFold(n_splits=5, random_state=2, shuffle=True)\n# create model\nmodel = lgb.LGBMRegressor(random_state = 2 )\n\n\n# Model with one hot encoding + non-transformed y + non-transformed x\nscores = cross_val_score(model, oh_x_nscaled, y, cv = cv, scoring = \"neg_mean_absolute_error\")\nprint('LGBM CV baseline score for - One hot encoded + non-transformed y + non-transformed x : ', -scores.mean())\n\n\n# Model with one hot encoding + non-transformed y + transformed x\nscores = cross_val_score(model, oh_x_scaled, y, cv = cv, scoring = \"neg_mean_absolute_error\")\nprint('LGBM CV baseline score for - One hot encoded + non-transformed y + transformed x', -scores.mean())\n\n\n# Model with one hot encoding + transformed y + non-transformed x\nscores = cross_val_score(model, oh_x_nscaled, y_log, cv = cv, scoring = \"neg_mean_absolute_error\")\nprint('LGBM CV baseline score for - One hot encoded + transformed y + non-transformed x' , -scores.mean())\n\n\n# Model with one hot encoding + transformed y + transformed x\nscores = cross_val_score(model, oh_x_scaled, y_log, cv = cv, scoring = \"neg_mean_absolute_error\")\nprint('LGBM CV baseline score for - One hot encoded + transformed y + transformed x' , -scores.mean())","31e4f39f":"'''\n# Data for submission\n\nmodel.fit(oh_x_scaled, y_log)\n\ntest_cont = test.select_dtypes(include = [np.number])\nfrom sklearn.preprocessing import StandardScaler\nss = StandardScaler()\nscale_test_cont = pd.DataFrame(ss.fit_transform(test_cont), columns = test_cont.columns)\n\ntest_cat = test.select_dtypes(exclude = [np.number])\noh_test_cat = pd.DataFrame(oh_encoder.fit_transform(test_cat))\n\nfinal_test = pd.concat([oh_test_cat, scale_test_cont], axis = 1)\n\nsample[\"target\"] = np.exp(model.predict(final_test))\nsample.to_csv('submission-> OneHot and scaled_x and y_log.csv', index = False)\n\n'''\nprint('Kaggle submission score - one hot encoded + transformed y + transformed x is 0.72748')\nprint('Kaggle submission score - one hot encoded + non-transformed y + non-transformed x is 0.72600')","10fc4737":"print(train_cat.columns)\nprint(train_cont.columns)","c3d02024":"# Creating the Frequency encoding of train data\nfreq_cat = pd.DataFrame()\nfor i in range (10):\n  # print(f'freq_cat{i}')\n  enc_nom = (train_cat.groupby(f'cat{i}').size()) \/ len(train_cat)\n  freq_cat[f'freq_cat{i}'] = train_cat[f'cat{i}'].apply(lambda x :enc_nom[x])\n# freq_cat\n\n\n# scaling of cont variables\nfrom sklearn.preprocessing import StandardScaler\nss = StandardScaler()\nscale_train_cont = pd.DataFrame(ss.fit_transform(train_cont.drop('target', axis = 1)), \n                                columns = train_cont.columns[:-1])\n# scale_train_cont","a4cca3f2":"# Non transformed x \nfreq_x_nscaled = pd.concat([freq_cat, train_cont], axis = 1)\nfreq_x_nscaled\n\n# scaled x \nfreq_scaled_x = pd.concat([freq_cat, scale_train_cont], axis = 1)\n#freq_scaled_x","4cee4f70":"# prepare the cross-validation procedure\ncv = KFold(n_splits=5, random_state=2, shuffle=True)\n\n# create model\nmodel = lgb.LGBMRegressor(random_state = 2 )\n\n\n# evaluate model - Frequency encoded + non-transformed y + non-transformed x\ny = freq_x_nscaled['target']\nx = freq_x_nscaled.drop(columns = 'target')\nscores = cross_val_score(model, x, y, cv = cv, scoring = \"neg_mean_absolute_error\")\nprint('LGBM CV baseline score for - Frequency encoded + non-transformed y + non-transformed x' , -scores.mean())\n\n\n# evaluate model - Frequency encoded + non-transformed y + transformed x\ny = train_cont['target']\nx = freq_scaled_x\nscores = cross_val_score(model, x, y, cv = cv, scoring = \"neg_mean_absolute_error\")\nprint('LGBM CV baseline score for - Frequency encoded + non-transformed y + transformed x' , -scores.mean())","77d3817c":"''''\nmodel.fit(x,y)\n\ntest_cont = test.select_dtypes(include = [np.number])\ntest_cat = test.select_dtypes(exclude = [np.number])\n\nfreq_test_cat = pd.DataFrame()\nfor i in range (10):\n  # print(f'freq_cat{i}')\n  enc_nom = (test_cat.groupby(f'cat{i}').size()) \/ len(test_cat)\n  freq_test_cat[f'freq_cat{i}'] = test_cat[f'cat{i}'].apply(lambda x :enc_nom[x])\n\nfinal_test = pd.concat([freq_test_cat, test_cont], axis = 1)\nfinal_test\n\nsample[\"target\"] = model.predict(final_test)\nsample.to_csv('submission-> Freq encoding and x and y.csv', index = False)\n\n'''\nprint('Kaggle submission score - Frequency encoded + non-transformed y + non transformed x is 0.72630')\n\n'''\n# Data for submission\n\nmodel.fit(x,y)\n\ntest_cont = test.select_dtypes(include = [np.number])\nfrom sklearn.preprocessing import StandardScaler\nss = StandardScaler()\nscale_test_cont = pd.DataFrame(ss.fit_transform(test_cont), columns = test_cont.columns)\n\ntest_cat = test.select_dtypes(exclude = [np.number])\nfreq_test_cat = pd.DataFrame()\nfor i in range (10):\n  # print(f'freq_cat{i}')\n  enc_nom = (test_cat.groupby(f'cat{i}').size()) \/ len(test_cat)\n  freq_test_cat[f'freq_cat{i}'] = test_cat[f'cat{i}'].apply(lambda x :enc_nom[x])\n\nfinal_test = pd.concat([freq_test_cat, scale_test_cont], axis = 1)\nfinal_test\nsample[\"target\"] = model.predict(final_test)\nsample.to_csv('submission-> Freq encoding and scaled x and y.csv', index = False)\n'''\n\nprint('Kaggle submission score - Frequency encoded + non-transformed y + transformed x is 0.72668')","85d38de6":"print(train_cat.columns)\nprint(train_cont.columns)","9eefd60f":"# ordinal encoding\nordinal_encoder = OrdinalEncoder()\nor_train_cat_x = pd.DataFrame(ordinal_encoder.fit_transform(train_cat))\n\n\n#feature scaling\nfrom sklearn.preprocessing import StandardScaler\nss = StandardScaler()\nscale_train_cont = pd.DataFrame(ss.fit_transform(train_cont.drop('target', axis = 1)), \n                                columns = train_cont.columns[:-1])\n# scale_train_cont","979d72cc":"# non transformed x and non transformed y\nor_enc_x = pd.concat([or_train_cat_x, train_cont], axis = 1)\n# or_enc_x\n\n# transformed x y\nor_enc_scaled_x = pd.concat([or_train_cat_x, scale_train_cont], axis = 1)\n# or_enc_scaled_x","719c5ad0":"# prepare the cross-validation procedure\ncv = KFold(n_splits=5, random_state=2, shuffle=True)\n\n# create model\nmodel = lgb.LGBMRegressor(random_state = 2 )\n\n\n# evaluate model - Ordinal encoded + non transformed y + non-transformed x\ny = or_enc_x.iloc[:,-1]\nx = or_enc_x.iloc[:,:-1]\nscores = cross_val_score(model, x, y, cv = cv, scoring = \"neg_mean_absolute_error\")\nprint('LGBM CV baseline score - ordinal encoded + non transformed y + non-transformed x :' , -scores.mean())\n\n\n# evaluate model - ordinal encoded + original y + scaled x \ny = train_cont.iloc[:,-1]\nx = or_enc_scaled_x\nscores = cross_val_score(model, x, y, cv = cv, scoring = \"neg_mean_absolute_error\")\nprint('LGBM CV baseline score - ordinal encoded + original y + scaled x ' , -scores.mean())","34469a20":"\n'''\n\nmodel.fit(x,y)\n\ntest_cont = test.select_dtypes(include = [np.number])\n\ntest_cat = test.select_dtypes(exclude = [np.number])\nor_test_cat_x = pd.DataFrame(ordinal_encoder.fit_transform(test_cat))\n\n\nfinal_test = pd.concat([or_test_cat_x, test_cont], axis = 1)\nfinal_test\n'''\nprint('Kaggle submission score - ordinal encoded + non-transformed y + non transformed x is 0.72647')\n\n\n'''\n# Data for submission\n\nmodel.fit(x,y)\n\ntest_cont = test.select_dtypes(include = [np.number])\nfrom sklearn.preprocessing import StandardScaler\nss = StandardScaler()\nscale_test_cont = pd.DataFrame(ss.fit_transform(test_cont), columns = test_cont.columns)\n\ntest_cat = test.select_dtypes(exclude = [np.number])\nor_test_cat_x = pd.DataFrame(ordinal_encoder.fit_transform(test_cat))\n\n\nfinal_test = pd.concat([or_test_cat_x, scale_test_cont], axis = 1)\nfinal_test\n\n'''\nprint('Kaggle submission score - ordinal encoded + non-transformed y + transformed x is 0.72671')","36b5118d":"print(train.columns, '\\n')\ntrain_cont = train.select_dtypes(include=[np.number])\ntrain_cat = train.select_dtypes(exclude=[np.number])\n\nprint('cat shape', train_cat.shape)\nprint('cont shape', train_cont.shape)\n\n\nprint(test.columns, '\\n')\ntest_cont = test.select_dtypes(include = [np.number])\ntest_cat = test.select_dtypes(exclude = [np.number])\n\nprint('test cat shape', test_cat.shape)\nprint('test cont shape', test_cont.shape)","74e4c857":"from sklearn.preprocessing import OneHotEncoder\n\noh_encoder = OneHotEncoder(handle_unknown='ignore', sparse=False)\n\noh_train_cat = pd.DataFrame(oh_encoder.fit_transform(train_cat))\noh_test_cat = pd.DataFrame(oh_encoder.fit_transform(test_cat))","64706f26":"# Final test and train dataset with one hot encoding\noh_train_x = pd.concat([oh_train_cat, train_cont.iloc[:,:-1]], axis = 1)\nprint('final x_train shape', oh_train_x.shape)\n\noh_test_x = pd.concat([oh_test_cat, test_cont], axis = 1)\nprint('final test shape', oh_test_x.shape)\n\ny_train = train_cont.iloc[:,-1]\nprint('final y_train shape', y_train.shape)","28404b30":"print(\"Best parameters {'random_state': 2, 'num_leaves': 50, 'n_estimators': 300, 'max_depth': 75}\")\nprint(\"Best score 0.574546807667127\")","fc68be03":"  model = lgb.LGBMRegressor(num_leaves = 50, max_depth = 75,\n                          n_estimators = 300, random_state = 2 )\n  \n  model.fit(oh_train_x, y_train)","15114e25":"sample[\"target\"] = model.predict(oh_test_x)\nsample.to_csv('submission-> final.csv', index = False)","8af368f0":"## Frequency encoding","2cbc22a4":"Good for me, no null, no duplicate values in the data set.\n","46869dee":"After performing hyperparameter tuning with random search CV, following were the best parameters for now. \n\n*P.S.- I will not type down the code, since it will take up a lot of time*","1041250c":"cat0 - cat9 --> categorical values\n\ncont0 - cont13 --> continuous  values","e8fe69ba":"# Aims of this project\n\nFerature engineering on cont variables like - normalisation of the data\n\nCompare One hot enconding and frequency encoding\n\n*Normal transformation of the independent variables*\n\nEffect of log transformation on the accuracy rate\n\nLGBM algorithm\n\n","56576862":"## simple exploration","3116b141":"# Final Model","56bc786a":"## Separating cont and cat variables","93fb6ecb":"This 30 Days ML challenge was really good. Here i have explored the different encoding methods and compared the results according. Any suggestion to this is always welcome. Also there were some situations which seems weird to me, any suggestions or explanations to that is appreciated. \n*P.S - some code lines will repeat, as that helps me in keeping my code understanding simple*","7c60df59":"## Dependent variables\n\nNow since the target varible is skewed, so I want try target log transformation and check out the result. ","3cd606a6":"The 5 point stats are not different, thus might not need to scale them, still have to plot to visualise the scale and data distribution","e57579e6":"# inference summary of encoding \nWell of all the encodig techniques, worked more or less quite the same during training. \n\nSimple recap:\n* Kaggle submission score - one hot encoded + transformed y + transformed x is 0.72748\n* Kaggle submission score - one hot encoded + non-transformed y + non-transformed x is 0.72600\n* Kaggle submission score - Frequency encoded + non-transformed y + non transformed x is 0.72630\n* Kaggle submission score - Frequency encoded + non-transformed y + transformed x is 0.72668\n* Kaggle submission score - ordinal encoded + non-transformed y + non transformed x is 0.72647\n* Kaggle submission score - ordinal encoded + non-transformed y + transformed x is 0.72671\n\nBut the surprising bit was when they were used to predict the test data and the kaggle score was weird, specially log transformation of target variables. I am not sure what happened and if anyone knows, pleasw do let me know. \n\nSo naturally, of all the methods, I'll go with non scaled x and y, along with one hot encoding as that seemed to be promising ","027bedf0":"# Cont feature engineering","64da54cf":"# Data exploration","b4c29b49":"## Independent variables","1fad8912":"# Categorical feature engineering\n\nNext that I wanted to explore is the effect of the different encoding techniques on the algorithm.\n\nI will be comparing the different encoding techniques, different combination of scaled\/log transformed data on LGBM algorithms.\nI know this line is confusing, but the following codes will make sense eventually.","7a3132f9":"# Importing Files and libraries","8a2e52b9":"## ordinal encoding","493e23f5":"The scale of the data is different, thus some scaling function has to be used. Also I will transform the target, to eliminate the skeweness. \n\nLabel encoding of the categorical variables ","5bce9e54":"## One hot encoding ","eca5ce43":"***Inference***\n\nThese baseline scores indicate that log transformed y and non scaled x, gave a better result. But unfortunately, when I submitted the prediction file, score was best for no transformation of x and y. I'll add the prediction code, commented out obviously, but would like to know the reason behind this unique behaviour.","deaab5ab":"## Graphical data exploration"}}