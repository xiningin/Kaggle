{"cell_type":{"a3e1b281":"code","289abac1":"code","339f4778":"code","6245a9c0":"code","fb0418f5":"code","b41999a7":"code","02440b1d":"code","0ca6e2eb":"code","819db9cc":"code","15033ad4":"code","17016f1d":"code","55c86901":"code","2b716647":"code","321bbda4":"code","30538698":"code","a3b55843":"code","49a33fdc":"code","af56e3d0":"code","2043b4b5":"code","1e2fc4f1":"code","ace61ef2":"code","56bba66b":"code","a3d1c560":"code","fd9bb649":"code","ef5d6ff8":"code","4c3b4c84":"code","5e7916a4":"code","12f5b86f":"code","e2f7ed59":"code","688bf138":"code","36895079":"code","8eccbf9e":"code","28bd62af":"code","a0d7c536":"code","1c09b22c":"code","ab1dacd2":"code","8dc08039":"code","c6caa32c":"code","4cd573ce":"code","c8833539":"code","425d4e66":"code","d0913f29":"code","f075e388":"code","e6c273b9":"code","a5ac9f13":"code","3746666f":"code","2c08f2d1":"code","74d51a5c":"code","ae7e94e3":"code","79b30086":"code","028cb654":"code","130d2182":"code","600c130e":"code","d58aecdb":"code","d21e21ce":"code","87cffb2b":"code","457aa453":"code","26af1eaa":"code","8a224735":"code","be7d1331":"code","7f08d69a":"code","804fc3df":"code","ceb80151":"code","4df3b744":"code","8ee1ef76":"code","e341207c":"code","085d64eb":"code","59ab2e96":"code","846ba285":"code","a69c85f0":"markdown","bb1d909a":"markdown","e8cd3d09":"markdown","58ad2e55":"markdown","026e197c":"markdown","c3cfa65e":"markdown","0f91a295":"markdown","d2cbde82":"markdown","c072fedf":"markdown","e1241b7d":"markdown","a119c0f2":"markdown","db3106ad":"markdown"},"source":{"a3e1b281":"import riiideducation\n# import dask.dataframe as dd\nimport pandas as pd\nimport numpy as np\nfrom sklearn.metrics import roc_auc_score\nenv = riiideducation.make_env()","289abac1":"train = pd.read_csv('\/kaggle\/input\/riiid-test-answer-prediction\/train.csv',\n                   usecols=[1, 2, 3, 4, 5, 7, 8, 9],\n                   dtype={'timestamp': 'int64',\n                          'user_id': 'int32',\n                          'content_id': 'int16',\n                          'content_type_id': 'int8',\n                          'task_container_id': 'int16',\n                          'answered_correctly':'int8',\n                          'prior_question_elapsed_time': 'float32',\n                          'prior_question_had_explanation': 'boolean'}\n                   )","339f4778":"#reading in question df\nquestions_df = pd.read_csv('\/kaggle\/input\/riiid-test-answer-prediction\/questions.csv',                         \n                            usecols=[0, 3],\n                            dtype={'question_id': 'int16',\n                              'part': 'int8'}\n                          )","6245a9c0":"#reading in lecture df\nlectures_df = pd.read_csv('\/kaggle\/input\/riiid-test-answer-prediction\/lectures.csv')","fb0418f5":"lectures_df['type_of'] = lectures_df['type_of'].replace('solving question', 'solving_question')\n\nlectures_df = pd.get_dummies(lectures_df, columns=['part', 'type_of'])\n\npart_lectures_columns = [column for column in lectures_df.columns if column.startswith('part')]\n\ntypes_of_lectures_columns = [column for column in lectures_df.columns if column.startswith('type_of_')]","b41999a7":"lectures_df.head()","02440b1d":"# merge lecture features to train dataset\ntrain_lectures = train[train.content_type_id == True].merge(lectures_df, left_on='content_id', right_on='lecture_id', how='left')","0ca6e2eb":"train_lectures.head()","819db9cc":"# collect per user stats\nuser_lecture_stats_part = train_lectures.groupby('user_id')[part_lectures_columns + types_of_lectures_columns].sum()","15033ad4":"user_lecture_stats_part.head()","17016f1d":"# add boolean features\nfor column in user_lecture_stats_part.columns:\n    bool_column = column + '_boolean'\n    user_lecture_stats_part[bool_column] = (user_lecture_stats_part[column] > 0).astype(int)","55c86901":"user_lecture_stats_part.head()","2b716647":"#clearing memory\ndel(train_lectures)","321bbda4":"#removing True or 1 for content_type_id\n\ntrain = train[train.content_type_id == False].sort_values('timestamp').reset_index(drop = True)","30538698":"train[(train.task_container_id == 9999)].tail()","a3b55843":"train[(train.content_type_id == False)].task_container_id.nunique()","49a33fdc":"#saving value to fillna\nelapsed_mean = train.prior_question_elapsed_time.mean()\n","af56e3d0":"group1 = train.loc[(train.content_type_id == False), ['task_container_id', 'user_id']].groupby(['task_container_id']).agg(['count'])\ngroup1.columns = ['avg_questions']\ngroup2 = train.loc[(train.content_type_id == False), ['task_container_id', 'user_id']].groupby(['task_container_id']).agg(['nunique'])\ngroup2.columns = ['avg_questions']\ngroup3 = group1 \/ group2","2043b4b5":"group3['avg_questions_seen'] = group3.avg_questions.cumsum()","1e2fc4f1":"group3.iloc[0].avg_questions_seen","ace61ef2":"results_u_final = train.loc[train.content_type_id == False, ['user_id','answered_correctly']].groupby(['user_id']).agg(['mean'])\nresults_u_final.columns = ['answered_correctly_user']\n\nresults_u2_final = train.loc[train.content_type_id == False, ['user_id','prior_question_had_explanation']].groupby(['user_id']).agg(['mean'])\nresults_u2_final.columns = ['explanation_mean_user']","56bba66b":"results_u2_final.explanation_mean_user.describe()","a3d1c560":"train = pd.merge(train, questions_df, left_on = 'content_id', right_on = 'question_id', how = 'left')","fd9bb649":"results_q_final = train.loc[train.content_type_id == False, ['question_id','answered_correctly']].groupby(['question_id']).agg(['mean'])\nresults_q_final.columns = ['quest_pct']","ef5d6ff8":"results_q2_final = train.loc[train.content_type_id == False, ['question_id','part']].groupby(['question_id']).agg(['count'])\nresults_q2_final.columns = ['count']","4c3b4c84":"question2 = pd.merge(questions_df, results_q_final, left_on = 'question_id', right_on = 'question_id', how = 'left')","5e7916a4":"question2 = pd.merge(question2, results_q2_final, left_on = 'question_id', right_on = 'question_id', how = 'left')","12f5b86f":"question2.quest_pct = round(question2.quest_pct,5)","e2f7ed59":"display(question2.head(), question2.tail())","688bf138":"train.head()","36895079":"len(train)","8eccbf9e":"len(train)","28bd62af":"train.answered_correctly.mean()","a0d7c536":"prior_mean_user = results_u2_final.explanation_mean_user.mean()","1c09b22c":"train.loc[(train.timestamp == 0)].answered_correctly.mean()","ab1dacd2":"train.loc[(train.timestamp != 0)].answered_correctly.mean()","8dc08039":"train.drop(['timestamp', 'content_type_id', 'question_id', 'part'], axis=1, inplace=True)","c6caa32c":"len(train)","4cd573ce":"validation = train.groupby('user_id').tail(5)\ntrain = train[~train.index.isin(validation.index)]\nlen(train) + len(validation)","c8833539":"validation.answered_correctly.mean()","425d4e66":"train.answered_correctly.mean()","d0913f29":"results_u_val = train[['user_id','answered_correctly']].groupby(['user_id']).agg(['mean'])\nresults_u_val.columns = ['answered_correctly_user']\n\nresults_u2_val = train[['user_id','prior_question_had_explanation']].groupby(['user_id']).agg(['mean'])\nresults_u2_val.columns = ['explanation_mean_user']","f075e388":"X = train.groupby('user_id').tail(18)\ntrain = train[~train.index.isin(X.index)]\nlen(X) + len(train) + len(validation)","e6c273b9":"X.answered_correctly.mean()","a5ac9f13":"train.answered_correctly.mean()","3746666f":"results_u_X = train[['user_id','answered_correctly']].groupby(['user_id']).agg(['mean'])\nresults_u_X.columns = ['answered_correctly_user']\n\nresults_u2_X = train[['user_id','prior_question_had_explanation']].groupby(['user_id']).agg(['mean'])\nresults_u2_X.columns = ['explanation_mean_user']","2c08f2d1":"#clearing memory\ndel(train)","74d51a5c":"X = pd.merge(X, group3, left_on=['task_container_id'], right_index= True, how=\"left\")\nX = pd.merge(X, results_u_X, on=['user_id'], how=\"left\")\nX = pd.merge(X, results_u2_X, on=['user_id'], how=\"left\")\n\nX = pd.merge(X, user_lecture_stats_part, on=['user_id'], how=\"left\")","ae7e94e3":"validation = pd.merge(validation, group3, left_on=['task_container_id'], right_index= True, how=\"left\")\nvalidation = pd.merge(validation, results_u_val, on=['user_id'], how=\"left\")\nvalidation = pd.merge(validation, results_u2_val, on=['user_id'], how=\"left\")\n\nvalidation = pd.merge(validation, user_lecture_stats_part, on=['user_id'], how=\"left\")","79b30086":"from sklearn.preprocessing import LabelEncoder\n\nlb_make = LabelEncoder()\n\nX.prior_question_had_explanation.fillna(False, inplace = True)\nvalidation.prior_question_had_explanation.fillna(False, inplace = True)\n\nvalidation[\"prior_question_had_explanation_enc\"] = lb_make.fit_transform(validation[\"prior_question_had_explanation\"])\nX[\"prior_question_had_explanation_enc\"] = lb_make.fit_transform(X[\"prior_question_had_explanation\"])","028cb654":"#reading in question df\n#question2 = pd.read_csv('\/kaggle\/input\/question2\/question2.csv)","130d2182":"content_mean = question2.quest_pct.mean()\n\nquestion2.quest_pct.mean()\n#there are a lot of high percentage questions, should use median instead?","600c130e":"#filling questions with no info with a new value\nquestion2.quest_pct = question2.quest_pct.mask((question2['count'] < 3), .65)\n\n\n#filling very hard new questions with a more reasonable value\nquestion2.quest_pct = question2.quest_pct.mask((question2.quest_pct < .2) & (question2['count'] < 21), .2)\n\n#filling very easy new questions with a more reasonable value\nquestion2.quest_pct = question2.quest_pct.mask((question2.quest_pct > .95) & (question2['count'] < 21), .95)","d58aecdb":"X = pd.merge(X, question2, left_on = 'content_id', right_on = 'question_id', how = 'left')\nvalidation = pd.merge(validation, question2, left_on = 'content_id', right_on = 'question_id', how = 'left')\nX.part = X.part - 1\nvalidation.part = validation.part - 1","d21e21ce":"X.head()","87cffb2b":"y = X['answered_correctly']\nX = X.drop(['answered_correctly'], axis=1)\nX.head()\n\ny_val = validation['answered_correctly']\nX_val = validation.drop(['answered_correctly'], axis=1)","457aa453":"X = X[['answered_correctly_user', 'explanation_mean_user', 'quest_pct', 'avg_questions_seen',\n       'prior_question_elapsed_time','prior_question_had_explanation_enc', 'part',\n       'part_1', 'part_2', 'part_3', 'part_4', 'part_5', 'part_6', 'part_7',\n       'type_of_concept', 'type_of_intention', 'type_of_solving_question', 'type_of_starter',\n       'part_1_boolean', 'part_2_boolean', 'part_3_boolean', 'part_4_boolean', 'part_5_boolean', 'part_6_boolean', 'part_7_boolean',\n       'type_of_concept_boolean', 'type_of_intention_boolean', 'type_of_solving_question_boolean', 'type_of_starter_boolean']]\nX_val = X_val[['answered_correctly_user', 'explanation_mean_user', 'quest_pct', 'avg_questions_seen',\n               'prior_question_elapsed_time','prior_question_had_explanation_enc', 'part',\n               'part_1', 'part_2', 'part_3', 'part_4', 'part_5', 'part_6', 'part_7',\n               'type_of_concept', 'type_of_intention', 'type_of_solving_question', 'type_of_starter',\n               'part_1_boolean', 'part_2_boolean', 'part_3_boolean', 'part_4_boolean', 'part_5_boolean', 'part_6_boolean', 'part_7_boolean',\n               'type_of_concept_boolean', 'type_of_intention_boolean', 'type_of_solving_question_boolean', 'type_of_starter_boolean']]","26af1eaa":"\n# Filling with 0.5 for simplicity; there could likely be a better value\nX['answered_correctly_user'].fillna(0.65,  inplace=True)\nX['explanation_mean_user'].fillna(prior_mean_user,  inplace=True)\nX['quest_pct'].fillna(content_mean, inplace=True)\n\nX['part'].fillna(4, inplace = True)\nX['avg_questions_seen'].fillna(1, inplace = True)\nX['prior_question_elapsed_time'].fillna(elapsed_mean, inplace = True)\nX['prior_question_had_explanation_enc'].fillna(0, inplace = True)\n\nX['part_1'].fillna(0, inplace = True)\nX['part_2'].fillna(0, inplace = True)\nX['part_3'].fillna(0, inplace = True)\nX['part_4'].fillna(0, inplace = True)\nX['part_5'].fillna(0, inplace = True)\nX['part_6'].fillna(0, inplace = True)\nX['part_7'].fillna(0, inplace = True)\nX['type_of_concept'].fillna(0, inplace = True)\nX['type_of_intention'].fillna(0, inplace = True)\nX['type_of_solving_question'].fillna(0, inplace = True)\nX['type_of_starter'].fillna(0, inplace = True)\nX['part_1_boolean'].fillna(0, inplace = True)\nX['part_2_boolean'].fillna(0, inplace = True)\nX['part_3_boolean'].fillna(0, inplace = True)\nX['part_4_boolean'].fillna(0, inplace = True)\nX['part_5_boolean'].fillna(0, inplace = True)\nX['part_6_boolean'].fillna(0, inplace = True)\nX['part_7_boolean'].fillna(0, inplace = True)\nX['type_of_concept_boolean'].fillna(0, inplace = True)\nX['type_of_intention_boolean'].fillna(0, inplace = True)\nX['type_of_solving_question_boolean'].fillna(0, inplace = True)\nX['type_of_starter_boolean'].fillna(0, inplace = True)","8a224735":"X_val['answered_correctly_user'].fillna(0.65,  inplace=True)\nX_val['explanation_mean_user'].fillna(prior_mean_user,  inplace=True)\nX_val['quest_pct'].fillna(content_mean,  inplace=True)\n\nX_val['part'].fillna(4, inplace = True)\nX_val['avg_questions_seen'].fillna(1, inplace = True)\nX_val['prior_question_elapsed_time'].fillna(elapsed_mean, inplace = True)\nX_val['prior_question_had_explanation_enc'].fillna(0, inplace = True)\n\nX_val['part_1'].fillna(0, inplace = True)\nX_val['part_2'].fillna(0, inplace = True)\nX_val['part_3'].fillna(0, inplace = True)\nX_val['part_4'].fillna(0, inplace = True)\nX_val['part_5'].fillna(0, inplace = True)\nX_val['part_6'].fillna(0, inplace = True)\nX_val['part_7'].fillna(0, inplace = True)\nX_val['type_of_concept'].fillna(0, inplace = True)\nX_val['type_of_intention'].fillna(0, inplace = True)\nX_val['type_of_solving_question'].fillna(0, inplace = True)\nX_val['type_of_starter'].fillna(0, inplace = True)\nX_val['part_1_boolean'].fillna(0, inplace = True)\nX_val['part_2_boolean'].fillna(0, inplace = True)\nX_val['part_3_boolean'].fillna(0, inplace = True)\nX_val['part_4_boolean'].fillna(0, inplace = True)\nX_val['part_5_boolean'].fillna(0, inplace = True)\nX_val['part_6_boolean'].fillna(0, inplace = True)\nX_val['part_7_boolean'].fillna(0, inplace = True)\nX_val['type_of_concept_boolean'].fillna(0, inplace = True)\nX_val['type_of_intention_boolean'].fillna(0, inplace = True)\nX_val['type_of_solving_question_boolean'].fillna(0, inplace = True)\nX_val['type_of_starter_boolean'].fillna(0, inplace = True)","be7d1331":"import lightgbm as lgb\n\nparams = {\n    'objective': 'binary',\n    'max_bin': 800,\n}\nparams['learning_rate'] = np.random.uniform(0, 1)\nparams['boosting_type'] = np.random.choice(['gbdt', 'goss'])\nparams['num_leaves'] = np.random.randint(20, 300)\nprint(params)\nlgb_train = lgb.Dataset(X, y, categorical_feature = ['part', 'prior_question_had_explanation_enc'])\nlgb_eval = lgb.Dataset(X_val, y_val, categorical_feature = ['part', 'prior_question_had_explanation_enc'], reference=lgb_train)","7f08d69a":"model = lgb.train(\n    params, lgb_train,\n    valid_sets=[lgb_train, lgb_eval],\n    verbose_eval=50,\n    num_boost_round=10000,\n    early_stopping_rounds=12\n)","804fc3df":"y_pred = model.predict(X_val)\ny_true = np.array(y_val)\nroc_auc_score(y_true, y_pred)","ceb80151":"#Set the max auc arbitrarily small\nmaxi = 0\ncount = 0 #Used for keeping track of the iteration number\n#How many runs to perform using randomly selected hyperparameters\niterations = 5\nfor i in range(iterations):\n    print('iteration number', count)\n    count += 1 #increment count\n    try:\n        params = {}\n        params['objective'] = np.random.choice(['binary'])\n        params['max_bin'] = np.random.choice([800])\n        params['learning_rate'] = np.random.uniform(0, 1)\n        params['boosting_type'] = np.random.choice(['gbdt', 'goss'])\n        params['num_leaves'] = np.random.randint(20, 300)\n        params['max_depth'] = np.random.randint(5, 200)\n        params['min_data'] = np.random.randint(10, 100)\n        iterations = np.random.randint(10, 10000)\n        print(params, iterations)\n        lgb_train = lgb.Dataset(X, y, categorical_feature = ['part', 'prior_question_had_explanation_enc'])\n        lgb_eval = lgb.Dataset(X_val, y_val, categorical_feature = ['part', 'prior_question_had_explanation_enc'], reference=lgb_train)\n        model = lgb.train(\n            params, lgb_train,\n            valid_sets=[lgb_train, lgb_eval],\n            verbose_eval=50,\n            num_boost_round=iterations,\n            early_stopping_rounds=12\n        )\n        y_pred = model.predict(X_val)\n        y_true = np.array(y_val)\n        scorez = roc_auc_score(y_true, y_pred)\n        print('auc:', scorez)\n        if scorez > maxi:\n            maxi = scorez\n            pp = params\n        print(\"Best so far is \", maxi)\n        \n    except: #in case something goes wrong\n            print('failed with')\n            print(params)\nprint(\"*\" * 50)\nprint('Maximum is: ', maxi)\nprint('Used params', pp)","4df3b744":"import matplotlib.pyplot as plt\nimport seaborn as sns","8ee1ef76":"#displaying the most important features by split\nlgb.plot_importance(model)\nplt.show()","e341207c":"#displaying the most important features by gain\nlgb.plot_importance(model, importance_type = 'gain')\nplt.show()","085d64eb":"iter_test = env.iter_test()","59ab2e96":"for (test_df, sample_prediction_df) in iter_test:\n    test_df['task_container_id'] = test_df.task_container_id.mask(test_df.task_container_id > 9999, 9999)\n    test_df = pd.merge(test_df, group3, left_on=['task_container_id'], right_index= True, how=\"left\")\n    test_df = pd.merge(test_df, question2, left_on = 'content_id', right_on = 'question_id', how = 'left')\n    test_df = pd.merge(test_df, results_u_final, on=['user_id'],  how=\"left\")\n    test_df = pd.merge(test_df, results_u2_final, on=['user_id'],  how=\"left\")\n    \n    test_df = pd.merge(test_df, user_lecture_stats_part, on=['user_id'], how=\"left\")\n    test_df['part_1'].fillna(0, inplace = True)\n    test_df['part_2'].fillna(0, inplace = True)\n    test_df['part_3'].fillna(0, inplace = True)\n    test_df['part_4'].fillna(0, inplace = True)\n    test_df['part_5'].fillna(0, inplace = True)\n    test_df['part_6'].fillna(0, inplace = True)\n    test_df['part_7'].fillna(0, inplace = True)\n    test_df['type_of_concept'].fillna(0, inplace = True)\n    test_df['type_of_intention'].fillna(0, inplace = True)\n    test_df['type_of_solving_question'].fillna(0, inplace = True)\n    test_df['type_of_starter'].fillna(0, inplace = True)\n    test_df['part_1_boolean'].fillna(0, inplace = True)\n    test_df['part_2_boolean'].fillna(0, inplace = True)\n    test_df['part_3_boolean'].fillna(0, inplace = True)\n    test_df['part_4_boolean'].fillna(0, inplace = True)\n    test_df['part_5_boolean'].fillna(0, inplace = True)\n    test_df['part_6_boolean'].fillna(0, inplace = True)\n    test_df['part_7_boolean'].fillna(0, inplace = True)\n    test_df['type_of_concept_boolean'].fillna(0, inplace = True)\n    test_df['type_of_intention_boolean'].fillna(0, inplace = True)\n    test_df['type_of_solving_question_boolean'].fillna(0, inplace = True)\n    test_df['type_of_starter_boolean'].fillna(0, inplace = True)\n    \n    test_df['answered_correctly_user'].fillna(0.65,  inplace=True)\n    test_df['explanation_mean_user'].fillna(prior_mean_user,  inplace=True)\n    test_df['quest_pct'].fillna(content_mean,  inplace=True)\n    test_df['part'] = test_df.part - 1\n\n    test_df['part'].fillna(4, inplace = True)\n    test_df['avg_questions_seen'].fillna(1, inplace = True)\n    test_df['prior_question_elapsed_time'].fillna(elapsed_mean, inplace = True)\n    test_df['prior_question_had_explanation'].fillna(False, inplace=True)\n    test_df[\"prior_question_had_explanation_enc\"] = lb_make.fit_transform(test_df[\"prior_question_had_explanation\"])\n    \n    test_df['answered_correctly'] =  model.predict(test_df[['answered_correctly_user', 'explanation_mean_user', 'quest_pct', 'avg_questions_seen',\n                                                            'prior_question_elapsed_time','prior_question_had_explanation_enc', 'part',\n                                                            'part_1', 'part_2', 'part_3', 'part_4', 'part_5', 'part_6', 'part_7',\n                                                            'type_of_concept', 'type_of_intention', 'type_of_solving_question', 'type_of_starter',\n                                                            'part_1_boolean', 'part_2_boolean', 'part_3_boolean', 'part_4_boolean', 'part_5_boolean', 'part_6_boolean', 'part_7_boolean',\n                                                            'type_of_concept_boolean', 'type_of_intention_boolean', 'type_of_solving_question_boolean', 'type_of_starter_boolean']])\n    env.predict(test_df.loc[test_df['content_type_id'] == 0, ['row_id', 'answered_correctly']])","846ba285":"#students don't appear in every task container ID what can I do about this, can't always follow sequentially?","a69c85f0":"## Data Exploration ##","bb1d909a":"\nThis notebook is a copy of that of @takamotoki available at https:\/\/www.kaggle.com\/takamotoki\/lgbm-iii-part3-adding-lecture-features.\n\nI have used elements from my article on LightGBM tuning available at https:\/\/medium.com\/@sergei740\/hyperparameter-tuning-lightgbm-using-random-grid-search-dc11c2f8c805 to tune the parameters and obtain a slightly better LB score.\n\nI have included the entire grid search code here - just adjust the number of iterations (currently seet at 2) to have a more extensive search. \n\nHope you enjoy!\n","e8cd3d09":"## Modeling ##","58ad2e55":"## Making Predictions for New Data ##","026e197c":"## Merging Data ##","c3cfa65e":"Does it make sense to use last questions as validation? Why is the rate of correct answers so low?\nI am convinced there is a better way to match the test data.","0f91a295":"Affirmatives (True) for content_type_id are only for those with a different type of content (lectures). These are not real questions.","d2cbde82":"## Examining Feature Importance ##","c072fedf":"## Creating Validation Set (Most Recent Answers by User) ##","e1241b7d":"This notebook is mostly based on https:\/\/www.kaggle.com\/dwit392\/lgbm-iii and slightly modified from https:\/\/www.kaggle.com\/takamotoki\/lgbm-iii-part2\n\n\n== modification from LGBM III part2 ==\n\n- add lecture features : This idea comes from the following notebook: https:\/\/www.kaggle.com\/pavelvpster\/riiid-fe-target-encoding-keras\n\n- lgb parameters : num_boost_round 1300 ==> 10000, early_stopping_rounds 8 ==> 12","a119c0f2":"## Extracting Training Data ##","db3106ad":"## Reading Data and Importing Libraries ##"}}