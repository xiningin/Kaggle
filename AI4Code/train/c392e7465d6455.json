{"cell_type":{"339cdba8":"code","82481fd8":"code","e92586f0":"code","a1580754":"code","268eab19":"code","95c7a556":"code","4bde223e":"code","bc71b05e":"code","35fcede9":"code","f918e8e4":"code","e68c9fa3":"code","4143cedd":"code","69cdb775":"code","9295388d":"code","bfb4953d":"code","b67175e8":"code","633bf3c0":"code","2b0fa422":"code","b1707d36":"code","47029d47":"code","fad1513f":"code","4ccb8bdb":"code","705e0b8d":"code","3229de4d":"code","917736cf":"code","4a4d0b02":"code","182f7d8c":"code","5afe887a":"code","b57c15f3":"code","5657aba8":"markdown","492482f6":"markdown","d472fe70":"markdown","a1094b62":"markdown","e4656e6b":"markdown","70d1a6d8":"markdown","ffbb1924":"markdown","ac262a45":"markdown","63ff3928":"markdown","555fc917":"markdown","e53e4523":"markdown","00fa1a92":"markdown","5533729c":"markdown","cf0408ca":"markdown","76704050":"markdown","fc609a23":"markdown","ece0c3ad":"markdown","994df963":"markdown","a4383e0b":"markdown","66f23e2c":"markdown","8ad5a053":"markdown","a35a27ac":"markdown","7f5187d9":"markdown","bce2c2bd":"markdown","122f5f31":"markdown","af67ca63":"markdown","220af1fa":"markdown","ca5b0857":"markdown","3ed50100":"markdown","b767dc1a":"markdown"},"source":{"339cdba8":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport joblib\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","82481fd8":"df = pd.read_csv('..\/input\/used-car-dataset-ford-and-mercedes\/bmw.csv')\ndf.head()","e92586f0":"df.info()","a1580754":"#Age calculation (present year - year of purchased)\ndf['age'] = 2020 - df['year']\ndf = df.drop(columns = 'year')\ndf.head()","268eab19":"#Since there might be error in data gathered (There is petrol and diesel car that have engine size of 0)\ndf[df['engineSize'] == 0]","95c7a556":"#Let's drop instances which fuelType are Diesel or Petrol but have 0.0 engineSize out.\ndf = df.drop(df[(df['engineSize'] == 0) & (df['fuelType'].isin(['Diesel','Petrol']))].index)\ndf[df['engineSize'] == 0]","4bde223e":"df.info()","bc71b05e":"cat_col = df.select_dtypes(include = object).columns.tolist()\nnum_col = df.select_dtypes(exclude = object).columns.tolist()","35fcede9":"df.describe()","f918e8e4":"#numerical data\nfig = plt.figure(figsize=(20,10))\nsns.set_style('darkgrid')\nfor index,col in enumerate(num_col):\n    plt.subplot(3,3,index+1)\n    sns.set(font_scale = 1.0)\n    sns.distplot(df[col], kde = False)\nfig.tight_layout(pad=1.0)","e68c9fa3":"#Categorical feature\nfig = plt.figure(figsize=(20,5))\nsns.set_style('darkgrid')\nfor index,col in enumerate(cat_col):\n    plt.subplot(1,3,index+1)\n    if(index == 0):\n        plt.xticks(rotation=90)\n    sns.set(font_scale = 1.0)\n    sns.countplot(df[col], order = df[col].value_counts().index)\n\n    \nfig.tight_layout(pad=1.0)","4143cedd":"#numerical data\nfig = plt.figure(figsize=(20,20))\nsns.set_style('darkgrid')\nfor index,col in enumerate(num_col):\n    if col == 'price':\n        plt.subplot(4,3,index+1)\n        sns.heatmap(df.corr(), annot=True, cmap='RdBu')\n    else:\n        plt.subplot(4,3,index+1)\n        sns.set(font_scale = 1.0)\n        sns.scatterplot(data = df, x = col, y = 'price',alpha = 0.7)\nfig.tight_layout(pad=1.0)","69cdb775":"sns.set(style=\"ticks\")\n\n# Initialize the figure\nf, ax = plt.subplots(figsize=(20, 10))\n\n# Plot model vs price\nplt.subplot(1,3,1)\nsns.boxplot(x=\"price\", y=\"model\", data=df,\n            whis=[0, 100], palette=\"vlag\",\n           order = df.groupby('model').median().sort_values(by = 'price').index)\nax.xaxis.grid(True)\nsns.despine(trim=True, left=True)\n\n# Plot transmission vs price\nplt.subplot(1,3,2)\nsns.boxplot(x=\"transmission\", y=\"price\", data=df,\n            whis=[0, 100], palette=\"vlag\")\nax.xaxis.grid(True)\nsns.despine(trim=True, left=True)\n\n# Plot fuelType vs price\nplt.subplot(1,3,3)\nsns.boxplot(x=\"fuelType\", y=\"price\", data=df,\n            whis=[0, 100], palette=\"vlag\")\n\nax.xaxis.grid(True)\nsns.despine(trim=True, left=True)","9295388d":"from sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.compose import ColumnTransformer\n\nfrom sklearn.linear_model import ElasticNet\nfrom sklearn.linear_model import SGDRegressor\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.svm import SVR\nimport xgboost as xgb","bfb4953d":"X = df.copy().drop(columns='price')\ny = df['price'].copy()\n#Before further dealing with data, let's split it into train and test set , since we'll not use test set for model development (for evaluating model performance only).\nX_train,X_test,y_train,y_test = train_test_split(X,y,random_state=1, test_size = 0.2)","b67175e8":"#Separate different attribute type to catergorical and numerical features.\ncat_col = ['model', 'transmission', 'fuelType']\nnum_col = ['mileage', 'tax', 'mpg', 'age','engineSize']","633bf3c0":"#For numberical features, let's standardized it before feeding into our model\n#For categorical features, perform one hot encoding before feeding into model\nfull_pipeline = ColumnTransformer([\n    ('num', StandardScaler(), num_col),\n    ('cat', OneHotEncoder(handle_unknown='ignore'), cat_col)\n])","2b0fa422":"#Apply data transformation to train set\nX_train_prepared = full_pipeline.fit_transform(X_train)","b1707d36":"#X_train_prepared\npd.DataFrame.sparse.from_spmatrix(X_train_prepared,columns = full_pipeline.transformers_[0][2]+full_pipeline.transformers_[1][1].get_feature_names().tolist()  )","47029d47":"model_list = [(ElasticNet(),'ElasticNet'),\n              (SGDRegressor(),'SGDRegressor'),\n              (SVR(kernel='linear'),'SVR-linear'),\n              (SVR(kernel='rbf'),'SVR-rbf'),\n              (RandomForestRegressor(),'RandomForestRegressor'),\n              (xgb.XGBRegressor(),'XGBoost')\n             ]\n\nmodel_score = []\n\nfor m in model_list:\n    model = m[0]\n    score = cross_val_score(model,X_train_prepared,y_train,cv=4, scoring='r2')\n    print(f'{m[1]} score = {score.mean()}')\n    model_score.append([m[1],score.mean()])","fad1513f":"%%script false --no-raise-error\nfrom sklearn.model_selection import GridSearchCV\n\n#Hyperparameter to be tweaked\nparam_grid = [\n    {'n_estimators': [100,200,300],\n    'max_depth' : [3,5,10],\n    'reg_lambda' : [0.1,1,3,10,30],\n    'reg_alpha': [0.1,1,3,10,30],\n    'learning_rate': [0.15,0.3,0.5]}\n]\n\nxgb_regressor = xgb.XGBRegressor()\ngrid_search = GridSearchCV(xgb_regressor,param_grid, cv=4, scoring = 'r2', return_train_score = True)","4ccb8bdb":"%%script false --no-raise-error\ngrid_search.fit(X_train_prepared,y_train)","705e0b8d":"%%script false --no-raise-error\n#Save model for later use\nmodel = grid_search.best_estimator_\njoblib.dump(model , 'XGBRegressor.pkl')","3229de4d":"my_model = joblib.load('..\/input\/bmw-price-prediction\/XGBRegressor.pkl')","917736cf":"# Apply preprocessing to test set\nX_test_prepared = full_pipeline.transform(X_test)\n# Fit model to training set\nmy_model.fit(X_train_prepared,y_train)\nprediction = my_model.predict(X_test_prepared)","4a4d0b02":"#Result of model's prediction (y_predicted) compared to actual car price (y_test)\nnp.random.seed(0)\ncompare = pd.DataFrame(data = [np.array(y_test),prediction])\ncompare = compare.T\ncompare.columns = ['Actual_price','Predicted_price']\ncompare.sample(10)","182f7d8c":"from yellowbrick.regressor import prediction_error\n# Create the train and test data\n# Instantiate the linear model and visualizer\nvisualizer = prediction_error(my_model, X_train_prepared, y_train, X_test_prepared, y_test)","5afe887a":"importance = pd.DataFrame(data=my_model.feature_importances_, index = full_pipeline.transformers_[0][2]+full_pipeline.transformers_[1][1].get_feature_names().tolist(),columns=['Importance_Score'])\nimportance.sort_values(by='Importance_Score',ascending=False)\n","b57c15f3":"sns.set(style=\"ticks\")\n\n# Initialize the figure\nf, ax = plt.subplots(figsize=(16, 12))\n\nsns.barplot(x = 'Importance_Score', y=importance.index, data=importance,\n           order = importance.sort_values(by='Importance_Score',ascending=False).index,\n            palette=\"vlag\")\n           \nax.xaxis.grid(True)\nax.set_title('Feature Importance Ranking')\nsns.despine(trim=True, left=True)","5657aba8":"Since our features can be separated into 2 group, we'll have different preprocessing step for them as follow.\n1. **Numerical features** : since there's no missing value for imputation, our preprocessing step will consist of just standardize them\n2. **Categorical features** : perform one hot encoding on them before feeding into model\n\n","492482f6":"# 7. Conclusion <a id=7>","d472fe70":"Data observation\n1. More than 50% of BMW on sold in our database are BMW 1 Series - 5 Series which are displayed above\n2. Semi automatic transmission is the most popular following by automatic and manual transmission\n3. 70% of them are Diesel powered following by petrol , hybrid and electric","a1094b62":"## 2.2 Bivariate Analysis <a id = '2.2'>","e4656e6b":"Data observation : \n1. Price distribution are right skew which has mean around 22800 and median around 20500\n1. Mileage distribution are right skew with most of them are under 50,000 miles of usage\n1. Tax distribution : most of our data are fixed value at 145\n1. mpg are neary normal distribution with mean and median of 56.4 and 53 and some outlier above 100 mpg\n1. age are lying from new cars to oldest of 24 years old which has mean and median around 3 years old","70d1a6d8":"[back to top](#0.1)","ffbb1924":"# Introduction\n\nDreaming of having a BMW? Here is your tools to help you select for best value BMW in the market.\n\nSince there are thousands of BMW car on sold online. It might be useful for buyers to understand which independents variables affect price of the car they're going to buy to help them select for most suitable car for there budget. For sellers, ability to set reasonable car price could make thier sale more attractive.\n\n","ac262a45":"# 1.  Data import and cleaning <a id = '1'>","63ff3928":"# 5. Hyperparameter tuning <a id=5>","555fc917":"from cross validation score, we decide to continue develop on **XGBoost** model since it has highest score on validation set.","e53e4523":"There is tons of regression algorithm availble. we'll explore some of them using default setting of each algorithm provided by scikit-learn library and evaluate there performance using cross-validation method. \n\nAfter we got best model, we'll continue fine tune them to increase its performance on validation set. \n\nFinally we'll evaluate model performance on test set which we hold it out before our data preprocessing step.","00fa1a92":"Data observation :\n1. Mileage and age show strong negative corelation against price (It would be important predictor of price)\n1. Tax and mpg show weak relationship with price\n1. As engineSize grow up, car price tend to go up","5533729c":"<a class=\"anchor\" id=\"0.1\"><\/a>\n\n## Table of Contents\n\n1. [Data import and cleaning](#1)\n1. [Exploratory Data Analysis](#2)\n    - [Univaraite analysis](#2.1)\n    - [Bivariate analysis](#2.2)\n1. [Data preprocessing](#3)\n1. [Models selection](#4)\n1. [Hyperparameters tuning](#5)\n1. [Prediction and evaluation](#6)\n1. [Conclusion](#7)\n","cf0408ca":"# 6. Prediction and evaluation <a id=6>","76704050":"from above table, Our model have done pretty good! <br>\nLet's visualize how good our model make a prediction compared to actual car price","fc609a23":"In this step, we will continue to develop our XGBoost regressor to achieve better performance on validation set by **GridSearchCV** method. <br> For short brief, GridSearchCV will go through every combination of hyperparameter listed in param_grid. In our case GridSearchCV will go through total of 3x3x5x5x3 = 675 models to find best combination of hyperparameters. \n\nNote : <br>In each models GridSearchCV will run cross validation with 4 folds ","ece0c3ad":"Data observation :\n1. Different model demonstrate different variation in price as show\n1. Semi-automatic transmission tend to slightly higher price than automatic and manual transmission\n1. Fuel type doesn't show strong relationship with price (However Hybrid tend to have a little bit higher price than other type)","994df963":"Now it's time to make a prediction from our model","a4383e0b":"In conclusion, factors which have strong effect to BMW car price are as follow\n* engineSize : which increasing in engineSize directly cause increasing in price as our observation in exploratory data analysis shown\n* Age and mileage : older vehicle depreciated in its value which result in lower price compared to newer one. However age is better predictor than mileage although they have same corelation with price. For next model it might be worth a try to eliminate mileage from our model, since it relevant to age attribute.\n* Different model : diffrent car model are important than the rest attributes, since different model have different price from their day 1 out of factory\n\nOut of 6 model in our list, XGBRegressor performance are outstanding with R2 on test set of 0.959","66f23e2c":"# 3. Data preprocessing <a id = '3'>","8ad5a053":"## Dataset explanation\n\nData is provided by 100,000 UK Used car dataset (https:\/\/www.kaggle.com\/adityadesai13\/used-car-dataset-ford-and-mercedes). For this notebook, we condiser only BMW car which has following features used for prediction of its price\n\n1. model : model of the cars (i.e. 5 Series, X3, etc.)\n2. year : year of 1st hand purchased\n3. transmission : mode of transmission (manual or automatic or semi-auto)\n4. mileage : total mileage of the car\n5. tax : road tax incured\n6. mpg : miles per gallon consumption\n7. engineSize : in units of litres","a35a27ac":"Dataset was properly collected. There are no missing value presented. So in data preprocessing step, we can skip imputation process.\n\nTo gain further insight, I decide to add new columns to calculate vehicle age from its year since it might be a useful predictor for predict car price","7f5187d9":"### **Here come to the end of this notebook, please note that this is the first regression problem solving for me, I'll try with more challenging dataset and algorithm in next problem. I have to apologize for any mistakes made here. I would greatly appreciate it if you kindly give me some feedback for this notebook. If you like it, please hit upvote! :D**","bce2c2bd":"# BMW Price prediction","122f5f31":"With vertical axis as predicted price and horizontal axis as actual price,\nIdeally, model with 100% accuracy will perform as gray dashed line (predicted price = actual price) with slope equal to one.<br>\n\nIn our case, our XGBRegressor model perform quite good with Rsquared of 0.959 ","af67ca63":"# 2.  Exploratory Data Analysis <a id='2'>","220af1fa":"There are 10781 instances of data. Total number of attributes are 9, of which 5 is quantitative, 3 is Qualitative.\n\nQuantitative: mileage, tax, mpg, engineSize, age\n\nQualitative: model, transimission, fuelType","ca5b0857":"<img src=\"https:\/\/static.bangkokpost.com\/media\/content\/20200305\/c1_1872299.jpg\" alt=\"BMW logo\" width=\"800.33\" height=\"200\">","3ed50100":"## 2.1 Univaraiate analysis <a id = '2.1'>","b767dc1a":"\n# 4. Model selection <a id = '4'>\n"}}