{"cell_type":{"314cc4de":"code","3da9f1ef":"code","bd334f55":"code","19a86819":"code","db958e4e":"code","91e8be18":"code","d68d5389":"code","f4303d49":"code","08fbfe73":"code","ab3d010a":"code","5a109a13":"code","28f4f4f3":"code","85780fdf":"code","e245a95d":"code","e46438bf":"code","5ae4ff15":"code","4550fc78":"code","9a531d2a":"code","2b0288ed":"code","5930ff76":"code","7fff381f":"code","105cdf1d":"code","1308aab6":"code","e38372d7":"code","1a5daec4":"code","350c1687":"code","6ab3c0df":"code","d2404625":"code","c37f06e5":"code","45954927":"code","2001c91e":"code","d61a2ca1":"code","a8a6a0a0":"code","c97fe813":"code","1b1011f4":"code","896d195c":"code","4d7d0b4c":"code","73cff217":"code","d729896e":"code","bf9ce529":"code","c86b170b":"code","166e9bb9":"code","93098ed0":"code","b759ee56":"code","db1c7f97":"code","7bae7c12":"code","08b964b6":"code","8dffc832":"code","91497e20":"code","6e441690":"code","f90b2e0d":"code","a78de77c":"code","dc02f123":"code","18ca5daf":"code","5a22f773":"code","a59591a3":"code","2eb43edd":"code","19cc9b17":"code","c49fcaa0":"code","6131e2d3":"code","01d58044":"code","9719af44":"code","390ca230":"code","e766e122":"code","9ab04eee":"code","e113ed2a":"markdown","6bee9d34":"markdown","1ff12291":"markdown","c657f59f":"markdown","548b4cb6":"markdown","08de6d64":"markdown","a699e8f2":"markdown","0d7a5dd4":"markdown","fe21b8f1":"markdown","8346ffdd":"markdown","5822d0a9":"markdown","d848d6f2":"markdown","dc3a6b99":"markdown","326c39f7":"markdown","afbdef46":"markdown","f744bf05":"markdown","575882d5":"markdown","518811f0":"markdown","d85fb107":"markdown","67c180c5":"markdown","b7e6e7f6":"markdown","9db498f6":"markdown","9f6701a2":"markdown","94e64066":"markdown","7feebf32":"markdown"},"source":{"314cc4de":"import pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nimport statsmodels.formula.api as sm\nimport scipy.stats as stats\nimport pandas_profiling\n\n%matplotlib inline\nplt.rcParams['figure.figsize'] = 10, 7.5\nplt.rcParams['axes.grid'] = True\nplt.gray()\n\nfrom matplotlib.backends.backend_pdf import PdfPages\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn import metrics\nfrom sklearn.linear_model import LogisticRegression\nfrom statsmodels.stats.outliers_influence import variance_inflation_factor\nfrom patsy import dmatrices","3da9f1ef":"bankloans = pd.read_csv(\"..\/input\/bankloans.csv\")","bd334f55":"bankloans.columns","19a86819":"bankloans.info()","db958e4e":"# pandas_profiling.ProfileReport(bankloans)","91e8be18":"numeric_var_names = [key for key, val in bankloans.dtypes.items() if val in [\"float64\",\"int64\",\"float32\",\"int32\"] ]\ncat_var_names = [key for key, val in bankloans.dtypes.items() if val in [\"object\"]]\nprint(numeric_var_names)\nprint(cat_var_names)","d68d5389":"bankloans_num = bankloans[numeric_var_names]\nbankloans_num.head()","f4303d49":"bankloans_cat = bankloans[cat_var_names]\nbankloans_cat.head()","08fbfe73":"def var_summary(x):\n    return pd.Series([x.count(), x.isnull().sum(), x.sum(), x.mean(), x.median(),  x.std(), x.var(), x.min(), x.dropna().quantile(0.01), x.dropna().quantile(0.05),x.dropna().quantile(0.10),x.dropna().quantile(0.25),x.dropna().quantile(0.50),x.dropna().quantile(0.75), x.dropna().quantile(0.90),x.dropna().quantile(0.95), x.dropna().quantile(0.99),x.max()], \n                  index=['N', 'NMISS', 'SUM', 'MEAN','MEDIAN', 'STD', 'VAR', 'MIN', 'P1' , 'P5' ,'P10' ,'P25' ,'P50' ,'P75' ,'P90' ,'P95' ,'P99' ,'MAX'])\n\nnum_summary=bankloans_num.apply(lambda x: var_summary(x)).T","ab3d010a":"num_summary","5a109a13":"bankloans_existing = bankloans_num[bankloans_num.default.isnull()==0]\nbankloans_new = bankloans_num[bankloans_num.default.isnull() == 1]","28f4f4f3":"def outlier_capping(x):\n    x = x.clip_upper(x.quantile(0.99))\n    x = x.clip_lower(x.quantile(0.01))\n    return x\nbankloans_existing = bankloans_existing.apply(lambda x: outlier_capping(x))","85780fdf":"bankloans_existing.corr()","e245a95d":"sns.heatmap(bankloans_existing.corr())\nplt.show()","e46438bf":"bankloans_existing.columns.difference([\"default\"])","5ae4ff15":"bp = PdfPages(\"Transformation plots.pdf\")\n\nfor num_variable in bankloans_existing.columns.difference(['default']):\n    binned = pd.cut(bankloans_existing[num_variable], bins = 10, labels = list(range(1,11)))\n    odds = bankloans_existing.groupby(binned)[\"default\"].sum()\/(bankloans_existing.groupby(binned)[\"default\"].count() - bankloans_existing.groupby(binned)[\"default\"].sum())\n    log_odds = np.log(odds)\n    fig, axes = plt.subplots(figsize= (10,4))\n    sns.barplot(x = log_odds.index, y = log_odds)\n    plt.title(str(\"Logit plot for identifying if the bucketing is required or not for varibale -- \")+ str(num_variable))\n    bp.savefig(fig)\nbp.close()","4550fc78":"bankloans_existing.columns","9a531d2a":"logreg_model = sm.logit(\"default~address\", data = bankloans_existing).fit()","2b0288ed":"p = logreg_model.predict(bankloans_existing)","5930ff76":"metrics.roc_auc_score(bankloans_existing[\"default\"], p)","7fff381f":"2*metrics.roc_auc_score(bankloans_existing[\"default\"], p)-1","105cdf1d":"somersd_df = pd.DataFrame()\nfor num_variable in bankloans_existing.columns.difference([\"default\"]):\n    logreg = sm.logit(formula = \"default~\"+str(num_variable), data = bankloans_existing)\n    result = logreg.fit()\n    y_score = pd.DataFrame(result.predict())\n    y_score.columns = [\"Score\"]\n    somers_d = 2* metrics.roc_auc_score(bankloans_existing[\"default\"], y_score)-1\n    temp = pd.DataFrame([num_variable, somers_d]).T\n    temp.columns = [\"Variable Name\", \"SomersD\"]\n    somersd_df = pd.concat([somersd_df, temp], axis = 0)\nsomersd_df\n\n    ","1308aab6":"somersd_df.sort_values(by = \"SomersD\", ascending = False)","e38372d7":"X = pd.concat([bankloans_existing[bankloans_existing.columns.difference([\"default\"])], bankloans_existing[\"default\"]], axis = 1)\nfeatures = \"+\".join(bankloans_existing.columns.difference([\"default\"]))\nX.head()","1a5daec4":"features","350c1687":"a, b = dmatrices(formula_like = \"default~\" + features, data = X, return_type = \"dataframe\")\nvif = pd.DataFrame()\nvif[\"VIF Factor\"] = [variance_inflation_factor(b.values, i) for i in range(b.shape[1])]\nvif[\"features\"] = b.columns\nprint(vif)","6ab3c0df":"train, test = train_test_split(bankloans_existing, test_size = 0.3, random_state = 43)\ntrain.columns","d2404625":"logreg = sm.logit(formula = \"default ~ address+age+creddebt+debtinc+employ\", data = train)\nresult = logreg.fit()","c37f06e5":"print(result.summary2())","45954927":"train_gini = 2* metrics.roc_auc_score(train[\"default\"], result.predict(train))-1\nprint(\"The Gini Index for the model built on the Train Data is : \", train_gini)\n\ntest_gini = 2* metrics.roc_auc_score(test[\"default\"], result.predict(test))-1\nprint(\"The Gini Index for the model built on the Test Data is :\", test_gini)","2001c91e":"train_auc = metrics.roc_auc_score(train[\"default\"], result.predict(train))\ntest_auc = metrics.roc_auc_score(test[\"default\"], result.predict(test))\n\nprint(\"The AUC for the model built on the Train Data is:\", train_auc)\nprint(\"The AUC for the model built on the Test Data is:\", test_auc)","d61a2ca1":"train_predicted_prob = pd.DataFrame(result.predict(train))\ntrain_predicted_prob.columns = [\"prob\"]\ntrain_actual = train[\"default\"]","a8a6a0a0":"train_predict=  pd.concat([train_actual, train_predicted_prob], axis = 1)\ntrain_predict.columns = [\"actual\", \"prob\"]\ntrain_predict.head()","c97fe813":"test_predicted_prob = pd.DataFrame(result.predict(test))\ntest_predicted_prob.columns = [\"prob\"]\ntest_actual = test[\"default\"]","1b1011f4":"test_predict = pd.concat([test_actual, test_predicted_prob], axis = 1)\ntest_predict.columns = [\"actual\", \"prob\"]\ntest_predict.head()","896d195c":"np.linspace(0,1,50)","4d7d0b4c":"train_predict.head()\n","73cff217":"train_predict[\"predicted\"] = train_predict[\"prob\"].apply(lambda x: 0.0 if x<0.2 else 1.0)","d729896e":"train_predict.head()","bf9ce529":"train_predict['tp'] = train_predict.apply(lambda x: 1.0 if x['actual']==1.0 and x['predicted']==1 else 0.0, axis=1)\ntrain_predict['fp'] = train_predict.apply(lambda x: 1.0 if x['actual']==0.0 and x['predicted']==1 else 0.0, axis=1)\ntrain_predict['tn'] = train_predict.apply(lambda x: 1.0 if x['actual']==0.0 and x['predicted']==0 else 0.0, axis=1)\ntrain_predict['fn'] = train_predict.apply(lambda x: 1.0 if x['actual']==1.0 and x['predicted']==0 else 0.0, axis=1)","c86b170b":"train_predict.head()","166e9bb9":"accuracy = (train_predict.tp.sum()+train_predict.tn.sum())\/(train_predict.tp.sum()+train_predict.tn.sum()+train_predict.fp.sum()+train_predict.fn.sum())\naccuracy","93098ed0":"Sensitivity = (train_predict.tp.sum())\/(train_predict.tp.sum()+train_predict.fn.sum())\nSensitivity","b759ee56":"roc_like_df = pd.DataFrame()\ntrain_temp = train_predict.copy()\nfor cut_off in np.linspace(0,1,50):\n    train_temp['cut_off'] = cut_off\n    train_temp['predicted'] = train_temp['prob'].apply(lambda x: 0.0 if x < cut_off else 1.0)\n    train_temp['tp'] = train_temp.apply(lambda x: 1.0 if x['actual']==1.0 and x['predicted']==1 else 0.0, axis=1)\n    train_temp['fp'] = train_temp.apply(lambda x: 1.0 if x['actual']==0.0 and x['predicted']==1 else 0.0, axis=1)\n    train_temp['tn'] = train_temp.apply(lambda x: 1.0 if x['actual']==0.0 and x['predicted']==0 else 0.0, axis=1)\n    train_temp['fn'] = train_temp.apply(lambda x: 1.0 if x['actual']==1.0 and x['predicted']==0 else 0.0, axis=1)\n    sensitivity = train_temp['tp'].sum() \/ (train_temp['tp'].sum() + train_temp['fn'].sum())\n    specificity = train_temp['tn'].sum() \/ (train_temp['tn'].sum() + train_temp['fp'].sum())\n    accuracy = (train_temp['tp'].sum()  + train_temp['tn'].sum() ) \/ (train_temp['tp'].sum() + train_temp['fn'].sum() + train_temp['tn'].sum() + train_temp['fp'].sum())\n    roc_like_table = pd.DataFrame([cut_off, sensitivity, specificity, accuracy]).T\n    roc_like_table.columns = ['cutoff', 'sensitivity', 'specificity', 'accuracy']\n    roc_like_df = pd.concat([roc_like_df, roc_like_table], axis=0)\n","db1c7f97":"roc_like_df.head()","7bae7c12":"roc_like_df[\"total\"] = roc_like_df[\"sensitivity\"] + roc_like_df[\"specificity\"]\nroc_like_df.head()","08b964b6":"roc_like_df[roc_like_df[\"total\"] == roc_like_df[\"total\"].max()] ","8dffc832":"roc_like_df[roc_like_df['sensitivity']==roc_like_df['sensitivity'].max()]","91497e20":"test_predict['predicted'] = test_predict['prob'].apply(lambda x: 1 if x > 0.2 else 0)\ntrain_predict['predicted'] = train_predict['prob'].apply(lambda x: 1 if x > 0.2 else 0)","6e441690":"pd.crosstab(train_predict['actual'], train_predict['predicted'])","f90b2e0d":"pd.crosstab(test_predict['actual'], test_predict['predicted'])","a78de77c":"print(\"The overall accuracy score for the Train Data is : \", metrics.accuracy_score(train_predict.actual, train_predict.predicted))\nprint(\"The overall accuracy score for the Test Data  is : \", metrics.accuracy_score(test_predict.actual, test_predict.predicted))","dc02f123":"print(metrics.classification_report(train_predict.actual, train_predict.predicted))","18ca5daf":"print(metrics.classification_report(test_predict.actual, test_predict.predicted))","5a22f773":"train_predict['Deciles']=pd.qcut(train_predict['prob'],10, labels=False)\ntrain_predict.head()","a59591a3":"test_predict['Deciles']=pd.qcut(test_predict['prob'],10, labels=False)\ntest_predict.head()","2eb43edd":"no_1s = train_predict[['Deciles','actual']].groupby(train_predict.Deciles).sum().sort_index(ascending=False)['actual']\nno_total = train_predict[['Deciles','actual']].groupby(train_predict.Deciles).count().sort_index(ascending=False)['actual']\nmax_prob = train_predict[['Deciles','prob']].groupby(train_predict.Deciles).max().sort_index(ascending=False)['prob']\nmin_prob = train_predict[['Deciles','prob']].groupby(train_predict.Deciles).min().sort_index(ascending=False)['prob']","19cc9b17":"Decile_analysis_train = pd.concat([ min_prob, max_prob, no_1s, no_total-no_1s, no_total], axis=1)\nDecile_analysis_train.columns = ['Min_prob', 'Max_prob', '#1', '#0', 'Total']\nDecile_analysis_train","c49fcaa0":"no_1s = test_predict[['Deciles','actual']].groupby(test_predict.Deciles).sum().sort_index(ascending=False)['actual']\nno_total = test_predict[['Deciles','actual']].groupby(test_predict.Deciles).count().sort_index(ascending=False)['actual']\nmax_prob = test_predict[['Deciles','prob']].groupby(test_predict.Deciles).max().sort_index(ascending=False)['prob']\nmin_prob = test_predict[['Deciles','prob']].groupby(test_predict.Deciles).min().sort_index(ascending=False)['prob']\n\nDecile_analysis_test = pd.concat([min_prob, max_prob, no_1s, no_total-no_1s, no_total], axis=1)\n\nDecile_analysis_test.columns = ['Min_prob', 'Max_prob', '#1', '#0', 'Total']\n\nDecile_analysis_test","6131e2d3":"bankloans_new.head()","01d58044":"bankloans_new['prob'] = result.predict(bankloans_new)","9719af44":"bankloans_new.head()","390ca230":"bankloans_new['default'] = bankloans_new['prob'].apply(lambda x: 1 if x > 0.20 else 0)","e766e122":"bankloans_new.head()","9ab04eee":"bankloans_new.default.value_counts()","e113ed2a":"### Train and Test split","6bee9d34":"### Visualize the correlation matrix in seaborn using heatmap","1ff12291":"## Predicting new customers (Implementation of Model on new data)","c657f59f":"Cut-off based on highest sum(sensitivity+ specificity) - common way of identifying cut-off","548b4cb6":"### Creating Data Audit report\nUse a general function that returns multiple values","08de6d64":"### Data Exploratory Analysis\n- Variable Transformation: (i) Bucketing","a699e8f2":"### Decile Analysis for test data","0d7a5dd4":"**Intuition behind ROC curve - confusion matrix for each different cut-off shows trade off in sensitivity and specificity**","fe21b8f1":"- LE = -2.1527 -0.0865\\*address+0.0432\\*age+0.6576\\*creddebt+0.1021\\*debtinc-0.2917\\*employ\n- p(Default=1) = exp(LE)\/(1+exp(LE))","8346ffdd":"### Building logistic Regression","5822d0a9":"### Intuition behind ROC curve - predicted probability as a tool for separating the 1's and 0's","d848d6f2":"intuition behind ROC curve - predicted probability as a tool for separating the 1's and 0's","dc3a6b99":"## We are rejecting applications for 66 customers based on cut-off = 0.20","326c39f7":"### 2.5 Data Exploratory Analysis\n- Variable reduction using Somer's D values","afbdef46":"### making a DataFrame with actual and prob columns","f744bf05":"## Credit Risk Analytics\n**Definition of Target and Outcome Window:**\n\nOne of the leading banks would like to predict bad customer while customer applying for loan. This model also called as PD models (Probability of Default)\n\n#### Data PreProcessing:\n- Missing values Treatment - Numerical (Mean\/Median imputation) and Categorical (separate missing category or merging)\n- Univariate Analysis - Outlier and Frequency analysis\n\n#### Data Exploratory Analysis\n- Bivariate Analysis - Numeric(ttest) and Categorical (Chisquare)\n- Bivariate Analysis - Visualization\n- Variable Transformation - P- value based selection\n- Variable Transformation - Bucketing\/ Binning for numerical variables and Dummy for categorical variables.\n- Variable Reduction 0 IV\/ Somers D\n- Variable Reduction - Multicollinearity.\n\n#### Model build and Model Diagnostics\n- Train and Test split\n- Significance of each variable\n- Gini and ROC \/ Concordance analysis - Rank ordering\n- Classfication Table Analysis - Accuracy\n\n#### Model Validation \n- OOS validation - p-value and sign testing for the model coefficients.\n- Diagnostics check to remain similar to Training model build\n- BootStrapping, if necessary\n\n#### Model interpretation for its properties\n- Interferencing for finding the most important contributors.\n- Prediction of risk and proactive prevention by targeting segments of the population","575882d5":"**Decile Analysis**\n- Top- two deciles - High Risk customers - will reject applications\n- 3rd, 4th, 5th decile - medium risk customers - will accept application with proper scrutiny.\n- 6th decile onwards - Low risk customers - accept the applications ","518811f0":"### Variance Inflation Factor assessment","d85fb107":"### Handling outliers","67c180c5":"### Decile Analysis for train data","b7e6e7f6":"#Choosen Best Cut-off is 0.20 based on highest (sensitivity+specicity)","9db498f6":"making a DataFrame with actual and prob columns","9f6701a2":"before scoring new customers, you need to process the data using the sames steps you followed while building the model\n","94e64066":"Cut off based on highest sensitivity","7feebf32":"finding ideal cut-off for checking if this remains same in OOS validation"}}