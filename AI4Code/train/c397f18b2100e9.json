{"cell_type":{"4fbb2aaf":"code","3cbe666c":"code","8e043450":"code","c4762715":"code","6809175e":"code","08d03225":"code","6cfeb871":"code","0a8f9385":"code","d942f91d":"code","7d4ae030":"code","649c4d9c":"code","d89d948c":"code","e95458e2":"code","c76121db":"code","26681a03":"code","a801da70":"code","df973e4d":"code","039d8989":"code","c93fe7b8":"markdown","de894d76":"markdown","4b15a68c":"markdown","7bf4d8a1":"markdown"},"source":{"4fbb2aaf":"! apt update\n! apt install -y python3-dev zlib1g-dev libjpeg-dev cmake swig python-pyglet python3-opengl libboost-all-dev libsdl2-dev libosmesa6-dev patchelf ffmpeg xvfb\n! pip install git+https:\/\/github.com\/openai\/gym.git#egg=gym[box2d]\n! pip install xvfbwrapper\n\nfrom IPython.display import clear_output\n\nclear_output()\n\nfrom xvfbwrapper import Xvfb\nvdisplay = Xvfb(width=1280, height=740)\nvdisplay.start()\n\nimport numpy as np\nimport random\nimport matplotlib.pyplot as plt\nimport matplotlib.animation as animation\nfrom matplotlib import animation, rc\nfrom IPython.display import Math, HTML\n\nfrom pylab import rcParams\n\nrcParams['figure.figsize'] = 5, 3\n\nimport gym\n\ndef render_frames(env, num_frame=50):\n    env.reset()\n    frames = []\n    for i in range(num_frame):\n        _, _, done, _ = env.step( env.action_space.sample() )\n        if done:\n            env.reset()        \n        frames.append(  env.render(mode=\"rgb_array\") )\n        \n    return frames\n\ndef create_animation(frames):\n    rc('animation', html='jshtml')\n    fig = plt.figure()\n    plt.axis(\"off\")\n    im = plt.imshow(frames[0], animated=True)\n\n    def updatefig(i):\n        im.set_array(frames[i])\n        return im,\n\n    ani = animation.FuncAnimation(fig, updatefig, frames=len(frames), interval=len(frames)\/10, blit=True)\n    display(HTML(ani.to_html5_video()))    \n    plt.close()    \n    \n    return ani","3cbe666c":"env = gym.make('LunarLander-v2')\ngame_obs = env.reset()\n\nani = create_animation(render_frames(env, 300))","8e043450":"%load_ext tensorboard.notebook \n\n! mkdir logs\n! pip install tensorboardX\n\nclear_output()","c4762715":"%tensorboard --logdir .\/logs","6809175e":"\u03b3 = 0.99","08d03225":"env.observation_space.low, env.observation_space.high","6cfeb871":"import torch\nimport torch.nn as nn","0a8f9385":"class ValueFunction(nn.Module):\n    def __init__(self, input_size):\n        super().__init__()       \n        \n        self.V = nn.Sequential(\n            nn.Linear(input_size, 128),\n            nn.SELU(),\n            nn.Linear(128, 256),\n            nn.SELU(),\n            nn.Linear(256, 1),\n        )        \n        \n    def forward(self, x):\n        v = self.V(x)\n        return v","d942f91d":"class SoftmaxPolicy(nn.Module):\n    def __init__(self, input_size, num_actions):\n        super().__init__()\n        \n        self.input_size = input_size\n        self.num_actions = num_actions\n        \n        self.\u03c0 = nn.Sequential(\n            nn.Linear(input_size, 128),\n            nn.SELU(),\n            nn.Linear(128, 256),\n            nn.SELU(),\n            nn.Linear(256, 256),\n            nn.SELU(),\n            nn.Linear(256, num_actions),\n            nn.LogSoftmax(dim=1)\n        )\n        \n    def forward(self, x):\n        logits = self.\u03c0(x)\n        \n        return logits","7d4ae030":"cc = 0","649c4d9c":"from torch.distributions.categorical import Categorical\ndef sample_action(logits):\n    return Categorical(logits=logits).sample()\n\nobs = env.reset()\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\nimport itertools\n\nfrom tensorboardX import SummaryWriter\n\ncc = cc + 1\nwriter = SummaryWriter(\".\/logs\/{}\".format(cc))\n\nstep = 0\n\nmy_\u03c0 = SoftmaxPolicy(input_size=8, num_actions=4).to(device)\nmy_\u03c0_old = SoftmaxPolicy(input_size=8, num_actions=4).to(device)\nmy_V = ValueFunction( input_size=8 ).to(device)\n\n\nstate = torch.from_numpy(obs).float()\nlogits = my_\u03c0(state.unsqueeze(0).to(device))\nlogits.squeeze(0)\n\nsample_action(logits.squeeze(0).cpu())","d89d948c":"def roll_out(env, \u03c0):\n    obs = env.reset()\n    \n    memory = []\n    rewards = []\n    states = []\n    actions = []\n    logps = []\n    \u03c0.eval()\n    with torch.no_grad():\n        for i in itertools.count():\n            state = torch.from_numpy(obs).float()\n            logits = \u03c0(state.unsqueeze(0).to(device))\n            act = sample_action(logits.cpu().squeeze())\n            obs, reward, is_done, info = env.step(act.item())\n            \n#             reward = 1.0\n            states.append(state)\n            actions.append(act)\n            rewards.append(torch.tensor(reward\/200.0, dtype=torch.float)) # .tanh())\n            logps.append(logits[0,act.item()])\n            \n            if is_done:\n                break\n                \n    values = []\n    acc = 0.0\n#     c = 0\n    for r in reversed(rewards):\n#         c += 1\n        acc = \u03b3*acc + r\n        values.insert(0, acc )\n\n    return zip(states, actions, values, logps)\n\n","e95458e2":"from torch.utils.data import DataLoader\n\nparams = itertools.chain(my_\u03c0.parameters(), my_V.parameters() )\noptim = torch.optim.Adam(params,  lr=1e-4)\n\nimport math\n\nfrom torch.nn.utils import clip_grad_value_","c76121db":"eps = 0.2\n\nfor loop in range(800):\n    experiences = []\n\n    for episole in range(30):\n        experiences.extend( roll_out(env, my_\u03c0) )\n    \n    dataloader = DataLoader(experiences, batch_size=32, shuffle=True)\n\n    my_V.train()\n        \n    my_\u03c0.train()\n    \n    my_\u03c0_old.load_state_dict( my_\u03c0.state_dict() )\n    my_\u03c0_old.eval()\n    \n    for state, action, value, logp_old in dataloader:\n        step += 1\n        ii = state.to(device)\n        ### V loss\n        vv = my_V(ii)\n        vv_loss = nn.functional.mse_loss(input=vv, target =value.unsqueeze(1).to(device) )\n\n        writer.add_scalar(\"V_loss\", vv_loss.item(), step)        \n        \n        ## Policy gradient\n        \n        logits = my_\u03c0(ii)\n        logp = logits.gather(dim=1, index=action.unsqueeze(1).to(device)) \n        with torch.no_grad():\n#             logits_old = my_\u03c0_old(ii)\n#             logp_old = logits_old.gather(dim=1, index=action.unsqueeze(1).to(device)) \n            advantage = value.unsqueeze(1).to(device) - vv\n            \n        ratio = (logp - logp_old.unsqueeze(1).to(device) ).exp()\n        \n        entropy = - logits * logits.exp()\n        pi_loss = - torch.min( ratio * advantage, torch.clamp(ratio, min=1.0 - eps, max=1.0 + eps) * advantage ) - 1e-4 * entropy\n\n        pi_loss = pi_loss.mean()\n                \n        loss = pi_loss + vv_loss\n    \n        writer.add_scalar(\"mean reward\", value.mean().item(), step)\n        writer.add_scalar(\"pi_loss\", pi_loss.item(), step)\n        writer.add_scalar(\"loss\", loss.item(), step)\n        \n        optim.zero_grad()\n        loss.backward()\n\n        clip_grad_value_( params , 1.0)\n        optim.step()\n","26681a03":"loop","a801da70":"obs = env.reset()","df973e4d":"def render_frames_with_env(env, \u03c0):\n\n    frames = []\n    \n    \u03c0.eval()\n    for i in range(10):\n        obs = env.reset()\n        with torch.no_grad():\n            for i in itertools.count():\n                state = torch.from_numpy(obs).float()\n                logits = \u03c0(state.unsqueeze(0).to(device))\n                act = sample_action(logits.cpu().squeeze())\n                obs, reward, is_done, info = env.step(act.item())\n\n                frames.append(  env.render(mode=\"rgb_array\") )\n\n                if is_done:\n                    break\n                \n    return frames\n\ndef create_animation(frames):\n    rc('animation', html='jshtml')\n    fig = plt.figure()\n    plt.axis(\"off\")\n    im = plt.imshow(frames[0], animated=True)\n\n    def updatefig(i):\n        im.set_array(frames[i])\n        return im,\n\n    ani = animation.FuncAnimation(fig, updatefig, frames=len(frames), interval=20, blit=True)\n    display(HTML(ani.to_html5_video()))    \n    plt.close()    \n    \n    return ani","039d8989":"ani = create_animation(render_frames_with_env(env, my_\u03c0))","c93fe7b8":"## Setup","de894d76":"## Results\n","4b15a68c":"## PPO","7bf4d8a1":"## Tensorboard"}}