{"cell_type":{"a13f63b1":"code","5db2b0c5":"code","d0796b29":"code","33290826":"code","fa6d39b5":"code","83644e22":"code","2f229c4c":"code","ff290586":"code","ce3ee84a":"code","5a2fdc6a":"code","cd62e821":"code","72207d68":"code","e1a2cdf3":"markdown","040236e2":"markdown","8aa7d3f8":"markdown","67272f79":"markdown","d1cd5ada":"markdown"},"source":{"a13f63b1":"import torch\nimport numpy as np\nimport matplotlib.pyplot as plt\ntorch.manual_seed(42)\nimport base64\nfrom tqdm import trange\nfrom functools import reduce","5db2b0c5":"def to_base_64(string):\n    bytes_from_string = bytes(string, 'utf-8')\n    return(base64.b64encode(bytes_from_string))\n    \ndef from_base_64(bytes_):\n    encoded_string = base64.b64decode(bytes_)\n    return encoded_string.decode('utf-8')","d0796b29":"print(from_base_64(\"Cmxvc3NlcyA9IFtdCmZvciBpIGluIHJhbmdlKDEwMDAwMCk6CiAgICBwcmVkcyA9IFggQCBXICsgYgogICAgbG9zcyA9IG1zZShwcmVkcywgWSkKICAgIGxvc3MuYmFja3dhcmQoKQogICAgCiAgICBsb3NzZXMuYXBwZW5kKG5wLmxvZyhsb3NzLml0ZW0oKSkpCiAgICAKICAgIHdpdGggdG9yY2gubm9fZ3JhZCgpOgogICAgICAgIFcgLT0gVy5ncmFkICogMC4yNQogICAgICAgIGIgLT0gYi5ncmFkICogMC4yNQogICAgICAgIFcuZ3JhZC56ZXJvXygpCiAgICAgICAgYi5ncmFkLnplcm9fKCkK\"))","33290826":"size=(1000,1000)\ntorch_tensor_actual = torch.rand(size=size)\ntorch_tensor_forecast = torch.rand(size=size)","fa6d39b5":"## Tutaj umie\u015b\u0107 sw\u00f3j kod\n\n\n###","83644e22":"## Umie\u015b\u0107 tutaj rozwi\u0105zanie zadania\n\n\n##","2f229c4c":"def mse(t1,t2):\n    elements = reduce((lambda x, y: x * y), t1.shape)\n    mse = (1\/elements) * torch.sum(torch.pow((t1 - t2),2))\n    return mse","ff290586":"X = torch.rand(size=(10000,3),requires_grad=False) * 0.1\n\nW_ = torch.tensor([[0.1,0.2], [0.3,0.4], [0.5,0.6]],dtype=torch.float32,requires_grad=False)\nb_ = torch.tensor([0.26,0.12],requires_grad=False)\nnoise_ = torch.normal(0,0.02,size=(10000, 2))\n\nY = X @ W_ + b_ + noise_\n\nW = torch.rand(size=(X.shape[-1],2),requires_grad=True)\nb = torch.rand(size=(2,),requires_grad=True)","ce3ee84a":"## Umie\u015b\u0107 tutaj sw\u00f3j kod\n\nlosses = []\nfor i in trange(100000):\n    preds = X @ W + b\n    loss = mse(preds, Y)\n    loss.backward()\n    \n    losses.append(np.log(loss.item()))\n    \n    with torch.no_grad():\n        W -= W.grad * 0.1\n        b -= b.grad * 0.1\n        W.grad.zero_()\n        b.grad.zero_()\n\n\n##","5a2fdc6a":"print(from_base_64(\"Cmxvc3NlcyA9IFtdCmZvciBpIGluIHJhbmdlKDEwMDAwMCk6CiAgICBwcmVkcyA9IFggQCBXICsgYgogICAgbG9zcyA9IG1zZShwcmVkcywgWSkKICAgIGxvc3MuYmFja3dhcmQoKQogICAgCiAgICBsb3NzZXMuYXBwZW5kKG5wLmxvZyhsb3NzLml0ZW0oKSkpCiAgICAKICAgIHdpdGggdG9yY2gubm9fZ3JhZCgpOgogICAgICAgIFcgLT0gVy5ncmFkICogMC4yNQogICAgICAgIGIgLT0gYi5ncmFkICogMC4yNQogICAgICAgIFcuZ3JhZC56ZXJvXygpCiAgICAgICAgYi5ncmFkLnplcm9fKCkK\"))","cd62e821":"W,b","72207d68":"plt.plot(losses)","e1a2cdf3":"### Zadanie pierwsze\nStw\u00f3rz funkcj\u0119 licz\u0105c\u0105 b\u0142\u0105d \u015bredniokwadratowy u\u017cywaj\u0105c wy\u0142\u0105cznie funkcji z biblioteki Pytorch i por\u00f3wnaj je z bazow\u0105 implementacj\u0105. \nWz\u00f3r dla b\u0142\u0119du \u015bredniokwadratowego: $$ \\frac{1}{n} * \\sum { (actual - forecast)^2} $$\n\nReferencyjne rozwi\u0105zanie u\u017cywaj\u0105c czystej pythonowej implementacji\n```python\ncriterion=torch.nn.MSELoss() \ncriterion(torch_tensor_actual,torch_tensor_forecast)\n# wynik: tensor(0.1669)\n```\n\n\n\nRozwi\u0105zanie zakodowane w formie bytes64:\nCmRlZiBtc2UodDEsdDIpOgogICAgZWxlbWVudHMgPSByZWR1Y2UoKGxhbWJkYSB4LCB5OiB4ICogeSksIHQxLnNoYXBlKQogICAgbXNlID0gKDEvZWxlbWVudHMpICogdG9yY2guc3VtKHRvcmNoLnBvdygodDEgLSB0MiksMikpCiAgICByZXR1cm4gbXNlCg==\n<br>\nDla odkodowania zadania u\u017cyj funkcji:\n\n```python\nprint(from_base_64(zakodowany_string))\n```\n","040236e2":"### Zadanie trzecie\nZaimplementuj liniow\u0105 regresj\u0119 w Pythorchu.\n\nJako fukcji straty u\u017cyj wcze\u015bniej zaimplementowanej funkcji b\u0142\u0119du \u015bredniokwadratowego.\n<br>\nModel funkcji liniowej zdefiniuj jako standardow\u0105 funkcj\u0119 liniowej regresji, tj. : $$ Y = W X + b $$\n\nGriadienty uzyskaj poprzez wywo\u0142anie funkcji ```loss.backward()```. Po wywo\u0142aniu owej funkcji nadpisane zostan\u0105 parametry ```W.grad``` oraz ```b.grad``` w przypadku naszego problemu.\n<br>\nPo uzyskaniu gradient\u00f3w nadpisz warto\u015bci wag i baiasu u\u017cywaj\u0105c formu\u0142y $$ W = W - \\alpha W.grad $$ $$ b = b - \\alpha b.grad $$\nWsp\u00f3\u0142czynnik alpha oznacza nasz wybrany learning rate. Zgodnie z konwencj\u0105 Pytorcha update ten powinien odbywa\u0107 sie w bloku otoczonym funkcj\u0105 ```with torch.no_grad()```\n\nPo zako\u0144czeniu update funkcji nale\u017cy zresetowa\u0107 gradient u\u017cyawj\u0105c funkcji ``` Zmienna.grad.zero_()```\n\nRozwi\u0105zanie zakodowane w formie bytes64:\nCmxvc3NlcyA9IFtdCmZvciBpIGluIHJhbmdlKDEwMDAwMCk6CiAgICBwcmVkcyA9IFggQCBXICsgYgogICAgbG9zcyA9IG1zZShwcmVkcywgWSkKICAgIGxvc3MuYmFja3dhcmQoKQogICAgCiAgICBsb3NzZXMuYXBwZW5kKG5wLmxvZyhsb3NzLml0ZW0oKSkpCiAgICAKICAgIHdpdGggdG9yY2gubm9fZ3JhZCgpOgogICAgICAgIFcgLT0gVy5ncmFkICogMC4yNQogICAgICAgIGIgLT0gYi5ncmFkICogMC4yNQogICAgICAgIFcuZ3JhZC56ZXJvXygpCiAgICAgICAgYi5ncmFkLnplcm9fKCkK","8aa7d3f8":"### Zadanie drugie\nStw\u00f3rz funkcj\u0119 hiperbolicznego tangentu oraz jej r\u00f3\u017cniczk\u0119.\n\nWz\u00f3r dla softmaxu to: $$ \\frac{e^{z} - e^{-z}}{e^{z} + e^{-z}} $$\n, natomiast r\u00f3\u017cniczka tej funkcji to: $$ 1 - tanh(z)^2 $$\n\nReferencyjne rozwi\u0105zanie u\u017cywaj\u0105c czysto Pytorchowej implementacji\n```python\nactivations = torch.tanh(torch_tensor_actual) \nderivative = 1 - activations**2\n# wynik: \ntensor([[0.4994, 0.4765, 0.8666,  ..., 0.7516, 0.8080, 0.9991],\n        [0.6122, 0.9291, 0.9442,  ..., 0.5714, 0.9692, 0.8199],\n        [0.8595, 0.9623, 0.6144,  ..., 0.5260, 0.9998, 0.7280],\n        ...,\n        [0.9372, 0.8336, 0.6664,  ..., 0.5818, 0.7004, 0.5721],\n        [0.9813, 0.6620, 0.9530,  ..., 0.6229, 0.5017, 0.9522],\n        [0.7807, 0.6983, 0.5640,  ..., 0.9956, 0.8378, 0.6645]])\n```\n\n\n\nRozwi\u0105zanie zakodowane w formie bytes64:\nCmFjdGl2YXRpb25zID0gKHRvcmNoLmV4cCh0b3JjaF90ZW5zb3JfYWN0dWFsKSAtIHRvcmNoLmV4cCgtdG9yY2hfdGVuc29yX2FjdHVhbCkpIC8gICh0b3JjaC5leHAodG9yY2hfdGVuc29yX2FjdHVhbCkgKyB0b3JjaC5leHAoLXRvcmNoX3RlbnNvcl9hY3R1YWwpKQpkZXJpdmF0aXZlID0gMSAtIGFjdGl2YXRpb25zKioyCg==","67272f79":"## [Pytorch](https:\/\/pytorch.org\/docs\/stable\/index.html) i tensory\nTensor zgodnie z definicj\u0105 jest wielowymiarow\u0105 struktur\u0105 trzymaj\u0105c\u0105 w sobie numeryczne. W zale\u017cno\u015bci od wymiar\u00f3w tensory mog\u0105 nosi\u0107 inne nazwy. Dla rz\u0119du zerowego jest to skalar, rz\u0119du pierwszego jest to wektor, natomiast rz\u0119du drugiego jest to matryca. W Pytorch'u jednak niezale\u017cnie od stopnia wszytko nazywamy tensorem. Dla optymalizacji ka\u017cdy tensor posiada tak\u017ce parametry takie jak typ, kt\u00f3re pozwalaj\u0105 na lepsze manipulowanie warto\u015bciami, jak i wp\u0142ywaj\u0105 na sam\u0105 optymalizacj\u0119. <br>\n\n![](https:\/\/www.kdnuggets.com\/wp-content\/uploads\/scalar-vector-matrix-tensor.jpg)\n\nTensor mo\u017cemy zdefiniowa\u0107, podaj\u0105c macierz numpy:\n\n```python\nnp_array = np.random.random(size=(10,10))\ntorch.tensor(np_array)\n```\n\nlub stworzy\u0107 zupe\u0142nie nowy tensor.\n\n```python\ntorch.rand(size=(10,10))\n```\n\nBiblioteka Pytorch posiada r\u00f3wnie\u017c zbi\u00f3r operacji na tensorach, w\u015br\u00f3d nich podstawowe operacje matematyczne z przeci\u0105\u017conymi operatorami takimi jak:\n- Dodawanie wyra\u017cone za pomoc\u0105 operatora \"+\"\n- Odejmowanie wyra\u017cone za pomoc\u0105 operatora \"-\"\n- Mno\u017cenie elementarne (element\u00f3w na tej samej pozycji mi\u0119dzy sob\u0105) wyra\u017cone za pomoc\u0105 operatora \"*\"\n- Mno\u017cenie macierzy wyra\u017cone za pomoc\u0105 operatora \"@\"\nTe same operacje maj\u0105 swoje odpowiedniki w funkcjach dost\u0119pnych w samej bibliotece. Oczywi\u015bcie biblioteka ta wpiera o wiele wi\u0119cej operacji z o wiele wy\u017cszym poziomem abstrakcji, lecz w wielu przypadkach niezb\u0119dne jest u\u017cywanie podstawowych narz\u0119dzi do kustomizacji swojego kodu.\n\n### Urz\u0105dzenia w Pytorch\n\nKolejnym wa\u017cnym konceptem w Pytorch'u s\u0105 urz\u0105dzenia. Pytorch wspiera zar\u00f3wno operacje na CPU jak i GPU. Wsparcie to jest szczeg\u00f3lnie istotne w przypadku uczenia stosunkowo du\u017cych modeli, przy kt\u00f3rych u\u017cycie nawet pojedynczego GPU mo\u017ce znacznie przy\u015bpieszy\u0107 proces uczenia. Najbardziej powszechn\u0105 deklaracj\u0105 urz\u0105dzenia dla biblioteki pytroch jest poni\u017csza linia kodu:\n```python\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n```\nPytorch domy\u015blnie u\u017cywa urz\u0105dzenia cpu, lecz kod bez wskazania takiego urz\u0105dzenia by\u0142by bardzo ci\u0119\u017cki do wykorzystania w praktyce, wi\u0119c wykorzystanie powy\u017cszej linii rozwi\u0105zuje w pewnym stopniu problem kompatybilno\u015bci.\n\n### [Warunki w Pytorch'u](https:\/\/pytorch.org\/docs\/stable\/generated\/torch.where.html)\n\nPodstawowym sposobem na u\u017cycie warunk\u00f3w w pythonie jest funkcja \"where\" z bibliotkeki Pytorch lub u\u017cycie przeci\u0105\u017conych operator\u00f3w. Funkcja where zachowuje wymiary tensora, natomiast u\u017cycie standardowej maski w Pytorchu spowoduje \"zp\u0142aszczenie\" tensora.  \n\nPrzyk\u0142ad: \n\nFunkcja ```torch_tensor_forecast[torch_tensor_forecast>0.8].shape ```  Zwr\u00f3ci output ```torch.Size([199708])```\n\n\n","d1cd5ada":"### Czym jest gradient?\nGradient to z definicji pole wektorowe wskazuj\u0105ce kierunku najszybszego wzrostu warto\u015bci w poszczeg\u00f3lnych punktach. Przy jego u\u017cyciu jeste\u015bmy w stanie uzyska\u0107 kierunek najszybszego spadku, a co za tym idzie najmniejszej warto\u015bci funkcji. W przypadku uczenia maszynowego, jak i uczenia g\u0142\u0119bokiego najcz\u0119stszym sposobem okre\u015blania jak dobrze dzia\u0142a algorytm jest u\u017cycie jednej z wielu funkcji straty. Funkcja ta maleje wraz ze wzrostem wydajno\u015bci modelu. <br>\n![](https:\/\/miro.medium.com\/max\/699\/1*mElyetzsTIJrNnKI8kTkCw.jpeg)"}}