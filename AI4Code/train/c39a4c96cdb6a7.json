{"cell_type":{"ee63a1f1":"code","a34b11f5":"code","3b621f95":"code","107bffe3":"code","953aae91":"code","d34c3ac8":"code","28397344":"code","2e33229e":"code","ff30afa1":"code","595b2390":"code","408794ba":"code","7570a8fb":"code","d688e305":"code","1779c0ca":"code","6d5be437":"code","409fdfe4":"code","2aa6a44f":"code","26c8989d":"code","47d717d3":"markdown","7c438d83":"markdown","efaad207":"markdown","2339c538":"markdown","571da436":"markdown","5ea4e707":"markdown","fc44cafd":"markdown","343cd9e3":"markdown","46246706":"markdown","dacaa1e2":"markdown","d50c0e29":"markdown"},"source":{"ee63a1f1":"class LeNet5(nn.Module):\n    def __init__(self):\n        super().__init__()\n        \n        self.layers = nn.Sequential(\n            \n            # C1\n            nn.Conv2d(1, 6, kernel_size=(5, 5), stride=(1, 1)),\n            nn.ReLU(),\n            \n            # S2\n            nn.AvgPool2d(kernel_size=(2, 2), stride=2, padding=0),\n            \n            # C3\n            nn.Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1)),\n            nn.ReLU(),\n            \n            # S4\n            nn.AvgPool2d(kernel_size=(2, 2), stride=2),\n            \n            # flat\n            nn.Flatten(start_dim=1, end_dim=-1),\n            \n            # C5\n            nn.Linear(in_features=400, out_features=120, bias=True),\n            nn.ReLU(),\n            \n            # F6\n            nn.Linear(in_features=120, out_features=84, bias=True),\n            nn.ReLU(),\n            \n            # OUTPUT\n            nn.Linear(in_features=84, out_features=10, bias=True),\n            nn.Softmax(dim=1),\n        )\n        \n\n    def forward(self, data):\n        x = self.layers(data)\n        return x","a34b11f5":"LeNet5()","3b621f95":"import torch\nimport torch.nn as nn\nfrom torch.utils.data import DataLoader, Dataset\nimport torch.nn.functional as F\nimport torchvision\nfrom torchvision import transforms\nfrom torch.utils.data import Subset\nimport torch.optim as optim\n\nfrom sklearn.model_selection import train_test_split\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport random\nimport time\nimport copy\n\nrandom_seed = 33\nrandom.seed(random_seed)\ntorch.manual_seed(random_seed)","107bffe3":"import torch\n\nif torch.cuda.is_available():\n    torch.backends.cudnn.deterministic = True\n\nprint(torch.cuda.is_available())\n\n!nvidia-smi # gpu check","953aae91":"class DigitDataset(Dataset):\n    # must define 3 func\n    def __init__(self, path, is_train, transform=None):\n        self.df = pd.read_csv(path)\n        self.transform = transform\n        self.is_train = is_train\n\n    def __len__(self):\n        return len(self.df)\n\n    def __getitem__(self, idx): \n        \n        # for train data(lebel included)\n        if self.is_train == True:\n            image = self.df.iloc[idx, 1:].values.reshape(28, 28).astype(np.uint8)\n            label = self.df.iloc[idx, 0].astype(np.uint8)\n        \n        # for test data(label excluded)\n        else:\n            image = self.df.iloc[idx, :].values.reshape(28, 28).astype(np.uint8)\n            label = 0\n        \n        image = self.transform(image)\n        return image, label","d34c3ac8":"transform = transforms.Compose([\n    transforms.ToTensor(),\n    transforms.Resize((32, 32)),\n    ])\n\ndigit_dataset = DigitDataset(\"..\/input\/digit-recognizer\/train.csv\", is_train=True, transform=transform)","28397344":"dataset = {}\ndataset_size = {}\n\ntrain_idx, val_idx = train_test_split(list(range(len(digit_dataset))), test_size=0.2, random_state=random_seed)\ndataset['train'] = Subset(digit_dataset, train_idx)\ndataset['valid'] = Subset(digit_dataset, val_idx)","2e33229e":"dataset_size['train'] = len(dataset['train'])\ndataset_size['valid'] = len(dataset['valid'])\nprint('train_data size:', dataset_size['train'])\nprint('valid_data_size:', dataset_size['valid'])","ff30afa1":"dataloaders, batch_num = {}, {}\nbatch_size = 128\n\ndataloaders['train'] = DataLoader(dataset['train'],\n                                  batch_size=batch_size, shuffle=True,\n                                  num_workers=8)\ndataloaders['valid'] = DataLoader(dataset['valid'],\n                                  batch_size=batch_size, shuffle=True,\n                                  num_workers=8)\n\nbatch_num['train'], batch_num['valid'] = len(dataloaders['train']), len(dataloaders['valid'])\nprint('batch_size : {},  train\/valid : {} \/ {}' .format(batch_size, batch_num['train'], batch_num['valid']))","595b2390":"dev = torch.device(\"cuda\") \n\nmodel = LeNet5()\nmodel = model.to(dev)\n\noptimizer = optim.Adam(model.parameters())\n\ncriterion = nn.CrossEntropyLoss()","408794ba":"num_epochs = 33\ntrn_loss_list, val_loss_list, trn_acc_list, val_acc_list = [], [], [], []\nbest_acc = 0.0\n\nsince = time.time()\nbest_model = copy.deepcopy(model.state_dict())\n\n\nfor epoch in range(num_epochs):    \n    trn_loss, trn_corrects, val_loss, val_corrects = 0.0, 0, 0.0, 0\n    \n    \n    # train data (33600)\n    model.train()\n    for data, target in dataloaders['train']:\n        data, target = data.to(dev), target.to(dev).long()\n\n        optimizer.zero_grad()\n        output = model(data)                \n    \n        loss = criterion(output, target)\n        loss.backward()\n        \n        optimizer.step()  \n        \n        _, preds = torch.max(output, 1)\n        trn_corrects += torch.sum(preds == target.data)\n        trn_loss += loss.item()\n\n        \n    # validation data (8400)\n    model.eval()\n    with torch.no_grad():\n        for val_data, val_target in dataloaders['valid']:\n            val_data, val_target = val_data.to(dev), val_target.to(dev).long()\n            \n            val_output = model(val_data)\n            v_loss = criterion(val_output, val_target)\n            \n            _, preds = torch.max(val_output, 1)\n            val_corrects += torch.sum(preds == val_target.data)\n            val_loss += v_loss.item()\n\n\n    # trace loss & accuracy        \n    trn_acc_list.append(trn_corrects * 0.003)\n    val_acc_list.append(val_corrects * 0.012)\n    trn_loss_list.append(trn_loss\/dataset_size['train'])\n    val_loss_list.append(val_loss\/dataset_size['valid'])\n    \n    # print current status\n    time_elapsed = time.time() - since\n    print(\"epoch: {}\/{} | trn loss: {:.4f} | val loss: {:.4f} | {:.0f}m {:.0f}s elapsed\".format(\n                epoch+1, num_epochs, trn_loss \/ dataset_size['train'], val_loss \/ dataset_size['valid'], \n                time_elapsed \/\/ 60, time_elapsed % 60))\n\n    # best model update(based on validation data accuracy)\n    if val_corrects * 0.012 > best_acc:\n      best_acc = val_corrects * 0.012\n      best_model = copy.deepcopy(model.state_dict())\n      print(\"best model updated-epoch: {} | val_accuracy: {:.4f}\".format(epoch+1, best_acc))\n\n    \n# print time\ntime_elapsed = time.time() - since\nprint('Training complete in {:.0f}m {:.0f}s'.format(time_elapsed \/\/ 60, time_elapsed % 60))\nprint('Best valid Acc: {:.1f}'.format(best_acc))\n\n# best model save\nmodel.load_state_dict(best_model)\ntorch.save(model.state_dict(), '\/kaggle\/working\/best_model.pt')\nprint(\"model saved\")","7570a8fb":"plt.plot(trn_loss_list, label='train_loss')\nplt.plot(val_loss_list, label='val_loss')\nplt.legend()","d688e305":"plt.plot(trn_acc_list, label='train_acc')\nplt.plot(val_acc_list, label='val_acc')\nplt.legend()","1779c0ca":"transform = transforms.Compose([\n    transforms.ToTensor(),\n    transforms.Resize((32, 32)),\n    ])\n\ndataset['test'] = DigitDataset(\"..\/input\/digit-recognizer\/test.csv\", is_train=False, transform=transform)\ndataloaders['test'] = DataLoader(dataset['test'],\n                                  batch_size=batch_size,\n                                  num_workers=8)\n\nprint('batch_size : {}, test : {}' .format(batch_size, len(dataloaders['test'])))","6d5be437":"best_model = LeNet5()\nbest_model.load_state_dict(torch.load('\/kaggle\/working\/best_model.pt'))\n\nbest_model.to(dev)","409fdfe4":"answer = pd.Series()\n\nmodel.eval()\nwith torch.no_grad():\n    for data, target in dataloaders['test']:\n        data, target = data.to(dev), target.to(dev)\n\n        output = best_model(data)\n        \n        _, preds = torch.max(output, 1)\n        \n        temp = pd.Series(data=preds.cpu().numpy())\n        answer = answer.append(temp, ignore_index=True)\n        \nanswer = answer.rename('label')","2aa6a44f":"imgid = pd.Series(data=range(1, 28001), name='ImageId')\n\nanswer = pd.concat([imgid, answer], axis=1)\n\nanswer","26c8989d":"answer.to_csv(\"submission.csv\", index=False)","47d717d3":"### Test Data","7c438d83":"### Load the best model","efaad207":"### Data transform\n\n- transforms.Resize automatically divides data by 255","2339c538":"### GPU","571da436":"### Custom dataset","5ea4e707":"### accuracy calculation\n\n- total number of train data = 33600\n- train acc = corrects \/ 33600 x 100 = corrects x **0.003**\n- total number of validation data = 8400\n- val acc = corrects \/ 8400 x 100 = corrects x **0.012**","fc44cafd":"## LeNet-5\n\n- LeNet-5 was created by Yann LeCun in 1998\n- It is widely used for the recognition of handwritten and machine-printed characters.\n\n\n![img](https:\/\/miro.medium.com\/max\/1000\/1*1TI1aGBZ4dybR6__DI9dzA.png)","343cd9e3":"### Test Score\n\n0.98496","46246706":"### Data Split\n\n- train(80%) \/ validation(20%)","dacaa1e2":"### Import Packages","d50c0e29":"### C1\n\n- convolutional layer\n- (1, 32, 32) -> (6, 28, 28)\n- Strides = 1 \n\n### S2\n\n- sub-sampling layer\n- (6, 28, 28) -> (6, 14, 14)\n- Strides = 2\n\n### C3\n\n- convolutional layer\n- (6, 14, 14) -> (16, 10, 10)\n- Strides = 1\n\n### S4\n\n- sub-sampling layer\n- (16, 10, 10) -> (16, 5, 5)\n- Strides = 2\n\n### C5\n\n- after unfolding\n- fully-connected layer\n- (400) -> (120)\n\n### F6\n\n- fully-connected layer\n- (120) -> (84) -> (10)\n"}}