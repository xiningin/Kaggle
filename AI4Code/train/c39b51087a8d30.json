{"cell_type":{"fad44c7c":"code","9a3c564e":"code","8303b013":"code","13db3bd7":"code","28e98cfb":"code","69a4b6c3":"code","fb4e94d3":"code","0cb6163d":"code","f3017027":"code","f430a36e":"code","df28d4eb":"code","9495fc33":"code","4a1da681":"code","04a5135f":"code","1111a172":"code","eddaae95":"code","e8b0e6ac":"code","c2ec4332":"code","a91feb45":"code","c0294f29":"code","4b1d5511":"code","4f2b33ee":"code","53708e45":"code","112fa9cd":"code","6f26672f":"code","bd3abc46":"code","b5429a4a":"code","8ebcce69":"code","e0c4a245":"code","437cda56":"code","c554337a":"code","6b10070e":"code","e919bb25":"markdown","dcc63482":"markdown","512b3297":"markdown","b9488574":"markdown","15a15220":"markdown","949a384f":"markdown","980dc23c":"markdown","ac472fe7":"markdown","aa806aba":"markdown","9fa9159a":"markdown","7da40305":"markdown","aa0d3bd0":"markdown","7580aeb9":"markdown","7ca5a114":"markdown","490bca6f":"markdown","ed482a41":"markdown","d536e7e4":"markdown","3bc5b694":"markdown","7b164739":"markdown","1525fa5e":"markdown","fc0c1c32":"markdown","364bfa30":"markdown","08d7e405":"markdown","71c92b9e":"markdown"},"source":{"fad44c7c":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split\n\n%matplotlib inline\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","9a3c564e":"# Read in training and test datasetes\n\ntraining_df = pd.read_csv(\"\/kaggle\/input\/titanic\/train.csv\")\ntesting_df = pd.read_csv(\"\/kaggle\/input\/titanic\/test.csv\")\n\nprint(training_df.shape)\nprint(testing_df.shape)","8303b013":"# Lets look at how values are in the training dataset\ntraining_df.head(5)","13db3bd7":"# Lets look at how values are in the test dataset\ntesting_df.head(5)","28e98cfb":"''' Deal with missing values. First, calculate the percentage of\nmissing values for every column, and plot them as a bar chart'''\n\nnull_vals = training_df.isnull().sum()\/len(training_df)*100\nnull_vals = pd.DataFrame(null_vals)\nnull_vals.reset_index(inplace = True)\nnull_vals.columns = [\"Feature\",\"Percent missing\"]\nplt.figure(figsize = (8,6))\nplt.xticks(rotation=45)\nsns.barplot(x = \"Feature\",y =\"Percent missing\",data = null_vals)","69a4b6c3":"# Dropping the cabin column which had a lot of missing values (~80%)\ntraining_df = training_df.drop([\"Cabin\"],axis = 1)\nprint(training_df.shape)\ntraining_df.head(3)","fb4e94d3":"'''Now lets see what are the datatypes of different columns \nand see what columns can be trasformed into categorical type'''\ntraining_df.info()","0cb6163d":"# Define a function to convert object types and string types to category type\n\ndef str_to_cat(training_df):\n    for p,q in training_df.items(): #training_df.items() is a generator in Python\n        if is_string_dtype(q): \n            training_df[p] = q.astype('category').cat.as_ordered()\n    return training_df","f3017027":"'''Lets define impute functions. Impute categorical NaNs with -1, \nwhere we add 1 to make it 0. For each \ncontinuous variables, we impute missing values with median values of that column'''\n\ndef mydf_to_nums(training_df, feature, null_status):\n    if not is_numeric_dtype(feature):\n        training_df[null_status] = feature.cat.codes + 1\n        \ndef mydf_imputer(training_df, feature, null_status, null_table):\n    if is_numeric_dtype(feature):\n        if pd.isnull(feature).sum() or (null_status in null_table):\n            filler = null_table[null_status] if null_status in null_table else feature.median()\n            training_df[null_status] = feature.fillna(filler)\n            null_table[null_status] = filler\n    return null_table   \n\ndef mydf_preprocessor(training_df, null_table):\n    '''null_table  = your table or None'''\n    \n    if null_table is None: \n        null_table = dict()\n    for p,q in training_df.items(): \n        null_table = mydf_imputer(training_df, q, p, null_table)\n    for p,q in training_df.items(): \n        mydf_to_nums(training_df, q, p)\n    training_df = pd.get_dummies(training_df, dummy_na = True)\n    res = [training_df, null_table]\n    return res","f430a36e":"# Checking the count for null values in each column. 'isnull().sum()' returns the count of nulls in each column\ntraining_df.isnull().sum()","df28d4eb":"''' Using seaborn to plot relationship between locations from where people embarked their journey \nvs how many survived from each location'''\n\nsns.catplot(x='Embarked', y='Survived', data=training_df,\n            kind='bar', palette='muted', ci=None)\nplt.show()","9495fc33":"# Using backfill method to impute data in Embarked column\ntraining_df['Embarked'].fillna(method='backfill', inplace=True)","4a1da681":"# For imputing data in 'Age' column we will use median method\ntraining_df['Age'].fillna(training_df['Age'].median(), inplace=True)","04a5135f":"# Now we can see all of the columns have non-null values\ntraining_df.isnull().sum()","1111a172":"'''Once data imputation is done we will start by converting object data types to categorical columns \nusing the data imputation functions'''\n\nfrom pandas.api.types import is_string_dtype,is_numeric_dtype\n\ntraining_df = str_to_cat(training_df)\ntrain_df,my_table = mydf_preprocessor(training_df,null_table = None)\nprint(train_df.shape)\ntrain_df.head(3)","eddaae95":"# After using the data imputation functions it can be seen that the data types are converted to float and int dtypes\ntrain_df.info()","e8b0e6ac":"# Lets see which columns have how many null values\ntesting_df.isnull().sum()","c2ec4332":"# Dropping the cabin column which had a lot of missing values (~80%)\ntesting_df = testing_df.drop([\"Cabin\"],axis = 1)\nprint(testing_df.shape)","a91feb45":"# For imputing data in 'Age' column we will use median method\ntesting_df['Age'].fillna(testing_df['Age'].median(), inplace=True)","c0294f29":"# Using median method to do data imputation for 'Fare' column using groupby of 'Pclass' column\nclass_fares = dict(testing_df.groupby('Pclass')['Fare'].median())\ntesting_df['median_fare'] = testing_df['Pclass'].apply(lambda x: class_fares[x])\ntesting_df['Fare'].fillna(testing_df['median_fare'], inplace=True, )\ndel testing_df['median_fare']","4b1d5511":"# Lets see how much null values we have using 'isnull().sum()' method\ntesting_df.isnull().sum()","4f2b33ee":"# Converting the object and string types to categorical columns using the data imputation functions\ntesting_df = str_to_cat(testing_df)\ntest_df,my_table = mydf_preprocessor(testing_df,null_table = None)\nprint(test_df.shape)\ntest_df.head(3)","53708e45":"test_df.info()","112fa9cd":"#Create X and Y for training dataset\nx_train = train_df.drop('Survived',axis = 1)\ny_train = train_df['Survived']\nprint(x_train.shape,y_train.shape)","6f26672f":"# Scaling the data using MixMaxScaler\n\nfrom sklearn.preprocessing import StandardScaler, MinMaxScaler\nscaler = MinMaxScaler().fit(x_train)\nscaled_X = scaler.transform(x_train)\nprint(scaled_X)","bd3abc46":"# Create X for test dataset\nx_test = test_df","b5429a4a":"# Scaling the data\nsc = MinMaxScaler().fit(x_test)\nsc_X = sc.transform(x_test)\nprint(sc_X)","8ebcce69":"# Lets try and train a RandomForestClassifier model\nfrom sklearn.ensemble import RandomForestClassifier\nmodel_1 = RandomForestClassifier(n_jobs = -1, n_estimators = 10,\n                                 bootstrap = True)\nmodel_1.fit(scaled_X,y_train)","e0c4a245":"# Predicting the values based on training dataset\npred = model_1.predict(scaled_X)","437cda56":"''' Lets start with RandomForestRegressor and tune the hyperparameters by \npassing different values in 'n_estimators' and 'max_features'''\n\nfrom sklearn.model_selection import GridSearchCV\n\nparameters = {'n_estimators': [20, 50, 60, 80, 90, 100, 120, 150, 200], 'max_features': [\"auto\", \"sqrt\", \"log2\"]}\nrfc = RandomForestClassifier(random_state = 1)\ncls = GridSearchCV(estimator = rfc, param_grid = parameters)\ncls.fit(scaled_X, y_train)\n\n# Displaying the best params \ncls.best_params_","c554337a":"# Using the best param values lets apply to training the model\nmodel_2 = RandomForestClassifier(n_estimators=50, max_features=\"auto\")\nmodel_2.fit(scaled_X, y_train)\ny_pred = model_2.predict(sc_X)","6b10070e":"# Creating the submission file using the predicted target variable and PassengerID\nsub_df = pd.DataFrame({'PassengerId' : test_df[\"PassengerId\"],'Survived' : y_pred})\nsubmission = sub_df.to_csv('submission.csv',index=False)","e919bb25":"#### Using GridSearchCV (https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.model_selection.GridSearchCV.html) we can see that the best params are 'auto' for max_features and '50' for n_estimators. Lets try it out on our model and predict the values for test dataset","dcc63482":"### Step 5 : Hyperparameter tuning","512b3297":"### Step 3 : Data imputation","b9488574":"#### Contents: \n1. [Importing necessary modules](https:\/\/www.kaggle.com\/niramaykelkar\/titanic-predicting-the-survivors#Step-1-:-Reading-in-the-data)\n2. [Data preprocessing](https:\/\/www.kaggle.com\/niramaykelkar\/titanic-predicting-the-survivors#Step-2-:-Data-preprocessing)\n3. [Data Imputation](https:\/\/www.kaggle.com\/niramaykelkar\/titanic-predicting-the-survivors#Step-3-:-Data-imputation) \n4. [Training the model](https:\/\/www.kaggle.com\/niramaykelkar\/titanic-predicting-the-survivors#Step-4-:-Train-our-model)\n5. [Hyperparameter tuning](https:\/\/www.kaggle.com\/niramaykelkar\/titanic-predicting-the-survivors#Step-5-:-Hyperparameter-tuning)\n6. [Testing out the model](https:\/\/www.kaggle.com\/niramaykelkar\/titanic-predicting-the-survivors#Step-6-:-Testing-out-the-model)","15a15220":"### Step 6 : Testing out the model","949a384f":"### Step 1 : Reading in the data ","980dc23c":"As we can notice the 2 values which were missing in '**Embarked**' column are now filled.","ac472fe7":"#### Once initial predictions are done then lets begin with doing Hyperparameter tuning and selecting the best params using GridSearchCV","aa806aba":"#### For beginning journey towards becoming a contributor, this is the best competition to begin with. This is where the journey starts of Machine Learning and establishing yourself in the Kaggle community. \n\n#### There is a notebook mentioned in the competition overview which would help guide you in a step by step manner:\n- https:\/\/www.kaggle.com\/alexisbcook\/titanic-tutorial\/notebook","9fa9159a":"#### Lets begin with Imputing missing values in Embarked column using backfill method in Pandas\n- (https:\/\/pandas.pydata.org\/pandas-docs\/stable\/reference\/api\/pandas.DataFrame.fillna.html)","7da40305":"#### Now as we have done data imputations lets start by scaling the data and training the model","aa0d3bd0":"#### As we have already seen 'Cabin' has large number of missing values so lets drop it and do data imputations in Age and Fare columns","7580aeb9":"One thing you can notice is that there is no 'Survived' column in test dataset. The reason being in competitions you train your model on training dataset wherein your target column is already given but you have to predict the results for test dataset.","7ca5a114":"#### The data imputations to training dataset is done so lets begin with doing imputations on test data as well","490bca6f":"As we can see there are missing values in Age(177 out of 891 values) and Embarked(2 out of 891 values) columns. ","ed482a41":"The reference to ensemble method : [https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.ensemble.RandomForestClassifier.html](http:\/\/)","d536e7e4":"#### Now once we have defined the functions for imputations lets begin with actually imputing data in the columns. ","3bc5b694":"#### We can also notice that there are a few 'NaNs' in the Cabin columns so there is a possibility that other columns might also be having null values or NaNs which affect the accuracy of the model. So lets proceed and perform data preprocessing to know what columns have values missing and then impute values in columns.","7b164739":"References: \n* https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.preprocessing.MinMaxScaler.html\n* https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.preprocessing.StandardScaler.html#sklearn.preprocessing.StandardScaler","1525fa5e":"#### From the above plot, it looks like Cabin has ~80%missing values. It would be meaningless to impute or fill in 80% values, so we drop the column. We will impute Age (which has ~ 20% missing, but we'll try to impute), Fare, and Embarked column.These have very little missing values and would be easier to impute.","fc0c1c32":"### Step 2 : Data preprocessing","364bfa30":"The reference to the plot can be found at https:\/\/seaborn.pydata.org\/generated\/seaborn.catplot.html.\nFeel free to play around by plotting any different types of graphs as well","08d7e405":"## Titanic - Machine Learning from Disaster Competition","71c92b9e":"### Step 4 : Train our model"}}