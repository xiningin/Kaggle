{"cell_type":{"364271e2":"code","ee134b88":"code","b03a6d0d":"code","f041ab52":"code","afd4b04b":"code","68c47393":"code","b436cc26":"code","ae76328f":"code","bf74c842":"code","ec020287":"code","baa94319":"code","ac713318":"code","83cfeb6e":"code","4fc65de9":"code","7602dfe3":"code","9d815221":"code","9825d44a":"code","dcf55526":"code","4a0b178b":"code","31cf38f9":"code","d2ac75f4":"code","8a813a3e":"code","ee6b9011":"code","8c5f6ea6":"code","f4e6dc5f":"code","4ad7e47d":"code","96aa3fe5":"code","fd6176e4":"code","75a8536e":"code","d3b9bf50":"code","3c6f8742":"code","e5e8714c":"code","405882a6":"code","f2365442":"code","9b98c31f":"code","c2fd3bf7":"code","7f0dbafe":"code","9f8ff74a":"code","9166a26e":"code","e42d3271":"code","f5398659":"code","bff187aa":"code","3e5e6183":"code","6437af34":"code","65afbca3":"code","27d4a90a":"code","405a60fb":"code","c36b385d":"code","fd10cf1d":"code","e43da7ed":"code","cdda085d":"code","185d8dff":"code","9fec2064":"code","0a4ac1a5":"code","b06c0073":"code","14aa686c":"code","e41a4fb8":"code","6f8bdc6b":"code","f3557d01":"code","4890255c":"code","ce93dbb3":"code","d690cf9a":"code","b5b94398":"markdown","dc926973":"markdown","5a97ae4f":"markdown","f479be8b":"markdown","5b20ebc3":"markdown","9b984f55":"markdown","85bc8ce5":"markdown","0c82698d":"markdown","1a3e4baf":"markdown","e2113143":"markdown","96f0d1c4":"markdown","f0482a89":"markdown","0432acbe":"markdown","48d3dccb":"markdown","857ebb28":"markdown","5d295040":"markdown","470b46af":"markdown","769a8726":"markdown","3591e3c1":"markdown","7a5cda4e":"markdown","49601eed":"markdown","f54826ce":"markdown","b494ea49":"markdown"},"source":{"364271e2":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport warnings\nwarnings.filterwarnings('ignore')","ee134b88":"train = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/train.csv')\ntest = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/test.csv')","b03a6d0d":"#check whether columns of train & test are equivalent\ntest.columns.isin(train.columns).all()","f041ab52":"#dataset consists of both numeric and categorical data\ntrain.dtypes","afd4b04b":"len(train.columns)","68c47393":"#extract categorical columns for transformation\ncat_col = list(train.dtypes[train.dtypes == 'object'].reset_index()['index'])","b436cc26":"len(cat_col)","ae76328f":"num_col = list(train.columns[~train.columns.isin(cat_col)])","bf74c842":"num_col.remove('Id')\nnum_col.remove('SalePrice')","ec020287":"train.shape","baa94319":"import xgboost as xgb\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_absolute_error\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.preprocessing import LabelEncoder","ac713318":"for col in cat_col:\n    train[col] = LabelEncoder().fit_transform(train[col].astype(str))\nX = train[cat_col + num_col]","83cfeb6e":"y = train['SalePrice']","4fc65de9":"X_train, X_val, y_train, y_val = train_test_split(X,y,test_size=.1, random_state=46)","7602dfe3":"dtrain = xgb.DMatrix(X_train, label=y_train)\ndval = xgb.DMatrix(X_val, label=y_val)","9d815221":"train_mean = np.mean(y_train)\nprint(train_mean)","9825d44a":"#Create a vector of train_mean copy \nbaseline_prediction = np.ones(y_val.shape) * train_mean","dcf55526":"#MAE\nmean_absolute_error(y_val, baseline_prediction)","4a0b178b":"#RMSE\nmean_squared_error(y_val, baseline_prediction, squared=False)","31cf38f9":"params = {\n    'eta':.3,\n    'gamma':0,\n    'max_depth':6,\n    'min_child_weight': 1,\n    'subsample': 1,\n    'colsample_bytree': 1,\n    'objective':'reg:linear',\n}","d2ac75f4":"params['eval_metric'] = \"mae\"","8a813a3e":"num_boost_round = 300","ee6b9011":"model = xgb.train(\n    params,\n    dtrain,\n    num_boost_round=num_boost_round,\n    evals=[(dval, \"Validation\")],\n    early_stopping_rounds=10\n)","8c5f6ea6":"model.best_score","f4e6dc5f":"model.best_iteration+1","4ad7e47d":"#Use RMSE as metric\nparams['eval_metric'] = \"rmse\"","96aa3fe5":"model = xgb.train(\n    params,\n    dtrain,\n    num_boost_round=num_boost_round,\n    evals=[(dval, \"Validation\")],\n    early_stopping_rounds=10\n)","fd6176e4":"model.best_score","75a8536e":"model.best_iteration+1","d3b9bf50":"cv_results = xgb.cv(\n    params,\n    dtrain,\n    num_boost_round=num_boost_round,\n    seed=46,\n    nfold=5,\n    metrics={'mae'},\n    early_stopping_rounds=10\n)\n\ncv_results","3c6f8742":"#focus on test's MAE only in this notebook\n#reach minimum with 32 trees\ncv_results['test-mae-mean'].min()","e5e8714c":"#create combination of grid-search\ngridsearch_params = [\n    (max_depth, min_child_weight)\n    for max_depth in range(8,12)\n    for min_child_weight in range(4,8)\n]","405882a6":"table = {'max_depth':[],\n        'min_child_weight':[],\n         'no of round':[],\n        'MAE':[]\n        }","f2365442":"for max_depth, min_child_weight in gridsearch_params:\n    table['max_depth'].append(max_depth)\n    table['min_child_weight'].append(min_child_weight)\n\n    params['max_depth'] = max_depth\n    params['min_child_weight'] = min_child_weight\n\n    #use the same cv setting as above\n    cv_results = xgb.cv(\n        params,\n        dtrain,\n        num_boost_round=num_boost_round,\n        seed=46,\n        nfold=5,\n        metrics={'mae'},\n        early_stopping_rounds=10\n    )\n\n    mean_mae = cv_results['test-mae-mean'].min()\n    boost_rounds = cv_results['test-mae-mean'].argmin()\n\n    table['no of round'].append(boost_rounds)\n    table['MAE'].append(mean_mae)","9b98c31f":"depth_child_weight = pd.DataFrame(table)","c2fd3bf7":"depth_child_weight","7f0dbafe":"depth_child_weight[depth_child_weight['MAE']==depth_child_weight['MAE'].min()]","9f8ff74a":"params = {\n    'eta':.3,\n    'gamma':0,\n    'max_depth':8,\n    'min_child_weight': 5,\n    'subsample': 1,\n    'colsample_bytree': 1,\n    'objective':'reg:linear',\n}","9166a26e":"#still add the subsample\/colsample_bytree=1 for a complete comparsion\ngridsearch_params = [\n    (subsample, colsample_bytree)\n    for subsample in [i\/10. for i in range(6,11)]\n    for colsample_bytree in [i\/10. for i in range(6,11)]\n]","e42d3271":"table2 = {'subsample':[],\n        'colsample_bytree':[],\n         'no of round':[],\n        'MAE':[]\n        }\n\nfor subsample, colsample_bytree in gridsearch_params:\n    table2['subsample'].append(subsample)\n    table2['colsample_bytree'].append(colsample_bytree)\n\n    params['subsample'] = subsample\n    params['colsample_bytree'] = colsample_bytree\n\n    #use the same cv setting as above\n    cv_results = xgb.cv(\n        params,\n        dtrain,\n        num_boost_round=num_boost_round,\n        seed=46,\n        nfold=5,\n        metrics={'mae'},\n        early_stopping_rounds=10\n    )\n\n    mean_mae = cv_results['test-mae-mean'].min()\n    boost_rounds = cv_results['test-mae-mean'].argmin()\n\n    table2['no of round'].append(boost_rounds)\n    table2['MAE'].append(mean_mae)","f5398659":"subsample_colsample_bytree = pd.DataFrame(table2)","bff187aa":"subsample_colsample_bytree","3e5e6183":"subsample_colsample_bytree[subsample_colsample_bytree['MAE']==subsample_colsample_bytree['MAE'].min()]","6437af34":"gridsearch_params = [\n    (eta, gamma)\n    for eta in [0.3, 0.2, 0.1, 0.05, 0.01, 0.005]\n    for gamma in [i for i in range(0,6)]\n]","65afbca3":"table3 = {'eta':[],\n        'gamma':[],\n         'no of round':[],\n        'MAE':[]\n        }\n\nfor eta, gamma in gridsearch_params:\n    table3['eta'].append(eta)\n    table3['gamma'].append(gamma)\n\n    params['eta'] = eta\n    params['gamma'] = gamma\n\n    #use the same cv setting as above\n    cv_results = xgb.cv(\n        params,\n        dtrain,\n        num_boost_round=num_boost_round,\n        seed=46,\n        nfold=5,\n        metrics={'mae'},\n        early_stopping_rounds=10\n    )\n\n    mean_mae = cv_results['test-mae-mean'].min()\n    boost_rounds = cv_results['test-mae-mean'].argmin()\n\n    table3['no of round'].append(boost_rounds)\n    table3['MAE'].append(mean_mae)","27d4a90a":"eta_gamma = pd.DataFrame(table3)","405a60fb":"eta_gamma","c36b385d":"eta_gamma[eta_gamma['MAE']==eta_gamma['MAE'].min()]","fd10cf1d":"params = {\n    'eta':.05,\n    'gamma':0,\n    'max_depth':8,\n    'min_child_weight': 5,\n    'subsample': 1,\n    'colsample_bytree': 1,\n    'objective':'reg:linear'\n}","e43da7ed":"modified_model = xgb.train(\n    params,\n    dtrain,\n    num_boost_round=num_boost_round,\n    evals=[(dval, \"Validation\")],\n    early_stopping_rounds=10\n)","cdda085d":"modified_model.best_score","185d8dff":"modified_model.best_iteration+1","9fec2064":"pip install hyperopt","0a4ac1a5":"from hyperopt import hp, fmin, tpe, Trials, STATUS_OK","b06c0073":"#set the search space\nhp_space={ 'eta': hp.choice('learning_rate',[0.3, 0.2, 0.1, 0.05, 0.01, 0.005]),\n            'gamma': hp.uniform ('gamma', 0,9),\n            'max_depth': hp.quniform(\"max_depth\", 3, 18, 1),\n            'min_child_weight' : hp.quniform('min_child_weight', 0, 10, 1),\n            'subsample': hp.uniform('subsample', 0.5,1),\n           'colsample_bytree' : hp.uniform('colsample_bytree', 0.5,1)\n            }","14aa686c":"def hyperparameter_tuning(hp_space):\n    model=xgb.XGBRegressor(eta = hp_space['eta'],\n                           gamma = hp_space['gamma'],\n                           max_depth = int(hp_space['max_depth']), \n                           min_child_weight = hp_space['min_child_weight'],\n                           subsample = hp_space['subsample'],\n                          colsample_bytree = hp_space['colsample_bytree'])\n    \n    evaluation = [(X_train, y_train), (X_val, y_val)]\n    \n    model.fit(X_train, y_train,\n            eval_set=evaluation, eval_metric=\"rmse\",\n            early_stopping_rounds=10,verbose=False)\n\n    pred = model.predict(X_val)\n    #Setting squared to False will return the RMSE\n    rmse= mean_squared_error(y_val, pred, squared=False)\n    print (\"SCORE:\", rmse)\n    #change the metric if you like\n    return {'loss':rmse, 'status': STATUS_OK, 'model': model}","e41a4fb8":"trials = Trials()\nbest = fmin(fn=hyperparameter_tuning,\n            space=hp_space,\n            algo=tpe.suggest,\n            max_evals=100,\n            trials=trials)\n\nprint (best)","6f8bdc6b":"#update the parameter list\nhp_params = {\n    'eta': 0.2,\n    'gamma':  6.193111198291378,\n    'max_depth':17,\n    'min_child_weight': 4,\n    'colsample_bytree': 0.7874259674469981,\n    'subsample': 0.8740120727976152\n}","f3557d01":"hp_params['eval_metric'] = \"rmse\"\n\nhp_model = xgb.train(\n    hp_params,\n    dtrain,\n    num_boost_round=num_boost_round,\n    evals=[(dval, \"Validation\")],\n    early_stopping_rounds=10\n)","4890255c":"hp_model.best_score","ce93dbb3":"xg_val = xgb.DMatrix(X_val)\ny_val_pred = hp_model.predict(xg_val)","d690cf9a":"mean_squared_error(y_val, y_val_pred, squared=False)","b5b94398":"- eta: also known as learning rate, can be reagarded as step size in each boosting step\n- gamma: factor to determine whether prune the tree. If gain minus gamma is equal to negative values, branch of leaf node will be removed.","dc926973":"# 5-fold cross-validation","5a97ae4f":"The model get the best score with a max_depth of 8 and min_child_weight of 5, so let's update the parameter list.","f479be8b":"# Parameter tuning","5b20ebc3":"From the result, keep these 2 parameters as default value got the best MAE, so let's keep unchanged on subsample & colsample_bytree.","9b984f55":"# XG boost parameter tuning\n1. Data handling\n2. Baseline model\n3. Parameter introduction\n4. Model without tuning\n5. Parameter tuning by Grid Search\n6. Parameter tuning by Hyperopt","85bc8ce5":"The rsme of hp model is 20479, which is a bit more than the result of grid-search of 19814, however, much time is saved","0c82698d":"For clear demostration, I write the parameter tuning process step by step. \nYou can develop a function to perform above steps.\n\nWithout any tuning, the RMSE of xgboost is 21248. RMSE drops a little bit to 19814 after grid search. ","1a3e4baf":"# Parameter: eta & gamma","e2113143":"Without any machine learning techniques\/modeling, the mae is on average with $59222 off from the actual price. Thus, our target is to build a model with smaller mae as possible.","96f0d1c4":"- early_stopping_rounds\n\n= stop when no improvement observed after particular # of rounds","f0482a89":"# Baseline model\n\n- check the error before any modeling is performed\n- Just simply take the average of sales price of train set as the prediction, which is $181106","0432acbe":"# Parameter grid-search","48d3dccb":"#  Hyperopt: hyperparameter optimization","857ebb28":"Best MAE is 15488.916016 with 54 rounds, which is already better than the baseline model.","5d295040":"# Parameter: max_depth and min_child_weight","470b46af":"- Set evaluation metrics\n\nFor other evaluation choices: https:\/\/xgboost.readthedocs.io\/en\/latest\/parameter.html","769a8726":"# Parameter: subsample and colsample_bytree\n\n- control the sampling of the dataset\n- default value is 1 in these 2 parameters, meaning all rows and features will be used in the training\n- using partial dataset can avoid the model to \"memorize\/rote\" the pattern","3591e3c1":"Manual tuning had been introduced above. However, Grid Search or Random Search, where the former is a brute-force approac and the latter is purely random.\n\nHere comes to another choices for parameter tuning - Hyperopt. It is a powerful python library that search through an hyperparameter space of values . It implements three functions for minimizing the cost function: Random Search, TPE (Tree Parzen Estimators) and Adaptive TPE. The principle behind is by using Bayesian optimization algorithm.\n\nHyperopt contains 4 important features:\n1. Search Space\n    - specify ranges for input parameters, eg \n        - hp.choice(label, options) ***rmb the output refer to the position, not the value\n        - hp.uniform(label, low, high)\n            - Returns a value uniformly between low and high\n        - hp.quniform(label, low, high, q)\n            - Returns a value like round(uniform(low, high) \/ q) * q\n            \n2. Objective Function\n    - during the optimization process, prediction error will be evaluated and give it back to the optimizer. The optimizer will decide which values to check and iterate again\n3. fmin\n    - iterates on different sets of algorithms and their hyperparameters and then minimizes the objective function\n4. Trial Object\n    - keep All hyperparameters, loss, and other information\n    - can access them after running optimization","7a5cda4e":"- tune together to find a good trade-off between model bias and variance","49601eed":"# Parameter introduction\n1. eta\n\n    = learning_rate\n    - default=0.3\n\n2. gamma\n\n    = minimum loss reduction required to partition\n    \n    = larger the gamma, more conservative the algorithm\n    - default=0\n    \n3. max_depth \n\n    = maximum depth of a tree\n    \n    = deeper the tree, more complex the model & tend to overfit\n    \n    = deeper tree consumes more memory\n    - default=6\n    \n4. min_child_weight\n\n    = minimum sum of instance weight (hessian) needed in a child node\n    \n    = If min_child_weight > leaf node with the sum of instance weight --> stop partitioning\n    \n    = larger the min_child_weight, more conservative the algorithm\n    - default=1\n    \n5. subsample\n\n    = idea comes from Random Forests\n    \n    = fraction of training samples (randomly selected) used to train each tree\n    eg. subsample=0.5 --> randomly sample half of training data before growing trees\n    --> prevent overfitting\n    \n    = occur once in every boosting iteration\n    - default=1\n    \n6. colsample_bytree\n\n    = idea comes from Random Forests\n    \n    = fraction of features (randomly selected) used to train each tree\n    \n    = subsampling occurs once for every tree constructed\n    - default=1\n\n7. objective \n\n    = loss-functions\n    - default: \"reg:squarederror\" (regression with squared loss)\n    - \"reg:linear\": regression problems\n    - \"reg:logistic\": classifcation problems (show decision only, not probability)\n    - \"binary:logistic\": show probability rather than just decision\n    \nFor more details on xgboost parameters, please refer to https:\/\/xgboost.readthedocs.io\/en\/latest\/parameter.html\n\nAt the beginning, I first used default setting, apart from \"objective\" in xgboost to train the model.","f54826ce":"- Under xgboost api, its cv() method support n-fold cross-validation\n- \"rows\" = # of boosting trees used at each boosting iteration\n- Use this function find the best combination of parameters","b494ea49":"The reason why using labelencoder and 10% train-test split is the same:\nthe training set is too small with many attributes (79).\n\nIf using one-hot encoding, it will create even more columns\/attributes, which affect the learning."}}