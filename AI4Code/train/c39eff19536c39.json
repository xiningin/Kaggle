{"cell_type":{"84a5496b":"code","6a6add2d":"code","d8d1f2b0":"code","778de8d9":"code","bd63d83a":"code","162cfb06":"code","5a28aa23":"code","4998ebbd":"code","e1fde0b3":"code","985ac41f":"code","c70efdb7":"code","e926a6f7":"code","e97376f7":"code","9838ec01":"code","a4d3d362":"code","68043201":"code","af991738":"code","c5772f83":"code","103c5f54":"markdown","3d8beb0b":"markdown","ee41642c":"markdown"},"source":{"84a5496b":"import numpy as np\nimport pandas as pd\nimport tensorflow as tf\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\ntraining_images = pd.read_csv('..\/input\/sign-language-mnist\/sign_mnist_train\/sign_mnist_train.csv')\ntesting_images = pd.read_csv('..\/input\/sign-language-mnist\/sign_mnist_test\/sign_mnist_test.csv')\nprint(training_images.head())","6a6add2d":"print(testing_images.head())","d8d1f2b0":"training_images_x = training_images.drop('label', axis=1)\ntraining_images_x.shape","778de8d9":"testing_images_x = testing_images.drop('label', axis=1)\nprint(testing_images_x.shape)","bd63d83a":"training_images_x = tf.reshape(training_images_x,[27455,28,28,1])\ntesting_images_x = tf.reshape(testing_images_x,[7172,28,28,1])","162cfb06":"training_images_x.shape","5a28aa23":"testing_images_x.shape","4998ebbd":"import matplotlib.pyplot as plt\nimport matplotlib.image as mpimg\n\nfig=plt.figure(figsize=(15, 15))\ncol = 5\nrows = 5\nprint('Training data: \\n')\nfor i in range(1, col*rows +1):\n    fig.add_subplot(rows, col, i)\n    plt.imshow(tf.reshape(training_images_x[i],[28,28]))\nplt.show()","e1fde0b3":"fig=plt.figure(figsize=(15, 15))\ncol = 5\nrows = 5\nprint('Testing data: \\n')\nfor i in range(1, col*rows +1):\n    fig.add_subplot(rows, col, i)\n    plt.imshow(tf.reshape(testing_images_x[i],[28,28]))\nplt.show()","985ac41f":"training_images_y = training_images['label']\ntesting_images_y = testing_images['label']\ntraining_images_y.shape","c70efdb7":"# To prevent dimensions of the form (m,)\n# instead we want of the form (m,n)\n\ntraining_images_y.to_numpy()\ntraining_images_y = pd.DataFrame(training_images_y)\nprint(training_images_y.shape)","e926a6f7":"testing_images_y.to_numpy()\ntesting_images_y = pd.DataFrame(testing_images_y)\nprint(testing_images_y.shape)","e97376f7":"from tensorflow import keras\nfrom sklearn.preprocessing import LabelBinarizer","9838ec01":"# One vs all classification\nlabel_binrizer = LabelBinarizer()\ntraining_images_y = label_binrizer.fit_transform(training_images_y)","a4d3d362":"training_images_x = training_images_x\/255\ntesting_images_x = testing_images_x\/255","68043201":"def lenet5():\n    model = tf.keras.models.Sequential([ \n        tf.keras.layers.Conv2D(6,(5,5),activation=tf.nn.relu, input_shape = (28,28,1)),\n        tf.keras.layers.MaxPooling2D(2,2),\n        tf.keras.layers.Conv2D(16,(5,5),activation=tf.nn.relu),\n        tf.keras.layers.MaxPooling2D(2,2),\n        tf.keras.layers.Flatten(),\n        tf.keras.layers.Dense(256, activation = tf.nn.relu),\n        tf.keras.layers.Dense(84, activation = tf.nn.relu),\n        tf.keras.layers.Dense(24, activation = tf.nn.softmax), # 24 units because there are 24 classes in the dataset\n    ])\n\n    model.compile(optimizer='adam', loss=keras.losses.categorical_crossentropy, metrics=['accuracy'])\n\n    history = model.fit(\n        training_images_x, training_images_y, epochs = 5,\n    )\n    print(model.summary())\n    return model","af991738":"model = lenet5()\nfrom sklearn.metrics import accuracy_score\ny_pred = model.predict(testing_images_x)","c5772f83":"testing_images_y = label_binrizer.fit_transform(testing_images_y)\nprint('LeNet-5 test accuracy: ',accuracy_score(testing_images_y, y_pred.round()))","103c5f54":"   # **LeNet-5 implementation**\n\nI've implemented the LeNet-5 CNN architecture by Yann LeChun, Leon Bottou, Yoshua Bengio, and Patrick Haffner on this Sign Language MNIST dataset. The model includes Convolution layers, Max pooling layers and fully connected layers. Find the research paper [here](http:\/\/yann.lecun.com\/exdb\/publis\/pdf\/lecun-01a.pdf)","3d8beb0b":"![image.png](attachment:image.png)","ee41642c":"The dataset includes 28x28 sized grayscale pictures of sign language representations. The dataset has 24 classes in total accounting for the 24 alphabets in the American english literature excluding J and Z because they require motion."}}