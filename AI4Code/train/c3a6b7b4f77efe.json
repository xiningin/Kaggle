{"cell_type":{"4d0de606":"code","93c0dda4":"code","45a7777b":"code","558dc57d":"code","4e670540":"code","91f00e07":"code","47961a69":"code","def0f3f2":"code","1f10f1dd":"code","33292ce6":"code","362ca93c":"markdown","2486e202":"markdown","0342f68b":"markdown","891936cc":"markdown","8b333a5a":"markdown","c2443da2":"markdown","5fbba64e":"markdown","6771fedd":"markdown","31c9fe4c":"markdown","e3d6bc95":"markdown","18812fa4":"markdown","8398b90c":"markdown","895f9506":"markdown","b03923c3":"markdown"},"source":{"4d0de606":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom keras.utils import to_categorical\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Conv2D, MaxPool2D, Flatten, Dropout, BatchNormalization\nfrom keras.callbacks import EarlyStopping, ReduceLROnPlateau\nfrom keras.preprocessing.image import ImageDataGenerator\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import confusion_matrix\nimport itertools","93c0dda4":"train = pd.read_csv('\/kaggle\/input\/digit-recognizer\/train.csv')\ntest = pd.read_csv('\/kaggle\/input\/digit-recognizer\/test.csv')","45a7777b":"# divide training data into features and labels\nX_train = train.iloc[:,1:]\ny_train = train.iloc[:,0]","558dc57d":"# Normalize and reshape image\nX_train = X_train.values.reshape(-1, 28, 28, 1)\/255.\ntest = test.values.reshape(-1, 28, 28, 1)\/255.\n# One Hot encoding the label\ny_train = to_categorical(y_train, 10)","4e670540":"datagen = ImageDataGenerator(\n            rotation_range=15,\n            width_shift_range=0.1,\n            height_shift_range=0.1,\n            zoom_range=0.1,\n            shear_range=0.2\n            )","91f00e07":"def create_model():\n    model = Sequential()\n\n    model.add(Conv2D(32, (3,3), padding='same', input_shape=X_train.shape[1:], activation='relu'))\n    model.add(Conv2D(32, (3,3), padding='same', activation='relu'))\n    model.add(BatchNormalization())\n    model.add(MaxPool2D(2,2))\n    model.add(Dropout(0.2))\n\n    model.add(Conv2D(64, (3,3), padding='same', activation='relu'))\n    model.add(Conv2D(64, (3,3), padding='same', activation='relu'))\n    model.add(BatchNormalization())\n    model.add(MaxPool2D(2,2))\n    model.add(Dropout(0.2))\n\n    model.add(Conv2D(128, (3,3), padding='same', activation='relu'))\n    model.add(Conv2D(128, (3,3), padding='same', activation='relu'))\n    model.add(BatchNormalization())\n    model.add(MaxPool2D(2,2))\n    model.add(Dropout(0.2))\n    \n    model.add(Flatten())\n    model.add(Dense(256, activation='relu'))\n    model.add(BatchNormalization())\n    model.add(Dropout(0.5))\n    model.add(Dense(10, activation='softmax'))\n    \n    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n    \n    return model","47961a69":"EPOCHS = 30\nBATCH_SIZE = 50\nENSEMBLES = 5 # number of models to ensemble\nresult_list = [] # store results for correlation matrix\nhistories = [] # store histories for training and validation curves\nresults = np.zeros((test.shape[0],10))\n\ncallback_list = [\n    ReduceLROnPlateau(monitor='val_loss', factor=0.25, patience=2),\n    EarlyStopping(monitor='val_loss', min_delta=0.0005, patience=4)\n]\n\nfor i in range(ENSEMBLES):\n    # split training and validation sets\n    X_train_tmp, X_val, y_train_tmp, y_val = train_test_split(X_train, y_train, test_size=0.1, random_state=i)\n    # create model\n    model = create_model()\n    # fit the model\n    print('training No.', i)\n    history = model.fit(datagen.flow(X_train_tmp, y_train_tmp, batch_size=BATCH_SIZE),\n                   verbose=0,\n                   epochs=EPOCHS,\n                   callbacks=callback_list,\n                   validation_data=(X_val, y_val),\n                   steps_per_epoch=X_train_tmp.shape[0] \/\/ BATCH_SIZE)\n    # save results\n    histories.append(history)\n    result = model.predict(test)\n    results += result\n    result_list.append(result)","def0f3f2":"# check correlation of each predictions\ncorr_preds = pd.DataFrame([np.argmax(result, axis=1) for result in result_list]).T.corr()\nfig = sns.heatmap(corr_preds, annot=True, fmt='.3f', cmap='rainbow')\nfig.set_title('Predictions correlation matrix', fontsize=16, y=1.05)\nplt.show()","1f10f1dd":"fig, ax = plt.subplots(2, 1, figsize=(12,6))\n\nfor e in range(ENSEMBLES):\n    loss = histories[e].history['loss']\n    val_loss = histories[e].history['val_loss']\n    acc = histories[e].history['accuracy']\n    val_acc = histories[e].history['val_accuracy']\n    ax[0].set_title('loss')\n    ax[0].plot(loss, 'b', linewidth=1)\n    ax[0].plot(val_loss, 'r', linewidth=1)\n    ax[0].grid(color='black', linestyle='-', linewidth=0.2)\n    ax[1].set_title('accuracy')\n    ax[1].plot(acc, 'b', linewidth=1)\n    ax[1].plot(val_acc, 'r', linewidth=1)\n    ax[1].grid(color='black', linestyle='-', linewidth=0.2)\n    \nax[0].legend(['Training loss', 'Validation loss'], shadow=True)     \nax[1].legend(['Training accuracy', 'Validation accuracy'], shadow=True)\n\nplt.tight_layout()\nplt.show()","33292ce6":"results = np.argmax(results, axis=1)\nresults = pd.Series(results, name='Label')\nsubmission = pd.concat([pd.Series(range(1,28001), name='ImageID'), results], axis=1)\nsubmission.to_csv('submission.csv', index=False)","362ca93c":"<a id=\"1\"><\/a>\n<h1 style='background:navy; border:0; color:white'><center>1. Import required libraries<\/center><\/h1>","2486e202":"<a id=\"3.1\"><\/a>\n## 3.1 Define the model","0342f68b":"<a id=\"6\"><\/a>\n<h1 style='background:navy; border:0; color:white'><center>6. Create submission<\/center><\/h1>","891936cc":"<a id=\"5.1\"><\/a>\n## 5.1 Predictions correlation matrix","8b333a5a":"<a id=\"2.2\"><\/a>\n## 2.2 Optimize data","c2443da2":"# MNIST using Keras CNN Ensemble(0.996 accuracy)\nThis notebook covers CNN, ensemble and visualization process step-by-step.\nI hope you find this notebook useful!","5fbba64e":"* [1. Import required libraries](#1)\n* [2. Data preparation](#2)\n    * [2.1 load dataset](#2.1)\n    * [2.2 Optimize data](#2.2)\n    * [2.3 Data Augmentation](#2.3)\n* [3. Build CNN](#3)\n    * [3.1 Define the model](#3.1)\n* [4. Training](#4)\n* [5. Evaluate the model](#5)\n    * [5.1 Predictions correlation matrix](#5.1)\n    * [5.2 Training and validation curves](#5.2)\n* [6. Create submission](#6)","6771fedd":"<a id=\"5.2\"><\/a>\n## 5.2 Training and validation curves","31c9fe4c":"<a id=\"2.3\"><\/a>\n## 2.3 Data Augmentation","e3d6bc95":"<a id=\"2\"><\/a>\n<h1 style='background:navy; border:0; color:white'><center>2. Data preparation<\/center><\/h1>","18812fa4":"<a id=\"4\"><\/a>\n<h1 style='background:navy; border:0; color:white'><center>4. Training<\/center><\/h1>","8398b90c":"<a id=\"3\"><\/a>\n<h1 style='background:navy; border:0; color:white'><center>3. Build CNN<\/center><\/h1>","895f9506":"<a id=\"2.1\"><\/a>\n## 2.1 load dataset","b03923c3":"<a id=\"5\"><\/a>\n<h1 style='background:navy; border:0; color:white'><center>5. Evaluate the model<\/center><\/h1>"}}