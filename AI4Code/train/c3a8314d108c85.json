{"cell_type":{"da6684d7":"code","d3d4e8a6":"code","6e88e43c":"code","cfbd8f26":"code","0761d490":"code","8d9f15a3":"code","762880e2":"code","ddbaf847":"code","153ffdae":"code","b4ccd960":"code","1a1eb517":"code","00b7494b":"code","884cbf89":"code","88eb02bb":"code","16dbbde9":"code","5cd4d98b":"code","a4830b53":"code","e90eaba9":"code","47b890ba":"code","5dff1ce8":"code","3c7c165a":"code","0f14f36c":"code","7f6f95b2":"code","d7ec360e":"code","86eae3ec":"code","75f52f0d":"code","3edf0eb8":"markdown","7a13e2a7":"markdown","bad26821":"markdown","16a5b148":"markdown","72cab736":"markdown","41176110":"markdown","7b67b2ff":"markdown","c2a2301e":"markdown","58e1ffe0":"markdown","aae18ff9":"markdown","4152d2b8":"markdown","631710f2":"markdown","e9f291d1":"markdown","b9f1dba1":"markdown","d6451297":"markdown","8e5f2388":"markdown","982a157f":"markdown","0924a63f":"markdown","25d845b6":"markdown","d7cfbfd1":"markdown","a9a7f368":"markdown","cc852e52":"markdown","1de7aca8":"markdown","4bece78e":"markdown","d797f606":"markdown","a2450010":"markdown"},"source":{"da6684d7":"import pip._internal as pip\npip.main(['install', '--upgrade', 'numpy==1.17.2'])\nimport numpy as np\nimport pandas as pd\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\nfrom lightgbm import LGBMClassifier\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\nfrom sklearn.experimental import enable_hist_gradient_boosting\nfrom sklearn.ensemble import AdaBoostClassifier, GradientBoostingClassifier, HistGradientBoostingClassifier\nfrom sklearn.ensemble import BaggingClassifier, ExtraTreesClassifier, RandomForestClassifier\nfrom sklearn.ensemble import IsolationForest\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score, confusion_matrix\nfrom sklearn.model_selection import cross_val_predict, cross_val_score, GridSearchCV\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.svm import SVC\nfrom sklearn.utils.multiclass import unique_labels\nfrom xgboost import XGBClassifier\n\nimport time\nimport pickle\n\nfrom lwoku import get_prediction, add_features\nfrom grid_search_utils import plot_grid_search, table_grid_search","d3d4e8a6":"# Read training and test files\nX_train = pd.read_csv('..\/input\/learn-together\/train.csv', index_col='Id', engine='python')\nX_test = pd.read_csv('..\/input\/learn-together\/test.csv', index_col='Id', engine='python')\n\n# Define the dependent variable\ny_train = X_train['Cover_Type'].copy()\n\n# Define a training set\nX_train = X_train.drop(['Cover_Type'], axis='columns')","6e88e43c":"# Add new features\nprint('     Number of features (before adding): {}'.format(len(X_train.columns)))\nX_train = add_features(X_train)\nX_test = add_features(X_test)\nprint('     Number of features (after adding): {}'.format(len(X_train.columns)))","cfbd8f26":"print('  -- Drop features')\nprint('    -- Drop columns with few values (or almost)')\nfeatures_to_drop = ['Soil_Type7', 'Soil_Type8', 'Soil_Type15', 'Soil_Type27', 'Soil_Type37']\nX_train.drop(features_to_drop, axis='columns', inplace=True)\nX_test.drop(features_to_drop, axis='columns', inplace=True)\nprint('    -- Drop columns with low importance')\nfeatures_to_drop = ['Soil_Type5', 'Slope', 'Soil_Type39', 'Soil_Type18', 'Soil_Type20', 'Soil_Type35', 'Soil_Type11',\n                    'Soil_Type31']\nX_train.drop(features_to_drop, axis='columns', inplace=True)\nX_test.drop(features_to_drop, axis='columns', inplace=True)\nprint('     Number of features (after drop): {}'.format(len(X_train.columns)))","0761d490":"features = X_train.columns.to_list()\nwith open(\"features.pickle\", \"wb\") as fp:\n    pickle.dump(features, fp)","8d9f15a3":"with open('..\/input\/tactic-03-hyperparameter-optimization-lr\/clf_liblinear.pickle', 'rb') as fp:\n    clf = pickle.load(fp)\nlr_li_clf = clf.best_estimator_\nlr_li_clf","762880e2":"with open('..\/input\/tactic-03-hyperparameter-optimization-lr\/clf_saga.pickle', 'rb') as fp:\n    clf = pickle.load(fp)\nlr_sa_clf = clf.best_estimator_\nlr_sa_clf","ddbaf847":"with open('..\/input\/tactic-03-hyperparameter-optimization-lr\/clf_sag.pickle', 'rb') as fp:\n    clf = pickle.load(fp)\nlr_sg_clf = clf.best_estimator_\nlr_sg_clf","153ffdae":"with open('..\/input\/tactic-03-hyperparameter-optimization-lr\/clf_lbfgs.pickle', 'rb') as fp:\n    clf = pickle.load(fp)\nlr_lb_clf = clf.best_estimator_\nlr_lb_clf","b4ccd960":"with open('..\/input\/tactic-03-hyperparameter-optimization-lr\/clf_newton-cg.pickle', 'rb') as fp:\n    clf = pickle.load(fp)\nlr_clf = clf.best_estimator_\nlr_clf","1a1eb517":"with open('..\/input\/tactic-03-hyperparameter-optimization-lda\/clf.pickle', 'rb') as fp:\n    clf = pickle.load(fp)\nlda_clf = clf.best_estimator_\nlda_clf","00b7494b":"with open('..\/input\/tactic-03-hyperparameter-optimization-knn\/clf.pickle', 'rb') as fp:\n    clf = pickle.load(fp)\nknn_clf = clf.best_estimator_\nknn_clf","884cbf89":"with open('..\/input\/tactic-03-hyperparameter-optimization-gnb\/clf.pickle', 'rb') as fp:\n    clf = pickle.load(fp)\ngnb_clf = clf.best_estimator_\ngnb_clf","88eb02bb":"svc_clf = SVC(random_state=42,\n              verbose=True)","16dbbde9":"with open('..\/input\/tactic-03-hyperparameter-optimization-bagging\/clf.pickle', 'rb') as fp:\n    clf = pickle.load(fp)\nbg_clf = clf.best_estimator_\nbg_clf","5cd4d98b":"with open('..\/input\/tactic-03-hyperparameter-optimization-xtra-trees\/clf.pickle', 'rb') as fp:\n    clf = pickle.load(fp)\nxt_clf = clf.best_estimator_\nxt_clf","a4830b53":"with open('..\/input\/tactic-03-hyperparameter-optimization-rf\/clf.pickle', 'rb') as fp:\n    clf = pickle.load(fp)\nrf_clf = clf.best_estimator_\nrf_clf","e90eaba9":"with open('..\/input\/tactic-03-hyperparameter-optimization-adaboost\/clf.pickle', 'rb') as fp:\n    clf = pickle.load(fp)\nab_clf = clf.best_estimator_\nab_clf","47b890ba":"with open('..\/input\/tactic-03-hyperparameter-optimization-gb\/clf.pickle', 'rb') as fp:\n    clf = pickle.load(fp)\ngb_clf = clf.best_estimator_\ngb_clf","5dff1ce8":"with open('..\/input\/tactic-03-hyperparameter-optimization-lightgbm\/clf.pickle', 'rb') as fp:\n    clf = pickle.load(fp)\nlg_clf = clf.best_estimator_\nlg_clf","3c7c165a":"with open('..\/input\/tactic-03-hyperparameter-optimization-xgboost\/clf.pickle', 'rb') as fp:\n    clf = pickle.load(fp)\nxg_clf = clf.best_estimator_\nxg_clf","0f14f36c":"models = [\n          ('lr_li', lr_li_clf),\n          ('lr_sa', lr_sa_clf),\n          ('lr_sg', lr_sg_clf),\n          ('lr_lb', lr_lb_clf),\n          ('lr', lr_clf),\n          ('lda', lda_clf),\n          ('knn', knn_clf),\n          ('gnb', gnb_clf),\n#           ('svc', svc_clf),\n          ('bg', bg_clf),\n          ('xt', xt_clf),\n          ('rf', rf_clf),\n          ('ab', ab_clf),\n          ('gb', gb_clf),\n          ('lg', lg_clf),\n          ('xg', xg_clf)\n]","7f6f95b2":"results = pd.DataFrame(columns = ['Model',\n                                  'Accuracy',\n                                  'Fit time',\n                                  'Predict test set time',\n                                  'Predict train set time'])\n\nfor name, model in models:\n\n    # Fit\n    t0 = time.time()\n    model.fit(X_train, y_train)\n    t1 = time.time()\n    t_fit = (t1 - t0)\n    \n    # Predict test set\n    t0 = time.time()\n    y_test_pred = pd.Series(model.predict(X_test), index=X_test.index)\n    t1 = time.time()\n    t_test_pred = (t1 - t0)\n\n    # Predict train set\n    t0 = time.time()\n    y_train_pred = pd.Series(get_prediction(model, X_train, y_train), index=X_train.index)\n    accuracy = accuracy_score(y_train, y_train_pred)\n    t1 = time.time()\n    t_train_pred = (t1 - t0)\n\n    # Submit\n    y_train_pred.to_csv('train_' + name + '.csv', header=['Cover_Type'], index=True, index_label='Id')\n    y_test_pred.to_csv('submission_' + name + '.csv', header=['Cover_Type'], index=True, index_label='Id')\n    print('\\n')\n    \n    results = results.append({\n        'Model': name,\n        'Accuracy': accuracy,\n        'Fit time': t_fit,\n        'Predict test set time': t_test_pred,\n        'Predict train set time': t_train_pred\n    }, ignore_index = True)","d7ec360e":"results = results.sort_values('Accuracy', ascending=False).reset_index(drop=True)\nresults.to_csv('results.csv', index=True, index_label='Id')\nresults","86eae3ec":"tactic_03_results = pd.read_csv('..\/input\/tactic-03-hyperparameter-optimization\/results.csv', index_col='Id', engine='python')\ntactic_03_results","75f52f0d":"comparison = pd.DataFrame(columns = ['Model',\n                                     'Accuracy',\n                                     'Fit time',\n                                     'Predict test set time',\n                                     'Predict train set time'])\n\ndef get_increment(df1, df2, model, column):\n    model1 = model.split('_', 1)[0]\n    v1 = float(df1[df1['Model'] == model1][column])\n    v2 = float(df2[df2['Model'] == model][column])\n    return '{:.2%}'.format((v2 - v1) \/ v1)\n\nfor model in results['Model']:\n    accuracy = get_increment(tactic_03_results, results, model, 'Accuracy')\n    fit_time = get_increment(tactic_03_results, results, model, 'Fit time')\n    predict_test_set_time = get_increment(tactic_03_results, results, model, 'Predict test set time')\n    predict_train_set_time = get_increment(tactic_03_results, results, model, 'Predict train set time')\n    comparison = comparison.append({\n        'Model': model,\n        'Accuracy': accuracy,\n        'Fit time': fit_time,\n        'Predict test set time': predict_test_set_time,\n        'Predict train set time': predict_train_set_time\n    }, ignore_index = True)    \n\ncomparison","3edf0eb8":"## [Ensemble Methods](https:\/\/scikit-learn.org\/stable\/modules\/classes.html#module-sklearn.ensemble)","7a13e2a7":"# AdaBoost classifier\n\nExamine the full hyperparameter optimization details for this model in the following notebook: [Tactic 03. Hyperparameter optimization. Adaboost](https:\/\/www.kaggle.com\/juanmah\/tactic-03-hyperparameter-optimization-adaboost)","bad26821":"## [Support Vector Machines](https:\/\/scikit-learn.org\/stable\/modules\/classes.html#module-sklearn.svm)","16a5b148":"## [Naive Bayes](https:\/\/scikit-learn.org\/stable\/modules\/classes.html#module-sklearn.naive_bayes)","72cab736":"# Bagging classifier\n\nExamine the full hyperparameter optimization details for this model in the following notebook: [Tactic 03. Hyperparameter optimization. Bagging](https:\/\/www.kaggle.com\/juanmah\/tactic-03-hyperparameter-optimization-bagging)","41176110":"There is a notable improve in the accuracy, except for knn.","7b67b2ff":"# C-Support Vector Classification\n\nExamine the full hyperparameter optimization details for this model in the following notebook: [Tactic 03. Hyperparameter optimization. SVC](https:\/\/www.kaggle.com\/juanmah\/tactic-03-hyperparameter-optimization-svc)","c2a2301e":"## [Discriminant Analysis](https:\/\/scikit-learn.org\/stable\/modules\/classes.html#module-sklearn.discriminant_analysis)","58e1ffe0":"## [Nearest Neighbors](https:\/\/scikit-learn.org\/stable\/modules\/classes.html#module-sklearn.neighbors)","aae18ff9":"# Prepare data","4152d2b8":"# XGBoost\n\nExamine the full hyperparameter optimization details for this model in the following notebook: [Tactic 03. Hyperparameter optimization. XGBoost](https:\/\/www.kaggle.com\/juanmah\/tactic-03-hyperparameter-optimization-xgboost)","631710f2":"# Compare","e9f291d1":"# Random forest classifier\n\nExamine the full hyperparameter optimization details for this model in the following notebook: [Tactic 03. Hyperparameter optimization. RF](https:\/\/www.kaggle.com\/juanmah\/tactic-03-hyperparameter-optimization-rf)","b9f1dba1":"# Gaussian Naive Bayes\n\nExamine the full hyperparameter optimization details for this model in the following notebook: [Tactic 03. Hyperparameter optimization. GNB](https:\/\/www.kaggle.com\/juanmah\/tactic-03-hyperparameter-optimization-gnb)","d6451297":"# LightGBM\n\nExamine the full hyperparameter optimization details for this model in the following notebook: [Tactic 03. Hyperparameter optimization. LightGBM](https:\/\/www.kaggle.com\/juanmah\/tactic-03-hyperparameter-optimization-lightgbm)","8e5f2388":"\n### Bagging\n","982a157f":"# Conclusions\n","0924a63f":"# Extra-trees classifier\n\nExamine the full hyperparameter optimization details for this model in the following notebook: [Tactic 03. Hyperparameter optimization. Xtra-trees](https:\/\/www.kaggle.com\/juanmah\/tactic-03-hyperparameter-optimization-xtra-trees)","25d845b6":"### Model list","d7cfbfd1":"# Gradient Boosting for classification\n\nExamine the full hyperparameter optimization details for this model in the following notebook: [Tactic 03. Hyperparameter optimization. GB](https:\/\/www.kaggle.com\/juanmah\/tactic-03-hyperparameter-optimization-gb)","a9a7f368":"# Introduction\n\nThe aim of this notebook is to optimize all the models of the notebook: [Tactic 01. Test classifiers\n](https:\/\/www.kaggle.com\/juanmah\/tactic-01-test-classifiers).\n\nThe models are fitted and predicted with the optimized parameters.\nThe results are collected at [Tactic 99. Summary](https:\/\/www.kaggle.com\/juanmah\/tactic-99-summary).","cc852e52":"# Linear Discriminant Analysis\n\nExamine the full hyperparameter optimization details for this model in the following notebook: [Tactic 03. Hyperparameter optimization. LDA](https:\/\/www.kaggle.com\/juanmah\/tactic-03-hyperparameter-optimization-lda).","1de7aca8":"## [Generalized Linear Models](https:\/\/scikit-learn.org\/stable\/modules\/classes.html#module-sklearn.linear_model)","4bece78e":"### Boosting","d797f606":"# k-nearest neighbors classifier\n\nExamine the full hyperparameter optimization details for this model in the following notebook: [Tactic 03. Hyperparameter optimization. KNN](https:\/\/www.kaggle.com\/juanmah\/tactic-03-hyperparameter-optimization-knn)","a2450010":"# Logistic Regression classifier\n\nExamine the full hyperparameter optimization details for this model in the following notebook: [Tactic 03. Hyperparameter optimization. LR](https:\/\/www.kaggle.com\/juanmah\/tactic-03-hyperparameter-optimization-lr)."}}