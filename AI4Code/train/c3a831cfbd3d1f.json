{"cell_type":{"4c017082":"code","d5a936f9":"code","a67a9b7d":"code","e8bd599f":"code","e202cce8":"code","df69d70a":"code","18f081ac":"code","f8eb8246":"code","e66bc348":"code","5888f8ed":"code","1caf1891":"code","85a39c22":"code","b45a510f":"code","5f19b66b":"code","2df68412":"code","91982961":"code","c1cf7203":"code","4e23d787":"code","5051f756":"code","a03b5c66":"code","a98e93fd":"code","d296f3af":"markdown","6012bfed":"markdown","de4d7727":"markdown","d3355312":"markdown","e14f3ca5":"markdown","096d5798":"markdown","246bd8b9":"markdown","8eb1ae39":"markdown","ba565b8a":"markdown","f6e87d5b":"markdown","6faff61c":"markdown","913df09a":"markdown","2195760c":"markdown","197d2b92":"markdown","6f9b9e1a":"markdown","ff73125b":"markdown","807dbcc4":"markdown","296d0707":"markdown","f00940a8":"markdown","bd17dc22":"markdown","759c979a":"markdown","5d7d1577":"markdown","725fa4d5":"markdown","f0d7675b":"markdown"},"source":{"4c017082":"import numpy as np \nimport pandas as pd\nimport os","d5a936f9":"data = pd.read_csv('\/kaggle\/input\/default-of-credit-card-clients-dataset\/UCI_Credit_Card.csv')\ndata.head()","a67a9b7d":"data.info()","e8bd599f":"data.describe()","e202cce8":"to_be_dropped = ['ID']\ncategorical_cols = ['SEX', 'EDUCATION', 'MARRIAGE']\nnumerical_cols = ['LIMIT_BAL', 'BILL_AMT1', 'BILL_AMT2', 'BILL_AMT3', 'BILL_AMT4', 'BILL_AMT5', 'BILL_AMT6', 'PAY_AMT1', 'PAY_AMT2',\n                  'PAY_AMT3', 'PAY_AMT4', 'PAY_AMT5', 'PAY_AMT6', 'PAY_1', 'PAY_2', 'PAY_3', 'PAY_4', 'PAY_5', 'PAY_6', 'AGE']\ntarget_col = ['will_pay']","df69d70a":"for col in ['PAY_0', 'PAY_2', 'PAY_3', 'PAY_4', 'PAY_5', 'PAY_6']:\n    data[col] = np.where(data[col]<=0, 0, data[col]) ","18f081ac":"data.isnull().sum()","f8eb8246":"# drop nans\ndata = data.dropna()\n\n# drop unneeded cols\ndata = data.drop(to_be_dropped, axis=1)","e66bc348":"data.info()","5888f8ed":"# change some columns name to be more understandable\ndata = data.rename(columns={'PAY_0': 'PAY_1', 'default.payment.next.month': 'will_pay'})\n\n# replace unknown values with nans\ndata['EDUCATION'] = data['EDUCATION'].replace('unknown', np.NaN) \ndata['MARRIAGE'] = data['MARRIAGE'].replace('unknown', np.NaN)\n\n# replace 'others' values with different name\ndata['EDUCATION'] = data['EDUCATION'].replace('others', 'other education') \ndata['MARRIAGE'] = data['MARRIAGE'].replace('others', 'other status')","1caf1891":"print(data['SEX'].value_counts(), '\\n')\nprint(data['EDUCATION'].value_counts(), '\\n')\nprint(data['MARRIAGE'].value_counts(), '\\n')\nprint(data['will_pay'].value_counts(), '\\n')","85a39c22":"# convert int to categorical\ndef int2cat(df, col, dic):\n    \"\"\"\n    Parameters:\n        df : dataframe object\n        col: column name in the dataframe\n        dic: int to categorical dictionary related to this column\n    Return:\n        df : return the dataframe with this column updated \n    \"\"\"\n    df[col] = df[col].apply(lambda x: dic[x])\n    return df\n\n\n\nsex_dic = {1:'male', 2:'female'}\neducation_dic = {1:'graduate school', 2:'university', 3:'high school', 4:'others', 5:'unknown', 6:'unknown', 0:'unknown'}\nmarriage_dic = {1:'married', 2:'single', 3:'others', 0: 'unknown'}\n\ncategorical_dics = [sex_dic, education_dic, marriage_dic]\n\nfor col, dic in list(zip(categorical_cols, categorical_dics)):\n    data = int2cat(data, col, dic)","b45a510f":"data.head()","5f19b66b":"from sklearn.model_selection import train_test_split\n\nx = data.drop('will_pay', axis=1)\ny = data['will_pay']","2df68412":"from sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import OneHotEncoder, MinMaxScaler, StandardScaler\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import confusion_matrix, classification_report, accuracy_score\n\n\n# Preprocess the categorical features\ncategorical_processor = Pipeline([\n    ('cat_encoder', OneHotEncoder(handle_unknown='ignore'))\n                                ])\n\nnumerical_processor = Pipeline([\n    ('standard_scaler', StandardScaler())\n                               ])\n\ndata_preprocessor = ColumnTransformer([\n    ('categorical_pre', categorical_processor, (categorical_cols)),\n    ('numerical_pre', numerical_processor, (numerical_cols))\n                                    ]) \n\npipeline = Pipeline([\n    ('data_preprocessing', data_preprocessor),\n    #('dt', RandomForestClassifier(max_depth=2, random_state=0))\n                    ])","91982961":"x_transformed = pipeline.fit_transform(x)","c1cf7203":"from imblearn.over_sampling import SMOTE\n\nsmote = SMOTE(random_state=101)\nsmote_x, smote_y = smote.fit_resample(x_transformed, y)","4e23d787":"print(smote_x.shape, smote_y.shape)","5051f756":"from sklearn.model_selection import KFold, cross_val_score\nfrom sklearn.svm import SVC\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier , GradientBoostingClassifier, AdaBoostClassifier\nfrom xgboost import XGBClassifier\n\nresults = {\n    'model': [],\n    'score': []\n}\n\nmodels = [\n    ('random_forest', RandomForestClassifier(n_estimators=10)),\n    ('svm', SVC(gamma='auto')),\n    ('decision_tree', DecisionTreeClassifier(max_depth = 3, class_weight = \"balanced\")),\n    ('xgboost', XGBClassifier())\n]\n\ncv = KFold(n_splits=5)\n\nfor (model_name, model) in models:\n    score = np.mean(cross_val_score(model, smote_x, smote_y, scoring='accuracy', cv=cv, n_jobs=1))\n    results['model'].append(model_name)\n    results['score'].append(score)\n","a03b5c66":"pd.DataFrame(results)","a98e93fd":"from sklearn.ensemble import RandomForestClassifier\nfrom sklearn.svm import SVC\n\n\nx_train, x_test, y_train, y_test = train_test_split(smote_x, smote_y, test_size=0.2)\n\nxgb = XGBClassifier(n_estimators=100, max_depth=5, booster='gbtree')\n\nxgb.fit(x_train, y_train)\n\ny_pred = xgb.predict(x_train)\nprint('\\n----------------------- TRAIN RESULTS ------------------------')\nprint(classification_report(y_train, y_pred))\n\ny_pred = xgb.predict(x_test)\nprint('\\n----------------------- TEST RESULTS -------------------------')\nprint(classification_report(y_test, y_pred))\n","d296f3af":"Thank you for spending time reading this notebook. if you find it useful, please give it upvote.","6012bfed":"Now, we have done the first step successfully in order to apply **one hot encoding**. the second step will be in the next section!","de4d7727":"# <a id=\"1\">Exploring the Data<\/a>","d3355312":"### convert int values to categorical variables in the dataframe","e14f3ca5":"by applying the **XGboost** model, we reached to **87% accuracy** on the test set and also high **F1-score** for the two classes. I think this may sound good result especially that we deal with high data imbalance. <br>\n\nFianlly, in the future work we can improve this result by applying the **GridSearch** technique in order to choose the best parameters that will give the model highest score!","096d5798":"### Apply SMOTE technique for oversampling ","246bd8b9":"# <a name=\"2\">Data Cleaning<\/a>","8eb1ae39":"by applying **K-Fold**, it ensures that every observation from the original dataset has the chance of appearing in training and test set. This is one among the best approach if we have a limited input data.","ba565b8a":"# <a name=\"6\">Conclusion<\/a>","f6e87d5b":"we see that **xgboost** model gives the highest score among all the models. so we will investigate in this model in the next section.","6faff61c":"### get the features and labels from the dataframe","913df09a":"### update some values","2195760c":"for `PAY_n values`, They all present an undocumented label -2. If 1,2,3, etc are the months of delay, 0 should be labeled 'pay duly' and every negative value should be seen as a 0","197d2b92":"# <a name=\"5\"> Model Training<\/a>","6f9b9e1a":"as the columns data has different types so we will deal with each type of them separately.","ff73125b":"Now, we need to apply **one hot encodings** on the categorical columns. and we will do this process in two steps: \n1. convert integer values to its original categorical values\n2. convert categorical values to one hot encoding","807dbcc4":"### create a pipeline for preprocessing and model training","296d0707":"### Remove NaNs","f00940a8":"# Customer Behaviour Prediction\n\nin this notebook we will prepare and clean the data then apply a predictive model to predict if a new client will pay the bill statement of the credict card or not.\n\n#### Content\n1. <a href=\"#1\">Exploring the Data<\/a><br>\n2. <a href=\"#2\">Data Cleaning<\/a><br>\n3. <a href=\"#3\">Data Preprocessing<\/a><br>\n4. <a href=\"#4\">K-Fold<\/a>\n5. <a href=\"#5\">Model Training<\/a>\n6. <a href=\"#6\">Conclusion<\/a>","bd17dc22":"It seems that the data is **unbalanced** as we see from the target values. we will handle this case later!","759c979a":"# <a name=\"3\">Data Preprocessing<\/a>","5d7d1577":"The unbalanced problem can add a bias to the model towards either of the two classes. one of the techniques to overcome this problem is **SMOTE**","725fa4d5":"# <a name=\"4\">K-Fold<\/a>","f0d7675b":"The main aim of **Data Cleaning** is to identify and remove errors & duplicate data, in order to create a reliable dataset. This improves the quality of the training data for analytics and enables accurate decision-making.\n\nin our dataset, we can apply some modifications on the dataframe in order to remove nans or outliers and improve the quality of the dataset."}}