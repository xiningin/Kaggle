{"cell_type":{"3858b36c":"code","bf157acc":"code","2d7847de":"code","6f6cd7d5":"code","1bf601a1":"code","c74f8eb6":"code","66a43e41":"code","e02c2813":"code","5c92f55e":"code","a1818061":"code","e9b435fc":"code","bce59b91":"code","68823b38":"code","104f3981":"code","a7a4836c":"code","2b320671":"code","d7c4af9c":"code","a67c2ce8":"markdown","1d4edb7e":"markdown","142df004":"markdown"},"source":{"3858b36c":"import numpy as np, pandas as pd, random as rn, os, gc, re, time\nstart = time.time()\nseed = 32\nos.environ['PYTHONHASHSEED'] = str(seed)\nos.environ['OMP_NUM_THREADS'] = '4'\nnp.random.seed(seed)\nrn.seed(seed)\nimport tensorflow as tf\nsession_conf = tf.ConfigProto(intra_op_parallelism_threads = 1,\n                              inter_op_parallelism_threads = 1)\ntf.set_random_seed(seed)\nsess = tf.Session(graph = tf.get_default_graph(), config = session_conf)\nfrom keras import backend as K\nK.set_session(sess)\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom sklearn.metrics import f1_score, precision_recall_curve, confusion_matrix\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import StratifiedKFold\n\nfrom keras.layers import Input, Dense, CuDNNLSTM, Bidirectional, Activation, Conv1D\nfrom keras.layers import Dropout, Embedding, GlobalMaxPooling1D, MaxPooling1D\nfrom keras.layers import Add, Flatten, BatchNormalization, GlobalAveragePooling1D\nfrom keras.layers import concatenate, SpatialDropout1D, CuDNNGRU, Lambda, GaussianDropout\nfrom keras.layers import PReLU, ReLU, ELU\nfrom keras import initializers, regularizers, constraints, optimizers, layers, callbacks\nfrom keras.callbacks import EarlyStopping, ModelCheckpoint, Callback\nfrom keras.initializers import he_normal, he_uniform,  glorot_normal\nfrom keras.initializers import glorot_uniform, zeros, orthogonal\nfrom keras.models import Model, load_model\nfrom keras.optimizers import Adam, RMSprop, SGD\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.callbacks import EarlyStopping, ModelCheckpoint\n\ntrain = pd.read_csv(\"..\/input\/train.csv\").fillna(\"missing\")\ntest = pd.read_csv(\"..\/input\/test.csv\").fillna(\"missing\")\n\nembedding_file1 = \"..\/input\/embeddings\/glove.840B.300d\/glove.840B.300d.txt\"\nembedding_file2 = \"..\/input\/embeddings\/paragram_300_sl999\/paragram_300_sl999.txt\"\n\nembed_size = 300\nmax_features = 100000\nmax_len = 60","bf157acc":"puncts = [',', '.', '\"', ':', ')', '(', '-', '!', '?', '|', ';', \"'\", '$', '&', \n          '\/', '[', ']', '>', '%', '=', '#', '*', '+', '\\\\', '\u2022',  '~', '@', '\u00a3', \n          '\u00b7', '_', '{', '}', '\u00a9', '^', '\u00ae', '`',  '<', '\u2192', '\u00b0', '\u20ac', '\u2122', '\u203a',  \n          '\u2665', '\u2190', '\u00d7', '\u00a7', '\u2033', '\u2032', '\u00c2', '\u2588', '\u00bd', '\u00e0', '\u2026', '\u201c', '\u2605', '\u201d', \n          '\u2013', '\u25cf', '\u00e2', '\u25ba', '\u2212', '\u00a2', '\u00b2', '\u00ac', '\u2591', '\u00b6', '\u2191', '\u00b1', '\u00bf', '\u25be', \n          '\u2550', '\u00a6', '\u2551', '\u2015', '\u00a5', '\u2593', '\u2014', '\u2039', '\u2500', '\u2592', '\uff1a', '\u00bc', '\u2295', '\u25bc', \n          '\u25aa', '\u2020', '\u25a0', '\u2019', '\u2580', '\u00a8', '\u2584', '\u266b', '\u2606', '\u00e9', '\u00af', '\u2666', '\u00a4', '\u25b2', \n          '\u00e8', '\u00b8', '\u00be', '\u00c3', '\u22c5', '\u2018', '\u221e', '\u2219', '\uff09', '\u2193', '\u3001', '\u2502', '\uff08', '\u00bb', \n          '\uff0c', '\u266a', '\u2569', '\u255a', '\u00b3', '\u30fb', '\u2566', '\u2563', '\u2554', '\u2557', '\u25ac', '\u2764', '\u00ef', '\u00d8', \n          '\u00b9', '\u2264', '\u2021', '\u221a', '\u03b2', '\u03b1', '\u2205', '\u03b8', '\u00f7', '\u20b9']\n\ndef clean_punct(x):\n    x = str(x)\n    for punct in puncts:\n        if punct in x:\n            x = x.replace(punct, f' {punct} ')\n    return x\n\ntrain[\"question_text\"] = train[\"question_text\"].apply(lambda x: clean_punct(x))\ntest[\"question_text\"] = test[\"question_text\"].apply(lambda x: clean_punct(x))","2d7847de":"test_shape = test.shape\nprint(test_shape)","6f6cd7d5":"sincere = train[train[\"target\"] == 0]\ninsincere = train[train[\"target\"] == 1]\n\nprint(\"Sincere questions {}; Insincere questions {}\".format(sincere.shape[0], insincere.shape[0]))","1bf601a1":"temp1 = sincere.sample(53552)\ntemp2 = insincere.sample(2818)\nfake_test = pd.concat([temp1, temp2], sort = True).reset_index()\ntrain = train.drop(fake_test.index).reset_index()\ntarget = train[\"target\"].values\n\nprint(\"Fake test data shape {}\".format(fake_test.shape))\nprint(\"New train data shape {}\".format(train.shape))","c74f8eb6":"train.head()","66a43e41":"fake_test.head()","e02c2813":"def get_glove(embedding_file):\n    def get_coefs(word,*arr): return word, np.asarray(arr, dtype='float32')\n    embeddings_index = dict(get_coefs(*o.split(\" \")) for o in open(embedding_file))\n    \n    all_embs = np.stack(embeddings_index.values())\n    emb_mean, emb_std = all_embs.mean(), all_embs.std()\n    return embeddings_index, emb_mean, emb_std\n\ndef get_para(embedding_file):\n    def get_coefs(word,*arr): return word, np.asarray(arr, dtype='float32')\n    embeddings_index = dict(get_coefs(*o.split(\" \")) for o in open(embedding_file, \n                                                                   encoding=\"utf8\", \n                                                                   errors='ignore') if len(o)>100)\n    all_embs = np.stack(embeddings_index.values())\n    emb_mean, emb_std = all_embs.mean(), all_embs.std()\n    return embeddings_index, emb_mean, emb_std\n\nglove_index, glove_mean, glove_std = get_glove(embedding_file1)\npara_index, para_mean, para_std = get_para(embedding_file2)","5c92f55e":"def get_embed(tokenizer = None, embeddings_index = None, emb_mean = None, emb_std = None):\n    word_index = tokenizer.word_index\n    nb_words = min(max_features, len(word_index))\n    embedding_matrix = np.random.normal(emb_mean, emb_std, (nb_words, embed_size))\n    for word, i in word_index.items():\n        if i >= max_features: continue\n        embedding_vector = embeddings_index.get(word)\n        if embedding_vector is not None: embedding_matrix[i] = embedding_vector\n            \n    return nb_words, embedding_matrix","a1818061":"tokenizer = Tokenizer(num_words = max_features, lower = True)\ntokenizer.fit_on_texts(train[\"question_text\"])\n\ntrain_token = tokenizer.texts_to_sequences(train[\"question_text\"])\nfake_test_token = tokenizer.texts_to_sequences(fake_test[\"question_text\"])\ntest_token = tokenizer.texts_to_sequences(test[\"question_text\"])\n\ntrain_seq = pad_sequences(train_token, maxlen = max_len)\nfake_test_seq = pad_sequences(fake_test_token, maxlen = max_len)\nX_test = pad_sequences(test_token, maxlen = max_len)\ndel train_token, fake_test_token, test_token; gc.collect()","e9b435fc":"nb_words, embedding_matrix1 = get_embed(tokenizer = tokenizer, embeddings_index = glove_index, \n                                        emb_mean = glove_mean, \n                                        emb_std = glove_std)\nnb_words, embedding_matrix2 = get_embed(tokenizer = tokenizer, embeddings_index = para_index, \n                                        emb_mean = para_mean, \n                                        emb_std = para_std)\nembedding_matrix = np.mean([embedding_matrix1, embedding_matrix2], axis = 0)\ndel embedding_matrix1, embedding_matrix2; gc.collect()\nprint(\"Embedding matrix completed!\")","bce59b91":"from keras.engine import Layer, InputSpec\nfrom keras.layers import K\n\nclass Attention(Layer):\n    def __init__(self, step_dim,\n                 W_regularizer=None, b_regularizer=None,\n                 W_constraint=None, b_constraint=None,\n                 bias=True, **kwargs):\n        self.supports_masking = True\n        self.init = initializers.get('glorot_uniform')\n\n        self.W_regularizer = regularizers.get(W_regularizer)\n        self.b_regularizer = regularizers.get(b_regularizer)\n\n        self.W_constraint = constraints.get(W_constraint)\n        self.b_constraint = constraints.get(b_constraint)\n\n        self.bias = bias\n        self.step_dim = step_dim\n        self.features_dim = 0\n        super(Attention, self).__init__(**kwargs)\n\n    def build(self, input_shape):\n        assert len(input_shape) == 3\n\n        self.W = self.add_weight((input_shape[-1],),\n                                 initializer=self.init,\n                                 name='{}_W'.format(self.name),\n                                 regularizer=self.W_regularizer,\n                                 constraint=self.W_constraint)\n        self.features_dim = input_shape[-1]\n\n        if self.bias:\n            self.b = self.add_weight((input_shape[1],),\n                                     initializer='zero',\n                                     name='{}_b'.format(self.name),\n                                     regularizer=self.b_regularizer,\n                                     constraint=self.b_constraint)\n        else:\n            self.b = None\n\n        self.built = True\n\n    def compute_mask(self, input, input_mask=None):\n        return None\n\n    def call(self, x, mask=None):\n        features_dim = self.features_dim\n        step_dim = self.step_dim\n\n        eij = K.reshape(K.dot(K.reshape(x, (-1, features_dim)),\n                        K.reshape(self.W, (features_dim, 1))), (-1, step_dim))\n\n        if self.bias:\n            eij += self.b\n\n        eij = K.tanh(eij)\n\n        a = K.exp(eij)\n\n        if mask is not None:\n            a *= K.cast(mask, K.floatx())\n\n        a \/= K.cast(K.sum(a, axis=1, keepdims=True) + K.epsilon(), K.floatx())\n\n        a = K.expand_dims(a)\n        weighted_input = x * a\n        return K.sum(weighted_input, axis=1)\n\n    def compute_output_shape(self, input_shape):\n        return input_shape[0],  self.features_dim","68823b38":"def get_f1(true, val):\n    precision, recall, thresholds = precision_recall_curve(true, val)\n    thresholds = np.append(thresholds, 1.001) \n    F = 2 \/ (1\/precision + 1\/recall)\n    best_score = np.max(F)\n    best_threshold = thresholds[np.argmax(F)]\n    \n    return best_threshold, best_score    ","104f3981":"def build_model(units = 40, dr = 0.3):\n    inp = Input(shape = (max_len, ))\n    embed_layer = Embedding(nb_words, embed_size, input_length = max_len,\n                            weights = [embedding_matrix], trainable = False)(inp)\n    x = SpatialDropout1D(dr, seed = seed)(embed_layer)\n    x = Bidirectional(CuDNNLSTM(units, kernel_initializer = glorot_normal(seed = seed), \n                                recurrent_initializer = orthogonal(gain = 1.0, seed = seed), \n                                return_sequences = True))(x)\n    x = Bidirectional(CuDNNGRU(units, kernel_initializer = glorot_normal(seed = seed),\n                               recurrent_initializer = orthogonal(gain = 1.0, seed = seed),\n                               return_sequences = True))(x)    \n    att = Attention(max_len)(x)\n    avg_pool = GlobalAveragePooling1D()(x)\n    max_pool = GlobalMaxPooling1D()(x)\n    \n    main = concatenate([att, avg_pool, max_pool])\n    main = Dense(64, kernel_initializer = glorot_normal(seed = seed))(main)\n    main = Activation(\"relu\")(main)\n    main = Dropout(0.1, seed = seed)(main)\n    \n    out = Dense(1, activation = \"sigmoid\", \n                kernel_initializer = glorot_normal(seed = seed))(main)\n    model = Model(inputs = inp, outputs = out)\n    model.compile(loss = \"binary_crossentropy\",\n                  optimizer = Adam(), \n                  metrics = None)\n    \n    return model","a7a4836c":"fold = 5\nbatch_size = 1024\nepochs = 5\noof_pred = np.zeros((train.shape[0], 1))\npred = np.zeros((test_shape[0], 1))\nfake_pred = np.zeros((test_shape[0], 1))\nthresholds = []\n\nk_fold = StratifiedKFold(n_splits = fold, random_state = seed, shuffle = True)\n\nfor i, (train_idx, val_idx) in enumerate(k_fold.split(train_seq, target)):\n    print(\"-\"*50)\n    print(\"Trainging fold {}\/{}\".format(i+1, fold))\n    \n    X_train, y_train = train_seq[train_idx], target[train_idx]\n    X_val, y_val = train_seq[val_idx], target[val_idx]\n    \n    K.clear_session()\n    model = build_model(units = 60)\n    model.fit(X_train, y_train, batch_size = batch_size, epochs = epochs,\n              validation_data = (X_val, y_val), verbose = 2)\n    val_pred = model.predict(X_val, batch_size = batch_size)\n    oof_pred[val_idx] = val_pred\n    fake_pred += model.predict(fake_test_seq, batch_size = batch_size)\/fold\n    pred += model.predict(X_test, batch_size = batch_size)\/fold\n    \n    threshold, score = get_f1(y_val, val_pred)\n    print(\"F1 score at threshold {} is {}\".format(threshold, score))","2b320671":"threshold, score = get_f1(target, oof_pred)\nprint(\"F1 score after K fold at threshold {} is {}\".format(threshold, score))\nfake_test[\"pred\"] = (fake_pred > threshold).astype(int)\nprint(\"Fake test F1 score is {}\".format(f1_score(fake_test[\"target\"], \n                                                 (fake_test[\"pred\"]).astype(int))))\ntest[\"prediction\"] = (pred > threshold).astype(int)","d7c4af9c":"submission = test[[\"qid\", \"prediction\"]]\nsubmission.to_csv(\"submission.csv\", index = False)\nsubmission.head()","a67c2ce8":"In this competition, no correlations between cross validation score and leaderboard score has bothered many participators a long time. The goal of this notebook is to mimic the leaderboard. ","1d4edb7e":"* Fist submission: \n    * threshold = 0.4211253821849823, cv = 0.6824454496590792, fake test = 0.6862931307375753, lb = 0.688\n* Second submission: \n    * threshold = 0.4139455258846283, cv = 0.682632138928462, fake test = 0.686315118687271, lb = 0.689\n* Third submission:\n    * threshold = 0.3900025486946106, cv = 0.682267364499246, fake test = 0.684897451833437, lb = 0.687\n    \nWe see that the f1 scores of fake testes and lb are somewhat correlated. So, we assume that is related to the testing data size (~56K) and class imbalance, which bring high variances with different thresholds. \n\nWe are not sure if this assumption is true because we only look at three submissions.  ","142df004":"### Create fake testing data from given training data\n\nBased on this [kernel](https:\/\/www.kaggle.com\/arthurtok\/target-visualization-t-sne-and-doc2vec), we know that the distribution of given training data and testing data are similar. We assume that the given testing data has about $95\\%$ sincere questions $(53552)$ and $5\\%$ insincere questions $(2818)$."}}