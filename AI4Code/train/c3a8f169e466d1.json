{"cell_type":{"d7a80afa":"code","e57652d3":"code","2715990c":"code","1df1a47c":"code","945e7bed":"code","316eec70":"code","004bec9e":"code","9d87f99c":"code","e076ce13":"code","ff84304c":"code","52b072dc":"code","022e72e4":"code","a95b1483":"code","efef846f":"code","0bb99b16":"code","787f8992":"code","6200a023":"code","6a89c681":"code","15fff160":"code","a3ae9806":"code","811b3a16":"code","9f17cc8a":"code","3a2cd651":"code","b3e305ae":"code","e4efceb3":"code","eff2cd66":"code","f9cbf3c4":"code","2bc89e63":"code","cf0bbe10":"markdown","6f2a09a3":"markdown","a42fa630":"markdown","03e3447a":"markdown","eb2dae60":"markdown","e266a935":"markdown","c83f1fb2":"markdown","71c4a4ed":"markdown","430065c1":"markdown","a1f18065":"markdown","d5e230f6":"markdown","3b7d148b":"markdown","625fbaf4":"markdown","eb79a8a4":"markdown","a80ee606":"markdown","b4c3dd56":"markdown","3f46b077":"markdown","58531f8b":"markdown","68c7c2f1":"markdown","f01ffc15":"markdown","dd03022f":"markdown","b115dbd8":"markdown","14ccdc1d":"markdown"},"source":{"d7a80afa":"import pandas as pd\nimport numpy as np\nfrom sklearn.feature_extraction import DictVectorizer\nfrom sklearn.feature_extraction.text import HashingVectorizer\nfrom sklearn.linear_model import Perceptron\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import SGDClassifier\nfrom sklearn.linear_model import PassiveAggressiveClassifier\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.metrics import classification_report\nimport nltk\nnltk.download('punkt')\nnltk.download('averaged_perceptron_tagger')\nfrom nltk.tokenize import word_tokenize, sent_tokenize\nimport datetime\nimport re\nnltk.download('wordnet')\nfrom nltk.stem import WordNetLemmatizer\nimport gensim\nimport matplotlib\nimport matplotlib.pyplot as plt\nmatplotlib.style.use('ggplot')\nimport torch\nimport torch.autograd as autograd\nimport torch.nn as nn\nimport torch.optim as optim\nfrom sklearn.metrics import classification_report\nimport torch.nn.functional as F\nimport math \nimport scipy","e57652d3":"def read_data(file_name, test=False):\n    data = pd.read_csv(file_name)\n    sentence = []\n    label = []\n    \n    # We only return sentences if we are reading the test file\n    if test == True:\n        for i in range(len(data)):\n            # Note that all words in the sentences are lower cased\n            x = data['sents'][i].lower().split()\n            sentence.append(x)\n        return sentence\n    \n    # We return both sentences and labels for training and validation files\n    for i in range(len(data)):\n        # Note that only words in sentences are lower cased, labels are not\n        x, y = data['sents'][i].lower().split(), data['labels'][i].split()\n        sentence.append(x)\n        label.append(y)\n    return sentence, label\n\ntrain_data, target_y_train = read_data(\"train.csv\")\nvalidation_data, target_y_validation = read_data(\"val.csv\")\ntest_data = read_data(\"test_without_labels.csv\", test=True)","2715990c":"print(len(train_data))\nprint(train_data[1])\nprint(target_y_train[1])","1df1a47c":"# Replace punctuations with NA\ndef remove_punctuation_re(x):\n    x = re.sub(r'[^\\w\\s]','NA',x)\n    return x\n\ndef remove_punctuation(data):\n    for i in range(len(data)):\n        sent = data[i]\n        for j in range(len(sent)):\n            data[i][j] = remove_punctuation_re(data[i][j])\n\n            \nremove_punctuation(train_data)\nremove_punctuation(validation_data)\nremove_punctuation(test_data)","945e7bed":"train_data[1][0:10]","316eec70":"# Download for local\n# !pip install https:\/\/github.com\/explosion\/spacy-models\/releases\/download\/en_core_web_sm-2.2.0\/en_core_web_sm-2.2.0.tar.gz","004bec9e":"import spacy\n# Load the downloaded model\nnlp = spacy.load('en_core_web_sm')\ndef lemmatisation(data):\n    for i in range(len(data)):\n        sent = data[i]\n        for j in range(len(sent)):\n            data[i][j] = nlp(data[i][j])[0].lemma_\n            \nlemmatisation(train_data)\nlemmatisation(validation_data)\nlemmatisation(test_data)    ","9d87f99c":"train_data[1][0:10]","e076ce13":"def pos_tag(data):\n    pos_tag = []\n    for i in range(len(data)):\n        sent_tag = []\n        sent = data[i]\n        # Tagger needs to be downloaded for local\n        tag_sent = nltk.pos_tag(sent)\n        for j in range(len(tag_sent)):\n            # The second element in the tuple is tag\n            tag = tag_sent[j][1]\n            sent_tag.append(tag)\n        pos_tag.append(sent_tag)\n    return pos_tag\n\ntrain_pos_tag = pos_tag(train_data)\nval_pos_tag = pos_tag(validation_data)\ntest_pos_tag = pos_tag(test_data)","ff84304c":"train_pos_tag[1][0:10]","52b072dc":"all_tags = train_pos_tag+val_pos_tag+test_pos_tag\n# Embedding length is 50. Every tag will be trained\npt_model = gensim.models.Word2Vec(all_tags,vector_size=50,min_count=1)","022e72e4":"word_tag_dict = {}\nword_to_ix = {}\n\ndef word_tag(data, pos_tag_data):\n    for i in range(len(data)):\n        # Get sentence\n        sent = data[i]\n        for j in range(len(sent)):\n            # Get word\n            word = sent[j]\n            if word not in word_to_ix:\n                # Word to index\n                word_to_ix[word] = len(word_to_ix)\n                # Word and tag dictionary\n                word_tag_dict[word] = pos_tag_data[i][j]\n                \nword_tag(train_data, train_pos_tag)    \nword_tag(validation_data, val_pos_tag)  \nword_tag(test_data, test_pos_tag)  \n\nword_list = list(word_to_ix.keys())","a95b1483":"# Generate target to index with start and stop tags\nSTART_TAG = \"<START>\"\nSTOP_TAG = \"<STOP>\"\ntag_to_ix = {START_TAG:0, STOP_TAG:1}\nfor tags in target_y_train+target_y_validation:\n    for tag in tags:\n        if tag not in tag_to_ix:\n            tag_to_ix[tag] = len(tag_to_ix)","efef846f":"import gensim.downloader as api\nword_emb_model = api.load(\"glove-twitter-100\") \n\n# Only w2v\nEMBEDDING_DIM_W2V = 100\n# w2v and postags\nEMBEDDING_DIM = 150\n\n# Only w2v\nembedding_matrix_w2v = []\n# w2v and postags\nembedding_matrix = []\n\n# Only w2v\nfor word in word_list:\n    try:\n        embedding_matrix_w2v.append(word_emb_model[word])\n    except:\n        embedding_matrix_w2v.append([0]*EMBEDDING_DIM_W2V)\n        \nembedding_matrix_w2v = np.array(embedding_matrix_w2v)\n\n# w2v and postags\nfor word in word_list:\n    try:\n        temp = word_emb_model[word]\n        tag = word_tag_dict[word]\n        temp2 = pt_model.wv[tag]\n        temp3 = list(temp) + list(temp2)\n        embedding_matrix.append(temp3)\n    except:\n        embedding_matrix.append([0]*EMBEDDING_DIM)\n        \nembedding_matrix = np.array(embedding_matrix)\n\n\nprint(embedding_matrix_w2v.shape)\nprint(embedding_matrix.shape)","0bb99b16":"def to_index(data, to_ix):\n    input_index_list = []\n    for sent in data:\n        input_index_list.append([to_ix[w] for w in sent])\n    return input_index_list\n\ntrain_input_index =  to_index(train_data,word_to_ix)\ntrain_output_index = to_index(target_y_train,tag_to_ix)\nval_input_index = to_index(validation_data,word_to_ix)\nval_output_index = to_index(target_y_validation,tag_to_ix)\ntest_input_index = to_index(test_data,word_to_ix)\n# test_output_index = to_index(target_y_test,tag_to_ix)","787f8992":"torch.manual_seed(1)\n\ndef argmax(vec):\n    # return the argmax as a python int\n    _, idx = torch.max(vec, 1)\n    return idx.item()\n\n\n# Compute log sum exp in a numerically stable way for the forward algorithm\ndef log_sum_exp(vec):\n    max_score = vec[0, argmax(vec)]\n    max_score_broadcast = max_score.view(1, -1).expand(1, vec.size()[1])\n    return max_score + \\\n        torch.log(torch.sum(torch.exp(vec - max_score_broadcast)))\n\nclass BiLSTM_CRF(nn.Module):\n\n    def __init__(self, vocab_size, tag_to_ix, embedding_dim, hidden_dim, matrix, stack, attention=False, crf=True):\n        super(BiLSTM_CRF, self).__init__()\n        self.embedding_dim = embedding_dim\n        self.hidden_dim = hidden_dim\n        self.vocab_size = vocab_size\n        self.tag_to_ix = tag_to_ix\n        self.tagset_size = len(tag_to_ix)\n        # Initialize embedding matrix\n        self.matrix = matrix\n        # Initialize stack variable to the original model\n        self.stack = stack\n        # Initialize attention method\n        self.attention = attention\n        # Initialize CRF attachment\n        self.crf = crf\n\n        self.word_embeds = nn.Embedding(vocab_size, embedding_dim)\n\n        \"\"\"Here we use the embedding matrix as the initial weights of nn.Embedding\"\"\"\n        self.word_embeds.weight.data.copy_(torch.from_numpy(self.matrix))\n        \n        self.lstm = nn.LSTM(embedding_dim, hidden_dim \/\/ 2,\n                            num_layers=self.stack, bidirectional=True)  # Adjust stack layers\n\n        # Maps the output of the LSTM into tag space.\n        self.hidden2tag = nn.Linear(hidden_dim, self.tagset_size)\n\n        # Matrix of transition parameters.  Entry i,j is the score of\n        # transitioning *to* i *from* j.\n        self.transitions = nn.Parameter(\n            torch.randn(self.tagset_size, self.tagset_size))\n\n        # These two statements enforce the constraint that we never transfer\n        # to the start tag and we never transfer from the stop tag\n        self.transitions.data[tag_to_ix[START_TAG], :] = -10000\n        self.transitions.data[:, tag_to_ix[STOP_TAG]] = -10000\n\n        self.hidden = self.init_hidden()\n\n    def init_hidden(self):\n        # Initialize LSTM parameters based on number of stacked layers\n        return (torch.randn(self.stack*2, 1, self.hidden_dim \/\/ 2).to(device),\n                torch.randn(self.stack*2, 1, self.hidden_dim \/\/ 2).to(device))\n\n    def _forward_alg(self, feats):\n        # Do the forward algorithm to compute the partition function\n        init_alphas = torch.full((1, self.tagset_size), -10000.).to(device)\n        # START_TAG has all of the score.\n        init_alphas[0][self.tag_to_ix[START_TAG]] = 0.\n\n        # Wrap in a variable so that we will get automatic backprop\n        forward_var = init_alphas\n\n        # Iterate through the sentence\n        for feat in feats:\n            alphas_t = []  # The forward tensors at this timestep\n            for next_tag in range(self.tagset_size):\n                # broadcast the emission score: it is the same regardless of\n                # the previous tag\n                emit_score = feat[next_tag].view(\n                    1, -1).expand(1, self.tagset_size)\n                # the ith entry of trans_score is the score of transitioning to\n                # next_tag from i\n                trans_score = self.transitions[next_tag].view(1, -1)\n                # The ith entry of next_tag_var is the value for the\n                # edge (i -> next_tag) before we do log-sum-exp\n                next_tag_var = forward_var + trans_score + emit_score\n                # The forward variable for this tag is log-sum-exp of all the\n                # scores.\n                alphas_t.append(log_sum_exp(next_tag_var).view(1))\n            forward_var = torch.cat(alphas_t).view(1, -1)\n        terminal_var = forward_var + self.transitions[self.tag_to_ix[STOP_TAG]]\n        alpha = log_sum_exp(terminal_var)\n        return alpha\n\n    def _get_lstm_features(self, sentence):\n        self.hidden = self.init_hidden()\n        embeds = self.word_embeds(sentence).view(len(sentence), 1, -1)\n        lstm_out, self.hidden = self.lstm(embeds, self.hidden)\n        \n        # No attention, same as base model\n        if self.attention == False:\n            lstm_out = lstm_out.view(len(sentence), self.hidden_dim)            \n            lstm_feats = self.hidden2tag(lstm_out)\n            return lstm_feats\n        \n        # Define two hidden outputs\n        outputs1 = torch.empty(0).to(device)  # Encoder\n        outputs2 = torch.empty(0).to(device)  # Decoder\n        \n        for i in sentence:\n            \n            # For each word in the sentence, get its hidden state\n            wv = self.word_embeds(i)\n            _, hidden = self.lstm(wv.view(1, 1, -1), self.hidden)\n            \n            # Bi-directional model gives a tuple, so split the tuple to form two outputs\n            hidden1 = torch.unsqueeze(hidden[0], 0)\n            hidden2 = torch.unsqueeze(hidden[1], 0)\n            \n            # Concat all words hidden states in the sentence\n            outputs1 = torch.cat((outputs1, hidden1))\n            outputs2 = torch.cat((outputs2, hidden2))\n\n        try:\n            # Transform the shape\n            # The output looks like [len(sent), 2, hidden_dim\/\/2]\n            # Reshape it to [len, hidden]\n            outputs1 = outputs1.squeeze().permute(1, 0, 2)\n            outputs1 = outputs1[0]+outputs1[1]\n            \n            # Reshape decoder as well and transpose it\n            outputs2 = outputs2.squeeze().permute(1, 0, 2)\n            outputs2 = outputs2[0]+outputs2[1]\n            outputs2 = outputs2.T\n            \n            \n        except:\n            # Same as the above, this if for the sentence that contains only one word\n            outputs1 = outputs1.squeeze()\n            outputs1 = outputs1[0] + outputs1[1]\n            outputs1 = outputs1.unsqueeze(1).T\n            \n            outputs2 = outputs2.squeeze()\n            outputs2 = outputs2[0] + outputs2[1]\n            outputs2 = outputs2.unsqueeze(1)\n\n\n        # Calculate scores and the outputs using three methods\n        if self.attention == 'dot':\n            attn_weights = F.softmax(torch.matmul(outputs1, outputs2),dim=-1)\n            attn_output = torch.matmul(attn_weights, outputs1)\n            concat_output = torch.cat((attn_output, outputs2.T), dim=-1)\n            \n\n        elif self.attention == 'scale':\n            attn_weights = F.softmax(1\/np.sqrt(self.hidden_dim)*torch.matmul(outputs1, outputs2),dim=-1)\n            attn_output = torch.matmul(attn_weights, outputs1)\n            concat_output = torch.cat((attn_output, outputs2.T), dim=-1)\n            \n\n#         elif self.attention == 'general':\n#             temp = torch.matmul(outputs1, self.wa)\n#             attn_weights = F.softmax(torch.matmul(temp, outputs2),dim=-1)\n#             attn_output = torch.matmul(attn_weights, outputs1)\n#             concat_output = torch.cat((attn_output, outputs2.T), dim=-1)  \n            \n        elif self.attention == 'cos':\n            cos = nn.CosineSimilarity(dim=1, eps=1e-6)\n            attn_weights = F.softmax(cos(outputs1, outputs2.T),dim=-1)                                         \n            attn_output = torch.matmul(attn_weights, outputs1)\n            \n            # Since the attention output for cos is only [hidden_dim]\n            # Concat it with the len(sentence) so it can be concat with decoder hidden states\n            temp2 = attn_output\n            attn_output = attn_output.unsqueeze(0)\n            for i in range(len(sentence)-1):\n                attn_output = torch.cat((attn_output, temp2.unsqueeze(0)))\n                \n            concat_output = torch.cat((attn_output, outputs2.T), dim=-1)  \n        \n        # Send concat output to make prediction for tags\n        lstm_feats = self.hidden2tag(concat_output)\n        return lstm_feats\n\n    def _score_sentence(self, feats, tags):\n        # Gives the score of a provided tag sequence\n        score = torch.zeros(1).to(device)\n        tags = torch.cat([torch.tensor([self.tag_to_ix[START_TAG]], dtype=torch.long).to(device), tags])\n        for i, feat in enumerate(feats):\n            score = score + \\\n                self.transitions[tags[i + 1], tags[i]] + feat[tags[i + 1]]\n        score = score + self.transitions[self.tag_to_ix[STOP_TAG], tags[-1]]\n        return score\n\n    def _viterbi_decode(self, feats):\n        backpointers = []\n\n        # Initialize the viterbi variables in log space\n        init_vvars = torch.full((1, self.tagset_size), -10000.).to(device)\n        init_vvars[0][self.tag_to_ix[START_TAG]] = 0\n\n        # forward_var at step i holds the viterbi variables for step i-1\n        forward_var = init_vvars\n        for feat in feats:\n            bptrs_t = []  # holds the backpointers for this step\n            viterbivars_t = []  # holds the viterbi variables for this step\n\n            for next_tag in range(self.tagset_size):\n                # next_tag_var[i] holds the viterbi variable for tag i at the\n                # previous step, plus the score of transitioning\n                # from tag i to next_tag.\n                # We don't include the emission scores here because the max\n                # does not depend on them (we add them in below)\n                next_tag_var = forward_var + self.transitions[next_tag]\n                best_tag_id = argmax(next_tag_var)\n                bptrs_t.append(best_tag_id)\n                viterbivars_t.append(next_tag_var[0][best_tag_id].view(1))\n            # Now add in the emission scores, and assign forward_var to the set\n            # of viterbi variables we just computed\n            forward_var = (torch.cat(viterbivars_t) + feat).view(1, -1)\n            backpointers.append(bptrs_t)\n\n        # Transition to STOP_TAG\n        terminal_var = forward_var + self.transitions[self.tag_to_ix[STOP_TAG]]\n        best_tag_id = argmax(terminal_var)\n        path_score = terminal_var[0][best_tag_id]\n\n        # Follow the back pointers to decode the best path.\n        best_path = [best_tag_id]\n        for bptrs_t in reversed(backpointers):\n            best_tag_id = bptrs_t[best_tag_id]\n            best_path.append(best_tag_id)\n        # Pop off the start tag (we dont want to return that to the caller)\n        start = best_path.pop()\n        assert start == self.tag_to_ix[START_TAG]  # Sanity check\n        best_path.reverse()\n        return path_score, best_path\n\n    def neg_log_likelihood(self, sentence, tags):\n        feats = self._get_lstm_features(sentence)\n        forward_score = self._forward_alg(feats)\n        gold_score = self._score_sentence(feats, tags)\n        return forward_score - gold_score\n\n    def forward(self, sentence):  # dont confuse this with _forward_alg above.\n        # Get the emission scores from the BiLSTM\n        lstm_feats = self._get_lstm_features(sentence)\n        \n        # If no CRF, return the outputs without going through Viterbi\n        if self.crf == False:\n            return lstm_feats, torch.argmax(lstm_feats, dim=-1)\n\n        # Find the best path, given the features.\n        score, tag_seq = self._viterbi_decode(lstm_feats)\n        return score, tag_seq","6200a023":"def cal_acc(model, input_index, output_index, crf = True):\n    ground_truth = []\n    predicted = []\n    for i,idxs in enumerate(input_index):\n        ground_truth += output_index[i]\n        \n        # If no CRF\n        if crf == False:\n            _, outputs = model(torch.tensor(idxs, dtype=torch.long).to(device))\n            predicted += list(outputs.cpu().numpy())\n            accuracy = sum(np.array(ground_truth) == np.array(predicted))\/len(ground_truth)\n            return predicted, ground_truth, accuracy\n        \n        score, pred = model(torch.tensor(idxs, dtype=torch.long).to(device))\n        predicted += pred\n    \n    accuracy = sum(np.array(ground_truth) == np.array(predicted))\/len(ground_truth)\n    return predicted, ground_truth, accuracy","6a89c681":"%%time\n\n# Initialize Hyperparameters\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nHIDDEN_DIM = 256\nstack = 1\nlr = 0.001\nweight_decay = 1e-4\n\n# Construct two models\nbase_model_w2v = BiLSTM_CRF(vocab_size = len(word_to_ix), tag_to_ix = tag_to_ix, embedding_dim = EMBEDDING_DIM_W2V, \n                   hidden_dim = HIDDEN_DIM, matrix = embedding_matrix_w2v, stack = stack).to(device)\nbase_model_combine = BiLSTM_CRF(vocab_size = len(word_to_ix), tag_to_ix = tag_to_ix, embedding_dim = EMBEDDING_DIM, \n                   hidden_dim = HIDDEN_DIM, matrix = embedding_matrix, stack = stack).to(device)\n\nindex = 0\nname = ['base_model_w2v', 'base_model_combine']\nfor model in (base_model_w2v, base_model_combine):\n    \n    optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n    trainig_loss, trainig_accuracy, validation_loss, validation_accuracy = [], [], [], []\n\n    for epoch in range(10):  \n        time1 = datetime.datetime.now()\n        train_loss = 0\n\n        model.train()\n        for i, idxs in enumerate(train_input_index):\n            tags_index = train_output_index[i]\n\n            # Clear gradients out before each instance\n            model.zero_grad()\n\n            # Step 2. Get inputs ready for the network\n            sentence_in = torch.tensor(idxs, dtype=torch.long).to(device)\n            targets = torch.tensor(tags_index, dtype=torch.long).to(device)\n\n            # Step 3. Run forward pass.\n            loss = model.neg_log_likelihood(sentence_in, targets)\n\n            # Step 4. Compute the loss, gradients, and update the parameters by\n            # calling optimizer.step()\n            loss.backward()\n            optimizer.step()\n\n            train_loss+=loss.item()\n\n        model.eval()\n        # Call the cal_acc functions\n        _, _, train_acc = cal_acc(model,train_input_index,train_output_index)\n        _, _, val_acc = cal_acc(model,val_input_index,val_output_index)\n\n        val_loss = 0\n        for i, idxs in enumerate(val_input_index):\n            tags_index = val_output_index[i]\n            sentence_in = torch.tensor(idxs, dtype=torch.long).to(device)\n            targets = torch.tensor(tags_index, dtype=torch.long).to(device)\n            loss = model.neg_log_likelihood(sentence_in, targets)\n            val_loss+=loss.item()\n        time2 = datetime.datetime.now()\n\n        print(\"Epoch:%d, Training loss: %.2f, train acc: %.4f, val loss: %.2f, val acc: %.4f, time: %.2fs\" %(epoch+1, train_loss,train_acc, val_loss, val_acc, (time2-time1).total_seconds()))\n        trainig_loss.append(train_loss)\n        trainig_accuracy.append(train_acc)\n        validation_loss.append(val_loss)\n        validation_accuracy.append(val_acc)\n    \n    # Plot the train and validation loss graph after finishing training for each model\n    plt.figure(figsize=(8, 6))\n    plt.plot(trainig_loss, color='orange', label='train loss')\n    plt.plot(validation_loss, color='red', label='validataion loss')\n    plt.xlabel('Epochs')\n    plt.ylabel('Loss')\n    plt.legend()\n    plt.savefig(name[index]+'.png')\n    plt.show()\n    \n    # Save the each model state dictionary\n    torch.save(model.state_dict(), name[index]+'_state.pt')\n    index += 1","15fff160":"# Initialize Hyperparameters\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nHIDDEN_DIM = 256\nstack = 1\nlr = 0.001\nweight_decay = 1e-4\n\n# Construct models\nmodel_without_crf = BiLSTM_CRF(vocab_size = len(word_to_ix), tag_to_ix = tag_to_ix, embedding_dim = EMBEDDING_DIM_W2V, \n                   hidden_dim = HIDDEN_DIM, matrix = embedding_matrix_w2v, stack = stack, crf = False).to(device)\n\noptimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n# \ncriterion = nn.CrossEntropyLoss()\ntrainig_loss, trainig_accuracy, validation_loss, validation_accuracy = [], [], [], []\n\nmodel = model_without_crf\nfor epoch in range(10):  \n    time1 = datetime.datetime.now()\n    train_loss = 0\n\n    model.train()\n    for i, idxs in enumerate(train_input_index):\n        tags_index = train_output_index[i]\n\n        # Clear gradients out before each instance\n        model.zero_grad()\n\n        # Step 2. Get inputs ready for the network\n        sentence_in = torch.tensor(idxs, dtype=torch.long).to(device)\n        targets = torch.tensor(tags_index, dtype=torch.long).to(device)\n\n        # Step 3. Run forward pass.\n        outputs, _ = model(sentence_in)\n        loss = criterion(outputs, targets)\n\n        # Step 4. Compute the loss, gradients, and update the parameters by\n        # calling optimizer.step()\n        loss.backward()\n        optimizer.step()\n\n        train_loss+=loss.item()\n\n    model.eval()\n    # Call the cal_acc functions\n    _, _, train_acc = cal_acc(model,train_input_index,train_output_index,crf=False)\n    _, _, val_acc = cal_acc(model,val_input_index,val_output_index,crf=False)\n\n    val_loss = 0\n    for i, idxs in enumerate(val_input_index):\n        tags_index = val_output_index[i]\n        sentence_in = torch.tensor(idxs, dtype=torch.long).to(device)\n        targets = torch.tensor(tags_index, dtype=torch.long).to(device)\n        outputs, _ = model(sentence_in)\n        loss = criterion(outputs, targets)\n        val_loss+=loss.item()\n    time2 = datetime.datetime.now()\n\n    print(\"Epoch:%d, Training loss: %.2f, train acc: %.4f, val loss: %.2f, val acc: %.4f, time: %.2fs\" %(epoch+1, train_loss,train_acc, val_loss, val_acc, (time2-time1).total_seconds()))\n    trainig_loss.append(train_loss)\n    trainig_accuracy.append(train_acc)\n    validation_loss.append(val_loss)\n    validation_accuracy.append(val_acc)\n\n# Plot the train and validation loss graph \nplt.figure(figsize=(8, 6))\nplt.plot(trainig_loss, color='orange', label='train loss')\nplt.plot(validation_loss, color='red', label='validataion loss')\nplt.xlabel('Epochs')\nplt.ylabel('Loss')\nplt.legend()\nplt.savefig('Without_CRF.png')\nplt.show()\n\n# Save the model state dictionary\ntorch.save(model.state_dict(), 'Without_CRF_state.pt')","a3ae9806":"%%time\n\n# Initialize Hyperparameters\ndevice = torch.device(\"cpu\")\nHIDDEN_DIM = 256\nstack = 1\nlr = 0.001\nweight_decay = 1e-4\n\n# Construct models\nattention_model_dot = BiLSTM_CRF(vocab_size = len(word_to_ix), tag_to_ix = tag_to_ix, embedding_dim = EMBEDDING_DIM_W2V, \n                   hidden_dim = HIDDEN_DIM, matrix = embedding_matrix_w2v, stack = stack, attention = 'dot').to(device)\nattention_model_scale = BiLSTM_CRF(vocab_size = len(word_to_ix), tag_to_ix = tag_to_ix, embedding_dim = EMBEDDING_DIM_W2V, \n                   hidden_dim = HIDDEN_DIM, matrix = embedding_matrix_w2v, stack = stack, attention = 'scale').to(device)\nattention_model_cos = BiLSTM_CRF(vocab_size = len(word_to_ix), tag_to_ix = tag_to_ix, embedding_dim = EMBEDDING_DIM_W2V, \n                   hidden_dim = HIDDEN_DIM, matrix = embedding_matrix_w2v, stack = stack, attention = 'cos').to(device)\n\nindex = 0\nname = [ 'attention_model_cos', 'attention_model_scale', 'attention_model_dot']\nfor model in (attention_model_cos, attention_model_scale, attention_model_dot):\n    optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n    trainig_loss, trainig_accuracy, validation_loss, validation_accuracy = [], [], [], []\n\n    for epoch in range(10):  # Using more epochs here\n        time1 = datetime.datetime.now()\n        train_loss = 0\n\n        model.train()\n        for i, idxs in enumerate(train_input_index):\n            tags_index = train_output_index[i]\n\n            # Clear gradients out before each instance\n            model.zero_grad()\n\n            # Step 2. Get inputs ready for the network\n            sentence_in = torch.tensor(idxs, dtype=torch.long).to(device)\n            targets = torch.tensor(tags_index, dtype=torch.long).to(device)\n\n            # Step 3. Run forward pass.\n            loss = model.neg_log_likelihood(sentence_in, targets)\n\n            # Step 4. Compute the loss, gradients, and update the parameters by\n            # calling optimizer.step()\n            loss.backward()\n            optimizer.step()\n\n            train_loss+=loss.item()\n\n        model.eval()\n        # Call the cal_acc functions\n        _, _, train_acc = cal_acc(model,train_input_index,train_output_index)\n        _, _, val_acc = cal_acc(model,val_input_index,val_output_index)\n\n        val_loss = 0\n        for i, idxs in enumerate(val_input_index):\n            tags_index = val_output_index[i]\n            sentence_in = torch.tensor(idxs, dtype=torch.long).to(device)\n            targets = torch.tensor(tags_index, dtype=torch.long).to(device)\n            loss = model.neg_log_likelihood(sentence_in, targets)\n            val_loss+=loss.item()\n        time2 = datetime.datetime.now()\n\n        print(\"Epoch:%d, Training loss: %.2f, train acc: %.4f, val loss: %.2f, val acc: %.4f, time: %.2fs\" %(epoch+1, train_loss,train_acc, val_loss, val_acc, (time2-time1).total_seconds()))\n        trainig_loss.append(train_loss)\n        trainig_accuracy.append(train_acc)\n        validation_loss.append(val_loss)\n        validation_accuracy.append(val_acc)\n        \n    # Plot the train and validation loss graph after finishing training for each model    \n    plt.figure(figsize=(8, 6))\n    plt.plot(trainig_loss, color='orange', label='train loss')\n    plt.plot(validation_loss, color='red', label='validataion loss')\n    plt.xlabel('Epochs')\n    plt.ylabel('Loss')\n    plt.legend()\n    plt.savefig(name[index]+'.png')\n    plt.show()\n    \n    # Save each model state dictionary\n    torch.save(model.state_dict(), name[index]+'_state.pt')\n    index += 1","811b3a16":"%%time\n\n# Initialize Hyperparameters\ndevice = torch.device(\"cpu\") # GPU refused to work\nHIDDEN_DIM = 256\nstacks = [2,4,8]\nlr = 0.001\nweight_decay = 1e-4\n\n# Construct models\nstack2 = BiLSTM_CRF(vocab_size = len(word_to_ix), tag_to_ix = tag_to_ix, embedding_dim = EMBEDDING_DIM_W2V, \n               hidden_dim = HIDDEN_DIM, matrix = embedding_matrix_w2v, stack = stacks[0]).to(device)\nstack4 = BiLSTM_CRF(vocab_size = len(word_to_ix), tag_to_ix = tag_to_ix, embedding_dim = EMBEDDING_DIM_W2V, \n               hidden_dim = HIDDEN_DIM, matrix = embedding_matrix_w2v, stack = stacks[1]).to(device)\nstack8 = BiLSTM_CRF(vocab_size = len(word_to_ix), tag_to_ix = tag_to_ix, embedding_dim = EMBEDDING_DIM_W2V, \n               hidden_dim = HIDDEN_DIM, matrix = embedding_matrix_w2v, stack = stacks[2]).to(device)\n\n\nindex = 0\nname = [ 'stack2', 'stack4', 'stack8']\nfor model in (stack2, stack4, stack8):\n\n    optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n    trainig_loss, trainig_accuracy, validation_loss, validation_accuracy = [], [], [], []\n\n    for epoch in range(7):  \n        time1 = datetime.datetime.now()\n        train_loss = 0\n\n        model.train()\n        for i, idxs in enumerate(train_input_index):\n            tags_index = train_output_index[i]\n\n            # Clear gradients out before each instance\n            model.zero_grad()\n\n            # Step 2. Get inputs ready for the network\n            sentence_in = torch.tensor(idxs, dtype=torch.long).to(device)\n            targets = torch.tensor(tags_index, dtype=torch.long).to(device)\n\n            # Step 3. Run forward pass.\n            loss = model.neg_log_likelihood(sentence_in, targets)\n\n            # Step 4. Compute the loss, gradients, and update the parameters by\n            # calling optimizer.step()\n            loss.backward()\n            optimizer.step()\n\n            train_loss+=loss.item()\n\n        model.eval()\n        # Call the cal_acc functions\n        _, _, train_acc = cal_acc(model,train_input_index,train_output_index)\n        _, _, val_acc = cal_acc(model,val_input_index,val_output_index)\n\n        val_loss = 0\n        for i, idxs in enumerate(val_input_index):\n            tags_index = val_output_index[i]\n            sentence_in = torch.tensor(idxs, dtype=torch.long).to(device)\n            targets = torch.tensor(tags_index, dtype=torch.long).to(device)\n            loss = model.neg_log_likelihood(sentence_in, targets)\n            val_loss+=loss.item()\n        time2 = datetime.datetime.now()\n\n        print(\"Epoch:%d, Training loss: %.2f, train acc: %.4f, val loss: %.2f, val acc: %.4f, time: %.2fs\" %(epoch+1, train_loss,train_acc, val_loss, val_acc, (time2-time1).total_seconds()))\n        trainig_loss.append(train_loss)\n        trainig_accuracy.append(train_acc)\n        validation_loss.append(val_loss)\n        validation_accuracy.append(val_acc)\n    \n    # Plot the train and validation loss graph after finishing training for each model   \n    plt.figure(figsize=(8, 6))\n    plt.plot(trainig_loss, color='orange', label='train loss')\n    plt.plot(validation_loss, color='red', label='validataion loss')\n    plt.xlabel('Epochs')\n    plt.ylabel('Loss')\n    plt.legend()\n    plt.savefig(name[index]+'.png')\n    plt.show()\n    \n    # Save each model state dictionary\n    torch.save(model.state_dict(), name[index]+'_state.pt')\n    index += 1","9f17cc8a":"def decode_output(output_list):\n    ix_to_tag = {v:k for k,v in tag_to_ix.items()}\n    return [ix_to_tag[output] for output in output_list]","3a2cd651":"device = torch.device(\"cpu\")\n\n# Load models with trained state\nbase_w2v = BiLSTM_CRF(vocab_size = len(word_to_ix), tag_to_ix = tag_to_ix, embedding_dim = EMBEDDING_DIM_W2V, \n                   hidden_dim = HIDDEN_DIM, matrix = embedding_matrix_w2v, stack=1)\nbase_w2v.load_state_dict(torch.load('base_model_w2v_state.pt'))\n\nbase_combine = BiLSTM_CRF(vocab_size = len(word_to_ix), tag_to_ix = tag_to_ix, embedding_dim = EMBEDDING_DIM, \n                   hidden_dim = HIDDEN_DIM, matrix = embedding_matrix, stack=1)\nbase_combine.load_state_dict(torch.load('base_model_combine_state.pt'))\n\n# Evaluation and produce classification reports\nbase_w2v.eval()\ny_pred, y_true, _ = cal_acc(base_w2v,val_input_index,val_output_index)\ny_true_decode = decode_output(y_true)\ny_pred_decode = decode_output(y_pred)\nprint('='*50 + 'Base Model Using Word2Vec' + '='*50)\nprint(classification_report(y_true_decode,y_pred_decode,digits=4))\n\nbase_combine.eval()\ny_pred, y_true, _ = cal_acc(base_combine,val_input_index,val_output_index)\ny_true_decode = decode_output(y_true)\ny_pred_decode = decode_output(y_pred)\nprint('='*50 + 'Base Model Using Word2Vec and PosTags' + '='*50)\nprint(classification_report(y_true_decode,y_pred_decode,digits=4))","b3e305ae":"device = torch.device(\"cpu\")\n\n# Load models with trained state\nmodel_without_crf = BiLSTM_CRF(vocab_size = len(word_to_ix), tag_to_ix = tag_to_ix, embedding_dim = EMBEDDING_DIM_W2V, \n                   hidden_dim = HIDDEN_DIM, matrix = embedding_matrix_w2v, stack = 1, crf = False)\nmodel_without_crf.load_state_dict(torch.load('Without_CRF_state.pt'))\n\n# Evaluation and produce classification reports\nmodel_without_crf.eval()\n\ny_pred, y_true, _ = cal_acc(model_without_crf,val_input_index,val_output_index, crf=False)\ny_true_decode = decode_output(y_true)\ny_pred_decode = decode_output(y_pred)\nprint('='*50 + 'Base Model Without CRF' + '='*50)\nprint(classification_report(y_true_decode,y_pred_decode,digits=4))","e4efceb3":"device = torch.device(\"cpu\")\n\n# Load models with trained state\ncontent = BiLSTM_CRF(vocab_size = len(word_to_ix), tag_to_ix = tag_to_ix, embedding_dim = EMBEDDING_DIM_W2V, \n                   hidden_dim = HIDDEN_DIM, matrix = embedding_matrix_w2v, stack = 1, attention = 'cos')\ncontent.load_state_dict(torch.load('attention_model_cos_state.pt'))\n\nscale = BiLSTM_CRF(vocab_size = len(word_to_ix), tag_to_ix = tag_to_ix, embedding_dim = EMBEDDING_DIM_W2V, \n                   hidden_dim = HIDDEN_DIM, matrix = embedding_matrix_w2v, stack = 1, attention = 'scale')\nscale.load_state_dict(torch.load('attention_model_scale_state.pt'))\n\ndot = BiLSTM_CRF(vocab_size = len(word_to_ix), tag_to_ix = tag_to_ix, embedding_dim = EMBEDDING_DIM_W2V, \n                   hidden_dim = HIDDEN_DIM, matrix = embedding_matrix_w2v, stack = 1, attention = 'dot')\ndot.load_state_dict(torch.load('attention_model_dot_state.pt'))\n\n# Evaluation and produce classification reports\ncontent.eval()\ny_pred, y_true, _ = cal_acc(content,val_input_index,val_output_index)\ny_true_decode = decode_output(y_true)\ny_pred_decode = decode_output(y_pred)\nprint('='*50 + 'Base Model Using Content Attention' + '='*50)\nprint(classification_report(y_true_decode,y_pred_decode,digits=4))\n\nscale.eval()\ny_pred, y_true, _ = cal_acc(scale,val_input_index,val_output_index)\ny_true_decode = decode_output(y_true)\ny_pred_decode = decode_output(y_pred)\nprint('='*50 + 'Base Model Using Scale Dot Product Attention' + '='*50)\nprint(classification_report(y_true_decode,y_pred_decode,digits=4))\n\ndot.eval()\ny_pred, y_true, _ = cal_acc(dot,val_input_index,val_output_index)\ny_true_decode = decode_output(y_true)\ny_pred_decode = decode_output(y_pred)\nprint('='*50 + 'Base Model Using Dot Product Attention' + '='*50)\nprint(classification_report(y_true_decode,y_pred_decode,digits=4))","eff2cd66":"device = torch.device(\"cpu\")\n\n# Load models with trained state\nstack2 = BiLSTM_CRF(vocab_size = len(word_to_ix), tag_to_ix = tag_to_ix, embedding_dim = EMBEDDING_DIM_W2V, \n                   hidden_dim = HIDDEN_DIM, matrix = embedding_matrix_w2v, stack = 2)\nstack2.load_state_dict(torch.load('stack2_state.pt'))\n\nstack4 = BiLSTM_CRF(vocab_size = len(word_to_ix), tag_to_ix = tag_to_ix, embedding_dim = EMBEDDING_DIM_W2V, \n                   hidden_dim = HIDDEN_DIM, matrix = embedding_matrix_w2v, stack = 4)\nstack4.load_state_dict(torch.load('stack4_state.pt'))\n\nstack8 = BiLSTM_CRF(vocab_size = len(word_to_ix), tag_to_ix = tag_to_ix, embedding_dim = EMBEDDING_DIM_W2V, \n                   hidden_dim = HIDDEN_DIM, matrix = embedding_matrix_w2v, stack = 8)\nstack8.load_state_dict(torch.load('stack8_state.pt'))\n\n# Evaluation and produce classification reports\nstack2.eval()\ny_pred, y_true, _ = cal_acc(stack2,val_input_index,val_output_index)\ny_true_decode = decode_output(y_true)\ny_pred_decode = decode_output(y_pred)\nprint('='*50 + 'Best Model Using Stacked 2 Layers' + '='*50)\nprint(classification_report(y_true_decode,y_pred_decode,digits=4))\n\nstack4.eval()\ny_pred, y_true, _ = cal_acc(stack4,val_input_index,val_output_index)\ny_true_decode = decode_output(y_true)\ny_pred_decode = decode_output(y_pred)\nprint('='*50 + 'Best Model Using Stacked 4 Layers' + '='*50)\nprint(classification_report(y_true_decode,y_pred_decode,digits=4))\n\nstack8.eval()\ny_pred, y_true, _ = cal_acc(stack8,val_input_index,val_output_index)\ny_true_decode = decode_output(y_true)\ny_pred_decode = decode_output(y_pred)\nprint('='*50 + 'Best Model Using Stacked 8 Layers' + '='*50)\nprint(classification_report(y_true_decode,y_pred_decode,digits=4))\n\n","f9cbf3c4":"# Reverse the dictionary\nreversed_tag_idx = {value : key for (key, value) in tag_to_ix.items()}\n\n# Make prediction and create IDs\nprediction = []\n\nmodel = base_w2v\nfor i in test_input_index:\n    _, output = model(torch.tensor(i, dtype=torch.long).to(device))\n    \n    for j in output:\n        prediction.append(reversed_tag_idx[j])\n\n\noutput_id = range(len(prediction))","2bc89e63":"prediction_dict = {'ID': output_id, 'Predicted': prediction}\nprediction_df = pd.DataFrame(prediction_dict)\nprediction_df.to_csv('Submission.csv', index=False)","cf0bbe10":"#### Train PosTags","6f2a09a3":"#### Lemmatisation","a42fa630":"#### Evaluation Function","03e3447a":"#### PosTagging","eb2dae60":"#### Test for Input Embedding","e266a935":"#### Test for Stacked Model","c83f1fb2":"#### Train Best Model for Stack","71c4a4ed":"### Preprocess","430065c1":"#### Train Base Model with Attention","a1f18065":"#### Test for CRF","d5e230f6":"#### convert dataset into idxs","3b7d148b":"## Training","625fbaf4":"### Prediction","eb79a8a4":"### Import Data","a80ee606":"#### Generate Embedding Matrix","b4c3dd56":"Using just Word2Vec is slightly better.","3f46b077":"#### Generate word_to_ix, tag_to_ix, and word_tag_dict","58531f8b":"#### Train the model for Input Embedding","68c7c2f1":"#### Train Base Model without CRF","f01ffc15":"### Model","dd03022f":"#### Test for Attention","b115dbd8":"### Testing","14ccdc1d":"#### Remove punctuations"}}