{"cell_type":{"07b5b282":"code","3065d962":"code","f580d440":"code","0275322a":"code","b16312f8":"code","2a5b53f6":"code","bdb3db43":"code","f66a0ef6":"code","3167c4bb":"code","4d2ef647":"code","cbc9cd53":"code","55bf2fcf":"code","b6678076":"code","8f9df50f":"code","02e28ee6":"code","9d0267fb":"code","d916416c":"code","1a8ecfe1":"code","32ec6288":"code","b780cb24":"code","f762296b":"code","8af57fb4":"code","265da23b":"code","0ba76033":"code","8dd8246c":"code","6254e5d9":"code","900d7000":"code","aa0c4204":"code","6f7d5c7a":"markdown","b906d4f0":"markdown","4ff89ddb":"markdown","ee8cf0dd":"markdown","68e1b6c9":"markdown","07899966":"markdown","44fcc4b9":"markdown","0112c5f9":"markdown","2a741d58":"markdown","7866bfe6":"markdown","db2b4ab7":"markdown","8eb507fd":"markdown","fc2bb8ef":"markdown","bd1055cc":"markdown","1033f53e":"markdown","69114b1f":"markdown","18b4d48b":"markdown","b7de41d0":"markdown","abeeeb08":"markdown","db3b15dd":"markdown","8b8e0493":"markdown","4cd9558a":"markdown","2f8f22fa":"markdown","3ee71493":"markdown","467c8d34":"markdown","1abe9daa":"markdown","22105f8b":"markdown"},"source":{"07b5b282":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline","3065d962":"df= pd.read_csv('..\/input\/hotel-booking\/hotel_booking.csv')","f580d440":"df.info()","0275322a":"#Calculating the percentage of missing data in each columns (feature) and then sort it\ndef missing_percentage(df):\n    nan_percent= 100*(df.isnull().sum()\/len(df))\n    nan_percent= nan_percent[nan_percent>0].sort_values()\n    return nan_percent\nnan_percent= missing_percentage(df)\nprint(nan_percent)","b16312f8":"plt.figure(figsize=(12,6))\nsns.barplot(x=nan_percent.index, y=nan_percent, color=(0.2, 0.4, 0.6, 0.6), edgecolor='blue')\nplt.xticks(rotation=90)","2a5b53f6":"df = df.drop(['company'],axis=1)","bdb3db43":"df[\"children\"]= df[\"children\"].fillna(0)","f66a0ef6":"df['agent'][df['agent'].isnull()]","3167c4bb":"df[\"agent\"]= df[\"agent\"].fillna(0)","4d2ef647":"df_num= df.select_dtypes(exclude='object')","cbc9cd53":"df_num.info()","55bf2fcf":"fig= plt.figure(figsize=(10,10), dpi=500)\nplt.rcParams['font.size'] = '8'\nsns.heatmap(df_num.corr(), annot=True, cmap=\"YlGnBu\", vmin=-1, vmax=1)","b6678076":"corr_matrix = df_num.corr()\nprint(corr_matrix[\"is_canceled\"].sort_values(ascending=False))","8f9df50f":"df_num = df_num.drop(['arrival_date_day_of_month', 'stays_in_weekend_nights', 'children', 'arrival_date_week_number'],axis=1)","02e28ee6":"fig= plt.figure(figsize=(6,6), dpi=300)\n\nsns.countplot(data=df_num, x='is_canceled')","9d0267fb":"df_num['is_canceled'].value_counts()","d916416c":"y = df_num['is_canceled']\nX = df_num.drop('is_canceled', axis = 1)","1a8ecfe1":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X,y,random_state=101,test_size=0.3)","32ec6288":"# pip install imbalanced-learn\nfrom imblearn.over_sampling import SMOTE\nsmote = SMOTE(random_state=101, sampling_strategy='minority')\nX_sm, y_sm = smote.fit_resample(X, y)\ny_sm.value_counts()","b780cb24":"X_train_sm, X_test_sm, y_train_sm, y_test_sm = train_test_split(X_sm, y_sm)","f762296b":"from sklearn.tree import DecisionTreeClassifier\nModel_DecisionTree = DecisionTreeClassifier()\nModel_DecisionTree.fit(X_train, y_train)\ny_pred = Model_DecisionTree.predict(X_test)\n\nfrom sklearn.metrics import confusion_matrix,classification_report\nprint(classification_report(y_test,y_pred))","8af57fb4":"from sklearn.tree import DecisionTreeClassifier\nModel_DecisionTree = DecisionTreeClassifier()\nModel_DecisionTree.fit(X_train_sm, y_train_sm)\nSMOTE_y_preds = Model_DecisionTree.predict(X_test_sm)\n\nfrom sklearn.metrics import confusion_matrix,classification_report\nprint(classification_report(y_test_sm,SMOTE_y_preds))","265da23b":"from sklearn.tree import plot_tree\npruned_tree = DecisionTreeClassifier(max_leaf_nodes=7)\npruned_tree.fit(X_train_sm,y_train_sm)\nplt.figure(figsize=(6,10),dpi=200)\nplot_tree(pruned_tree,filled=True,feature_names=X.columns);","0ba76033":"# Feature Scaling\nfrom sklearn.preprocessing import StandardScaler\nscaler= StandardScaler()\nscaler.fit(X_train)\nscaled_X_train= scaler.transform(X_train)\nscaled_X_test= scaler.transform(X_test)\n\n# Training the model\nfrom sklearn.neighbors import KNeighborsClassifier\nknn_model= KNeighborsClassifier(n_neighbors=1)\nknn_model.fit(scaled_X_train, y_train)\ny_pred= knn_model.predict(scaled_X_test)\n\nfrom sklearn.metrics import confusion_matrix,classification_report\nprint(classification_report(y_test,y_pred))","8dd8246c":"# Feature Scaling\nfrom sklearn.preprocessing import StandardScaler\nscaler= StandardScaler()\nscaler.fit(X_train_sm)\nscaled_X_train_sm= scaler.transform(X_train_sm)\nscaled_X_test_sm= scaler.transform(X_test_sm)\n\n# Training the model\nfrom sklearn.neighbors import KNeighborsClassifier\nknn_model= KNeighborsClassifier(n_neighbors=1)\nknn_model.fit(scaled_X_train_sm, y_train_sm)\ny_pred_sm= knn_model.predict(scaled_X_test_sm)\n\nfrom sklearn.metrics import confusion_matrix,classification_report\nprint(classification_report(y_test_sm,y_pred_sm))","6254e5d9":"# Feature Scaling\nfrom sklearn.preprocessing import StandardScaler\nscaler= StandardScaler()\nscaler.fit(X_train)\nscaled_X_train= scaler.transform(X_train)\nscaled_X_test= scaler.transform(X_test)\n\n# Training the model\nfrom sklearn.linear_model import LogisticRegression\nlog_model= LogisticRegression()\nlog_model.fit(scaled_X_train, y_train)\ny_pred= log_model.predict(scaled_X_test)\n\nfrom sklearn.metrics import confusion_matrix,classification_report\nprint(classification_report(y_test,y_pred))","900d7000":"# creating the dataset\nfig= plt.figure(figsize=(4,2), dpi=250)\nplt.rcParams['font.size'] = '4'\ndata = {'Decision Tree Algorithm':0.82, 'Decision Tree Algorithm using SMOTE':0.84, 'K Nearest Neighbors':0.80,\n        'K Nearest Neighbors using SMOTE':0.82, 'Logistic Regression':0.74}\nAlgorithms= list(data.keys())\nAccuracy= list(data.values())\n\n \n# creating the bar plot\nplt.bar(Algorithms, Accuracy, color ='maroon',\n        width = 0.7)\n \nplt.xlabel(\"Classification machine learning algorithms\")\nplt.ylabel(\"Accuracy\")\nplt.xticks(rotation=90)\nplt.title(\"A comparison among various classification machine learning algorithms\")\n","aa0c4204":"Results = {'Accuracy':[0.82, 0.84, 0.80, 0.82, 0.74]}  \nDataframe_of_results = pd.DataFrame(Results, index =['Decision Tree Algorithm', 'Decision Tree Algorithm using SMOTE', 'K Nearest Neighbors', 'K Nearest Neighbors using SMOTE', 'Logistic Regression'])\nDataframe_of_results.sort_values('Accuracy', ascending=False)","6f7d5c7a":"#### It seems like that the missing values in agent should be equal to zero, therefore, we fill them by 0.","b906d4f0":"#### In the above heatmap,\n\n#### *  -1 indicates a perfectly negative linear correlation between two variables;\n#### *  0 indicates no linear correlation between two variables;\n#### *  1 indicates a perfectly positive linear correlation between two variables.","4ff89ddb":"# Step 5: Determining X (Features) and y (Target Variable)","ee8cf0dd":"## 1. Decision Tree Algorithm\n\n#### A classification tree is used to predict a qualitative response rather than a quantitative one <span style=\"color:crimson;\">(James et al., 2021).\n\n### References\n\n#### * James, G., Witten, D., Hastie, T., Tibshirani, R. (2021). An introduction to statistical learning, 2nd Edition. New York: springer.","68e1b6c9":"#### This data set contains booking information for a city hotel and a resort hotel, and includes information such as when the booking was made, length of stay, the number of adults, children, and\/or babies, and the number of available parking spaces, among other things.","07899966":"### Regular Logistic Regression Algorithm","44fcc4b9":"# Step 7: Coping with imbalanced data applying SMOTE\n\n#### Imbalanced data profoundly affects our results. Thus, we employ SMOTE (synthetic minority oversampling technique) in order to cope with imbalanced data. SMOTE (Synthetic Minority Over-sampling Technique) is an over-sampling approach in which the minority class is over-sampled by creating \u201csynthetic\u201d examples <span style=\"color:crimson;\"> (Chawla et al., 2002).\n#### The SMOTE algorithm is a popular approach for oversampling the minority class. This technique can be used to reduce the imbalance or to make the class distribution even. This can be achieved by simply duplicating examples in the minority class, but these examples do not add any new information. Instead, new examples from the minority can be synthesized using existing examples in the training dataset. These new examples will be \u201cclose\u201d to existing examples in the feature space, but different in small but random ways <span style=\"color:crimson;\">(Brownlee, 2021).\n\n### References\n\n#### * Chawla, N.V., Bowyer, K.W., Hall, L.O., Kegelmeyer, W.P. (2002). SMOTE: synthetic minority over-sampling technique. Journal of artificial intelligence research, 16, 321-357.\n#### * Brownlee, J. (2021). Imbalanced Classification With Python. https:\/\/machinelearningmastery.com\/imbalanced-classification-with-python-7-day-mini-course\/ (Accessed 12 August 2021)","0112c5f9":"# Step 6: Spliting Train and Test","2a741d58":"# Discussion\n    \n#### As shown, we compared five different classification machine learning algorithms, i.e., Decision Tree, Decision Tree using SMOTE, K Nearest Neighbors, K Nearest Neighbors using SMOTE, and Logistic Regression, in order to predit cancelation. Eventually, results revealed that the Decision Tree using SMOTE outperformes other classification machine learning algorithms. According to outcomes, the Decision Tree using SMOTE brings about an accuracy of 0.84 for cancelation forecasting.    ","7866bfe6":"### Coping with missing values","db2b4ab7":"## Step 2: Importing the hotel booking data set","8eb507fd":"## 3. Logistic Regression Algorithm\n\n#### Logistic Regression transforms a Linear Regression into classification model using the below equation:\n\n#### $\\sigma (x) = 1\/(1 + e^{-x})$\n\n#### Hence, the output always lays between 0 and 1.","fc2bb8ef":"### Decision Tree Algorithm using SMOTE","bd1055cc":"#### Looks like we have imbalanced data. To put it in other words, the number of individuals that have canceled is way more than those who have not canceled.","1033f53e":"## 2. K Nearest Neighbors Algorithm\n\n#### K nearset neighbors (KNN) assigns a label to new data according to the distance between the old data and the new data.\n\n####  $Pr(Y=j|X=x_0) = 1\/K \\times \\sum_{i \\in N_0} I(y_i = j)$\n\n#### *Given the fact that feature scaling is a compulsory task in the KNN algorithm, we do feature scaling before training the model.*","69114b1f":"### K Nearest Neighbors Algorithm using SMOTE","18b4d48b":"#### As demonstrated, there are no missing data.","b7de41d0":"#### It is likely that missing values in children column should be equal to zero, hence, we fill them by 0.","abeeeb08":"### Regular Decision Tree Algorithm ","db3b15dd":"## Step 4: Exploratory Data Analysis","8b8e0493":"#### As indicated, more than 90% of data in company column are missing; so, we eliminate this feature.","4cd9558a":"#### Next, we drop useless features (with less than 0.01 correlation)","2f8f22fa":"# Step 8: Classification Machine Learning Algorithms","3ee71493":"## Step 1: Importing required libraries","467c8d34":"#### Here, we drop those columns that are string in nature","1abe9daa":"### Regular K Nearest Neighbors Algorithm","22105f8b":"## Step 3: Data Preparation"}}