{"cell_type":{"237e5eb5":"code","b6d6f3fe":"code","f5136163":"code","00cf1684":"code","aca88459":"code","ebbcc959":"code","496fba31":"code","387cd440":"code","b57a069a":"code","19961136":"code","a8b01dfb":"code","190e6422":"code","b0bd09db":"code","f9d587e5":"markdown","2b4238e1":"markdown","ca6d3db4":"markdown","d1303e5c":"markdown","0eaebb9b":"markdown","ee344623":"markdown","a1e145c1":"markdown","dc24050f":"markdown","150df3dc":"markdown","ec09c039":"markdown","8d97a3df":"markdown","c6a18301":"markdown","038a0b7f":"markdown","f4979afd":"markdown","0225bace":"markdown","e2d2c285":"markdown","45606ac3":"markdown","973a9976":"markdown","bc050a86":"markdown","e9c52a8a":"markdown"},"source":{"237e5eb5":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nfrom sklearn.cluster import KMeans\nimport pandas as pd\nfrom sklearn.preprocessing import MinMaxScaler\nfrom matplotlib import pyplot as plt\n%matplotlib inline\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","b6d6f3fe":"data='\/kaggle\/input\/mall-customers\/Mall_Customers.csv'\ndf=pd.read_csv(data)\ndf.head()","f5136163":"plt.scatter(df[\"Spending Score (1-100)\"],df[\"Annual Income (k$)\"])","00cf1684":"km=KMeans(n_clusters=5)\ny_predicted=km.fit_predict(df[[\"Spending Score (1-100)\",\"Annual Income (k$)\"]])","aca88459":"df[\"cluster\"]=y_predicted\n#df2=df.drop(['Age','Genre','CustomerID'], axis=1)\ndf.head()","ebbcc959":"df1=df[df.cluster==0]\ndf2=df[df.cluster==1]\ndf3=df[df.cluster==2]\ndf4=df[df.cluster==3]\ndf5=df[df.cluster==4]\n\nplt.scatter(df1[\"Spending Score (1-100)\"],df1[\"Annual Income (k$)\"],color='green')\nplt.scatter(df2[\"Spending Score (1-100)\"],df2[\"Annual Income (k$)\"],color='blue')\nplt.scatter(df3[\"Spending Score (1-100)\"],df3[\"Annual Income (k$)\"],color='red')\nplt.scatter(df4[\"Spending Score (1-100)\"],df4[\"Annual Income (k$)\"],color='yellow')\nplt.scatter(df5[\"Spending Score (1-100)\"],df5[\"Annual Income (k$)\"],color='black')\n\nplt.xlabel('Spending Score')\nplt.ylabel('Annual Income')","496fba31":"scaler=MinMaxScaler()\n\ndf['Spending Score (1-100)']=scaler.fit_transform(df[['Spending Score (1-100)']])\ndf[\"Annual Income (k$)\"]=scaler.fit_transform(df[['Annual Income (k$)']])\n\ndf.head()","387cd440":"plt.scatter(df[\"Spending Score (1-100)\"],df[\"Annual Income (k$)\"])","b57a069a":"km=KMeans(n_clusters=5)\ny_predicted=km.fit_predict(df[['Spending Score (1-100)',\"Annual Income (k$)\"]])\ndf.cluster=y_predicted\ndf.head()","19961136":"km.cluster_centers_","a8b01dfb":"df1=df[df.cluster==0]\ndf2=df[df.cluster==1]\ndf3=df[df.cluster==2]\ndf4=df[df.cluster==3]\ndf5=df[df.cluster==4]\n\nplt.scatter(df1[\"Spending Score (1-100)\"],df1[\"Annual Income (k$)\"],color='green')\nplt.scatter(df2[\"Spending Score (1-100)\"],df2[\"Annual Income (k$)\"],color='blue')\nplt.scatter(df3[\"Spending Score (1-100)\"],df3[\"Annual Income (k$)\"],color='red')\nplt.scatter(df4[\"Spending Score (1-100)\"],df4[\"Annual Income (k$)\"],color='yellow')\nplt.scatter(df5[\"Spending Score (1-100)\"],df5[\"Annual Income (k$)\"],color='black')\nplt.scatter(km.cluster_centers_[:,0],km.cluster_centers_[:,1],color='purple',marker='*')\n\nplt.xlabel('Spending Score')\nplt.ylabel('Annual Income')","190e6422":"k_rng=range(1,11)\nsse=[]\nfor k in k_rng:\n    km=KMeans(n_clusters=k)\n    km.fit(df[['Spending Score (1-100)',\"Annual Income (k$)\"]])\n    sse.append(km.inertia_)","b0bd09db":"plt.xlabel('K')\nplt.ylabel('SSE')\n\nplt.plot(k_rng,sse)\nplt.scatter(k_rng,sse)","f9d587e5":"https:\/\/www.tutorialspoint.com\/machine_learning_with_python\/machine_learning_with_python_clustering_algorithms_k_means.htm\n\nhttps:\/\/www.youtube.com\/watch?v=EItlUEPCIzM\n\nhttps:\/\/www.analyticsvidhya.com\/blog\/2020\/10\/a-simple-explanation-of-k-means-clustering\/\n\n","2b4238e1":"# **Import dataset**","ca6d3db4":"# **Summary**","d1303e5c":"Again, the same process with the scaled data","0eaebb9b":"As we can see, five different clusters are seen in the image. so, I am picking 5 as the number of clusters.","ee344623":"Now, I am creating a new column \"cluster\" and copy the predictions to the new column","a1e145c1":"in the next plot, I am showing the cenroids by purple color and the * marker","dc24050f":"Hello Kagglers,\n\nK-Means clustering is the most popular unsupervised machine learning algorithm, which is used when you have unlabeled data. The goal of this algorithm is to find groups in the data, with the number of groups represented by the variable K. The algorithm works iteratively to assign each data point to one of K groups based on the features that are provided. Data points are clustered based on feature similarity. The results of the K-means clustering algorithm are:\n* The centroids of the K clusters, which can be used to label new data\n* Labels for the training data (each data point is assigned to a single cluster)\n\n![upvote.png](attachment:ce55333f-00f4-4ee3-8864-7a119beddd35.png)\n\nSo, let's go.","150df3dc":"# **Import Libraries**","ec09c039":"# **References**","8d97a3df":"Now, let's find the optimal value of K by using the elbow method. \n\nwe will try the K values from 1 to 10 and we will copy the SSE values to the sse.","c6a18301":"# **How Does the K-means clustering algorithm work?**\n\nK-Means clustering can be represented diagrammatically as follows:\n![Image](https:\/\/miro.medium.com\/max\/1122\/0*mQCGBdYhzZ8YMZPv.png)\n\nk-means clustering tries to group similar kinds of items in form of clusters. It finds the similarity between the items and groups them into the clusters. K-means clustering algorithm works in four steps:\n\nStep 1 \u2212 First, we need to specify the number of clusters, K, need to be generated by this algorithm.\n\nStep 2 \u2212 Next, randomly select K data points and assign each data point to a cluster. In simple words, classify the data based on the number of data points.\n\nStep 3 \u2212 Now, it will compute the cluster centroids.\n\nStep 4 \u2212 Next, keep iterating the following until we find optimal centroid which is the assignment of data points to the clusters that are not changing any more:\n\n    4.1. First, the sum of squared error (SSE) between data points and centroids would be computed.\n   \n    4.2. Now, we have to assign each data point to the cluster that is closer than other cluster (centroid).\n  \n    4.3. At last compute the centroids for the clusters by taking the average of all data points of that cluster.\n\n\nFigures 1 to 4 represent a simple explanation of the K-Means algorithm:\n\n![Image](https:\/\/editor.analyticsvidhya.com\/uploads\/34513k%20means.png)\n\n\nFigure 1 shows the representation of data of two different items. the first item has shown in blue color and the second item has shown in red color. In the first step, the value of K is chosen randomly.\n    In figure 2, to find out centroid, we will draw a perpendicular line to that line. The points will move to their centroid. If you will notice there, then you will see that some of the red points are now moved to the blue points. Now, these points belong to the group of blue color items.\n    The same process will be done in figure 3. The two points are joined and a new perpendicular line is drawn to find the centroid. Again, some of the red points get converted to blue points.\n    The same process is happening in figure 4. This process will be continued until and unless we get two completely different clusters of these groups.\n    we will keep iterating the following until we find optimal centroid which is the assignment of data points to the clusters that are not changing any more\n\n    NOTE: Please note that the K-means clustering uses the euclidean distance method to find out the distance between the points\n\n\n","038a0b7f":"Now, plot the scaled values","f4979afd":"we can obtain the value of the cenroids by the \"cluster_centers_\"","0225bace":"In some cases the data is not clustered optimally. This is due to the fact that our scaling is not right and there is a big gap between the values in the y-axis and the x-axis. So, we need to scale the data prior to the clustreing.\nlet's do the scaling with the MinMaxSclaer which we already imorted the library.","e2d2c285":"In this project, I implemented the most popular unsupervised clustering technique called K-Means Clustering.\n\n1. I showed how we can use sclaing to normalize the input data in case of having a large gap betwwn the input features.\n1. I applied the elbow method. The elbow method show that K=5 (k is number of clusters) can be considered an optimal number of cluster.\n\n\n\n\n\n","45606ac3":"now, we need to separate the 5 clusters into different dataframes. then plot the clusters in different colors","973a9976":"No, let's plot the sse plot. As we can see, the K=5 is the optimal value ","bc050a86":"As you can see, there are 5 columns in the dataset. For the sake of simplicity, in this example, I am using the columns \"*Spending Score (1-100)*\" and \"*Annual Income (k$)*\"\n\nThe scatter plot of the 2 columns is shwon in the next cell.","e9c52a8a":"# **How to pick the optimal value of K?**\n\nthe performance of K-Means algorithm depends upon the value of K. We should choose the optimal value of K to have best performance. There are different techniques to find the optimal value of K. The most common technique is the \"elbow method\" which is described below.\n\n![Image](https:\/\/editor.analyticsvidhya.com\/uploads\/62725cluster0.PNG)\n\nElbow method is an empirical method to find out the best value of k. it picks up a range of values and takes the best among them by running the algorithm multiple times over a loop, with an increasing number of cluster choice and then plotting a clustering score as a function of the number of clusters.\n\n    Note: the elbow method calculates the sum of the square of the points and calculates the average distance."}}