{"cell_type":{"ae20a3d6":"code","c526b6dd":"code","c2f80084":"code","c2882491":"code","4c2df374":"code","2775dd92":"code","6891001f":"code","7b1fcf86":"code","e101c279":"code","ec2a2736":"code","bb2b5fe9":"code","c8a8da66":"code","03c0b126":"code","2b41b202":"code","858e74a0":"code","05d2796f":"code","5ac868d6":"code","a543a6fe":"code","f74a0335":"code","c9724583":"code","26387c06":"code","ac711ebd":"code","9c56e1fe":"code","d4ba6e71":"code","6974ad9b":"code","b718bfc4":"code","d17aa59c":"code","e462d3f6":"code","af8462b0":"code","101cbe26":"code","3580fa97":"code","36e66529":"code","b129d317":"code","3fa71dbd":"code","3abe936e":"code","3e580ffe":"code","a8f6a6f7":"code","f07b30aa":"code","164f7a27":"code","54dd7cd0":"code","276d74dd":"code","e7343f03":"code","8229a445":"code","379cde3a":"code","cc341c9b":"code","cf0c14bd":"code","2ef37ed6":"code","795d01b5":"code","782da7a6":"code","7c8522cb":"code","46e7b15f":"code","f71a8a97":"code","e81d6598":"code","6e4755a3":"code","7e40b386":"code","6adf38dc":"code","757766d1":"code","7b79c883":"markdown","09bb1362":"markdown","de58bcbf":"markdown","a5294a64":"markdown","6589b820":"markdown","41dd2d0a":"markdown","db0ba298":"markdown","1b9cb967":"markdown","386a25f9":"markdown","b5e49436":"markdown","7212295c":"markdown","1147a8f1":"markdown","80027f14":"markdown","7b5f8e9d":"markdown","67430add":"markdown","c5680ca6":"markdown","b1557b90":"markdown","f5a4cd0d":"markdown","d6da9d95":"markdown","72a66e4a":"markdown","a972479a":"markdown","aa424c32":"markdown","fa45533a":"markdown","6bf84c3f":"markdown","f14ac429":"markdown","f8217f7e":"markdown","d393789a":"markdown","3ceb70e1":"markdown","0ea43381":"markdown","057f99b6":"markdown","cf32863b":"markdown","3b27dd91":"markdown"},"source":{"ae20a3d6":"# import the usual modules\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport seaborn as sns # data visualisation\nimport matplotlib.pyplot as plt # plot\nimport os\nimport warnings\n#import funcs\nfrom datetime import datetime\nfrom matplotlib.pyplot import cm","c526b6dd":"# Some commands\nwarnings.filterwarnings('ignore')\n# color map","c2f80084":"import os\nimport zipfile\n#\ndir_zip = '..\/input\/walmart-recruiting-store-sales-forecasting\/'\nzip_features = dir_zip+'features.csv.zip'\nzip_train = dir_zip+'train.csv.zip'\nzip_test = dir_zip+'test.csv.zip'\nzip_sample = dir_zip+'sampleSubmission.csv.zip'\n#\nzip_ref = zipfile.ZipFile(zip_features, 'r')\nzip_ref.extractall()\nzip_ref.close()\n#\nzip_ref = zipfile.ZipFile(zip_train, 'r')\nzip_ref.extractall()\nzip_ref.close()\n#\nzip_ref = zipfile.ZipFile(zip_test, 'r')\nzip_ref.extractall()\nzip_ref.close()\n#\nzip_ref = zipfile.ZipFile(zip_sample, 'r')\nzip_ref.extractall()\nzip_ref.close()\n#\n","c2882491":"# read csv files and create dataframes\ndf_train = pd.read_csv('train.csv', parse_dates=['Date']).set_index(keys=['Store','Dept','Date'], drop=False)\ndf_test = pd.read_csv('test.csv', parse_dates=['Date']).set_index(keys=['Store','Dept','Date'], drop=False)\ndf_stores = pd.read_csv('..\/input\/walmart-recruiting-store-sales-forecasting\/stores.csv').set_index(keys=['Store'], drop=False)\ndf_features = pd.read_csv('features.csv', parse_dates=['Date']).set_index(keys=['Store','Date'], drop=False)\ndf_train.index.names = ['Store_ind', 'Dept_ind', 'Date_ind']\ndf_test.index.names = ['Store_ind', 'Dept_ind', 'Date_ind']","4c2df374":"# create new features \ndf_train['Weekofyear'] = df_train['Date'].dt.weekofyear\ndf_test['Weekofyear'] = df_test['Date'].dt.weekofyear\ndf_train['Month'] = df_train['Date'].dt.month\ndf_test['Month'] = df_test['Date'].dt.month\ndf_train['Year'] = df_train['Date'].dt.year\ndf_test['Year'] = df_test['Date'].dt.year\ndf_features['Temperature'] = (df_features['Temperature']-32.)\/1.8 # to Celsius degree\ndf_features['Fuel_Price'] = df_features['Fuel_Price']\/3.785411784 # to liters\ndf_train['pctchange_Weekly_Sales'] = df_train['Weekly_Sales'].pct_change().fillna(0.)","2775dd92":"# join dataframes\ndf_train_all = df_train.join(df_features[['Temperature', 'Fuel_Price', 'CPI', 'Unemployment', \n                                          'MarkDown1', 'MarkDown2', 'MarkDown3', 'MarkDown4', 'MarkDown5']],\n                             on=['Store', 'Date'])\ndf_train_all = df_train_all.join(df_stores[['Type','Size']], on=['Store'])\ndf_test_all = df_test.join(df_features[['Temperature', 'Fuel_Price', 'CPI', 'Unemployment', \n                                        'MarkDown1', 'MarkDown2', 'MarkDown3', 'MarkDown4', 'MarkDown5']],\n                           on=['Store', 'Date'])\ndf_test_all = df_test_all.join(df_stores[['Type','Size']], on=['Store'])","6891001f":"# perform some data cleaning\nlist_dummies = ['IsHoliday', 'Type']\nfor dummy in list_dummies:\n    df_train_all[dummy] = pd.get_dummies(df_train_all[dummy],drop_first=True)\n    df_test_all[dummy] = pd.get_dummies(df_train_all[dummy],drop_first=True)\nlist_markdown = ['MarkDown1', 'MarkDown2', 'MarkDown3', 'MarkDown4', 'MarkDown5']\ndf_train_all[list_markdown] = df_train_all[list_markdown].fillna(0.)\ndf_test_all[list_markdown] = df_test_all[list_markdown].fillna(0.) ","7b1fcf86":"list_inpfeat = ['Temperature', 'Fuel_Price', 'CPI', 'Unemployment', 'IsHoliday', 'Type', 'Size']\nlist_inpdate = ['Weekofyear', 'Month', 'Year']\nlist_store_dept = [(1,1),(2,2),(3,3),(4,4),(5,5)]","e101c279":"df_train_all.head(10) ","ec2a2736":"df_train_all2 = df_train_all.loc[(slice(None), slice(None), slice('2010-02-05','2012-02-03')),:] #\ndf_valid_all = df_train_all.loc[(slice(None), slice(None), slice('2012-02-10','2015-02-05')),:]\n#print(len(df_valid_all.loc[(1,1)]))\n#df_valid_all.head()\n#df_train_all.head()","bb2b5fe9":"# first statistics\nNstore = df_train.index.max()[0] ; Ndept = df_train.index.max()[1]\ndf_stats = pd.concat([df_train.groupby(['Store','Dept']).median().Weekly_Sales,\n                      df_train.groupby(['Store','Dept']).mean().Weekly_Sales,\n                      df_train.groupby(['Store','Dept']).std().Weekly_Sales,\n                      df_train.groupby(['Store','Dept']).max().Weekly_Sales, \n                      df_train.groupby(['Store','Dept']).min().Weekly_Sales],\n                     axis=1)\ndf_stats.columns = ['Median', 'Mean', 'Std', 'Max', 'Min']\ndf_stats['Norm_std'] = df_stats['Std']\/(df_stats['Mean']) #np.abs\ndf_stats['Norm_maxmin'] = (df_stats['Max']-df_stats['Min'])\/df_stats['Median']\ndf_stats = df_stats.sort_values(by=['Norm_std'], axis=0, ascending=False)\ndf_stats.to_csv('summary_stores_stats.csv')\ndf_stats.head()","c8a8da66":"# Distribution of averaged weekly sales and normalised standard deviations\nfrom scipy.stats import norm, skew, probplot #for some statistics\nplt.figure(0,figsize=[15,5])\nplt.subplots_adjust(wspace=0.2, hspace=0.5)\nplt.subplot(1,2,1)\nsns.distplot(df_stats.Mean)#, fit=norm\nplt.subplot(1,2,2)\nplt.xlim([0,5])\nsns.distplot(df_stats.Norm_std.replace([np.inf, -np.inf], np.nan).dropna(),bins=1000)#, fit=norm\nplt.show()\nplt.close(0)","03c0b126":"# plot target feature with time\ndef plot_target(df, storedept, target, plotname, dpi=100):\n    plt.figure(0,figsize=(9,3), dpi=dpi)\n    for storedept_tupple in storedept:\n        namemag = str(storedept_tupple[0])+' '+str(storedept_tupple[1])\n        try:\n            df2 = df.loc[storedept_tupple,target]#.drop('Date',axis=1)\n            plt.plot(df2.index.to_pydatetime(),df2,label=namemag)# #, color='tab:red')\n        except:\n            pass\n    plt.gca().set(xlabel='Date', ylabel=target, title='Store '+str(storedept_tupple[0])+' '+str(storedept_tupple[1]))\n    plt.legend()\n    #plt.savefig('figures\/'+plotname+'.pdf',bbox_inches='tight',transparent=True)\n    plt.show()\n    plt.close(0)","2b41b202":"# plot weekly sale for all departments in individual stores with time\nplot_target(df_train_all,list_store_dept,'Weekly_Sales','sale_store')","858e74a0":"# plot target feature with time\ndef plot_target_diff(df, storedept, list_diff, target, plotname, dpi=100):\n    plt.figure(0,figsize=(9,3), dpi=dpi)\n    for storedept_tupple in storedept:\n        namemag = str(storedept_tupple[0])+' '+str(storedept_tupple[1])\n        try:\n            df2 = df.loc[storedept_tupple,target]#.drop('Date',axis=1)\n            for diff in list_diff:\n                df2 = df2.diff(diff)\n            plt.plot(df2.index.to_pydatetime(),df2,label=namemag)# #, color='tab:red')\n        except:\n            pass\n    plt.gca().set(xlabel='Date', ylabel=target, title='Store '+str(storedept_tupple[0])+' '+str(storedept_tupple[1]))\n    plt.legend()\n    #plt.savefig('figures\/'+plotname+'.pdf',bbox_inches='tight',transparent=True)\n    plt.show()\n    plt.close(0)","05d2796f":"# plot weekly sale for all departments in individual stores with time\nplot_target_diff(df_train_all,list_store_dept,[1,52],'Weekly_Sales','sale_store')","5ac868d6":"# Distribution of target feature\ndef plot_targetdistrib(y,plottitle,plotname):\n\tfrom scipy.stats import norm, skew, probplot #for some statistics\n\tplt.figure(0,figsize=[15,5])\n\tplt.subplots_adjust(wspace=0.2, hspace=0.5)\n\tplt.subplot(1,2,1)\n\t(mu, sigma) = norm.fit(y)\n\t#print( 'mu = {:.2f} and sigma = {:.2f}'.format(mu, sigma))\n\tsns.distplot(y, fit=norm)\n\tplt.legend(['Normal dist. ($\\mu=$ {:.2f} and $\\sigma=$ {:.2f} )'.format(mu, sigma)],\n\t            loc='best')\n\tplt.ylabel('Frequency')\n\tplt.title(plottitle)\n\t# QQ-plot wrt normal distribution\n\tplt.subplot(1,2,2)\n\tres = probplot(y, plot=plt)\n\t#plt.savefig('figures\/'+plotname+'.pdf',bbox_inches='tight',transparent=True)\n\tplt.show()\n\tplt.close(0)\n","a543a6fe":"# plot weekly sale distribution for a given department\ndf2 = df_train_all.loc[(1,1)]\nplot_targetdistrib(df2.Weekly_Sales,'Weekly_Sales','saledistrib1')","f74a0335":"# plot individual feature with time\ndef plot_features(df, list_feature, storedept, plotname, dpi=100):\n    Nstore = df.index.max()[0]\n    plt.figure(0,figsize=(16,5), dpi=dpi)\n    plt.subplots_adjust(wspace=0.25, hspace=0.5)\n    #\n    for ifeat, feat in enumerate(list_feature):\n        plt.subplot(1,len(list_feature),ifeat+1)\n        plt.gca().set(xlabel='Date', ylabel=feat)\n        plt.xticks(rotation=45)\n        for storedept_tupple in storedept:\n            try:\n                df2 = df.loc[storedept_tupple].drop('Date',axis=1)\n                plt.plot(df2.index.to_pydatetime(), df2[feat])# #, color='tab:red')\n            except:\n                pass\n    #plt.savefig('figures\/'+plotname+'.pdf',bbox_inches='tight',transparent=True)\n    plt.show()\n    plt.close(0)","c9724583":"# plot evolution of input features with time\nplot_features(df_train_all,['Temperature', 'Fuel_Price', 'CPI', 'Unemployment'],\n              list_store_dept, 'temp_time')","26387c06":"# compare features vs weekly sale\ndef comp_features(df, list_x, y, storedept, plotname, dpi=100):\n    Nstore = df.index.max()[0]\n    plt.figure(0,figsize=(16,5), dpi=dpi)\n    plt.subplots_adjust(wspace=0.3, hspace=0.5)\n    #\n    for ix, x in enumerate(list_x):\n        plt.subplot(1,len(list_x),ix+1)\n        plt.gca().set(xlabel=x, ylabel=y)\n        plt.xticks(rotation=45)\n        for storedept_tupple in storedept:\n            try:\n                df2 = df.loc[storedept_tupple].drop('Date',axis=1)\n                plt.scatter(df2[x], df2[y])# #, color='tab:red')\n            except:\n                pass\n    #plt.savefig('figures\/'+plotname+'.pdf',bbox_inches='tight',transparent=True)\n    plt.show()\n    plt.close(0)","ac711ebd":"comp_features(df_train_all,['Temperature', 'Fuel_Price', 'CPI', 'Unemployment'], 'Weekly_Sales',\n                   list_store_dept, 'features_vs_weeklysale')","9c56e1fe":"# compare features vs weekly sale\ndef corr_depts(df, store, list_dept, target, plotname, dpi=100):\n    Nstore = df.index.max()[0]\n    plt.figure(0,figsize=(16,5), dpi=dpi)\n    plt.subplots_adjust(wspace=0.3, hspace=0.5)\n    #\n    iplot = 1\n    for x in (list_dept[1:]):\n        plt.subplot(1,len(list_dept)-1,iplot)\n        plt.gca().set(xlabel=x, ylabel=list_dept[0])\n        plt.xticks(rotation=45)\n        try:\n            dfx = df.loc[(store,x)].drop('Date',axis=1)[target]\n            dfy = df.loc[(store,list_dept[0])].drop('Date',axis=1)[target]\n            plt.scatter(dfx, dfy, label=('%.2f' % (dfx.corr(dfy))))# #, color='tab:red')\n        except:\n            pass\n        iplot += 1\n        plt.legend()\n    #plt.savefig('figures\/'+plotname+'.pdf',bbox_inches='tight',transparent=True)\n    plt.show()\n    plt.close(0)","d4ba6e71":"corr_depts(df_train_all,1, [1,2,3,4,5],'Weekly_Sales', 'features_vs_weeklysale')","6974ad9b":"corr_depts(df_train_all,1, [1,2,3,4,5],'pctchange_Weekly_Sales', 'features_vs_weeklysale')","b718bfc4":"# ADF Test\nfrom statsmodels.tsa.stattools import adfuller, kpss\ndef adf_test(df):\n    if len(df.Weekly_Sales) > 10:\n        result = adfuller(df.Weekly_Sales, autolag='AIC')\n    else:\n        result = [np.nan, np.nan, np.nan, np.nan, np.nan]\n    return result[1] #result[0], result[1], result[4]\n#df_stats['ADFstats'] = df_train_all.groupby(['Store','Dept']).apply(adf_test)\ndf_stats['ADF_pvalue'] = df_train_all.groupby(['Store','Dept']).apply(adf_test)\ndf_stats.sort_values(['Store','Dept'], axis=0, ascending=True).head()","d17aa59c":"# Seasonal Decomposition\nfrom statsmodels.tsa.seasonal import seasonal_decompose\ndef plot_season(df, list_store_dept):\n    plt.rcParams.update({'figure.figsize': (8,5)})\n    #\n    # Multiplicative Decomposition \n    # Additive Decomposition\n    plt.figure(figsize=[15,10])\n    plt.subplots_adjust(wspace=0.33, hspace=0.5)\n    for ist, storedept in enumerate(list_store_dept):\n        result_add = seasonal_decompose(df.loc[storedept,'Weekly_Sales'], \n                                        model='additive', extrapolate_trend='freq', freq=52)\n        # Plot\n        plt.subplot(len(list_store_dept),4,ist*4+1) ; plt.xticks(rotation=30)\n        plt.plot(df.loc[storedept,'Weekly_Sales'],label=storedept) ; plt.ylabel('Weekly Sales') ; plt.legend()\n        plt.subplot(len(list_store_dept),4,ist*4+2) ; plt.xticks(rotation=30)\n        plt.plot(result_add.trend,label=storedept) ; plt.ylabel('Trend') ; plt.legend()\n        plt.subplot(len(list_store_dept),4,ist*4+3) ; plt.xticks(rotation=30)\n        plt.plot(result_add.seasonal) ; plt.ylabel('Seasonal')\n        plt.subplot(len(list_store_dept),4,ist*4+4) ; plt.xticks(rotation=30)\n        plt.plot(result_add.resid) ; plt.ylabel('Residual')#.plot()#.suptitle('Additive Decompose', fontsize=16)","e462d3f6":"plot_season(df_train_all, list_store_dept)","af8462b0":"from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n\ndef plot_diff(df,list_diff,nlags):\n    plt.rcParams.update({'figure.figsize':(9,5), 'figure.dpi':120})\n    #\n    df2 = df.Weekly_Sales.reset_index()#\n    #\n    fig, (ax1, ax2, ax3) = plt.subplots(1, 3, sharex=False, sharey=False, figsize=(15, 5))\n    for diff in list_diff:\n        if diff > 0:\n            df2 = df2.diff(diff)\n    ax1.plot(df2.Weekly_Sales) ; ax1.set_title('Order Differencing '+str(list_diff))\n    plot_acf(df2.Weekly_Sales.dropna(), ax=ax2,lags=nlags); ax2.set_xlim(0,nlags) ; ax2.set_ylim(-1,1) \n    plot_pacf(df2.Weekly_Sales.dropna(), ax=ax3,lags=nlags) ; ax3.set_xlim(0,nlags) ; ax3.set_ylim(-1,1) ","101cbe26":"plot_diff(df_train_all.loc[(2,2)],[0],53)","3580fa97":"plot_diff(df_train_all.loc[(2,2)],[52,1],20)","36e66529":"# create a function that plots the rolling mean and std, and performs the ADF test\nfrom statsmodels.tsa.stattools import adfuller\ndef test_stationarity(ts,window):\n    #\n    #Determing rolling statistics\n    rolmean = ts.rolling(window=window).mean() #pd.rolling_mean(ts, window=12)\n    rolstd = ts.rolling(window=window).std() #pd.rolling_std(ts, window=12)\n\n    #Plot rolling statistics:\n    plt.figure(figsize=(8,3))\n    orig = plt.plot(ts, color='blue',label='Original')\n    mean = plt.plot(rolmean, color='red', label='Rolling Mean')\n    std = plt.plot(rolstd, color='black', label = 'Rolling Std')\n    plt.legend(loc='best')\n    plt.title('Rolling Mean & Standard Deviation')\n    plt.show(block=False)\n    \n    #Perform Dickey-Fuller test:\n    print('Results of Dickey-Fuller Test:')\n    dftest = adfuller(ts, autolag='AIC')\n    dfoutput = pd.Series(dftest[0:4], index=['Test Statistic','p-value','#Lags Used','Number of Observations Used'])\n    for key,value in dftest[4].items():\n        dfoutput['Critical Value (%s)'%key] = value\n    print(dfoutput)","b129d317":"# Accuracy metrics\nfrom statsmodels.tsa.stattools import acf\ndef forecast_accuracy(forecast, actual):\n    mape = np.mean(np.abs(forecast - actual)\/np.abs(actual))  # Mean Absolute Percentage Error\n    me = np.mean(forecast - actual)             # ME\n    mae = np.mean(np.abs(forecast - actual))    # MAE\n    mpe = np.mean((forecast - actual)\/actual)   # MPE\n    rmse = np.mean((forecast - actual)**2)**.5  # RMSE\n    corr = np.corrcoef(forecast, actual)[0,1]   # corr\n    mins = np.amin(np.hstack([forecast[:,None], \n                              actual[:,None]]), axis=1)\n    maxs = np.amax(np.hstack([forecast[:,None], \n                              actual[:,None]]), axis=1)\n    minmax = 1 - np.mean(mins\/maxs)             # minmax\n    #acf1 = acf(fc-test)[1]                      # ACF1\n    return({'mape':mape, 'me':me, 'mae': mae, \n            'mpe': mpe, 'rmse':rmse, #'acf1':acf1, \n            'corr':corr, 'minmax':minmax})","3fa71dbd":"# Averaging the yearly evolution\nmean_sale = df_train_all2.groupby(['Store','Dept','Weekofyear'])['Weekly_Sales'].mean()\ndf_valid_pred = df_valid_all[['Store', 'Dept', 'Weekofyear']].join(mean_sale, on=['Store', 'Dept', 'Weekofyear'])","3abe936e":"list_index = [] ; list_mape = []\nfor storedept in list_store_dept:\n    dftrain = df_train_all2.loc[storedept] ; dfpred = df_valid_pred.loc[storedept] ; dfactual = df_valid_all.loc[storedept]\n    list_index.append(storedept) ; list_mape.append(forecast_accuracy(dfpred.Weekly_Sales, dfactual.Weekly_Sales)['mape'])\ndf_mape = pd.DataFrame({'MAPE_simplemodel' : list_mape}, index = list_index)\ndf_mape.head()","3e580ffe":"# plot the prediction \ndef plot_predtarget(list_storedept, df_train, df_test, df_actual, target, plotname, dpi=100):\n    color=cm.tab10(np.linspace(0,1,len(list_storedept)*2))\n    plt.figure(0,figsize=(16,5), dpi=dpi)\n    for ist, storedept in enumerate(list_storedept):\n        dftrain = df_train.loc[storedept] ; dfpred = df_test.loc[storedept] ; dfactual = df_actual.loc[storedept]\n        plt.plot(dftrain.index.to_pydatetime(),dftrain[target],color=color[ist],label=storedept)# #, color='tab:red')\n        plt.plot(dfpred.index.to_pydatetime(),dfpred[target],color=color[ist],ls='--')# #, color='tab:red')\n        plt.plot(dfactual.index.to_pydatetime(),dfactual[target],color=color[ist],ls='-')# #, color='tab:red')\n    plt.gca().set(xlabel='Date', ylabel=target, title='Store ')\n    plt.legend()\n    #plt.savefig('figures\/'+plotname+'.pdf',bbox_inches='tight',transparent=True)\n    plt.show() ; plt.close(0)","a8f6a6f7":"plot_predtarget(list_store_dept,df_train_all2,df_valid_pred,df_valid_all,'Weekly_Sales','predsale_store1')","f07b30aa":"train = df_train_all2.loc[list_store_dept[1],'Weekly_Sales']\nvalid = df_valid_all.loc[list_store_dept[1],'Weekly_Sales']","164f7a27":"#Perform a grid search by: varying (p,d,q) and (P,D,Q), using the AIC as information criterion\nfrom statsmodels.tsa.arima_model import ARIMA\nimport pmdarima as pm\n\nsmodel = pm.auto_arima(train, \n                       start_p=1, start_q=1, start_d=1, # start p, q, d \n                       max_p=2, max_q=2,     # maximum p and q\n                       max_d=2, d=None,        # let model determine 'd'\n                       seasonal=True,        # Seasonality\n                       m=52,                 # frequency of series\n                       start_P=1, start_Q=1, # \n                       max_P=1, max_Q=1,     # maximum P and Q\n                       D=1,                  # one \"yearly\" differentiation\n                       test='adf',           # use adftest to find optimal 'd'\n                       information_criterion='aic', # used to select best model\n                       trace=True,          # print results whilst training\n                       error_action='ignore',   # ignore orders that don't work\n                       suppress_warnings=True, \n                       stepwise=True,       # apply intelligent order search\n                      )\n\n\nprint(smodel.summary())","54dd7cd0":"smodel.plot_diagnostics()\nplt.show()","276d74dd":"# Forecast\nn_periods = len(valid.index) \nfitted, confint = smodel.predict(n_periods=n_periods, return_conf_int=True)\nin_sample_preds, in_sample_confint = smodel.predict_in_sample(return_conf_int=True)\nindex_of_fc = valid.index #pd.date_range(valid.index[0], periods = Nteststeps, freq='7D')\n#\n# make forecast series\ntrain_fc = pd.Series(in_sample_preds, index=train.index)\nvalid_fc = pd.Series(fitted, index=index_of_fc)\nvalid_fc_lower = pd.Series(confint[:, 0], index=index_of_fc)\nvalid_fc_upper = pd.Series(confint[:, 1], index=index_of_fc)","e7343f03":"# Compare predicted and actual test TS\nfig, axes = plt.subplots(1, 1, figsize=(10,5), dpi=100, sharex=True)\nplt.plot(train, label='Training set')\nplt.plot(train_fc, label='Fit training set')\nplt.plot(valid_fc, color='darkgreen', label='Forecast')\nplt.fill_between(valid_fc_lower.index, \n                 valid_fc_lower, \n                 valid_fc_upper, \n                 color='k', alpha=.15)\nplt.plot(valid, label='Validation set')\nplt.legend()\nplt.title(\"SARIMA - Forecast on validation set\")\nplt.show()","8229a445":"forecast_accuracy(valid_fc, valid)","379cde3a":"#Perform a grid search by: varying (p,d,q) and (P,D,Q), using the AIC as information criterion\nfrom statsmodels.tsa.arima_model import ARIMA\nimport pmdarima as pm\n\nsarima_index = [] ; sarima_mape = []\nsarima_train_fc = [] ; sarima_valid_fc = []\nsarima_valid_fc_lower = [] ; sarima_valid_fc_upper = []\n#\nfor storedept in list_store_dept: #[:2]:\n    train = df_train_all2.loc[storedept,'Weekly_Sales'] ; valid = df_valid_all.loc[storedept,'Weekly_Sales']\n    #\n    # run the grid search for given timeseries\n    smodel = pm.auto_arima(train, \n                           start_p=1, start_q=1, \n                           max_p=2, max_q=2,     # maximum p and q\n                           d=None,               # let model determine 'd'\n                           seasonal=True,        # Seasonality\n                           m=52,                 # frequency of series\n                           start_P=1, start_Q=1, # \n                           max_P=1, max_Q=1,     # maximum P and Q\n                           D=1,                  # one \"yearly\" differentiation\n                           test='adf',           # use adftest to find optimal 'd'\n                           information_criterion='aic', # used to select best model\n                           trace=True,          # print results whilst training\n                           error_action='ignore',   # ignore orders that don't work\n                           suppress_warnings=True, \n                           stepwise=True,       # apply intelligent order search\n                          )\n    #\n    # get the forecast TS\n    n_periods = len(valid.index)\n    fitted, confint = smodel.predict(n_periods=n_periods, return_conf_int=True)\n    in_sample_preds, in_sample_confint = smodel.predict_in_sample(return_conf_int=True)\n    index_of_fc = valid.index #pd.date_range(holdout.index[0], periods = Nteststeps, freq='7D')\n    #\n    # make forecast series\n    train_fc = pd.Series(in_sample_preds, index=train.index)\n    valid_fc = pd.Series(fitted, index=index_of_fc)\n    valid_fc_lower = pd.Series(confint[:, 0], index=index_of_fc)\n    valid_fc_upper = pd.Series(confint[:, 1], index=index_of_fc)\n    #\n    #print(smodel.summary())\n    # save the forecast \n    sarima_train_fc.append(train_fc) ; sarima_valid_fc.append(valid_fc)\n    sarima_valid_fc_lower.append(valid_fc_lower) ; sarima_valid_fc_upper.append(valid_fc_upper)\n    #\n    # evaluate the error\n    sarima_mape.append(forecast_accuracy(valid_fc, valid)['mape'])\n    sarima_index.append(storedept)\n","cc341c9b":"df_mape['MAPE_SARIMA'] = pd.DataFrame({'MAPE_simeplemodel' : sarima_mape}, index = sarima_index)\ndf_mape.head()","cf0c14bd":"# Compare predicted and actual test TS\ndef plot_forecast(stdep, train, trainfc, valid, validfc, validfclower, validfcupper, plottitle):\n    fig, axes = plt.subplots(1, 1, figsize=(10,3), dpi=100, sharex=True)\n    plt.plot(train, label='Training set')\n    plt.plot(trainfc, label='Fit training set')\n    plt.plot(validfc, color='darkgreen', label='Forecast')\n    try:\n        plt.fill_between(validfclower.index, \n                         validfclower, \n                         validfcupper, \n                         color='k', alpha=.15)\n    except:\n        pass\n    plt.plot(valid, label='Validation set')\n    plt.legend()\n    plt.title(plottitle+\" - Forecast on validation set of \"+str(stdep))\n    plt.show()","2ef37ed6":"for isd, storedept in enumerate(list_store_dept): #[:2]:\n    train = df_train_all2.loc[storedept,'Weekly_Sales'] ; valid = df_valid_pred.loc[storedept,'Weekly_Sales']\n    train_fc = sarima_train_fc[isd]\n    valid_fc = sarima_valid_fc[isd] ; valid_fc_lower = sarima_valid_fc_lower[isd] ; valid_fc_upper = sarima_valid_fc_upper[isd] ; \n    plot_forecast(storedept, train, train_fc, valid, valid_fc, valid_fc_lower, valid_fc_upper, 'SARIMA')","795d01b5":"#Perform a grid search by: varying (p,d,q) and (P,D,Q), using the AIC as information criterion\nfrom statsmodels.tsa.arima_model import ARIMA\nimport pmdarima as pm\n\nsarimax_index = [] ; sarimax_mape = [] ; sarimax_train_fc = [] ; sarimax_valid_fc = []\nsarimax_valid_fc_lower = [] ; sarimax_valid_fc_upper = []\n#\nfor storedept in list_store_dept: # [:2]\n    train = df_train_all2.loc[storedept,'Weekly_Sales'] ; valid = df_valid_all.loc[storedept,'Weekly_Sales']\n    feattrain = df_train_all2.loc[storedept, list_inpfeat] ; featvalid = df_valid_all.loc[storedept, list_inpfeat]\n    #\n    # run the grid search for given timeseries\n    smodel = pm.auto_arima(train, \n                           start_p=1, start_q=1, \n                           max_p=2, max_q=2,     # maximum p and q\n                           d=None,               # let model determine 'd'\n                           seasonal=True,        # Seasonality\n                           m=52,                 # frequency of series\n                           start_P=1, start_Q=1, # \n                           max_P=1, max_Q=1,     # maximum P and Q\n                           D=1,                  # one \"yearly\" differentiation\n                           exogenous=feattrain,  # exogeneous variables\n                           test='adf',           # use adftest to find optimal 'd'\n                           information_criterion='aic', # used to select best model\n                           trace=True,           # print results whilst training\n                           error_action='ignore',# ignore orders that don't work\n                           suppress_warnings=True, \n                           stepwise=True,       # apply intelligent order search\n                          )\n    #\n    # get the forecast TS\n    n_periods = len(valid.index)\n    fitted, confint = smodel.predict(n_periods=n_periods, return_conf_int=True, exogenous=featvalid)\n    in_sample_preds, in_sample_confint = smodel.predict_in_sample(return_conf_int=True, exogenous=feattrain)\n    index_of_fc = valid.index \n    #\n    # make forecast series\n    train_fc = pd.Series(in_sample_preds, index=train.index)\n    valid_fc = pd.Series(fitted, index=index_of_fc)\n    valid_fc_lower = pd.Series(confint[:, 0], index=index_of_fc)\n    valid_fc_upper = pd.Series(confint[:, 1], index=index_of_fc)\n    #\n    #print(smodel.summary())\n    # save the forecast \n    sarimax_train_fc.append(train_fc) ; sarimax_valid_fc.append(valid_fc)\n    sarimax_valid_fc_lower.append(valid_fc_lower) ; sarimax_valid_fc_upper.append(valid_fc_upper)\n    #\n    # evaluate the error\n    sarimax_mape.append(forecast_accuracy(valid_fc, valid)['mape'])\n    sarimax_index.append(storedept)\n","782da7a6":"df_mape['MAPE_SARIMAX'] = pd.DataFrame({'MAPE_simeplemodel' : sarimax_mape}, index = sarimax_index)\ndf_mape.head()","7c8522cb":"for isd, storedept in enumerate(list_store_dept): #[:2]:\n    train = df_train_all2.loc[storedept,'Weekly_Sales'] ; valid = df_valid_pred.loc[storedept,'Weekly_Sales']\n    train_fc = sarimax_train_fc[isd]\n    valid_fc = sarimax_valid_fc[isd] ; valid_fc_lower = sarimax_valid_fc_lower[isd] ; valid_fc_upper = sarimax_valid_fc_upper[isd] ; \n    #valid_fc.head()\n    plot_forecast(storedept, train, train_fc, valid, valid_fc, valid_fc_lower, valid_fc_upper, 'SARIMAX')","46e7b15f":"def mean_absolute_percentage_error(y_true, y_pred): \n    y_true, y_pred = check_arrays(y_true, y_pred)\n    return np.mean(np.abs((y_true - y_pred) \/ y_true)) #* 100","f71a8a97":"def create_lags(df, maxshift):\n    # data is a pandas Series containing time series data\n    data = pd.Series(...)\n    # Shifts\n    shifts = np.arange(1,maxshift+1)\n    # Create a dictionary of time-shifted data\n    many_shifts = {'lag_{}'.format(ii): df.shift(ii) for ii in shifts}\n    # Convert them into a dataframe\n    many_shifts = pd.DataFrame(many_shifts).fillna(0.)\n    return many_shifts","e81d6598":"import xgboost as xgb\nfrom sklearn.linear_model import Ridge\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.model_selection import TimeSeriesSplit, GridSearchCV\n\nNlags = 10\ninpfeature = list_inpdate  + ['lag_'+str(i+1) for i in range(Nlags)] # + list_inpfeat+ list_markdown['lag_1', 'lag_2', 'lag_3', 'lag_4', 'lag_5']\nlist_rescv = ['params', 'mean_test_score', 'std_test_score', 'mean_test_score', 'std_test_score', 'rank_test_score']\n#\n# define list of models and parameters\nlist_models = [('RF', RandomForestRegressor()),\n               ('XGB', xgb.XGBRegressor())\n              ]\n\nlist_params = [#{'Ridge__alpha' : np.logspace(0,4,10)},\n               {'max_depth' : [2, 4, 6, 8, 10, 12], #\n                #'n_estimators' : [10, 20, 50, 100, 200, 500, 1000]\n                },\n               {'max_depth' : [2, 4, 6, 8, 10, 12], #\n                #'n_estimators' : [5, 10, 20, 50, 100, 200]\n                }\n              ]\n#\nlist_train_fc = [] ; list_valid_fc = [] ; \nlist_bestparams = [] ; list_bestest = [] ; list_cvres = []\n#\nfor im, model in enumerate(list_models): \n    list_bestparams2 = [] ; list_bestest2 = [] ; list_cvres2 = []\n    list_train_fc2 = [] ; list_valid_fc2 = [] \n    list_mape = [] ; list_index = []\n    for storedept in list_store_dept:\n        print(storedept)\n        #\n        # introduce time-shifted features\n        dftrain2 = df_train_all2.loc[storedept]\n        dfvalid2 = df_valid_all.loc[storedept]\n        dftrainlag = create_lags(dftrain2.Weekly_Sales, Nlags)\n        dfvalidlag = create_lags(dfvalid2.Weekly_Sales, Nlags)\n        dftrain2 = dftrain2.join(dftrainlag,on='Date')\n        dfvalid2 = dfvalid2.join(dfvalidlag,on='Date')\n        #\n        # define training and validation sets\n        X = dftrain2[inpfeature] #df_train_all2.loc[storedept,inpfeature]\n        y = df_train_all2.loc[storedept,'Weekly_Sales']\n        X_valid = dfvalid2[inpfeature] #df_valid_all.loc[storedept,inpfeature]\n        y_valid = df_valid_all.loc[storedept,'Weekly_Sales']\n        #\n        # run grid search\n        param_search = list_params[im]\n        tscv = TimeSeriesSplit(n_splits=5)\n        gsearch = GridSearchCV(estimator=model[1],        # choice of model\n                               cv=tscv,                   # choice of splitting\n                               param_grid=param_search,   # grid of parameters\n                               verbose=1,                 # print messages\n                               return_train_score=True,   # return train score in CV grid result\n                               n_jobs=-1,                 # number of CPUs to be used\n                               scoring='neg_mean_absolute_error' # metrics to be used\n                              )\n        #\n        # save useful output\n        gsearch.fit(X, y)\n        list_bestparams2.append(gsearch.best_params_)\n        list_bestest2.append(gsearch.best_estimator_)\n        cvres = pd.DataFrame(gsearch.cv_results_)[list_rescv].sort_values(by='rank_test_score',ascending=True)\n        list_cvres2.append(cvres)\n        print('Best-fit parameters: ',gsearch.best_params_)\n        #\n        # get the forecast TS\n        #n_periods = len(valid.index)\n        valid_fc = pd.Series(gsearch.predict(X_valid), index=y_valid.index) #gsearch.predict(X_valid) \n        train_fc = pd.Series(gsearch.predict(X), index=X.index) #gsearch.predict(X)\n        #\n        # save the forecast \n        list_train_fc2.append(train_fc) ; list_valid_fc2.append(valid_fc)\n        #\n        # evaluate the error\n        list_mape.append(forecast_accuracy(valid_fc, y_valid)['mape'])\n        list_index.append(storedept)\n    #\n    list_train_fc.append(list_train_fc2) ; list_valid_fc.append(list_valid_fc2)\n    list_bestparams.append(list_bestparams2) ; list_bestest.append(list_bestest2) ; list_cvres.append(list_cvres2)\n    df_mape['MAPE_'+model[0]] = pd.DataFrame({'MAPE_'+model[0] : list_mape}, index = list_index)","6e4755a3":"df_mape.head()","7e40b386":"fig, axes = plt.subplots(len(list_store_dept), 1, figsize=(8,8), dpi=100, sharex=False)\nfig.tight_layout()\nnum_xgb = 1\nfor isd, storedept in enumerate(list_store_dept):\n    #print(list_bestest[num_xgb][isd])\n    xgb.plot_importance(list_bestest[num_xgb][isd], show_values=False, xlim=None, height=0.8, max_num_features=8, ax=axes[isd]) ; axes[isd].set_title(str(storedept))\n    if isd != len(list_store_dept)-1:\n        axes[isd].set_xlabel(' ')","6adf38dc":"# Compare predicted and actual test TS\ndef plot_forecast(stdep, train, trainfc, valid, validfc, validfclower, validfcupper, plottitle):\n    fig, axes = plt.subplots(1, 1, figsize=(10,3), dpi=100, sharex=True)\n    plt.plot(train, label='Training set')\n    plt.plot(trainfc, label='Fit training set')\n    plt.plot(validfc, color='darkgreen', label='Forecast')\n    try:\n        plt.fill_between(validfclower.index, \n                         validfclower, \n                         validfcupper, \n                         color='k', alpha=.15)\n    except:\n        pass\n    plt.plot(valid, label='Validation set')\n    plt.legend()\n    plt.title(plottitle+\" - Forecast on validation set of \"+str(stdep))\n    plt.show()","757766d1":"for im, model in enumerate(list_models): \n    for isd, storedept in enumerate(list_store_dept): #[:2]:\n        train = df_train_all2.loc[storedept,'Weekly_Sales'] ; valid = df_valid_pred.loc[storedept,'Weekly_Sales']\n        train_fc = list_train_fc[im][isd]\n        valid_fc = list_valid_fc[im][isd]\n        #valid_fc.head()\n        plot_forecast(storedept, train, train_fc, valid, valid_fc, 0., 0., model[0])\n        \n        ","7b79c883":"#### Distribution of averaged weekly sales and normalised standard deviations","09bb1362":"### [1. Importing and cleaning the data](#1)\n\n### [2. Exploratory Data Analysis (EDA)](#2)\n\n### [3. Analysis of time series](#3)\n\n### [4. Metrics and application with a super simple model](#4)\n\n### [5. SARIMA and SARIMAX](#5)\n\n### [6. Random Forest and Gradient Boosting](#6)\n\n### [7. Conclusions and Perspectives](#7)\n","de58bcbf":"#### Weekly sales and their pct_changes ","a5294a64":"### SARIMAX model with exogeneous variables","6589b820":"I choose here to use two algorithms, Random Forest and XGBoost, recognized for their performance on the modelling of time series.","41dd2d0a":"Let's start with a very simple model: the weekly sales averaged over the first two years. Two goals here: \n\n- get a first idea of the typical error for the chosen metrics \n- get a reference model to evaluate the improvement of more sophisticated models ","db0ba298":"> ### SARIMA model","1b9cb967":"## 2. EDA\n<a name=\"2\">\nlink\n<\/a>","386a25f9":"Let's have a look at the train dataframe.","b5e49436":"### Autocorrelation ","7212295c":"Clean the dataframes a bit (remove NaN and One-Hot encode categories). ","1147a8f1":"## 3. Time series analysis\n<a name=\"4\">\nlink\n<\/a>","80027f14":"Let's perform the same process for other departments","7b5f8e9d":"Join dataframes 'features' and 'stores' to 'train' and 'test'.","67430add":"## 1. Importing and cleaning the data\n<a name=\"1\">\nlink\n<\/a>","c5680ca6":"#### Weekly sales vs exogeneous variables","b1557b90":"Let's start by importing the four dataframes.","f5a4cd0d":"Let's compare the predictions to the actual sales","d6da9d95":"#### Weekly sales for some \"test\" departments (chosen arbitrarily). ","72a66e4a":"### Stationarity: ADF tests on the original TS","a972479a":"## 4. Metrics and application with a super simple model\n<a name=\"4\">\nlink\n<\/a>","aa424c32":"p-value higher than 0.05 and ADF Statistic higher than any critical values means that TS is clearly non stationary.","fa45533a":"### Time series transformation","6bf84c3f":"Let's compute some first stats of the series, and let's save them into a dataframe. ","f14ac429":"[Wikipedia](https:\/\/en.wikipedia.org\/wiki\/Autoregressive_integrated_moving_average)\n[auto_arima](https:\/\/alkaline-ml.com\/pmdarima\/modules\/generated\/pmdarima.arima.auto_arima.html)\n\n#### ARIMA model in words:\n\nPredicted Yt = Constant + Linear combination Lags of Y (upto p lags) + Linear Combination of Lagged forecast errors (upto q lags)\n\n\n#### 3 parameters to consider:\n- Number of AR (Auto-Regressive) terms (p): AR terms are just lags of dependent variable. For instance if p is 5, the predictors for x(t) will be x(t-1)\u2026.x(t-5).\n- Number of MA (Moving Average) terms (q): MA terms are lagged forecast errors in prediction equation. For instance if q is 5, the predictors for x(t) will be e(t-1)\u2026.e(t-5) where e(i) is the difference between the moving average at ith instant and actual value.\n- Number of Differences (d): minimum number of differencing needed to make the series stationary.\n","f8217f7e":"#### Exogeneous variables","d393789a":"## 5. Autoregressive Integrated Moving Average (ARIMA)\n<a name=\"5\">\nlink\n<\/a>","3ceb70e1":"## 6.  Random Forest and Gradient Boosting\n<a name=\"6\">\nlink\n<\/a>","0ea43381":"#### Distribution of a typical weekly sales series","057f99b6":"### Seasonality: decomposition of time series","cf32863b":"Choose exogeneous variables that will be used for our predictions. ","3b27dd91":"Let's create new features. "}}