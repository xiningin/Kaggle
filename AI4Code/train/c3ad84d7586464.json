{"cell_type":{"01223023":"code","abd2c9d1":"code","86556acd":"code","a69d8411":"code","64618d1c":"code","394c57cd":"code","6a2f9d7f":"code","f812fa80":"code","0a029aeb":"code","8ab0a7f5":"code","54049727":"markdown"},"source":{"01223023":"import numpy as np \nimport pandas as pd\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\nimport nltk\nimport re\nfrom nltk.corpus import stopwords\n#from nltk.stem.porter import PorterStemmer\nfrom nltk.stem import WordNetLemmatizer\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set(color_codes = True)","abd2c9d1":"df = pd.read_csv('\/kaggle\/input\/sms-spam-collection-dataset\/spam.csv',encoding='ISO-8859-1')\ndf.head()","86556acd":"df = df.iloc[:,:2]\ndf.rename(columns = {'v1':'label','v2':'message'},inplace = True)","a69d8411":"corpus = []\nlm = WordNetLemmatizer()\nfor i in range(0, len(df)):\n    review = re.sub('[^a-zA-Z]', ' ', df['message'][i])\n    review = review.lower()\n    review = review.split()\n    review = [lm.lemmatize(word) for word in review if not word in stopwords.words('english')]\n    review = ' '.join(review)\n    corpus.append(review)","64618d1c":"from sklearn.feature_extraction.text import TfidfVectorizer\ncv = TfidfVectorizer()\nX = cv.fit_transform(corpus).toarray()\n\n\n#TF-IDF stands for \u201cTerm Frequency \u2014 Inverse Document Frequency\u201d. \n#This is a technique to quantify a word in documents, \n#we generally compute a weight to each word which signifies the importance of the word in the document and corpus. \n#This method is a widely used technique in Information Retrieval and Text Mining.\n\ny=pd.get_dummies(df['label'])\ny=y.iloc[:,1].values","394c57cd":"# Train Test Split\n\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.20, random_state = 0)","6a2f9d7f":"from sklearn import svm\n\nclf = svm.SVC()\nclf.fit(X_train,y_train)\ny_predicted = clf.predict(X_test)\nscore = clf.score(X_test,y_test)\n\nprint(score)","f812fa80":"from sklearn.metrics import confusion_matrix\n\n#Confusion Matrix\n# Compute confusion matrix\ncnf_matrix = confusion_matrix(y_test, y_predicted)\nnp.set_printoptions(precision=2)\ncnf_matrix","0a029aeb":"import itertools\n\ndef plot_confusion_matrix(cm, classes,\n                          normalize=False,\n                          title='Confusion matrix',\n                          cmap=plt.cm.Blues):\n    if normalize:\n        cm = cm.astype('float') \/ cm.sum(axis=1)[:, np.newaxis]\n        print(\"Normalized confusion matrix\")\n    else:\n        print('Confusion matrix, without normalization')\n\n    print(cm)\n\n    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n    plt.title(title)\n    plt.colorbar()\n    tick_marks = np.arange(len(classes))\n    plt.xticks(tick_marks, classes, rotation=45)\n    plt.yticks(tick_marks, classes)\n\n    fmt = '.2f' if normalize else 'd'\n    thresh = cm.max() \/ 2.\n    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n        plt.text(j, i, format(cm[i, j], fmt),\n                 horizontalalignment=\"center\",\n                 color=\"white\" if cm[i, j] > thresh else \"black\")\n\n    plt.ylabel('True label')\n    plt.xlabel('Predicted label')\n    plt.tight_layout()","8ab0a7f5":"classes = df[\"label\"].value_counts()\n#With Normalization\nplt.figure()\nplot_confusion_matrix(cnf_matrix, classes=classes.index,\n                      title='Confusion matrix, without normalization')\n# With normalization\nplt.figure()\nplot_confusion_matrix(cnf_matrix, classes= classes.index, normalize=True,\n                      title='Normalized confusion matrix')\n\nplt.show()","54049727":"# Spam Detection using Natural Language Toolkit\n\n## Libraries\n\n#### 1. stopwords - NLTK has a list of stopwords from 16 languages. Stopwords are removed so the algorithm can process input faster. \n\n{\u2018ourselves\u2019, \u2018hers\u2019, \u2018between\u2019, \u2018yourself\u2019, \u2018but\u2019, \u2018again\u2019, \u2018there\u2019, \u2018about\u2019, \u2018once\u2019, \u2018during\u2019, \u2018out\u2019, \u2018very\u2019, \u2018having\u2019, \u2018with\u2019, \u2018they\u2019, \u2018own\u2019, \u2018an\u2019, \u2018be\u2019, \u2018some\u2019, \u2018for\u2019, \u2018do\u2019, \u2018its\u2019, \u2018yours\u2019, \u2018such\u2019, \u2018into\u2019, \u2018of\u2019, \u2018most\u2019, \u2018itself\u2019, \u2018other\u2019, \u2018off\u2019, \u2018is\u2019, \u2018s\u2019, \u2018am\u2019, \u2018or\u2019, \u2018who\u2019, \u2018as\u2019, \u2018from\u2019, \u2018him\u2019, \u2018each\u2019, \u2018the\u2019, \u2018themselves\u2019, \u2018until\u2019, \u2018below\u2019, \u2018are\u2019, \u2018we\u2019, \u2018these\u2019, \u2018your\u2019, \u2018his\u2019, \u2018through\u2019, \u2018don\u2019, \u2018nor\u2019, \u2018me\u2019, \u2018were\u2019, \u2018her\u2019, \u2018more\u2019, \u2018himself\u2019, \u2018this\u2019, \u2018down\u2019, \u2018should\u2019, \u2018our\u2019, \u2018their\u2019, \u2018while\u2019, \u2018above\u2019, \u2018both\u2019, \u2018up\u2019, \u2018to\u2019, \u2018ours\u2019, \u2018had\u2019, \u2018she\u2019, \u2018all\u2019, \u2018no\u2019, \u2018when\u2019, \u2018at\u2019, \u2018any\u2019, \u2018before\u2019, \u2018them\u2019, \u2018same\u2019, \u2018and\u2019, \u2018been\u2019, \u2018have\u2019, \u2018in\u2019, \u2018will\u2019, \u2018on\u2019, \u2018does\u2019, \u2018yourselves\u2019, \u2018then\u2019, \u2018that\u2019, \u2018because\u2019, \u2018what\u2019, \u2018over\u2019, \u2018why\u2019, \u2018so\u2019, \u2018can\u2019, \u2018did\u2019, \u2018not\u2019, \u2018now\u2019, \u2018under\u2019, \u2018he\u2019, \u2018you\u2019, \u2018herself\u2019, \u2018has\u2019, \u2018just\u2019, \u2018where\u2019, \u2018too\u2019, \u2018only\u2019, \u2018myself\u2019, \u2018which\u2019, \u2018those\u2019, \u2018i\u2019, \u2018after\u2019, \u2018few\u2019, \u2018whom\u2019, \u2018t\u2019, \u2018being\u2019, \u2018if\u2019, \u2018theirs\u2019, \u2018my\u2019, \u2018against\u2019, \u2018a\u2019, \u2018by\u2019, \u2018doing\u2019, \u2018it\u2019, \u2018how\u2019, \u2018further\u2019, \u2018was\u2019, \u2018here\u2019, \u2018than\u2019} \n\n#### 2. WordNetLemmatizer - Lemmatization is the grouping together of different forms of the same word. Lemmatization considers the context and converts the word to its meaningful base form. Wordnet is a database that offers lemmatization capabilities. \n\nEx: 'caring' -> lemmatization -> 'care'\n\n\n#### 3. TF-IDF Vectorizer - Transform text into a meaningful representation of numbers which is used to fit machine algorithm for prediction. Provides a weightage to each word.\n"}}