{"cell_type":{"e9bf063a":"code","3dcbf14b":"code","23c2a101":"code","0fea3e4b":"code","d5bcbb6e":"code","ae94a03b":"code","e39af709":"code","1c3f20e0":"code","5d0417f2":"code","6a47596f":"code","99177046":"code","14ab927c":"code","f2adca86":"code","29dea3d1":"code","f7f1d666":"code","1635fbf9":"code","ea7acc29":"code","ad310283":"code","a530486c":"code","fa87ba35":"code","b9b07da9":"code","843280f5":"code","11fe2be7":"code","127e57f1":"code","cb3fc3b3":"code","e97926e2":"code","686cf40b":"markdown","fffb4e55":"markdown","deae816a":"markdown","19430b5b":"markdown","a3855492":"markdown","e380ebd5":"markdown","4c7df213":"markdown","67ecf48e":"markdown","d0551858":"markdown","b8413828":"markdown","62feefd7":"markdown","0d70a9f6":"markdown","99dbf04f":"markdown","fefb1474":"markdown","8f0ae545":"markdown","e50f57b3":"markdown","73831c19":"markdown","493f0cf0":"markdown","0ef338ed":"markdown","7a072682":"markdown","90df05f6":"markdown","2da5ece9":"markdown","e1ef07b0":"markdown"},"source":{"e9bf063a":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","3dcbf14b":"train_data = pd.read_csv(\"\/kaggle\/input\/cap-4611-2021-fall-assignment-02\/train.csv\", low_memory=False)\ntest_data =  pd.read_csv(\"\/kaggle\/input\/cap-4611-2021-fall-assignment-02\/eval.csv\", low_memory=False)\nsample_submission =  pd.read_csv(\"\/kaggle\/input\/cap-4611-2021-fall-assignment-02\/sample_submission.csv\", low_memory=False)","23c2a101":"train_data.describe()","0fea3e4b":"train_data.head()","d5bcbb6e":"unknownFeature = train_data['no_descriptors']\ntrain_data = train_data.drop(['id','title','no_descriptors'],axis=1)\ntrain_data.describe()\ntrain_data.columns\n\nfor i in train_data.columns:\n    print(\"Train Data Column {}: {} is null\".format(i, train_data[i].isnull().sum()))\ntrain_data.describe()\ntrain_data.info()\ntrain_data.columns","ae94a03b":"y = train_data['esrb_rating']\ntrain_data = train_data.drop('esrb_rating',axis=1)","e39af709":"import matplotlib.pyplot as plt\n\ncolumns = []\nfor i in train_data.columns:\n    columns.append(i)\nfor i in columns:\n    plt.bar(y,train_data[i])\n    plt.xlabel(\"ESRB RATING\")\n    plt.ylabel(i)\n    plt.show()","1c3f20e0":"plt.bar(y,unknownFeature)","5d0417f2":"train_data = train_data.drop(['use_of_drugs_and_alcohol','mild_fantasy_violence','crude_humor','alcohol_reference','console'],axis=1)\nprint(train_data.columns)","6a47596f":"columns = []\nfor i in train_data.columns:\n    columns.append(i)\nfor i in columns:\n    plt.bar(y,train_data[i])\n    plt.xlabel(\"ESRB RATING\")\n    plt.ylabel(i)\n    plt.show()","99177046":"train_data_copy = train_data\ntrain_data = train_data.drop(['drug_reference', 'fantasy_violence', 'intense_violence', 'language',\n       'lyrics', 'mature_humor', 'mild_blood', 'mild_cartoon_violence',\n       'mild_language', 'mild_lyrics','mild_violence', 'nudity', 'partial_nudity','simulated_gambling', 'strong_janguage',\n       'strong_sexual_content', 'suggestive_themes', 'use_of_alcohol',\n       'violence'],axis=1)\n","14ab927c":"columns = []\nfor i in train_data.columns:\n    columns.append(i)\nfor i in columns:\n    plt.bar(y,train_data[i])\n    plt.xlabel(\"ESRB RATING\")\n    plt.ylabel(i)\n    plt.show()","f2adca86":"corr_data = train_data\ncorr_data['esrb_rating'] = y\ncorr_data.esrb_rating.unique()\ndef encodeESRB(corr_data,conversion):\n    if(conversion == 0):\n        for i in range(len(corr_data['esrb_rating'])):\n            if(corr_data['esrb_rating'][i] == 'E'):\n                corr_data['esrb_rating'][i] = 0\n            if(corr_data['esrb_rating'][i] == 'ET'):\n                corr_data['esrb_rating'][i] = 1\n            if(corr_data['esrb_rating'][i] == 'T'):\n                corr_data['esrb_rating'][i] = 2\n            if(corr_data['esrb_rating'][i] == 'M'):\n                corr_data['esrb_rating'][i] = 3\n    if(conversion == 1):\n        for i in range(len(corr_data['esrb_rating'])):\n            if(corr_data['esrb_rating'][i] == 0):\n                corr_data['esrb_rating'][i] = 'E'\n            if(corr_data['esrb_rating'][i] == 1):\n                corr_data['esrb_rating'][i] = 'ET'\n            if(corr_data['esrb_rating'][i] == 2):\n                corr_data['esrb_rating'][i] = 'T'\n            if(corr_data['esrb_rating'][i] == 3):\n                corr_data['esrb_rating'][i] = 'M'\n    return corr_data\ncorr_data = encodeESRB(corr_data,0)\ncorr_data.corr()","29dea3d1":"from sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.model_selection import RandomizedSearchCV\nfrom sklearn.model_selection import GridSearchCV\nimport xgboost as xgb\n\nrs = 123\ny = corr_data['esrb_rating'].astype('int')\nX = corr_data.drop(['esrb_rating'],axis=1)\nX_train, X_test, y_train, y_test = train_test_split(X, y, train_size = .80, random_state=rs)\ntest_data_predicted = test_data.drop(['id','console','no_descriptors','use_of_drugs_and_alcohol','mild_fantasy_violence','crude_humor','alcohol_reference','console'],axis=1)","f7f1d666":"LRParams = {\n    'max_iter': [50,100,150,200,250,300],\n    'C':[.1,.25,.33,.5,.66,.75,1,1.25,1.5]\n}\ndef LogisticRegressionRandom(LRParams,X_train,y_train,X_test,y_test,weight):\n    cv = [3,5,10,15,25]\n    predictions = []\n    distdf = []\n    testPredictions = []\n    if(weight == 'balanced'):\n        LR = LogisticRegression(random_state=rs,class_weight=weight,solver='liblinear')\n    else:\n        LR = LogisticRegression(random_state=rs,solver='liblinear')\n    for i in cv:\n        randomLR = RandomizedSearchCV(\n            estimator=LR,\n            param_distributions=LRParams,\n            n_iter=50,\n            scoring='accuracy',\n            refit=True,\n            cv=i,\n            return_train_score=True)\n        randomLR.fit(X_train,y_train)\n        print(randomLR.best_score_)\n        print(randomLR.best_estimator_)\n        predictions.append(randomLR.predict(X_test))\n        testPredictions.append(randomLR.predict(test_data_predicted))\n    distdf = []\n    for i in predictions:\n        distdf.append(accuracy_score(y_test,i))\n    distribution = pd.DataFrame({'Logistic Regression':distdf})\n    return (distribution , predictions, testPredictions)\n# (LGdistB, LGPredB) = LogisticRegressionRandom(LRParams,X_train,y_train,X_test,y_test,'balanced')\n# print(LGdistB.describe())\n# (LGdistN, LGPredN) = LogisticRegressionRandom(LRParams,X_train,y_train,X_test,y_test,'None')\n# print(LGdistN.describe())","1635fbf9":"\nSVCParams = {\n    'cache_size': [50,100,150,200,250,300,400,500],\n    'C':[.1,.25,.33,.5,.66,.75,1,1.25,1.5]\n}\ndef SVCRandom(SVCParams,X_train,y_train,X_test,y_test,weight):\n    cv = [3,5,10,15,25]\n    predictions = []\n    distdf = []\n    testPredictions = []\n    if(weight == 'balanced'):\n        SVCRand = SVC(random_state=rs,class_weight=weight)\n    else:\n        SVCRand = SVC(random_state=rs)\n    for i in cv:\n        randomSVC = RandomizedSearchCV(\n            estimator=SVCRand,\n            param_distributions=SVCParams,\n            n_iter=50,\n            scoring='accuracy',\n            refit=True,\n            cv=i,\n            return_train_score=True)\n        randomSVC.fit(X_train,y_train)\n        print(randomSVC.best_score_)\n        print(randomSVC.best_estimator_)\n        predictions.append(randomSVC.predict(X_test))\n        testPredictions.append(randomSVC.predict(test_data_predicted))\n    distdf = []\n    for i in predictions:\n        distdf.append(accuracy_score(y_test,i))\n    distribution = pd.DataFrame({'SVC':distdf})\n    return (distribution , predictions, testPredictions)   \n# (SVCdistB,SVCpredB) = SVCRandom(SVCParams,X_train,y_train,X_test,y_test,'balanced')\n# print(SVCdistB.describe())\n# (SVCdistN,SVCpredN) = SVCRandom(SVCParams,X_train,y_train,X_test,y_test,'None')\n# print(SVCdistN.describe())","ea7acc29":"DTCParams = {\n    'max_features': ['auto','sqrt'],\n    'splitter':['best','random'],\n    'criterion':['gini','entropy'],\n    'max_depth': list(range(5,100))\n}\ndef DTCRandom(DTCParams,X_train,y_train,X_test,y_test,weight):\n    cv = [3,5,10,15,25]\n    predictions = []\n    distdf = []\n    testPredictions = []\n    if(weight == 'balanced'):\n        DTCRand = DecisionTreeClassifier(random_state=rs,class_weight=weight)\n    else:\n        DTCRand = DecisionTreeClassifier(random_state=rs)\n    for i in cv:\n        randomDTC = RandomizedSearchCV(\n            estimator=DTCRand,\n            param_distributions=DTCParams,\n            n_iter=250,\n            scoring='accuracy',\n            refit=True,\n            cv=i,\n            return_train_score=True)\n        randomDTC.fit(X_train,y_train)\n        print(randomDTC.best_score_)\n        print(randomDTC.best_estimator_)\n        predictions.append(randomDTC.predict(X_test))\n        testPredictions.append(randomDTC.predict(test_data_predicted))\n    distdf = []\n    for i in predictions:\n        distdf.append(accuracy_score(y_test,i))\n    distribution = pd.DataFrame({'DTC':distdf})\n    return (distribution , predictions, testPredictions)   \n# (DTCdistB,DTCpredB) = DTCRandom(DTCParams,X_train,y_train,X_test,y_test,'balanced')\n# print(DTCdistB.describe())\n# (DTCdistN,DTCpredN) = DTCRandom(DTCParams,X_train,y_train,X_test,y_test,'None')\n# print(DTCdistN.describe())","ad310283":"RFCParams = {\n    'max_features': ['auto','sqrt'],\n    'criterion':['gini','entropy'],\n    'max_depth': list(range(5,100))\n}\ndef RFCRandom(RFCParams,X_train,y_train,X_test,y_test,weight):\n    cv = [3,5,10,15,25]\n    predictions = []\n    distdf = []\n    testPredictions = []\n    if(weight == 'balanced'):\n        RFCRand = RandomForestClassifier(random_state=rs,class_weight=weight)\n    else:\n        RFCRand = RandomForestClassifier(random_state=rs)\n    for i in cv:\n        randomRFC = RandomizedSearchCV(\n            estimator=RFCRand,\n            param_distributions=RFCParams,\n            n_iter=25,\n            scoring='accuracy',\n            refit=True,\n            cv=i,\n            return_train_score=True)\n        randomRFC.fit(X_train,y_train)\n        print(randomRFC.best_score_)\n        print(randomRFC.best_estimator_)\n        predictions.append(randomRFC.predict(X_test))\n        testPredictions.append(randomRFC.predict(test_data_predicted))\n    distdf = []\n    for i in predictions:\n        distdf.append(accuracy_score(y_test,i))\n    distribution = pd.DataFrame({'RFC':distdf})\n    return (distribution , predictions, testPredictions)   \n# (RFCdistB,RFCpredB) = RFCRandom(RFCParams,X_train,y_train,X_test,y_test,'balanced')\n# print(RFCdistB.describe())\n# (RFCdistN,RFCpredN) = RFCRandom(RFCParams,X_train,y_train,X_test,y_test,'None')\n# print(RFCdistN.describe())","a530486c":"KNNParams = {\n    'algorithm': ['auto','ball_tree','kd_tree','brute'],\n    'n_neighbors': list(range(1,50))\n}\ndef KNNRandom(KNNParams,X_train,y_train,X_test,y_test,weight):\n    cv = [3,5,10,15,25]\n    predictions = []\n    distdf = []\n    testPredictions = []\n    if(weight == 'uniform'):\n        KNNRand = KNeighborsClassifier(weights=weight)\n    else:\n        KNNRand = KNeighborsClassifier(weights=weight)\n    for i in cv:\n        randomKNN = RandomizedSearchCV(\n            estimator=KNNRand,\n            param_distributions=KNNParams,\n            n_iter=50,\n            scoring='accuracy',\n            refit=True,\n            cv=i,\n            return_train_score=True)\n        randomKNN.fit(X_train,y_train)\n        print(randomKNN.best_score_)\n        print(randomKNN.best_estimator_)\n        predictions.append(randomKNN.predict(X_test))\n        testPredictions.append(randomKNN.predict(test_data_predicted))\n    distdf = []\n    for i in predictions:\n        distdf.append(accuracy_score(y_test,i))\n    distribution = pd.DataFrame({'KNN':distdf})\n    return (distribution , predictions, testPredictions)   \n# (KNNdistB,KNNpredB) = KNNRandom(KNNParams,X_train,y_train,X_test,y_test,'uniform')\n# print(KNNdistB.describe())\n# (KNNdistN,KNNpredN) = KNNRandom(KNNParams,X_train,y_train,X_test,y_test,'distance')\n# print(KNNdistN.describe())","fa87ba35":"XGBParams = {\n    'n_estimators':list(range(5,25)),\n    'max_depth': list(range(5,100)),\n    'booster': ['gbtree','gblinear','dart']\n}\ndef XGBRandom(XGBParams,X_train,y_train,X_test,y_test):\n    cv = [3,5,10,15,25]\n    predictions = []\n    distdf = []\n    testPredictions = []\n    XGBRand = xgb.XGBClassifier(eval_metric='mlogloss', use_label_encoder=False, random_state=rs, verbosity=0)\n\n    for i in cv:\n        randomXGB = RandomizedSearchCV(\n            estimator=XGBRand,\n            param_distributions=XGBParams,\n            n_iter=50,\n            scoring='accuracy',\n            refit=True,\n            cv=i,\n            return_train_score=True)\n        randomXGB.fit(X_train,y_train)\n        print(randomXGB.best_score_)\n        print(randomXGB.best_estimator_)\n        predictions.append(randomXGB.predict(X_test))\n        testPredictions.append(randomXGB.predict(test_data_predicted))\n    distdf = []\n    for i in predictions:\n        distdf.append(accuracy_score(y_test,i))\n    distribution = pd.DataFrame({'XGB':distdf})\n    return (distribution , predictions, testPredictions)   \n# (XGBdistB,XGBpredB) = XGBRandom(XGBParams,X_train,y_train,X_test,y_test)\n# print(XGBdistB.describe())","b9b07da9":"# def fitPredict(X_train,y_train,X_test,y_test):\n#     arr = ['Logistic Regression','Support Vector Machine','Decision Tree','Random Forest','K Nearest Neighbor',\"XGBoost\"]\n#     LR.fit(X_train,y_train)\n#     SVC.fit(X_train,y_train)\n#     DTC.fit(X_train,y_train)\n#     RFC.fit(X_train,y_train)\n#     KNN.fit(X_train,y_train)\n#     XGB.fit(X_train,y_train)\n#     predictions = []\n#     predictions.append(LR.predict(X_test))\n#     predictions.append(SVC.predict(X_test))\n#     predictions.append(DTC.predict(X_test))\n#     predictions.append(RFC.predict(X_test))\n#     predictions.append(KNN.predict(X_test))\n#     predictions.append(XGB.predict(X_test))\n#     for i in range(len(predictions)):\n#         print(\"{} accuracy:{}\".format(arr[i],accuracy_score(y_test,predictions[i])))\n# fitPredict(X_train,y_train,X_test,y_test)","843280f5":"X = train_data_copy\nX_train, X_test, y_train, y_test = train_test_split(X, y, train_size = .80, random_state=rs)\n(LGdistB, LGpredB, LGTPB) = LogisticRegressionRandom(LRParams,X_train,y_train,X_test,y_test,'balanced')\n(SVCdistB,SVCpredB, SVCTPB) = SVCRandom(SVCParams,X_train,y_train,X_test,y_test,'balanced')\n(DTCdistB,DTCpredB, DTCTPB) = DTCRandom(DTCParams,X_train,y_train,X_test,y_test,'balanced')\n(RFCdistB,RFCpredB, RFCTPB) = RFCRandom(RFCParams,X_train,y_train,X_test,y_test,'balanced')\n(KNNdistU,KNNpredU, KNNTPU) = KNNRandom(KNNParams,X_train,y_train,X_test,y_test,'uniform')\nprint('Logistic Regression\\n{}'.format(LGdistB.describe()))\nprint('SVC\\n{}'.format(SVCdistB.describe()))\nprint('Decision Tree\\n{}'.format(DTCdistB.describe()))\nprint('Random Forest\\n{}'.format(RFCdistB.describe()))\nprint('K Nearest Neighbor\\n{}'.format(KNNdistU.describe()))\n(LGdistN, LGpredN, LGTPN) = LogisticRegressionRandom(LRParams,X_train,y_train,X_test,y_test,'None')\n(SVCdistN,SVCpredN, SVCTPN) = SVCRandom(SVCParams,X_train,y_train,X_test,y_test,'None')\n(DTCdistN,DTCpredN, DTCTPN) = DTCRandom(DTCParams,X_train,y_train,X_test,y_test,'None')\n(RFCdistN,RFCpredN, RFCTPN) = RFCRandom(RFCParams,X_train,y_train,X_test,y_test,'None')\n(KNNdistD,KNNpredD, KNNTPD) = KNNRandom(KNNParams,X_train,y_train,X_test,y_test,'distance')\n(XGBdist,XGBpred, XGBTP) = XGBRandom(XGBParams,X_train,y_train,X_test,y_test)\nprint('Logistic Regression\\n{}'.format(LGdistN.describe()))\nprint('SVC\\n{}'.format(SVCdistN.describe()))\nprint('Decision Tree\\n{}'.format(DTCdistN.describe()))\nprint('Random Forest\\n{}'.format(RFCdistN.describe()))\nprint('K Nearest Neighbor\\n{}'.format(KNNdistD.describe()))\nprint('XGBoost\\n{}'.format(XGBdist.describe()))","11fe2be7":"print('\\nLogistic Regression Accuracy\\n')\nfor i in LGpredB:\n    print('\\nLogistic Regression Accuracy balanced: {}'.format(accuracy_score(y_test, i)))\nfor i in LGpredN:\n    print('\\nLogistic Regression Accuracy : {}'.format(accuracy_score(y_test, i)))\nprint('\\nSupport Vector Accuracy\\n')\nfor i in SVCpredB:\n    print('\\nSVC Accuracy balanced: {}'.format(accuracy_score(y_test, i)))\nfor i in SVCpredN:\n    print('\\nSVC Accuracy : {}'.format(accuracy_score(y_test, i)))\nprint('\\nDecision Tree Accuracy\\n')\nfor i in DTCpredB:\n    print('\\nDecision Tree Accuracy balanced: {}'.format(accuracy_score(y_test, i)))\nfor i in DTCpredN:\n    print('\\nDecision Tree Accuracy : {}'.format(accuracy_score(y_test, i)))\nprint('\\nRandom Forest Accuracy\\n')\nfor i in RFCpredB:\n    print('\\nRandom Forest Accuracy balanced: {}'.format(accuracy_score(y_test, i)))\nfor i in RFCpredN:\n    print('\\nRandom Forest Accuracy: {}'.format(accuracy_score(y_test, i)))\nprint('\\nK Nearest Neighbor Accuracy\\n')\nfor i in KNNpredU:\n    print('\\nRandom Forest Accuracy uniform: {}'.format(accuracy_score(y_test, i)))\nfor i in KNNpredD:\n    print('\\nRandom Forest Accuracy distance: {}'.format(accuracy_score(y_test, i)))\nprint(\"\\nXGBoost Accuracies\\n\")\nfor i in XGBpred:\n    print('XGBoost Accuracy: {}'.format(accuracy_score(y_test, i)))\n","127e57f1":"test_data_predicted = test_data.drop(['id','console','no_descriptors','use_of_drugs_and_alcohol','mild_fantasy_violence','crude_humor','alcohol_reference','console'],axis=1)","cb3fc3b3":"#Current Winner with XGB accuracy of ~.85ish in X_test,y_test\nXGB = xgb.XGBClassifier(eval_metric='mlogloss', use_label_encoder=False, random_state=rs, verbosity=0)\nXGB.fit(X_train,y_train)\nunencodedRating = XGB.predict(test_data_predicted)\n# models = [LGTPB,SVCTPB,DTCTPB,RFCTPB,KNNTPU,LGTPN,SVCTPN,DTCTPN,RFCTPN,KNNTPD,XGTPB]\n#Taken from previous run model (Time constraint)\n# XGB = xgb.XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n#               colsample_bynode=1, colsample_bytree=1, eval_metric='mlogloss',\n#               gamma=0, gpu_id=-1, importance_type='gain',\n#               interaction_constraints='', learning_rate=0.300000012,\n#               max_delta_step=0, max_depth=32, min_child_weight=1,\n#               monotone_constraints='()', n_estimators=7, n_jobs=2,\n#               num_parallel_tree=1, objective='multi:softprob', random_state=123,\n#               reg_alpha=0, reg_lambda=1, scale_pos_weight=None, subsample=1,\n#               tree_method='exact', use_label_encoder=False,\n#               validate_parameters=1, verbosity=0)\n# SVCTest = SVC(C=1.25, cache_size=250, random_state=123)\n# RFCTest = RandomForestClassifier(criterion='entropy', max_depth=43, max_features='sqrt',\n#                        random_state=123)\n# XGB.fit(X_train,y_train)\n# SVCTest.fit(X_train,y_train)\n# RFCTest.fit(X_train,y_train)\n# unencodedRating = XGB.predict(test_data_predicted)\n# unencodedRating = SVCTest.predict(test_data_predicted)\nunencodedRating = XGB.predict(test_data_predicted)\nunencodedRating = pd.DataFrame({'id':sample_submission.id, 'esrb_rating':unencodedRating})\nencodedRating = encodeESRB(unencodedRating,1)","e97926e2":"encodedRating.to_csv('submission.csv',index=False)\nprint(encodedRating)","686cf40b":"# !!! Describe Data !!!","fffb4e55":"# !!! EDA !!!","deae816a":"## Why no distribution graph?\nWhether I am saving the wrong data, or just dont understand, 5 samples with miniscule changes between each iteration was not varied enough to show a distribution. ","19430b5b":"## Much better accuracy! Lets try and submit once without Hyperparameter tuning\n### Edit: XGB had best submission of .87552 accuracy without hyperparameter tuning\n","a3855492":"## Another Gamble! Lets only pick features that correlate to 2 ratings. This would leave no indicators for E or everyone. But lets see what happens, But I will save the current setup ","e380ebd5":"### This was the bad model! But good insight!","4c7df213":"# !!! Building Models and Validation !!!","67ecf48e":"## id, title removed because not relevent. No descriptors at this point have no reference to importance, so for now I will save and drop that column","d0551858":"## None of the features are correlated with each other which is a good thing! We want features that are distinct to determine and output","b8413828":"## Missing Values\nNone\n\n## Outliers\nRemoving features where all ratings had a criteria of including a feature is considered an 'outlier' to me, though not in a direct sense. In a decision tree, those features would be difficult to determine which direction to go and causes more uncertainty (high entropy). \n\n## Data transformation\nChanging esrb rating to 0-3 was my form of encoding the data to be used in predicting a model. Other then that, no data transformation or feature engineering was done (combining groups, segmenting data based on sets of values etc.)\n\n","62feefd7":"## No Null or Missing Values","0d70a9f6":"# !!! Output !!!","99dbf04f":"## Now lets see correlation between remaining features and rating","fefb1474":"# !!! Load Data !!!","8f0ae545":"## Prep test data","e50f57b3":"## Old Function before hyperparameter tuning","73831c19":"## Now lets try and determine correlations, first, seperate out the the y","493f0cf0":"## Now we will partition away the y column value","0ef338ed":"## Determine relevent columns","7a072682":"## Now the no_descriptor feature hist","90df05f6":"### Edit: Turns out not great, between 48 - 52% accuracy in test data, will try new set, but will leave info here as proof of lack of concept ","2da5ece9":"## Previous best score!\nBecause I was a dumb dumb, I have to submit the previous best run score, while loading new models in the process... My fault :( But I did output the classifier criteria used above in the previous submitted version.","e1ef07b0":"## This might be a gamble, but any feature that describes any of the four features will be dropped, slightly reducing our features"}}