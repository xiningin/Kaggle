{"cell_type":{"cba5afee":"code","c6f3ddce":"code","a55c9dae":"code","3d64c62c":"code","65e1ea75":"code","e2f939dd":"code","83056091":"code","0d2c0b48":"markdown","5972be14":"markdown","561ed867":"markdown","c4c517cb":"markdown","bf35c85f":"markdown","38d22d7c":"markdown","56d4650b":"markdown"},"source":{"cba5afee":"# data analysis libraries:\nimport numpy as np\nimport pandas as pd\nfrom pandas.api.types import CategoricalDtype \n# data visualization libraries:\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n# to ignore warnings:\nimport warnings\nwarnings.filterwarnings('ignore')\n#timer library\nfrom contextlib import contextmanager\nimport time\n#ML libraries\nfrom sklearn.model_selection import train_test_split, GridSearchCV\nfrom sklearn.metrics import accuracy_score\nfrom catboost import CatBoostClassifier","c6f3ddce":"@contextmanager\ndef timer(title):\n    t0 = time.time()\n    yield\n    print(\"{} - done in {:.0f}s\".format(title, time.time() - t0))","a55c9dae":"def spilit_and_preprocessing():\n    print(\"Data Preprocessing Has Been Started\" \"\\n\")\n    data= pd.read_csv('Churn_Modelling.csv')\n    df= data.copy()\n    df= df.drop(['RowNumber','Surname'], axis=1)\n    return df","3d64c62c":"def feature_engineering(df):\n    \n    print(\"Feature Engineering Process Has Been Started\" \"\\n\")\n    agg_list = {\"CreditScore\":[\"mean\",'std','min','max'],\n                \"Tenure\":[\"mean\",'min', 'max'],\n                \"Balance\":[\"median\",'std','min','max'],\n                \"NumOfProducts\":[\"mean\",'min', 'max'],\n                \"EstimatedSalary\":[\"median\",'std','min','max'],\n                \"Age\":[\"mean\",'min','max'] }\n    \n    df_gender = df.groupby(\"Gender\").agg(agg_list)\n    df_gender.columns = pd.Index([col[0] + \"_\" + col[1].upper() + '_Gender' for col in df_gender.columns.tolist()])\n    df_gender=df_gender.reset_index()\n\n    df_geography = df.groupby(\"Geography\").agg(agg_list)\n    df_geography.columns = pd.Index([col[0] + \"_\" + col[1].upper() + '_Geography' for col in df_geography.columns.tolist()])\n    df_geography=df_geography.reset_index()\n \n    df_HasCrCard = df.groupby(\"HasCrCard\").agg(agg_list)\n    df_HasCrCard.columns = pd.Index([col[0] + \"_\" + col[1].upper() + '_HasCrCard' for col in df_HasCrCard.columns.tolist()])\n    df_HasCrCard=df_HasCrCard.reset_index()\n\n    df_IsActiveMember = df.groupby(\"IsActiveMember\").agg(agg_list)\n    df_IsActiveMember.columns = pd.Index([col[0] + \"_\" + col[1].upper() + '_IsActiveMember' for col in df_IsActiveMember.columns.tolist()])\n    df_IsActiveMember=df_IsActiveMember.reset_index()\n\n    df = df.merge(df_gender, how='left', on= 'Gender')\n    df = df.merge(df_geography, how='left', on= 'Geography')\n    df = df.merge(df_HasCrCard, how='left', on= 'HasCrCard')\n    df = df.merge(df_IsActiveMember, how='left', on= 'IsActiveMember')\n    print(\"Categorical Features Aggregations Process has been finished\" \"\\n\")\n    df['HasCrCard_2'] = df['HasCrCard'].replace(to_replace = 0, value = -1)\n    df['IsActiveMember_2'] = df['IsActiveMember'].replace(to_replace = 0, value = -1)\n    df['Age_Balance_Ratio'] = df.Balance \/ df.Age\n    df['Age_Cross_NumOfProducts'] = df.Age * df.NumOfProducts\n    df['Score_Age_Ratio'] = df.CreditScore \/ df.Age\n    df['Age_Salary_Ratio'] = df.EstimatedSalary \/ df.Age\n    df['Balance_Salary_Ratio'] = df.Balance \/ df.EstimatedSalary\n    df['Score_Balance_Ratio'] = df.Balance \/ df.CreditScore\n    df['Score_Salary_Ratio'] = df.CreditScore \/ df.EstimatedSalary\n    df['Score_HasCrCard_2'] = df.CreditScore * df.HasCrCard_2\n    df['Score_Cross_NumOfProducts'] = df.CreditScore * df.NumOfProducts\n    df['Balance_NumOfProducts_Ratio'] = df.CreditScore \/ df.NumOfProducts\n    df['Age_Cross_HasCrCard_2'] = df.Age * df.HasCrCard_2\n    df['Balance_Cross_HasCrCard_2'] = df.Balance * df.HasCrCard_2\n    df['Age_Cross_IsActiveMember_2'] = df.Age * df.IsActiveMember_2\n    df['Score_Cross_IsActiveMember_2'] = df.CreditScore * df.IsActiveMember_2\n    df['Balance_Cross_IsActiveMember_2'] = df.Balance * df.IsActiveMember_2\n    df['Tenure_Cross_Balance'] = df.Balance * df.Tenure\n    df['Tenure_Cross_NumOfProducts'] = df.Tenure * df.NumOfProducts\n    df['Score_Cross_Tenure'] = df.Tenure * df.CreditScore\n    df['Tenure_Salary_Ratio'] = df.Tenure \/ df.EstimatedSalary\n    \n    df= df.drop(['IsActiveMember_2', 'HasCrCard_2'], axis=1)    \n    print(\"Cross Accounts Features Process has been finished\" \"\\n\")    \n    \n    df = pd.get_dummies(df, columns = [\"Gender\"], prefix = [\"Gender\"], drop_first= True)\n    df = pd.get_dummies(df, columns = [\"Geography\"], prefix = [\"Geography\"], drop_first= True)\n    df = pd.get_dummies(df, columns = [\"HasCrCard\"], prefix = [\"HasCrCard\"], drop_first= True)\n    df = pd.get_dummies(df, columns = [\"IsActiveMember\"], prefix = [\"IsActiveMember\"], drop_first= True)    \n    print(\"Label Encoding Process has been finished\" \"\\n\")    \n    return df","65e1ea75":"def training_model_and_predicting(df):\n    print(\"Model fitting process has been started\" \"\\n\")    \n    # Main test - train data splitting\n    target_1=  df[df['Exited']==1]\n    test_1= target_1.sample(400)\n    target_0=  df[df['Exited']==0]\n    test_0= target_0.sample(1600)\n    test = pd.concat([test_1, test_0], axis=0)\n    df = df.drop(test.index)\n    test_data_for_final=test[['CustomerId', 'Exited']]# this is for random sample success score\n    test= test.drop('Exited', axis=1)    \n    # train test spilit on train data \n    predictors = df.drop(['Exited', 'CustomerId'], axis=1)\n    target = df[\"Exited\"]\n    x_train, x_val, y_train, y_val = train_test_split(predictors, target, test_size = 0.20, random_state = 0)\n        \n    \n    # Tuned model fitting with optimum parameters\n    catb = CatBoostClassifier(iterations = 300, learning_rate = 0.04, depth = 5)\n    catb_tuned = catb.fit(x_train, y_train)\n    y_pred = catb_tuned.predict(x_val)\n    acc_catb = round(accuracy_score(y_pred, y_val) * 100, 2)\n    print('Train Data Success Score: %' + str(acc_catb))     \n    \n    #set ids as CustomerId and predict Exited# fitting final model with the best model \n    ids = test['CustomerId']\n    predictions = catb_tuned.predict(test.drop('CustomerId', axis=1))\n    \n    #set the output as a dataframe and convert to csv file named submission.csv\n    output = pd.DataFrame({ 'CustomerId' : ids, 'Exited': predictions })\n    output.to_csv('submission.csv', index=False)    \n    \n    #Finally, I test the success of the model with 400 samples randomly selected from the test data.\n    test_data_last = test_data_for_final.sample(400)\n    testing = test_data_last.merge(output, how= 'left', on = 'CustomerId')\n    testing.columns= ['CustomerId','Exited_test','Exited_output']\n    testing['toplam'] = testing.Exited_test + testing.Exited_output\n    a = testing[testing.toplam!=1].count().iloc[0]\n    b= testing.count().iloc[0]    \n    print('Random Sample Success Score: %' + str(a\/b*100))    \n    print(\"Submission file has been created\")\n    \n    #feature importances table\n    feature_imp = pd.Series(catb_tuned.feature_importances_,\n                        index=x_train.columns).sort_values(ascending=False).iloc[:20]\n\n    sns.barplot(x=feature_imp, y=feature_imp.index)\n    plt.xlabel('Feature importances scores')\n    plt.ylabel('Features')\n    plt.title(\"Feature importances\")\n    plt.figure(figsize=(15,20))\n    plt.tight_layout()\n    plt.savefig('Catboost_importances01.png')","e2f939dd":"def main():    \n    with timer(\"Preprocessing Time\"):\n        df = spilit_and_preprocessing()        \n    with timer(\"Feature Engineering Time\"):\n        df = feature_engineering(df)                \n    with timer(\"Modelling Time\"):\n        training_model_and_predicting(df)\n    ","83056091":"if __name__ == \"__main__\":\n    with timer(\"Full model run\"):\n        main()","0d2c0b48":"As a result of data examinations; it was observed that there were no outlier values or missing values in the data. After that, feature engineering operations were started directly.\n\nSome addition operations related to numerical variables were made on the basis of categorical variables.\n\nSome crossing and ratio operations are done on numerical variables.\n\nFinally, encoding was performed in categorical variables.","5972be14":"# 4. Feature Engineering","561ed867":"The competition data are not divided into test and train. To test the success of the model, the test data must first be separated from the main data.\n\nI determined the ratio of the number of observations of the test data to the number of observations of the main table as 1\/5.\n\nCreated the test data taking into account the proportions of the classes of the target variable in the main table.\n\nChose Catboost, one of the algorithms that gives the best results on this data.\n\nAlso tried to achieve the best result by doing hyperparameter optimization on the catboost algorithm for this job.\n\nI specified the prediction success of the algorithm on the train set as \"Train Data Success Score\".\n\nFinally, I used 400 samples randomly selected from test data to measure the success of the algorithm's estimation on test data. I mentioned this as \"Random Sample Success Score\".\n","c4c517cb":"# 1.Import Libraries","bf35c85f":"# 2. Timer Function","38d22d7c":"# 5. Machine Learning And Predictions","56d4650b":"# 3. Data Preprocessing"}}