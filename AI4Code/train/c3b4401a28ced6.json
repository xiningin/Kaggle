{"cell_type":{"a3a4b837":"code","25b900a6":"code","32475f9a":"code","c71df55b":"code","b1cec3de":"code","fa328405":"code","073ddcc3":"code","33c36d16":"code","68137292":"code","7daa1de7":"code","8e52f409":"code","6b711c59":"code","c5a0cd32":"code","e4b885d5":"code","f1f729db":"code","735bcb05":"code","67f8e600":"code","cd9449db":"code","05922863":"code","3b4d37c1":"code","ef75aa0d":"code","f49a9e10":"code","615f566e":"code","b3d569fa":"code","d3eabca5":"code","1bb73b5a":"code","2136cc72":"code","1e20b51c":"code","300e39b1":"code","214fe540":"code","c5849be0":"code","7568db9c":"code","ff61add2":"code","fa915aab":"code","61def939":"code","664992ff":"code","915730fe":"code","b6123312":"code","1ead1380":"code","dca92e6a":"code","9d9d65a5":"code","52c81936":"code","1918ad3c":"code","dfee5d34":"code","4686dd8c":"code","4478d694":"code","a5a7d45d":"code","bd12d064":"markdown","6ef143b0":"markdown","029bc7b6":"markdown","a08ecbed":"markdown","be44b9ea":"markdown","b7c8d76b":"markdown","ffea24f9":"markdown","dcd3085e":"markdown","6681c289":"markdown","aba250b2":"markdown","0c478c2f":"markdown","048557dd":"markdown","68fae1dd":"markdown","06b4e205":"markdown","0f26707f":"markdown","454df7a0":"markdown","ca1e1c26":"markdown","90ea7205":"markdown","6e51b4ec":"markdown","98e4b87c":"markdown","3fb77142":"markdown","eb4a4c84":"markdown","0f3ec8be":"markdown","fe7d72be":"markdown","5436e119":"markdown","bc21744f":"markdown","cf179150":"markdown","47117416":"markdown","2344998d":"markdown","19298dd2":"markdown","1bd4064e":"markdown","527f2f2a":"markdown"},"source":{"a3a4b837":"import numpy as np\nimport pandas as pd\nfrom nltk.classify import NaiveBayesClassifier\nfrom nltk.corpus import subjectivity\nfrom nltk.sentiment import SentimentAnalyzer\nfrom nltk.sentiment.util import *\nimport matplotlib.pyplot as mlpt\nimport math\nimport keras\nfrom keras.models import Sequential\nfrom keras.layers import Dense\nfrom keras.metrics import Accuracy\nfrom keras.layers import LSTM\nfrom keras.layers import Dropout\nfrom keras.layers import *\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.metrics import mean_absolute_error\nfrom sklearn.model_selection import train_test_split\nfrom keras.callbacks import EarlyStopping","25b900a6":"!pip3 install tweepy\nimport tweepy\nimport csv\nimport pandas as pd\nimport random\nimport numpy as np\nimport pandas as pd","32475f9a":"consumer_key    = '3jmA1BqasLHfItBXj3KnAIGFB'\nconsumer_secret = 'imyEeVTctFZuK62QHmL1I0AUAMudg5HKJDfkx0oR7oFbFinbvA'\n\naccess_token  = '265857263-pF1DRxgIcxUbxEEFtLwLODPzD3aMl6d4zOKlMnme'\naccess_token_secret = 'uUFoOOGeNJfOYD3atlcmPtaxxniXxQzAU4ESJLopA1lbC'\n\nauth = tweepy.OAuthHandler(consumer_key, consumer_secret)\nauth.set_access_token(access_token, access_token_secret)\napi = tweepy.API(auth,wait_on_rate_limit=True)","c71df55b":"fetch_tweets=tweepy.Cursor(api.user_timeline, screen_name='@sundarpichai', tweet_mode=\"extended\").items()\n# fetch_tweets=tweepy.Cursor(api.search, q=\"#YESBANK\",count=100000, lang =\"en\",since=\"2020-01-01\", tweet_mode=\"extended\").items()\ndata=pd.DataFrame(data=[[tweet_info.created_at.date(),tweet_info.full_text]for tweet_info in fetch_tweets],columns=['Date','Tweets'])","b1cec3de":"data.to_csv(\"Tweets.csv\")\ncdata=pd.DataFrame(columns=['Date','Tweets'])\ntotal=100\nindex=0\nfor index,row in data.iterrows():\n    stre=row[\"Tweets\"]\n    my_new_string = re.sub('[^ a-zA-Z0-9]', '', stre)\n    cdata.sort_index()\n    cdata.at[index,'Date'] = row[\"Date\"]\n    cdata.at[index,'Tweets'] = my_new_string\n    index=index+1\n#print(cdata.dtypes)","fa328405":"print(cdata, min(cdata['Date']))","073ddcc3":"ccdata=pd.DataFrame(columns=['Date','Tweets'])","33c36d16":"indx=0\nget_tweet=\"\"\nfor i in range(0,len(cdata)-1):\n    get_date=cdata.Date.iloc[i]\n    next_date=cdata.Date.iloc[i+1]\n    if(str(get_date)==str(next_date)):\n        get_tweet=get_tweet+cdata.Tweets.iloc[i]+\" \"\n    if(str(get_date)!=str(next_date)):\n        ccdata.at[indx,'Date'] = get_date\n        ccdata.at[indx,'Tweets'] = get_tweet\n        indx=indx+1\n        get_tweet=\" \"","68137292":"ccdata","7daa1de7":"\nread_stock_p=pd.read_csv('..\/input\/nsefinancedata\/stocks.csv')\nread_stock_p","8e52f409":"ccdata['Prices']=\"\"","6b711c59":"indx=0\nfor i in range (0,len(ccdata)):\n    for j in range (0,len(read_stock_p)):\n        get_tweet_date=ccdata.Date.iloc[i]\n        get_stock_date=read_stock_p.Date.iloc[j]\n        if(str(get_stock_date)==str(get_tweet_date)):\n            #print(get_stock_date,\" \",get_tweet_date)\n            ccdata.at[i,'Prices'] = int(read_stock_p.Close[j])\n            break","c5a0cd32":"print(ccdata)","e4b885d5":"mean=0\nsumm=0\ncount=0\nfor i in range(0,len(ccdata)):\n    if(ccdata.Prices.iloc[i]!=\"\"):\n        summ=summ+int(ccdata.Prices.iloc[i])\n        count=count+1\nmean=summ\/count\nfor i in range(0,len(ccdata)):\n    if(ccdata.Prices.iloc[i]==\"\"):\n        ccdata.Prices.iloc[i]=int(mean)","f1f729db":"ccdata","735bcb05":"ccdata['Prices'] = ccdata['Prices'].apply(np.int64)","67f8e600":"ccdata[\"Comp\"] = ''\nccdata[\"Negative\"] = ''\nccdata[\"Neutral\"] = ''\nccdata[\"Positive\"] = ''\nccdata","cd9449db":"import nltk\nnltk.download('vader_lexicon')","05922863":"from nltk.sentiment.vader import SentimentIntensityAnalyzer\nfrom nltk.sentiment.vader import SentimentIntensityAnalyzer\nimport unicodedata\nsentiment_i_a = SentimentIntensityAnalyzer()\nfor indexx, row in ccdata.T.iteritems():\n    try:\n        sentence_i = unicodedata.normalize('NFKD', ccdata.loc[indexx, 'Tweets'])\n        sentence_sentiment = sentiment_i_a.polarity_scores(sentence_i)\n        ccdata.at[indexx, 'Comp'] = sentence_sentiment['compound']\n        ccdata.at[indexx, 'Negative'] = sentence_sentiment['neg']\n        ccdata.at[indexx, 'Neutral'] = sentence_sentiment['neu']\n        ccdata.at[indexx, 'Positive'] = sentence_sentiment['pos']\n    except TypeError:\n        print (stocks_dataf.loc[indexx, 'Tweets'])\n        print (indexx)","3b4d37c1":"ccdata","ef75aa0d":"posi=0\nnega=0\nfor i in range (0,len(ccdata)):\n    get_val=ccdata.Comp[i]\n    if(float(get_val)<(0)):\n        nega=nega+1\n    if(float(get_val>(0))):\n        posi=posi+1\nposper=(posi\/(len(ccdata)))*100\nnegper=(nega\/(len(ccdata)))*100\nprint(\"% of positive tweets= \",posper)\nprint(\"% of negative tweets= \",negper)\narr=np.asarray([posper,negper], dtype=int)\nmlpt.pie(arr,labels=['positive','negative'])\nmlpt.plot()","f49a9e10":"df_=ccdata[['Date','Prices','Comp','Negative','Neutral','Positive']].copy()","615f566e":"df_","b3d569fa":"train_start_index = '0'\ntrain_end_index = '6'\ntest_start_index = '7'\ntest_end_index = '9'\ntrain = df_.loc[train_start_index : train_end_index]\ntest = df_.loc[test_start_index:test_end_index]","d3eabca5":"sentiment_score_list = []\nfor date, row in train.T.iteritems():\n    sentiment_score = np.asarray([df_.loc[date, 'Negative'],df_.loc[date, 'Positive']])\n    sentiment_score_list.append(sentiment_score)\nnumpy_df_train = np.asarray(sentiment_score_list)","1bb73b5a":"print(numpy_df_train)","2136cc72":"sentiment_score_list = []\nfor date, row in test.T.iteritems():\n    sentiment_score = np.asarray([df_.loc[date, 'Negative'],df_.loc[date, 'Positive']])\n    sentiment_score_list.append(sentiment_score)\nnumpy_df_test = np.asarray(sentiment_score_list)","1e20b51c":"print(numpy_df_test)","300e39b1":"y_train = pd.DataFrame(train['Prices'])\n#y_train=[91,91,91,92,91,92,91]\ny_test = pd.DataFrame(test['Prices'])\nprint(y_train)","214fe540":"!pip3 install treeinterpreter\nfrom treeinterpreter import treeinterpreter as ti\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.metrics import classification_report,confusion_matrix\n\nrf = RandomForestRegressor()\nrf.fit(numpy_df_train, y_train)","c5849be0":"prediction, bias, contributions = ti.predict(rf, numpy_df_test)","7568db9c":"print(prediction)","ff61add2":"import matplotlib.pyplot as plt","fa915aab":"idx=np.arange(int(test_start_index),int(test_end_index)+198)\npredictions_df_ = pd.DataFrame(data=prediction[0:], index = idx, columns=['Prices'])","61def939":"predictions_df_\n","664992ff":"ax = predictions_df_.rename(columns={\"Prices\": \"predicted_price\"}).plot(title='Random Forest predicted prices')#predicted value\nax.set_xlabel(\"Indexes\")\nax.set_ylabel(\"Stock Prices\")\nfig = y_test.rename(columns={\"Prices\": \"actual_price\"}).plot(ax = ax).get_figure()#actual value\nfig.savefig(\"random forest.png\")","915730fe":"from treeinterpreter import treeinterpreter as ti\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import classification_report,confusion_matrix\n\nreg = LinearRegression()\nreg.fit(numpy_df_train, y_train)","b6123312":"reg.predict(numpy_df_test)\n\n# reg.score(reg.predict(numpy_df_test), y_test)\n","1ead1380":"df = pd.read_csv('..\/input\/googlestockdata\/stocks.csv')","dca92e6a":"df.head(5)","9d9d65a5":"training_set = df.iloc[:800, 1:2].values\ntest_set = df.iloc[800:, 1:2].values","52c81936":"# Feature Scaling\nsc = MinMaxScaler(feature_range = (0, 1))\ntraining_set_scaled = sc.fit_transform(training_set)\n# Creating a data structure with 60 time-steps and 1 output\nX_train = []\ny_train = []\nfor i in range(60, 800):\n    X_train.append(training_set_scaled[i-60:i, 0])\n    y_train.append(training_set_scaled[i, 0])\nX_train, y_train = np.array(X_train), np.array(y_train)\nX_train = np.reshape(X_train, (X_train.shape[0], X_train.shape[1], 1))\n#(740, 60, 1)","1918ad3c":"model = Sequential()\n#Adding the first LSTM layer and some Dropout regularisation\nmodel.add(LSTM(units = 50, return_sequences = True, input_shape = (X_train.shape[1], 1)))\nmodel.add(Dropout(0.2))\n# Adding a second LSTM layer and some Dropout regularisation\nmodel.add(LSTM(units = 50, return_sequences = True))\nmodel.add(Dropout(0.2))\n# Adding a third LSTM layer and some Dropout regularisation\nmodel.add(LSTM(units = 50, return_sequences = True))\nmodel.add(Dropout(0.2))\n# Adding a fourth LSTM layer and some Dropout regularisation\nmodel.add(LSTM(units = 50))\nmodel.add(Dropout(0.2))\n# Adding the output layer\nmodel.add(Dense(units = 1))\n# Compiling the RNN\nmodel.compile(optimizer = 'adam', loss = 'mean_squared_error')\n# Fitting the RNN to the Training set\nhistory = model.fit(X_train, y_train, epochs = 50, batch_size = 32)","dfee5d34":"# Getting the predicted stock price of 2017\ndataset_train = df.iloc[:800, 1:2]\ndataset_test = df.iloc[800:, 1:2]\ndataset_total = pd.concat((dataset_train, dataset_test), axis = 0)\ninputs = dataset_total[len(dataset_total) - len(dataset_test) - 60:].values\ninputs = inputs.reshape(-1,1)\ninputs = sc.transform(inputs)\nX_test = []\n\nfor i in range(60, 519):\n    X_test.append(inputs[i-60:i, 0])\n    \nX_test = np.array(X_test)\nX_test = np.reshape(X_test, (X_test.shape[0], X_test.shape[1], 1))\nprint(X_test.shape)\n# (459, 60, 1)","4686dd8c":"predicted_stock_price = model.predict(X_test)\npredicted_stock_price = sc.inverse_transform(predicted_stock_price)","4478d694":"df.columns","a5a7d45d":"# Visualising the results\nplt.plot(df.loc[800:,'Date'],dataset_test.values, color = 'red', label = 'Real GOOGLE Stock Price')\nplt.plot(df.loc[800:,'Date'][:459],predicted_stock_price, color = 'blue', label = 'Predicted GOOGLE Stock Price')\nplt.xticks(np.arange(0,459,50))\nplt.title('GOOGLE Stock Price Prediction')\nplt.xlabel('Time')\nplt.ylabel('GOOGLE Stock Price')\nplt.legend()\nplt.show()","bd12d064":"#### Removing special character from each tweets","6ef143b0":"#### Now to know the \"closing price\" of each day we will import STOCK PRICE DATA for GOOGLE from \"yahoo.finance\". We will consider \"Close\" price only.","029bc7b6":"#### Making a new dataframe with necessary columns for providing machine learning.","a08ecbed":"#### Adding a \"Price\" column in our dataframe and fetching the stock price as per the date in our dataframe.","be44b9ea":"#### Making \"prices\" column as integer so mathematical operations could be performed easily.","b7c8d76b":"#### Defining index position for the test data. Making dataframe for the predicted value.","ffea24f9":"#### So we take the mean for the close price and put it in the blank value","dcd3085e":"#### Making a 2D array that will store the Negative and Positive sentiment for Testing dataset.","6681c289":"#### Making 2 dataframe for Training and Testing \"Prices\". You can also make 1-D array for the same.","aba250b2":"#### Making a 2D array that will store the Negative and Positive sentiment for Training dataset.","0c478c2f":"#### importing library to fetch data from twitter","048557dd":"#### All the tweets has been clubbed as per their date.","68fae1dd":"#### Plotting the graph for the Predicted_price VS Actual Price","06b4e205":"#### Calculating the percentage of postive and negative tweets, and plotting the PIE chart for the same.","0f26707f":"Gather The STock DATA from Yahoo FInance","454df7a0":"#### Creating a dataframe where we will combine the tweets date wise and store into","ca1e1c26":"#### Making Predictions","90ea7205":"#### Displaying the data with date and tweets, you can notice there are multiple tweets for each day. So we will club them together later.","6e51b4ec":"#### Importing matplotlib library for plotting graph","98e4b87c":"#### setting up consumer key and access token","3fb77142":"#### importing machine learning libraries","eb4a4c84":"#### Dividing the dataset into train and test.","0f3ec8be":"**Store the predicted Stock Price**","fe7d72be":"Divide the data into Training and Testing","5436e119":"#### Fetching tweets for Google from Sundar Pichai's timeline in extended mode (means entire tweet will come and not just few words + link)","bc21744f":"#### Prices are fetched but some entires are blank as close price might not be available for that day due to some reason (like holiday, etc.)","cf179150":"#### This part of the code is responsible for assigning the polarity for each statement. That is how much positive, negative, neutral you statement is. And also assign the compound value that is overall sentiment of the statement.","47117416":"#### Fitting the sentiments(this acts as in independent value) and prices(this acts as a dependent value (like class-lables in iris dataset))","2344998d":"#### Downloading this package was essential to perform sentiment analysis.","19298dd2":"#### Adding 4 new columns in our dataframe so that sentiment analysis could be performed.. Comp is \"Compound\" it will tell whether the statement is overall negative or positive. If it has negative value then it is negative, if it has positive value then it is positive. If it has value 0, then it is neutral.","1bd4064e":"#### Now all the entries have some value","527f2f2a":"# STOCK PREDICTION USING TWITTER SENTIMENT ANALYSIS"}}