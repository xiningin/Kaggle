{"cell_type":{"cfa968d0":"code","decb56c2":"code","5801322c":"code","cbe054ae":"code","8a15ad1c":"code","732a3667":"code","57efbe06":"code","def2393a":"code","792ffd47":"code","838c95a8":"code","25886b58":"code","7a95c6a2":"code","4b667f49":"code","4d9d1fc3":"code","6525b0d7":"markdown","638d671e":"markdown","896641d0":"markdown","82853817":"markdown","e04fbdfe":"markdown","d7d7680b":"markdown","0e807926":"markdown","0cbf0dfc":"markdown","713e2c5e":"markdown","1ec7d1cf":"markdown","456489c8":"markdown","5bda5a87":"markdown"},"source":{"cfa968d0":"# please read https:\/\/github.com\/librosa\/librosa\n!pip install librosa --user","decb56c2":"# you may need ffmpeg to be installed\n!conda install -c conda-forge ffmpeg","5801322c":"# Librosa for audio\nimport librosa\n# And the display module for visualization\nimport librosa.display\nimport os.path\naudios = ['mp3\/ACDC.mp3', 'mp3\/iamthemorning.mp3', 'mp3\/Little Big.mp3']\nrequest_audio = 'mp3\/request.mp3'\nmp3_dir='..\/input\/airhw6mp3\/'\naudios=[os.path.join(mp3_dir,audio) for audio in audios]\nrequest_audio=os.path.join(mp3_dir,request_audio)\ndef read_and_resample(path, sample_rate):\n    # read and resample to 22KHz\n    y, sr = librosa.load(path, sr=sample_rate)    \n    print(f\"{path} length is {y.shape[0] \/ sample_rate \/ 60.:.2f} min\")\n    return y\n\ndataset = {}\nsample_rate = 22050\n# reading all audios\nfor path in audios:\n    dataset[path] = read_and_resample(path, sample_rate)\n\n# reading request audio\nrequest_data = read_and_resample(request_audio, sample_rate)","cbe054ae":"import numpy as np\nimport matplotlib.pyplot as plt\n\n# y = dataset['mp3\/ACDC.mp3']\ny = dataset[os.path.join(mp3_dir,'mp3\/ACDC.mp3')]\n# Let's make and display a mel-scaled power (energy-squared) spectrogram\nS = librosa.feature.melspectrogram(y, sr=sample_rate, n_mels=128)\n\nprint(f\"Spectrogram shape: {S.shape}\")\n\n# Convert to log scale (dB). We'll use the peak power (max) as reference.\nlog_S = librosa.power_to_db(S, ref=np.max)\n\nplt.figure(figsize=(12,4))\nlibrosa.display.specshow(log_S, sr=sample_rate, x_axis='time', y_axis='mel')\nplt.title('mel power spectrogram')\nplt.colorbar(format='%+02.0f dB')\nplt.tight_layout()","8a15ad1c":"from scipy.ndimage.filters import maximum_filter\nimport scipy.ndimage as ndimage\nneighborhood_size = 20\n\n# sec\/sample - constant for all files\n# wav = dataset[\"mp3\/ACDC.mp3\"]\nwav=dataset[os.path.join(mp3_dir,'mp3\/ACDC.mp3')]\ntime_resolution = (wav.shape[0] \/ sample_rate) \/ S.shape[1]\nprint(\"Time resolution:\", time_resolution)\n\ndef form_constellation(name, wav, sample_rate, time_resolution):\n    S = librosa.feature.melspectrogram(wav, sr=sample_rate, n_mels=256, fmax=4000)\n    S = librosa.power_to_db(S, ref=np.max)\n    # get local maxima\n    Sb = maximum_filter(S, neighborhood_size) == S\n    \n    Sbd, num_objects = ndimage.label(Sb)\n    objs = ndimage.find_objects(Sbd)\n    points = []\n    for dy, dx in objs:\n        x_center = (dx.start + dx.stop - 1) \/\/ 2\n        y_center = (dy.start + dy.stop - 1) \/\/ 2    \n        if (dx.stop - dx.start) * (dy.stop - dy.start) == 1:\n            points.append((x_center, y_center))\n            \n    # 20 seconds\n    low = int(0 \/ time_resolution)\n    hi = int(20 \/ time_resolution)\n    print(name)\n    # here we show results of max filter. This is NOT exactly the same which is saved.\n    # Only single-point CC will fall into `points` array\n    plt.figure(figsize=(12,6))\n    plt.imshow(Sb[:, low:hi])\n    plt.show()\n    print(len(points))\n    return sorted(points) \n    \n\nconstellations = {}\nfor name, wav in dataset.items():\n    constellations[name] = form_constellation(name, wav, sample_rate, time_resolution)\n\nrequest_constellation = form_constellation(request_audio, request_data, sample_rate, time_resolution)","732a3667":"print(time_resolution)\ntarget = (int(1 \/ time_resolution), int(5 \/ time_resolution), -50, 50)    # start, end, Hz low, Hz high \n\nindex = {}\nrequest = {}\n\ndef build_constellation_index(constellation_collection, target):\n    # TODO: build shazam index for a collection of constellations\n    # consider some window (target) for every point\n    # for every point inside this window make an entry in the index (or update it, if the key exists)\n    # keys are triples of the form (f1, f2, dt), values are lists with tuples (t, name)\n    result_index = {}   \n    timelow=target[0]\n    timehigh=target[1]\n    freqlow=target[2]\n    freqhigh=target[3]\n    for file in constellation_collection:\n      print(file)\n      for i in range(len(constellation_collection[file])):\n        t1=constellation_collection[file][i][0]\n        f1=constellation_collection[file][i][1]\n        for star in (constellation_collection[file][:i]+constellation_collection[file][i:]):\n          t2=star[0]\n          f2=star[1]\n          if freqlow<(f2-f1)<freqhigh and timelow<(t2-t1)<timehigh:\n            result_index[(f1,f2,t2-t1)]=(t1,file)\n    return result_index\n\nindex = build_constellation_index(constellations, target)\nrequest = build_constellation_index({request_audio:request_constellation}, target)","57efbe06":"# TODO for every audio find all keys that match request audio keys \n# for every such match calculate time offset (difference between audio and request)\n# display counts of matches for every time offset (using histogram, for example)\nfrom tqdm.notebook import tqdm\n# for key1 in tqdm(request.keys()):\n#     for key2 in index.keys():\n#         if key1==key2:\n#             audio_name=index[key1][1]\n#             offset=index[key1][0]-request[key2][0]\n#             if audio_name not in matches.keys():\n#                 matches[audio_name]=[]\n#             matches[audio_name].append(offset)\nmatches={}\nset1=set(request.keys())\nset2=set(index.keys())\nmatches_list=list(set1.intersection(set2))\nfor match in tqdm(matches_list):\n    audio_name=index[match][1]\n    offset=index[match][0]-request[match][0]\n    if audio_name not in matches.keys():\n        matches[audio_name]=[]\n    matches[audio_name].append(offset)\nimport matplotlib.pyplot as plt\ni=1\nfor audio in matches.keys():\n    x=matches[audio]\n    plt.figure(figsize=(7,5))\n    print(audio)\n    plt.hist(x,30)\n    i+=1","def2393a":"from collections import Counter\ndef similarity(audio1, audio2):   \n    # ... write your code here\n    audio1_name=os.path.join(mp3_dir,audio1)\n    audio2_name=os.path.join(mp3_dir,audio2)\n    audio1=read_and_resample(audio1_name, sample_rate)\n    audio1=form_constellation(audio1, request_data, sample_rate, time_resolution)\n    audio2=read_and_resample(audio2_name, sample_rate)\n    audio2=form_constellation(audio2, request_data, sample_rate, time_resolution)\n    audio1_fingerprint=build_constellation_index({audio1_name:audio1},target)\n    audio2_fingerprint=build_constellation_index({audio2_name:audio2},target)\n    set1=set(audio1_fingerprint.keys())\n    set2=set(audio2_fingerprint.keys())\n    matches_list=list(set1.intersection(set2))\n    similarity = len(matches_list)\/min(len(set1),len(set2))\n    most_common=Counter(matches_list).most_common(1)[0][0]\n    offset = index[most_common][0]-request[most_common][0]\n    return similarity, offset*time_resolution\n\n    \nsim, off = similarity('mp3\/ACDC.mp3', 'mp3\/request.mp3')\nassert abs(abs(off) - 60) < 5, \"Offset value for these 2 tracks should be around 1 minute.\"","792ffd47":"print(sim,off)","838c95a8":"!yes | pip uninstall opencv-python\n!yes | pip uninstall opencv-contrib-python\n!yes | pip install opencv-contrib-python","25886b58":"import cv2 as cv\nfrom matplotlib import pyplot as plt\n\nimg_dir = '..\/input\/101-objectcategories\/101_ObjectCategories'\nimg = cv.imread(img_dir + '\/gramophone\/image_0018.jpg')\ngray= cv.cvtColor(img,cv.COLOR_BGR2GRAY)\nsift = cv.xfeatures2d.SIFT_create()\nkp = sift.detect(gray,None)\nimg=cv.drawKeypoints(gray,kp,img,flags=cv.DRAW_MATCHES_FLAGS_DRAW_RICH_KEYPOINTS)\nplt.imshow(img)","7a95c6a2":"## TODO do your job here\n## key is a descriptor, value is a filename\nimport numpy as np\nimport os\nfrom tqdm.notebook import tqdm\nimport cv2 as cv\nfrom matplotlib import pyplot as plt\nimport pickle\nfrom annoy import AnnoyIndex\nimg_dir = '..\/input\/101-objectcategories\/101_ObjectCategories'\nkeys=[]\nlabels=[]\nclass_folders=os.listdir(img_dir)\nsift = cv.xfeatures2d.SIFT_create()\nif os.path.exists('..\/input\/image-sift\/keys (1).p') and os.path.exists('..\/input\/lables\/labels.p'):\n    with open('..\/input\/image-sift\/keys (1).p','rb') as fp:\n        keys=pickle.load(fp)\n        fp.close()\n    with open('..\/input\/lables\/labels.p','rb') as fp:\n        labels=pickle.load(fp)\n        fp.close()\nelse:\n    for folder in tqdm(class_folders):\n      if os.path.isdir(os.path.join(img_dir,folder)):\n        for img in os.listdir(os.path.join(img_dir,folder)):\n          if os.path.isfile(os.path.join(img_dir,folder)+'\/'+img):\n            data = cv.cvtColor(cv.imread(os.path.join(img_dir,folder)+'\/'+img), cv.COLOR_BGR2GRAY)\n            kp, des = sift.detectAndCompute(data,None)\n            if des is not None:\n              for vector in des:\n                keys.append(vector)\n                labels.append((folder,img))\n    with open(\"labels.p\",\"wb\") as fp:\n      pickle.dump(labels,fp)\n      fp.close()\n    with open(\"keys.p\",\"wb\") as fp:\n      pickle.dump(keys,fp)\n      fp.close()\nlen(keys)\nindex = AnnoyIndex(128, 'angular')  \n\nfor idx,vec in tqdm(enumerate(keys)):\n    index.add_item(idx,vec)\nindex.build(10,n_jobs=-1)","4b667f49":"from collections import Counter\nlabels_counter=Counter(labels)\ndef anns(imagename, k):\n    ## ... do your job here\n    imagepath=os.path.join(img_dir,imagename)\n    res=[]\n    idxs=[]\n    dists=[]\n    img=cv.cvtColor(cv.imread(imagepath), cv.COLOR_BGR2GRAY)\n    kp, img = sift.detectAndCompute(img,None)\n    for vector in img:\n        idx,dist=index.get_nns_by_vector(vector,50,include_distances=True)\n        idxs+=idx   \n        counter=Counter([labels[idx] for idx in idxs])\n    for key in counter.keys():\n        similarity=counter[key]\n        res.append((similarity,os.path.join(key[0],key[1])))\n    return sorted(res,reverse=True)[:k]\n\n\n# finds query image in the result, as it is indexed\nfilename = 'strawberry\/image_0022.jpg'\nassert any([f[1] == filename for f in anns(filename, 10)]), \"Should return a duplicate\"","4d9d1fc3":"## write your code here\nimport os.path\nimport math\nqueries=[\n 'accordion\/image_0043.jpg', \n 'laptop\/image_0052.jpg', \n 'pagoda\/image_0038.jpg',\n 'revolver\/image_0043.jpg',\n 'rhino\/image_0040.jpg', \n 'sea_horse\/image_0038.jpg', \n 'soccer_ball\/image_0057.jpg', \n 'starfish\/image_0011.jpg', \n 'strawberry\/image_0022.jpg',\n 'wrench\/image_0013.jpg']\n# queries=[\n#  'accordion\/image_0043.jpg', \n#  'laptop\/image_0052.jpg']\ndef search_and_check_relevant(filename,k=10):\n    base_class=os.path.split(filename)[0]\n    nns=anns(filename,k) #sorted([(similarity, filename)])\n    nns_class=[os.path.split(x[1])[0] for x in nns]\n    return [1 if x==base_class else 0 for x in nns_class]\ndef DCG(relevences):\n    k=len(relevences[0])\n    Zk=1\/sum([1\/math.log2(1+m) for m in range(1,k+1)])#is a normalization factor\n    return Zk*sum([sum([(2**relevence[m-1]-1)\/math.log2(1+m) for m in range(1,k+1)])\\\n                   for relevence in relevences])\/len(relevences)\ndef pFound(relevences,pBreak=0.15,pRel_rel=0.4):\n    pFound=[]\n    for relevence in relevences:\n        pLook=[]\n        pRel=[0.4 if x==1 else 0 for x in relevence]\n        for i in range(len(relevence)):\n            if i==0:\n                pLook.append(1)\n            else:\n                pLook.append(pLook[i-1]*(1-pBreak)*(1-pRel[i-1]))\n            pFound.append(pLook[i]*pRel[i])       \n    return sum(pFound)\/len(relevences)\n    \nrelevences=[search_and_check_relevant(filename) for filename in tqdm(queries)]\nprint(DCG(relevences))\nprint(pFound(relevences))","6525b0d7":"### 2.3.2. [20] Implement search function\n\nImplement a function which returns `k` neighbours (names) sorted for a given image name.","638d671e":"## 1.2. Visualizing example spectrogram\n\nWe use [Mel scale](https:\/\/en.wikipedia.org\/wiki\/Mel_scale) to emphasize perception of pitch rather than normal frequencies. No theoretical basis, just for you to know what is this :)","896641d0":"## 1.4. [50] Build index from constellations \n\n### 1.4.1. [30] Index construction\n\nHere you will build an in-memory index from constellations. Follow the algorithm:\n1. For every \"star\" $(t_1, f_1)$ on constellation consider some constellation window (`target`) of time and frequency shift. In example this is `(+1, +5) sec` and `(-50, +50) Hz`. In original paper they propose to use SOME \"stars\". If you propose your own selection method - this is also good, as it speeds up computations.\n2. If a \"star\" $(t_2, f_2)$ falls into a `target` window, create or update and index entry. Form a key as $(f1, f2, \\Delta t=t_2-t_1)$, and a value is $(t_1, filename)$.","82853817":"## 2.2. SIFT example\n\nBelow is an example how to extract SIFT keyponts using `opencv`. [This](https:\/\/docs.opencv.org\/trunk\/da\/df5\/tutorial_py_sift_intro.html) is a dedicated tutorial, and [this](https:\/\/docs.opencv.org\/master\/dc\/dc3\/tutorial_py_matcher.html) is another tutorial you may need to find matches between two images (use in your code `cv.drawMatches()` function to display keypoint matches).","e04fbdfe":"# 1. [50] Shazam\n\nHave you ever wondered what is happening under the hood of the popular music search tool? How can a track be indentified among the millions of other tracks by a short sample in just a few seconds? Obviously, this is not a mere brute force. Today we will remove the veil and find out what kind of magic happens there by implementing the algorithm on our own.\n\nRefer to the [original article](https:\/\/www.ee.columbia.edu\/~dpwe\/papers\/Wang03-shazam.pdf) for implementation details.\n\n## 1.0. Installing needed libraries","d7d7680b":"### 1.4.2. [20] Compare `request.mp3` track with the database using index queries.\n\n1. For every audio file from index find all keys that match query keys.\n2. for every such match calculate time offset (difference between audio $t_1$ and query $t_1$)\n3. Display counts of matches for every time offset (using histogram, for example)","0e807926":"## 1.4.3. [Extra] Similarity function\n\nImplement a function, which will be estimating similarity of 2 audio tracks in some numbers. It should return 2 numbers: relevance estimation and audio offset (how start of one track is shifted with respect to another, if possible).\n\nWe are not giving any particular instructions on HOW to do this, but you can definitely start thinking from:\n1. Norming and thresholding.\n2. [Mode](https:\/\/en.wikipedia.org\/wiki\/Mode_(statistics)). E.g. [in python](https:\/\/docs.python.org\/3\/library\/statistics.html).\n3. [Kurtosis](https:\/\/en.wikipedia.org\/wiki\/Kurtosis). E.g. [in python](https:\/\/docs.scipy.org\/doc\/scipy\/reference\/generated\/scipy.stats.kurtosis.html).","0cbf0dfc":"## 1.3. Forming constellations\n\nWe will do something like this from our spectrogram. Not exactly as in the paper, but very similar :)\n\n<img src=\"http:\/\/coding-geek.com\/wp-content\/uploads\/2015\/05\/shazam_filtered_spectrogram-min.png\" width=\"400\"\/>\n\nThis will include:\n- get spectrogram\n- find local maxima with [computer vision algrithm (wow!)](https:\/\/docs.scipy.org\/doc\/scipy\/reference\/generated\/scipy.ndimage.maximum_filter.html): run maximum filtering and then compare original values with filtered.\n- `ndimage.label()` labels different connected components\n- `ndimage.find_objects()` returns their coordinates\n- for each connected component of size 1 (`(dx.stop - dx.start) * (dy.stop - dy.start) == 1`) save a center, which will correspond to a \"star\"","713e2c5e":"## 2.3.3. [Extra] Estimate quality\n\nBuild a bucket from these images.\n```\naccordion\/image_0043.jpg\nlaptop\/image_0052.jpg\npagoda\/image_0038.jpg\nrevolver\/image_0043.jpg\nrhino\/image_0040.jpg\nsea_horse\/image_0038.jpg\nsoccer_ball\/image_0057.jpg\nstarfish\/image_0011.jpg\nstrawberry\/image_0022.jpg\nwrench\/image_0013.jpg\n```\nConsider `relevant` if **class of the query and class of the result match**. Compute `DCG` and `pFound` for every query and for the bucket in average.","1ec7d1cf":"## 2.3. [50] Index of keypoints\n\nLet's suppose we've found image descriptors. How do we find similar images, having this information? In our case the descriptors are 128-dinensional vectors per keypoint, and there can be hundreds of such points. To enable fast search of similar images, you will index descriptors of all images using some data structure for approximate nearest neighbors search, such as Navigable Small World or Annoy. Then, for a new (query) image you will generate its descriptors, and for each of them find its `k` nearest neighbors (using Euclidean or Cosine distance, which you prefer). Finally, you will sort potential similar images (retrieved from neighbor descriptors) by frequency with which they appear in k nearest neighbors (more matches -- higher the rank).\n\n### 2.3.1. [30] Build an index\n\nRead all images, saving category information. For every image generate SIFT descriptors and index them using HNSW from [`nmslib`](https:\/\/github.com\/nmslib\/nmslib), FAISS, Annoy or whatever.","456489c8":"## 1.1. Reading and resampling audio tracks database\n\nBased on [this librosa demo](https:\/\/github.com\/librosa\/librosa\/blob\/main\/examples\/LibROSA%20demo.ipynb). we read audio files in `mp3` format and then resample to common sample rate of 22kHz. \n\nMake sure you've downloaded all 4 files from [here](https:\/\/github.com\/IUCVLab\/information-retrieval\/tree\/main\/datasets\/mp3).","5bda5a87":"# 2. [50] Image search using SIFT\n\nLet's think about information retrieval in the context of image search. How can we find images similar to a query in a fast way (faster than doing pair-wise comparison with all images in a database)? How can we identify same objects taken in slightly different contexts? \n\nOne way to do this is to find special points of interest in every image, so called keypoints (or descriptors), which characterize the image and which are more or less invariant to scaling, orientation, illumination changes, and some other distortions. There are several algorithms available that identify such keypoints, and today we will focus on [SIFT](https:\/\/en.wikipedia.org\/wiki\/Scale-invariant_feature_transform). \n\nYour task is to apply SIFT to a dataset of images and enable similar images search.\n\n## 2.1. Get dataset\n\nWe will use `Caltech 101` dataset, download it from [here](http:\/\/www.vision.caltech.edu\/Image_Datasets\/Caltech101\/). It consists of pictures of objects belonging to 101 categories. About 40 to 800 images per category. Most categories have about 50 images. The size of each image is roughly 300 x 200 pixels."}}