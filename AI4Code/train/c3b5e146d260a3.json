{"cell_type":{"3719dc61":"code","fed7e5dc":"code","163920f3":"code","a635385c":"code","9b93bda2":"code","0af4d745":"code","a12ba964":"code","bfdd7c13":"code","900a934a":"code","1adf0eae":"code","d0cddeec":"code","91acff2c":"code","0fc0e0ec":"code","d8690061":"markdown","17a489aa":"markdown","7aff4739":"markdown"},"source":{"3719dc61":"import pandas as pd\nimport tensorflow as tf\nimport numpy as np\nfrom sklearn import preprocessing\nimport matplotlib.pyplot as plt","fed7e5dc":"df=pd.read_csv('..\/input\/2020-jan-nifty\/2020 JAN NIFTY.txt',header=None)\ndf.drop(labels=[0,7,8],axis=1,inplace=True)\nd=379\ndf.columns=['date','time','open','high','low','close']\nplt.plot(range(379),df.close[:379])\nplt.plot(range(379,758),df.close[379:758])\nplt.plot(range(2*d,3*d),df.close[2*d:3*d])\nplt.plot(range(3*d,4*d),df.close[3*d:4*d])\ndff=df['close']","163920f3":"def windowed_dataset(series, window_size, batch_size, shuffle_buffer):\n    series = tf.expand_dims(series, axis=-1)\n    ds = tf.data.Dataset.from_tensor_slices(series)\n    ds = ds.window(window_size + 1, shift=1, drop_remainder=True)\n    ds = ds.flat_map(lambda w: w.batch(window_size + 1))\n    ds = ds.shuffle(shuffle_buffer)\n    ds = ds.map(lambda w: (w[:-1], w[1:]))\n    return ds.batch(batch_size).prefetch(1)\ndef model_forecast(model, series, window_size):\n    ds = tf.data.Dataset.from_tensor_slices(series)\n    ds = ds.window(window_size, shift=1, drop_remainder=True)\n    ds = ds.flat_map(lambda w: w.batch(window_size))\n    ds = ds.batch(32).prefetch(1)\n    forecast = model.predict(ds)\n    return forecast","a635385c":"tf.keras.backend.clear_session()\ntf.random.set_seed(51)\nnp.random.seed(51)\nseries=dff\nsplit_time = 303\nx_train = dff[:split_time]\nx_valid = dff[split_time:379]\ntf.random.set_seed(51)\nnp.random.seed(51)\nx_valid=x_valid.values.reshape(-1,1)\nx_train=x_train.values.reshape(-1,1)\nseries=series.values.reshape(-1,1)\nscaler=preprocessing.MinMaxScaler()\nx_train=scaler.fit_transform(x_train)\nx_valid=scaler.transform(x_valid)\nseries=scaler.transform(series)\nx_valid=x_valid.reshape(-1)\nx_train=x_train.reshape(-1)\nseries=series.reshape(-1)\nwindow_size = 15\nbatch_size=30\nshuffle_buffer=20\ntrain_set = windowed_dataset(x_train, window_size, batch_size, shuffle_buffer)","9b93bda2":"model = tf.keras.models.Sequential([\n  tf.keras.layers.Conv1D(filters=32, kernel_size=5,\n                      strides=1, padding=\"causal\",\n                      activation=\"relu\",\n                      input_shape=[None, 1]),\n  tf.keras.layers.LSTM(128, return_sequences=True),\n  tf.keras.layers.LSTM(512, return_sequences=True),\n\n  tf.keras.layers.LSTM(64, return_sequences=True),\n  tf.keras.layers.Dense(10),\n  tf.keras.layers.Lambda(lambda x: x * 200)\n])\nlr_schedule = tf.keras.callbacks.LearningRateScheduler(\n    lambda epoch: 1e-8 * 10**(epoch \/ 20))\noptimizer = tf.keras.optimizers.SGD(lr=1e-8, momentum=0.9)\nmodel.compile(loss=tf.keras.losses.Huber(),\n              optimizer=optimizer,\n              metrics=[\"mae\"])\nhistory = model.fit(train_set, epochs=100, callbacks=[lr_schedule])","0af4d745":"plt.semilogx(history.history[\"lr\"], history.history[\"loss\"])\n\nplt.axis([1e-8, 1e-4, 0, 1])\n","a12ba964":"tf.keras.backend.clear_session()\ntf.random.set_seed(51)\nnp.random.seed(51)\n\ndataset = windowed_dataset(x_train, window_size, batch_size, shuffle_buffer)\n\nmodel = tf.keras.models.Sequential([\n  tf.keras.layers.Conv1D(filters=32, kernel_size=5,\n                      strides=1, padding=\"causal\",\n                      activation=None,\n                      input_shape=[None, 1]),\n  tf.keras.layers.LSTM(128, return_sequences=True),\n  tf.keras.layers.LSTM(512, return_sequences=True),\n\n  tf.keras.layers.LSTM(64, return_sequences=True),\n  tf.keras.layers.Dense(10),\n  tf.keras.layers.Lambda(lambda x: x * 200)\n])\n\noptimizer = tf.keras.optimizers.Adam(lr=1e-4)\nmodel.compile(loss=tf.keras.losses.Huber(),\n              optimizer=optimizer,\n              metrics=[\"mae\"])\nhistory = model.fit(dataset,epochs=60)","bfdd7c13":"plt.figure(figsize=(10,6))\nplt.plot(range(len(history.history[\"loss\"])), history.history[\"loss\"])\n","900a934a":"rnn_forecast = model_forecast(model, series[..., np.newaxis], window_size)\nrnn_forecast = rnn_forecast[split_time - window_size:379-window_size, -1, 0]","1adf0eae":"\nplt.figure(figsize=(10, 10))\n\nplt.plot(range(len(x_valid)), x_valid)\nplt.plot(range(len(rnn_forecast)), rnn_forecast)\n","d0cddeec":"tf.keras.metrics.mean_absolute_error(x_valid, rnn_forecast).numpy()","91acff2c":"x_valid=x_valid.reshape(-1,1)\nx_valid=scaler.inverse_transform(x_valid)\nrnn_forecast=rnn_forecast.reshape(-1,1)\nrnn_forecast=scaler.inverse_transform(rnn_forecast)\nplt.figure(figsize=(10, 10))\n\nplt.plot(range(len(x_valid)), x_valid)\nplt.plot(range(len(rnn_forecast)), rnn_forecast)\n","0fc0e0ec":"print(tf.keras.metrics.mean_absolute_error(x_valid, rnn_forecast).numpy().mean())\n","d8690061":"LSTM with optimised LR","17a489aa":"Inverse transform to get original values","7aff4739":"Optimising LR"}}