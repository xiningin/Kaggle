{"cell_type":{"6d922dcb":"code","51d83b27":"code","804e38c1":"code","6ab7ade3":"code","970a7bf3":"code","4f937885":"code","260b6355":"code","df17be13":"code","317f9d40":"code","3fdd5648":"code","c4b36b87":"code","edd91014":"code","41e5dae4":"code","0381ca42":"code","1d12df4b":"code","04770cb8":"code","f521ce93":"code","0775a80f":"code","ca438120":"code","8318d6f6":"code","9c130535":"code","a8d164b0":"code","510b24af":"code","99726259":"code","b6b9f261":"code","7b41eee8":"code","afd0b8fb":"code","d6482cc5":"code","e0fffa11":"code","97e820d4":"code","c9b8ab90":"code","25521479":"code","0757ce0e":"code","c082a44b":"code","3cf88cde":"code","72962239":"code","8ca47b22":"code","69a7a21b":"code","a50a3389":"code","ce0a5cfd":"code","0586ba99":"code","ad148f1c":"code","6cf7f009":"code","e7481c0e":"code","38529db3":"code","b71e89f1":"code","6a51b7da":"code","b5a53125":"markdown","ba055ea0":"markdown","75cd6cfe":"markdown","4fa3cabd":"markdown","77b43793":"markdown","fda0caf2":"markdown","d48de244":"markdown","511beef9":"markdown","f54b8faf":"markdown","6a77cdef":"markdown","cdc92839":"markdown","6157633f":"markdown","34d30119":"markdown","92e1bae4":"markdown","05597d8d":"markdown","c4623789":"markdown","6ad6caf4":"markdown"},"source":{"6d922dcb":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","51d83b27":"import warnings\nwarnings.filterwarnings('ignore')","804e38c1":"import matplotlib.pyplot as plt\nimport seaborn as sns","6ab7ade3":"data = pd.read_csv('\/kaggle\/input\/breast-cancer-wisconsin-data\/data.csv')\ndata.head()\ndf = data.drop('Unnamed: 32', axis = 1)\ndf.head()","970a7bf3":"df.info()","4f937885":"print('The Number of missing values per feature is:')\ndf.isna().sum()","260b6355":"# separating out the target feature\ntarget = df['diagnosis']","df17be13":"print('We have a pretty balanced dataset with 357 Benign cases and 212 Malignant cases.')\ntarget.value_counts()","317f9d40":"df_predictors = df.drop(['diagnosis', 'id'], axis = 1)\ndf_predictors.head()","3fdd5648":"print('Number of discrete numerical features are {}'.format(len([feature for feature in df_predictors.columns\nif df_predictors[feature].nunique() < 25])))","c4b36b87":"df_predictors.columns","edd91014":"plt.figure(figsize=(20,15))\npn = 1\nfor feature in df_predictors.columns:\n    if pn <= 30:\n        ax = plt.subplot(5,6,pn)\n        sns.distplot(df_predictors[feature], hist_kws = {'color': 'm', 'alpha': 1}, kde_kws = {'color': 'b'} )\n        plt.xlabel(feature)\n    pn += 1\nplt.tight_layout()\nplt.show()","41e5dae4":"dfmod = data.drop(['id', 'Unnamed: 32'], axis = 1)\nplt.figure(figsize = (20,15))\npn = 1\nfor feature in df_predictors.columns:\n    if pn <= 30:\n        ax = plt.subplot(5,6,pn)\n        dfmod.groupby('diagnosis')[feature].median().plot.bar(color = ['g', 'm'])\n        plt.xlabel(feature)\n    pn += 1\nplt.tight_layout()\nplt.show()","0381ca42":"plt.figure(figsize = (20,15))\ncm = dfmod.corr()\nsns.heatmap(cm[abs(cm) > 0.85], annot = True, square = True, linewidths = 2, linecolor = 'black', cmap = 'rainbow', robust = True, mask = np.triu(cm, k = 0))","1d12df4b":"selected_features = ['radius_mean', 'texture_mean', 'smoothness_mean', 'compactness_mean', 'concavity_mean','symmetry_mean', 'fractal_dimension_mean',\n                     'radius_se', 'texture_se', 'smoothness_se', 'compactness_se', 'concavity_se', 'concave points_se', 'symmetry_se', 'fractal_dimension_se', 'smoothness_worst', 'symmetry_worst', 'fractal_dimension_worst']","04770cb8":"len(selected_features)","f521ce93":"data = df_predictors.copy()\nfor feature in selected_features:\n    if 0 in df_predictors[feature].unique():\n        pass\n    else:\n        df_predictors[feature] = np.log(df_predictors[feature])","0775a80f":"for feature in selected_features:\n    sns.distplot(df_predictors[feature])\n    plt.title(feature)\n    plt.show()","ca438120":"plt.figure(figsize = (20,15))\npn = 1\nfor feature in selected_features:\n    if pn <= 30:\n        ax = plt.subplot(5,6,pn)\n        sns.boxplot(y = data[feature], color = 'orange')\n        plt.title(feature)\n    pn += 1\nplt.tight_layout()   \nplt.show()","8318d6f6":"target = df.diagnosis","9c130535":"tr = target.copy()\ntr = pd.Series(np.where(tr == 'M', 1,0))","a8d164b0":"from sklearn.ensemble import ExtraTreesClassifier\nmod = ExtraTreesClassifier()\nmod.fit(df_predictors[selected_features], tr)","510b24af":"feat_imp = pd.Series(mod.feature_importances_, index = selected_features)\nfeat_imp.nlargest(15).plot(kind = 'bar')\nplt.title('Feature Importance')\nplt.show()","99726259":"feature_list = feat_imp.sort_values(ascending = False)[:11].index","b6b9f261":"target = df['diagnosis']\ntr = pd.Series(np.where(target == 'M', 1, 0))","7b41eee8":"for feature in feature_list:\n    if 0 in df[feature].unique():\n        pass\n    else:\n        df[feature] = np.log(df[feature])","afd0b8fb":"X = df[feature_list]\nX.head()","d6482cc5":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, tr, test_size = 0.3, random_state = 0)\nX_train.shape, X_test.shape","e0fffa11":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.naive_bayes import GaussianNB, CategoricalNB\nfrom xgboost import XGBClassifier","97e820d4":"from sklearn.model_selection import GridSearchCV\nfrom sklearn.metrics import confusion_matrix, accuracy_score, recall_score, precision_score, roc_auc_score, plot_confusion_matrix\nfrom sklearn.model_selection import cross_val_score","c9b8ab90":"lr = LogisticRegression(random_state=0)\nclflr = GridSearchCV(lr, param_grid = {'penalty': ['l1', 'l2'], 'C':np.arange(0.1, 10, 0.2), 'solver': ['liblinear', 'lbfgs']}, cv = 5, scoring = 'recall')\nclflr.fit(X_train, y_train)\nprint('''Recall score on training set is : {}\nwith parameters as: {}'''.format(clflr.best_score_, clflr.best_params_))","25521479":"lrf = LogisticRegression(C= 6.1000000000000005, penalty= 'l1', solver= 'liblinear', random_state=0)\nlrf.fit(X_train, y_train)\ny_predlr = lrf.predict(X_test)\nprint('Recall score on test data for Logistic regression  is: {}'.format(recall_score(y_test, y_predlr)))\nrecall_lr = recall_score(y_test, y_predlr)","0757ce0e":"dt = DecisionTreeClassifier(random_state=0)\nclfdt = GridSearchCV(dt, param_grid = {'criterion': ['gini', 'entropy']}, cv = 5, scoring = 'recall')\nclfdt.fit(X_train, y_train)\nprint('''Recall score on training set is : {}\nwith parameters as: {}'''.format(clfdt.best_score_, clfdt.best_params_))","c082a44b":"dtf = DecisionTreeClassifier(criterion = 'entropy', random_state=0)\ndtf.fit(X_train, y_train)\ny_preddt = dtf.predict(X_test)\nprint('Recall score on test data for Decision Tree Classifier is: {}'.format(recall_score(y_test, y_preddt)))\nrecall_dt = recall_score(y_test, y_preddt)","3cf88cde":"rf = RandomForestClassifier(random_state=0)\nclfrf = GridSearchCV(rf, param_grid = {'n_estimators': np.arange(100, 170, 10), 'criterion': ['gini', 'entropy']}, cv = 5, scoring = 'recall')\nclfrf.fit(X_train, y_train)\nprint('''Recall score on training set is : {}\nwith parameters as: {}'''.format(clfrf.best_score_, clfrf.best_params_))","72962239":"rff = RandomForestClassifier(criterion= 'gini', n_estimators= 120, random_state=0)\nrff.fit(X_train, y_train)\ny_predrf = rff.predict(X_test)\nprint('Recall score on test data for Random Forest Classifier is: {}'.format(recall_score(y_test, y_predrf)))\nrecall_rf = recall_score(y_test, y_predrf)","8ca47b22":"svm = SVC(random_state=0)\nclfsvm = GridSearchCV(svm, param_grid = {'C': np.arange(0.1, 10, 0.2), 'kernel': ['rbf', 'linear', 'poly']}, cv = 5, scoring = 'recall')\nclfsvm.fit(X_train, y_train)\nprint('''Recall score on training set is : {}\nwith parameters as: {}'''.format(clfsvm.best_score_, clfsvm.best_params_))","69a7a21b":"svmf = SVC(C = 6.7, kernel = 'linear', random_state=0)\nsvmf.fit(X_train, y_train)\ny_predsvm = svmf.predict(X_test)\nprint('Recall score on test data for svm is: {}'.format(recall_score(y_test, y_predsvm)))\nrecall_svm = recall_score(y_test, y_predsvm)","a50a3389":"ada = AdaBoostClassifier(random_state=0)\nclfada = GridSearchCV(ada, param_grid = {'n_estimators': np.arange(50,160,10), 'learning_rate': np.arange(0.1,2,0.1)}, cv = 5, scoring = 'recall')\nclfada.fit(X_train, y_train)\nprint('''Recall score on training set is : {}\nwith parameters as: {}'''.format(clfada.best_score_, clfada.best_params_))","ce0a5cfd":"adaf = AdaBoostClassifier(n_estimators=50, learning_rate=1, random_state=0)\nadaf.fit(X_train, y_train)\ny_predada = adaf.predict(X_test)\nprint('Recall score on test data for Adaboost is: {}'.format(recall_score(y_test, y_predada)))\nrecall_ada = recall_score(y_test, y_predada)","0586ba99":"gb = GradientBoostingClassifier(random_state=0)\nclfgb = GridSearchCV(gb, param_grid = {'n_estimators': np.arange(50,160,10), 'learning_rate': np.arange(0.1,1.9,0.2)}, cv = 5, scoring = 'recall')\nclfgb.fit(X_train, y_train)\nprint('''Recall score on training set is : {}\nwith parameters as: {}'''.format(clfgb.best_score_, clfgb.best_params_))","ad148f1c":"gbf = GradientBoostingClassifier(learning_rate=1.5, n_estimators=60,random_state=0)\ngbf.fit(X_train, y_train)\ny_predgb = gbf.predict(X_test)\nprint('Recall score on test data for Gradient boost is: {}'.format(recall_score(y_test, y_predgb)))\nrecall_gb = recall_score(y_test, y_predgb)","6cf7f009":"xgb = XGBClassifier(random_state=0)\nclfxgb = GridSearchCV(xgb, param_grid = {'n_estimators': np.arange(50,160,10), 'learning_rate': np.arange(0.1,2.1,0.1)}, cv = 5, scoring = 'recall')\nclfxgb = clfxgb.fit(X_train, y_train)\nprint('''Recall score on training set is : {}\nwith parameters as: {}'''.format(clfxgb.best_score_, clfxgb.best_params_))","e7481c0e":"xgbf = XGBClassifier(learning_rate=1.7, n_estimators=50,random_state=0)\nxgbf.fit(X_train, y_train)\ny_predxgb = xgbf.predict(X_test)\nprint('Recall score on test data for Gradient boost is: {}'.format(recall_score(y_test, y_predxgb)))\nrecall_xgb = recall_score(y_test, y_predxgb)","38529db3":"model = ['Logistic Regression', 'Decision Tree', 'Random Forest', 'SVM', 'AdaBoost', 'Gradient Boost', 'XgBoost']\nscores = [recall_lr,recall_dt,recall_rf,recall_svm,recall_ada,recall_gb,recall_xgb]","b71e89f1":"score = pd.DataFrame()\nscore['Model'] = model\nscore['Test Score'] = scores\nscore.set_index('Model')\nscore.sort_values(by = 'Test Score',ascending = False)","6a51b7da":"#plt.subplot(2,2,1, frameon = True)\nplot_confusion_matrix(lrf, X_test, y_test, display_labels = ['Benign', 'Malignant'])\nplt.title('Logistic Regression Model')\nplt.show()\n#plt.subplot(2,2,2,frameon = True)\nplot_confusion_matrix(xgbf, X_test, y_test, display_labels = ['Benign', 'Malignant'])\nplt.title('XgBoost Model')\nplt.show()\n#plt.subplot(2,2,3,frameon = True)\nplot_confusion_matrix(gbf, X_test, y_test, display_labels = ['Benign', 'Malignant'])\nplt.title('Gradient Boost Model')\nplt.show()","b5a53125":"## Adaboost","ba055ea0":"## XgBoost","75cd6cfe":"1) **ID number**\n\n2) **Diagnosis** (M = malignant, B = benign)\n\n\n- Ten real-valued features are computed for each cell nucleus:\n\n1) **radius** (mean of distances from center to points on the perimeter)\n\n2) **texture** (standard deviation of gray-scale values)\n\n3) **perimeter**\n\n4) **area**\n\n5) **smoothness** (local variation in radius lengths)\n\n6) **compactness** (perimeter^2 \/ area - 1.0)\n\n7) **concavity** (severity of concave portions of the contour)\n\n8) **concave points** (number of concave portions of the contour)\n\n9) **symmetry**\n\n10) **fractal dimension** (\"coastline approximation\" - 1)\n\n- The mean, standard error and \"worst\" or largest (mean of the three largest values) of these features were computed for each image,resulting in 30 features. For instance, field 3 is Mean Radius, field 13 is Radius SE, field 23 is Worst Radius.\n\n- All feature values are recoded with four significant digits.","4fa3cabd":"## Bivariate Analysis","77b43793":"## Univariate Analysis","fda0caf2":"- **For a feature to be considered a discrete numeric one, I will choose it to have less than 25 discrete values:**","d48de244":"- We have only one 'Object' type variable that is the target variable, **diagnosis**","511beef9":"## SVM","f54b8faf":"- **Therefore, all the predictors are continuous features.**","6a77cdef":"## Random Forest Classifier","cdc92839":"## Decision Tree Classifier","6157633f":"## Gradient Boosting","34d30119":"- **The above heatmap showcases the highly correlated predictors, Of these, we can eleminate a few for our model building.**","92e1bae4":"- **I Have considered 0.85 as a threshold for strong correlation between features**","05597d8d":"- **We can observe that most predictors follow near normal distribution but a few are right skewed. therefore they need to be dealt with before model creation.**","c4623789":"## Logistic Regression","6ad6caf4":"- **Here I have taken the evaluation metric as Recall as we should minimize our False Positives in this use case.**\n- **This is because It would be fatal for the patients if our model started predicting Malignant as Benign.**"}}