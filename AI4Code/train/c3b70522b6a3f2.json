{"cell_type":{"af95353d":"code","0eb858ed":"code","1fd42023":"code","36dce31e":"code","6271dde3":"code","24c739a6":"code","05cbe8a6":"code","1031aaa4":"code","6b03d4ee":"code","d0be3895":"code","ec1e25f5":"code","22bc8999":"code","a1538e23":"code","063cf962":"code","710fac32":"code","c9b22ff3":"code","e173a09b":"code","bf6f10d4":"code","c21082c3":"code","f312f906":"code","b0b19923":"code","e088b20b":"code","dbb77158":"code","57f9fc3f":"code","2b5f98c8":"code","bfd90667":"code","e902eeee":"code","ff5fc0d7":"code","5d44a412":"code","2adb0444":"code","b771f8aa":"code","71d22844":"code","eea8f7a3":"code","54da1f4e":"code","5f14bedc":"code","c050d933":"code","59516c75":"code","9ed1228b":"code","081095f6":"code","4734d725":"code","501ca756":"code","d73d8b99":"code","bd149a33":"code","062d9c5d":"code","c026280b":"code","07b3e0d4":"code","cda1e805":"markdown","01e2dab1":"markdown","1deedb62":"markdown","f0d517f6":"markdown","95163103":"markdown","26f17de0":"markdown","32a31f64":"markdown","0980c8b0":"markdown","7f3d4218":"markdown","b4dbdd13":"markdown","6cb482fe":"markdown","687a8310":"markdown","7af9af72":"markdown","59143ede":"markdown","579765bd":"markdown","59ff75a1":"markdown","1744faff":"markdown","c3362be4":"markdown","343eefa0":"markdown","94c523c2":"markdown","46185bff":"markdown","1747c417":"markdown","ef966a9c":"markdown","3293e5f1":"markdown","1842b904":"markdown","14b08b88":"markdown","46c5765b":"markdown","1d6ac7b7":"markdown","35acc18a":"markdown","d66fada6":"markdown","87eb2c1f":"markdown","d0af91f6":"markdown","3241b7b2":"markdown","3f89ed3c":"markdown"},"source":{"af95353d":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","0eb858ed":"#panel data analysis - pandas\nimport pandas as pd\n\n#numpy for operations related to numpy arrays\/series\nimport numpy as np\n\n#matplotlib & seaborn for visualisations\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n%matplotlib inline  \n#the above line of code is a magic function, enables us to display plots within our notebooks just below the code.\n\n#maths, stats and stuffs\nfrom scipy import stats","1fd42023":"train_df=pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/train.csv')\ntest_df=pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/test.csv')","36dce31e":"#.columns method gives us name of all the columns in our dataframe\ntrain_df.columns","6271dde3":"#.dtypes returns a series of all columns and their respective datatype\ntrain_df.dtypes","24c739a6":"test_df.columns","05cbe8a6":"test_df.dtypes","1031aaa4":"#dataframe.info() is like one stop destination for all info on metadata\ntrain_df.info()","6b03d4ee":"test_df.info()","d0be3895":"# by default it displays the first 5 records, any other integer can be specified within the parens as I did, giving 6.\ntrain_df.head(6)","ec1e25f5":"test_df.head(6)","22bc8999":"#describe gives us a stats about every NUMERICAL column.\ntrain_df.describe()","a1538e23":"#using select_dtypes to select specified data type\ntrain_df.select_dtypes('object').describe()   #this code gives a short statistical summary of categorical data. # object means string type here","063cf962":"# unique () used to get all distinct values from columns\ntrain_df.LotShape.unique()","710fac32":"# value_counts() is even better, gives distinct values with their frequencies.\ntrain_df.LotShape.value_counts()","c9b22ff3":"num_col=train_df.select_dtypes(exclude='object')","e173a09b":"for i in num_col.columns:\n    num_col[i].plot.hist(bins=40,color=('r'))\n    plt.xlabel(i)\n    plt.show()","bf6f10d4":"# the .corr() method helps compute correlation of columns with each other, it excludes nulls automatically\ncorrmap=train_df.corr()","c21082c3":"# .corr(), returns a correlation matrix, which is displayed below, entries are correlation values\ncorrmap","f312f906":"# below code is to get those features which have correlation greater than 0.5 with Target-SalePrice\n\nbest_corrd=corrmap.index[abs(corrmap['SalePrice'])>0.5] #-ve corr. value means they are correlated but inversely, still we need 'em, hence abs()\nprint(best_corrd)","b0b19923":"plt.figure(figsize=(12,12))\nsns.heatmap(train_df[best_corrd].corr(),annot=True,cmap='RdYlBu')","e088b20b":"# all in one plot - the seaborn pairplot\ncol= ['SalePrice', 'OverallQual', 'GrLivArea', 'GarageCars', 'TotalBsmtSF', 'FullBath', 'YearBuilt']\nsns.pairplot(train_df[col],height=1.9,aspect=0.99,diag_kind='kde')  # for each facet -> width=aspect*height & height is in inches","dbb77158":"f,ax=plt.subplots(4,2,figsize=(15,10))    #method of matplot, allows to plot 2 or more graphs in same figure\n\n#plotting OverallQual with SalePrice.. here both lineplot and barplot of OverallQual are plotted on same graph\nsns.lineplot(train_df['OverallQual'],train_df['SalePrice'],ax=ax[0,0])\nsns.barplot(train_df['OverallQual'],train_df['SalePrice'],ax=ax[0,0])\n\n#plotting TotalBsmtSF with SalePrice..\nsns.scatterplot(train_df['TotalBsmtSF'],train_df['SalePrice'],ax=ax[0,1])\n\n#plotting 1stFlrSF with SalePrice..\nsns.scatterplot(train_df['1stFlrSF'],train_df['SalePrice'],marker=\"*\",ax=ax[1,0])    #marker is the shape of the points on scatterplot\n\n#plotting GrLivArea with SalePrice..\nsns.scatterplot(train_df['GrLivArea'],train_df['SalePrice'],marker=\"+\",ax=ax[1,1])\n\n#plotting GarageCars with SalePrice..\nsns.lineplot(train_df['GarageCars'],train_df['SalePrice'],ax=ax[2,0])\nsns.barplot(train_df['GarageCars'],train_df['SalePrice'],ax=ax[2,0])\n\n#plotting GarageArea with SalePrice..\nsns.scatterplot(train_df['GarageArea'],train_df['SalePrice'],ax=ax[2,1])\n\n#plotting TotRmsAbvGrd with SalePrice..\nsns.barplot(train_df['TotRmsAbvGrd'],train_df['SalePrice'],ax=ax[3,0])\n\n#plotting YearBuilt with SalePrice..\nsns.lineplot(train_df['YearBuilt'],train_df['SalePrice'],ax=ax[3,1])\n\nplt.tight_layout()   #this automatically adjusts the placement of the plots in the figure area, without this, the figure labels were overlapping ","57f9fc3f":"train_df.drop(train_df[train_df.GrLivArea>4000].index, inplace = True)\ntrain_df.drop(train_df[train_df.TotalBsmtSF>3000].index, inplace = True)\ntrain_df.drop(train_df[train_df.GrLivArea>4000].index, inplace = True)\ntrain_df.drop(train_df[train_df.YearBuilt<1900].index, inplace = True)  # we see peak in sale price for few houses pre 1900, hence OUTLIER","2b5f98c8":"#post outlier treatment\nsns.scatterplot(train_df.GrLivArea,train_df.SalePrice)","bfd90667":"#post outlier treatment\nsns.scatterplot(train_df.TotalBsmtSF,train_df.SalePrice)","e902eeee":"#extracting our target into separate var\ny_train=train_df.SalePrice\ntrain_df.drop(columns=['SalePrice'],inplace=True)","ff5fc0d7":"train_df.columns==test_df.columns # to show train and test datasets have similar columns, so concat them and treat nulls","5d44a412":"df_merged = pd.concat([train_df, test_df], axis = 0) #axis=0 to concat along rows ; axis=1 is for columns\ndf_merged.shape","2adb0444":"#some vars, though have numerical values but are actually categorical, so convert them\ndf_merged[['MSSubClass', 'OverallQual', 'OverallCond', 'MoSold', 'YrSold']] = df_merged[['MSSubClass', 'OverallQual', 'OverallCond', 'MoSold', 'YrSold']].astype('object')\ndf_merged.dtypes.value_counts()","b771f8aa":"#columns with missing values\nmissing_columns = df_merged.columns[df_merged.isnull().any()]\nprint(missing_columns)\nprint(len(missing_columns))","71d22844":"#to find how many nulls\ndf_merged[missing_columns].isnull().sum().sort_values(ascending=False)","eea8f7a3":"# impute by \"NONE\", wherever NaN means absence of that feature in the house\n\nnone_imputer = df_merged[['PoolQC','MiscFeature','Alley', 'Fence', 'FireplaceQu', 'GarageType', 'GarageCond','GarageFinish','GarageQual','BsmtFinType2','BsmtExposure','BsmtQual','BsmtCond','BsmtFinType1','MasVnrType']]\nfor i in none_imputer.columns:\n    df_merged[i].fillna('None', inplace = True)","54da1f4e":"# filling nulls in categorical vars with mode\n\nmode_imputer =  df_merged[['Electrical', 'MSZoning','Utilities','Exterior1st','Exterior2nd','KitchenQual','Functional', 'SaleType']]\nfor i in mode_imputer.columns:\n    df_merged[i].fillna(df_merged[i].mode()[0], inplace = True)  #.mode()[0] because if var. is multimodal, then take the first one","5f14bedc":"# dealing with numericals, filling with median (robust to outliers)\n\nmedian_imputer = df_merged[['BsmtFullBath','BsmtHalfBath', 'GarageCars', 'MasVnrArea', 'GarageYrBlt', 'BsmtFinSF1', 'BsmtFinSF2', 'BsmtUnfSF', 'TotalBsmtSF', 'GarageArea','LotFrontage']]\nfor i in median_imputer.columns:\n    df_merged[i].fillna(df_merged[i].median(), inplace = True)","c050d933":"print(len(df_merged.columns))\nprint(df_merged.isnull().any().value_counts())  #checking if any nulls in any columns..","59516c75":"#checking our target variable\nsns.distplot(y_train)\nprint('SalePrice skew :',stats.skew(y_train))","9ed1228b":"# applying log trnsm on saleprice\ny_train=np.log1p(y_train)      #remember we imported numpy as np\nsns.distplot(y_train)\nprint('SalePrice skew post transformation:',stats.skew(y_train))","081095f6":"#checking skewness of other variables\nskewed = pd.DataFrame(data = df_merged.select_dtypes(exclude='object').skew(), columns=['Skew']).sort_values(by='Skew',ascending=False)\n","4734d725":"plt.figure(figsize=(6,13))\nsns.barplot(y=skewed.index,x='Skew',data=skewed)","501ca756":"#filtering numeric vars\ndf_merged_num = df_merged.select_dtypes(exclude='object')\ndf_merged_num.head(2)","d73d8b99":"# transforming vars where skew is high\ndf_trnsfmed=np.log1p(df_merged_num[df_merged_num.skew()[df_merged_num.skew()>0.5].index])\n\n#other vars which have skew<0.5\ndf_untrnsfmd=df_merged_num[df_merged_num.skew()[df_merged_num.skew()<0.5].index]\n\n#concat them\ndf_allnums=pd.concat([df_trnsfmed,df_untrnsfmd],axis=1)  #axis=1 coz conact along columns\n\ndf_merged_num.update(df_allnums)\ndf_merged_num.shape","bd149a33":"#filtering only those which are categorical in type\ndf_merged_cat=df_merged.select_dtypes(exclude=['int64','float64'])\ndf_merged_cat.head()","062d9c5d":"#encoding the vars using pandas get dummies, get_dummies encodes all cat. vars.\ndf_dummy_cat=pd.get_dummies(df_merged_cat)","c026280b":"#final merging of normalised numerical vars and encoded categorical vars.\ndf_final_merge=pd.concat([df_merged_num,df_dummy_cat],axis=1)","07b3e0d4":"# the above final_merge contains both train & test data(remember we combined both), time to separate them now\ndf_train_final = df_final_merge.iloc[0:1438, :] # first 1438 rows were train data\ndf_test_final = df_final_merge.iloc[1438:, :]   #all rows below 1438 were test data\nprint(df_train_final.shape)\nprint(df_test_final.shape)\nprint(y_train.shape)      #our target from train data we separated","cda1e805":"**There is our correlation heatmap.. Now analysing it**\n\n* As color of the cell shifts towards blue, means it is highly correlated compared to others, to target (also indicated by the value in it)\n\n* Looking at the map, we see :\n\n    * *OverallQual*, *TotalBsmtSF*, *GrLivArea*, *GarageArea* are highly correlated to our target-SalePrice.\n    \n    ","01e2dab1":"**CHECKING RELATIONSHIP OF SAID FEATURES TO TARGET INDIVIDUALLY**","1deedb62":"## **BEGIN ANALYSIS....**","f0d517f6":"* Variables with high number of null values must be dropped.\n\n\n* **BUT** , in the description of dataset, it is given that nulls have meaning, eg : PoolQC (decribes pool quality), where NaN in PoolQC means there is NO POOL or 0 pool in the house, which is actually relevant data. Similarly NaN in Fence means there is NO Fence.","95163103":"As seen from above, now our target is forced to follow bell shape by applying log transform also, its skewness is within acceptable range now.","26f17de0":"Thank you !\n\n~ Varun","32a31f64":"**OUR OBJECTIVE**\n\n* As seen in above output of **test_df.info()**, there is only 1 column missing that is the \"SALE PRICE\" column\n* Our very obejctive is to predict that \"SALE PRICE\" column for the test data, based on our analysis on the train data.\n* Hence, SALE PRICE is our target in this analysis.\n* Our analysis majorly includes finding out, which all features (columns) have a relation to\/affect the SALE PRICE column, and using those features in our prediction, eliminating rest of the columns in the process.","0980c8b0":"Visualising distribution of data is best done using histograms. So what is a histogram ?\n\n* Histogram forms bins across the range of data, and plots bars which represents the no. of data that fall within each of those bins formed.","7f3d4218":"### Thats all folks !\n\n\n* **Now your data is ready to be trained & tested ! Until this step, we performed what is known as pre-processing the data which is a major part of any analysis and prediction.**\n\n\n* **The next step would be to import necessary suitable models, fit the training data, evaluate the model and then finally predict the  target i.e SalePrice for the test data.**","b4dbdd13":"## **INSIGHTS USING DATA VISUALISATION**","6cb482fe":"## Importing necessary libraries\n\nAs usual, first step is to** import all necessary packages** which we'll be needing in our analysis.","687a8310":"The above is a short statistical summary of various numerical columns from the dataframe. Looking at the SALE PRICE, we get several info on it, such as :\n\n* mean sale price is 180921.19\n\n* sale price has a std. deviation of 79442.50. Std. deviation denotes dispersion among values, i.e by how much the values differ from the central tendency.\n\n* min. sale price is 34900\n\n* max is sale price is 755000\n\n","7af9af72":"The above pairplot is a informative one :\n\n* We can analyse whether the target is normally distributed. In our case it is not, SalePrice is skewed.\n\n\n* Distribution of predictor variables themselves.\n\n\n* We can observe if our target-the SalePrice is linearly related to our predictor variables.\n\n    * This linear relationship is clearly visible across most variables we plotted.\n\n\n* Not only that, we can also observe MULTI-COLLINEARITY, i.e. if the the predictor variables being correlated among themselves.\n\n    * Eg: plot between ( GrLivArea and TotalBsmtSF ), ( OverallQual and GrLivArea ) likewise.","59143ede":"From above, it is clear that \n* Our train data has numerous columns- 81 in total\n* Range of index is 1460 (this is the max no. of entries, for simplicity we'll take it as the no. of rows)\n* With several columns missing many entries (null values). For eg. looking at Garage Type, it has 1379 non-null entries, meaning 81(1460-1379) null values.\n* Different columns are of different datatypes, here we get a glance of which is numerical and which is categorical feature just by looking at the name of column and its data type.","579765bd":"**CATEGORICAL DATA**\n\n* The type of data which can be categorised, or the values of which can be fit into different groups\/categoris is called categorical data.\n\n* eg: gender column, where there are only 2 categories, male-female.","59ff75a1":"**INSIGHTS INTO THE DATA**\n\n* Before begining with any analysis, it's important to have a decent knowledge about the dataset, what type of analysis on which type of data needs to be done, and what our target\/objective is.","1744faff":"* As seen above, our target variable is not normally distributed - it ain't symmetric and also not a proper bell shape as it shows peak (pointy top).\n* If skew between -0.5 and 0.5, it can be taken as near symmetrical data.\n* If skew is in between (-0.5 & -1) or between (0.5 & 1), it is said to be moderately skewed.\n* Where skewness is less than -1 and greater than 1, means highly skewed, such as our target SalePrice which has skew=1.5+\n* However a variable which is not normally distributed or which is SKEWED may be forced to be normal by applying LOG transformation. ","c3362be4":"**UNDERSTANDING THE DATASETS GIVEN**\n\n* We've been given 3 files, namely \"train.csv\", \"test.csv\" & \"sample_submission.csv\".\n* We will do our analysis on the train data.\n* Based on our analysis, make our predictions for the test data.\n* Generate an output file based on sample_submission data format.","343eefa0":"## *About the kernel*\n\n* Aim of this kernel is to do exploratory data analysis (EDA) of Kaggle competition data - House Prices:Advanced Regression Techniques\n\n* **I've explained every step in detail without making things complex, to make concepts easy to understand for beginners.**\n\n* Hope you find this kernel useful.\n\n* My other kernel : [titanic machine learning from disaster competition](https:\/\/www.kaggle.com\/cvarun\/titanic-survival-a-beginner-s-analysis)","94c523c2":"**Now, finding the relation of features with our target - Sale Price**\n\n* This involves, plotting scatter plots, between our target(Sale Price) and other predictor features(other than Sale Price).\n\n* And to analyse whether the said predictor feature has any relation to our target.\n\n* Before that, there is way to see, if the predictor fearures are related to the target-SALEPRICE. This can be visualised using correlation heatmap.","46185bff":"## **Encoding of Categorical Variables**\n\n* We know what categorical vars are.\n* We encode categorical vars, so as to make it understandable to the machine.","1747c417":"It is a good practice to self identify columns with numerical data, looking at the values, data types. If not pandas provides us with everything.","ef966a9c":"The above summary of categorical vars, gives us several imp info, such as :\n\n* Take for eg: the LotShape column:\n\n    * It has 4 unique vales (displayed below)\n    \n    * With \"Reg\" being the top shape, which occurs 925 times out of 1460","3293e5f1":"## **Null Values Treatment**\n\n* Null values are a hinderance in analysis and prediction, hence needs to be dealt with.\n\n* Best way is to, replace the nulls with the central tendency or the most representative value of that variable, which may be \n    * Mean (avg) - for numerical vars.\n    * Median (one at the center) - preferred where data is distorted, has outliers etc. **MEDIAN IS IMMUNE TO OUTLIERS**\n    * Mode (most occuring) - preferred for categorical vars.\n    \n\n* But, clear knowledge of the data is required before replacing them with central tendency, each of which has its own drawbacks.\n\n\n* Since both training and testing data has nulls, instead of treating them separately we'll combine them and treat them after taking out SalePrice out.","1842b904":"**EXPLANATION OF ABOVE PLOTS**\n\n* OverallQual has a strong near-linear relation to the SalePrice.\n\n* The scatter plot of TotalBsmtSF also shows relation to SalePrice, i.e as TotalBsmtSF increases, so does SalePrice.\n\n* 1stFlrSF, GarageArea, GarageCars & GrLivArea also have a strong correlation to SalePrice.\n\n* TotRmsAbvGrd also has a linear relation to our target !\n\n* Looking at YearBuilt-SalePrice plot, though the relation with target is not as strong as the others, still we can see new houses have high sale price compared to older ones (with some exceptions-outliers-explained later).\n\n## -------------------------------------\n\nIt is also important to know what the features are, a logical understanding of these features...\n\n* Looking at GarageCars and GarageArea...\n    \n    * GarageCars is the number of cars that can be fitted into a given GarageArea, so basically they are the same (sum of area under all the cars = area of entire garage)... Hence we won't need both of these features, just one is enough, and we shall **retain GarageCars** since it is strongly correlated to our target than GarageArea.\n    \n* Looking at TotRmsAbvGrd and GrLivArea\n    \n    * Similarly, TotRmsAbvGrd which is Total Rooms above ground, and GrLivArea which is living area above ground, these 2 are also the same, (rooms above ground, living area above ground are ofcourse the same thing !) hence we **retain only GrLivArea**, which has higher correlation to target.\n    \n    \n    ","14b08b88":"## **Outlier Treatment of Highly Correlated Features**\n\n* Before we proceed, a little bit about outliers.\n\n\n* Outliers are the datapoints\/observations that lie outside or are far away from the main group of data. (imagine them being the odd one out in the group)\n\n    * In above plot of SalePrice vs TotalBsmtSF, we see few dots that are away from the main group of dots, these OUTLIERS are beyond 3000 on x-axis, which have to be removed.\n    \n    * Similarly in SalePrice vs GrLivArea, we see few markers away from the main group, they are beyond 4000 on x-axis and need to be removed.\n\n\n* Outlier treatment involves dropping those extreme values. ","46c5765b":"* As we see lot of vars have skewness more than 1, which means they are highly skewed !\n* Hence we shall transform all such vars. , force them to follow normal distribution by applying log transformation.","1d6ac7b7":"**NUMERICAL TYPE DATA**\n\n* Simple definition : that data which is expressed as numbers, such data have meaning on measurement, eg : height, weight, age, income, stock price etc. \n\n* Numerical data is further split as discrete numerical & continuous numerical.\n\n    * Discrete numerical data takes on values that can be counted, the values are distinct, separate & the list\/range of values it can take may be finite(1,2,3,4) or may go upto infinite(1,2,3,4.....). eg: no of people living in a colony.\n    * Continuous numerical data can not be counted, only measured and can be described using intervals. eg: temperature of a given place, over a period.","35acc18a":"Reading the train and test datas into separate dataframes, as below.","d66fada6":"As seen above, all nulls have been taken care of.","87eb2c1f":"## **Normality of Data**\n\n* Normality is a preferred property in analytics and modelling.\n* There are many reasons for this preference, but to put simply normally distributed data makes our job easier.\n* Normally distributed data is like a bell curve, symmetrical in nature.","d0af91f6":"* The above correlation matrix, is really insightful but is a bit lengthy to analyse.\n\n* But, it can be understood easily using visualisation.\n\n* We shall now, plot the above corr. matrix using heatmap.","3241b7b2":"* The above plots are all histograms.\n\n\n* Looking at the above plots, it is now even clear which all feature are continuous and which ones discrete.\n\n\n* eg: look at histogram plot of YearBuilt, BsmtUnfSF are continuous in range, while OverallQual, BsmtHalfBath are discrete.\n\n\n* NOTE : Some of the features though may have numerical values, may come under categorical.\n","3f89ed3c":"**BASIC Analysis**"}}