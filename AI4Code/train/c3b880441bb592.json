{"cell_type":{"1da68f0a":"code","cb73bba8":"code","700c00a5":"code","0ecc9e3f":"code","8803ceb5":"code","7d8fdda7":"code","4eeebdf4":"code","0555dca1":"code","626dcdb3":"code","40385159":"code","e9314f5b":"code","b6ac7c95":"code","c247637a":"code","0d2fd607":"code","3da4f1a7":"code","6a7cf510":"code","94a70d8c":"code","d3a91f12":"code","35a72a4b":"code","39252706":"code","d0c074a2":"code","277b0fc6":"code","cb220500":"code","6a998cbd":"code","4ad754c6":"code","3afd4745":"code","649a34ea":"code","37fbb4ad":"code","6dc9b423":"code","65e6dcdc":"code","1027bcef":"code","1fae2e08":"code","f4b514ee":"code","d6a4820a":"code","f0bdb0ff":"code","99d36438":"code","54d61af5":"code","dd445701":"code","1034e6b7":"code","e072ac6f":"code","a72dcba7":"code","9fd28502":"code","bcce6832":"code","5a7868e5":"code","2483ee53":"code","e93f883f":"code","076a7507":"code","b96faf36":"code","7a2ddb13":"code","93a8cf2a":"code","c3397895":"code","938c2780":"code","f2b076cd":"code","72087815":"code","dae539d7":"code","5029db09":"code","89c14c84":"code","413a4b25":"code","ffc4b4f1":"code","c3559c91":"code","91d27033":"code","c82e5ddf":"code","5cc04008":"code","42d479c3":"code","e03c149e":"code","901f3ef5":"code","332139c2":"code","e3e12ed0":"code","b542558a":"code","ab957040":"code","ccf7ade9":"code","d8c624f1":"code","dda0068e":"code","b79338fa":"markdown","ebad4cb6":"markdown","7e167523":"markdown","94c9888c":"markdown","3cb772e6":"markdown","8a28f3d1":"markdown","72dc0009":"markdown","611fac9f":"markdown","6049bdd2":"markdown","567b78f2":"markdown","7f8bb23a":"markdown","214b7c30":"markdown","18476941":"markdown","30bcf0e8":"markdown","dc10f79c":"markdown","ef1b0856":"markdown","c2f85d6c":"markdown","eb54208b":"markdown","0f153b2d":"markdown","82f11ada":"markdown","8425a671":"markdown","f6d46c5a":"markdown","9b2ee5e2":"markdown"},"source":{"1da68f0a":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom IPython.display import clear_output\n%matplotlib inline\npd.set_option('display.float_format', lambda x: '%.5f' % x)","cb73bba8":"df = pd.DataFrame({\"x\":np.arange(101)}) # Creates data frame with x ranging from 0 to 100","700c00a5":"df[\"y\"] = df[\"x\"] * 11 # adding the target feature y, which is x*11 which makes it perfect for linear model","0ecc9e3f":"arr = df.values\nx = arr[:,0]\ny = arr[:,1]","8803ceb5":"# Initializing parameters to 0\ntheta0 = 0 #intercept\ntheta1 = 0","7d8fdda7":"learning_rate = 0.0001","4eeebdf4":"loss_df = pd.DataFrame() # data frame to track the loss and parameter values for each iteration","0555dca1":"same = 0 # variables that check if the loss is same as its previous and ends the loop if the loss value is same for 10 times\ni=0\nwhile same < 10:\n    loss_df_0 = pd.DataFrame()\n    loss = (np.mean(((theta0 + theta1 * x) - y) ** 2)) # Mean squred error\n    loss_df_0 = pd.DataFrame({\"iter\":pd.Series(i),\"t0\":pd.Series(theta0),\"t1\":pd.Series(theta1),\"loss\":pd.Series(loss)})\n    if i == 0:\n        loss_df = loss_df.append(loss_df_0)\n    t0 = theta0 - (learning_rate * np.mean((theta0 + theta1 * x) - y)) #updating theta0 or the intercept\n    t1 = theta1 - (learning_rate * np.mean(((theta0 + theta1 * x) - y) * x)) #updating theta1\n    theta0 = t0\n    theta1 = t1\n    if np.round(loss,5) == np.round(loss_df.iloc[-1,3],5): #incrementing variable 'same' if the loss is same as it's previous\n        same += 1\n    if i > 0:\n        loss_df = loss_df.append(loss_df_0)\n    i += 1\n    print(loss_df.tail())\n    clear_output(wait = True)","626dcdb3":"loss_df.plot(x=\"iter\",y=\"loss\") # plotting the loss function for all the iterations\nplt.title(\"Loss Per Iteration\")\nplt.xlabel(\"Iteration\")\nplt.ylabel(\"Loss\")\nplt.show()","40385159":"loss_df.tail(10) # inspecting last few rows of loss data frame","e9314f5b":"final_t0 = np.round(loss_df.iloc[-1,1],2) # storing theta0\nfinal_t1 = np.round(loss_df.iloc[-1,2],2) # storing theta1\n\nfinal_t0, final_t1","b6ac7c95":"df[\"Pred\"] = final_t0 + final_t1 * df[\"x\"] #making predictions on training data (in sample)\ndf[\"y\"].plot(kind=\"kde\",legend=True)\ndf[\"Pred\"].plot(kind=\"kde\",legend=True)\nplt.show()","c247637a":"#MSE\nnp.mean(((df[\"y\"]-df[\"Pred\"])**2))","0d2fd607":"np.random.seed(11)\ndf = pd.DataFrame({\"x1\":np.arange(10,111),\"x2\":np.arange(100,201),\"x3\":np.arange(200,301),\"x4\":np.arange(800,901),\"x5\":np.linspace(-200,200,101),\n                  \"x6\":np.random.randint(100,10000,101)}) #generating dataset for the problem","3da4f1a7":"# generating the target feature y using some random linear computations which makes it perfect for linear regression\ndf[\"y\"] = (df[\"x1\"] * 111) + (df[\"x2\"] * 20) - (0.002 * df[\"x3\"]) - (1.002 * df[\"x4\"]) + (1.5 * df[\"x5\"]) - (19.5 * df[\"x6\"])","6a7cf510":"learning_rate = 0.5 # initializing the learning rate, alpha","94a70d8c":"df.head()","d3a91f12":"arr = df.values","35a72a4b":"X = arr[:,:-1].copy() # storing input features in X","39252706":"#Feature scaling helps the algorith converge faster, hence sacling the input features within range [0,1]\nfor i in range(X.shape[1]):\n    X[:,i] = X[:,i] \/ np.max(X[:,i])","d0c074a2":"all_ones = np.ones_like(arr[:,0]).reshape(-1,1)\nX = np.append(all_ones,X,axis=1) # adding a vector of ones before the matrix X","277b0fc6":"X[:5,:] ","cb220500":"y = arr[:,-1].reshape(-1,1) # storing the target feature in y","6a998cbd":"theta = np.zeros_like(X[0,:]).reshape(-1,1) # initializing the theta vector to 0s","4ad754c6":"loss_df = pd.DataFrame() # data frame to track losses per iteration","3afd4745":"same = 0 # variable that tracks if the loss is same as previous loss\ni=0\nwhile same < 10:\n    loss_df_0 = pd.DataFrame()\n    loss = np.mean((np.dot(X,theta)-y) ** 2) #mean squared error.\n    err = np.dot(X,theta)-y #error vector of mx1 dim\n    theta = theta - learning_rate * (1\/X.shape[0])*(np.dot(err.T,X)).reshape(-1,1) #updating theta in a single step using matrix operations\n    loss_df_0.insert(0,\"iter\",pd.Series(i))\n    for j in range(len(theta)):\n        loss_df_0.insert((j+1),(\"t\"+str(j)),pd.Series(theta[j]))\n    loss_df_0.insert((j+2),\"loss\",pd.Series(loss))\n    \n    \n    if i == 0:\n        loss_df = loss_df.append(loss_df_0)\n    if np.round(loss,5) == np.round(loss_df.iloc[-1,-1],5):\n        same += 1 #increments if loss is same as revious loss and breaks the loop once it's 10\n    if i > 0:\n        loss_df = loss_df.append(loss_df_0)\n    i += 1\n    print(loss_df.tail())\n    clear_output(wait = True)","649a34ea":"loss_df.plot(x=\"iter\",y=\"loss\") # plotting loss per iteration\nplt.show()","37fbb4ad":"theta # the final coefficients which minimize the loss","6dc9b423":"df[\"Pred\"] = np.dot(X,theta) #adding predictions to data frame","65e6dcdc":"df[\"y\"].plot(kind=\"kde\",legend=True)\ndf[\"Pred\"].plot(kind=\"kde\",legend=True)\nplt.show()","1027bcef":"#MSE\nnp.mean(((df[\"y\"]-df[\"Pred\"])**2))","1fae2e08":"loss_df_re = loss_df.melt(id_vars=[\"iter\",\"loss\"]).copy()\nsns.relplot(kind=\"line\",x=\"value\",y=\"loss\",data=loss_df_re,col=\"variable\",col_wrap=4) # plotting loss per parameter\nplt.show()","f4b514ee":"df = pd.DataFrame({\"x\":np.arange(101)}) # generating data for the problem","d6a4820a":"np.random.seed(11)\ndf[\"y\"] = (df[\"x\"] ** 2) + np.linspace(1000,10000,len(df[\"x\"])) # adding y feature to make it a binomial function","f0bdb0ff":"sns.regplot(x=\"x\",y=\"y\",data=df,marker=\"+\") # the plot shows a non-linear relationship b\/w input and target features\nplt.show()","99d36438":"df.insert(1,\"x_sq\",df[\"x\"]**2) # since the relationship in binomial function, adding x^2  term to input features","54d61af5":"df.head()","dd445701":"#feature scaling speeds up the execution\ndf[\"x\"] = df[\"x\"] \/ df[\"x\"].max()\ndf[\"x_sq\"] = df[\"x_sq\"] \/ df[\"x_sq\"].max()","1034e6b7":"arr = df.values","e072ac6f":"x = arr[:,0]\nxsq = arr[:,1] # storing input features in variables","a72dcba7":"y = arr[:,2] # target feature","9fd28502":"theta0 = 0\ntheta1 = 0\nthetasq = 0 # initializing theta variables ton 0","bcce6832":"learning_rate = 1.415","5a7868e5":"loss_df = pd.DataFrame()","2483ee53":"same = 0\ni=0\nwhile same < 10:\n    loss_df_0 = pd.DataFrame()\n    loss = (np.mean(((theta0 + theta1 * x + thetasq * xsq) - y) ** 2))\n    loss_df_0 = pd.DataFrame({\"iter\":pd.Series(i),\"t0\":pd.Series(theta0),\"t1\":pd.Series(theta1),\"t_sq\":pd.Series(thetasq),\"loss\":pd.Series(loss)})\n    if i == 0:\n        loss_df = loss_df.append(loss_df_0)\n    t0 = theta0 - (learning_rate * np.mean((theta0 + theta1 * x + thetasq * xsq) - y))\n    t1 = theta1 - (learning_rate * np.mean(((theta0 + theta1 * x + thetasq * xsq) - y) * x))\n    tsq = thetasq - (learning_rate * np.mean(((theta0 + theta1 * x + thetasq * xsq) - y) * xsq))\n    theta0 = t0\n    theta1 = t1\n    thetasq = tsq\n    if np.round(loss,5) == np.round(loss_df.iloc[-1,4],5):\n        same += 1\n    if i > 0:\n        loss_df = loss_df.append(loss_df_0)\n    i += 1\n    print(loss_df.tail())\n    clear_output(wait = True)","e93f883f":"loss_df.plot(x=\"iter\",y=\"loss\") #plotting loss vs iteration\nplt.show()","076a7507":"final_t0 = np.round(loss_df.iloc[-1,1],2)\nfinal_t1 = np.round(loss_df.iloc[-1,2],2)\nfinal_tsq = np.round(loss_df.iloc[-1,3],2)\nfinal_t0, final_t1, final_tsq","b96faf36":"df[\"Pred\"] = final_t0 + final_t1 * df[\"x\"] + final_tsq * df[\"x_sq\"] #adding prediction to data frame","7a2ddb13":"df[\"y\"].plot(kind=\"kde\",legend=True)\ndf[\"Pred\"].plot(kind=\"kde\",legend=True)\nplt.show()","93a8cf2a":"df[[\"y\",\"Pred\"]].plot() #actual y vs prediction\nplt.show()","c3397895":"#MSE\nnp.mean(((df[\"y\"]-df[\"Pred\"])**2))","938c2780":"loss_df_re = loss_df.melt(id_vars = [\"iter\",\"loss\"]).copy()\nsns.relplot(kind=\"line\",x=\"value\",y=\"loss\",col=\"variable\",data=loss_df_re) # loss per parameter\nplt.show()","f2b076cd":"df = pd.DataFrame({\"x1\":np.arange(1,101)}) #generating dataset of marks","72087815":"df[\"y\"] = 0\ndf.loc[df[\"x1\"]>=50,\"y\"]=1 # adding target feature showing pass or fail. 1 is pass and 0 is fail","dae539d7":"learning_rate = 11","5029db09":"df.head()","89c14c84":"arr = df.values\narr = np.array(arr,dtype=np.float64)","413a4b25":"X = arr[:,:-1].copy() # storing input features in X","ffc4b4f1":"# Feature Scaling\nfor i in range(X.shape[1]):\n    X[:,i] = X[:,i] \/ np.max(X[:,i])","c3559c91":"all_ones = np.ones_like(arr[:,0]).reshape(-1,1)\nX = np.append(all_ones,X,axis=1) # add ones before the matrix X","91d27033":"X[:5,:]","c82e5ddf":"y = arr[:,-1].reshape(-1,1) # storing target feature in y","5cc04008":"theta = np.zeros_like(X[0,:]).reshape(-1,1) # initializing theta to 0s","42d479c3":"loss_df = pd.DataFrame() #dataframe to track loo per iteration","e03c149e":"same = 0\ni=0\nwhile same < 100:\n    loss_df_0 = pd.DataFrame()\n    hx = 1 \/ (1+np.exp(-np.dot(X,theta))) # h(x)=1\/(1+e^-(theta0*x0+theta1*x1...+thetan*xn))\n    \n    loss = np.mean((y*np.log(hx)) + ((1-y)*np.log(1-hx)))*-1 #-1\/m((y1*log(h(x)1) + ((1-y1)*log(1-h(x)1)))))\n    err = hx-y #error vector of mx1 dim\n    theta = theta - learning_rate * (1\/X.shape[0])*(np.dot(err.T,X)).reshape(-1,1) #same as in linear regression\n    loss_df_0.insert(0,\"iter\",pd.Series(i))\n    for j in range(len(theta)):\n        loss_df_0.insert((j+1),(\"t\"+str(j)),pd.Series(theta[j]))\n    loss_df_0.insert((j+2),\"loss\",pd.Series(loss))\n\n    \n    if i == 0:\n        loss_df = loss_df.append(loss_df_0)\n    if np.round(loss,5) == np.round(loss_df.iloc[-1,-1],5):\n        same += 1\n    if i > 0:\n        loss_df = loss_df.append(loss_df_0)\n    i += 1\n    print(loss_df.tail())\n    clear_output(wait = True)","901f3ef5":"loss_df.plot(x=\"iter\",y=\"loss\") #plotting loss vs iteration\nplt.show()","332139c2":"theta #final parameters","e3e12ed0":"df[\"Pred\"] = 1 \/ (1+np.exp(-np.dot(X,theta))) #adding predictions on training set using sigmoid function to dataframe","b542558a":"df[[\"y\",\"Pred\"]].plot(figsize=(10,10)) #plotting prediction vs actual y\nplt.axhline(y=0.5,xmin=0,xmax=100,color=\"red\")\nplt.text(x=0.1,y=0.51,s=\"0.5\")\nplt.show()","ab957040":"df.loc[df[\"Pred\"]<0.5,\"Pred\"]=0\ndf.loc[df[\"Pred\"]>=0.5,\"Pred\"]=1 # h(x)>=0.5 means y=1 else y=0","ccf7ade9":"df[[\"y\",\"Pred\"]].plot(figsize=(10,10)) # plotting prediction vs actual y after converting it into binary output\nplt.show()","d8c624f1":"loss_df_re = loss_df.melt(id_vars=[\"iter\",\"loss\"]).copy()\nsns.relplot(kind=\"line\",x=\"value\",y=\"loss\",data=loss_df_re,col=\"variable\") # plotting loss per parameter\nplt.show()","dda0068e":"pd.crosstab(index=df[\"Pred\"],columns=df[\"y\"],values=df[\"y\"],aggfunc=\"count\").fillna(0) # confusion matrix","b79338fa":"As we are following vectorized operationm which is matrix multiplication we are inserting 1s before the input matrix which is like x0, and it gets multiplied with the intercept term","ebad4cb6":"Below are the final parameters that has the minimum loss.","7e167523":"<h2>The code below can be used as a template to solve any multiple linear regression problem with any number of input variables provided the input variables are named x1, x2,...,xn and the output variable is named y and is placed to the end of the input dataframe","94c9888c":"<u><h1>Now let's solve a MULTIPLE LINEAR REGRESSION problem with Gradient Descent<\/h1><\/u>\n\n<h2>The process is similar to the one we used for simple linear regression, the only difference is we need to find multiple parameters. Since the execution time increases with the number of input variables, we follw a vectorized operation to speed up the execution\n\n\n","3cb772e6":"Since the prediction of logistic regression is a probability of output variable 'y'=1. We change the prediction greater than equal to 0.5 to 1 else 0","8a28f3d1":"<u><h1>Now let's solve a POLYNOMIAL REGRESSION problem with Gradient Descent<\/h1><\/u>\n\n<h2>The process is similar to the one we used for multiple linear regression, the only difference is we don't use vectorized operation and we need to add a polynomial transform by adding a squared of input variable, since it is a polynomial of degree 2.<\/h2>\n    \n    \n<h2>The code below can be used to solve any polynomial regression problem of degree 2 and 1 input variable provided the input variable is named x and the output variable is named y and is placed to the end of the input dataframe.","72dc0009":"<u><h1>Now let's solve a LOGISTIC REGRESSION problem with Gradient Descent<\/h1><\/u>\n\n<h2>The process is similar to the one we used for multiple linear regression, the only difference is we use a different cost function.<\/h2>\n    \n    \n<h2>The code below can be used to solve any logistic regression problem with any number of input variables provided the input variables are named x1, x2,...,xn and the output variable is named y and is placed to the end of the input dataframe","611fac9f":"Learning rate specifies the pace at which the gradient moves. A high learnig rate may cause the loss function to increase for every iteration and hence the gradient may never converge to the minimum. A low learning rate may take very long for the gradient to converge to the minimum.\n\nHence, it is advised to plot\/visualize the loss function per iteration. If the loss function increases then lower the learning rate. If the loss function decreases at a very slow pace, the increase the learning rate.\n\nWe'll initialize the parameters\/coefficients to 0 and start gradient descent","6049bdd2":"We are applying the parameters to the training set to see if the algorithm could find the parameters perfectly","567b78f2":"MSE is close to 0 and shows that the actual y and prediction are same.","7f8bb23a":"<h2>The code below can be used as a template to solve any simple linear regression problem provided the input variable is named x and the output variable is named y and is placed to the end of the input dataframe.","214b7c30":"We can see the loss reducing per iteration","18476941":"We'll end the algorithm when there is no change in the loss continuously for 10 iterations. It means the gradient has converged to the minimum","30bcf0e8":"Gradient Descent is an optimization algorithm that iteratively finds the optimal parameters of a function such that the loss\/cost function reduces after every iteration.\n\nFunction here is the mapping function between input and target variables (for example linear regression: b<sub>0<\/sub>+b<sub>1<\/sub>x<sub>1<\/sub>+...+b<sub>n<\/sub>x<sub>n<\/sub>). \n\nParameters are the coefficients of the mapping function (for example b<sub>0<\/sub>, b<sub>1<\/sub>,..b<sub>n<\/sub>).\n\nCost\/loss function is a metric to quantify the error (difference between actual output and the output predicted by the mapping function). For linear regression problems it may be Mean squared error, Root mean squared error, Mean absolute error, etc.\n\nFor linear regression problem imagine a loss function to be a bowl-shaped\/convex\/U-shaped function with loss on the y-axis and the coefficient on the x-axis. The job of Gradient descent is to find the set\/combination of coefficients that minimize the cost function i.e get the loss to the bottom of the bowl\/U","dc10f79c":"The confusion matrix also shows the predictions are same","ef1b0856":"We are scaling the input features to speed up the execution","c2f85d6c":"The plot above shows that the actual y and predictions are exactly matching. We'll also calculate the RMSE","eb54208b":"<h1>Solving Linear, Polynomial, and Logistic Regression with Gradient Descent<\/h1>","0f153b2d":"MSE is close to 0 which shows a perfect prediction","82f11ada":"The plot shows a perfect prediction","8425a671":"The plot shows the target variable is same as prediction","f6d46c5a":"<u><h1>First let's start with solving a SIMPLE LINEAR REGRESSION problem using Gradient Descent<\/h1><\/u>\n\n<h3>We'll work on a synthetic dataset so that it fits perfectly to the predictions and we'll compare the fit of predictions on the training set only, as the test set would also be of the similar kind it is not compare the predictions on a test set.","9b2ee5e2":"MSE also is close to 0 which shows a perfect prediction. Below is the loss function for each of the input parameter"}}