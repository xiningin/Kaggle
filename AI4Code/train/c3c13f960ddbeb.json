{"cell_type":{"70581239":"code","339ce512":"code","4fe5ed3a":"code","55e3cc01":"code","9bd94edb":"code","a500d032":"code","ec8a27aa":"code","94b53ba9":"code","910cb328":"code","3dea1533":"code","b0bde3a3":"code","cb3c947b":"code","e29652ce":"code","f39eb765":"code","d6c2ebc7":"code","a042c501":"code","8f3786ce":"code","f7eff859":"code","5a19a6e9":"code","029c7973":"code","014a29fc":"code","95afaf62":"code","a33c6284":"code","0c11d927":"code","5627ee65":"code","d63c0d2e":"code","a4df80b2":"code","652e65d5":"code","fccf814b":"code","9d68f6f8":"code","eda9dad9":"code","157351b4":"code","406b3c4e":"code","1765e24a":"code","05bcbdf6":"code","72f8a958":"code","d917c96a":"code","3f1cd89c":"code","7367775a":"code","0916412c":"code","f70bdeba":"code","0a2a6bcb":"code","88d272ec":"code","5717e2ff":"code","cfd358b0":"code","066b3e90":"markdown","445adfae":"markdown","80e49263":"markdown","498fb02c":"markdown","9e55be4d":"markdown","d2a4200d":"markdown","846d0a41":"markdown","c2aa7f36":"markdown","a91d1d0b":"markdown","7b239c74":"markdown","24b075cc":"markdown","3a863080":"markdown"},"source":{"70581239":"import numpy as np\nimport pandas as pd\nimport cv2\nimport matplotlib.pyplot as plt\nimport matplotlib.patches as patches\nfrom tensorflow.keras.preprocessing.image import load_img, img_to_array\nimport os\nimport tensorflow as tf\nimport seaborn as sns","339ce512":"img_path=os.path.join(\"\/kaggle\/input\/face-mask-detection-dataset\/Medical mask\/Medical mask\/Medical Mask\/images\")\nanno_path=os.path.join(\"\/kaggle\/input\/face-mask-detection-dataset\/Medical mask\/Medical mask\/Medical Mask\/annotations\")\ntrain_df = pd.read_csv('..\/input\/face-mask-detection-dataset\/train.csv')\nsubmission = pd.read_csv('..\/input\/face-mask-detection-dataset\/submission.csv')\nprint(len(os.listdir(img_path)))\nprint(len(os.listdir(anno_path)))","4fe5ed3a":"#sorting images based on indexes\nimages=os.listdir(img_path)\nannot=os.listdir(anno_path)\nimages.sort()\nannot.sort()","55e3cc01":"train_images=images[1698:]\ntest_images=images[:1698]","9bd94edb":"len(train_images)+len(test_images)","a500d032":"img=plt.imread(os.path.join(img_path,train_images[3]))\nplt.imshow(img)\nplt.show()","ec8a27aa":"img=plt.imread(os.path.join(img_path,train_images[10]))\nplt.imshow(img)\nplt.show()","94b53ba9":"train_df.head()","910cb328":"train_df['name'].nunique()","3dea1533":"#Total classes\ntrain_df['classname'].nunique()","b0bde3a3":"train_df['classname'].value_counts()","cb3c947b":"ax = sns.catplot(x='classname',kind='count',data=train_df,orient=\"h\",height=10,aspect=2)\nax.fig.suptitle('Count of Classnames',fontsize=16,color=\"r\")\nax.fig.autofmt_xdate()","e29652ce":"train_df['name'].value_counts().max()","f39eb765":"train_df.groupby(['name']).count().sort_values(['x1'])","d6c2ebc7":"box=[]\nfor i in range(len(train_df)):\n    arr=[]\n    for j in train_df.iloc[i][['x1','x2','y1','y2']]:\n        arr.append(j)\n    box.append(arr)\ntrain_df['box']=box","a042c501":"def get_boxes(id):\n    boxes=[]\n    for i in train_df[train_df[\"name\"]==str(id)][\"box\"]:\n        boxes.append(i)\n    return boxes","8f3786ce":"print(get_boxes(train_images[0]))","f7eff859":"image = '1914.jpg'\nimg=plt.imread(os.path.join(img_path, image))\nfig, ax = plt.subplots(1)\nax.imshow(img)\nboxes=get_boxes(image)\nfor box in boxes:\n    rect = patches.Rectangle((box[0],box[1]),box[2]-box[0],box[3]-box[1],linewidth=2,edgecolor='r',facecolor='none')\n    ax.add_patch(rect)\nplt.show()","5a19a6e9":"img_size=50\nx=[]\ny=[]\nfor i in range(len(train_df)):\n    arr=[]\n    for item in train_df.iloc[i]:\n        arr.append(item)\n    img = cv2.imread(os.path.join(img_path, arr[0]), cv2.IMREAD_GRAYSCALE)\n    img = img[arr[2]:arr[4], arr[1]:arr[3]]\n    try:\n        img = cv2.resize(img, (img_size, img_size))\n    except:\n        continue\n    x.append(img)\n    y.append(arr[5])","029c7973":"with_mask =['face_with_mask']","014a29fc":"#Creating a seperate copy of data\nx_mask=[]\ny_mask=[]\nfor i in range(len(x)):\n    x_mask.append(x[i])\n    y_mask.append(y[i])","95afaf62":"from sklearn.preprocessing import LabelEncoder\nlr = LabelEncoder()\ny_mask = lr.fit_transform(y_mask)","a33c6284":"#Normalizing the data\nx_mask=tf.keras.utils.normalize(x_mask,axis=1)\nx_mask=np.array(x_mask).reshape(-1,50,50,1)\nfrom keras.utils import to_categorical\ny_mask = to_categorical(y_mask)","0c11d927":"np.shape(x_mask)","5627ee65":"from tensorflow.keras.layers import Dense, Input, Dropout,Flatten, Conv2D\nfrom tensorflow.keras.layers import BatchNormalization, Activation, MaxPooling2D, LeakyReLU\nfrom tensorflow.keras.models import Model, Sequential\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.callbacks import ModelCheckpoint, ReduceLROnPlateau,EarlyStopping\nfrom tensorflow.keras.utils import plot_model\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom sklearn.model_selection import train_test_split\nfrom tensorflow.keras.regularizers import l2\nfrom tensorflow.keras.constraints import UnitNorm\n\nfrom IPython.display import SVG, Image","d63c0d2e":"X_train,X_val,Y_train,Y_val=train_test_split(x_mask, y_mask,train_size=0.95,random_state=0)","a4df80b2":"batch_size = 32\ndatagen_train = ImageDataGenerator(horizontal_flip=True, rotation_range=20,brightness_range=(0., 2.), shear_range=40)\n\ntrain_generator = datagen_train.flow(X_mask, Y_mask, batch_size=batch_size, shuffle=True)\n#val_generator = datagen_train.flow(X_val, Y_val,batch_size=batch_size, shuffle=True)","652e65d5":"model = Sequential()\n\n# 1 - Convolution\nmodel.add(Conv2D(64,(3,3), padding='same', input_shape=(50,50,1)))\nmodel.add(BatchNormalization())\nmodel.add(Activation('relu'))\nmodel.add(MaxPooling2D(pool_size=(2, 2)))\n\n# 2nd Convolution layer\nmodel.add(Conv2D(128,(5,5), padding='same'))\nmodel.add(BatchNormalization())\nmodel.add(Activation('relu'))\nmodel.add(MaxPooling2D(pool_size=(2, 2)))\n\n# 3rd Convolution layer\nmodel.add(Conv2D(256,(3,3), padding='same'))\nmodel.add(BatchNormalization())\nmodel.add(Activation('relu'))\nmodel.add(MaxPooling2D(pool_size=(2, 2)))\n\n# 4th Convolution layer\nmodel.add(Conv2D(512,(3,3), padding='same'))\nmodel.add(BatchNormalization())\nmodel.add(Activation('relu'))\nmodel.add(MaxPooling2D(pool_size=(1,1)))\n\n# 5th Convolution layer\nmodel.add(Conv2D(1024,(3,3), padding='same'))\nmodel.add(BatchNormalization())\nmodel.add(Activation('relu'))\nmodel.add(MaxPooling2D(pool_size=(2, 2)))\n\n# 6th Convolution layer\nmodel.add(Conv2D(2048,(3,3), padding='same'))\nmodel.add(BatchNormalization())\nmodel.add(Activation('relu'))\nmodel.add(MaxPooling2D(pool_size=(2, 2)))\n\n# Flattening\nmodel.add(Flatten())\n\n# Fully connected layer 1st layer\nmodel.add(Dense(1024))\nmodel.add(BatchNormalization())\nmodel.add(Activation('relu'))\nmodel.add(Dropout(0.25))\n\n# Fully connected layer 2nd layer\nmodel.add(Dense(512))\nmodel.add(BatchNormalization())\nmodel.add(Activation('relu'))\nmodel.add(Dropout(0.25))\n\nmodel.add(Dense(20, activation='softmax'))\n\nopt = Adam(lr=0.0005)\nmodel.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy'])\nmodel.summary()\n","fccf814b":"!pip install visualkeras","9d68f6f8":"import visualkeras\nvisualkeras.layered_view(model, to_file='output.png').show()\n","eda9dad9":"epochs = 70\nsteps_per_epoch = train_generator.n\/\/train_generator.batch_size\n\ncallbacks=[ReduceLROnPlateau(monitor='train_loss', patience=2, verbose=1), \n           ]\n\nhistory = model.fit(train_generator,steps_per_epoch=steps_per_epoch, \n                    epochs=epochs, callbacks=[callbacks])","157351b4":"fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n\naxes[0].plot(history.history['loss'], label='train')\n#axes[0].plot(history.history['val_loss'], label='val')\naxes[0].set_title('loss')\naxes[0].legend()\naxes[1].plot(history.history['accuracy'], label='train')\n#axes[1].plot(history.history['val_accuracy'], label='val')\naxes[1].set_title('accuracy')\naxes[1].legend()","406b3c4e":"pip install mtcnn","1765e24a":"from mtcnn.mtcnn import MTCNN","05bcbdf6":"def predict(pic):\n    cvNet = cv2.dnn.readNetFromCaffe('..\/input\/caffepretrainedfacemodel\/architecture.txt',\n                                     '..\/input\/caffepretrainedfacemodel\/weights.caffemodel')\n    #detector=MTCNN()\n    img = plt.imread(pic)\n    inp=cv2.imread(pic, cv2.IMREAD_GRAYSCALE)\n    (h, w) = inp.shape[:2]\n    blob = cv2.dnn.blobFromImage(cv2.resize(img.copy(), (300,300)), 1.0, (300, 300), (104.0, 177.0, 123.0))\n    cvNet.setInput(blob)\n    detections = cvNet.forward()\n    for i in range(0, detections.shape[2]):\n        box = detections[0, 0, i, 3:7] * np.array([w, h, w, h])\n        (startX, startY, endX, endY) = box.astype(\"int\")\n        frame = inp[startY:endY, startX:endX]\n        try:\n            im = cv2.resize(frame,(50,50))\n        except:\n            continue\n        confidence = detections[0, 0, i, 2]\n        if confidence > 0.8:\n            cv2.rectangle(img, (startX, startY), (endX, endY), (0, 255, 0), 2)\n            im = im.reshape(-1,50,50,1)\n            pred=model.predict(im)\n            pred=np.argmax(pred)\n            if pred==14 or pred==15:\n                pred=5\n            elif pred==3:\n                pred=2\n            pred=lr.inverse_transform([pred])[0]\n\n    return img,pred","72f8a958":"fig, ax=plt.subplots(1,5,figsize=(20,20))\na=361\nfor i in range(a,366):\n    img,pred=predict(os.path.join(img_path, test_images[i]))\n    ax[i-a].imshow(img)\n    ax[i-a].set_title(pred)\n    ax[i-a].set_yticklabels([])\n    ax[i-a].set_xticklabels([])\n","d917c96a":"d={}\nd['Class']=lr.classes_\nd['Probability']=a.flatten()\nd=pd.DataFrame.from_dict(d)","3f1cd89c":"sns.barplot(x='Class', y='Probability', data=d)\nplt.xticks(rotation=90)","7367775a":"model.save('model.h5')","0916412c":"from tensorflow.keras.models import Model\nlayer_outputs = [layer.output for layer in model.layers]\nactivation_model = Model(inputs=model.input, outputs=layer_outputs)\nactivations = activation_model.predict(x_mask[23].reshape(1,50,50,1))\n \ndef display_activation(activations, col_size, row_size, act_index): \n    activation = activations[act_index]\n    activation_index=0\n    fig, ax = plt.subplots(row_size, col_size, figsize=(row_size*2.5,col_size*1.5))\n    for row in range(0,row_size):\n        for col in range(0,col_size):\n            ax[row][col].imshow(activation[0, :, :, activation_index], cmap='gray')\n            ax[row][col].set_yticklabels([])\n            ax[row][col].set_xticklabels([])\n            activation_index += 1","f70bdeba":"display_activation(activations, 5, 5, 19)","0a2a6bcb":"plt.imshow(x[16])","88d272ec":"len(model.layers)","5717e2ff":"fig, ax=plt.subplots(1,20,figsize=(15,10))\nfor i in range(10,30):\n    ax[i-10].imshow(x_mask[i],cmap='gray')","cfd358b0":"lr.classes_","066b3e90":"This shows a single picture might contain more than one person","445adfae":"Train\/Val loss plot","80e49263":"Listing all different categories","498fb02c":"# **Data Preparation**\n\nCropping the images to the get face images. These facial shots will be used for model training for the classification task","9e55be4d":"Let us define a function to draw boxes around the faces","d2a4200d":"# **Train\/Test Split**","846d0a41":"# **Face Mask Detection**\n\nIn this notebook I am going to detect the face covering on a person. I have a dataset of 20 different labels like face_with_mask, no_mask, hijab, helmet, sunglasss, hat, etc.","c2aa7f36":"# Model Training","a91d1d0b":"Let us find and plot this image","7b239c74":"## Encoding the labels","24b075cc":"Let us visualize a few examples!","3a863080":"To detect and localize the facial images from the input images we will be using the MTCNN model"}}