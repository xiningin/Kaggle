{"cell_type":{"6f6f0190":"code","fec3365b":"code","49ba75e9":"code","7a1f057c":"code","5ae5c9f6":"code","c67034e6":"code","7188f4be":"code","bd559137":"code","63c196ea":"code","39e3b855":"code","aa9d221c":"code","f7e7ddee":"code","dd37d41c":"code","5f5485bb":"code","a9f3da2d":"code","2e0d7852":"code","47a4cd9b":"code","efa32a41":"code","b93f4b1f":"code","eabb9584":"code","e97c7a0b":"code","a165e7c3":"code","b5ec91c8":"code","d0dd732a":"code","ec74fab6":"code","cc39038e":"code","35935981":"code","8a77a3b9":"code","99879c00":"markdown","0d83ddc3":"markdown","236b687d":"markdown","bfe138cb":"markdown","49e2a2e4":"markdown","6cd1d324":"markdown","c9087243":"markdown"},"source":{"6f6f0190":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","fec3365b":"import tensorflow as tf\nfrom tensorflow import keras\n\nimport re\nfrom nltk.corpus import stopwords\n\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set_style(\"darkgrid\")","49ba75e9":"df = pd.read_csv(\"..\/input\/language-translation-englishfrench\/eng_-french.csv\",names=['english','french'])\nprint(df.head(5))\n\nprint(f\"\\n\\n Shape of the data >>{df.shape}\")\ndf.sample(5)","7a1f057c":"print(\"Unique values before dropping duplicates\")\nprint(df.english.nunique())\nprint(df.french.nunique())\n\ndf.drop_duplicates(subset=['english'],inplace=True)\ndf.drop_duplicates(subset=['french'],inplace=True)\n\nprint(\"\\n\\nUnique values before dropping duplicates\")\nprint(df.english.nunique())\nprint(df.french.nunique())","5ae5c9f6":"print(\"Checking NA values\\n\")\nprint(df.isnull().any(),'\\n')\nprint(df.isnull().sum())","c67034e6":"print(\"before preprocessing\")\ndf.tail(6)","7188f4be":"contractions = {\"ain't\": \"is not\", \"aren't\": \"are not\",\"can't\": \"cannot\", \"'cause\": \"because\", \"could've\": \"could have\", \"couldn't\": \"could not\", \"didn't\": \"did not\",  \"doesn't\": \"does not\", \"don't\": \"do not\", \"hadn't\": \"had not\", \"hasn't\": \"has not\", \"haven't\": \"have not\", \"he'd\": \"he would\",\"he'll\": \"he will\", \"he's\": \"he is\", \"how'd\": \"how did\", \"how'd'y\": \"how do you\", \"how'll\": \"how will\", \"how's\": \"how is\", \"I'd\": \"I would\", \"I'd've\": \"I would have\", \"I'll\": \"I will\", \"I'll've\": \"I will have\",\"I'm\": \"I am\", \"I've\": \"I have\", \"i'd\": \"i would\", \"i'd've\": \"i would have\", \"i'll\": \"i will\",  \"i'll've\": \"i will have\",\"i'm\": \"i am\", \"i've\": \"i have\", \"isn't\": \"is not\", \"it'd\": \"it would\", \"it'd've\": \"it would have\", \"it'll\": \"it will\", \"it'll've\": \"it will have\",\"it's\": \"it is\", \"let's\": \"let us\", \"ma'am\": \"madam\", \"mayn't\": \"may not\", \"might've\": \"might have\",\"mightn't\": \"might not\",\"mightn't've\": \"might not have\", \"must've\": \"must have\", \"mustn't\": \"must not\", \"mustn't've\": \"must not have\", \"needn't\": \"need not\", \"needn't've\": \"need not have\",\"o'clock\": \"of the clock\", \"oughtn't\": \"ought not\", \"oughtn't've\": \"ought not have\", \"shan't\": \"shall not\", \"sha'n't\": \"shall not\", \"shan't've\": \"shall not have\", \"she'd\": \"she would\", \"she'd've\": \"she would have\", \"she'll\": \"she will\", \"she'll've\": \"she will have\", \"she's\": \"she is\", \"should've\": \"should have\", \"shouldn't\": \"should not\", \"shouldn't've\": \"should not have\", \"so've\": \"so have\",\"so's\": \"so as\", \"this's\": \"this is\",\"that'd\": \"that would\", \"that'd've\": \"that would have\", \"that's\": \"that is\", \"there'd\": \"there would\", \"there'd've\": \"there would have\", \"there's\": \"there is\", \"here's\": \"here is\",\"they'd\": \"they would\", \"they'd've\": \"they would have\", \"they'll\": \"they will\", \"they'll've\": \"they will have\", \"they're\": \"they are\", \"they've\": \"they have\", \"to've\": \"to have\", \"wasn't\": \"was not\", \"we'd\": \"we would\", \"we'd've\": \"we would have\", \"we'll\": \"we will\", \"we'll've\": \"we will have\", \"we're\": \"we are\", \"we've\": \"we have\", \"weren't\": \"were not\", \"what'll\": \"what will\", \"what'll've\": \"what will have\", \"what're\": \"what are\", \"what's\": \"what is\", \"what've\": \"what have\", \"when's\": \"when is\", \"when've\": \"when have\", \"where'd\": \"where did\", \"where's\": \"where is\", \"where've\": \"where have\", \"who'll\": \"who will\", \"who'll've\": \"who will have\", \"who's\": \"who is\", \"who've\": \"who have\", \"why's\": \"why is\", \"why've\": \"why have\", \"will've\": \"will have\", \"won't\": \"will not\", \"won't've\": \"will not have\", \"would've\": \"would have\", \"wouldn't\": \"would not\", \"wouldn't've\": \"would not have\", \"y'all\": \"you all\", \"y'all'd\": \"you all would\",\"y'all'd've\": \"you all would have\",\"y'all're\": \"you all are\",\"y'all've\": \"you all have\", \"you'd\": \"you would\", \"you'd've\": \"you would have\", \"you'll\": \"you will\", \"you'll've\": \"you will have\", \"you're\": \"you are\", \"you've\": \"you have\"}\neng_stopwords = set(stopwords.words(\"english\"))\n\n#language => either 'english' or 'french'\ndef preprocess(sentence,language):\n    sentence = sentence.lower()\n    if language == \"english\":\n        sentence = ' '.join([contractions[word] if word in contractions else word for word in sentence.split()])\n#         sentence = ' '.join([word for word in sentence.split() if word not in eng_stopwords])\n    sentence = re.sub(r\"[.'!#$%&\\'()*+,-.\/:;<=>?@[\\\\]^ `{|}~]\",\" \",sentence)\n    sentence = ' '.join([word for word in sentence.split()])\n    \n    return sentence\n    ","bd559137":"df.english = df.english.apply(lambda x:preprocess(x,'english'))\ndf.french = df.french.apply(lambda x:preprocess(x,'french'))\n\nprint(df.shape,'\\n')\ndf.info()","63c196ea":"print(\"after preprocessing\")\ndf.tail(6)","39e3b855":"df[\"french_input\"] = df.french.apply(lambda x:'sostoken ' + x)\ndf[\"french_label\"] = df.french.apply(lambda x:x + ' eostoken')\n\nencoder_input = np.array(df.english)\ndecoder_input = np.array(df.french_input)\ndecoder_label = np.array(df.french_label)\n\n\nindices = np.arange(116544)\nnp.random.shuffle(indices)\n\nencoder_input = encoder_input[indices]\ndecoder_input = decoder_input[indices]\ndecoder_label = decoder_label[indices]\n\ndf.head()","aa9d221c":"total = df.shape[0]\ntest_size = 0.3\n\ntrain_encoder_input = encoder_input[:-int(total*test_size)]\ntrain_decoder_input = decoder_input[:-int(total*test_size)]\ntrain_decoder_label = decoder_label[:-int(total*test_size)]\n\ntest_encoder_input = encoder_input[-int(total*test_size):]\ntest_decoder_input = decoder_input[-int(total*test_size):]\ntest_decoder_label = decoder_label[-int(total*test_size):]\n\nprint(\"train dataset shape\")\nprint(train_encoder_input.shape)\nprint(train_decoder_input.shape)\nprint(train_decoder_label.shape)\n\nprint(\"\\n\\ntest dataset shape\")\nprint(test_encoder_input.shape)\nprint(test_decoder_input.shape)\nprint(test_decoder_label.shape)","f7e7ddee":"eng_tok = Tokenizer()\neng_tok.fit_on_texts(train_encoder_input)\nprint(f\"Number of unique words used in english sentences >> {len(eng_tok.index_word)}\")\n\nfr_tok = Tokenizer()\nfr_tok.fit_on_texts(train_decoder_input)\nfr_tok.fit_on_texts(train_decoder_label)\nprint(f\"Number of unique words used in french sentences >> {len(fr_tok.index_word)}\")","dd37d41c":"total_counts = 0\nrare_counts = 0\ntotal_freq = 0\nrare_freq = 0\n\nleast_occurence = 3\nfor k,v in eng_tok.word_counts.items():\n    total_counts +=1\n    total_freq += v\n    if v < least_occurence:\n        rare_counts+=1\n        rare_freq += v\n\nprint(\"=\"*25,\"english\",\"=\"*25)\nprint(f\"{rare_counts} of {total_counts} words are used less than {least_occurence}times,\")\nprint(f\"which is only {np.round(rare_counts\/total_counts*100)}% of total words used\")\nprint(f\"But they occupy {np.round(rare_freq\/total_freq*100)}% of total frequency \")","5f5485bb":"total_counts = 0\nrare_counts = 0\ntotal_freq = 0\nrare_freq = 0\n\nleast_occurence = 3\nfor k,v in fr_tok.word_counts.items():\n    total_counts +=1\n    total_freq += v\n    if v < least_occurence:\n        rare_counts+=1\n        rare_freq += v\n\nprint(\"=\"*25,\"french\",\"=\"*25)\nprint(f\"{rare_counts} of {total_counts} words are used less than {least_occurence}times,\")\nprint(f\"which is only {np.round(rare_counts\/total_counts*100)}% of total words used\")\nprint(f\"But they occupy {np.round(rare_freq\/total_freq*100)}% of total frequency \")","a9f3da2d":"eng_word_size = 6000\neng_vocab_size = eng_word_size+1\nfr_word_size = 12000\nfr_vocab_size = fr_word_size+1\n\neng_tok = Tokenizer(num_words=eng_word_size)\neng_tok.fit_on_texts(train_encoder_input)\n\ntrain_encoder_input = eng_tok.texts_to_sequences(train_encoder_input)\ntest_encoder_input = eng_tok.texts_to_sequences(test_encoder_input)\n\nfr_tok = Tokenizer(num_words=fr_word_size)\nfr_tok.fit_on_texts(train_decoder_input)\nfr_tok.fit_on_texts(train_decoder_label)\n\ntrain_decoder_input = fr_tok.texts_to_sequences(train_decoder_input)\ntrain_decoder_label = fr_tok.texts_to_sequences(train_decoder_label)\n\ntest_decoder_input = fr_tok.texts_to_sequences(test_decoder_input)\ntest_decoder_label = fr_tok.texts_to_sequences(test_decoder_label)","2e0d7852":"print(\"english\")\neng_lens = [len(seq) for seq in train_encoder_input]\nprint(\"mean >> \",np.mean(eng_lens))\nplt.subplot(2,1,1)\nplt.hist(eng_lens,bins=50)\n\n\nprint(\"french\")\nfr_lens = [len(seq) for seq in train_decoder_input]\nprint(\"mean >> \",np.mean(fr_lens))\nplt.subplot(2,1,2)\nplt.hist(fr_lens,bins=50)\nplt.show()","47a4cd9b":"eng_sequence_size = 10\nfr_sequence_size = 20\n\ntrain_encoder_input = pad_sequences(train_encoder_input,padding='post',truncating='post',maxlen=eng_sequence_size)\ntest_encoder_input = pad_sequences(test_encoder_input,padding='post',truncating='post',maxlen=eng_sequence_size)\n\ntrain_decoder_input = pad_sequences(train_decoder_input,padding='post',truncating='post',maxlen=fr_sequence_size)\ntrain_decoder_label = pad_sequences(train_decoder_label,padding='post',truncating='post',maxlen=fr_sequence_size)\n\ntest_decoder_input = pad_sequences(test_decoder_input,padding='post',truncating='post',maxlen=fr_sequence_size)\ntest_decoder_label = pad_sequences(test_decoder_label,padding='post',truncating='post',maxlen=fr_sequence_size)\n\nprint(\"train dataset shape\")\nprint(train_encoder_input.shape)\nprint(train_decoder_input.shape)\nprint(train_decoder_label.shape)\n\nprint(\"\\n\\ntest dataset shape\")\nprint(test_encoder_input.shape)\nprint(test_decoder_input.shape)\nprint(test_decoder_label.shape)","efa32a41":"from keras.layers import Input,Embedding,LSTM,Dense,Concatenate,Attention\nfrom keras.models import Model\nfrom keras.utils import plot_model\nfrom keras import backend as K\n\n#hyperparameters\nembedding_size = 256\nhidden_size = 256\n\n# trainer model (generator model will use the same encoder tho)\nencoder_input = Input(shape=[eng_sequence_size])\nencoder_embedding = Embedding(eng_vocab_size,embedding_size,mask_zero=True)\nencoder_embedded = encoder_embedding(encoder_input)\n\nencoder_lstm1 = LSTM(hidden_size,return_sequences=True,return_state=True,dropout=0.2,recurrent_dropout=0.2)\nencoder_output1,encoder_h1,encoder_c1 = encoder_lstm1(encoder_embedded)\n\nencoder_lstm2 = LSTM(hidden_size,return_sequences=True,return_state=True,dropout=0.2,recurrent_dropout=0.2)\nencoder_output2,encoder_h2,encoder_c2 = encoder_lstm2(encoder_output1)\n\nencoder_lstm3 = LSTM(hidden_size,return_sequences=True,return_state=True,dropout=0.2,recurrent_dropout=0.2)\nencoder_output3,encoder_h3,encoder_c3 = encoder_lstm3(encoder_output1)\n\ndecoder_input = Input(shape=(None,))\ndecoder_embedding = Embedding(fr_vocab_size,embedding_size,mask_zero=True)\ndecoder_embedded = decoder_embedding(decoder_input)\n\ndecoder_lstm = LSTM(hidden_size,return_sequences=True,return_state=True,dropout=0.2,recurrent_dropout=0.2)\ndecoder_output,_,_ = decoder_lstm(decoder_embedded,initial_state=[encoder_h3,encoder_c3])\n\nattn_layer = Attention()\nattn_context = attn_layer([decoder_output,encoder_output3])\n\ndecoder_output = Concatenate(axis=-1)([decoder_output,attn_context])\ntanh_dense= Dense(hidden_size*2,activation=K.tanh)\ndecoder_output = tanh_dense(decoder_output)\n\nsoftmax_dense = Dense(fr_vocab_size,activation='softmax')\ndecoder_output = softmax_dense(decoder_output)\n\ntrainer_model = Model([encoder_input,decoder_input],decoder_output)\ntrainer_model.compile(loss='sparse_categorical_crossentropy',optimizer='adam',metrics=['accuracy'])","b93f4b1f":"plot_model(trainer_model)","eabb9584":"trainer_hist =trainer_model.fit([train_encoder_input,train_decoder_input],train_decoder_label,epochs=25,batch_size=128,validation_split=0.2)","e97c7a0b":"#generator model\ngen_encoder = Model([encoder_input],[encoder_output3,encoder_h3,encoder_c3])\n\ngen_decoder_values_input = Input(shape=(eng_sequence_size,hidden_size))\ngen_decoder_h_input = Input(shape=[hidden_size])\ngen_decoder_c_input = Input(shape=[hidden_size])\n\ngen_decoder_embedded = decoder_embedding(decoder_input)\ngen_decoder_output,gen_decoder_h,gen_decoder_c = decoder_lstm(gen_decoder_embedded,initial_state=[gen_decoder_h_input,gen_decoder_c_input])\n\nattn_context = attn_layer([gen_decoder_output,gen_decoder_values_input])\ngen_decoder_output = Concatenate(axis=-1)([gen_decoder_output,attn_context])\n\ngen_decoder_output = tanh_dense(gen_decoder_output)\ngen_decoder_output = softmax_dense(gen_decoder_output)\n\ngen_decoder = Model([decoder_input]+[gen_decoder_values_input,gen_decoder_h_input,gen_decoder_c_input],[gen_decoder_output,gen_decoder_h,gen_decoder_c])","a165e7c3":"plot_model(gen_encoder)","b5ec91c8":"plot_model(gen_decoder)","d0dd732a":"def seq2eng(seq):\n    ret =[]\n    for n in seq:\n        if n != 0:\n            ret.append(eng_tok.index_word[n])\n    ret = ' '.join(ret)\n    return ret\n\ndef seq2fr(seq):\n    ret =[]\n    for n in seq:\n        if n != 0 and fr_tok.index_word[n] != 'eostoken':\n            ret.append(fr_tok.index_word[n])\n    ret = ' '.join(ret)\n    return ret","ec74fab6":"def generate_from_encoder_input(encoder_input):\n    encoder_input = encoder_input.reshape(1,-1)\n    values,h,c = gen_encoder.predict(encoder_input)\n    \n    single_tok = np.zeros((1,1))\n    single_tok[0,0] = fr_tok.word_index['sostoken']\n    decoder_input = single_tok\n    \n    generated = []\n    count = 0\n    while(True):\n        decoder_output,new_h,new_c = gen_decoder.predict([decoder_input]+[values,h,c])\n        count +=1\n        \n        sampled_index = np.argmax(decoder_output[0,-1,:])\n        sampled_word = fr_tok.index_word[sampled_index]\n        \n        if sampled_word != 'eostoken' and sampled_index != 0:\n            generated.append(sampled_word)\n        if count >= fr_sequence_size or sampled_word == 'eostoken':\n            break\n        \n        h,c = new_h,new_c\n        decoder_input[0,0] = sampled_index\n    \n    generated = ' '.join(generated)\n    return generated","cc39038e":"for i in range(520,525):\n    print(\"\\n<<sample encoder input english sentence>>\")\n    print(seq2eng(train_encoder_input[i]))\n    print(\"\\n\")\n    print(\"<<sample generated french sentence>>\")\n    print(generate_from_encoder_input(train_encoder_input[i]))\n    print(\"\\n\")\n    print(\"<<answer french sentence>>\")\n    print(seq2fr(train_decoder_label[i]))\n    print(\"========================================\\n\")","35935981":"idx = [24,1525,666,2222,52212]\n\nprint(\"Results on Train Dataset\")\nfor i in idx:\n    print(\"\\nINPUT ENG>>\")\n    print(seq2eng(train_encoder_input[i]))\n    print(\"\\n\")\n    print(\"GENERATED FR>>\")\n    print(generate_from_encoder_input(train_encoder_input[i]))\n    print(\"\\n\")\n    print(\"ANSWER FR>>\")\n    print(seq2fr(train_decoder_label[i]))\n    print(\"=====================================================================\\n\")","8a77a3b9":"idx = [24,1525,666,2222,52212]\n\nprint(\"Results on Test Dataset\")\nfor i in idx:\n    print(\"\\nINPUT ENG>\")\n    print(seq2eng(test_encoder_input[i]))\n    print(\"\\n\")\n    print(\"<<GENERATED FR>>\")\n    print(generate_from_encoder_input(test_encoder_input[i]))\n    print(\"\\n\")\n    print(\"<<ANSWER FR>>\")\n    print(seq2fr(test_decoder_label[i]))\n    print(\"=====================================================================\\n\")","99879c00":"**7.Evaluate the model**\n1. first on train dataset\n2. next on test dataset","0d83ddc3":"**1.Load Dataset**","236b687d":"**5.Tokenize & Pad sequences**","bfe138cb":"**2.Check Dataset**\n* duplicates\n* NA values","49e2a2e4":"**6.Encoder-Decoder Model : Stacked LSTM + Luong Attention**","6cd1d324":"**3.Preprocessing Text**\n* lowercase for both\n* drop stopwords for only english -> let's not remove stopwords\n* drop .'!#$%&\\'()*+,-.\/:;<=>?@[\\\\]^ `{|}~\n* switch contractions for eng\n","c9087243":"**4.Train \/ Test separate**"}}