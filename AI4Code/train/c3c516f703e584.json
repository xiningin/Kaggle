{"cell_type":{"e652c7d9":"code","baf245a2":"code","7e20ff73":"code","aabd0253":"code","957dfc71":"code","28fc304f":"code","dea0220c":"code","4330ec07":"code","0a56841f":"code","27cfe33a":"code","4734da96":"code","73f63a56":"code","a010def9":"code","6086d0c6":"code","662aa3e6":"code","0c245f12":"code","603c594a":"code","c5b26472":"code","0ff93617":"code","34a362a1":"code","07ad91e4":"code","d02aa2eb":"code","8197aae2":"code","f29418da":"code","d4590b73":"code","2affc78e":"code","aee4dfe3":"code","8eb59bf8":"code","e378b28b":"code","a114386d":"code","042ff9ec":"code","1ce42e87":"code","2274b8ea":"code","2c4c5e96":"code","7602c90a":"code","fd6461ae":"code","fe2e67d2":"code","5ab388d5":"code","9156cdf6":"code","1b397847":"code","d2c2b353":"code","c8a00ac5":"code","9d2508e0":"code","439882c1":"code","6b43fa84":"code","559dd1ee":"code","147041f3":"code","08bcc2d1":"code","2a225972":"code","dea5fd71":"code","d7754d25":"code","2bc1c12e":"code","002bbdf7":"code","78ba4963":"code","29ab8255":"code","14d15686":"code","6285ebea":"code","6d2a7aaa":"code","853904d1":"code","f52a4712":"code","a9f997cb":"code","b405cdb1":"code","1263a150":"code","fdd964e2":"code","fb8cf026":"markdown","3b67e359":"markdown","83aaeef4":"markdown","6ea6dd71":"markdown","c4619b79":"markdown","29bb5401":"markdown","743796e7":"markdown","32333f18":"markdown","841a9f38":"markdown","f8cf9532":"markdown","352791a3":"markdown","9a8912cd":"markdown","1220b45f":"markdown","b0bc20dc":"markdown","6b364278":"markdown","058e76df":"markdown","4607884f":"markdown","411990b1":"markdown","afc775e5":"markdown","785b64b5":"markdown","a8168f65":"markdown","ad1552aa":"markdown","6c77191e":"markdown","aae91ce8":"markdown","e5c4ff99":"markdown"},"source":{"e652c7d9":"!pip install xgboost==1.5.1\n!pip install shap\n!pip install optuna\n!pip install seaborn\n!pip install pandas_profiling==3.1.0\n!pip install scikit-learn-intelex","baf245a2":"import numpy as np \nimport pandas as pd \n\nfrom pandas_profiling import ProfileReport\n\nfrom lightgbm import LGBMClassifier, LGBMRegressor\nfrom xgboost import XGBClassifier, XGBRegressor\nimport xgboost as xgb\n\nfrom sklearnex import patch_sklearn\npatch_sklearn()\n\nfrom sklearn.linear_model import LogisticRegression, LinearRegression, Ridge, \\\n                                SGDClassifier, RidgeClassifier, PassiveAggressiveClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.svm import SVC\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.model_selection import KFold\nfrom sklearn.metrics import mean_squared_error, classification_report, f1_score, roc_auc_score, accuracy_score, balanced_accuracy_score, log_loss\nfrom sklearn.model_selection import train_test_split, KFold, StratifiedKFold\nfrom sklearn.preprocessing import LabelEncoder, RobustScaler, PowerTransformer\nfrom sklearn.utils import shuffle\nfrom sklearn import metrics\n\nimport seaborn as sns\nfrom matplotlib import pyplot as plt\n\nimport optuna\nfrom optuna.samplers import TPESampler\n\nfrom tqdm.notebook import tqdm\nimport time\nimport gc\nimport shap\nimport pickle\n\n%matplotlib inline\n\n#plt.rcParams['figure.dpi'] = 100\n#plt.rcParams.update({'font.size': 16})\n\n# load JS visualization code to notebook\nshap.initjs()","7e20ff73":"path_with_data = '\/kaggle\/input\/tabular-playground-series-dec-2021\/'\npath_to_data = '\/kaggle\/working\/'","aabd0253":"DEBUG = True\nTRAIN_MODEL = True\nINFER_TEST = True\nONE_FOLD_ONLY = True\nCOMPUTE_IMPORTANCE = True\nOOF = True","957dfc71":"train, test, sub = pd.read_csv(path_with_data + \"train.csv\"), \\\n    pd.read_csv(path_with_data + \"test.csv\"), \\\n    pd.read_csv(path_with_data + \"sample_submission.csv\")\n\nif DEBUG:\n    train = train.sample(n=2000000, random_state=0)\n    #test = test.sample(n=100000, random_state=0)\n\nprint(f'Train shape: {train.shape}')\nprint(f'Test shape: {test.shape}')\n\ntarget = 'Cover_Type'","28fc304f":"train.head(5)","dea0220c":"def reduce_mem_usage(df):\n    \"\"\" iterate through all the columns of a dataframe and modify the data type\n        to reduce memory usage.        \n    \"\"\"\n    start_mem = df.memory_usage().sum() \/ 1024**2\n    print('Memory usage of dataframe is {:.2f} MB'.format(start_mem))\n    \n    for col in df.columns:\n        col_type = df[col].dtype\n        \n        if col_type != object:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)  \n            else:\n                if c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)\n        else:\n            df[col] = df[col].astype('category')\n\n    end_mem = df.memory_usage().sum() \/ 1024**2\n    print('Memory usage after optimization is: {:.2f} MB'.format(end_mem))\n    print('Decreased by {:.1f}%'.format(100 * (start_mem - end_mem) \/ start_mem))\n    \n    return df","4330ec07":"train = reduce_mem_usage(train)\ntest = reduce_mem_usage(test)","0a56841f":"profile = ProfileReport(train, title=\"Pandas Profiling Report\", explorative=False, minimal=True, dark_mode=True)\nprofile","27cfe33a":"sns.relplot(data=train, x=train['Elevation'], y=train['Wilderness_Area1'], kind='scatter', hue=target)","4734da96":"sns.displot(data=train, x='Elevation', kind='hist', hue=target)","73f63a56":"sns.displot(data=train, x='Elevation', kind='kde', hue=target, fill=True)","a010def9":"sns.jointplot(data=train, x=train['Elevation'], y=train['Wilderness_Area1'], kind='scatter', hue=target)","6086d0c6":"predictors_amount = 20 + 1  # should div by 4  + 1\n\ncolormap = plt.cm.RdBu\nplt.figure(figsize=(14,12))\nplt.title('Spearman Correlation of Features', y=1.05, size=15)\ncorrmat = train.corr(method='spearman').abs()\ncols = corrmat.nlargest(predictors_amount, target)[target].index\ncm = abs(np.corrcoef(train[cols].values.T))\nsns.set(font_scale=1.25)\nhm = sns.heatmap(cm, cbar=True, annot=True, square=True, fmt='.2f', annot_kws={'size': 10}, yticklabels=cols.values, xticklabels=cols.values)\nmost_correlated = list(set(cols) - set([target]))","662aa3e6":"# plot the first most correlated features \n\ni = 1\ncols_amount = 4\nrows_amount = int(len(most_correlated) \/ cols_amount) \nplt.figure()\nfig, ax = plt.subplots(rows_amount, cols_amount, figsize=(20, 22))\nfor feature in most_correlated:\n    plt.subplot(rows_amount, cols_amount, i)\n    sns.histplot(train[feature],color=\"blue\", kde=True, bins=100, label='train_'+feature)\n    sns.histplot(test[feature],color=\"olive\", kde=True, bins=100, label='test_'+feature)\n    plt.xlabel(feature, fontsize=9); plt.legend()\n    i += 1\nplt.show()","0c245f12":"sns.boxplot(data=train[most_correlated])","603c594a":"columns = train.columns\npreproc = dict()\npreproc['target'] = target","c5b26472":"to_drop = [target]","0ff93617":"train.drop(train[train[target]==5].index,inplace=True)","34a362a1":"categoricals_features = []","07ad91e4":"features = [col for col in train.columns if col not in to_drop ]\npreproc['features'] = features","d02aa2eb":"cols_large_miss_val = train[features].columns[(train[features].isnull().mean() > 0.8)]\nprint(cols_large_miss_val)\nfeatures = [col for col in features if col not in cols_large_miss_val]\npreproc['features'] = features","8197aae2":"# Threshold for removing correlated variables\nthreshold = 0.90\n# Absolute value correlation matrix\ncorr_matrix = train[features].corr(method='spearman').abs()\n# Upper triangle of correlations\nupper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(bool))\n# Select columns with correlations above threshold\nhighly_correlated = [column for column in upper.columns if any(upper[column] > threshold)]\nprint(highly_correlated)\nfeatures = [col for col in features if col not in highly_correlated]\npreproc['features'] = features","f29418da":"threshold = 0\nzero_std = train[features].std().index[train[features].std() <= threshold]\nprint(zero_std)\nfeatures = [col for col in features if col not in zero_std]    \npreproc['features'] = features","d4590b73":"threshold = 1  # in %\nzero_cv = (100 * train[features].std() \/ train[features].mean()).index[(100 * train[features].std() \/ train[features].mean()) <= threshold]\nprint(zero_cv)\nfeatures = [col for col in features if col not in zero_cv]\npreproc['features'] = features","2affc78e":"scaler = RobustScaler()\nscaler.fit(train[features])\ntrain[features] = scaler.transform(train[features])\ntest[features] = scaler.transform(test[features])\npreproc['scaler'] = scaler","aee4dfe3":"if 0:\n  pt = PowerTransformer()\n  pt.fit(train[features])\n  train[features] = pt.transform(train[features])\n  test[features] = pt.transform(test[features])\n  preproc['power_transformer'] = pt","8eb59bf8":"# plot the first most correlated features \n\ni = 1\ncols_amount = 4\nrows_amount = int(len(most_correlated) \/ cols_amount) \nplt.figure()\nfig, ax = plt.subplots(rows_amount, cols_amount, figsize=(20, 22))\nfor feature in most_correlated:\n    plt.subplot(rows_amount, cols_amount, i)\n    sns.histplot(train[feature],color=\"blue\", kde=True, bins=100, label='train_'+feature)\n    sns.histplot(test[feature],color=\"olive\", kde=True, bins=100, label='test_'+feature)\n    plt.xlabel(feature, fontsize=9); plt.legend()\n    i += 1\nplt.show()","e378b28b":"features = preproc['features']\nprint(features)","a114386d":"# Mapping for XGBoost\nforward_map = {6:0, 7:5, 1:1, 2:2, 3:3, 4:4}\nbackward_map = {0:6, 5:7, 1:1, 2:2, 3:3, 4:4}","042ff9ec":"train[target] = train[target].map(forward_map)","1ce42e87":"X_train, X_test, y_train, y_test = train_test_split(train[features], \n                                                    train[target],\n                                                    stratify=train[target], \n                                                    test_size=0.25, \n                                                    random_state=42)","2274b8ea":"clfs = {\n        #'Logistic Regression': LogisticRegression(random_state=0), \n        'Naive Bayes': GaussianNB(),\n        #'SVM': SVC(gamma='auto'),\n        #'Random Forest': RandomForestClassifier(random_state=0),\n        'SGD Classifier': SGDClassifier(random_state=0),\n        'Ridge': RidgeClassifier(random_state=0),\n        'Passive Aggressive Classifier': PassiveAggressiveClassifier(random_state=0),\n        #'KNN': KNeighborsClassifier(),\n        #'MLP': MLPClassifier(),\n        'Decision Tree': DecisionTreeClassifier()\n       }","2c4c5e96":"for clf_name in clfs:   \n    clf = clfs[clf_name].fit(X_train, y_train)\n    y_pred = clf.predict(X_test)       \n    print(f'{clf_name}: , Accuracy = {accuracy_score(y_test, y_pred)}, Balanced Accuracy = {balanced_accuracy_score(y_test, y_pred)}')","7602c90a":"clfs = {\n        'LGB': LGBMClassifier(),\n        'XGBoost': XGBClassifier(tree_method='gpu_hist', predictor='gpu_predictor', \n                                 use_label_encoder=False, eval_metric='mlogloss'),\n       }","fd6461ae":"for clf_name in clfs:   \n    clf = clfs[clf_name].fit(X_train, y_train)\n    y_pred = clf.predict(X_test)\n    print(f'{clf_name}: , Accuracy = {accuracy_score(y_test, y_pred)}, Balanced Accuracy = {balanced_accuracy_score(y_test, y_pred)}')","fe2e67d2":"baseline_model = XGBClassifier(tree_method='gpu_hist', predictor='gpu_predictor', \n                               use_label_encoder=False, eval_metric='mlogloss')\nbaseline_model.fit(X_train, y_train)","5ab388d5":"y_pred = baseline_model.predict(X_test) \nprint(f' XGBoost: , Accuracy = {accuracy_score(y_test, y_pred)}, Balanced Accuracy = {balanced_accuracy_score(y_test, y_pred)}')","9156cdf6":"baseline_model.get_booster().feature_names = features\npreds = baseline_model.predict(test[features])","1b397847":"sub[target] = preds\nsub[target] = sub[target].map(backward_map)\nsub.to_csv(path_to_data + 'submission_bl.csv', index=False)","d2c2b353":"train_oof = np.zeros((train.shape[0],))\ntest_preds = 0\ntrain_oof.shape","c8a00ac5":"xgb_params= {\n        #\"objective\": \"multi:softprob\",        \n        #\"num_class\": len(np.unique(train[target])),\n        #\"seed\": 2001,\n        'tree_method': \"gpu_hist\",\n        'predictor': 'gpu_predictor',\n        #'use_label_encoder': False, \n        'eval_metric': 'mlogloss'\n    }","9d2508e0":"test_xgb = xgb.DMatrix(test[features])","439882c1":"NUM_FOLDS = 3\nkf = KFold(n_splits=NUM_FOLDS, shuffle=True, random_state=0)\n\nfor f, (train_ind, val_ind) in tqdm(enumerate(kf.split(train[features], train[target]))):\n        #print(f'Fold {f}')\n        train_df, val_df = train[features].iloc[train_ind], train[features].iloc[val_ind]\n        train_target, val_target = train[target].iloc[train_ind], train[target].iloc[val_ind]\n                      \n        train_df = xgb.DMatrix(train_df, label=train_target)\n        val_df = xgb.DMatrix(val_df, label=val_target)\n        \n        model =  xgb.train(xgb_params, train_df, 100)\n        temp_oof = model.predict(val_df)\n        temp_test = model.predict(test_xgb)\n\n        train_oof[val_ind] = temp_oof\n        test_preds += temp_test\/NUM_FOLDS\n        \n        print(accuracy_score(np.round(temp_oof), val_target))        ","6b43fa84":"%%time\nshap_preds = model.predict(test_xgb, pred_contribs=True)","559dd1ee":"# summarize the effects of all the features\nshap.summary_plot(shap_preds[:,:-1], test[features])","147041f3":"shap.summary_plot(shap_preds[:,:-1], test[features], plot_type=\"bar\")","08bcc2d1":"%%time\nshap_interactions = model.predict(xgb.DMatrix(test[features][:50000]), pred_interactions=True)","2a225972":"def plot_top_k_interactions(feature_names, shap_interactions, k):\n    # Get the mean absolute contribution for each feature interaction\n    aggregate_interactions = np.mean(np.abs(shap_interactions[:, :-1, :-1]), axis=0)\n    interactions = []\n    for i in range(aggregate_interactions.shape[0]):\n        for j in range(aggregate_interactions.shape[1]):\n            if j < i:\n                interactions.append(\n                    (feature_names[i] + \"*\" + feature_names[j], aggregate_interactions[i][j] * 2))\n    # sort by magnitude\n    interactions.sort(key=lambda x: x[1], reverse=True)\n    interaction_features, interaction_values = map(tuple, zip(*interactions))\n    plt.bar(interaction_features[:k], interaction_values[:k])\n    plt.xticks(rotation=90)\n    plt.tight_layout()\n    plt.show()\n    return interaction_features\n\ninteractions_to_add = 16\ninteraction_features = plot_top_k_interactions(features, shap_interactions, interactions_to_add)","dea5fd71":"if 0:\n    interaction_features = ('Wilderness_Area3*Elevation',\n      'Horizontal_Distance_To_Roadways*Elevation',\n      'Horizontal_Distance_To_Fire_Points*Elevation',\n      'Soil_Type38*Elevation',\n      'Soil_Type39*Elevation',\n      'Horizontal_Distance_To_Fire_Points*Horizontal_Distance_To_Roadways',\n      'Vertical_Distance_To_Hydrology*Elevation',\n      'Wilderness_Area1*Elevation')\n    interactions_to_add = 8\n","d7754d25":"def add_new_features(df, interaction_features, amount_of_features):\n    features_list = interaction_features[:amount_of_features]\n    for feat in features_list:\n        first_name, second_name = feat.split('*')\n        df[feat] = df[first_name]*df[second_name]\n    return df, features_list","2bc1c12e":"train, features_added = add_new_features(train, interaction_features, interactions_to_add)\ntest, _ = add_new_features(test, interaction_features, interactions_to_add)\nfeatures += list(features_added)\n\ntry:\n    del test_xgb\n    del shap_interactions\nexcept:\n    pass\ngc.collect()","002bbdf7":"features_added","78ba4963":"scaler = RobustScaler()\nscaler.fit(train[features])\ntrain[features] = scaler.transform(train[features])\ntest[features] = scaler.transform(test[features])\npreproc['scaler'] = scaler","29ab8255":"X_train, X_test, y_train, y_test = train_test_split(train[features], \n                                                    train[target],\n                                                    stratify=train[target], \n                                                    test_size=0.25, \n                                                    random_state=42)","14d15686":"baseline_model_af = XGBClassifier(tree_method='gpu_hist', predictor='gpu_predictor', \n                               use_label_encoder=False, eval_metric='mlogloss')\nbaseline_model_af.fit(X_train, y_train)\ny_pred = baseline_model_af.predict(X_test) \nprint(f' XGBoost: , Accuracy = {accuracy_score(y_test, y_pred)}, Balanced Accuracy = {balanced_accuracy_score(y_test, y_pred)}')","6285ebea":"baseline_model_af.get_booster().feature_names = features\npreds = baseline_model_af.predict(test[features])","6d2a7aaa":"sub[target] = preds\nsub[target] = sub[target].map(backward_map)\nsub.to_csv(path_to_data + 'submission_blaf.csv', index=False)","853904d1":"# HPO using opuna\n\ndef xgb_objective(trial):\n    params = {\n        'max_depth': trial.suggest_int('max_depth', 3, 20),\n        #'n_estimators': trial.suggest_int('n_estimators', 100, 3000),\n        'learning_rate': trial.suggest_loguniform('learning_rate', 1e-4, 1.0),       \n        'random_state': 42,\n        'tree_method' : 'gpu_hist',\n        'predictor' : 'gpu_predictor',\n        'eval_metric': 'mlogloss',\n        'use_label_encoder': False\n    }\n    \n    X_train, X_val, y_train, y_val = train_test_split(train[features], train[target], test_size = 0.25, random_state = 42)\n    \n    model = XGBClassifier(**params)\n    model.fit(X_train, y_train)\n    pred_val = model.predict(X_val)\n    \n    return balanced_accuracy_score(y_val, pred_val)","f52a4712":"sampler = TPESampler(seed = 42)\nstudy = optuna.create_study(study_name = 'XGBoost optimization',\n                            direction = 'maximize',\n                            sampler = sampler)\nstudy.optimize(xgb_objective, n_trials = 20)\n\nprint(\"Best logloss:\", study.best_value)\nprint(\"Best params:\", study.best_params)","a9f997cb":"if 1:\n    params = study.best_params    \n    params['random_state'] = 42\n    params['tree_method'] = 'gpu_hist'\n    params['predictor'] = 'gpu_predictor'\n    params['eval_metric'] = 'mlogloss'\n    params['use_label_encoder'] = False    \nelse:    \n    params = {\n        'max_depth': 15, \n        'n_estimators': 159,\n        'learning_rate': 0.7579479953348001,\n        'random_state': 42,\n        'tree_method' : 'gpu_hist',\n        'predictor' : 'gpu_predictor',\n        'eval_metric': 'mlogloss',\n        'use_label_encoder': False\n    }\nprint(params)","b405cdb1":"#EPOCH = 250\n#BATCH_SIZE = 512\nNUM_FOLDS = 7\nCOLS = features.copy()\n\nkf = StratifiedKFold(n_splits=NUM_FOLDS, shuffle=True, random_state=42)\ntest_preds = []\noof_preds = []\nfor fold, (train_idx, test_idx) in enumerate(kf.split(train[features], train[target])):\n        \n        print('-'*15, '>', f'Fold {fold+1}', '<', '-'*15)\n        X_train, X_valid = train[features].iloc[train_idx], train[features].iloc[test_idx]\n        y_train, y_valid = train[target].iloc[train_idx], train[target].iloc[test_idx]\n        \n        filename = f\"folds{fold}.pkl\"\n        \n        if TRAIN_MODEL:            \n            model = XGBClassifier(**params)\n            model.fit(X_train, y_train)\n            pickle.dump(model, open(path_to_data + filename, 'wb'))                                    \n            \n        else:                  \n            model = pickle.load(open(path_to_data + filename, 'rb'))                  \n    \n        if OOF:\n            print(' Predicting OOF data...')                \n            oof = model.predict(X_valid)\n            baseline_accuracy = accuracy_score(y_valid, oof)            \n            oof_preds.append(baseline_accuracy)\n            print('OOF Accuracy = {0}'.format(baseline_accuracy))\n            print(' Done!')\n                       \n        if INFER_TEST:\n            print(' Predicting test data...')\n            model.get_booster().feature_names = features\n            preds = model.predict(test[features])\n            test_preds.append(np.array(preds))\n            print(' Done!')\n                    \n        if COMPUTE_IMPORTANCE:\n            # from  https:\/\/www.kaggle.com\/cdeotte\/lstm-feature-importance\n            results = []\n            print(' Computing feature importance...')\n            \n            # COMPUTE BASELINE (NO SHUFFLE)\n            oof = model.predict(X_valid)\n            baseline_accuracy = accuracy_score(y_valid, oof)\n            results.append({'feature':'BASELINE','accuracy':baseline_accuracy})\n                                    \n            for k in tqdm(range(len(COLS))):\n                \n                # SHUFFLE FEATURE K\n                save_col = X_valid.copy()\n                np.random.shuffle(X_valid[COLS[k]].values)\n                                \n                # COMPUTE OOF Accuracy WITH FEATURE K SHUFFLED\n                oof = model.predict(X_valid)\n                acc = accuracy_score(y_valid, oof)\n                results.append({'feature':COLS[k],'accuracy':acc})                               \n                \n                X_valid = save_col.copy()\n         \n            # DISPLAY FEATURE IMPORTANCE\n            print()\n            df = pd.DataFrame(results)\n            df = df.sort_values('accuracy')\n            plt.figure(figsize=(10,20))\n            plt.barh(np.arange(len(COLS)+1),df.accuracy)\n            plt.yticks(np.arange(len(COLS)+1),df.feature.values)\n            plt.title('Feature Importance',size=16)\n            plt.ylim((-1,len(COLS)+1))\n            plt.plot([baseline_accuracy,baseline_accuracy],[-1,len(COLS)+1], '--', color='orange',\n                     label=f'Baseline OOF\\naccuracy={baseline_accuracy:.3f}')\n            plt.xlabel(f'Fold {fold+1} OOF accuracy with feature permuted',size=14)\n            plt.ylabel('Feature',size=14)\n            plt.legend()\n            plt.show()\n                               \n            # SAVE LSTM FEATURE IMPORTANCE\n            df = df.sort_values('accuracy',ascending=False)\n            df.to_csv(f'feature_importance_fold_{fold+1}.csv',index=False)\n                               \n        # ONLY DO ONE FOLD\n        if ONE_FOLD_ONLY: break","1263a150":"print('Mean of OOF: {0}, StD of OOF: {1}'.format(np.mean(oof_preds), np.std(oof_preds)))","fdd964e2":"if ONE_FOLD_ONLY:\n    sub[target] = np.array(test_preds[0])\n    sub[target] = sub[target].map(backward_map)\n    sub.to_csv(path_to_data + 'submission_final.csv', index=False)\nelse:\n    sub[target] = np.array(sum(test_preds)[0] \/ NUM_FOLDS)\n    sub[target] = sub[target].map(backward_map)\n    sub.to_csv(path_to_data + 'submission_final.csv', index=False)","fb8cf026":"## Baseline model","3b67e359":"#### Select column names with <80% missing values","83aaeef4":"### Memory reducing","6ea6dd71":"### Feature preprocessing","c4619b79":"#### Scaler transform","29bb5401":"#### Correlation heatmap","743796e7":"#### Collinear (highly correlated) features","32333f18":"#### Intermediate conclusion:\nTree-based methods are most accurate for this data","841a9f38":"#### Power transform","f8cf9532":"### Submission prepare","352791a3":"#### Distribution Plots with changes","9a8912cd":"## Classic ML baselines","1220b45f":"#### Pandas profiler","b0bc20dc":"In this notebook we will build simple XGBoost-based solution. But as a metric we will use balanced accuracy. LB=0.953 using one fold mode and 20000000 samples only\n\nSources:\n\nhttps:\/\/www.kaggle.com\/rumasinha\/featureselectionanddiffmodelexperiments https:\/\/www.kaggle.com\/tunguz\/tps-02-21-feature-importance-with-xgboost-and-shap https:\/\/www.kaggle.com\/hamzaghanmi\/make-it-simple\n\nContents:\n\n- Simple basic EDA\n\n- Feature preprocessing\n\n- Classic ML baselines\n\n- Modern ML baselines\n\n- Best Baseline model\n\n- Add new features using XGBoost and SHAP\n\n- Baseline model with added features\n\n- Hyperparameters tuning (Optuna)\n\n- Cross-validation with optimized params\n\n- Submission prepare","6b364278":"#### Seaborn plots","058e76df":"#### Select features","4607884f":"## Modern ML baselines","411990b1":"## Hyperparameters tuning (Optuna)","afc775e5":"## Baseline model with added features","785b64b5":"#### Zero coefficient of variantion","a8168f65":"#### Zero standard deviation","ad1552aa":"## Add new features using XGBoost and SHAP","6c77191e":"### Simple EDA","aae91ce8":"## Cross-validation with optimized params","e5c4ff99":"#### Scaler transform"}}