{"cell_type":{"ff16a4db":"code","b9274821":"code","aba7ff4b":"code","0f97078b":"code","e12d2688":"code","7dae7412":"code","0d44145a":"code","aeb19c64":"code","3631ba0a":"code","1e7109ed":"code","7a368b8e":"code","2e6954b9":"code","5a01d735":"code","96a83e3a":"code","a2993f5b":"code","917c02a4":"code","df2e985c":"code","17448e84":"code","906afad2":"code","2f99a76e":"code","e2676696":"code","a9551174":"code","f3d9c168":"markdown","3b802cc1":"markdown","6c9ccf7b":"markdown","b7830a76":"markdown","b2dc842a":"markdown","2ee68412":"markdown","5fedaaee":"markdown","cc0149da":"markdown"},"source":{"ff16a4db":"!pip install --upgrade tensorflow==1.14.0\nimport tensorflow as tf\ntf.__version__","b9274821":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\nfrom sklearn.metrics import f1_score, confusion_matrix\nfrom tqdm import tqdm\n#import tensorflow as tf\nfrom tensorflow.python.layers.core import Dense\nimport argparse\nimport pickle\nimport sys\nimport matplotlib.pyplot as plt\nimport seaborn as sn\n\n\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","aba7ff4b":"class LSTM_Model():\n    def __init__(self, input_shape, lr, a_dim, v_dim, t_dim, emotions, attn_fusion=True, unimodal=False,\n                 enable_attn_2=False, seed=1234):\n        if unimodal:\n            self.input = tf.placeholder(dtype=tf.float32, shape=(None, input_shape[0], input_shape[1]))\n        else:\n            self.a_input = tf.placeholder(dtype=tf.float32, shape=(None, input_shape[0], a_dim))\n            self.v_input = tf.placeholder(dtype=tf.float32, shape=(None, input_shape[0], v_dim))\n            self.t_input = tf.placeholder(dtype=tf.float32, shape=(None, input_shape[0], t_dim))\n        self.emotions = emotions\n        self.mask = tf.placeholder(dtype=tf.float32, shape=(None, input_shape[0]))\n        self.seq_len = tf.placeholder(tf.int32, [None, ], name=\"seq_len\")\n        self.y = tf.placeholder(tf.int32, [None, input_shape[0], self.emotions], name=\"y\")\n        self.lr = lr\n        self.seed = seed\n        self.attn_fusion = attn_fusion\n        self.unimodal = unimodal\n        self.lstm_dropout = tf.placeholder(tf.float32, name=\"lstm_dropout\")\n        self.dropout = tf.placeholder(tf.float32, name=\"dropout\")\n        self.lstm_inp_dropout = tf.placeholder(tf.float32, name=\"lstm_inp_dropout\")\n        self.dropout_lstm_out = tf.placeholder(tf.float32, name=\"dropout_lstm_out\")\n        self.attn_2 = enable_attn_2\n\n        # Build the model\n        self._build_model_op()\n        self._initialize_optimizer()\n\n    def GRU(self, inputs, output_size, name, dropout_keep_rate):\n        with tf.variable_scope('rnn_' + name, reuse=tf.AUTO_REUSE):\n            kernel_init = tf.glorot_uniform_initializer(seed=self.seed, dtype=tf.float32)\n            bias_init = tf.zeros_initializer()\n\n            cell = tf.contrib.rnn.GRUCell(output_size, name='gru', reuse=tf.AUTO_REUSE, activation=tf.nn.tanh,\n                                          kernel_initializer=kernel_init, bias_initializer=bias_init)\n            cell = tf.contrib.rnn.DropoutWrapper(cell, output_keep_prob=dropout_keep_rate)\n\n            output, _ = tf.nn.dynamic_rnn(cell, inputs, sequence_length=self.seq_len, dtype=tf.float32)\n\n            return output\n\n    def GRU2(self, inputs, output_size, name, dropout_keep_rate):\n        with tf.variable_scope('rnn_' + name, reuse=tf.AUTO_REUSE):\n            kernel_init = tf.glorot_uniform_initializer(seed=self.seed, dtype=tf.float32)\n            bias_init = tf.zeros_initializer()\n\n            fw_cell = tf.contrib.rnn.GRUCell(output_size, name='gru', reuse=tf.AUTO_REUSE, activation=tf.nn.tanh,\n                                             kernel_initializer=kernel_init, bias_initializer=bias_init)\n            fw_cell = tf.contrib.rnn.DropoutWrapper(fw_cell, output_keep_prob=dropout_keep_rate)\n\n            bw_cell = tf.contrib.rnn.GRUCell(output_size, name='gru', reuse=tf.AUTO_REUSE, activation=tf.nn.tanh,\n                                             kernel_initializer=kernel_init, bias_initializer=bias_init)\n            bw_cell = tf.contrib.rnn.DropoutWrapper(bw_cell, output_keep_prob=dropout_keep_rate)\n\n            output_fw, _ = tf.nn.dynamic_rnn(fw_cell, inputs, sequence_length=self.seq_len, dtype=tf.float32)\n            output_bw, _ = tf.nn.dynamic_rnn(bw_cell, inputs, sequence_length=self.seq_len, dtype=tf.float32)\n\n            output = tf.concat([output_fw, output_bw], axis=-1)\n            return output\n\n    def BiGRU(self, inputs, output_size, name, dropout_keep_rate):\n        with tf.variable_scope('rnn_' + name, reuse=tf.AUTO_REUSE):\n            kernel_init = tf.glorot_uniform_initializer(seed=self.seed, dtype=tf.float32)\n            bias_init = tf.zeros_initializer()\n\n            fw_cell = tf.contrib.rnn.GRUCell(output_size, name='gru', reuse=tf.AUTO_REUSE, activation=tf.nn.tanh,\n                                             kernel_initializer=kernel_init, bias_initializer=bias_init)\n            fw_cell = tf.contrib.rnn.DropoutWrapper(fw_cell, output_keep_prob=dropout_keep_rate)\n\n            # bw_cell = tf.contrib.rnn.GRUCell(output_size, name='gru', reuse=tf.AUTO_REUSE, activation=tf.nn.tanh,\n            #                                 kernel_initializer=kernel_init, bias_initializer=bias_init)\n            # bw_cell = tf.contrib.rnn.DropoutWrapper(bw_cell, output_keep_prob=dropout_keep_rate)\n\n            outputs, _ = tf.nn.bidirectional_dynamic_rnn(cell_fw=fw_cell, cell_bw=fw_cell, inputs=inputs,\n                                                         sequence_length=self.seq_len, dtype=tf.float32)\n\n            output_fw, output_bw = outputs\n            output = tf.concat([output_fw, output_bw], axis=-1)\n            return output\n\n    def self_attention(self, inputs_a, inputs_v, inputs_t, name):\n        \"\"\"\n\n        :param inputs_a: audio input (B, T, dim)\n        :param inputs_v: video input (B, T, dim)\n        :param inputs_t: text input (B, T, dim)\n        :param name: scope name\n        :return:\n        \"\"\"\n\n        inputs_a = tf.expand_dims(inputs_a, axis=1)\n        inputs_v = tf.expand_dims(inputs_v, axis=1)\n        inputs_t = tf.expand_dims(inputs_t, axis=1)\n        # inputs = (B, 3, T, dim)\n        inputs = tf.concat([inputs_a, inputs_v, inputs_t], axis=1)\n        t = inputs.get_shape()[2].value\n        share_param = True\n        hidden_size = inputs.shape[-1].value  # D value - hidden size of the RNN layer\n        kernel_init1 = tf.glorot_uniform_initializer(seed=self.seed, dtype=tf.float32)\n        # kernel_init2 = tf.random_normal_initializer(seed=self.seed, dtype=tf.float32,stddev=0.01)\n        # bias_init = tf.zeros_initializer()\n        dense = Dense(hidden_size, kernel_initializer=kernel_init1)\n        if share_param:\n            scope_name = 'self_attn'\n        else:\n            scope_name = 'self_attn' + name\n        # print(scope_name)\n        inputs = tf.transpose(inputs, [2, 0, 1, 3])\n        with tf.variable_scope(scope_name):\n            outputs = []\n            for x in range(t):\n                t_x = inputs[x, :, :, :]\n                # t_x => B, 3, dim\n                den = True\n                if den:\n                    x_proj = dense(t_x)\n                    x_proj = tf.nn.tanh(x_proj)\n                else:\n                    x_proj = t_x\n                u_w = tf.Variable(tf.random_normal([hidden_size, 1], stddev=0.01, seed=1234))\n                x = tf.tensordot(x_proj, u_w, axes=1)\n                alphas = tf.nn.softmax(x, axis=-1)\n                output = tf.matmul(tf.transpose(t_x, [0, 2, 1]), alphas)\n                output = tf.squeeze(output, -1)\n                outputs.append(output)\n\n            final_output = tf.stack(outputs, axis=1)\n            # print('final_output', final_output.get_shape())\n            return final_output\n\n    def attention(self, inputs_a, inputs_b, attention_size, params, mask=None, return_alphas=False):\n        \"\"\"\n        inputs_a = (b, 18, 100)\n        inputs_b = (b, 100)\n        :param inputs_a:\n        :param inputs_b:\n        :param attention_size:\n        :param time_major:\n        :param return_alphas:\n        :return:\n        \"\"\"\n        if mask is not None:\n            mask = tf.cast(self.mask, tf.bool)\n        shared = True\n        if shared:\n            scope_name = 'attn'\n        else:\n            scope_name = 'attn_'\n        with tf.variable_scope(scope_name, reuse=tf.AUTO_REUSE):\n            hidden_size = inputs_a.shape[2].value  # D value - hidden size of the RNN layer\n            den = False\n            x_proj = inputs_a\n            y_proj = inputs_b\n            # print('x_proj', x_proj.get_shape())\n            # print('y_proj', y_proj.get_shape())\n\n            # Trainable parameters\n            w_omega = params['w_omega']\n            b_omega = params['b_omega']\n            # dense_attention_2 = params['dense']\n            with tf.variable_scope('v', reuse=tf.AUTO_REUSE):\n                # Applying fully connected layer with non-linear activation to each of the B*T timestamps;\n                #  the shape of `v` is (B,T,D)*(D,A)=(B,T,A), where A=attention_size\n\n                v = tf.tensordot(x_proj, w_omega, axes=1) + b_omega\n                # v  = dense_attention_2(x_proj)\n\n            # For each of the timestamps its vector of size A from `v` is reduced with `u` vector\n            vu = tf.tanh(tf.matmul(v, tf.expand_dims(y_proj, -1), name='vu'))  # (B,T) shape (B T A) * (B A 1) = (B T)\n            vu = tf.squeeze(vu, -1)\n            # print('vu', vu.get_shape())\n            # masking\n            # mask = None\n            if mask is not None:\n                vu = tf.where(mask, vu, tf.zeros(tf.shape(vu), dtype=tf.float32))\n\n            alphas = tf.nn.softmax(vu, 1, name='alphas')  # (B,T) shape\n            if mask is not None:\n                alphas = tf.where(mask, alphas, tf.zeros(tf.shape(alphas), dtype=tf.float32))\n                a = tf.reduce_sum(tf.expand_dims(alphas, -1), axis=1)\n                condition = tf.equal(a, 0.0)\n                case_true = tf.ones(tf.shape(a), tf.float32)\n                a_m = tf.where(condition, case_true, a)\n                alphas = tf.divide(alphas, a_m)\n\n            # print('alphas', alphas.get_shape())\n\n            # Output of (Bi-)RNN is reduced with attention vector; the result has (B,D) shape\n            output = tf.matmul(tf.transpose(inputs_a, [0, 2, 1]), tf.expand_dims(alphas, -1))\n            output = tf.squeeze(output, -1)\n            # print('r', output.get_shape())\n            # output = tf.reduce_sum(r, 1)\n\n            if not return_alphas:\n                return tf.expand_dims(output, 1)\n            else:\n                return tf.expand_dims(output, 1), alphas\n\n    def self_attention_2(self, inputs, name):\n        \"\"\"\n\n        :param inputs_a: audio input (B, T, dim)\n        :param inputs_v: video input (B, T, dim)\n        :param inputs_t: text input (B, T, dim)\n        :param name: scope name\n        :return:\n        \"\"\"\n\n        t = inputs.get_shape()[1].value\n        share_param = True\n        hidden_size = inputs.shape[-1].value  # D value - hidden size of the RNN layer\n        if share_param:\n            scope_name = 'self_attn_2'\n        else:\n            scope_name = 'self_attn_2' + name\n        # print(scope_name)\n        # inputs = tf.transpose(inputs, [2, 0, 1, 3])\n        # dense = Dense(hidden_size)\n        # init1 = tf.random_normal_initializer(seed=self.seed, dtype=tf.float32,stddev=0.01)\n        attention_size = hidden_size\n        w_omega = tf.Variable(tf.random_normal([hidden_size, attention_size], stddev=0.01, seed=self.seed))\n        b_omega = tf.Variable(tf.random_normal([attention_size], stddev=0.01, seed=self.seed))\n        # dense_attention_2 = Dense(attention_size, activation=None,kernel_initializer=init1,kernel_regularizer=tf.contrib.layers.l2_regularizer(0.001))\n        params = {'w_omega': w_omega,\n                  'b_omega': b_omega,\n                  # 'dense': dense_attention_2\n                  }\n        with tf.variable_scope(scope_name, reuse=tf.AUTO_REUSE):\n            outputs = []\n            for x in range(t):\n                t_x = inputs[:, x, :]\n\n                output = self.attention(inputs, t_x, hidden_size, params, self.mask)  # (b, d)\n                outputs.append(output)\n\n            final_output = tf.concat(outputs, axis=1)\n            return final_output\n\n    def _build_model_op(self):\n        # self attention\n        if self.unimodal:\n            input = self.input\n        else:\n            if self.attn_fusion:\n                input = self.self_attention(self.a_input, self.v_input, self.t_input, '')\n                input = input * tf.expand_dims(self.mask, axis=-1)\n            else:\n                input = tf.concat([self.a_input, self.v_input, self.t_input], axis=-1)\n\n        # input = tf.nn.dropout(input, 1-self.lstm_inp_dropout)\n        self.gru_output = self.BiGRU(input, 100, 'gru', 1 - self.lstm_dropout)\n        self.inter = tf.nn.dropout(self.gru_output, 1 - self.dropout_lstm_out)\n        # self.inter = self.gru_output\n        if self.attn_2:\n            self.inter = self.self_attention_2(self.inter, '')\n        init = tf.glorot_uniform_initializer(seed=self.seed, dtype=tf.float32)\n        if self.unimodal:\n            self.inter1 = Dense(100, activation=tf.nn.tanh,\n                                kernel_initializer=init, kernel_regularizer=tf.contrib.layers.l2_regularizer(0.001))(\n                self.inter)\n        else:\n            self.inter1 = Dense(200, activation=tf.nn.relu,\n                                kernel_initializer=init, kernel_regularizer=tf.contrib.layers.l2_regularizer(0.001))(\n                self.inter)\n            self.inter1 = self.inter1 * tf.expand_dims(self.mask, axis=-1)\n            self.inter1 = Dense(200, activation=tf.nn.relu,\n                                kernel_initializer=init, kernel_regularizer=tf.contrib.layers.l2_regularizer(0.001))(\n                self.inter1)\n            self.inter1 = self.inter1 * tf.expand_dims(self.mask, axis=-1)\n            self.inter1 = Dense(200, activation=tf.nn.relu,\n                                kernel_initializer=init, kernel_regularizer=tf.contrib.layers.l2_regularizer(0.001))(\n                self.inter1)\n        self.inter1 = self.inter1 * tf.expand_dims(self.mask, axis=-1)\n        self.inter1 = tf.nn.dropout(self.inter1, 1 - self.dropout)\n        self.output = Dense(self.emotions, kernel_initializer=init,\n                            kernel_regularizer=tf.contrib.layers.l2_regularizer(0.001))(self.inter1)\n        # print('self.output', self.output.get_shape())\n        self.preds = tf.nn.softmax(self.output)\n        # To calculate the number correct, we want to count padded steps as incorrect\n        correct = tf.cast(\n            tf.equal(tf.argmax(self.preds, -1, output_type=tf.int32), tf.argmax(self.y, -1, output_type=tf.int32)),\n            tf.int32) * tf.cast(self.mask, tf.int32)\n\n        # To calculate accuracy we want to divide by the number of non-padded time-steps,\n        # rather than taking the mean\n        self.accuracy = tf.reduce_sum(tf.cast(correct, tf.float32)) \/ tf.reduce_sum(tf.cast(self.seq_len, tf.float32))\n        # y = tf.argmax(self.y, -1)\n\n        loss = tf.nn.softmax_cross_entropy_with_logits_v2(logits=self.output, labels=self.y)\n        loss = loss * self.mask\n\n        self.loss = tf.reduce_sum(loss) \/ tf.reduce_sum(self.mask)\n\n    def _initialize_optimizer(self):\n        train_vars = tf.trainable_variables()\n        reg_loss = []\n        total_parameters = 0\n        for train_var in train_vars:\n            # print(train_var.name)\n            reg_loss.append(tf.nn.l2_loss(train_var))\n\n            shape = train_var.get_shape()\n            variable_parameters = 1\n            for dim in shape:\n                variable_parameters *= dim.value\n            total_parameters += variable_parameters\n        # print(total_parameters)\n        print('Trainable parameters:', total_parameters)\n\n        self.loss = self.loss + 0.00001 * tf.reduce_mean(reg_loss)\n        self.global_step = tf.get_variable(shape=[], initializer=tf.constant_initializer(0), dtype=tf.int32,\n                                           name='global_step')\n        self._optimizer = tf.train.AdamOptimizer(learning_rate=self.lr, beta1=0.9, beta2=0.999)\n        # self._optimizer = tf.train.AdadeltaOptimizer(learning_rate=1.0, rho=0.95, epsilon=1e-08)\n\n        self.train_op = self._optimizer.minimize(self.loss, global_step=self.global_step)\n","0f97078b":"def multimodal(unimodal_activations, data, classes, attn_fusion=True, enable_attn_2=False):\n    print(\"starting multimodal\")\n    # Fusion (appending) of features\n\n    text_train = unimodal_activations['text_train']\n    audio_train = unimodal_activations['audio_train']\n    video_train = unimodal_activations['video_train']\n\n    text_test = unimodal_activations['text_test']\n    audio_test = unimodal_activations['audio_test']\n    video_test = unimodal_activations['video_test']\n\n    train_mask = unimodal_activations['train_mask']\n    test_mask = unimodal_activations['test_mask']\n\n    print('train_mask', train_mask.shape)\n\n    train_label = unimodal_activations['train_label']\n    print('train_label', train_label.shape)\n    test_label = unimodal_activations['test_label']\n    print('test_label', test_label.shape)\n\n    # print(train_mask_bool)\n    seqlen_train = np.sum(train_mask, axis=-1)\n    print('seqlen_train', seqlen_train.shape)\n    seqlen_test = np.sum(test_mask, axis=-1)\n    print('seqlen_test', seqlen_test.shape)\n\n    a_dim = audio_train.shape[-1]\n    v_dim = video_train.shape[-1]\n    t_dim = text_train.shape[-1]\n    if attn_fusion:\n        print('With attention fusion')\n    allow_soft_placement = True\n    log_device_placement = False\n\n    # Multimodal model\n    session_conf = tf.compat.v1.ConfigProto(\n        # device_count={'GPU': gpu_count},\n        allow_soft_placement=allow_soft_placement,\n        log_device_placement=log_device_placement,\n        gpu_options=tf.GPUOptions(allow_growth=True))\n    gpu_device = 0\n    best_acc = 0\n    best_loss_accuracy = 0\n    best_loss = 10000000.0\n    best_epoch = 0\n    best_epoch_loss = 0\n    hist = {'epoch': [i for i in range(1,epochs+1)],'trainacc':[],'trainloss':[], 'testacc':[],'testloss':[], 'f1score':[], 'conmat':[]}\n    with tf.device('\/device:GPU:%d' % gpu_device):\n        print('Using GPU - ', '\/device:GPU:%d' % gpu_device)\n        with tf.Graph().as_default():\n            tf.set_random_seed(seed)\n            sess = tf.Session(config=session_conf)\n            with sess.as_default():\n                model = LSTM_Model(text_train.shape[1:], 0.0001, a_dim=a_dim, v_dim=v_dim, t_dim=t_dim,\n                                   emotions=classes, attn_fusion=attn_fusion,\n                                   unimodal=False, enable_attn_2=enable_attn_2,\n                                   seed=seed)\n                sess.run(tf.group(tf.global_variables_initializer(),\n                                  tf.local_variables_initializer()))\n\n                test_feed_dict = {\n                    model.t_input: text_test,\n                    model.a_input: audio_test,\n                    model.v_input: video_test,\n                    model.y: test_label,\n                    model.seq_len: seqlen_test,\n                    model.mask: test_mask,\n                    model.lstm_dropout: 0.0,\n                    model.lstm_inp_dropout: 0.0,\n                    model.dropout: 0.0,\n                    model.dropout_lstm_out: 0.0\n                }\n\n                # print('\\n\\nDataset: %s' % (data))\n                print(\"\\nEvaluation before training:\")\n                # Evaluation after epoch\n                step, loss, accuracy = sess.run(\n                    [model.global_step, model.loss, model.accuracy],\n                    test_feed_dict)\n                print(\"EVAL: epoch {}: step {}, loss {:g}, acc {:g}\".format(\n                    0, step, loss, accuracy))\n\n                for epoch in range(epochs):\n                    epoch += 1\n\n                    batches = batch_iter(list(\n                        zip(text_train, audio_train, video_train, train_mask, seqlen_train, train_label)),\n                        batch_size)\n\n                    # Training loop. For each batch...\n                    print('\\nTraining epoch {}'.format(epoch))\n                    l = []\n                    a = []\n                    for i, batch in tqdm(enumerate(batches)):\n                        b_text_train, b_audio_train, b_video_train, b_train_mask, b_seqlen_train, b_train_label = zip(\n                            *batch)\n                        # print('batch_hist_v', len(batch_utt_v))\n                        feed_dict = {\n                            model.t_input: b_text_train,\n                            model.a_input: b_audio_train,\n                            model.v_input: b_video_train,\n                            model.y: b_train_label,\n                            model.seq_len: b_seqlen_train,\n                            model.mask: b_train_mask,\n                            model.lstm_dropout: 0.4,\n                            model.lstm_inp_dropout: 0.0,\n                            model.dropout: 0.2,\n                            model.dropout_lstm_out: 0.2\n                        }\n\n                        _, step, loss, accuracy = sess.run([model.train_op, model.global_step,model.loss, model.accuracy],feed_dict)\n                        l.append(loss)\n                        a.append(accuracy)\n\n                    print(\"\\t \\tEpoch {}:, loss {:g}, accuracy {:g}\".format(epoch, np.average(l), np.average(a)))\n                    hist['trainacc'].append(np.average(a)*100)\n                    hist['trainloss'].append(np.average(l)*100)\n                    \n                    # Evaluation after epoch on test set\n                    step, loss, accuracy, preds, y, mask = sess.run([model.global_step, model.loss, model.accuracy,model.preds, model.y, model.mask],test_feed_dict)\n                    f1 = f1_score(np.ndarray.flatten(tf.argmax(y, -1, output_type=tf.int32).eval()),\n                                  np.ndarray.flatten(tf.argmax(preds, -1, output_type=tf.int32).eval()),\n                                  sample_weight=np.ndarray.flatten(tf.cast(mask, tf.int32).eval()), average=\"weighted\")\n                    hist['conmat'].append(confusion_matrix(np.ndarray.flatten(tf.argmax(y, -1, output_type=tf.int32).eval()), np.ndarray.flatten(tf.argmax(preds, -1, output_type=tf.int32).eval()), sample_weight=np.ndarray.flatten(tf.cast(mask, tf.int32).eval())))\n                    print(\"EVAL: After epoch {}: step {}, loss {:g}, acc {:g}, f1 {:g}\".format(epoch, step,loss \/ test_label.shape[0], accuracy, f1))\n                    hist['testacc'].append(accuracy*100)\n                    hist['testloss'].append((step,loss \/ test_label.shape[0]) *100)\n                    hist['f1score'].append(f1)\n                    \n                    if accuracy > best_acc:\n                        best_epoch = epoch\n                        best_acc = accuracy\n                    if loss < best_loss:\n                        best_loss = loss\n                        best_loss_accuracy = accuracy\n                        best_epoch_loss = epoch\n\n                print(\n                    \"\\n\\nBest epoch: {}\\nBest test accuracy: {}\\nBest epoch loss: {}\\nBest test accuracy when loss is least: {}\".format(\n                        best_epoch, best_acc, best_epoch_loss, best_loss_accuracy))\n    return hist, best_epoch\n                \n                \n#                 print('Sample video testing')\n#                 activation_no = 10\n#                 test_feed_dict_sample = {\n#                     model.t_input: text_test[activation_no:activation_no+1],\n#                     model.a_input: audio_test[activation_no:activation_no+1],\n#                     model.v_input: video_test[activation_no:activation_no+1],\n#                     model.y: test_label[activation_no:activation_no+1],\n#                     model.seq_len: seqlen_test[activation_no:activation_no+1],\n#                     model.mask: test_mask[activation_no:activation_no+1],\n#                     model.lstm_dropout: 0.0,\n#                     model.lstm_inp_dropout: 0.0,\n#                     model.dropout: 0.0,\n#                     model.dropout_lstm_out: 0.0\n#                 }\n#                 step, loss, accuracy, preds, y, mask = sess.run(\n#                     [model.global_step, model.loss, model.accuracy,\n#                      model.preds, model.y, model.mask],\n#                     test_feed_dict_sample)\n#                 #print(y, preds)\n#                 print(\"Video id : \", activation_no)\n\n#                 print(\"True sentiment : \", tf.argmax(y, axis=-1, output_type=tf.int32).eval(),\n#                       \"Predicted sentiment : \", tf.argmax(preds, axis=-1, output_type=tf.int32).eval())\n\n#                 print(\"True sentiment : \", tf.argmax(tf.math.bincount(tf.argmax(y, axis=-1, output_type=tf.int32))).eval(),\n#                       \"Predicted sentiment : \", tf.argmax(tf.math.bincount(tf.argmax(preds, axis=-1, output_type=tf.int32))).eval())\n","e12d2688":"def batch_iter(data, batch_size, shuffle=True):\n    \"\"\"\n    Generates a batch iterator for a dataset.\n    \"\"\"\n    data = np.array(data)\n    data_size = len(data)\n    num_batches_per_epoch = int((len(data) - 1) \/ batch_size) + 1\n    # Shuffle the data at each epoch\n    if shuffle:\n        shuffle_indices = np.random.permutation(np.arange(data_size))\n        shuffled_data = data[shuffle_indices]\n    else:\n        shuffled_data = data\n    for batch_num in range(num_batches_per_epoch):\n        start_index = batch_num * batch_size\n        end_index = min((batch_num + 1) * batch_size, data_size)\n        yield shuffled_data[start_index:end_index]\n","7dae7412":"def plot_train_history(hist):\n    plt.figure()\n    plt.xlabel('Epoch')\n    plt.ylabel('')\n    plt.plot(hist['epoch'], hist['acc'], label='Accuracy')\n    plt.plot(hist['epoch'], hist['loss'], label = 'Loss')\n    plt.legend()","0d44145a":"batch_size = 20\nepochs = 30\nemotions = '6'\ndata = 'iemocap'\nseed = 1234\nnp.random.seed(seed)\ntf.set_random_seed(seed)\nunimodal_activations = {}\nwith open('..\/input\/iemocap\/unimodal_{0}_{1}way.pickle'.format(data,emotions),'rb') as handle:\n    u = pickle.Unpickler(handle,encoding = 'latin1')\n    unimodal_activations = u.load()\n","aeb19c64":"hist1, bestEpoch = multimodal(unimodal_activations,data,emotions,False,False)","3631ba0a":"print(\"concatenation based fusion\")\nprint(\"best epoch: \", bestEpoch)\nprint(\"Accuracy on test set: \",hist1['testacc'][bestEpoch-1])\nprint(\"f1score: \",hist1['f1score'][bestEpoch-1])","1e7109ed":"df_cm = pd.DataFrame(hist1['conmat'][bestEpoch-1], index = [i for i in ['hap', 'sad', 'neu', 'ang', 'exc', 'fru']],\n                  columns = [i for i in ['hap', 'sad', 'neu', 'ang', 'exc', 'fru']])\nsn.set(font_scale=1) # for label size\nsn.heatmap(df_cm, annot=True, annot_kws={\"size\": 10}, cmap=sn.cm.rocket_r, fmt=\"d\") # font size\nplt.xlabel('Predicted')\nplt.ylabel('True')\nplt.show()","7a368b8e":"#test accuracy\nplt.figure()\nplt.xlabel('Epoch')\nplt.ylabel('Test Accuracy')\nplt.plot(hist1['epoch'], hist1['testacc'], label='Concatenation-based fusion')\nplt.legend()\n\n#f1 score\nplt.figure()\nplt.xlabel('Epoch')\nplt.ylabel('f1score')\nplt.plot(hist1['epoch'], hist1['f1score'], label='Concatenation-based fusion')\nplt.legend()","2e6954b9":"hist2 , bestEpoch = multimodal(unimodal_activations,data,emotions,True,False)","5a01d735":"print(\"Attention based fusion\")\nprint(\"best epoch: \", bestEpoch)\nprint(\"Accuracy on test set: \",hist2['testacc'][bestEpoch-1])\nprint(\"f1score: \",hist2['f1score'][bestEpoch-1])","96a83e3a":"df_cm = pd.DataFrame(hist2['conmat'][bestEpoch-1], index = [i for i in ['hap', 'sad', 'neu', 'ang', 'exc', 'fru']],\n                  columns = [i for i in ['hap', 'sad', 'neu', 'ang', 'exc', 'fru']])\nsn.set(font_scale=1) # for label size\nsn.heatmap(df_cm, annot=True, annot_kws={\"size\": 10}, cmap=sn.cm.rocket_r, fmt=\"d\") # font size\nplt.xlabel('Predicted')\nplt.ylabel('True')\nplt.show()","a2993f5b":"#test accuracy\nplt.figure()\nplt.xlabel('Epoch')\nplt.ylabel('Test Accuracy')\nplt.plot(hist2['epoch'], hist2['testacc'], label='Attention-based fusion')\nplt.legend()\n\n#f1 score\nplt.figure()\nplt.xlabel('Epoch')\nplt.ylabel('f1score')\nplt.plot(hist2['epoch'], hist2['f1score'], label='Attention-based fusion')\nplt.legend()","917c02a4":"hist3 , bestEpoch = multimodal(unimodal_activations,data,emotions,True,True)","df2e985c":"print(\"Attention based fusion with utterance level attention\")\nprint(\"best epoch: \", bestEpoch)\nprint(\"Accuracy on test set: \",hist3['testacc'][bestEpoch-1])\nprint(\"f1score: \",hist3['f1score'][bestEpoch-1])","17448e84":"df_cm = pd.DataFrame(hist3['conmat'][bestEpoch-1], index = [i for i in ['hap', 'sad', 'neu', 'ang', 'exc', 'fru']],\n                  columns = [i for i in ['hap', 'sad', 'neu', 'ang', 'exc', 'fru']])\nsn.set(font_scale=1) # for label size\nsn.heatmap(df_cm, annot=True, annot_kws={\"size\": 10}, cmap=sn.cm.rocket_r, fmt=\"d\") # font size\nplt.xlabel('Predicted')\nplt.ylabel('True')\nplt.show()","906afad2":"#test accuracy\nplt.figure()\nplt.xlabel('Epoch')\nplt.ylabel('Test Accuracy')\nplt.plot(hist3['epoch'], hist3['testacc'], label='Utterance level attention')\nplt.legend()\n\n#f1 score\nplt.figure()\nplt.xlabel('Epoch')\nplt.ylabel('f1score')\nplt.plot(hist3['epoch'], hist3['f1score'], label='Utterence level attention')\nplt.legend()","2f99a76e":"plt.figure()\nplt.xlabel('Epoch')\nplt.ylabel('Test Accuracy')\nplt.plot(hist1['epoch'], hist1['testacc'], label='Concatenation-based fusion')\nplt.plot(hist2['epoch'], hist2['testacc'], label = 'Attention-based fusion')\nplt.plot(hist3['epoch'], hist3['testacc'], label = 'Utterance level attention')\nplt.legend()","e2676696":"plt.figure()\nplt.xlabel('Epoch')\nplt.ylabel('F1score')\nplt.plot(hist1['epoch'], hist1['f1score'], label='Concatenation-based fusion')\nplt.plot(hist2['epoch'], hist2['f1score'], label = 'Attention-based fusion')\nplt.plot(hist3['epoch'], hist3['f1score'], label = 'Utterance level attention')\nplt.legend()","a9551174":"plt.figure()\nplt.xlabel('Epoch')\nplt.ylabel('Train Accuracy')\nplt.plot(hist1['epoch'], hist1['trainacc'], label='Concatenation-based fusion')\nplt.plot(hist2['epoch'], hist2['trainacc'], label = 'Attention-based fusion')\nplt.plot(hist3['epoch'], hist3['trainacc'], label = 'Utterance level attention')\nplt.legend()","f3d9c168":"# **Multimodal Fusion**\n\n![](https:\/\/github.com\/soujanyaporia\/multimodal-sentiment-analysis\/blob\/master\/atlstm3.jpg?raw=true)","3b802cc1":"**2) Attention-based fusion:**","6c9ccf7b":"Comparing f1 score of three different three different fusion methods","b7830a76":"Comparing test accuracy of three different fusion methods","b2dc842a":"**3) Attention-based fudion with Utterance level attention::**","2ee68412":"# **Multimodal emotion analysis in videos**\nDataset: IEMOCAP","5fedaaee":"Comparing train accuracy of three different three different fusion methods","cc0149da":"**1) Concatenation-based fusion:**"}}