{"cell_type":{"9585a67e":"code","f985f600":"code","7f1b7fdd":"code","2a45bcf1":"code","2b604681":"code","aee59567":"code","7eae5195":"code","7e2c4c85":"code","c2c8229f":"code","fd23d000":"code","8a7f3632":"code","28a09aae":"code","a0ec0d5f":"markdown","d9371f85":"markdown","4b72cc0b":"markdown","e9a3d9b4":"markdown","221bde99":"markdown","795a7f9e":"markdown","08343d16":"markdown","02ea7c42":"markdown","2d1e47b7":"markdown","af4aff91":"markdown","6db5252c":"markdown","d5327dc3":"markdown","f75d03da":"markdown","8efe1cf2":"markdown","9b167a75":"markdown"},"source":{"9585a67e":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\nimport matplotlib.pyplot as plt\nfrom sklearn.cluster import MiniBatchKMeans, KMeans\nimport seaborn as sns\n\nfrom sklearn.metrics.cluster import contingency_matrix,adjusted_rand_score\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","f985f600":"df = pd.read_csv('..\/input\/drug-classification\/drug200.csv')\n\ndf.head(3)","7f1b7fdd":"df[\"Cholesterol\"].unique()\n#array(['HIGH', 'NORMAL'], dtype=object)\ndf[\"BP\"].unique()\n#array(['HIGH', 'LOW', 'NORMAL'], dtype=object)\ndf[\"Drug\"].unique()\n#array(['DrugY', 'drugC', 'drugX', 'drugA', 'drugB'], dtype=object)\n\nlvl={\"HIGH\":3,\"NORMAL\":2,\"LOW\":1}\ndf['Cholesterol']= [lvl[i] for i in df['Cholesterol'] ]\ndf['BP']= [lvl[i] for i in df['BP'] ]\n\nDrug={\"drugA\":1,\"drugB\":2,\"drugC\":3,\"drugX\":4,\"DrugY\":5}\ndf['Drug']= [Drug[i] for i in df['Drug'] ]\n\nDrug={\"F\":1,\"M\":2}\ndf['Sex']= [Drug[i] for i in df['Sex'] ]\n\n#df = df[df['Drug'] <3]\nlen(df)\ndf.head(3)","2a45bcf1":"X=df.iloc[:,:-1]\nY=df.iloc[:,-1:]","2b604681":"\nimport seaborn as sns \n# generating correlation heatmap \nsns.heatmap(df.corr(), annot = True) \n# posting correlation heatmap to output console  \nplt.show() \n\nsns.pairplot(df,hue='Drug')\n\n#sns.stripplot(x=list(X[\"Na_to_K\"]),y=list(X[\"Cholesterol\"]),hue=list(Y[\"Drug\"]),jitter=True)","aee59567":"k_means = KMeans(init='k-means++', n_clusters=5)\nk_means.fit(X)\ny_pred=k_means.predict(X)","7eae5195":"cont_m=contingency_matrix(y_pred,Y)\ncont_m\n#adjusted_rand_score\n","7e2c4c85":"puirty=np.sum(np.max(cont_m,axis=1))\/np.sum(cont_m)\nrand_score=adjusted_rand_score(list(y_pred),list(Y.iloc[:,0]))\nprint(\"puirty:\"+str(puirty))\nprint(\"rand_score:\"+str(rand_score))","c2c8229f":"from scipy.cluster.hierarchy import linkage,dendrogram,cut_tree\nfrom matplotlib import pyplot as plt\n\nZ= linkage(X,\"single\")\n\n#Cute Tree used to specify how many cluster I want \ncutree = cut_tree(Z, n_clusters=[5])\n\n\nfig = plt.figure(figsize=(10, 5))\n#dn = dendrogram(Z,truncate_mode='lastp',p=6,    leaf_font_size=12.,show_contracted=True)\n\nfancy_dendrogram(\n    Z,\n    truncate_mode='lastp',\n    p=5,\n    leaf_rotation=90.,\n    leaf_font_size=12.,\n    show_contracted=True,\n    annotate_above=2,\n    max_d=4,  # plot a horizontal cut-off line\n)\nplt.show()","fd23d000":"cont_m=contingency_matrix(cutree[:,0],Y)\npuirty=np.sum(np.max(cont_m,axis=1))\/np.sum(cont_m)\nrand_score=adjusted_rand_score(list(cutree[:,0]),list(Y.iloc[:,0]))\nprint(\"puirty:\"+str(puirty))\nprint(\"rand_score:\"+str(rand_score))\n","8a7f3632":"from sklearn.cluster import DBSCAN\nimport numpy as np\n\nrun=True\neps_val= 0.1\n\n#loop to define eps value for 5 clusters\nwhile run:\n    eps_val =eps_val + 0.1\n    clustering = DBSCAN(eps=eps_val, min_samples=2).fit(X)\n    #print(clustering.labels_)\n    cluster_count = len(set(clustering.labels_[clustering.labels_ != -1]))\n    #print(\"N:\"+str(cluster_count))\n    outlairs = len(clustering.labels_[clustering.labels_ == -1])\n    #print(\"O:\"+str(outlairs))\n    \n    if cluster_count ==5:\n        print(\"eps_val:\"+str(eps_val) + \" outlairs:\"+str(outlairs) + \" cluster_count:\"+str(cluster_count))\n    if (cluster_count ==5 and outlairs== 0) or round(eps_val)==10:\n        run=False\n    \n","28a09aae":"clustering = DBSCAN(eps=5.1999999999999975, min_samples=2).fit(X)\ncluster_count = len(set(clustering.labels_))\n\ncont_m=contingency_matrix(clustering.labels_,Y)\npuirty=np.sum(np.max(cont_m,axis=1))\/np.sum(cont_m)\nrand_score=adjusted_rand_score(list(clustering.labels_),list(Y.iloc[:,0]))\nprint(\"puirty:\"+str(puirty))\nprint(\"rand_score:\"+str(rand_score))","a0ec0d5f":"# 1- Partition-based clustering -  k-mean \nOur dataset have 5 classes so I target K-means to produce 5 clusters","d9371f85":"# Encode catagorial columns","4b72cc0b":"# Evaluate K-means ","e9a3d9b4":"# Evaluate Hierarchical clustering","221bde99":"# 3- Density-based clustering: DBSCAN\ndensity-based techniques are more efficient in arbitrary shaped clusters and outliers detection .\nThe main idea behind DBSCAN is that a point belongs to a cluster if it is close to many points from that cluster.\n\n","795a7f9e":"![image.png](attachment:image.png)","08343d16":"# Results Show that K-mean gives best results for my Dataset\n\n\n1.  K-means\n    *     puirty:0.51\n    *     rand_score:0.06510626450680837\n\n\n2. Agglomerative\n    * puirty:0.455\n    * rand_score:-0.06899522570656587\n\n\n\n3. DBSCAN\n    * puirty:0.455\n    * rand_score:-0.0703785243555056\n\n","02ea7c42":"# visualizing Data and Correlation","2d1e47b7":"# results ","af4aff91":"# split features and label column","6db5252c":"# loop to define eps value for 5 clusters\nin Density-based clustering you can not predefine the numbers of clusters So I make loop to get 5 clusters\n\neps: The distance that specifies the neighborhoods. \nTwo points are considered to be neighbors if the distance between them are less than or equal to eps.","d5327dc3":"# Comparing K-means , Agglomerative and DBSCAN Clusters","f75d03da":"# 2- Hierarchical clustering : Agglomerative \n","8efe1cf2":"# load Libraries","9b167a75":"# Loading Dataset"}}