{"cell_type":{"4aef54a5":"code","719ce380":"code","bb558157":"code","a2516ed6":"code","42cb78fc":"code","b48103a6":"code","681c0d87":"code","33a0367c":"code","2c33c313":"code","91720f4c":"code","c753c9a2":"code","2f8c99c4":"code","c3298439":"code","23d65e8a":"code","42ec9c7d":"code","40895eae":"code","b05ea91a":"code","8ce9c5f6":"code","75a17ba3":"code","8af892f0":"code","3e880af5":"code","925fe55d":"code","d7f571bc":"code","9f5adfb3":"code","7fd33e1f":"code","f0e97de0":"code","0a954fdb":"code","d12d85ef":"code","88a1ad40":"code","23230d06":"code","1f7af167":"code","b2e8c048":"code","c7efa818":"code","dd14ad6b":"markdown","5fa1a6ef":"markdown","9575d4f9":"markdown","34c3522d":"markdown","402f046e":"markdown","053cf740":"markdown","99fe365d":"markdown","a6246e50":"markdown","34adde66":"markdown","2277f2d9":"markdown","968debea":"markdown","52f2b2a7":"markdown","88814bc4":"markdown"},"source":{"4aef54a5":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)","719ce380":"training_set = pd.read_csv('\/kaggle\/input\/unsw-nb15\/UNSW_NB15_training-set.csv')\ntraining_set.info()","bb558157":"training_set.head()","a2516ed6":"print(training_set['label'].unique())\nprint(training_set['attack_cat'].unique())","42cb78fc":"mask = (training_set.dtypes == np.object) #\u79bb\u6563\u578b\u53d8\u91cf\u7684\u7c7b\u578b\nprint(training_set.loc[:,mask].head())\nlist_cat = training_set.loc[:,mask].columns.tolist()\nprint(list_cat)\nprint(training_set.loc[:,mask].values)\n","b48103a6":"mask = (training_set.dtypes != np.object)\nprint(training_set.loc[:,mask].head())\nlist_cat = training_set.loc[:,mask].columns.tolist()\nprint(list_cat)\ntraining_set.loc[:,mask].describe()","681c0d87":"# number of occurrences for each attack category\ntraining_set.attack_cat.value_counts()","33a0367c":"mask = (training_set.label == 1)\nprint(training_set.loc[mask,:].service.value_counts())\nprint(training_set.loc[mask,:].proto.value_counts())","2c33c313":"mask = (training_set.label == 0)\nprint(training_set.loc[mask,:].service.value_counts())\nprint(training_set.loc[mask,:].proto.value_counts())","91720f4c":"#\u5c06attack_cat\u591a\u7c7b\u8f6c\u4e3a\u6570\u503c\nfrom sklearn import preprocessing\nle=preprocessing.LabelEncoder()\nnum_cat = le.fit_transform(training_set.attack_cat)\nprint(le.classes_)\nprint(np.unique(num_cat))","c753c9a2":"Y=num_cat.tolist()\nX = training_set.drop(columns=['id','attack_cat','label']) #\u53bb\u9664\u65e0\u5173\u53d8\u91cf\nmask = (X.dtypes == np.object)\nlist_cat = X.loc[:,mask].columns.tolist()\nlist_cat","2f8c99c4":"#X = pd.get_dummies(X, columns=list_cat)\n#X.head()\nX.nunique()","c3298439":"#X.nunique()\nX=X.drop('rate',axis=1)\nX=X.drop('dur',axis=1)\nX=X.drop('sload',axis=1)\nX=X.drop('dload',axis=1)\nX=X.drop('sinpkt',axis=1)\nX=X.drop('dinpkt',axis=1)\nX=X.drop('sjit',axis=1)\nX=X.drop('djit',axis=1)\nX=X.drop('stcpb',axis=1)\nX=X.drop('dtcpb',axis=1)\nX=X.drop('tcprtt',axis=1)\nX=X.drop('synack',axis=1)\nX=X.drop('ackdat',axis=1)\nX\nX.nunique()","23d65e8a":"X=X.drop('sbytes',axis=1)\nX=X.drop('dbytes',axis=1)\nX = pd.get_dummies(X, columns=list_cat)\nX.head()","42ec9c7d":"import sklearn\nfrom sklearn.model_selection import train_test_split\nimport matplotlib.pyplot as plt\nX_train, X_test, y_train, y_test = train_test_split(X, Y, test_size = 0.2, random_state = 42)","40895eae":"import xgboost as xgb\nfrom sklearn import svm\nfrom sklearn.metrics import classification_report,roc_auc_score,average_precision_score,confusion_matrix\n","b05ea91a":"params = {\n    'max_depth': 10,\n    'objective': 'multi:softmax',  # error evaluation for multiclass training\n    'num_class': 10,                # Number of classes \n    'n_gpus': 1\n}\n\nxg_clf = xgb.XGBClassifier(**params)\n\nimport sklearn\nfrom sklearn.model_selection import train_test_split\nimport matplotlib.pyplot as plt\nX_train, X_test, y_train, y_test = train_test_split(X, Y, test_size = 0.2, random_state = 42)","8ce9c5f6":"from sklearn.preprocessing import LabelEncoder\nfrom keras.utils import np_utils\n\nseed_random = 115\n\nlabel_encoder = LabelEncoder()\nlabel_encoder = label_encoder.fit(y_train)\n\nx_train, x_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.3, random_state=seed_random)","75a17ba3":"x_test=X_test\nfrom keras import backend as K\nfrom keras.losses import categorical_crossentropy,mean_squared_error\n#input_size = len(x_train.columns)\n#print(input_size)\ndef keras_custom_loss(y_true, y_pred):\n    categorical_crossentropy_value=categorical_crossentropy(y_true, y_pred)\n    mean_squared_error_value=mean_squared_error(y_true, y_pred)\n    custom_loss_value=categorical_crossentropy_value+0.2*mean_squared_error_value\n    return custom_loss_value","8af892f0":"x_train_label, x_train_unlabel, y_train_label, y_train_unlabel=train_test_split(x_train, y_train, test_size=0.99, random_state=15)","3e880af5":"#x_train_label","925fe55d":"#x_train_label.drop('col2',axis=1)\n#x_train_unlabel.drop('col2',axis=1)\n#x_test.drop('col2',axis=1)","d7f571bc":"from keras.layers import Input, Dense, Concatenate, BatchNormalization\nfrom keras.models import Model\nfrom keras.optimizers import Adam, Nadam\nfrom keras.losses import categorical_crossentropy,mean_squared_error\nfrom keras.utils import normalize\nfrom sklearn.utils import class_weight\n# \u8fd9\u90e8\u5206\u8fd4\u56de\u4e00\u4e2a\u5f20\u91cf\ninputs = Input(shape=(175,))\n\n# \u5c42\u7684\u5b9e\u4f8b\u662f\u53ef\u8c03\u7528\u7684\uff0c\u5b83\u4ee5\u5f20\u91cf\u4e3a\u53c2\u6570\uff0c\u5e76\u4e14\u8fd4\u56de\u4e00\u4e2a\u5f20\u91cf\noutput_1 = Dense(256, activation='softplus')(inputs)\noutput_2 = Dense(128, activation='relu')(output_1)\na = BatchNormalization()(output_2)\noutput_3 = Dense(64, activation='relu')(a)\nz = Concatenate(axis=1)([inputs, output_3])\noutput_4 = Dense(32, activation='relu')(z)\npredictions = Dense(10, activation='softmax')(output_4)\n\n# \u8fd9\u90e8\u5206\u521b\u5efa\u4e86\u4e00\u4e2a\u5305\u542b\u8f93\u5165\u5c42\u548c\u4e09\u4e2a\u5168\u8fde\u63a5\u5c42\u7684\u6a21\u578b\ndeep_model = Model(inputs=inputs, outputs=predictions)\n\ndeep_model.compile(loss=keras_custom_loss, \n                   optimizer=Adam(learning_rate=0.01, beta_1=0.9, beta_2=0.99, amsgrad=True),\n                   metrics=['accuracy'])\nweights = class_weight.compute_class_weight('balanced',\n                                            np.unique(y_train),\n                                            y_train)","9f5adfb3":"y_train_econded = label_encoder.transform(y_train)\ny_train_label_econded = label_encoder.transform(y_train_label)\ny_train_unlabel_econded = label_encoder.transform(y_train_unlabel)\ny_val_econded = label_encoder.transform(y_val)\ny_test_econded = label_encoder.transform(y_test)\n\ny_train_dummy = np_utils.to_categorical(y_train_econded)\ny_train_label_dummy = np_utils.to_categorical(y_train_label_econded)\ny_train_unlabel_dummy = np_utils.to_categorical(y_train_unlabel_econded)\ny_val_dummy = np_utils.to_categorical(y_val_econded)\ny_test_dummy = np_utils.to_categorical(y_test_econded)","7fd33e1f":"deep_model.fit(x_train_label, y_train_label_dummy, \n               epochs=300, \n               batch_size=5000,\n               validation_data=(x_val, y_val_dummy),\n               class_weight=weights\n              )","f0e97de0":"deep_train_pred = deep_model.predict(x_train_unlabel)\ndeep_train_pred=np.argmax(deep_train_pred,axis=1)\ndeep_train_pred_decoded = label_encoder.inverse_transform(deep_train_pred)","0a954fdb":"set(deep_train_pred_decoded)","d12d85ef":"for i in range(1):\n    deep_train_pred = deep_model.predict(x_train_unlabel)\n    deep_train_pred=np.argmax(deep_train_pred,axis=1)\n    deep_train_pred_decoded = label_encoder.inverse_transform(deep_train_pred)\n    y_unlabel_econded = label_encoder.transform(deep_train_pred_decoded)\n    y_unlabel_dummy = np_utils.to_categorical(y_unlabel_econded)\n    deep_model.fit(x_train_unlabel, y_unlabel_dummy, \n               epochs=25, \n               batch_size=2500,\n               validation_data=(x_val, y_val_dummy))\n    deep_model.fit(x_train_label, y_train_label_dummy, \n               epochs=300, \n               batch_size=3000,\n               validation_data=(x_val, y_val_dummy),\n               class_weight=weights\n              )","88a1ad40":"deep_val_pred = deep_model.predict(x_val)\ndeep_val_pred=np.argmax(deep_val_pred,axis=1)\ndeep_val_pred_decoded = label_encoder.inverse_transform(deep_val_pred)\n\ndeep_test_pred = deep_model.predict(x_test)\ndeep_test_pred=np.argmax(deep_test_pred,axis=1)\ndeep_test_pred_decoded = label_encoder.inverse_transform(deep_test_pred)","23230d06":"import seaborn as sns\nfrom sklearn.metrics import confusion_matrix, f1_score, mean_absolute_error, make_scorer \nf1_score(y_val, deep_val_pred_decoded, average = 'macro')","1f7af167":"f1_score(y_test, deep_test_pred_decoded, average = 'macro')","b2e8c048":"#pred = xg_clf.fit(X_train, y_train).predict(X_test)\n#print(classification_report(y_test, pred))\n#print(confusion_matrix(y_test, pred))","c7efa818":"#lf = svm.SVC(gamma='auto')\n#predsvm=clf.fit(X_train,y_train).predict(X_test)\n#print(classification_report(y_test, predsvm))\n#print(confusion_matrix(y_test, predsvm))","dd14ad6b":"## import","5fa1a6ef":"### \u6570\u503c\u578b\u53d8\u91cf","9575d4f9":"## 3\u3001Data cleaning","34c3522d":"### \u79bb\u6563\u578b\u53d8\u91cf","402f046e":"## 4\u3001Machine Learning Analysis","053cf740":"### Evaluation of the training dataset","99fe365d":"#### proto\u548cservice\u5728\u6b63\u5e38\u6570\u636e\u91cc\u7684\u5206\u5e03","a6246e50":"\u8be5\u6570\u636e\u96c6\u6709\u4e5d\u79cd\u7c7b\u578b\u7684\u653b\u51fb\uff0c\u5373\u6a21\u7cca\u653b\u51fb\u3001\u5206\u6790\u653b\u51fb\u3001\u540e\u95e8\u653b\u51fb\u3001DoS\u653b\u51fb\u3001\u5229\u7528\u653b\u51fb\u3001\u901a\u7528\u653b\u51fb\u3001\u4fa6\u5bdf\u653b\u51fb\u3001\u5916\u58f3\u4ee3\u7801\u653b\u51fb\u548c\u8815\u866b\u653b\u51fb\u3002\uff08Fuzzers, Analysis, Backdoors, DoS, Exploits, Generic, Reconnaissance, Shellcode and Worms.\uff09","34adde66":"# Predicting attacks : UNSW-NB 15 dataset","2277f2d9":"#### proto\u548cservice\u5728\u653b\u51fb\u6570\u636e\u91cc\u7684\u5206\u5e03","968debea":"## \u6570\u636e\u96c6\u7b80\u4ecb","52f2b2a7":"## 2. Dataset Observation","88814bc4":"## 1. Loading Data"}}