{"cell_type":{"d2d5fb52":"code","c6ef2eb2":"code","41b32cc7":"code","2b29c72f":"code","a7b622d6":"code","83265346":"code","a0fc635e":"code","ac033d93":"code","982ef91e":"code","1a204d84":"code","9bb3772d":"code","aa7f8d4e":"code","fb968405":"code","41a85bc9":"code","c4a059d9":"code","338904a8":"code","26d04dfa":"code","819c1e2f":"code","c3814331":"code","a648d48b":"code","41b2b1b8":"code","d472807a":"code","ed11d166":"code","7092bd03":"code","9282fbe5":"code","435322f1":"code","91274afd":"code","85501e6a":"code","0e787388":"code","da7a3845":"code","4d1fede4":"code","b5ba677d":"markdown","32e0b69c":"markdown","32bcb3ba":"markdown","2e62e3c1":"markdown","43bc33be":"markdown","3aea4961":"markdown","0b0931a0":"markdown","9aace5e5":"markdown","44173d3c":"markdown","b72d8b5e":"markdown","79e199fc":"markdown"},"source":{"d2d5fb52":"import matplotlib.pyplot as plt\n\nfrom sklearn.model_selection import train_test_split\nfrom keras.datasets import fashion_mnist\nfrom keras.models import Sequential\nfrom keras.utils.np_utils import to_categorical","c6ef2eb2":"(X_train, y_train), (X_test, y_test) = fashion_mnist.load_data()","41b32cc7":"plt.imshow(X_train[0])    # show first number in the dataset\nplt.show()\nprint('Label: ', y_train[0])\n","2b29c72f":"plt.imshow(X_test[0])    # show first number in the dataset\nplt.show()\nprint('Label: ', y_test[0])","a7b622d6":"# reshaping X data: (n, 28, 28) => (n, 784)\nX_train = X_train.reshape((X_train.shape[0], -1))\nX_test = X_test.reshape((X_test.shape[0], -1))","83265346":"# converting y data into categorical (one-hot encoding)\ny_train = to_categorical(y_train)\ny_test = to_categorical(y_test)","a0fc635e":"print(X_train.shape, X_test.shape, y_train.shape, y_test.shape)","ac033d93":"from keras.models import Sequential\nfrom keras.layers import Activation, Dense\nfrom keras import optimizers","982ef91e":"model = Sequential()","1a204d84":"  model.add(Dense(50, input_shape = (784, )))\n  model.add(Activation('sigmoid'))\n  model.add(Dense(50))\n  model.add(Activation('sigmoid'))\n  model.add(Dense(50))\n  model.add(Activation('sigmoid'))\n  model.add(Dense(50))\n  model.add(Activation('sigmoid'))\n  model.add(Dense(10))\n  model.add(Activation('softmax'))","9bb3772d":"sgd = optimizers.SGD(lr = 0.01)\nmodel.compile(optimizer = sgd, loss = 'categorical_crossentropy', metrics = ['accuracy'])","aa7f8d4e":"history = model.fit(X_train, y_train, batch_size = 200, epochs = 100, verbose = 1)","fb968405":"results = model.evaluate(X_test, y_test)","41a85bc9":"print('Test accuracy: ', results[1])","c4a059d9":"# from now on, create a function to generate (return) models\ndef mlp_model():\n    model = Sequential()\n    \n    model.add(Dense(50, input_shape = (784, ), kernel_initializer='he_normal'))     # use he_normal initializer\n    model.add(Activation('sigmoid'))    \n    model.add(Dense(50, kernel_initializer='he_normal'))                            # use he_normal initializer\n    model.add(Activation('sigmoid'))    \n    model.add(Dense(50, kernel_initializer='he_normal'))                            # use he_normal initializer\n    model.add(Activation('sigmoid'))    \n    model.add(Dense(50, kernel_initializer='he_normal'))                            # use he_normal initializer\n    model.add(Activation('sigmoid'))    \n    model.add(Dense(10, kernel_initializer='he_normal'))                            # use he_normal initializer\n    model.add(Activation('softmax'))\n    \n    sgd = optimizers.SGD(lr = 0.001)\n    model.compile(optimizer = sgd, loss = 'categorical_crossentropy', metrics = ['accuracy'])\n    \n    return model","338904a8":"model = mlp_model()\nhistory = model.fit(X_train, y_train, batch_size=200, epochs = 100, verbose = 1)","26d04dfa":"results = model.evaluate(X_test, y_test)","819c1e2f":"print('Test accuracy: ', results[1])\n","c3814331":"def mlp_model():\n    model = Sequential()\n    \n    model.add(Dense(50, input_shape = (784, )))\n    model.add(Activation('relu'))    \n    model.add(Dense(50))\n    model.add(Activation('relu'))    \n    model.add(Dense(50))\n    model.add(Activation('relu'))    \n    model.add(Dense(50))\n    model.add(Activation('relu'))    \n    model.add(Dense(10))\n    model.add(Activation('softmax'))\n    \n    sgd = optimizers.SGD(lr = 0.001)\n    model.compile(optimizer = sgd, loss = 'categorical_crossentropy', metrics = ['accuracy'])\n    \n    return model","a648d48b":"model = mlp_model()\nhistory = model.fit(X_train, y_train, epochs = 10, verbose = 1)","41b2b1b8":"results = model.evaluate(X_test, y_test)\n","d472807a":"print('Test accuracy: ', results[1])","ed11d166":"from keras.layers import BatchNormalization, Dropout\n","7092bd03":"def mlp_model():\n    model = Sequential()\n    \n    model.add(Dense(50, input_shape = (784, )))\n    model.add(BatchNormalization())                    \n    model.add(Activation('relu'))    \n    model.add(Dense(50))\n    model.add(BatchNormalization())                    \n    model.add(Activation('relu'))    \n    model.add(Dense(50))\n    model.add(BatchNormalization())                    \n    model.add(Activation('relu'))    \n    model.add(Dense(50))\n    model.add(BatchNormalization())                    \n    model.add(Activation('relu'))    \n    model.add(Dense(10))\n    model.add(Activation('softmax'))\n    \n    sgd = optimizers.SGD(lr = 0.001)\n    model.compile(optimizer = sgd, loss = 'categorical_crossentropy', metrics = ['accuracy'])\n    \n    return model","9282fbe5":"model = mlp_model()\nhistory = model.fit(X_train, y_train, epochs = 20, verbose = 1)\n","435322f1":"results = model.evaluate(X_test, y_test)","91274afd":"print('Test accuracy: ', results[1])\n","85501e6a":"def mlp_model():\n    model = Sequential()\n    \n    model.add(Dense(50, input_shape = (784, ), kernel_initializer='he_normal'))\n    model.add(BatchNormalization())\n    model.add(Activation('relu'))\n    model.add(Dropout(0.2))\n    model.add(Dense(50, kernel_initializer='he_normal'))\n    model.add(BatchNormalization())\n    model.add(Activation('relu'))    \n    model.add(Dropout(0.2))\n    model.add(Dense(50, kernel_initializer='he_normal'))\n    model.add(BatchNormalization())\n    model.add(Activation('relu'))\n    model.add(Dropout(0.2))\n    model.add(Dense(50, kernel_initializer='he_normal'))\n    model.add(BatchNormalization())\n    model.add(Activation('relu'))\n    model.add(Dropout(0.2))\n    model.add(Dense(10, kernel_initializer='he_normal'))\n    model.add(Activation('softmax'))\n    \n    adam = optimizers.Adam(lr = 0.001)\n    model.compile(optimizer = adam, loss = 'categorical_crossentropy', metrics = ['accuracy'])\n    \n    return model","0e787388":"model = mlp_model()\nhistory = model.fit(X_train, y_train, epochs = 10, verbose = 1)","da7a3845":"results = model.evaluate(X_test, y_test)","4d1fede4":"print('Test accuracy: ', results[1])\n","b5ba677d":"### 2. Nonlinearity (Activation function)\n\nSigmoid functions suffer from gradient vanishing problem, making training slower\n\nThere are many choices apart from sigmoid and tanh; try many of them!\n\n'relu' (rectified linear unit) is one of the most popular ones\n\nRef: https:\/\/keras.io\/activations\/","32e0b69c":"### Dropout","32bcb3ba":"### Load dataset\n\nFashion-MNIST dataset\n\nsource: https:\/\/www.kaggle.com\/zalando-research\/fashionmnist\n","2e62e3c1":"Batch normalization layer is usually inserted after dense\/convolution and before nonlinearity\n\n","43bc33be":"### Data Pre-processing","3aea4961":"### Basic NN model\n\nNaive MLP model without any alterations","0b0931a0":"### 1. Weight Initialization\n\nChanging weight initialization scheme can significantly improve training of the model by preventing vanishing gradient problem up to some degree\n\nRef: https:\/\/keras.io\/initializers\/","9aace5e5":"### 3. Batch Normalization\n\nBatch Normalization, one of the methods to prevent the \"internal covariance shift\" problem, has proven to be highly effective\n\nNormalize each mini-batch before nonlinearity\n\nRef: https:\/\/keras.io\/optimizers\/","44173d3c":"## Image Classification\n\n\nImage classification is one of the important use cases in our daily life. Automotive, e-commerce, retail, manufacturing industries, security, surveillance, healthcare, farming etc., can have a wide application of image classification.\n\n**Objective:** In this notebook, we will build a neural network to classifiy the image based on the object present in the image.\n","b72d8b5e":"### About Dataset\n\n\nEach image is 28 pixels in height and 28 pixels in width, for a total of 784 pixels in total. Each pixel has a single pixel-value associated with it, indicating the lightness or darkness of that pixel, with higher numbers meaning darker. This pixel-value is an integer between 0 and 255. The training and test data sets have 785 columns. The first column consists of the class labels (see above), and represents the article of clothing. The rest of the columns contain the pixel-values of the associated image.\n\n#### Labels\n\nEach training and test example is assigned to one of the following labels:\n\n0 T-shirt\/top\n\n1 Trouser\n\n2 Pullover\n\n3 Dress\n\n4 Coat\n\n5 Sandal\n\n6 Shirt\n\n7 Sneaker\n\n8 Bag\n\n9 Ankle boot ","79e199fc":"\n## Advanced techniques for training neural networks\n\nWeight Initialization\n\nNonlinearity (different Activation functions)\n\nOptimizers(different optimizers)\n\nBatch Normalization\n\nDropout"}}