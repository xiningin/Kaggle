{"cell_type":{"12ed0132":"code","19637fa9":"code","3a4b19ce":"code","93483249":"code","e2c13e5f":"code","5b460de9":"code","4dacc557":"code","539e1c44":"code","b4bc58d7":"code","96bbc545":"code","2d99f913":"code","bb03741e":"code","bb4622aa":"code","90181249":"code","6854fc8a":"code","4e66f60c":"code","69d0a22f":"code","81cfde0a":"code","ef756f83":"code","3133df92":"code","7db651c3":"code","e1fdc17c":"code","05784b33":"code","3ca97fcb":"code","08c55697":"code","aa7ec308":"code","760da1c2":"code","eb7be122":"code","64900b08":"code","9c6285e9":"markdown","facdab58":"markdown","26b7f143":"markdown","b70cfdc5":"markdown","9e3ab491":"markdown","a7e747fc":"markdown","e5a0aba6":"markdown","da601a42":"markdown","279383d9":"markdown","a01263ff":"markdown"},"source":{"12ed0132":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport warnings\nwarnings.filterwarnings('ignore')","19637fa9":"data_train = pd.read_csv('..\/input\/mobile-price-classification\/train.csv')\ndata_test = pd.read_csv('..\/input\/mobile-price-classification\/test.csv')\ndf = data_train.copy()\npd.set_option('display.max_row',df.shape[0])\npd.set_option('display.max_column',df.shape[1]) \ndf.head()","3a4b19ce":"df.dtypes.value_counts() # Compte les nombre de types de variables","93483249":"print('There is' , df.shape[0] , 'rows')\nprint('There is' , df.shape[1] , 'columns')","e2c13e5f":"plt.figure(figsize=(10,10))\nsns.heatmap(df.isna(),cbar=False)\nplt.show()","5b460de9":"df['price_range'].value_counts(normalize=True) #Classes d\u00e9s\u00e9quilibr\u00e9es","4dacc557":"for col in df.select_dtypes(include=['float64','int64']):\n    plt.figure()\n    sns.displot(df[col],kind='kde',height=3)\n    plt.show()","539e1c44":"for col in df.select_dtypes(include=['float64','int64']):\n    print(f'{col :-<50} {df[col].unique()}')","b4bc58d7":"def encoding(df):\n    code = {\n           }\n    for col in df.select_dtypes('object'):\n        df.loc[:,col]=df[col].map(code)\n        \n    return df\n\ndef imputation(df):\n    \n    #df = df.dropna(axis=0)\n    df = df.fillna(df.mean())\n    \n    return df\n\ndef feature_engineering(df):\n    useless_columns = []\n    for feature in useless_columns:\n        if feature in df:\n            df = df.drop(feature,axis=1)\n    return df\n\n\n# No changes on the data sets to do here (just need to scale features)","96bbc545":"def preprocessing(df):\n    df = encoding(df)\n    df = feature_engineering(df)\n    df = imputation(df)\n    \n    X = df.drop('price_range',axis=1)\n    y = df['price_range'].astype(int)\n      \n    return df,X,y","2d99f913":"df=data_train.copy()\ndf,X,y = preprocessing(df)\ndf.head()","bb03741e":"Range_0 = df[y == 0]\nRange_1 = df[y == 1]\nRange_2 = df[y == 2]\nRange_3 = df[y == 3]","bb4622aa":"corr = df.corr(method='pearson').abs()\n\nfig = plt.figure(figsize=(30,20))\nsns.heatmap(corr, annot=True, cmap='tab10', vmin=0, vmax=+1)\nplt.title('Pearson Correlation')\nplt.show()","90181249":"df.corr()['price_range'].abs().sort_values()","6854fc8a":"for col in df.columns:\n    plt.figure(figsize=(4,4))\n    plt.title(col)\n    sns.distplot(Range_0[col],label = \"Range 0\")\n    sns.distplot(Range_1[col],label = \"Range 1\")\n    sns.distplot(Range_2[col],label = \"Range 2\")\n    sns.distplot(Range_3[col],label = \"Range 3\")\n    plt.legend()\n    plt.show()","4e66f60c":"from sklearn.model_selection import train_test_split\ndf = data_train.copy()\ntrainset, valset = train_test_split(df, test_size=0.2, random_state=0)\nprint(trainset['price_range'].value_counts())\nprint(valset['price_range'].value_counts())","69d0a22f":"_, X_train, y_train = preprocessing(trainset)\n_, X_val, y_val = preprocessing(valset)","81cfde0a":"from sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import StandardScaler, RobustScaler\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.decomposition import PCA","ef756f83":"preprocessor = make_pipeline(StandardScaler())\n\nPCAPipeline = make_pipeline(preprocessor, PCA(n_components=2,random_state=0))\n\nRandomPipeline = make_pipeline(preprocessor,RandomForestClassifier(random_state=0))\nAdaPipeline = make_pipeline(preprocessor,AdaBoostClassifier(random_state=0))\nSVMPipeline = make_pipeline(preprocessor,SVC(random_state=0,probability=True))\nKNNPipeline = make_pipeline(preprocessor,KNeighborsClassifier())\nLRPipeline = make_pipeline(preprocessor,LogisticRegression(solver='sag'))","3133df92":"PCA_df = pd.DataFrame(PCAPipeline.fit_transform(X))\nPCA_df = pd.concat([PCA_df, data_train['price_range']], axis=1)\nPCA_df.head()","7db651c3":"plt.figure(figsize=(8,8))\nsns.scatterplot(PCA_df[0],PCA_df[1],hue=PCA_df['price_range'],palette=sns.color_palette(\"tab10\", 4))\nplt.show()","e1fdc17c":"dict_of_models = {'RandomForest': RandomPipeline,\n'AdaBoost': AdaPipeline,\n'SVM': SVMPipeline,\n'KNN': KNNPipeline,\n'LR': LRPipeline}","05784b33":"from sklearn.metrics import f1_score, accuracy_score, confusion_matrix, classification_report, roc_curve\nfrom sklearn.model_selection import learning_curve, cross_val_score, GridSearchCV\n\ndef evaluation(model):\n    model.fit(X_train, y_train)\n    # calculating the probabilities\n    y_pred_proba = model.predict_proba(X_val)\n\n    # finding the predicted valued\n    y_pred = np.argmax(y_pred_proba,axis=1)\n    print('Accuracy = ', accuracy_score(y_val, y_pred))\n    print('-')\n    print(confusion_matrix(y_val,y_pred))\n    print('-')\n    print(classification_report(y_val,y_pred))\n    print('-')\n    \n    N, train_score, val_score = learning_curve(model, X_train, y_train, cv=4, scoring='accuracy', train_sizes=np.linspace(0.1,1,10))\n    \n    plt.figure(figsize=(8,6))\n    plt.plot(N, train_score.mean(axis=1), label='train score')\n    plt.plot(N, val_score.mean(axis=1), label='validation score')\n    plt.legend()","3ca97fcb":"for name, model in dict_of_models.items():\n    print('---------------------------------')\n    print(name)\n    evaluation(model)","08c55697":"from sklearn.metrics import roc_curve, auc\nfrom sklearn.preprocessing import label_binarize\nfrom sklearn.multiclass import OneVsRestClassifier\n\n# Binarize the output\ny_train = label_binarize(y_train, classes=[0, 1, 2, 3])\ny_val = label_binarize(y_val, classes=[0, 1, 2, 3])\nn_classes = y_train.shape[1]\n\n# Learn to predict each class against the other\nclassifier = OneVsRestClassifier(LRPipeline)\ny_score = classifier.fit(X_train, y_train).decision_function(X_val)\n\n# Compute ROC curve and ROC area for each class\nfpr = dict()\ntpr = dict()\nroc_auc = dict()\nfor i in range(n_classes):\n    fpr[i], tpr[i], _ = roc_curve(y_val[:, i], y_score[:, i])\n    roc_auc[i] = auc(fpr[i], tpr[i])\n\n# Compute micro-average ROC curve and ROC area\nfpr[\"micro\"], tpr[\"micro\"], _ = roc_curve(y_val.ravel(), y_score.ravel())\nroc_auc[\"micro\"] = auc(fpr[\"micro\"], tpr[\"micro\"])\n\n# # Plot of a ROC curve for a specific class\n# plt.figure()\n# plt.plot(fpr[2], tpr[2], label='ROC curve (area = %0.2f)' % roc_auc[2])\n# plt.plot([0, 1], [0, 1], 'k--')\n# plt.xlim([0.0, 1.0])\n# plt.ylim([0.0, 1.05])\n# plt.xlabel('False Positive Rate')\n# plt.ylabel('True Positive Rate')\n# plt.title('Receiver operating characteristic for class 2')\n# plt.legend(loc=\"lower right\")\n# plt.show()\n\n# Plot ROC curve\nplt.figure()\nplt.plot(fpr[\"micro\"], tpr[\"micro\"],\n         label='micro-average ROC curve (area = {0:0.2f})'\n               ''.format(roc_auc[\"micro\"]))\nfor i in range(n_classes):\n    plt.plot(fpr[i], tpr[i], label='ROC curve of class {0} (area = {1:0.2f})'\n                                   ''.format(i, roc_auc[i]))\n\nplt.plot([0, 1], [0, 1], 'k--')\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('Receiver operating characteristic to multi-class')\nplt.legend(loc=\"lower right\")\nplt.show()","aa7ec308":"df_test=data_test.copy()\ndf_test = df_test.drop('id',axis=1)\ndf_test.head()","760da1c2":"predicted_proba = classifier.predict_proba(df_test)\ni=0\nfor price_range in predicted_proba:\n    print(\"Id :\",i,\" - Price_range :\",np.argmax(price_range,axis=0),\"with probability\",round(max(price_range)*100),\"%\")\n    i+=1","eb7be122":"predictions = classifier.predict(df_test)\npredicted_range=[]\nfor price_range in predictions :\n    predicted_range.append(np.argmax(price_range))\nprint(predicted_range)","64900b08":"df_test['Price_range'] = predicted_range\ndf_test.head()","9c6285e9":"# Exploratory Data Analysis\n\n## Aim :\n- Understand the data (\"A small step forward is better than a big one backwards\")\n- Begin to develop a modelling strategy\n- Optimize the model having the best accuracy on the train set\n\n## Target\n\nprice_range\n\n## Features\n\n* id\n* battery_power : Total energy a battery can store in one time measured in mAh\n* blue : Has bluetooth or not\n* clock_speed : speed at which microprocessor executes instructions \n* dual_sim : Has dual sim support or not\n* fc : Front Camera mega pixels\n* four_g : Has 4G or not\n* int_memory : Internal Memory in Gigabytes\n* m_dep : Mobile Depth in cm\n* mobile_wt : Weight of mobile phone\n* n_cores : Number of cores of processor\n* pc : Primary Camera mega pixels\n* px_height : Pixel Resolution Height\n* px_width : Pixel Resolution Width\n* ram : Random Access Memory in Megabytes\n* sc_h : Screen Height of mobile in cm\n* sc_w : Screen Width of mobile in cm\n* talk_time : longest time that a single battery charge will last when you are \n* three_g : Has 3G or not\n* touch_screen : Has touch screen or not\n* wifi : Has wifi or not\n\n## Base Checklist\n#### Shape Analysis :\n- **target feature** : price_range\n- **rows and columns (train set)** : 2000 , 21\n- **rows and columns (test set)** : 1000 , 21\n- **features types** : qualitatives : 0 , quantitatives : 20\n- **NaN analysis** :\n    - NaN  : 0%\n\n#### Columns Analysis :\n- **Target Analysis** :\n    - Balanced (Yes\/No) : Yes\n    - Percentages : 4 classes, repr. 25% of the dataset each (perfectly balanced)\n- **Categorical values**\n    - There is 6 binary categorical features (not inluding the target)","facdab58":"# A bit of data engineering ...","26b7f143":"# Modelling","b70cfdc5":"# Conclusion : 95.5% Accuracy reached using LogisticRegressor\n\nFor the 5 models tested hereabove, here are the accuracies :\n- LogisticRegressor : 95.5%\n- SVM : 90%\n- RandomForest : 86%\n- AdaBoost : 70%\n- KNN : 51%\n\n### The best model is the LogisticRegressor","9e3ab491":"## Classification problem","a7e747fc":"# Predictions","e5a0aba6":"## PCA Analysis","da601a42":"# Detailed analysis","279383d9":"# If you like please upvote !\n## Also check my other notebooks :\n#### EDA & Modelling (95.5% acc.) - Mobile price : https:\/\/www.kaggle.com\/dorianvoydie\/eda-modelling-95-5-acc-mobile-price\n#### EDA & Modelling - Mice (100% acc.) : https:\/\/www.kaggle.com\/dorianvoydie\/eda-modelling-mice-100-acc\n#### EDA & Modelling - Breast Cancer Detection : https:\/\/www.kaggle.com\/dorianvoydie\/eda-modelling-breast-cancer-detection\n#### Accuracy 99% - Trying several models : https:\/\/www.kaggle.com\/dorianvoydie\/accuracy-99-trying-several-models\n#### Meteo Forecasting : https:\/\/www.kaggle.com\/dorianvoydie\/meteo-forecasting\n#### EDA & Modelling - Heart Attack 90% Accuracy Score : https:\/\/www.kaggle.com\/dorianvoydie\/eda-modelling-heart-attack-90-accuracy-score","a01263ff":"## Examining target and features"}}