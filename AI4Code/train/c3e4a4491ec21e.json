{"cell_type":{"0ce2e399":"code","d0b9daa7":"code","737e3b8e":"code","5b51aa67":"code","afddf901":"code","41a48091":"code","5cf1f3fc":"code","41ca8336":"code","516453c5":"code","cc9e83b9":"code","a2cda3b1":"code","30c2e3a5":"code","6e6c21bf":"code","68eb6d83":"code","a87f04b1":"code","ae726952":"code","c55ca8e8":"code","03bf7eb2":"code","cf626efb":"code","0a5ca568":"code","d042f084":"code","4cd60ebd":"code","7d6fed29":"code","86584a73":"code","f9f226d3":"code","2b7309bd":"code","d2244fba":"code","f6dfc168":"code","2a28e125":"code","0b4e8560":"code","0d0e8b47":"code","96119374":"code","86c4867c":"code","b46d35e8":"code","11df9c3f":"code","a56939a7":"code","bc1ef1da":"code","f4030a06":"code","41ac4bc9":"code","32ceaff7":"code","bf2e4eae":"code","8f79f215":"code","34081ebf":"code","59d3ae07":"code","bbcec3df":"code","b4efcd38":"code","b4702540":"code","401bedcb":"code","696b44ed":"code","b14ba739":"code","16c2bb5b":"code","01fc8245":"code","ca1e6ca5":"code","c134feb7":"markdown","e8c3c156":"markdown","27d93158":"markdown","5d996a3c":"markdown","d71291b4":"markdown","61953486":"markdown","f8693da5":"markdown","1bdfae07":"markdown","8d875fed":"markdown","0977ab99":"markdown","68937feb":"markdown","81aed98e":"markdown","077b10fe":"markdown","14eedae8":"markdown","7a0df2f9":"markdown","21adb426":"markdown"},"source":{"0ce2e399":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","d0b9daa7":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns","737e3b8e":"df = pd.read_csv(\"..\/input\/Reviews.csv\")","5b51aa67":"df=df.sample(1000)","afddf901":"df.reset_index(inplace=True)","41a48091":"df.drop(['index'],axis=1,inplace=True)","5cf1f3fc":"df.shape","41ca8336":"df.head()","516453c5":"df['Score'].value_counts()","cc9e83b9":"df['Score'].isna().sum()","a2cda3b1":"x=[]\nfor i in df['Score'] :\n    if i<3 :\n        x.append(0)\n    else :\n        x.append(1)","30c2e3a5":"df['target']=x","6e6c21bf":"df_new=df[['Text','Summary','Score','target']]","68eb6d83":"df_new.head()","a87f04b1":"special_char=[]\nfor i in df_new['Text']:\n    for j in i:\n        if j.isalpha()==False:\n            special_char.append(j)","ae726952":"alpha_char=[]\nnum_char=[]\nfor i in df_new['Text'].str.split():\n    for j in i:\n        if j.isalpha()==False:\n            num_char.append(j)\n        else:\n            alpha_char.append(j)","c55ca8e8":"len(alpha_char)","03bf7eb2":"len(num_char)","cf626efb":"from nltk import word_tokenize\nfrom nltk.corpus import stopwords\nstop = set(stopwords.words('english'))","0a5ca568":"x=[]\nfor i in alpha_char:\n    for j in stop:\n        if i not in j:\n            x.append(i)","d042f084":"x = set(x)","4cd60ebd":"len(x)","7d6fed29":"import nltk\nporter = nltk.PorterStemmer()","86584a73":"words_stemming=[]\nfor i in x:\n       words_stemming.append(porter.stem(i))","f9f226d3":"words_stemming[0:10]","2b7309bd":"import nltk\nnltk.download('wordnet')","d2244fba":"WNlemma = nltk.WordNetLemmatizer()\nword_list_lem=[WNlemma.lemmatize(t) for t in words_stemming]","f6dfc168":"words_lem=[]\nfor i in words_stemming:\n    WNlemma = nltk.WordNetLemmatizer()\n    words_lem.append(WNlemma.lemmatize(i))","2a28e125":"len(words_lem)","0b4e8560":"print(len(set(words_stemming)))\nprint(len(set(words_lem)))","0d0e8b47":"import nltk\nfrom nltk.corpus import stopwords\nfrom nltk.classify import SklearnClassifier\nfrom wordcloud import WordCloud,STOPWORDS\nimport matplotlib.pyplot as plt","96119374":"x=\" \"\nfor i in words_lem:\n       x=x+\" \"+i","86c4867c":"x=str(x)\nx=x[0:10000]","b46d35e8":"wordcloud = WordCloud(stopwords=STOPWORDS,\n                      background_color='black',\n                      width=2500,\n                      height=2000\n                     ).generate(x)\nplt.figure(1,figsize=(13, 13))\nplt.imshow(wordcloud)\nplt.axis('off')\nplt.show()","11df9c3f":"df_new.shape","a56939a7":"len(words_lem)","bc1ef1da":"df_new.columns","f4030a06":"df_new.values","41ac4bc9":"x=[]\nfor j in words_lem:\n    for i in df_new.Text:\n        if j in i:\n            x.append(1)\n        else:\n            x.append(0)","32ceaff7":"import numpy as np\nx=np.array(x)","bf2e4eae":"len(x)","8f79f215":"df_lem=pd.DataFrame(x.reshape(1000,6787))","34081ebf":"df_lem.columns=words_lem","59d3ae07":"df_lem.head()","bbcec3df":"y=df_new['target']\nx=df_new['Text']","b4efcd38":"from sklearn.model_selection import train_test_split\n# Split data into training and test sets\nX_train, X_test, y_train, y_test = train_test_split(x,y,test_size=0.20,random_state=0)\nprint(X_train.shape, X_test.shape, y_train.shape, y_test.shape)","b4702540":"from sklearn.feature_extraction.text import CountVectorizer\n\n# Fit the CountVectorizer to the training data\nvect = CountVectorizer().fit(X_train)\n# transform the documents in the training data to a document-term matrix\nX_train_vectorized = vect.transform(X_train)\nX_test_vectorized = vect.transform(X_test)","401bedcb":"from sklearn.linear_model import LogisticRegression\nmodel = LogisticRegression()\nmodel.fit(X_train_vectorized, y_train)\nmodel.predict(X_test_vectorized)\nfrom sklearn.metrics import confusion_matrix,accuracy_score\nprint(confusion_matrix(model.predict(X_test_vectorized),y_test))\nprint(accuracy_score(model.predict(X_test_vectorized),y_test))","696b44ed":"from sklearn.neighbors import KNeighborsClassifier\nmodel = KNeighborsClassifier(n_neighbors=2)\nmodel.fit(X_train_vectorized, y_train)\nmodel.predict(X_test_vectorized)\nfrom sklearn.metrics import confusion_matrix,accuracy_score\nprint(confusion_matrix(model.predict(X_test_vectorized),y_test))\nprint(accuracy_score(model.predict(X_test_vectorized),y_test))","b14ba739":"from sklearn.naive_bayes import BernoulliNB\nmodel = BernoulliNB()\nmodel.fit(X_train_vectorized, y_train)\nmodel.predict(X_test_vectorized)\nfrom sklearn.metrics import confusion_matrix,accuracy_score\nprint(confusion_matrix(model.predict(X_test_vectorized),y_test))\nprint(accuracy_score(model.predict(X_test_vectorized),y_test))","16c2bb5b":"from sklearn.tree import DecisionTreeClassifier\nmodel = DecisionTreeClassifier()\nmodel.fit(X_train_vectorized, y_train)\nmodel.predict(X_test_vectorized)\nfrom sklearn.metrics import confusion_matrix,accuracy_score\nprint(confusion_matrix(model.predict(X_test_vectorized),y_test))\nprint(accuracy_score(model.predict(X_test_vectorized),y_test))","01fc8245":"from sklearn.ensemble import RandomForestClassifier\nmodel = RandomForestClassifier()\nmodel.fit(X_train_vectorized, y_train)\nmodel.predict(X_test_vectorized)\nfrom sklearn.metrics import confusion_matrix,accuracy_score\nprint(confusion_matrix(model.predict(X_test_vectorized),y_test))\nprint(accuracy_score(model.predict(X_test_vectorized),y_test))","ca1e6ca5":"from sklearn.ensemble import BaggingClassifier\nmodel = BaggingClassifier()\nmodel.fit(X_train_vectorized, y_train)\nmodel.predict(X_test_vectorized)\nfrom sklearn.metrics import confusion_matrix,accuracy_score\nprint(confusion_matrix(model.predict(X_test_vectorized),y_test))\nprint(accuracy_score(model.predict(X_test_vectorized),y_test))","c134feb7":"## Lemmatization","e8c3c156":"# kNN Classification","27d93158":"## Get Special Character","5d996a3c":"# Word Clouds","d71291b4":"## Stemming","61953486":"## Manual Way of Count Vectorization","f8693da5":"# Logistic Regression","1bdfae07":"## Import Dataset","8d875fed":"## Converting it into Binary form","0977ab99":"# Decision Tree","68937feb":"## Import Library","81aed98e":"# Random Forest","077b10fe":"## Count Vectorization - In Python Library","14eedae8":"# Bagging Classifier","7a0df2f9":"## Stopwords Exclusion","21adb426":"# Naive Bayes"}}