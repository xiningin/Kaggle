{"cell_type":{"c3e52cbc":"code","e3eed413":"code","92bdac85":"code","cf8a262e":"code","cf4f3bf0":"code","8d353bd5":"code","bf455768":"code","3426c35e":"code","06434b82":"code","973d2da2":"code","4fe15ccf":"code","572bdfab":"code","9b164136":"code","98284abd":"code","163c0fa4":"code","55e75bbd":"code","429f9cfb":"code","9daf5f10":"code","f70b7cc4":"code","cbc9250b":"code","f362773c":"code","1c63951f":"code","a7cf1787":"code","c1f10744":"code","36a91092":"code","409cc6f7":"code","af3f45dc":"code","f13a652b":"code","1e78ac9a":"code","e849d5dd":"code","30d14957":"code","617356d2":"code","428688f6":"code","41546db4":"code","bd405afb":"code","a40f60a0":"code","b87b8620":"code","caa996ce":"code","75428549":"code","022ee688":"code","df464af7":"code","b23ab574":"code","5f2e3016":"code","d3cebd85":"code","58670e6f":"code","c1be6bb1":"code","3c133990":"code","faffaf3f":"code","b8d5dd4e":"code","0ec7dc2e":"code","75e85069":"code","51f2b0ce":"code","9ecd7ba4":"code","d0d7ba19":"code","c10764cb":"code","b69b8d65":"code","68916000":"code","af969f68":"code","c1ddef95":"code","0c4e2968":"code","504ae98d":"code","1c60de03":"code","3efa5ec2":"code","ec98499a":"code","025df527":"markdown","cf0f8499":"markdown","0afc68a9":"markdown","8ed97061":"markdown","75a5f292":"markdown","2c662311":"markdown","c3e76fb3":"markdown","257457c4":"markdown","95425ab8":"markdown","2bdba0f8":"markdown","ad9caf09":"markdown","01d2f401":"markdown","3d84c2d2":"markdown"},"source":{"c3e52cbc":"#GENERAL\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport random\n#PATH PROCESS\nimport os\nimport os.path\nfrom pathlib import Path\nimport glob\n#IMAGE PROCESS\nfrom PIL import Image\nfrom keras.preprocessing import image\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nimport cv2\nfrom keras.applications.vgg16 import preprocess_input, decode_predictions\nfrom keras.preprocessing import image\nfrom skimage.feature import hessian_matrix, hessian_matrix_eigvals\nfrom scipy.ndimage.filters import convolve\nfrom skimage import data, io, filters\nimport skimage\nfrom skimage.morphology import convex_hull_image, erosion\n#SCALER & TRANSFORMATION\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.preprocessing import MinMaxScaler\nfrom keras.utils.np_utils import to_categorical\nfrom sklearn.model_selection import train_test_split\nfrom keras import regularizers\nfrom sklearn.preprocessing import LabelEncoder\n#ACCURACY CONTROL\nfrom sklearn.metrics import confusion_matrix, accuracy_score, classification_report, roc_auc_score, roc_curve\nfrom sklearn.model_selection import GridSearchCV, cross_val_score\nfrom sklearn.metrics import mean_squared_error, r2_score\n#OPTIMIZER\nfrom keras.optimizers import RMSprop,Adam,Optimizer,Optimizer, SGD\n#MODEL LAYERS\nfrom tensorflow.keras.models import Sequential\nfrom keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPool2D, BatchNormalization,MaxPooling2D,BatchNormalization,\\\n                        Permute, TimeDistributed, Bidirectional,GRU, SimpleRNN, LSTM, GlobalAveragePooling2D, SeparableConv2D,\\\nZeroPadding2D, Convolution2D, ZeroPadding2D, Conv2DTranspose,ReLU, UpSampling2D, Concatenate, Conv2DTranspose\nfrom keras import models\nfrom keras import layers\nimport tensorflow as tf\nfrom keras.applications import VGG16,VGG19,inception_v3\nfrom keras import backend as K\nfrom keras.utils import plot_model\nfrom keras.models import load_model\nfrom keras import backend\nfrom tensorflow.keras.applications.inception_resnet_v2 import InceptionResNetV2, decode_predictions, preprocess_input\n#SKLEARN CLASSIFIER\nfrom xgboost import XGBClassifier, XGBRegressor\nfrom lightgbm import LGBMClassifier, LGBMRegressor\nfrom catboost import CatBoostClassifier, CatBoostRegressor\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\nfrom sklearn.ensemble import GradientBoostingClassifier, GradientBoostingRegressor\nfrom sklearn.ensemble import BaggingRegressor\nfrom sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor\nfrom sklearn.neural_network import MLPClassifier, MLPRegressor\nfrom sklearn.neighbors import KNeighborsClassifier, KNeighborsRegressor\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.cross_decomposition import PLSRegression\nfrom sklearn.linear_model import Ridge\nfrom sklearn.linear_model import RidgeCV\nfrom sklearn.linear_model import Lasso\nfrom sklearn.linear_model import LassoCV\nfrom sklearn.linear_model import ElasticNet\nfrom sklearn.linear_model import ElasticNetCV\n#IGNORING WARNINGS\nfrom warnings import filterwarnings\nfilterwarnings(\"ignore\",category=DeprecationWarning)\nfilterwarnings(\"ignore\", category=FutureWarning) \nfilterwarnings(\"ignore\", category=UserWarning)","e3eed413":"Gray_NPY = np.load(\"..\/input\/image-colorization\/l\/gray_scale.npy\")\nAB_1_NPY = np.load(\"..\/input\/image-colorization\/ab\/ab\/ab1.npy\")","92bdac85":"print(type(Gray_NPY))\nprint(type(AB_1_NPY))","cf8a262e":"print(Gray_NPY.shape)\nprint(AB_1_NPY.shape)","cf4f3bf0":"splitting_count = 300\nend_splitting_count = 600","8d353bd5":"X_Train = Gray_NPY[:splitting_count,:,:].astype(\"float32\").reshape(splitting_count,\n                                                                  Gray_NPY.shape[1],\n                                                                   Gray_NPY.shape[2])","bf455768":"X_Non_Seen = Gray_NPY[splitting_count:end_splitting_count,:,:].astype(\"float32\").reshape(splitting_count,\n                                                                  Gray_NPY.shape[1],\n                                                                   Gray_NPY.shape[2])","3426c35e":"Y_Train = AB_1_NPY[:splitting_count,:,:].astype(\"float32\")","06434b82":"print(X_Train.shape)\nprint(X_Non_Seen.shape)\nprint(Y_Train.shape)","973d2da2":"def line_image(gray_images,splitting_count=splitting_count,preprocess_function=preprocess_input):\n    Zeros_Imp = np.zeros((splitting_count,224,224,3))\n    for indexing in range(0,3):\n        Zeros_Imp[:splitting_count,:,:,indexing] = gray_images[:splitting_count]\n        \n    return preprocess_function(Zeros_Imp)","4fe15ccf":"Input_Images = line_image(X_Train,splitting_count)","572bdfab":"Non_Seen_Input = line_image(X_Non_Seen,splitting_count)","9b164136":"print(Input_Images.shape)","98284abd":"print(Non_Seen_Input.shape)","163c0fa4":"def from_lab_to_rgb(gray_images,ab_images,n=10):\n    Zeros_Imp = np.zeros((n,224,224,3))\n    \n    Zeros_Imp[:,:,:,0] = gray_images[0:n:]\n    Zeros_Imp[:,:,:,1:] = ab_images[0:n:]\n    \n    Zeros_Imp = Zeros_Imp.astype(\"uint8\")\n    \n    Main_Img = []\n    \n    for indexing in range(0,n):\n        Main_Img.append(cv2.cvtColor(Zeros_Imp[indexing],cv2.COLOR_LAB2RGB))\n        \n    Main_Img = np.array(Main_Img)\n    \n    return Main_Img","55e75bbd":"Output_Images = preprocess_input(from_lab_to_rgb(X_Train,Y_Train,n=splitting_count))","429f9cfb":"print(Output_Images.shape)","9daf5f10":"Gray_Zeros_Imp = np.zeros((splitting_count,Gray_NPY.shape[1],Gray_NPY.shape[2],1))\nGray_Zeros_Imp[:,:,:,0] = X_Train","f70b7cc4":"print(Gray_Zeros_Imp.shape)","cbc9250b":"print(Y_Train.shape)","f362773c":"print(Y_Train[:,:,:,1:].shape)","1c63951f":"print(Y_Train[:,:,:,1:][2].shape)","a7cf1787":"X_Train_Testing = np.zeros(shape=(splitting_count,224,224,3))\n\nfor indexing in range(len(Y_Train)):\n    X_Train_Testing[:,:,:,0][indexing] = X_Train[:,:,:][indexing]\n    X_Train_Testing[:,:,:,1:][indexing] = Y_Train[:,:,:,:][indexing]\n\nX_Train_Testing = X_Train_Testing.astype(\"uint8\")","c1f10744":"plt.imshow(X_Train_Testing[30]) # LAB","36a91092":"print(X_Train_Testing.shape)","409cc6f7":"Main_Img_Testing = []\n    \nfor indexing in range(0,splitting_count):\n    Main_Img_Testing.append(cv2.cvtColor(X_Train_Testing[indexing],cv2.COLOR_LAB2RGB))","af3f45dc":"plt.imshow(Main_Img_Testing[30]) # RGB","f13a652b":"figure,axis = plt.subplots(1,2,figsize=(10,10))\n\naxis[0].imshow(Output_Images[0])\naxis[1].imshow(Input_Images[0])","1e78ac9a":"figure,axis = plt.subplots(1,2,figsize=(10,10))\n\naxis[0].imshow(Output_Images[123])\naxis[1].imshow(Input_Images[123])","e849d5dd":"figure,axis = plt.subplots(1,2,figsize=(10,10))\n\naxis[0].imshow(Output_Images[0])\naxis[1].imshow(X_Train[0],cmap=\"gray\")","30d14957":"figure,axis = plt.subplots(1,2,figsize=(10,10))\n\naxis[0].imshow(Output_Images[10])\naxis[1].imshow(X_Train[10],cmap=\"gray\")","617356d2":"figure,axis = plt.subplots(1,2,figsize=(10,10))\n\naxis[0].imshow(Output_Images[100])\naxis[1].imshow(X_Train[100],cmap=\"gray\")","428688f6":"figure,axis = plt.subplots(1,2,figsize=(10,10))\n\naxis[0].imshow(Output_Images[200])\naxis[1].imshow(X_Train[200],cmap=\"gray\")","41546db4":"figure,axis = plt.subplots(1,2,figsize=(10,10))\n\naxis[0].imshow(Output_Images[13])\naxis[1].imshow(X_Train[13],cmap=\"gray\")","bd405afb":"compile_loss = \"mse\"\ncompile_optimizer = RMSprop(lr=0.0001,decay=1e-8)\ncompile_metrics = [\"accuracy\"]\ninput_dim = (Input_Images.shape[1],Input_Images.shape[2],Input_Images.shape[3])\noutput_class = 1","a40f60a0":"Early_Stopper = tf.keras.callbacks.EarlyStopping(monitor=\"loss\",patience=3,mode=\"min\")\nCheckpoint_Model = tf.keras.callbacks.ModelCheckpoint(monitor=\"val_accuracy\",\n                                                      save_best_only=True,\n                                                      save_weights_only=True,\n                                                      filepath=\".\/modelcheck\")","b87b8620":"Encoder_G = Sequential()\nEncoder_G.add(Conv2D(32,(2,2),kernel_initializer = 'he_normal',padding = \"same\",use_bias = True))\nEncoder_G.add(BatchNormalization())\nEncoder_G.add(ReLU())\n#\nEncoder_G.add(Conv2D(64,(2,2),kernel_initializer = 'he_normal',padding = \"same\",use_bias = True))\nEncoder_G.add(BatchNormalization())\nEncoder_G.add(ReLU())\n#\nEncoder_G.add(Conv2D(128,(2,2),kernel_initializer = 'he_normal',padding = \"same\",use_bias = True))\nEncoder_G.add(BatchNormalization())\nEncoder_G.add(ReLU())\n\nEncoder_G.add(Conv2D(256,(2,2),kernel_initializer = 'he_normal',padding = \"same\",use_bias = True))\nEncoder_G.add(BatchNormalization())\nEncoder_G.add(ReLU())\n\n\nDecoder_G = Sequential()\nDecoder_G.add(Conv2DTranspose(128,(2,2),padding = \"same\",use_bias = True))\nDecoder_G.add(ReLU())\n#\nDecoder_G.add(Conv2DTranspose(64,(2,2),padding = \"same\",use_bias = True))\nDecoder_G.add(ReLU())\n#\nDecoder_G.add(Conv2DTranspose(32,(2,2),padding = \"same\",use_bias = True))\nDecoder_G.add(ReLU())\n#\nDecoder_G.add(Conv2DTranspose(3,(2,2),padding = \"same\",use_bias = True))\nDecoder_G.add(ReLU())","caa996ce":"Auto_Encoder = Sequential([Encoder_G,Decoder_G])","75428549":"Auto_Encoder.compile(loss=compile_loss,optimizer=compile_optimizer,metrics=compile_metrics)","022ee688":"Auto_Encoder_Model = Auto_Encoder.fit(Input_Images,Output_Images,epochs=25,callbacks=[Early_Stopper,Checkpoint_Model],batch_size=16)","df464af7":"Prediction_IMG = Auto_Encoder.predict(Input_Images[:30])","b23ab574":"Prediction_Non_Seen = Auto_Encoder.predict(Non_Seen_Input[:30])","5f2e3016":"print(Prediction_IMG[20].shape)\nprint(Prediction_Non_Seen[20].shape)","d3cebd85":"figure,axis = plt.subplots(1,2,figsize=(10,10))\nprediction_img_number = 1\n\nOriginal_Img = Input_Images[prediction_img_number]\nPredict_Image_AE = Prediction_IMG[prediction_img_number]\n\naxis[0].imshow(Original_Img)\naxis[0].set_xlabel(Original_Img.shape)\naxis[0].set_ylabel(Original_Img.size)\naxis[0].set_title(\"INPUT\")\naxis[1].imshow(Predict_Image_AE)\naxis[1].set_xlabel(Predict_Image_AE.shape)\naxis[1].set_ylabel(Predict_Image_AE.size)\naxis[1].set_title(\"AUTO ENCODER OUTPUT\")","58670e6f":"figure,axis = plt.subplots(1,2,figsize=(10,10))\nprediction_img_number = 10\n\nOriginal_Img = Input_Images[prediction_img_number]\nPredict_Image_AE = Prediction_IMG[prediction_img_number]\n\naxis[0].imshow(Original_Img)\naxis[0].set_xlabel(Original_Img.shape)\naxis[0].set_ylabel(Original_Img.size)\naxis[0].set_title(\"INPUT\")\naxis[1].imshow(Predict_Image_AE)\naxis[1].set_xlabel(Predict_Image_AE.shape)\naxis[1].set_ylabel(Predict_Image_AE.size)\naxis[1].set_title(\"AUTO ENCODER OUTPUT\")","c1be6bb1":"figure,axis = plt.subplots(1,2,figsize=(10,10))\nprediction_img_number = 22\n\nOriginal_Img = Input_Images[prediction_img_number]\nPredict_Image_AE = Prediction_IMG[prediction_img_number]\n\naxis[0].imshow(Original_Img)\naxis[0].set_xlabel(Original_Img.shape)\naxis[0].set_ylabel(Original_Img.size)\naxis[0].set_title(\"INPUT\")\naxis[1].imshow(Predict_Image_AE)\naxis[1].set_xlabel(Predict_Image_AE.shape)\naxis[1].set_ylabel(Predict_Image_AE.size)\naxis[1].set_title(\"AUTO ENCODER OUTPUT\")","3c133990":"figure,axis = plt.subplots(1,2,figsize=(10,10))\nprediction_img_number = 2\n\nOriginal_Img = Non_Seen_Input[prediction_img_number]\nPredict_Image_AE = Prediction_Non_Seen[prediction_img_number]\n\naxis[0].imshow(Original_Img)\naxis[0].set_xlabel(Original_Img.shape)\naxis[0].set_ylabel(Original_Img.size)\naxis[0].set_title(\"NON SEEN INPUT\")\naxis[1].imshow(Predict_Image_AE)\naxis[1].set_xlabel(Predict_Image_AE.shape)\naxis[1].set_ylabel(Predict_Image_AE.size)\naxis[1].set_title(\"AUTO ENCODER OUTPUT\")","faffaf3f":"figure,axis = plt.subplots(1,2,figsize=(10,10))\nprediction_img_number = 29\n\nOriginal_Img = Non_Seen_Input[prediction_img_number]\nPredict_Image_AE = Prediction_Non_Seen[prediction_img_number]\n\naxis[0].imshow(Original_Img)\naxis[0].set_xlabel(Original_Img.shape)\naxis[0].set_ylabel(Original_Img.size)\naxis[0].set_title(\"NON SEEN INPUT\")\naxis[1].imshow(Predict_Image_AE)\naxis[1].set_xlabel(Predict_Image_AE.shape)\naxis[1].set_ylabel(Predict_Image_AE.size)\naxis[1].set_title(\"AUTO ENCODER OUTPUT\")","b8d5dd4e":"figure,axis = plt.subplots(1,2,figsize=(10,10))\nprediction_img_number = 15\n\nOriginal_Img = Non_Seen_Input[prediction_img_number]\nPredict_Image_AE = Prediction_Non_Seen[prediction_img_number]\n\naxis[0].imshow(Original_Img)\naxis[0].set_xlabel(Original_Img.shape)\naxis[0].set_ylabel(Original_Img.size)\naxis[0].set_title(\"NON SEEN INPUT\")\naxis[1].imshow(Predict_Image_AE)\naxis[1].set_xlabel(Predict_Image_AE.shape)\naxis[1].set_ylabel(Predict_Image_AE.size)\naxis[1].set_title(\"AUTO ENCODER OUTPUT\")","0ec7dc2e":"print(Output_Images.shape)\nprint(X_Train.shape)","75e85069":"compile_loss_B = \"mse\"\ncompile_optimizer_B = \"adam\"\ncompile_metrics_B = [\"accuracy\"]\ninput_dim = (Input_Images.shape[1],Input_Images.shape[2],Input_Images.shape[3])\noutput_class_B = 1","51f2b0ce":"Encoder_B = Sequential()\nEncoder_B.add(Conv2D(32,(2,2),kernel_initializer = 'he_normal',padding = \"same\",use_bias = True))\nEncoder_B.add(BatchNormalization())\nEncoder_B.add(ReLU())\n#\nEncoder_B.add(Conv2D(64,(2,2),kernel_initializer = 'he_normal',padding = \"same\",use_bias = True))\nEncoder_B.add(BatchNormalization())\nEncoder_B.add(ReLU())\n#\nEncoder_B.add(Conv2D(128,(2,2),kernel_initializer = 'he_normal',padding = \"same\",use_bias = True))\nEncoder_B.add(BatchNormalization())\nEncoder_B.add(ReLU())\n\n\nDecoder_B = Sequential()\n#\nDecoder_B.add(Conv2DTranspose(64,(2,2),padding = \"same\",use_bias = True))\nEncoder_B.add(BatchNormalization())\nDecoder_B.add(ReLU())\n#\nDecoder_B.add(Conv2DTranspose(32,(2,2),padding = \"same\",use_bias = True))\nEncoder_B.add(BatchNormalization())\nDecoder_B.add(ReLU())\n#\nDecoder_B.add(Conv2DTranspose(output_class_B,(2,2),padding = \"same\",use_bias = True))\nEncoder_B.add(BatchNormalization())\nDecoder_B.add(ReLU())","9ecd7ba4":"Auto_Encoder_B = Sequential([Encoder_B,Decoder_B])","d0d7ba19":"Auto_Encoder_B.compile(loss=compile_loss_B,optimizer=compile_optimizer_B,metrics=compile_metrics_B)","c10764cb":"Auto_Encoder_Model_B = Auto_Encoder_B.fit(Output_Images,X_Train,epochs=25,callbacks=Early_Stopper)","b69b8d65":"Prediction_IMG_B = Auto_Encoder_B.predict(Output_Images[:30])","68916000":"figure,axis = plt.subplots(1,2,figsize=(10,10))\nprediction_img_number = 1\n\nOriginal_Img_B = Output_Images[prediction_img_number]\nPredict_Image_AE_B = Prediction_IMG_B[prediction_img_number]\n\naxis[0].imshow(Original_Img_B)\naxis[0].set_xlabel(Original_Img_B.shape)\naxis[0].set_ylabel(Original_Img_B.size)\naxis[0].set_title(\"INPUT\")\naxis[1].imshow(Predict_Image_AE_B,cmap=\"gray\")\naxis[1].set_xlabel(Predict_Image_AE_B.shape)\naxis[1].set_ylabel(Predict_Image_AE_B.size)\naxis[1].set_title(\"AUTO ENCODER OUTPUT\")","af969f68":"figure,axis = plt.subplots(1,2,figsize=(10,10))\nprediction_img_number = 19\n\nOriginal_Img_B = Output_Images[prediction_img_number]\nPredict_Image_AE_B = Prediction_IMG_B[prediction_img_number]\n\naxis[0].imshow(Original_Img_B)\naxis[0].set_xlabel(Original_Img_B.shape)\naxis[0].set_ylabel(Original_Img_B.size)\naxis[0].set_title(\"INPUT\")\naxis[1].imshow(Predict_Image_AE_B,cmap=\"gray\")\naxis[1].set_xlabel(Predict_Image_AE_B.shape)\naxis[1].set_ylabel(Predict_Image_AE_B.size)\naxis[1].set_title(\"AUTO ENCODER OUTPUT\")","c1ddef95":"Non_Seen_Path = \"..\/input\/satellite-images-of-water-bodies\/Water Bodies Dataset\/Images\/water_body_1004.jpg\"\n\nreading_Img = cv2.cvtColor(cv2.imread(Non_Seen_Path),cv2.COLOR_BGR2RGB)\n\nplt.imshow(reading_Img)\nplt.xlabel(reading_Img.shape)","0c4e2968":"reading_Img_For_Prediction = reading_Img.reshape(-1,reading_Img.shape[0],reading_Img.shape[1],reading_Img.shape[2])","504ae98d":"Prediction_IMG_Non_Seen = Auto_Encoder_B.predict(reading_Img_For_Prediction)","1c60de03":"print(Prediction_IMG_Non_Seen.shape)","3efa5ec2":"Prediction_IMG_Non_Seen = Prediction_IMG_Non_Seen.reshape(Prediction_IMG_Non_Seen.shape[1],Prediction_IMG_Non_Seen.shape[2],Prediction_IMG_Non_Seen.shape[3])","ec98499a":"figure,axis = plt.subplots(1,2,figsize=(10,10))\n\nOriginal_Img_B = reading_Img\nPredict_Image_AE_B = Prediction_IMG_Non_Seen\n\naxis[0].imshow(Original_Img_B)\naxis[0].set_xlabel(Original_Img_B.shape)\naxis[0].set_ylabel(Original_Img_B.size)\naxis[0].set_title(\"INPUT\")\naxis[1].imshow(Predict_Image_AE_B,cmap=\"gray\")\naxis[1].set_xlabel(Predict_Image_AE_B.shape)\naxis[1].set_ylabel(Predict_Image_AE_B.size)\naxis[1].set_title(\"AUTO ENCODER OUTPUT\")","025df527":"#### PREDICTION","cf0f8499":"# MODEL FOR BLACK AND WHITE","0afc68a9":"#### CREATING LAB TO RGB AND TESTING SET","8ed97061":"#### ANOTHER METHOD FOR TRANSFORMATION TO 3D FROM 2D","75a5f292":"# HISTORY\n\n#### Content\nThe dataset consists of two compressed zip files:\n* ab.zip : This contains 25 .npy files consisting of a and b dimensions of LAB color space images, of the MIRFLICKR25k randomly sized colored image dataset. The LAB color space generally takes up large disk spaces, hence is a lot slower to load.\n\n* l.zip : This consists of a gray_scale.npy file which is the grayscale version of the MIRFLICKR25k dataset.","2c662311":"#### SPLITTING TRAIN AND TEST","c3e76fb3":"#### PREDICTION","257457c4":"# PACKAGES AND LIBRARIES","95425ab8":"#### MAIN NPY","2bdba0f8":"# PATH, LABEL, TRANSFORMATION","ad9caf09":"#### NON SEEN PREDICTION","01d2f401":"# MODEL FOR COLORIZATION","3d84c2d2":"# VISION"}}