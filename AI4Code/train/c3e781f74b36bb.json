{"cell_type":{"563ed918":"code","05470bb9":"code","885f26bd":"code","c9f781f4":"code","d60462d6":"code","16914c77":"code","f16bf951":"code","3e16a5d2":"code","5d46cd0e":"code","febe9783":"code","b7a96955":"code","78e5cbcc":"code","e303ffc2":"code","9fde1855":"code","48d2c3eb":"code","7a4d4e99":"code","d9a582d6":"code","8184b7b6":"markdown","1dbbb042":"markdown","8a8dbbd0":"markdown","2653238f":"markdown","b087c27c":"markdown","6e939528":"markdown","c5ea22a0":"markdown","6c759f36":"markdown","5893ae0f":"markdown","d6019841":"markdown","ebed3a54":"markdown","74488024":"markdown","a30571d9":"markdown","9471b230":"markdown","c94ef26f":"markdown","50008746":"markdown","dab3a239":"markdown","b5956f5a":"markdown","be590c27":"markdown","b2ac5c5c":"markdown","752859f6":"markdown","ec80934f":"markdown"},"source":{"563ed918":"import sys\nsys.path.append('..\/input\/timm-pytorch-image-models\/pytorch-image-models-master')","05470bb9":"!pip install --upgrade -q wandb\n!pip install -q pytorch-lightning\n!pip install -q nnAudio\n!pip install -q deepspeed\n!pip install -q timm","885f26bd":"# Hide Warning\nimport warnings\nwarnings.filterwarnings('ignore', category=DeprecationWarning)\nwarnings.filterwarnings('ignore', category=FutureWarning)\nwarnings.filterwarnings('ignore', category=UserWarning)\n\n# Python Libraries\nfrom tqdm import tqdm\nfrom collections import defaultdict\nimport pandas as pd\nimport numpy as np\nimport os\nimport random\nimport glob\nimport math\n\n\n# Visualizations\nfrom PIL import Image\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport cv2\nimport plotly.express as px\n%matplotlib inline\nsns.set(style=\"whitegrid\")\n\n# Utilities and Metrics\nfrom sklearn.model_selection import StratifiedKFold, train_test_split, KFold\nfrom sklearn.metrics import roc_auc_score\n\n# Pytorch \nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.optim import Adam, SGD\nfrom torch.utils.data import Dataset, DataLoader, WeightedRandomSampler\nfrom torch.optim.lr_scheduler import CosineAnnealingLR\nfrom torch.optim.optimizer import Optimizer, required \nfrom pytorch_lightning.plugins import DeepSpeedPlugin\nfrom deepspeed.ops.adam import FusedAdam\n\n\n# Pytorch Lightning\nimport pytorch_lightning as pl\nfrom pytorch_lightning import seed_everything\nfrom pytorch_lightning import Callback\nfrom pytorch_lightning.callbacks import ModelCheckpoint, EarlyStopping\nfrom pytorch_lightning.loggers import WandbLogger\n\n# Pytorch Image Models\nimport timm\n\n# Image Augmentation Library\nimport albumentations\nfrom albumentations.pytorch import ToTensorV2\nfrom albumentations.core.transforms_interface import DualTransform\nfrom albumentations.augmentations import functional as AF\n\nfrom nnAudio.Spectrogram import CQT2010v2, MFCC, CQT1992v2, MelSpectrogram\n\n\n# Weights and Biases Tool\nimport wandb\nwandb.login()","c9f781f4":"params = {\n    'seed': 42,\n    'model': 'efficientnet_b1',\n    'size' : 224,\n    'inp_channels': 1,\n    'lr': 1e-3,\n    'weight_decay': 1e-6,\n    'batch_size': 64,\n    'num_workers' : 8,\n    'epochs': 5,\n    'out_features': 1,\n    'name': 'CosineAnnealingLR',\n    'T_max': 10,\n    'min_lr': 1e-6,\n    'nfolds': 5,\n    'precision': 16,\n    'competetion': 'G2Net',\n    '_wandb_kernel':'tang'\n}","d60462d6":"def seed_everything(seed=42):\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = True\n    \nseed_everything(params['seed'])","16914c77":"train_dir = ('..\/input\/g2net-gravitational-wave-detection\/train')\ntest_dir = ('..\/input\/g2net-gravitational-wave-detection\/test')\ntrain_df = pd.read_csv('..\/input\/g2net-gravitational-wave-detection\/training_labels.csv')\ntest_df = pd.read_csv('..\/input\/g2net-gravitational-wave-detection\/sample_submission.csv')","f16bf951":"def return_filpath(name, folder=train_dir):\n    path = os.path.join(folder, f'{name[0]}\/{name[1]}\/{name[2]}\/{name}.npy')\n    return path","3e16a5d2":"train_df['image_path'] = train_df['id'].apply(lambda x: return_filpath(x))\ntest_df['image_path'] = test_df['id'].apply(lambda x: return_filpath(x, folder=test_dir))\ntrain_df.head()","5d46cd0e":"train_df = train_df.sample(n=50000, random_state=params['seed']).reset_index(drop=True)","febe9783":"run = wandb.init(project='G2Net-PL-Baseline', \n                           config=params,\n                           group='Effnet-CQT', \n                           job_type='Visualize')\n\nsample_data_tables = wandb.Table(columns=[\"ID\",\"CQT\", \"target\"])\n\n\ndef apply_qtransform(waves, transform=CQT1992v2(sr=2048, fmin=20, fmax=1024, hop_length=64)):\n    waves = np.hstack(waves)\n    waves = waves \/ np.max(waves)\n    waves = torch.from_numpy(waves).float()\n    image = transform(waves)\n    return image\n\n\nfor i in range(10):\n    waves = np.load(train_df.loc[i, 'image_path'])\n    image = apply_qtransform(waves)\n    target = train_df.loc[i, 'target']\n    sample_data_tables.add_data(train_df.loc[i, 'id'],\n                                wandb.Image(image),\n                                target)\n    \n    \n\nwandb.log({'wandb_viz': sample_data_tables})    \nwandb.finish()","b7a96955":"dist = train_df.target.map({0:'Target 0', 1:'Target 1'})\ndist = dist.value_counts()\nfig = px.pie(dist,\n             values='target',\n             names=dist.index,\n             hole=.4,title=\"Target Distribution\")\nfig.update_traces(textinfo='percent+label', pull=0.05)\nfig.show()","78e5cbcc":"def get_train_transforms():\n    return albumentations.Compose(\n        [\n\n            ToTensorV2(p=1.0)\n        ]\n    )\n\ndef get_valid_transforms():\n    return albumentations.Compose(\n        [\n            ToTensorV2(p=1.0)\n        ]\n    )\n\ndef get_test_transforms():\n        return albumentations.Compose(\n            [\n                ToTensorV2(p=1.0)\n            ]\n        )","e303ffc2":"class SETIDataset(Dataset):\n    def __init__(self, images_filepaths, targets, transform=None):\n        self.images_filepaths = images_filepaths\n        self.targets = targets\n        self.transform = transform\n        self.wave_transform = CQT1992v2(sr=2048, fmin=20, fmax=1024, hop_length=64, verbose=True)\n\n    def __len__(self):\n        return len(self.images_filepaths)\n    \n    def apply_cqt(self, image, transform):\n        image = np.hstack(image)\n        image = image \/ np.max(image)\n        image = torch.from_numpy(image).float()\n        image = transform(image)\n        return image\n\n    def __getitem__(self, idx):\n        image_filepath = self.images_filepaths[idx]\n        image = np.load(image_filepath)\n        image = self.apply_cqt(image, self.wave_transform)\n\n        if self.transform is not None:\n            image = image.squeeze().numpy()\n            image = self.transform(image=image)[\"image\"]\n        else:\n            image = image[np.newaxis,:,:]\n            image = torch.from_numpy(image).float()\n        \n        label = torch.tensor(self.targets[idx]).float()\n        return image, label","9fde1855":"class ImagePredictionLogger(Callback):\n    def __init__(self, val_samples = None, num_samples=32):\n        super().__init__()\n        self.num_samples = num_samples\n        self.val_imgs, self.val_labels = val_samples\n          \n        \n    def on_validation_epoch_end(self, trainer, pl_module):\n        val_imgs = self.val_imgs.to(device=pl_module.device)\n        val_labels = self.val_labels.to(device=pl_module.device)\n        logits = pl_module(val_imgs)\n        preds = torch.argmax(logits, -1)\n        trainer.logger.experiment.log({\n            \"Predictions\":[wandb.Image(x, caption=f\"Pred:{pred}, Target:{y}\") \n                           for x, pred, y in zip(val_imgs[:self.num_samples], \n                                                 preds[:self.num_samples], \n                                                 val_labels[:self.num_samples])]\n            }, commit=False)","48d2c3eb":"class DataModule(pl.LightningDataModule):\n\n    def __init__(self, train_data, valid_data, test_data):\n        super().__init__()\n        self.train_data = train_data\n        self.valid_data = valid_data\n        self.test_data = test_data\n        \n    def setup(self, stage=None):\n        self.train_dataset = SETIDataset(\n        images_filepaths=self.train_data['image_path'].values,\n        targets=self.train_data['target'].values,\n        transform = get_train_transforms()    \n            )\n        \n        self.val_dataset = SETIDataset(\n        images_filepaths=self.valid_data['image_path'].values,\n        targets=self.valid_data['target'].values,\n        transform = get_valid_transforms()\n        )\n\n        self.test_dataset = SETIDataset(\n        images_filepaths = self.test_data['image_path'].values,\n        targets = self.test_data['target'].values,\n        transform = get_test_transforms()\n        )\n\n    def train_dataloader(self):\n        return DataLoader(\n        self.train_dataset,\n        batch_size=params['batch_size'],\n        shuffle=True,\n        num_workers=params['num_workers'],\n        pin_memory=True\n            )\n\n    def val_dataloader(self):\n        return DataLoader(\n        self.val_dataset,\n        batch_size=params['batch_size'],\n        shuffle=False,\n        num_workers=params['num_workers'],\n        pin_memory=True\n            )\n\n    def test_dataloader(self):\n        return DataLoader(\n        self.test_dataset, batch_size=params['batch_size'],\n        shuffle=False, num_workers=params['num_workers'],\n        pin_memory=True\n            )","7a4d4e99":"class Trainer(pl.LightningModule):\n\n    def __init__(self, model_name=params['model'],out_features=params['out_features'],\n                 inp_channels=params['inp_channels'],pretrained=True):\n        super().__init__()\n        \n        self.model = timm.create_model(model_name, pretrained=pretrained,\n                                       in_chans=inp_channels)\n        \n        if model_name == 'resnet18d':\n            n_features = self.model.fc.in_features\n            self.model.fc = nn.Linear(n_features, out_features, bias=True)\n            \n        if model_name == 'nfnet_f1':\n            n_features = self.model.head.fc.in_features\n            self.model.head.fc = nn.Linear(n_features, out_features, bias=True)\n            \n        elif model_name == 'efficientnet_b1':\n            n_features = self.model.classifier.in_features\n            self.model.classifier = nn.Linear(n_features, out_features, bias=True) \n    \n        self.criterion = nn.BCEWithLogitsLoss()\n                \n    def forward(self, x):\n        output = self.model(x)\n        return output\n\n\n    def training_step(self, batch, batch_idx):\n\n        x, y = batch\n        output = self.model(x)\n        labels = y.unsqueeze(1)\n        loss = self.criterion(output, labels)\n        \n        try:\n            auc=roc_auc_score(labels.detach().cpu(), output.sigmoid().detach().cpu()) \n\n            self.log(\"auc\", auc, on_step= True, prog_bar=True, logger=True)\n            self.log(\"Train Loss\", loss, on_step= True,prog_bar=True, logger=True)\n        \n        except:\n            pass\n\n        return {\"loss\": loss, \"predictions\": output, \"labels\": labels}\n\n    def training_epoch_end(self, outputs):\n\n        preds = []\n        labels = []\n        \n        for output in outputs:\n            \n            preds += output['predictions']\n            labels += output['labels']\n\n        labels = torch.stack(labels)\n        preds = torch.stack(preds)\n\n        train_auc=roc_auc_score(labels.detach().cpu(), preds.sigmoid().detach().cpu())\n        self.log(\"mean_train_auc\", train_auc, prog_bar=True, logger=True)\n\n    def validation_step(self, batch, batch_idx):\n        x, y = batch\n        output = self.model(x)\n        labels = y.unsqueeze(1)\n        loss = self.criterion(output, labels)\n        \n        self.log('val_loss', loss, on_step= True, prog_bar=True, logger=True)\n        return {\"predictions\": output, \"labels\": labels}\n      \n\n    def validation_epoch_end(self, outputs):\n\n        preds = []\n        labels = []\n        \n        for output in outputs:\n            preds += output['predictions']\n            labels += output['labels']\n\n        labels = torch.stack(labels)\n        preds = torch.stack(preds)\n\n        val_auc=roc_auc_score(labels.detach().cpu(), preds.sigmoid().detach().cpu())\n        self.log(\"val_auc\", val_auc, prog_bar=True, logger=True)\n        \n\n    def test_step(self, batch, batch_idx):\n        x = batch        \n        output = self(x).sigmoid()\n        return output   \n\n    def configure_optimizers(self):\n\n        param_optimizer = list(self.model.named_parameters())\n        no_decay = [\"bias\", \"LayerNorm.bias\", \"LayerNorm.weight\"]\n        optimizer_parameters = [\n            {\n                \"params\": [\n                    p for n, p in param_optimizer if not any(nd in n for nd in no_decay)\n                ],\n                \"weight_decay\": params['weight_decay'],\n            },\n            {\n                \"params\": [\n                    p for n, p in param_optimizer if any(nd in n for nd in no_decay)\n                ],\n                \"weight_decay\": 0.0,\n            },\n        ]\n    \n        optimizer = FusedAdam(optimizer_parameters, lr=params['lr'])\n        \n        \n        scheduler = CosineAnnealingLR(optimizer,\n                              T_max=params['T_max'],\n                              eta_min=params['min_lr'],\n                              last_epoch=-1)\n\n        return dict(\n          optimizer=optimizer,\n          lr_scheduler=scheduler\n        )\n","d9a582d6":"kfolds = StratifiedKFold(n_splits=params['nfolds'], shuffle=True, random_state=params['seed'])\n\nmodel = Trainer()\n\nfor fold, (trn_idx, val_idx) in enumerate(kfolds.split(train_df[\"id\"], train_df[\"target\"])):\n    \n    if fold != 0:\n        continue\n        \n    wandb_logger = WandbLogger(project='G2Net-PL-Baseline', \n                           config=params,\n                           group='Effnet-CQT', \n                           job_type='train',\n                           name = f'Fold{fold}')\n    \n    \n    print(f\"{'='*38} Fold: {fold} {'='*38}\")\n    \n    train_data = train_df.loc[trn_idx]\n    valid_data = train_df.loc[val_idx]\n    \n    data_module = DataModule(\n      train_data,\n      valid_data, \n      valid_data, \n    )\n    \n    data_module.setup()\n    val_samples = next(iter(data_module.val_dataloader()))\n    \n    \n    early_stopping_callback = EarlyStopping(monitor='val_auc',mode=\"max\", patience=1)\n    \n    checkpoint_callback = ModelCheckpoint(\n      dirpath=\"checkpoints\",\n      filename=\"best-checkpoint-fold{fold}-val_auc{val_auc:.3f}\",\n      save_top_k=params['epochs'],\n      verbose=True,\n      monitor=\"val_auc\",\n      mode=\"max\"\n    )\n    \n    trainer = pl.Trainer(\n      gpus = 1,\n      checkpoint_callback=checkpoint_callback,\n      callbacks=[early_stopping_callback,\n                       ImagePredictionLogger(val_samples)],\n      max_epochs=params['epochs'],\n      precision = params['precision'],\n      progress_bar_refresh_rate=1, \n      stochastic_weight_avg = True,  \n      logger=wandb_logger\n    )\n    \n    trainer.fit(model, data_module)","8184b7b6":"<p p style = \"font-family: garamond; font-size:30px; font-style: normal;background-color: #f6f5f5; color :#6666ff; border-radius: 10px 10px; text-align:center\">Pytorch Lightning Module<\/p>\n<center><img src=\"data:image\/png;base64,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\" width=\"400\" alt=\"Weights & Biases\" \/><\/center><br>\n\n<p style = \"font-family: garamond; font-size: 20px; font-style: normal; border-radius: 10px 10px; text-align:center\"> A LightningModule organizes your PyTorch code into 5 sections<br>1. Computations (init).<br>2. Train loop (training_step)<br>3. Validation loop (validation_step)<br>4. Test loop (test_step)<br>5. Optimizers (configure_optimizers)<br><br>\nA LightningModule is a torch.nn.Module but with added functionality. ","1dbbb042":"<center><img src=\"https:\/\/i.imgur.com\/gb6B4ig.png\" width=\"400\" alt=\"Weights & Biases\" \/><\/center><br>\n<p style = \"font-family: garamond; font-size: 20px; font-style: normal; border-radius: 10px 10px; text-align:center\">W&B Tables accelerate the ML development lifecycle by giving users the ability to rapidly extract meaningful insights from data. The W&B Table Visualizer provides an interactive interface to perform powerful analytics functions like grouping, joining, and creating custom fields while simultaneously supporting rich media annotations such as bounding boxes and segmentation masks.<br><br> W&B Tables is designed generically to work well for a wide range of use cases - from analyzing intermediate data transformations to reviewing model predictions - while being directly integrated directly into the WB UI dashboard, allowing users to learn, adapt, and improve their models effectively and efficiently.<\/p>","8a8dbbd0":"<p p style = \"font-family: garamond; font-size:30px; font-style: normal;background-color: #f6f5f5; color :#6666ff; border-radius: 10px 10px; text-align:center\">Import Libraries<\/p>","2653238f":"<p p style = \"font-family: garamond; font-size:30px; font-style: normal;background-color: #f6f5f5; color :#6666ff; border-radius: 10px 10px; text-align:center\">Single Fold Metrics<\/p><br>\n\n<center><img src=\"https:\/\/i.imgur.com\/bMEoitM.png\" width=\"1500\" alt=\"metrics\" \/><\/center>","b087c27c":"<p p style = \"font-family: garamond; font-size:30px; font-style: normal;background-color: #f6f5f5; color :#6666ff; border-radius: 10px 10px; text-align:center\">Define Seed for Reproducibility<\/p>","6e939528":"<p p style = \"font-family: garamond; font-size:30px; font-style: normal;background-color: #f6f5f5; color :#6666ff; border-radius: 10px 10px; text-align:center\">GPU Utilization<\/p><br>\n\n<center><img src=\"https:\/\/i.imgur.com\/KEV03yD.png\" width=\"1500\" alt=\"metrics\" \/><\/center>","c5ea22a0":"<p p style = \"font-family: garamond; font-size:30px; font-style: normal;background-color: #f6f5f5; color :#6666ff; border-radius: 10px 10px; text-align:center\">Load Train and Test<\/p>","6c759f36":"<p p style = \"font-family: garamond; font-size:30px; font-style: normal;background-color: #f6f5f5; color :#6666ff; border-radius: 10px 10px; text-align:center\">Validation Loss per Epoch<\/p><br>\n\n<center><img src=\"https:\/\/i.imgur.com\/zDkwKbv.png\" width=\"1500\" alt=\"metrics\" \/><\/center>","5893ae0f":"<p p style = \"font-family: garamond; font-size:35px; font-style: normal;background-color: #f6f5f5; color :#ff0066; border-radius: 10px 10px; text-align:center\">Upvote the kernel if you find it insightful!<\/p>","d6019841":"<p p style = \"font-family: garamond; font-size:30px; font-style: normal;background-color: #f6f5f5; color :#6666ff; border-radius: 10px 10px; text-align:center\">Final Metrics<\/p><br>\n\n<center><img src=\"https:\/\/i.imgur.com\/lheGhFa.png\" width=\"1500\" alt=\"metrics\" \/><\/center>","ebed3a54":"<p p style = \"font-family: garamond; font-size:40px; font-style: normal;background-color: #f6f5f5; color :#6666ff; border-radius: 10px 10px; text-align:center\">What are we discussing today? <\/p>\n <p p style = \"font-family: garamond; font-size:30px; font-style: normal;background-color: #f6f5f5; color :#006699; border-radius: 10px 10px; text-align:center\"> Constant Q Transform<br>\n DeepSpeed Fused Adam optimizer <br>\n Pytorch Lightning <br>\n Stratified K-Fold with PL \u26a1\ufe0f <br>\n Weights and Biases for Experiment Tracking including W&B Tables","74488024":"<p p style = \"font-family: garamond; font-size:30px; font-style: normal;background-color: #f6f5f5; color :#6666ff; border-radius: 10px 10px; text-align:center\">Custom Dataset<\/p>","a30571d9":"<p p style = \"font-family: garamond; font-size:40px; font-style: normal;background-color: #f6f5f5; color :#6666ff; border-radius: 10px 10px; text-align:center\">G2Net Gravitational Waves<\/p>\n\n![](https:\/\/www.g2net.eu\/wp-content\/uploads\/2019\/07\/2ndconference_V2-1170x600.jpg)\n\n\n\n<p p style = \"font-family: garamond; font-size:30px; font-style: normal;background-color: #f6f5f5; color :#006699; border-radius: 10px 10px; text-align:center\"> \nThe breakthrough discovery of gravitational waves on September 14, 2015 was made possible through synergy of techniques drawing from expertise in physics, mathematics, information science and computing. In this competition you are provided with a training set of time series data containing simulated gravitational wave measurements from a network of 3 gravitational wave interferometers (LIGO Hanford, LIGO Livingston, and Virgo). Each time series contains either detector noise or detector noise plus a simulated gravitational wave signal.<\/p>\n\n<center><img src=\"https:\/\/www.phys.ufl.edu\/wp\/wp-content\/uploads\/2020\/09\/image.png\" width=\"1000\" alt=\"Weights & Biases\" \/><\/center><br>","9471b230":"<p p style = \"font-family: garamond; font-size:30px; font-style: normal;background-color: #f6f5f5; color :#6666ff; border-radius: 10px 10px; text-align:center\">Custom Callback for Viewing Predictions in W&B<\/p>\n","c94ef26f":"<center><img src=\"https:\/\/i.imgur.com\/gb6B4ig.png\" width=\"400\" alt=\"Weights & Biases\" \/><\/center><br>\n<p style = \"font-family: garamond; font-size: 20px; font-style: normal; border-radius: 10px 10px; text-align:center\">Wandb is a developer tool for companies turn deep learning research projects into deployed software by helping teams track their models, visualize model performance and easily automate training and improving models.\nWe will use their tools to log hyperparameters and output metrics from your runs, then visualize and compare results and quickly share findings with your colleagues.<br><br>We'll be using this to train our K Fold Cross Validation and gain better insights about our training. <br><br><\/p>\n\n![img](https:\/\/i.imgur.com\/BGgfZj3.png)","50008746":"<p p style = \"font-family: garamond; font-size:30px; font-style: normal;background-color: #f6f5f5; color :#6666ff; border-radius: 10px 10px; text-align:center\">Pytorch Lightning Trainer<\/p>\n\n# <center><img src=\"data:image\/png;base64,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\" width=\"400\" alt=\"Weights & Biases\" \/><\/center>\n\n<p style = \"font-family: garamond; font-size: 20px; font-style: normal; border-radius: 10px 10px; text-align:center\"> Once you\u2019ve organized your PyTorch code into a LightningModule, the Trainer automates everything else.<br><br>\ud83d\udccd This abstraction achieves the following:<br><br>1. You maintain control over all aspects via PyTorch code without an added abstraction.<br>2. The trainer uses best practices embedded by contributors and users from top AI labs such as Facebook AI Research, NYU, MIT, Stanford, etc\u2026<br>3. The trainer allows overriding any key part that you don\u2019t want automated.<br><br>\n\ud83d\udccd Under the hood<br>\nUnder the hood, the Lightning Trainer handles the training loop details for you, some examples include:<br>1. Automatically enabling\/disabling grads<br>2. Running the training, validation and test dataloaders<br>3. Calling the Callbacks at the appropriate times<br>4. Putting batches and computations on the correct devices<br><br>\n\ud83d\udccd The code below applies Stratified K-Fold with PL \u26a1\ufe0f\n\n","dab3a239":"<p p style = \"font-family: garamond; font-size:30px; font-style: normal;background-color: #f6f5f5; color :#6666ff; border-radius: 10px 10px; text-align:center\">DeepSpeed Fused Adam Optimizer<\/p>\n\n<p style = \"font-family: garamond; font-size: 20px; font-style: normal; border-radius: 10px 10px; text-align:center\"> DeepSpeed offers high-performance implementations of Adam optimizer on CPU; FusedAdam, FusedLamb, OnebitAdam, OnebitLamb optimizers on GPU. <br><br> Here we implement Fused Adam which implements Adam algorithm but this version implements 2 fusions.<br><br> 1. Fusion of the Adam update\u2019s elementwise operations. <br>2.A multi-tensor apply launch that batches the elementwise updates applied to all the model\u2019s parameters into one or a few kernel launches.<br><br> The Adam optimizer in Pytorch (like all Pytorch optimizers) carries out optimizer.step() by looping over parameters, and launching a series of kernels for each parameter. This can require hundreds of small launches that are mostly bound by CPU-side Python looping and kernel launch overhead, resulting in poor device utilization. Currently, the FusedAdam implementation in Apex flattens the parameters for the optimization step, then carries out the optimization step itself via a fused kernel that combines all the Adam operations. In this way, the loop over parameters as well as the internal series of Adam operations for each parameter are fused such that optimizer.step() requires only a few kernel launches.","b5956f5a":"<p p style = \"font-family: garamond; font-size:30px; font-style: normal;background-color: #f6f5f5; color :#6666ff; border-radius: 10px 10px; text-align:center\"><a href = 'https:\/\/wandb.ai\/tanishqgautam\/G2Net-PL-Baseline'>Check out the Weights and Biases Dashboard here $\\rightarrow$ <\/a><\/p>","be590c27":"\n<p p style = \"font-family: garamond; font-size:30px; font-style: normal;background-color: #f6f5f5; color :#6666ff; border-radius: 10px 10px; text-align:center\">W&B Table for CQT Visualization<\/p><br>\n\n<center><img src=\"https:\/\/i.imgur.com\/GwOF2DS.png\" width=\"1500\" alt=\"metrics\" \/><\/center>","b2ac5c5c":"<p p style = \"font-family: garamond; font-size:30px; font-style: normal;background-color: #f6f5f5; color :#6666ff; border-radius: 10px 10px; text-align:center\">Define Configurations\/Parameters<\/p>","752859f6":"<p p style = \"font-family: garamond; font-size:30px; font-style: normal;background-color: #f6f5f5; color :#6666ff; border-radius: 10px 10px; text-align:center\">Pytorch Lightning DataModule<\/p>\n\n# <center><img src=\"data:image\/png;base64,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\" width=\"400\" alt=\"Weights & Biases\" \/><\/center>\n\n\n<p style = \"font-family: garamond; font-size: 20px; font-style: normal; border-radius: 10px 10px; text-align:center\"> A datamodule is a shareable, reusable class that encapsulates all the steps needed to process data. A datamodule encapsulates the five steps involved in data processing in PyTorch:<br>1. Download \/ tokenize \/ process.<br>2. Clean and (maybe) save to disk.<br>3. Load inside Dataset.<br>4. Apply transforms (rotate, tokenize, etc\u2026).<br>5. Wrap inside a DataLoader.<br><br>\n\ud83d\udccd Why do I need a DataModule?<br>In normal PyTorch code, the data cleaning\/preparation is usually scattered across many files. This makes sharing and reusing the exact splits and transforms across projects impossible.<br>Datamodules are for you if you ever asked the questions:<br>1. what splits did you use?<br>2. what transforms did you use?<br>3. what normalization did you use?<br>4. how did you prepare\/tokenize the data?<br><br> To summarize: A DataModule is simply a collection of a train_dataloader, val_dataloader(s), test_dataloader(s) along with the matching transforms and data processing\/downloads steps required.","ec80934f":"<p p style = \"font-family: garamond; font-size:30px; font-style: normal;background-color: #f6f5f5; color :#6666ff; border-radius: 10px 10px; text-align:center\">Constant Q Transform<\/p>\n\n<p style = \"font-family: garamond; font-size: 20px; font-style: normal; border-radius: 10px 10px; text-align:center\"> In mathematics and signal processing, the constant-Q transform, simply known as CQT transforms a data series to the frequency domain. It is related to the Fourier transform[1] and very closely related to the complex Morlet wavelet transform.<br><br> An efficient method of transforming a discrete Fourier transform (DFT) into a constant Q transform, where Q is the ratio of center frequency to bandwidth, has been devised. This method involves the calculation of kernels that are then applied to each subsequent DFT. Only a few multiples are involved in the calculation of each component of the constant Q transform, so this transformation adds a small amount to the computation. In effect, this method makes it possible to take full advantage of the computational efficiency of the fast Fourier transform (FFT)."}}