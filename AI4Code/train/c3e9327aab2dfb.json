{"cell_type":{"957e7e1d":"code","5bde2e6e":"code","863ab377":"code","3437b86d":"code","b330fbb3":"code","2c1029a8":"code","73bb1f2a":"code","2e222e8f":"code","2f9c0bde":"code","212136b1":"code","4b5e2274":"code","e147dcbe":"code","2ae11415":"code","788ec817":"code","fcac0c9b":"code","dfd80f36":"code","989b0e18":"code","645fb755":"code","b9299169":"code","7a8e790b":"code","b51ece07":"code","922d8eba":"code","167e6b51":"code","02934aa0":"code","ac428c78":"code","66588bbf":"code","52bd6b54":"code","66205d10":"code","02012f91":"code","b47b8d5f":"code","d3af1997":"code","9ea74324":"code","c1ee81a3":"code","489908d7":"code","33cc3214":"code","7a931805":"code","1a09f1a2":"code","557d2233":"code","2f2ca621":"code","b99f77e9":"code","beb06044":"code","08defb04":"code","131920bb":"code","861805f3":"code","55ad0380":"code","7fea05cc":"code","c18528d0":"code","f305bc49":"code","ccdc64af":"code","70771700":"code","2ba3a390":"code","76a634ad":"code","af3d017e":"markdown"},"source":{"957e7e1d":"#import library\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\nfrom pandas import DataFrame\nfrom sklearn.model_selection import GridSearchCV \n#modeling parametes\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier\n\nfrom sklearn.metrics import accuracy_score, roc_auc_score, roc_curve\n\nimport os\nprint(os.listdir(\"..\/input\"))","5bde2e6e":"#read data\ndataset = pd.read_csv(\"..\/input\/pima-indians-diabetes-database\/diabetes.csv\")\nX = dataset.iloc[:, :-1].values\ny = dataset.iloc[:, 8].values\nprint(dataset)\n\n\n","863ab377":"dataset.head()","3437b86d":"dataset.info()","b330fbb3":"import itertools\n\ncolumns=dataset.columns[:8]\nplt.subplots(figsize=(18,15))\nlength=len(columns)\nfor i,j in itertools.zip_longest(columns,range(length)):\n    plt.subplot((length\/2),3,j+1)\n    plt.subplots_adjust(wspace=0.2,hspace=0.5)\n    dataset[i].hist(bins=20,edgecolor='black')\n    plt.title(i)\nplt.show()","2c1029a8":"data1 = dataset[dataset[\"Outcome\"]==1]\ncolumns = dataset.columns[:8]\nplt.subplots(figsize=(18,15))\nlength =len(columns)\nfor i,j in itertools.zip_longest(columns,range(length)):\n    plt.subplot((length\/2),3,j+1)\n    plt.subplots_adjust(wspace=0.2,hspace=0.5)\n    plt.ylabel(\"Count\")\n    data1[i].hist(bins=20,edgecolor='black')\n    plt.title(i)\nplt.show()","73bb1f2a":"print(dataset.groupby('Outcome').size())","2e222e8f":"import seaborn as sns\nsns.countplot(dataset['Outcome'],label=\"Count\")","2f9c0bde":"sns.pairplot(data=dataset,hue='Outcome',diag_kind='kde')\nplt.show()","212136b1":"# Splitting the dataset into the Training set and Test set\n# Memisahkan dataset ke dalam set Pelatihan dan set Tes\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.25, \n                                                    random_state = 42)","4b5e2274":"X_train.shape","e147dcbe":"X_test.shape","2ae11415":"y_train.shape","788ec817":"y_test.shape","fcac0c9b":"#scalling feature\nimport seaborn as sns\n# fig, (ax2) = plt.subplots(ncols=1, figsize=(6, 5))\n# x = pd.DataFrame({\n#     # Distribution with lower outliers\n#     'x1': np.concatenate([np.random.normal(20, 1, 1000), np.random.normal(1, 1, 25)]),\n#     # Distribution with higher outliers\n#     'x2': np.concatenate([np.random.normal(30, 1, 1000), np.random.normal(50, 1, 25)]),\n# })\n\nfrom sklearn.preprocessing import StandardScaler\nsc = StandardScaler()\n#x\nX_train = sc.fit_transform(X_train)\nX_test = sc.transform(X_test)\n#y\n# y_train = sc.fit_transform(y_train)\n# y_test = sc.transform(y_test)\n\n\n                                            \n# scaled_df = pd.DataFrame(X_train,columns=['x1', 'x2','x3','x4','x5','x6','x7','x8'])\n# ax2.set_title('testing')                                                                                        \n# sns.kdeplot(scaled_df['x1'], ax=ax2)\n# sns.kdeplot(scaled_df['x2'], ax=ax2)\n# sns.kdeplot(scaled_df)\n# plt.show\nprint('ini data x train',X_train)\nprint('ini data x train',X_test)","dfd80f36":"#gradient boasting method \n\n# Parameter evaluation with GSC validation\ngbe = GradientBoostingClassifier(random_state=42)\nparameters={'learning_rate': [0.05, 0.1, 0.5],\n            'max_features': [0.5, 1],\n            'max_depth': [3, 4, 5]\n}\ngridsearch=GridSearchCV(gbe, parameters, cv=100, scoring='roc_auc')\ngridsearch.fit(X, y)\nprint(gridsearch.best_params_)\nprint(gridsearch.best_score_)","989b0e18":"#gradient boasting method \n\n# Adjusting development threshold\ngbi = GradientBoostingClassifier(learning_rate=0.05, max_depth=3,\n                                 max_features=0.5,\n                                 random_state=42)\nX_train,X_test,y_train, y_test = train_test_split(X, y, random_state=42)\ngbi.fit(X_train, y_train)\nprint(\"Accuracy on training set: {:.3f}\".format(gbi.score(X_train, y_train)))\nprint(\"Accuracy on test set: {:.3f}\".format(gbi.score(X_test, y_test)))","645fb755":"#gradient boasting method\n\n# Storing the prediction\ny_pred = gbi.predict_proba(X_test)[:,1]\nprint('y prediksi',y_pred)","b9299169":"#gradient boasting method\n\n# Making the Confusion Matrix\nfrom sklearn.metrics import classification_report, confusion_matrix\ncm = confusion_matrix(y_test, y_pred.round())\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nf, ax = plt.subplots(figsize=(5,5))\nsns.heatmap(cm,annot = True, linewidths=0.5,linecolor=\"red\",fmt=\".0f\",ax=ax)\nplt.xlabel(\"Prediction(Ypred)\")\nplt.ylabel(\"Ytrue\")\nplt.show()\n\nprint('TN - True Negative {}'.format(cm[0,0]))\nprint('FP - False Positive {}'.format(cm[0,1]))\nprint('FN - False Negative {}'.format(cm[1,0]))\nprint('TP - True Positive {}'.format(cm[1,1]))\nprint('Accuracy Rate: {}'.format(np.divide(np.sum([cm[0,0],cm[1,1]]),np.sum(cm))))\nprint('Misclassification Rate: {}'.format(np.divide(np.sum([cm[0,1],cm[1,0]]),np.sum(cm))))","7a8e790b":"from sklearn.metrics import f1_score\n# Plotting the predictions\n\nplt.hist(y_pred,bins=10)\nplt.xlim(0,1)\nplt.xlabel(\"Predicted Proababilities\")\nplt.ylabel(\"Frequency\")\n\nround(roc_auc_score(y_test,y_pred),5)\nprint('akurasi model gradient bosting :',round(roc_auc_score(y_test,y_pred),5))","b51ece07":"#gradient boasting\n\nfrom sklearn.metrics import classification_report, confusion_matrix, roc_curve, auc\n# accuracy_score(y_test, y_pred.round(), normalize=True)\nprint(\"Confusion Matrix:\")\nprint(confusion_matrix(y_test, y_pred.round() ))\nprint()\nprint(\"Classification Report\")\nprint(classification_report(y_test,y_pred.round()))\nprint('akurasi model gradient bosting :',round(roc_auc_score(y_test,y_pred),5))","922d8eba":"GB = GradientBoostingClassifier()\nGBscore = round(roc_auc_score(y_test,y_pred),5)\nprint (GBscore)","167e6b51":"#decision Tree\n\n# Parameter evaluation\ntreeclf = DecisionTreeClassifier(random_state=42)\nparameters = {'max_depth': [6, 7, 8, 9],\n              'min_samples_split': [2, 3, 4, 5,6],\n              'max_features': [1, 2, 3, 4,5,6]\n}\ngridsearch=GridSearchCV(treeclf, parameters, cv=100, scoring='roc_auc')\ngridsearch.fit(X,y)\nprint(gridsearch.best_params_)\nprint(gridsearch.best_score_)","02934aa0":"#decision Tree\n\n# Adjusting development threshold2\ntree = DecisionTreeClassifier(max_depth = 6, \n                              max_features = 4, \n                              min_samples_split = 4, \n                              random_state=42)\nX_train,X_test,y_train,y_test = train_test_split(X, y, random_state=42)\ntree.fit(X_train, y_train)\nprint(\"Accuracy on training set: {:.3f}\".format(tree.score(X_train, y_train)))\nprint(\"Accuracy on test set: {:.3f}\".format(tree.score(X_test, y_test)))\n","ac428c78":"#decision Tree\n\n# Predicting the Test set results\ny_pred = tree.predict(X_test)\n# y_pred.shape\nprint('y predict', y_pred)","66588bbf":"#decision Tree\n\n# Making the Confusion Matrix\nfrom sklearn.metrics import classification_report, confusion_matrix\ncm = confusion_matrix(y_test, y_pred)\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nf, ax = plt.subplots(figsize=(5,5))\nsns.heatmap(cm,annot = True, linewidths=0.5,linecolor=\"red\",fmt=\".0f\",ax=ax)\nplt.xlabel(\"Prediction(Ypred)\")\nplt.ylabel(\"Ytrue\")\nplt.show()\n\nprint('TN - True Negative {}'.format(cm[0,0]))\nprint('FP - False Positive {}'.format(cm[0,1]))\nprint('FN - False Negative {}'.format(cm[1,0]))\nprint('TP - True Positive {}'.format(cm[1,1]))\nprint('Accuracy Rate: {}'.format(np.divide(np.sum([cm[0,0],cm[1,1]]),np.sum(cm))))\nprint('Misclassification Rate: {}'.format(np.divide(np.sum([cm[0,1],cm[1,0]]),np.sum(cm))))","52bd6b54":"#decision Tree\n\n# Plotting the predictions\nplt.hist(y_pred,bins=10)\nplt.xlim(0,1)\nplt.xlabel(\"Predicted Proababilities\")\nplt.ylabel(\"Frequency\")\n\nround(roc_auc_score(y_test,y_pred),5)","66205d10":"#decision tree\nfrom sklearn.metrics import classification_report, confusion_matrix, roc_curve, auc\n\nprint(\"Confusion Matrix:\")\nprint(confusion_matrix(y_test, y_pred))\nprint()\nprint(\"Classification Report\")\nprint(classification_report(y_test, y_pred))\nprint('akurasi model decision tree :',round(roc_auc_score(y_test,y_pred),5))","02012f91":"DS = DecisionTreeClassifier()\nDSscore = round(roc_auc_score(y_test,y_pred),5)\nprint (DSscore)","b47b8d5f":"#K-NN model\n# Parameter evaluation\nknnclf = KNeighborsClassifier()\nparameters={'n_neighbors': range(1, 20)}\ngridsearch=GridSearchCV(knnclf, parameters, cv=100, scoring='roc_auc')\ngridsearch.fit(X, y)\nprint('grid',gridsearch)\n","d3af1997":"# plt.legend()\n# plt.savefig('TESTINGS')\nprint(gridsearch.best_params_)\nprint(gridsearch.best_score_)","9ea74324":"#K-NN model\n# Fitting K-NN to the Training set\nknnClassifier = KNeighborsClassifier(n_neighbors = 18)\nknnClassifier.fit(X_train, y_train)\nprint('Accuracy of K-NN classifier on training set: {:.2f}'.format(knnClassifier.score(X_train, y_train)))\nprint('Accuracy of K-NN classifier on test set: {:.2f}'.format(knnClassifier.score(X_test, y_test)))\n","c1ee81a3":"#K-NN\n# Predicting the Test set results\ny_pred = knnClassifier.predict(X_test)\nprint('y predict',y_pred)","489908d7":"#K-NN\n# Making the Confusion Matrix\nfrom sklearn.metrics import classification_report, confusion_matrix\ncm = confusion_matrix(y_test, y_pred)\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nf, ax = plt.subplots(figsize=(5,5))\nsns.heatmap(cm,annot = True, linewidths=0.5,linecolor=\"red\",fmt=\".0f\",ax=ax)\nplt.xlabel(\"Prediction(Ypred)\")\nplt.ylabel(\"Ytrue\")\nplt.show()\n\nprint('TN - True Negative {}'.format(cm[0,0]))\nprint('FP - False Positive {}'.format(cm[0,1]))\nprint('FN - False Negative {}'.format(cm[1,0]))\nprint('TP - True Positive {}'.format(cm[1,1]))\nprint('Accuracy Rate: {}'.format(np.divide(np.sum([cm[0,0],cm[1,1]]),np.sum(cm))))\nprint('Misclassification Rate: {}'.format(np.divide(np.sum([cm[0,1],cm[1,0]]),np.sum(cm))))","33cc3214":"# Plotting the predictions\nplt.hist(y_pred,bins=10)\nplt.xlim(0,1)\nplt.xlabel(\"Predicted Proababilities\")\nplt.ylabel(\"Frequency\")\n\nround(roc_auc_score(y_test,y_pred),5)","7a931805":"from sklearn.metrics import classification_report, confusion_matrix, roc_curve, auc\n\nprint(\"Confusion Matrix:\")\nprint(confusion_matrix(y_test, y_pred))\nprint()\nprint(\"Classification Report\")\nprint(classification_report(y_test, y_pred))\nprint('akurasi model K-NN:',round(roc_auc_score(y_test,y_pred),5))","1a09f1a2":"KNN = KNeighborsClassifier()\nKNNscore = round(roc_auc_score(y_test,y_pred),5)\nprint (KNNscore)","557d2233":"#Logistic Regresion\n# Parameter evaluation\nlogclf = LogisticRegression(random_state=42)\nparameters={'C': [1, 4, 10], 'penalty': ['l1', 'l2']}\ngridsearch=GridSearchCV(logclf, parameters, cv=100, scoring='roc_auc')\ngridsearch.fit(X, y)\nprint(gridsearch.best_params_)\nprint(gridsearch.best_score_)\n","2f2ca621":"#Logistic Regresion\n\n# Adjusting development threshold\nlogreg_classifier = LogisticRegression(C = 1, penalty = 'l1')\nX_train,X_test,y_train, y_test = train_test_split(X, y, random_state=42)\nlogreg_classifier.fit(X_train, y_train)\nprint(\"Training set score: {:.3f}\".format(logreg_classifier.score(X_train, y_train)))\nprint(\"Test set score: {:.3f}\".format(logreg_classifier.score(X_test, y_test)))","b99f77e9":"#Logistic Regresion\n\n# Predicting the Test set results\ny_pred = logreg_classifier.predict(X_test)\nprint('y predict',y_pred)","beb06044":"#Logistic Regresion\n\n# Making the Confusion Matrix\nfrom sklearn.metrics import classification_report, confusion_matrix\ncm = confusion_matrix(y_test, y_pred)\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nf, ax = plt.subplots(figsize=(5,5))\nsns.heatmap(cm,annot = True, linewidths=0.5,linecolor=\"red\",fmt=\".0f\",ax=ax)\nplt.xlabel(\"Prediction(Ypred)\")\nplt.ylabel(\"Ytrue\")\nplt.show()\n\nprint('TN - True Negative {}'.format(cm[0,0]))\nprint('FP - False Positive {}'.format(cm[0,1]))\nprint('FN - False Negative {}'.format(cm[1,0]))\nprint('TP - True Positive {}'.format(cm[1,1]))\nprint('Accuracy Rate: {}'.format(np.divide(np.sum([cm[0,0],cm[1,1]]),np.sum(cm))))\nprint('Misclassification Rate: {}'.format(np.divide(np.sum([cm[0,1],cm[1,0]]),np.sum(cm))))","08defb04":"#Logistic Regresion\n\n# Plotting the predictions\nplt.hist(y_pred,bins=10)\nplt.xlim(0,1)\nplt.xlabel(\"Predicted Proababilities\")\nplt.ylabel(\"Frequency\")\n\nround(roc_auc_score(y_test,y_pred),5)","131920bb":"#Logistic Regresion\n\nfrom sklearn.metrics import classification_report, confusion_matrix, roc_curve, auc\n\nprint(\"Confusion Matrix:\")\nprint(confusion_matrix(y_test, y_pred))\nprint()\nprint(\"Classification Report\")\nprint(classification_report(y_test, y_pred))\nprint('akurasi model Logistic Regresion : ',round(roc_auc_score(y_test,y_pred),5))","861805f3":"LS = LogisticRegression()\nLSscore = round(roc_auc_score(y_test,y_pred),5)\nprint (LSscore)","55ad0380":"#Random Forest \n\n# Parameter evaluation\n#evaluasi parameter\nrfclf = RandomForestClassifier(random_state=42)\nparameters={'n_estimators': [50, 100],\n            'max_features': ['auto', 'sqrt', 'log2'],\n            'max_depth' : [4,5,6,7],\n            'criterion' :['gini', 'entropy']\n}\ngridsearch=GridSearchCV(rfclf, parameters, cv=50, scoring='roc_auc', n_jobs = -1)\ngridsearch.fit(X, y)\nprint(gridsearch.best_params_)\nprint(gridsearch.best_score_)","7fea05cc":"#Random Forest \n\n#acuaration random forest\nrf = RandomForestClassifier(n_estimators=100, criterion = 'gini', max_depth = 6, \n                            max_features = 'auto', random_state=0)\nrf.fit(X_train, y_train)\nprint(\"Accuracy on training set: {:.3f}\".format(rf.score(X_train, y_train)))\nprint(\"Accuracy on test set: {:.3f}\".format(rf.score(X_test, y_test)))","c18528d0":"#Random Forest \n# Predicting the Test set results\ny_pred = rf.predict(X_test)\nprint('y predict', y_pred)","f305bc49":"#random Forest\n\n# Making the Confusion Matrix\nfrom sklearn.metrics import classification_report, confusion_matrix\ncm = confusion_matrix(y_test, y_pred)\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nf, ax = plt.subplots(figsize=(5,5))\nsns.heatmap(cm,annot = True, linewidths=0.5,linecolor=\"red\",fmt=\".0f\",ax=ax)\nplt.xlabel(\"Prediction(Ypred)\")\nplt.ylabel(\"Ytrue\")\nplt.show()\n\nprint('T - True Negative {}'.format(cm[0,0]))\nprint('FP - False Positive {}'.format(cm[0,1]))\nprint('FN - False Negative {}'.format(cm[1,0]))\nprint('TP - True Positive {}'.format(cm[1,1]))\nprint('Accuracy Rate: {}'.format(np.divide(np.sum([cm[0,0],cm[1,1]]),np.sum(cm))))\nprint('Misclassification Rate: {}'.format(np.divide(np.sum([cm[0,1],cm[1,0]]),np.sum(cm))))","ccdc64af":"#Random Forest\n# Plotting the predictions\nplt.hist(y_pred,bins=10)\nplt.xlim(0,1)\nplt.xlabel(\"Predicted Proababilities\")\nplt.ylabel(\"Frequency\")\n\nround(roc_auc_score(y_test,y_pred),5)","70771700":"from sklearn.metrics import classification_report, confusion_matrix, roc_curve, auc\n\nprint(\"Confusion Matrix:\")\nprint(confusion_matrix(y_test, y_pred))\nprint()\nprint(\"Classification Report\")\nprint(classification_report(y_test, y_pred))\nprint('akurasi model Random Forest :',round(roc_auc_score(y_test,y_pred),5))","2ba3a390":"RF = RandomForestClassifier()\nRFscore = round(roc_auc_score(y_test,y_pred),5)\nprint (RFscore)","76a634ad":"# plotly\nimport plotly\nfrom plotly.offline import init_notebook_mode, iplot\nplotly.offline.init_notebook_mode(connected=True)\nimport plotly.offline as py\nimport plotly.graph_objs as go\nimport itertools\nplt.style.use('fivethirtyeight')\n\nscores=[GBscore,KNNscore,DSscore,LSscore,RFscore]\nAlgorthmsName=[\"Gradient Boasting\",\"K-NN\",\"Decision Tree\",\"Logistic Regresion\",\"Random Forest\"]\n#create traces\ntrace1 = go.Scatter(\n    x = AlgorthmsName,\n    y= scores,\n    name='Algortms Name',\n    marker =dict(color='rgba(0,255,0,0.5)',\n               line =dict(color='rgb(0,0,0)',width=2)),\n                text=AlgorthmsName\n)\ndata = [trace1]\nlayout = go.Layout(barmode = \"group\",\n                  xaxis= dict(title= 'ML Algorithms',ticklen= 5,zeroline= False),\n              yaxis= dict(title= 'Accuracy Scores',ticklen= 5,zeroline= False))\nfig = go.Figure(data = data, layout = layout)\niplot(fig)","af3d017e":"Machine learning is an application of artificial intelligence (AI) that provides systems the ability to automatically learn and improve from experience without being explicitly programmed. Machine learning (ML) techniques allows us to obtain predictive, the dataset we are testing is pima-indian-diabetes with a dataset of 765 raw data with 8 data features and 1 data label we developed a method to achieve the best accuracy from the 5 methods we use with the stages of separation traning and testing the dataset, scaling features, parameters evaluation, confusion matrix and we get the accuracy of each method, and the results of the accuracy we get with these 5 methods Gradient-boasting is best with an accuracy score of 0.8, Decision Tree 0.72, Random Forest 0.72, next is Logistic Regression 0.7, and then followed by K-NN method with a score of 0.65"}}