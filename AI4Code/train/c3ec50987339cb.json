{"cell_type":{"29c6d87f":"code","628d1b5e":"code","c70beb39":"code","ea0b58c7":"code","db42cbc6":"code","9666928d":"code","2fbeb160":"code","3f8b7881":"code","a1ad574f":"code","dda78820":"code","e76ff1b5":"code","40d83889":"code","1d6597c1":"code","95575c7d":"code","7178cc5a":"code","3f82ed86":"code","fa28b3ff":"code","a8d0fd78":"code","9a5d68ff":"code","38364856":"code","22b6ead9":"code","2ea59c0f":"code","7b26644c":"code","f9696bb7":"code","16835c73":"code","50ee5d38":"code","dc1423a5":"code","ca883406":"code","360ed8d4":"code","61b273a2":"code","f6f637ef":"code","d7bb8c29":"code","278a4b24":"code","e8b3926c":"code","13ecd03c":"code","b6345bf2":"code","bba54166":"code","1ac785c2":"code","4fe83c60":"code","e036cb2a":"code","f7d1a73b":"code","f9c40dec":"markdown","aa2347b9":"markdown","1669b2df":"markdown","7a465dff":"markdown","d4919f7d":"markdown","8fe8bdc1":"markdown","58c25ac4":"markdown","82b40761":"markdown","c2b144ca":"markdown","77e1b06b":"markdown","03409a5f":"markdown","abaaf451":"markdown","4786e84d":"markdown","dc851618":"markdown","6b621696":"markdown","5c2d9243":"markdown","fa9d6fc4":"markdown","3f164cda":"markdown","4ec13650":"markdown","09163b8c":"markdown","df11678c":"markdown","c0864e6a":"markdown"},"source":{"29c6d87f":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\nimport matplotlib\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom collections import Counter\nimport gc\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import train_test_split \nimport lightgbm as lgb\n%matplotlib inline \n# Any results you write to the current directory are saved as output.","628d1b5e":"PATH = \"..\/input\"\nlist_of_files = os.listdir(PATH)\n\napplication_train = pd.read_csv(PATH+\"\/application_train.csv\")\napplication_test = pd.read_csv(PATH+\"\/application_test.csv\")\nbureau = pd.read_csv(PATH+\"\/bureau.csv\")\nbureau_balance = pd.read_csv(PATH+\"\/bureau_balance.csv\")\ncredit_card_balance = pd.read_csv(PATH+\"\/credit_card_balance.csv\")\ninstallments_payments = pd.read_csv(PATH+\"\/installments_payments.csv\")\nprevious_application = pd.read_csv(PATH+\"\/previous_application.csv\")\nPOS_CASH_balance = pd.read_csv(PATH+\"\/POS_CASH_balance.csv\")","c70beb39":"list_of_files.sort()\nlist_of_files.remove(\"sample_submission.csv\")\nprint(list_of_files)\nshape = {\"rows\" : [POS_CASH_balance.shape[0],\n        application_test.shape[0], \n        application_train.shape[0], \n        bureau.shape[0], \n        bureau_balance.shape[0],\n        credit_card_balance.shape[0],\n        installments_payments.shape[0],\n        previous_application.shape[0]], \n        \"cols\" : [POS_CASH_balance.shape[1],\n        application_test.shape[1], \n        application_train.shape[1], \n        bureau.shape[1], \n        bureau_balance.shape[1],\n        credit_card_balance.shape[1],\n        installments_payments.shape[1],\n        previous_application.shape[1]]}\n\nshapes = pd.DataFrame(shape, index = list_of_files)\nprint (shapes)","ea0b58c7":"POS_CASH_balance.head()","db42cbc6":"application_test.head()","9666928d":"application_train.head()","2fbeb160":"bureau.head()","3f8b7881":"bureau_balance.head()","a1ad574f":"credit_card_balance.head()","dda78820":"installments_payments.head()","e76ff1b5":"previous_application.head()","40d83889":"total_IDS = np.concatenate((application_test[\"SK_ID_CURR\"].values, application_train[\"SK_ID_CURR\"].values))\nprint(len(np.unique(np.array(total_IDS))) == len(total_IDS))#No redundent IDs within train and test data","1d6597c1":"\nPOS_CASH_balance_IDS = POS_CASH_balance[\"SK_ID_CURR\"].values\nbureau_IDS = bureau[\"SK_ID_CURR\"].values\ncredit_card_balance_IDS = credit_card_balance[\"SK_ID_CURR\"].values\ninstallments_payments_IDS = installments_payments[\"SK_ID_CURR\"].values\nprevious_application_IDS = previous_application[\"SK_ID_CURR\"].values\n\ntot = len(total_IDS)\nprint(tot)\n\nprint (len(np.intersect1d(POS_CASH_balance_IDS, total_IDS))\/tot*100,\nlen(np.intersect1d(bureau_IDS, total_IDS))\/tot*100,\nlen(np.intersect1d(credit_card_balance_IDS, total_IDS))\/tot*100,\nlen(np.intersect1d(installments_payments_IDS, total_IDS))\/tot*100,\nlen(np.intersect1d(previous_application_IDS, total_IDS))\/tot*100)","95575c7d":"prev = previous_application[\"SK_ID_PREV\"].values\n\nPOS_CASH_balance_IDS_prev = POS_CASH_balance[\"SK_ID_PREV\"].values\ncredit_card_balance_IDS_prev = credit_card_balance[\"SK_ID_PREV\"].values\ninstallments_payments_IDS_prev = installments_payments[\"SK_ID_PREV\"].values\n\nprev_num = len(prev)\n\nprint (prev_num)\n\nprint(len(np.intersect1d(POS_CASH_balance_IDS_prev, prev))\/prev_num*100,\nlen(np.intersect1d(credit_card_balance_IDS_prev, prev))\/prev_num*100,\nlen(np.intersect1d(installments_payments_IDS_prev, prev))\/prev_num*100)\n","7178cc5a":"bureau_br = np.unique(bureau[\"SK_ID_BUREAU\"].values)\nprint(len(np.intersect1d(np.unique(bureau_balance[\"SK_ID_BUREAU\"].values), bureau_br))\/len(bureau_br)*100)","3f82ed86":"breau_total = np.unique(np.intersect1d(bureau_IDS, total_IDS)) #by SK_ID_CURR\nbureau_filtered = bureau.loc[bureau[\"SK_ID_CURR\"].isin(breau_total)] #bureau & train \/ test - sharing SK_ID_CURR\n\nb = np.intersect1d(np.unique(bureau_filtered[\"SK_ID_BUREAU\"].values), np.unique(bureau_balance[\"SK_ID_BUREAU\"].values)) #br & br balance\n\n\nbureau_filtered = bureau_filtered.loc[bureau_filtered[\"SK_ID_BUREAU\"].isin(b)]\nlen(bureau_filtered[\"SK_ID_CURR\"].values)\nbureau_filtered","fa28b3ff":"print (len(np.unique(bureau_filtered[\"SK_ID_CURR\"].values))\/tot*100)","a8d0fd78":"train = application_train.drop([\"TARGET\"], axis = 1)\ntrain_target = application_train[\"TARGET\"]\ntest= application_test.copy()\ntr = len(application_train)\nprint (all(i ==True for i in train.columns==test.columns))\n\n","9a5d68ff":"#Dividing categorical and numerical features: \ndf = pd.concat([train, test])\n\ndel train, test, application_train, application_test\ngc.collect()\n\n\ndef categorical_features(data):\n    features = [i for i in list(data.columns) if data[i].dtype == 'object']\n    return features\n\ncategorical = categorical_features(df)\nnumerical = [i for i in df.columns if i not in categorical]\nnumerical.remove(\"SK_ID_CURR\")\nIDs = df[\"SK_ID_CURR\"]\n","38364856":"#Processing categorical features\nfor feature in categorical:\n    df[feature].fillna(\"unidentified\")\n    print(f'Transforming {feature}...')\n    encoder = LabelEncoder()\n    encoder.fit(df[feature].astype(str))\n    df[feature] = encoder.transform(df[feature].astype(str))\n    \ndf.head()","22b6ead9":"#processing numeric features #Try log operation later\nfor feats in df.columns:\n    df[feats] = df[feats].fillna(-1)\n        \ndf.head()","2ea59c0f":"#POS_CASH_balance, bureau, credit_card_balance, installments_payments, previous_application\n\n#POS_CASH_balance\nPOS_CASH_balance_G1 = POS_CASH_balance.loc[POS_CASH_balance[\"SK_ID_CURR\"].isin(total_IDS)]\nprint (len(np.unique(POS_CASH_balance_G1[\"SK_ID_CURR\"].values)))\nPOS_CASH_balance_G1.head()","7b26644c":"def plot_distribution(dataframe,feature,color):\n    plt.figure(figsize=(10,6))\n    plt.title(\"Distribution of %s\" % feature)\n    sns.distplot(dataframe[feature].dropna(),color=color, kde=True,bins=100)\n    plt.show()   \n\ncolors = [\"blue\", \"red\", \"green\", \"tomato\", \"brown\", \"black\", \"Gray\"]\nfor i, j in zip(POS_CASH_balance_G1.drop(\"NAME_CONTRACT_STATUS\", axis =1).columns, colors):\n    plot_distribution(POS_CASH_balance_G1, i, j)\n    \ndic = Counter(POS_CASH_balance_G1[\"NAME_CONTRACT_STATUS\"])\nplt.bar(range(len(dic)), list(dic.values()))\nplt.xticks(range(len(dic)), list(dic.keys()), rotation = 90)\nplt.show()","f9696bb7":"np.unique(POS_CASH_balance_G1[\"NAME_CONTRACT_STATUS\"].values)\nPOS_CASH_balance_G1_num = (POS_CASH_balance_G1.groupby(\"SK_ID_CURR\", as_index=False).mean())\nnb = POS_CASH_balance_G1[[\"SK_ID_CURR\", \"NAME_CONTRACT_STATUS\"]].groupby(\"SK_ID_CURR\", as_index = False).count()\nnb[\"num_in_POS_CASH\"] = nb[\"NAME_CONTRACT_STATUS\"]\n\ndf = df.merge(POS_CASH_balance_G1_num.drop(\"SK_ID_PREV\", axis = 1), on='SK_ID_CURR', how='left').fillna(-1)\ndf = df.merge(nb.drop(\"NAME_CONTRACT_STATUS\", axis = 1), on='SK_ID_CURR', how='left').fillna(-1)\n\n\ndel nb, POS_CASH_balance_G1_num, POS_CASH_balance_G1\ngc.collect()\n\n\ndf.head()\n","16835c73":"#from POS_CASH_balance_G1, let's merge categorical feature - NAME_CONTRACT_STATUS\n#Also let's do log processing for large numbers","50ee5d38":"#Bureau\nbureau_G1 = bureau.drop([\"SK_ID_BUREAU\"], axis = 1).loc[bureau[\"SK_ID_CURR\"].isin(total_IDS)]\nprint (len(np.unique(bureau_G1[\"SK_ID_CURR\"].values)))\nbureau_G1.head()","dc1423a5":"for i in (bureau_G1.drop([\"CREDIT_ACTIVE\", \"CREDIT_CURRENCY\", \"CREDIT_TYPE\"], axis =1).columns): #numerical values\n    plot_distribution(bureau_G1, i, \"blue\")\n\n\nfor i in [\"CREDIT_ACTIVE\", \"CREDIT_CURRENCY\", \"CREDIT_TYPE\"]: #categorical values\n    dic = Counter(bureau_G1[i])\n    plt.bar(range(len(dic)), list(dic.values()))\n    plt.xticks(range(len(dic)), list(dic.keys()), rotation = 90)\n    plt.title(i)\n    plt.show()","ca883406":"bureau_G1_num = (bureau_G1.groupby(\"SK_ID_CURR\", as_index=False).mean())\nnb = bureau_G1[[\"SK_ID_CURR\", \"CREDIT_ACTIVE\"]].groupby(\"SK_ID_CURR\", as_index = False).count()\nnb[\"num_in_bureau\"] = nb[\"CREDIT_ACTIVE\"]\n\ndf = df.merge(bureau_G1_num, on='SK_ID_CURR', how='left').fillna(-1)\ndf = df.merge(nb.drop(\"CREDIT_ACTIVE\", axis=1), on='SK_ID_CURR', how='left').fillna(-1)\n\n\ndel nb, bureau_G1_num, bureau_G1\ngc.collect()\n\n\ndf.head()","360ed8d4":"#let's work on categoricals - \"CREDIT_ACTIVE\", \"CREDIT_CURRENCY\", \"CREDIT_TYPE\"","61b273a2":"##credit_card_balance\ncredit_card_balance_G1 = credit_card_balance.drop([\"SK_ID_PREV\"], axis = 1).loc[credit_card_balance[\"SK_ID_CURR\"].isin(total_IDS)]\nprint (len(np.unique(credit_card_balance_G1[\"SK_ID_CURR\"].values)))\ncredit_card_balance_G1.head()","f6f637ef":"for i in (credit_card_balance_G1.drop([\"NAME_CONTRACT_STATUS\"], axis =1).columns): #numerical values\n    plot_distribution(credit_card_balance_G1, i, \"blue\")\n\n\nfor i in [\"NAME_CONTRACT_STATUS\"]: #categorical values\n    dic = Counter(credit_card_balance_G1[i])\n    plt.bar(range(len(dic)), list(dic.values()))\n    plt.xticks(range(len(dic)), list(dic.keys()), rotation = 90)\n    plt.title(i)\n    plt.show()","d7bb8c29":"credit_card_balance_G1_num = (credit_card_balance_G1.groupby(\"SK_ID_CURR\", as_index=False).mean())\nnb = credit_card_balance_G1[[\"SK_ID_CURR\", \"NAME_CONTRACT_STATUS\"]].groupby(\"SK_ID_CURR\", as_index = False).count()\nnb[\"num_in_credit_card\"] = nb[\"NAME_CONTRACT_STATUS\"]\n\ndf = df.merge(credit_card_balance_G1_num, on='SK_ID_CURR', how='left').fillna(-1)\ndf = df.merge(nb.drop(\"NAME_CONTRACT_STATUS\", axis=1), on='SK_ID_CURR', how='left').fillna(-1)\n\n\ndel nb, credit_card_balance_G1_num, credit_card_balance_G1\ngc.collect()\n\n\ndf.head()","278a4b24":"##installments_payments\ninstallments_payments_G1 = installments_payments.drop([\"SK_ID_PREV\"], axis = 1).loc[installments_payments[\"SK_ID_CURR\"].isin(total_IDS)]\nprint (len(np.unique(installments_payments_G1[\"SK_ID_CURR\"].values)))\ninstallments_payments_G1.head()","e8b3926c":"for i in (installments_payments_G1.columns): #numerical values\n    plot_distribution(installments_payments_G1, i, \"blue\")\n","13ecd03c":"installments_payments_G1_num = (installments_payments_G1.groupby(\"SK_ID_CURR\", as_index=False).mean())\nnb = installments_payments_G1[[\"SK_ID_CURR\", \"NUM_INSTALMENT_VERSION\"]].groupby(\"SK_ID_CURR\", as_index = False).count()\nnb[\"num_in_install_pay\"] = nb[\"NUM_INSTALMENT_VERSION\"]\n\ndf = df.merge(installments_payments_G1_num, on='SK_ID_CURR', how='left').fillna(-1)\ndf = df.merge(nb.drop(\"NUM_INSTALMENT_VERSION\", axis=1), on='SK_ID_CURR', how='left').fillna(-1)\n\n\ndel nb, installments_payments_G1_num, installments_payments_G1\ngc.collect()\n\n\ndf.head()","b6345bf2":"##previous_application\nprevious_application_G1 = previous_application.drop([\"SK_ID_PREV\"], axis = 1).loc[previous_application[\"SK_ID_CURR\"].isin(total_IDS)]\nprint (len(np.unique(previous_application_G1[\"SK_ID_CURR\"].values)))\nprevious_application_G1.head()","bba54166":"categorical = categorical_features(previous_application_G1)\nnumerical = [i for i in previous_application_G1.columns if i not in categorical]\nnumerical.remove(\"SK_ID_CURR\")\n\nfor i in numerical: #numerical values\n    plot_distribution(previous_application_G1, i, \"blue\")\n\n\nfor i in categorical: #categorical values\n    dic = Counter(previous_application_G1[i])\n    plt.bar(range(len(dic)), list(dic.values()))\n    plt.xticks(range(len(dic)), list(dic.keys()), rotation = 90)\n    plt.title(i)\n    plt.show()","1ac785c2":"#Merge with numerical values\nprevious_application_G1_num = (previous_application_G1.groupby(\"SK_ID_CURR\", as_index=False).mean())\nnb = previous_application_G1[[\"SK_ID_CURR\", \"NAME_CONTRACT_TYPE\"]].groupby(\"SK_ID_CURR\", as_index = False).count()\nnb[\"num_in_previous_app\"] = nb[\"NAME_CONTRACT_TYPE\"]\n\ndf = df.merge(previous_application_G1_num, on='SK_ID_CURR', how='left').fillna(-1)\ndf = df.merge(nb.drop(\"NAME_CONTRACT_TYPE\", axis=1), on='SK_ID_CURR', how='left').fillna(-1)\n\n\ndel nb, previous_application_G1_num, previous_application_G1\ngc.collect()\n\n\ndf.head()","4fe83c60":"from lightgbm import LGBMClassifier\nfrom sklearn.metrics import roc_auc_score, log_loss\nfrom sklearn.model_selection import StratifiedKFold, KFold\n\ntrain_X = df[:tr].drop(\"SK_ID_CURR\", axis = 1)\ntest_X = df[tr:].drop(\"SK_ID_CURR\", axis = 1)\n\ntrain_X[\"TARGET\"] = train_target\n#label: train_target\ny = train_target\n\nfolds = KFold(n_splits=5, shuffle=True, random_state=42)\noof_preds = np.zeros(train_X.shape[0])\nsub_preds = np.zeros(test_X.shape[0])\nfeats = [f for f in train_X.columns if f not in ['SK_ID_CURR','TARGET']]","e036cb2a":"for n_fold, (trn_idx, val_idx) in enumerate(folds.split(train_X)):\n    trn_x, trn_y = train_X[feats].iloc[trn_idx], train_X.iloc[trn_idx]['TARGET']\n    val_x, val_y = train_X[feats].iloc[val_idx], train_X.iloc[val_idx]['TARGET']\n    \n    clf = LGBMClassifier(\n        n_estimators=10000,\n        learning_rate=0.01,\n        num_leaves=30,\n        colsample_bytree=.8,\n        subsample=.9,\n        max_depth=7,\n        reg_alpha=.1,\n        reg_lambda=.1,\n        min_split_gain=.01,\n        min_child_weight=100,\n        #scale_pos_weight=12.5,\n        silent=-1,\n        verbose=-1,\n    )\n    \n    clf.fit(trn_x, trn_y, \n            eval_set= [(trn_x, trn_y), (val_x, val_y)], \n            eval_metric='auc', verbose=100, early_stopping_rounds=100  #30\n           )\n    \n    oof_preds[val_idx] = clf.predict_proba(val_x, num_iteration=clf.best_iteration_)[:, 1]\n    sub_preds += clf.predict_proba(test_X[feats], num_iteration=clf.best_iteration_)[:, 1] \/ folds.n_splits\n    \n    print('Fold %2d AUC : %.6f' % (n_fold + 1, roc_auc_score(val_y, oof_preds[val_idx])))\n    del trn_x, trn_y, val_x, val_y\n    gc.collect()","f7d1a73b":"submission = pd.read_csv(\"..\/input\/sample_submission.csv\")\nsubmission['TARGET'] = sub_preds\nsubmission.to_csv(\"baseline2.csv\", index=False)\nsubmission.head()","f9c40dec":"Processing application_train\/test","aa2347b9":"**Processing and merging *previous_application***","1669b2df":"**Group 1 - train \/ test dataset , bureau, POS_CASH_balance, credit_card_balance, installment_payment, and previous_application**","7a465dff":"**Try LGBM with merged data**\n\nYet, I have much more things to be done (I noted them at the bottom of this notebook). However, I'll try LGBM with what I've got so far (Group 1)","d4919f7d":"Let's merge numerical values by the means within same SK_ID","8fe8bdc1":"**Processing and merging *bureau***","58c25ac4":"First let's see to what extent each group shares SK_IDs","82b40761":"More things to do:\n- Adding categorical features when merging files (I only added numerical features for now. This was because each file have so many data on redundant SK_ID, so in case of numerical data I just merged average value within identical IDs, but in case of categorical data I'm not sure what would be the best way to merge it. For now I'm trying to get mode of categorical values** (But identifying mode of each IDs and merging them takes too much time and memory since there exists too much different IDs and categories.) I'll be glad to share any ideas on how to merge categorical values here!**\n\n- Do log processing for numerical values (Haven't processed yet due to irregularity of numerical features, and some of files having too much features)\n\n- More carefully handle Nan values : I just put -1 to all Nan values, but in some cases, say, if there exists value '-1' (or near -1) within the columns -1 would not appropriately identify 'Non existing value' for the column.\n\n- Merge group 2 (previous payment as a hub, SK_ID_PREV as keys) to find out information on previous application. \n\n- Merge Bureau balance to df (Group 3)","c2b144ca":"**Let's look at the data**\nNumber of rows of most files are larger than numbers of rows of test \/ train data. Therefore, if all if entities (IDs) from train \/ test dataset are included in other files, we can merge other files' features into applicaion_test \/ application_train dataframe. If not all, but if most of IDs from applicaion_test \/ application_train are included in other files, we could merge them by estimating empty values. ","77e1b06b":"* **Processing and merging* credit_card_balance***","03409a5f":"**Group 2 - previous_application, POS_CASH_balance,  credit_card_balance, installment_payment**","abaaf451":"**Group 3 - bureau and bureau_balance**","4786e84d":"**Processing and mergins *installments_payments***","dc851618":"**To sum up, In group 1, train \/ test data as hub, we can merge : **\n\n* POS_CASH_balance : 94.66589942597297 %\n* bureau : 85.84047943186762 %\n* credit_card_balance : 29.06850430169401 %\n* installments_payments : 95.3213288234551 %\n* previous_application : 95.11641941867482%\n\n** In group 2, previous_application as a hub, we can merge: **\n* POS_CASH_balance : 53.81963029887188 %\n* credit_card_balance : 5.564257035326012  %\n* installments_payments : 57.41210407768106 %\n\n** Linking train \/ test data with bureau_balance, using bureau as a link: **\n* 37.7656453944506%\n\nof entire rows (IDs) could be merged","6b621696":"**Ideas on hot to merge \/ concatenate data**\n\n![](https:\/\/storage.googleapis.com\/kaggle-media\/competitions\/home-credit\/home_credit.png)\nAccording to the image on dataset explanation, there are some keys to merge \/ concatenate sparse data into some organized form. \n\n1. First of all, with SK_ID CURR, we can find linkage between ***train \/ test dataset , bureau, POS_CASH_balance, credit_card_balance, installment_payment, and previous_application*** together. Here, since train \/ test dataset is our main target to work on, train\/test dataset could be the hub of these data. \n\n2. With SK_ID_PREV, we cand find linkage between ***previous_application, POS_CASH_balance,  credit_card_balance, installment_payment*** and here, previoud_application may be the hub. (Since the linkage is 'SK_ID_PREV' and the infomation is mainly about previous information.\n\n3. Lastly, with SK_ID_BUREAU, we can link ***bureau and bureau_balance*** thereby linking these to the first group - to train \/ test dataset. \n\nWhen we merge or link these separate data, we should consider** how to deal with Nan values. **If most of the files in each group shares most of the SK_IDs within there group, we can merge them on SK_ID and fill Nans by rather **1) putting estimated values, or 2) putting 0 or negative values as a sign of 'unidentified''** If 'unable to identify value' itself have significant implication (ex. if, say,  applicants with certain range of risk probability have tendency to have more missing values), putting 0 or negative value as sign of missing value would be more effective. ","5c2d9243":"Now let's look at the shape of this dataset","fa9d6fc4":"However, here, I'm thinking of linking Bureau data train \/ test dataset. Since Bureau shares 85% of it's id with train \/ test data, I'll check how much of those shared IDs are also shared with bureau_balance. ","3f164cda":"**Now, let's merge group 1**\n\n","4ec13650":"**credit_card_balance**shows only little amount of IDS shared with training \/ test dataset. Others seems like including most of ID from training \/ test data. Seems like only if we handle them well, we could link them and work on it together. ","09163b8c":"There are many redundant IDs. There are some value differences within the identical IDs, so later we'll think of ways to work on them - say, merge their values into the average, etc. For now, let's just check how many of independent IDs are shared. ","df11678c":"**Processing and merging *POS_CASH_balance***","c0864e6a":"let's look at how it is distribute.\n I refered to the function of plotting from [this kernal](http:\/\/https:\/\/www.kaggle.com\/gpreda\/home-credit-default-risk-extensive-eda)"}}