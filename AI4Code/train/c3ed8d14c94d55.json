{"cell_type":{"bd8cc7ab":"code","19e95e25":"code","7d98ce39":"code","70a58f34":"code","08d8c762":"code","f57d9f95":"code","d4d36b40":"code","c8dda6e6":"code","84fd3d16":"code","6fda49b7":"code","8a1fb4e1":"code","45147fc8":"code","d56d349b":"code","652ad707":"code","04c16a2f":"code","2b2e91c0":"code","bc26cf98":"code","8dc5bb84":"markdown","ee8d27c3":"markdown","bdc02aed":"markdown","d9510522":"markdown","b1c91f08":"markdown","7b3a8e1a":"markdown","861e4e19":"markdown","c7ec021f":"markdown","015dab1b":"markdown","4791f48b":"markdown","baa92b97":"markdown","3deffadc":"markdown","2406cea9":"markdown","1ab78e35":"markdown"},"source":{"bd8cc7ab":"import numpy as np\nimport pandas as pd\n\nimport urllib.request\nfrom PIL import Image\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_squared_error","19e95e25":"\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.svm import SVR\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.model_selection import RepeatedStratifiedKFold, RepeatedKFold # RepeatedStratifiedKFold s\u00f3lo se utiliza para categor\u00edas \n\n\n# se generan valores num\u00e9ricos con todas las columnas\n# cut (distribuci\u00f3n logaritmica 'manual') -> Ideal=100, Premium=95, Very Good= 85, Good=70 y Fair=50\ndef cut_to_number (cut, log_list) :\n    if cut == 'Ideal':\n        return log_list[0]\n    elif cut == 'Premium':\n        return log_list[1]\n    elif cut == 'Very Good':\n        return log_list[2]\n    elif cut == 'Good':\n        return log_list[3]\n    else:\n        return log_list[4]\n\n# color (\u00eddem) -> D=100, E=95, F=85, G=70 y H=50\ndef color_to_number (color, log_list) :\n    if color == 'D':\n        return log_list[0]\n    elif color == 'E':\n        return log_list[1]\n    elif color == 'F':\n        return log_list[2]\n    elif color == 'G':\n        return log_list[3]\n    else:\n        return log_list[4]\n\n# clarity (\u00eddem) -> IF=100, VVS1=98, VVS2=94, VS1=88, VS2=80, SI1=69, SI2=55 y  I1=37\ndef clarity_to_number (clarity, log_list) :\n    if clarity == 'IF':\n        return log_list[0]\n    elif clarity == 'VVS1':\n        return log_list[1]\n    elif clarity == 'VVS2':\n        return log_list[2]\n    elif clarity == 'VS1':\n        return log_list[3]\n    elif clarity == 'VS2':\n        return log_list[4]\n    elif clarity == 'SI1':\n        return log_list[5]\n    elif clarity == 'SI2':\n        return log_list[6]\n    else:\n        return log_list[7]\n\n\n","7d98ce39":"log_list5 = [100, 95, 85, 70, 50]\nlog_list8 = [100, 98, 94, 88, 80,69, 55, 37]\n\n#log_list5 = [5, 4, 3, 2, 1]\n#log_list8 = [8, 7, 6, 5, 4,3, 2, 1]\n\n#log_list5 = [100, 98, 95, 92, 88]\n#log_list8 = [100, 99, 98, 97, 95,92, 89, 85]\n\n### Para que funcione necesitas bajarte los archivos de datos de Kaggle \ndf = pd.read_csv(\"diamonds_train.csv\", index_col=0)\ndf['cut'] = df['cut'].apply(lambda x: cut_to_number(x, log_list5))\ndf['color'] = df['color'].apply(lambda x: color_to_number(x, log_list5))\ndf['clarity'] = df['clarity'].apply(lambda x: clarity_to_number(x, log_list8))\ndf","70a58f34":"# 1. Definir X e y\nX = np.array(df.drop(columns=['price'])) # se elige toda la tabla menos el precio, depth y\ttable\n#X = np.array(df.drop(columns=['price','depth','table'])) # se elige toda la tabla menos el precio, depth y\ttable\n#X = np.array(df[['carat','cut','color','clarity','depth','table']]) # se selecciona las columnas m\u00e1s importantes\ny = np.array(df[\"price\"])\n\n","08d8c762":"# 2. Dividir X_train, X_test, y_train, y_test\nX_train, X_test, y_train, y_test = train_test_split(X, y, shuffle= True, test_size = 0.20, random_state = 42)\nX_train","f57d9f95":"X_train","d4d36b40":"model = RandomForestRegressor(n_estimators=5000,oob_score=True,bootstrap=True,max_features=4,min_samples_split=10,min_samples_leaf=2)\nmodel.fit(X_train,y_train)\n\n# 5. Predecir con el modelo ya entrenado con X_test\npredictions = model.predict(X_test)\nprint('predicciones test:\\n', predictions)\n\n# 6. Sacar m\u00e9tricas, valorar el modelo; en la competici\u00f3n se va a evaluar con la m\u00e9trica de RMSE.\nprint('score (RMSE) de las predicciones:', np.sqrt(mean_squared_error(y_test, predictions)))\n\n# Saving feature names for later use\nfeature_list = list(df.columns)\n# Get numerical feature importances\nimportances = list(model.feature_importances_)\n# List of tuples with variable and importance\nfeature_importances = [(feature, round(importance, 2)) for feature, importance in zip(feature_list, importances)]\n# Sort the feature importances by most important first\nfeature_importances = sorted(feature_importances, key = lambda x: x[1], reverse = True)\n# Print out the feature and importances \n[print('Variable: {:20} Importance: {}'.format(*pair)) for pair in feature_importances];","c8dda6e6":"k_fold = RepeatedKFold(n_splits=15, n_repeats=3, random_state=1)\nval_score = []\ntrain_score = []\n# define the model\nn_stimators = 1500\nmodel = RandomForestRegressor(n_estimators=2000,oob_score=True,bootstrap=True,max_features=4,min_samples_split=10,min_samples_leaf=2, warm_start=True)\npath = os.getcwd() + os.sep\nprint(\"path:\", path)\n# train son las POSICIONES de los elementos que hay que coger por iteraci\u00f3n como parte del conjunto de entrenamiento\n# val son las POSICIONES de los elementos que hay que coger por iteraci\u00f3n como parte del conjunto de validaci\u00f3n\nfor i, (train, val) in enumerate(k_fold.split(X_train)):\n    print(\"Iteraci\u00f3n:\", i+1)\n    print(\"train\/\u00edndices batch:\", train)\n    print(\"batch_size:\", len(train))\n    #print(\"val:\", val)\n    print(\"val_size:\", len(val))\n    #print(\"batch:\", X_train[train])\n\n    # train the model\n    model.fit(X_train[train], y_train[train]) # se cogen los \u00edndices que se generan en kfold_split del for\n\n    # Add more stimators\n    model.n_estimators += 100 # para poder tener en cuenta los nuevos datos, hay que aumentar los n_estimators\n\n    score_val = model.score(X_train[val], y_train[val])\n    val_score.append(score_val)\n    score_train = model.score(X_train[train], y_train[train])\n    train_score.append(score_train)","84fd3d16":"\npredictions = model.predict(X_test)\n\n# 5. Predecir con el modelo ya entrenado con X_test\n#predictions = model.predict(X_test)\nprint('predicciones test:\\n', predictions)\n\n# 6. Sacar m\u00e9tricas, valorar el modelo; en la competici\u00f3n se va a evaluar con la m\u00e9trica de RMSE.\nprint('score (RMSE) de las predicciones:', np.sqrt(mean_squared_error(y_test, predictions)))","6fda49b7":"df_test = pd.read_csv(\"diamonds_test.csv\", index_col=0)\ndf_test['cut'] = df_test['cut'].apply(lambda x: cut_to_number(x, log_list5))\ndf_test['color'] = df_test['color'].apply(lambda x: color_to_number(x, log_list5))\ndf_test['clarity'] = df_test['clarity'].apply(lambda x: clarity_to_number(x,log_list8))\n\nsample = pd.read_csv(\"sample_submission.csv\")\n\ndf_test.head()","8a1fb4e1":"#X_pred = np.array(df_test[['carat','cut','color','clarity','depth','table']]) #\n#X_pred = np.array(df_test.drop(columns=['depth','table'])) # pruebas con las tabla completa\nX_pred = np.array(df_test) # pruebas con las tabla completa\ny_sample = sample['price']\nX_pred.shape\n#y_sample.shape","45147fc8":"# l\u00edneas para el non-linear regression model\n#X_pred = polinominal_model.transform(X_pred)\npredictions_submit = model.predict(X_pred)\n\n# l\u00edneas para todos los dem\u00e1s\n#predictions_submit = model.predict(X_pred)\npredictions_submit","d56d349b":"# control de error\nnp.sqrt(mean_squared_error(y_sample, predictions_submit))","652ad707":"submission = pd.DataFrame({\"id\": range(len(predictions_submit)), \"price\": predictions_submit})","04c16a2f":"submission.head()","2b2e91c0":"submission.shape","bc26cf98":"def chequeator(df_to_submit):\n    \"\"\"\n    Esta funci\u00f3n se asegura de que tu submission tenga la forma requerida por Kaggle.\n    \n    Si es as\u00ed, se guardar\u00e1 el dataframe en un `csv` y estar\u00e1 listo para subir a Kaggle.\n    \n    Si no, LEE EL MENSAJE Y HAZLE CASO.\n    \n    Si a\u00fan no:\n    - apaga tu ordenador, \n    - date una vuelta, \n    - enciendelo otra vez, \n    - abre este notebook y \n    - leelo todo de nuevo. \n    Todos nos merecemos una segunda oportunidad. Tambi\u00e9n t\u00fa.\n    \"\"\"\n    if df_to_submit.shape == sample.shape:\n        if df_to_submit.columns.all() == sample.columns.all():\n            if df_to_submit.id.all() == sample.id.all():\n                print(\"You're ready to submit!\")\n                submission.to_csv(\"submission.csv\", index = False) #muy importante el index = False\n                urllib.request.urlretrieve(\"https:\/\/i.kym-cdn.com\/photos\/images\/facebook\/000\/747\/556\/27a.jpg\", \"gfg.png\")     \n                img = Image.open(\"gfg.png\")\n                img.show()   \n            else:\n                print(\"Check the ids and try again\")\n        else:\n            print(\"Check the names of the columns and try again\")\n    else:\n        print(\"Check the number of rows and\/or columns and try again\")\n        print(\"\\nMensaje secreto de Clara: No me puedo creer que despu\u00e9s de todo este notebook hayas hecho alg\u00fan cambio en las filas de `diamonds_test.csv`. Lloro.\")\nchequeator(submission)","8dc5bb84":"## Una vez listo el modelo, toca predecir con el dataset de predicci\u00f3n ","ee8d27c3":"### 4. Mete tus predicciones en un dataframe. \n\nEn este caso, la **MISMA** forma que `sample_submission.csv`. ","bdc02aed":"![image](competi.png)","d9510522":"### 1. Entrena dicho modelo con TODOS tus datos de train, esto es con `diamonds_train.csv` al completo.\n\n**CON LAS TRANSFORMACIONES QUE LE HAYAS REALIZADO A `X` INCLU\u00cdDAS.**\n\nV\u00e9ase:\n- Estandarizaci\u00f3n\/Normalizaci\u00f3n\n- Eliminaci\u00f3n de Outliers\n- Eliminaci\u00f3n de columnas\n- Creaci\u00f3n de columnas nuevas\n- Gesti\u00f3n de valores nulos\n- Y un largo etc\u00e9tera de t\u00e9cnicas que como Data Scientist hayas considerado las mejores para tu dataset.","b1c91f08":"#### Aqu\u00ed encontrar\u00e1s todo lo que necesitas saber: https:\/\/www.kaggle.com\/t\/ab8726f0cfc84544abbae69a6be88071","7b3a8e1a":"## Importaci\u00f3n de clase y c\u00f3digo de funciones de transformaci\u00f3n de los valores de las columnas categ\u00f3ricas","861e4e19":"**Importante:**\n\n   - Si quitas columnas o creas columnas nuevas a partir de otras, o cualquier modificaci\u00f3n column-wise tendr\u00e1s que aplicarlo al dataset de `diamond_test.csv` de cara a hacer la predicci\u00f3n.\n   - Si por lo contrario, decides por ejemplo, quitar los outliers o hacer un `dropna()`, o cualquier modificaci\u00f3n row-wise eso NO PODR\u00c1S (ni debes) aplicarlo al dataset de `diamond_test.csv` de cara a hacer la predicci\u00f3n. \u00bfPor qu\u00e9? Porque si el conjunto de test tiene 50 observaciones (filas) la predicci\u00f3n se espera que tenga 50 filas.","c7ec021f":"### 2. Carga los datos de `diamonds_test.csv` para predecir.\n\n**\u00bfDe d\u00f3nde saco `diamonds_test.csv`?**","015dab1b":"# Crear las variables de uso para el entrenamiento del modelo","4791f48b":"### 3. Asignar el modelo (vac\u00edo) a una variable\nAqu\u00ed meter\u00edais los par\u00e1metros. \n\n**Consejo**: Usa GridSearch y vu\u00e9lvete loca o loco probando modelos e hiperpar\u00e1metros.","baa92b97":"### Manipular las variables categ\u00f3ricas del dataset para hacerlas explotable num\u00e9ricamente","3deffadc":"**\u00a1PERO! Para subir a Kaggle la predicci\u00f3n, \u00e9sta tendr\u00e1 que tener una forma espec\u00edfica y no valdr\u00e1 otra.**\n\nEn este caso, la **MISMA** forma que `sample_submission.csv`. ","2406cea9":"### 5. P\u00e1sale el CHEQUEATOR para comprobar que efectivamente est\u00e1 listo para subir a Kaggle.","1ab78e35":"Siempre hay tiempo para una historia:\nhttps:\/\/catboost.ai\/docs\/concepts\/python-reference_catboostregressor.html"}}