{"cell_type":{"d8648e2c":"code","c60d7cdd":"code","23b0a49c":"code","7582e9d9":"code","f866bdb6":"code","9c2eff60":"code","c78e6358":"code","ce64fd3e":"code","c22a9e43":"code","29a41680":"code","c23a2f34":"code","65682b56":"code","26185104":"code","4bbcc5b4":"code","813e57e5":"code","2f4ecbd8":"code","3bdadd40":"code","9b14bfc3":"code","341ae586":"code","4b852319":"markdown","084251f2":"markdown"},"source":{"d8648e2c":"import pandas as pd\nimport numpy as np\nfrom scipy.stats import ttest_ind\nimport os","c60d7cdd":"for dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","23b0a49c":"# Loading Data\nall_homes_at_a_city_level = pd.read_csv(\"\/kaggle\/input\/zillow-all-homes-data\/City_Zhvi_AllHomes.csv\")\ngdp_over_time = pd.read_excel(\"\/kaggle\/input\/zillow-all-homes-data\/gdplev.xls\", skiprows=7)\nuniverstiy_towns = pd.read_csv(\"\/kaggle\/input\/zillow-all-homes-data\/university_towns.txt\", sep=\"\\t\", header=None)","7582e9d9":"all_homes_at_a_city_level.head()","f866bdb6":"gdp_over_time.head()","9c2eff60":"universtiy_towns.head()","c78e6358":"# We will use this dictionary to map state names to two letter acronyms\nstates = {'OH': 'Ohio', 'KY': 'Kentucky', 'AS': 'American Samoa', 'NV': 'Nevada', 'WY': 'Wyoming', 'NA': 'National', 'AL': 'Alabama', 'MD': 'Maryland', 'AK': 'Alaska', 'UT': 'Utah', 'OR': 'Oregon', 'MT': 'Montana', 'IL': 'Illinois', 'TN': 'Tennessee', 'DC': 'District of Columbia', 'VT': 'Vermont', 'ID': 'Idaho', 'AR': 'Arkansas', 'ME': 'Maine', 'WA': 'Washington', 'HI': 'Hawaii', 'WI': 'Wisconsin', 'MI': 'Michigan', 'IN': 'Indiana', 'NJ': 'New Jersey', 'AZ': 'Arizona', 'GU': 'Guam', 'MS': 'Mississippi', 'PR': 'Puerto Rico', 'NC': 'North Carolina', 'TX': 'Texas', 'SD': 'South Dakota', 'MP': 'Northern Mariana Islands', 'IA': 'Iowa', 'MO': 'Missouri', 'CT': 'Connecticut', 'WV': 'West Virginia', 'SC': 'South Carolina', 'LA': 'Louisiana', 'KS': 'Kansas', 'NY': 'New York', 'NE': 'Nebraska', 'OK': 'Oklahoma', 'FL': 'Florida', 'CA': 'California', 'CO': 'Colorado', 'PA': 'Pennsylvania', 'DE': 'Delaware', 'NM': 'New Mexico', 'RI': 'Rhode Island', 'MN': 'Minnesota', 'VI': 'Virgin Islands', 'NH': 'New Hampshire', 'MA': 'Massachusetts', 'GA': 'Georgia', 'ND': 'North Dakota', 'VA': 'Virginia'}","ce64fd3e":"def get_list_of_university_towns():\n    \"\"\"Returns a DataFrame of towns and the states they are in from the \n    university_towns.txt list. The format of the DataFrame should be:\n    DataFrame( [ [\"Michigan\", \"Ann Arbor\"], [\"Michigan\", \"Yipsilanti\"] ], \n    columns=[\"State\", \"RegionName\"]  )\n    \n    The following cleaning needs to be done:\n\n    1. For \"State\", removing characters from \"[\" to the end.\n    2. For \"RegionName\", when applicable, removing every character from \" (\" to the end.\"\"\"\n    \n    with open('\/kaggle\/input\/zillow-all-homes-data\/university_towns.txt') as f:\n        unviersity_towns = f.readlines()\n    data = []\n    for line in unviersity_towns:\n        uni = line[:-1] # Fallback in case it isn't a state and doesn't have a \" (\"\n        if line[-7:] == '[edit]\\n':\n            state = line[:-7]\n            continue\n        elif '(' in line:\n            uni = line[:line.find('(') - 1]\n\n        data.append((state, uni))\n    \n    return pd.DataFrame(data=data, columns=['State', 'RegionName'])","c22a9e43":"get_list_of_university_towns().head()","29a41680":"def get_recession_start():\n    '''Returns the year and quarter of the recession start time as a \n    string value in a format such as 2005q3'''    \n    df = gdp_over_time[['Unnamed: 4','Unnamed: 5']]\n    df.columns = ['Quarter','GDP']\n    # We are only going to look for data from the first quarter of 2000 onward so :\n    df = df.iloc[212:]\n    recession_start = \"\"\n    recession = list()\n    recession_start = []\n    for i in range(len(df) - 4):\n        if ((df.iloc[i][1] > df.iloc[i+1][1]) & (df.iloc[i+1][1] > df.iloc[i+2][1])):\n            recession_start.append(df.iloc[i][0])\n    return recession_start[0]","c23a2f34":"get_recession_start()","65682b56":"def get_recession_end():\n    '''Returns the year and quarter of the recession end time as a \n    string value in a format such as 2005q3'''\n    df = gdp_over_time[['Unnamed: 4','Unnamed: 5']]\n    df.columns = ['Quarter','GDP']\n    # We are only going to look for data from the first quarter of 2000 onward so :\n    df = df.iloc[212:]\n    recession_end = \"\"\n    for i in range(len(df)-4):\n        if((df.iloc[i][1] > df.iloc[i+1][1]) & (df.iloc[i+1][1] > df.iloc[i+2][1]) & (df.iloc[i+3][1] > df.iloc[i+2][1]) & (df.iloc[i+4][1] > df.iloc[i+3][1])):\n            recession_end = df.iloc[i+4][0]\n    return recession_end","26185104":"get_recession_end()","4bbcc5b4":"def get_recession_bottom():\n    '''Returns the year and quarter of the recession bottom time as a \n    string value in a format such as 2005q3'''\n    df = gdp_over_time[['Unnamed: 4','Unnamed: 5']]\n    df.columns = ['Quarter','GDP']\n    # We are only going to look for data from the first quarter of 2000 onward so :\n    df = df.iloc[212:]\n    recession_bottom = \"\"\n    recession = list()\n    for i in range(len(df)-4):\n        if((df.iloc[i][1] > df.iloc[i+1][1]) & (df.iloc[i+1][1] > df.iloc[i+2][1]) & (df.iloc[i+3][1] > df.iloc[i+2][1]) & (df.iloc[i+4][1] > df.iloc[i+3][1])):\n            recession.append([df.iloc[i][0], df.iloc[i+1][0], df.iloc[i+2][0], df.iloc[i+3][0], df.iloc[i+4][0]])\n            # We will select the element with index 2 in the recession array since it represents the bottom of the recession.\n            recession_bottom = recession[0][2]\n    return recession_bottom","813e57e5":"get_recession_bottom()","2f4ecbd8":"def convert_housing_data_to_quarters():\n    '''Converts the housing data to quarters and returns it as mean \n    values in a dataframe. This dataframe should be a dataframe with\n    columns for 2000q1 through 2016q3, and should have a multi-index\n    in the shape of [\"State\",\"RegionName\"].\n    '''\n    \n    # Getting rid of the RegionID, and the years before the year 2000\n    df = all_homes_at_a_city_level.drop(all_homes_at_a_city_level.columns[[0] + list(range(3,51))], axis=1)\n    # Replacing all the states names\n    df[\"State\"] = df[\"State\"].replace(states)\n    df.set_index([\"State\", \"RegionName\"], inplace=True)\n   \n    # Createing a new DataFrame that contains means for each quarter from 2000 to 2015.\n    new_data = pd.DataFrame()\n    # We are not adding the year 2016 because we don't have all the 4 quarters data available.\n    for year in range(2000,2016):\n        new_data[str(year) + 'q1'] = df[[str(year) + '-01', str(year) + '-02', str(year) + '-03']].mean(axis = 1)\n        new_data[str(year) + 'q2'] = df[[str(year) + '-04', str(year) + '-05', str(year) + '-06']].mean(axis = 1)\n        new_data[str(year) + 'q3'] = df[[str(year) + '-07', str(year) + '-08', str(year) + '-09']].mean(axis = 1)\n        new_data[str(year) + 'q4'] = df[[str(year) + '-10', str(year) + '-11', str(year) + '-12']].mean(axis = 1)\n    # Now adding the year 2016's remaining quarters since they have not been added in the previous loop\n    new_data[str(2016) + 'q1'] = df[[str(2016) + '-01', str(year) + '-02', str(2016) + '-03']].mean(axis = 1)\n    new_data[str(2016) + 'q2'] = df[[str(2016) + '-04', str(year) + '-05', str(2016) + '-06']].mean(axis = 1)\n    new_data[str(2016) + 'q3'] = df[[str(2016) + '-07', str(year) + '-08']].mean(axis = 1)\n    return new_data","3bdadd40":"#convert_housing_data_to_quarters().loc[\"Texas\"].loc[\"Austin\"].loc[\"2010q3\"]\nconvert_housing_data_to_quarters()","9b14bfc3":"def run_ttest():\n    '''First creates new data showing the decline or growth of housing prices\n    between the recession start and the recession bottom. Then runs a ttest\n    comparing the university town values to the non-university towns values, \n    return whether the alternative hypothesis (that the two groups are the same)\n    is true or not as well as the p-value of the confidence. \n    \n    Returns the tuple (different, p, better) where different=True if the t-test is\n    True at a p<0.01 (we reject the null hypothesis), or different=False if \n    otherwise (we cannot reject the null hypothesis). The variable p should\n    be equal to the exact p value returned from scipy.stats.ttest_ind(). The\n    value for better should be either \"university town\" or \"non-university town\"\n    depending on which has a lower mean price ratio (which is equivilent to a\n    reduced market loss).'''\n    \n     \n    unitowns = get_list_of_university_towns()\n    bottom = get_recession_bottom()\n    start = get_recession_start()\n    housing_data = convert_housing_data_to_quarters()\n    # Selecting the quarter before the recession\n    bstart = housing_data.columns[housing_data.columns.get_loc(start) - 1]\n        \n    \"\"\" \n    The formula for price ratio is :\n    price_ratio=quarter_before_recession\/recession_bottom\n    \"\"\"\n    housing_data['ratio'] =  housing_data[bstart] \/ housing_data[bottom]\n    housing_data = housing_data[[bottom, bstart, 'ratio']]\n    housing_data = housing_data.reset_index()\n    \n    unitowns_hdata = pd.merge(housing_data,unitowns,how='inner',on=['State','RegionName'])\n    unitowns_hdata['IsUniversityTown'] = True\n    housing_data_complete = pd.merge(housing_data, unitowns_hdata, how='outer', on=['State','RegionName',bottom, bstart, 'ratio'])\n    housing_data_complete['IsUniversityTown'] = housing_data_complete['IsUniversityTown'].fillna(False)\n    \n    university_towns = housing_data_complete[housing_data_complete['IsUniversityTown'] == True]\n    non_universtiy_towns = housing_data_complete[housing_data_complete['IsUniversityTown'] == False]\n    \n    # Executing the Hypothesis test all while dropping the na values\n    t,p = ttest_ind(university_towns['ratio'].dropna(), non_universtiy_towns['ratio'].dropna())\n    # We are comparing the p value to 0.01 in our case\n    different_prices = True if p<0.01 else False\n    # Better contains the tag of the family of towns that have the better prices\n    better_family = \"university town\" if university_towns['ratio'].mean() < non_universtiy_towns['ratio'].mean() else \"non-university town\"\n    \n    return (different_prices, p, better_family)","341ae586":"run_ttest()","4b852319":"# Hypothesis Testing on House Prices Of University Towns\n\nIn this notebook we will see which type of town is less affected by recessions : **_University Towns_** or **_Non University Towns_**\n\nDefinitions:\n* A _quarter_ is a specific three month period, Q1 is January through March, Q2 is April through June, Q3 is July through September, Q4 is October through December.\n* A _recession_ is defined as starting with two consecutive quarters of GDP decline, and ending with two consecutive quarters of GDP growth.\n* A _recession bottom_ is the quarter within a recession which had the lowest GDP.\n* A _university town_ is a city which has a high percentage of university students compared to the total population of the city.\n\n**Hypothesis**: University towns have their mean housing prices less effected by recessions. Run a t-test to compare the ratio of the mean price of houses in university towns the quarter before the recession starts compared to the recession bottom. (`price_ratio=quarter_before_recession\/recession_bottom`)\n\nThe following data files that will be used:\n* From the [Zillow research data site](http:\/\/www.zillow.com\/research\/data\/) there is housing data for the United States. In particular the datafile for [all homes at a city level](http:\/\/files.zillowstatic.com\/research\/public\/City\/City_Zhvi_AllHomes.csv), ```City_Zhvi_AllHomes.csv```, has median home sale prices at a fine grained level.\n* From the Wikipedia page on college towns is a list of [university towns in the United States](https:\/\/en.wikipedia.org\/wiki\/List_of_college_towns#College_towns_in_the_United_States) which has been copy and pasted into the file ```university_towns.txt```.\n* From Bureau of Economic Analysis, US Department of Commerce, the [GDP over time](http:\/\/www.bea.gov\/national\/index.htm#gdp) of the United States in current dollars (we will use the chained value in 2009 dollars), in quarterly intervals, in the file ```gdplev.xls```. We will also only look at GDP data from the first quarter of 2000 onward.","084251f2":"The mean test returns a p value of 0.00272 which is **lower** than our critical value \u03b1 = 0.01  for this test, so the two means are different.\n\nWe can now say that the **university towns** house prices are **less** affected by the recession."}}