{"cell_type":{"05305c70":"code","245d812e":"code","11ee6381":"code","637dd099":"code","eff95ff6":"code","59ba520f":"code","e5dfd18d":"code","ce35e217":"code","7e46c7bd":"code","c9e85a81":"code","87ff5695":"code","09f2ad6d":"code","99ae78f7":"code","64e7623d":"code","2ab6e003":"code","938bd820":"code","53c88fcc":"code","4c21e7ac":"code","acdcf9ce":"code","5e3fbb3c":"code","e200a613":"code","6ec3f530":"code","e0ba5347":"code","7b4cd421":"code","2bd5e73a":"code","cbc4ff0d":"code","80b499c4":"code","b7b61be4":"code","d88ba7bc":"code","68ccfe86":"code","55d9ad95":"code","3acf0daf":"code","e9ec17d9":"code","f2f5598d":"code","f0c65430":"code","36caac0e":"code","9b5b03e2":"code","f1f7954a":"code","34f975c4":"code","d6e1f519":"code","9e6fbe52":"code","a95e7352":"code","fdc14bd3":"code","61ba4b84":"code","2a5875f4":"code","12c0975c":"code","6de210cf":"code","878cf17f":"code","14beecbc":"code","3e758a58":"code","6d454e33":"code","7e8e2610":"code","e6c01542":"code","d99b6709":"code","c7d1905f":"code","3ed784db":"code","7551afaa":"code","1062ab89":"code","20ee74de":"code","dc29fc39":"code","8fc2a9aa":"code","bafd3522":"code","cc79cc90":"code","9571cf21":"code","75706a58":"code","a0e958e3":"code","a1ca35df":"code","7e74c632":"code","99b106b3":"code","81920adf":"code","5dda58c5":"code","46ee955b":"code","60c25f55":"code","801fafa5":"code","0d1d496c":"code","fe022345":"code","8bdb18d0":"code","1fed92f2":"code","1ee4c8f3":"code","21a552e8":"code","910e32eb":"code","3bdb7e8d":"markdown","ce9b6408":"markdown","11106253":"markdown","0e23998e":"markdown","880dbd6a":"markdown","52373044":"markdown","f7a0e25e":"markdown","3f24ba03":"markdown","0c34ef85":"markdown","5e42bcf8":"markdown","6744ada2":"markdown","f5afcd20":"markdown","d36da96f":"markdown","27a75e14":"markdown","f35ff24a":"markdown","d3c4a0d9":"markdown","c043c09c":"markdown","2191b338":"markdown","33eb39ea":"markdown","b6d3a378":"markdown","28a8818b":"markdown","d5c6cb73":"markdown","f7ee28f7":"markdown","dbb2c4f9":"markdown","ba2284e4":"markdown","c79e4bd9":"markdown","9d8989d9":"markdown","5408cdfb":"markdown","fefe409f":"markdown","1bd3b382":"markdown","f34c7fc0":"markdown","db40e9c8":"markdown","42db09c6":"markdown","e61e9843":"markdown","f3a85f75":"markdown","a332dfd5":"markdown","09ae3902":"markdown","27f0c3e1":"markdown","138b56b5":"markdown","2be67b34":"markdown","0b22d0af":"markdown","4b39a9e3":"markdown","850fb611":"markdown","f8eb3682":"markdown","56815dea":"markdown","97f035cb":"markdown","8f6f76c6":"markdown","df828203":"markdown","8ffcfb54":"markdown","43054a56":"markdown","46999307":"markdown","0293b0c2":"markdown","651fd9ae":"markdown","5fe28cee":"markdown","4929a987":"markdown","c11ab87b":"markdown","6c3d2731":"markdown","66664ca2":"markdown","e659f5ef":"markdown","fb64b12a":"markdown"},"source":{"05305c70":"# Importing libraries\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\nfrom scipy.stats import norm, skew \nfrom scipy import stats\nimport warnings\nwarnings.filterwarnings(\"ignore\")","245d812e":"#Read data set\nX = pd.read_excel('..\/input\/flight-fare-prediction-mh\/Data_Train.xlsx',parse_dates = ['Date_of_Journey'] )\ntest = pd.read_excel('..\/input\/flight-fare-prediction-mh\/Test_set.xlsx',parse_dates = ['Date_of_Journey'] )","11ee6381":"#Read first few rows of the data set\nX.head()","637dd099":"#Rows and columns of the data set\nX.shape","eff95ff6":"#Summary of numerical variables\nX.describe()","59ba520f":"#Summary of categorical variables\nX.describe(include = ['object'])","e5dfd18d":"X.dropna(inplace=True)","ce35e217":"X.isnull().any()","7e46c7bd":"#Combining test set and tarining set to conduct features engineering\nall_data = pd.concat([X,test],axis = 0)\n\n#X = all_data.iloc[:10682]\n#test = all_data.iloc[10682:]","c9e85a81":"all_data.Total_Stops.value_counts()","87ff5695":"no_stops = {'1 stop':1,'non-stop':0,'2 stops':2,'3 stops':3,'4 stops':4}\nall_data.Total_Stops = all_data.Total_Stops.map(no_stops)","09f2ad6d":"all_data.Total_Stops.value_counts()","99ae78f7":"all_data.Date_of_Journey.head()","64e7623d":"#Add journey month and day of week, then drop the original date of jounery column\nall_data['Journey_month'] = all_data.Date_of_Journey.dt.month\nall_data['Journey_day_of_week'] = all_data.Date_of_Journey.dt.dayofweek\nall_data['Journey_year'] = all_data.Date_of_Journey.dt.year\nall_data['Journey_day'] = all_data.Date_of_Journey.dt.day\nall_data.drop('Date_of_Journey',axis = 1,inplace = True)","2ab6e003":"all_data['Journey_year'].value_counts()","938bd820":"all_data.drop('Journey_year',axis = 1,inplace = True)","53c88fcc":"all_data[['Dep_Time', 'Arrival_Time']].head()","4c21e7ac":"#add new 4 features in form of hour and mintues of departure time and arrival time.\nall_data[\"Dep_hour\"],all_data[\"Arr_hour\"]= pd.to_datetime(all_data['Dep_Time']).dt.hour,pd.to_datetime(all_data['Arrival_Time']).dt.hour\n\nall_data[\"Dep_min\"],all_data[\"Arr_min\"]= pd.to_datetime(all_data['Dep_Time']).dt.minute,pd.to_datetime(all_data['Arrival_Time']).dt.minute","acdcf9ce":"#Drop the original depoarture time and arrival time columns\nall_data.drop(['Arrival_Time'],axis =1 ,inplace =True)\nall_data.drop(['Dep_Time'],axis =1,inplace =True)","5e3fbb3c":"all_data.head()","e200a613":"all_data.Duration.head()","6ec3f530":"d = list(all_data.Duration)\nduration_hour = []\nduration_min = []\n\nfor time in d:\n    if len(time.split()) == 2: #cell with both hour and mintues data\n        hour = time.split()[0].rsplit('h')[0]\n        mintues = time.split()[1].rsplit('m')[0]\n        duration_hour.append(hour)\n        duration_min.append(mintues)\n    else: \n        #data with only hour or mintue information.\n        if 'h' in time.split()[0]:\n            hour =  time.split()[0].rsplit('h')[0]\n            duration_hour.append(int(hour))\n            duration_min.append(0) \n            # 0 mintues in there are no mintues data\n        elif 'm'in time.split()[0]:\n            mintues =  time.split()[0].rsplit('m')[0]\n            duration_hour.append(0)\n            duration_min.append(int(mintues))","e0ba5347":"#Check whether the length of the two lists we just created is same with the data set.\nlen(duration_hour) == len(duration_min ) == len(all_data)","7b4cd421":"all_data['duration_hour'] = pd.DataFrame(duration_hour).astype('int32')\nall_data['duration_min'] = pd.DataFrame(duration_min).astype('int32')","2bd5e73a":"all_data.drop(['Duration'],axis = 1, inplace = True)","cbc4ff0d":"all_data['duration_min'].value_counts()","80b499c4":"all_data['duration_hour'].value_counts()","b7b61be4":"all_data.head()","d88ba7bc":"all_data.head()","68ccfe86":"#Check again to ensure there are no missing data\nall_data.isnull().any()","55d9ad95":"X = all_data.iloc[:10682]\ntest = all_data.iloc[10682:]","3acf0daf":"sns.distplot(X['Price'] , fit=norm);\n\n# Get the fitted parameters used by the function\n(mu, sigma) = norm.fit(X['Price'])\nprint( '\\n mu = {:.2f} and sigma = {:.2f}\\n'.format(mu, sigma))\n\n#Now plot the distribution\nplt.legend(['Normal dist. ($\\mu=$ {:.2f} and $\\sigma=$ {:.2f} )'.format(mu, sigma)],\n            loc='best')\nplt.ylabel('Frequency')\nplt.title('Price distribution')\n\n#Get also the QQ-plot\nfig = plt.figure()\nres = stats.probplot(X['Price'], plot=plt)\nplt.show()","e9ec17d9":"#We use the numpy fuction log1p which  applies log(1+x) to all elements of the column\nX[\"Price\"] = np.log1p(X[\"Price\"])\n\n#Check the new distribution \nsns.distplot(X[\"Price\"] , fit=norm);\n\n# Get the fitted parameters used by the function\n(mu, sigma) = norm.fit(X[\"Price\"])\nprint( '\\n mu = {:.2f} and sigma = {:.2f}\\n'.format(mu, sigma))\n\n#Now plot the distribution\nplt.legend(['Normal dist. ($\\mu=$ {:.2f} and $\\sigma=$ {:.2f} )'.format(mu, sigma)],\n            loc='best')\nplt.ylabel('Frequency')\nplt.title('Price distribution')\n\n#Get also the QQ-plot\nfig = plt.figure()\nres = stats.probplot(X[\"Price\"], plot=plt)\nplt.show()","f2f5598d":"#list of categorical and numerical variables\ncat_cols = X.dtypes =='object'\ncat_cols = list(cat_cols[cat_cols].index)\nnum_cols = X.dtypes != 'object'\nnum_cols = list(num_cols[num_cols].index)\nnum_cols.remove('Price')","f0c65430":"def cat_boxplot(col,target,train,a=8,b=6):\n    f, ax = plt.subplots(figsize=(a, b))\n    fig = sns.boxplot(x=col, y=str(target), data=train)\n    #fig.axis(xmin=0, xmax=x);","36caac0e":"#Airline and log Price\ncat_boxplot('Price',cat_cols[0],X,)","9b5b03e2":"cat_boxplot('Price',cat_cols[1],X)","f1f7954a":"cat_boxplot('Price',cat_cols[2],X)","34f975c4":"cat_boxplot('Price',cat_cols[3],X,a = 15,b= 15)","d6e1f519":"cat_boxplot('Price',cat_cols[4],X)","9e6fbe52":"def num_plot(col,target,train,a=8,b=6):\n    f, ax = plt.subplots(figsize=(a, b))\n    fig = sns.scatterplot(x=col, y=str(target), data=train)\n    #fig.axis(ymin=0, ymax=x);","a95e7352":"num_cols","fdc14bd3":"for i in range(len(num_cols)):\n    num_plot(num_cols[i],'Price',X)","61ba4b84":"for i in range(len(num_cols)):\n    cat_boxplot(num_cols[i],'Price',X)","2a5875f4":"X.head()","12c0975c":"import category_encoders as ce\n\ncat_features = ['Additional_Info','Route']\n# Create the encoder\ntarget_enc = ce.CatBoostEncoder(cols=cat_features)\ntarget_enc.fit(X[cat_features], X['Price'])\n\n# Transform the features, rename columns with _cb suffix, and join to dataframe\nX_CBE = X.join(target_enc.transform(X[cat_features]).add_suffix('_cb'))\ntest_CBE = test.join(target_enc.transform(test[cat_features]).add_suffix('_cb'))","6de210cf":"test_CBE['Route_cb'].value_counts()","878cf17f":"X = X_CBE.copy()\ntest = test_CBE.copy()\nX.drop(['Route', \"Additional_Info\"], axis = 1, inplace = True)\ntest.drop(['Route', \"Additional_Info\"], axis = 1, inplace = True)","14beecbc":"#Function to create a data frame with number and percentage of missing data in a data frame\ndef missing_to_df(df):\n    #Number and percentage of missing data in training data set for each column\n    total_missing_df = df.isnull().sum().sort_values(ascending =False)\n    percent_missing_df = (df.isnull().sum()\/df.isnull().count()*100).sort_values(ascending=False)\n    missing_data_df = pd.concat([total_missing_df, percent_missing_df], axis=1, keys=['Total', 'Percent'])\n    return missing_data_df\nmissing_df = missing_to_df(X)\nmissing_df[missing_df['Total'] > 0]","3e758a58":"#Correlation map to see how features are correlated with Price\ncorrmat = X.corr()\nplt.subplots(figsize=(12,9))\nsns.heatmap(corrmat, vmax=0.9, square=True)","6d454e33":"all_data = pd.concat([X,test],axis = 0)\nall_data = pd.get_dummies(all_data)\nall_data.shape","7e8e2610":"X = all_data.iloc[:10682]\ntest = all_data.iloc[10682:]","e6c01542":"test = test.drop('Price',axis = 1)","d99b6709":"def get_data_splits(dataframe, valid_fraction=0.1):\n    valid_fraction = 0.1\n    valid_size = int(len(dataframe) * valid_fraction)\n\n    train = dataframe[:-valid_size * 2]\n    # valid size == test size, last two sections of the data\n    valid = dataframe[-valid_size * 2:-valid_size]\n    test = dataframe[-valid_size:]\n    \n    return train, valid, test","c7d1905f":"from sklearn.feature_selection import SelectKBest, f_classif\n\nsel_train, valid, _ = get_data_splits(X)\nfeature_cols = list(sel_train.columns)\nfeature_cols.remove('Price')\n# Keep 10 features\nselector = SelectKBest(f_classif, k=30)\n\nX_new = selector.fit_transform(sel_train[feature_cols], sel_train['Price'])","3ed784db":"# Get back the features we've kept, zero out all other features\nselected_features = pd.DataFrame(selector.inverse_transform(X_new), \n                                 index=sel_train.index, \n                                 columns=feature_cols)","7551afaa":"# Dropped columns have values of all 0s, so var is 0, drop them\nselected_columns = selected_features.columns[selected_features.var() != 0]\n\n# Get the valid dataset with the selected features.\nselected_columns","1062ab89":"from sklearn.linear_model import ElasticNet, Lasso,  BayesianRidge, LassoLarsIC\nfrom sklearn.ensemble import RandomForestRegressor,  GradientBoostingRegressor\nfrom sklearn.kernel_ridge import KernelRidge\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import RobustScaler\nfrom sklearn.base import BaseEstimator, TransformerMixin, RegressorMixin, clone\nfrom sklearn.model_selection import KFold, cross_val_score, train_test_split\nfrom sklearn.metrics import mean_squared_error,mean_absolute_error,r2_score\nimport lightgbm as lgb","20ee74de":"y = X.Price","dc29fc39":"X = X.drop('Price',axis = 1)","8fc2a9aa":"#Validation function\nn_folds = 5\n\ndef rmsle_cv(model):\n    kf = KFold(n_folds, shuffle=True, random_state=42).get_n_splits(X.values)\n    rmse= np.sqrt(-cross_val_score(model, X.values, \n                                   y.values, \n                                   scoring=\"neg_mean_squared_error\", cv = kf))\n    return(rmse)","bafd3522":"lasso = make_pipeline(RobustScaler(), Lasso(alpha =0.0005, random_state=1))","cc79cc90":"ENet = make_pipeline(RobustScaler(), ElasticNet(alpha=0.0005, l1_ratio=.9, random_state=3))","9571cf21":"KRR = KernelRidge(alpha=0.6, kernel='polynomial', degree=2, coef0=2.5)","75706a58":"GBoost = GradientBoostingRegressor(n_estimators=1500, learning_rate=0.05,\n                                   max_depth=4, max_features='sqrt',\n                                   min_samples_leaf=15, min_samples_split=10, \n                                   loss='huber', random_state =5)","a0e958e3":"model_lgb = lgb.LGBMRegressor(objective='regression',num_leaves=5,\n                              learning_rate=0.05, n_estimators=720,\n                              max_bin = 55, bagging_fraction = 0.8,\n                              bagging_freq = 5, feature_fraction = 0.2319,\n                              feature_fraction_seed=9, bagging_seed=9,\n                              min_data_in_leaf =6, min_sum_hessian_in_leaf = 11)","a1ca35df":"lasso_score = rmsle_cv(lasso)\nprint(\"\\nLasso score: {:.4f} ({:.4f})\\n\".format(lasso_score.mean(), lasso_score.std()))","7e74c632":"enet_score = rmsle_cv(ENet)\nprint(\"ElasticNet score: {:.4f} ({:.4f})\\n\".format(enet_score.mean(), enet_score.std()))","99b106b3":"krr_score = rmsle_cv(KRR)\nprint(\"Kernel Ridge score: {:.4f} ({:.4f})\\n\".format(krr_score.mean(), krr_score.std()))","81920adf":"gboost_score = rmsle_cv(GBoost)\nprint(\"Gradient Boosting score: {:.4f} ({:.4f})\\n\".format(gboost_score.mean(), gboost_score.std()))","5dda58c5":"lgb_score = rmsle_cv(model_lgb)\nprint(\"LGBM score: {:.4f} ({:.4f})\\n\" .format(lgb_score.mean(), lgb_score.std()))","46ee955b":"models = pd.DataFrame({\n    'Model': ['LASSO', 'ENet', 'KRR', \n              'GBoost', 'lgb'],#,'XGBoost'],\n    'Mean_Score': [lasso_score.mean(), enet_score.mean(), krr_score.mean(), \n              gboost_score.mean(), lgb_score.mean()]})\n#xgb_score.mean()\nmodels.sort_values(by='Mean_Score', ascending=True)","60c25f55":"#Validation function\nn_folds = 5\n\ndef rmsle_cv(model):\n    kf = KFold(n_folds, shuffle=True, random_state=42).get_n_splits(X[selected_columns].values)\n    rmse= np.sqrt(-cross_val_score(model, X[selected_columns].values, \n                                   y.values, \n                                   scoring=\"neg_mean_squared_error\", cv = kf))\n    return(rmse)","801fafa5":"lasso_score = rmsle_cv(lasso)\nprint(\"\\nLasso score: {:.4f} ({:.4f})\\n\".format(lasso_score.mean(), lasso_score.std()))\nenet_score = rmsle_cv(ENet)\nprint(\"ElasticNet score: {:.4f} ({:.4f})\\n\".format(enet_score.mean(), enet_score.std()))\nkrr_score = rmsle_cv(KRR)\nprint(\"Kernel Ridge score: {:.4f} ({:.4f})\\n\".format(krr_score.mean(), krr_score.std()))\ngboost_score = rmsle_cv(GBoost)\nprint(\"Gradient Boosting score: {:.4f} ({:.4f})\\n\".format(gboost_score.mean(), gboost_score.std()))\nlgb_score = rmsle_cv(model_lgb)\nprint(\"LGBM score: {:.4f} ({:.4f})\\n\" .format(lgb_score.mean(), lgb_score.std()))","0d1d496c":"models_sel = pd.DataFrame({\n    'Model': ['LASSO', 'ENet', 'KRR', \n              'GBoost', 'lgb'], #'XGBoost',\n    'Mean_Score': [lasso_score.mean(), enet_score.mean(), krr_score.mean(), \n              gboost_score.mean(), lgb_score.mean()]})  #xgb_score.mean(),\nmodels_sel.sort_values(by='Mean_Score', ascending=True)","fe022345":"def rmsle(y, y_pred):\n    return np.sqrt(mean_squared_error(y, y_pred))","8bdb18d0":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 42)\nGBoost.fit(X_train, y_train)\ny_pred = GBoost.predict(X_test)\nprint('MAE:', mean_absolute_error(np.expm1(y_test), np.expm1(y_pred)))\nprint('MSE:', mean_squared_error(np.expm1(y_test),np.expm1(y_pred)))\nprint('RMSE:', np.sqrt(mean_squared_error(np.expm1(y_test),np.expm1(y_pred))))","1fed92f2":"print('R square error:', r2_score(np.expm1(y_test),np.expm1(y_pred)))","1ee4c8f3":"GBoost = GradientBoostingRegressor(n_estimators=1500, learning_rate=0.05,\n                                   max_depth=20, max_features='sqrt',\n                                   min_samples_leaf=15, min_samples_split=10, \n                                   loss='huber', random_state =5)","21a552e8":"GBoost.fit(X,y)\ngboost_pred = (GBoost.predict(X))\ny_pred = np.expm1(GBoost.predict(test))\nprint('mean_squared_error: ',(rmsle(np.expm1(y),np.expm1(gboost_pred))))","910e32eb":"submission = pd.DataFrame({'Price':y_pred})\nsubmission.head()","3bdb7e8d":"**Data Summary**","ce9b6408":"No missing value in features columns (price is target, the missing values come from test set).\n\nSeparate the data set back into orginial form.","11106253":"There are only one value in Journey_year, we can drop it.","0e23998e":"Date_of_Journey can be separate into month and day of week.","880dbd6a":"Dep_Time and Arrival_Time need to convert into date time formate before processing it.","52373044":"**Duration**","f7a0e25e":"# **Features engineering (Part 1)**","3f24ba03":"Let's see the performance of these base models.","0c34ef85":"Cross_val_score function of Sklearn is used. ","5e42bcf8":"Taking [Stacked Regressions : Top 4% on LeaderBoard](https:\/\/www.kaggle.com\/serigne\/stacked-regressions-top-4-on-leaderboard\/log) as reference, this notebook follows what the reference did.","6744ada2":"# Predict Flight Fare","f5afcd20":"One-Hot Encode the categorical columns.","d36da96f":"Scatterplots are created to see whether there are linear trend between pirce and each numerical variables.","27a75e14":"The models without attribute selection are better.","f35ff24a":"* **Gradient Boosting Regression**\n\nWith huber loss that makes it robust to outliers","d3c4a0d9":"# Base models","c043c09c":"Let's take a look at MAE and MSE, the value smaller, the better the model is.","2191b338":"There are not obvious trend, so let's take a look the relationship in boxplot form.","33eb39ea":"# Data overview","b6d3a378":"As you may notice, I've not yet convert the cateogrical features in numeric form. I will do it here. \n\nI do this here to avoid having too much columns when I visualizated the data.","28a8818b":"* **Kernel Ridge Regression**","d5c6cb73":"**CatBoost Encoding**","f7ee28f7":"There are no missing value now.","dbb2c4f9":"The distributions have some difference, obviuosly.","ba2284e4":"The smaller the score is, the perforrmance of model is better.","c79e4bd9":"GBoost has the best result, so I will use GBoost to make prediction.","9d8989d9":"Take a look of unique value and its count of these two new features.","5408cdfb":"Duration is in xh ym formate, some don't have information of mintues. I will separte duration into hour in mintue in the following section.","fefe409f":"Look like there are price different accross differnt airline companies.","1bd3b382":"Date_of_Journey , Dep_Time and Arrival_Time contain date time value, we can separate them into more detail columns for further use.","f34c7fc0":"Total_Stops to numerical feature.","db40e9c8":"**Missing Data**","42db09c6":"Including only important features may increase the accuracy of prediction. In the following section, I will select the 30 most important independent variables for prediction.","e61e9843":"# Modelling","f3a85f75":"**Categorical variables**","a332dfd5":"Although it isn't normal yet, the data appears normally distributed now.","09ae3902":"CatBoost Encoding replaces a categorical value with the average value of the target for that value of the feature. \n\nFor each row, the target probability is calculated only from the rows before it.","27f0c3e1":"# Data pre-processing","138b56b5":"The distributions have some difference.","2be67b34":"# Data Visualization","0b22d0af":"**Base models scores**","4b39a9e3":"The distributions have some difference.","850fb611":"**Cross validation strategy**","f8eb3682":"Number of missing data is small, it's safe to drop them.","56815dea":"There are 1 cells in Route and Total stops, there may have one or two rows with missing value (This data set has 10683 rows in total). We can just drop the missing rows, they are small in size.","97f035cb":"# Feature engineering (part 2)","8f6f76c6":"**Numerical Variables**","df828203":"This note book is used to predict flight fare by some basic information of the flight, like the airline company, route and so on.","8ffcfb54":"Import librairies for modelling","43054a56":"**Data Correlation**","46999307":"* **LASSO Regression**:\n\nThis model is sensitive to outliers. Robustscaler() will use to robust outliers.","0293b0c2":"**Price** is the variable we want to predict. So let's do some analysis on this variable first.","651fd9ae":"The distributions have some difference.","5fe28cee":"* **LightGBM**","4929a987":"**Models with features selection**","c11ab87b":"**Feature selection**","6c3d2731":"* **Elastic Net Regression**","66664ca2":"**Log-transformation of price**","e659f5ef":"*Date time features*","fb64b12a":"The target varibale is right skewed. Let's try to transform this variable and make it more normaaly distributed."}}