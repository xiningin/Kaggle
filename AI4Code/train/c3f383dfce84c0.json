{"cell_type":{"bcc0c06f":"code","62177a0f":"code","c469d133":"code","f61ffdbb":"code","80327e09":"code","662b84ac":"code","4468a7e9":"code","8dbab7fa":"code","1b1883ce":"code","68b8b07f":"code","e1c094c9":"code","f35b69d3":"code","ec3186c0":"code","e169ef2d":"code","32a97a13":"code","97fe8302":"code","69a792a8":"code","d6ab4ca3":"code","031493c2":"code","339afde4":"code","c53b2b32":"code","3e0b717a":"code","7d90eebd":"code","c38db225":"code","a503b67d":"code","1f19fc8f":"code","2c251535":"code","2fe307d3":"code","e594d9af":"code","97d3758f":"code","f47f22ae":"code","75f2cfb8":"markdown","589691ef":"markdown","6ad1fd1d":"markdown","9175e9f4":"markdown","e6ef08be":"markdown","9d8fd5c1":"markdown","bd505593":"markdown","8d1e7717":"markdown","37d970aa":"markdown","3c3f0a3c":"markdown","62eb51b9":"markdown","90788e06":"markdown","193098eb":"markdown","7ae558f7":"markdown","04ffd461":"markdown","ab59fff8":"markdown","6d2107e7":"markdown","d9b96b29":"markdown","ad8a9c3d":"markdown"},"source":{"bcc0c06f":"# Importing required libraries\nimport pandas as pd\n\n# Loading the dataset\n#directory = \"..\/input\/used-vehicles-prices.csv\"\ndf_vehicle = pd.read_csv(\"..\/input\/used-vehicles-prices\/vehicle_dataset.csv\")","62177a0f":"# Displaying the shape and size of the dataset\nprint(\"Shape of the dataset: \", df_vehicle.shape)\nprint(\"Size of the dataset: \", df_vehicle.size)","c469d133":"# Displaying the column names for the dataset\ndf_vehicle.columns","f61ffdbb":"# Displaying the top 5 rows from the dataset\ndf_vehicle.head()","80327e09":"# Displaying unique values in each columns\nprint(\"Unique values for Seller Type:     \", df_vehicle['Seller_Type'].unique())\nprint(\"Unique values for Fuel Type:       \", df_vehicle['Fuel_Type'].unique())\nprint(\"Unique values for Transmission:    \", df_vehicle['Transmission'].unique())\nprint(\"Unique values for Previous owners: \", df_vehicle['Owner'].unique())","662b84ac":"# Checking for missing values in each column\ndf_vehicle.isnull().sum()","4468a7e9":"# Statistical description of the dataset\ndf_vehicle.describe(include = \"all\")","8dbab7fa":"# Selecting required features for the final dataset\nfinal_dataset = df_vehicle[['Year',\n                            'Selling_Price',\n                            'Present_Price',\n                            'Kms_Driven',\n                            'Fuel_Type',\n                            'Seller_Type',\n                            'Transmission',\n                            'Owner'\n                           ]]\n\n# Displaying top 5 columns of the final_dataset\nfinal_dataset.head()","1b1883ce":"# Sorting the dataset having current year as 2020\nfinal_dataset['Current Year'] = 2020\n\n# Displaying the dataset after sorting\nfinal_dataset.head()","68b8b07f":"# Finding the number of years after the release of the vehicle\nfinal_dataset['no_year'] = final_dataset['Current Year'] - final_dataset['Year']\n\n# Displaying the top 5 values from the dataset\nfinal_dataset.head()","e1c094c9":"# Dropping the release year column as we dont need it now\nfinal_dataset.drop(['Year'], axis = 1, inplace = True)\n\n# Displaying the top 5 values from the dataset\nfinal_dataset.head()","f35b69d3":"# Setting dummy values for the features\nfinal_dataset = pd.get_dummies(final_dataset, drop_first = True)\n\n# Displaying the top 5 rows with dummy values\nfinal_dataset.head()","ec3186c0":"# Dropping the Current Year column from the dataset\nfinal_dataset = final_dataset.drop(['Current Year'], axis = 1)\n\n# Displaying the top 5 rows of the modifies dataset\nfinal_dataset.head()","e169ef2d":"# Creating a correlational table for the final variables\nfinal_dataset.corr()","32a97a13":"# Importing seaborn for plotting graphs\nimport seaborn as sns\n\n# Plotting a pairplot for the final variables\nsns.pairplot(final_dataset)","97fe8302":"# Importing matplotlib for plotting \nimport matplotlib.pyplot as plt\n\n# Get correlations of each features in dataset\ncorrmat = df_vehicle.corr()\ntop_corr_features = corrmat.index\nplt.figure(figsize = (10, 10))\n\n# Plotting the heat map\ng = sns.heatmap(df_vehicle[top_corr_features].corr(), \n                annot = True, \n                cmap = \"RdYlGn\")","69a792a8":"# Seperating dataset for fitting the model\nX = final_dataset.iloc[:, 1:]\ny = final_dataset.iloc[:, 0]\n\n# Displaying the unique values of total owners of the vehicle\nX['Owner'].unique()","d6ab4ca3":"# Displaying the top 5 rows of the new dataset\nX.head()","031493c2":"y.head()","339afde4":"# Importing the required libraries\nfrom sklearn.ensemble import ExtraTreesRegressor\n\n# Creating an Regressor Model\nmodel = ExtraTreesRegressor()\n\n# Fitting the model\nmodel.fit(X, y)\n\n# Displaying the feature importance array\nprint(model.feature_importances_)","c53b2b32":"# Plot graph of feature importances for better visualization\nfeat_importances = pd.Series(model.feature_importances_, index = X.columns)\nfeat_importances.nlargest(5).plot(kind = 'barh')\nplt.show()","3e0b717a":"# Importing the library to split the data into train and test sets\nfrom sklearn.model_selection import train_test_split\n\n# Beginning the splitting of data into train and test sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 0)\n\n# Importing the library for Random Forest Regression\nfrom sklearn.ensemble import RandomForestRegressor\n\n# Creating a Random Forest Regression model\nregressor = RandomForestRegressor()","7d90eebd":"# Importing Randomized Search CV\nfrom sklearn.model_selection import RandomizedSearchCV\n\n# Number of trees in random forest\nimport numpy as np\nn_estimators = [int(x) for x in np.linspace(start = 100, stop = 1200, num = 12)]\n\n# Number of features to consider at every split\nmax_features = ['auto', 'sqrt']\n\n# Maximum number of levels in tree\nmax_depth = [int(x) for x in np.linspace(5, 30, num = 6)]\n\n# max_depth.append(None)\n# Minimum number of samples required to split a node\nmin_samples_split = [2, 5, 10, 15, 100]\n\n# Minimum number of samples required at each leaf node\nmin_samples_leaf = [1, 2, 5, 10]","c38db225":"# Create the random grid\nrandom_grid = {'n_estimators': n_estimators,\n               'max_features': max_features,\n               'max_depth': max_depth,\n               'min_samples_split': min_samples_split,\n               'min_samples_leaf': min_samples_leaf}\n\n# Printing the Random Grid\nprint(random_grid)","a503b67d":"# Use the random grid to search for best hyperparameters\n# First create the base model to tune\n# Creating a Random Forest Regression model\nrf = RandomForestRegressor()\n\n# Random search of parameters, using 3 fold cross validation, \n# search across 100 different combinations\nrf_random = RandomizedSearchCV(estimator = rf, \n                               param_distributions = random_grid,\n                               scoring = 'neg_mean_squared_error', \n                               n_iter = 10, \n                               cv = 5, \n                               verbose = 2, \n                               random_state = 42, \n                               n_jobs = 1)\n\n# Fitting a Random Forest Regression Model\nrf_random.fit(X_train,y_train)","1f19fc8f":"# Displaying the best paramenters for the Regression\nrf_random.best_params_","2c251535":"# Displaying the best score for the Regression\nrf_random.best_score_","2fe307d3":"# Predicting on the test dataset\npredictions = rf_random.predict(X_test)\n\n# Distribution plot for the predictions\nsns.distplot(y_test-predictions)","e594d9af":"# Scatter plot for the predictions\nplt.scatter(y_test, predictions)","97d3758f":"# Importing libraries to calculate the metrics\nfrom sklearn import metrics\n\n# Displaying the metrics for the analysis\nprint('MAE:', metrics.mean_absolute_error(y_test, predictions))\nprint('MSE:', metrics.mean_squared_error(y_test, predictions))\nprint('RMSE:', np.sqrt(metrics.mean_squared_error(y_test, predictions)))","f47f22ae":"#importing required library\n#import pickle\n\n#open a file, where you ant to store the data\n#file = open('random_forest_regression_model.pkl', 'wb')\n\n#dump information to that file\n#pickle.dump(rf_random, file)","75f2cfb8":"Inserting a new column in the dataset with current year as 2020","589691ef":"Take a look at the columns in our data","6ad1fd1d":"Take a look at the dataset","9175e9f4":"Descriptive statistics of the dataframe","e6ef08be":"### Step 2: Cleaning the data to make it ready for further steps\n\n---\nSelecting the required features from the dataset","9d8fd5c1":"Take a look at the shape of our data to have a general idea about its size","bd505593":"### Step 4: Machine Learning starts here","8d1e7717":"Converting the categorical variables to dummy\/indicator variables","37d970aa":"### Sep 3: Plotting graphs for better understanding of the data\n\n---\nImporting the required libraries","3c3f0a3c":"### Step 6: Serializing the object structure","62eb51b9":"Take a look at the total count of NULL values from each column","90788e06":"### Step 5: Analysing the model","193098eb":"Dropping the year column","7ae558f7":"Take a look at unique values from each column having categorical value","04ffd461":"Computing pairwise correlation of columns excluding the NA\/NULL values","ab59fff8":"Calculating the total number of years the car been owned for by subtracting the year of buing (year) from current year","6d2107e7":"Dropping the current year column that we used for calculating the total number of years","d9b96b29":"## Predicting Price for Used Vehicles\n\n---\nIn this project we are trying to predict the price of used card based on a dataset containing prices of several used cars.","ad8a9c3d":"### Step 1: Performing Exploratory Data Analysis on the given dataset\n\n---\nWe import the required library and dataset and create a DataFrame 'df_vehicle'"}}