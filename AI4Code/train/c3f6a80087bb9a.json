{"cell_type":{"9980db40":"code","d130d348":"code","296cdcbd":"code","c6fca776":"code","e5cf05f0":"code","ea158b35":"code","a5749fd6":"code","c5096303":"code","8d377bc8":"code","753f648d":"code","9835bd92":"code","4644a896":"code","5b1336cf":"code","e9439fd1":"code","f09140fb":"code","6caa3753":"code","ce0f1e8a":"code","6c56c696":"code","09677994":"code","041f1658":"code","863f897a":"code","24e5708e":"code","ce3ede89":"markdown","18c64172":"markdown","aa015758":"markdown","7bde5912":"markdown","044a5e13":"markdown","ef7c7c32":"markdown","0957a1df":"markdown","a4a0cd48":"markdown","7e3e1633":"markdown","9d15b0dc":"markdown","d3d72fe8":"markdown","d46cca22":"markdown","64ee1fc1":"markdown","51659b24":"markdown","5d3c747e":"markdown","7cce1f41":"markdown"},"source":{"9980db40":"# import os\n# for dirname, _, filenames in os.walk('\/kaggle\/input'):\n#     for filename in filenames:\n#         print(os.path.join(dirname, filename))\n","d130d348":"# Data                \n# \u2514\u2500\u2500\u2500train\n# \u2502   \u2514\u2500\u2500\u2500daisy\n# \u2502   \u2502      daisy_images.jpeg     \n# \u2502   \u2514\u2500\u2500\u2500dandelioin\n# \u2502   \u2502      dandelion_images.jpeg\n# \u2502   \u2514\u2500\u2500\u2500rose\n# \u2502   \u2502      rose_images.jpeg     \n# \u2502   \u2514\u2500\u2500\u2500sunflower\n# \u2502   \u2502      sunflower_images.jpeg\n# \u2502   \u2514\u2500\u2500\u2500tulip\n# \u2502         tulip_images.jpeg\n# \u2502\n# \u2514\u2500\u2500\u2500Validation\n# \u2502   \u2514\u2500\u2500\u2500daisy\n# \u2502   \u2502      daisy_images.jpeg     \n# \u2502   \u2514\u2500\u2500\u2500dandelioin\n# \u2502   \u2502      dandelion_images.jpeg\n# \u2502   \u2514\u2500\u2500\u2500rose\n# \u2502   \u2502      rose_images.jpeg     \n# \u2502   \u2514\u2500\u2500\u2500sunflower\n# \u2502   \u2502      sunflower_images.jpeg\n# \u2502   \u2514\u2500\u2500\u2500tulip\n#            tulip_images.jpeg","296cdcbd":"import os\nimport shutil\nfrom os.path import isfile, join, abspath, exists, isdir, expanduser\nfrom os import listdir, makedirs, getcwd, remove\nfrom pathlib import Path","c6fca776":"import pandas as pd\nimport numpy as np","e5cf05f0":"# Check for the directory and if it doesn't exist, make one.\ncache_dir = expanduser(join('~', '.keras'))\nif not exists(cache_dir):\n    makedirs(cache_dir)\n    \n# make the models sub-directory\nmodels_dir = join(cache_dir, 'models')\nif not exists(models_dir):\n    makedirs(models_dir)\n# original dataset folder, you can see above\ninput_path = Path('\/kaggle\/input\/flowers-recognition\/flowers')\nflowers_path = input_path \/ 'flowers'","ea158b35":"flowers = []\nflower_types = os.listdir(flowers_path)\nfor species in flower_types:\n    all_flowers = os.listdir(flowers_path \/ species)\n    for flower in all_flowers:\n        flowers.append((species, str(flowers_path\/species) \\\n        + '\/' + flower))\nflowers = pd.DataFrame(data=flowers, \n                       columns = ['category', 'image'],\n                       index = None\n                      )","a5749fd6":"flowers.head()","c5096303":"# Let's check how many samples for each category are present\nprint(\"Total number of flowers in the dataset: \", len(flowers))\nfl_count = flowers['category'].value_counts()\nprint(\"Flowers in each category: \")\nprint(fl_count)","8d377bc8":"# Make a parent directory `data` and two sub directories `train` and `valid`\n%mkdir -p data\/train\n%mkdir -p data\/valid\n\n# Inside the train and validation sub=directories, make sub-directories for each catgeory\n%cd data\n%mkdir -p train\/daisy\n%mkdir -p train\/tulip\n%mkdir -p train\/sunflower\n%mkdir -p train\/rose\n%mkdir -p train\/dandelion\n\n%mkdir -p valid\/daisy\n%mkdir -p valid\/tulip\n%mkdir -p valid\/sunflower\n%mkdir -p valid\/rose\n%mkdir -p valid\/dandelion\n\n%cd ..","753f648d":"for category in fl_count.index:\n    samples = flowers['image'][flowers['category'] == category].values\n    perm = np.random.permutation(samples)\n    # Copy first 100 samples to the validation directory and rest to the train directory\n    for i in range(100):\n        name = perm[i].split('\/')[-1]\n        shutil.copyfile(perm[i],'.\/data\/valid\/' + str(category) + '\/'+ name)\n    for i in range(101,len(perm)):\n        name = perm[i].split('\/')[-1]\n        shutil.copyfile(perm[i],'.\/data\/train\/' + str(category) + '\/' + name)","9835bd92":"import numpy as np\nimport pandas as pd\nimport seaborn as sns\nsns.set_style('darkgrid')\nimport matplotlib.pyplot as plt\nimport matplotlib.image as mimg","4644a896":"from tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom tensorflow.keras.layers import Dense, Input, Dropout,Flatten, Conv2D\nfrom tensorflow.keras.layers import BatchNormalization, Activation, MaxPooling2D\nfrom tensorflow.keras.models import Model, Sequential\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.callbacks import ModelCheckpoint, ReduceLROnPlateau\nfrom tensorflow.keras.utils import plot_model\n\nimport tensorflow as tf\nprint(\"Tensorflow version:\", tf.__version__)","5b1336cf":"# Define the generators\nbatch_size = 32\nimg_size = 240\n# this is the augmentation configuration we will use for training\ntrain_datagen = ImageDataGenerator(\n        rescale=1.\/255,\n        rotation_range=40,\n        width_shift_range=0.2,\n        height_shift_range=0.2,\n        shear_range=0.2,\n        zoom_range=0.2,\n        horizontal_flip=True)\n\n# this is the augmentation configuration we will use for testing:\n# only rescaling\ntest_datagen = ImageDataGenerator(rescale=1.\/255)\n\n\ntrain_generator = train_datagen.flow_from_directory(\"data\/train\/\",\n                                                    target_size=(img_size,img_size),\n                                                    batch_size=batch_size,\n                                                    class_mode='categorical',\n                                                    shuffle=True)\n\nvalidation_generator = test_datagen.flow_from_directory(\"data\/valid\/\",\n                                                    target_size=(img_size,img_size),\n                                                    batch_size=batch_size,\n                                                    class_mode='categorical',\n                                                    shuffle=False)","e9439fd1":"# SHOWING AUGUEMENTED iMAGES\nfrom keras.preprocessing import image\nfnames = [os.path.join('data\/train\/rose', fname) for\nfname in os.listdir('data\/train\/rose')]\nimg_path = fnames[1]\nimg = image.load_img(img_path, target_size=(240, 240))\n\nx = image.img_to_array(img)\nx = x.reshape((1,) + x.shape)\ni = 0\nf, axes = plt.subplots(1,4,figsize=(14,4))\nfor batch in train_datagen.flow(x, batch_size=1):\n    imgplot = axes[i].imshow(image.array_to_img(batch[0]))\n    i += 1\n    if i % 4 == 0:\n        break\nplt.show()","f09140fb":"steps_per_epoch = train_generator.n\/\/train_generator.batch_size\nvalidation_steps = validation_generator.n\/\/validation_generator.batch_size\n\nreduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.1,\n                              patience=2, min_lr=0.00001, mode='auto')\ncheckpoint = ModelCheckpoint(\"model_weights.h5\", monitor='val_accuracy',\n                             save_weights_only=True, mode='max', verbose=1)\ncallbacks = [checkpoint, reduce_lr]","6caa3753":"# Initialising the CNN\nmodel = Sequential()\n\n# 1 - Convolution\nmodel.add(Conv2D(64,(3,3), padding='same', input_shape=(240, 240,3)))\nmodel.add(BatchNormalization())\nmodel.add(Activation('relu'))\nmodel.add(MaxPooling2D(pool_size=(2, 2)))\nmodel.add(Dropout(0.25))\n\n# 2nd Convolution layer\nmodel.add(Conv2D(128,(5,5), padding='same'))\nmodel.add(BatchNormalization())\nmodel.add(Activation('relu'))\nmodel.add(MaxPooling2D(pool_size=(2, 2)))\nmodel.add(Dropout(0.25))\n\n# 3rd Convolution layer\nmodel.add(Conv2D(512,(3,3), padding='same'))\nmodel.add(BatchNormalization())\nmodel.add(Activation('relu'))\nmodel.add(MaxPooling2D(pool_size=(2, 2)))\nmodel.add(Dropout(0.25))\n\n# 4th Convolution layer\nmodel.add(Conv2D(512,(3,3), padding='same'))\nmodel.add(BatchNormalization())\nmodel.add(Activation('relu'))\nmodel.add(MaxPooling2D(pool_size=(2, 2)))\nmodel.add(Dropout(0.25))\n\n# Flattening\nmodel.add(Flatten())\n\n# Fully connected layer 1st layer\nmodel.add(Dense(256))\nmodel.add(BatchNormalization())\nmodel.add(Activation('relu'))\nmodel.add(Dropout(0.25))\n\n# Fully connected layer 2nd layer\nmodel.add(Dense(512))\nmodel.add(BatchNormalization())\nmodel.add(Activation('relu'))\nmodel.add(Dropout(0.25))\n\nmodel.add(Dense(5, activation='softmax'))\n\nopt = Adam(lr=0.0005)\nmodel.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy'])\nmodel.summary()","ce0f1e8a":"epochs_cnn = 25\nhistory_cnn = model.fit(\n    x=train_generator,\n    steps_per_epoch=steps_per_epoch,\n    epochs=epochs_cnn,\n    validation_data = validation_generator,\n    validation_steps = validation_steps,\n    callbacks=callbacks\n)","6c56c696":"acc = history_cnn.history['accuracy']\nval_acc = history_cnn.history['val_accuracy']\nloss = history_cnn.history['loss']\nval_loss = history_cnn.history['val_loss']\nepochs = range(1, len(acc) + 1)\n\nf, axes = plt.subplots(1,2,figsize=(14,4))\n\naxes[0].plot(epochs, acc, 'b', label='Training acc')\naxes[0].plot(epochs, val_acc, 'r', label='Validation acc')\naxes[0].legend()\n\naxes[1].plot(epochs, loss, 'b', label='Training loss')\naxes[1].plot(epochs, val_loss, 'r', label='Validation loss')\naxes[1].legend()\n\nplt.show()","09677994":"from keras.applications import VGG19","041f1658":"pre_trained_model = VGG19(input_shape=(224,224,3), include_top=False, weights=\"imagenet\")\n\nfor layer in pre_trained_model.layers[:19]:\n    layer.trainable = False\n\nmodel_vgg = Sequential([\n    pre_trained_model,\n    MaxPooling2D((2,2) , strides = 2),\n    Flatten(),\n    Dense(5 , activation='softmax')])\nmodel_vgg.compile(optimizer = \"adam\" , loss = 'categorical_crossentropy' , metrics = ['accuracy'])\nmodel_vgg.summary()","863f897a":"epochs_vgg = 25\nhistory_vgg = model.fit(\n    x=train_generator,\n    steps_per_epoch=steps_per_epoch,\n    epochs=epochs_cnn,\n    validation_data = validation_generator,\n    validation_steps = validation_steps,\n    callbacks=callbacks\n)","24e5708e":"acc = history_vgg.history['accuracy']\nval_acc = history_vgg.history['val_accuracy']\nloss = history_vgg.history['loss']\nval_loss = history_vgg.history['val_loss']\nepochs = range(1, len(acc) + 1)\n\nf, axes = plt.subplots(1,2,figsize=(14,4))\n\naxes[0].plot(epochs, acc, 'b', label='Training acc')\naxes[0].plot(epochs, val_acc, 'r', label='Validation acc')\naxes[0].legend()\n\naxes[1].plot(epochs, loss, 'b', label='Training loss')\naxes[1].plot(epochs, val_loss, 'r', label='Validation loss')\naxes[1].legend()\n\nplt.show()","ce3ede89":"## Dataset Information","18c64172":"![image.png](attachment:image.png)","aa015758":"# 2. Data Exploration","7bde5912":"## 3.1 CNN ","044a5e13":"# 1. Data Structure and Organization","ef7c7c32":"### Why?\n\nWe want to train our CNN with images. This can be easily done with Keras inbuilt [ImageDataGenerator](https:\/\/keras.io\/api\/preprocessing\/image\/) class (more on this later). The class expects data to be organized in the format specified below:\n\n","0957a1df":"### So At first we would set up folder and data structure to match the expected.","a4a0cd48":"![image.png](attachment:image.png)","7e3e1633":"### Running below code (kaggle provides this on default) also provides information about how datasets are organized","9d15b0dc":"# Objective","d3d72fe8":"* This dataset contains 4242 images of flowers of five classes: chamomile, tulip, rose, sunflower, dandelion.\n* The data collection is based on the data flicr, google images, yandex images.\n* Photos are not high resolution, about 320x240 pixels. Photos are not reduced to a single size, they have different proportions","d46cca22":"# 3. Building Models","64ee1fc1":"This notebooks ","51659b24":"Possibly first step is to explore how the data is organized and structured. Simplest way to do that is by using kaggle data explorer on the data page fig(1.1) or through data section of right sidebar fig(1.2)","5d3c747e":"For example this is what a file path looks for an image\n>>> \/kaggle\/input\/flowers-recognition\/flowers\/flowers\/tulip\/12764617214_12211c6a0c_m.jpg","7cce1f41":"## Restructuring the folder structure"}}