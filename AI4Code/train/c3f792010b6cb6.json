{"cell_type":{"fe91928e":"code","c9a783ea":"code","7d9b0749":"code","f39fdeea":"code","dc6b27ca":"code","b279b5ce":"code","e953f0ac":"code","b35cae0a":"code","99a58d6f":"code","373fe2f3":"code","c637aec1":"code","46376e76":"code","976c1447":"code","4ef126b4":"code","1e6ef185":"code","e67bf6d6":"code","9858c1b9":"code","358827b1":"markdown","f2ce7f8b":"markdown"},"source":{"fe91928e":"import cv2 as cv\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split\n\n%matplotlib inline ","c9a783ea":"#name = np.genfromtxt('pollen_data.csv',dtype='str',skip_header=1,delimiter=',',usecols=(1))\n \nname = np.genfromtxt(\"..\/input\/pollendataset\/PollenDataset\/pollen_data.csv\",dtype='str',skip_header=1,delimiter=',',usecols=(1))\n\npath = \"..\/input\/pollendataset\/PollenDataset\/images\/\"\nimlist = []\nfor i in name:\n    imlist.append(path + i)  ","7d9b0749":"def dataset(file_list,size=(180,300),flattened=False):  \n    '''\n    Function to create a dataset. It will load all the images into a np.array \n    \n    Parameters: \n    \n    - file_list: List of all the images you want to include in the dataset. \n    - Size : Size of the images, by default is 180x300 which is the original size. \n    - flattened: By default is False. Creates a dataset, but each image get converted into a big vector. \n    \n    Output: \n    \n    data: it outputs the dataset as a big np array \n    labels : It outputs the binary label. 1 for pollen 0 for non pollen. \n    \n    '''\n    data = []\n    for i, file in enumerate(file_list):\n        \n        image = cv.imread(file)\n        image2 = cv.cvtColor(image, cv.COLOR_RGB2BGR)\n        #image2 = cv.cvtColor(image, cv.COLOR_BGR2GRAY)\n        image = cv.resize(image2, size)\n        if flattened:\n            image = image.flatten()\n\n        data.append(image)\n        \n\n    labels = np.genfromtxt(\"..\/input\/pollendataset\/PollenDataset\/pollen_data.csv\",skip_header=1,delimiter=',',usecols=(2))\n    \n    return np.array(data), np.array(labels)","f39fdeea":"X,y=dataset(imlist)","dc6b27ca":"X_treinamento, X_teste, y_treinamento, y_teste = train_test_split(X, y, test_size=0.25, random_state=42)","b279b5ce":"#Example of a image in the dataset with its label. \nplt.imshow(X[9])\nplt.title(y[9])","e953f0ac":"X_treinamento = np.asarray(X_treinamento, dtype = np.float64)\nX_teste = np.asarray(X_teste, dtype = np.float64)\n\ny_treinamento = np.asarray(y_treinamento, dtype = np.int32)\ny_teste = np.asarray(y_teste, dtype = np.int32)","b35cae0a":"import tensorflow as tf","99a58d6f":"X_treinamento.shape\n","373fe2f3":"def cria_rede(features, labels, mode):\n    # batch_size, largura, altura, canais\n    #canais para coloridas = 3\n    #-1 quando n\u00e3o sabemos a quantidade\n    entrada = tf.reshape(features['X'], [-1, 300, 180, 3])\n    \n    # recebe [batch_size, 300, 180, 3]\n    # retorna [batch_size, 300, 180, 32]\n    #o 32 vem dos 32 filtros adicionados \n    convolucao1 = tf.layers.conv2d(inputs = entrada, filters = 32, kernel_size=[10,10], activation = tf.nn.relu,\n                                  padding = 'same')\n    # retorna [batch_size, 300, 180, 32]\n    # retorna [batch_size, 150, 90, 32]\n    pooling1 = tf.layers.max_pooling2d(inputs = convolucao1, pool_size = [5,5], strides = 5)\n    \n    # retorna [batch_size, 150, 90, 32]\n    # retorna [batch_size, 150, 90, 64]\n    convolucao2 = tf.layers.conv2d(inputs = pooling1, filters = 64, kernel_size = [10,10], activation = tf.nn.relu,\n                                  padding = 'same')\n    # retorna [batch_size, 150, 90, 64]\n    # retorna [batch_size, 75, 45, 64]\n    pooling2 = tf.layers.max_pooling2d(inputs = convolucao2, pool_size = [3,3], strides = 3)\n    \n    # retorna [batch_size, 75, 45, 64]\n    # retornar [batch_size, 216000]\n\n    flattening = tf.reshape(pooling2, [-1, 20 * 12 * 64])\n    #flattening = tf.reshape(pooling2, [-1,15360])\n\n    \n    # 216000 (entradas) -> 3000 (oculta) -> 2 (sa\u00edda)\n    # recebe [batch_size, 15360]\n    # retornar [batch_size, 3000]\n    densa = tf.layers.dense(inputs = flattening, units = 3000, activation = tf.nn.relu)\n    # dropout\n    #zera algumas entradas. rate e a porcentagem. \n    dropout = tf.layers.dropout(inputs = densa, rate = 0.3, training=mode == tf.estimator.ModeKeys.TRAIN)\n    \n    # recebe [batch_size, 3000]\n    # retornar [batch_size, 2]\n    saida = tf.layers.dense(inputs = dropout, units = 2)\n    \n    previsoes = tf.argmax(saida, axis = 1)\n    \n    if mode == tf.estimator.ModeKeys.PREDICT:\n        return tf.estimator.EstimatorSpec(mode = mode, predictions = previsoes)\n    \n    \n    erro = tf.losses.sparse_softmax_cross_entropy(labels = labels, logits = saida)\n    \n    \n    #erro = tf.losses.softmax_cross_entropy(onehot_labels = labels, logits = saida)\n    #erro = tf.losses.sigmoid_cross_entropy(multi_class_labels = labels, logits = saida)\n\n    if mode == tf.estimator.ModeKeys.TRAIN:\n        otimizador = tf.train.AdamOptimizer(learning_rate = 0.001)\n        treinamento = otimizador.minimize(erro, global_step = tf.train.get_global_step())\n        return tf.estimator.EstimatorSpec(mode = mode, loss = erro, train_op = treinamento)\n    \n    if mode == tf.estimator.ModeKeys.EVAL:\n        eval_metrics_ops = {'accuracy': tf.metrics.accuracy(labels = labels, predictions = previsoes)}\n        return tf.estimator.EstimatorSpec(mode = mode, loss = erro, eval_metric_ops = eval_metrics_ops)","c637aec1":"classificador = tf.estimator.Estimator(model_fn = cria_rede)","46376e76":"funcao_treinamento = tf.estimator.inputs.numpy_input_fn(x = {'X': X_treinamento}, y = y_treinamento,\n                                                       batch_size = 128, num_epochs = None, shuffle = True)\nclassificador.train(input_fn=funcao_treinamento, steps = 200)","976c1447":"funcao_teste = tf.estimator.inputs.numpy_input_fn(x = {'X': X_teste}, y = y_teste, num_epochs = 1,\n                                                      shuffle = False)\nresultados = classificador.evaluate(input_fn=funcao_teste)\nresultados","4ef126b4":"X_imagem_teste = X_teste[97]\nX_imagem_teste.shape","1e6ef185":"X_imagem_teste = X_imagem_teste.reshape(1,-1)\nX_imagem_teste.shape","e67bf6d6":"funcao_previsao = tf.estimator.inputs.numpy_input_fn(x = {'X': X_imagem_teste}, shuffle = False)\npred = list(classificador.predict(input_fn = funcao_previsao))","9858c1b9":"X_image_teste = X_imagem_teste.reshape(300,180,3)\nX_image_teste.shape\nX_image_teste = np.asarray(X_image_teste,dtype = np.uint8)\nplt.imshow(X_image_teste)\nplt.title(str(pred))","358827b1":"# CNN Polen","f2ce7f8b":"https:\/\/www.tensorflow.org\/tutorials\/estimators\/cnn#logits_layer"}}