{"cell_type":{"5af6edb7":"code","a6617f24":"code","af1186af":"code","96bd5d77":"code","d1149a68":"code","f20bc3e1":"markdown","7d2ae3f1":"markdown","82e71c65":"markdown","70546bff":"markdown","2e30b1c7":"markdown","aa90fd58":"markdown","3fd9170c":"markdown","e12c37e3":"markdown"},"source":{"5af6edb7":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom scipy import stats\nfrom sklearn import metrics\nfrom sklearn.metrics import confusion_matrix, classification_report\nfrom sklearn.model_selection import train_test_split\nfrom xgboost import XGBRegressor\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.model_selection import GridSearchCV\nfrom catboost import CatBoostClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn import linear_model\nfrom sklearn.ensemble import GradientBoostingRegressor\nfrom sklearn import tree\nfrom sklearn import svm\nfrom sklearn.ensemble import AdaBoostRegressor\nfrom lightgbm import LGBMClassifier","a6617f24":"def AlgoFun (X_train, Y_train, X_test, Y_test, Algo, Result):\n    \n    if (Algo == 'LOG'):\n        log_reg=LogisticRegression(C=1000,max_iter=50000)\n        log_reg.fit(X_train, Y_train)\n        Y_pred = log_reg.predict(X_test)\n        \n    elif (Algo == 'LIN'):\n        model = linear_model.LinearRegression()\n        model.fit(X_train, Y_train)\n        Y_pred = model.predict(X_test)\n        \n    elif (Algo == 'KNN'):\n        KNN=KNeighborsClassifier(n_neighbors=20)\n        KNN.fit(X_train, Y_train)\n        Y_pred=KNN.predict(X_test)\n\n    elif (Algo == 'RFC'):\n        Clf =  RandomForestClassifier(n_estimators = 500, n_jobs = -1)\n        Clf.fit(X_train, Y_train)\n        Y_pred=Clf.predict(X_test) \n        compare1 = pd.DataFrame()\n        compare1[0] = Clf.feature_importances_\n        compare1[1] = X_test.columns\n        print('Feature importance: ')\n        print(compare1.sort_values(by=0,ascending= False))\n        \n    elif (Algo == 'NN'):\n        NN = MLPClassifier(solver='sgd', alpha=1e-5, hidden_layer_sizes=(5, 2), random_state=1, max_iter=1000)\n        NN.fit(X_train, Y_train)\n        Y_pred = NN.predict(X_test)\n        \n    elif (Algo == 'DTR'):\n        DTR = tree.DecisionTreeClassifier()\n        DTR.fit(X_train, Y_train)\n        Y_pred=DTR.predict(X_test)\n        \n    elif (Algo == 'GSCV'):\n        estimator = RandomForestRegressor(random_state = 42,criterion='mse')\n        para_grids = {\n                    \"n_estimators\" : [10,50,100],\n                    \"max_features\" : [\"auto\", \"log2\", \"sqrt\"],\n                    'max_depth' : [4,5,6,7,8,9,15],\n                    \"bootstrap\"    : [True, False]\n                }\n        Grid = GridSearchCV(estimator, para_grids,cv= 5)\n        Grid.fit(X_train, Y_train)\n        best_param = Grid.best_estimator_\n        print(best_param)\n        Y_pred = best_param.predict(X_test)\n\n        \n    elif (Algo == 'RFR'):\n        # Using the best model from Grid Serach CV\n        model = RandomForestRegressor(max_depth=15, random_state=42) \n        model.fit(X_train, Y_train)\n        Y_pred = model.predict(X_test)\n      \n    elif (Algo == 'GBR'):   \n        GBR = GradientBoostingRegressor(n_estimators=100, max_depth=4)\n        GBR.fit(X_train, Y_train)\n        Y_pred = GBR.predict(X_test)\n        \n    elif (Algo == 'GNB'):   \n        GNB = GaussianNB()\n        GNB.fit(X_train, Y_train)\n        Y_pred = GNB.predict(X_test)\n      \n    elif (Algo == 'ADA'):\n        model = AdaBoostRegressor(random_state=0, n_estimators=100)\n        model.fit(X_train, Y_train)\n        Y_pred = model.predict(X_test)\n        \n    elif (Algo == 'XGB'):\n        model = XGBRegressor()\n        model.fit(X_train, Y_train)\n        Y_pred = model.predict(X_test)\n        \n    elif (Algo == 'LGB'):\n        model = LGBMClassifier(objective='multiclass', random_state=5)\n        model.fit(X_train, Y_train)\n        Y_pred = model.predict(X_test)        \n   \n    elif (Algo == 'CAT'):\n        Cat = CatBoostClassifier(silent = True)\n        details = Cat.fit(X_train, Y_train)\n        Y_pred = Cat.predict(X_test)\n        \n    elif (Algo == 'SVM'):\n        model = svm.SVC(kernel='linear') # Linear Kernel\n        model.fit(X_train, Y_train)\n        Y_pred = model.predict(X_test)       \n            \n    else:\n        print(\"Wrong Algo\")\n    \n    Y_test=pd.DataFrame(Y_test).iloc[:, [0]].to_numpy()\n    Y_pred=pd.DataFrame(Y_pred).iloc[:, [0]].to_numpy()\n    \n    ActVPred = pd.DataFrame({'Actual': Y_test[:,0], 'Predicted': Y_pred[:,0]})\n    print(ActVPred)\n\n    #Checking the accuracy\n    print('Mean Absolute Error:', metrics.mean_absolute_error(Y_test, Y_pred))\n    print('Mean Squared Error:', metrics.mean_squared_error(Y_test, Y_pred))\n    print('Root Mean Squared Error:', np.sqrt(metrics.mean_squared_error(Y_test, Y_pred)))\n\n    if (Result == 'Binary'):\n        Count_row = []\n        Visual_rep = []\n\n        index = 0\n        for i, row in ActVPred.iterrows():\n            if (row['Predicted'] < 0.5):\n                Visual_rep.append(0)\n            else:\n                Visual_rep.append(1)\n\n            if (row['Actual'] < 1):\n                if (row['Predicted'] < 0.5):\n                    Count_row.append(1)\n                else:\n                    Count_row.append(0)\n            else:\n                if (row['Predicted'] >= 0.5):\n                    Count_row.append(1)\n                else:\n                    Count_row.append(0)\n            index = index + 1\n\n        print('--------------------------------------------------------------------------')\n        print(Algo)\n        print('Model accruracy scores: {:.3f}'.format(Count_row.count(1)\/index))\n\n\n        ax = plt.subplots(figsize=(10, 10))\n        ax = sns.heatmap(confusion_matrix(Visual_rep,Y_test),annot=True,cmap='coolwarm',fmt='d')\n        ax.set_title('Prediction vs Original Data (Confusion Matrix)',fontsize=18)\n        ax.set_xticklabels(['Actual 0','Actual 1'],fontsize=18)\n        ax.set_yticklabels(['Predicted 0','Predicted 1'],fontsize=18)\n\n        filename = 'ConfusionMatrix.jpg'\n        plt.savefig(Algo + \" \" + filename)\n        plt.show()\n    \n    elif (Result == 'Analog'):\n        fig, ax = plt.subplots()\n        minimum = min (Y_test.min(), Y_pred.min())\n        maximum = max (Y_test.max(), Y_pred.max())\n        ax.scatter(Y_test, Y_pred)\n        ax.plot([minimum, maximum], [minimum, maximum], 'k--', lw=4)\n        ax.set_xlabel('Measured')\n        ax.set_ylabel('Predicted')\n        filename = 'Result Plot.jpg'\n        plt.savefig(Algo + \" \" + filename)\n        plt.show()\n    \n    else:\n        print(\"Wrong parameter\")\n\n    return (ActVPred)","af1186af":"X_train = pd.read_csv('..\/input\/exampledata\/X_train.csv')\nX_test = pd.read_csv('..\/input\/exampledata\/X_test.csv')\nY_train = pd.read_csv('..\/input\/exampledata\/Y_train.csv').to_numpy()[:,1]\nY_test = pd.read_csv('..\/input\/exampledata\/Y_test.csv').to_numpy()[:,1]","96bd5d77":"result = AlgoFun(X_train, Y_train, X_test, Y_test,'CAT', 'Binary')","d1149a68":"result = AlgoFun(X_train, Y_train, X_test, Y_test,'ADA','Binary')","f20bc3e1":"# Sample Test and Train data","7d2ae3f1":"# Running Logistic Regression","82e71c65":"<h2><span style=\"color:crimson;font-family:bold;\">  \n    \nHave you ever struggled to find the right functions for various ML algorithms or\n\nHad trouble finding the right syntax and how the RMSE or accuracy has to be calculated or\n\nSpent time trying to test out multiple algorithms \n\n<\/span><\/h2>","70546bff":"<div class=\"alert alert-block alert-info\"> <span style=\"color:green;font-family:cursive;font-size:18px;\"> \n\nThis notebook is a humble attempt to bring all the ML fuctions to train and deploy algorithms to one function. \n\nJust call the function with the algorithm of your choice and the training and test dataset and leave the rest for the algorithm to do.\n\nThis is to help deploy ML algorithms faster and to help you test the quickly. \n\nI am a novice and thought this would help beginners such as me to help deploy and try out algorithms faster. <\/span> <\/div>\n\n***","2e30b1c7":"<div style=\"color:white;\n           display:fill;\n           border-radius:5px;\n           background-color:#5642C5;\n           font-size:110%;\n           font-family:Verdana;\n           letter-spacing:0.5px\">\n\n<p style=\"padding: 10px;\n              color:white;\">\n    \n<center> <h1> Function AlgoFun <\/h1> <\/center>\n\n\n<h2> The below function takes in a parameter called Algo. \nYou need to pass any of the below values depending on which algorithm to run <\/h2>\n\n\n<h2> Algo Values: <\/h2>\n\n<h3> LOG : Logistic Regression <\/h3> \n\n<h3> LIN : Linear Regression <\/h3>\n\n<h3> KNN : K Nearest Neighbor <\/h3>\n\n<h3> RFC : Random Forest Classifier <\/h3>\n\n<h3> NN  : Neural Network <\/h3>\n\n<h3> DTR : Decision Tree <\/h3>\n\n<h3> GSCV: Grid Search CV <\/h3>\n\n<h3> RFR : Random Forest Regression <\/h3>\n\n<h3> GBR : Gradient Boosting Regression <\/h3>\n\n<h3> GNB : Gaussian Naive Bayes <\/h3>\n\n<h3> ADA : Ada Boost <\/h3>\n\n<h3> XGB : XG Boost <\/h3>\n\n<h3> LGB : Light GBM <\/h3>\n\n<h3> CAT : Cat Boost <\/h3>\n\n<h3> SVM : Support Vector Machine <\/h3>\n\n\n\n<h2> Result Values <\/h2>\n\n<h3> Binary : If predictions are 0 or 1 <\/h3>\n\n<h3> Analog : If predictions is a number <\/h3>\n\n    \n<\/p><\/div>\n","aa90fd58":"# Running CATBoost","3fd9170c":"![](https:\/\/nypost.com\/wp-content\/uploads\/sites\/2\/2018\/12\/got.jpg?quality=90&strip=all&w=1236&h=820&crop=1)","e12c37e3":"<center> <h1> \ud83d\udc78\u2728\u2728\ud83d\udc78 Mother of all Algorithms \ud83d\udc78\u2728\u2728\ud83d\udc78 <\/h1><\/center>\n\n***"}}