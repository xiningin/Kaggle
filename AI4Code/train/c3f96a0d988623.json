{"cell_type":{"a5680598":"code","e57469f9":"code","2362884a":"code","c8a050c0":"code","7ce1ca48":"code","b46c0471":"code","78b717ca":"code","b64425eb":"code","8ca3c82d":"code","729307e6":"code","637d0a32":"markdown","ad80f23c":"markdown","acced815":"markdown","a22aa31c":"markdown"},"source":{"a5680598":"!pip uninstall fsspec -qq -y\n!pip install --no-index --find-links=..\/input\/hf-datasets\/wheels datasets -qq\n!pip uninstall transformers -qq -y\n!pip install --no-index --find-links=..\/input\/transformers-latest-model transformers -qq","e57469f9":"from datasets import Dataset\nimport pandas as pd\nfrom tqdm import tqdm\nimport torch\nfrom transformers import AutoModelForQuestionAnswering, AutoTokenizer, pipeline\nimport numpy as np, json\nfrom hf_qa_utils import *\nfrom transformers import TrainingArguments, Trainer\nfrom transformers import default_data_collator\ndata_collator = default_data_collator\n\ntorch.set_grad_enabled(False)\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\ndevice_id = 0 if torch.cuda.is_available() else -1\n\nargs = TrainingArguments(\n    f\"dummy\",\n    report_to=[\"tensorboard\"],\n    per_device_eval_batch_size=256,\n)\n\nmkeys = [\"muril\", \"rembert\", \"xlmr\"]","2362884a":"test_df = pd.read_csv(\"..\/input\/chaii-hindi-and-tamil-question-answering\/test.csv\")\ntest_ds = Dataset.from_pandas(test_df)","c8a050c0":"topk = 5\nSCALE_SCORES = True\ndef get_top_candidates(candidates):\n    preddf = []\n    for qid, arr in candidates.items():\n        arr = sorted(arr, reverse=True, key=lambda x: x[\"score\"])[:topk]\n        for rec in arr:\n            rec[\"id\"] = qid\n        preddf.extend(arr)\n    \n    preddf = pd.DataFrame(preddf)\n    preddf = preddf.sort_values(\"score\", ascending=False)\n    preddf[\"aclean\"] = preddf[\"text\"].apply(clean_answer)\n    preddf = preddf.groupby([\"id\", \"aclean\"])[\"score\"].max().reset_index()\n    if SCALE_SCORES:\n        mn, mx = preddf[\"score\"].min(), preddf[\"score\"].max()\n        preddf[\"score\"] = (preddf[\"score\"]-mn)\/(mx-mn)\n\n    return preddf\n","7ce1ca48":"muril_ckpts = [\"..\/input\/muril-finetuning-indicx-on-squad2-epoch-2\/muril_lg_indix_sq2ep1\/checkpoint-3480\", \\\n               \"..\/input\/folds-1-and-2-muril-indicx-sq2ep2-finetuning\/muril_lg_indix_sq2ep1_fold1\/checkpoint-3463\", \\\n               \"..\/input\/folds-1-and-2-muril-indicx-sq2ep2-finetuning\/muril_lg_indix_sq2ep1_fold2\/checkpoint-3452\", \\\n               \"..\/input\/folds-3-and-4-muril-indicx-sq2ep2-finetuning\/muril_lg_indix_sq2ep1_fold3\/checkpoint-3485\", \\\n               \"..\/input\/folds-3-and-4-muril-indicx-sq2ep2-finetuning\/muril_lg_indix_sq2ep1_fold4\/checkpoint-3472\"\n              ]\nrembert_ckpts = [\"..\/input\/rembert-finetuning-indicx-on-sq2-epoch3\/rembert_indicx_over_squad2\/checkpoint-1021\"]\nxlmr_ckpts = [f\"..\/input\/folds-consolidated-xlmr-qa-finetune-on-indix\/fold{i}\" for i in range(5)]\n\nckpt_meta = [(\"muril\", muril_ckpts), (\"rembert\", rembert_ckpts), (\"xlmr\", xlmr_ckpts)]\n\npred_dfs = []\nfor model_type, ckpts in ckpt_meta:\n    tokenizer = AutoTokenizer.from_pretrained(ckpts[0])\n    tkwargs = {\"tokenizer\": tokenizer} \n    ds_feats = test_ds.map(prepare_validation_features, batched=True, remove_columns=test_ds.column_names, fn_kwargs=tkwargs)\n    \n    starts, ends = None, None\n    for mname in ckpts:\n        model = AutoModelForQuestionAnswering.from_pretrained(mname)\n        trainer = Trainer(model, args, data_collator=data_collator, tokenizer=tokenizer)\n        raw_vals = trainer.predict(ds_feats)\n        if starts is None:\n            starts, ends = raw_vals.predictions\n        else:\n            starts += raw_vals.predictions[0]\n            ends += raw_vals.predictions[1]\n    starts \/= len(ckpts)\n    ends \/= len(ckpts)\n    \n    ds_feats.set_format(type=ds_feats.format[\"type\"], columns=list(ds_feats.features.keys()))\n    _, candidates = postprocess_qa_predictions(test_ds, ds_feats, (starts, ends), \\\n                                               cls_token_id=tokenizer.cls_token_id, n_best_size=5, \\\n                                               pp_cleanup=False, return_candidates=True)\n    cdf = get_top_candidates(candidates)\n    pred_dfs.append((model_type, cdf))","b46c0471":"highpred = pd.merge(pred_dfs[0][-1], pred_dfs[1][-1], \\\n                    on=[\"id\", \"aclean\"], how=\"outer\", suffixes=[\"_\"+pred_dfs[0][0], \"_\"+pred_dfs[1][0]])\nhighpred = pd.merge(highpred, pred_dfs[2][-1], on=[\"id\", \"aclean\"], how=\"outer\")\nhighpred.rename(columns={\"score\": f\"score_{pred_dfs[2][0]}\"}, inplace=True)\n\nmkeys = list(map(lambda x:x[0], ckpt_meta))\nfor mkey in mkeys:\n    minval = 0 if SCALE_SCORES else highpred[~highpred[f\"score_{mkey}\"].isna()][f\"score_{mkey}\"].min()-1\n    highpred[f\"score_{mkey}\"].fillna(minval, inplace=True)\n\nhighpred['tot_score'] = highpred[f\"score_{mkeys[0]}\"]\nfor mkey in mkeys[1:]:\n    highpred['tot_score'] += highpred[f\"score_{mkey}\"]\n\nhighpred = highpred.sort_values(\"tot_score\", ascending=False) \nhighpred = highpred.groupby([\"id\"]).head(1).reset_index(drop=True) \nhighpred.rename(columns={\"aclean\": \"PredictionString\"}, inplace=True)","78b717ca":"test_df = pd.merge(test_df, highpred, on=\"id\", how=\"left\")\ntest_df['PredictionString'].fillna('', inplace=True)","b64425eb":"import re\nyear_ptrn = re.compile(\"\\d{4}\")\n\ntime_prefixes = [\"\u0b95\u0bbf.\u0bae\u0bc1\", \"\u0b95\u0bbf.\u0baa\u0bbf\", \" \u0908\", \"\u0908.\u092a\u0942\", \"\u0935\u0930\u094d\u0937\", \"\u0938\u0928\"]\ndef update_year_answer(pred_ans):\n    if any([tp in pred_ans for tp in time_prefixes]):\n        return pred_ans\n    ypreds = year_ptrn.findall(pred_ans)\n    if len(ypreds)!=1:\n        return pred_ans\n    return ypreds[0]\n\nyears = [\"\u0b8e\u0ba8\u0bcd\u0ba4 \u0b86\u0ba3\u0bcd\u0b9f\u0bc1\", \"\u0915\u093f\u0938 \u0935\u0930\u094d\u0937\", \"\u0915\u093f\u0938 \u0938\u093e\u0932\"]\nis_ans_year = (test_df[\"question\"].str.contains(\"|\".join(years), regex=True))\nif is_ans_year.any():\n    test_df.loc[is_ans_year, \"PredictionString\"] = test_df.loc[is_ans_year, \"PredictionString\"].apply(update_year_answer)\ntest_df['PredictionString'].fillna('', inplace=True)","8ca3c82d":"_ = \"\"\"import unicodedata\nhin = [chr(i) for i in range(2406, 2416)]\nenn = [f\"{i}\" for i in range(10)]\n\nis_pred_hin = test_df[\"PredictionString\"].apply(lambda x: set(x)<=set(hin))\nif is_pred_hin.any():\n    test_df[\"trans\"] = test_df[\"PredictionString\"].copy()\n    test_df.loc[is_pred_hin, \"trans\"] = test_df.loc[is_pred_hin, \"trans\"].apply(lambda txt: \"\".join([enn[hin.index(c)] for c in txt]))\n\n    is_trans_in_context = test_df.apply(lambda row: row[\"trans\"] in row[\"context\"], axis=1)\n    if (is_pred_hin&is_trans_in_context).any():\n        test_df.loc[is_pred_hin&is_trans_in_context, \"PredictionString\"] = test_df.loc[is_pred_hin&is_trans_in_context, \"trans\"]\ntest_df['PredictionString'].fillna('', inplace=True)\n\"\"\"","729307e6":"test_df[['id', 'PredictionString']].to_csv('submission.csv', index=False) #With muril for hin and xlmr for tam and without excessive pp, public lb=0.82","637d0a32":"# Generate best answer from all model types","ad80f23c":"# Postprocessing","acced815":"Approach summaries. Knitty-gritties in corresponding versions\n\n- V1. V1: Blind XLMR, followed by Rembert followed by XLMR (squad1)\n- V2. Quantile scoring approach\n- V6. 3 Remberts, 2 XLMRs predicted, their top answer cleaned text and question embedding difference is fed to an LGBM Ranker. If all models are useless according to LGBM (How to guess this? +\/- values?, fallback to rembert sq2ep3 ft)\n- V10: Top 3 answers from 3 models(5 fold Muril, Single rembert, 5 fold XLMR) are fed to an LGBM booster optimised for ndcg.\n- V16: Top 3 answers from 3 models(5 fold Muril, Single rembert, 5 fold XLMR) retrieve candidate texts using Stanza. These new question-goldensent pair are fed to bert ft on tydi ft on chaii. (V17, V18 = individual language stats)\n- V19-?: All three models ensemble by mapping offsets to characters and mean of softmax scores. Then retokenize using XLMR and assign mean of character scores to a given token and post process as usual. Few versions are variants of same approach.\n- (This notebook): Rudimentary, my dear Watson!\n\nJust get intersection of cleaned answers from all three models and use scaled scores to rank them and generate outputs.","a22aa31c":"# Get top 5 candidates per model type"}}