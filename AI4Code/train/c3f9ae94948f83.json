{"cell_type":{"0c7cceed":"code","5901be94":"code","2ecc0d5c":"code","bc0e074e":"code","8bf19e59":"code","befccbc5":"code","a0fa8673":"code","aa11eefc":"code","07dbae01":"code","e5a6dd3d":"code","544d1369":"code","165fdb94":"code","743f6523":"code","a319011a":"code","2bd3aa27":"code","bc39d2e0":"code","76ba5a07":"code","6c9ac5e1":"code","80f4b837":"markdown","16340885":"markdown","b4cacdda":"markdown","136c10d5":"markdown","7544c1a4":"markdown","36fff9b3":"markdown","6f5251d6":"markdown","61d7c21e":"markdown","4fa297a3":"markdown","80a8cd2a":"markdown","5c9890e2":"markdown","33ccb4ce":"markdown","8786f195":"markdown"},"source":{"0c7cceed":"# import module\nimport numpy as np\nimport pandas as pd\n\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.naive_bayes import *\nfrom sklearn.metrics import *\n\nfrom io import StringIO\nimport re\n","5901be94":"# TODO 1: Loading the data\n# \u8bfb\u53d6sql\u6709\u4e09\u79cd\u65b9\u6cd5\uff1a\u5185\u7f6eopen\u3001pymysql\u3001pandas\nsql_file = \"\/kaggle\/input\/news-with-french\/data_raw.sql\"\ndef read_sql():\n    sql = open(sql_file, \"r\", encoding = \"utf8\")\n    sqltxt = sql.readlines()\n    sql.close()\n    print(sqltext[0]) #\u96be\u4ee5\u5206\n# read_sql()\n# def\u00a0read_sql_script_all(sql_file_path,\u00a0quotechar=\"'\")\u00a0->\u00a0(str,\u00a0dict):\n\ndef read_sql_script_all(sql_file_path, quotechar=\"'\"):\n    insert_check=re.compile(\"insert +into +`?(\\w+?)`?\\(\", re.I|re.A)\n    with open(sql_file_path, encoding=\"utf-8\") as f:\n        sql_txt=f.read()\n    print(len(sql_txt))\n    end_pos = -1\n    df_dict = {}\n    while True:\n        match_obj = insert_check.search(sql_txt, end_pos+1)\n        print(match_obj)\n        if not match_obj: \n            break\n        table_name = match_obj.group(1)\n        start_pos = match_obj.span()[1]+1\n        end_pos = sql_txt.find(\";\", start_pos)\n        tmp = re.sub(r\"\\)(\u00a0values\u00a0|,)\\(\",\"\\n\",sql_txt[start_pos:end_pos])\n        tmp =re.sub(r\"[`()]\",\"\",tmp)\n        df=pd.read_csv(StringIO(tmp),quotechar=quotechar)\n        dfs=df_dict.setdefault(table_name,[])\n        dfs.append(df)\n    for table_name, dfs in df_dict.items():\n        df_dict[table_name]=pd.concat(dfs)\n    return df_dict\n\nread_sql_script_all(sql_file)","2ecc0d5c":"# TODO 2: Generating the raw_data.csv","bc0e074e":"French_text = \"\/kaggle\/input\/news-with-french\/Paris et Londres en 1793.txt\"\nEnglish_text = \"\/kaggle\/input\/news-with-french\/A Tale of Two Cities.txt\"\nraw_french  = open(French_text,encoding='utf8').read()\nraw_english  = open(English_text,encoding='utf8').read()\nraw_english[:500]","8bf19e59":"#\u5206\u8bcd\uff1ahttps:\/\/zhuanlan.zhihu.com\/p\/242247311\ndef sentence_split_nltk(strs):\n#     import nltk\n#     nltk.download()\n\n    from nltk.tokenize import sent_tokenize\n    sent_tokenize_list = sent_tokenize(strs)\n    return sent_tokenize_list\n\ndef sentence_split(strs):\n    strs = strs.replace(\"!\",\".\").replace(\"?\",\".\").replace(\"\\t\",\" \").replace(\"\\n\",\" \")\n    return strs.split(\".\")\nsen_en = sentence_split(raw_english) #https:\/\/www.dtmao.cc\/news_show_1850208.shtml\nsen_french = sentence_split(raw_french)\nx = sen_en + sen_french\ny = ['en']*len(sen_en) + ['fe']*len(sen_french)","befccbc5":"def test_Count():\n    a =\"\u81ea\u7136\u8bed\u8a00\u5904\u7406\u662f\u8ba1\u7b97\u673a\u79d1\u5b66\u9886\u57df\u4e0e\u4eba\u5de5\u667a\u80fd\u9886\u57df\u4e2d\u7684\u4e00\u4e2a\u91cd\u8981\u65b9\u5411\u3002\u5b83\u7814\u7a76\u80fd\u5b9e\u73b0\u4eba\u4e0e\u8ba1\u7b97\u673a\u4e4b\u95f4\u7528\u81ea\u7136\u8bed\u8a00\u8fdb\u884c\u6709\u6548\u901a\u4fe1\u7684\u5404\u79cd\u7406\u8bba\u548c\u65b9\u6cd5\u3002\u81ea\u7136\u8bed\u8a00\u5904\u7406\u662f\u4e00\u95e8\u878d\u8bed\u8a00\u5b66\u3001\u8ba1\u7b97\u673a\u79d1\u5b66\u3001\u6570\u5b66\u4e8e\u4e00\u4f53\u7684\u79d1\u5b66\"\n    b = \"\u56e0\u6b64\uff0c\u8fd9\u4e00\u9886\u57df\u7684\u7814\u7a76\u5c06\u6d89\u53ca\u81ea\u7136\u8bed\u8a00\uff0c\u5373\u4eba\u4eec\u65e5\u5e38\u4f7f\u7528\u7684\u8bed\u8a00\uff0c\u6240\u4ee5\u5b83\u4e0e\u8bed\u8a00\u5b66\u7684\u7814\u7a76\u6709\u7740\u5bc6\u5207\u7684\u8054\u7cfb\uff0c\u4f46\u53c8\u6709\u91cd\u8981\u7684\u533a\u522b\u3002\u81ea\u7136\u8bed\u8a00\u5904\u7406\u5e76\u4e0d\u662f\u4e00\u822c\u5730\u7814\u7a76\u81ea\u7136\u8bed\u8a00\uff0c\u800c\u5728\u4e8e\u7814\u5236\u80fd\u6709\u6548\u5730\u5b9e\u73b0\u81ea\u7136\u8bed\u8a00\u901a\u4fe1\u7684\u8ba1\u7b97\u673a\u7cfb\u7edf\uff0c\u7279\u522b\u662f\u5176\u4e2d\u7684\u8f6f\u4ef6\u7cfb\u7edf\u3002\"\n    c =\"\u56e0\u800c\u5b83\u662f\u8ba1\u7b97\u673a\u79d1\u5b66\u7684\u4e00\u90e8\u5206\u3002\u81ea\u7136\u8bed\u8a00\u5904\u7406\uff08NLP\uff09\u662f\u8ba1\u7b97\u673a\u79d1\u5b66\uff0c\u4eba\u5de5\u667a\u80fd\uff0c\u8bed\u8a00\u5b66\u5173\u6ce8\u8ba1\u7b97\u673a\u548c\u4eba\u7c7b\uff08\u81ea\u7136\uff09\u8bed\u8a00\u4e4b\u95f4\u7684\u76f8\u4e92\u4f5c\u7528\u7684\u9886\u57df\u3002\"\n    import jieba\n    all_list= ['  '.join(jieba.cut(s,cut_all = False)) for s in [a,b,c]]\n    # print((all_list)[0])\n    count_vec=CountVectorizer()\n    count_vec.fit_transform([a,b,c]).toarray()\n    print('\\nvocabulary list:\\n\\n',count_vec.get_feature_names())\n    print( '\\nvocabulary dic :\\n\\n',count_vec.vocabulary_)\n    print ('vocabulary:\\n\\n')\n    for key,value in count_vec.vocabulary_.items():\n        print(key,value)","a0fa8673":"def remove_noise(document):\n    noise_pattern = re.compile(\"|\".join([\"http\\S+\", \"\\@\\w+\", \"\\#\\w+\"]))\n    clean_text = re.sub(noise_pattern, \"\", document)\n    return clean_text.strip()\nremove_noise(\"Trump images are now more popular than cat gifs. @trump #trends http:\/\/www.trumptrends.html\")","aa11eefc":"bigram_count = CountVectorizer(ngram_range=(2,2),analyzer='char_wb',#\n    max_features=1000,  # keep the most common 1000 ngrams\n    preprocessor=remove_noise\n)\nbigram_count\n# https:\/\/www.dtmao.cc\/news_show_1850208.shtml\npipline = Pipeline([(\"vectorzier\",bigram_count),(\"model\",MultinomialNB())])\npipline","07dbae01":"x_train,x_test,y_train,y_test = train_test_split(x,y,test_size=0.2,random_state=1024)\npipline.fit(x_train,y_train)\ny_prd = pipline.predict(x_test)\nprint(confusion_matrix(y_test,y_prd))\nprint(classification_report(y_test,y_prd))\ndef predict_sentence(sentence:str):\n    y_prd = pipline.predict([sentence])\n    return y_prd[0]\ndef predict_article(article:str):\n    y_prd = list(pipline.predict(sentence_split(article)))\n#     print(y_prd)\n    c = max(y_prd,key=y_prd.count)\n    return c=='fe'\npredict_article(\"J'utilise souvent SQL dans mon travail, et il existe de nombreuses nuances et limitations ennuyeuses, mais en derni\u00e8re analyse, c'est la pierre angulaire de l'industrie des donn\u00e9es. Par cons\u00e9quent, pour chaque travailleur dans le domaine des donn\u00e9es, SQL est indispensable. La ma\u00eetrise de SQL est d'une grande importance.\")","e5a6dd3d":"# TODO 4: Generating the data.csv","544d1369":"df = pd.read_csv(\"\/kaggle\/input\/news-with-french\/raw_data.csv\")\n# \u4e24\u4e2a\u6e05\u9664\u7a7a\u884c\u65b9\u6cd5np\u7684isnan\u548cdf.dropna()\n# df = df[~np.isnan(df['clean_text'])] pandas\u7684\u95ee\u9898nan\u7b97\u4e3afloat\n# df['clean_text'][69].map(predict_article)\n# float\u62a5\u9519\u901a\u8fc7\u627e\u5230\u62a5\u9519\u9879 \u5f97\u77e5[df['clean_text'][69]] nan\u662f\u4e00\u4e2afloat\uff0c\u8fd8\u4e0d\u80fd\u7528isnan\u7b49\u53bb\u53bb\u9664\/\/\u53ea\u80fd\u7528pd\u7684\u65b9\u6cd5\u624d\u80fd\u5224\u65ad\ndf = df[~pd.isnull(df['clean_text'])] \ndf['Class'] = df['clean_text'].map(predict_article)\ndf.head()","165fdb94":"df.to_csv(\".\/data.csv\",index=None)","743f6523":"df = pd.read_csv(\".\/data.csv\")\ndf_french = df[df['Class']==True]\nlen(df_french)","a319011a":"# 1.parse datetime\nimport datetime as dt\nfrom datetime import datetime\n# df_french['post_data_gmt'] = df_french['post_data_gmt'].map(lambda x:x.strftime('%Y-%m-%d %H:%M:%S'))\ndef parse_ymd(s): #\u901f\u5ea6\u6bd4datetime\u81ea\u5e26\u51fd\u6570\u5feb\n    year_s, mon_s, day_s, hour_s, min_s, second_s= s.replace(\":\",\"-\").replace(\" \",\"-\").split('-')\n    return datetime(int(year_s), int(mon_s), int(day_s),int(hour_s),int(min_s),int(second_s))\ndf_french['post_data_gmt'] = df_french['post_data_gmt'].map(parse_ymd)\ndf_french = df_french.set_index('post_data_gmt')\n# 2.set a col to mark month\ndf_french['month'] = df_french.index.map(lambda x:x.month)\n# 3.get series and plot\nmonth_count = df_french.groupby('month').agg('count')['Class']\nmonth_count.plot(kind='bar')\nprint(f\"every month publish {int(month_count.mean())} on average\")","2bd3aa27":"# set a col to mark day\ndf_french['day'] = df_french.index.map(lambda x:x.date)\n# get series and plot\nday_count = df_french.groupby('day').agg('count')['Class']\nday_count.plot()\nprint(f\"every day publish {int(day_count.mean())} on average\")","bc39d2e0":"# use between_time() to get a period\n# df_french.between_time(\"06:00\", \"22:00\") #\u53ea\u80fd\u662ftime\u6240\u4ee5\u4e0d\u884c\nmax_pub = 0\nmax_sday = df_french.index[0]\nfor i in df_french.index:\n#     days = len(df_french[i.strftime('%Y-%m-%d %H:%M:%S'):(i+dt.timedelta(days=15)).strftime('%Y-%m-%d %H:%M:%S')])\n    start = df_french.index.searchsorted(i)\n    end = df_french.index.searchsorted(i+dt.timedelta(days=15))\n    days = len(df_french[start:end])\n    if max_pub < days: \n        max_pub = days\n        max_sday = i\nprint(f\"between {max_sday} to {max_sday+dt.timedelta(days=15)} had published {max_pub} articles\")","76ba5a07":"df_other = df[df['Class']==False]\ndf_other['post_data_gmt'] = df_other['post_data_gmt'].map(parse_ymd)\ndf_other = df_other.set_index('post_data_gmt')\ndf_other['day'] = df_other.index.map(lambda x:x.date)\n# get series and plot\nimport seaborn as sns; sns.set()\n%matplotlib inline\nday_count_other = df_other.groupby('day').agg('count')['Class']\nday_count.name = 'Other'\nday_count_other.name = 'French'","6c9ac5e1":"dd=[day_count_other,day_count]\nax = sns.lineplot(data=dd)\nplt.show()","80f4b837":"define model","16340885":"train and test model","b4cacdda":"- Make a visualization that compares the daily article publications in French and Other languages (s) for the whole data in data.csv","136c10d5":"- [How to use CountVectorizer](https:\/\/blog.csdn.net\/m0_37788308\/article\/details\/80933915) \u5305\u62ec\u4e2d\u6587","7544c1a4":"# Pre-processing for data mining\n## Introduction\nWith this notebook, you are provided an exported SQL file data_raw.sql, a record of news content from different sources.\n\nThe intial table had the following clumns: `ID`, `post_author`, `post_date`, `post_date_gmt`, `post_content`, `post_title`, `post_excerpt`, `post_status`, `comment_status`, `ping_status`, `post_password`, `post_name`, `to_ping`, `pinged`, `post_modified`, `post_modified_gmt`, `post_content_filtered`, `post_parent`, `guid`, `menu_order`, `post_type`, `post_mime_type`, `comment_count`\n\n\nBelow are two examples of the recorded data:\n- example 1:\n\n> (6189, 0, '2015-08-27 15:28:37', '2015-08-27 15:28:37', 'DHL \u00a0: am\u00e9lioration de la logistique des transports (NewsMada)', 'DHL \u00a0: am\u00e9lioration de la logistique des transports (NewsMada)', 'DHL \u00a0: am\u00e9lioration de la logistique des transports (NewsMada)', 'inherit', 'open', 'closed', '', 'dhl-amelioration-de-la-logistique-des-transports-newsmada-2', '', '', '2015-08-27 15:28:37', '2015-08-27 15:28:37', '', 6188, 'http:\/\/example.com\/wp-content\/uploads\/2015\/08\/DHL-\u00a0-am\u00e9lioration-de-la-logistique-des-transports-NewsMada.png', 0, 'attachment', 'image\/png', 0)\n\n- example 2:\n\n\n> (6190, 1, '2015-08-26 09:19:57', '2015-08-26 09:19:57', ' [ad_1]\\r\\n<br><div id=\\\"\\\"><p style=\\\"text-align: justify;\\\">En collaboration avec la r\u00e9gion Vakinankaratra, le Centre international de recherches agronomiques pour le d\u00e9veloppement (Cirad) et l\u2019Institut international des sciences sociales (IISS), ayant trait \u00e0 la prospective territoriale et locale, l\u2019Agence fran\u00e7aise de d\u00e9veloppement (AFD) ont organis\u00e9 r\u00e9cemment dans la ville d\u2019Eaux un atelier sur la prospective territoriale participative. D\u2019apr\u00e8s le chef de r\u00e9gion Mandrindra Andrianjanaka, l\u2019objectif est d\u2019exp\u00e9rimenter une nouvelle approche territoriale, en plus de l\u2019approche sectorielle dont on avait l\u2019habitude auparavant. 25 personnes disposant \u00e0 titre personnel de connaissances compl\u00e9mentaires en la mati\u00e8re ont particip\u00e9 \u00e0 cette rencontre.<\/p>\\n<p style=\\\"text-align: justify;\\\">Il s\u2019agissait en l\u2019occurrence d\u2019int\u00e9grer les dynamiques d\u00e9mographiques dans les strat\u00e9gies de d\u00e9veloppement en se projetant sur une p\u00e9riode de 20 ans, plus exactement jusqu\u2019en 2035. Une telle action permet ainsi d\u2019avoir une vision globale de l\u2019\u00e9volution future de la r\u00e9gion Vakinankaratra et d\u2019identifier les forces qui permettraient d\u2019influencer son d\u00e9veloppement. Les cinq jours d\u2019atelier ont ainsi fait ressortir diff\u00e9rents sc\u00e9narii se rapportant \u00e0 la vision fix\u00e9e\u00a0 de 2035. Des r\u00e9sultats qui se veulent un outil de prise de d\u00e9cision pour le d\u00e9veloppement de la r\u00e9gion.<\/p>\\n<p style=\\\"text-align: justify;\\\">La r\u00e9gion Vakinankaratra est donc honor\u00e9e d\u2019avoir \u00e9t\u00e9 choisie par l\u2019AFD qui a d\u00e9j\u00e0 initi\u00e9 une recherche exp\u00e9rimentale sur plusieurs territoires ruraux d\u2019Afrique du m\u00eame genre, en promouvant une d\u00e9marche participative bas\u00e9e sur l\u2019implication des acteurs du territoire concern\u00e9. Mandrindra Andrianjanaka, dans son discours de cl\u00f4ture de l\u2019atelier a soulign\u00e9 que les r\u00e9solutions prises allaient \u00eatre r\u00e9ellement prises en consid\u00e9ration.<\/p>\\n<p style=\\\"text-align: right;\\\"><strong>Jeannot Ratsimbazafy<\/strong><\/p>\\n\\n<section id=\\\"text-5\\\" class=\\\"widget widget_text\\\"\/><!-- #comments -->\t\t<\/div>\\r\\n<br>[ad_2]\\r\\n<br><a href=\\\"http:\/\/www.newsmada.com\/2015\/08\/26\/antsirabe-a-lheure-de-la-prospective-territoriale-participative\/\\\">Source link <\/a>', 'Antsirabe : \u00e0 l\u2019heure de la prospective territoriale participative (NewsMada)', '', 'publish', 'open', 'open', '', 'antsirabe-a-lheure-de-la-prospective-territoriale-participative-newsmada', '', '', '2015-08-26 09:19:57', '2015-08-26 09:19:57', '', 0, 'http:\/\/example.com\/antsirabe-a-lheure-de-la-prospective-territoriale-participative-newsmada\/', 0, 'post', '', 0)\n\nIn this notebook, you will pre-process this data as part of a data mining pipeline. Your task will be completed when the required raw texts are extracted and classifed whether being in French or others.\n\n- **Preprocess** \n    - You'll extract the news content from author 1 as clean text (without HTML tags)\n    - You will create a new CSV file raw_data.csv that contains everything you collected from the previous step plus the post_date_gmt and the ```Source``` link. E.g., http:\/\/www.newsmada.com\/2015\/08\/26\/antsirabe-a-lheure-de-la-prospective-territoriale-participative in example 2. You will set missing values to None. You will also add a new column for the ```Source domain``` which is the root domain name of each source link.\n\n- **Models**\n    - You will a function process which accepts a string (text content) as input and returns a probability of content being in French.\n    - You will run process() on each clean news content from raw_data.csv.\n    - You will create a new CSV file data.csv, a copy of raw_data.csv with a new column value to specify the language being used represented by 1 when french is used and 0 overwise.\n\n- **Prediction**: \n    It is essential to note that news content containing only a few French words (names, etc.) should not be considered french news content.\n\n","36fff9b3":"Answer the following questions:\n\n- What is the total number of french news articles from the generated data.csv\n- How many French articles a month were published on average?\n- Make a visualization of the number of French articles published daily based on data.csv\n- Which period (lasting 15 days) had the most French articles publication?\n- Make a visualization that compares the daily article publications in French and Other languages (s) for the whole data in data.csv\n- How many unique source domains are there that in total?\n- How many unique source domains are writing content in French only.\n- Visualize the number of French articles published in the different source domains per year.","6f5251d6":"- Which period (lasting 15 days) had the most French articles publication?","61d7c21e":"- What is the total number of french news articles from the generated data.csv","4fa297a3":"### [The method of building the model](https:\/\/zhuanlan.zhihu.com\/p\/27447133)(traditional)","80a8cd2a":"# TODO 3: Your model for language detection or identification\n- Some ideas about natural language processing [link](https:\/\/www.zhihu.com\/question\/356132676)","5c9890e2":"- Make a visualization of the number of French articles published daily based on data.csv","33ccb4ce":"- How many French articles a month were published on average?","8786f195":"\u4e3a\u4e86\u89e3\u51b3\u5206\u53e5\u7684\u95ee\u9898\uff0c\u6211\u9996\u5148\u5c1d\u8bd5\u57fa\u4e8eptyhon\u7684NLTK\u5957\u4ef6\u6765\uff0c\u53d1\u73b0\u9ed8\u8ba4\u7684\u7b97\u6cd5\u5e76\u4e0d\u5b8c\u5584\uff0c\u6216\u8005\u8bf4\u65e0\u6cd5\u9002\u5e94\u5404\u79cd\u4e0d\u540c\u76ee\u7684\u7684\u9700\u6c42\uff0c\u8fd8\u662f\u9700\u8981\u540e\u7eed\u5904\u7406\u3002\u6bd4\u5982NLTK\u3001textstats\u7b49\u9ed8\u8ba4\u628a\u65e0\u6807\u70b9\u7ed3\u5c3e\u800c\u662f\u56de\u8f66\u7b26\u7684\u884c\u672b\u7edf\u4e00\u66ff\u6362\u4e3a\\n\uff0c\u5168\u90e8\u5f53\u4f5c\u540c\u4e00\u4e2a\u53e5\u5b50\uff0c\u6bd4\u5982\u4e66\u7c4d\u524d\u8fb9\u7684\u76ee\u5f55\uff0c\u8fd9\u6837\u7684\u8bdd\u5c31\u4ea7\u751f\u4e86\u4e00\u4e9b\u4e0d\u5408\u7406\u7684\u8d85\u957f\u53e5\u5b50\u3002"}}