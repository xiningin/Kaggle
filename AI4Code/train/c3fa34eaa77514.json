{"cell_type":{"6b3dcb9b":"code","63f9c031":"code","b35a284b":"code","b889de8c":"code","96763796":"code","f1ac7711":"code","20edc666":"code","89ee0cb9":"code","8bc9de5b":"code","82e096ee":"code","6831953d":"code","c9c40cc6":"code","128ba44c":"code","8425eb0b":"code","fe4864b8":"code","dc967eef":"code","ae31d99b":"code","bc9ba186":"code","1f9e7b80":"code","b7c78807":"code","4a5ee109":"code","a0662343":"code","dfe2bc45":"code","e3ecfbda":"code","fe81515b":"code","e28194e5":"code","ac8eeb59":"code","e97d2b36":"code","ea9d312c":"code","d9982d1a":"code","7d5e2096":"code","4d757b27":"code","ba2609c1":"code","0940595b":"code","7a210a98":"code","cde95b5f":"code","d0c8ff23":"code","649fb62c":"code","8aaa16f7":"code","401eb21f":"code","e533f645":"code","496bd718":"code","edc56e22":"markdown","d14069da":"markdown","819e6600":"markdown","bc7056c9":"markdown","f68ea334":"markdown","e56ab799":"markdown","e4ab939e":"markdown","2d2e120d":"markdown","936e47d8":"markdown","4958c2a3":"markdown","6dbed556":"markdown","d9cbcad6":"markdown","37f3feda":"markdown","99743c4a":"markdown","286f4de9":"markdown","2ba5be51":"markdown","d45dc2f2":"markdown","cbf80ca2":"markdown","28553d27":"markdown","a025b7ad":"markdown","2b4a94b4":"markdown","c20f28c8":"markdown","94ce893b":"markdown","0ad89477":"markdown","a73ba205":"markdown","b557bcef":"markdown","bcf1fa87":"markdown"},"source":{"6b3dcb9b":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by t|he kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport warnings\nwarnings.filterwarnings(\"ignore\")\nnp.random.seed(42)\n\n\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.\n","63f9c031":"df=pd.read_csv(\"..\/input\/heart.csv\")","b35a284b":"df.head()","b889de8c":"df.index","96763796":"df.dtypes","f1ac7711":"df.isnull().sum()","20edc666":"df.describe()","89ee0cb9":"df.head()","8bc9de5b":"fig, ax = plt.subplots(figsize=(5, 8))\nsns.countplot(df['target'])\nplt.title('Target values')","82e096ee":"fig, ax = plt.subplots(figsize=(5, 8))\nsns.countplot(df['sex'])\nplt.title('male and female distribution')","6831953d":"sns.distplot(df['age'])\n","c9c40cc6":"sns.jointplot(x = 'age', y = 'oldpeak', kind = 'kde', color = 'red', data = df)\n","128ba44c":"sns.set(font_scale=2)\nfig, ax = plt.subplots(figsize=(20, 10))\nplt.title('Target values with different cp in male and female')\nsns.barplot(x='sex',y='target',hue='cp',data=df)","8425eb0b":"ages=['age']\nbins = [29,35,45,55,65,77]\nlabels = [ '25-40','41-50', '51-60', '61-70', '70+']\ndf['agerange'] = pd.cut(df.age, bins, labels = labels,include_lowest = True)\nfig, ax = plt.subplots(figsize=(20, 10))\nplt.title('maximum heart rate as the agerange with and without excersize induced angina ')\nsns.barplot(x='agerange',y='thalach',hue='exang',data=df)","fe4864b8":"sns.set(font_scale=1)\nfig, ax = plt.subplots(figsize=(20, 20))\nplt.title('correlation between features')\nsns.heatmap(df.corr(), robust=True, fmt='.2f',\n                square=True, linewidths=.5, annot=True, cbar_kws={\"shrink\": .75})","dc967eef":"fig, ax = plt.subplots(figsize=(20, 10))\nplt.title('change in cholestrol with the age in male and female')\nsns. barplot(x=\"agerange\", y=\"chol\",hue='sex' ,data=df)\n","ae31d99b":"ax1=sns.catplot(x=\"agerange\", y=\"chol\",hue=\"sex\",kind=\"violin\",split=True,data=df)\nplt.title('change in cholestrol with the age in men and women')\nfig1, ax2 = plt.subplots(figsize=(10, 5))\nax2=sns.boxplot(x=\"sex\", y=\"chol\",data=df)\nplt.title('change in cholestrol with the age')\n","bc9ba186":"from sklearn.metrics import classification_report, confusion_matrix\nfrom sklearn.model_selection import train_test_split \nfrom sklearn.linear_model import LogisticRegression\n","1f9e7b80":"X=df.drop(columns=[\"target\",\"agerange\"],axis=1)\ny=df['target']","b7c78807":"df.head()","4a5ee109":"test_size=[0.05,0.1,0.15,0.2,0.25,0.3,0.35,0.4,0.45]\nfor i in test_size:\n  X_train,X_test,y_train,y_test=train_test_split(X,y,test_size=i)\n  clf=LogisticRegression()\n  clf.fit(X_train,y_train)\n  clf.predict(X_test)\n  accuracy=clf.score(X_test,y_test)\n  print(\"accuracy for test size\",i,\"is\",accuracy )","a0662343":"from sklearn.model_selection import train_test_split \nX_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.30)\nfrom sklearn.preprocessing import StandardScaler\n# scale the train and test ages\ndf_num=['age', 'trestbps', 'chol','thalach', 'oldpeak']\nscaler = StandardScaler()\nX_train.age = scaler.fit_transform(X_train.age.values.reshape(-1,1))\nX_test.age = scaler.transform(X_test.age.values.reshape(-1,1))\nX_train.trestbps = scaler.fit_transform(X_train.trestbps.values.reshape(-1,1))\nX_test.trestbps = scaler.transform(X_test.trestbps.values.reshape(-1,1))\nX_train.chol = scaler.fit_transform(X_train.chol.values.reshape(-1,1))\nX_test.chol = scaler.transform(X_test.chol.values.reshape(-1,1))\nX_train.thalach = scaler.fit_transform(X_train.thalach.values.reshape(-1,1))\nX_test.thalach = scaler.transform(X_test.thalach.values.reshape(-1,1))\nX_train.oldpeak = scaler.fit_transform(X_train.oldpeak.values.reshape(-1,1))\nX_test.oldpeak = scaler.transform(X_test.oldpeak.values.reshape(-1,1))\n","dfe2bc45":"clf=LogisticRegression()\nclf.fit(X_train,y_train)\nclf.predict(X_test)\naccuracy=clf.score(X_test,y_test)\nprint(\"accuracy with feauture scaling is\",accuracy )\n","e3ecfbda":"import sklearn\nimport sklearn.datasets\nfrom sklearn import preprocessing\ndf=pd.read_csv(\"..\/input\/heart.csv\")\n","fe81515b":"df.columns","e28194e5":"X=df.drop(['target'],axis=1)\nY=df.as_matrix(columns=['target'])\nshape_X = X.shape\nshape_Y = Y.shape\nm = X.shape[0]\nprint ('The shape of X is: ' + str(shape_X))\nprint ('The shape of Y is: ' + str(shape_Y))\nprint ('I have m = %d training examples!' % (m))","ac8eeb59":"np.random.seed(42)\nX_train,X_test,Y_train,Y_test=train_test_split(X,Y,test_size=0.25)\nfrom sklearn.preprocessing import StandardScaler\n# scale the train and test ages\ndf_num=['age', 'trestbps', 'chol','thalach', 'oldpeak']\nscaler = StandardScaler()\nX_train.age = scaler.fit_transform(X_train.age.values.reshape(-1,1))\nX_test.age = scaler.transform(X_test.age.values.reshape(-1,1))\nX_train.trestbps = scaler.fit_transform(X_train.trestbps.values.reshape(-1,1))\nX_test.trestbps = scaler.transform(X_test.trestbps.values.reshape(-1,1))\nX_train.chol = scaler.fit_transform(X_train.chol.values.reshape(-1,1))\nX_test.chol = scaler.transform(X_test.chol.values.reshape(-1,1))\nX_train.thalach = scaler.fit_transform(X_train.thalach.values.reshape(-1,1))\nX_test.thalach = scaler.transform(X_test.thalach.values.reshape(-1,1))\nX_train.oldpeak = scaler.fit_transform(X_train.oldpeak.values.reshape(-1,1))\nX_test.oldpeak = scaler.transform(X_test.oldpeak.values.reshape(-1,1))\n","e97d2b36":"X_train=X_train.T\nprint(X_train.shape)\nY_train=Y_train.T\nprint(Y_train.shape)\nX_test=X_test.T\nprint(X_test.shape)\nY_test=Y_test.T\nprint(Y_test.shape)","ea9d312c":"print(X_train.shape)\nprint(Y_train.shape)\nprint(X_test.shape)\nprint(Y_test.shape)","d9982d1a":"def sigmoid(z):\n    s = 1\/(1+np.exp(-z))\n    return s","7d5e2096":"def layer_sizes(X, Y):\n    \n    n_x = X.shape[0] # size of input layer\n    n_h = 4\n    n_y = Y.shape[0] # size of output layer\n    return (n_x, n_h, n_y)","4d757b27":"def initialize_parameters(n_x, n_h, n_y):\n    np.random.seed(2) # we set up a seed so that your output matches ours although the initialization is random.\n    \n    W1 = np.random.randn(n_h,n_x) * 0.01\n    b1 = np.zeros((n_h,1))\n    W2 = np.random.randn(n_y,n_h) * 0.01\n    b2 = np.zeros((n_y,1))\n    parameters = {\"W1\": W1,\n                  \"b1\": b1,\n                  \"W2\": W2,\n                  \"b2\": b2}\n    \n    return parameters","ba2609c1":"def forward_propagation(X, parameters):\n    W1 = parameters[\"W1\"]\n    b1 = parameters[\"b1\"]\n    W2 = parameters[\"W2\"]\n    b2 = parameters[\"b2\"]\n    Z1 = np.dot(W1,X)+b1\n    A1 = np.tanh(Z1)\n    Z2 = np.dot(W2,A1)+b2\n    A2 = sigmoid(Z2)\n    cache = {\"Z1\": Z1,\n             \"A1\": A1,\n             \"Z2\": Z2,\n             \"A2\": A2}\n    \n    return A2, cache","0940595b":"def compute_cost(A2, Y, parameters):\n    m = Y.shape[1] # number of example\n    logprobs = (Y*np.log(A2))+((1-Y)*np.log(1-A2))\n    cost = (-1\/m)*np.sum(logprobs)\n    cost = np.squeeze(cost)     # makes sure cost is the dimension we expect. \n    return cost","7a210a98":"# GRADED FUNCTION: backward_propagation\n\ndef backward_propagation(parameters, cache, X, Y):\n    m = X.shape[1]\n    W1= parameters[\"W1\"]\n    W2 = parameters[\"W2\"]\n    A1 = cache[\"A1\"]\n    A2 = cache[\"A2\"]\n    dZ2 = A2-Y\n    dW2 = (1\/m)*np.dot(dZ2,A1.T)\n    db2 = (1\/m)*np.sum(dZ2,axis=1,keepdims=True)\n    dZ1 = np.dot(W2.T,dZ2)*(1-np.power(A1,2))\n    dW1 = (1\/m)*np.dot(dZ1,X.T)\n    db1 = (1\/m)*np.sum(dZ1,axis=1,keepdims=True)\n    grads = {\"dW1\": dW1,\n             \"db1\": db1,\n             \"dW2\": dW2,\n             \"db2\": db2}\n    \n    return grads","cde95b5f":"def update_parameters(parameters, grads, learning_rate = 1.2):\n    W1 = parameters[\"W1\"]\n    b1 = parameters[\"b1\"]\n    W2 = parameters[\"W2\"]\n    b2 = parameters[\"b2\"]\n    dW1 = grads[\"dW1\"]\n    db1 = grads[\"db1\"]\n    dW2 = grads[\"dW2\"]\n    db2 = grads[\"db2\"]\n    W1 = W1-(learning_rate*dW1)\n    b1 = b1-(learning_rate*db1)\n    W2 = W2-(learning_rate*dW2)\n    b2 = b2-(learning_rate*db2)\n    parameters = {\"W1\": W1,\n                  \"b1\": b1,\n                  \"W2\": W2,\n                  \"b2\": b2}\n    \n    return parameters","d0c8ff23":"def nn_model(X, Y, n_h, num_iterations = 10000, print_cost=False):\n    np.random.seed(3)\n    n_x = layer_sizes(X, Y)[0]\n    n_y = layer_sizes(X, Y)[2]\n    parameters = initialize_parameters(n_x, n_h, n_y)\n    W1 = parameters[\"W1\"]\n    b1 = parameters[\"b1\"]\n    W2 = parameters[\"W2\"]\n    b2 = parameters[\"b2\"]\n    for i in range(0, num_iterations):\n        A2, cache = forward_propagation(X, parameters)\n        cost = compute_cost(A2, Y, parameters)\n        grads = backward_propagation(parameters, cache, X, Y)\n        parameters = update_parameters(parameters, grads, learning_rate = 1.2)\n  \n        if print_cost and i % 100000 == 0:\n            print (\"Cost after iteration %i: %f\" %(i, cost))\n\n    return parameters","649fb62c":"def predict(parameters, X):\n    A2, cache = forward_propagation(X, parameters)\n    predictions = (A2>0.5)\n    \n    return predictions","8aaa16f7":"parameters = nn_model(X_train, Y_train, n_h = 4, num_iterations = 1000000 , print_cost=True)\n","401eb21f":"predictions = predict(parameters, X_test)\npredictions","e533f645":"print(predictions.shape)\nprint(Y_test.shape)","496bd718":"predictions = predict(parameters, X_test)\nprint ('Accuracy: %d' % float((np.dot(Y_test,predictions.T) + np.dot(1-(Y_test),1-predictions.T))\/float(((Y_test)).size)*100) + '%')","edc56e22":"\n**Below i have shown a plot by samoling the age feature in some parts to observe the effect of excersize on the maximum heart rate achieved in different agegroup.\nwe can see that here the maximum heart beat decreases as the age increases but it further decreases as the patient has excersize induced angina. by this we can say that by achieving lower maximum heart beat we can predict and save ourselves from excersize induced angina.**","d14069da":"**Now lets see how many male and female in our data**","819e6600":"**Below i have removed columns as mentioned above during logistic regression**","bc7056c9":"here we are observing the bar plot with the belive that different type of chest pain causes different type of targets in male and female.\nwe can see that cp type 4 is more responsible in female while chest pain type 2 is more responsible in male. ","f68ea334":"\n\n**Below i have shown the relation between age and oldpeak as the jointplot.**","e56ab799":"**Below a i have shown a correlation matrix to show if there is any corelelation between different features.and we can say that there is not any serious corelation between features.**","e4ab939e":"****Reading the data through pandas****\n","2d2e120d":"**Below i have shown the relation between age and cholestrol in men and women. we can say that as the age increases the change in cholestrol in women is more then the men.**","936e47d8":"**Below i have created function to update the parameters W,b as per their gradients dW,db computed in the last step**","4958c2a3":"**Kindly comment if you find any mistake and give addvice as it will be a great honour to learn from you.**  ","6dbed556":"**Now below i have shown distribution of age over database.**","d9cbcad6":"**Above we can see that as the test size decreases or we can say train size increases accuracy increases upto some point.but sometimes after that at less values of test set the model underfits as the no. of example increases for the same no. of feature. so we should select value of test size so that model doesnt underfit. here we should take test size around 0.15 and 0.20 so that the model doesnt underfit and gives best accuracy most of the time.**","37f3feda":"****Observing the data****","99743c4a":"**Below i have made a function for forward propogation in neural network. I have loaded parameters from dictionery parameters and used the equation from the course that i have learned in deep learning. activation function for hidden layer is tanh function and that for the output layer is sigmoid function as this is the binary classification.**","286f4de9":"**Observing the target as it is the output column which shows us if the patients had the heart dieases or not.here we are observing target values and we can observe that we have more people with cases having heart dieases then people not having heart dieases in our data.**","2ba5be51":"**Below i have created a function for backward propogation that will compute the gradients of the function**","d45dc2f2":"**Below i have made a fuction to compute the cost. its equation is also taken from the course i have learnt in deep learning**","cbf80ca2":"**Below i have taken different test size as a list\nthen i have applied logistic regression and i have printed accuracy score for different size of test set**\n","28553d27":"**Below i have defined a function to randomly initalize the parameter and and also saved that parameters so that they can be used during back propogation**","a025b7ad":"**Now lets try different test size as it will have a great impact on accuracy of algorithm**","2b4a94b4":"**Below i have defined a function which gives you the number of nodes in the layer of neural network.\ni have taken the hidden layer nodes as 4 as per my previous experiences. you can any other values also**","c20f28c8":"Below i have imported all the library that we will need in this kernel.\n&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;****numpy****-for numerical operation\n&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;**panda**-for data wrangling \n&emsp;&emsp;&emsp;&emsp;&emsp;**matplotlib and seaborn**-for data visualisation\n&emsp;&emsp;&emsp;&emsp;&emsp;**sklearn**-to apply machine learning algorithm","94ce893b":"**Now lets try neural network for classification**\nBelow i have imported library that are used in algorithm","0ad89477":"**Now we will start applying machine learning.we would start with logistic regression**\n    ","a73ba205":"**By below violin plot shows the distribution of cholestrol level over the different age levels. we can see that here the highest cholestrol level are found in the agerange of 55-64.mostly the cholestrol level increases with the increase in the age.also there is change in distribution in male and women.**","b557bcef":"**Now droping the Tagret column as it the value that we want to predict from the input set. also we have dropped the agerange column which we created as the age column is included in the data. so there is no meaning of having another column with same feature.\nAlso we are splitting the data in test and train part.**","bcf1fa87":"****There is no missing data in the database****"}}