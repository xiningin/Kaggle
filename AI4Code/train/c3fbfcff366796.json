{"cell_type":{"661c3ff2":"code","2eb72318":"code","4d392774":"code","9a6e2fad":"code","9cd4a9c7":"code","83e9ea53":"code","2ec4b33f":"code","9f5f45cf":"code","1732b38b":"code","47a6dbe8":"code","8bdfb2c4":"code","6a9bf3af":"code","4ec24333":"code","69778de4":"code","3c1b766e":"code","1f863158":"code","07521f4f":"code","efe062bc":"code","a3923a09":"code","0a6fcbc3":"code","009bf22f":"markdown","cdcdc9e6":"markdown","5f6c8d39":"markdown","45b8049c":"markdown"},"source":{"661c3ff2":"! pip install pyod","2eb72318":"from sklearn.preprocessing import StandardScaler\nfrom scipy.stats import norm, skew\nimport matplotlib.pyplot as plt\nimport seaborn as sns","4d392774":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","9a6e2fad":"# import data from source\nimport pandas as pd\nsample_submission = pd.read_csv(\"..\/input\/house-prices-advanced-regression-techniques\/sample_submission.csv\")\ntest = pd.read_csv(\"..\/input\/house-prices-advanced-regression-techniques\/test.csv\")\ntrain = pd.read_csv(\"..\/input\/house-prices-advanced-regression-techniques\/train.csv\")","9cd4a9c7":"# Dataset: TRAIN  | Operation: Identify null values\nmissing_vals = pd.isnull(train).sum()\nmissing_vals = missing_vals[missing_vals > 0]\nplt.xticks(rotation=90)\nsns.barplot(x=missing_vals.index, y=missing_vals)\nplt.show()","83e9ea53":"# Dataset: TEST  | Operation: Identify null values\nmissing_vals = pd.isnull(test).sum()\nmissing_vals = missing_vals[missing_vals > 0]\nplt.xticks(rotation=90)\nsns.barplot(x=missing_vals.index, y=missing_vals)\nplt.show()","2ec4b33f":"# Dataset: TEST  | Operation: matrix to find the pattern of missingness in the dataset\nimport missingno as msno\n# msno.matrix(train.sample(500))\nmsno.matrix(train)","9f5f45cf":"#Dataset: Train  | Operation: Categorizing neumerical and categorical cols.\nn_cols = []\nc_cols = []\n\nn_cols = [col for col in train.columns if type(train[col][0]) is not str]\nc_cols = [col for col in train.columns if type(train[col][0]) is str]\n\n# Integer columns, which are categorical\ncat = ['MSSubClass', 'Alley', 'FireplaceQu', 'PoolQC', 'Fence', 'MiscFeature', 'MoSold'] # Handwork !\n\nc_cols += cat\nn_cols = list(set(n_cols) - set(cat) - set(['Id', 'SalePrice']))","1732b38b":"# Dataset: Train  | Operation: Checking skewness, before and after log transform\nsale_pr = pd.DataFrame({\"price\":train[\"SalePrice\"], \"log-price\":np.log1p(train[\"SalePrice\"])})\nsale_pr.hist()\ntmp = np.log1p(train[\"SalePrice\"])","47a6dbe8":"# Dataset: Train  | Operation: Mean and SD, before and after log transform\nprint(\"Before log transform:  Mean: %f, Standard Deviation: %f\" %norm.fit(train['SalePrice']))\nprint(\"After log transform: Mean: %f, Standard Deviation: %f\" %norm.fit(tmp))","8bdfb2c4":"# Dataset: Train  | Operation: features correlation with the target variable\nstorage = []\nfor col in n_cols:\n    na_idx = pd.isnull(train[col])\n    correlation = np.corrcoef(x= train[col][~na_idx], y=train['SalePrice'][~na_idx])[0,1]\n    storage.append((col, correlation))\nstorage.sort(key=lambda x : -abs(x[1]))\n\nstorage[:4]","6a9bf3af":"# Dataset: Train  | Operation: Graph plot of feature and SalesPrice, for top 15 correlated features.\nb = [p for (p,q) in storage[0:15]]\nN=15\n\nfig, ax = plt.subplots(int(np.ceil(N\/2)),2, figsize=(15,14*2))\nfor i, col in enumerate(b):\n    sns.scatterplot(data=train, \n             x=col, \n             y=\"SalePrice\", \n             alpha=0.4, \n             ax=ax[i\/\/2][i%2])\n    ax[i\/\/2][i%2].set_xlabel(col, fontsize=18)\n    ax[i\/\/2][i%2].set_ylabel('SalePrice', fontsize=18)\nplt.show()","4ec24333":"# Visuals for some correlated features.\n\nsns.set()\ncols = ['SalePrice', 'OverallQual', 'GrLivArea', 'GarageCars', 'TotalBsmtSF', 'FullBath', 'YearBuilt']\nsns.pairplot(train[cols], size = 2.5)\nplt.show()","69778de4":"# Dataset: Train  | Operation: correlation matrix for neumerical columns.\ncorr_matrix = train[n_cols].corr()\nsns.set(rc={'axes.facecolor':'white', 'figure.facecolor':'white'})\nf, ax = plt.subplots(figsize=(12, 9))\nsns.heatmap(corr_matrix, vmax=.8, square=True)","3c1b766e":"# Dataset: Train  | Operation: Correlated pairs where abs(corr) > 0.4 , for neumerical columns.\ntop_correlations = []\ntmp = corr_matrix[abs(corr_matrix)>0.4]\nfor col in tmp.columns:\n    for row in tmp[col][~pd.isnull(tmp[col])].index:\n        if col == row:\n            break\n        top_correlations.append((col,row, tmp[col][row]))\ntop_correlations.sort(key = lambda x : -x[2])\ntop_correlations[:5]","1f863158":"# Dataset: Train  | Operation: Outliers from GrLivArea\nsns.scatterplot(data=train, x='GrLivArea', y='SalePrice') \nplt.show()","07521f4f":"# Dataset: Train  | Operation: outlier detection in scaled and transformed target column\nscaler = StandardScaler()\nsc_tr = scaler.fit_transform(train[['SalePrice']])\nsc_tr_srt = sorted(np.squeeze(sc_tr))\nfor a, b in zip(sc_tr_srt[:10], sc_tr_srt[-10:]):\n    print('{} {} {}' .format(round(a, 5), ' '*10, round(b,5)))","efe062bc":"# Dataset: Train  | Operation: skewness of columns\nskews = []\nfor col in n_cols:\n    skews.append((col, skew(train[col])))\nskews.sort(key=lambda x : -abs(x[1]))","a3923a09":"# Dataset: Train  | Operation: plot before log transform\nfig, ax = plt.subplots(1,2, figsize=(15,5))\nsns.scatterplot(data=train, x='GrLivArea', y='SalePrice', ax=ax[0])\nsns.scatterplot(data=train, x='GarageArea', y='SalePrice', ax=ax[1])\nplt.show()","0a6fcbc3":"# Dataset: Train  | Operation: plot after log transform\nfig, ax = plt.subplots(1,2, figsize=(15,5))\ndat = train.copy()\ndat['SalePrice'] = np.log1p(dat['SalePrice'])\ndat['GrLivArea'] = np.log1p(dat['GrLivArea'])\ndat['MasVnrArea'] = np.log1p(dat['MasVnrArea'])\nsns.scatterplot(data=dat, x='GrLivArea', y='SalePrice', ax=ax[0])\nsns.scatterplot(data=dat, x='GarageArea', y='SalePrice', ax=ax[1])\nplt.show()","009bf22f":"# Exploratory Data Analysis:\n1. Generating insights.\n2. Suggest hypothesis about the underlying process that generated data.\n3. Validate assumptions about distribution\n4. Spot anomalies\n5. Identify irrelevant features\n\n\n# Import Libraries\nNumpy and Pandas are scientific computation libraries, used to manage data in frames and numbers.\nSeaborn and matplotlib are used to generate graphs for visualizing data.\n","cdcdc9e6":"# Identifying outliers","5f6c8d39":"### The below analysis is done with respect to Target column: SalePrice","45b8049c":"### All Columns Analysis"}}