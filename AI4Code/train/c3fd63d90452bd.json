{"cell_type":{"ae60c92a":"code","4f9161e7":"code","a16edc43":"code","39ac2e48":"code","a2a8085d":"code","68a3b8be":"code","63da2ef9":"code","56d5f644":"code","10878044":"code","7cfd2f79":"code","7a35a7e3":"markdown","260fe0de":"markdown","bc6c368a":"markdown","5df2b648":"markdown","35ac7b83":"markdown","8ab927c7":"markdown","4db6f566":"markdown","9c9a201f":"markdown","5520d251":"markdown","8d320ee3":"markdown","03920ea5":"markdown","6a660fac":"markdown","39a7bbc5":"markdown"},"source":{"ae60c92a":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt","4f9161e7":"train = pd.read_csv('..\/input\/tabular-playground-series-nov-2021\/train.csv')\ntrain_old = pd.read_csv('..\/input\/november21\/train.csv')","a16edc43":"y = train['target']\ny_old = train_old['target']","39ac2e48":"CS = 60000","a2a8085d":"is_flipped = (y != y_old)","68a3b8be":"num_flipped = np.zeros(10, dtype = int)\nfor i in range(10):\n    num_flipped[i] = np.sum(is_flipped[i*CS:(i+1)*CS])\n    print(num_flipped[i])","63da2ef9":"mu = np.mean(num_flipped)\nstd = np.std(num_flipped)\nprint(f'Mean is {mu:.0f} and standard deviation {std:.0f}')","56d5f644":"N = CS        # number of draws\np = mu\/N      # success probability\nq = 1 - p     # failure probability\n\nexpected_std = np.sqrt(p*q*N)  # see e.g. Wikipedia link above\nprint(f'Expected standard deviation is {expected_std:.0f}')","10878044":"plt.scatter(range(10), num_flipped, label = 'Training data')\nplt.axhline(mu, c = 'gray', label = 'Expected (68% c.l.)')\nplt.axhline(mu + expected_std, c = 'gray')\nplt.axhline(mu - expected_std, c = 'gray')\nplt.ylabel('Number of flips in the chunk')\nplt.xlabel('Chunk number');\nplt.legend(loc = 4);\nplt.ylim([14000, 16000]);","7cfd2f79":"std\/expected_std","7a35a7e3":"As discussed [here](https:\/\/www.kaggle.com\/c\/tabular-playground-series-nov-2021\/discussion\/286731) by [@grayjay](https:\/\/www.kaggle.com\/grayjay), the training data seems to be a combination of ten chunks of size 60k, each with slightly different distributions.","260fe0de":"For convenience:","bc6c368a":"Comparison of the observed numbers of flips in the ten training chunks with the expected mean and standard deviation (assuming a binomial distribution). The number of flips in chunks seems to fluctuate way more than it should..","5df2b648":"There is almost a factor four difference between the observed and expected standard deviation..","35ac7b83":"Calculate how many targets were flipped in each of the chunks","8ab927c7":"# Summary","4db6f566":"Basic statistics","9c9a201f":"Import competition train data, both after and before the reshuffling","5520d251":"Parameters of the [binomial distribution](https:\/\/en.wikipedia.org\/wiki\/Binomial_distribution) and standard deviation we would expect if the flips were randomly drawn from this distribution","8d320ee3":"# Import libraries, load data","03920ea5":"# Study random flips in the train set","6a660fac":"This is an array telling us which of the targets were flipped","39a7bbc5":"It has been [argued](https:\/\/www.kaggle.com\/motloch\/nov21-mislabeled-25) that about 25% of the target values in the test set have been randomly flipped.\n\nWe use the [known target values before flip](https:\/\/www.kaggle.com\/c\/tabular-playground-series-nov-2021\/discussion\/287047) to calculate flip probability in all [ten chunks discovered earlier](https:\/\/www.kaggle.com\/c\/tabular-playground-series-nov-2021\/discussion\/286731).\n\nIf the random flips really are independent of the feature values and chunks, we [would expect](https:\/\/en.wikipedia.org\/wiki\/Binomial_distribution) the number of flips in each chunk to be about \n$$\\approx 0.25 \\times 60000 = 15000,$$ \nwith standard deviation \n$$\\approx \\sqrt{0.25 \\times (1-0.25) \\times 60000} = 106.$$\n\nWe observe chunk-to-chunk standard deviation of about 400. Assuming our calculation is correct, this suggest there might be a weak feature dependence to the flip probability (as the chunks have different feature distributions). This could also be driven by something related to the process of chunking - for example each chunk might have a slightly different flip probability (but independent of the features)."}}