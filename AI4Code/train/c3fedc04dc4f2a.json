{"cell_type":{"072e2902":"code","9402dba7":"code","d543ce45":"code","e44ad8d4":"code","aac81c88":"code","ca510bf7":"code","4edde2cc":"code","f6b0b1fb":"code","ee5773c4":"code","bf1c13af":"code","4e7a1b23":"code","c767a2c8":"code","99daa857":"code","48fa9fc2":"code","1ea39fee":"code","d601f24b":"code","3aca5fa5":"code","6a2491ce":"code","17a4ad93":"code","1fdc0a0d":"code","c33075ab":"code","1cf3a6e5":"code","6d85314e":"code","654b9e2d":"code","16c1a072":"code","4b7eb233":"code","4461ce44":"code","f40557c8":"code","0c5855de":"code","2edd6059":"code","4abaca75":"code","45573c10":"code","786ca61d":"code","68f4bc97":"code","7af42516":"code","0243e4cc":"code","623b0f6c":"code","ec7d66a1":"markdown","035c038b":"markdown","65ed55ec":"markdown","5a7ebba6":"markdown","7854dd9e":"markdown","239363f8":"markdown","0a46040d":"markdown","ab62db68":"markdown","73ee5c45":"markdown","757a0b33":"markdown","bbc54ac6":"markdown","e44a2f0a":"markdown","c72f88fc":"markdown","1da58234":"markdown","9806341e":"markdown","7517677c":"markdown","2d0a0400":"markdown","7cbae0ed":"markdown","9c2b5773":"markdown","2ee9ab58":"markdown","96d9a4fa":"markdown","e0710624":"markdown","d810186c":"markdown","91caa45b":"markdown","30403f22":"markdown","e0ad25c4":"markdown","1c765522":"markdown","9a07eb3f":"markdown","4dae3e4a":"markdown","93582f29":"markdown","0eee36b5":"markdown","ed1c0a16":"markdown","32385a42":"markdown","7a12627d":"markdown","ea3ae99e":"markdown"},"source":{"072e2902":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session\n\n\n#Libraries used for preprocessing data\/exploratory data analysis\nimport numpy as np  \nimport pandas as pd  \nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n#Importing models used in the notebook\nfrom sklearn.ensemble import RandomForestClassifier\nfrom xgboost import XGBClassifier\nimport statsmodels.api as sm \n\n#Importing objects to help tune hyperparameters and create train\/validation split\nfrom sklearn.model_selection import RandomizedSearchCV, GridSearchCV, train_test_split\n\n#Importing objects used for cross-validation\nfrom sklearn.model_selection import StratifiedKFold\n\n#Importing metrics used to analyze model performance\nfrom sklearn.metrics import confusion_matrix, classification_report, auc, roc_curve, plot_roc_curve, roc_auc_score\n\n#Importing libraries used for feature importance \nimport eli5\nfrom eli5.sklearn import PermutationImportance\nfrom pdpbox import pdp, get_dataset, info_plots\n\n#Load the data\ntrain_data = pd.read_csv('\/kaggle\/input\/titanic\/train.csv')\ntest_data = pd.read_csv('\/kaggle\/input\/titanic\/test.csv')\nall_data = pd.concat([train_data, test_data], axis = 0)\n\n\nsns.set()  #Setting the style of plots to seaborns default style\n%matplotlib inline  #Plots will automatically show","9402dba7":"print(train_data.info())\ntrain_data.head(5)","d543ce45":"test_data.info()","e44ad8d4":"all_data.describe(include = 'all') #include is used to include categorical variables such as Sex","aac81c88":"#Table showing correlation values amongst features.\ntrain_data.corr()","ca510bf7":"#Heatmap of the correlated table - useful for when there are lots of features - easier to read.\nplt.figure(figsize=(8,8))\nsns.heatmap(train_data.corr(), cmap = 'viridis', annot = True, square = True, lw=1, linecolor='black')","4edde2cc":"#Percentage of survivors\ntotal_pass = 100 * train_data[train_data['Survived'] == 0]['Survived'].count()\/train_data['Survived'].count() #% of Passed \ntotal_surv = (100 - total_pass) #Percentage of survivors\nprint(f\"% of passengers who passed: {total_pass.round(2)}% \\n% of passengers who survived: {total_surv.round(2)}%\")\n\n#Plotting Number of survivors\nsns.countplot(x = 'Survived', data = train_data)","f6b0b1fb":"#Creating the axes for the plots\nfig, axes = plt.subplots(nrows=1, ncols=3, figsize=(22,10))\n\n#Plotting the target distribution of the categorical features - excluding Name and Ticket \n#Feature engineering for these are covered in section 2\nsns.countplot(x='Sex', data=train_data, hue = 'Survived', ax=axes[0])\naxes[0].set_xlabel('Sex', fontsize=15)\nsns.countplot(x='Embarked', data=train_data, hue='Survived', ax=axes[1])\naxes[1].set_xlabel('Embarked', fontsize=15)\nsns.countplot(x='Pclass', data=train_data, hue='Survived', ax=axes[2])\naxes[2].set_xlabel('Pclass', fontsize=15)\n\n#Storing PassengerID for later use in submission CSV file\ntest_PassengerID = test_data['PassengerId']\n\n#Dropping PassengerID and Name because they will no longer be used.\n#Cabin and Ticket help in understanding missing data in section 3 and will therefore not be dropped yet.\ntrain_data.drop(['PassengerId'], axis=1, inplace=True)\ntest_data.drop(['PassengerId'], axis=1, inplace=True)","ee5773c4":"fig, axes = plt.subplots(nrows=2, ncols=2, figsize=(15,10))\n\n#Plotting the feature Age\ntrain_data[train_data['Survived'] == 0]['Age'].plot(kind = 'hist', bins = 20, color = 'blue', label = 'Passed', ax = axes[0][0])\ntrain_data[train_data['Survived'] == 1]['Age'].plot(kind = 'hist', bins = 20, color = 'orange', alpha = 0.7, label = 'Survived', ax = axes[0][0])\naxes[0][0].legend()\naxes[0][0].set_xlabel('Age Dist.', fontsize=15)\n\n#Defining the 95th quantile of ticket prices & plotting the feature Fare\nquant_95 = np.quantile(train_data['Fare'], 0.95)\ntrain_data[(train_data['Survived']==0)&(train_data['Fare']<quant_95)]['Fare'].plot(kind = 'hist', bins = 20, color = 'blue', label = 'Passed', ax=axes[0][1])\ntrain_data[(train_data['Survived']==1)&(train_data['Fare']<quant_95)]['Fare'].plot(kind = 'hist', bins = 20, color = 'orange', alpha = 0.7, label = 'Survived', ax=axes[0][1])\naxes[0][1].legend()\naxes[0][1].set_xlabel('Fare Dist.', fontsize=15)\n\n#Plotting Sibsp\nsns.countplot('SibSp', data = train_data, hue='Survived', ax=axes[1][0])\naxes[1][0].legend(loc='upper right')\naxes[1][0].set_xlabel('SibSp Dist.', fontsize=15)\n\n#Plotting Parch\nsns.countplot('Parch', data = train_data, hue='Survived', ax=axes[1][1])\naxes[1][1].legend(loc='upper right')\naxes[1][1].set_xlabel('Parch Dist.', fontsize=15)","bf1c13af":"#Extracting the prefix from the entire name - ex: Mr, Mrs, Capt, etc.\ntrain_data['Unique_Title'] = train_data['Name'].apply(lambda full_name: full_name.split(', ')[1].split('.')[0])\ntest_data['Unique_Title'] = test_data['Name'].apply(lambda full_name: full_name.split(', ')[1].split('.')[0])\n\n#Plotting survival rate for each title\nplt.figure(figsize=(20,7))\nsns.countplot('Unique_Title',data=train_data, hue='Survived')\nplt.legend(['Passed','Survived'], loc='upper right', prop={'size':15})","4e7a1b23":"#Grouping the 17 unique titles into the 4 categories - Mr, Miss\/Mrs, Job, Master\n#Titles Mr and Master don't need to be touched because they are their own category\ntrain_data['Title'] = train_data['Unique_Title']\ntrain_data['Title'] = train_data['Title'].replace(['Dr','Col','Major','Jonkheer','Capt','Sir','Don','Rev'], 'Job')\ntrain_data['Title'] = train_data['Title'].replace(['Miss','Mrs','Ms','Mlle','Lady','Mme','the Countess','Dona'], 'Miss\/Mrs')\ntest_data['Title'] = test_data['Unique_Title']\ntest_data['Title'] = test_data['Title'].replace(['Dr','Col','Major','Jonkheer','Capt','Sir','Don','Rev'], 'Job')\ntest_data['Title'] = test_data['Title'].replace(['Miss','Mrs','Ms','Mlle','Lady','Mme','the Countess','Dona'], 'Miss\/Mrs')\n\n#Plotting 4 categories \nplt.figure(figsize=(16,7))\nsns.countplot('Title', data=train_data, hue = 'Survived')","c767a2c8":"#Creating boolean feature Married\ntrain_data['Married'] = train_data['Unique_Title']=='Mrs'\ntest_data['Married'] = test_data['Unique_Title']=='Mrs'\n\n#Plotting new feature married\nsns.countplot('Married', data=train_data, hue='Survived')\n\n#Dropping columns that are not needed anymore - Name, Unique_Title\ntrain_data.drop(['Name', 'Unique_Title'], axis=1, inplace=True)\ntest_data.drop(['Name', 'Unique_Title'], axis=1, inplace=True)","99daa857":"#Plotting SibSP and Parch to see distribution of target\nfig, axs = plt.subplots(ncols = 3, nrows = 1,figsize=(18,6))\naxs[0].set_title('Siblings\/spouses', size=15)\naxs[1].set_title('Parents\/children', size=15)\nsns.countplot('SibSp', data = train_data, ax=axs[0], hue='Survived')\nsns.countplot('Parch', data = train_data, ax=axs[1], hue='Survived')\n\n#Adding SibSp and Parch to create Fam_Size\ntrain_data['Fam_Size'] = train_data['SibSp'] + train_data['Parch']\ntest_data['Fam_Size'] = test_data['SibSp'] + test_data['Parch']\n\n#Making function to bin family sizes\ndef grouping(fam_size):\n    if fam_size == 0:\n        return 'Individual'\n    elif 1 <= fam_size <= 2:\n        return 'Small Family'\n    elif 3 <= fam_size <= 5:\n        return 'Medium Family'\n    else:\n        return 'Large Family'\n\n#Applying grouping function to Fam_Size\ntrain_data['Fam_Size'] = train_data['Fam_Size'].apply(grouping)\ntest_data['Fam_Size'] = test_data['Fam_Size'].apply(grouping)\n\n#Plotting new feature Fam_Size\naxs[2].set_title('Family Size', size=15)\nsns.countplot('Fam_Size', data=train_data, hue='Survived', ax=axs[2])\n\n#Dropping Parch and SibSp\ntrain_data.drop(['SibSp','Parch'], axis=1, inplace=True)\ntest_data.drop(['SibSp','Parch'], axis=1, inplace=True)","48fa9fc2":"#Displaying the median for each class while considering sex \nall_data.groupby(['Pclass', 'Sex'])['Age'].median()","1ea39fee":"#Filling the missing age datapoints with the median for each class while considering sex\n#Using the population data to fill the missing data because it will give the most accurate representation for age\nall_data['Age'] = all_data.groupby(['Pclass', 'Sex'])['Age'].transform(lambda age: age.fillna(age.median()))\n\n#Now move data from combined dataframe to training and test datasets.\ntrain_data['Age'] = all_data.iloc[0:891]['Age']\ntest_data['Age'] = all_data.iloc[891:]['Age']","d601f24b":"#Finding the specific two passengers who are missing from embarked\ntrain_data[train_data['Embarked'].isnull()]","3aca5fa5":"#Finding the most frequent port women in first class embarked from (use all the data to get accurate representation)\nembarked_mode = all_data[(all_data['Sex']=='female') & all_data['Pclass']==1]['Embarked'].value_counts().index[0]\n\n#Filling the missing values\ntrain_data['Embarked'] = train_data['Embarked'].fillna(embarked_mode)\n\n#Features Cabin and Ticket have not been dropped yet because they helped find information about the missing embarked data\n#They completed their purpose and may now be dropped\ntrain_data.drop(['Cabin', 'Ticket'], axis=1, inplace=True)\ntest_data.drop(['Cabin', 'Ticket'], axis=1, inplace=True)","6a2491ce":"#Displaying the missing fare data point\ntest_data[test_data['Fare'].isnull()]","17a4ad93":"#Finding the median for fare, only considering passengers in 3rd class\nclass3_fare_median = all_data[all_data['Pclass']==3]['Fare'].median()\n\n#Filling the missing data point with the median\ntest_data['Fare'] = test_data['Fare'].fillna(class3_fare_median)","1fdc0a0d":"#Define the categories that need to be onehotencoded\none_hot_feat = ['Sex', 'Embarked', 'Title', 'Married', 'Fam_Size']\n\n#Use pandas get_dummies to onehot encode the features for training\/test sets\n#drop_first is on to eliminate multicollinearity\nOH_features_train = pd.get_dummies(train_data[one_hot_feat], drop_first=True)\nOH_features_test = pd.get_dummies(test_data[one_hot_feat], drop_first=True)\n\n#Concatenate onehot encoded features to training\/test sets\ntrain_data = pd.concat([train_data,OH_features_train],axis=1)\ntest_data = pd.concat([test_data,OH_features_test],axis=1)\n\n#Drop the categorical features that are not one hot encoded\ntrain_data.drop(one_hot_feat, axis=1, inplace = True)\ntest_data.drop(one_hot_feat, axis=1, inplace = True)","c33075ab":"#Defining X and y\nX_train = train_data.drop('Survived',axis=1)\nX_test = test_data\ny = train_data['Survived']\n\n#Defining the seed used for all random_state arguments\nSEED=42\n\n#Making arrays of the X and y dataframes - Used for cross-validation in ensemble models\nX_train_np_array = X_train.values\ny_np_array = y.values\nX_test_np_array = X_test.values\n\n#Creating train\/test splits of the X and y dataframes - used for validation\/predictions in the logit model\nX_train_split, X_val_split, y_train_split, y_val_split = train_test_split(X_train, y, test_size=0.2, random_state=SEED)","1cf3a6e5":"#Statsmodels does not automatically add an intercept with their logit model - do this manually\nlogit_X_train_with_intercept = sm.add_constant(X_train_split)\n\n#Fit the model to the training data\nlogit = sm.Logit(y_train_split, logit_X_train_with_intercept).fit()\n\n#Statsmodels provides a nice summary of the model, with p-values\nlogit.summary()","6d85314e":"#Must add intercept to validation data before using logit model\nlogit_X_val_with_intercept = sm.add_constant(X_val_split)\n\n#Calculating probabilities of survival for each passenger\nlogit_X_val_prob = logit.predict(logit_X_val_with_intercept)\n\n#Make function to round probabilities to finalized answers. Cutoff point is: P(X) = 0.5.\ndef logistic(Column):\n    if Column <= 0.5:\n        return 0\n    else: \n        return 1\n\n#Apply function to get validation data predictions\nlogit_X_val_pred = logit_X_val_prob.apply(logistic)","654b9e2d":"#Creating a function to report metrics for logit model\n#Function will return confusion matrix, classification report, ROC curve and ROC-AUC Score\ndef metrics(model_target_probabilities, model_target_predictions):\n    print('Confusion Matrix:\\n', confusion_matrix(y_val_split, model_target_predictions))\n    print('\\nClassification Report:\\n', classification_report(y_val_split, model_target_predictions))\n        \n    #Creating tpr and fpr for ROC curve\n    false_pos_rates, true_pos_rates, thresholds = roc_curve(y_val_split, model_target_probabilities)\n    \n    #Calculating ROC_AUC score\n    model_roc_auc_score = auc(false_pos_rates, true_pos_rates).round(4)\n    print(\"\\n\\nAUC_Score: \", model_roc_auc_score)\n    \n    \n    #Plotting ROC Curve\n    plt.figure(figsize=(15,10))\n    plt.ylabel('TPR', size=20)\n    plt.xlabel('FPR', size=20)\n    plt.title('ROC Curve', size = 35)\n    plt.plot([0, 1], [0, 1], linestyle='--', lw=2, color='r', alpha=0.8, label='Random Guessing')\n    sns.lineplot(x=false_pos_rates,y=true_pos_rates, label=f'ROC Curve (AUC = {model_roc_auc_score})')\n    plt.legend(loc='lower right')\n    \nmetrics(logit_X_val_prob, logit_X_val_pred)","16c1a072":"#Make predictions for submission - add intercept to test data\nlogit_test_with_intercept = sm.add_constant(X_test)\nlogit_test_prob = logit.predict(logit_test_with_intercept)\nlogit_test_pred = pd.DataFrame(logit_test_prob.apply(logistic))\n\n#Save predictions into a CSV that can be uploaded to kaggle\nlogit_entry = pd.concat([test_PassengerID,logit_test_pred],axis=1)\nlogit_entry.columns = ['PassengerId', 'Survived']\nlogit_entry = logit_entry.set_index('PassengerId')\nlogit_entry.to_csv('Logit Entry')","4b7eb233":"#Define a range of values for all the hyperparameters being optimized\nn_est = [int(x) for x in np.linspace(start = 100, stop = 2000, num = 20)]\nmax_features = ['auto', 'sqrt']\nmax_depth = [int(x) for x in np.linspace(1, 20, num = 10)]\nmax_depth.append(None)  #Manually add no limit for tree depth - this is standard input\nmin_samples_split = [2,4,6,8]\nmin_samples_leaf = [1,2,3]\n\n#Create the random grid by creating a dictionary of the inputs.\nrf_param_grid = {'n_estimators':n_est,\n                'max_features':max_features,\n                'max_depth':max_depth,\n                'min_samples_split':min_samples_split,\n                'min_samples_leaf':min_samples_leaf}\n\n#Create the model\nrf = RandomForestClassifier()","4461ce44":"#Instantiate RandomizedSearchCV object\nrf_random = RandomizedSearchCV(estimator=rf, param_distributions=rf_param_grid,\n                               n_iter=200, cv=3, verbose=2, random_state=SEED, n_jobs=-1)\n\n#Fit the data to the object and perform search\nrf_random.fit(X_train,y)\n\n#Best parameters\nrf_best_parameters = rf_random.best_params_\nprint('Best parameters:\\n', rf_best_parameters)","f40557c8":"#Define a range of values for all the hyperparameters being optimized\nn_est = [int(x) for x in np.linspace(start = 300, stop = 700, num = 16)]  #Intervals of 25\nmax_features = ['auto', 'sqrt']\nmax_depth = [int(x) for x in np.linspace(15, 25, num = 10)]  \nmax_depth.append(None)  #Manually add no limit for tree depth - this is standard input\nmin_samples_split = [5,6,7]\nmin_samples_leaf = [2,3,4]\n\n#Create the random grid by creating a dictionary of the inputs.\nrf_param_grid = {'n_estimators':n_est,\n                'max_features':max_features,\n                'max_depth':max_depth,\n                'min_samples_split':min_samples_split,\n                'min_samples_leaf':min_samples_leaf}\n\n\n#Create the model\nrf = RandomForestClassifier()\n\n\n#Instantiate GriddSearchCV object\nrf_random = GridSearchCV(estimator=rf, param_grid=rf_param_grid, cv=3, verbose=2, n_jobs=-1)\n\n#Fit the data to the gridsearch object and perform search\nrf_random.fit(X_train,y)\n\n#Best parameters\nrf_best_parameters = rf_random.best_params_\nprint('Best parameters:\\n', rf_best_parameters)","0c5855de":"#Create the model with the optimized variables\nrf = RandomForestClassifier(n_estimators=353, max_depth=17, max_features='auto', \n                            min_samples_leaf=3, min_samples_split=7,random_state=SEED)\n\n#Fit the model to the training data and get validation predictions\/probability which are used for classification metrics\nrf.fit(X_train_split, y_train_split)","2edd6059":"#Creating a function that crossvalidates data for a model and reports ROC Curve\n#Function takes the following as arguments: number of splits for cv, model optimized w\/ h.param., modelname (for title of ROC_curve)\ndef cv_roc_score(num_of_splits, model, title):\n\n    #First define the cross-validation object and the model\n    cv = StratifiedKFold(n_splits=num_of_splits)\n    classifier = model\n\n    #Next create place holders for the true positive rates and AUC scores for each experiment\/split\n    tprs = []\n    aucs = []\n    mean_fpr = np.linspace(0, 1, 100)\n\n\n    #Creating plot and iterating to find tpr and AUC score for each split\n    fig, ax = plt.subplots(figsize=(15,15))\n    for i, (train, test) in enumerate(cv.split(X_train_np_array, y_np_array)):\n        classifier.fit(X_train_np_array[train], y_np_array[train])\n        viz = plot_roc_curve(classifier, X_train_np_array[test], y_np_array[test],\n                             name='ROC fold {}'.format(i),\n                             alpha=0.3, lw=1, ax=ax)\n        interp_tpr = np.interp(mean_fpr, viz.fpr, viz.tpr)\n        interp_tpr[0] = 0.0\n        tprs.append(interp_tpr)\n        aucs.append(viz.roc_auc)\n\n    #Plotting line that represents ROC curve if random guessing were implemented \n    ax.plot([0, 1], [0, 1], linestyle='--', lw=2, color='r',\n            label='Chance', alpha=.8)\n\n    #Creating the ROC curves, mean AUC score and standard deviation of the AUC score\n    mean_tpr = np.mean(tprs, axis=0)\n    mean_tpr[-1] = 1.0\n    mean_auc = auc(mean_fpr, mean_tpr)\n    std_auc = np.std(aucs)\n    ax.plot(mean_fpr, mean_tpr, color='b',\n            label=r'Mean ROC (AUC = %0.2f $\\pm$ %0.2f)' % (mean_auc, std_auc),\n            lw=2, alpha=.8)\n\n    #Creating grey space for standard deviation of ROC curve\n    std_tpr = np.std(tprs, axis=0)\n    tprs_upper = np.minimum(mean_tpr + std_tpr, 1)\n    tprs_lower = np.maximum(mean_tpr - std_tpr, 0)\n    ax.fill_between(mean_fpr, tprs_lower, tprs_upper, color='grey', alpha=.2,\n                    label=r'$\\pm$ 1 std. dev.')\n\n\n    #Setting axis limits, title and legend\n    ax.set_xlim([-0.05,1.05])\n    ax.set_ylim([-0.05,1.05])\n    ax.set_title(f\"ROC Curve - {title}\", fontsize=20)\n    ax.legend(loc=\"lower right\")\n\n    \n    \n#Defining paramters for cv_roc_score function\nnum_splits = 5\ntitle = 'RandomForestClassifier'\nrf_classifier = RandomForestClassifier(n_estimators=353, max_depth=17, max_features='auto', \n                                min_samples_leaf=3, min_samples_split=7,random_state=SEED)\n\n#Implementing cv_roc_score function\ncv_roc_score(num_of_splits=num_splits, model=rf_classifier, title=title)","4abaca75":"#Make a function that creates cross-validation predictions\n#The function will take 2 arguments: the number of splits used in cross-validation, the model\n#Function will return predictions as a dataframe \ndef cv_pred(num_of_splits, model):\n    #Creating a dataframe to store the probabilities from each split\n    cv_prob_np_array = np.zeros(shape=(X_test.shape[0],num_of_splits))\n    cv_prob = pd.DataFrame(cv_prob_np_array, columns=[f'Fold_{split}' for split in range(num_of_splits)])\n    \n    #First define the cross-validation object and the model\n    cv = StratifiedKFold(n_splits=num_of_splits)\n    classifier = model\n    \n    for split, (train, test) in enumerate(cv.split(X_train_np_array, y_np_array)):\n        classifier.fit(X_train_np_array[train], y_np_array[train])\n        cv_prob.loc[:,f'Fold_{split}'] = classifier.predict_proba(X_test_np_array)[:,1]\n    \n    avg_prob = cv_prob.mean(axis=1)\n    \n    #Create function to round probabilities into predictions\n    def predictions(Column):\n        if Column <= 0.5:\n            return 0\n        else: \n            return 1\n    \n    #Use function to make predictions\n    final_predictions = avg_prob.apply(predictions)\n    \n    return final_predictions\n\n\n\n#Defining paramters for cv_pred function\nnum_splits = 5\ntitle = 'RandomForestClassifier'\nrf_classifier = RandomForestClassifier(n_estimators=353, max_depth=17, max_features='auto', \n                                min_samples_leaf=3, min_samples_split=7,random_state=SEED)\n\n#Implementing cv_pred function\nrf_predictions = cv_pred(num_of_splits=num_splits, model=rf_classifier)","45573c10":"#Gathering and cleaning data for submission\nrf_entry = pd.concat([test_PassengerID, rf_predictions],axis=1)\nrf_entry.columns = ['PassengerId', 'Survived']\nrf_entry = rf_entry.set_index('PassengerId')\nrf_entry.to_csv('Random Forest Entry')","786ca61d":"#Define a range of values for all the hyperparameters being optimized\neta = [0.05,0.1,0.15,0.2,0.25]\nn_est = [int(x) for x in np.linspace(start = 100, stop = 1000, num = 18)] #Increments of 50\nmax_depth = [int(x) for x in range(1,11)]\nmax_depth.append(None)  #Manually add no limit - this is standard input\nmin_child_weight = [int(x) for x in range(1,6,2)]\ngamma = [x\/10.0 for x in range(0,5)]\n\n#Create the random grid by creating a dictionary of the inputs.\nxgb_param_grid = {'learning_rate':eta,\n                'n_estimators':n_est,\n                'max_depth':max_depth,\n                'min_child_weight':min_child_weight,\n                'gamma':gamma}\n\n\n#Create the model\nxgb = XGBClassifier()\n\n\n#Instantiate RandomizedSearchCV object with model and parameters\nxgb_random = RandomizedSearchCV(estimator=xgb, param_distributions=xgb_param_grid,\n                               n_iter=200, cv=3, verbose=2, random_state=SEED, n_jobs=-1)\n\n#Fit the data to the object and perform search\nxgb_random.fit(X_train,y)\n\n#Best parameters\nxgb_best_parameters = xgb_random.best_params_\nprint('Best parameters:\\n', xgb_best_parameters)","68f4bc97":"#Define a range of values for all the hyperparameters being optimized\neta = [0.1,0.15,0.2,0.25,0.3]\nn_est = [int(x) for x in np.linspace(start = 700, stop = 1300, num = 12)] #Increments of 50\nmax_depth = [int(x) for x in np.linspace(start=3, stop=9, num=6)]\nmax_depth.append(None)  #Manually add no limit - this is standard input\nmin_child_weight = [int(x) for x in range(3,7,4)]\ngamma = [x\/10.0 for x in range(1,5)]\n\n#Create the random grid by creating a dictionary of the inputs.\nxgb_param_grid = {'learning_rate':eta,\n                'n_estimators':n_est,\n                'max_depth':max_depth,\n                'min_child_weight':min_child_weight,\n                'gamma':gamma}\n\n#Create the model\nxgb = XGBClassifier()\n\n\n#Instantiate GriddSearchCV object\nxgb_random = GridSearchCV(estimator=xgb, param_grid=xgb_param_grid, cv=3, verbose=2, n_jobs=-1)\n\n#Fit the data to the object and perform search\nxgb_random.fit(X_train,y)\n\n#Best parameters\nxgb_best_parameters = xgb_random.best_params_\nprint('Best parameters:\\n', xgb_best_parameters)","7af42516":"#Defining paramters for cv_roc_score function\nnum_splits = 5\ntitle = 'XGBClassifier'\nxgb_classifier = XGBClassifier(n_estimators=700, max_depth=6, gamma=0.4, eta=0.25, min_child_weight=3, random_state=SEED)\n\n#Implementing cv_roc_score function\ncv_roc_score(num_of_splits=num_splits, model=xgb_classifier, title=title)","0243e4cc":"#Defining paramters for cv_pred function\nnum_splits = 5\nxgb_classifier = XGBClassifier(n_estimators=700, max_depth=6, gamma=0.4, eta=0.25, min_child_weight=3, random_state=SEED)\n\n#Implementing cv_pred function\nxgb_predictions = cv_pred(num_of_splits=num_splits, model=xgb_classifier)","623b0f6c":"#Gathering and cleaning data for submission\nxgb_entry = pd.concat([test_PassengerID, xgb_predictions],axis=1)\nxgb_entry.columns = ['PassengerId', 'Survived']\nxgb_entry = xgb_entry.set_index('PassengerId')\nxgb_entry.to_csv('Extreme Gradient Boost Entry')","ec7d66a1":"Now that the hyper parameters have been defined in a grid and the model has been created, the RandomizedSearchCV object must be instantiated with the model and the parameter grid.\n\n**Don't run the cell below - it may take a while to complete running.** \n\nAfter running the random search object, the best parameters were determined to be: <br>\n{'max_depth': 17, <br>\n 'max_features': 'sqrt', <br>\n 'min_samples_leaf': 3, <br>\n 'min_samples_split': 6, <br>\n 'n_estimators': 500}","035c038b":"## 3.3 - Numerical Data\nThere are two numerical features that will be used `Age` and `Fare`. <br>\nWhen dealing with numerical data, it is a common practice to implement some form of transformation to standardize the data. This is known as feature scaling. However, for the models being considered in this notebook, feature scaling is not needed. In fact, leaving the data in it's raw state will allow for a more interpretable analysis of the `logit` model in section 4 - modelling. \n\nWhen left in it's raw form it will be possible to answer questions such as: <br>\n*What are the odds a passenger survives if the price of their ticket was 100 dollars compared to someone who paid 10 dollars?*\n\nHowever, when modelling numerical data, feature scaling can help to highlight the variables which are important automatically. It does this by punishing lowering weights of the non-significant variables - thus their effect on the model approximates to 0. \n\n\n# 4 - Modelling\nThis section will cover three different models:\n* Statsmodels `Logit`\n* ScikitLearn's `RandomForestClassifier` \n* XGBoost's `XGBoostClassifier`.\n\nLet's begin by creating the train\/validation split used to assess the models.","65ed55ec":"**Observations:**\n* `Age` appears to be normally distributed minus the following discrepancies.\n  * The distribution skews right.\n  * There are a considerable amount of children\/babies - the high number of children makes it appear as if there are 2 distributions - one for children and one for adults.\n* The general trend observed for `Fare` is that the higher the ticket price, the larger the number of survivors in that bin. \n  * This makes sense because first class passengers had the highest survival rate and the features `Fare` and `Pclass` had the highest correlation amongst all the features at -0.55. \n* The general trends seen in `Parch` and `SibSp` are the same.\n  * People that came alone - value of 0 - have the lowest survival rates.\n  * As number of relatives increases to values of 1-2, the survival rate increases.\n  * For large groups at values of 3+, the survival rate sharply decreases.\n  * Therefore the two features can be combined to form one feature about family size, this will be done in section 2 - Feature Engineering. \n\n\n# 2 - Feature Engineering\nLet's begin with a general description of feature engineering: <br>\nIt is the process of using domain knowledge to extract new features from raw data. \n\nIn this section feature engineering is performed on the original columns `Name`, `SibSp` and `Parch`.\n\n## 2.1 - `Title` and `Married`\nTwo new features called `Title` and `Married` are extracted from the original feature `Name`. As discussed in section 1.3, the registered passenger names have 17 unique prefixes or titles. This means the cardinality of the new feature `Title` is 17 - cardinality refers to how many categories there are for any particular feature. For the sake of processing times, it is important to lower the cardinality of this feature. In most cases, as cardinality increases, so does processing time. The cause behind this will be covered in section 3 - Preprocessing.\n\nLet's examine the target distribution within the feature `Title`.","5a7ebba6":"# 1 - Exploratory Data Analysis\nThis section provides provides an overview of the data and explains how the data imbalance will be dealt with. Also, it provides a more detailed analysis of the target and features. If you already have an understanding of the dataset skip to section 2 - Feature Engineering.\n\n## 1.1 - Overview","7854dd9e":"Now that predictions have been made by the model, they must be assessed. Below there is a function that reports the confusion matrix, classification report, ROC_AUC Score and ROC Curve.","239363f8":"The results turned out great, the cardinality of this feature has significantly decreased and it captures important trends amongst different titles. Now let's create the feature `Married`.","0a46040d":"## 4.1 - `Logit`\nStatsmodels is a Python library that can be used to create different statistical models and tests, such as the `Logit` model. The logit is derived from a logistic regression, which is used for binary classification. The logistic regression makes use of a sigmoid function that is responsible for calculating the probability of the target. The logit is simply an algebraic reoganization of the logistic model and it is done in a manner such that the regression is linear. \n\nlog(*odds*) = linear model <br>\nor<br>\nlog (P(X)\/(1-P(X)) = Bo + B1X1 + ... +BpXp\n\nWhere..\n* P(X) is the probability a passenger survives\n* 1-P(X) is the probability a passenger does not survive\n* Xo... Xp are features\n* Bo... Bp are coefficients\n\n### 4.1.1 - Fitting the `Logit` Model","ab62db68":"## 3.2 - One Hot Encoded Categorical Data\nThere are multiple categorical features that need to be one hot encoded including `Sex`, `Embarked`, `Title`,`Married` and `Fam_Size`. The other categorical feature `Pclass` is ordinal - there is no need to transform it. Ordinal data means there is a natural ordering to the categorical data.","73ee5c45":"## 4.2 - `RandomForestClassifier`\nRandomForest's are ensemble models, this means they are comprised of many \"weak learners\" or base models (Ex: Decision Tree). Scikit learn's `RandomForestClassifier` uses a technique called bagging. Bagging uses the bootstrap technique (random sampling with replacement) to train L weak learners. These weak learners can be trained in parallel (at the same time). Then the ensemble aggregates the results from the L weak learners in a specific way to produce a prediction. Depending on the type of problem at hand (regression or classification) there are different methods for aggregating the weak learner predictions to get the ensemble prediction. \n* For regression a common method is to use a simple average of the loss functions (error in each weak learner)\n* For classification there are 2 common ways to aggregate soft vote and hard vote.\n\nThere are many parameters to be considered when working with sklearn's `RandomForestClassifier`. Therefore, hyperparameter tuning must be implemented to create an ensemble that will produce more accurate predictions. To do this, use sklearn's `RandomizedSearchCV` object to iterate through different possible combinations of hyperparameters - we define the range for each parameter. During each iteration it uses training data to create predictions for the current hyperparameters and compares them to the target. It then returns the hyperparameters that performed the best. It does not iterate through every possible combination, the number of iterations is specified to save time. This object is meant to identify a hyper parameter values that make the model perform well, but the parameters can be optimized further.\n\nAfter using sklearn's `RandomizedSearchCV` to find hyperparameters that perform well, there is another sklearn object that can be used to further tune hyperparmaters. The object `GridSearchCV` can be used to evaluate ALL possible combinations the model is provided. A range of values is created around the values obtained from the random search. The new range of values will be placed in the grid search object to test all possible combinations. At the end of the process, the hyperparameters will be optimized. \n\n\n### 4.2.1 - Fitting the RandomForest Model & Tuning Hyperparameters\nHere is a quick overview of the hyperparamters that are being optimized. Note that all the hyperparameters that are being optimized are for the weak learners, none of the hyperparameters are modifying the ensemble - excluding n_est.\n* **n_est**: the number of weak learners in the ensemble (decision trees)\n* **max_features**: The number of features to consider when looking for the best split in a decision node\n* **max_depth**: Is the depth of the decision tree (length of longest path from root to leaf)\n* **min_samples_split**: The minimum number of samples required to split a node\n* **min_samples_leaf**: The minimum numer of samples required at each leaf node","757a0b33":"### 4.3.4 - Competition Submission\nThis code is provided for anyone who would like to make a submission with the extreme gradient boosting model - it achieved a score of .0.75598.","bbc54ac6":"**Comments**\n* The logit model is significantly different than a null model - this is evident by observing two things. \n  * The log likelihood > LL-null\n  * LLR p-value = 0.000 \n* Some variables are statistically significant while others are not, to determine which features are relevent the hypothesis test is employed. \n  * The null hypothesis for each feature states that the feature does not affect the targets - aka coefficient = 0. The level of significance (alpha) being considered is.. *\u03b1 = 0.05*.\n  * This means that if *p-value < \u03b1* then the null hypothesis is rejected and the weights are considered significant to the model.\n* With the summary statistics statsmodels provides you can make important insights about your data. Here is an example\n\n**Ex - Insight from model** <br>\n*What are the odds a passenger survives if the price of their ticket was 100 dollars compared to someone who paid 5 dollars?* <br>\nFare 1 = 100 <br>\nFare 2 = 5\n* Assumptions:\n  * The feature `Fare` is statistically significant: *p-value < \u03b1*\n  * All other features are kept the same.<br>\n  \nGeneral Model: log(odds) = 1.8558 + -0.9518x*Pclass* - 0.0109x*Age* + .... \n\nlog(odds1) = 1.8558 + 0.0025x*Fare1*<br>\nlog(odds2) = 1.8558 + 0.0025x*Fare2* <br>\n\n\nlog(odds1)-log(odds2) = 0.0025 x *\u0394Fare*\n\nodds1\/odds2 = exp(0.0025 x *\u0394Fare*)\n\nodds1 = 1.26 x odds2\n\nTherefore the odds of survival are 26% higher for a passenger who paid 100 dollars than a passenger who paid 5 dollars.","e44a2f0a":"# 5 - Conclusion\nIf you made it this far, congratulations on finishing the notebook! I hope this helped you understand some new data science concepts. **Please upvote the notebook!! It would help me out alot!**\n\nPS: If you have any notes or tips - please feel free to leave a comment or message me!","c72f88fc":"**Observations**\n* The most significant titles are Mr, Mrs, Miss and Master.\n* Mrs has the highest survival rate amongst all titles corresponding to women. \n* In most categories females have a higher survival rate than males.\n* The title Master, which are males under the age of 26, have an extremely high survival rate compared to other males.\n* The title Rev likely has the lowest survival rate amongst all titles.\n* Many of the job title prefixes do not have a significant amount of data.\n\n\n**Comments**<br>\n* Based on the observations from the graph, there is a reasonable to way to lower the cardinality of the feature `Title` while preserving the general trends observed in the data.\n  * Titles corresponding to men and women will be seperate - their survival rates are significantly different and this trend must be preserved.\n  * The title Master has the highest survival rate for any title corresponding to a man, this title must be made into its own category to capture this trend.\n  * The titles based on jobs will be the last category as they don't fit well into the male, female or master categories.\n  * By grouping into these 4 categories, the cardinality of the feature `Title` has been significantly lowered while capturing as much information as possible.\n* Due to the fact married women have the highest survival rate, it may be useful to create a boolean feature, `Married`, to indicate if a woman is married","1da58234":"### 4.3.3 - Predictions with XGBClassifier\nThe same function used to predict with the `RandomForestClassifier` will be implemented here.","9806341e":"Now that the optimal parameters have been found, use them in the `XGBoostClassifier`.\n\nThe best parameters were determined to be: <br>\n{'n_estimators': 700,<br>\n 'min_child_weight': 3,<br>\n 'max_depth': 6,<br>\n 'learning_rate': 0.25,<br>\n 'gamma': 0.4}","7517677c":"**Observations:**\n* The most significantly correlated feature to `Survived` is `Pclass` at a negative correlation of -0.34.\n* As expected, `Fare` is negatively correlated with `Pclass` at -0.55. \n\nThe overview has provided some good observations but it is always a good idea to investigate each feature and it's distribution with the target. By doing this, perhaps some relationships can be discovered between the features and the target. Also, it provides one with the opportunity to see if there is any preprocessing\/feature engineering that can be performed.\n\n\n## 1.2 - Data Imbalance\nIt's already been seen that the dataset is imbalanced, this section aims to cover issues that arise from it and how they can be dealt with.","2d0a0400":"### 4.1.4 - Competition Submission\nThis code is provided for anyone who would like to make a submission with the logit model - it achieved a score of 0.77511.","7cbae0ed":"### 4.1.2 - Validating the `Logit` Model\nWith the fit model, it is possible to make predictions using the validation data. After making predictions on the features of the validation data, it is possible to assess the predictions of the model using the known validation targets.\n\nThere are many different metrics for classification problems including accuracy, precision, recall, sensitivity, F1-score etc. However, the ROC curve and ROC-AUC score are the most important.\n\nAll of these metrics are based on the confusion matrix, which provides a summary of the models performance. It shows how many type 1 and type 2 errors the model made with its predictions and how many predictions were correct.<br>\nBased on this information, all the metrics mentioned above can be calculated.","9c2b5773":"**Target Dist. in Categ. Features - Observations**\n* There were approximately twice as many males aboard the titanic as there were females but the survival rates varied dramatically between the sexes.\n  * Approximately 1\/5 of all male passengers survived\n  * Approximately 3\/4 of all female passengers survived.\n* The survival rate decreases for each step down in socioeconomic class. \n  * There are considerably more 3rd class passengers than 1st and 2nd class passengers.\n* The majority of passengers left from South Hampton and they have the lowest survival rate.\n  * Fact: 50% of the people who boarded at South Hampton were third class passengers.\n  \n## 1.4 - Numerical Features\nThere are four numeric features being considered - `Age`, `Fare`, `Parch` and `SibSp`. \n\nLet's begin by examining the target distribution in the four features. It's known `Fare` is heavily skewed by expensive tickets. To get a better visual of the distribution the top 5% of the most expensive tickets will be dropped.","2ee9ab58":"### 3.1.3 - `Fare` Missing Data\nThere is only 1 missing data point from `Fare` in the test dataset, the passenger is a 60 year old male who is 3rd class and embarked from South Hampton. `Fare` was most highly correlated to `Pclass`, therefore it is reasonable to use the median of ticket prices for people who are in 3rd class.","96d9a4fa":"## 4.3 - `XGBClassifier`\nXGB stands for extreme gradient boosting, this is another type of ensemble model. Gradient boosting uses weak learners like the bagging technique does. However, the weak learners are fit in a different manner - it is an iterative process. Training\/fitting begins by initializing a single weak learner, after this point the iterative cycle continues until all weak learners have been fit to create the ensemble. \n* Step 1: Fit the first weak learner - after this the iterative cycle begins.\n* Step 2: Use the current ensemble to generate predictions for each observation in the dataset. The prediction of the current ensemble comes from predictions of ALL weak learners that have been trained. \n* Step 3: Use predictions to calculate loss function (Error function used to assess the model)\n* Step 4: Use gradient descent on loss function to determine parameters of the new model. These new parameters are used to fit the new model\n* Step 5: Add the new model to ensemble\n* Step 6: Repeat until completed\n\n\n### 4.3.1 - Fitting the Model\nThe process for fitting the XGBClassifier will be similar to fitting the random forest model, there are hyperparameters that need to be optimized before the model will perform well. To tune the hyperparameters the same methods will be implemented which entail the following.\n* General search for range of optimal hyperparameters using `RandomizedSearchCV`\n* Intensive search using `GridSearchCV`, which checks every possible combination to find optimal hyperparameters based on previous search.\n\nHere is a quick overview of the hyperparameters being optimized. Note that some parameters are optimizing the weak learners, while others are optimizing the aggregation portion of the ensemble.\n* **eta**: #Learning rate - makes model more robust by shrinking weights on each step\n* **n_est**: the number of weak learners in the ensemble (decision trees)\n* **max_depth**: Is the depth of the decision tree (length of longest path from root to leaf)\n* **min_child_weight**: Minimum sum of instance weight(hessian) needed in a child \n* **gamma**: #Minimum loss reduction required to make a further partition on a leaf node of the tree.\n\n**Don't run the cell below - it will take a while to complete running.** \n\nThe best parameters were determined to be: <br>\n{'n_estimators': 1000,<br>\n 'min_child_weight': 5,<br>\n 'max_depth': 6,<br>\n 'learning_rate': 0.25,<br>\n 'gamma': 0.3}","e0710624":"### 4.2.4 - Competition Submission\nThis code is provided for anyone who would like to make a submission with the random forest model - it achieved a score of 0.79425.","d810186c":"### 3.1.2 - `Embarked` Missing Data\nThere are 2 missing values from `Embarked` in the training dataset. They are both women who have the same ticketnumber, reside in the same cabin and are both first class passengers. This means they came together, however there is no way to know which port they left from. The best way to estimate which port they departed is to use the most frequent occurance for female passengers in first class, aka the mode.","91caa45b":"Now that suboptimal parameters have been found with the `RandomizedSearchCV` object, the optimal parameters can be found using the `GridSearchCV` object. Again, the grid search must be instantiated with a base model and grid paramaters. The cell below does this.\n\n**Don't run the cell below - it will take a while to complete running.** \n\nAfter running the grid search object, the best parameters were determined to be: <br>\n{'max_depth': 17, <br>\n 'max_features': 'auto', <br>\n 'min_samples_leaf': 3, <br>\n 'min_samples_split': 7, <br>\n 'n_estimators': 353}","30403f22":"Now that the optimal parameters have been found, use them in the `RandomForestClassifier`.\n\nThe best parameters were determined to be: <br>\n{'max_depth': 17, <br>\n 'max_features': 'auto', <br>\n 'min_samples_leaf': 3, <br>\n 'min_samples_split': 7, <br>\n 'n_estimators': 353}","e0ad25c4":"The results turned out great, the new feature `Fam_Size` manages to capture the same trends that are present in the features `SibSp` and `Parch`.\n* Individuals have a much lower survival rate\n* Small to medium families had a higher survival rate\n* Larger families seem to have a lower survival rate as well.\n\n# 3 - Preprocessing \nThis section will cover a series of preprocessing steps - handling missing data, converting categorical data into machine readable data. A very common technique used to convert categorical data is one hot encoding. One hot encoding creates a binary column for each category of a specific feature which indicates if the category is active. This is why it is important to lower the cardinality of the categorical features, if possible. The number of columns created is equal to the cardinality of the feature, this means with a high cardinality the number of columns dataset is being increased dramatically. This would significantly slow down processing times for the machine learning algorithms.\n\n## 3.1 - Handling Missing Data\nIt was discovered in section 1.1 - dataset overview - that the training set and test set both have missing data. <br>\nThe training set has missing data from the following columns - `Age`, `Column` and `Embarked`.<br>\nThe test set has missing data from the following columns - `Age`, `Column` and `Fare`.<br>\nThe techniques used to fill the missing data are based off descriptive statistics of the features.\n\n### 3.1.1 - `Age` Missing Data\nApproximately 20% of the data points are missing the feature `Age`. Socioeconemic status will be used to aid in filling the missing data because `Pclass` and `Age` were highly correlated. A robust measure, the median, will be used to fill in the missing age for men and women of each socioeconomic class. To be as accurate as possible the feature `Sex` will also be considered while using the median.\n\nThis method for filling data will capture the trend that younger passengers generally have a lower socioeconomic status while also considering the age difference in the sexes. ","1c765522":"### 4.2.2 - Cross-Validation of the RandomForest Model\nThe metric that will be used to assess the `RandomForestClassifier` is the ROC score. However, because this is a relatively small amount of data cross-validation will be employed. Cross-validation is a technique used to train models when there is not enough data to perform a MEANINGFUL train\/validation\/test split.\n\nA **general description** of cross validation: <br>\nThe data used to train the model is split into N chunks\/splits and subsequently there are also N experiments that must be performed. For each experiment, one of the chunks\/splits will be used as the validation set until all the chunks have been used as the validation set. The scores from each validation set are then used to assess the models performance.<br>\n\nFor example: <br>\nIf N = 5, the data is split 5 ways where each split accounts for 20% of the total training data, In experiment 1, the first split will be used as the validation data (which is used to make predictions and assess the models accuracy\/ROC score etc.). In experiment 2, the second split will be used as the validation set and this continues for N iterations. <br>\nAt the end of this process, there are N sets of results and these results can be used to assess the performance of the model - average, std. dev., etc..\n**There are many ways to split the data for the N splits - the general premise remains the same.**\n\n**NOTE** <br>\n* The data should be randomly shuffled before making the splits - thus insuring the data is independent. In this case, it is handled by the `RandomForestClassifier` object.\n* The model is trained on ALL of the training data and validation data - just at different points. This may lead to overfitting, so if there is an abundance of data this method should be avoided.\n\nSci-kit learn does provide a bunch of objects that can be used for cross-validation, the one that is used below is the `StratifiedKFold` object. To use this object, the data must be put into a numpy array.","9a07eb3f":"### 4.2.3 - Predictions with the RandomForest Model\nCross-validation technique will be implemented again to create predictions for the test dataset & submission. The strategy is to train the model using the same cross-validation split for consistency but calculate target probabilities instead of calculating metrics to assess the models performance. Target probabilities will be calculated **for every** split and then averaged out over N splits. \n\n**The resulting cross-validation predictions will be based on the average target probabilities of the N splits.**","4dae3e4a":"### 4.3.2 Cross-Validation of XGBClassifier\nThe same method used to cross-validate the random forest ensemble will be used for the `XGBClassifier`. The cv_roc_score function used before will now be implemented. It requires 3 parameters - the number of splits for cv, the model and the title for the curve.","93582f29":"# Introduction to the Titanic Dataset\n\nThis is a notebook that explores the titanic dataset in its entirety, it uses machine learning to create a model that predicts which passengers survived the Titanic shipwreck. It covers an array of data science concepts including exploratory data analysis (EDA), data preprocessing techniques, feature engineering and modelling. The goal of this notebook is to provide easy to consume data science knowledge for beginners.\n\nLet's begin by getting acquainted with the raw data features. All of the following information can be found under the titanic competition's data section.\n\n*  `Embarked:` is the port where they embarked\n   *  C = Cherbourg\n   *  Q = Queenstown\n   *  S = Southampton\n*  `Pclass`: A proxy for socio-economic status (SES)\n   *  1st = Upper \n   *  2nd = Middle\n   *  3rd = Lower \n*  `Sex:` Male or Female\n*  `Name:` Name of passenger\n*  `Age:` Age in years, fractional if less than 1. If the age is estimated, is it in the form of xx.5\n*  `SibSp:` Number of siblings\/spouses aboard the titanic. The dataset defines family relations in this way... \n   *  Sibling = brother, sister, stepbrother, stepsister\n   *  Spouse = husband, wife (mistresses and fianc\u00e9s were ignored)\n*  `Parch:` Number of parents\/children aboard the titanic, the dataset defines family relations in this way... \n   *  Parent = mother, father\n   *  Child = daughter, son, stepdaughter, stepson <br>\n   *  Some children travelled only with a nanny, therefore parch=0 for them.<br>\n*  `Fare:` is the passenger fare\t\n*  `Ticket:` is the ticket number of the passenger \n*  `PassengerID:` is the unique id of the row \n*  `Cabin:` is the cabin number of the passenger\n\nThe corresponding target is..\n\n*  **`Survived:`** \n   *  1 = yes\n   *  0 = no\n\n# Setup\nImport everything necessary for the notebook - titanic dataset and data science libraries.","0eee36b5":"**Comments:**\n* The training set has missing data from the following columns - `Age`, `Cabin` and `Embarked`.\n* The test set has missing data from the following columns - `Age`, `Cabin` and `Fare`.\n* The shape of the training set and test are (891, 12) and  (418, 11) respectively. \n  * This means the training set consist of 891 samples, 11 predictors and the target. The test set consists of 418 samples and 11 predictors which are used to predict corresponding targets that are submitted to kaggle. \n  *  This means that the training set accounts for 68% of the population data.","ed1c0a16":"**Observations:**\n* **The dataset is imbalanced** - more passengers did not survive the accident than those who did. The only possible values are 1 and 0 - the mean is 0.38.\n* `Pclass` shows there were more 'lower class' passengers than 'upper class' passengers aboard the titanic - observable by the mean.\n* `Sex` shows that there were more males than females aboard the titanic. \n* The interquartile range of `Age` shows the passengers are between the ages 21 - 39 and the average age of a passenger is ~30 years old.\n* `SibSp` and `Parch` at the 50th percentile are 0, this means 50% of the people aboard the titanic came with no family.\n* `Fare` is skewed heavily to the right. The average price of a ticket is higher than the 75th quantile. \n* `Embarked` shows that the majority of the titanics occupants embarked from South Hampton.","32385a42":"Now that suboptimal parameters have been found with the `RandomizedSearchCV` object, the `GridSearchCV` object will be used to find optimized hyperparameters. Again, the grid search must be instantiated with a base model and grid paramaters. The cell below does this.\n\n**Don't run the cell below - it will take a while to complete running.** \n\nThe best parameters were determined to be: <br>\n{'n_estimators': 700,<br>\n 'min_child_weight': 3,<br>\n 'max_depth': 6,<br>\n 'learning_rate': 0.25,<br>\n 'gamma': 0.4}","7a12627d":"This would have been a more useful feature if it were possible to distinguish which men were married as well, however it still may add value to the model.\n\n## 2.2 - `Fam_Size`\nA new feature called `Fam_Size` is extracted from the original features `SibSp` and `Parch`. `Fam_Size` will be created in the following manner:\n\n`Fam_Size` = `SibSp` + `Parch` \n\nThe new feature will include different sizes of families starting from individual passengers to large sized families. These categories are defined in the following way.\n\n* Individual:  0 family members\n* Small Family:  1 to 2 family members\n* Medium Family:  3 to 5 family members\n* Large Family:  6+ family members","ea3ae99e":"**Comments about imbalance:**\n* The people who survived are considered a minority class while those who did not survive are the majority class. Most datasets in the real world are imbalanced, similar to the titanic dataset. In such cases, the machine learning algorithms used for classification will not be optimized. This is because they are designed to learn around an equal number of samples for each class.\n* Another problem that can arise with an imbalanced dataset is that it can fool one into believing they created a \"good\" model. For example, imagine a binary classification problem similar to the titanic dataset where the targets are SO imbalanced such that the majority class accounts for 90% of the target while the minority class accounts for only 10% of the targets. This means that if you produce a model that ALWAYS guesses the majority class it will be accurate 90% of the time, however anyone could make this predicition which makes the model useless.\n* There are a few possible solutions to tackle this problem of imbalanced data...\n  * Selecting performance metrics, such as those that focus on the minority class.\n  * Implementing data preparation methods, such as those that attempt to re-balance the classes (Upsampling & Downsampling)\n\nThe technique that is implemented in this notebook is the prior - focusing on metrics that focus on the minority class.\n\n\n## 1.3 - Categorical Features\nThere are seven categorical features in the dataset - `Ticket`, `Embarked`, `Cabin`, `PassengerID`, `Pclass`, `Sex` and `Names`.\n\nHowever some of the features will not be useful for the model and will therefore be dropped. These are the reasons why they will not be included.\n* `PassengerID` is arbitrarily chosen and therefore will not be valuable to the model.\n* `Cabin` has a large amount of data missing, the training set is missing 687 entries and the test set is missing 327 entries. Some of this information can be found from researching the titanic online, cabins\/decks where passengers stayed are based on their socioeconomic class and fare. However, some decks had a mix of passengers from first\/second class and second\/third class, it would take too long to research this and fill the data manually. The other option is to assign decks\/cabins to passengers based on their socioeconomic class until each deck hits its capacity. But there would be no way to gaurantee that the passenger belonged in that deck (unless we go through the data manually) and the model may pickup on an inaccurate trend based on this. \n* `Ticket` has 681 unique entries, this means some passengers share the same ticket number. If passengers share the same ticket number they likely travelled together and stayed in the same cabin\/deck. However, passengers that travelled in groups are likely caught by the features `SibSp` and `Parch`. There may be some passengers who travelled together that are not family - however introducing a grouping based off ticket number will introduce repititive information to the model (from the groups of families). Any other information contained in `Ticket` would likely be captured by the other features. i.e.: Prefix for ticket could represent the port the passenger departed from.\n\nLet's focus on the target distribution in the categorical features that will be included in the model - `Sex`, `Embarked`, `Pclass` and `Name`\n\n**NOTE:**  \n* There are 17 unique titles that can be extracted from the feature `Name`. Feature engineering on `Name` will be covered in section 2."}}