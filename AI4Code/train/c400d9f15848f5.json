{"cell_type":{"b72b03bc":"code","2e1fdfe5":"code","123f6869":"code","8a88b82a":"code","cd6d8fe4":"code","e690dcb0":"code","41d14a2b":"code","beca1ddd":"code","0de5a7af":"code","0bbbecf0":"code","4ea735b1":"code","293d2b3a":"code","d5136eb2":"code","4a42d1d1":"code","e57a8ab5":"code","fca31908":"code","b7d58449":"code","12827af7":"code","c7af0592":"code","a7192de8":"code","492c5873":"code","8a84e547":"code","fb6b4008":"code","bb01fd65":"code","5b093c9e":"code","0383c636":"code","d327f264":"code","3c79ac60":"code","6cd341b8":"code","02a439e4":"code","150bf844":"code","0e1baebe":"code","7524bad4":"code","7ab17f2a":"code","13de6cc3":"code","5b463ec1":"code","807596db":"code","498a1b5e":"code","1bd757df":"code","8bd1a691":"code","645fa6b7":"code","a0007720":"code","d05bd0a7":"code","24cbf4d4":"code","bc427176":"code","59bfdaa6":"code","b9fcdc16":"code","26ff49f3":"code","a4ebdff0":"code","3f08a12c":"code","b6027cf1":"code","f6ffb605":"markdown","d9b06c79":"markdown","ed9d9706":"markdown","a89eb911":"markdown","27d2e806":"markdown","f35de964":"markdown","633c0b2d":"markdown","6a39f504":"markdown","506e4dda":"markdown","8c5b9cf0":"markdown","c0451112":"markdown","a25955c5":"markdown","6124cd73":"markdown","dd6696a3":"markdown","e233ed41":"markdown","9cac681a":"markdown","c4a589c0":"markdown","063d4b28":"markdown","6a836462":"markdown","10a3d14b":"markdown","2b46cf4b":"markdown","c1a18e31":"markdown","af2dc131":"markdown","7aac1155":"markdown","39779d00":"markdown","153496cd":"markdown"},"source":{"b72b03bc":"import numpy as np\nimport tensorflow as tf\nfrom tensorflow.keras.applications import vgg19\nfrom tensorflow.keras.models import load_model,Model\nfrom PIL import Image\nimport time\nimport matplotlib.pyplot as plt\nimport matplotlib\nimport requests\nimport base64\nimport os\nfrom pathlib import Path\nfrom io import BytesIO\nmatplotlib.rcParams['figure.figsize'] = (12,12)\nmatplotlib.rcParams['axes.grid'] = False","2e1fdfe5":"def load_image(image_path,dim=None,resize=False):\n    img= Image.open(image_path)\n    if dim:\n        if resize:\n            img=img.resize(dim)\n        else:\n            img.thumbnail(dim)\n    img= img.convert(\"RGB\")\n    return np.array(img)","123f6869":"def load_url_image(url,dim=None,resize=False):\n    img_request=requests.get(url)\n    img= Image.open(BytesIO(img_request.content))\n    if dim:\n        if resize:\n            img=img.resize(dim)\n        else:\n            img.thumbnail(dim)\n    img= img.convert(\"RGB\")\n    return np.array(img)","8a88b82a":"def array_to_img(array):\n    array=np.array(array,dtype=np.uint8)\n    if np.ndim(array)>3:\n        assert array.shape[0]==1\n        array=array[0]\n    return Image.fromarray(array)","cd6d8fe4":"def show_image(image,title=None):\n    if len(image.shape)>3:\n        image=tf.squeeze(image,axis=0)\n    plt.imshow(image)\n    if title:\n        plt.title=title\n    plt.show()","e690dcb0":"def plot_images_grid(images,num_rows=1):\n    n=len(images)\n    if n > 1:\n        num_cols=np.ceil(n\/num_rows)\n        fig,axes=plt.subplots(ncols=int(num_cols),nrows=int(num_rows))\n        axes=axes.flatten()\n        fig.set_size_inches((20,20))\n        for i,image in enumerate(images):\n            axes[i].axis('off')\n            axes[i].imshow(image)\n    else:\n        plt.figure(figsize=(10,10))\n        plt.imshow(images[0])","41d14a2b":"vgg=vgg19.VGG19(weights='imagenet',include_top=False)\nvgg.summary()","beca1ddd":"content_layers=['block4_conv2']\nstyle_layers=['block1_conv1',\n            'block2_conv1',\n            'block3_conv1',\n            'block4_conv1',\n            'block5_conv1']\ncontent_layers_weights=[1]\nstyle_layers_weights=[1]*5","0de5a7af":"class LossModel:\n    def __init__(self,pretrained_model,content_layers,style_layers):\n        self.model=pretrained_model\n        self.content_layers=content_layers\n        self.style_layers=style_layers\n        self.loss_model=self.get_model()\n\n    def get_model(self):\n        self.model.trainable=False\n        layer_names=self.style_layers + self.content_layers\n        outputs=[self.model.get_layer(name).output for name in layer_names]\n        new_model=Model(inputs=self.model.input,outputs=outputs)\n        return new_model\n    \n    def get_activations(self,inputs):\n        inputs=inputs*255.0\n        style_length=len(self.style_layers)\n        outputs=self.loss_model(vgg19.preprocess_input(inputs))\n        style_output,content_output=outputs[:style_length],outputs[style_length:]\n        content_dict={name:value for name,value in zip(self.content_layers,content_output)}\n        style_dict={name:value for name,value in zip(self.style_layers,style_output)}\n        return {'content':content_dict,'style':style_dict}","0bbbecf0":"loss_model=LossModel(vgg,content_layers,style_layers)","4ea735b1":"def content_loss(placeholder,content,weight):\n    assert placeholder.shape == content.shape\n    return weight*tf.reduce_mean(tf.square(placeholder-content))","293d2b3a":"def gram_matrix(x):\n    gram=tf.linalg.einsum('bijc,bijd->bcd', x, x)\n    return gram\/tf.cast(x.shape[1]*x.shape[2]*x.shape[3],tf.float32)","d5136eb2":"def style_loss(placeholder,style,weight):\n    assert placeholder.shape == style.shape\n    s=gram_matrix(style)\n    p=gram_matrix(placeholder)\n    return weight*tf.reduce_mean(tf.square(s-p))","4a42d1d1":"def preceptual_loss(predicted_activations,content_activations,style_activations,content_weight,style_weight,content_layers_weights,style_layer_weights):\n    pred_content=predicted_activations[\"content\"]\n    pred_style=predicted_activations[\"style\"]\n    c_loss=tf.add_n([content_loss(pred_content[name],content_activations[name],content_layers_weights[i]) for i,name in enumerate(pred_content.keys())])\n    c_loss=c_loss*content_weight\n    s_loss=tf.add_n([style_loss(pred_style[name],style_activations[name],style_layer_weights[i]) for i,name in enumerate(pred_style.keys())])\n    s_loss=s_loss*style_weight\n    return c_loss+s_loss","e57a8ab5":"class ReflectionPadding2D(tf.keras.layers.Layer):\n    def __init__(self, padding=(1, 1), **kwargs):\n        super(ReflectionPadding2D, self).__init__(**kwargs)\n        self.padding = tuple(padding)\n    def call(self, input_tensor):\n        padding_width, padding_height = self.padding\n        return tf.pad(input_tensor, [[0,0], [padding_height, padding_height], [padding_width, padding_width], [0,0] ], 'REFLECT')","fca31908":"class InstanceNormalization(tf.keras.layers.Layer):\n    def __init__(self,**kwargs):\n        super(InstanceNormalization, self).__init__(**kwargs)\n    def call(self,inputs):\n        batch, rows, cols, channels = [i for i in inputs.get_shape()]\n        mu, var = tf.nn.moments(inputs, [1,2], keepdims=True)\n        shift = tf.Variable(tf.zeros([channels]))\n        scale = tf.Variable(tf.ones([channels]))\n        epsilon = 1e-3\n        normalized = (inputs-mu)\/tf.sqrt(var + epsilon)\n        return scale * normalized + shift","b7d58449":"class ConvLayer(tf.keras.layers.Layer):\n    def __init__(self,filters,kernel_size,strides=1,**kwargs):\n        super(ConvLayer,self).__init__(**kwargs)\n        self.padding=ReflectionPadding2D([k\/\/2 for k in kernel_size])\n        self.conv2d=tf.keras.layers.Conv2D(filters,kernel_size,strides)\n        self.bn=InstanceNormalization()\n    def call(self,inputs):\n        x=self.padding(inputs)\n        x=self.conv2d(x)\n        x=self.bn(x)\n        return x","12827af7":"class ResidualLayer(tf.keras.layers.Layer):\n    def __init__(self,filters,kernel_size,**kwargs):\n        super(ResidualLayer,self).__init__(**kwargs)\n        self.conv2d_1=ConvLayer(filters,kernel_size)\n        self.conv2d_2=ConvLayer(filters,kernel_size)\n        self.relu=tf.keras.layers.ReLU()\n        self.add=tf.keras.layers.Add()\n    def call(self,inputs):\n        residual=inputs\n        x=self.conv2d_1(inputs)\n        x=self.relu(x)\n        x=self.conv2d_2(x)\n        x=self.add([x,residual])\n        return x","c7af0592":"class UpsampleLayer(tf.keras.layers.Layer):\n    def __init__(self,filters,kernel_size,strides=1,upsample=2,**kwargs):\n        super(UpsampleLayer,self).__init__(**kwargs)\n        self.upsample=tf.keras.layers.UpSampling2D(size=upsample)\n        self.padding=ReflectionPadding2D([k\/\/2 for k in kernel_size])\n        self.conv2d=tf.keras.layers.Conv2D(filters,kernel_size,strides)\n        self.bn=InstanceNormalization()\n    def call(self,inputs):\n        x=self.upsample(inputs)\n        x=self.padding(x)\n        x=self.conv2d(x)\n        return self.bn(x)","a7192de8":"class StyleTransferModel(tf.keras.Model):\n    def __init__(self,**kwargs):\n        super(StyleTransferModel, self).__init__(name='StyleTransferModel',**kwargs)\n        self.conv2d_1= ConvLayer(filters=32,kernel_size=(9,9),strides=1,name=\"conv2d_1_32\")\n        self.conv2d_2= ConvLayer(filters=64,kernel_size=(3,3),strides=2,name=\"conv2d_2_64\")\n        self.conv2d_3= ConvLayer(filters=128,kernel_size=(3,3),strides=2,name=\"conv2d_3_128\")\n        self.res_1=ResidualLayer(filters=128,kernel_size=(3,3),name=\"res_1_128\")\n        self.res_2=ResidualLayer(filters=128,kernel_size=(3,3),name=\"res_2_128\")\n        self.res_3=ResidualLayer(filters=128,kernel_size=(3,3),name=\"res_3_128\")\n        self.res_4=ResidualLayer(filters=128,kernel_size=(3,3),name=\"res_4_128\")\n        self.res_5=ResidualLayer(filters=128,kernel_size=(3,3),name=\"res_5_128\")\n        self.deconv2d_1= UpsampleLayer(filters=64,kernel_size=(3,3),name=\"deconv2d_1_64\")\n        self.deconv2d_2= UpsampleLayer(filters=32,kernel_size=(3,3),name=\"deconv2d_2_32\")\n        self.deconv2d_3= ConvLayer(filters=3,kernel_size=(9,9),strides=1,name=\"deconv2d_3_3\")\n        self.relu=tf.keras.layers.ReLU()\n    def call(self, inputs):\n        x=self.conv2d_1(inputs)\n        x=self.relu(x)\n        x=self.conv2d_2(x)\n        x=self.relu(x)\n        x=self.conv2d_3(x)\n        x=self.relu(x)\n        x=self.res_1(x)\n        x=self.res_2(x)\n        x=self.res_3(x)\n        x=self.res_4(x)\n        x=self.res_5(x)\n        x=self.deconv2d_1(x)\n        x=self.relu(x)\n        x=self.deconv2d_2(x)\n        x=self.relu(x)\n        x=self.deconv2d_3(x)\n        x = (tf.nn.tanh(x) + 1) * (255.0 \/ 2)\n        return x\n    \n    ## used to print shapes of each layer to check if input shape == output shape\n    ## I don't know any better solution to this right now\n    def print_shape(self,inputs):\n        print(inputs.shape)\n        x=self.conv2d_1(inputs)\n        print(x.shape)\n        x=self.relu(x)\n        x=self.conv2d_2(x)\n        print(x.shape)\n        x=self.relu(x)\n        x=self.conv2d_3(x)\n        print(x.shape)\n        x=self.relu(x)\n        x=self.res_1(x)\n        print(x.shape)\n        x=self.res_2(x)\n        print(x.shape)\n        x=self.res_3(x)\n        print(x.shape)\n        x=self.res_4(x)\n        print(x.shape)\n        x=self.res_5(x)\n        print(x.shape)\n        x=self.deconv2d_1(x)\n        print(x.shape)\n        x=self.relu(x)\n        x=self.deconv2d_2(x)\n        print(x.shape)\n        x=self.relu(x)\n        x=self.deconv2d_3(x)\n        print(x.shape)","492c5873":"input_shape=(256,256,3)\nbatch_size=16","8a84e547":"style_model = StyleTransferModel()","fb6b4008":"style_model.print_shape(tf.zeros(shape=(1,*input_shape)))","bb01fd65":"optimizer = tf.keras.optimizers.Adam(learning_rate=1e-3)","5b093c9e":"def train_step(dataset,style_activations,steps_per_epoch,style_model,loss_model,optimizer,checkpoint_path=\".\/\",content_weight=1e4,style_weight=1e-2,total_variation_weight=0.004,content_layers_weights=[1],style_layers_weights=[1]*5):\n    batch_losses=[]\n    steps=1\n    save_path=os.path.join(checkpoint_path,f\"model_checkpoint.ckpt\")\n    print(\"Model Checkpoint Path: \",save_path)\n    for input_image_batch in dataset:\n        if steps-1 >= steps_per_epoch:\n            break\n        with tf.GradientTape() as tape:\n            outputs=style_model(input_image_batch)\n            outputs=tf.clip_by_value(outputs, 0, 255)\n            pred_activations=loss_model.get_activations(outputs\/255.0)\n            content_activations=loss_model.get_activations(input_image_batch)[\"content\"] \n            curr_loss=preceptual_loss(pred_activations,content_activations,style_activations,content_weight,\n                                      style_weight,content_layers_weights,style_layers_weights)\n            curr_loss += total_variation_weight*tf.image.total_variation(outputs)\n        batch_losses.append(curr_loss)\n        grad = tape.gradient(curr_loss,style_model.trainable_variables)\n        optimizer.apply_gradients(zip(grad,style_model.trainable_variables))\n        if steps%100==0:\n            print(\"checkpoint saved \",end=\" \")\n            style_model.save_weights(save_path)\n            print(f\"Loss: {tf.reduce_mean(batch_losses).numpy()}\")\n        steps+=1\n    return tf.reduce_mean(batch_losses)","0383c636":"input_path = \"..\/input\/gan-getting-started\/photo_jpg\"\nstyle_path = \"..\/input\/gan-getting-started\/monet_jpg\"","d327f264":"class TensorflowDatasetLoader:\n    def __init__(self,dataset_path,batch_size=4, image_size=(256, 256),num_images=None):\n        images_paths = [str(path) for path in Path(dataset_path).glob(\"*.jpg\")]\n        self.length=len(images_paths)\n        if num_images is not None:\n            images_paths = images_paths[0:num_images]\n        dataset = tf.data.Dataset.from_tensor_slices(images_paths).map(\n            lambda path: self.load_tf_image(path, dim=image_size),\n            num_parallel_calls=tf.data.experimental.AUTOTUNE,\n        )\n        dataset = dataset.batch(batch_size,drop_remainder=True)\n        dataset = dataset.repeat()\n        dataset = dataset.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\n        self.dataset=dataset\n    def __len__(self):\n        return self.length\n    def load_tf_image(self,image_path,dim):\n        image = tf.io.read_file(image_path)\n        image = tf.image.decode_jpeg(image, channels=3)\n        image= tf.image.resize(image,dim)\n        image= image\/255.0\n        image = tf.image.convert_image_dtype(image, tf.float32)\n        return image","3c79ac60":"loader=TensorflowDatasetLoader(input_path,batch_size=batch_size)","6cd341b8":"loader.dataset.element_spec","02a439e4":"plot_images_grid(next(iter(loader.dataset.take(1))),num_rows=4)","150bf844":"# setting up style image\n\nstyle_image_path =os.path.join(style_path,\"3417ace946.jpg\")\nstyle_image=load_image(style_image_path,dim=(input_shape[0],input_shape[1]),resize=True)\nstyle_image=style_image\/255.0","0e1baebe":"show_image(style_image)","7524bad4":"style_image=style_image.astype(np.float32)\nstyle_image_batch=np.repeat([style_image],batch_size,axis=0)\nstyle_activations=loss_model.get_activations(style_image_batch)[\"style\"]","7ab17f2a":"epochs=10\ncontent_weight=2*1e1\nstyle_weight=1e2\ntotal_variation_weight=0.004","13de6cc3":"num_images=len(loader)\nsteps_per_epochs=num_images\/\/batch_size\nprint(steps_per_epochs)","5b463ec1":"model_save_path=\"model_checkpoint\"","807596db":"os.makedirs(model_save_path,exist_ok=True)","498a1b5e":"try:\n    policy = tf.keras.mixed_precision.experimental.Policy('mixed_float16')\n    tf.keras.mixed_precision.experimental.set_policy(policy) \nexcept:\n    pass","1bd757df":"try:\n    tf.config.optimizer.set_jit(True)\nexcept:\n    pass","8bd1a691":"if os.path.isfile(os.path.join(model_save_path,\"model_checkpoint.ckpt.index\")):\n    style_model.load_weights(os.path.join(model_save_path,\"model_checkpoint.ckpt\"))\n    print(\"resuming training ...\")\nelse:\n    print(\"training scratch ...\")","645fa6b7":"epoch_losses=[]\nfor epoch in range(1,epochs+1):\n    print(f\"epoch: {epoch}\")\n    batch_loss=train_step(loader.dataset,style_activations,steps_per_epochs,style_model,loss_model,optimizer,\n                          model_save_path,\n                          content_weight,style_weight,total_variation_weight,\n                          content_layers_weights,style_layers_weights)\n    style_model.save_weights(os.path.join(model_save_path,\"model_checkpoint.ckpt\"))\n    print(\"Model Checkpointed at: \",os.path.join(model_save_path,\"model_checkpoint.ckpt\"))\n    print(f\"loss: {batch_loss.numpy()}\")\n    epoch_losses.append(batch_loss)","a0007720":"plt.plot(epoch_losses)\nplt.xlabel(\"Epochs\")\nplt.ylabel(\"Loss\")\nplt.title(\"Training Process\")\nplt.show()","d05bd0a7":"if os.path.isfile(os.path.join(model_save_path,\"model_checkpoint.ckpt.index\")):\n    style_model.load_weights(os.path.join(model_save_path,\"model_checkpoint.ckpt\"))\n    print(\"loading weights ...\")\nelse:\n    print(\"no weights found ...\")","24cbf4d4":"styled_images = []\n\nfor images in loader.dataset.take(1):\n    images = images * 255.0\n    generated_images = style_model(images)\n    generated_images = np.clip(generated_images,0,255)\n    generated_images = generated_images.astype(np.uint8)\n    for image in generated_images:\n        styled_images.append(image)","bc427176":"plot_images_grid(next(iter(loader.dataset.take(1))),num_rows = 4)","59bfdaa6":"plot_images_grid(styled_images,num_rows = 4)","b9fcdc16":"os.makedirs(\"images\",exist_ok=True)","26ff49f3":"i = 1\nfor images in loader.dataset.take(500):\n    images = images * 255.0\n    generated_images = style_model(images)\n    generated_images = np.clip(generated_images,0,255)\n    generated_images = generated_images.astype(np.uint8)\n    for image in generated_images:\n        img = Image.fromarray(image)\n        img.save(os.path.join(\"images\",f\"{i}.jpg\"))\n        i=i+1","a4ebdff0":"import shutil\n\nshutil.make_archive('\/kaggle\/working\/images\/', 'zip', 'images')","3f08a12c":"shutil.make_archive('\/kaggle\/working\/model_checkpoint\/', 'zip', 'model_checkpoint')","b6027cf1":"!rm -rf model_checkpoint\n!rm -rf images","f6ffb605":"### plotting loss with respect to epochs","d9b06c79":"# FAST STYLE TRANSFER MODEL\n\nIt is a encoder-decoder architecture with residual layers. Input images are passed to encoder part and it propogates to decoder part of same size as input and predict generated image. For training this generated image is passed to our loss model (VGG19) and features from different layers were extracted (content layers and style layers) these features are then used to calculate style loss and content loss, whose weighted sum produce perceptual loss that trains the network. The below image from paper describe it well.\n\n![https:\/\/arxiv.org\/abs\/1603.08155](https:\/\/miro.medium.com\/max\/1574\/1*Um82GJ99gauIOh0U-S11hQ.png)\n\nThe main highlights of network:\n\n- Residual Layers\n- Encoder Decoder Model\n- output from decoder is passed to loss model(VGG) to alculate loss and train","ed9d9706":"# SETTING UP LOSS MODEL\n\nHere pretrained VGG19 model was used to calculate perceptual loss as describe in this [paper](https:\/\/arxiv.org\/abs\/1603.08155). \n\nIf you know about Gatys style transfer then it is same, we need to calculate style and content loss using a pretrained model using them we create total loss. \n\nThis loss is calculated by passing style and content images both into this VGG network and using some layers of network to extract features. Higher layer learns complex features which also preserves some content of image (like higher layers can represent face of dog but lower layers represent lower features like eyes, nose, ears etc.), so for content loss higher layers of networks are used. For style loss lower layers of networks are used since they have learned small features like edges, countours etc. that can imitate brush stroke from a painting. We use multiple layers for style because every layer learns differnt strokes and to look realistic we need some variations in stroke. ","a89eb911":"### Defining loss functions","27d2e806":"# ZIPPING SUBMISSION DATA","f35de964":"### setting up our style image for training","633c0b2d":"### Constructing style transfer model","6a39f504":"### Creating Loss model which is used to calcuate perceptual loss by combining style and content loss ","506e4dda":"### Defining content and style layers from pretrained model's layers","8c5b9cf0":"<div>\n  <img src='https:\/\/github.com\/tarun-bisht\/fast-style-transfer\/raw\/master\/data\/images\/style.jpg' height=\"346px\">\n  <img src='https:\/\/github.com\/tarun-bisht\/fast-style-transfer\/raw\/master\/data\/images\/content.jpg' height=\"346px\">\n  <img src='https:\/\/github.com\/tarun-bisht\/fast-style-transfer\/raw\/master\/output\/styled.jpg' height=\"512px\">\n<\/div>","c0451112":"### Define input shape and batch size","a25955c5":"### Utility Functions","6124cd73":"# SETTING UP","dd6696a3":"### initializing optimizer for backpropogation","e233ed41":"### Loading previous saved checkpoints if exists","9cac681a":"# FAST STYLE TRANSFER","c4a589c0":"# TRAINING UTILITY\n\nThe output from decoder is passed to loss model (VGG) from which we extract features and calculate style loss and content loss whose weighted sum provide perceptual loss. Using loss, gradients are calculated with respect to style model's trainable parameters.","063d4b28":"# GENERATING ARTWORKS","6a836462":"### Defining model layers","10a3d14b":"# TRAINING STYLE MODEL","2b46cf4b":"### creating instance of style model","c1a18e31":"Stylize any photo or video in style of famous paintings using Neural Style Transfer. It let us to train once and generate infinite images.\n- This is hundreds of times faster than the optimization-based method presented by [Gatys et al](https:\/\/arxiv.org\/abs\/1508.06576) so called fast style transfer.\n- We train a feedforward network that apply artistic styles to images using loss function defined in [Gatys et al](https:\/\/arxiv.org\/abs\/1508.06576) paper.\n- Feed forward network is a residual autoencoder network that takes content image as input and spits out stylized image.\n- Model also uses instance normalization instead of batch normalization based on the paper [Instance Normalization: The Missing Ingredient for Fast Stylization](https:\/\/arxiv.org\/abs\/1607.08022)\n- Training is done by using perceptual loss defined in paper [Perceptual Losses for Real-Time Style Transfer and Super-Resolution](https:\/\/arxiv.org\/abs\/1603.08155).\n- Vgg19 is used to calculate perceptual loss more working described on paper.\n\nIf someone want to try style transfer in video and images, I have created a [github repository](https:\/\/github.com\/tarun-bisht\/fast-style-transfer) for same purpose with instructions.","af2dc131":"### Importing all dependencies","7aac1155":"# SETTING UP DATASET\n\ntf.data api from tensorflow is used to set up training data.","39779d00":"### enabling mix precision and jit for training optimization","153496cd":"# IF YOU LIKE THIS WORK CONSIDER UPVOTING \u2714\u2714\u2714\ud83d\udc4d"}}