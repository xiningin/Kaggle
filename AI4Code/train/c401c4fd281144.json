{"cell_type":{"928d6afd":"code","1e058909":"code","ae9a2df0":"code","14ec1644":"code","ec750b3f":"code","98774532":"code","66aeb6c2":"code","2b62afd0":"code","c1693a8c":"code","746ba734":"code","ee1118d1":"code","731430ba":"code","1f4a67c1":"code","3f1bafb3":"code","1b0eb57e":"code","caa59d65":"code","29212837":"code","f4cb091b":"code","68aaa695":"markdown","05ad0652":"markdown","18108f34":"markdown","0b2d7725":"markdown","2f7664ea":"markdown","1036e9c1":"markdown","08fd250c":"markdown","fd6c4804":"markdown","6b0fa828":"markdown","b0b55e24":"markdown","886228df":"markdown"},"source":{"928d6afd":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\nimport time\nstart_time = time.time()\nimport gc","1e058909":"train = pd.read_csv(\"..\/input\/titanic\/train.csv\")\ntest = pd.read_csv(\"..\/input\/titanic\/test.csv\")","ae9a2df0":"print(train.info())\ndisplay(train)","14ec1644":"print(test.info())\ndisplay(test)","ec750b3f":"train.hist()","98774532":"test.hist()","66aeb6c2":"#Fill the NA\ntrain.Cabin = train.Cabin.fillna('NaN')\ntest.Cabin = test.Cabin.fillna('NaN')","2b62afd0":"# Define function to extract titles from passenger names\n# Based from Saif Uddin's kernel\nimport re\n\ndef get_title(name):\n    title_search = re.search(' ([A-Za-z]+)\\.', name)\n    # If the title exists, extract and return it.\n    if title_search:\n        return title_search.group(1)\n    return \"\"\n\ndef process(dataset):\n    # Create a new feature Title, containing the titles of passenger names\n    dataset['Title'] = dataset['Name'].apply(get_title)\n    # Group all non-common titles into one single grouping \"Rare\"\n    dataset['Title'] = dataset['Title'].replace(['Lady', 'Countess','Capt', 'Col','Don', \n                                                'Dr', 'Major', 'Rev', 'Sir', 'Jonkheer', 'Dona'], 'Rare')\n\n    dataset['Title'] = dataset['Title'].replace('Mlle', 'Miss')\n    dataset['Title'] = dataset['Title'].replace('Ms', 'Miss')\n    dataset['Title'] = dataset['Title'].replace('Mme', 'Mrs')\n    return dataset['Title']","c1693a8c":"def feature_engineering(from_df):\n    df = pd.DataFrame()\n    #PassengerID isn't really relevant for training but we will keep it\n    df['PassengerId'] = from_df.PassengerId\n    \n    #For Name titles, we will be focusing on the name prefixes\n    dummy = pd.get_dummies(process(from_df))\n    for i in dummy.columns:\n        df['Title_'+str(i)] = dummy[i]\n        \n    #For Pclass, we will be making a one-hot-encoding of the three classes\n    dummy = pd.get_dummies(from_df.Pclass.fillna(0))\n    for i in dummy.columns:\n        df['Pclass_'+str(i)] = dummy[i]\n        \n    #For the sex, we will be turning 'male' to 0, and 'female' to 1\n    df['Sex'] = [0 if x=='male' else 1 for x in from_df.Sex]\n    \n    #For age, we will group them by age group, in gaps of some years up to 100 yo\n    gap = 3\n    ages = from_df.Age.fillna(0)\n    for i in range(gap, 100, gap):\n        df[str(i-gap)+'<Age<'+str(i)] = [int(x<i and x>i-gap) for x in ages]\n        \n    #We wouldn't touch the number of Family (siblings or parent) But we only need\n    #to know if there is a family or none\n    df['SibSp'] = [1 if x!=0 else 0 for x in from_df.SibSp.fillna(0)]\n    df['Parch'] = [1 if x!=0 else 0 for x in from_df.Parch.fillna(0)]\n    \n    #For the Fare, similar to the age, we would group them by their aggregate amount\n    fares = from_df.Fare.fillna(0)\n    amount = 20\n    for i in range(amount, 600, amount):\n        df[str(i-amount)+'<Fare<'+str(i)] = [int(x<i and x>i-amount) for x in fares]\n        \n    #We will turn the Cabin into a dummy too using the First letter of their Cabins\n    cabins = from_df.Cabin\n    for key in set(train.Cabin.tolist()+test.Cabin.tolist()):\n        try: #To avoid overwriting the whole Cabin Key, we keep the previous values too\n            df['Cabin_'+key[0]] = [int(x[0]==key[0] or y) for x, y in zip(cabins, df['Cabin_'+key[0]])]\n        except:\n            df['Cabin_'+key[0]] = [int(x[0]==key[0]) for x in cabins]\n    #For `Embarked` we will use the same patter to `Cabin`\n    for i in set(train.Embarked.tolist()+test.Embarked.tolist()):\n        df['Embarked_'+str(i)] = [int(x==i) for x in from_df.Embarked]\n\n    return df","746ba734":"#For the Train\nX = feature_engineering(train)\nY = train.Survived\nprint(X.info())\ndisplay(X)","ee1118d1":"#For the test\ntest_X = feature_engineering(test)\nprint(test_X.info())\ndisplay(test_X)","731430ba":"#We will now remove some unused columns so as to simplify the dataset, and we will use the same columns for the\n#prediction\ndrop_cols = ['PassengerId']\nfor col in X.columns:\n    if X[col].sum() == 0:\n        drop_cols.append(col)\ntrain_cols = [col for col in X.columns if col not in drop_cols]\ndisplay(train_cols)","1f4a67c1":"import matplotlib.pyplot as plt\nimport seaborn as sns\nsns.heatmap(X[train_cols].corr(),annot=False, linewidth=0.2)\nfig=plt.gcf()\nfig.set_size_inches(20,20)\nplt.show()","3f1bafb3":"from tensorflow.keras.models import Sequential, Model\nfrom tensorflow.keras.layers import Dense, Dropout, BatchNormalization, Input, Multiply\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.regularizers import l1_l2\nfrom sklearn.model_selection import train_test_split as tts\n\ndef c_model(shape, lr=0.001):\n    a = 2000\n    \n    x = Sequential()\n    x.add(Dense(a, activation='relu', input_dim=shape))\n    x.add(Dropout(0.25))\n    \n    x.add(Dense(a\/\/4, activation='relu', kernel_regularizer=l1_l2(0.002, 0.002)))\n    x.add(BatchNormalization())\n    x.add(Dropout(0.75))\n    \n    x.add(Dense(a\/\/10, activation='relu', kernel_regularizer=l1_l2(0.002, 0.002)))\n    x.add(BatchNormalization())\n    x.add(Dropout(0.75))\n    \n    x.add(Dense(a\/\/10, activation='relu', kernel_regularizer=l1_l2(0.002, 0.002)))\n    x.add(BatchNormalization())\n    x.add(Dropout(0.75))\n    \n    x.add(Dense(2, activation='softmax'))\n    \n    opt = Adam(lr=lr, decay=1e-3, beta_1=0.95, beta_2=0.995, amsgrad=True)\n    x.compile(optimizer=opt, loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n    \n    return x","1b0eb57e":"import matplotlib.pyplot as plt\ndef plotter(history, n):\n    plt.plot(history.history['accuracy'])\n    plt.plot(history.history['val_accuracy'])\n    plt.title('MODEL ACCURACY #%i' %n)\n    plt.ylabel('Accuracy')\n    plt.xlabel('Epoch')\n    plt.legend(['Train', 'Test'], loc='upper right')\n    plt.ylim(top=1, bottom=0.01)\n    plt.savefig('history_accuracy_{}.png'.format(n))\n    plt.show()\n    \n    plt.plot(history.history['loss'])\n    plt.plot(history.history['val_loss'])\n    plt.title('MODEL LOSS #%i' %n)\n    plt.ylabel('Loss')\n    plt.xlabel('Epoch')\n    plt.legend(['Train', 'Test'], loc='upper right')\n    #plt.ylim(top=2, bottom=0.01)\n    plt.savefig('history_loss_{}.png'.format(n))\n    plt.show()","caa59d65":"from tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping, ModelCheckpoint\nlrr = ReduceLROnPlateau(monitor = 'val_accuracy',\n                         patience = 50,\n                         verbose = 1,\n                         factor = 0.5,\n                         min_lr = 1e-8)\n\nes1= EarlyStopping(monitor='val_loss',\n                   mode='min',\n                   verbose=1,\n                   patience=100,\n                   restore_best_weights=True)\nes2= EarlyStopping(monitor='val_accuracy',\n                   mode='max',\n                   verbose=1,\n                   patience=750,\n                   restore_best_weights=True)\n\n\n\nfolds = 10\nepochs = 5000\nbatch_size = X.shape[0]\n\ntrain_history = []\nall_predictions = None\nall_scores = []\n\nfor n in range(1, folds+1):\n    mcp = ModelCheckpoint(f'model_weights_{n}.hdf5', monitor='val_accuracy', verbose=0,\n                          save_best_only=True, mode='max', period=10)\n    \n    print(f\"Currently training on Fold: {n}\")\n    \n    xt, xv, yt, yv = tts(X[train_cols], Y, test_size=0.2, random_state=1771, shuffle=True, stratify=Y)\n    model = c_model(xt.shape[1], 3e-4)\n    hist = model.fit(xt, yt, validation_data=(xv, yv),\n                     epochs=epochs, batch_size=batch_size,\n                     callbacks=[lrr, es1, es2, mcp], verbose=0)\n    \n    train_history.append(hist)\n    plotter(hist, n)\n    \n    loss, acc = model.evaluate(xv, yv)\n    predicted = model.predict(test_X[train_cols])\n    \n    model.load_weights(f'model_weights_{n}.hdf5')\n    loss2, acc2 = model.evaluate(xv, yv)\n    \n    if acc < acc2 or (acc==acc2 and loss < loss2):\n        predicted = model.predict(test_X[train_cols])\n        loss, acc = loss2, acc2\n        \n    all_scores.append([loss, acc])\n    \n    if acc > .77:\n        try:\n            all_predictions += predicted*acc\n        except:\n            all_predictions = predicted*acc","29212837":"sub = pd.DataFrame()\nsub['PassengerId'] = test.PassengerId\nsub['Survived'] = np.argmax(all_predictions, axis=1)\ndisplay(sub)\nsub.to_csv(\"submission.csv\", index=False)","f4cb091b":"total = time.time() - start_time\nh = total\/\/3600\nm = (total%3600)\/\/60\ns = total%60\nprint(\"Total Spent time: %i:%i:%i\" %(h, m ,s))","68aaa695":"# Peek to the Data\n\nFirst of all, we need to import the `Train` and `Test` Datasets provided to this competition.","05ad0652":"# Prediction\n\nLast, we do our prediction and save them for the competition.","18108f34":"We will do a multi-fold training using the same model on different permutations of the data. The prediction is weighted by the model's prediction accuracy before ensemble. Since we will be using a `Model Checkpoint`, the accuracy should be maintained.","0b2d7725":"Now, let's analyze the correlation between features using Seaborn.","2f7664ea":"# Feature Engineering\n\nFirst, we must make a new function to make sure that the application of feature engineering is reproducible across all DataFrames. The basic Feature engineering will be based on the above Histogram for both `train` and `test` data.","1036e9c1":"Seeing this, we can determine that we only have a few samples, roughly 891 rows of train data. Further on, we will process and augment the data thorugh Feature Engineering","08fd250c":"We will now check the first few elements and the last few elements of each Dataset.","fd6c4804":"# End\n\nThat's all for now. Thank you for reading this kernel.","6b0fa828":"As you can see, we did as 2-category softmax insted of a 1-target binary. This makes the probability of both \"Dying\" and \"Surviving\" apparent, which should increase the accuracy.","b0b55e24":"# The Previous Best Score is:\nVersion 28 - 78.947%","886228df":"# Model\n\nNow that we are finished with our Feature Engineering, we will now design our model. Since we are only doing a binary prediction, we will be using a basic Keras NN Model to do this. As they say \"Simple is Best.\""}}