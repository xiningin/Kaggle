{"cell_type":{"33add0ce":"code","272acfa2":"code","7d97d301":"code","3bf03ba3":"code","2b6b46cc":"code","dc078fbc":"code","1435b296":"code","0361c3db":"code","600e7113":"code","d6043ab2":"code","415dd905":"code","2ef122f6":"code","e4ce6330":"code","51077617":"code","79cf68fd":"code","5476f61f":"code","844d18a7":"code","92353888":"code","a763acd2":"code","1f82e642":"markdown","8326ca33":"markdown"},"source":{"33add0ce":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","272acfa2":"import gensim\nfrom gensim.models import Word2Vec\nfrom gensim.utils import simple_preprocess\n\nfrom gensim.models.keyedvectors import KeyedVectors\n","7d97d301":"filepath = \"..\/input\/GoogleNews-vectors-negative300.bin\"\n\nfrom gensim.models import KeyedVectors\nwv_from_bin = KeyedVectors.load_word2vec_format(filepath, binary=True) \n#extracting words7 vectors from google news vector\nembeddings_index = {}\nfor word, vector in zip(wv_from_bin.vocab, wv_from_bin.vectors):\n    coefs = np.asarray(vector, dtype='float32')\n    embeddings_index[word] = coefs","3bf03ba3":"def avg_feature_vector(sentence, model, num_features):\n    words = sentence.split()\n    #feature vector is initialized as an empty array\n    feature_vec = np.zeros((num_features, ), dtype='float32')\n    n_words = 0\n    for word in words:\n        if word in embeddings_index.keys():\n            n_words += 1\n            feature_vec = np.add(feature_vec, model[word])\n    if (n_words > 0):\n        feature_vec = np.divide(feature_vec, n_words)\n    return feature_vec","2b6b46cc":" from scipy.spatial import distance","dc078fbc":"s1_afv = avg_feature_vector('Why the second proforma does not coincide with the first, what has changed', model= embeddings_index, num_features=300 )\ns2_afv = avg_feature_vector('Again came the proforma double.In the morning there was already a proforma with the same positions, but under a different number',model= embeddings_index, num_features=300)\ncos = distance.cosine(s1_afv, s2_afv)\nprint(cos)","1435b296":"s1_afv = avg_feature_vector('Why the second proforma does not coincide with the first, what has changed', model= embeddings_index, num_features=300 )\ns2_afv = avg_feature_vector('Specify, please, why two pro forma came? After all, we have one order',model= embeddings_index, num_features=300)\ncos = distance.cosine(s1_afv, s2_afv)\nprint(cos)","0361c3db":"s1_afv = avg_feature_vector('Why the second proforma does not coincide with the first, what has changed', model= embeddings_index, num_features=300 )\ns2_afv = avg_feature_vector('\"Again came the proforma double.In the morning there was already a proforma with the same positions, but under a different number.',model= embeddings_index, num_features=300)\ncos = distance.cosine(s1_afv, s2_afv)\nprint(cos)","600e7113":"s1_afv = avg_feature_vector('Why the second proforma does not coincide with the first, what has changed', model= embeddings_index, num_features=300 )\ns2_afv = avg_feature_vector('Again came the proforma double.In the morning there was already a proforma with the same positions, but under a different number.',model= embeddings_index, num_features=300)\ncos = distance.cosine(s1_afv, s2_afv)\nprint(cos)","d6043ab2":"s1_afv = avg_feature_vector('Why the second proforma does not coincide with the first, what has changed', model= embeddings_index, num_features=300 )\ns2_afv = avg_feature_vector('Tell me why I got 2 identical pro forma, but with different numbers?',model= embeddings_index, num_features=300)\ncos = distance.cosine(s1_afv, s2_afv)\nprint(cos)","415dd905":"s1_afv = avg_feature_vector('Why the second proforma does not coincide with the first, what has changed', model= embeddings_index, num_features=300 )\ns2_afv = avg_feature_vector('Until now, since yesterday there is no pro forma. Please send.',model= embeddings_index, num_features=300)\ncos = distance.cosine(s1_afv, s2_afv)\nprint(cos)","2ef122f6":"s1_afv = avg_feature_vector('Why the second proforma does not coincide with the first, what has changed', model= embeddings_index, num_features=300 )\ns2_afv = avg_feature_vector('Until now, since yesterday there is no pro forma. Please send.',model= embeddings_index, num_features=300)\ncos = distance.cosine(s1_afv, s2_afv)\nprint(cos)","e4ce6330":"#In the form price with discount DT\ns1_afv = avg_feature_vector('In the form price with discount DT', model= embeddings_index, num_features=300 )\ns2_afv = avg_feature_vector('Why is the amount in the form and pro forma different?',model= embeddings_index, num_features=300)\ncos = distance.cosine(s1_afv, s2_afv)\nprint(cos)","51077617":"s1_afv = avg_feature_vector('The discrepancy between prices in the form and on request', model= embeddings_index, num_features=300 )\ns2_afv = avg_feature_vector('Why the discrepancy between the blank and the order?',model= embeddings_index, num_features=300)\ncos = distance.cosine(s1_afv, s2_afv)\nprint(cos)","79cf68fd":"s1_afv = avg_feature_vector('Shipment after delivery to quickly add to customer base', model= embeddings_index, num_features=300 )\ns2_afv = avg_feature_vector('Larisa, it is clear that they will arrive with the goods. I asked you to send scans of invoices for the preliminary posting of the goods.', model= embeddings_index, num_features=300)\ncos = distance.cosine(s1_afv, s2_afv)\nprint(cos)","5476f61f":"s1_afv = avg_feature_vector('Shipment after delivery to quickly add to customer base', model= embeddings_index, num_features=300 )\ns2_afv = avg_feature_vector('I beg you to promptly send the facts of shipments from 04\/05\/2019',model= embeddings_index, num_features=300)\ncos = distance.cosine(s1_afv, s2_afv)\nprint(cos)","844d18a7":"s1_afv = avg_feature_vector('Shipment after delivery to quickly add to customer base', model= embeddings_index, num_features=300 )\ns2_afv = avg_feature_vector('Catherine, I need invoices that will arrive on Saturday, the car has already shipped?',model= embeddings_index, num_features=300)\ncos = distance.cosine(s1_afv, s2_afv)\nprint(cos)","92353888":"s1_afv = avg_feature_vector('Who uploads discounts', model= embeddings_index, num_features=300 )\ns2_afv = avg_feature_vector('Who uploads discounts',model= embeddings_index, num_features=300)\ncos = distance.cosine(s1_afv, s2_afv)\nprint(cos)","a763acd2":"s1_afv = avg_feature_vector('Who uploads discounts', model= embeddings_index, num_features=300 )\ns2_afv = avg_feature_vector('Who for discounts',model= embeddings_index, num_features=300)\ncos = distance.cosine(s1_afv, s2_afv)\nprint(cos)","1f82e642":"**Word2vec-GoogleNews-vectors**. \n\nThis repository hosts the word2vec pre-trained Google News corpus (3 billion running words) word vector model (3 million 300-dimension English word vectors)\n\nThe word2vec tool takes a text corpus as input and produces the word vectors as output. It first constructs a vocabulary from the training text data and then learns vector representation of words. The resulting word vector file can be used as features in many natural language processing and machine learning applications.\n\nA simple way to investigate the learned representations is to find the closest words for a user-specified word. The distance tool serves that purpose. For example, if you enter 'france', distance will display the most similar words and their distances to 'france', which should look like:\n\n**Word Cosine distance**\n            spain              0.678515\n          belgium              0.665923\n      netherlands              0.652428\n            italy              0.633130\n      switzerland              0.622323\n       luxembourg              0.610033\n         portugal              0.577154\n           russia              0.571507\n          germany              0.563291\n        catalonia              0.534176","8326ca33":"Gensim is a free Python library designed to automatically extract semantic topics from documents, as efficiently (computer-wise) and painlessly (human-wise) as possible.\n\nGensim is designed to process raw, unstructured digital texts (\u201cplain text\u201d).\n\nThe algorithms in Gensim, such as Word2Vec, FastText, Latent Semantic Analysis (LSI, LSA, see LsiModel), Latent Dirichlet Allocation (LDA, see LdaModel) etc, automatically discover the semantic structure of documents by examining statistical co-occurrence patterns within a corpus of training documents. These algorithms are unsupervised, which means no human input is necessary \u2013 you only need a corpus of plain text documents.\n\nOnce these statistical patterns are found, any plain text documents (sentence, phrase, word\u2026) can be succinctly expressed in the new, semantic representation and queried for topical similarity against other documents (words, phrases\u2026)\n\n**We are using a pre-trained gensim Word2vec model since we have limited data to perform similarity check on. In case of document corpus, where we need to check similarity for user queries, we will return paragraph based on similarity match **\n\n"}}