{"cell_type":{"81ee1679":"code","449dc1d3":"code","55d737d9":"code","74663ac0":"code","ea3f6b68":"code","3984276c":"code","cc48b953":"code","9088b934":"code","7d7a0351":"code","0818a645":"code","f77ceaab":"code","5fdbcc25":"code","3b3dbf2d":"code","7170aece":"code","b4e9d294":"code","0993cd87":"code","f4fca999":"code","e70e83a7":"code","94e0ff9d":"code","68ece8fe":"markdown","a1215d10":"markdown","b51a06b8":"markdown","e54fbbec":"markdown","3217384d":"markdown","0885fcee":"markdown","aa892cae":"markdown","91fab14b":"markdown","4ff6cd60":"markdown","62e2a9dd":"markdown","22770e61":"markdown","970bc22d":"markdown","42af662c":"markdown","94edb9a4":"markdown","6d087e4f":"markdown","8d508183":"markdown","9970fc43":"markdown","a1804cf5":"markdown"},"source":{"81ee1679":"use_dataset = True","449dc1d3":"if not use_dataset:\n    !pip install psaw\n    #import praw # Official Reddit API\n    from psaw import PushshiftAPI # Unofficial Reddit API (for getting content by date)","55d737d9":"from timeit import default_timer as timer\nfrom time import sleep\nfrom datetime import datetime, timezone, timedelta\nimport os\nimport json\nimport pandas as pd\nimport time\nimport pickle\nimport nltk\nfrom pprint import pprint\nimport matplotlib.pyplot as plt\nimport sklearn\nfrom tensorflow import keras\n\nsubreddits = [\"MachineLearning\"]\ndate_begin = datetime(2009, 7, 29)\ndate_final = datetime(2020, 3, 1)\nrequest_delay = 2\nmonths_per_interval = 6","74663ac0":"if use_dataset:\n    submissions = pickle.load(open('\/kaggle\/input\/redditmachinelearning\/submissions_fe.pickle', 'rb'))\n    comments    = pickle.load(open('\/kaggle\/input\/redditmachinelearning\/comments_fe.pickle'   , 'rb'))","ea3f6b68":"class terms_list:\n    terms_models = {\n        \"SVM\": [\"support vector machine\", \"svm\", \"support vector\"],\n        \"RNN\": [\"rnn\", \"recurrent neural network\", \"recurrent neural net\", \"recurrent nn\"],\n        \"LSTM\": [\"lstm\", \"long short term memory\"],\n        \"CNN\": [\"convolutional neural network\", \"convolutional neural net\", \"convolutional nn\", \"cnn\", \"convolution\", \"pool\", \"pooling\"],\n        \"Neural Network\": [\"ann\", \"nn\", \"neural network\", \"neural net\"],\n        \"Deep Learning\": [\"deep learning\"],\n\n        # Regression Models\n        \"Linear Regression\": [\"linear regression\"],\n        \"Logistic Regression\": [\"logistic regression\"],\n        \"LASSO Regression\": [\"lasso regression\", \"lasso\"],\n        \"Ridge Regression\": [\"ridge regression\", \"ridge\"],\n        \"Regression\": [\"regression\"],\n\n        \"Random Forest\": [\"random forest\"],\n        \"Decision Tree\": [\"decision tree\"],\n        \"Naive Bayes\": [\"naive bayes\", \"na\u00efve bayes\"],\n        \"K-Means\": [\"k means\"],\n        \"KNN\": [\"knn\", \"k nearest neighbors\", \"k nearest neighbor\"],\n        \"PCA\": [\"pca\", \"principal component analysis\"],\n    }\n\n    terms_ensembles = {\n        \"Ensemble\": [\"ensemble learning\", \"ensemble\"],\n        \"Bagging\": [\"bagging\", \"bagged\"],\n        \"Boosting\": [\"boosting\"],\n        \"Stacking\": [\"stacking\", \"stacked\"],\n        \"Blending\": [\"blending\", \"blended\"],\n        \"XGBoost\": [\"xgboost\", \"xg boost\"],\n        \"ADABoost\": [\"adaboost\", \"ada boost\"],\n        \"Random Forest\": [\"random forest\", \"randomforest\"],\n    }\n\n    terms_activations = {\n        \"Sigmoid\": [\"sigmoid\", \"logistic activation\"],\n        \"tanh\": [\"tanh\", \"hyperbolic tangent\"],\n        \"Leaky ReLU\": [\"leaky relu\", \"leakyrelu\"],\n        \"Parametric ReLU\": [\"parametric relu\", \"parametricrelu\"],\n        \"ReLU\": [\"relu\"],\n        \"ELU\": [\"elu\"],\n        \"Softmax\": [\"softmax\", \"soft max\"],\n        \"Swish\": [\"swish\"], # What is this?\n    }\n\n    terms_neural_nets = {\n        \"RNN\": [\"rnn\", \"recurrent neural network\", \"recurrent neural net\", \"recurrent nn\"],\n        \"LSTM\": [\"lstm\", \"long short term memory\"],\n        \"CNN\": [\"convolutional neural network\", \"convolutional neural net\", \"convolutional nn\", \"cnn\", \"convolution\", \"pool\", \"pooling\"],\n        \"Neural Network\": [\"ann\", \"nn\", \"neural network\", \"neural net\"],\n        \"Deep Learning\": [\"deep learning\"],\n        \"Network\": [\"network\", \"net\"],\n        \"Neuron\": [\"neuron\"],\n        \"Perceptron\": [\"perceptron\"],\n        \"Layer\": [\"hidden layer\", \"layer\"],\n        \"Dropout\": [\"dropout\"],\n        \"Fully Connected\": [\"fully connected\"],\n        \"Dense\": [\"dense\"],\n        \"Tensorflow\": [\"tensorflow\", \"tensor flow\"],\n        \"TensorBoard\": [\"tensorboard\", \"tensor board\"],\n        \"Torch\": [\"pytorch\", \"torch\"],\n        \"Keras\": [\"keras\"],\n        \"Theano\": [\"theano\"],\n        \"Caffe\": [\"caffe\", \"caffe2\"]\n    }\n    terms_neural_nets.update(terms_activations)\n\n    terms_misc = {\n        \"Dying ReLU\": [\"dying relu\"],\n        \"Vanishing Gradient\": [\"vanishing gradient\"],\n        \"Kernel\": [\"kernel\"],\n    }\n\n    terms_loss = {\n        # Regression\n        \"Huber Loss\": [\"huber\", \"smooth mean absolute error\"],\n        \"Log-Cosh Loss\": [\"log cosh loss\"],\n        \"MSE\": [\"mse\", \"mean squared error\", \"mean square error\", \"l2 loss\", \"quadratic loss\"],\n        \"MAE\": [\"mae\", \"mean absolute error\", \"l1 loss\"],\n        # Classification\n        \"Log Loss\": [\"logarithmic loss\", \"log loss\", \"logistic loss\", \"binary cross entropy\", \"binary entropy\"],\n        \"Categorical Cross-Entropy\": [\"categorical cross entropy\", \"categorical entropy\"],\n        \"Hinge Loss\": [\"hinge\", \"hinge loss\"],\n    }\n\n    terms_optimizers = {\n        \"Gradient Descent\": [\"gradient descent\"],\n        \"Adagrad\": [\"adagrad\", \"ada grad\"],\n        \"RMSProp\": [\"rmsprop\", \"rms prop\"],\n        \"Adam\": [\"adam\"],\n    }\n\n    terms_regularization = {\n        \"Regularization\": [\"regularization\", \"regularized\", \"regularize\"],\n        \"LASSO Regression\": [\"lasso regression\", \"lasso\"],\n        \"Ridge Regression\": [\"ridge regression\", \"ridge\"],\n        \"Dropout\": [\"dropout\"],\n    }\n\n    terms_areas = {\n        \"Regression\": [\"regression\"],\n        \"Classification\": [\"classification\", \"classifier\"],\n        \"Unsupervised Learning\": [\"unsupervised\"],\n        \"Supervised Learning\": [\"supervised\"],\n        \"Reinforcement Learning\": [\"reinforcement learning\", \"rl\"],\n        \"Clustering\": [\"clustering\", \"clusters\", \"cluster\"],\n        \"Dimensionality Reduction\": [\"dimensionality reduction\", \"dimensional reduction\", \"dimension reduction\"],\n        \"NLP\": [\"nlp\", \"natural language processing\"]\n    }\n    \n    def __init__(self):\n        self.terms_all = dict()\n        self.terms_all.update(self.terms_models)\n        self.terms_all.update(self.terms_ensembles)\n        self.terms_all.update(self.terms_neural_nets)\n        self.terms_all.update(self.terms_activations)\n        self.terms_all.update(self.terms_misc)\n        self.terms_all.update(self.terms_loss)\n        self.terms_all.update(self.terms_optimizers)\n        self.terms_all.update(self.terms_regularization)\n        self.terms_all.update(self.terms_areas)\n    \nterms = terms_list()","3984276c":"if not use_dataset:\n    pushapi = PushshiftAPI()\n\n    # Create necessary directories\n    if not os.path.exists(\"data\"):\n        os.mkdir(\"data\")\n    for subreddit in subreddits:\n        if not os.path.exists(\"data\/{}\".format(subreddit)):\n            os.mkdir(\"data\/{}\".format(subreddit))\n        if not os.path.exists(\"data\/{}\/submission\".format(subreddit)):\n            os.mkdir(\"data\/{}\/submission\".format(subreddit))\n        if not os.path.exists(\"data\/{}\/comment\".format(subreddit)):\n            os.mkdir(\"data\/{}\/comment\".format(subreddit))\n\n    # Get submissions and comments from each subreddit\n    for subreddit in subreddits:\n        # Should we resume progress?\n        if len(os.listdir(\"data\/\" + subreddit + \"\/comment\")) > 0:\n            # Set day to last recorded day + 1 (assume files haven't been deleted)\n            files = os.listdir(\"data\/{}\/submission\".format(subreddit))\n            files.sort(key=lambda date: datetime.strptime(date, \"%Y-%m-%d.json\"))\n            date_start = datetime.strptime(files[len(files) - 1], \"%Y-%m-%d.json\") + timedelta(days=1)\n            print(\"Resuming {} at day {}\".format(subreddit, date_start))\n        else:\n            date_start = date_begin\n\n        date_end = date_start + timedelta(days=1)\n\n        # Iterate over each day\n        while True:\n            submissions = list()\n            if date_end > date_final: break\n\n            # Get submissions\n            subsearch = pushapi.search_submissions(\n                subreddit = subreddit,\n                after = date_start,\n                before = date_end,\n                filter = [\n                    \"title\",        # Title of submission\n                    \"selftext\",     # Submission body of text (unless it's media)\n                    \"score\",        # Upvotes - Downvotes\n                    \"num_comments\", # Number of comments\n                    \"id\",           # Submission ID, to link the post\n                    \"author\",       # Username\n                    #\"num_crossposts\"\n                ], # Where is upvote_ratio?\n                limit = 500, # Max number of results\n            )\n            sleep(request_delay)\n\n            # Get comments\n            comsearch = pushapi.search_comments(\n                subreddit = subreddit,\n                after = date_start,\n                before = date_end,\n                filter = [\n                    \"body\",                  # Comment body of text\n                    \"score\",                 # Upvotes - Downvotes\n                    \"author\",                # Username\n                    \"total_awards_received\", # Number of rewards received\n                    \"id\",                    # ID of comment\n                    \"parent_id\",             # Comment being replied to (equal to link_id if replying to submission)\n                    \"link_id\",               # Original submission link\n                ],\n                limit = 500\n            )\n            sleep(request_delay)\n\n            # Extract list of dictionaries from results\n            submissions = [submission.d_ for submission in subsearch]\n            comments    = [comment.d_    for comment    in comsearch]\n\n            if len(submissions) > 500: print(\"\\tWARNING: Failed to capture all submission data with limit=500\")\n            if len(comments)    > 500: print(\"\\tWARNING: Failed to capture all comment data with limit=500\")\n\n            # Save to JSON file\n            path = \"data\/{}\/{}\/{}\"\n            fname = \"{}-{}-{}.json\".format(\n                date_start.year, date_start.month, date_start.day\n            )\n            with open(path.format(subreddit, \"submission\", fname), 'w') as fp:\n                json.dump(submissions, fp)\n            with open(path.format(subreddit, \"comment\", fname), 'w') as fp:\n                json.dump(comments, fp)\n\n            print(fname)\n\n            # Re-iterate\n            date_start += timedelta(days=1)\n            date_end   += timedelta(days=1)\n        print(\"done\")","cc48b953":"if not use_dataset:\n    submissions_list = list()\n    comments_list    = list()\n    for subreddit in os.listdir(\"data\"):\n        if not subreddit in subreddits: continue\n        for day_json in os.listdir(\"data\/{}\/submission\".format(subreddit)):\n            if not day_json.endswith(\".json\"): continue\n            # Assume submission dir is same size as comment dir\n            with open(\"data\/{}\/submission\/{}\".format(subreddit, day_json), \"r\") as f_sub, \\\n                 open(\"data\/{}\/comment\/{}\"   .format(subreddit, day_json), \"r\") as f_com:\n                day_subs = json.load(f_sub)\n                day_coms = json.load(f_com)\n                for sub in day_subs: sub[\"subreddit\"] = subreddit\n                for com in day_coms: com[\"subreddit\"] = subreddit\n                submissions_list.extend(day_subs)\n                comments_list   .extend(day_coms)","9088b934":"if not use_dataset:\n    submissions = pd.DataFrame(submissions_list)\n    submissions[\"created_utc\"] = submissions[\"created_utc\"].apply(datetime.utcfromtimestamp)\n    submissions.drop([\"created\"], axis=1, inplace=True)\n    submissions = submissions.astype({\"subreddit\": \"category\"})\n    submissions = submissions[submissions[\"selftext\"] != \"[removed]\"]\n    submissions = submissions[submissions[\"selftext\"] != \"[deleted]\"]\n\n    comments = pd.DataFrame(comments_list)\n    comments[\"created_utc\"] = comments[\"created_utc\"].apply(datetime.utcfromtimestamp)\n    comments.drop([\"created\"], axis=1, inplace=True)\n    comments = comments.astype({\"subreddit\": \"category\"})\n    comments = comments[comments[\"body\"] != \"[removed]\"]\n    comments = comments[comments[\"body\"] != \"[deleted]\"]\n\n    # Get direct replies to comments\n    post_replies = comments[\"parent_id\"].map(lambda x: x.split(\"_\")[1]).value_counts()\n    def get_replies(cid):\n        if cid in post_replies: return post_replies[cid]\n        return 0\n    comments[\"direct_replies\"] = comments[\"id\"].apply(get_replies)\n\n    # Get total replies to comments (includes replies to replies of comment)\n    total_replies = {com_id: 0 for com_id in comments[\"id\"].unique()}\n    def increment_replies(row):\n        if row[\"link_id\"] == row[\"parent_id\"]: return\n        pid = row[\"parent_id\"].split(\"_\")[1]\n        if pid not in total_replies: return # Skip unrecorded comments\n        total_replies[pid] += 1\n        new_row = comments[comments[\"id\"] == pid]\n        if len(new_row) > 1: print(len(new_row))\n        increment_replies(new_row.iloc[0])\n    for idx, row in comments.iterrows():\n        if   idx == 0: time_start = time.time()\n        if idx % 1000 == 0:\n            print(str(idx) + \" \/ \" + str(len(comments)))\n        increment_replies(row)\n    comments[\"total_replies\"] = comments[\"id\"].apply(lambda x: total_replies[x])\n    \n    display(submissions.dtypes)\n    display(comments.dtypes)","7d7a0351":"if not use_dataset:\n    # Given a text, frequency dictionary, and row number, records each ML term found in the text in the frequencies\n    stemmer = nltk.SnowballStemmer(\"english\")\n    def count_terms(\n        tokens,      # Tokenized text\n        frequencies, # Current frequencies to be modified\n        row,         # Which row of frequencies to modify\n    ):\n        # Iterate over each term\n        for term in terms.terms_all.keys():\n            count = 0\n            # Iterate over all representations of term (neural net vs ANN, etc.)\n            for rep in terms.terms_all[term]:\n                # Tokenize term words\n                rep_words = rep.split()\n\n                # Search for representation in text\n                start = 0\n                end = len(rep_words)\n                while True:\n                    if end > len(tokens): break\n                    found = True\n\n                    # Check if tokens match\n                    for i in range(start, end):\n                        if stemmer.stem(tokens[i]) != rep_words[i-start] and tokens[i] != rep_words[i-start] and stemmer.stem(tokens[i]) != stemmer.stem(rep_words[i-start]):\n                            found = False\n                            break\n\n                    if found:\n                        count += 1\n                        # Remove term from text\n                        for i in range(start, end):\n                            del tokens[start]\n                    else:\n                        start += 1\n                        end += 1\n\n            # Output total count from each representation\n            frequencies[term][row] += count","0818a645":"if not use_dataset:\n    stopwords = nltk.corpus.stopwords.words('english')\n\n    # Returns a tokenized representation of a submission title\n    def tokenize_title(title):\n        text = str(title)\n        text = text.lower()\n        text = text.replace(\"-\", \" \")\n        text = text.replace(\"'\", \"\")\n        text = text.replace(\",\", \" \")\n        text = text.replace(\"[Discussion]\", \"\")\n        text = text.replace(\"[D]\", \"\")\n        text = text.replace(\"[News]\", \"\")\n        text = text.replace(\"[N]\", \"\")\n        text = text.replace(\"[Research]\", \"\")\n        text = text.replace(\"[R]\", \"\")\n        text = text.replace(\"[Project]\", \"\")\n        text = text.replace(\"[P]\", \"\")\n        # Tokenize the text\n        tokens = nltk.word_tokenize(text)\n        # Filter stopwords for efficiency\n        tokens = list(filter(lambda w: not w in stopwords, tokens))\n        return tokens\n\n    # Returns a tokenized representation of a submission body or comment\n    def tokenize_body(body):\n        text = str(body)\n        text = text.lower()\n        text = text.replace(\"-\", \" \")\n        text = text.replace(\"'\", \"\")\n        text = text.replace(\",\", \" \")\n        # Tokenize the text\n        tokens = nltk.word_tokenize(text)\n        # Filter stopwords for efficiency\n        tokens = list(filter(lambda w: not w in stopwords, tokens))\n        return tokens\n\n    # Add columns to DF's for each term\n    for term in terms.terms_all.keys():\n        submissions[term] = 0\n        comments[term] = 0\n\n    num_subs = len(submissions)\n    num_coms = len(comments)\n\n    # Set up dictionary of frequencies of terms, with a list representing the rows\n    frequencies_subs = terms.terms_all.copy()\n    frequencies_coms = terms.terms_all.copy()\n    for term in terms.terms_all.keys():\n        frequencies_subs[term] = [0 for _ in range(num_subs)]\n        frequencies_coms[term] = [0 for _ in range(num_coms)]\n\n    # Iterate over submission titles\n    print(\"Submission Titles:\")\n    for row, title in enumerate(submissions.title):\n        if row % 2000 == 0: print(str(row) + \" \/ \" + str(num_subs))\n        tokens = tokenize_title(title)             # Tokenize the text\n        count_terms(tokens, frequencies_subs, row) # Count occurences of each term\n    print(\"done\\n\")\n\n    # Iterate over submission bodies\n    print(\"Submission Bodies:\")\n    for row, selftext in enumerate(submissions.selftext):\n        if row % 2000 == 0: print(str(row) + \" \/ \" + str(num_subs))\n        tokens = tokenize_body(selftext)           # Tokenize the text\n        count_terms(tokens, frequencies_subs, row) # Count occurences of each term\n    print(\"done\\n\")\n\n    # Add to DataFrame\n    for term in terms.terms_all.keys():\n        submissions[term] = frequencies_subs[term]\n\n    pickle.dump(submissions, open(\"submissions_fe.pickle\", \"wb\"))\n\n    # Iterate over comment bodies\n    print(\"Comment Bodies:\")\n    for row, body in enumerate(comments.body):\n        if row % 500 == 0: print(str(row) + \" \/ \" + str(num_coms))\n        tokens = tokenize_body(body)               # Tokenize the text\n        count_terms(tokens, frequencies_coms, row) # Count occurences of each term\n        \n    print(\"done\")\n\n    # Add to DataFrame\n    for term in terms.terms_all.keys():\n        comments[term] = frequencies_coms[term]\n\n    pickle.dump(comments, open(\"comments_fe.pickle\", \"wb\"))\n    \n    # Display counts of each term\n    for i in frequencies_subs.keys():\n        print(str(i) + \": \" + str(sum(frequencies_subs[i])))","f77ceaab":"# Group posts into date ranges\ndate_ranges = list()\ni = 0\nwhile True:\n    date1 = date_begin + timedelta(days = i * months_per_interval * 365 \/ 12)\n    date2 = date_begin + timedelta(days = (i+1) * months_per_interval * 365 \/ 12)\n    # Skip last interval (not full)\n    if date2 >= date_final: break\n    date_ranges.append((date1, date2))\n    i += 1\n\nsubmissions_by_date = list()\ncomments_by_date    = list()\nfor date_range in date_ranges:\n    submissions_by_date.append(submissions[(submissions[\"created_utc\"] > date_range[0]) & (submissions[\"created_utc\"] < date_range[1])])\n    comments_by_date   .append(comments   [(comments   [\"created_utc\"] > date_range[0]) & (comments   [\"created_utc\"] < date_range[1])])","5fdbcc25":"def show_barplot(term_list, frequencies, date_strs, ymax=100, colored_date=\"\", use_frequencies=True):\n    fig, ax = plt.subplots(1, len(term_list), figsize=(7.5 * len(term_list), 7.5\/2))\n    \n    for plot_idx, term in enumerate(term_list):\n        if len(term_list) > 1:\n            if use_frequencies: ax[plot_idx].set_title(\"% usage of term '{}' over time\".format(term))\n            else:               ax[plot_idx].set_title(\"# of occurences of term '{}' over time\".format(term))\n    #         ax[plot_idx].tick_params(labelrotation=45)\n            ax[plot_idx].set_xticklabels(date_strs, rotation=45, ha=\"right\")\n            ax[plot_idx].set_ylim([0, ymax])\n    #         ax[plot_idx].set_xticks(rotation=45, ha=\"right\")\n            for idx in range(len(date_strs)):\n                date = date_strs[idx]\n                color = \"blue\"\n                if date == colored_date: color = \"red\"\n                ax[plot_idx].bar(date, frequencies[term][idx], color=color)\n        else:\n            if use_frequencies: ax.set_title(\"% usage of term '{}' over time\".format(term))\n            else:               ax.set_title(\"# of occurences of term '{}' over time\".format(term))\n    #         ax[plot_idx].tick_params(labelrotation=45)\n            ax.set_xticklabels(date_strs, rotation=45, ha=\"right\")\n            ax.set_ylim([0, ymax])\n    #         ax[plot_idx].set_xticks(rotation=45, ha=\"right\")\n            for idx in range(len(date_strs)):\n                date = date_strs[idx]\n                color = \"blue\"\n                if date == colored_date: color = \"red\"\n                ax.bar(date, frequencies[term][idx], color=color)\n        \n    plt.show()\n\ndef get_terms_by_date(terms_list, use_frequencies):\n    global frequencies\n    global date_strs\n    \n    frequencies = dict()\n    date_strs = list()\n    \n    for term in terms_list: frequencies[term] = list()\n\n    for dateidx in range(len(submissions_by_date)):\n        subs = submissions_by_date[dateidx]\n    #     date_start = date_ranges[dateidx][0]\n        date_end   = date_ranges[dateidx][1]\n        date_strs.append(date_end.strftime(\"%m\/%d\/%y\"))\n\n        total = 0 # Total count of all terms\n        if use_frequencies:\n            for term in terms_list: total += subs[term].sum()\n        for term in terms_list:\n            if use_frequencies: frequencies[term].append(subs[term].sum() \/ total * 100)\n            else:               frequencies[term].append(subs[term].sum())","3b3dbf2d":"frequencies = None\ndate_strs = None\n\nget_terms_by_date(terms.terms_models, True)\nshow_barplot([\"Deep Learning\", \"Neural Network\"], frequencies, date_strs, 60, \"01\/26\/16\")\nshow_barplot([\"Regression\", \"Deep Learning\", \"SVM\"], frequencies, date_strs, 30, \"01\/26\/16\")\nshow_barplot([\"RNN\", \"LSTM\", \"CNN\"], frequencies, date_strs, 30, \"01\/26\/16\")","7170aece":"get_terms_by_date(terms.terms_ensembles, True)\nshow_barplot([\"Bagging\", \"Boosting\", \"Stacking\"], frequencies, date_strs, 70, \"07\/28\/15\")\nshow_barplot([\"XGBoost\", \"ADABoost\", \"Random Forest\"], frequencies, date_strs, 50, \"07\/28\/15\")","b4e9d294":"my_terms = terms.terms_misc\nget_terms_by_date(my_terms, False)\nshow_barplot([\"Vanishing Gradient\"], frequencies, date_strs, 12, \"07\/28\/14\", use_frequencies=False)\n\nmy_terms = terms.terms_activations\nget_terms_by_date(my_terms, True)\nshow_barplot([\"Sigmoid\", \"tanh\", \"ReLU\", ], frequencies, date_strs, 100, \"07\/28\/14\")\nshow_barplot([\"ReLU\", \"ELU\"], frequencies, date_strs, 40, \"07\/28\/14\")","0993cd87":"my_terms = terms.terms_areas\nget_terms_by_date(my_terms, True)\nshow_barplot([\"Supervised Learning\", \"Unsupervised Learning\", \"Reinforcement Learning\"], frequencies, date_strs, 30)","f4fca999":"# Get total sub votes for each date range\ntotal_sub_votes = [0 for _ in range(len(submissions))]\nfor i in range(len(submissions_by_date)):\n    sel = submissions_by_date[i].id.unique()\n    sub_votes = submissions.id.apply(lambda id1: id1 in sel)\n    sub_votes *= submissions_by_date[i].score.sum()\n    total_sub_votes += sub_votes\nsubmissions[\"total_sub_votes\"] = total_sub_votes\nsubmissions = submissions[submissions.total_sub_votes != 0]","e70e83a7":"subs = submissions\n# subs = submissions_by_date[20]\n\nX = subs.drop([\n    \"created_utc\", \"num_comments\", \"score\", \"selftext\", \"title\", \"subreddit\", \"id\", \"author\"\n], axis=1)\ny = subs[[\"score\"]]\n# X = submissions_by_date[len(submissions_by_date)-1].drop([\n#     \"created_utc\", \"num_comments\", \"score\", \"selftext\", \"title\", \"subreddit\", \"id\", \"author\"\n# ], axis=1)\n# y = submissions_by_date[len(submissions_by_date)-1][[\"score\"]]\n\nX_train, X_test, y_train, y_test = sklearn.model_selection.train_test_split(\n    X, y, test_size = 0.20, random_state=42\n)\n\nX_train, X_val, y_train, y_val = sklearn.model_selection.train_test_split(\n    X_train, y_train, test_size = 0.25\n)","94e0ff9d":"model_lg = sklearn.linear_model.Lasso(alpha=.015)\nmodel_lg = model_lg.fit(X_train, y_train)\n\nprint(sklearn.metrics.mean_absolute_error(\n    y_test, model_lg.predict(X_test)\n))\n\nprint(sklearn.metrics.mean_squared_error(\n    y_test, model_lg.predict(X_test)\n))\n\n# print(model_lg.coef_.argmax())\n# print(X.columns[model_lg.coef_.argmax()])\ncoefs = sorted(model_lg.coef_)\nfor i in range(len(coefs)):\n    print(X.columns[i] + \" : \" + str(coefs[i]))","68ece8fe":"Now, we'll see if these terms being mentioned can influence the score of a post. We'll fit a linear regression LASSO model here to predict scores and see how well it does. Then, we can extract the weights of the model to see just how much influence each term has.\n\nAs we have plenty of terms being used here which are all sparse, we'll use LASSO regression here. Of course, the popularity of a post depends on the popularity of the subreddit. And, a subreddit grows over time. For that reason, all posts will include a feature for the number of votes. This will be passed into the model to roughly control for it.","a1215d10":"For the bar charts, we'll mark a single bar red at the same date among each chart, which corresponds to a time where some significant event occurred. For example, in the ensemble category, it will correspond to when XGBoost was first mentioned.","b51a06b8":"<div id=\"Modeling\">\n\n# Modeling","e54fbbec":"# Data Visualization","3217384d":"We would like to group posts according to what time period they were created. Here, we group together posts going in increments of 6 months, and construct a separate dataframe for each. We'll use date_ranges to indicate the start and end date for each period.","0885fcee":"What the model suggest here is that mentioning SVM's and most common deep learning terms will likely detract from the score of your post. Perhaps a lot of beginners like to use them. And the best term to mention here is NLP. It may be that most NLP posts are interesting or fun to read, and they capture people's attention.\n\nRemember that these features are not boolean - we are not marking whether a term occurred, we are marking how many times the term occurred. Meaning, mentioning a term multiple time will influence the predicted score. The degree to which this is accurate is debatable. Creating a post just saying 'NLP' over and over would probably not give you a good score.\n\nThe MAE and MSE error metrics are reported here too. This suggests that we are, on average, off in our predictions by 15 points. This may not be the most accurate model, but this metric really depends on a lot of factors - for example, the popularity of the subreddit.\n\nMost posts on the subreddit should be scored according to the content of their posts: is it interesting, does it makes sense, is it helpful, etc. So analyzing the terms used may not be all that important for this subreddit, but this model may be insightful nonetheless.","aa892cae":"### Get Term Frequencies","91fab14b":"This will guide you through how to scrape data from Reddit (using their API) and do some simple data analysis. This will include measuring the usage of Machine Learning terms over time using bar charts, and building a simple model to predict the score of a post by looking at which terms are used and how many times.\n\nIf you'd like to use the pre-made dataset, set use_dataset below to True. This will skip nearly all the data manipulation code.","4ff6cd60":"# Initialization","62e2a9dd":"# Data Scraping","22770e61":"After loading the .json files, we'll build two dataframes. We want to clean the dataset a little. We should set the datatypes of the columns for three reasons:\n1. Lowers the space used in memory\n2. Speeds up computation (we handle smaller amounts of data)\n3. Easier to use (retrieved data is in the right format. For example, timestamps are now treated as datetime objects)\n\nAlso, we'll give each post a *subreddit* feature incase we want to use multiple subreddits (we won't here)\n\nAnother thing we might want to do is seperate the number of replies to comments into direct replies, or number of replies within the subtree of each comment (for example, replies to replies of a comment). Though, we won't use this here. Feel free to comment it out - it can take a while.\n\nLastly, we'll filter out posts that have been deleted, as they'll be useless for text analysis, but we'll keep their comments. And we'll remove columns we no longer need (the created column)","970bc22d":"# Data Formatting","42af662c":"We'll plot some bar charts here. First, we'll set up a couple functions for making this easy.\n\nWe've created the get_terms_by_date function to count the occurrence of terms within each date range, with the use_frequencies parameter allowing us to control whether we want to get the total number off occurrences, or the frequency of occurrences. For example, in the category of activation functions, we may find that, say, 63% of all terms found in the category are referring to sigmoid.","94edb9a4":"We'll load the JSON files back from the disk","6d087e4f":"Here's where we'll do some text processing to find the terms used. We've already established a list of terms and their common representations (e.g. neural network vs neural net vs NN). Now, we'll use the NLTK library to match some more representations. To do this, we'll use a 'stemmer'. This essentially takes a term and produces a 'stem' string which ideally should allow us to differentiate between term representations by checking whether their stems match. For example, car', car's, and cars should have the same stem, and ally and allies should too. Words with similar meaning may have the same stem, but different prefixes and suffixes. Or, they may be pluralized... The list goes on.\n\nThere's no standard definition of a stem, and each stemmer may result in different stems. Many words will have a similar structure but different meaning. For example, should car and carpool have the same stem? This is subjective, but in this case, we use the Snowball stemmer.\n\nAnother trick we'll use is replacement. If a post uses a term 'neural-net', we want to change it to 'neural net' so it matches a term. We also want to ignore punctuation. We'll throw a few more tricks in there to speed up computation, though it still could be heavily optimized.\n\nOne thing we do is filter out common strings. Since terms can be multiple words, our code for finding them can be a little complex, so filtering can speed up our algorithm. Common strings may include post tags ([A]), and stopwords. Stopwords are simply words that are typically filtered out before processing, such as 'the' or 'and'. We don't have a use for these, so we'll filter them out using NLTK's list of stopwords. We only really care about words that are found within term representations. So, we could have built a hashmap of all these words to do better filtering, or perhaps built a different algorithm for finding terms.","8d508183":"# Data Splitting","9970fc43":"Here, we'll use the Pushshift API to gather posts and export them to JSON one at a time.\n\nThis allows us to pick up where we left off incase an error occurs at any point of this lengthy operation.","a1804cf5":"# Data Loading"}}