{"cell_type":{"58f85d96":"code","72a0687b":"code","6465db50":"code","a587d4bf":"code","cd993ba5":"code","7f3e08d5":"code","577a57cb":"code","7179fec0":"code","9dab95b4":"code","e91ab8a2":"code","45702530":"code","31b7130b":"code","1f51fdcd":"code","701f50be":"code","7ed4fc5a":"code","312bf7ce":"code","24adcc31":"code","2458124a":"code","e298051c":"code","b7a4b0a7":"code","4c8f2a03":"code","3b014dc8":"code","3a67bb71":"code","9c2853b8":"code","43c19afb":"code","44758aeb":"code","d6c65982":"code","dacad6d0":"code","22c67101":"code","8d2cee99":"code","71cd27a8":"code","d8068ee1":"code","63cc7bff":"code","3b01c355":"code","4933b5d8":"code","560e4ab5":"code","bafcc711":"code","9e97defb":"code","aadd58ce":"code","e8271919":"code","0c00bda8":"code","54e3ac69":"code","2e73e21c":"code","67187cfa":"code","73e5efe9":"code","dd1312db":"code","0119fb1a":"code","f4596816":"code","f069d239":"code","d691543c":"code","bacf1053":"code","f6bed4d2":"code","bb5a7c25":"code","b8104560":"code","36278d46":"code","1fe61654":"code","122f4d45":"code","48a36418":"code","3cc306d2":"markdown","3bb38242":"markdown","9ffcf787":"markdown","4d1f95a3":"markdown","6c6ef8c4":"markdown","0d12b499":"markdown","54c14fc8":"markdown","e13a1f32":"markdown","953f5a9d":"markdown","30ae9aec":"markdown"},"source":{"58f85d96":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport gc\nsns.set_style('darkgrid')\nsns.set_palette('viridis', n_colors=10)\ncolours = sns.color_palette('viridis', n_colors=10)\nfrom scipy import stats\nfrom sklearn.model_selection import RandomizedSearchCV\nfrom sklearn.metrics import mean_absolute_error\nfrom catboost import CatBoostRegressor, Pool  #, MetricVisualizer","72a0687b":"# Training CatBoost on a Kaggle kernel proved hard as \n# it quickly runs out of CPU RAM and the kernel restarts\n# The following is an attempt to reduce the memory footprint\n# https:\/\/www.kaggle.com\/gemartin\/load-data-reduce-memory-usage\n\ndef reduce_mem_usage(df):\n    \"\"\" iterate through all the columns of a dataframe and modify the data type\n        to reduce memory usage.        \n    \"\"\"\n    start_mem = df.memory_usage().sum() \/ 1024**2\n    print('Memory usage of dataframe is {:.2f} MB'.format(start_mem))\n    \n    for col in df.columns:\n        col_type = df[col].dtype\n        \n        if col_type != object:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)  \n            else:\n                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                    df[col] = df[col].astype(np.float16)\n                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)\n        else:\n            df[col] = df[col].astype('category')\n\n    end_mem = df.memory_usage().sum() \/ 1024**2\n    print('Memory usage after optimization is: {:.2f} MB'.format(end_mem))\n    print('Decreased by {:.1f}%'.format(100 * (start_mem - end_mem) \/ start_mem))\n    \n    return df\n\n\ndef import_data(file):\n    \"\"\"create a dataframe and optimize its memory usage\"\"\"\n    df = pd.read_csv(file, parse_dates=True, keep_date_col=True)\n    df = reduce_mem_usage(df)\n    return df","6465db50":"\nprint('-' * 80)\nprint('train')\ndf_train = import_data('..\/input\/allstate-claims-severity\/train.csv').drop('id', axis=1)\n\nprint('-' * 80)\nprint('test')\ndf_test = import_data('..\/input\/allstate-claims-severity\/test.csv')\ntest_ids = df_test['id']\ndf_test = df_test.drop('id', axis=1)","a587d4bf":"print('Shape of training data:', df_train.shape)\nprint('Shape of test data:', df_test.shape)","cd993ba5":"train_nulls = df_train.isna().sum()\ntest_nulls = df_train.isna().sum()\nprint('Train nulls:', train_nulls[train_nulls != 0])\nprint('Test nulls:', test_nulls[test_nulls != 0])\ndel train_nulls\ndel test_nulls","7f3e08d5":"df_train.describe().T","577a57cb":"skew = stats.skew(df_train['loss'])\nkurtosis = stats.kurtosis(df_train['loss'])\n\nfig, ax = plt.subplots(figsize=(10, 5))\nsns.histplot(data=df_train['loss'], ax=ax)\nax.set_title('Distribution of the target variable', \n             loc='left', \n             fontsize=16, \n             color=colours[0])\nax.set_xlabel('Loss (USD)')\ntextstr = f\"Mean: {df_train['loss'].mean():,.2f}\\\n            \\nMedian: {df_train['loss'].median():,.2f}\\\n            \\nSkew: {skew:.2f}\\nKurtosis: {kurtosis:.2f}\"\nprops = dict(facecolor='k', alpha=0.2)\nax.text(x=90000, y=5000, s=textstr, fontsize=12, bbox=props)\n\nplt.show()\n\ndel skew, kurtosis, fig, ax","7179fec0":"transformed_loss = np.log1p(df_train['loss'])\ndf_train_transform = pd.concat([df_train.drop('loss', axis=1), transformed_loss], \n                               axis=1)\nprint('Length of train df post transformation:', df_train_transform.shape[0])\n\ndel transformed_loss","9dab95b4":"skew = stats.skew(df_train_transform['loss'])\nkurtosis = stats.kurtosis(df_train_transform['loss'])\n\nfig, ax = plt.subplots(figsize=(10, 5))\nsns.histplot(data=df_train_transform['loss'], ax=ax)\nax.set_title('Distribution of the target variable', \n             loc='left', \n             fontsize=16, \n             color=colours[0])\nax.set_xlabel('Loss (USD)')\ntextstr = f\"Mean: {df_train_transform['loss'].mean():,.2f}\\\n            \\nMedian: {df_train_transform['loss'].median():,.2f}\\\n            \\nSkew: {skew:.2f}\\nKurtosis: {kurtosis:.2f}\"\nprops = dict(facecolor='k', alpha=0.2)\nax.text(x=0.5, y=3000, s=textstr, fontsize=12, bbox=props)\n\nplt.show()\n\ndel skew, kurtosis, fig, ax","e91ab8a2":"index_to_remove = df_train_transform[df_train_transform['loss'] < 1].index\nprint('Number of outliers to remove:', len(index_to_remove))\ndf_train_transform = df_train_transform.drop(index_to_remove)\nprint('Number of train observations:', df_train_transform.shape[0])\n\ndel index_to_remove","45702530":"cont_var = df_train.columns.values[-15:-1]\ncat_var = df_train.columns.values[:-15]","31b7130b":"# Check the skew of the other continuous variables\nprint('Skew of numerical variables:')\nfor var in cont_var:\n    print(f'{var.ljust(7)}: {stats.skew(df_train[var]):.2f}')\n    \ndel cont_var","1f51fdcd":"# Check for correlation with the target or amongst the independent variables\ncorrelation = df_train.corr()\nfig, ax = plt.subplots(figsize=(10, 7))\nsns.heatmap(correlation, annot=True, fmt='.2f', linewidth=2, ax=ax, cmap='viridis')\nax.set_title('Correlation of continuous variables', loc='left', fontsize=16, color=colours[3])\nplt.show()\n\ndel correlation, fig, ax","701f50be":"rng = np.random.RandomState(0)","7ed4fc5a":"n_observations = len(df_train_transform)\nprint('Number of training observations prior to split:', n_observations)\nval_ratio = 0.2\nn_val_observations = int(n_observations * val_ratio)\ndf_val = df_train_transform.sample(n=n_val_observations, \n                                    random_state=rng)\ndf_train = df_train_transform.drop(df_val.index)\n\nprint('======================================================')\nprint('Number of training observations post split:', df_train.shape[0])\nprint('Number of validation observations post split:', df_val.shape[0])\n\ndel n_observations, val_ratio","312bf7ce":"X_train = df_train.drop('loss', axis=1)\ny_train = df_train['loss']\nX_val =df_val.drop('loss', axis=1)\ny_val = df_val['loss']","24adcc31":"train_pool = Pool(data=X_train, \n                  label=y_train, \n                  cat_features=cat_var)\nval_pool = Pool(data=X_val, \n                label=y_val, \n                cat_features=cat_var)","2458124a":"model = CatBoostRegressor(iterations=4000,\n                          verbose=200,\n                          random_seed=101,\n                          loss_function='MAE',\n                          task_type='GPU')","e298051c":"model.fit(train_pool,\n          eval_set=val_pool, \n          early_stopping_rounds=100)","b7a4b0a7":"# Visualise loss on the training and validation data\ndef plot_loss(model):\n    \"\"\"\n    Plots a line graph of the training and validation error by iteration\n    for a catboost estimator. \n    Takes the catboost estimator as a parameter. \n    \"\"\"\n    train_error = model.evals_result_['learn']['MAE']\n    val_error = model.evals_result_['validation']['MAE']\n\n    fig, ax = plt.subplots(figsize=(8, 5))\n    ax.plot(train_error, color=colours[2], label='Training error')\n    ax.plot(val_error, color=colours[6], label='Validation error')\n    ax.set_xlabel('Iteration')\n    ax.set_ylabel('Log loss (MAE)')\n    ax.set_title('Training and validation loss', loc='left', fontsize=16, color=colours[2])\n    plt.legend()\n    plt.show()\n    \n\n# Evaluate MAE of predictions in USD not log USD\ndef evaluate_predictions(log_act, log_pred):\n    \"\"\"\n    Calculates the MAE for predictions made on a natural log scale.\n    log_act: the log transformed actual observations\n    log_pred: the log transformed predictions \n    \n    Returns the inverse transformed MAE\n    \"\"\"\n    inverse_pred = np.expm1(log_pred)\n    inverse_act = np.expm1(log_act)\n    return mean_absolute_error(inverse_act, inverse_pred)","4c8f2a03":"# MetricVisualizer('Catboost_regressor').start()\nplot_loss(model=model)","3b014dc8":"# Check the performance metric of the model on the inverse transformed target\npred = model.predict(X_val)\nmae = evaluate_predictions(y_val, pred)\nprint(f'MAE: {mae:.4f}')","3a67bb71":"# Plotting the residuals\ndef plot_residuals(y_true, y_pred):\n    residuals = y_true - y_pred\n    fig, axs = plt.subplots(nrows=1, ncols=2, figsize=(12, 5))\n\n    axs[0].hist(residuals, bins=60, color=colours[1])\n    ave = residuals.mean()\n    med = residuals.median()\n    skew = stats.skew(residuals)\n    textstr = f'Mean: {ave:.2f}\\nMedian: {med:.2f}\\nSkew: {skew:.2f}'\n#     axs[0].text(x=-5, \n#                  y=3500, \n#                  s=textstr, \n#                  fontsize=12)\n    axs[0].annotate(textstr,\n                    xy=(0.05, 0.8),\n                    xycoords='axes fraction',\n                    fontsize=12)\n    axs[0].set_title('Distribution of residuals', \n                      loc='left', \n                      fontsize=16,\n                      color=colours[1])\n    axs[0].set_xlabel('Residuals (log USD)')\n\n    axs[1].scatter(x=y_pred, y=residuals, color=colours[1], s=4)\n    axs[1].hlines(y=0, xmin=6, xmax=11, linestyle='--', color=colours[-2], linewidth=3)\n    axs[1].set_ylabel('Residuals (log USD)')\n    axs[1].set_xlabel('Predicted loss (log USD)')\n    axs[1].set_title('Residuals plot', \n                     loc='left', \n                     fontsize=16, \n                     color=colours[1])\n\n    plt.tight_layout()\n    plt.show()","9c2853b8":"plot_residuals(y_val, pred)","43c19afb":"del model, pred, mae\ngc.collect()","44758aeb":"# params = {\n#     'max_depth' : np.arange(4, 14, dtype=int),\n#     'colsample_bylevel': stats.uniform(loc=0.2, scale=0.3),\n#     'subsample': stats.uniform(loc=0.2, scale=0.3),\n#     'l2_leaf_reg': stats.uniform(loc=1, scale=9),\n#     'learning_rate': stats.uniform(loc=0.001, scale=0.199),\n#     'min_child_samples': np.arange(10, 500, dtype=int),\n#     'bagging_temperature': np.arange(1, 100, dtype=int),\n#     'random_strength': np.arange(0, 100, dtype=int)\n# }","d6c65982":"# model = CatBoostRegressor(iterations=2000, \n#                           verbose=200, \n#                           random_seed=101, \n#                           loss_function='MAE')","dacad6d0":"# search = RandomizedSearchCV(model, \n#                             param_distributions=params,\n#                             cv=3,\n#                             n_iter=10, \n#                             verbose=2)","22c67101":"# results = search.fit(X=X_train,\n#                      y=y_train,\n#                      eval_set=(X_val, y_val),\n#                      early_stopping_rounds=100,\n#                      cat_features=cat_var)\n\n# print(results.best_params_)","8d2cee99":"# best_params = {\n#     'bagging_temperature': 88, \n#     'colsample_bylevel': 0.26769090768437076, \n#     'l2_leaf_reg': 9.132279703937915, \n#     'learning_rate': 0.054376348705150734, \n#     'max_depth': 9, \n#     'min_child_samples': 296, \n#     'random_strength': 74, \n#     'subsample': 0.45510384145401844\n# }","71cd27a8":"# model = CatBoostRegressor(iterations=2500, \n#                           verbose=100, \n#                           random_seed=101, \n#                           loss_function='MAE', \n#                           train_dir='Catboost_regressor',\n#                           task_type='GPU', \n#                           **best_params)","d8068ee1":"# model.fit(train_pool,\n#           eval_set=val_pool,\n#           early_stopping_rounds=250)","63cc7bff":"# MetricVisualizer('Catboost_regressor').start()\n# plot_loss(model=model)","3b01c355":"# Check the performance metric of the model on the inverse transformed target\n# pred = model.predict(X_val)\n# mae = evaluate_predictions(y_val, pred)\n# print(f'MAE: {mae:.4f}')","4933b5d8":"df_test['loss'] = np.NaN\ndf_full = pd.concat([df_train_transform, df_test])\nprint('Total number observations:', df_full.shape[0])","560e4ab5":"def filter_cat(x, to_remove):\n    if x in to_remove:\n        return 'null'\n    else:\n        return x","bafcc711":"for col in list(df_train_transform.select_dtypes(include=['object']).columns):\n    if df_train_transform[col].nunique() != df_test[col].nunique() or\\\n        df_train_transform[col].nunique() != df_full[col].nunique():\n        set_train = set(df_train[col].unique())\n        set_test = set(df_test[col].unique())\n        remove_train = set_train - set_test\n        remove_test = set_test - set_train\n        remove = remove_train.union(remove_test)\n        \n        df_full[col] = df_full[col].apply(lambda x: filter_cat(x, remove), 1)","9e97defb":"df_train_2 = df_full[df_full['loss'].notna()]\ndf_test_2 = df_full[df_full['loss'].isna()]\n\nprint(f'shape of train data: {df_train_2.shape}')\nprint(f'shape of test data: {df_test_2.shape}')","aadd58ce":"df_val_2 = df_train_2.sample(n=n_val_observations, \n                             random_state=rng)\ndf_train_2 = df_train_2.drop(df_val_2.index)\n\nprint('======================================================')\nprint('Number of training observations post split:', df_train_2.shape[0])\nprint('Number of validation observations post split:', df_val_2.shape[0])","e8271919":"X_train_2 = df_train_2.drop('loss', axis=1)\ny_train_2 = df_train_2['loss']\nX_val_2 =df_val_2.drop('loss', axis=1)\ny_val_2 = df_val_2['loss']\n\ndel X_train, y_train, X_val, y_val, train_pool, val_pool","0c00bda8":"train_pool_2 = Pool(data=X_train_2, \n                  label=y_train_2, \n                  cat_features=cat_var)\nval_pool_2 = Pool(data=X_val_2, \n                label=y_val_2, \n                cat_features=cat_var)","54e3ac69":"# best_params = {\n#     'bagging_temperature': 88, \n#     'colsample_bylevel': 0.26769090768437076, \n#     'l2_leaf_reg': 9.132279703937915, \n#     'learning_rate': 0.054376348705150734, \n#     'max_depth': 9, \n#     'min_child_samples': 296, \n#     'random_strength': 74, \n#     'subsample': 0.45510384145401844\n# }","2e73e21c":"model = CatBoostRegressor(\n    iterations=4000, \n    verbose=200, \n    random_seed=101, \n    loss_function='MAE',\n    task_type='GPU'\n#     train_dir='Catboost_regressor_3',\n#     **best_params\n)","67187cfa":"model.fit(\n    train_pool_2,\n    eval_set=val_pool_2,\n    early_stopping_rounds=250\n)","73e5efe9":"# MetricVisualizer('Catboost_regressor_3').start()\nplot_loss(model=model)","dd1312db":"# Check the performance metric of the model on the inverse transformed target\npred = model.predict(X_val_2)\nmae = evaluate_predictions(y_val_2, pred)\nprint(f'MAE: {mae:.4f}')","0119fb1a":"# Plotting the residuals\nplot_residuals(y_val_2, pred)","f4596816":"df_train_3 = df_full[df_full['loss'].notna()]\ndf_test_3 = df_full[df_full['loss'].isna()]\n\nprint(f'Number of training observations: {df_train_3.shape[0]}')\nprint(f'Number of testing observations: {df_test_3.shape[0]}')","f069d239":"X_train_3 = df_train_3.drop('loss', axis=1)\ny_train_3 = df_train_3['loss']","d691543c":"train_pool_3 = Pool(data=X_train_3,\n                    label=y_train_3,\n                    cat_features=cat_var)","bacf1053":"model = CatBoostRegressor(iterations=5000, \n                          verbose=300, \n                          random_seed=101, \n                          loss_function='MAE',\n                          task_type='GPU')","f6bed4d2":"model.fit(\n    train_pool_3,\n#     eval_set=val_pool, \n#     early_stopping_rounds=2000\n)","bb5a7c25":"model.save_model('insurance_claim_severity_catboost')","b8104560":"pred = model.predict(df_test_3)\ninv_pred = np.expm1(pred)","36278d46":"submission = pd.DataFrame(\n    data=inv_pred,\n    columns=['loss']\n)\nsubmission.index = test_ids","1fe61654":"submission.loss.hist(bins=100)\nplt.show()","122f4d45":"submission.to_csv('submission.csv')","48a36418":"# Scored 1122.15397 with a late submission","3cc306d2":"## Removing categories unique to the training or testing data\n\nThere are numerous categorical features, which have categories only present in either the train or test data. These could be affecting model performance so I will remove these and replace them with 'null'. This will flag them as special values to the CatBoost model.","3bb38242":"# **Train and validation split**\n\nSince there is a large number of training observations, I will split some out as a validation set which I will use in appraising the performance of my models and which I will be able to use for early stopping of training of boosting algorithms.","9ffcf787":"There seems to be one datapoint that could be an outlier near zero, so I will remove this one row","4d1f95a3":"A modest improvement. When I ran this locally on CPU, combined with the hyper parameter tuning, I was getting MAE in the range of roughly 1030 to 1050 USD, depending on the cut of the data. The validation loss was still decreasing so I will increase the number of iterations greatly for the final model.","6c6ef8c4":"In the right hand plot there appears to be some heteroscedasticity in the error terms, however this likely just be due to the fact that there are many more insurance pay outs between 7 to 8 log USD and fewer around 9 log USD. \n\nThe histogram on the left suggests that the error terms are normally distributed.|","0d12b499":"The target variable, loss, appears to have a non-normal distribution, with the median significantly lower than the mean. This suggests that it has a positive skew, with a few very large insurance claims. This kind of distribution is common with amounts of money.","54c14fc8":"## Hyper-parameter tuning\n\n**NOTE: Unfortunately I was unable to get the good results from hyper-parameter tuning on the Kaggle GPU that I was getting locally on a CPU. Further, I cannot run the CatBoost training on a Kaggle CPU as it runs of memory and the kernel restarts. As such, I will just have to use the default hyper parameters.**","e13a1f32":"No null values in either the train or test data, which is good news.","953f5a9d":"# **Modelling**\n\n## Initial performance prior to tuning","30ae9aec":"# **Final model**\n\nI will train the model on all of the training observations (train + val), in addition to running more iterations. "}}