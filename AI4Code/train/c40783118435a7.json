{"cell_type":{"d72792a0":"code","06bca729":"code","d90e4c81":"code","cf4d2743":"code","0b734321":"code","62d0ea88":"code","383daed7":"code","7a504eb7":"code","4f9ed345":"code","6c2034a9":"code","e17b5903":"code","ba18a01e":"code","409a47ac":"markdown","74616a32":"markdown","2ca7e7c2":"markdown","b21aebcf":"markdown","b2441734":"markdown","18068ceb":"markdown","531642d7":"markdown","9a94a8d1":"markdown","e7d4cce9":"markdown","db25fe68":"markdown","e687fcc8":"markdown","46beee63":"markdown","fd19d09c":"markdown"},"source":{"d72792a0":"from mpl_toolkits import mplot3d\nfrom mpl_toolkits.mplot3d import Axes3D\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport matplotlib\nfrom matplotlib import pyplot as plt\nfrom celluloid import Camera\nfrom IPython.display import HTML\nimport warnings\nwarnings.filterwarnings('ignore')\n\n%matplotlib inline","06bca729":"fig = plt.figure(figsize=(10,7))\nax = plt.axes(projection='3d')\nu = np.linspace(-5.5, 5.5, 100)\nx,y = np.meshgrid(u,u)\nz = ((x)**2 + (y)**2)\nax.plot_surface(x,y,z,rstride=1,cstride=1,\n                cmap='viridis', edgecolor = 'none')\nax.set_title('Visualiza\u00e7\u00e3o de f(w)');\nax.set_xlabel('$w_1$')\nax.set_ylabel('$w_2$')\nax.set_zlabel('f(w)');","d90e4c81":"fig = plt.figure(figsize=(10,7))\nax = plt.axes()\ncontours = plt.contour(x,y,z,15, colors = 'black')\nplt.clabel(contours, inline = True, fontsize = 8)\nplt.imshow(z, extent=[-5.5, 5.5, -5.5, 5.5], origin = 'lower',\n           cmap = 'viridis', alpha = 0.5, )\nax.set_title('Curvas de n\u00edvel')\nax.set_xlabel('$w_1$')\nax.set_ylabel('$w_2$')\nplt.colorbar();","cf4d2743":"fig = plt.figure(figsize=(10,7))\nax = plt.axes(projection='3d')\nax.contour3D(x,y,z, 15, cmap='viridis')\nax.set_xlabel('x')\nax.set_ylabel('y')\nax.set_zlabel('z')\nax.set_title('Curvas de N\u00edvel de f(x,y)');","0b734321":"def objective_f(w):\n    return w[0]**2 + w[1]**2","62d0ea88":"def gradient(w):\n    first_term = w[0]*2\n    second_term = w[1]*2\n    return np.array([first_term, second_term])","383daed7":"def gradient_descent(w, ts, max_iter = 400, verbose = False):\n    ##Lista de todos os valores da fun\u00e7\u00e3o e dos pontos estimados para todos os t\n    f_values = list()\n    ws = list()\n    for t in ts:\n        ##Lista de valores da fun\u00e7\u00e3o e dos pontos estimados para cada valor de t especifico\n        w_t = list()\n        f_value = list()\n        k = 0\n        w_atual = w\n        w_ant = np.array([np.inf] * len(w))\n        while k < max_iter:\n            w_t.append(w_atual)\n            k += 1\n            f_value.append(objective_f(w_atual))            \n            grad = gradient(w_atual)\n            w_ant = w_atual.copy()\n            ##Atualiza\u00e7\u00e3o da estimativa de w\n            w_atual = w_atual - t * grad\n            if verbose:\n                print('Itera\u00e7\u00e3o {}: {}\\nt: {}'.format(k, w_atual, t))\n        f_values.append(f_value)\n        ws.append(w_t)\n        print('------------------------------------------------------------------------')\n        print('\\033[1m'+'M\u00e9todo do Gradiente Estoc\u00e1stico para max_iter = {} e t = {}'.format(max_iter, t) + '\\033[0m')\n        print('------------------------------------------------------------------------')\n        print('W = {}\\nf = {}\\nItera\u00e7\u00f5es = {}'.format(w_atual, objective_f(w_atual), k))\n        print('------------------------------------------------------------------------')\n    return f_values, ts, ws","7a504eb7":"points, ts, ws = gradient_descent(np.array([1,4]),[1, 0.1, 0.001], 50)","4f9ed345":"fig = plt.figure(figsize=(10,7))      \nfor point, t in zip(points, ts):\n    plt.plot(point, label = 't = ' + str(round(t,10)));\n\nplt.legend(loc='upper right', frameon = False)\nplt.title('Descida da fun\u00e7\u00e3o', size = 14)\nplt.xlabel('N\u00famero de intera\u00e7\u00f5es', size = 14)\nplt.ylabel(r'$f(x)$', size = 14);","6c2034a9":"fig = plt.figure(figsize=(10,7))\ncamera = Camera(fig)\n\nfor i in range(50):\n    ax = plt.axes()\n    contours = plt.contour(x,y,z,15, colors = 'black')\n    plt.clabel(contours, inline = True, fontsize = 8)\n    plt.imshow(z, extent=[-5.5, 5.5, -5.5, 5.5], origin = 'lower',\n               cmap = 'viridis', alpha = 0.5, )\n    ax.set_title('Curvas de n\u00edvel')\n    ax.set_xlabel('$w_1$')\n    ax.set_ylabel('$w_2$')\n    plt.plot([ws[0][i][0]], [ws[0][i][1]], marker='o', markersize=5, color=\"red\")\n    camera.snap()\nanimation = camera.animate()\nplt.close(\"all\")\nHTML(animation.to_html5_video())","e17b5903":"fig = plt.figure(figsize=(10,7))\ncamera = Camera(fig)\n\nfor i in range(50):\n    ax = plt.axes()\n    contours = plt.contour(x,y,z,15, colors = 'black')\n    plt.clabel(contours, inline = True, fontsize = 8)\n    plt.imshow(z, extent=[-5.5, 5.5, -5.5, 5.5], origin = 'lower',\n               cmap = 'viridis', alpha = 0.5, )\n    ax.set_title('Curvas de n\u00edvel')\n    ax.set_xlabel('$w_1$')\n    ax.set_ylabel('$w_2$')\n    plt.plot([ws[1][i][0]], [ws[1][i][1]], marker='o', markersize=5, color=\"red\")\n    camera.snap()\nanimation = camera.animate()\nplt.close(\"all\")\nHTML(animation.to_html5_video())","ba18a01e":"fig = plt.figure(figsize=(10,7))\ncamera = Camera(fig)\n\nfor i in range(50):\n    ax = plt.axes()\n    contours = plt.contour(x,y,z,15, colors = 'black')\n    plt.clabel(contours, inline = True, fontsize = 8)\n    plt.imshow(z, extent=[-5.5, 5.5, -5.5, 5.5], origin = 'lower',\n               cmap = 'viridis', alpha = 0.5, )\n    ax.set_title('Curvas de n\u00edvel')\n    ax.set_xlabel('$w_1$')\n    ax.set_ylabel('$w_2$')\n    plt.plot([ws[2][i][0]], [ws[2][i][1]], marker='o', markersize=5, color=\"red\")\n    camera.snap()\nanimation = camera.animate()\nplt.close(\"all\")\nHTML(animation.to_html5_video())","409a47ac":"Considere a fun\u00e7\u00e3o $f(w) = w_1^2 + w_2^2$.\n\nTemos que o gr\u00e1fico para essa fun\u00e7\u00e3o \u00e9 o exposto abaixo:","74616a32":"## Gradiente Descendente","2ca7e7c2":"### Gradiente Descendente para $t=0.001$","b21aebcf":"Por \u00faltimo, quis trazer uma visualiza\u00e7\u00e3o de como esse processo est\u00e1 ocorrendo quando olhamos para o gr\u00e1fico da fun\u00e7\u00e3o.\n\n### Gradiente Descendente para $t=1$","b2441734":"### Gradiente Descendente para $t=0.1$","18068ceb":"Agora, faremos a implementa\u00e7\u00e3o do m\u00e9todo do gradiente descendente. Como argumentos esse m\u00e9todo receber\u00e1 o ponto inicial, uma lista com diferentes valores de $t$ (taxa de aprendizado) e o n\u00famero m\u00e1ximo de itera\u00e7\u00f5es. Tamb\u00e9m passamos um argumento de verbosidade para casos em que queiramos ver em detalhes o que est\u00e1 ocorrendo.\n\nComo outputs, essa fun\u00e7\u00e3o nos retornar\u00e1 uma lista com os valores da fun\u00e7\u00e3o a cada itera\u00e7\u00e3o, uma lista com os valores de $t$ utilizados e uma lista com os valores do pontos que foram sendo atualizados a cada itera\u00e7\u00e3o.","531642d7":"## Implementa\u00e7\u00e3o\n\nA seguir iremos realizar a implementa\u00e7\u00e3o do m\u00e9todo do gradiente descendente para a fun\u00e7\u00e3o exposta. Vale comentar que esse \u00e9 um exemplo ilustrativo e por esse motivo escolhi essa fun\u00e7\u00e3o, mesmo sabendo que a mesma possui uma solu\u00e7\u00e3o fechada.\n\nPrimeiramente iremos definir a fun\u00e7\u00e3o e o gradiente dela, para reutiliz\u00e1-los depois.","9a94a8d1":"Para facilitar a compreens\u00e3o das curvas de n\u00edvel, podemos verificar como ocorre a cria\u00e7\u00e3o delas. \u00c9 como se peg\u00e1ssemos cortes da figura em 3 dimens\u00f5es e depois projet\u00e1ssemos no plano inferior, de forma que, ao observar de cima, vemos apenas as curvas de n\u00edvel.","e7d4cce9":"## Gradiente Descendente\n\nO m\u00e9todo do Gradiente Descendente \u00e9 um m\u00e9todo iterativo de otimiza\u00e7\u00e3o que faz uso do oposto do gradiente para encontrar o m\u00ednimo de uma fun\u00e7\u00e3o (minimizar a fun\u00e7\u00e3o). Esse processo iterativo ocorre da seguinte maneira:\n\n- Damos uma estimativa de solu\u00e7\u00e3o, que \u00e9 conhecido como ponto inicial;\n- Atualizamos essa estimativa por meio de do oposto do gradiente;\n- Realizamos os passos acima at\u00e9 que um dos crit\u00e9rios de parada, que discutiremos adiante, seja satisfeito.\n\nLogo, podemos escrever esse m\u00e9todo como:\n\n$$x^{k+1}=x^k-t^k\\nabla f(x^k)$$\n\nOnde $t$ \u00e9 a taxa de aprendizado, que pode ser interpretada como sendo o quanto estamos andando na dire\u00e7\u00e3o escolhida. A escolha de $t$ afeta diretamente a converg\u00eancia do m\u00e9todo. Se escolhermos um valor de $t$ muito grande o m\u00e9todo pode n\u00e3o convergir e se a escolha for um $t$ muito pequeno o m\u00e9todo pode demorar muito para convergir.\n\nA escolha de $t$ n\u00e3o precisa ser feita de maneira aleat\u00f3ria, existindo alguns m\u00e9todos que podem auxiliar, como por exemplo a Busca Linear, conhecido como *Backtracking*.\n\n### Crit\u00e9rios de parada\n\nExistem alguns poss\u00edveis crit\u00e9rios de paradas. Dentre eles podemos citar:\n\n- N\u00famero m\u00e1ximo de itera\u00e7\u00f5es;\n- Toler\u00e2ncia na varia\u00e7\u00e3o de $x^{k+1}$ e $x^k$, ou seja, a m\u00ednima varia\u00e7\u00e3o aceit\u00e1vel;\n- Norma do gradiente\n\nNos exemplos abaixo utilizaremos como crit\u00e9rio de parada o n\u00famero m\u00e1ximo de itera\u00e7\u00f5es.","db25fe68":"Ent\u00e3o iremos analisar os resultados para diferentes taxas de aprendizado (\"grande\", \"m\u00e9dia\" e \"pequena\"), para um n\u00famero m\u00e1ximo de 50 itera\u00e7\u00f5es. Depois iremos discutir os resultados.\n\nUtilizamos como ponto inicial $x=1$ e $y=4$, escolhidos aleatoriamente. J\u00e1 para as taxas de aprendizado iremos avaliar $t=1$, $t=0.1$ e $t=0.001$.","e687fcc8":"## Mas o que \u00e9 o Gradiente?\n\nO gradiente \u00e9 o vetor que indica o sentido e a dire\u00e7\u00e3o de onde temos o maior incremento da fun\u00e7\u00e3o, em outras palavras, \u00e9 o vetor que indica o ponto onde a fun\u00e7\u00e3o apresenta os maiores resultados.\n\nMas se n\u00f3s queremos encontrar o m\u00ednimo da fun\u00e7\u00e3o e o gradiente n\u00f3s aponta o m\u00e1ximo, como conseguimos relacionar os dois?\n\n**\u00c9 simples, maximizar $f(x)$ \u00e9 o mesmo que minimizar $-f(x)$.** Logo, se queremos minimizar $f(w)$, basta a gente maximizar $-f(w)$ e dessa forma conseguimos relacionar a minimiza\u00e7\u00e3o com o gradiente. Sendo assim, para encontrar o sentido e a dire\u00e7\u00e3o do local de maior decrescimento da fun\u00e7\u00e3o basta olharmos para $-\\nabla f(w)$.\n\n### E como calculamos o gradiente?\n\n$$\\nabla f(w) = \\left({\\begin{array}{c} \\frac{\\partial f(w)}{\\partial w_1} \\\\ \\frac{\\partial f(w)}{\\partial w_2} \\end{array}}\\right)$$\n\nPara o c\u00e1lculo do gradiente \u00e9 necess\u00e1rio calcular todas as derivadas parciais de f. Sendo assim, para o nosso caso, temos:\n\n$$\\frac{\\partial f(w)}{\\partial w_1} = 2w_1$$\n$$\\frac{\\partial f(w)}{\\partial w_2} = 2w_2$$\n\n$$\\therefore \\nabla f(w) = \\left({\\begin{array}{c} 2w_1 \\\\ 2w_2 \\end{array}}\\right)$$","46beee63":"Abaixo, temos o plot do valor da fun\u00e7\u00e3o ao longo das itera\u00e7\u00f5es para cada um dos valores de $t$. Como podemos ver, para o caso de $t=1$ a fun\u00e7\u00e3o n\u00e3o converge e tamb\u00e9m n\u00e3o muda o seu valor. J\u00e1 para o caso de $t=0.1$ vemos que ocorreu a converg\u00eancia de forma r\u00e1pida, enquanto para $t=0.001$ vemos que est\u00e1 tendendo a convergir, mas de forma extremamente lenta.","fd19d09c":"Ao observar de suas curvas de n\u00edveis podemos entender melhor sobre as \u00e1reas com resultados mais elevados e tamb\u00e9m sobre as \u00e1reas com os menores valores de $f(w)$. No caso, temos que a regi\u00e3o central \u00e9 a que apresenta o menor valor para $f(w)$, enquanto as bordas apresentam valores mais elevados. Logo, se quisessemos minimizar essa fun\u00e7\u00e3o nosso objetivo seria chegar \u00e0 regi\u00e3o central, de forma simplista."}}