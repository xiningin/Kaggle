{"cell_type":{"ee73935b":"code","06841451":"code","05dac0cb":"code","cc885496":"code","3fa80279":"code","a56eca9c":"code","eb7c01ba":"code","d73747a5":"code","32919d48":"code","49a8768f":"code","5ed942e9":"code","2c4319df":"code","2ff7554d":"code","63a2a8e3":"code","f2dead1f":"code","847a91d9":"code","c6ef05da":"code","038ac406":"code","37f7df3f":"code","465472ce":"code","b16d4113":"code","a85dafd8":"code","bb2651e0":"code","d44b584c":"code","60db36e1":"code","e1c40d86":"markdown","de4c83d9":"markdown","4610cb2e":"markdown","cf7a325b":"markdown","f127b491":"markdown","f469820d":"markdown","f424151f":"markdown","9b15cf93":"markdown","86302bbe":"markdown","e6f2a4fe":"markdown","11ba6321":"markdown","bccd76b5":"markdown","68d96b1b":"markdown","48819dcf":"markdown","09b33f11":"markdown"},"source":{"ee73935b":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport os\nfrom mlxtend.frequent_patterns import apriori\nfrom mlxtend.frequent_patterns import association_rules\n\nplt.rcParams['figure.figsize'] = [12, 8]","06841451":"# Load dataset\ndf = pd.read_csv('\/kaggle\/input\/gufhtugu-publications-dataset-challenge\/GP Orders - 4.csv')\ndf.head(20)","05dac0cb":"df.info()  # View column names and their data types","cc885496":"# Get number of missing values in each columnn\npd.isna(df).sum()","3fa80279":"# Rename columns to make them easier to use\ndf.columns = ['order_number', 'order_status', 'book_name', 'order_date', 'city']\ndf.head()","a56eca9c":"df['order_status'].value_counts()  # View values in column 'order_status' along with their frequencies","eb7c01ba":"df['book_name'].value_counts().head(10)  # View 10 most frequent values in column 'book_name' along with their frequencies","d73747a5":"df['city'].value_counts().head(10)  # View 10 most frequent values in column 'book_name' along with their frequencies","32919d48":"# Split the column on '\/'\ns = df['book_name'].str.split('\/', expand=True).stack()\n\n# Melting dataframe so that we have one book in each row\ns.index = s.index.droplevel(-1) # to line up with df's index\ns.name = 'book_name' # needs a name to join\n\ndf = df.drop(columns='book_name').join(s)\ndf.head(10)","49a8768f":"missing_book_names_mask = df['book_name'].str.contains(pat=\"^[-? ]+$\", na=False)\ndf.loc[missing_book_names_mask, 'book_name'] = np.nan\n\nmissing_cities_mask = df['city'].str.contains(pat=\"^[-? ]+$\", na=False)\ndf.loc[missing_cities_mask, 'city'] = np.nan\n\ndf.head()","5ed942e9":"# Further clean book_name and city columns by transforming entries to upper case and stripping periods\ndf['book_name'] = df['book_name'].str.upper()\ndf['city'] = df['city'].str.upper().str.strip('.')\n\ndf.head()","2c4319df":"# We notice that some books are appearing multiple times under different names. We replace some of the most popular books that have this problem\ndf['book_name'].replace({\"Linux - An Introduction  (Release Data - October 3, 2020)\": \"LINUX - AN INTRODUCTION\", \n                         \"PYTHON PROGRAMMING- RELEASE DATE: AUGUST 14, 2020\": \"PYTHON PROGRAMMING\",\n                         \"(C++) ++\u0633\u06cc\" : \"(C++)\"}, inplace=True)","2ff7554d":"# order_status is a categorical variable and order_date is datetime so change dtypes accordingly.\ndf['order_status'] = pd.Categorical(df['order_status'])\ndf['order_date'] = pd.to_datetime(df['order_date'])","63a2a8e3":"# Plot number of orders against time\n\ndf['date'] = df['order_date'].dt.date\ndf_sales = pd.DataFrame({'count_sales': df['date'].value_counts().sort_index()})\ndf_sales.index = pd.to_datetime(df_sales.index)\n\ndf_sales.head()","f2dead1f":"df_sales.plot(y='count_sales')\nplt.show()","847a91d9":"# Smooth plot so we can see trends more clearly. Using weekly moving average.\ndf_weekly_ma = df_sales.rolling(7, min_periods=1).mean()\n\ndf_weekly_ma.plot(y='count_sales')\nplt.show()","c6ef05da":"# Visualize order status values and label each bar with percentage of total\ntotal = len(df)\nax = sns.countplot(x=\"order_status\", data=df)\nfor p in ax.patches:\n    height = p.get_height()\n    ax.text(p.get_x()+p.get_width()\/2.,\n            height + 3,\n            f\"{height\/total*100 :1.2f}%\",\n            ha=\"center\")\n\nax.set(xlabel='Order Status', ylabel='Total Sales Received')\nax.set_title(\"Order Status Values (as Percentages of Total Orders)\",fontsize=15)\n\nplt.show()","038ac406":"n = 5  # Number of best selling books to show\n\nbest_sellers = df['book_name'].value_counts().head(n)\n\n# Visualize sales of best selling books and label each bar with percentage of total\ntotal = len(df)\nplt.figure(figsize=(16,8))\nax = sns.barplot(best_sellers.index, best_sellers.values)\nfor p in ax.patches:\n    height = p.get_height()\n    ax.text(p.get_x()+p.get_width()\/2.,\n            height + 3,\n            f\"{height\/total*100 :1.2f}%\",\n            ha=\"center\")\n\nax.set(xlabel='Book Names', ylabel='Total Sales')\nax.set_title(\"Best Selling Books (Sales as Percentages of Total Orders)\",fontsize=15)\nax.set_xticklabels(ax.get_xticklabels(), rotation=40, ha=\"right\")\n\nplt.show()","37f7df3f":"df_sales['day_of_week'] = df_sales.index.day_name()\ndf_sales['day_of_week'] = pd.Categorical(df_sales['day_of_week'], ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday'])\n\ndf_sales['month'] = df_sales.index.month_name()\ndf_sales['month'] = pd.Categorical(df_sales['month'], df_sales['month'].drop_duplicates().tolist())\n\ndf_sales['year'] = df_sales.index.year\n\ndf_sales.head()","465472ce":"month_aggregated = pd.DataFrame(df_sales.groupby(\"month\")[\"count_sales\"].sum()).reset_index().sort_values('month')\nax = sns.barplot(data=month_aggregated,x=\"month\",y=\"count_sales\")\nax.set(xlabel='Month', ylabel='Total Sales received')\nax.set_title(\"Total Sales By Month\",fontsize=15)\n\nplt.show()","b16d4113":"day_aggregated = pd.DataFrame(df_sales.groupby(\"day_of_week\")[\"count_sales\"].sum()).reset_index().sort_values('count_sales')\nax = sns.barplot(data=day_aggregated,x=\"day_of_week\",y=\"count_sales\")\nax.set(xlabel='Day of the Week', ylabel='Total Sales received')\nax.set_title(\"Total Sales by Day of the Week\",fontsize=15)\n\nplt.show()","a85dafd8":"# Transform data into form required by mlextend\ndf_dropped_book_names_na= df.dropna(subset=['book_name'])  # Drop NAs in book name column\nseries_book_names_as_lists = df_dropped_book_names_na.groupby('order_number')['book_name'].apply(list)\n\n# The mlextend library requires book names to be columns and values to represent whether that book was present in the order \n# (binary value- 1 denotes book was present in order while 0 denotes that it was not present)\ndf_counts_binarized = series_book_names_as_lists.map(lambda x: '\/'.join((map(str, x)))).str.get_dummies(sep='\/')\ndf_counts_binarized","bb2651e0":"freq_items = apriori(df_counts_binarized, min_support=0.01, use_colnames=True, verbose=1)\nfreq_items.head(10)","d44b584c":"rules_mlxtend = association_rules(freq_items, metric=\"lift\", min_threshold=1).sort_values('lift', ascending=False) # sort by lift to get rules where association is strongest\nrules_mlxtend.head(10)","60db36e1":"import networkx as nx\nimport matplotlib.pyplot as plt\n\ndef draw_graph(rules, rules_to_show):\n    G1 = nx.DiGraph()\n    color_map=[]\n    N = 50\n    colors = np.random.rand(N)    \n    strs=['R0', 'R1', 'R2', 'R3', 'R4', 'R5', 'R6', 'R7', 'R8', 'R9', 'R10', 'R11']\n\n    for i in range(rules_to_show):\n        G1.add_nodes_from([\"R\"+str(i)])\n        for a in rules.iloc[i]['antecedents']:\n            G1.add_nodes_from([a])\n            G1.add_edge(a, \"R\"+str(i), color=colors[i] , weight = 2)\n        for c in rules.iloc[i]['consequents']:\n            G1.add_nodes_from([c])\n            G1.add_edge(\"R\"+str(i), c, color=colors[i],  weight=2)\n\n    for node in G1:\n        found_a_string = False\n        for item in strs: \n            if node==item:\n                found_a_string = True\n        if found_a_string:\n            color_map.append('yellow')\n        else:\n            color_map.append('green')       \n\n    edges = G1.edges()\n    colors = [G1[u][v]['color'] for u,v in edges]\n    weights = [G1[u][v]['weight'] for u,v in edges]\n\n    pos = nx.spring_layout(G1, k=16, scale=1)\n    nx.draw(G1, pos, edges=edges, node_color = color_map, edge_color=colors, width=weights, font_size=16, \n            with_labels=False)            \n\n    for p in pos:  # raise text positions\n        pos[p][1] += 0.07\n        nx.draw_networkx_labels(G1, pos)\n        plt.show()\n        \ndraw_graph (rules_mlxtend, 10)","e1c40d86":"# Data Cleaning","de4c83d9":"## How does the number of orders vary by month?","4610cb2e":"We also notice that missing values in the book_name and city columns are represented with question marks. The number of question marks is not fixed and entries that denote missing values may also contain - or whitespace. So we find all such entries using a regular expression and replace with NA.","cf7a325b":"## Does the number of sales depend on the day of the week?","f127b491":"Support is a measure of the popularity of the itemset. It is the proportion of transactions that the itemset is included in (for example if a book is part of 50% of transactions, it's support is 0.5). We set value of minimum support to 0.01 i.e. we only consider itemsets that are part of at least 1% of all orders on the site.","f469820d":"We uncover an interesting insight. Sales are lowest mid-week (on tuesday, wednesday and thursday) while they are highest on the weekend.","f424151f":"Lift indicates how likely the purchase of an item Y is when an item X is purchased while controlling for the popularity of item Y. High values of lift mean that item Y is likely to be purchased when item X is purchased. A lift value of 1 implies there is no association between the sale of the two items while values smaller than 1 denote that item Y is unlikely to be purchased when item X is purchased.","9b15cf93":"# Exploratory Data Analysis","86302bbe":"We notice that each entry in the column may contain multiple book names seperated by '\/'. So we split the column on '\/'. We then transform the obtained dataframe so that each row corresponds to a single book sale.","e6f2a4fe":"We see an abrupt drop in demand in February 2020 which increases in May. This is likely an affect of COVID and the resulting lockdown.","11ba6321":"We do not have enough data to judge whether there is yearly seasonality.","bccd76b5":"## Ideas for further work:\n1. Fix urdu text being unreadable in plots,\n2. Answering questions included in dataset description,\n3. Analyze order cities,\n4. Visualize order timings,\n5. Analyze returns (which cities are they coming from, which books are frequently returned etc.)\n6. Timeseries Analysis (check for stationarity, transform as required, fit and analyze timeseries models).\n\nMore coming soon!","68d96b1b":"## What are the best selling books?","48819dcf":"# Gufhtugu Publications- Visualization & Basket Analysis (Apriori Algorithm)\nThis notebook aims to uncover interesting insights from the Gufhtugu dataset, which is one of the largest public e-commerce datasets from Pakistan. It also applies and visualizes the apriori algorthm to obtain association rules from the dataset.\n\nThis notebook is split into 3 main parts:\n1. Data cleaning,\n2. Visualization & analysis,\n3. Market basket analysis (apriori algorithm).\n    \nThe analysis addresses the following questions:\n* What are the best selling books?\n* How does the number of orders vary by month?\n* Does the number of sales depend on the day of the week?\n    \nMore coming soon! Contributions and feedback welcome. Connect with me on [Linkedin](http:\/\/www.linkedin.com\/in\/muhammad-ali-857016172\/).","09b33f11":"# Market Basket Analysis Using Association Rules and the Apriori Algorithm\n\n## What is the Apriori Algorithm?\n> \"Apriori is an algorithm for frequent item set mining and association rule learning over relational databases. It proceeds by identifying the frequent individual items in the database and extending them to larger and larger item sets as long as those item sets appear sufficiently often in the database. The frequent item sets determined by Apriori can be used to determine association rules which highlight general trends in the database: this has applications in domains such as market basket analysis.\"\n\\- Wikipedia, 2020\n\n## Useful Links:\nThe following resources were used as references for this section of the notebook:\n1. [Wikipedia entry on the Apriori algorithm](https:\/\/en.wikipedia.org\/wiki\/Apriori_algorithm),\n2. [Kdnuggets.com article \"Association Rules and the Apriori Algorithm: A Tutorial\"](https:\/\/www.kdnuggets.com\/2016\/04\/association-rules-apriori-algorithm-tutorial.html\/2),\n3. [Kaggle notebook by Yogesh](https:\/\/www.kaggle.com\/yugagrawal95\/market-basket-analysis-apriori-in-python) which was used as a reference in writing this code,\n4. Network graph code via [intelligentonlinetools.com](https:\/\/intelligentonlinetools.com\/blog\/2018\/02\/10\/how-to-create-data-visualization-for-association-rules-in-data-mining\/)."}}