{"cell_type":{"1d4dc853":"code","06fd23e7":"code","9defe3bd":"code","94603df3":"code","1a6e7eaf":"code","42e6b46e":"code","4eff8e6f":"code","3cd9e825":"code","2880d769":"code","f25d7bb3":"code","5c6edd23":"code","0aeb9812":"code","985a97dc":"code","d7be2aeb":"code","ec742af2":"code","c47a4c47":"code","ff71e62a":"code","6ed7cb2b":"code","6a0f76f2":"code","bfa64c85":"code","4af1e4af":"code","e224ed1f":"code","ea2a478c":"code","5aa5b36c":"code","69837a41":"code","83ce3c8d":"code","dec8a294":"code","71f38cd4":"code","d551f99d":"code","759e3887":"code","4f3cf850":"code","ee31e4e3":"code","6b545498":"code","c6c36ef8":"code","ef77d77a":"code","a42663b7":"code","a28d8ea8":"code","11f34997":"code","04b1c670":"code","b12696dc":"code","7c64807a":"code","d1d07f71":"markdown","ec0b8890":"markdown","51460c5f":"markdown","07f050fd":"markdown"},"source":{"1d4dc853":"# Import the latest version of wandb\n# !pip install -q --upgrade wandb","06fd23e7":"!\/opt\/conda\/bin\/python3.7 -m pip install --upgrade pip\n! pip install -q efficientnet","9defe3bd":"!nvidia-smi","94603df3":"import tensorflow as tf\nprint(tf.__version__)\nfrom tensorflow.keras import layers\nfrom tensorflow.keras import models\nimport tensorflow_addons as tfa\nfrom tensorflow.keras import mixed_precision\n\n\n\nimport os\nimport gc\nimport json\nimport numpy as np\nimport pandas as pd\nfrom tqdm import tqdm\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.utils.class_weight import compute_class_weight\n\n# Imports for augmentations. \nfrom albumentations import (Compose, RandomResizedCrop, Cutout, Rotate, HorizontalFlip, \n                            VerticalFlip, RandomBrightnessContrast, ShiftScaleRotate, \n                            CenterCrop, Resize)","1a6e7eaf":"# Increase GPU memory as per the need.\ngpus = tf.config.list_physical_devices('GPU')\nif gpus:\n  try:\n    # Currently, memory growth needs to be the same across GPUs\n    for gpu in gpus:\n      tf.config.experimental.set_memory_growth(gpu, True)\n    logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n    print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n  except RuntimeError as e:\n    # Memory growth must be set before GPUs have been initialized\n    print(e)","42e6b46e":"Train_path='..\/input\/siim-covid19-resized-to-256px-jpg\/train\/'  # for 256 img_size\n\n#Train_path='..\/input\/siim-covid19-resized-to-512px-png\/train\/'   # for 512 img_size\n","4eff8e6f":"# read df_train csv file\n\ndf=pd.read_csv('..\/input\/df-train\/df_train.csv')","3cd9e825":"# read study_df from original dataset\n\ntrain_study_df=pd.read_csv(\"..\/input\/siim-covid19-detection\/train_study_level.csv\")","2880d769":"train_study_df.head()","f25d7bb3":"# Using study_id in train_df beacuse original files don't have the study_id and del all non-useable  columns \n# for train this model","5c6edd23":"train_study_df= train_study_df.drop(['Negative for Pneumonia','Typical Appearance','Indeterminate Appearance','Atypical Appearance'],axis=1)","0aeb9812":"train_study_df['StudyInstanceUID']=train_study_df['id'].apply(lambda x: x.replace('_study',''))","985a97dc":"\ntrain_study_df=train_study_df.rename(columns={'id':'study_id'})\ntrain_study_df.head()","d7be2aeb":"df=df.merge(train_study_df,on='StudyInstanceUID')\n","ec742af2":"df.head()","c47a4c47":"df['path']=df.apply(lambda row:Train_path + row.id +'.jpg',axis=1)  # for 256 img_size\n\n#df['path']=df.apply(lambda row:Train_path + row.id +'.png',axis=1)   # for 512 img_size","ff71e62a":"df['path'][0]","6ed7cb2b":"print('000a312787f2.jpg' in os.listdir(Train_path))","6a0f76f2":"labels = df[['Negative for Pneumonia','Typical Appearance','Indeterminate Appearance','Atypical Appearance']].values\nlabels = np.argmax(labels, axis=1)\ndf['study_level'] = labels","bfa64c85":"df=df.drop(['StudyInstanceUID_count'],axis=1)","4af1e4af":"df.columns","e224ed1f":"TRAIN_PATH = '..\/input\/siim-covid19-resized-to-256px-jpg\/train\/'\n\nTEST_PATH = '..\/input\/siim-covid19-resized-to-256px-jpg\/test\/'\n\n","ea2a478c":"# for 512 images\n\n#TRAIN_PATH='..\/input\/siim-covid19-resized-to-512px-png\/train\/'\n#TEST_PATH ='..\/input\/siim-covid19-resized-to-512px-png\/test\/'","5aa5b36c":"train_df=df.copy()","69837a41":"study_df=train_df[['study_id',\n                  'Negative for Pneumonia','Typical Appearance',\n                  'Indeterminate Appearance','Atypical Appearance']]\n\n\n\n","83ce3c8d":"study_df.head()","dec8a294":"train_df= train_df.drop(['Unnamed: 0','boxes','label','index','0','Path', 'w', 'h', 'class','x_max', 'x_min', 'y_min',\n               'label_int','OpacityCount', 'y_max','study_id','Negative for Pneumonia',\n               'Typical Appearance','Indeterminate Appearance','Atypical Appearance'],axis=1)","71f38cd4":"train_df.head()","d551f99d":"AUTO = tf.data.experimental.AUTOTUNE","759e3887":"NUM_CLASSES = 4\nHEIGHT,WIDTH = 256,256\n#HEIGHT,WIDTH = 512,512\n\nCHANNELS = 3\nBATCH_SIZE = 8\nSEED = 143","4f3cf850":"def process_img(filepath,label):\n    image = tf.io.read_file(filepath)\n    image = tf.image.decode_jpeg(image, channels=CHANNELS)\n    image = tf.image.convert_image_dtype(image, tf.float32) \n    #image = tf.image.resize(image, [HEIGHT,WIDTH])\n    return image,label\n\n\ndef data_augment(image, label):\n    p_spatial = tf.random.uniform([], 0, 1.0, dtype=tf.float32)\n    p_rotate = tf.random.uniform([], 0, 1.0, dtype=tf.float32)\n    p_pixel_1 = tf.random.uniform([], 0, 1.0, dtype=tf.float32)\n    p_pixel_2 = tf.random.uniform([], 0, 1.0, dtype=tf.float32)\n    p_pixel_3 = tf.random.uniform([], 0, 1.0, dtype=tf.float32)\n    p_crop = tf.random.uniform([], 0, 1.0, dtype=tf.float32)\n            \n    # Flips\n    image = tf.image.random_flip_left_right(image)\n    image = tf.image.random_flip_up_down(image)\n    if p_spatial > .75:\n        image = tf.image.transpose(image)\n        \n    # Rotates\n    if p_rotate > .75:\n        image = tf.image.rot90(image, k=3) \n    elif p_rotate > .5:\n        image = tf.image.rot90(image, k=2) \n    elif p_rotate > .25:\n        image = tf.image.rot90(image, k=1) \n        \n    \n    if p_pixel_1 >= .4:\n        image = tf.image.random_saturation(image, lower=.7, upper=1.3)\n    if p_pixel_2 >= .4:\n        image = tf.image.random_contrast(image, lower=.8, upper=1.2)\n    if p_pixel_3 >= .4:\n        image = tf.image.random_brightness(image, max_delta=.1)\n        \n    \n    if p_crop > .7:\n        if p_crop > .9:\n            image = tf.image.central_crop(image, central_fraction=.7)\n        elif p_crop > .8:\n            image = tf.image.central_crop(image, central_fraction=.8)\n        else:\n            image = tf.image.central_crop(image, central_fraction=.9)\n    elif p_crop > .4:\n        crop_size = tf.random.uniform([], int(HEIGHT*.8), HEIGHT, dtype=tf.int32)\n        image = tf.image.random_crop(image, size=[crop_size, crop_size, CHANNELS])\n    \n    image = tf.image.resize(image, [HEIGHT,WIDTH])\n    return image,label\n\ndef get_dataset(filenames,labels, training=True):\n    dataset = tf.data.Dataset.from_tensor_slices((filenames,labels))\n    dataset = dataset.map(process_img,num_parallel_calls=AUTO)\n    dataset = dataset.map(data_augment,num_parallel_calls=AUTO)\n    dataset = dataset.cache()\n    dataset = dataset.repeat()\n    if training:\n        dataset = dataset.shuffle(2048)\n    dataset = dataset.batch(BATCH_SIZE)\n    dataset = dataset.prefetch(AUTO)\n    return dataset","ee31e4e3":"import efficientnet.tfkeras as efn\n\n#def create_model():\n    \n#    pretrained = efn.EfficientNetB4(include_top=False, weights=\"imagenet\",input_shape=[HEIGHT,WIDTH, 3])\n            \n#    x = pretrained.output\n#    x = tf.keras.layers.GlobalAveragePooling2D() (x)\n#    outputs = tf.keras.layers.Dense(NUM_CLASSES,activation=\"softmax\", dtype='float32')(x)\n        \n#    model = tf.keras.Model(pretrained.input, outputs)\n#    return model\n\n#model = create_model()\n#model.summary()\n","6b545498":"def create_model():\n    \n    base_model = efn.EfficientNetB0(include_top=False, weights='imagenet')\n    base_model.trainabe = True\n\n    inputs = layers.Input((HEIGHT,WIDTH, 3))\n    x = base_model(inputs, training=True)\n    x = layers.GlobalAveragePooling2D()(x)\n    x = layers.Dropout(0.5)(x)\n    \n    outputs = layers.Dense(NUM_CLASSES, kernel_regularizer=tf.keras.regularizers.l2(0.001))(x)\n    outputs = layers.Activation('softmax', dtype='float32', name='predictions')(outputs)\n    \n    return models.Model(inputs, outputs)\n\ntf.keras.backend.clear_session()\n#model = create_model()\n#model.summary()","c6c36ef8":"import tensorflow_addons as tfa\n\ndef compile_model(model, lr=0.001):\n    \n    optimizer = tf.keras.optimizers.Adam(lr=lr)\n    \n    loss = tf.keras.losses.CategoricalCrossentropy()\n   \n    metrics = [\n       tfa.metrics.F1Score(num_classes = NUM_CLASSES,average = \"macro\", name = \"f1_score\"),\n       tf.keras.metrics.CategoricalAccuracy(name='acc')\n    ]\n\n    model.compile(optimizer=optimizer, loss=loss, metrics=metrics)\n\n    return model","ef77d77a":"METRIC = \"val_acc\"\n# METRIC=\"val_auc\"\ndef create_callbacks(kfold,metric = METRIC):\n    \n    cpk_path = f'.\/best_model_{kfold}.h5'\n    \n    checkpoint = tf.keras.callbacks.ModelCheckpoint(\n        filepath=cpk_path,\n        monitor= metric,\n        mode='max',\n        save_best_only=True,\n        verbose=1,\n    )\n\n    reducelr = tf.keras.callbacks.ReduceLROnPlateau(\n        monitor= metric,\n        mode='max',\n        factor=0.1,\n        patience=3,\n        verbose=0\n    )\n\n    earlystop = tf.keras.callbacks.EarlyStopping(\n        monitor= metric,\n        mode='max',\n        patience=10, \n        verbose=1\n    )\n    \n    callbacks = [checkpoint, reducelr, earlystop]         \n    \n    return callbacks","a42663b7":"train_df.head(2)\ntrain_df.to_csv('train_df.csv')","a28d8ea8":"study_df.head(3)\nstudy_df.to_csv('study_df.csv')","11f34997":"from tqdm import tqdm\nfiles_ls= train_df['path']\n\nfiles_df = pd.DataFrame(list(files_ls), columns = [\"filepath\"])\n\n\nlabels = np.zeros((len(files_ls),NUM_CLASSES))\ntmp_labels = np.zeros((len(files_ls)))\n\ndef get_id(filepath):\n    tmp = filepath.split(\"\/\")[-1]\n    tmp = tmp.split(\".\")[0]\n    tmp = tmp.split(\"_\")[-1]\n    return tmp\n\nfor i in tqdm(range(len(files_ls))):\n    image_id = get_id(files_ls[i])\n    label_id = train_df[train_df[\"id\"] == image_id][\"study_level\"]\n    labels[i][label_id] = 1\n    tmp_labels[i] = label_id\n    \nprint(\"Labels shape: \",labels.shape)\nprint(files_ls)","04b1c670":"EPOCHS = 30\nVERBOSE = 1\nN_SPLITS = 5\n\nkfold = StratifiedKFold(n_splits=N_SPLITS, shuffle=True, random_state=SEED)\nhistory = {}\n\n\nfor fold,(tID,vID) in enumerate(kfold.split(files_ls,tmp_labels)):\n    tFiles, tLabels = list(files_df.iloc[tID][\"filepath\"]) , labels[tID]\n    vFiles, vLabels = list(files_df.iloc[vID][\"filepath\"]) , labels[vID]\n    print(\"Number of Training Images: \",len(tID))\n    print(\"Number of Validation Images: \",len(vID))\n    \n    STEPS_PER_EPOCH  = len(tID)\/\/BATCH_SIZE\n    VALID_STEPS = len(vID)\/\/BATCH_SIZE\n    \n    tf.keras.backend.clear_session()\n    \n    train_ds = get_dataset(tFiles,tLabels, training = True)\n    val_ds = get_dataset(vFiles, vLabels, training = False)\n    \n    \n\n    model = create_model()\n    #model = chxnet\n    model = compile_model(model, lr=0.00001)\n    callbacks = create_callbacks(kfold = fold)\n\n    print(\"------------------Fold - \",fold+1,\" --------------------------\")\n    history[fold] = model.fit(\n                        train_ds,\n                        epochs=EPOCHS,\n                        callbacks=callbacks,\n                        validation_data = val_ds,\n                        verbose=VERBOSE,\n                        steps_per_epoch = STEPS_PER_EPOCH,\n                        validation_steps=VALID_STEPS\n                       )","b12696dc":"plt.figure(figsize=(8*N_SPLITS,24))\n\nfor i in range(N_SPLITS):\n    acc = history[i].history['acc']\n    val_acc = history[i].history['val_acc']\n    f1 = history[i].history['f1_score']\n    val_f1 = history[i].history['val_f1_score']\n    loss = history[i].history['loss']\n    val_loss = history[i].history['val_loss']\n    epochs_range = range(len(history[i].history['val_loss'])) \n    \n    plt.subplot(N_SPLITS, 3,i*3+1)\n    plt.plot(epochs_range, acc, label='Training Accuracy')\n    plt.plot(epochs_range, val_acc, label='Validation  Accuracy')\n    plt.legend(loc='lower right')\n    plt.title(f'FOLD:{str(i)} Training and Validation  Accuracy')\n    \n    plt.subplot(N_SPLITS, 3,i*3+2)\n    plt.plot(epochs_range, f1, label='Training F1 score')\n    plt.plot(epochs_range, val_f1, label='Validation  F1 score')\n    plt.legend(loc='lower right')\n    plt.title(f'FOLD:{str(i)} Training and Validation  F1 score')\n    \n    plt.subplot(N_SPLITS, 3, i*3+3)\n    plt.plot(epochs_range, loss, label='Training Loss')\n    plt.plot(epochs_range, val_loss, label='Validation Loss')\n    plt.legend(loc='upper right')\n    plt.title(f'FOLD:{str(i)} Training and Validation Loss')\n\nplt.show()","7c64807a":"\nBATCH_SIZE = 16","d1d07f71":"# Hyperparameter","ec0b8890":" Train model for Study Level ","51460c5f":"# GPU Run Access","07f050fd":"EPOCHS = 30\nVERBOSE = 1\nN_SPLITS = 5\n\nkfold = StratifiedKFold(n_splits=N_SPLITS, shuffle=True, random_state=SEED)\nhistory = {}\n\n\n\n        \n\nfor fold,(tID,vID) in enumerate(kfold.split(files_ls,tmp_labels)):\n    tFiles, tLabels = list(files_df.iloc[tID][\"filepath\"]) , labels[tID]\n    vFiles, vLabels = list(files_df.iloc[vID][\"filepath\"]) , labels[vID]\n    print(\"Number of Training Images: \",len(tID))\n    print(\"Number of Validation Images: \",len(vID))\n    \n    STEPS_PER_EPOCH  = len(tID)\/\/BATCH_SIZE\n    VALID_STEPS = len(vID)\/\/BATCH_SIZE\n    \n    tf.keras.backend.clear_session()\n    \n    train_ds = get_dataset(tFiles,tLabels, training = True)\n    val_ds = get_dataset(vFiles, vLabels, training = False)\n    \n        \n    model = tf.keras.Sequential([\n        efn.EfficientNetB5(\n            input_shape=(HEIGHT, WIDTH, 3),\n            weights='imagenet',\n            include_top=False),\n        tf.keras.layers.GlobalAveragePooling2D(),\n        tf.keras.layers.Dense(NUM_CLASSES, activation='softmax')\n    ])\n    model.compile(\n        optimizer=tf.keras.optimizers.Adam(),\n        loss='categorical_crossentropy',\n        metrics=[tf.keras.metrics.AUC(multi_label=True)])\n    callbacks = create_callbacks(kfold = fold)\n    print(\"------------------Fold - \",fold+1,\" --------------------------\")\n    history[fold] = model.fit(\n                        train_ds,\n                        epochs=EPOCHS,\n                        callbacks=callbacks,\n                        validation_data = val_ds,\n                        verbose=VERBOSE,\n                        steps_per_epoch = STEPS_PER_EPOCH,\n                        validation_steps=VALID_STEPS\n                       )"}}