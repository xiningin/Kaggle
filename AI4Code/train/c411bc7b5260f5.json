{"cell_type":{"95a406eb":"code","3d9d495e":"code","70263459":"code","c33957db":"code","5470c098":"code","e94396d9":"code","cdc0c9e9":"code","0029b21e":"code","e48b52bb":"code","b1ba2640":"code","63ee9e60":"code","ae3333f4":"code","435d59ff":"code","a522aff0":"code","85e855a5":"code","1bf9f1a0":"code","11472549":"code","e122a8c0":"code","8cffbd54":"code","59ceca8c":"code","f329e538":"code","523e447b":"code","1e417488":"code","61e35696":"code","ceb8c789":"code","f67d0302":"code","7c01d9d3":"code","db1bbfbc":"code","be539f2c":"code","0be7b4fd":"code","621bc2e5":"code","4720bc3a":"code","2dd9bf8a":"code","5e898683":"code","0d553ce2":"markdown","4a2c747e":"markdown","7f955f22":"markdown","9324dd42":"markdown","438619ae":"markdown","b7d1b493":"markdown","2906554b":"markdown","5107d83d":"markdown","e6a966cb":"markdown","31ba8f97":"markdown","c85258a9":"markdown","4d99a6e1":"markdown","afcb9bb7":"markdown"},"source":{"95a406eb":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","3d9d495e":"import seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.model_selection import train_test_split\nfrom sklearn import metrics\nfrom sklearn.metrics import accuracy_score,f1_score,recall_score,precision_score, confusion_matrix\nsns.set(color_codes=True)\nfrom IPython.display import display\nfrom sklearn.feature_extraction.text import CountVectorizer\nimport missingno as msno_plot","70263459":"df = pd.read_csv('\/kaggle\/input\/music-genre-classification\/dataset.csv')","c33957db":"df.head()","5470c098":"df.columns","e94396d9":"df.info()","cdc0c9e9":"sns.countplot(df['label'])\n#equally distributed classes","0029b21e":"df.label.nunique()","e48b52bb":"df.isnull().sum()","b1ba2640":"#-------------------------------------------Barplot of non-missing values--------------------------------\nplt.title('#Non-missing Values by Columns')\nmsno_plot.bar(df);","63ee9e60":"df1 = df.copy()\ndf1.drop('filename',axis=1,inplace=True)","ae3333f4":"col_names = df1.columns[:-1]","435d59ff":"display(col_names)","a522aff0":"for i in col_names:\n    q1, q2, q3 = df1[i].quantile([0.25,0.5,0.75])\n    IQR = q3 - q1\n    lower_cap=q1-1.5*IQR\n    upper_cap=q3+1.5*IQR\n    df1[i]=df1[i].apply(lambda x: upper_cap if x>(upper_cap) else (lower_cap if x<(lower_cap) else x))","85e855a5":"plt.figure(figsize=(20,10))\nsns.heatmap(df1.corr(),\n            annot=True,\n            linewidths=.5,\n            center=0,\n            cbar=False,\n            cmap=\"YlGnBu\")\nplt.show()","1bf9f1a0":"plt.figure(figsize=(10,8))\nsns.countplot(df1['label']);","11472549":"from sklearn import preprocessing\nle = preprocessing.LabelEncoder()\nle.fit(df1['label'])","e122a8c0":"le.classes_","8cffbd54":"df1['label'] = le.transform(df1['label'])","59ceca8c":"df1['label'].unique()","f329e538":"df1['label'].value_counts(normalize=True)","523e447b":"# splitting data into training and test set for independent attributes\n\nX_train, X_test, y_train, y_test =train_test_split(df1.drop('label',axis=1), df1['label'], test_size=.3,random_state=22)","1e417488":"X_train.shape,y_train.shape","61e35696":"X_test.shape,y_test.shape","ceb8c789":"clf_pruned = DecisionTreeClassifier(criterion = \"gini\", random_state = 100,\n                               max_depth=3, min_samples_leaf=5)\nclf_pruned.fit(X_train, y_train)","f67d0302":"from sklearn.tree import export_graphviz\nfrom six import StringIO  \nfrom IPython.display import Image  \nimport pydotplus\nimport graphviz","7c01d9d3":"xvar = df1.drop('label', axis=1)\nfeature_cols = xvar.columns\n\ndot_data = StringIO()\nexport_graphviz(clf_pruned, out_file=dot_data,  \n                filled=True, rounded=True,\n                special_characters=True,feature_names = feature_cols,class_names=['0','1','2','3','4','5','6','7','8','9'])\nfrom pydot import graph_from_dot_data\n(graph, ) = graph_from_dot_data(dot_data.getvalue())\nImage(graph.create_png())","db1bbfbc":"preds_pruned = clf_pruned.predict(X_test)\npreds_pruned_train = clf_pruned.predict(X_train)\nprint(accuracy_score(y_test,preds_pruned))\nprint(accuracy_score(y_train,preds_pruned_train))","be539f2c":"## Calculating feature importance\nfeat_importance = clf_pruned.tree_.compute_feature_importances(normalize=False)\nfeat_imp_dict = dict(zip(feature_cols, clf_pruned.feature_importances_))\nfeat_imp = pd.DataFrame.from_dict(feat_imp_dict, orient='index')\nfeat_imp.rename(columns = {0:'FeatureImportance'}, inplace = True)\nfeat_imp.sort_values(by=['FeatureImportance'], ascending=False).head()","0be7b4fd":"path = clf_pruned.cost_complexity_pruning_path(X_train, y_train)\nccp_alphas, impurities = path.ccp_alphas, path.impurities\n\nfig, ax = plt.subplots(figsize=(16,8));\nax.plot(ccp_alphas[:-1], impurities[:-1], marker='o', drawstyle=\"steps-post\");\nax.set_xlabel(\"effective alpha\");\nax.set_ylabel(\"total impurity of leaves\");\nax.set_title(\"Total Impurity vs effective alpha for training set\");","621bc2e5":"clfs = []\nfor ccp_alpha in ccp_alphas:\n    clf = DecisionTreeClassifier(random_state=0, ccp_alpha=ccp_alpha)\n    clf.fit(X_train, y_train)\n    clfs.append(clf)\nprint(\"Number of nodes in the last tree is: {} with ccp_alpha: {}\".format(\n      clfs[-1].tree_.node_count, ccp_alphas[-1]))","4720bc3a":"clfs = clfs[:-1]\nccp_alphas = ccp_alphas[:-1]\n\nnode_counts = [clf.tree_.node_count for clf in clfs]\ndepth = [clf.tree_.max_depth for clf in clfs]\nfig, ax = plt.subplots(2, 1)\nax[0].plot(ccp_alphas, node_counts, marker='o', drawstyle=\"steps-post\")\nax[0].set_xlabel(\"alpha\")\nax[0].set_ylabel(\"number of nodes\")\nax[0].set_title(\"Number of nodes vs alpha\")\nax[1].plot(ccp_alphas, depth, marker='o', drawstyle=\"steps-post\")\nax[1].set_xlabel(\"alpha\")\nax[1].set_ylabel(\"depth of tree\")\nax[1].set_title(\"Depth vs alpha\")\nfig.tight_layout()","2dd9bf8a":"fig, ax = plt.subplots(figsize=(16,8)); #-----------------Setting size of the canvas\ntrain_scores = [clf.score(X_train, y_train) for clf in clfs]\ntest_scores = [clf.score(X_test, y_test) for clf in clfs]\nax.set_xlabel(\"alpha\")\nax.set_ylabel(\"accuracy\")\nax.set_title(\"Accuracy vs alpha for training and testing sets\")\nax.plot(ccp_alphas, train_scores, marker='o', label=\"train\",\n        drawstyle=\"steps-post\")\nax.plot(ccp_alphas, test_scores, marker='o', label=\"test\",\n        drawstyle=\"steps-post\")\nax.legend()\nplt.show()","5e898683":"i = np.arange(len(ccp_alphas))\nccp = pd.DataFrame({'Depth': pd.Series(depth,index=i),'Node' : pd.Series(node_counts, index=i),\\\n                    'ccp' : pd.Series(ccp_alphas, index = i),'train_scores' : pd.Series(train_scores, index = i),\n                   'test_scores' : pd.Series(test_scores, index = i)})\nccp.tail()\nccp[ccp['test_scores']==ccp['test_scores'].max()]\n#The above code provides the cost computation pruning value that produces the highest accuracy in the test data.","0d553ce2":"we remove the last element in clfs and ccp_alphas, because it is the trivial tree with only one node. Here we show that the number of nodes and tree depth decreases as alpha increases.","4a2c747e":"Feature importance refers to a class of techniques for assigning scores to input features of a predictive model that indicates the relative importance of each feature when making a prediction.","7f955f22":"Cost complexity pruning provides another option to control the size of a tree. In DecisionTreeClassifier, this pruning technique is parameterized by the cost complexity parameter, ccp_alpha. Greater values of ccp_alpha increase the number of nodes pruned \n\nIn simpler terms, cost complexity is a threshold value. The model split a node further into its child node only when the overall impurity of the model is improved by a value greater than this threshold else it stops.\n\nWhen CCP values are low, a higher number of nodes are created. Higher the nodes, the higher is the depth of the tree as well.","9324dd42":"### Cost Complexity Pruning (CCP)","438619ae":"100% accuracy with train data and 43% accuracy with test data","b7d1b493":"**The following parameters can be tuned to improve the model output**\n1. criterion \u2014 Gini impurity is used to decide the variables based on which root node and following decision nodes should be split\n2. class_weight \u2014 None; All classes are assigned weight 1\n3. max_depth \u2014 3; Pruning is done. When \u201cNone\u201d, it signifies that nodes will be expanded till all leaves are homogeneous\n4. max_features \u2014 None; All features or independent variables are considered while deciding split of a node\n5. max_leaf_nodes \u2014 None;\n6. min_impurity_decrease \u2014 0.0; A node is split only when the split ensures a decrease in the impurity of greater than or equal to zero\n7. min_impurity_split \u2014 None;\n8. min_samples_leaf \u2014 1; Minimum number of samples required for a leaf to exists\n9. min_samples_split \u2014 2; If min_samples_leaf =1, it signifies that the right and the left node should have 1 sample each, i.e. the parent node or the root node should have at least two samples\n10. splitter \u2014 \u2018best\u2019; Strategy used to choose the split at each node. Best ensure that all features are considered while deciding the split","2906554b":"Classification problems are sensitive to class imbalance. A class imbalance is a scenario when the dependent attribute has a higher proportion of ones than zeros or vice versa. In a multiclass problem, class imbalance occurs when the proportion of one of the class values is much higher","5107d83d":"Outliers above are winsorized using Q1\u20131.5*IQR and Q3+1.5*IQR values. Q1, Q3, and IQR stand for Quartile 1, Quartile 3, and Inter Quartile Range respectively.","e6a966cb":"In Decision Trees, we need not remove highly correlated variables as nodes are divided into sub-nodes using one independent variable only, hence even if two or more variables are highly correlated, the variable producing the highest information gain will be used for the analysis.","31ba8f97":"### Decision Tree Classifiers\nDecision trees consist of nodes and branches. The nodes can further be classified into a root node (starting node of the tree), decision nodes (sub-nodes that splits based on conditions), and leaf nodes (nodes that don\u2019t branch out further). Since the decision tree follows an if-else structure, every node uses one and only one independent variable to split into two or more branches. The independent variable can be categorical or continuous. For categorical variables, the categories are used to decide the split of the node, for continuous variables the algorithm comes up with multiple threshold values that act as the decision-maker","c85258a9":"**Decision Tree usually consists of:**\n\n**Root Node** \u2014 Represents the sample or the population that gets divided into further homogeneous groups\n**Splitting** \u2014 Process of dividing the nodes into two sub-nodes\n**Decision Node** \u2014 When a sub-node splits into further sub-nodes based on a certain condition, it is called a decision node\n**Leaf or Terminal Node** \u2014 Sub nodes that don\u2019t split further\n**Information Gain** \u2014 To split the nodes using a condition (say most informative feature) we need to define an objective function that can be optimized. In the decision tree algorithm, we tend to maximize the information gain at each split. Three impurity measures are used commonly in measuring the information gain. They are the Gini impurity, Entropy, and the Classification error","4d99a6e1":"#### Outlier check & Treatment","afcb9bb7":"A decision tree model is developed using the Gini criterion. Note that for the sake of simplicity we have pruned the tree to a maximum depth of three\nThe above criterions can be generated using gridSearchCV"}}