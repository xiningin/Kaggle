{"cell_type":{"600d97f7":"code","0d28c8c1":"code","c240c8b0":"code","c2b0ec07":"code","1a2df6ec":"code","bad3c075":"code","f8f2d930":"code","ee107471":"code","1c0c2cf5":"code","9eb49c2d":"code","4d473347":"code","8e349364":"code","a7455040":"code","275d6786":"code","fe65c392":"code","dea42b98":"code","1235ba60":"code","f51904b7":"code","3c899908":"code","977198e1":"code","5d3e3c1a":"code","6724bdab":"code","d31726ca":"code","332878f6":"code","7bd88656":"code","91387b1f":"code","c2f4af43":"code","4f9494a9":"code","42a86d97":"markdown","8db2934e":"markdown","828ed96d":"markdown","f8c941d5":"markdown","836ccc39":"markdown","7b439bc7":"markdown","02176712":"markdown","bc1337ea":"markdown","8cd8fef0":"markdown","8809af6b":"markdown","8a671059":"markdown","0155304f":"markdown","eb05dd99":"markdown","acbf77a7":"markdown"},"source":{"600d97f7":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import plot_confusion_matrix\nfrom sklearn import metrics\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","0d28c8c1":"data = pd.read_csv('\/kaggle\/input\/creditcardfraud\/creditcard.csv')\ndata.head()","c240c8b0":"data.describe()","c2b0ec07":"data.columns","1a2df6ec":"data.info()","bad3c075":"data.shape","f8f2d930":"#check for missing values\ndata.isnull().sum().sum()","ee107471":"import seaborn as sns\nprint(data['Class'].value_counts())\nsns.set_style(\"darkgrid\")\nsns.countplot(data['Class']);","1c0c2cf5":"fraud_data = data[data['Class']==1]\ngenuine_data = data[data['Class']==0]","9eb49c2d":"fraud_data.Amount.describe()","4d473347":"genuine_data.Amount.describe()","8e349364":"import matplotlib.pyplot as plt\nf, (ax1, ax2) = plt.subplots(2, 1, sharex=True, figsize=(15, 10))\nf.suptitle('Amount per transaction by class')\n\nax1.hist(fraud_data.Amount, 100)\nax1.set_title('Fraud')\nax2.hist(genuine_data.Amount, 100)\nax2.set_title('Genuine')\nplt.xlabel('Amount ($)')\nplt.ylabel('Number of Transactions')\nplt.xlim((0, 20000))\nplt.yscale('log')\nplt.show();","a7455040":"#scaling the amount column using a standard scaler\nfrom sklearn.preprocessing import StandardScaler\nscaler=StandardScaler()\ndata[\"Amount\"]=scaler.fit_transform(np.array(data[\"Amount\"]).reshape(-1,1))","275d6786":"# We Will check Do fraudulent transactions occur more often during certain time frame ? Let us find out with a visual representation.\n\nf, (ax1, ax2) = plt.subplots(2, 1, sharex=True, figsize=(15, 10))\nf.suptitle('Time of transaction vs Amount by class')\nax1.scatter(fraud_data.Time, fraud_data.Amount)\nax1.set_title('Fraud')\nax2.scatter(genuine_data.Time, genuine_data.Amount)\nax2.set_title('Normal')\nplt.xlabel('Time (in Seconds)')\nplt.ylabel('Amount')\nplt.show()","fe65c392":"#splitting data for training and testing\nimport sklearn\nfrom sklearn.model_selection import train_test_split\nx = data.drop(\"Class\", axis=1)\ny = data['Class']\nx_train,x_test,y_train,y_test = train_test_split(x,y,test_size=0.2, random_state = 100)","dea42b98":"from sklearn.svm import LinearSVC\nclf=LinearSVC()\nclf.fit(x_train,y_train)\ny_pred=clf.predict(x_test)","1235ba60":"from sklearn.metrics import confusion_matrix\nimport mlxtend \n\nprint(\"Accuracy:\",metrics.accuracy_score(y_test, y_pred))\nprint(\"F1 score:\",metrics.f1_score(y_test, y_pred))\nprint(\"Recall:\",metrics.recall_score(y_test, y_pred))\nprint(\"Precision:\",metrics.precision_score(y_test, y_pred))\n","f51904b7":"\nimport sklearn\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.svm import LinearSVC\nfrom sklearn.metrics import plot_confusion_matrix\nfrom sklearn import metrics\n\n#splitting data for training and testing\nx = data.drop(\"Class\", axis=1)\ny = data['Class']\nx_train,x_test,y_train,y_test = train_test_split(x,y,test_size=0.2, random_state = 100)\n\n#Classifying data using LinearSVC\nclf=LinearSVC()\nclf.fit(x_train,y_train)\ny_pred=clf.predict(x_test)\n\n# Performance Evaluation\nprint(\"Accuracy:\",metrics.accuracy_score(y_test, y_pred))\n","3c899908":"#SMOTE\nfrom collections import Counter\nfrom imblearn.over_sampling import SMOTE\n\nx_resampled, y_resampled = SMOTE().fit_sample(x_train, y_train)\nprint(sorted(Counter(y_resampled).items()))","977198e1":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import confusion_matrix\nfrom mlxtend.plotting import plot_confusion_matrix\nclf=LogisticRegression(solver='saga')\nclf.fit(x_resampled,y_resampled)\ny_pred=clf.predict(x_test)","5d3e3c1a":"# Model Accuracy, how often is the classifier correct?\nprint(\"Accuracy:\",metrics.accuracy_score(y_test, y_pred))\nprint(\"F1 Score:\",metrics.f1_score(y_test, y_pred))\nprint(\"Recall:\",metrics.recall_score(y_test, y_pred))\nprint(\"Precision:\",metrics.precision_score(y_test, y_pred))\n#plot_confusion_matrix(clf, x_test, y_test) \nfig, ax = plot_confusion_matrix(conf_mat=confusion_matrix(y_test, y_pred),\n                                show_absolute=True,\n                                show_normed=True,\n                                colorbar=True)\nplt.show()","6724bdab":"from sklearn.svm import LinearSVC\nclf=LinearSVC(max_iter = 190) \nclf.fit(x_resampled,y_resampled)\ny_pred=clf.predict(x_test)","d31726ca":"# Model Accuracy, how often is the classifier correct?\nprint(\"Accuracy:\",metrics.accuracy_score(y_test, y_pred))\nprint(\"F1 Score:\",metrics.f1_score(y_test, y_pred))\nprint(\"Recall:\",metrics.recall_score(y_test, y_pred))\nprint(\"Precision:\",metrics.precision_score(y_test, y_pred))\n#plot_confusion_matrix(clf, x_test, y_test) \nfig, ax = plot_confusion_matrix(conf_mat=confusion_matrix(y_test, y_pred),\n                                show_absolute=True,\n                                show_normed=True,\n                                colorbar=True)\nplt.show()","332878f6":"clf=RandomForestClassifier()\n\n#Train the model using the training sets y_pred=clf.predict(X_test)\nclf.fit(x_resampled,y_resampled)\ny_pred=clf.predict(x_test)","7bd88656":"# Model Accuracy, how often is the classifier correct?\nprint(\"Accuracy:\",metrics.accuracy_score(y_test, y_pred))\nprint(\"F1 Score:\",metrics.f1_score(y_test, y_pred))\nprint(\"Recall:\",metrics.recall_score(y_test, y_pred))\nprint(\"Precision:\",metrics.precision_score(y_test, y_pred))\nfig, ax = plot_confusion_matrix(conf_mat=confusion_matrix(y_test, y_pred),\n                                show_absolute=True,\n                                show_normed=True,\n                                colorbar=True)\nplt.show()","91387b1f":"# Separate input features and target\nX = data.drop('Class', axis=1)\ny = data['Class']\n\n# Split dataset into training set and test set\nx_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2) # 80% training and 20% test","c2f4af43":"clf=RandomForestClassifier()\n\n#Train the model using the training sets y_pred=clf.predict(X_test)\nclf.fit(x_train,y_train)\ny_pred=clf.predict(x_test)","4f9494a9":"# Model Accuracy, how often is the classifier correct?\nprint(\"Accuracy:\",metrics.accuracy_score(y_test, y_pred))\nprint(\"F1 Score:\",metrics.f1_score(y_test, y_pred))\nprint(\"Recall:\",metrics.recall_score(y_test, y_pred))\nprint(\"Precision:\",metrics.precision_score(y_test, y_pred))\nfig, ax = plot_confusion_matrix(conf_mat=confusion_matrix(y_test, y_pred),\n                                show_absolute=True,\n                                show_normed=True,\n                                colorbar=True)\nplt.show()","42a86d97":" There are no missing values in the dataset.","8db2934e":"Random forest with SMOTE works really well. ","828ed96d":"There are 31 columns in the dataset. Our task is to predict the column 'Class'.","f8c941d5":"**We should split the data before sampling**","836ccc39":"# Trying Random Forest with Sampling","7b439bc7":"This shows that our data is imbalanced. There are only few fraudulent transactions. So, accuracy is not the best option to compare the performace of the model. Other performance metrics are confusion matrix, precision, recall and F1 score. \n\nPrecision =  Number of true positives\/ number of true positives and false positives. A low value of precision means a large number of false positives.\n\nRecall =  Number of true positives\/ number of true positives and false negatives. A low value of recall indicates a large number of false negatives.\n\nF1 score = 2*((precision x recall)\/(precision+recall)). It is a measure of the balance between precision and recall.","02176712":"If you are an absolute beginner, please check this [article](http:\/\/medium.com\/analytics-vidhya\/credit-card-fraud-detection-a-case-study-for-handling-class-imbalance-f81abf997421).\n","bc1337ea":"# Trying Logistic Regression With SMOTE","8cd8fef0":"The amount column in the data is skewed.","8809af6b":"References\n* https:\/\/www.geeksforgeeks.org\/confusion-matrix-machine-learning\/\n* https:\/\/towardsdatascience.com\/methods-for-dealing-with-imbalanced-data-5b761be45a18\n* https:\/\/www.datacamp.com\/community\/tutorials\/random-forests-classifier-python","8a671059":"Decision trees generally perform well on imbalanced data. Decion trees does not support missing values. Our dataset has no missing values.\n\nLet us use Random Forest Classifier. A forest is comprised of trees. It is said that the more trees it has, the more robust a forest is. Random forests creates decision trees on randomly selected data samples, gets prediction from each tree and selects the best solution by means of voting. It also provides a pretty good indicator of the feature importance.","0155304f":"We couldn't figure out any specific insights about the timing of fraudulent transaction. We have information regarding only 2 days. So, we can't make any conclusions regarding the time at which most fraudulent activiies occur.","eb05dd99":"We detected almost all the genuine cases. But the model performed too bad in detecting the fraudulent cases. Only very few fraudulent activities were detected. ","acbf77a7":"# Trying Random Forest without sampling"}}