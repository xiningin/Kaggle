{"cell_type":{"2f01da55":"code","6e0b3258":"code","60738190":"code","c7624e09":"code","3b5f3ef1":"code","28cb8701":"code","a981bd94":"code","b9cdbde1":"code","398969ae":"code","966ceb65":"code","b42997a2":"code","39d21dec":"code","ade1e2bc":"code","da082ed1":"code","0394d543":"code","0bb2c8a8":"code","bd78c62f":"code","21bfc5c2":"code","925eeb13":"code","1e252da8":"code","5c695dd7":"code","c1b36fc3":"code","729f3abb":"code","9384d26e":"code","cd57a0b4":"code","340cb8e1":"code","ea2a03b7":"code","8467e1cd":"code","1775932b":"code","f82c3613":"code","94356c53":"code","2e7284ff":"code","baae418a":"code","d4449422":"code","60e57e3b":"code","3fdc529b":"code","e5826daa":"code","71ce10dd":"code","ebc15439":"code","25f7f97b":"code","da8142f1":"code","46b9614e":"code","4648edc7":"code","1d84ed20":"code","b71f5c0f":"code","6cfee389":"code","655bd4eb":"code","617a92a5":"code","288e5872":"code","a540633e":"code","8a90b0f3":"code","4e1f1d7e":"code","9be88ce0":"code","a350460f":"code","0462b6e8":"code","800fe3cb":"code","eb453f4e":"code","baf9cc4a":"code","4d30d35f":"code","d0e55683":"code","5a64d9bd":"code","ca337d77":"code","657b4a20":"code","a2e7719a":"code","d14f0663":"code","7d810ae6":"code","e8e2a875":"code","e70f47ed":"code","a9aaf30f":"code","e604a64e":"code","1b38d67d":"code","ed18187c":"code","9d1405cd":"code","64c0b043":"code","316b46ff":"code","83761116":"code","0a10c0fb":"code","40753612":"code","1b61354f":"code","59417e2a":"code","6a7314c9":"code","9b5c7147":"code","708e9271":"code","3c7d7af6":"code","88cc9557":"markdown","bf2d048f":"markdown","eefe9f73":"markdown","3256d289":"markdown","998d98cb":"markdown","a688dfa0":"markdown","f5da0140":"markdown","5e57824d":"markdown","fe7040e9":"markdown","a887fd30":"markdown","9dae1dea":"markdown","48487243":"markdown","97a6a9f9":"markdown","50f4311a":"markdown","6322d8e7":"markdown","02b3ec68":"markdown","19d4e431":"markdown","029c9f57":"markdown","40b4991f":"markdown","5ec05ade":"markdown","fcb7830e":"markdown","d4baf9f9":"markdown","0ec5af68":"markdown","2944c077":"markdown","671f079e":"markdown","a73c66ef":"markdown","e8afba23":"markdown","366659b7":"markdown","6d0ee11b":"markdown","290512ff":"markdown","6c7dda78":"markdown","11f1bbe5":"markdown","0e98b859":"markdown","31893b9f":"markdown","31e8004b":"markdown","3c5387ef":"markdown","14e78825":"markdown","f1a20811":"markdown","74d80949":"markdown","cd9d4869":"markdown","b97714cd":"markdown","d62ebe93":"markdown","e78a2e0f":"markdown","23ccd2e6":"markdown","cab4a1a8":"markdown","c36d9470":"markdown","8250fc18":"markdown","d34bfdf6":"markdown","a99f7e0b":"markdown","ae3af179":"markdown","d3764924":"markdown","445dcc22":"markdown","92bfe6c8":"markdown","320e1082":"markdown","3a15fa3f":"markdown","68e350ad":"markdown","511d77a2":"markdown","2c8fbcc7":"markdown","a3fc3d36":"markdown","548f38d2":"markdown","426a2f79":"markdown","9ec77371":"markdown","2a2318d7":"markdown","16949bf6":"markdown","c2293494":"markdown","8759d703":"markdown","5f6873e2":"markdown","36e7d051":"markdown","e0c1fbff":"markdown","336d68f0":"markdown","d5b3e66f":"markdown","a1c97818":"markdown","2a54e504":"markdown","64c7d50d":"markdown","7ae9f6dd":"markdown","57ef527a":"markdown","73dfb4f3":"markdown","5398bce4":"markdown","f5dc9aa0":"markdown","bc10e86e":"markdown","a5b78ecc":"markdown","ad082077":"markdown","776800eb":"markdown","3bb06463":"markdown","720f0beb":"markdown","cd7bd145":"markdown","0997ba3d":"markdown","be6ccb8f":"markdown","6644f7cd":"markdown","a365fa68":"markdown","ead0cb35":"markdown","990bafad":"markdown","eddf6f5a":"markdown"},"source":{"2f01da55":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport warnings\ndef ignore_warn(*args, **kwargs):\n    pass\nwarnings.warn = ignore_warn #ignore annoying warning (from sklearn and seaborn)\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","6e0b3258":"!pip install vecstack","60738190":"train = pd.read_csv('..\/input\/train.csv')\ntest = pd.read_csv('..\/input\/test.csv')\n\n#Save the 'Id' column\ntrain_ID = train['Id']\ntest_ID = test['Id']\n\n#Now drop the  'Id' colum since it's unnecessary for  the prediction process.\ntrain.drop(\"Id\", axis = 1, inplace = True)\ntest.drop(\"Id\", axis = 1, inplace = True)","c7624e09":"desc = open('..\/input\/data_description.txt', \"r\") \nprint(desc.read())","3b5f3ef1":"plt.figure(figsize=(10,8))\n\n#saleprice correlation matrix\ncols = train.corr().nlargest(10, 'SalePrice')['SalePrice'].index\ncorr_mat = np.corrcoef(train[cols].values.T)\nsns.set(font_scale=1.25)\n\n#plot corr matrix\nsns.heatmap(corr_mat, annot=True, fmt='.2f', annot_kws={'size': 10}, yticklabels=cols.values, xticklabels=cols.values, center=0.25)\nplt.show()","28cb8701":"fig, ax = plt.subplots()\nax.scatter(x = train['OverallQual'], y = train['SalePrice'])\nplt.ylabel('SalePrice', fontsize=13)\nplt.xlabel('OverallQual', fontsize=13)\nplt.title('OverallQual vs SalePrice')\nplt.show()","a981bd94":"fig, ax = plt.subplots()\nax.scatter(x = train['GrLivArea'], y = train['SalePrice'])\nplt.ylabel('SalePrice', fontsize=13)\nplt.xlabel('GrLivArea', fontsize=13)\nplt.title('GrLivArea vs SalePrice')\nplt.show()","b9cdbde1":"#Deleting outliers\ntrain = train.drop(train[(train['GrLivArea']>4000) & (train['SalePrice']<300000)].index)\n\n#Check the graphic again\nfig, ax = plt.subplots()\nax.scatter(train['GrLivArea'], train['SalePrice'])\nplt.ylabel('SalePrice', fontsize=13)\nplt.xlabel('GrLivArea', fontsize=13)\nplt.title('GrLivArea vs SalePrice')\nplt.show()","398969ae":"fig, ax = plt.subplots()\nax.scatter(x = train['GarageCars'], y = train['SalePrice'])\nplt.ylabel('SalePrice', fontsize=13)\nplt.xlabel('GarageCars', fontsize=13)\nplt.title('GarageCars vs SalePrice')\nplt.show()","966ceb65":"fig, ax = plt.subplots()\nax.scatter(x = train['GarageArea'], y = train['SalePrice'])\nplt.ylabel('SalePrice', fontsize=13)\nplt.xlabel('GarageArea', fontsize=13)\nplt.title('GarageArea vs SalePrice')\nplt.show()","b42997a2":"fig, ax = plt.subplots()\nax.scatter(x = train['TotalBsmtSF'], y = train['SalePrice'])\nplt.ylabel('SalePrice', fontsize=13)\nplt.xlabel('TotalBsmtSF', fontsize=13)\nplt.title('TotalBsmtSF vs SalePrice')\nplt.show()","39d21dec":"fig, ax = plt.subplots()\nax.scatter(x = train['1stFlrSF'], y = train['SalePrice'])\nplt.ylabel('SalePrice', fontsize=13)\nplt.xlabel('1stFlrSF', fontsize=13)\nplt.title('1stFlrSF vs SalePrice')\nplt.show()","ade1e2bc":"from scipy import stats\nfrom scipy.stats import norm, skew #for some statistics","da082ed1":"sns.distplot(train['SalePrice'] , fit=norm);\n\n# Get the fitted parameters used by the function\n(mu, sigma) = norm.fit(train['SalePrice'])\n\n#Now plot the distribution\nplt.legend(['Normal dist. ($\\mu=$ {:.2f} and $\\sigma=$ {:.2f} )'.format(mu, sigma)],\n            loc='best')\nplt.ylabel('Frequency')\nplt.title('SalePrice distribution')","0394d543":"#We use the numpy fuction log1p which  applies log(1+x) to all elements of the column\ntrain[\"SalePrice\"] = np.log1p(train[\"SalePrice\"])\n\n#Check the new distribution \nsns.distplot(train['SalePrice'] , fit=norm);\n\n#Now plot the distribution\nplt.legend(['Normal dist. ($\\mu=$ {:.2f} and $\\sigma=$ {:.2f} )'.format(mu, sigma)],\n            loc='best')\nplt.ylabel('Frequency')\nplt.title('SalePrice distribution')","0bb2c8a8":"ntrain = train.shape[0]\nntest = test.shape[0]\ny_train = train.SalePrice.values\nfeatures = pd.concat((train, test), sort=False).reset_index(drop=True)\nfeatures.drop(['SalePrice'], axis=1, inplace=True)\nfeatures.head()","bd78c62f":"features_na = (features.isnull().sum() \/ len(features)) * 100\nfeatures_na = features_na.drop(features_na[features_na == 0].index).sort_values(ascending=False)[:20]\nmissing_data = pd.DataFrame({'Missing Ratio' :features_na})\nmissing_data","21bfc5c2":"f, ax = plt.subplots(figsize=(10, 8))\nplt.xticks(rotation='90')\nsns.barplot(x=features_na.index, y=features_na, color='red')\nplt.xlabel('Features')\nplt.ylabel('Percent of missing values')\nplt.title('Percent missing data by feature')","925eeb13":"\nprint(features['PoolQC'].unique())\nprint(features['MiscFeature'].unique())\nprint(features['Alley'].unique())\nprint(features['Fence'].unique())","1e252da8":"for col in features.columns:\n    length = len(features)\n    null_sum = features[col].isnull().sum()\n    if null_sum \/ length >= 0.80:\n        features[col] = features[col].fillna('None')","5c695dd7":"print(features['PoolQC'].unique())\nprint(features['MiscFeature'].unique())\nprint(features['Alley'].unique())\nprint(features['Fence'].unique())","c1b36fc3":"features[\"FireplaceQu\"] = features[\"FireplaceQu\"].fillna(\"None\")","729f3abb":"#Group by neighborhood and fill in missing value by the median LotFrontage of all the neighborhood\nfeatures[\"LotFrontage\"] = features.groupby(\"Neighborhood\")[\"LotFrontage\"].transform(\n    lambda x: x.fillna(x.median()))","9384d26e":"for col in ('GarageType', 'GarageFinish', 'GarageQual', 'GarageCond'):\n    features[col] = features[col].fillna('None')","cd57a0b4":"for col in ('GarageYrBlt', 'GarageArea', 'GarageCars'):\n    features[col] = features[col].fillna(0)","340cb8e1":"for col in ('BsmtFinSF1', 'BsmtFinSF2', 'BsmtUnfSF','TotalBsmtSF', 'BsmtFullBath', 'BsmtHalfBath'):\n    features[col] = features[col].fillna(0)","ea2a03b7":"for col in ('BsmtQual', 'BsmtCond', 'BsmtExposure', 'BsmtFinType1', 'BsmtFinType2'):\n    features[col] = features[col].fillna('None')","8467e1cd":"features[\"MasVnrType\"] = features[\"MasVnrType\"].fillna(\"None\")\nfeatures[\"MasVnrArea\"] = features[\"MasVnrArea\"].fillna(0)","1775932b":"features['MSZoning'].value_counts()","f82c3613":"features['MSZoning'] = features['MSZoning'].fillna(features['MSZoning'].mode()[0])","94356c53":"features['Utilities'].value_counts()","2e7284ff":"features = features.drop(['Utilities'], axis=1)","baae418a":"features[\"Functional\"] = features[\"Functional\"].fillna(\"Typ\")","d4449422":"features['Electrical'].value_counts()","60e57e3b":"features['Electrical'] = features['Electrical'].fillna(features['Electrical'].mode()[0])","3fdc529b":"features['KitchenQual'].value_counts()","e5826daa":"features['KitchenQual'] = features['KitchenQual'].fillna(features['KitchenQual'].mode()[0])","71ce10dd":"print(features['Exterior1st'].value_counts())\nprint(features['Exterior2nd'].value_counts())","ebc15439":"features['Exterior1st'] = features['Exterior1st'].fillna(features['Exterior1st'].mode()[0])\nfeatures['Exterior2nd'] = features['Exterior2nd'].fillna(features['Exterior2nd'].mode()[0])","25f7f97b":"features['SaleType'].value_counts()","da8142f1":"features['SaleType'] = features['SaleType'].fillna(features['SaleType'].mode()[0])","46b9614e":"features['MSSubClass'] = features['MSSubClass'].fillna(\"None\")","4648edc7":"features_na = (features.isnull().sum() \/ len(features)) * 100\nfeatures_na = features_na.drop(features_na[features_na == 0].index).sort_values(ascending=False)[:20]\nmissing_data = pd.DataFrame({'Missing Ratio' :features_na})\nmissing_data","1d84ed20":"#MSSubClass=The building class\nfeatures['MSSubClass'] = features['MSSubClass'].apply(str)\n\n\n#Changing OverallCond into a categorical variable\nfeatures['OverallCond'] = features['OverallCond'].astype(str)\n\n\n#Year and month sold are transformed into categorical features.\nfeatures['YrSold'] = features['YrSold'].astype(str)\nfeatures['MoSold'] = features['MoSold'].astype(str)","b71f5c0f":"# Adding total sqfootage feature \nfeatures['TotalSF'] = features['TotalBsmtSF'] + features['1stFlrSF'] + features['2ndFlrSF']","6cfee389":"# get dummies all categorical(object) columns\n\ncategorical_cols = features.select_dtypes(include=['object']).columns\nfor col in categorical_cols:\n    dum = pd.get_dummies(features[col],prefix=col, drop_first=True)\n    features.drop(col,axis=1,inplace=True)\n    features = pd.concat([features,dum],axis=1)\nprint(features.shape)","655bd4eb":"train = features[:ntrain]\ntest = features[ntrain:]","617a92a5":"train.head()","288e5872":"from sklearn.model_selection import KFold, cross_val_score, train_test_split\nfrom sklearn.metrics import mean_squared_error","a540633e":"#Validation function\nn_folds = 5\n\ndef rmsle_cv(model):\n    kf = KFold(n_folds, shuffle=True, random_state=42).get_n_splits(train.values)\n    rmse= np.sqrt(-cross_val_score(model, train.values, y_train, scoring=\"neg_mean_squared_error\", cv = kf))\n    return rmse","8a90b0f3":"from sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import StandardScaler, RobustScaler","4e1f1d7e":"from sklearn.linear_model import LinearRegression","9be88ce0":"reg = LinearRegression()","a350460f":"reg.fit(train, y_train)","0462b6e8":"# Reverse the log1p transformation\nreg_pred = np.expm1(reg.predict(test))\nreg_pred","800fe3cb":"# To see what your score is\nscore = rmsle_cv(reg)\nprint(\"\\nRegression score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))","eb453f4e":"from sklearn.tree import DecisionTreeRegressor","baf9cc4a":"tree = make_pipeline(RobustScaler(), DecisionTreeRegressor())","4d30d35f":"tree.fit(train, y_train)","d0e55683":"# Reverse the log transformation\ntree_pred = np.expm1(tree.predict(test))\ntree_pred","5a64d9bd":"# To see what your score is\nscore = rmsle_cv(tree)\nprint(\"\\nDecision Tree score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))","ca337d77":"from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor","657b4a20":"forest = make_pipeline(RobustScaler(), RandomForestRegressor(min_samples_leaf=3, max_features=0.5, n_jobs=-1, random_state=0, n_estimators=100, bootstrap=True))","a2e7719a":"forest.fit(train, y_train)","d14f0663":"# To see what your score is\nscore = rmsle_cv(forest)\nprint(\"\\nRandom Forest score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))","7d810ae6":"from sklearn.linear_model import Lasso","e8e2a875":"lasso = make_pipeline(RobustScaler(), Lasso(alpha =0.0005, random_state=1))","e70f47ed":"lasso.fit(train, y_train)","a9aaf30f":"# Reverse the log transformation\nlasso_pred = np.expm1(lasso.predict(test))\nlasso_pred","e604a64e":"score = rmsle_cv(lasso)\nprint(\"\\nLasso(scaled) score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))","1b38d67d":"from sklearn.linear_model import ElasticNet","ed18187c":"ENet = make_pipeline(RobustScaler(), ElasticNet(alpha=0.0005, l1_ratio=.9, random_state=0))","9d1405cd":"ENet.fit(train, y_train)\nenet_pred = np.expm1(ENet.predict(test))\nenet_pred","64c0b043":"score = rmsle_cv(ENet)\nprint(\"\\nENet(scaled) score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))","316b46ff":"GBoost = GradientBoostingRegressor(n_estimators=3000, learning_rate=0.05,\n                                   max_depth=4, max_features='sqrt',\n                                   min_samples_leaf=3,min_samples_split=10, \n                                   loss='huber', random_state =5)","83761116":"score = rmsle_cv(GBoost)\nprint(\"\\nGBoost score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))","0a10c0fb":"from vecstack import stacking","40753612":"from sklearn.metrics import mean_squared_log_error","1b61354f":"models = [lasso, ENet, GBoost] \nS_train, S_test = stacking(models,                   \n                           train, y_train, test,   \n                           regression=True, \n                           mode='oof_pred_bag',\n                           needs_proba=False,\n                           save_dir=None,\n                           metric=mean_squared_log_error,\n                           n_folds=4,\n                           stratified=True,\n                           shuffle=True,  \n                           random_state=0,\n                           verbose=2)","59417e2a":"model_lasso = lasso.fit(S_train, y_train)\nsm_pred = np.expm1(model_lasso.predict(S_test))\nsm_pred","6a7314c9":"sample = pd.read_csv('..\/input\/sample_submission.csv')\nsample.head()","9b5c7147":"submission = pd.DataFrame(data = sample['Id'], columns= ['Id'])\nsubmission['SalePrice'] = sm_pred","708e9271":"submission.to_csv('submit_2.csv', index=False)","3c7d7af6":"submission.head()","88cc9557":"Only one NA value, and same as Electrical, we set 'TA' (which is the most frequent) for the missing value.","bf2d048f":"## GarageCars","eefe9f73":"# Target Variable","3256d289":"#### Let's find all columns that have nulls","998d98cb":"## TotalBsmtSF","a688dfa0":"For this categorical feature all records are \"AllPub\", except for one \"NoSeWa\" and 2 NA. \nSince the house with 'NoSewa' is in the training set, this feature won't help in predictive modelling. We can then safely remove it.","f5da0140":"Fill in again with most frequent which is \"WD\"","5e57824d":"This Notebook was a team effort from EDSA_Team_03_JHB, for the Kaggle Housing Prices competition.","fe7040e9":"# Multiple Linear Regression","a887fd30":"#### Functional: \nData description says NA means typical","9dae1dea":"## OverallQual","48487243":"It has one NA value. Since this feature has mostly 'SBrkr', we can set that for the missing value.","97a6a9f9":"There aren't any obvious outliers here, so we move on.","50f4311a":"#### KitchenQual: ","6322d8e7":"#### Missing Values?","02b3ec68":"We don't see any significantly noticeable outliers, so we move on.","19d4e431":"### We transform this variable and make it more normally distributed.\n#### We use a log1p transformation: np.log1p = log(1+p)\n#### We do this to speed up the learning and convergence time of Linear ML Models ","029c9f57":"# GarageArea","40b4991f":"We replace Nulls with 'None' because the data description tells us Null means the feature doesn't exist.","5ec05ade":"Lasso is a relatively recent adaptation of ridge which is capable of dropping predictors entirely.   \n\nA lasso model is fit under the constraint of minimizing the following equation:   \n$$\\sum_{i=1}^n(y_i-(a+\\sum_{j=1}^pb_jx_{ij}))^2 + \\alpha\\sum_{j=1}^p|b_j|$$   \n\n$$RSS + \\alpha\\sum_{j=1}^p|b_j|$$\n\n$\\alpha$ above is a tuning parameter","fcb7830e":"# Pip install vecstack library","d4baf9f9":"# train_test_split","0ec5af68":"# More features","2944c077":"Undo the concatenation that we made in the beginning","671f079e":"#### Utilities: ","a73c66ef":"# Lasso","e8afba23":"https:\/\/www.kaggle.com\/pmarcelino\/comprehensive-data-exploration-with-python","366659b7":"# Visualise Correlations","6d0ee11b":"#### Confirm that Nulls were replaced","290512ff":"#### Exterior1st and Exterior2nd: ","6c7dda78":"We use cross validation instead of a normal train_test_split to split our train set.","11f1bbe5":"#### Elastic-net is a mix of both L1(Lasso) and L2(Ridge) regularizations. A penalty is applied to the sum of the absolute values and to the sum of the squared values:\n$$RSS + (1-\\lambda)\\alpha\\sum_{j=1}^pb_j^2 + \\lambda\\alpha\\sum_{j=1}^p|b_j|$$\n$\\alpha$ above is a tuning parameter\n- When $\\lambda$ = 1 we have Lasso Regression\n- When $\\lambda$ = 0 we have Ridge Regression","0e98b859":"Concatenating the train and test datasets in the beginning helps us engineer the features of both datasets at the same time.\nWe will separate them later.","31893b9f":"# Read Data Description","31e8004b":"#### MSZoning (The general zoning classification): ","3c5387ef":"We notice two obvious outliers in 'GrLiv Area vs SalePrice', so we remove them.","14e78825":" We notice that the points are scattered widely across, so we don't remove any points","f1a20811":"# Log-transform target var","74d80949":"'RL' is by far the most common value. So we can fill in missing values with 'RL'","cd9d4869":"As a general rule for our data, we will not be removing any outliers from discrete data. We will just scale our data before using models that are sensitive to outliers.","b97714cd":"#### BsmtQual, BsmtCond, BsmtExposure, BsmtFinType1 and BsmtFinType2: \nFor all these categorical basement-related features, NaN means that there is no basement.","d62ebe93":"https:\/\/www.kaggle.com\/juliencs\/a-study-on-regression-applied-to-the-ames-dataset","e78a2e0f":"## Numerical vars that are really categorical","23ccd2e6":"#### MasVnrArea and MasVnrType: \nNA most likely means no masonry veneer for these houses. We can fill 0 for the area and None for the type.","cab4a1a8":"# Other missing value:","c36d9470":"#### Electrical: ","8250fc18":"# Random Forest","d34bfdf6":"#### BsmtFinSF1, BsmtFinSF2, BsmtUnfSF, TotalBsmtSF, BsmtFullBath and BsmtHalfBath: \nMissing values are likely zero for having no basement","a99f7e0b":"#### MSSubClass: \nNA most likely means No building class. We can replace missing values with None","ae3af179":"## Dealing with Nulls","d3764924":"## Concat Data","445dcc22":"### Use vecstack library to automate the stacking process","92bfe6c8":"### Decision process\nConsider the example of a new datapoint with an X value of 4.5. Suppose we wanted to predict the Y value for this new datapoint - the path that the new point would follow along the decision tree is shown below:       \n\n![Tree3](https:\/\/github.com\/James-Leslie\/Learning-Library\/blob\/master\/Regression\/3_Tree-Based_Regression\/tree_3.png?raw=true)\nThe new datapoint would follow the green path shown above and would eventually be assigned a Y value of 2.   \n","320e1082":"# Model Stacking","3a15fa3f":"- Library that automates the process of Model Stacking for us.\n- Link to the GitHub repo: https:\/\/github.com\/vecxoz\/vecstack","68e350ad":"# Gradient Boosting","511d77a2":"# Make Pipeline","2c8fbcc7":"# Reference to Public Notebooks that helped:","a3fc3d36":"##### The main idea behind the structure of a stacked generalization is to use one or more first level models, make predictions using these models and then use these predictions as features to fit one or more second level models on top","548f38d2":"##### Gradient boosting is a type of machine learning boosting, it specifically boosts the perfomance of Decision Trees\nIt relies on the intuition that the best possible next model, when combined with previous models, minimizes the overall prediction error. The key idea is to set the target outcomes for this next model in order to minimize the error.","426a2f79":"# Feature Engineering","9ec77371":"#### GarageYrBlt, GarageArea and GarageCars: \nWe replace missing data with 0 (Since No garage = no cars in the garage)","2a2318d7":"## 1stFlrSF","16949bf6":"#### The random forest is a model made up of many decision trees. \n##### Rather than just simply averaging the prediction of trees (which we could call a \u201cforest\u201d), this model uses two key concepts that gives it the name random:\n1. Random sampling of training data points when building trees\n2. Random subsets of features considered when splitting node","c2293494":"### Let's look at the distribution of the Target Variable","8759d703":"The Hyper-parameters used in this model were a result of brute force trail and error. Not all possible combinations were explored, but we were satisfied with the chosen combination.","5f6873e2":"\n#### Further investigation of the columns with the most Nulls","36e7d051":"## Columns that have missing values over 80%","e0c1fbff":"#### LotFrontage: \nSince the area of each street connected to the house property most likely has a similar area to other houses in its neighborhood , we can fill in missing values by the median LotFrontage of the neighborhood.","336d68f0":"We don't see any easy way to remove outliers in 'OverallQual vs SalePrice', so we will take care of this by using a Robust Scalar in models that are sensitive to outliers.","d5b3e66f":"# Sample Submission","a1c97818":"# Final Submission","2a54e504":"Here our model is:   \n\n$$Y = \\beta_0 + \\beta_1X_1 + \\beta_2X_2 + ... + \\beta_pX_p$$   \n   \nIn this case, $Y$ is the reponse variable which depends on the $p$ predictor variables.\n\n$\\beta_0$ is the intercept. This can be interpreted as the value of $Y$ when all predictor variables are equal to zero.\n\n$\\beta_j$ is the average effect on $Y$ of a one unit increase in $X_j$.","64c7d50d":"#### Train our meta model(Lasso) using the new stacked train, amd predict using our stacked test set.","7ae9f6dd":"# Elastic Net ","57ef527a":"This validation function uses kfold falidation to make a train_test_split of the training data.\nThe evaluation metric is root mean squared error(rmse). \nIt becomes root mean squared log error(rmsle) because our Target Variable has been log transformed.","73dfb4f3":"#### FireplaceQu: \nData description says NA means \"no fireplace\"","5398bce4":"# Cross Validation Scoring Strategy","f5dc9aa0":"## Adding another feature","bc10e86e":"### There are other probably outliers in the training data. \n#### However, removing them all may badly affect our models if ever there were also outliers in the test data.\n#### Instead of removing them all, we will just manage to make some of our models robust(using Robust Scalar) on them.","a5b78ecc":"# Decision Trees","ad082077":"A machine learning pipeline is used to help automate machine learning workflows. They operate by enabling a sequence of data to be transformed and correlated together in a model that can be tested and evaluated to achieve an outcome, whether positive or negative","776800eb":"#### We will visualise and remove outliers of columns that have a 0.6(or higher) correlation to SalePrice","3bb06463":"#### Get a new train and test set from our base model predictions","720f0beb":"Both Exterior 1 & 2 have only one missing value. We will just substitute in the most common string","cd7bd145":"# Dealing with Categorical Vars","0997ba3d":"#### GarageType, GarageFinish, GarageQual and GarageCond: \nWe replacing missing data with None","be6ccb8f":"- Link for more info: https:\/\/towardsdatascience.com\/automate-stacking-in-python-fc3e7834772e","6644f7cd":"#### SaleType:","a365fa68":"# Outliers","ead0cb35":"## GrLivArea","990bafad":"# Preparation","eddf6f5a":"# Import Data"}}