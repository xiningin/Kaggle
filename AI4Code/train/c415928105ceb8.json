{"cell_type":{"479138f6":"code","065b0dfc":"code","1bad6d85":"code","56123626":"code","23362e14":"code","e037c853":"code","72f7c6d0":"code","d27ea72c":"code","18bdd9ce":"code","d5edc1c9":"code","7e16206e":"code","dda44ad1":"code","8e19c01a":"code","07915696":"code","4679ca18":"code","3f3586f5":"code","9c2c9092":"code","914189a1":"code","76346d85":"code","6e5ed235":"code","1a873019":"code","2451bd41":"code","5ae3f155":"code","a573d42e":"code","d6722b34":"markdown","a07f90c1":"markdown","0699a522":"markdown"},"source":{"479138f6":"from IPython.core.interactiveshell import InteractiveShell\nInteractiveShell.ast_node_interactivity = 'all'\nfrom datetime import datetime\nfrom pytz import timezone\ndatetime.now(timezone('Asia\/Tokyo')).strftime('%Y\/%m\/%d %H:%M:%S')\n\ndef refer_args(x):\n    if type(x) == 'method':\n        print(*x.__code__.co_varnames.split(), sep='\\n')\n    else:\n        print(*[x for x in dir(x) if not x.startswith('__')], sep='\\n')","065b0dfc":"from collections import Counter, defaultdict\nimport os\nfrom operator import itemgetter\nimport re\nimport string\n\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport numpy as np\n\nfrom tqdm import tqdm\nfrom nltk.corpus import stopwords\nfrom nltk.util import ngrams\nfrom nltk.tokenize import word_tokenize\nimport gensim\nfrom gensim.models import word2vec\nfrom wordcloud import WordCloud\nfrom janome.tokenizer import Tokenizer\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.model_selection import train_test_split\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.models import Sequential\nfrom keras.layers import Embedding,LSTM,Dense,SpatialDropout1D\nfrom keras.initializers import Constant\nfrom keras.optimizers import Adam\n \nplt.style.use('ggplot')\nstop = set(stopwords.words('english')) | {\n            'i','im','you','youre','they','theyre','he','hes','she','shes','we','our','us','were','arent',\\\n            'can','cant','could','couldnt','will','wont','would','wouldnt','should','shouldnt','may',\\\n            'dont','didnt','doesnt'}\n\npd.set_option('display.max_colwidth', 200)","1bad6d85":"from gensim.models.doc2vec import Doc2Vec,TaggedDocument\nPretrained_Model=Doc2Vec.load('..\/input\/pretrained-0923-2249\/Pretrained_2249.model')","56123626":"tweet=pd.read_csv('..\/input\/nlp-getting-started\/train.csv')\ntest=pd.read_csv('..\/input\/nlp-getting-started\/test.csv')","23362e14":"df=pd.concat([tweet,test],sort=False)","e037c853":"def remove_URL(text):\n    url = re.compile(r'https?:\/\/\\S+|www\\.\\S+')\n    return url.sub(r'',text)\ndf['text']=df['text'].apply(lambda x: remove_URL(x))\n\ndef remove_html(text):\n    html=re.compile(r'<.*?>')\n    return html.sub(r'',text)\ndf['text']=df['text'].apply(lambda x: remove_html(x))\n\ndef remove_emoji(text):\n    emoji_pattern = re.compile(\"[\"\n                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n                           u\"\\U00002702-\\U000027B0\"\n                           u\"\\U000024C2-\\U0001F251\"\n                           \"]+\", flags=re.UNICODE)\n    return emoji_pattern.sub(r'', text)\ndf['text']=df['text'].apply(lambda x: remove_emoji(x))\n\ndef remove_punct(text):\n    table=str.maketrans('','',string.punctuation)\n    return text.translate(table)\ndf['text']=df['text'].apply(lambda x: remove_punct(x))\n\ndef string_lower(text):\n    return text.lower()\ndf['text']=df['text'].apply(lambda x: string_lower(x))","72f7c6d0":"tweet=df[:len(tweet)]\ntest=df[len(tweet):]","d27ea72c":"words1=tweet['text'][5].split()\nwords2=tweet['text'][3].split()\n' '.join(words1)\n' '.join(words2)","18bdd9ce":"Pretrained_Model.docvecs.similarity_unseen_docs(Pretrained_Model,words1,words2,alpha=1,min_alpha=0.0001,steps=5)","d5edc1c9":"newvecs=[Pretrained_Model.infer_vector(tweet['text'][i].split()) for i in range(len(tweet))]\ntrain=pd.DataFrame(data=newvecs)\ntrain.head()","7e16206e":"Y_train=tweet['target'].apply(lambda x:int(x))\nY_train.head()","dda44ad1":"tweet.head()","8e19c01a":"from tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, Input\nfrom tensorflow.keras.optimizers import Adam","07915696":"model=Sequential()\nmodel.add(Dense(256,activation=\"relu\"))\nmodel.add(Dense(128,activation=\"relu\"))\nmodel.add(Dense(1,activation=\"sigmoid\"))\noptimzer=Adam(learning_rate=1e-5)\n\nmodel.compile(loss='binary_crossentropy',optimizer=optimzer,metrics=['accuracy'])\nmodel.fit(train,Y_train,epochs=200,validation_split=0.2)","4679ca18":"model.summary()","3f3586f5":"Test_newvecs=[Pretrained_Model.infer_vector(test['text'][i].split()) for i in range(len(test))]","9c2c9092":"TEST=pd.DataFrame(data=Test_newvecs)","914189a1":"TEST.head()","76346d85":"predict=model.predict(TEST)","6e5ed235":"predict=np.round(predict).astype(int).reshape(3263)","1a873019":"sub=pd.read_csv('..\/input\/nlp-getting-started\/sample_submission.csv')","2451bd41":"sub['target']=predict","5ae3f155":"sub.to_csv('submission.csv',index=False)","a573d42e":"sub.head()","d6722b34":"# Text Cleaning","a07f90c1":"[Reference](https:\/\/www.kaggle.com\/shahules\/basic-eda-cleaning-and-glove)","0699a522":"# Keras"}}