{"cell_type":{"91cf06f6":"code","01ddb220":"code","de86245b":"code","f5c403aa":"code","9d8d4d33":"code","f578de27":"code","172999dd":"code","cc7d7c73":"code","ec3bf096":"code","04803de6":"code","05e2a825":"code","d33cb50e":"code","16c4b43f":"code","eaefba07":"markdown","8ed88445":"markdown","ae7dcafe":"markdown","9af3e435":"markdown","7aeff8c3":"markdown","170aa113":"markdown","26502dfb":"markdown","0d9b7d20":"markdown","ac4b630f":"markdown","30cf6e58":"markdown","37fa432d":"markdown","a62683a5":"markdown","83787505":"markdown","6d60a59d":"markdown","30258fda":"markdown"},"source":{"91cf06f6":"import numpy as np \nimport pandas as pd \nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline","01ddb220":"df = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/train.csv')\ntest = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/test.csv')","de86245b":"from sklearn.compose import ColumnTransformer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.model_selection import train_test_split\n\n# Define feature column categories by column type\ncategorical_cols = [col for col in df.columns if df[col].dtype == 'object']\nnumeric_cols = [col for col in df.columns if df[col].dtype != 'object'] \n\n# Remove the target column (SalePrice) from our feature list\nnumeric_cols.remove('SalePrice')\n\n# Preprocessing for numerical data\nnumerical_transformer = Pipeline(steps=[\n    ('scaler', MinMaxScaler()),\n    ('imputer', SimpleImputer(strategy='mean'))\n\n])\n\n# Preprocessing for categorical data\ncategorical_transformer = Pipeline(steps=[\n    ('imputer', SimpleImputer(strategy='most_frequent')),\n    ('onehot', OneHotEncoder(handle_unknown='ignore')) # ignore set so new categories in validation set won't trigger an error post test set fit\n])\n\n# Bundle preprocessing for numerical and categorical data\npreprocessor = ColumnTransformer(\n    transformers=[\n        ('numeric', numerical_transformer, numeric_cols),\n        ('categorical', categorical_transformer, categorical_cols)\n    ])","f5c403aa":"\n# Grab target as y, remove target from X\ntrain_test = df.copy()\ny = train_test.SalePrice\nX = train_test.drop(columns=['SalePrice'])\n\n# Split into train, test\ntrain_X, val_X, train_y, val_y = train_test_split(X, y, train_size=0.8, random_state = 17)\n\n# Fit the preprocessor using the training data\ntrain_X_cleaned = preprocessor.fit_transform(train_X)\n\n# Run the validation set (and all future sets) through the transform without fitting again, or else you'll end up with a different pipeline!\nval_X_cleaned = preprocessor.transform(val_X)\n\n# Printing shapes of the processed arrays to make sure we haven't gone too far wrong.\nprint (\"Initial train_X shape: \",train_X.shape)\nprint (\"Initial val_X shape: \",val_X.shape)\nprint (\"Processed train_X shape: \", train_X_cleaned.shape)\nprint (\"Processed val_X shape: \",val_X_cleaned.shape)","9d8d4d33":"import xgboost as xgb\n\n# Create an initial parameter list, which we'll tune as we go.\nparams = {\n    'max_depth':6,\n    'min_child_weight': 1,\n    'eta':.3,\n    'subsample': 1,\n    'colsample_bytree': 1,\n    'objective':'reg:squarederror',\n    'eval_metric':'mae'\n}\n\n# Set num_boost_rounds for future use\nNUM_BOOST_ROUNDS=999\n\n# Take our inputs and format them as DMatrices\ndtrain = xgb.DMatrix(train_X_cleaned, label=train_y)\ndtest = xgb.DMatrix(val_X_cleaned, label=val_y)\n\n# Train our first XGB model!\nxbg_model = xgb.train(\n    params,\n    dtrain,\n    num_boost_round=NUM_BOOST_ROUNDS,\n    evals=[(dtest, \"Test\")],\n    early_stopping_rounds=10\n)\n  \nprint(\"Best MAE: {:.2f}, found at round {}\".format(\n                 xbg_model.best_score,\n                 xbg_model.best_iteration))","f578de27":"# Calculate cross validation\ncv_results = xgb.cv(\n    params,\n    dtrain,\n    num_boost_round=NUM_BOOST_ROUNDS,\n    seed=42,\n    nfold=5,\n    metrics={'mae'},\n    early_stopping_rounds=10\n)\n\n# Plot cross validation results\nplt.plot(cv_results['train-mae-mean'], label='train mae')\nplt.plot(cv_results['test-mae-mean'], label='test mae')\nplt.title(\"XGB Cross Validation Error\")\nplt.xlabel('Round Number')\nplt.ylabel('Mean Absoulte Error (MAE)')\nplt.legend()\nplt.show()","172999dd":"# Tune max_depth and min_child_weight\ngridsearch_params = [\n    (max_depth, min_child_weight)\n    for max_depth in range(3,8)\n    for min_child_weight in range(1,6)\n]\n\n# Define initial best params and MAE\nmin_mae = float(\"Inf\")\nbest_params = None\nfor max_depth, min_child_weight in gridsearch_params:\n    print(\"CV with max_depth={}, min_child_weight={}\".format(\n                             max_depth,\n                             min_child_weight))\n    # Update our parameters\n    params['max_depth'] = max_depth\n    params['min_child_weight'] = min_child_weight\n    # Run CV\n    cv_results = xgb.cv(\n        params,\n        dtrain,\n        num_boost_round=NUM_BOOST_ROUNDS,\n        seed=42,\n        nfold=5,\n        metrics={'mae'},\n        early_stopping_rounds=10\n    )\n    # Update best MAE\n    mean_mae = cv_results['test-mae-mean'].min()\n    boost_rounds = cv_results['test-mae-mean'].idxmin()\n    print(\"\\tMAE {} for {} rounds\\n\".format(mean_mae, boost_rounds))\n    if mean_mae < min_mae:\n        min_mae = mean_mae\n        best_params = (max_depth,min_child_weight)\nprint(\"Best params: {}, {}, MAE: {}\".format(best_params[0], best_params[1], min_mae))","cc7d7c73":"params['max_depth'] = 5\nparams['min_child_weight'] = 1","ec3bf096":"# Next, tune subsample and colsample\ngridsearch_params = [\n    (subsample, colsample)\n    for subsample in [i\/10. for i in range(7,11)]\n    for colsample in [i\/10. for i in range(7,11)]\n]\n\nmin_mae = float(\"Inf\")\nbest_params = None\n# We start by the largest values and go down to the smallest\nfor subsample, colsample in reversed(gridsearch_params):\n    print(\"CV with subsample={}, colsample={}\".format(\n                             subsample,\n                             colsample))\n    # We update our parameters\n    params['subsample'] = subsample\n    params['colsample_bytree'] = colsample\n    # Run CV\n    cv_results = xgb.cv(\n        params,\n        dtrain,\n        num_boost_round=NUM_BOOST_ROUNDS,\n        seed=42,\n        nfold=5,\n        metrics={'mae'},\n        early_stopping_rounds=10\n    )\n    # Update best score\n    mean_mae = cv_results['test-mae-mean'].min()\n    boost_rounds = cv_results['test-mae-mean'].idxmin()\n    print(\"\\tMAE {} for {} rounds\\n\".format(mean_mae, boost_rounds))\n    if mean_mae < min_mae:\n        min_mae = mean_mae\n        best_params = (subsample,colsample)\nprint(\"Best params: {}, {}, MAE: {}\".format(best_params[0], best_params[1], min_mae))","04803de6":"params['subsample'] = 1.0\nparams['colsample_bytree'] = 1.0","05e2a825":"min_mae = float(\"Inf\")\nbest_params = None\nfor eta in [.3, .2, .1, .05, .01, .005]:\n    print(\"CV with eta={}\".format(eta))\n    # We update our parameters\n    params['eta'] = eta\n    # Run and time CV\n    cv_results = xgb.cv(\n            params,\n            dtrain,\n            num_boost_round=NUM_BOOST_ROUNDS,\n            seed=42,\n            nfold=5,\n            metrics=['mae'],\n            early_stopping_rounds=10\n          )\n    # Update best score\n    mean_mae = cv_results['test-mae-mean'].min()\n    boost_rounds = cv_results['test-mae-mean'].idxmin()\n    print(\"\\tMAE {} for {} rounds\\n\".format(mean_mae, boost_rounds))\n    if mean_mae < min_mae:\n        min_mae = mean_mae\n        best_params = eta\nprint(\"Best params: {}, MAE: {}\".format(best_params, min_mae))","d33cb50e":"params['eta'] = 0.05\nparams","16c4b43f":"# Put together a final model, using the parameters found in tuning\nfinal_model = xgb.XGBRegressor(n_estimators=268, \n                               learning_rate=0.05, \n                               max_depth = 5, \n                               min_child_weight=1, \n                               subsample=1.0, \n                               colsample_bytree=1.0)\n\nfinal_pipeline = Pipeline(steps=[('preprocessor', preprocessor),\n                                ('model', final_model)])\n\n# Preprocessing of validation data, get predictions\nfinal_pipeline.fit(train_X,train_y)\ntest_data_labels = final_pipeline.predict(test)\n\n# Create predictions to be submitted!\npd.DataFrame({'Id': test.Id, 'SalePrice': test_data_labels}).to_csv('XGB.csv', index =False)  \nprint(\"Done :D\")","eaefba07":"## Tuning max_depth and min_child_weight\n\nFirst parameters to tune: max_depth and min_child_weight. We'll tune these concurrently using a basic grid search, meaning we'll test all combinations of the two variables in a certain range. Whichever combination has the best cross validation score, we'll pick as our values for the parameters!\n\nWhat do these two parameters control?\n - **max_depth** is the largest depth allowed on any decision tree in the ensemble, where tree depth is the number of nodes from the root down to the farthest away leaf. Larger max_depth values allow for more complex trees, which means a larger chance of capturing complicated features but also a larger chance of overfitting.\n - **min_child_weight** is a regularization factor that changes how often tree nodes split in tree creation. Only nodes with a [hessian](https:\/\/stats.stackexchange.com\/questions\/317073\/explanation-of-min-child-weight-in-xgboost-algorithm) (second order partial derivative) larger than min_child_weight are allowed to split. Smaller min_child_weight values mean more complex trees, and therefore more chance of overfitting.\n \nAs cited in the introduction, this code is partially taken from [Hyperparameter tuning in XGBoost](https:\/\/blog.cambridgespark.com\/hyperparameter-tuning-in-xgboost-4ff9100a3b2f).","8ed88445":"## Untuned XGBoost, Trained with Cross Validation\n\nWhile tuning via a train and validation set like we did above is just fine, even better is taking advantage of XGBoost's built in [cross validation](https:\/\/rdrr.io\/cran\/xgboost\/man\/xgb.cv.html). Let's try those same parameters as above, but using xgb.cv to calculate the cross validation error.","ae7dcafe":"As expected, both train and test set error initially decreases as round number increases, and training set error is less than test set error. However, we test set error stops decreasing much at all after ~20 rounds, so it's good that early_stopping_rounds has cut off the iterations to just 49 rounds rather than our maximum number, num_boost_round=999!\n\nAt this point, if we had infinite time, we could tune the rest of the parameters by training all possible sets of values via a gridsearch. However, often datasets are too large for that approach to be feasible, so instead we'll go through the parameters one or two at a time. This is a common choice of which parameters to tune and in what order to get a decently tuned XGB, even without a full gridsearch!","9af3e435":"After running cross validation on all possible combinations, we see that a max_depth of 5 and min_child_weight of 1 gives us the best MAE, so we'll update our parameter dictionary accordingly.","7aeff8c3":"## Train Untuned XGBoost\n\nBefore we tune XGBoost, we first need to make sure we can run the algorithm! XGBoost uses a set of parameters to determine how quickly it converges and what shape it takes. \n\nTechnical notes about XGBoost that are useful to know before digging into the algorithm:\n - The xgboost algorithm uses [DMatrix](https:\/\/xgboost.readthedocs.io\/en\/latest\/python\/python_api.html#xgboost.DMatrix) input rather than numpy arrays or pandas dataframes. A DMatrix can contain both the training X and y data (the y data is the \"label\" of a DMatrix). \n - Different versions of gradient boosting algorithms give different names to the same tuning parameters, which can get confusing. The following names are equivalent in meaning, although different algorithms may use one or the other as the input name:\n \n     - eta = learning_rate\n     - min_child_weight = min_child_leaf\n     - num_estimators = num_boost_rounds \n     \n[XGBoost](https:\/\/xgboost.readthedocs.io\/en\/latest\/#) uses the left side terms above, whereas [Scikit learn Gradient Boosting](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.ensemble.GradientBoostingClassifier.html) uses the right side terms above.\n\nWhile we'll go into more depth on each of these parameters, let's try to run XGBoost first. We'll start by saving a set of generic values for all of the parameters in a **params** dictionary, which we can then pass to xgb along with our training and validation data.\n\nAlong with our parameters, we need to give XGBoost two pieces of information:\n - **num_boost_round**, which is the maximum number of iterations we'll allow the model to compute. We can set this number high (ex. 999) and hope we never actually have to compute that many iterations, thanks to early_stopping_rounds below!\n - **early_stopping_rounds**, which is the number of rounds (iterations) the algorithm will compute in a row without model improvement. For example, say early_stopping_rounds = 10. After every round, the algorithm checks model performance. If the model performance hasn't improved in 10 rounds, then training stops, even if we haven't gone through all num_boost_round rounds.\n \nIn this first XGB run, we'll report both the best MAE and what round that MAE value occured.","170aa113":"From the cross validation, we're getting that the best subsample and colsample_by_tree values are both 1. Updating the parameter dictionary accordingly!","26502dfb":"Here, we're defining a column transform processor to cleanly deal with numeric and categoric column features. This imputes to fill in NaN values and one hot encodes string (categoric) features.","0d9b7d20":"## Tune eta\n\n**Eta** in XGBoost works the same way as a learning_rate, in that it controls how quickly each step changes. The smaller the eta, the smaller each step, which means the algorithm will likely take more steps to converge. ","ac4b630f":"## Import and Clean Data\n\nThis initial data cleaning code is taken from my earlier notebook on [how to process the housing price dataset using a pipeline, and use that pipeline to submit a basic random forest regressor model](https:\/\/www.kaggle.com\/dinasinclair\/housing-prices-from-basics-to-random-forest). If you haven't looked at the house price dataset yet, I'd recommend starting there for more context! The only reason to jump right into modeling in this notebook that I've gotten a chance to engage with this data previously.","30cf6e58":"The best eta value in this case looks like it's 0.05. Add it to the parameters! Since this is the last parameter we're going to tune, **we can also pay attention to the number of rounds the model took: 268**. If we don't want to use early stopping, we can set the number of rounds manually to 268 (essentially, num_boosting_rounds is the final parameter to tune - it changes so much based on the other parameters that it's key to tune last!).","37fa432d":"## Finished! Submit Your Tuned Model\n\nWe've tuned the main parameters, and we can create a final model! There other parameters to tune (outlined [here](https:\/\/www.analyticsvidhya.com\/blog\/2016\/03\/complete-guide-parameter-tuning-xgboost-with-codes-python\/)) but these five are enough for most purposes.\n\nWhile we've been using the XGBoost algorithm directly for tuning (mostly because it makes cross validation so easy!), there is a scikit learn wrapper around XGBoost called XGBRegressor. [The XGBoost algorithm is more customizeable](https:\/\/stackoverflow.com\/questions\/47152610\/what-is-the-difference-between-xgb-train-and-xgb-xgbregressor-or-xgb-xgbclassif), but the scikit learn wrapper plays more nicely with pipelines (and doesn't need DMatrix input), so use whichever version fits your current need best.\n\nThe main warning there is that **the input variables have different names in the two models!!**. For example, xgb.XGBRegressor uses the phrase \"n_estimators\" for the identical parameter \"num_boost_round\" in the original XGBoost.\n\nHowever, once you correctly translate, we can use xgb.XGBRegressor as our model in a normal columnTransfer pipeline without issues, and submit a tuned XGBoost model. Congrats! \ud83d\ude00 \ud83d\ude03 \ud83d\ude04 ","a62683a5":"## What's XGBoost?\n\nXGBoost stands for eXtreme Gradient Boosting. It's a form of [gradient boosting](https:\/\/towardsdatascience.com\/understanding-gradient-boosting-machines-9be756fe76ab), which iteratively improves on a set (ensemble) of models, usually decision trees. Each round of the XGBoost algorithm creates a new tree to add to the ensemble. XGBoost improves on traditional gradient boost methods by adding **regularization and parallel processing**, making it quite fast. ","83787505":"Now we can put the training data through our transformer! We split the data into a train and test set (since the final test set isn't given to us, realistically we're splitting into train and validation sets here. One for training, one for parameter tuning!).","6d60a59d":"## What's in This Notebook\n\nXGBoost is a fast and accurate algorithm that comes up everywhere. Unfortunately, it's a hard model to tune well - this notebook walks through that tuning process and ends by submitting XGBoost results to the Kaggle House Prices competition. A well tuned XGBoost algorithm will likely get you into the top 40% of the leaderboard. Nothing fancy, but it works well enough and is fast to implement!\n\nShoutout to the following references, which I use throughout! Happy tuning \ud83d\ude0a \n- [Hyperparameter Tuning in XGBoost](https:\/\/blog.cambridgespark.com\/hyperparameter-tuning-in-xgboost-4ff9100a3b2f)\n- [Complete Guide to Parameter Tuning XGBoost](https:\/\/www.analyticsvidhya.com\/blog\/2016\/03\/complete-guide-parameter-tuning-xgboost-with-codes-python\/)\n- [Understanding Gradient Boosting Machines](https:\/\/towardsdatascience.com\/understanding-gradient-boosting-machines-9be756fe76ab)\n- [From Zero to Hero in XGBoost Tuning](https:\/\/towardsdatascience.com\/from-zero-to-hero-in-xgboost-tuning-e48b59bfaf58)\n\n","30258fda":"## Tune subsample and colsample\n\nGreat, time to move on to our next two parameters! What do these parameters control?\n- **subsample** is the fraction of datapoints (rows of the training data) to sample each round (each tree), with 0 meaning no rows are sampled and 1 meaning all rows are sampled. The higher the value, the more likely to overfit the data.\n- **colsample_bytree** is the fraction of features (columns of the training data) to sample each round (each tree), with 0 meaning no columns are sampled and 1 meaning all columns are sampled. The higher the value, the more likely to overfit the data."}}