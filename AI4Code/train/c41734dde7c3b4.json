{"cell_type":{"fdcf20fe":"code","3c788332":"code","960cbaf7":"code","7c9d0215":"code","6a609de2":"code","b09aacd1":"code","e2897895":"code","4b43a8b2":"code","c8600c98":"code","0f07899b":"code","79f7e0ac":"code","ee9c30e2":"code","9fa3f69e":"code","94c6f37e":"code","f69049ac":"code","e82b2dbf":"code","0d0dca73":"code","85e3918f":"code","279a4368":"code","bb1bc291":"code","4edab2a4":"code","30b969ed":"code","b9d5faae":"code","218d575f":"code","40957063":"code","0c027123":"code","4dc11428":"code","d9ae8f0a":"code","1a16d3f3":"markdown","d609f5e2":"markdown","e53d6a40":"markdown","88e6b417":"markdown","66b2f5ca":"markdown","b0aacb4f":"markdown","8c6a063d":"markdown","2d300f4d":"markdown","cbf0be97":"markdown","8dccc509":"markdown","36a5f4d7":"markdown","fc4b1760":"markdown","ebf649ec":"markdown","da48731b":"markdown","40cf5802":"markdown","af28bfef":"markdown"},"source":{"fdcf20fe":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","3c788332":"data = pd.read_csv(\"..\/input\/biomechanical-features-of-orthopedic-patients\/column_2C_weka.csv\")","960cbaf7":"data.head()","7c9d0215":"data.info()","6a609de2":"data.describe()","b09aacd1":"data.loc[:,'class'].value_counts()","e2897895":"# train test split\nx,y = data.loc[:,data.columns!='class',],data.loc[:,'class']\nfrom sklearn.model_selection import train_test_split\nx_train,x_test,y_train,y_test = train_test_split(x,y,test_size=0.3,random_state=1)\nfrom sklearn.neighbors import KNeighborsClassifier\nknn = KNeighborsClassifier(n_neighbors=3)\nknn.fit(x_train,y_train)\nprediction = knn.predict(x_test)\nprint(\"With KNN (k=3) accuracy is: \", knn.score(x_test,y_test))","4b43a8b2":"# model complexity\nneig = np.arange(1,25)\ntrain_accuracy = []\ntest_accuracy = []\n# Loop over different values of k\nfor i, k in enumerate(neig):\n    knn = KNeighborsClassifier(n_neighbors=k)\n    knn.fit(x_train,y_train)\n    train_accuracy.append(knn.score(x_train,y_train))\n    test_accuracy.append(knn.score(x_test,y_test))\n# Plot\nplt.figure(figsize=(13,8))\nplt.plot(neig, test_accuracy, label = \"Testing Accuracy\")\nplt.plot(neig, train_accuracy, label = \"Training Accuracy\")\nplt.legend()\nplt.title('-value VS Accuracy')\nplt.xlabel('Number of Neighbors')\nplt.ylabel('Accuracy')\nplt.xticks(neig)\nplt.savefig('graph.png')\nplt.show()\nprint(\"Best accuracy is {} with K={}\". format(np.max(test_accuracy),1+test_accuracy.index(np.max(test_accuracy))))","c8600c98":"data = pd.read_csv(\"..\/input\/biomechanical-features-of-orthopedic-patients\/column_2C_weka.csv\")","0f07899b":"data[\"class\"] = [1 if each ==\"Abnormal\" else 0 for each in data[\"class\"]]\ny = data[\"class\"].values\nx_data = data.drop([\"class\"],axis=1)","79f7e0ac":"# normalization\nx = ((x_data - np.min(x_data)) \/ (np.max(x_data) - np.min(x_data)))","ee9c30e2":"# train test split\nfrom sklearn.model_selection import train_test_split\nx_train, x_test, y_train, y_test = train_test_split(x,y,test_size=0.3, random_state=1)","9fa3f69e":"# SVM\nfrom sklearn.svm import SVC\nsvm = SVC(random_state=1)\nsvm.fit(x_train,y_train)","94c6f37e":"print(\"Accuracy of svm algo: \", svm.score(x_test,y_test))","f69049ac":"from sklearn.naive_bayes import GaussianNB\nnb = GaussianNB()\nnb.fit(x_train,y_train)\nprint(\"test score with naive bayes: \", nb.score(x_test,y_test))","e82b2dbf":"from sklearn.tree import DecisionTreeClassifier\ndt = DecisionTreeClassifier()\ndt.fit(x_train,y_train)\nprint(\"test score with decision tree: \", dt.score(x_test,y_test))","0d0dca73":"from sklearn.ensemble import RandomForestClassifier\nrf = RandomForestClassifier(n_estimators=100, random_state=1)\nrf.fit(x_train,y_train)\nprint(\"test score with random forest classification: \", rf.score(x_test,y_test))","85e3918f":"from xgboost import XGBClassifier\nxgbc = XGBClassifier()\nxgbc.fit(x_train, y_train)\nprint(\"test score with XGBoost: \", xgbc.score(x_test,y_test))","279a4368":"from catboost import CatBoostClassifier\ncbc = CatBoostClassifier()\ncbc.fit(x_train, y_train)\nprint(\"test score with CatBoost: \", cbc.score(x_test,y_test))","bb1bc291":"from sklearn.linear_model import LogisticRegression\nlr = LogisticRegression()\nlr.fit(x_train,y_train)\nprint(\"test score with logistic regression: \", lr.score(x_test,y_test))","4edab2a4":"X = x.values   # ann needs array as input\nprint(type(X))","30b969ed":"\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.3, random_state=1)","b9d5faae":"import tensorflow as tf\nann = tf.keras.models.Sequential()\nann.add(tf.keras.layers.Dense(units=3, activation='relu')) \nann.add(tf.keras.layers.Dense(units=3, activation='relu'))  \nann.add(tf.keras.layers.Dense(units=1, activation='sigmoid'))\nann.compile(optimizer = 'adam' , loss = 'binary_crossentropy' , metrics = ['accuracy'])\nann.fit(X_train, y_train, batch_size = 32, epochs = 100)","218d575f":"y_pred = ann.predict(X_test)\ny_pred = (y_pred > 0.5)\nfrom sklearn.metrics import accuracy_score\naccuracy_score(y_test, y_pred)","40957063":"#We get the highest accuracy with Random Forest Classification which is %87.\n#Lets evaluate Random Forest prediction with confusion matrix\nfrom sklearn.ensemble import RandomForestClassifier \nrf = RandomForestClassifier(n_estimators=100, random_state=1) \nrf.fit(x_train,y_train) \nprint(\"test score with random forest classification: \", rf.score(x_test,y_test))\n\ny_pred = rf.predict(x_test)\ny_true = y_test\n\nfrom sklearn.metrics import confusion_matrix\ncm = confusion_matrix(y_true,y_pred)\nprint(cm)","0c027123":"# confusion matrix visualiation\nf,ax = plt.subplots(figsize=(5,5))\nsns.heatmap(cm,annot=True,linewidths=0.5, linecolor=\"red\",fmt=\".0f\",ax=ax)\nplt.xlabel(\"y_pred\")\nplt.ylabel(\"y_true\")\nplt.show","4dc11428":"from sklearn.model_selection import cross_val_score\naccuracies = cross_val_score(estimator = rf, X = x_train, y = y_train, cv = 10)\nprint(\"Accuracy: {:.2f} %\".format(accuracies.mean()*100))\nprint(\"Standard Deviation: {:.2f} %\".format(accuracies.std()*100))","d9ae8f0a":"from sklearn.model_selection import GridSearchCV\nparameters = [{'n_estimators':[50, 100, 200, 500], 'criterion': ['gini', 'entropy']}]\ngrid_search = GridSearchCV(estimator = rf, \n                           param_grid = parameters,\n                           scoring = 'accuracy',\n                           cv = 10,\n                           n_jobs = -1)\ngrid_search.fit(x_train, y_train)\nbest_accuracy = grid_search.best_score_   \nbest_parameters = grid_search.best_params_  \nprint(\"Best Accuracy: {:.2f} %\".format(best_accuracy*100))\nprint(\"Best Parameters: \", best_parameters)","1a16d3f3":"**CONFUSION MATRIX**","d609f5e2":"**CatBoost**","e53d6a40":"**DECISION TREE CLASSIFICATION**","88e6b417":"**K-NEAREST NEIGHBOURS (KNN) CLASSIFICATION**","66b2f5ca":"**Applying k-Fold Cross Validation**","b0aacb4f":"When we apply apply grid-search with 10-fold cross validation, we get the best result if we choose gini as criterion and 200 as n_estimators.So we manage to reach an accuracy of 87.56%","8c6a063d":"This kernel has been prepared for practicing classification algorithms with the data of \"biomechanical features of orthopedic patients\".","2d300f4d":"**NAIVE-BAYES CLASSIFICATION**","cbf0be97":"**SUPPORT VECTOR MACHINE CLASSIFICATION (SVM)**","8dccc509":"**XGBoost**","36a5f4d7":"**Applying Grid Search to find the best model and the best parameters**","fc4b1760":"We made 6 wrong predictions for both abnormal and normal samples in data with Random Forest Classification.","ebf649ec":"**EXPLORATORY DATA ANALYSIS**","da48731b":"**RANDOM FOREST CLASSIFICATION**","40cf5802":"**LOGISTIC REGRESSION**","af28bfef":"ARTIFICIAL NEURAL NETWORK (ANN)"}}