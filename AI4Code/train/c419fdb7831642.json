{"cell_type":{"f4b991dc":"code","b1b14903":"code","e37b5b8b":"code","7cdb26d4":"code","c1e92aff":"code","02c530a9":"code","c0c18cb8":"code","2b11934f":"code","80aa33fc":"code","02a950b6":"code","08deea1e":"code","1a8e0b17":"code","ffc4a96e":"code","c4e2db62":"code","79090efc":"code","409b2d14":"code","464d6dd2":"code","fead8126":"code","fe9a7a9e":"code","3f091400":"code","bb7ef0cf":"code","4149fe8e":"code","6b091c04":"code","01bee778":"code","4c775e25":"code","71e2d513":"code","a0d02bbc":"code","582f5ac3":"code","fa8fbcb7":"code","35b4415f":"code","5c375c7c":"code","c56501d5":"code","aea47e3f":"code","c66af7a0":"code","cc013f76":"markdown","32b8477a":"markdown","82509fb6":"markdown","977688d3":"markdown","68b604a5":"markdown","9afe076a":"markdown","9a17c6c5":"markdown","f2139b48":"markdown","f3f576a6":"markdown","4a389a40":"markdown"},"source":{"f4b991dc":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\nimport matplotlib.pyplot as plt\nimport seaborn as sns               # Provides a high level interface for drawing attractive and informative statistical graphics\n%matplotlib inline\nsns.set()\nfrom subprocess import check_output\n\nimport warnings                                            # Ignore warning related to pandas_profiling\nwarnings.filterwarnings('ignore') \n\ndef annot_plot(ax,w,h):                                    # function to add data to plot\n    ax.spines['top'].set_visible(False)\n    ax.spines['right'].set_visible(False)\n    for p in ax.patches:\n        ax.annotate('{0:.1f}'.format(p.get_height()), (p.get_x()+w, p.get_height()+h))\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","b1b14903":"df = pd.read_csv('..\/input\/creditcard.csv')","e37b5b8b":"df.head()","7cdb26d4":"len(df[df['Class']==1]), len(df[df['Class']==0])","c1e92aff":"percentage_of_Class_0 = ((df[df['Class']==0].count())\/df['Class'].count())*100\npercentage_of_Class_1 = ((df[df['Class']==1].count())\/df['Class'].count())*100\nprint(percentage_of_Class_0['Class'],'%')\nprint(percentage_of_Class_1['Class'],'%')","02c530a9":"ax = sns.countplot('Class',data = df)\nannot_plot(ax, 0.08, 1)","c0c18cb8":"y = df['Class']\nx = df.drop('Class', axis = 1)","2b11934f":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(x, y, test_size = 0.30, random_state = 42)","80aa33fc":"from sklearn.preprocessing import StandardScaler\nScaler_X = StandardScaler()\nX_train = Scaler_X.fit_transform(X_train)\nX_test = Scaler_X.transform(X_test)","02a950b6":"from sklearn.linear_model import LogisticRegression\nlr = LogisticRegression()\nlr.fit(X_train,y_train)\ny_pred = lr.predict(X_test)","08deea1e":"lr.score(X_test,y_test)","1a8e0b17":"from sklearn.metrics import accuracy_score, confusion_matrix\nprint(accuracy_score(y_pred,y_test))\nprint(confusion_matrix(y_pred,y_test))","ffc4a96e":"from xgboost import XGBClassifier\nmodel = XGBClassifier()\nmodel.fit(X_train,y_train)\ny_pred = model.predict(X_test)\nprint(accuracy_score(y_pred,y_test))\nprint(confusion_matrix(y_pred,y_test))","c4e2db62":"from sklearn.model_selection import KFold\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.metrics import confusion_matrix, accuracy_score\nk_fold = KFold(n_splits=10, shuffle=True, random_state=0)","79090efc":"# Class count\ncount_class_0, count_class_1 = df.Class.value_counts()\n\n# Divide by class\ndf_class_0 = df[df['Class'] == 0]\ndf_class_1 = df[df['Class'] == 1]","409b2d14":"df_class_0_under = df_class_0.sample(count_class_1)\ndf_test_under = pd.concat([df_class_0_under, df_class_1], axis=0)\n\nprint('Random under-sampling:')\nprint(df_test_under.Class.value_counts())\ndf_test_under.Class.value_counts().plot(kind='bar',title = 'count(Class)')","464d6dd2":"y = df_test_under['Class']\nx = df_test_under.drop('Class', axis = 1)","fead8126":"X_train_under, X_test_under, y_train_under, y_test_under = train_test_split(x,y, test_size = 0.20, random_state = 42)\n\nmodel = XGBClassifier()\nmodel.fit(X_train_under,y_train_under)\ny_under_pred = model.predict(X_test_under)\n\nprint(accuracy_score(y_under_pred,y_test_under)) \nconfusion_matrix(y_under_pred,y_test_under)","fe9a7a9e":"from sklearn.linear_model import LogisticRegression\nlr = LogisticRegression()\nlr.fit(X_train_under,y_train_under)\nylr_under_pred = lr.predict(X_test_under)\n\nacuuracy_score = accuracy_score(y_under_pred,y_test_under)\nprint(acuuracy_score) \ncm = confusion_matrix(ylr_under_pred,y_test_under)\ncm","3f091400":"df_class_1_over = df_class_1.sample(count_class_0, replace=True)\ndf_test_over = pd.concat([df_class_0, df_class_1_over], axis=0)\n\nprint('Random over-sampling:')\nprint(df_test_over.Class.value_counts())\n\ndf_test_over.Class.value_counts().plot(kind='bar', title='Count (target)');","bb7ef0cf":"y_over = df_test_over['Class']\nX_over = df_test_over.drop('Class', axis = 1)","4149fe8e":"X_train_over, X_test_over, y_train_over, y_test_over = train_test_split(X_over,y_over, test_size = 0.20, random_state = 42)\nmodel.fit(X_train_over,y_train_over)\ny_over_pred = model.predict(X_test_over)\n\nprint(accuracy_score(y_over_pred,y_test_over))\nconfusion_matrix(y_over_pred, y_test_over)","6b091c04":"lr.fit(X_train_over,y_train_over)\nylr_over_pred = lr.predict(X_test_over)\n\nprint(accuracy_score(ylr_over_pred,y_test_over))\nconfusion_matrix(ylr_over_pred, y_test_over)\n","01bee778":"import imblearn","4c775e25":"from sklearn.datasets import make_classification\n\nX, y = make_classification(\n    n_classes=2, class_sep=1.5, weights=[0.9, 0.1],\n    n_informative=3, n_redundant=1, flip_y=0,\n    n_features=20, n_clusters_per_class=1,\n    n_samples=200, random_state=10\n)\n\ndf = pd.DataFrame(X)\ndf['Class'] = y\n\ndf.Class.value_counts().plot(kind = 'bar', title = 'count(Class)')","71e2d513":"def plot_2d_space(X, y, label='Classes'):   \n    colors = ['#1F77B4', '#FF7F0E']\n    markers = ['o', 's']\n    for l, c, m in zip(np.unique(y), colors, markers):\n        plt.scatter(\n            X[y==l, 0],\n            X[y==l, 1],\n            c=c, label=l, marker=m\n        )\n    plt.title(label)\n    plt.legend(loc='upper right')\n    plt.show()","a0d02bbc":"from sklearn.decomposition import PCA\n\npca = PCA(n_components=2)\nX = pca.fit_transform(X)\n\nplot_2d_space(X, y, 'Imbalanced dataset (2 PCA components)')\n","582f5ac3":"#Random under-sampling and over-sampling with imbalanced-learn\n\nfrom imblearn.under_sampling import RandomUnderSampler\n\nrus = RandomUnderSampler(return_indices = True)\nX_rus, y_rus, id_rus = rus.fit_sample(X,y)\n\nprint('Removed indexes:', id_rus)\n\nplot_2d_space(X_rus, y_rus, 'Random under-sampling')","fa8fbcb7":"from imblearn.over_sampling import RandomOverSampler\n\nros = RandomOverSampler()\nX_ros, y_ros = ros.fit_sample(X, y)\n\nprint(X_ros.shape[0] - X.shape[0], 'new random picked points')\n\nplot_2d_space(X_ros, y_ros, 'Random over-sampling')","35b4415f":"from imblearn.under_sampling import TomekLinks\n\ntl = TomekLinks(return_indices=True, ratio='majority')\nX_tl, y_tl, id_tl = tl.fit_sample(X, y)\n\nprint('Removed indexes:', id_tl)\n\nplot_2d_space(X_tl, y_tl, 'Tomek links under-sampling')","5c375c7c":"from imblearn.under_sampling import ClusterCentroids\n\ncc = ClusterCentroids(ratio={0: 10})\nX_cc, y_cc = cc.fit_sample(X, y)\n\nplot_2d_space(X_cc, y_cc, 'Cluster Centroids under-sampling')","c56501d5":"from imblearn.over_sampling import SMOTE\n\nsmote = SMOTE(ratio='minority')\nX_sm, y_sm = smote.fit_sample(X, y)\n\nplot_2d_space(X_sm, y_sm, 'SMOTE over-sampling')","aea47e3f":"from imblearn.combine import SMOTETomek\n\nsmt = SMOTETomek(ratio='auto')\nX_smt, y_smt = smt.fit_sample(X, y)\n\nplot_2d_space(X_smt, y_smt, 'SMOTE + Tomek links')","c66af7a0":"# ROC CURVE\nlr = LogisticRegression(C = best_c, penalty = 'l1')\ny_pred_undersample_score = lr.fit(X_train_under,y_train_under.values.ravel()).decision_function(X_test_under.values)\n\nfpr, tpr, thresholds = roc_curve(y_test_under.values.ravel(),y_pred_under_score)\nroc_auc = auc(fpr,tpr)\n\n# Plot ROC\nplt.title('Receiver Operating Characteristic')\nplt.plot(fpr, tpr, 'b',label='AUC = %0.2f'% roc_auc)\nplt.legend(loc='lower right')\nplt.plot([0,1],[0,1],'r--')\nplt.xlim([-0.1,1.0])\nplt.ylim([-0.1,1.01])\nplt.ylabel('True Positive Rate')\nplt.xlabel('False Positive Rate')\nplt.show()\n","cc013f76":"as we can see that we are getting 99% accuracy for this model but this is not the case. Either majority class is overlapping or the minority class is being ignored.","32b8477a":"## Random over_sampling:","82509fb6":"## Over-sampling followed by under-sampling","977688d3":"## Random under-sampling:","68b604a5":"## Python imbalanced-learn module\nA number of more sophisticated resapling techniques have been proposed in the scientific literature.\n\nFor example, we can cluster the records of the majority class, and do the under-sampling by removing records from each cluster, thus seeking to preserve information. In over-sampling, instead of creating exact copies of the minority class records, we can introduce small variations into those copies, creating more diverse synthetic samples.\n\nLet's apply some of these resampling techniques, using the Python library imbalanced-learn. It is compatible with scikit-learn and is part of scikit-learn-contrib projects","9afe076a":"from sklearn import model_selection\nfrom sklearn.neighbors import KNeighborsClassifier\n\n#Neighbors\nneighbors = np.arange(0,25)\n\n#Create empty list that will hold cv scores\ncv_scores = []\n\nfor k in neighbors:\n    k_value = k+1\n    knn = KNeighborsClassifier(n_neighbors = k_value, weights='uniform', p=2, metric='euclidean')\n    kfold = model_selection.KFold(n_splits=10, random_state=123)\n    scores = model_selection.cross_val_score(knn, X_train, y_train, cv=kfold, scoring='accuracy')\n    cv_scores.append(scores.mean()*100)\n    print(\"k=%d %0.2f (+\/- %0.2f)\" % (k_value, scores.mean()*100, scores.std()*100))\n\noptimal_k = neighbors[cv_scores.index(max(cv_scores))]\nprint (\"The optimal number of neighbors is %d with %0.1f%%\" % (optimal_k, cv_scores[optimal_k]))\nplt.plot(neighbors, cv_scores)\nplt.xlabel('Number of Neighbors K')\nplt.ylabel('Train Accuracy')\nplt.show()","9a17c6c5":"## imblearn.under_sampling: Under-sampling methods","f2139b48":"## Over-sampling: SMOTE\nSMOTE (Synthetic Minority Oversampling TEchnique) consists of synthesizing elements for the minority class, based on those that already exist. It works randomly picingk a point from the minority class and computing the k-nearest neighbors for this point. The synthetic points are added between the chosen point and its neighbors.","f3f576a6":"## Plotting ROC curve and Precision-Recall curve.\n* I find precision-recall curve much more convenient in this case as our problems relies on the \"positive\" class being more interesting than the negative class, but as we have calculated the recall precision, I am not going to plot the precision recall curves yet.\n\n* AUC and ROC curve are also interesting to check if the model is also predicting as a whole correctly and not making many errors","4a389a40":"## Split data into training and test set:"}}